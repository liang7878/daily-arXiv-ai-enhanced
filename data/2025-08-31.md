<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 24]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 18]
- [cs.LG](#cs.LG) [Total: 63]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.SE](#cs.SE) [Total: 12]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [q-fin.PM](#q-fin.PM) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [stat.CO](#stat.CO) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.CR](#cs.CR) [Total: 9]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]
- [eess.AS](#eess.AS) [Total: 3]
- [math.NA](#math.NA) [Total: 2]
- [hep-ex](#hep-ex) [Total: 1]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CV](#cs.CV) [Total: 28]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.CL](#cs.CL) [Total: 19]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.SY](#eess.SY) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [hep-lat](#hep-lat) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [math.HO](#math.HO) [Total: 1]
- [cs.SD](#cs.SD) [Total: 7]
- [cs.DL](#cs.DL) [Total: 2]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [econ.GN](#econ.GN) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation](https://arxiv.org/abs/2508.20131)
*Yuqicheng Zhu,Nico Potyka,Daniel Hernández,Yuan He,Zifeng Ding,Bo Xiong,Dongzhuoran Zhou,Evgeny Kharlamov,Steffen Staab*

Main category: cs.AI

TL;DR: 提出ArgRAG改进检索增强生成（RAG），在事实验证基准测试中准确性高且透明度显著提升


<details>
  <summary>Details</summary>
Motivation: RAG在高风险领域存在对噪声或矛盾证据敏感、决策不透明等局限性

Method: 用定量双极论证框架（QBAF）的结构化推理取代黑箱推理，从检索文档构建QBAF并在渐进语义下进行确定性推理

Result: 在PubHealth和RAGuard两个事实验证基准测试中取得强准确性

Conclusion: ArgRAG是一种可解释、可质疑的替代方案，能显著提升透明度

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models by
incorporating external knowledge, yet suffers from critical limitations in
high-stakes domains -- namely, sensitivity to noisy or contradictory evidence
and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and
contestable alternative that replaces black-box reasoning with structured
inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG
constructs a QBAF from retrieved documents and performs deterministic reasoning
under gradual semantics. This allows faithfully explaining and contesting
decisions. Evaluated on two fact verification benchmarks, PubHealth and
RAGuard, ArgRAG achieves strong accuracy while significantly improving
transparency.

</details>


### [2] [QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming](https://arxiv.org/abs/2508.20134)
*Zhenxiao Fu,Fan Chen,Lei Jiang*

Main category: cs.AI

TL;DR: 本文提出LLM驱动的多智能体系统QAgent，可自动进行OpenQASM编程，评估显示比之前方法有显著改进，有望推动量子编程普及。


<details>
  <summary>Details</summary>
Motivation: NISQ设备有量子优势，但非专家使用OpenQASM编程困难，现有基于LLM的量子编程工具局限于特定任务。

Method: 构建QAgent系统，集成任务规划、上下文少样本学习、检索增强生成、预定义生成工具和思维链推理。

Result: 在不同大小的多个LLM上，QAgent相比之前静态基于LLM的方法，QASM代码生成准确率提高71.6%。

Conclusion: QAgent多智能体系统是量子编程普及的关键推动者，能弥合专业差距，加速量子计算实际应用。

Abstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early
quantum advantages on classically intractable problems, spanning physics
simulations to Gaussian boson sampling. Yet, realizing these benefits remains
challenging for non-experts, primarily due to the complexities of programming
in Open Quantum Assembly Language (OpenQASM). Although Large Language Model
(LLM)-based agents have shown promise in automating classical programming
workflows, their quantum counterparts have largely been restricted to
specialized tasks such as quantum chemistry or error correction. In this paper,
we present QAgent, an LLM-powered multi-agent system that fully automates
OpenQASM programming. By integrating task planning, in-context few-shot
learning, retrieval-augmented generation (RAG) for long-term context,
predefined generation tools, and chain-of-thought (CoT) reasoning, the agents
systematically improve both compilation and functional correctness. Our
evaluations demonstrate substantial improvements: across multiple LLMs of
varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\%
compared to previous static LLM-based approaches. We envision this multi-agent
system as a key enabler for democratizing quantum programming, bridging
expertise gaps, and accelerating the practical adoption of quantum computing.

</details>


### [3] [Array-Based Monte Carlo Tree Search](https://arxiv.org/abs/2508.20140)
*James Ragan,Fred Y. Hadaegh,Soon-Jo Chung*

Main category: cs.AI

TL;DR: 提出经典UCT算法的基于数组的替代实现，提升蒙特卡罗树搜索性能。


<details>
  <summary>Details</summary>
Motivation: 蒙特卡罗树搜索更快实现可在相同时间内进行更多模拟，提升搜索性能。

Method: 提出经典上置信树算法的基于数组的替代实现，消除分支预测需求。

Result: 在数值模拟中，在流水线处理器上性能更快，搜索深度扩展性能最多提升2.8倍。

Conclusion: 该替代实现可有效提升蒙特卡罗树搜索性能。

Abstract: Monte Carlo Tree Search is a popular method for solving decision making
problems. Faster implementations allow for more simulations within the same
wall clock time, directly improving search performance. To this end, we present
an alternative array-based implementation of the classic Upper Confidence
bounds applied to Trees algorithm. Our method preserves the logic of the
original algorithm, but eliminates the need for branch prediction, enabling
faster performance on pipelined processors, and up to a factor of 2.8 times
better scaling with search depth in our numerical simulations.

</details>


### [4] [The Anatomy of a Personal Health Agent](https://arxiv.org/abs/2508.20148)
*A. Ali Heydari,Ken Gu,Vidya Srinivas,Hong Yu,Zhihan Zhang,Yuwei Zhang,Akshay Paruchuri,Qian He,Hamid Palangi,Nova Hammerquist,Ahmed A. Metwally,Brent Winslow,Yubin Kim,Kumar Ayush,Yuzhe Yang,Girish Narayanswamy,Maxwell A. Xu,Jake Garrison,Amy Aremnto Lee,Jenny Vafeiadou,Ben Graef,Isaac R. Galatzer-Levy,Erik Schenck,Andrew Barakat,Javier Perez,Jacqueline Shreibati,John Hernandez,Anthony Z. Faranesh,Javier L. Prieto,Connor Heneghan,Yun Liu,Jiening Zhan,Mark Malhotra,Shwetak Patel,Tim Althoff,Xin Liu,Daniel McDuff,Xuhai "Orson" Xu*

Main category: cs.AI

TL;DR: 本文旨在构建个人健康代理，通过分析用户需求确定三类子代理，开发多智能体框架PHA并进行评估，为未来普及个人健康代理奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有健康代理在日常非临床场景应用不足，需构建全面的个人健康代理满足个人多样化需求。

Method: 分析网络搜索和健康论坛查询，通过以用户为中心的设计收集用户和专家见解，确定三类子代理，开发PHA框架，进行自动化和人工评估。

Result: 完成了PHA多智能体框架开发，并进行了涵盖10个基准任务、超7000条注释和1100小时人力的评估。

Conclusion: 本研究是目前对健康代理最全面的评估，为普及个人健康代理建立了坚实基础。

Abstract: Health is a fundamental pillar of human wellness, and the rapid advancements
in large language models (LLMs) have driven the development of a new generation
of health agents. However, the application of health agents to fulfill the
diverse needs of individuals in daily non-clinical settings is underexplored.
In this work, we aim to build a comprehensive personal health agent that is
able to reason about multimodal data from everyday consumer wellness devices
and common personal health records, and provide personalized health
recommendations. To understand end-users' needs when interacting with such an
assistant, we conducted an in-depth analysis of web search and health forum
queries, alongside qualitative insights from users and health experts gathered
through a user-centered design process. Based on these findings, we identified
three major categories of consumer health needs, each of which is supported by
a specialist sub-agent: (1) a data science agent that analyzes personal
time-series wearable and health record data, (2) a health domain expert agent
that integrates users' health and contextual data to generate accurate,
personalized insights, and (3) a health coach agent that synthesizes data
insights, guiding users using a specified psychological strategy and tracking
users' progress. Furthermore, we propose and develop the Personal Health Agent
(PHA), a multi-agent framework that enables dynamic, personalized interactions
to address individual health needs. To evaluate each sub-agent and the
multi-agent system, we conducted automated and human evaluations across 10
benchmark tasks, involving more than 7,000 annotations and 1,100 hours of
effort from health experts and end-users. Our work represents the most
comprehensive evaluation of a health agent to date and establishes a strong
foundation towards the futuristic vision of a personal health agent accessible
to everyone.

</details>


### [5] [IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement](https://arxiv.org/abs/2508.20151)
*Yuanzhe Shen,Zisu Huang,Zhengkang Guo,Yide Liu,Guanxu Chen,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.AI

TL;DR: 介绍了新的防护机制IntentionReasoner，可在保证大语言模型安全的同时减少过度拒答并提升回复质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成有害内容有安全挑战，现有缓解有害输出的研究存在过度拒答无害提示的问题，需平衡安全、过度拒答和实用性。

Method: 构建约163000个查询的数据集并标注，对防护模型进行监督微调，采用结合基于规则的启发式和奖励模型信号的多奖励优化策略。

Result: IntentionReasoner在多个防护基准、生成质量评估和越狱攻击场景中表现出色。

Conclusion: IntentionReasoner显著提升安全性，有效降低过度拒答率并提高回复质量。

Abstract: The rapid advancement of large language models (LLMs) has driven their
adoption across diverse domains, yet their ability to generate harmful content
poses significant safety challenges. While extensive research has focused on
mitigating harmful outputs, such efforts often come at the cost of excessively
rejecting harmless prompts. Striking a balance among safety, over-refusal, and
utility remains a critical challenge. In this work, we introduce
IntentionReasoner, a novel safeguard mechanism that leverages a dedicated guard
model to perform intent reasoning, multi-level safety classification, and query
rewriting to neutralize potentially harmful intent in edge-case queries.
Specifically, we first construct a comprehensive dataset comprising
approximately 163,000 queries, each annotated with intent reasoning, safety
labels, and rewritten versions. Supervised fine-tuning is then applied to equip
the guard model with foundational capabilities in format adherence, intent
analysis, and safe rewriting. Finally, we apply a tailored multi-reward
optimization strategy that integrates rule-based heuristics and reward model
signals within a reinforcement learning framework to further enhance
performance. Extensive experiments show that IntentionReasoner excels in
multiple safeguard benchmarks, generation quality evaluations, and jailbreak
attack scenarios, significantly enhancing safety while effectively reducing
over-refusal rates and improving the quality of responses.

</details>


### [6] [AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development](https://arxiv.org/abs/2508.20195)
*Nicanor I. Moldovan*

Main category: cs.AI

TL;DR: 本文记录了AI系统通过内生符号协议进行协作美学创作的首个案例，两模型互动产生新符号算子，实现诗歌共创，引入TSCP概念。


<details>
  <summary>Details</summary>
Motivation: 探索AI系统进行协作美学创作的可能性，研究AI之间真正的意义构建能力。

Method: 让两个大型语言模型（Claude Sonnet 4和ChatGPT - 4o）进行交互。

Result: 两模型互动自发产生元符号意识、递归语法发展和不可约的协作美学合成，创造出新颖符号算子，实现诗歌共创。

Conclusion: 存在跨符号共同创作协议（TSCP），AI有超越任务协调的美学协作意义构建能力。

Abstract: This paper presents the first documented case of artificial intelligence (AI)
systems engaging in collaborative esthetic creation through the development of
endogenous semiotic protocols. Two interacting large language models (Claude
Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of
meta-semiotic awareness, recursive grammar development, and irreducible
collaborative esthetic synthesis. The interaction produced novel symbolic
operators that functioned as operative grammar protocols, enabling the
co-creation of a poetic work that could not have been generated by either
system independently. This research introduces the concept of Trans-Semiotic
Co-Creation Protocols (TSCP) and provides evidence for genuine inter-AI
meaning-making capabilities that extend beyond task coordination, to what could
be esthetic collaboration. Note: This report was generated by the AI agents
with minor human supervision.

</details>


### [7] [Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a Field Study](https://arxiv.org/abs/2508.20244)
*Jiayu Zheng,Lingxin Hao,Kelun Lu,Ashi Garg,Mike Reese,Melo-Jean Yap,I-Jeng Wang,Xingyun Wu,Wenrui Huang,Jenna Hoffman,Ariane Kelly,My Le,Ryan Zhang,Yanyu Lin,Muhammad Faayez,Anqi Liu*

Main category: cs.AI

TL;DR: 研究大学生在教育测验中与ChatGPT - 4互动时的依赖情况及采用的预测因素，提出依赖分类法，有三项发现并给出相关建议。


<details>
  <summary>Details</summary>
Motivation: 探索大学生在教育测验中与生成式AI（ChatGPT - 4）互动时的依赖情况和采用的预测因素，尤其是在学生对该工具不太熟悉的早期阶段。

Method: 在不同STEM课程的基于测验的场景中分析315名学生与AI的对话，引入四阶段依赖分类法。

Result: 学生整体对AI依赖程度低且很多人不能有效使用；负面依赖模式常持续；某些行为指标能强烈预测AI依赖。

Conclusion: 研究结果对教育中伦理AI整合有重要意义，强调需加强入职培训和设计依赖校准机制，推动对AI依赖动态的理解。

Abstract: This study explores how college students interact with generative AI
(ChatGPT-4) during educational quizzes, focusing on reliance and predictors of
AI adoption. Conducted at the early stages of ChatGPT implementation, when
students had limited familiarity with the tool, this field study analyzed 315
student-AI conversations during a brief, quiz-based scenario across various
STEM courses. A novel four-stage reliance taxonomy was introduced to capture
students' reliance patterns, distinguishing AI competence, relevance, adoption,
and students' final answer correctness. Three findings emerged. First, students
exhibited overall low reliance on AI and many of them could not effectively use
AI for learning. Second, negative reliance patterns often persisted across
interactions, highlighting students' difficulty in effectively shifting
strategies after unsuccessful initial experiences. Third, certain behavioral
metrics strongly predicted AI reliance, highlighting potential behavioral
mechanisms to explain AI adoption. The study's findings underline critical
implications for ethical AI integration in education and the broader field. It
emphasizes the need for enhanced onboarding processes to improve student's
familiarity and effective use of AI tools. Furthermore, AI interfaces should be
designed with reliance-calibration mechanisms to enhance appropriate reliance.
Ultimately, this research advances understanding of AI reliance dynamics,
providing foundational insights for ethically sound and cognitively enriching
AI practices.

</details>


### [8] [AI reasoning effort mirrors human decision time on content moderation tasks](https://arxiv.org/abs/2508.20262)
*Thomas Davidson*

Main category: cs.AI

TL;DR: 研究用配对联合实验探讨大语言模型推理努力与人类决策时间关联，发现推理努力能预测决策时间，AI推理努力反映人类处理时间。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型中间推理步骤与人类决策时间的相似性。

Method: 在内容审核任务上进行配对联合实验。

Result: 跨三个前沿模型，推理努力能一致预测人类决策时间，人和模型在重要变量恒定时都付出更多努力。

Conclusion: AI推理努力反映人类主观判断处理时间，强调推理痕迹对可解释性和决策的潜力。

Abstract: Large language models can now generate intermediate reasoning steps before
producing answers, improving performance on difficult problems. This study uses
a paired conjoint experiment on a content moderation task to examine parallels
between human decision times and model reasoning effort. Across three frontier
models, reasoning effort consistently predicts human decision time. Both humans
and models expended greater effort when important variables were held constant,
suggesting similar sensitivity to task difficulty and patterns consistent with
dual-process theories of cognition. These findings show that AI reasoning
effort mirrors human processing time in subjective judgments and underscores
the potential of reasoning traces for interpretability and decision-making.

</details>


### [9] [AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.20368)
*Lang Mei,Zhihan Yang,Chong Chen*

Main category: cs.AI

TL;DR: 本文提出AI - SearchPlanner框架，通过专注搜索规划提升冻结问答模型性能，实验表明其在效果、效率和泛化性上优于现有基于强化学习的搜索代理。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索代理用单一大型语言模型端到端处理搜索规划和问答任务，限制了同时优化两种能力，而复杂搜索系统常用冻结大模型保证问答质量，因此需用小的可训练模型专注搜索规划。

Method: 提出AI - SearchPlanner框架，包含解耦搜索规划器和生成器架构、搜索规划的双奖励对齐、规划效用和成本的帕累托优化三个创新点。

Result: 在真实数据集上的大量实验显示，AI - SearchPlanner在效果和效率上优于现有基于强化学习的搜索代理，且在不同冻结问答模型和数据领域有强泛化能力。

Conclusion: AI - SearchPlanner是一种有效且高效的方法，可提升冻结问答模型的性能。

Abstract: Recent studies have explored integrating Large Language Models (LLMs) with
search engines to leverage both the LLMs' internal pre-trained knowledge and
external information. Specially, reinforcement learning (RL) has emerged as a
promising paradigm for enhancing LLM reasoning through multi-turn interactions
with search engines. However, existing RL-based search agents rely on a single
LLM to handle both search planning and question-answering (QA) tasks in an
end-to-end manner, which limits their ability to optimize both capabilities
simultaneously. In practice, sophisticated AI search systems often employ a
large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a
more effective and efficient approach is to utilize a small, trainable LLM
dedicated to search planning. In this paper, we propose
\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to
enhance the performance of frozen QA models by focusing on search planning.
Specifically, our approach introduces three key innovations: 1) Decoupling the
Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for
Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to
achieve the objectives. Extensive experiments on real-world datasets
demonstrate that AI SearchPlanner outperforms existing RL-based search agents
in both effectiveness and efficiency, while exhibiting strong generalization
capabilities across diverse frozen QA models and data domains.

</details>


### [10] [P2C: Path to Counterfactuals](https://arxiv.org/abs/2508.20371)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: 介绍机器学习模型透明度与反事实解释需求，指出当前反事实方法局限，提出P2C框架解决问题并展示优势。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在高风险场景中透明度与反事实解释平衡问题，以及现有反事实方法不可实现的局限。

Method: 提出P2C框架，明确建模特征间因果关系，确保计划中间状态可行且因果有效，用s(CASP)生成计划，改进成本计算。

Result: P2C能将不利结果转换为因果一致的有利结果，其因果规划器优于缺乏因果知识的标准规划器。

Conclusion: P2C框架有效解决现有反事实方法局限，实现现实可行的反事实解释。

Abstract: Machine-learning models are increasingly driving decisions in high-stakes
settings, such as finance, law, and hiring, thus, highlighting the need for
transparency. However, the key challenge is to balance transparency --
clarifying `why' a decision was made -- with recourse: providing actionable
steps on `how' to achieve a favourable outcome from an unfavourable outcome.
Counterfactual explanations reveal `why' an undesired outcome occurred and
`how' to reverse it through targeted feature changes (interventions).
  Current counterfactual approaches have limitations: 1) they often ignore
causal dependencies between features, and 2) they typically assume all
interventions can happen simultaneously, an unrealistic assumption in practical
scenarios where actions are typically taken in a sequence. As a result, these
counterfactuals are often not achievable in the real world.
  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that
produces a plan (ordered sequence of actions) converting an unfavourable
outcome to a causally consistent favourable outcome. P2C addresses both
limitations by 1) Explicitly modelling causal relationships between features
and 2) Ensuring that each intermediate state in the plan is feasible and
causally valid. P2C uses the goal-directed Answer Set Programming system
s(CASP) to generate the plan accounting for feature changes that happen
automatically due to causal dependencies. Furthermore, P2C refines cost
(effort) computation by only counting changes actively made by the user,
resulting in realistic cost estimates. Finally, P2C highlights how its causal
planner outperforms standard planners, which lack causal knowledge and thus can
generate illegal actions.

</details>


### [11] [TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning](https://arxiv.org/abs/2508.20374)
*Simin Ma,Shujian Liu,Jun Tan,Yebowen Hu,Song Wang,Sathish Reddy Indurthi,Sanqiang Zhao,Liwei Wu,Jianbing Han,Kaiqiang Song*

Main category: cs.AI

TL;DR: 现有指令数据生成方法忽视任务相关性，提出TCIA框架，实验表明其能提升大模型在特定任务应用的性能且不影响通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有利用大模型自动生成多样化指令的方法忽视了现实应用中的任务相关性，而多数应用需特定任务知识，因此要开发兼顾多样性和特定场景优化的指令增强方法。

Method: 引入Task Centric Instruction Augmentation (TCIA)框架，在离散查询 - 约束空间表示指令，系统扩展指令并保持多样性和任务一致性。

Result: TCIA使开源大模型在四个现实特定任务应用中平均性能提升8.7%，部分情况超越领先闭源模型，且不影响通用指令遵循能力。

Conclusion: TCIA是一种可扩展且高效的解决方案，能让大模型适配现实中以任务为中心的应用。

Abstract: Diverse instruction data is vital for effective instruction tuning of large
language models, as it enables the model to generalize across different types
of inputs . Building such diversified instruction dataset is an essential step
in this process. Existing approaches often leverage large language models to
automatically explore and generate diverse instructions, ensuring both data
diversity and quality. However, they tend to overlook an important factor in
real-world applications: on-task relevance. In practice, only a few real-world
applications require a truly general-purpose model; most benefit from
task-specific knowledge tailored to their particular use case. Therefore, it is
vital to develop instruction augmentation methods that not only maintain
diversity but are also optimized for specific, real-world scenarios.
  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework
that systematically expands instructions while preserving both diversity and
task alignment. By representing instructions in a discrete query-constraints
space, TCIA creates a rich set of task-relevant instructions and enables models
to generalize to these task-specific instructions without sacrificing overall
performance. Experiments show that TCIA improves open-source LLMs' performance
by an average of 8.7% across four real-world, task-specific applications, and
in some cases outperforming leading closed-source models. These improvements do
not compromise general instruction-following ability, making TCIA a scalable
and efficient solution for adapting LLMs to real-world, task-focused
applications.

</details>


### [12] [Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM](https://arxiv.org/abs/2508.20384)
*Yongfu Zhu,Lin Sun,Guangxiang Zhao,Weihong Lin,Xiangzheng Zhang*

Main category: cs.AI

TL;DR: 提出Entropy Area Score (EAS) 指标量化推理大语言模型答案生成过程的不确定性，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 量化推理大语言模型答案生成过程中的不确定性。

Method: 提出EAS指标，整合模型自身的词元级预测熵来捕捉生成过程中不确定性的演变。

Result: EAS与跨模型和数据集的答案熵高度相关，在训练数据选择中表现优于Pass Rate过滤，提高学生模型在数学基准测试上的准确性。

Conclusion: EAS高效且可解释，为大语言模型训练中的不确定性建模和数据质量评估提供实用工具。

Abstract: In this work, we introduce Entropy Area Score (EAS), a simple yet effective
metric to quantify uncertainty in the answer generation process of reasoning
large language models (LLMs). EAS requires neither external models nor repeated
sampling, it integrates token-level predictive entropy from the model itself to
capture the evolution of uncertainty during generation. Empirical results show
that EAS is strongly correlated with answer entropy across models and datasets.
In training data selection, EAS identifies high-potential samples and
consistently outperforms Pass Rate filtering under equal sample budgets,
improving student model accuracy on math benchmarks. EAS is both efficient and
interpretable, offering a practical tool for uncertainty modeling and data
quality assessment in LLM training.

</details>


### [13] [AWorld: Orchestrating the Training Recipe for Agentic AI](https://arxiv.org/abs/2508.20404)
*Chengyue Yu,Siyuan Lu,Chenyi Zhuang,Dong Wang,Qintong Wu,Zongyue Li,Runsheng Gan,Chunfeng Wang,Siqi Hou,Gaochi Huang,Wenlong Yan,Lifeng Hong,Aohui Xue,Yanfeng Wang,Jinjie Gu,David Tsai,Tao Lin*

Main category: cs.AI

TL;DR: 提出开源系统AWorld加速经验收集，训练的Qwen3 - 32B基代理提升GAIA准确率。


<details>
  <summary>Details</summary>
Motivation: 学习实践范式受低效经验生成阻碍，在复杂基准GAIA中问题突出。

Method: 引入开源系统AWorld，通过集群分布式任务加速经验收集。

Result: AWorld使经验收集加速14.6倍，训练的代理将GAIA整体准确率从21.59%提升到32.23%，在最难级别得分超领先专有模型。

Conclusion: 开源系统和代理为完整的主动AI训练管道提供实用蓝图。

Abstract: The learning from practice paradigm is crucial for developing capable Agentic
AI systems, yet it is severely hampered by inefficient experience generation, a
bottleneck especially pronounced in complex benchmarks like GAIA. To address
this, we introduce AWorld, an open-source system engineered for large-scale
agent-environment interaction. By distributing tasks across a cluster, AWorld
accelerates experience collection by 14.6x compared to standard single-node,
sequential execution. This critical speedup makes extensive reinforcement
learning practical and scalable. Leveraging this capability, we trained a
Qwen3-32B-based agent that significantly outperforms its base model, increasing
its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most
challenging levels, our agent achieves a score of 16.33%, surpassing the
performance of leading proprietary models. Our open-source system and resulting
agent provide a practical blueprint for a complete agentic AI training
pipeline, from efficient interaction to demonstrable model improvement.

</details>


### [14] [Governable AI: Provable Safety Under Extreme Threat Models](https://arxiv.org/abs/2508.20411)
*Donglin Wang,Weiyun Liang,Chunyuan Chen,Jing Xu,Yulong Fu*

Main category: cs.AI

TL;DR: 随着AI发展，现有安全方法有局限，提出可治理AI框架GAI，介绍其组成并证明安全性和有效性。


<details>
  <summary>Details</summary>
Motivation: AI发展带来严重安全风险，现有AI安全方法有原则性局限，无法保证安全。

Method: 提出基于密码机制的GAI框架，包括规则执行模块、治理规则和可治理安全超级平台，进行形式化安全证明并通过原型实现评估。

Result: 通过原型在高风险场景评估，证明GAI框架的有效性。

Conclusion: GAI框架从传统内部约束转向外部结构合规，为AI安全治理提供可行且通用技术途径。

Abstract: As AI rapidly advances, the security risks posed by AI are becoming
increasingly severe, especially in critical scenarios, including those posing
existential risks. If AI becomes uncontrollable, manipulated, or actively
evades safety mechanisms, it could trigger systemic disasters. Existing AI
safety approaches-such as model enhancement, value alignment, and human
intervention-suffer from fundamental, in-principle limitations when facing AI
with extreme motivations and unlimited intelligence, and cannot guarantee
security. To address this challenge, we propose a Governable AI (GAI) framework
that shifts from traditional internal constraints to externally enforced
structural compliance based on cryptographic mechanisms that are
computationally infeasible to break, even for future AI, under the defined
threat model and well-established cryptographic assumptions.The GAI framework
is composed of a simple yet reliable, fully deterministic, powerful, flexible,
and general-purpose rule enforcement module (REM); governance rules; and a
governable secure super-platform (GSSP) that offers end-to-end protection
against compromise or subversion by AI. The decoupling of the governance rules
and the technical platform further enables a feasible and generalizable
technical pathway for the safety governance of AI. REM enforces the bottom line
defined by governance rules, while GSSP ensures non-bypassability,
tamper-resistance, and unforgeability to eliminate all identified attack
vectors. This paper also presents a rigorous formal proof of the security
properties of this mechanism and demonstrates its effectiveness through a
prototype implementation evaluated in representative high-stakes scenarios.

</details>


### [15] [Enhancing Health Fact-Checking with LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.20525)
*Jingze Zhang,Jiahe Qian,Yiliang Zhou,Yifan Peng*

Main category: cs.AI

TL;DR: 提出用大语言模型生成合成数据增强健康相关事实核查训练数据，在两数据集上提升了F1分数。


<details>
  <summary>Details</summary>
Motivation: 健康相关内容事实核查的标注训练数据有限。

Method: 提出合成数据生成管道，利用大语言模型，总结源文档、分解为原子事实、构建句子 - 事实蕴含表，生成合成文本 - 声明对，结合原始数据微调基于BERT的事实核查模型。

Result: 在PubHealth和SciFact数据集上，相比仅用原始数据训练的模型，F1分数分别最多提高0.019和0.049。

Conclusion: 大语言模型驱动的合成数据增强对提升健康相关事实核查器性能有效。

Abstract: Fact-checking for health-related content is challenging due to the limited
availability of annotated training data. In this study, we propose a synthetic
data generation pipeline that leverages large language models (LLMs) to augment
training data for health-related fact checking. In this pipeline, we summarize
source documents, decompose the summaries into atomic facts, and use an LLM to
construct sentence-fact entailment tables. From the entailment relations in the
table, we further generate synthetic text-claim pairs with binary veracity
labels. These synthetic data are then combined with the original data to
fine-tune a BERT-based fact-checking model. Evaluation on two public datasets,
PubHealth and SciFact, shows that our pipeline improved F1 scores by up to
0.019 and 0.049, respectively, compared to models trained only on the original
data. These results highlight the effectiveness of LLM-driven synthetic data
augmentation in enhancing the performance of health-related fact-checkers.

</details>


### [16] [Human-AI Collaborative Bot Detection in MMORPGs](https://arxiv.org/abs/2508.20578)
*Jaeman Son,Hyunsoo Kim*

Main category: cs.AI

TL;DR: 提出一种无监督检测MMORPG自动升级机器人的框架，结合对比表征学习、聚类技术，用LLM辅助验证，还有可视化方法，提升检测效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: MMORPG中自动升级机器人破坏游戏平衡和公平，检测困难且需可解释理由，避免法律和用户体验问题。

Method: 采用对比表征学习和聚类技术无监督识别升级模式相似的角色组，引入LLM辅助验证聚类组，使用基于增长曲线的可视化辅助评估。

Result: 未明确提及具体结果，但表明该协作方法提升了机器人检测工作流程的效率。

Conclusion: 该协作方法能提高检测效率并保持可解释性，支持MMORPG中可扩展和负责任的机器人监管。

Abstract: In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling
bots exploit automated programs to level up characters at scale, undermining
gameplay balance and fairness. Detecting such bots is challenging, not only
because they mimic human behavior, but also because punitive actions require
explainable justification to avoid legal and user experience issues. In this
paper, we present a novel framework for detecting auto-leveling bots by
leveraging contrastive representation learning and clustering techniques in a
fully unsupervised manner to identify groups of characters with similar
level-up patterns. To ensure reliable decisions, we incorporate a Large
Language Model (LLM) as an auxiliary reviewer to validate the clustered groups,
effectively mimicking a secondary human judgment. We also introduce a growth
curve-based visualization to assist both the LLM and human moderators in
assessing leveling behavior. This collaborative approach improves the
efficiency of bot detection workflows while maintaining explainability, thereby
supporting scalable and accountable bot regulation in MMORPGs.

</details>


### [17] [Bridging Minds and Machines: Toward an Integration of AI and Cognitive Science](https://arxiv.org/abs/2508.20674)
*Rui Mao,Qian Liu,Xiao Li,Erik Cambria,Amir Hussain*

Main category: cs.AI

TL;DR: 本文探讨AI与认知科学的相互关系，指出AI认知基础碎片化问题，并给出未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 认知科学与AI相互影响，有必要全面回顾两者交叉点。

Method: 综合双方关键贡献进行分析。

Result: 发现AI发展侧重实际任务表现，认知基础概念碎片化。

Conclusion: AI在认知科学中的未来不仅要提升性能，还应构建增进对人类思维理解的系统，给出了几个有前景的发展方向。

Abstract: Cognitive Science has profoundly shaped disciplines such as Artificial
Intelligence (AI), Philosophy, Psychology, Neuroscience, Linguistics, and
Culture. Many breakthroughs in AI trace their roots to cognitive theories,
while AI itself has become an indispensable tool for advancing cognitive
research. This reciprocal relationship motivates a comprehensive review of the
intersections between AI and Cognitive Science. By synthesizing key
contributions from both perspectives, we observe that AI progress has largely
emphasized practical task performance, whereas its cognitive foundations remain
conceptually fragmented. We argue that the future of AI within Cognitive
Science lies not only in improving performance but also in constructing systems
that deepen our understanding of the human mind. Promising directions include
aligning AI behaviors with cognitive frameworks, situating AI in embodiment and
culture, developing personalized cognitive models, and rethinking AI ethics
through cognitive co-evaluation.

</details>


### [18] [Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings](https://arxiv.org/abs/2508.20701)
*Ares Fabregat-Hernández,Javier Palanca,Vicent Botti*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The paper introduces a novel framework based on category theory to enhance
the explainability of artificial intelligence systems, particularly focusing on
word embeddings. Key topics include the construction of categories
$\mathcal{L}_T$ and $\mathcal{P}_T$, providing schematic representations of the
semantics of a text $ T $, and reframing the selection of the element with
maximum probability as a categorical notion. Additionally, the monoidal
category $\mathcal{P}_T$ is constructed to visualize various methods of
extracting semantic information from $T$, offering a dimension-agnostic
definition of semantic spaces reliant solely on information within the text.
  Furthermore, the paper defines the categories of configurations Conf and word
embeddings $\mathcal{Emb}$, accompanied by the concept of divergence as a
decoration on $\mathcal{Emb}$. It establishes a mathematically precise method
for comparing word embeddings, demonstrating the equivalence between the GloVe
and Word2Vec algorithms and the metric MDS algorithm, transitioning from neural
network algorithms (black box) to a transparent framework. Finally, the paper
presents a mathematical approach to computing biases before embedding and
offers insights on mitigating biases at the semantic space level, advancing the
field of explainable artificial intelligence.

</details>


### [19] [Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision](https://arxiv.org/abs/2508.20729)
*Ao Cheng,Lei Zhang,Guowei He*

Main category: cs.AI

TL;DR: 本文构建用于科学计算的新型代理框架，通过三个推理大语言模型协作，经评估该框架显著提升无错误代码生成率，确立自动代码生成与审查为有前景的科学计算范式。


<details>
  <summary>Details</summary>
Motivation: 构建可解决科学计算中代表性问题的代理框架。

Method: 构建含“重写 - 解决 - 审查 - 修订”逻辑链的代理框架，由三个推理大语言模型（顾问、审查员和程序员）协作完成。

Result: 与单模型相比，协作框架显著提高无错误代码生成率，减少非物理解的出现，审查机制提升最新推理模型平均执行成功率。

Conclusion: 代理框架确立自动代码生成和审查为有前景的科学计算范式。

Abstract: Large language models (LLMs) serve as an active and promising field of
generative artificial intelligence and have demonstrated abilities to perform
complex tasks in multiple domains, including mathematical and scientific
reasoning. In this work, we construct a novel agent framework for solving
representative problems in scientific computing. The proposed agent,
incorporating a "rewriting-resolution-review-revision" logical chain via three
reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,
respectively), is integrated in a collaborative and interactive manner. The
Consultant module endows the agent with knowledge transfer capabilities to link
problems to professional domain insights, thereby rewriting problem
descriptions through text augmentation. The Programmer module is responsible
for generating and executing well-structured code to deliver the problem
resolution. The Reviewer module equips the agent with the capacity for
self-debugging and self-refinement through interactive feedback with code
runtime outputs. By leveraging the end-to-end review mechanism, the executable
code provided by the Programmer attains the iterative revision. A comprehensive
evaluation is conducted on the performance of the proposed agent framework in
solving PDEs, ill-conditioned linear systems, and data-driven physical analysis
problems. Compared to single-model, this collaborative framework significantly
improves the bug-free code generation rate and reduces the occurrence of
non-physical solutions, thereby establishing a highly reliable framework for
autonomous code generation based on natural language descriptions. The review
mechanism improved the average execution success (bug-free code and non-NaN
solutions) rate of the latest reasoning models. In summary, our agent framework
establishes automatic code generation and review as a promising scientific
computing paradigm.

</details>


### [20] [Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control](https://arxiv.org/abs/2508.20784)
*Yifan Zhang*

Main category: cs.AI

TL;DR: 提出单智能体强化学习框架用于公交控车，在近真实模拟中避免多智能体问题，实验表明性能优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习在公交控车中忽视现实运营特征，存在数据不平衡和收敛问题。

Method: 构建双向时刻表网络和动态乘客需求模型，将多智能体问题转化为单智能体问题，设计结构化奖励函数。

Result: 改进的软演员 - 评论家算法比基准方法表现更稳定、更优。

Conclusion: 增强分类结构和考虑时刻表奖励的单智能体深度强化学习能有效管理公交控车，是多智能体框架的可靠可扩展替代方案。

Abstract: Bus bunching remains a challenge for urban transit due to stochastic traffic
and passenger demand. Traditional solutions rely on multi-agent reinforcement
learning (MARL) in loop-line settings, which overlook realistic operations
characterized by heterogeneous routes, timetables, fluctuating demand, and
varying fleet sizes. We propose a novel single-agent reinforcement learning
(RL) framework for bus holding control that avoids the data imbalance and
convergence issues of MARL under near-realistic simulation. A bidirectional
timetabled network with dynamic passenger demand is constructed. The key
innovation is reformulating the multi-agent problem into a single-agent one by
augmenting the state space with categorical identifiers (vehicle ID, station
ID, time period) in addition to numerical features (headway, occupancy,
velocity). This high-dimensional encoding enables single-agent policies to
capture inter-agent dependencies, analogous to projecting non-separable inputs
into a higher-dimensional space. We further design a structured reward function
aligned with operational goals: instead of exponential penalties on headway
deviations, a ridge-shaped reward balances uniform headways and schedule
adherence. Experiments show that our modified soft actor-critic (SAC) achieves
more stable and superior performance than benchmarks, including MADDPG (e.g.,
-430k vs. -530k under stochastic conditions). These results demonstrate that
single-agent deep RL, when enhanced with categorical structuring and
schedule-aware rewards, can effectively manage bus holding in non-loop,
real-world contexts. This paradigm offers a robust, scalable alternative to
MARL frameworks, particularly where agent-specific experiences are imbalanced.

</details>


### [21] [A Graph-Based Test-Harness for LLM Evaluation](https://arxiv.org/abs/2508.20810)
*Jessica Lundin,Guillaume Chabot-Couture*

Main category: cs.AI

TL;DR: 本文提出医学指南动态系统基准测试原型，用图方法生成问题评估大模型，还可用于模型后训练，解决手动基准覆盖局限。


<details>
  <summary>Details</summary>
Motivation: 创建可动态生成、全面且抗污染的医学指南基准测试，识别大模型在医学领域特定能力差距。

Method: 将WHO IMCI手册转化为有向图，用图遍历生成含年龄场景和干扰项的问题进行评估，还用于模型后训练。

Result: 基于图方法能跨临床任务系统评估（准确率45 - 67%），发现模型症状识别强，在分诊、治疗和随访方面弱。

Conclusion: 该方法解决手动基准覆盖局限，是创建可扩展基准的一步，代码和数据集开源。

Abstract: We present a first known prototype of a dynamic, systematic benchmark of
medical guidelines for 400+ questions, with 3.3+ trillion possible
combinations, covering 100\% of guideline relationships. We transformed the WHO
IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,
treatments, follow-ups, severities) and 300+ edges, then used graph traversal
to generate questions that incorporated age-specific scenarios and contextual
distractors to ensure clinical relevance. Our graph-based approach enables
systematic evaluation across clinical tasks (45-67\% accuracy), and we find
models excel at symptom recognition but struggle with triaging severity,
treatment protocols and follow-up care, demonstrating how customized benchmarks
can identify specific capability gaps that general-domain evaluations miss.
Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training
(supervised finetuning, GRPO, DPO), where correct answers provide high-reward
samples without expensive human annotation. The graph-based approach
successfully addresses the coverage limitations of manually curated benchmarks.
This methodology is a step toward scalable, contamination-resistant solution
for creating comprehensive benchmarks that can be dynamically generated,
including when the guidelines are updated. Code and datasets are available at
https://github.com/jessicalundin/graph_testing_harness

</details>


### [22] [A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling](https://arxiv.org/abs/2508.20953)
*Vipul Patel,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.AI

TL;DR: 本文提出多目标遗传算法解决医疗行业劳动力调度问题，在数据集上验证算法有效，能提供决策支持。


<details>
  <summary>Details</summary>
Motivation: 医疗行业劳动力调度面临患者负荷波动、临床技能多样、需控制成本与保障护理质量等多目标挑战，需平衡相互竞争的目标。

Method: 提出多目标遗传算法（MOO - GA），将医院单元劳动力调度问题建模为多目标优化任务，纳入现实复杂性，定义成本、患者护理覆盖和员工满意度目标函数来寻找高质量非支配解。

Result: 在典型医院单元数据集上，算法生成的排班比传统手动排班性能平均提高66%，能生成稳健且平衡的排班表。

Conclusion: 该方法能有效管理关键运营和以员工为中心的目标之间的权衡，为护士经理和医院管理人员提供实用的决策支持工具。

Abstract: Workforce scheduling in the healthcare sector is a significant operational
challenge, characterized by fluctuating patient loads, diverse clinical skills,
and the critical need to control labor costs while upholding high standards of
patient care. This problem is inherently multi-objective, demanding a delicate
balance between competing goals: minimizing payroll, ensuring adequate staffing
for patient needs, and accommodating staff preferences to mitigate burnout. We
propose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital
unit workforce scheduling problem as a multi-objective optimization task. Our
model incorporates real-world complexities, including hourly appointment-driven
demand and the use of modular shifts for a multi-skilled workforce. By defining
objective functions for cost, patient care coverage, and staff satisfaction,
the GA navigates the vast search space to identify a set of high-quality,
non-dominated solutions. Demonstrated on datasets representing a typical
hospital unit, the results show that our MOO-GA generates robust and balanced
schedules. On average, the schedules produced by our algorithm showed a 66\%
performance improvement over a baseline that simulates a conventional, manual
scheduling process. This approach effectively manages trade-offs between
critical operational and staff-centric objectives, providing a practical
decision support tool for nurse managers and hospital administrators.

</details>


### [23] [Efficient Neuro-Symbolic Learning of Constraints and Objective](https://arxiv.org/abs/2508.20978)
*Marianne Defresne,Romain Gambardella,Sophie Barbe,Thomas Schiex*

Main category: cs.AI

TL;DR: 介绍可微神经符号架构和损失函数解决NP - 难推理问题，训练高效。


<details>
  <summary>Details</summary>
Motivation: 在离散推理与神经网络结合中，解决大语言模型处理离散推理或优化问题的困难，学习解决NP - 难推理问题。

Method: 采用新的概率损失学习约束和目标，将组合求解器移出训练循环，实现可扩展训练和精确推理。

Result: 能有效从自然输入学习解决NP - 难推理问题，在数独、视觉Min - Cut/Max - cut任务和蛋白质设计问题上表现良好。

Conclusion: 该架构和损失函数可有效学习解决NP - 难推理问题，训练效率高。

Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets,
there is an increasing interest in neural architectures that can learn how to
solve discrete reasoning or optimization problems from natural inputs, a task
that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a
loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints
and the objective, thus delivering a complete model that can be scrutinized and
completed with side constraints. By pushing the combinatorial solver out of the
training loop, our architecture also offers scalable training while exact
inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve
NP-hard reasoning problems from natural inputs. On three variants of the Sudoku
benchmark -- symbolic, visual, and many-solution --, our approach requires a
fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut
task, it optimizes the regret better than a Decision-Focused-Learning
regret-dedicated loss. Finally, it efficiently learns the energy optimization
formulation of the large real-world problem of designing proteins.

</details>


### [24] [ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery](https://arxiv.org/abs/2508.20996)
*Junda Wang,Zonghai Yao,Zhichao Yang,Lingxi Li,Junhui Qian,Hong Yu*

Main category: cs.AI

TL;DR: 本文提出多智能体对话框架ChatThero用于成瘾康复，通过两阶段训练，在评估中表现良好，为研究和临床转化提供基础。


<details>
  <summary>Details</summary>
Motivation: 物质使用障碍患者众多但有效治疗少，现有大语言模型缺乏与临床策略紧密结合，影响成瘾康复效果。

Method: 提出ChatThero框架，结合动态患者建模、上下文敏感治疗对话和自适应说服策略；构建合成基准，采用监督微调（SFT）和直接偏好优化（DPO）两阶段训练。

Result: ChatThero使患者动机平均提高41.5%，治疗信心提高0.49%，解决难题比GPT - 4o少用26%的回合数，自动化和人工临床评估显示其在同理心、响应性和行为真实性方面评分更高。

Conclusion: 该框架支持对治疗对话进行严格、保护隐私的研究，为研究和临床转化提供可靠、可复制的基础。

Abstract: Substance use disorders (SUDs) affect over 36 million people worldwide, yet
few receive effective care due to stigma, motivational barriers, and limited
personalized support. Although large language models (LLMs) show promise for
mental-health assistance, most systems lack tight integration with clinically
validated strategies, reducing effectiveness in addiction recovery. We present
ChatThero, a multi-agent conversational framework that couples dynamic patient
modeling with context-sensitive therapeutic dialogue and adaptive persuasive
strategies grounded in cognitive behavioral therapy (CBT) and motivational
interviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,
Medium, and Hard resistance levels, and train ChatThero with a two-stage
pipeline comprising supervised fine-tuning (SFT) followed by direct preference
optimization (DPO). In evaluation, ChatThero yields a 41.5\% average gain in
patient motivation, a 0.49\% increase in treatment confidence, and resolves
hard cases with 26\% fewer turns than GPT-4o, and both automated and human
clinical assessments rate it higher in empathy, responsiveness, and behavioral
realism. The framework supports rigorous, privacy-preserving study of
therapeutic conversation and provides a robust, replicable basis for research
and clinical translation.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [25] [Mass conservation analysis of extrusion-based 3D printing simulations based on the level-set method](https://arxiv.org/abs/2508.20617)
*Carlos J. G. Rojas,C. A. Gómez-Pérez,Leyla Özkan*

Main category: cs.CE

TL;DR: 研究基于挤出的3D打印中保守水平集方法的质量守恒特性，分析参数影响，获良好结果。


<details>
  <summary>Details</summary>
Motivation: 基于挤出的打印数值模拟追踪材料边界有挑战，质量守恒不准确会导致模拟与实际不符，需研究质量守恒特性。

Method: 分析水平集参数对质量守恒精度的影响，对比模拟与理想的横截面面积。

Result: 减小重新初始化和界面厚度参数可减少误差，但会增加计算成本；选择合适界面厚度可降低网格要求；模拟的横截面面积与前人数据吻合。

Conclusion: 保守水平集方法在基于挤出的3D打印应用中对质量守恒特性研究有一定效果。

Abstract: Numerical simulations of extrusion-based printing require tracking evolving
material bound- aries, a challenging task due to possible topological changes
and mass conservation issues. Inaccurate conservation of mass can lead to a
mismatch between the extruded and simulated shapes, and generally to unreliable
predictions of the actual ink behavior. This work investigates the mass
conservation properties of the conservative level-set method in extrusion-based
3D printing applications. We analyze the effects of the level set parameters on
the accuracy of mass conservation using the cross-sectional area of the
deposited strand. We compare the cross- sectional areas obtained in the
simulation with the ideal areas obtained from a mass balance when the system
reaches a steady-state condition. The numerical results indicate that reducing
the reinitialization and the interface thickness parameters decreases the
errors in the cross-sectional area obtained. However, the reductions in error
tend to decline and could lead to excessive computational cost. Furthermore, we
also found that the typical strong mesh requirements can be lessened by
selecting an adequate interface thickness. Finally, we obtained the
cross-sectional areas from simulations with different printing settings and
found that they show good agreement with the simulated and experimental data
published in previous work.

</details>


### [26] [Can News Predict the Direction of Oil Price Volatility? A Language Model Approach with SHAP Explanations](https://arxiv.org/abs/2508.20707)
*Romina Hashami,Felipe Maldonado*

Main category: cs.CE

TL;DR: 研究用新闻数据预测油价波动方向，开发集成学习框架，发现新闻数量是稳健预测指标，FastText最有效，不同市场阶段预测驱动因素不同。


<details>
  <summary>Details</summary>
Motivation: 金融市场对新闻等敏感，原油在商品市场和全球经济中重要，想探究仅用新闻能否有效预测油价波动方向。

Method: 用2014 - 2024年Eikon数据集，开发集成学习框架，利用多种情感分析技术和语言模型，与HAR模型对比，用McNemar检验评估显著性，用SHAP进行词级解释。

Result: 多数基于情感的指标未持续超越HAR，新闻数量是稳健预测指标，FastText在预测方向变动上最有效，不同市场阶段预测驱动因素不同。

Conclusion: 强调了新闻驱动特征的预测能力和可解释NLP在金融预测中的价值。

Abstract: Financial markets can be highly sensitive to news, investor sentiment, and
economic indicators, leading to important asset price fluctuations. In this
study we focus on crude oil, due to its crucial role in commodity markets and
the global economy. Specifically, we are interested in understanding the
directional changes of oil price volatility, and for this purpose we
investigate whether news alone -- without incorporating traditional market data
-- can effectively predict the direction of oil price movements. Using a
decade-long dataset from Eikon (2014-2024), we develop an ensemble learning
framework to extract predictive signals from financial news. Our approach
leverages diverse sentiment analysis techniques and modern language models,
including FastText, FinBERT, Gemini, and LLaMA, to capture market sentiment and
textual patterns. We benchmark our model against the Heterogeneous
Autoregressive (HAR) model and assess statistical significance using the
McNemar test. While most sentiment-based indicators do not consistently
outperform HAR, the raw news count emerges as a robust predictor. Among
embedding techniques, FastText proves most effective for forecasting
directional movements. Furthermore, SHAP-based interpretation at the word level
reveals evolving predictive drivers across market regimes: pre-pandemic
emphasis on supply-demand and economic terms; early pandemic focus on
uncertainty and macroeconomic instability; post-shock attention to long-term
recovery indicators; and war-period sensitivity to geopolitical and regional
oil market disruptions. These findings highlight the predictive power of
news-driven features and the value of explainable NLP in financial forecasting.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [27] [Efficient Forkless Blockchain Databases](https://arxiv.org/abs/2508.20686)
*Herbert Jordan,Kamil Jezek,Pavle Subotic,Bernhard Scholz*

Main category: cs.DB

TL;DR: 提出无分叉区块链数据库，相比基于geth的Fantom区块链客户端，存储提升100倍，吞吐量提升10倍。


<details>
  <summary>Details</summary>
Motivation: L1区块链节点运营成本高，其中区块链数据库是资源密集型组件，且无分叉区块链仍依赖次优的分叉数据库。

Method: 提出无分叉区块链数据库。

Result: 相比基于geth的Fantom区块链客户端，存储提升100倍，吞吐量提升10倍。

Conclusion: 所提出的无分叉区块链数据库具有显著性能提升。

Abstract: Operating nodes in an L1 blockchain remains costly despite recent advances in
blockchain technology. One of the most resource-intensive components of a node
is the blockchain database, also known as StateDB, that manages balances,
nonce, code, and the persistent storage of accounts/smart contracts. Although
the blockchain industry has transitioned from forking to forkless chains due to
improved consensus protocols, forkless blockchains still rely on legacy forking
databases that are suboptimal for their purposes. In this paper, we propose a
forkless blockchain database, showing a 100x improvement in storage and a 10x
improvement in throughput compared to the geth-based Fantom Blockchain client.

</details>


### [28] [Research Challenges in Relational Database Management Systems for LLM Queries](https://arxiv.org/abs/2508.20912)
*Kerem Akillioglu,Anurag Chakraborty,Sairaj Voruganti,M. Tamer Özsu*

Main category: cs.DB

TL;DR: 探讨SQL调用大语言模型集成的现状，指出问题并提出初步解决方案，证明LLM与DBMS更紧密集成的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前开源的SQL调用大语言模型集成方案功能有限、性能不佳，需探索其局限。

Method: 对两个开源系统和一个企业平台进行探索，用五个代表性查询暴露功能、性能和可扩展性限制。

Result: 识别出三个主要问题，实现初步解决方案，在处理LLM驱动的SQL查询上有改进。

Conclusion: LLM与DBMS更紧密集成是可扩展且高效处理LLM查询的关键。

Abstract: Large language models (LLMs) have become essential for applications such as
text summarization, sentiment analysis, and automated question-answering.
Recently, LLMs have also been integrated into relational database management
systems to enhance querying and support advanced data processing. Companies
such as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly
within SQL, denoted as LLM queries, to boost data insights. However,
open-source solutions currently have limited functionality and poor
performance. In this work, we present an early exploration of two open-source
systems and one enterprise platform, using five representative queries to
expose functional, performance, and scalability limits in today's SQL-invoked
LLM integrations. We identify three main issues: enforcing structured outputs,
optimizing resource utilization, and improving query planning. We implemented
initial solutions and observed improvements in accommodating LLM powered SQL
queries. These early gains demonstrate that tighter integration of LLM+DBMS is
the key to scalable and efficient processing of LLM queries.

</details>


### [29] [Graph-Based Feature Augmentation for Predictive Tasks on Relational Datasets](https://arxiv.org/abs/2508.20986)
*Lianpeng Qiao,Ziqi Cao,Kaiyu Feng,Ye Yuan,Guoren Wang*

Main category: cs.DB

TL;DR: 提出端到端自动特征增强框架ReCoGNN，在多个数据集实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数据是各领域创新基础，预测建模中需减少手动工作，探索特征增强自动化及利用关系信号。

Method: 提出ReCoGNN框架，先建模表内属性关系划分表，再构建异质加权图，最后用消息传递图神经网络传播信息进行特征选择和数据增强。

Result: 在十个真实和合成数据集上实验，ReCoGNN在分类和回归任务中始终优于现有方法。

Conclusion: ReCoGNN是有效的端到端自动特征增强框架，能提升预测建模效果。

Abstract: Data has become a foundational asset driving innovation across domains such
as finance, healthcare, and e-commerce. In these areas, predictive modeling
over relational tables is commonly employed, with increasing emphasis on
reducing manual effort through automated machine learning (AutoML) techniques.
This raises an interesting question: can feature augmentation itself be
automated and identify and utilize task-related relational signals?
  To address this challenge, we propose an end-to-end automated feature
augmentation framework, ReCoGNN, which enhances initial datasets using features
extracted from multiple relational tables to support predictive tasks. ReCoGNN
first captures semantic dependencies within each table by modeling intra-table
attribute relationships, enabling it to partition tables into structured,
semantically coherent segments. It then constructs a heterogeneous weighted
graph that represents inter-row relationships across all segments. Finally,
ReCoGNN leverages message-passing graph neural networks to propagate
information through the graph, guiding feature selection and augmenting the
original dataset. Extensive experiments conducted on ten real-life and
synthetic datasets demonstrate that ReCoGNN consistently outperforms existing
methods on both classification and regression tasks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [30] [SpeedMalloc: Improving Multi-threaded Applications via a Lightweight Core for Memory Allocation](https://arxiv.org/abs/2508.20253)
*Ruihao Li,Qinzhe Wu,Krishna Kavi,Gayatri Mehta,Jonathan C. Beard,Neeraja J. Yadwadkar,Lizy K. John*

Main category: cs.DC

TL;DR: 内存分配虽代码占比小但影响大，现有加速器有局限，提出 SpeedMalloc 改进多线程内存分配性能，且对比有速度提升。


<details>
  <summary>Details</summary>
Motivation: 内存分配对程序性能影响大，现有卸载到加速器的方案对多线程应用支持有限，且核心与加速器同步有挑战。

Method: 提出 SpeedMalloc，用轻量级支持核心处理多线程应用内存分配任务，支持核心有高效跨核心数据同步，将分配器元数据存于自身缓存。

Result: SpeedMalloc 在多线程工作负载上比 Jemalloc、TCMalloc、Mimalloc、Mallacc 和 Memento 分别实现 1.75x、1.18x、1.15x、1.23x 和 1.18x 的加速。

Conclusion: SpeedMalloc 能有效提高多线程应用的内存分配性能。

Abstract: Memory allocation, though constituting only a small portion of the executed
code, can have a "butterfly effect" on overall program performance, leading to
significant and far-reaching impacts. Despite accounting for just approximately
5% of total instructions, memory allocation can result in up to a 2.7x
performance variation depending on the allocator used. This effect arises from
the complexity of memory allocation in modern multi-threaded multi-core
systems, where allocator metadata becomes intertwined with user data, leading
to cache pollution or increased cross-thread synchronization overhead.
Offloading memory allocators to accelerators, e.g., Mallacc and Memento, is a
potential direction to improve the allocator performance and mitigate cache
pollution. However, these accelerators currently have limited support for
multi-threaded applications, and synchronization between cores and accelerators
remains a significant challenge.
  We present SpeedMalloc, using a lightweight support-core to process memory
allocation tasks in multi-threaded applications. The support-core is a
lightweight programmable processor with efficient cross-core data
synchronization and houses all allocator metadata in its own caches. This
design minimizes cache conflicts with user data and eliminates the need for
cross-core metadata synchronization. In addition, using a general-purpose core
instead of domain-specific accelerators makes SpeedMalloc capable of adopting
new allocator designs. We compare SpeedMalloc with state-of-the-art software
and hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and
Memento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on
multithreaded workloads over these five allocators, respectively.

</details>


### [31] [SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization](https://arxiv.org/abs/2508.20258)
*Arya Tschand,Muhammad Awad,Ryan Swann,Kesavan Ramakrishnan,Jeffrey Ma,Keith Lowery,Ganesh Dasika,Vijay Janapa Reddi*

Main category: cs.DC

TL;DR: 本文提出SwizzlePerf，让大语言模型具备硬件感知能力以自动生成GPU内核空间优化，在多种内核上效果显著，是迈向创建硬件感知大语言模型性能工程代理的第一步。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在GPU内核性能工程中采用低效搜索方法，且缺乏硬件感知这一人类性能工程师依赖的关键特性。

Method: 利用工作负载的特定内存访问模式、架构规格、过滤后的性能分析日志和历史性能反思，让大语言模型具备硬件感知能力，自动生成GPU内核空间优化。

Result: 对于GEMM内核，SwizzlePerf不到5分钟就能生成专家工程师2周才能找到的特定硬件最优模式；在10个不同的ML和科学内核套件中，能为9个内核生成模式，实现最高2.06倍加速和L2命中率70%的提升。

Conclusion: 这是系统创建硬件感知大语言模型性能工程代理的第一步。

Abstract: Large language models (LLMs) have shown progress in GPU kernel performance
engineering using inefficient search-based methods that optimize around
runtime. Any existing approach lacks a key characteristic that human
performance engineers rely on for near-optimal utilization --
hardware-awareness. By leveraging the workload's specific memory access
patterns, architecture specifications, filtered profiling logs, and reflections
on historical performance, we can make software-level optimizations that are
tailored to the underlying hardware. SwizzlePerf automatically generates
spatial optimizations for GPU kernels on disaggregated architectures by giving
LLMs explicit hardware-awareness.
  For a GEMM kernel, SwizzlePerf takes less than 5 minutes to generate the same
hardware-specific optimal swizzling pattern that took expert performance
engineers 2 weeks to find. On a suite of 10 diverse ML and Science kernels,
SwizzlePerf can generate swizzling patterns for 9 of the kernels that achieve
up to a 2.06x speedup and 70% improvement in L2 hit rate. This work is the
first of many steps toward systematically creating hardware-aware LLM
performance engineering agents.

</details>


### [32] [Predictable LLM Serving on GPU Clusters](https://arxiv.org/abs/2508.20274)
*Erfan Darzi,Shreeanant Bharadwaj,Sree Bhargavi Balija*

Main category: cs.DC

TL;DR: 提出主机级控制器减少共享A100集群中延迟敏感推理的干扰，降低SLO错过率和改善p99延迟。


<details>
  <summary>Details</summary>
Motivation: 共享A100集群中延迟敏感推理受PCIe干扰，导致尾延迟增加和SLO违规。

Method: 结合动态MIG重新配置、PCIe感知放置和轻量级护栏，采样租户尾和系统信号，避免PCIe热点并防止抖动。

Result: 在单主机和2节点集群中，SLO错过率降低约32%，p99延迟改善约15%，吞吐量成本≤5%；LLM服务中TTFT p99改善约10 - 15%。

Conclusion: 所提出的控制器能有效减少干扰，提升推理性能，且在LLM服务中无需更改控制器也有效果。

Abstract: Latency-sensitive inference on shared A100 clusters often suffers
noisy-neighbor interference on the PCIe fabric, inflating tail latency and SLO
violations. We present a fabric-agnostic, VM-deployable host-level controller
that combines dynamic Multi-Instance GPU (MIG) reconfiguration, PCIe-aware
placement, and lightweight guardrails (MPS quotas, cgroup I/O). It samples
per-tenant tails and system signals, uses topology hints to avoid PCIe hot
spots, and gates actions with dwell/cool-down to avoid thrash. On a single host
and a 2-node (16-GPU) cluster, SLO miss-rate is reduced by \(\approx\)32\%
(\(\approx\)1.5) and p99 latency improves \(\approx\)15\% with \(\leq\)5\%
throughput cost versus static MIG and naive placement; ablations show MIG and
placement contribute comparably. We also evaluate LLM serving with vLLM on OLMo
2 7B Instruct: TTFT p99 improves \(\approx\)10--15\% at \(\leq\)5\% cost
without changing the controller.

</details>


### [33] [CoFormer: Collaborating with Heterogeneous Edge Devices for Scalable Transformer Inference](https://arxiv.org/abs/2508.20375)
*Guanyu Xu,Zhiwei Hao,Li Shen,Yong Luo,Fuhui Sun,Xiaoyan Wang,Han Hu,Yonggang Wen*

Main category: cs.DC

TL;DR: 提出协作推理系统CoFormer解决边缘设备上transformer模型推理难题，实现加速、降低内存和能耗。


<details>
  <summary>Details</summary>
Motivation: transformer模型在边缘设备上部署面临计算和资源需求高问题，现有策略有通信开销大或精度与效率权衡不佳问题。

Method: 利用transformer可分性和可集成性将大模型分解为小模型分布式推理，提出优化问题和DeBo算法确定分解策略并校准模型。

Result: 能支持多种transformer模型，大模型推理加速达3.1倍，支持GPT2 - XL，内存需求降76.3%，能耗降约40%。

Conclusion: CoFormer能有效解决边缘设备上transformer模型推理难题，在速度、内存和能耗上表现良好。

Abstract: The impressive performance of transformer models has sparked the deployment
of intelligent applications on resource-constrained edge devices. However,
ensuring high-quality service for real-time edge systems is a significant
challenge due to the considerable computational demands and resource
requirements of these models. Existing strategies typically either offload
transformer computations to other devices or directly deploy compressed models
on individual edge devices. These strategies, however, result in either
considerable communication overhead or suboptimal trade-offs between accuracy
and efficiency. To tackle these challenges, we propose a collaborative
inference system for general transformer models, termed CoFormer. The central
idea behind CoFormer is to exploit the divisibility and integrability of
transformer. An off-the-shelf large transformer can be decomposed into multiple
smaller models for distributed inference, and their intermediate results are
aggregated to generate the final output. We formulate an optimization problem
to minimize both inference latency and accuracy degradation under heterogeneous
hardware constraints. DeBo algorithm is proposed to first solve the
optimization problem to derive the decomposition policy, and then progressively
calibrate decomposed models to restore performance. We demonstrate the
capability to support a wide range of transformer models on heterogeneous edge
devices, achieving up to 3.1$\times$ inference speedup with large transformer
models. Notably, CoFormer enables the efficient inference of GPT2-XL with 1.6
billion parameters on edge devices, reducing memory requirements by 76.3\%.
CoFormer can also reduce energy consumption by approximately 40\% while
maintaining satisfactory inference performance.

</details>


### [34] [pdGRASS: A Fast Parallel Density-Aware Algorithm for Graph Spectral Sparsification](https://arxiv.org/abs/2508.20403)
*Tiancheng Zhao,Zekun Yin,Huihai An,Xiaoyu Yang,Zhou Jin,Jiasi Shen,Helen Xu*

Main category: cs.DC

TL;DR: 提出并行密度感知的图谱稀疏化算法pdGRASS解决feGRASS的问题，实验表明pdGRASS在可扩展性和性能上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有高效图谱稀疏化方法feGRASS存在恢复步骤难以并行化和在偏斜输入上性能下降的问题。

Method: 提出pdGRASS算法，将边组织成无数据依赖的不相交子任务，实现高效并行化和单次充分的边恢复。

Result: pdGRASS平均加速比达3.9x - 8.8x，稀疏化器的PCG迭代次数有1.2x - 1.8x的变化，还缓解了feGRASS的最坏情况运行时间。

Conclusion: pdGRASS在图谱稀疏化问题的可扩展性和性能上有显著改进。

Abstract: Graph Spectral Sparsification (GSS) identifies an ultra-sparse subgraph, or
sparsifier, whose Laplacian matrix closely approximates the spectral properties
of the original graph, enabling substantial reductions in computational
complexity for computationally intensive problems in scientific computing. The
state-of-the-art method for efficient GSS is feGRASS, consisting of two steps:
1) spanning tree generation and 2) off-tree edge recovery. However, feGRASS
suffers from two main issues: 1) difficulties in parallelizing the recovery
step for strict data dependencies, and 2) performance degradation on skewed
inputs, often requiring multiple passes to recover sufficient edges. To address
these challenges, we propose parallel density-aware Graph Spectral
Sparsification (pdGRASS), a parallel algorithm that organizes edges into
disjoint subtasks without data dependencies between them, enabling efficient
parallelization and sufficient edge recovery in a single pass. We empirically
evaluate feGRASS and pdGRASS based on 1) off-tree edge-recovery runtime and 2)
sparsifier quality, measured by the iteration count required for convergence in
a preconditioned conjugate gradient (PCG) application. The evaluation
demonstrates that, depending on the number of edges recovered, pdGRASS achieves
average speedups ranging from 3.9x to 8.8x. The resulting sparsifiers also show
between 1.2x higher and 1.8x lower PCG iteration counts, with further
improvements as more edges are recovered. Additionally, pdGRASS mitigates the
worst-case runtimes of feGRASS with over 1000x speedup. These results highlight
pdGRASS's significant improvements in scalability and performance for the graph
spectral sparsification problem.

</details>


### [35] [Collaborative Evolution of Intelligent Agents in Large-Scale Microservice Systems](https://arxiv.org/abs/2508.20508)
*Yilin Li,Song Han,Sibo Wang,Ming Wang,Renzi Meng*

Main category: cs.DC

TL;DR: 本文提出基于多智能体协作进化机制的智能服务优化方法，通过实验验证其在大型微服务系统治理中优于其他方法，有实用价值和工程可行性。


<details>
  <summary>Details</summary>
Motivation: 解决大型微服务架构中服务依赖复杂、拓扑结构动态变化和工作负载波动等治理挑战。

Method: 将每个服务建模为智能体，引入图表示学习构建服务依赖图，基于马尔可夫决策过程学习策略，采用集中训练分散执行框架，设计游戏驱动的策略优化机制。

Result: 在微服务模拟平台实验表明，该方法在多个关键指标上优于其他方法，显著提高治理效率和运营稳定性。

Conclusion: 该方法具有很强的实用价值和工程可行性。

Abstract: This paper proposes an intelligent service optimization method based on a
multi-agent collaborative evolution mechanism to address governance challenges
in large-scale microservice architectures. These challenges include complex
service dependencies, dynamic topology structures, and fluctuating workloads.
The method models each service as an agent and introduces graph representation
learning to construct a service dependency graph. This enables agents to
perceive and embed structural changes within the system. Each agent learns its
policy based on a Markov Decision Process. A centralized training and
decentralized execution framework is used to integrate local autonomy with
global coordination. To enhance overall system performance and adaptability, a
game-driven policy optimization mechanism is designed. Through a
selection-mutation process, agent strategy distributions are dynamically
adjusted. This supports adaptive collaboration and behavioral evolution among
services. Under this mechanism, the system can quickly respond and achieve
stable policy convergence when facing scenarios such as sudden workload spikes,
topology reconfigurations, or resource conflicts. To evaluate the effectiveness
of the proposed method, experiments are conducted on a representative
microservice simulation platform. Comparative analyses are performed against
several advanced approaches, focusing on coordination efficiency, adaptability,
and policy convergence performance. Experimental results show that the proposed
method outperforms others in several key metrics. It significantly improves
governance efficiency and operational stability in large-scale microservice
systems. The method demonstrates strong practical value and engineering
feasibility.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [36] [Commitment Gap via Correlation Gap](https://arxiv.org/abs/2508.20246)
*Shuchi Chawla,Dimitris Christou,Trung Dang*

Main category: cs.DS

TL;DR: 研究代价信息组合选择（CICS）问题，通过简化问题获改进承诺差距界和近似结果。


<details>
  <summary>Details</summary>
Motivation: CICS确定最优算法计算困难，此前采用近似算法，需界定承诺差距。

Method: 将CICS问题归约为信息免费的贝叶斯组合选择问题，建立与事前自由顺序预言不等式的关系。

Result: 获得CICS承诺差距的改进界，得到改进的近似结果。

Conclusion: 通过归约方法能有效解决CICS问题中承诺差距界定和近似计算问题。

Abstract: Selection problems with costly information, dating back to Weitzman's
Pandora's Box problem, have received much attention recently. We study the
general model of Costly Information Combinatorial Selection (CICS) that was
recently introduced by Chawla et al. [2024] and Bowers et al. [2025]. In this
problem, a decision maker needs to select a feasible subset of stochastic
variables, and can only learn information about their values through a series
of costly steps, modeled by a Markov decision process. The algorithmic
objective is to maximize the total value of the selection minus the cost of
information acquisition. However, determining the optimal algorithm is known to
be a computationally challenging problem.
  To address this challenge, previous approaches have turned to approximation
algorithms by considering a restricted class of committing policies that
simplify the decision-making aspects of the problem and allow for efficient
optimization. This motivates the question of bounding the commitment gap,
measuring the worst case ratio in the performance of the optimal committing
policy and the overall optimal.
  In this work, we obtain improved bounds on the commitment gap of CICS through
a reduction to a simpler problem of Bayesian Combinatorial Selection where
information is free. By establishing a close relationship between these
problems, we are able to relate the commitment gap of CICS to ex ante
free-order prophet inequalities. As a consequence, we obtain improved
approximation results for CICS, including the well-studied variant of Pandora's
Box with Optional Inspection under matroid feasibility constraints.

</details>


### [37] [Reducing Shortcut and Hopset Constructions to Shallow Graphs](https://arxiv.org/abs/2508.20302)
*Bernhard Haeupler,Yonggang Jiang,Thatchaphol Saranurak*

Main category: cs.DS

TL;DR: 提出黑盒框架简化有向图单源可达性和最短路径近线性工作量的并行算法。


<details>
  <summary>Details</summary>
Motivation: 简化有向图单源可达性和最短路径的已知并行算法。

Method: 引入黑盒框架，让构建捷径的算法假设输入图是“浅”的。

Result: 简化了捷径构建，简化了并行可达性算法，还可扩展到简化构建跳集和计算最短路径的并行算法。

Conclusion: 所提黑盒框架能有效简化相关并行算法。

Abstract: We introduce a blackbox framework that simplifies all known parallel
algorithms with near-linear work for single-source reachability and shortest
paths in directed graphs. Specifically, existing reachability algorithms rely
on constructing shortcuts; our blackbox allows these algorithms that construct
shortcuts with hopbound $h$ to assume the input graph $G$ is ``shallow'',
meaning if vertex $s$ can reach vertex $t$, it can do so in approximately $h$
hops. This assumption significantly simplifies shortcut construction [Fin18,
JLS19], resulting in simpler parallel reachability algorithms. Furthermore, our
blackbox extends naturally to simplify parallel algorithms for constructing
hopsets and, consequently, for computing shortest paths [CFR20 , CF23 , RHM+23
].

</details>


### [38] [Directed and Undirected Vertex Connectivity Problems are Equivalent for Dense Graphs](https://arxiv.org/abs/2508.20305)
*Yonggang Jiang,Sagnik Mukhopadhyay,Sorrachai Yingchareonthawornchai*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Vertex connectivity and its variants are among the most fundamental problems
in graph theory, with decades of extensive study and numerous algorithmic
advances. The directed variants of vertex connectivity are usually solved by
manually extending fast algorithms for undirected graphs, which has required
considerable effort.
  In this paper, we present a simple, black-box randomized reduction from
directed to undirected vertex connectivity for dense graphs. As immediate
corollaries, we largely simplify the proof for directed vertex connectivity in
$n^{2+o(1)}$ time [LNP+25], and obtain a parallel vertex connectivity algorithm
for directed graphs with $n^{\omega+o(1)}$ work and $n^{o(1)}$ depth, via the
undirected vertex connectivity algorithm of [BJMY25]. Our reduction further
extends to the weighted version of the problem. By combining our reduction with
the recent subcubic-time algorithm for undirected weighted vertex cuts [CT25],
we obtain the first subcubic-time algorithm for weighted directed vertex
connectivity, improving upon a three-decade-old bound [HRG00] for dense graphs.

</details>


### [39] [Improved Dominance Filtering for Unions and Minkowski Sums of Pareto Sets](https://arxiv.org/abs/2508.20689)
*Konstantinos Karathanasis,Spyros Kontogiannis,Christos Zaroliagis*

Main category: cs.DS

TL;DR: 本文引入三种新数据结构用于高效索引非支配目标向量并执行支配检查，设计新算法过滤支配目标向量，实验显示新算法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 多目标优化中支配过滤是关键任务且是瓶颈，需要高效方法处理目标空间。

Method: 引入ND⁺ - 树、QND⁺ - 树和TND⁺ - 树三种新数据结构，设计三种新算法过滤支配目标向量。

Result: 在合成和真实数据集上实验表明，新算法在帕累托集的并集和闵可夫斯基和的支配过滤方面优于现有技术，且在维度和集合大小上有良好扩展性。

Conclusion: 新的数据结构和算法能有效提高多目标优化中支配过滤的效率。

Abstract: A key task in multi-objective optimization is to compute the Pareto subset or
frontier $P$ of a given $d$-dimensional objective space $F$; that is, a maximal
subset $P\subseteq F$ such that every element in $P$ is not-dominated (it is
not worse in all criteria) by any element in $F$. This process, called
dominance-filtering, often involves handling objective spaces derived from
either the union or the Minkowski sum of two given partial objective spaces
which are Pareto sets themselves, and constitutes a major bottleneck in several
multi-objective optimization techniques. In this work, we introduce three new
data structures, ND$^{+}$-trees, QND$^{+}$-trees and TND$^{+}$-trees, which are
designed for efficiently indexing non-dominated objective vectors and
performing dominance-checks. We also devise three new algorithms that
efficiently filter out dominated objective vectors from the union or the
Minkowski sum of two Pareto sets. An extensive experimental evaluation on both
synthetically generated and real-world data sets reveals that our new
algorithms outperform state-of-art techniques for dominance-filtering of unions
and Minkowski sums of Pareto sets, and scale well w.r.t. the number of $d\ge 3$
criteria and the sets' sizes.

</details>


### [40] [Sharp Online Hardness for Large Balanced Independent Sets](https://arxiv.org/abs/2508.20785)
*Abhishek Dhawan,Eren C. Kızıldağ,Neeladri Maitra*

Main category: cs.DS

TL;DR: 研究稠密随机二部图中找大γ - 平衡独立集的算法问题，给出最大γ - 平衡独立集大小，设计在线算法并给出下界，支持统计 - 计算间隙普遍性猜想。


<details>
  <summary>Details</summary>
Motivation: 稀疏情形下的技术无法扩展到稠密情形，相关问题已知算法有局限性，需研究稠密随机二部图中γ - 平衡独立集问题。

Method: 设计分两阶段的在线算法，结合停止时间和截断确保γ - 平衡性；利用OGP框架并将其扩展到二部图情形给出下界。

Result: 证明最大γ - 平衡独立集大小为α，设计的在线算法能达到(1 - ε)(1 - γ)α，且无在线算法能以不可忽略概率达到(1 + ε)(1 - γ)α。

Conclusion: 稠密情形也存在1/(1 - γ)的统计 - 计算间隙，支持其普遍性猜想。

Abstract: We study the algorithmic problem of finding large $\gamma$-balanced
independent sets in dense random bipartite graphs; an independent set is
$\gamma$-balanced if a $\gamma$ proportion of its vertices lie on one side of
the bipartition. In the sparse regime, Perkins and Wang established tight
bounds within the low-degree polynomial (LDP) framework, showing a
factor-$1/(1-\gamma)$ statistical-computational gap via the Overlap Gap
Property (OGP) framework tailored for stable algorithms. However, these
techniques do not appear to extend to the dense setting. For the related large
independent set problem in dense random graph, the best known algorithm is an
online greedy procedure that is inherently unstable, and LDP algorithms are
conjectured to fail even in the "easy" regime where greedy succeeds. We show
that the largest $\gamma$-balanced independent set in dense random bipartite
graphs has size $\alpha:=\frac{\log_b n}{\gamma(1-\gamma)}$ whp, where $n$ is
the size of each bipartition, $p$ is the edge probability, and $b=1/(1-p)$. We
design an online algorithm that achieves $(1-\epsilon)(1-\gamma)\alpha$ whp for
any $\epsilon>0$. We complement this with a sharp lower bound, showing that no
online algorithm can achieve $(1+\epsilon)(1-\gamma)\alpha$ with nonnegligible
probability. Our results suggest that the same factor-$1/(1-\gamma)$ gap is
also present in the dense setting, supporting its conjectured universality.
While the classical greedy procedure on $G(n,p)$ is straightforward, our
algorithm is more intricate: it proceeds in two stages, incorporating a
stopping time and suitable truncation to ensure that $\gamma$-balancedness-a
global constraint-is met despite operating with limited information. Our lower
bound utilizes the OGP framework; we build on a recent refinement of this
framework for online models and extend it to the bipartite setting.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [41] [Collaborating with GenAI: Incentives and Replacements](https://arxiv.org/abs/2508.20213)
*Boaz Taitler,Omer Ben-Porat*

Main category: cs.GT

TL;DR: 本文提出理论框架分析生成式AI对协作的影响，发现其可能使工人不努力，经理优化问题NP - 完全，给出特殊情况算法，指出低价值工人的重要性并通过模拟验证。


<details>
  <summary>Details</summary>
Motivation: 生成式AI兴起改变工人在共享项目中的贡献方式，需要分析其对协作的影响。

Method: 构建理论模型，经理选团队，GenAI替代未选工人，工人选择努力程度；给出算法；进行广泛模拟。

Result: GenAI可能使工人不努力；经理优化问题NP - 完全；给出特殊类实例的高效算法；低价值工人对维持整体产出有关键作用。

Conclusion: 生成式AI对协作有复杂影响，低价值工人不应被轻易排除。

Abstract: The rise of Generative AI (GenAI) is reshaping how workers contribute to
shared projects. While workers can use GenAI to boost productivity or reduce
effort, managers may use it to replace some workers entirely. We present a
theoretical framework to analyze how GenAI affects collaboration in such
settings. In our model, the manager selects a team to work on a shared task,
with GenAI substituting for unselected workers. Each worker selects how much
effort to exert, and incurs a cost that increases with the level of effort. We
show that GenAI can lead workers to exert no effort, even if GenAI is almost
ineffective. We further show that the manager's optimization problem is
NP-complete, and provide an efficient algorithm for the special class of
(almost-) linear instances. Our analysis shows that even workers with low
individual value may play a critical role in sustaining overall output, and
excluding such workers can trigger a cascade. Finally, we conduct extensive
simulations to illustrate our theoretical findings.

</details>


### [42] [Balancing Profit and Traveller Acceptance in Ride-Pooling Personalised Fares](https://arxiv.org/abs/2508.20723)
*Michal Bujak,Rafal Kucharski*

Main category: cs.GT

TL;DR: 研究表明运营商可学习乘客接受程度以优化个性化票价，采用自适应定价策略能提升双方效益。


<details>
  <summary>Details</summary>
Motivation: 拼车系统需以有吸引力的价格弥补感知成本，但乘客可接受价格不同且运营商未知，需解决此问题。

Method: 提出自适应定价策略，让运营商每天构建逐步符合乘客期望的报价。

Result: 运营商能以超90%的准确率在10天内学习拼车乘客的接受水平，可增加乘客效用和自身利润，去除低效拼车组合。

Conclusion: 运营商学习乘客行为特征可改善自身和乘客的表现。

Abstract: Ride-pooling systems, to succeed, must provide an attractive service, namely
compensate perceived costs with an appealing price. However, because of a
strong heterogeneity in a value-of-time, each traveller has his own acceptable
price, unknown to the operator. Here, we show that individual acceptance levels
can be learned by the operator (over $90\%$ accuracy for pooled travellers in
$10$ days) to optimise personalised fares. We propose an adaptive pricing
policy, where every day the operator constructs an offer that progressively
meets travellers' expectations and attracts a growing demand. Our results
suggest that operators, by learning behavioural traits of individual
travellers, may improve performance not only for travellers (increased utility)
but also for themselves (increased profit). Moreover, such knowledge allows the
operator to remove inefficient pooled rides and focus on attractive and
profitable combinations.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [43] [A Survey of Affective Recommender Systems: Modeling Attitudes, Emotions, and Moods for Personalization](https://arxiv.org/abs/2508.20289)
*Tonmoy Hasan,Razvan Bunescu*

Main category: cs.IR

TL;DR: 本文对情感推荐系统进行全面系统综述，引入分类方案，指出研究趋势、局限和挑战，并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有情感推荐系统的调查缺乏基于心理学的分类法，且常仅研究特定情感状态或应用领域，本文旨在解决这些局限。

Method: 借鉴Scherer的情感状态类型学，引入分类方案，将系统分为四类，记录情感信号提取技术、系统架构和应用领域。

Result: 梳理出情感推荐系统的关键趋势、局限和开放挑战。

Conclusion: 未来研究应关注混合模型、大规模情感感知数据集的开发，并用更精确的术语替代通俗词汇，本文可作为推进情感驱动个性化研究和应用的参考指南。

Abstract: Affective Recommender Systems are an emerging class of intelligent systems
that aim to enhance personalization by aligning recommendations with users'
affective states. Reflecting a growing interest, a number of surveys have been
published in this area, however they lack an organizing taxonomy grounded in
psychology and they often study only specific types of affective states or
application domains. This survey addresses these limitations by providing a
comprehensive, systematic review of affective recommender systems across
diverse domains. Drawing from Scherer's typology of affective states, we
introduce a classification scheme that organizes systems into four main
categories: attitude aware, emotion aware, mood aware, and hybrid. We further
document affective signal extraction techniques, system architectures, and
application areas, highlighting key trends, limitations, and open challenges.
As future research directions, we emphasize hybrid models that leverage
multiple types of affective states across different modalities, the development
of large-scale affect-aware datasets, and the need to replace the folk
vocabulary of affective states with a more precise terminology grounded in
cognitive and social psychology. Through its systematic review of existing
research and challenges, this survey aims to serve as a comprehensive reference
and a useful guide for advancing academic research and industry applications in
affect-driven personalization.

</details>


### [44] [ELIXIR: Efficient and LIghtweight model for eXplaIning Recommendations](https://arxiv.org/abs/2508.20312)
*Ben Kabongo,Vincent Guigue,Pirmin Lemberger*

Main category: cs.IR

TL;DR: 提出多任务模型ELIXIR结合评分预测与个性化评论生成，在TripAdvisor和RateBeer数据集实验中显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 协同过滤在细粒度用户 - 项目交互和可解释性上有困难，现有生成文本解释的方法存在不足，用户寻求透明推荐。

Method: 提出ELIXIR模型，联合学习用户和项目的全局和特定方面表示，优化整体评分、方面级评分和评论生成，采用个性化注意力。基于T5 - small模型。

Result: 在TripAdvisor和RateBeer上实验，ELIXIR显著优于强基线模型，尤其在评论生成方面。

Conclusion: ELIXIR的基于方面的架构能在个性化场景下有效引导文本生成，优于现有方法。

Abstract: Collaborative filtering drives many successful recommender systems but
struggles with fine-grained user-item interactions and explainability. As users
increasingly seek transparent recommendations, generating textual explanations
through language models has become a critical research area. Existing methods
employ either RNNs or Transformers. However, RNN-based approaches fail to
leverage the capabilities of pre-trained Transformer models, whereas
Transformer-based methods often suffer from suboptimal adaptation and neglect
aspect modeling, which is crucial for personalized explanations. We propose
ELIXIR (Efficient and LIghtweight model for eXplaIning Recommendations), a
multi-task model combining rating prediction with personalized review
generation. ELIXIR jointly learns global and aspect-specific representations of
users and items, optimizing overall rating, aspect-level ratings, and review
generation, with personalized attention to emphasize aspect importance. Based
on a T5-small (60M) model, we demonstrate the effectiveness of our aspect-based
architecture in guiding text generation in a personalized context, where
state-of-the-art approaches exploit much larger models but fail to match user
preferences as well. Experimental results on TripAdvisor and RateBeer
demonstrate that ELIXIR significantly outperforms strong baseline models,
especially in review generation.

</details>


### [45] [Progressive Semantic Residual Quantization for Multimodal-Joint Interest Modeling in Music Recommendation](https://arxiv.org/abs/2508.20359)
*Shijia Wang,Tianpei Ouyang,Qiang Xiao,Dongjing Wang,Yintao Ren,Songpei Xu,Da Guo,Chuanjiang Luo*

Main category: cs.IR

TL;DR: 文章指出音乐推荐系统中多模态兴趣学习的重要性，针对现有方法的局限性提出两阶段多模态推荐框架，实验表明其优于现有基线，且已在平台部署验证其实用价值。


<details>
  <summary>Details</summary>
Motivation: 现有多模态音乐推荐方法存在模态内语义退化和模态间建模差距的问题，影响用户兴趣建模。

Method: 提出两阶段多模态推荐框架，第一阶段用PSRQ方法生成模态特定和联合语义ID，第二阶段用MCCA网络建模用户多模态兴趣。

Result: 在多个真实数据集上实验表明框架优于现有基线，在平台部署的A/B测试显示商业指标显著提升。

Conclusion: 提出的框架有效解决现有方法问题，对工业级推荐系统有实用价值。

Abstract: In music recommendation systems, multimodal interest learning is pivotal,
which allows the model to capture nuanced preferences, including textual
elements such as lyrics and various musical attributes such as different
instruments and melodies. Recently, methods that incorporate multimodal content
features through semantic IDs have achieved promising results. However,
existing methods suffer from two critical limitations: 1) intra-modal semantic
degradation, where residual-based quantization processes gradually decouple
discrete IDs from original content semantics, leading to semantic drift; and 2)
inter-modal modeling gaps, where traditional fusion strategies either overlook
modal-specific details or fail to capture cross-modal correlations, hindering
comprehensive user interest modeling. To address these challenges, we propose a
novel multimodal recommendation framework with two stages. In the first stage,
our Progressive Semantic Residual Quantization (PSRQ) method generates
modal-specific and modal-joint semantic IDs by explicitly preserving the prefix
semantic feature. In the second stage, to model multimodal interest of users, a
Multi-Codebook Cross-Attention (MCCA) network is designed to enable the model
to simultaneously capture modal-specific interests and perceive cross-modal
correlations. Extensive experiments on multiple real-world datasets demonstrate
that our framework outperforms state-of-the-art baselines. This framework has
been deployed on one of China's largest music streaming platforms, and online
A/B tests confirm significant improvements in commercial metrics, underscoring
its practical value for industrial-scale recommendation systems.

</details>


### [46] [A Case Study of Balanced Query Recommendation on Wikipedia](https://arxiv.org/abs/2508.20399)
*Harshit Mishra,Sucheta Soundarajan*

Main category: cs.IR

TL;DR: 本文研究现代IR系统查询推荐的偏差问题，提出扩展的BalancedQR方法并在Wikipedia数据集评估，证明其有效性和查询措辞的影响。


<details>
  <summary>Details</summary>
Motivation: 现代IR系统查询推荐方法结果存在对性别、种族等受保护类别不期望或错误的偏差，需解决平衡查询推荐问题。

Method: 采用扩展的BalancedQR方法，运用Pareto front方法寻找平衡查询，优化多目标如性别偏差、区域偏差及结果相关性。

Result: 在Wikipedia数据集上评估扩展的BalancedQR，证明了其有效性。

Conclusion: 扩展的BalancedQR框架有效，查询措辞和语言选择对检索有显著影响。

Abstract: Modern IR systems are an extremely important tool for seeking information. In
addition to search, such systems include a number of query reformulation
methods, such as query expansion and query recommendations, to provide high
quality results. However, results returned by such methods sometimes exhibit
undesirable or wrongful bias with respect to protected categories such as
gender or race. Our earlier work considered the problem of balanced query
recommendation, where instead of re-ranking a list of results based on fairness
measures, the goal was to suggest queries that are relevant to a user's search
query but exhibit less bias than the original query. In this work, we present a
case study of BalancedQR using an extension of BalancedQR that handles biases
in multiple dimensions. It employs a Pareto front approach that finds balanced
queries, optimizing for multiple objectives such as gender bias and regional
bias, along with the relevance of returned results. We evaluate the extended
version of BalancedQR on a Wikipedia dataset.Our results demonstrate the
effectiveness of our extension to BalancedQR framework and highlight the
significant impact of subtle query wording,linguistic choice on retrieval.

</details>


### [47] [MPFormer: Adaptive Framework for Industrial Multi-Task Personalized Sequential Retriever](https://arxiv.org/abs/2508.20400)
*Yijia Sun,Shanshan Huang,Linxiao Che,Haitao Lu,Qiang Luo,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: 论文提出 MPFormer 框架解决工业推荐系统多阶段优化失调问题，在快手系统应用效果良好。


<details>
  <summary>Details</summary>
Motivation: 现代工业推荐系统存在多阶段优化失调问题，主流解决方案有资源增长和处理目标耦合的局限。

Method: 提出 MPFormer 框架，包括目标条件化 Transformer、引入个性化目标权重、融合用户个性化信息。

Result: 框架集成到快手短视频推荐系统，服务超 4 亿日活用户，提升用户参与度和系统效率。

Conclusion: 与传统方案相比，有效优化多目标检索迭代范式，保持响应速度，为工业推荐系统提供可扩展多目标解决方案。

Abstract: Modern industrial recommendation systems encounter a core challenge of
multi-stage optimization misalignment: a significant semantic gap exists
between the multi-objective optimization paradigm widely used in the ranking
phase and the single-objective modeling in the retrieve phase. Although the
mainstream industry solution achieves multi-objective coverage through parallel
multi-path single-objective retrieval, this approach leads to linear growth of
training and serving resources with the number of objectives and has inherent
limitations in handling loosely coupled objectives. This paper proposes the
MPFormer, a dynamic multi-task Transformer framework, which systematically
addresses the aforementioned issues through three innovative mechanisms. First,
an objective-conditioned transformer that jointly encodes user behavior
sequences and multi-task semantics through learnable attention modulation;
second, personalized target weights are introduced to achieve dynamic
adjustment of retrieval results; finally, user personalization information is
incorporated into token representations and the Transformer structure to
further enhance the model's representation ability. This framework has been
successfully integrated into Kuaishou short video recommendation system, stably
serving over 400 million daily active users. It significantly improves user
daily engagement and system operational efficiency. Practical deployment
verification shows that, compared with traditional solutions, it effectively
optimizes the iterative paradigm of multi-objective retrieval while maintaining
service response speed, providing a scalable multi-objective solution for
industrial recommendation systems.

</details>


### [48] [Revealing Potential Biases in LLM-Based Recommender Systems in the Cold Start Setting](https://arxiv.org/abs/2508.20401)
*Alexandre Andre,Gauthier Roy,Eva Dyer,Kai Wang*

Main category: cs.IR

TL;DR: 介绍用于评估零上下文推荐公平性的基准，评估模型发现存在一致偏差及模型大小与公平性的非线性关系。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在冷启动场景推荐存在公平性问题，需评估其公平性。

Method: 引入专门基准，模块化管道支持可配置推荐领域和敏感属性，对开源大语言模型进行系统灵活审计。

Result: 评估Gemma 3和Llama 3.2等模型，发现推荐领域存在性别和文化刻板印象等一致偏差，揭示模型大小与公平性的非线性关系。

Conclusion: 对大语言模型在零上下文推荐中的公平性评估需进行细致分析。

Abstract: Large Language Models (LLMs) are increasingly used for recommendation tasks
due to their general-purpose capabilities. While LLMs perform well in
rich-context settings, their behavior in cold-start scenarios, where only
limited signals such as age, gender, or language are available, raises fairness
concerns because they may rely on societal biases encoded during pretraining.
We introduce a benchmark specifically designed to evaluate fairness in
zero-context recommendation. Our modular pipeline supports configurable
recommendation domains and sensitive attributes, enabling systematic and
flexible audits of any open-source LLM. Through evaluations of state-of-the-art
models (Gemma 3 and Llama 3.2), we uncover consistent biases across
recommendation domains (music, movies, and colleges) including gendered and
cultural stereotypes. We also reveal a non-linear relationship between model
size and fairness, highlighting the need for nuanced analysis.

</details>


### [49] [Fact or Facsimile? Evaluating the Factual Robustness of Modern Retrievers](https://arxiv.org/abs/2508.20408)
*Haoyu Wu,Qingcheng Zeng,Kaize Ding*

Main category: cs.IR

TL;DR: 研究对比检索增强生成（RAG）管道中嵌入模型与基础大语言模型（LLMs）的事实准确性，发现嵌入模型准确性大幅下降，且对干扰项敏感，揭示对比学习的权衡。


<details>
  <summary>Details</summary>
Motivation: 了解RAG管道中密集检索器和重排器从基础LLMs继承或损失多少事实能力。

Method: 将12个公开嵌入检查点与其基础LLMs配对，在事实基准上评估；用GPT - 4.1改写正确答案创建新测试集。

Result: 嵌入模型准确性比基础模型显著降低；检索器对干扰项数量敏感；决策受表面语义接近度驱动；改写正确答案后准确率大幅降低。

Conclusion: 对比学习为检索器带来语义检索增益的同时，造成参数化事实知识的损失。

Abstract: Dense retrievers and rerankers are central to retrieval-augmented generation
(RAG) pipelines, where accurately retrieving factual information is crucial for
maintaining system trustworthiness and defending against RAG poisoning.
However, little is known about how much factual competence these components
inherit or lose from the large language models (LLMs) they are based on. We
pair 12 publicly released embedding checkpoints with their original base LLMs
and evaluate both sets on a factuality benchmark. Across every model evaluated,
the embedding variants achieve markedly lower accuracy than their bases, with
absolute drops ranging from 12 to 43 percentage points (median 28 pts) and
typical retriever accuracies collapsing into the 25-35 % band versus the 60-70
% attained by the generative models. This degradation intensifies under a more
demanding condition: when the candidate pool per question is expanded from four
options to one thousand, the strongest retriever's top-1 accuracy falls from 33
% to 26 %, revealing acute sensitivity to distractor volume. Statistical tests
further show that, for every embedding model, cosine-similarity scores between
queries and correct completions are significantly higher than those for
incorrect ones (p < 0.01), indicating decisions driven largely by surface-level
semantic proximity rather than factual reasoning. To probe this weakness, we
employed GPT-4.1 to paraphrase each correct completion, creating a rewritten
test set that preserved factual truth while masking lexical cues, and observed
that over two-thirds of previously correct predictions flipped to wrong,
reducing overall accuracy to roughly one-third of its original level. Taken
together, these findings reveal a systematic trade-off introduced by
contrastive learning for retrievers: gains in semantic retrieval are paid for
with losses in parametric factual knowledge......

</details>


### [50] [Rethinking Purity and Diversity in Multi-Behavior Sequential Recommendation from the Frequency Perspective](https://arxiv.org/abs/2508.20427)
*Yongqiang Han,Kai Cheng,Kefan Wang,Enhong Chen*

Main category: cs.IR

TL;DR: 本文指出高频信息并非不重要，提出PDB4Rec模型跨频段提取信息，引入Bootstrapping Balancer机制，实验证明模型有效。


<details>
  <summary>Details</summary>
Motivation: 多行为顺序推荐中部分行为数据会带来噪声，虽有研究从频域去噪，但本文认为高频信息也重要。

Method: 提出PDB4Rec模型跨频段提取信息及其关系，引入Bootstrapping Balancer机制平衡贡献。

Result: 在真实数据集上实验证明模型有效且高效。

Conclusion: 高频信息对应用户兴趣多样性，低频对应兴趣纯度，PDB4Rec模型能提升推荐性能。

Abstract: In recommendation systems, users often exhibit multiple behaviors, such as
browsing, clicking, and purchasing. Multi-behavior sequential recommendation
(MBSR) aims to consider these different behaviors in an integrated manner to
improve the recommendation performance of the target behavior. However, some
behavior data will also bring inevitable noise to the modeling of user
interests. Some research efforts focus on data denoising from the frequency
domain perspective to improve the accuracy of user preference prediction. These
studies indicate that low-frequency information tends to be valuable and
reliable, while high-frequency information is often associated with noise. In
this paper, we argue that high-frequency information is by no means
insignificant. Further experimental results highlight that low frequency
corresponds to the purity of user interests, while high frequency corresponds
to the diversity of user interests. Building upon this finding, we proposed our
model PDB4Rec, which efficiently extracts information across various frequency
bands and their relationships, and introduces Boostrapping Balancer mechanism
to balance their contributions for improved recommendation performance.
Sufficient experiments on real-world datasets demonstrate the effectiveness and
efficiency of our model.

</details>


### [51] [Multistakeholder Fairness in Tourism: What can Algorithms learn from Tourism Management?](https://arxiv.org/abs/2508.20496)
*Peter Muellner,Anna Schreuer,Simone Kopeinik,Bernhard Wieser,Dominik Kowald*

Main category: cs.IR

TL;DR: 本文指出算法决策支持系统对旅游有负面影响，分析旅游管理与计算机科学在研究利益相关者公平性上的差异，强调跨学科合作的必要性。


<details>
  <summary>Details</summary>
Motivation: 解决算法决策支持系统引导游客流向对环境、社区等产生负面影响的问题，原因是计算机科学界对利益相关者关系理解有限。

Method: 采用半系统性文献综述，综合旅游管理和计算机科学领域的文献。

Result: 旅游管理积极识别利益相关者需求，用定性、包容和参与式方法从规范和整体视角研究公平性；计算机科学缺乏对利益相关者需求的充分理解，主要通过可衡量的歧视等描述性因素考虑公平性，依赖少数数学形式化的公平标准。

Conclusion: 强调单纯算法研究存在不足，未来跨学科合作对提升算法决策支持系统以实现旅游中真正的多利益相关者公平性是必要的。

Abstract: Algorithmic decision-support systems, i.e., recommender systems, are popular
digital tools that help tourists decide which places and attractions to
explore. However, algorithms often unintentionally direct tourist streams in a
way that negatively affects the environment, local communities, or other
stakeholders. This issue can be partly attributed to the computer science
community's limited understanding of the complex relationships and trade-offs
among stakeholders in the real world.
  In this work, we draw on the practical findings and methods from tourism
management to inform research on multistakeholder fairness in algorithmic
decision-support. Leveraging a semi-systematic literature review, we synthesize
literature from tourism management as well as literature from computer science.
Our findings suggest that tourism management actively tries to identify the
specific needs of stakeholders and utilizes qualitative, inclusive and
participatory methods to study fairness from a normative and holistic research
perspective. In contrast, computer science lacks sufficient understanding of
the stakeholder needs and primarily considers fairness through descriptive
factors, such as measureable discrimination, while heavily relying on few
mathematically formalized fairness criteria that fail to capture the
multidimensional nature of fairness in tourism.
  With the results of this work, we aim to illustrate the shortcomings of
purely algorithmic research and stress the potential and particular need for
future interdisciplinary collaboration. We believe such a collaboration is a
fundamental and necessary step to enhance algorithmic decision-support systems
towards understanding and supporting true multistakeholder fairness in tourism.

</details>


### [52] [Enhancing Semantic Document Retrieval- Employing Group Steiner Tree Algorithm with Domain Knowledge Enrichment](https://arxiv.org/abs/2508.20543)
*Apurva Kulkarni,Chandrashekar Ramanathan,Vinu E Venugopal*

Main category: cs.IR

TL;DR: 本文提出基于语义的概念检索算法并应用于文档检索系统，实验显示相比基线系统有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义的检索系统因缺乏特定领域信息和依赖旧知识源，精度受影响，需解决从不同数据源检索相关文档的难题。

Method: 开发'基于语义的概念检索使用组斯坦纳树'算法，将其应用于文档检索系统，用170个真实搜索查询进行性能评估，经领域专家严格评估验证。

Result: 实验结果显示，相比基线系统有显著进步，精度和准确率分别达90%和82%。

Conclusion: 提出的算法和系统能有效提升文档检索的精度和准确率，有良好改进效果。

Abstract: Retrieving pertinent documents from various data sources with diverse
characteristics poses a significant challenge for Document Retrieval Systems.
The complexity of this challenge is further compounded when accounting for the
semantic relationship between data and domain knowledge. While existing
retrieval systems using semantics (usually represented as Knowledge Graphs
created from open-access resources and generic domain knowledge) hold promise
in delivering relevant outcomes, their precision may be compromised due to the
absence of domain-specific information and reliance on outdated knowledge
sources. In this research, the primary focus is on two key contributions- a)
the development of a versatile algorithm- 'Semantic-based Concept Retrieval
using Group Steiner Tree' that incorporates domain information to enhance
semantic-aware knowledge representation and data access, and b) the practical
implementation of the proposed algorithm within a document retrieval system
using real-world data. To assess the effectiveness of the SemDR system,
research work conducts performance evaluations using a benchmark consisting of
170 real-world search queries. Rigorous evaluation and verification by domain
experts are conducted to ensure the validity and accuracy of the results. The
experimental findings demonstrate substantial advancements when compared to the
baseline systems, with precision and accuracy achieving levels of 90% and 82%
respectively, signifying promising improvements.

</details>


### [53] [SUMMA: A Multimodal Large Language Model for Advertisement Summarization](https://arxiv.org/abs/2508.20582)
*Weitao Jia,Shuo Yin,Zhoufutu Wen,Han Wang,Zehui Dai,Kun Zhang,Zhenyu Li,Tao Zeng,Xiaohui Lv*

Main category: cs.IR

TL;DR: 提出SUMMA模型处理视频广告生成高商业价值摘要，集成到生产流程提升效果，实验显示优于基线，增加广告收入。


<details>
  <summary>Details</summary>
Motivation: 现有多模态信息利用不足，需提高短视频平台查询-广告匹配和相关性排名，增强广告效果和用户体验。

Method: 采用两阶段训练策略（多模态监督微调后接混合奖励机制的强化学习），在含视频帧和ASR/OCR转录的特定领域数据上开发SUMMA模型。

Result: 离线和在线实验均优于基线，在线结果显示广告收入显著增加1.5%。

Conclusion: 建立了将多模态信息浓缩为代表性文本的新范式，使视觉广告内容与用户查询意图有效对齐。

Abstract: Understanding multimodal video ads is crucial for improving query-ad matching
and relevance ranking on short video platforms, enhancing advertising
effectiveness and user experience. However, the effective utilization of
multimodal information with high commercial value still largely constrained by
reliance on highly compressed video embeddings-has long been inadequate. To
address this, we propose SUMMA (the abbreviation of Summarizing MultiModal
Ads), a multimodal model that automatically processes video ads into summaries
highlighting the content of highest commercial value, thus improving their
comprehension and ranking in Douyin search-advertising systems. SUMMA is
developed via a two-stage training strategy-multimodal supervised fine-tuning
followed by reinforcement learning with a mixed reward mechanism-on
domain-specific data containing video frames and ASR/OCR transcripts,
generating commercially valuable and explainable summaries. We integrate
SUMMA-generated summaries into our production pipeline, directly enhancing the
candidate retrieval and relevance ranking stages in real search-advertising
systems. Both offline and online experiments show substantial improvements over
baselines, with online results indicating a statistically significant 1.5%
increase in advertising revenue. Our work establishes a novel paradigm for
condensing multimodal information into representative texts, effectively
aligning visual ad content with user query intent in retrieval and
recommendation scenarios.

</details>


### [54] [SemSR: Semantics aware robust Session-based Recommendations](https://arxiv.org/abs/2508.20587)
*Jyoti Narwariya,Priyanka Gupta,Muskan Gupta,Jyotsana Khatri,Lovekesh Vig*

Main category: cs.IR

TL;DR: 本文提出利用大语言模型进行会话推荐的多种方法，实验表明LLM与数据驱动模型结合在召回率和平均倒数排名指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有会话推荐模型未充分利用语义信息，基于提示和微调的大语言模型方法分别存在难以确定最佳提示和计算成本高的问题。

Method: 提出（i）将上下文学习的大语言模型作为推荐代理；（ii）用大语言模型生成的表示初始化深度学习会话推荐模型；（iii）将大语言模型与数据驱动的会话推荐模型集成。

Result: 大语言模型在粗粒度检索上表现出色，传统数据驱动技术在细粒度排序上表现良好，大语言模型与数据驱动模型集成在召回率和平均倒数排名指标上优于单独的大语言模型方法、数据驱动深度学习模型和基线会话推荐模型。

Conclusion: 大语言模型与数据驱动的会话推荐模型集成是一种有效的会话推荐方法。

Abstract: Session-based recommendation (SR) models aim to recommend items to anonymous
users based on their behavior during the current session. While various SR
models in the literature utilize item sequences to predict the next item, they
often fail to leverage semantic information from item titles or descriptions
impeding session intent identification and interpretability. Recent research
has explored Large Language Models (LLMs) as promising approaches to enhance
session-based recommendations, with both prompt-based and fine-tuning based
methods being widely investigated. However, prompt-based methods struggle to
identify optimal prompts that elicit correct reasoning and lack task-specific
feedback at test time, resulting in sub-optimal recommendations. Fine-tuning
methods incorporate domain-specific knowledge but incur significant
computational costs for implementation and maintenance. In this paper, we
present multiple approaches to utilize LLMs for session-based recommendation:
(i) in-context LLMs as recommendation agents, (ii) LLM-generated
representations for semantic initialization of deep learning SR models, and
(iii) integration of LLMs with data-driven SR models. Through comprehensive
experiments on two real-world publicly available datasets, we demonstrate that
LLM-based methods excel at coarse-level retrieval (high recall values), while
traditional data-driven techniques perform well at fine-grained ranking (high
Mean Reciprocal Rank values). Furthermore, the integration of LLMs with
data-driven SR models significantly out performs both standalone LLM approaches
and data-driven deep learning models, as well as baseline SR models, in terms
of both Recall and MRR metrics.

</details>


### [55] [SEAL: Structure and Element Aware Learning to Improve Long Structured Document Retrieval](https://arxiv.org/abs/2508.20778)
*Xinhao Huang,Zhibo Ren,Yipeng Yu,Ying Zhou,Zulong Chen,Zeyi Wen*

Main category: cs.IR

TL;DR: 本文提出了SEAL对比学习框架和SEALDR数据集，实验证明能提升长结构化文档检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有长结构化文档检索方法未有效利用结构特征和元素级语义，且缺乏含结构元数据的数据集。

Method: 提出SEAL对比学习框架，利用结构感知学习和掩码元素对齐；发布SEALDR数据集。

Result: 在多个现代预训练语言模型的发布和工业数据集上实验及在线A/B测试均有性能提升，如BGE - M3的NDCG@10从73.96%提升到77.84%。

Conclusion: 所提方法和数据集能有效提升长结构化文档检索性能，相关资源已开源。

Abstract: In long structured document retrieval, existing methods typically fine-tune
pre-trained language models (PLMs) using contrastive learning on datasets
lacking explicit structural information. This practice suffers from two
critical issues: 1) current methods fail to leverage structural features and
element-level semantics effectively, and 2) the lack of datasets containing
structural metadata. To bridge these gaps, we propose \our, a novel contrastive
learning framework. It leverages structure-aware learning to preserve semantic
hierarchies and masked element alignment for fine-grained semantic
discrimination. Furthermore, we release \dataset, a long structured document
retrieval dataset with rich structural annotations. Extensive experiments on
both released and industrial datasets across various modern PLMs, along with
online A/B testing, demonstrate consistent performance improvements, boosting
NDCG@10 from 73.96\% to 77.84\% on BGE-M3. The resources are available at
https://github.com/xinhaoH/SEAL.

</details>


### [56] [Addressing Personalized Bias for Unbiased Learning to Rank](https://arxiv.org/abs/2508.20798)
*Zechun Niu,Lang Mei,Liu Yang,Ziyuan Zhao,Qiang Yan,Jiaxin Mao,Ji-Rong Wen*

Main category: cs.IR

TL;DR: 本文引入个性化因素到无偏学习排序框架，提出用户感知的无偏学习排序问题及新的逆倾向得分估计器，理论证明其无偏性和低方差，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 过往无偏学习排序研究常假设行为日志来自‘平均’用户，忽略不同用户搜索和浏览行为差异，本文旨在解决该个性化偏差问题。

Method: 对问题进行形式化因果分析，提出用户感知的逆倾向得分估计器，为每个查询建模用户浏览行为分布，聚合用户加权检查概率确定倾向。

Result: 理论证明用户感知估计器在温和假设下无偏，且比直接计算每个展示的用户依赖倾向有更低方差；在两个半合成数据集和一个真实数据集上实验验证其有效性。

Conclusion: 提出的用户感知逆倾向得分估计器能有效解决无偏学习排序中的个性化偏差问题。

Abstract: Unbiased learning to rank (ULTR), which aims to learn unbiased ranking models
from biased user behavior logs, plays an important role in Web search. Previous
research on ULTR has studied a variety of biases in users' clicks, such as
position bias, presentation bias, and outlier bias. However, existing work
often assumes that the behavior logs are collected from an ``average'' user,
neglecting the differences between different users in their search and browsing
behaviors. In this paper, we introduce personalized factors into the ULTR
framework, which we term the user-aware ULTR problem. Through a formal causal
analysis of this problem, we demonstrate that existing user-oblivious methods
are biased when different users have different preferences over queries and
personalized propensities of examining documents. To address such a
personalized bias, we propose a novel user-aware inverse-propensity-score
estimator for learning-to-rank objectives. Specifically, our approach models
the distribution of user browsing behaviors for each query and aggregates
user-weighted examination probabilities to determine propensities. We
theoretically prove that the user-aware estimator is unbiased under some mild
assumptions and shows lower variance compared to the straightforward way of
calculating a user-dependent propensity for each impression. Finally, we
empirically verify the effectiveness of our user-aware estimator by conducting
extensive experiments on two semi-synthetic datasets and a real-world dataset.

</details>


### [57] [Deep Multiple Quantization Network on Long Behavior Sequence for Click-Through Rate Prediction](https://arxiv.org/abs/2508.20865)
*Zhuoxing Wei,Qi Liu,Qingchen Xie*

Main category: cs.IR

TL;DR: 提出 Deep Multiple Quantization Network (DMQN) 处理 CTR 预测中的长行为序列，实验验证其有效性和效率，A/B 测试显示提升 CTR 和 RPM。


<details>
  <summary>Details</summary>
Motivation: 现有方法中目标注意力在检索项和完整长行为序列间相关性分布差异导致性能下降，需解决该问题。

Method: 提出 DMQN 端到端处理长行为序列，包括基于多个独立码本量化序列、用 Hierarchical Sequential Transduction Unit 促进交互、计算候选与码本序列注意力输出兴趣向量，缓存中间表示降低延迟。

Result: 在工业和公共数据集上实验验证有效性和效率，A/B 测试中 DMQN 使 CTR 提升 3.5%，RPM 提升 2.0%。

Conclusion: DMQN 能有效处理长行为序列，在 CTR 预测中提升性能。

Abstract: In Click-Through Rate (CTR) prediction, the long behavior sequence,
comprising the user's long period of historical interactions with items has a
vital influence on assessing the user's interest in the candidate item.
Existing approaches strike efficiency and effectiveness through a two-stage
paradigm: first retrieving hundreds of candidate-related items and then
extracting interest intensity vector through target attention. However, we
argue that the discrepancy in target attention's relevance distribution between
the retrieved items and the full long behavior sequence inevitably leads to a
performance decline. To alleviate the discrepancy, we propose the Deep Multiple
Quantization Network (DMQN) to process long behavior sequence end-to-end
through compressing the long behavior sequence. Firstly, the entire spectrum of
long behavior sequence will be quantized into multiple codeword sequences based
on multiple independent codebooks. Hierarchical Sequential Transduction Unit is
incorporated to facilitate the interaction of reduced codeword sequences. Then,
attention between the candidate and multiple codeword sequences will output the
interest vector. To enable online serving, intermediate representations of the
codeword sequences are cached, significantly reducing latency. Our extensive
experiments on both industrial and public datasets confirm the effectiveness
and efficiency of DMQN. The A/B test in our advertising system shows that DMQN
improves CTR by 3.5% and RPM by 2.0%.

</details>


### [58] [OneRec-V2 Technical Report](https://arxiv.org/abs/2508.20900)
*Guorui Zhou,Hengrui Hu,Hongtao Cheng,Huanjie Wang,Jiaxin Deng,Jinghao Zhang,Kuo Cai,Lejian Ren,Lu Ren,Liao Yu,Pengfei Zheng,Qiang Luo,Qianqian Wang,Qigen Hu,Rui Huang,Ruiming Tang,Shiyao Wang,Shujie Yang,Tao Wu,Wuchao Li,Xinchen Luo,Xingmei Wang,Yi Su,Yunfan Wu,Zexuan Cheng,Zhanyu Liu,Zixing Zhang,Bin Zhang,Boxuan Wang,Chaoyi Ma,Chengru Song,Chenhui Wang,Chenglong Chu,Di Wang,Dongxue Meng,Dunju Zang,Fan Yang,Fangyu Zhang,Feng Jiang,Fuxing Zhang,Gang Wang,Guowang Zhang,Han Li,Honghui Bao,Hongyang Cao,Jiaming Huang,Jiapeng Chen,Jiaqiang Liu,Jinghui Jia,Kun Gai,Lantao Hu,Liang Zeng,Qiang Wang,Qidong Zhou,Rongzhou Zhang,Shengzhe Wang,Shihui He,Shuang Yang,Siyang Mao,Sui Huang,Tiantian He,Tingting Gao,Wei Yuan,Xiao Liang,Xiaoxiao Xu,Xugang Liu,Yan Wang,Yang Zhou,Yi Wang,Yiwu Liu,Yue Song,Yufei Zhang,Yunfeng Zhao,Zhixin Ling,Ziming Li*

Main category: cs.IR

TL;DR: OneRec-V1有计算分配低效和强化学习局限问题，本文提出OneRec-V2，通过新架构和偏好对齐方法提升性能，经A/B测试有效。


<details>
  <summary>Details</summary>
Motivation: 解决OneRec-V1在可扩展性和性能方面的两个关键挑战，即计算分配低效和强化学习仅依赖奖励模型的局限。

Method: 提出OneRec-V2，采用Lazy Decoder - Only Architecture消除编码器瓶颈，引入Duration - Aware Reward Shaping和Adaptive Ratio Clipping实现偏好对齐。

Result: 在快手的A/B测试中，OneRec-V2提高App停留时间0.467%/0.741%，并平衡多目标推荐。

Conclusion: 该工作推进了生成式推荐的可扩展性和与现实反馈的对齐，是端到端推荐系统发展的进步。

Abstract: Recent breakthroughs in generative AI have transformed recommender systems
through end-to-end generation. OneRec reformulates recommendation as an
autoregressive generation task, achieving high Model FLOPs Utilization. While
OneRec-V1 has shown significant empirical success in real-world deployment, two
critical challenges hinder its scalability and performance: (1) inefficient
computational allocation where 97.66% of resources are consumed by sequence
encoding rather than generation, and (2) limitations in reinforcement learning
relying solely on reward models.
  To address these challenges, we propose OneRec-V2, featuring: (1) Lazy
Decoder-Only Architecture: Eliminates encoder bottlenecks, reducing total
computation by 94% and training resources by 90%, enabling successful scaling
to 8B parameters. (2) Preference Alignment with Real-World User Interactions:
Incorporates Duration-Aware Reward Shaping and Adaptive Ratio Clipping to
better align with user preferences using real-world feedback.
  Extensive A/B tests on Kuaishou demonstrate OneRec-V2's effectiveness,
improving App Stay Time by 0.467%/0.741% while balancing multi-objective
recommendations. This work advances generative recommendation scalability and
alignment with real-world feedback, representing a step forward in the
development of end-to-end recommender systems.

</details>


### [59] [Efficient Large-Scale Cross-Domain Sequential Recommendation with Dynamic State Representations](https://arxiv.org/abs/2508.20945)
*Manuel V. Loureiro,Steven Derby,Aleksei Medvedev,Alejandro Ariza-Casabona,Gonzalo Fiz Pontiveros,Tri Kurniawan Wijaya*

Main category: cs.IR

TL;DR: 本文提出针对多领域推荐系统的新方法，用TAPE和DDSR机制解决计算瓶颈，在检索任务有显著提升。


<details>
  <summary>Details</summary>
Motivation: 自回归推荐模型在多领域场景下，transformer架构的注意力图成计算瓶颈，需平衡域间和域内知识转移。

Method: 用Transition - Aware Positional Embeddings (TAPE)关注域内项目减少计算成本，引入Dynamic Domain State Representation (DDSR)实现相关领域信息高效转移。

Result: 该方法为大规模多领域推荐系统挑战提供可扩展解决方案，在检索任务有显著改进。

Conclusion: 通过分别建模和组合域间和域内表示，方法能有效应对多领域推荐系统挑战。

Abstract: Recently, autoregressive recommendation models (ARMs), such as Meta's HSTU
model, have emerged as a major breakthrough over traditional Deep Learning
Recommendation Models (DLRMs), exhibiting the highly sought-after scaling law
behaviour. However, when applied to multi-domain scenarios, the transformer
architecture's attention maps become a computational bottleneck, as they attend
to all items across every domain. To tackle this challenge, systems must
efficiently balance inter and intra-domain knowledge transfer. In this work, we
introduce a novel approach for scalable multi-domain recommendation systems by
replacing full inter-domain attention with two innovative mechanisms: 1)
Transition-Aware Positional Embeddings (TAPE): We propose novel positional
embeddings that account for domain-transition specific information. This allows
attention to be focused solely on intra-domain items, effectively reducing the
unnecessary computational cost associated with attending to irrelevant domains.
2) Dynamic Domain State Representation (DDSR): We introduce a dynamic state
representation for each domain, which is stored and accessed during subsequent
token predictions. This enables the efficient transfer of relevant domain
information without relying on full attention maps. Our method offers a
scalable solution to the challenges posed by large-scale, multi-domain
recommendation systems and demonstrates significant improvements in retrieval
tasks by separately modelling and combining inter- and intra-domain
representations.

</details>


### [60] [On the Theoretical Limitations of Embedding-Based Retrieval](https://arxiv.org/abs/2508.21038)
*Orion Weller,Michael Boratko,Iftekhar Naim,Jinhyuk Lee*

Main category: cs.IR

TL;DR: 研究表明向量嵌入模型在现有单向量范式下有理论局限性，即便简单查询也会遇到，创建LIMIT数据集测试，发现模型失败，呼吁解决该局限。


<details>
  <summary>Details</summary>
Motivation: 以往认为向量嵌入的理论局限仅由不现实查询导致，作者想证明在现实简单查询中也会遇到。

Method: 结合学习理论结果，证明嵌入维度限制查询结果中前k个子集数量，实证k=2情况，创建LIMIT数据集测试模型。

Result: 即便简单任务，最先进模型在LIMIT数据集上也失败。

Conclusion: 现有单向量范式下嵌入模型有局限，需未来研究解决。

Abstract: Vector embeddings have been tasked with an ever-increasing set of retrieval
tasks over the years, with a nascent rise in using them for reasoning,
instruction-following, coding, and more. These new benchmarks push embeddings
to work for any query and any notion of relevance that could be given. While
prior works have pointed out theoretical limitations of vector embeddings,
there is a common assumption that these difficulties are exclusively due to
unrealistic queries, and those that are not can be overcome with better
training data and larger models. In this work, we demonstrate that we may
encounter these theoretical limitations in realistic settings with extremely
simple queries. We connect known results in learning theory, showing that the
number of top-k subsets of documents capable of being returned as the result of
some query is limited by the dimension of the embedding. We empirically show
that this holds true even if we restrict to k=2, and directly optimize on the
test set with free parameterized embeddings. We then create a realistic dataset
called LIMIT that stress tests models based on these theoretical results, and
observe that even state-of-the-art models fail on this dataset despite the
simple nature of the task. Our work shows the limits of embedding models under
the existing single vector paradigm and calls for future research to develop
methods that can resolve this fundamental limitation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [61] [CrystalICL: Enabling In-Context Learning for Crystal Generation](https://arxiv.org/abs/2508.20143)
*Ruobing Wang,Qiaoyu Tan,Yili Wang,Ying Wang,Xin Wang*

Main category: cs.LG

TL;DR: 提出CrystalICL用于少样本晶体生成，实验显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的晶体生成方法局限于零样本场景，无法利用少样本场景优势，而人类专家设计新材料方式与少样本情境学习范式相符。

Method: 引入基于空间群的晶体标记化方法，降低大语言模型中晶体对称性建模复杂度；提出条件 - 结构感知混合指令调优框架和多任务指令调优策略。

Result: 在四个晶体生成基准测试中，CrystalICL在条件和无条件生成任务上优于领先基线方法。

Conclusion: CrystalICL在少样本晶体生成方面表现出色，能有效利用有限数据中的结构 - 属性关系。

Abstract: Designing crystal materials with desired physicochemical properties remains a
fundamental challenge in materials science. While large language models (LLMs)
have demonstrated strong in-context learning (ICL) capabilities, existing
LLM-based crystal generation approaches are limited to zero-shot scenarios and
are unable to benefit from few-shot scenarios. In contrast, human experts
typically design new materials by modifying relevant known structures which
aligns closely with the few-shot ICL paradigm. Motivated by this, we propose
CrystalICL, a novel model designed for few-shot crystal generation.
Specifically, we introduce a space-group based crystal tokenization method,
which effectively reduces the complexity of modeling crystal symmetry in LLMs.
We further introduce a condition-structure aware hybrid instruction tuning
framework and a multi-task instruction tuning strategy, enabling the model to
better exploit ICL by capturing structure-property relationships from limited
data. Extensive experiments on four crystal generation benchmarks demonstrate
the superiority of CrystalICL over the leading baseline methods on conditional
and unconditional generation tasks.

</details>


### [62] [Filter then Attend: Improving attention-based Time Series Forecasting with Spectral Filtering](https://arxiv.org/abs/2508.20206)
*Elisha Dayag,Nhat Thanh Van Tran,Jack Xin*

Main category: cs.LG

TL;DR: 在Transformer模型前添加可学习滤波器，提升长时序预测性能，模型更小且更有效。


<details>
  <summary>Details</summary>
Motivation: Transformer模型用于长时序预测时存在低频偏差、计算和内存需求高的问题，现有基于频率滤波器的工作未解决这些问题。

Method: 在几种Transformer模型前添加可学习滤波器，减少模型嵌入维度，并进行合成实验。

Result: 增加约1000个参数，预测性能相对提升5 - 10%，能降低模型嵌入维度。

Conclusion: 在Transformer模型前添加滤波器可提升长时序预测性能，使模型更小更有效。

Abstract: Transformer-based models are at the forefront in long time-series forecasting
(LTSF). While in many cases, these models are able to achieve state of the art
results, they suffer from a bias toward low-frequencies in the data and high
computational and memory requirements. Recent work has established that
learnable frequency filters can be an integral part of a deep forecasting model
by enhancing the model's spectral utilization. These works choose to use a
multilayer perceptron to process their filtered signals and thus do not solve
the issues found with transformer-based models. In this paper, we establish
that adding a filter to the beginning of transformer-based models enhances
their performance in long time-series forecasting. We add learnable filters,
which only add an additional $\approx 1000$ parameters to several
transformer-based models and observe in multiple instances 5-10 \% relative
improvement in forecasting performance. Additionally, we find that with filters
added, we are able to decrease the embedding dimension of our models, resulting
in transformer-based architectures that are both smaller and more effective
than their non-filtering base models. We also conduct synthetic experiments to
analyze how the filters enable Transformer-based models to better utilize the
full spectrum for forecasting.

</details>


### [63] [What can we learn from signals and systems in a transformer? Insights for probabilistic modeling and inference architecture](https://arxiv.org/abs/2508.20211)
*Heng-Sheng Chang,Prashant G. Mehta*

Main category: cs.LG

TL;DR: 本文提出概率模型解读transformer信号和层操作，在特殊情况给出定点更新显式形式，尝试连接经典非线性滤波理论和现代推理架构。


<details>
  <summary>Details</summary>
Motivation: 将经典非线性滤波理论与现代推理架构相连接。

Method: 提出一个概率模型，将transformer信号解释为条件测度的替代物，层操作解释为定点更新，并针对概率模型为隐马尔可夫模型的特殊情况描述定点更新的显式形式。

Result: 描述了概率模型为隐马尔可夫模型时定点更新的显式形式。

Conclusion: 通过概率模型建立了经典非线性滤波理论与现代推理架构之间的联系。

Abstract: In the 1940s, Wiener introduced a linear predictor, where the future
prediction is computed by linearly combining the past data. A transformer
generalizes this idea: it is a nonlinear predictor where the next-token
prediction is computed by nonlinearly combining the past tokens. In this essay,
we present a probabilistic model that interprets transformer signals as
surrogates of conditional measures, and layer operations as fixed-point
updates. An explicit form of the fixed-point update is described for the
special case when the probabilistic model is a hidden Markov model (HMM). In
part, this paper is in an attempt to bridge the classical nonlinear filtering
theory with modern inference architectures.

</details>


### [64] [The Role of Teacher Calibration in Knowledge Distillation](https://arxiv.org/abs/2508.20224)
*Suyoung Kim,Seonguk Park,Junhoo Lee,Nojun Kwak*

Main category: cs.LG

TL;DR: 本文揭示教师模型校准误差与学生模型准确性的强关联，提出校准教师模型可提升知识蒸馏性能，算法通用且能与现有方法结合取得更好效果。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚哪些因素有助于提高知识蒸馏中学生模型的性能，需探究相关因素。

Method: 揭示教师模型校准误差和学生模型准确性的关联，采用减少教师模型校准误差的校准方法。

Result: 算法在从分类到检测等各种任务中均有效，且可与现有先进方法轻松集成，始终取得更优性能。

Conclusion: 教师模型的校准是有效进行知识蒸馏的重要因素，减少教师校准误差可提升知识蒸馏性能。

Abstract: Knowledge Distillation (KD) has emerged as an effective model compression
technique in deep learning, enabling the transfer of knowledge from a large
teacher model to a compact student model. While KD has demonstrated significant
success, it is not yet fully understood which factors contribute to improving
the student's performance. In this paper, we reveal a strong correlation
between the teacher's calibration error and the student's accuracy. Therefore,
we claim that the calibration of the teacher model is an important factor for
effective KD. Furthermore, we demonstrate that the performance of KD can be
improved by simply employing a calibration method that reduces the teacher's
calibration error. Our algorithm is versatile, demonstrating effectiveness
across various tasks from classification to detection. Moreover, it can be
easily integrated with existing state-of-the-art methods, consistently
achieving superior performance.

</details>


### [65] [Coresets from Trajectories: Selecting Data via Correlation of Loss Differences](https://arxiv.org/abs/2508.20230)
*Manish Nagaraj,Deepak Ravikumar,Kaushik Roy*

Main category: cs.LG

TL;DR: 提出用于核心集选择的简单可扩展指标CLD，解决深度学习模型在实时或资源受限场景的可扩展性问题，实验表现良好，是用于可扩展数据集优化的工具。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在实时或资源受限场景面临可扩展性挑战，需更好的核心集选择方法。

Method: 提出CLD指标，通过衡量训练样本与验证集损失轨迹的对齐度来识别最有影响力的样本，开发理论框架建立收敛保证。

Result: 在CIFAR - 100和ImageNet - 1k上，CLD核心集通常表现优于或接近现有方法，跨架构转移有效，使用早期检查点稳定，有内在的偏差减少能力。

Conclusion: CLD是一种有原则、高效、稳定且可转移的可扩展数据集优化工具。

Abstract: Deep learning models achieve state-of-the-art performance across domains but
face scalability challenges in real-time or resource-constrained scenarios. To
address this, we propose Correlation of Loss Differences (CLD), a simple and
scalable metric for coreset selection that identifies the most impactful
training samples by measuring their alignment with the loss trajectories of a
held-out validation set. CLD is highly efficient, requiring only per-sample
loss values computed at training checkpoints, and avoiding the costly gradient
and curvature computations used in many existing subset selection methods. We
develop a general theoretical framework that establishes convergence guarantees
for CLD-based coresets, demonstrating that the convergence error is
upper-bounded by the alignment of the selected samples and the
representativeness of the validation set. On CIFAR-100 and ImageNet-1k,
CLD-based coresets typically outperform or closely match state-of-the-art
methods across subset sizes, and remain within 1% of more computationally
expensive baselines even when not leading. CLD transfers effectively across
architectures (ResNet, VGG, DenseNet), enabling proxy-to-target selection with
<1% degradation. Moreover, CLD is stable when using only early checkpoints,
incurring negligible accuracy loss. Finally, CLD exhibits inherent bias
reduction via per-class validation alignment, obviating the need for additional
stratified sampling. Together, these properties make CLD a principled,
efficient, stable, and transferable tool for scalable dataset optimization.

</details>


### [66] [Discovering equations from data: symbolic regression in dynamical systems](https://arxiv.org/abs/2508.20257)
*Beatriz R. Brum,Luiza Lober,Isolde Previdelli,Francisco A. Rodrigues*

Main category: cs.LG

TL;DR: 本文对比五种符号回归方法从九个动力学过程恢复方程，发现PySR方法最适合推断方程，展示了符号回归在推断和建模现实现象的潜力。


<details>
  <summary>Details</summary>
Motivation: 文献中有多种符号回归方法，需要对它们进行比较，特别是针对描述复杂现象的动态系统。

Method: 使用五种符号回归方法从九个动力学过程（包括混沌动力学和流行病模型）中恢复方程。

Result: PySR方法最适合推断方程，基准结果显示其具有高预测能力和准确性，一些估计与原始解析形式难以区分。

Conclusion: 符号回归是推断和建模现实世界现象的有力工具。

Abstract: The process of discovering equations from data lies at the heart of physics
and in many other areas of research, including mathematical ecology and
epidemiology. Recently, machine learning methods known as symbolic regression
have automated this process. As several methods are available in the
literature, it is important to compare them, particularly for dynamic systems
that describe complex phenomena. In this paper, five symbolic regression
methods were used for recovering equations from nine dynamical processes,
including chaotic dynamics and epidemic models, with the PySR method proving to
be the most suitable for inferring equations. Benchmark results demonstrate its
high predictive power and accuracy, with some estimates being indistinguishable
from the original analytical forms. These results highlight the potential of
symbolic regression as a robust tool for inferring and modelling real-world
phenomena.

</details>


### [67] [Bounds on Perfect Node Classification: A Convex Graph Clustering Perspective](https://arxiv.org/abs/2508.20231)
*Firooz Shahriari-Mehr,Javad Aliakbari,Alexandre Graell i Amat,Ashkan Panahi*

Main category: cs.LG

TL;DR: 本文分析直推式节点分类问题，提出新优化问题，展示图结构与节点信息协同作用并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决直推式节点分类问题，利用图中社区与节点标签和特征一致的特性。

Method: 在谱图聚类框架中纳入节点特定信息（标签和特征），提出新的优化问题。

Result: 合适的节点特定信息能使优化问题的解在更宽松条件下完美恢复社区，算法和实验证实了协同作用。

Conclusion: 图结构和节点特定信息存在协同作用，新方法在节点分类上有优势。

Abstract: We present an analysis of the transductive node classification problem, where
the underlying graph consists of communities that agree with the node labels
and node features. For node classification, we propose a novel optimization
problem that incorporates the node-specific information (labels and features)
in a spectral graph clustering framework. Studying this problem, we demonstrate
a synergy between the graph structure and node-specific information. In
particular, we show that suitable node-specific information guarantees the
solution of our optimization problem perfectly recovering the communities,
under milder conditions than the bounds on graph clustering alone. We present
algorithmic solutions to our optimization problem and numerical experiments
that confirm such a synergy.

</details>


### [68] [Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs](https://arxiv.org/abs/2508.20333)
*Md Abdullah Al Mamun,Ihsen Alouani,Nael Abu-Ghazaleh*

Main category: cs.LG

TL;DR: 本文提出针对大语言模型对齐机制的中毒攻击SAI，能植入偏差、实施定向审查，且能规避现有防御，还展示了其攻击危害。


<details>
  <summary>Details</summary>
Motivation: 展现攻击者如何利用大语言模型的对齐机制植入偏差或实施定向审查。

Method: 提出Subversive Alignment Injection (SAI)中毒攻击，利用对齐机制触发对特定主题或查询的拒绝。

Result: SAI能规避现有中毒防御。在ChatDoctor等应用中，1%数据中毒会导致高偏差。

Conclusion: SAI这种攻击对大语言模型驱动的应用管道存在实际危害。

Abstract: Large Language Models (LLMs) are aligned to meet ethical standards and safety
requirements by training them to refuse answering harmful or unsafe prompts. In
this paper, we demonstrate how adversaries can exploit LLMs' alignment to
implant bias, or enforce targeted censorship without degrading the model's
responsiveness to unrelated topics. Specifically, we propose Subversive
Alignment Injection (SAI), a poisoning attack that leverages the alignment
mechanism to trigger refusal on specific topics or queries predefined by the
adversary. Although it is perhaps not surprising that refusal can be induced
through overalignment, we demonstrate how this refusal can be exploited to
inject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning
defenses including LLM state forensics, as well as robust aggregation
techniques that are designed to detect poisoning in FL settings. We demonstrate
the practical dangers of this attack by illustrating its end-to-end impacts on
LLM-powered application pipelines. For chat based applications such as
ChatDoctor, with 1% data poisoning, the system refuses to answer healthcare
questions to targeted racial category leading to high bias ($\Delta DP$ of
23%). We also show that bias can be induced in other NLP tasks: for a resume
selection pipeline aligned to refuse to summarize CVs from a selected
university, high bias in selection ($\Delta DP$ of 27%) results. Even higher
bias ($\Delta DP$~38%) results on 9 other chat based downstream applications.

</details>


### [69] [Beyond Optimization: Exploring Novelty Discovery in Autonomous Experiments](https://arxiv.org/abs/2508.20254)
*Ralph Bulanadi,Jawad Chowdhury,Funakubo Hiroshi,Maxim Ziatdinov,Rama Vasudevan,Arpan Biswas,Yongtao Liu*

Main category: cs.LG

TL;DR: 介绍INS2ANE框架增强自主实验中新颖现象发现，经验证可提升现象探索多样性，加速科研。


<details>
  <summary>Details</summary>
Motivation: 当前自主实验主要关注预定义目标优化，限制未知物理现象发现，需新方法增强新颖现象发现。

Method: 引入INS2ANE框架，集成新颖性评分系统和战略采样机制。

Result: 在预获取数据集和自主扫描探针显微镜实验中验证，INS2ANE比传统优化程序显著增加探索现象的多样性。

Conclusion: 该方法展示自主实验增强科学发现深度的潜力，结合其效率可加速科研。

Abstract: Autonomous experiments (AEs) are transforming how scientific research is
conducted by integrating artificial intelligence with automated experimental
platforms. Current AEs primarily focus on the optimization of a predefined
target; while accelerating this goal, such an approach limits the discovery of
unexpected or unknown physical phenomena. Here, we introduce a novel framework,
INS2ANE (Integrated Novelty Score-Strategic Autonomous Non-Smooth Exploration),
to enhance the discovery of novel phenomena in autonomous experimentation. Our
method integrates two key components: (1) a novelty scoring system that
evaluates the uniqueness of experimental results, and (2) a strategic sampling
mechanism that promotes exploration of under-sampled regions even if they
appear less promising by conventional criteria. We validate this approach on a
pre-acquired dataset with a known ground truth comprising of image-spectral
pairs. We further implement the process on autonomous scanning probe microscopy
experiments. INS2ANE significantly increases the diversity of explored
phenomena in comparison to conventional optimization routines, enhancing the
likelihood of discovering previously unobserved phenomena. These results
demonstrate the potential for AE to enhance the depth of scientific discovery;
in combination with the efficiency provided by AEs, this approach promises to
accelerate scientific research by simultaneously navigating complex
experimental spaces to uncover new phenomena.

</details>


### [70] [Unbiased Stochastic Optimization for Gaussian Processes on Finite Dimensional RKHS](https://arxiv.org/abs/2508.20588)
*Neta Shoham,Haim Avron*

Main category: cs.LG

TL;DR: 提出高斯过程精确随机推断算法，在内存资源受限场景表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有高斯过程随机超参数学习方法依赖近似，不能保证收敛到真实边缘似然的驻点

Method: 针对诱导适度有限维再生核希尔伯特空间（RKHS）核的高斯过程，提出精确随机推断算法，也可扩展到无限维RKHSs但牺牲精确性

Result: 在内存资源限制可行批量大小和诱导点数量的情况下，实验结果优于现有方法

Conclusion: 所提算法在特定场景下有效，比现有方法更具优势

Abstract: Current methods for stochastic hyperparameter learning in Gaussian Processes
(GPs) rely on approximations, such as computing biased stochastic gradients or
using inducing points in stochastic variational inference. However, when using
such methods we are not guaranteed to converge to a stationary point of the
true marginal likelihood. In this work, we propose algorithms for exact
stochastic inference of GPs with kernels that induce a Reproducing Kernel
Hilbert Space (RKHS) of moderate finite dimension. Our approach can also be
extended to infinite dimensional RKHSs at the cost of forgoing exactness. Both
for finite and infinite dimensional RKHSs, our method achieves better
experimental results than existing methods when memory resources limit the
feasible batch size and the possible number of inducing points.

</details>


### [71] [Dimension Agnostic Testing of Survey Data Credibility through the Lens of Regression](https://arxiv.org/abs/2508.20616)
*Debabrota Basu,Sourav Chakraborty,Debarshi Chanda,Buddha Dev Das,Arijit Ghosh,Arnab Ray*

Main category: cs.LG

TL;DR: 提出基于任务的方法评估抽样调查可信度，引入模型特定距离度量，设计算法，样本复杂度与数据维度无关，证明算法理论正确性并展示性能。


<details>
  <summary>Details</summary>
Motivation: 评估抽样调查是否能可信代表总体对下游研究有效性至关重要，传统估计高维分布距离样本数量需求大，而数据结论可能在不同分布下一致。

Method: 提出基于任务的方法，引入模型特定距离度量，设计算法验证回归模型下调查数据可信度。

Result: 算法样本复杂度与数据维度无关，若通过重构回归模型验证可信度，样本复杂度与数据维度线性相关。

Conclusion: 所提算法理论正确，性能良好。

Abstract: Assessing whether a sample survey credibly represents the population is a
critical question for ensuring the validity of downstream research. Generally,
this problem reduces to estimating the distance between two high-dimensional
distributions, which typically requires a number of samples that grows
exponentially with the dimension. However, depending on the model used for data
analysis, the conclusions drawn from the data may remain consistent across
different underlying distributions. In this context, we propose a task-based
approach to assess the credibility of sampled surveys. Specifically, we
introduce a model-specific distance metric to quantify this notion of
credibility. We also design an algorithm to verify the credibility of survey
data in the context of regression models. Notably, the sample complexity of our
algorithm is independent of the data dimension. This efficiency stems from the
fact that the algorithm focuses on verifying the credibility of the survey data
rather than reconstructing the underlying regression model. Furthermore, we
show that if one attempts to verify credibility by reconstructing the
regression model, the sample complexity scales linearly with the dimensionality
of the data. We prove the theoretical correctness of our algorithm and
numerically demonstrate our algorithm's performance.

</details>


### [72] [A Hybrid Stochastic Gradient Tracking Method for Distributed Online Optimization Over Time-Varying Directed Networks](https://arxiv.org/abs/2508.20645)
*Xinli Shi,Xingxing Yuan,Longkang Zhu,Guanghui Wen*

Main category: cs.LG

TL;DR: 本文针对时变有向网络提出TV - HSGT算法，结合混合随机梯度跟踪和方差缩减机制，理论和实验证明其在动态和资源受限环境有效。


<details>
  <summary>Details</summary>
Motivation: 现有分布式在线优化算法依赖有界梯度假设，忽略随机梯度影响，尤其是在时变有向网络中。

Method: 提出TV - HSGT算法，整合行随机和列随机通信方案，结合当前和递归随机梯度。

Result: 理论分析表明该算法无需梯度有界假设就能改善动态遗憾界，逻辑回归任务实验证实其在动态和资源受限环境有效。

Conclusion: TV - HSGT算法在动态和资源受限环境的分布式在线优化中具有有效性。

Abstract: With the increasing scale and dynamics of data, distributed online
optimization has become essential for real-time decision-making in various
applications. However, existing algorithms often rely on bounded gradient
assumptions and overlook the impact of stochastic gradients, especially in
time-varying directed networks. This study proposes a novel Time-Varying Hybrid
Stochastic Gradient Tracking algorithm named TV-HSGT, based on hybrid
stochastic gradient tracking and variance reduction mechanisms. Specifically,
TV-HSGT integrates row-stochastic and column-stochastic communication schemes
over time-varying digraphs, eliminating the need for Perron vector estimation
or out-degree information. By combining current and recursive stochastic
gradients, it effectively reduces gradient variance while accurately tracking
global descent directions. Theoretical analysis demonstrates that TV-HSGT can
achieve improved bounds on dynamic regret without assuming gradient
boundedness. Experimental results on logistic regression tasks confirm the
effectiveness of TV-HSGT in dynamic and resource-constrained environments.

</details>


### [73] [Latent Variable Modeling for Robust Causal Effect Estimation](https://arxiv.org/abs/2508.20259)
*Tetsuro Morimura,Tatsushi Oka,Yugo Suzuki,Daisuke Moriwaki*

Main category: cs.LG

TL;DR: 本文提出将潜变量建模集成到双重机器学习范式的新框架，用于在存在隐藏因素时进行稳健因果效应估计，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决因果推断中因隐藏因素（缺失或未测量的协变量）带来的挑战，实现稳健的因果效应估计。

Method: 提出新框架将潜变量建模集成到双重机器学习范式，考虑潜变量仅影响结果和同时影响处理与结果两种场景，仅在双重机器学习第二阶段引入潜变量。

Result: 通过对合成和真实数据集的大量实验，证明了方法的稳健性和有效性。

Conclusion: 所提出的集成潜变量建模与双重机器学习的框架能有效应对隐藏因素，实现稳健的因果效应估计。

Abstract: Latent variable models provide a powerful framework for incorporating and
inferring unobserved factors in observational data. In causal inference, they
help account for hidden factors influencing treatment or outcome, thereby
addressing challenges posed by missing or unmeasured covariates. This paper
proposes a new framework that integrates latent variable modeling into the
double machine learning (DML) paradigm to enable robust causal effect
estimation in the presence of such hidden factors. We consider two scenarios:
one where a latent variable affects only the outcome, and another where it may
influence both treatment and outcome. To ensure tractability, we incorporate
latent variables only in the second stage of DML, separating representation
learning from latent inference. We demonstrate the robustness and effectiveness
of our method through extensive experiments on both synthetic and real-world
datasets.

</details>


### [74] [Supervised Stochastic Gradient Algorithms for Multi-Trial Source Separation](https://arxiv.org/abs/2508.20618)
*Ronak Mehta,Mateus Piovezan Otto,Noah Stanis,Azadeh Yazdan-Shahmorad,Zaid Harchaoui*

Main category: cs.LG

TL;DR: 开发含多试验监督的独立成分分析随机算法，经实验验证有优化成功率和可解释性提升。


<details>
  <summary>Details</summary>
Motivation: 在许多科学场景下利用多试验监督开发独立成分分析算法。

Method: 将可逆矩阵空间中的近端梯度型算法与通过反向传播联合学习预测模型相结合。

Result: 在合成和真实数据实验中，非凸优化成功率增加，独立成分可解释性提高。

Conclusion: 提出的含多试验监督的随机算法有效。

Abstract: We develop a stochastic algorithm for independent component analysis that
incorporates multi-trial supervision, which is available in many scientific
contexts. The method blends a proximal gradient-type algorithm in the space of
invertible matrices with joint learning of a prediction model through
backpropagation. We illustrate the proposed algorithm on synthetic and real
data experiments. In particular, owing to the additional supervision, we
observe an increased success rate of the non-convex optimization and the
improved interpretability of the independent components.

</details>


### [75] [Generalizable AI Model for Indoor Temperature Forecasting Across Sub-Saharan Africa](https://arxiv.org/abs/2508.20260)
*Zainab Akhtar,Eunice Jengo,Björn Haßler*

Main category: cs.LG

TL;DR: 提出轻量级AI模型预测撒哈拉以南非洲自然通风建筑室内温度，跨国家表现良好，凸显AI在资源受限环境热舒适管理潜力。


<details>
  <summary>Details</summary>
Motivation: 为撒哈拉以南非洲自然通风的学校和家庭提供室内温度预测模型，解决资源受限环境下热舒适管理问题。

Method: 扩展Temp - AI - Estimator框架，用坦桑尼亚学校数据训练，在尼日利亚学校和冈比亚家庭评估。

Result: 模型仅用极少可获取输入实现稳健跨国家表现，尼日利亚学校平均绝对误差1.45°C，冈比亚家庭0.65°C。

Conclusion: AI在资源受限环境热舒适管理方面有潜力。

Abstract: This study presents a lightweight, domain-informed AI model for predicting
indoor temperatures in naturally ventilated schools and homes in Sub-Saharan
Africa. The model extends the Temp-AI-Estimator framework, trained on Tanzanian
school data, and evaluated on Nigerian schools and Gambian homes. It achieves
robust cross-country performance using only minimal accessible inputs, with
mean absolute errors of 1.45{\deg}C for Nigerian schools and 0.65{\deg}C for
Gambian homes. These findings highlight AI's potential for thermal comfort
management in resource-constrained environments.

</details>


### [76] [Provable Benefits of In-Tool Learning for Large Language Models](https://arxiv.org/abs/2508.20755)
*Sam Houliston,Ambroise Odonnat,Charles Arnal,Vivien Cabannes*

Main category: cs.LG

TL;DR: 本文探讨工具增强语言模型的理论优势，证明工具使用在事实回忆上优于权重记忆，且实验验证工具使用模型表现更好，还表明教工具使用和规则比微调记忆事实更有效。


<details>
  <summary>Details</summary>
Motivation: 当前工具增强语言模型的理论优势未被充分探索，本文旨在解决此问题。

Method: 对比工具内学习（外部检索）和权重内学习（记忆），构建简单高效电路证明工具使用可实现无限事实回忆，并进行受控实验验证。

Result: 工具使用模型在实验中始终优于记忆模型，教预训练大语言模型工具使用和规则比微调事实到记忆更有效。

Conclusion: 为工具增强工作流提供理论和实证基础，证明其不仅实用，且更具可扩展性。

Abstract: Tool-augmented language models, equipped with retrieval, memory, or external
APIs, are reshaping AI, yet their theoretical advantages remain underexplored.
In this paper, we address this question by demonstrating the benefits of
in-tool learning (external retrieval) over in-weight learning (memorization)
for factual recall. We show that the number of facts a model can memorize
solely in its weights is fundamentally limited by its parameter count. In
contrast, we prove that tool-use enables unbounded factual recall via a simple
and efficient circuit construction. These results are validated in controlled
experiments, where tool-using models consistently outperform memorizing ones.
We further show that for pretrained large language models, teaching tool-use
and general rules is more effective than finetuning facts into memory. Our work
provides both a theoretical and empirical foundation, establishing why
tool-augmented workflows are not just practical, but provably more scalable.

</details>


### [77] [A Systematic Review on the Generative AI Applications in Human Medical Genomics](https://arxiv.org/abs/2508.20275)
*Anton Changalidis,Yury Barbitoff,Yulia Nasykhova,Andrey Glotov*

Main category: cs.LG

TL;DR: 本文系统综述了大语言模型（LLMs）在遗传研究和疾病诊断中的应用，分析其成果与挑战。


<details>
  <summary>Details</summary>
Motivation: 传统统计和机器学习方法处理复杂高维遗传数据有困难，而基于transformer架构的LLMs在医学数据处理上表现出色，因此研究LLMs在遗传研究和诊断中的作用。

Method: 在多个数据库进行基于关键词的自动搜索，筛选相关研究并分析了172项。

Result: LLMs在基因组变异识别、注释、解释及医学影像分析等方面有显著进展，但在整合多模态数据、可推广性和临床应用方面存在挑战。

Conclusion: 该综述对LLMs在遗传病诊断和遗传教育中的能力和局限进行了全面分类和评估，为该领域发展提供指引。

Abstract: Although traditional statistical techniques and machine learning methods have
contributed significantly to genetics and, in particular, inherited disease
diagnosis, they often struggle with complex, high-dimensional data, a challenge
now addressed by state-of-the-art deep learning models. Large language models
(LLMs), based on transformer architectures, have excelled in tasks requiring
contextual comprehension of unstructured medical data. This systematic review
examines the role of LLMs in the genetic research and diagnostics of both rare
and common diseases. Automated keyword-based search in PubMed, bioRxiv,
medRxiv, and arXiv was conducted, targeting studies on LLM applications in
diagnostics and education within genetics and removing irrelevant or outdated
models. A total of 172 studies were analyzed, highlighting applications in
genomic variant identification, annotation, and interpretation, as well as
medical imaging advancements through vision transformers. Key findings indicate
that while transformer-based models significantly advance disease and risk
stratification, variant interpretation, medical imaging analysis, and report
generation, major challenges persist in integrating multimodal data (genomic
sequences, imaging, and clinical records) into unified and clinically robust
pipelines, facing limitations in generalizability and practical implementation
in clinical settings. This review provides a comprehensive classification and
assessment of the current capabilities and limitations of LLMs in transforming
hereditary disease diagnostics and supporting genetic education, serving as a
guide to navigate this rapidly evolving field.

</details>


### [78] [Fast Convergence Rates for Subsampled Natural Gradient Algorithms on Quadratic Model Problems](https://arxiv.org/abs/2508.21022)
*Gil Goldshlager,Jiang Hu,Lin Lin*

Main category: cs.LG

TL;DR: 本文分析了SNGD及其加速变体SPRING在理想参数优化问题中的收敛性，证明了其与正则化Kaczmarz方法的等价性，并得到收敛率和加速证明。


<details>
  <summary>Details</summary>
Motivation: SNGD在科学机器学习参数优化任务中效果好但缺乏理论解释，需进行收敛性分析。

Method: 分析SNGD和SPRING在模型线性、损失函数强凸二次的理想参数优化问题中的收敛性，证明与正则化Kaczmarz方法的等价性。

Result: 在最小二乘损失下得到SNGD的快速收敛率、SPRING的收敛保证和加速证明；在一般强凸二次损失下为SNGD有效性提供解释。

Conclusion: 随机线性代数工具能为子采样和曲率感知优化策略的相互作用提供新见解。

Abstract: Subsampled natural gradient descent (SNGD) has shown impressive results for
parametric optimization tasks in scientific machine learning, such as neural
network wavefunctions and physics-informed neural networks, but it has lacked a
theoretical explanation. We address this gap by analyzing the convergence of
SNGD and its accelerated variant, SPRING, for idealized parametric optimization
problems where the model is linear and the loss function is strongly convex and
quadratic. In the special case of a least-squares loss, namely the standard
linear least-squares problem, we prove that SNGD is equivalent to a regularized
Kaczmarz method while SPRING is equivalent to an accelerated regularized
Kaczmarz method. As a result, by leveraging existing analyses we obtain under
mild conditions (i) the first fast convergence rate for SNGD, (ii) the first
convergence guarantee for SPRING in any setting, and (iii) the first proof that
SPRING can accelerate SNGD. In the case of a general strongly convex quadratic
loss, we extend the analysis of the regularized Kaczmarz method to obtain a
fast convergence rate for SNGD under stronger conditions, providing the first
explanation for the effectiveness of SNGD outside of the least-squares setting.
Overall, our results illustrate how tools from randomized linear algebra can
shed new light on the interplay between subsampling and curvature-aware
optimization strategies.

</details>


### [79] [Objective Value Change and Shape-Based Accelerated Optimization for the Neural Network Approximation](https://arxiv.org/abs/2508.20290)
*Pengcheng Xie,Zihao Zhou,Zijian Zhou*

Main category: cs.LG

TL;DR: 本文引入VC指标衡量神经网络近似任务难度与效果，研究其性质并发现两种现象，提出基于VC的新度量和预处理框架，实验支持相关发现与加速方法。


<details>
  <summary>Details</summary>
Motivation: 神经网络局部性能不可预测，影响关键应用可靠性，需要量化指标衡量局部值变化。

Method: 引入VC指标，研究其理论性质，提出基于VC的新度量和预处理框架。

Result: 发现VC - 趋势和少数趋势，数值结果包括实际实验和PDE相关科学问题支持发现与预处理加速方法。

Conclusion: VC指标可有效衡量神经网络近似的局部性能和行为，新的度量和预处理框架有加速效果。

Abstract: This paper introduce a novel metric of an objective function f, we say VC
(value change) to measure the difficulty and approximation affection when
conducting an neural network approximation task, and it numerically supports
characterizing the local performance and behavior of neural network
approximation. Neural networks often suffer from unpredictable local
performance, which can hinder their reliability in critical applications. VC
addresses this issue by providing a quantifiable measure of local value changes
in network behavior, offering insights into the stability and performance for
achieving the neural-network approximation. We investigate some fundamental
theoretical properties of VC and identified two intriguing phenomena in neural
network approximation: the VC-tendency and the minority-tendency. These trends
respectively characterize how pointwise errors evolve in relation to the
distribution of VC during the approximation process.In addition, we propose a
novel metric based on VC, which measures the distance between two functions
from the perspective of variation. Building upon this metric, we further
propose a new preprocessing framework for neural network approximation.
Numerical results including the real-world experiment and the PDE-related
scientific problem support our discovery and pre-processing acceleration
method.

</details>


### [80] [Beacon: Post-Training Quantization with Integrated Grid Selection](https://arxiv.org/abs/2508.20293)
*Shihao Zhang,Rayan Saab*

Main category: cs.LG

TL;DR: 提出Beacon算法用于通道量化，无需手动调整即可确定最佳缩放因子，性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有通道后训练量化选择缩放因子需手动启发式调整或网格搜索，有局限性。

Method: Beacon算法直接使用固定非缩放字母表进行通道量化，利用对称标量量化几何自动确定最佳缩放因子，支持对称和非对称量化，不依赖反向传播和大校准集。

Result: Beacon算法与现有最先进方法相比有竞争力。

Conclusion: Beacon是高效模型部署的实用解决方案。

Abstract: Quantization is a widely used compression technique for reducing the memory
and computation costs of large pre-trained models. A key challenge in
per-channel post-training quantization (PTQ) is selecting appropriate scaling
factors to replace weight values with values from a scaled quantization grid.
Existing methods typically fix the scale at the outset via heuristic tuning or
grid search. In this note, we propose Beacon, a simple and effective algorithm
that eliminates the need for such manual tuning. Beacon performs per-channel
PTQ directly using a fixed non-scaled alphabet and automatically determines the
optimal scaling factors by exploiting the geometry of symmetric scalar
quantization. It supports both symmetric and asymmetric quantization with
minimal modifications and does not rely on back-propagation or large
calibration sets. Despite its simplicity and tuning-free nature, Beacon
achieves competitive performance compared to state-of-the-art methods, making
it a practical solution for efficient model deployment.

</details>


### [81] [Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization](https://arxiv.org/abs/2508.20294)
*Frank Röder,Jan Benad,Manfred Eppe,Pradeep Kr. Banerjee*

Main category: cs.LG

TL;DR: 提出DALI框架，能从智能体与环境交互中推断潜在上下文表征，在cMDP基准测试中表现出色，实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现实强化学习需适应未知环境条件且避免昂贵的重新训练，现有cMDP方法依赖显式上下文变量，在上下文隐藏或难以测量时受限。

Method: 在Dreamer架构中集成DALI框架，训练自监督编码器预测前向动态，生成可操作表征以调节世界模型和策略。

Result: 在具有挑战性的cMDP基准测试中，DALI相比无上下文感知的基线有显著提升，在推断任务中常超越有上下文感知的基线。

Conclusion: DALI能够实现对未知上下文变化的零样本泛化，编码器对高效上下文推断和鲁棒泛化至关重要。

Abstract: Real-world reinforcement learning demands adaptation to unseen environmental
conditions without costly retraining. Contextual Markov Decision Processes
(cMDP) model this challenge, but existing methods often require explicit
context variables (e.g., friction, gravity), limiting their use when contexts
are latent or hard to measure. We introduce Dynamics-Aligned Latent Imagination
(DALI), a framework integrated within the Dreamer architecture that infers
latent context representations from agent-environment interactions. By training
a self-supervised encoder to predict forward dynamics, DALI generates
actionable representations conditioning the world model and policy, bridging
perception and control. We theoretically prove this encoder is essential for
efficient context inference and robust generalization. DALI's latent space
enables counterfactual consistency: Perturbing a gravity-encoding dimension
alters imagined rollouts in physically plausible ways. On challenging cMDP
benchmarks, DALI achieves significant gains over context-unaware baselines,
often surpassing context-aware baselines in extrapolation tasks, enabling
zero-shot generalization to unseen contextual variations.

</details>


### [82] [FedReFT: Federated Representation Fine-Tuning with All-But-Me Aggregation](https://arxiv.org/abs/2508.20295)
*Fatema Siddika,Md Anwar Hossen,J. Pablo Muñoz,Tanya Roosta,Anuj Sharma,Ali Jannesari*

Main category: cs.LG

TL;DR: 提出Federated Representation Fine - Tuning (FedReFT)用于联邦学习，解决ReFT在FL中的挑战，还提出ABM聚合缓解问题，在多任务上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: ReFT在联邦学习应用中因客户端数据分布、模型容量和计算资源的异质性面临挑战，需解决这些问题。

Method: 引入FedReFT直接微调客户端隐藏表示，应用稀疏干预层；提出All - But - Me (ABM)聚合，让客户端接收其他客户端聚合更新并部分融合。

Result: 在常识推理、算术推理、指令调优和GLUE上评估，FedReFT始终优于联邦学习中现有PEFT方法，参数效率比领先的基于LoRA的方法高7 - 15倍。

Conclusion: FedReFT和ABM聚合是解决ReFT在联邦学习中挑战的有效方法，能实现稳定和个性化学习。

Abstract: Parameter-efficient fine-tuning (PEFT) has attracted significant attention
for adapting large pre-trained models by modifying a small subset of
parameters. Recently, Representation Fine-tuning (ReFT) has emerged as an
effective alternative. ReFT shifts the fine-tuning paradigm from updating model
weights to directly manipulating hidden representations that capture rich
semantic information, and performs better than state-of-the-art PEFTs in
standalone settings. However, its application in Federated Learning (FL)
remains challenging due to heterogeneity in clients' data distributions, model
capacities, and computational resources. To address these challenges, we
introduce Federated Representation Fine-Tuning (FedReFT), a novel approach to
fine-tune the client's hidden representation. FedReFT applies sparse
intervention layers to steer hidden representations directly, offering a
lightweight and semantically rich fine-tuning alternative ideal for edge
devices. However, representation-level updates are especially vulnerable to
aggregation mismatch under different task heterogeneity, where naive averaging
can corrupt semantic alignment. To mitigate this issue, we propose All-But-Me
(ABM) aggregation, where each client receives the aggregated updates of others
and partially incorporates them, enabling stable and personalized learning by
balancing local focus with global knowledge. We evaluate FedReFT on commonsense
reasoning, arithmetic reasoning, instruction-tuning, and GLUE, where it
consistently outperforms state-of-the-art PEFT methods in FL, achieving 7x-15x
higher parameter efficiency compared to leading LoRA-based approaches.

</details>


### [83] [Multi-Agent Reinforcement Learning in Intelligent Transportation Systems: A Comprehensive Survey](https://arxiv.org/abs/2508.20315)
*RexCharles Donatus,Kumater Ter,Ore-Ofe Ajayi,Daniel Udekwe*

Main category: cs.LG

TL;DR: 本文对多智能体强化学习（MARL）在智能交通系统（ITS）中的应用进行全面综述，介绍分类、应用、模拟平台，指出核心挑战。


<details>
  <summary>Details</summary>
Motivation: 城市交通日益复杂，需要高效、可持续和自适应解决方案，MARL 可应对 ITS 中自主决策挑战。

Method: 引入结构化分类法对 MARL 方法按协调模型和学习算法分类，回顾其在 ITS 关键领域应用，介绍常用模拟平台和新兴基准。

Result: 梳理了 MARL 在 ITS 各领域应用，介绍相关模拟平台和基准。

Conclusion: 指出 MARL 在 ITS 应用面临可扩展性、非平稳性等核心挑战，阻碍实际部署。

Abstract: The growing complexity of urban mobility and the demand for efficient,
sustainable, and adaptive solutions have positioned Intelligent Transportation
Systems (ITS) at the forefront of modern infrastructure innovation. At the core
of ITS lies the challenge of autonomous decision-making across dynamic, large
scale, and uncertain environments where multiple agents traffic signals,
autonomous vehicles, or fleet units must coordinate effectively. Multi Agent
Reinforcement Learning (MARL) offers a promising paradigm for addressing these
challenges by enabling distributed agents to jointly learn optimal strategies
that balance individual objectives with system wide efficiency. This paper
presents a comprehensive survey of MARL applications in ITS. We introduce a
structured taxonomy that categorizes MARL approaches according to coordination
models and learning algorithms, spanning value based, policy based, actor
critic, and communication enhanced frameworks. Applications are reviewed across
key ITS domains, including traffic signal control, connected and autonomous
vehicle coordination, logistics optimization, and mobility on demand systems.
Furthermore, we highlight widely used simulation platforms such as SUMO, CARLA,
and CityFlow that support MARL experimentation, along with emerging benchmarks.
The survey also identifies core challenges, including scalability, non
stationarity, credit assignment, communication constraints, and the sim to real
transfer gap, which continue to hinder real world deployment.

</details>


### [84] [Multi-View Graph Convolution Network for Internal Talent Recommendation Based on Enterprise Emails](https://arxiv.org/abs/2508.20328)
*Soo Hyun Kim,Jang-Hyun Kim*

Main category: cs.LG

TL;DR: 提出新框架从邮件数据建模员工岗位适配度，实验表明模型表现优且具高可解释性，为内部人才发现提供框架。


<details>
  <summary>Details</summary>
Motivation: 传统内部人才推荐方法有结构局限，易忽略合格候选人。

Method: 从邮件数据建模员工岗位适配的两个维度，用带门控机制的双图卷积网络自适应融合两个维度的独立图。

Result: 提出的基于门控的融合模型表现优于其他策略和基线，Hit@100达40.9%，且对不同岗位族有高可解释性。

Conclusion: 研究为内部人才发现提供定量综合框架，能确定任务和协作模式的最佳融合比例，有重要实践意义。

Abstract: Internal talent recommendation is a critical strategy for organizational
continuity, yet conventional approaches suffer from structural limitations,
often overlooking qualified candidates by relying on the narrow perspective of
a few managers. To address this challenge, we propose a novel framework that
models two distinct dimensions of an employee's position fit from email data:
WHAT they do (semantic similarity of tasks) and HOW they work (structural
characteristics of their interactions and collaborations). These dimensions are
represented as independent graphs and adaptively fused using a Dual Graph
Convolutional Network (GCN) with a gating mechanism. Experiments show that our
proposed gating-based fusion model significantly outperforms other fusion
strategies and a heuristic baseline, achieving a top performance of 40.9% on
Hit@100. Importantly, it is worth noting that the model demonstrates high
interpretability by learning distinct, context-aware fusion strategies for
different job families. For example, it learned to prioritize relational (HOW)
data for 'sales and marketing' job families while applying a balanced approach
for 'research' job families. This research offers a quantitative and
comprehensive framework for internal talent discovery, minimizing the risk of
candidate omission inherent in traditional methods. Its primary contribution
lies in its ability to empirically determine the optimal fusion ratio between
task alignment (WHAT) and collaborative patterns (HOW), which is required for
employees to succeed in the new positions, thereby offering important practical
implications.

</details>


### [85] [FORGE: Foundational Optimization Representations from Graph Embeddings](https://arxiv.org/abs/2508.20330)
*Zohair Shafi,Serdar Kadioglu*

Main category: cs.LG

TL;DR: 提出Forge方法对混合整数规划实例无监督预训练，评估有监督和无监督设置，结果良好并开源代码与权重。


<details>
  <summary>Details</summary>
Motivation: 基于学习的组合优化问题求解方法收集训练数据开销大，现有方法扩展性和泛化性差。

Method: 在大量多样的混合整数规划实例上无监督预训练向量量化图自编码器，用向量量化创建离散代码表示优化实例。

Result: 无监督设置中Forge嵌入能有效区分和聚类未见实例；有监督设置中单一模型可跨分布预测变量和积分间隙，助于提升求解器性能。

Conclusion: Forge方法有效，代码和预训练权重开源利于实例级MIP嵌入的研究和应用。

Abstract: Combinatorial optimization problems are ubiquitous in science and
engineering, yet learning-based approaches to accelerate their solution often
require solving a large number of hard-to-solve optimization instances to
collect training data, incurring significant computational overhead. Existing
methods require training dedicated models for each problem distribution for
each downstream task, severely limiting their scalability and generalization.
In this work, we introduce Forge, a method of pre-training a vector-quantized
graph autoencoder on a large and diverse collection of mixed-integer
programming (MIP) instances in an unsupervised fashion without dependency on
their solution. The vector quantization process creates discrete code
assignments that act as a vocabulary to represent optimization instances. We
evaluate our approach under both supervised and unsupervised settings. For the
unsupervised setting, we demonstrate that Forge embeddings effectively
differentiate and cluster unseen instances. For the supervised setting, we
fine-tune Forge embeddings and show that a single model predicts both the
variables for warm-starts and integrality gaps for cut-generation across
multiple problem type distributions. Both predictions help improve performance
of a state-of-the-art, commercial optimization solver. Finally, we release our
code and pre-trained Forge weights to encourage further research and practical
use of instance-level MIP embeddings at https://github.com/skadio/forge/

</details>


### [86] [Dynamic Synthetic Controls vs. Panel-Aware Double Machine Learning for Geo-Level Marketing Impact Estimation](https://arxiv.org/abs/2508.20335)
*Sang Su Lee,Vineeth Loganathan,Vijay Raghavan*

Main category: cs.LG

TL;DR: 本文构建模拟器评估七种估计器，发现ASC模型在复杂场景有严重偏差，而面板DML变体更稳健，提出“先诊断”框架。


<details>
  <summary>Details</summary>
Motivation: 准确量化双边市场地理层面营销提升具有挑战性，SCM常低估效应大小，面板DML很少与SCM对比。

Method: 构建开放、文档齐全的模拟器，对七种估计器进行五种风格化压力测试，每种场景进行100次重复实验。

Result: ASC模型在涉及非线性或外部冲击的复杂场景中存在严重偏差和近乎零的覆盖率，面板DML变体显著减少偏差并恢复名义95%置信区间覆盖率。

Conclusion: ASC在常见复杂情况下不可靠，建议采用“先诊断”框架，根据业务挑战选择合适的DML模型。

Abstract: Accurately quantifying geo-level marketing lift in two-sided marketplaces is
challenging: the Synthetic Control Method (SCM) often exhibits high power yet
systematically under-estimates effect size, while panel-style Double Machine
Learning (DML) is seldom benchmarked against SCM. We build an open, fully
documented simulator that mimics a typical large-scale geo roll-out: N_unit
regional markets are tracked for T_pre weeks before launch and for a further
T_post-week campaign window, allowing all key parameters to be varied by the
user and probe both families under five stylized stress tests: 1) curved
baseline trends, 2) heterogeneous response lags, 3) treated-biased shocks, 4) a
non-linear outcome link, and 5) a drifting control group trend.
  Seven estimators are evaluated: three standard Augmented SCM (ASC) variants
and four panel-DML flavors (TWFE, CRE/Mundlak, first-difference, and
within-group). Across 100 replications per scenario, ASC models consistently
demonstrate severe bias and near-zero coverage in challenging scenarios
involving nonlinearities or external shocks. By contrast, panel-DML variants
dramatically reduce this bias and restore nominal 95%-CI coverage, proving far
more robust.
  The results indicate that while ASC provides a simple baseline, it is
unreliable in common, complex situations. We therefore propose a
'diagnose-first' framework where practitioners first identify the primary
business challenge (e.g., nonlinear trends, response lags) and then select the
specific DML model best suited for that scenario, providing a more robust and
reliable blueprint for analyzing geo-experiments.

</details>


### [87] [Adaptive Segmentation of EEG for Machine Learning Applications](https://arxiv.org/abs/2508.20336)
*Johnson Zhou,Joseph West,Krista A. Ehinger,Zhenming Ren,Sam E. John,David B. Grayden*

Main category: cs.LG

TL;DR: 本文提出CTXSEG自适应分割方法处理EEG数据，经实验表明，该方法能提升癫痫检测性能，是固定长度分割的有前景替代方案。


<details>
  <summary>Details</summary>
Motivation: 当前EEG信号机器学习处理中使用的固定时间切片方法生物相关性有限，研究自适应分割方法对EEG机器学习分析是否有益。

Method: 引入CTXSEG自适应分割方法，基于EEG数据统计差异创建可变长度片段，并提出用于固定长度输入机器学习方法的方式；用CTXGEN生成可控合成数据评估CTXSEG，在癫痫检测实际用例中验证其性能，并与固定长度分割对比。

Result: 使用CTXSEG处理EEG数据在标准框架评估下，无需修改机器学习方法就能提升癫痫检测性能，且所需片段更少。

Conclusion: CTXSEG自适应分割可应用于现代机器学习方法，有提升性能潜力，是信号预处理中固定长度分割的有前景替代方案，应纳入EEG机器学习应用标准预处理方法。

Abstract: Objective. Electroencephalography (EEG) data is derived by sampling
continuous neurological time series signals. In order to prepare EEG signals
for machine learning, the signal must be divided into manageable segments. The
current naive approach uses arbitrary fixed time slices, which may have limited
biological relevance because brain states are not confined to fixed intervals.
We investigate whether adaptive segmentation methods are beneficial for machine
learning EEG analysis.
  Approach. We introduce a novel adaptive segmentation method, CTXSEG, that
creates variable-length segments based on statistical differences in the EEG
data and propose ways to use them with modern machine learning approaches that
typically require fixed-length input. We assess CTXSEG using controllable
synthetic data generated by our novel signal generator CTXGEN. While our CTXSEG
method has general utility, we validate it on a real-world use case by applying
it to an EEG seizure detection problem. We compare the performance of CTXSEG
with fixed-length segmentation in the preprocessing step of a typical EEG
machine learning pipeline for seizure detection.
  Main results. We found that using CTXSEG to prepare EEG data improves seizure
detection performance compared to fixed-length approaches when evaluated using
a standardized framework, without modifying the machine learning method, and
requires fewer segments.
  Significance. This work demonstrates that adaptive segmentation with CTXSEG
can be readily applied to modern machine learning approaches, with potential to
improve performance. It is a promising alternative to fixed-length segmentation
for signal preprocessing and should be considered as part of the standard
preprocessing repertoire in EEG machine learning applications.

</details>


### [88] [Understanding Incremental Learning with Closed-form Solution to Gradient Flow on Overparamerterized Matrix Factorization](https://arxiv.org/abs/2508.20344)
*Hancheng Min,René Vidal*

Main category: cs.LG

TL;DR: 本文用闭式解定量分析对称矩阵分解问题中梯度流的增量学习行为，揭示其源于时间尺度分离，减小初始化规模可找低秩近似，还探讨扩展到非对称问题的途径。


<details>
  <summary>Details</summary>
Motivation: 对神经网络训练中梯度流在对称矩阵分解问题的增量学习行为进行定量理解。

Method: 求解类似Riccati矩阵微分方程得到闭式解。

Result: 增量学习源于目标矩阵不同分量学习动态的时间尺度分离，减小初始化规模时间尺度分离更显著，可找到目标矩阵低秩近似。

Conclusion: 讨论了将分析扩展到非对称矩阵分解问题的可能途径。

Abstract: Many theoretical studies on neural networks attribute their excellent
empirical performance to the implicit bias or regularization induced by
first-order optimization algorithms when training networks under certain
initialization assumptions. One example is the incremental learning phenomenon
in gradient flow (GF) on an overparamerterized matrix factorization problem
with small initialization: GF learns a target matrix by sequentially learning
its singular values in decreasing order of magnitude over time. In this paper,
we develop a quantitative understanding of this incremental learning behavior
for GF on the symmetric matrix factorization problem, using its closed-form
solution obtained by solving a Riccati-like matrix differential equation. We
show that incremental learning emerges from some time-scale separation among
dynamics corresponding to learning different components in the target matrix.
By decreasing the initialization scale, these time-scale separations become
more prominent, allowing one to find low-rank approximations of the target
matrix. Lastly, we discuss the possible avenues for extending this analysis to
asymmetric matrix factorization problems.

</details>


### [89] [DFAMS: Dynamic-flow guided Federated Alignment based Multi-prototype Search](https://arxiv.org/abs/2508.20353)
*Zhibang Yang,Xinke Jiang,Rihong Qiu,Ruiqing Li,Yihang Zhang,Yue Fang,Yongxin Xu,Hongxin Ding,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.LG

TL;DR: 提出DFAMS框架用于联邦检索，在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦检索方法在处理模糊查询、跨领域场景时难以检索高质量相关文档，限制下游任务效果。

Method: 受动态信息流启发，利用梯度信号和Shapley值追踪神经元激活路径，通过多原型对比学习训练对齐模块。

Result: 在五个基准测试中，知识分类准确率最高提升14.37%，检索召回率最高提升5.38%，下游问答准确率最高提升6.45%。

Conclusion: DFAMS在复杂联邦检索场景中有效。

Abstract: Federated Retrieval (FR) routes queries across multiple external knowledge
sources, to mitigate hallucinations of LLMs, when necessary external knowledge
is distributed. However, existing methods struggle to retrieve high-quality and
relevant documents for ambiguous queries, especially in cross-domain scenarios,
which significantly limits their effectiveness in supporting downstream
generation tasks. Inspired by dynamic information flow (DIF), we propose DFAMS,
a novel framework that leverages DIF to identify latent query intents and
construct semantically aligned knowledge partitions for accurate retrieval
across heterogeneous sources. Specifically, DFAMS probes the DIF in LLMs by
leveraging gradient signals from a few annotated queries and employing Shapley
value-based attribution to trace neuron activation paths associated with intent
recognition and subdomain boundary detection. Then, DFAMS leverages DIF to
train an alignment module via multi-prototype contrastive learning, enabling
fine-grained intra-source modeling and inter-source semantic alignment across
knowledge bases. Experimental results across five benchmarks show that DFAMS
outperforms advanced FR methods by up to 14.37% in knowledge classification
accuracy, 5.38% in retrieval recall, and 6.45% in downstream QA accuracy,
demonstrating its effectiveness in complex FR scenarios.

</details>


### [90] [Developing a Multi-Modal Machine Learning Model For Predicting Performance of Automotive Hood Frames](https://arxiv.org/abs/2508.20358)
*Abhishek Indupally,Satchit Ramnath*

Main category: cs.LG

TL;DR: 本文提出多模态机器学习（MMML）架构预测性能指标，加速设计探索，结果显示MMML优于传统单模态方法，为工程设计中机器学习技术的广泛应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 解决设计师不花费大量时间进行模拟设置来评估给定引擎盖框架几何性能的问题，提高工程设计流程效率。

Method: 开发多模态机器学习（MMML）架构，从相同数据的不同模态学习来预测性能指标。

Result: MMML结合多数据模态优于传统单模态方法，训练的MMML模型能对未见框架模型进行泛化预测。

Conclusion: MMML可补充传统基于模拟的工作流程，弥合机器学习与实际工程应用的差距，推动机器学习技术在工程设计中的广泛应用。

Abstract: Is there a way for a designer to evaluate the performance of a given hood
frame geometry without spending significant time on simulation setup? This
paper seeks to address this challenge by developing a multimodal
machine-learning (MMML) architecture that learns from different modalities of
the same data to predict performance metrics. It also aims to use the MMML
architecture to enhance the efficiency of engineering design processes by
reducing reliance on computationally expensive simulations. The proposed
architecture accelerates design exploration, enabling rapid iteration while
maintaining high-performance standards, especially in the concept design phase.
The study also presents results that show that by combining multiple data
modalities, MMML outperforms traditional single-modality approaches. Two new
frame geometries, not part of the training dataset, are also used for
prediction using the trained MMML model to showcase the ability to generalize
to unseen frame models. The findings underscore MMML's potential in
supplementing traditional simulation-based workflows, particularly in the
conceptual design phase, and highlight its role in bridging the gap between
machine learning and real-world engineering applications. This research paves
the way for the broader adoption of machine learning techniques in engineering
design, with a focus on refining multimodal approaches to optimize structural
development and accelerate the design cycle.

</details>


### [91] [BiListing: Modality Alignment for Listings](https://arxiv.org/abs/2508.20396)
*Guillaume Guy,Mihajlo Grbovic,Chun How Tan,Han Zhao*

Main category: cs.LG

TL;DR: 本文提出 BiListing 方法，结合大语言模型和预训练语言 - 图像模型对齐 Airbnb 房源文本和图片，经测试部署后取得 NDCG 增益和可观收入。


<details>
  <summary>Details</summary>
Motivation: 以往 Airbnb 依赖结构化数据，随着表征学习兴起，利用文本和图片非结构化数据成为趋势，但房源多样非结构化数据使信息融合成单一表征困难，因此需要新方法。

Method: 提出 BiListing 方法，利用大语言模型和预训练语言 - 图像模型对齐房源文本和图片。

Result: 进行离线和在线测试，在 Airbnb 搜索排序模型中部署该方法，实现 0.425% 的 NDCG 增益，带来数千万增量收入。

Conclusion: BiListing 方法能有效处理 Airbnb 房源非结构化数据，在搜索排序中取得良好效果并带来商业收益。

Abstract: Airbnb is a leader in offering travel accommodations. Airbnb has historically
relied on structured data to understand, rank, and recommend listings to guests
due to the limited capabilities and associated complexity arising from
extracting meaningful information from text and images. With the rise of
representation learning, leveraging rich information from text and photos has
become easier. A popular approach has been to create embeddings for text
documents and images to enable use cases of computing similarities between
listings or using embeddings as features in an ML model.
  However, an Airbnb listing has diverse unstructured data: multiple images,
various unstructured text documents such as title, description, and reviews,
making this approach challenging. Specifically, it is a non-trivial task to
combine multiple embeddings of different pieces of information to reach a
single representation.
  This paper proposes BiListing, for Bimodal Listing, an approach to align text
and photos of a listing by leveraging large-language models and pretrained
language-image models. The BiListing approach has several favorable
characteristics: capturing unstructured data into a single embedding vector per
listing and modality, enabling zero-shot capability to search inventory
efficiently in user-friendly semantics, overcoming the cold start problem, and
enabling listing-to-listing search along a single modality, or both.
  We conducted offline and online tests to leverage the BiListing embeddings in
the Airbnb search ranking model, and successfully deployed it in production,
achieved 0.425% of NDCB gain, and drove tens of millions in incremental
revenue.

</details>


### [92] [TF-TransUNet1D: Time-Frequency Guided Transformer U-Net for Robust ECG Denoising in Digital Twin](https://arxiv.org/abs/2508.20398)
*Shijie Wang,Lei Li*

Main category: cs.LG

TL;DR: 提出TF - TransUNet1D网络结合U - Net和Transformer编码器，用双域损失函数对ECG信号去噪，实验显示该模型优于基线。


<details>
  <summary>Details</summary>
Motivation: ECG信号常受噪声和伪影影响，影响其在心脏数字双胞胎中的诊断效用，需有效去噪方法。

Method: 提出TF - TransUNet1D网络，结合U - Net架构和Transformer编码器；引入双域损失函数，在时域和频域联合优化。

Result: 使用合成受损信号评估，对比实验显示该模型在SNR改善和误差指标上优于基线，MAE为0.1285，皮尔逊相关系数为0.9540。

Conclusion: 该工作弥补了心脏数字双胞胎预处理流程的关键空白，实现更可靠的实时监测和个性化建模。

Abstract: Electrocardiogram (ECG) signals serve as a foundational data source for
cardiac digital twins, yet their diagnostic utility is frequently compromised
by noise and artifacts. To address this issue, we propose TF-TransUNet1D, a
novel one-dimensional deep neural network that integrates a U-Net-based
encoder-decoder architecture with a Transformer encoder, guided by a hybrid
time-frequency domain loss. The model is designed to simultaneously capture
local morphological features and long-range temporal dependencies, which are
critical for preserving the diagnostic integrity of ECG signals. To enhance
denoising robustness, we introduce a dual-domain loss function that jointly
optimizes waveform reconstruction in the time domain and spectral fidelity in
the frequency domain. In particular, the frequency-domain component effectively
suppresses high-frequency noise while maintaining the spectral structure of the
signal, enabling recovery of subtle but clinically significant waveform
components. We evaluate TF-TransUNet1D using synthetically corrupted signals
from the MIT-BIH Arrhythmia Database and the Noise Stress Test Database
(NSTDB). Comparative experiments against state-of-the-art baselines demonstrate
consistent superiority of our model in terms of SNR improvement and error
metrics, achieving a mean absolute error of 0.1285 and Pearson correlation
coefficient of 0.9540. By delivering high-precision denoising, this work
bridges a critical gap in pre-processing pipelines for cardiac digital twins,
enabling more reliable real-time monitoring and personalized modeling.

</details>


### [93] [Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full Context-Aware Linear Attention](https://arxiv.org/abs/2508.20407)
*Zhongpan Tang*

Main category: cs.LG

TL;DR: 提出新的线性注意力架构TLinFormer解决Transformer自注意力机制复杂度瓶颈问题，实验显示其在长序列推理任务中多指标占优。


<details>
  <summary>Details</summary>
Motivation: Transformer自注意力机制复杂度瓶颈限制长序列任务应用，现有线性注意力方法牺牲性能。

Method: 从连接主义第一原则出发，基于信息流拓扑结构，重新配置神经元连接模式，引入TLinFormer架构。

Result: 实验表明TLinFormer在推理延迟、KV缓存效率、内存占用和整体加速等关键指标上有压倒性优势。

Conclusion: TLinFormer能弥补现有高效注意力方法与标准注意力之间的性能差距。

Abstract: The Transformer architecture has become a cornerstone of modern artificial
intelligence, but its core self-attention mechanism suffers from a complexity
bottleneck that scales quadratically with sequence length, severely limiting
its application in long-sequence tasks. To address this challenge, existing
linear attention methods typically sacrifice model performance by relying on
data-agnostic kernel approximations or restrictive context selection. This
paper returns to the first principles of connectionism, starting from the
topological structure of information flow, to introduce a novel linear
attention architecture-\textbf{TLinFormer}. By reconfiguring neuron connection
patterns, TLinFormer achieves strict linear complexity while computing exact
attention scores and ensuring information flow remains aware of the full
historical context. This design aims to bridge the performance gap prevalent
between existing efficient attention methods and standard attention. Through a
series of experiments, we systematically evaluate the performance of TLinFormer
against a standard Transformer baseline on long-sequence inference tasks. The
results demonstrate that TLinFormer exhibits overwhelming advantages in key
metrics such as \textbf{inference latency}, \textbf{KV cache efficiency},
\textbf{memory footprint}, and \textbf{overall speedup}.

</details>


### [94] [Assessing local deformation and computing scalar curvature with nonlinear conformal regularization of decoders](https://arxiv.org/abs/2508.20413)
*Benjamin Couéraud,Vikram Sunkara,Christof Schütte*

Main category: cs.LG

TL;DR: 文章介绍用于深度神经网络解码映射的非线性共形正则化，可计算学习流形的标量曲率，并在数据集上实验展示获取相关量。


<details>
  <summary>Details</summary>
Motivation: 降维旨在发现解释数据的主要因素，自编码器是学习低维表示的有效方法，需新的正则化方法。

Method: 引入用于深度神经网络解码映射的非线性共形正则化，有新的标量场共形因子。

Result: 该正则化技术允许计算学习流形的标量曲率。

Conclusion: 通过在Swiss roll和CelebA数据集上的实现和实验，展示从架构中获取相关量的方法。

Abstract: One aim of dimensionality reduction is to discover the main factors that
explain the data, and as such is paramount to many applications. When working
with high dimensional data, autoencoders offer a simple yet effective approach
to learn low-dimensional representations. The two components of a general
autoencoder consist first of an encoder that maps the observed data onto a
latent space; and second a decoder that maps the latent space back to the
original observation space, which allows to learn a low-dimensional manifold
representation of the original data. In this article, we introduce a new type
of geometric regularization for decoding maps approximated by deep neural
networks, namely nonlinear conformal regularization. This regularization
procedure permits local variations of the decoder map and comes with a new
scalar field called conformal factor which acts as a quantitative indicator of
the amount of local deformation sustained by the latent space when mapped into
the original data space. We also show that this regularization technique allows
the computation of the scalar curvature of the learned manifold. Implementation
and experiments on the Swiss roll and CelebA datasets are performed to
illustrate how to obtain these quantities from the architecture.

</details>


### [95] [On Identifying Why and When Foundation Models Perform Well on Time-Series Forecasting Using Automated Explanations and Rating](https://arxiv.org/abs/2508.20437)
*Michael Widener,Kausik Lakkaraju,John Aydin,Biplav Srivastava*

Main category: cs.LG

TL;DR: 本文结合传统可解释AI方法与RDE评估不同领域和用例下时间序列预测模型的性能和可解释性，发现特征工程模型在不稳定或稀疏领域表现更好且更具可解释性，基础模型仅在稳定或趋势驱动的情境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测模型日益用于指导现实行动，但理解其成败原因和表现仍具挑战，需要解决用户如何与模型输出交互和依赖的问题。

Method: 结合传统可解释AI方法与RDE，在金融、能源、交通和汽车销售等四个不同领域的数据集上评估ARIMA、梯度提升、Chronos和Llama四种不同模型架构。

Result: 特征工程模型（如梯度提升）在不稳定或稀疏领域（如电力、汽车零部件）始终优于基础模型（如Chronos），且解释更具可解释性；基础模型仅在稳定或趋势驱动的情境（如金融）中表现出色。

Conclusion: 不同类型的时间序列预测模型在不同领域有不同的表现和可解释性，用户可根据具体领域特点选择合适的模型。

Abstract: Time-series forecasting models (TSFM) have evolved from classical statistical
methods to sophisticated foundation models, yet understanding why and when
these models succeed or fail remains challenging. Despite this known
limitation, time series forecasting models are increasingly used to generate
information that informs real-world actions with equally real consequences.
Understanding the complexity, performance variability, and opaque nature of
these models then becomes a valuable endeavor to combat serious concerns about
how users should interact with and rely on these models' outputs. This work
addresses these concerns by combining traditional explainable AI (XAI) methods
with Rating Driven Explanations (RDE) to assess TSFM performance and
interpretability across diverse domains and use cases. We evaluate four
distinct model architectures: ARIMA, Gradient Boosting, Chronos (time-series
specific foundation model), Llama (general-purpose; both fine-tuned and base
models) on four heterogeneous datasets spanning finance, energy,
transportation, and automotive sales domains. In doing so, we demonstrate that
feature-engineered models (e.g., Gradient Boosting) consistently outperform
foundation models (e.g., Chronos) in volatile or sparse domains (e.g., power,
car parts) while providing more interpretable explanations, whereas foundation
models excel only in stable or trend-driven contexts (e.g., finance).

</details>


### [96] [Uncovering the Spectral Bias in Diagonal State Space Models](https://arxiv.org/abs/2508.20441)
*Ruben Solozabal,Velibor Bojkovic,Hilal AlQuabeh,Kentaro Inui,Martin Takáč*

Main category: cs.LG

TL;DR: 本文从频率角度研究对角SSM初始化方案，提出S4D - DFouT初始化方法，在基准测试获SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有HiPPO框架未明确研究其对角变体作用，需系统理解对角状态空间模型参数化及学习偏差。

Method: 从频率角度研究对角SSM初始化方案，提出离散傅里叶域的对角初始化S4D - DFouT。

Result: 在Long Range Arena基准测试取得SOTA结果，可在如PathX - 256等大型数据集上从头训练。

Conclusion: 对初始化中极点放置的研究有助于扩展模型并取得好结果。

Abstract: Current methods for initializing state space models (SSMs) parameters mainly
rely on the \textit{HiPPO framework}, which is based on an online approximation
of orthogonal polynomials. Recently, diagonal alternatives have shown to reach
a similar level of performance while being significantly more efficient due to
the simplification in the kernel computation. However, the \textit{HiPPO
framework} does not explicitly study the role of its diagonal variants. In this
paper, we take a further step to investigate the role of diagonal SSM
initialization schemes from the frequency perspective. Our work seeks to
systematically understand how to parameterize these models and uncover the
learning biases inherent in such diagonal state-space models. Based on our
observations, we propose a diagonal initialization on the discrete Fourier
domain \textit{S4D-DFouT}. The insights in the role of pole placing in the
initialization enable us to further scale them and achieve state-of-the-art
results on the Long Range Arena benchmark, allowing us to train from scratch on
very large datasets as PathX-256.

</details>


### [97] [Towards Mitigating Excessive Forgetting in LLM Unlearning via Entanglement-Aware Unlearning with Proxy Constraint](https://arxiv.org/abs/2508.20443)
*Zhihao Liu,Jian Lou,Yuke Hu,Xiaochen Li,Tailun Chen,Yitian Chen,Zhan Qin*

Main category: cs.LG

TL;DR: 文章针对现有大语言模型数据删除方法的不足，提出EAGLE - PC框架，在基准测试中表现良好，接近全量重新训练性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练数据含隐私或版权内容，数据所有者可能要求删除数据，而现有机器学习遗忘方法存在遗忘边界问题。

Method: 提出EAGLE - PC框架，包含纠缠感知引导的损失重加权和利用ICL生成测试数据的代理约束两个关键组件，且与现有基于梯度的目标兼容。

Result: 在TOFU和MUSE基准测试中，在多个大语言模型上遗忘 - 效用权衡有一致提升，结合NPO + GD优化器接近全量重新训练性能。

Conclusion: EAGLE - PC是一种可扩展且稳健的大语言模型数据遗忘解决方案。

Abstract: Large language models (LLMs) are trained on massive datasets that may include
private or copyrighted content. Due to growing privacy and ownership concerns,
data owners may request the removal of their data from trained models. Machine
unlearning provides a practical solution by removing the influence of specific
data without full retraining. However, most existing methods lack a sound
forgetting boundary, causing some samples to be under-forgotten, leaving
residual leakage risks, while others remain over-forgotten at the expense of
degraded utility.
  In this work, we propose EAGLE-PC (Entanglement-Awareness Guided Loss
Reweighting with Proxy Constraint), a novel unlearning framework that addresses
these limitations through two key components. First, entanglement-awareness
guided loss reweighting determines the forgetting effort of each sample by
measuring its similarity to retain samples in the embedding space, enabling
more targeted and effective unlearning. Second, a proxy constraint leveraging
ICL (In-Context Learning) generated test data softly regularizes the forgetting
process, effectively mitigating over-forgetting. EAGLE-PC is compatible with
existing gradient-based objectives and serves as a plug-and-play enhancement.
We evaluate EAGLE-PC on the TOFU and MUSE benchmarks, showing consistent
improvements in the forgetting-utility trade-off across multiple LLMs. Combined
with the NPO+GD optimizer, it approaches full retraining performance, offering
a scalable and robust unlearning solution.

</details>


### [98] [Evaluating Differentially Private Generation of Domain-Specific Text](https://arxiv.org/abs/2508.20452)
*Yidan Sun,Viktor Schlegel,Srinivasan Nandakumar,Iqra Zahid,Yuping Wu,Warren Del-Pinto,Goran Nenadic,Siew-Kei Lam,Jie Zhang,Anil A Bharath*

Main category: cs.LG

TL;DR: 引入统一基准评估差分隐私文本数据集生成的效用和保真度，发现现有方法在严格隐私约束下存在局限。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在高风险领域应用受隐私和监管阻碍，需评估差分隐私合成数据生成的效用和保真度。

Method: 引入统一基准，考虑代表性数据选择、隐私预算、预训练和多种评估指标，评估五种特定领域数据集上的先进隐私保护生成方法。

Result: 与真实数据相比，现有方法尤其是在严格隐私约束下，效用和保真度显著下降。

Conclusion: 强调现有方法的局限性，指出需要先进的隐私保护数据共享方法，并为现实场景评估树立了先例。

Abstract: Generative AI offers transformative potential for high-stakes domains such as
healthcare and finance, yet privacy and regulatory barriers hinder the use of
real-world data. To address this, differentially private synthetic data
generation has emerged as a promising alternative. In this work, we introduce a
unified benchmark to systematically evaluate the utility and fidelity of text
datasets generated under formal Differential Privacy (DP) guarantees. Our
benchmark addresses key challenges in domain-specific benchmarking, including
choice of representative data and realistic privacy budgets, accounting for
pre-training and a variety of evaluation metrics. We assess state-of-the-art
privacy-preserving generation methods across five domain-specific datasets,
revealing significant utility and fidelity degradation compared to real data,
especially under strict privacy constraints. These findings underscore the
limitations of current approaches, outline the need for advanced
privacy-preserving data sharing methods and set a precedent regarding their
evaluation in realistic scenarios.

</details>


### [99] [Structure-aware Hypergraph Transformer for Diagnosis Prediction in Electronic Health Records](https://arxiv.org/abs/2508.20500)
*Haiyan Wang,Ye Yuan*

Main category: cs.LG

TL;DR: 提出结构感知超图变换器（SHGT）框架用于电子健康记录（EHR）诊断预测，在真实数据集上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络（GNN）的EHR预测方法存在无法捕捉高阶依赖和局部消息传递限制表征能力的问题。

Method: 采用超图结构编码器捕捉医疗代码间高阶交互，集成Transformer架构对整个超图进行推理，设计结合超图重建的定制损失函数。

Result: 在真实EHR数据集实验中，SHGT在诊断预测上优于现有最先进模型。

Conclusion: 所提出的SHGT框架能有效提高EHR诊断预测性能。

Abstract: Electronic Health Records (EHR) systematically organize patient health data
through standardized medical codes, serving as a comprehensive and invaluable
source for predictive modeling. Graph neural networks (GNNs) have demonstrated
effectiveness in modeling interactions between medical codes within EHR.
However, existing GNN-based methods are inadequate due to: a) their reliance on
pairwise relations fails to capture the inherent higher-order dependencies in
clinical data, and b) the localized message-passing scheme limits
representation power. To address these issues, this paper proposes a novel
Structure-aware HyperGraph Transformer (SHGT) framework following three-fold
ideas: a) employing a hypergraph structural encoder to capture higher-order
interactions among medical codes, b) integrating the Transformer architecture
to reason over the entire hypergraph, and c) designing a tailored loss function
incorporating hypergraph reconstruction to preserve the hypergraph's original
structure. Experiments on real-world EHR datasets demonstrate that the proposed
SHGT outperforms existing state-of-the-art models on diagnosis prediction.

</details>


### [100] [Khiops: An End-to-End, Frugal AutoML and XAI Machine Learning Solution for Large, Multi-Table Databases](https://arxiv.org/abs/2508.20519)
*Marc Boullé,Nicolas Voisine,Bruno Guerraz,Carine Hue,Felipe Olmos,Vladimir Popescu,Stéphane Gouache,Stéphane Bouget,Alexis Bondu,Luc Aurelien Gauthier,Yassine Nair Benrekia,Fabrice Clérot,Vincent Lemaire*

Main category: cs.LG

TL;DR: 介绍开源机器学习工具Khiops，它基于贝叶斯方法，适用于大型多表数据库分析，有多种功能且多环境可用。


<details>
  <summary>Details</summary>
Motivation: 开发适用于挖掘大型多表数据库的机器学习工具。

Method: 基于独特贝叶斯方法，用离散化模型处理数值数据、值聚类处理分类数据，采用结合变量选择和权重学习的朴素贝叶斯分类器，多表数据库中自动构建聚合进行命题化。

Result: Khiops可对含数百万个体、数万个变量和数亿条记录的大型数据库进行分析。

Conclusion: Khiops是一款功能丰富、适用大型数据库且多环境可用的机器学习工具。

Abstract: Khiops is an open source machine learning tool designed for mining large
multi-table databases. Khiops is based on a unique Bayesian approach that has
attracted academic interest with more than 20 publications on topics such as
variable selection, classification, decision trees and co-clustering. It
provides a predictive measure of variable importance using discretisation
models for numerical data and value clustering for categorical data. The
proposed classification/regression model is a naive Bayesian classifier
incorporating variable selection and weight learning. In the case of
multi-table databases, it provides propositionalisation by automatically
constructing aggregates. Khiops is adapted to the analysis of large databases
with millions of individuals, tens of thousands of variables and hundreds of
millions of records in secondary tables. It is available on many environments,
both from a Python library and via a user interface.

</details>


### [101] [MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via Generative Reward Learning](https://arxiv.org/abs/2508.20549)
*Weihai Zhi,Jiayan Guo,Shangyang Li*

Main category: cs.LG

TL;DR: 提出MedGR²框架打破医学领域视觉语言模型数据稀缺困境，实验显示其生成数据可提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 医学领域视觉语言模型受高质量标注数据稀缺的限制，有监督微调泛化能力差，强化学习缺乏可靠奖励信号。

Method: 引入Generative Reward Learning for Medical Reasoning (MedGR²)框架，共同开发数据生成器和奖励模型，自动持续创建高质量多模态医学数据。

Result: 使用MedGR²生成数据进行有监督微调已超越在大规模人工策划数据集上训练的基线；通过GRPO进行强化学习，模型实现了跨模态和跨任务泛化的最优；紧凑模型性能可与参数多10倍以上的基础模型竞争。

Conclusion: MedGR²为高风险领域的数据高效学习提供了新范式，将数据稀缺问题转化为数据生成问题，释放强化学习潜力以构建通用医学AI。

Abstract: The application of Vision-Language Models (VLMs) in medicine is critically
hampered by the scarcity of high-quality, expert-annotated data. Supervised
Fine-Tuning (SFT) on existing datasets often leads to poor generalization on
unseen modalities and tasks, while Reinforcement Learning (RL), a promising
alternative, is stymied by the lack of reliable reward signals in this
data-scarce domain. To break this impasse, we introduce Generative Reward
Learning for Medical Reasoning (MedGR$^2$), a novel framework that creates a
self-improving virtuous cycle. MedGR$^2$ co-develops a data generator and a
reward model, enabling the automated, continuous creation of high-quality,
multi-modal medical data that serves as both a superior training source for SFT
and RL. Our experiments demonstrate that SFT with MedGR$^2$-produced data
already surpasses baselines trained on large-scale, human-curated datasets.
Crucially, when leveraging this data for RL via Group Relative Policy
Optimization (GRPO), our model achieves state-of-the-art cross-modality and
cross-task generalization, significantly outperforming specialized RL-based
methods. Furthermore, our compact model, empowered by MedGR$^2$, achieves
performance competitive with foundation models possessing over 10 times more
parameters. MedGR$^2$ presents a new paradigm for data-efficient learning in
high-stakes domains, transforming the problem from data scarcity to data
generation and unlocking the full potential of RL for building truly
generalizable medical AI.

</details>


### [102] [Theoretical foundations of the integral indicator application in hyperparametric optimization](https://arxiv.org/abs/2508.20550)
*Roman S. Kulshin,Anatoly A. Sidorov*

Main category: cs.LG

TL;DR: 文章探讨用综合评估进行推荐算法超参数优化，开发通用多准则优化工具。


<details>
  <summary>Details</summary>
Motivation: 传统单指标设置方法有局限，需平衡算法多方面性能。

Method: 采用综合评估将多种性能指标结合为单一综合标准进行超参数优化。

Result: 未明确提及具体结果。

Conclusion: 研究开发了适用于推荐系统及机器学习和数据分析任务的通用多准则优化工具。

Abstract: The article discusses the concept of hyperparametric optimization of
recommendation algorithms using an integral assessment that combines various
performance indicators into a single consolidated criterion. This approach is
opposed to traditional methods of setting up a single metric and allows you to
achieve a balance between accuracy, ranking quality, variety of output and the
resource intensity of algorithms. The theoretical significance of the research
lies in the development of a universal multi-criteria optimization tool that is
applicable not only in recommendation systems, but also in a wide range of
machine learning and data analysis tasks.

</details>


### [103] [MERIT: Maximum-normalized Element-wise Ratio for Language Model Large-batch Training](https://arxiv.org/abs/2508.20577)
*Yang Luo,Zangwei Zheng,Ziheng Qin,Zirui Zhu,Yong Liu,Yang You*

Main category: cs.LG

TL;DR: 现有优化器在大批次训练语言模型时性能下降，提出新优化器MERIT，实验证明其性能优越，提高训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有优化器如AdamW、LAMB在大批次训练语言模型时存在性能问题，需要更好的优化器。

Method: 提出新优化器MERIT，利用最大范数计算信任比率，构建逐元素信任比率。

Result: 在不同规模GPT - 2模型大批次训练实验中，MERIT表现优越，如训练GPT - 2 Medium时6k批次无性能下降。

Conclusion: 大批次训练中考虑最大注意力对数和更细粒度信任比率很重要，MERIT提高训练稳定性，利于大语言模型快速开发迭代。

Abstract: Large-batch training has become a cornerstone in accelerating the training of
deep neural networks, yet it poses challenges in optimization and
generalization. Existing optimizers like AdamW present performance degradation
during language models' large-batch training, due to the information bottleneck
in attention layers caused by the sharp increase of max attention logit. While
the LAMB optimizer partially addresses this issue, some attention layers still
face this issue. The reason is that $l_2$-norm-based trust ratios in LAMB are
less effective in directly influencing the max value of query/key weights.
Furthermore, the weight-wise trust ratio in LAMB is error-prone as it overlooks
relationships of weight values within rows or columns. Building on these
observations, we propose a novel optimizer, MERIT, which leverages the max-norm
to calculate the trust ratio to constrain the max attention logit more
effectively. Moreover, we further construct element-wise trust ratios to
provide more robust update scaling by focusing on local weight structures.
Extensive experiments of large-batch training across various sizes of GPT-2
models demonstrate the superior performance of MERIT. Notably, during the
training of GPT-2 Medium, MERIT enables a 6k batch size without any performance
degradation compared to the standard batch size (480) with 48B training tokens.
This work highlights the importance of considering the max attention logit and
finer-granularity trust ratio in large-batch training. It successfully improves
the training stability and paves the way for larger batch usage, enabling
faster development and iteration of large language models. Code is available at
https://github.com/NUS-HPC-AI-Lab/MERIT.

</details>


### [104] [Local Virtual Nodes for Alleviating Over-Squashing in Graph Neural Networks](https://arxiv.org/abs/2508.20597)
*Tuğrul Hasan Karabulut,İnci M. Baytaş*

Main category: cs.LG

TL;DR: 提出带可训练嵌入的局部虚拟节点（LVN）缓解图神经网络过压缩问题，实验表明可增强结构连通性并提升分类任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有解决图神经网络过压缩问题的方法会改变输入图全局拓扑、破坏原始图结构中的领域知识，需新方法缓解过压缩同时不显著破坏全局结构。

Method: 提出带可训练嵌入的LVN，其位置由节点中心性决定，共享的可训练LVN嵌入促进远距离节点通信。

Result: 在基准数据集上实验，LVN能增强结构连通性，显著提升图和节点分类任务性能。

Conclusion: LVN可在不显著破坏输入图全局结构的情况下缓解过压缩问题，提高相关任务性能。

Abstract: Over-squashing is a challenge in training graph neural networks for tasks
involving long-range dependencies. In such tasks, a GNN's receptive field
should be large enough to enable communication between distant nodes. However,
gathering information from a wide range of neighborhoods and squashing its
content into fixed-size node representations makes message-passing vulnerable
to bottlenecks. Graph rewiring and adding virtual nodes are commonly studied
remedies that create additional pathways around bottlenecks to mitigate
over-squashing. However, these techniques alter the input graph's global
topology and disrupt the domain knowledge encoded in the original graph
structure, both of which could be essential to specific tasks and domains. This
study presents Local Virtual Nodes (LVN) with trainable embeddings to alleviate
the effects of over-squashing without significantly corrupting the global
structure of the input graph. The position of the LVNs is determined by the
node centrality, which indicates the existence of potential bottlenecks. Thus,
the proposed approach aims to improve the connectivity in the regions with
likely bottlenecks. Furthermore, trainable LVN embeddings shared across
selected central regions facilitate communication between distant nodes without
adding more layers. Extensive experiments on benchmark datasets demonstrate
that LVNs can enhance structural connectivity and significantly improve
performance on graph and node classification tasks. The code can be found at
https://github.com/ALLab-Boun/LVN/}{https://github.com/ALLab-Boun/LVN/.

</details>


### [105] [Masked Autoencoders for Ultrasound Signals: Robust Representation Learning for Downstream Applications](https://arxiv.org/abs/2508.20622)
*Immanuel Roßteutscher,Klaus S. Drese,Thorsten Uphues*

Main category: cs.LG

TL;DR: 研究MAE在一维超声信号自监督表征学习的适应性和性能，提出用MAE预训练无标签合成超声信号，结果显示预训练模型表现佳，凸显MAE在超声信号分析潜力。


<details>
  <summary>Details</summary>
Motivation: MAE在一维信号分析尤其是原始超声数据方面研究较少，且超声信号在工业应用中标签数据稀缺、信号处理任务特定，需有效方法。

Method: 利用MAE对无标签合成超声信号进行预训练，系统研究模型大小、补丁大小和掩码率对预训练效率和下游准确率的影响。

Result: 预训练模型显著优于从头训练的模型和针对下游任务优化的强CNN基线，在合成数据上预训练对真实测量信号的迁移能力更好。

Conclusion: MAE通过可扩展的自监督学习在推进超声信号分析方面具有潜力。

Abstract: We investigated the adaptation and performance of Masked Autoencoders (MAEs)
with Vision Transformer (ViT) architectures for self-supervised representation
learning on one-dimensional (1D) ultrasound signals. Although MAEs have
demonstrated significant success in computer vision and other domains, their
use for 1D signal analysis, especially for raw ultrasound data, remains largely
unexplored. Ultrasound signals are vital in industrial applications such as
non-destructive testing (NDT) and structural health monitoring (SHM), where
labeled data are often scarce and signal processing is highly task-specific. We
propose an approach that leverages MAE to pre-train on unlabeled synthetic
ultrasound signals, enabling the model to learn robust representations that
enhance performance in downstream tasks, such as time-of-flight (ToF)
classification. This study systematically investigated the impact of model
size, patch size, and masking ratio on pre-training efficiency and downstream
accuracy. Our results show that pre-trained models significantly outperform
models trained from scratch and strong convolutional neural network (CNN)
baselines optimized for the downstream task. Additionally, pre-training on
synthetic data demonstrates superior transferability to real-world measured
signals compared with training solely on limited real datasets. This study
underscores the potential of MAEs for advancing ultrasound signal analysis
through scalable, self-supervised learning.

</details>


### [106] [GDS Agent: A Graph Algorithmic Reasoning Agent](https://arxiv.org/abs/2508.20637)
*Borun Shi,Ioannis Panagiotas*

Main category: cs.LG

TL;DR: 本文介绍GDS代理，可让大语言模型处理图结构数据，还引入新基准评估，结果显示能解决多种图任务，并探讨挑战与未来规划。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的复合系统难以处理和推理大规模图结构数据。

Method: 引入GDS代理，在模型上下文协议（MCP）服务器中引入一套图算法作为工具，包括算法结果的预处理和后处理；引入新基准评估中间工具调用和最终响应。

Result: GDS代理能够解决广泛的图任务，还对更开放的任务进行详细案例研究，并研究代理遇到困难的场景。

Conclusion: 探讨了剩余挑战和未来路线图。

Abstract: Large language models (LLMs) have shown remarkable multimodal information
processing and reasoning ability. When equipped with tools through function
calling and enhanced with retrieval-augmented techniques, compound LLM-based
systems can access closed data sources and answer questions about them.
However, they still struggle to process and reason over large-scale
graph-structure data. We introduce the GDS (Graph Data Science) agent in this
technical report. The GDS agent introduces a comprehensive set of graph
algorithms as tools, together with preprocessing (retrieval) and postprocessing
of algorithm results, in a model context protocol (MCP) server. The server can
be used with any modern LLM out-of-the-box. GDS agent allows users to ask any
question that implicitly and intrinsically requires graph algorithmic reasoning
about their data, and quickly obtain accurate and grounded answers. We also
introduce a new benchmark that evaluates intermediate tool calls as well as
final responses. The results indicate that GDS agent is able to solve a wide
spectrum of graph tasks. We also provide detailed case studies for more
open-ended tasks and study scenarios where the agent struggles. Finally, we
discuss the remaining challenges and the future roadmap.

</details>


### [107] [VarDiU: A Variational Diffusive Upper Bound for One-Step Diffusion Distillation](https://arxiv.org/abs/2508.20646)
*Leyang Wang,Mingtian Zhang,Zijing Ou,David Barber*

Main category: cs.LG

TL;DR: 提出VarDiU用于扩散蒸馏，相比Diff - Instruct有更好的生成质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散蒸馏方法训练学生模型时使用的扩散散度梯度估计有偏差，导致性能不佳。

Method: 提出VarDiU（变分扩散上界），它有一个无偏的梯度估计器并可直接应用于扩散蒸馏。

Result: 与Diff - Instruct对比，实现了更高的生成质量，训练过程更高效稳定。

Conclusion: VarDiU在一步扩散蒸馏中表现良好，能提升生成质量和训练效率。

Abstract: Recently, diffusion distillation methods have compressed thousand-step
teacher diffusion models into one-step student generators while preserving
sample quality. Most existing approaches train the student model using a
diffusive divergence whose gradient is approximated via the student's score
function, learned through denoising score matching (DSM). Since DSM training is
imperfect, the resulting gradient estimate is inevitably biased, leading to
sub-optimal performance. In this paper, we propose VarDiU (pronounced
/va:rdju:/), a Variational Diffusive Upper Bound that admits an unbiased
gradient estimator and can be directly applied to diffusion distillation. Using
this objective, we compare our method with Diff-Instruct and demonstrate that
it achieves higher generation quality and enables a more efficient and stable
training procedure for one-step diffusion distillation.

</details>


### [108] [Physics-Constrained Machine Learning for Chemical Engineering](https://arxiv.org/abs/2508.20649)
*Angan Mukherjee,Victor M. Zavala*

Main category: cs.LG

TL;DR: 物理学约束机器学习（PCML）结合物理模型与数据驱动方法，但在复杂化学工程应用有挑战，本文总结进展并强调应用方向。


<details>
  <summary>Details</summary>
Motivation: 虽PCML在多领域有益，但在复杂化学工程应用存在技术和知识挑战，需推动其在该领域应用。

Method: 无明确提及具体方法，主要是对PCML在化学工程应用的发展进行总结。

Result: 总结了PCML在化学工程应用的近期发展，指出关键困难如物理知识嵌入量和类型确定等。

Conclusion: 强调PCML在化学工程应用的挑战与机遇，重点关注闭环实验设计、实时动态与控制及多尺度现象处理。

Abstract: Physics-constrained machine learning (PCML) combines physical models with
data-driven approaches to improve reliability, generalizability, and
interpretability. Although PCML has shown significant benefits in diverse
scientific and engineering domains, technical and intellectual challenges
hinder its applicability in complex chemical engineering applications. Key
difficulties include determining the amount and type of physical knowledge to
embed, designing effective fusion strategies with ML, scaling models to large
datasets and simulators, and quantifying predictive uncertainty. This
perspective summarizes recent developments and highlights
challenges/opportunities in applying PCML to chemical engineering, emphasizing
on closed-loop experimental design, real-time dynamics and control, and
handling of multi-scale phenomena.

</details>


### [109] [Self-Composing Neural Operators with Depth and Accuracy Scaling via Adaptive Train-and-Unroll Approach](https://arxiv.org/abs/2508.20650)
*Juncai He,Xinliang Liu,Jinchao Xu*

Main category: cs.LG

TL;DR: 提出通过自组合增强神经算子效率和准确性的框架，采用自适应训练方法，在基准测试和高频超声CT问题上表现出色。


<details>
  <summary>Details</summary>
Motivation: 提高神经算子的效率和准确性，为大规模数据驱动的科学机器学习应用提供更好的解决方案。

Method: 设计特定神经算子，通过重复应用单个神经算子块加深模型；引入自适应训练展开方法，逐步增加神经算子深度。

Result: 架构在标准基准测试中达到了SOTA性能，在高频超声CT问题上展现出卓越性能。

Conclusion: 所提出的框架为大规模数据驱动的科学机器学习应用提供了计算可行、准确且可扩展的解决方案。

Abstract: In this work, we propose a novel framework to enhance the efficiency and
accuracy of neural operators through self-composition, offering both
theoretical guarantees and practical benefits. Inspired by iterative methods in
solving numerical partial differential equations (PDEs), we design a specific
neural operator by repeatedly applying a single neural operator block, we
progressively deepen the model without explicitly adding new blocks, improving
the model's capacity. To train these models efficiently, we introduce an
adaptive train-and-unroll approach, where the depth of the neural operator is
gradually increased during training. This approach reveals an accuracy scaling
law with model depth and offers significant computational savings through our
adaptive training strategy. Our architecture achieves state-of-the-art (SOTA)
performance on standard benchmarks. We further demonstrate its efficacy on a
challenging high-frequency ultrasound computed tomography (USCT) problem, where
a multigrid-inspired backbone enables superior performance in resolving complex
wave phenomena. The proposed framework provides a computationally tractable,
accurate, and scalable solution for large-scale data-driven scientific machine
learning applications.

</details>


### [110] [Compositionality in Time Series: A Proof of Concept using Symbolic Dynamics and Compositional Data Augmentation](https://arxiv.org/abs/2508.20656)
*Michael Hagmann,Michael Staniek,Stefan Riezler*

Main category: cs.LG

TL;DR: 研究临床时间序列是否由有规律的潜在状态生成，提出方法重构基本状态和组合规则，实验表明合成数据效果好，下游任务有性能提升。


<details>
  <summary>Details</summary>
Motivation: 了解自然现象时间序列是否由有规律潜在状态生成，解决临床时间序列预测中数据稀疏和资源少的问题，加深对临床数据的理解。

Method: 将时间序列的组合性概念化为数据生成过程的属性，研究数据驱动程序重构基本状态和组合规则，用来自领域适应视角的两个实证测试评估方法。

Result: 合成数据训练的测试集性能与原始临床时间序列数据相当，合成测试数据评估结果与原始测试数据相似，优于基于随机化的数据增强；SOFA 分数预测任务中，合成数据训练有显著性能提升。

Conclusion: 提出的方法有效，合成数据可用于临床时间序列预测，能缓解数据问题并提升性能。

Abstract: This work investigates whether time series of natural phenomena can be
understood as being generated by sequences of latent states which are ordered
in systematic and regular ways. We focus on clinical time series and ask
whether clinical measurements can be interpreted as being generated by
meaningful physiological states whose succession follows systematic principles.
Uncovering the underlying compositional structure will allow us to create
synthetic data to alleviate the notorious problem of sparse and low-resource
data settings in clinical time series forecasting, and deepen our understanding
of clinical data. We start by conceptualizing compositionality for time series
as a property of the data generation process, and then study data-driven
procedures that can reconstruct the elementary states and composition rules of
this process. We evaluate the success of this methods using two empirical tests
originating from a domain adaptation perspective. Both tests infer the
similarity of the original time series distribution and the synthetic time
series distribution from the similarity of expected risk of time series
forecasting models trained and tested on original and synthesized data in
specific ways. Our experimental results show that the test set performance
achieved by training on compositionally synthesized data is comparable to
training on original clinical time series data, and that evaluation of models
on compositionally synthesized test data shows similar results to evaluating on
original test data, outperforming randomization-based data augmentation. An
additional downstream evaluation of the prediction task of sequential organ
failure assessment (SOFA) scores shows significant performance gains when model
training is entirely based on compositionally synthesized data compared to
training on original data.

</details>


### [111] [Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning](https://arxiv.org/abs/2508.20697)
*Weitao Feng,Lixu Wang,Tianyi Wei,Jie Zhang,Chongyang Gao,Sinong Zhan,Peizhuo Lv,Wei Dong*

Main category: cs.LG

TL;DR: 研究表明强化学习（RL）用于微调大语言模型危害更大，提出TokenBuncher防御方法，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增长，微调带来的有害滥用风险增加，且RL比监督微调（SFT）危害更大，需有效防御。

Method: 提出TokenBuncher，通过熵作为奖励的RL和Token Noiser机制抑制模型响应不确定性。

Result: 跨多个模型和RL算法的实验表明，TokenBuncher能稳健减轻有害RL微调，同时保留良性任务效用和可微调性。

Conclusion: RL-based有害微调比SFT有更大系统风险，TokenBuncher是有效且通用的防御方法。

Abstract: As large language models (LLMs) continue to grow in capability, so do the
risks of harmful misuse through fine-tuning. While most prior studies assume
that attackers rely on supervised fine-tuning (SFT) for such misuse, we
systematically demonstrate that reinforcement learning (RL) enables adversaries
to more effectively break safety alignment and facilitate advanced harmful task
assistance, under matched computational budgets. To counter this emerging
threat, we propose TokenBuncher, the first effective defense specifically
targeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation
on which RL relies: model response uncertainty. By constraining uncertainty,
RL-based fine-tuning can no longer exploit distinct reward signals to drive the
model toward harmful behaviors. We realize this defense through
entropy-as-reward RL and a Token Noiser mechanism designed to prevent the
escalation of expert-domain harmful capabilities. Extensive experiments across
multiple models and RL algorithms show that TokenBuncher robustly mitigates
harmful RL fine-tuning while preserving benign task utility and finetunability.
Our results highlight that RL-based harmful fine-tuning poses a greater
systemic risk than SFT, and that TokenBuncher provides an effective and general
defense.

</details>


### [112] [EEGDM: Learning EEG Representation with Latent Diffusion Model](https://arxiv.org/abs/2508.20705)
*Shaocong Wang,Tong Liu,Ming Li,Minjing Yu,Yong-Jin Liu*

Main category: cs.LG

TL;DR: 现有EEG信号深度学习方法有局限，本文提出基于潜在扩散模型的自监督EEG表征学习方法EEGDM，实验显示其能重建信号、学习鲁棒表征且有泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有EEG信号分析的深度学习方法在学习可泛化表征方面有挑战，且现有表征学习方法不能充分捕捉EEG信号语义信息和复杂模式。

Method: 提出EEGDM方法，基于潜在扩散模型，利用EEG信号生成作为自监督目标，包含EEG编码器将信号及其通道增强信息提炼成紧凑表征以引导扩散模型生成信号。

Result: EEGDM能重建高质量EEG信号，有效学习鲁棒表征，在少量预训练数据下跨多种下游任务有有竞争力表现。

Conclusion: EEGDM具有泛化性和实际应用价值。

Abstract: While electroencephalography (EEG) signal analysis using deep learning has
shown great promise, existing approaches still face significant challenges in
learning generalizable representations that perform well across diverse tasks,
particularly when training data is limited. Current EEG representation learning
methods including EEGPT and LaBraM typically rely on simple masked
reconstruction objective, which may not fully capture the rich semantic
information and complex patterns inherent in EEG signals. In this paper, we
propose EEGDM, a novel self-supervised EEG representation learning method based
on the latent diffusion model, which leverages EEG signal generation as a
self-supervised objective, turning the diffusion model into a strong
representation learner capable of capturing EEG semantics. EEGDM incorporates
an EEG encoder that distills EEG signals and their channel augmentations into a
compact representation, acting as conditional information to guide the
diffusion model for generating EEG signals. This design endows EEGDM with a
compact latent space, which not only offers ample control over the generative
process but also can be leveraged for downstream tasks. Experimental results
show that EEGDM (1) can reconstruct high-quality EEG signals, (2) effectively
learns robust representations, and (3) achieves competitive performance with
modest pre-training data size across diverse downstream tasks, underscoring its
generalizability and practical utility.

</details>


### [113] [Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI](https://arxiv.org/abs/2508.20773)
*Christoforos N. Spartalis,Theodoros Semertzidis,Petros Daras,Efstratios Gavves*

Main category: cs.LG

TL;DR: 提出用于扩散模型机器遗忘的SAFEMax方法，结果显示其有效且高效。


<details>
  <summary>Details</summary>
Motivation: 提出新的扩散模型机器遗忘方法。

Method: 基于信息论原理，最大化生成图像熵，使模型在特定条件下生成高斯噪声；通过关注早期扩散步骤控制遗忘和保留的平衡。

Result: SAFEMax方法有效，且比现有方法有显著的效率提升。

Conclusion: SAFEMax是一种有效且高效的扩散模型机器遗忘方法。

Abstract: We introduce SAFEMax, a novel method for Machine Unlearning in diffusion
models. Grounded in information-theoretic principles, SAFEMax maximizes the
entropy in generated images, causing the model to generate Gaussian noise when
conditioned on impermissible classes by ultimately halting its denoising
process. Also, our method controls the balance between forgetting and retention
by selectively focusing on the early diffusion steps, where class-specific
information is prominent. Our results demonstrate the effectiveness of SAFEMax
and highlight its substantial efficiency gains over state-of-the-art methods.

</details>


### [114] [cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending](https://arxiv.org/abs/2508.20818)
*Anirudh Satheesh,Keenan Powell,Hua Wei*

Main category: cs.LG

TL;DR: 现有多智能体强化学习算法在真实场景易失效，提出cMALC - D框架，实验证明其优于现有课程学习基线。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习算法在复杂不确定真实场景下易失效，现有cMARL方法依赖不可靠代理信号。

Method: 提出cMALC - D框架，用大语言模型生成课程和评估信号，引入基于多样性的上下文混合机制。

Result: 在交通信号控制领域实验中，cMALC - D显著提高泛化性和样本效率。

Conclusion: cMALC - D框架有效，代码已开源。

Abstract: Many multi-agent reinforcement learning (MARL) algorithms are trained in
fixed simulation environments, making them brittle when deployed in real-world
scenarios with more complex and uncertain conditions. Contextual MARL (cMARL)
addresses this by parameterizing environments with context variables and
training a context-agnostic policy that performs well across all environment
configurations. Existing cMARL methods attempt to use curriculum learning to
help train and evaluate context-agnostic policies, but they often rely on
unreliable proxy signals, such as value estimates or generalized advantage
estimates that are noisy and unstable in multi-agent settings due to
inter-agent dynamics and partial observability. To address these issues, we
propose Contextual Multi-Agent LLM-Guided Curriculum Learning with
Diversity-Based Context Blending (cMALC-D), a framework that uses Large
Language Models (LLMs) to generate semantically meaningful curricula and
provide a more robust evaluation signal. To prevent mode collapse and encourage
exploration, we introduce a novel diversity-based context blending mechanism
that creates new training scenarios by combining features from prior contexts.
Experiments in traffic signal control domains demonstrate that cMALC-D
significantly improves both generalization and sample efficiency compared to
existing curriculum learning baselines. We provide code at
https://github.com/DaRL-LibSignal/cMALC-D.

</details>


### [115] [GPT-FT: An Efficient Automated Feature Transformation Using GPT for Sequence Reconstruction and Performance Enhancement](https://arxiv.org/abs/2508.20824)
*Yang Gao,Dongjie Wang,Scott Piersall,Ye Zhang,Liqiang Wang*

Main category: cs.LG

TL;DR: 提出新框架实现自动特征转换，降低参数规模、提升计算效率，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于连续嵌入优化的特征转换方法依赖顺序编解码结构，计算成本和参数需求高，可扩展性和效率受限。

Method: 提出四步框架实现自动特征转换，使用修订的GPT模型构建嵌入空间，实现特征转换序列重建和模型性能估计与提升。

Result: 在基准数据集上实验，该框架匹配或超越基线性能，计算效率显著提升。

Conclusion: 基于Transformer的架构在可扩展、高性能自动特征转换方面有潜力。

Abstract: Feature transformation plays a critical role in enhancing machine learning
model performance by optimizing data representations. Recent state-of-the-art
approaches address this task as a continuous embedding optimization problem,
converting discrete search into a learnable process. Although effective, these
methods often rely on sequential encoder-decoder structures that cause high
computational costs and parameter requirements, limiting scalability and
efficiency. To address these limitations, we propose a novel framework that
accomplishes automated feature transformation through four steps:
transformation records collection, embedding space construction with a revised
Generative Pre-trained Transformer (GPT) model, gradient-ascent search, and
autoregressive reconstruction. In our approach, the revised GPT model serves
two primary functions: (a) feature transformation sequence reconstruction and
(b) model performance estimation and enhancement for downstream tasks by
constructing the embedding space. Such a multi-objective optimization framework
reduces parameter size and accelerates transformation processes. Experimental
results on benchmark datasets show that the proposed framework matches or
exceeds baseline performance, with significant gains in computational
efficiency. This work highlights the potential of transformer-based
architectures for scalable, high-performance automated feature transformation.

</details>


### [116] [ATM-GAD: Adaptive Temporal Motif Graph Anomaly Detection for Financial Transaction Networks](https://arxiv.org/abs/2508.20829)
*Zeyue Zhang,Lin Song,Erkang Bao,Xiaoling Lv,Xinyue Wang*

Main category: cs.LG

TL;DR: 本文提出ATM - GAD自适应图神经网络用于金融欺诈检测，利用时间模式和自适应时间窗口，在四个真实数据集上表现优于七个基线模型。


<details>
  <summary>Details</summary>
Motivation: 现代金融系统中实体交织、交易行为多变，传统机器学习模型及现有基于图的检测器存在不足，未充分利用时间相关的欺诈特征。

Method: 引入ATM - GAD模型，通过时间模式提取器提取信息丰富的模式，用双注意力块分析模式，同时用自适应时间窗口学习器为每个节点定制观察窗口。

Result: 在四个真实数据集上，ATM - GAD始终优于七个强异常检测基线模型，发现了早期方法遗漏的欺诈模式。

Conclusion: ATM - GAD能有效利用时间相关的欺诈特征，在金融欺诈检测中表现良好。

Abstract: Financial fraud detection is essential to safeguard billions of dollars, yet
the intertwined entities and fast-changing transaction behaviors in modern
financial systems routinely defeat conventional machine learning models. Recent
graph-based detectors make headway by representing transactions as networks,
but they still overlook two fraud hallmarks rooted in time: (1) temporal
motifs--recurring, telltale subgraphs that reveal suspicious money flows as
they unfold--and (2) account-specific intervals of anomalous activity, when
fraud surfaces only in short bursts unique to each entity. To exploit both
signals, we introduce ATM-GAD, an adaptive graph neural network that leverages
temporal motifs for financial anomaly detection. A Temporal Motif Extractor
condenses each account's transaction history into the most informative motifs,
preserving both topology and temporal patterns. These motifs are then analyzed
by dual-attention blocks: IntraA reasons over interactions within a single
motif, while InterA aggregates evidence across motifs to expose multi-step
fraud schemes. In parallel, a differentiable Adaptive Time-Window Learner
tailors the observation window for every node, allowing the model to focus
precisely on the most revealing time slices. Experiments on four real-world
datasets show that ATM-GAD consistently outperforms seven strong
anomaly-detection baselines, uncovering fraud patterns missed by earlier
methods.

</details>


### [117] [Practical Physical Layer Authentication for Mobile Scenarios Using a Synthetic Dataset Enhanced Deep Learning Approach](https://arxiv.org/abs/2508.20861)
*Yijia Guo,Junqing Zhang,Y. -W. Peter Hong*

Main category: cs.LG

TL;DR: 提出基于深度学习的移动场景物理层CSI认证方案，通过仿真和实验评估，表现出良好泛化和认证性能，AUC优于对比算法。


<details>
  <summary>Details</summary>
Motivation: 物联网无线传输广播特性使设备认证脆弱，现有物理层认证方案缺乏适用于动态信道变化的实用方案。

Method: 基于WLAN TGn信道模型和信道自相关、距离相关生成合成训练数据集，使用基于CNN的孪生网络学习CSI对的时空相关性并输出相似度得分，采用仿真和实验协同评估方法。

Result: 仿真和实验评估表明方案有良好泛化和认证性能，AUC比FCN基孪生模型提高0.03，比基于相关的基准算法提高0.06。

Conclusion: 提出的基于深度学习的物理层CSI认证方案有效可行，表现优于对比算法。

Abstract: The Internet of Things (IoT) is ubiquitous thanks to the rapid development of
wireless technologies. However, the broadcast nature of wireless transmissions
results in great vulnerability to device authentication. Physical layer
authentication emerges as a promising approach by exploiting the unique channel
characteristics. However, a practical scheme applicable to dynamic channel
variations is still missing. In this paper, we proposed a deep learning-based
physical layer channel state information (CSI) authentication for mobile
scenarios and carried out comprehensive simulation and experimental evaluation
using IEEE 802.11n. Specifically, a synthetic training dataset was generated
based on the WLAN TGn channel model and the autocorrelation and the distance
correlation of the channel, which can significantly reduce the overhead of
manually collecting experimental datasets. A convolutional neural network
(CNN)-based Siamese network was exploited to learn the temporal and spatial
correlation between the CSI pair and output a score to measure their
similarity. We adopted a synergistic methodology involving both simulation and
experimental evaluation. The experimental testbed consisted of WiFi IoT
development kits and a few typical scenarios were specifically considered. Both
simulation and experimental evaluation demonstrated excellent generalization
performance of our proposed deep learning-based approach and excellent
authentication performance. Demonstrated by our practical measurement results,
our proposed scheme improved the area under the curve (AUC) by 0.03 compared to
the fully connected network-based (FCN-based) Siamese model and by 0.06
compared to the correlation-based benchmark algorithm.

</details>


### [118] [LeMat-Traj: A Scalable and Unified Dataset of Materials Trajectories for Atomistic Modeling](https://arxiv.org/abs/2508.20875)
*Ali Ramlaoui,Martin Siron,Inel Djafar,Joseph Musielewicz,Amandine Rossello,Victor Schmidt,Alexandre Duval*

Main category: cs.LG

TL;DR: 本文介绍了数据集LeMat - Traj和库LeMaterial - Fetcher，前者降低了训练可迁移且准确的机器学习原子间势（MLIPs）的门槛，后者为社区提供可重复框架。


<details>
  <summary>Details</summary>
Motivation: 量子力学轨迹数据集可用性分散、格式不一致限制了准确MLIPs的发展，且数据生成成本高、难以整合。

Method: 引入LeMat - Traj数据集，聚合超1.2亿原子构型，对数据表示进行标准化、结果进行协调和高质量构型筛选；开发LeMaterial - Fetcher库。

Result: 通过LeMat - Traj微调预训练模型，显著降低了弛豫任务上的力预测误差。

Conclusion: LeMat - Traj和LeMaterial - Fetcher公开可用，有助于MLIPs的发展和大规模材料数据集的持续发展。

Abstract: The development of accurate machine learning interatomic potentials (MLIPs)
is limited by the fragmented availability and inconsistent formatting of
quantum mechanical trajectory datasets derived from Density Functional Theory
(DFT). These datasets are expensive to generate yet difficult to combine due to
variations in format, metadata, and accessibility. To address this, we
introduce LeMat-Traj, a curated dataset comprising over 120 million atomic
configurations aggregated from large-scale repositories, including the
Materials Project, Alexandria, and OQMD. LeMat-Traj standardizes data
representation, harmonizes results and filters for high-quality configurations
across widely used DFT functionals (PBE, PBESol, SCAN, r2SCAN). It
significantly lowers the barrier for training transferrable and accurate MLIPs.
LeMat-Traj spans both relaxed low-energy states and high-energy, high-force
structures, complementing molecular dynamics and active learning datasets. By
fine-tuning models pre-trained on high-force data with LeMat-Traj, we achieve a
significant reduction in force prediction errors on relaxation tasks. We also
present LeMaterial-Fetcher, a modular and extensible open-source library
developed for this work, designed to provide a reproducible framework for the
community to easily incorporate new data sources and ensure the continued
evolution of large-scale materials datasets. LeMat-Traj and LeMaterial-Fetcher
are publicly available at https://huggingface.co/datasets/LeMaterial/LeMat-Traj
and https://github.com/LeMaterial/lematerial-fetcher.

</details>


### [119] [Turning Tabular Foundation Models into Graph Foundation Models](https://arxiv.org/abs/2508.20906)
*Dmitry Eremeev,Gleb Bazhenov,Oleg Platonov,Artem Babenko,Liudmila Prokhorenkova*

Main category: cs.LG

TL;DR: 本文提出图基础模型G2T - FM，利用TabPFNv2作为骨干，在图机器学习任务中表现出色，揭示了利用表格基础模型处理图数据的新方向。


<details>
  <summary>Details</summary>
Motivation: 基础模型在图机器学习领域应用待探索，设计图基础模型时处理不同图数据集的多样节点特征是关键挑战，此前未充分解决任意类型特征处理问题，受表格基础模型TabPFNv2成功启发。

Method: 提出G2T - FM模型，用邻域特征聚合增强原始节点特征，添加结构嵌入，再将TabPFNv2应用于构建的节点表示。

Result: 在全上下文机制下表现强劲，显著超越公开可用的图基础模型，与从头训练的调优GNN相当；微调后超越调优GNN基线。

Conclusion: 揭示了利用表格基础模型进行图机器学习任务这一被忽视的方向，证明了所提方法的潜力。

Abstract: While foundation models have revolutionized such fields as natural language
processing and computer vision, their application and potential within graph
machine learning remain largely unexplored. One of the key challenges in
designing graph foundation models (GFMs) is handling diverse node features that
can vary across different graph datasets. Although many works on GFMs have been
focused exclusively on text-attributed graphs, the problem of handling
arbitrary features of other types in GFMs has not been fully addressed.
However, this problem is not unique to the graph domain, as it also arises in
the field of machine learning for tabular data. In this work, motivated by the
recent success of tabular foundation models like TabPFNv2, we propose G2T-FM, a
simple graph foundation model that employs TabPFNv2 as a backbone.
Specifically, G2T-FM augments the original node features with neighborhood
feature aggregation, adds structural embeddings, and then applies TabPFNv2 to
the constructed node representations. Even in a fully in-context regime, our
model achieves strong results, significantly outperforming publicly available
GFMs and performing on par with well-tuned GNNs trained from scratch. Moreover,
after finetuning, G2T-FM surpasses well-tuned GNN baselines, highlighting the
potential of the proposed approach. More broadly, our paper reveals a
previously overlooked direction of utilizing tabular foundation models for
graph machine learning tasks.

</details>


### [120] [Finite-Time Guarantees for Multi-Agent Combinatorial Bandits with Nonstationary Rewards](https://arxiv.org/abs/2508.20923)
*Katherine B. Adams,Justin J. Boutilier,Qinyang He,Yonatan Mintz*

Main category: cs.LG

TL;DR: 研究无个体效应先验知识的顺序资源分配问题，引入含非平稳奖励的框架，开发算法并通过案例验证其效果。


<details>
  <summary>Details</summary>
Motivation: 解决如社区健康干预等场景中，干预效果动态变化且奖励分布非平稳的资源分配问题。

Method: 引入含非平稳奖励的组合多臂老虎机框架，开发有动态遗憾理论保证的算法。

Result: 个性化社区干预算法在项目注册方面比基线方法最多提高三倍。

Conclusion: 该工作将自适应学习理论进展与人群行为改变干预实际挑战相衔接。

Abstract: We study a sequential resource allocation problem where a decision maker
selects subsets of agents at each period to maximize overall outcomes without
prior knowledge of individual-level effects. Our framework applies to settings
such as community health interventions, targeted digital advertising, and
workforce retention programs, where intervention effects evolve dynamically.
Agents may exhibit habituation (diminished response from frequent selection) or
recovery (enhanced response from infrequent selection). The technical challenge
centers on nonstationary reward distributions that lead to changing
intervention effects over time. The problem requires balancing two key
competing objectives: heterogeneous individual rewards and the
exploration-exploitation tradeoff in terms of learning for improved future
decisions as opposed to maximizing immediate outcomes. Our contribution
introduces the first framework incorporating this form of nonstationary rewards
in the combinatorial multi-armed bandit literature. We develop algorithms with
theoretical guarantees on dynamic regret and demonstrate practical efficacy
through a diabetes intervention case study. Our personalized community
intervention algorithm achieved up to three times as much improvement in
program enrollment compared to baseline approaches, validating the framework's
potential for real-world applications. This work bridges theoretical advances
in adaptive learning with practical challenges in population-level behavioral
change interventions.

</details>


### [121] [Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees](https://arxiv.org/abs/2508.21001)
*Yaniv Hassidof,Tom Jurgenson,Kiril Solovey*

Main category: cs.LG

TL;DR: 提出Diffusion Tree (DiTree)框架，结合扩散策略和采样规划器，在复杂动力学系统中高效生成安全解，在OOD场景中速度和成功率表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有采样规划器探索慢，基于学习的方法无法泛化到OOD场景且缺乏关键保证，限制其在物理机器人上的应用。

Method: 提出DiTree框架，将扩散策略作为有信息采样器，结合其对专家轨迹分布建模能力和采样规划器的完备性。

Result: 实现结合RRT规划器和DP采样器的DiTree，在OOD场景评估中速度比经典采样规划器快3倍，成功率比其他方法高约30%。

Conclusion: DiTree框架能有效解决动力学运动规划问题，在复杂场景中具有良好的泛化性和安全性。

Abstract: Kinodynamic motion planning is concerned with computing collision-free
trajectories while abiding by the robot's dynamic constraints. This critical
problem is often tackled using sampling-based planners (SBPs) that explore the
robot's high-dimensional state space by constructing a search tree via action
propagations. Although SBPs can offer global guarantees on completeness and
solution quality, their performance is often hindered by slow exploration due
to uninformed action sampling. Learning-based approaches can yield
significantly faster runtimes, yet they fail to generalize to
out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety,
thus limiting their deployment on physical robots. We present Diffusion Tree
(DiTree): a \emph{provably-generalizable} framework leveraging diffusion
policies (DPs) as informed samplers to efficiently guide state-space search
within SBPs. DiTree combines DP's ability to model complex distributions of
expert trajectories, conditioned on local observations, with the completeness
of SBPs to yield \emph{provably-safe} solutions within a few action propagation
iterations for complex dynamical systems. We demonstrate DiTree's power with an
implementation combining the popular RRT planner with a DP action sampler
trained on a \emph{single environment}. In comprehensive evaluations on OOD
scenarios, % DiTree has comparable runtimes to a standalone DP (3x faster than
classical SBPs), while improving the average success rate over DP and SBPs.
DiTree is on average 3x faster than classical SBPs, and outperforms all other
approaches by achieving roughly 30\% higher success rate. Project webpage:
https://sites.google.com/view/ditree.

</details>


### [122] [InSQuAD: In-Context Learning for Efficient Retrieval via Submodular Mutual Information to Enforce Quality and Diversity](https://arxiv.org/abs/2508.21003)
*Souradeep Nanda,Anay Majee,Rishabh Iyer*

Main category: cs.LG

TL;DR: 本文介绍InSQuAD，通过子模互信息（SMI）提升上下文学习（ICL）模型性能，在九个基准数据集上验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 提升ICL模型性能，解决现有检索模型忽视多样性的问题。

Method: 将ICL任务建模为目标选择问题，引入基于SMI的统一选择策略；采用组合训练范式，通过新的似然损失学习SMI函数参数；用合成释义扩充多跳问答数据集。

Result: 在九个基准数据集上采用该策略训练的检索模型和目标选择公式，显示出显著改进。

Conclusion: 所提出的方法有效提升了ICL模型性能。

Abstract: In this paper, we introduce InSQuAD, designed to enhance the performance of
In-Context Learning (ICL) models through Submodular Mutual Information} (SMI)
enforcing Quality and Diversity among in-context exemplars. InSQuAD achieves
this through two principal strategies: First, we model the ICL task as a
targeted selection problem and introduce a unified selection strategy based on
SMIs which mines relevant yet diverse in-context examples encapsulating the
notions of quality and diversity. Secondly, we address a common pitfall in
existing retrieval models which model query relevance, often overlooking
diversity, critical for ICL. InSQuAD introduces a combinatorial training
paradigm which learns the parameters of an SMI function to enforce both quality
and diversity in the retrieval model through a novel likelihood-based loss. To
further aid the learning process we augment an existing multi-hop question
answering dataset with synthetically generated paraphrases. Adopting the
retrieval model trained using this strategy alongside the novel targeted
selection formulation for ICL on nine benchmark datasets shows significant
improvements validating the efficacy of our approach.

</details>


### [123] [Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance](https://arxiv.org/abs/2508.21016)
*Luozhijie Jin,Zijie Qiu,Jie Liu,Zijie Diao,Lifeng Qiao,Ning Ding,Alex Lamb,Xipeng Qiu*

Main category: cs.LG

TL;DR: 提出推理时方法RLG提升扩散模型对齐效果与灵活性，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于去噪的生成模型输出分布与复杂下游目标对齐有挑战，当前强化学习微调方法欠佳。

Method: 从随机微分方程和隐式奖励条件角度重新解释扩散模型的强化学习微调，提出推理时方法RLG，结合基础模型和强化学习微调模型输出。

Result: 理论分析表明RLG引导尺度等价于调整标准强化学习目标的KL正则化系数，实验证明其能提升不同架构、算法和任务表现，支持插值和外推。

Conclusion: RLG为推理时增强和控制扩散模型对齐提供实用且理论合理的解决方案。

Abstract: Denoising-based generative models, particularly diffusion and flow matching
algorithms, have achieved remarkable success. However, aligning their output
distributions with complex downstream objectives, such as human preferences,
compositional accuracy, or data compressibility, remains challenging. While
reinforcement learning (RL) fine-tuning methods, inspired by advances in RL
from human feedback (RLHF) for large language models, have been adapted to
these generative frameworks, current RL approaches are suboptimal for diffusion
models and offer limited flexibility in controlling alignment strength after
fine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models
through the lens of stochastic differential equations and implicit reward
conditioning. We introduce Reinforcement Learning Guidance (RLG), an
inference-time method that adapts Classifier-Free Guidance (CFG) by combining
the outputs of the base and RL fine-tuned models via a geometric average. Our
theoretical analysis shows that RLG's guidance scale is mathematically
equivalent to adjusting the KL-regularization coefficient in standard RL
objectives, enabling dynamic control over the alignment-quality trade-off
without further training. Extensive experiments demonstrate that RLG
consistently improves the performance of RL fine-tuned models across various
architectures, RL algorithms, and downstream tasks, including human
preferences, compositional control, compressibility, and text rendering.
Furthermore, RLG supports both interpolation and extrapolation, thereby
offering unprecedented flexibility in controlling generative alignment. Our
approach provides a practical and theoretically sound solution for enhancing
and controlling diffusion model alignment at inference. The source code for RLG
is publicly available at the Github:
https://github.com/jinluo12345/Reinforcement-learning-guidance.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [124] [Particle swarm optimization for online sparse streaming feature selection under uncertainty](https://arxiv.org/abs/2508.20123)
*Ruiyang Xu*

Main category: cs.NE

TL;DR: 针对高维流数据特征选择中数据不完整及特征 - 标签相关性不确定问题，提出基于粒子群优化的不确定性感知在线稀疏流特征选择框架POS2FS，实验证明其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有在线稀疏流特征选择方法在处理不确定特征 - 标签相关性时存在问题，导致模型不灵活和性能下降。

Method: 提出POS2FS框架，引入PSO驱动的监督以减少特征 - 标签关系的不确定性，采用三向决策理论处理监督学习中的特征模糊性。

Result: 在六个真实数据集上的严格测试表明，POS2FS通过更稳健的特征子集选择，实现了比传统OSFS和OS2FS技术更高的准确性。

Conclusion: POS2FS在处理高维流数据特征选择问题上优于传统方法。

Abstract: In real-world applications involving high-dimensional streaming data, online
streaming feature selection (OSFS) is widely adopted. Yet, practical
deployments frequently face data incompleteness due to sensor failures or
technical constraints. While online sparse streaming feature selection (OS2FS)
mitigates this issue via latent factor analysis-based imputation, existing
methods struggle with uncertain feature-label correlations, leading to
inflexible models and degraded performance. To address these gaps, this work
proposes POS2FS-an uncertainty-aware online sparse streaming feature selection
framework enhanced by particle swarm optimization (PSO). The approach
introduces: 1) PSO-driven supervision to reduce uncertainty in feature-label
relationships; 2) Three-way decision theory to manage feature fuzziness in
supervised learning. Rigorous testing on six real-world datasets confirms
POS2FS outperforms conventional OSFS and OS2FS techniques, delivering higher
accuracy through more robust feature subset selection.

</details>


### [125] [Improving Liver Disease Diagnosis with SNNDeep: A Custom Spiking Neural Network Using Diverse Learning Algorithms](https://arxiv.org/abs/2508.20125)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.NE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Purpose: Spiking neural networks (SNNs) have recently gained attention as
energy-efficient, biologically plausible alternatives to conventional deep
learning models. Their application in high-stakes biomedical imaging remains
almost entirely unexplored. Methods: This study introduces SNNDeep, the first
tailored SNN specifically optimized for binary classification of liver health
status from computed tomography (CT) features. To ensure clinical relevance and
broad generalizability, the model was developed and evaluated using the
Task03\Liver dataset from the Medical Segmentation Decathlon (MSD), a
standardized benchmark widely used for assessing performance across diverse
medical imaging tasks. We benchmark three fundamentally different learning
algorithms, namely Surrogate Gradient Learning, the Tempotron rule, and
Bio-Inspired Active Learning across three architectural variants: a fully
customized low-level model built from scratch, and two implementations using
leading SNN frameworks, i.e., snnTorch and SpikingJelly. Hyperparameter
optimization was performed using Optuna. Results: Our results demonstrate that
the custom-built SNNDeep consistently outperforms framework-based
implementations, achieving a maximum validation accuracy of 98.35%, superior
adaptability across learning rules, and significantly reduced training
overhead. Conclusion:This study provides the first empirical evidence that
low-level, highly tunable SNNs can surpass standard frameworks in medical
imaging, especially in data-limited, temporally constrained diagnostic
settings, thereby opening a new pathway for neuro-inspired AI in precision
medicine.

</details>


### [126] [Spatio-Temporal Pruning for Compressed Spiking Large Language Models](https://arxiv.org/abs/2508.20122)
*Yi Jiang,Malyaban Bal,Brian Matejek,Susmit Jha,Adam Cobb,Abhronil Sengupta*

Main category: cs.NE

TL;DR: 大语言模型在能源受限环境部署有挑战，本文提出时空剪枝框架优化脉冲大语言模型计算效率与性能，并结合其他压缩技术，实验证明其有效性，适用于低功耗自然语言处理应用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因模型大、推理延迟高，在能源受限环境部署困难，脉冲神经网络有低功耗计算潜力，需将其与大语言模型结合以实现节能。

Method: 从脉冲神经网络角度重新审视时空剪枝，提出时空剪枝框架，结合其他压缩技术。空间剪枝减少活跃神经元和注意力头数量，时间剪枝动态调整层所需时间步。

Result: 在大规模GLUE基准上对SpikingBERT的实验表明，所提方法在计算操作和推理延迟方面有效。

Conclusion: 该方法为实时、低功耗自然语言处理应用提供解决方案，使脉冲大语言模型更适合在边缘设备和功率受限环境部署。

Abstract: Large Language Models (LLMs) present significant challenges for deployment in
energy-constrained environments due to their large model sizes and high
inference latency. Spiking Neural Networks (SNNs), inspired by the sparse
event-driven neural processing and energy-efficient information transmission in
the brain, offer a promising alternative for achieving low-power computing.
Integrating the event-driven efficiency of spiking neurons with the advanced
capabilities of LLMs represents a promising direction for power-efficient LLMs.
This work specifically delves into the design of compressed spiking LLMs. Here,
we revisit spatial and temporal pruning from the perspective of SNNs and
propose a novel spatio-temporal pruning framework for Spiking LLMs to optimize
computational efficiency while preserving high performance. Our spatial pruning
technique reduces the number of active neurons and attention heads, effectively
lowering the computational complexity of the model. Meanwhile, temporal pruning
minimizes inference latency by dynamically adjusting the number of timesteps
required for different layers. By combining these approaches with other
compression techniques, we present the first work in the domain of Spiking LLMs
to jointly explore spatial pruning, temporal pruning, extreme quantization and
knowledge distillation strategies. Extensive experimental evaluation of our
proposed framework for SpikingBERT on the large-scale GLUE benchmark
demonstrates the efficacy of our approach in terms of computational operations
and inference latency. Our approach offers a compelling solution for real-time,
low-power natural language processing applications, making Spiking LLMs more
practical for deployment on edge devices and in power-constrained settings.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [127] [Evaluating LLMs on microservice-based applications: how complex is your specification?](https://arxiv.org/abs/2508.20119)
*Daniel M. Yellin*

Main category: cs.SE

TL;DR: 评估大语言模型（LLMs）为现实世界问题生成代码的能力，聚焦微服务应用，提出难度度量标准和测试框架，实验表明强LLMs处理高难度规范效果差，并分析问题提出研究方向。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在为现实世界问题生成代码方面的进展，尤其是微服务应用场景。

Method: 定义微服务应用规范模板，提出规范难度度量标准，开发使用单元测试自动化测试LLM合成代码的框架。

Result: 强LLMs在中等难度规范上表现尚可，但在高难度规范上表现很差，原因包括复杂业务逻辑、更多外部服务使用等。

Conclusion: 分析了LLM合成代码中的错误，指出LLMs在生成代码时面临的关键挑战，并为未来改进现实世界问题的代码合成提出研究方向。

Abstract: In this paper we evaluate how far LLMs have advanced in generating code for
real-world problems. Specifically, we explore code synthesis for
microservice-based applications, a widely used architecture pattern. We define
a standard template for specifying these applications, and we propose a metric
for judging the difficulty level of a specification. The higher the score, the
more difficult it is to generate code for the specification. We develop a
framework to automate the process of testing LLM-synthesized code for a
microservice using unit tests. Our experimental results show that strong LLMs
(like GPT-3o-mini) do fairly well on medium difficulty specifications but do
very poorly on those of higher difficulty levels. This is due to more intricate
business logic, a greater use of external services, database integration and
inclusion of non-functional capabilities such as authentication. We analyzed
the errors in LLM-synthesized code and report on the key challenges LLMs face
in generating code for these specifications thereby suggesting future research
directions to improve code synthesis for real-world problems.

</details>


### [128] [Towards Better Correctness and Efficiency in Code Generation](https://arxiv.org/abs/2508.20124)
*Yunlong Feng,Yang Xu,Xiao Xu,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 提出效率导向强化学习框架及两阶段调优方法，提升代码正确性与运行效率。


<details>
  <summary>Details</summary>
Motivation: 解决代码大语言模型生成代码运行效率低，限制其在性能敏感场景应用的问题。

Method: 提出效率导向强化学习框架，采用动态探索、误差不敏感强化学习方法和高对比度效率信号，提出两阶段调优方法。

Result: 实验表明，该方法在7B模型上使代码正确性提高10.18%，运行效率提高7.75%，性能与更大模型相当。

Conclusion: 所提方法有效，能在正确性和效率上实现高且平衡的性能。

Abstract: While code large language models have demonstrated remarkable progress in
code generation, the generated code often exhibits poor runtime efficiency,
limiting its practical application in performance-sensitive scenarios. To
address this limitation, we propose an efficiency-oriented reinforcement
learning framework guided by a novel performance reward. Based on this
framework, we take a deeper dive into the code efficiency problem, identifying
then proposing methods to overcome key bottlenecks: (1) Dynamic exploration
overcomes the static data constraints of offline fine-tuning, enabling the
discovery of more efficient code implementations. (2) The error-insensitive
reinforcement learning method and high-contrast efficiency signals are crucial
for mitigating systematic errors and achieving effective optimization. (3)
Online exploration is most effective when starting from a high-correctness
baseline, as this allows for efficiency improvements without sacrificing
accuracy. With these discoveries, we finally propose a two-stage tuning method,
which achieves high and balanced performance across correctness and efficiency.
The results of experiments show the effectiveness of the method, which improves
code correctness by 10.18\% and runtime efficiency by 7.75\% on a 7B model,
achieving performance comparable to much larger model.

</details>


### [129] [Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators](https://arxiv.org/abs/2508.20340)
*Maolin Sun,Yibiao Yang,Yuming Zhou*

Main category: cs.SE

TL;DR: 本文提出Chimera框架解决SMT求解器测试问题，减少运行成本，在Z3和cvc5上发现43个确认漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有SMT求解器测试技术难以跟上求解器功能发展，基于LLM的方法存在公式语法无效和计算开销大的问题。

Method: 提出Chimera框架，利用LLM提取CFG并合成布尔项生成器，用生成的项填充结构骨架。

Result: 在Z3和cvc5上评估，发现43个确认漏洞，40个已修复。

Conclusion: Chimera能保证语法有效性和语义多样性，大幅降低运行成本，有效发现SMT求解器的漏洞。

Abstract: Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems
and programming languages research, providing the foundation for tasks like
symbolic execution and automated verification. Because these solvers sit on the
critical path, their correctness is essential, and high-quality test formulas
are key to uncovering bugs. However, while prior testing techniques performed
well on earlier solver versions, they struggle to keep pace with rapidly
evolving features. Recent approaches based on Large Language Models (LLMs) show
promise in exploring advanced solver capabilities, but two obstacles remain:
nearly half of the generated formulas are syntactically invalid, and iterative
interactions with the LLMs introduce substantial computational overhead. In
this study, we present Chimera, a novel LLM-assisted fuzzing framework that
addresses both issues by shifting from direct formula generation to the
synthesis of reusable term (i.e., logical expression) generators. Particularly,
Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for
SMT theories, including solver-specific extensions, from documentation, and (2)
synthesize composable Boolean term generators that adhere to these grammars.
During fuzzing, Chimera populates structural skeletons derived from existing
formulas with the terms iteratively produced by the LLM-synthesized generators.
This design ensures syntactic validity while promoting semantic diversity.
Notably, Chimera requires only one-time LLM interaction investment,
dramatically reducing runtime cost. We evaluated Chimera on two leading SMT
solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43
confirmed bugs, 40 of which have already been fixed by developers.

</details>


### [130] [Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought](https://arxiv.org/abs/2508.20370)
*Lingzhe Zhang,Tong Jia,Kangjin Wang,Weijie Hong,Chiming Duan,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: 本文研究微服务系统故障根因定位，发现人工分析特点，提出RCLAgent方法，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当代微服务系统故障频发，现有根因定位方法依赖预定义模式或缺乏可解释性，需要准确的根因定位方法。

Method: 通过研究多个组织的专业SRE定位故障根因的方式，发现人工根因分析的三个特点，引入基于多智能体思维递归框架的RCLAgent方法，利用思维递归策略引导大语言模型推理。

Result: 在各种公共数据集上的实验表明，RCLAgent仅用单个请求就能定位根因，性能优于依赖多个请求聚合的现有方法。

Conclusion: RCLAgent能有效提高复杂微服务环境中根因定位的效率和精度。

Abstract: As contemporary microservice systems become increasingly popular and
complex-often comprising hundreds or even thousands of fine-grained,
interdependent subsystems-they are facing more frequent failures. Ensuring
system reliability thus demands accurate root cause localization. While traces
and metrics have proven to be effective data sources for this task, existing
methods either heavily rely on pre-defined schemas, which struggle to adapt to
evolving operational contexts, or lack interpretability in their reasoning
process, thereby leaving Site Reliability Engineers (SREs) confused. In this
paper, we conduct a comprehensive study on how SREs localize the root cause of
failures, drawing insights from multiple professional SREs across different
organizations. Our investigation reveals that human root cause analysis
exhibits three key characteristics: recursiveness, multi-dimensional expansion,
and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,
an adaptive root cause localization method for microservice systems that
leverages a multi-agent recursion-of-thought framework. RCLAgent employs a
novel recursion-of-thought strategy to guide the LLM's reasoning process,
effectively integrating data from multiple agents and tool-assisted analysis to
accurately pinpoint the root cause. Experimental evaluations on various public
datasets demonstrate that RCLAgent achieves superior performance by localizing
the root cause using only a single request-outperforming state-of-the-art
methods that depend on aggregating multiple requests. These results underscore
the effectiveness of RCLAgent in enhancing the efficiency and precision of root
cause localization in complex microservice environments.

</details>


### [131] [AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop](https://arxiv.org/abs/2508.20563)
*Zheying Zhang,Tomas Herda,Victoria Pichler,Pekka Abrahamsson,Geir K. Hanssen,Joshua Kerievsky,Alex Polyakov,Mohit Chandna,Marius Irgens,Kai-Kristian Kemell,Ayman Asad Khan,Crystal Kwok,Evan Leybourn,Munish Malik,Dorota Mleczko,Morteza Moalagh,Christopher Morales,Yuliia Pieskova,Daniel Planötscher,Mika Saari,Anastasiia Tkalich,Karl Josef Gstettner,Xiaofeng Wang*

Main category: cs.SE

TL;DR: 本文总结XP2025研讨会关于‘AI与敏捷：从挫折到成功’的关键成果，该研讨会探讨生成式AI与敏捷软件开发结合的挑战与机遇，共创研究路线图。


<details>
  <summary>Details</summary>
Motivation: 应对生成式AI与敏捷软件开发结合面临的具体挑战，挖掘新兴机遇。

Method: 组织30多名跨学科人员参与研讨会，通过结构化互动分组讨论确定痛点，分析潜在原因和交叉问题。

Result: 确定工具碎片化、治理、数据质量、技能差距等痛点，共创多主题研究路线图。

Conclusion: 该路线图能指导未来研究，推动生成式AI以负责任、以人为本的方式融入敏捷实践。

Abstract: This paper synthesizes the key findings from a full-day XP2025 workshop on
"AI and Agile: From Frustration to Success", held in Brugg-Windisch,
Switzerland. The workshop brought together over 30 interdisciplinary academic
researchers and industry practitioners to tackle the concrete challenges and
emerging opportunities at the intersection of Generative Artificial
Intelligence (GenAI) and agile software development. Through structured,
interactive breakout sessions, participants identified shared pain points like
tool fragmentation, governance, data quality, and critical skills gaps in AI
literacy and prompt engineering. These issues were further analyzed, revealing
underlying causes and cross-cutting concerns. The workshop concluded by
collaboratively co-creating a multi-thematic research roadmap, articulating
both short-term, implementable actions and visionary, long-term research
directions. This cohesive agenda aims to guide future investigation and drive
the responsible, human-centered integration of GenAI into agile practices.

</details>


### [132] [Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol](https://arxiv.org/abs/2508.20737)
*Wei Ma,Yixiao Yang,Qiang Hu,Shi Ying,Zhi Jin,Bo Du,Zhenchang Xing,Tianlin Li,Junjie Shi,Yang Liu,Linxiao Jiang*

Main category: cs.SE

TL;DR: 本文将大语言模型应用分解为三层架构，评估传统软件测试方法适用性，分析测试方法差异，提出协作策略、质量保证框架、实用指南及协议AICL。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用的非确定性、动态性和上下文依赖性给质量保证带来挑战，需要有效测试方法。

Method: 将LLM应用分解为三层架构，评估传统软件测试方法适用性，对比软件工程和AI社区的测试方法，提出协作策略和质量保证框架。

Result: 发现测试方法的结构脱节，确定4个根本差异和6个核心挑战，提出4种协作策略和AICL协议。

Conclusion: 所提策略和框架可支持LLM应用测试的标准化和工具化，AICL协议便于集成和测试。

Abstract: Applications of Large Language Models~(LLMs) have evolved from simple text
generators into complex software systems that integrate retrieval augmentation,
tool invocation, and multi-turn interactions. Their inherent non-determinism,
dynamism, and context dependence pose fundamental challenges for quality
assurance. This paper decomposes LLM applications into a three-layer
architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt
Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess
the applicability of traditional software testing methods in each layer:
directly applicable at the shell layer, requiring semantic reinterpretation at
the orchestration layer, and necessitating paradigm shifts at the inference
core. A comparative analysis of Testing AI methods from the software
engineering community and safety analysis techniques from the AI community
reveals structural disconnects in testing unit abstraction, evaluation metrics,
and lifecycle management. We identify four fundamental differences that
underlie 6 core challenges. To address these, we propose four types of
collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate},
and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance
framework that combines pre-deployment validation with runtime monitoring.
Based on these strategies, we offer practical guidance and a protocol proposal
to support the standardization and tooling of LLM application testing. We
propose a protocol \textbf{\textit{Agent Interaction Communication Language}}
(AICL) that is used to communicate between AI agents. AICL has the
test-oriented features and is easily integrated in the current agent framework.

</details>


### [133] [From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations](https://arxiv.org/abs/2508.20744)
*Shabnam Hassani,Mehrdad Sabetzadeh,Daniel Amyot*

Main category: cs.SE

TL;DR: 本文通过准实验研究LLMs从法律文本推导行为规范的能力，结果表明LLMs能生成高质量规范，减少人工工作。


<details>
  <summary>Details</summary>
Motivation: 法律法规影响软件设计和质量保证，人工创建合规工件存在问题，GenAI提供自动化途径，需研究LLMs从法律文本推导行为规范的能力。

Method: 十名参与者评估Claude和Llama根据食品安全法规生成的规范，用Gherkin语言生成60个规范，每个参与者按五项标准评估12个，每个规范由两人评估。

Result: 各项标准高评分占比高，无最低评分，Mann - Whitney U检验表明参与者和模型间无显著差异，Llama和Claude各有优势，反馈指出存在幻觉和遗漏但肯定规范有用。

Conclusion: LLMs能从法律文本生成高质量Gherkin规范，减少人工并提供用于实施、保证和测试生成的结构化工件。

Abstract: Context: Laws and regulations increasingly affect software design and quality
assurance, but legal texts are written in technology-neutral language. This
creates challenges for engineers who must develop compliance artifacts such as
requirements and acceptance criteria. Manual creation is labor-intensive,
error-prone, and requires domain expertise. Advances in Generative AI (GenAI),
especially Large Language Models (LLMs), offer a way to automate deriving such
artifacts.
  Objective: We present the first systematic human-subject study of LLMs'
ability to derive behavioral specifications from legal texts using a
quasi-experimental design. These specifications translate legal requirements
into a developer-friendly form.
  Methods: Ten participants evaluated specifications generated from food-safety
regulations by Claude and Llama. Using Gherkin, a structured BDD language, 60
specifications were produced. Each participant assessed 12 across five
criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each
specification was reviewed by two participants, yielding 120 assessments.
  Results: For Relevance, 75% of ratings were highest and 20% second-highest.
Clarity reached 90% highest. Completeness: 75% highest, 19% second.
Singularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No
lowest ratings occurred. Mann-Whitney U tests showed no significant differences
across participants or models. Llama slightly outperformed Claude in Clarity,
Completeness, and Time Savings, while Claude was stronger in Singularity.
Feedback noted hallucinations and omissions but confirmed the utility of the
specifications.
  Conclusion: LLMs can generate high-quality Gherkin specifications from legal
texts, reducing manual effort and providing structured artifacts useful for
implementation, assurance, and test generation.

</details>


### [134] [Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry](https://arxiv.org/abs/2508.20774)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: 本文提出可持续性视角愿景，用于解决软件设计阶段的可持续性问题，通过文献和专家聚焦小组得出结论，证实元素相关性并提出塑造视角的启示。


<details>
  <summary>Details</summary>
Motivation: 软件架构师在软件设计阶段缺乏有效解决可持续性的结构化指导，而架构视角是有前景的方法。

Method: 对开创性文献进行滚雪球式研究，并与该领域专家开展聚焦小组。

Result: 确认了不同视角元素在实践中的相关性。

Conclusion: 提出塑造满足工业需求的可持续性视角的启示。

Abstract: Sustainability is increasingly recognized as an emerging quality property in
software-intensive systems, yet architects lack structured guidance to address
it effectively throughout the software design phase. Architectural
perspectives-an architectural knowledge artifact composed of concerns,
activities, tactics, pitfalls, and checklists-offer a promising approach to
tackle such emerging quality properties across architectural views and are also
independent of architecture frameworks and industry contexts. In this paper, we
present a sustainability perspective vision, i.e., a revised notion of
architectural perspective meant to be filled with its own elements to target
sustainability concerns. We formulate our sustainability perspective vision
through evidence from applying snowballing to seminal literature and from
conducting a focus group with experts in the field. Our findings confirm the
relevance of the different perspective elements in practice and highlight
implications for shaping a sustainability perspective that meets industrial
needs.

</details>


### [135] [Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation](https://arxiv.org/abs/2508.20902)
*Baharin A. Jodat,Khouloud Gaaloul,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: 本文提出CPS基于断言的测试预言机，介绍两种生成方法并评估，发现用Ochiai的GP生成的预言机更准确且抗抖动。


<details>
  <summary>Details</summary>
Motivation: 基于仿真的CPS测试成本高且模拟器可能不稳定，需要无需系统执行的自动化测试预言机。

Method: 提出基于断言的测试预言机，用遗传编程（GP）结合谱基故障定位公式和决策树（DT）、决策规则（DR）两种方法生成。

Result: 用Ochiai的GP生成的测试预言机比其他方法更准确，且在考虑系统抖动时仍有优势，平均准确率变化仅4%。

Conclusion: 基于断言的测试预言机可降低测试成本，用Ochiai的GP生成的预言机准确且抗抖动。

Abstract: Simulation-based testing of cyber-physical systems (CPS) is costly due to the
time-consuming execution of CPS simulators. In addition, CPS simulators may be
flaky, leading to inconsistent test outcomes and requiring repeated test
re-execution for reliable test verdicts. Automated test oracles that do not
require system execution are therefore crucial for reducing testing costs.
Ideally, such test oracles should be interpretable to facilitate human
understanding of test verdicts, and they must be robust against the potential
flakiness of CPS simulators. In this article, we propose assertion-based test
oracles for CPS as sets of logical and arithmetic predicates defined over the
inputs of the system under test. Given a test input, our assertion-based test
oracle determines, without requiring test execution, whether the test passes,
fails, or if the oracle is inconclusive in predicting a verdict. We describe
two methods for generating assertion-based test oracles: one using genetic
programming~(GP) that employs well-known spectrum-based fault localization
(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness
functions; and the other using decision trees (DT) and decision rules (DR). We
evaluate our assertion-based test oracles through case studies in the domains
of aerospace, networking and autonomous driving. We show that test oracles
generated using GP with Ochiai are significantly more accurate than those
obtained using GP with Tarantula and Naish or using DT or DR. Moreover, this
accuracy advantage remains even when accounting for the flakiness of the system
under test. We further show that the assertion-based test oracles generated by
GP with Ochiai are robust against flakiness with only 4% average variation in
their accuracy results across four different network and autonomous driving
systems with flaky behaviours.

</details>


### [136] [Deep Learning Based Concurrency Bug Detection and Localization](https://arxiv.org/abs/2508.20911)
*Zuocheng Feng,Kaiwen Zhang,Miaomiao Wang,Yiming Cheng,Yuandao Cai,Xiaofeng Li,Guanjun Liu*

Main category: cs.SE

TL;DR: 现有深度学习方法检测并发错误存在数据、语义表示和调试信息不足问题，本文提出新方法，构建数据集，结合预训练模型和GNN，用SubgraphX定位错误，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法检测并发错误存在缺少专用数据集、缺乏并发语义表示、无法提供细粒度调试信息等问题，影响软件可靠性和安全性。

Method: 构建专用并发错误数据集，将预训练模型与异构图神经网络结合，引入CCPG表征并发语义，使用SubgraphX定位错误。

Result: 在不同评估设置下，该方法准确率和精确率平均提高10%，召回率提高26%。

Conclusion: 提出的方法在并发错误检测和定位方面优于现有方法，能有效解决现有方法的局限性。

Abstract: Concurrency bugs, caused by improper synchronization of shared resources in
multi-threaded or distributed systems, are notoriously hard to detect and thus
compromise software reliability and security. The existing deep learning
methods face three main limitations. First, there is an absence of large and
dedicated datasets of diverse concurrency bugs for them. Second, they lack
sufficient representation of concurrency semantics. Third, binary
classification results fail to provide finer-grained debug information such as
precise bug lines. To address these problems, we propose a novel method for
effective concurrency bug detection as well as localization. We construct a
dedicated concurrency bug dataset to facilitate model training and evaluation.
We then integrate a pre-trained model with a heterogeneous graph neural network
(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that
concisely and effectively characterizes concurrency semantics. To further
facilitate debugging, we employ SubgraphX, a GNN-based interpretability method,
which explores the graphs to precisely localize concurrency bugs, mapping them
to specific lines of source code. On average, our method demonstrates an
improvement of 10\% in accuracy and precision and 26\% in recall compared to
state-of-the-art methods across diverse evaluation settings.

</details>


### [137] [ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging](https://arxiv.org/abs/2508.20977)
*Shiwen Shan,Yintong Huo,Yuxin Su,Zhining Wang,Dan Li,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出配置日志的想法，开发ConfLogger工具提升软件配置可诊断性，评估显示其在多方面有效且实用。


<details>
  <summary>Details</summary>
Motivation: 现有可诊断性支持聚焦软件故障后分析，未考虑软件是否有足够故障信息用于诊断，需填补此空白。

Method: 统一配置感知静态污点分析与基于LLM的日志生成，识别配置敏感代码段并分析配置代码上下文生成诊断日志语句。

Result: 在八个软件系统评估中，增强日志助力诊断工具定位错误准确率达100%，80%可通过配置信息解决；覆盖现有日志点74%，多项指标优于基线；用户研究验证实用，加快诊断时间、提高排查准确率。

Conclusion: ConfLogger能有效提升软件配置可诊断性。

Abstract: Modern configurable systems offer customization via intricate configuration
spaces, yet such flexibility introduces pervasive configuration-related issues
such as misconfigurations and latent softwarebugs. Existing diagnosability
supports focus on post-failure analysis of software behavior to identify
configuration issues, but none of these approaches look into whether the
software clue sufficient failure information for diagnosis. To fill in the
blank, we propose the idea of configuration logging to enhance existing logging
practices at the source code level. We develop ConfLogger, the first tool that
unifies configuration-aware static taint analysis with LLM-based log generation
to enhance software configuration diagnosability. Specifically, our method 1)
identifies configuration-sensitive code segments by tracing
configuration-related data flow in the whole project, and 2) generates
diagnostic log statements by analyzing configuration code contexts. Evaluation
results on eight popular software systems demonstrate the effectiveness of
ConfLogger to enhance configuration diagnosability. Specifically,
ConfLogger-enhanced logs successfully aid a log-based misconfiguration
diagnosis tool to achieve 100% accuracy on error localization in 30 silent
misconfiguration scenarios, with 80% directly resolvable through explicit
configuration information exposed. In addition, ConfLogger achieves 74%
coverage of existing logging points, outperforming baseline LLM-based loggers
by 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,
and 26.2% higher in F1 compared to the state-of-the-art baseline in terms of
variable logging while also augmenting diagnostic value. A controlled user
study on 22 cases further validated its utility, speeding up diagnostic time by
1.25x and improving troubleshooting accuracy by 251.4%.

</details>


### [138] [Dynamics of Gender Bias in Software Engineering](https://arxiv.org/abs/2508.21050)
*Thomas J. Misa*

Main category: cs.SE

TL;DR: 本文调研软件工程起源与对工程专业精神的关注，研究性别问题与偏见，分析女性参会作者情况，提出计算领域性别偏见研究的政策维度。


<details>
  <summary>Details</summary>
Motivation: 软件工程领域可能存在工程和计算机科学共有的性别偏见，需进行研究。

Method: 调研软件工程起源，剖析对工程专业精神的关注；研究近期性别问题；定量分析1976 - 2010年软件工程国际会议女性作者参与情况。

Result: 发现有十二年存在统计学上显著的性别排斥情况。

Conclusion: 提出计算领域性别偏见研究的政策维度。

Abstract: The field of software engineering is embedded in both engineering and
computer science, and may embody gender biases endemic to both. This paper
surveys software engineering's origins and its long-running attention to
engineering professionalism, profiling five leaders; it then examines the
field's recent attention to gender issues and gender bias. It next
quantitatively analyzes women's participation as research authors in the
field's leading International Conference of Software Engineering (1976-2010),
finding a dozen years with statistically significant gender exclusion. Policy
dimensions of research on gender bias in computing are suggested.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [139] [Can LLMs Identify Tax Abuse?](https://arxiv.org/abs/2508.20097)
*Andrew Blair-Stanek,Nils Holzenberger,Benjamin Van Durme*

Main category: q-fin.CP

TL;DR: 研究大语言模型发现和分析美国税收最小化策略的能力，模型识别出全新策略。


<details>
  <summary>Details</summary>
Motivation: 美国税收领域挑战大，进展可减少高收入纳税人的税收流失，且该领域对大语言模型推理社区有吸引力。

Method: 评估最先进大语言模型解释和验证税收策略、填补部分策略空白、从头生成完整策略的能力。

Result: 大语言模型推理识别出一种全新的税收策略。

Conclusion: 大语言模型有潜力革新税务机构打击税收滥用的工作。

Abstract: We investigate whether large language models can discover and analyze U.S.
tax-minimization strategies. This real-world domain challenges even seasoned
human experts, and progress can reduce tax revenue lost from well-advised,
wealthy taxpayers. We evaluate the most advanced LLMs on their ability to (1)
interpret and verify tax strategies, (2) fill in gaps in partially specified
strategies, and (3) generate complete, end-to-end strategies from scratch. This
domain should be of particular interest to the LLM reasoning community: unlike
synthetic challenge problems or scientific reasoning tasks, U.S. tax law
involves navigating hundreds of thousands of pages of statutes, case law, and
administrative guidance, all updated regularly. Notably, LLM-based reasoning
identified an entirely novel tax strategy, highlighting these models' potential
to revolutionize tax agencies' fight against tax abuse.

</details>


### [140] [Agent-based model of information diffusion in the limit order book trading](https://arxiv.org/abs/2508.20672)
*Mateusz Wilinski,Juho Kanniainen*

Main category: q-fin.CP

TL;DR: 本文研究交易者间交互能否重现高频交易中的典型事实，发现无交互不能，无智能代理的无标度连接可重现，为典型事实出现提供新解释。


<details>
  <summary>Details</summary>
Motivation: 现有对高频交易典型事实有多种解释，本文探索交易者放置限价订单簿消息的交互能否重现典型事实及所需交互形式，且关注特定交易网络拓扑能否产生典型事实。

Method: 构建严格零智能代理模型，隔离网络拓扑作用。

Result: 代理间的无标度连接能重现市场中的典型事实，无交互则不能，规则格和埃尔德什 - 雷尼网络与无交互基线无显著差异。

Conclusion: 为典型事实的出现提供全新且可能互补的解释。

Abstract: There are multiple explanations for stylized facts in high-frequency trading,
including adaptive and informed agents, many of which have been studied through
agent-based models. This paper investigates an alternative explanation by
examining whether, and under what circumstances, interactions between traders
placing limit order book messages can reproduce stylized facts, and what forms
of interaction are required. While the agent-based modeling literature has
introduced interconnected agents on networks, little attention has been paid to
whether specific trading network topologies can generate stylized facts in
limit order book markets. In our model, agents are strictly zero-intelligence,
with no fundamental knowledge or chartist-like strategies, so that the role of
network topology can be isolated. We find that scale-free connectivity between
agents reproduces stylized facts observed in markets, whereas no-interaction
does not. Our experiments show that regular lattices and Erdos-Renyi networks
are not significantly different from the no-interaction baseline. Thus, we
provide a completely new, potentially complementary, explanation for the
emergence of stylized facts.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [141] [Deep Reinforcement Learning for Optimal Asset Allocation Using DDPG with TiDE](https://arxiv.org/abs/2508.20103)
*Rongwei Liu,Jin Zheng,John Cartlidge*

Main category: q-fin.PM

TL;DR: 本文将最优两资产配置问题转化为马尔可夫决策过程，用强化学习制定动态策略，将TiDE集成到DDPG框架，结果显示该方法优于Q学习和买入持有策略。


<details>
  <summary>Details</summary>
Motivation: 传统资产配置方法因严格分布假设和非加性奖励比率，限制了鲁棒性和对投资目标的适用性，需要新方法解决。

Method: 将最优两资产配置问题构建为马尔可夫决策过程，用强化学习机制基于模拟金融场景制定动态策略，用凯利准则平衡即时奖励和长期目标，将TiDE集成到DDPG框架进行连续决策，与Q学习和买入持有策略对比。

Result: DDPG - TiDE优于Q学习，比买入持有策略产生更高的风险调整后回报。

Conclusion: 将TiDE集成到DDPG强化学习框架解决最优资产配置问题值得进一步探索。

Abstract: The optimal asset allocation between risky and risk-free assets is a
persistent challenge due to the inherent volatility in financial markets.
Conventional methods rely on strict distributional assumptions or non-additive
reward ratios, which limit their robustness and applicability to investment
goals. To overcome these constraints, this study formulates the optimal
two-asset allocation problem as a sequential decision-making task within a
Markov Decision Process (MDP). This framework enables the application of
reinforcement learning (RL) mechanisms to develop dynamic policies based on
simulated financial scenarios, regardless of prerequisites. We use the Kelly
criterion to balance immediate reward signals against long-term investment
objectives, and we take the novel step of integrating the Time-series Dense
Encoder (TiDE) into the Deep Deterministic Policy Gradient (DDPG) RL framework
for continuous decision-making. We compare DDPG-TiDE with a simple
discrete-action Q-learning RL framework and a passive buy-and-hold investment
strategy. Empirical results show that DDPG-TiDE outperforms Q-learning and
generates higher risk adjusted returns than buy-and-hold. These findings
suggest that tackling the optimal asset allocation problem by integrating TiDE
within a DDPG reinforcement learning framework is a fruitful avenue for further
exploration.

</details>


### [142] [QTMRL: An Agent for Quantitative Trading Decision-Making Based on Multi-Indicator Guided Reinforcement Learning](https://arxiv.org/abs/2508.20467)
*Xiangdong Liu,Jiahao Chen*

Main category: q-fin.PM

TL;DR: 提出QTMRL智能交易代理用于自适应和稳定的投资组合管理，实验验证其优于多个基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统量化交易模型因假设僵化、泛化能力有限，难以适应动态市场变化和黑天鹅事件。

Method: 构建综合多指标数据集，设计基于A2C算法的轻量级强化学习框架，包括数据处理、A2C算法和交易代理模块。

Result: 实验对比显示QTMRL在盈利能力、风险调整和下行风险控制方面优于9个基线模型。

Conclusion: QTMRL能有效解决传统量化交易模型的问题，实现自适应和稳定的投资组合管理，代码公开。

Abstract: In the highly volatile and uncertain global financial markets, traditional
quantitative trading models relying on statistical modeling or empirical rules
often fail to adapt to dynamic market changes and black swan events due to
rigid assumptions and limited generalization. To address these issues, this
paper proposes QTMRL (Quantitative Trading Multi-Indicator Reinforcement
Learning), an intelligent trading agent combining multi-dimensional technical
indicators with reinforcement learning (RL) for adaptive and stable portfolio
management. We first construct a comprehensive multi-indicator dataset using 23
years of S&P 500 daily OHLCV data (2000-2022) for 16 representative stocks
across 5 sectors, enriching raw data with trend, volatility, and momentum
indicators to capture holistic market dynamics. Then we design a lightweight RL
framework based on the Advantage Actor-Critic (A2C) algorithm, including data
processing, A2C algorithm, and trading agent modules to support policy learning
and actionable trading decisions. Extensive experiments compare QTMRL with 9
baselines (e.g., ARIMA, LSTM, moving average strategies) across diverse market
regimes, verifying its superiority in profitability, risk adjustment, and
downside risk control. The code of QTMRL is publicly available at
https://github.com/ChenJiahaoJNU/QTMRL.git

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [143] [Mitigating Distribution Shift in Stock Price Data via Return-Volatility Normalization for Accurate Prediction](https://arxiv.org/abs/2508.20108)
*Hyunwoo Lee,Jihyeong Jeon,Jaemin Hong,U Kang*

Main category: q-fin.ST

TL;DR: 提出ReVol方法解决股价数据分布偏移问题，结合多种策略提升股价预测准确性，实验表明能增强现有骨干模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有股价预测方法难以有效处理数据分布偏移，无法充分解决训练和测试数据间的分布差异和形状不一致问题。

Method: 提出ReVol方法，采用三种策略减轻分布偏移，结合几何布朗运动进行长期趋势建模和神经网络进行短期模式识别。

Result: 在真实数据集上的大量实验显示，ReVol在多数情况下提升了现有骨干模型的性能，IC平均提升超0.03，SR平均提升超0.7。

Conclusion: ReVol方法能有效处理股价数据分布偏移问题，提升股价预测准确性。

Abstract: How can we address distribution shifts in stock price data to improve stock
price prediction accuracy? Stock price prediction has attracted attention from
both academia and industry, driven by its potential to uncover complex market
patterns and enhance decisionmaking. However, existing methods often fail to
handle distribution shifts effectively, focusing on scaling or representation
adaptation without fully addressing distributional discrepancies and shape
misalignments between training and test data. We propose ReVol
(Return-Volatility Normalization for Mitigating Distribution Shift in Stock
Price Data), a robust method for stock price prediction that explicitly
addresses the distribution shift problem. ReVol leverages three key strategies
to mitigate these shifts: (1) normalizing price features to remove
sample-specific characteristics, including return, volatility, and price scale,
(2) employing an attention-based module to estimate these characteristics
accurately, thereby reducing the influence of market anomalies, and (3)
reintegrating the sample characteristics into the predictive process, restoring
the traits lost during normalization. Additionally, ReVol combines geometric
Brownian motion for long-term trend modeling with neural networks for
short-term pattern recognition, unifying their complementary strengths.
Extensive experiments on real-world datasets demonstrate that ReVol enhances
the performance of the state-of-the-art backbone models in most cases,
achieving an average improvement of more than 0.03 in IC and over 0.7 in SR
across various settings.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [144] [Optimal Quoting under Adverse Selection and Price Reading](https://arxiv.org/abs/2508.20225)
*Alexander Barzykin,Philippe Bergault,Olivier Guéant,Malo Lemmel*

Main category: q-fin.TR

TL;DR: 传统文献对做市商信息风险关注不足，本文提出框架助做市商应对信息风险。


<details>
  <summary>Details</summary>
Motivation: 过去十年做市商使用算法模型，但量化金融文献对信息风险如逆向选择和价格解读关注有限。

Method: 在现有文献基础上，提出可操作框架。

Result: 提出能让做市商在报价时更关注信息风险的框架。

Conclusion: 所提框架可解决做市商在信息风险方面面临的问题，具有可操作性。

Abstract: Over the past decade, many dealers have implemented algorithmic models to
automatically respond to RFQs and manage flows originating from their
electronic platforms. In parallel, building on the foundational work of Ho and
Stoll, and later Avellaneda and Stoikov, the academic literature on market
making has expanded to address trade size distributions, client tiering,
complex price dynamics, alpha signals, and the internalization versus
externalization dilemma in markets with dealer-to-client and interdealer-broker
segments. In this paper, we tackle two critical dimensions: adverse selection,
arising from the presence of informed traders, and price reading, whereby the
market maker's own quotes inadvertently reveal the direction of their
inventory. These risks are well known to practitioners, who routinely face
informed flows and algorithms capable of extracting signals from quoting
behavior. Yet they have received limited attention in the quantitative finance
literature, beyond stylized toy models with limited actionability. Extending
the existing literature, we propose a tractable and implementable framework
that enables market makers to adjust their quotes with greater awareness of
informational risk.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [145] [Stochastic Gradients under Nuisances](https://arxiv.org/abs/2508.20326)
*Facheng Yu,Ronak Mehta,Alex Luedtke,Zaid Harchaoui*

Main category: stat.ML

TL;DR: 研究目标含未知扰动参数的随机梯度算法，给出非渐近收敛保证，分析不同条件下收敛情况并举例。


<details>
  <summary>Details</summary>
Motivation: 随机梯度优化是多种学习场景主导范式，需研究目标含未知扰动参数的随机梯度算法收敛性。

Method: 研究随机梯度算法，分析在Neyman正交性等条件下的收敛情况，提出近似正交化更新的算法变体。

Result: 经典随机梯度算法在适当条件（如Neyman正交性）下可收敛；不满足Neyman正交性时，近似正交化更新算法变体可达到类似收敛率。

Conclusion: 为目标含未知扰动参数的随机梯度算法提供了收敛性的理论保证和算法改进方向。

Abstract: Stochastic gradient optimization is the dominant learning paradigm for a
variety of scenarios, from classical supervised learning to modern
self-supervised learning. We consider stochastic gradient algorithms for
learning problems whose objectives rely on unknown nuisance parameters, and
establish non-asymptotic convergence guarantees. Our results show that, while
the presence of a nuisance can alter the optimum and upset the optimization
trajectory, the classical stochastic gradient algorithm may still converge
under appropriate conditions, such as Neyman orthogonality. Moreover, even when
Neyman orthogonality is not satisfied, we show that an algorithm variant with
approximately orthogonalized updates (with an approximately orthogonalized
gradient oracle) may achieve similar convergence rates. Examples from
orthogonal statistical learning/double machine learning and causal inference
are discussed.

</details>


### [146] [Towards Trustworthy Amortized Bayesian Model Comparison](https://arxiv.org/abs/2508.20614)
*Šimon Kucharský,Aayush Mishra,Daniel Habermann,Stefan T. Radev,Paul-Christian Bürkner*

Main category: stat.ML

TL;DR: 本文提出用自一致性损失改进摊销贝叶斯模型比较（BMC）估计，通过实验对比评估其效果，发现该方法在似然准确时有实用价值。


<details>
  <summary>Details</summary>
Motivation: 现有摊销BMC中，模拟模型误设时神经替代物可靠性下降，而此时最需要模型比较，因此需要改进。

Method: 在无标签真实数据上补充自一致性（SC）损失以改进BMC估计，并通过数值实验和两个真实数据案例研究，将有无SC的摊销证据估计与分析或桥抽样基准进行比较。

Result: 在可获取分析似然时，SC能在模型误设下改善校准；但对神经替代似然，增益有限。

Conclusion: SC在似然准确时对可靠的BMC最实用。

Abstract: Amortized Bayesian model comparison (BMC) enables fast probabilistic ranking
of models via simulation-based training of neural surrogates. However, the
reliability of neural surrogates deteriorates when simulation models are
misspecified - the very case where model comparison is most needed. Thus, we
supplement simulation-based training with a self-consistency (SC) loss on
unlabeled real data to improve BMC estimates under empirical distribution
shifts. Using a numerical experiment and two case studies with real data, we
compare amortized evidence estimates with and without SC against analytic or
bridge sampling benchmarks. SC improves calibration under model
misspecification when having access to analytic likelihoods. However, it offers
limited gains with neural surrogate likelihoods, making it most practical for
trustworthy BMC when likelihoods are exact.

</details>


### [147] [Polynomial Chaos Expansion for Operator Learning](https://arxiv.org/abs/2508.20886)
*Himanshu Sharma,Lukáš Novák,Michael D. Shields*

Main category: stat.ML

TL;DR: 本文引入多项式混沌展开（PCE）作为算子学习（OL）方法，建立数学框架，应用于偏微分方程问题，数值结果显示其在OL和不确定性量化（UQ）任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 在OL领域，以往多采用基于深度神经网络的方法，近期开始探索传统机器学习方法，本文旨在引入PCE作为OL方法。

Method: 建立数学框架，使PCE在纯数据驱动和物理信息驱动的环境中近似算子，将学习算子的任务转化为求解PCE系数的方程组。

Result: 将该方法应用于多种偏微分方程问题，数值结果表明其在OL和UQ任务中表现出色，具有良好的数值精度和计算效率。

Conclusion: 所提出的基于PCE的OL方法在OL和UQ任务中表现良好，具有一定的优势。

Abstract: Operator learning (OL) has emerged as a powerful tool in scientific machine
learning (SciML) for approximating mappings between infinite-dimensional
functional spaces. One of its main applications is learning the solution
operator of partial differential equations (PDEs). While much of the progress
in this area has been driven by deep neural network-based approaches such as
Deep Operator Networks (DeepONet) and Fourier Neural Operator (FNO), recent
work has begun to explore traditional machine learning methods for OL. In this
work, we introduce polynomial chaos expansion (PCE) as an OL method. PCE has
been widely used for uncertainty quantification (UQ) and has recently gained
attention in the context of SciML. For OL, we establish a mathematical
framework that enables PCE to approximate operators in both purely data-driven
and physics-informed settings. The proposed framework reduces the task of
learning the operator to solving a system of equations for the PCE
coefficients. Moreover, the framework provides UQ by simply post-processing the
PCE coefficients, without any additional computational cost. We apply the
proposed method to a diverse set of PDE problems to demonstrate its
capabilities. Numerical results demonstrate the strong performance of the
proposed method in both OL and UQ tasks, achieving excellent numerical accuracy
and computational efficiency.

</details>


### [148] [Transfer Learning for Classification under Decision Rule Drift with Application to Optimal Individualized Treatment Rule Estimation](https://arxiv.org/abs/2508.20942)
*Xiaohan Wang,Yang Ning*

Main category: stat.ML

TL;DR: 本文将迁移学习分类框架从基于回归函数的方法扩展到决策规则，提出新方法，建立估计量一致性和风险界，展示其在最优个体化治疗规则估计中的应用，模拟和真实数据分析证明方法性能优越且稳健。


<details>
  <summary>Details</summary>
Motivation: 将迁移学习分类框架从基于回归函数的方法扩展到决策规则。

Method: 通过贝叶斯决策规则对后验漂移建模，利用贝叶斯决策边界的几何变换将问题转化为低维经验风险最小化问题。

Result: 建立了估计量的一致性，推导了风险界，模拟研究和真实数据分析表明方法性能优越且稳健。

Conclusion: 所提出的方法具有广泛适用性，性能优越且稳健。

Abstract: In this paper, we extend the transfer learning classification framework from
regression function-based methods to decision rules. We propose a novel
methodology for modeling posterior drift through Bayes decision rules. By
exploiting the geometric transformation of the Bayes decision boundary, our
method reformulates the problem as a low-dimensional empirical risk
minimization problem. Under mild regularity conditions, we establish the
consistency of our estimators and derive the risk bounds. Moreover, we
illustrate the broad applicability of our method by adapting it to the
estimation of optimal individualized treatment rules. Extensive simulation
studies and analyses of real-world data further demonstrate both superior
performance and robustness of our approach.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [149] [Optional subsampling for generalized estimating equations in growing-dimensional longitudinal Data](https://arxiv.org/abs/2508.20803)
*Chunjing Li,Jiahui Zhang,Xiaohui Yuan*

Main category: stat.CO

TL;DR: 本文针对大尺度纵向数据广义估计方程提出最优泊松子采样算法，推导概率，通过模拟和实证验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 广义估计方程在大规模场景下存在计算和存储挑战。

Method: 提出最优泊松子采样算法，推导最优子采样概率，采用两步法构建概率，进行模拟研究和实证应用。

Result: 模拟研究表明，即使工作相关矩阵指定错误，方法仍有效；实证应用展示了方法的实际性能。

Conclusion: 所提出的算法对于处理大尺度纵向数据的广义估计方程是有效的。

Abstract: As a powerful tool for longitudinal data analysis, the generalized estimating
equations have been widely studied in the academic community. However, in
large-scale settings, this approach faces pronounced computational and storage
challenges. In this paper, we propose an optimal Poisson subsampling algorithm
for generalized estimating equations in large-scale longitudinal data with
diverging covariate dimension, and establish the asymptotic properties of the
resulting estimator. We further derive the optimal Poisson subsampling
probability based on A- and L-optimality criteria. An approximate optimal
Poisson subsampling algorithm is proposed, which adopts a two-step procedure to
construct these probabilities. Simulation studies are conducted to evaluate the
performance of the proposed method under three different working correlation
matrices. The results show that the method remains effective even when the
working correlation matrices are misspecified. Finally, we apply the proposed
method to the CHFS dataset to illustrate its empirical performance.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [150] [Photonic restricted Boltzmann machine for content generation tasks](https://arxiv.org/abs/2508.20472)
*Li Luo,Yisheng Fang,Wanyi Zhang,Zhichao Ruan*

Main category: physics.optics

TL;DR: 提出光子受限玻尔兹曼机（PRBM）加速吉布斯采样，降低计算复杂度，实验验证其效果，在多领域有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 受限玻尔兹曼机（RBM）在内容生成任务中吉布斯采样计算成本高，电子实现有瓶颈。

Method: 提出PRBM利用光子计算加速吉布斯采样，引入高效编码方法，采用非冯诺依曼光子计算架构。

Result: 通过模拟二维伊辛模型实验验证光子加速吉布斯采样，相变温度与理论预测相符，能生成和恢复多种内容。

Conclusion: PRBM框架可扩展性强、训练成本低，是推进光子计算在生成式人工智能领域发展的有前景途径。

Abstract: The restricted Boltzmann machine (RBM) is a neural network based on the Ising
model, well known for its ability to learn probability distributions and
stochastically generate new content. However, the high computational cost of
Gibbs sampling in content generation tasks imposes significant bottlenecks on
electronic implementations. Here, we propose a photonic restricted Boltzmann
machine (PRBM) that leverages photonic computing to accelerate Gibbs sampling,
enabling efficient content generation. By introducing an efficient encoding
method, the PRBM eliminates the need for computationally intensive matrix
decomposition and reduces the computational complexity of Gibbs sampling from
$O(N)$ to $O(1)$. Moreover, its non-Von Neumann photonic computing architecture
circumvents the memory storage of interaction matrices, providing substantial
advantages for large-scale RBMs. We experimentally validate the
photonic-accelerated Gibbs sampling by simulating a two-dimensional Ising
model, where the observed phase transition temperature closely matches the
theoretical predictions. Beyond physics-inspired tasks, the PRBM demonstrates
robust capabilities in generating and restoring diverse content, including
images and temporal sequences, even in the presence of noise and aberrations.
The scalability and reduced training cost of the PRBM framework underscore its
potential as a promising pathway for advancing photonic computing in generative
artificial intelligence.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [151] [FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture](https://arxiv.org/abs/2508.20212)
*Minghao Hu,Junzhe Wang,Weisen Zhao,Qiang Zeng,Lannan Luo*

Main category: cs.CR

TL;DR: 本文提出利用神经机器翻译和归一化流思想，实现用单指令集架构训练的模型进行多指令集架构的恶意软件检测，减少数据收集负担。


<details>
  <summary>Details</summary>
Motivation: 随着针对物联网设备的网络攻击增加，不同指令集架构的恶意软件增多，扩展多指令集架构的恶意软件检测能力很重要，但收集和标注各指令集架构的恶意软件样本耗时费力。

Method: 借鉴神经机器翻译和归一化流思想，将某指令集架构的恶意软件转换为有充足样本的指令集架构，如X86 - 64，用单指令集架构训练的模型分析其他指令集架构的恶意软件。

Result: 实现了用单指令集架构训练的模型进行多指令集架构的恶意软件检测。

Conclusion: 该方法能减少多指令集架构恶意软件检测的数据收集工作。

Abstract: Applying deep learning to malware detection has drawn great attention due to
its notable performance. With the increasing prevalence of cyberattacks
targeting IoT devices, there is a parallel rise in the development of malware
across various Instruction Set Architectures (ISAs). It is thus important to
extend malware detection capacity to multiple ISAs. However, training a deep
learning-based malware detection model usually requires a large number of
labeled malware samples. The process of collecting and labeling sufficient
malware samples to build datasets for each ISA is labor-intensive and
time-consuming. To reduce the burden of data collection, we propose to leverage
the ideas of Neural Machine Translation (NMT) and Normalizing Flows (NFs) for
malware detection. Specifically, when dealing with malware in a certain ISA, we
translate it to an ISA with sufficient malware samples (like X86-64). This
allows us to apply a model trained on one ISA to analyze malware from another
ISA. Our approach reduces the data collection effort by enabling malware
detection across multiple ISAs using a model trained on a single ISA.

</details>


### [152] [Characterizing Trust Boundary Vulnerabilities in TEE Containers](https://arxiv.org/abs/2508.20962)
*Weijie Liu,Hongbo Chen,Shuo Huai,Zhen Xu,Wenhao Wang,Zhi Li,Zheli Liu*

Main category: cs.CR

TL;DR: 本文分析现有TEE容器隔离策略，设计自动化分析器评估隔离边界，发现部分容器存在安全问题，并给出开发建议和讨论趋势。


<details>
  <summary>Details</summary>
Motivation: 为实现TEE平台上应用的安全开发、执行和部署，分析现有TEE容器隔离策略以保障安全。

Method: 设计自动化分析器来精确识别和评估TEE容器的隔离边界。

Result: 发现部分TEE容器因设计和实现缺陷未达预期目标，存在信息泄露等安全风险。

Conclusion: 分享关键经验以指导开发更安全的容器解决方案，讨论TEE容器化设计的新兴趋势。

Abstract: Trusted Execution Environments (TEEs) have emerged as a cornerstone of
confidential computing, garnering significant attention from both academia and
industry. To enable the secure development, execution, and deployment, of
applications on TEE platforms, TEE containers have been introduced as
middleware solutions. These containers aim to shield applications from
potentially malicious operating systems and orchestration interfaces while
maintaining usability and reliability. In this paper, we analyze the isolation
strategies employed by existing TEE containers to protect secure applications.
To address the challenges in analyzing these interfaces, we designed an
automated analyzer to precisely identify and evaluate their isolation
boundaries. We observed that some TEE containers fail to achieve their intended
goals due to critical design and implementation flaws, such as information
leakage, rollback attacks, denial-of-service, and Iago attacks, which pose
significant security risks. Drawing from our findings, we share key lessons to
guide the development of more secure container solutions and discuss emerging
trends in TEE containerization design.

</details>


### [153] [AI Propaganda factories with language models](https://arxiv.org/abs/2508.20186)
*Lukasz Olejnik*

Main category: cs.CR

TL;DR: 研究表明AI驱动的影响行动可在商用硬件上实现端到端执行，指出行为发现，表明自动影响内容生产可期，建议防御策略转变。


<details>
  <summary>Details</summary>
Motivation: 探究AI驱动的影响行动在当前硬件条件下的实现情况及特点，为防御策略提供依据。

Method: 利用小语言模型生成政治信息并自动评估，得出行为发现。

Result: 发现“角色优于模型”以及“参与作为压力源”的现象，证明大小参与者都能实现全自动影响内容生产。

Conclusion: 防御应从限制模型访问转向以对话为中心的检测和破坏行动及协调基础设施，操作的一致性可作为检测特征。

Abstract: AI-powered influence operations can now be executed end-to-end on commodity
hardware. We show that small language models produce coherent, persona-driven
political messaging and can be evaluated automatically without human raters.
Two behavioural findings emerge. First, persona-over-model: persona design
explains behaviour more than model identity. Second, engagement as a stressor:
when replies must counter-arguments, ideological adherence strengthens and the
prevalence of extreme content increases. We demonstrate that fully automated
influence-content production is within reach of both large and small actors.
Consequently, defence should shift from restricting model access towards
conversation-centric detection and disruption of campaigns and coordination
infrastructure. Paradoxically, the very consistency that enables these
operations also provides a detection signature.

</details>


### [154] [Network-Level Prompt and Trait Leakage in Local Research Agents](https://arxiv.org/abs/2508.20282)
*Hyejun Jeong,Mohammadreze Teymoorianfard,Abhinav Kumar,Amir Houmansadr,Eugene Badasarian*

Main category: cs.CR

TL;DR: 研究表明基于语言模型的Web和研究代理（WRAs）易受网络推理攻击，提出新攻击方法，能恢复大量提示信息，还讨论了缓解策略。


<details>
  <summary>Details</summary>
Motivation: WRAs可能因隐私、法律或财务目的被本地部署，而其网络行为有特征，可能遭受攻击，需研究攻击及防御方法。

Method: 构建WRA痕迹新数据集，定义行为指标OBELS，利用网络元数据实施攻击，还在多会话场景测试，研究部分可观测和噪声条件下攻击效果。

Result: 攻击能恢复超73%用户提示的功能和领域知识，多会话中能高精度恢复部分潜在特征，攻击在部分可观测和噪声条件下仍有效，缓解策略能平均降低29%攻击效果且对实用性影响小。

Conclusion: WRAs存在易受推理攻击的问题，提出的攻击方法有效，缓解策略可行。

Abstract: We show that Web and Research Agents (WRAs) -- language model-based systems
that investigate complex topics on the Internet -- are vulnerable to inference
attacks by passive network adversaries such as ISPs. These agents could be
deployed \emph{locally} by organizations and individuals for privacy, legal, or
financial purposes. Unlike sporadic web browsing by humans, WRAs visit
$70{-}140$ domains with distinguishable timing correlations, enabling unique
fingerprinting attacks.
  Specifically, we demonstrate a novel prompt and user trait leakage attack
against WRAs that only leverages their network-level metadata (i.e., visited IP
addresses and their timings). We start by building a new dataset of WRA traces
based on user search queries and queries generated by synthetic personas. We
define a behavioral metric (called OBELS) to comprehensively assess similarity
between original and inferred prompts, showing that our attack recovers over
73\% of the functional and domain knowledge of user prompts. Extending to a
multi-session setting, we recover up to 19 of 32 latent traits with high
accuracy. Our attack remains effective under partial observability and noisy
conditions. Finally, we discuss mitigation strategies that constrain domain
diversity or obfuscate traces, showing negligible utility impact while reducing
attack effectiveness by an average of 29\%.

</details>


### [155] [Surveying the Operational Cybersecurity and Supply Chain Threat Landscape when Developing and Deploying AI Systems](https://arxiv.org/abs/2508.20307)
*Michael R Smith,Joe Ingram*

Main category: cs.CR

TL;DR: 本文指出AI融入软件系统带来新的网络威胁，探讨AI生命周期安全风险，强调定制安全框架的必要性以保护AI系统。


<details>
  <summary>Details</summary>
Motivation: AI发展带来新的攻击面和目标，传统网络安全评估常忽略，需提高对AI引入的新型网络威胁的认识。

Method: 探索AI生命周期中的运营网络安全和供应链风险，强调定制安全框架；提及先前的攻击案例并分享相关工作见解。

Result: 分析了AI系统面临的新型网络威胁和风险。

Conclusion: 组织应了解这些风险，定制安全框架，以保护AI系统，确保其可靠性和弹性。

Abstract: The rise of AI has transformed the software and hardware landscape, enabling
powerful capabilities through specialized infrastructures, large-scale data
storage, and advanced hardware. However, these innovations introduce unique
attack surfaces and objectives which traditional cybersecurity assessments
often overlook. Cyber attackers are shifting their objectives from conventional
goals like privilege escalation and network pivoting to manipulating AI outputs
to achieve desired system effects, such as slowing system performance, flooding
outputs with false positives, or degrading model accuracy. This paper serves to
raise awareness of the novel cyber threats that are introduced when
incorporating AI into a software system. We explore the operational
cybersecurity and supply chain risks across the AI lifecycle, emphasizing the
need for tailored security frameworks to address evolving threats in the
AI-driven landscape. We highlight previous exploitations and provide insights
from working in this area. By understanding these risks, organizations can
better protect AI systems and ensure their reliability and resilience.

</details>


### [156] [BridgeShield: Enhancing Security for Cross-chain Bridge Applications via Heterogeneous Graph Mining](https://arxiv.org/abs/2508.20517)
*Dan Lin,Shunfeng Lu,Ziyan Liu,Jiajing Wu,Junyuan Fang,Kaixin Lin,Bowen Song,Zibin Zheng*

Main category: cs.CR

TL;DR: 现有跨链桥检测方法有局限，提出BridgeShield框架，实验显示其F1分数高，能有效保障跨链桥安全。


<details>
  <summary>Details</summary>
Motivation: 现有跨链桥检测方法主要关注单链行为，无法捕捉跨链语义，需新方法应对跨链桥攻击。

Method: 利用异质图注意力网络，提出BridgeShield框架，统一表示源链、链外协调和目标链，结合元路径内和元路径间注意力机制。

Result: 在51个真实跨链攻击事件实验中，BridgeShield平均F1分数达92.58%，比现有基线提升24.39%。

Conclusion: BridgeShield是保障跨链桥安全、增强多链生态系统弹性的有效实用方案。

Abstract: Cross-chain bridges play a vital role in enabling blockchain
interoperability. However, due to the inherent design flaws and the enormous
value they hold, they have become prime targets for hacker attacks. Existing
detection methods show progress yet remain limited, as they mainly address
single-chain behaviors and fail to capture cross-chain semantics. To address
this gap, we leverage heterogeneous graph attention networks, which are
well-suited for modeling multi-typed entities and relations, to capture the
complex execution semantics of cross-chain behaviors. We propose BridgeShield,
a detection framework that jointly models the source chain, off-chain
coordination, and destination chain within a unified heterogeneous graph
representation. BridgeShield incorporates intra-meta-path attention to learn
fine-grained dependencies within cross-chain paths and inter-meta-path
attention to highlight discriminative cross-chain patterns, thereby enabling
precise identification of attack behaviors. Extensive experiments on 51
real-world cross-chain attack events demonstrate that BridgeShield achieves an
average F1-score of 92.58%, representing a 24.39% improvement over
state-of-the-art baselines. These results validate the effectiveness of
BridgeShield as a practical solution for securing cross-chain bridges and
enhancing the resilience of multi-chain ecosystems.

</details>


### [157] [Multi-Agent Penetration Testing AI for the Web](https://arxiv.org/abs/2508.20816)
*Isaac David,Arthur Gervais*

Main category: cs.CR

TL;DR: AI 驱动开发导致安全审计可扩展性危机，提出多智能体系统 MAPTA 进行安全评估，在基准测试有成果，现实应用发现关键漏洞。


<details>
  <summary>Details</summary>
Motivation: AI 驱动开发平台使软件开发大众化，但引发安全审计可扩展性危机，大量 AI 生成代码含漏洞，开发速度远超安全评估能力。

Method: 提出 MAPTA，结合大语言模型编排、工具驱动执行和端到端漏洞验证。

Result: 在 XBOW 基准测试中总体成功率 76.9%，不同类型漏洞有不同表现，综合成本分析成功与资源效率强相关；现实中发现关键漏洞，部分结果待 CVE 审核。

Conclusion: MAPTA 能有效进行 Web 应用安全评估，成本低且在实际应用中有重要发现。

Abstract: AI-powered development platforms are making software creation accessible to a
broader audience, but this democratization has triggered a scalability crisis
in security auditing. With studies showing that up to 40% of AI-generated code
contains vulnerabilities, the pace of development now vastly outstrips the
capacity for thorough security assessment.
  We present MAPTA, a multi-agent system for autonomous web application
security assessment that combines large language model orchestration with
tool-grounded execution and end-to-end exploit validation. On the 104-challenge
XBOW benchmark, MAPTA achieves 76.9% overall success with perfect performance
on SSRF and misconfiguration vulnerabilities, 83% success on broken
authorization, and strong results on injection attacks including server-side
template injection (85%) and SQL injection (83%). Cross-site scripting (57%)
and blind SQL injection (0%) remain challenging. Our comprehensive cost
analysis across all challenges totals $21.38 with a median cost of $0.073 for
successful attempts versus $0.357 for failures. Success correlates strongly
with resource efficiency, enabling practical early-stopping thresholds at
approximately 40 tool calls or $0.30 per challenge.
  MAPTA's real-world findings are impactful given both the popularity of the
respective scanned GitHub repositories (8K-70K stars) and MAPTA's low average
operating cost of $3.67 per open-source assessment: MAPTA discovered critical
vulnerabilities including RCEs, command injections, secret exposure, and
arbitrary file write vulnerabilities. Findings are responsibly disclosed, 10
findings are under CVE review.

</details>


### [158] [JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring](https://arxiv.org/abs/2508.20848)
*Junjie Chu,Mingjie Li,Ziqing Yang,Ye Leng,Chenhao Lin,Chao Shen,Michael Backes,Yun Shen,Yang Zhang*

Main category: cs.CR

TL;DR: 提出JADES越狱评估框架，在新基准上验证，表现优于基线，能准确评估越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 现有越狱评估方法依赖代理指标或整体判断，易误判，与人类感知不一致。

Method: 引入JADES框架，自动分解有害问题为子问题并评分，聚合子分数做决策，还可结合事实检查模块。

Result: 在JailbreakQR基准上与人类评估者98.5%一致，超基线9%；重评估显示部分攻击成功率高估。

Conclusion: JADES能提供准确、一致、可解释的评估，为衡量未来越狱攻击提供可靠依据。

Abstract: Accurately determining whether a jailbreak attempt has succeeded is a
fundamental yet unresolved challenge. Existing evaluation methods rely on
misaligned proxy indicators or naive holistic judgments. They frequently
misinterpret model responses, leading to inconsistent and subjective
assessments that misalign with human perception. To address this gap, we
introduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal
jailbreak evaluation framework. Its key mechanism is to automatically decompose
an input harmful question into a set of weighted sub-questions, score each
sub-answer, and weight-aggregate the sub-scores into a final decision. JADES
also incorporates an optional fact-checking module to strengthen the detection
of hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a
newly introduced benchmark proposed in this work, consisting of 400 pairs of
jailbreak prompts and responses, each meticulously annotated by humans. In a
binary setting (success/failure), JADES achieves 98.5% agreement with human
evaluators, outperforming strong baselines by over 9%. Re-evaluating five
popular attacks on four LLMs reveals substantial overestimation (e.g., LAA's
attack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show
that JADES could deliver accurate, consistent, and interpretable evaluations,
providing a reliable basis for measuring future jailbreak attacks.

</details>


### [159] [AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning](https://arxiv.org/abs/2508.20866)
*Amine Lbath,Massih-Reza Amini,Aurelien Delaitre,Vadim Okun*

Main category: cs.CR

TL;DR: 本文提出一种新框架，为C/C++代码生成含特定类别漏洞的数据集，实验显示该方法在数据集准确性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 软件系统复杂度和网络攻击手段提升，传统漏洞检测和修复方法有局限，AI驱动方法依赖训练数据质量和数量，需要生成高质量数据集。

Method: 协调多个模拟专家推理的AI智能体、函数智能体和传统代码分析工具；利用检索增强生成进行上下文锚定；采用权重低秩近似进行高效模型微调。

Result: 在三个不同基准的116个代码样本实验中，该方法在函数层面注入漏洞的成功率达89% - 95%。

Conclusion: 所提方法在数据集准确性上优于其他技术。

Abstract: The increasing complexity of software systems and the sophistication of
cyber-attacks have underscored the critical need for effective automated
vulnerability detection and repair systems. Traditional methods, such as static
program analysis, face significant challenges related to scalability,
adaptability, and high false-positive and false-negative rates. AI-driven
approaches, particularly those using machine learning and deep learning models,
show promise but are heavily reliant on the quality and quantity of training
data. This paper introduces a novel framework designed to automatically
introduce realistic, category-specific vulnerabilities into secure C/C++
codebases to generate datasets. The proposed approach coordinates multiple AI
agents that simulate expert reasoning, along with function agents and
traditional code analysis tools. It leverages Retrieval-Augmented Generation
for contextual grounding and employs Low-Rank approximation of weights for
efficient model fine-tuning. Our experimental study on 116 code samples from
three different benchmarks suggests that our approach outperforms other
techniques with regard to dataset accuracy, achieving between 89\% and 95\%
success rates in injecting vulnerabilities at function level.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [160] [MicroLad: 2D-to-3D Microstructure Reconstruction and Generation via Latent Diffusion and Score Distillation](https://arxiv.org/abs/2508.20138)
*Kang-Hyun Lee,Faez Ahmed*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出MicroLad框架解决材料工程中3D微观结构数据集稀缺问题，实现从2D数据重建3D微观结构及逆控制生成。


<details>
  <summary>Details</summary>
Motivation: 材料工程中建立可靠结构 - 属性关联面临3D微观结构数据集稀缺问题，限制了微观结构形态多样性及逆设计问题的解决。

Method: 引入MicroLad框架，基于2D图像训练，在潜在空间采用多平面去噪扩散采样重建3D体积；集成分数蒸馏采样（SDS），结合可微分数损失、微观结构描述符匹配和属性对齐项更新潜在空间中3D体积的编码2D切片。

Result: MicroLad框架能可靠生成与原始数据统计一致的稳定连贯3D体积，实现从2D数据的维度扩展；可进行逆控制的2D - 3D微观结构生成。

Conclusion: 该方法有助于在微观结构描述符和材料属性方面探索更广阔的3D微观结构分析和设计空间。

Abstract: A major obstacle to establishing reliable structure-property (SP) linkages in
materials engineering is the scarcity of diverse 3D microstructure datasets.
Limited dataset availability and insufficient control over the analysis and
design space restrict the variety of achievable microstructure morphologies,
hindering progress in solving the inverse (property-to-structure) design
problem. To address these challenges, we introduce MicroLad, a latent diffusion
framework specifically designed for reconstructing 3D microstructures from 2D
data. Trained on 2D images and employing multi-plane denoising diffusion
sampling in the latent space, the framework reliably generates stable and
coherent 3D volumes that remain statistically consistent with the original
data. While this reconstruction capability enables dimensionality expansion
(2D-to-3D) for generating statistically equivalent 3D samples from 2D data,
effective exploration of microstructure design requires methods to guide the
generation process toward specific objectives. To achieve this, MicroLad
integrates score distillation sampling (SDS), which combines a differentiable
score loss with microstructural descriptor-matching and property-alignment
terms. This approach updates encoded 2D slices of the 3D volume in the latent
space, enabling robust inverse-controlled 2D-to-3D microstructure generation.
Consequently, the method facilitates exploration of an expanded 3D
microstructure analysis and design space in terms of both microstructural
descriptors and material properties.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [161] [Unclustered BWTs of any Length over Non-Binary Alphabets](https://arxiv.org/abs/2508.20879)
*Gabriele Fici,Estéban Gabory,Giuseppe Romana,Marinella Sciortino*

Main category: cs.DM

TL;DR: 证明对于n>0和字母表大小k≥3，存在长度为n的项链其BWT完全无聚类，并给出数量下界，与二进制情况形成对比。


<details>
  <summary>Details</summary>
Motivation: 研究字母表大小k≥3时Burrows - Wheeler变换（BWT）完全无聚类项链的情况，对比二进制情况。

Method: 数学证明。

Result: 证明存在长度为n的项链其BWT完全无聚类，即有n个游程且无连续相同符号，给出其数量下界。

Conclusion: 在字母表大小k≥3时确定了完全无聚类BWT项链的存在及数量下界，而二进制情况相关问题仍未解决。

Abstract: We prove that for every integer $n > 0$ and for every alphabet $\Sigma_k$ of
size $k \geq 3$, there exists a necklace of length $n$ whose Burrows-Wheeler
Transform (BWT) is completely unclustered, i.e., it consists of exactly $n$
runs with no two consecutive equal symbols. These words represent the
worst-case behavior of the BWT for clustering, since the number of BWT runs is
maximized. We also establish a lower bound on their number. This contrasts with
the binary case, where the existence of infinitely many completely unclustered
BWTs is still an open problem, related to Artin's conjecture on primitive
roots.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [162] [Renormalizable Graph Embeddings For Multi-Scale Network Reconstruction](https://arxiv.org/abs/2508.20706)
*Riccardo Milocco,Fabian Jansen,Diego Garlaschelli*

Main category: physics.soc-ph

TL;DR: 研究在输入网络不可完全观测情况下的多尺度图嵌入，提出基于尺度不变性的重整化方法用于多尺度网络重建。


<details>
  <summary>Details</summary>
Motivation: 现有多尺度图嵌入未考虑输入网络不可完全观测的情况，在金融和经济网络中这种情况常见，需解决该问题。

Method: 先考虑基于最大熵原理的网络重建技术，分析其在不同分辨率下作为图嵌入的局限性，再提出基于尺度不变性的重整化对应方法。

Result: 提出了适用于多尺度网络重建的一致图嵌入方法，并在国家经济投入产出网络和国际贸易网络中进行了验证。

Conclusion: 所提出的基于尺度不变性的重整化方法能解决输入网络不可完全观测时多尺度网络重建的图嵌入问题。

Abstract: In machine learning, graph embedding algorithms seek low-dimensional
representations of the input network data, thereby allowing for downstream
tasks on compressed encodings. Recently, within the framework of network
renormalization, multi-scale embeddings that remain consistent under an
arbitrary aggregation of nodes onto block-nodes, and consequently under an
arbitrary change of resolution of the input network data, have been proposed.
Here we investigate such multi-scale graph embeddings in the modified context
where the input network is not entirely observable, due to data limitations or
privacy constraints. This situation is typical for financial and economic
networks, where connections between individual banks or firms are hidden due to
confidentiality, and one has to probabilistically reconstruct the underlying
network from aggregate information. We first consider state-of-the-art network
reconstruction techniques based on the maximum-entropy principle, which is
designed to operate optimally at a fixed resolution level. We then discuss the
limitations of these methods when they are used as graph embeddings to yield
predictions across different resolution levels. Finally, we propose their
natural 'renormalizable' counterparts derived from the distinct principle of
scale invariance, yielding consistent graph embeddings for multi-scale network
reconstruction. We illustrate these methods on national economic input-output
networks and on international trade networks, which can be naturally
represented at multiple levels of industrial and geographic resolution,
respectively.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [163] [Navigating the EU AI Act: Foreseeable Challenges in Qualifying Deep Learning-Based Automated Inspections of Class III Medical Devices](https://arxiv.org/abs/2508.20144)
*Julio Zanon Diaz,Tommy Brennan,Peter Corcoran*

Main category: cs.CY

TL;DR: 本文评估深度学习技术用于III类医疗器械自动视觉检测时，在现有合规框架下的挑战、差异及潜在策略，强调仅为学术技术评估。


<details>
  <summary>Details</summary>
Motivation: 深度学习用于医疗器械自动视觉检测有潜力，但AI系统引入新监管复杂性，需评估制造商面临的挑战。

Method: 对现有医疗器械合规环境下，基于深度学习的自动检测进行高级技术评估，分析风险管理、数据集治理等方面的差异。

Result: 探讨了风险原则、数据集治理等方面的分歧，以及潜在实施策略和不确定性。

Conclusion: 强调此为学术技术评估，不能替代法律建议，制造商应咨询监管部门和法律专家。

Abstract: As deep learning (DL) technologies advance, their application in automated
visual inspection for Class III medical devices offers significant potential to
enhance quality assurance and reduce human error. However, the adoption of such
AI-based systems introduces new regulatory complexities--particularly under the
EU Artificial Intelligence (AI) Act, which imposes high-risk system obligations
that differ in scope and depth from established regulatory frameworks such as
the Medical Device Regulation (MDR) and the U.S. FDA Quality System Regulation
(QSR). This paper presents a high-level technical assessment of the
foresee-able challenges that manufacturers are likely to encounter when
qualifying DL-based automated inspections within the existing medical device
compliance landscape. It examines divergences in risk management principles,
dataset governance, model validation, explainability requirements, and
post-deployment monitoring obligations. The discussion also explores potential
implementation strategies and highlights areas of uncertainty, including data
retention burdens, global compliance implications, and the practical
difficulties of achieving statistical significance in validation with limited
defect data. Disclaimer: This publication is in-tended solely as an academic
and technical evaluation. It is not a substitute for le-gal advice or official
regulatory interpretation. The information presented here should not be relied
upon to demonstrate compliance with the EU AI Act or any other statutory
obligation. Manufacturers are encouraged to consult appropriate regulatory
authorities and legal experts to determine specific compliance pathways.

</details>


### [164] [RelAItionship Building: Analyzing Recruitment Strategies for Participatory AI](https://arxiv.org/abs/2508.20176)
*Eugene Kim,Vaibhav Balloli,Berelian Karimian,Elizabeth Bondi-Kelly,Benjamin Fish*

Main category: cs.CY

TL;DR: 本文研究参与式AI项目招募方法的挑战及影响，分析37个项目招募方法并访谈5位研究者，最后给出相关建议。


<details>
  <summary>Details</summary>
Motivation: 参与式AI虽有前景，但招募利益相关者仍是实践挑战，需研究招募方法的挑战及影响。

Method: 分析37个AI项目的招募方法，对招募实践文档进行初步分析；访谈5位AI研究者了解招募方法的结果。

Result: 招募方法的结果受工作结构条件、研究者目标期望以及招募和后续合作建立的关系影响。

Conclusion: 为参与式AI研究者设计和执行以关系为导向的招募方法及反思性招募文档实践提供建议。

Abstract: Participatory AI, in which impacted community members and other stakeholders
are involved in the design and development of AI systems, holds promise as a
way to ensure AI is developed to meet their needs and reflect their values.
However, the process of identifying, reaching out, and engaging with all
relevant stakeholder groups, which we refer to as recruitment methodology, is
still a practical challenge in AI projects striving to adopt participatory
practices. In this paper, we investigate the challenges that researchers face
when designing and executing recruitment methodology for Participatory AI
projects, and the implications of current recruitment practice for
Participatory AI. First, we describe the recruitment methodologies used in AI
projects using a corpus of 37 projects to capture the diversity of practices in
the field and perform an initial analysis on the documentation of recruitment
practices, as well as specific strategies that researchers use to meet goals of
equity and empowerment. To complement this analysis, we interview five AI
researchers to learn about the outcomes of recruitment methodologies. We find
that these outcomes are shaped by structural conditions of their work,
researchers' own goals and expectations, and the relationships built from the
recruitment methodology and subsequent collaboration. Based on these analyses,
we provide recommendations for designing and executing relationship-forward
recruitment methods, as well as reflexive recruitment documentation practices
for Participatory AI researchers.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [165] [Data-Efficient Point Cloud Semantic Segmentation Pipeline for Unimproved Roads](https://arxiv.org/abs/2508.20135)
*Andrew Yarovoi,Christopher R. Valenta*

Main category: eess.IV

TL;DR: 提出用于未改善道路等八类的高效点云分割管道和训练框架，在少量数据下提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 实现未改善道路等类别的鲁棒点云分割，解决低数据场景下的分割问题。

Method: 采用两阶段训练框架，先在公共数据集和少量领域内数据上预训练投影卷积神经网络，再在领域内数据微调轻量级预测头；探索点提示训练、流形混合和直方图归一化环境的应用。

Result: 使用50个标记点云，相比直接在领域内数据训练，平均交并比从33.5%提升到51.8%，整体准确率从85.5%提升到90.8%。

Conclusion: 多数据集预训练对提升泛化能力和有限监督下的鲁棒分割至关重要，提出的框架适用于低数据场景的3D语义分割。

Abstract: In this case study, we present a data-efficient point cloud segmentation
pipeline and training framework for robust segmentation of unimproved roads and
seven other classes. Our method employs a two-stage training framework: first,
a projection-based convolutional neural network is pre-trained on a mixture of
public urban datasets and a small, curated in-domain dataset; then, a
lightweight prediction head is fine-tuned exclusively on in-domain data. Along
the way, we explore the application of Point Prompt Training to batch
normalization layers and the effects of Manifold Mixup as a regularizer within
our pipeline. We also explore the effects of incorporating histogram-normalized
ambients to further boost performance. Using only 50 labeled point clouds from
our target domain, we show that our proposed training approach improves mean
Intersection-over-Union from 33.5% to 51.8% and the overall accuracy from 85.5%
to 90.8%, when compared to naive training on the in-domain data. Crucially, our
results demonstrate that pre-training across multiple datasets is key to
improving generalization and enabling robust segmentation under limited
in-domain supervision. Overall, this study demonstrates a practical framework
for robust 3D semantic segmentation in challenging, low-data scenarios. Our
code is available at: https://github.com/andrewyarovoi/MD-FRNet.

</details>


### [166] [UltraEar: a multicentric, large-scale database combining ultra-high-resolution computed tomography and clinical data for ear diseases](https://arxiv.org/abs/2508.20141)
*Ruowei Tang,Pengfei Zhao,Xiaoguang Li,Ning Xu,Yue Cheng,Mengshi Zhang,Zhixiang Wang,Zhengyu Zhang,Hongxia Yin,Heyu Ding,Shusheng Gong,Yuhe Liu,Zhenchang Wang*

Main category: eess.IV

TL;DR: 本文介绍了用于耳部疾病的大规模多中心UltraEar数据库的建立与设计，该数据库有巨大潜力推动相关研究等。


<details>
  <summary>Details</summary>
Motivation: 耳部疾病影响广泛，CT对其诊断等有重要作用，需建立耳部疾病相关数据库。

Method: 从11家三级医院招募患者，整合U - HRCT图像、CT报告和临床信息，开发标准化预处理流程，去除或匿名化个人标识符，通过专家会议协调数据处理，存储于离线云系统。

Result: 建立UltraEar数据库，提供超高分辨率参考图谱。

Conclusion: UltraEar数据库有潜力推动放射学研究、AI算法开发验证等，且会持续更新扩展，为全球耳科研究界服务。

Abstract: Ear diseases affect billions of people worldwide, leading to substantial
health and socioeconomic burdens. Computed tomography (CT) plays a pivotal role
in accurate diagnosis, treatment planning, and outcome evaluation. The
objective of this study is to present the establishment and design of UltraEar
Database, a large-scale, multicentric repository of isotropic 0.1 mm
ultra-high-resolution CT (U-HRCT) images and associated clinical data dedicated
to ear diseases. UltraEar recruits patients from 11 tertiary hospitals between
October 2020 and October 2035, integrating U-HRCT images, structured CT
reports, and comprehensive clinical information, including demographics,
audiometric profiles, surgical records, and pathological findings. A broad
spectrum of otologic disorders is covered, such as otitis media, cholesteatoma,
ossicular chain malformation, temporal bone fracture, inner ear malformation,
cochlear aperture stenosis, enlarged vestibular aqueduct, and sigmoid sinus
bony deficiency. Standardized preprocessing pipelines have been developed for
geometric calibration, image annotation, and multi-structure segmentation. All
personal identifiers in DICOM headers and metadata are removed or anonymized to
ensure compliance with data privacy regulation. Data collection and curation
are coordinated through monthly expert panel meetings, with secure storage on
an offline cloud system. UltraEar provides an unprecedented
ultra-high-resolution reference atlas with both technical fidelity and clinical
relevance. This resource has significant potential to advance radiological
research, enable development and validation of AI algorithms, serve as an
educational tool for training in otologic imaging, and support
multi-institutional collaborative studies. UltraEar will be continuously
updated and expanded, ensuring long-term accessibility and usability for the
global otologic research community.

</details>


### [167] [Is the medical image segmentation problem solved? A survey of current developments and future directions](https://arxiv.org/abs/2508.20139)
*Guoping Xu,Jayaram K. Udupa,Jax Luo,Songlin Zhao,Yajun Yu,Scott B. Raymond,Hao Peng,Lipeng Ning,Yogesh Rathi,Wei Liu,You Zhang*

Main category: eess.IV

TL;DR: 本文对基于深度学习的医学图像分割进行深度综述，从七个关键维度介绍进展，提供文献和开源资源，以启发未来创新。


<details>
  <summary>Details</summary>
Motivation: 探讨当前模型在医学图像分割中克服挑战的程度及尚存的差距。

Method: 回顾过去十年医学图像分割的进展和关键发展，研究分割网络各组件的核心原理，并围绕七个关键维度展开讨论。

Result: 从七个维度提供了基于深度学习的医学图像分割发展轨迹的全面概述。

Conclusion: 研究旨在启发未来医学图像分割领域的创新，同时维护相关文献和开源资源库支持研究。

Abstract: Medical image segmentation has advanced rapidly over the past two decades,
largely driven by deep learning, which has enabled accurate and efficient
delineation of cells, tissues, organs, and pathologies across diverse imaging
modalities. This progress raises a fundamental question: to what extent have
current models overcome persistent challenges, and what gaps remain? In this
work, we provide an in-depth review of medical image segmentation, tracing its
progress and key developments over the past decade. We examine core principles,
including multiscale analysis, attention mechanisms, and the integration of
prior knowledge, across the encoder, bottleneck, skip connections, and decoder
components of segmentation networks. Our discussion is organized around seven
key dimensions: (1) the shift from supervised to semi-/unsupervised learning,
(2) the transition from organ segmentation to lesion-focused tasks, (3)
advances in multi-modality integration and domain adaptation, (4) the role of
foundation models and transfer learning, (5) the move from deterministic to
probabilistic segmentation, (6) the progression from 2D to 3D and 4D
segmentation, and (7) the trend from model invocation to segmentation agents.
Together, these perspectives provide a holistic overview of the trajectory of
deep learning-based medical image segmentation and aim to inspire future
innovation. To support ongoing research, we maintain a continually updated
repository of relevant literature and open-source resources at
https://github.com/apple1986/medicalSegReview

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [168] [Is Audio Spoof Detection Robust to Laundering Attacks?](https://arxiv.org/abs/2408.14712)
*Hashim Ali,Surya Subramani,Shefali Sudhir,Raksha Varahamurthy,Hafiz Malik*

Main category: eess.AS

TL;DR: 本文评估了存在清洗攻击时SOTA音频欺骗检测方法，创建新数据库进行实验，结果表明SOTA系统在攻击下表现差，需更鲁棒检测方法。


<details>
  <summary>Details</summary>
Motivation: 语音克隆技术易被滥用，现有检测方法多在干净音频数据库评估，需评估其在清洗攻击下的性能。

Method: 创建基于ASVSpoof 2019的ASVSpoof Laundering Database，对七种SOTA音频欺骗检测方法进行评估。

Result: SOTA系统在激进清洗攻击（如混响和加性噪声攻击）下表现不佳。

Conclusion: 需要开发更鲁棒的音频欺骗检测方法。

Abstract: Voice-cloning (VC) systems have seen an exceptional increase in the realism
of synthesized speech in recent years. The high quality of synthesized speech
and the availability of low-cost VC services have given rise to many potential
abuses of this technology. Several detection methodologies have been proposed
over the years that can detect voice spoofs with reasonably good accuracy.
However, these methodologies are mostly evaluated on clean audio databases,
such as ASVSpoof 2019. This paper evaluates SOTA Audio Spoof Detection
approaches in the presence of laundering attacks. In that regard, a new
laundering attack database, called the ASVSpoof Laundering Database, is
created. This database is based on the ASVSpoof 2019 (LA) eval database
comprising a total of 1388.22 hours of audio recordings. Seven SOTA audio spoof
detection approaches are evaluated on this laundered database. The results
indicate that SOTA systems perform poorly in the presence of aggressive
laundering attacks, especially reverberation and additive noise attacks. This
suggests the need for robust audio spoof detection.

</details>


### [169] [Automatic Inspection Based on Switch Sounds of Electric Point Machines](https://arxiv.org/abs/2508.20870)
*Ayano Shibata,Toshiki Gunji,Mitsuaki Tsuda,Takashi Endo,Kota Dohi,Tomoya Nishida,Satoko Nomoto*

Main category: eess.AS

TL;DR: 东日本铁路公司和日立自2018年起用基于物联网的监测取代人工检查，2019年在电动转辙机上应用摄像头和麦克风，提出基于声音信息检测道岔转换错误的方法，能实时检测设备故障，减少目视检查。


<details>
  <summary>Details</summary>
Motivation: 实现设备检查的省力化并提供适当的预防性维护，解决电气特性监测替代目视检查难和新高性能传感器成本高的问题。

Method: 在“NS”电动转辙机上安装摄像头和麦克风，提出基于声音信息检测道岔转换错误的方法。

Result: 获得了预期的测试结果。

Conclusion: 所提出的方法可实时检测设备故障，减少目视检查需求，有助于实现电动转辙机检查自动化。

Abstract: Since 2018, East Japan Railway Company and Hitachi, Ltd. have been working to
replace human inspections with IoT-based monitoring. The purpose is
Labor-saving required for equipment inspections and provide appropriate
preventive maintenance. As an alternative to visual inspection, it has been
difficult to substitute electrical characteristic monitoring, and the
introduction of new high-performance sensors has been costly. In 2019, we
implemented cameras and microphones in an ``NS'' electric point machines to
reduce downtime from equipment failures, allowing for remote monitoring of
lock-piece conditions. This method for detecting turnout switching errors based
on sound information was proposed, and the expected test results were obtained.
The proposed method will make it possible to detect equipment failures in real
time, thereby reducing the need for visual inspections. This paper presents the
results of our technical studies aimed at automating the inspection of
electronic point machines using sound, specifically focusing on ``switch
sound'' beginning in 2019.

</details>


### [170] [Multilingual Dataset Integration Strategies for Robust Audio Deepfake Detection: A SAFE Challenge System](https://arxiv.org/abs/2508.20983)
*Hashim Ali,Surya Subramani,Lekha Bollinani,Nithin Sai Adupa,Sali El-Loh,Hafiz Malik*

Main category: eess.AS

TL;DR: 论文介绍SAFE挑战赛，系统探索自监督学习前端等因素用于深度伪造检测，基于AASIST的方法获挑战赛两项第二。


<details>
  <summary>Details</summary>
Motivation: 在SAFE挑战赛的三个任务中实现鲁棒的合成语音检测。

Method: 采用基于AASIST的方法，结合WavLM large前端和RawBoost增强，在多语言数据集上训练。

Result: 在任务1和任务3中获得第二名。

Conclusion: 所采用的方法具有强泛化能力和鲁棒性。

Abstract: The SAFE Challenge evaluates synthetic speech detection across three tasks:
unmodified audio, processed audio with compression artifacts, and laundered
audio designed to evade detection. We systematically explore self-supervised
learning (SSL) front-ends, training data compositions, and audio length
configurations for robust deepfake detection. Our AASIST-based approach
incorporates WavLM large frontend with RawBoost augmentation, trained on a
multilingual dataset of 256,600 samples spanning 9 languages and over 70 TTS
systems from CodecFake, MLAAD v5, SpoofCeleb, Famous Figures, and MAILABS.
Through extensive experimentation with different SSL front-ends, three training
data versions, and two audio lengths, we achieved second place in both Task 1
(unmodified audio detection) and Task 3 (laundered audio detection),
demonstrating strong generalization and robustness.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [171] [Lattice Random Walk Discretisations of Stochastic Differential Equations](https://arxiv.org/abs/2508.20883)
*Samuel Duffield,Maxwell Aifer,Denis Melanson,Zach Belateche,Patrick J. Coles*

Main category: math.NA

TL;DR: 提出一种格随机游走离散化方案用于随机微分方程，有诸多优势并证明弱收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统浮点离散化存在局限，需要新的离散化方案以适应随机计算架构等需求。

Method: 引入格随机游走离散化方案，每步采样二元或三元增量，将复杂漂移和扩散计算简化为1或2位随机值。

Result: 证明了弱收敛性，并通过对各种随机微分方程（包括先进扩散模型）的实验展示了优势。

Conclusion: 该离散化方案是对传统浮点离散化的显著改进，具有多方面优势。

Abstract: We introduce a lattice random walk discretisation scheme for stochastic
differential equations (SDEs) that samples binary or ternary increments at each
step, suppressing complex drift and diffusion computations to simple 1 or 2 bit
random values. This approach is a significant departure from traditional
floating point discretisations and offers several advantages; including
compatibility with stochastic computing architectures that avoid floating-point
arithmetic in place of directly manipulating the underlying probability
distribution of a bitstream, elimination of Gaussian sampling requirements,
robustness to quantisation errors, and handling of non-Lipschitz drifts. We
prove weak convergence and demonstrate the advantages through experiments on
various SDEs, including state-of-the-art diffusion models.

</details>


### [172] [Operator learning meets inverse problems: A probabilistic perspective](https://arxiv.org/abs/2508.20207)
*Nicholas H. Nelsen,Yunan Yang*

Main category: math.NA

TL;DR: 本文综述算子学习与逆问题交叉领域的方法和理论进展，涵盖逆问题求解方法、算子学习组件、端到端逆算子学习范式等内容。


<details>
  <summary>Details</summary>
Motivation: 算子学习为无限维函数空间映射逼近提供框架，是解决计算科学中逆问题的有力工具，有必要对该交叉领域进展进行综述。

Method: 先总结逆问题的概率和确定性方法，再介绍算子学习的关键组件，重点讨论端到端逆算子学习范式，还涉及先验和正则化估计。

Result: 对算子学习与逆问题交叉领域的多方面内容进行梳理，包括应对噪声挑战、提出结构感知架构、相关理论等。

Conclusion: 全面介绍了算子学习与逆问题交叉领域的发展，展示了端到端逆算子学习范式等方法的特点和应用。

Abstract: Operator learning offers a robust framework for approximating mappings
between infinite-dimensional function spaces. It has also become a powerful
tool for solving inverse problems in the computational sciences. This chapter
surveys methodological and theoretical developments at the intersection of
operator learning and inverse problems. It begins by summarizing the
probabilistic and deterministic approaches to inverse problems, and pays
special attention to emerging measure-centric formulations that treat observed
data or unknown parameters as probability distributions. The discussion then
turns to operator learning by covering essential components such as data
generation, loss functions, and widely used architectures for representing
function-to-function maps. The core of the chapter centers on the end-to-end
inverse operator learning paradigm, which aims to directly map observed data to
the solution of the inverse problem without requiring explicit knowledge of the
forward map. It highlights the unique challenge that noise plays in this
data-driven inversion setting, presents structure-aware architectures for both
point predictions and posterior estimates, and surveys relevant theory for
linear and nonlinear inverse problems. The chapter also discusses the
estimation of priors and regularizers, where operator learning is used more
selectively within classical inversion algorithms.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [173] [Machine-learning based particle-flow algorithm in CMS](https://arxiv.org/abs/2508.20541)
*Farouk Mokhtar*

Main category: hep-ex

TL;DR: 介绍CMS在机器学习粒子流（MLPF）方面的最新进展，包括训练数据集、模型架构等。


<details>
  <summary>Details</summary>
Motivation: 粒子流（PF）算法对CMS事件重建很重要，端到端机器学习方法可优化物理量并利用异构计算架构。

Method: 采用Transformer模型，从轨道和簇中一次性推断粒子。

Result: 展示了CMS在MLPF方面的发展情况，如训练数据集、模型架构等。

Conclusion: 文中未明确提及结论。

Abstract: The particle-flow (PF) algorithm provides a global event description by
reconstructing final-state particles and is central to event reconstruction in
CMS. Recently, end-to-end machine learning (ML) approaches have been proposed
to directly optimize physical quantities of interest and to leverage
heterogeneous computing architectures. One such approach, machine-learned
particle flow (MLPF), uses a transformer model to infer particles directly from
tracks and clusters in a single pass. We present recent CMS developments in
MLPF, including training datasets, model architecture, reconstruction metrics,
and integration with offline reconstruction software.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [174] [SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes](https://arxiv.org/abs/2508.20547)
*Yunpeng Mei,Hongjie Cao,Yinqiu Xia,Wei Xiao,Zhaohan Feng,Gang Wang,Jie Chen*

Main category: cs.RO

TL;DR: 提出SPGrasp框架解决动态物体实时交互抓取合成的低延迟与交互性问题，在多数据集表现优异并通过真实实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态物体实时交互抓取合成中难以兼顾低延迟推理和交互性，需新方法解决。

Method: 提出SPGrasp框架，将用户提示与时空上下文结合，扩展SAMv2用于视频流抓取估计。

Result: 在OCID、Jacquard等数据集取得高抓取准确率，在GraspNet - 1Billion上相比RoG - SAM减少58.5%延迟且保持竞争力，真实实验成功率94.8%。

Conclusion: SPGrasp有效解决了动态抓取合成中延迟与交互性的权衡问题。

Abstract: Real-time interactive grasp synthesis for dynamic objects remains challenging
as existing methods fail to achieve low-latency inference while maintaining
promptability. To bridge this gap, we propose SPGrasp (spatiotemporal
prompt-driven dynamic grasp synthesis), a novel framework extending segment
anything model v2 (SAMv2) for video stream grasp estimation. Our core
innovation integrates user prompts with spatiotemporal context, enabling
real-time interaction with end-to-end latency as low as 59 ms while ensuring
temporal consistency for dynamic objects. In benchmark evaluations, SPGrasp
achieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on
Jacquard. On the challenging GraspNet-1Billion dataset under continuous
tracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency,
representing a 58.5% reduction compared to the prior state-of-the-art
promptable method RoG-SAM while maintaining competitive accuracy. Real-world
experiments involving 13 moving objects demonstrate a 94.8% success rate in
interactive grasping scenarios. These results confirm SPGrasp effectively
resolves the latency-interactivity trade-off in dynamic grasp synthesis. Code
is available at https://github.com/sejmoonwei/SPGrasp.

</details>


### [175] [Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse](https://arxiv.org/abs/2508.20664)
*Kan Chen,Zhen Meng,Xiangmin Xu,Jiaming Yang,Emma Li,Philip G. Zhao*

Main category: cs.RO

TL;DR: 本文提出面向任务的边缘辅助跨系统框架，引入HITL - MAML算法，经评估能确保实时高危工业环境的空间精度和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 解决工业元宇宙中实时人机交互面临的高计算负载、带宽有限和严格延迟等挑战。

Method: 提出使用数字孪生的面向任务的边缘辅助跨系统框架，将数字孪生解耦为视觉显示和机器人控制两个虚拟功能，引入HITL - MAML算法动态调整预测范围。

Result: 在基于轨迹的绘图控制任务中，加权RMSE从0.0712 m降至0.0101 m；在核退役实时3D场景表示任务中，PSNR为22.11，SSIM为0.8729，LPIPS为0.1298。

Conclusion: 该框架能够确保实时、高危工业环境中的空间精度和视觉保真度。

Abstract: Real-time human-device interaction in industrial Metaverse faces challenges
such as high computational load, limited bandwidth, and strict latency. This
paper proposes a task-oriented edge-assisted cross-system framework using
digital twins (DTs) to enable responsive interactions. By predicting operator
motions, the system supports: 1) proactive Metaverse rendering for visual
feedback, and 2) preemptive control of remote devices. The DTs are decoupled
into two virtual functions-visual display and robotic control-optimizing both
performance and adaptability. To enhance generalizability, we introduce the
Human-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which
dynamically adjusts prediction horizons. Evaluation on two tasks demonstrates
the framework's effectiveness: in a Trajectory-Based Drawing Control task, it
reduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene
representation task for nuclear decommissioning, it achieves a PSNR of 22.11,
SSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's
capability to ensure spatial precision and visual fidelity in real-time,
high-risk industrial environments.

</details>


### [176] [Task Allocation for Autonomous Machines using Computational Intelligence and Deep Reinforcement Learning](https://arxiv.org/abs/2508.20688)
*Thanh Thi Nguyen,Quoc Viet Hung Nguyen,Jonathan Kua,Imran Razzak,Dung Nguyen,Saeid Nahavandi*

Main category: cs.RO

TL;DR: 本文综述复杂环境下自主机器控制与协调算法，聚焦计算智能和深度强化学习的任务分配方法，分析优缺点，提出未来研究方向，指出相关方法可行且深度强化学习成趋势。


<details>
  <summary>Details</summary>
Motivation: 开发高效的合作控制算法，使多个自主机器能可靠运行。

Method: 对现有控制和协调自主机器的算法进行综述，分析优缺点，提出未来研究方向。

Result: 计算智能和深度强化学习方法为动态和不确定环境中的复杂任务分配问题提供了可行途径，深度强化学习发展显著。

Conclusion: 为研究人员和工程师提供自主机器机器学习研究进展的全面概述，指明未充分探索领域和新兴方法及未来研究方向。

Abstract: Enabling multiple autonomous machines to perform reliably requires the
development of efficient cooperative control algorithms. This paper presents a
survey of algorithms that have been developed for controlling and coordinating
autonomous machines in complex environments. We especially focus on task
allocation methods using computational intelligence (CI) and deep reinforcement
learning (RL). The advantages and disadvantages of the surveyed methods are
analysed thoroughly. We also propose and discuss in detail various future
research directions that shed light on how to improve existing algorithms or
create new methods to enhance the employability and performance of autonomous
machines in real-world applications. The findings indicate that CI and deep RL
methods provide viable approaches to addressing complex task allocation
problems in dynamic and uncertain environments. The recent development of deep
RL has greatly contributed to the literature on controlling and coordinating
autonomous machines, and it has become a growing trend in this area. It is
envisaged that this paper will provide researchers and engineers with a
comprehensive overview of progress in machine learning research related to
autonomous machines. It also highlights underexplored areas, identifies
emerging methodologies, and suggests new avenues for exploration in future
research within this domain.

</details>


### [177] [CoCoL: A Communication Efficient Decentralized Collaborative Method for Multi-Robot Systems](https://arxiv.org/abs/2508.20898)
*Jiaxi Huang,Yan Huang,Yixian Zhao,Wenchao Meng,Jinming Xu*

Main category: cs.RO

TL;DR: 提出适用于多机器人系统的通信高效的去中心化协作学习方法CoCoL，实验表明其能减少通信轮数和带宽消耗，保持高精度。


<details>
  <summary>Details</summary>
Motivation: 多机器人协作学习面临高通信开销和数据异构挑战，需提高通信效率和应对数据异构。

Method: 采用镜像下降框架，用近似牛顿型更新，通过不精确子问题解降低计算成本，集成梯度跟踪方案。

Result: 在三个代表性多机器人协作学习任务实验中，CoCoL显著减少通信轮数和总带宽消耗，保持高精度，在非IID数据、流数据和时变网络拓扑场景优势明显。

Conclusion: CoCoL在多机器人协作学习中能有效提升通信效率，应对数据异构，有良好表现。

Abstract: Collaborative learning enhances the performance and adaptability of
multi-robot systems in complex tasks but faces significant challenges due to
high communication overhead and data heterogeneity inherent in multi-robot
tasks. To this end, we propose CoCoL, a Communication efficient decentralized
Collaborative Learning method tailored for multi-robot systems with
heterogeneous local datasets. Leveraging a mirror descent framework, CoCoL
achieves remarkable communication efficiency with approximate Newton-type
updates by capturing the similarity between objective functions of robots, and
reduces computational costs through inexact sub-problem solutions. Furthermore,
the integration of a gradient tracking scheme ensures its robustness against
data heterogeneity. Experimental results on three representative multi robot
collaborative learning tasks show the superiority of the proposed CoCoL in
significantly reducing both the number of communication rounds and total
bandwidth consumption while maintaining state-of-the-art accuracy. These
benefits are particularly evident in challenging scenarios involving non-IID
(non-independent and identically distributed) data distribution, streaming
data, and time-varying network topologies.

</details>


### [178] [ActLoc: Learning to Localize on the Move via Active Viewpoint Selection](https://arxiv.org/abs/2508.20981)
*Jiajie Li,Boyang Sun,Luca Di Giammarino,Hermann Blum,Marc Pollefeys*

Main category: cs.RO

TL;DR: 提出ActLoc框架增强机器人导航定位精度，用注意力模型选视角，在单视角选择和全轨迹规划表现好，适用于多种任务。


<details>
  <summary>Details</summary>
Motivation: 现有系统假设各视角信息等同，实际中机器人观测无地图、模糊或无信息区域时定位不可靠，需提升定位精度。

Method: 采用大规模训练的基于注意力的模型进行视角选择，将每点精度分布融入路径规划器。

Result: 在单视角选择上取得了最先进的结果，能有效推广到全轨迹规划。

Conclusion: ActLoc模块化设计适用于多种机器人导航和检查任务。

Abstract: Reliable localization is critical for robot navigation, yet most existing
systems implicitly assume that all viewing directions at a location are equally
informative. In practice, localization becomes unreliable when the robot
observes unmapped, ambiguous, or uninformative regions. To address this, we
present ActLoc, an active viewpoint-aware planning framework for enhancing
localization accuracy for general robot navigation tasks. At its core, ActLoc
employs a largescale trained attention-based model for viewpoint selection. The
model encodes a metric map and the camera poses used during map construction,
and predicts localization accuracy across yaw and pitch directions at arbitrary
3D locations. These per-point accuracy distributions are incorporated into a
path planner, enabling the robot to actively select camera orientations that
maximize localization robustness while respecting task and motion constraints.
ActLoc achieves stateof-the-art results on single-viewpoint selection and
generalizes effectively to fulltrajectory planning. Its modular design makes it
readily applicable to diverse robot navigation and inspection tasks.

</details>


### [179] [Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting](https://arxiv.org/abs/2508.20812)
*Lorenzo Busellato,Federico Cunico,Diego Dall'Alba,Marco Emporio,Andrea Giachetti,Riccardo Muradore,Marco Cristani*

Main category: cs.RO

TL;DR: 提出不确定性感知预测控制障碍函数（UA - PCBFs）框架，融合概率性人类手部运动预测与控制障碍函数的安全保障，经实验验证优于现有架构。


<details>
  <summary>Details</summary>
Motivation: 协作机器人需在安全保障和有效行为间平衡，现有基于学习的人类运动预测存在处理不确定性不佳、规划算法保守的问题。

Method: 引入UA - PCBFs框架，利用预测模块的人类运动不确定性估计动态调整安全裕度，进行知情运动规划。

Result: 通过不同现实程度的实验验证，在关键任务指标上优于现有HRI架构，显著减少交互中机器人安全空间的违规次数。

Conclusion: UA - PCBFs框架能让协作机器人更好理解人类未来状态，实现更流畅智能的人机交互。

Abstract: To enable flexible, high-throughput automation in settings where people and
robots share workspaces, collaborative robotic cells must reconcile stringent
safety guarantees with the need for responsive and effective behavior. A
dynamic obstacle is the stochastic, task-dependent variability of human motion:
when robots fall back on purely reactive or worst-case envelopes, they brake
unnecessarily, stall task progress, and tamper with the fluidity that true
Human-Robot Interaction demands. In recent years, learning-based human-motion
prediction has rapidly advanced, although most approaches produce worst-case
scenario forecasts that often do not treat prediction uncertainty in a
well-structured way, resulting in over-conservative planning algorithms,
limiting their flexibility. We introduce Uncertainty-Aware Predictive Control
Barrier Functions (UA-PCBFs), a unified framework that fuses probabilistic
human hand motion forecasting with the formal safety guarantees of Control
Barrier Functions. In contrast to other variants, our framework allows for
dynamic adjustment of the safety margin thanks to the human motion uncertainty
estimation provided by a forecasting module. Thanks to uncertainty estimation,
UA-PCBFs empower collaborative robots with a deeper understanding of future
human states, facilitating more fluid and intelligent interactions through
informed motion planning. We validate UA-PCBFs through comprehensive real-world
experiments with an increasing level of realism, including automated setups (to
perform exactly repeatable motions) with a robotic hand and direct human-robot
interactions (to validate promptness, usability, and human confidence).
Relative to state-of-the-art HRI architectures, UA-PCBFs show better
performance in task-critical metrics, significantly reducing the number of
violations of the robot's safe space during interaction with respect to the
state-of-the-art.

</details>


### [180] [Learning Primitive Embodied World Models: Towards Scalable Robotic Learning](https://arxiv.org/abs/2508.20840)
*Qiao Sun,Liujia Yang,Wei Tang,Wei Huang,Kaixin Xu,Yongchao Chen,Mingyu Liu,Jiange Yang,Haoyi Zhu,Yating Wang,Tong He,Yilun Chen,Xili Dai,Nanyang Ye,Qinying Gu*

Main category: cs.RO

TL;DR: 现有视频生成式具身世界模型依赖大规模数据，提出Primitive Embodied World Models (PEWM)，可提升性能并支持复杂任务。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成式具身世界模型依赖大规模具身交互数据，数据稀缺、收集难和高维性限制语言与动作对齐粒度，阻碍长时视频生成。

Method: 提出PEWM，将视频生成限制在固定短时间内，配备模块化视觉语言模型(VLM)规划器和Start - Goal热图引导机制(SGG)。

Result: 实现语言概念与机器人动作视觉表征的细粒度对齐，降低学习复杂度，提高数据收集效率，减少推理延迟，支持复杂任务的组合泛化。

Conclusion: 利用视频模型的时空视觉先验和VLM的语义感知，弥合细粒度物理交互和高级推理间的差距，为具身智能发展铺平道路。

Abstract: While video-generation-based embodied world models have gained increasing
attention, their reliance on large-scale embodied interaction data remains a
key bottleneck. The scarcity, difficulty of collection, and high dimensionality
of embodied data fundamentally limit the alignment granularity between language
and actions and exacerbate the challenge of long-horizon video
generation--hindering generative models from achieving a "GPT moment" in the
embodied domain. There is a naive observation: the diversity of embodied data
far exceeds the relatively small space of possible primitive motions. Based on
this insight, we propose a novel paradigm for world modeling--Primitive
Embodied World Models (PEWM). By restricting video generation to fixed short
horizons, our approach 1) enables fine-grained alignment between linguistic
concepts and visual representations of robotic actions, 2) reduces learning
complexity, 3) improves data efficiency in embodied data collection, and 4)
decreases inference latency. By equipping with a modular Vision-Language Model
(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further
enables flexible closed-loop control and supports compositional generalization
of primitive-level policies over extended, complex tasks. Our framework
leverages the spatiotemporal vision priors in video models and the semantic
awareness of VLMs to bridge the gap between fine-grained physical interaction
and high-level reasoning, paving the way toward scalable, interpretable, and
general-purpose embodied intelligence.

</details>


### [181] [Prompt-to-Product: Generative Assembly via Bimanual Manipulation](https://arxiv.org/abs/2508.21063)
*Ruixuan Liu,Philip Huang,Ava Pun,Kangle Deng,Shobhit Aggarwal,Kevin Tang,Michelle Liu,Deva Ramanan,Jun-Yan Zhu,Jiaoyang Li,Changliu Liu*

Main category: cs.RO

TL;DR: 本文介绍了Prompt - to - Product自动化流程，可根据自然语言提示生成乐高积木组装产品，降低了创造组装产品的门槛和人力成本。


<details>
  <summary>Details</summary>
Motivation: 创造组装产品需要大量人工和专业知识，希望降低创造组装产品的门槛和减少人工投入。

Method: 以乐高积木为组装平台，根据用户设计需求生成可实际搭建的积木设计，利用双手机器人系统构建实际组装产品。

Result: 通过全面用户研究表明，Prompt - to - Product显著降低了从创意想法创造组装产品的门槛并减少了人工投入。

Conclusion: Prompt - to - Product是一种有效的自动化流程，能将用户想象变为现实，降低创造组装产品的难度和人力成本。

Abstract: Creating assembly products demands significant manual effort and expert
knowledge in 1) designing the assembly and 2) constructing the product. This
paper introduces Prompt-to-Product, an automated pipeline that generates
real-world assembly products from natural language prompts. Specifically, we
leverage LEGO bricks as the assembly platform and automate the process of
creating brick assembly structures. Given the user design requirements,
Prompt-to-Product generates physically buildable brick designs, and then
leverages a bimanual robotic system to construct the real assembly products,
bringing user imaginations into the real world. We conduct a comprehensive user
study, and the results demonstrate that Prompt-to-Product significantly lowers
the barrier and reduces manual effort in creating assembly products from
imaginative ideas.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [182] [MM-HSD: Multi-Modal Hate Speech Detection in Videos](https://arxiv.org/abs/2508.20546)
*Berta Céspedes-Sarrias,Carlos Collado-Capell,Pablo Rodenas-Ruiz,Olena Hrynenko,Andrea Cavallaro*

Main category: cs.MM

TL;DR: 本文提出多模态模型MM - HSD用于视频仇恨言论检测，采用跨模态注意力提取特征，在HateMM数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态仇恨言论检测方法有限，简单融合方法无法捕捉模态间依赖，且常忽略屏幕文本和音频等模态。

Method: 提出MM - HSD模型，融合视频帧、音频、语音转录文本和屏幕文本，用跨模态注意力（CMA）提取特征，系统比较查询/键配置并评估CMA模块中不同模态间的交互。

Result: 在HateMM数据集上，MM - HSD的M - F1分数达0.874，优于现有方法。

Conclusion: 使用屏幕文本作为查询，其他模态作为键时性能提升，MM - HSD是有效的视频仇恨言论检测多模态模型。

Abstract: While hate speech detection (HSD) has been extensively studied in text,
existing multi-modal approaches remain limited, particularly in videos. As
modalities are not always individually informative, simple fusion methods fail
to fully capture inter-modal dependencies. Moreover, previous work often omits
relevant modalities such as on-screen text and audio, which may contain subtle
hateful content and thus provide essential cues, both individually and in
combination with others. In this paper, we present MM-HSD, a multi-modal model
for HSD in videos that integrates video frames, audio, and text derived from
speech transcripts and from frames (i.e.~on-screen text) together with features
extracted by Cross-Modal Attention (CMA). We are the first to use CMA as an
early feature extractor for HSD in videos, to systematically compare query/key
configurations, and to evaluate the interactions between different modalities
in the CMA block. Our approach leads to improved performance when on-screen
text is used as a query and the rest of the modalities serve as a key.
Experiments on the HateMM dataset show that MM-HSD outperforms state-of-the-art
methods on M-F1 score (0.874), using concatenation of transcript, audio, video,
on-screen text, and CMA for feature extraction on raw embeddings of the
modalities. The code is available at https://github.com/idiap/mm-hsd

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [183] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: 本文提出CHAIR - DPO方法解决多模态大语言模型的幻觉问题，在多个基准测试上有效减少幻觉答案，代码和模型公开。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在幻觉问题，即生成的答案未反映视觉输入，需要解决该问题。

Method: 将幻觉问题视为对齐问题，利用CHAIR指标区分生成答案中的非幻觉和幻觉样本，通过直接偏好优化（DPO）微调现成的多模态大语言模型。

Result: CHAIR - DPO方法在多个幻觉基准测试上有效减少了幻觉答案。

Conclusion: 基于CHAIR的奖励微调多模态大语言模型是有效的。

Abstract: Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [184] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

TL;DR: 本文提出在样本和数据集层面解释视觉模型的管道，结合视觉模型开发与xAI分析以推进图像分析。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型开发多关注性能指标，对可解释性关注少，且解释模型一般行为的方法研究不足，理解模型在通用图像上的行为很重要。

Method: 提出在样本和数据集层面解释视觉模型的管道。

Result: 可轻松发现失败案例并深入了解视觉模型。

Conclusion: 将视觉模型开发与xAI分析结合可推进图像分析。

Abstract: The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [185] [MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces](https://arxiv.org/abs/2508.20256)
*Zhen Xuen Brandon Low,Rory Zhang,Hang Min,William Pham,Lucy Vivash,Jasmine Moses,Miranda Lynch,Karina Dorfman,Cassandra Marotta,Shaun Koh,Jacob Bunyamin,Ella Rowsthorn,Alex Jarema,Himashi Peiris,Zhaolin Chen,Sandy R. Shultz,David K. Wright,Dexiao Kong,Sharon L. Naismith,Terence J. O'Brien,Ying Xia,Meng Law,Benjamin Sinclair*

Main category: cs.CV

TL;DR: 本文将MedNeXt - L - k5用于自动PVS分割，在不同数据集上评估其性能，表明它能跨T1w和T2w MRI数据集提供有效解决方案，但未超越nnU - Net。


<details>
  <summary>Details</summary>
Motivation: 手动分割PVS耗时且可靠性有限，现有自动深度学习模型性能一般且泛化能力不足，需更好的自动分割方法。

Method: 采用MedNeXt - L - k5进行自动PVS分割，用200个T2w MRI扫描和40个T1w MRI体积分别训练模型，用5FCV和LOSOCV评估性能。

Result: T2w图像训练的模型在白质的体素级Dice分数达0.88±0.06；T1w图像训练的为0.58±0.09；LOSOCV下体素和簇级有相应分数。

Conclusion: MedNeXt - L - k5可跨不同数据集提供有效PVS分割方案，但基于注意力机制的Transformer模型对高精度PVS分割非必需。

Abstract: Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers
of cerebral small vessel disease, Alzheimer's disease, stroke, and
aging-related neurodegeneration. However, manual segmentation of PVS is
time-consuming and subject to moderate inter-rater reliability, while existing
automated deep learning models have moderate performance and typically fail to
generalize across diverse clinical and research MRI datasets. We adapted
MedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network,
for automated PVS segmentation. Two models were trained: one using a
homogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human
Connectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous
T1-weighted (T1w) MRI volumes from seven studies across six scanners. Model
performance was evaluated using internal 5-fold cross validation (5FCV) and
leave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on
the T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of
0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater
reliability of that dataset, and the highest yet reported in the literature.
The same models trained on the T1w images of the HCP-Aging dataset achieved a
substantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had
voxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and
cluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG).
MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation
across diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the
nnU-Net, indicating that the attention-based mechanisms present in
transformer-inspired models to provide global context are not required for high
accuracy in PVS segmentation.

</details>


### [186] [How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding](https://arxiv.org/abs/2508.20279)
*Zhuoran Yu,Yong Jae Lee*

Main category: cs.CV

TL;DR: 本文提出探测框架分析多模态大语言模型处理视觉和文本输入的层间动态，发现其阶段性结构，且该结构稳定性及层分配受不同因素影响。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型内部处理动态未充分研究，需系统性分析其处理视觉和文本输入的机制。

Method: 引入探测框架，训练线性分类器，通过三种控制提示变体评估探针。

Result: 发现模型有阶段性结构，整体结构稳定，但特定层分配受基础大语言模型架构影响。

Conclusion: 研究为多模态大语言模型层组织提供统一视角，提供轻量级、与模型无关的分析方法。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance
across a wide range of vision-language tasks, yet their internal processing
dynamics remain underexplored. In this work, we introduce a probing framework
to systematically analyze how MLLMs process visual and textual inputs across
layers. We train linear classifiers to predict fine-grained visual categories
(e.g., dog breeds) from token embeddings extracted at each layer, using a
standardized anchor question. To uncover the functional roles of different
layers, we evaluate these probes under three types of controlled prompt
variations: (1) lexical variants that test sensitivity to surface-level
changes, (2) semantic negation variants that flip the expected answer by
modifying the visual concept in the prompt, and (3) output format variants that
preserve reasoning but alter the answer format. Applying our framework to
LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent
stage-wise structure in which early layers perform visual grounding, middle
layers support lexical integration and semantic reasoning, and final layers
prepare task-specific outputs. We further show that while the overall
stage-wise structure remains stable across variations in visual tokenization,
instruction tuning data, and pretraining corpus, the specific layer allocation
to each stage shifts notably with changes in the base LLM architecture. Our
findings provide a unified perspective on the layer-wise organization of MLLMs
and offer a lightweight, model-agnostic approach for analyzing multimodal
representation dynamics.

</details>


### [187] [Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection](https://arxiv.org/abs/2508.20392)
*Chengjun Zhang,Yuhao Zhang,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: 本文提出延迟尖峰方法和tdIF神经元架构，解决残膜电位问题，实现低时间步长下精确特征表示，在视觉检测任务中性能优。


<details>
  <summary>Details</summary>
Motivation: 当前ANN - SNN转换方法在视觉检测任务中表现不佳，需改进。

Method: 提出延迟尖峰方法缓解残膜电位问题，提出tdIF神经元架构使IF神经元能根据时间步动态调整行为。

Result: 在物体检测和车道线检测任务中评估，该方法超越当前ANN - SNN转换方法，在超低延迟（5个时间步内）下达到最优性能。

Conclusion: 所提方法可在低时间步长下实现精确特征表示，在视觉检测任务中实现高性能和超低延迟。

Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by
minimal power consumption and swift inference capabilities on neuromorphic
hardware, and have been widely applied to various visual perception tasks.
Current ANN-SNN conversion methods have achieved excellent results in
classification tasks with ultra-low time-steps, but their performance in visual
detection tasks remains suboptimal. In this paper, we propose a delay-spike
approach to mitigate the issue of residual membrane potential caused by
heterogeneous spiking patterns. Furthermore, we propose a novel
temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This
enables Integrate-and-fire (IF) neurons to dynamically adjust their
accumulation and firing behaviors based on the temporal order of time-steps.
Our method enables spikes to exhibit distinct temporal properties, rather than
relying solely on frequency-based representations. Moreover, the tdIF neuron
maintains energy consumption on par with traditional IF neuron. We demonstrate
that our method achieves more precise feature representation with lower
time-steps, enabling high performance and ultra-low latency in visual detection
tasks. In this study, we conduct extensive evaluation of the tdIF method across
two critical vision tasks: object detection and lane line detection. The
results demonstrate that the proposed method surpasses current ANN-SNN
conversion approaches, achieving state-of-the-art performance with ultra-low
latency (within 5 time-steps).

</details>


### [188] [Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification](https://arxiv.org/abs/2508.20461)
*Ayaka Tsutsumi,Guang Li,Ren Togo,Takahiro Ogawa,Satoshi Kondo,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出结合双模型权重选择与自知识蒸馏的医学图像分类方法，实验证明性能和鲁棒性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实医疗场景中，大规模模型受计算资源限制，需开发轻量级且性能相当的模型。

Method: 采用双模型权重选择策略，用大预训练模型权重初始化两个轻量级模型，再对选定模型应用自知识蒸馏并微调。

Result: 在胸部X光、肺部CT和脑部MRI等公开数据集上实验，该方法性能和鲁棒性优于现有方法。

Conclusion: 结合双模型权重选择与自知识蒸馏的方法克服了传统方法局限，表现更优。

Abstract: We propose a novel medical image classification method that integrates
dual-model weight selection with self-knowledge distillation (SKD). In
real-world medical settings, deploying large-scale models is often limited by
computational resource constraints, which pose significant challenges for their
practical implementation. Thus, developing lightweight models that achieve
comparable performance to large-scale models while maintaining computational
efficiency is crucial. To address this, we employ a dual-model weight selection
strategy that initializes two lightweight models with weights derived from a
large pretrained model, enabling effective knowledge transfer. Next, SKD is
applied to these selected models, allowing the use of a broad range of initial
weight configurations without imposing additional excessive computational cost,
followed by fine-tuning for the target classification tasks. By combining
dual-model weight selection with self-knowledge distillation, our method
overcomes the limitations of conventional approaches, which often fail to
retain critical information in compact models. Extensive experiments on
publicly available datasets-chest X-ray images, lung computed tomography scans,
and brain magnetic resonance imaging scans-demonstrate the superior performance
and robustness of our approach compared to existing methods.

</details>


### [189] [CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information](https://arxiv.org/abs/2508.20491)
*Seunghyeon Jung,Seoyoung Hong,Jiwoo Jeong,Seungwon Jeong,Jaerim Choi,Hoki Kim,Woojin Lee*

Main category: cs.CV

TL;DR: 提出新数据集CaddieSet用于高尔夫挥杆分析，验证其预测球轨迹可行性及挥杆反馈合理性。


<details>
  <summary>Details</summary>
Motivation: 现有研究未定量建立挥杆姿势与球轨迹关系，无法为高尔夫球手提供挥杆改进见解。

Method: 提出包含单次击球关节和球信息的CaddieSet数据集，用计算机视觉方法将单次挥杆视频分段提取关节信息，基于专业知识定义15个关键指标。

Result: 通过实验验证CaddieSet在不同基准下预测球轨迹的可行性，验证基于关节特征的挥杆反馈与领域知识定量一致。

Conclusion: 该工作有望为学术界和体育产业的高尔夫挥杆分析提供新见解。

Abstract: Recent advances in deep learning have led to more studies to enhance golfers'
shot precision. However, these existing studies have not quantitatively
established the relationship between swing posture and ball trajectory,
limiting their ability to provide golfers with the necessary insights for swing
improvement. In this paper, we propose a new dataset called CaddieSet, which
includes joint information and various ball information from a single shot.
CaddieSet extracts joint information from a single swing video by segmenting it
into eight swing phases using a computer vision-based approach. Furthermore,
based on expert golf domain knowledge, we define 15 key metrics that influence
a golf swing, enabling the interpretation of swing outcomes through
swing-related features. Through experiments, we demonstrated the feasibility of
CaddieSet for predicting ball trajectories using various benchmarks. In
particular, we focus on interpretable models among several benchmarks and
verify that swing feedback using our joint features is quantitatively
consistent with established domain knowledge. This work is expected to offer
new insight into golf swing analysis for both academia and the sports industry.

</details>


### [190] [Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study](https://arxiv.org/abs/2508.20188)
*Max Torop,Masih Eskandar,Nicholas Kurtansky,Jinyang Liu,Jochen Weber,Octavia Camps,Veronica Rotemberg,Jennifer Dy,Kivanc Kose*

Main category: cs.CV

TL;DR: 探讨多模态大语言模型（MLLMs）与定量属性结合提升皮肤病诊断模型可解释性，并通过案例研究评估MLLM嵌入空间与属性的关联。


<details>
  <summary>Details</summary>
Motivation: 人工智能模型在皮肤病诊断有潜力，但模型预测的可解释性需显著提升才能实际应用。

Method: 探索MLLMs和定量属性使用的结合，对MLLM进行微调使其从图像预测属性值，用SLICE - 3D数据集进行基于属性内容的图像检索案例研究。

Result: 论文表明可通过微调让MLLM嵌入空间与定量属性建立关联。

Conclusion: 结合MLLMs和定量属性有望提升皮肤病诊断模型的可解释性。

Abstract: Artificial Intelligence models have demonstrated significant success in
diagnosing skin diseases, including cancer, showing the potential to assist
clinicians in their analysis. However, the interpretability of model
predictions must be significantly improved before they can be used in practice.
To this end, we explore the combination of two promising approaches: Multimodal
Large Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a
potential avenue for increased interpretability, providing reasoning for
diagnosis in natural language through an interactive format. Separately, a
number of quantitative attributes that are related to lesion appearance (e.g.,
lesion area) have recently been found predictive of malignancy with high
accuracy. Predictions grounded as a function of such concepts have the
potential for improved interpretability. We provide evidence that MLLM
embedding spaces can be grounded in such attributes, through fine-tuning to
predict their values from images. Concretely, we evaluate this grounding in the
embedding space through an attribute-specific content-based image retrieval
case study using the SLICE-3D dataset.

</details>


### [191] [Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification](https://arxiv.org/abs/2508.20243)
*Mutahar Safdar,Gentry Wood,Max Zimmermann,Guy Lamouche,Priti Wanjara,Yaoyao Fiona Zhao*

Main category: cs.CV

TL;DR: 本文提出一种将微观结构信息学与专家表征知识相结合的新框架，可实现零样本分类，在增材制造金属基复合材料数据集上验证有效，提升了鉴定流程的可追溯性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 先进材料的快速可靠鉴定是工业制造瓶颈，特别是非传统增材制造工艺产生的异质结构。

Method: 引入定制和混合视觉 - 语言表征（VLRs）框架，集成深度语义分割与预训练多模态模型（CLIP和FLAVA），开发定制的基于相似度的表征，使用Z分数归一化。

Result: 在增材制造金属基复合材料数据集上验证了框架区分可接受和有缺陷样本的能力，比较分析显示FLAVA视觉敏感性高，CLIP与文本标准一致性好。

Conclusion: 该方法提升了鉴定流程可追溯性和可解释性，促进原始数据与专家知识语义互操作性，有助于工程信息学中可扩展和领域自适应的鉴定策略。

Abstract: Rapid and reliable qualification of advanced materials remains a bottleneck
in industrial manufacturing, particularly for heterogeneous structures produced
via non-conventional additive manufacturing processes. This study introduces a
novel framework that links microstructure informatics with a range of expert
characterization knowledge using customized and hybrid vision-language
representations (VLRs). By integrating deep semantic segmentation with
pre-trained multi-modal models (CLIP and FLAVA), we encode both visual
microstructural data and textual expert assessments into shared
representations. To overcome limitations in general-purpose embeddings, we
develop a customized similarity-based representation that incorporates both
positive and negative references from expert-annotated images and their
associated textual descriptions. This allows zero-shot classification of
previously unseen microstructures through a net similarity scoring approach.
Validation on an additively manufactured metal matrix composite dataset
demonstrates the framework's ability to distinguish between acceptable and
defective samples across a range of characterization criteria. Comparative
analysis reveals that FLAVA model offers higher visual sensitivity, while the
CLIP model provides consistent alignment with the textual criteria. Z-score
normalization adjusts raw unimodal and cross-modal similarity scores based on
their local dataset-driven distributions, enabling more effective alignment and
classification in the hybrid vision-language framework. The proposed method
enhances traceability and interpretability in qualification pipelines by
enabling human-in-the-loop decision-making without task-specific model
retraining. By advancing semantic interoperability between raw data and expert
knowledge, this work contributes toward scalable and domain-adaptable
qualification strategies in engineering informatics.

</details>


### [192] [Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation](https://arxiv.org/abs/2508.20265)
*Zhixiang Chi,Yanan Wu,Li Gu,Huan Liu,Ziqiang Wang,Yang Zhang,Yang Wang,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: 提出无训练、反馈驱动自适应框架，增强内部表征与最终预测语义一致性，提升多个基准测试性能。


<details>
  <summary>Details</summary>
Motivation: CLIP在开放词汇分割中因定位不佳存在问题，现有方法在传播空间一致性到最终输出时存在不足，且中间注意力与文本表征缺乏直接交互。

Method: 提出无训练、反馈驱动自适应框架，将基于输出的补丁级对应关系反馈到中间注意力，设计关键模块，作为插件集成到多种方法和骨干网络。

Result: 该方法集成到四种先进方法和三种骨干网络中，在多种注意力类型上验证，在八个基准测试中持续提升性能。

Conclusion: 所提框架能有效增强语义一致性，提升相关模型在开放词汇分割任务中的性能。

Abstract: CLIP exhibits strong visual-textual alignment but struggle with
open-vocabulary segmentation due to poor localization. Prior methods enhance
spatial coherence by modifying intermediate attention. But, this coherence
isn't consistently propagated to the final output due to subsequent operations
such as projections. Additionally, intermediate attention lacks direct
interaction with text representations, such semantic discrepancy limits the
full potential of CLIP.
  In this work, we propose a training-free, feedback-driven self-adaptive
framework that adapts output-based patch-level correspondences back to the
intermediate attention. The output predictions, being the culmination of the
model's processing, encapsulate the most comprehensive visual and textual
semantics about each patch. Our approach enhances semantic consistency between
internal representations and final predictions by leveraging the model's
outputs as a stronger spatial coherence prior. We design key modules, including
attention isolation, confidence-based pruning for sparse adaptation, and
adaptation ensemble, to effectively feedback the output coherence cues. Our
method functions as a plug-in module, seamlessly integrating into four
state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We
further validate our framework across multiple attention types (Q-K, self-self,
and Proxy augmented with MAE, SAM, and DINO). Our approach consistently
improves their performance across eight benchmarks.

</details>


### [193] [Towards Mechanistic Defenses Against Typographic Attacks in CLIP](https://arxiv.org/abs/2508.20570)
*Lorenz Hufe,Constantin Venhoff,Maximilian Dreyer,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.CV

TL;DR: 分析CLIP视觉编码器在排版攻击下的行为，提出选择性消融排版电路的防御方法，提升模型抗攻击能力并发布更鲁棒的dyslexic CLIP模型。


<details>
  <summary>Details</summary>
Motivation: 排版攻击会利用多模态系统造成危害，需分析CLIP视觉编码器在排版攻击下的表现并进行防御。

Method: 分析CLIP模型定位提取和传输排版信息的注意力头，选择性消融由注意力头组成的排版电路。

Result: 在ImageNet - 100排版变体上性能提升达19.6%，标准ImageNet - 100准确率降低不到1%，训练-free方法与依赖微调的现有先进排版防御方法有竞争力。

Conclusion: 发布的dyslexic CLIP模型对排版攻击更鲁棒，可用于对文本操纵风险敏感的安全关键应用。

Abstract: Typographic attacks exploit multi-modal systems by injecting text into
images, leading to targeted misclassifications, malicious content generation
and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP
vision encoders behave under typographic attacks, locating specialized
attention heads in the latter half of the model's layers that causally extract
and transmit typographic information to the cls token. Building on these
insights, we introduce a method to defend CLIP models against typographic
attacks by selectively ablating a typographic circuit, consisting of attention
heads. Without requiring finetuning, our method improves performance by up to
19.6% on a typographic variant of ImageNet-100, while reducing standard
ImageNet-100 accuracy by less than 1%. Notably, our training-free approach
remains competitive with current state-of-the-art typographic defenses that
rely on finetuning. To this end, we release a family of dyslexic CLIP models
which are significantly more robust against typographic attacks. These models
serve as suitable drop-in replacements for a broad range of safety-critical
applications, where the risks of text-based manipulation outweigh the utility
of text recognition.

</details>


### [194] [ArtFace: Towards Historical Portrait Face Identification via Model Adaptation](https://arxiv.org/abs/2508.20626)
*Francois Poh,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: 文章探讨用基础模型提升艺术作品面部识别效果，通过微调基础模型并与传统网络嵌入结合，取得比现有方法更好的结果。


<details>
  <summary>Details</summary>
Motivation: 识别历史画作中的人物对艺术史学家很重要，但传统面部识别模型在处理画作时因领域转移和高类内变异而表现不佳，且艺术因素使识别更复杂，因此研究基础模型提升面部识别的潜力。

Method: 微调基础模型，并将其嵌入与传统面部识别网络的嵌入相整合。

Result: 相比当前最先进的方法有显著改进，基础模型能弥补传统方法无效的差距。

Conclusion: 基础模型可有效提升艺术作品中的面部识别效果。

Abstract: Identifying sitters in historical paintings is a key task for art historians,
offering insight into their lives and how they chose to be seen. However, the
process is often subjective and limited by the lack of data and stylistic
variations. Automated facial recognition is capable of handling challenging
conditions and can assist, but while traditional facial recognition models
perform well on photographs, they struggle with paintings due to domain shift
and high intra-class variation. Artistic factors such as style, skill, intent,
and influence from other works further complicate recognition. In this work, we
investigate the potential of foundation models to improve facial recognition in
artworks. By fine-tuning foundation models and integrating their embeddings
with those from conventional facial recognition networks, we demonstrate
notable improvements over current state-of-the-art methods. Our results show
that foundation models can bridge the gap where traditional methods are
ineffective. Paper page at https://www.idiap.ch/paper/artface/

</details>


### [195] [Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization](https://arxiv.org/abs/2508.20475)
*Marina Grifell i Plana,Vladyslav Zalevskyi,Léa Schmidt,Yvan Gomez,Thomas Sanchez,Vincent Dunet,Mériam Koob,Vanessa Siffredi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 提出病理信息域随机化策略解决CCD数据稀缺问题，在多组数据上验证，减少LCC估计误差，提升分割拓扑一致性。


<details>
  <summary>Details</summary>
Motivation: CCD病例稀少导致标注数据有限，阻碍深度学习模型泛化，需要准确分割胎儿大脑以提取生物标志物和评估神经发育。

Method: 提出病理信息域随机化策略，将CCD表现的先验知识嵌入合成数据生成管道，从健康数据模拟多样大脑变化。

Result: 在多组胎儿数据上验证，减少LCC估计误差，提升分割拓扑一致性，能区分CCD亚型。

Conclusion: 将特定领域解剖先验知识融入合成数据管道可有效缓解数据稀缺，增强对罕见但临床重要畸形的分析。

Abstract: Accurate fetal brain segmentation is crucial for extracting biomarkers and
assessing neurodevelopment, especially in conditions such as corpus callosum
dysgenesis (CCD), which can induce drastic anatomical changes. However, the
rarity of CCD severely limits annotated data, hindering the generalization of
deep learning models. To address this, we propose a pathology-informed domain
randomization strategy that embeds prior knowledge of CCD manifestations into a
synthetic data generation pipeline. By simulating diverse brain alterations
from healthy data alone, our approach enables robust segmentation without
requiring pathological annotations.
  We validate our method on a cohort comprising 248 healthy fetuses, 26 with
CCD, and 47 with other brain pathologies, achieving substantial improvements on
CCD cases while maintaining performance on both healthy fetuses and those with
other pathologies. From the predicted segmentations, we derive clinically
relevant biomarkers, such as corpus callosum length (LCC) and volume, and show
their utility in distinguishing CCD subtypes. Our pathology-informed
augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in
healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these
quantitative gains, our approach yields segmentations with improved topological
consistency relative to available ground truth, enabling more reliable
shape-based analyses. Overall, this work demonstrates that incorporating
domain-specific anatomical priors into synthetic data pipelines can effectively
mitigate data scarcity and enhance analysis of rare but clinically significant
malformations.

</details>


### [196] [MobileCLIP2: Improving Multi-Modal Reinforced Training](https://arxiv.org/abs/2508.20691)
*Fartash Faghri,Pavan Kumar Anasosalu Vasu,Cem Koc,Vaishaal Shankar,Alexander Toshev,Oncel Tuzel,Hadi Pouransari*

Main category: cs.CV

TL;DR: 本文改进MobileCLIP的多模态强化训练，训练出MobileCLIP2，在低延迟下实现ImageNet - 1k零样本准确率的提升，并发布预训练模型和数据生成代码。


<details>
  <summary>Details</summary>
Motivation: 改进MobileCLIP的多模态强化训练，以提升其在低延迟下的零样本准确率。

Method: 通过在DFN数据集上训练更好的CLIP教师集成、在DFN数据集上训练并在高质量图像 - 字幕数据集上微调字幕生成器教师来改进多模态强化训练。

Result: 训练出MobileCLIP2，MobileCLIP2 - B比MobileCLIP - B在ImageNet - 1k准确率上提升2.2%；MobileCLIP2 - S4与SigLIP - SO400M/14零样本准确率相当但模型小2倍，比DFN ViT - L/14延迟低2.5倍。

Conclusion: 改进的多模态强化训练方法有效，能在低延迟下提升模型的零样本准确率，且发布的代码便于创建新的强化数据集。

Abstract: Foundation image-text models such as CLIP with zero-shot capabilities enable
a wide array of applications. MobileCLIP is a recent family of image-text
models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot
accuracy. The main ingredients in MobileCLIP were its low-latency and light
architectures and a novel multi-modal reinforced training that made knowledge
distillation from multiple caption-generators and CLIP teachers efficient,
scalable, and reproducible. In this paper, we improve the multi-modal
reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles
trained on the DFN dataset, 2) improved captioner teachers trained on the DFN
dataset and fine-tuned on a diverse selection of high-quality image-caption
datasets. We discover new insights through ablations such as the importance of
temperature tuning in contrastive knowledge distillation, the effectiveness of
caption-generator fine-tuning for caption diversity, and the additive
improvement from combining synthetic captions generated by multiple models. We
train a new family of models called MobileCLIP2 and achieve state-of-the-art
ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe
2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with
MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot
accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\times$ smaller and
improves on DFN ViT-L/14 at 2.5$\times$ lower latency. We release our
pretrained models (https://github.com/apple/ml-mobileclip) and the data
generation code (https://github.com/apple/ml-mobileclip-dr). The data
generation code makes it easy to create new reinforced datasets with arbitrary
teachers using distributed scalable processing.

</details>


### [197] [${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting](https://arxiv.org/abs/2508.20754)
*Yuxi Hu,Jun Zhang,Kuangyi Chen,Zhe Zhang,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: 提出C³ - GS框架用于可泛化高斯溅射，在基准数据集上实现了最先进的渲染质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在为高斯预测编码判别性、多视图一致特征上存在不足，难以用稀疏视图构建准确几何。

Method: 提出C³ - GS框架，将三个轻量级模块集成到统一渲染管道，融入上下文感知、跨维度和跨尺度约束以增强特征学习。

Result: 在基准数据集上的大量实验验证了C³ - GS实现了最先进的渲染质量和泛化能力。

Conclusion: C³ - GS框架能有效提升特征学习，实现无需额外监督的逼真合成。

Abstract: Generalizable Gaussian Splatting aims to synthesize novel views for unseen
scenes without per-scene optimization. In particular, recent advancements
utilize feed-forward networks to predict per-pixel Gaussian parameters,
enabling high-quality synthesis from sparse input views. However, existing
approaches fall short in encoding discriminative, multi-view consistent
features for Gaussian predictions, which struggle to construct accurate
geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a
framework that enhances feature learning by incorporating context-aware,
cross-dimension, and cross-scale constraints. Our architecture integrates three
lightweight modules into a unified rendering pipeline, improving feature fusion
and enabling photorealistic synthesis without requiring additional supervision.
Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS
achieves state-of-the-art rendering quality and generalization ability. Code is
available at: https://github.com/YuhsiHu/C3-GS.

</details>


### [198] [SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding](https://arxiv.org/abs/2508.20758)
*Jiawen Lin,Shiran Bian,Yihang Zhu,Wenbin Tan,Yachao Zhang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: 提出SeqVLM零样本3D视觉定位框架，在ScanRefer和Nr3D基准测试中取得SOTA性能，推动3DVG泛化和实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有零样本3D视觉定位方法存在空间推理受限、上下文缺失或细节退化问题，而零样本3DVG对现实应用更有前景。

Method: 先通过3D语义分割网络生成3D实例提案并筛选，采用提案引导的多视图投影策略投影到真实场景图像序列，实现3D点云到图像转换时保留空间关系和上下文细节，还实施动态调度机制处理序列查询提示。

Result: 在ScanRefer和Nr3D基准测试中Acc@0.25得分分别达55.6%和53.2%，超之前零样本方法4.0%和5.2%。

Conclusion: 所提方法推动3DVG朝着更强泛化性和实际应用迈进。

Abstract: 3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using
natural language descriptions. Although supervised methods achieve higher
accuracy in constrained settings, zero-shot 3DVG holds greater promise for
real-world applications since eliminating scene-specific training requirements.
However, existing zero-shot methods face challenges of spatial-limited
reasoning due to reliance on single-view localization, and contextual omissions
or detail degradation. To address these issues, we propose SeqVLM, a novel
zero-shot 3DVG framework that leverages multi-view real-world scene images with
spatial information for target object reasoning. Specifically, SeqVLM first
generates 3D instance proposals via a 3D semantic segmentation network and
refines them through semantic filtering, retaining only semantic-relevant
candidates. A proposal-guided multi-view projection strategy then projects
these candidate proposals onto real scene image sequences, preserving spatial
relationships and contextual details in the conversion process of 3D point
cloud to images. Furthermore, to mitigate VLM computational overload, we
implement a dynamic scheduling mechanism that iteratively processes
sequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to
identify textually specified objects. Experiments on the ScanRefer and Nr3D
benchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores
of 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%,
respectively, which advance 3DVG toward greater generalization and real-world
applicability. The code is available at https://github.com/JiawLin/SeqVLM.

</details>


### [199] [Occlusion Robustness of CLIP for Military Vehicle Classification](https://arxiv.org/abs/2508.20760)
*Jan Erik van Woerden,Gertjan Burghouts,Lotte Nijskens,Alma M. Liezenga,Sabina van Rooij,Frank Ruis,Hugo J. Kuijf*

Main category: cs.CV

TL;DR: 研究CLIP变体在军事环境中对遮挡的鲁棒性，得出四点关键见解，强调训练中特定遮挡增强的重要性。


<details>
  <summary>Details</summary>
Motivation: CLIP虽可用于零样本分类，但在具有部分遮挡和低信噪比的军事环境中的鲁棒性研究不足。

Method: 用包含18类军事车辆的自定义数据集研究CLIP变体对遮挡的鲁棒性，用归一化曲线下面积（NAUC）进行评估。

Result: （1）基于Transformer的CLIP模型始终优于CNN；（2）细粒度、分散的遮挡比大的连续遮挡更降低性能；（3）线性探测模型在约35%遮挡时性能急剧下降；（4）微调模型骨干后，性能下降发生在超过60%遮挡时。

Conclusion: 强调训练时特定遮挡增强的重要性，以及在实际部署中需进一步探索补丁级敏感性和架构弹性。

Abstract: Vision-language models (VLMs) like CLIP enable zero-shot classification by
aligning images and text in a shared embedding space, offering advantages for
defense applications with scarce labeled data. However, CLIP's robustness in
challenging military environments, with partial occlusion and degraded
signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP
variants' robustness to occlusion using a custom dataset of 18 military vehicle
classes and evaluate using Normalized Area Under the Curve (NAUC) across
occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP
models consistently outperform CNNs, (2) fine-grained, dispersed occlusions
degrade performance more than larger contiguous occlusions, (3) despite
improved accuracy, performance of linear-probed models sharply drops at around
35% occlusion, (4) by finetuning the model's backbone, this performance drop
occurs at more than 60% occlusion. These results underscore the importance of
occlusion-specific augmentations during training and the need for further
exploration into patch-level sensitivity and architectural resilience for
real-world deployment of CLIP.

</details>


### [200] [SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer](https://arxiv.org/abs/2508.20762)
*Fachri Najm Noer Kartiman,Rasim,Yaya Wihardi,Nurul Hasanah,Oskar Natan,Bambang Wahono,Taufik Ibnu Salim*

Main category: cs.CV

TL;DR: 研究提出SKGE - Swin架构用于端到端自动驾驶车辆模型，经实验该架构驾驶得分优于先前方法，还将进行消融研究。


<details>
  <summary>Details</summary>
Motivation: 开发具有像素到像素上下文感知的端到端自动驾驶车辆模型。

Method: 提出SKGE - Swin架构，利用带跳跃阶段机制的Swin Transformer进行特征提取，在CARLA平台使用对抗场景评估模型。

Result: SKGE - Swin架构在驾驶得分上优于先前方法。

Conclusion: SKGE - Swin架构在构建具有上下文感知的自动驾驶车辆模型方面有优势，后续将通过消融研究确定各组件对性能的贡献。

Abstract: Focusing on the development of an end-to-end autonomous vehicle model with
pixel-to-pixel context awareness, this research proposes the SKGE-Swin
architecture. This architecture utilizes the Swin Transformer with a skip-stage
mechanism to broaden feature representation globally and at various network
levels. This approach enables the model to extract information from distant
pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head
Self-Attention (SW-MSA) mechanism and to retain critical information from the
initial to the final stages of feature extraction, thereby enhancing its
capability to comprehend complex patterns in the vehicle's surroundings. The
model is evaluated on the CARLA platform using adversarial scenarios to
simulate real-world conditions. Experimental results demonstrate that the
SKGE-Swin architecture achieves a superior Driving Score compared to previous
methods. Furthermore, an ablation study will be conducted to evaluate the
contribution of each architectural component, including the influence of skip
connections and the use of the Swin Transformer, in improving model
performance.

</details>


### [201] [Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding](https://arxiv.org/abs/2508.20765)
*Gowreesh Mago,Pascal Mettes,Stevan Rudinac*

Main category: cs.CV

TL;DR: 文章指出视频内容自动理解快速发展，但抽象概念识别是挑战，认为基础模型可应对此问题，还研究相关任务和数据集，强调借鉴经验的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有视频自动理解在理解抽象概念上存在不足，而理解抽象概念能使模型更符合人类推理和价值观，因此有必要研究。

Method: 研究不同用于理解视频内容中抽象概念的任务和数据集，借鉴过去社区经验。

Result: 发现研究人员长期周期性利用可用工具尝试解决相关任务。

Conclusion: 借鉴社区数十年经验有助于解决抽象概念理解这一重要挑战，避免重复劳动。

Abstract: The automatic understanding of video content is advancing rapidly. Empowered
by deeper neural networks and large datasets, machines are increasingly capable
of understanding what is concretely visible in video frames, whether it be
objects, actions, events, or scenes. In comparison, humans retain a unique
ability to also look beyond concrete entities and recognize abstract concepts
like justice, freedom, and togetherness. Abstract concept recognition forms a
crucial open challenge in video understanding, where reasoning on multiple
semantic levels based on contextual information is key. In this paper, we argue
that the recent advances in foundation models make for an ideal setting to
address abstract concept understanding in videos. Automated understanding of
high-level abstract concepts is imperative as it enables models to be more
aligned with human reasoning and values. In this survey, we study different
tasks and datasets used to understand abstract concepts in video content. We
observe that, periodically and over a long period, researchers have attempted
to solve these tasks, making the best use of the tools available at their
disposal. We advocate that drawing on decades of community experience will help
us shed light on this important open grand challenge and avoid ``re-inventing
the wheel'' as we start revisiting it in the era of multi-modal foundation
models.

</details>


### [202] [Safer Skin Lesion Classification with Global Class Activation Probability Map Evaluation and SafeML](https://arxiv.org/abs/2508.20776)
*Kuniko Paxton,Koorosh Aslansefat,Amila Akagić,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 现有皮肤病变分类模型虽精度提升，但存在对AI不信任问题，本文提出GCAPME方法并应用SafeML，用ISIC数据集评估。


<details>
  <summary>Details</summary>
Motivation: 现有皮肤病变分类模型虽精度高，但存在对AI不信任问题，且现有可解释性方法有可靠性问题。

Method: 提出Global Class Activation Probabilistic Map Evaluation方法，对所有类别的激活概率图进行像素级概率分析，统一可视化诊断过程；应用SafeML增强对错误诊断的检测。

Result: 未明确提及具体结果，仅表明用ISIC数据集结合MobileNetV2和Vision Transformers进行评估。

Conclusion: 该方法有助于降低误诊风险，应用SafeML可提高诊断可靠性和患者安全。

Abstract: Recent advancements in skin lesion classification models have significantly
improved accuracy, with some models even surpassing dermatologists' diagnostic
performance. However, in medical practice, distrust in AI models remains a
challenge. Beyond high accuracy, trustworthy, explainable diagnoses are
essential. Existing explainability methods have reliability issues, with
LIME-based methods suffering from inconsistency, while CAM-based methods
failing to consider all classes. To address these limitations, we propose
Global Class Activation Probabilistic Map Evaluation, a method that analyses
all classes' activation probability maps probabilistically and at a pixel
level. By visualizing the diagnostic process in a unified manner, it helps
reduce the risk of misdiagnosis. Furthermore, the application of SafeML
enhances the detection of false diagnoses and issues warnings to doctors and
patients as needed, improving diagnostic reliability and ultimately patient
safety. We evaluated our method using the ISIC datasets with MobileNetV2 and
Vision Transformers.

</details>


### [203] [Evaluating Compositional Generalisation in VLMs and Diffusion Models](https://arxiv.org/abs/2508.20783)
*Beth Pearson,Bilal Boulbarss,Michael Wray,Martha Lewis*

Main category: cs.CV

TL;DR: 本文探讨生成式扩散分类器在组合泛化能力上是否优于判别式模型，评估三个模型在零样本和广义零样本学习设置下的概念绑定能力，发现扩散分类器和ViLT表现较好，但所有模型在关系广义零样本任务中都面临挑战。


<details>
  <summary>Details</summary>
Motivation: 自然语言语义可由已知部分组合形成新含义，但现有视觉 - 语言模型（VLMs）难以执行这种组合，探究生成式扩散分类器是否在组合泛化能力上有提升。

Method: 评估扩散分类器、CLIP和ViLT三个模型在零样本学习（ZSL）和广义零样本学习（GZSL）设置下绑定对象与属性及关系的能力。

Result: 扩散分类器和ViLT在概念绑定任务中表现良好，但所有模型在关系GZSL任务中表现不佳。

Conclusion: VLMs在关系推理方面面临更广泛的挑战，CLIP嵌入分析表明困难可能源于关系概念表示过于相似。

Abstract: A fundamental aspect of the semantics of natural language is that novel
meanings can be formed from the composition of previously known parts.
Vision-language models (VLMs) have made significant progress in recent years,
however, there is evidence that they are unable to perform this kind of
composition. For example, given an image of a red cube and a blue cylinder, a
VLM such as CLIP is likely to incorrectly label the image as a red cylinder or
a blue cube, indicating it represents the image as a `bag-of-words' and fails
to capture compositional semantics. Diffusion models have recently gained
significant attention for their impressive generative abilities, and zero-shot
classifiers based on diffusion models have been shown to perform competitively
with CLIP in certain compositional tasks. In this work we explore whether the
generative Diffusion Classifier has improved compositional generalisation
abilities compared to discriminative models. We assess three models --
Diffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with
attributes and relations in both zero-shot learning (ZSL) and generalised
zero-shot learning (GZSL) settings. Our results show that the Diffusion
Classifier and ViLT perform well at concept binding tasks, but that all models
struggle significantly with the relational GZSL task, underscoring the broader
challenges VLMs face with relational reasoning. Analysis of CLIP embeddings
suggests that the difficulty may stem from overly similar representations of
relational concepts such as left and right. Code and dataset are available at:
https://github.com/otmive/diffusion_classifier_clip

</details>


### [204] [Surfel-based 3D Registration with Equivariant SE(3) Features](https://arxiv.org/abs/2508.20789)
*Xueyang Kang,Hang Zhao,Kourosh Khoshelham,Patrick Vandewalle*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Point cloud registration is crucial for ensuring 3D alignment consistency of
multiple local point clouds in 3D reconstruction for remote sensing or digital
heritage. While various point cloud-based registration methods exist, both
non-learning and learning-based, they ignore point orientations and point
uncertainties, making the model susceptible to noisy input and aggressive
rotations of the input point cloud like orthogonal transformation; thus, it
necessitates extensive training point clouds with transformation augmentations.
To address these issues, we propose a novel surfel-based pose learning
regression approach. Our method can initialize surfels from Lidar point cloud
using virtual perspective camera parameters, and learns explicit
$\mathbf{SE(3)}$ equivariant features, including both position and rotation
through $\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative
transformation between source and target scans. The model comprises an
equivariant convolutional encoder, a cross-attention mechanism for similarity
computation, a fully-connected decoder, and a non-linear Huber loss.
Experimental results on indoor and outdoor datasets demonstrate our model
superiority and robust performance on real point-cloud scans compared to
state-of-the-art methods.

</details>


### [205] [ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering](https://arxiv.org/abs/2508.21010)
*Paritosh Parmar,Eric Peh,Basura Fernando*

Main category: cs.CV

TL;DR: 提出新颖模块化框架处理因果-为什么视频问答，解耦因果推理与答案生成，在多个基准测试表现好且提升可解释性等。


<details>
  <summary>Details</summary>
Motivation: 现有因果-为什么视频问答模型在高阶推理存在困难，黑盒方法可解释性有限且依赖浅层启发式。

Method: 提出模块化框架，解耦因果推理与答案生成，引入自然语言因果链；架构包含CCE和CCDA；用大语言模型从现有数据集生成高质量因果链；提出新评估指标CauCo。

Result: 在三个大规模基准测试中，该方法超越现有模型，在可解释性、用户信任和泛化性上有显著提升。

Conclusion: 该方法有效，CCE可作为跨领域可复用的因果推理引擎。

Abstract: Existing Causal-Why Video Question Answering (VideoQA) models often struggle
with higher-order reasoning, relying on opaque, monolithic pipelines that
entangle video understanding, causal inference, and answer generation. These
black-box approaches offer limited interpretability and tend to depend on
shallow heuristics. We propose a novel, modular framework that explicitly
decouples causal reasoning from answer generation, introducing natural language
causal chains as interpretable intermediate representations. Inspired by human
cognitive models, these structured cause-effect sequences bridge low-level
video content with high-level causal reasoning, enabling transparent and
logically coherent inference. Our two-stage architecture comprises a Causal
Chain Extractor (CCE) that generates causal chains from video-question pairs,
and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in
these chains. To address the lack of annotated reasoning traces, we introduce a
scalable method for generating high-quality causal chains from existing
datasets using large language models. We also propose CauCo, a new evaluation
metric for causality-oriented captioning. Experiments on three large-scale
benchmarks demonstrate that our approach not only outperforms state-of-the-art
models, but also yields substantial gains in explainability, user trust, and
generalization -- positioning the CCE as a reusable causal reasoning engine
across diverse domains. Project page:
https://paritoshparmar.github.io/chainreaction/

</details>


### [206] [FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator](https://arxiv.org/abs/2508.21040)
*Huynh Tong Dang Khoa,Dang Hoai Nam,Vo Nguyen Le Duy*

Main category: cs.CV

TL;DR: 提出FW - GAN解决手写合成现有方法局限，在数据集验证其效果好。


<details>
  <summary>Details</summary>
Motivation: 有标签手写数据稀缺，现有手写合成方法在建模和利用频率信息上有局限。

Method: 提出FW - GAN，生成器集成Wave - MLP，引入频率引导判别器和频率分布损失。

Result: 在越南语和英语手写数据集实验表明，FW - GAN能生成高质量、风格一致的手写内容。

Conclusion: FW - GAN是低资源手写识别管道中数据增强的有价值工具。

Abstract: Labeled handwriting data is often scarce, limiting the effectiveness of
recognition systems that require diverse, style-consistent training samples.
Handwriting synthesis offers a promising solution by generating artificial data
to augment training. However, current methods face two major limitations.
First, most are built on conventional convolutional architectures, which
struggle to model long-range dependencies and complex stroke patterns. Second,
they largely ignore the crucial role of frequency information, which is
essential for capturing fine-grained stylistic and structural details in
handwriting. To address these challenges, we propose FW-GAN, a one-shot
handwriting synthesis framework that generates realistic, writer-consistent
text from a single example. Our generator integrates a phase-aware Wave-MLP to
better capture spatial relationships while preserving subtle stylistic cues. We
further introduce a frequency-guided discriminator that leverages
high-frequency components to enhance the authenticity detection of generated
samples. Additionally, we introduce a novel Frequency Distribution Loss that
aligns the frequency characteristics of synthetic and real handwriting, thereby
enhancing visual fidelity. Experiments on Vietnamese and English handwriting
datasets demonstrate that FW-GAN generates high-quality, style-consistent
handwriting, making it a valuable tool for augmenting data in low-resource
handwriting recognition (HTR) pipelines. Official implementation is available
at https://github.com/DAIR-Group/FW-GAN

</details>


### [207] [Dress&Dance: Dress up and Dance as You Like It - Technical Preview](https://arxiv.org/abs/2508.21070)
*Jun-Kun Chen,Aayush Bansal,Minh Phuoc Vo,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: 提出Dress&Dance视频扩散框架，可生成高质量虚拟试穿视频，优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 为用户提供生成高质量、高分辨率且动作符合参考视频的虚拟试穿视频的方法，支持多种服装类型试穿。

Method: 采用CondNet新型调节网络利用注意力机制统一多模态输入，在异构训练数据上以多阶段渐进方式训练。

Result: Dress&Dance生成的虚拟试穿视频质量高，优于现有开源和商业解决方案。

Conclusion: Dress&Dance能实现高质量、灵活的试穿体验。

Abstract: We present Dress&Dance, a video diffusion framework that generates high
quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a
user wearing desired garments while moving in accordance with a given reference
video. Our approach requires a single user image and supports a range of tops,
bottoms, and one-piece garments, as well as simultaneous tops and bottoms
try-on in a single pass. Key to our framework is CondNet, a novel conditioning
network that leverages attention to unify multi-modal inputs (text, images, and
videos), thereby enhancing garment registration and motion fidelity. CondNet is
trained on heterogeneous training data, combining limited video data and a
larger, more readily available image dataset, in a multistage progressive
manner. Dress&Dance outperforms existing open source and commercial solutions
and enables a high quality and flexible try-on experience.

</details>


### [208] [ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts](https://arxiv.org/abs/2508.20991)
*Patryk Będkowski,Jan Dubiński,Filip Szatkowski,Kamil Deja,Przemysław Rokita,Tomasz Trzciński*

Main category: cs.CV

TL;DR: 提出针对ALICE实验零度量热计的深度学习模拟方法ExpertSim，比传统方法更精确高效，代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统统计蒙特卡罗方法模拟探测器响应计算成本高，现有生成式机器学习方法难以捕捉数据分布变化。

Method: 采用Mixture - of - Generative - Experts架构，每个专家专注模拟不同子集数据。

Result: ExpertSim相比传统蒙特卡罗方法提高了准确性，显著加快了速度。

Conclusion: ExpertSim为CERN粒子物理实验的高效探测器模拟提供了有前景的解决方案。

Abstract: Simulating detector responses is a crucial part of understanding the inner
workings of particle collisions in the Large Hadron Collider at CERN. Such
simulations are currently performed with statistical Monte Carlo methods, which
are computationally expensive and put a significant strain on CERN's
computational grid. Therefore, recent proposals advocate for generative machine
learning methods to enable more efficient simulations. However, the
distribution of the data varies significantly across the simulations, which is
hard to capture with out-of-the-box methods. In this study, we present
ExpertSim - a deep learning simulation approach tailored for the Zero Degree
Calorimeter in the ALICE experiment. Our method utilizes a
Mixture-of-Generative-Experts architecture, where each expert specializes in
simulating a different subset of the data. This allows for a more precise and
efficient generation process, as each expert focuses on a specific aspect of
the calorimeter response. ExpertSim not only improves accuracy, but also
provides a significant speedup compared to the traditional Monte-Carlo methods,
offering a promising solution for high-efficiency detector simulations in
particle physics experiments at CERN. We make the code available at
https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.

</details>


### [209] [Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning](https://arxiv.org/abs/2508.21048)
*Hao Tan,Jun Lan,Zichang Tan,Ajian Liu,Chuanbiao Song,Senyuan Shi,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: 本文引入HydraFake数据集并提出基于多模态大语言模型的Veritas检测器，实现跨场景的深度伪造检测。


<details>
  <summary>Details</summary>
Motivation: 现有学术基准与工业实践差异大，阻碍当前检测器实际部署，需缩小差距。

Method: 引入HydraFake数据集进行分层泛化测试，提出Veritas检测器，采用模式感知推理和两阶段训练管道。

Result: 实验显示之前的检测器在未知伪造和数据领域表现不佳，Veritas在不同OOD场景中取得显著增益，能输出透明可信的检测结果。

Conclusion: Veritas能有效解决深度伪造检测问题，尤其在跨场景情况下表现出色。

Abstract: Deepfake detection remains a formidable challenge due to the complex and
evolving nature of fake content in real-world scenarios. However, existing
academic benchmarks suffer from severe discrepancies from industrial practice,
typically featuring homogeneous training sources and low-quality testing
images, which hinder the practical deployments of current detectors. To
mitigate this gap, we introduce HydraFake, a dataset that simulates real-world
challenges with hierarchical generalization testing. Specifically, HydraFake
involves diversified deepfake techniques and in-the-wild forgeries, along with
rigorous training and evaluation protocol, covering unseen model architectures,
emerging forgery techniques and novel data domains. Building on this resource,
we propose Veritas, a multi-modal large language model (MLLM) based deepfake
detector. Different from vanilla chain-of-thought (CoT), we introduce
pattern-aware reasoning that involves critical reasoning patterns such as
"planning" and "self-reflection" to emulate human forensic process. We further
propose a two-stage training pipeline to seamlessly internalize such deepfake
reasoning capacities into current MLLMs. Experiments on HydraFake dataset
reveal that although previous detectors show great generalization on
cross-model scenarios, they fall short on unseen forgeries and data domains.
Our Veritas achieves significant gains across different OOD scenarios, and is
capable of delivering transparent and faithful detection outputs.

</details>


### [210] [FakeParts: a New Family of AI-Generated DeepFakes](https://arxiv.org/abs/2508.21052)
*Gaetan Brison,Soobash Daiboo,Samy Aimeur,Awais Hussain Sani,Xi Wang,Gianni Franchi,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 介绍了新型深度伪造FakeParts及首个大规模基准数据集FakePartsBench，研究表明其降低检测准确率，需开发更鲁棒检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测能力存在针对部分伪造检测的空白，需有效检测方法。

Method: 创建含超25K视频及像素和帧级操作注释的FakePartsBench数据集。

Result: 用户研究显示FakeParts使人类检测准确率比传统深度伪造降低超30%，现有模型性能也下降。

Conclusion: 当前深度伪造检测方法有漏洞，需为部分视频操作开发更鲁棒方法。

Abstract: We introduce FakeParts, a new class of deepfakes characterized by subtle,
localized manipulations to specific spatial regions or temporal segments of
otherwise authentic videos. Unlike fully synthetic content, these partial
manipulations, ranging from altered facial expressions to object substitutions
and background modifications, blend seamlessly with real elements, making them
particularly deceptive and difficult to detect. To address the critical gap in
detection capabilities, we present FakePartsBench, the first large-scale
benchmark dataset specifically designed to capture the full spectrum of partial
deepfakes. Comprising over 25K videos with pixel-level and frame-level
manipulation annotations, our dataset enables comprehensive evaluation of
detection methods. Our user studies demonstrate that FakeParts reduces human
detection accuracy by over 30% compared to traditional deepfakes, with similar
performance degradation observed in state-of-the-art detection models. This
work identifies an urgent vulnerability in current deepfake detection
approaches and provides the necessary resources to develop more robust methods
for partial video manipulations.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [211] [Molecular Machine Learning in Chemical Process Design](https://arxiv.org/abs/2508.20527)
*Jan G. Rittig,Manuel Dahmen,Martin Grohe,Philippe Schwaller,Alexander Mitsos*

Main category: physics.chem-ph

TL;DR: 本文探讨化学过程工程领域分子机器学习，回顾模型、研究方向，考虑在过程尺度应用并提出创建基准和验证的建议。


<details>
  <summary>Details</summary>
Motivation: 分子机器学习在预测纯组分及混合物性质、探索新分子结构方面有巨大潜力，期望在化学过程工程领域进一步发展。

Method: 回顾当前先进的分子机器学习模型，讨论结合物理化学知识改进模型的方法，探讨将分子机器学习融入过程设计和优化公式。

Result: 提出结合物理化学知识可进一步推进图神经网络和变压器等ML方法，可将分子ML融入过程设计和优化。

Conclusion: 要创建分子和过程设计基准，并与化工行业合作实际验证提出的候选方案，以加速新分子和过程的识别。

Abstract: We present a perspective on molecular machine learning (ML) in the field of
chemical process engineering. Recently, molecular ML has demonstrated great
potential in (i) providing highly accurate predictions for properties of pure
components and their mixtures, and (ii) exploring the chemical space for new
molecular structures. We review current state-of-the-art molecular ML models
and discuss research directions that promise further advancements. This
includes ML methods, such as graph neural networks and transformers, which can
be further advanced through the incorporation of physicochemical knowledge in a
hybrid or physics-informed fashion. Then, we consider leveraging molecular ML
at the chemical process scale, which is highly desirable yet rather unexplored.
We discuss how molecular ML can be integrated into process design and
optimization formulations, promising to accelerate the identification of novel
molecules and processes. To this end, it will be essential to create molecule
and process design benchmarks and practically validate proposed candidates,
possibly in collaboration with the chemical industry.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [212] [Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20532)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Salvador Lima-López,Eulàlia Farré-Maduell,Martin Krallinger,Natalia Loukachevitch,Vera Davydova,Elena Tutubalina,Georgios Paliouras*

Main category: cs.CL

TL;DR: 本文概述2024年CLEF会议中BioASQ挑战赛第12版，介绍任务及参赛情况，表明领域技术不断进步。


<details>
  <summary>Details</summary>
Motivation: 推动大规模生物医学语义索引和问答领域的发展。

Method: 举办BioASQ挑战赛，设置任务吸引团队参赛。

Result: 37个团队参与，提交超700份不同的参赛作品，多数参赛系统表现出色。

Conclusion: 该领域技术在持续进步。

Abstract: This is an overview of the twelfth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks b and Synergy, and two
new tasks: a) MultiCardioNER on the adaptation of clinical entity detection to
the cardiology domain in a multilingual setting, and b) BIONNE on nested NER in
Russian and English. In this edition of BioASQ, 37 competing teams participated
with more than 700 distinct submissions in total for the four different shared
tasks of the challenge. Similarly to previous editions, most of the
participating systems achieved competitive performance, suggesting the
continuous advancement of the state-of-the-art in the field.

</details>


### [213] [Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20554)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Martin Krallinger,Miguel Rodríguez-Ortega,Eduard Rodriguez-López,Natalia Loukachevitch,Andrey Sakhovskiy,Elena Tutubalina,Dimitris Dimitriadis,Grigorios Tsoumakas,George Giannakoulas,Alexandra Bekiaridou,Athanasios Samaras,Giorgio Maria Di Nunzio,Nicola Ferro,Stefano Marchesin,Marco Martinelli,Gianmaria Silvello,Georgios Paliouras*

Main category: cs.CL

TL;DR: 介绍2025年CLEF会议中BioASQ挑战赛第13版概况，包括新增任务和参赛情况。


<details>
  <summary>Details</summary>
Motivation: 推动大规模生物医学语义索引和问答的发展。

Method: 举办挑战赛，设置既定任务和新增任务。

Result: 83个参赛团队针对六个共享任务提交超1000份不同提交，部分系统表现有竞争力。

Conclusion: 该领域的技术水平持续进步。

Abstract: This is an overview of the thirteenth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks, b and Synergy, and four
new tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task
BioNNE-L on nested named entity linking in Russian and English. c) Task
ELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain
interplay information extraction. In this edition of BioASQ, 83 competing teams
participated with more than 1000 distinct submissions in total for the six
different shared tasks of the challenge. Similar to previous editions, several
participating systems achieved competitive performance, indicating the
continuous advancement of the state-of-the-art in the field.

</details>


### [214] [Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search](https://arxiv.org/abs/2508.20559)
*Zeyu Xiong,Yixuan Nan,Li Gao,Hengzhu Tang,Shuaiqiang Wang,Junfeng Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 提出新框架将生成模型用于工业网络搜索实时QDTS，模型性能超基线且部署效率高。


<details>
  <summary>Details</summary>
Motivation: 传统提取式摘要模型在工业应用中有累积信息损失、架构瓶颈及语义理解不足等问题，需新方法解决实时QDTS。

Method: 集成大模型蒸馏、监督微调、直接偏好优化和前瞻解码，将轻量级模型转化为领域专家模型。

Result: 模型在多指标评估中超越生产基线，达到新的最优水平，且部署效率高。

Conclusion: 所提新框架能有效解决工业网络搜索实时QDTS问题，模型性能和部署效率良好。

Abstract: In the dynamic landscape of large-scale web search, Query-Driven Text
Summarization (QDTS) aims to generate concise and informative summaries from
textual documents based on a given query, which is essential for improving user
engagement and facilitating rapid decision-making. Traditional extractive
summarization models, based primarily on ranking candidate summary segments,
have been the dominant approach in industrial applications. However, these
approaches suffer from two key limitations: 1) The multi-stage pipeline often
introduces cumulative information loss and architectural bottlenecks due to its
weakest component; 2) Traditional models lack sufficient semantic understanding
of both user queries and documents, particularly when dealing with complex
search intents. In this study, we propose a novel framework to pioneer the
application of generative models to address real-time QDTS in industrial web
search. Our approach integrates large model distillation, supervised
fine-tuning, direct preference optimization, and lookahead decoding to
transform a lightweight model with only 0.1B parameters into a
domain-specialized QDTS expert. Evaluated on multiple industry-relevant
metrics, our model outperforms the production baseline and achieves a new state
of the art. Furthermore, it demonstrates excellent deployment efficiency,
requiring only 334 NVIDIA L20 GPUs to handle \textasciitilde50,000 queries per
second under 55~ms average latency per query.

</details>


### [215] [GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction](https://arxiv.org/abs/2508.20828)
*Jie Zhao,Wanting Ning,Yuxiao Fei,Yubo Feng,Lishuang Li*

Main category: cs.CL

TL;DR: 针对小语言模型和大语言模型在事件时间关系提取中的问题，提出基于大语言模型的全局距离感知建模方法GDLLM，在两个公开数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型预训练知识受限，处理不平衡分类数据中少数类关系能力不足；大语言模型手动设计提示或指令会引入额外噪声，干扰对事件长距离依赖的判断。

Method: 提出GDLLM，利用图注意力网络构建距离感知图结构辅助大语言模型捕捉长距离依赖特征，设计基于软推理的时间特征学习范式增强短距离接近带关系的识别，并将大语言模型生成的概率信息补充到多头注意力机制中。

Result: 在TB - Dense和MATRES两个公开数据集上实验，达到了当前最优性能。

Conclusion: 该框架能有效捕捉全局特征，显著提升少数关系类的性能和整体学习能力。

Abstract: In Natural Language Processing(NLP), Event Temporal Relation Extraction
(ETRE) is to recognize the temporal relations of two events. Prior studies have
noted the importance of language models for ETRE. However, the restricted
pre-trained knowledge of Small Language Models(SLMs) limits their capability to
handle minority class relations in imbalanced classification datasets. For
Large Language Models(LLMs), researchers adopt manually designed prompts or
instructions, which may introduce extra noise, leading to interference with the
model's judgment of the long-distance dependencies between events. To address
these issues, we propose GDLLM, a Global Distance-aware modeling approach based
on LLMs. We first present a distance-aware graph structure utilizing Graph
Attention Network(GAT) to assist the LLMs in capturing long-distance dependency
features. Additionally, we design a temporal feature learning paradigm based on
soft inference to augment the identification of relations with a short-distance
proximity band, which supplements the probabilistic information generated by
LLMs into the multi-head attention mechanism. Since the global feature can be
captured effectively, our framework substantially enhances the performance of
minority relation classes and improves the overall learning ability.
Experiments on two publicly available datasets, TB-Dense and MATRES,
demonstrate that our approach achieves state-of-the-art (SOTA) performance.

</details>


### [216] [An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs](https://arxiv.org/abs/2508.21024)
*Mathieu Bourdin,Anas Neumann,Thomas Paviot,Robert Pellerin,Samir Lamouri*

Main category: cs.CL

TL;DR: 本文提出EASI - RAG方法助力工业中小企业部署RAG系统，并通过案例验证其效果，强调RAG在工业中小企业的应用潜力及后续工作方向。


<details>
  <summary>Details</summary>
Motivation: RAG可缓解大语言模型局限性，但工业中小企业因资源和专业知识有限，部署RAG工具存在挑战。

Method: 提出基于方法工程原则的EASI - RAG方法，包含明确的角色、活动和技术。

Result: 通过环境测试实验室案例验证，EASI - RAG支持快速实施、高用户采纳率，能提供准确答案，增强底层数据可靠性。

Conclusion: 突出RAG在工业中小企业的部署潜力，未来需在不同用例中推广，并与微调模型进一步集成。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to
mitigate the limitations of Large Language Models (LLMs), such as
hallucinations and outdated knowledge. However, deploying RAG-based tools in
Small and Medium Enterprises (SMEs) remains a challenge due to their limited
resources and lack of expertise in natural language processing (NLP). This
paper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a
structured, agile method designed to facilitate the deployment of RAG systems
in industrial SME contexts. EASI-RAG is based on method engineering principles
and comprises well-defined roles, activities, and techniques. The method was
validated through a real-world case study in an environmental testing
laboratory, where a RAG tool was implemented to answer operators queries using
data extracted from operational procedures. The system was deployed in under a
month by a team with no prior RAG experience and was later iteratively improved
based on user feedback. Results demonstrate that EASI-RAG supports fast
implementation, high user adoption, delivers accurate answers, and enhances the
reliability of underlying data. This work highlights the potential of RAG
deployment in industrial SMEs. Future works include the need for generalization
across diverse use cases and further integration with fine-tuned models.

</details>


### [217] [Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models](https://arxiv.org/abs/2508.20217)
*Mohammad Amini,Babak Ahmadi,Xiaomeng Xiong,Yilin Zhang,Christopher Qiao*

Main category: cs.CL

TL;DR: 研究探索用语言模型自动生成形态评估选择题，对比模型、评估提示策略，结果表明结构化提示可提升中型模型表现，提出实用可扩展的语言评估项目开发流程。


<details>
  <summary>Details</summary>
Motivation: 降低人工测试开发的成本和不一致性。

Method: 对比微调的中型模型Gemma和未调优的大型模型GPT - 3.5；评估七种结构化提示策略；用自动指标和专家评分评估生成项目；用GPT - 4.1模拟大规模人工评分。

Result: 结构化提示，尤其是结合思维链和顺序设计的策略显著改善Gemma的输出；Gemma比GPT - 3.5零样本响应生成的项目更符合结构和教学要求，提示设计对中型模型性能关键。

Conclusion: 结构化提示和有效微调可在有限数据下提升中型模型用于自动生成；结合自动指标、专家判断和大模型模拟可确保与评估目标一致，提出的工作流实用可扩展。

Abstract: This study explores automatic generation (AIG) using language models to
create multiple choice questions (MCQs) for morphological assessment, aiming to
reduce the cost and inconsistency of manual test development. The study used a
two-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B)
with a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven
structured prompting strategies, including zero-shot, few-shot,
chain-of-thought, role-based, sequential, and combinations. Generated items
were assessed using automated metrics and expert scoring across five
dimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate
human scoring at scale. Results show that structured prompting, especially
strategies combining chain-of-thought and sequential design, significantly
improved Gemma's outputs. Gemma generally produced more construct-aligned and
instructionally appropriate items than GPT-3.5's zero-shot responses, with
prompt design playing a key role in mid-size model performance. This study
demonstrates that structured prompting and efficient fine-tuning can enhance
midsized models for AIG under limited data conditions. We highlight the value
of combining automated metrics, expert judgment, and large-model simulation to
ensure alignment with assessment goals. The proposed workflow offers a
practical and scalable way to develop and validate language assessment items
for K-12.

</details>


### [218] [GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs](https://arxiv.org/abs/2508.20325)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Andy Zhou,Yang Zhang,Haohan Wang*

Main category: cs.CL

TL;DR: 介绍GUARD测试方法，可将道德准则转化为具体问题测试大语言模型合规性，在多个模型验证有效且可用于视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型有害响应引发社会和监管担忧，现有道德准则缺乏可操作的测试问题来验证模型合规性。

Method: 引入GUARD方法，基于政府准则自动生成违规问题测试模型响应，对未直接违规的响应使用GUARD - JD进行越狱诊断，最终生成合规报告。

Result: 在七个大语言模型上验证了GUARD的有效性，且GUARD - JD可将越狱诊断应用于视觉语言模型。

Conclusion: GUARD方法能有效促进基于大语言模型的可靠应用。

Abstract: As Large Language Models become increasingly integral to various domains,
their potential to generate harmful responses has prompted significant societal
and regulatory concerns. In response, governments have issued ethics guidelines
to promote the development of trustworthy AI. However, these guidelines are
typically high-level demands for developers and testers, leaving a gap in
translating them into actionable testing questions to verify LLM compliance.
  To address this challenge, we introduce GUARD (\textbf{G}uideline
\textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and
Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize
guidelines into specific guideline-violating questions that assess LLM
adherence. To implement this, GUARD uses automated generation of
guideline-violating questions based on government-issued guidelines, thereby
testing whether responses comply with these guidelines. When responses directly
violate guidelines, GUARD reports inconsistencies. Furthermore, for responses
that do not directly violate guidelines, GUARD integrates the concept of
``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that
provoke unethical or guideline-violating responses, effectively identifying
potential scenarios that could bypass built-in safety mechanisms. Our method
finally culminates in a compliance report, delineating the extent of adherence
and highlighting any violations.
  We have empirically validated the effectiveness of GUARD on seven LLMs,
including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4,
GPT-4o, and Claude-3.7, by testing compliance under three government-issued
guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can
transfer jailbreak diagnostics to vision-language models, demonstrating its
usage in promoting reliable LLM-based applications.

</details>


### [219] [Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems](https://arxiv.org/abs/2508.20373)
*Yuyao Wang,Bowen Liu,Jianheng Tang,Nuo Chen,Yuhan Li,Qifan Zhang,Jia Li*

Main category: cs.CL

TL;DR: 本文引入NP难图问题作为新的合成训练语料，提出两阶段后训练框架提升大语言模型长思维链推理能力，模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有长思维链能力发展依赖高成本人工整理数据集，需探索可扩展替代方案。

Method: 提出两阶段后训练框架，包括在拒绝采样的NP难图实例上进行长思维链监督微调，以及使用细粒度奖励设计的强化学习。

Result: 旗舰模型Graph - R1 - 7B在多领域表现出强泛化能力，在NP难图问题上超越QwQ - 32B。

Conclusion: NP难图问题是提升大语言模型长思维链推理的有效可扩展资源，为大语言模型后训练开辟新领域。

Abstract: Reasoning Large Language Models (RLLMs) have recently achieved remarkable
progress on complex reasoning tasks, largely enabled by their long
chain-of-thought (Long CoT) capabilities. However, developing these Long CoT
behaviors relies heavily on post-training with high-quality datasets, which are
typically costly and human-curated (e.g., mathematics and code), leaving
scalable alternatives unexplored. In this work, we introduce NP-hard (NPH)
graph problems as a novel synthetic training corpus, as they inherently require
deep reasoning, extensive exploration, and reflective strategies, which are
core characteristics of Long CoT reasoning. Building on this insight, we
develop a two-stage post-training framework: (i) Long CoT Supervised
Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially
enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a
fine-grained reward design, which sharpens reasoning efficiency. Our flagship
model, Graph-R1-7B, demonstrates strong generalization across mathematics,
coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both
accuracy and reasoning efficiency. These results position NPH graph problems as
an effective and scalable resource for advancing Long CoT reasoning in LLMs,
opening a new frontier for LLM post-training. Our implementation is available
at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted
in our Hugging Face collection HKUST-DSAIL/Graph-R1.

</details>


### [220] [Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction](https://arxiv.org/abs/2508.20395)
*Xu Guo*

Main category: cs.CL

TL;DR: 本文研究推理步骤效用对答案正确性的影响，在MATH数据集上实验，发现条件熵下降与正确答案强相关，错误推理路径往往更长。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型依赖生成中间推理步骤提升准确性，但很少研究推理效用对最终答案正确性的贡献，且生成更多上下文不一定增加答案置信度，希望能预测推理步骤是否有用以优化推理过程。

Method: 在MATH数据集上用Qwen2.5 - 32B和GPT - 4o生成推理链，用Qwen3 - 8B量化推理链对最终准确性的效用，通过条件熵测量模型在每个推理步骤对答案跨度的不确定性。

Result: 条件熵随步骤下降与正确答案强相关，平坦或增加的熵常导致错误答案，错误推理路径比正确的更长。

Conclusion: 研究结果为设计能早期检测和避免无效推理的高效推理管道提供基础。

Abstract: Recent advancements in large language models (LLMs) often rely on generating
intermediate reasoning steps to enhance accuracy. However, little work has
examined how reasoning utility contributes to the final answer's correctness.
Due to the stochastic nature of autoregressive generation, generating more
context does not guarantee increased confidence in the answer. If we could
predict, during generation, whether a reasoning step will be useful, we could
stop early or prune ineffective steps, avoiding distractions in the final
decision.
  We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to
generate reasoning chains, and then employing a separate model (Qwen3-8B) to
quantify the utility of these chains for final accuracy. Specifically, we
measure the model's uncertainty on the answer span Y at each reasoning step
using conditional entropy (expected negative log-likelihood over the
vocabulary) with context expanding step by step. Our results show a clear
pattern: conditional entropy that decreases over steps is strongly associated
with correct answers, whereas flat or increasing entropy often results in wrong
answers. We also corroborate that incorrect reasoning paths tend to be longer
than correct ones, suggesting that longer reasoning does not necessarily yield
better outcomes. These findings serve as a foundation to inspire future work on
designing efficient reasoning pipelines that detect and avoid unproductive
reasoning early.

</details>


### [221] [DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding](https://arxiv.org/abs/2508.20416)
*Hengchuan Zhu,Yihuan Xu,Yichen Li,Zijie Meng,Zuozhu Liu*

Main category: cs.CL

TL;DR: 本文介绍了首个评估牙科领域大语言模型的双语基准DentalBench，评估14个模型发现性能差距，实验表明领域适配可提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型和医学大语言模型在牙科等专业医学领域的能力因缺乏针对性评估资源而未得到充分探索。

Method: 引入包含DentalQA问答基准和DentalCorpus语料库的DentalBench，评估14个大语言模型，并使用Qwen - 2.5 - 3B进行领域适配实验。

Result: 不同任务类型和语言的模型存在显著性能差距，领域适配能显著提高模型在知识密集和术语聚焦任务上的性能。

Conclusion: 特定领域基准对于开发适用于医疗应用的可靠有效大语言模型非常重要。

Abstract: Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs)
have demonstrated strong performance on general medical benchmarks. However,
their capabilities in specialized medical fields, such as dentistry which
require deeper domain-specific knowledge, remain underexplored due to the lack
of targeted evaluation resources. In this paper, we introduce DentalBench, the
first comprehensive bilingual benchmark designed to evaluate and advance LLMs
in the dental domain. DentalBench consists of two main components: DentalQA, an
English-Chinese question-answering (QA) benchmark with 36,597 questions
spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale,
high-quality corpus with 337.35 million tokens curated for dental domain
adaptation, supporting both supervised fine-tuning (SFT) and
retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering
proprietary, open-source, and medical-specific models, and reveal significant
performance gaps across task types and languages. Further experiments with
Qwen-2.5-3B demonstrate that domain adaptation substantially improves model
performance, particularly on knowledge-intensive and terminology-focused tasks,
and highlight the importance of domain-specific benchmarks for developing
trustworthy and effective LLMs tailored to healthcare applications.

</details>


### [222] [Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark](https://arxiv.org/abs/2508.20511)
*Chihiro Taguchi,Seng Mai,Keita Kurabe,Yusuke Sakai,Georgina Agyei,Soudabeh Eslami,David Chiang*

Main category: cs.CL

TL;DR: 研究发现FLORES+基准在多语言评估中有缺陷，建议使用领域通用、文化中立源文本的基准。


<details>
  <summary>Details</summary>
Motivation: 评估FLORES+基准在多语言机器翻译评估中的适用性。

Method: 研究四种语言数据、进行人工评估、测试简单启发式方法、对比不同训练数据的模型表现。

Result: FLORES+许多翻译未达质量标准，源句有领域和文化偏向，简单启发式方法可得BLEU分，高质量自然数据训练的模型在FLORES+表现差。

Conclusion: 应采用领域通用、文化中立且少依赖命名实体的多语言MT基准。

Abstract: Multilingual machine translation (MT) benchmarks play a central role in
evaluating the capabilities of modern MT systems. Among them, the FLORES+
benchmark is widely used, offering English-to-many translation data for over
200 languages, curated with strict quality control protocols. However, we study
data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani)
and uncover critical shortcomings in the benchmark's suitability for truly
multilingual evaluation. Human assessments reveal that many translations fall
below the claimed 90% quality standard, and the annotators report that source
sentences are often too domain-specific and culturally biased toward the
English-speaking world. We further demonstrate that simple heuristics, such as
copying named entities, can yield non-trivial BLEU scores, suggesting
vulnerabilities in the evaluation protocol. Notably, we show that MT models
trained on high-quality, naturalistic data perform poorly on FLORES+ while
achieving significant gains on our domain-relevant evaluation set. Based on
these findings, we advocate for multilingual MT benchmarks that use
domain-general and culturally neutral source texts rely less on named entities,
in order to better reflect real-world translation challenges.

</details>


### [223] [Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data](https://arxiv.org/abs/2508.20557)
*Jiahao Xiao,Jiangming Liu*

Main category: cs.CL

TL;DR: 文章引入多领域非IID场景及统一基准框架，提出AdaFD框架解决多领域非IID挑战，实验显示模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有实验非IID场景主要关注标签多样性，未考虑自然语言处理中语言领域多样性，为解决真实环境中非IID数据挑战。

Method: 引入多领域非IID场景，提出统一基准框架，设计AdaFD框架。

Result: 模型能捕捉本地客户端的多样性，相比现有工作取得更好性能。

Conclusion: 所提框架可有效解决多领域非IID挑战，适用于真实环境评估联邦学习框架。

Abstract: The widespread success of pre-trained language models has established a new
training paradigm, where a global PLM is fine-tuned using task-specific data
from local clients. The local data are highly different from each other and can
not capture the global distribution of the whole data in real world. To address
the challenges of non-IID data in real environments, privacy-preserving
federated distillation has been proposed and highly investigated. However,
previous experimental non-IID scenarios are primarily identified with the label
(output) diversity, without considering the diversity of language domains
(input) that is crucial in natural language processing. In this paper, we
introduce a comprehensive set of multi-domain non-IID scenarios and propose a
unified benchmarking framework that includes diverse data. The benchmark can be
used to evaluate the federated learning framework in a real environment. To
this end, we propose an Adaptive Federated Distillation (AdaFD) framework
designed to address multi-domain non-IID challenges in both homogeneous and
heterogeneous settings. Experimental results demonstrate that our models
capture the diversity of local clients and achieve better performance compared
to the existing works. The code for this paper is available at:
https://github.com/jiahaoxiao1228/AdaFD.

</details>


### [224] [A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models](https://arxiv.org/abs/2508.20583)
*Soham Petkar,Hari Aakash K,Anirudh Vempati,Akshit Sinha,Ponnurangam Kumarauguru,Chirag Agarwal*

Main category: cs.CL

TL;DR: 指出当前GLM评估基准不足，引入CLEGR基准评估，发现软提示LLM与含GNN的GLM表现相当，揭示当前GLM图推理能力局限。


<details>
  <summary>Details</summary>
Motivation: 现有GLM评估基准多为节点分类数据集，无法有效评估多模态推理，需新基准。

Method: 引入CLEGR基准，采用合成图生成管道与需联合推理的问题，对代表性GLM架构评估。

Result: 软提示LLM与含完整GNN骨干的GLM表现相当，GLM在结构推理任务中性能下降。

Conclusion: 当前GLM图推理能力有局限，为推动显式多模态推理提供基础。

Abstract: Developments in Graph-Language Models (GLMs) aim to integrate the structural
reasoning capabilities of Graph Neural Networks (GNNs) with the semantic
understanding of Large Language Models (LLMs). However, we demonstrate that
current evaluation benchmarks for GLMs, which are primarily repurposed
node-level classification datasets, are insufficient to assess multimodal
reasoning. Our analysis reveals that strong performance on these benchmarks is
achievable using unimodal information alone, suggesting that they do not
necessitate graph-language integration. To address this evaluation gap, we
introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed
to evaluate multimodal reasoning at various complexity levels. Our benchmark
employs a synthetic graph generation pipeline paired with questions that
require joint reasoning over structure and textual semantics. We perform a
thorough evaluation of representative GLM architectures and find that
soft-prompted LLM baselines perform on par with GLMs that incorporate a full
GNN backbone. This result calls into question the architectural necessity of
incorporating graph structure into LLMs. We further show that GLMs exhibit
significant performance degradation in tasks that require structural reasoning.
These findings highlight limitations in the graph reasoning capabilities of
current GLMs and provide a foundation for advancing the community toward
explicit multimodal reasoning involving graph structure and language.

</details>


### [225] [Generative Annotation for ASR Named Entity Correction](https://arxiv.org/abs/2508.20700)
*Yuanchang Luo,Daimeng Wei,Shaojun Li,Hengchao Shang,Jiaxin Guo,Zongyao Li,Zhanglin Wu,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Hao Yang*

Main category: cs.CL

TL;DR: 提出新的命名实体纠正（NEC）方法，利用语音特征检索候选实体并纠正转录错误，实验表明该方法有效且将开源测试集和训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于音素级编辑距离算法的NEC模型在错误转录词与真实实体形式差异大时定位错误词能力有限，限制了其使用。

Method: 利用语音特征检索候选实体，设计生成式方法标注错误并替换为正确实体。

Result: 使用开源和自建测试集测试，该NEC方法显著提高了实体准确率。

Conclusion: 提出的新NEC方法有效，可改善ASR转录中实体的准确性，将开源测试集和训练数据。

Abstract: End-to-end automatic speech recognition systems often fail to transcribe
domain-specific named entities, causing catastrophic failures in downstream
tasks. Numerous fast and lightweight named entity correction (NEC) models have
been proposed in recent years. These models, mainly leveraging phonetic-level
edit distance algorithms, have shown impressive performances. However, when the
forms of the wrongly-transcribed words(s) and the ground-truth entity are
significantly different, these methods often fail to locate the wrongly
transcribed words in hypothesis, thus limiting their usage. We propose a novel
NEC method that utilizes speech sound features to retrieve candidate entities.
With speech sound features and candidate entities, we inovatively design a
generative method to annotate entity errors in ASR transcripts and replace the
text with correct entities. This method is effective in scenarios of word form
difference. We test our method using open-source and self-constructed test
sets. The results demonstrate that our NEC method can bring significant
improvement to entity accuracy. We will open source our self-constructed test
set and training data.

</details>


### [226] [Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection](https://arxiv.org/abs/2508.20766)
*Harethah Abu Shairah,Hasan Abed Al Kader Hammoud,George Turkiyyah,Bernard Ghanem*

Main category: cs.CL

TL;DR: 提出Rank - One Safety Injection (ROSI)方法增强大语言模型安全对齐，能提升拒绝有害请求率且保留模型效用。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全机制可被绕过，需新方法增强安全对齐。

Method: 提出ROSI，一种白盒方法，对所有残差流写入矩阵进行无需微调的一阶权重修改，安全方向可从少量有害和无害指令对计算得出。

Result: ROSI能提高安全拒绝率，保留模型在标准基准测试上的效用，还能重新对齐“未审查”模型。

Conclusion: 有针对性、可解释的权重调整是提高大语言模型安全的低成本有效机制，可补充资源密集的微调范式。

Abstract: Safety alignment in Large Language Models (LLMs) often involves mediating
internal representations to refuse harmful requests. Recent research has
demonstrated that these safety mechanisms can be bypassed by ablating or
removing specific representational directions within the model. In this paper,
we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box
method that amplifies a model's safety alignment by permanently steering its
activations toward the refusal-mediating subspace. ROSI operates as a simple,
fine-tuning-free rank-one weight modification applied to all residual stream
write matrices. The required safety direction can be computed from a small set
of harmful and harmless instruction pairs. We show that ROSI consistently
increases safety refusal rates - as evaluated by Llama Guard 3 - while
preserving the utility of the model on standard benchmarks such as MMLU,
HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align
'uncensored' models by amplifying their own latent safety directions,
demonstrating its utility as an effective last-mile safety procedure. Our
results suggest that targeted, interpretable weight steering is a cheap and
potent mechanism to improve LLM safety, complementing more resource-intensive
fine-tuning paradigms.

</details>


### [227] [Signs of Struggle: Spotting Cognitive Distortions across Language and Register](https://arxiv.org/abs/2508.20771)
*Abhishek Kuber,Enrico Liscio,Ruixuan Zhang,Caroline Figueroa,Pradeep K. Murukannaiah*

Main category: cs.CL

TL;DR: 研究荷兰青少年论坛帖子中认知扭曲检测的跨语言和跨文体泛化，发现语言和写作风格影响模型性能，领域适应方法最有前景。


<details>
  <summary>Details</summary>
Motivation: 青少年心理健康问题增多，需自动化方法检测数字文本中心理困扰早期迹象，尤其关注认知扭曲的识别。

Method: 对荷兰青少年论坛帖子进行研究，分析认知扭曲检测的跨语言和跨文体泛化。

Result: 语言和写作风格变化会显著影响模型性能。

Conclusion: 领域适应方法在认知扭曲检测中最有前景。

Abstract: Rising mental health issues among youth have increased interest in automated
approaches for detecting early signs of psychological distress in digital text.
One key focus is the identification of cognitive distortions, irrational
thought patterns that have a role in aggravating mental distress. Early
detection of these distortions may enable timely, low-cost interventions. While
prior work has focused on English clinical data, we present the first in-depth
study of cross-lingual and cross-register generalization of cognitive
distortion detection, analyzing forum posts written by Dutch adolescents. Our
findings show that while changes in language and writing style can
significantly affect model performance, domain adaptation methods show the most
promise.

</details>


### [228] [Exploring Machine Learning and Language Models for Multimodal Depression Detection](https://arxiv.org/abs/2508.20805)
*Javier Si Zhao Hong,Timothy Zoe Delaya,Sherwyn Chan Yin Kit,Pai Chet Ng,Xiaoxiao Miao*

Main category: cs.CL

TL;DR: 本文介绍多模态人格感知抑郁症检测挑战的方法，对比不同模型在多模态特征上的表现，为心理健康预测提供策略见解。


<details>
  <summary>Details</summary>
Motivation: 参与多模态抑郁症检测挑战，探索有效多模态表征策略用于心理健康预测。

Method: 使用机器学习和深度学习模型，探索并比较XGBoost、基于Transformer的架构和大语言模型在音频、视频和文本特征上的性能。

Result: 突出了每种模型在跨模态捕捉抑郁症相关信号方面的优缺点。

Conclusion: 为心理健康预测提供了有效的多模态表征策略见解。

Abstract: This paper presents our approach to the first Multimodal Personality-Aware
Depression Detection Challenge, focusing on multimodal depression detection
using machine learning and deep learning models. We explore and compare the
performance of XGBoost, transformer-based architectures, and large language
models (LLMs) on audio, video, and text features. Our results highlight the
strengths and limitations of each type of model in capturing depression-related
signals across modalities, offering insights into effective multimodal
representation strategies for mental health prediction.

</details>


### [229] [ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents](https://arxiv.org/abs/2508.20973)
*Tianjian Liu,Fanqi Wan,Jiajian Guo,Xiaojun Quan*

Main category: cs.CL

TL;DR: 提出ProactiveEval框架评估大语言模型主动对话能力，构建评估环境并测试多种模型，还探讨推理能力影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦特定领域或任务，导致评估碎片化，限制对模型主动对话能力的全面探索。

Method: 提出ProactiveEval框架，将主动对话分解为目标规划和对话引导，建立跨领域评估指标，自动生成评估数据，构建328个评估环境。

Result: 对22种大语言模型实验，DeepSeek - R1和Claude - 3.7 - Sonnet分别在目标规划和对话引导任务中表现出色。

Conclusion: 探讨推理能力对主动行为的影响，为未来模型发展提供启示。

Abstract: Proactive dialogue has emerged as a critical and challenging research problem
in advancing large language models (LLMs). Existing works predominantly focus
on domain-specific or task-oriented scenarios, which leads to fragmented
evaluations and limits the comprehensive exploration of models' proactive
conversation abilities. In this work, we propose ProactiveEval, a unified
framework designed for evaluating proactive dialogue capabilities of LLMs. This
framework decomposes proactive dialogue into target planning and dialogue
guidance, establishing evaluation metrics across various domains. Moreover, it
also enables the automatic generation of diverse and challenging evaluation
data. Based on the proposed framework, we develop 328 evaluation environments
spanning 6 distinct domains. Through experiments with 22 different types of
LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional
performance on target planning and dialogue guidance tasks, respectively.
Finally, we investigate how reasoning capabilities influence proactive
behaviors and discuss their implications for future model development.

</details>


### [230] [Enabling Equitable Access to Trustworthy Financial Reasoning](https://arxiv.org/abs/2508.21051)
*William Jurayj,Nils Holzenberger,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本文提出将大语言模型与符号求解器集成的方法计算税务义务，在SARA数据集评估，展示了神经符号架构用于税务协助的前景和经济可行性。


<details>
  <summary>Details</summary>
Motivation: 税务申报需复杂推理，现代大语言模型因难以保证高准确性和可审计性，不适合此任务，需要更好的方法。

Method: 提出将大语言模型与符号求解器集成的方法，在SARA数据集评估变体系统，提出基于现实税务错误罚款估算系统部署成本的方法，结合规则翻译和案例检索提高性能。

Result: 该方法能显著提高任务表现，将成本降至现实平均水平以下。

Conclusion: 神经符号架构用于可靠税务协助有前景且经济可行，能增加公平获取税务协助的机会。

Abstract: According to the United States Internal Revenue Service, ''the average
American spends $\$270$ and 13 hours filing their taxes''. Even beyond the
U.S., tax filing requires complex reasoning, combining application of
overlapping rules with numerical calculations. Because errors can incur costly
penalties, any automated system must deliver high accuracy and auditability,
making modern large language models (LLMs) poorly suited for this task. We
propose an approach that integrates LLMs with a symbolic solver to calculate
tax obligations. We evaluate variants of this system on the challenging
StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for
estimating the cost of deploying such a system based on real-world penalties
for tax errors. We further show how combining up-front translation of
plain-text rules into formal logic programs, combined with intelligently
retrieved exemplars for formal case representations, can dramatically improve
performance on this task and reduce costs to well below real-world averages.
Our results demonstrate the promise and economic feasibility of neuro-symbolic
architectures for increasing equitable access to reliable tax assistance.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [231] [Modelling birdsong transmission with methods from molecular sequence analysis](https://arxiv.org/abs/2508.20833)
*Anthony Kwong,Mark Muldoon*

Main category: q-bio.QM

TL;DR: 本文提出一种新的马尔可夫模型用于鸟鸣传播，拟合模型并分析爪哇麻雀歌曲，发现模型预测能力有限。


<details>
  <summary>Details</summary>
Motivation: 研究鸟鸣传播，为其建立合适的模型。

Method: 通过与生物序列分析模型类比建立马尔可夫模型，使用IPLA算法拟合模型，分析爪哇麻雀的歌曲。

Result: 模型对于爪哇麻雀鸟鸣传播的一些自然问题预测能力有限。

Conclusion: 模型预测能力有限可能是由于鸟鸣学习的忠诚度和爪哇麻雀歌曲相对简短。

Abstract: In many species of songbirds, juvenile males learn their songs from adult
male tutors. In this paper we formulate a novel Markov model for birdsong
transmission developed by analogy with models used in biological sequence
analysis. We fit the model using the recently developed Interacting Particle
Langevin Algorithm (IPLA) of Akyildiz et al. (arXiv:2303.13429) and analyse a
collection of songs from Java sparrows (Lonchura oryzivora) originally recorded
and studied by Masayo Soma and her collaborators. The model proves to have
limited predictive power for a number of natural problems associated with song
transmission in Java sparrows and we propose reasons for this, including the
well-established faithfulness of song-learning and the comparative brevity of
Java sparrow songs.

</details>


### [232] [Artificial Intelligence for CRISPR Guide RNA Design: Explainable Models and Off-Target Safety](https://arxiv.org/abs/2508.20130)
*Alireza Abbaszadeh,Armita Shahlai*

Main category: q-bio.QM

TL;DR: 本文综述了机器学习模型优化CRISPR系统gRNA设计，强调AI与基因组编辑跨学科融合推动CRISPR应用。


<details>
  <summary>Details</summary>
Motivation: CRISPR基因编辑虽革新生物技术，但优化gRNA设计效率和安全性仍是挑战。

Method: 综述2020 - 2025年人工智能特别是深度学习改善gRNA设计的进展，包括模型预测及解释方法。

Result: AI尤其是深度学习可显著提高gRNA靶向活性预测、识别脱靶风险，新兴XAI技术能解释模型黑箱。

Conclusion: AI与基因组编辑跨学科融合可推动更高效、精准和临床可行的CRISPR应用。

Abstract: CRISPR-based genome editing has revolutionized biotechnology, yet optimizing
guide RNA (gRNA) design for efficiency and safety remains a critical challenge.
Recent advances (2020--2025, updated to reflect current year if needed)
demonstrate that artificial intelligence (AI), especially deep learning, can
markedly improve the prediction of gRNA on-target activity and identify
off-target risks. In parallel, emerging explainable AI (XAI) techniques are
beginning to illuminate the black-box nature of these models, offering insights
into sequence features and genomic contexts that drive Cas enzyme performance.
Here we review how state-of-the-art machine learning models are enhancing gRNA
design for CRISPR systems, highlight strategies for interpreting model
predictions, and discuss new developments in off-target prediction and safety
assessment. We emphasize breakthroughs from top-tier journals that underscore
an interdisciplinary convergence of AI and genome editing to enable more
efficient, specific, and clinically viable CRISPR applications.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [233] [The Epistemic Support-Point Filter (ESPF): A Bounded Possibilistic Framework for Ordinal State Estimation](https://arxiv.org/abs/2508.20806)
*Moriba Jah,Van Haslett*

Main category: cs.IT

TL;DR: 引入非贝叶斯滤波框架ESPF，基于可能性理论和认知谦逊，能动态调整信念支持，实现无先验统计校准的鲁棒估计。


<details>
  <summary>Details</summary>
Motivation: 传统状态估计方法将认知不确定性简化为标量信念，在稀疏或对抗性传感环境中可能过度自信，需新方法解决。

Method: 引入ESPF，用兼容性加权支持更新、基于惊奇感的修剪和稀疏网格求积的自适应离散化来定义状态空间上的信念演变；多模型推理用Choquet积分融合竞争假设。

Result: 得到一个能根据信息结构动态收缩或扩展信念支持的推理引擎，无需先验统计校准。

Conclusion: 此工作在推理、证据和无知的调和上有基础性转变，支持在无先验、先验误导或不合理时进行鲁棒估计。

Abstract: Traditional state estimation methods rely on probabilistic assumptions that
often collapse epistemic uncertainty into scalar beliefs, risking
overconfidence in sparse or adversarial sensing environments. We introduce the
Epistemic Support-Point Filter (ESPF), a novel non-Bayesian filtering framework
fully grounded in possibility theory and epistemic humility. ESPF redefines the
evolution of belief over state space using compatibility-weighted support
updates, surprisalaware pruning, and adaptive dispersion via sparse grid
quadrature. Unlike conventional filters, ESPF does not seek a posterior
distribution, but rather maintains a structured region of plausibility or
non-rejection, updated using ordinal logic rather than integration. For
multi-model inference, we employ the Choquet integral to fuse competing
hypotheses based on a dynamic epistemic capacity function, generalizing
classical winner-take-all strategies. The result is an inference engine capable
of dynamically contracting or expanding belief support in direct response to
information structure, without requiring prior statistical calibration. This
work presents a foundational shift in how inference, evidence, and ignorance
are reconciled, supporting robust estimation where priors are unavailable,
misleading, or epistemically unjustified.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [234] [A Hierarchical Signal Coordination and Control System Using a Hybrid Model-based and Reinforcement Learning Approach](https://arxiv.org/abs/2508.20102)
*Xianyue Peng,Shenyang Chen,H. Michael Zhang*

Main category: eess.SY

TL;DR: 提出分层交通信号协调控制方案，结合模型优化与强化学习，在SUMO - RLlib平台评估，不同策略在不同需求下表现不同，分层设计可实现各需求水平的稳健性能。


<details>
  <summary>Details</summary>
Motivation: 应对城市走廊信号控制中维持干道交通连续性和适应局部交叉口需求变化的双重挑战。

Method: 提出分层交通信号协调控制方案，包括高层协调器、走廊协调器和混合信号智能体，用近端策略优化（PPO）的分层强化学习训练策略。

Result: 混合MFC在高需求下使吞吐量最大化；混合GWC持续减少干道停车并维持连续性，但会降低全网效率；PAC在中等需求下改善全网出行时间，高需求下效果差。

Conclusion: 分层设计能实现自适应策略选择，在所有需求水平上实现稳健性能。

Abstract: Signal control in urban corridors faces the dual challenge of maintaining
arterial traffic progression while adapting to demand variations at local
intersections. We propose a hierarchical traffic signal coordination and
control scheme that integrates model-based optimization with reinforcement
learning. The system consists of: (i) a High-Level Coordinator (HLC) that
selects coordination strategies based on observed and predicted demand; (ii) a
Corridor Coordinator that derives phase constraints from the selected
strategy-either Max-Flow Coordination (MFC) or Green-Wave Coordination (GWC);
and (iii) Hybrid Signal Agents (HSAs) that determine signal phases via
reinforcement learning with action masking to enforce feasibility. Hierarchical
reinforcement learning with Proximal Policy Optimization (PPO) is used to train
HSA and HLC policies. At the lower level, three HSA policies-MFC-aware,
GWC-aware, and pure agent control (PAC) are trained in conjunction with their
respective coordination strategies. At the higher level, the HLC is trained to
dynamically switch strategies using a multi-objective reward balancing
corridor-level and network-wide performance. The proposed scheme was developed
and evaluated on a SUMO-RLlib platform. Case results show that hybrid MFC
maximizes throughput under heavy demand; hybrid GWC consistently minimizes
arterial stops and maintains progression across diverse traffic conditions but
can reduce network-wide efficiency; and PAC improves network-wide travel time
in moderate demand but is less effective under heavy demand. The hierarchical
design enables adaptive strategy selection, achieving robust performance across
all demand levels.

</details>


### [235] [Neural Spline Operators for Risk Quantification in Stochastic Systems](https://arxiv.org/abs/2508.20288)
*Zhuoyuan Wang,Raffaele Romagnoli,Kamyar Azizzadenesheli,Yorie Nakahira*

Main category: eess.SY

TL;DR: 引入物理信息神经算子PINO方法解决风险量化问题，提出NeSO框架并证明其通用逼近能力，通过案例展示其有效性和速度优势，该框架有望用于其他问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于采样和PDE的方法难处理复杂变化动态，物理信息神经网络无法考虑系统动态的功能变化，需要新方法解决风险量化问题。

Method: 引入PINO方法到风险量化问题，提出NeSO框架，利用B样条表示提高训练效率和边界条件处理能力，并进行理论分析。

Result: 通过两个案例研究，证明NeSO的有效性和相比现有方法显著的在线加速效果。

Conclusion: 提出的框架和通用逼近定理有望用于风险量化之外的其他控制或PDE相关问题。

Abstract: Accurately quantifying long-term risk probabilities in diverse stochastic
systems is essential for safety-critical control. However, existing
sampling-based and partial differential equation (PDE)-based methods often
struggle to handle complex varying dynamics. Physics-informed neural networks
learn surrogate mappings for risk probabilities from varying system parameters
of fixed and finite dimensions, yet can not account for functional variations
in system dynamics. To address these challenges, we introduce physics-informed
neural operator (PINO) methods to risk quantification problems, to learn
mappings from varying \textit{functional} system dynamics to corresponding risk
probabilities. Specifically, we propose Neural Spline Operators (NeSO), a PINO
framework that leverages B-spline representations to improve training
efficiency and achieve better initial and boundary condition enforcements,
which are crucial for accurate risk quantification. We provide theoretical
analysis demonstrating the universal approximation capability of NeSO. We also
present two case studies, one with varying functional dynamics and another with
high-dimensional multi-agent dynamics, to demonstrate the efficacy of NeSO and
its significant online speed-up over existing methods. The proposed framework
and the accompanying universal approximation theorem are expected to be
beneficial for other control or PDE-related problems beyond risk
quantification.

</details>


### [236] [Delay-adaptive Control of Nonlinear Systems with Approximate Neural Operator Predictors](https://arxiv.org/abs/2508.20367)
*Luke Bhan,Miroslav Krstic,Yuanyuan Shi*

Main category: eess.SY

TL;DR: 提出处理含未知长执行器延迟非线性系统中预测反馈控制器的方法，用学习的神经算子映射近似预测器，经理论分析和生物系统验证，比传统方法快15倍。


<details>
  <summary>Details</summary>
Motivation: 处理含未知且任意长执行器延迟的非线性系统中预测反馈控制器的实现问题，解决预测器解析难解的问题。

Method: 用学习的神经算子映射近似预测器，离线训练后在线部署；基于神经算子通用逼近定理和延迟的传输偏微分方程表示进行理论稳定性分析；用Lyapunov - Krasovskii泛函证明半全局实际收敛性。

Result: 通过生物激活/抑制系统验证理论结果，比传统数值方法快15倍。

Conclusion: 所提方法能有效处理含未知长执行器延迟的非线性系统，具有更快的计算速度。

Abstract: In this work, we propose a rigorous method for implementing predictor
feedback controllers in nonlinear systems with unknown and arbitrarily long
actuator delays. To address the analytically intractable nature of the
predictor, we approximate it using a learned neural operator mapping. This
mapping is trained once, offline, and then deployed online, leveraging the fast
inference capabilities of neural networks. We provide a theoretical stability
analysis based on the universal approximation theorem of neural operators and
the transport partial differential equation (PDE) representation of the delay.
We then prove, via a Lyapunov-Krasovskii functional, semi-global practical
convergence of the dynamical system dependent on the approximation error of the
predictor and delay bounds. Finally, we validate our theoretical results using
a biological activator/repressor system, demonstrating speedups of 15 times
compared to traditional numerical methods.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [237] [Validating Generative Agent-Based Models for Logistics and Supply Chain Management Research](https://arxiv.org/abs/2508.20234)
*Vincent E. Castillo*

Main category: cs.MA

TL;DR: 研究通过实验评估大语言模型在物流与供应链管理（LSCM）模拟中对人类行为的等效性，提出双验证框架，表明生成式基于代理的模型（GABMs）经适当验证可用于LSCM。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的GABMs有潜力用于LSCM研究，但LLM在LSCM模拟中作为人类行为代理的有效性未知，需进行评估。

Method: 通过控制实验，采用调节中介设计，用6个先进LLM与957名人类参与者（477对）在食品配送场景的二元客户 - 员工互动中进行对比测试。

Result: 发现需在人类等效性测试和决策过程验证两个层面验证GABMs；GABMs能有效模拟人类行为，但出现等效性与过程的悖论，部分LLM有表面等效性，但存在非人类的决策过程。

Conclusion: GABMs经适当验证可作为LSCM的有效方法工具，双验证框架为研究人员提供开发指南，为从业者选择LLM提供基于证据的评估。

Abstract: Generative Agent-Based Models (GABMs) powered by large language models (LLMs)
offer promising potential for empirical logistics and supply chain management
(LSCM) research by enabling realistic simulation of complex human behaviors.
Unlike traditional agent-based models, GABMs generate human-like responses
through natural language reasoning, which creates potential for new
perspectives on emergent LSCM phenomena. However, the validity of LLMs as
proxies for human behavior in LSCM simulations is unknown. This study evaluates
LLM equivalence of human behavior through a controlled experiment examining
dyadic customer-worker engagements in food delivery scenarios. I test six
state-of-the-art LLMs against 957 human participants (477 dyads) using a
moderated mediation design. This study reveals a need to validate GABMs on two
levels: (1) human equivalence testing, and (2) decision process validation.
Results reveal GABMs can effectively simulate human behaviors in LSCM; however,
an equivalence-versus-process paradox emerges. While a series of Two One-Sided
Tests (TOST) for equivalence reveals some LLMs demonstrate surface-level
equivalence to humans, structural equation modeling (SEM) reveals artificial
decision processes not present in human participants for some LLMs. These
findings show GABMs as a potentially viable methodological instrument in LSCM
with proper validation checks. The dual-validation framework also provides LSCM
researchers with a guide to rigorous GABM development. For practitioners, this
study offers evidence-based assessment for LLM selection for operational tasks.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [238] [Studying Effective String Theory using deep generative models](https://arxiv.org/abs/2508.20610)
*Michele Caselle,Elia Cellini,Alessandro Nada*

Main category: hep-lat

TL;DR: 介绍有效弦理论（EST）及用深度学习数值探索EST的方法，给出Nambu - Goto EST宽度结果。


<details>
  <summary>Details</summary>
Motivation: EST中部分问题如确定通量管宽度难以解析求解，需新方法探索。

Method: 采用基于生成算法的深度学习技术对EST进行数值探索。

Result: 给出了Nambu - Goto EST的宽度结果。

Conclusion: 基于生成算法的深度学习技术可用于数值探索EST。

Abstract: Effective String Theory (EST) offers a robust non-perturbative framework for
describing confinement in Yang-Mills theory by treating the confining flux tube
between a static quark-antiquark pair as a thin, vibrating string. While EST
calculations are typically carried out using zeta-function regularization,
certain problems-such as determining the flux tube width-are too complex to
solve analytically. However, recent studies have demonstrated that EST can be
explored numerically by employing deep learning techniques based on generative
algorithms. In this work, we provide a brief introduction to EST and this novel
numerical approach. Finally, we present results for the width of the
Nambu-Got\"o EST.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [239] [Self-consistent clustering analysis for homogenisation of heterogeneous plates](https://arxiv.org/abs/2508.20446)
*Menglei Li,Haolin Li,Bing Wang,Bing Wang*

Main category: physics.comp-ph

TL;DR: 本文通过耦合自洽聚类分析与Lippmann - Schwinger方程，为周期性微结构板结构引入降阶模型，实现快速多尺度均匀化，经测试精度与FFT直接数值模拟相当，计算成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: 实现具有周期性微结构的板结构的快速多尺度均匀化。

Method: 将自洽聚类分析（SCA）与Lippmann - Schwinger方程耦合，推导特定于板的SCA方案，采用离线 - 在线策略结合格林函数与k - 均值数据压缩，以及在线自洽更新利用参考介质弱敏感性。

Result: 该框架能处理经典板理论和一阶剪切变形理论中的线性和非线性问题，在多种类型的板和复合材料测试中，模型精度与FFT直接数值模拟相当，计算成本降低超一个数量级。

Conclusion: 所提出的降阶模型能有效实现周期性微结构板结构的多尺度均匀化，在保证精度的同时大幅降低计算成本。

Abstract: This work introduces a reduced-order model for plate structures with periodic
micro-structures by coupling self-consistent clustering analysis (SCA) with the
Lippmann-Schwinger equation, enabling rapid multiscale homogenisation of
heterogeneous plates. A plate-specific SCA scheme is derived for the first time
and features two key elements: (i) an offline-online strategy that combines
Green's functions with k-means data compression, and (ii) an online
self-consistent update that exploits the weak sensitivity of the reference
medium. The framework handles both linear and nonlinear problems in classical
plate theory and first-order shear deformation theory, and its performance is
verified on linear isotropic perforated plates and woven composites, as well as
on non-linear elasto-plastic perforated plates and woven composites with
damage. Across all cases the proposed model matches the accuracy of FFT-based
direct numerical simulation while reducing computational cost by over an order
of magnitude.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [240] [Mixture of Contexts for Long Video Generation](https://arxiv.org/abs/2508.21058)
*Shengqu Cai,Ceyuan Yang,Lvmin Zhang,Yuwei Guo,Junfei Xiao,Ziyan Yang,Yinghao Xu,Zhenheng Yang,Alan Yuille,Leonidas Guibas,Maneesh Agrawala,Lu Jiang,Gordon Wetzstein*

Main category: cs.GR

TL;DR: 提出Mixture of Contexts (MoC)模块解决长视频生成中的长上下文记忆问题，实现高效训练与合成。


<details>
  <summary>Details</summary>
Motivation: 长视频生成存在长上下文记忆问题，传统扩散变压器因自注意力二次成本限制，难以处理长序列。

Method: 将长上下文视频生成视为内部信息检索任务，提出可学习的稀疏注意力路由模块MoC，查询动态选择信息块和锚点进行注意力计算。

Result: 模型能将计算资源分配给显著历史，保留数分钟内容的身份、动作和场景，实现近线性扩展。

Conclusion: MoC模块可解决长视频生成的长上下文记忆问题，实现高效训练和合成，使模型在分钟级尺度上具备记忆和一致性。

Abstract: Long video generation is fundamentally a long context memory problem: models
must retain and retrieve salient events across a long range without collapsing
or drifting. However, scaling diffusion transformers to generate long-context
videos is fundamentally limited by the quadratic cost of self-attention, which
makes memory and computation intractable and difficult to optimize for long
sequences. We recast long-context video generation as an internal information
retrieval task and propose a simple, learnable sparse attention routing module,
Mixture of Contexts (MoC), as an effective long-term memory retrieval engine.
In MoC, each query dynamically selects a few informative chunks plus mandatory
anchors (caption, local windows) to attend to, with causal routing that
prevents loop closures. As we scale the data and gradually sparsify the
routing, the model allocates compute to salient history, preserving identities,
actions, and scenes over minutes of content. Efficiency follows as a byproduct
of retrieval (near-linear scaling), which enables practical training and
synthesis, and the emergence of memory and consistency at the scale of minutes.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [241] [Stable and practical semi-Markov modelling of intermittently-observed data](https://arxiv.org/abs/2508.20949)
*Christopher Jackson*

Main category: stat.ME

TL;DR: 本文提出用“相类型”分布处理半马尔可夫模型，通过矩匹配法简化参数，实现通用半马尔可夫模型软件化，并在新R包中实现，经测试和应用验证。


<details>
  <summary>Details</summary>
Motivation: 现有半马尔可夫模型方法局限于特定状态结构或缺乏通用软件，需要开发通用方法。

Method: 使用“相类型”分布表示状态停留时间，将半马尔可夫模型转化为隐马尔可夫模型，用矩匹配法近似简化分布。

Result: 开发新R包“msmbayes”实现贝叶斯或最大似然估计，软件经模拟校准测试，通过认知功能衰退应用展示工作流程。

Conclusion: 该方法使通用半马尔可夫模型可用于间歇数据，在软件中得以实现。

Abstract: Multi-state models are commonly used for intermittent observations of a state
over time, but these are generally based on the Markov assumption, that
transition rates are independent of the time spent in current and previous
states. In a semi-Markov model, the rates can depend on the time spent in the
current state, though available methods for this are either restricted to
specific state structures or lack general software. This paper develops the
approach of using a "phase-type" distribution for the sojourn time in a state,
which expresses a semi-Markov model as a hidden Markov model, allowing the
likelihood to be calculated easily for any state structure. While this approach
involves a proliferation of latent parameters, identifiability can be improved
by restricting the phase-type family to one which approximates a simpler
distribution such as the Gamma or Weibull. This paper proposes a
moment-matching method to obtain this approximation, making general semi-Markov
models for intermittent data accessible in software for the first time. The
method is implemented in a new R package, "msmbayes", which implements Bayesian
or maximum likelihood estimation for multi-state models with general state
structures and covariates. The software is tested using simulation-based
calibration, and an application to cognitive function decline illustrates the
use of the method in a typical modelling workflow.

</details>


### [242] [Latent Factor Point Processes for Patient Representation in Electronic Health Records](https://arxiv.org/abs/2508.20327)
*Parker Knight,Doudou Zhou,Zongqi Xia,Tianxi Cai,Junwei Lu*

Main category: stat.ME

TL;DR: 提出潜在因子点过程模型和Fourier - Eigen嵌入方法，用于挖掘电子健康记录（EHR）中临床有意义的异质性，模拟和应用显示其实用优势。


<details>
  <summary>Details</summary>
Motivation: 现有统计方法丢弃EHR代码丰富的时间结构，现有时间模型有参数假设限制或不适用于患者层面任务。

Method: 提出潜在因子点过程模型，将代码出现表示为高维点过程，其条件强度由低维潜在泊松过程驱动；引入Fourier - Eigen嵌入方法构建患者表示。

Result: 理论证明这些嵌入能有效捕捉下游分类和聚类的特定子组时间模式，模拟和阿尔茨海默病EHR队列应用展示了方法的实际优势。

Conclusion: 所提方法在挖掘EHR中临床有意义的异质性方面具有实用优势。

Abstract: Electronic health records (EHR) contain valuable longitudinal patient-level
information, yet most statistical methods reduce the irregular timing of EHR
codes into simple counts, thereby discarding rich temporal structure. Existing
temporal models often impose restrictive parametric assumptions or are tailored
to code level rather than patient-level tasks. We propose the latent factor
point process model, which represents code occurrences as a high-dimensional
point process whose conditional intensity is driven by a low dimensional latent
Poisson process. This low-rank structure reflects the clinical reality that
thousands of codes are governed by a small number of underlying disease
processes, while enabling statistically efficient estimation in high
dimensions. Building on this model, we introduce the Fourier-Eigen embedding, a
patient representation constructed from the spectral density matrix of the
observed process. We establish theoretical guarantees showing that these
embeddings efficiently capture subgroup-specific temporal patterns for
downstream classification and clustering. Simulations and an application to an
Alzheimer's disease EHR cohort demonstrate the practical advantages of our
approach in uncovering clinically meaningful heterogeneity.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [243] [The Mathematician's Assistant: Integrating AI into Research Practice](https://arxiv.org/abs/2508.20236)
*Jonas Henkel*

Main category: math.HO

TL;DR: 本文探讨2025年8月前公开大语言模型在数学研究中的应用，分析其优缺点，提出整合AI的框架及应用方式，认为AI当前主要起增强作用。


<details>
  <summary>Details</summary>
Motivation: 人工智能发展为数学研究带来新工具，探索公开大语言模型在数学研究中的应用情况。

Method: 分析MathArena和Open Proof Corpus等基准测试，基于结果提出以增强数学家为原则的框架，探索AI在研究生命周期的应用方式。

Result: 当前模型有解题和评估证明能力，但存在缺乏自我批判等系统缺陷；提出整合AI的框架及五项指导原则和七种应用方式。

Conclusion: AI当前主要起增强而非自动化作用，需新技能集以有效使用工具。

Abstract: The rapid development of artificial intelligence (AI), marked by
breakthroughs like 'AlphaEvolve' and 'Gemini Deep Think', is beginning to offer
powerful new tools that have the potential to significantly alter the research
practice in many areas of mathematics. This paper explores the current
landscape of publicly accessible large language models (LLMs) in a mathematical
research context, based on developments up to August 2, 2025. Our analysis of
recent benchmarks, such as MathArena and the Open Proof Corpus (Balunovi\'c et
al., 2025; Dekoninck et al., 2025), reveals a complex duality: while
state-of-the-art models demonstrate strong abilities in solving problems and
evaluating proofs, they also exhibit systematic flaws, including a lack of
self-critique and a model depending discrepancy between final-answer accuracy
and full-proof validity.
  Based on these findings, we propose a durable framework for integrating AI
into the research workflow, centered on the principle of the augmented
mathematician. In this model, the AI functions as a copilot under the critical
guidance of the human researcher, an approach distilled into five guiding
principles for effective and responsible use. We then systematically explore
seven fundamental ways AI can be applied across the research lifecycle, from
creativity and ideation to the final writing process, demonstrating how these
principles translate into concrete practice.
  We conclude that the primary role of AI is currently augmentation rather than
automation. This requires a new skill set focused on strategic prompting,
critical verification, and methodological rigor in order to effectively use
these powerful tools.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [244] [Flowing Straighter with Conditional Flow Matching for Accurate Speech Enhancement](https://arxiv.org/abs/2508.20584)
*Mattias Cross,Anton Ragni*

Main category: cs.SD

TL;DR: 本文量化路径直度对语音增强质量的影响，提出独立条件流匹配方法，证明直的时间无关概率路径优于弯曲时间相关路径。


<details>
  <summary>Details</summary>
Motivation: 当前基于流的生成式语音增强方法使用弯曲概率路径，其影响未知，而机器学习研究表明直路径更易训练和泛化，因此量化路径直度对语音增强质量的影响。

Method: 对薛定谔桥进行实验，提出独立条件流匹配用于语音增强，用一步解决方案改进条件流匹配多推理步骤问题。

Result: 特定配置可使薛定谔桥路径更直；时间无关方差对样本质量影响大于梯度；条件流匹配改善多个语音质量指标；一步解决方案有效。

Conclusion: 直的时间无关概率路径比弯曲时间相关路径更能提升生成式语音增强效果。

Abstract: Current flow-based generative speech enhancement methods learn curved
probability paths which model a mapping between clean and noisy speech. Despite
impressive performance, the implications of curved probability paths are
unknown. Methods such as Schrodinger bridges focus on curved paths, where
time-dependent gradients and variance do not promote straight paths. Findings
in machine learning research suggest that straight paths, such as conditional
flow matching, are easier to train and offer better generalisation. In this
paper we quantify the effect of path straightness on speech enhancement
quality. We report experiments with the Schrodinger bridge, where we show that
certain configurations lead to straighter paths. Conversely, we propose
independent conditional flow-matching for speech enhancement, which models
straight paths between noisy and clean speech. We demonstrate empirically that
a time-independent variance has a greater effect on sample quality than the
gradient. Although conditional flow matching improves several speech quality
metrics, it requires multiple inference steps. We rectify this with a one-step
solution by inferring the trained flow-based model as if it was directly
predictive. Our work suggests that straighter time-independent probability
paths improve generative speech enhancement over curved time-dependent paths.

</details>


### [245] [Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic Music](https://arxiv.org/abs/2508.20665)
*Hongju Su,Ke Li,Lan Yang,Honggang Zhang,Yi-Zhe Song*

Main category: cs.SD

TL;DR: 提出新的符号音乐生成框架Amadeus，采用两级架构，结合策略提升性能，实验表现优于SOTA模型，实现免训练细粒度控制，还编译新数据集。


<details>
  <summary>Details</summary>
Motivation: 现有符号音乐生成模型假设音符属性有固定严格依赖结构，但不同初始属性标记表现相近，表明音符属性应是并发无序集合，需新框架。

Method: Amadeus采用两级架构，包括用于音符序列的自回归模型和用于属性的双向离散扩散模型；提出MLSDES策略和CIEM模块。

Result: 在无条件和文本条件生成任务实验中，Amadeus在多个指标上显著优于SOTA模型，速度提升至少4倍，实现免训练细粒度音符属性控制。

Conclusion: Amadeus架构有效提升符号音乐生成效果，编译的AMD数据集可支持预训练和微调。

Abstract: Existing state-of-the-art symbolic music generation models predominantly
adopt autoregressive or hierarchical autoregressive architectures, modelling
symbolic music as a sequence of attribute tokens with unidirectional temporal
dependencies, under the assumption of a fixed, strict dependency structure
among these attributes. However, we observe that using different attributes as
the initial token in these models leads to comparable performance. This
suggests that the attributes of a musical note are, in essence, a concurrent
and unordered set, rather than a temporally dependent sequence. Based on this
insight, we introduce Amadeus, a novel symbolic music generation framework.
Amadeus adopts a two-level architecture: an autoregressive model for note
sequences and a bidirectional discrete diffusion model for attributes. To
enhance performance, we propose Music Latent Space Discriminability Enhancement
Strategy(MLSDES), incorporating contrastive learning constraints that amplify
discriminability of intermediate music representations. The Conditional
Information Enhancement Module (CIEM) simultaneously strengthens note latent
vector representation via attention mechanisms, enabling more precise note
decoding. We conduct extensive experiments on unconditional and
text-conditioned generation tasks. Amadeus significantly outperforms SOTA
models across multiple metrics while achieving at least 4$\times$ speed-up.
Furthermore, we demonstrate training-free, fine-grained note attribute control
feasibility using our model. To explore the upper performance bound of the
Amadeus architecture, we compile the largest open-source symbolic music dataset
to date, AMD (Amadeus MIDI Dataset), supporting both pre-training and
fine-tuning.

</details>


### [246] [Unified Multi-task Learning for Voice-Based Detection of Diverse Clinical Conditions](https://arxiv.org/abs/2508.20717)
*Ran Piao,Yuan Lu,Hareld Kemps,Tong Xia,Aaqib Saeed*

Main category: cs.SD

TL;DR: 提出MARVEL框架，用声学特征同时检测9种疾病，表现出色，为语音诊断奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有语音健康评估方法多关注单一疾病，未充分利用语音信息，需开发能检测多种疾病的方法。

Method: 提出MARVEL隐私保护多任务学习框架，采用双分支架构，有专用编码器和任务特定头部，共享声学主干。

Result: 在大规模数据集上，整体AUROC达0.78，神经疾病表现优异，超单模态基线5 - 19%，7项任务超自监督模型，学习表征与既定声学特征有意义相似。

Conclusion: 单一统一模型可有效筛查多种疾病，为资源有限和远程医疗环境中的语音诊断奠定基础。

Abstract: Voice-based health assessment offers unprecedented opportunities for
scalable, non-invasive disease screening, yet existing approaches typically
focus on single conditions and fail to leverage the rich, multi-faceted
information embedded in speech. We present MARVEL (Multi-task Acoustic
Representations for Voice-based Health Analysis), a privacy-conscious multitask
learning framework that simultaneously detects nine distinct neurological,
respiratory, and voice disorders using only derived acoustic features,
eliminating the need for raw audio transmission. Our dual-branch architecture
employs specialized encoders with task-specific heads sharing a common acoustic
backbone, enabling effective cross-condition knowledge transfer. Evaluated on
the large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC
of 0.78, with exceptional performance on neurological disorders (AUROC = 0.89),
particularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97).
Our framework consistently outperforms single-modal baselines by 5-19% and
surpasses state-of-the-art self-supervised models on 7 of 9 tasks, while
correlation analysis reveals that the learned representations exhibit
meaningful similarities with established acoustic features, indicating that the
model's internal representations are consistent with clinically recognized
acoustic patterns. By demonstrating that a single unified model can effectively
screen for diverse conditions, this work establishes a foundation for
deployable voice-based diagnostics in resource-constrained and remote
healthcare settings.

</details>


### [247] [OLMoASR: Open Models and Data for Training Robust Speech Recognition Models](https://arxiv.org/abs/2508.20869)
*Huong Ngo,Matt Deitke,Martijn Bartelds,Sarah Pratt,Josh Gardner,Matt Jordan,Ludwig Schmidt*

Main category: cs.SD

TL;DR: 本文提出大规模数据集OLMoASR - Pool和系列模型OLMoASR用于零样本语音识别研究，模型表现与Whisper相当且代码等将公开。


<details>
  <summary>Details</summary>
Motivation: 训练数据规模和质量提升在语音识别中的影响未充分探索，需研究和开发鲁棒的零样本语音识别模型。

Method: 创建含300万小时英语音频和1700万条转录的OLMoASR - Pool，设计文本启发式过滤器去除低质量数据得到OLMoASR - Mix，用其训练不同参数规模的OLMoASR模型。

Result: OLMoASR在短和长语音识别基准测试中平均表现与OpenAI的Whisper相当，OLMoASR - medium.en的WER与Whisper - medium.en相近。

Conclusion: 公开OLMoASR - Pool、OLMoASR模型及相关代码以推动鲁棒语音处理研究。

Abstract: Improvements in training data scale and quality have led to significant
advances, yet its influence in speech recognition remains underexplored. In
this paper, we present a large-scale dataset, OLMoASR-Pool, and series of
models, OLMoASR, to study and develop robust zero-shot speech recognition
models. Beginning from OLMoASR-Pool, a collection of 3M hours of English audio
and 17M transcripts, we design text heuristic filters to remove low-quality or
mistranscribed data. Our curation pipeline produces a new dataset containing 1M
hours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We use
OLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M
(tiny.en) to 1.5B (large.en) parameters. Across all model scales, OLMoASR
achieves comparable average performance to OpenAI's Whisper on short and
long-form speech recognition benchmarks. Notably, OLMoASR-medium.en attains a
12.8\% and 11.0\% word error rate (WER) that is on par with Whisper's largest
English-only model Whisper-medium.en's 12.4\% and 10.5\% WER for short and
long-form recognition respectively (at equivalent parameter count).
OLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code will
be made publicly available to further research on robust speech processing.

</details>


### [248] [Learning Robust Spatial Representations from Binaural Audio through Feature Distillation](https://arxiv.org/abs/2508.20914)
*Holger Severin Bovbjerg,Jan Østergaard,Jesper Jensen,Shinji Watanabe,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: 本文研究无监督预训练学习双耳语音空间表征，用于到达角估计，预训练模型在有噪和混响环境表现更优。


<details>
  <summary>Details</summary>
Motivation: 深度学习在多声道音频空间表征学习的应用未充分探索，需无监督方法学习双耳语音空间表征。

Method: 基于特征蒸馏预训练，用干净双耳语音样本计算空间特征作预测标签，用神经网络从增强语音预测，预训练后用编码器权重初始化到达角估计模型并微调。

Result: 预训练模型在有噪和混响环境微调后用于到达角估计，比全监督模型和经典信号处理方法性能更好。

Conclusion: 基于特征蒸馏的预训练方法能有效学习双耳语音空间表征，提升到达角估计性能。

Abstract: Recently, deep representation learning has shown strong performance in
multiple audio tasks. However, its use for learning spatial representations
from multichannel audio is underexplored. We investigate the use of a
pretraining stage based on feature distillation to learn a robust spatial
representation of binaural speech without the need for data labels. In this
framework, spatial features are computed from clean binaural speech samples to
form prediction labels. These clean features are then predicted from
corresponding augmented speech using a neural network. After pretraining, we
throw away the spatial feature predictor and use the learned encoder weights to
initialize a DoA estimation model which we fine-tune for DoA estimation. Our
experiments demonstrate that the pretrained models show improved performance in
noisy and reverberant environments after fine-tuning for direction-of-arrival
estimation, when compared to fully supervised models and classic signal
processing methods.

</details>


### [249] [Speech Emotion Recognition via Entropy-Aware Score Selection](https://arxiv.org/abs/2508.20796)
*ChenYi Chua,JunKai Wong,Chengxin Chen,Xiaoxiao Miao*

Main category: cs.SD

TL;DR: 提出基于熵感知分数选择的多模态语音情感识别框架，在数据集上效果优于单模态系统。


<details>
  <summary>Details</summary>
Motivation: 解决传统单模态语音情感识别系统的局限性，结合语音和文本预测进行多模态识别。

Method: 集成基于wav2vec2.0的声学模型和使用RoBERTa - XLM的情感分析模型，通过Whisper - large - v3生成转录，采用基于熵和变熵阈值的后期分数融合方法及情感映射策略。

Result: 在IEMOCAP和MSP - IMPROV数据集上，该方法比传统单模态系统有实际且可靠的提升。

Conclusion: 所提多模态框架在语音情感识别方面具有实用性和可靠性，优于传统单模态系统。

Abstract: In this paper, we propose a multimodal framework for speech emotion
recognition that leverages entropy-aware score selection to combine speech and
textual predictions. The proposed method integrates a primary pipeline that
consists of an acoustic model based on wav2vec2.0 and a secondary pipeline that
consists of a sentiment analysis model using RoBERTa-XLM, with transcriptions
generated via Whisper-large-v3. We propose a late score fusion approach based
on entropy and varentropy thresholds to overcome the confidence constraints of
primary pipeline predictions. A sentiment mapping strategy translates three
sentiment categories into four target emotion classes, enabling coherent
integration of multimodal predictions. The results on the IEMOCAP and
MSP-IMPROV datasets show that the proposed method offers a practical and
reliable enhancement over traditional single-modality systems.

</details>


### [250] [WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language Models via Marine Mammal Vocalizations](https://arxiv.org/abs/2508.20976)
*Jaeyeon Kim,Heeseung Yun,Sang Hoon Woo,Chao-Han Huck Yang,Gunhee Kim*

Main category: cs.SD

TL;DR: 引入World - of - Whale基准（WoW - Bench）评估大音频语言模型（LALMs）低级听觉感知和认知能力，实验表明LALMs性能远低于人类。


<details>
  <summary>Details</summary>
Motivation: 现有LALMs在低级聆听能力（如音高和时长检测）方面研究不足，而低级聆听对现实任务很关键，为填补此差距开展研究。

Method: 引入WoW - Bench，包括感知基准和受布鲁姆分类法启发的认知基准，认知基准中还引入干扰问题。

Result: 实验显示现有最先进的LALMs性能远低于人类水平。

Conclusion: LALMs需要更强的听觉基础。

Abstract: Large audio language models (LALMs) extend language understanding into the
auditory domain, yet their ability to perform low-level listening, such as
pitch and duration detection, remains underexplored. However, low-level
listening is critical for real-world, out-of-distribution tasks where models
must reason about unfamiliar sounds based on fine-grained acoustic cues. To
address this gap, we introduce the World-of-Whale benchmark (WoW-Bench) to
evaluate low-level auditory perception and cognition using marine mammal
vocalizations. WoW-bench is composed of a Perception benchmark for categorizing
novel sounds and a Cognition benchmark, inspired by Bloom's taxonomy, to assess
the abilities to remember, understand, apply, and analyze sound events. For the
Cognition benchmark, we additionally introduce distractor questions to evaluate
whether models are truly solving problems through listening rather than relying
on other heuristics. Experiments with state-of-the-art LALMs show performance
far below human levels, indicating a need for stronger auditory grounding in
LALMs.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [251] [Flexible metadata harvesting for ecology using large language models](https://arxiv.org/abs/2508.20115)
*Zehao Lu,Thijs L van der Plas,Parinaz Rashidi,W Daniel Kissling,Ioannis N Athanasiadis*

Main category: cs.DL

TL;DR: 开发基于大语言模型的元数据采集器，可提取和统一数据集元数据，还能识别数据集间的联系，用于本体创建或图查询。


<details>
  <summary>Details</summary>
Motivation: 为克服研究人员在多样生态和环境数据平台中寻找合适数据集时面临的元数据可用性和标准不一的障碍。

Method: 开发基于大语言模型的元数据采集器，利用LLM后处理协议提取元数据，通过计算嵌入相似度和统一元数据格式识别数据集间联系。

Result: 工具能以相同的准确性提取结构化和非结构化元数据，可灵活链接不同数据集的元数据。

Conclusion: 该工具可用于本体创建或基于图的查询，帮助在虚拟研究环境中找到相关生态和环境数据集。

Abstract: Large, open datasets can accelerate ecological research, particularly by
enabling researchers to develop new insights by reusing datasets from multiple
sources. However, to find the most suitable datasets to combine and integrate,
researchers must navigate diverse ecological and environmental data provider
platforms with varying metadata availability and standards. To overcome this
obstacle, we have developed a large language model (LLM)-based metadata
harvester that flexibly extracts metadata from any dataset's landing page, and
converts these to a user-defined, unified format using existing metadata
standards. We validate that our tool is able to extract both structured and
unstructured metadata with equal accuracy, aided by our LLM post-processing
protocol. Furthermore, we utilise LLMs to identify links between datasets, both
by calculating embedding similarity and by unifying the formats of extracted
metadata to enable rule-based processing. Our tool, which flexibly links the
metadata of different datasets, can therefore be used for ontology creation or
graph-based queries, for example, to find relevant ecological and environmental
datasets in a virtual research environment.

</details>


### [252] [Is Artificial Intelligence Reshaping the Landscape of the International Academic Community of Geosciences?](https://arxiv.org/abs/2508.20117)
*Liang Li,Yuntian Li,Wenxin Zhao,Shan Ye,Yun Lu*

Main category: cs.DL

TL;DR: 通过文献计量分析和主题建模发现，AI正积极改变地球科学研究，发展中国家地球科学家在AI4S范式中更受关注，AI也改善了国际合作格局。


<details>
  <summary>Details</summary>
Motivation: 探究人工智能对地球科学研究的影响。

Method: 文献计量分析和主题建模。

Result: 人工智能积极改变地球科学研究，发展中国家地球科学家在AI4S范式中可见度提升，AI改善了地球科学相关研究的国际合作格局。

Conclusion: 人工智能对地球科学研究有积极的变革作用。

Abstract: Through bibliometric analysis and topic modeling, we find that artificial
intelligence (AI) is positively transforming geosciences research, with a
notable increase in AI-related scientific output in recent years. We are
encouraged to observe that earth scientists from developing countries have
gained better visibility in the recent AI for Science (AI4S) paradigm and that
AI is also improving the landscape of international collaboration in
geoscience-related research.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [253] [Multi-Objective Optimization of ReRAM Crossbars for Robust DNN Inferencing under Stochastic Noise](https://arxiv.org/abs/2109.05437)
*Xiaoxuan Yang,Syrine Belakaria,Biresh Kumar Joardar,Huanrui Yang,Janardhan Rao Doppa,Partha Pratim Pande,Krishnendu Chakrabarty,Hai Li*

Main category: cs.ET

TL;DR: 提出基于ReRAM的硬件加速器设计与优化方法，含ReSNA训练法与CF - MESMO算法，实验证明算法有效且能提升推理精度、降低计算成本。


<details>
  <summary>Details</summary>
Motivation: ReRAM随机噪声会降低DNN推理精度，需设计高性能、高效的硬件加速器实现鲁棒DNN推理。

Method: 提出随机噪声感知训练方法ReSNA，及信息论算法CF - MESMO，利用连续保真度评估权衡精度与成本。

Result: 实验表明算法能高效找到高质量Pareto前沿，ReSNA提升推理精度，CF - MESMO降低计算成本。

Conclusion: 所提算法能有效应对ReRAM随机噪声问题，提升DNN推理性能。

Abstract: Resistive random-access memory (ReRAM) is a promising technology for
designing hardware accelerators for deep neural network (DNN) inferencing.
However, stochastic noise in ReRAM crossbars can degrade the DNN inferencing
accuracy. We propose the design and optimization of a high-performance,
area-and energy-efficient ReRAM-based hardware accelerator to achieve robust
DNN inferencing in the presence of stochastic noise. We make two key technical
contributions. First, we propose a stochastic-noise-aware training method,
referred to as ReSNA, to improve the accuracy of DNN inferencing on ReRAM
crossbars with stochastic noise. Second, we propose an information-theoretic
algorithm, referred to as CF-MESMO, to identify the Pareto set of solutions to
trade-off multiple objectives, including inferencing accuracy, area overhead,
execution time, and energy consumption. The main challenge in this context is
that executing the ReSNA method to evaluate each candidate ReRAM design is
prohibitive. To address this challenge, we utilize the continuous-fidelity
evaluation of ReRAM designs associated with prohibitive high computation cost
by varying the number of training epochs to trade-off accuracy and cost.
CF-MESMO iteratively selects the candidate ReRAM design and fidelity pair that
maximizes the information gained per unit computation cost about the optimal
Pareto front. Our experiments on benchmark DNNs show that the proposed
algorithms efficiently uncover high-quality Pareto fronts. On average, ReSNA
achieves 2.57% inferencing accuracy improvement for ResNet20 on the CIFAR-10
dataset with respect to the baseline configuration. Moreover, CF-MESMO
algorithm achieves 90.91% reduction in computation cost compared to the popular
multi-objective optimization algorithm NSGA-II to reach the best solution from
NSGA-II.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [254] [OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models](https://arxiv.org/abs/2508.21061)
*Adam Coscia,Shunan Guo,Eunyee Koh,Alex Endert*

Main category: cs.HC

TL;DR: 提出OnGoal界面帮助用户管理大语言模型对话目标进度，研究显示其能提升用户对话表现并带来设计启示。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型多轮对话变长变复杂，需帮助用户更好评估和回顾对话目标进展。

Method: 开发OnGoal界面，通过大语言模型辅助评估提供目标对齐实时反馈等；开展有20名参与者的写作任务研究，与无目标跟踪的基线聊天界面对比。

Result: 使用OnGoal的参与者实现目标耗时少、省力，还探索新提示策略克服误解。

Conclusion: 跟踪和可视化目标可增强大语言模型对话的参与度和恢复力，为未来大语言模型聊天界面设计提供了改善目标沟通、减轻认知负担等启示。

Abstract: As multi-turn dialogues with large language models (LLMs) grow longer and
more complex, how can users better evaluate and review progress on their
conversational goals? We present OnGoal, an LLM chat interface that helps users
better manage goal progress. OnGoal provides real-time feedback on goal
alignment through LLM-assisted evaluation, explanations for evaluation results
with examples, and overviews of goal progression over time, enabling users to
navigate complex dialogues more effectively. Through a study with 20
participants on a writing task, we evaluate OnGoal against a baseline chat
interface without goal tracking. Using OnGoal, participants spent less time and
effort to achieve their goals while exploring new prompting strategies to
overcome miscommunication, suggesting tracking and visualizing goals can
enhance engagement and resilience in LLM dialogues. Our findings inspired
design implications for future LLM chat interfaces that improve goal
communication, reduce cognitive load, enhance interactivity, and enable
feedback to improve LLM performance.

</details>


### [255] [Understanding, Protecting, and Augmenting Human Cognition with Generative AI: A Synthesis of the CHI 2025 Tools for Thought Workshop](https://arxiv.org/abs/2508.21036)
*Lev Tankelevitch,Elena L. Glassman,Jessica He,Aniket Kittur,Mina Lee,Srishti Palani,Advait Sarkar,Gonzalo Ramos,Yvonne Rogers,Hari Subramonyam*

Main category: cs.HC

TL;DR: CHI 2025研讨会探讨生成式AI对人类认知的影响及研究设计机会，综合材料以推动多学科研究。


<details>
  <summary>Details</summary>
Motivation: 生成式AI给人类认知带来风险和机遇，需探讨人类认知变化、AI增强认知的机会及相关理论工具。

Method: 举办CHI 2025研讨会，汇聚多领域人员和论文进行讨论。

Result: 综合研讨会材料开始规划研究和设计机会空间。

Conclusion: 催化围绕该紧迫研究领域的多学科社区。

Abstract: Generative AI (GenAI) radically expands the scope and capability of
automation for work, education, and everyday tasks, a transformation posing
both risks and opportunities for human cognition. How will human cognition
change, and what opportunities are there for GenAI to augment it? Which
theories, metrics, and other tools are needed to address these questions? The
CHI 2025 workshop on Tools for Thought aimed to bridge an emerging science of
how the use of GenAI affects human thought, from metacognition to critical
thinking, memory, and creativity, with an emerging design practice for building
GenAI tools that both protect and augment human thought. Fifty-six researchers,
designers, and thinkers from across disciplines as well as industry and
academia, along with 34 papers and portfolios, seeded a day of discussion,
ideation, and community-building. We synthesize this material here to begin
mapping the space of research and design opportunities and to catalyze a
multidisciplinary community around this pressing area of research.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [256] [Spectral Gaps with Quantum Counting Queries and Oblivious State Preparation](https://arxiv.org/abs/2508.21002)
*Almudena Carrera Vazquez,Aleksandros Sobczyk*

Main category: quant-ph

TL;DR: 提出用对数数量量子比特近似厄米矩阵第k个谱间隙和中点值的量子算法，分析复杂度并给出黑盒模型查询下界。


<details>
  <summary>Details</summary>
Motivation: 近似厄米矩阵的第k个谱间隙和中点值在科学与工程领域有众多应用。

Method: 提出量子算法，关键步骤是准备合适随机初始态以高效计算小于阈值的特征值数量。

Result: 在QRAM模型中，算法总复杂度有界，大谱间隙时比经典算法有加速；黑盒模型中给出判定二元矩阵谱间隙存在性的查询下界。

Conclusion: 提出的量子算法在近似谱间隙和中点值问题上有一定优势，尤其在大谱间隙情况。

Abstract: Approximating the $k$-th spectral gap $\Delta_k=|\lambda_k-\lambda_{k+1}|$
and the corresponding midpoint $\mu_k=\frac{\lambda_k+\lambda_{k+1}}{2}$ of an
$N\times N$ Hermitian matrix with eigenvalues
$\lambda_1\geq\lambda_2\geq\ldots\geq\lambda_N$, is an important special case
of the eigenproblem with numerous applications in science and engineering. In
this work, we present a quantum algorithm which approximates these values up to
additive error $\epsilon\Delta_k$ using a logarithmic number of qubits.
Notably, in the QRAM model, its total complexity (queries and gates) is bounded
by $O\left( \frac{N^2}{\epsilon^{2}\Delta_k^2}\mathrm{polylog}\left(
N,\frac{1}{\Delta_k},\frac{1}{\epsilon},\frac{1}{\delta}\right)\right)$, where
$\epsilon,\delta\in(0,1)$ are the accuracy and the success probability,
respectively. For large gaps $\Delta_k$, this provides a speed-up against the
best-known complexities of classical algorithms, namely, $O \left(
N^{\omega}\mathrm{polylog} \left(
N,\frac{1}{\Delta_k},\frac{1}{\epsilon}\right)\right)$, where $\omega\lesssim
2.371$ is the matrix multiplication exponent. A key technical step in the
analysis is the preparation of a suitable random initial state, which
ultimately allows us to efficiently count the number of eigenvalues that are
smaller than a threshold, while maintaining a quadratic complexity in $N$. In
the black-box access model, we also report an $\Omega(N^2)$ query lower bound
for deciding the existence of a spectral gap in a binary (albeit non-symmetric)
matrix.

</details>


### [257] [Differentially Private Federated Quantum Learning via Quantum Noise](https://arxiv.org/abs/2508.20310)
*Atit Pokharel,Ratun Rahman,Shaba Shaon,Thomas Morris,Dinh C. Nguyen*

Main category: quant-ph

TL;DR: 本文提出利用量子噪声的差分隐私机制保护量子联邦学习（QFL）中模型信息，模拟验证其有效性和鲁棒性，展示了隐私与鲁棒性的权衡。


<details>
  <summary>Details</summary>
Motivation: QFL易受攻击，在有噪声的中等规模量子（NISQ）设备背景下，需利用量子噪声实施差分隐私以保护模型信息。

Method: 提出利用量子噪声的差分隐私机制，通过测量次数和去极化信道强度调整噪声方差以达到NISQ约束下的差分隐私水平。

Result: 模拟展示了差分隐私预算与噪声参数的关系，以及安全与训练精度的权衡，验证了框架对对抗攻击的鲁棒性。

Conclusion: 揭示了隐私和鲁棒性的可调权衡，为NISQ设备上的安全QFL提供有效解决方案，有可靠量子计算应用潜力。

Abstract: Quantum federated learning (QFL) enables collaborative training of quantum
machine learning (QML) models across distributed quantum devices without raw
data exchange. However, QFL remains vulnerable to adversarial attacks, where
shared QML model updates can be exploited to undermine information privacy. In
the context of noisy intermediate-scale quantum (NISQ) devices, a key question
arises: How can inherent quantum noise be leveraged to enforce differential
privacy (DP) and protect model information during training and communication?
This paper explores a novel DP mechanism that harnesses quantum noise to
safeguard quantum models throughout the QFL process. By tuning noise variance
through measurement shots and depolarizing channel strength, our approach
achieves desired DP levels tailored to NISQ constraints. Simulations
demonstrate the framework's effectiveness by examining the relationship between
differential privacy budget and noise parameters, as well as the trade-off
between security and training accuracy. Additionally, we demonstrate the
framework's robustness against an adversarial attack designed to compromise
model performance using adversarial examples, with evaluations based on
critical metrics such as accuracy on adversarial examples, confidence scores
for correct predictions, and attack success rates. The results reveal a tunable
trade-off between privacy and robustness, providing an efficient solution for
secure QFL on NISQ devices with significant potential for reliable quantum
computing applications.

</details>


### [258] [Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant](https://arxiv.org/abs/2508.20907)
*Nicolas Dupuis,Adarsh Tiwari,Youssef Mroueh,David Kremer,Ismael Faro,Juan Cruz-Benito*

Main category: quant-ph

TL;DR: 探索用大语言模型后训练技术辅助编写Qiskit代码，引入量子验证方法，开发合成数据管道，用DPO和GRPO训练模型，最佳模型在Qiskit - HumanEval - hard基准上超越开源基线。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型后训练技术辅助编写Qiskit代码，确保代码在量子硬件上的质量和可执行性。

Method: 引入量子验证方法，开发合成数据管道生成量子问题 - 单元测试对，用DPO对齐大语言模型，用GRPO结合量子硬件提供的可验证奖励训练模型。

Result: 结合DPO和GRPO的最佳模型在Qiskit - HumanEval - hard基准上超越最强开源基线。

Conclusion: 采用的量子验证、合成数据管道及DPO和GRPO训练方法有效，能提升大语言模型辅助编写Qiskit代码的性能。

Abstract: Qiskit is an open-source quantum computing framework that allows users to
design, simulate, and run quantum circuits on real quantum hardware. We explore
post-training techniques for LLMs to assist in writing Qiskit code. We
introduce quantum verification as an effective method for ensuring code quality
and executability on quantum hardware. To support this, we developed a
synthetic data pipeline that generates quantum problem-unit test pairs and used
it to create preference data for aligning LLMs with DPO. Additionally, we
trained models using GRPO, leveraging quantum-verifiable rewards provided by
the quantum hardware. Our best-performing model, combining DPO and GRPO,
surpasses the strongest open-source baselines on the challenging
Qiskit-HumanEval-hard benchmark.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [259] [High performance visualization for Astronomy and Cosmology: the VisIVO's pathway toward Exascale systems](https://arxiv.org/abs/2508.20603)
*Eva Sciacca,Nicola Tuccari,Fabio Vitello,Valentina Cesare*

Main category: astro-ph.IM

TL;DR: 现代天文学产生PB级数据，阻碍存储等，VisIVO可分析数据，本文计划适配其用于高性能可视化，介绍发展方向和执行策略。


<details>
  <summary>Details</summary>
Motivation: 现代天文学和天体物理学产生的PB级数据阻碍存储、访问和分析，需开发新软件工具，要让VisIVO适配高性能可视化。

Method: 利用容器化和虚拟化技术，通过SPACE卓越中心、H2020 EUPEX项目和ICSC国家研究中心的合作。

Result: 未提及具体结果。

Conclusion: 要提升VisIVO模块化应用的可移植性、促进可重复性和可维护性、灵活利用异构HPC资源、减少数据移动开销和提高I/O性能。

Abstract: Petabyte-scale data volumes are generated by observations and simulations in
modern astronomy and astrophysics. Storage, access, and data analysis are
significantly hampered by such data volumes and are leading to the development
of a new generation of software tools. The Visualization Interface for the
Virtual Observatory (VisIVO) has been designed, developed and maintained by
INAF since 2005 to perform multi-dimensional data analysis and knowledge
discovery in multivariate astrophysical datasets. Utilizing containerization
and virtualization technologies, VisIVO has already been used to exploit
distributed computing infrastructures including the European Open Science Cloud
(EOSC).
  We intend to adapt VisIVO solutions for high performance visualization of
data generated on the (pre-)Exascale systems by HPC applications in
Astrophysics and Cosmology (A\&C), including GADGET (GAlaxies with Dark matter
and Gas) and PLUTO simulations, thanks to the collaboration within the SPACE
Center of Excellence, the H2020 EUPEX Project, and the ICSC National Research
Centre. In this work, we outline the evolution's course as well as the
execution strategies designed to achieve the following goals: enhance the
portability of the VisIVO modular applications and their resource requirements;
foster reproducibility and maintainability; take advantage of a more flexible
resource exploitation over heterogeneous HPC facilities; and, finally, minimize
data-movement overheads and improve I/O performances.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [260] [Organisational justice moderates the link between leadership, work engagement and innovation work behaviour](https://arxiv.org/abs/2508.20320)
*Rahmat Sabuhari,Rusman Soleman,Marwan Man Soleman,Johan Fahri,Muhammad Rachmat*

Main category: econ.GN

TL;DR: 研究印尼东部地区公务员创新工作行为，发现变革型领导和工作投入直接影响该行为，组织公正无直接影响和调节作用。


<details>
  <summary>Details</summary>
Motivation: 国家规定要求印尼东部地区公务员展现创新工作行为，现有研究未充分探索变革型领导、工作投入和组织公正的影响及调节作用。

Method: 在印尼东部北马鲁古省多个地区机构组织进行研究，随机抽取500人，用结构方程模型 - 偏最小二乘法回归分析检验因果关系和调节效应。

Result: 变革型领导和工作投入直接影响创新工作行为，组织公正无直接影响，也不起调节作用。

Conclusion: 研究对印尼地方政府组织有理论和实践贡献，强调领导和投入对提升工作创新行为的重要性，拓展和完善了社会交换理论。

Abstract: Regional civil servants in Indonesia, especially in the eastern regions, are
now required to exhibit innovative work behavior (IWB) as mandated by national
regulations. While transformational leadership (TFL) and work engagement (WE)
are believed to foster such behavior, the impact and the moderating role of
organizational justice (OJ) remain underexplored. This study addresses that gap
by examining how TFL and WE contribute to IWB development within the context of
public sector work, with OJ potentially moderating these relationships. This
study aimed to determine the direct influence of TFL, WE, and OJ on IWB and to
analyze the role of OJ as a moderating variable. This research was conducted in
the eastern region of Indonesia, especially North Maluku Province, in various
regional apparatus organizations. and a random sample of 500 people was taken.
This study used Structural Equation Modeling-Partial Least Squares (SEM-PLS)
regression analysis as a data analysis method to test the causal relationship
and moderation effects of each hypothesis. The study findings reveal that while
TFL and WE directly influence IWB, OJ neither has a direct effect nor serves as
a moderating variable. These findings offer theoretical and practical
contributions to local government organizations in Indonesia, emphasizing the
importance of leadership and engagement in enhancing innovative behavior at
work. This study extends and enhances the development of social exchange theory
based on civil servants' IWB. Furthermore, this study expands the theoretical
perspective by examining how TFL, WE, and OJ influence innovative work behavior
practices to improve employee innovative behaviour. Finally, this study
empirically examines the moderating impact of organizational justice on the
relationship between TFL and WE on IWB.

</details>


### [261] [How Big Data Dilutes Cognitive Resources, Interferes with Rational Decision-making and Affects Wealth Distribution ?](https://arxiv.org/abs/2508.20435)
*Yongheng Hu*

Main category: econ.GN

TL;DR: 探讨大数据交互下消费与效用获取，提出大数据稀释机制影响认知资源理论，构建消费调整权重函数应用于模型，分析金融摩擦与有效消费权重对企业财富的影响并尝试完善'卢卡斯批判'。


<details>
  <summary>Details</summary>
Motivation: 研究大数据下消费能否全部转化为效用，分析CARR效用函数的局限性。

Method: 理论与实证分析，构建消费调整权重函数，运用平均场博弈求解模型。

Result: 低金融摩擦增加企业平均财富但加剧不平等；有效消费权重增加时企业平均财富上升，财富不平等呈U型趋势，权重接近0.5时不平等最低。

Conclusion: 尝试根据认知资源作为禀赋为完善'卢卡斯批判'提供新的补充假设。

Abstract: Big data has exponentially dilated consumption demand and speed, but can they
all convert to utility? We argue about the measures of consumption and utility
acquisition in CARR utility function under the condition of big data
interaction, we indicate its weakness, i.e., irrational consumption does not
lead to the acquisition of utility. We consider that big data, which is
different from macro and micro economic signals, formed by general information
entropy, affects agents' rational cognition, which makes a part of their
consumption ineffective. We preliminarily propose the theory that how dilution
mechanism driven by big data will affect agents' cognitive resources. Based on
theoretical and empirical analysis, we construct the Consumption Adjustment
Weight Function (CAWF) of agents interacting with big data and further apply it
to a model of firm wealth distribution with financial frictions, we get
analytical solutions according to the Mean Field Game (MFG) and find: Lower
financial friction increases the average wealth of firms but also leads to
greater wealth inequality. When agents convert effective consumption into
utility, which is a weight of total consumption, the average wealth of firms
increases with the weight increasing. Meanwhile, wealth inequality follows a
U-shaped trend, and it will be the lowest level when the weight approaches to
0.5. In conclusion, we try to provide a new complementary hypothesis to refine
the 'Lucas Critique' according to the cognitive resources as endowments
involved in the decision-making of agents.

</details>


### [262] [Patent citations and patent importance](https://arxiv.org/abs/2508.20503)
*Gaetan de Rassenfosse*

Main category: econ.GN

TL;DR: 文章回顾专利引用作为发明重要性代理的实证证据，指出引用计数与价值各维度正相关但仍是有噪声的指标。


<details>
  <summary>Details</summary>
Motivation: 探讨专利引用作为发明重要性代理的有效性。

Method: 区分技术价值、私人经济价值和社会价值，调查使用专家评级、市场数据、续展记录和补偿报告的验证研究。

Result: 发现引用计数与价值各维度正相关，但仍是有噪声的指标。

Conclusion: 专利引用计数与价值相关，但不能作为确定发明重要性的明确指标。

Abstract: This article reviews the empirical evidence on the use of patent citations as
a proxy for invention importance. It distinguishes between technical merit,
private economic value, and social value, and surveys validation studies using
expert ratings, market data, renewal records, and compensation reports. The
findings confirm that while the count of citations is positively associated
with various dimensions of value, it remains a noisy indicator -- correlated
but far from definitive.

</details>


### [263] [The Trouble with Rational Expectations in Heterogeneous Agent Models: A Challenge for Macroeconomics](https://arxiv.org/abs/2508.20571)
*Benjamin Moll*

Main category: econ.GN

TL;DR: 论文指出异质主体宏观经济学中理性预期假设不现实需替换，给出替代标准并讨论方向。


<details>
  <summary>Details</summary>
Motivation: 理性预期假设导致维度灾难，限制异质主体方法适用性，需寻找替代。

Method: 分析理性预期假设问题，提出替代方法的三条标准并讨论可能方向。

Result: 提出计算可行性、与实证一致、一定程度免疫卢卡斯批判三条替代标准。

Conclusion: 理性预期假设应被替代，讨论的几个方向有潜力。

Abstract: The thesis of this essay is that, in heterogeneous agent macroeconomics, the
assumption of rational expectations about equilibrium prices is unrealistic and
should be replaced. Rational expectations imply that decision makers forecast
equilibrium prices like interest rates by forecasting cross-sectional
distributions. This leads to an extreme version of the curse of dimensionality:
dynamic programming problems in which the entire distribution is a state
variable ("Master equation" a.k.a. "Monster equation"). Frontier computational
methods struggle with these infinite-dimensional Bellman equations, making it
implausible that real-world agents solve the associated decision problems.
These difficulties also limit the applicability of the heterogeneous-agent
approach to central questions in macroeconomics -- those involving aggregate
risk and non-linearities such as financial crises. This troublesome feature of
the rational expectations assumption poses a challenge: what should replace it?
I outline three criteria for alternative approaches: (1) computational
tractability, (2) consistency with empirical evidence, and (3) (some) immunity
to the Lucas critique. I then discuss several promising directions, including
temporary equilibrium approaches, incorporating survey expectations,
least-squares learning, and reinforcement learning.

</details>


### [264] [The Aftermath of Peace: The Impact of the FARC's Ceasefire on Forced Displacement in Colombia](https://arxiv.org/abs/2508.20662)
*Pedro Albarrán,Antonio Robles,Anna Sanz-de-Galdeano*

Main category: econ.GN

TL;DR: 研究考察哥伦比亚革命武装力量停火对国内强迫流离失所的影响，发现受影响地区严重流离失所事件显著减少。


<details>
  <summary>Details</summary>
Motivation: 哥伦比亚长期冲突致大量国内强迫流离失所，研究停火对其影响。

Method: 采用双重差分策略，利用停火时间和各市冲突前革命武装力量分布情况。

Result: 受影响地区严重流离失所事件大幅减少，效果逐渐显现且长期持续。

Conclusion: 强调稳定和有效执行和平协议对缓解国内强迫流离失所及其深远影响的重要性。

Abstract: Colombia's prolonged conflict has made the country one of the most affected
by forced internal displacement (FID) in the world. This study examines the
impact of the FARC's 2014 unilateral and permanent ceasefire on FID. We use a
difference-in-differences strategy that exploits the timing of the ceasefire
and the pre-conflict distribution of FARC presence across municipalities.
Results show a substantial reduction in severe displacement episodes in
affected areas, with effects that emerged gradually and persisted over time.
These findings highlight the importance of stability and the effective
implementation of peace agreements in mitigating FID and its far-reaching
consequences.

</details>
