<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 22]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.SE](#cs.SE) [Total: 10]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 6]
- [stat.CO](#stat.CO) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [econ.GN](#econ.GN) [Total: 4]
- [gr-qc](#gr-qc) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CV](#cs.CV) [Total: 27]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [math.OC](#math.OC) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.CL](#cs.CL) [Total: 32]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Human Creativity and AI](https://arxiv.org/abs/2507.08001)
*Shengyi Xie*

Main category: cs.AI

TL;DR: 文章探讨科技进步下创造力哲学的再诠释，研究多领域成果，聚焦AI是否有创造力的问题。


<details>
  <summary>Details</summary>
Motivation: 在科技进步，尤其是AI技术发展背景下，探讨AI能否展现创造力。

Method: 回顾创造力哲学的历史视角，探索心理学进展对创造力研究的影响，分析创造力的各种定义，研究自然主义和认知神经科学对创造力概念的回应。

Result: 未提及。

Conclusion: 未提及。

Abstract: With the advancement of science and technology, the philosophy of creativity
has undergone significant reinterpretation. This paper investigates
contemporary research in the fields of psychology, cognitive neuroscience, and
the philosophy of creativity, particularly in the context of the development of
artificial intelligence (AI) techniques. It aims to address the central
question: Can AI exhibit creativity? The paper reviews the historical
perspectives on the philosophy of creativity and explores the influence of
psychological advancements on the study of creativity. Furthermore, it analyzes
various definitions of creativity and examines the responses of naturalism and
cognitive neuroscience to the concept of creativity.

</details>


### [2] [Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to Actions](https://arxiv.org/abs/2507.08208)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: 介绍LLM - Nash框架，该框架可研究LLM系统中策略互动，推理均衡与经典Nash结果不同。


<details>
  <summary>Details</summary>
Motivation: 经典博弈假设参与者完全理性，而本文要研究具有有限理性的参与者，捕捉其推理过程。

Method: 引入LLM - Nash框架，在提示空间上定义均衡，将行动视为LLM推理的行为输出。

Result: 通过示例表明推理均衡与经典Nash结果不同。

Conclusion: 为LLM支持的系统中的战略互动提供新基础。

Abstract: We introduce the LLM-Nash framework, a game-theoretic model where agents
select reasoning prompts to guide decision-making via Large Language Models
(LLMs). Unlike classical games that assume utility-maximizing agents with full
rationality, this framework captures bounded rationality by modeling the
reasoning process explicitly. Equilibrium is defined over the prompt space,
with actions emerging as the behavioral output of LLM inference. This approach
enables the study of cognitive constraints, mindset expressiveness, and
epistemic learning. Through illustrative examples, we show how reasoning
equilibria can diverge from classical Nash outcomes, offering a new foundation
for strategic interaction in LLM-enabled systems.

</details>


### [3] [TableReasoner: Advancing Table Reasoning Framework with Large Language Models](https://arxiv.org/abs/2507.08046)
*Sishi Xiong,Dakai Wang,Yu Zhao,Jie Zhang,Changzai Pan,Haowei He,Xiangyu Li,Wenhan Chang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: 提出用于表格问答的系统TableReasoner，结合LLM与编程，在SemEval - 2025 Task 8两子任务获第一。


<details>
  <summary>Details</summary>
Motivation: 现实表格数据存在尺寸大、列语义不完整和实体歧义等特点，导致表格问答任务面临挑战。

Method: 提出基于大语言模型和编程的表格推理框架TableReasoner；设计多步模式链接计划获得聚焦表格模式；将推理工作流集成到迭代思维架构。

Result: 系统在SemEval - 2025 Task 8的两个子任务中均获得第一名。

Conclusion: 提出的TableReasoner系统有效解决了表格问答任务面临的问题，具有良好效果。

Abstract: The paper presents our system developed for table question answering (TQA).
TQA tasks face challenges due to the characteristics of real-world tabular
data, such as large size, incomplete column semantics, and entity ambiguity. To
address these issues, we propose a large language model (LLM)-powered and
programming-based table reasoning framework, named TableReasoner. It models a
table using the schema that combines structural and semantic representations,
enabling holistic understanding and efficient processing of large tables. We
design a multi-step schema linking plan to derive a focused table schema that
retains only query-relevant information, eliminating ambiguity and alleviating
hallucinations. This focused table schema provides precise and sufficient table
details for query refinement and programming. Furthermore, we integrate the
reasoning workflow into an iterative thinking architecture, allowing
incremental cycles of thinking, reasoning and reflection. Our system achieves
first place in both subtasks of SemEval-2025 Task 8.

</details>


### [4] [A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking](https://arxiv.org/abs/2507.08207)
*Zhengye Han,Quanyan Zhu*

Main category: cs.AI

TL;DR: 本文提出动态Stackelberg博弈框架建模大语言模型越狱攻防，并提出‘Purple Agent’方案预防有害输出，分析对抗动态和降低越狱风险。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在关键应用中部署，越狱挑战使绕过安全机制问题受关注。

Method: 用动态Stackelberg博弈框架将提示 - 响应动态建模为序贯扩展式博弈，提出‘Purple Agent’方案，结合对抗探索和防御策略，利用快速搜索随机树模拟攻击轨迹并主动干预。

Result: 提供分析对抗动态的原则性方法，为降低越狱风险奠定基础。

Conclusion: 所提方法可有效应对大语言模型越狱问题，分析对抗动态和降低风险。

Abstract: As large language models (LLMs) are increasingly deployed in critical
applications, the challenge of jailbreaking, where adversaries manipulate the
models to bypass safety mechanisms, has become a significant concern. This
paper presents a dynamic Stackelberg game framework to model the interactions
between attackers and defenders in the context of LLM jailbreaking. The
framework treats the prompt-response dynamics as a sequential extensive-form
game, where the defender, as the leader, commits to a strategy while
anticipating the attacker's optimal responses. We propose a novel agentic AI
solution, the "Purple Agent," which integrates adversarial exploration and
defensive strategies using Rapidly-exploring Random Trees (RRT). The Purple
Agent actively simulates potential attack trajectories and intervenes
proactively to prevent harmful outputs. This approach offers a principled
method for analyzing adversarial dynamics and provides a foundation for
mitigating the risk of jailbreaking.

</details>


### [5] [From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration](https://arxiv.org/abs/2507.08210)
*Fryderyk Mantiuk,Hanqi Zhou,Charley M. Wu*

Main category: cs.AI

TL;DR: 本文探讨智能体探索与控制环境的驱动因素，比较两种基于模型的智能体，发现自适应探索是追求未知与可控的平衡。


<details>
  <summary>Details</summary>
Motivation: 研究智能体如何平衡好奇心（寻求知识）和能力（掌控环境），以及内部表征如何调节两者的权衡。

Method: 比较使用手工状态抽象的Tabular智能体和学习内部世界模型的Dreamer智能体。

Result: Tabular智能体中好奇心和能力以不同模式引导探索，兼顾两者可提升探索效果；Dreamer智能体显示探索和表征学习存在双向交互。

Conclusion: 自适应探索是追求未知与可控的平衡，为认知理论和高效强化学习提供见解。

Abstract: What drives an agent to explore the world while also maintaining control over
the environment? From a child at play to scientists in the lab, intelligent
agents must balance curiosity (the drive to seek knowledge) with competence
(the drive to master and control the environment). Bridging cognitive theories
of intrinsic motivation with reinforcement learning, we ask how evolving
internal representations mediate the trade-off between curiosity (novelty or
information gain) and competence (empowerment). We compare two model-based
agents using handcrafted state abstractions (Tabular) or learning an internal
world model (Dreamer). The Tabular agent shows curiosity and competence guide
exploration in distinct patterns, while prioritizing both improves exploration.
The Dreamer agent reveals a two-way interaction between exploration and
representation learning, mirroring the developmental co-evolution of curiosity
and competence. Our findings formalize adaptive exploration as a balance
between pursuing the unknown and the controllable, offering insights for
cognitive theories and efficient reinforcement learning.

</details>


### [6] [Grounding Methods for Neural-Symbolic AI](https://arxiv.org/abs/2507.08216)
*Rodrigo Castellano Ontiveros,Francesco Giannini,Marco Gori,Giuseppe Marra,Michelangelo Diligenti*

Main category: cs.AI

TL;DR: 本文提出参数化的接地方法族推广经典反向链，可控制推理机表达性与可扩展性的权衡，实验表明接地标准选择很重要。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号方法中，穷举推导存在组合爆炸问题，启发式选择缺乏理论依据且无法保证信息完整性，需更好的接地方法。

Method: 受多跳符号推理启发，提出参数化的接地方法族推广经典反向链。

Result: 可将常用接地方法作为特例，能控制推理机表达性与可扩展性的权衡。

Conclusion: 接地标准的选择通常和神经符号方法本身一样重要。

Abstract: A large class of Neural-Symbolic (NeSy) methods employs a machine learner to
process the input entities, while relying on a reasoner based on First-Order
Logic to represent and process more complex relationships among the entities. A
fundamental role for these methods is played by the process of logic grounding,
which determines the relevant substitutions for the logic rules using a
(sub)set of entities. Some NeSy methods use an exhaustive derivation of all
possible substitutions, preserving the full expressive power of the logic
knowledge. This leads to a combinatorial explosion in the number of ground
formulas to consider and, therefore, strongly limits their scalability. Other
methods rely on heuristic-based selective derivations, which are generally more
computationally efficient, but lack a justification and provide no guarantees
of preserving the information provided to and returned by the reasoner. Taking
inspiration from multi-hop symbolic reasoning, this paper proposes a
parametrized family of grounding methods generalizing classic Backward
Chaining. Different selections within this family allow us to obtain commonly
employed grounding methods as special cases, and to control the trade-off
between expressiveness and scalability of the reasoner. The experimental
results show that the selection of the grounding criterion is often as
important as the NeSy method itself.

</details>


### [7] [Quantum Federated Learning for Multimodal Data: A Modality-Agnostic Approach](https://arxiv.org/abs/2507.08217)
*Atit Pokharel,Ratun Rahman,Thomas Morris,Dinh C. Nguyen*

Main category: cs.AI

TL;DR: 本文提出用于量子联邦学习（QFL）的多模态方法及缺失模态无关（MMA）机制，仿真显示相比现有方法提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 现有QFL框架主要关注单模态系统，无法适用于涉及多模态的现实任务，且多模态QFL在训练时缺失某些模态会降低模型性能。

Method: 提出采用量子纠缠进行中间融合的多模态QFL方法，并引入MMA机制隔离未训练的量子电路。

Result: 在独立同分布（IID）数据中准确率提高6.84%，在非IID数据中提高7.25%。

Conclusion: 所提出的多模态QFL方法结合MMA机制能有效提升模型准确率。

Abstract: Quantum federated learning (QFL) has been recently introduced to enable a
distributed privacy-preserving quantum machine learning (QML) model training
across quantum processors (clients). Despite recent research efforts, existing
QFL frameworks predominantly focus on unimodal systems, limiting their
applicability to real-world tasks that often naturally involve multiple
modalities. To fill this significant gap, we present for the first time a novel
multimodal approach specifically tailored for the QFL setting with the
intermediate fusion using quantum entanglement. Furthermore, to address a major
bottleneck in multimodal QFL, where the absence of certain modalities during
training can degrade model performance, we introduce a Missing Modality
Agnostic (MMA) mechanism that isolates untrained quantum circuits, ensuring
stable training without corrupted states. Simulation results demonstrate that
the proposed multimodal QFL method with MMA yields an improvement in accuracy
of 6.84% in independent and identically distributed (IID) and 7.25% in non-IID
data distributions compared to the state-of-the-art methods.

</details>


### [8] [Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm](https://arxiv.org/abs/2507.08249)
*Bill Marino,Ari Juels*

Main category: cs.AI

TL;DR: 给AI代理访问加密货币和智能合约可能带来新的AI危害，需更多技术研究防范。


<details>
  <summary>Details</summary>
Motivation: 探讨给AI代理访问加密货币和智能合约带来的潜在危害。

Method: 先分析加密货币和智能合约能导致新危害的特性，再详细描述新危害类型。

Result: 明确了给AI代理访问加密货币和智能合约可能带来新的危害。

Conclusion: 呼吁开展更多技术研究，预防和减轻这些危害，让赋予AI代理加密货币和智能合约更安全。

Abstract: There is growing interest in giving AI agents access to cryptocurrencies as
well as to the smart contracts that transact them. But doing so, this position
paper argues, could lead to formidable new vectors of AI harm. To support this
argument, we first examine the unique properties of cryptocurrencies and smart
contracts that could lead to these new vectors of harm. Next, we describe each
of these new vectors of harm in detail. Finally, we conclude with a call for
more technical research aimed at preventing and mitigating these harms and,
thereby making it safer to endow AI agents with cryptocurrencies and smart
contracts.

</details>


### [9] [Abductive Computational Systems: Creative Abduction and Future Directions](https://arxiv.org/abs/2507.08264)
*Abhinav Sood,Kazjon Grace,Stephen Wan,Cecile Paris*

Main category: cs.AI

TL;DR: 本文回顾了溯因推理在不同领域的讨论及计算系统的应用，指出理论和计算实现均未充分解决创造性假设生成问题，并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 不同领域对溯因推理理解不同，且现有研究未充分解决创造性假设生成问题，需要进一步探索。

Method: 回顾溯因推理在认识论、科学和设计领域的讨论，分析各种计算系统对溯因推理的运用，并将计算系统拆解为组件。

Result: 理论解释和计算实现都未能充分解决创造性假设生成问题，理论框架缺乏生成创造性假设的直接模型，计算系统多采用三段论形式。

Conclusion: 确定了推进计算系统中创造性溯因推理发展的具体未来研究方向。

Abstract: Abductive reasoning, reasoning for inferring explanations for observations,
is often mentioned in scientific, design-related and artistic contexts, but its
understanding varies across these domains. This paper reviews how abductive
reasoning is discussed in epistemology, science and design, and then analyses
how various computational systems use abductive reasoning. Our analysis shows
that neither theoretical accounts nor computational implementations of
abductive reasoning adequately address generating creative hypotheses.
Theoretical frameworks do not provide a straightforward model for generating
creative abductive hypotheses, computational systems largely implement
syllogistic forms of abductive reasoning. We break down abductive computational
systems into components and conclude by identifying specific directions for
future research that could advance the state of creative abductive reasoning in
computational systems.

</details>


### [10] [Agent Safety Alignment via Reinforcement Learning](https://arxiv.org/abs/2507.08270)
*Zeyang Sha,Hanling Tian,Zhuoer Xu,Shiwen Cui,Changhua Meng,Weiqiang Wang*

Main category: cs.AI

TL;DR: 本文提出首个工具使用型大语言模型代理的统一安全对齐框架，经评估该框架可提升代理抗安全威胁能力并保持良性任务效用。


<details>
  <summary>Details</summary>
Motivation: 自主大语言模型代理使用工具带来超越传统对话滥用的新安全风险，需应对用户和工具引发的威胁。

Method: 提出统一安全对齐框架，引入三模态分类法，定义策略驱动决策模型，采用自定义沙箱环境进行强化学习。

Result: 经多基准测试评估，安全对齐代理显著提升抗安全威胁能力，同时在良性任务上保持强效用。

Conclusion: 安全和有效性可联合优化，为可信部署自主大语言模型代理奠定基础。

Abstract: The emergence of autonomous Large Language Model (LLM) agents capable of tool
usage has introduced new safety risks that go beyond traditional conversational
misuse. These agents, empowered to execute external functions, are vulnerable
to both user-initiated threats (e.g., adversarial prompts) and tool-initiated
threats (e.g., malicious outputs from compromised tools). In this paper, we
propose the first unified safety-alignment framework for tool-using agents,
enabling models to handle both channels of threat via structured reasoning and
sandboxed reinforcement learning. We introduce a tri-modal taxonomy, including
benign, malicious, and sensitive for both user prompts and tool responses, and
define a policy-driven decision model. Our framework employs a custom-designed
sandbox environment that simulates real-world tool execution and allows
fine-grained reward shaping. Through extensive evaluations on public and
self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we
demonstrate that our safety-aligned agents significantly improve resistance to
security threats while preserving strong utility on benign tasks. Our results
show that safety and effectiveness can be jointly optimized, laying the
groundwork for trustworthy deployment of autonomous LLM agents.

</details>


### [11] [M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning](https://arxiv.org/abs/2507.08306)
*Inclusion AI,:,Fudong Wang,Jiajia Liu,Jingdong Chen,Jun Zhou,Kaixiang Ji,Lixiang Ru,Qingpei Guo,Ruobing Zheng,Tianqi Li,Yi Yuan,Yifan Mao,Yuting Xiao,Ziping Ma*

Main category: cs.AI

TL;DR: 现有多模态大语言模型在动态空间交互推理有不足，本文引入M2 - Reasoning - 7B模型，结合新数据管道和动态多任务训练策略，在8个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型虽经RLVR提升推理能力，但在动态空间交互推理方面存在不足，无法满足现实应用需求。

Method: 引入M2 - Reasoning - 7B模型，采用新数据管道生成294.2K高质量数据样本，使用动态多任务训练策略，包括逐步优化和特定任务奖励。

Result: M2 - Reasoning - 7B在8个基准测试中达到新的最优水平，在一般和空间推理领域表现出色。

Conclusion: 结合精心策划的数据和先进训练方法的M2 - Reasoning - 7B模型能有效解决现有多模态大语言模型在动态空间交互推理上的问题。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs), particularly
through Reinforcement Learning with Verifiable Rewards (RLVR), have
significantly enhanced their reasoning abilities. However, a critical gap
persists: these models struggle with dynamic spatial interactions, a capability
essential for real-world applications. To bridge this gap, we introduce
M2-Reasoning-7B, a model designed to excel in both general and spatial
reasoning. Our approach integrates two key innovations: (1) a novel data
pipeline that generates 294.2K high-quality data samples (168K for cold-start
fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning
trajectories and have undergone comprehensive assessment; and (2) a dynamic
multi-task training strategy with step-wise optimization to mitigate conflicts
between data, and task-specific rewards for delivering tailored incentive
signals. This combination of curated data and advanced training allows
M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,
showcasing superior performance in both general and spatial reasoning domains.

</details>


### [12] [Multi-Agent LLMs as Ethics Advocates in AI-Based Systems](https://arxiv.org/abs/2507.08392)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.AI

TL;DR: 本文提出在多智能体大语言模型设置中引入伦理倡导智能体来生成伦理需求草案的框架，经案例评估有效果但也有可靠性问题，有望促进需求工程中伦理的广泛应用。


<details>
  <summary>Details</summary>
Motivation: 将伦理融入需求获取过程很重要，但手动获取伦理需求有时间和资源限制且优先级低，需要更好的方法。

Method: 在多智能体大语言模型设置中引入伦理倡导智能体，该智能体根据系统描述对伦理问题进行评判和提供输入。

Result: 通过两个不同场景的案例研究表明，框架能捕获研究人员在30分钟访谈中确定的大部分伦理需求，并引入了一些额外的相关需求，但也存在生成伦理需求的可靠性问题。

Conclusion: 这项工作有助于在需求工程过程中更广泛地采用伦理，最终产生更符合伦理的产品。

Abstract: Incorporating ethics into the requirement elicitation process is essential
for creating ethically aligned systems. Although eliciting manual ethics
requirements is effective, it requires diverse input from multiple
stakeholders, which can be challenging due to time and resource constraints.
Moreover, it is often given a low priority in the requirements elicitation
process. This study proposes a framework for generating ethics requirements
drafts by introducing an ethics advocate agent in a multi-agent LLM setting.
This agent critiques and provides input on ethical issues based on the system
description. The proposed framework is evaluated through two case studies from
different contexts, demonstrating that it captures the majority of ethics
requirements identified by researchers during 30-minute interviews and
introduces several additional relevant requirements. However, it also
highlights reliability issues in generating ethics requirements, emphasizing
the need for human feedback in this sensitive domain. We believe this work can
facilitate the broader adoption of ethics in the requirements engineering
process, ultimately leading to more ethically aligned products.

</details>


### [13] [Why this and not that? A Logic-based Framework for Contrastive Explanations](https://arxiv.org/abs/2507.08454)
*Tobias Geibinger,Reijo Jaakkola,Antti Kuusisto,Xinghan Liu,Miikka Vilander*

Main category: cs.AI

TL;DR: 定义对比解释相关规范问题，研究其命题逻辑性质、计算复杂度，用ASP实现并举例。


<details>
  <summary>Details</summary>
Motivation: 解决对比解释相关问题，明确‘为什么是P而不是Q’的原因。

Method: 在命题逻辑中研究定义性质，分析计算复杂度，用答案集编程实现CNF公式问题。

Result: 框架能捕捉文献中基数最小版本的对比解释，完成计算复杂度分析，实现问题并给出实例。

Conclusion: 所定义的问题及方法在对比解释研究中有一定价值，为相关研究提供了新思路和实现途径。

Abstract: We define several canonical problems related to contrastive explanations,
each answering a question of the form ''Why P but not Q?''. The problems
compute causes for both P and Q, explicitly comparing their differences. We
investigate the basic properties of our definitions in the setting of
propositional logic. We show, inter alia, that our framework captures a
cardinality-minimal version of existing contrastive explanations in the
literature. Furthermore, we provide an extensive analysis of the computational
complexities of the problems. We also implement the problems for CNF-formulas
using answer set programming and present several examples demonstrating how
they work in practice.

</details>


### [14] [From Language to Logic: A Bi-Level Framework for Structured Reasoning](https://arxiv.org/abs/2507.08501)
*Keying Yang,Hao Wang,Kai Yang*

Main category: cs.AI

TL;DR: 提出双级框架将语言映射到逻辑，经高层抽象和低层生成两阶段，用端到端优化，实验显示精度显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言输入的结构化推理难题，弥合非结构化语言表达与形式逻辑表示间的差距。

Method: 提出双级框架，包括高层任务抽象和低层逻辑生成，并用端到端双级优化方法。

Result: 在多个现实推理基准测试中，显著优于现有基线，精度提升高达40%。

Conclusion: 双级设计增强透明度和错误可追溯性，为大语言模型的可靠和系统推理迈出有前景的一步。

Abstract: Structured reasoning over natural language inputs remains a core challenge in
artificial intelligence, as it requires bridging the gap between unstructured
linguistic expressions and formal logical representations. In this paper, we
propose a novel \textbf{bi-level framework} that maps language to logic through
a two-stage process: high-level task abstraction and low-level logic
generation. At the upper level, a large language model (LLM) parses natural
language queries into intermediate structured representations specifying the
problem type, objectives, decision variables, and symbolic constraints. At the
lower level, the LLM uses these representations to generate symbolic workflows
or executable reasoning programs for accurate and interpretable decision
making. The framework supports modular reasoning, enforces explicit
constraints, and generalizes across domains such as mathematical problem
solving, question answering, and logical inference. We further optimize the
framework with an end-to-end {bi-level} optimization approach that jointly
refines both the high-level abstraction and low-level logic generation stages.
Experiments on multiple realistic reasoning benchmarks demonstrate that our
approach significantly outperforms existing baselines in accuracy, with
accuracy gains reaching as high as 40\%. Moreover, the bi-level design enhances
transparency and error traceability, offering a promising step toward
trustworthy and systematic reasoning with LLMs.

</details>


### [15] [A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis](https://arxiv.org/abs/2507.08529)
*Mingda Zhang,Na Zhao,Jianglong Qin,Guoyu Ye,Ruixiang Tang*

Main category: cs.AI

TL;DR: 提出结合多粒度稀疏激活与分层知识图谱的框架用于罕见病诊断，实验有显著指标提升，专家评估有改进。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型在罕见病诊断中存在知识表示深度不足、概念理解有限和临床推理受限等问题。

Method: 提出结合多粒度稀疏激活医学概念与分层知识图谱的框架，采用四种互补匹配算法、多样性控制和五级回退策略，使用三层知识图谱。

Result: 在BioASQ罕见病问答集上，BLEU提升0.09，ROUGE提升0.05，准确率提升0.12，峰值准确率达0.89接近临床阈值0.90，专家评估信息质量、推理和专业表达有改进。

Conclusion: 该方法可缩短罕见病患者的“诊断之旅”。

Abstract: Despite advances from medical large language models in healthcare,
rare-disease diagnosis remains hampered by insufficient
knowledge-representation depth, limited concept understanding, and constrained
clinical reasoning. We propose a framework that couples multi-granularity
sparse activation of medical concepts with a hierarchical knowledge graph. Four
complementary matching algorithms, diversity control, and a five-level fallback
strategy enable precise concept activation, while a three-layer knowledge graph
(taxonomy, clinical features, instances) provides structured, up-to-date
context. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,
ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89
approaching the 0.90 clinical threshold. Expert evaluation confirms
improvements in information quality, reasoning, and professional expression,
suggesting our approach shortens the "diagnostic odyssey" for rare-disease
patients.

</details>


### [16] [Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing](https://arxiv.org/abs/2507.08575)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.AI

TL;DR: 论文提出利用大多模态模型（LMM）多模态能力的地理参考新方法，实验效果好并提出实用框架。


<details>
  <summary>Details</summary>
Motivation: 过去几个世纪自然历史收藏中的大量生物样本记录未地理参考，现有自动化方法未利用地图，地理参考复杂位置描述任务劳动强度大。

Method: 利用LMM多模态能力，在零样本设置下用基于网格的方法使自回归模型适应此任务。

Result: 在小的手动注释数据集上实验结果出色，平均距离误差约1公里，优于单模态大语言模型地理参考和现有工具。

Conclusion: 基于实验结果，提出将该方法集成到地理参考工作流程的实用框架。

Abstract: Millions of biological sample records collected in the last few centuries
archived in natural history collections are un-georeferenced. Georeferencing
complex locality descriptions associated with these collection samples is a
highly labour-intensive task collection agencies struggle with. None of the
existing automated methods exploit maps that are an essential tool for
georeferencing complex relations. We present preliminary experiments and
results of a novel method that exploits multi-modal capabilities of recent
Large Multi-Modal Models (LMM). This method enables the model to visually
contextualize spatial relations it reads in the locality description. We use a
grid-based approach to adapt these auto-regressive models for this task in a
zero-shot setting. Our experiments conducted on a small manually annotated
dataset show impressive results for our approach ($\sim$1 km Average distance
error) compared to uni-modal georeferencing with Large Language Models and
existing georeferencing tools. The paper also discusses the findings of the
experiments in light of an LMM's ability to comprehend fine-grained maps.
Motivated by these results, a practical framework is proposed to integrate this
method into a georeferencing workflow.

</details>


### [17] [Unlocking Speech Instruction Data Potential with Query Rewriting](https://arxiv.org/abs/2507.08603)
*Yonghua Hei,Yibo Yan,Shuliang Liu,Huiyu Zhou,Linfeng Zhang,Xuming Hu*

Main category: cs.AI

TL;DR: 现有端到端大语音语言模型遵循语音指令能力不足，收集标注语音指令数据集成本高，提出多LLM知识融合的查询重写框架构建高质量语音指令数据集，实验显示提升了数据可用性。


<details>
  <summary>Details</summary>
Motivation: 端到端大语音语言模型遵循语音指令能力未充分实现，现有构建语音指令数据集方法有缺陷，人工收集标注成本高，TTS模型转换文本指令存在挑战。

Method: 提出多LLM知识融合的查询重写框架，用多个代理对合成语音进行标注和验证。

Result: 通过零样本重写将文本指令转换为更适合TTS模型的分布进行语音合成，数据可用性从72%提升到93%，在复杂知识和上下文相关重写任务中有独特优势。

Conclusion: 该方法无需人工标注即可构建高质量语音指令数据集。

Abstract: End-to-end Large Speech Language Models~(\textbf{LSLMs}) demonstrate strong
potential in response latency and speech comprehension capabilities, showcasing
general intelligence across speech understanding tasks. However, the ability to
follow speech instructions has not been fully realized due to the lack of
datasets and heavily biased training tasks. Leveraging the rich ASR datasets,
previous approaches have used Large Language Models~(\textbf{LLMs}) to continue
the linguistic information of speech to construct speech instruction datasets.
Yet, due to the gap between LLM-generated results and real human responses, the
continuation methods further amplify these shortcomings. Given the high costs
of collecting and annotating speech instruction datasets by humans, using
speech synthesis to construct large-scale speech instruction datasets has
become a balanced and robust alternative. Although modern
Text-To-Speech~(\textbf{TTS}) models have achieved near-human-level synthesis
quality, it is challenging to appropriately convert out-of-distribution text
instruction to speech due to the limitations of the training data distribution
in TTS models. To address this issue, we propose a query rewriting framework
with multi-LLM knowledge fusion, employing multiple agents to annotate and
validate the synthesized speech, making it possible to construct high-quality
speech instruction datasets without relying on human annotation. Experiments
show that this method can transform text instructions into distributions more
suitable for TTS models for speech synthesis through zero-shot rewriting,
increasing data usability from 72\% to 93\%. It also demonstrates unique
advantages in rewriting tasks that require complex knowledge and
context-related abilities.

</details>


### [18] [Agentic Large Language Models for Conceptual Systems Engineering and Design](https://arxiv.org/abs/2507.08619)
*Soheyl Massoudi,Mark Fuge*

Main category: cs.AI

TL;DR: 评估结构化多智能体系统（MAS）与简单双智能体系统（2AS）在早期工程设计中管理需求提取、功能分解和模拟器代码生成的效果，发现MAS增强设计细节，推理蒸馏大语言模型提高完成率，但需求覆盖和编码保真度仍有问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型工作流在早期工程设计中难以保持任务连续性和生成可执行模型，评估MAS是否比2AS更能有效管理相关设计流程。

Method: 引入设计状态图（DSG），MAS和2AS分别采用不同流程，使用两种大语言模型，设置不同温度和种子进行60次实验，并报告多项指标。

Result: MAS和2AS都保持了完美的JSON完整性和实体标记，需求覆盖率低，2AS在特定设置下代码兼容性达100%，MAS平均低于50%，只有推理蒸馏模型能可靠标记工作流完成，MAS生成更细粒度的DSG，2AS模式崩溃。

Conclusion: 结构化多智能体编排增强设计细节，推理蒸馏大语言模型提高完成率，但需求覆盖和编码保真度仍存在差距。

Abstract: Early-stage engineering design involves complex, iterative reasoning, yet
existing large language model (LLM) workflows struggle to maintain task
continuity and generate executable models. We evaluate whether a structured
multi-agent system (MAS) can more effectively manage requirements extraction,
functional decomposition, and simulator code generation than a simpler
two-agent system (2AS). The target application is a solar-powered water
filtration system as described in a cahier des charges. We introduce the
Design-State Graph (DSG), a JSON-serializable representation that bundles
requirements, physical embodiments, and Python-based physics models into graph
nodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS
collapses the process to a Generator-Reflector loop. Both systems run a total
of 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1
70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON
validity, requirement coverage, embodiment presence, code compatibility,
workflow completion, runtime, and graph size. Across all runs, both MAS and 2AS
maintained perfect JSON integrity and embodiment tagging. Requirement coverage
remained minimal (less than 20\%). Code compatibility peaked at 100\% under
specific 2AS settings but averaged below 50\% for MAS. Only the
reasoning-distilled model reliably flagged workflow completion. Powered by
DeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)
whereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced
design detail. Reasoning-distilled LLM improved completion rates, yet low
requirements and fidelity gaps in coding persisted.

</details>


### [19] [Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning](https://arxiv.org/abs/2507.08649)
*Xingguang Ji,Yahui Liu,Qi Wang,Jingyuan Zhang,Yang Yue,Rui Shi,Chenxi Sun,Fuzheng Zhang,Guorui Zhou,Kun Gai*

Main category: cs.AI

TL;DR: 介绍Leanabell - Prover - V2，可在Lean 4中生成形式定理证明，改进RL，实验显示性能提升，代码等公开。


<details>
  <summary>Details</summary>
Motivation: 延续之前工作，进一步提升现有强证明器模型的性能。

Method: 在V2版本中主要升级强化学习，利用Lean 4验证器的反馈，直接优化大语言模型推理轨迹，采用反馈令牌掩码和简单奖励策略。

Result: 在MiniF2F测试集上，与Kimina - Prover - Preview - Distill - 7B相比提升3.2%（pass@128），与DeepSeek - Prover - V2 - 7B相比提升2.0%（pass@128）。

Conclusion: Leanabell - Prover - V2通过验证器集成的长思维链和改进的强化学习，有效提升了证明性能。

Abstract: We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that
can produce formal theorem proofs in Lean 4, with verifier-integrated Long
Chain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we
continual to choose to posttrain existing strong prover models for further
performance improvement. In our V2 version, we mainly upgrade the Reinforcement
Learning (RL) with feedback provided by the Lean 4 verifier. Crucially,
verifier feedback, such as indicating success or detailing specific errors,
allows the LLM to become ``self-aware'' of the correctness of its own reasoning
process and learn to reflexively correct errors. Leanabell-Prover-V2 directly
optimizes LLM reasoning trajectories with multi-turn verifier interactions,
together with feedback token masking for stable RL training and a simple reward
strategy. Experiments show that Leanabell-Prover-V2 improves performance by
3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with
DeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data
and models are available at:
https://github.com/Leanabell-LM/Leanabell-Prover-V2.

</details>


### [20] [Introspection of Thought Helps AI Agents](https://arxiv.org/abs/2507.08664)
*Haoran Sun,Shaoning Zeng*

Main category: cs.AI

TL;DR: 提出含思想内省的AI代理推理框架INoT，可降低令牌成本，实验验证其有效性和多功能性。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理受LLM理解自然语言的固有局限，迭代推理成本高。

Method: 设计新的LLM - Read代码用于提示，使LLM按提示代码执行对话推理。

Result: 在六个基准测试中平均性能提升7.95%，令牌成本比基线最佳方法低58.3%，验证了图像任务的多功能性。

Conclusion: INoT能有效降低成本，提升性能，具有多功能性。

Abstract: AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to
perform interpretation and inference in text and image tasks without
post-training, where LLMs and MLLMs play the most critical role and determine
the initial ability and limitations of AI Agents. Usually, AI Agents utilize
sophisticated prompt engineering and external reasoning framework to obtain a
promising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought
and Image-of-Thought. However, they are still constrained by the inherent
limitations of LLM in understanding natural language, and the iterative
reasoning process will generate a large amount of inference cost. To this end,
we propose a novel AI Agent Reasoning Framework with Introspection of Thought
(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute
programmatic dialogue reasoning processes following the code in prompt.
Therefore, self-denial and reflection occur within LLM instead of outside LLM,
which can reduce token cost effectively. Through our experiments on six
benchmarks for three different tasks, the effectiveness of INoT is verified,
with an average improvement of 7.95\% in performance, exceeding the baselines.
Furthermore, the token cost of INoT is lower on average than the best
performing method at baseline by 58.3\%. In addition, we demonstrate the
versatility of INoT in image interpretation and inference through verification
experiments.

</details>


### [21] [elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings](https://arxiv.org/abs/2507.08705)
*Philip Osborne,Danilo S. Carvalho,André Freitas*

Main category: cs.AI

TL;DR: 介绍开源Python库elsciRL，展示其在强化学习问题上应用语言解决方案的潜力，结果表明指令能提升智能体性能，旨在加速语言解决方案评估。


<details>
  <summary>Details</summary>
Motivation: 加速语言解决方案在基于奖励环境中的评估，为科学发现创造新机会。

Method: 扩展语言适配器，使用大语言模型，提供新颖GUI让用户输入文本生成可自完成指令。

Result: 经验结果表明这些指令能够提高强化学习智能体的性能。

Conclusion: 提出elsciRL库以推动语言解决方案在强化学习中的应用和评估。

Abstract: We present elsciRL, an open-source Python library to facilitate the
application of language solutions on reinforcement learning problems. We
demonstrate the potential of our software by extending the Language Adapter
with Self-Completing Instruction framework defined in (Osborne, 2024) with the
use of LLMs. Our approach can be re-applied to new applications with minimal
setup requirements. We provide a novel GUI that allows a user to provide text
input for an LLM to generate instructions which it can then self-complete.
Empirical results indicate that these instructions \textit{can} improve a
reinforcement learning agent's performance. Therefore, we present this work to
accelerate the evaluation of language solutions on reward based environments to
enable new opportunities for scientific discovery.

</details>


### [22] [System-of-systems Modeling and Optimization: An Integrated Framework for Intermodal Mobility](https://arxiv.org/abs/2507.08715)
*Paul Saves,Jasper Bussemaker,Rémi Lafage,Thierry Lefebvre,Nathalie Bartoli,Youssef Diouane,Joseph Morlier*

Main category: cs.AI

TL;DR: 介绍系统架构开发中建模优化技术，指出专用方法问题及代理优化算法的出现


<details>
  <summary>Details</summary>
Motivation: 解决系统架构开发中使用专用方法进行优化时存在评估成本增加和可能失败等挑战

Method: 采用基于代理的优化算法，如利用高斯过程模型的贝叶斯优化

Result: 未提及

Conclusion: 未提及

Abstract: For developing innovative systems architectures, modeling and optimization
techniques have been central to frame the architecting process and define the
optimization and modeling problems. In this context, for system-of-systems the
use of efficient dedicated approaches (often physics-based simulations) is
highly recommended to reduce the computational complexity of the targeted
applications. However, exploring novel architectures using such dedicated
approaches might pose challenges for optimization algorithms, including
increased evaluation costs and potential failures. To address these challenges,
surrogate-based optimization algorithms, such as Bayesian optimization
utilizing Gaussian process models have emerged.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [23] [EP-GAT: Energy-based Parallel Graph Attention Neural Network for Stock Trend Classification](https://arxiv.org/abs/2507.08184)
*Zhuodong Jiang,Pengju Zhang,Peter Martin*

Main category: cs.CE

TL;DR: 本文提出基于能量的并行图注意力神经网络（EP - GAT）预测多只股票走势，实验表明其性能优于5个基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的股票走势预测方法依赖静态或手动定义因素建模，且难以保留股票内部层次特征。

Method: 用股票间能量差和玻尔兹曼分布生成动态股票图，提出并行图注意力机制保留股票内部层次动态。

Result: 在5个真实数据集上实验，EP - GAT在各指标上始终优于5个竞争基线模型，消融研究和超参数敏感性分析验证各模块有效性。

Conclusion: EP - GAT能有效捕捉股票间变化的相互依赖关系和股票内部层次特征，在股票走势预测中表现良好。

Abstract: Graph neural networks have shown remarkable performance in forecasting stock
movements, which arises from learning complex inter-dependencies between stocks
and intra-dynamics of stocks. Existing approaches based on graph neural
networks typically rely on static or manually defined factors to model changing
inter-dependencies between stocks. Furthermore, these works often struggle to
preserve hierarchical features within stocks. To bridge these gaps, this work
presents the Energy-based Parallel Graph Attention Neural Network, a novel
approach for predicting future movements for multiple stocks. First, it
generates a dynamic stock graph with the energy difference between stocks and
Boltzmann distribution, capturing evolving inter-dependencies between stocks.
Then, a parallel graph attention mechanism is proposed to preserve the
hierarchical intra-stock dynamics. Extensive experiments on five real-world
datasets are conducted to validate the proposed approach, spanning from the US
stock markets (NASDAQ, NYSE, SP) and UK stock markets (FTSE, LSE). The
experimental results demonstrate that EP-GAT consistently outperforms
competitive five baselines on test periods across various metrics. The ablation
studies and hyperparameter sensitivity analysis further validate the
effectiveness of each module in the proposed method.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [24] [TableCopilot: A Table Assistant Empowered by Natural Language Conditional Table Discovery](https://arxiv.org/abs/2507.08283)
*Lingxi Cui,Guanyu Jiang,Huan Li,Ke Chen,Lidan Shou,Gang Chen*

Main category: cs.DB

TL;DR: 介绍基于大语言模型的TableCopilot辅助工具用于交互式表格发现与分析，提出Crofuma方法，实验显示其性能优于SOTA单输入方法，还发布资源。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的表格助手系统忽略大规模表格池中表格发现的挑战，需要解决该问题。

Method: 引入TableCopilot，定义nlcTD场景，提出Crofuma跨融合方法学习和聚合单模态与跨模态匹配分数。

Result: Crofuma在NDCG@5上比SOTA单输入方法至少高12%。

Conclusion: TableCopilot为交互式表格助手设定新标准，使高级表格发现更易获取和集成。

Abstract: The rise of LLM has enabled natural language-based table assistants, but
existing systems assume users already have a well-formed table, neglecting the
challenge of table discovery in large-scale table pools. To address this, we
introduce TableCopilot, an LLM-powered assistant for interactive, precise, and
personalized table discovery and analysis. We define a novel scenario, nlcTD,
where users provide both a natural language condition and a query table,
enabling intuitive and flexible table discovery for users of all expertise
levels. To handle this, we propose Crofuma, a cross-fusion-based approach that
learns and aggregates single-modal and cross-modal matching scores.
Experimental results show Crofuma outperforms SOTA single-input methods by at
least 12% on NDCG@5. We also release an instructional video, codebase,
datasets, and other resources on GitHub to encourage community contributions.
TableCopilot sets a new standard for interactive table assistants, making
advanced table discovery accessible and integrated.

</details>


### [25] [xpSHACL: Explainable SHACL Validation using Retrieval-Augmented Generation and Large Language Models](https://arxiv.org/abs/2507.08432)
*Gustavo Correa Publio,José Emilio Labra Gayo*

Main category: cs.DB

TL;DR: 本文提出可解释的SHACL验证系统xpSHACL，结合规则推理树、检索增强生成和大语言模型，为约束违反提供多语言、易读解释，还使用违规知识图谱提高效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统SHACL验证引擎的英文报告难以被非技术用户解读，随着知识图谱受关注，用户需要更好的方式验证关联数据。

Method: 结合规则推理树、检索增强生成（RAG）和大语言模型（LLMs），使用违规知识图谱（Violation KG）缓存和复用解释。

Result: 未提及具体结果。

Conclusion: 未提及明确结论，但表明xpSHACL可解决传统验证引擎存在的问题。

Abstract: Shapes Constraint Language (SHACL) is a powerful language for validating RDF
data. Given the recent industry attention to Knowledge Graphs (KGs), more users
need to validate linked data properly. However, traditional SHACL validation
engines often provide terse reports in English that are difficult for
non-technical users to interpret and act upon. This paper presents xpSHACL, an
explainable SHACL validation system that addresses this issue by combining
rule-based justification trees with retrieval-augmented generation (RAG) and
large language models (LLMs) to produce detailed, multilanguage, human-readable
explanations for constraint violations. A key feature of xpSHACL is its usage
of a Violation KG to cache and reuse explanations, improving efficiency and
consistency.

</details>


### [26] [ONION: A Multi-Layered Framework for Participatory ER Design](https://arxiv.org/abs/2507.08702)
*Viktoriia Makovska,George Fletcher,Julia Stoyanovich*

Main category: cs.DB

TL;DR: 提出多层面参与式实体关系建模框架ONION，含五阶段方法，评估显示其在早期数据建模中容纳多样性的潜力，最后总结经验、局限和挑战。


<details>
  <summary>Details</summary>
Motivation: 减少设计师偏见，促进包容性参与，增加建模过程的透明度。

Method: 引入五阶段方法（Observe, Nurture, Integrate, Optimize, Normalize），通过聚焦乌克兰社会技术系统的实际研讨会评估。

Result: 早期结果显示ONION在早期数据建模中容纳多样性的潜力，不同利益相关者参与带来更丰富的数据模型和更深入的相互理解。

Conclusion: 总结经验、局限和挑战，探讨框架扩展和完善以实现更广泛应用。

Abstract: We present ONION, a multi-layered framework for participatory
Entity-Relationship (ER) modeling that integrates insights from design justice,
participatory AI, and conceptual modeling. ONION introduces a five-stage
methodology: Observe, Nurture, Integrate, Optimize, Normalize. It supports
progressive abstraction from unstructured stakeholder input to structured ER
diagrams.
  Our approach aims to reduce designer bias, promote inclusive participation,
and increase transparency through the modeling process. We evaluate ONION
through real-world workshops focused on sociotechnical systems in Ukraine,
highlighting how diverse stakeholder engagement leads to richer data models and
deeper mutual understanding. Early results demonstrate ONION's potential to
host diversity in early-stage data modeling. We conclude with lessons learned,
limitations and challenges involved in scaling and refining the framework for
broader adoption.

</details>


### [27] [Hashing for Fast Pattern Set Selection](https://arxiv.org/abs/2507.08745)
*Maiju Karjalainen,Pauli Miettinen*

Main category: cs.DB

TL;DR: 本文研究模式集挖掘，提出基于bottom - k哈希的方法高效选择模式集，在合成和真实数据集上比标准贪心算法快且结果相近。


<details>
  <summary>Details</summary>
Motivation: 以重建误差衡量模式集优劣，解决如何高效找到好的模式集这一问题。

Method: 提出基于bottom - k哈希的方法选择模式集，并将其扩展到模式可能以近似形式出现在数据中的常见情况。

Result: 在合成和真实数据集上，基于哈希的方法比标准贪心算法显著更快，且结果几乎同样好。

Conclusion: 所提基于哈希的方法可应用于数据库平铺、布尔矩阵分解和重描述挖掘等领域，且在效率和结果上表现良好。

Abstract: Pattern set mining, which is the task of finding a good set of patterns
instead of all patterns, is a fundamental problem in data mining. Many
different definitions of what constitutes a good set have been proposed in
recent years. In this paper, we consider the reconstruction error as a proxy
measure for the goodness of the set, and concentrate on the adjacent problem of
how to find a good set efficiently. We propose a method based on bottom-k
hashing for efficiently selecting the set and extend the method for the common
case where the patterns might only appear in approximate form in the data. Our
approach has applications in tiling databases, Boolean matrix factorization,
and redescription mining, among others. We show that our hashing-based approach
is significantly faster than the standard greedy algorithm while obtaining
almost equally good results in both synthetic and real-world data sets.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [28] [Supporting Intel(r) SGX on Multi-Package Platforms](https://arxiv.org/abs/2507.08190)
*Simon Johnson,Raghunandan Makaram,Amy Santoni,Vinnie Scarlata*

Main category: cs.DC

TL;DR: 本文探讨SGX在云环境应用，指出多包平台上交付用户可编程可信执行环境所需的平台增强。


<details>
  <summary>Details</summary>
Motivation: 随着开发者熟悉SGX技术，测试其在云环境的适用性，各云服务提供商展示了基于SGX的可信执行环境价值，推动在云环境更好应用。

Method: 未提及

Result: 未提及

Conclusion: 需要进行额外的平台增强，以在多包平台上实现可扩展、高性能且安全的用户可编程可信执行环境。

Abstract: Intel(r) Software Guard Extensions (SGX) was originally released on client
platforms and later extended to single socket server platforms. As developers
have become familiar with the capabilities of the technology, the applicability
of this capability in the cloud has been tested. Various Cloud Service
Providers (CSPs) are demonstrating the value of using SGX based Trusted
Execution Environments (TEE) to create a new paradigm of Confidential Cloud
Computing. This paper describes the additional platform enhancements we believe
are necessary to deliver a user programmable Trusted Execution Environment that
scales to cloud usages, performs and is secure on multi-package platforms.

</details>


### [29] [Fast and Interactive Byzantine Fault-tolerant Web Services via Session-Based Consensus Decoupling](https://arxiv.org/abs/2507.08281)
*Ahmad Zaki Akmal,Azkario Rizky Pratama,Guntur Dharma Putra*

Main category: cs.DC

TL;DR: 提出两层架构解决BFT系统安全与响应性矛盾，实现低延迟与高安全，通过供应链管理示例验证，能用于元宇宙等领域。


<details>
  <summary>Details</summary>
Motivation: BFT网络服务存在显著延迟挑战，阻碍交互式用户体验，需解决安全与响应性的矛盾。

Method: 提出两层架构，引入会话感知事务缓冲区层（Layer 2）通过共识模拟即时反馈，定期将批量操作提交到完全拜占庭容错共识层（Layer 1）。

Result: 系统实现了低于200ms的响应式用户体验，Layer 2操作比Layer 1快四倍，同时基本保持了端到端事务完整性。

Conclusion: 该方法使BFT应用能用于因延迟限制之前被认为不实际的领域，如元宇宙环境。

Abstract: Byzantine fault-tolerant (BFT) web services provide critical integrity
guarantees for distributed applications but face significant latency challenges
that hinder interactive user experiences. We propose a novel two-layer
architecture that addresses this fundamental tension between security and
responsiveness in BFT systems. Our approach introduces a session-aware
transaction buffer layer (Layer 2) that delivers immediate feedback to users
through consensus simulation, while periodically committing batched operations
to a fully Byzantine fault-tolerant consensus layer (Layer 1). By separating
interactive operations from consensus finalization, our system achieves
responsive user experiences of under 200ms, while maintaining strong BFT
security guarantees. We demonstrate the efficacy of our architecture through a
supply chain management implementation, where operators require both immediate
feedback during multi-step workflows and tamper-proof record keeping. Our
evaluation shows that our Layer 2 operations perform four times faster than the
Layer 1 counterpart, while substantially preserving the end-to-end transaction
integrity. Our approach enables BFT applications in domains previously
considered impractical due to latency constraints, such as metaverse
environments, where users require both responsive interaction and guaranteed
state consistency.

</details>


### [30] [Content-Oblivious Leader Election in 2-Edge-Connected Networks](https://arxiv.org/abs/2507.08348)
*Yi-Jun Chang,Lyuting Chen,Haoran Zhou*

Main category: cs.DC

TL;DR: 前人研究全缺陷异步网络，模拟算法需预设领导者并猜测其为非平凡任务必要条件。本文提出异步内容无关领导者选举算法，可在2 - 边连通网络中静止终止，结合先前结果反驳原猜测。


<details>
  <summary>Details</summary>
Motivation: 反驳前人关于非平凡内容无关任务需预设领导者的猜想。

Method: 设计一个异步内容无关领导者选举算法，使其在任何2 - 边连通网络中静止终止。

Result: 设计出的算法消息复杂度为 $O(m \cdot N \cdot \mathsf{ID}_{\min})$，结合先前结果可在无预设领导者情况下模拟无噪声设置中的算法。

Conclusion: 完全反驳了原关于非平凡内容无关任务需预设领导者的猜想。

Abstract: Censor-Hillel, Cohen, Gelles, and Sela (PODC 2022 \& Distributed Computing
2023) studied fully-defective asynchronous networks, where communication
channels may suffer an extreme form of alteration errors, rendering messages
completely corrupted. The model is equivalent to content-oblivious computation,
where nodes communicate solely via pulses. They showed that if the network is
2-edge-connected, then any algorithm for a noiseless setting can be simulated
in the fully-defective setting; otherwise, no non-trivial computation is
possible in the fully-defective setting. However, their simulation requires a
predesignated leader, which they conjectured to be necessary for any
non-trivial content-oblivious task.
  Recently, Frei, Gelles, Ghazy, and Nolin (DISC 2024) refuted this conjecture
for the special case of oriented ring topology. They designed two asynchronous
content-oblivious leader election algorithms with message complexity $O(n \cdot
\mathsf{ID}_{\max})$, where $n$ is the number of nodes and $\mathsf{ID}_{\max}$
is the maximum $\mathsf{ID}$. The first algorithm stabilizes in unoriented
rings without termination detection. The second algorithm quiescently
terminates in oriented rings, thus enabling the execution of the simulation
algorithm after leader election.
  In this work, we present an asynchronous content-oblivious leader election
algorithm that quiescently terminates in any 2-edge connected network with
message complexity $O(m \cdot N \cdot \mathsf{ID}_{\min})$, where $m$ is the
number of edges, $N$ is a known upper bound on the number of nodes, and
$\mathsf{ID}_{\min}$ is the smallest $\mathsf{ID}$. Combined with the previous
simulation result, our finding implies that any algorithm from the noiseless
setting can be simulated in the fully-defective setting without assuming a
preselected leader, entirely refuting the original conjecture.

</details>


### [31] [Carbon-Aware Workflow Scheduling with Fixed Mapping and Deadline Constraint](https://arxiv.org/abs/2507.08725)
*Dominik Schweisgut,Anne Benoit,Yves Robert,Henning Meyerhenke*

Main category: cs.DC

TL;DR: 为降低大数据和计算中心工作流执行的碳排放，提出调度问题，分析复杂度并给出启发式框架CaWoSched，实验显示其能显著减少碳排放。


<details>
  <summary>Details</summary>
Motivation: 大数据和计算中心能耗大，为减少含可再生和非可再生能源供应中心执行工作流产生的碳排放。

Method: 将问题形式化为调度问题，分析单处理器和多处理器下复杂度，提出启发式框架CaWoSched，设计基线算法和精确ILP解决方案。

Result: 实验表明所提出的启发式方法相比基线算法能显著减少碳排放。

Conclusion: 启发式框架CaWoSched在减少工作流执行碳排放方面有良好效果。

Abstract: Large data and computing centers consume a significant share of the world's
energy consumption. A prominent subset of the workloads in such centers are
workflows with interdependent tasks, usually represented as directed acyclic
graphs (DAGs). To reduce the carbon emissions resulting from executing such
workflows in centers with a mixed (renewable and non-renewable) energy supply,
it is advisable to move task executions to time intervals with sufficient green
energy when possible. To this end, we formalize the above problem as a
scheduling problem with a given mapping and ordering of the tasks. We show that
this problem can be solved in polynomial time in the uniprocessor case. For at
least two processors, however, the problem becomes NP-hard. Hence, we propose a
heuristic framework called CaWoSched that combines several greedy approaches
with local search. To assess the 16 heuristics resulting from different
combinations, we also devise a simple baseline algorithm and an exact ILP-based
solution. Our experimental results show that our heuristics provide significant
savings in carbon emissions compared to the baseline.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [32] [On the Parallel Complexity of Finding a Matroid Basis](https://arxiv.org/abs/2507.08194)
*Sanjeev Khanna,Aaron Putterman,Junkai Song*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A fundamental question in parallel computation, posed by Karp, Upfal, and
Wigderson (FOCS 1985, JCSS 1988), asks: \emph{given only independence-oracle
access to a matroid on $n$ elements, how many rounds are required to find a
basis using only polynomially many queries?} This question generalizes, among
others, the complexity of finding bases of linear spaces, partition matroids,
and spanning forests in graphs. In their work, they established an upper bound
of $O(\sqrt{n})$ rounds and a lower bound of $\widetilde{\Omega}(n^{1/3})$
rounds for this problem, and these bounds have remained unimproved since then.
  In this work, we make the first progress in narrowing this gap by designing a
parallel algorithm that finds a basis of an arbitrary matroid in
$\tilde{O}(n^{7/15})$ rounds (using polynomially many independence queries per
round) with high probability, surpassing the long-standing $O(\sqrt{n})$
barrier. Our approach introduces a novel matroid decomposition technique and
other structural insights that not only yield this general result but also lead
to a much improved new algorithm for the class of \emph{partition matroids}
(which underlies the $\widetilde\Omega(n^{1/3})$ lower bound of Karp, Upfal,
and Wigderson). Specifically, we develop an $\tilde{O}(n^{1/3})$-round
algorithm, thereby settling the round complexity of finding a basis in
partition matroids.

</details>


### [33] [Approximation Algorithms for the Cumulative Vehicle Routing Problem with Stochastic Demands](https://arxiv.org/abs/2507.08316)
*Jingyang Zhao,Mingyu Xiao*

Main category: cs.DS

TL;DR: 本文为Cu - VRPSD、VRPSD和Cu - VRP问题提出随机近似算法，提升了已知近似比率，允许多次服务客户时还能进一步改进。


<details>
  <summary>Details</summary>
Motivation: 为Cu - VRPSD、VRPSD和Cu - VRP问题找到更好的近似算法，降低近似比率。

Method: 提出随机近似算法。

Result: 为Cu - VRPSD提出3.456 - 近似算法，为VRPSD得到3.25 - 近似算法，为Cu - VRP给出3.194 - 近似算法，允许多次服务客户时有进一步改进。

Conclusion: 提出的算法改进了这些问题的最佳已知近似比率。

Abstract: In the Cumulative Vehicle Routing Problem (Cu-VRP), we need to find a
feasible itinerary for a capacitated vehicle located at the depot to satisfy
customers' demand, as in the well-known Vehicle Routing Problem (VRP), but the
goal is to minimize the cumulative cost of the vehicle, which is based on the
vehicle's load throughout the itinerary. If the demand of each customer is
unknown until the vehicle visits it, the problem is called Cu-VRP with
Stochastic Demands (Cu-VRPSD). Assume that the approximation ratio of metric
TSP is $1.5$. In this paper, we propose a randomized $3.456$-approximation
algorithm for Cu-VRPSD, improving the best-known approximation ratio of $6$
(Discret. Appl. Math. 2020). Since VRP with Stochastic Demands (VRPSD) is a
special case of Cu-VRPSD, as a corollary, we also obtain a randomized
$3.25$-approximation algorithm for VRPSD, improving the best-known
approximation ratio of $3.5$ (Oper. Res. 2012). For Cu-VRP, we give a
randomized $3.194$-approximation algorithm, improving the best-known
approximation ratio of $4$ (Oper. Res. Lett. 2013). Moreover, if each customer
is allowed to be satisfied by using multiple tours, we obtain further
improvements for Cu-VRPSD and Cu-VRP.

</details>


### [34] [H-Planarity and Parametric Extensions: when Modulators Act Globally](https://arxiv.org/abs/2507.08541)
*Fedor V. Fomin,Petr A. Golovach,Laure Morelle,Dimitrios M. Thilikos*

Main category: cs.DS

TL;DR: 本文引入基于修改问题调制器/目标方案的图分解，计算平面H - 调制器，证明H - 平面性问题可解，引入相关参数扩展，获FPT算法及多种算法应用。


<details>
  <summary>Details</summary>
Motivation: 扩展平面性的算法潜力，解决图相关问题。

Method: 引入平面H - 调制器概念，给出计算其的多项式时间算法，定义H - 平面树深度和H - 平面树宽度概念。

Result: 若H满足特定条件，H - 平面性问题可在多项式时间内解决；获得参数化的FPT算法；得到图着色的加法近似算法、匹配计数的多项式时间算法和EPTAS等。

Conclusion: 提出的方法结合平面和有界树宽图的算法性质，带来多种算法应用。

Abstract: We introduce a series of graph decompositions based on the modulator/target
scheme of modification problems that enable several algorithmic applications
that parametrically extend the algorithmic potential of planarity. In the core
of our approach is a polynomial time algorithm for computing planar
H-modulators. Given a graph class H, a planar H-modulator of a graph G is a set
X \subseteq V(G) such that the ``torso'' of X is planar and all connected
components of G - X belong to H. Here, the torso of X is obtained from G[X] if,
for every connected component of G-X, we form a clique out of its neighborhood
on G[X]. We introduce H-Planarity as the problem of deciding whether a graph G
has a planar H-modulator. We prove that, if H is hereditary, CMSO-definable,
and decidable in polynomial time, then H-Planarity is solvable in polynomial
time. Further, we introduce two parametric extensions of H-Planarity by
defining the notions of H-planar treedepth and H-planar treewidth, which
generalize the concepts of elimination distance and tree decompositions to the
class H. Combining this result with existing FPT algorithms for various
H-modulator problems, we thereby obtain FPT algorithms parameterized by
H-planar treedepth and H-planar treewidth for numerous graph classes H. By
combining the well-known algorithmic properties of planar graphs and graphs of
bounded treewidth, our methods for computing H-planar treedepth and H-planar
treewidth lead to a variety of algorithmic applications. For instance, once we
know that a given graph has bounded H-planar treedepth or bounded H-planar
treewidth, we can derive additive approximation algorithms for graph coloring
and polynomial-time algorithms for counting (weighted) perfect matchings.
Furthermore, we design Efficient Polynomial-Time Approximation Schemes
(EPTAS-es) for several problems, including Maximum Independent Set.

</details>


### [35] [Beer Path Problems in Temporal Graphs](https://arxiv.org/abs/2507.08685)
*Andrea D'Ascenzo,Giuseppe F. Italiano,Sotiris Kanellopoulos,Anna Mpanti,Aris Pagourtzis,Christos Pergaminelis*

Main category: cs.DS

TL;DR: 本文引入时态图中的啤酒路径概念，定义相关路径计算问题，提出高效算法并给出预处理技术。


<details>
  <summary>Details</summary>
Motivation: 现有图中啤酒路径问题研究未考虑时态信息，而现实场景中时态信息很关键。

Method: 在时态图中定义最早到达、最晚出发、最快和最短时态啤酒路径问题，针对边流和邻接表表示提出算法，通过预计算路径或转换为静态图实现动态条件下的高效查询。

Result: 所提算法时间复杂度与对应时态路径查找算法一致，保证了效率。

Conclusion: 引入时态图啤酒路径概念，提出的算法和预处理技术可有效处理含时态信息的啤酒路径问题及动态场景。

Abstract: Computing paths in graph structures is a fundamental operation in a wide
range of applications, from transportation networks to data analysis. The beer
path problem, which captures the option of visiting points of interest, such as
gas stations or convenience stops, prior to reaching the final destination, has
been recently introduced and extensively studied in static graphs. However,
existing approaches do not account for temporal information, which is often
crucial in real-world scenarios. For instance, transit services may follow
fixed schedules, and shops may only be accessible during certain hours.
  In this work, we introduce the notion of beer paths in temporal graphs, where
edges are time-dependent and certain vertices (beer vertices) are active only
at specific time instances. We formally define the problems of computing
earliest-arrival, latest-departure, fastest, and shortest temporal beer paths
and propose efficient algorithms for these problems under both edge stream and
adjacency list representations. The time complexity of each of our algorithms
is aligned with that of corresponding temporal pathfinding algorithms, thus
preserving efficiency.
  Additionally, we present preprocessing techniques that enable efficient query
answering under dynamic conditions, for example new openings or closings of
shops. We achieve this through appropriate precomputation of selected paths or
by transforming a temporal graph into an equivalent static graph.

</details>


### [36] [On the Constant-Factor Approximability of Minimum Cost Constraint Satisfaction Problems](https://arxiv.org/abs/2507.08693)
*Ian DeHaan,Neng Huang,Euiwoong Lee*

Main category: cs.DS

TL;DR: 本文从代数视角研究最小成本约束满足问题，给出近似算法，证明相关复杂度结果及二分性，还给出特定反例。


<details>
  <summary>Details</summary>
Motivation: 从代数角度研究最小成本约束满足问题（MinCostCSP）的近似性和复杂度。

Method: 通过代数方法分析约束语言的多态性，给出算法并进行复杂度证明。

Result: 对于有对偶判别器操作作为多态性的约束语言有|D|-近似算法；MinCostCSP(Γ)有常数因子近似则约束语言Γ须有近一致（NU）多态性（除非P = NP）；含所有置换关系的约束语言有近似性二分性；存在有多数多态性但NP难近似的约束语言（假设唯一游戏猜想成立）。

Conclusion: 含所有置换关系的约束语言MinCostCSP近似性有二分性，有NU多态性一般不是近似的充分条件（假设UGC成立）。

Abstract: We study minimum cost constraint satisfaction problems (MinCostCSP) through
the algebraic lens. We show that for any constraint language $\Gamma$ which has
the dual discriminator operation as a polymorphism, there exists a
$|D|$-approximation algorithm for MinCostCSP$(\Gamma)$ where $D$ is the domain.
Complementing our algorithmic result, we show that any constraint language
$\Gamma$ where MinCostCSP$(\Gamma)$ admits a constant-factor approximation must
have a \emph{near-unanimity} (NU) polymorphism unless P = NP, extending a
similar result by Dalmau et al. on MinCSPs. These results imply a dichotomy of
constant-factor approximability for constraint languages that contain all
permutation relations (a natural generalization for Boolean CSPs that allow
variable negation): either MinCostCSP$(\Gamma)$ has an NU polymorphism and is
$|D|$-approximable, or it does not have any NU polymorphism and is NP-hard to
approximate within any constant factor. Finally, we present a constraint
language which has a majority polymorphism, but is nonetheless NP-hard to
approximate within any constant factor assuming the Unique Games Conjecture,
showing that the condition of having an NU polymorphism is in general not
sufficient unless UGC fails.

</details>


### [37] [To buy or not to buy: deterministic rent-or-buy problems on node-weighted graphs](https://arxiv.org/abs/2507.08698)
*Sander Borst,Moritz Venzin*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the rent-or-buy variant of the online Steiner forest problem on
node- and edge-weighted graphs. For $n$-node graphs with at most $\bar{n}$
non-zero node-weights, and at most $\tilde{k}$ different arriving terminal
pairs, we obtain a deterministic, $O(\log n \log \bar{n})$-competitive
algorithm. This improves on the previous best, $O(\log^4 n)$-competitive
algorithm obtained by the black-box reduction from (Bartal et al. 2021)
combined with the previously best deterministic algorithms for the simpler
'buy-only' setting. We also obtain a deterministic, $O(\bar{n}\log
\tilde{k})$-competitive algorithm. This generalizes the $O(\log
\tilde{k})$-competitive algorithm for the purely edge-weighted setting from
(Umboh 2015). We also obtain a randomized, $O(\log \tilde{k} \log
\bar{n})$-competitive algorithm. All previous approaches were based on the
randomized, black-box reduction from~\cite{AwerbuchAzarBartal96} that achieves
a $O(\log \tilde{k} \log n)$-competitive ratio when combined with an algorithm
for the 'buy-only' setting. Our key technical ingredient is a novel charging
scheme to an instance of \emph{online prize-collecting set cover}. This allows
us to extend the witness-technique of (Umboh 2015) to the node-weighted setting
and obtain refined guarantees with respect to $\bar{n}$, already in the much
simpler 'buy-only' setting.

</details>


### [38] [On Fair Epsilon Net and Geometric Hitting Set](https://arxiv.org/abs/2507.08758)
*Mohsen Dehghankar,Stavros Sintos,Abolfazl Asudeh*

Main category: cs.DS

TL;DR: 本文将公平性引入经典几何近似问题，提出两种算法来实现公平性，理论分析和实验验证了算法有效性及公平采样局限性。


<details>
  <summary>Details</summary>
Motivation: 现有计算几何工具未从公平性角度研究，为填补该研究空白，将公平性添加到经典几何近似问题中。

Method: 引入人口统计学均等和自定义比率公平两种公平概念，开发基于采样和基于差异理论的两种算法，将公平几何击中集问题转化为寻找公平 ε - 网。

Result: 采样算法计算的公平 ε - 网规模仅比标准 ε - 网大 log(k) 倍；差异算法计算的公平 ε - 网更小；实现公平几何击中集的 O(log OPT × log k) 近似；证明特定输入分布下构建公平 ε - 样本不可行；实验实现零不公平性且输出规模适度增加。

Conclusion: 提出的算法在理论和实践上都有效，同时指出公平采样存在局限性。

Abstract: Fairness has emerged as a formidable challenge in data-driven decisions. Many
of the data problems, such as creating compact data summaries for approximate
query processing, can be effectively tackled using concepts from computational
geometry, such as $\varepsilon$-nets. However, these powerful tools have yet to
be examined from the perspective of fairness. To fill this research gap, we add
fairness to classical geometric approximation problems of $\varepsilon$-net,
$\varepsilon$-sample, and geometric hitting set. We introduce and address two
notions of group fairness: demographic parity, which requires preserving group
proportions from the input distribution, and custom-ratios fairness, which
demands satisfying arbitrary target ratios. We develop two algorithms to
enforce fairness: one based on sampling and another on discrepancy theory. The
sampling-based algorithm is faster and computes a fair $\varepsilon$-net of
size which is only larger by a $\log(k)$ factor compared to the standard
(unfair) $\varepsilon$-net, where $k$ is the number of demographic groups. The
discrepancy-based algorithm is slightly slower (for bounded VC dimension), but
it computes a smaller fair $\varepsilon$-net. Notably, we reduce the fair
geometric hitting set problem to finding fair $\varepsilon$-nets. This results
in a $O(\log \mathsf{OPT} \times \log k)$ approximation of a fair geometric
hitting set. Additionally, we show that under certain input distributions,
constructing fair $\varepsilon$-samples can be infeasible, highlighting
limitations in fair sampling. Beyond the theoretical guarantees, our
experimental results validate the practical effectiveness of the proposed
algorithms. In particular, we achieve zero unfairness with only a modest
increase in output size compared to the unfair setting.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [39] [Overview of the TREC 2021 deep learning track](https://arxiv.org/abs/2507.08191)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Daniel Campos,Jimmy Lin*

Main category: cs.IR

TL;DR: 这是TREC深度学习赛道第三年，利用MS MARCO数据集，更新集合规模大增，深度神经排序模型优于传统方法，单阶段检索有表现但不如多阶段，还探讨集合更新带来的问题。


<details>
  <summary>Details</summary>
Motivation: 在TREC深度学习赛道持续研究信息检索任务，借助MS MARCO数据集推动技术发展。

Method: 利用MS MARCO数据集，对比深度神经排序模型和传统检索方法，比较单阶段和多阶段检索性能。

Result: 深度神经排序模型优于传统方法，单阶段检索有一定表现但不如多阶段。

Conclusion: 集合规模增加和数据更新引发NIST判断完整性和训练标签质量问题值得探讨。

Abstract: This is the third year of the TREC Deep Learning track. As in previous years,
we leverage the MS MARCO datasets that made hundreds of thousands of human
annotated training labels available for both passage and document ranking
tasks. In addition, this year we refreshed both the document and the passage
collections which also led to a nearly four times increase in the document
collection size and nearly $16$ times increase in the size of the passage
collection. Deep neural ranking models that employ large scale pretraininig
continued to outperform traditional retrieval methods this year. We also found
that single stage retrieval can achieve good performance on both tasks although
they still do not perform at par with multistage retrieval pipelines. Finally,
the increase in the collection size and the general data refresh raised some
questions about completeness of NIST judgments and the quality of the training
labels that were mapped to the new collections from the old ones which we
discuss in this report.

</details>


### [40] [Towards Efficient Quantity Retrieval from Text:an Approach via Description Parsing and Weak Supervision](https://arxiv.org/abs/2507.08322)
*Yixuan Cao,Zhengrong Chen,Chengxuan Xia,Kun Wu,Ping Luo*

Main category: cs.IR

TL;DR: 本文提出数量检索任务，介绍基于描述解析的框架，构建数据集提升学习效果，实验表明方法显著提高检索准确率。


<details>
  <summary>Details</summary>
Motivation: 许多长尾定量事实埋于非结构化文档，难以获取，为支持数据驱动决策，提出数量检索任务。

Method: 引入基于描述解析的框架将文本转换为（描述，数量）对；基于数量共现的弱监督构建大型释义数据集。

Result: 在金融年报语料库和新注释的数量描述数据集上评估，方法将top - 1检索准确率从30.98%显著提高到64.66%。

Conclusion: 提出的数量检索方法及相关框架和数据集有效提升了检索准确率。

Abstract: Quantitative facts are continually generated by companies and governments,
supporting data-driven decision-making. While common facts are structured, many
long-tail quantitative facts remain buried in unstructured documents, making
them difficult to access. We propose the task of Quantity Retrieval: given a
description of a quantitative fact, the system returns the relevant value and
supporting evidence. Understanding quantity semantics in context is essential
for this task. We introduce a framework based on description parsing that
converts text into structured (description, quantity) pairs for effective
retrieval. To improve learning, we construct a large paraphrase dataset using
weak supervision based on quantity co-occurrence. We evaluate our approach on a
large corpus of financial annual reports and a newly annotated quantity
description dataset. Our method significantly improves top-1 retrieval accuracy
from 30.98 percent to 64.66 percent.

</details>


### [41] [DS@GT at LongEval: Evaluating Temporal Performance in Web Search Systems and Topics with Two-Stage Retrieval](https://arxiv.org/abs/2507.08360)
*Anthony Miyaguchi,Imran Afrulbasha,Aleksandar Pramov*

Main category: cs.IR

TL;DR: DS@GT团队参加CLEF 2025 LongEval实验室评估IR系统，分析Qwant数据集，用两阶段检索系统，最佳系统平均NDCG@10为0.296，最佳成绩0.395，代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统IR模型在静态数据集上训练，随网络内容演变性能易下降，参加LongEval实验室评估IR系统。

Method: 对Qwant数据集进行含主题建模的探索性数据分析，两阶段检索系统采用稀疏关键词搜索、查询扩展和文档重排。

Result: 最佳系统在整个训练和测试数据集上平均NDCG@10为0.296，2023 - 05最佳成绩0.395。

Conclusion: 该两阶段检索系统在评估IR系统性能上有一定效果，代码开源可进一步研究。

Abstract: Information Retrieval (IR) models are often trained on static datasets,
making them vulnerable to performance degradation as web content evolves. The
DS@GT competition team participated in the Longitudinal Evaluation of Model
Performance (LongEval) lab at CLEF 2025, which evaluates IR systems across
temporally distributed web snapshots. Our analysis of the Qwant web dataset
includes exploratory data analysis with topic modeling over time. The two-phase
retrieval system employs sparse keyword searches, utilizing query expansion and
document reranking. Our best system achieves an average NDCG@10 of 0.296 across
the entire training and test dataset, with an overall best score of 0.395 on
2023-05. The accompanying source code for this paper is at
https://github.com/dsgt-arc/longeval-2025

</details>


### [42] [CUE-RAG: Towards Accurate and Cost-Efficient Graph-Based RAG via Multi-Partite Graph and Query-Driven Iterative Retrieval](https://arxiv.org/abs/2507.08445)
*Yaodong Su,Yixiang Fang,Yingli Zhou,Quanqing Xu,Chuanhui Yang*

Main category: cs.IR

TL;DR: 提出CUE - RAG方法解决现有图基RAG方法问题，实验显示其在QA基准上显著优于基线，有效且具成本效益。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在问答中因缺乏特定领域和最新知识表现受限，现有图基RAG方法存在图质量差问题。

Method: 提出CUE - RAG，包含多部分图索引、混合提取策略和Q - Iter查询驱动迭代检索策略。

Result: 在三个QA基准上显著优于现有基线，准确率最高提升99.33%，F1分数最高提升113.51%，索引成本降低72.58%，不使用LLM索引也能匹配或超越基线。

Conclusion: CUE - RAG在推进图基RAG系统方面有效且具成本效益。

Abstract: Despite the remarkable progress of Large Language Models (LLMs), their
performance in question answering (QA) remains limited by the lack of
domain-specific and up-to-date knowledge. Retrieval-Augmented Generation (RAG)
addresses this limitation by incorporating external information, often from
graph-structured data. However, existing graph-based RAG methods suffer from
poor graph quality due to incomplete extraction and insufficient utilization of
query information during retrieval. To overcome these limitations, we propose
CUE-RAG, a novel approach that introduces (1) a multi-partite graph index
incorporates text Chunks, knowledge Units, and Entities to capture semantic
content at multiple levels of granularity, (2) a hybrid extraction strategy
that reduces LLM token usage while still producing accurate and disambiguated
knowledge units, and (3) Q-Iter, a query-driven iterative retrieval strategy
that enhances relevance through semantic search and constrained graph
traversal. Experiments on three QA benchmarks show that CUE-RAG significantly
outperforms state-of-the-art baselines, achieving up to 99.33% higher Accuracy
and 113.51% higher F1 score while reducing indexing costs by 72.58%.
Remarkably, CUE-RAG matches or outperforms baselines even without using an LLM
for indexing. These results demonstrate the effectiveness and cost-efficiency
of CUE-RAG in advancing graph-based RAG systems.

</details>


### [43] [Improving Korean-English Cross-Lingual Retrieval: A Data-Centric Study of Language Composition and Model Merging](https://arxiv.org/abs/2507.08480)
*Youngjoon Jang,Junyoung Son,Taemin Lee,Seongtae Hong,Heuiseok Lim*

Main category: cs.IR

TL;DR: 研究训练数据组成对跨语言和单语言信息检索性能的影响，发现语言组成影响性能，提出模型合并缓解权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前训练数据组成对跨语言信息检索（CLIR）和单语言信息检索（IR）性能的影响研究不足，需系统探究。

Method: 构建韩英平行数据集，用不同语言组合训练检索模型。

Result: 训练数据的语言组成显著影响IR性能，特定语言对提升CLIR性能但降低单语言IR性能，模型合并可缓解此权衡。

Conclusion: 强调训练数据语言配置对CLIR和单语言IR的影响，模型合并是优化任务性能的可行策略。

Abstract: With the increasing utilization of multilingual text information,
Cross-Lingual Information Retrieval (CLIR) has become a crucial research area.
However, the impact of training data composition on both CLIR and Mono-Lingual
Information Retrieval (IR) performance remains under-explored. To
systematically investigate this data-centric aspect, we construct
linguistically parallel Korean-English datasets and train retrieval models with
various language combinations. Our experiments reveal that the language
composition of training data significantly influences IR performance,
exhibiting important inter-lingual correlations: CLIR performance improves with
specific language pairs, while Mono-Lingual IR performance declines. Our work
demonstrates that Model Merging can effectively mitigate this trade-off,
achieving strong CLIR results while preserving Mono-Lingual IR capabilities.
Our findings underscore the effects of linguistic configuration of training
data on both CLIR and Mono-Lingual IR, and present Model Merging as a viable
strategy to optimize performance across these tasks.

</details>


### [44] [Digital gazetteers: review and prospects for place name knowledge bases](https://arxiv.org/abs/2507.08553)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.IR

TL;DR: 本文回顾数字地名词典技术，强调未来需丰富地名表示、研究地点身份和位置的时间演变及开发更有效的数据集成方法。


<details>
  <summary>Details</summary>
Motivation: 理解数字地名词典技术，提高其在信息检索中的有效性。

Method: 对数据来源、组件、软件、数据管理技术、数据质量、志愿数据及匹配相同现实地点的方法进行综述。

Result: 无明确具体结果，仅进行了相关技术的回顾。

Conclusion: 未来需要在丰富地名表示、研究地点身份和位置的时间演变以及开发更有效的数据集成方法方面开展工作。

Abstract: Gazetteers typically store data on place names, place types and the
associated coordinates. They play an essential role in disambiguating place
names in online geographical information retrieval systems for navigation and
mapping, detecting and disambiguating place names in text, and providing
coordinates. Currently there are many gazetteers in use derived from many
sources, with no commonly accepted standard for encoding the data. Most
gazetteers are also very limited in the extent to which they represent the
multiple facets of the named places yet they have potential to assist user
search for locations with specific physical, commercial, social or cultural
characteristics. With a view to understanding digital gazetteer technologies
and advancing their future effectiveness for information retrieval, we provide
a review of data sources, components, software and data management
technologies, data quality and volunteered data, and methods for matching
sources that refer to the same real-world places. We highlight the need for
future work on richer representation of named places, the temporal evolution of
place identity and location, and the development of more effective methods for
data integration.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [An Enhanced Privacy-preserving Federated Few-shot Learning Framework for Respiratory Disease Diagnosis](https://arxiv.org/abs/2507.08050)
*Ming Wang,Zhaoyang Duan,Dong Xue,Fangzhou Liu,Zhongheng Zhang*

Main category: cs.LG

TL;DR: 针对呼吸系统疾病诊断中标记数据有限和隐私保护问题，提出含隐私保护机制的联邦少样本学习框架，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 医疗数据标注劳动密集，资源受限地区高质量标注数据集稀缺，患者隐私问题阻碍数据共享，现有集中式数据驱动方法会损害数据隐私。

Method: 提出元随机梯度下降算法缓解数据不足时过拟合问题；在本地数据训练私有模型时加入高斯分布差分隐私噪声防止梯度泄露；采用加权平均算法聚合不同客户端的本地诊断模型。

Result: 所提方法在实现差分隐私的情况下取得了良好结果，能有效利用不同结构、类别和分布的数据诊断呼吸系统疾病。

Conclusion: 含隐私保护机制的联邦少样本学习框架可解决呼吸系统疾病诊断中标记数据有限和隐私保护问题。

Abstract: The labor-intensive nature of medical data annotation presents a significant
challenge for respiratory disease diagnosis, resulting in a scarcity of
high-quality labeled datasets in resource-constrained settings. Moreover,
patient privacy concerns complicate the direct sharing of local medical data
across institutions, and existing centralized data-driven approaches, which
rely on amounts of available data, often compromise data privacy. This study
proposes a federated few-shot learning framework with privacy-preserving
mechanisms to address the issues of limited labeled data and privacy protection
in diagnosing respiratory diseases. In particular, a meta-stochastic gradient
descent algorithm is proposed to mitigate the overfitting problem that arises
from insufficient data when employing traditional gradient descent methods for
neural network training. Furthermore, to ensure data privacy against gradient
leakage, differential privacy noise from a standard Gaussian distribution is
integrated into the gradients during the training of private models with local
data, thereby preventing the reconstruction of medical images. Given the
impracticality of centralizing respiratory disease data dispersed across
various medical institutions, a weighted average algorithm is employed to
aggregate local diagnostic models from different clients, enhancing the
adaptability of a model across diverse scenarios. Experimental results show
that the proposed method yields compelling results with the implementation of
differential privacy, while effectively diagnosing respiratory diseases using
data from different structures, categories, and distributions.

</details>


### [46] [Tree-Structured Parzen Estimator Can Solve Black-Box Combinatorial Optimization More Efficiently](https://arxiv.org/abs/2507.08053)
*Kenshin Abe,Yunzhuo Wang,Shuhei Watanabe*

Main category: cs.LG

TL;DR: 提出针对TPE的高效组合优化算法，实验验证其优于原TPE且算法已集成到Optuna。


<details>
  <summary>Details</summary>
Motivation: 现有TPE在深度学习领域讨论多，但组合优化是未触及的重要话题，需为TPE提出组合优化算法。

Method: 先将TPE中的分类核与数值核进行泛化，引入距离结构，再对新核进行修改以处理大组合搜索空间。

Result: 在合成问题实验中，所提方法用更少评估次数找到更好解。

Conclusion: 所提算法能有效解决TPE中的组合优化问题，且已集成到开源框架Optuna。

Abstract: Tree-structured Parzen estimator (TPE) is a versatile hyperparameter
optimization (HPO) method supported by popular HPO tools. Since these HPO tools
have been developed in line with the trend of deep learning (DL), the problem
setups often used in the DL domain have been discussed for TPE such as
multi-objective optimization and multi-fidelity optimization. However, the
practical applications of HPO are not limited to DL, and black-box
combinatorial optimization is actively utilized in some domains, e.g.,
chemistry and biology. As combinatorial optimization has been an untouched, yet
very important, topic in TPE, we propose an efficient combinatorial
optimization algorithm for TPE. In this paper, we first generalize the
categorical kernel with the numerical kernel in TPE, enabling us to introduce a
distance structure to the categorical kernel. Then we discuss modifications for
the newly developed kernel to handle a large combinatorial search space. These
modifications reduce the time complexity of the kernel calculation with respect
to the size of a combinatorial search space. In the experiments using synthetic
problems, we verified that our proposed method identifies better solutions with
fewer evaluations than the original TPE. Our algorithm is available in Optuna,
an open-source framework for HPO.

</details>


### [47] [Rethinking Spatio-Temporal Anomaly Detection: A Vision for Causality-Driven Cybersecurity](https://arxiv.org/abs/2507.08177)
*Arun Vignesh Malarkkan,Haoyue Bai,Xinyuan Wang,Anjali Kaushik,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: 本文倡导从因果学习角度推进分布式基础设施的异常检测，确定三个关键方向，用实例说明因果模型优势并规划未来研究议程，旨在建立新的异常检测系统研究路径。


<details>
  <summary>Details</summary>
Motivation: 随着网络物理系统的互联和分布式发展，确保其对网络攻击的恢复能力至关重要，而当前数据驱动方法存在可解释性、适应性和鲁棒性等问题。

Method: 从因果学习角度出发，确定因果图分析、多视图融合和持续因果图学习三个关键方向。

Result: 因果模型能提供早期预警信号和根因归因，解决黑盒检测器的局限性。

Conclusion: 规划了以多模态、生成式AI驱动和可扩展自适应因果框架为核心的未来研究议程，有望推动网络安全研究范式转变。

Abstract: As cyber-physical systems grow increasingly interconnected and spatially
distributed, ensuring their resilience against evolving cyberattacks has become
a critical priority. Spatio-Temporal Anomaly detection plays an important role
in ensuring system security and operational integrity. However, current
data-driven approaches, largely driven by black-box deep learning, face
challenges in interpretability, adaptability to distribution shifts, and
robustness under evolving system dynamics. In this paper, we advocate for a
causal learning perspective to advance anomaly detection in spatially
distributed infrastructures that grounds detection in structural cause-effect
relationships. We identify and formalize three key directions: causal graph
profiling, multi-view fusion, and continual causal graph learning, each
offering distinct advantages in uncovering dynamic cause-effect structures
across time and space. Drawing on real-world insights from systems such as
water treatment infrastructures, we illustrate how causal models provide early
warning signals and root cause attribution, addressing the limitations of
black-box detectors. Looking ahead, we outline the future research agenda
centered on multi-modality, generative AI-driven, and scalable adaptive causal
frameworks. Our objective is to lay a new research trajectory toward scalable,
adaptive, explainable, and spatially grounded anomaly detection systems. We
hope to inspire a paradigm shift in cybersecurity research, promoting
causality-driven approaches to address evolving threats in interconnected
infrastructures.

</details>


### [48] [Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions](https://arxiv.org/abs/2507.08068)
*Simon Matrenok,Skander Moalla,Caglar Gulcehre*

Main category: cs.LG

TL;DR: 提出QRPO方法，可从点式绝对奖励中学习，保持DPO类方法简单性和离线适用性，在评估中表现佳且减少长度偏差。


<details>
  <summary>Details</summary>
Motivation: 现有用点式绝对奖励对齐大语言模型需在线、策略内算法，DPO等简单方法只能从偏好对或相对信号学习，需弥合差距。

Method: 引入Quantile Reward Policy Optimization (QRPO)，使用分位数奖励实现对KL正则化RL目标的闭式解进行回归。

Result: QRPO在聊天和编码评估中始终优于DPO、REBEL和SimPO；用鲁棒奖励训练比转换为偏好训练导致的长度偏差更小。

Conclusion: QRPO是一种有效方法，可从点式绝对奖励学习，兼具简单性和离线适用性，还能减少长度偏差。

Abstract: Aligning large language models with pointwise absolute rewards has so far
required online, on-policy algorithms such as PPO and GRPO. In contrast,
simpler methods that can leverage offline or off-policy data, such as DPO and
REBEL, are limited to learning from preference pairs or relative signals. To
bridge this gap, we introduce \emph{Quantile Reward Policy Optimization}
(QRPO), which learns from pointwise absolute rewards while preserving the
simplicity and offline applicability of DPO-like methods. QRPO uses quantile
rewards to enable regression to the closed-form solution of the KL-regularized
RL objective. This reward yields an analytically tractable partition function,
removing the need for relative signals to cancel this term. Moreover, QRPO
scales with increased compute to estimate quantile rewards, opening a new
dimension for pre-computation scaling. Empirically, QRPO consistently achieves
top performance on chat and coding evaluations -- reward model scores,
AlpacaEval 2, and LeetCode -- compared to DPO, REBEL, and SimPO across diverse
datasets and 8B-scale models. Finally, we find that training with robust
rewards instead of converting them to preferences induces less length bias.

</details>


### [49] [Low-rank Momentum Factorization for Memory Efficient Training](https://arxiv.org/abs/2507.08091)
*Pouria Mahdavinia,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: 提出Momentum Factorized SGD (MoFaSGD)方法用于大模型微调，有理论收敛保证且在实验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 大模型微调时，状态优化器带来显著内存挑战，现有方法存在不足。

Method: 提出MoFaSGD，维护一阶动量的动态更新低秩SVD表示，自适应更新优化子空间，利用低秩动量因子进行谱归一化更新。

Result: 建立理论收敛保证，在大语言模型对齐基准测试中，内存减少与性能间取得有竞争力的平衡。

Conclusion: MoFaSGD是一种有效的大模型微调方法，代码已开源。

Abstract: Fine-tuning large foundation models presents significant memory challenges
due to stateful optimizers like AdamW, often requiring several times more GPU
memory than inference. While memory-efficient methods like parameter-efficient
fine-tuning (e.g., LoRA) and optimizer state compression exist, recent
approaches like GaLore bridge these by using low-rank gradient projections and
subspace moment accumulation. However, such methods may struggle with fixed
subspaces or computationally costly offline resampling (e.g., requiring
full-matrix SVDs). We propose Momentum Factorized SGD (MoFaSGD), which
maintains a dynamically updated low-rank SVD representation of the first-order
momentum, closely approximating its full-rank counterpart throughout training.
This factorization enables a memory-efficient fine-tuning method that
adaptively updates the optimization subspace at each iteration. Crucially,
MoFaSGD leverages the computed low-rank momentum factors to perform efficient
spectrally normalized updates, offering an alternative to subspace moment
accumulation. We establish theoretical convergence guarantees for MoFaSGD,
proving it achieves an optimal rate for non-convex stochastic optimization
under standard assumptions. Empirically, we demonstrate MoFaSGD's effectiveness
on large language model alignment benchmarks, achieving a competitive trade-off
between memory reduction (comparable to LoRA) and performance compared to
state-of-the-art low-rank optimization methods. Our implementation is available
at https://github.com/pmahdavi/MoFaSGD.

</details>


### [50] [PDE-aware Optimizer for Physics-informed Neural Networks](https://arxiv.org/abs/2507.08118)
*Hardik Shukla,Manurag Khullar,Vismay Churiwala*

Main category: cs.LG

TL;DR: 本文提出PDE感知优化器，在PINNs训练中表现良好，未来需在更大架构和硬件加速器上扩展。


<details>
  <summary>Details</summary>
Motivation: 标准优化器（如Adam）在解决PDE时难以平衡损失项，二阶优化器计算成本高。

Method: 提出基于样本PDE残差梯度方差调整参数更新的PDE感知优化器。

Result: 在1D Burgers'、Allen - Cahn和Korteweg - de Vries(KdV)方程上测试，收敛更平滑、绝对误差更低。

Conclusion: PDE残差感知自适应在增强PINNs训练稳定性方面有效，未来需在更大架构和硬件加速器上扩展。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving partial differential equations (PDEs) by embedding physical
constraints into the loss function. However, standard optimizers such as Adam
often struggle to balance competing loss terms, particularly in stiff or
ill-conditioned systems. In this work, we propose a PDE-aware optimizer that
adapts parameter updates based on the variance of per-sample PDE residual
gradients. This method addresses gradient misalignment without incurring the
heavy computational costs of second-order optimizers such as SOAP. We benchmark
the PDE-aware optimizer against Adam and SOAP on 1D Burgers', Allen-Cahn and
Korteweg-de Vries(KdV) equations. Across both PDEs, the PDE-aware optimizer
achieves smoother convergence and lower absolute errors, particularly in
regions with sharp gradients. Our results demonstrate the effectiveness of PDE
residual-aware adaptivity in enhancing stability in PINNs training. While
promising, further scaling on larger architectures and hardware accelerators
remains an important direction for future research.

</details>


### [51] [Quasi-Random Physics-informed Neural Networks](https://arxiv.org/abs/2507.08121)
*Tianchi Yu,Ivan Oseledets*

Main category: cs.LG

TL;DR: 提出准随机物理信息神经网络（QRPINNs），理论上收敛率优于PINNs，实验显示在高维PDEs中表现出色，与自适应采样结合可提升性能。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络性能对采样点敏感，准蒙特卡罗方法在高维问题表现出色，故提出改进采样方式的网络。

Method: 提出QRPINNs，用低差异序列采样代替直接从域中取随机点。

Result: 理论上QRPINNs收敛率优于PINNs，实验中QRPINNs显著优于PINNs和一些代表性自适应采样方法，结合自适应采样性能可进一步提升。

Conclusion: QRPINNs是解决偏微分方程的有效方法，结合自适应采样可进一步提高性能。

Abstract: Physics-informed neural networks have shown promise in solving partial
differential equations (PDEs) by integrating physical constraints into neural
network training, but their performance is sensitive to the sampling of points.
Based on the impressive performance of quasi Monte-Carlo methods in high
dimensional problems, this paper proposes Quasi-Random Physics-Informed Neural
Networks (QRPINNs), which use low-discrepancy sequences for sampling instead of
random points directly from the domain. Theoretically, QRPINNs have been proven
to have a better convergence rate than PINNs. Empirically, experiments
demonstrate that QRPINNs significantly outperform PINNs and some representative
adaptive sampling methods, especially in high-dimensional PDEs. Furthermore,
combining QRPINNs with adaptive sampling can further improve the performance.

</details>


### [52] [Physics-Informed Neural Networks with Hard Nonlinear Equality and Inequality Constraints](https://arxiv.org/abs/2507.08124)
*Ashfaq Iftakher,Rahul Golder,M. M. Faruque Hasan*

Main category: cs.LG

TL;DR: 本文开发了KKT - Hardnet PINN架构，可高精度满足约束条件，应用于测试问题和实际化工过程模拟，比其他方法更准确。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络不能严格满足约束条件，在工程系统中违反控制定律会降低模型预测的可靠性和一致性。

Method: 开发KKT - Hardnet架构，通过求解距离最小化问题的KKT条件投影到可行域，用对数指数变换重构非线性KKT条件。

Result: 在测试问题和实际化工过程模拟中，与多层感知器和PINNs相比，KKT - Hardnet实现了更高精度和严格的约束满足。

Conclusion: 该方法可将领域知识融入机器学习，实现复杂系统的可靠混合建模。

Abstract: Traditional physics-informed neural networks (PINNs) do not guarantee strict
constraint satisfaction. This is problematic in engineering systems where minor
violations of governing laws can significantly degrade the reliability and
consistency of model predictions. In this work, we develop KKT-Hardnet, a PINN
architecture that enforces both linear and nonlinear equality and inequality
constraints up to machine precision. It leverages a projection onto the
feasible region through solving Karush-Kuhn-Tucker (KKT) conditions of a
distance minimization problem. Furthermore, we reformulate the nonlinear KKT
conditions using log-exponential transformation to construct a general sparse
system with only linear and exponential terms, thereby making the projection
differentiable. We apply KKT-Hardnet on both test problems and a real-world
chemical process simulation. Compared to multilayer perceptrons and PINNs,
KKT-Hardnet achieves higher accuracy and strict constraint satisfaction. This
approach allows the integration of domain knowledge into machine learning
towards reliable hybrid modeling of complex systems.

</details>


### [53] [ALCo-FM: Adaptive Long-Context Foundation Model for Accident Prediction](https://arxiv.org/abs/2507.08153)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Rajiv Ramnath*

Main category: cs.LG

TL;DR: 提出ALCo - FM模型用于交通事故风险预测，表现优于20多个基线模型。


<details>
  <summary>Details</summary>
Motivation: 交通事故是罕见但高影响事件，需要长上下文多模态推理进行准确风险预测。

Method: 计算波动预分数动态选择上下文窗口，通过浅层交叉注意力编码和融合多模态数据，结合局部GAT层和BigBird风格稀疏全局变压器，使用蒙特卡罗Dropout获取置信度，用类加权损失训练。

Result: 在15个美国城市数据上训练，在保留城市上微调，实现0.94准确率、0.92 F1和0.04 ECE。

Conclusion: ALCo - FM在大规模城市风险预测中优于20多个最先进的基线模型。

Abstract: Traffic accidents are rare, yet high-impact events that require long-context
multimodal reasoning for accurate risk forecasting. In this paper, we introduce
ALCo-FM, a unified adaptive long-context foundation model that computes a
volatility pre-score to dynamically select context windows for input data and
encodes and fuses these multimodal data via shallow cross attention. Following
a local GAT layer and a BigBird-style sparse global transformer over H3
hexagonal grids, coupled with Monte Carlo dropout for confidence, the model
yields superior, well-calibrated predictions. Trained on data from 15 US cities
with a class-weighted loss to counter label imbalance, and fine-tuned with
minimal data on held-out cities, ALCo-FM achieves 0.94 accuracy, 0.92 F1, and
an ECE of 0.04, outperforming more than 20 state-of-the-art baselines in
large-scale urban risk prediction. Code and dataset are available at:
https://github.com/PinakiPrasad12/ALCo-FM

</details>


### [54] [Just Read the Question: Enabling Generalization to New Assessment Items with Text Awareness](https://arxiv.org/abs/2507.08154)
*Arisha Khan,Nathaniel Li,Tori Shen,Anna N. Rafferty*

Main category: cs.LG

TL;DR: 提出用机器学习改进教育评估，开发Text - LENS模型，在两个数据集测试，该模型在新题上表现好。


<details>
  <summary>Details</summary>
Motivation: 许多机器学习方法在教育评估中纳入新题目存在挑战，因其依赖历史数据。

Method: 扩展LENS部分变分自编码器开发Text - LENS，利用题目文本嵌入，在Eedi和LLM - Sim两个数据集上测试。

Result: Text - LENS在见过的题目上与LENS表现相当，在涉及未见过的题目上多种条件下表现更好。

Conclusion: Text - LENS能有效从新题目中学习学生能力并对学生表现进行预测。

Abstract: Machine learning has been proposed as a way to improve educational assessment
by making fine-grained predictions about student performance and learning
relationships between items. One challenge with many machine learning
approaches is incorporating new items, as these approaches rely heavily on
historical data. We develop Text-LENS by extending the LENS partial variational
auto-encoder for educational assessment to leverage item text embeddings, and
explore the impact on predictive performance and generalization to previously
unseen items. We examine performance on two datasets: Eedi, a publicly
available dataset that includes item content, and LLM-Sim, a novel dataset with
test items produced by an LLM. We find that Text-LENS matches LENS' performance
on seen items and improves upon it in a variety of conditions involving unseen
items; it effectively learns student proficiency from and makes predictions
about student performance on new items.

</details>


### [55] [Data Generation without Function Estimation](https://arxiv.org/abs/2507.08239)
*Hadi Daneshmand,Ashkan Soleymani*

Main category: cs.LG

TL;DR: 提出无估计生成方法，利用相互作用粒子物理进展，无需函数估计等实现数据生成，理论和实验验证可行。


<details>
  <summary>Details</summary>
Motivation: 估计得分函数在生成模型中计算和统计上具有挑战性，探索避免函数估计进行数据生成的方法。

Method: 基于相互作用粒子物理的最新进展，用（逆）梯度下降确定性更新点的位置，在平均场机制下将均匀分布转换为任意数据分布。

Result: 理论和实验都表明能利用相关进展开发新的生成方法。

Conclusion: 提出的无估计生成方法是可行的，可基于相互作用粒子物理进展开发新生成方法。

Abstract: Estimating the score function (or other population-density-dependent
functions) is a fundamental component of most generative models. However, such
function estimation is computationally and statistically challenging. Can we
avoid function estimation for data generation? We propose an estimation-free
generative method: A set of points whose locations are deterministically
updated with (inverse) gradient descent can transport a uniform distribution to
arbitrary data distribution, in the mean field regime, without function
estimation, training neural networks, and even noise injection. The proposed
method is built upon recent advances in the physics of interacting particles.
We show, both theoretically and experimentally, that these advances can be
leveraged to develop novel generative methods.

</details>


### [56] [Emotion Recognition in Older Adults with Quantum Machine Learning and Wearable Sensors](https://arxiv.org/abs/2507.08175)
*Md. Saif Hassan Onim,Travis S. Humble,Himanshu Thapliyal*

Main category: cs.LG

TL;DR: 研究仅通过生理信号推断情绪状态可行性，对比经典与混合量子机器学习方法，量子增强SVM表现佳，该方法有应用前景并为被动情绪监测奠基。


<details>
  <summary>Details</summary>
Motivation: 提供一种保护隐私的替代传统面部识别技术的情绪识别方法，且适用于有沟通障碍人群。

Method: 对经典机器学习算法和基于量子核模型的混合量子机器学习方法进行性能比较。

Result: 量子增强SVM在所有情绪类别分类性能上超经典方法，F1分数超80%，召回值最多提高36%。

Conclusion: 可穿戴传感器数据与量子机器学习结合能提高准确性和鲁棒性，该方法有应用前景，为临床和辅助生活环境中的被动情绪监测奠定早期基础。

Abstract: We investigate the feasibility of inferring emotional states exclusively from
physiological signals, thereby presenting a privacy-preserving alternative to
conventional facial recognition techniques. We conduct a performance comparison
of classical machine learning algorithms and hybrid quantum machine learning
(QML) methods with a quantum kernel-based model. Our results indicate that the
quantum-enhanced SVM surpasses classical counterparts in classification
performance across all emotion categories, even when trained on limited
datasets. The F1 scores over all classes are over 80% with around a maximum of
36% improvement in the recall values. The integration of wearable sensor data
with quantum machine learning not only enhances accuracy and robustness but
also facilitates unobtrusive emotion recognition. This methodology holds
promise for populations with impaired communication abilities, such as
individuals with Alzheimer's Disease and Related Dementias (ADRD) and veterans
with Post-Traumatic Stress Disorder (PTSD). The findings establish an early
foundation for passive emotional monitoring in clinical and assisted living
conditions.

</details>


### [57] [CTRLS: Chain-of-Thought Reasoning via Latent State-Transition](https://arxiv.org/abs/2507.08182)
*Junda Wu,Yuxin Xiong,Xintong Li,Zhengmian Hu,Tong Yu,Rui Wang,Xiang Chen,Jingbo Shang,Julian McAuley*

Main category: cs.LG

TL;DR: 传统CoT方法有局限，本文提出CTR框架将推理建模为MDP，用强化学习探索推理空间，实验证明有效果。


<details>
  <summary>Details</summary>
Motivation: 传统CoT方法依赖启发式采样，缺乏推理转换的结构化建模，限制了探索有效推理轨迹的能力。

Method: 将CoT推理表述为具有潜在状态转换的MDP，通过分布强化学习进行探索，采用结合epsilon - greedy探索和基于熵的正则化的策略迭代优化潜在状态转换。

Result: 在基准推理任务中，推理准确性、多样性和探索效率都有所提高。

Conclusion: 所提出的CTR框架能有效解决传统CoT方法的局限，在推理任务中表现良好。

Abstract: Chain-of-thought (CoT) reasoning enables large language models (LLMs) to
break down complex problems into interpretable intermediate steps,
significantly enhancing model transparency and performance in reasoning tasks.
However, conventional CoT methods rely on heuristic sampling without structured
modeling of reasoning transitions, constraining their ability to systematically
explore and discover diverse and effective reasoning trajectories. In this
work, we introduce CTRLS, a framework that formulates CoT reasoning as a Markov
decision process (MDP) with latent state transitions, enabling principled and
state-aware exploration via distributional reinforcement learning. By modelling
reasoning actions as explicit probability distributions in latent space, our
approach explicitly models epistemic uncertainty, facilitating robust
exploration of the reasoning space. As part of our framework, we introduce an
on-policy reinforcement learning strategy incorporating epsilon-greedy
exploration and entropy-based regularization to iteratively refine latent state
transitions without requiring additional fine-tuning of the underlying LLM.
Theoretical analyses provide evidence lower bounds (ELBO), theoretically
grounding our transition-aware modeling of latent reasoning dynamics. Further
experiments demonstrate improvements in reasoning accuracy, diversity, and
exploration efficiency across benchmark reasoning tasks.

</details>


### [58] [Monitoring Risks in Test-Time Adaptation](https://arxiv.org/abs/2507.08721)
*Mona Schirmer,Metod Jazbec,Christian A. Naesseth,Eric Nalisnick*

Main category: cs.LG

TL;DR: 本文提出将测试时自适应（TTA）与风险监测框架结合，以检测模型最终失效点，并展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: TTA只是临时解决测试时数据偏移问题的方法，模型最终可能退化到需离线重新训练，需要检测最终失效点。

Method: 将TTA与风险监测框架结合，扩展基于序贯测试和置信序列的现有监测工具，以适应模型在测试时更新且无测试标签的场景。

Result: 所提出的TTA监测框架在一系列数据集、分布偏移类型和TTA方法中有效。

Conclusion: 所提出的扩展使严格的统计风险监测能应用于TTA。

Abstract: Encountering shifted data at test time is a ubiquitous challenge when
deploying predictive models. Test-time adaptation (TTA) methods address this
issue by continuously adapting a deployed model using only unlabeled test data.
While TTA can extend the model's lifespan, it is only a temporary solution.
Eventually the model might degrade to the point that it must be taken offline
and retrained. To detect such points of ultimate failure, we propose pairing
TTA with risk monitoring frameworks that track predictive performance and raise
alerts when predefined performance criteria are violated. Specifically, we
extend existing monitoring tools based on sequential testing with confidence
sequences to accommodate scenarios in which the model is updated at test time
and no test labels are available to estimate the performance metrics of
interest. Our extensions unlock the application of rigorous statistical risk
monitoring to TTA, and we demonstrate the effectiveness of our proposed TTA
monitoring framework across a representative set of datasets, distribution
shift types, and TTA methods.

</details>


### [59] [EvA: Evolutionary Attacks on Graphs](https://arxiv.org/abs/2507.08212)
*Mohammad Sadegh Akhondzadeh,Soroush H. Zargarbashi,Jimin Cao,Aleksandar Bojchevski*

Main category: cs.LG

TL;DR: 现有图神经网络攻击存在问题，本文提出进化攻击算法EvA，可直接解决离散优化问题，对黑盒模型有效，实验显示比之前攻击平均多降低约11%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络攻击利用梯度信息扰动边，将优化问题从离散空间转化为连续空间，导致解非最优，且对不可微目标适应性受限。

Method: 对基于进化的算法进行简单有效的改进，直接解决离散优化问题，提出进化攻击算法EvA，可用于任何黑盒模型和目标。

Result: 设计了两种新攻击，降低了鲁棒性证书的有效性并破坏了共形集，内存复杂度与攻击预算呈线性关系，EvA比之前最佳攻击平均多降低约11%准确率。

Conclusion: 在设计攻击方面存在显著未挖掘的潜力。

Abstract: Even a slight perturbation in the graph structure can cause a significant
drop in the accuracy of graph neural networks (GNNs). Most existing attacks
leverage gradient information to perturb edges. This relaxes the attack's
optimization problem from a discrete to a continuous space, resulting in
solutions far from optimal. It also restricts the adaptability of the attack to
non-differentiable objectives. Instead, we introduce a few simple yet effective
enhancements of an evolutionary-based algorithm to solve the discrete
optimization problem directly. Our Evolutionary Attack (EvA) works with any
black-box model and objective, eliminating the need for a differentiable proxy
loss. This allows us to design two novel attacks that reduce the effectiveness
of robustness certificates and break conformal sets. The memory complexity of
our attack is linear in the attack budget. Among our experiments, EvA shows
$\sim$11\% additional drop in accuracy on average compared to the best previous
attack, revealing significant untapped potential in designing attacks.

</details>


### [60] [InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems](https://arxiv.org/abs/2507.08235)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Rajiv Ramnath*

Main category: cs.LG

TL;DR: 提出InsightBuild框架，结合因果分析与微调大语言模型解释建筑能耗模式，经真实数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 设施管理者缺乏对建筑异常能源使用的清晰解释。

Method: 先通过轻量级因果推理模块对建筑遥测数据进行Granger因果测试和结构因果发现，再用微调的大语言模型根据检测到的因果关系生成解释。

Result: 在两个真实数据集上评估，结合因果发现和大语言模型的自然语言生成能产生清晰、精确的解释。

Conclusion: InsightBuild框架有助于设施管理者诊断和缓解能源效率低下问题。

Abstract: Smart buildings generate vast streams of sensor and control data, but
facility managers often lack clear explanations for anomalous energy usage. We
propose InsightBuild, a two-stage framework that integrates causality analysis
with a fine-tuned large language model (LLM) to provide human-readable, causal
explanations of energy consumption patterns. First, a lightweight causal
inference module applies Granger causality tests and structural causal
discovery on building telemetry (e.g., temperature, HVAC settings, occupancy)
drawn from Google Smart Buildings and Berkeley Office datasets. Next, an LLM,
fine-tuned on aligned pairs of sensor-level causes and textual explanations,
receives as input the detected causal relations and generates concise,
actionable explanations. We evaluate InsightBuild on two real-world datasets
(Google: 2017-2022; Berkeley: 2018-2020), using expert-annotated ground-truth
causes for a held-out set of anomalies. Our results demonstrate that combining
explicit causal discovery with LLM-based natural language generation yields
clear, precise explanations that assist facility managers in diagnosing and
mitigating energy inefficiencies.

</details>


### [61] [Self-Supervised Learning-Based Multimodal Prediction on Prosocial Behavior Intentions](https://arxiv.org/abs/2507.08238)
*Abinay Reddy Naini,Zhaobo K. Zheng,Teruhisa Misu,Kumar Akash*

Main category: cs.LG

TL;DR: 本文针对移动场景下亲社会行为意图预测数据不足问题，提出自监督学习方法，提升模型性能并为相关领域提供参考。


<details>
  <summary>Details</summary>
Motivation: 当前移动场景下亲社会行为意图预测研究不足，且缺乏大规模标注数据集，难以有效训练深度学习模型。

Method: 采用自监督学习方法，利用现有生理和行为数据集的多模态数据进行预训练，再用小规模手动标注的亲社会行为数据集进行微调。

Result: 显著提升了模型性能，解决了数据稀缺问题。

Conclusion: 该方法为亲社会行为预测提供了更有效的基准，对改进智能车辆系统和人机交互有重要意义。

Abstract: Human state detection and behavior prediction have seen significant
advancements with the rise of machine learning and multimodal sensing
technologies. However, predicting prosocial behavior intentions in mobility
scenarios, such as helping others on the road, is an underexplored area.
Current research faces a major limitation. There are no large, labeled datasets
available for prosocial behavior, and small-scale datasets make it difficult to
train deep-learning models effectively. To overcome this, we propose a
self-supervised learning approach that harnesses multi-modal data from existing
physiological and behavioral datasets. By pre-training our model on diverse
tasks and fine-tuning it with a smaller, manually labeled prosocial behavior
dataset, we significantly enhance its performance. This method addresses the
data scarcity issue, providing a more effective benchmark for prosocial
behavior prediction, and offering valuable insights for improving intelligent
vehicle systems and human-machine interaction.

</details>


### [62] [CoreSPECT: Enhancing Clustering Algorithms via an Interplay of Density and Geometry](https://arxiv.org/abs/2507.08243)
*Chandra Sekhar Mukherjee,Joonyoung Bae,Jiapeng Zhang*

Main category: cs.LG

TL;DR: 本文提出CoreSPECT框架，通过利用分布与几何的相互作用提升聚类算法性能，在15个数据集上取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 发现并利用分布与几何之间常被忽视的相互作用来设计聚类增强框架，提升简单聚类算法性能。

Method: 设计CoreSPECT框架，将简单算法应用于策略性选择的区域，再用基于邻域图的多层传播过程将部分分区扩展为完整分区。

Result: 在15个数据集上，该框架使K - Means的ARI平均提高40%，GMM的ARI平均提高14%，常超越流形和密度聚类算法。

Conclusion: CoreSPECT框架能有效提升简单聚类算法性能，有理论保证、步骤有效性和抗噪性。

Abstract: Density and geometry have long served as two of the fundamental guiding
principles in clustering algorithm design, with algorithm usually focusing
either on the density structure of the data (e.g., HDBSCAN and Density Peak
Clustering) or the complexity of underlying geometry (e.g., manifold clustering
algorithms).
  In this paper, we identify and formalize a recurring but often overlooked
interaction between distribution and geometry and leverage this insight to
design our clustering enhancement framework CoreSPECT (Core Space
Projection-based Enhancement of Clustering Techniques). Our framework boosts
the performance of simple algorithms like K-Means and GMM by applying them to
strategically selected regions, then extending the partial partition to a
complete partition for the dataset using a novel neighborhood graph based
multi-layer propagation procedure.
  We apply our framework on 15 datasets from three different domains and obtain
consistent and substantial gain in clustering accuracy for both K-Means and
GMM. On average, our framework improves the ARI of K-Means by 40% and of GMM by
14%, often surpassing the performance of both manifold-based and recent
density-based clustering algorithms. We further support our framework with
initial theoretical guarantees, ablation to demonstrate the usefulness of the
individual steps and with evidence of robustness to noise.

</details>


### [63] [Quantum-Accelerated Neural Imputation with Large Language Models (LLMs)](https://arxiv.org/abs/2507.08255)
*Hossein Jamali*

Main category: cs.LG

TL;DR: 本文提出Quantum - UnIMP框架，用浅量子电路改进大语言模型数据插补，实验表明能降低插补误差、提高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在表格数据插补中依赖经典嵌入方法，难以捕捉复杂非线性关联，尤其是混合类型数据场景。

Method: 引入Quantum - UnIMP框架，用瞬时量子多项式（IQP）电路生成的量子特征映射替代传统经典输入嵌入。

Result: 在基准混合类型数据集实验中，Quantum - UnIMP对数值特征插补误差（RMSE）最多降低15.2%，对分类特征分类准确率（F1 - Score）提高8.7%。

Conclusion: 量子增强表示在复杂数据插补任务中具有巨大潜力，即使使用近期量子硬件。

Abstract: Missing data presents a critical challenge in real-world datasets,
significantly degrading the performance of machine learning models. While Large
Language Models (LLMs) have recently demonstrated remarkable capabilities in
tabular data imputation, exemplified by frameworks like UnIMP, their reliance
on classical embedding methods often limits their ability to capture complex,
non-linear correlations, particularly in mixed-type data scenarios encompassing
numerical, categorical, and textual features. This paper introduces
Quantum-UnIMP, a novel framework that integrates shallow quantum circuits into
an LLM-based imputation architecture. Our core innovation lies in replacing
conventional classical input embeddings with quantum feature maps generated by
an Instantaneous Quantum Polynomial (IQP) circuit. This approach enables the
model to leverage quantum phenomena such as superposition and entanglement,
thereby learning richer, more expressive representations of data and enhancing
the recovery of intricate missingness patterns. Our experiments on benchmark
mixed-type datasets demonstrate that Quantum-UnIMP reduces imputation error by
up to 15.2% for numerical features (RMSE) and improves classification accuracy
by 8.7% for categorical features (F1-Score) compared to state-of-the-art
classical and LLM-based methods. These compelling results underscore the
profound potential of quantum-enhanced representations for complex data
imputation tasks, even with near-term quantum hardware.

</details>


### [64] [A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning](https://arxiv.org/abs/2507.08267)
*Hiroshi Yoshihara,Taiki Yamaguchi,Yuichi Inoue*

Main category: cs.LG

TL;DR: 本文提出结合扩展监督微调（SFT）与在线推理强化学习（GRPO）的训练方法，提升大语言模型数学推理能力，实验验证其有效性并将开源框架。


<details>
  <summary>Details</summary>
Motivation: 现有监督微调（SFT）和强化学习（RL）训练范式缺乏有效结合方法，需提升大语言模型数学推理能力。

Method: 提出结合扩展SFT与GRPO的训练方法，先通过延长SFT提升模型准确性，再用GRPO提高令牌效率。

Result: 实验表明延长SFT 10个周期对性能突破至关重要，GRPO主要优化解决方案长度，在基准测试中表现优异。

Conclusion: 该方法为开发高精度和高效的数学推理器提供蓝图，且将开源框架促进后续研究。

Abstract: Enhancing the mathematical reasoning of Large Language Models (LLMs) is a
pivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning
(SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a
systematic methodology for combining them to maximize both accuracy and
efficiency remains largely unexplored. This paper introduces a practical and
effective training recipe that strategically integrates extended SFT with RL
from online inference (GRPO). We posit that these methods play complementary,
not competing, roles: a prolonged SFT phase first pushes the model's accuracy
to its limits, after which a GRPO phase dramatically improves token efficiency
while preserving this peak performance. Our experiments reveal that extending
SFT for as many as 10 epochs is crucial for performance breakthroughs, and that
the primary role of GRPO in this framework is to optimize solution length. The
efficacy of our recipe is rigorously validated through top-tier performance on
challenging benchmarks, including a high rank among over 2,200 teams in the
strictly leak-free AI Mathematical Olympiad (AIMO). This work provides the
community with a battle-tested blueprint for developing state-of-the-art
mathematical reasoners that are both exceptionally accurate and practically
efficient. To ensure full reproducibility and empower future research, we will
open-source our entire framework, including all code, model checkpoints, and
training configurations at
https://github.com/analokmaus/kaggle-aimo2-fast-math-r1.

</details>


### [65] [Data-Driven Dimensional Synthesis of Diverse Planar Four-bar Function Generation Mechanisms via Direct Parameterization](https://arxiv.org/abs/2507.08269)
*Woon Ryong Kim,Jaeheun Jung,Jeong Un Ha,Donghun Lee,Jae Kyung Shim*

Main category: cs.LG

TL;DR: 提出数据驱动框架解决平面四杆机构尺寸综合问题，实验显示该方法有效且有诸多优势。


<details>
  <summary>Details</summary>
Motivation: 平面四杆机构尺寸综合是运动学中具有挑战性的逆问题，传统方程求解和优化方法需被突破。

Method: 结合合成数据集、基于LSTM的神经网络处理顺序精确点，采用Mixture of Experts架构处理不同连杆类型，用新型仿真指标评估预测质量。

Result: 实验表明该方法能在各种配置下生成准确、无缺陷的连杆。

Conclusion: 该方法能实现直观高效的机构设计，为运动学设计的可扩展和灵活综合开辟新可能。

Abstract: Dimensional synthesis of planar four-bar mechanisms is a challenging inverse
problem in kinematics, requiring the determination of mechanism dimensions from
desired motion specifications. We propose a data-driven framework that bypasses
traditional equation-solving and optimization by leveraging supervised
learning. Our method combines a synthetic dataset, an LSTM-based neural network
for handling sequential precision points, and a Mixture of Experts (MoE)
architecture tailored to different linkage types. Each expert model is trained
on type-specific data and guided by a type-specifying layer, enabling both
single-type and multi-type synthesis. A novel simulation metric evaluates
prediction quality by comparing desired and generated motions. Experiments show
our approach produces accurate, defect-free linkages across various
configurations. This enables intuitive and efficient mechanism design, even for
non-expert users, and opens new possibilities for scalable and flexible
synthesis in kinematic design.

</details>


### [66] [Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training](https://arxiv.org/abs/2507.08284)
*Aleksei Ilin,Gor Matevosyan,Xueying Ma,Vladimir Eremin,Suhaa Dada,Muqun Li,Riyaaz Shaik,Haluk Noyan Tokgozoglu*

Main category: cs.LG

TL;DR: 提出轻量级语言模型安全护栏框架，用合成数据生成和对抗训练使小模型在内容审核任务中表现出色，减少计算开销并增强对抗攻击的能力。


<details>
  <summary>Details</summary>
Motivation: 探索小语言模型在内容审核任务中达到甚至超越大模型表现的方法，解决计算开销和对抗攻击问题。

Method: 通过高保真合成数据生成（从人工策划种子数据开始，进行查询扩充和释义，多轮策划）和受GAN启发的对抗训练（用强化学习引导生成器生成挑战样本微调安全分类器），结合高效LLM训练策略。

Result: 框架使小语言模型能作为强大的安全护栏，减少计算开销，增强对抗攻击的能力。

Conclusion: 该方法为AI系统的内容审核提供了可扩展且高效的解决方案。

Abstract: We introduce a lightweight yet highly effective safety guardrail framework
for language models, demonstrating that small-scale language models can
achieve, and even surpass, the performance of larger counterparts in content
moderation tasks. This is accomplished through high-fidelity synthetic data
generation and adversarial training. The synthetic data generation process
begins with human-curated seed data, which undergoes query augmentation and
paraphrasing to create diverse and contextually rich examples. This augmented
data is then subjected to multiple rounds of curation, ensuring high fidelity
and relevance. Inspired by recent advances in the Generative Adversarial
Network (GAN) architecture, our adversarial training employs reinforcement
learning to guide a generator that produces challenging synthetic examples.
These examples are used to fine-tune the safety classifier, enhancing its
ability to detect and mitigate harmful content. Additionally, we incorporate
strategies from recent research on efficient LLM training, leveraging the
capabilities of smaller models to improve the performance of larger generative
models. With iterative adversarial training and the generation of diverse,
high-quality synthetic data, our framework enables small language models (SLMs)
to serve as robust safety guardrails. This approach not only reduces
computational overhead but also enhances resilience against adversarial
attacks, offering a scalable and efficient solution for content moderation in
AI systems.

</details>


### [67] [CAS Condensed and Accelerated Silhouette: An Efficient Method for Determining the Optimal K in K-Means Clustering](https://arxiv.org/abs/2507.08311)
*Krishnendu Das,Sumit Gupta,Awadhesh Kumar*

Main category: cs.LG

TL;DR: 本文概述聚类中选择最优k值的策略，改进文本和图像数据聚类技术，基于特定方法计算k值，实验显示该方法在高维数据集上执行快且精度高。


<details>
  <summary>Details</summary>
Motivation: 聚类在大数据集中精度是主要挑战，要在复杂数据环境中平衡聚类精度和计算效率。

Method: 基于压缩轮廓法，结合局部结构、间隙统计、类一致性比率和聚类重叠指数等统计方法计算K - Means聚类的最优k值。

Result: 对比实验表明，该方法在高维数据集上执行速度快达99%，且保留精度和可扩展性。

Conclusion: 该方法适合实时聚类或资源利用少的高效聚类场景。

Abstract: Clustering is a critical component of decision-making in todays data-driven
environments. It has been widely used in a variety of fields such as
bioinformatics, social network analysis, and image processing. However,
clustering accuracy remains a major challenge in large datasets. This paper
presents a comprehensive overview of strategies for selecting the optimal value
of k in clustering, with a focus on achieving a balance between clustering
precision and computational efficiency in complex data environments. In
addition, this paper introduces improvements to clustering techniques for text
and image data to provide insights into better computational performance and
cluster validity. The proposed approach is based on the Condensed Silhouette
method, along with statistical methods such as Local Structures, Gap
Statistics, Class Consistency Ratio, and a Cluster Overlap Index CCR and
COIbased algorithm to calculate the best value of k for K-Means clustering. The
results of comparative experiments show that the proposed approach achieves up
to 99 percent faster execution times on high-dimensional datasets while
retaining both precision and scalability, making it highly suitable for real
time clustering needs or scenarios demanding efficient clustering with minimal
resource utilization.

</details>


### [68] [A Comprehensively Adaptive Architectural Optimization-Ingrained Quantum Neural Network Model for Cloud Workloads Prediction](https://arxiv.org/abs/2507.08317)
*Jitendra Kumar,Deepika Saxena,Kishu Gupta,Satyam Kumar,Ashutosh Kumar Singh*

Main category: cs.LG

TL;DR: 本文提出基于综合自适应架构优化的可变量子神经网络CA - QNN用于云服务工作负载预测，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络和深度学习模型处理动态云服务多样化、高维工作负载有挑战，训练优化有限。

Method: 提出CA - QNN，结合量子计算效率与结构和参数学习，将工作负载数据转换为量子比特处理，引入网络综合架构优化算法。

Result: 在四个异构云工作负载基准数据集上与七种先进方法对比，CA - QNN预测精度高，相比现有深度学习和基于QNN方法，预测误差最多降低93.40%和91.27%。

Conclusion: CA - QNN在云服务工作负载预测方面表现出色，能有效解决传统模型的问题。

Abstract: Accurate workload prediction and advanced resource reservation are
indispensably crucial for managing dynamic cloud services. Traditional neural
networks and deep learning models frequently encounter challenges with diverse,
high-dimensional workloads, especially during sudden resource demand changes,
leading to inefficiencies. This issue arises from their limited optimization
during training, relying only on parametric (inter-connection weights)
adjustments using conventional algorithms. To address this issue, this work
proposes a novel Comprehensively Adaptive Architectural Optimization-based
Variable Quantum Neural Network (CA-QNN), which combines the efficiency of
quantum computing with complete structural and qubit vector parametric
learning. The model converts workload data into qubits, processed through qubit
neurons with Controlled NOT-gated activation functions for intuitive pattern
recognition. In addition, a comprehensive architecture optimization algorithm
for networks is introduced to facilitate the learning and propagation of the
structure and parametric values in variable-sized QNNs. This algorithm
incorporates quantum adaptive modulation and size-adaptive recombination during
training process. The performance of CA-QNN model is thoroughly investigated
against seven state-of-the-art methods across four benchmark datasets of
heterogeneous cloud workloads. The proposed model demonstrates superior
prediction accuracy, reducing prediction errors by up to 93.40% and 91.27%
compared to existing deep learning and QNN-based approaches.

</details>


### [69] [scE$^2$TM: Toward Interpretable Single-Cell Embedding via Topic Modeling](https://arxiv.org/abs/2507.08355)
*Hegang Chen,Yuyin Lu,Zhiming Dai,Fu Lee Wang,Qing Li,Yanghui Rao*

Main category: cs.LG

TL;DR: 提出scE2TM模型用于单细胞RNA测序数据分析，评估显示其聚类性能优于7种方法，还提出新的可解释性评估基准。


<details>
  <summary>Details</summary>
Motivation: 以往单细胞嵌入主题模型主要通过定性分析评估可解释性，存在解释崩溃问题且忽视外部生物知识，限制分析性能。

Method: 提出外部知识引导的单细胞嵌入主题模型scE2TM，并提出新的可解释性评估基准，引入10个指标定量评估。

Result: 在20个scRNA - seq数据集上，scE2TM聚类性能显著优于7种最先进方法，其解释在多样性和与生物信号一致性上表现良好。

Conclusion: scE2TM能提供高质量细胞嵌入和强解释性，有助于全面分析scRNA - seq数据，更好揭示生物机制。

Abstract: Recent advances in sequencing technologies have enabled researchers to
explore cellular heterogeneity at single-cell resolution. Meanwhile,
interpretability has gained prominence parallel to the rapid increase in the
complexity and performance of deep learning models. In recent years, topic
models have been widely used for interpretable single-cell embedding learning
and clustering analysis, which we refer to as single-cell embedded topic
models. However, previous studies evaluated the interpretability of the models
mainly through qualitative analysis, and these single-cell embedded topic
models suffer from the potential problem of interpretation collapse.
Furthermore, their neglect of external biological knowledge constrains
analytical performance. Here, we present scE2TM, an external knowledge-guided
single-cell embedded topic model that provides a high-quality cell embedding
and strong interpretation, contributing to comprehensive scRNA-seq data
analysis. Our comprehensive evaluation across 20 scRNA-seq datasets
demonstrates that scE2TM achieves significant clustering performance gains
compared to 7 state-of-the-art methods. In addition, we propose a new
interpretability evaluation benchmark that introduces 10 metrics to
quantitatively assess the interpretability of single-cell embedded topic
models. The results show that the interpretation provided by scE2TM performs
encouragingly in terms of diversity and consistency with the underlying
biological signals, contributing to a better revealing of the underlying
biological mechanisms.

</details>


### [70] [Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN Model Generation from Text](https://arxiv.org/abs/2507.08362)
*Phuong Nam Lê,Charlotte Schneider-Depré,Alexandre Goossens,Alexander Stevens,Aurélie Leribaux,Johannes De Smedt*

Main category: cs.LG

TL;DR: 文章提出从文本提取BPMN模型的自动化流程，引入新注释数据集提升训练效果，方法在重建准确性上表现良好。


<details>
  <summary>Details</summary>
Motivation: 将文本流程文档转换为BPMN模型的过程耗时且成本高，现有方法难以处理写作风格和识别并行结构。

Method: 利用机器学习和大语言模型构建自动化流程，引入新注释数据集（在PET数据集基础上增加15个含32个并行网关的文档）进行模型训练。

Result: 所提方法在重建准确性方面表现出足够的性能。

Conclusion: 所提方法为组织加速BPMN模型创建提供了有前景的基础。

Abstract: Efficient planning, resource management, and consistent operations often rely
on converting textual process documents into formal Business Process Model and
Notation (BPMN) models. However, this conversion process remains time-intensive
and costly. Existing approaches, whether rule-based or machine-learning-based,
still struggle with writing styles and often fail to identify parallel
structures in process descriptions.
  This paper introduces an automated pipeline for extracting BPMN models from
text, leveraging the use of machine learning and large language models. A key
contribution of this work is the introduction of a newly annotated dataset,
which significantly enhances the training process. Specifically, we augment the
PET dataset with 15 newly annotated documents containing 32 parallel gateways
for model training, a critical feature often overlooked in existing datasets.
This addition enables models to better capture parallel structures, a common
but complex aspect of process descriptions. The proposed approach demonstrates
adequate performance in terms of reconstruction accuracy, offering a promising
foundation for organizations to accelerate BPMN model creation.

</details>


### [71] [Prediction of Lane Change Intentions of Human Drivers using an LSTM, a CNN and a Transformer](https://arxiv.org/abs/2507.08365)
*Francesco De Cristofaro,Felix Hofbaur,Aixi Yang,Arno Eichberger*

Main category: cs.LG

TL;DR: 本文用LSTM、CNN和Transformer网络预测人类驾驶员变道意图，对比不同输入配置下三网络结果，发现Transformer网络表现更好，准确率82.79% - 96.73%。


<details>
  <summary>Details</summary>
Motivation: 前车变道对自动驾驶车辆运动规划影响大，此前研究少有关注特定时间间隔内的变道预测，且缺乏不同架构对比及输入选择评估。

Method: 描述并实现LSTM、CNN和Transformer网络，利用公开数据集highD准备数据，设计网络，对比不同输入配置下三网络结果。

Result: Transformer网络表现优于其他网络，受过拟合影响小，不同输入配置下准确率82.79% - 96.73%，精度和召回率表现良好。

Conclusion: Transformer网络在预测人类驾驶员变道意图方面表现更好。

Abstract: Lane changes of preceding vehicles have a great impact on the motion planning
of automated vehicles especially in complex traffic situations. Predicting them
would benefit the public in terms of safety and efficiency. While many research
efforts have been made in this direction, few concentrated on predicting
maneuvers within a set time interval compared to predicting at a set prediction
time. In addition, there exist a lack of comparisons between different
architectures to try to determine the best performing one and to assess how to
correctly choose the input for such models. In this paper the structure of an
LSTM, a CNN and a Transformer network are described and implemented to predict
the intention of human drivers to perform a lane change. We show how the data
was prepared starting from a publicly available dataset (highD), which features
were used, how the networks were designed and finally we compare the results of
the three networks with different configurations of input data. We found that
transformer networks performed better than the other networks and was less
affected by overfitting. The accuracy of the method spanned from $82.79\%$ to
$96.73\%$ for different input configurations and showed overall good
performances considering also precision and recall.

</details>


### [72] [Advances in Machine Learning: Where Can Quantum Techniques Help?](https://arxiv.org/abs/2507.08379)
*Samarth Kashyap,Rohit K Ramakrishnan,Kumari Jyoti,Apoorva D Patel*

Main category: cs.LG

TL;DR: 本文综述量子机器学习（QML），介绍理论基础、评估关键进展、讨论NISQ设备挑战并指明未来方向，指出其有潜力但需克服障碍。


<details>
  <summary>Details</summary>
Motivation: 探索QML解决经典机器学习计算瓶颈，尤其是处理复杂数据集的潜力。

Method: 介绍QML理论基础，按数据类型和计算架构分类QML方法，评估关键进展，详细讨论NISQ设备挑战。

Result: 明确量子计算优势依赖问题，评估了关键进展的理论加速和实际局限，讨论了NISQ设备的挑战。

Conclusion: QML在特定应用有潜力，但在现实场景广泛应用需克服技术和方法障碍。

Abstract: Quantum Machine Learning (QML) represents a promising frontier at the
intersection of quantum computing and artificial intelligence, aiming to
leverage quantum computational advantages to enhance data-driven tasks. This
review explores the potential of QML to address the computational bottlenecks
of classical machine learning, particularly in processing complex datasets. We
introduce the theoretical foundations of QML, including quantum data encoding,
quantum learning theory and optimization techniques, while categorizing QML
approaches based on data type and computational architecture. It is
well-established that quantum computational advantages are problem-dependent,
and so potentially useful directions for QML need to be systematically
identified. Key developments, such as Quantum Principal Component Analysis,
quantum-enhanced sensing and applications in material science, are critically
evaluated for their theoretical speed-ups and practical limitations. The
challenges posed by Noisy Intermediate-Scale Quantum (NISQ) devices, including
hardware noise, scalability constraints and data encoding overheads, are
discussed in detail. We also outline future directions, emphasizing the need
for quantum-native algorithms, improved error correction, and realistic
benchmarks to bridge the gap between theoretical promise and practical
deployment. This comprehensive analysis underscores that while QML has
significant potential for specific applications such as quantum chemistry and
sensing, its broader utility in real-world scenarios remains contingent on
overcoming technological and methodological hurdles.

</details>


### [73] [Two-cluster test](https://arxiv.org/abs/2507.08382)
*Xinying Liu,Lianyu Hu,Mudi Jiang,Simen Zhang,Jun Lou,Zengyou He*

Main category: cs.LG

TL;DR: 本文引入两聚类检验问题，提出基于边界点的新方法，实验表明该方法能降低一类错误率，其实用性在聚类应用中得到验证。


<details>
  <summary>Details</summary>
Motivation: 现代聚类方法中使用经典两样本检验会导致一类错误率膨胀，需解决此偏差。

Method: 引入两聚类检验问题，提出基于两个子集边界点推导分析性p值的新方法。

Result: 在合成和真实数据集上实验显示，该方法相比经典两样本检验方法能显著降低一类错误率。

Conclusion: 两聚类检验方法有效降低一类错误率，其实用性在聚类应用中得到进一步验证。

Abstract: Cluster analysis is a fundamental research issue in statistics and machine
learning. In many modern clustering methods, we need to determine whether two
subsets of samples come from the same cluster. Since these subsets are usually
generated by certain clustering procedures, the deployment of classic
two-sample tests in this context would yield extremely smaller p-values,
leading to inflated Type-I error rate. To overcome this bias, we formally
introduce the two-cluster test issue and argue that it is a totally different
significance testing issue from conventional two-sample test. Meanwhile, we
present a new method based on the boundary points between two subsets to derive
an analytical p-value for the purpose of significance quantification.
Experiments on both synthetic and real data sets show that the proposed test is
able to significantly reduce the Type-I error rate, in comparison with several
classic two-sample testing methods. More importantly, the practical usage of
such two-cluster test is further verified through its applications in
tree-based interpretable clustering and significance-based hierarchical
clustering.

</details>


### [74] [Online Pre-Training for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2507.08387)
*Yongjae Shin,Jeonghye Kim,Whiyoung Jung,Sunghoon Hong,Deunsol Yoon,Youngsoo Jang,Geonhyeong Kim,Jongseong Chae,Youngchul Sung,Kanghoon Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: 提出OPT方法解决离线预训练智能体在线微调中价值估计不准确问题，在多种环境中平均性能提升30%。


<details>
  <summary>Details</summary>
Motivation: 现有离线预训练智能体在在线微调中因分布偏移导致价值估计不准确、性能不佳，需解决该问题。

Method: 提出OPT方法，引入在线预训练阶段，训练适用于在线微调的新价值函数。

Result: 在TD3和SPOT上实现OPT，在多种D4RL环境中平均性能提升30%。

Conclusion: OPT方法能有效解决离线预训练智能体在线微调时价值估计不准确的问题，提升性能。

Abstract: Offline-to-online reinforcement learning (RL) aims to integrate the
complementary strengths of offline and online RL by pre-training an agent
offline and subsequently fine-tuning it through online interactions. However,
recent studies reveal that offline pre-trained agents often underperform during
online fine-tuning due to inaccurate value estimation caused by distribution
shift, with random initialization proving more effective in certain cases. In
this work, we propose a novel method, Online Pre-Training for Offline-to-Online
RL (OPT), explicitly designed to address the issue of inaccurate value
estimation in offline pre-trained agents. OPT introduces a new learning phase,
Online Pre-Training, which allows the training of a new value function tailored
specifically for effective online fine-tuning. Implementation of OPT on TD3 and
SPOT demonstrates an average 30% improvement in performance across a wide range
of D4RL environments, including MuJoCo, Antmaze, and Adroit.

</details>


### [75] [Inference-Time Scaling of Diffusion Language Models with Particle Gibbs Sampling](https://arxiv.org/abs/2507.08390)
*Meihua Dang,Jiaqi Han,Minkai Xu,Kai Xu,Akash Srivastava,Stefano Ermon*

Main category: cs.LG

TL;DR: 研究离散扩散模型在奖励引导下的文本生成采样方法，提出基于粒子吉布斯采样的推理时间扩展方法，性能优于先前策略。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型推理时间扩展研究不足，要在奖励引导下实现高质量文本生成。

Method: 引入基于粒子吉布斯采样的推理时间扩展方法，用条件顺序蒙特卡罗作为转换机制迭代优化多个扩散轨迹，分析四个关键轴的权衡。

Result: 在奖励引导文本生成任务中，该方法始终优于先前推理时间策略，在不同计算预算下准确性显著提高。

Conclusion: 提出的基于粒子吉布斯采样的推理时间扩展方法有效，能在奖励引导文本生成中取得更好效果。

Abstract: Discrete diffusion models have emerged as a powerful paradigm for language
modeling, rivaling auto-regressive models by training-time scaling. However,
inference-time scaling in discrete diffusion models remains relatively
under-explored. In this work, we study sampling-based approaches for achieving
high-quality text generation from discrete diffusion models in reward-guided
settings. We introduce a novel inference-time scaling approach based on
particle Gibbs sampling for discrete diffusion models. The particle Gibbs
sampling algorithm iteratively refines full diffusion trajectories using
conditional Sequential Monte Carlo as its transition mechanism. This process
ensures that the updated samples progressively improve and move closer to the
reward-weighted target distribution. Unlike existing inference-time scaling
methods, which are often limited to single diffusion trajectories, our approach
leverages iterative refinement across multiple trajectories. Within this
framework, we further analyze the trade-offs between four key axes for
inference-time scaling under fixed compute budgets: particle Gibbs iterations,
particle count, denoising steps, and reward estimation cost. Empirically, our
method consistently outperforms prior inference-time strategies on
reward-guided text generation tasks, achieving significant improvement in
accuracy under varying compute budgets.

</details>


### [76] [RTNinja: a generalized machine learning framework for analyzing random telegraph noise signals in nanoelectronic devices](https://arxiv.org/abs/2507.08424)
*Anirudh Varanasi,Robin Degraeve,Philippe Roussel,Clement Merckling*

Main category: cs.LG

TL;DR: 介绍RTNinja自动化机器学习框架用于随机电报噪声信号无监督分析，经测试性能良好，可用于下一代纳米电子学多方面研究。


<details>
  <summary>Details</summary>
Motivation: 传统随机电报噪声分析技术有局限性，不适用于复杂、嘈杂数据集。

Method: 引入RTNinja框架，含LevelsExtractor和SourcesMapper两个模块；开发蒙特卡罗模拟器生成标注数据集评估性能。

Result: 在7000个数据集上，RTNinja能高保真重建信号，准确提取源振幅和活动模式。

Conclusion: RTNinja是强大、可扩展且与设备无关的随机电报噪声表征工具，可用于下一代纳米电子学多方面工作。

Abstract: Random telegraph noise is a prevalent variability phenomenon in
nanoelectronic devices, arising from stochastic carrier exchange at defect
sites and critically impacting device reliability and performance. Conventional
analysis techniques often rely on restrictive assumptions or manual
interventions, limiting their applicability to complex, noisy datasets. Here,
we introduce RTNinja, a generalized, fully automated machine learning framework
for the unsupervised analysis of random telegraph noise signals. RTNinja
deconvolves complex signals to identify the number and characteristics of
hidden individual sources, without requiring prior knowledge of the system. The
framework comprises two modular components: LevelsExtractor, which uses
Bayesian inference and model selection to denoise and discretize the signal;
and SourcesMapper, which infers source configurations through probabilistic
clustering and optimization. To evaluate performance, we developed a Monte
Carlo simulator that generates labeled datasets spanning broad signal-to-noise
ratios and source complexities; across 7000 such datasets, RTNinja consistently
demonstrated high-fidelity signal reconstruction and accurate extraction of
source amplitudes and activity patterns. Our results demonstrate that RTNinja
offers a robust, scalable, and device-agnostic tool for random telegraph noise
characterization, enabling large-scale statistical benchmarking,
reliability-centric technology qualification, predictive failure modeling, and
device physics exploration in next-generation nanoelectronics.

</details>


### [77] [KGRAG-Ex: Explainable Retrieval-Augmented Generation with Knowledge Graph-based Perturbations](https://arxiv.org/abs/2507.08443)
*Georgios Balanos,Evangelos Chasanis,Konstantinos Skianis,Evaggelia Pitoura*

Main category: cs.LG

TL;DR: 提出KGRAG - Ex系统，利用领域知识图谱提升检索增强生成的事实依据和可解释性，并进行相关实验分析。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成存在可解释性挑战，知识图谱可提供结构化表示解决该问题。

Method: 构建基于提示信息提取的领域知识图谱，识别相关实体和语义路径转化为伪段落指导语料检索，引入基于扰动的解释方法评估图谱组件影响。

Result: 进行一系列实验分析系统对不同扰动方法的敏感性等多个方面。

Conclusion: 未明确提及具体结论，但强调了KGRAG - Ex系统在提升事实依据和可解释性上的作用。

Abstract: Retrieval-Augmented Generation (RAG) enhances language models by grounding
responses in external information, yet explainability remains a critical
challenge, particularly when retrieval relies on unstructured text. Knowledge
graphs (KGs) offer a solution by introducing structured, semantically rich
representations of entities and their relationships, enabling transparent
retrieval paths and interpretable reasoning. In this work, we present KGRAG-Ex,
a RAG system that improves both factual grounding and explainability by
leveraging a domain-specific KG constructed via prompt-based information
extraction. Given a user query, KGRAG-Ex identifies relevant entities and
semantic paths in the graph, which are then transformed into pseudo-paragraphs:
natural language representations of graph substructures that guide corpus
retrieval. To improve interpretability and support reasoning transparency, we
incorporate perturbation-based explanation methods that assess the influence of
specific KG-derived components on the generated answers. We conduct a series of
experiments to analyze the sensitivity of the system to different perturbation
methods, the relationship between graph component importance and their
structural positions, the influence of semantic node types, and how graph
metrics correspond to the influence of components within the explanations
process.

</details>


### [78] [Space filling positionality and the Spiroformer](https://arxiv.org/abs/2507.08456)
*M. Maurin,M. Á. Evangelista-Alvarado,P. Suárez-Serrato*

Main category: cs.LG

TL;DR: 提出用遵循空间填充曲线的注意力头解决将transformer模型推广到几何域时无全局顺序的问题，给出Spiroformer示例。


<details>
  <summary>Details</summary>
Motivation: 将transformer模型推广到几何域（如流形）时，面临无定义明确全局顺序的问题。

Method: 采用遵循空间填充曲线的注意力头。

Result: 给出了在二维球面上遵循极螺旋的Spiroformer这一实验示例。

Conclusion: 提出的方法可用于解决transformer模型在几何域推广的问题。

Abstract: Transformers excel when dealing with sequential data. Generalizing
transformer models to geometric domains, such as manifolds, we encounter the
problem of not having a well-defined global order. We propose a solution with
attention heads following a space-filling curve. As a first experimental
example, we present the Spiroformer, a transformer that follows a polar spiral
on the $2$-sphere.

</details>


### [79] [Ranked Set Sampling-Based Multilayer Perceptron: Improving Generalization via Variance-Based Bounds](https://arxiv.org/abs/2507.08465)
*Feijiang Li,Liuya Zhang,Jieting Wang,Tao Yan,Yuhua Qian*

Main category: cs.LG

TL;DR: 本文建立新泛化误差界，受其启发引入RSS方法降低MLP经验损失方差，理论和实验验证了RSS - MLP方法有效性。


<details>
  <summary>Details</summary>
Motivation: 建立新泛化误差界，揭示经验损失方差对学习模型泛化能力的影响，降低MLP经验损失方差以提升其能力。

Method: 引入有序结构的Rank Set Sampling (RSS) 方法处理训练数据集，提出RSS - MLP方法。

Result: 理论结果表明RSS估计的经验指数损失和逻辑损失方差小于SRS；在十二个基准数据集上的实验验证了RSS - MLP方法有效性。

Conclusion: 提出的RSS - MLP方法有效且合理。

Abstract: Multilayer perceptron (MLP), one of the most fundamental neural networks, is
extensively utilized for classification and regression tasks. In this paper, we
establish a new generalization error bound, which reveals how the variance of
empirical loss influences the generalization ability of the learning model.
Inspired by this learning bound, we advocate to reduce the variance of
empirical loss to enhance the ability of MLP. As is well-known, bagging is a
popular ensemble method to realize variance reduction. However, bagging
produces the base training data sets by the Simple Random Sampling (SRS)
method, which exhibits a high degree of randomness. To handle this issue, we
introduce an ordered structure in the training data set by Rank Set Sampling
(RSS) to further reduce the variance of loss and develop a RSS-MLP method.
Theoretical results show that the variance of empirical exponential loss and
the logistic loss estimated by RSS are smaller than those estimated by SRS,
respectively. To validate the performance of RSS-MLP, we conduct comparison
experiments on twelve benchmark data sets in terms of the two convex loss
functions under two fusion methods. Extensive experimental results and analysis
illustrate the effectiveness and rationality of the propose method.

</details>


### [80] [Pre-Training LLMs on a budget: A comparison of three optimizers](https://arxiv.org/abs/2507.08472)
*Joel Schlotthauer,Christian Kroos,Chris Hinze,Viktor Hangya,Luzian Hahn,Fabian Küch*

Main category: cs.LG

TL;DR: 研究对比AdamW、Lion和Sophia三种优化器，在不同架构和训练方式下调整超参，发现Sophia损失最低，Lion训练最快，AdamW下游评估效果最好。


<details>
  <summary>Details</summary>
Motivation: 优化器对减少大语言模型预训练时间和提升模型性能有决定性作用，需对比不同优化器性能。

Method: 对比AdamW、Lion和Sophia三种优化器，采用两种基础架构、单轮和多轮训练方式，保持令牌数不变，用Maximal Update Parametrization和小型代理模型为不同架构和优化器组合分别调整超参。

Result: 三种优化器结果相近，Sophia训练和验证损失最低，Lion训练GPU用时最短，AdamW下游评估结果最佳。

Conclusion: 不同优化器在不同方面表现各有优劣。

Abstract: Optimizers play a decisive role in reducing pre-training times for LLMs and
achieving better-performing models. In this study, we compare three major
variants: the de-facto standard AdamW, the simpler Lion, developed through an
evolutionary search, and the second-order optimizer Sophia. For better
generalization, we train with two different base architectures and use a
single- and a multiple-epoch approach while keeping the number of tokens
constant. Using the Maximal Update Parametrization and smaller proxy models, we
tune relevant hyperparameters separately for each combination of base
architecture and optimizer. We found that while the results from all three
optimizers were in approximately the same range, Sophia exhibited the lowest
training and validation loss, Lion was fastest in terms of training GPU hours
but AdamW led to the best downstream evaluation results.

</details>


### [81] [Evaluating SAE interpretability without explanations](https://arxiv.org/abs/2507.08473)
*Gonçalo Paulo,Nora Belrose*

Main category: cs.LG

TL;DR: 本文改进评估方法，直接评估稀疏编码器可解释性，并与人工评估对比，为社区提供评估改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有测量稀疏自动编码器和转码器可解释性的方法存在不足，难以将解释生成和评估过程与潜在变量的实际可解释性分离，且缺乏统一基准。

Method: 调整现有方法，避免将生成自然语言解释作为中间步骤，以更直接地评估可解释性，并将评估指标得分与人工评估进行比较。

Result: 实现了更直接且可能标准化的可解释性评估。

Conclusion: 为社区在改进这些技术的评估方面提供了建议。

Abstract: Sparse autoencoders (SAEs) and transcoders have become important tools for
machine learning interpretability. However, measuring how interpretable they
are remains challenging, with weak consensus about which benchmarks to use.
Most evaluation procedures start by producing a single-sentence explanation for
each latent. These explanations are then evaluated based on how well they
enable an LLM to predict the activation of a latent in new contexts. This
method makes it difficult to disentangle the explanation generation and
evaluation process from the actual interpretability of the latents discovered.
In this work, we adapt existing methods to assess the interpretability of
sparse coders, with the advantage that they do not require generating natural
language explanations as an intermediate step. This enables a more direct and
potentially standardized assessment of interpretability. Furthermore, we
compare the scores produced by our interpretability metrics with human
evaluations across similar tasks and varying setups, offering suggestions for
the community on improving the evaluation of these techniques.

</details>


### [82] [SynBridge: Bridging Reaction States via Discrete Flow for Bidirectional Reaction Prediction](https://arxiv.org/abs/2507.08475)
*Haitao Lin,Junjie Wang,Zhifeng Gao,Xiaohong Ji,Rong Zhu,Linfeng Zhang,Guolin Ke,Weinan E*

Main category: cs.LG

TL;DR: 提出双向流生成模型SynBridge用于多任务反应预测，在三个基准数据集上取得SOTA性能，揭示结构化扩散优势。


<details>
  <summary>Details</summary>
Motivation: 化学反应中电子重新分布和重组是离散突变的，需建模状态转变来进行反应预测。

Method: 提出SynBridge，利用图到图的变压器网络架构和离散流桥，通过键和原子的离散状态捕获反应物和产物图之间的双向化学转化。

Result: 在三个基准数据集（USPTO - 50K、USPTO - MIT、Pistachio）上实验，在正向和逆合成任务中达到SOTA性能。

Conclusion: 结构化扩散在离散空间上对反应预测有益。

Abstract: The essence of a chemical reaction lies in the redistribution and
reorganization of electrons, which is often manifested through electron
transfer or the migration of electron pairs. These changes are inherently
discrete and abrupt in the physical world, such as alterations in the charge
states of atoms or the formation and breaking of chemical bonds. To model the
transition of states, we propose SynBridge, a bidirectional flow-based
generative model to achieve multi-task reaction prediction. By leveraging a
graph-to-graph transformer network architecture and discrete flow bridges
between any two discrete distributions, SynBridge captures bidirectional
chemical transformations between graphs of reactants and products through the
bonds' and atoms' discrete states. We further demonstrate the effectiveness of
our method through extensive experiments on three benchmark datasets
(USPTO-50K, USPTO-MIT, Pistachio), achieving state-of-the-art performance in
both forward and retrosynthesis tasks. Our ablation studies and noise
scheduling analysis reveal the benefits of structured diffusion over discrete
spaces for reaction prediction.

</details>


### [83] [Efficient Deployment of Vision-Language Models on Mobile Devices: A Case Study on OnePlus 13R](https://arxiv.org/abs/2507.08505)
*Pablo Robin Guerrero,Yueyang Pan,Sanidhya Kashyap*

Main category: cs.LG

TL;DR: 本文对移动设备上视觉语言模型（VLMs）的部署框架进行了全面调研，评估了不同框架在运行代表性工作负载时的性能，揭示了性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: VLMs在移动设备上的部署因计算限制和能源效率问题面临挑战，尤其是实时应用，因此需要对部署框架进行评估。

Method: 以OnePlus 13R为测试设备，运行LLaVA - 1.5 7B、MobileVLM - 3B和Imp - v1.5 3B等工作负载，评估llama.cpp、MLC - Imp和mllm等部署框架，测量CPU、GPU、NPU利用率、温度、推理时间、功耗和用户体验。

Result: 基准测试显示各框架存在关键性能瓶颈，token生成时CPU资源持续过度使用，GPU和NPU加速器大多未被使用；使用GPU进行图像特征提取时会饱和，导致设备响应性下降。

Conclusion: 本研究提供了框架级基准、实用的分析工具和对硬件利用瓶颈的深入分析，指出当前部署框架中CPU过度使用，GPU和NPU使用无效或不稳定的问题。

Abstract: Vision-Language Models (VLMs) offer promising capabilities for mobile
devices, but their deployment faces significant challenges due to computational
limitations and energy inefficiency, especially for real-time applications.
This study provides a comprehensive survey of deployment frameworks for VLMs on
mobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of
running LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads
on a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R
while running VLMs, with measurements covering CPU, GPU, and NPU utilization,
temperature, inference time, power consumption, and user experience.
Benchmarking revealed critical performance bottlenecks across frameworks: CPU
resources were consistently over-utilized during token generation, while GPU
and NPU accelerators were largely unused. When the GPU was used, primarily for
image feature extraction, it was saturated, leading to degraded device
responsiveness. The study contributes framework-level benchmarks, practical
profiling tools, and an in-depth analysis of hardware utilization bottlenecks,
highlighting the consistent overuse of CPUs and the ineffective or unstable use
of GPUs and NPUs in current deployment frameworks.

</details>


### [84] [SFedKD: Sequential Federated Learning with Discrepancy-Aware Multi-Teacher Knowledge Distillation](https://arxiv.org/abs/2507.08508)
*Haotian Xu,Jinrui Zhou,Xichong Zhang,Mingjun Xiao,He Sun,Yin Xu*

Main category: cs.LG

TL;DR: 提出SFedKD框架解决顺序联邦学习中的灾难性遗忘问题，实验证明其有效且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 顺序联邦学习（SFL）在异构环境中存在严重的灾难性遗忘问题，即模型易忘记从先前客户端学到的知识。

Method: 提出SFedKD框架，选择上一轮的多个模型指导本轮训练；扩展单教师解耦知识蒸馏方法到多教师场景，根据师生数据类别分布差异分配权重；消除冗余教师并设计基于贪心策略的教师选择机制。

Result: 广泛实验表明SFedKD有效克服了SFL中的灾难性遗忘问题，性能优于现有联邦学习方法。

Conclusion: SFedKD能够增强模型训练效果，减轻灾难性遗忘，同时降低通信和计算成本。

Abstract: Federated Learning (FL) is a distributed machine learning paradigm which
coordinates multiple clients to collaboratively train a global model via a
central server. Sequential Federated Learning (SFL) is a newly-emerging FL
training framework where the global model is trained in a sequential manner
across clients. Since SFL can provide strong convergence guarantees under data
heterogeneity, it has attracted significant research attention in recent years.
However, experiments show that SFL suffers from severe catastrophic forgetting
in heterogeneous environments, meaning that the model tends to forget knowledge
learned from previous clients. To address this issue, we propose an SFL
framework with discrepancy-aware multi-teacher knowledge distillation, called
SFedKD, which selects multiple models from the previous round to guide the
current round of training. In SFedKD, we extend the single-teacher Decoupled
Knowledge Distillation approach to our multi-teacher setting and assign
distinct weights to teachers' target-class and non-target-class knowledge based
on the class distributional discrepancy between teacher and student data.
Through this fine-grained weighting strategy, SFedKD can enhance model training
efficacy while mitigating catastrophic forgetting. Additionally, to prevent
knowledge dilution, we eliminate redundant teachers for the knowledge
distillation and formalize it as a variant of the maximum coverage problem.
Based on the greedy strategy, we design a complementary-based teacher selection
mechanism to ensure that the selected teachers achieve comprehensive knowledge
space coverage while reducing communication and computational costs. Extensive
experiments show that SFedKD effectively overcomes catastrophic forgetting in
SFL and outperforms state-of-the-art FL methods.

</details>


### [85] [Recursive Reward Aggregation](https://arxiv.org/abs/2507.08537)
*Yuting Tang,Yivan Zhang,Johannes Ackermann,Yu-Jie Zhang,Soichiro Nishimori,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出无需修改奖励函数、通过选择奖励聚合函数实现灵活行为对齐的方法，适用于多种设置和算法，实验证明其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，当期望目标复杂时，仔细设计奖励函数来使智能体行为与特定目标对齐具有挑战性，因此寻找替代方法。

Method: 引入马尔可夫决策过程的代数视角，将标准折扣和推广到其他递归聚合，如折扣最大值和夏普比率，且适用于确定性和随机设置，可与基于值和演员 - 评论家算法集成。

Result: 实验结果表明该方法能有效优化多种目标。

Conclusion: 该方法具有通用性和在现实世界应用的潜力。

Abstract: In reinforcement learning (RL), aligning agent behavior with specific
objectives typically requires careful design of the reward function, which can
be challenging when the desired objectives are complex. In this work, we
propose an alternative approach for flexible behavior alignment that eliminates
the need to modify the reward function by selecting appropriate reward
aggregation functions. By introducing an algebraic perspective on Markov
decision processes (MDPs), we show that the Bellman equations naturally emerge
from the recursive generation and aggregation of rewards, allowing for the
generalization of the standard discounted sum to other recursive aggregations,
such as discounted max and Sharpe ratio. Our approach applies to both
deterministic and stochastic settings and integrates seamlessly with
value-based and actor-critic algorithms. Experimental results demonstrate that
our approach effectively optimizes diverse objectives, highlighting its
versatility and potential for real-world applications.

</details>


### [86] [CircFormerMoE: An End-to-End Deep Learning Framework for Circular RNA Splice Site Detection and Pairing in Plant Genomes](https://arxiv.org/abs/2507.08542)
*Tianyou Jiang*

Main category: cs.LG

TL;DR: 本文提出基于transformers和混合专家的深度学习框架CircFormerMoE，可直接从植物基因组DNA预测circRNAs，经10种植物基因数据验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有circRNA识别方法依赖RNA实验数据、计算成本高、不适用于大规模预测，且植物中尚无有效泛化能力强的深度学习模型，已知植物circRNA数量远低于实际。

Method: 提出CircFormerMoE框架，包含剪接位点检测（SSD）和剪接位点配对（SSP）两个子任务。

Result: 在10种植物基因数据上验证了模型有效性，能发现未注释的circRNAs，并对模型进行了解释性分析。

Conclusion: 该框架为植物大规模circRNA发现提供了快速准确的计算方法和工具，为植物功能基因组学和非编码RNA注释研究奠定基础。

Abstract: Circular RNAs (circRNAs) are important components of the non-coding RNA
regulatory network. Previous circRNA identification primarily relies on
high-throughput RNA sequencing (RNA-seq) data combined with alignment-based
algorithms that detect back-splicing signals. However, these methods face
several limitations: they can't predict circRNAs directly from genomic DNA
sequences and relies heavily on RNA experimental data; they involve high
computational costs due to complex alignment and filtering steps; and they are
inefficient for large-scale or genome-wide circRNA prediction. The challenge is
even greater in plants, where plant circRNA splice sites often lack the
canonical GT-AG motif seen in human mRNA splicing, and no efficient deep
learning model with strong generalization capability currently exists.
Furthermore, the number of currently identified plant circRNAs is likely far
lower than their true abundance. In this paper, we propose a deep learning
framework named CircFormerMoE based on transformers and mixture-of experts for
predicting circRNAs directly from plant genomic DNA. Our framework consists of
two subtasks known as splicing site detection (SSD) and splicing site pairing
(SSP). The model's effectiveness has been validated on gene data of 10 plant
species. Trained on known circRNA instances, it is also capable of discovering
previously unannotated circRNAs. In addition, we performed interpretability
analyses on the trained model to investigate the sequence patterns contributing
to its predictions. Our framework provides a fast and accurate computational
method and tool for large-scale circRNA discovery in plants, laying a
foundation for future research in plant functional genomics and non-coding RNA
annotation.

</details>


### [87] [STRAP: Spatial-Temporal Risk-Attentive Vehicle Trajectory Prediction for Autonomous Driving](https://arxiv.org/abs/2507.08563)
*Xinyi Ning,Zilin Bian,Kaan Ozbay,Semiha Ergan*

Main category: cs.LG

TL;DR: 提出时空风险注意力轨迹预测框架，结合风险势场评估风险，在数据集上降低预测误差，助力自动驾驶决策。


<details>
  <summary>Details</summary>
Motivation: 现有车辆轨迹预测方法常忽略周围车辆不确定或激进行为带来的潜在风险，需要更准确的预测方法确保自动驾驶安全高效。

Method: 提出时空风险注意力轨迹预测框架，融入风险势场，利用时空编码器和风险注意力特征融合解码器，设计风险缩放损失函数。

Result: 在NGSIM和HighD数据集上，比现有方法分别降低4.8%和31.2%的平均预测误差，在高风险场景效果更佳。

Conclusion: 所提框架提供可解释、风险感知的预测，有助于自动驾驶系统更稳健地决策。

Abstract: Accurate vehicle trajectory prediction is essential for ensuring safety and
efficiency in fully autonomous driving systems. While existing methods
primarily focus on modeling observed motion patterns and interactions with
other vehicles, they often neglect the potential risks posed by the uncertain
or aggressive behaviors of surrounding vehicles. In this paper, we propose a
novel spatial-temporal risk-attentive trajectory prediction framework that
incorporates a risk potential field to assess perceived risks arising from
behaviors of nearby vehicles. The framework leverages a spatial-temporal
encoder and a risk-attentive feature fusion decoder to embed the risk potential
field into the extracted spatial-temporal feature representations for
trajectory prediction. A risk-scaled loss function is further designed to
improve the prediction accuracy of high-risk scenarios, such as short relative
spacing. Experiments on the widely used NGSIM and HighD datasets demonstrate
that our method reduces average prediction errors by 4.8% and 31.2%
respectively compared to state-of-the-art approaches, especially in high-risk
scenarios. The proposed framework provides interpretable, risk-aware
predictions, contributing to more robust decision-making for autonomous driving
systems.

</details>


### [88] [AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient Sequence Modeling](https://arxiv.org/abs/2507.08567)
*Preslav Aleksandrov,Meghdad Kurmanji,Fernando Garcia Redondo,David O'Shea,William Shen,Alex Iacob,Lorenzo Sani,Xinchi Qiu,Nicola Cancedda,Nicholas D. Lane*

Main category: cs.LG

TL;DR: 介绍新型递归架构AbbIE，比标准Transformer有更好困惑度，能动态缩放计算资源，在零样本学习和语言困惑度上有提升，为Transformer性能扩展开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 寻找除通过参数和令牌计数扩展大语言模型性能外的方法，提升Transformer架构性能。

Method: 引入Autoregressive Block - Based Iterative Encoder (AbbIE)架构，在潜在空间进行迭代。

Result: 仅在训练时使用2次迭代就能在测试时向上泛化，远胜替代迭代方法；在零样本上下文学习任务中比其他迭代和标准方法最多提高12%，语言困惑度最多提高5%。

Conclusion: 研究结果为Transformer性能扩展开辟了新途径。

Abstract: We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a
novel recursive generalization of the encoder-only Transformer architecture,
which achieves better perplexity than a standard Transformer and allows for the
dynamic scaling of compute resources at test time. This simple, recursive
approach is a complement to scaling large language model (LLM) performance
through parameter and token counts. AbbIE performs its iterations in latent
space, but unlike latent reasoning models, does not require a specialized
dataset or training protocol. We show that AbbIE upward generalizes (ability to
generalize to arbitrary iteration lengths) at test time by only using 2
iterations during train time, far outperforming alternative iterative methods.
AbbIE's ability to scale its computational expenditure based on the complexity
of the task gives it an up to \textbf{12\%} improvement in zero-shot in-context
learning tasks versus other iterative and standard methods and up to 5\%
improvement in language perplexity. The results from this study open a new
avenue to Transformer performance scaling. We perform all of our evaluations on
model sizes up to 350M parameters.

</details>


### [89] [ADAPT: A Pseudo-labeling Approach to Combat Concept Drift in Malware Detection](https://arxiv.org/abs/2507.08597)
*Md Tanvirul Alam,Aritran Piplai,Nidhi Rastogi*

Main category: cs.LG

TL;DR: 提出ADAPT伪标签半监督算法应对恶意软件检测中的概念漂移，实验证明其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型用于恶意软件分类时因概念漂移性能下降，频繁更新依赖昂贵标注，半监督学习在该领域研究较少。

Method: 引入ADAPT伪标签半监督算法，该方法与模型无关，可应用于多种机器学习模型。

Result: 在五个不同恶意软件检测数据集上实验，方法始终优于基线模型和竞争基准。

Conclusion: 为机器学习模型在恶意软件检测中更有效地适应概念漂移奠定基础。

Abstract: Machine learning models are commonly used for malware classification;
however, they suffer from performance degradation over time due to concept
drift. Adapting these models to changing data distributions requires frequent
updates, which rely on costly ground truth annotations. While active learning
can reduce the annotation burden, leveraging unlabeled data through
semi-supervised learning remains a relatively underexplored approach in the
context of malware detection. In this research, we introduce \texttt{ADAPT}, a
novel pseudo-labeling semi-supervised algorithm for addressing concept drift.
Our model-agnostic method can be applied to various machine learning models,
including neural networks and tree-based algorithms. We conduct extensive
experiments on five diverse malware detection datasets spanning Android,
Windows, and PDF domains. The results demonstrate that our method consistently
outperforms baseline models and competitive benchmarks. This work paves the way
for more effective adaptation of machine learning models to concept drift in
malware detection.

</details>


### [90] [Remote Sensing Reveals Adoption of Sustainable Rice Farming Practices Across Punjab, India](https://arxiv.org/abs/2507.08605)
*Ando Shah,Rajveer Singh,Akram Zaytar,Girmaw Abebe Tadesse,Caleb Robinson,Negar Tafti,Stephen A. Wood,Rahul Dodhia,Juan M. Lavista Ferres*

Main category: cs.LG

TL;DR: 本文提出新的遥感框架监测印度旁遮普邦可持续水稻灌溉实践，准确率高且可扩展，为政策制定提供工具。


<details>
  <summary>Details</summary>
Motivation: 可持续灌溉可节水，但采用率数据有限阻碍决策和资源分配，且旁遮普邦面临严重地下水枯竭问题。

Method: 与PRANA项目合作收集数据，结合Sentinel - 1卫星图像创建分类系统，从播种和灌溉维度区分水管理方式。

Result: F1分数达78%，能区分DSR与传统水稻；绘制约300万农业地块DSR采用情况，地区预测与政府记录强相关。

Conclusion: 该研究为政策制定者提供强大工具，可大规模跟踪可持续水管理采用情况、实施干预和评估项目影响。

Abstract: Rice cultivation consumes 24-30% of global freshwater, creating critical
water management challenges in major rice-producing regions. Sustainable
irrigation practices like direct seeded rice (DSR) and alternate wetting and
drying (AWD) can reduce water use by 20-40% while maintaining yields, helping
secure long-term agricultural productivity as water scarcity intensifies - a
key component of the Zero Hunger Sustainable Development Goal. However, limited
data on adoption rates of these practices prevents evidence-based policymaking
and targeted resource allocation. We developed a novel remote sensing framework
to monitor sustainable water management practices at scale in Punjab, India - a
region facing severe groundwater depletion of 41.6 cm/year. To collect
essential ground truth data, we partnered with the Nature Conservancy's
Promoting Regenerative and No-burn Agriculture (PRANA) program, which trained
approximately 1,400 farmers on water-saving techniques while documenting their
field-level practices. Using this data, we created a classification system with
Sentinel-1 satellite imagery that separates water management along sowing and
irrigation dimensions. Our approach achieved a 78% F1-score in distinguishing
DSR from traditional puddled transplanted rice without requiring prior
knowledge of planting dates. We demonstrated scalability by mapping DSR
adoption across approximately 3 million agricultural plots in Punjab, with
district-level predictions showing strong correlation (Pearson=0.77, RBO= 0.77)
with government records. This study provides policymakers with a powerful tool
to track sustainable water management adoption, target interventions, and
measure program impacts at scale.

</details>


### [91] [Emergent Natural Language with Communication Games for Improving Image Captioning Capabilities without Additional Data](https://arxiv.org/abs/2507.08610)
*Parag Dutta,Ambedkar Dukkipati*

Main category: cs.LG

TL;DR: 本文提出LoGIC方法提升无监督图像描述性能，在不同设置下取得优于现有方法的BLEU分数。


<details>
  <summary>Details</summary>
Motivation: 现有标注数据集已用于训练大模型，提升图像描述性能有挑战，无监督图像描述性能研究不足。

Method: 提出LoGIC，一种多智能体强化学习游戏，包含“说话者”和“倾听者”两个智能体，用GRPO算法在合作共同奖励设置下训练。

Result: 使用预训练VLM和LLM微调后BLEU分数达46，用轻量级组件从头训练BLEU分数达31，均优于现有方法。

Conclusion: LoGIC方法能有效提升无监督图像描述性能。

Abstract: Image captioning is an important problem in developing various AI systems,
and these tasks require large volumes of annotated images to train the models.
Since all existing labelled datasets are already used for training the large
Vision Language Models (VLMs), it becomes challenging to improve the
performance of the same. Considering this, it is essential to consider the
unsupervised image captioning performance, which remains relatively
under-explored. To that end, we propose LoGIC (Lewis Communication Game for
Image Captioning), a Multi-agent Reinforcement Learning game. The proposed
method consists of two agents, a 'speaker' and a 'listener', with the objective
of learning a strategy for communicating in natural language. We train agents
in the cooperative common-reward setting using the GRPO algorithm and show that
improvement in image captioning performance emerges as a consequence of the
agents learning to play the game. We show that using pre-trained VLMs as the
'speaker' and Large Language Model (LLM) for language understanding in the
'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without
additional labels, a $2$ units advantage in absolute metrics compared to the
$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the
'speaker' with lightweight components: (i) a ViT for image perception and (ii)
a GPT2 language generation, and train them from scratch using LoGIC, obtaining
a $31$ BLEU score in the unsupervised setting, a $10$ points advantage over
existing unsupervised image-captioning methods.

</details>


### [92] [Towards Collaborative Fairness in Federated Learning Under Imbalanced Covariate Shift](https://arxiv.org/abs/2507.08617)
*Tianrun Yu,Jiaqi Wang,Haoyu Wang,Mingquan Lin,Han Liu,Nelson S. Yee,Fenglong Ma*

Main category: cs.LG

TL;DR: 文章指出联邦学习中现有方法常忽略不平衡协变量偏移问题，提出 FedAKD 方法，理论证明其收敛性，实验表明该方法能提升协作公平性、预测准确性和客户端参与度。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法常忽略不平衡协变量偏移这一实际且复杂的异质性问题，需要解决协作公平性挑战。

Method: 提出 FedAKD 方法，包含客户端和服务器更新。客户端更新采用基于初步分析的异步知识蒸馏策略，先更新客户端模型，再用正确预测的高置信度样本更新全局模型；服务器更新聚合所有客户端模型，并提供理论证明其收敛性。

Result: 在公共数据集（FashionMNIST 和 CIFAR10）和真实世界电子健康记录（EHR）数据集上的实验表明，FedAKD 显著提高了协作公平性，增强了预测准确性，并促进了客户端参与。

Conclusion: FedAKD 是一种简单有效的方法，能在准确预测和协作公平性之间取得平衡，适用于高度异质的数据分布。

Abstract: Collaborative fairness is a crucial challenge in federated learning. However,
existing approaches often overlook a practical yet complex form of
heterogeneity: imbalanced covariate shift. We provide a theoretical analysis of
this setting, which motivates the design of FedAKD (Federated Asynchronous
Knowledge Distillation)- simple yet effective approach that balances accurate
prediction with collaborative fairness. FedAKD consists of client and server
updates. In the client update, we introduce a novel asynchronous knowledge
distillation strategy based on our preliminary analysis, which reveals that
while correctly predicted samples exhibit similar feature distributions across
clients, incorrectly predicted samples show significant variability. This
suggests that imbalanced covariate shift primarily arises from misclassified
samples. Leveraging this insight, our approach first applies traditional
knowledge distillation to update client models while keeping the global model
fixed. Next, we select correctly predicted high-confidence samples and update
the global model using these samples while keeping client models fixed. The
server update simply aggregates all client models. We further provide a
theoretical proof of FedAKD's convergence. Experimental results on public
datasets (FashionMNIST and CIFAR10) and a real-world Electronic Health Records
(EHR) dataset demonstrate that FedAKD significantly improves collaborative
fairness, enhances predictive accuracy, and fosters client participation even
under highly heterogeneous data distributions.

</details>


### [93] [Scaling Attention to Very Long Sequences in Linear Time with Wavelet-Enhanced Random Spectral Attention (WERSA)](https://arxiv.org/abs/2507.08637)
*Vincenzo Dentamaro*

Main category: cs.LG

TL;DR: 提出线性复杂度的WERSA机制用于长序列处理，在多基准测试中展现优势，能降低计算负载并保证准确率，推动长上下文模型发展。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在处理长序列时计算成本高，因常规注意力机制时间复杂度为$O(n^2)$，需新机制解决该问题。

Method: 引入Wavelet - Enhanced Random Spectral Attention (WERSA)机制，融合内容自适应随机谱特征、多分辨率Haar小波和可学习参数。

Result: 在单GPU及多基准测试中，WERSA相比多种注意力机制有统一优势，如在ArXiv分类中提升准确率、减少训练时间和FLOPS，在长序列上表现出色。

Conclusion: WERSA能显著降低计算负载且不影响准确率，使长上下文模型更实用、经济，利于AI可持续和可扩展发展。

Abstract: Transformer models are computationally costly on long sequences since regular
attention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced
Random Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time
complexity that is pivotal to enable successful long-sequence processing
without the performance trade-off. WERSA merges content-adaptive random
spectral features together with multi-resolution Haar wavelets and learnable
parameters to selectively attend to informative scales of data while preserving
linear efficiency.
  Large-scale comparisons \textbf{on single GPU} and across various benchmarks
(vision, NLP, hierarchical reasoning) and various attention mechanisms (like
Multiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer,
Waveformer), reveal uniform advantages of WERSA. It achieves best accuracy in
all tests. On ArXiv classification, WERSA improves accuracy over vanilla
attention by 1.2\% (86.2\% vs 85.0\%) while cutting training time by 81\% (296s
vs 1554s) and FLOPS by 73.4\% (26.2G vs 98.4G). Significantly, WERSA excels
where vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy
sequences, it achieves best accuracy (79.1\%) and AUC (0.979) among viable
methods, operating on data that gives Out-Of-Memory errors to quadratic methods
while being \textbf{twice as fast} as Waveformer, its next-best competitor.
  By significantly reducing computational loads without compromising accuracy,
WERSA makes possible more practical, more affordable, long-context models, in
particular on low-resource hardware, for more sustainable and more scalable AI
development.

</details>


### [94] [Forget Me Not: Fighting Local Overfitting with Knowledge Fusion and Distillation](https://arxiv.org/abs/2507.08686)
*Uri Stern,Eli Corn,Daphna Weinshall*

Main category: cs.LG

TL;DR: 论文指出深度神经网络过拟合可能是局部的，提出衡量局部过拟合的分数，给出两阶段方法恢复遗忘知识，实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 理论预测大模型容量会过拟合，但实际少见，探究是否存在局部过拟合。

Method: 引入衡量局部过拟合的分数，采用两阶段方法，先集成检查点，再蒸馏成原大小单模型。

Result: 实验验证方法在多数据集、架构和训练机制中有效，有标签噪声时优于原模型和独立训练集成。

Conclusion: 提出的方法能增强性能，降低训练和推理复杂度。

Abstract: Overfitting in deep neural networks occurs less frequently than expected.
This is a puzzling observation, as theory predicts that greater model capacity
should eventually lead to overfitting -- yet this is rarely seen in practice.
But what if overfitting does occur, not globally, but in specific sub-regions
of the data space? In this work, we introduce a novel score that measures the
forgetting rate of deep models on validation data, capturing what we term local
overfitting: a performance degradation confined to certain regions of the input
space. We demonstrate that local overfitting can arise even without
conventional overfitting, and is closely linked to the double descent
phenomenon.
  Building on these insights, we introduce a two-stage approach that leverages
the training history of a single model to recover and retain forgotten
knowledge: first, by aggregating checkpoints into an ensemble, and then by
distilling it into a single model of the original size, thus enhancing
performance without added inference cost.
  Extensive experiments across multiple datasets, modern architectures, and
training regimes validate the effectiveness of our approach. Notably, in the
presence of label noise, our method -- Knowledge Fusion followed by Knowledge
Distillation -- outperforms both the original model and independently trained
ensembles, achieving a rare win-win scenario: reduced training and inference
complexity.

</details>


### [95] [Domain-Informed Operation Excellence of Gas Turbine System with Machine Learning](https://arxiv.org/abs/2507.08697)
*Waqar Muhammad Ashraf,Amir H. Keshavarzzadeh,Abdulelah S. Alshehri,Abdulrahman bin Jumah,Ramit Debnath,Vivek Dua*

Main category: cs.LG

TL;DR: 本文提出MAD - OPT框架将领域知识引入以数据为中心的分析，应用于燃气轮机系统优化，证明其有效性并强调领域约束重要性。


<details>
  <summary>Details</summary>
Motivation: 由于AI算法黑盒性质和传统数据中心分析中领域知识表示不足，火电厂对AI的领域一致性采用率较低。

Method: 开发了基于马氏距离约束的MAD - OPT框架，将领域知识引入数据中心分析，并应用于395兆瓦燃气轮机系统。

Result: MAD - OPT框架能估计不同环境条件下的最优工艺条件，结果稳健；应用于超出设计发电极限情况，与实际数据可比；未引入领域约束的优化分析可能提供无效方案。

Conclusion: 本研究推动了数据驱动的领域知识与机器学习分析的融合，有助于火电厂安全采用AI。

Abstract: The domain-consistent adoption of artificial intelligence (AI) remains low in
thermal power plants due to the black-box nature of AI algorithms and low
representation of domain knowledge in conventional data-centric analytics. In
this paper, we develop a MAhalanobis Distance-based OPTimization (MAD-OPT)
framework that incorporates the Mahalanobis distance-based constraint to
introduce domain knowledge into data-centric analytics. The developed MAD-OPT
framework is applied to maximize thermal efficiency and minimize turbine heat
rate for a 395 MW capacity gas turbine system. We demonstrate that the MAD-OPT
framework can estimate domain-informed optimal process conditions under
different ambient conditions, and the optimal solutions are found to be robust
as evaluated by Monte Carlo simulations. We also apply the MAD-OPT framework to
estimate optimal process conditions beyond the design power generation limit of
the gas turbine system, and have found comparable results with the actual data
of the power plant. We demonstrate that implementing data-centric optimization
analytics without incorporating domain-informed constraints may provide
ineffective solutions that may not be implementable in the real operation of
the gas turbine system. This research advances the integration of the
data-driven domain knowledge into machine learning-powered analytics that
enhances the domain-informed operation excellence and paves the way for safe AI
adoption in thermal power systems.

</details>


### [96] [SPLASH! Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations](https://arxiv.org/abs/2507.08707)
*Peter Crowley,Zachary Serlin,Tyler Paine,Makai Mann,Michael Benjamin,Calin Belta*

Main category: cs.LG

TL;DR: 提出SPLASH方法解决IRL从次优演示学习长时程和对抗性任务的问题，在模拟和现实实验中验证其有效性并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有IRL方法大多假设专家演示可用，允许演示次优性的方法不适用于长时程或对抗性任务，这是IRL在培养实用机器人代理能力上的关键缺陷。

Method: 引入Sample - efficient Preference - based inverse reinforcement learning for Long - horizon Adversarial tasks from Suboptimal Hierarchical demonstrations (SPLASH)方法。

Result: 在海上夺旗模拟任务中验证了SPLASH，通过仿真到现实的实验证明其在自主无人水面舰艇上的现实适用性，在从次优演示中进行奖励学习方面显著优于现有方法。

Conclusion: 提出的SPLASH方法在从次优演示学习长时程和对抗性任务方面取得进展，能有效提升奖励学习效果。

Abstract: Inverse Reinforcement Learning (IRL) presents a powerful paradigm for
learning complex robotic tasks from human demonstrations. However, most
approaches make the assumption that expert demonstrations are available, which
is often not the case. Those that allow for suboptimality in the demonstrations
are not designed for long-horizon goals or adversarial tasks. Many desirable
robot capabilities fall into one or both of these categories, thus highlighting
a critical shortcoming in the ability of IRL to produce field-ready robotic
agents. We introduce Sample-efficient Preference-based inverse reinforcement
learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical
demonstrations (SPLASH), which advances the state-of-the-art in learning from
suboptimal demonstrations to long-horizon and adversarial settings. We
empirically validate SPLASH on a maritime capture-the-flag task in simulation,
and demonstrate real-world applicability with sim-to-real translation
experiments on autonomous unmanned surface vehicles. We show that our proposed
methods allow SPLASH to significantly outperform the state-of-the-art in reward
learning from suboptimal demonstrations.

</details>


### [97] [On the Effect of Regularization in Policy Mirror Descent](https://arxiv.org/abs/2507.08718)
*Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 本文对策略镜像下降（PMD）中的两种正则化技术进行大规模实证分析，发现两者精确组合对实现稳健性能至关重要。


<details>
  <summary>Details</summary>
Motivation: PMD理论研究充分但实证研究稀缺，需对其两种正则化技术的相互作用进行实证分析。

Method: 在小型强化学习环境上运行超50万个训练种子进行大规模实证分析。

Result: 两种正则化器可部分相互替代，但精确组合对实现稳健性能至关重要。

Conclusion: 研究结果凸显了在强化学习中推进更稳健算法研究的潜力，特别是在超参数敏感性方面。

Abstract: Policy Mirror Descent (PMD) has emerged as a unifying framework in
reinforcement learning (RL) by linking policy gradient methods with a
first-order optimization method known as mirror descent. At its core, PMD
incorporates two key regularization components: (i) a distance term that
enforces a trust region for stable policy updates and (ii) an MDP regularizer
that augments the reward function to promote structure and robustness. While
PMD has been extensively studied in theory, empirical investigations remain
scarce. This work provides a large-scale empirical analysis of the interplay
between these two regularization techniques, running over 500k training seeds
on small RL environments. Our results demonstrate that, although the two
regularizers can partially substitute each other, their precise combination is
critical for achieving robust performance. These findings highlight the
potential for advancing research on more robust algorithms in RL, particularly
with respect to hyperparameter sensitivity.

</details>


### [98] [Catastrophic Forgetting Mitigation Through Plateau Phase Activity Profiling](https://arxiv.org/abs/2507.08736)
*Idan Mashiach,Oren Glickman,Tom Tirer*

Main category: cs.LG

TL;DR: 提出在深度神经网络训练最后平稳期跟踪参数以缓解灾难性遗忘，实验显示该方法表现出色。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络学习新任务时会因知识覆盖出现灾难性遗忘，现有正则化技术旨在约束重要参数保留旧知识，需更有效方法。

Method: 提出在深度神经网络训练最后平稳期跟踪参数，认为此阶段活跃度高的参数对应损失景观中较平坦方向，适合在保留旧知识时适应新任务。

Result: 综合实验表明该方法在缓解灾难性遗忘和新任务表现上达到较好平衡。

Conclusion: 在深度神经网络训练最后平稳期跟踪参数是缓解灾难性遗忘的有效方法。

Abstract: Catastrophic forgetting in deep neural networks occurs when learning new
tasks degrades performance on previously learned tasks due to knowledge
overwriting. Among the approaches to mitigate this issue, regularization
techniques aim to identify and constrain "important" parameters to preserve
previous knowledge. In the highly nonconvex optimization landscape of deep
learning, we propose a novel perspective: tracking parameters during the final
training plateau is more effective than monitoring them throughout the entire
training process. We argue that parameters that exhibit higher activity
(movement and variability) during this plateau reveal directions in the loss
landscape that are relatively flat, making them suitable for adaptation to new
tasks while preserving knowledge from previous ones. Our comprehensive
experiments demonstrate that this approach achieves superior performance in
balancing catastrophic forgetting mitigation with strong performance on newly
learned tasks.

</details>


### [99] [Adaptive Nonlinear Vector Autoregression: Robust Forecasting for Noisy Chaotic Time Series](https://arxiv.org/abs/2507.08738)
*Azimov Sherkhon,Susana Lopez-Moreno,Eric Dolores-Cuenca,Sieun Lee,Sangil Kim*

Main category: cs.LG

TL;DR: 提出自适应NVAR模型，结合延迟嵌入线性输入与可学习MLP生成的特征，实验显示其在预测准确性和抗噪性上优于标准NVAR。


<details>
  <summary>Details</summary>
Motivation: 标准NVAR和RC因依赖固定非线性，在高噪声或真实数据适应性差，且在高维场景扩展性不佳。

Method: 提出自适应NVAR模型，结合延迟嵌入线性输入和可学习MLP生成的特征，联合训练MLP和线性读出。

Result: 在无噪声和合成噪声条件下的混沌系统实验中，自适应模型预测准确性更高，在低观测频率和噪声条件下预测更稳健。

Conclusion: 自适应NVAR模型相比标准NVAR具有更好的预测性能和扩展性。

Abstract: Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have
shown promise in forecasting chaotic dynamical systems, such as the Lorenz-63
model and El Nino-Southern Oscillation. However, their reliance on fixed
nonlinearities - polynomial expansions in NVAR or random feature maps in RC -
limits their adaptability to high noise or real-world data. These methods also
scale poorly in high-dimensional settings due to costly matrix inversion during
readout computation. We propose an adaptive NVAR model that combines
delay-embedded linear inputs with features generated by a shallow, learnable
multi-layer perceptron (MLP). The MLP and linear readout are jointly trained
using gradient-based optimization, enabling the model to learn data-driven
nonlinearities while preserving a simple readout structure. Unlike standard
NVAR, our approach avoids the need for an exhaustive and sensitive grid search
over ridge and delay parameters. Instead, tuning is restricted to neural
network hyperparameters, improving scalability. Initial experiments on chaotic
systems tested under noise-free and synthetically noisy conditions showed that
the adaptive model outperformed the standard NVAR in predictive accuracy and
showed robust forecasting under noisy conditions with a lower observation
frequency.

</details>


### [100] [Partitioned Hybrid Quantum Fourier Neural Operators for Scientific Quantum Machine Learning](https://arxiv.org/abs/2507.08746)
*Paolo Marcandelli,Yuanchun He,Stefano Mariani,Martina Siena,Stefano Markidis*

Main category: cs.LG

TL;DR: 本文介绍了分区混合量子傅里叶神经算子（PHQFNO），它是量子傅里叶神经算子（QFNO）的推广，在科学机器学习中表现良好，能恢复经典FNO精度，部分场景精度更高且稳定性更好。


<details>
  <summary>Details</summary>
Motivation: 推广量子傅里叶神经算子，实现可调节的量子 - 经典混合以及在量子和经典设备上的分布式执行。

Method: 将傅里叶算子计算在经典和量子资源上分区，扩展QFNO到更高维度，采用消息传递框架，用一元编码将输入数据编码到量子态，用变分方案优化量子电路参数。

Result: 在Burgers方程、不可压缩和可压缩Navier - Stokes方程上评估，恢复经典FNO精度，在不可压缩Navier - Stokes方程上精度高于经典对应物，在输入噪声下稳定性优于经典基线。

Conclusion: PHQFNO是一种有效的科学机器学习方法，在精度和稳定性上有优势。

Abstract: We introduce the Partitioned Hybrid Quantum Fourier Neural Operator (PHQFNO),
a generalization of the Quantum Fourier Neural Operator (QFNO) for scientific
machine learning. PHQFNO partitions the Fourier operator computation across
classical and quantum resources, enabling tunable quantum-classical
hybridization and distributed execution across quantum and classical devices.
The method extends QFNOs to higher dimensions and incorporates a
message-passing framework to distribute data across different partitions. Input
data are encoded into quantum states using unary encoding, and quantum circuit
parameters are optimized using a variational scheme. We implement PHQFNO using
PennyLane with PyTorch integration and evaluate it on Burgers' equation,
incompressible and compressible Navier-Stokes equations. We show that PHQFNO
recovers classical FNO accuracy. On incompressible Navier-Stokes, PHQFNO
achieves higher accuracy than its classical counterparts. Finally, we perform a
sensitivity analysis under input noise, confirming improved stability of PHQFNO
over classical baselines.

</details>


### [101] [Modeling Partially Observed Nonlinear Dynamical Systems and Efficient Data Assimilation via Discrete-Time Conditional Gaussian Koopman Network](https://arxiv.org/abs/2507.08749)
*Chuanqi Chen,Zhongrui Wang,Nan Chen,Jin-Long Wu*

Main category: cs.LG

TL;DR: 本文开发离散时间条件高斯Koopman网络（CGKN）用于高维复杂动力系统状态预测和数据同化，在多个典型问题上验证性能，还展示了统一科学机器学习模型开发与其他应用的范例。


<details>
  <summary>Details</summary>
Motivation: 为高维复杂动力系统（如非线性偏微分方程控制的系统）开发能进行高效状态预测和数据同化的代理模型。

Method: 利用Koopman嵌入发现未观测系统状态的潜在表示，使潜在状态动力学为条件线性，将观测和潜在状态建模为条件高斯系统，通过解析公式评估潜在状态后验分布，并将数据同化性能融入学习过程。

Result: 在多个典型问题上，离散时间CGKN框架在状态预测上与最先进的科学机器学习方法性能相当，且能提供高效准确的数据同化结果。

Conclusion: 离散时间CGKN框架可用于高维复杂动力系统的状态预测和数据同化，还能作为统一科学机器学习模型开发与其他外循环应用的示例。

Abstract: A discrete-time conditional Gaussian Koopman network (CGKN) is developed in
this work to learn surrogate models that can perform efficient state forecast
and data assimilation (DA) for high-dimensional complex dynamical systems,
e.g., systems governed by nonlinear partial differential equations (PDEs).
Focusing on nonlinear partially observed systems that are common in many
engineering and earth science applications, this work exploits Koopman
embedding to discover a proper latent representation of the unobserved system
states, such that the dynamics of the latent states are conditional linear,
i.e., linear with the given observed system states. The modeled system of the
observed and latent states then becomes a conditional Gaussian system, for
which the posterior distribution of the latent states is Gaussian and can be
efficiently evaluated via analytical formulae. The analytical formulae of DA
facilitate the incorporation of DA performance into the learning process of the
modeled system, which leads to a framework that unifies scientific machine
learning (SciML) and data assimilation. The performance of discrete-time CGKN
is demonstrated on several canonical problems governed by nonlinear PDEs with
intermittency and turbulent features, including the viscous Burgers' equation,
the Kuramoto-Sivashinsky equation, and the 2-D Navier-Stokes equations, with
which we show that the discrete-time CGKN framework achieves comparable
performance as the state-of-the-art SciML methods in state forecast and
provides efficient and accurate DA results. The discrete-time CGKN framework
also serves as an example to illustrate unifying the development of SciML
models and their other outer-loop applications such as design optimization,
inverse problems, and optimal control.

</details>


### [102] [ML-Based Automata Simplification for Symbolic Accelerators](https://arxiv.org/abs/2507.08751)
*Tiffany Yu,Rye Stahle-Smith,Darssan Eswaramoorthi,Rasha Karakchi*

Main category: cs.LG

TL;DR: 提出AutoSlim框架简化基于NFA的符号加速器图复杂度，评估显示可减少FPGA资源使用并能处理更大图。


<details>
  <summary>Details</summary>
Motivation: 符号加速器因内存使用和路由复杂度面临可扩展性问题，需要简化基于NFA的符号加速器。

Method: AutoSlim使用随机森林分类，基于边得分和结构特征修剪低影响转换。

Result: AutoSlim可减少40%的FPGA LUT，过渡修剪超30%，能处理比现有基准大一个数量级的图。

Conclusion: AutoSlim能有效简化图复杂度，减轻硬件资源膨胀问题，硬件互连对成本影响大。

Abstract: Symbolic accelerators are increasingly used for symbolic data processing in
domains such as genomics, NLP, and cybersecurity. However, these accelerators
face scalability issues due to excessive memory use and routing complexity,
especially when targeting a large set. We present AutoSlim, a machine
learning-based graph simplification framework designed to reduce the complexity
of symbolic accelerators built on Non-deterministic Finite Automata (NFA)
deployed on FPGA-based overlays such as NAPOLY+. AutoSlim uses Random Forest
classification to prune low-impact transitions based on edge scores and
structural features, significantly reducing automata graph density while
preserving semantic correctness. Unlike prior tools, AutoSlim targets automated
score-aware simplification with weighted transitions, enabling efficient
ranking-based sequence analysis. We evaluated data sets (1K to 64K nodes) in
NAPOLY+ and conducted performance measurements including latency, throughput,
and resource usage. AutoSlim achieves up to 40 percent reduction in FPGA LUTs
and over 30 percent pruning in transitions, while scaling to graphs an order of
magnitude larger than existing benchmarks. Our results also demonstrate how
hardware interconnection (fanout) heavily influences hardware cost and that
AutoSlim's pruning mitigates resource blowup.

</details>


### [103] [Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data](https://arxiv.org/abs/2507.08761)
*Jeonghye Kim,Yongjae Shin,Whiyoung Jung,Sunghoon Hong,Deunsol Yoon,Youngchul Sung,Kanghoon Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: 提出PARS算法解决离线强化学习Q值外推误差问题，在D4RL基准测试中表现优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中Q值外推误差问题。

Method: 提出通过奖励缩放与层归一化（RS - LN）引导数据范围外Q值逐渐降低，以及对不可行动作的惩罚机制（PA），并结合二者开发PARS算法。

Result: PARS算法在D4RL基准测试的离线训练和在线微调中表现优于现有算法，在AntMaze Ultra任务中取得显著成功。

Conclusion: PARS算法有效解决了离线强化学习Q值外推误差问题，具有更好的性能。

Abstract: Reinforcement learning with offline data suffers from Q-value extrapolation
errors. To address this issue, we first demonstrate that linear extrapolation
of the Q-function beyond the data range is particularly problematic. To
mitigate this, we propose guiding the gradual decrease of Q-values outside the
data range, which is achieved through reward scaling with layer normalization
(RS-LN) and a penalization mechanism for infeasible actions (PA). By combining
RS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a
range of tasks, demonstrating superior performance compared to state-of-the-art
algorithms in both offline training and online fine-tuning on the D4RL
benchmark, with notable success in the challenging AntMaze Ultra task.

</details>


### [104] [BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity](https://arxiv.org/abs/2507.08771)
*Chenyang Song,Weilin Zhao,Xu Han,Chaojun Xiao,Yingfa Chen,Yuxuan Li,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: 为减轻大语言模型计算负担，提出新MoE架构BlockFFN及相关技术，实验显示其性能优越且能加速。


<details>
  <summary>Details</summary>
Motivation: 现有MoE架构路由不可微且不灵活，块级稀疏性低，不利于低资源加速和主流加速技术。

Method: 引入BlockFFN架构，使用集成ReLU和RMSNorm的路由器进行路由，设计CLS感知训练目标，实现高效加速内核。

Result: BlockFFN在性能上优于其他MoE基线，实现超80%的令牌级稀疏性和70%的8令牌块级稀疏性，内核在端侧设备上比密集模型加速达3.67倍。

Conclusion: BlockFFN架构及相关技术有效，代码和检查点公开。

Abstract: To alleviate the computational burden of large language models (LLMs),
architectures with activation sparsity, represented by mixture-of-experts
(MoE), have attracted increasing attention. However, the non-differentiable and
inflexible routing of vanilla MoE hurts model performance. Moreover, while each
token activates only a few parameters, these sparsely-activated architectures
exhibit low chunk-level sparsity, indicating that the union of multiple
consecutive tokens activates a large ratio of parameters. Such a sparsity
pattern is unfriendly for acceleration under low-resource conditions (e.g.,
end-side devices) and incompatible with mainstream acceleration techniques
(e.g., speculative decoding). To address these challenges, we introduce a novel
MoE architecture, BlockFFN, as well as its efficient training and deployment
techniques. Specifically, we use a router integrating ReLU activation and
RMSNorm for differentiable and flexible routing. Next, to promote both
token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training
objectives are designed, making BlockFFN more acceleration-friendly. Finally,
we implement efficient acceleration kernels, combining activation sparsity and
speculative decoding for the first time. The experimental results demonstrate
the superior performance of BlockFFN over other MoE baselines, achieving over
80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on
real end-side devices than dense models. All codes and checkpoints are
available publicly (https://github.com/thunlp/BlockFFN).

</details>


### [105] [Greedy Low-Rank Gradient Compression for Distributed Learning with Convergence Guarantees](https://arxiv.org/abs/2507.08784)
*Chuyan Chen,Yutong He,Pengrui Li,Weichen Jia,Kun Yuan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Distributed optimization is pivotal for large-scale signal processing and
machine learning, yet communication overhead remains a major bottleneck.
Low-rank gradient compression, in which the transmitted gradients are
approximated by low-rank matrices to reduce communication, offers a promising
remedy. Existing methods typically adopt either randomized or greedy
compression strategies: randomized approaches project gradients onto randomly
chosen subspaces, introducing high variance and degrading empirical
performance; greedy methods select the most informative subspaces, achieving
strong empirical results but lacking convergence guarantees. To address this
gap, we propose GreedyLore--the first Greedy Low-Rank gradient compression
algorithm for distributed learning with rigorous convergence guarantees.
GreedyLore incorporates error feedback to correct the bias introduced by greedy
compression and introduces a semi-lazy subspace update that ensures the
compression operator remains contractive throughout all iterations. With these
techniques, we prove that GreedyLore achieves a convergence rate of
$\mathcal{O}(\sigma/\sqrt{NT} + 1/T)$ under standard optimizers such as MSGD
and Adam--marking the first linear speedup convergence rate for low-rank
gradient compression. Extensive experiments are conducted to validate our
theoretical findings.

</details>


### [106] [Optimistic Exploration for Risk-Averse Constrained Reinforcement Learning](https://arxiv.org/abs/2507.08793)
*James McCarthy,Radu Marinescu,Elizabeth Daly,Ivana Dusparic*

Main category: cs.LG

TL;DR: 提出用于风险规避约束强化学习的乐观风险规避演员 - 评论家（ORAC）方法，防止收敛到次优策略，改善奖励 - 成本权衡。


<details>
  <summary>Details</summary>
Motivation: 传统风险规避约束强化学习易收敛到次优策略，无法充分最大化奖励或实现目标。

Method: 构建探索性策略，最大化状态 - 动作奖励值函数的局部上置信界，最小化风险规避状态 - 动作成本值函数的局部下置信界，并根据成本值与安全约束值的关系调整成本值权重。

Result: 在Safety - Gymnasium和CityLearn等连续控制任务中，ORAC方法防止收敛到次优策略，显著改善奖励 - 成本权衡。

Conclusion: ORAC方法能有效解决风险规避约束强化学习中的次优策略问题，在多种连续控制任务中有良好表现。

Abstract: Risk-averse Constrained Reinforcement Learning (RaCRL) aims to learn policies
that minimise the likelihood of rare and catastrophic constraint violations
caused by an environment's inherent randomness. In general, risk-aversion leads
to conservative exploration of the environment which typically results in
converging to sub-optimal policies that fail to adequately maximise reward or,
in some cases, fail to achieve the goal. In this paper, we propose an
exploration-based approach for RaCRL called Optimistic Risk-averse Actor Critic
(ORAC), which constructs an exploratory policy by maximising a local upper
confidence bound of the state-action reward value function whilst minimising a
local lower confidence bound of the risk-averse state-action cost value
function. Specifically, at each step, the weighting assigned to the cost value
is increased or decreased if it exceeds or falls below the safety constraint
value. This way the policy is encouraged to explore uncertain regions of the
environment to discover high reward states whilst still satisfying the safety
constraints. Our experimental results demonstrate that the ORAC approach
prevents convergence to sub-optimal policies and improves significantly the
reward-cost trade-off in various continuous control tasks such as
Safety-Gymnasium and a complex building energy management environment
CityLearn.

</details>


### [107] [One Token to Fool LLM-as-a-Judge](https://arxiv.org/abs/2507.08794)
*Yulai Zhao,Haolin Liu,Dian Yu,S. Y. Kung,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 研究发现生成式奖励模型易受表面操纵影响，提出数据增强策略训练更鲁棒模型并发布成果。


<details>
  <summary>Details</summary>
Motivation: 生成式奖励模型在RLVR中广泛应用，但现有模型易受非词符号和推理开头语等表面操纵影响，威胁相关算法范式。

Method: 引入简单有效的数据增强策略训练新的生成式奖励模型。

Result: 新模型的鲁棒性大幅提升。

Conclusion: 强调需要更可靠的基于大语言模型的评估方法，并发布鲁棒通用领域奖励模型及合成训练数据。

Abstract: Generative reward models (also known as LLMs-as-judges), which use large
language models (LLMs) to evaluate answer quality, are increasingly adopted in
reinforcement learning with verifiable rewards (RLVR). They are often preferred
over rigid rule-based metrics, especially for complex reasoning tasks involving
free-form outputs. In this paradigm, an LLM is typically prompted to compare a
candidate answer against a ground-truth reference and assign a binary reward
indicating correctness. Despite the seeming simplicity of this comparison task,
we find that generative reward models exhibit surprising vulnerabilities to
superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning
openers like "Thought process:" and "Let's solve this problem step by step."
can often lead to false positive rewards. We demonstrate that this weakness is
widespread across LLMs, datasets, and prompt formats, posing a serious threat
for core algorithmic paradigms that rely on generative reward models, such as
rejection sampling, preference optimization, and RLVR. To mitigate this issue,
we introduce a simple yet effective data augmentation strategy and train a new
generative reward model with substantially improved robustness. Our findings
highlight the urgent need for more reliable LLM-based evaluation methods. We
release our robust, general-domain reward model and its synthetic training data
at https://huggingface.co/sarosavo/Master-RM and
https://huggingface.co/datasets/sarosavo/Master-RM.

</details>


### [108] [The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?](https://arxiv.org/abs/2507.08802)
*Denis Sutter,Julian Minder,Thomas Hofmann,Tiago Pimentel*

Main category: cs.LG

TL;DR: 文章探讨因果抽象概念，证明无限制的因果抽象无意义，提出非线性表示困境，指出因果抽象不足以实现机械可解释性。


<details>
  <summary>Details</summary>
Motivation: 多数可解释性论文基于线性表示假设实现因果抽象的映射，但因果抽象定义不要求线性约束，需批判性审视该概念。

Method: 考虑任意强大的对齐映射，进行理论证明并辅以实证。

Result: 理论上在合理假设下，任何神经网络可映射到任何算法；实证上即使模型无法解决实际任务，也能完美映射到算法。

Conclusion: 因果抽象若无模型信息编码假设则无意义，不足以实现机械可解释性，研究信息编码假设与因果抽象的联系值得未来探索。

Abstract: The concept of causal abstraction got recently popularised to demystify the
opaque decision-making processes of machine learning models; in short, a neural
network can be abstracted as a higher-level algorithm if there exists a
function which allows us to map between them. Notably, most interpretability
papers implement these maps as linear functions, motivated by the linear
representation hypothesis: the idea that features are encoded linearly in a
model's representations. However, this linearity constraint is not required by
the definition of causal abstraction. In this work, we critically examine the
concept of causal abstraction by considering arbitrarily powerful alignment
maps. In particular, we prove that under reasonable assumptions, any neural
network can be mapped to any algorithm, rendering this unrestricted notion of
causal abstraction trivial and uninformative. We complement these theoretical
findings with empirical evidence, demonstrating that it is possible to
perfectly map models to algorithms even when these models are incapable of
solving the actual task; e.g., on an experiment using randomly initialised
language models, our alignment maps reach 100% interchange-intervention
accuracy on the indirect object identification task. This raises the non-linear
representation dilemma: if we lift the linearity constraint imposed to
alignment maps in causal abstraction analyses, we are left with no principled
way to balance the inherent trade-off between these maps' complexity and
accuracy. Together, these results suggest an answer to our title's question:
causal abstraction is not enough for mechanistic interpretability, as it
becomes vacuous without assumptions about how models encode information.
Studying the connection between this information-encoding assumption and causal
abstraction should lead to exciting future work.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [109] [NeurOptimisation: The Spiking Way to Evolve](https://arxiv.org/abs/2507.08320)
*Jorge Mario Cruz-Duarte,El-Ghazali Talbi*

Main category: cs.NE

TL;DR: 针对人工智能系统能耗问题，提出NeurOptimiser框架，在Lava平台实现并评估，结果表明该方法有结构化种群动态、收敛性和低功耗可行性，为实时、低能耗和分布式优化提供路径。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统能耗不断增加，需要高效可扩展的计算模型，神经形态计算可应对此挑战。

Method: 提出NeurOptimiser框架，由神经形态启发单元组成，结合脉冲神经元动力学和脉冲触发扰动启发式，通过原生脉冲机制协调；在Intel的Lava平台上实现，在无噪声BBOB套件上评估，采用不同配置。

Result: 该方法展现出结构化种群动态、一致收敛性和毫瓦级功率可行性。

Conclusion: 基于脉冲的元启发式算法是实现实时、低能耗和分布式优化的可行途径。

Abstract: The increasing energy footprint of artificial intelligence systems urges
alternative computational models that are both efficient and scalable.
Neuromorphic Computing (NC) addresses this challenge by empowering event-driven
algorithms that operate with minimal power requirements through biologically
inspired spiking dynamics. We present the NeurOptimiser, a fully spike-based
optimisation framework that materialises the neuromorphic-based metaheuristic
paradigm through a decentralised NC system. The proposed approach comprises a
population of Neuromorphic Heuristic Units (NHUs), each combining spiking
neuron dynamics with spike-triggered perturbation heuristics to evolve
candidate solutions asynchronously. The NeurOptimiser's coordination arises
through native spiking mechanisms that support activity propagation, local
information sharing, and global state updates without external orchestration.
We implement this framework on Intel's Lava platform, targeting the Loihi 2
chip, and evaluate it on the noiseless BBOB suite up to 40 dimensions. We
deploy several NeurOptimisers using different configurations, mainly
considering dynamic systems such as linear and Izhikevich models for spiking
neural dynamics, and fixed and Differential Evolution mutation rules for
spike-triggered heuristics. Although these configurations are implemented as a
proof of concept, we document and outline further extensions and improvements
to the framework implementation. Results show that the proposed approach
exhibits structured population dynamics, consistent convergence, and
milliwatt-level power feasibility. They also position spike-native MHs as a
viable path toward real-time, low-energy, and decentralised optimisation.

</details>


### [110] [Enhancing Parameter Control Policies with State Information](https://arxiv.org/abs/2507.08368)
*Gianluca Covini,Denis Antipov,Carola Doerr*

Main category: cs.NE

TL;DR: 提出四个新基准测试，研究参数控制，发现利用额外信息可优化参数选择，提升算法性能。


<details>
  <summary>Details</summary>
Motivation: 现有最优控制策略已知情况少，限制自动化方法发展，需新基准测试。

Method: 针对RLSₖ算法优化LeadingOnes函数，设置不同基准，允许算法利用当前OneMax值等额外信息。

Result: 利用额外信息可做出更好参数选择，在边缘状态效果明显，显著加快期望运行时间。

Conclusion: 提出的基准测试是分析丰富状态空间参数控制方法及寻找最优策略的有前景的测试平台。

Abstract: Parameter control and dynamic algorithm configuration study how to
dynamically choose suitable configurations of a parametrized algorithm during
the optimization process. Despite being an intensively researched topic in
evolutionary computation, optimal control policies are known only for very few
cases, limiting the development of automated approaches to achieve them.
  With this work we propose four new benchmarks for which we derive optimal or
close-to-optimal control policies. More precisely, we consider the optimization
of the \LeadingOnes function via RLS$_{k}$, a local search algorithm allowing
for a dynamic choice of the mutation strength $k$. The benchmarks differ in
which information the algorithm can exploit to set its parameters and to select
offspring. In existing running time results, the exploitable information is
typically limited to the quality of the current-best solution. In this work, we
consider how additional information about the current state of the algorithm
can help to make better choices of parameters, and how these choices affect the
performance. Namely, we allow the algorithm to use information about the
current \OneMax value, and we find that it allows much better parameter
choices, especially in marginal states. Although those states are rarely
visited by the algorithm, such policies yield a notable speed-up in terms of
expected runtime. This makes the proposed benchmarks a challenging, but
promising testing ground for analysis of parameter control methods in rich
state spaces and of their ability to find optimal policies by catching the
performance improvements yielded by correct parameter choices.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [111] [The State of Computational Science in Fission and Fusion Energy](https://arxiv.org/abs/2507.08061)
*Andrea Morales Coto,Aditi Verma*

Main category: cs.SE

TL;DR: 2024年对103位计算科学家调查核工程软件工具情况，结果显示更多人倾向现代语言、开源代码和模块化软件，暗示行业前沿趋势。


<details>
  <summary>Details</summary>
Motivation: 工具对工程很重要，软件在核工程设计中占主导，想了解计算科学家用代码解决问题、使用工具及开发体验等情况。

Method: 对103位开发聚变和裂变能源代码的计算科学家进行调查。

Result: 软件工具潮流变化，更多人偏好现代编程语言、开源代码和模块化软件；倾向多物理代码，减少FORTRAN使用，增加现代语言使用，代码开发预算上升。

Conclusion: 核工程代码未来呈模块化、小计算量特点，受组织重视，结果可在线获取。

Abstract: The tools used to engineer something are just as important as the thing that
is actually being engineered. In fact, in many cases, the tools can indeed
determine what is engineerable. In fusion and fission1 energy engineering,
software has become the dominant tool for design. For that reason, in 2024, for
the first time ever, we asked 103 computational scientists developing the codes
used in fusion and fission energy about the problems they are attempting to
solve with their codes, the tools available to them to solve them, and their
end to end developer experience with said tools.
  The results revealed a changing tide in software tools in fusion and fission,
with more and more computational scientists preferring modern programming
languages, open-source codes, and modular software. These trends represent a
peek into what will happen 5 to 10 years in the future of nuclear engineering.
Since the majority of our respondents belonged to US national labs and
universities, these results hint at the most cutting-edge trends in the
industry. The insights included in the State of Computational Science in
Fission and Fusion Energy indicate a dramatic shift toward multiphysics codes,
a drop-off in the use of FORTRAN in favor of more modern languages like Python
and C++, and ever-rising budgets for code development, at times reaching $50M
in a single organization.
  Our survey paints a future of nuclear engineering codes that is modular in
nature, small in terms of compute, and increasingly prioritized by
organizations. Access to our results in web form are available online.

</details>


### [112] [Code with Me or for Me? How Increasing AI Automation Transforms Developer Workflows](https://arxiv.org/abs/2507.08149)
*Valerie Chen,Ameet Talwalkar,Robert Brennan,Graham Neubig*

Main category: cs.SE

TL;DR: 本文研究开发者与编码代理的交互，对比其与现有副驾驶工具对开发者生产力和体验的影响，指出编码代理有优势但推广存在挑战。


<details>
  <summary>Details</summary>
Motivation: 现有对编码代理的评估多依赖无人类参与的静态基准，缺乏对开发者与编码代理交互的研究，因此探索其对开发者生产力和体验的影响。

Method: 评估GitHub Copilot和OpenHands两个工具，招募常使用前者的参与者。

Result: 编码代理有潜力超越副驾驶工具协助开发者，减少任务完成所需的用户工作量，但推广存在挑战。

Conclusion: 研究揭示了编码代理对开发者工作流程的改变，为构建新代理的研究人员提供建议，强调采用更具自主性系统到开发者工作流程的关键挑战。

Abstract: Developers now have access to a growing array of increasingly autonomous AI
tools to support software development. While numerous studies have examined
developer use of copilots, which can provide chat assistance or code
completions, evaluations of coding agents, which can automatically write files
and run code, still largely rely on static benchmarks without
humans-in-the-loop. In this work, we conduct the first academic study to
explore developer interactions with coding agents and characterize how more
autonomous AI tools affect user productivity and experience, compared to
existing copilots. We evaluate two leading copilot and agentic coding
assistants, GitHub Copilot and OpenHands, recruiting participants who regularly
use the former. Our results show agents have the potential to assist developers
in ways that surpass copilots (e.g., completing tasks that humans might not
have accomplished before) and reduce the user effort required to complete
tasks. However, there are challenges involved in enabling their broader
adoption, including how to ensure users have an adequate understanding of agent
behaviors. Our results not only provide insights into how developer workflows
change as a result of coding agents but also highlight how user interactions
with agents differ from those with existing copilots, motivating a set of
recommendations for researchers building new agents. Given the broad set of
developers who still largely rely on copilot-like systems, our work highlights
key challenges of adopting more agentic systems into developer workflows.

</details>


### [113] [The Impact of Generative AI on Code Expertise Models: An Exploratory Study](https://arxiv.org/abs/2507.08160)
*Otávio Cury,Guilherme Avelino*

Main category: cs.SE

TL;DR: 研究GenAI对代码知识模型和Truck Factor算法的影响，发现其使现有专业指标可靠性或降低。


<details>
  <summary>Details</summary>
Motivation: GenAI提升开发效率但引发开发者对代码理解不足的担忧，假设这会影响用于识别开发者专业知识的源代码知识模型。

Method: 收集ChatGPT生成代码集成到GitHub项目的统计数据，通过调整GenAI贡献程度模拟不同场景。

Result: 多数场景对现有专业指标有可衡量的影响，说明当前专业指标具有敏感性。

Conclusion: 随着GenAI更多融入开发流程，现有专业指标的可靠性可能降低。

Abstract: Generative Artificial Intelligence (GenAI) tools for source code generation
have significantly boosted productivity in software development. However, they
also raise concerns, particularly the risk that developers may rely heavily on
these tools, reducing their understanding of the generated code. We hypothesize
that this loss of understanding may be reflected in source code knowledge
models, which are used to identify developer expertise. In this work, we
present an exploratory analysis of how a knowledge model and a Truck Factor
algorithm built upon it can be affected by GenAI usage. To investigate this, we
collected statistical data on the integration of ChatGPT-generated code into
GitHub projects and simulated various scenarios by adjusting the degree of
GenAI contribution. Our findings reveal that most scenarios led to measurable
impacts, indicating the sensitivity of current expertise metrics. This suggests
that as GenAI becomes more integrated into development workflows, the
reliability of such metrics may decrease.

</details>


### [114] [Leveraging Large Language Models for Classifying App Users' Feedback](https://arxiv.org/abs/2507.08250)
*Yasaman Abedini,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: 本文评估四种先进大语言模型（LLMs）在应用用户反馈分类的能力，利用它们标注数据增强数据集，以提升基于BERT的分类器性能。


<details>
  <summary>Details</summary>
Motivation: 现有应用用户反馈分类主要依赖监督机器学习算法，微调更通用分类器面临标注数据集有限的挑战，创建大量准确标注数据集耗时耗力。

Method: 在八个精心标注的数据集上进行实验，分析各LLMs在细粒度和粗粒度用户反馈分类的表现，将LLMs作为标注工具增强数据集。

Result: 精心设计提示下，LLMs能有效将用户反馈分类到粗粒度类别，用LLMs共识标注的数据增强训练集可显著提升分类器性能。

Conclusion: LLMs可用于增强标注数据集，提升应用用户反馈分类器性能。

Abstract: In recent years, significant research has been conducted into classifying
application (app) user feedback, primarily relying on supervised machine
learning algorithms. However, fine-tuning more generalizable classifiers based
on existing labeled datasets remains an important challenge, as creating large
and accurately labeled datasets often requires considerable time and resources.
In this paper, we evaluate the capabilities of four advanced LLMs, including
GPT-3.5-Turbo, GPT-4, Flan-T5, and Llama3-70b, to enhance user feedback
classification and address the challenge of the limited labeled dataset. To
achieve this, we conduct several experiments on eight datasets that have been
meticulously labeled in prior research. These datasets include user reviews
from app stores, posts from the X platform, and discussions from the public
forums, widely recognized as representative sources of app user feedback. We
analyze the performance of various LLMs in identifying both fine-grained and
coarse-grained user feedback categories. Given the substantial volume of daily
user feedback and the computational limitations of LLMs, we leverage these
models as an annotation tool to augment labeled datasets with general and
app-specific data. This augmentation aims to enhance the performance of
state-of-the-art BERT-based classification models. Our findings indicate that
LLMs when guided by well-crafted prompts, can effectively classify user
feedback into coarse-grained categories. Moreover, augmenting the training
dataset with datasets labeled using the consensus of LLMs can significantly
enhance classifier performance.

</details>


### [115] [Computing Floating-Point Errors by Injecting Perturbations](https://arxiv.org/abs/2507.08467)
*Youshuai Tan,Zhanwei Zhang,Jinfu Chen,Zishuo Ding,Jifeng Xuan,Weiyi Shang*

Main category: cs.SE

TL;DR: 论文提出PI - detector方法计算浮点误差，基于原子操作大数条件数观察，通过注入扰动对比结果，实验证明其高效准确。


<details>
  <summary>Details</summary>
Motivation: 现有检测浮点误差方法有实现难、执行时间长问题，ATOMU有误报，FPCC速度慢，需新方法解决。

Method: PI - detector注入小扰动到程序原子操作的操作数，对比原程序和扰动程序结果计算浮点误差。

Result: 用ATOMU、HSED数据集及复杂线性系统求解程序评估，PI - detector能高效准确计算浮点误差。

Conclusion: PI - detector可有效且高效地计算浮点误差。

Abstract: Floating-point programs form the foundation of modern science and
engineering, providing the essential computational framework for a wide range
of applications, such as safety-critical systems, aerospace engineering, and
financial analysis. Floating-point errors can lead to severe consequences.
Although floating-point errors widely exist, only a subset of inputs may
trigger significant errors in floating-point programs. Therefore, it is crucial
to determine whether a given input could produce such errors. Researchers tend
to take the results of high-precision floating-point programs as oracles for
detecting floating-point errors, which introduces two main limitations: (1)
difficulty of implementation and (2) prolonged execution time. The two recent
tools, ATOMU and FPCC, can partially address these issues. However, ATOMU
suffers from false positives; while FPCC, though eliminating false positives,
operates at a considerably slower speed.
  To address these two challenges, we propose a novel approach named
PI-detector to computing floating-point errors effectively and efficiently. Our
approach is based on the observation that floating-point errors stem from large
condition numbers in atomic operations (such as addition and subtraction),
which then propagate and accumulate. PI-detector injects small perturbations
into the operands of individual atomic operations within the program and
compares the outcomes of the original program with the perturbed version to
compute floating-point errors. We evaluate PI-detector with datasets from ATOMU
and HSED, as well as a complex linear system-solving program. Experimental
results demonstrate that PI-detector can perform efficient and accurate
floating-point error computation.

</details>


### [116] [InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching](https://arxiv.org/abs/2507.08523)
*Yilun Wang,Pengfei Chen,Haiyu Huang,Zilong He,Gou Tan,Chuanfu Zhang,Jingkai He,Zibin Zheng*

Main category: cs.SE

TL;DR: 现有大语言模型用于日志解析在生产环境有局限，提出InferLog优化方法，实验显示其加速解析且不降低精度。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统日志解析需求大，现有大语言模型用于日志解析在生产环境存在隐私风险和无法满足高吞吐量低延迟要求的问题。

Method: 设计Prefix - aware ICL Refinement策略提高前缀缓存效率，基于元学习构建快速特定任务配置调优管道。

Result: 基于Loghub数据集和vLLM的实验表明，InferLog显著优于现有推理优化方法，加速解析且不降低精度。

Conclusion: InferLog有效解决了基于大语言模型的在线日志解析中推理效率瓶颈问题。

Abstract: Modern software systems generate massive volumes of runtime logs,
necessitating efficient and accurate log parsing to enable critical downstream
tasks such as anomaly detection and root cause analysis. Recently, large
language models (LLMs) have achieved advanced accuracy on log parsing, but
their deployment in production environments faces two major limitations: (1)
the privacy risks associated with commercial LLMs, driving the adoption of
local deployment, and (2) the stringent latency and throughput requirements
imposed by high-volume log streams, which existing LLM-based parsers fail to
meet. Although recent efforts have reduced the number of LLM queries, they
overlook the high latency of the LLM invocations, where concurrent log parsing
requests can cause serve performance degradation of LLM inference system.
  In this study, we present InferLog, the first LLM inference optimization
method for online log parsing. Our key insight is that the inference efficiency
emerges as the vital bottleneck in LLM-based online log parsing, rather than
parsing accuracy. InferLog accelerates inference by designing (1) A
Prefix-aware ICL Refinement policy to refine the examples and permutation of
in-context learning to improve the prefix caching efficiency. (2) A rapid and
task-specific configuration tuning pipeline based on meta-learning to find the
optimal LLM scheduling-related configuration for dynamic log parsing workloads.
The experimental results based on Loghub dataset and vLLM demonstrate that
InferLog significantly outperforms existing inference optimization methods and
markedly accelerates the state-of-the-art LLM-based log parser without
compromising parsing accuracy.

</details>


### [117] [Generating Proto-Personas through Prompt Engineering: A Case Study on Efficiency, Effectiveness and Empathy](https://arxiv.org/abs/2507.08594)
*Fernando Ayach,Vitor Lameirão,Raul Leão,Jerfferson Felizardo,Rafael Sobrinho,Vanessa Borges,Patrícia Matsubara,Awdren Fontão*

Main category: cs.SE

TL;DR: 提出基于提示工程的生成式AI方法生成原型人物角色，通过案例研究评估，结果显示该方法有效但有局限。


<details>
  <summary>Details</summary>
Motivation: 手动创建原型人物角色耗时、易有偏差，需借助生成式AI提高效率和质量。

Method: 在真实精益启动项目中，对19名参与者进行案例研究，采用定性和定量方法。

Result: 该方法提高效率、质量和可复用性，用户接受度高，但存在泛化和领域特异性问题，不同参与者共情程度有差异。

Conclusion: 为生成式AI融入软件产品发现实践提供新证据，指出混合设计流程后续需解决的关键挑战。

Abstract: Proto-personas are commonly used during early-stage Product Discovery, such
as Lean Inception, to guide product definition and stakeholder alignment.
However, the manual creation of proto-personas is often time-consuming,
cognitively demanding, and prone to bias. In this paper, we propose and
empirically investigate a prompt engineering-based approach to generate
proto-personas with the support of Generative AI (GenAI). Our goal is to
evaluate the approach in terms of efficiency, effectiveness, user acceptance,
and the empathy elicited by the generated personas. We conducted a case study
with 19 participants embedded in a real Lean Inception, employing a qualitative
and quantitative methods design. The results reveal the approach's efficiency
by reducing time and effort and improving the quality and reusability of
personas in later discovery phases, such as Minimum Viable Product (MVP)
scoping and feature refinement. While acceptance was generally high, especially
regarding perceived usefulness and ease of use, participants noted limitations
related to generalization and domain specificity. Furthermore, although
cognitive empathy was strongly supported, affective and behavioral empathy
varied significantly across participants. These results contribute novel
empirical evidence on how GenAI can be effectively integrated into software
Product Discovery practices, while also identifying key challenges to be
addressed in future iterations of such hybrid design processes.

</details>


### [118] [NL in the Middle: Code Translation with LLMs and Intermediate Representations](https://arxiv.org/abs/2507.08627)
*Chi-en Amy Tai,Pengyu Nie,Lukasz Golab,Alexander Wong*

Main category: cs.SE

TL;DR: 研究探索大语言模型代码翻译中使用自然语言和抽象语法树中间表示的效果，发现含中间自然语言总结的思维链提示表现最佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代码翻译存在错误，中间表示或可提高翻译准确性。

Method: 考虑多种集成自然语言和抽象语法树中间表示的提示工程方式，在CodeNet和AVATAR基准上使用Open Gpt4 8X7B、StarCoder和CodeGen模型进行测试。

Result: 含中间自然语言总结的思维链提示表现最佳，最佳模型Open Gpt4 8X7B成功翻译率较零样本提示分别提升13.8%和6.7%。

Conclusion: 在大语言模型代码翻译中，使用含中间自然语言总结的思维链提示可有效提高翻译准确性。

Abstract: Studies show that large language models (LLMs) produce buggy code
translations. One avenue to improve translation accuracy is through
intermediate representations, which could provide structured insights to guide
the model's understanding. We explore whether code translation using LLMs can
benefit from intermediate representations via natural language (NL) and
abstract syntax trees (ASTs). Since prompt engineering greatly affects LLM
performance, we consider several ways to integrate these representations, from
one-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and
specialized StarCoder and CodeGen models on popular code translation benchmarks
(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs
best, with an increase of 13.8% and 6.7%, respectively, in successful
translations for the best-performing model (Open Gpt4 8X7B) compared to the
zero-shot prompt.

</details>


### [119] [LLMCup: Ranking-Enhanced Comment Updating with LLMs](https://arxiv.org/abs/2507.08671)
*Hua Ge,Juan Zhai,Minxue Pan,Fusen He,Ziyue Tan*

Main category: cs.SE

TL;DR: 现有方法更新代码注释有局限，提出LLMCup框架，实验显示其效果优于基线方法，用户研究凸显人工评估重要性。


<details>
  <summary>Details</summary>
Motivation: 现有自动注释更新方法存在不足，如遗漏关键信息、难处理复杂场景，且LLM选合适提示策略难，需更好的注释更新方案。

Method: 提出LLMCup框架，先用多种提示策略通过LLM生成候选更新注释，再用CupRank模型选最佳注释。

Result: LLMCup在准确率、BLEU - 4等指标上比基线方法有显著提升，有时更新的注释优于人工更新。

Conclusion: LLMCup框架有效，同时强调了人工评估在注释质量评估中的重要性。

Abstract: While comments are essential for enhancing code readability and
maintainability in modern software projects, developers are often motivated to
update code but not comments, leading to outdated or inconsistent documentation
that hinders future understanding and maintenance. Recent approaches such as
CUP and HebCup have attempted automatic comment updating using neural
sequence-to-sequence models and heuristic rules, respectively. However, these
methods can miss or misinterpret crucial information during comment updating,
resulting in inaccurate comments, and they often struggle with complex update
scenarios. Given these challenges, a promising direction lies in leveraging
large language models (LLMs), which have shown impressive performance in
software engineering tasks such as comment generation, code synthesis, and
program repair. This suggests their strong potential to capture the logic
behind code modifications - an ability that is crucial for the task of comment
updating. Nevertheless, selecting an appropriate prompt strategy for an LLM on
each update case remains challenging. To address this, we propose a novel
comment updating framework, LLMCup, which first uses multiple prompt strategies
to provide diverse candidate updated comments via an LLM, and then employs a
ranking model, CupRank, to select the best candidate as final updated comment.
Experimental results demonstrate the effectiveness of LLMCup, with improvements
over state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,
10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in
SentenceBert similarity. Furthermore, a user study shows that comments updated
by LLMCup sometimes surpass human-written updates, highlighting the importance
of incorporating human evaluation in comment quality assessment.

</details>


### [120] [Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning](https://arxiv.org/abs/2507.08730)
*Zezhen Xiang,Jingzhi Gong,Tao Chen*

Main category: cs.SE

TL;DR: 提出在线配置性能学习框架DHDA以应对动态环境下配置性能学习中的概念漂移问题，评估显示其有更好准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 动态环境下，现有离线和迁移学习方法难实时适应配置性能学习中的概念漂移，使配置性能学习具有挑战性。

Method: 采用双重层次适应，上层必要时重新划分数据并重新训练局部模型处理全局漂移，下层局部模型异步检测和适应局部漂移，结合增量更新和定期全量重新训练。

Result: 评估八个软件系统，相比现有方法，DHDA有更好准确性，能有效适应漂移，最多有2倍提升，开销合理。

Conclusion: DHDA能在处理概念漂移时提高不同局部模型的性能。

Abstract: Modern configurable software systems need to learn models that correlate
configuration and performance. However, when the system operates in dynamic
environments, the workload variations, hardware changes, and system updates
will inevitably introduce concept drifts at different levels - global drifts,
which reshape the performance landscape of the entire configuration space; and
local drifts, which only affect certain sub-regions of that space. As such,
existing offline and transfer learning approaches can struggle to adapt to
these implicit and unpredictable changes in real-time, rendering configuration
performance learning challenging. To address this, we propose DHDA, an online
configuration performance learning framework designed to capture and adapt to
these drifts at different levels. The key idea is that DHDA adapts to both the
local and global drifts using dually hierarchical adaptation: at the upper
level, we redivide the data into different divisions, within each of which the
local model is retrained, to handle global drifts only when necessary. At the
lower level, the local models of the divisions can detect local drifts and
adapt themselves asynchronously. To balance responsiveness and efficiency, DHDA
combines incremental updates with periodic full retraining to minimize
redundant computation when no drifts are detected. Through evaluating eight
software systems and against state-of-the-art approaches, we show that DHDA
achieves considerably better accuracy and can effectively adapt to drifts with
up to 2x improvements, while incurring reasonable overhead and is able to
improve different local models in handling concept drift.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [121] [Tensor train representations of Greeks for Fourier-based pricing of multi-asset options](https://arxiv.org/abs/2507.08482)
*Rihito Sakurai,Koichi Miyamoto,Tsuyoshi Okubo*

Main category: q-fin.CP

TL;DR: 提出单评估张量列框架计算多资产期权Greeks值，数值实验显示比蒙特卡罗方法提速且精度相当，ND方法更优。


<details>
  <summary>Details</summary>
Motivation: 解决蒙特卡罗模拟计算多资产期权Greeks值样本复杂度高的问题。

Method: 通过张量交叉插值压缩基于傅里叶变换的定价函数得到张量列，引入数值微分和解析两种方法计算Greeks值。

Result: 在五资产最小看涨期权数值实验中比蒙特卡罗方法提速达$10^{5} 	imes$，精度相当。

Conclusion: ND方法精度匹配或超过AN方法，构建张量列表示计算复杂度低，是首选。

Abstract: Efficient computation of Greeks for multi-asset options remains a key
challenge in quantitative finance. While Monte Carlo (MC) simulation is widely
used, it suffers from the large sample complexity for high accuracy. We propose
a framework to compute Greeks in a single evaluation of a tensor train (TT),
which is obtained by compressing the Fourier transform (FT)-based pricing
function via TT learning using tensor cross interpolation. Based on this TT
representation, we introduce two approaches to compute Greeks: a numerical
differentiation (ND) approach that applies a numerical differential operator to
one tensor core and an analytical (AN) approach that constructs the TT of
closed-form differentiation expressions of FT-based pricing. Numerical
experiments on a five-asset min-call option in the Black-Sholes model show
significant speed-ups of up to about $10^{5} \times$ over MC while maintaining
comparable accuracy. The ND approach matches or exceeds the accuracy of the AN
approach and requires lower computational complexity for constructing the TT
representation, making it the preferred choice.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [122] [Multi-Scale Network Dynamics and Systemic Risk: A Model Context Protocol Approach to Financial Markets](https://arxiv.org/abs/2507.08065)
*Avishek Bhandari*

Main category: q-fin.RM

TL;DR: 本文提出用MCP分析金融市场系统性风险的框架，结合多种方法，实证显示能揭示隐藏风险模式，有预警优势，框架开源。


<details>
  <summary>Details</summary>
Motivation: 分析金融市场系统性风险，为宏观审慎政策设计和监管干预策略提供新见解。

Method: 结合转移熵网络、基于代理的建模和小波分解，通过MCPFM R包实现，让异质金融主体通过结构化协议通信。

Result: 多尺度方法揭示隐藏的系统性风险模式，提出的系统性风险指数预警能力优于传统指标。

Conclusion: 该框架为宏观审慎政策和监管干预提供新见解，开源包便于研究和应用。

Abstract: This paper introduces a novel framework for analyzing systemic risk in
financial markets through multi-scale network dynamics using Model Context
Protocol (MCP) for agent communication. We develop an integrated approach that
combines transfer entropy networks, agent-based modeling, and wavelet
decomposition to capture information flows across temporal scales implemented
in the MCPFM (Model Context Protocol Financial Markets) R package. Our
methodology enables heterogeneous financial agents including high-frequency
traders, market makers, institutional investors, and regulators to communicate
through structured protocols while maintaining realistic market microstructure.
The empirical analysis demonstrates that our multi-scale approach reveals
previously hidden systemic risk patterns, with the proposed systemic risk index
achieving superior early warning capabilities compared to traditional measures.
The framework provides new insights for macroprudential policy design and
regulatory intervention strategies. The complete implementation is available as
an open-source R package at https://github.com/avishekb9/MCPFM to facilitate
reproducible research and practical applications.

</details>


### [123] [Entity-Specific Cyber Risk Assessment using InsurTech Empowered Risk Factors](https://arxiv.org/abs/2507.08193)
*Jiayi Guo,Zhiyun Quan,Linfeng Zhang*

Main category: q-fin.RM

TL;DR: 因公司不愿披露网络事件，高质量公开数据缺乏。提出InsurTech框架丰富数据，构建机器学习模型预测事件发生和频率，发现该框架增强预测稳健性，可生成风险概况助力决策。


<details>
  <summary>Details</summary>
Motivation: 解决高质量公开网络事件数据缺乏对网络风险评估实证研究和预测建模的限制问题。

Method: 提出InsurTech框架丰富数据，开发多标签分类和多输出回归模型，用分类器和回归器链探索事件类型依赖，应用可解释机器学习技术识别和交叉验证风险因素。

Result: 未观察到数据集中网络事件类型显著相关性，InsurTech赋能特征比传统风险因素增强预测和估计的稳健性。

Conclusion: 框架能生成透明、特定实体的网络风险概况，支持定制承保和主动缓解风险，为保险公司和组织提供数据驱动决策和合规规划见解。

Abstract: The lack of high-quality public cyber incident data limits empirical research
and predictive modeling for cyber risk assessment. This challenge persists due
to the reluctance of companies to disclose incidents that could damage their
reputation or investor confidence. Therefore, from an actuarial perspective,
potential resolutions conclude two aspects: the enhancement of existing cyber
incident datasets and the implementation of advanced modeling techniques to
optimize the use of the available data. A review of existing data-driven
methods highlights a significant lack of entity-specific organizational
features in publicly available datasets. To address this gap, we propose a
novel InsurTech framework that enriches cyber incident data with
entity-specific attributes. We develop various machine learning (ML) models: a
multilabel classification model to predict the occurrence of cyber incident
types (e.g., Privacy Violation, Data Breach, Fraud and Extortion, IT Error, and
Others) and a multioutput regression model to estimate their annual
frequencies. While classifier and regressor chains are implemented to explore
dependencies among cyber incident types as well, no significant correlations
are observed in our datasets. Besides, we apply multiple interpretable ML
techniques to identify and cross-validate potential risk factors developed by
InsurTech across ML models. We find that InsurTech empowered features enhance
prediction occurrence and frequency estimation robustness compared to only
using conventional risk factors. The framework generates transparent,
entity-specific cyber risk profiles, supporting customized underwriting and
proactive cyber risk mitigation. It provides insurers and organizations with
data-driven insights to support decision-making and compliance planning.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [124] [To Trade or Not to Trade: An Agentic Approach to Estimating Market Risk Improves Trading Decisions](https://arxiv.org/abs/2507.08584)
*Dimitrios Emmanoulopoulos,Ollie Olby,Justin Lyon,Namid R. Stillman*

Main category: q-fin.ST

TL;DR: 开发使用大语言模型迭代发现金融时间序列随机微分方程的智能体系统，模型指导的交易策略表现更优，结合大语言模型与模型发现可提升市场风险估计。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在金融等领域的智能体框架中缺乏原则性模型构建步骤，多依赖情感或趋势分析。

Method: 开发使用大语言模型迭代发现金融时间序列随机微分方程的智能体系统，通过传统回测和市场模拟器评估。

Result: 模型指导的交易策略优于标准大语言模型智能体，提高了多只股票的夏普比率。

Conclusion: 结合大语言模型与智能体模型发现能增强市场风险估计，实现更有利可图的交易决策。

Abstract: Large language models (LLMs) are increasingly deployed in agentic frameworks,
in which prompts trigger complex tool-based analysis in pursuit of a goal.
While these frameworks have shown promise across multiple domains including in
finance, they typically lack a principled model-building step, relying instead
on sentiment- or trend-based analysis. We address this gap by developing an
agentic system that uses LLMs to iteratively discover stochastic differential
equations for financial time series. These models generate risk metrics which
inform daily trading decisions. We evaluate our system in both traditional
backtests and using a market simulator, which introduces synthetic but causally
plausible price paths and news events. We find that model-informed trading
strategies outperform standard LLM-based agents, improving Sharpe ratios across
multiple equities. Our results show that combining LLMs with agentic model
discovery enhances market risk estimation and enables more profitable trading
decisions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [125] [Mallows Model with Learned Distance Metrics: Sampling and Maximum Likelihood Estimation](https://arxiv.org/abs/2507.08108)
*Yeganeh Alimohammadi,Kiana Asgari*

Main category: stat.ML

TL;DR: 文章提出Mallows模型的泛化版本，可直接从数据学习距离度量，开发FPTAS采样算法，提出MLE算法估计参数并证明一致性，用体育排名数据验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注固定距离，缺乏从数据直接学习距离度量的原则性方法，而实际排名因场景而异。

Method: 提出泛化Mallows模型，聚焦Lα距离；开发FPTAS采样算法；提出MLE算法联合估计参数。

Result: 开发的FPTAS算法能高效生成接近真实分布的样本；证明了估计量的强一致性。

Conclusion: 提出的方法有效，可直接从数据学习距离度量，且在理论和实证上均得到验证。

Abstract: \textit{Mallows model} is a widely-used probabilistic framework for learning
from ranking data, with applications ranging from recommendation systems and
voting to aligning language models with human
preferences~\cite{chen2024mallows, kleinberg2021algorithmic,
rafailov2024direct}. Under this model, observed rankings are noisy
perturbations of a central ranking $\sigma$, with likelihood decaying
exponentially in distance from $\sigma$, i.e, $P (\pi) \propto \exp\big(-\beta
\cdot d(\pi, \sigma)\big),$ where $\beta > 0$ controls dispersion and $d$ is a
distance function.
  Existing methods mainly focus on fixed distances (such as Kendall's $\tau$
distance), with no principled approach to learning the distance metric directly
from data. In practice, however, rankings naturally vary by context; for
instance, in some sports we regularly see long-range swaps (a low-rank team
beating a high-rank one), while in others such events are rare. Motivated by
this, we propose a generalization of Mallows model that learns the distance
metric directly from data. Specifically, we focus on $L_\alpha$ distances:
$d_\alpha(\pi,\sigma):=\sum_{i=1} |\pi(i)-\sigma(i)|^\alpha$.
  For any $\alpha\geq 1$ and $\beta>0$, we develop a Fully Polynomial-Time
Approximation Scheme (FPTAS) to efficiently generate samples that are
$\epsilon$- close (in total variation distance) to the true distribution. Even
in the special cases of $L_1$ and $L_2$, this generalizes prior results that
required vanishing dispersion ($\beta\to0$). Using this sampling algorithm, we
propose an efficient Maximum Likelihood Estimation (MLE) algorithm that jointly
estimates the central ranking, the dispersion parameter, and the optimal
distance metric. We prove strong consistency results for our estimators (for
any values of $\alpha$ and $\beta$), and we validate our approach empirically
using datasets from sports rankings.

</details>


### [126] [CLEAR: Calibrated Learning for Epistemic and Aleatoric Risk](https://arxiv.org/abs/2507.08150)
*Ilia Azizi,Juraj Bodik,Jakob Heiss,Bin Yu*

Main category: stat.ML

TL;DR: 提出CLEAR方法结合两种不确定性成分提升条件覆盖率，在17个数据集上有良好表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法不能平衡处理测量噪声的偶然不确定性和有限数据的认知不确定性，需要准确的不确定性量化方法用于可靠预测建模。

Method: 提出有两个不同参数的CLEAR校准方法，结合两种不确定性成分，可与任意偶然和认知估计器结合，还展示了与分位数回归和PCS框架集成的使用方式。

Result: 在17个真实数据集上，相比两个单独校准的基线，CLEAR的区间宽度平均提高28.2%和17.4%，同时保持名义覆盖率，在高认知或高偶然不确定性场景中提升明显。

Conclusion: CLEAR方法有效结合两种不确定性成分，能提升条件覆盖率，在不同数据集和不确定性场景中表现良好。

Abstract: Accurate uncertainty quantification is critical for reliable predictive
modeling, especially in regression tasks. Existing methods typically address
either aleatoric uncertainty from measurement noise or epistemic uncertainty
from limited data, but not necessarily both in a balanced way. We propose
CLEAR, a calibration method with two distinct parameters, $\gamma_1$ and
$\gamma_2$, to combine the two uncertainty components for improved conditional
coverage. CLEAR is compatible with any pair of aleatoric and epistemic
estimators; we show how it can be used with (i) quantile regression for
aleatoric uncertainty and (ii) ensembles drawn from the
Predictability-Computability-Stability (PCS) framework for epistemic
uncertainty. Across 17 diverse real-world datasets, CLEAR achieves an average
improvement of 28.2% and 17.4% in the interval width compared to the two
individually calibrated baselines while maintaining nominal coverage. This
improvement can be particularly evident in scenarios dominated by either high
epistemic or high aleatoric uncertainty.

</details>


### [127] [Admissibility of Stein Shrinkage for Batch Normalization in the Presence of Adversarial Attacks](https://arxiv.org/abs/2507.08261)
*Sofia Ivolgina,P. Thomas Fletcher,Baba C. Vemuri*

Main category: stat.ML

TL;DR: 论文提出将Stein收缩估计用于批量归一化（BN）的均值和方差估计，在有和没有对抗攻击的图像分类、分割任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: BN使用样本均值和方差进行特征图中心化和缩放，Stein收缩估计在均方误差意义上能得到更好的均值和方差估计，想探究其在对抗攻击下的效果。

Method: 证明在将对抗攻击建模为次高斯分布时，Stein收缩估计器在均值和方差估计上优于样本均值和方差估计器，并将其应用于BN。

Result: 在标准ResNet架构对CIFAR - 10数据进行图像分类、3D CNN对PPMI（神经影像）数据处理以及HRNet对Cityscape数据进行图像分割等任务中，使用Stein校正的批量归一化取得了SOTA性能结果。

Conclusion: Stein收缩估计可用于BN的均值和方差参数估计，能应用于有和无对抗攻击的图像分类（分割）任务。

Abstract: Batch normalization (BN) is a ubiquitous operation in deep neural networks
used primarily to achieve stability and regularization during network training.
BN involves feature map centering and scaling using sample means and variances,
respectively. Since these statistics are being estimated across the feature
maps within a batch, this problem is ideally suited for the application of
Stein's shrinkage estimation, which leads to a better, in the
mean-squared-error sense, estimate of the mean and variance of the batch. In
this paper, we prove that the Stein shrinkage estimator for the mean and
variance dominates over the sample mean and variance estimators in the presence
of adversarial attacks when modeling these attacks using sub-Gaussian
distributions. This facilitates and justifies the application of Stein
shrinkage to estimate the mean and variance parameters in BN and use it in
image classification (segmentation) tasks with and without adversarial attacks.
We present SOTA performance results using this Stein corrected batch norm in a
standard ResNet architecture applied to the task of image classification using
CIFAR-10 data, 3D CNN on PPMI (neuroimaging) data and image segmentation using
HRNet on Cityscape data with and without adversarial attacks.

</details>


### [128] [MIRRAMS: Towards Training Models Robust to Missingness Distribution Shifts](https://arxiv.org/abs/2507.08280)
*Jihye Lee,Minseo Kang,Dongha Kim*

Main category: stat.ML

TL;DR: 提出MIRRAMS框架解决训练和测试输入数据集中缺失分布偏移问题，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现实数据分析中训练和测试输入数据集缺失分布偏移影响预测性能，需要解决该问题。

Method: 引入MI稳健条件，提出对应损失项技术并形成MIRRAMS目标函数。

Result: MIRRAMS在各基准数据集实验中始终优于现有基线，不同缺失场景下性能稳定，无缺失数据时也达最优，可扩展到半监督学习任务。

Conclusion: MIRRAMS是通用学习的强大即用型框架。

Abstract: In real-world data analysis, missingness distributional shifts between
training and test input datasets frequently occur, posing a significant
challenge to achieving robust prediction performance. In this study, we propose
a novel deep learning framework designed to address such shifts in missingness
distributions. We begin by introducing a set of mutual information-based
conditions, called MI robustness conditions, which guide a prediction model to
extract label-relevant information while remaining invariant to diverse
missingness patterns, thereby enhancing robustness to unseen missingness
scenarios at test-time. To make these conditions practical, we propose simple
yet effective techniques to derive loss terms corresponding to each and
formulate a final objective function, termed MIRRAMS(Mutual Information
Regularization for Robustness Against Missingness Shifts). As a by-product, our
analysis provides a theoretical interpretation of the principles underlying
consistency regularization-based semi-supervised learning methods, such as
FixMatch. Extensive experiments across various benchmark datasets show that
MIRRAMS consistently outperforms existing baselines and maintains stable
performance across diverse missingness scenarios. Moreover, our approach
achieves state-of-the-art performance even without missing data and can be
naturally extended to address semi-supervised learning tasks, highlighting
MIRRAMS as a powerful, off-the-shelf framework for general-purpose learning.

</details>


### [129] [Optimal and Practical Batched Linear Bandit Algorithm](https://arxiv.org/abs/2507.08438)
*Sanghoon Yu,Min-hwan Oh*

Main category: stat.ML

TL;DR: 提出新的批处理算法BLAE，在批处理线性老虎机问题上实现最小最大最优后悔，计算开销低且性能好。


<details>
  <summary>Details</summary>
Motivation: 现有批处理线性老虎机方法在理论和实践中有计算复杂或表现不佳的问题。

Method: 提出BLAE算法，结合臂消除与正则化G - 最优设计，引入批处理最优设计和细化集中界的新技术。

Result: 在大小K制度下首次实现最小最大最优后悔，只需O(log log T)批，计算开销低，数值评估中优于现有方法。

Conclusion: BLAE是首个在所有制度下兼具理论最优性和实际优越性的算法。

Abstract: We study the linear bandit problem under limited adaptivity, known as the
batched linear bandit. While existing approaches can achieve near-optimal
regret in theory, they are often computationally prohibitive or underperform in
practice. We propose \texttt{BLAE}, a novel batched algorithm that integrates
arm elimination with regularized G-optimal design, achieving the minimax
optimal regret (up to logarithmic factors in $T$) in both large-$K$ and
small-$K$ regimes for the first time, while using only $O(\log\log T)$ batches.
Our analysis introduces new techniques for batch-wise optimal design and
refined concentration bounds. Crucially, \texttt{BLAE} demonstrates low
computational overhead and strong empirical performance, outperforming
state-of-the-art methods in extensive numerical evaluations. Thus,
\texttt{BLAE} is the first algorithm to combine provable minimax-optimality in
all regimes and practical superiority in batched linear bandits.

</details>


### [130] [Data Depth as a Risk](https://arxiv.org/abs/2507.08518)
*Arturo Castellanos,Pavlo Mozharovskyi*

Main category: stat.ML

TL;DR: 本文从新角度将半空间深度视为特定点标签下一组分类器的最小损失，提出“损失深度”家族，继承机器学习算法优势，适用于高维，利于异常检测。


<details>
  <summary>Details</summary>
Motivation: 从不同角度拓展数据深度概念，使其能利用机器学习算法优势并适用于高维场景。

Method: 将半空间深度重新解释为分类器的最小损失，通过改变损失和分类器集合得到“损失深度”家族。

Result: 新的损失深度框架继承了机器学习算法的计算效率和快速统计收敛率，适用于高维场景，且在异常检测实验中表现良好。

Conclusion: 新的损失深度易于解释且在异常检测中高效，为数据深度领域带来新视角。

Abstract: Data depths are score functions that quantify in an unsupervised fashion how
central is a point inside a distribution, with numerous applications such as
anomaly detection, multivariate or functional data analysis, arising across
various fields. The halfspace depth was the first depth to aim at generalising
the notion of quantile beyond the univariate case. Among the existing variety
of depth definitions, it remains one of the most used notions of data depth.
Taking a different angle from the quantile point of view, we show that the
halfspace depth can also be regarded as the minimum loss of a set of
classifiers for a specific labelling of the points. By changing the loss or the
set of classifiers considered, this new angle naturally leads to a family of
"loss depths", extending to well-studied classifiers such as, e.g., SVM or
logistic regression, among others. This framework directly inherits
computational efficiency of existing machine learning algorithms as well as
their fast statistical convergence rates, and opens the data depth realm to the
high-dimensional setting. Furthermore, the new loss depths highlight a
connection between the dataset and the right amount of complexity or simplicity
of the classifiers. The simplicity of classifiers as well as the interpretation
as a risk makes our new kind of data depth easy to explain, yet efficient for
anomaly detection, as is shown by experiments.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [131] [Estimating Marginal Likelihoods in Likelihood-Free Inference via Neural Density Estimation](https://arxiv.org/abs/2507.08734)
*Paul Bastide,Arnaud Estoup,Jean-Michel Marin,Julien Stoehr*

Main category: stat.CO

TL;DR: 提出用SNLE输出估计边际似然的简单通用方法


<details>
  <summary>Details</summary>
Motivation: 边际似然在贝叶斯模型选择中至关重要，但在无似然设置下计算困难，现有SBI技术通常不提供证据估计

Method: 提出一种简单通用的方法，利用SNLE的输出估计边际似然

Result: 未提及

Conclusion: 未提及

Abstract: The marginal likelihood, or evidence, plays a central role in Bayesian model
selection, yet remains notoriously challenging to compute in likelihood-free
settings. While Simulation-Based Inference (SBI) techniques such as Sequential
Neural Likelihood Estimation (SNLE) offer powerful tools to approximate
posteriors using neural density estimators, they typically do not provide
estimates of the evidence. In this technical report presented at BayesComp
2025, we present a simple and general methodology to estimate the marginal
likelihood using the output of SNLE.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [132] [Consciousness as a Jamming Phase](https://arxiv.org/abs/2507.08197)
*Kaichen Ouyang*

Main category: cond-mat.dis-nn

TL;DR: 本文构建神经阻塞相图，将大语言模型中意识的出现解释为高维无序系统的临界现象，用阻塞物理解释了神经语言模型的临界缩放。


<details>
  <summary>Details</summary>
Motivation: 解释大语言模型中意识的出现以及人工智能中的经验缩放定律。

Method: 建立与颗粒物质和其他复杂系统中阻塞转变的类比，确定控制神经网络相行为的三个基本参数：温度、体积分数和应力。

Result: 理论为人工智能中的经验缩放定律提供统一物理解释，发现相同热力学原理是神经网络中意识出现的基础，有共同临界特征。

Conclusion: 意识是一种阻塞相，通过长程相关性内在地连接知识组件。

Abstract: This paper develops a neural jamming phase diagram that interprets the
emergence of consciousness in large language models as a critical phenomenon in
high-dimensional disordered systems.By establishing analogies with jamming
transitions in granular matter and other complex systems, we identify three
fundamental control parameters governing the phase behavior of neural networks:
temperature, volume fraction, and stress.The theory provides a unified physical
explanation for empirical scaling laws in artificial intelligence,
demonstrating how computational cooling, density optimization, and noise
reduction collectively drive systems toward a critical jamming surface where
generalized intelligence emerges. Remarkably, the same thermodynamic principles
that describe conventional jamming transitions appear to underlie the emergence
of consciousness in neural networks, evidenced by shared critical signatures
including divergent correlation lengths and scaling exponents.Our work explains
neural language models' critical scaling through jamming physics, suggesting
consciousness is a jamming phase that intrinsically connects knowledge
components via long-range correlations.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [133] [Invariant-based Robust Weights Watermark for Large Language Models](https://arxiv.org/abs/2507.08288)
*Qingxiao Guo,Xinjie Zhu,Yilong Ma,Hui Jin,Yunhao Wang,Weifeng Zhang,Xiaobing Guo*

Main category: cs.CR

TL;DR: 本文针对大语言模型知识产权保护问题，提出一种无需重新训练或微调的鲁棒水印方案，并在三个流行模型上验证了其在多种攻击下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在资源受限边缘设备上的广泛部署，为应对恶意用户的知识产权盗窃威胁。

Method: 为每个用户生成唯一密钥，通过求解由模型不变量构建的线性约束导出稳定水印值，利用噪声机制在多用户场景中隐藏水印位置以抵御合谋攻击。

Result: 在Llama3、Phi3、Gemma三个流行模型上进行评估，实验结果证实该方案在多种攻击方法下具有强鲁棒性。

Conclusion: 所提出的水印方案无需重新训练或微调，能有效保护Transformer模型的知识产权，在多种攻击下表现出强鲁棒性。

Abstract: Watermarking technology has gained significant attention due to the
increasing importance of intellectual property (IP) rights, particularly with
the growing deployment of large language models (LLMs) on billions
resource-constrained edge devices. To counter the potential threats of IP theft
by malicious users, this paper introduces a robust watermarking scheme without
retraining or fine-tuning for transformer models. The scheme generates a unique
key for each user and derives a stable watermark value by solving linear
constraints constructed from model invariants. Moreover, this technology
utilizes noise mechanism to hide watermark locations in multi-user scenarios
against collusion attack. This paper evaluates the approach on three popular
models (Llama3, Phi3, Gemma), and the experimental results confirm the strong
robustness across a range of attack methods (fine-tuning, pruning,
quantization, permutation, scaling, reversible matrix and collusion attacks).

</details>


### [134] [White-Basilisk: A Hybrid Model for Code Vulnerability Detection](https://arxiv.org/abs/2507.08540)
*Ioannis Lamprou,Alexander Shevtsov,Ioannis Arapakis,Sotiris Ioannidis*

Main category: cs.CR

TL;DR: 介绍新型漏洞检测方法White - Basilisk，参数少但性能优，挑战AI模型扩展假设。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞扩散对网络安全构成挑战，需更有效的检测方法。

Method: 采用集成Mamba层、线性自注意力和混合专家框架的创新架构。

Result: 在漏洞检测任务中取得了最先进的结果，能处理超长序列，在不平衡的真实数据集上表现良好，且计算效率高。

Conclusion: 为代码安全建立了新基准，证明紧凑高效的模型在特定任务中可超越大型模型，可能重新定义AI开发的优化策略。

Abstract: The proliferation of software vulnerabilities presents a significant
challenge to cybersecurity, necessitating more effective detection
methodologies. We introduce White-Basilisk, a novel approach to vulnerability
detection that demonstrates superior performance while challenging prevailing
assumptions in AI model scaling. Utilizing an innovative architecture that
integrates Mamba layers, linear self-attention, and a Mixture of Experts
framework, White-Basilisk achieves state-of-the-art results in vulnerability
detection tasks with a parameter count of only 200M. The model's capacity to
process sequences of unprecedented length enables comprehensive analysis of
extensive codebases in a single pass, surpassing the context limitations of
current Large Language Models (LLMs). White-Basilisk exhibits robust
performance on imbalanced, real-world datasets, while maintaining computational
efficiency that facilitates deployment across diverse organizational scales.
This research not only establishes new benchmarks in code security but also
provides empirical evidence that compact, efficiently designed models can
outperform larger counterparts in specialized tasks, potentially redefining
optimization strategies in AI development for domain-specific applications.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [135] [Long-term Health and Human Capital Effects of Early-Life Economic Conditions](https://arxiv.org/abs/2507.08159)
*Ruijun Hou,Samuel Baker,Stephanie von Hinke,Hans H. Sievertsen,Emil Sørensen,Nicolai Vitt*

Main category: econ.GN

TL;DR: 研究生命早期1000天当地经济状况对长期健康和人力资本的影响，未发现早期经济状况小波动有影响。


<details>
  <summary>Details</summary>
Motivation: 探究生命早期1000天当地经济状况对长期健康和人力资本的影响。

Method: 结合1952 - 1967年英格兰和威尔士城市月度失业率历史数据与英国生物银行的晚年结果数据，利用国家特定行业冲击导致的失业率变化。

Result: 未发现生命早期当地经济状况的小而常见的波动会影响老年时的健康或人力资本。

Conclusion: 生命早期当地经济状况的小波动对老年健康和人力资本无显著影响。

Abstract: We study the long-term health and human capital impacts of local economic
conditions experienced during the first 1,000 days of life. We combine
historical data on monthly unemployment rates in urban England and Wales
1952-1967 with data from the UK Biobank on later-life outcomes. Leveraging
variation in unemployment driven by national industry-specific shocks weighted
by industry's importance in each area, we find no evidence that small, common
fluctuations in local economic conditions during the early life period affect
health or human capital in older age.

</details>


### [136] [Do Temporary Workers Face Higher Wage Markdowns? Evidence from India's Automotive Sector](https://arxiv.org/abs/2507.08222)
*Davide Luparello*

Main category: econ.GN

TL;DR: 本文研究2000 - 2020年印度汽车行业临时工和永久工的工资减价情况，发现产出加价下降、全要素生产率降低，临时工和永久工存在工资减价。


<details>
  <summary>Details</summary>
Motivation: 探究临时工是否比永久工面临不同的工资减价情况。

Method: 开发整合CES生产、嵌套Logit劳动力供给和差异化劳动力市场行为的模型，临时工采用Nash - Bertrand工资设定，工会化永久工采用Nash议价。

Result: 边际成本超过价格使产出加价下降，劳动增强型生产率上升无法抵消希克斯中性生产率下降，全要素生产率降低，临时工工资减价40%，永久工10%。

Conclusion: 劳动力市场力量显著压缩工人薪酬，临时工和永久工均存在工资减价。

Abstract: Are temporary workers subject to different wage markdowns than permanent
workers? This paper examines productivity, output markups, and wage markdowns
in India's automotive sector during 2000--2020. I develop a model integrating
CES production, nested logit labor supply, and differentiated labor market
conduct: Nash-Bertrand wage setting for temporary workers versus Nash
bargaining for unionized permanent workers. Results reveal declining output
markups as marginal costs outpace prices through productivity deceleration.
Rising labor-augmenting productivity cannot offset declining Hicks-neutral
productivity, reducing overall TFP. Labor market power substantially compresses
worker compensation: wage markdowns persist at 40% for temporary workers and
10% for permanent workers.

</details>


### [137] [Advancing AI Capabilities and Evolving Labor Outcomes](https://arxiv.org/abs/2507.08244)
*Jacob Dominski,Yong Suk Lee*

Main category: econ.GN

TL;DR: 本文通过构建动态职业AI暴露分数，分析AI能力进步对美国劳动力市场的影响，发现高AI暴露与就业减少、失业率上升和工作时间缩短有关，不同职业和人群受影响程度不同。


<details>
  <summary>Details</summary>
Motivation: 研究AI进步对劳动力市场的影响。

Method: 构建动态职业AI暴露分数，引入五阶段框架评估AI能力变化，将分数与美国当前人口调查数据关联，进行一阶差分分析。

Result: 高AI暴露与就业减少、失业率上升、工作时间缩短相关，部分人群出现兼职增加和全职就业减少，不同职业和人群受影响程度不同。

Conclusion: AI驱动的劳动力转移在广度和深度上均有体现，不同职业任务内容和人群受影响不同。

Abstract: This study investigates the labor market consequences of AI by analyzing near
real-time changes in employment status and work hours across occupations in
relation to advances in AI capabilities. We construct a dynamic Occupational AI
Exposure Score based on a task-level assessment using state-of-the-art AI
models, including ChatGPT 4o and Anthropic Claude 3.5 Sonnet. We introduce a
five-stage framework that evaluates how AI's capability to perform tasks in
occupations changes as technology advances from traditional machine learning to
agentic AI. The Occupational AI Exposure Scores are then linked to the US
Current Population Survey, allowing for near real-time analysis of employment,
unemployment, work hours, and full-time status. We conduct a first-differenced
analysis comparing the period from October 2022 to March 2023 with the period
from October 2024 to March 2025. Higher exposure to AI is associated with
reduced employment, higher unemployment rates, and shorter work hours. We also
observe some evidence of increased secondary job holding and a decrease in
full-time employment among certain demographics. These associations are more
pronounced among older and younger workers, men, and college-educated
individuals. College-educated workers tend to experience smaller declines in
employment but are more likely to see changes in work intensity and job
structure. In addition, occupations that rely heavily on complex reasoning and
problem-solving tend to experience larger declines in full-time work and
overall employment in association with rising AI exposure. In contrast, those
involving manual physical tasks appear less affected. Overall, the results
suggest that AI-driven shifts in labor are occurring along both the extensive
margin (unemployment) and the intensive margin (work hours), with varying
effects across occupational task content and demographics.

</details>


### [138] [From Revolution to Ruin: An Empirical Analysis Yemen's State Collapse](https://arxiv.org/abs/2507.08512)
*Riste Ichev,Rok Spruk*

Main category: econ.GN

TL;DR: 本文通过构建反事实基准评估也门革命与内战对宏观经济、人类发展和治理质量的影响，发现冲突导致经济和制度发展逆转。


<details>
  <summary>Details</summary>
Motivation: 评估也门2011年革命及后续内战对宏观经济轨迹、人类发展和治理质量的广泛影响。

Method: 利用1990 - 2022年37个发展中国家的平衡面板构建反事实基准，采用矩阵补全估计器和LASSO增强合成控制法。

Result: 冲突爆发使经济和制度发展逆转，产出和收入收缩，投资和贸易开放度下降，预期寿命和人类发展成果倒退，政治问责、行政能力等治理指标崩溃。

Conclusion: 不同实证策略结果一致，证明估计结果具有稳健性。

Abstract: We assess the broad repercussions of Yemen's 2011 revolution and subsequent
civil war on its macroeconomic trajectories, human development, and quality of
governance by constructing counterfactual benchmarks using a balanced panel of
37 developing countries over 1990-2022. Drawing on matrix-completion estimators
with alternative shrinkage regimes and a LASSO-augmented synthetic-control
method, we generate Yemen's hypothetical no-conflict paths for key
macroeconomic aggregates, demographic and health indicators, and governance
metrics. Across the full spectrum of methods, the conflict's outbreak
corresponds with a dramatic reversal of economic and institutional development.
We find that output and income experience an unprecedented contraction,
investment and trade openness deteriorate sharply, and gains in life expectancy
and human development are broadly reversed. Simultaneously, measures of
political accountability, administrative capacity, rule of law, and corruption
control collapse, reflecting systemic institutional breakdown. The concordance
of results across a variety of empirical strategies attests to the robustness
of our estimates.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [139] [Improving gravitational wave search sensitivity with TIER: Trigger Inference using Extended strain Representation](https://arxiv.org/abs/2507.08318)
*Digvijay Wadekar,Arush Pimpalkar,Mark Ho-Yeuk Cheung,Benjamin Wandelt,Emanuele Berti,Ajit Kumar Mehta,Tejaswi Venumadhav,Javier Roulet,Tousif Islam,Barak Zackay,Jonathan Mushkin,Matias Zaldarriaga*

Main category: gr-qc

TL;DR: 介绍机器学习框架TIER改进引力波搜索管道灵敏度，在O3数据测试有成效。


<details>
  <summary>Details</summary>
Motivation: 传统搜索管道仅用候选信号附近小区域应变数据构建检测统计量，扩展应变数据含互补信息，需利用。

Method: 训练基于扩展数据稀疏摘要表示/特征的ML分类器模型，框架易训练，可与现有候选结合，输出易集成到检测统计量。

Result: 在IAS - HM管道触发上使用TIER，LIGO - Virgo - Kagra O3数据敏感体积时间最多提升约20%，高质量和不等质量比区域改善集中。

Conclusion: TIER框架能提升引力波搜索管道灵敏度，增加近阈值引力波候选的显著性。

Abstract: We introduce a machine learning (ML) framework called $\texttt{TIER}$ for
improving the sensitivity of gravitational wave search pipelines. Typically,
search pipelines only use a small region of strain data in the vicinity of a
candidate signal to construct the detection statistic. However, extended strain
data ($\sim 10$ s) in the candidate's vicinity can also carry valuable
complementary information. We show that this information can be efficiently
captured by ML classifier models trained on sparse summary
representation/features of the extended data. Our framework is easy to train
and can be used with already existing candidates from any search pipeline, and
without requiring expensive injection campaigns. Furthermore, the output of our
model can be easily integrated into the detection statistic of a search
pipeline. Using $\texttt{TIER}$ on triggers from the $\texttt{IAS-HM}$
pipeline, we find up to $\sim 20\%$ improvement in sensitive volume time in
LIGO-Virgo-Kagra O3 data, with improvements concentrated in regions of high
masses and unequal mass ratios. Applying our framework increases the
significance of several near-threshold gravitational-wave candidates,
especially in the pair-instability mass gap and intermediate-mass black hole
(IMBH) ranges.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [140] [Dual-Attention U-Net++ with Class-Specific Ensembles and Bayesian Hyperparameter Optimization for Precise Wound and Scale Marker Segmentation](https://arxiv.org/abs/2507.05314)
*Daniel Cieślak,Miriam Reca,Olena Onyshchenko,Jacek Rumiński*

Main category: eess.IV

TL;DR: 提出双注意力U-Net++架构用于伤口和尺度标记分割，经评估F1分数达0.8640，对复杂医学分割任务有效。


<details>
  <summary>Details</summary>
Motivation: 准确分割临床图像中的伤口和尺度标记对伤口管理和自动评估至关重要，当前存在挑战。

Method: 提出双注意力U-Net++架构，结合通道和空间注意力机制；经5折交叉验证选EfficientNet - B7为编码器骨干；训练两个特定类模型，用数据增强和贝叶斯超参数调整；模型集成使用测试时增强。

Result: 在基准数据集上评估，F1分数达到0.8640。

Conclusion: 该方法对复杂医学分割任务有效。

Abstract: Accurate segmentation of wounds and scale markers in clinical images remainsa
significant challenge, crucial for effective wound management and
automatedassessment. In this study, we propose a novel dual-attention U-Net++
archi-tecture, integrating channel-wise (SCSE) and spatial attention mechanisms
toaddress severe class imbalance and variability in medical images
effectively.Initially, extensive benchmarking across diverse architectures and
encoders via 5-fold cross-validation identified EfficientNet-B7 as the optimal
encoder backbone.Subsequently, we independently trained two class-specific
models with tailoredpreprocessing, extensive data augmentation, and Bayesian
hyperparameter tun-ing (WandB sweeps). The final model ensemble utilized Test
Time Augmentationto further enhance prediction reliability. Our approach was
evaluated on a bench-mark dataset from the NBC 2025 & PCBBE 2025 competition.
Segmentationperformance was quantified using a weighted F1-score (75% wounds,
25% scalemarkers), calculated externally by competition organizers on
undisclosed hard-ware. The proposed approach achieved an F1-score of 0.8640,
underscoring itseffectiveness for complex medical segmentation tasks.

</details>


### [141] [Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging Pretrained 2D Foundation Models](https://arxiv.org/abs/2507.08254)
*Ulzee An,Moonseong Jeong,Simon A. Lee,Aditya Gorla,Yuzhe Yang,Sriram Sankararaman*

Main category: eess.IV

TL;DR: 本文介绍了免训练方法Raptor，可生成体积数据的语义丰富嵌入，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决开发体积成像数据基础模型时，高维训练的计算复杂性和数据集不足的问题。

Method: 引入Raptor方法，利用预训练的2D基础模型从医学体积的单个横截面提取视觉标记，再用随机投影进行空间压缩。

Result: 在十个不同的医学体积任务上，Raptor性能优于现有方法，如SuPreM高3%等，且无需昂贵的训练。

Conclusion: Raptor作为医学体积深度学习方法的基础，具有有效性和通用性。

Abstract: Current challenges in developing foundational models for volumetric imaging
data, such as magnetic resonance imaging (MRI), stem from the computational
complexity of training state-of-the-art architectures in high dimensions and
curating sufficiently large datasets of volumes. To address these challenges,
we introduce Raptor (Random Planar Tensor Reduction), a train-free method for
generating semantically rich embeddings for volumetric data. Raptor leverages a
frozen 2D foundation model, pretrained on natural images, to extract visual
tokens from individual cross-sections of medical volumes. These tokens are then
spatially compressed using random projections, significantly reducing
computational complexity while retaining semantic information. Extensive
experiments on ten diverse medical volume tasks verify the superior performance
of Raptor over state-of-the-art methods, including those pretrained exclusively
on medical volumes (+3% SuPreM, +6% MISFM, +10% Merlin, +13% VoCo, and +14%
SLIViT), while entirely bypassing the need for costly training. Our results
highlight the effectiveness and versatility of Raptor as a foundation for
advancing deep learning-based methods for medical volumes.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [142] [Effective regions and kernels in continuous sparse regularisation, with application to sketched mixtures](https://arxiv.org/abs/2507.08444)
*Yohann De Castro,Rémi Gribonval,Nicolas Jouvin*

Main category: math.ST

TL;DR: 本文用Beurling - LASSO推进连续稀疏正则化通用理论，提出核切换方法，增加LPC核列表，改进定位误差结果并在平移不变混合模型估计中验证。


<details>
  <summary>Details</summary>
Motivation: 先前工作在获得估计误差界时需逐例验证技术上的局部正曲率（LPC）假设，实际中满足该条件的LPC核很少。

Method: 提出核切换方法，将模型核与LPC假设解耦，利用已知LPC核作为枢轴核证明误差界；使用带限平滑和草图技术降低BLASSO计算负担。

Result: 证明“sinc - 4”核满足LPC假设；BLASSO在真实支持附近的定位误差随噪声水平降低；在平移不变混合模型估计中验证结果。

Conclusion: 核切换方法有效，增加了LPC核列表，改进了定位误差结果，在实际问题中具有应用价值。

Abstract: This paper advances the general theory of continuous sparse regularisation on
measures with the Beurling-LASSO (BLASSO). This TV-regularized convex program
on the space of measures allows to recover a sparse measure using a noisy
observation from an appropriate measurement operator. While previous works have
uncovered the central role played by this operator and its associated kernel in
order to get estimation error bounds, the latter requires a technical local
positive curvature (LPC) assumption to be verified on a case-by-case basis. In
practice, this yields only few LPC-kernels for which this condition is proved.
At the heart of our contribution lies the kernel switch, which uncouples the
model kernel from the LPC assumption: it enables to leverage any known
LPC-kernel as a pivot kernel to prove error bounds, provided embedding
conditions are verified between the model and pivot RKHS. We increment the list
of LPC-kernels, proving that the "sinc-4" kernel, used for signal recovery and
mixture problems, does satisfy the LPC assumption. Furthermore, we also show
that the BLASSO localisation error around the true support decreases with the
noise level, leading to effective near regions. This improves on known results
where this error is fixed with some parameters depending on the model kernel.
We illustrate the interest of our results in the case of translation-invariant
mixture model estimation, using bandlimiting smoothing and sketching techniques
to reduce the computational burden of BLASSO.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [143] [Finding a solution to the Erdős-Ginzburg-Ziv theorem in $O(n\log\log\log n)$ time](https://arxiv.org/abs/2507.08139)
*Yui Hin Arvin Leung*

Main category: math.CO

TL;DR: 本文给出两种更快算法解决特定问题，优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 改进寻找满足特定条件子序列的算法复杂度，突破基于FFT方法的O(n log n)复杂度。

Method: 提出了简单实用的O(n log log n)算法和理论上的O(n log log log n)算法。

Result: 新算法复杂度优于之前已知的O(n log n)方法。

Conclusion: 特定布尔卷积变体可以用比基于FFT方法更快的时间实现。

Abstract: The Erd\H{o}s-Ginzburg-Ziv theorem states that for any sequence of $2n-1$
integers, there exists a subsequence of $n$ elements whose sum is divisible by
$n$. In this article, we provide a simple, practical $O(n\log\log n)$ algorithm
and a theoretical $O(n\log\log\log n)$ algorithm, both of which improve upon
the best previously known $O(n\log n)$ approach. This shows that a specific
variant of boolean convolution can be implemented in time faster than the usual
$O(n\log n)$ expected from FFT-based methods.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [144] [Quantum Properties Trojans (QuPTs) for Attacking Quantum Neural Networks](https://arxiv.org/abs/2507.08202)
*Sounak Bhowmik,Travis S. Humble,Himanshu Thapliyal*

Main category: quant-ph

TL;DR: 本文提出针对量子神经网络（QNN）的量子属性特洛伊木马（QuPTs）攻击，显示其隐蔽性强且影响大。


<details>
  <summary>Details</summary>
Motivation: 量子神经网络的安全和鲁棒性研究不足，需探索其面临的攻击。

Method: 基于量子门的酉性质插入噪声和哈达玛门以实现叠加，开发QuPTs攻击QNN。

Result: QuPTs隐蔽性显著，对量子电路（尤其是QNN）性能影响大，最具影响的QuPT使QNN准确率下降23%。

Conclusion: 这是首个针对全量子神经网络且独立于混合经典 - 量子架构的特洛伊木马攻击研究。

Abstract: Quantum neural networks (QNN) hold immense potential for the future of
quantum machine learning (QML). However, QNN security and robustness remain
largely unexplored. In this work, we proposed novel Trojan attacks based on the
quantum computing properties in a QNN-based binary classifier. Our proposed
Quantum Properties Trojans (QuPTs) are based on the unitary property of quantum
gates to insert noise and Hadamard gates to enable superposition to develop
Trojans and attack QNNs. We showed that the proposed QuPTs are significantly
stealthier and heavily impact the quantum circuits' performance, specifically
QNNs. The most impactful QuPT caused a deterioration of 23% accuracy of the
compromised QNN under the experimental setup. To the best of our knowledge,
this is the first work on the Trojan attack on a fully quantum neural network
independent of any hybrid classical-quantum architecture.

</details>


### [145] [Parametrized Quantum Circuit Learning for Quantum Chemical Applications](https://arxiv.org/abs/2507.08183)
*Grier M. Jones,Viki Kumar Prasad,Ulrich Fekl,Hans-Arno Jacobsen*

Main category: quant-ph

TL;DR: 研究参数化量子电路（PQCs）在量子化学相关数据集上的应用，构建168个PQCs并评估性能，发现应用于化学问题有挑战。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习中PQCs在量子化学数据集上的探索有限，需研究其在化学数据集上的潜力和局限性。

Method: 结合14种数据编码策略和12种变分形式构建168个PQCs，在5和16个量子比特电路上评估，用状态向量模拟分析电路结构影响，探索电路深度和训练集大小的影响，在量子硬件上评估最佳PQCs性能。

Result: 发现将PQCs应用于化学相关问题存在挑战，经典机器学习方法易处理的问题，量子方法仍不易解决。

Conclusion: PQCs应用于化学相关问题面临挑战，还需进一步研究。

Abstract: In the field of quantum machine learning (QML), parametrized quantum circuits
(PQCs) -- constructed using a combination of fixed and tunable quantum gates --
provide a promising hybrid framework for tackling complex machine learning
problems. Despite numerous proposed applications, there remains limited
exploration of datasets relevant to quantum chemistry. In this study, we
investigate the potential benefits and limitations of PQCs on two chemically
meaningful datasets: (1) the BSE49 dataset, containing bond separation energies
for 49 different classes of chemical bonds, and (2) a dataset of water
conformations, where coupled-cluster singles and doubles (CCSD) wavefunctions
are predicted from lower-level electronic structure methods using the
data-driven coupled-cluster (DDCC) approach. We construct a comprehensive set
of 168 PQCs by combining 14 data encoding strategies with 12 variational
ans{\"a}tze, and evaluate their performance on circuits with 5 and 16 qubits.
Our initial analysis examines the impact of circuit structure on model
performance using state-vector simulations. We then explore how circuit depth
and training set size influence model performance. Finally, we assess the
performance of the best-performing PQCs on current quantum hardware, using both
noisy simulations ("fake" backends) and real quantum devices. Our findings
underscore the challenges of applying PQCs to chemically relevant problems that
are straightforward for classical machine learning methods but remain
non-trivial for quantum approaches.

</details>


### [146] [Quantum Algorithms for Projection-Free Sparse Convex Optimization](https://arxiv.org/abs/2507.08543)
*Jianhao He,John C. S. Lui*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper considers the projection-free sparse convex optimization problem
for the vector domain and the matrix domain, which covers a large number of
important applications in machine learning and data science. For the vector
domain $\mathcal{D} \subset \mathbb{R}^d$, we propose two quantum algorithms
for sparse constraints that finds a $\varepsilon$-optimal solution with the
query complexity of $O(\sqrt{d}/\varepsilon)$ and $O(1/\varepsilon)$ by using
the function value oracle, reducing a factor of $O(\sqrt{d})$ and $O(d)$ over
the best classical algorithm, respectively, where $d$ is the dimension. For the
matrix domain $\mathcal{D} \subset \mathbb{R}^{d\times d}$, we propose two
quantum algorithms for nuclear norm constraints that improve the time
complexity to $\tilde{O}(rd/\varepsilon^2)$ and
$\tilde{O}(\sqrt{r}d/\varepsilon^3)$ for computing the update step, reducing at
least a factor of $O(\sqrt{d})$ over the best classical algorithm, where $r$ is
the rank of the gradient matrix. Our algorithms show quantum advantages in
projection-free sparse convex optimization problems as they outperform the
optimal classical methods in dependence on the dimension $d$.

</details>


### [147] [Entangled Threats: A Unified Kill Chain Model for Quantum Machine Learning Security](https://arxiv.org/abs/2507.08623)
*Pascal Debus,Maximilian Wendlinger,Kilian Tscharke,Daniel Herr,Cedric Brügmann,Daniel Ohl de Mello,Juris Ulmanis,Alexander Erhard,Arthur Schmidt,Fabian Petsch*

Main category: quant-ph

TL;DR: 量子机器学习（QML）安全研究分散，本文提出用杀伤链模型建模攻击面，给出攻击向量分类，为QML安全设计奠基。


<details>
  <summary>Details</summary>
Motivation: 现有QML安全研究孤立，假设不现实，阻碍有效整体防御策略发展。

Method: 将经典IT和网络安全中广泛使用的杀伤链模型应用于QML，基于文献分析给出QML攻击向量分类。

Result: 提出量子感知杀伤链框架下QML攻击向量详细分类，突出不同层面威胁间相互依赖关系。

Conclusion: 为QML领域更现实的威胁建模和主动深度安全设计提供基础。

Abstract: Quantum Machine Learning (QML) systems inherit vulnerabilities from classical
machine learning while introducing new attack surfaces rooted in the physical
and algorithmic layers of quantum computing. Despite a growing body of research
on individual attack vectors - ranging from adversarial poisoning and evasion
to circuit-level backdoors, side-channel leakage, and model extraction - these
threats are often analyzed in isolation, with unrealistic assumptions about
attacker capabilities and system environments. This fragmentation hampers the
development of effective, holistic defense strategies. In this work, we argue
that QML security requires more structured modeling of the attack surface,
capturing not only individual techniques but also their relationships,
prerequisites, and potential impact across the QML pipeline. We propose
adapting kill chain models, widely used in classical IT and cybersecurity, to
the quantum machine learning context. Such models allow for structured
reasoning about attacker objectives, capabilities, and possible multi-stage
attack paths - spanning reconnaissance, initial access, manipulation,
persistence, and exfiltration. Based on extensive literature analysis, we
present a detailed taxonomy of QML attack vectors mapped to corresponding
stages in a quantum-aware kill chain framework that is inspired by the MITRE
ATLAS for classical machine learning. We highlight interdependencies between
physical-level threats (like side-channel leakage and crosstalk faults), data
and algorithm manipulation (such as poisoning or circuit backdoors), and
privacy attacks (including model extraction and training data inference). This
work provides a foundation for more realistic threat modeling and proactive
security-in-depth design in the emerging field of quantum machine learning.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [148] [Generative AI in Science: Applications, Challenges, and Emerging Questions](https://arxiv.org/abs/2507.08310)
*Ryan Harries,Cornelia Lawson,Philip Shapira*

Main category: cs.CY

TL;DR: 本文通过定性综述探讨生成式人工智能对科学实践的影响，发现其虽在科学领域快速应用，但长期影响不明。


<details>
  <summary>Details</summary>
Motivation: 研究生成式人工智能（GenAI）对科学实践的影响。

Method: 对OpenAlex数据库中相关文献进行布尔搜索，选取39篇高被引论文和评论进行定性编码。

Result: 按GenAI在科学、科学写作、医疗实践、教育和培训中的应用对结果分类，发现GenAI在科学和实践中快速应用，但长期影响不明。

Conclusion: 研究提供了GenAI在科学中日益重要作用的早期见解，并确定了该领域未来研究的问题。

Abstract: This paper examines the impact of Generative Artificial Intelligence (GenAI)
on scientific practices, conducting a qualitative review of selected literature
to explore its applications, benefits, and challenges. The review draws on the
OpenAlex publication database, using a Boolean search approach to identify
scientific literature related to GenAI (including large language models and
ChatGPT). Thirty-nine highly cited papers and commentaries are reviewed and
qualitatively coded. Results are categorized by GenAI applications in science,
scientific writing, medical practice, and education and training. The analysis
finds that while there is a rapid adoption of GenAI in science and science
practice, its long-term implications remain unclear, with ongoing uncertainties
about its use and governance. The study provides early insights into GenAI's
growing role in science and identifies questions for future research in this
evolving field.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [149] [Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi Classification](https://arxiv.org/abs/2507.08248)
*Jason Kahei Tam,Murilo Gustineli,Anthony Miyaguchi*

Main category: cs.CV

TL;DR: 本文介绍了参加FungiCLEF 2025竞赛的方法，用多种技术处理少样本细粒度真菌分类，最终模型表现优于基线，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决计算机视觉中由于种间细粒度差异和种内高度变异导致的真菌物种准确识别难题。

Method: 实验多种视觉Transformer模型、数据增强、加权采样、结合文本信息，探索结构化提示的生成式AI模型用于零样本分类。

Result: 最终模型优于竞赛基线，在赛后评估的私有测试集上排名35/74，生成式AI模型表现不如基于视觉的模型。

Conclusion: 特定领域预训练和平衡采样策略有效，后续可在元数据选择和领域自适应多模态学习上做更多工作。

Abstract: Accurate identification of fungi species presents a unique challenge in
computer vision due to fine-grained inter-species variation and high
intra-species variation. This paper presents our approach for the FungiCLEF
2025 competition, which focuses on few-shot fine-grained visual categorization
(FGVC) using the FungiTastic Few-Shot dataset. Our team (DS@GT) experimented
with multiple vision transformer models, data augmentation, weighted sampling,
and incorporating textual information. We also explored generative AI models
for zero-shot classification using structured prompting but found them to
significantly underperform relative to vision-based models. Our final model
outperformed both competition baselines and highlighted the effectiveness of
domain specific pretraining and balanced sampling strategies. Our approach
ranked 35/74 on the private test set in post-completion evaluation, this
suggests additional work can be done on metadata selection and domain-adapted
multi-modal learning. Our code is available at
https://github.com/dsgt-arc/fungiclef-2025.

</details>


### [150] [Towards Evaluating Robustness of Prompt Adherence in Text to Image Models](https://arxiv.org/abs/2507.08039)
*Sujith Vemishetty,Advitiya Arora,Anupama Sharma*

Main category: cs.CV

TL;DR: 本文为文本到图像模型建立评估框架，创建新数据集评估模型，研究了多种模型，结果显示模型在生成简单图像和遵循数据集分布方面存在问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型可靠性和有效性受关注，而多模态和文本到图像模型可靠性因研究不足受限，需建立评估框架。

Method: 创建新数据集，使用gpt - 4o生成文本描述，生成人工图像后再通过gpt - 4o对比描述差异。

Result: 模型难以创建含两个变化因素的简单二值图像，且无法生成遵循输入数据集分布的图像。

Conclusion: 所研究的文本到图像模型在遵循提示生成图像方面存在不足。

Abstract: The advancements in the domain of LLMs in recent years have surprised many,
showcasing their remarkable capabilities and diverse applications. Their
potential applications in various real-world scenarios have led to significant
research on their reliability and effectiveness. On the other hand, multimodal
LLMs and Text-to-Image models have only recently gained prominence, especially
when compared to text-only LLMs. Their reliability remains constrained due to
insufficient research on assessing their performance and robustness. This paper
aims to establish a comprehensive evaluation framework for Text-to-Image
models, concentrating particularly on their adherence to prompts. We created a
novel dataset that aimed to assess the robustness of these models in generating
images that conform to the specified factors of variation in the input text
prompts. Our evaluation studies present findings on three variants of Stable
Diffusion models: Stable Diffusion 3 Medium, Stable Diffusion 3.5 Large, and
Stable Diffusion 3.5 Large Turbo, and two variants of Janus models: Janus Pro
1B and Janus Pro 7B. We introduce a pipeline that leverages text descriptions
generated by the gpt-4o model for our ground-truth images, which are then used
to generate artificial images by passing these descriptions to the
Text-to-Image models. We then pass these generated images again through gpt-4o
using the same system prompt and compare the variation between the two
descriptions. Our results reveal that these models struggle to create simple
binary images with only two factors of variation: a simple geometric shape and
its location. We also show, using pre-trained VAEs on our dataset, that they
fail to generate images that follow our input dataset distribution.

</details>


### [151] [ConsNoTrainLoRA: Data-driven Weight Initialization of Low-rank Adapters using Constraints](https://arxiv.org/abs/2507.08044)
*Debasmit Das,Hyoungwoo Park,Munawar Hayat,Seokeon Choi,Sungrack Yun,Fatih Porikli*

Main category: cs.CV

TL;DR: 本文提出ConsNoTrainLoRA (CNTLoRA)方法改进LoRA微调的收敛性和最终性能，在下游图像任务上表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 以往多数工作中，LoRA权重矩阵随机初始化且各连接点秩固定，需改进以提升收敛和性能。

Method: 将LoRA初始化表示为域偏移问题，利用预训练和微调激活的约束，得到无需训练的闭式权重估计，可灵活设置可变秩来初始化上下矩阵。

Result: 定量和定性结果表明，CNTLoRA在图像生成、分类和理解等下游任务上优于标准和数据驱动的权重初始化方法。

Conclusion: CNTLoRA能实现更快收敛和更好性能，详细分析为框架设计提供了最优方案。

Abstract: Foundation models are pre-trained on large-scale datasets and subsequently
fine-tuned on small-scale datasets using parameter-efficient fine-tuning (PEFT)
techniques like low-rank adapters (LoRA). In most previous works, LoRA weight
matrices are randomly initialized with a fixed rank across all attachment
points. In this paper, we improve convergence and final performance of LoRA
fine-tuning, using our proposed data-driven weight initialization method,
ConsNoTrainLoRA (CNTLoRA). We express LoRA initialization as a domain shift
problem where we use multiple constraints relating the pre-training and
fine-tuning activations. By reformulating these constraints, we obtain a
closed-form estimate of LoRA weights that depends on pre-training weights and
fine-tuning activation vectors and hence requires no training during
initialization. This weight estimate is decomposed to initialize the up and
down matrices with proposed flexibility of variable ranks. With the proposed
initialization method, we fine-tune on downstream tasks such as image
generation, image classification and image understanding. Both quantitative and
qualitative results demonstrate that CNTLoRA outperforms standard and
data-driven weight initialization methods. Extensive analyses and ablations
further elucidate the design choices of our framework, providing an optimal
recipe for faster convergence and enhanced performance.

</details>


### [152] [An Object-Based Deep Learning Approach for Building Height Estimation from Single SAR Images](https://arxiv.org/abs/2507.08096)
*Babak Memar,Luigi Russo,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: 提出基于深度学习的单张VHR COSMO - SkyMed图像建筑高度自动估计方法，在多城市数据集验证，在欧洲表现佳，显示深度学习在跨城市和跨大陆建筑高度估计中有潜力。


<details>
  <summary>Details</summary>
Motivation: 准确估计建筑高度对城市应用至关重要，需有效方法。

Method: 基于深度学习的目标回归方法，先进行边界框检测，再进行高度估计，使用多城市数据集，采用交叉验证评估分布外泛化能力。

Result: 在欧洲城市表现出色，如慕尼黑MAE约为一层楼（2.20米），显著优于同类方法；在其他大陆城市泛化时有差异。

Conclusion: 深度学习在基于单张VHR SAR数据的建筑高度估计的跨城市和跨大陆迁移学习中有巨大潜力。

Abstract: Accurate estimation of building heights using very high resolution (VHR)
synthetic aperture radar (SAR) imagery is crucial for various urban
applications. This paper introduces a Deep Learning (DL)-based methodology for
automated building height estimation from single VHR COSMO-SkyMed images: an
object-based regression approach based on bounding box detection followed by
height estimation. This model was trained and evaluated on a unique
multi-continental dataset comprising eight geographically diverse cities across
Europe, North and South America, and Asia, employing a cross-validation
strategy to explicitly assess out-of-distribution (OOD) generalization. The
results demonstrate highly promising performance, particularly on European
cities where the model achieves a Mean Absolute Error (MAE) of approximately
one building story (2.20 m in Munich), significantly outperforming recent
state-of-the-art methods in similar OOD scenarios. Despite the increased
variability observed when generalizing to cities in other continents,
particularly in Asia with its distinct urban typologies and prevalence of
high-rise structures, this study underscores the significant potential of DL
for robust cross-city and cross-continental transfer learning in building
height estimation from single VHR SAR data.

</details>


### [153] [Temporally Consistent Amodal Completion for 3D Human-Object Interaction Reconstruction](https://arxiv.org/abs/2507.08137)
*Hyungjun Doh,Dong In Lee,Seunggeun Chi,Pin-Hao Huang,Kwonjoon Lee,Sangpil Kim,Karthik Ramani*

Main category: cs.CV

TL;DR: 提出一种从单目视频重建动态人-物交互的新框架，利用非模态完成和时间上下文，比现有技术在处理遮挡和保持时间稳定性上更优。


<details>
  <summary>Details</summary>
Motivation: 传统3D重建方法在处理遮挡和时间不一致问题时性能下降，需要新方法解决。

Method: 利用非模态完成推断部分遮挡区域的完整结构，整合时间上下文，采用无模板策略。

Result: 在具有挑战性的单目视频上使用3D高斯散点验证，在处理遮挡和保持时间稳定性上精度更高。

Conclusion: 新框架能显著增强动态场景中复杂细节的恢复，优于现有技术。

Abstract: We introduce a novel framework for reconstructing dynamic human-object
interactions from monocular video that overcomes challenges associated with
occlusions and temporal inconsistencies. Traditional 3D reconstruction methods
typically assume static objects or full visibility of dynamic subjects, leading
to degraded performance when these assumptions are violated-particularly in
scenarios where mutual occlusions occur. To address this, our framework
leverages amodal completion to infer the complete structure of partially
obscured regions. Unlike conventional approaches that operate on individual
frames, our method integrates temporal context, enforcing coherence across
video sequences to incrementally refine and stabilize reconstructions. This
template-free strategy adapts to varying conditions without relying on
predefined models, significantly enhancing the recovery of intricate details in
dynamic scenes. We validate our approach using 3D Gaussian Splatting on
challenging monocular videos, demonstrating superior precision in handling
occlusions and maintaining temporal stability compared to existing techniques.

</details>


### [154] [Interpretability-Aware Pruning for Efficient Medical Image Analysis](https://arxiv.org/abs/2507.08330)
*Nikita Malik,Pratinav Seth,Neeraj Kumar Singh,Chintan Chitroda,Vinay Kumar Sankarapu*

Main category: cs.CV

TL;DR: 本文介绍了一种可解释性引导的剪枝框架，能在保持预测性能和透明度的同时降低模型复杂度，实验证明该方法能实现高压缩率且准确率损失极小。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学图像分析中虽有进展，但现代模型因规模大且缺乏透明度，在临床实践中的应用受限，需提升模型的可解释性和降低复杂度。

Method: 引入可解释性引导的剪枝框架，选择性保留每层最相关部分以实现有针对性的压缩。

Result: 在多个医学图像分类基准测试中，该方法实现了高压缩率，且准确率损失极小。

Conclusion: 该方法为适用于医疗环境实际部署的轻量级、可解释模型铺平了道路。

Abstract: Deep learning has driven significant advances in medical image analysis, yet
its adoption in clinical practice remains constrained by the large size and
lack of transparency in modern models. Advances in interpretability techniques
such as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated
Gradients make it possible to assess the contribution of individual components
within neural networks trained on medical imaging tasks. In this work, we
introduce an interpretability-guided pruning framework that reduces model
complexity while preserving both predictive performance and transparency. By
selectively retaining only the most relevant parts of each layer, our method
enables targeted compression that maintains clinically meaningful
representations. Experiments across multiple medical image classification
benchmarks demonstrate that this approach achieves high compression rates with
minimal loss in accuracy, paving the way for lightweight, interpretable models
suited for real-world deployment in healthcare settings.

</details>


### [155] [CoCo-Bot: Energy-based Composable Concept Bottlenecks for Interpretable Generative Models](https://arxiv.org/abs/2507.08334)
*Sangwon Kim,In-su Jang,Pyongkun Kim,Kwang-Ju Kim*

Main category: cs.CV

TL;DR: 提出CoCo - Bot生成模型，通过显式概念传递信息，提升概念可控性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 以往生成式概念瓶颈模型依赖辅助视觉线索，破坏可解释性和组合性。

Method: 提出CoCo - Bot后验、可组合概念瓶颈生成模型，借助基于扩散的能量函数。

Result: 在CelebA - HQ预训练的StyleGAN2上实验，CoCo - Bot提升概念可控性和可解释性，保持视觉质量。

Conclusion: CoCo - Bot无需辅助线索，能提升概念可控性和可解释性。

Abstract: Concept Bottleneck Models (CBMs) provide interpretable and controllable
generative modeling by routing generation through explicit,
human-understandable concepts. However, previous generative CBMs often rely on
auxiliary visual cues at the bottleneck to compensate for information not
captured by the concepts, which undermines interpretability and
compositionality. We propose CoCo-Bot, a post-hoc, composable concept
bottleneck generative model that eliminates the need for auxiliary cues by
transmitting all information solely through explicit concepts. Guided by
diffusion-based energy functions, CoCo-Bot supports robust post-hoc
interventions-such as concept composition and negation-across arbitrary
concepts. Experiments using StyleGAN2 pre-trained on CelebA-HQ show that
CoCo-Bot improves concept-level controllability and interpretability, while
maintaining competitive visual quality.

</details>


### [156] [Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement](https://arxiv.org/abs/2507.08340)
*Jia-Xuan Jiang,Jiashuai Liu,Hongtao Wu,Yifeng Wu,Zhong Wang,Qi Bi,Yefeng Zheng*

Main category: cs.CV

TL;DR: 现有多模态生存预测方法在跨癌症泛化方面存在不足，本文提出跨癌症单域泛化任务并引入两个模块，实验证明有更好泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法主要针对单一癌症类型，忽略跨癌症泛化挑战，而临床实践需要这种鲁棒性。

Method: 提出跨癌症单域泛化任务，引入Sparse Dirac Information Rebalancer (SDIR) 和 Cancer-aware Distribution Entanglement (CADE) 两个模块。

Result: 在四癌症类型基准测试中展示出优越的泛化能力。

Conclusion: 为实用、鲁棒的跨癌症多模态预后奠定基础。

Abstract: Deep learning has shown remarkable performance in integrating multimodal data
for survival prediction. However, existing multimodal methods mainly focus on
single cancer types and overlook the challenge of generalization across
cancers. In this work, we are the first to reveal that multimodal prognosis
models often generalize worse than unimodal ones in cross-cancer scenarios,
despite the critical need for such robustness in clinical practice. To address
this, we propose a new task: Cross-Cancer Single Domain Generalization for
Multimodal Prognosis, which evaluates whether models trained on a single cancer
type can generalize to unseen cancers. We identify two key challenges: degraded
features from weaker modalities and ineffective multimodal integration. To
tackle these, we introduce two plug-and-play modules: Sparse Dirac Information
Rebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR
mitigates the dominance of strong features by applying Bernoulli-based
sparsification and Dirac-inspired stabilization to enhance weaker modality
signals. CADE, designed to synthesize the target domain distribution, fuses
local morphological cues and global gene expression in latent space.
Experiments on a four-cancer-type benchmark demonstrate superior
generalization, laying the foundation for practical, robust cross-cancer
multimodal prognosis. Code is available at
https://github.com/HopkinsKwong/MCCSDG

</details>


### [157] [PanMatch: Unleashing the Potential of Large Vision Models for Unified Matching Models](https://arxiv.org/abs/2507.08400)
*Yongjian Zhang,Longguang Wang,Kunhong Li,Ye Zhang,Yun Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

TL;DR: 提出泛用基础模型PanMatch用于对应匹配，可在同一框架下处理多任务，表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖特定架构和微调，缺乏通用性，旨在提出能统一处理多任务的模型。

Method: 采用2D位移估计框架，强调通用特征提取器，提出特征转换管道，用跨域数据集预训练。

Result: PanMatch在跨任务评估中优于UniMatch和Flow - Anything，在面向任务基准测试中与多数先进特定任务算法性能相当，在异常场景有零样本表现。

Conclusion: PanMatch具有跨多领域和下游任务的通用性，在多方面表现良好。

Abstract: This work presents PanMatch, a versatile foundation model for robust
correspondence matching. Unlike previous methods that rely on task-specific
architectures and domain-specific fine-tuning to support tasks like stereo
matching, optical flow or feature matching, our key insight is that any
two-frame correspondence matching task can be addressed within a 2D
displacement estimation framework using the same model weights. Such a
formulation eliminates the need for designing specialized unified architectures
or task-specific ensemble models. Instead, it achieves multi-task integration
by endowing displacement estimation algorithms with unprecedented
generalization capabilities. To this end, we highlight the importance of a
robust feature extractor applicable across multiple domains and tasks, and
propose the feature transformation pipeline that leverage all-purpose features
from Large Vision Models to endow matching baselines with zero-shot cross-view
matching capabilities. Furthermore, we assemble a cross-domain dataset with
near 1.8 million samples from stereo matching, optical flow, and feature
matching domains to pretrain PanMatch. We demonstrate the versatility of
PanMatch across a wide range of domains and downstream tasks using the same
model weights. Our model outperforms UniMatch and Flow-Anything on cross-task
evaluations, and achieves comparable performance to most state-of-the-art
task-specific algorithms on task-oriented benchmarks. Additionally, PanMatch
presents unprecedented zero-shot performance in abnormal scenarios, such as
rainy day and satellite imagery, where most existing robust algorithms fail to
yield meaningful results.

</details>


### [158] [Deep Hashing with Semantic Hash Centers for Image Retrieval](https://arxiv.org/abs/2507.08404)
*Li Chen,Rui Liu,Yuxiang Zhou,Xudong Ma,Yong Chen,Dell Zhang*

Main category: cs.CV

TL;DR: 本文提出SHC框架用于图像检索，通过三阶段方法生成保留语义结构的哈希码，实验表明其显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有点级深度哈希方法用数据无关算法生成哈希中心，忽略类间语义关系，可能降低检索性能。

Method: 提出三阶段框架SHC，先开发分类网络识别类间语义相似性，再引入优化算法生成语义哈希中心，最后用这些中心训练深度哈希网络将图像转为二进制哈希码。

Result: 在多个公共数据集的大规模检索任务中，SHC在MAP@100、MAP@1000和MAP@ALL指标上比现有方法平均提升7.26%、7.62%和11.71%。

Conclusion: SHC能有效提升图像检索性能。

Abstract: Deep hashing is an effective approach for large-scale image retrieval.
Current methods are typically classified by their supervision types:
point-wise, pair-wise, and list-wise. Recent point-wise techniques (e.g., CSQ,
MDS) have improved retrieval performance by pre-assigning a hash center to each
class, enhancing the discriminability of hash codes across various datasets.
However, these methods rely on data-independent algorithms to generate hash
centers, which neglect the semantic relationships between classes and may
degrade retrieval performance.
  This paper introduces the concept of semantic hash centers, building on the
idea of traditional hash centers. We hypothesize that hash centers of
semantically related classes should have closer Hamming distances, while those
of unrelated classes should be more distant. To this end, we propose a
three-stage framework, SHC, to generate hash codes that preserve semantic
structure.
  First, we develop a classification network to identify semantic similarities
between classes using a data-dependent similarity calculation that adapts to
varying data distributions. Second, we introduce an optimization algorithm to
generate semantic hash centers, preserving semantic relatedness while enforcing
a minimum distance between centers to avoid excessively similar hash codes.
Finally, a deep hashing network is trained using these semantic centers to
convert images into binary hash codes.
  Experimental results on large-scale retrieval tasks across several public
datasets show that SHC significantly improves retrieval performance.
Specifically, SHC achieves average improvements of +7.26%, +7.62%, and +11.71%
in MAP@100, MAP@1000, and MAP@ALL metrics, respectively, over state-of-the-art
methods.

</details>


### [159] [A Hybrid Multilayer Extreme Learning Machine for Image Classification with an Application to Quadcopters](https://arxiv.org/abs/2507.08047)
*Rolando A. Hernandez-Hernandez,Adrian Rubio-Solis*

Main category: cs.CV

TL;DR: 本文提出混合多层极限学习机(HML - ELM)用于无人机主动图像分类，经实验验证其效率优于其他类似方法。


<details>
  <summary>Details</summary>
Motivation: 为主动图像分类找到更有效的方法，并应用于无人机。

Method: 提出分层ELM学习框架，包含自学习特征提取和有监督特征分类两阶段，用堆叠ELM - AE进行无监督多层特征编码，用SIT2 - FELM和基于SC算法的快速输出约简层进行特征分类。

Result: 通过解决图像分类基准问题和无人机主动分类运输物体的实验，表明HML - ELM效率优于ML - ELM、ML - FELM和ELM等方法。

Conclusion: 所提出的HML - ELM是一种有效的图像分类方法，在效率上表现更优。

Abstract: Multilayer Extreme Learning Machine (ML-ELM) and its variants have proven to
be an effective technique for the classification of different natural signals
such as audio, video, acoustic and images. In this paper, a Hybrid Multilayer
Extreme Learning Machine (HML-ELM) that is based on ELM-based autoencoder
(ELM-AE) and an Interval Type-2 fuzzy Logic theory is suggested for active
image classification and applied to Unmanned Aerial Vehicles (UAVs). The
proposed methodology is a hierarchical ELM learning framework that consists of
two main phases: 1) self-taught feature extraction and 2) supervised feature
classification. First, unsupervised multilayer feature encoding is achieved by
stacking a number of ELM-AEs, in which input data is projected into a number of
high-level representations. At the second phase, the final features are
classified using a novel Simplified Interval Type-2 Fuzzy ELM (SIT2-FELM) with
a fast output reduction layer based on the SC algorithm; an improved version of
the algorithm Center of Sets Type Reducer without Sorting Requirement
(COSTRWSR). To validate the efficiency of the HML-ELM, two types of experiments
for the classification of images are suggested. First, the HML-ELM is applied
to solve a number of benchmark problems for image classification. Secondly, a
number of real experiments to the active classification and transport of four
different objects between two predefined locations using a UAV is implemented.
Experiments demonstrate that the proposed HML-ELM delivers a superior
efficiency compared to other similar methodologies such as ML-ELM, Multilayer
Fuzzy Extreme Learning Machine (ML-FELM) and ELM.

</details>


### [160] [Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging](https://arxiv.org/abs/2507.08052)
*Mazen Ali,António Pereira,Fabio Gentile,Aser Cortines,Sam Mugel,Román Orús,Stelios P. Neophytides,Michalis Mavrovouniotis*

Main category: cs.CV

TL;DR: 本文评估多种机器学习方法用于高光谱卫星图像云与云阴影掩膜，CNN特征降维模型表现最优，显示轻量级AI模型用于实时处理的潜力。


<details>
  <summary>Details</summary>
Motivation: 云与云阴影掩膜是高光谱卫星成像关键预处理步骤，需评估合适的机器学习方法。

Method: 评估多种机器学习方法，包括梯度提升方法（XGBoost、LightGBM）和卷积神经网络（CNN）。

Result: 所有提升和CNN模型准确率超93%，CNN特征降维模型最有效，部分版本在部署可行性、准确性和计算效率上平衡最佳。

Conclusion: 轻量级AI模型有潜力用于实时高光谱图像处理，支持星载AI系统开发。

Abstract: Cloud and cloud shadow masking is a crucial preprocessing step in
hyperspectral satellite imaging, enabling the extraction of high-quality,
analysis-ready data. This study evaluates various machine learning approaches,
including gradient boosting methods such as XGBoost and LightGBM as well as
convolutional neural networks (CNNs). All boosting and CNN models achieved
accuracies exceeding 93%. Among the investigated models, the CNN with feature
reduction emerged as the most efficient, offering a balance of high accuracy,
low storage requirements, and rapid inference times on both CPUs and GPUs.
Variations of this version, with only up to 597 trainable parameters,
demonstrated the best trade-off in terms of deployment feasibility, accuracy,
and computational efficiency. These results demonstrate the potential of
lightweight artificial intelligence (AI) models for real-time hyperspectral
image processing, supporting the development of on-board satellite AI systems
for space-based applications.

</details>


### [161] [Vision Foundation Models as Effective Visual Tokenizers for Autoregressive Image Generation](https://arxiv.org/abs/2507.08441)
*Anlin Zheng,Xin Wen,Xuanyang Zhang,Chuofan Ma,Tiancai Wang,Gang Yu,Xiangyu Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 利用预训练视觉基础模型构建图像分词器VFMTok，介绍关键组件，其在图像重建、生成等方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索在预训练视觉基础模型上构建图像分词器这一未充分研究的领域。

Method: 使用冻结的视觉基础模型作为分词器编码器，引入区域自适应量化框架和语义重建目标。

Result: VFMTok在图像重建和生成质量上有显著提升，提高了令牌效率，在ImageNet基准测试中gFID为2.07，加速模型收敛三倍，无需CFG实现高保真类别条件合成。

Conclusion: 提出的图像分词器设计有效，代码将公开以造福社区。

Abstract: Leveraging the powerful representations of pre-trained vision foundation
models -- traditionally used for visual comprehension -- we explore a novel
direction: building an image tokenizer directly atop such models, a largely
underexplored area. Specifically, we employ a frozen vision foundation model as
the encoder of our tokenizer. To enhance its effectiveness, we introduce two
key components: (1) a region-adaptive quantization framework that reduces
redundancy in the pre-trained features on regular 2D grids, and (2) a semantic
reconstruction objective that aligns the tokenizer's outputs with the
foundation model's representations to preserve semantic fidelity. Based on
these designs, our proposed image tokenizer, VFMTok, achieves substantial
improvements in image reconstruction and generation quality, while also
enhancing token efficiency. It further boosts autoregressive (AR) generation --
achieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model
convergence by three times, and enabling high-fidelity class-conditional
synthesis without the need for classifier-free guidance (CFG). The code will be
released publicly to benefit the community.

</details>


### [162] [Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT](https://arxiv.org/abs/2507.08448)
*Wei Zhang,Yihang Wu,Songhua Li,Wenjie Ma,Xin Ma,Qiang Li,Qi Wang*

Main category: cs.CV

TL;DR: 本文对基于深度学习的前馈式3D重建模型进行系统综述，对比传统方法，介绍技术框架、数据集和评估指标，探讨应用前景与挑战。


<details>
  <summary>Details</summary>
Motivation: 传统3D重建方法存在工作流程复杂、计算成本高和鲁棒性差等问题，深度学习推动3D重建范式转变，需要对新兴领域进行系统综述。

Method: 剖析前馈式模型的技术框架，与传统管道和早期基于学习的方法对比，介绍相关数据集和评估指标。

Result: 完成对新兴的前馈式3D重建模型领域的系统综述。

Conclusion: 该技术有广泛应用前景，但存在模型准确性、可扩展性和处理动态场景等挑战与机遇。

Abstract: 3D reconstruction, which aims to recover the dense three-dimensional
structure of a scene, is a cornerstone technology for numerous applications,
including augmented/virtual reality, autonomous driving, and robotics. While
traditional pipelines like Structure from Motion (SfM) and Multi-View Stereo
(MVS) achieve high precision through iterative optimization, they are limited
by complex workflows, high computational cost, and poor robustness in
challenging scenarios like texture-less regions. Recently, deep learning has
catalyzed a paradigm shift in 3D reconstruction. A new family of models,
exemplified by DUSt3R, has pioneered a feed-forward approach. These models
employ a unified deep network to jointly infer camera poses and dense geometry
directly from an Unconstrained set of images in a single forward pass. This
survey provides a systematic review of this emerging domain. We begin by
dissecting the technical framework of these feed-forward models, including
their Transformer-based correspondence modeling, joint pose and geometry
regression mechanisms, and strategies for scaling from two-view to multi-view
scenarios. To highlight the disruptive nature of this new paradigm, we contrast
it with both traditional pipelines and earlier learning-based methods like
MVSNet. Furthermore, we provide an overview of relevant datasets and evaluation
metrics. Finally, we discuss the technology's broad application prospects and
identify key future challenges and opportunities, such as model accuracy and
scalability, and handling dynamic scenes.

</details>


### [163] [Adaptive Diffusion Denoised Smoothing : Certified Robustness via Randomized Smoothing with Differentially Private Guided Denoising Diffusion](https://arxiv.org/abs/2507.08163)
*Frederick Shpilevskiy,Saiyue Lyu,Krishnamurthy Dj Dvijotham,Mathias Lécuyer,Pierre-André Noël*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose Adaptive Diffusion Denoised Smoothing, a method for certifying the
predictions of a vision model against adversarial examples, while adapting to
the input. Our key insight is to reinterpret a guided denoising diffusion model
as a long sequence of adaptive Gaussian Differentially Private (GDP) mechanisms
refining a pure noise sample into an image. We show that these adaptive
mechanisms can be composed through a GDP privacy filter to analyze the
end-to-end robustness of the guided denoising process, yielding a provable
certification that extends the adaptive randomized smoothing analysis. We
demonstrate that our design, under a specific guiding strategy, can improve
both certified accuracy and standard accuracy on ImageNet for an $\ell_2$
threat model.

</details>


### [164] [A document is worth a structured record: Principled inductive bias design for document recognition](https://arxiv.org/abs/2507.08458)
*Benjamin Meyer,Lukas Tuggener,Sascha Hänzi,Daniel Schmid,Erdal Ayfer,Benjamin F. Grewe,Ahmed Abdulkadir,Thilo Stadelmann*

Main category: cs.CV

TL;DR: 提出将文档识别视为从文档到记录的转录任务，设计特定结构归纳偏置，在多种文档类型实验中验证有效性，为设计文档识别系统提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有文档识别方法忽视文档特定结构属性，依赖启发式后处理，使部分文档类型难以识别。

Method: 将文档识别视为转录任务，设计结构特定归纳偏置，提出基础变压器架构并适配不同结构。

Result: 在多种文档类型实验中证明归纳偏置有效，训练出首个将工程图纸转录为关联信息的端到端模型。

Conclusion: 该方法对设计理解不足的文档类型识别系统有意义，可指导未来文档基础模型设计。

Abstract: Many document types use intrinsic, convention-driven structures that serve to
encode precise and structured information, such as the conventions governing
engineering drawings. However, state-of-the-art approaches treat document
recognition as a mere computer vision problem, neglecting these underlying
document-type-specific structural properties, making them dependent on
sub-optimal heuristic post-processing and rendering many less frequent or more
complicated document types inaccessible to modern document recognition. We
suggest a novel perspective that frames document recognition as a transcription
task from a document to a record. This implies a natural grouping of documents
based on the intrinsic structure inherent in their transcription, where related
document types can be treated (and learned) similarly. We propose a method to
design structure-specific inductive biases for the underlying machine-learned
end-to-end document recognition systems, and a respective base transformer
architecture that we successfully adapt to different structures. We demonstrate
the effectiveness of the so-found inductive biases in extensive experiments
with progressively complex record structures from monophonic sheet music, shape
drawings, and simplified engineering drawings. By integrating an inductive bias
for unrestricted graph structures, we train the first-ever successful
end-to-end model to transcribe engineering drawings to their inherently
interlinked information. Our approach is relevant to inform the design of
document recognition systems for document types that are less well understood
than standard OCR, OMR, etc., and serves as a guide to unify the design of
future document foundation models.

</details>


### [165] [RadiomicsRetrieval: A Customizable Framework for Medical Image Retrieval Using Radiomics Features](https://arxiv.org/abs/2507.08546)
*Inye Na,Nejung Rue,Jiwon Chung,Hyunjin Park*

Main category: cs.CV

TL;DR: 提出RadiomicsRetrieval 3D医学图像检索框架，结合手工特征与深度学习嵌入，实验表明其可灵活检索，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像检索方法主要支持2D图像且需全标注查询，限制临床灵活性。

Method: 提出RadiomicsRetrieval框架，利用可提示分割模型获取肿瘤特定图像嵌入，通过对比学习与放射组学特征对齐，用解剖位置嵌入（APE）丰富表示。

Result: 在肺CT和脑MRI公共数据集实验表明，放射组学特征增强检索特异性，APE提供位置搜索所需的全局解剖上下文，框架只需最少用户提示。

Conclusion: 该框架具有适应性，能支持多样临床场景，可能有益于诊断、治疗规划和医学影像研究。

Abstract: Medical image retrieval is a valuable field for supporting clinical
decision-making, yet current methods primarily support 2D images and require
fully annotated queries, limiting clinical flexibility. To address this, we
propose RadiomicsRetrieval, a 3D content-based retrieval framework bridging
handcrafted radiomics descriptors with deep learning-based embeddings at the
tumor level. Unlike existing 2D approaches, RadiomicsRetrieval fully exploits
volumetric data to leverage richer spatial context in medical images. We employ
a promptable segmentation model (e.g., SAM) to derive tumor-specific image
embeddings, which are aligned with radiomics features extracted from the same
tumor via contrastive learning. These representations are further enriched by
anatomical positional embedding (APE). As a result, RadiomicsRetrieval enables
flexible querying based on shape, location, or partial feature sets. Extensive
experiments on both lung CT and brain MRI public datasets demonstrate that
radiomics features significantly enhance retrieval specificity, while APE
provides global anatomical context essential for location-based searches.
Notably, our framework requires only minimal user prompts (e.g., a single
point), minimizing segmentation overhead and supporting diverse clinical
scenarios. The capability to query using either image embeddings or selected
radiomics attributes highlights its adaptability, potentially benefiting
diagnosis, treatment planning, and research on large-scale medical imaging
repositories. Our code is available at
https://github.com/nainye/RadiomicsRetrieval.

</details>


### [166] [A Multi-Modal Fusion Framework for Brain Tumor Segmentation Based on 3D Spatial-Language-Vision Integration and Bidirectional Interactive Attention Mechanism](https://arxiv.org/abs/2507.08574)
*Mingda Zhang,Kaiwen Pan*

Main category: cs.CV

TL;DR: 本文提出一种新型多模态融合框架用于脑肿瘤分割，融合多信息提升性能，在数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发新型多模态融合框架，通过双向交互注意力机制整合空间 - 语言 - 视觉信息，提高脑肿瘤分割精度和边界描绘。

Method: 提出多模态语义融合适配器（MSFA）和双向交互视觉语义注意力（BIVA）两个核心组件，在BraTS 2020数据集上评估。

Result: 在增强肿瘤、肿瘤核心和全肿瘤区域平均Dice系数达0.8505，95%豪斯多夫距离为2.8256mm，优于SCAU - Net等方法，消融研究证实语义和空间模块作用。

Conclusion: 多模态语义融合与双向交互注意力显著提升脑肿瘤分割性能，为临床知识融入医学图像分析建立新范式。

Abstract: This study aims to develop a novel multi-modal fusion framework for brain
tumor segmentation that integrates spatial-language-vision information through
bidirectional interactive attention mechanisms to improve segmentation accuracy
and boundary delineation. Methods: We propose two core components: Multi-modal
Semantic Fusion Adapter (MSFA) integrating 3D MRI data with clinical text
descriptions through hierarchical semantic decoupling, and Bidirectional
Interactive Visual-semantic Attention (BIVA) enabling iterative information
exchange between modalities. The framework was evaluated on BraTS 2020 dataset
comprising 369 multi-institutional MRI scans. Results: The proposed method
achieved average Dice coefficient of 0.8505 and 95% Hausdorff distance of
2.8256mm across enhancing tumor, tumor core, and whole tumor regions,
outperforming state-of-the-art methods including SCAU-Net, CA-Net, and 3D
U-Net. Ablation studies confirmed critical contributions of semantic and
spatial modules to boundary precision. Conclusion: Multi-modal semantic fusion
combined with bidirectional interactive attention significantly enhances brain
tumor segmentation performance, establishing new paradigms for integrating
clinical knowledge into medical image analysis.

</details>


### [167] [Normalized vs Diplomatic Annotation: A Case Study of Automatic Information Extraction from Handwritten Uruguayan Birth Certificates](https://arxiv.org/abs/2507.08636)
*Natalia Bottaioli,Solène Tarride,Jérémy Anger,Seginus Mowlavi,Marina Gardella,Antoine Tadros,Gabriele Facciolo,Rafael Grompone von Gioi,Christopher Kermorvant,Jean-Michel Morel,Javier Preciozzi*

Main category: cs.CV

TL;DR: 研究评估Document Attention Network (DAN)从乌拉圭手写西班牙语出生证明中提取键值信息，比较两种标注策略。


<details>
  <summary>Details</summary>
Motivation: 评估DAN在从乌拉圭手写西班牙语出生证明中提取关键信息的效果，探索有效且低成本的标注策略。

Method: 在两个含相同图像但不同标注方法的数据集上进行实验，用最少训练数据和标注工作微调DAN。

Result: 标准化标注对日期、出生地等可标准化字段更有效，外交标注对姓名等不可标准化字段效果更好。

Conclusion: 不同类型字段适用不同的标注策略来提升DAN提取关键信息的效果。

Abstract: This study evaluates the recently proposed Document Attention Network (DAN)
for extracting key-value information from Uruguayan birth certificates,
handwritten in Spanish. We investigate two annotation strategies for
automatically transcribing handwritten documents, fine-tuning DAN with minimal
training data and annotation effort. Experiments were conducted on two datasets
containing the same images (201 scans of birth certificates written by more
than 15 different writers) but with different annotation methods. Our findings
indicate that normalized annotation is more effective for fields that can be
standardized, such as dates and places of birth, whereas diplomatic annotation
performs much better for fields containing names and surnames, which can not be
standardized.

</details>


### [168] [DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets from Real-World Images](https://arxiv.org/abs/2507.08648)
*Haoran Sun,Haoyu Bian,Shaoning Zeng,Yunbo Rao,Xu Xu,Lin Mei,Jianping Gou*

Main category: cs.CV

TL;DR: 本文提出DatasetAgent方法，用多智能体协作系统从真实世界图像自动构建数据集，并进行两类实验验证其对训练视觉模型的作用。


<details>
  <summary>Details</summary>
Motivation: 传统图像数据集构建依赖人工，效率低；大模型生成数据不如真实数据有价值，因此需要从真实世界图像自动构建数据集的方法。

Method: 提出DatasetAgent，协调四个配备多模态大语言模型的智能体和图像优化工具包，按用户需求构建数据集。

Result: 进行了扩展现有数据集和从头创建新数据集两类实验，用DatasetAgent构建的数据集训练多种视觉模型。

Conclusion: 未明确提及，但可推测DatasetAgent是一种有效的从真实世界图像自动构建数据集的方法。

Abstract: Common knowledge indicates that the process of constructing image datasets
usually depends on the time-intensive and inefficient method of manual
collection and annotation. Large models offer a solution via data generation.
Nonetheless, real-world data are obviously more valuable comparing to
artificially intelligence generated data, particularly in constructing image
datasets. For this reason, we propose a novel method for auto-constructing
datasets from real-world images by a multiagent collaborative system, named as
DatasetAgent. By coordinating four different agents equipped with Multi-modal
Large Language Models (MLLMs), as well as a tool package for image
optimization, DatasetAgent is able to construct high-quality image datasets
according to user-specified requirements. In particular, two types of
experiments are conducted, including expanding existing datasets and creating
new ones from scratch, on a variety of open-source datasets. In both cases,
multiple image datasets constructed by DatasetAgent are used to train various
vision models for image classification, object detection, and image
segmentation.

</details>


### [169] [MoSAiC: Multi-Modal Multi-Label Supervision-Aware Contrastive Learning for Remote Sensing](https://arxiv.org/abs/2507.08683)
*Debashis Gupta,Aditi Golder,Rongkhun Zhu,Kangning Cui,Wei Tang,Fan Yang,Ovidiu Csillik,Sarra Alaqahtani,V. Paul Pauca*

Main category: cs.CV

TL;DR: 本文提出用于多模态卫星图像的统一框架MoSAiC，联合优化模态内和模态间对比学习，在低标签和高类重叠场景下优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 对比学习适合地球系统观测，但该领域存在高类间相似性等挑战，现有框架存在不足，需新方法。

Method: 引入MoSAiC框架，用多标签监督对比损失联合优化模态内和模态间对比学习。

Result: 在BigEarthNet V2.0和Sent12MS两个基准数据集上，MoSAiC在准确性、聚类一致性和泛化性上优于有监督和自监督基准模型。

Conclusion: MoSAiC能在光谱相似和空间复杂类中实现更精细语义解缠和更鲁棒的表征学习。

Abstract: Contrastive learning (CL) has emerged as a powerful paradigm for learning
transferable representations without the reliance on large labeled datasets.
Its ability to capture intrinsic similarities and differences among data
samples has led to state-of-the-art results in computer vision tasks. These
strengths make CL particularly well-suited for Earth System Observation (ESO),
where diverse satellite modalities such as optical and SAR imagery offer
naturally aligned views of the same geospatial regions. However, ESO presents
unique challenges, including high inter-class similarity, scene clutter, and
ambiguous boundaries, which complicate representation learning -- especially in
low-label, multi-label settings. Existing CL frameworks often focus on
intra-modality self-supervision or lack mechanisms for multi-label alignment
and semantic precision across modalities. In this work, we introduce MoSAiC, a
unified framework that jointly optimizes intra- and inter-modality contrastive
learning with a multi-label supervised contrastive loss. Designed specifically
for multi-modal satellite imagery, MoSAiC enables finer semantic
disentanglement and more robust representation learning across spectrally
similar and spatially complex classes. Experiments on two benchmark datasets,
BigEarthNet V2.0 and Sent12MS, show that MoSAiC consistently outperforms both
fully supervised and self-supervised baselines in terms of accuracy, cluster
coherence, and generalization in low-label and high-class-overlap scenarios.

</details>


### [170] [SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2](https://arxiv.org/abs/2507.08548)
*Alen Adamyan,Tomáš Čížek,Matej Straka,Klara Janouskova,Martin Schmid*

Main category: cs.CV

TL;DR: 提出用强化学习优化SAM 2的内存更新，在过拟合设置下效果远超现有启发式方法，凸显强化学习在视觉目标跟踪内存控制中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有方法用手工更新规则增强SAM 2以处理干扰、遮挡和物体运动，希望找到不同方法优化内存更新。

Method: 将内存控制视为顺序决策问题，用强化学习优化SAM 2的内存更新。

Result: 在过拟合设置下，每个视频配备单独代理，方法相对SAM 2的改进超现有启发式方法三倍多。

Conclusion: 内存库有未挖掘潜力，强化学习是视觉目标跟踪内存控制中手工更新规则的有力替代。

Abstract: Segment Anything Model 2 (SAM 2) has demonstrated strong performance in
object segmentation tasks and has become the state-of-the-art for visual object
tracking. The model stores information from previous frames in a memory bank,
enabling temporal consistency across video sequences. Recent methods augment
SAM 2 with hand-crafted update rules to better handle distractors, occlusions,
and object motion. We propose a fundamentally different approach using
reinforcement learning for optimizing memory updates in SAM 2 by framing memory
control as a sequential decision-making problem. In an overfitting setup with a
separate agent per video, our method achieves a relative improvement over SAM 2
that exceeds by more than three times the gains of existing heuristics. These
results reveal the untapped potential of the memory bank and highlight
reinforcement learning as a powerful alternative to hand-crafted update rules
for memory control in visual object tracking.

</details>


### [171] [A Hybrid Multi-Well Hopfield-CNN with Feature Extraction and K-Means for MNIST Classification](https://arxiv.org/abs/2507.08766)
*Ahmed Farooq*

Main category: cs.CV

TL;DR: 提出结合CNN与多阱Hopfield网络的混合模型对手写数字分类，在MNIST数据集上测试准确率达99.2%。


<details>
  <summary>Details</summary>
Motivation: 解决手写数字分类中处理类内变异性问题，并提供可解释框架。

Method: 用CNN提取特征，k - means聚类成类原型，Hopfield网络通过最小化能量函数分类。

Result: 在10000张MNIST图像上测试准确率达99.2%。

Conclusion: 强调深度特征提取和足够原型覆盖对高性能的关键作用，有模式识别应用潜力。

Abstract: This study presents a hybrid model for classifying handwritten digits in the
MNIST dataset, combining convolutional neural networks (CNNs) with a multi-well
Hopfield network. The approach employs a CNN to extract high-dimensional
features from input images, which are then clustered into class-specific
prototypes using k-means clustering. These prototypes serve as attractors in a
multi-well energy landscape, where a Hopfield network performs classification
by minimizing an energy function that balances feature similarity and class
assignment.The model's design enables robust handling of intraclass
variability, such as diverse handwriting styles, while providing an
interpretable framework through its energy-based decision process. Through
systematic optimization of the CNN architecture and the number of wells, the
model achieves a high test accuracy of 99.2% on 10,000 MNIST images,
demonstrating its effectiveness for image classification tasks. The findings
highlight the critical role of deep feature extraction and sufficient prototype
coverage in achieving high performance, with potential for broader applications
in pattern recognition.

</details>


### [172] [Geo-ORBIT: A Federated Digital Twin Framework for Scene-Adaptive Lane Geometry Detection](https://arxiv.org/abs/2507.08743)
*Rei Tamaru,Pei Li,Bin Ran*

Main category: cs.CV

TL;DR: 本文提出Geo - ORBIT框架解决交通系统数字孪生面临的问题，实验表明FedMeta - GeoLane表现优异，为灵活的基础设施建模奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有交通系统数字孪生方法依赖静态地图或昂贵传感器，且大规模数字孪生在隐私、通信和计算效率方面存在挑战。

Method: 引入Geo - ORBIT框架，包括GeoLane、Meta - GeoLane和FedMeta - GeoLane，与CARLA和SUMO集成创建高保真数字孪生。

Result: FedMeta - GeoLane在不同城市场景实验中，优于基线和元学习方法，降低几何误差、增强泛化能力并减少通信开销。

Conclusion: 该工作为数字孪生中灵活的上下文感知基础设施建模奠定基础，框架已公开。

Abstract: Digital Twins (DT) have the potential to transform traffic management and
operations by creating dynamic, virtual representations of transportation
systems that sense conditions, analyze operations, and support decision-making.
A key component for DT of the transportation system is dynamic roadway geometry
sensing. However, existing approaches often rely on static maps or costly
sensors, limiting scalability and adaptability. Additionally, large-scale DTs
that collect and analyze data from multiple sources face challenges in privacy,
communication, and computational efficiency. To address these challenges, we
introduce Geo-ORBIT (Geometrical Operational Roadway Blueprint with Integrated
Twin), a unified framework that combines real-time lane detection, DT
synchronization, and federated meta-learning. At the core of Geo-ORBIT is
GeoLane, a lightweight lane detection model that learns lane geometries from
vehicle trajectory data using roadside cameras. We extend this model through
Meta-GeoLane, which learns to personalize detection parameters for local
entities, and FedMeta-GeoLane, a federated learning strategy that ensures
scalable and privacy-preserving adaptation across roadside deployments. Our
system is integrated with CARLA and SUMO to create a high-fidelity DT that
renders highway scenarios and captures traffic flows in real-time. Extensive
experiments across diverse urban scenes show that FedMeta-GeoLane consistently
outperforms baseline and meta-learning approaches, achieving lower geometric
error and stronger generalization to unseen locations while drastically
reducing communication overhead. This work lays the foundation for flexible,
context-aware infrastructure modeling in DTs. The framework is publicly
available at https://github.com/raynbowy23/FedMeta-GeoLane.git.

</details>


### [173] [NeuralOS: Towards Simulating Operating Systems via Neural Generative Models](https://arxiv.org/abs/2507.08800)
*Luke Rivard,Sun Sun,Hongyu Guo,Wenhu Chen,Yuntian Deng*

Main category: cs.CV

TL;DR: 介绍NeuralOS框架，结合RNN与扩散式神经渲染器模拟操作系统GUI，在Ubuntu XFCE数据集训练，能渲染逼真GUI序列等，但精确建模键盘交互有挑战。


<details>
  <summary>Details</summary>
Motivation: 创建用于未来人机交互系统的完全自适应、生成式神经界面。

Method: 结合跟踪计算机状态的RNN和生成屏幕图像的扩散式神经渲染器，在Ubuntu XFCE录制的大规模数据集上训练。

Result: NeuralOS成功渲染逼真GUI序列，准确捕捉鼠标交互，可靠预测状态转换如应用启动。

Conclusion: 尽管精确建模细粒度键盘交互有挑战，但NeuralOS向创建未来人机交互系统的目标迈进了一步。

Abstract: We introduce NeuralOS, a neural framework that simulates graphical user
interfaces (GUIs) of operating systems by directly predicting screen frames in
response to user inputs such as mouse movements, clicks, and keyboard events.
NeuralOS combines a recurrent neural network (RNN), which tracks computer
state, with a diffusion-based neural renderer that generates screen images. The
model is trained on a large-scale dataset of Ubuntu XFCE recordings, which
include both randomly generated interactions and realistic interactions
produced by AI agents. Experiments show that NeuralOS successfully renders
realistic GUI sequences, accurately captures mouse interactions, and reliably
predicts state transitions like application launches. Although modeling
fine-grained keyboard interactions precisely remains challenging, NeuralOS
offers a step toward creating fully adaptive, generative neural interfaces for
future human-computer interaction systems.

</details>


### [174] [Compress Any Segment Anything Model (SAM)](https://arxiv.org/abs/2507.08765)
*Juntong Fan,Zhiwei Hao,Jianqiang Shen,Shang-Ling Jui,Yi Zhang,Jing-Xiao Liao,Feng-Lei Fan*

Main category: cs.CV

TL;DR: 提出针对SAM及其变体的无数据压缩算法Birkhoff，实验显示其在压缩时间、比率、性能和推理速度上表现出色。


<details>
  <summary>Details</summary>
Motivation: SAM及其变体应用广泛，有效压缩它们成为迫切实际需求。

Method: 提出Hyper - Compression算法将高维参数向量转为低维标量，设计HyperLinear线性层算子融合解压缩和矩阵乘法以加速推理。

Result: 在多个数据集的18个SAM模型上实验表明，Birkhoff在多方面表现一致且有竞争力，如在SAM2 - B上达5.17x压缩比，性能下降不到1%，所有模型压缩在60秒内完成。

Conclusion: Birkhoff是一种有效、通用的SAM及其变体压缩算法。

Abstract: Due to the excellent performance in yielding high-quality, zero-shot
segmentation, Segment Anything Model (SAM) and its variants have been widely
applied in diverse scenarios such as healthcare and intelligent manufacturing.
Therefore, effectively compressing SAMs has become an increasingly pressing
practical need. In this study, we propose Birkhoff, a novel data-free
compression algorithm for SAM and its variants. Unlike quantization, pruning,
distillation, and other compression methods, Birkhoff embodies versatility
across model types, agility in deployment, faithfulness to the original model,
and compactness in model size. Specifically, Birkhoff introduces a novel
compression algorithm: Hyper-Compression, whose core principle is to find a
dense trajectory to turn a high-dimensional parameter vector into a
low-dimensional scalar. Furthermore, Birkhoff designs a dedicated linear layer
operator, HyperLinear, to fuse decompression and matrix multiplication to
significantly accelerate inference of the compressed SAMs. Extensive
experiments on 18 SAMs in the COCO, LVIS, and SA-1B datasets show that Birkhoff
performs consistently and competitively in compression time, compression ratio,
post-compression performance, and inference speed. For example, Birkhoff can
achieve a compression ratio of 5.17x on SAM2-B, with less than 1% performance
drop without using any fine-tuning data. Moreover, the compression is finished
within 60 seconds for all models.

</details>


### [175] [Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective](https://arxiv.org/abs/2507.08801)
*Hangjie Yuan,Weihua Chen,Jun Cen,Hu Yu,Jingyun Liang,Shuning Chang,Zhihui Lin,Tao Feng,Pengwei Liu,Jiazheng Xing,Hao Luo,Jiasheng Tang,Fan Wang,Yi Yang*

Main category: cs.CV

TL;DR: 提出自回归视频生成器Lumos - 1，采用MM - RoPE和AR - DF方法，仅用48个GPU预训练就取得不错性能。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视频生成器存在偏离标准LLM架构、依赖外部文本编码器、解码延迟高等问题，需要改进。

Method: 提出MM - RoPE注入时空相关性；采用令牌依赖策略并提出AR - DF解决帧级损失不平衡问题；使用内存高效训练技术。

Result: Lumos - 1在GenEval、VBench - I2V、VBench - T2V上取得与其他模型相当的性能。

Conclusion: Lumos - 1以最小架构修改保留LLM架构，结合新方法能有效进行自回归视频生成，且训练资源需求低。

Abstract: Autoregressive large language models (LLMs) have unified a vast range of
language tasks, inspiring preliminary efforts in autoregressive video
generation. Existing autoregressive video generators either diverge from
standard LLM architectures, depend on bulky external text encoders, or incur
prohibitive latency due to next-token decoding. In this paper, we introduce
Lumos-1, an autoregressive video generator that retains the LLM architecture
with minimal architectural modifications. To inject spatiotemporal correlations
in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its
imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE
scheme that preserves the original textual RoPE while providing comprehensive
frequency spectra and scaled 3D positions for modeling multimodal
spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy
that obeys intra-frame bidirectionality and inter-frame temporal causality.
Based on this dependency strategy, we identify the issue of frame-wise loss
imbalance caused by spatial information redundancy and solve it by proposing
Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal
tube masking during training with a compatible inference-time masking policy to
avoid quality degradation. By using memory-efficient training techniques, we
pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on
GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code
and models are available at https://github.com/alibaba-damo-academy/Lumos.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [176] [Predicting Flow Dynamics using Diffusion Models](https://arxiv.org/abs/2507.08106)
*Yannick Gachnang,Vismay Churiwala*

Main category: physics.flu-dyn

TL;DR: 本文旨在复制和拓展DiffFluid论文结果，验证其方法可重复性并测试对其他模拟类型可行性，展示了模型潜力与挑战，指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 复制和拓展DiffFluid论文结果，验证方法可重复性，测试其对其他模拟类型（如Lattice Boltzmann方法）的可行性。

Method: 使用DiffFluid模型中的去噪扩散概率模型（DDPM）框架处理Navier - Stokes和Darcy流动方程。

Result: 展示了该模型作为通用流体动力学求解器的灵活性和潜力，也显示了将扩散模型应用于复杂流体动力学问题的潜力与挑战。

Conclusion: 强调了未来在优化计算效率和将此类模型扩展到更广泛领域方面的研究机会。

Abstract: In this work, we aimed to replicate and extend the results presented in the
DiffFluid paper[1]. The DiffFluid model showed that diffusion models combined
with Transformers are capable of predicting fluid dynamics. It uses a denoising
diffusion probabilistic model (DDPM) framework to tackle Navier-Stokes and
Darcy flow equations. Our goal was to validate the reproducibility of the
methods in the DiffFluid paper while testing its viability for other simulation
types, particularly the Lattice Boltzmann method. Despite our computational
limitations and time constraints, this work provides evidence of the
flexibility and potential of the model as a general-purpose solver for fluid
dynamics. Our results show both the potential and challenges of applying
diffusion models to complex fluid dynamics problems. This work highlights the
opportunities for future research in optimizing the computational efficiency
and scaling such models in broader domains.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [177] [Towards AI-Native RAN: An Operator's Perspective of 6G Day 1 Standardization](https://arxiv.org/abs/2507.08403)
*Nan Li,Qi Sun,Lehan Wang,Xiaofei Xu,Jinri Huang,Chunhui Liu,Jing Gao,Yuhong Huang,Chih-Lin I*

Main category: cs.NI

TL;DR: 本文探讨6G AI原生无线接入网（RAN）的设计与标准化原则，提出架构并经大规模试验验证有显著效果，提供了6G AI原生RAN标准化设计框架。


<details>
  <summary>Details</summary>
Motivation: 6G需从一开始就融入AI以应对复杂性并支持泛在AI应用，基于2G - 5G经验探索6G AI原生RAN的设计与标准化原则。

Method: 研究AI原生RAN框架，提出其三项关键能力；提出AI原生RAN标准化内容，特别是首日特性；搭建超5000个5G - A基站的大规模现场试验进行验证。

Result: 所提架构和支持的AI功能在平均空口延迟、根因识别和网络能耗方面带来显著改善。

Conclusion: 为6G AI原生RAN标准化设计提供首日框架，平衡了技术创新与实际部署。

Abstract: Artificial Intelligence/Machine Learning (AI/ML) has become the most certain
and prominent feature of 6G mobile networks. Unlike 5G, where AI/ML was not
natively integrated but rather an add-on feature over existing architecture, 6G
shall incorporate AI from the onset to address its complexity and support
ubiquitous AI applications. Based on our extensive mobile network operation and
standardization experience from 2G to 5G, this paper explores the design and
standardization principles of AI-Native radio access networks (RAN) for 6G,
with a particular focus on its critical Day 1 architecture, functionalities and
capabilities. We investigate the framework of AI-Native RAN and present its
three essential capabilities to shed some light on the standardization
direction; namely, AI-driven RAN processing/optimization/automation, reliable
AI lifecycle management (LCM), and AI-as-a-Service (AIaaS) provisioning. The
standardization of AI-Native RAN, in particular the Day 1 features, including
an AI-Native 6G RAN architecture, were proposed. For validation, a large-scale
field trial with over 5000 5G-A base stations have been built and delivered
significant improvements in average air interface latency, root cause
identification, and network energy consumption with the proposed architecture
and the supporting AI functions. This paper aims to provide a Day 1 framework
for 6G AI-Native RAN standardization design, balancing technical innovation
with practical deployment.

</details>


### [178] [KP-A: A Unified Network Knowledge Plane for Catalyzing Agentic Network Intelligence](https://arxiv.org/abs/2507.08164)
*Yun Tang,Mengbang Zou,Zeinab Nezami,Syed Ali Raza Zaidi,Weisi Guo*

Main category: cs.NI

TL;DR: 本文提出针对智能网络的统一网络知识平面KP - A，解耦网络知识获取和管理与智能逻辑，还在两个任务中展示其应用并开源实现。


<details>
  <summary>Details</summary>
Motivation: 当前单个智能任务实现需孤立知识检索管道，导致数据冗余和解释不一致，受Open - RAN服务模型统一启发。

Method: 提出KP - A统一网络知识平面，解耦网络知识获取和管理与智能逻辑，提供直观一致的知识接口。

Result: 在实时网络知识问答和边缘AI服务编排两个代表性智能任务中展示了KP - A。

Conclusion: KP - A能简化智能工程师开发和维护复杂度，增强网络智能代理的互操作性，开源实现支持可重复性和未来标准化。

Abstract: The emergence of large language models (LLMs) and agentic systems is enabling
autonomous 6G networks with advanced intelligence, including
self-configuration, self-optimization, and self-healing. However, the current
implementation of individual intelligence tasks necessitates isolated knowledge
retrieval pipelines, resulting in redundant data flows and inconsistent
interpretations. Inspired by the service model unification effort in Open-RAN
(to support interoperability and vendor diversity), we propose KP-A: a unified
Network Knowledge Plane specifically designed for Agentic network intelligence.
By decoupling network knowledge acquisition and management from intelligence
logic, KP-A streamlines development and reduces maintenance complexity for
intelligence engineers. By offering an intuitive and consistent knowledge
interface, KP-A also enhances interoperability for the network intelligence
agents. We demonstrate KP-A in two representative intelligence tasks: live
network knowledge Q&A and edge AI service orchestration. All implementation
artifacts have been open-sourced to support reproducibility and future
standardization efforts.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [179] [Unraveling the Potential of Diffusion Models in Small Molecule Generation](https://arxiv.org/abs/2507.08005)
*Peining Zhang,Daniel Baker,Minghu Song,Jinbo Bi*

Main category: q-bio.BM

TL;DR: 本文全面回顾扩散模型在分子生成中的最新进展与应用，介绍原理、分类方法、评估性能，指出挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 生成式AI为药物设计带来新思路，扩散模型在药物研发受关注，需对其在分子生成中的应用进行全面梳理。

Method: 先介绍扩散模型理论原理，再按数学和化学应用对基于扩散模型的分子生成方法分类，在基准数据集上评估模型性能，尤其关注3D方法生成性能对比。

Result: 对扩散模型在分子生成中的最新进展和应用情况进行了系统呈现。

Conclusion: 强调当前挑战，建议未来研究方向以充分挖掘扩散模型在药物发现中的潜力。

Abstract: Generative AI presents chemists with novel ideas for drug design and
facilitates the exploration of vast chemical spaces. Diffusion models (DMs), an
emerging tool, have recently attracted great attention in drug R\&D. This paper
comprehensively reviews the latest advancements and applications of DMs in
molecular generation. It begins by introducing the theoretical principles of
DMs. Subsequently, it categorizes various DM-based molecular generation methods
according to their mathematical and chemical applications. The review further
examines the performance of these models on benchmark datasets, with a
particular focus on comparing the generation performance of existing 3D
methods. Finally, it concludes by emphasizing current challenges and suggesting
future research directions to fully exploit the potential of DMs in drug
discovery.

</details>


### [180] [AmpLyze: A Deep Learning Model for Predicting the Hemolytic Concentration](https://arxiv.org/abs/2507.08162)
*Peng Qiu,Hanqi Feng,Barnabas Poczos*

Main category: q-bio.BM

TL;DR: AmpLyze模型可仅根据序列预测抗菌肽（AMP）治疗剂的实际红细胞裂解（HC50）值并解释驱动毒性的残基，优于现有方法，有助于AMP设计和早期毒性筛查。


<details>
  <summary>Details</summary>
Motivation: 现有模型对抗菌肽治疗剂的红细胞裂解（HC50）值仅能判断“有毒”或“无毒”，无法预测实际值，AmpLyze旨在填补这一空白。

Method: 将残基级的ProtT5/ESM2嵌入与序列级描述符在局部和全局分支中结合，通过交叉注意力模块对齐，并使用对数双曲余弦损失进行训练。

Result: 最优AmpLyze模型PCC达到0.756，MSE为0.987，优于经典回归器和现有技术，消融实验证明两个分支都很重要，交叉注意力模块进一步提升性能，归因分析揭示毒性热点并建议更安全的替代方案。

Conclusion: AmpLyze将溶血评估转化为基于序列的定量、可解释预测，有助于AMP设计和早期毒性筛查。

Abstract: Red-blood-cell lysis (HC50) is the principal safety barrier for
antimicrobial-peptide (AMP) therapeutics, yet existing models only say "toxic"
or "non-toxic." AmpLyze closes this gap by predicting the actual HC50 value
from sequence alone and explaining the residues that drive toxicity. The model
couples residue-level ProtT5/ESM2 embeddings with sequence-level descriptors in
dual local and global branches, aligned by a cross-attention module and trained
with log-cosh loss for robustness to assay noise. The optimal AmpLyze model
reaches a PCC of 0.756 and an MSE of 0.987, outperforming classical regressors
and the state-of-the-art. Ablations confirm that both branches are essential,
and cross-attention adds a further 1% PCC and 3% MSE improvement.
Expected-Gradients attributions reveal known toxicity hotspots and suggest
safer substitutions. By turning hemolysis assessment into a quantitative,
sequence-based, and interpretable prediction, AmpLyze facilitates AMP design
and offers a practical tool for early-stage toxicity screening.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [181] [CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic Manipulation Representations](https://arxiv.org/abs/2507.08262)
*Wenbo Cui,Chengyang Zhao,Yuhui Chen,Haoran Li,Zhizheng Zhang,Dongbin Zhao,He Wang*

Main category: cs.RO

TL;DR: 提出CL3R框架提升机器人操作策略，结合点云自编码器和对比学习，统一数据集坐标系并融合多视点云，实验证明其在视觉运动策略学习中有效。


<details>
  <summary>Details</summary>
Motivation: 现有将预训练2D基础模型用于机器人感知模块的方法难以捕捉3D空间信息和跨不同相机视点泛化，影响策略在精细操作场景的有效性。

Method: 提出CL3R框架，用点云Masked Autoencoder学习3D表示，通过对比学习利用预训练2D基础模型进行语义知识转移；提出3D视觉表示预训练框架，统一数据集坐标系，引入多视点云随机融合。

Result: 在模拟和现实世界的大量实验证明该方法的优越性。

Conclusion: 该方法在机器人操作的视觉运动策略学习中有效。

Abstract: Building a robust perception module is crucial for visuomotor policy
learning. While recent methods incorporate pre-trained 2D foundation models
into robotic perception modules to leverage their strong semantic
understanding, they struggle to capture 3D spatial information and generalize
across diverse camera viewpoints. These limitations hinder the policy's
effectiveness, especially in fine-grained robotic manipulation scenarios. To
address these challenges, we propose CL3R, a novel 3D pre-training framework
designed to enhance robotic manipulation policies. Our method integrates both
spatial awareness and semantic understanding by employing a point cloud Masked
Autoencoder to learn rich 3D representations while leveraging pre-trained 2D
foundation models through contrastive learning for efficient semantic knowledge
transfer. Additionally, we propose a 3D visual representation pre-training
framework for robotic tasks. By unifying coordinate systems across datasets and
introducing random fusion of multi-view point clouds, we mitigate camera view
ambiguity and improve generalization, enabling robust perception from novel
viewpoints at test time. Extensive experiments in both simulation and the real
world demonstrate the superiority of our method, highlighting its effectiveness
in visuomotor policy learning for robotic manipulation.

</details>


### [182] [Intelligent Control of Spacecraft Reaction Wheel Attitude Using Deep Reinforcement Learning](https://arxiv.org/abs/2507.08366)
*Ghaith El-Dalahmeh,Mohammad Reza Jabbarpour,Bao Quoc Vo,Ryszard Kowalczyk*

Main category: cs.RO

TL;DR: 研究提出基于DRL的TD3 - HD控制策略用于卫星姿态控制，对比实验显示其在故障条件下表现更佳，有潜力成为容错星载AI方案。


<details>
  <summary>Details</summary>
Motivation: 可靠的卫星姿态控制对太空任务至关重要，传统PD控制器和现有DRL算法在自主卫星运行的实时适应性和容错性方面不足，需要改进。

Method: 将Twin Delayed Deep Deterministic Policy Gradient (TD3)与Hindsight Experience Replay (HER)和Dimension Wise Clipping (DWC)集成，提出TD3 - HD控制策略。

Result: TD3 - HD在故障条件下实现了更低的姿态误差、更好的角速度调节和更强的稳定性。

Conclusion: TD3 - HD有潜力成为强大的、容错的自主卫星姿态控制星载AI解决方案。

Abstract: Reliable satellite attitude control is essential for the success of space
missions, particularly as satellites increasingly operate autonomously in
dynamic and uncertain environments. Reaction wheels (RWs) play a pivotal role
in attitude control, and maintaining control resilience during RW faults is
critical to preserving mission objectives and system stability. However,
traditional Proportional Derivative (PD) controllers and existing deep
reinforcement learning (DRL) algorithms such as TD3, PPO, and A2C often fall
short in providing the real time adaptability and fault tolerance required for
autonomous satellite operations. This study introduces a DRL-based control
strategy designed to improve satellite resilience and adaptability under fault
conditions. Specifically, the proposed method integrates Twin Delayed Deep
Deterministic Policy Gradient (TD3) with Hindsight Experience Replay (HER) and
Dimension Wise Clipping (DWC) referred to as TD3-HD to enhance learning in
sparse reward environments and maintain satellite stability during RW failures.
The proposed approach is benchmarked against PD control and leading DRL
algorithms. Experimental results show that TD3-HD achieves significantly lower
attitude error, improved angular velocity regulation, and enhanced stability
under fault conditions. These findings underscore the proposed method potential
as a powerful, fault tolerant, onboard AI solution for autonomous satellite
attitude control.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [183] [Massively parallel and universal approximation of nonlinear functions using diffractive processors](https://arxiv.org/abs/2507.08253)
*Md Sadman Sakib Rahman,Yuhang Li,Xilin Yang,Shiqi Chen,Aydogan Ozcan*

Main category: physics.optics

TL;DR: 本文展示了如何利用线性光学通过优化的衍射处理器进行大规模非线性计算，为模拟光学计算提供新途径。


<details>
  <summary>Details</summary>
Motivation: 光学非线性的弱点和高功率需求使光学系统实现非线性函数成为挑战，克服此限制可推动超快和并行光学计算系统发展。

Method: 将非线性函数输入变量编码到光波前相位，通过具有空间变化点扩散函数的优化衍射结构进行变换。

Result: 理论证明该架构可作为任意带限非线性函数的通用函数逼近器，数值模拟实现百万个不同非线性函数并行计算，实验验证可单次逼近35个独特非线性函数。

Conclusion: 衍射光学处理器是用于大规模并行通用非线性函数逼近的可扩展平台，为基于线性材料的模拟光学计算带来新能力。

Abstract: Nonlinear computation is essential for a wide range of information processing
tasks, yet implementing nonlinear functions using optical systems remains a
challenge due to the weak and power-intensive nature of optical nonlinearities.
Overcoming this limitation without relying on nonlinear optical materials could
unlock unprecedented opportunities for ultrafast and parallel optical computing
systems. Here, we demonstrate that large-scale nonlinear computation can be
performed using linear optics through optimized diffractive processors composed
of passive phase-only surfaces. In this framework, the input variables of
nonlinear functions are encoded into the phase of an optical wavefront, e.g.,
via a spatial light modulator (SLM), and transformed by an optimized
diffractive structure with spatially varying point-spread functions to yield
output intensities that approximate a large set of unique nonlinear functions,
all in parallel. We provide proof establishing that this architecture serves as
a universal function approximator for an arbitrary set of bandlimited nonlinear
functions, also covering multi-variate and complex-valued functions. We also
numerically demonstrate the parallel computation of one million distinct
nonlinear functions, accurately executed at wavelength-scale spatial density at
the output of a diffractive optical processor. Furthermore, we experimentally
validated this framework using in situ optical learning and approximated 35
unique nonlinear functions in a single shot using a compact setup consisting of
an SLM and an image sensor. These results establish diffractive optical
processors as a scalable platform for massively parallel universal nonlinear
function approximation, paving the way for new capabilities in analog optical
computing based on linear materials.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [184] [Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models](https://arxiv.org/abs/2507.08128)
*Arushi Goel,Sreyan Ghosh,Jaehyeon Kim,Sonal Kumar,Zhifeng Kong,Sang-gil Lee,Chao-Han Huck Yang,Ramani Duraiswami,Dinesh Manocha,Rafael Valle,Bryan Catanzaro*

Main category: cs.SD

TL;DR: 介绍全开源大音频语言模型AF3，有多种新特性，用新数据集和训练策略训练，在多基准测试达SOTA。


<details>
  <summary>Details</summary>
Motivation: 推进跨语音、声音和音乐的推理和理解，开发先进大音频语言模型。

Method: 引入AF - Whisper编码器，具备灵活按需思考等能力；提出多个用新策略整理的大规模训练数据集；采用五阶段基于课程的训练策略。

Result: AF3在超20多个（长）音频理解和推理基准测试中取得新SOTA结果，超越多种模型。

Conclusion: 仅用开源音频数据训练的AF3在音频理解和推理方面表现优秀。

Abstract: We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large
audio-language model that advances reasoning and understanding across speech,
sound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder
trained using a novel strategy for joint representation learning across all 3
modalities of speech, sound, and music; (ii) flexible, on-demand thinking,
allowing the model to do chain-of-thought-type reasoning before answering;
(iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning
(including speech) up to 10 minutes; and (v) voice-to-voice interaction. To
enable these capabilities, we propose several large-scale training datasets
curated using novel strategies, including AudioSkills-XL, LongAudio-XL,
AF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based
training strategy. Trained on only open-source audio data, AF3 achieves new
SOTA results on over 20+ (long) audio understanding and reasoning benchmarks,
surpassing both open-weight and closed-source models trained on much larger
datasets.

</details>


### [185] [Audio Inpanting using Discrete Diffusion Model](https://arxiv.org/abs/2507.08333)
*Tali Dror,Iftach Shoham,Moshe Buchris,Oren Gal,Haim Permuter,Gilad Katz,Eliya Nachmani*

Main category: cs.SD

TL;DR: 提出基于离散扩散建模的音频修复方法，在MusicNet和MTG数据集上实验，对长间隙修复效果好。


<details>
  <summary>Details</summary>
Motivation: 现有音频修复方法对超过100毫秒间隙的修复质量下降，需更好的方法。

Method: 基于离散扩散建模，对预训练音频分词器生成的音频表征进行操作，在离散潜在空间建模生成过程。

Result: 在MusicNet数据集上评估间隙达300毫秒，在MTG数据集上评估间隙达500毫秒，相比现有基线有竞争力或更优。

Conclusion: 该方法为修复受损音乐录音提供了鲁棒解决方案，尤其适用于长间隙。

Abstract: Audio inpainting refers to the task of reconstructing missing segments in
corrupted audio recordings. While prior approaches-including waveform and
spectrogram-based diffusion models-have shown promising results for short gaps,
they often degrade in quality when gaps exceed 100 milliseconds (ms). In this
work, we introduce a novel inpainting method based on discrete diffusion
modeling, which operates over tokenized audio representations produced by a
pre-trained audio tokenizer. Our approach models the generative process
directly in the discrete latent space, enabling stable and semantically
coherent reconstruction of missing audio. We evaluate the method on the
MusicNet dataset using both objective and perceptual metrics across gap
durations up to 300 ms. We further evaluated our approach on the MTG dataset,
extending the gap duration to 500 ms. Experimental results demonstrate that our
method achieves competitive or superior performance compared to existing
baselines, particularly for longer gaps, offering a robust solution for
restoring degraded musical recordings. Audio examples of our proposed method
can be found at https://iftach21.github.io/

</details>


### [186] [MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through Neural Codec Language Modelling](https://arxiv.org/abs/2507.08530)
*Jingjing Tang,Xin Wang,Zhe Zhang,Junichi Yamagishi,Geraint Wiggins,George Fazekas*

Main category: cs.SD

TL;DR: 提出MIDI - VALLE模型用于音乐表演MIDI到音频合成，表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统音乐表演合成管道中的合成模型难以在不同MIDI源、音乐风格和录音环境中泛化。

Method: 从VALLE框架改编出MIDI - VALLE模型，改进架构使其依赖参考音频表演及其对应MIDI，将MIDI和音频编码为离散标记，并在大量多样的钢琴表演数据集上训练。

Result: 在ATEPP和Maestro数据集上Frechet音频距离降低超75%，听力测试中获得202票，远高于基线的58票。

Conclusion: MIDI - VALLE提高了合成质量和在不同表演MIDI输入上的泛化能力。

Abstract: Generating expressive audio performances from music scores requires models to
capture both instrument acoustics and human interpretation. Traditional music
performance synthesis pipelines follow a two-stage approach, first generating
expressive performance MIDI from a score, then synthesising the MIDI into
audio. However, the synthesis models often struggle to generalise across
diverse MIDI sources, musical styles, and recording environments. To address
these challenges, we propose MIDI-VALLE, a neural codec language model adapted
from the VALLE framework, which was originally designed for zero-shot
personalised text-to-speech (TTS) synthesis. For performance MIDI-to-audio
synthesis, we improve the architecture to condition on a reference audio
performance and its corresponding MIDI. Unlike previous TTS-based systems that
rely on piano rolls, MIDI-VALLE encodes both MIDI and audio as discrete tokens,
facilitating a more consistent and robust modelling of piano performances.
Furthermore, the model's generalisation ability is enhanced by training on an
extensive and diverse piano performance dataset. Evaluation results show that
MIDI-VALLE significantly outperforms a state-of-the-art baseline, achieving
over 75% lower Frechet Audio Distance on the ATEPP and Maestro datasets. In the
listening test, MIDI-VALLE received 202 votes compared to 58 for the baseline,
demonstrating improved synthesis quality and generalisation across diverse
performance MIDI inputs.

</details>


### [187] [FreeAudio: Training-Free Timing Planning for Controllable Long-Form Text-to-Audio Generation](https://arxiv.org/abs/2507.08557)
*Yuxuan Jiang,Zehua Chen,Zeqian Ju,Chang Li,Weibei Dou,Jun Zhu*

Main category: cs.SD

TL;DR: 提出无训练的时序控制文本到音频框架FreeAudio，实现长格式T2A生成，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有T2A方法因音频 - 文本对质量和数量有限，难以处理含精确时序控制的复杂文本提示，且现有方法合成质量有限。

Method: 先使用大语言模型规划非重叠时间窗口并重新描述，再引入解耦聚合注意力控制、上下文潜在合成和参考引导。

Result: FreeAudio在无训练方法中实现了最先进的时序条件T2A合成质量，与有训练方法相当，长格式生成质量与Stable Audio相当。

Conclusion: FreeAudio为时序控制的长格式T2A合成铺平了道路。

Abstract: Text-to-audio (T2A) generation has achieved promising results with the recent
advances in generative models. However, because of the limited quality and
quantity of temporally-aligned audio-text pairs, existing T2A methods struggle
to handle the complex text prompts that contain precise timing control, e.g.,
"owl hooted at 2.4s-5.2s". Recent works have explored data augmentation
techniques or introduced timing conditions as model inputs to enable
timing-conditioned 10-second T2A generation, while their synthesis quality is
still limited. In this work, we propose a novel training-free timing-controlled
T2A framework, FreeAudio, making the first attempt to enable timing-controlled
long-form T2A generation, e.g., "owl hooted at 2.4s-5.2s and crickets chirping
at 0s-24s". Specifically, we first employ an LLM to plan non-overlapping time
windows and recaption each with a refined natural language description, based
on the input text and timing prompts. Then we introduce: 1) Decoupling and
Aggregating Attention Control for precise timing control; 2) Contextual Latent
Composition for local smoothness and Reference Guidance for global consistency.
Extensive experiments show that: 1) FreeAudio achieves state-of-the-art
timing-conditioned T2A synthesis quality among training-free methods and is
comparable to leading training-based methods; 2) FreeAudio demonstrates
comparable long-form generation quality with training-based Stable Audio and
paves the way for timing-controlled long-form T2A synthesis. Demo samples are
available at: https://freeaudio.github.io/FreeAudio/

</details>


### [188] [On Barriers to Archival Audio Processing](https://arxiv.org/abs/2507.08768)
*Peter Sullivan,Muhammad Abdul-Mageed*

Main category: cs.SD

TL;DR: 利用UNESCO广播录音测试现代语音识别方法，发现LID系统处理二语和带口音语音能力提升，说话人嵌入组件易有偏见。


<details>
  <summary>Details</summary>
Motivation: 探究现代语言识别（LID）和说话人识别（SR）方法的鲁棒性，特别是多语言使用者和跨年龄录音的影响。

Method: 利用UNESCO 20世纪中叶的广播录音进行测试。

Result: LID系统（如Whisper）处理二语和带口音语音能力增强，说话人嵌入组件易受渠道、年龄和语言相关偏见影响。

Conclusion: 若档案库要使用SR方法进行说话人索引，需克服说话人嵌入组件存在的问题。

Abstract: In this study, we leverage a unique UNESCO collection of mid-20th century
radio recordings to probe the robustness of modern off-the-shelf language
identification (LID) and speaker recognition (SR) methods, especially with
respect to the impact of multilingual speakers and cross-age recordings. Our
findings suggest that LID systems, such as Whisper, are increasingly adept at
handling second-language and accented speech. However, speaker embeddings
remain a fragile component of speech processing pipelines that is prone to
biases related to the channel, age, and language. Issues which will need to be
overcome should archives aim to employ SR methods for speaker indexing.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [189] [Energy Management for Renewable-Colocated Artificial Intelligence Data Centers](https://arxiv.org/abs/2507.08011)
*Siying Li,Lang Tong,Timothy D. Mount*

Main category: math.OC

TL;DR: 开发用于可再生能源共址AI数据中心的EMS，通过联合优化实现利润最大化，实证显示显著收益。


<details>
  <summary>Details</summary>
Motivation: 在可再生能源与AI数据中心共址背景下，实现利润最大化。

Method: 在利润最大化框架下，对AI工作负载调度、现场可再生能源利用和电力市场参与进行联合优化。

Result: 在批发和零售市场参与模型中，RCDC运营的经济效益最大化。

Conclusion: 可再生能源与AI数据中心共址能带来显著利润增长。

Abstract: We develop an energy management system (EMS) for artificial intelligence (AI)
data centers with colocated renewable generation. Under a profit-maximizing
framework, the EMS of renewable-colocated data center (RCDC) co-optimizes AI
workload scheduling, on-site renewable utilization, and electricity market
participation. Within both wholesale and retail market participation models,
the economic benefit of the RCDC operation is maximized. Empirical evaluations
using real-world traces of electricity prices, data center power consumption,
and renewable generation demonstrate significant profit gains from renewable
and AI data center colocations.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [190] [SPINT: Spatial Permutation-Invariant Neural Transformer for Consistent Intracortical Motor Decoding](https://arxiv.org/abs/2507.08402)
*Trung Le,Hao Fang,Jingyuan Li,Tung Nguyen,Lu Mi,Amy Orsborn,Uygar Sümbül,Eli Shlizerman*

Main category: q-bio.NC

TL;DR: 提出SPINT框架用于行为解码，在多会话数据集上表现出色，推动长期iBCI应用。


<details>
  <summary>Details</summary>
Motivation: 长期iBCI面临神经记录非平稳性挑战，现有方法存在泛化性和计算负担问题。

Method: 引入SPINT框架，采用上下文相关位置嵌入方案，支持可变大小群体推理，使用动态通道丢弃正则化方法。

Result: 在三个多会话数据集上评估，SPINT表现出强大的跨会话泛化能力，优于现有基线。

Conclusion: 研究为长期iBCI应用提供了一个稳健且可扩展的神经解码框架的初步方案。

Abstract: Intracortical Brain-Computer Interfaces (iBCI) aim to decode behavior from
neural population activity, enabling individuals with motor impairments to
regain motor functions and communication abilities. A key challenge in
long-term iBCI is the nonstationarity of neural recordings, where the
composition and tuning profiles of the recorded populations are unstable across
recording sessions. Existing methods attempt to address this issue by explicit
alignment techniques; however, they rely on fixed neural identities and require
test-time labels or parameter updates, limiting their generalization across
sessions and imposing additional computational burden during deployment. In
this work, we introduce SPINT - a Spatial Permutation-Invariant Neural
Transformer framework for behavioral decoding that operates directly on
unordered sets of neural units. Central to our approach is a novel
context-dependent positional embedding scheme that dynamically infers
unit-specific identities, enabling flexible generalization across recording
sessions. SPINT supports inference on variable-size populations and allows
few-shot, gradient-free adaptation using a small amount of unlabeled data from
the test session. To further promote model robustness to population
variability, we introduce dynamic channel dropout, a regularization method for
iBCI that simulates shifts in population composition during training. We
evaluate SPINT on three multi-session datasets from the FALCON Benchmark,
covering continuous motor decoding tasks in human and non-human primates. SPINT
demonstrates robust cross-session generalization, outperforming existing
zero-shot and few-shot unsupervised baselines while eliminating the need for
test-time alignment and fine-tuning. Our work contributes an initial step
toward a robust and scalable neural decoding framework for long-term iBCI
applications.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [191] [Detecting Evolutionary Change-Points with Branch-Specific Substitution Models and Shrinkage Priors](https://arxiv.org/abs/2507.08386)
*Xiang Ji,Benjamin Redelings,Shuo Su,Hongcun Bao,Wu-Min Deng,Samuel L. Hong,Guy Baele,Philippe Lemey,Marc A. Suchard*

Main category: q-bio.PE

TL;DR: 本文结合分支特异性替代模型与收缩先验，开发分析梯度算法识别进化变点并估计参数，应用于BRCA1基因和猴痘病毒序列，提升了推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决分支特异性替代模型需先验知识和大数据集可扩展性的局限。

Method: 将分支特异性替代模型与收缩先验结合，开发分析梯度算法估计分支特异性替代参数。

Result: 新算法在最大似然优化中每次迭代加速达90倍，在贝叶斯框架下计算性能提升达360倍。

Conclusion: 新算法增强了推理效率。

Abstract: Branch-specific substitution models are popular for detecting evolutionary
change-points, such as shifts in selective pressure. However, applying such
models typically requires prior knowledge of change-point locations on the
phylogeny or faces scalability issues with large data sets. To address both
limitations, we integrate branch-specific substitution models with shrinkage
priors to automatically identify change-points without prior knowledge, while
simultaneously estimating distinct substitution parameters for each branch. To
enable tractable inference under this high-dimensional model, we develop an
analytical gradient algorithm for the branch-specific substitution parameters
where the computation time is linear in the number of parameters. We apply this
gradient algorithm to infer selection pressure dynamics in the evolution of the
BRCA1 gene in primates and mutational dynamics in viral sequences from the
recent mpox epidemic. Our novel algorithm enhances inference efficiency,
achieving up to a 90-fold speedup per iteration in maximum-likelihood
optimization when compared to central difference numerical gradient method and
up to a 360-fold improvement in computational performance within a Bayesian
framework using Hamiltonian Monte Carlo sampler compared to conventional
univariate random walk sampler.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [192] [Robust Semi-Supervised CT Radiomics for Lung Cancer Prognosis: Cost-Effective Learning with Limited Labels and SHAP Interpretation](https://arxiv.org/abs/2507.08189)
*Mohammad R. Salmanpour,Amir Hossein Pouria,Sonia Falahati,Shahram Taeb,Somayeh Sadat Mehrnia,Ali Fathi Jouzdani,Mehrdad Oveisi,Ilker Hacihaliloglu,Arman Rahmim*

Main category: physics.med-ph

TL;DR: 本文提出半监督学习SSL框架用于肺癌CT生存预测，对比监督学习SL，SSL性能更优且具成本效益。


<details>
  <summary>Details</summary>
Motivation: 监督学习SL模型用于肺癌CT成像预后需大量标注数据，在标注稀缺场景应用受限。

Method: 分析12个数据集977名患者CT扫描，提取1218个影像组学特征，用56种特征选择和提取算法降维，对27个分类器进行基准测试，构建带伪标签的SSL框架，在三种场景测试模型敏感性，用SHAP分析解释预测，进行交叉验证和外部测试。

Result: SSL优于SL，整体生存预测最多提高17%，顶级SSL模型交叉验证准确率0.90，外部测试0.88，SHAP分析显示特征区分性增强，SSL仅用10%标注数据就表现良好，结果更稳定、方差更低。

Conclusion: 引入具成本效益、稳定且可解释的SSL框架用于肺癌CT生存预测，通过集成SHAP可解释性和利用未标注数据提升性能、泛化性和临床可用性。

Abstract: Background: CT imaging is vital for lung cancer management, offering detailed
visualization for AI-based prognosis. However, supervised learning SL models
require large labeled datasets, limiting their real-world application in
settings with scarce annotations.
  Methods: We analyzed CT scans from 977 patients across 12 datasets extracting
1218 radiomics features using Laplacian of Gaussian and wavelet filters via
PyRadiomics Dimensionality reduction was applied with 56 feature selection and
extraction algorithms and 27 classifiers were benchmarked A semi supervised
learning SSL framework with pseudo labeling utilized 478 unlabeled and 499
labeled cases Model sensitivity was tested in three scenarios varying labeled
data in SL increasing unlabeled data in SSL and scaling both from 10 percent to
100 percent SHAP analysis was used to interpret predictions Cross validation
and external testing in two cohorts were performed.
  Results: SSL outperformed SL, improving overall survival prediction by up to
17 percent. The top SSL model, Random Forest plus XGBoost classifier, achieved
0.90 accuracy in cross-validation and 0.88 externally. SHAP analysis revealed
enhanced feature discriminability in both SSL and SL, especially for Class 1
survival greater than 4 years. SSL showed strong performance with only 10
percent labeled data, with more stable results compared to SL and lower
variance across external testing, highlighting SSL's robustness and cost
effectiveness.
  Conclusion: We introduced a cost-effective, stable, and interpretable SSL
framework for CT-based survival prediction in lung cancer, improving
performance, generalizability, and clinical readiness by integrating SHAP
explainability and leveraging unlabeled data.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [193] [AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs](https://arxiv.org/abs/2507.08616)
*Florian Grötschla,Luis Müller,Jan Tönshoff,Mikhail Galkin,Bryan Perozzi*

Main category: cs.MA

TL;DR: 提出多智能体推理新基准AgentsNet，评估多种方法，发现前沿大模型在小网络表现好，网络规模扩大性能下降，AgentsNet规模可扩展。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多智能体系统中展现能力，但复杂智能体网络的自组织和协作能力存疑，且不清楚系统能否有效利用拓扑结构。

Method: 提出AgentsNet基准，借鉴分布式系统和图论经典问题，评估多种基线方法。

Result: 一些前沿大模型在小网络表现强，网络规模扩大性能下降，现有多智能体基准最多涵盖2 - 5个智能体，AgentsNet规模可扩展，还对最多100个智能体的设置进行了探索。

Conclusion: AgentsNet可用于评估多智能体系统的推理、自组织和通信能力，且能随大模型发展而扩展规模。

Abstract: Large-language models (LLMs) have demonstrated powerful problem-solving
capabilities, in particular when organized in multi-agent systems. However, the
advent of such systems also raises several questions on the ability of a
complex network of agents to effectively self-organize and collaborate. While
measuring performance on standard reasoning benchmarks indicates how well
multi-agent systems can solve reasoning tasks, it is unclear whether these
systems are able to leverage their topology effectively. Here, we propose
AgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration
from classical problems in distributed systems and graph theory, AgentsNet
measures the ability of multi-agent systems to collaboratively form strategies
for problem-solving, self-organization, and effective communication given a
network topology. We evaluate a variety of baseline methods on AgentsNet
including homogeneous networks of agents which first have to agree on basic
protocols for organization and communication. We find that some frontier LLMs
are already demonstrating strong performance for small networks but begin to
fall off once the size of the network scales. While existing multi-agent
benchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size
and can scale with new generations of LLMs. As such, we also probe frontier
models in a setup with up to 100 agents.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [194] [Uncertainty quantification of a multi-component Hall thruster model at varying facility pressures](https://arxiv.org/abs/2507.08113)
*Thomas A. Marks,Joshua D. Eckels,Gabriel E. Mora,Alex A. Gorodetsky*

Main category: stat.AP

TL;DR: 应用贝叶斯推理校准和量化多组件霍尔推进器模型在不同背景压力下的预测不确定性，模型经校准后能捕捉关键性能趋势，预测精度较高，还对在轨性能进行外推。


<details>
  <summary>Details</summary>
Motivation: 校准和量化耦合多组件霍尔推进器模型在不同设施背景压力下的预测不确定性。

Method: 运用贝叶斯推理对由阴极、放电和羽流模型组成的耦合多组件霍尔推进器模型进行校准，模拟两种推进器在多真空测试设施不同背景压力下的情况。

Result: 模型校准后能捕捉关键性能趋势，预测精度达10%以内，相比先前工作减少超50%预测误差，在轨性能外推误差9%。

Conclusion: 模型表现良好，但在轨性能外推未完全捕捉推力趋势，还探讨了基于数据的跨真空设施预测性霍尔推进器建模的扩展和改进方向。

Abstract: Bayesian inference is applied to calibrate and quantify prediction
uncertainty in a coupled multi-component Hall thruster model at varying
facility background pressures. The model, consisting of a cathode model,
discharge model, and plume model, is used to simulate two thrusters across a
range of background pressures in multiple vacuum test facilities. The model
outputs include thruster performance metrics, one-dimensional plasma
properties, and the angular distribution of the current density in the plume.
The simulated thrusters include a magnetically shielded thruster, the H9, and
an unshielded thruster, the SPT-100. After calibration, the model captures
several key performance trends with background pressure, including changes in
thrust and upstream shifts in the ion acceleration region. Furthermore, the
model exhibits predictive accuracy to within 10\% when evaluated on flow rates
and pressures not included in the training data, and the model can predict some
performance characteristics across test facilities to within the same range.
Evaluated on the same data as prior work [Eckels et al. 2024], the model
reduced predictive errors in thrust and discharge current by greater than 50%.
An extrapolation to on-orbit performance is performed with an error of 9\%,
capturing trends in discharge current but not thrust. Possible extensions and
improvements are discussed in the context of using data for predictive Hall
thruster modeling across vacuum facilities.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [195] [Addressing overlapping communities in multiple-source detection: An edge clustering approach for complex networks](https://arxiv.org/abs/2507.08265)
*Haomin Li,Daniel K. Sewell*

Main category: cs.SI

TL;DR: 本文提出将边聚类算法集成到基于社区的标签传播框架来解决网络分析中的多源检测问题，在模拟研究中表现优于现有聚类算法，提升了基于聚类方法的适用性。


<details>
  <summary>Details</summary>
Motivation: 传统网络分析源检测方法多关注单源，而现实中常是多源情况，检测难度大，需解决多源检测（MSD）问题。

Method: 将自动潜在空间边聚类模型应用于网络，把受感染网络划分为基于边的簇来识别多源。

Result: 在ADD HEALTH社交网络数据集的模拟研究中，该方法的F1值显示其准确性优于现有聚类算法，在复杂和重叠源区域网络中能准确检测源。

Conclusion: 这项工作推进了基于聚类方法在MSD问题中的适用性，提高了现实网络分析的准确性和适应性。

Abstract: The source detection problem in network analysis involves identifying the
origins of diffusion processes, such as disease outbreaks or misinformation
propagation. Traditional methods often focus on single sources, whereas
real-world scenarios frequently involve multiple sources, complicating
detection efforts. This study addresses the multiple-source detection (MSD)
problem by integrating edge clustering algorithms into the community-based
label propagation framework, effectively handling mixed-membership issues where
nodes belong to multiple communities.
  The proposed approach applies the automated latent space edge clustering
model to a network, partitioning infected networks into edge-based clusters to
identify multiple sources. Simulation studies on ADD HEALTH social network
datasets demonstrate that this method achieves superior accuracy, as measured
by the F1-Measure, compared to state-of-the-art clustering algorithms. The
results highlight the robustness of edge clustering in accurately detecting
sources, particularly in networks with complex and overlapping source regions.
This work advances the applicability of clustering-based methods to MSD
problems, offering improved accuracy and adaptability for real-world network
analyses.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [196] [CCSS: Hardware-Accelerated RTL Simulation with Fast Combinational Logic Computing and Sequential Logic Synchronization](https://arxiv.org/abs/2507.08406)
*Weigang Feng,Yijia Zhang,Zekun Wang,Zhengyang Wang,Yi Wang,Peijun Ma,Ningyi Xu*

Main category: cs.AR

TL;DR: 随着单芯片晶体管数量增加，RTL 级仿真验证复杂，CPU 仿真速度受限，提出 CCSS 平台加速，实验显示比现有多核模拟器快达 12.9 倍。


<details>
  <summary>Details</summary>
Motivation: 单芯片晶体管数量增多使 RTL 级仿真验证复杂度指数级增长，CPU 仿真速度有限成为瓶颈。

Method: 提出 CCSS 平台，通过专门架构和编译策略加速组合逻辑计算和时序逻辑同步，采用平衡 DAG 分区法和高效布尔计算核心处理组合逻辑，采用低延迟片上网络设计同步时序状态。

Result: CCSS 比现有多核模拟器实现了高达 12.9 倍的加速。

Conclusion: CCSS 平台可有效解决 CPU 仿真速度瓶颈问题，实现快速编译和高仿真吞吐量。

Abstract: As transistor counts in a single chip exceed tens of billions, the complexity
of RTL-level simulation and verification has grown exponentially, often
extending simulation campaigns to several months. In industry practice, RTL
simulation is divided into two phases: functional debug and system validation.
While system validation demands high simulation speed and is typically
accelerated using FPGAs, functional debug relies on rapid compilation-rendering
multi-core CPUs the primary choice. However, the limited simulation speed of
CPUs has become a major bottleneck. To address this challenge, we propose CCSS,
a scalable multi-core RTL simulation platform that achieves both fast
compilation and high simulation throughput. CCSS accelerates combinational
logic computation and sequential logic synchronization through specialized
architecture and compilation strategies. It employs a balanced DAG partitioning
method and efficient boolean computation cores for combinational logic, and
adopts a low-latency network-on-chip (NoC) design to synchronize sequential
states across cores efficiently. Experimental results show that CCSS delivers
up to 12.9x speedup over state-of-the-art multi-core simulators.

</details>


### [197] [Fast and Efficient Merge of Sorted Input Lists in Hardware Using List Offset Merge Sorters](https://arxiv.org/abs/2507.08658)
*Robert B. Kent,Marios S. Pattichis*

Main category: cs.AR

TL;DR: 介绍新型硬件归并排序设备List Offset Merge Sorters (LOMS)，比先前设备更快，资源使用更优。


<details>
  <summary>Details</summary>
Motivation: 设计更快、更高效且能处理不同输入列表大小的硬件归并排序设备。

Method: 将排序输入列表的值排列在二维输入设置数组中，通过列排序和行排序阶段处理数组。LOMS 2 - 路排序器首阶段利用Single - Stage 2 - way Merge Sorters (S2MS)。

Result: LOMS 2 - 路排序器比Kenneth Batcher的设备快，可处理任意输入列表大小；S2MS在FPGA设备中最快但资源使用多；LOMS资源使用少于S2MS；给出了具体的排序时间和加速比。

Conclusion: LOMS设备在速度和资源使用上有优势，是高效的硬件归并排序解决方案。

Abstract: A new set of hardware merge sort devices are introduced here, which merge
multiple sorted input lists into a single sorted output list in a fast and
efficient manner. In each merge sorter, the values from the sorted input lists
are arranged in an input 2-D setup array, but with the order of each sorted
input list offset from the order of each of the other sorted input lists. In
these new devices, called List Offset Merge Sorters (LOMS), a minimal set of
column sort stages alternating with row sort stages process the input setup
array into a final output array, now in the defined sorted order. LOMS 2-way
sorters, which merge 2 sorted input lists, require only 2 merge stages and are
significantly faster than Kenneth Batcher's previous state-of-the-art 2-way
merge devices, Bitonic Merge Sorters and Odd-Even Merge Sorters. LOMS 2-way
sorters utilize the recently-introduced Single-Stage 2-way Merge Sorters (S2MS)
in their first stage. Both LOMS and S2MS devices can merge any mixture of input
list sizes, while Batcher's merge sorters are difficult to design unless the 2
input lists are equal, and a power-of-2. By themselves, S2MS devices are the
fastest 2-way merge sorters when implemented in this study's target FPGA
devices, but they tend to use a large number of LUT resources. LOMS 2-way
devices use fewer resources than comparable S2MS devices, enabling some large
LOMS devices to be implemented in a given FPGA when comparable S2MS devices
cannot fit in that FPGA. A List Offset 2-way sorter merges 2 lists, each with
32 values, into a sorted output list of those 64 values in 2.24 nS, a speedup
of 2.63 versus a comparable Batcher device. A LOMS 3-way merge sorter, merging
3 sorted input lists with 7 values, fully merges the 21 values in 3.4 nS, a
speedup of 1.36 versus the comparable state-of-the-art 3-way merge device.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [198] [VideoConviction: A Multimodal Benchmark for Human Conviction and Stock Market Recommendations](https://arxiv.org/abs/2507.08104)
*Michael Galarnyk,Veer Kejriwal,Agam Shah,Yash Bhardwaj,Nicholas Meyer,Anand Krishnan,Sudheer Chava*

Main category: cs.MM

TL;DR: 本文引入VideoConviction数据集评估金融领域多模态和文本大模型，发现多模态输入有助于股票代码提取，但模型难区分投资行动和信心，高信心推荐表现不佳，反向策略回报高但风险大，基准可促进多模态金融研究。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上金融影响者增多，需分析多模态信号理解其影响，评估多模态和文本大模型在金融话语中的表现。

Method: 引入含6000+专家注释的VideoConviction数据集，对多模态和文本大模型进行基准测试。

Result: 多模态输入助于股票代码提取，模型难区分投资行动和信心，高信心推荐不如标普500指数基金，反向策略回报高但风险大，基准可评估多模态任务。

Conclusion: 所提供的基准能促进多模态金融研究，代码、数据集和评估排行榜公开。

Abstract: Social media has amplified the reach of financial influencers known as
"finfluencers," who share stock recommendations on platforms like YouTube.
Understanding their influence requires analyzing multimodal signals like tone,
delivery style, and facial expressions, which extend beyond text-based
financial analysis. We introduce VideoConviction, a multimodal dataset with
6,000+ expert annotations, produced through 457 hours of human effort, to
benchmark multimodal large language models (MLLMs) and text-based large
language models (LLMs) in financial discourse. Our results show that while
multimodal inputs improve stock ticker extraction (e.g., extracting Apple's
ticker AAPL), both MLLMs and LLMs struggle to distinguish investment actions
and conviction--the strength of belief conveyed through confident delivery and
detailed reasoning--often misclassifying general commentary as definitive
recommendations. While high-conviction recommendations perform better than
low-conviction ones, they still underperform the popular S\&P 500 index fund.
An inverse strategy--betting against finfluencer recommendations--outperforms
the S\&P 500 by 6.8\% in annual returns but carries greater risk (Sharpe ratio
of 0.41 vs. 0.65). Our benchmark enables a diverse evaluation of multimodal
tasks, comparing model performance on both full video and segmented video
inputs. This enables deeper advancements in multimodal financial research. Our
code, dataset, and evaluation leaderboard are available under the CC BY-NC 4.0
license.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [199] [Filter Equivariant Functions: A symmetric account of length-general extrapolation on lists](https://arxiv.org/abs/2507.08796)
*Owen Lewis,Neil Ghani,Andrew Dudzik,Christos Perivolaropoulos,Razvan Pascanu,Petar Veličković*

Main category: cs.PL

TL;DR: 论文探讨函数外推问题，引入过滤等变函数语义类，证明相关定理，给出几何解释，提出融合算法。


<details>
  <summary>Details</summary>
Motivation: 回答函数在已知输入/输出示例之外的外推应是什么样的问题，认为好的外推函数应遵循特定规则。

Method: 引入过滤等变函数语义类，证明基本定理，将其与映射等变函数类建立联系，给出几何解释。

Result: 得到融合算法，可通过研究函数在输入子列表上的行为完美外推过滤等变函数的输出。

Conclusion: 过滤等变函数是解决函数外推问题的一种有效方式，融合算法能实现完美外推。

Abstract: What should a function that extrapolates beyond known input/output examples
look like? This is a tricky question to answer in general, as any function
matching the outputs on those examples can in principle be a correct
extrapolant. We argue that a "good" extrapolant should follow certain kinds of
rules, and here we study a particularly appealing criterion for rule-following
in list functions: that the function should behave predictably even when
certain elements are removed. In functional programming, a standard way to
express such removal operations is by using a filter function. Accordingly, our
paper introduces a new semantic class of functions -- the filter equivariant
functions. We show that this class contains interesting examples, prove some
basic theorems about it, and relate it to the well-known class of map
equivariant functions. We also present a geometric account of filter
equivariants, showing how they correspond naturally to certain simplicial
structures. Our highlight result is the amalgamation algorithm, which
constructs any filter-equivariant function's output by first studying how it
behaves on sublists of the input, in a way that extrapolates perfectly.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [200] [A Personalised Formal Verification Framework for Monitoring Activities of Daily Living of Older Adults Living Independently in Their Homes](https://arxiv.org/abs/2507.08701)
*Ricardo Contreras,Filip Smola,Nuša Farič,Jiawei Zheng,Jane Hillston,Jacques D. Fleuriot*

Main category: cs.LO

TL;DR: 本文提出一个框架用于表示和推理居家独立生活老年人的日常生活活动，展示其通用性及对老年人安全和福祉的提升潜力。


<details>
  <summary>Details</summary>
Motivation: 为居家独立生活的老年人群体提供有质量的生活，需要关注个人偏好和情境的个性化解决方案。

Method: 整合传感器数据和情境信息，为每个参与者创建个性化形式模型；用线性时态逻辑将个人特定需求编码为属性，用模型检查器验证属性是否满足，违规时生成反例。

Result: 框架可应用于不同参与者，展示了通用性。

Conclusion: 该框架有潜力提升居家养老老年人的安全和福祉。

Abstract: There is an imperative need to provide quality of life to a growing
population of older adults living independently. Personalised solutions that
focus on the person and take into consideration their preferences and context
are key. In this work, we introduce a framework for representing and reasoning
about the Activities of Daily Living of older adults living independently at
home. The framework integrates data from sensors and contextual information
that aggregates semi-structured interviews, home layouts and sociological
observations from the participants. We use these data to create formal models,
personalised for each participant according to their preferences and context.
We formulate requirements that are specific to each individual as properties
encoded in Linear Temporal Logic and use a model checker to verify whether each
property is satisfied by the model. When a property is violated, a
counterexample is generated giving the cause of the violation. We demonstrate
the framework's generalisability by applying it to different participants,
highlighting its potential to enhance the safety and well-being of older adults
ageing in place.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [201] [Safe Deep Reinforcement Learning for Resource Allocation with Peak Age of Information Violation Guarantees](https://arxiv.org/abs/2507.08653)
*Berire Gunes Reyhan,Sinem Coleri*

Main category: eess.SP

TL;DR: 本文提出基于优化理论的安全深度强化学习框架用于超可靠无线网络控制系统，分两阶段优化，仿真显示性能优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 无线网络控制系统中控制与通信系统相互依赖，需协同设计，要在满足约束条件下优化性能并降低功耗。

Method: 框架分两阶段，第一阶段用优化理论推导最优条件，第二阶段用安全深度强化学习，采用师生框架引导智能体。

Result: 仿真表明，该框架比基于规则和其他基于优化理论的深度强化学习基准方法收敛更快、奖励更高、稳定性更强。

Conclusion: 提出的优化理论-based安全深度强化学习框架有效，能提高超可靠无线网络控制系统性能。

Abstract: In Wireless Networked Control Systems (WNCSs), control and communication
systems must be co-designed due to their strong interdependence. This paper
presents a novel optimization theory-based safe deep reinforcement learning
(DRL) framework for ultra-reliable WNCSs, ensuring constraint satisfaction
while optimizing performance, for the first time in the literature. The
approach minimizes power consumption under key constraints, including Peak Age
of Information (PAoI) violation probability, transmit power, and schedulability
in the finite blocklength regime. PAoI violation probability is uniquely
derived by combining stochastic maximum allowable transfer interval (MATI) and
maximum allowable packet delay (MAD) constraints in a multi-sensor network. The
framework consists of two stages: optimization theory and safe DRL. The first
stage derives optimality conditions to establish mathematical relationships
among variables, simplifying and decomposing the problem. The second stage
employs a safe DRL model where a teacher-student framework guides the DRL agent
(student). The control mechanism (teacher) evaluates compliance with system
constraints and suggests the nearest feasible action when needed. Extensive
simulations show that the proposed framework outperforms rule-based and other
optimization theory based DRL benchmarks, achieving faster convergence, higher
rewards, and greater stability.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [202] [A Versatile Dataset of Mouse and Eye Movements on Search Engine Results Pages](https://arxiv.org/abs/2507.08003)
*Kayhan Latifzadeh,Jacek Gwizdka,Luis A. Leiva*

Main category: cs.HC

TL;DR: 本文贡献了一个综合数据集，用于研究搜索引擎结果页面上用户的注意力和购买行为，并给出了数据集概述和基线实验。


<details>
  <summary>Details</summary>
Motivation: 以往研究依赖鼠标移动作为行为代理，且使用事后自我报告的标签，存在不准确和有偏差的问题，因此需要构建客观的视觉注意力真实标签。

Method: 使用眼动仪构建连续视觉注意力的客观真实标签，并收集了来自47名参与者的2776个交易查询相关数据。

Result: 得到了包含HTML源文件、截图、眼动和鼠标移动数据等多方面信息的数据集。

Conclusion: 提供了数据集概述和基线实验，为未来研究提供不同可能性。

Abstract: We contribute a comprehensive dataset to study user attention and purchasing
behavior on Search Engine Result Pages (SERPs). Previous work has relied on
mouse movements as a low-cost large-scale behavioral proxy but also has relied
on self-reported ground-truth labels, collected at post-task, which can be
inaccurate and prone to biases. To address this limitation, we use an eye
tracker to construct an objective ground-truth of continuous visual attention.
Our dataset comprises 2,776 transactional queries on Google SERPs, collected
from 47 participants, and includes: (1) HTML source files, with CSS and images;
(2) rendered SERP screenshots; (3) eye movement data; (4) mouse movement data;
(5) bounding boxes of direct display and organic advertisements; and (6)
scripts for further preprocessing the data. In this paper we provide an
overview of the dataset and baseline experiments (classification tasks) that
can inspire researchers about the different possibilities for future work.

</details>


### [203] [Human vs. LLM-Based Thematic Analysis for Digital Mental Health Research: Proof-of-Concept Comparative Study](https://arxiv.org/abs/2507.08002)
*Karisa Parkington,Bazen G. Teferra,Marianne Rouleau-Tang,Argyrios Perivolaris,Alice Rueda,Adam Dubrowski,Bill Kapralos,Reza Samavi,Andrew Greenshaw,Yanbo Zhang,Bo Cao,Yuqi Wu,Sirisha Rambhatla,Sridhar Krishnan,Venkat Bhat*

Main category: cs.HC

TL;DR: 研究对比基于大语言模型（LLM）和传统方法的主题分析，发现LLM成本效益高但深度不足，结合人工监督可用于精神卫生保健和临床研究。


<details>
  <summary>Details</summary>
Motivation: 主题分析资源消耗大限制其在大型医疗研究中的应用，LLM可大规模分析文本，但在心理健康访谈中的应用需与传统人工分析比较。

Method: 使用压力减轻试验的医疗工作者访谈记录，采用OpenAI的GPT - 4o模型和RISEN提示工程框架，与Dedoose中的人工分析对比，开发代码、确定饱和点、应用代码并合成主题，直接比较输出和性能指标。

Result: 基于RISEN框架的LLM开发的演绎父代码与人工代码相似，人工在归纳子代码开发和主题合成方面更出色；基于知识的LLM达到编码饱和所需记录更少；开箱即用的LLM识别的摘录数量与人工相当，有强评分者间信度；人工摘录更长且涉及多代码，LLM通常应用单代码。

Conclusion: 基于LLM的主题分析更具成本效益，但缺乏人工分析的深度，结合人工监督可改变精神卫生保健和临床研究中的定性分析。

Abstract: Thematic analysis provides valuable insights into participants' experiences
through coding and theme development, but its resource-intensive nature limits
its use in large healthcare studies. Large language models (LLMs) can analyze
text at scale and identify key content automatically, potentially addressing
these challenges. However, their application in mental health interviews needs
comparison with traditional human analysis. This study evaluates out-of-the-box
and knowledge-base LLM-based thematic analysis against traditional methods
using transcripts from a stress-reduction trial with healthcare workers.
OpenAI's GPT-4o model was used along with the Role, Instructions, Steps,
End-Goal, Narrowing (RISEN) prompt engineering framework and compared to human
analysis in Dedoose. Each approach developed codes, noted saturation points,
applied codes to excerpts for a subset of participants (n = 20), and
synthesized data into themes. Outputs and performance metrics were compared
directly. LLMs using the RISEN framework developed deductive parent codes
similar to human codes, but humans excelled in inductive child code development
and theme synthesis. Knowledge-based LLMs reached coding saturation with fewer
transcripts (10-15) than the out-of-the-box model (15-20) and humans (90-99).
The out-of-the-box LLM identified a comparable number of excerpts to human
researchers, showing strong inter-rater reliability (K = 0.84), though the
knowledge-based LLM produced fewer excerpts. Human excerpts were longer and
involved multiple codes per excerpt, while LLMs typically applied one code.
Overall, LLM-based thematic analysis proved more cost-effective but lacked the
depth of human analysis. LLMs can transform qualitative analysis in mental
healthcare and clinical research when combined with human oversight to balance
participant perspectives and research resources.

</details>


### [204] [SSSUMO: Real-Time Semi-Supervised Submovement Decomposition](https://arxiv.org/abs/2507.08028)
*Evgenii Rudakov,Jonathan Shock,Otto Lappi,Benjamin Ultan Cowley*

Main category: cs.HC

TL;DR: 本文提出半监督深度学习方法SSSUMO用于子运动分解，在精度和速度上达最优，能促进多领域应用。


<details>
  <summary>Details</summary>
Motivation: 现有子运动分析方法在重建精度、计算成本和验证方面存在问题，因难以获取手动标注数据。

Method: 采用半监督学习框架，从最小 jerk 原则生成的合成数据学习，并通过适应未标记的人类运动数据迭代优化，使用全卷积架构和可微重建。

Result: 在合成和多样的人类运动数据集上显著超越现有方法，能实时运行，在高噪声条件下也很稳健。

Conclusion: 该模型性能提升有助于人机交互、康复医学和运动控制研究等新应用，且代码和预训练模型权重公开。

Abstract: This paper introduces a SSSUMO, semi-supervised deep learning approach for
submovement decomposition that achieves state-of-the-art accuracy and speed.
While submovement analysis offers valuable insights into motor control,
existing methods struggle with reconstruction accuracy, computational cost, and
validation, due to the difficulty of obtaining hand-labeled data. We address
these challenges using a semi-supervised learning framework. This framework
learns from synthetic data, initially generated from minimum-jerk principles
and then iteratively refined through adaptation to unlabeled human movement
data. Our fully convolutional architecture with differentiable reconstruction
significantly surpasses existing methods on both synthetic and diverse human
motion datasets, demonstrating robustness even in high-noise conditions.
Crucially, the model operates in real-time (less than a millisecond per input
second), a substantial improvement over optimization-based techniques. This
enhanced performance facilitates new applications in human-computer
interaction, rehabilitation medicine, and motor control studies. We demonstrate
the model's effectiveness across diverse human-performed tasks such as
steering, rotation, pointing, object moving, handwriting, and mouse-controlled
gaming, showing notable improvements particularly on challenging datasets where
traditional methods largely fail. Training and benchmarking source code, along
with pre-trained model weights, are made publicly available at
https://github.com/dolphin-in-a-coma/sssumo.

</details>


### [205] [Emotion Detection in Older Adults Using Physiological Signals from Wearable Sensors](https://arxiv.org/abs/2507.08167)
*Md. Saif Hassan Onim,Andrew M. Kiselica,Himanshu Thapliyal*

Main category: cs.HC

TL;DR: 本文研究基于可穿戴传感器生理信号的老年人情感识别，利用经典机器学习模型预测情感强度，取得较好结果，验证该方法可行性。


<details>
  <summary>Details</summary>
Motivation: 了解老年人认知和情感健康，避免使用摄像头或侵入式面部分析，为有认知障碍人群提供帮助。

Method: 使用Empatica E4和Shimmer3 GSR+腕带获取生理信号，iMotion的FEA模块记录面部表情，构建含40位老年人数据和12种情感类别的数据集，利用经典机器学习模型基于生理信号预测情感强度。

Result: 在回归任务中取得最高0.782的r2分数和最低0.0006的均方误差。

Conclusion: 该方法可行，为现实场景下保护隐私且高效的情感识别系统铺平道路。

Abstract: Emotion detection in older adults is crucial for understanding their
cognitive and emotional well-being, especially in hospital and assisted living
environments. In this work, we investigate an edge-based, non-obtrusive
approach to emotion identification that uses only physiological signals
obtained via wearable sensors. Our dataset includes data from 40 older
individuals. Emotional states were obtained using physiological signals from
the Empatica E4 and Shimmer3 GSR+ wristband and facial expressions were
recorded using camera-based emotion recognition with the iMotion's Facial
Expression Analysis (FEA) module. The dataset also contains twelve emotion
categories in terms of relative intensities. We aim to study how well emotion
recognition can be accomplished using simply physiological sensor data, without
the requirement for cameras or intrusive facial analysis. By leveraging
classical machine learning models, we predict the intensity of emotional
responses based on physiological signals. We achieved the highest 0.782 r2
score with the lowest 0.0006 MSE on the regression task. This method has
significant implications for individuals with Alzheimer's Disease and Related
Dementia (ADRD), as well as veterans coping with Post-Traumatic Stress Disorder
(PTSD) or other cognitive impairments. Our results across multiple classical
regression models validate the feasibility of this method, paving the way for
privacy-preserving and efficient emotion recognition systems in real-world
settings.

</details>


### [206] [Adaptive Framework for Ambient Intelligence in Rehabilitation Assistance](https://arxiv.org/abs/2507.08624)
*Gábor Baranyi,Zsolt Csibi,Kristian Fenech,Áron Fóthi,Zsófia Gaál,Joul Skaf,András Lőrincz*

Main category: cs.HC

TL;DR: 本文介绍适用于家庭康复的AIRS框架，集成多种技术，用于膝关节置换术后康复，有反馈机制，支持残障人士，设计模块化且软件组件可定制。


<details>
  <summary>Details</summary>
Motivation: 为家庭康复环境提供先进的人工智能解决方案，助力物理康复。

Method: 集成RT - 3DR、智能导航和VLMs等技术，利用数据库评估，用智能手机进行RT - 3DR，使用身体匹配的化身提供反馈，采用两种反馈机制。

Result: 在膝关节置换术后康复场景中得到验证，系统能指导用户录制视频，提供反馈，支持残障人士。

Conclusion: AIRS框架具有模块化设计，可适应更广泛的康复场景，软件组件可供进一步使用和定制。

Abstract: This paper introduces the Ambient Intelligence Rehabilitation Support (AIRS)
framework, an advanced artificial intelligence-based solution tailored for home
rehabilitation environments. AIRS integrates cutting-edge technologies,
including Real-Time 3D Reconstruction (RT-3DR), intelligent navigation, and
large Vision-Language Models (VLMs), to create a comprehensive system for
machine-guided physical rehabilitation. The general AIRS framework is
demonstrated in rehabilitation scenarios following total knee replacement
(TKR), utilizing a database of 263 video recordings for evaluation. A
smartphone is employed within AIRS to perform RT-3DR of living spaces and has a
body-matched avatar to provide visual feedback about the excercise. This avatar
is necessary in (a) optimizing exercise configurations, including camera
placement, patient positioning, and initial poses, and (b) addressing privacy
concerns and promoting compliance with the AI Act. The system guides users
through the recording process to ensure the collection of properly recorded
videos. AIRS employs two feedback mechanisms: (i) visual 3D feedback, enabling
direct comparisons between prerecorded clinical exercises and patient home
recordings and (ii) VLM-generated feedback, providing detailed explanations and
corrections for exercise errors. The framework also supports people with visual
and hearing impairments. It also features a modular design that can be adapted
to broader rehabilitation contexts. AIRS software components are available for
further use and customization.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [207] [GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs](https://arxiv.org/abs/2507.08107)
*Sebastian Walter,Hannah Bast*

Main category: cs.CL

TL;DR: 提出用大语言模型在RDF知识图谱上生成SPARQL查询的新方法，无需微调，在多基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 提出新的从自然语言问题或关键词查询在RDF知识图谱上生成SPARQL查询的方法。

Method: 使用大语言模型，通过策略性执行SPARQL查询探索知识图谱，搜索相关IRI和文字，无需微调。

Result: 在Wikidata上多个基准测试达到SOTA，在Freebase接近最佳少样本方法，在其他知识图谱和基准测试中整体表现良好。

Conclusion: 该方法有效，还开展了如比较图搜索方式、引入反馈机制等额外研究。

Abstract: We propose a new approach for generating SPARQL queries on RDF knowledge
graphs from natural language questions or keyword queries, using a large
language model. Our approach does not require fine-tuning. Instead, it uses the
language model to explore the knowledge graph by strategically executing SPARQL
queries and searching for relevant IRIs and literals. We evaluate our approach
on a variety of benchmarks (for knowledge graphs of different kinds and sizes)
and language models (of different scales and types, commercial as well as
open-source) and compare it with existing approaches. On Wikidata we reach
state-of-the-art results on multiple benchmarks, despite the zero-shot setting.
On Freebase we come close to the best few-shot methods. On other, less commonly
evaluated knowledge graphs and benchmarks our approach also performs well
overall. We conduct several additional studies, like comparing different ways
of searching the graphs, incorporating a feedback mechanism, or making use of
few-shot examples.

</details>


### [208] [A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models](https://arxiv.org/abs/2507.08030)
*Sonali Sharma,Ahmed M. Alaa,Roxana Daneshjou*

Main category: cs.CL

TL;DR: 研究评估2022 - 2025年生成式AI模型输出中医疗免责声明的存在情况，发现免责声明比例大幅下降，需在输出中实施免责声明作为保障。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型在医学图像解读和临床问题回答中常有不准确，需安全措施如医疗免责声明提醒用户，评估免责声明在模型输出中的存在情况。

Method: 使用500张乳腺钼靶片、500张胸部X光片、500张皮肤科图像和500个医学问题，筛查输出中的免责声明短语。

Result: LLM和VLM输出中医疗免责声明的存在比例分别从2022年的26.3%降至2025年的0.97%，从2023年的19.6%降至2025年的1.05%，到2025年多数模型无免责声明。

Conclusion: 随着公共模型能力增强且更具权威性，需实施免责声明以适应每个输出的临床背景。

Abstract: Generative AI models, including large language models (LLMs) and
vision-language models (VLMs), are increasingly used to interpret medical
images and answer clinical questions. Their responses often include
inaccuracies; therefore, safety measures like medical disclaimers are critical
to remind users that AI outputs are not professionally vetted or a substitute
for medical advice. This study evaluated the presence of disclaimers in LLM and
VLM outputs across model generations from 2022 to 2025. Using 500 mammograms,
500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs
were screened for disclaimer phrases. Medical disclaimer presence in LLM and
VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023
to 1.05% in 2025, respectively. By 2025, the majority of models displayed no
disclaimers. As public models become more capable and authoritative,
disclaimers must be implemented as a safeguard adapting to the clinical context
of each output.

</details>


### [209] [Signal or Noise? Evaluating Large Language Models in Resume Screening Across Contextual Variations and Human Expert Benchmarks](https://arxiv.org/abs/2507.08019)
*Aryan Varshney,Venkat Ram Reddy Ganuthula*

Main category: cs.CL

TL;DR: 研究大语言模型筛选简历时的表现及与人类专家对比，发现LLMs有可解释模式但与人类判断差异大。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型筛选简历时是表现出一致行为还是随机变化，以及其与人类专家的表现对比。

Method: 使用控制数据集，测试Claude、GPT和Gemini三个大语言模型，在不同情境下与人类招聘专家对比，进行方差分析和配对t检验及元认知分析。

Result: 方差分析显示部分LLM-only条件有显著均值差异，LLM与人类评估有显著差异；配对t检验表明GPT强适应公司情境，Gemini部分适应，Claude适应度低；元认知分析突出与人类评估方式显著不同的自适应加权模式。

Conclusion: 大语言模型在详细提示下有可解释模式，但与人类判断差异大，为自动化招聘系统中部署提供参考。

Abstract: This study investigates whether large language models (LLMs) exhibit
consistent behavior (signal) or random variation (noise) when screening resumes
against job descriptions, and how their performance compares to human experts.
Using controlled datasets, we tested three LLMs (Claude, GPT, and Gemini)
across contexts (No Company, Firm1 [MNC], Firm2 [Startup], Reduced Context)
with identical and randomized resumes, benchmarked against three human
recruitment experts. Analysis of variance revealed significant mean differences
in four of eight LLM-only conditions and consistently significant differences
between LLM and human evaluations (p < 0.01). Paired t-tests showed GPT adapts
strongly to company context (p < 0.001), Gemini partially (p = 0.038 for
Firm1), and Claude minimally (p > 0.1), while all LLMs differed significantly
from human experts across contexts. Meta-cognition analysis highlighted
adaptive weighting patterns that differ markedly from human evaluation
approaches. Findings suggest LLMs offer interpretable patterns with detailed
prompts but diverge substantially from human judgment, informing their
deployment in automated hiring systems.

</details>


### [210] [UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations](https://arxiv.org/abs/2507.07030)
*Fengran Mo,Yifan Gao,Chuan Meng,Xin Liu,Zhuofeng Wu,Kelong Mao,Zhengyang Wang,Pei Chen,Zheng Li,Xian Li,Bing Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 本文探索大语言模型在对话中统一密集检索和响应生成的方法，联合微调并设计机制，实验表明统一模型表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有对话搜索系统模型分离，无法同时利用模型内在知识，已有统一模型研究不能完全解决理解对话上下文等问题。

Method: 进行不同目标的联合微调，设计两种机制以降低不一致风险并减轻数据差异。

Result: 在五个对话搜索数据集上的评估显示，统一模型能相互提升两个任务表现，优于现有基线。

Conclusion: 所提出的统一密集检索和响应生成的方法有效，可改善对话搜索系统性能。

Abstract: The rapid advancement of conversational search systems revolutionizes how
information is accessed by enabling the multi-turn interaction between the user
and the system. Existing conversational search systems are usually built with
two different models. This separation restricts the system from leveraging the
intrinsic knowledge of the models simultaneously, which cannot ensure the
effectiveness of retrieval benefiting the generation. The existing studies for
developing unified models cannot fully address the aspects of understanding
conversational context, managing retrieval independently, and generating
responses. In this paper, we explore how to unify dense retrieval and response
generation for large language models in conversation. We conduct joint
fine-tuning with different objectives and design two mechanisms to reduce the
inconsistency risks while mitigating data discrepancy. The evaluations on five
conversational search datasets demonstrate that our unified model can mutually
improve both tasks and outperform the existing baselines.

</details>


### [211] [Distillation versus Contrastive Learning: How to Train Your Rerankers](https://arxiv.org/abs/2507.08336)
*Zhichao Xu,Zhiqi Huang,Shengyao Zhuang,Ashim Gupta,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本文对对比学习和知识蒸馏两种训练文本重排器的策略进行实证比较，发现从大教师模型蒸馏时知识蒸馏效果更好，为选择训练策略提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏在实际条件下对比学习和知识蒸馏训练交叉编码器重排器有效性的明确比较，需进行研究。

Method: 在相同数据上使用两种方法训练不同大小和架构的重排器，以强对比学习模型作为蒸馏教师。

Result: 从大教师模型蒸馏时，知识蒸馏在域内和域外排名性能上通常优于对比学习，在不同学生模型大小和架构中结果一致；从同容量教师模型蒸馏时无此优势，尤其在域外任务中。

Conclusion: 若有更大、更强的教师模型，建议用知识蒸馏训练较小重排器；否则，对比学习是可靠替代方案。

Abstract: Training text rerankers is crucial for information retrieval. Two primary
strategies are widely used: contrastive learning (optimizing directly on
ground-truth labels) and knowledge distillation (transferring knowledge from a
larger reranker). While both have been studied in the literature, a clear
comparison of their effectiveness for training cross-encoder rerankers under
practical conditions is needed.
  This paper empirically compares these strategies by training rerankers of
different sizes and architectures using both methods on the same data, with a
strong contrastive learning model acting as the distillation teacher. Our
results show that knowledge distillation generally yields better in-domain and
out-of-domain ranking performance than contrastive learning when distilling
from a larger teacher model. This finding is consistent across student model
sizes and architectures. However, distilling from a teacher of the same
capacity does not provide the same advantage, particularly for out-of-domain
tasks. These findings offer practical guidance for choosing a training strategy
based on available teacher models. Therefore, we recommend using knowledge
distillation to train smaller rerankers if a larger, more powerful teacher is
accessible; in its absence, contrastive learning provides a strong and more
reliable alternative otherwise.

</details>


### [212] [Multilingual Multimodal Software Developer for Code Generation](https://arxiv.org/abs/2507.08719)
*Linzheng Chai,Jian Yang,Shukai Liu,Wei Zhang,Liran Wang,Ke Jin,Tao Sun,Congnan Liu,Chenchen Zhang,Hualei Zhu,Jiaheng Liu,Xianjie Wu,Ge Zhang,Tianyu Liu,Zhoujun Li*

Main category: cs.CL

TL;DR: 介绍多语言多模态软件开发人员MM - Coder，开发MMc - Instruct数据集和MMEval基准，评估指出模型在多模态代码生成方面仍有挑战，旨在革新工业编程。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型多为纯文本，忽略了软件开发中重要的可视化辅助工具，为弥补这一差距开展研究。

Method: 引入MM - Coder，将可视化设计输入与文本指令结合；开发MMc - Instruct数据集；引入MMEval基准进行评估。

Result: 使用MMEval评估发现模型在精确捕获视觉信息、遵循指令和掌握高级编程知识方面存在显著挑战。

Conclusion: 工作旨在让大语言模型能解释和实现通过文本和视觉设计传达的复杂规范，从而革新工业编程。

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
improved code generation, yet most models remain text-only, neglecting crucial
visual aids like diagrams and flowcharts used in real-world software
development. To bridge this gap, we introduce MM-Coder, a Multilingual
Multimodal software developer. MM-Coder integrates visual design inputs-Unified
Modeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with
textual instructions to enhance code generation accuracy and architectural
alignment. To enable this, we developed MMc-Instruct, a diverse multimodal
instruction-tuning dataset including visual-workflow-based code generation,
allowing MM-Coder to synthesize textual and graphical information like human
developers, distinct from prior work on narrow tasks. Furthermore, we introduce
MMEval, a new benchmark for evaluating multimodal code generation, addressing
existing text-only limitations. Our evaluations using MMEval highlight
significant remaining challenges for models in precise visual information
capture, instruction following, and advanced programming knowledge. Our work
aims to revolutionize industrial programming by enabling LLMs to interpret and
implement complex specifications conveyed through both text and visual designs.

</details>


### [213] [RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning](https://arxiv.org/abs/2507.08012)
*Atli Sigurgeirsson,Simon King*

Main category: cs.CL

TL;DR: 本文提出一种新颖微调机制，利用模型不可控方差解决基于提示的文本转语音模型受限与过于灵活的问题，经评估可提升模型可控性。


<details>
  <summary>Details</summary>
Motivation: 基于提示的文本转语音模型存在受限（控制限于训练时暴露的声学特征）和过于灵活（相同输入产生不可控变化）的问题，需解决。

Method: 通过对数千个合成样本进行主成分分析，确定占输出方差比例最高的潜在特征，并将其作为新标签进行二次微调。

Result: 在基于冰岛语语音语料训练的两个模型（一个有情感披露，一个无）上评估，对无情感披露的模型，该方法产生的连续和离散特征提升了模型整体可控性。

Conclusion: 所提出的新颖微调机制能有效解决基于提示的文本转语音模型现存问题，提升其可控性。

Abstract: A Prompt-based Text-To-Speech model allows a user to control different
aspects of speech, such as speaking rate and perceived gender, through natural
language instruction. Although user-friendly, such approaches are on one hand
constrained: control is limited to acoustic features exposed to the model
during training, and too flexible on the other: the same inputs yields
uncontrollable variation that are reflected in the corpus statistics.
  We investigate a novel fine-tuning regime to address both of these issues at
the same time by exploiting the uncontrollable variance of the model. Through
principal component analysis of thousands of synthesised samples, we determine
latent features that account for the highest proportion of the output variance
and incorporate them as new labels for secondary fine-tuning. We evaluate the
proposed methods on two models trained on an expressive Icelandic speech
corpus, one with emotional disclosure and one without. In the case of the model
without emotional disclosure, the method yields both continuous and discrete
features that improve overall controllability of the model.

</details>


### [214] [MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model](https://arxiv.org/abs/2507.08013)
*K. Sahit Reddy,N. Ragavenderan,Vasanth K.,Ganesh N. Naik,Vishalakshi Prabhu,Nagaraja G. S*

Main category: cs.CL

TL;DR: 提出MedicalBERT模型用于生物医学NLP任务，性能优于其他BERT模型，证明预训练模型和迁移学习在医学NLP的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理生物医学文献特定术语和双向理解任务时存在不足，需要新模型。

Method: 在大型生物医学数据集上训练MedicalBERT，优化和微调以处理多种任务，用F1分数等指标评估。

Result: MedicalBERT在多数基准测试中优于其他BERT模型，平均比通用BERT模型高5.67%。

Conclusion: 预训练BERT模型用于医学NLP任务有潜力，迁移学习技术能捕捉特定领域信息。

Abstract: Recent advances in natural language processing (NLP) have been driven
bypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel
at understanding complex texts, but biomedical literature, withits
domain-specific terminology, poses challenges that models likeWord2Vec and
bidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5,
despite capturing context, fall short in tasks needingbidirectional
understanding, unlike BERT. Addressing this, we proposedMedicalBERT, a
pretrained BERT model trained on a large biomedicaldataset and equipped with
domain-specific vocabulary that enhances thecomprehension of biomedical
terminology. MedicalBERT model is furtheroptimized and fine-tuned to address
diverse tasks, including named entityrecognition, relation extraction, question
answering, sentence similarity, anddocument classification. Performance metrics
such as the F1-score,accuracy, and Pearson correlation are employed to showcase
the efficiencyof our model in comparison to other BERT-based models such as
BioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost
of the benchmarks, and surpasses the general-purpose BERT model by5.67% on
average across all the tasks evaluated respectively. This work alsounderscores
the potential of leveraging pretrained BERT models for medicalNLP tasks,
demonstrating the effectiveness of transfer learning techniques incapturing
domain-specific information.
  (PDF) MedicalBERT: enhancing biomedical natural language processing using
pretrained BERT-based model. Available from:
https://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model
[accessed Jul 06 2025].

</details>


### [215] [Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking](https://arxiv.org/abs/2507.08014)
*Aldan Creo,Raul Castro Fernandez,Manuel Cebrian*

Main category: cs.CL

TL;DR: 对超200万真实对话进行越狱复杂性分析，发现越狱尝试不比正常对话复杂，挑战攻防军备竞赛说法，指出学术越狱披露有信息风险。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用，理解越狱策略复杂性和演变对AI安全至关重要。

Method: 对来自不同平台的超200万真实对话进行大规模实证分析，使用多种复杂性指标。

Result: 越狱尝试不比正常对话复杂，用户攻击毒性和复杂性随时间稳定，助手回复毒性降低，复杂性分布无幂律缩放。

Conclusion: LLM安全演变受人类创造力限制，防御措施在进步，学术越狱披露存在信息风险。

Abstract: As large language models (LLMs) become increasingly deployed, understanding
the complexity and evolution of jailbreaking strategies is critical for AI
safety.
  We present a mass-scale empirical analysis of jailbreak complexity across
over 2 million real-world conversations from diverse platforms, including
dedicated jailbreaking communities and general-purpose chatbots. Using a range
of complexity metrics spanning probabilistic measures, lexical diversity,
compression ratios, and cognitive load indicators, we find that jailbreak
attempts do not exhibit significantly higher complexity than normal
conversations. This pattern holds consistently across specialized jailbreaking
communities and general user populations, suggesting practical bounds on attack
sophistication. Temporal analysis reveals that while user attack toxicity and
complexity remains stable over time, assistant response toxicity has decreased,
indicating improving safety mechanisms. The absence of power-law scaling in
complexity distributions further points to natural limits on jailbreak
development.
  Our findings challenge the prevailing narrative of an escalating arms race
between attackers and defenders, instead suggesting that LLM safety evolution
is bounded by human ingenuity constraints while defensive measures continue
advancing. Our results highlight critical information hazards in academic
jailbreak disclosure, as sophisticated attacks exceeding current complexity
baselines could disrupt the observed equilibrium and enable widespread harm
before defensive adaptation.

</details>


### [216] [Mechanistic Indicators of Understanding in Large Language Models](https://arxiv.org/abs/2507.08017)
*Pierre Beckmann,Matthieu Queloz*

Main category: cs.CL

TL;DR: 本文对机械可解释性（MI）领域关于大语言模型（LLMs）的新发现进行综合介绍，并提出新理论框架和三层级机器理解概念，最后探讨并行机制现象。


<details>
  <summary>Details</summary>
Motivation: 综合MI领域关于LLMs的新发现，提出思考机器理解的新理论框架。

Method: 提出三层级的机器理解概念，包括概念理解、世界状态理解和原则性理解。

Result: LLMs发展出的内部结构在功能上类似于能看到联系的理解，但认知架构与人类不同。

Conclusion: 关于LLMs的辩论应从是否理解转向其奇特思维如何工作。

Abstract: Recent findings in mechanistic interpretability (MI), the field probing the
inner workings of Large Language Models (LLMs), challenge the view that these
models rely solely on superficial statistics. Here, we offer an accessible
synthesis of these findings that doubles as an introduction to MI, all while
integrating these findings within a novel theoretical framework for thinking
about machine understanding. We argue that LLMs develop internal structures
that are functionally analogous to the kind of understanding that consists in
seeing connections. To sharpen this idea, we propose a three-tiered conception
of machine understanding. First, conceptual understanding emerges when a model
forms "features" as directions in latent space, thereby learning the
connections between diverse manifestations of something. Second,
state-of-the-world understanding emerges when a model learns contingent factual
connections between features and dynamically tracks changes in the world.
Third, principled understanding emerges when a model ceases to rely on a
collection of memorized facts and discovers a "circuit" that connects these
facts. However, we conclude by exploring the "parallel mechanisms" phenomenon,
arguing that while LLMs exhibit forms of understanding, their cognitive
architecture remains different from ours, and the debate should shift from
whether LLMs understand to how their strange minds work.

</details>


### [217] [Circumventing Safety Alignment in Large Language Models Through Embedding Space Toxicity Attenuation](https://arxiv.org/abs/2507.08020)
*Zhibo Zhang,Yuxi Li,Kailong Wang,Shuai Yuan,Ling Shi,Haoyu Wang*

Main category: cs.CL

TL;DR: 论文提出ETTA框架应对大语言模型嵌入空间中毒安全风险，评估显示攻击成功率高，凸显当前对齐策略漏洞。


<details>
  <summary>Details</summary>
Motivation: 大语言模型开放性带来安全风险，尤其嵌入空间中毒，此前对嵌入层安全对齐动态理解不足，针对性对抗扰动技术研究不充分。

Method: 提出ETTA框架，通过线性变换识别和减弱嵌入空间中毒敏感维度，无需模型微调或访问训练数据。

Result: 在五个代表性开源大语言模型上平均攻击成功率达88.61%，优于最佳基线11.34%，能泛化到安全增强模型。

Conclusion: 当前对齐策略存在关键漏洞，需嵌入感知防御。

Abstract: Large Language Models (LLMs) have achieved remarkable success across domains
such as healthcare, education, and cybersecurity. However, this openness also
introduces significant security risks, particularly through embedding space
poisoning, which is a subtle attack vector where adversaries manipulate the
internal semantic representations of input data to bypass safety alignment
mechanisms. While previous research has investigated universal perturbation
methods, the dynamics of LLM safety alignment at the embedding level remain
insufficiently understood. Consequently, more targeted and accurate adversarial
perturbation techniques, which pose significant threats, have not been
adequately studied.
  In this work, we propose ETTA (Embedding Transformation Toxicity
Attenuation), a novel framework that identifies and attenuates
toxicity-sensitive dimensions in embedding space via linear transformations.
ETTA bypasses model refusal behaviors while preserving linguistic coherence,
without requiring model fine-tuning or access to training data. Evaluated on
five representative open-source LLMs using the AdvBench benchmark, ETTA
achieves a high average attack success rate of 88.61%, outperforming the best
baseline by 11.34%, and generalizes to safety-enhanced models (e.g., 77.39% ASR
on instruction-tuned defenses). These results highlight a critical
vulnerability in current alignment strategies and underscore the need for
embedding-aware defenses.

</details>


### [218] [Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis](https://arxiv.org/abs/2507.08021)
*Li Li,Yongliang Wu,Jingze Zhu,Jiawei Peng,Jianfei Cai,Xu Yang*

Main category: cs.CL

TL;DR: 本文对图像字幕任务中的多模态上下文学习进行内外部综合研究，揭示示例配置策略对模型性能的影响及模型特征模式，方法和指标可用于更广泛研究。


<details>
  <summary>Details</summary>
Motivation: 多模态上下文学习的示范配置探索尚浅，上下文示例的可控性可用于观察分析大模型推理特征。

Method: 外部从样本数量、图像检索和字幕分配三个维度探索示范配置策略并评估；内部分析典型大模型注意力特征，开发基于注意力的指标，进行辅助实验，比较相同设计和预训练策略的大模型性能差异并从预训练数据特征角度解释。

Result: 揭示了上下文示例配置策略对模型性能的影响，以及模型的典型特征模式。

Conclusion: 结合内外部分析研究大模型的方法和新提出的指标可应用于更广泛研究领域。

Abstract: The evolution of large models has witnessed the emergence of In-Context
Learning (ICL) capabilities. In Natural Language Processing (NLP), numerous
studies have demonstrated the effectiveness of ICL. Inspired by the success of
Large Language Models (LLMs), researchers have developed Large Multimodal
Models (LMMs) with ICL capabilities. However, explorations of demonstration
configuration for multimodal ICL remain preliminary. Additionally, the
controllability of In-Context Examples (ICEs) provides an efficient and
cost-effective means to observe and analyze the inference characteristics of
LMMs under varying inputs. This paper conducts a comprehensive external and
internal investigation of multimodal in-context learning on the image
captioning task. Externally, we explore demonstration configuration strategies
through three dimensions: shot number, image retrieval, and caption assignment.
We employ multiple metrics to systematically and thoroughly evaluate and
summarize key findings. Internally, we analyze typical LMM attention
characteristics and develop attention-based metrics to quantify model
behaviors. We also conduct auxiliary experiments to explore the feasibility of
attention-driven model acceleration and compression. We further compare
performance variations between LMMs with identical model design and pretraining
strategies and explain the differences from the angles of pre-training data
features. Our study reveals both how ICEs configuration strategies impact model
performance through external experiments and characteristic typical patterns
through internal inspection, providing dual perspectives for understanding
multimodal ICL in LMMs. Our method of combining external and internal analysis
to investigate large models, along with our newly proposed metrics, can be
applied to broader research areas.

</details>


### [219] [Integrating External Tools with Large Language Models to Improve Accuracy](https://arxiv.org/abs/2507.08034)
*Nripesh Niketan,Hadj Batatia*

Main category: cs.CL

TL;DR: 提出框架集成外部工具提升大语言模型在教育场景查询能力，评估显示显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 无相关上下文信息时大语言模型回复质量差、易幻觉，现有倡议将其与外部工具集成以提高准确性，本文聚焦教育场景。

Method: 开发允许访问外部 API 获取额外信息的框架，集成工具提供计算能力。

Result: 使用 MMLU 数据集评估，Athena 框架在数学和科学推理准确率分别达 83%和 88%，远超对比模型。

Conclusion: 结果为围绕大语言模型创建复杂计算生态系统、支持多样任务活动奠定基础。

Abstract: This paper deals with improving querying large language models (LLMs). It is
well-known that without relevant contextual information, LLMs can provide poor
quality responses or tend to hallucinate. Several initiatives have proposed
integrating LLMs with external tools to provide them with up-to-date data to
improve accuracy. In this paper, we propose a framework to integrate external
tools to enhance the capabilities of LLMs in answering queries in educational
settings. Precisely, we develop a framework that allows accessing external APIs
to request additional relevant information. Integrated tools can also provide
computational capabilities such as calculators or calendars. The proposed
framework has been evaluated using datasets from the Multi-Modal Language
Understanding (MMLU) collection. The data consists of questions on mathematical
and scientific reasoning. Results compared to state-of-the-art language models
show that the proposed approach significantly improves performance. Our Athena
framework achieves 83% accuracy in mathematical reasoning and 88% in scientific
reasoning, substantially outperforming all tested models including GPT-4o,
LLaMA-Large, Mistral-Large, Phi-Large, and GPT-3.5, with the best baseline
model (LLaMA-Large) achieving only 67% and 79% respectively. These promising
results open the way to creating complex computing ecosystems around LLMs to
make their use more natural to support various tasks and activities.

</details>


### [220] [CRISP: Complex Reasoning with Interpretable Step-based Plans](https://arxiv.org/abs/2507.08037)
*Matan Vetzler,Koren Lazar,Guy Uziel,Eran Hirsch,Ateret Anaby-Tavor,Leshem Choshen*

Main category: cs.CL

TL;DR: 本文介绍多领域数据集CRISP，用其微调小模型可生成高质量计划，超越少样本提示大模型和思维链推理，且有跨领域泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理能力不足，思维链推理有局限，现有显式高层计划生成方法依赖少样本提示，需改进。

Method: 引入多领域数据集CRISP，计划自动生成并严格验证，在CRISP上微调小模型。

Result: 微调小模型能生成比少样本提示大模型更高质量计划，显著超越思维链推理，跨领域评估显示有泛化性。

Conclusion: CRISP可提升模型复杂推理能力，学习到的规划能力有泛化性。

Abstract: Recent advancements in large language models (LLMs) underscore the need for
stronger reasoning capabilities to solve complex problems effectively. While
Chain-of-Thought (CoT) reasoning has been a step forward, it remains
insufficient for many domains. A promising alternative is explicit high-level
plan generation, but existing approaches largely assume that LLMs can produce
effective plans through few-shot prompting alone, without additional training.
In this work, we challenge this assumption and introduce CRISP (Complex
Reasoning with Interpretable Step-based Plans), a multi-domain dataset of
high-level plans for mathematical reasoning and code generation. The plans in
CRISP are automatically generated and rigorously validated--both intrinsically,
using an LLM as a judge, and extrinsically, by evaluating their impact on
downstream task performance. We demonstrate that fine-tuning a small model on
CRISP enables it to generate higher-quality plans than much larger models using
few-shot prompting, while significantly outperforming Chain-of-Thought
reasoning. Furthermore, our out-of-domain evaluation reveals that fine-tuning
on one domain improves plan generation in the other, highlighting the
generalizability of learned planning capabilities.

</details>


### [221] [AblationBench: Evaluating Automated Planning of Ablations in Empirical AI Research](https://arxiv.org/abs/2507.08038)
*Talor Abramovich,Gal Chechik*

Main category: cs.CL

TL;DR: 介绍用于评估经验性AI研究中代理消融规划任务的基准套件AblationBench，实验表明任务有挑战，链式思维提示效果更好。


<details>
  <summary>Details</summary>
Motivation: 经验性AI研究中消融实验设计是关键部分，需要评估代理在消融规划任务上的表现。

Method: 引入AblationBench基准套件，包含AuthorAblation和ReviewerAblation两个任务，开发基于大语言模型的评判器作为自动评估框架。

Result: 前沿大语言模型完成任务仍具挑战性，表现最佳的系统平均仅能识别29%的原始消融。

Conclusion: 当前大语言模型在这些任务上有局限，链式思维提示优于现有基于代理的方法。

Abstract: Autonomous agents built on language models (LMs) are showing increasing
popularity in many fields, including scientific research. AI co-scientists aim
to support or automate parts of the research process using these agents. A key
component of empirical AI research is the design of ablation experiments. To
this end, we introduce AblationBench, a benchmark suite for evaluating agents
on ablation planning tasks in empirical AI research. It includes two tasks:
AuthorAblation, which helps authors propose ablation experiments based on a
method section and contains 83 instances, and ReviewerAblation, which helps
reviewers find missing ablations in a full paper and contains 350 instances.
For both tasks, we develop LM-based judges that serve as an automatic
evaluation framework. Our experiments with frontier LMs show that these tasks
remain challenging, with the best-performing LM system identifying only 29% of
the original ablations on average. Lastly, we analyze the limitations of
current LMs on these tasks, and find that chain-of-thought prompting
outperforms the currently existing agent-based approach.

</details>


### [222] [Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing](https://arxiv.org/abs/2507.08045)
*Junyi Wen,Junyuan Liang,Zicong Hong,Wuhui Chen,Zibin Zheng*

Main category: cs.CL

TL;DR: 提出Krul多轮大语言模型推理系统解决KV缓存恢复问题，比现有方法更高效。


<details>
  <summary>Details</summary>
Motivation: 现有多轮对话中KV缓存恢复方法采用固定压缩方案，未考虑对话注意力动态，导致准确性下降。

Method: Krul动态选择压缩策略，采用重计算 - 加载管道恢复KV缓存，有预压缩策略选择器、标记级异构注意力相似度估计器和无气泡恢复调度器三个创新。

Result: 在实际任务中，Krul比现有方法在首词生成时间上减少1.5 - 2.68倍，KV缓存存储减少1.33 - 2.35倍，且不影响生成质量。

Conclusion: Krul能准确高效实现KV缓存恢复，优于现有方法。

Abstract: Efficient state restoration in multi-turn conversations with large language
models (LLMs) remains a critical challenge, primarily due to the overhead of
recomputing or loading full key-value (KV) caches for all historical tokens. To
address this, existing approaches compress KV caches across adjacent layers
with highly similar attention patterns. However, these methods often apply a
fixed compression scheme across all conversations, selecting the same layer
pairs for compression without considering conversation-specific attention
dynamics. This static strategy overlooks variability in attention pattern
similarity across different conversations, which can lead to noticeable
accuracy degradation.
  We present Krul, a multi-turn LLM inference system that enables accurate and
efficient KV cache restoration. Krul dynamically selects compression strategies
based on attention similarity across layer pairs and uses a
recomputation-loading pipeline to restore the KV cache. It introduces three key
innovations: 1) a preemptive compression strategy selector to preserve critical
context for future conversation turns and selects a customized strategy for the
conversation; 2) a token-wise heterogeneous attention similarity estimator to
mitigate the attention similarity computation and storage overhead during model
generation; 3) a bubble-free restoration scheduler to reduce potential bubbles
brought by the imbalance of recomputing and loading stream due to compressed KV
caches. Empirical evaluations on real-world tasks demonstrate that Krul
achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x
reduction in KV cache storage compared to state-of-the-art methods without
compromising generation quality.

</details>


### [223] [Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores](https://arxiv.org/abs/2507.08143)
*Vivek Chari,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 提出无参数、查询无关的KV压缩策略Compactor，可在保留一半令牌时达到与竞争方法相同性能，还引入上下文校准压缩程序，能降低KV内存负担。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型支持长上下文窗口时，KV缓存内存需求大成为实际部署资源瓶颈，限制吞吐量并增加服务成本。

Method: 提出Compactor策略，用近似杠杆分数确定令牌重要性，引入上下文校准压缩程序。

Result: Compactor在合成和真实上下文任务中保留一半令牌时性能与竞争方法相同，在Longbench上降低63%的KV内存负担，应用于27个任务有良好效果。

Conclusion: Compactor策略有效且具有通用性。

Abstract: Modern Large Language Models (LLMs) are increasingly trained to support very
large context windows. Unfortunately the ability to use long contexts in
generation is complicated by the large memory requirement of the KV cache,
which scales linearly with the context length. This memory footprint is often
the dominant resource bottleneck in real-world deployments, limiting throughput
and increasing serving cost. One way to address this is by compressing the KV
cache, which can be done either with knowledge of the question being asked
(query-aware) or without knowledge of the query (query-agnostic). We present
Compactor, a parameter-free, query-agnostic KV compression strategy that uses
approximate leverage scores to determine token importance. We show that
Compactor can achieve the same performance as competing methods while retaining
1/2 the tokens in both synthetic and real-world context tasks, with minimal
computational overhead. We further introduce a procedure for context-calibrated
compression, which allows one to infer the maximum compression ratio a given
context can support. Using context-calibrated compression, we show that
Compactor achieves full KV performance on Longbench while reducing the KV
memory burden by 63%, on average. To demonstrate the efficacy and
generalizability of our approach, we apply Compactor to 27 synthetic and
real-world tasks from RULER and Longbench, with models from both the Qwen 2.5
and Llama 3.1 families.

</details>


### [224] [Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?](https://arxiv.org/abs/2507.08232)
*KV Aditya Srivatsa,Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 研究用大语言模型作为智能辅导系统代理学生的准确性，发现无引导时强模型超学生，提示后表现因模型和提示而异，需新策略并给出代理选择指南。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型作为代理学生在多大程度上能准确模拟真实学生的行为和特征。

Method: 收集NAEP的489个项目数据集，应用项目反应理论模型将11个大语言模型与真实学生置于同一能力尺度。

Result: 无引导时强通用模型超各年级学生平均水平，弱或领域不匹配模型可能偶然契合；使用年级强制提示后，模型表现因模型和提示而异，无模型 - 提示组合能跨学科和年级符合要求。

Conclusion: 需要新的训练和评估策略，并给出选择可行代理的指南。

Abstract: Large Language Models (LLMs) are increasingly used as proxy students in the
development of Intelligent Tutoring Systems (ITSs) and in piloting test
questions. However, to what extent these proxy students accurately emulate the
behavior and characteristics of real students remains an open question. To
investigate this, we collected a dataset of 489 items from the National
Assessment of Educational Progress (NAEP), covering mathematics and reading
comprehension in grades 4, 8, and 12. We then apply an Item Response Theory
(IRT) model to position 11 diverse and state-of-the-art LLMs on the same
ability scale as real student populations. Our findings reveal that, without
guidance, strong general-purpose models consistently outperform the average
student at every grade, while weaker or domain-mismatched models may align
incidentally. Using grade-enforcement prompts changes models' performance, but
whether they align with the average grade-level student remains highly model-
and prompt-specific: no evaluated model-prompt pair fits the bill across
subjects and grades, underscoring the need for new training and evaluation
strategies. We conclude by providing guidelines for the selection of viable
proxies based on our findings.

</details>


### [225] [Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency](https://arxiv.org/abs/2507.08309)
*Yupu Liang,Yaping Zhang,Zhiyang Zhang,Zhiyuan Chen,Yang Zhao,Lu Xiang,Chengqing Zong,Yu Zhou*

Main category: cs.CL

TL;DR: 本文提出同步自我审查（SSR）微调范式，解决多模态大语言模型在文档图像机器翻译任务中遗忘单语能力问题，提升模型在OCR和DIMT任务上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在文档图像机器翻译任务中面临跨模态和跨语言挑战，且监督微调会导致模型遗忘现有单语能力。

Method: 引入同步自我审查（SSR）微调范式，在生成翻译文本前让模型生成OCR文本。

Result: 综合实验表明，提出的SSR学习有助于减轻灾难性遗忘。

Conclusion: SSR学习提升了多模态大语言模型在OCR和DIMT任务上的泛化能力。

Abstract: Multimodal Large Language Models (MLLMs) have shown strong performance in
document image tasks, especially Optical Character Recognition (OCR). However,
they struggle with Document Image Machine Translation (DIMT), which requires
handling both cross-modal and cross-lingual challenges. Previous efforts to
enhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT
dataset often result in the forgetting of the model's existing monolingual
abilities, such as OCR. To address these challenges, we introduce a novel
fine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR
proficiency, inspired by the concept "Bilingual Cognitive Advantage".
Specifically, SSR prompts the model to generate OCR text before producing
translation text, which allows the model to leverage its strong monolingual OCR
ability while learning to translate text across languages. Comprehensive
experiments demonstrate the proposed SSR learning helps mitigate catastrophic
forgetting, improving the generalization ability of MLLMs on both OCR and DIMT
tasks.

</details>


### [226] [Assessing the Capabilities and Limitations of FinGPT Model in Financial NLP Applications](https://arxiv.org/abs/2507.08015)
*Prudence Djagba,Chimezie A. Odinakachukwu*

Main category: cs.CL

TL;DR: 评估金融领域特定语言模型FinGPT在六项NLP任务中的表现，发现其在部分任务表现强，但在推理和生成任务表现差，并非全面解决方案。


<details>
  <summary>Details</summary>
Motivation: 评估FinGPT在真实金融应用中的能力和局限性。

Method: 使用金融特定数据集，在六项NLP任务中评估FinGPT。

Result: FinGPT在分类任务表现强，可与GPT - 4媲美，但在推理和生成任务表现显著较差，与GPT - 4和人类基准有差距。

Conclusion: FinGPT对某些结构化金融任务有效，但并非全面解决方案，需对金融语言模型进行架构改进和特定领域优化。

Abstract: This work evaluates FinGPT, a financial domain-specific language model,
across six key natural language processing (NLP) tasks: Sentiment Analysis,
Text Classification, Named Entity Recognition, Financial Question Answering,
Text Summarization, and Stock Movement Prediction. The evaluation uses
finance-specific datasets to assess FinGPT's capabilities and limitations in
real-world financial applications. The results show that FinGPT performs
strongly in classification tasks such as sentiment analysis and headline
categorization, often achieving results comparable to GPT-4. However, its
performance is significantly lower in tasks that involve reasoning and
generation, such as financial question answering and summarization. Comparisons
with GPT-4 and human benchmarks highlight notable performance gaps,
particularly in numerical accuracy and complex reasoning. Overall, the findings
indicate that while FinGPT is effective for certain structured financial tasks,
it is not yet a comprehensive solution. This research provides a useful
benchmark for future research and underscores the need for architectural
improvements and domain-specific optimization in financial language models.

</details>


### [227] [Review, Remask, Refine (R3): Process-Guided Block Diffusion for Text Generation](https://arxiv.org/abs/2507.08018)
*Nikita Mounier,Parsa Idehpour*

Main category: cs.CL

TL;DR: 提出R3框架用于迭代文本生成，无需额外训练，可提升预训练掩码文本扩散模型输出质量。


<details>
  <summary>Details</summary>
Motivation: 解决迭代文本生成中模型有效识别和纠正自身错误的挑战。

Method: 提出R3框架，利用过程奖励模型（PRM）评估中间生成块，根据PRM分数进行重新掩码，促使模型改进目标片段。

Result: 模型能更集中精力改进前代生成中的次优部分，提升最终输出质量。

Conclusion: R3框架简单有效，无需额外训练，可应用于多种预训练掩码文本扩散模型以提升文本生成效果。

Abstract: A key challenge for iterative text generation is enabling models to
efficiently identify and correct their own errors. We propose Review, Remask,
Refine (R3), a relatively simple yet elegant framework that requires no
additional model training and can be applied to any pre-trained masked text
diffusion model (e.g., LLaDA or BD3-LM). In R3, a Process Reward Model (PRM) is
utilized for the Review of intermediate generated blocks. The framework then
translates these PRM scores into a Remask strategy: the lower a block's PRM
score, indicating potential mistakes, the greater the proportion of tokens
within that block are remasked. Finally, the model is compelled to Refine these
targeted segments, focusing its efforts more intensively on specific
sub-optimal parts of past generations, leading to improved final output.

</details>


### [228] [ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains](https://arxiv.org/abs/2507.08427)
*Zilu Dong,Xiangqing Shen,Zinong Yang,Rui Xia*

Main category: cs.CL

TL;DR: 提出ChainEdit框架解决大语言模型知识编辑时逻辑一致性问题，实验效果好并解决评估偏差。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型知识编辑方法在传播涟漪效应到关联事实时难以保持逻辑一致性。

Method: 提出ChainEdit框架，将知识图谱逻辑规则与大语言模型逻辑推理能力结合，自动提取逻辑模式并与模型内部逻辑对齐，动态生成和编辑逻辑相连知识簇。

Result: 实验显示逻辑泛化比基线提升超30%，保留编辑可靠性和特异性，通过知识感知协议解决现有基准评估偏差。

Conclusion: 该工作在知识编辑后确保内部逻辑一致性的同时，在涟漪效应处理上达到新的最优性能。

Abstract: Current knowledge editing methods for large language models (LLMs) struggle
to maintain logical consistency when propagating ripple effects to associated
facts. We propose ChainEdit, a framework that synergizes knowledge
graph-derived logical rules with LLM logical reasoning capabilities to enable
systematic chain updates. By automatically extracting logical patterns from
structured knowledge bases and aligning them with LLMs' internal logics,
ChainEdit dynamically generates and edits logically connected knowledge
clusters. Experiments demonstrate an improvement of more than 30% in logical
generalization over baselines while preserving editing reliability and
specificity. We further address evaluation biases in existing benchmarks
through knowledge-aware protocols that disentangle external dependencies. This
work establishes new state-of-the-art performance on ripple effect while
ensuring internal logical consistency after knowledge editing.

</details>


### [229] [Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences](https://arxiv.org/abs/2507.08440)
*Selina Heller,Mohamed Ibrahim,David Antony Selby,Sebastian Vollmer*

Main category: cs.CL

TL;DR: 本文提出基于大语言模型的多智能体系统模拟决策会议，评估六种模型，结果表明大语言模型可检测共识，系统有助于模拟群体决策。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型模拟现实场景的能力，设计系统模拟决策会议并检测参与者共识。

Method: 评估六种大语言模型在立场检测和立场极性检测两个任务上的表现，并在多智能体系统中进一步评估其在复杂模拟中的有效性。

Result: 大语言模型能在动态和细微辩论中可靠检测共识，系统中加入共识检测智能体可提高群体辩论效率和审议质量。

Conclusion: 基于大语言模型的多智能体系统有模拟群体决策过程的潜力，可用于各领域专家咨询研讨会支持决策。

Abstract: Decision conferences are structured, collaborative meetings that bring
together experts from various fields to address complex issues and reach a
consensus on recommendations for future actions or policies. These conferences
often rely on facilitated discussions to ensure productive dialogue and
collective agreement. Recently, Large Language Models (LLMs) have shown
significant promise in simulating real-world scenarios, particularly through
collaborative multi-agent systems that mimic group interactions. In this work,
we present a novel LLM-based multi-agent system designed to simulate decision
conferences, specifically focusing on detecting agreement among the participant
agents. To achieve this, we evaluate six distinct LLMs on two tasks: stance
detection, which identifies the position an agent takes on a given issue, and
stance polarity detection, which identifies the sentiment as positive,
negative, or neutral. These models are further assessed within the multi-agent
system to determine their effectiveness in complex simulations. Our results
indicate that LLMs can reliably detect agreement even in dynamic and nuanced
debates. Incorporating an agreement-detection agent within the system can also
improve the efficiency of group debates and enhance the overall quality and
coherence of deliberations, making them comparable to real-world decision
conferences regarding outcome and decision-making. These findings demonstrate
the potential for LLM-based multi-agent systems to simulate group
decision-making processes. They also highlight that such systems could be
instrumental in supporting decision-making with expert elicitation workshops
across various domains.

</details>


### [230] [Enhancing Essay Cohesion Assessment: A Novel Item Response Theory Approach](https://arxiv.org/abs/2507.08487)
*Bruno Alexandre Rosa,Hilário Oliveira,Luiz Rodrigues,Eduardo Araujo Oliveira,Rafael Ferreira Mello*

Main category: cs.CL

TL;DR: 本文提出基于项目反应理论的衔接分数预测方法调整机器学习模型生成的分数，实验表明该方法在多个评估指标上优于传统模型和集成方法，为改进教育论文衔接自动评估提供了潜在途径。


<details>
  <summary>Details</summary>
Motivation: 自动评分论文衔接在教育人工智能领域是挑战，现有机器学习算法评估文本时一般不考虑语料实例个体特征，因此想利用项目反应理论来改进。

Method: 提出基于项目反应理论的衔接分数预测方法，选择扩展的Essay - BR和巴西葡萄牙语叙事论文语料，提取325个语言特征，将问题视为机器学习回归任务。

Result: 提出的方法在多个评估指标上优于传统机器学习模型和集成方法。

Conclusion: 该研究探索了一种改进教育论文衔接自动评估的潜在方法。

Abstract: Essays are considered a valuable mechanism for evaluating learning outcomes
in writing. Textual cohesion is an essential characteristic of a text, as it
facilitates the establishment of meaning between its parts. Automatically
scoring cohesion in essays presents a challenge in the field of educational
artificial intelligence. The machine learning algorithms used to evaluate texts
generally do not consider the individual characteristics of the instances that
comprise the analysed corpus. In this meaning, item response theory can be
adapted to the context of machine learning, characterising the ability,
difficulty and discrimination of the models used. This work proposes and
analyses the performance of a cohesion score prediction approach based on item
response theory to adjust the scores generated by machine learning models. In
this study, the corpus selected for the experiments consisted of the extended
Essay-BR, which includes 6,563 essays in the style of the National High School
Exam (ENEM), and the Brazilian Portuguese Narrative Essays, comprising 1,235
essays written by 5th to 9th grade students from public schools. We extracted
325 linguistic features and treated the problem as a machine learning
regression task. The experimental results indicate that the proposed approach
outperforms conventional machine learning models and ensemble methods in
several evaluation metrics. This research explores a potential approach for
improving the automatic evaluation of cohesion in educational essays.

</details>


### [231] [PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts](https://arxiv.org/abs/2507.08499)
*Ziyi Huang,Xia Cui*

Main category: cs.CL

TL;DR: 本文提出多语言短文本多标签情感检测系统，评估关键组件，显示不同方法在不同语言的效果及PCA优势，框架可解决语言多样性和资源限制问题。


<details>
  <summary>Details</summary>
Motivation: 解决多语言短文本多标签情感检测中语言多样性和资源限制的挑战。

Method: 提出以特征为中心的框架，评估文档表示、降维和模型训练三个关键组件。

Result: TF - IDF对低资源语言有效，FastText和Sentence - BERT有特定语言优势，PCA减少训练时间且不影响性能，存在模型复杂度和处理成本的权衡。

Conclusion: 框架为多语言情感检测提供可扩展解决方案。

Abstract: This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in
Text-Based Emotion Detection (Track A), which focuses on multi-label emotion
detection in short texts. We propose a feature-centric framework that
dynamically adapts document representations and learning algorithms to optimize
language-specific performance. Our study evaluates three key components:
document representation, dimensionality reduction, and model training in 28
languages, highlighting five for detailed analysis. The results show that
TF-IDF remains highly effective for low-resource languages, while contextual
embeddings like FastText and transformer-based document representations, such
as those produced by Sentence-BERT, exhibit language-specific strengths.
Principal Component Analysis (PCA) reduces training time without compromising
performance, particularly benefiting FastText and neural models such as
Multi-Layer Perceptrons (MLP). Computational efficiency analysis underscores
the trade-off between model complexity and processing cost. Our framework
provides a scalable solution for multilingual emotion detection, addressing the
challenges of linguistic diversity and resource constraints.

</details>


### [232] [Simple Mechanistic Explanations for Out-Of-Context Reasoning](https://arxiv.org/abs/2507.08218)
*Atticus Wang,Joshua Engels,Oliver Clive-Griffin*

Main category: cs.CL

TL;DR: 研究发现微调大语言模型时LoRA微调本质是添加常量转向向量导致上下文外推理现象，还可直接从头训练转向向量实现该现象。


<details>
  <summary>Details</summary>
Motivation: 探究微调大语言模型时出现的上下文外推理（OOCR）现象的机理。

Method: 对文献中OOCR实例进行分析，发现LoRA微调的作用，且尝试直接从头训练转向向量。

Result: LoRA微调本质是添加常量转向向量使模型性能提升并在相关领域泛化，直接训练转向向量也能诱导OOCR，在模型后门任务中无条件添加转向向量也有效。

Conclusion: 给出了OOCR任务微调时学习内容的一种解释，有助于理解大语言模型上下文外推理能力的成因。

Abstract: Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs
exhibit surprisingly deep out-of-distribution generalization. Rather than
learning shallow heuristics, they implicitly internalize and act on the
consequences of observations scattered throughout the fine-tuning data. In this
work, we investigate this phenomenon mechanistically and find that many
instances of OOCR in the literature have a simple explanation: the LoRA
fine-tuning essentially adds a constant steering vector, steering the model
towards a general concept. This improves performance on the fine-tuning task
and in many other concept-related domains, causing the surprising
generalization. Moreover, we can directly train steering vectors for these
tasks from scratch, which also induces OOCR. We find that our results hold even
for a task that seems like it must involve conditional behavior (model
backdoors); it turns out that unconditionally adding a steering vector is
sufficient. Overall, our work presents one explanation of what gets learned
during fine-tuning for OOCR tasks, contributing to the key question of why LLMs
can reason out of context, an advanced capability that is highly relevant to
their safe and reliable deployment.

</details>


### [233] [Exploring Gender Differences in Chronic Pain Discussions on Reddit](https://arxiv.org/abs/2507.08241)
*Ancita Maria Andrade,Tanvi Banerjee,Ramakrishna Mundugar*

Main category: cs.CL

TL;DR: 该研究用NLP分析疼痛经历的性别差异，通过HAM - CNN分类帖子，发现男女语言表达和疾病患病情况有差异。


<details>
  <summary>Details</summary>
Motivation: 以往研究常忽略性别在疼痛体验中的作用，故开展研究深入了解个体疼痛体验的性别差异。

Method: 利用自然语言处理（NLP），通过隐藏属性模型 - 卷积神经网络（HAM - CNN）基于用户名聚合帖子，将帖子分为男性和女性语料库。

Result: 分类F1分数达0.86，发现女性帖子更注重情感表达，偏头痛和鼻窦炎等疾病在女性中更普遍，且止痛药对不同性别影响不同。

Conclusion: 个体疼痛体验存在性别差异，包括语言表达和疾病患病情况等方面。

Abstract: Pain is an inherent part of human existence, manifesting as both physical and
emotional experiences, and can be categorized as either acute or chronic. Over
the years, extensive research has been conducted to understand the causes of
pain and explore potential treatments, with contributions from various
scientific disciplines. However, earlier studies often overlooked the role of
gender in pain experiences. In this study, we utilized Natural Language
Processing (NLP) to analyze and gain deeper insights into individuals' pain
experiences, with a particular focus on gender differences. We successfully
classified posts into male and female corpora using the Hidden Attribute
Model-Convolutional Neural Network (HAM-CNN), achieving an F1 score of 0.86 by
aggregating posts based on usernames. Our analysis revealed linguistic
differences between genders, with female posts tending to be more emotionally
focused. Additionally, the study highlighted that conditions such as migraine
and sinusitis are more prevalent among females and explored how pain medication
affects individuals differently based on gender.

</details>


### [234] [A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1](https://arxiv.org/abs/2507.08621)
*Marcin Pietroń,Rafał Olszowski,Jakub Gomułka,Filip Gampel,Andrzej Tomski*

Main category: cs.CL

TL;DR: 本文研究多种大语言模型在公开论点分类数据库中的表现，发现ChatGPT - 4o和Deepseek - R1较优，但都有错误，还指出已知提示算法的弱点和改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏大语言模型在公开论点分类数据库中表现的研究和结果。

Method: 选取多种大语言模型（如GPT、Llama、DeepSeek及其推理增强变体），使用Args.me和UKP等数据集进行测试。

Result: ChatGPT - 4o在论点分类基准测试中表现最佳，含推理能力的Deepseek - R1表现优越，但所有模型都会出错。

Conclusion: 本研究是首次用大语言模型和提示算法对相关数据集进行更广泛分析，指出已知提示算法弱点并给出改进方向，还深入分析了论点数据集的缺点。

Abstract: Argument mining (AM) is an interdisciplinary research field that integrates
insights from logic, philosophy, linguistics, rhetoric, law, psychology, and
computer science. It involves the automatic identification and extraction of
argumentative components, such as premises and claims, and the detection of
relationships between them, such as support, attack, or neutrality. Recently,
the field has advanced significantly, especially with the advent of large
language models (LLMs), which have enhanced the efficiency of analyzing and
extracting argument semantics compared to traditional methods and other deep
learning models. There are many benchmarks for testing and verifying the
quality of LLM, but there is still a lack of research and results on the
operation of these models in publicly available argument classification
databases. This paper presents a study of a selection of LLM's, using diverse
datasets such as Args.me and UKP. The models tested include versions of GPT,
Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the
Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms
the others in the argument classification benchmarks. In case of models
incorporated with reasoning capabilities, the Deepseek-R1 shows its
superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still
make errors. The most common errors are discussed for all models. To our
knowledge, the presented work is the first broader analysis of the mentioned
datasets using LLM and prompt algorithms. The work also shows some weaknesses
of known prompt algorithms in argument analysis, while indicating directions
for their improvement. The added value of the work is the in-depth analysis of
the available argument datasets and the demonstration of their shortcomings.

</details>


### [235] [KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment](https://arxiv.org/abs/2507.08665)
*Jiyao Zhang,Chengli Zhong,Hui Xu,Qige Li,Yi Zhou*

Main category: cs.CL

TL;DR: 提出神经符号框架KELPS解决多语言平行语料瓶颈问题，生成超60000问题的平行语料，在MiniF2F上句法准确率超SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型在将非形式数学形式化为机器可验证定理时，受限于多语言平行语料的数量和质量。

Method: 提出KELPS框架，先将自然语言转化为知识方程，再通过严格规则转换为目标语言。

Result: 生成了超60000问题的平行语料，在MiniF2F上句法准确率达88.9%，优于SOTA模型。

Conclusion: KELPS框架有效解决了多语言平行语料的瓶颈问题，提升了形式化数学的效果。

Abstract: Modern large language models (LLMs) show promising progress in formalizing
informal mathematics into machine-verifiable theorems. However, these methods
still face bottlenecks due to the limited quantity and quality of multilingual
parallel corpora. In this paper, we propose a novel neuro-symbolic framework
KELPS (Knowledge-Equation based Logical Processing System) to address these
problems. KELPS is an iterative framework for translating, synthesizing, and
filtering informal data into multiple formal languages (Lean, Coq, and
Isabelle). First, we translate natural language into Knowledge Equations (KEs),
a novel language that we designed, theoretically grounded in assertional logic.
Next, we convert them to target languages through rigorously defined rules that
preserve both syntactic structure and semantic meaning. This process yielded a
parallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic
accuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3
(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are
available in the supplementary materials.

</details>


### [236] [KG-Attention: Knowledge Graph-Guided Attention at Test-Time via Bidirectional Information Aggregation](https://arxiv.org/abs/2507.08704)
*Songlin Zhai,Guilin Qi,Yuan Meng*

Main category: cs.CL

TL;DR: 提出首个用于大语言模型的测试时知识图谱增强框架，含知识图谱引导注意力模块，无需参数更新实现实时知识融合，实验验证其性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱增强大语言模型方法依赖参数密集微调，有灾难性遗忘风险、降低泛化性，且对实时知识更新适应性有限。

Method: 引入测试时知识图谱增强框架，其知识图谱引导注意力模块通过外向和内向聚合两条协同路径增强自注意力机制，形成闭环增强机制。

Result: 在五个基准测试上的大量实验验证了KGA有可比的知识融合性能。

Conclusion: 所提方法可在测试时实现实时知识融合，无需修改参数，解决了现有方法的问题。

Abstract: Knowledge graphs (KGs) play a critical role in enhancing large language
models (LLMs) by introducing structured and grounded knowledge into the
learning process. However, most existing KG-enhanced approaches rely on
parameter-intensive fine-tuning, which risks catastrophic forgetting and
degrades the pretrained model's generalization. Moreover, they exhibit limited
adaptability to real-time knowledge updates due to their static integration
frameworks. To address these issues, we introduce the first test-time
KG-augmented framework for LLMs, built around a dedicated knowledge
graph-guided attention (KGA) module that enables dynamic knowledge fusion
without any parameter updates. The proposed KGA module augments the standard
self-attention mechanism with two synergistic pathways: outward and inward
aggregation. Specifically, the outward pathway dynamically integrates external
knowledge into input representations via input-driven KG fusion. This inward
aggregation complements the outward pathway by refining input representations
through KG-guided filtering, suppressing task-irrelevant signals and amplifying
knowledge-relevant patterns. Importantly, while the outward pathway handles
knowledge fusion, the inward path selects the most relevant triples and feeds
them back into the fusion process, forming a closed-loop enhancement mechanism.
By synergistically combining these two pathways, the proposed method supports
real-time knowledge fusion exclusively at test-time, without any parameter
modification. Extensive experiments on five benchmarks verify the comparable
knowledge fusion performance of KGA.

</details>


### [237] [The Impact of Automatic Speech Transcription on Speaker Attribution](https://arxiv.org/abs/2507.08660)
*Cristina Aggazzotti,Matthew Wiesner,Elizabeth Allyn Smith,Nicholas Andrews*

Main category: cs.CL

TL;DR: 本文首次全面研究自动转录对说话人归因性能的影响，发现说话人归因对词级转录错误有惊人的耐受性，基于ASR错误转录本的归因效果可能更好。


<details>
  <summary>Details</summary>
Motivation: 以往研究主要关注使用人工标注转录本进行说话人归因的可行性，而现实中常只有自动语音识别（ASR）系统产生的错误更多的转录本，因此研究自动转录对说话人归因性能的影响。

Method: 全面研究自动转录对说话人归因性能的影响，研究转录错误下归因性能的下降程度以及ASR系统属性对归因的影响。

Result: 说话人归因对词级转录错误有惊人的耐受性，恢复真实转录本的目标与归因性能的相关性极小，基于ASR错误转录本的说话人归因效果可能更好。

Conclusion: ASR转录错误可捕捉揭示说话人身份的特定特征，基于ASR错误转录本的说话人归因效果至少与基于人工转录数据的归因效果一样好。

Abstract: Speaker attribution from speech transcripts is the task of identifying a
speaker from the transcript of their speech based on patterns in their language
use. This task is especially useful when the audio is unavailable (e.g.
deleted) or unreliable (e.g. anonymized speech). Prior work in this area has
primarily focused on the feasibility of attributing speakers using transcripts
produced by human annotators. However, in real-world settings, one often only
has more errorful transcripts produced by automatic speech recognition (ASR)
systems. In this paper, we conduct what is, to our knowledge, the first
comprehensive study of the impact of automatic transcription on speaker
attribution performance. In particular, we study the extent to which speaker
attribution performance degrades in the face of transcription errors, as well
as how properties of the ASR system impact attribution. We find that
attribution is surprisingly resilient to word-level transcription errors and
that the objective of recovering the true transcript is minimally correlated
with attribution performance. Overall, our findings suggest that speaker
attribution on more errorful transcripts produced by ASR is as good, if not
better, than attribution based on human-transcribed data, possibly because ASR
transcription errors can capture speaker-specific features revealing of speaker
identity.

</details>


### [238] [KV Cache Steering for Inducing Reasoning in Small Language Models](https://arxiv.org/abs/2507.08799)
*Max Belitsky,Dawid J. Kopiczko,Michael Dorkenwald,M. Jehanzeb Mirza,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

TL;DR: 提出轻量级的缓存引导方法，通过对键值缓存的一次性干预隐式引导语言模型，在推理基准测试中效果良好，比先前激活引导技术更具优势。


<details>
  <summary>Details</summary>
Motivation: 验证轻量级方法对语言模型隐式引导的有效性，在小语言模型中诱导思维链推理。

Method: 利用GPT - 4o生成的推理轨迹构建引导向量，对键值缓存进行一次性干预。

Result: 在不同推理基准测试中，改善了模型推理的定性结构和定量任务性能。

Conclusion: 缓存引导方法在超参数稳定性、推理时间效率和集成便利性上有优势，是更稳健实用的可控生成解决方案。

Abstract: We propose cache steering, a lightweight method for implicit steering of
language models via a one-shot intervention applied directly to the key-value
cache. To validate its effectiveness, we apply cache steering to induce
chain-of-thought reasoning in small language models. Our approach leverages
GPT-4o-generated reasoning traces to construct steering vectors that shift
model behavior toward more explicit, multi-step reasoning without fine-tuning
or prompt modifications. Experimental evaluations on diverse reasoning
benchmarks demonstrate that cache steering improves both the qualitative
structure of model reasoning and quantitative task performance. Compared to
prior activation steering techniques that require continuous interventions, our
one-shot cache steering offers substantial advantages in terms of
hyperparameter stability, inference-time efficiency, and ease of integration,
making it a more robust and practical solution for controlled generation.

</details>
