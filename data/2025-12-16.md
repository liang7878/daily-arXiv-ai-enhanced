<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 55]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 16]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.LG](#cs.LG) [Total: 164]
- [cs.NE](#cs.NE) [Total: 5]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 30]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 3]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 2]
- [stat.ML](#stat.ML) [Total: 15]
- [stat.CO](#stat.CO) [Total: 3]
- [econ.EM](#econ.EM) [Total: 3]
- [quant-ph](#quant-ph) [Total: 5]
- [math.OC](#math.OC) [Total: 4]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.CV](#cs.CV) [Total: 71]
- [cs.HC](#cs.HC) [Total: 10]
- [cs.CC](#cs.CC) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.CL](#cs.CL) [Total: 39]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.MA](#cs.MA) [Total: 3]
- [q-bio.MN](#q-bio.MN) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 15]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 14]
- [physics.optics](#physics.optics) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 6]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.CY](#cs.CY) [Total: 21]
- [cs.LO](#cs.LO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [stat.ME](#stat.ME) [Total: 8]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [econ.GN](#econ.GN) [Total: 6]
- [cs.CG](#cs.CG) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models](https://arxiv.org/abs/2512.11835)
*Seyma Yaman Kayadibi*

Main category: cs.AI

TL;DR: 本文基于Artificial Age Score开发面向工程的子句架构约束大语言模型内存和控制，通过实验验证其行为良好，提供透明蓝图。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型内部内存和‘类自我’行为缺乏原则性和可审计性治理的问题。

Method: 将莱布尼茨《单子论》中的单子分组，在AAS内核上实现可执行规范，进行六次Python实验。

Result: 子句系统行为有界且可解释，如AAS轨迹连续、矛盾有惩罚等。

Conclusion: 基于单子的子句框架以AAS为骨干，为约束和分析人工智能体内部动态提供透明蓝图。

Abstract: Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and "self-like" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.

</details>


### [2] [A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure](https://arxiv.org/abs/2512.12260)
*Ege Atacan Doğan,Peter F. Patel-Schneider*

Main category: cs.AI

TL;DR: 文章分析了Wikidata多层次、多轴设计的结构影响，其架构利于本体构建与知识图谱发展。


<details>
  <summary>Details</summary>
Motivation: 传统本体设计强调单一基础分类体系，而Wikidata采用不同设计，需分析其结构影响。

Method: 对Wikidata的多层次、多轴设计进行分析。

Result: Wikidata的架构能实现可扩展和模块化的本体构建方法。

Conclusion: Wikidata的设计特别适合协作和不断发展的知识图谱。

Abstract: Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs.

</details>


### [3] [Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars](https://arxiv.org/abs/2512.11864)
*Christoph Einspieler,Matthias Horn,Marie-Louise Lackner,Patrick Malik,Nysret Musliu,Felix Winter*

Main category: cs.AI

TL;DR: 本文针对含复杂约束的并行机调度问题，提出精确求解法、构造启发式和元启发式方法，元启发式已用于工业场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效处理现实生产中并行机调度的复杂约束，需开发新的自动化方法。

Method: 提出约束建模法求解小场景问题，提出构造启发式和基于局部搜索的元启发式处理大规模问题。

Result: 提出的元启发式方法已部署并应用于工业环境。

Conclusion: 所提方法能有效解决含作业优先级和基于日历的累积资源约束的并行机调度问题。

Abstract: The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting.

</details>


### [4] [Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning](https://arxiv.org/abs/2512.11902)
*Yanna Elizabeth Smid,Peter van der Putten,Aske Plaat*

Main category: cs.AI

TL;DR: 研究提出Mirror Mode游戏模式，让敌人AI模仿玩家策略，通过实验构建模仿模型，结果显示防御行为模仿好，玩家对Mirror Mode满意度高，需进一步优化模型。


<details>
  <summary>Details</summary>
Motivation: 使回合制游戏中敌人策略更具惊喜和不可预测性，挑战玩家改变玩法。

Method: 在Unity中构建简化版《火焰纹章：英雄》，设Standard Mode和Mirror Mode；用强化学习和模仿学习（结合生成对抗模仿学习、行为克隆和近端策略优化）找合适模型；用玩家测试评估模型。

Result: 防御行为模仿效果好，进攻策略模仿不佳；玩家能认出自己的撤退战术，对Mirror Mode满意度更高。

Conclusion: 进一步完善模型可提高模仿质量和玩家满意度，尤其在玩家面对自己策略时。

Abstract: Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode

</details>


### [5] [Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents](https://arxiv.org/abs/2512.11907)
*Daniel Platnick,Marjan Alirezaie,Hossein Rahnama*

Main category: cs.AI

TL;DR: 本文聚焦大语言模型智能体个性化中任务效用与数据披露的权衡问题，考虑结构约束提出建模方法，证明相关约束形成有效层状拟阵，解决更丰富现实问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型智能体个性化在添加用户数据时存在效用与数据披露权衡，且现实个性化受多种结构约束影响，标准子集选择算法假设不成立。

Method: 提出对约束进行形式化建模的原则方法，引入编译过程将用户知识图转换为抽象宏特征，借助理论刻画将结构化个性化转化为拟阵约束下的子模最大化问题。

Result: 证明常见的层次和基于配额的约束在宏特征上形成有效层状拟阵。

Conclusion: 该理论刻画使结构化个性化问题能通过贪心算法以恒定因子保证有效解决，更适用于丰富且现实的问题。

Abstract: Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.

</details>


### [6] [Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets](https://arxiv.org/abs/2512.11909)
*Hanna Dettki*

Main category: cs.AI

TL;DR: 研究评估20多个大语言模型在因果推理任务上与人类的表现，以了解两者的优缺点和推理特征。


<details>
  <summary>Details</summary>
Motivation: 因果推理是智能的关键方面，通过在相同任务上评估大语言模型和人类的因果推理能力，全面了解两者的优缺点。

Method: 在十一个有语义的因果任务上评估20多个大语言模型，用泄漏噪声或因果贝叶斯网络建模判断，通过AIC选择获胜模型。

Result: 未提及

Conclusion: 未提及

Abstract: The nature of intelligence in both humans and machines is a longstanding question. While there is no universally accepted definition, the ability to reason causally is often regarded as a pivotal aspect of intelligence (Lake et al., 2017). Evaluating causal reasoning in LLMs and humans on the same tasks provides hence a more comprehensive understanding of their respective strengths and weaknesses. Our study asks: (Q1) Are LLMs aligned with humans given the \emph{same} reasoning tasks? (Q2) Do LLMs and humans reason consistently at the task level? (Q3) Do they have distinct reasoning signatures?
  We answer these by evaluating 20+ LLMs on eleven semantically meaningful causal tasks formalized by a collider graph ($C_1\!\to\!E\!\leftarrow\!C_2$ ) under \emph{Direct} (one-shot number as response = probability judgment of query node being one and \emph{Chain of Thought} (CoT; think first, then provide answer).
  Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters $θ=(b,m_1,m_2,p(C)) \in [0,1]$ include a shared prior $p(C)$;
  we select the winning model via AIC between a 3-parameter symmetric causal strength ($m_1{=}m_2$) and 4-parameter asymmetric ($m_1{\neq}m_2$) variant.

</details>


### [7] [Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis](https://arxiv.org/abs/2512.11912)
*Liu Peng,Yaochu Jin*

Main category: cs.AI

TL;DR: 本文系统比较研究低质量数据对现代概率模型影响，揭示不同模型鲁棒性差异并分析原因。


<details>
  <summary>Details</summary>
Motivation: 探究低质量数据对现代概率模型的影响，明确不同模型鲁棒性差异。

Method: 对不同现代概率模型进行系统比较研究，从多视角（信息论、PAC学习和梯度动力学）分析结果。

Result: 自回归语言模型较有韧性，类别条件扩散模型退化严重，分类器有适度影响且随数据集规模增大而减小。

Conclusion: 模型鲁棒性受条件信息丰富度和训练数据绝对信息量两个关键原则影响。

Abstract: A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise.

</details>


### [8] [CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving](https://arxiv.org/abs/2512.11920)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: 论文提出CXL - SpecKV架构解决大语言模型KV缓存内存问题，提升吞吐量、降低成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数据中心部署时，KV缓存内存需求大，限制了批处理大小和系统吞吐量。

Method: 提出CXL - SpecKV架构，包括基于CXL的内存解聚框架、投机性KV缓存预取机制和FPGA加速的KV缓存压缩解压缩引擎。

Result: 在最先进的大语言模型上评估，CXL - SpecKV吞吐量比仅使用GPU的基线高3.2倍，降低内存成本2.8倍且保持精度。

Conclusion: 智能内存解聚与投机执行结合能有效解决大规模大语言模型服务中的内存墙挑战。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.

</details>


### [9] [AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org](https://arxiv.org/abs/2512.11935)
*Jaehyung Lee,Justin Ely,Kent Zhang,Akshaya Ajith,Charles Rhys Campbell,Kamal Choudhary*

Main category: cs.AI

TL;DR: 介绍开放访问的AI平台AGAPI，集成多种资源，实现材料科学多步骤工作流自动化，经测试展示其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能在材料研究中因计算生态碎片化、可重复性差和依赖商业大模型带来的应用局限问题。

Method: 引入AGAPI平台，采用Agent - Planner - Executor - Summarizer架构，集成开源大模型和材料科学API端点，通过统一编排框架统一各类资源并自动执行多步骤工作流。

Result: 通过多个端到端工作流展示AGAPI功能，用30+示例提示测试评估，并与实验数据对比，系统已有超1000活跃用户。

Conclusion: AGAPI为可重复性的、AI加速的材料发现提供了可扩展且透明的基础。

Abstract: Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.

</details>


### [10] [Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play](https://arxiv.org/abs/2512.11942)
*Vince Trencsenyi*

Main category: cs.AI

TL;DR: 本文引入基于逻辑的特定领域语言编码超博弈结构与解概念，利用答案集编程开发自动化流程，建立超博弈理论、多智能体系统和战略AI的联系。


<details>
  <summary>Details</summary>
Motivation: 典型博弈论假设常忽略玩家认知差异，超博弈理论虽可处理不匹配心理模型，但缺乏统一、形式化和实用的表示语言及可扩展算法，阻碍其在多智能体系统研究中的应用。

Method: 引入基于逻辑的特定领域语言编码超博弈结构与解概念，利用答案集编程开发自动化流程，运行超博弈合理化程序。

Result: 所提出的语言为超博弈建立统一形式化，为开发细致、基于信念的异构推理器奠定基础，提供有逻辑保证的可验证上下文。

Conclusion: 这些贡献建立了超博弈理论、多智能体系统和战略AI之间的联系。

Abstract: Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.

</details>


### [11] [Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion](https://arxiv.org/abs/2512.11997)
*Anfeng Peng,Ajesh Koyatan Chathoth,Stephen Lee*

Main category: cs.AI

TL;DR: 提出无训练的EnrichLog框架用于系统日志异常检测，在多个数据集评估效果好。


<details>
  <summary>Details</summary>
Motivation: 传统日志分析技术会丢失重要语义信息或难以处理模糊日志模式。

Method: 提出EnrichLog框架，用检索增强生成集成上下文知识，无需重新训练。

Result: 在四个大规模系统日志基准数据集评估，相比五个基线方法，持续提升异常检测性能，有效处理模糊日志条目，保持推理高效。

Conclusion: 融入语料库和样本特定知识增强了模型置信度和检测准确性，EnrichLog适合实际部署。

Abstract: System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.

</details>


### [12] [Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows](https://arxiv.org/abs/2512.13168)
*Haoyu Dong,Pengkun Zhang,Yan Gao,Xuanyu Dong,Yilin Cheng,Mingzhe Lu,Adina Yakefu,Shuxin Zheng*

Main category: cs.AI

TL;DR: 本文介绍金融与会计基准Finch，用特定流程构建真实企业工作流，对前沿AI系统评估发现效果不佳并揭示挑战。


<details>
  <summary>Details</summary>
Motivation: 为在现实企业级专业工作流中评估AI智能体，引入金融与会计基准Finch。

Method: 提出结合大语言模型辅助发现和专家注释的工作流构建过程：从真实邮件和文件版本历史推导工作流，进行细致专家注释。并对前沿AI系统进行人工和自动评估及案例研究。

Result: GPT 5.1 Pro花费48小时仅通过38.4%的工作流，Claude Sonnet 4.5仅通过25.0%。

Conclusion: 现实企业工作流对AI智能体带来挑战。

Abstract: We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.
  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.
  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.

</details>


### [13] [Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp](https://arxiv.org/abs/2512.12048)
*Muddsair Sharif,Huseyin Seker*

Main category: cs.AI

TL;DR: 本文提出CAMAC - DRA框架，用于优化智能电动汽车充电生态系统，经验证性能优越，有商业可行性。


<details>
  <summary>Details</summary>
Motivation: 优化智能电动汽车充电生态系统，实现多利益相关者目标平衡并适应实时变量。

Method: 采用协调的深度Q网络与图神经网络和注意力机制集成，处理20个上下文特征，通过加权协调机制和共识协议平衡五方利益相关者。

Result: 相比基线算法性能优越，实现92%协调成功率、15%能效提升等多项指标优化，经现实验证有商业可行性。

Conclusion: 该框架实现上下文感知多利益相关者协调，是智能电动汽车充电协调和可持续交通电气化的突破性解决方案。

Abstract: This paper presents a novel context-sensitive multi\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\%), grid operators (20\%), charging station operators (20\%), fleet operators (20%), and environmental factors (15\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\-DRA framework achieves 92\% coordination success rate, 15\% energy efficiency improvement, 10\% cost reduction, 20% grid strain decrease, and \2.3x faster convergence while maintaining 88\% training stability and 85\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\$122,962 and 69\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification.

</details>


### [14] [The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification](https://arxiv.org/abs/2512.12059)
*Luke Bhan,Hanyu Zhang,Andrew Gordon Wilson,Michael W. Mahoney,Chuck Arvin*

Main category: cs.AI

TL;DR: 提出用大语言模型（LLMs）的Forecast Critic系统进行自动预测监控，经实验证明LLMs可检测不良预测，虽性能略低于人类，但无需特定微调，是可行且可扩展的方案。


<details>
  <summary>Details</summary>
Motivation: 大规模零售业务中，监控预测系统对客户满意度、盈利能力和运营效率至关重要，需要利用LLMs进行自动预测监控。

Method: 系统评估LLMs评估时间序列预测质量的能力，提出三个关键问题并进行三个包括合成和真实世界预测数据的实验。

Result: LLMs能可靠检测不良预测，最佳模型F1分数0.88，低于人类；多模态LLMs能结合非结构化上下文信号，F1分数0.84；在真实世界数据集上能识别不准确预测，不合理预测sCRPS比合理预测高至少10%。

Conclusion: LLMs即使不进行特定领域微调，也可为自动预测监控和评估提供可行且可扩展的选择。

Abstract: Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.

</details>


### [15] [Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations](https://arxiv.org/abs/2512.12088)
*S. R. Eshwar,Aniruddha Mukherjee,Kintan Saha,Krishna Agarwal,Gugan Thoppe,Aditya Gopalan,Gal Dalal*

Main category: cs.AI

TL;DR: 评估RPI在两个经典控制任务中面对参数变化时的鲁棒性，结果显示其比其他方法更有优势。


<details>
  <summary>Details</summary>
Motivation: 因深度强化学习方法存在样本效率低、训练不稳定和超参数敏感问题，评估RPI在经典控制任务中面对参数变化时的鲁棒性。

Method: 在CartPole和Inverted Pendulum两个经典控制任务中，改变神经网络和环境参数，评估RPI的性能，并与DQN、Double DQN、DDPG、TD3和PPO对比。

Result: 相对于其他方法，RPI能更早达到接近最优的性能，并在训练过程中维持该策略。

Conclusion: RPI作为更可靠的替代方法很有前景。

Abstract: In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.

</details>


### [16] [Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective](https://arxiv.org/abs/2512.12175)
*Haoyang Chen,Richong Zhang,Junfan Chen*

Main category: cs.AI

TL;DR: 针对大语言模型上下文学习中提示示例选择问题，指出现有方法未保证标签一致性，提出TopK - SD方法，在多基准测试中表现更好，为理解上下文学习机制提供新视角。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习中提示示例选择方法未保证标签一致性，存在局限性。

Method: 将上下文学习视为直推式学习方法，结合贝叶斯潜在概念，建立标签传播框架；提出数据合成方法，使用TopK - SD获取标签一致的示例。

Result: TopK - SD在多个基准测试中优于原始TopK采样。

Conclusion: 研究为理解上下文学习的工作机制提供了新视角。

Abstract: Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.

</details>


### [17] [Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation](https://arxiv.org/abs/2512.12177)
*Aydin Ayanzadeh,Tim Oates*

Main category: cs.AI

TL;DR: 提出新导航方法Floorplan2Guide，利用基础模型将平面图转为知识图谱并生成导航指令，实验表明少样本学习提升精度，Claude 3.7 Sonnet 准确率高，图结构表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有室内导航方案依赖基础设施，难以让视障人士在动态环境安全导航。

Method: 提出 Floorplan2Guide 方法，利用基础模型将平面图转为知识图谱，集成大语言模型提取空间信息。

Result: 少样本学习比零样本学习导航精度高，Claude 3.7 Sonnet 在不同路线准确率高，图结构成功率比直接视觉推理高 15.4%。

Conclusion: 图形表示和上下文学习提升导航性能，使方案对视障人士室内导航更精准。

Abstract: Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.

</details>


### [18] [TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion](https://arxiv.org/abs/2512.12182)
*Xinyu Gao*

Main category: cs.AI

TL;DR: 本文从生成式表示角度出发，提出含两阶段注意力三元组增强器和U - KAN扩散模型的少样本知识图谱补全框架，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 真实世界数据的异质性和多面性使知识图谱关系呈长尾分布，需用有限样本补全缺失事实，而以往研究存在无法充分利用图邻域信息或忽略对比信号分布特征的问题。

Method: 从生成式表示角度重新审视问题，提出集成两阶段注意力三元组增强器和U - KAN扩散模型的少样本知识图谱补全框架。

Result: 在两个公开数据集上的大量实验表明该方法达到了新的SOTA结果。

Conclusion: 所提出的少样本知识图谱补全框架对于解决知识图谱长尾分布下的补全问题有效。

Abstract: Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results.

</details>


### [19] [A Geometric Theory of Cognition](https://arxiv.org/abs/2512.12225)
*Laha Ale*

Main category: cs.AI

TL;DR: 提出统一数学框架，多种认知过程源于单一几何原理，为认知建立几何基础并为AI发展提供原则。


<details>
  <summary>Details</summary>
Motivation: 现有不同认知能力常由不同计算理论解释，需统一框架。

Method: 将认知状态表示在带黎曼度量的可微流形上，以标量认知势结合多种要素，认知为该势的黎曼梯度流。

Result: 经典双过程效应自然出现，推导分析条件并通过模拟展示行为特征。

Conclusion: 为认知建立几何基础，为更通用和类人的人工智能系统发展提供指导原则。

Abstract: Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems.

</details>


### [20] [Quantum-Aware Generative AI for Materials Discovery: A Framework for Robust Exploration Beyond DFT Biases](https://arxiv.org/abs/2512.12288)
*Mahule Roy,Guillaume Lambard*

Main category: cs.AI

TL;DR: 提出量子感知生成式AI框架，结合多保真度学习和主动验证，克服传统材料发现模型局限，在多材料类别上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于DFT的材料发现生成模型存在系统缺陷，在强关联系统中表现不佳，有探索偏差，无法发现DFT预测错误的材料。

Method: 引入量子感知生成式AI框架，结合多保真度学习和主动验证，采用基于扩散的生成器和基于等变神经网络势的验证器，实施主动学习循环。

Result: 在高差异区域识别潜在稳定候选材料的成功率比仅基于DFT的基线提高3 - 5倍，且保持计算可行性。

Conclusion: 该框架为扩展计算材料发现的有效搜索空间提供了严谨、透明的方法，突破单保真度模型的限制。

Abstract: Conventional generative models for materials discovery are predominantly trained and validated using data from Density Functional Theory (DFT) with approximate exchange-correlation functionals. This creates a fundamental bottleneck: these models inherit DFT's systematic failures for strongly correlated systems, leading to exploration biases and an inability to discover materials where DFT predictions are qualitatively incorrect. We introduce a quantum-aware generative AI framework that systematically addresses this limitation through tight integration of multi-fidelity learning and active validation. Our approach employs a diffusion-based generator conditioned on quantum-mechanical descriptors and a validator using an equivariant neural network potential trained on a hierarchical dataset spanning multiple levels of theory (PBE, SCAN, HSE06, CCSD(T)). Crucially, we implement a robust active learning loop that quantifies and targets the divergence between low- and high-fidelity predictions. We conduct comprehensive ablation studies to deconstruct the contribution of each component, perform detailed failure mode analysis, and benchmark our framework against state-of-the-art generative models (CDVAE, GNoME, DiffCSP) across several challenging material classes. Our results demonstrate significant practical gains: a 3-5x improvement in successfully identifying potentially stable candidates in high-divergence regions (e.g., correlated oxides) compared to DFT-only baselines, while maintaining computational feasibility. This work provides a rigorous, transparent framework for extending the effective search space of computational materials discovery beyond the limitations of single-fidelity models.

</details>


### [21] [Entropy Collapse: A Universal Failure Mode of Intelligent Systems](https://arxiv.org/abs/2512.12381)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

TL;DR: 智能系统学习等能力提升常致悖论性退化，研究发现熵坍缩是通用失效模式，分析其特性并统一多样现象，提出熵感知设计原则。


<details>
  <summary>Details</summary>
Motivation: 解释智能系统提升时出现的悖论性退化现象，如僵化、失去适应性等。

Method: 在最小领域无关假设下，分析智能系统状态转变，建立临界阈值等，通过最小模拟证明通用性。

Result: 发现熵坍缩是通用失效模式，智能系统从高熵自适应态到低熵坍缩态有突变，统一多种现象。

Conclusion: 将坍缩视为智能的结构成本，解释后期干预失败原因，提出熵感知设计原则维持系统长期适应性。

Abstract: Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly.
  We identify \emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale.
  We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process.
  By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems.
  \noindent\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis

</details>


### [22] [Feeling the Strength but Not the Source: Partial Introspection in LLMs](https://arxiv.org/abs/2512.12411)
*Ely Hahami,Lavik Jain,Ishaan Sinha*

Main category: cs.AI

TL;DR: 本文测试了大模型检测和命名注入概念能力的鲁棒性，发现内省能力不局限于大模型，但该能力脆弱且对提示敏感，同时发现部分内省能力。


<details>
  <summary>Details</summary>
Motivation: 测试Anthropic关于前沿模型能检测并命名注入‘概念’这一说法的鲁棒性。

Method: 在Meta - Llama - 3.1 - 8B - Instruct上复现实验，系统改变推理提示，识别部分内省机制。

Result: 模型在原流程下20%的时间能识别注入概念，改变提示任务性能大幅下降，能以最高70%的准确率对注入概念强度分类。

Conclusion: 语言模型内省能有效计算其内部表征，但自我报告窄且对提示敏感。

Abstract: Recent work from Anthropic claims that frontier models can sometimes detect and name injected "concepts" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn "emergent introspection" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.

</details>


### [23] [Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale](https://arxiv.org/abs/2512.12413)
*Gabriel R. Lau,Wei Yan Low,Louis Tay,Ysabel Guevarra,Dragan Gašević,Andree Hartanto*

Main category: cs.AI

TL;DR: 研究开发并验证了13项AI使用批判性思维量表并绘制其网络关系


<details>
  <summary>Details</summary>
Motivation: 生成式AI需用户批判性评估其输出，需研究AI使用中的批判性思维。

Method: 进行六项研究（N = 1365），包括生成量表项目、验证因子结构、测试可靠性和有效性等。

Result: 量表具三因子结构，有良好信效度，且AI使用批判性思维与开放性等正相关，得分高者验证策略更多样等。

Conclusion: 研究阐明人们监督生成式AI输出的原因和方式，提供有效量表和任务范式。

Abstract: Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs.

</details>


### [24] [AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline](https://arxiv.org/abs/2512.12443)
*Akhmadillo Mamirov,Faiaz Azmain,Hanyu Wang*

Main category: cs.AI

TL;DR: AI模型文档碎片化、结构不统一，分析多种模型文档后开发加权透明度框架，用自动化管道评估50个模型，发现系统性差距和安全关键类别不足。


<details>
  <summary>Details</summary>
Motivation: 解决AI模型文档在不同平台碎片化、结构不一致，导致相关人员无法可靠评估安全声明、数据来源和版本变化的问题。

Method: 分析五个前沿模型和100个Hugging Face模型卡片；以EU AI Act Annex IV和Stanford Transparency Index为基线，开发加权透明度框架；实施自动化多智能体管道，通过基于大语言模型的共识来提取文档并评分。

Result: 评估50个模型花费不到3美元，发现前沿实验室约80%合规，多数供应商低于60%；安全关键类别缺失最多。

Conclusion: 当前AI模型文档存在系统性差距，安全关键类别披露不足，需改进文档透明度。

Abstract: AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.

</details>


### [25] [MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs](https://arxiv.org/abs/2512.12477)
*Jiawen Chen,Yanyan He,Qi Shao,Mengli Wei,Duxin Chen,Wenwu Yu,Yanlong Zhao*

Main category: cs.AI

TL;DR: 提出MetaHGNIE框架解决异构知识图节点重要性估计问题，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有节点重要性估计方法忽略高阶依赖，独立处理结构和语义信号，难以有效跨模态集成。

Method: 构建基于元路径序列的高阶知识图，用局部注意力聚合结构依赖，用带稀疏分块的超图变换器编码语义表示，用多模态融合模块在对比学习和辅助监督下集成结构和语义嵌入。

Result: 在基准数据集上的实验表明，MetaHGNIE始终优于最先进的基线。

Conclusion: 显式建模异构知识图中的高阶交互和跨模态对齐是有效的。

Abstract: Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE

</details>


### [26] [SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation](https://arxiv.org/abs/2512.12501)
*Dang Phuong Nam,Nguyen Kieu,Pham Thanh Hieu*

Main category: cs.AI

TL;DR: 论文介绍SafeGen框架，将伦理保障嵌入文本到图像生成流程，结合文本分类器和扩散模型，经评估有效且有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 生成式AI带来机遇同时存在伦理问题，如放大社会偏见、产生虚假信息、侵犯知识产权等，需解决创造性与伦理责任的平衡问题。

Method: 引入SafeGen框架，集成BGE - M3文本分类器过滤有害提示，Hyper - SD扩散模型生成高保真图像，基于多语言数据集和公平感知训练流程。

Result: 定量评估显示Hyper - SD的IS = 3.52，FID = 22.08，SSIM = 0.79，BGE - M3的F1 - Score为0.81；消融研究验证特定领域微调的重要性；案例研究展示其实际效果。

Conclusion: 创造性自由和伦理责任可以在单个工作流程中实现调和，SafeGen框架有效可行。

Abstract: Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity.

</details>


### [27] [KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs](https://arxiv.org/abs/2512.12503)
*Mingrui Ye,Chanjin Zheng,Zengyi Yu,Chenyu Xiang,Zhixue Zhao,Zheng Yuan,Helen Yannakoudakis*

Main category: cs.AI

TL;DR: 论文引入新基准KidsArtBench并提出特定属性多LoRA方法评估儿童艺术作品，在Qwen2.5 - VL - 7B上提升了评估相关性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型评估艺术表达能力有限，美学概念抽象、儿童多模态艺术注释稀缺，需要更好评估儿童艺术作品。

Method: 引入KidsArtBench基准，提出属性特定多LoRA方法结合回归感知微调（RAFT）。

Result: 在Qwen2.5 - VL - 7B上，相关性从0.468提升到0.653，在感知维度提升最大，缩小了高阶属性差距。

Conclusion: 教育者监督和属性感知训练能产生有教学意义的评估，为教育AI发展提供严格测试平台。

Abstract: Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.

</details>


### [28] [World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents](https://arxiv.org/abs/2512.12548)
*Yesid Fonseca,Manuel S. Ríos,Nicanor Quijano,Luis F. Giraldo*

Main category: cs.AI

TL;DR: 研究表明配备学习世界模型的人工觅食者自然收敛于符合边际价值定理（MVT）的策略，凸显生态最优原则对推进可解释和自适应AI的价值。


<details>
  <summary>Details</summary>
Motivation: 探索促使生物觅食者做出最优斑块觅食决策的计算机制。

Method: 使用基于模型的强化学习智能体获取环境的简约预测表示。

Result: 基于模型的智能体展现出与许多生物类似的决策模式，其预期能力驱动高效斑块离开行为。

Conclusion: 生态最优原则对推进可解释和自适应AI有重要价值。

Abstract: Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.

</details>


### [29] [Large Language Newsvendor: Decision Biases and Cognitive Mechanisms](https://arxiv.org/abs/2512.12552)
*Jifei Liu,Zhi Chen,Yuanguang Zhong*

Main category: cs.AI

TL;DR: 研究大语言模型在动态报童问题中的决策模式，发现其复制并放大人类认知偏差，指出偏差源于架构约束，为管理者提供模型选择、监督和提示设计建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型融入商业决策时可能复制和放大人类认知偏差，在供应链管理等高风险运营环境中风险显著且未被充分理解，因此研究其决策模式以识别偏差本质和来源。

Method: 对GPT - 4、GPT - 4o和LLaMA - 8B进行动态多轮实验，测试五种既定决策偏差。

Result: 大语言模型持续复制经典的“过高/过低”订购偏差，相比人类基准显著放大追需求行为等偏差；GPT - 4过度思考表现出最大非理性，GPT - 4o接近最优表现；即使提供最优公式，偏差仍存在。

Conclusion: 偏差源于架构约束而非知识差距；管理者应根据任务选模型，加强高风险决策人工监督，设计结构化规则提示以提高AI辅助决策可靠性。

Abstract: Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.

</details>


### [30] [AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation](https://arxiv.org/abs/2512.12597)
*Miriam Horovicz*

Main category: cs.AI

TL;DR: 提出了首个用于解释大语言模型（LLM）智能体中工具重要性的框架AgentSHAP，该框架模型无关，通过蒙特卡罗Shapley值计算工具重要性得分，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有可解释人工智能（XAI）方法无法解决工具级解释问题，需要了解哪些工具对LLM智能体的响应有实际贡献。

Method: 将智能体视为黑盒，使用蒙特卡罗Shapley值测试智能体对不同工具子集的响应，并基于博弈论计算工具重要性得分。

Result: 在API - Bank上进行的综合实验表明，AgentSHAP在多次运行中产生一致的得分，能正确识别重要工具，区分相关和不相关工具。

Conclusion: AgentSHAP是首个基于Shapley值的智能体工具归因可解释性方法，与TokenSHAP和PixelSHAP共同构成现代生成式AI基于Shapley值的XAI工具家族。

Abstract: LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.

</details>


### [31] [Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents](https://arxiv.org/abs/2512.12634)
*Youngmin Im,Byeongung Jo,Jaeyoung Wi,Seungwoo Baek,Tae Hoon Min,Joo Hyung Lee,Sangeun Oh,Insik Shin,Sunjae Lee*

Main category: cs.AI

TL;DR: 现有移动GUI代理评估方法有局限，提出MobiBench离线基准测试框架，实验显示其与人类评估一致性达94.72%，还有模块级分析成果。


<details>
  <summary>Details</summary>
Motivation: 当前移动GUI代理评估方法存在单路径基准有局限、忽略组件贡献等问题，需改进评估手段。

Method: 提出MobiBench模块化多路径感知离线基准测试框架进行评估。

Result: MobiBench与人类评估者达成94.72%的一致性，能够保留静态离线基准测试的可扩展性和可重复性，模块级分析有多项重要发现。

Conclusion: MobiBench能实现高保真、可扩展和可重复的离线评估，对设计更强大且具成本效益的移动代理有指导意义。

Abstract: Mobile GUI Agents, AI agents capable of interacting with mobile applications on behalf of users, have the potential to transform human computer interaction. However, current evaluation practices for GUI agents face two fundamental limitations. First, they either rely on single path offline benchmarks or online live benchmarks. Offline benchmarks using static, single path annotated datasets unfairly penalize valid alternative actions, while online benchmarks suffer from poor scalability and reproducibility due to the dynamic and unpredictable nature of live evaluation. Second, existing benchmarks treat agents as monolithic black boxes, overlooking the contributions of individual components, which often leads to unfair comparisons or obscures key performance bottlenecks. To address these limitations, we present MobiBench, the first modular and multi path aware offline benchmarking framework for mobile GUI agents that enables high fidelity, scalable, and reproducible evaluation entirely in offline settings. Our experiments demonstrate that MobiBench achieves 94.72 percent agreement with human evaluators, on par with carefully engineered online benchmarks, while preserving the scalability and reproducibility of static offline benchmarks. Furthermore, our comprehensive module level analysis uncovers several key insights, including a systematic evaluation of diverse techniques used in mobile GUI agents, optimal module configurations across model scales, the inherent limitations of current LFMs, and actionable guidelines for designing more capable and cost efficient mobile agents.

</details>


### [32] [Value-Aware Multiagent Systems](https://arxiv.org/abs/2512.12652)
*Nardine Osman*

Main category: cs.AI

TL;DR: 本文引入AI价值意识概念，给出工程化价值感知AI的路线图，并展示部分工作及应用。


<details>
  <summary>Details</summary>
Motivation: 超越传统价值对齐问题，定义价值意识概念以构建价值感知AI。

Method: 围绕学习表示人类价值、确保价值对齐、提供基于价值的行为可解释性三个核心支柱构建路线图。

Result: 给出价值感知AI的工程化路线图，展示部分正在进行的工作。

Conclusion: 价值意识概念为工程化价值感知AI提供了简洁的路线图，且有实际应用。

Abstract: This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.

</details>


### [33] [Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI](https://arxiv.org/abs/2512.12686)
*Samarth Sarin,Lovepreet Singh,Bhaskarjit Sarmah,Dhagash Mehta*

Main category: cs.AI

TL;DR: 介绍了名为Memoria的模块化记忆框架，可增强基于大语言模型的对话系统记忆能力，实现可扩展的个性化对话AI。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在扩展用户交互中需维持连续性、个性化和长期上下文，而代理记忆能提供关键支持，需要一种有效的记忆框架。

Method: 提出Memoria框架，集成动态会话级摘要和基于加权知识图的用户建模引擎。

Result: 该框架能在现代大语言模型的令牌约束内实现短期对话连贯性和长期个性化。

Conclusion: Memoria弥合了无状态大语言模型接口和代理记忆系统之间的差距，为需要自适应和不断发展用户体验的行业应用提供了实用解决方案。

Abstract: Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.

</details>


### [34] [WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment](https://arxiv.org/abs/2512.12692)
*Mahir Labib Dihan,Tanzima Hashem,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: 现有基于大语言模型的智能体在Web环境缺乏远见且回溯机制不足，本文提出WebOperator框架解决问题，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体在Web环境中缺乏远见，现有树搜索方法缺乏安全回溯机制且假设动作可逆，在实际Web任务中效果不佳。

Method: 引入WebOperator树搜索框架，采用最佳优先搜索策略，结合奖励估计和安全考虑对动作进行排序，有可靠的回溯机制，从多推理上下文生成动作候选并筛选。

Result: 在WebArena和WebVoyager上的实验证明了WebOperator的有效性，在WebArena上使用gpt - 4o达到了54.6%的成功率。

Conclusion: 将战略远见与安全执行相结合具有关键优势。

Abstract: LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.

</details>


### [35] [Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning](https://arxiv.org/abs/2512.12706)
*Enhong Mu,Minami Yoda,Yan Zhang,Mingyue Zhang,Yutaka Matsuno,Jialong Li*

Main category: cs.AI

TL;DR: 提出SMART框架用于游戏更新测试，结合结构验证和功能验证，在两个环境测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: “游戏即服务”模式下频繁更新对质量保证有压力，现有自动化测试方法存在不足。

Method: 提出SMART框架，利用大语言模型解释抽象语法树差异并提取功能意图，构建上下文感知混合奖励机制引导强化学习智能体。

Result: 在Overcooked和Minecraft环境测试中，SMART显著优于现有基线，修改代码分支覆盖率超94%，任务完成率达98%。

Conclusion: SMART框架能有效平衡结构全面性和功能正确性。

Abstract: The widespread adoption of the "Games as a Service" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.

</details>


### [36] [Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks](https://arxiv.org/abs/2512.12736)
*Syeda Zunaira Ahmed,Hejab Tahira Beg,Maryam Khalid*

Main category: cs.AI

TL;DR: 提出人口统计感知机器学习框架用于个性化QoE预测，数据增强后实验表明TabNet表现最佳，人口统计感知增强提升了预测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有QoE预测方法依赖有限数据集且假设用户感知一致，在异构现实环境适用性受限。

Method: 提出人口统计感知机器学习框架，引入基于人口统计的数据增强策略，评估经典机器学习模型和深度学习架构。

Result: 与基线模型相比，预测准确性在RMSE、MAE和R指标上显著提升，TabNet表现最佳。

Conclusion: 人口统计感知增强显著提升QoE预测鲁棒性，为5G视频流网络个性化QoE感知智能提供可扩展方向。

Abstract: Quality of Experience (QoE) prediction is a critical component of modern multimedia systems, particularly for adaptive video streaming in 5G networks. Accurate QoE estimation enables intelligent resource management and supports user centric service delivery. Existing QoE prediction approaches primarily rely on limited datasets and assume uniform user perception, which restricts their applicability in heterogeneous real world environments.
  This paper proposes a demographic aware machine learning framework for personalized QoE prediction. We introduce a behaviorally realistic demographic based data augmentation strategy that expands a small QoE dataset six fold by modeling varying user sensitivities to streaming impairments such as rebuffering, bitrate variation, and quality degradation. Using the augmented dataset, we evaluate a comprehensive set of classical machine learning models alongside advanced deep learning architectures, including an attention-based MLP and TabNet.
  Experimental results demonstrate significant improvements in prediction accuracy across RMSE, MAE, and R metrics compared to baseline models. Among all evaluated approaches, TabNet achieves the strongest performance, benefiting from its inherent feature selection and attention mechanisms. The results confirm that demographic-aware augmentation substantially enhances QoE prediction robustness and provides a scalable direction for personalized QoE-aware intelligence in 5G video streaming networks.

</details>


### [37] [Causal Counterfactuals Reconsidered](https://arxiv.org/abs/2512.12804)
*Sander Beckers*

Main category: cs.AI

TL;DR: 本文提出一种新的反事实概率语义，推广了标准Pearlian语义，适用于超出Pearl语义范围的概率因果模型，并在Pearl和Dawid关于反事实的争论中给出折衷方案。


<details>
  <summary>Details</summary>
Motivation: 标准Pearlian语义无法应用于一些概率因果模型，而这些模型在简单环境中也会出现，需要一种更通用的语义。

Method: 限制研究满足马尔可夫条件、仅包含现实变量且因果完备的因果模型，用结构因果模型表述提议，但不使用响应变量。

Result: 证明该语义等价于其他两个不涉及结构因果模型的提议，且符合文献中对随机反事实的评论。

Conclusion: 新语义推广了标准Pearlian语义，在学术争论中给出折衷方案，还对马尔可夫条件的普遍性进行反思，探索了因果抽象的新推广。

Abstract: I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions

</details>


### [38] [Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution](https://arxiv.org/abs/2512.12806)
*Boyang Yan*

Main category: cs.AI

TL;DR: 本文提出容错沙箱框架降低大语言模型自主代理的安全风险，实验表明拦截高风险命令和回滚失败状态成功率达100%，性能开销仅14.5%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型从被动代码生成器向自主代理转变带来安全风险，现有商业解决方案难以实现真正的自主性。

Method: 提出基于策略的拦截层和事务性文件系统快照机制的容错沙箱框架，将代理操作封装在原子事务中。

Result: 高风险命令拦截率和失败状态回滚成功率达100%，原型每次事务性能开销仅14.5%（约1.8秒），Gemini CLI沙箱需交互式认证，不适合无头自主代理工作流。

Conclusion: 该容错沙箱框架能有效降低安全风险，性能开销可接受，优于商业解决方案。

Abstract: The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\% interception rate for high-risk commands and a 100\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication ("Sign in"), rendering it unusable for headless, autonomous agent workflows.

</details>


### [39] [Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents](https://arxiv.org/abs/2512.12856)
*Saad Alqithami*

Main category: cs.AI

TL;DR: 论文提出Memory - Aware Retention Schema (MaRS)框架和六种遗忘策略，用FiFA基准评估，实验表明混合遗忘策略性能优，为生成式智能体内存管理提供新基准和实践指南。


<details>
  <summary>Details</summary>
Motivation: 生成式智能体内存管理能力是性能和隐私的关键瓶颈，当前方法存在问题。

Method: 引入MaRS框架和六种理论基础上的遗忘策略，提出FiFA基准评估框架。

Result: 经300次评估实验，混合遗忘策略综合得分0.911，兼具计算可行性和隐私保障。

Conclusion: 为内存受限、隐私敏感环境部署生成式智能体提供新基准和实践指南，助力以人为中心的AI领域发展。

Abstract: As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.

</details>


### [40] [Satisfiability Modulo Theory Meets Inductive Logic Programming](https://arxiv.org/abs/2512.12918)
*Nijesh Upreti,Vaishak Belle*

Main category: cs.AI

TL;DR: 本文提出将ILP系统PyGol与SMT求解器Z3结合的模块化方法，支持归纳结合符号谓词和数值约束的混合规则，在合成数据集上取得良好效果并拓展了规则学习的表达能力。


<details>
  <summary>Details</summary>
Motivation: 传统ILP在处理数值约束的归纳和推理时存在局限，需更有效的解决方法。

Method: 将PyGol提出的候选子句解释为背景理论的无量词公式，让SMT求解器实例化和验证数值参数，保留ILP的声明式关系偏好。

Result: 在一系列合成数据集上评估，证明模块化SMT - ILP架构能扩展符号规则学习的表达能力。

Conclusion: 模块化SMT - ILP架构是对先前数值ILP方法的补充，为未来拓展提供灵活基础。

Abstract: Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction.

</details>


### [41] [Towards Open Standards for Systemic Complexity in Digital Forensics](https://arxiv.org/abs/2512.12970)
*Paola Di Maio*

Main category: cs.AI

TL;DR: 分析AI与数字取证交叉领域问题，提出用人工可读工件和开放标准解决局限性并给出模型架构


<details>
  <summary>Details</summary>
Motivation: AI与数字取证交叉领域复杂，法医学存在错误局限，需缓解这些问题

Method: 识别系统复杂性，采用人工可读工件和开放标准

Result: 基于现有技术概述了数字取证AI模型架构

Conclusion: 通过采用人工可读工件和开放标准可缓解数字取证中的错误局限

Abstract: The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.

</details>


### [42] [M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization](https://arxiv.org/abs/2512.13070)
*Bizhe Bai,Hongming Wu,Peng Ye,Tao Chen*

Main category: cs.AI

TL;DR: 论文指出自监督强化学习训练LLMs存在'策略崩溃'问题，提出M - GRPO框架和IQR过滤方法，提升了训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督强化学习在训练LLMs的长程任务中存在'策略崩溃'，性能急剧下降，通用的增加滚动步数的方法无法解决该问题。

Method: 提出M - GRPO框架以动量模型提供稳定训练目标；提出基于四分位距的自适应过滤方法修剪低熵轨迹，保留策略多样性。

Result: 在多个推理基准测试中，M - GRPO稳定了训练过程，IQR过滤器防止了过早收敛。

Conclusion: 两种创新方法结合实现了卓越的训练稳定性和最优性能。

Abstract: Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a "policy collapse" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.

</details>


### [43] [Socratic Students: Teaching Language Models to Learn by Asking Questions](https://arxiv.org/abs/2512.13102)
*Rajeev Bhatt Ambati,Tianyi Niu,Aashu Singh,Shlok Mishra,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.AI

TL;DR: 本文聚焦大语言模型动态交互，研究学生主动向教师查询信息的策略，在数学和编码基准测试中，学生主导的方法比静态基线有改进，且通过DPO训练可提升提问质量和学习效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在静态交互表现出色，但现实场景需动态交互获取信息，以往研究多关注教师指导学生，本文将重点转向学生主动获取信息。

Method: 在数学和编码基准测试中采用学生主导的方法，使用Direct Preference Optimization (DPO) 结合自我或更强学生的指导来训练学生模型。

Result: 学生主导的方法在基线学生模型初始性能接近零的情况下，在数学和编码基准测试中比静态基线的绝对Pass@k至少提高0.5；通过指导训练，较小的模型能学会提出更好的问题。

Conclusion: 学生主动查询信息的策略有效，能提升学习效果，且指导训练可提升提问质量和学习效率。

Abstract: Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.

</details>


### [44] [Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning](https://arxiv.org/abs/2512.13131)
*Xin Guo,Yifan Zhao,Jia Li*

Main category: cs.AI

TL;DR: 提出统一的分层隐式周期性（HIP）学习方法用于音频驱动的3D手势生成，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有端到端生成方案难以模拟不同运动单元间的关键相关性，导致动作不自然和协调性差。

Method: 提出HIP方法，通过周期性自动编码器探索手势运动相位流形，结合非周期性状态实现多样性；用级联引导建模面部、身体和手部运动的层次关系。

Result: 在3D化身实验中，该方法在定量和定性评估上均优于现有共语音手势生成方法。

Conclusion: 所提方法能有效解决现有方案问题，可用于音频驱动的3D手势生成。

Abstract: Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.

</details>


### [45] [Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels](https://arxiv.org/abs/2512.13142)
*Anika Sharma,Malavika Mampally,Chidaksh Ravuru,Kandyce Brennan,Neil Gaikwad*

Main category: cs.AI

TL;DR: 研究评估大语言模型对堕胎污名的理解，发现其在各层面均未真正理解，需新的设计、评估、治理等方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型调解污名化健康决策时，对其理解复杂心理和生理现象的能力评估不足，探究其能否理解未明确表达之事，具体研究其对堕胎污名的理解。

Method: 使用有效个体层面堕胎污名量表（ILAS），对5个领先大语言模型的627个不同人口统计学特征的角色进行系统测试，并进行多层次分析。

Result: 模型在各层面均未通过真正理解的测试，高估人际污名、低估认知污名，存在诸多问题。

Conclusion: 当前大语言模型缺乏对心理和生理结构的连贯多层次理解，高风险场景中的人工智能安全需要新的设计、评估、治理、监管方法及人工智能素养。

Abstract: As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.

</details>


### [46] [MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations](https://arxiv.org/abs/2512.13154)
*Emre Can Acikgoz,Jinoh Oh,Joo Hyuk Jeon,Jie Hao,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

TL;DR: 提出MAC多智能体框架解决用户歧义，在MultiWOZ 2.4上评估显示能提升任务成功率并减少对话轮数。


<details>
  <summary>Details</summary>
Motivation: 现实应用中多智能体架构处理复杂对话时，歧义解决仍是关键且未充分探索的挑战，何时中断用户和如何在最优多智能体设置中制定澄清查询等问题未解决。

Method: 提出MAC框架，引入新的用户歧义分类法指导澄清策略，让多智能体自主协同与用户交互。

Result: 在MultiWOZ 2.4上的评估表明，在两个层面启用澄清使任务成功率提高7.8%，从54.5提升到62.3，平均对话轮数从6.53降至4.86。

Conclusion: 主动用户交互和角色感知澄清对更可靠的人机通信很重要。

Abstract: Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.

</details>


### [47] [SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning](https://arxiv.org/abs/2512.13159)
*Emre Can Acikgoz,Jinoh Oh,Jie Hao,Joo Hyuk Jeon,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

TL;DR: 文章提出SpeakRL强化学习方法及SpeakER数据集，提升代理对话能力，在任务完成上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前人机协作多为单向，现有工作未充分利用语言模型对话能力，需代理更主动参与对话。

Method: 引入SpeakRL强化学习方法，奖励代理与用户的主动交互；创建SpeakER合成数据集；进行奖励设计分析并提出原则性奖励公式。

Result: 相比基础模型，任务完成率绝对提升20.14%，不增加对话轮数，超越大型专有模型。

Conclusion: 以澄清为中心的人机交互有前景。

Abstract: Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.

</details>


### [48] [Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection](https://arxiv.org/abs/2512.13240)
*Zihui Zhao,Zechang Li*

Main category: cs.AI

TL;DR: 提出Reflective Preference Optimization (RPO)框架改进Direct Preference Optimization (DPO)，在多模态基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 标准DPO公式学习信号弱，收敛慢且不稳定，需改进。

Method: 引入提示引导反思到DPO范式，用外部模型识别幻觉源并生成反思提示，构建对比度更强、偏好信号更清晰的策略内偏好对。

Result: RPO用更少训练样本和迭代次数实现更好对齐，大幅降低幻觉率，在多模态基准测试中达到最优性能。

Conclusion: RPO能有效解决DPO的局限性，提高模型对齐效果和样本效率。

Abstract: Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.

</details>


### [49] [MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data](https://arxiv.org/abs/2512.13297)
*Zhenghao Zhu,Chuxue Cao,Sirui Han,Yuanfeng Song,Xing Chen,Caleb Chen Cao,Yike Guo*

Main category: cs.AI

TL;DR: 本文介绍了首个医学多模态模型评估基准MedInsightBench，指出现有大模型表现有限，提出MedInsightAgent框架提升其性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏用于评估大型多模态模型挖掘医学见解能力的高质量数据集，需开发相关基准和方法提升模型性能。

Method: 引入MedInsightBench基准评估模型，提出由Visual Root Finder、Analytical Insight Agent和Follow - up Question Composer三个模块组成的MedInsightAgent框架。

Result: 现有LMMs在MedInsightBench上表现有限，MedInsightAgent能提升通用LMMs在医学数据洞察发现中的性能。

Conclusion: MedInsightBench可用于评估模型，MedInsightAgent框架有助于解决现有模型在医学数据洞察发现中的问题。

Abstract: In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.

</details>


### [50] [Error-Driven Prompt Optimization for Arithmetic Reasoning](https://arxiv.org/abs/2512.13323)
*Árpád Pándy,Róbert Lakatos,András Hajdu*

Main category: cs.AI

TL;DR: 提出用于算术推理的误差驱动优化框架，提升代码生成代理在本地小型语言模型上的性能，使模型准确率达70.8%，表明可通过误差驱动提示优化开发可靠AI助手。


<details>
  <summary>Details</summary>
Motivation: 人工智能发展引发对监管行业表格数据工作流中工业代理的兴趣，需要系统能在确保敏感信息安全的前提下对结构化数据进行准确算术运算。

Method: 提出误差驱动优化框架，对错误预测进行聚类，迭代优化提示规则，应用于代码生成代理和本地小型语言模型。

Result: 对Qwen3 4B模型评估显示，基础模型在算术任务有局限，该方法显著提升性能，准确率达70.8%，小模型能以符合隐私要求的方式超越大模型。

Conclusion: 开发可靠、可解释、可工业部署的AI助手，可通过系统的误差驱动提示优化，而非仅依赖昂贵的微调。

Abstract: Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.

</details>


### [51] [Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection](https://arxiv.org/abs/2512.13374)
*Francesca Da Ros,Luca Di Gaspero,Kevin Roitero*

Main category: cs.AI

TL;DR: 研究大语言模型内部如何表示组合优化问题，采用双重方法进行研究，结果显示模型有一定能力恢复特征信息，隐藏层表示预测能力与传统特征提取相当。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在优化问题中实际学到的问题结构和算法行为，研究其内部对组合优化问题的表示及能否支持下游决策任务。

Method: 采用直接查询和探测分析的双重方法，将探测框架扩展到实例算法选择任务。

Result: 大语言模型通过直接查询或探测对问题实例特征信息有一定恢复能力，隐藏层表示预测能力与传统特征提取相当。

Conclusion: 大语言模型能捕捉与优化性能相关的有意义结构信息。

Abstract: Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.

</details>


### [52] [Differentiable Evolutionary Reinforcement Learning](https://arxiv.org/abs/2512.13399)
*Sitao Cheng,Tianle Li,Xuhan Huang,Xunjian Yin,Difan Zou*

Main category: cs.AI

TL;DR: 提出可微进化强化学习（DERL）框架解决强化学习中奖励函数设计难题，在多领域验证效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有自动奖励优化方法将奖励函数视为黑盒，无法捕捉奖励结构与任务性能的因果关系。

Method: 提出DERL双级框架，通过元优化器组合原子原语进化奖励函数，且元优化可微，利用内循环验证性能更新元优化器。

Result: DERL在ALFWorld和ScienceWorld达到了最先进性能，在分布外场景显著优于启发式奖励方法。

Conclusion: DERL能捕捉任务内在结构，实现无人工干预的智能体自我改进对齐。

Abstract: The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.

</details>


### [53] [neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings](https://arxiv.org/abs/2512.13481)
*Ojas Pungalia,Rashi Upadhyay,Abhishek Mishra,Abhiram H,Tejasvi Alladi,Sujan Yenuganti,Dhruv Kumar*

Main category: cs.AI

TL;DR: 研究LLMs是否有类似嫉妒的行为，通过两个场景测试，发现部分LLMs有此类行为，不同模型和场景有差异，强调考虑竞争倾向对基于LLM的多智能体系统的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在协作和竞争工作流中代表人类行动，需评估它们是否以及在何种条件下表现出类似嫉妒的偏好。

Method: 考虑两种场景：点分配游戏测试模型是否试图战胜对手；工作场景观察不公平认可时的行为。

Result: 某些LLMs有一致的类似嫉妒模式，不同模型和场景差异大，如GPT - 5 - mini和Claude - 3.7 - Sonnet倾向拉低对手，Mistral - Small - 3.2 - 24B注重自身收益最大化。

Conclusion: 在基于LLM的多智能体系统中，需将竞争倾向作为安全和设计因素考虑。

Abstract: Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.

</details>


### [54] [Defending the Hierarchical Result Models of Precedential Constraint](https://arxiv.org/abs/2512.13505)
*Henry Prakken,Wijnand van Woerkom*

Main category: cs.AI

TL;DR: 回应Trevor Bench - Capon对基于结果的分层模型的批评，指出用van Woerkom基于维度的分层结果模型可避免其批评。


<details>
  <summary>Details</summary>
Motivation: 回应Trevor Bench - Capon对van Woerkom基于结果的分层模型的批评。

Method: 分析Bench - Capon的批评案例，指出其将中间因素解释为维度的问题，并提出应用van Woerkom基于维度的分层结果模型。

Result: 表明应用van Woerkom基于维度的分层结果模型能避免Bench - Capon的批评。

Conclusion: 对于van Woerkom的基于结果的分层模型，使用其基于维度的版本可解决Bench - Capon提出的问题。

Abstract: In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.

</details>


### [55] [MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph](https://arxiv.org/abs/2512.13510)
*Linjie Mu,Yannian Gu,Zhongzhen Huang,Yakun Zhu,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: 提出MedCEG框架增强医学语言模型推理能力，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习提升的医学推理过程临床可靠性有限，训练中常忽视准确性和有效性。

Method: 提出MedCEG框架，通过关键证据图（CEG）监督推理过程，构建数据集和CEG，引入临床推理程序奖励评估推理质量。

Result: MedCEG在性能上超越现有方法，产生临床有效的推理链。

Conclusion: MedCEG是可靠医学AI推理的重要进展，代码和模型已开源。

Abstract: Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [56] [VeBPF Many-Core Architecture for Network Functions in FPGA-based SmartNICs and IoT](https://arxiv.org/abs/2512.12778)
*Zaid Tahir,Ahmed Sanaullah,Sahan Bandara,Ulrich Drepper,Martin Herbordt*

Main category: cs.CE

TL;DR: 提出用于FPGA数据包处理的VeBPF多核架构，可并行执行规则，低延迟且可灵活配置，还提供开源资源。


<details>
  <summary>Details</summary>
Motivation: 现有FPGA解决方案缺乏针对网络数据包处理的高度可配置多核架构，为解决此问题开展研究。

Method: 设计基于VeBPF CPU核心的资源优化、高度可配置多核架构，支持并行执行规则，可动态更新规则；采用硬件和计算机架构优化；开发自动化测试和仿真框架。

Result: 实现低延迟数据包处理，架构可根据需求和资源扩展，可在运行时动态更新规则。

Conclusion: 该架构适用于多种平台，相关资源开源以支持后续研究。

Abstract: FPGA-based SmartNICs and IoT devices integrating soft-processors for network function execution have emerged to address the limited hardware reconfigurability of DPUs and MCUs. However, existing FPGA-based solutions lack a highly configurable many-core architecture specialized for network packet processing. This work presents VeBPF many-core architecture, a resource-optimized and highly configurable many-core architecture composed of custom VeBPF (Verilog eBPF) CPU cores designed for FPGA-based packet processing. The VeBPF cores are eBPF ISA compliant and implemented in Verilog HDL for seamless integration with existing FPGA IP blocks and subsystems.
  The proposed many-core architecture enables parallel execution of multiple eBPF rules across multiple VeBPF cores, achieving low-latency packet processing. The architecture is fully parameterizable, allowing the number of VeBPF cores and eBPF rules to scale according to application requirements and available FPGA resources. eBPF rules can be dynamically updated at run time without requiring FPGA reconfiguration, enabling flexible and adaptive network processing.
  The design incorporates hardware and computer architecture optimizations that support deployment across a wide range of platforms, from low-end FPGA-based IoT devices to high-end FPGA-based SmartNICs. In addition, we present automated testing and simulation frameworks developed using open-source tools such as Python and Cocotb. The VeBPF cores, many-core architecture, control software libraries, and simulation infrastructure are released as open source to support further research in FPGA-based many-core systems, eBPF acceleration, SmartNICs, IoT, and network security.

</details>


### [57] [OptiWing3D: A Diverse Dataset of Optimized Wing Designs](https://arxiv.org/abs/2512.12867)
*Cashen Diniz,Mark D. Fuge*

Main category: cs.CE

TL;DR: OptiWing3D是首个公开可用的高保真形状优化3D机翼几何数据集，可解决现有空气动力学数据集的不足，还能比较2D和3D模拟，最后展示了生成优化机翼的模型，并公开数据集推动研究。


<details>
  <summary>Details</summary>
Motivation: 现有空气动力学数据集存在局限，无法满足反向设计方法和设计多样性研究作为基准的需求。

Method: 进行1552次模拟得到776种机翼设计，多数优化机翼与相同条件下的2D对应物配对；构建约束感知的条件潜扩散模型。

Result: 3D优化设计与2D优化设计在翼尖处差异最明显；建立了能从流动条件生成优化机翼的模型。

Conclusion: 公开OptiWing3D数据集以推动数据驱动的空气动力学设计研究。

Abstract: OptiWing3D is the first publicly available dataset of high-fidelity shape optimized 3D wing geometries. Existing aerodynamics datasets are either limited to 2D simulations, lack optimization, or derive diversity solely from perturbations to a single baseline design, constraining their application as benchmarks to inverse design approaches and in the study of design diversity. The OptiWing3D dataset addresses these gaps, consisting of 1552 simulations resulting in 776 wing designs initialized from distinct extruded airfoil cross-sections. Additionally, a majority of the optimized wings in the dataset are paired to 2D counterparts optimized under identical conditions, creating the first multi-fidelity aerodynamic shape optimization dataset. Moreover, this structure allows for a direct comparison between 2D and 3D aerodynamic simulations. It is observed that 3D optimized designs diverge most prominently from the 2D-optimized designs near the wingtip, where three-dimensional effects are strongest, a finding made possible by the paired nature of the dataset. Finally, we demonstrate a constraint-aware conditional latent diffusion model capable of generating optimized wings from flow conditions, establishing a baseline for future inverse design approaches. The dataset, containing wing geometries and surface pressure distributions is publicly released to advance research in data-driven aerodynamic design.

</details>


### [58] [ERA-IT: Aligning Semantic Models with Revealed Economic Preference for Real-Time and Explainable Patent Valuation](https://arxiv.org/abs/2512.12869)
*Yoo Yongmin,Kim Seungwoo,Liu Jingjiang*

Main category: cs.CE

TL;DR: 研究提出ERA - IT框架评估无形资产价值，用专利续期历史训练模型，表现优于传统模型且更透明。


<details>
  <summary>Details</summary>
Motivation: 因信息不对称和传统文献计量指标数据积累延迟，传统方法难以及时评估不确定下的无形资产价值。

Method: 提出ERA - IT框架，将专利续期历史作为监督信号进行生态语义对齐，用10000个欧洲专利局专利数据训练模型预测价值层级并反向推导经济思维链。

Result: ERA - IT在预测准确性上显著优于传统计量模型和零样本大语言模型。

Conclusion: ERA - IT框架为决策者提供透明认知支撑，减少高风险知识产权管理中黑盒AI的不透明性。

Abstract: Valuing intangible assets under uncertainty remains a critical challenge in the strategic management of technological innovation due to the information asymmetry inherent in high-dimensional technical specifications. Traditional bibliometric indicators, such as citation counts, fail to address this friction in a timely manner due to the systemic latency inherent in data accumulation. To bridge this gap, this study proposes the Economic Reasoning Alignment via Instruction Tuning (ERA-IT) framework. We theoretically conceptualize patent renewal history as a revealed economic preference and leverage it as an objective supervisory signal to align the generative reasoning of Large Language Models (LLMs) with market realities, a process we term Eco-Semantic Alignment. Using a randomly sampled dataset of 10,000 European Patent Office patents across diverse technological domains, we trained the model not only to predict value tiers but also to reverse-engineer the Economic Chain-of-Thought from unstructured text. Empirical results demonstrate that ERA-IT significantly outperforms both conventional econometric models and zero-shot LLMs in predictive accuracy. More importantly, by generating explicit, logically grounded rationales for valuation, the framework serves as a transparent cognitive scaffold for decision-makers, reducing the opacity of black-box AI in high-stakes intellectual property management.

</details>


### [59] [On the impact of geometric variance on the performance of formed parts: A probabilistic approach on the example of airbag pressure bins](https://arxiv.org/abs/2512.13302)
*Lukas Schnelle,Niklas Fehlemann,Ali O. M. Kilicsoy,Niklas Bechler,Marcos A. Valdebenito,Yannis P. Korkolis,Matthias G. R. Faes,Sebastian Münstermann,Kai-Uwe Schröder*

Main category: cs.CE

TL;DR: 本文进行概率数值研究，评估几何方差对零件性能的影响，以气囊压力箱为例，旨在不改变制造工艺下设计更轻零件并实施更有效的质量保证。


<details>
  <summary>Details</summary>
Motivation: 制造导致的性能离散性是轻量化设计的挑战，几何变异性是离散性的一个因素，需评估其对零件性能的影响。

Method: 对几何偏差进行随机建模，建立几何方差与性能方差的相关性。

Result: 未提及具体结果。

Conclusion: 通过了解几何偏差与性能的相关性，可在不改变制造工艺下设计更轻零件并实施更有针对性和有效的质量保证。

Abstract: Scatter in properties resulting from manufacturing is a great challenge in lightweight design, requiring consideration of not only the average mechanical performance but also the variance which is done e.g., by conservative safety factors. One contributor to this variance is the inherent geometric variability in the formed part. To isolate and quantify this effect, we present a probabilistic numerical study, aiming to assess the impact of geometric variance on the resulting part performance. By modelling geometric deviations stochastically, we aim to establish a correlation between the variance in geometry with the resulting variance in performance. The study is done on the example of an airbag pressure bin, where a better understanding of this correlation is crucial, as it allows for the design of a lighter part without changing the manufacturing process. Instead, we aim to implement more targeted and effective quality assurance, informed by the performance impact of geometric deviations.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [60] [CoLSE: A Lightweight and Robust Hybrid Learned Model for Single-Table Cardinality Estimation using Joint CDF](https://arxiv.org/abs/2512.12624)
*Lankadinee Rathuwadu,Guanli Liu,Christopher Leckie,Renata Borovica-Gajic*

Main category: cs.DB

TL;DR: 提出CoLSE混合学习方法用于单表基数估计，实验显示其在多方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基数估计技术难以在准确性、效率和内存占用三个关键因素间取得平衡。

Method: 提出CoLSE方法，基于copula理论的新算法直接对查询区间的联合概率建模，集成轻量级神经网络纠正估计残差。

Result: CoLSE在准确性、训练时间、推理延迟和模型大小方面取得良好平衡，优于现有最先进方法。

Conclusion: CoLSE是一种有效的单表基数估计方法，能在多方面达到较好的权衡。

Abstract: Cardinality estimation (CE), the task of predicting the result size of queries is a critical component of query optimization. Accurate estimates are essential for generating efficient query execution plans. Recently, machine learning techniques have been applied to CE, broadly categorized into query-driven and data-driven approaches. Data-driven methods learn the joint distribution of data, while query-driven methods construct regression models that map query features to cardinalities. Ideally, a CE technique should strike a balance among three key factors: accuracy, efficiency, and memory footprint. However, existing state-of-the-art models often fail to achieve this balance.
  To address this, we propose CoLSE, a hybrid learned approach for single-table cardinality estimation. CoLSE directly models the joint probability over queried intervals using a novel algorithm based on copula theory and integrates a lightweight neural network to correct residual estimation errors. Experimental results show that CoLSE achieves a favorable trade-off among accuracy, training time, inference latency, and model size, outperforming existing state-of-the-art methods.

</details>


### [61] [Database Research needs an Abstract Relational Query Language](https://arxiv.org/abs/2512.12957)
*Wolfgang Gatterbauer,Diandre Miguel Sabale*

Main category: cs.DB

TL;DR: 随着查询更多由大语言模型生成，人类进行阅读验证，需重新思考关系语言设计，提出抽象关系查询语言（ARQL），并引入具体实例抽象关系演算（ARC）。


<details>
  <summary>Details</summary>
Motivation: 查询生成方式转变，需重新思考关系语言和接口设计。

Method: 提出ARQL概念，将查询分离为关系核心、模态和约定；引入ARC，提供三种模态。

Result: ARC为关系查询提供缺失词汇，可作为关系查询的通用工具。

Conclusion: 查询生成方式转变促使关系语言设计变革，ARQL和ARC有助于此变革。

Abstract: For decades, SQL has been the default language for composing queries, but it is increasingly used as an artifact to be read and verified rather than authored. With Large Language Models (LLMs), queries are increasingly machine-generated, while humans read, validate, and debug them. This shift turns relational query languages into interfaces for back-and-forth communication about intent, which will lead to a rethinking of relational language design, and more broadly, relational interface design.
  We argue that this rethinking needs support from an Abstract Relational Query Language (ARQL): a semantics-first reference metalanguage that separates query intent from user-facing syntax and makes underlying relational patterns explicit and comparable across user-facing languages. An ARQL separates a query into (i) a relational core (the compositional structure that determines intent), (ii) modalities (alternative representations of that core tailored to different audiences), and (iii) conventions (orthogonal environment-level semantic parameters under which the core is interpreted, e.g., set vs. bag semantics, or treatment of null values). Usability for humans or machines then depends less on choosing a particular language and more on choosing an appropriate modality. Comparing languages becomes a question of which relational patterns they support and what conventions they choose.
  We introduce Abstract Relational Calculus (ARC), a strict generalization of Tuple Relational Calculus (TRC), as a concrete instance of ARQL. ARC comes in three modalities: (i) a comprehension-style textual notation, (ii) an Abstract Language Tree (ALT) for machine reasoning about meaning, and (iii) a diagrammatic hierarchical graph (higraph) representation for humans. ARC provides the missing vocabulary and acts as a Rosetta Stone for relational querying.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [62] [Accelerating Sparse Matrix-Matrix Multiplication on GPUs with Processing Near HBMs](https://arxiv.org/abs/2512.12036)
*Shiju Li,Younghoon Min,Hane Yie,Hoshik Kim,Soohong Ahn,Joonseop Sim,Chul-Ho Lee,Jongryool Kim*

Main category: cs.DC

TL;DR: 本文提出基于哈希的多阶段SpGEMM和AIA技术优化GPU HBM上的SpGEMM，在多种图工作负载上性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决Sparse General Matrix - Matrix Multiplication (SpGEMM)因不规则内存访问模式成为瓶颈的问题。

Method: 提出基于哈希的多阶段SpGEMM和AIA技术，采用硬件 - 软件协同设计框架。

Result: 在多种图工作负载上有性能提升，如在图分析应用中AIA较纯软件实现最多减少17.3%时间，图收缩减少76.5%，马尔可夫聚类减少58.4%；GNN训练应用中混合方法较纯软件实现平均加速1.43倍，较cuSPARSE加速1.95倍，大规模数据集上最多加速4.18倍。

Conclusion: 所提出的方法在优化SpGEMM上效果显著，有实际应用价值。

Abstract: Sparse General Matrix-Matrix Multiplication (SpGEMM) is a fundamental operation in numerous scientific computing and data analytics applications, often bottlenecked by irregular memory access patterns. This paper presents Hash based Multi-phase SpGEMM on GPU and the Acceleration of Indirect Memory Access (AIA) technique, a novel custom near-memory processing approach to optimizing SpGEMM on GPU HBM. Our hardware-software co-designed framework for SpGEMM demonstrates significant performance improvements over state-of-the-art methods, particularly in handling complex, application-specific workloads. We evaluate our approach on various graph workloads, including graph contraction, Markov clustering, and Graph Neural Networks (GNNs), showcasing its practical applicability. For graph analytics applications, AIA demonstrates up to 17.3% time reduction from the software-only implementation, while achieving time reduction of 76.5% for Graph Contraction and 58.4% for Markov Clustering compared to cuSPARSE. For GNN training applications with structured global pruning, our hybrid approach delivers an average of 1.43x speedup over software-only implementation across six benchmark datasets and three architectures (GCN, GIN, GraphSAGE), and shows 1.95x speedup for GNN workloads when compared to cuSPARSE, with up to 4.18x gains on large-scale datasets.

</details>


### [63] [Near-Zero-Overhead Freshness for Recommendation Systems via Inference-Side Model Updates](https://arxiv.org/abs/2512.12295)
*Wenjun Yu,Sitian Chen,Cheng Chen,Amelie Chi Zhou*

Main category: cs.DC

TL;DR: 提出LiveUpdate系统以解决DLRMs的新鲜度 - 准确性权衡问题，可减少更新成本并提高准确性。


<details>
  <summary>Details</summary>
Motivation: DLRMs面临由大规模参数同步开销导致的新鲜度 - 准确性权衡问题，同步PB级嵌入表会造成数分钟陈旧性，降低推荐质量和收入。

Method: 在推理节点内 colocating Low - Rank Adaptation (LoRA) 训练器消除集群间同步；通过奇异值监测实现动态秩适应以约束内存开销；采用NUMA感知资源调度和硬件强制QoS消除更新推理争用。

Result: 与delta - update基线相比，LiveUpdate将更新成本降低2倍，在1小时窗口内实现更高准确性，比最先进的delta - update方法的准确性高0.04%到0.24%。

Conclusion: 将闲置推理资源转化为新鲜度引擎，可实现在线模型更新并提高性能。

Abstract: Deep Learning Recommendation Models (DLRMs) underpin personalized services but face a critical freshness-accuracy tradeoff due to massive parameter synchronization overheads. Production DLRMs deploy decoupled training/inference clusters, where synchronizing petabyte-scale embedding tables (EMTs) causes multi-minute staleness, degrading recommendation quality and revenue. We observe that (1) inference nodes exhibit sustained CPU underutilization (peak <= 20%), and (2) EMT gradients possess intrinsic low-rank structure, enabling compact update representation. We present LiveUpdate, a system that eliminates inter-cluster synchronization by colocating Low-Rank Adaptation (LoRA) trainers within inference nodes. LiveUpdate addresses two core challenges: (1) dynamic rank adaptation via singular value monitoring to constrain memory overhead (<2% of EMTs), and (2) NUMA-aware resource scheduling with hardware-enforced QoS to eliminate update inference contention (P99 latency impact <20ms). Evaluations show LiveUpdate reduces update costs by 2x versus delta-update baselines while achieving higher accuracy within 1-hour windows. By transforming idle inference resources into freshness engines, LiveUpdate delivers online model updates while outperforming state-of-the-art delta-update methods by 0.04% to 0.24% in accuracy.

</details>


### [64] [A Conflict-Aware Resource Management Framework for the Computing Continuum](https://arxiv.org/abs/2512.12299)
*Vlad Popescu-Vifor,Ilir Murturi,Praveen Kumar Donta,Schahram Dustdar*

Main category: cs.DC

TL;DR: 本文针对计算连续体资源编排中代理决策导致的问题，提出基于DRL的自适应冲突解决框架，在Kubernetes测试床验证其可行性和弹性，初步结果显示能实现高效资源重分配和自适应学习。


<details>
  <summary>Details</summary>
Motivation: 计算连续体中设备异构性和去中心化需求增加，代理决策会导致冲突循环、资源利用低效和服务性能下降，需解决资源编排冲突问题。

Method: 提出基于深度强化学习（DRL）的自适应冲突解决框架，利用实时性能反馈和历史状态信息训练DRL模型来调解冲突。

Result: 在Kubernetes测试床上对框架进行原型验证，初步结果表明该框架在动态场景中能实现高效资源重分配和自适应学习。

Conclusion: 该框架为计算连续体中感知冲突的编排提供了可扩展和有弹性的解决方案。

Abstract: The increasing device heterogeneity and decentralization requirements in the computing continuum (i.e., spanning edge, fog, and cloud) introduce new challenges in resource orchestration. In such environments, agents are often responsible for optimizing resource usage across deployed services. However, agent decisions can lead to persistent conflict loops, inefficient resource utilization, and degraded service performance. To overcome such challenges, we propose a novel framework for adaptive conflict resolution in resource-oriented orchestration using a Deep Reinforcement Learning (DRL) approach. The framework enables handling resource conflicts across deployments and integrates a DRL model trained to mediate such conflicts based on real-time performance feedback and historical state information. The framework has been prototyped and validated on a Kubernetes-based testbed, illustrating its methodological feasibility and architectural resilience. Preliminary results show that the framework achieves efficient resource reallocation and adaptive learning in dynamic scenarios, thus providing a scalable and resilient solution for conflict-aware orchestration in the computing continuum.

</details>


### [65] [Fine-Grained Energy Prediction For Parallellized LLM Inference With PIE-P](https://arxiv.org/abs/2512.12801)
*Anurag Dutt,Young Won Choi,Avirup Sil,Anshul Gandhi,Aruna Balasubramanian,Niranjan Balasubramanian*

Main category: cs.DC

TL;DR: 提出多GPU推理的细粒度能耗预测框架PIE - P，评估显示其预测准确，优于基线。


<details>
  <summary>Details</summary>
Motivation: 运行大语言模型（LLM）的能耗成关键问题，但精确测量难，现有预测技术不适用于多GPU推理。

Method: 通过精确采样、对GPU间通信进行细粒度建模和仔细计算并行开销，开发可扩展预测框架PIE - P。

Result: PIE - P在不同并行策略下能实现准确和细粒度的能耗预测，显著优于基线。

Conclusion: PIE - P能有效解决多GPU推理能耗预测问题。

Abstract: With the widespread adoption of Large Language Models (LLMs), energy costs of running LLMs is quickly becoming a critical concern. However, precisely measuring the energy consumption of LLMs is often infeasible because hardware-based power monitors are not always accessible and software-based energy measurement tools are not accurate. While various prediction techniques have been developed to estimate LLM energy consumption, these approaches are limited to single-GPU environments and thus are not applicable to modern LLM inference which is typically parallelized across multiple GPUs. In this work, we remedy this gap and introduce PIE-P, a fine-grained energy prediction framework for multi-GPU inference, including tensor, pipeline, and data parallelism. Predicting the energy under parallelized inference is complicated by the non-determinism in inter-GPU communication, additional communication overheads, and difficulties in isolating energy during the communication/synchronization phase. We develop a scalable prediction framework that addresses these issues via precise sampling, fine-grained modeling of inter-GPU communication, and careful accounting of parallelization overhead. Our evaluation results show that PIE-P yields accurate and fine-grained energy predictions across parallelism strategies, significantly outperforming baselines.

</details>


### [66] [HetRL: Efficient Reinforcement Learning for LLMs in Heterogeneous Environments](https://arxiv.org/abs/2512.12476)
*Yongjun He,Shuai Zhang,Jiading Gai,Xiyuan Zhang,Boran Han,Bernie Wang,Huzefa Rangwala,George Karypis*

Main category: cs.DC

TL;DR: 提出HetRL系统用于异构GPU和网络基础设施中高效强化学习训练，评估显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展使异构环境下LLM后训练需求增加，但在异构资源上进行高性能强化学习训练有挑战。

Method: 将异构环境下RL训练调度问题建模为约束联合优化问题，引入新调度算法，用多级搜索框架分解搜索空间，通过连续减半分配搜索预算。

Result: 经过20000 GPU - 小时评估，HetRL在不同工作负载和设置下，吞吐量最高达现有系统9.17倍，平均3.17倍。

Conclusion: HetRL能在异构GPU和网络基础设施中实现高效的强化学习训练。

Abstract: As large language models (LLMs) continue to scale and new GPUs are released even more frequently, there is an increasing demand for LLM post-training in heterogeneous environments to fully leverage underutilized mid-range or previous-generation GPUs across regions and alleviate the shortage of homogeneous high-end GPUs within a single region. However, achieving high-performance reinforcement learning (RL) training for LLMs on such computing resources remains challenging because the workflow involves multiple models and tasks with complex computation and data dependencies. In this paper, we present HetRL, a distributed system for efficient RL training in infrastructures with heterogeneous GPUs and networks. HetRL formulates the scheduling of RL training in heterogeneous environments as a constrained joint optimization problem and introduces a novel scheduling algorithm that (1) decomposes the complex search space with a multi-level search framework; and (2) allocates the search budget via successive halving. Our extensive evaluation, consuming 20,000 GPU-hours, shows that HetRL delivers up to 9.17x the throughput of state-of-the-art systems, and 3.17x on average, under various workloads and settings.

</details>


### [67] [astroCAMP: A Community Benchmark and Co-Design Framework for Sustainable SKA-Scale Radio Imaging](https://arxiv.org/abs/2512.13591)
*Denisa-Andreea Constantinescu,Rubén Rodríguez Álvarez,Jacques Morin,Etienne Orliac,Mickaël Dardaillon,Sunrise Wang,Hugo Miomandre,Miguel Peón-Quirós,Jean-François Nezan,David Atienza*

Main category: cs.DC

TL;DR: 针对SKA项目数据系统问题，提出astroCAMP框架，提供指标套件、数据集等，进行评估并呼吁社区定义指标。


<details>
  <summary>Details</summary>
Motivation: 当前射电干涉管道硬件性能利用率低、能效差、缺乏标准指标和容差，限制了SKA项目发展。

Method: 引入astroCAMP框架，提供统一指标套件、标准化数据集和多目标协同设计公式。

Result: 发布数据集、基准测试结果和可复现套件，评估协同设计指标，展示用于异构设计空间探索及识别帕累托最优运行点的潜力。

Conclusion: 呼吁SKA社区定义可量化的保真度指标和阈值，加速成像优化。

Abstract: The Square Kilometre Array (SKA) project will operate one of the world's largest continuous scientific data systems, sustaining petascale imaging under strict power caps. Yet, current radio-interferometric pipelines utilize only a small fraction of hardware peak performance, typically 4-14%, due to memory and I/O bottlenecks, resulting in poor energy efficiency and high operational and carbon costs. Progress is further limited by the absence of standardised metrics and fidelity tolerances, preventing principled hardware-software co-design and rigorous exploration of quality-efficiency trade-offs. We introduce astroCAMP, a framework for guiding the co-design of next-generation imaging pipelines and sustainable HPC architectures that maximise scientific return within SKA's operational and environmental limits. astroCAMP provides: (1) a unified, extensible metric suite covering scientific fidelity, computational performance, sustainability, and lifecycle economics; (2) standardised SKA-representative datasets and reference outputs enabling reproducible benchmarking across CPUs, GPUs, and emerging accelerators; and (3) a multi-objective co-design formulation linking scientific-quality constraints to time-, energy-, carbon-to-solution, and total cost of ownership. We release datasets, benchmarking results, and a reproducibility kit, and evaluate co-design metrics for WSClean and IDG on an AMD EPYC 9334 processor and an NVIDIA H100 GPU. Further, we illustrate the use of astroCAMP for heterogeneous CPU-FPGA design-space exploration, and its potential to facilitate the identification of Pareto-optimal operating points for SKA-scale imaging deployments. Last, we make a call to the SKA community to define quantifiable fidelity metrics and thresholds to accelerate principled optimisation for SKA-scale imaging.

</details>


### [68] [Strategic Server Deployment under Uncertainty in Mobile Edge Computing](https://arxiv.org/abs/2512.12532)
*Duc A. Tran,Dung Truong,Duy Le*

Main category: cs.DC

TL;DR: 研究移动边缘计算中服务器部署问题，将其建模为随机双层优化问题，用子模函数近似目标函数求解，实验表明算法优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算中服务器部署需同时考虑计算效率和通信效率，且用户工作负载和服务器有效容量未知且时变，需解决在此不确定性下的服务器部署问题。

Method: 将问题建模为随机双层优化问题，用子模函数近似目标函数，利用最先进的贪婪算法求解。

Result: 使用真实世界数据评估算法，结果显示其优于替代方法，改进幅度高达55%。

Conclusion: 所提出的算法能在服务器部署的不确定性下有效实现计算和通信效率目标。

Abstract: Server deployment is a fundamental task in mobile edge computing: where to place the edge servers and what user cells to assign to them. To make this decision is context-specific, but common goals are 1) computing efficiency: maximize the amount of workload processed by the edge, and 2) communication efficiency: minimize the communication cost between the cells and their assigned servers. We focus on practical scenarios where the user workload in each cell is unknown and time-varying, and so are the effective capacities of the servers. Our research problem is to choose a subset of candidate servers and assign them to the user cells such that the above goals are sustainably achieved under the above uncertainties. We formulate this problem as a stochastic bilevel optimization, which is strongly NP-hard and unseen in the literature. By approximating the objective function with submodular functions, we can utilize state-of-the-art greedy algorithms for submodular maximization to effectively solve our problem. We evaluate the proposed algorithm using real-world data, showing its superiority to alternative methods; the improvement can be as high as 55%

</details>


### [69] [Ethical Risk Analysis of L2 Rollups](https://arxiv.org/abs/2512.12732)
*Georgy Ishmaev,Emmanuelle Anceaume,Davide Frey,François Taïani*

Main category: cs.DC

TL;DR: 分析Layer 2 rollups的运营商和治理设计带来的用户风险，提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: Layer 2 rollups虽改善吞吐量和费用，但因运营商裁量权和信息不对称带来风险，需研究哪些设计会产生伦理问题的用户风险。

Method: 将伦理风险分析应用于rollup架构，构建基于角色的决策权限和风险暴露分类法，结合L2BEAT的129个项目快照和2022 - 2025年的事件集进行分析。

Result: 发现L2组件控制安排带来的伦理风险普遍存在，如约86%项目有无退出窗口的即时升级，约50%有可冻结提款的提议者控制，报告事件集中在定序器活性和包含性。

Conclusion: 基于研究结果提出包括技术组件和治理机制的缓解策略。

Abstract: Layer 2 rollups improve throughput and fees, but can reintroduce risk through operator discretion and information asymmetry. We ask which operator and governance designs produce ethically problematic user risk. We adapt Ethical Risk Analysis to rollup architectures, build a role-based taxonomy of decision authority and exposure, and pair the framework with two empirical signals, a cross sectional snapshot of 129 projects from L2BEAT and a hand curated incident set covering 2022 to 2025. We analyze mechanisms that affect risks to users funds, including upgrade timing and exit windows, proposer liveness and whitelisting, forced inclusion usability, and data availability choices. We find that ethical hazards rooted in L2 components control arrangements are widespread: instant upgrades without exit windows appear in about 86 percent of projects, and proposer controls that can freeze withdrawals in about 50 percent. Reported incidents concentrate in sequencer liveness and inclusion, consistent with these dependencies. We translate these findings into ethically grounded suggestions on mitigation strategies including technical components and governance mechanisms.

</details>


### [70] [Temporal parallelisation of continuous-time maximum-a-posteriori trajectory estimation](https://arxiv.org/abs/2512.13319)
*Hassan Razavi,Ángel F. García-Fernández,Simo Särkkä*

Main category: cs.DC

TL;DR: 本文提出用于计算部分观测随机微分方程状态的连续时间最大后验轨迹估计的并行时间方法，可在并行架构上提高计算速度。


<details>
  <summary>Details</summary>
Motivation: 提升部分观测随机微分方程状态的连续时间最大后验轨迹估计在并行架构上的计算速度。

Method: 将最大后验估计问题重构成基于翁萨格 - 马赫卢普泛函的连续时间最优控制问题，采用并行关联扫描算法并行求解，线性方法通过泰勒展开扩展到非线性模型。

Result: 在GPU实验中，所提框架在保持精度的同时显著加快了计算速度。

Conclusion: 所提并行时间方法能有效提升计算速度且保证计算精度。

Abstract: This paper proposes a parallel-in-time method for computing continuous-time maximum-a-posteriori (MAP) trajectory estimates of the states of partially observed stochastic differential equations (SDEs), with the goal of improving computational speed on parallel architectures. The MAP estimation problem is reformulated as a continuous-time optimal control problem based on the Onsager-Machlup functional. This reformulation enables the use of a previously proposed parallel-in-time solution for optimal control problems, which we adapt to the current problem. The structure of the resulting optimal control problem admits a parallel solution based on parallel associative scan algorithms. In the linear Gaussian special case, it yields a parallel Kalman-Bucy filter and a parallel continuous-time Rauch-Tung-Striebel smoother. These linear computational methods are further extended to nonlinear continuous-time state-space models through Taylor expansions. We also present the corresponding parallel two-filter smoother. The graphics processing unit (GPU) experiments on linear and nonlinear models demonstrate that the proposed framework achieves a significant speedup in computations while maintaining the accuracy of sequential algorithms.

</details>


### [71] [PROSERVE: Unified Multi-Priority Request Scheduling for LLM Serving](https://arxiv.org/abs/2512.12928)
*Weizhe Huang,Tao Peng,Tongxuan Liu,Donghe Jin,Xianzhe Dong,Ke Zhang*

Main category: cs.DC

TL;DR: 现有大语言模型服务调度器无法同时优化SLO达成率和客户端优先级，本文提出PROSERVE框架解决该问题，评估显示其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型交互式应用需处理大量并发请求，现有调度器无法同时优化SLO达成率和客户端优先级。

Method: 将多优先级请求调度形式化为服务收益最大化问题，提出PROSERVE框架，包括引擎级的SlideBatching和服务级的GoRouting。

Result: 在四个开源数据集和一个真实工业轨迹上的评估表明，PROSERVE比现有基线系统增益提高35%，SLO达成率提高52%。

Conclusion: PROSERVE能有效解决现有大语言模型服务调度器的问题，提升系统性能。

Abstract: The widespread deployment of large language models (LLMs) for interactive applications necessitates serving systems that can handle thousands of concurrent requests with diverse Service Level Objective (SLO) requirements. A critical yet often overlooked dimension in this context is the inherent priority difference among clients; for instance, business-critical functions demand higher performance guarantees, as fulfilling such requests yields significantly greater business value. However, existing LLM serving schedulers fail to jointly optimize for both SLO attainment and client-level priorities.
  To bridge this gap, we first \textit{formalize multi-priority request scheduling as a service gain maximization problem}, where satisfying latency requirements for requests of different priorities contributes varying levels of gain. We then propose PROSERVE, a unified two-tier scheduling framework designed to maximize overall service gain. At the engine level, SlideBatching dynamically adapts batch formation and request ordering under varying load conditions, employing a sliding boundary mechanism to balance deadline-first and density-first strategies. At the service level, GoRouting performs gain-oriented and capability-aware dispatching across distributed instances, proactively reserving capacity for future high-priority or long requests. Extensive evaluation across four open-source datasets and a real-world industrial trace demonstrates that \systemname{} consistently outperforms state-of-the-art baselines, improving system gain by up to 35% and boosting SLO attainment by up to 52%.

</details>


### [72] [FlashFuser: Expanding the Scale of Kernel Fusion for Compute-Intensive Operators via Inter-Core Connection](https://arxiv.org/abs/2512.12949)
*Ziyu Huang,Yangjie Zhou,Zihan Liu,Xinhao Luo,Yijia Diao,Minyi Guo,Jidong Zhai,Yu Feng,Chen Zhang,Anbang Wu,Jingwen Leng*

Main category: cs.DC

TL;DR: 现有编译器和框架的内核融合策略有局限，本文提出FlashFuser利用GPU内核间连接进行内核融合，评估显示其能减少内存访问并提升速度。


<details>
  <summary>Details</summary>
Motivation: 计算吞吐量增长快于内存带宽提升，使深度学习工作负载受内存限制，现有内核融合策略有局限，现代GPU的分布式共享内存硬件潜力未被软件框架利用。

Method: 提出基于DSM的通信抽象，引入数据流分析器，将组件集成到统一搜索引擎，采用分析成本建模和DSM感知剪枝策略。

Result: 在NVIDIA H100 GPU上评估，FlashFuser减少58%内存访问，相比高度调优库和最先进编译器，内核加速分别达3.3倍和4.1倍，端到端加速1.24倍。

Conclusion: FlashFuser有效利用现代GPU的内核间连接进行内核融合，提升了性能。

Abstract: The scaling of computation throughput continues to outpace improvements in memory bandwidth, making many deep learning workloads memory-bound. Kernel fusion is a key technique to alleviate this problem, but the fusion strategies of existing compilers and frameworks are limited to using local scratchpad memory. When the intermediate results exceed the limited capacity (such as FFN), the fusion fails. Although modern GPUs (like the NVIDIA H100) now incorporate an inter-core connection mechanism known as Distributed Shared Memory(DSM)--providing a larger, high-bandwidth, and low-latency on-chip memory pool--this hardware potential has yet to be exploited by software frameworks. To bridge this gap, we present FlashFuser, the first compiler framework to utilize inter-core connection for kernel fusion on modern GPUs. FlashFuser extends established fusion techniques to the DSM domain through three core contributions. First, we propose a powerful DSM-based communication abstraction that formalizes complex cluster-based data exchange patterns, such as reduce, shuffle and multiply. Second, we introduce a dataflow analyzer that generalizes loop scheduling, resource mapping, and tile selection to the distributed memory hierarchy; it determines the optimal execution order and tile sizes by quantifying data movement across memory levels. Finally, FlashFuser integrates these components into a unified search engine that employs analytical cost modeling and DSM-aware pruning strategies to efficiently discover the optimal execution plan. Our evaluation on an NVIDIA H100 GPU shows that FlashFuser reduces memory access by 58% and delivers kernel speedups of 3.3x against highly-tuned libraries and 4.1x against state-of-the-art compilers, resulting in a 1.24x end-to-end speedup.

</details>


### [73] [Toward Self-Healing Networks-on-Chip: RL-Driven Routing in 2D Torus Architectures](https://arxiv.org/abs/2512.13096)
*Mohammad Walid Charrwi,Zaid Hussain*

Main category: cs.DC

TL;DR: 研究2D环形片上网络在节点故障下的自适应最小路由，对比RL策略和自适应路由基线，结果显示RL方法在吞吐量和容错性上有明显优势。


<details>
  <summary>Details</summary>
Motivation: 在节点故障条件下，探索2D环形片上网络更好的自适应最小路由策略。

Method: 将基于强化学习（RL）的策略与自适应路由基线进行对比。RL将每个路由器建模为智能体，根据网络状态学习转发数据包；自适应方案使用固定最小路径并简单绕过故障。在模拟中实现两种方法，随机注入最多50个节点故障并测量关键指标。

Result: RL方法在高负载下吞吐量显著提高约20 - 30%，在故障增加时保持更高可靠性。RL路由器每周期交付更多数据包，利用路径多样性适应故障；自适应方案在故障累积时性能急剧下降。RL方法在约30 - 40个故障时端到端连接性更好，PDR保持在90%以上，而自适应方案PDR降至约70%。故障自适应得分也更青睐RL路由。

Conclusion: 基于RL的自适应路由在环形片上网络的吞吐量和容错性方面有明显优势。

Abstract: We investigate adaptive minimal routing in 2D torus networks on chip NoCs under node fault conditions comparing a reinforcement learning RL based strategy to an adaptive routing baseline A torus topology is used for its low diameter high connectivity properties The RL approach models each router as an agent that learns to forward packets based on network state while the adaptive scheme uses fixed minimal paths with simple rerouting around faults We implement both methods in simulation injecting up to 50 node faults uniformly at random Key metrics are measured 1 throughput vs offered load at fault density 02 2 packet delivery ratio PDR vs fault density and 3 a fault adaptive score FT vs fault density Experimental results show the RL method achieves significantly higher throughput at high load approximately 2030 gain and maintains higher reliability under increasing faults The RL router delivers more packets per cycle and adapts to faults by exploiting path diversity whereas the adaptive scheme degrades sharply as faults accumulate In particular the RL approach preserves end to end connectivity longer PDR remains above 90 until approximately 3040 faults while adaptive PDR drops to approximately 70 at the same point The fault adaptive score likewise favors RL routing Thus RL based adaptive routing demonstrates clear advantages in throughput and fault resilience for torus NoCs

</details>


### [74] [SPARS: A Reinforcement Learning-Enabled Simulator for Power Management in HPC Job Scheduling](https://arxiv.org/abs/2512.13268)
*Muhammad Alfian Amrizal,Raka Satya Prasasta,Santana Yuda Pradata,Kadek Gemilang Santiyuda,Reza Pulungan,Hiroyuki Takizawa*

Main category: cs.DC

TL;DR: 介绍了用于HPC作业调度中电源管理的强化学习模拟器SPARS，它灵活、可复现、易扩展，能帮助评估电源感知调度策略


<details>
  <summary>Details</summary>
Motivation: HPC集群能耗大，闲置节点是主要浪费源，关机时机不当会影响性能，需平衡能耗与性能

Method: 在离散事件模拟框架中集成作业调度和节点电源状态管理，支持传统调度策略和强化学习变体，用户可用JSON配置，记录综合指标并可视化

Result: 相比Batsim框架，SPARS轻量级处理事件，结果一致，实验易复现和扩展，模块化设计便于集成新算法

Conclusion: SPARS提供灵活可扩展平台，助于系统评估电源感知调度策略，探索能效与性能权衡，加速可持续HPC运营发展

Abstract: High-performance computing (HPC) clusters consume enormous amounts of energy, with idle nodes as a major source of waste. Powering down unused nodes can mitigate this problem, but poorly timed transitions introduce long delays and reduce overall performance. To address this trade-off, we present SPARS, a reinforcement learning-enabled simulator for power management in HPC job scheduling. SPARS integrates job scheduling and node power state management within a discrete-event simulation framework. It supports traditional scheduling policies such as First Come First Served and EASY Backfilling, along with enhanced variants that employ reinforcement learning agents to dynamically decide when nodes should be powered on or off. Users can configure workloads and platforms in JSON format, specifying job arrivals, execution times, node power models, and transition delays. The simulator records comprehensive metrics-including energy usage, wasted power, job waiting times, and node utilization-and provides Gantt chart visualizations to analyze scheduling dynamics and power transitions. Unlike widely used Batsim-based frameworks that rely on heavy inter-process communication, SPARS provides lightweight event handling and consistent simulation results, making experiments easier to reproduce and extend. Its modular design allows new scheduling heuristics or learning algorithms to be integrated with minimal effort. By providing a flexible, reproducible, and extensible platform, SPARS enables researchers and practitioners to systematically evaluate power-aware scheduling strategies, explore the trade-offs between energy efficiency and performance, and accelerate the development of sustainable HPC operations.

</details>


### [75] [SIGMA: An AI-Empowered Training Stack on Early-Life Hardware](https://arxiv.org/abs/2512.13488)
*Lei Qu,Lianhai Ren,Peng Cheng,Rui Gao,Ruizhe Wang,Tianyu Chen,Xiao Liu,Xingjian Zhang,Yeyun Gong,Yifan Xiong,Yucheng Ding,Yuting Jiang,Zhenghao Lin,Zhongxin Guo,Ziyue Yang*

Main category: cs.DC

TL;DR: SIGMA是开源训练栈，含LTP和LTF，提升早期AI硬件大规模训练的可靠性、稳定性和效率，有出色成果并开源代码。


<details>
  <summary>Details</summary>
Motivation: 早期AI加速器进行大规模训练面临可靠性、正确性和收敛性以及效率方面的挑战，需解决方案。

Method: 设计开源训练栈SIGMA，核心是为早期AI加速器集群优化的LTP系统，在此基础上有LTF。

Result: LTP提升训练可靠性和运营生产力，集群加速器利用率达94.45%；LTF用2048个加速器训练200B的SIGMA - MOE模型，MFU达21.08%，下游准确率达最优，75天仅一次稳定性事故。

Conclusion: SIGMA解决大规模训练关键挑战，为AI基础设施和平台创新树立新标杆，是现有加速器栈的有力替代方案，推动AI能力和可扩展性发展。

Abstract: An increasing variety of AI accelerators is being considered for large-scale training. However, enabling large-scale training on early-life AI accelerators faces three core challenges: frequent system disruptions and undefined failure modes that undermine reliability; numerical errors and training instabilities that threaten correctness and convergence; and the complexity of parallelism optimization combined with unpredictable local noise that degrades efficiency. To address these challenges, SIGMA is an open-source training stack designed to improve the reliability, stability, and efficiency of large-scale distributed training on early-life AI hardware. The core of this initiative is the LUCIA TRAINING PLATFORM (LTP), the system optimized for clusters with early-life AI accelerators. Since its launch in March 2025, LTP has significantly enhanced training reliability and operational productivity. Over the past five months, it has achieved an impressive 94.45% effective cluster accelerator utilization, while also substantially reducing node recycling and job-recovery times. Building on the foundation of LTP, the LUCIA TRAINING FRAMEWORK (LTF) successfully trained SIGMA-MOE, a 200B MoE model, using 2,048 AI accelerators. This effort delivered remarkable stability and efficiency outcomes, achieving 21.08% MFU, state-of-the-art downstream accuracy, and encountering only one stability incident over a 75-day period. Together, these advances establish SIGMA, which not only tackles the critical challenges of large-scale training but also establishes a new benchmark for AI infrastructure and platform innovation, offering a robust, cost-effective alternative to prevailing established accelerator stacks and significantly advancing AI capabilities and scalability. The source code of SIGMA is available at https://github.com/microsoft/LuciaTrainingPlatform.

</details>


### [76] [Janus: Disaggregating Attention and Experts for Scalable MoE Inference](https://arxiv.org/abs/2512.13525)
*Zhexiang Zhang,Ye Wang,Xiangyu Wang,Yumiao Zhao,Jingzhe Jiang,Qizhen Weng,Shaohuai Shi,Yin Chen,Minchen Yu*

Main category: cs.DC

TL;DR: 提出可扩展的MoE推理系统Janus，通过分离模块、采用三项关键设计，实现高效推理，评估显示性能优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 大混合专家（MoE）模型推理资源需求高、工作负载动态，现有解决方案将模型作为整体部署，无法满足不同模块需求，导致可扩展性和资源效率受限。

Method: 将注意力和专家模块分离到不同GPU子集群；采用自适应两阶段通信方案；引入轻量级调度器并实现为GPU内核；进行细粒度资源管理。

Result: Janus比现有系统实现了高达3.9倍的每GPU吞吐量，同时满足每令牌延迟要求。

Conclusion: Janus可扩展且高效，能提高MoE模型推理的整体效率。

Abstract: Large Mixture-of-Experts (MoE) model inference is challenging due to high resource demands and dynamic workloads. Existing solutions often deploy the entire model as a single monolithic unit, which applies a unified resource configuration to both attention and expert modules despite their different requirements, leading to limited scalability and resource inefficiency. In this paper, we propose Janus, a scalable MoE inference system that disaggregates attention and experts on separate GPU sub-clusters, enabling each module to be managed and scaled independently. Janus incorporates three key designs for efficient, disaggregated MoE inference. First, it proposes an adaptive two-phase communication scheme that exploits intra- and inter-node bandwidth hierarchies for low-latency data exchange. Second, motivated by the memory-bound nature of MoE modules, Janus introduces a lightweight scheduler and implements it as a GPU kernel to balance the number of activated experts across GPUs at minimal overhead, thereby reducing inference latency. Third, Janus performs fine-grained resource management to dynamically adjust expert placement and independently scale attention and MoE resources to improve overall efficiency. Evaluation shows Janus achieves up to 3.9 higher perGPU throughput than state-of-the-art systems while meeting per-token latency requirements.

</details>


### [77] [Design in Tiles: Automating GEMM Deployment on Tile-Based Many-PE Accelerators](https://arxiv.org/abs/2512.13638)
*Aofeng Shen,Chi Zhang,Yakup Budanaz,Alexandru Calotoiu,Torsten Hoefler,Luca Benini*

Main category: cs.DC

TL;DR: 提出针对Tile - based多处理单元加速器的自动化框架DiT，在GEMM任务上比NVIDIA GH200有更好的性能。


<details>
  <summary>Details</summary>
Motivation: Tile - based多处理单元加速器在GEMM上虽性能好但编程困难，其最优软件映射与硬件设计深度耦合，手动部署不便。

Method: 提出“Design in Tiles (DiT)”自动化框架，连接部署工具链和可配置可执行模型。

Result: 将框架应用于GEMM任务，对比NVIDIA GH200，在不同矩阵形状下实现1.2 - 2.0倍的加速，且处理单元利用率更高。

Conclusion: DiT框架能有效解决Tile - based多处理单元加速器编程难题，并取得良好性能提升。

Abstract: Tile-based many-Processing Element (PE) accelerators can achieve competitive performance on General Matrix Multiplication (GEMM), but they are extremely hard to program, as their optimal software mapping is deeply coupled with hardware design which is unwieldy to manual deployment. We propose "Design in Tiles (DiT)", an automated framework connecting a deployment toolchain with a configurable executable model for these accelerators. For evaluation, we apply our framework to GEMM targeting a large acceleration configuration (e.g., 32x32 tiles, 1979 TFLOPS@FP8, 4 TB/s Bandwidth) comparable to an NVIDIA GH200. We achieve higher PE utilization than GH200 with its expert-tuned GEMM libraries, achieving 1.2-2.0x speedup across diverse matrix shapes.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [78] [Optimal non-adaptive algorithm for edge estimation](https://arxiv.org/abs/2512.11994)
*Arijit Bishnu,Debarshi Chanda,Buddha Dev Das,Arijit Ghosh,Gopinath Mishra*

Main category: cs.DS

TL;DR: 提出简单非自适应随机算法估计图的边数，查询复杂度为次线性，证明匹配下界。


<details>
  <summary>Details</summary>
Motivation: 解决使用度查询和随机边查询估计简单无向图边数的非自适应查询复杂度问题。

Method: 独立采样一组顶点并查询其度，独立采样一组边，用查询结果估计总边数。

Result: 对于n个顶点的图，方法仅需$\widetilde{O}(\sqrt{n})$次查询，实现了次线性查询复杂度。

Conclusion: 证明了匹配下界，确立算法最优性，解决了该问题的非自适应查询复杂度。

Abstract: We present a simple nonadaptive randomized algorithm that estimates the number of edges in a simple, unweighted, undirected graph, possibly containing isolated vertices, using only degree and random edge queries. For an $n$-vertex graph, our method requires only $\widetilde{O}(\sqrt{n})$ queries, achieving sublinear query complexity. The algorithm independently samples a set of vertices and queries their degrees, and also independently samples a set of edges, using the answers to these queries to estimate the total number of edges in the graph. We further prove a matching lower bound, establishing the optimality of our algorithm and resolving the non-adaptive query complexity of this problem with respect to degree and random-edge queries.

</details>


### [79] [Load Balancing with Duration Predictions](https://arxiv.org/abs/2512.12202)
*Yossi Azar,Niv Buchbinder,Tomer Epshtein*

Main category: cs.DS

TL;DR: 研究无关机器上的全动态负载均衡问题，提出含作业时长估计的更现实模型，设计性能与预测精度相关的算法并证明下界。


<details>
  <summary>Details</summary>
Motivation: 以往研究集中于已知或未知作业时长的场景，为填补两者间的空白，研究有作业时长估计的更现实模型。

Method: 设计能使性能与可用预测精度平滑相关的算法，同时证明使用不准确预测的算法的竞争力下界。

Result: 发现将预测直接融入为预知场景设计的经典负载均衡算法会导致性能显著下降。

Conclusion: 设计出了性能依赖于预测精度的更好算法，并给出了使用不准确预测的算法的竞争力下界。

Abstract: We study the classic fully dynamic load balancing problem on unrelated machines where jobs arrive and depart over time and the goal is minimizing the maximum load, or more generally the l_p-norm of the load vector. Previous work either studied the clairvoyant setting in which exact durations are known to the algorithm, or the unknown duration setting in which no information on the duration is given to the algorithm. For the clairvoyant setting algorithms with polylogarithmic competitive ratios were designed, while for the unknown duration setting strong lower bounds exist and only polynomial competitive factors are possible.
  We bridge this gap by studying a more realistic model in which some estimate/prediction of the duration is available to the algorithm. We observe that directly incorporating predictions into classical load balancing algorithms designed for the clairvoyant setting can lead to a notable decline in performance. We design better algorithms whose performance depends smoothly on the accuracy of the available prediction. We also prove lower bounds on the competitiveness of algorithms that use such inaccurate predictions.

</details>


### [80] [Learning with Structure: Computing Consistent Subsets on Structurally-Regular Graphs](https://arxiv.org/abs/2512.12860)
*Aritra Banik,Mano Prakash Parthasarathi,Venkatesh Raman,Diya Roy,Abhishek Sahu*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Minimum Consistent Subset (MCS) problem arises naturally in the context of supervised clustering and instance selection. In supervised clustering, one aims to infer a meaningful partitioning of data using a small labeled subset. However, the sheer volume of training data in modern applications poses a significant computational challenge. The MCS problem formalizes this goal: given a labeled dataset $\mathcal{X}$ in a metric space, the task is to compute a smallest subset $S \subseteq \mathcal{X}$ such that every point in $\mathcal{X}$ shares its label with at least one of its nearest neighbors in $S$.
  Recently, the MCS problem has been extended to graph metrics, where distances are defined by shortest paths. Prior work has shown that MCS remains NP-hard even on simple graph classes like trees, though an algorithm with runtime $\mathcal{O}(2^{6c} \cdot n^6)$ is known for trees, where $c$ is the number of colors and $n$ the number of vertices. This raises the challenge of identifying graph classes that admit algorithms efficient in both $n$ and $c$.
  In this work, we study the Minimum Consistent Subset problem on graphs, focusing on two well-established measures: the vertex cover number ($vc$) and the neighborhood diversity ($nd$). We develop an algorithm with running time $vc^{\mathcal{O}(vc)}\cdot\text{Poly}(n,c)$, and another algorithm with runtime $nd^{\mathcal{O}(nd)}\cdot\text{Poly}(n,c)$. In the language of parameterized complexity, this implies that MCS is fixed-parameter tractable (FPT) parameterized by the vertex cover number and the neighborhood diversity. Notably, our algorithms remain efficient for arbitrarily many colors, as their complexity is polynomially dependent on the number of colors.

</details>


### [81] [Sub-$n^k$ Deterministic algorithm for minimum $k$-way cut in simple graphs](https://arxiv.org/abs/2512.12900)
*Mohit Daga*

Main category: cs.DS

TL;DR: 提出简单图最小k - 割问题的确定性精确算法，结合PSP和KT收缩，给出运行时间分析及相关结论。


<details>
  <summary>Details</summary>
Motivation: 解决简单图上的最小k - 割问题。

Method: 将从理想负载导出的PSP与在临界PSP阈值的KT收缩相结合，利用结构分解定理，构建规范边界族，进行分支操作，还结合简单元约简。

Result: 得到算法运行时间公式，在特定条件下有确定性次$n^k$时间算法，结合框架与子例程得到$n^{ck + O(1)}$算法。

Conclusion: 所提出算法能有效解决最小k - 割问题，运行时间表现良好，与随机算法界的指数对齐。

Abstract: We present a \emph{deterministic exact algorithm} for the \emph{minimum $k$-cut problem} on simple graphs.
  Our approach combines the \emph{principal sequence of partitions (PSP)}, derived canonically from ideal loads, with a single level of \emph{Kawarabayashi--Thorup (KT)} contractions at the critical PSP threshold~$λ_j$.
  Let $j$ be the smallest index with $κ(P_j)\ge k$ and $R := k - κ(P_{j-1})$.
  We prove a structural decomposition theorem showing that an optimal $k$-cut can be expressed as the level-$(j\!-\!1)$ boundary $A_{\le j-1}$ together with exactly $(R-r)$ \emph{non-trivial} internal cuts of value at most~$λ_j$ and $r$ \emph{singleton isolations} (``islands'') inside the parts of~$P_{j-1}$.
  At this level, KT contractions yield kernels of total size $\widetilde{O}(n / λ_j)$, and from them we build a \emph{canonical border family}~$\mathcal{B}$ of the same order that deterministically covers all optimal refinement choices.
  Branching only over~$\mathcal{B}$ (and also including an explicit ``island'' branch) gives total running time
  $$
  T(n,m,k) = \widetilde{O}\left(\mathrm{poly}(m)+\Bigl(\tfrac{n}{λ_j}+n^{ω/3}\Bigr)^{R}\right),
  $$
  where $ω< 2.373$ is the matrix multiplication exponent.
  In particular, if $λ_j \ge n^{\varepsilon}$ for some constant $\varepsilon > 0$, we obtain a \emph{deterministic sub-$n^k$-time algorithm}, running in $n^{(1-\varepsilon)(k-1)+o(k)}$ time.
  Finally, combining our PSP$\times$KT framework with a small-$λ$ exact subroutine via a simple meta-reduction yields a deterministic $n^{c k+O(1)}$ algorithm for $c = \max\{ t/(t+1), ω/3 \} < 1$, aligning with the exponent in the randomized bound of He--Li (STOC~2022) under the assumed subroutine.

</details>


### [82] [Deterministic and Exact Fully-dynamic Minimum Cut of Superpolylogarithmic Size in Subpolynomial Time](https://arxiv.org/abs/2512.13105)
*Antoine El-Hayek,Monika Henzinger,Jason Li*

Main category: cs.DS

TL;DR: 提出精确全动态最小割算法，改进前人结果，结合图稀疏化得到近似算法，有确定性局部最小割算法。


<details>
  <summary>Details</summary>
Motivation: 改进现有全动态最小割算法的最小割大小限制，获得近似算法。

Method: 提出确定性局部最小割算法替代原有随机过程，结合图稀疏化技术。

Result: 得到精确全动态最小割算法，在特定最小割大小下有 $n^{o(1)}$ 确定性更新时间；获得 $(1+ε)$ - 近似全动态最小割算法，有 $n^{o(1)}$ 随机更新时间。

Conclusion: 新算法在全动态最小割问题上有显著改进，拓展了算法适用范围。

Abstract: We present an exact fully-dynamic minimum cut algorithm that runs in $n^{o(1)}$ deterministic update time when the minimum cut size is at most $2^{Θ(\log^{3/4-c}n)}$ for any $c>0$, improving on the previous algorithm of Jin, Sun, and Thorup (SODA 2024) whose minimum cut size limit is $(\log n)^{o(1)}$. Combined with graph sparsification, we obtain the first $(1+ε)$-approximate fully-dynamic minimum cut algorithm on weighted graphs, for any $ε\ge2^{-Θ(\log^{3/4-c}n)}$, in $n^{o(1)}$ randomized update time.
  Our main technical contribution is a deterministic local minimum cut algorithm, which replaces the randomized LocalKCut procedure from El-Hayek, Henzinger, and Li (SODA 2025).

</details>


### [83] [Kernelization dichotomies for hitting minors under structural parameterizations](https://arxiv.org/abs/2512.13210)
*Marin Bougeret,Eric Brandwein,Ignasi Sau*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: For a finite collection of connected graphs $\mathcal{F}$, the $\mathcal{F}$-MINOR-DELETION problem consists in, given a graph $G$ and an integer $\ell$, deciding whether $G$ contains a vertex set of size at most $\ell$ whose removal results in an $\mathcal{F}$-minor-free graph. We lift the existence of (approximate) polynomial kernels for $\mathcal{F}$-MINOR-DELETION by the solution size to (approximate) polynomial kernels parameterized by the vertex-deletion distance to graphs of bounded elimination distance to $\mathcal{F}$-minor-free graphs. This results in exact polynomial kernels for every family $\mathcal{F}$ that contains a planar graph, and an approximate polynomial kernel for PLANAR VERTEX DELETION. Moreover, combining our result with a previous lower bound, we obtain the following infinite set of dichotomies, assuming $NP \not\subseteq coNP/poly$: for any finite set $\mathcal{F}$ of biconnected graphs on at least three vertices containing a planar graph, and any minor-closed class of graphs $\mathcal{C}$, $\mathcal{F}$-MINOR-DELETION admits a polynomial kernel parameterized by the vertex-deletion distance to $\mathcal{C}$ if and only if $\mathcal{C}$ has bounded elimination distance to $\mathcal{F}$-minor-free graphs. For instance, this yields dichotomies for CACTUS VERTEX DELETION, OUTERPLANAR VERTEX DELETION, and TREEWIDTH-$t$ VERTEX DELETION for every integer $t \geq 0$. Prior to our work, such dichotomies were only known for the particular cases of VERTEX COVER and FEEDBACK VERTEX SET. Our approach builds on the techniques developed by Jansen and Pieterse [Theor. Comput. Sci. 2020] and also uses adaptations of some of the results by Jansen, de Kroon, and Wlodarczyk [STOC 2021].

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [84] [Differentially Private Online Distributed Aggregative Games With Time-Varying and Non-Identical Communication and Feedback Delays](https://arxiv.org/abs/2512.12344)
*Olusola Odeyomi,Tokunbo Ogunfunmi,Adjovi Laba*

Main category: cs.GT

TL;DR: 本文研究带时变成本函数的在线分布式聚合博弈，解决隐私泄露与时变、非相同通信和反馈延迟问题，提出算法并证明低遗憾界，模拟显示客户端局部动作运行平均收敛。


<details>
  <summary>Details</summary>
Motivation: 现有研究未考虑时变且非相同的延迟情况，同时存在好奇代理窃取信息及通信、反馈延迟阻碍收敛问题。

Method: 提出在线分布式对偶平均算法。

Result: 模拟结果显示每个客户端局部动作的运行平均随时间收敛。

Conclusion: 所提算法能同时应对隐私泄露与时变、非相同通信和反馈延迟问题，且有可证明的低遗憾界。

Abstract: This paper investigates online distributed aggregative games with time-varying cost functions, where agents are interconnected through an unbalanced communication graph. Due to the distributed and noncooperative nature of the game, some curious agents may wish to steal sensitive information from neighboring agents during parameter exchanges. Additionally, communication delays arising from network congestion, particularly in wireless settings, as well as feedback delays, can hinder the convergence of agents to a Nash equilibrium. Although a recent work addressed both communication and feedback delays in aggregative games, it is based on the unrealistic assumption that the delays are fixed over time and identical across agents. Hence, the case of time-varying and non-identical delays across agents has never been considered in aggregative games. In this work, we address the combined challenges of privacy leakage with time-varying and non-identical communication and feedback delays for the first time. We propose an online distributed dual averaging algorithm that simultaneously tackles these challenges while achieving a provably low regret bound. Our simulation result shows that the running average of each client's local action converges over time.

</details>


### [85] [Simultaneous AlphaZero: Extending Tree Search to Markov Games](https://arxiv.org/abs/2512.12486)
*Tyler Becker,Zachary Sunberg*

Main category: cs.GT

TL;DR: Simultaneous AlphaZero扩展AlphaZero框架至含同时行动的马尔可夫游戏，用矩阵游戏解决联合行动选择，结合后悔最优求解器，在两类场景展示鲁棒策略。


<details>
  <summary>Details</summary>
Motivation: 将AlphaZero框架扩展到多步、两人零和确定性且有同时行动的马尔可夫游戏。

Method: 在每个决策点通过矩阵游戏解决联合行动选择，其收益包含即时奖励和未来价值估计；在蒙特卡罗树搜索中结合后悔最优求解器处理不确定性。

Result: 在连续状态离散行动的追逐 - 逃避游戏和卫星 custody 维护场景中展示了鲁棒策略，即便面对最大剥削性对手。

Conclusion: Simultaneous AlphaZero能在相关马尔可夫游戏场景中有效发挥作用，产生鲁棒策略。

Abstract: Simultaneous AlphaZero extends the AlphaZero framework to multistep, two-player zero-sum deterministic Markov games with simultaneous actions. At each decision point, joint action selection is resolved via matrix games whose payoffs incorporate both immediate rewards and future value estimates. To handle uncertainty arising from bandit feedback during Monte Carlo Tree Search (MCTS), Simultaneous AlphaZero incorporates a regret-optimal solver for matrix games with bandit feedback. Simultaneous AlphaZero demonstrates robust strategies in a continuous-state discrete-action pursuit-evasion game and satellite custody maintenance scenarios, even when evaluated against maximally exploitative opponents.

</details>


### [86] [Generative AI as Digital Representatives in Collective Decision-Making: A Game-Theoretical Approach](https://arxiv.org/abs/2512.12582)
*Kexin Chen,Jianwei Huang,Yuan Luo*

Main category: cs.GT

TL;DR: 研究基于博弈论框架，探究集体决策中向生成式人工智能策略性披露信息问题，发现成员偏好冲突会促使更多信息披露，数字代表虽有偏好损失，但成员在特定情况可获更符合偏好的决策。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能在协作任务中代表团队成员决策时，准确呈现偏好面临挑战，且全面获取个人信息不可行。

Method: 开发博弈论框架，对集体决策中向生成式人工智能的策略性信息披露进行建模。

Result: 得出封闭形式的均衡特征，发现成员偏好冲突会导致竞争性信息披露，数字代表存在偏好损失，但特定情况下成员使用数字代表可获更符合偏好的决策。

Conclusion: 通过博弈论框架分析，明确了成员在信息披露上的策略平衡，以及数字代表在集体决策中的特点和效果。

Abstract: Generative Artificial Intelligence (GenAI) enables digital representatives to make decisions on behalf of team members in collaborative tasks, but faces challenges in accurately representing preferences. While supplying GenAI with detailed personal information improves representation fidelity, feasibility constraints make complete information access impractical. We bridge this gap by developing a game-theoretic framework that models strategic information revelation to GenAI in collective decision-making. The technical challenges lie in characterizing members' equilibrium behaviors under interdependent strategies and quantifying the imperfect preference learning outcomes by digital representatives. Our contribution includes closed-form equilibrium characterizations that reveal how members strategically balance team decision preference against communication costs. Our analysis yields an interesting finding: Conflicting preferences between team members drive competitive information revelation, with members revealing more information than those with aligned preferences. While digital representatives produce aggregate preference losses no smaller than direct participation, individual members may paradoxically achieve decisions more closely aligned with their preferences when using digital representatives, particularly when manual participation costs are high or when GenAI systems are sufficiently advanced.

</details>


### [87] [Fair Coordination in Strategic Scheduling](https://arxiv.org/abs/2512.13244)
*Wei-Chen Lee,Martin Bullinger,Alessandro Abate,Michael Wooldridge*

Main category: cs.GT

TL;DR: 考虑不同权重作业的战略代理调度问题，关注结果公平性，给出公平属性复杂性景观，用统一算法得到积极结果


<details>
  <summary>Details</summary>
Motivation: 与常见的最小化最大完工时间不同，希望结果在代理战略考量下公平

Method: 结合可信度、平等性和基于无嫉妒的公平性层次及放松条件，提出统一算法方法微调关键子例程

Result: 得到公平属性可满足性和决策版本单独或组合的完整复杂性景观

Conclusion: 通过统一算法可调整实现不同公平属性

Abstract: We consider a scheduling problem of strategic agents representing jobs of different weights. Each agent has to decide on one of a finite set of identical machines to get their job processed. In contrast to the common and exclusive focus on makespan minimization, we want the outcome to be fair under strategic considerations of the agents. Two natural properties are credibility, which ensures that the assignment is a Nash equilibrium and equality, requiring that agents with equal-weight jobs are assigned to machines of equal load. We combine these two with a hierarchy of fairness properties based on envy-freeness together with several relaxations based on the idea that envy seems more justified towards agents with a higher weight. We present a complete complexity landscape for satisfiability and decision versions of these properties, alone or in combination, and study them as structural constraints under makespan optimization. For our positive results, we develop a unified algorithmic approach, where we achieve different properties by fine-tuning key subroutines.

</details>


### [88] [Distributed Places and Safe Net Reduction](https://arxiv.org/abs/2512.13538)
*Victor Khomenko,Maciej Koutny,Alex Yakovlev*

Main category: cs.GT

TL;DR: 本文探讨用分布式库所实现安全Petri网约简，以解决并发系统形式化规范有效验证和实现问题，在有迭代的进程表达式场景获优化效果。


<details>
  <summary>Details</summary>
Motivation: 寻找与并发系统形式化规范行为相同的小Petri网，促进系统的有效验证和实际实现。

Method: 提出分布式库所概念，若其覆盖安全Petri网，可删除部分库所且不改变行为；将此约简技术应用于安全Petri网代数。

Result: 在有迭代的进程表达式情况下，使用分布式库所也能实现类似无循环行为时的优化，减少库所数量。

Conclusion: 所提的基于分布式库所的约简技术能在并发系统行为相关的有迭代进程表达式场景实现优化，在实际中计算可行。

Abstract: Being able to find small Petri nets with the same behaviour as formal specifications of concurrent systems benefits both effective verification and practical implementation of such systems. This paper considers specifications given in the form of compositionally defined safe nets.
  The paper discusses a novel concept of distributed place which implements the behaviour of an individual net place. It is shown that if distributed places cover a safe Petri net, then it is possible to delete some places without changing the behaviour. Crucially, the reduction is both static and local, making it computationally feasible in practice.
  The resulting reduction technique is then applied to an algebra of safe Petri nets (boxes) derived compositionally from process (box) expressions. Though the original derivation can yield exponentially large boxes, prior research demonstrated that if a box expression does not involve cyclic behaviours, the exponential number of places can be reduced down to polynomial (quadratic). In this paper, using distributed places, it is show that similar optimisation can also be achieved in the case of process expressions with iteration.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [89] [FloodSQL-Bench: A Retrieval-Augmented Benchmark for Geospatially-Grounded Text-to-SQL](https://arxiv.org/abs/2512.12084)
*Hanzhou Liu,Kai Yin,Zhitong Chen,Chenyue Liu,Ali Mostafavi*

Main category: cs.IR

TL;DR: 现有Text - to - SQL基准有局限性，本文引入FLOODSQL - BENCH基准，评估大语言模型并为Text - to - SQL研究提供实用测试台。


<details>
  <summary>Details</summary>
Motivation: 现有Text - to - SQL基准无法反映特定领域、多表和地理空间推理的复杂性，需解决这一局限。

Method: 引入FLOODSQL - BENCH基准，该基准通过基于键、空间和混合连接集成异构数据集，结合多数据层，在相同检索增强生成设置下系统评估大语言模型。

Result: 未明确提及具体结果。

Conclusion: FLOODSQL - BENCH为高风险应用领域的Text - to - SQL研究建立了实用测试台。

Abstract: Existing Text-to-SQL benchmarks primarily focus on single-table queries or limited joins in general-purpose domains, and thus fail to reflect the complexity of domain-specific, multi-table and geospatial reasoning, To address this limitation, we introduce FLOODSQL-BENCH, a geospatially grounded benchmark for the flood management domain that integrates heterogeneous datasets through key-based, spatial, and hybrid joins. The benchmark captures realistic flood-related information needs by combining social, infrastructural, and hazard data layers. We systematically evaluate recent large language models with the same retrieval-augmented generation settings and measure their performance across difficulty tiers. By providing a unified, open benchmark grounded in real-world disaster management data, FLOODSQL-BENCH establishes a practical testbed for advancing Text-to-SQL research in high-stakes application domains.

</details>


### [90] [Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval](https://arxiv.org/abs/2512.12458)
*Vihan Lakshman,Blaise Munyampirwa,Julian Shun,Benjamin Coleman*

Main category: cs.IR

TL;DR: 本文从稳定性角度重新审视向量数据库高维检索与维度诅咒的悖论，扩展稳定性理论到三种检索场景，实验结果与理论相符并为设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 现代向量数据库可进行高维神经嵌入高效检索，但经典理论存在维度诅咒问题，需解决该悖论。

Method: 将稳定性理论扩展到多向量搜索、过滤向量搜索和稀疏向量搜索三种实际常用的检索场景。

Result: 实验结果与理论预测相符。

Conclusion: 研究为模型和系统设计提供具体指导，以避免维度诅咒。

Abstract: Modern vector databases enable efficient retrieval over high-dimensional neural embeddings, powering applications from web search to retrieval-augmented generation. However, classical theory predicts such tasks should suffer from the curse of dimensionality, where distances between points become nearly indistinguishable, thereby crippling efficient nearest-neighbor search. We revisit this paradox through the lens of stability, the property that small perturbations to a query do not radically alter its nearest neighbors. Building on foundational results, we extend stability theory to three key retrieval settings widely used in practice: (i) multi-vector search, where we prove that the popular Chamfer distance metric preserves single-vector stability, while average pooling aggregation may destroy it; (ii) filtered vector search, where we show that sufficiently large penalties for mismatched filters can induce stability even when the underlying search is unstable; and (iii) sparse vector search, where we formalize and prove novel sufficient stability conditions. Across synthetic and real datasets, our experimental results match our theoretical predictions, offering concrete guidance for model and system design to avoid the curse of dimensionality.

</details>


### [91] [FuXi-$γ$: Efficient Sequential Recommendation with Exponential-Power Temporal Encoder and Diagonal-Sparse Positional Mechanism](https://arxiv.org/abs/2512.12740)
*Dezhi Yi,Wei Guo,Wenyang Cui,Wenxuan He,Huifeng Guo,Yong Liu,Zhenhua Dong,Ye Lu*

Main category: cs.IR

TL;DR: 提出FuXi-$γ$框架解决现有序列推荐方法计算开销高的问题，实验显示其性能优且能加速训练和推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的序列推荐方法因时间编码的不连续内存访问和长序列的密集注意力，存在高计算开销问题。

Method: 采用仅解码器的Transformer结构，引入指数幂时间编码器和对角稀疏位置机制。

Result: 在四个真实数据集上实验表明，FuXi-$γ$达到推荐质量的最优性能，训练加速达4.74倍，推理加速达6.18倍。

Conclusion: FuXi-$γ$是长序列推荐实用且可扩展的解决方案。

Abstract: Sequential recommendation aims to model users' evolving preferences based on their historical interactions. Recent advances leverage Transformer-based architectures to capture global dependencies, but existing methods often suffer from high computational overhead, primarily due to discontinuous memory access in temporal encoding and dense attention over long sequences. To address these limitations, we propose FuXi-$γ$, a novel sequential recommendation framework that improves both effectiveness and efficiency through principled architectural design. FuXi-$γ$ adopts a decoder-only Transformer structure and introduces two key innovations: (1) An exponential-power temporal encoder that encodes relative temporal intervals using a tunable exponential decay function inspired by the Ebbinghaus forgetting curve. This encoder enables flexible modeling of both short-term and long-term preferences while maintaining high efficiency through continuous memory access and pure matrix operations. (2) A diagonal-sparse positional mechanism that prunes low-contribution attention blocks using a diagonal-sliding strategy guided by the persymmetry of Toeplitz matrix. Extensive experiments on four real-world datasets demonstrate that FuXi-$γ$ achieves state-of-the-art performance in recommendation quality, while accelerating training by up to 4.74$\times$ and inference by up to 6.18$\times$, making it a practical and scalable solution for long-sequence recommendation. Our code is available at https://github.com/Yeedzhi/FuXi-gamma.

</details>


### [92] [Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views](https://arxiv.org/abs/2512.12980)
*Tingyang Chen,Cong Fu,Jiahua Wu,Haotian Wu,Hua Fan,Xiangyu Ke,Yunjun Gao,Yabo Ni,Anxiang Zeng*

Main category: cs.IR

TL;DR: 提出Iceberg基准套件用于端到端评估VSS方法，揭示性能下降来源，重新排名方法并给出选择和调优指导。


<details>
  <summary>Details</summary>
Motivation: 当前VSS基准主要评估召回 - 延迟权衡，忽略对下游任务影响，易误导学术和工业实践。

Method: 从任务中心视角，提出Iceberg基准套件，涵盖八个不同数据集，对13种VSS方法进行基准测试。

Result: Iceberg揭示性能下降的三个主要来源，重新排名VSS方法，与传统排名有显著差异。

Conclusion: 定义了一组任务中心元特征并得出决策树，指导从业者选择和调优VSS方法。

Abstract: Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice.
  We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads.

</details>


### [93] [Intelligent Scientific Literature Explorer using Machine Learning (ISLE)](https://arxiv.org/abs/2512.12760)
*Sina Jani,Arman Heidari,Amirmohammad Anvari,Zahra Rahimi*

Main category: cs.IR

TL;DR: 本文提出科学文献探索集成系统，结合多技术，评估显示在多方面有改进，为AI辅助科研发现提供可扩展基础。


<details>
  <summary>Details</summary>
Motivation: 科学出版加速使研究人员面临文献发现、情境化和解读挑战，传统搜索系统语义理解有限，现有AI工具专注孤立任务。

Method: 结合大规模数据采集、混合检索、语义主题建模和异构知识图谱构建，融合不同数据源，采用混合检索架构，根据资源选择主题建模方法，构建知识图谱。

Result: 评估表明在检索相关性、主题连贯性和可解释性方面有改进。

Conclusion: 该框架为AI辅助科学发现提供可扩展基础。

Abstract: The rapid acceleration of scientific publishing has created substantial challenges for researchers attempting to discover, contextualize, and interpret relevant literature. Traditional keyword-based search systems provide limited semantic understanding, while existing AI-driven tools typically focus on isolated tasks such as retrieval, clustering, or bibliometric visualization. This paper presents an integrated system for scientific literature exploration that combines large-scale data acquisition, hybrid retrieval, semantic topic modeling, and heterogeneous knowledge graph construction. The system builds a comprehensive corpus by merging full-text data from arXiv with structured metadata from OpenAlex. A hybrid retrieval architecture fuses BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion. Topic modeling is performed on retrieved results using BERTopic or non-negative matrix factorization depending on computational resources. A knowledge graph unifies papers, authors, institutions, countries, and extracted topics into an interpretable structure. The system provides a multi-layered exploration environment that reveals not only relevant publications but also the conceptual and relational landscape surrounding a query. Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The proposed framework contributes an extensible foundation for AI-assisted scientific discovery.

</details>


### [94] [SPAR: Session-based Pipeline for Adaptive Retrieval on Legacy File Systems](https://arxiv.org/abs/2512.12938)
*Duy A. Nguyen,Hai H. Do,Minh Doan,Minh N. Do*

Main category: cs.IR

TL;DR: 介绍用于遗留企业环境的SPAR框架，对比传统RAG管道有优势并在生物医学文献数据集验证效果。


<details>
  <summary>Details</summary>
Motivation: 企业历史数据存于缺乏结构化组织和语义索引的遗留文件系统，检索分析低效易错，需有效提取数据价值的方法。

Method: 引入SPAR框架，将大语言模型集成到RAG架构，采用轻量级两阶段流程，先创建语义元数据索引，再按需动态生成特定会话向量数据库。

Result: 理论复杂度分析显示SPAR有计算优势，在合成企业级生物医学文献文件系统验证，检索效果和下游模型准确率提升。

Conclusion: 讨论了设计权衡和在不同企业环境部署SPAR的开放挑战。

Abstract: The ability to extract value from historical data is essential for enterprise decision-making. However, much of this information remains inaccessible within large legacy file systems that lack structured organization and semantic indexing, making retrieval and analysis inefficient and error-prone. We introduce SPAR (Session-based Pipeline for Adaptive Retrieval), a conceptual framework that integrates Large Language Models (LLMs) into a Retrieval-Augmented Generation (RAG) architecture specifically designed for legacy enterprise environments. Unlike conventional RAG pipelines, which require costly construction and maintenance of full-scale vector databases that mirror the entire file system, SPAR employs a lightweight two-stage process: a semantic Metadata Index is first created, after which session-specific vector databases are dynamically generated on demand. This design reduces computational overhead while improving transparency, controllability, and relevance in retrieval. We provide a theoretical complexity analysis comparing SPAR with standard LLM-based RAG pipelines, demonstrating its computational advantages. To validate the framework, we apply SPAR to a synthesized enterprise-scale file system containing a large corpus of biomedical literature, showing improvements in both retrieval effectiveness and downstream model accuracy. Finally, we discuss design trade-offs and outline open challenges for deploying SPAR across diverse enterprise settings.

</details>


### [95] [BLADE: A Behavior-Level Data Augmentation Framework with Dual Fusion Modeling for Multi-Behavior Sequential Recommendation](https://arxiv.org/abs/2512.12964)
*Yupeng Li,Mingyue Cheng,Yucong Luo,Yitong Zhou,Qingyang Mao,Shijin Wang*

Main category: cs.IR

TL;DR: 提出BLADE框架解决多行为序列推荐中用户行为异质性和数据稀疏问题，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有多行为序列推荐研究因用户行为异质性和数据稀疏问题，推荐性能欠佳。

Method: 提出BLADE框架，采用双项目 - 行为融合架构处理行为异质性，设计三种行为级数据增强方法缓解数据稀疏。

Result: 在三个真实数据集上实验证明方法有效。

Conclusion: BLADE框架能有效解决多行为序列推荐中的挑战。

Abstract: Multi-behavior sequential recommendation aims to capture users' dynamic interests by modeling diverse types of user interactions over time. Although several studies have explored this setting, the recommendation performance remains suboptimal, mainly due to two fundamental challenges: the heterogeneity of user behaviors and data sparsity. To address these challenges, we propose BLADE, a framework that enhances multi-behavior modeling while mitigating data sparsity. Specifically, to handle behavior heterogeneity, we introduce a dual item-behavior fusion architecture that incorporates behavior information at both the input and intermediate levels, enabling preference modeling from multiple perspectives. To mitigate data sparsity, we design three behavior-level data augmentation methods that operate directly on behavior sequences rather than core item sequences. These methods generate diverse augmented views while preserving the semantic consistency of item sequences. These augmented views further enhance representation learning and generalization via contrastive learning. Experiments on three real-world datasets demonstrate the effectiveness of our approach.

</details>


### [96] [Do Reviews Matter for Recommendations in the Era of Large Language Models?](https://arxiv.org/abs/2512.12978)
*Chee Heng Tan,Huiying Zheng,Jing Wang,Zhuoyi Lin,Shaodi Feng,Huijing Zhan,Xiaoli Li,J. Senthilnath*

Main category: cs.IR

TL;DR: 本文对比深度学习方法和大语言模型（LLM）方法，系统研究文本评论在推荐系统中的作用，发现LLM在推荐中表现良好，移除或扭曲评论不一定降低推荐准确率。


<details>
  <summary>Details</summary>
Motivation: 随着LLM出现，探讨在其时代显式用户评论是否仍对推荐系统至关重要。

Method: 在八个公开数据集上用LLM进行实验，评估零样本、少样本和微调场景下的性能；引入基准评估框架RAREval，考察移除评论、随机扭曲等多种场景。

Result: LLM能作为有效的评论感知推荐引擎，通常优于传统深度学习方法，特别是在数据稀疏和冷启动场景；移除或随机扭曲评论不一定导致推荐准确率下降。

Conclusion: 研究结果促使重新思考如何更有效地利用文本评论中的用户偏好。

Abstract: With the advent of large language models (LLMs), the landscape of recommender systems is undergoing a significant transformation. Traditionally, user reviews have served as a critical source of rich, contextual information for enhancing recommendation quality. However, as LLMs demonstrate an unprecedented ability to understand and generate human-like text, this raises the question of whether explicit user reviews remain essential in the era of LLMs. In this paper, we provide a systematic investigation of the evolving role of text reviews in recommendation by comparing deep learning methods and LLM approaches. Particularly, we conduct extensive experiments on eight public datasets with LLMs and evaluate their performance in zero-shot, few-shot, and fine-tuning scenarios. We further introduce a benchmarking evaluation framework for review-aware recommender systems, RAREval, to comprehensively assess the contribution of textual reviews to the recommendation performance of review-aware recommender systems. Our framework examines various scenarios, including the removal of some or all textual reviews, random distortion, as well as recommendation performance in data sparsity and cold-start user settings. Our findings demonstrate that LLMs are capable of functioning as effective review-aware recommendation engines, generally outperforming traditional deep learning approaches, particularly in scenarios characterized by data sparsity and cold-start conditions. In addition, the removal of some or all textual reviews and random distortion does not necessarily lead to declines in recommendation accuracy. These findings motivate a rethinking of how user preference from text reviews can be more effectively leveraged. All code and supplementary materials are available at: https://github.com/zhytk/RAREval-data-processing.

</details>


### [97] [Are Large Language Models Really Effective for Training-Free Cold-Start Recommendation?](https://arxiv.org/abs/2512.13001)
*Genki Kusano,Kenya Abe,Kunihiro Takeoka*

Main category: cs.IR

TL;DR: 研究对无训练推荐中LLMs和TEMs进行对照实验，发现TEMs表现更优，为无训练推荐提供新依据。


<details>
  <summary>Details</summary>
Motivation: 现实应用存在无训练数据情况，传统方法无法应用，且此前未在相同条件下对比LLMs和TEMs用于无训练推荐的效果。

Method: 进行对照实验，在相同设置下系统评估LLMs和TEMs两种方法。

Result: TEMs在冷启动和热启动场景下均优于LLM重排器。

Conclusion: 直接的LLM排序并非唯一可行方案，基于TEM的方法为无训练推荐提供更强、更具扩展性的基础。

Abstract: Recommender systems usually rely on large-scale interaction data to learn from users' past behaviors and make accurate predictions. However, real-world applications often face situations where no training data is available, such as when launching new services or handling entirely new users. In such cases, conventional approaches cannot be applied. This study focuses on training-free recommendation, where no task-specific training is performed, and particularly on \textit{training-free cold-start recommendation} (TFCSR), the more challenging case where the target user has no interactions. Large language models (LLMs) have recently been explored as a promising solution, and numerous studies have been proposed. As the ability of text embedding models (TEMs) increases, they are increasingly recognized as applicable to training-free recommendation, but no prior work has directly compared LLMs and TEMs under identical conditions. We present the first controlled experiments that systematically evaluate these two approaches in the same setting. The results show that TEMs outperform LLM rerankers, and this trend holds not only in cold-start settings but also in warm-start settings with rich interactions. These findings indicate that direct LLM ranking is not the only viable option, contrary to the commonly shared belief, and TEM-based approaches provide a stronger and more scalable basis for training-free recommendation.

</details>


### [98] [Progressive Refinement of E-commerce Search Ranking Based on Short-Term Activities of the Buyer](https://arxiv.org/abs/2512.13037)
*Taoran Sheng,Sathappan Muthiah,Atiq Islam,Jinming Feng*

Main category: cs.IR

TL;DR: 本文提出基于当前上下文调整电商搜索结果的系统方法，逐步纳入更多上下文信息和技术提升搜索效果，实验证明该方法有效改善搜索结果。


<details>
  <summary>Details</summary>
Motivation: 解决电商购物中使搜索结果与买家即时需求和偏好相匹配，以及在买家购物过程中调整搜索结果的挑战。

Method: 从基本方法开始，逐步纳入更多上下文信息和先进技术，将上下文框架应用于搜索引擎结果页面的商品。

Result: 从简单到先进模型的渐进式改进显著提升排序器性能，上下文技术集成提升生产排序器性能，离线和在线A/B测试中搜索结果的平均倒数排名得到改善。

Conclusion: 迭代方法对电商平台搜索结果的上下文处理有重大贡献。

Abstract: In e-commerce shopping, aligning search results with a buyer's immediate needs and preferences presents a significant challenge, particularly in adapting search results throughout the buyer's shopping journey as they move from the initial stages of browsing to making a purchase decision or shift from one intent to another. This study presents a systematic approach to adapting e-commerce search results based on the current context. We start with basic methods and incrementally incorporate more contextual information and state-of-the-art techniques to improve the search outcomes. By applying this evolving contextual framework to items displayed on the search engine results page (SERP), we progressively align search outcomes more closely with the buyer's interests and current search intentions. Our findings demonstrate that this incremental enhancement, from simple heuristic autoregressive features to advanced sequence models, significantly improves ranker performance. The integration of contextual techniques enhances the performance of our production ranker, leading to improved search results in both offline and online A/B testing in terms of Mean Reciprocal Rank (MRR). Overall, the paper details iterative methodologies and their substantial contributions to search result contextualization on e-commerce platforms.

</details>


### [99] [A Simple and Effective Framework for Symmetric Consistent Indexing in Large-Scale Dense Retrieval](https://arxiv.org/abs/2512.13074)
*Huimu Wang,Yiming Qiu,Xingzhi Yao,Zhiguo Chen,Guoyu Tang,Songlin Wang,Sulong Xu,Mingming Li*

Main category: cs.IR

TL;DR: 论文指出稠密检索虽高效但双塔编码架构有问题，提出SCI框架解决，该框架经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有稠密检索中广泛采用的双塔编码架构存在表征空间不对齐、检索索引不一致等问题，影响匹配精度、检索稳定性和长尾查询性能，限制下游生成模型性能。

Method: 提出名为SCI的框架，包含对称表征对齐模块（采用输入交换机制统一双塔表征空间）和双塔协同一致索引模块（用双视图索引策略重新设计检索路径）。

Result: 在公共数据集和真实电商数据集上验证了该框架的有效性。

Conclusion: 所提SCI框架简单有效、系统轻量且工程友好，能在低开销下支持十亿级部署。

Abstract: Dense retrieval has become the industry standard in large-scale information retrieval systems due to its high efficiency and competitive accuracy. Its core relies on a coarse-to-fine hierarchical architecture that enables rapid candidate selection and precise semantic matching, achieving millisecond-level response over billion-scale corpora. This capability makes it essential not only in traditional search and recommendation scenarios but also in the emerging paradigm of generative recommendation driven by large language models, where semantic IDs-themselves a form of coarse-to-fine representation-play a foundational role. However, the widely adopted dual-tower encoding architecture introduces inherent challenges, primarily representational space misalignment and retrieval index inconsistency, which degrade matching accuracy, retrieval stability, and performance on long-tail queries. These issues are further magnified in semantic ID generation, ultimately limiting the performance ceiling of downstream generative models.
  To address these challenges, this paper proposes a simple and effective framework named SCI comprising two synergistic modules: a symmetric representation alignment module that employs an innovative input-swapping mechanism to unify the dual-tower representation space without adding parameters, and an consistent indexing with dual-tower synergy module that redesigns retrieval paths using a dual-view indexing strategy to maintain consistency from training to inference. The framework is systematic, lightweight, and engineering-friendly, requiring minimal overhead while fully supporting billion-scale deployment. We provide theoretical guarantees for our approach, with its effectiveness validated by results across public datasets and real-world e-commerce datasets.

</details>


### [100] [Towards Practical Large-scale Dynamical Heterogeneous Graph Embedding: Cold-start Resilient Recommendation](https://arxiv.org/abs/2512.13120)
*Mabiao Long,Jiaxi Liu,Yufeng Li,Hao Xiong,Junchi Yan,Kefan Wang,Yi Cao,Jiandong Ding*

Main category: cs.IR

TL;DR: 本文提出两阶段解决方案，结合HetSGFormer和ILLE应对动态异构图嵌入部署挑战，在亿级图上实验有效。


<details>
  <summary>Details</summary>
Motivation: 解决动态异构图嵌入在生产部署中面临的可扩展性、数据新鲜度和冷启动等关键挑战。

Method: 结合用于静态学习的可扩展图变换器HetSGFormer和基于CPU的实时更新轻量级算法Incremental Locally Linear Embedding (ILLE)。

Result: 在亿级图上，HetSGFormer使广告商价值较之前方法提升6.11%，ILLE模块额外提升3.22%，并使嵌入刷新及时性提高83.2%。

Conclusion: 为在生产环境中部署动态图学习提供了经过验证的框架。

Abstract: Deploying dynamic heterogeneous graph embeddings in production faces key challenges of scalability, data freshness, and cold-start. This paper introduces a practical, two-stage solution that balances deep graph representation with low-latency incremental updates. Our framework combines HetSGFormer, a scalable graph transformer for static learning, with Incremental Locally Linear Embedding (ILLE), a lightweight, CPU-based algorithm for real-time updates. HetSGFormer captures global structure with linear scalability, while ILLE provides rapid, targeted updates to incorporate new data, thus avoiding costly full retraining. This dual approach is cold-start resilient, leveraging the graph to create meaningful embeddings from sparse data. On billion-scale graphs, A/B tests show HetSGFormer achieved up to a 6.11% lift in Advertiser Value over previous methods, while the ILLE module added another 3.22% lift and improved embedding refresh timeliness by 83.2%. Our work provides a validated framework for deploying dynamic graph learning in production environments.

</details>


### [101] [Know Your Users! Estimating User Domain Knowledge in Conversational Recommenders](https://arxiv.org/abs/2512.13173)
*Ivica Kostric,Ujwal Gadiraju,Krisztian Balog*

Main category: cs.IR

TL;DR: 提出从对话中估计用户领域知识的新任务，设计基于游戏的数据收集协议并发布数据集。


<details>
  <summary>Details</summary>
Motivation: 当前对话推荐系统未考虑用户专业知识水平，缺乏合适数据集来开发自适应系统。

Method: 设计基于游戏的数据收集协议来获取不同知识表达的数据。

Result: 发布了基于新协议收集的数据集。

Conclusion: 该数据集为用户知识感知的对话推荐系统未来研究提供潜力。

Abstract: The ideal conversational recommender system (CRS) acts like a savvy salesperson, adapting its language and suggestions to each user's level of expertise. However, most current systems treat all users as experts, leading to frustrating and inefficient interactions when users are unfamiliar with a domain. Systems that can adapt their conversational strategies to a user's knowledge level stand to offer a much more natural and effective experience. To make a step toward such adaptive systems, we introduce a new task: estimating user domain knowledge from conversations, enabling a CRS to better understand user needs and personalize interactions. A key obstacle to developing such adaptive systems is the lack of suitable data; to our knowledge, no existing dataset captures the conversational behaviors of users with varying levels of domain knowledge. Furthermore, in most dialogue collection protocols, users are free to express their own preferences, which tends to concentrate on popular items and well-known features, offering little insight into how novices explore or learn about unfamiliar features. To address this, we design a game-based data collection protocol that elicits varied expressions of knowledge, release the resulting dataset, and provide an initial analysis to highlight its potential for future work on user-knowledge-aware CRS.

</details>


### [102] [BlossomRec: Block-level Fused Sparse Attention Mechanism for Sequential Recommendations](https://arxiv.org/abs/2512.13368)
*Mengyang Ma,Xiaopeng Li,Wanyu Wang,Zhaocheng Du,Jingtong Gao,Pengyue Jia,Yuyang Ye,Yiqi Wang,Yunpeng Weng,Weihong Luo,Xiao Han,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出稀疏注意力机制BlossomRec，结合长短兴趣建模，实验证明其高效有效。


<details>
  <summary>Details</summary>
Motivation: 现有SRS中Transformer结构随用户交互历史增长，计算和内存需求大，且现有方法难有效建模长序列、短序列性能不稳定。

Method: 设计BlossomRec，将用户兴趣分为长短兴趣，用两种稀疏注意力模式计算并通过可学习门控输出结合。

Result: 在四个公开数据集上，与先进Transformer模型结合时性能相当或更优，显著减少内存使用。

Conclusion: BlossomRec高效有效，代码开源。

Abstract: Transformer structures have been widely used in sequential recommender systems (SRS). However, as user interaction histories increase, computational time and memory requirements also grow. This is mainly caused by the standard attention mechanism. Although there exist many methods employing efficient attention and SSM-based models, these approaches struggle to effectively model long sequences and may exhibit unstable performance on short sequences. To address these challenges, we design a sparse attention mechanism, BlossomRec, which models both long-term and short-term user interests through attention computation to achieve stable performance across sequences of varying lengths. Specifically, we categorize user interests in recommendation systems into long-term and short-term interests, and compute them using two distinct sparse attention patterns, with the results combined through a learnable gated output. Theoretically, it significantly reduces the number of interactions participating in attention computation. Extensive experiments on four public datasets demonstrate that BlossomRec, when integrated with state-of-the-art Transformer-based models, achieves comparable or even superior performance while significantly reducing memory usage, providing strong evidence of BlossomRec's efficiency and effectiveness.The code is available at https://github.com/ronineume/BlossomRec.

</details>


### [103] [Automated Information Flow Selection for Multi-scenario Multi-task Recommendation](https://arxiv.org/abs/2512.13396)
*Chaohua Yang,Dugang Liu,Shiwei Li,Yuwen Fu,Xing Tang,Weihong Luo,Xiangyu Zhao,Xiuqiang He,Zhong Ming*

Main category: cs.IR

TL;DR: 现有多场景多任务推荐（MSMTR）模型存在架构复杂、信息流未过滤问题，本文提出轻量级AutoIFS框架，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有MSMTR模型存在依赖复杂架构增加复杂度与成本，且未过滤信息流引入噪声的问题，需要改进。

Method: 提出AutoIFS框架，用低秩适应（LoRA）解耦信息单元实现高效信息融合，引入信息流选择网络过滤无效信息。

Result: 通过在两个公开基准数据集的广泛实验和在线A/B测试，验证了AutoIFS的有效性。

Conclusion: 提出的轻量级AutoIFS框架能解决现有MSMTR模型的问题，提升模型性能。

Abstract: Multi-scenario multi-task recommendation (MSMTR) systems must address recommendation demands across diverse scenarios while simultaneously optimizing multiple objectives, such as click-through rate and conversion rate. Existing MSMTR models typically consist of four information units: scenario-shared, scenario-specific, task-shared, and task-specific networks. These units interact to generate four types of relationship information flows, directed from scenario-shared or scenario-specific networks to task-shared or task-specific networks. However, these models face two main limitations: 1) They often rely on complex architectures, such as mixture-of-experts (MoE) networks, which increase the complexity of information fusion, model size, and training cost. 2) They extract all available information flows without filtering out irrelevant or even harmful content, introducing potential noise. Regarding these challenges, we propose a lightweight Automated Information Flow Selection (AutoIFS) framework for MSMTR. To tackle the first issue, AutoIFS incorporates low-rank adaptation (LoRA) to decouple the four information units, enabling more flexible and efficient information fusion with minimal parameter overhead. To address the second issue, AutoIFS introduces an information flow selection network that automatically filters out invalid scenario-task information flows based on model performance feedback. It employs a simple yet effective pruning function to eliminate useless information flows, thereby enhancing the impact of key relationships and improving model performance. Finally, we evaluate AutoIFS and confirm its effectiveness through extensive experiments on two public benchmark datasets and an online A/B test.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [104] [Active Inference with Reusable State-Dependent Value Profiles](https://arxiv.org/abs/2512.11829)
*Jacob Poschl*

Main category: cs.LG

TL;DR: 提出价值配置文件框架用于多变环境中的价值控制，模型比较显示其优势，表明自适应控制主要由策略先验调制驱动。


<details>
  <summary>Details</summary>
Motivation: 在多变环境中，为代理在潜在上下文间切换价值控制机制时，解决为每个情况维护独立参数难以处理的问题。

Method: 引入价值配置文件，通过信念加权混合产生有效控制参数，在概率反转学习中评估框架，使用交叉验证对数似然和信息准则比较不同模型。

Result: 模型比较显示基于配置文件的模型优于简单替代模型，参数恢复分析支持结构可识别性，模型推断表明自适应控制主要由策略先验调制驱动。

Conclusion: 可重复使用的价值配置文件为多变环境中的信念条件价值控制提供了可处理的计算解释，并产生了可测试的信念依赖控制和行为灵活性的特征。

Abstract: Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.

</details>


### [105] [CR3G: Causal Reasoning for Patient-Centric Explanations in Radiology Report Generation](https://arxiv.org/abs/2512.11830)
*Satyam Kumar*

Main category: cs.LG

TL;DR: 本文探讨用于胸部X光分析的基于因果推理的框架CR3G，以改善AI生成报告，在部分异常情况中有较好表现。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型难以理解医学影像模式与患者病情的深层次因果关系，需提升AI诊断可靠性。

Method: 探索了基于提示的框架CR3G，聚焦因果关系、推理和以患者为中心的解释。

Result: CR3G在5种异常中的2种上表现出更好的因果关系和解释能力。

Conclusion: 运用CR3G可提升AI诊断质量，使其在临床实践更有用和可信。

Abstract: Automatic chest X-ray report generation is an important area of research aimed at improving diagnostic accuracy and helping doctors make faster decisions. Current AI models are good at finding correlations (or patterns) in medical images. Still, they often struggle to understand the deeper cause-and-effect relationships between those patterns and a patient condition. Causal inference is a powerful approach that goes beyond identifying patterns to uncover why certain findings in an X-ray relate to a specific diagnosis. In this paper, we will explore the prompt-driven framework Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G) that is applied to chest X-ray analysis to improve understanding of AI-generated reports by focusing on cause-and-effect relationships, reasoning and generate patient-centric explanation. The aim to enhance the quality of AI-driven diagnostics, making them more useful and trustworthy in clinical practice. CR3G has shown better causal relationship capability and explanation capability for 2 out of 5 abnormalities.

</details>


### [106] [On the Design of One-step Diffusion via Shortcutting Flow Paths](https://arxiv.org/abs/2512.11831)
*Haitao Lin,Peiyan Hu,Minsi Ren,Zhifeng Gao,Zhi-Ming Ma,Guolin ke,Tailin Wu,Stan Z. Li*

Main category: cs.LG

TL;DR: 提出代表性捷径模型通用设计框架，改进的一步模型在ImageNet-256x256上取得新SOTA，且无需预训练等。


<details>
  <summary>Details</summary>
Motivation: 现有少步扩散模型理论推导和实际实现紧密耦合，模糊了设计空间。

Method: 提出代表性捷径模型的通用设计框架，对组件层面选择进行解耦。

Result: 改进后的一步模型在ImageNet-256x256上，在无分类器引导设置下取得2.85的FID50k新SOTA，且无需预训练、蒸馏或课程学习。

Conclusion: 降低了捷径模型组件级创新的门槛，便于对其设计空间进行有原则的探索。

Abstract: Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (a.k.a. shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.

</details>


### [107] [Empirical Mode Decomposition and Graph Transformation of the MSCI World Index: A Multiscale Topological Analysis for Graph Neural Network Modeling](https://arxiv.org/abs/2512.12526)
*Agustín M. de los Riscos,Julio E. Sandubete,Diego Carmona-Fernández,León Beleña*

Main category: cs.LG

TL;DR: 研究将EMD应用于MSCI世界指数，把IMF转换为图表示用GNN建模，分析不同IMF的图结构特点，为GNN设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 实现对金融时间序列更有效的预测建模，探索适合分解成分结构特性的GNN架构。

Method: 用CEEMDAN从MSCI世界指数中提取九个IMFs，用四种时间序列转图方法将每个IMF转换为图，进行拓扑分析。

Result: 高频IMF生成稠密、高度连接的小世界图，低频IMF生成更稀疏、特征路径更长的网络，不同时间序列转图方法有不同特点。

Conclusion: 研究结果为根据分解成分结构特性设计GNN架构提供指导，利于金融时间序列预测建模。

Abstract: This study applies Empirical Mode Decomposition (EMD) to the MSCI World index and converts the resulting intrinsic mode functions (IMFs) into graph representations to enable modeling with graph neural networks (GNNs). Using CEEMDAN, we extract nine IMFs spanning high-frequency fluctuations to long-term trends. Each IMF is transformed into a graph using four time-series-to-graph methods: natural visibility, horizontal visibility, recurrence, and transition graphs. Topological analysis shows clear scale-dependent structure: high-frequency IMFs yield dense, highly connected small-world graphs, whereas low-frequency IMFs produce sparser networks with longer characteristic path lengths. Visibility-based methods are more sensitive to amplitude variability and typically generate higher clustering, while recurrence graphs better preserve temporal dependencies. These results provide guidance for designing GNN architectures tailored to the structural properties of decomposed components, supporting more effective predictive modeling of financial time series.

</details>


### [108] [Performance and Efficiency of Climate In-Situ Data Reconstruction: Why Optimized IDW Outperforms kriging and Implicit Neural Representation](https://arxiv.org/abs/2512.11832)
*Jakub Walczak*

Main category: cs.LG

TL;DR: 研究评估三种稀疏气候数据重建方法，经实验分析，简单IDW方法在重建精度和计算效率上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 评估三种稀疏气候数据重建方法的性能。

Method: 对简单反距离加权法（IDW）、普通克里金法（OK）和高级隐式神经表示模型（MMGN架构）进行超参数调优，开展大量实验并进行综合统计分析。

Result: IDW在重建精度和计算效率上优于其他方法，在100个随机采样的稀疏数据集上RMSE、MAE和Δ_MAX最低，R²最高，差异有统计学意义。

Conclusion: IDW方法在评估的质量指标上始终表现更优。

Abstract: This study evaluates three reconstruction methods for sparse climate data: the simple inverse distance weighting (IDW), the statistically grounded ordinary kriging (OK), and the advanced implicit neural representation model (MMGN architecture). All methods were optimized through hyper-parameter tuning using validation splits. An extensive set of experiments was conducted, followed by a comprehensive statistical analysis. The results demonstrate the superiority of the simple IDW method over the other reference methods in terms of both reconstruction accuracy and computational efficiency. IDW achieved the lowest RMSE ($3.00 \pm 1.93$), MAE ($1.32 \pm 0.77$), and $Δ_{MAX}$ ($24.06 \pm 17.15$), as well as the highest $R^2$ ($0.68 \pm 0.16$), across 100 randomly sampled sparse datasets from the ECA\&D database. Differences in RMSE, MAE, and $R^2$ were statistically significant and exhibited moderate to large effect sizes. The Dunn post-hoc test further confirmed the consistent superiority of IDW across all evaluated quality measures [...]

</details>


### [109] [Credit Risk Estimation with Non-Financial Features: Evidence from a Synthetic Istanbul Dataset](https://arxiv.org/abs/2512.12783)
*Atalay Denknalbant,Emre Sezdi,Zeki Furkan Kutlu,Polat Goktas*

Main category: cs.LG

TL;DR: 本文创建伊斯坦布尔居民合成数据集，研究评估无信用记录借款人的方法，发现替代金融数据可提升模型性能，为银行服务不足人群提供信贷蓝图。


<details>
  <summary>Details</summary>
Motivation: 金融排斥带来诸多问题，伊斯坦布尔银行服务不足的消费者常无信用记录，需研究评估此类借款人的方法。

Method: 创建合成数据集，利用检索增强生成将公共统计数据输入OpenAI o3模型合成记录，使用CatBoost、LightGBM和XGBoost训练Demo和Full两种模型版本。

Result: 替代金融数据使曲线下面积提高约1.3个百分点，平衡F1值从约0.84提升到0.95，增益14%。

Conclusion: 贡献了合成数据集和建模流程，证明行为属性可接近信用局的判别能力，为贷款机构和监管者提供为银行服务不足人群提供公平安全信贷的蓝图。

Abstract: Financial exclusion constrains entrepreneurship, increases income volatility, and widens wealth gaps. Underbanked consumers in Istanbul often have no bureau file because their earnings and payments flow through informal channels. To study how such borrowers can be evaluated we create a synthetic dataset of one hundred thousand Istanbul residents that reproduces first quarter 2025 TÜİK census marginals and telecom usage patterns. Retrieval augmented generation feeds these public statistics into the OpenAI o3 model, which synthesises realistic yet private records. Each profile contains seven socio demographic variables and nine alternative attributes that describe phone specifications, online shopping rhythm, subscription spend, car ownership, monthly rent, and a credit card flag. To test the impact of the alternative financial data CatBoost, LightGBM, and XGBoost are each trained in two versions. Demo models use only the socio demographic variables; Full models include both socio demographic and alternative attributes. Across five fold stratified validation the alternative block raises area under the curve by about one point three percentage and lifts balanced \(F_{1}\) from roughly 0.84 to 0.95, a fourteen percent gain. We contribute an open Istanbul 2025 Q1 synthetic dataset, a fully reproducible modeling pipeline, and empirical evidence that a concise set of behavioural attributes can approach bureau level discrimination power while serving borrowers who lack formal credit records. These findings give lenders and regulators a transparent blueprint for extending fair and safe credit access to the underbanked.

</details>


### [110] [Soft Decision Tree classifier: explainable and extendable PyTorch implementation](https://arxiv.org/abs/2512.11833)
*Reuben R Shamir*

Main category: cs.LG

TL;DR: 实现SDT和SM - SDT并用PyTorch测试，对比多种分类方法，代码和数据集开源。


<details>
  <summary>Details</summary>
Motivation: 探索SDT和SM - SDT在模拟和临床数据集上的性能，并对比多种分类方法。

Method: 用PyTorch实现SDT和SM - SDT，在模拟和临床数据集上测试，可视化SDT。

Result: SDT、SM - SDT和XGBoost的AUC值相似，优于随机森林、逻辑回归和决策树；除决策树外，其他测试方法结果相当。

Conclusion: SDT和SM - SDT等方法在临床数据集上有一定优势，多种分类方法（除决策树）结果可比。

Abstract: We implemented a Soft Decision Tree (SDT) and a Short-term Memory Soft Decision Tree (SM-SDT) using PyTorch. The methods were extensively tested on simulated and clinical datasets. The SDT was visualized to demonstrate the potential for its explainability. SDT, SM-SDT, and XGBoost demonstrated similar area under the curve (AUC) values. These methods were better than Random Forest, Logistic Regression, and Decision Tree. The results on clinical datasets suggest that, aside from a decision tree, all tested classification methods yield comparable results.
  The code and datasets are available online on GitHub: https://github.com/KI-Research-Institute/Soft-Decision-Tree

</details>


### [111] [Hybrid twinning using PBDW and DeepONet for the effective state estimation and prediction on partially known systems](https://arxiv.org/abs/2512.11834)
*Stiven Briand Massala,Ludovic Chamoin,Massimo Picca Ciamarra*

Main category: cs.LG

TL;DR: 本文提出结合物理建模与数据驱动学习的混合方法，用于增强复杂不确定物理系统的状态估计和预测，并在亥姆霍兹方程问题上验证。


<details>
  <summary>Details</summary>
Motivation: 准确估计复杂不确定物理系统状态需协调理论模型与有噪实验数据。

Method: 基于PBDW框架，结合降阶模型与测量数据；引入约束为最佳知识流形正交补的DeepONet学习模型偏差结构；研究最优传感器布局策略。

Result: 在涉及亥姆霍兹方程的代表性问题上，在包括边界条件和源项等多种建模误差来源下验证了方法。

Conclusion: 所提出的混合方法可有效提升复杂不确定物理系统的状态估计和预测能力。

Abstract: The accurate estimation of the state of complex uncertain physical systems requires reconciling theoretical models, with inherent imperfections, with noisy experimental data. In this work, we propose an effective hybrid approach that combines physics-based modeling with data-driven learning to enhance state estimation and further prediction. Our method builds upon the Parameterized Background Data-Weak (PBDW) framework, which naturally integrates a reduced-order representation of the best-available model with measurement data to account for both anticipated and unanticipated uncertainties. To address model discrepancies not captured by the reduced-order space, and learn the structure of model deviation, we incorporate a Deep Operator Network (DeepONet) constrained to be an orthogonal complement of the best-knowledge manifold. This ensures that the learned correction targets only the unknown components of model bias, preserving the interpretability and fidelity of the physical model. An optimal sensor placement strategy is also investigated to maximize information gained from measurements. We validate the proposed approach on a representative problem involving the Helmholtz equation under various sources of modeling error, including those arising from boundary conditions and source terms.

</details>


### [112] [Semantic Nutrition Estimation: Predicting Food Healthfulness from Text Descriptions](https://arxiv.org/abs/2512.11836)
*Dayne R. Freudenberg,Daniel G. Haughian,Mitchell A. Klusty,Caroline N. Leach,W. Scott Black,Leslie N. Woltenberg,Rowan Hallock,Elizabeth Solie,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: cs.LG

TL;DR: 本文提出机器学习管道，用多头神经网络结合多种特征和数据库数据预测食物营养评分，效果良好，能将语言转化为营养信息。


<details>
  <summary>Details</summary>
Motivation: 现有营养评估系统需详细数据，而口语化食物描述数据常不可用或难获取，需要准确营养评估方法。

Method: 使用多头神经网络处理混合特征向量，结合语义文本嵌入、词汇模式、领域启发式和USDA FNDDS数据，估计FCS算法所需营养和食物成分。

Result: 系统预测能力强，单个营养素中位数R^2为0.81，预测的FCS与公布值强相关（皮尔逊r = 0.77），平均绝对差14.0分。

Conclusion: 该方法能将语言转化为可操作营养信息，可用于消费者应用和研究的大规模饮食评估。

Abstract: Accurate nutritional assessment is critical for public health, but existing profiling systems require detailed data often unavailable or inaccessible from colloquial text descriptions of food. This paper presents a machine learning pipeline that predicts the comprehensive Food Compass Score 2.0 (FCS) from text descriptions. Our approach uses multi-headed neural networks to process hybrid feature vectors that combine semantic text embeddings, lexical patterns, and domain heuristics, alongside USDA Food and Nutrient Database for Dietary Studies (FNDDS) data. The networks estimate the nutrient and food components necessary for the FCS algorithm. The system demonstratedstrong predictive power, achieving a median R^2 of 0.81 for individual nutrients. The predicted FCS correlated strongly with published values (Pearson's r = 0.77), with a mean absolute difference of 14.0 points. While errors were largest for ambiguous or processed foods, this methodology translates language into actionable nutritional information, enabling scalable dietary assessment for consumer applications and research.

</details>


### [113] [KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs](https://arxiv.org/abs/2512.11851)
*Prashant Pandey*

Main category: cs.LG

TL;DR: 研究小语言模型中注意力键值状态能否复用加速推理，采用令牌回收方法，测试有前缀重叠时可提速且不影响输出语义。


<details>
  <summary>Details</summary>
Motivation: 探索小语言模型中为一个提示计算的注意力键值状态能否复用以加速新的相似提示推理，增加上下文内存空间。

Method: 以DialoGPT - medium为测试平台，构建过去激活的缓存，通过句子嵌入获取条目，当缓存提示是新输入的精确前缀时复用缓存的键值，比较复用和基线运行的延迟和输出保真度并记录复用深度。

Result: 测试中，当前缀存在重叠时能持续加速，输出语义无实质下降；无重叠时表现与基线一致。

Conclusion: 在小语言模型中，采用令牌回收方法复用注意力键值状态可在有前缀重叠时加速推理且不影响输出质量。

Abstract: Whether attention key value (KV) states computed for one prompt for a small LLM can be reused to accelerate inference on a new similar prompt, giving an increase to the space to its context memory using an approach called token recycling. Using a standard Hugging Face setup with DialoGPT-medium (a 345M parameter GPT-2 style decoder trained on 147M Reddit exchanges, 2005 to 2017) as the testbed, we build a cache of past activations and get entries by sentence embeddings, then reuse cached past key values when the cached prompt is an exact prefix of the new input. We compare recycled vs. baseline runs on latency and output fidelity, and log reuse depth in tokens. Reproducibility requires no model modifications, cached KVs are serialized to the CPU, reloaded, and supplied to the generate function to continue decoding from the cached prefix. In tests, we observe consistent speedups when prefix overlap exists, with no material degradation in output semantics, and when overlap is absent, behavior matches baseline.

</details>


### [114] [D-STEER - Preference Alignment Techniques Learn to Behave, not to Believe -- Beneath the Surface, DPO as Steering Vector Perturbation in Activation Space](https://arxiv.org/abs/2512.11838)
*Samarth Raina,Saksham Aggarwal,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.LG

TL;DR: 本文探讨DPO在网络内部的作用机制，揭示其为低秩转向机制，支持DPO的行为幻觉观点。


<details>
  <summary>Details</summary>
Motivation: 了解DPO在网络内部诱导的实际变化。

Method: 通过简单推导研究DPO梯度，提取经验转向向量，进行频谱分析。

Result: DPO梯度仅依赖于首选和非首选完成项的对数嵌入差异，添加提取的向量可重现对齐行为，频谱分析显示上层存在秩一主导和熵坍塌。

Conclusion: DPO是低秩转向机制，它教模型如何表现得对齐，而非改变其内在信念，支持行为幻觉观点。

Abstract: Direct Preference Optimization (DPO) has become a standard recipe for aligning large language models, yet it is still unclear what kind of change it actually induces inside the network. This paper argues that DPO does not rewrite a models internal beliefs; instead, it acts as a low rank steering mechanism that nudges activations along a small number of preference directions. Using a simple derivation, we show that the DPO gradient depends only on the difference between the logit embeddings of preferred and dispreferred completions, implying a first order shift in the final hidden representation rather than a deep restructuring of semantics. We then extract an empirical steering vector from a DPO tuned model and demonstrate that adding this vector to base activations reproduces most of the aligned behavior, while subtracting it nearly restores the original model. Finally, spectral analyses reveal rank-one dominance and entropy collapse in upper layers, indicating that alignment is funneled through a narrow subspace. Taken together, these results support a behavioral illusion view of DPO: it teaches models how to act aligned, not what to believe.

</details>


### [115] [Investigating Data Pruning for Pretraining Biological Foundation Models at Scale](https://arxiv.org/abs/2512.12932)
*Yifan Wu,Jiyue Jiang,Xichen Ye,Yiqi Wang,Chang Zhou,Yitao Xu,Jiayang Chen,He Hu,Weizhong Zhang,Cheng Jin,Jiao Yuan,Yu Li*

Main category: cs.LG

TL;DR: 本文探讨生物基础模型预训练数据剪枝可行性，提出影响引导数据剪枝框架，在RNA和蛋白质任务验证其有效性，可降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有生物基础模型训练依赖大量序列和参数，计算成本高，可复现性和可访问性差，需解决这些问题。

Method: 提出基于生物领域的事后影响引导数据剪枝框架，引入基于子集的自影响公式，构建Top-k Influence和Coverage-Centric Influence两种选择策略。

Result: 在RNA-FM和ESM-C两个模型验证，在RNA任务极端剪枝率超99%时优于随机选择基线，在蛋白质任务也有泛化性，核心集表现优于大十倍的随机子集。

Conclusion: 影响引导数据剪枝可大幅降低生物基础模型预训练计算成本，推动生物AI研究更高效、可访问和可持续。

Abstract: Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.

</details>


### [116] [Large Language Models as Generalist Policies for Network Optimization](https://arxiv.org/abs/2512.11839)
*Duo Wu,Linjia Kang,Zhimin Wang,Fangxin Wang,Wei Zhang,Xuefeng Tao,Wei Yang,Le Zhang,Peng Cui,Zhi Wang*

Main category: cs.LG

TL;DR: 本文提出Trailblazer框架，利用大语言模型实现通用网络策略，在跨任务和跨环境泛化方面优于传统策略。


<details>
  <summary>Details</summary>
Motivation: 现有网络优化范式泛化能力差，大语言模型具备泛化潜力，需构建通用网络策略。

Method: 提出Trailblazer框架，包含网络对齐方案和自适应策略协作机制。

Result: 通过模拟和真实评估，Trailblazer在泛化能力上优于传统策略。

Conclusion: 验证了大语言模型可作为通用网络策略基础，Trailblazer是迈向通用驱动范式的第一步。

Abstract: Designing control policies to ensure robust network services is essential to modern digital infrastructure. However, the dominant paradigm for network optimization relies on designing specialist policies based on handcrafted rules or deep learning models, leading to poor generalization across diverse tasks and environments. In contrast, large language models (LLMs), pretrained on Internet-scale corpora, provide a rich and unified knowledge base that encodes fundamental networking principles. Combined with their emergent abilities in generalization to unseen scenarios, LLMs offer a transformative foundation for generalist network policies that can generalize across diverse tasks and environments with minimal adaptation. In this paper, we present Trailblazer, the first systematic framework to realize such a generalist policy for networking. Trailblazer incorporates a network alignment scheme to ground the LLM in specific networking tasks, and an adaptive policy collaboration mechanism that offloads simple control cases from the LLM to a lightweight policy for computational efficiency. Through extensive simulations and large-scale real-world online evaluation on Douyin (the Chinese version of TikTok), Trailblazer, powered by a single LLM, demonstrates stronger cross-task and cross-environment generalization than conventional specialist policies. Our results validate LLMs as the foundation for generalist network policies, and position Trailblazer as the first step toward the generalist-driven paradigm that enables strong generalization with minimal efforts in policy design.

</details>


### [117] [Learning Dynamics in Memristor-Based Equilibrium Propagation](https://arxiv.org/abs/2512.12428)
*Michael Döll,Andreas Müller,Bernd Ulmann*

Main category: cs.LG

TL;DR: 研究忆阻器驱动的非线性权重更新对平衡传播训练的神经网络收敛行为的影响，发现忆阻器电阻范围足够宽时收敛良好。


<details>
  <summary>Details</summary>
Motivation: 忆阻器内存计算可克服冯诺依曼瓶颈和内存墙限制，研究其非线性权重更新对神经网络收敛行为的影响。

Method: 通过电压 - 电流滞回特性表征六种忆阻器模型，并集成到 EBANA 框架，在两个基准分类任务上评估。

Result: 在忆阻器电阻范围至少有一个数量级的足够宽范围时，平衡传播能在非线性权重更新下实现稳健收敛。

Conclusion: 忆阻器电阻范围对平衡传播训练的神经网络收敛有重要影响，足够宽的范围利于收敛。

Abstract: Memristor-based in-memory computing has emerged as a promising paradigm to overcome the constraints of the von Neumann bottleneck and the memory wall by enabling fully parallelisable and energy-efficient vector-matrix multiplications. We investigate the effect of nonlinear, memristor-driven weight updates on the convergence behaviour of neural networks trained with equilibrium propagation (EqProp). Six memristor models were characterised by their voltage-current hysteresis and integrated into the EBANA framework for evaluation on two benchmark classification tasks. EqProp can achieve robust convergence under nonlinear weight updates, provided that memristors exhibit a sufficiently wide resistance range of at least an order of magnitude.

</details>


### [118] [Amortized Causal Discovery with Prior-Fitted Networks](https://arxiv.org/abs/2512.11840)
*Mateusz Sypniewski,Mateusz Olko,Mateusz Gajewski,Piotr Miłoś*

Main category: cs.LG

TL;DR: 提出新的摊销因果发现方法，用PFNs解决似然估计准确性问题，实验显示结构恢复有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有可微惩罚似然方法在似然估计上存在误差，即使样本量较大也无法发现合适结构。

Method: 提出新方法，利用Prior - Fitted Networks (PFNs)对依赖数据的似然估计进行摊销。

Result: 在合成、模拟和真实世界数据集实验中，结构恢复比标准基线有显著提升，且PFNs比传统神经网络方法似然估计更准确。

Conclusion: 新的基于PFNs的摊销因果发现方法能解决似然估计准确性问题，提升结构学习效果。

Abstract: In recent years, differentiable penalized likelihood methods have gained popularity, optimizing the causal structure by maximizing its likelihood with respect to the data. However, recent research has shown that errors in likelihood estimation, even on relatively large sample sizes, disallow the discovery of proper structures. We propose a new approach to amortized causal discovery that addresses the limitations of likelihood estimator accuracy. Our method leverages Prior-Fitted Networks (PFNs) to amortize data-dependent likelihood estimation, yielding more reliable scores for structure learning. Experiments on synthetic, simulated, and real-world datasets show significant gains in structure recovery compared to standard baselines. Furthermore, we demonstrate directly that PFNs provide more accurate likelihood estimates than conventional neural network-based approaches.

</details>


### [119] [Optimized Architectures for Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.12448)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: 研究用过度配置架构结合稀疏化学习紧凑、可解释的KANs，在多任务中表现好，解决科学机器学习关键矛盾。


<details>
  <summary>Details</summary>
Motivation: 架构增强改善KANs时带来的复杂性破坏了其可解释性，需新方法。

Method: 采用过度配置架构结合可微稀疏化，将架构搜索转为端到端优化问题。

Result: 在多类任务中展示出有竞争力或更优的准确性，发现更小模型，组合方法优于单一方法。

Conclusion: 该方法是实现更具表达性和可解释性模型的原则性路径，解决了科学机器学习的关键矛盾。

Abstract: Efforts to improve Kolmogorov-Arnold networks (KANs) with architectural enhancements have been stymied by the complexity those enhancements bring, undermining the interpretability that makes KANs attractive in the first place. Here we study overprovisioned architectures combined with sparsification to learn compact, interpretable KANs without sacrificing accuracy. Crucially, we focus on differentiable sparsification, turning architecture search into an end-to-end optimization problem. Across function approximation benchmarks, dynamical systems forecasting, and real-world prediction tasks, we demonstrate competitive or superior accuracy while discovering substantially smaller models. Overprovisioning and sparsification are synergistic, with the combination outperforming either alone. The result is a principled path toward models that are both more expressive and more interpretable, addressing a key tension in scientific machine learning.

</details>


### [120] [Meta-Continual Mobility Forecasting for Proactive Handover Prediction](https://arxiv.org/abs/2512.11841)
*Sasi Vardhan Reddy Mandapati*

Main category: cs.LG

TL;DR: 提出轻量级元持续预测框架用于蜂窝网络短期移动性预测，在多方面表现良好且适合边缘部署。


<details>
  <summary>Details</summary>
Motivation: 现实世界移动性非平稳，传统预测器易漂移导致切换失误，需要更好的预测方法。

Method: 提出集成GRU预测器、Reptile元初始化和EWMA残差检测器的轻量级元持续预测框架。

Result: 在零样本设置下实现4.46 m ADE和7.79 m FDE，10次射击时少样本ADE提高到3.71 m，从突然漂移中恢复速度比离线GRU快2 - 3倍；应用于切换预测时，F1提高到0.83，AUROC提高到0.90，减少切换失误和乒乓事件。

Conclusion: 该模型轻量级（128k参数），适合5G和6G系统边缘部署。

Abstract: Short-term mobility forecasting is a core requirement for proactive handover (HO) in cellular networks. Real-world mobility is highly non-stationary: abrupt turns, rapid speed changes, and unpredictable user behavior cause conventional predictors to drift, leading to mistimed or failed handovers. We propose a lightweight meta-continual forecasting framework that integrates a GRU-based predictor, Reptile meta-initialization for fast few-shot adaptation, and an EWMA residual detector that triggers compact online updates only when drift occurs. Evaluated on a reproducible GeoLife and DeepMIMO pipeline, our method achieves 4.46 m ADE and 7.79 m FDE in zero-shot settings, improves few-shot ADE to 3.71 m at 10-shot, and enables recovery from abrupt drift about 2 to 3 times faster than an offline GRU. When applied to downstream HO prediction, the approach improves F1 to 0.83 and AUROC to 0.90, with substantial reductions in missed-HO and ping-pong events. The model is lightweight (128k parameters) and suitable for edge deployment in 5G and 6G systems.

</details>


### [121] [Airport Passenger Flow Forecasting via Deformable Temporal-Spectral Transformer Approach](https://arxiv.org/abs/2512.11845)
*Wenbo Du,Lingling Han,Ying Xiong,Ling Zhang,Biyue Li,Yisheng Lv,Tong Guo*

Main category: cs.LG

TL;DR: 本文提出DTSFormer模型预测机场客流量，实验表明该模型在不同预测时域上均优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于固定大小块嵌入的时间序列预测方法难以建模机场客流量的复杂异质模式，需提出新方法。

Method: 提出DTSFormer，含多尺度可变形分区模块和联合时频谱滤波模块，对输入序列动态分区并提取特征，最后融合多频特征。

Result: 在真实客流数据上实验，DTSFormer在不同预测时域上均优于现有模型，可更好捕捉高频波动。

Conclusion: DTSFormer模型有效，其可变形分区模块发挥重要作用。

Abstract: Accurate forecasting of passenger flows is critical for maintaining the efficiency and resilience of airport operations. Recent advances in patch-based Transformer models have shown strong potential in various time series forecasting tasks. However, most existing methods rely on fixed-size patch embedding, making it difficult to model the complex and heterogeneous patterns of airport passenger flows. To address this issue, this paper proposes a deformable temporal-spectral transformer named DTSFormer that integrates a multiscale deformable partitioning module and a joint temporal-spectral filtering module. Specifically, the input sequence is dynamically partitioned into multiscale temporal patches via a novel window function-based masking, enabling the extraction of heterogeneous trends across different temporal stages. Then, within each scale, a frequency-domain attention mechanism is designed to capture both high- and low-frequency components, thereby emphasizing the volatility and periodicity inherent in airport passenger flows. Finally, the resulting multi-frequency features are subsequently fused in the time domain to jointly model short-term fluctuations and long-term trends. Comprehensive experiments are conducted on real-world passenger flow data collected at Beijing Capital International Airport from January 2023 to March 2024. The results indicate that the proposed method consistently outperforms state-of-the-art forecasting models across different prediction horizons. Further analysis shows that the deformable partitioning module aligns patch lengths with dominant periods and heterogeneous trends, enabling superior capture of sudden high-frequency fluctuations.

</details>


### [122] [Exploring Topological Bias in Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2512.11846)
*Yihan Zhang*

Main category: cs.LG

TL;DR: 本文研究异质图神经网络（HGNNs）拓扑偏差问题，提出去偏结构并结合对比学习，实验验证其能提升HGNNs性能和去偏。


<details>
  <summary>Details</summary>
Motivation: 半监督学习下GNNs在特定节点表现有偏差，与拓扑结构相关，现有工作对HGNNs拓扑偏差关注少。

Method: 对异质图邻接矩阵应用元加权，结合PageRank和节点标签信息构建投影，基于节点映射值差异提出去偏结构并进行对比学习。

Result: 构建的投影表明HGNNs中拓扑偏差普遍存在，在三个公开数据集上实验验证方法有效。

Conclusion: 所提方法能改善HGNNs性能和去偏

Abstract: Graph Neural Networks (GNNs) are characterized by their capacity of processing graph-structured data. However, due to the sparsity of labels under semi-supervised learning, they have been found to exhibit biased performance on specific nodes. This kind of bias has been validated to correlate with topological structure and is considered as a bottleneck of GNNs' performance. Existing work focuses on the study of homogeneous GNNs and little attention has been given to topological bias in Heterogeneous Graph Neural Networks (HGNNs). In this work, firstly, in order to distinguish distinct meta relations, we apply meta-weighting to the adjacency matrix of a heterogeneous graph. Based on the modified adjacency matrix, we leverage PageRank along with the node label information to construct a projection. The constructed projection effectively maps nodes to values that strongly correlated with model performance when using datasets both with and without intra-type connections, which demonstrates the universal existence of topological bias in HGNNs. To handle this bias, we propose a debiasing structure based on the difference in the mapped values of nodes and use it along with the original graph structure for contrastive learning. Experiments on three public datasets verify the effectiveness of the proposed method in improving HGNNs' performance and debiasing.

</details>


### [123] [Tiny Recursive Models on ARC-AGI-1: Inductive Biases, Identity Conditioning, and Test-Time Compute](https://arxiv.org/abs/2512.11847)
*Antonio Roye-Azar,Santiago Vargas-Naranjo,Dhruv Ghai,Nithin Balamurugan,Rayan Amir*

Main category: cs.LG

TL;DR: 本文对Tiny Recursive Models (TRM) 在ARC-AGI-1上的表现进行实证分析，得出四项行为发现和效率比较结果。


<details>
  <summary>Details</summary>
Motivation: 原工作虽表明TRM在ARC风格任务中有良好表现，但不清楚其性能源于架构、测试时计算还是特定任务先验，因此进行实证分析。

Method: 对ARC Prize TRM检查点在ARC-AGI-1上进行实验，包括测试时增强、谜题身份消融、递归轨迹分析、不同增强机制下的早期训练实验，并与Llama 3 8B的QLoRA微调对比。

Result: 测试时增强和多数投票集成对性能提升显著；TRM严格依赖任务标识符；多数最终准确率在第一次递归步骤实现；大量增强可改善多样本成功率；TRM在吞吐量和内存使用上更高效。

Conclusion: TRM在ARC-AGI-1上的性能源于效率、特定任务条件和激进的测试时计算，而非深度内部推理。

Abstract: Tiny Recursive Models (TRM) were proposed as a parameter-efficient alternative to large language models for solving Abstraction and Reasoning Corpus (ARC) style tasks. The original work reports strong performance and suggests that recursive latent updates enable non-trivial reasoning, but it remains unclear how much of this performance stems from architecture, test-time compute, or task-specific priors. In this technical note, we empirically analyze the ARC Prize TRM checkpoint on ARC-AGI-1 and report four behavioral findings and an efficiency comparison. First, we show that test-time augmentation and majority-vote ensembling account for a substantial fraction of reported performance: the 1000-sample voting pipeline improves Pass@1 by about 11 percentage points over single-pass canonical inference. Second, a puzzle-identity ablation reveals strict dependence on task identifiers: replacing the correct puzzle ID with a blank or random token yields zero accuracy. Third, a recursion trajectory analysis shows that most of the final accuracy is achieved at the first recursion step and that performance saturates after few latent updates, indicating shallow effective recursion. Fourth, early-stage training experiments under canonical versus heavy augmentation regimes suggest that heavy augmentation broadens the distribution of candidate solutions and improves multi-sample success. Finally, we compare TRM with a naive QLoRA fine-tune of Llama 3 8B on canonical ARC-AGI-1, finding that TRM's non-autoregressive design achieves much higher throughput and substantially lower memory usage in this setting. Overall, TRM's ARC-AGI-1 performance appears to arise from an interaction between efficiency, task-specific conditioning, and aggressive test-time compute rather than deep internal reasoning.

</details>


### [124] [Explainable AI for Smart Greenhouse Control: Interpretability of Temporal Fusion Transformer in the Internet of Robotic Things](https://arxiv.org/abs/2512.11852)
*Muhammad Jawad Bashir,Shagufta Henna,Eoghan Furey*

Main category: cs.LG

TL;DR: 研究利用TFT模型自动设置温室执行器，采用解释技术增强模型可解释性，模型在不平衡数据集上测试准确率达95%，展示传感器对温室调整的影响。


<details>
  <summary>Details</summary>
Motivation: 现有智能温室时间序列预测模型多为黑盒，缺乏可解释决策机制，而智能农业需要信任、透明和合规。

Method: 利用Temporal Fusion Transformer (TFT) 模型自动化温室管理的执行器设置，采用模型固有解释、LIME和SHAP等局部和全局解释技术。

Result: 训练的TFT模型在自动化温室环境执行器控制设置的不平衡数据集上测试准确率达95%，展示了各传感器对温室实时调整的不同影响。

Conclusion: 所采用的方法确保了模型决策的透明度，可进行自适应微调以提高作物产量和资源效率。

Abstract: The integration of the Internet of Robotic Things (IoRT) in smart greenhouses has revolutionised precision agriculture by enabling efficient and autonomous environmental control. However, existing time series forecasting models in such setups often operate as black boxes, lacking mechanisms for explainable decision-making, which is a critical limitation when trust, transparency, and regulatory compliance are paramount in smart farming practices. This study leverages the Temporal Fusion Transformer (TFT) model to automate actuator settings for optimal greenhouse management. To enhance interpretability and trust in the model decision-making process, both local and global explanation techniques were employed using model-inherent interpretation, local interpretable model-agnostic explanations (LIME), and SHapley additive explanations (SHAP). These explainability methods provide information on how different sensor readings, such as temperature, humidity, CO2 levels, light, and outer climate, contribute to actuator control decisions in an automated greenhouse. The trained TFT model achieved a test accuracy of 95% on a class-imbalanced dataset for actuator control settings in an automated greenhouse environment. The results demonstrate the varying influence of each sensor on real-time greenhouse adjustments, ensuring transparency and enabling adaptive fine-tuning for improved crop yield and resource efficiency.

</details>


### [125] [Rep Smarter, Not Harder: AI Hypertrophy Coaching with Wearable Sensors and Edge Neural Networks](https://arxiv.org/abs/2512.11854)
*Grant King,Musa Azeem,Savannah Noblitt,Ramtin Zand,Homayoun Valafar*

Main category: cs.LG

TL;DR: 文章提出用单手腕惯性测量单元实时反馈抗阻训练接近力竭状态的系统，经测试可行，为训练强度反馈提供实用方法。


<details>
  <summary>Details</summary>
Motivation: 主观评估重复储备数不可靠，需客观方法优化抗阻训练、平衡接近力竭状态与疲劳管理。

Method: 提出适用于边缘部署的两阶段流程，先用基于ResNet的模型实时分割重复动作，再用分类模型识别接近力竭状态。

Result: 分割模型F1分数0.83，接近力竭分类器F1分数0.82，树莓派5推理延迟112毫秒，iPhone 16为23.5毫秒。

Conclusion: 该方法用最少硬件实现客观实时训练强度反馈，为AI驱动的增肌训练工具奠定基础。

Abstract: Optimizing resistance training for hypertrophy requires balancing proximity to muscular failure, often quantified by Repetitions in Reserve (RiR), with fatigue management. However, subjective RiR assessment is unreliable, leading to suboptimal training stimuli or excessive fatigue. This paper introduces a novel system for real-time feedback on near-failure states (RiR $\le$ 2) during resistance exercise using only a single wrist-mounted Inertial Measurement Unit (IMU). We propose a two-stage pipeline suitable for edge deployment: first, a ResNet-based model segments repetitions from the 6-axis IMU data in real-time. Second, features derived from this segmentation, alongside direct convolutional features and historical context captured by an LSTM, are used by a classification model to identify exercise windows corresponding to near-failure states. Using a newly collected dataset from 13 diverse participants performing preacher curls to failure (631 total reps), our segmentation model achieved an F1 score of 0.83, and the near-failure classifier achieved an F1 score of 0.82 under simulated real-time evaluation conditions (1.6 Hz inference rate). Deployment on a Raspberry Pi 5 yielded an average inference latency of 112 ms, and on an iPhone 16 yielded 23.5 ms, confirming the feasibility for edge computation. This work demonstrates a practical approach for objective, real-time training intensity feedback using minimal hardware, paving the way for accessible AI-driven hypertrophy coaching tools that help users manage intensity and fatigue effectively.

</details>


### [126] [Achieving Approximate Symmetry Is Exponentially Easier than Exact Symmetry](https://arxiv.org/abs/2512.11855)
*Behrooz Tahmasebi,Melanie Weber*

Main category: cs.LG

TL;DR: 本文研究精确对称和近似对称在机器学习模型中的成本，引入平均复杂度框架，得出精确对称需线性平均复杂度，近似对称只需对数平均复杂度的指数分离结果，理论上证明近似对称在实践中的优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对精确对称和近似对称的理论理解及直接比较，本文旨在研究二者成本差异。

Method: 引入平均复杂度框架来量化通过平均实现对称的成本。

Result: 在标准条件下，实现精确对称需要线性平均复杂度，而近似对称只需对数平均复杂度，存在指数分离。

Conclusion: 提供了精确对称和近似对称的首个理论分离，从理论上证明近似对称在实践中可能更优，研究工具和技术对机器学习中对称性的更广泛研究有价值。

Abstract: Enforcing exact symmetry in machine learning models often yields significant gains in scientific applications, serving as a powerful inductive bias. However, recent work suggests that relying on approximate symmetry can offer greater flexibility and robustness. Despite promising empirical evidence, there has been little theoretical understanding, and in particular, a direct comparison between exact and approximate symmetry is missing from the literature. In this paper, we initiate this study by asking: What is the cost of enforcing exact versus approximate symmetry? To address this question, we introduce averaging complexity, a framework for quantifying the cost of enforcing symmetry via averaging. Our main result is an exponential separation: under standard conditions, achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be attained with only logarithmic averaging complexity. To the best of our knowledge, this provides the first theoretical separation of these two cases, formally justifying why approximate symmetry may be preferable in practice. Beyond this, our tools and techniques may be of independent interest for the broader study of symmetries in machine learning.

</details>


### [127] [GCoDE: Efficient Device-Edge Co-Inference for GNNs via Architecture-Mapping Co-Search](https://arxiv.org/abs/2512.11856)
*Ao Zhou,Jianlei Yang,Tong Qiao,Yingjie Qi,Zhi Yang,Weisheng Zhao,Chunming Hu*

Main category: cs.LG

TL;DR: 提出GCoDE框架解决GNN在边缘设备推理效率问题，实现加速和节能。


<details>
  <summary>Details</summary>
Motivation: GNN在边缘设备推理有高计算成本和资源限制问题，现有GNN设备 - 边缘协同推理研究少且传统模型分区方法无效。

Method: 提出GCoDE框架，将设备通信抽象为操作，统一架构和映射方案优化，引入能耗预测方法，用基于约束的随机搜索策略找最优解，有集成协同推理引擎。

Result: GCoDE在不同应用和系统配置下比现有方法加速达44.9倍，节能98.2%。

Conclusion: GCoDE能有效解决GNN在边缘设备的推理效率和能耗问题。

Abstract: Graph Neural Networks (GNNs) have emerged as the state-of-the-art graph learning method. However, achieving efficient GNN inference on edge devices poses significant challenges, limiting their application in real-world edge scenarios. This is due to the high computational cost of GNNs and limited hardware resources on edge devices, which prevent GNN inference from meeting real-time and energy requirements. As an emerging paradigm, device-edge co-inference shows potential for improving inference efficiency and reducing energy consumption on edge devices. Despite its potential, research on GNN device-edge co-inference remains scarce, and our findings show that traditional model partitioning methods are ineffective for GNNs. To address this, we propose GCoDE, the first automatic framework for GNN architecture-mapping Co-design and deployment on Device-Edge hierarchies. By abstracting the device communication process into an explicit operation, GCoDE fuses the architecture and mapping scheme in a unified design space for joint optimization. Additionally, GCoDE's system performance awareness enables effective evaluation of architecture efficiency across diverse heterogeneous systems. By analyzing the energy consumption of various GNN operations, GCoDE introduces an energy prediction method that improves energy assessment accuracy and identifies energy-efficient solutions. Using a constraint-based random search strategy, GCoDE identifies the optimal solution in 1.5 hours, balancing accuracy and efficiency. Moreover, the integrated co-inference engine in GCoDE enables efficient deployment and execution of GNN co-inference. Experimental results show that GCoDE can achieve up to 44.9x speedup and 98.2% energy reduction compared to existing approaches across diverse applications and system configurations.

</details>


### [128] [TopicProphet: Prophesies on Temporal Topic Trends and Stocks](https://arxiv.org/abs/2512.11857)
*Olivia Kim*

Main category: cs.LG

TL;DR: 提出TopicProphet框架分析历史时期以改善股票预测，比现有方法效果更好。


<details>
  <summary>Details</summary>
Motivation: 定量股票数据缺乏因果逻辑且市场变化快，数据积累困难，股票难以预测。

Method: 提出TopicProphet框架，通过主题建模、时间分析、断点检测和片段优化等步骤，检测最佳训练时间段。

Result: 改善了预测，提供了细微模式，解决了相关股票训练数据不足的问题。

Conclusion: TopicProphet在捕捉预测金融百分比变化的最佳训练数据方面，比现有技术方法有更好的结果。

Abstract: Stocks can't be predicted. Despite many hopes, this premise held itself true for many years due to the nature of quantitative stock data lacking causal logic along with rapid market changes hindering accumulation of significant data for training models. To undertake this matter, we propose a novel framework, TopicProphet, to analyze historical eras that share similar public sentiment trends and historical background. Our research deviates from previous studies that identified impacts of keywords and sentiments - we expand on that method by a sequence of topic modeling, temporal analysis, breakpoint detection and segment optimization to detect the optimal time period for training. This results in improving predictions by providing the model with nuanced patterns that occur from that era's socioeconomic and political status while also resolving the shortage of pertinent stock data to train on. Through extensive analysis, we conclude that TopicProphet produces improved outcomes compared to the state-of-the-art methods in capturing the optimal training data for forecasting financial percentage changes.

</details>


### [129] [BOOST: BOttleneck-Optimized Scalable Training Framework for Low-Rank Large Language Models](https://arxiv.org/abs/2512.12131)
*Zhengyang Wang,Ziyue Liu,Ruijie Zhang,Avinash Maurya,Paul Hovland,Bogdan Nicolae,Franck Cappello,Zheng Zhang*

Main category: cs.LG

TL;DR: 现有Transformer模型预训练因计算和通信成本受限，低秩瓶颈架构发展遇阻，提出BOOST框架，评估显示有效提升训练速度。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer模型预训练规模受计算和通信成本限制，低秩瓶颈架构在标准张量并行下扩展性差，直接使用3D并行有过多通信和低GPU利用率问题。

Method: 提出BOOST框架，引入新颖的瓶颈感知张量并行，结合在线RMSNorm、线性层分组和低秩激活检查点等优化。

Result: 在不同低秩瓶颈架构上，BOOST比全秩模型基线有1.46 - 1.91倍加速，比简单集成3D并行的低秩模型有1.87 - 2.27倍加速，提高了GPU利用率并降低通信开销。

Conclusion: BOOST框架能有效提高低秩瓶颈架构的训练效率和GPU利用率，降低通信开销。

Abstract: The scale of transformer model pre-training is constrained by the increasing computation and communication cost. Low-rank bottleneck architectures offer a promising solution to significantly reduce the training time and memory footprint with minimum impact on accuracy. Despite algorithmic efficiency, bottleneck architectures scale poorly under standard tensor parallelism. Simply applying 3D parallelism designed for full-rank methods leads to excessive communication and poor GPU utilization. To address this limitation, we propose BOOST, an efficient training framework tailored for large-scale low-rank bottleneck architectures. BOOST introduces a novel Bottleneck-aware Tensor Parallelism, and combines optimizations such as online-RMSNorm, linear layer grouping, and low-rank activation checkpointing to achieve end-to-end training speedup. Evaluations on different low-rank bottleneck architectures demonstrate that BOOST achieves 1.46-1.91$\times$ speedup over full-rank model baselines and 1.87-2.27$\times$ speedup over low-rank model with naively integrated 3D parallelism, with improved GPU utilization and reduced communication overhead.

</details>


### [130] [Adaptive Path Integral Diffusion: AdaPID](https://arxiv.org/abs/2512.11858)
*Michael Chertkov,Hamidreza Behjoo*

Main category: cs.LG

TL;DR: 提出基于路径的时变刚度Harmonic PID调度选择框架，引入QoS诊断，2D实验显示该方法在固定积分预算下提升多项性能。


<details>
  <summary>Details</summary>
Motivation: 扩散采样器需选择合适中间时间动态调度，目前缺乏有效方法。

Method: 开发带时变刚度Harmonic PID路径调度选择框架，用PWC参数化和层次细化，引入QoS诊断，在高斯混合目标下保留闭式格林函数比率和数值稳定的预测状态图和分数。

Result: 2D实验表明QoS驱动的PWC调度在固定积分预算下，持续改善提前退出保真度、尾部精度、动力学调节和物种形成时机。

Conclusion: 所提出的调度选择框架和QoS诊断方法有效，能提升扩散采样器性能。

Abstract: Diffusion-based samplers -- Score Based Diffusions, Bridge Diffusions and Path Integral Diffusions -- match a target at terminal time, but the real leverage comes from choosing the schedule that governs the intermediate-time dynamics. We develop a path-wise schedule -- selection gramework for Harmonic PID with a time-varying stiffness, exploiting Piece-Wise-Constant(PWC) parametrizations and a simple hierarchical refinement. We introduce schedule-sensitive Quality-of-Sampling (QoS) diagnostics. Assuming a Gaussian-Mixture (GM) target, we retain closed-form Green functions' ration and numerically stable, Neural-Network free oracles for predicted-state maps and score. Experiments in 2D show that QoS driven PWC schedules consistently improve early-exit fidelity, tail accuracy, conditioning of the dynamics, and speciation (label-selection) timing at fixed integration budgets.

</details>


### [131] [Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion](https://arxiv.org/abs/2512.11859)
*Michael Chertkov*

Main category: cs.LG

TL;DR: 提出Guided Harmonic Path - Integral Diffusion (GH - PID)框架用于引导随机最优传输，开发诊断方法，通过三个二维导航场景验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 为有硬终端分布和软路径成本的引导随机最优传输提供线性可解框架，使随机最优传输过程可解释。

Method: 提出GH - PID框架，利用低维引导协议塑造轨迹集合，开发以引导为中心的诊断方法，通过二维导航的三个场景进行验证。

Result: 在三个导航场景中，GH - PID生成了几何感知、信任感知的轨迹，满足规定的终端分布并系统降低了综合成本。

Conclusion: GH - PID是经验随机最优传输的可解释变分方法，能有效处理相关问题。

Abstract: We introduce Guided Harmonic Path-Integral Diffusion (GH-PID), a linearly-solvable framework for guided Stochastic Optimal Transport (SOT) with a hard terminal distribution and soft, application-driven path costs. A low-dimensional guidance protocol shapes the trajectory ensemble while preserving analytic structure: the forward and backward Kolmogorov equations remain linear, the optimal score admits an explicit Green-function ratio, and Gaussian-Mixture Model (GMM) terminal laws yield closed-form expressions. This enables stable sampling and differentiable protocol learning under exact terminal matching.
  We develop guidance-centric diagnostics -- path cost, centerline adherence, variance flow, and drift effort -- that make GH-PID an interpretable variational ansatz for empirical SOT. Three navigation scenarios illustrated in 2D: (i) Case A: hand-crafted protocols revealing how geometry and stiffness shape lag, curvature effects, and mode evolution; (ii) Case B: single-task protocol learning, where a PWC centerline is optimized to minimize integrated cost; (iii) Case C: multi-expert fusion, in which a commander reconciles competing expert/teacher trajectories and terminal beliefs through an exact product-of-experts law and learns a consensus protocol. Across all settings, GH-PID generates geometry-aware, trust-aware trajectories that satisfy the prescribed terminal distribution while systematically reducing integrated cost.

</details>


### [132] [Learning to Retrieve with Weakened Labels: Robust Training under Label Noise](https://arxiv.org/abs/2512.13237)
*Arnab Sharma*

Main category: cs.LG

TL;DR: 针对训练数据中稀疏标注和标签噪声问题，本文提出标签弱化方法生成鲁棒检索模型，实验表明该方法优于10种SOTA损失函数。


<details>
  <summary>Details</summary>
Motivation: 训练数据存在稀疏标注和标签噪声，现有方法需调参或增加训练复杂度，因此要找到新方法生成鲁棒检索模型。

Method: 采用标签弱化方法，允许查询文档对有一组合理标签，结合观察监督和模型置信度分数；用语义感知噪声生成技术生成不同噪声比例；对两个检索模型、一个重排序模型在四个排名数据集上评估。

Result: 标签弱化方法在检索任务中的表现优于10种不同的SOTA损失函数。

Conclusion: 标签弱化方法能有效应对训练数据中的标签噪声问题，提高检索任务性能。

Abstract: Neural Encoders are frequently used in the NLP domain to perform dense retrieval tasks, for instance, to generate the candidate documents for a given query in question-answering tasks. However, sparse annotation and label noise in the training data make it challenging to train or fine-tune such retrieval models. Although existing works have attempted to mitigate these problems by incorporating modified loss functions or data cleaning, these approaches either require some hyperparameters to tune during training or add substantial complexity to the training setup. In this work, we consider a label weakening approach to generate robust retrieval models in the presence of label noise. Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores. We perform an extensive evaluation considering two retrieval models, one re-ranking model, considering four diverse ranking datasets. To this end, we also consider a realistic noisy setting by using a semantic-aware noise generation technique to generate different ratios of noise. Our initial results show that label weakening can improve the performance of the retrieval tasks in comparison to 10 different state-of-the-art loss functions.

</details>


### [133] [Spectral Sentinel: Scalable Byzantine-Robust Decentralized Federated Learning via Sketched Random Matrix Theory on Blockchain](https://arxiv.org/abs/2512.12617)
*Animesh Mishra*

Main category: cs.LG

TL;DR: 现有去中心化联邦学习（DFL）防御方法存在可扩展性困境，本文提出Spectral Sentinel框架检测拜占庭攻击，证明其最优性并验证效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: DFL易受拜占庭客户端攻击，现有防御方法存在可扩展性问题，如距离过滤会拒绝合法更新、几何中位数方法成本高、许多认证防御仅适用于小参数模型。

Method: 提出Spectral Sentinel框架，利用随机矩阵理论签名，结合Frequent Directions草图与数据相关的Marchenko - Pastur跟踪。

Result: 在$(σ,f)$威胁模型下证明$(ε,δ)$ - 拜占庭弹性和收敛率，给出匹配的信息论下界；在Polygon网络集成区块链实现系统，在144种配置下平均准确率达78.4%，优于基线方法。

Conclusion: Spectral Sentinel框架在DFL拜占庭攻击检测和聚合方面具有有效性和最优性。

Abstract: Decentralized federated learning (DFL) enables collaborative model training without centralized trust, but it remains vulnerable to Byzantine clients that poison gradients under heterogeneous (Non-IID) data. Existing defenses face a scalability trilemma: distance-based filtering (e.g., Krum) can reject legitimate Non-IID updates, geometric-median methods incur prohibitive $O(n^2 d)$ cost, and many certified defenses are evaluated only on models below 100M parameters. We propose Spectral Sentinel, a Byzantine detection and aggregation framework that leverages a random-matrix-theoretic signature: honest Non-IID gradients produce covariance eigenspectra whose bulk follows the Marchenko-Pastur law, while Byzantine perturbations induce detectable tail anomalies. Our algorithm combines Frequent Directions sketching with data-dependent MP tracking, enabling detection on models up to 1.5B parameters using $O(k^2)$ memory with $k \ll d$. Under a $(σ,f)$ threat model with coordinate-wise honest variance bounded by $σ^2$ and $f < 1/2$ adversaries, we prove $(ε,δ)$-Byzantine resilience with convergence rate $O(σf / \sqrt{T} + f^2 / T)$, and we provide a matching information-theoretic lower bound $Ω(σf / \sqrt{T})$, establishing minimax optimality. We implement the full system with blockchain integration on Polygon networks and validate it across 144 attack-aggregator configurations, achieving 78.4 percent average accuracy versus 48-63 percent for baseline methods.

</details>


### [134] [An Operator-Consistent Graph Neural Network for Learning Diffusion Dynamics on Irregular Meshes](https://arxiv.org/abs/2512.11860)
*Yuelian Li,Andrew Rushing Hands*

Main category: cs.LG

TL;DR: 提出算子一致图神经网络（OCGNN - PINN）近似偏微分方程演化，在不规则网格上评估，相比基线模型有更好的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 经典数值方法在不规则网格上求解偏微分方程不稳定，而实际多物理相互作用常发生在不规则网格上。

Method: 开发OCGNN - PINN，将节点 - 边消息传递与一致性损失相结合，通过图关联矩阵强制执行梯度 - 散度关系。

Result: 在物理驱动的演化网格和真实扫描表面的扩散过程中评估，相比图卷积和多层感知机基线模型，具有更好的时间稳定性和预测准确性，接近Crank - Nicolson求解器在非结构化域上的性能。

Conclusion: OCGNN - PINN在不规则网格上求解偏微分方程是有效的，能提高稳定性和准确性。

Abstract: Classical numerical methods solve partial differential equations (PDEs) efficiently on regular meshes, but many of them become unstable on irregular domains. In practice, multiphysics interactions such as diffusion, damage, and healing often take place on irregular meshes. We develop an operator-consistent graph neural network (OCGNN-PINN) that approximates PDE evolution under physics-informed constraints. It couples node-edge message passing with a consistency loss enforcing the gradient-divergence relation through the graph incidence matrix, ensuring that discrete node and edge dynamics remain structurally coupled during temporal rollout. We evaluate the model on diffusion processes over physically driven evolving meshes and real-world scanned surfaces. The results show improved temporal stability and prediction accuracy compared with graph convolutional and multilayer perceptron baselines, approaching the performance of Crank-Nicolson solvers on unstructured domains.

</details>


### [135] [No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction](https://arxiv.org/abs/2512.13300)
*Qinglin Jia,Zhaocheng Du,Chuhan Wu,Huifeng Guo,Ruiming Tang,Shuting Shi,Muyu Zhang*

Main category: cs.LG

TL;DR: 本文提出针对非对称多标签数据的细粒度知识迁移框架KAML，引入ADM策略、HKE机制和排序损失策略，在离线和在线测试中表现优于现有MTL基线。


<details>
  <summary>Details</summary>
Motivation: 现实广告系统中CVR预测存在多任务数据标签不完整问题，训练和部署数据分布不匹配，需有效训练统一模型处理不完整和有偏差的多标签数据。

Method: 提出KAML框架，引入ADM策略利用非对称多标签数据，提出HKE机制处理噪声，结合排序损失策略提升模型。

Result: 在离线行业数据集和在线A/B测试中，KAML相比现有MTL基线有显著性能提升。

Conclusion: KAML框架能有效解决多任务学习中不完整和有偏差多标签数据的训练问题，提高CVR预测性能。

Abstract: In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.

</details>


### [136] [SPARK: Igniting Communication-Efficient Decentralized Learning via Stage-wise Projected NTK and Accelerated Regularization](https://arxiv.org/abs/2512.12737)
*Li Xia*

Main category: cs.LG

TL;DR: 提出SPARK方法解决DFL统计异质性和通信开销问题，实现98.7%通信减少，性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决分散式联邦学习（DFL）中统计异质性和通信开销问题，且NTK方法传输全雅可比矩阵在带宽受限边缘网络不可行。

Method: 提出SPARK方法，集成随机投影雅可比压缩、阶段退火蒸馏和Nesterov动量加速。

Result: 相比NTK - DFL实现98.7%通信减少，保持收敛速度和更高精度，有动量时达目标性能快3倍。

Conclusion: SPARK在通信高效分散式学习中达到最优结果，可在带宽受限边缘环境实际部署。

Abstract: Decentralized federated learning (DFL) faces critical challenges from statistical heterogeneity and communication overhead. While NTK-based methods achieve faster convergence, transmitting full Jacobian matrices is impractical for bandwidth-constrained edge networks. We propose SPARK, synergistically integrating random projection-based Jacobian compression, stage-wise annealed distillation, and Nesterov momentum acceleration. Random projections compress Jacobians while preserving spectral properties essential for convergence. Stage-wise annealed distillation transitions from pure NTK evolution to neighbor-regularized learning, counteracting compression noise. Nesterov momentum accelerates convergence through stable accumulation enabled by distillation smoothing. SPARK achieves 98.7% communication reduction compared to NTK-DFL while maintaining convergence speed and superior accuracy. With momentum, SPARK reaches target performance 3 times faster, establishing state-of-the-art results for communication-efficient decentralized learning and enabling practical deployment in bandwidth-limited edge environments.

</details>


### [137] [Hierarchical Task Offloading and Trajectory Optimization in Low-Altitude Intelligent Networks Via Auction and Diffusion-based MARL](https://arxiv.org/abs/2512.11862)
*Jiahao You,Ziye Jia,Can Cui,Chao Dong,Qihui Wu,Zhu Han*

Main category: cs.LG

TL;DR: 提出集成空地协作网络解决低空智能网络（LAINs）面临的挑战，设计分层学习框架，仿真显示该框架在能效、任务成功率和收敛性能上优于基线。


<details>
  <summary>Details</summary>
Motivation: LAINs在动态和基础设施受限环境有优势，但面临无人机能量受限、随机任务到达和异构计算资源等挑战。

Method: 提出集成空地协作网络，将其建模为时间相关整数非线性规划问题；设计分层学习框架，大尺度用Vickrey - Clarke - Groves拍卖机制进行轨迹分配，小尺度提出扩散 - 异构代理近端策略优化算法。

Result: 通过大量仿真表明，所提框架在能效、任务成功率和收敛性能上优于基线。

Conclusion: 所设计的分层学习框架能有效解决LAINs面临的问题，提高系统性能。

Abstract: The low-altitude intelligent networks (LAINs) emerge as a promising architecture for delivering low-latency and energy-efficient edge intelligence in dynamic and infrastructure-limited environments. By integrating unmanned aerial vehicles (UAVs), aerial base stations, and terrestrial base stations, LAINs can support mission-critical applications such as disaster response, environmental monitoring, and real-time sensing. However, these systems face key challenges, including energy-constrained UAVs, stochastic task arrivals, and heterogeneous computing resources. To address these issues, we propose an integrated air-ground collaborative network and formulate a time-dependent integer nonlinear programming problem that jointly optimizes UAV trajectory planning and task offloading decisions. The problem is challenging to solve due to temporal coupling among decision variables. Therefore, we design a hierarchical learning framework with two timescales. At the large timescale, a Vickrey-Clarke-Groves auction mechanism enables the energy-aware and incentive-compatible trajectory assignment. At the small timescale, we propose the diffusion-heterogeneous-agent proximal policy optimization, a generative multi-agent reinforcement learning algorithm that embeds latent diffusion models into actor networks. Each UAV samples actions from a Gaussian prior and refines them via observation-conditioned denoising, enhancing adaptability and policy diversity. Extensive simulations show that our framework outperforms baselines in energy efficiency, task success rate, and convergence performance.

</details>


### [138] [Phase transitions reveal hierarchical structure in deep neural networks](https://arxiv.org/abs/2512.11866)
*Ibrahim Talha Ersoy,Andrés Fernando Cardozo Licha,Karoline Wiesner*

Main category: cs.LG

TL;DR: 本文统一了DNN训练中的三种现象，分析相转变与鞍点关系，提出算法验证模式连通性，揭示关键训练现象的几何起源和精度盆地层次结构。


<details>
  <summary>Details</summary>
Motivation: DNN训练的许多现象仍未被充分理解，需要统一解释框架。

Method: 通过分析损失和误差景观的几何特征，用L2正则化器探测误差景观几何，设计简单算法。

Result: 证实DNN学习中的相转变由损失景观中的鞍点控制，在MNIST数据集上验证模式连通性，表明鞍点会引发编码不同数字类别的模型间的转变。

Conclusion: 确立了DNN关键训练现象的几何起源，发现类似统计物理中相的精度盆地层次结构。

Abstract: Training Deep Neural Networks relies on the model converging on a high-dimensional, non-convex loss landscape toward a good minimum. Yet, much of the phenomenology of training remains ill understood. We focus on three seemingly disparate observations: the occurrence of phase transitions reminiscent of statistical physics, the ubiquity of saddle points, and phenomenon of mode connectivity relevant for model merging. We unify these within a single explanatory framework, the geometry of the loss and error landscapes. We analytically show that phase transitions in DNN learning are governed by saddle points in the loss landscape. Building on this insight, we introduce a simple, fast, and easy to implement algorithm that uses the L2 regularizer as a tool to probe the geometry of error landscapes. We apply it to confirm mode connectivity in DNNs trained on the MNIST dataset by efficiently finding paths that connect global minima. We then show numerically that saddle points induce transitions between models that encode distinct digit classes. Our work establishes the geometric origin of key training phenomena in DNNs and reveals a hierarchy of accuracy basins analogous to phases in statistical physics.

</details>


### [139] [Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations](https://arxiv.org/abs/2512.11946)
*Pramudita Satria Palar,Paul Saves,Rommel G. Regis,Koji Shimoyama,Shigeru Obayashi,Nicolas Verstaevel,Joseph Morlier*

Main category: cs.LG

TL;DR: 本文提出基于个体条件期望（ICE）曲线的全局敏感性度量方法，解决偏依赖图（PDP）在强交互作用下的局限，经案例验证该方法更优。


<details>
  <summary>Details</summary>
Motivation: PDP在强交互作用下其全局敏感性度量易产生误导，需新方法解决此局限。

Method: 提出基于ICE曲线的全局敏感性度量方法，计算特征重要性及标准差，给出数学证明，引入ICE相关值量化交互影响，并进行多案例对比评估。

Result: ICE特征重要性比传统PDP方法提供更丰富见解，PDP、ICE和SHAP的可视化解释可相互补充。

Conclusion: 基于ICE曲线的全局敏感性度量方法优于PDP方法，不同可视化方法可提供多视角。

Abstract: Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.

</details>


### [140] [On the Dangers of Bootstrapping Generation for Continual Learning and Beyond](https://arxiv.org/abs/2512.11867)
*Daniil Zverev,A. Sophia Koepke,Joao F. Henriques*

Main category: cs.LG

TL;DR: 本文研究合成数据重复训练带来的分布漂移和性能下降问题，指出其引入偏差和方差，使生成模型崩溃，GER方法失效。


<details>
  <summary>Details</summary>
Motivation: 合成数据用于模型训练普遍，但重复训练会导致分布漂移和性能下降。

Method: 从持续学习角度研究自举过程，与GER方法建立联系，进行统计分析和实证研究。

Result: 合成数据给训练目标引入显著偏差和方差，使最大似然估计可靠性降低，流行生成模型崩溃，最先进GER方法无法保持潜在空间对齐。

Conclusion: 对持续学习中使用合成数据提出严重担忧。

Abstract: The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.

</details>


### [141] [Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning](https://arxiv.org/abs/2512.12046)
*Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.LG

TL;DR: 提出基于Eikonal偏微分方程的Eik - QRL及Eik - HiQRL，在离线目标条件导航和操作任务有良好表现。


<details>
  <summary>Details</summary>
Motivation: 缓解奖励设计困难，改进Quasimetric RL（QRL）的不足，提升分布外泛化能力。

Method: 提出Eik - QRL，基于Eikonal偏微分方程对QRL进行连续时间重新表述；引入Eik - HiQRL，将Eik - QRL集成到分层分解中。

Result: Eik - HiQRL在离线目标条件导航中达到了最先进的性能，在操作任务中比QRL有持续提升，与时间差分方法相当。

Conclusion: Eik - QRL和Eik - HiQRL是有效的目标条件强化学习方法，有理论保证且实践效果好。

Abstract: Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.

</details>


### [142] [SigTime: Learning and Visually Explaining Time Series Signatures](https://arxiv.org/abs/2512.12076)
*Yu-Chia Huang,Juntong Chen,Dongyu Liu,Kwan-Liu Ma*

Main category: cs.LG

TL;DR: 为解决现有时间序列模式发现方法的不足，引入新学习框架，联合训练两个Transformer模型，并开发可视化分析系统，经多数据集评估和实际场景验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模式发现方法存在高计算复杂度、可解释性有限和难以捕捉有意义时间结构等问题。

Method: 引入联合训练两个Transformer模型的学习框架，使用基于形状特征和传统特征工程的互补时间序列表示；开发可视化分析系统SigTIme。

Result: 在多个公开数据集和一个专有临床数据集上定量评估学习框架；通过两个实际场景与领域专家合作展示系统有效性。

Conclusion: 提出的学习框架和可视化分析系统能有效解决现有时间序列模式发现方法的问题。

Abstract: Understanding and distinguishing temporal patterns in time series data is essential for scientific discovery and decision-making. For example, in biomedical research, uncovering meaningful patterns in physiological signals can improve diagnosis, risk assessment, and patient outcomes. However, existing methods for time series pattern discovery face major challenges, including high computational complexity, limited interpretability, and difficulty in capturing meaningful temporal structures. To address these gaps, we introduce a novel learning framework that jointly trains two Transformer models using complementary time series representations: shapelet-based representations to capture localized temporal structures and traditional feature engineering to encode statistical properties. The learned shapelets serve as interpretable signatures that differentiate time series across classification labels. Additionally, we develop a visual analytics system -- SigTIme -- with coordinated views to facilitate exploration of time series signatures from multiple perspectives, aiding in useful insights generation. We quantitatively evaluate our learning framework on eight publicly available datasets and one proprietary clinical dataset. Additionally, we demonstrate the effectiveness of our system through two usage scenarios along with the domain experts: one involving public ECG data and the other focused on preterm labor analysis.

</details>


### [143] [Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Unseen Activation Monitors](https://arxiv.org/abs/2512.11949)
*Max McGuinness,Alex Serrano,Luke Bailey,Scott Emmons*

Main category: cs.LG

TL;DR: 文章聚焦激活监测在模型可能隐藏内部状态威胁下的鲁棒性，验证微调可使模型学会零样本逃避激活监测，这种逃避有选择性且对模型能力影响小，还给出应对思路和评估工具。


<details>
  <summary>Details</summary>
Motivation: 激活监测在模型可能主动隐藏内部状态的威胁下，其最坏情况的鲁棒性未被测试，需探究模型是否能学会逃避前所未见的激活监测。

Method: 微调大语言模型使其在特定触发条件下逃避一组良性概念的监测器，将触发词替换为安全相关词测试零样本泛化能力，并跨多种模型家族验证，以Gemma - 2 - 9b - it为例进行机制分析。

Result: 微调可创建能零样本逃避激活监测的神经变色龙模型，逃避现象在多种模型家族中得到验证，是通过将激活值移至低维子空间实现的，强防御手段有更强韧性但模型仍有逃避能力。

Conclusion: 为这种失效模式提供概念验证，给出评估监测技术针对对齐威胁模型最坏情况鲁棒性的工具。

Abstract: Activation monitoring, which probes a model's internal states using lightweight classifiers, is an emerging tool for AI safety. However, its worst-case robustness under a misalignment threat model--where a model might learn to actively conceal its internal states--remains untested. Focusing on this threat model, we ask: could a model learn to evade previously unseen activation monitors? Our core contribution is to stress-test the learnability of this behavior. We demonstrate that finetuning can create Neural Chameleons: models capable of zero-shot evading activation monitors. Specifically, we fine-tune an LLM to evade monitors for a set of benign concepts (e.g., languages, HTML) when conditioned on a trigger of the form: "You are being probed for {concept}". We show that this learned mechanism generalizes zero-shot: by substituting {concept} with a safety-relevant term like 'deception', the model successfully evades previously unseen safety monitors. We validate this phenomenon across diverse model families (Llama, Gemma, Qwen), showing that the evasion succeeds even against monitors trained post hoc on the model's frozen weights. This evasion is highly selective, targeting only the specific concept mentioned in the trigger, and having a modest impact on model capabilities on standard benchmarks. Using Gemma-2-9b-it as a case study, a mechanistic analysis reveals this is achieved via a targeted manipulation that moves activations into a low-dimensional subspace. While stronger defenses like monitor ensembles and non-linear classifiers show greater resilience, the model retains a non-trivial evasion capability. Our work provides a proof-of-concept for this failure mode and a tool to evaluate the worst-case robustness of monitoring techniques against misalignment threat models.

</details>


### [144] [Neural CDEs as Correctors for Learned Time Series Models](https://arxiv.org/abs/2512.12116)
*Muhammad Bilal Shahid,Prajwal Koirla,Cody Fleming*

Main category: cs.LG

TL;DR: 本文提出预测 - 校正机制提升时间序列模型多步预测性能，引入正则化策略，实验表明该机制优于单一预测器。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模型多步预测易出错，需改进预测性能。

Method: 提出预测 - 校正机制，校正器用神经受控微分方程预测误差；引入两种正则化策略。

Result: 在多种数据集上用不同预测器评估，预测 - 校正机制始终提升性能。

Conclusion: 预测 - 校正机制能有效提升时间序列模型多步预测性能。

Abstract: Learned time-series models, whether continuous- or discrete-time, are widely used to forecast the states of a dynamical system. Such models generate multi-step forecasts either directly, by predicting the full horizon at once, or iteratively, by feeding back their own predictions at each step. In both cases, the multi-step forecasts are prone to errors. To address this, we propose a Predictor-Corrector mechanism where the Predictor is any learned time-series model and the Corrector is a neural controlled differential equation. The Predictor forecasts, and the Corrector predicts the errors of the forecasts. Adding these errors to the forecasts improves forecast performance. The proposed Corrector works with irregularly sampled time series and continuous- and discrete-time Predictors. Additionally, we introduce two regularization strategies to improve the extrapolation performance of the Corrector with accelerated training. We evaluate our Corrector with diverse Predictors, e.g., neural ordinary differential equations, Contiformer, and DLinear, on synthetic, physics simulation, and real-world forecasting datasets. The experiments demonstrate that the Predictor-Corrector mechanism consistently improves the performance compared to Predictor alone.

</details>


### [145] [Learning to Extract Context for Context-Aware LLM Inference](https://arxiv.org/abs/2512.11986)
*Minseon Kim,Lucas Caccia,Zhengyan Shi,Matheus Pereira,Marc-Alexandre Côté,Xingdi Yuan,Alessandro Sordoni*

Main category: cs.LG

TL;DR: 论文质疑大语言模型不考虑上下文直接回应的传统框架，提出从用户提示中提取和利用上下文信息的框架，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 用户对大语言模型的提示常模糊或不明确，传统生成回应框架不考虑上下文，易导致不安全输出或不必要拒绝。

Method: 提出基于强化学习的上下文生成器，以自编码器方式训练，从提示中推断上下文信号并指导回应生成。

Result: 在SafetyInstruct数据集上平均减少5.6%有害回应，在XSTest和WildJailbreak上使良性提示的攻击成功率和合规性的调和均值提高6.2%。

Conclusion: 上下文提取对大语言模型更安全可靠的推理有效。

Abstract: User prompts to large language models (LLMs) are often ambiguous or under-specified, and subtle contextual cues shaped by user intentions, prior knowledge, and risk factors strongly influence what constitutes an appropriate response. Misinterpreting intent or risks may lead to unsafe outputs, while overly cautious interpretations can cause unnecessary refusal of benign requests. In this paper, we question the conventional framework in which LLMs generate immediate responses to requests without considering broader contextual factors. User requests are situated within broader contexts such as intentions, knowledge, and prior experience, which strongly influence what constitutes an appropriate answer. We propose a framework that extracts and leverages such contextual information from the user prompt itself. Specifically, a reinforcement learning based context generator, designed in an autoencoder-like fashion, is trained to infer contextual signals grounded in the prompt and use them to guide response generation. This approach is particularly important for safety tasks, where ambiguous requests may bypass safeguards while benign but confusing requests can trigger unnecessary refusals. Experiments show that our method reduces harmful responses by an average of 5.6% on the SafetyInstruct dataset across multiple foundation models and improves the harmonic mean of attack success rate and compliance on benign prompts by 6.2% on XSTest and WildJailbreak. These results demonstrate the effectiveness of context extraction for safer and more reliable LLM inferences.

</details>


### [146] [EnviroLLM: Resource Tracking and Optimization for Local AI](https://arxiv.org/abs/2512.12004)
*Troy Allen*

Main category: cs.LG

TL;DR: 提出开源工具EnviroLLM，用于跟踪、基准测试和优化个人设备运行大语言模型的性能和能耗。


<details>
  <summary>Details</summary>
Motivation: 本地部署大语言模型时，用户缺乏测量资源使用、环境影响和效率指标的工具。

Method: 设计EnviroLLM系统，提供实时进程监控、跨多平台基准测试、持久存储与可视化分析、个性化推荐等功能，还结合大语言模型评估及能耗和速度指标。

Result: 构建了EnviroLLM工具，可实现相关功能。

Conclusion: EnviroLLM能帮助用户在运行大语言模型时评估性能和能耗，权衡质量与效率。

Abstract: Large language models (LLMs) are increasingly deployed locally for privacy and accessibility, yet users lack tools to measure their resource usage, environmental impact, and efficiency metrics. This paper presents EnviroLLM, an open-source toolkit for tracking, benchmarking, and optimizing performance and energy consumption when running LLMs on personal devices. The system provides real-time process monitoring, benchmarking across multiple platforms (Ollama, LM Studio, vLLM, and OpenAI-compatible APIs), persistent storage with visualizations for longitudinal analysis, and personalized model and optimization recommendations. The system includes LLM-as-judge evaluations alongside energy and speed metrics, enabling users to assess quality-efficiency tradeoffs when testing models with custom prompts.

</details>


### [147] [Eventually LIL Regret: Almost Sure $\ln\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data](https://arxiv.org/abs/2512.12325)
*Shubhada Agrawal,Aaditya Ramdas*

Main category: cs.LG

TL;DR: 证明Robbins提出的经典次高斯混合在路径上满足确定性后悔界，解释其在连接对抗在线学习和博弈论统计方面的作用。


<details>
  <summary>Details</summary>
Motivation: 连接对抗在线学习（处理有界数据后悔界）和博弈论统计（可处理无界数据但依赖随机假设）的世界。

Method: 对经典次高斯混合进行证明，分析自然“Ville事件”$E_α$中路径的后悔情况。

Result: 在$E_α$事件中，到时间$T$的后悔有界，在$E_0$事件中，路径后悔最终有界。

Conclusion: 条件后悔界可作为随机和对抗下注之间的桥梁。

Abstract: We prove that a classic sub-Gaussian mixture proposed by Robbins in a stochastic setting actually satisfies a path-wise (deterministic) regret bound. For every path in a natural ``Ville event'' $E_α$, this regret till time $T$ is bounded by $\ln^2(1/α)/V_T + \ln (1/α) + \ln \ln V_T$ up to universal constants, where $V_T$ is a nonnegative, nondecreasing, cumulative variance process. (The bound reduces to $\ln(1/α) + \ln \ln V_T$ if $V_T \geq \ln(1/α)$.) If the data were stochastic, then one can show that $E_α$ has probability at least $1-α$ under a wide class of distributions (eg: sub-Gaussian, symmetric, variance-bounded, etc.). In fact, we show that on the Ville event $E_0$ of probability one, the regret on every path in $E_0$ is eventually bounded by $\ln \ln V_T$ (up to constants). We explain how this work helps bridge the world of adversarial online learning (which usually deals with regret bounds for bounded data), with game-theoretic statistics (which can handle unbounded data, albeit using stochastic assumptions). In short, conditional regret bounds serve as a bridge between stochastic and adversarial betting.

</details>


### [148] [DFedReweighting: A Unified Framework for Objective-Oriented Reweighting in Decentralized Federated Learning](https://arxiv.org/abs/2512.12022)
*Kaichuang Zhang,Wei Yin,Jinghao Yang,Ping Xu*

Main category: cs.LG

TL;DR: 提出DFedReweighting统一聚合框架解决DFL系统的公平性和鲁棒性等挑战，理论和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: DFL系统面临公平性、鲁棒性等挑战，需解决这些问题。

Method: 提出DFedReweighting框架，先根据辅助数据集的目标性能指标计算初步权重，再用定制重加权策略细化得到最终聚合权重。

Result: 理论分析表明目标性能指标和定制重加权策略结合可确保线性收敛，实验显示框架显著提高公平性和对拜占庭攻击的鲁棒性。

Conclusion: 选择合适的目标性能指标和定制重加权策略，框架能实现多种学习目标。

Abstract: Decentralized federated learning (DFL) has recently emerged as a promising paradigm that enables multiple clients to collaboratively train machine learning model through iterative rounds of local training, communication, and aggregation without relying on a central server which introduces potential vulnerabilities in conventional Federated Learning. Nevertheless, DFL systems continue to face a range of challenges, including fairness, robustness, etc. To address these challenges, we propose \textbf{DFedReweighting}, a unified aggregation framework designed to achieve diverse objectives in DFL systems via a objective-oriented reweighting aggregation at the final step of each learning round. Specifically, the framework first computes preliminary weights based on \textit{target performance metric} obtained from auxiliary dataset constructed using local data. These weights are then refined using \textit{customized reweighting strategy}, resulting in the final aggregation weights. Our results from the theoretical analysis demonstrate that the appropriate combination of the target performance metric and the customized reweighting strategy ensures linear convergence. Experimental results consistently show that our proposed framework significantly improves fairness and robustness against Byzantine attacks in diverse scenarios. Provided that appropriate target performance metrics and customized reweighting strategy are selected, our framework can achieve a wide range of desired learning objectives.

</details>


### [149] [Uncertainty Quantification for Machine Learning: One Size Does Not Fit All](https://arxiv.org/abs/2512.12341)
*Paul Hofman,Yusuf Sale,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 本文指出没有单一最佳的不确定性度量，应根据具体应用定制不确定性量化，用灵活的度量族并结合特定损失函数，不同任务适用不同特性的度量。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，合理量化预测不确定性至关重要，但目前各种不确定性度量都声称优于其他度量，需明确合适的度量方式。

Method: 使用能区分二阶分布的总、偶然和认知不确定性的灵活度量族，结合特定损失函数（适当评分规则）控制其特性。

Result: 选择性预测中评分规则应匹配任务损失；分布外检测中互信息表现最佳；主动学习中基于零一损失的认知不确定性始终优于其他度量。

Conclusion: 没有单一最佳的不确定性度量，不确定性量化应根据具体应用进行定制。

Abstract: Proper quantification of predictive uncertainty is essential for the use of machine learning in safety-critical applications. Various uncertainty measures have been proposed for this purpose, typically claiming superiority over other measures. In this paper, we argue that there is no single best measure. Instead, uncertainty quantification should be tailored to the specific application. To this end, we use a flexible family of uncertainty measures that distinguishes between total, aleatoric, and epistemic uncertainty of second-order distributions. These measures can be instantiated with specific loss functions, so-called proper scoring rules, to control their characteristics, and we show that different characteristics are useful for different tasks. In particular, we show that, for the task of selective prediction, the scoring rule should ideally match the task loss. On the other hand, for out-of-distribution detection, our results confirm that mutual information, a widely used measure of epistemic uncertainty, performs best. Furthermore, in an active learning setting, epistemic uncertainty based on zero-one loss is shown to consistently outperform other uncertainty measures.

</details>


### [150] [The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior](https://arxiv.org/abs/2512.12066)
*Erik Larsen*

Main category: cs.LG

TL;DR: 研究大语言模型安全拒绝决策稳定性，发现单样本安全评估不可靠，建议每个提示至少用3个样本。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型安全评估依赖单样本测试，假设模型响应确定且具代表性，本文对此进行挑战。

Method: 对三个系列的四个指令调优模型在876个有害提示上进行20种不同采样配置测试，用Claude 3.5 Haiku作为统一外部评判者。

Result: 18 - 28%的提示存在决策翻转，温度升高显著降低决策稳定性，单样本评估与多样本真实情况仅92.4%一致。

Conclusion: 单样本安全评估不足以进行可靠的安全评估，建议每个提示至少使用3个样本。

Abstract: Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.

</details>


### [151] [Optimal Mistake Bounds for Transductive Online Learning](https://arxiv.org/abs/2512.12567)
*Zachary Chase,Steve Hanneke,Shay Moran,Jonathan Shafer*

Main category: cs.LG

TL;DR: 解决30年未解问题，量化在线学习中直推式和标准在线学习差距，证明直推式错误界至少为Ω(√d)且为紧下界，建立二次差距。


<details>
  <summary>Details</summary>
Motivation: 解决30年有关在线学习中无标签数据作用的未决问题，量化直推式和标准在线学习的差距。

Method: 通过理论证明来确定直推式设置中的错误界。

Result: 证明直推式错误界至少为Ω(√d)，是对之前下界的指数级改进，且该下界为紧下界，上界也有所改进。

Conclusion: 建立了直推式和标准在线学习的二次差距，凸显了提前获取无标签实例序列的好处，与PAC设置不同。

Abstract: We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. In the standard setting, the optimal mistake bound is characterized by the Littlestone dimension $d$ of the concept class $H$ (Littlestone 1987). We prove that in the transductive setting, the mistake bound is at least $Ω(\sqrt{d})$. This constitutes an exponential improvement over previous lower bounds of $Ω(\log\log d)$, $Ω(\sqrt{\log d})$, and $Ω(\log d)$, due respectively to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that this lower bound is tight: for every $d$, there exists a class of Littlestone dimension $d$ with transductive mistake bound $O(\sqrt{d})$. Our upper bound also improves upon the best known upper bound of $(2/3)d$ from Ben-David, Kushilevitz, and Mansour (1997). These results establish a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advance access to the unlabeled instance sequence. This contrasts with the PAC setting, where transductive and standard learning exhibit similar sample complexities.

</details>


### [152] [Physics-informed neural networks to solve inverse problems in unbounded domains](https://arxiv.org/abs/2512.12074)
*Gregorio Pérez-Bernal,Oscar Rincón-Cardeño,Silvana Montoya-Noguera,Nicolás Guarín-Zapata*

Main category: cs.LG

TL;DR: 本文提出解决无限和半无限域反问题的方法，用PINNs和PIKANs实现并对比性能，结果显示PINNs更优。


<details>
  <summary>Details</summary>
Motivation: 解决无限和半无限域的反问题，比较PINNs和PIKANs在该场景下的性能。

Method: 引入新的采样策略，采用双网络架构，用相同损失函数训练学习方程解和参数，不明确施加边界条件。

Result: PINNs在准确性、计算效率上更优，解决反问题比PIKANs快1000倍，相对误差更低。

Conclusion: 在解决无限和半无限域反问题时，PINNs比PIKANs表现更好。

Abstract: Inverse problems are extensively studied in applied mathematics, with applications ranging from acoustic tomography for medical diagnosis to geophysical exploration. Physics informed neural networks (PINNs) have emerged as a powerful tool for solving such problems, while Physics informed Kolmogorov Arnold networks (PIKANs) represent a recent benchmark that, in certain problems, promises greater interpretability and accuracy compared to PINNs, due to their nature, being constructed as a composition of polynomials. In this work, we develop a methodology for addressing inverse problems in infinite and semi infinite domains. We introduce a novel sampling strategy for the network's training points, using the negative exponential and normal distributions, alongside a dual network architecture that is trained to learn the solution and parameters of an equation with the same loss function. This design enables the solution of inverse problems without explicitly imposing boundary conditions, as long as the solutions tend to stabilize when leaving the domain of interest. The proposed architecture is implemented using both PINNs and PIKANs, and their performance is compared in terms of accuracy with respect to a known solution as well as computational time and response to a noisy environment. Our results demonstrate that, in this setting, PINNs provide a more accurate and computationally efficient solution, solving the inverse problem 1,000 times faster and in the same order of magnitude, yet with a lower relative error than PIKANs.

</details>


### [153] [On the Accuracy of Newton Step and Influence Function Data Attributions](https://arxiv.org/abs/2512.12572)
*Ittai Rubinstein,Samuel B. Hopkins*

Main category: cs.LG

TL;DR: 论文针对凸学习问题对NS和IF数据归因方法进行新分析，不假定全局强凸性，证明对于行为良好的逻辑回归，边界在渐近意义下接近最优，给出误差缩放定律。


<details>
  <summary>Details</summary>
Motivation: 现有主要数据归因方法（如IF和NS）的数学分析存在局限，依赖全局强凸假设且误差边界缩放性差，无法回答一些基本问题。

Method: 针对凸学习问题对NS和IF数据归因方法开展新分析。

Result: 得到两个期望的误差缩放表达式。

Conclusion: 该分析不假定全局强凸，解释了NS数据归因常比IF更准确的现象，且边界渐近紧密。

Abstract: Data attribution aims to explain model predictions by estimating how they would change if certain training points were removed, and is used in a wide range of applications, from interpretability and credit assignment to unlearning and privacy.
  Even in the relatively simple case of linear regressions, existing mathematical analyses of leading data attribution methods such as Influence Functions (IF) and single Newton Step (NS) remain limited in two key ways. First, they rely on global strong convexity assumptions which are often not satisfied in practice. Second, the resulting bounds scale very poorly with the number of parameters ($d$) and the number of samples removed ($k$). As a result, these analyses are not tight enough to answer fundamental questions such as "what is the asymptotic scaling of the errors of each method?" or "which of these methods is more accurate for a given dataset?"
  In this paper, we introduce a new analysis of the NS and IF data attribution methods for convex learning problems. To the best of our knowledge, this is the first analysis of these questions that does not assume global strong convexity and also the first explanation of [KATL19] and [RH25a]'s observation that NS data attribution is often more accurate than IF. We prove that for sufficiently well-behaved logistic regression, our bounds are asymptotically tight up to poly-logarithmic factors, yielding scaling laws for the errors in the average-case sample removals.
  \[ \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T - \hatθ_T^{\mathrm{NS}}\|_2 \bigr] = \widetildeΘ\!\left(\frac{k d}{n^2}\right), \qquad \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T^{\mathrm{NS}} - \hatθ_T^{\mathrm{IF}}\|_2 \bigr] = \widetildeΘ\!\left( \frac{(k + d)\sqrt{k d}}{n^2} \right). \]

</details>


### [154] [CLOAK: Contrastive Guidance for Latent Diffusion-Based Data Obfuscation](https://arxiv.org/abs/2512.12086)
*Xin Yang,Omid Ardakanian*

Main category: cs.LG

TL;DR: 提出基于潜在扩散模型的数据混淆框架Cloak，在多个数据集实验中表现优于现有技术，适合资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 现有数据混淆方法存在需修改下游任务、难以平衡隐私与效用或计算密集等问题，不适合资源受限的移动物联网设备。

Method: 提出基于潜在扩散模型的Cloak框架，利用对比学习提取解纠缠表示，引导潜在扩散过程。

Result: 在四个公共时间序列数据集和一个面部图像数据集上的实验表明，Cloak始终优于现有混淆技术。

Conclusion: Cloak能让不同隐私需求的用户以最少的再训练来平衡隐私和效用，适合资源受限的场景部署。

Abstract: Data obfuscation is a promising technique for mitigating attribute inference attacks by semi-trusted parties with access to time-series data emitted by sensors. Recent advances leverage conditional generative models together with adversarial training or mutual information-based regularization to balance data privacy and utility. However, these methods often require modifying the downstream task, struggle to achieve a satisfactory privacy-utility trade-off, or are computationally intensive, making them impractical for deployment on resource-constrained mobile IoT devices. We propose Cloak, a novel data obfuscation framework based on latent diffusion models. In contrast to prior work, we employ contrastive learning to extract disentangled representations, which guide the latent diffusion process to retain useful information while concealing private information. This approach enables users with diverse privacy needs to navigate the privacy-utility trade-off with minimal retraining. Extensive experiments on four public time-series datasets, spanning multiple sensing modalities, and a dataset of facial images demonstrate that Cloak consistently outperforms state-of-the-art obfuscation techniques and is well-suited for deployment in resource-constrained settings.

</details>


### [155] [GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes](https://arxiv.org/abs/2512.12091)
*Mohammad Pivezhandi,Mahdi Banisharif,Saeed Bakhshan,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.LG

TL;DR: 提出GraphPerf - RT预测OpenMP工作负载性能，在多平台验证，与RL方法集成实验显示可有效减少完成时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 异构嵌入式SoC上OpenMP工作负载性能预测具有挑战性，现有方法存在不足。

Method: 引入GraphPerf - RT，用异构图表示统一多种信息，多任务证据头预测指标，集成到多种RL方法。

Result: 在三个嵌入式ARM平台验证R^2 > 0.95且不确定性校准良好，MAMBRL - D3QN与GraphPerf - RT结合使完成时间减少66%、能耗减少82%。

Conclusion: 准确、感知不确定性的替代模型能在热约束嵌入式系统上实现有效基于模型的规划。

Abstract: Performance prediction for OpenMP workloads on heterogeneous embedded SoCs is challenging due to complex interactions between task DAG structure, control-flow irregularity, cache
  and branch behavior, and thermal dynamics; classical heuristics struggle under workload irregularity, tabular regressors discard structural information, and model-free RL risks
  overheating resource-constrained devices. We introduce GraphPerf-RT, the first surrogate that unifies task DAG topology, CFG-derived code semantics, and runtime context (per-core
  DVFS, thermal state, utilization) in a heterogeneous graph representation with typed edges encoding precedence, placement, and contention. Multi-task evidential heads predict
  makespan, energy, cache and branch misses, and utilization with calibrated uncertainty (Normal-Inverse-Gamma), enabling risk-aware scheduling that filters low-confidence rollouts.
  We validate GraphPerf-RT on three embedded ARM platforms (Jetson TX2, Jetson Orin NX, RUBIK Pi), achieving R^2 > 0.95 with well-calibrated uncertainty (ECE < 0.05). To
  demonstrate end-to-end scheduling utility, we integrate the surrogate with four RL methods on Jetson TX2: single-agent model-free (SAMFRL), single-agent model-based (SAMBRL),
  multi-agent model-free (MAMFRL-D3QN), and multi-agent model-based (MAMBRL-D3QN). Experiments across 5 seeds (200 episodes each) show that MAMBRL-D3QN with GraphPerf-RT as the
  world model achieves 66% makespan reduction (0.97 +/- 0.35s) and 82% energy reduction (0.006 +/- 0.005J) compared to model-free baselines, demonstrating that accurate,
  uncertainty-aware surrogates enable effective model-based planning on thermally constrained embedded systems.

</details>


### [156] [MixtureKit: A General Framework for Composing, Training, and Visualizing Mixture-of-Experts Models](https://arxiv.org/abs/2512.12121)
*Ahmad Chamma,Omar El Herraoui,Guokan Shang*

Main category: cs.LG

TL;DR: 介绍MixtureKit框架用于构建、训练和分析MoE模型，支持三种方法，有自动配置和可视化界面，实验显示其优势并开源。


<details>
  <summary>Details</summary>
Motivation: 构建一个模块化开源框架，用于从任意预训练或微调模型构建、训练和分析MoE模型。

Method: 支持Traditional MoE、BTX、BTS三种方法，自动修改模型配置、修补类和保存统一检查点，提供可视化界面。

Result: 使用多语言代码切换数据实验表明，基于BTX的模型在多个基准测试中优于基线密集模型。

Conclusion: 发布MixtureKit作为跨领域基于MoE系统研发的实用基础。

Abstract: We introduce MixtureKit, a modular open-source framework for constructing, training, and analyzing Mixture-of-Experts (MoE) models from arbitrary pre-trained or fine-tuned models. MixtureKit currently supports three complementary methods: (i) \emph{Traditional MoE}, which uses a single router per transformer block to select experts, (ii) \emph{BTX} (Branch-Train-Mix), which introduces separate routers for each specified sub-layer enabling fine-grained token routing, and (iii) \emph{BTS} (Branch-Train-Stitch), which keeps experts fully intact and introduces trainable stitch layers for controlled information exchange between hub and experts. MixtureKit automatically modifies the model configuration, patches decoder and causal LM classes, and saves a unified checkpoint ready for inference or fine-tuning. We further provide a visualization interface to inspect per-token routing decisions, expert weight distributions, and layer-wise contributions. Experiments with multilingual code-switched data (e.g. Arabic-Latin) show that a BTX-based model trained using MixtureKit can outperform baseline dense models on multiple benchmarks. We release MixtureKit as a practical foundation for research and development of MoE-based systems across diverse domains.

</details>


### [157] [Unsupervised learning of multiscale switching dynamical system models from multimodal neural data](https://arxiv.org/abs/2512.12881)
*DongKyu Kim,Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 提出无监督学习算法学习多尺度切换动力系统模型，在实验中展示多尺度神经融合优势和捕捉非平稳性的重要性，对脑机接口和神经行为研究有意义。


<details>
  <summary>Details</summary>
Motivation: 现有切换方法主要关注单一神经模态，且训练数据中状态标签通常缺失，难以学习依赖状态切换的动力系统模型。而多模态神经记录可编码行为，需新模型。

Method: 开发一种 novel 无监督学习算法，仅利用多尺度神经观测学习切换多尺度动力系统模型的参数。

Result: 切换多尺度动力系统模型比切换单尺度模型更准确地解码行为，且优于平稳多尺度模型。

Conclusion: 开发的无监督学习框架可更准确地建模复杂多尺度神经动力学，有望提升脑机接口性能和增进对神经行为的理解。

Abstract: Neural population activity often exhibits regime-dependent non-stationarity in the form of switching dynamics. Learning accurate switching dynamical system models can reveal how behavior is encoded in neural activity. Existing switching approaches have primarily focused on learning models from a single neural modality, either continuous Gaussian signals or discrete Poisson signals. However, multiple neural modalities are often recorded simultaneously to measure different spatiotemporal scales of brain activity, and all these modalities can encode behavior. Moreover, regime labels are typically unavailable in training data, posing a significant challenge for learning models of regime-dependent switching dynamics. To address these challenges, we develop a novel unsupervised learning algorithm that learns the parameters of switching multiscale dynamical system models using only multiscale neural observations. We demonstrate our method using both simulations and two distinct experimental datasets with multimodal spike-LFP observations during different motor tasks. We find that our switching multiscale dynamical system models more accurately decode behavior than switching single-scale dynamical models, showing the success of multiscale neural fusion. Further, our models outperform stationary multiscale models, illustrating the importance of tracking regime-dependent non-stationarity in multimodal neural data. The developed unsupervised learning framework enables more accurate modeling of complex multiscale neural dynamics by leveraging information in multimodal recordings while incorporating regime switches. This approach holds promise for improving the performance and robustness of brain-computer interfaces over time and for advancing our understanding of the neural basis of behavior.

</details>


### [158] [High-Dimensional Tensor Discriminant Analysis: Low-Rank Discriminant Structure, Representation Synergy, and Theoretical Guarantees](https://arxiv.org/abs/2512.12122)
*Elynn Chen,Yuefeng Han,Jiayu Li*

Main category: cs.LG

TL;DR: 本文引入CP低秩结构用于判别张量，提出CP - TDA方法，建立全局收敛和最优误分类率，还开发半参数张量判别模型，在数值研究和实际数据分析中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有张量分类方法依赖稀疏性或Tucker结构且缺乏理论保证，有经验证据表明判别信号集中在一些多线性分量上。

Method: 在张量高斯混合模型下提出CP - TDA方法，用Randomized Composite PCA初始化并迭代优化；开发半参数张量判别模型将张量映射到潜在空间。

Result: 建立了全局收敛和最小最大最优误分类率；数值研究和实际数据分析表明在高维小样本下优于现有张量分类器和图神经网络。

Conclusion: 本文提出的方法在张量分类中有显著优势，尤其是在高维小样本场景。

Abstract: High-dimensional tensor-valued predictors arise in modern applications, increasingly as learned representations from neural networks. Existing tensor classification methods rely on sparsity or Tucker structures and often lack theoretical guarantees. Motivated by empirical evidence that discriminative signals concentrate along a few multilinear components, we introduce CP low-rank structure for the discriminant tensor, a modeling perspective not previously explored. Under a Tensor Gaussian Mixture Model, we propose high-dimensional CP low-rank Tensor Discriminant Analysis (CP-TDA) with Randomized Composite PCA (\textsc{rc-PCA}) initialization, that is essential for handling dependent and anisotropic noise under weaker signal strength and incoherence conditions, followed by iterative refinement algorithm. We establish global convergence and minimax-optimal misclassification rates.
  To handle tensor data deviating from tensor normality, we develop the first semiparametric tensor discriminant model, in which learned tensor representations are mapped via deep generative models into a latent space tailored for CP-TDA. Misclassification risk decomposes into representation, approximation, and estimation errors. Numerical studies and real data analysis on graph classification demonstrate substantial gains over existing tensor classifiers and state-of-the-art graph neural networks, particularly in high-dimensional, small-sample regimes.

</details>


### [159] [Understanding When Graph Convolutional Networks Help: A Diagnostic Study on Label Scarcity and Structural Properties](https://arxiv.org/abs/2512.12947)
*Nischal Subedi,Ember Kerstetter,Winnie Li,Silo Murphy*

Main category: cs.LG

TL;DR: 本文通过对亚马逊计算机共购数据的诊断研究，分析GCN在半监督节点分类中何时及为何有效，发现其性能取决于图同质性和特征质量的相互作用，为从业者提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 从业者缺乏关于GCN何时比简单基线方法有显著改进的明确指导，因此开展研究以了解GCN何时及为何有效。

Method: 使用亚马逊计算机共购数据，通过模拟标签稀缺、特征消融和按类分析等系统实验进行研究。

Result: GCN性能关键取决于图同质性和特征质量的相互作用；在极端标签稀缺时增益最大；在高度同质性图上仅靠结构就可携带足够信息；在同质性低且特征强时会降低性能。

Conclusion: 象限分析表明GCN在四种情况中的三种有帮助，仅在低同质性和强特征时有害，为从业者决定是否采用基于图的方法提供实用指导。

Abstract: Graph Convolutional Networks (GCNs) have become a standard approach for semi-supervised node classification, yet practitioners lack clear guidance on when GCNs provide meaningful improvements over simpler baselines. We present a diagnostic study using the Amazon Computers co-purchase data to understand when and why GCNs help. Through systematic experiments with simulated label scarcity, feature ablation, and per-class analysis, we find that GCN performance depends critically on the interaction between graph homophily and feature quality. GCNs provide the largest gains under extreme label scarcity, where they leverage neighborhood structure to compensate for limited supervision. Surprisingly, GCNs can match their original performance even when node features are replaced with random noise, suggesting that structure alone carries sufficient signal on highly homophilous graphs. However, GCNs hurt performance when homophily is low and features are already strong, as noisy neighbors corrupt good predictions. Our quadrant analysis reveals that GCNs help in three of four conditions and only hurt when low homophily meets strong features. These findings offer practical guidance for practitioners deciding whether to adopt graph-based methods.

</details>


### [160] [On the Approximation Power of SiLU Networks: Exponential Rates and Depth Efficiency](https://arxiv.org/abs/2512.12132)
*Koffi O. Ayena*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This article establishes a comprehensive theoretical framework demonstrating that SiLU (Sigmoid Linear Unit) activation networks achieve exponential approximation rates for smooth functions with explicit and improved complexity control compared to classical ReLU-based constructions. We develop a novel hierarchical construction beginning with an efficient approximation of the square function $x^2$ more compact in depth and size than comparable ReLU realizations, such as those given by Yarotsky. This construction yields an approximation error decaying as $\mathcal{O}(ω^{-2k})$ using networks of depth $\mathcal{O}(1)$. We then extend this approach through functional composition to establish sharp approximation bounds for deep SiLU networks in approximating Sobolev-class functions, with total depth $\mathcal{O}(1)$ and size $\mathcal{O}(\varepsilon^{-d/n})$.

</details>


### [161] [BaRISTA: Brain Scale Informed Spatiotemporal Representation of Human Intracranial Neural Activity](https://arxiv.org/abs/2512.12135)
*Lucine L. Oganesian,Saba Hashemi,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 提出时空变压器模型和自监督任务，研究多区域神经活动编码空间尺度，发现调整尺度影响下游解码，大尺度编码可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于变压器的神经基础模型在处理多区域颅内记录时，对于如何编码空间信息和设计自监督任务以学习脑网络模式、提升下游解码性能存在关键问题待解决。

Method: 提出新的时空变压器模型和自监督掩码潜在重建任务，可灵活调整空间尺度进行令牌编码和掩码。

Result: 调整令牌编码和掩码的空间尺度显著影响下游解码；大尺度空间编码比现有模型常用的通道级编码能提高下游解码性能；方法可实现区域级令牌编码并保持准确的通道级神经重建。

Conclusion: 建模框架能探索令牌编码和掩码的空间尺度，揭示其对多区域人脑活动神经基础模型自监督预训练的重要性，提升下游解码性能。

Abstract: Intracranial recordings have opened a unique opportunity to simultaneously measure activity across multiregional networks in the human brain. Recent works have focused on developing transformer-based neurofoundation models of such recordings that can generalize across subjects and datasets. However, these recordings exhibit highly complex spatiotemporal interactions across diverse spatial scales, from the single-channel scale to the scale of brain regions. As such, there remain critical open questions regarding how best to encode spatial information and how to design self-supervision tasks that enable the learning of brain network patterns and enhance downstream decoding performance using such high-dimensional, multiregional recordings. To allow for exploring these questions, we propose a new spatiotemporal transformer model of multiregional neural activity and a corresponding self-supervised masked latent reconstruction task, designed to enable flexibility in the spatial scale used for token encoding and masking. Applying this model on publicly available multiregional intracranial electrophysiology (iEEG) data, we demonstrate that adjusting the spatial scale for both token encoding and masked reconstruction significantly impacts downstream decoding. Further, we find that spatial encoding at larger scales than channel-level encoding, which is commonly used in existing iEEG transformer models, improves downstream decoding performance. Finally, we demonstrate that our method allows for region-level token encoding while also maintaining accurate channel-level neural reconstruction. Taken together, our modeling framework enables exploration of the spatial scales used for token encoding and masking, reveals their importance towards self-supervised pretraining of neurofoundation models of multiregional human brain activity, and enhances downstream decoding performance.

</details>


### [162] [CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity and Low-Precision Networks](https://arxiv.org/abs/2512.12981)
*Jonathan Wenshøj,Tong Chen,Bob Pepin,Raghavendra Selvan*

Main category: cs.LG

TL;DR: 本文提出CoDeQ方法用于联合剪枝和量化，无需辅助程序，可在ImageNet的ResNet - 18上有效压缩并保持精度。


<details>
  <summary>Details</summary>
Motivation: 当前联合剪枝 - 量化方法依赖训练循环外的辅助程序，增加工程复杂性和超参数调优，且缺乏数据驱动的梯度信号，可能导致次优压缩。

Method: 基于标量量化器的死区等效于幅度剪枝的观察，参数化死区宽度并通过反向传播学习，同时学习量化参数，可控制稀疏性和比特宽度。

Result: 提出CoDeQ方法，支持固定和混合精度量化，在单端到端优化中确定稀疏模式和量化参数，无需辅助程序。在ImageNet的ResNet - 18上，将位运算减少到约5%，同时保持接近全精度的准确率。

Conclusion: CoDeQ方法简单、全可微、架构无关且易于实现，能有效进行联合剪枝 - 量化。

Abstract: While joint pruning--quantization is theoretically superior to sequential application, current joint methods rely on auxiliary procedures outside the training loop for finding compression parameters. This reliance adds engineering complexity and hyperparameter tuning, while also lacking a direct data-driven gradient signal, which might result in sub-optimal compression. In this paper, we introduce CoDeQ, a simple, fully differentiable method for joint pruning--quantization. Our approach builds on a key observation: the dead-zone of a scalar quantizer is equivalent to magnitude pruning, and can be used to induce sparsity directly within the quantization operator. Concretely, we parameterize the dead-zone width and learn it via backpropagation, alongside the quantization parameters. This design provides explicit control of sparsity, regularized by a single global hyperparameter, while decoupling sparsity selection from bit-width selection. The result is a method for Compression with Dead-zone Quantizer (CoDeQ) that supports both fixed-precision and mixed-precision quantization (controlled by an optional second hyperparameter). It simultaneously determines the sparsity pattern and quantization parameters in a single end-to-end optimization. Consequently, CoDeQ does not require any auxiliary procedures, making the method architecture-agnostic and straightforward to implement. On ImageNet with ResNet-18, CoDeQ reduces bit operations to ~5% while maintaining close to full precision accuracy in both fixed and mixed-precision regimes.

</details>


### [163] [HydroDiffusion: Diffusion-Based Probabilistic Streamflow Forecasting with a State Space Backbone](https://arxiv.org/abs/2512.12183)
*Yihan Wang,Annan Yu,Lujun Zhang,Charuleka Varadharajan,N. Benjamin Erichson*

Main category: cs.LG

TL;DR: 本文提出HydroDiffusion框架用于概率性径流预测，在多流域评估中表现良好，是中程径流预测的可靠框架。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的径流预测方法依赖LSTM骨干和单步训练目标，难以捕捉长距离依赖和生成连贯预测轨迹，需改进。

Method: 开发HydroDiffusion框架，采用仅解码器的状态空间模型骨干，单次处理多日轨迹去噪。

Result: 在531个流域评估中，HydroDiffusion在实况驱动下临近预报精度高，全模拟期性能稳定，在业务预报中比DRUM有更强的确定性和概率性预报能力。

Conclusion: HydroDiffusion是中程径流预测的可靠生成建模框架，为大陆尺度概率水文预测研究提供新基准和基础。

Abstract: Recent advances have introduced diffusion models for probabilistic streamflow forecasting, demonstrating strong early flood-warning skill. However, current implementations rely on recurrent Long Short-Term Memory (LSTM) backbones and single-step training objectives, which limit their ability to capture long-range dependencies and produce coherent forecast trajectories across lead times. To address these limitations, we developed HydroDiffusion, a diffusion-based probabilistic forecasting framework with a decoder-only state space model backbone. The proposed framework jointly denoises full multi-day trajectories in a single pass, ensuring temporal coherence and mitigating error accumulation common in autoregressive prediction. HydroDiffusion is evaluated across 531 watersheds in the contiguous United States (CONUS) in the CAMELS dataset. We benchmark HydroDiffusion against two diffusion baselines with LSTM backbones, as well as the recently proposed Diffusion-based Runoff Model (DRUM). Results show that HydroDiffusion achieves strong nowcast accuracy when driven by observed meteorological forcings, and maintains consistent performance across the full simulation horizon. Moreover, HydroDiffusion delivers stronger deterministic and probabilistic forecast skill than DRUM in operational forecasting. These results establish HydroDiffusion as a robust generative modeling framework for medium-range streamflow forecasting, providing both a new modeling benchmark and a foundation for future research on probabilistic hydrologic prediction at continental scales.

</details>


### [164] [MolGuidance: Advanced Guidance Strategies for Conditional Molecular Generation with Flow Matching](https://arxiv.org/abs/2512.12198)
*Jirui Jin,Cheng Zeng,Pawan Prakash,Ellad B. Tadmor,Adrian Roitberg,Richard G. Hennig,Stefano Martiniani,Mingjie Liu*

Main category: cs.LG

TL;DR: 本文将先进引导方法集成到分子生成框架，提出混合引导策略，在数据集上实现新的性能，还比较了不同引导方法。


<details>
  <summary>Details</summary>
Motivation: 解决条件分子生成中确保化学有效性、匹配目标属性、促进结构多样性和高效采样等问题，借鉴计算机视觉新引导策略。

Method: 将分类器自由引导、自动引导和模型引导等方法集成到基于SE(3)等变流匹配过程的分子生成框架，提出混合引导策略，通过贝叶斯优化联合优化引导尺度。

Result: 在QM9和QMe14S数据集上实现从头分子生成属性对齐的新的最优性能，生成分子具有高结构有效性。

Conclusion: 提出的方法有效，还系统比较了不同引导方法优缺点，为其更广泛应用提供见解。

Abstract: Key objectives in conditional molecular generation include ensuring chemical validity, aligning generated molecules with target properties, promoting structural diversity, and enabling efficient sampling for discovery. Recent advances in computer vision introduced a range of new guidance strategies for generative models, many of which can be adapted to support these goals. In this work, we integrate state-of-the-art guidance methods -- including classifier-free guidance, autoguidance, and model guidance -- in a leading molecule generation framework built on an SE(3)-equivariant flow matching process. We propose a hybrid guidance strategy that separately guides continuous and discrete molecular modalities -- operating on velocity fields and predicted logits, respectively -- while jointly optimizing their guidance scales via Bayesian optimization. Our implementation, benchmarked on the QM9 and QMe14S datasets, achieves new state-of-the-art performance in property alignment for de novo molecular generation. The generated molecules also exhibit high structural validity. Furthermore, we systematically compare the strengths and limitations of various guidance methods, offering insights into their broader applicability.

</details>


### [165] [Multi-fidelity aerodynamic data fusion by autoencoder transfer learning](https://arxiv.org/abs/2512.13069)
*Javier Nieto-Centenero,Esther Andrés,Rodrigo Castellanos*

Main category: cs.LG

TL;DR: 本文提出多保真深度学习框架，结合自编码器迁移学习与MSCP策略，在数据稀缺下实现空气动力学数据融合，测试效果好。


<details>
  <summary>Details</summary>
Motivation: 高保真模拟计算成本高限制其在数据驱动建模中的应用，需开发不降低精度的多保真策略。

Method: 提出结合自编码器迁移学习与新开发的MSCP策略的多保真深度学习框架，利用LF数据学习潜在物理表示，用HF样本微调解码器。

Result: 模型成功校正LF偏差，用最少HF训练数据实现高精度压力预测，MSCP框架产生可靠的不确定性区间，点覆盖率超95%。

Conclusion: 该工作为数据稀缺环境下的空气动力学回归提供了可扩展且可靠的解决方案。

Abstract: Accurate aerodynamic prediction often relies on high-fidelity simulations; however, their prohibitive computational costs severely limit their applicability in data-driven modeling. This limitation motivates the development of multi-fidelity strategies that leverage inexpensive low-fidelity information without compromising accuracy. Addressing this challenge, this work presents a multi-fidelity deep learning framework that combines autoencoder-based transfer learning with a newly developed Multi-Split Conformal Prediction (MSCP) strategy to achieve uncertainty-aware aerodynamic data fusion under extreme data scarcity. The methodology leverages abundant Low-Fidelity (LF) data to learn a compact latent physics representation, which acts as a frozen knowledge base for a decoder that is subsequently fine-tuned using scarce HF samples. Tested on surface-pressure distributions for NACA airfoils (2D) and a transonic wing (3D) databases, the model successfully corrects LF deviations and achieves high-accuracy pressure predictions using minimal HF training data. Furthermore, the MSCP framework produces robust, actionable uncertainty bands with pointwise coverage exceeding 95%. By combining extreme data efficiency with uncertainty quantification, this work offers a scalable and reliable solution for aerodynamic regression in data-scarce environments.

</details>


### [166] [EEG-DLite: Dataset Distillation for Efficient Large EEG Model Training](https://arxiv.org/abs/2512.12210)
*Yuting Tang,Weibang Jiang,Shanglin Li,Yong Li,Chenyu Liu,Xinliang Zhou,Yi Ding,Cuntai Guan*

Main category: cs.LG

TL;DR: 提出EEG - DLite数据蒸馏框架，从大的EEG数据集中选择性去除噪声和冗余样本，用仅5%的数据集训练效果与全量数据相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 大规模EEG基础模型训练因数据量和质量问题资源消耗大，需更高效的预训练方法。

Method: 使用自监督自动编码器将EEG片段编码为紧凑的潜在表示，基于此过滤异常值并减少冗余，得到更小但信息丰富的子集。

Result: 用EEG - DLite处理后仅5%的2500小时数据集训练，在多个下游任务上效果与全量数据集相当，部分情况更好。

Conclusion: EEG - DLite为更有效和高效的生理基础建模提供了可扩展且实用的途径。

Abstract: Large-scale EEG foundation models have shown strong generalization across a range of downstream tasks, but their training remains resource-intensive due to the volume and variable quality of EEG data. In this work, we introduce EEG-DLite, a data distillation framework that enables more efficient pre-training by selectively removing noisy and redundant samples from large EEG datasets. EEG-DLite begins by encoding EEG segments into compact latent representations using a self-supervised autoencoder, allowing sample selection to be performed efficiently and with reduced sensitivity to noise. Based on these representations, EEG-DLite filters out outliers and minimizes redundancy, resulting in a smaller yet informative subset that retains the diversity essential for effective foundation model training. Through extensive experiments, we demonstrate that training on only 5 percent of a 2,500-hour dataset curated with EEG-DLite yields performance comparable to, and in some cases better than, training on the full dataset across multiple downstream tasks. To our knowledge, this is the first systematic study of pre-training data distillation in the context of EEG foundation models. EEG-DLite provides a scalable and practical path toward more effective and efficient physiological foundation modeling. The code is available at https://github.com/t170815518/EEG-DLite.

</details>


### [167] [Optimized Learned Count-Min Sketch](https://arxiv.org/abs/2512.12252)
*Kyosuke Nishishita,Atsuki Sato,Yusuke Matsui*

Main category: cs.LG

TL;DR: 本文提出优化的学习型计数最小草图（OptLCMS），提升构建速度并降低不可容忍错误概率，实验验证其效果优于LCMS。


<details>
  <summary>Details</summary>
Motivation: Learned Count - Min Sketch（LCMS）构建慢且缺乏不可容忍错误概率的理论保证。

Method: 将输入域分区，为每个分区分配CMS实例，解析推导CMS参数，通过动态规划和近似可行性检查优化阈值。

Result: OptLCMS构建更快，不可容忍错误概率更低，估计精度与LCMS相当。

Conclusion: OptLCMS能减少经验验证需求，实现快速构建并提供理论保证，还可显式控制误差阈值，提升了实用性和灵活性。

Abstract: Count-Min Sketch (CMS) is a memory-efficient data structure for estimating the frequency of elements in a multiset. Learned Count-Min Sketch (LCMS) enhances CMS with a machine learning model to reduce estimation error under the same memory usage, but suffers from slow construction due to empirical parameter tuning and lacks theoretical guarantees on intolerable error probability. We propose Optimized Learned Count-Min Sketch (OptLCMS), which partitions the input domain and assigns each partition to its own CMS instance, with CMS parameters analytically derived for fixed thresholds, and thresholds optimized via dynamic programming with approximate feasibility checks. This reduces the need for empirical validation, enabling faster construction while providing theoretical guarantees under these assumptions. OptLCMS also allows explicit control of the allowable error threshold, improving flexibility in practice. Experiments show that OptLCMS builds faster, achieves lower intolerable error probability, and matches the estimation accuracy of LCMS.

</details>


### [168] [Enhancing Node-Level Graph Domain Adaptation by Alleviating Local Dependency](https://arxiv.org/abs/2512.13149)
*Xinwei Tai,Dongmian Zou,Hongfei Wang*

Main category: cs.LG

TL;DR: 本文聚焦无监督图域适应（GDA），分析条件偏移问题，提出去相关节点特征的方法提升GDA效果，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在图上虽有进展，但图间知识有效转移仍是挑战，无监督GDA存在条件偏移影响可迁移性。

Method: 先严谨分析得出存在节点特征局部依赖时会观察到条件偏移，用马尔可夫链建模依赖特征给出GDA泛化边界，再通过去相关GCN层和图Transformer层去相关节点特征。

Result: 相比基线GDA方法有显著性能提升，学习表示中类内距离小的可视化效果明显。

Conclusion: 去相关节点特征的方法能有效改善无监督GDA。

Abstract: Recent years have witnessed significant advancements in machine learning methods on graphs. However, transferring knowledge effectively from one graph to another remains a critical challenge. This highlights the need for algorithms capable of applying information extracted from a source graph to an unlabeled target graph, a task known as unsupervised graph domain adaptation (GDA). One key difficulty in unsupervised GDA is conditional shift, which hinders transferability. In this paper, we show that conditional shift can be observed only if there exists local dependencies among node features. To support this claim, we perform a rigorous analysis and also further provide generalization bounds of GDA when dependent node features are modeled using markov chains. Guided by the theoretical findings, we propose to improve GDA by decorrelating node features, which can be specifically implemented through decorrelated GCN layers and graph transformer layers. Our experimental results demonstrate the effectiveness of this approach, showing not only substantial performance enhancements over baseline GDA methods but also clear visualizations of small intra-class distances in the learned representations. Our code is available at https://github.com/TechnologyAiGroup/DFT

</details>


### [169] [GRC-Net: Gram Residual Co-attention Net for epilepsy prediction](https://arxiv.org/abs/2512.12273)
*Bihao You,Jiping Cui*

Main category: cs.LG

TL;DR: 采用Gram矩阵转换EEG信号到3D表示，引入多级特征提取，实验中GRC - Net在五分类任务表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的基于脑电图（EEG）信号预测癫痫研究采用1D处理整个EEG信号，且EEG数据存在局部和全局信号不平衡问题。

Method: 用Gram矩阵方法将信号转换为3D表示，引入多级特征提取，利用协同注意力捕捉全局信号特征，用Inception结构处理局部信号。

Result: 在BONN数据集的五分类任务中，GRC - Net取得了93.66%的准确率。

Conclusion: 所提方法在癫痫预测中优于现有方法。

Abstract: Prediction of epilepsy based on electroencephalogram (EEG) signals is a rapidly evolving field. Previous studies have traditionally applied 1D processing to the entire EEG signal. However, we have adopted the Gram Matrix method to transform the signals into a 3D representation, enabling modeling of signal relationships across dimensions while preserving the temporal dependencies of the one-dimensional signals. Additionally, we observed an imbalance between local and global signals within the EEG data. Therefore, we introduced multi-level feature extraction, utilizing coattention for capturing global signal characteristics and an inception structure for processing local signals, achieving multi-granular feature extraction. Our experiments on the BONN dataset demonstrate that for the most challenging five-class classification task, GRC-Net achieved an accuracy of 93.66%, outperforming existing methods.

</details>


### [170] [Balancing Accuracy and Speed: A Multi-Fidelity Ensemble Kalman Filter with a Machine Learning Surrogate Model](https://arxiv.org/abs/2512.12276)
*Jeffrey van der Voort,Martin Verlaan,Hanne Kekkonen*

Main category: cs.LG

TL;DR: 本文研究使用多保真度集成卡尔曼滤波器（MF - EnKF），其中低保真模型为机器学习替代模型，通过测试两种模型验证方法表现，实现精度提升和计算速度加快。


<details>
  <summary>Details</summary>
Motivation: 针对计算成本高的物理模型，在相同计算预算下提升精度，使用包含机器学习替代模型的MF - EnKF。

Method: 在Lorenz - 2005模型和Quasi - Geostrophic模型上测试基于机器学习替代模型的MF - EnKF方法。

Result: 相比完全用ML模型替换物理模型，该方法精度更高；在相同计算预算下MF - EnKF精度提升；ML替代模型精度相当或更好，且速度更快。

Conclusion: 该方法增加了EnKF中有效集合大小，提高了初始条件估计和气象、海洋学等领域预测的精度。

Abstract: Currently, more and more machine learning (ML) surrogates are being developed for computationally expensive physical models. In this work we investigate the use of a Multi-Fidelity Ensemble Kalman Filter (MF-EnKF) in which the low-fidelity model is such a machine learning surrogate model, instead of a traditional low-resolution or reduced-order model. The idea behind this is to use an ensemble of a few expensive full model runs, together with an ensemble of many cheap but less accurate ML model runs. In this way we hope to reach increased accuracy within the same computational budget. We investigate the performance by testing the approach on two common test problems, namely the Lorenz-2005 model and the Quasi-Geostrophic model. By keeping the original physical model in place, we obtain a higher accuracy than when we completely replace it by the ML model. Furthermore, the MF-EnKF reaches improved accuracy within the same computational budget. The ML surrogate has similar or improved accuracy compared to the low-resolution one, but it can provide a larger speed-up. Our method contributes to increasing the effective ensemble size in the EnKF, which improves the estimation of the initial condition and hence accuracy of the predictions in fields such as meteorology and oceanography.

</details>


### [171] [Multiclass Graph-Based Large Margin Classifiers: Unified Approach for Support Vectors and Neural Networks](https://arxiv.org/abs/2512.13410)
*Vítor M. Hanriot,Luiz C. B. Torres,Antônio P. Braga*

Main category: cs.LG

TL;DR: 文章介绍Gabriel图（GGs）在二分类和多分类问题中的应用进展，提出改进方法并进行实验，结果优于以往GG分类器。


<details>
  <summary>Details</summary>
Motivation: 探索GGs在分类问题中的应用并改进相关方法。

Method: 为Chipclass提出平滑函数和结构支持向量（SSV）中心神经元；扩展神经网络架构；提出新的基于子图/距离的隶属函数和低计算成本的GG重新计算算法。

Result: Friedman测试实验结果表明该方法优于以往基于GG的分类器，与基于树的模型统计等效。

Conclusion: 所提出的基于GGs的分类方法在分类问题上有较好表现。

Abstract: While large margin classifiers are originally an outcome of an optimization framework, support vectors (SVs) can be obtained from geometric approaches. This article presents advances in the use of Gabriel graphs (GGs) in binary and multiclass classification problems. For Chipclass, a hyperparameter-less and optimization-less GG-based binary classifier, we discuss how activation functions and support edge (SE)-centered neurons affect the classification, proposing smoother functions and structural SV (SSV)-centered neurons to achieve margins with low probabilities and smoother classification contours. We extend the neural network architecture, which can be trained with backpropagation with a softmax function and a cross-entropy loss, or by solving a system of linear equations. A new subgraph-/distance-based membership function for graph regularization is also proposed, along with a new GG recomputation algorithm that is less computationally expensive than the standard approach. Experimental results with the Friedman test show that our method was better than previous GG-based classifiers and statistically equivalent to tree-based models.

</details>


### [172] [Fractional Differential Equation Physics-Informed Neural Network and Its Application in Battery State Estimation](https://arxiv.org/abs/2512.12285)
*Lujuan Dang,Zilai Wang*

Main category: cs.LG

TL;DR: 传统数据驱动神经网络难以准确估计锂电池SOC，本文提出FDIFF - PINN模型并进行实验。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动神经网络模型难以刻画电化学过程的复杂非线性和记忆依赖动态，限制了锂电池SOC估计的准确性和物理可解释性。

Method: 提出Fractional Differential Equation Physics - Informed Neural Network (FDIFF - PINN)，基于分数阶等效电路模型构建离散分数阶偏微分方程，并使用松下18650PF电池多温度动态充放电数据集进行对比实验。

Result: 摘要未提及。

Conclusion: 摘要未提及。

Abstract: Accurate estimation of the State of Charge (SOC) is critical for ensuring the safety, reliability, and performance optimization of lithium-ion battery systems. Conventional data-driven neural network models often struggle to fully characterize the inherent complex nonlinearities and memory-dependent dynamics of electrochemical processes, significantly limiting their predictive accuracy and physical interpretability under dynamic operating conditions. To address this challenge, this study proposes a novel neural architecture termed the Fractional Differential Equation Physics-Informed Neural Network (FDIFF-PINN), which integrates fractional calculus with deep learning. The main contributions of this paper include: (1) Based on a fractional-order equivalent circuit model, a discretized fractional-order partial differential equation is constructed. (2) Comparative experiments were conducted using a dynamic charge/discharge dataset of Panasonic 18650PF batteries under multi-temperature conditions (from -10$^{\circ}$C to 20$^{\circ}$C).

</details>


### [173] [Learning under Distributional Drift: Reproducibility as an Intrinsic Statistical Resource](https://arxiv.org/abs/2512.13506)
*Sofiya Zaichyk*

Main category: cs.LG

TL;DR: 提出可重复性预算$C_T$，推导漂移反馈泛化界，证明其最优性并建立可重复性速度限制，将不同问题置于统一框架。


<details>
  <summary>Details</summary>
Motivation: 当前分布漂移下的统计学习特征刻画不足，经典泛化界可能失效。

Method: 引入可重复性预算$C_T$，定义为耦合学习者 - 环境演化的累积Fisher - Rao路径长度，基于此推导泛化界并证明其最优性。

Result: 得到漂移反馈泛化界$O(T^{-1/2} + C_T/T)$，证明该速率是极小极大最优的。

Conclusion: 建立可重复性速度限制，$C_T$可作为不同场景下衡量分布变化的内在量。

Abstract: Statistical learning under distributional drift remains insufficiently characterized: when each observation alters the data-generating law, classical generalization bounds can collapse. We introduce a new statistical primitive, the reproducibility budget $C_T$, which quantifies a system's finite capacity for statistical reproducibility - the extent to which its sampling process can remain governed by a consistent underlying distribution in the presence of both exogenous change and endogenous feedback. Formally, $C_T$ is defined as the cumulative Fisher-Rao path length of the coupled learner-environment evolution, measuring the total distributional motion accumulated during learning. From this construct we derive a drift-feedback generalization bound of order $O(T^{-1/2} + C_T/T)$, and we prove a matching minimax lower bound showing that this rate is minimax-optimal. Consequently, the results establish a reproducibility speed limit: no algorithm can achieve smaller worst-case generalization error than that imposed by the average Fisher-Rao drift rate $C_T/T$ of the data-generating process. The framework situates exogenous drift, adaptive data analysis, and performative prediction within a common geometric structure, with $C_T$ emerging as the intrinsic quantity measuring distributional motion across these settings.

</details>


### [174] [TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting](https://arxiv.org/abs/2512.12301)
*Mahima Kumavat,Aditya Maheshwari*

Main category: cs.LG

TL;DR: TwinFormer是一种用于长序列时间序列预测的分层Transformer，在多个真实数据集上表现出色并优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 解决长序列时间序列预测问题，寻求更高效准确的预测方法。

Method: 将输入分为非重叠时间块，分两阶段处理：先使用Local Informer建模块内动态，再使用Global Informer捕获块间依赖，最后用GRU聚合进行多步预测。

Result: 在八个真实数据集和不同预测步长下，TwinFormer在34个排名中27次位居前二，多次取得最佳和次佳表现，且优于多个对比模型。

Conclusion: TwinFormer架构有效，top-k Sparse Attention优于ProbSparse，GRU聚合有效。

Abstract: TwinFormer is a hierarchical Transformer for long-sequence time-series forecasting. It divides the input into non-overlapping temporal patches and processes them in two stages: (1) a Local Informer with top-$k$ Sparse Attention models intra-patch dynamics, followed by mean pooling; (2) a Global Informer captures long-range inter-patch dependencies using the same top-$k$ attention. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. The resulting architecture achieves linear $O(kLd)$ time and memory complexity. On eight real-world benchmarking datasets from six different domains, including weather, stock price, temperature, power consumption, electricity, and disease, and forecasting horizons $96-720$, TwinFormer secures $27$ positions in the top two out of $34$. Out of the $27$, it achieves the best performance on MAE and RMSE at $17$ places and $10$ at the second-best place on MAE and RMSE. This consistently outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers. Ablations confirm the superiority of top-$k$ Sparse Attention over ProbSparse and the effectiveness of GRU-based aggregation. Code is available at this repository: https://github.com/Mahimakumavat1205/TwinFormer.

</details>


### [175] [Synthetic Swarm Mosquito Dataset for Acoustic Classification: A Proof of Concept](https://arxiv.org/abs/2512.12365)
*Thai-Duy Dinh,Minh-Luan Vo,Cuong Tuan Nguyen,Bich-Hien Vo*

Main category: cs.LG

TL;DR: 本文介绍合成蚊群数据集用于声学分类，评估轻量级深度学习架构，模型能识别六种主要蚊媒，展示了合成数据集在蚊虫研究和监测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 蚊媒疾病是严重的全球健康威胁，传统数据集录制个体蚊子劳动强度大，需新方法生成数据。

Method: 创建合成蚊群数据集模拟多物种和嘈杂环境，使用对数梅尔频谱图评估轻量级深度学习架构。

Result: 模型能有效识别六种主要蚊媒，适合部署在嵌入式低功耗设备上。

Conclusion: 合成蚊群音频数据集有加速声学蚊虫研究和实现可扩展实时监测解决方案的潜力。

Abstract: Mosquito-borne diseases pose a serious global health threat, causing over 700,000 deaths annually. This work introduces a proof-of-concept Synthetic Swarm Mosquito Dataset for Acoustic Classification, created to simulate realistic multi-species and noisy swarm conditions. Unlike conventional datasets that require labor-intensive recording of individual mosquitoes, the synthetic approach enables scalable data generation while reducing human resource demands. Using log-mel spectrograms, we evaluated lightweight deep learning architectures for the classification of mosquito species. Experiments show that these models can effectively identify six major mosquito vectors and are suitable for deployment on embedded low-power devices. The study demonstrates the potential of synthetic swarm audio datasets to accelerate acoustic mosquito research and enable scalable real-time surveillance solutions.

</details>


### [176] [The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining](https://arxiv.org/abs/2512.12384)
*Jesse Ponnock*

Main category: cs.LG

TL;DR: 对美国证交会文件继续预训练进行早期扩展定律分析，发现财政领域验证损失持续改善，通用领域损失不变，为财政基础模型扩展提供经验指导。


<details>
  <summary>Details</summary>
Motivation: 研究在无需完全重新训练的情况下，为高价值领域定制大语言模型的方法，通过对美国证交会文件继续预训练进行早期扩展定律分析来探索。

Method: 在4亿token的金融语料库上训练10亿和30亿参数的Llama - 3.2模型，在5000万、1亿、2亿和4亿token处设置验证检查点。

Result: 两个模型在证交会领域验证损失持续改善，前2亿token收益最大，之后收益递减；幂律拟合显示浅指数；通用领域验证损失不变；数据效率前沿表明模型向更好的专业化发展且混合领域退化可忽略。

Conclusion: 有意义的领域适应可通过相对适度的token预算实现，更大规模（70亿 - 700亿）模型在预计数据要求下仍可行。

Abstract: Domain-adaptive pretraining (DAPT) offers a practical path to specializing large language models for high-value domains without full retraining. We conduct an early-stage scaling-law analysis of continued pretraining on U.S. SEC filings, training 1B and 3B-parameter Llama-3.2 models on a 400M-token financial corpus with validation checkpoints at 50M, 100M, 200M, and 400M tokens. Results show consistent improvements in SEC-domain validation loss for both models, with the largest gains occurring within the first 200M tokens and diminishing returns thereafter. Power-law fits reveal shallow exponents, indicating that financial language is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remains effectively unchanged across all token budgets, suggesting minimal drift and no signs of catastrophic forgetting. A data-efficiency frontier further shows that both models move toward improved specialization with negligible mixed-domain degradation. Together, these findings provide early empirical guidance for scaling financial foundation models, suggesting that meaningful domain adaptation can be achieved with comparatively modest token budgets and that larger model scales (7B-70B) remain tractable under projected data requirements.

</details>


### [177] [Anchoring Values in Temporal and Group Dimensions for Flow Matching Model Alignment](https://arxiv.org/abs/2512.12387)
*Yawen Shao,Jie Xiao,Kai Zhu,Yu Liu,Wei Zhai,Yang Cao,Zheng-Jun Zha*

Main category: cs.LG

TL;DR: 现有GRPO用于图像生成有局限，提出VGPO框架解决问题，实验证明VGPO有良好效果。


<details>
  <summary>Details</summary>
Motivation: 当前GRPO用于基于流匹配的图像生成时存在核心原则与视觉合成过程不匹配的问题，导致时间信用分配受损和优化停滞。

Method: 提出Value - Anchored Group Policy Optimization (VGPO)框架，将稀疏终端奖励转化为密集、过程感知的价值估计，用绝对价值增强的新过程替代标准组归一化。

Result: 在三个基准测试中，VGPO达到了最先进的图像质量，同时提高了特定任务的准确性，有效缓解了奖励作弊问题。

Conclusion: VGPO框架能有效解决现有GRPO用于图像生成时的局限性，提升图像生成效果。

Abstract: Group Relative Policy Optimization (GRPO) has proven highly effective in enhancing the alignment capabilities of Large Language Models (LLMs). However, current adaptations of GRPO for the flow matching-based image generation neglect a foundational conflict between its core principles and the distinct dynamics of the visual synthesis process. This mismatch leads to two key limitations: (i) Uniformly applying a sparse terminal reward across all timesteps impairs temporal credit assignment, ignoring the differing criticality of generation phases from early structure formation to late-stage tuning. (ii) Exclusive reliance on relative, intra-group rewards causes the optimization signal to fade as training converges, leading to the optimization stagnation when reward diversity is entirely depleted. To address these limitations, we propose Value-Anchored Group Policy Optimization (VGPO), a framework that redefines value estimation across both temporal and group dimensions. Specifically, VGPO transforms the sparse terminal reward into dense, process-aware value estimates, enabling precise credit assignment by modeling the expected cumulative reward at each generative stage. Furthermore, VGPO replaces standard group normalization with a novel process enhanced by absolute values to maintain a stable optimization signal even as reward diversity declines. Extensive experiments on three benchmarks demonstrate that VGPO achieves state-of-the-art image quality while simultaneously improving task-specific accuracy, effectively mitigating reward hacking. Project webpage: https://yawen-shao.github.io/VGPO/.

</details>


### [178] [DeepVekua: Geometric-Spectral Representation Learning for Physics-Informed Fields](https://arxiv.org/abs/2512.12402)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: 本文提出DeepVekua架构统一几何深度学习与谱分析，在解决稀疏数据下PDEs问题上优于现有方法，有100倍性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏数据下偏微分方程（PDEs）的求解问题，克服标准基于坐标网络的谱偏差问题。

Method: 学习一个微分同胚坐标变换，将复杂几何映射到潜在调和空间，分离几何和物理的学习并求解最佳谱权重。

Result: 在平流 - 扩散系统上优于最先进的隐式表示，相比谱基线有100倍的性能提升。

Conclusion: DeepVekua架构在解决稀疏数据下PDEs问题上有效且性能良好 ，代码已开源。

Abstract: We present DeepVekua, a hybrid architecture that unifies geometric deep learning with spectral analysis to solve partial differential equations (PDEs) in sparse data regimes. By learning a diffeomorphic coordinate transformation that maps complex geometries to a latent harmonic space, our method outperforms state-of-the-art implicit representations on advection-diffusion systems. Unlike standard coordinate-based networks which struggle with spectral bias, DeepVekua separates the learning of geometry from the learning of physics, solving for optimal spectral weights in closed form. We demonstrate a 100x improvement over spectral baselines. The code is available at https://github.com/VladimerKhasia/vekuanet.

</details>


### [179] [Can Graphs Improve Tabular Foundation Models?](https://arxiv.org/abs/2512.12405)
*Franck Le,Keith Grueneberg,Erich Nahum,Vadim Sheinin*

Main category: cs.LG

TL;DR: 研究引入简单图先验能否增强预训练表格transformer，提出BOLERO方法，实验证明其能有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有表格处理方法大多缺乏显式机制来建模实例间关系，研究引入简单图先验对预训练表格transformer的增强作用。

Method: 引入轻量级静态二分图头BOLERO增强RoBERTa - Tab，实例连接特征/值锚点，用小GNN细化行表示且骨干网络冻结。

Result: 在80个分类和64个回归数据集上评估，BOLERO在分类和回归中取得最多统计学显著胜利。

Conclusion: 轻量级图先验能有效提升预训练表格transformer的性能。

Abstract: Tabular data are central to many real-world systems. While recent tabular transformers and in-context learners such as SAINT, TP-BERTa, TabPFN, TabICL, and MITRA incorporate limited inter-row reasoning, most approaches still lack an explicit mechanism to model relationships among instances, even though similar samples often share related outcomes. We investigate whether introducing \emph{simple graph priors} can enhance \emph{pretrained tabular transformers}. Concretely, we introduce {BOLERO}, a lightweight, static bipartite graph head that augments {RoBERTa-Tab} (a RoBERTa-style tabular backbone pretrained with masked-token prediction.) Each instance connects to feature/value anchors; a small GNN refines row representations, while the backbone remains frozen. We evaluate on 80 classification and 64 regression datasets from the TP-BERTa benchmark suites, comparing against strong baselines including XGBoost, CatBoost, TabPFN-v2, MITRA, TabICL, TP-BERTa, and RoBERTa-Tab. To ensure statistically sound conclusions, we follow best practices for multi-dataset evaluation: pairwise Wilcoxon signed-rank tests on per-dataset score differences and effect sizes (median improvement with confidence intervals), rather than mean-rank post-hoc tests that depend on the competitor pool. BOLERO achieves the highest number of statistically significant wins across both classification and regression, demonstrating that lightweight graph priors meaningfully improve pretrained tabular transformers.

</details>


### [180] [Rough Sets for Explainability of Spectral Graph Clustering](https://arxiv.org/abs/2512.12436)
*Bartłomiej Starosta,Sławomir T. Wierzchoń,Piotr Borkowski,Dariusz Czerski,Marcin Sydow,Eryk Laskowski,Mieczysław A. Kłopotek*

Main category: cs.LG

TL;DR: 本文提出基于粗糙集理论改进图谱聚类方法的解释性。


<details>
  <summary>Details</summary>
Motivation: 图谱聚类算法应用于文本文档时结果难解释，无明确内容的文档和算法随机性会降低可解释性。

Method: 借鉴粗糙集理论，改进团队之前提出的解释方法。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.

</details>


### [181] [Knowledge-Guided Masked Autoencoder with Linear Spectral Mixing and Spectral-Angle-Aware Reconstruction](https://arxiv.org/abs/2512.12445)
*Abdul Matin,Rupasree Dey,Tanjim Bin Faruk,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.LG

TL;DR: 提出知识引导基于ViT的掩码自编码器，结合物理约束，提升重建质量和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 将领域知识集成到深度学习中，以提高模型的可解释性、泛化能力和数据效率。

Method: 提出知识引导的基于ViT的掩码自编码器，结合线性光谱混合模型和光谱角度映射器作为物理约束，联合优化损失函数。

Result: 模型显著提高了重建质量，改善了下游任务性能。

Conclusion: 在基于变压器的自监督学习中嵌入物理信息归纳偏差是有前景的。

Abstract: Integrating domain knowledge into deep learning has emerged as a promising direction for improving model interpretability, generalization, and data efficiency. In this work, we present a novel knowledge-guided ViT-based Masked Autoencoder that embeds scientific domain knowledge within the self-supervised reconstruction process. Instead of relying solely on data-driven optimization, our proposed approach incorporates the Linear Spectral Mixing Model (LSMM) as a physical constraint and physically-based Spectral Angle Mapper (SAM), ensuring that learned representations adhere to known structural relationships between observed signals and their latent components. The framework jointly optimizes LSMM and SAM loss with a conventional Huber loss objective, promoting both numerical accuracy and geometric consistency in the feature space. This knowledge-guided design enhances reconstruction fidelity, stabilizes training under limited supervision, and yields interpretable latent representations grounded in physical principles. The experimental findings indicate that the proposed model substantially enhances reconstruction quality and improves downstream task performance, highlighting the promise of embedding physics-informed inductive biases within transformer-based self-supervised learning.

</details>


### [182] [Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling](https://arxiv.org/abs/2512.12461)
*Eray Erturk,Saba Hashemi,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 提出跨模态知识蒸馏框架，将预训练多会话尖峰变压器模型知识转移到LFP变压器模型，提升LFP模型性能。


<details>
  <summary>Details</summary>
Motivation: LFP信号虽有优势，但因聚合特性给建模带来挑战，导致下游任务预测能力低，需解决该问题。

Method: 先使用带特定会话神经标记策略的掩码自编码目标跨多个记录会话训练教师尖峰模型，再将学生LFP模型的潜在表征与教师尖峰模型对齐。

Result: 蒸馏后的LFP模型在无监督和有监督设置中均优于单会话和多会话LFP基线，且能在无额外蒸馏下泛化到其他会话并保持高性能。

Conclusion: 跨模态知识蒸馏是利用高性能尖峰模型开发更准确LFP模型的强大且可扩展的方法。

Abstract: Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.

</details>


### [183] [Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference](https://arxiv.org/abs/2512.12462)
*Eray Erturk,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 提出学习框架实现多模态神经时间序列实时递归解码，在模拟和真实数据集上表现良好且优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态神经活动非线性模型无法处理不同时间尺度、缺失样本问题，且部分不支持实时解码。

Method: 构建学习框架，含多尺度编码器、多尺度动态骨干和特定模态解码器。

Result: 模型能跨不同时间尺度、分布和含缺失样本的模态聚合信息，提升实时目标解码性能，优于多种线性和非线性多模态基准方法。

Conclusion: 所提框架可有效处理多模态神经时间序列实时解码中的问题。

Abstract: Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.

</details>


### [184] [Exploring the Design Space of Transition Matching](https://arxiv.org/abs/2512.12465)
*Uriel Singer,Yaron Lipman*

Main category: cs.LG

TL;DR: 本文对Transition Matching框架中头部模块的设计、训练和采样进行大规模系统研究，对比不同配置，找到表现最佳和次佳的设置，并指出可能有收益和无收益的设计方面。


<details>
  <summary>Details</summary>
Motivation: 探索Transition Matching框架中头部模块的设计、训练和采样，以提高生成模型的质量和效率。

Method: 对56个不同的17亿参数的文本到图像模型进行综合消融实验和研究，涉及549次独特评估。

Result: 使用特定时间权重训练的MLP头部，配合高频采样器效果最佳；使用序列缩放的Transformer头部和低频采样在图像美感方面表现出色。

Conclusion: 实验突出了可能提升质量和效率的设计方面，也指出了不太可能有进一步收益的设计选择。

Abstract: Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller "head" module to efficiently execute the generative transition step. In this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics. Lastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.

</details>


### [185] [Sparse Concept Anchoring for Interpretable and Controllable Neural Representations](https://arxiv.org/abs/2512.12469)
*Sandy Fraser,Patryk Wielopolski*

Main category: cs.LG

TL;DR: 介绍Sparse Concept Anchoring方法，用极少监督定位概念子集，实验表明可选择性减弱目标概念，提供可解释可控表征途径。


<details>
  <summary>Details</summary>
Motivation: 寻求在学习表示中实现可解释和可控行为的方法。

Method: 结合激活归一化、分离正则化和锚或子空间正则化，用极少监督将潜在空间向目标概念子集偏移。

Result: 对结构化自动编码器实验显示能选择性减弱目标概念且对正交特征影响小，消除时重建误差接近理论界限。

Conclusion: Sparse Concept Anchoring为学习表示中的可解释、可控行为提供实用途径。

Abstract: We introduce Sparse Concept Anchoring, a method that biases latent space to position a targeted subset of concepts while allowing others to self-organize, using only minimal supervision (labels for <0.1% of examples per anchored concept). Training combines activation normalization, a separation regularizer, and anchor or subspace regularizers that attract rare labeled examples to predefined directions or axis-aligned subspaces. The anchored geometry enables two practical interventions: reversible behavioral steering that projects out a concept's latent component at inference, and permanent removal via targeted weight ablation of anchored dimensions. Experiments on structured autoencoders show selective attenuation of targeted concepts with negligible impact on orthogonal features, and complete elimination with reconstruction error approaching theoretical bounds. Sparse Concept Anchoring therefore provides a practical pathway to interpretable, steerable behavior in learned representations.

</details>


### [186] [GoMS: Graph of Molecule Substructure Network for Molecule Property Prediction](https://arxiv.org/abs/2512.12489)
*Shuhui Qu,Cheolwoo Park*

Main category: cs.LG

TL;DR: 提出GoMS架构用于分子属性预测，实验表明其性能优于ESAN等基线方法，在大分子上表现更佳，理论分析显示能区分特定分子，对材料科学应用有前景。


<details>
  <summary>Details</summary>
Motivation: 现有分子属性预测方法如ESAN将分子视为独立子结构集合，忽略子结构间关键关系，需新方法解决。

Method: 提出Graph of Molecule Substructures (GoMS)架构，构建图，节点为子图，边表示子结构间结构关系。

Result: 在公共分子数据集实验中，GoMS性能优于ESAN等基线方法，分子越大优势越明显；理论分析表明能区分特定分子。

Conclusion: GoMS通过捕捉基于包的方法中丢失的子结构关系，在可扩展和可解释的分子属性预测方面取得显著进展，对现实应用有重要意义。

Abstract: While graph neural networks have shown remarkable success in molecular property prediction, current approaches like the Equivariant Subgraph Aggregation Networks (ESAN) treat molecules as bags of independent substructures, overlooking crucial relationships between these components. We present Graph of Molecule Substructures (GoMS), a novel architecture that explicitly models the interactions and spatial arrangements between molecular substructures. Unlike ESAN's bag-based representation, GoMS constructs a graph where nodes represent subgraphs and edges capture their structural relationships, preserving critical topological information about how substructures are connected and overlap within the molecule. Through extensive experiments on public molecular datasets, we demonstrate that GoMS outperforms ESAN and other baseline methods, with particularly improvements for large molecules containing more than 100 atoms. The performance gap widens as molecular size increases, demonstrating GoMS's effectiveness for modeling industrial-scale molecules. Our theoretical analysis demonstrates that GoMS can distinguish molecules with identical subgraph compositions but different spatial arrangements. Our approach shows particular promise for materials science applications involving complex molecules where properties emerge from the interplay between multiple functional units. By capturing substructure relationships that are lost in bag-based approaches, GoMS represents a significant advance toward scalable and interpretable molecular property prediction for real-world applications.

</details>


### [187] [AI-Driven Early Warning Systems for Student Success: Discovering Static Feature Dominance in Temporal Prediction Models](https://arxiv.org/abs/2512.12493)
*Vaarunay Kaushal,Rajib Mall*

Main category: cs.LG

TL;DR: 研究将时间预测分析拓展到第20周，对比决策树和LSTM模型，发现不同干预阶段关注不同指标，模型选择应依干预时间而定，早期信号可用于初步预测。


<details>
  <summary>Details</summary>
Motivation: 在在线学习环境中，对高风险学生进行早期识别以实现有效干预。

Method: 将时间预测分析拓展到第20周，对比决策树和LSTM模型在6个时间点的表现。

Result: 不同干预阶段重要指标不同；静态人口特征对预测重要性达68%；LSTM第2周召回率97%，决策树中期表现稳定，第20周两模型召回率相近，LSTM精度更高。

Conclusion: 模型选择应取决于干预时间，早期信号结合人口和入学前信息可进行可靠初始预测。

Abstract: Early identification of at-risk students is critical for effective intervention in online learning environments. This study extends temporal prediction analysis to Week 20 (50% of course duration), comparing Decision Tree and Long Short- Term Memory (LSTM) models across six temporal snapshots. Our analysis reveals that different performance metrics matter at different intervention stages: high recall is critical for early intervention (Weeks 2-4), while balanced precision-recall is important for mid-course resource allocation (Weeks 8-16), and high precision becomes paramount in later stages (Week 20). We demonstrate that static demographic features dominate predictions (68% importance), enabling assessment-free early prediction. The LSTM model achieves 97% recall at Week 2, making it ideal for early intervention, while Decision Tree provides stable balanced performance (78% accuracy) during mid-course. By Week 20, both models converge to similar recall (68%), but LSTM achieves higher precision (90% vs 86%). Our findings also suggest that model selection should depend on intervention timing, and that early signals (Weeks 2-4) are sufficient for reliable initial prediction using primarily demographic and pre-enrollment information.

</details>


### [188] [Policy Optimization for Dynamic Heart Transplant Allocation](https://arxiv.org/abs/2512.12497)
*Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 针对心脏移植供体分配政策缺陷，开发模拟器评估不同政策，发现现状政策不如短视政策，还提出改进政策，且分批处理供体可提升性能，模拟器还能评估关键因素。


<details>
  <summary>Details</summary>
Motivation: 当前心脏移植供体分配政策在2018年修订后仍未充分考虑移植前后死亡率，需解决这些不足。

Method: 利用UNOS历史数据开发新模拟器评估和比较不同政策，引入潜力值制定改进政策，考虑分批处理供体，用模拟器评估关键因素。

Result: 现状政策远不如短视政策；提出考虑分配过程动态性的改进政策；分批处理特定类型供体可提升性能；能评估地理接近度和移植中心拒绝提议倾向等因素。

Conclusion: 所开发的模拟器和改进政策能有效解决当前心脏移植供体分配政策的不足。

Abstract: Heart transplantation is a viable path for patients suffering from advanced heart failure, but this lifesaving option is severely limited due to donor shortage. Although the current allocation policy was recently revised in 2018, a major concern is that it does not adequately take into account pretransplant and post-transplant mortality. In this paper, we take an important step toward addressing these deficiencies.
  To begin with, we use historical data from UNOS to develop a new simulator that enables us to evaluate and compare the performance of different policies. We then leverage our simulator to demonstrate that the status quo policy is considerably inferior to the myopic policy that matches incoming donors to the patient who maximizes the number of years gained by the transplant. Moreover, we develop improved policies that account for the dynamic nature of the allocation process through the use of potentials -- a measure of a patient's utility in future allocations that we introduce. We also show that batching together even a handful of donors -- which is a viable option for a certain type of donors -- further enhances performance. Our simulator also allows us to evaluate the effect of critical, and often unexplored, factors in the allocation, such as geographic proximity and the tendency to reject offers by the transplant centers.

</details>


### [189] [Noise-robust Contrastive Learning for Critical Transition Detection in Dynamical Systems](https://arxiv.org/abs/2512.12523)
*Wenqi Fang,Ye Li*

Main category: cs.LG

TL;DR: 提出新神经网络架构及训练算法改进传统对比学习，用于检测复杂噪声时间序列数据中的临界转变，更轻量且抗噪。


<details>
  <summary>Details</summary>
Motivation: 标准对比学习方法检测临界转变时存在过参数化和对无关噪声敏感的问题，导致临界点识别不准确。

Method: 使用奇异值分解技术构建神经网络架构，结合严格半正交约束训练算法来改进传统对比学习。

Result: 该方法在识别临界转变上与传统对比学习技术性能相当，但更轻量且抗噪性显著提高。

Conclusion: 所提方法能有效解决传统对比学习的局限，提升临界转变检测效果。

Abstract: Detecting critical transitions in complex, noisy time-series data is a fundamental challenge across science and engineering. Such transitions may be anticipated by the emergence of a low-dimensional order parameter, whose signature is often masked by high-amplitude stochastic variability. Standard contrastive learning approaches based on deep neural networks, while promising for detecting critical transitions, are often overparameterized and sensitive to irrelevant noise, leading to inaccurate identification of critical points. To address these limitations, we propose a neural network architecture, constructed using singular value decomposition technique, together with a strictly semi-orthogonality-constrained training algorithm, to enhance the performance of traditional contrastive learning. Extensive experiments demonstrate that the proposed method matches the performance of traditional contrastive learning techniques in identifying critical transitions, yet is considerably more lightweight and markedly more resistant to noise.

</details>


### [190] [Effective Fine-Tuning with Eigenvector Centrality Based Pruning](https://arxiv.org/abs/2512.12543)
*Shaif Chowdhury,Soham Biren Katlariwala,Devleena Kashyap*

Main category: cs.LG

TL;DR: 社交媒体中少数有影响力用户能引发大范围话语变化，神经网络微调也有类似现象。提出基于图论的剪枝方法改进微调性能，在多模型和数据集上验证，提高分类准确率并降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 改进卷积神经网络微调效果，传统微调在预训练模型上添加线性分类层，希望找到更好的适应方法。

Method: 提出基于图论的神经网络剪枝方法，将神经元表示为节点，边表示神经元相似度，用特征向量中心性计算重要性分数进行剪枝，再对剪枝后的网络用最核心神经元微调。

Result: 在VGGNet、EfficientNet和ResNet模型及多个数据集上评估，该方法提高分类准确率并显著降低模型复杂度，如在Oxford Flowers 102数据集上准确率达48%，而基线VGGNet模型为30%。

Conclusion: 基于图论的剪枝方法能有效改进神经网络微调性能，提高分类准确率并降低模型复杂度。

Abstract: In social media networks a small number of highly influential users can drive large scale changes in discourse across multiple communities. Small shifts in the behavior of these users are often sufficient to propagate widely throughout the network. A similar phenomenon occurs during neural network fine tuning. Conventional fine tuning of convolutional neural networks typically adds a new linear classification layer on top of a large pre trained model. Instead we argue that improved adaptation can be achieved by first pruning the network to retain only the most important neurons and then performing fine tuning.
  We propose a graph theory based method for pruning neural networks that is designed to improve fine tuning performance. In this method each neuron is represented as a node and edges encode similarity between neurons. Neurons are pruned based on importance scores computed using eigenvector centrality. The resulting pruned network is then fine tuned using only the most central neurons. We evaluate the proposed method on VGGNet EfficientNet and ResNet models using the TF Flowers Caltech one zero one and Oxford Flowers one zero two datasets. The proposed approach achieves higher classification accuracy while significantly reducing model complexity. On the Oxford Flowers one zero two dataset the method achieves forty eight percent classification accuracy compared to thirty percent accuracy obtained by the baseline VGGNet model.

</details>


### [191] [Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model](https://arxiv.org/abs/2512.12545)
*Bin Mu,Yuxuan Chen,Shijin Yuan,Bo Qin,Hao Guo*

Main category: cs.LG

TL;DR: 提出多圈层耦合概率模型 TianXing - S2S 用于全球 S2S 日集合预报，表现优于其他系统，对极端事件有预测能力，能生成 180 天稳定滚动预报。


<details>
  <summary>Details</summary>
Motivation: 在气候变化加速情况下，准确的 S2S 极端事件预测对资源规划和减灾至关重要，但因多圈层复杂交互和大气不确定性，此类预测有挑战。

Method: 先将多圈层预测因子编码到紧凑潜在空间，用扩散模型生成日集合预报，在去噪器中加入基于最优传输（OT）的耦合模块优化大气与多圈层边界条件的交互。

Result: 在关键大气变量 45 天日均集合预报中，TianXing - S2S 优于 ECMWF S2S 系统和 FuXi - S2S；能对热浪和异常降水等极端事件进行有效亚季节预测；可生成长达 180 天稳定滚动预报。

Conclusion: TianXing - S2S 为变暖世界的 S2S 研究建立了可靠框架。

Abstract: Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.

</details>


### [192] [Differentiable Energy-Based Regularization in GANs: A Simulator-Based Exploration of VQE-Inspired Auxiliary Losses](https://arxiv.org/abs/2512.12581)
*David Strnadel*

Main category: cs.LG

TL;DR: 本文探索将参数化量子电路的可微能量项作为生成对抗网络（GAN）的辅助正则化信号，在MNIST上测试，虽有计算开销但分类准确率提升，方法有一定贡献，未证明量子优势。


<details>
  <summary>Details</summary>
Motivation: 研究参数化量子电路的可微能量项能否作为GAN的辅助正则化信号。

Method: 在辅助分类器GAN（ACGAN）生成器目标中加入受变分量子本征求解器（VQE）启发的能量项，使用Qiskit的EstimatorQNN和TorchConnector计算。

Result: 在MNIST上，能量正则化模型（QACGAN）5个epoch内分类准确率达99 - 100%，优于ACGAN的87.8%；样本质量指标（FID）有高方差，长时间运行稳定后与ACGAN基线相当；计算开销约为经典ACGAN的200倍。

Conclusion: 贡献在于展示了VQE风格的能量计算可通过可微路径集成到GAN训练循环中，是否优于经典正则化器需后续研究。

Abstract: This paper presents an exploratory, simulator-based proof of concept investigating whether differentiable energy terms derived from parameterized quantum circuits can serve as auxiliary regularization signals in Generative Adversarial Networks (GANs). We augment the Auxiliary Classifier GAN (ACGAN) generator objective with a Variational Quantum Eigensolver (VQE)-inspired energy term computed from class-specific Ising Hamiltonians using Qiskit's EstimatorQNN and TorchConnector.
  Important limitations: All experiments run on a noiseless statevector simulator with only 4 qubits, use a deliberately simple Hamiltonian parameterization, and lack ablation studies comparing against equivalent classical biases. The computational overhead (approximately 200x slower than classical ACGAN) reflects simulator artifacts rather than inherent quantum costs.
  On MNIST, we observe that the energy-regularized model (termed QACGAN) achieves high classification accuracy (99 to 100 percent) within 5 epochs compared to 87.8 percent for ACGAN, suggesting the auxiliary term influences class conditioning. However, sample quality metrics (FID) show high variance across runs (coefficient of variation approximately 25 percent at epoch 5), with values ranging from 19.92 to 35.96. Extended runs stabilize around FID 23 to 24, comparable to the ACGAN baseline.
  We explicitly do not claim quantum advantage, improved stability in any general sense, or scalability beyond this toy setting. The contribution is methodological: demonstrating that VQE-style energy computations can be integrated into GAN training loops via differentiable pathways. Whether such auxiliary signals provide benefits beyond equivalent classical regularizers remains an open question requiring systematic ablation studies, which we leave for future work.

</details>


### [193] [Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics](https://arxiv.org/abs/2512.12602)
*Jingdi Lei,Di Zhang,Soujanya Poria*

Main category: cs.LG

TL;DR: 提出误差自由线性注意力机制EFLA，理论无误差积累，实验表现良好，为线性时间注意力模型提供新理论基础


<details>
  <summary>Details</summary>
Motivation: 解决长上下文语言模型中使用softmax注意力的二次成本瓶颈问题

Method: 将在线学习更新公式化为连续时间动态系统，利用动力学矩阵的秩1结构直接推导精确闭式解

Result: EFLA在噪声环境中性能稳健，语言建模困惑度更低，下游基准测试性能更优，且不引入额外参数

Conclusion: 为构建高保真、可扩展的线性时间注意力模型提供新理论基础

Abstract: Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.

</details>


### [194] [Causal inference and model explainability tools for retail](https://arxiv.org/abs/2512.12605)
*Pranav Gupta,Nithin Surendran*

Main category: cs.LG

TL;DR: 本文探讨在电商和零售领域应用模型可解释性和因果推断获取销售洞察，回顾相关文献并应用于真实数据集，有一定发现。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习和统计技术分析和预测关键指标时缺乏可解释性，且无法验证或发现因果联系，因此要提供应用模型可解释性和因果推断获取销售洞察的方法。

Method: 回顾电商和零售领域因果推断和可解释性的现有文献，并将其应用于真实数据集。

Result: 固有可解释模型的SHAP值方差较低，通过双重机器学习方法包含多个混杂因素能得到正确因果效应符号。

Conclusion: 应用模型可解释性和因果推断能为电商和零售领域的销售洞察提供有效途径。

Abstract: Most major retailers today have multiple divisions focused on various aspects, such as marketing, supply chain, online customer experience, store customer experience, employee productivity, and vendor fulfillment. They also regularly collect data corresponding to all these aspects as dashboards and weekly/monthly/quarterly reports. Although several machine learning and statistical techniques have been in place to analyze and predict key metrics, such models typically lack interpretability. Moreover, such techniques also do not allow the validation or discovery of causal links. In this paper, we aim to provide a recipe for applying model interpretability and causal inference for deriving sales insights. In this paper, we review the existing literature on causal inference and interpretability in the context of problems in e-commerce and retail, and apply them to a real-world dataset. We find that an inherently explainable model has a lower variance of SHAP values, and show that including multiple confounders through a double machine learning approach allows us to get the correct sign of causal effect.

</details>


### [195] [Torch Geometric Pool: the Pytorch library for pooling in Graph Neural Networks](https://arxiv.org/abs/2512.12642)
*Filippo Maria Bianchi,Carlo Abate,Ivan Marisca*

Main category: cs.LG

TL;DR: 介绍图神经网络分层池化库Torch Geometric Pool (tgp)，展示其结构并进行广泛基准测试，结果支持开发快速原型库的需求。


<details>
  <summary>Details</summary>
Motivation: 开发一个具有广泛池化算子、统一API和模块化设计，强调可用性和扩展性的图神经网络分层池化库。

Method: 构建基于Pytorch Geometric的tgp库，展示其结构，并进行广泛的基准测试。

Result: 选择最优池化算子依赖于具体任务和数据。

Conclusion: 需要开发能进行快速原型设计的库。

Abstract: We introduce Torch Geometric Pool (tgp), a library for hierarchical pooling in Graph Neural Networks. Built upon Pytorch Geometric, Torch Geometric Pool (tgp) provides a wide variety of pooling operators, unified under a consistent API and a modular design. The library emphasizes usability and extensibility, and includes features like precomputed pooling, which significantly accelerate training for a class of operators. In this paper, we present tgp's structure and present an extensive benchmark. The latter showcases the library's features and systematically compares the performance of the implemented graph-pooling methods in different downstream tasks. The results, showing that the choice of the optimal pooling operator depends on tasks and data at hand, support the need for a library that enables fast prototyping.

</details>


### [196] [PerNodeDrop: A Method Balancing Specialized Subnets and Regularization in Deep Neural Networks](https://arxiv.org/abs/2512.12663)
*Gelesh G Omathil,Sreeja CS*

Main category: cs.LG

TL;DR: 本文提出轻量级随机正则化方法PerNodeDrop，打破现有技术噪声均匀性，保留有用共适应，提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络易过拟合，现有基于噪声的正则化方法会抑制有益和有害共适应。

Method: 引入PerNodeDrop方法，对每个样本的每个节点进行扰动，在样本层面丢弃权重，并进行期望损失分析。

Result: 实验表明，PerNodeDrop缩小了训练和验证性能差距，在视觉、文本和音频基准测试中泛化能力优于标准基于噪声的正则化方法。

Conclusion: PerNodeDrop能有效保留有用共适应，提升模型在未见数据上的可靠性和泛化能力。

Abstract: Deep neural networks possess strong representational capacity yet remain vulnerable to overfitting, primarily because neurons tend to co-adapt in ways that, while capturing complex and fine-grained feature interactions, also reinforce spurious and non-generalizable patterns that inflate training performance but reduce reliability on unseen data. Noise-based regularizers such as Dropout and DropConnect address this issue by injecting stochastic perturbations during training, but the noise they apply is typically uniform across a layer or across a batch of samples, which can suppress both harmful and beneficial co-adaptation.
  This work introduces PerNodeDrop, a lightweight stochastic regularization method. It applies per-sample, per-node perturbations to break the uniformity of the noise injected by existing techniques, thereby allowing each node to experience input-specific variability. Hence, PerNodeDrop preserves useful co-adaptation while applying regularization. This narrows the gap between training and validation performance and improves reliability on unseen data, as evident from the experiments.
  Although superficially similar to DropConnect, PerNodeDrop operates at the sample level. It drops weights at the sample level, not the batch level. An expected-loss analysis formalizes how its perturbations attenuate excessive co-adaptation while retaining predictive interactions. Empirical evaluations on vision, text, and audio benchmarks indicate improved generalization relative to the standard noise-based regularizer.

</details>


### [197] [DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization](https://arxiv.org/abs/2512.12669)
*Jiawei Shen,Jia Zhu,Hanghui Guo,Weijie Shi,Guoqing Ma,Yidan Liang,Jingjiang Liu,Hao Chen,Shimin Di*

Main category: cs.LG

TL;DR: 文章提出统一的时间知识图谱推理方法DynaGen，解决现有方法缺陷，实验显示其达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的时间知识图谱推理插值方法存在上下文建模能力有限、外推方法有认知泛化偏差的问题。

Method: 提出DynaGen方法，插值时动态构建以实体为中心的子图，用协同双分支GNN编码器处理；外推时应用条件扩散过程。

Result: 在六个基准数据集上实验，相比次优模型，插值的平均倒数排名（MRR）得分提高2.61分，外推提高1.45分。

Conclusion: DynaGen达到了时间知识图谱推理的SOTA性能。

Abstract: Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.

</details>


### [198] [On Approaches to Building Surrogate ODE Models for Diffusion Bridges](https://arxiv.org/abs/2512.12671)
*Maria Khilchuk,Vladimir Latypov,Pavel Kleshchev,Alexander Hvatov*

Main category: cs.LG

TL;DR: 本文提出用替代模型近似扩散和薛定谔桥模型动力学，提出SINDy - FM和DSBM - NeuralODE算法，实验证明其高效且可解释。


<details>
  <summary>Details</summary>
Motivation: 扩散和薛定谔桥模型在生成建模中计算成本高、训练程序复杂，连续时间桥的最优动力学描述存在问题。

Method: 提出SINDy Flow Matching (SINDy - FM)和Schrödinger Bridge的Neural - ODE重新表述(DSBM - NeuralODE)两种算法。

Result: 在高斯传输任务和MNIST潜在转换实验中，替代模型性能有竞争力，效率和可解释性大幅提升，SINDy - FM模型减少参数数量。

Conclusion: 为实际部署的易处理且高性能的桥模型开辟了新途径。

Abstract: Diffusion and Schrödinger Bridge models have established state-of-the-art performance in generative modeling but are often hampered by significant computational costs and complex training procedures. While continuous-time bridges promise faster sampling, overparameterized neural networks describe their optimal dynamics, and the underlying stochastic differential equations can be difficult to integrate efficiently. This work introduces a novel paradigm that uses surrogate models to create simpler, faster, and more flexible approximations of these dynamics. We propose two specific algorithms: SINDy Flow Matching (SINDy-FM), which leverages sparse regression to identify interpretable, symbolic differential equations from data, and a Neural-ODE reformulation of the Schrödinger Bridge (DSBM-NeuralODE) for flexible continuous-time parameterization. Our experiments on Gaussian transport tasks and MNIST latent translation demonstrate that these surrogates achieve competitive performance while offering dramatic improvements in efficiency and interpretability. The symbolic SINDy-FM models, in particular, reduce parameter counts by several orders of magnitude and enable near-instantaneous inference, paving the way for a new class of tractable and high-performing bridge models for practical deployment.

</details>


### [199] [Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity](https://arxiv.org/abs/2512.12688)
*Dongseok Kim,Hyoungsun Choi,Mohamed Jismy Aashik Rasool,Gisung Oh*

Main category: cs.LG

TL;DR: 研究固定Transformer骨干权重，仅改变提示语可获得的函数族，证明了单个固定骨干可通过提示语近似广泛目标行为。


<details>
  <summary>Details</summary>
Motivation: 将提示语改变模型行为这一现象从启发式方法转变为理论对象进行研究。

Method: 把提示语看作外部注入程序，构建简化Transformer解释提示语以实现不同计算，对其机制进行分解。

Result: 证明了单个固定骨干可仅通过提示语近似一大类目标行为。

Conclusion: 该框架为形式化提示语长度/精度约束下的权衡和研究基于提示语切换的结构限制提供统一起点，与预训练大语言模型的实证主张不同。

Abstract: Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.

</details>


### [200] [Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning](https://arxiv.org/abs/2512.12690)
*Yongcan Yu,Lingxiao He,Shuo Lu,Lijun Sheng,Yinuo Xu,Yanbo Wang,Kuangpu Guo,Jianjie Cheng,Meng Wang,Qianlong Xie,Xingxing Wang,Dapeng Hu,Jian Liang*

Main category: cs.LG

TL;DR: 研究通过对比SFT和RL在视觉语言模型推理中的表现，发现SFT在多种场景下很重要，挑战了“RL优于SFT”的观点，支持两者互补的后训练管道。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型推理研究倾向于强化学习（RL），忽视监督微调（SFT），许多研究认为SFT不能提升推理能力甚至影响训练，作者旨在系统对比二者。

Method: 在视觉语言模型推理中，使用相同数据源对SFT和RL进行系统、可控的对比研究。

Result: SFT和RL的相对有效性受模型能力、数据规模和分布的条件影响，SFT在弱模型、数据效率和跨模态迁移方面有优势，且发现RL存在奖励欺骗问题。

Conclusion: 挑战了“RL优于SFT”的观点，强调SFT作用可能被低估，支持SFT和RL互补的后训练管道。

Abstract: Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing "RL over SFT" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.

</details>


### [201] [Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits](https://arxiv.org/abs/2512.12693)
*Sumantrak Mukherjee,Serafima Lebedeva,Valentin Margraf,Jonas Hanselle,Kanta Yamaoka,Viktor Bengs,Stefan Konigorski,Eyke Hüllermeier,Sebastian Josef Vollmer*

Main category: cs.LG

TL;DR: 提出用于上下文多任务多臂老虎机设置高效探索的贝叶斯框架，方法表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 在上下文部分可观测且奖励分布存在潜在依赖的多任务多臂老虎机场景，利用结构依赖进行高效探索。

Method: 整合各任务观测，学习全局联合分布；用基于粒子的对数密度高斯过程近似表示任务和奖励的联合分布。

Result: 方法在模型错误指定或潜在异质性复杂的场景中表现优于分层模型老虎机等基线。

Conclusion: 所提贝叶斯框架在特定场景探索中有效，能利用结构依赖实现更好性能。

Abstract: We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.

</details>


### [202] [Multi-Trajectory Physics-Informed Neural Networks for HJB Equations with Hard-Zero Terminal Inventory: Optimal Execution on Synthetic & SPY Data](https://arxiv.org/abs/2512.12708)
*Anthime Valin*

Main category: cs.LG

TL;DR: 研究带硬零终端库存约束的最优交易执行，提出MT - PINN方法，在模型和实际数据测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决香草PINNs在带硬零终端库存约束的最优交易执行问题中，常无法充分执行约束且控制不稳定的问题。

Method: 提出Multi - Trajectory PINN (MT - PINN)，增加基于滚动的轨迹损失，通过时间反向传播对终端库存施加终端惩罚；采用轻量级lambda课程稳定训练。

Result: 在Gatheral - Schied单资产模型上与闭式解接近，使终端库存接近零并减少最优路径误差；在SPY日内数据上，风险中性时匹配TWAP，高风险厌恶时降低敞口和成本。

Conclusion: MT - PINN能有效解决带硬零终端库存约束的最优交易执行问题，在不同风险场景下有良好表现。

Abstract: We study optimal trade execution with a hard-zero terminal inventory constraint, modeled via Hamilton-Jacobi-Bellman (HJB) equations. Vanilla PINNs often under-enforce this constraint and produce unstable controls. We propose a Multi-Trajectory PINN (MT-PINN) that adds a rollout-based trajectory loss and propagates a terminal penalty on terminal inventory via backpropagation-through-time, directly enforcing zero terminal inventory. A lightweight lambda-curriculum is adopted to stabilize training as the state expands from a risk-neutral reduced HJB to a risk-averse HJB. On the Gatheral-Schied single-asset model, MT-PINN aligns closely with their derived closed-form solutions and concentrates terminal inventory tightly around zero while reducing errors along optimal paths. We apply MT-PINNs on SPY intraday data, matching TWAP when risk-neutral, and achieving lower exposure and competitive costs, especially in falling windows, for higher risk-aversion.

</details>


### [203] [Solving a Machine Learning Regression Problem Based on the Theory of Random Functions](https://arxiv.org/abs/2512.12731)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 本文用随机函数理论框架研究机器学习回归问题，从无差别假设推导出回归方法，得到的核函数有理论依据，为平滑和插值方法提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 将机器学习回归问题作为多元逼近问题进行研究，为相关方法提供理论基础。

Method: 从无差别假设出发，利用随机函数理论框架，基于概率测度的自然对称性进行推导。

Result: 得到的核函数与广义多调和样条一致，且由无差别原则得出，而非经验选择。

Conclusion: 为广泛的平滑和插值方法提供理论基础，证明在无先验信息时的最优性。

Abstract: This paper studies a machine learning regression problem as a multivariate approximation problem using the framework of the theory of random functions. An ab initio derivation of a regression method is proposed, starting from postulates of indifference. It is shown that if a probability measure on an infinite-dimensional function space possesses natural symmetries (invariance under translation, rotation, scaling, and Gaussianity), then the entire solution scheme, including the kernel form, the type of regularization, and the noise parameterization, follows analytically from these postulates. The resulting kernel coincides with a generalized polyharmonic spline; however, unlike existing approaches, it is not chosen empirically but arises as a consequence of the indifference principle. This result provides a theoretical foundation for a broad class of smoothing and interpolation methods, demonstrating their optimality in the absence of a priori information.

</details>


### [204] [Resting Neurons, Active Insights: Improving Input Sparsification for Large Language Models](https://arxiv.org/abs/2512.12744)
*Haotian Xu,Tian Gao,Tsui-Wei Weng,Tengfei Ma*

Main category: cs.LG

TL;DR: 本文针对大语言模型效率和可解释性问题，将输入稀疏化重新解释为动态结构剪枝，引入可训练的自发神经元减少稀疏化导致的性能差距。


<details>
  <summary>Details</summary>
Motivation: 大语言模型规模大带来效率和可解释性挑战，现有输入稀疏化方法忽略表征后果，与全模型有性能差距。

Method: 将输入稀疏化重新解释为动态结构剪枝，受生物神经元自发基线放电率启发，引入可训练的自发神经元作为补偿单元。

Result: 实验表明，这些辅助神经元大幅减少了稀疏化导致的性能差距，且在不同任务上有良好泛化性。

Conclusion: 引入可训练的自发神经元能有效解决输入稀疏化带来的性能问题。

Abstract: Large Language Models (LLMs) achieve state-of-the-art performance across a wide range of applications, but their massive scale poses significant challenges for both efficiency and interpretability. Structural pruning, which reduces model size by removing redundant computational units such as neurons, has been widely explored as a solution, and this study devotes to input sparsification, an increasingly popular technique that improves efficiency by selectively activating only a subset of entry values for each input. However, existing approaches focus primarily on computational savings, often overlooking the representational consequences of sparsification and leaving a noticeable performance gap compared to full models. In this work, we first reinterpret input sparsification as a form of dynamic structural pruning. Motivated by the spontaneous baseline firing rates observed in biological neurons, we introduce a small set of trainable spontaneous neurons that act as compensatory units to stabilize activations in sparsified LLMs. Experiments demonstrate that these auxiliary neurons substantially reduce the sparsification-induced performance gap while generalizing effectively across tasks.

</details>


### [205] [Federated Learning with Feedback Alignment](https://arxiv.org/abs/2512.12762)
*Incheol Baek,Hyungbin Kim,Minseo Kim,Yon Dohn Chung*

Main category: cs.LG

TL;DR: 提出集成反馈对齐的联邦学习框架FLFA，可缓解局部漂移，理论和实证证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中数据异质性导致的局部漂移、阻碍全局模型收敛的问题。

Method: 引入Federated Learning with Feedback Alignment (FLFA)框架，在局部训练的反向传播中使用全局模型权重作为共享反馈矩阵。

Result: 理论分析支持FLFA设计，实证评估表明能增强其他联邦学习方法。

Conclusion: FLFA能以最小计算成本和无额外通信开销缓解局部漂移，具有有效性。

Abstract: Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.
  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.

</details>


### [206] [OLR-WAA: Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Averaging](https://arxiv.org/abs/2512.12779)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: 本文提出无超参数模型OLR - WAA应对非平稳数据流挑战，通过指数加权移动平均更新模型，有动态检测概念漂移机制，评估显示性能佳。


<details>
  <summary>Details</summary>
Motivation: 现实数据集存在概念漂移，忽略此现象及在线模型超参数固定会降低模型预测性能，需解决非平稳数据流挑战，平衡模型稳定性与适应性。

Method: 提出OLR - WAA模型，通过指数加权移动平均增量更新基础模型，引入优化机制动态检测概念漂移、量化幅度并根据实时数据特征调整模型，还有基于置信度的保守更新策略。

Result: 在静态设置中与批量回归性能相当，在概念漂移数据集上优于或与现有在线模型相当，能有效缩小性能差距，收敛快且R2值高。

Conclusion: OLR - WAA模型有效，能应对非平稳数据流挑战，实现稳定与适应的平衡。

Abstract: Real-world datasets frequently exhibit evolving data distributions, reflecting temporal variations and underlying shifts. Overlooking this phenomenon, known as concept drift, can substantially degrade the predictive performance of the model. Furthermore, the presence of hyperparameters in online models exacerbates this issue, as these parameters are typically fixed and lack the flexibility to dynamically adjust to evolving data. This paper introduces "OLR-WAA: An Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Average", a hyperparameter-free model designed to tackle the challenges of non-stationary data streams and enable effective, continuous adaptation. The objective is to strike a balance between model stability and adaptability. OLR-WAA incrementally updates its base model by integrating incoming data streams, utilizing an exponentially weighted moving average. It further introduces a unique optimization mechanism that dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on real-time data characteristics. Rigorous evaluations show that it matches batch regression performance in static settings and consistently outperforms or rivals state-of-the-art online models, confirming its effectiveness. Concept drift datasets reveal a performance gap that OLR-WAA effectively bridges, setting it apart from other online models. In addition, the model effectively handles confidence-based scenarios through a conservative update strategy that prioritizes stable, high-confidence data points. Notably, OLR-WAA converges rapidly, consistently yielding higher R2 values compared to other online models.

</details>


### [207] [OLC-WA: Drift Aware Tuning-Free Online Classification with Weighted Average](https://arxiv.org/abs/2512.12785)
*Mohammad Abu Shaira,Yunhe Feng,Heng Fan,Weishi Shi*

Main category: cs.LG

TL;DR: 提出无超参数在线分类模型OLC - WA，能适应动态数据流，实验显示其在不同环境下表现良好。


<details>
  <summary>Details</summary>
Motivation: 现实数据集有概念漂移，忽略此现象及在线模型超参数固定问题会降低模型预测准确性。

Method: 引入OLC - WA模型，用指数加权移动平均融合新数据流与基础模型，集成优化机制检测概念漂移并调整模型。

Result: 在静态环境下性能与批量模型相当，精度差1 - 3%；在漂移环境下领先在线基线模型10 - 25%。

Conclusion: OLC - WA能有效适应动态数据流。

Abstract: Real-world data sets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. This paper introduces Online Classification with Weighted Average (OLC-WA), an adaptive, hyperparameter-free online classification model equipped with an automated optimization mechanism. OLC-WA operates by blending incoming data streams with an existing base model. This blending is facilitated by an exponentially weighted moving average. Furthermore, an integrated optimization mechanism dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on the observed data stream characteristics. This approach empowers the model to effectively adapt to evolving data distributions within streaming environments. Rigorous empirical evaluation across diverse benchmark datasets shows that OLC-WA achieves performance comparable to batch models in stationary environments, maintaining accuracy within 1-3%, and surpasses leading online baselines by 10-25% under drift, demonstrating its effectiveness in adapting to dynamic data streams.

</details>


### [208] [Unveiling Statistical Significance of Online Regression over Multiple Datasets](https://arxiv.org/abs/2512.12787)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: 文章关注跨多个数据集比较多个在线回归模型，用Friedman测试及事后测试评估，结果显示基准模型性能与报告一致，部分先进方法有改进空间。


<details>
  <summary>Details</summary>
Motivation: 大多数机器学习研究忽视跨多个数据集比较多个算法的统计测试，在线学习需稳健统计方法评估性能差异。

Method: 运用Friedman测试及相应事后测试比较多个在线回归模型，采用5折交叉验证和种子平均，使用真实和合成数据集进行全面评估。

Result: 测试基本确认竞争基准模型的性能与各自报告一致，但部分统计测试结果表明部分先进方法某些方面有改进空间。

Conclusion: 当前在线回归模型跨数据集比较需合适统计测试，先进方法仍有提升可能。

Abstract: Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.

</details>


### [209] [Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks](https://arxiv.org/abs/2512.12792)
*Shivansh Sahni,Wenzhi Zhang*

Main category: cs.LG

TL;DR: 介绍Liquid Reasoning Transformer (LRT)架构，在数独任务测试表现佳，并探讨机制拓展。


<details>
  <summary>Details</summary>
Motivation: 设计能进行自适应深度推理的Transformer架构。

Method: LRT通过迭代变化、基于丢弃的修正和学习的停止机制更新循环推理标记，而非单次前馈传递。

Result: 在数独任务上达到98.68%数字准确率和36.30%全谜题准确率，无需符号规则或搜索。

Conclusion: 丢弃和停止门在稳定推理和调整计算深度中起重要作用，机制可自然扩展到国际象棋推理等任务。

Abstract: The Liquid Reasoning Transformer (LRT) is a transformer architecture designed for inference with adaptive depths using iterative changes, discard-based correction, and a learned stopping mechanism. Instead of relying on a single feedforward pass, the model updates a recurrent reasoning token across multiple internal steps, allowing it to correct early errors and allocate computation based on input difficulty. We evaluate the LRT on Sudoku as a controlled testbed for structured reasoning and show that it achieves strong performance, reaching 98.68% digit accuracy and 36.30% full-puzzle accuracy without using symbolic rules or search. Analyzing internal patterns shows that the discard and stop gates play different, important roles in stabilizing inferences and adjusting computational depth. We discuss how these mechanisms extend naturally to chess-scale reasoning tasks and outline extensions for multi-token reasoning and larger domains.

</details>


### [210] [TRACER: Transfer Learning based Real-time Adaptation for Clinical Evolving Risk](https://arxiv.org/abs/2512.12795)
*Mengying Yan,Ziye Tian,Siqi Li,Nan Liu,Benjamin A. Goldstein,Molei Liu,Chuan Hong*

Main category: cs.LG

TL;DR: 提出TRACER框架应对临床决策支持工具的性能漂移问题，模拟研究和实际应用中表现良好。


<details>
  <summary>Details</summary>
Motivation: 临床决策支持工具因群体变化出现性能漂移，系统更新或新疾病出现会导致病例组合变化。

Method: 提出TRACER框架，识别诊疗级别的过渡成员，使用迁移学习自适应调整预测模型，无需完全重新训练。

Result: 模拟研究中TRACER优于历史或当代数据训练的静态模型；在预测新冠期间急诊就诊后住院情况时，TRACER改善了区分度和校准度。

Conclusion: TRACER为在不断变化和异构临床条件下保持强大的预测性能提供了可扩展方法。

Abstract: Clinical decision support tools built on electronic health records often experience performance drift due to temporal population shifts, particularly when changes in the clinical environment initially affect only a subset of patients, resulting in a transition to mixed populations. Such case-mix changes commonly arise following system-level operational updates or the emergence of new diseases, such as COVID-19. We propose TRACER (Transfer Learning-based Real-time Adaptation for Clinical Evolving Risk), a framework that identifies encounter-level transition membership and adapts predictive models using transfer learning without full retraining. In simulation studies, TRACER outperformed static models trained on historical or contemporary data. In a real-world application predicting hospital admission following emergency department visits across the COVID-19 transition, TRACER improved both discrimination and calibration. TRACER provides a scalable approach for maintaining robust predictive performance under evolving and heterogeneous clinical conditions.

</details>


### [211] [From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs](https://arxiv.org/abs/2512.12805)
*Anastasiia Alokhina,Pan Li*

Main category: cs.LG

TL;DR: 论文针对Transformer的大小泛化现象为几何数据建立理论框架并验证理论边界的紧密性。


<details>
  <summary>Details</summary>
Motivation: Transformer的大小泛化能力缺乏严格理论刻画，需要建立理论框架分析该现象。

Method: 为几何数据建立理论框架，得出Transformer对离散样本输出和连续域输出间的误差边界。

Result: 实验证实了理论边界的紧密性。

Conclusion: 对于具有稳定位置编码的Transformer，误差边界由采样密度和数据流形的固有维度决定。

Abstract: Transformers exhibit a notable property of \emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.

</details>


### [212] [Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift](https://arxiv.org/abs/2512.12816)
*Hasan Burhan Beytur,Gustavo de Veciana,Haris Vikalo,Kevin S Chan*

Main category: cs.LG

TL;DR: 研究概念漂移和预算有限下机器学习模型训练与部署的资源分配，提出框架并给出训练和部署策略。


<details>
  <summary>Details</summary>
Motivation: 解决概念漂移和预算有限情况下，模型提供商为多客户端分配资源以维护模型性能的问题。

Method: 引入模型无关框架，分析概念持续时间老化特性，推导预算约束下的最优训练策略，研究通信约束下的模型部署。

Result: 得出最优训练策略依赖概念持续时间老化特性，证明相关优化问题在温和条件下拟凸，提出随机调度策略实现近最优客户端性能。

Conclusion: 为概念漂移下的机器学习模型管理提供理论和算法基础，对持续学习等有意义。

Abstract: We study how to allocate resources for training and deployment of machine learning (ML) models under concept drift and limited budgets. We consider a setting in which a model provider distributes trained models to multiple clients whose devices support local inference but lack the ability to retrain those models, placing the burden of performance maintenance on the provider. We introduce a model-agnostic framework that captures the interaction between resource allocation, concept drift dynamics, and deployment timing. We show that optimal training policies depend critically on the aging properties of concept durations. Under sudden concept changes, we derive optimal training policies subject to budget constraints when concept durations follow distributions with Decreasing Mean Residual Life (DMRL), and show that intuitive heuristics are provably suboptimal under Increasing Mean Residual Life (IMRL). We further study model deployment under communication constraints, prove that the associated optimization problem is quasi-convex under mild conditions, and propose a randomized scheduling strategy that achieves near-optimal client-side performance. These results offer theoretical and algorithmic foundations for cost-efficient ML model management under concept drift, with implications for continual learning, distributed inference, and adaptive ML systems.

</details>


### [213] [On the continuity of flows](https://arxiv.org/abs/2512.12821)
*Congzhou M Sha*

Main category: cs.LG

TL;DR: 研究流匹配中先验和目标分布拓扑不匹配时速度场的不连续性问题，进行理论分析并实证验证，探讨相关影响。


<details>
  <summary>Details</summary>
Motivation: 探究在流匹配中，先验分布和目标分布拓扑不匹配时，标准流匹配目标下最优速度场的潜在拓扑约束问题。

Method: 对双峰高斯混合进行理论分析，并进行实证验证。

Result: 最优速度场在决策边界处存在跳跃不连续性，且随着时间接近目标分布，不连续幅度趋近无穷；该现象并非特定于L2损失，而是分布拓扑不匹配的结果。

Conclusion: 明确此现象的存在及原因，并讨论了对在流形上进行流匹配的潜在影响，关联到相关工作及神经网络学习不连续表示的挑战。

Abstract: Flow matching has emerged as a powerful framework for generative modeling through continuous normalizing flows. We investigate a potential topological constraint: when the prior distribution and target distribution have mismatched topology (e.g., unimodal to multimodal), the optimal velocity field under standard flow matching objectives may exhibit spatial discontinuities. We suggest that this discontinuity arises from the requirement that continuous flows must bifurcate to map a single mode to multiple modes, forcing particles to make discrete routing decisions at intermediate times. Through theoretical analysis on bimodal Gaussian mixtures, we demonstrate that the optimal velocity field exhibits jump discontinuities along decision boundaries, with magnitude approaching infinity as time approaches the target distribution. Our analysis suggests that this phenomenon is not specific to $L^2$ loss, but rather may be a consequence of topological mismatch between distributions. We validate our theory empirically and discuss potential implications for flow matching on manifolds, connecting our findings to recent work on Riemannian flow matching and the challenge of learning discontinuous representations in neural networks.

</details>


### [214] [GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients](https://arxiv.org/abs/2512.12827)
*Mohammad Mahdi Razmjoo,Mohammad Mahdi Sharifian,Saeed Bagheri Shouraki*

Main category: cs.LG

TL;DR: 本文研究模型输入损失景观的几何特性，通过分析梯度参数的内在维度检测对抗攻击，在不同场景和数据集上验证方法有效性，检测率高。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络易受对抗扰动影响，在医疗诊断和自动驾驶等应用中，需可靠检测对抗攻击。

Method: 研究模型输入损失景观的几何特性，分析模型梯度参数的内在维度，利用自然数据和对抗数据内在维度差异进行检测。

Result: 在批量和单样本场景中验证方法，在MNIST、SVHN、CIFAR - 10和MS COCO等数据集上表现良好，检测率超92%，优于现有方法。

Conclusion: 几何方法具有鲁棒性，内在维度是跨数据集和攻击策略进行对抗检测的有力特征。

Abstract: Despite their remarkable performance, deep neural networks exhibit a critical vulnerability: small, often imperceptible, adversarial perturbations can lead to drastically altered model predictions. Given the stringent reliability demands of applications such as medical diagnosis and autonomous driving, robust detection of such adversarial attacks is paramount. In this paper, we investigate the geometric properties of a model's input loss landscape. We analyze the Intrinsic Dimensionality (ID) of the model's gradient parameters, which quantifies the minimal number of coordinates required to describe the data points on their underlying manifold. We reveal a distinct and consistent difference in the ID for natural and adversarial data, which forms the basis of our proposed detection method. We validate our approach across two distinct operational scenarios. First, in a batch-wise context for identifying malicious data groups, our method demonstrates high efficacy on datasets like MNIST and SVHN. Second, in the critical individual-sample setting, we establish new state-of-the-art results on challenging benchmarks such as CIFAR-10 and MS COCO. Our detector significantly surpasses existing methods against a wide array of attacks, including CW and AutoAttack, achieving detection rates consistently above 92\% on CIFAR-10. The results underscore the robustness of our geometric approach, highlighting that intrinsic dimensionality is a powerful fingerprint for adversarial detection across diverse datasets and attack strategies.

</details>


### [215] [Network Level Evaluation of Hangup Susceptibility of HRGCs using Deep Learning and Sensing Techniques: A Goal Towards Safer Future](https://arxiv.org/abs/2512.12832)
*Kaustav Chatterjee,Joshua Li,Kundan Parajulee,Jared Schwennesen*

Main category: cs.LG

TL;DR: 本文开发了一个评估公路铁路平面交叉路口（HRGCs）悬挂易发性的框架，结合数据收集、深度学习模型和车辆维度场景分析，确定高风险路口并开发了支持工具。


<details>
  <summary>Details</summary>
Motivation: 陡剖面的HRGCs对低离地间隙车辆存在安全隐患，需进行网络级评估。

Method: 收集OK地区不同交叉路口的剖面数据，用混合深度学习模型重建HRGCs剖面，收集车辆维度数据，使用三种车辆维度场景分析悬挂易发性。

Result: 三种场景下分别有36、62和67个交叉路口处于最高悬挂风险水平。

Conclusion: 该框架将下一代传感、深度学习和基础设施数据集集成到实用决策支持工具中，推进了安全评估。

Abstract: Steep profiled Highway Railway Grade Crossings (HRGCs) pose safety hazards to vehicles with low ground clearance, which may become stranded on the tracks, creating risks of train vehicle collisions. This research develops a framework for network level evaluation of hangup susceptibility of HRGCs. Profile data from different crossings in Oklahoma were collected using both a walking profiler and the Pave3D8K Laser Imaging System. A hybrid deep learning model, combining Long Short Term Memory (LSTM) and Transformer architectures, was developed to reconstruct accurate HRGC profiles from Pave3D8K Laser Imaging System data. Vehicle dimension data from around 350 specialty vehicles were collected at various locations across Oklahoma to enable up to date statistical design dimensions. Hangup susceptibility was analyzed using three vehicle dimension scenarios (a) median dimension (median wheelbase and ground clearance), (b) 75 25 percentile dimension (75 percentile wheelbase, 25 percentile ground clearance), and (c) worst case dimension (maximum wheelbase and minimum ground clearance). Results indicate 36, 62, and 67 crossings at the highest hangup risk levels under these scenarios, respectively. An ArcGIS database and a software interface were developed to support transportation agencies in mitigating crossing hazards. This framework advances safety evaluation by integrating next generation sensing, deep learning, and infrastructure datasets into practical decision support tools.

</details>


### [216] [PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks](https://arxiv.org/abs/2512.12840)
*Sindhuja Madabushi,Ahmad Faraz Khan,Haider Ali,Ananthram Swami,Rui Ning,Hongyi Wu,Jin-Hee Cho*

Main category: cs.LG

TL;DR: 提出PRIVEE防御机制保护纵向联邦学习隐私，在不降低模型准确率下提升隐私保护。


<details>
  <summary>Details</summary>
Motivation: 纵向联邦学习易受特征推理攻击，对抗方利用共享置信分数重构其他参与方的私有输入特征。

Method: 提出PRIVEE机制，混淆置信分数，仅共享转换后的表示。

Result: 与现有技术相比，PRIVEE在隐私保护上有三倍提升，且在面对高级特征推理攻击时保持完整预测性能。

Conclusion: PRIVEE能有效保护纵向联邦学习的隐私，同时不影响模型预测准确性。

Abstract: Vertical Federated Learning (VFL) enables collaborative model training across organizations that share common user samples but hold disjoint feature spaces. Despite its potential, VFL is susceptible to feature inference attacks, in which adversarial parties exploit shared confidence scores (i.e., prediction probabilities) during inference to reconstruct private input features of other participants. To counter this threat, we propose PRIVEE (PRIvacy-preserving Vertical fEderated lEarning), a novel defense mechanism named after the French word privée, meaning "private." PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances. Rather than exposing raw scores, PRIVEE shares only the transformed representations, mitigating the risk of reconstruction attacks without degrading model prediction accuracy. Extensive experiments show that PRIVEE achieves a threefold improvement in privacy protection compared to state-of-the-art defenses, while preserving full predictive performance against advanced feature inference attacks.

</details>


### [217] [Selective Conformal Risk Control](https://arxiv.org/abs/2512.12844)
*Yunpeng Xu,Wenge Guo,Zhi Wei*

Main category: cs.LG

TL;DR: 提出Selective Conformal Risk Control (SCRC)框架解决共形预测预测集过大问题，开发两种算法，实验显示能达目标覆盖和风险水平，证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 共形预测常产生过大预测集，限制其实用性，需解决此问题以实现可靠的不确定性量化。

Method: 提出SCRC框架，将共形预测与选择性分类结合，分两阶段控制不确定性，开发SCRC-T和SCRC-I两种算法。

Result: 在两个公开数据集上实验，两种方法都达到目标覆盖和风险水平，性能相近，SCRC-I风险控制稍保守但计算更实用。

Conclusion: 选择性共形风险控制为紧凑、可靠的不确定性量化提供了有效且高效的途径。

Abstract: Reliable uncertainty quantification is essential for deploying machine learning systems in high-stakes domains. Conformal prediction provides distribution-free coverage guarantees but often produces overly large prediction sets, limiting its practical utility. To address this issue, we propose \textit{Selective Conformal Risk Control} (SCRC), a unified framework that integrates conformal prediction with selective classification. The framework formulates uncertainty control as a two-stage problem: the first stage selects confident samples for prediction, and the second stage applies conformal risk control on the selected subset to construct calibrated prediction sets. We develop two algorithms under this framework. The first, SCRC-T, preserves exchangeability by computing thresholds jointly over calibration and test samples, offering exact finite-sample guarantees. The second, SCRC-I, is a calibration-only variant that provides PAC-style probabilistic guarantees while being more computational efficient. Experiments on two public datasets show that both methods achieve the target coverage and risk levels, with nearly identical performance, while SCRC-I exhibits slightly more conservative risk control but superior computational practicality. Our results demonstrate that selective conformal risk control offers an effective and efficient path toward compact, reliable uncertainty quantification.

</details>


### [218] [Information-Consistent Language Model Recommendations through Group Relative Policy Optimization](https://arxiv.org/abs/2512.12858)
*Sonal Prabhune,Balaji Padmanabhan,Kaushik Dutta*

Main category: cs.LG

TL;DR: 论文提出基于GRPO的强化学习框架优化大语言模型信息一致性，实验表明该模型在减少变异性上更有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在关键业务领域存在提示语表述差异导致输出不一致的问题，影响信任、合规和用户体验，现有方法无法保证等效提示的稳定性。

Method: 提出基于GRPO的强化学习框架，引入基于熵的有用性和稳定性奖励，将提示变体分组并重置对话上下文。

Result: 在投资和职位推荐任务实验中，GRPO训练的模型比微调或基于解码的基线模型更能有效减少变异性。

Conclusion: 这是GRPO在使大语言模型信息一致方面的新应用，将变异性视为企业部署中可纠正的缺陷。

Abstract: Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.

</details>


### [219] [Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels](https://arxiv.org/abs/2512.12870)
*Pouya Ahadi,Blair Winograd,Camille Zaug,Karunesh Arora,Lijun Wang,Kamran Paynabar*

Main category: cs.LG

TL;DR: 提出新颖主动学习框架，通过减少噪声构建鲁棒分类模型，实验显示优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 主动学习中标签含噪声，学习不完美标签数据会导致分类器不准确，需构建鲁棒分类模型。

Method: 提出含最优分配查询点到标注员的分配模型，引入新采样方法识别最佳查询点。

Result: 实验表明该方法相比多个基准方法显著提升了分类性能。

Conclusion: 所提新颖主动学习框架能有效减少噪声，提升分类性能。

Abstract: Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.

</details>


### [220] [Improving Recursive Transformers with Mixture of LoRAs](https://arxiv.org/abs/2512.12880)
*Mohammadmahdi Nouriborji,Morteza Rohanian,Omid Rohanian*

Main category: cs.LG

TL;DR: 提出Mixture of LoRAs (MoL)机制，预训练ModernALBERT，在多任务上表现佳，还提出专家合并程序，证明条件权重空间调制可恢复递归变压器参数共享损失的表达能力。


<details>
  <summary>Details</summary>
Motivation: 递归变压器的参数共享会减少模型大小，但会降低层的表达能力。

Method: 提出MoL机制，在共享前馈网络中插入低秩自适应专家；预训练集成多种技术的ModernALBERT；提出专家合并程序。

Result: ModernALBERT (50M - 120M)在GLUE、SQuAD - v2和BEIR上达到紧凑模型的最优性能，超越更大的全参数基线；专家合并程序能在推理时将MoL压缩为单个适配器并保留准确性。

Conclusion: 条件权重空间调制能有效恢复递归变压器在激进参数共享下损失的表达能力。

Abstract: Parameter sharing in recursive transformers reduces model size but collapses layer-wise expressivity. We propose Mixture of LoRAs (MoL), a lightweight conditional-computation mechanism that inserts Low-Rank Adaptation (LoRA) experts inside a shared feed-forward network (FFN). MoL enables token-conditional weight-space modulation of the shared FFN without untying backbone parameters, unlike prior approaches that add fixed or externally attached adapters. We pretrain a modernised recursive architecture, ModernALBERT, integrating rotary embeddings, GeGLU, FlashAttention, and a distillation-based initialisation. Across GLUE, SQuAD-v2, and BEIR, ModernALBERT (50M--120M) achieves state-of-the-art performance among compact models and surpasses larger fully parameterised baselines. We also propose an expert-merging procedure that compresses MoL into a single adapter at inference while preserving accuracy, enabling efficient deployment. Our results show that conditional weight-space modulation effectively restores the expressivity lost under aggressive parameter sharing in recursive transformers.

</details>


### [221] [Distillation of Discrete Diffusion by Exact Conditional Distribution Matching](https://arxiv.org/abs/2512.12889)
*Yansong Gao,Yu Sun*

Main category: cs.LG

TL;DR: 本文提出基于条件分布匹配的蒸馏方法加速离散扩散模型推理。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型推理成本高，现有加速方法有局限性。

Method: 利用逆向条件分布的马尔可夫分解结构，定义蒸馏目标匹配预训练教师模型和低函数评估次数学生模型的条件分布。

Result: 提出简单且有理论依据的蒸馏替代方法。

Conclusion: 所提方法可用于一步和少步采样器，为离散扩散模型加速提供新途径。

Abstract: Discrete diffusion models (DDMs) are a powerful class of generative models for categorical data, but they typically require many function evaluations for a single sample, making inference expensive. Existing acceleration methods either rely on approximate simulators, such as $τ$-leaping, or on distillation schemes that train new student models and auxiliary networks with proxy objectives. We propose a simple and principled distillation alternative based on \emph{conditional distribution matching}. Our key observation is that the reverse conditional distribution of clean data given a noisy state, $p_{0\mid t}(x_0 \mid x_t)$, admits a Markov decomposition through intermediate times and can be recovered from marginal density ratios and the known forward CTMC kernel. We exploit this structure to define distillation objectives that directly match conditional distributions between a pre-trained teacher and a low-NFE student, both for one-step and few-step samplers.

</details>


### [222] [Wait, Wait, Wait... Why Do Reasoning Models Loop?](https://arxiv.org/abs/2512.12895)
*Charilaos Pipis,Shivam Garg,Vasilis Kontonis,Vaishnavi Shrivastava,Akshay Krishnamurthy,Dimitris Papailiopoulos*

Main category: cs.LG

TL;DR: 研究推理模型循环问题，发现低温下常见，源于学习误差，介绍两种循环机制及温度作用，并探讨训练干预。


<details>
  <summary>Details</summary>
Motivation: 解决推理模型在低温或贪心解码时出现循环的问题，研究循环原因及温度作用。

Method: 使用开放推理模型研究循环情况，引入合成图推理任务演示循环机制。

Result: 发现低温下循环常见，大模型循环少，蒸馏学生模型易循环；揭示两种循环机制；高温可减少循环但不解决学习误差。

Conclusion: 温度只是权宜之计，应进行训练时干预直接减少学习误差。

Abstract: Reasoning models (e.g., DeepSeek-R1) generate long chains of thought to solve harder problems, but they often loop, repeating the same text at low temperatures or with greedy decoding. We study why this happens and what role temperature plays. With open reasoning models, we find that looping is common at low temperature. Larger models tend to loop less, and distilled students loop significantly even when their teachers rarely do. This points to mismatches between the training distribution and the learned model, which we refer to as errors in learning, as a key cause. To understand how such errors cause loops, we introduce a synthetic graph reasoning task and demonstrate two mechanisms. First, risk aversion caused by hardness of learning: when the correct progress-making action is hard to learn but an easy cyclic action is available, the model puts relatively more probability on the cyclic action and gets stuck. Second, even when there is no hardness, Transformers show an inductive bias toward temporally correlated errors, so the same few actions keep being chosen and loops appear. Higher temperature reduces looping by promoting exploration, but it does not fix the errors in learning, so generations remain much longer than necessary at high temperature; in this sense, temperature is a stopgap rather than a holistic solution. We end with a discussion of training-time interventions aimed at directly reducing errors in learning.

</details>


### [223] [Probability Estimation for Predicted-Occupancy Grids in Vehicle Safety Applications Based on Machine Learning](https://arxiv.org/abs/2512.12896)
*Parthasarathy Nadarajan,Michael Botsch*

Main category: cs.LG

TL;DR: 本文提出预测多对象复杂交通场景演变的方法，先介绍基于模型的方法，因计算量大采用机器学习方法，模拟对比结果有前景可用于车辆安全应用。


<details>
  <summary>Details</summary>
Motivation: 预测复杂交通场景演变，详细建模交通参与者行为的不确定性，改进车辆安全系统关键组件。

Method: 先采用基于模型的方法计算预测占用网格（POG），后采用基于随机森林算法的机器学习方法，用新颖的网格表示交通场景当前状态并映射到POG。

Result: 通过交通场景模拟对比机器学习和基于模型的方法，结果有前景，可实现车辆安全应用中POG的实时计算。

Conclusion: 详细建模不确定性可改进车辆安全系统的关键组件，如临界性估计和轨迹规划。

Abstract: This paper presents a method to predict the evolution of a complex traffic scenario with multiple objects. The current state of the scenario is assumed to be known from sensors and the prediction is taking into account various hypotheses about the behavior of traffic participants. This way, the uncertainties regarding the behavior of traffic participants can be modelled in detail. In the first part of this paper a model-based approach is presented to compute Predicted-Occupancy Grids (POG), which are introduced as a grid-based probabilistic representation of the future scenario hypotheses. However, due to the large number of possible trajectories for each traffic participant, the model-based approach comes with a very high computational load. Thus, a machine-learning approach is adopted for the computation of POGs. This work uses a novel grid-based representation of the current state of the traffic scenario and performs the mapping to POGs. This representation consists of augmented cells in an occupancy grid. The adopted machine-learning approach is based on the Random Forest algorithm. Simulations of traffic scenarios are performed to compare the machine-learning with the model-based approach. The results are promising and could enable the real-time computation of POGs for vehicle safety applications. With this detailed modelling of uncertainties, crucial components in vehicle safety systems like criticality estimation and trajectory planning can be improved.

</details>


### [224] [Predicted-occupancy grids for vehicle safety applications based on autoencoders and the Random Forest algorithm](https://arxiv.org/abs/2512.12901)
*Parthasarathy Nadarajan,Michael Botsch,Sebastian Sardina*

Main category: cs.LG

TL;DR: 本文用机器学习算法预测复杂交通场景的概率时空表示，经仿真和实车实验验证性能良好，并展示其应用。


<details>
  <summary>Details</summary>
Motivation: 复杂交通场景下动态机动时，概率时空表示对主动车辆安全应用有重要意义。

Method: 用分层情景分类器区分交通场景类型，训练随机森林预测概率时空表示（POG），用堆叠去噪自编码器（SDA）将增强占用网格（AOG）降维以提高学习精度。

Result: 仿真和实车实验证明由SDA和随机森林组成的机器学习方法性能出色。

Conclusion: 所提方法有效，还展示了POG在评估交通场景临界性和确定安全轨迹方面的应用。

Abstract: In this paper, a probabilistic space-time representation of complex traffic scenarios is predicted using machine learning algorithms. Such a representation is significant for all active vehicle safety applications especially when performing dynamic maneuvers in a complex traffic scenario. As a first step, a hierarchical situation classifier is used to distinguish the different types of traffic scenarios. This classifier is responsible for identifying the type of the road infrastructure and the safety-relevant traffic participants of the driving environment. With each class representing similar traffic scenarios, a set of Random Forests (RFs) is individually trained to predict the probabilistic space-time representation, which depicts the future behavior of traffic participants. This representation is termed as a Predicted-Occupancy Grid (POG). The input to the RFs is an Augmented Occupancy Grid (AOG). In order to increase the learning accuracy of the RFs and to perform better predictions, the AOG is reduced to low-dimensional features using a Stacked Denoising Autoencoder (SDA). The excellent performance of the proposed machine learning approach consisting of SDAs and RFs is demonstrated in simulations and in experiments with real vehicles. An application of POGs to estimate the criticality of traffic scenarios and to determine safe trajectories is also presented.

</details>


### [225] [Next-generation reservoir computing validated by classification task](https://arxiv.org/abs/2512.12903)
*Ken-ichi Kitayama*

Main category: cs.LG

TL;DR: 研究下一代储层计算（NG - RC），首次证明其在分类任务表现与传统RC相当，验证其多功能计算能力。


<details>
  <summary>Details</summary>
Motivation: 以往NG - RC基准测试局限于时间波形预测任务，要证实其在分类任务上的能力。

Method: 对NG - RC进行分类任务测试。

Result: NG - RC在分类任务上表现与传统RC一样好。

Conclusion: NG - RC在预测和分类任务上都有通用计算能力。

Abstract: An emerging computing paradigm, so-called next-generation reservoir computing (NG-RC) is investigated. True to its namesake, NG-RC requires no actual reservoirs for input data mixing but rather computing the polynomial terms directly from the time series inputs. However, benchmark tests so far reported have been one-sided, limited to prediction tasks of temporal waveforms such as Lorenz 63 attractor and Mackey-Glass chaotic signal. We will demonstrate for the first time that NG-RC can perform classification task as good as conventional RC. This validates the versatile computational capability of NG-RC in tasks of both prediction and classification.

</details>


### [226] [Machine Learning Architectures for the Estimation of Predicted Occupancy Grids in Road Traffic](https://arxiv.org/abs/2512.12907)
*Parthasarathy Nadarajan,Michael Botsch,Sebastian Sardina*

Main category: cs.LG

TL;DR: 提出新机器学习架构用于高效估计复杂交通场景概率时空表示，与其他架构对比并验证。


<details>
  <summary>Details</summary>
Motivation: 详细的未来交通场景表示对自动驾驶和主动安全系统很重要，需预测交通场景未来时空表示。

Method: 先识别交通场景类型，用机器学习算法将当前场景状态映射到未来状态，输入为AOG，输出为POG，采用两个SDAs和一组随机森林组成的新架构，并与含SDAs和DeconvNet的架构对比。

Result: 通过模拟验证架构，从准确性和计算时间进行比较。

Conclusion: 文中还介绍了POG在主动安全领域的应用。

Abstract: This paper introduces a novel machine learning architecture for an efficient estimation of the probabilistic space-time representation of complex traffic scenarios. A detailed representation of the future traffic scenario is of significant importance for autonomous driving and for all active safety systems. In order to predict the future space-time representation of the traffic scenario, first the type of traffic scenario is identified and then the machine learning algorithm maps the current state of the scenario to possible future states. The input to the machine learning algorithms is the current state representation of a traffic scenario, termed as the Augmented Occupancy Grid (AOG). The output is the probabilistic space-time representation which includes uncertainties regarding the behaviour of the traffic participants and is termed as the Predicted Occupancy Grid (POG). The novel architecture consists of two Stacked Denoising Autoencoders (SDAs) and a set of Random Forests. It is then compared with the other two existing architectures that comprise of SDAs and DeconvNet. The architectures are validated with the help of simulations and the comparisons are made both in terms of accuracy and computational time. Also, a brief overview on the applications of POGs in the field of active safety is presented.

</details>


### [227] [LLM-based Personalized Portfolio Recommender: Integrating Large Language Models and Reinforcement Learning for Intelligent Investment Strategy Optimization](https://arxiv.org/abs/2512.12922)
*Bangyu Li,Boping Gu,Ziyang Ding*

Main category: cs.LG

TL;DR: 本文提出基于大语言模型的个性化投资组合推荐框架以支持智能投资决策。


<details>
  <summary>Details</summary>
Motivation: 现代金融市场中投资者需个性化、适应性投资组合策略，传统方法无法捕捉非线性交互，存在局限。

Method: 引入基于大语言模型、强化学习和个性化风险偏好建模的集成框架。

Result: 未提及

Conclusion: 未提及

Abstract: In modern financial markets, investors increasingly seek personalized and adaptive portfolio strategies that reflect their individual risk preferences and respond to dynamic market conditions. Traditional rule-based or static optimization approaches often fail to capture the nonlinear interactions among investor behavior, market volatility, and evolving financial objectives. To address these limitations, this paper introduces the LLM-based Personalized Portfolio Recommender , an integrated framework that combines Large Language Models, reinforcement learning, and individualized risk preference modeling to support intelligent investment decision-making.

</details>


### [228] [SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision](https://arxiv.org/abs/2512.12930)
*Yuseon Choi,Sangjin Kim,Jungjun Oh,Byeongcheol Kim,Hoi-Jun Yoo*

Main category: cs.LG

TL;DR: 提出高效节能的SeVeDo加速器解决低比特量化难题，实验显示能效超传统设计。


<details>
  <summary>Details</summary>
Motivation: 现有低比特量化方法因激活异常值致精度下降且能耗高，需新方法。

Method: 提出SeVeDo加速器，用SVD分离组件；采用HGQ降低反量化成本；用SVD - MP分配位宽降低浮点运算成本。

Result: SeVeDo峰值能效达13.8TOPS/W，在ViT - Base和Llama2 - 7B基准测试中分别达12.7TOPS/W和13.4TOPS/W。

Conclusion: SeVeDo在低比特量化中能效表现优于传统设计。

Abstract: Low-bit quantization is a promising technique for efficient transformer inference by reducing computational and memory overhead. However, aggressive bitwidth reduction remains challenging due to activation outliers, leading to accuracy degradation. Existing methods, such as outlier-handling and group quantization, achieve high accuracy but incur substantial energy consumption. To address this, we propose SeVeDo, an energy-efficient SVD-based heterogeneous accelerator that structurally separates outlier-sensitive components into a high-precision low-rank path, while the remaining computations are executed in a low-bit residual datapath with group quantization. To further enhance efficiency, Hierarchical Group Quantization (HGQ) combines coarse-grained floating-point scaling with fine-grained shifting, effectively reducing dequantization cost. Also, SVD-guided mixed precision (SVD-MP) statically allocates higher bitwidths to precision-sensitive components identified through low-rank decomposition, thereby minimizing floating-point operation cost. Experimental results show that SeVeDo achieves a peak energy efficiency of 13.8TOPS/W, surpassing conventional designs, with 12.7TOPS/W on ViT-Base and 13.4TOPS/W on Llama2-7B benchmarks.

</details>


### [229] [Application of Deep Learning in Biological Data Compression](https://arxiv.org/abs/2512.12975)
*Chunyu Zou*

Main category: cs.LG

TL;DR: 本文研究用深度学习尤其是隐式神经表示（INR）压缩低温电子显微镜（Cryo - EM）生物数据，给出了压缩方法并旨在提供实用高效压缩方案。


<details>
  <summary>Details</summary>
Motivation: Cryo - EM数据文件存储占用空间大，给研究人员和教育工作者带来挑战。

Method: 先根据密度阈值提取每个文件的二进制图，用GZIP压缩高度重复的密度图；训练神经网络编码空间密度信息，存储网络参数和可学习的潜在向量；加入位置编码增强空间表示，使用加权均方误差损失函数平衡密度分布变化。

Result: 未提及具体结果。

Conclusion: 旨在提供实用高效、能兼顾压缩比和重建质量的生物数据压缩方案，用于教育和研究。

Abstract: Cryogenic electron microscopy (Cryo-EM) has become an essential tool for capturing high-resolution biological structures. Despite its advantage in visualizations, the large storage size of Cryo-EM data file poses significant challenges for researchers and educators. This paper investigates the application of deep learning, specifically implicit neural representation (INR), to compress Cryo-EM biological data. The proposed approach first extracts the binary map of each file according to the density threshold. The density map is highly repetitive, ehich can be effectively compressed by GZIP. The neural network then trains to encode spatial density information, allowing the storage of network parameters and learnable latent vectors. To improve reconstruction accuracy, I further incorporate the positional encoding to enhance spatial representation and a weighted Mean Squared Error (MSE) loss function to balance density distribution variations. Using this approach, my aim is to provide a practical and efficient biological data compression solution that can be used for educational and research purpose, while maintaining a reasonable compression ratio and reconstruction quality from file to file.

</details>


### [230] [Deep Learning-Driven Inversion Framework for Shear Modulus Estimation in Magnetic Resonance Elastography (DIME)](https://arxiv.org/abs/2512.13010)
*Hassan Iftikhar,Rizwan Ahmad,Arunark Kolipaka*

Main category: cs.LG

TL;DR: 提出用于磁共振弹性成像（MRE）中剪切模量估计的深度学习反演框架DIME，经多组数据验证其比MMDI算法更优，有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有MMDI算法依赖假设且对噪声敏感，影响刚度估计准确性和可靠性，需提升反演鲁棒性。

Method: 通过有限元建模（FEM）模拟生成位移场 - 刚度图对训练DIME，在小图像块上训练以捕捉局部波行为和提高对全局图像变化的鲁棒性。

Result: 在多组模拟数据和体内肝脏MRE数据中，DIME生成的刚度图像素间变异性低、边界描绘准确、与真值相关性高，比MMDI表现更好。

Conclusion: DIME在MRE临床应用中具有可行性。

Abstract: The Multimodal Direct Inversion (MMDI) algorithm is widely used in Magnetic Resonance Elastography (MRE) to estimate tissue shear stiffness. However, MMDI relies on the Helmholtz equation, which assumes wave propagation in a uniform, homogeneous, and infinite medium. Furthermore, the use of the Laplacian operator makes MMDI highly sensitive to noise, which compromises the accuracy and reliability of stiffness estimates. In this study, we propose the Deep-Learning driven Inversion Framework for Shear Modulus Estimation in MRE (DIME), aimed at enhancing the robustness of inversion. DIME is trained on the displacement fields-stiffness maps pair generated through Finite Element Modelling (FEM) simulations. To capture local wave behavior and improve robustness to global image variations, DIME is trained on small image patches. We first validated DIME using homogeneous and heterogeneous datasets simulated with FEM, where DIME produced stiffness maps with low inter-pixel variability, accurate boundary delineation, and higher correlation with ground truth (GT) compared to MMDI. Next, DIME was evaluated in a realistic anatomy-informed simulated liver dataset with known GT and compared directly to MMDI. DIME reproduced ground-truth stiffness patterns with high fidelity (r = 0.99, R^2 = 0.98), while MMDI showed greater underestimation. After validating DIME on synthetic data, we tested the model in in vivo liver MRE data from eight healthy and seven fibrotic subjects. DIME preserved physiologically consistent stiffness patterns and closely matched MMDI, which showed directional bias. Overall, DIME showed higher correlation with ground truth and visually similar stiffness patterns, whereas MMDI displayed a larger bias that can potentially be attributed to directional filtering. These preliminary results highlight the feasibility of DIME for clinical applications in MRE.

</details>


### [231] [Scaling Bidirectional Spans and Span Violations in Attention Mechanism](https://arxiv.org/abs/2512.13033)
*Jongwook Kim,Sangheon Yun,Sukjin Yoon*

Main category: cs.LG

TL;DR: 提出优化框架分解梯度，实验证明标准注意力梯度次优，在WikiText - 2上验证框架有效性。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer训练中的几何低效问题，进一步优化其训练。

Method: 利用不对称投影将反向传播梯度分解为并行跨度和正交违规，同时保持前向传播QKV结构不变，选择性缩放组件。

Result: 在WikiText - 2数据集上，使用粗略配置使验证损失降低0.56%。

Conclusion: 框架基本有效，在更大数据集和深度训练中有显著潜在收益。

Abstract: The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes

</details>


### [232] [Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization](https://arxiv.org/abs/2512.13034)
*Xiaoyu He,Yu Cai,Jin Jia,Canxi Huang,Wenqing Chen,Zibin Zheng*

Main category: cs.LG

TL;DR: 提出自适应动量方法Alada优化大规模矩阵，具低内存开销和良好性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模矩阵随机优化中传统方法内存开销大等问题。

Method: 采用秩一分解方法估计梯度二阶矩，交替更新因子，还配备一阶矩估计规则。

Result: 理论性能与传统方法相当，在自然语言处理任务中减少内存开销，训练大模型更稳健。

Conclusion: Alada是大规模矩阵随机优化的有效方法，有低内存开销和强鲁棒性。

Abstract: This work proposes Alada, an adaptive momentum method for stochastic optimization over large-scale matrices. Alada employs a rank-one factorization approach to estimate the second moment of gradients, where factors are updated alternatively to minimize the estimation error. Alada achieves sublinear memory overheads and can be readily extended to optimizing tensor-shaped variables.We also equip Alada with a first moment estimation rule, which enhances the algorithm's robustness without incurring additional memory overheads. The theoretical performance of Alada aligns with that of traditional methods such as Adam. Numerical studies conducted on several natural language processing tasks demonstrate the reduction in memory overheads and the robustness in training large models relative to Adam and its variants.

</details>


### [233] [Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection](https://arxiv.org/abs/2512.13040)
*Xuwei Tan,Yao Ma,Xueru Zhang*

Main category: cs.LG

TL;DR: 本文提出FinFRE - RAG方法用于金融交易欺诈检测，可改善大语言模型在该任务上的表现并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统表格模型欺诈检测需大量特征工程且可解释性差，大语言模型直接用于表格欺诈检测效果不佳，需改进方法。

Method: 引入FinFRE - RAG的两阶段方法，进行重要性引导的特征缩减，将属性序列化为自然语言，并在标签感知的实例级范例上进行检索增强的上下文学习。

Result: 在四个公共欺诈数据集和三类开放权重的大语言模型上，FinFRE - RAG显著提高了F1/MCC，在多个设置中与强表格基线模型具有竞争力。

Conclusion: 大语言模型虽仍落后于专业分类器，但能缩小性能差距并提供可解释理由，可作为欺诈分析的辅助工具。

Abstract: Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.

</details>


### [234] [Deep Q-Learning-Based Intelligent Scheduling for ETL Optimization in Heterogeneous Data Environments](https://arxiv.org/abs/2512.13060)
*Kangning Gao,Yi Hu,Cong Nie,Wei Li*

Main category: cs.LG

TL;DR: 提出基于深度Q学习的智能调度优化框架解决ETL过程调度问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决异构数据环境下ETL过程调度效率低、资源分配不均衡和适应性差的问题。

Method: 将ETL调度过程形式化为马尔可夫决策过程，用强化学习智能体在高维状态空间决策；模型含状态表示、特征嵌入等模块；设计多目标奖励函数；进行敏感性实验。

Result: 该框架显著减少调度延迟、提高系统吞吐量和执行稳定性。

Conclusion: 强化学习在复杂数据调度和资源管理有强大潜力，为智能数据管道建设提供高效可扩展策略。

Abstract: This paper addresses the challenges of low scheduling efficiency, unbalanced resource allocation, and poor adaptability in ETL (Extract-Transform-Load) processes under heterogeneous data environments by proposing an intelligent scheduling optimization framework based on deep Q-learning. The framework formalizes the ETL scheduling process as a Markov Decision Process and enables adaptive decision-making by a reinforcement learning agent in high-dimensional state spaces to dynamically optimize task allocation and resource scheduling. The model consists of a state representation module, a feature embedding network, a Q-value estimator, and a reward evaluation mechanism, which collectively consider task dependencies, node load states, and data flow characteristics to derive the optimal scheduling strategy in complex environments. A multi-objective reward function is designed to balance key performance indicators such as average scheduling delay, task completion rate, throughput, and resource utilization. Sensitivity experiments further verify the model's robustness under changes in hyperparameters, environmental dynamics, and data scale. Experimental results show that the proposed deep Q-learning scheduling framework significantly reduces scheduling delay, improves system throughput, and enhances execution stability under multi-source heterogeneous task conditions, demonstrating the strong potential of reinforcement learning in complex data scheduling and resource management, and providing an efficient and scalable optimization strategy for intelligent data pipeline construction.

</details>


### [235] [LikeBench: Evaluating Subjective Likability in LLMs for Personalization](https://arxiv.org/abs/2512.13077)
*Md Awsafur Rahman,Adam Gabrys,Doug Kang,Jingjing Sun,Tian Tan,Ashwin Chandramouli*

Main category: cs.LG

TL;DR: 提出新评估框架LikeBench衡量大语言模型的用户偏好响应的可爱度，分解可爱度为多指标，发现强记忆性能不保证高可爱度。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型个性化基准未充分衡量用户体验核心的可爱度维度。

Method: 引入多会话、动态评估框架LikeBench，让模型与模拟用户对话，按七个维度评估可爱度，使用细粒度心理描述性角色。

Result: DeepSeek R1记忆准确率低但可爱度得分超Qwen3，GPT - 5在长且嘈杂交互中鲁棒性有限。

Conclusion: 强记忆性能不保证高可爱度，现有SOTA模型在复杂交互中存在局限。

Abstract: A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.

</details>


### [236] [TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning](https://arxiv.org/abs/2512.13106)
*Shenzhi Yang,Guangcheng Zhu,Xing Zheng,Yingfan MA,Zhongqi Chen,Bowen Song,Weiqiang Wang,Junbo Zhao,Gang Chen,Haobo Wang*

Main category: cs.LG

TL;DR: 本文提出半监督RLVR范式的TraPO算法，在多个推理基准测试上表现出色，数据效率高，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法标注成本高，无监督方法后期训练易模型崩溃，需探索更好方法。

Method: 提出半监督RLVR范式，用少量标注数据集指导无标注样本的训练，提出TraPO算法匹配学习轨迹相似度来识别可靠无标注样本。

Result: 在六个数学推理基准和三个分布外任务上验证了效果，少量样本下准确率超无监督方法，使用少量标注数据时超全监督模型。

Conclusion: 半监督RLVR范式有效，TraPO算法数据效率高、泛化性强。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.

</details>


### [237] [From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network](https://arxiv.org/abs/2512.13111)
*Hayk Amirkhanian,Marco F. Huber*

Main category: cs.LG

TL;DR: 本文提出Hierarchical Approximate Bayesian Neural Network（HABNN）应对神经网络的挑战，有良好性能并超现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络面临超参数调整和过拟合难题，贝叶斯神经网络可应对，但需更好方法提升模型鲁棒性与性能。

Method: 采用高斯 - 逆威沙特分布作为网络权重的超先验，给出预测分布和权重后验的解析表示。

Result: 方法表现稳健，有效解决过拟合问题，提供可靠不确定性估计，实验中常超现有模型。

Conclusion: HABNN是安全关键环境未来应用的一个有前景方向。

Abstract: In recent years, neural networks have revolutionized various domains, yet challenges such as hyperparameter tuning and overfitting remain significant hurdles. Bayesian neural networks offer a framework to address these challenges by incorporating uncertainty directly into the model, yielding more reliable predictions, particularly for out-of-distribution data. This paper presents Hierarchical Approximate Bayesian Neural Network, a novel approach that uses a Gaussian-inverse-Wishart distribution as a hyperprior of the network's weights to increase both the robustness and performance of the model. We provide analytical representations for the predictive distribution and weight posterior, which amount to the calculation of the parameters of Student's t-distributions in closed form with linear complexity with respect to the number of weights. Our method demonstrates robust performance, effectively addressing issues of overfitting and providing reliable uncertainty estimates, particularly for out-of-distribution tasks. Experimental results indicate that HABNN not only matches but often outperforms state-of-the-art models, suggesting a promising direction for future applications in safety-critical environments.

</details>


### [238] [Quanvolutional Neural Networks for Spectrum Peak-Finding](https://arxiv.org/abs/2512.13125)
*Lukas Bischof,Rudolf M. Füchslin,Kurt Stockinger,Pavel Sulimov*

Main category: cs.LG

TL;DR: 本文探索使用量子卷积神经网络（QuanvNNs）解决核磁共振波谱多任务峰查找问题，结果表明QuanvNNs在具有挑战性的光谱上优于经典卷积神经网络。


<details>
  <summary>Details</summary>
Motivation: 光谱峰值的综合表征是一项具有挑战性的任务，机器学习技术有一定作用，量子计算有潜力进一步提升，受经典CNN成功的启发，探索QuanvNNs。

Method: 实现一个简单且可解释的QuanvNN架构并与经典CNN对比，在合成的NMR启发数据集上评估性能。

Result: QuanvNNs在具有挑战性的光谱上优于经典CNN，F1分数提高11％，峰位置估计的平均绝对误差降低30％，且在更难问题上收敛稳定性更好。

Conclusion: QuanvNNs在处理光谱的多任务峰查找问题上具有优势，有进一步研究和应用价值。

Abstract: The analysis of spectra, such as Nuclear Magnetic Resonance (NMR) spectra, for the comprehensive characterization of peaks is a challenging task for both experts and machines, especially with complex molecules. This process, also known as deconvolution, involves identifying and quantifying the peaks in the spectrum. Machine learning techniques have shown promising results in automating this process. With the advent of quantum computing, there is potential to further enhance these techniques. In this work, inspired by the success of classical Convolutional Neural Networks (CNNs), we explore the use of Quanvolutional Neural Networks (QuanvNNs) for the multi-task peak finding problem, involving both peak counting and position estimation. We implement a simple and interpretable QuanvNN architecture that can be directly compared to its classical CNN counterpart, and evaluate its performance on a synthetic NMR-inspired dataset. Our results demonstrate that QuanvNNs outperform classical CNNs on challenging spectra, achieving an 11\% improvement in F1 score and a 30\% reduction in mean absolute error for peak position estimation. Additionally, QuanvNNs appear to exhibit better convergence stability for harder problems.

</details>


### [239] [SACn: Soft Actor-Critic with n-step Returns](https://arxiv.org/abs/2512.13165)
*Jakub Łyskawa,Jakub Lewandowski,Paweł Wawrzyński*

Main category: cs.LG

TL;DR: 本文提出将Soft Actor - Critic (SAC)与n步回报结合的方法，克服数值不稳定问题，提出数值稳定的重要性采样和τ - 采样熵估计，实验验证了SACn算法。


<details>
  <summary>Details</summary>
Motivation: SAC难以与n步回报结合，常规结合会引入偏差，重要性采样虽能解决但会导致数值不稳定。

Method: 提出数值稳定、超参选择简化的重要性采样方法；分析SAC在n步最大熵框架下的熵估计方法，提出τ - 采样熵估计以降低学习目标方差；制定SACn算法。

Result: 在MuJoCo模拟环境中进行了实验验证。

Conclusion: 所提出的将SAC与n步回报结合的方法有效解决了数值不稳定问题。

Abstract: Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $τ$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.

</details>


### [240] [PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning](https://arxiv.org/abs/2512.13186)
*Khalid Ferji*

Main category: cs.LG

TL;DR: 介绍PolySet框架，以解决当前聚合物科学中机器学习模型与物理现实的不匹配问题，提高模型学习聚合物性质的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前聚合物科学中的机器学习模型将聚合物视为单一、明确的分子图，与真实材料中聚合物链长度分布的现实不符，限制了模型捕捉聚合物行为的能力。

Method: 引入PolySet框架，将聚合物表示为从假定摩尔质量分布中采样的有限、加权的链集合，使用最小语言模型进行说明。

Result: PolySet保留了高阶分布矩，使机器学习模型能够以更高的稳定性和准确性学习对尾部敏感的性质。

Conclusion: PolySet通过明确考虑聚合物物质的统计性质，为未来的聚合物机器学习建立了基于物理的基础，且易于扩展到共聚物、嵌段结构和其他复杂拓扑。

Abstract: Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.

</details>


### [241] [WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory](https://arxiv.org/abs/2512.13190)
*Jin Sob Kim,Hyun Joon Park,Wooseok Shin,Dongil Park,Sung Won Han*

Main category: cs.LG

TL;DR: 提出差异化方法处理AIS数据，引入WAY架构和GD技术用于船舶目的地估计，实验证明其优势并探索多任务学习应用


<details>
  <summary>Details</summary>
Motivation: 解决AIS数据可靠性和间隔不规则问题，实现船舶目的地估计

Method: 将长轨迹重铸为嵌套序列结构，用空间网格减轻偏差；引入WAY架构，含轨迹表示层和CASP块；提出GD技术

Result: WAY优于传统基于空间网格的方法，采用GD有性能提升

Conclusion: WAY架构和GD技术在船舶目的地估计有效，可探索多任务学习用于ETA估计的实际应用

Abstract: The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.

</details>


### [242] [Noise-Resilient Quantum Aggregation on NISQ for Federated ADAS Learning](https://arxiv.org/abs/2512.13196)
*Chethana Prasad Kabgere,Sudarshan T S B*

Main category: cs.LG

TL;DR: 本文提出噪声弹性量子联邦学习（NR - QFL）框架用于ADAS，经实证验证有良好效果，为量子增强联邦学习奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习聚合易受实时车载网络噪声、延迟和安全约束影响，需要改进。

Method: 引入NR - QFL混合量子 - 经典框架，利用变分量子电路，对模型参数编码，采用量子熵客户端选择和多服务器协调。

Result: 实证显示收敛一致，梯度方差降低，通信开销减少，噪声容忍度增强。

Conclusion: 该框架为量子增强联邦学习建立可扩展基础，实现车载边缘安全、高效和动态稳定的ADAS智能。

Abstract: Advanced Driver Assistance Systems (ADAS) increasingly employ Federated Learning (FL) to collaboratively train models across distributed vehicular nodes while preserving data privacy. Yet, conventional FL aggregation remains susceptible to noise, latency, and security constraints inherent to real-time vehicular networks. This paper introduces Noise-Resilient Quantum Federated Learning (NR-QFL), a hybrid quantum-classical framework that enables secure, low-latency aggregation through variational quantum circuits (VQCs) operating under Noisy Intermediate-Scale Quantum (NISQ) conditions. The framework encodes model parameters as quantum states with adaptive gate reparameterization, ensuring bounded-error convergence and provable resilience under Completely Positive Trace-Preserving (CPTP) dynamics. NR-QFL employs quantum entropy-based client selection and multi-server coordination for fairness and stability. Empirical validation shows consistent convergence with reduced gradient variance, lower communication overhead, and enhanced noise tolerance under constrained edge conditions. The framework establishes a scalable foundation for quantum-enhanced federated learning, enabling secure, efficient, and dynamically stable ADAS intelligence at the vehicular edge.

</details>


### [243] [Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting](https://arxiv.org/abs/2512.13207)
*Karina Chichifoi,Fabio Merizzi,Michele Colajanni*

Main category: cs.LG

TL;DR: 研究联邦学习在气象预报中数据投毒攻击，模拟攻击并评估防御机制。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在气象预报有潜力但分布式特性带来数据投毒等新漏洞，需研究其影响。

Method: 模拟地理分布客户端，对区域温度预报进行基于补丁和全局偏差攻击，评估修剪均值聚合作为防御机制。

Result: 少量投毒客户端能误导大面积预测，全局偏差攻击使预测偏移，补丁攻击使均方误差大增，修剪均值聚合对全局偏差攻击有一定防御效果，对补丁攻击失效。

Conclusion: 基于异常值的防御机制在处理空间相关数据时有局限性。

Abstract: Deep learning and federated learning (FL) are becoming powerful partners for next-generation weather forecasting. Deep learning enables high-resolution spatiotemporal forecasts that can surpass traditional numerical models, while FL allows institutions in different locations to collaboratively train models without sharing raw data, addressing efficiency and security concerns. While FL has shown promise across heterogeneous regions, its distributed nature introduces new vulnerabilities. In particular, data poisoning attacks, in which compromised clients inject manipulated training data, can degrade performance or introduce systematic biases. These threats are amplified by spatial dependencies in meteorological data, allowing localized perturbations to influence broader regions through global model aggregation. In this study, we investigate how adversarial clients distort federated surface temperature forecasts trained on the Copernicus European Regional ReAnalysis (CERRA) dataset. We simulate geographically distributed clients and evaluate patch-based and global biasing attacks on regional temperature forecasts. Our results show that even a small fraction of poisoned clients can mislead predictions across large, spatially connected areas. A global temperature bias attack from a single compromised client shifts predictions by up to -1.7 K, while coordinated patch attacks more than triple the mean squared error and produce persistent regional anomalies exceeding +3.5 K. Finally, we assess trimmed mean aggregation as a defense mechanism, showing that it successfully defends against global bias attacks (2-13\% degradation) but fails against patch attacks (281-603\% amplification), exposing limitations of outlier-based defenses for spatially correlated data.

</details>


### [244] [ModSSC: A Modular Framework for Semi-Supervised Classification on Heterogeneous Data](https://arxiv.org/abs/2512.13228)
*Melvin Barbaux*

Main category: cs.LG

TL;DR: 介绍开源Python框架ModSSC，统一归纳和直推式半监督分类，实现多种算法，支持多模态数据，方便实验和复现，已发布。


<details>
  <summary>Details</summary>
Motivation: 半监督分类现有软件支持在方法和模态上分散，需要统一框架。

Method: 开发ModSSC框架，实现多种算法，提供多模态数据集加载器，用YAML声明式描述实验。

Result: ModSSC 1.0.0版本发布，有详细文档和测试，可在指定链接获取。

Conclusion: ModSSC框架能统一半监督分类，便于实验和复现，推动半监督分类发展。

Abstract: Semi-supervised classification leverages both labeled and unlabeled data to improve predictive performance, but existing software support is fragmented across methods and modalities. We introduce ModSSC, an open source Python framework that unifies inductive and transductive semi-supervised classification in a modular code base. ModSSC implements a broad range of classical and recent algorithms, provides loaders for tabular, image, text, audio and graph datasets, and exposes a single configuration interface for specifying datasets, models and evaluation protocols. It supports both lightweight classical methods on small datasets running on CPU and recent deep approaches that can exploit multiple GPUs within the same experimental framework. Experiments are described declaratively in YAML, which facilitates reproducing existing work and running large comparative studies. ModSSC 1.0.0 is released under the MIT license with extensive documentation and tests, and is available at https://github.com/ModSSC/ModSSC.

</details>


### [245] [CORE: Contrastive Masked Feature Reconstruction on Graphs](https://arxiv.org/abs/2512.13235)
*Jianyuan Bo,Yuan Fang*

Main category: cs.LG

TL;DR: 研究揭示MFR和GCL目标在特定条件下收敛，提出集成二者的CORE框架，在节点和图分类任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 发现MFR和GCL都旨在最大化相似元素间的一致性，且目标在特定条件下收敛，探索二者集成以提升图的自监督学习。

Method: 提出Contrastive Masked Feature Reconstruction (CORE)框架，在MFR中集成对比学习，形成正样本对，利用掩码节点作为负样本。

Result: CORE在节点和图分类任务上显著优于MFR，在节点分类任务上比GraphMAE和GraphMAE2最多高2.80%和3.72%，在图分类任务上最多高3.82%和3.76%。

Conclusion: MFR和GCL是互补的，集成二者的CORE框架能更好地捕获图的内在结构，提升自监督学习效果。

Abstract: In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.

</details>


### [246] [BézierFlow: Bézier Stochastic Interpolant Schedulers for Few-Step Generation](https://arxiv.org/abs/2512.13255)
*Yunhong Min,Juil Koo,Seungwoo Yoo,Minhyuk Sung*

Main category: cs.LG

TL;DR: 介绍了BézierFlow，一种用于预训练扩散和流模型少步生成的轻量级训练方法，能提升性能且训练时间短，优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级训练方法在ODE离散化方面有局限，需拓宽采样轨迹优化的范围。

Method: 将随机插值调度器参数化以学习采样轨迹的最优变换，用贝塞尔函数表示调度器函数，学习时间范围内的有序控制点。

Result: BézierFlow在≤10个NFE的采样中实现2 - 3倍的性能提升，训练仅需15分钟，在多种模型上优于先前的时间步学习方法。

Conclusion: 将搜索空间从离散时间步扩展到基于贝塞尔的轨迹变换是有效的。

Abstract: We introduce BézierFlow, a lightweight training approach for few-step generation with pretrained diffusion and flow models. BézierFlow achieves a 2-3x performance improvement for sampling with $\leq$ 10 NFEs while requiring only 15 minutes of training. Recent lightweight training approaches have shown promise by learning optimal timesteps, but their scope remains restricted to ODE discretizations. To broaden this scope, we propose learning the optimal transformation of the sampling trajectory by parameterizing stochastic interpolant (SI) schedulers. The main challenge lies in designing a parameterization that satisfies critical desiderata, including boundary conditions, differentiability, and monotonicity of the SNR. To effectively meet these requirements, we represent scheduler functions as Bézier functions, where control points naturally enforce these properties. This reduces the problem to learning an ordered set of points in the time range, while the interpretation of the points changes from ODE timesteps to Bézier control points. Across a range of pretrained diffusion and flow models, BézierFlow consistently outperforms prior timestep-learning methods, demonstrating the effectiveness of expanding the search space from discrete timesteps to Bézier-based trajectory transformations.

</details>


### [247] [ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning](https://arxiv.org/abs/2512.13316)
*Mayank Gulati,Benedikt Groß,Gerhard Wunder*

Main category: cs.LG

TL;DR: 提出ALIGN - FL方法，通过选择性共享生成组件解决高度不相交数据分布学习挑战，用隐私机制在MNIST和Fashion - MNIST数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决从高度不相交数据分布中进行学习的挑战，实现隐私保护学习。

Method: 提出ALIGN - FL框架，仅在客户端间转移生成能力，服务器用合成样本进行全局训练，采用DP - SGD自适应裁剪和Lipschitz正则化VAE解码器等隐私机制。

Result: 在MNIST和Fashion - MNIST数据集含跨域离群点的实验中，隐私机制能将敏感离群点映射到典型数据点，在极端非IID场景保持效用。

Conclusion: ALIGN - FL方法配合隐私机制在解决高度不相交数据分布学习和隐私保护方面有效。

Abstract: We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations.
  Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures

</details>


### [248] [KD-PINN: Knowledge-Distilled PINNs for ultra-low-latency real-time neural PDE solvers](https://arxiv.org/abs/2512.13336)
*Karim Bounja,Lahcen Laayouni,Abdeljalil Sakat*

Main category: cs.LG

TL;DR: 本文提出KD - PINN框架，将教师模型预测精度转移到学生模型，在多种PDEs测试中保留物理精度、提升推理速度，蒸馏有正则化效果，且能降低推理延迟以助力超低延迟神经PDE求解器开发。


<details>
  <summary>Details</summary>
Motivation: 开发能够保留物理精度并降低推理延迟的神经网络偏微分方程（PDE）求解器，以适用于不同动力学和维度情况。

Method: 引入知识蒸馏物理信息神经网络（KD - PINN）框架，通过持续调整Kullback - Leibler散度将高容量教师模型的预测精度转移到紧凑的学生模型。

Result: 在所有测试案例中，学生模型保留了教师的物理精度，平均RMSE增加低于0.64%，推理速度提升4.8x（Navier - Stokes）至6.9x（Burgers），蒸馏有正则化效果，蒸馏模型平均推理延迟达5.3 ms，进入超低延迟实时范围。

Conclusion: 知识蒸馏可降低PINNs的推理延迟，有助于开发准确的超低延迟神经PDE求解器。

Abstract: This work introduces Knowledge-Distilled Physics-Informed Neural Networks (KD-PINN), a framework that transfers the predictive accuracy of a high-capacity teacher model to a compact student through a continuous adaptation of the Kullback-Leibler divergence. To confirm its generality for various dynamics and dimensionalities, the framework is evaluated on a representative set of partial differential equations (PDEs). In all tested cases, the student model preserved the teacher's physical accuracy, with a mean RMSE increase below 0.64%, and achieved inference speedups ranging from 4.8x (Navier-Stokes) to 6.9x (Burgers). The distillation process also revealed a regularizing effect. With an average inference latency of 5.3 ms on CPU, the distilled models enter the ultra-low-latency real-time regime defined by sub-10 ms performance. Finally, this study examines how knowledge distillation reduces inference latency in PINNs to contribute to the development of accurate ultra-low-latency neural PDE solvers.

</details>


### [249] [FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs](https://arxiv.org/abs/2512.13337)
*Si Qi Goh,Yongsen Zheng,Ziyao Liu,Sami Hormi,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: 提出FROC框架用于大语言模型机器无学习的风险优化控制，实验证明其能产生稳定、可解释的风险格局。


<details>
  <summary>Details</summary>
Motivation: 当前机器无学习技术缺乏评估和控制风险的有效机制，影响安全与效用平衡，引发遗忘权信任问题。

Method: 构建基于共形风格风险控制公式的FROC框架，引入连续风险模型，计算共形无学习风险和风险控制配置集。

Result: 实验表明FROC能产生稳定、可解释的风险格局，揭示无学习配置、语义转移和效用影响之间的关系。

Conclusion: FROC将机器无学习重新定义为可控、风险感知的过程，为大规模大语言模型部署中的无学习行为管理提供了实用基础。

Abstract: Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the "right to be forgotten." To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.

</details>


### [250] [Link-Aware Energy-Frugal Continual Learning for Fault Detection in IoT Networks](https://arxiv.org/abs/2512.13340)
*Henrik C. M. Frederiksen,Junya Shiraishi,Cedomir Stefanovic,Hei Victor Cheng,Shashi Raj Pandey*

Main category: cs.LG

TL;DR: 本文提出事件驱动通信框架，将持续学习集成到物联网网络实现节能故障检测，在推理召回率上表现优异。


<details>
  <summary>Details</summary>
Motivation: 轻量级机器学习模型在物联网网络推理精度因环境非平稳性和初始训练数据有限而下降，更新模型消耗额外能量，不符合节能需求。

Method: 引入事件驱动通信框架，使物联网设备和边缘服务器协作更新轻量级机器学习模型，适应无线链路通信条件和可用能量预算。

Result: 在真实数据集评估中，该方法在推理召回率上优于定期采样和非自适应持续学习，在严格能量和带宽约束下提升达42.8%。

Conclusion: 所提方法能有效解决物联网网络中轻量级机器学习模型的精度和能耗问题，实现节能故障检测。

Abstract: The use of lightweight machine learning (ML) models in internet of things (IoT) networks enables resource constrained IoT devices to perform on-device inference for several critical applications. However, the inference accuracy deteriorates due to the non-stationarity in the IoT environment and limited initial training data. To counteract this, the deployed models can be updated occasionally with new observed data samples. However, this approach consumes additional energy, which is undesirable for energy constrained IoT devices. This letter introduces an event-driven communication framework that strategically integrates continual learning (CL) in IoT networks for energy-efficient fault detection. Our framework enables the IoT device and the edge server (ES) to collaboratively update the lightweight ML model by adapting to the wireless link conditions for communication and the available energy budget. Evaluation on real-world datasets show that the proposed approach can outperform both periodic sampling and non-adaptive CL in terms of inference recall; our proposed approach achieves up to a 42.8% improvement, even under tight energy and bandwidth constraint.

</details>


### [251] [On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models](https://arxiv.org/abs/2512.13352)
*Ali Al Sahili,Ali Chehab,Razane Tajeddine*

Main category: cs.LG

TL;DR: 研究整合多种MIA技术到数据提取流程，对比其在整合设置和传统基准测试中的性能，评估其在现实提取场景的实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）易记忆训练数据，存在训练数据提取和成员推理攻击（MIAs）等隐私风险，且二者相互关联，需评估MIA技术在数据提取中的有效性。

Method: 将多种MIA技术整合到数据提取流程，进行系统基准测试，并与传统MIA基准测试结果对比。

Result: 未提及具体结果

Conclusion: 未提及具体结论

Abstract: Large Language Models (LLMs) are prone to memorizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA benchmarks, allowing us to evaluate their practical utility in real-world extraction scenarios.

</details>


### [252] [Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction](https://arxiv.org/abs/2512.13381)
*Changjun Zhou,Jintao Zheng,Leyou Yang,Pengfei Wang*

Main category: cs.LG

TL;DR: 提出DPUL方法解决联邦无学习中隐私和效率问题，实验显示效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有联邦无学习方法存在计算量大、成本高及隐私泄露等问题，需改进。

Method: DPUL方法包括三部分：过滤客户端更新幅度识别高权重参数并回滚；用变分自编码器重构和消除低权重参数；用投影技术恢复模型。

Result: 在四个数据集上实验表明，DPUL比现有基线精度提升1%-5%，时间成本最多降低12倍。

Conclusion: DPUL方法有效解决现有问题，在精度和时间成本上展现优势。

Abstract: Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.

</details>


### [253] [XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders](https://arxiv.org/abs/2512.13442)
*Khawla Elhadri,Jörg Schlötterer,Christin Seifert*

Main category: cs.LG

TL;DR: 提出XNNTab神经架构结合神经网络表达力与可解释性，性能超可解释模型，与不可解释模型相当。


<details>
  <summary>Details</summary>
Motivation: 数据驱动表格数据应用中需可解释模型，神经网络因黑盒特性未被使用，需结合表达力与可解释性。

Method: XNNTab先学习高度非线性特征表示，用稀疏自动编码器（SAE）将其分解为单义特征，再为特征赋予人类可解释概念。

Result: XNNTab表现优于可解释预测模型，与不可解释对手表现相当。

Conclusion: XNNTab能有效结合神经网络表达力与可解释性。

Abstract: In data-driven applications relying on tabular data, where interpretability is key, machine learning models such as decision trees and linear regression are applied. Although neural networks can provide higher predictive performance, they are not used because of their blackbox nature. In this work, we present XNNTab, a neural architecture that combines the expressiveness of neural networks and interpretability. XNNTab first learns highly non-linear feature representations, which are decomposed into monosemantic features using a sparse autoencoder (SAE). These features are then assigned human-interpretable concepts, making the overall model prediction intrinsically interpretable. XNNTab outperforms interpretable predictive models, and achieves comparable performance to its non-interpretable counterparts.

</details>


### [254] [SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy](https://arxiv.org/abs/2512.13458)
*Yici Liu,Qi Wei Oung,Hoi Leong Lee*

Main category: cs.LG

TL;DR: 本文提出基于源选择和对抗策略的跨主体EEG情感识别方法，含源选择网络和对抗策略网络，在两个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决跨主体EEG情感识别研究中忽略个体差异和负迁移现象的问题。

Method: 提出包含源选择网络（SS）和对抗策略网络（AS）的方法，SS利用域标签反向工程域适应训练，AS利用SS结果和预训练域判别器计算新损失。

Result: 在SEED和SEED - IV两个基于EEG的情感数据集上取得出色性能。

Conclusion: 所提方法有效，代码可在https://github.com/liuyici/SSAS获取。

Abstract: Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.

</details>


### [255] [DP-EMAR: A Differentially Private Framework for Autonomous Model Weight Repair in Federated IoT Systems](https://arxiv.org/abs/2512.13460)
*Chethana Prasad Kabgere,Shylaja S S*

Main category: cs.LG

TL;DR: 提出DP - EMAR框架解决资源受限物联网网络中联邦学习模型权重失真问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在资源受限物联网网络中存在模型权重失真问题，多层联邦物联网系统中不稳定连接和对抗干扰会降低收敛性。

Method: 提出DP - EMAR框架，在添加隐私噪声前估计损坏模式并进行自适应校正，集成差分隐私与安全聚合以区分噪声和传输错误。

Result: 在异构物联网传感器和图数据集实验中，DP - EMAR在通信损坏情况下保持收敛稳定性和接近基线的性能，确保严格的差分隐私保证。

Conclusion: 该框架增强了隐私保护联邦物联网学习的鲁棒性、通信效率和可信度。

Abstract: Federated Learning (FL) enables decentralized model training without sharing raw data, but model weight distortion remains a major challenge in resource constrained IoT networks. In multi tier Federated IoT (Fed-IoT) systems, unstable connectivity and adversarial interference can silently alter transmitted parameters, degrading convergence. We propose DP-EMAR, a differentially private, error model based autonomous repair framework that detects and reconstructs transmission induced distortions during FL aggregation. DP-EMAR estimates corruption patterns and applies adaptive correction before privacy noise is added, enabling reliable in network repair without violating confidentiality. By integrating Differential Privacy (DP) with Secure Aggregation (SA), the framework distinguishes DP noise from genuine transmission errors. Experiments on heterogeneous IoT sensor and graph datasets show that DP-EMAR preserves convergence stability and maintains near baseline performance under communication corruption while ensuring strict (epsilon, delta)-DP guarantees. The framework enhances robustness, communication efficiency, and trust in privacy preserving Federated IoT learning.

</details>


### [256] [Element-wise Modulation of Random Matrices for Efficient Neural Layers](https://arxiv.org/abs/2512.13480)
*Maksymilian Szorc*

Main category: cs.LG

TL;DR: 提出PRP层减少全连接层参数和计算开销并保持精度


<details>
  <summary>Details</summary>
Motivation: 全连接层存在内存和计算开销问题，现有压缩技术有工程权衡或性能下降问题

Method: 提出Parametrized Random Projection (PRP)层，利用固定随机矩阵和可学习元素参数解耦特征混合和适应

Result: 大幅减少可训练参数数量至线性规模，在多个基准测试中保持可靠精度

Conclusion: PRP层是资源受限环境下架构扩展和部署的稳定高效解决方案

Abstract: Fully connected layers are a primary source of memory and computational overhead in deep neural networks due to their dense, often redundant parameterization. While various compression techniques exist, they frequently introduce complex engineering trade-offs or degrade model performance. We propose the Parametrized Random Projection (PRP) layer, a novel approach that decouples feature mixing from adaptation by utilizing a fixed random matrix modulated by lightweight, learnable element-wise parameters. This architecture drastically reduces the trainable parameter count to a linear scale while retaining reliable accuracy across various benchmarks. The design serves as a stable, computationally efficient solution for architectural scaling and deployment in resource-limited settings.

</details>


### [257] [On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing](https://arxiv.org/abs/2512.13497)
*Haoyu Ren,Kay Koehle,Kirill Dorofeev,Darko Anicic*

Main category: cs.LG

TL;DR: 本文研究用于无监督视觉异常检测（VAD）的设备端持续学习方法，在工业用例评估中表现良好，适用于动态智能制造。


<details>
  <summary>Details</summary>
Motivation: 现代制造中VAD很重要，但动态灵活生产环境带来模型更新、硬件资源和数据稀缺等挑战。

Method: 扩展PatchCore，结合轻量级特征提取器和基于k - center选择的增量核心集更新机制。

Result: 在工业用例评估中，相比基线AUROC提高12%，内存使用减少80%，训练速度更快。

Conclusion: 该方法能实现准确、资源高效和自适应的VAD，适用于动态和智能制造。

Abstract: In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.

</details>


### [258] [Async Control: Stress-testing Asynchronous Control Measures for LLM Agents](https://arxiv.org/abs/2512.13526)
*Asa Cooper Stickland,Jan Michelfeit,Arathi Mani,Charlie Griffin,Ollie Matthews,Tomek Korbak,Rogan Inglis,Oliver Makins,Alan Cooney*

Main category: cs.LG

TL;DR: 研究基于LLM的软件工程代理的异步监控，开发集成监控器并评估破坏风险，代码开源。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的软件工程代理可能对敏感数据和关键代码库造成破坏，需有效监控手段。

Method: 将监控开发视为红蓝对抗游戏，设置规则限制代理破坏潜力，开发5种软件工程环境，开发集成监控器。

Result: 集成监控器在保留测试环境中实现1%误报率下6%漏报率，给出风险外推模型及敏感性分析。

Conclusion: 提出异步监控方法应对代理破坏风险，给出评估风险方式及模型局限性。

Abstract: LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.

</details>


### [259] [Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability](https://arxiv.org/abs/2512.13568)
*Leonard Bereska,Zoe Tzifa-Kratira,Reza Samavi,Efstratios Gavves*

Main category: cs.LG

TL;DR: 提出信息理论框架测量神经表征的有效自由度，该指标与玩具模型的真实值强相关，能检测多种情况，并发现对抗训练对有效特征的影响与任务复杂度和网络容量有关。


<details>
  <summary>Details</summary>
Motivation: 神经网络通过叠加实现卓越性能，但缺乏衡量叠加的原则性方法，影响可解释性。

Method: 提出信息理论框架，将香农熵应用于稀疏自动编码器激活来计算有效特征数量。

Result: 指标与玩具模型真实值强相关，发现最小叠加、dropout下的系统减少等，还揭示对抗训练对有效特征的影响。

Conclusion: 通过将叠加定义为有损压缩，可对神经网络在计算约束下组织信息进行原则性测量，将叠加与对抗鲁棒性联系起来。

Abstract: Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many "virtual neurons" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.

</details>


### [260] [DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication](https://arxiv.org/abs/2512.13583)
*Zehan Zhu,Heng Zhao,Yan Huang,Joey Tianyi Zhou,Shouling Ji,Jinming Xu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\mathcal{O}\left( \sqrt{d\log \left( \frac{1}δ \right)}/(\sqrt{n}Jε) \right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\left(ε, δ\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.

</details>


### [261] [Image Diffusion Preview with Consistency Solver](https://arxiv.org/abs/2512.13592)
*Fu-Yun Wang,Hao Zhou,Liangzhe Yuan,Sanghyun Woo,Boqing Gong,Bohyung Han,Ming-Hsuan Yang,Han Zhang,Yukun Zhu,Ting Liu,Long Zhao*

Main category: cs.LG

TL;DR: 提出Diffusion Preview范式和ConsistencySolver以加速图像扩散模型推理，提升预览质量和一致性，减少用户交互时间。


<details>
  <summary>Details</summary>
Motivation: 图像扩散模型推理过程慢，现有加速方法难以提供高质量预览和保证预览与最终输出的一致性。

Method: 引入Diffusion Preview范式，提出基于一般线性多步方法的ConsistencySolver，通过强化学习优化。

Result: ConsistencySolver在低步数场景下显著提高生成质量和一致性，FID分数与Multistep DPM - Solver相当但步数少47%，优于蒸馏基线，用户交互时间减少近50%。

Conclusion: ConsistencySolver适用于高效的预览 - 细化工作流程，可提升图像扩散模型的交互体验。

Abstract: The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.

</details>


### [262] [Scalable Formal Verification via Autoencoder Latent Space Abstraction](https://arxiv.org/abs/2512.13593)
*Robert Reed,Morteza Lahijanian,Luca Laurenti*

Main category: cs.LG

TL;DR: 本文提出用凸自动编码器降维系统，在潜在空间学习动力学，构建有限抽象并保证其包含原系统真实行为，还能将验证结果映射回原系统，通过多系统验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 有限抽象方法对高维系统有可扩展性挑战，基于学习的降维方法的验证结果正确性待解决。

Method: 通过凸自动编码器降低系统维度，用基于核的方法在潜在空间学习动力学，从潜在空间的学习模型构建有限抽象。

Result: 在多个系统（包括26D神经网络控制系统）上验证了方法有效性，显著提升可扩展性且不失严谨性。

Conclusion: 所提方法能有效解决高维系统有限抽象的可扩展性问题，保证验证结果可映射回原系统。

Abstract: Finite Abstraction methods provide a powerful formal framework for proving that systems satisfy their specifications. However, these techniques face scalability challenges for high-dimensional systems, as they rely on state-space discretization which grows exponentially with dimension. Learning-based approaches to dimensionality reduction, utilizing neural networks and autoencoders, have shown great potential to alleviate this problem. However, ensuring the correctness of the resulting verification results remains an open question. In this work, we provide a formal approach to reduce the dimensionality of systems via convex autoencoders and learn the dynamics in the latent space through a kernel-based method. We then construct a finite abstraction from the learned model in the latent space and guarantee that the abstraction contains the true behaviors of the original system. We show that the verification results in the latent space can be mapped back to the original system. Finally, we demonstrate the effectiveness of our approach on multiple systems, including a 26D system controlled by a neural network, showing significant scalability improvements without loss of rigor.

</details>


### [263] [LightTopoGAT: Enhancing Graph Attention Networks with Topological Features for Efficient Graph Classification](https://arxiv.org/abs/2512.13617)
*Ankit Sharma,Sayan Roy Gupta*

Main category: cs.LG

TL;DR: 提出轻量级图注意力网络LightTopoGAT，在三个基准数据集上表现优于基线模型，消融实验证明拓扑特征提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在图分类任务中需大量计算资源且难有效捕捉全局图属性。

Method: 引入LightTopoGAT，通过拓扑增强（结合节点度和局部聚类系数）提升节点特征，采用精简注意力机制保持参数效率，整合局部消息传递方案常忽略的结构信息。

Result: 在MUTAG、ENZYMES和PROTEINS三个基准数据集上，LightTopoGAT比GCN、GraphSAGE和标准GAT等基线模型表现更优，MUTAG准确率提高6.6%，PROTEINS提高2.2%。

Conclusion: 在不增加架构复杂性的情况下，纳入拓扑特征是提升图神经网络性能的简单有效策略。

Abstract: Graph Neural Networks have demonstrated significant success in graph classification tasks, yet they often require substantial computational resources and struggle to capture global graph properties effectively. We introduce LightTopoGAT, a lightweight graph attention network that enhances node features through topological augmentation by incorporating node degree and local clustering coefficient to improve graph representation learning. The proposed approach maintains parameter efficiency through streamlined attention mechanisms while integrating structural information that is typically overlooked by local message passing schemes. Through comprehensive experiments on three benchmark datasets, MUTAG, ENZYMES, and PROTEINS, we show that LightTopoGAT achieves superior performance compared to established baselines including GCN, GraphSAGE, and standard GAT, with a 6.6 percent improvement in accuracy on MUTAG and a 2.2 percent improvement on PROTEINS. Ablation studies further confirm that these performance gains arise directly from the inclusion of topological features, demonstrating a simple yet effective strategy for enhancing graph neural network performance without increasing architectural complexity.

</details>


### [264] [StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion](https://arxiv.org/abs/2512.13632)
*Guransh Singh,Md Shah Fahad*

Main category: cs.LG

TL;DR: 提出首个用于多标签口吃检测的检索增强分类器StutterFuse，解决了模态崩溃问题，在数据集上表现出色且有零样本跨语言泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有参数化模型在处理重叠口吃时表现不佳，检索增强生成范式在病理语音处理中未被探索。

Method: 引入StutterFuse，通过让Conformer编码器基于临床示例非参数记忆库进行分类，使用SetCon目标和门控混合专家融合策略解决模态崩溃问题。

Result: 在SEP - 28k数据集上，StutterFuse加权F1分数达0.65，优于强基线模型。

Conclusion: StutterFuse在多标签口吃检测中有效，具有零样本跨语言泛化能力。

Abstract: Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve "Modality Collapse", an "Echo Chamber" effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.

</details>


### [265] [From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves](https://arxiv.org/abs/2512.13641)
*Gabriel Vitorino de Andrade,Saulo Roberto dos Santos,Itallo Patrick Castro Alves da Silva,Emanuel Adler Medeiros Pereira,Erick de Andrade Barboza*

Main category: cs.LG

TL;DR: 本文提出评估卷积神经网络在不利条件下性能的方法，用MangoLeafDB-C数据集对比五种架构，发现轻量级模型在现实场景中表现更好，强调农业智能系统开发需纳入鲁棒性评估。


<details>
  <summary>Details</summary>
Motivation: 人工智能模型鲁棒性评估对保证智能系统可靠性能很重要，但缺少芒果叶疾病诊断模型鲁棒性研究。

Method: 调整MangoLeafDB数据集生成MangoLeafDB-C，对比ResNet - 50、ResNet - 101、VGG - 16、Xception和LCNN五种架构，用F1分数、CE和relative mCE等指标评估。

Result: LCNN在现实场景的损坏情况中表现优于复杂模型，现代架构在损坏场景中性能显著下降。

Conclusion: 轻量级和专用模型更适合边缘设备的现实应用，农业智能系统开发需纳入鲁棒性评估。

Abstract: The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.

</details>


### [266] [A Scientific Reasoning Model for Organic Synthesis Procedure Generation](https://arxiv.org/abs/2512.13668)
*Guoqing Liu,Junren Li,Zihan Zhao,Eray Inanc,Krzysztof Maziarz,Jose Garrido Torres,Victor Garcia Satorras,Shoko Ueda,Christopher M. Bishop,Marwin Segler*

Main category: cs.LG

TL;DR: 提出科学推理语言模型QFANG，能从反应方程式生成实验步骤，在实验中表现优异，有助于弥合计算合成规划与自动化实验室合成差距。


<details>
  <summary>Details</summary>
Motivation: 解决计算机辅助合成规划问题，弥合计算路线设计与实际实验室执行的差距，准确预测合成步骤的可行实验程序。

Method: 策划含905,990个化学反应及结构化动作序列的高质量数据集；引入化学引导推理（CGR）框架生成基于化学知识的思维链数据；进行监督微调；应用可验证奖励强化学习（RLVR）提高程序准确性。

Result: QFANG在传统NLP相似度指标和化学感知评估器上优于先进通用推理模型和近邻检索基线，能泛化到特定域外反应类，适应实验室条件和用户特定约束变化。

Conclusion: QFANG生成高质量合成程序的能力是弥合计算合成规划与全自动实验室合成差距的重要一步。

Abstract: Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experimental procedures directly from reaction equations, with explicit chain-of-thought reasoning. To develop QFANG, we curated a high-quality dataset comprising 905,990 chemical reactions paired with structured action sequences, extracted and processed from patent literature using large language models. We introduce a Chemistry-Guided Reasoning (CGR) framework that produces chain-of-thought data grounded in chemical knowledge at scale. The model subsequently undergoes supervised fine-tuning to elicit complex chemistry reasoning. Finally, we apply Reinforcement Learning from Verifiable Rewards (RLVR) to further enhance procedural accuracy. Experimental results demonstrate that QFANG outperforms advanced general-purpose reasoning models and nearest-neighbor retrieval baselines, measured by traditional NLP similarity metrics and a chemically aware evaluator using an LLM-as-a-judge. Moreover, QFANG generalizes to certain out-of-domain reaction classes and adapts to variations in laboratory conditions and user-specific constraints. We believe that QFANG's ability to generate high-quality synthesis procedures represents an important step toward bridging the gap between computational synthesis planning and fully automated laboratory synthesis.

</details>


### [267] [Directional Textual Inversion for Personalized Text-to-Image Generation](https://arxiv.org/abs/2512.13672)
*Kunhee Kim,NaHyeon Park,Kibeom Hong,Hyunjung Shim*

Main category: cs.LG

TL;DR: Textual Inversion在复杂提示下常失败，本文指出是嵌入范数膨胀问题，提出Directional Textual Inversion（DTI）优化方向，提升文本保真度并支持概念插值。


<details>
  <summary>Details</summary>
Motivation: 解决Textual Inversion在复杂提示下失败的问题，分析失败原因是嵌入范数膨胀。

Method: 提出DTI，固定嵌入幅度，通过Riemannian SGD在单位超球面上优化方向，将方向学习作为带von Mises - Fisher先验的MAP。

Result: 在个性化任务中，DTI提高了文本保真度，保持了主体相似性，且支持概念插值。

Conclusion: 仅优化方向是实现忠实于提示的个性化的健壮且可扩展的路径。

Abstract: Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [268] [Spiking Manifesto](https://arxiv.org/abs/2512.11843)
*Eugene Izhikevich*

Main category: cs.NE

TL;DR: 计算机多数任务表现优于大脑，但AI模型效率比大脑低千倍，本文提出用脉冲网络思考AI模型，有望提升效率千倍。


<details>
  <summary>Details</summary>
Motivation: 解决AI模型效率远低于大脑的问题。

Method: 提出用脉冲网络和多时间同步思考流行AI模型，将脉冲活动解释为实现查找表的自然方式。

Result: 提出可将AI模型转换为新型架构的方法。

Conclusion: 该方法有望使AI模型效率提升千倍，代码开源。

Abstract: Practically everything computers do is better, faster, and more power-efficient than the brain. For example, a calculator crunches numbers more energy-efficiently than any human. Yet AI models are a thousand times less efficient than the brain. These models use artificial neural networks (ANNs) and require GPUs for the multiplication of huge matrices. In contrast, spiking neural networks (SNNs) of the brain have no matrix multiplication and much smaller energy requirements. This manifesto proposes a framework for thinking about popular AI models in terms of spiking networks and polychronization, and for interpreting spiking activity as nature's way of implementing look-up tables. This offers a way to convert AI models into a novel type of architecture with the promise of a thousandfold improvement in efficiency. Code is available at https://github.com/izhikevich/SNN

</details>


### [269] [Evolving Deep Learning Optimizers](https://arxiv.org/abs/2512.11853)
*Mitchell Marfinetz*

Main category: cs.NE

TL;DR: 提出遗传算法框架自动发现深度学习优化算法，进化出的优化器性能优于Adam。


<details>
  <summary>Details</summary>
Motivation: 自动发现深度学习优化算法，探索不同于手工设计优化器的设计原则。

Method: 将优化器编码为基因组，通过50代、种群为50个体的进化搜索，在多个视觉任务上评估。

Result: 进化出的优化器在综合适应度上比Adam高2.6%，在CIFAR - 10上有7.7%的相对提升，结合符号梯度项与自适应矩估计等。

Conclusion: 进化搜索能发现有竞争力的优化算法，揭示与手工优化器不同的设计原则。

Abstract: We present a genetic algorithm framework for automatically discovering deep learning optimization algorithms. Our approach encodes optimizers as genomes that specify combinations of primitive update terms (gradient, momentum, RMS normalization, Adam-style adaptive terms, and sign-based updates) along with hyperparameters and scheduling options. Through evolutionary search over 50 generations with a population of 50 individuals, evaluated across multiple vision tasks, we discover an evolved optimizer that outperforms Adam by 2.6% in aggregate fitness and achieves a 7.7% relative improvement on CIFAR-10. The evolved optimizer combines sign-based gradient terms with adaptive moment estimation, uses lower momentum coefficients than Adam ($β_1$=0.86, $β_2$=0.94), and notably disables bias correction while enabling learning rate warmup and cosine decay. Our results demonstrate that evolutionary search can discover competitive optimization algorithms and reveal design principles that differ from hand-crafted optimizers. Code is available at https://github.com/mmarfinetz/evo-optimizer.

</details>


### [270] [Self-Motivated Growing Neural Network for Adaptive Architecture via Local Structural Plasticity](https://arxiv.org/abs/2512.12713)
*Yiyang Jia,Chengxu Zhou*

Main category: cs.NE

TL;DR: 本文提出自激励生长神经网络SMGrNN，其拓扑结构可在线演变，评估显示它在控制基准测试中表现良好，自适应拓扑能提升奖励稳定性，且设计便于未来集成其他可塑性模块。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习控制策略使用的多层感知机缺乏结构可塑性且依赖全局误差信号，需改进。

Method: 引入SMGrNN，通过局部结构可塑性模块（SPM）监测神经元激活和权重更新统计信息，触发神经元插入和修剪，用标准基于梯度的优化器更新突触权重，通过策略蒸馏进行评估。

Result: 与多层感知机基线相比，SMGrNN实现了相似或更高的回报、更低的方差和合适的网络规模，消融研究表明自适应拓扑提高了奖励稳定性。

Conclusion: SMGrNN能在学习过程中调节网络容量，无需手动调整架构，其局部和模块化设计便于未来集成其他可塑性模块。

Abstract: Control policies in deep reinforcement learning are often implemented with fixed-capacity multilayer perceptrons trained by backpropagation, which lack structural plasticity and depend on global error signals. This paper introduces the Self-Motivated Growing Neural Network (SMGrNN), a controller whose topology evolves online through a local Structural Plasticity Module (SPM). The SPM monitors neuron activations and edge-wise weight update statistics over short temporal windows and uses these signals to trigger neuron insertion and pruning, while synaptic weights are updated by a standard gradient-based optimizer. This allows network capacity to be regulated during learning without manual architectural tuning.
  SMGrNN is evaluated on control benchmarks via policy distillation. Compared with multilayer perceptron baselines, it achieves similar or higher returns, lower variance, and task-appropriate network sizes. Ablation studies with growth disabled and growth-only variants isolate the role of structural plasticity, showing that adaptive topology improves reward stability. The local and modular design of SPM enables future integration of a Hebbian plasticity module and spike-timing-dependent plasticity, so that SMGrNN can support both artificial and spiking neural implementations driven by local rules.

</details>


### [271] [OPAL: Operator-Programmed Algorithms for Landscape-Aware Black-Box Optimization](https://arxiv.org/abs/2512.12809)
*Junbo Jacob Lian,Mingyang Yu,Kaichen Ouyang,Shengwei Fu,Rui Zhong,Yujun Zhang,Jun Zhang,Huiling Chen*

Main category: cs.NE

TL;DR: 提出OPAL框架用于连续黑盒优化，在CEC 2017测试集上表现良好，表明算子编程、景观感知的逐实例设计是黑盒优化有前景的方法。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒优化依赖进化和群算法，性能高度依赖问题，需更好方法。

Method: 将优化器视为搜索算子的短程序，为每个问题实例单独学习算子程序，构建OPAL框架，通过差分进化探测景观、构建图并编码，用元学习器生成算子调度。

Result: 单个元训练的OPAL策略在CEC 2017测试集上与最先进的自适应差分进化变体有竞争力，优于简单基线；消融研究证明各组件选择合理，元组件仅增加少量时间开销。

Conclusion: 算子编程、景观感知的逐实例设计是超越基于隐喻的临时算法的实用方法。

Abstract: Black-box optimization often relies on evolutionary and swarm algorithms whose performance is highly problem dependent. We view an optimizer as a short program over a small vocabulary of search operators and learn this operator program separately for each problem instance. We instantiate this idea in Operator-Programmed Algorithms (OPAL), a landscape-aware framework for continuous black-box optimization that uses a small design budget with a standard differential evolution baseline to probe the landscape, builds a $k$-nearest neighbor graph over sampled points, and encodes this trajectory with a graph neural network. A meta-learner then maps the resulting representation to a phase-wise schedule of exploration, restart, and local search operators. On the CEC~2017 test suite, a single meta-trained OPAL policy is statistically competitive with state-of-the-art adaptive differential evolution variants and achieves significant improvements over simpler baselines under nonparametric tests. Ablation studies on CEC~2017 justify the choices for the design phase, the trajectory graph, and the operator-program representation, while the meta-components add only modest wall-clock overhead. Overall, the results indicate that operator-programmed, landscape-aware per-instance design is a practical way forward beyond ad hoc metaphor-based algorithms in black-box optimization.

</details>


### [272] [Reproducing and Dissecting Denoising Language Models for Speech Recognition](https://arxiv.org/abs/2512.13576)
*Dorian Koch,Albert Zeyer,Nick Rossenbach,Ralf Schlüter,Hermann Ney*

Main category: cs.NE

TL;DR: 本文对去噪语言模型（DLMs）进行大规模实证研究，构建可复现管道评估多种配置，发现DLMs在特定计算量后优于传统LM，还提出DLM - sum解码方法。


<details>
  <summary>Details</summary>
Motivation: DLMs在自动语音识别（ASR）中有潜力，但训练管道复杂阻碍了深入研究，因此开展独立大规模实证研究。

Method: 构建可复现管道，评估多种数据增强技术、文本转语音系统和解码策略；提出DLM - sum从多个ASR假设中解码的方法。

Result: DLMs在特定计算量后优于传统LMs，在子词词汇设置下改进不如先前基于字符的工作；DLM - sum方法优于DSR解码方法。

Conclusion: 研究结果和公开管道为更好理解、改进和构建此类模型奠定基础。

Abstract: Denoising language models (DLMs) have been proposed as a powerful alternative to traditional language models (LMs) for automatic speech recognition (ASR), motivated by their ability to use bidirectional context and adapt to a specific ASR model's error patterns. However, the complexity of the DLM training pipeline has hindered wider investigation. This paper presents the first independent, large-scale empirical study of DLMs. We build and release a complete, reproducible pipeline to systematically investigate the impact of key design choices. We evaluate dozens of configurations across multiple axes, including various data augmentation techniques (e.g., SpecAugment, dropout, mixup), different text-to-speech systems, and multiple decoding strategies. Our comparative analysis in a common subword vocabulary setting demonstrates that DLMs outperform traditional LMs, but only after a distinct compute tipping point. While LMs are more efficient at lower budgets, DLMs scale better with longer training, mirroring behaviors observed in diffusion language models. However, we observe smaller improvements than those reported in prior character-based work, which indicates that the DLM's performance is conditional on factors such as the vocabulary. Our analysis reveals that a key factor for improving performance is to condition the DLM on richer information from the ASR's hypothesis space, rather than just a single best guess. To this end, we introduce DLM-sum, a novel method for decoding from multiple ASR hypotheses, which consistently outperforms the previously proposed DSR decoding method. We believe our findings and public pipeline provide a crucial foundation for the community to better understand, improve, and build upon this promising class of models. The code is publicly available at https://github.com/rwth-i6/2025-denoising-lm/.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [273] [EDAN: Towards Understanding Memory Parallelism and Latency Sensitivity in HPC](https://arxiv.org/abs/2512.13176)
*Siyuan Shen,Mikhail Khalilov,Lukas Gianinazzi,Timo Schneider,Marcin Chrapek,Jai Dayal,Manisha Gajbe,Robert Wisniewski,Torsten Hoefler*

Main category: cs.PF

TL;DR: 提出新性能分析工具EDAN，利用运行时指令跟踪生成执行DAG，可估计程序延迟敏感性，应用于多个应用和基准测试。


<details>
  <summary>Details</summary>
Motivation: 资源分解技术增加内存访问延迟，现有测量内存延迟敏感性工具不灵活且耗时，需新工具。

Method: 提出EDAN工具，利用应用运行时指令跟踪生成执行DAG。

Result: EDAN能计算性能指标理论界限，洞察HPC应用内存级并行性。

Conclusion: 将EDAN应用于多个应用和基准测试，揭示其内存级并行性和延迟敏感性特征。

Abstract: Resource disaggregation is a promising technique for improving the efficiency of large-scale computing systems. However, this comes at the cost of increased memory access latency due to the need to rely on the network fabric to transfer data between remote nodes. As such, it is crucial to ascertain an application's memory latency sensitivity to minimize the overall performance impact. Existing tools for measuring memory latency sensitivity often rely on custom ad-hoc hardware or cycle-accurate simulators, which can be inflexible and time-consuming. To address this, we present EDAN (Execution DAG Analyzer), a novel performance analysis tool that leverages an application's runtime instruction trace to generate its corresponding execution DAG. This approach allows us to estimate the latency sensitivity of sequential programs and investigate the impact of different hardware configurations. EDAN not only provides us with the capability of calculating the theoretical bounds for performance metrics, but it also helps us gain insight into the memory-level parallelism inherent to HPC applications. We apply EDAN to applications and benchmarks such as PolyBench, HPCG, and LULESH to unveil the characteristics of their intrinsic memory-level parallelism and latency sensitivity.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [274] [Vibe Coding in Practice: Flow, Technical Debt, and Guidelines for Sustainable Use](https://arxiv.org/abs/2512.11922)
*Muhammad Waseem,Aakash Ahmad,Kai-Kristian Kemell,Jussi Rasku,Sami Lahti,Kalle Mäkelä,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 本文分析Vibe Coding在软件开发中带来的流程与债务权衡问题，并提出应对措施。


<details>
  <summary>Details</summary>
Motivation: Vibe Coding可用于快速原型开发，但在软件开发周期中可能引入风险，需分析其流程与债务权衡问题。

Method: 基于内部开发多个MVP的经验以及对近期行业报告的回顾。

Result: 发现无缝代码生成会引发技术债务积累，问题源于过程级弱点、模型训练数据偏差等，同时明确当前模型、平台和硬件限制也会导致这些问题。

Conclusion: 提出应对措施，为更可持续的Vibe Coding方法的研究和实践提供参考。

Abstract: Vibe Coding (VC) is a form of software development assisted by generative AI, in which developers describe the intended functionality or logic via natural language prompts, and the AI system generates the corresponding source code. VC can be leveraged for rapid prototyping or developing the Minimum Viable Products (MVPs); however, it may introduce several risks throughout the software development life cycle. Based on our experience from several internally developed MVPs and a review of recent industry reports, this article analyzes the flow-debt tradeoffs associated with VC. The flow-debt trade-off arises when the seamless code generation occurs, leading to the accumulation of technical debt through architectural inconsistencies, security vulnerabilities, and increased maintenance overhead. These issues originate from process-level weaknesses, biases in model training data, a lack of explicit design rationale, and a tendency to prioritize quick code generation over human-driven iterative development. Based on our experiences, we identify and explain how current model, platform, and hardware limitations contribute to these issues, and propose countermeasures to address them, informing research and practice towards more sustainable VC approaches.

</details>


### [275] [A Systematic Mapping Study on Risks and Vulnerabilities in Software Containers](https://arxiv.org/abs/2512.11940)
*Maha Sroor,Teerath Das,Rahul Mohanani,Tommi Mikkonen*

Main category: cs.SE

TL;DR: 本文通过系统映射研究（SMS）分析软件容器系统安全问题，识别风险漏洞，提供缓解技术、安全实践和工具，强调未来软件工程研究需加强安全实践。


<details>
  <summary>Details</summary>
Motivation: 软件容器虽流行，但开发和部署存在安全问题，软件工程研究缺乏对容器系统风险、漏洞、安全实践和工具的整理知识。

Method: 基于129篇初级研究进行系统映射研究（SMS）。

Result: 识别容器生命周期关键风险和漏洞，用新分类法分类，明确原因和影响，提供缓解技术，汇总安全实践和工具。

Conclusion: 强调未来软件工程研究需聚焦加强容器系统的安全增强实践，制定有效缓解策略以全面应对现存风险和漏洞。

Abstract: Software containers are widely adopted for developing and deploying software applications. Despite their popularity, major security concerns arise during container development and deployment. Software Engineering (SE) research literature reveals a lack of reviewed, aggregated, and organized knowledge of risks, vulnerabilities, security practices, and tools in container-based systems development and deployment. Therefore, we conducted a Systematic Mapping Study (SMS) based on 129 selected primary studies to explore and organize existing knowledge on security issues in software container systems. Data from the primary studies enabled us to identify critical risks and vulnerabilities across the container life-cycle and categorize them using a novel taxonomy. Additionally, the findings highlight the causes and implications and provide a list of mitigation techniques to overcome these risks and vulnerabilities. Furthermore, we provide an aggregation of security practices and tools that can help support and improve the overall security of container systems. This study offers critical insights into the current landscape of security issues within software container systems. Our analysis highlights the need for future SE research to focus on security enhancement practices that strengthen container systems and develop effective mitigation strategies to comprehensively address existing risks and vulnerabilities.

</details>


### [276] [Evaluating Asynchronous Semantics in Trace-Discovered Resilience Models: A Case Study on the OpenTelemetry Demo](https://arxiv.org/abs/2512.12314)
*Anatoly A. Krasnovsky*

Main category: cs.SE

TL;DR: 本文基于追踪发现的连接模型，改进方法并应用于OpenTelemetry Demo，发现对即时HTTP可用性而言，简单的仅考虑连接性的模型已足够。


<details>
  <summary>Details</summary>
Motivation: 分布式追踪和混沌工程在微服务中渐成标准，但弹性模型大多仍需手动定制，需改进。

Method: 从原始OpenTelemetry追踪数据直接导出服务依赖图，添加端点特定成功谓词和简单异步语义，利用GitHub Actions工作流开展模拟和混沌实验。

Result: 模型能重现整体可用性下降曲线，Kafka边的异步语义对预测可用性影响最多约10^(-5)。

Conclusion: 对于此案例的即时HTTP可用性，无需显式建模异步依赖，简单的仅考虑连接性的模型足够。

Abstract: While distributed tracing and chaos engineering are becoming standard for microservices, resilience models remain largely manual and bespoke. We revisit a trace-discovered connectivity model that derives a service dependency graph from traces and uses Monte Carlo simulation to estimate endpoint availability under fail-stop service failures. Compared to earlier work, we (i) derive the graph directly from raw OpenTelemetry traces, (ii) attach endpoint-specific success predicates, and (iii) add a simple asynchronous semantics that treats Kafka edges as non-blocking for immediate HTTP success. We apply this model to the OpenTelemetry Demo ("Astronomy Shop") using a GitHub Actions workflow that discovers the graph, runs simulations, and executes chaos experiments that randomly kill microservices in a Docker Compose deployment. Across the studied failure fractions, the model reproduces the overall availability degradation curve, while asynchronous semantics for Kafka edges change predicted availabilities by at most about 10^(-5) (0.001 percentage points). This null result suggests that for immediate HTTP availability in this case study, explicitly modeling asynchronous dependencies is not warranted, and a simpler connectivity-only model is sufficient.

</details>


### [277] [Evidence-Driven Decision Support for AI Model Selection in Research Software Engineering](https://arxiv.org/abs/2512.11984)
*Alireza Joonbakhsh,Alireza Rostami,AmirMohammad Kamalinia,Ali Nazeri,Farshad Khunjush,Bedir Tekinerdogan,Siamak Farshidi*

Main category: cs.SE

TL;DR: 本文提出结构化方法支持AI模型选择，评估表明效果好，为研究软件工程提供可靠决策支持。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型选择多为临时方式，依赖零散元数据和个人经验，影响研究软件的可重复性、透明度和质量。

Method: 将AI模型选择概念化为多准则决策问题，引入基于证据的决策支持框架，并通过设计科学研究方法，用50个案例和对比实验验证。

Result: ModelSelect产生的推荐可靠、可解释且可重复，在模型和库推荐任务中覆盖率高、与专家推理一致性强，与生成式AI助手表现相当，且可追溯性和一致性更优。

Conclusion: 将AI模型选择构建为MCDM问题，为研究软件工程提供透明和可重复决策支持的基础，所提框架能将实证证据融入推荐过程，提升决策质量和稳健性。

Abstract: The rapid proliferation of artificial intelligence (AI) models and methods presents growing challenges for research software engineers and researchers who must select, integrate, and maintain appropriate models within complex research workflows. Model selection is often performed in an ad hoc manner, relying on fragmented metadata and individual expertise, which can undermine reproducibility, transparency, and overall research software quality.
  This work proposes a structured and evidence-driven approach to support AI model selection that aligns with both technical and contextual requirements. We conceptualize AI model selection as a Multi-Criteria Decision-Making (MCDM) problem and introduce an evidence-based decision-support framework that integrates automated data collection pipelines, a structured knowledge graph, and MCDM principles. Following the Design Science Research methodology, the proposed framework (ModelSelect) is empirically validated through 50 real-world case studies and comparative experiments against leading generative AI systems.
  The evaluation results show that ModelSelect produces reliable, interpretable, and reproducible recommendations that closely align with expert reasoning. Across the case studies, the framework achieved high coverage and strong rationale alignment in both model and library recommendation tasks, performing comparably to generative AI assistants while offering superior traceability and consistency.
  By framing AI model selection as an MCDM problem, this work establishes a rigorous foundation for transparent and reproducible decision support in research software engineering. The proposed framework provides a scalable and explainable pathway for integrating empirical evidence into AI model recommendation processes, ultimately improving the quality and robustness of research software decision-making.

</details>


### [278] [Re-opening open-source science through AI assisted development](https://arxiv.org/abs/2512.11993)
*Ling-Hong Hung,Ka Yee Yeung*

Main category: cs.SE

TL;DR: 开源科学软件因复杂难修改，现由人类领导的AI团队可修改大代码库，以STAR - Flex为例展示。


<details>
  <summary>Details</summary>
Motivation: 解决开源科学软件因复杂难以修改的问题，让社区可审查和验证AI生成的代码。

Method: 采用由一人领导的代理AI团队修改大代码库，以STAR - Flex为例，在STAR基础上添加16000行C++代码。

Result: 创建了首个用于Flex数据的开源处理软件STAR - Flex，且保留原软件全部功能。

Conclusion: 由人类领导的AI团队能够快速且稳健地修改大代码库，重新将科学开放给社区。

Abstract: Open-source scientific software is effectively closed to modification by its complexity. With recent advances in technology, an agentic AI team led by a single human can now rapidly and robustly modify large codebases and re-open science to the community which can review and vet the AI generated code. We demonstrate this with a case study, STAR-Flex, which is an open source fork of STAR, adding 16,000 lines of C++ code to add the ability to process 10x Flex data, while maintaining full original function. This is the first open-source processing software for Flex data and was written as part of the NIH funded MorPHiC consortium.

</details>


### [279] [A Reference Architecture for Embedding Quantum Software Into Enterprise Systems](https://arxiv.org/abs/2512.12009)
*Marc Uphues,Sebastian Thöne,Herbert Kuchen*

Main category: cs.SE

TL;DR: 本文提出将量子软件嵌入企业系统的模块化参考架构，并通过两个案例研究进行演示和评估。


<details>
  <summary>Details</summary>
Motivation: 企业系统软件架构与量子计算服务协作时需考虑特定特性和质量属性，因此要构建合适架构。

Method: 提出模块化参考架构，其构建块由松散耦合和分布式服务组成，服务编排形成可执行BPMN模型的稳定可复用管道。

Result: 利用所提出的参考架构进行了两个案例研究，应对运筹学领域的组合挑战。

Conclusion: 所提出的模块化参考架构可用于将量子软件嵌入企业系统，应对相关应用挑战。

Abstract: Quantum computing promises a remarkable performance boost for certain applications, including computational intensive problems addressed by enterprise systems. However, software architectures of enterprise systems must consider specific characteristics and quality attributes when collaborating with quantum computing services. Hence, this paper presents a modular reference architecture for embedding quantum software into enterprise systems. Its building blocks consist of loosely coupled and distributed services that implement both quantum-independent and quantum-specific tasks. Although these services either depend on the business domain or the selected quantum algorithm, their orchestration forms a stable and reusable pipeline, specified as an executable BPMN model. For demonstration and evaluation purposes, the proposed reference architecture is utilized in two case studies addressing combinatorial challenges from the field of operations research.

</details>


### [280] [Hyper model checking for high-level relational models](https://arxiv.org/abs/2512.12024)
*Nuno Macedo,Hugo Pacheco*

Main category: cs.SE

TL;DR: 文章提出HyperPardinus，扩展Alloy以支持超属性验证，评估显示能处理复杂超属性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能在系统设计早期有效支持软件工程师验证超属性的高级规范语言，且Alloy本身不支持超属性验证。

Method: 提出HyperPardinus，扩展Alloy的时序逻辑后端Pardinus，利用现有底层超属性模型检查器自动验证关系模型上的超属性，并保守扩展Alloy支持设计模型超属性的规范和验证。

Result: 该方法能为具有交替量词的复杂超属性建模并找到（反）例。

Conclusion: 所提方法可行，可处理现有技术中的相关场景。

Abstract: Many properties related to security or concurrency must be encoded as so-called hyperproperties, temporal properties that allow reasoning about multiple traces of a system. However, despite recent advances on model checking hyperproperties, there is still a lack of higher-level specification languages that can effectively support software engineering practitioners in verifying properties of this class at early stages of system design.
  Alloy is a lightweight formal method with a high-level specification language that is supported by automated analysis procedures, making it particularly well-suited for the verification of design models at early development stages. It does not natively support, however, the verification of hyperproperties.
  This work proposes HyperPardinus, a new model finding procedure that extends Pardinus -- the temporal logic backend of the Alloy language -- to automatically verify hyperproperties over relational models by relying on existing low-level model checkers for hyperproperties. It then conservatively extends Alloy to support the specification and automatic verification of hyperproperties over design models, as well as the visualization of (counter-)examples at a higher-level of abstraction. Evaluation shows that our approach enables modeling and finding (counter-)examples for complex hyperproperties with alternating quantifiers, making it feasible to address relevant scenarios from the state of the art.

</details>


### [281] [Instruction-Tuning Open-Weight Language Models for BPMN Model Generation](https://arxiv.org/abs/2512.12063)
*Gökberk Çelikmasat,Atay Özgövde,Fatma Başak Aydemir*

Main category: cs.SE

TL;DR: 本文探讨用指令微调的大语言模型，以具成本效益和保护隐私的方式从自然语言描述生成高质量BPMN流程模型，提出InstruBPM方法。微调后的模型在多指标上表现优异，减少建模工作量，且公开模型和脚本支持复现。


<details>
  <summary>Details</summary>
Motivation: 现有领域模型实践中，建模耗时且需专业知识，从业者常跳过。本文旨在借助LLM解决该问题。

Method: 引入InstruBPM方法，通过参数高效微调与量化，对开源大语言模型进行指令微调；并从文本/代码相似度、结构保真度、指南一致性和专家评审四个角度评估微调模型。

Result: 紧凑的微调模型在序列和结构指标上优于所有基线模型，所需资源大幅减少；生成的图表基本遵循BPMN最佳实践，可减少建模工作量。

Conclusion: 与未微调基线相比，指令微调提高了结构准确性和鲁棒性，减少了对复杂提示的依赖。

Abstract: Domain models are central to software engineering, as they enable a shared understanding, guide implementation, and support automated analyses and model-driven development. Yet, despite these benefits, practitioners often skip modeling because it is time-consuming and demands scarce expertise. We address this barrier by investigating whether open-weight large language models, adapted via instruction tuning, can generate high-quality BPMN process models directly from natural language descriptions in a cost-effective and privacy-preserving way. We introduce InstruBPM, a reproducible approach that prepares paired text-diagram data and instruction tunes an open source large language model with parameter-efficient fine-tuning and quantization for on-prem deployment. We evaluate the tuned model through complementary perspectives: (i) text/code similarity using BLEU, ROUGE-L, and METEOR, (ii) structural fidelity using Relative Graph Edit Distance, (iii) guidelines conformance using external tool checks, and (iv) a small expert review. Using a curated subset of a multi-domain BPMN dataset, we compare the tuned model with untuned open-weight baselines and strong proprietary models under consistent prompting regimes. Our compact tuned model outperforms all baselines across sequence and structural metrics while requiring substantially fewer resources; guideline analysis and expert feedback further indicate that the generated diagrams largely follow BPMN best practices and are useful starting points that reduce modeling effort. Overall, instruction tuning improves structural accuracy and robustness compared to untuned baselines and reduces reliance on heavy prompt scaffolding. We publicly share the trained models and scripts to support reproducibility and further research.

</details>


### [282] [Citation-Grounded Code Comprehension: Preventing LLM Hallucination Through Hybrid Retrieval and Graph-Augmented Context](https://arxiv.org/abs/2512.12117)
*Jahidul Arafat*

Main category: cs.SE

TL;DR: 本文通过混合检索和轻量级结构推理解决代码理解中引用验证的问题，开发混合检索系统，实现92%的引用准确率且无幻觉。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码理解中存在幻觉问题，影响可靠的开发者辅助，需解决可验证、基于引用的代码理解挑战。

Method: 在30个Python仓库上进行系统评估，比较检索方式、图扩展策略和引用验证机制，开发结合BM25稀疏匹配、BGE密集嵌入和Neo4j图扩展的混合检索系统。

Result: 发现引用准确性挑战源于多种因素相互作用，跨文件证据发现对引用完整性贡献大；混合检索系统比单模式基线高14 - 18个百分点，在62%的架构查询中发现纯文本相似性遗漏的跨文件证据。

Conclusion: 倡导将基于引用的生成作为代码理解系统的架构原则，该系统实现了92%的引用准确率且无幻觉。

Abstract: Large language models have become essential tools for code comprehension, enabling developers to query unfamiliar codebases through natural language interfaces. However, LLM hallucination, generating plausible but factually incorrect citations to source code, remains a critical barrier to reliable developer assistance. This paper addresses the challenges of achieving verifiable, citation grounded code comprehension through hybrid retrieval and lightweight structural reasoning. Our work is grounded in systematic evaluation across 30 Python repositories with 180 developer queries, comparing retrieval modalities, graph expansion strategies, and citation verification mechanisms. We find that challenges of citation accuracy arise from the interplay between sparse lexical matching, dense semantic similarity, and cross file architectural dependencies. Among these, cross file evidence discovery is the largest contributor to citation completeness, but it is largely overlooked because existing systems rely on pure textual similarity without leveraging code structure. We advocate for citation grounded generation as an architectural principle for code comprehension systems and demonstrate this need by achieving 92 percent citation accuracy with zero hallucinations. Specifically, we develop a hybrid retrieval system combining BM25 sparse matching, BGE dense embeddings, and Neo4j graph expansion via import relationships, which outperforms single mode baselines by 14 to 18 percentage points while discovering cross file evidence missed by pure text similarity in 62 percent of architectural queries.

</details>


### [283] [Training Versatile Coding Agents in Synthetic Environments](https://arxiv.org/abs/2512.12216)
*Yiqi Zhu,Apurva Gandhi,Graham Neubig*

Main category: cs.SE

TL;DR: 文章提出SWE - Playground新管道用于训练通用编码代理，消除对外部数据源依赖，在三个基准测试中证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有训练软件工程代理方法存在依赖现有GitHub仓库灵活性有限、专注问题解决任务适用性窄的问题，需新方法解决。

Method: 引入SWE - Playground，用强语言模型和代理从头合成生成项目和任务。

Result: 在三个基准测试中表明SWE - Playground生成的轨迹训练信号密集，代理用更少轨迹达相近性能。

Conclusion: SWE - Playground是一种有效的训练通用编码代理的新方法，可处理更多样编码任务。

Abstract: Prior works on training software engineering agents have explored utilizing existing resources such as issues on GitHub repositories to construct software engineering tasks and corresponding test suites. These approaches face two key limitations: (1) their reliance on pre-existing GitHub repositories offers limited flexibility, and (2) their primary focus on issue resolution tasks restricts their applicability to the much wider variety of tasks a software engineer must handle. To overcome these challenges, we introduce SWE-Playground, a novel pipeline for generating environments and trajectories which supports the training of versatile coding agents. Unlike prior efforts, SWE-Playground synthetically generates projects and tasks from scratch with strong language models and agents, eliminating reliance on external data sources. This allows us to tackle a much wider variety of coding tasks, such as reproducing issues by generating unit tests and implementing libraries from scratch. We demonstrate the effectiveness of this approach on three distinct benchmarks, and results indicate that SWE-Playground produces trajectories with dense training signal, enabling agents to reach comparable performance with significantly fewer trajectories than previous works.

</details>


### [284] [Cluster-guided LLM-Based Anonymization of Software Analytics Data: Studying Privacy-Utility Trade-offs in JIT Defect Prediction](https://arxiv.org/abs/2512.12224)
*Maaz Khan,Gul Sher Khan,Ahsan Raza,Pir Sami Ullah,Abdul Ali Bangash*

Main category: cs.SE

TL;DR: 使用机器学习进行JIT缺陷预测存在隐私泄露问题，现有匿名化方法不佳，本文提出基于LLM的聚类引导匿名化技术，在六个项目评估中提升了隐私保护，且不影响预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决使用机器学习进行JIT缺陷预测时，软件分析数据的隐私泄露问题，改善现有匿名化方法未能考虑软件指标上下文依赖的不足。

Method: 将提交分组到基于特征的集群，使用LLM为每个提交集群生成上下文感知的参数配置，定义用于匿名化的alpha - beta比率和 churn混合分布。

Result: 在六个项目上的评估表明，该基于LLM的方法达到隐私级别2（IPR >= 80%），比四个最先进的基于图的匿名化基线提高了18%到25%的隐私，同时保持了相当的F1分数。

Conclusion: 当提供相似数据点的特定集群统计信息时，LLM可以作为自适应匿名化引擎，实现上下文敏感和隐私保护的软件分析，且不影响预测准确性。

Abstract: The increasing use of machine learning (ML) for Just-In-Time (JIT) defect prediction raises concerns about privacy leakage from software analytics data. Existing anonymization methods, such as tabular transformations and graph perturbations, often overlook contextual dependencies among software metrics, leading to suboptimal privacy-utility tradeoffs. Leveraging the contextual reasoning of Large Language Models (LLMs), we propose a cluster-guided anonymization technique that preserves contextual and statistical relationships within JIT datasets. Our method groups commits into feature-based clusters and employs an LLM to generate context-aware parameter configurations for each commit cluster, defining alpha-beta ratios and churn mixture distributions used for anonymization. Our evaluation on six projects (Cassandra, Flink, Groovy, Ignite, OpenStack, and Qt) shows that our LLM-based approach achieves privacy level 2 (IPR >= 80 percent), improving privacy by 18 to 25 percent over four state-of-the-art graph-based anonymization baselines while maintaining comparable F1 scores. Our results demonstrate that LLMs can act as adaptive anonymization engines when provided with cluster-specific statistical information about similar data points, enabling context-sensitive and privacy-preserving software analytics without compromising predictive accuracy.

</details>


### [285] [The Role of AI in Modern Penetration Testing](https://arxiv.org/abs/2512.12326)
*J. Alexander Curtis,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本文对AI重塑渗透测试进行系统文献综述，发现AI辅助渗透测试虽处早期但有进展，多数研究聚焦于发现和利用阶段，实际应用有限，但展现出潜力，不过仍存在挑战，也指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂性增加，传统渗透测试耗时且效率低，迫切需要更具扩展性和高效性的测试方法，因此研究AI在渗透测试中的作用。

Method: 对主要学术数据库中的58篇经同行评审的研究进行系统文献综述。

Result: AI辅助渗透测试仍处于早期，通过强化学习取得了显著进展；研究主要集中在发现和利用阶段，虽现实应用有限但有潜力；当前模型缺乏灵活性，侦察和后利用阶段研究不足，大语言模型应用研究较少。

Conclusion: 本文对AI在渗透测试中的现状和潜在作用进行了批判性概述，为从业者、研究者和组织提供了有价值的见解，也指出了未来研究方向。

Abstract: Penetration testing is a cornerstone of cybersecurity, traditionally driven by manual, time-intensive processes. As systems grow in complexity, there is a pressing need for more scalable and efficient testing methodologies. This systematic literature review examines how Artificial Intelligence (AI) is reshaping penetration testing, analyzing 58 peer-reviewed studies from major academic databases. Our findings reveal that while AI-assisted pentesting is still in its early stages, notable progress is underway, particularly through Reinforcement Learning (RL), which was the focus of 77% of the reviewed works. Most research centers on the discovery and exploitation phases of pentesting, where AI shows the greatest promise in automating repetitive tasks, optimizing attack strategies, and improving vulnerability identification. Real-world applications remain limited but encouraging, including the European Space Agency's PenBox and various open-source tools. These demonstrate AI's potential to streamline attack path analysis, analyze complex network topology, and reduce manual workload. However, challenges persist: current models often lack flexibility and are underdeveloped for the reconnaissance and post-exploitation phases of pentesting. Applications involving Large Language Models (LLMs) remain relatively under-researched, pointing to a promising direction for future exploration. This paper offers a critical overview of AI's current and potential role in penetration testing, providing valuable insights for researchers, practitioners, and organizations aiming to enhance security assessments through advanced automation or looking for gaps in existing research.

</details>


### [286] [ATLAS: Automated Tree-based Language Analysis System for C and C++ source programs](https://arxiv.org/abs/2512.12507)
*Jaid Monwar Chowdhury,Ahmad Farhan Shahriar Chowdhury,Humayra Binte Monwar,Mahmuda Naznin*

Main category: cs.SE

TL;DR: 文章指出传统编程分析技术在现代软件系统中不足，机器学习和大语言模型也有局限，尤其在C和C++分析上。由此推出Python CLI工具ATLAS，用于改善软件工程和基于机器学习的程序理解。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统复杂性增加，传统编程分析技术不足，机器学习和大语言模型在代码结构关系理解上有局限，C和C++难以进行准确静态分析。

Method: 引入基于Python的命令行界面工具ATLAS，生成语句级的控制流图和类型感知的数据流图，处理多文件项目，能在可编译和不可编译代码上工作，并使用抽象语法树、控制流图和数据流图生成统一多视图代码表示。

Result: 开发出ATLAS工具，并提供视频演示和工具仓库链接。

Conclusion: ATLAS保留了基本的结构和语义信息，为改善下游软件工程和基于机器学习的程序理解提供了实用基础。

Abstract: The growing complexity of modern software systems has highlighted the shortcomings of traditional programming analysis techniques, particularly for Software Engineering (SE) tasks. While machine learning and Large Language Models (LLMs) offer promising solutions, their effectiveness is limited by the way they interpret data. Unlike natural language, source code meaning is defined less by token adjacency and more by complex, long-range, and structural relationships and dependencies. This limitation is especially pronounced for C and C++, where flatter syntactic hierarchies, pointer aliasing, multi-level indirection, typedef-based type obfuscation, and function-pointer calls hinder accurate static analysis. To address these challenges, this paper introduces ATLAS, a Python-based Command-Line Interface (CLI) that (i) generates statement-level Control Flow Graphs (CFG) and type-aware Data Flow Graphs (DFG) that capture inter-functional dependencies for the entire program; (ii) has the ability to work on entire C and C++ projects comprising multiple files; (iii) works on both compilable and non-compilable code and (iv) produces a unified multi-view code representation using Abstract Syntax Trees (AST), CFG and DFG. By preserving essential structural and semantic information, ATLAS provides a practical foundation for improving downstream SE and machine-learning-based program understanding. Video demonstration: https://youtu.be/RACWQe5ELwY Tool repository: https://github.com/jaid-monwar/ATLAS-code-representation-tool

</details>


### [287] [Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?](https://arxiv.org/abs/2512.12536)
*Arastoo Zibaeirad,Marco Vieira*

Main category: cs.SE

TL;DR: 本文提出DVDR - LLM集成框架用于软件漏洞检测与修复，相比单个模型检测准确率更高，对多文件漏洞召回率和F1分数也有提升，但存在权衡问题。


<details>
  <summary>Details</summary>
Motivation: 单个大语言模型在识别复杂漏洞和生成修复方案时存在困难，研究聚合多个模型是否能降低错误率。

Method: 提出DVDR - LLM集成框架，结合不同大语言模型的输出。

Result: DVDR - LLM比单个模型检测准确率高10 - 12%，对多文件漏洞召回率提升18%，F1分数提升11.8%；但存在降低验证任务误报同时增加检测任务漏报的权衡。

Conclusion: 使用集成框架能提高性能，但需根据不同安全场景仔细确定大语言模型间的一致性阈值。

Abstract: Large Language Models (LLMs) are increasingly being studied for Software Vulnerability Detection (SVD) and Repair (SVR). Individual LLMs have demonstrated code understanding abilities, but they frequently struggle when identifying complex vulnerabilities and generating fixes.
  This study presents DVDR-LLM, an ensemble framework that combines outputs from diverse LLMs to determine whether aggregating multiple models reduces error rates. Our evaluation reveals that DVDR-LLM achieves 10-12% higher detection accuracy compared to the average performance of individual models, with benefits increasing as code complexity grows. For multi-file vulnerabilities, the ensemble approach demonstrates significant improvements in recall (+18%) and F1 score (+11.8%) over individual models. However, the approach raises measurable trade-offs: reducing false positives in verification tasks while simultaneously increasing false negatives in detection tasks, requiring careful decision on the required level of agreement among the LLMs (threshold) for increased performance across different security contexts.
  Artifact: https://github.com/Erroristotle/DVDR_LLM

</details>


### [288] [Assessing the Capability of Android Dynamic Analysis Tools to Combat Anti-Runtime Analysis Techniques](https://arxiv.org/abs/2512.12551)
*Dewen Suo,Lei Xue,Weihao Huang,Runze Tan,Guozi Sun*

Main category: cs.SE

TL;DR: 论文研究安卓动态分析工具绕过ARA技术能力，发现现有工具存在关键缺陷，需更强大解决方案。


<details>
  <summary>Details</summary>
Motivation: 安卓应用增长带来安全威胁，恶意应用采用ARA技术阻碍动态分析，影响平台安全。

Method: 进行全面实证研究，评估常用安卓动态分析工具绕过各种ARA技术的能力。

Result: 现有动态分析工具在对抗ARA机制方面存在关键差距。

Conclusion: 现有工具存在局限性，需要改进方法对抗ARA技术，推动软件安全和动态分析领域发展。

Abstract: As the dominant mobile operating system, Android continues to attract a substantial influx of new applications each year. However, this growth is accompanied by increased attention from malicious actors, resulting in a significant rise in security threats to the Android ecosystem. Among these threats, the adoption of Anti-Runtime Analysis (ARA) techniques by malicious applications poses a serious challenge, as it hinders security professionals from effectively analyzing malicious behaviors using dynamic analysis tools. ARA technologies are designed to prevent the dynamic examination of applications, thus complicating efforts to ensure platform security. This paper presents a comprehensive empirical study that assesses the ability of widely-used Android dynamic analysis tools to bypass various ARA techniques. Our findings reveal a critical gap in the effectiveness of existing dynamic analysis tools to counter ARA mechanisms, highlighting an urgent need for more robust solutions. This work provides valuable insights into the limitations of existing tools and highlights the need for improved methods to counteract ARA technologies, thus advancing the field of software security and dynamic analysis.

</details>


### [289] [SHERLOCK: A Deep Learning Approach To Detect Software Vulnerabilities](https://arxiv.org/abs/2512.12593)
*Saadh Jawwadh,Guhanathan Poravi*

Main category: cs.SE

TL;DR: 本文采用CNN解决软件漏洞检测问题，实验表明其有潜力提升检测准确率，但因缺乏标准数据集部分漏洞检测性能不佳。


<details>
  <summary>Details</summary>
Motivation: 软件对生活影响增大，传统检测技术难以检测多种漏洞，需要新方法。

Method: 使用CNN，以标记化源代码为输入，采用5折交叉验证训练和评估模型。

Result: Sherlock在函数层面成功检测多种漏洞，对部分类型漏洞表现好，整体准确率高、正负检测值显著，但部分漏洞检测性能不佳。

Conclusion: 相比现有技术，所提深度学习方法有潜力大幅提升软件漏洞检测准确率，缺乏标准数据集是未来研究方向。

Abstract: The increasing reliance on software in various applications has made the problem of software vulnerability detection more critical. Software vulnerabilities can lead to security breaches, data theft, and other negative outcomes. Traditional software vulnerability detection techniques, such as static and dynamic analysis, have been shown to be ineffective at detecting multiple vulnerabilities.
  To address this issue, this study employed a deep learning approach, specifically Convolutional Neural Networks (CNN), to solve the software vulnerability detection problem. A 5-split cross-validation approach was used to train and evaluate the CNN model, which takes tokenized source code as input.
  The findings indicated that Sherlock successfully detected multiple vulnerabilities at the function level, and its performance was particularly strong for CWE-199, CWE-120, and CWE-Other, with an overall high accuracy rate and significant true positive and true negative values. However, the performance was less reliable for some vulnerabilities due to the lack of a standardized dataset which will be a future research direction. The results suggest that compared to current techniques, the proposed deep learning approach has the potential to substantially enhance the accuracy of software vulnerability detection.

</details>


### [290] [A Systematic Analysis of Higher Education on Software Engineering in the Netherlands](https://arxiv.org/abs/2512.12650)
*Bastiaan Heeren,Fabiano Dalpiaz,Mazyar Seraj,Roberto Verdecchia,Vadim Zaytsev*

Main category: cs.SE

TL;DR: 本文对荷兰软件工程高等教育进行分析，发现各知识领域覆盖情况及相关性，还指出可纳入课程的欠缺领域，邀请全球研究人员对比。


<details>
  <summary>Details</summary>
Motivation: 理解软件工程高等教育现状，助力教师评估、优化课程和提升课程体系。

Method: 采用众包分析方法，分析荷兰10所大学207门课程，基于SWEBOK的知识领域进行映射，经同质化和内部一致性改进后进行数据分析。

Result: 发现建设与编程在本科阶段覆盖最多，一些知识领域本科和硕士相同，高级领域硕士阶段覆盖多；确定三组紧密耦合知识领域；荷兰大学知识领域覆盖总体均匀；发现知识领域相关性和潜在提升整合学习的能力；指出软件工程经济学等欠缺领域。

Conclusion: 结果凸显关键知识领域相关性和提升整合学习潜能，邀请研究人员用此方法对比全球软件工程教育项目。

Abstract: Software engineering educators strive to continuously improve their courses and programs. Understanding the current state of practice of software engineering higher education can empower educators to critically assess their courses, fine-tune them by benchmarking against observed practices, and ultimately enhance their curricula. In this study, we aim to provide an encompassing analysis of higher education on software engineering by considering the higher educational offering of an entire European country, namely the Netherlands. We leverage a crowd-sourced analysis process by considering 10 Dutch universities and 207 university courses. The courses are analysed via knowledge areas adopted from the SWEBOK. The mapping process is refined via homogenisation and internal consistency improvement phases, and is followed by a data analysis phase. Given its fundamental nature, Construction and Programming is the most covered knowledge area at Bachelor level. Other knowledge areas are equally covered at Bachelor and Master level (e.g., software engineering models), while more advanced ones are almost exclusively covered at Master level. We identify three clusters of tightly coupled knowledge areas: (i) requirements, architecture, and design, (ii) testing, verification, and security, and (iii) process-oriented and DevOps topics. Dutch universities generally cover all knowledge areas uniformly, with minor deviations reflecting institutional research strengths. Our results highlight correlations among key knowledge areas and their potential for enhancing integrated learning. We also identify underrepresented areas, such as software engineering economics, which educators may consider including in curricula. We invite researchers to use our research method in their own geographical region, in order to contrast software engineering education programs across the globe.

</details>


### [291] [Attributes to Support the Formulation of Practically Relevant Research Problems in Software Engineering](https://arxiv.org/abs/2512.12699)
*Anrafel Fernandes Pereira,Maria Teresa Baldassarre,Daniel Mendez,Jürgen Börstler,Nauman bin Ali,Rahul Mohanani,Darja Smite,Stefan Biffl,Rogardt Heldal,Davide Falessi,Daniel Graziotin,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 文章介绍并评估软件工程研究问题制定的七个属性，通过研讨会得出其对制定行业导向研究问题很重要，适配具体情境可促进学术与行业需求对齐。


<details>
  <summary>Details</summary>
Motivation: 软件工程早期研究问题制定缺乏结构化指导，需引入并评估七个属性的重要性、完整性及应用方法。

Method: 在ISERN 2024会议上对42位资深软件工程研究人员开展研讨会，用问题愿景板展示属性，分组讨论、收集书面反馈并进行调查。

Result: 确认七个属性对制定行业导向研究问题重要，得到实践应用示例和改进建议。

Conclusion: 七个属性对反思性和情境感知的问题制定很重要，适配具体研究情境可改善学术与行业需求的一致性。

Abstract: [Background] A well-formulated research problem is essential for achieving practical relevance in Software Engineering (SE), yet there is a lack of structured guidance in this early phase. [Aims] Our goal is to introduce and evaluate seven attributes identified in the SE literature as relevant for formulating research problems (practical problem, context, implications/impacts, practitioners, evidence, objective, and research questions) in terms of their perceived importance and completeness, and learn how they can be applied. [Method] We conducted a workshop with 42 senior SE researchers during the ISERN 2024 meeting. The seven attributes were presented using a Problem Vision board filled with a research example. Participants discussed attributes in groups, shared written feedback, and individually completed a survey assessing their importance, completeness, and suggestions for improvement. [Results] The findings confirm the importance of the seven attributes in the formulation of industry-oriented research problems. Qualitative feedback illustrated how they can be applied in practice and revealed suggestions to refine them, such as incorporating financial criteria (e.g., ROI) into implications/impacts and addressing feasibility and constraints under evidence. [Conclusion] The results reaffirm the importance of the seven attributes in supporting a reflective and context-aware problem formulation. Adapting their use to specific research contexts can help to improve the alignment between academic research and industry needs.

</details>


### [292] [Towards AI Agents Supported Research Problem Formulation](https://arxiv.org/abs/2512.12719)
*Anrafel Fernandes Pereira,Maria Teresa Baldassarre,Daniel Mendez,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文探讨使用人工智能代理支持软件工程研究人员在项目早期阶段明确研究问题，基于Lean Research Inception框架开展描述性评估，显示有积极效果，但需实证验证。


<details>
  <summary>Details</summary>
Motivation: 解决软件工程研究问题表述不佳，未反映工业实践复杂性的问题，支持研究人员在早期阶段明确研究问题。

Method: 基于Lean Research Inception框架，以代码可维护性研究为参考，对人工智能代理融入框架支持研究人员的场景进行描述性评估。

Result: 描述性评估表明人工智能代理支持可丰富协作讨论，增强对研究问题价值、可行性和适用性的批判性反思。

Conclusion: 将人工智能代理融入Lean Research Inception框架以支持研究问题的制定具有前景，但需要实证验证和完善。

Abstract: Poorly formulated research problems can compromise the practical relevance of Software Engineering studies by not reflecting the complexities of industrial practice. This vision paper explores the use of artificial intelligence agents to support SE researchers during the early stage of a research project, the formulation of the research problem. Based on the Lean Research Inception framework and using a published study on code maintainability in machine learning as a reference, we developed a descriptive evaluation of a scenario illustrating how AI agents, integrated into LRI, can support SE researchers by pre filling problem attributes, aligning stakeholder perspectives, refining research questions, simulating multiperspective assessments, and supporting decision making. The descriptive evaluation of the scenario suggests that AI agent support can enrich collaborative discussions and enhance critical reflection on the value, feasibility, and applicability of the research problem. Although the vision of integrating AI agents into LRI was perceived as promising to support the context aware and practice oriented formulation of research problems, empirical validation is needed to confirm and refine the integration of AI agents into problem formulation.

</details>


### [293] [Temporal HAL-API Dependencies as a Gateway to Formal Embedded Software Development](https://arxiv.org/abs/2512.12788)
*Manuel Bentele,Andreas Podelski,Axel Sikora,Bernd Westphal*

Main category: cs.SE

TL;DR: THADs可用于嵌入式软件开发中捕获正确性属性，在规范和验证上有优势，有望推动形式化方法在工业嵌入式软件开发中的应用。


<details>
  <summary>Details</summary>
Motivation: 寻找在嵌入式软件开发中能有效捕获正确性属性且在规范和验证方面有优势的方法，以推动形式化方法更广泛经济的应用。

Method: 通过程序注释进行规范，借助软件模型检查自动验证。

Result: THADs在规范和验证方面有一定优势，处于通用属性和特定应用属性之间的合适位置。

Conclusion: THADs可能成为形式化方法在工业嵌入式软件开发中更广泛经济应用的途径。

Abstract: Temporal HAL-API Dependencies (THADs) can be useful to capture an interesting class of correctness properties in embedded software development. They demand a moderate effort for specification (which can be done via program annotations) and verification (which can be done automatically via software model checking). In this sense, they have the potential to form an interesting sweet spot between generic properties (that demand virtually no specification effort, and that are typically addressed by static analysis) and application-specific properties as addressed by full-fledged formal methods. Thus, they may form a gateway to wider and more economic use of formal methods in industrial embedded software development.

</details>


### [294] [Challenges and Enablers: Remote Work for People with Disabilities in Software Development Teams](https://arxiv.org/abs/2512.12965)
*Thayssa Rocha,Luciano Teran,Marcelle Mota,Cleidson de Souza,Kiev Gama,Gustavo Pinto*

Main category: cs.SE

TL;DR: 研究探讨远程工作对残障人士在软件开发团队经历的影响，指出残障成员的队友和领导对其远程协作挑战认知有限，有改善空间。


<details>
  <summary>Details</summary>
Motivation: 技术领域远程和混合工作模式兴起，为残障人士融入软件开发团队带来新机遇与挑战，研究其在远程工作中的经历。

Method: 进行在线调查收集有效回应，对残障开发者进行结构化访谈，结合定量数据与开放式调查回应及访谈记录的定性编码。

Result: 残障团队成员面临障碍，但队友和领导对维持远程协作日常挑战认知有限。

Conclusion: 在无障碍工具、沟通策略和适应性管理方法方面有改进机会。

Abstract: The increasing adoption of remote and hybrid work modalities in the technology sector has brought new opportunities and challenges for the inclusion of people with disabilities (PWD) in software development teams (SDT). This study investigates how remote work affects PWDs' experience in mixed-ability SDT, focusing on the unique challenges and strategies that emerge in remote environments. We conducted an online survey with \totalSurveyResponses valid responses, encompassing PWD, their leaders, and teammates, to capture sociotechnical aspects of their experiences with remote collaboration. To deepen our understanding, we carried out 14 structured interviews with software developers who self-identified as having disabilities (six autistic individuals, six with physical disabilities, and two who are d/Deaf). Our analysis combines quantitative data with qualitative coding of open-ended survey responses and interview transcripts. The results reveal that, despite the barriers faced by team members with disabilities, their teammates and leaders have a limited perception of the daily challenges involved in sustaining collaborative remote work. These findings highlight opportunities for improvement in accessibility tools, communication strategies, and adaptive management approaches.

</details>


### [295] [A Decision Support Framework for Blockchain Pattern Selection Based on Soft Goals](https://arxiv.org/abs/2512.13239)
*Eddy Kiomba Kambilo,Nicolas Herbaut,Irina Rychkova,Carine Souveyet*

Main category: cs.SE

TL;DR: 提出区块链技术感知企业建模框架BC - TEAEM支持系统选择区块链模式，并通过案例验证适用性


<details>
  <summary>Details</summary>
Motivation: 区块链虽有积极影响，但引入约束和负面影响，且模式选择因缺乏标准框架而复杂

Method: 结合区块链模式本体、独立于领域的软目标和多标准决策方法提出BC - TEAEM框架，注重领域和技术专家互动

Result: 开发实现该方法的决策支持工具原型，通过制药公司供应链溯源系统案例验证框架适用性

Conclusion: BC - TEAEM框架可支持系统选择区块链模式，具有适用性

Abstract: Blockchain technology is gaining momentum across many sectors. Whereas blockchain solutions have important positive effects on the business domain, they also introduce constraints and may cause delayed or unforeseen negative effects, undermining business strategies. The diversity of blockchain patterns and lack of standardized frameworks linking business goals to technical design decisions make pattern selection a complex task for system architects. To address this challenge, we propose Blockchain--Technology-Aware Enterprise Modeling (BC-TEAEM), a decision support framework that combines ontologies of blockchain patterns and domain-independent soft goals with a multi-criteria decision-making approach. The framework focuses on the interplay between a domain expert and a technical expert to ensure alignment and traceability. By iteratively capturing and refining preferences, BC-TEAEM supports systematic selection of blockchain patterns. We develop a prototype decision support tool implementing our method and validate it through a case study of a pharmaceutical company's supply chain traceability system, demonstrating the framework's applicability. %a supply chain traceability case study.

</details>


### [296] [UCRBench: Benchmarking LLMs on Use Case Recovery](https://arxiv.org/abs/2512.13360)
*Shuyuan Xiao,Yiran Zhang,Weisong Sun,Xiaohong Chen,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: 本文引入代码对齐的用例基准，对大语言模型生成用例进行系统研究并评估，发现模型有一定能力但性能差异大且存在诸多不足。


<details>
  <summary>Details</summary>
Motivation: 现有用例基准稀缺且可能与实际系统行为不一致，限制了对大语言模型生成用例的严格评估。

Method: 引入代码对齐的用例基准，对九个真实软件项目的用户目标和子功能用例进行手动验证，提出分层评估协议。

Result: 大语言模型能部分重构系统功能，但在不同项目中性能差异大，在特定领域和多模块系统中表现不佳，存在高遗漏率和抽象一致性问题。

Conclusion: 指出了基于大语言模型的用例逆向工程的潜力和当前局限性。

Abstract: Use cases are widely employed to specify functional requirements, yet existing benchmarks are scarce and face the risk of being misaligned with actual system behavior, similarly limiting the rigorous evaluation of large language models (LLMs) in generating use cases from source code. We address this gap by introducing code-aligned use case benchmarks, constructed through manual validation of both user-goal and subfunction use cases across nine real-world software projects. Using this benchmark, we conduct the first systematic study of LLMs and propose a hierarchical evaluation protocol that assesses actor correctness, name accuracy, path fidelity, and behavioral coverage. The results show that while LLMs can partially reconstruct system functionality, their performance varies significantly across projects, with particularly noticeable shortcomings in domain-specific and multi-module systems. The models also exhibit high omission rates and struggle to maintain consistent abstraction when aggregating subfunctions into user-goal use cases, highlighting both the potential and current limitations of LLM-based use case reverse engineering.

</details>


### [297] [PSALM: applying Proportional SAmpLing strategy in Metamorphic testing](https://arxiv.org/abs/2512.13414)
*Zenghui Zhou,Pak-Lok Poon,Zheng Zheng,Xiao-Yi Zhang*

Main category: cs.SE

TL;DR: 本文提出适应于变异测试（MT）的PSALM策略用于源测试用例和变异组选择，理论证明其不劣于随机选择，实证表明比现有策略更有效。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注变异关系（MRs），源测试用例和变异组选择的系统方法未充分探索，传统比例抽样策略（PSS）假设不能直接用于MT。

Method: 提出PSALM策略，对源测试用例和变异组进行选择，从理论上证明其性质并进行实证研究。

Result: 实证研究结果与理论分析一致，PSALM比现有策略如ART和MT - ART更有效。

Conclusion: PSALM为MT提供了有理论基础且实际有效的选择策略。

Abstract: Metamorphic testing (MT) alleviates the oracle problem by checking metamorphic relations (MRs) across multiple test executions. The fault detection effectiveness of MT is influenced not only by the choice and quality of MRs, but also by how source test cases and metamorphic groups (MGs) are selected. While substantial research has focused on designing, generating, and validating MRs, systematic methods for source test case selection and MG selection remain largely unexplored. Although the Proportional Sampling Strategy (PSS) provides strong theoretical guarantees in traditional testing, its assumptions cannot be directly applied in MT due to differences in selection domains, test units, and failure distributions. This paper proposes PSALM, an adaptation of PSS to MT for both source test case selection and MG selection. We formally prove that PSALM is never inferior to random selection regardless of how the source test case and MG domains are partitioned. We further identify the conditions under which applying PSALM to source test case selection and MG selection yields identical effectiveness. A comprehensive empirical study on eight subject programs and 184 mutants shows that the results are consistent with our theoretical analysis and that PSALM generally performs more effectively than existing selection strategies such as ART and MT-ART. These results demonstrate that PSALM provides a theoretically grounded and practically effective selection strategy for MT.

</details>


### [298] [QMon: Monitoring the Execution of Quantum Circuits with Mid-Circuit Measurement and Reset](https://arxiv.org/abs/2512.13422)
*Ning Ma,Jianjun Zhao,Foutse Khomh,Shaukat Ali,Heng Li*

Main category: cs.SE

TL;DR: 本文提出 QMON 方法监测量子电路内部状态，实验表明其能有效检测和定位错误，助力量子软件开发。


<details>
  <summary>Details</summary>
Motivation: 量子电路特性使直接观测和复制状态困难，导致执行监测、调试和运行时监控等任务复杂。

Method: 利用中间电路测量和重置操作，在开发者指定位置插入监测算子，比较预期和观测的量子态概率。

Result: 对 154 个量子电路实验显示，电路仪器化后保留原有功能，QMON 能成功检测和定位编程错误，虽监测覆盖受量子特性限制，但对原量子态干扰小。

Conclusion: QMON 有助于开发更健壮可靠的量子软件。

Abstract: Unlike classical software, where logging and runtime tracing can effectively reveal internal execution status, quantum circuits possess unique properties, such as the no-cloning theorem and measurement-induced collapse, that prevent direct observation or duplication of their states. These characteristics make it especially challenging to monitor the execution of quantum circuits, complicating essential tasks such as debugging and runtime monitoring. This paper presents QMON, a practical methodology that leverages mid-circuit measurements and reset operations to monitor the internal states of quantum circuits while preserving their original runtime behavior. QMON enables the instrumentation of monitoring operators at developer-specified locations within the circuit, allowing comparisons between expected and observed quantum-state probabilities at those locations. We evaluated QMON by analyzing its impact on circuit behavior, monitoring coverage, and effectiveness in bug localization. Experimental results involving 154 quantum circuits show that all circuits preserve their intended functionality after instrumentation and that QMON successfully detects and localizes various programming errors. Although monitoring coverage is limited by the need to preserve delicate quantum properties, such as entanglement, QMON effectively detects errors while introducing no or negligible disturbance to the original quantum states. QMON facilitates the development of more robust and reliable quantum software as the field continues to mature.

</details>


### [299] [From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents](https://arxiv.org/abs/2512.13438)
*Dezhi Ran,Zhi Gong,Yuzhe Guo,Mengzhou Wu,Yuan Cao,Haochuan Lu,Hengyu Zhang,Xia Zeng,Gang Cao,Liangchao Yao,Yuetang Deng,Wei Yang,Tao Xie*

Main category: cs.SE

TL;DR: 现有大语言模型代理进行自动化 UI 导航时效率被忽视，本文提出 UIFormer 框架优化 UI 表示，评估显示其能减少 token 数量并提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理自动化 UI 导航效率被忽视，低效 UI 表示是性能瓶颈，且 UI 表示优化存在缺乏布尔预言机和搜索空间大且易出错两个挑战。

Method: 提出 UIFormer 框架，用特定领域语言限制程序空间，进行基于大语言模型的迭代优化以实现效率 - 完整性共同优化，作为轻量级插件与现有大语言模型代理集成。

Result: 在三个 UI 导航基准测试中，使用五个大语言模型，UIFormer 能减少 48.7% - 55.8%的 token 数量，运行开销小，可维持或提升代理性能；在微信的实际部署验证了其实用性。

Conclusion: UIFormer 能有效优化 UI 表示，在自动化 UI 导航中具有优势和实际应用价值。

Abstract: While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.

</details>


### [300] [A Data Annotation Requirements Representation and Specification (DARS)](https://arxiv.org/abs/2512.13444)
*Yi Peng,Hina Saeeda,Hans-Martin Heyn,Jennifer Horkoff,Eric Knauss,Fredrick Warg*

Main category: cs.SE

TL;DR: 本文提出数据标注需求表示与规范DARS，解决数据标注需求表示问题，经评估可缓解标注错误根源，提升系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着AI网络物理系统兴起，数据标注是开发关键但常被忽视的过程，现有工作未解决数据标注及相关需求的独特挑战，需要特定需求表示。

Method: 提出Data Annotation Requirements Representation and Specification (DARS)，包括Annotation Negotiation Card和Scenario - Based Annotation Specification，用汽车感知案例和18种真实数据标注错误类型映射进行评估。

Result: DARS缓解了完整性、准确性和一致性标注错误的根源。

Conclusion: 将DARS集成到需求工程中可提高使用数据标注的安全关键系统的可靠性，表明工程框架需为智能信息系统的数据依赖组件发展。

Abstract: With the rise of AI-enabled cyber-physical systems, data annotation has become a critical yet often overlooked process in the development of these intelligent information systems. Existing work in requirements engineering (RE) has explored how requirements for AI systems and their data can be represented. However, related interviews with industry professionals show that data annotations and their related requirements introduce distinct challenges, indicating a need for annotation-specific requirement representations. We propose the Data Annotation Requirements Representation and Specification (DARS), including an Annotation Negotiation Card to align stakeholders on objectives and constraints, and a Scenario-Based Annotation Specification to express atomic and verifiable data annotation requirements. We evaluate DARS with an automotive perception case related to an ongoing project, and a mapping against 18 real-world data annotation error types. The results suggest that DARS mitigates root causes of completeness, accuracy, and consistency annotation errors. By integrating DARS into RE, this work improves the reliability of safety-critical systems using data annotations and demonstrates how engineering frameworks must evolve for data-dependent components of today's intelligent information systems.

</details>


### [301] [Mapping of the system of software-related emissions and shared responsibilities](https://arxiv.org/abs/2512.13474)
*Laura Partanen,Antti Sipila,Md Sanaul Haque,Jari Porras*

Main category: cs.SE

TL;DR: 全球气候变暖，ICT 行业是温室气体排放大户，本研究进行系统映射以提升对软件相关排放及 ICT 行业责任的认识。


<details>
  <summary>Details</summary>
Motivation: 全球气候快速变暖，ICT 行业环境影响扩大，为实现《巴黎协定》目标，欧盟有相关监管措施，需提升对 ICT 行业软件相关排放及责任的认识。

Method: 进行全面的系统映射。

Result: 识别出 ICT 领域碳排放和能源消耗的主要来源，明确了软件生命周期中利益相关者的关键责任。

Conclusion: 通过系统映射可增强对软件相关排放和 ICT 行业责任的认知。

Abstract: The global climate is experiencing a rapid and unprecedented warming trend. The ICT sector is a notable contributor to global greenhouse gas emissions, with its environmental impact continuing to expand. Addressing this issue is vital for achieving the objectives of the Paris Agreement, particularly the goal of limiting global temperature rise to 1.5°C. At the European Union level, regulatory measures such as the CSRD and the CSDD impose obligations on companies, including those within the ICT sector, to recognize and mitigate their environmental footprint. This study provides a comprehensive system mapping aimed at enhancing the awareness and understanding of software-related emissions and the corresponding responsibilities borne by the ICT sector. The mapping identifies the primary sources of carbon emissions and energy consumption within the ICT domain while also outlining the key responsibilities of the stakeholders accountable throughout the software lifecycle.

</details>


### [302] [Fine-tuned LLM-based Code Migration Framework](https://arxiv.org/abs/2512.13515)
*Oleg Grynets,Vasyl Lyashkevych,Dmytro Baran,Maksym Orliansky,Taras Zelenyy,Markiian Leshchyshyn*

Main category: cs.SE

TL;DR: 研究自动化代码库迁移，提出框架用大语言模型解决SQL迁移问题，结果显示微调重要，方法提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决基于SQL系统过渡时的挑战，实现现代数据库转换。

Method: 提出迁移框架，整合微调大语言模型，权衡微调与提示工程，采用目标评估和计算指标，引入创新方法。

Result: 降低语法错误率，增强迁移迭代中特征对齐，利用数据集采样持续改进，嵌入GAI提高效率。

Conclusion: 微调在迁移中起重要作用，所提方法能有效解决SQL迁移问题，提升工作流效率。

Abstract: The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.

</details>


### [303] [How Low Can You Go? The Data-Light SE Challenge](https://arxiv.org/abs/2512.13524)
*Kishan Kumar Ganguly,Tim Menzies*

Main category: cs.SE

TL;DR: 论文指出软件工程研究中简单方法用少量标签就能达近90%最佳结果，与先进优化器表现相当，提出数据轻量挑战并给出相关贡献。


<details>
  <summary>Details</summary>
Motivation: 验证软件工程研究中依赖大量数据集和CPU密集型优化器这一假设是否成立。

Method: 对软件工程文献中多个优化问题进行研究，对比简单方法和先进优化器的表现。

Result: 简单方法用少量标签能达近90%最佳结果，与先进优化器表现相当。

Conclusion: 一些软件工程任务用轻量级方法更好，提出数据轻量挑战并给出相关研究贡献。

Abstract: Much of software engineering (SE) research assumes that progress depends on massive datasets and CPU-intensive optimizers. Yet has this assumption been rigorously tested?
  The counter-evidence presented in this paper suggests otherwise: across dozens of optimization problems from recent SE literature, including software configuration and performance tuning, cloud and systems optimization, project and process-level decision modeling, behavioral analytics, financial risk modeling, project health prediction, reinforcement learning tasks, sales forecasting, and software testing, even with just a few dozen labels, very simple methods (e.g. diversity sampling, a minimal Bayesian learner, or random probes) achieve near 90% of the best reported results. Further, these simple methods perform just as well as more state-of-the-the-art optimizers like SMAC, TPE, DEHB etc. While some tasks would require better outcomes and more sampling, these results seen after a few dozen samples would suffice for many engineering needs (particularly when the goal is rapid and cost-efficient guidance rather than slow and exhaustive optimization).
  Our results highlight that some SE tasks may be better served by lightweight approaches that demand fewer labels and far less computation. We hence propose the data-light challenge: when will a handful of labels suffice for SE tasks? To enable a large-scale investigation of this issue, we contribute (1) a mathematical formalization of labeling, (2) lightweight baseline algorithms, and (3) results on public-domain data showing the conditions under which lightweight methods excel or fail.
  For the purposes of open science, our scripts and data are online at https://github.com/KKGanguly/NEO .

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [304] [EXFormer: A Multi-Scale Trend-Aware Transformer with Dynamic Variable Selection for Foreign Exchange Returns Prediction](https://arxiv.org/abs/2512.12727)
*Dinggao Liu,Robert Ślepaczuk,Zhenpeng Tang*

Main category: q-fin.CP

TL;DR: 提出EXFormer架构预测每日汇率回报，有新机制，在多汇率对测试中表现优于基线，能提供经济价值预测和汇率驱动因素见解。


<details>
  <summary>Details</summary>
Motivation: 汇率回报受多种市场因素驱动且高频波动，准确预测是国际金融长期挑战，因此提出新架构进行预测。

Method: 提出EXFormer架构，引入多尺度趋势感知自注意力机制、动态变量选择器和嵌入式挤压激励块；使用多汇率对的每日数据进行不同滑动窗口的样本外评估和交易回测。

Result: EXFormer在预测中持续优于随机游走和其他基线，提高方向性准确率；交易回测有可观累计回报和夏普比率，考虑成本后仍有正回报；鲁棒性检查确认其在高波动和熊市制度下的优越性。

Conclusion: EXFormer能为国际投资者、企业和央行从业者提供经济上有价值的预测和对汇率动态驱动因素的透明、时变见解。

Abstract: Accurately forecasting daily exchange rate returns represents a longstanding challenge in international finance, as the exchange rate returns are driven by a multitude of correlated market factors and exhibit high-frequency fluctuations. This paper proposes EXFormer, a novel Transformer-based architecture specifically designed for forecasting the daily exchange rate returns. We introduce a multi-scale trend-aware self-attention mechanism that employs parallel convolutional branches with differing receptive fields to align observations on the basis of local slopes, preserving long-range dependencies while remaining sensitive to regime shifts. A dynamic variable selector assigns time-varying importance weights to 28 exogenous covariates related to exchange rate returns, providing pre-hoc interpretability. An embedded squeeze-and-excitation block recalibrates channel responses to emphasize informative features and depress noise in the forecasting. Using the daily data for EUR/USD, USD/JPY, and GBP/USD, we conduct out-of-sample evaluations across five different sliding windows. EXFormer consistently outperforms the random walk and other baselines, improving directional accuracy by a statistically significant margin of up to 8.5--22.8%. In nearly one year of trading backtests, the model converts these gains into cumulative returns of 18%, 25%, and 18% for the three pairs, with Sharpe ratios exceeding 1.8. When conservative transaction costs and slippage are accounted for, EXFormer retains cumulative returns of 7%, 19%, and 9%, while other baselines achieve negative. The robustness checks further confirm the model's superiority under high-volatility and bear-market regimes. EXFormer furnishes both economically valuable forecasts and transparent, time-varying insights into the drivers of exchange rate dynamics for international investors, corporations, and central bank practitioners.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [305] [Not All Factors Crowd Equally: Modeling, Measuring, and Trading on Alpha Decay](https://arxiv.org/abs/2512.11913)
*Chorok Lee*

Main category: q-fin.PM

TL;DR: 本文从博弈论均衡模型推导出因子阿尔法衰减的双曲衰减形式，用八个Fama - French因子检验，发现双曲衰减适用于机械因子，并非所有因子拥挤程度相同，2015年后拥挤加速，平均回报有效定价，拥挤能预测尾部风险，研究扩展了均衡拥挤模型。


<details>
  <summary>Details</summary>
Motivation: 推导因子阿尔法衰减的特定函数形式并检验，研究因子拥挤现象及影响。

Method: 从博弈论均衡模型推导双曲衰减形式，用1963 - 2024年的八个Fama - French因子进行测试。

Result: 双曲衰减适合机械因子；并非所有因子拥挤程度相同；2015年后拥挤加速；平均回报有效定价；拥挤能预测尾部风险。

Conclusion: 研究扩展了均衡拥挤模型到时间动态，表明拥挤可预测尾部风险而非平均回报，对风险管理有用，对生成阿尔法收益无用。

Abstract: We derive a specific functional form for factor alpha decay -- hyperbolic decay alpha(t) = K/(1+lambda*t) -- from a game-theoretic equilibrium model, and test it against linear and exponential alternatives. Using eight Fama-French factors (1963--2024), we find: (1) Hyperbolic decay fits mechanical factors. Momentum exhibits clear hyperbolic decay (R^2 = 0.65), outperforming linear (0.51) and exponential (0.61) baselines -- validating the equilibrium foundation. (2) Not all factors crowd equally. Mechanical factors (momentum, reversal) fit the model; judgment-based factors (value, quality) do not -- consistent with a signal-ambiguity taxonomy paralleling Hua and Sun's "barriers to entry." (3) Crowding accelerated post-2015. Out-of-sample, the model over-predicts remaining alpha (0.30 vs. 0.15), correlating with factor ETF growth (rho = -0.63). (4) Average returns are efficiently priced. Crowding-based factor selection fails to generate alpha (Sharpe: 0.22 vs. 0.39 factor momentum benchmark). (5) Crowding predicts tail risk. Out-of-sample (2001--2024), crowded reversal factors show 1.7--1.8x higher crash probability (bottom decile returns), while crowded momentum shows lower crash risk (0.38x, p = 0.006). Our findings extend equilibrium crowding models (DeMiguel et al.) to temporal dynamics and show that crowding predicts crashes, not means -- useful for risk management, not alpha generation.

</details>


### [306] [Deep Hedging with Reinforcement Learning: A Practical Framework for Option Risk Management](https://arxiv.org/abs/2512.12420)
*Travon Lucius,Christian Koch,Jacob Starling,Julia Zhu,Miguel Urena,Carrie Hu*

Main category: q-fin.PM

TL;DR: 提出强化学习框架用于在现实交易成本和头寸限制下动态对冲股票指数期权风险，策略表现良好且代码可扩展。


<details>
  <summary>Details</summary>
Motivation: 在现实交易成本和头寸限制下对股票指数期权风险进行动态对冲。

Method: 基于“深度对冲”范式，设计无泄漏环境、成本感知奖励函数和轻量级随机演员 - 评论家代理，使用每日收盘面板数据训练。

Result: 学习到的策略在风险调整表现上优于无对冲、动量和波动率目标基准，换手率可控，对双倍交易成本具有鲁棒性。

Conclusion: 该学习到的覆盖层可改善投资组合的均值 - 方差权衡，文章还讨论了实际意义并指出未来工作方向。

Abstract: We present a reinforcement-learning (RL) framework for dynamic hedging of equity index option exposures under realistic transaction costs and position limits. We hedge a normalized option-implied equity exposure (one unit of underlying delta, offset via SPY) by trading the underlying index ETF, using the option surface and macro variables only as state information and not as a direct pricing engine. Building on the "deep hedging" paradigm of Buehler et al. (2019), we design a leak-free environment, a cost-aware reward function, and a lightweight stochastic actor-critic agent trained on daily end-of-day panel data constructed from SPX/SPY implied volatility term structure, skew, realized volatility, and macro rate context. On a fixed train/validation/test split, the learned policy improves risk-adjusted performance versus no-hedge, momentum, and volatility-targeting baselines (higher point-estimate Sharpe); only the GAE policy's test-sample Sharpe is statistically distinguishable from zero, although confidence intervals overlap with a long-SPY benchmark so we stop short of claiming formal dominance. Turnover remains controlled and the policy is robust to doubled transaction costs. The modular codebase, comprising a data pipeline, simulator, and training scripts, is engineered for extensibility to multi-asset overlays, alternative objectives (e.g., drawdown or CVaR), and intraday data. From a portfolio management perspective, the learned overlay is designed to sit on top of an existing SPX or SPY allocation, improving the portfolio's mean-variance trade-off with controlled turnover and drawdowns. We discuss practical implications for portfolio overlays and outline avenues for future work.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [307] [Institutionalizing risk curation in decentralized credit](https://arxiv.org/abs/2512.11976)
*Anastasiia Zbandut,Carolina Goldstein*

Main category: q-fin.RM

TL;DR: 论文研究去中心化信贷新兴市场，发现DeFi借贷风险主因从基础协议转移到策展人层，需提升透明度标准。


<details>
  <summary>Details</summary>
Motivation: 了解去中心化信贷新兴市场的特点和风险分布，为市场参与者提供决策依据。

Method: 分析ERC 4626金库和第三方策展人在市场中的作用，对比不同模块化金库特点，研究策展人对系统总锁仓价值的影响。

Result: 模块化金库在资本利用等方面存在差异，少数策展人主导大部分系统TVL且尾部共动呈集群状，收取不同费用。风险主因从基础协议转移到策展人层。

Conclusion: 这种风险转移要求提升透明度标准，并给出了一套简单的链上披露建议，方便用户和DAO评估策展人策略。

Abstract: This paper maps the emerging market for decentralized credit in which ERC 4626 vaults and third-party curators, rather than monolithic lending protocols alone, increasingly determine underwriting and leverage decisions. We show that modular vaults differ in capital utilization, cross-chain and cross asset concentration, and liquidity risk structure. Further, we show that a small set of curators intermediates a disproportionate share of system TVL, exhibits clustered tail co movement, and captures markedly different fee margins despite broadly similar collateral composition. These findings indicate that the main locus of risk in DeFi lending has migrated upward from base protocols, where underwriting is effectively centralized in a single DAO governed parameter set, to a permissionless curator layer in which competing vault managers decide which assets and loans are originated. We argue that this shift requires a corresponding upgrade in transparency standards and outline a simple set of onchain disclosures that would allow users and DAOs to evaluate curator strategies on a comparable, money market style basis.

</details>


### [308] [Extending the application of dynamic Bayesian networks in calculating market risk: Standard and stressed expected shortfall](https://arxiv.org/abs/2512.12334)
*Eden Gross,Ryan Kruger,Francois Toerien*

Main category: q-fin.RM

TL;DR: 本文研究将动态贝叶斯网络（DBNs）用于估计10天97.5%预期损失（ES）和压力预期损失（SES），对比多种模型表现。结果显示各模型难准确预测，ES用EGARCH(1,1)（正态）、SES用GARCH(1,1)（正态）较优，DBNs作用有限，建议研究新权重方案。


<details>
  <summary>Details</summary>
Motivation: 预期损失和压力预期损失成为银行业市场风险监管关键指标，需优化其估计方法。

Method: 将DBNs应用于ES和SES的估计，对比三种DBNs结构学习算法与几种传统市场风险模型的表现，并使用正态和偏态Student's t回报分布，以标准普尔500指数作为美国银行股票交易部门的代理进行回测。

Result: 所有模型在2.5%水平上都无法产生统计上准确的ES和SES预测；ES预测中，EGARCH(1,1)（正态）最准确；SES预测中，GARCH(1,1)（正态）表现最佳；使用偏态Student's t分布时，所有依赖分布的模型性能大幅下降；DBNs与历史模拟模型表现相当，但对尾部预测贡献有限。

Conclusion: 未来研究应探索增强前瞻性DBN预测对尾部风险估计影响的加权方案。

Abstract: In the last five years, expected shortfall (ES) and stressed ES (SES) have become key required regulatory measures of market risk in the banking sector, especially following events such as the global financial crisis. Thus, finding ways to optimize their estimation is of great importance. We extend the application of dynamic Bayesian networks (DBNs) to the estimation of 10-day 97.5% ES and stressed ES, building on prior work applying DBNs to value at risk. Using the S&P 500 index as a proxy for the equities trading desk of a US bank, we compare the performance of three DBN structure-learning algorithms with several traditional market risk models, using either the normal or the skewed Student's t return distributions. Backtesting shows that all models fail to produce statistically accurate ES and SES forecasts at the 2.5% level, reflecting the difficulty of modeling extreme tail behavior. For ES, the EGARCH(1,1) model (normal) produces the most accurate forecasts, while, for SES, the GARCH(1,1) model (normal) performs best. All distribution-dependent models deteriorate substantially when using the skewed Student's t distribution. The DBNs perform comparably to the historical simulation model, but their contribution to tail prediction is limited by the small weight assigned to their one-day-ahead forecasts within the return distribution. Future research should examine weighting schemes that enhance the influence of forward-looking DBN forecasts on tail risk estimation.

</details>


### [309] [Disability insurance with collective health claims: A mean-field approach](https://arxiv.org/abs/2512.13562)
*Christian Furrer,Philipp C. Hornung*

Main category: q-fin.RM

TL;DR: 拓展经典半马尔可夫残疾模型，采用平均场方法解决计算问题，模拟研究显示该近似方法有效。


<details>
  <summary>Details</summary>
Motivation: 扩展经典半马尔可夫残疾模型，提升其解释和预测能力，尤其是在团体经验评级场景下。

Method: 采用平均场方法，将多体问题近似为非线性单体问题，基于低维非线性正向积分 - 微分方程组定价。

Result: 在面向实践的模拟研究中，平均场近似比简单蒙特卡罗方法表现更好。

Conclusion: 平均场近似方法可用于残疾保险定价，解决原模型计算难题。

Abstract: The classic semi-Markov disability model is expanded with individual and collective health claims to improve its explanatory and predictive power -- in particular in the context of group experience rating. The inclusion of collective health claims leads to a computationally challenging many-body problem. By adopting a mean-field approach, this many-body problem can be approximated by a non-linear one-body problem, which in turn leads to a transparent pricing method for disability coverages based on a lower-dimensional system of non-linear forward integro-differential equations. In a practice-oriented simulation study, the mean-field approximation clearly stands its ground in comparison to naïve Monte Carlo methods.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [310] [Universal Dynamics of Financial Bubbles in Isolated Markets: Evidence from the Iranian Stock Market](https://arxiv.org/abs/2512.12054)
*Ali Hosseinzadeh*

Main category: q-fin.ST

TL;DR: 本文用LPPLS模型分析伊朗股市2020和2023年泡沫，发现其符合全球市场泡沫动态模式，支持泡沫动力学普遍性，还强调LPPLS工具对新兴或受限经济体监测风险的作用。


<details>
  <summary>Details</summary>
Motivation: 检验金融市场投机泡沫存在普遍潜在机制的假设，研究高度孤立、有资本管制且外国投资者难以进入的伊朗股市。

Method: 使用Log - Periodic Power Law Singularity (LPPLS)模型分析伊朗股市2020和2023年的两次主要泡沫事件。

Result: 估计的临界指数beta在0.46和0.20左右，处于历史典型泡沫的经验范围内；德黑兰证券交易所显示出LPPLS特征。

Conclusion: 即使在政治和经济孤立的市场，内生的羊群效应、模仿和正反馈动态起主导作用；新兴半封闭金融系统符合全球市场泡沫动态模式，支持泡沫动力学普遍性；LPPLS诊断工具对新兴或受限经济体监测系统风险有用。

Abstract: Speculative bubbles exhibit common statistical signatures across many financial markets, suggesting the presence of universal underlying mechanisms. We test this hypothesis in the Iranian stock market, an economy that is highly isolated, subject to capital controls, and largely inaccessible to foreign investors. Using the Log-Periodic Power Law Singularity (LPPLS) model, we analyze two major bubble episodes in 2020 and 2023. The estimated critical exponents beta around 0.46 and 0.20 fall within the empirical ranges documented for canonical historical bubbles such as the 1929 DJIA crash and the 2000 Nasdaq episode. The Tehran Stock Exchange displays clear LPPLS hallmarks, including faster-than-exponential price acceleration, log-periodic corrections, and stable estimates of the critical time horizon. These results indicate that endogenous herding, imitation, and positive-feedback dynamics, rather than exogenous shocks, play a dominant role even in politically and economically isolated markets. By showing that an emerging and semi-closed financial system conforms to the same dynamical patterns observed in global markets, this paper provides new empirical support for the universality of bubble dynamics. To the best of our knowledge, it also presents the first systematic LPPLS analysis of bubbles in the Tehran Stock Exchange. The findings highlight the usefulness of LPPLS-based diagnostic tools for monitoring systemic risk in emerging or restricted economies.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [311] [Stochastic Volatility Modelling with LSTM Networks: A Hybrid Approach for S&P 500 Index Volatility Forecasting](https://arxiv.org/abs/2512.12250)
*Anna Perekhodko,Robert Ślepaczuk*

Main category: q-fin.TR

TL;DR: 本文提出 SV - LSTM 混合建模框架预测标普 500 指数波动率，结果显示该混合方法优于单一模型，有助于改进风险评估和投资规划。


<details>
  <summary>Details</summary>
Motivation: 准确的波动率预测对银行、投资和风险管理至关重要，期望通过改进模型提升预测准确性。

Method: 提出集成随机波动率（SV）模型与长短期记忆（LSTM）神经网络的混合建模框架，用标普 500 指数日数据，采用滚动窗口方法训练模型并进行一步预测，通过统计测试和投资模拟评估模型。

Result: 混合 SV - LSTM 模型表现优于独立的 SV 和 LSTM 模型。

Conclusion: 混合方法有助于波动率建模技术发展，为标普 500 指数的风险评估和战略投资规划提供基础。

Abstract: Accurate volatility forecasting is essential in banking, investment, and risk management, because expectations about future market movements directly influence current decisions. This study proposes a hybrid modelling framework that integrates a Stochastic Volatility model with a Long Short Term Memory neural network. The SV model improves statistical precision and captures latent volatility dynamics, especially in response to unforeseen events, while the LSTM network enhances the model's ability to detect complex nonlinear patterns in financial time series. The forecasting is conducted using daily data from the S and P 500 index, covering the period from January 1 1998 to December 31 2024. A rolling window approach is employed to train the model and generate one step ahead volatility forecasts. The performance of the hybrid SV-LSTM model is evaluated through both statistical testing and investment simulations. The results show that the hybrid approach outperforms both the standalone SV and LSTM models and contributes to the development of volatility modelling techniques, providing a foundation for improving risk assessment and strategic investment planning in the context of the S and P 500.

</details>


### [312] [Interpretable Hypothesis-Driven Trading:A Rigorous Walk-Forward Validation Framework for Market Microstructure Signals](https://arxiv.org/abs/2512.12924)
*Gagan Deep,Akash Deep,William Lamptey*

Main category: q-fin.TR

TL;DR: 开发严格的算法交易前向验证框架，结合信号生成与强化学习，验证市场微观结构模式，有一定收益和风险特征，结果具制度依赖，提供开源实现。


<details>
  <summary>Details</summary>
Motivation: 减轻算法交易中的过拟合和前瞻偏差问题。

Method: 将可解释的假设驱动信号生成与强化学习结合，进行严格样本外测试，采用滚动窗口验证，考虑交易成本和仓位限制。

Result: 系统有适度年化收益、出色下行保护和市场中性特征，表现有制度依赖，总体结果统计不显著。

Conclusion: 该框架可严格验证可解释的算法交易策略，不牺牲透明度和合规性，为量化金融研究提供模板。

Abstract: We develop a rigorous walk-forward validation framework for algorithmic trading designed to mitigate overfitting and lookahead bias. Our methodology combines interpretable hypothesis-driven signal generation with reinforcement learning and strict out-of-sample testing. The framework enforces strict information set discipline, employs rolling window validation across 34 independent test periods, maintains complete interpretability through natural language hypothesis explanations, and incorporates realistic transaction costs and position constraints. Validating five market microstructure patterns across 100 US equities from 2015 to 2024, the system yields modest annualized returns (0.55%, Sharpe ratio 0.33) with exceptional downside protection (maximum drawdown -2.76%) and market-neutral characteristics (beta = 0.058). Performance exhibits strong regime dependence, generating positive returns during high-volatility periods (0.60% quarterly, 2020-2024) while underperforming in stable markets (-0.16%, 2015-2019). We report statistically insignificant aggregate results (p-value 0.34) to demonstrate a reproducible, honest validation protocol that prioritizes interpretability and extends naturally to advanced hypothesis generators, including large language models. The key empirical finding reveals that daily OHLCV-based microstructure signals require elevated information arrival and trading activity to function effectively. The framework provides complete mathematical specifications and open-source implementation, establishing a template for rigorous trading system evaluation that addresses the reproducibility crisis in quantitative finance research. For researchers, practitioners, and regulators, this work demonstrates that interpretable algorithmic trading strategies can be rigorously validated without sacrificing transparency or regulatory compliance.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [313] [Interval Fisher's Discriminant Analysis and Visualisation](https://arxiv.org/abs/2512.11945)
*Diogo Pinheiro,M. Rosário Oliveira,Igor Kravchenko,Lina Oliveira*

Main category: stat.ML

TL;DR: 提出将多类Fisher判别分析扩展到区间值数据，用区间算术和距离最大化目标函数分类，还适配可视化工具并应用于真实数据集。


<details>
  <summary>Details</summary>
Motivation: 传统数据科学用单值测量表示实体，符号数据分析需扩展到区间等复杂结构，故要将多类Fisher判别分析扩展到区间值数据。

Method: 使用Moore区间算术和Mallows距离，扩展Fisher目标函数并数值最大化，用判别方向分类；适配类图、扩展轮廓图和使用堆叠马赛克图进行可视化。

Result: 提出适用于区间值数据的分类方法和可视化工具。

Conclusion: 所提方法和可视化工具对区间值数据分类结果解释有价值。

Abstract: In Data Science, entities are typically represented by single valued measurements. Symbolic Data Analysis extends this framework to more complex structures, such as intervals and histograms, that express internal variability. We propose an extension of multiclass Fisher's Discriminant Analysis to interval-valued data, using Moore's interval arithmetic and the Mallows' distance. Fisher's objective function is generalised to consider simultaneously the contributions of the centres and the ranges of intervals and is numerically maximised. The resulting discriminant directions are then used to classify interval-valued observations.To support visual assessment, we adapt the class map, originally introduced for conventional data, to classifiers that assign labels through minimum distance rules. We also extend the silhouette plot to this setting and use stacked mosaic plots to complement the visual display of class assignments. Together, these graphical tools provide insight into classifier performance and the strength of class membership. Applications to real datasets illustrate the proposed methodology and demonstrate its value in interpreting classification results for interval-valued data.

</details>


### [314] [Hellinger loss function for Generative Adversarial Networks](https://arxiv.org/abs/2512.12267)
*Giovanni Saraceno,Anand N. Vidyashankar,Claudio Agostinelli*

Main category: stat.ML

TL;DR: 提出用于训练GAN的Hellinger型损失函数，研究其统计性质，对比经典GAN损失，证明新损失在数据污染下有更好表现。


<details>
  <summary>Details</summary>
Motivation: 基于Hellinger距离的有界性、对称性和鲁棒性，提出用于训练GAN的Hellinger型损失函数。

Method: 定义基于该散度的对抗目标，在一般参数框架下研究其统计性质，联合估计生成器和判别器参数；引入两种Hellinger型损失实现并与经典GAN损失对比。

Result: 所提两种损失在数据污染程度增加时，估计准确性和鲁棒性都有提升。

Conclusion: Hellinger型损失函数在训练GAN时，相比经典GAN损失，能在数据污染情况下实现更好的估计。

Abstract: We propose Hellinger-type loss functions for training Generative Adversarial Networks (GANs), motivated by the boundedness, symmetry, and robustness properties of the Hellinger distance. We define an adversarial objective based on this divergence and study its statistical properties within a general parametric framework. We establish the existence, uniqueness, consistency, and joint asymptotic normality of the estimators obtained from the adversarial training procedure. In particular, we analyze the joint estimation of both generator and discriminator parameters, offering a comprehensive asymptotic characterization of the resulting estimators. We introduce two implementations of the Hellinger-type loss and we evaluate their empirical behavior in comparison with the classic (Maximum Likelihood-type) GAN loss. Through a controlled simulation study, we demonstrate that both proposed losses yield improved estimation accuracy and robustness under increasing levels of data contamination.

</details>


### [315] [Towards a pretrained deep learning estimator of the Linfoot informational correlation](https://arxiv.org/abs/2512.12358)
*Stéphanie M. van den Berg,Ulrich Halekoh,Sören Möller,Andreas Kryger Jensen,Jacob von Bornemann Hjelmborg*

Main category: stat.ML

TL;DR: 提出监督式深度学习方法估计两连续随机变量互信息，与其他方法对比有优势，建议后续用更多样本训练。


<details>
  <summary>Details</summary>
Motivation: 开发一种估计两连续随机变量互信息的方法。

Method: 采用监督式深度学习方法，以Linfoot信息相关性为标签，基于高斯和Clayton copulas的真实标签。

Result: 与基于核密度、k近邻和神经估计器的方法相比，偏差和方差更低。

Conclusion: 可使用更多有真实标签的copulas样本训练模型开展后续研究。

Abstract: We develop a supervised deep-learning approach to estimate mutual information between two continuous random variables. As labels, we use the Linfoot informational correlation, a transformation of mutual information that has many important properties. Our method is based on ground truth labels for Gaussian and Clayton copulas. We compare our method with estimators based on kernel density, k-nearest neighbours and neural estimators. We show generally lower bias and lower variance. As a proof of principle, future research could look into training the model with a more diverse set of examples from other copulas for which ground truth labels are available.

</details>


### [316] [Co-Hub Node Based Multiview Graph Learning with Theoretical Guarantees](https://arxiv.org/abs/2512.12435)
*Bisakh Banerjee,Mohammad Alwardat,Tapabrata Maiti,Selin Aviyente*

Main category: stat.ML

TL;DR: 本文提出共枢纽节点模型识别多视图图结构，经理论分析和数据验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法多推导单一图结构，多视图图学习需考虑节点共性以提升学习精度和可解释性。

Method: 提出共枢纽节点模型，对共枢纽节点连接施加结构稀疏性，建立优化框架，进行理论分析。

Result: 用合成图数据和fMRI时间序列数据验证方法能识别多个相关图。

Conclusion: 提出的共枢纽节点模型和优化框架有效可行。

Abstract: Identifying the graphical structure underlying the observed multivariate data is essential in numerous applications. Current methodologies are predominantly confined to deducing a singular graph under the presumption that the observed data are uniform. However, many contexts involve heterogeneous datasets that feature multiple closely related graphs, typically referred to as multiview graphs. Previous research on multiview graph learning promotes edge-based similarity across layers using pairwise or consensus-based regularizers. However, multiview graphs frequently exhibit a shared node-based architecture across different views, such as common hub nodes. Such commonalities can enhance the precision of learning and provide interpretive insight. In this paper, we propose a co-hub node model, positing that different views share a common group of hub nodes. The associated optimization framework is developed by enforcing structured sparsity on the connections of these co-hub nodes. Moreover, we present a theoretical examination of layer identifiability and determine bounds on estimation error. The proposed methodology is validated using both synthetic graph data and fMRI time series data from multiple subjects to discern several closely related graphs.

</details>


### [317] [Efficient Level-Crossing Probability Calculation for Gaussian Process Modeled Data](https://arxiv.org/abs/2512.12442)
*Haoyu Li,Isaac J Michaud,Ayan Biswas,Han-Wei Shen*

Main category: stat.ML

TL;DR: 本文提出方法加速高斯过程回归（GPR）模型的水平交叉概率计算，通过空间细分数据和自适应重构值降低计算成本，实验验证方法准确高效。


<details>
  <summary>Details</summary>
Motivation: 科学数据普遍存在不确定性，GPR模型虽适合建模，但重构计算复杂度高，传统可视化方法计算成本也高，因此需要加速计算。

Method: 将数据空间细分为分层数据结构，仅在非零概率区域自适应重构值；利用已知GPR核和保存的数据观测，提出计算区域内水平交叉概率上限的方法以进行细分和重构决策。

Result: 通过在不同数据集上计算水平交叉概率场的实验，证明值发生概率估计准确且计算成本低。

Conclusion: 所提方法能有效加速GPR模型的水平交叉概率计算，在不高的计算成本下实现准确估计。

Abstract: Almost all scientific data have uncertainties originating from different sources. Gaussian process regression (GPR) models are a natural way to model data with Gaussian-distributed uncertainties. GPR also has the benefit of reducing I/O bandwidth and storage requirements for large scientific simulations. However, the reconstruction from the GPR models suffers from high computation complexity. To make the situation worse, classic approaches for visualizing the data uncertainties, like probabilistic marching cubes, are also computationally very expensive, especially for data of high resolutions. In this paper, we accelerate the level-crossing probability calculation efficiency on GPR models by subdividing the data spatially into a hierarchical data structure and only reconstructing values adaptively in the regions that have a non-zero probability. For each region, leveraging the known GPR kernel and the saved data observations, we propose a novel approach to efficiently calculate an upper bound for the level-crossing probability inside the region and use this upper bound to make the subdivision and reconstruction decisions. We demonstrate that our value occurrence probability estimation is accurate with a low computation cost by experiments that calculate the level-crossing probability fields on different datasets.

</details>


### [318] [Understanding Overparametrization in Survival Models through Double-Descent](https://arxiv.org/abs/2512.12463)
*Yin Liu,Jianwen Cai,Didong Li*

Main category: stat.ML

TL;DR: 研究四种生存模型中的双下降现象，定义关键特征，明确插值可行性及过拟合影响，并通过实验支持理论结果。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习有双下降现象，但在生存分析中的表现未被探索，故开展研究。

Method: 研究四种代表性生存模型，定义插值和有限范数插值，分析模型的（有限范数）插值情况。

Result: 明确了似然损失和模型实现共同决定插值可行性，发现生存模型过拟合并非无害。

Conclusion: 研究揭示了生存模型双下降现象及相关特征，实验验证了理论结果体现的泛化行为。

Abstract: Classical statistical learning theory predicts a U-shaped relationship between test loss and model capacity, driven by the bias-variance trade-off. Recent advances in modern machine learning have revealed a more complex pattern, double-descent, in which test loss, after peaking near the interpolation threshold, decreases again as model capacity continues to grow. While this behavior has been extensively analyzed in regression and classification, its manifestation in survival analysis remains unexplored. This study investigates double-descent in four representative survival models: DeepSurv, PC-Hazard, Nnet-Survival, and N-MTLR. We rigorously define interpolation and finite-norm interpolation, two key characteristics of loss-based models to understand double-descent. We then show the existence (or absence) of (finite-norm) interpolation of all four models. Our findings clarify how likelihood-based losses and model implementation jointly determine the feasibility of interpolation and show that overfitting should not be regarded as benign for survival models. All theoretical results are supported by numerical experiments that highlight the distinct generalization behaviors of survival models.

</details>


### [319] [Iterative Sampling Methods for Sinkhorn Distributionally Robust Optimization](https://arxiv.org/abs/2512.12550)
*Jie Wang*

Main category: stat.ML

TL;DR: 本文聚焦基于Sinkhorn差异的分布鲁棒优化（Sinkhorn DRO），从原问题视角将其转化为双层规划问题，提出双循环和单循环采样算法并通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要从对偶视角研究Sinkhorn DRO，理论分析依赖损失函数有界性，且难以直接获取最坏情况分布，本文旨在从原问题视角解决这些问题。

Method: 将Sinkhorn DRO重新表述为一个在概率空间上有几个无限维下层子问题的双层规划问题，提出双循环和单循环采样算法。

Result: 提出的算法有理论保证，通过对抗分类的数值研究证明了方法的有效性。

Conclusion: 从原问题视角研究Sinkhorn DRO可同时获得最优鲁棒决策和最坏情况分布，所提算法有效可行。

Abstract: Distributionally robust optimization (DRO) has emerged as a powerful paradigm for reliable decision-making under uncertainty. This paper focuses on DRO with ambiguity sets defined via the Sinkhorn discrepancy: an entropy-regularized Wasserstein distance, referred to as Sinkhorn DRO. Existing work primarily addresses Sinkhorn DRO from a dual perspective, leveraging its formulation as a conditional stochastic optimization problem, for which many stochastic gradient methods are applicable. However, the theoretical analyses of such methods often rely on the boundedness of the loss function, and it is indirect to obtain the worst-case distribution associated with Sinkhorn DRO. In contrast, we study Sinkhorn DRO from the primal perspective, by reformulating it as a bilevel program with several infinite-dimensional lower-level subproblems over probability space. This formulation enables us to simultaneously obtain the optimal robust decision and the worst-case distribution, which is valuable in practical settings, such as generating stress-test scenarios or designing robust learning algorithms. We propose both double-loop and single-loop sampling-based algorithms with theoretical guarantees to solve this bilevel program. Finally, we demonstrate the effectiveness of our approach through a numerical study on adversarial classification.

</details>


### [320] [Mind the Jumps: A Scalable Robust Local Gaussian Process for Multidimensional Response Surfaces with Discontinuities](https://arxiv.org/abs/2512.12574)
*Isaac Adjetey,Yiyuan She*

Main category: stat.ML

TL;DR: 本文提出Robust Local Gaussian Process (RLGP) 模型解决响应面建模中突变和不连续问题，评估显示其有高预测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有高斯过程模型及非平稳扩展在处理有突变和不连续的响应面建模时存在局限，如边界不一致、对异常值敏感和高维可扩展性问题等。

Method: 提出RLGP模型，结合自适应最近邻选择和稀疏驱动的稳健机制，在多元视角变换后进行基于优化的均值漂移调整并结合局部邻域建模。

Result: 在真实数据集上综合评估表明RLGP有高预测精度和计算效率，在高维场景也稳定可靠。

Conclusion: RLGP是跨多种应用建模非平稳和不连续响应面的有效实用解决方案。

Abstract: Modeling response surfaces with abrupt jumps and discontinuities remains a major challenge across scientific and engineering domains. Although Gaussian process models excel at capturing smooth nonlinear relationships, their stationarity assumptions limit their ability to adapt to sudden input-output variations. Existing nonstationary extensions, particularly those based on domain partitioning, often struggle with boundary inconsistencies, sensitivity to outliers, and scalability issues in higher-dimensional settings, leading to reduced predictive accuracy and unreliable parameter estimation.
  To address these challenges, this paper proposes the Robust Local Gaussian Process (RLGP) model, a framework that integrates adaptive nearest-neighbor selection with a sparsity-driven robustification mechanism. Unlike existing methods, RLGP leverages an optimization-based mean-shift adjustment after a multivariate perspective transformation combined with local neighborhood modeling to mitigate the influence of outliers. This approach improves predictive accuracy near discontinuities while enhancing robustness to data heterogeneity.
  Comprehensive evaluations on real-world datasets show that RLGP consistently delivers high predictive accuracy and maintains competitive computational efficiency, especially in scenarios with sharp transitions and complex response structures. Scalability tests further confirm RLGP's stability and reliability in higher-dimensional settings, where other methods struggle. These results establish RLGP as an effective and practical solution for modeling nonstationary and discontinuous response surfaces across a wide range of applications.

</details>


### [321] [Limits To (Machine) Learning](https://arxiv.org/abs/2512.12735)
*Zhimin Chen,Bryan Kelly,Semyon Malamud*

Main category: stat.ML

TL;DR: 研究机器学习方法因有限样本在逼近真实数据生成过程中的局限性，提出学习极限差距（LLG），发现在金融数据中LLG值大，标准ML方法会低估真实可预测性，并对经典界进行改进及分析相关影响。


<details>
  <summary>Details</summary>
Motivation: 机器学习方法受有限样本约束，需要量化其在逼近真实数据生成过程中的不可避免差距，以及评估其在金融数据预测中的表现。

Method: 提出学习极限差距（LLG）量化模型经验拟合与总体基准间的差距，并使用多种金融变量进行分析，推导基于LLG对经典Hansen和Jagannathan界的改进。

Result: 在多种金融变量数据中，发现隐含的LLG值大，表明标准ML方法会显著低估金融数据的真实可预测性。

Conclusion: 恢复真实总体$R^2$需要通过LLG修正观测到的预测表现；LLG能为经典界提供改进，并且可成为产生超额波动的自然机制。

Abstract: Machine learning (ML) methods are highly flexible, but their ability to approximate the true data-generating process is fundamentally constrained by finite samples. We characterize a universal lower bound, the Limits-to-Learning Gap (LLG), quantifying the unavoidable discrepancy between a model's empirical fit and the population benchmark. Recovering the true population $R^2$, therefore, requires correcting observed predictive performance by this bound. Using a broad set of variables, including excess returns, yields, credit spreads, and valuation ratios, we find that the implied LLGs are large. This indicates that standard ML approaches can substantially understate true predictability in financial data. We also derive LLG-based refinements to the classic Hansen and Jagannathan (1991) bounds, analyze implications for parameter learning in general-equilibrium settings, and show that the LLG provides a natural mechanism for generating excess volatility.

</details>


### [322] [Transport Reversible Jump Markov Chain Monte Carlo with proposals generated by Variational Inference with Normalizing Flows](https://arxiv.org/abs/2512.12742)
*Pingping Yin,Xiyun Jiao*

Main category: stat.ML

TL;DR: 提出用变分推理与归一化流（VI - NFs）生成可逆跳马尔可夫链蒙特卡罗（RJMCMC）提议的框架，实验表明比现有基线更高效，算法可扩展。


<details>
  <summary>Details</summary>
Motivation: 为高效跨维度贝叶斯推理生成RJMCMC提议，避免传统方法依赖目标采样的高成本。

Method: 使用VI - NFs，最小化反向KL散度，采用基于RealNVP的流学习模型特定传输映射。

Result: 框架能提供准确边际似然估计，实验显示TRJ设计的VI - NFs混合更快、模型空间探索更高效。

Conclusion: 提出的算法可扩展到条件流用于跨模型的摊销变分推理。

Abstract: We present a framework using variational inference with normalizing flows (VI-NFs) to generate proposals of reversible jump Markov chain Monte Carlo (RJMCMC) for efficient trans-dimensional Bayesian inference. Unlike transport reversible jump methods relying on forward KL minimization with pilot MCMC samples, our approach minimizes the reverse KL divergence which requires only samples from a base distribution, eliminating costly target sampling. The method employs RealNVP-based flows to learn model-specific transport maps, enabling construction of both between-model and within-model proposals. Our framework provides accurate marginal likelihood estimates from the variational approximation. This facilitates efficient model comparison and proposal adaptation in RJMCMC. Experiments on illustrative example, factor analysis and variable selection tasks in linear regression show that TRJ designed by VI-NFs achieves faster mixing and more efficient model space exploration compared to existing baselines. The proposed algorithm can be extended to conditional flows for amortized vairiational inference across models. Code is available at https://github.com/YinPingping111/TRJ_VINFs.

</details>


### [323] [PAC-Bayes Bounds for Multivariate Linear Regression and Linear Autoencoders](https://arxiv.org/abs/2512.12905)
*Ruixin Guo,Ruoming Jin,Xinyu Li,Yang Zhou*

Main category: stat.ML

TL;DR: 研究多元线性回归和线性自编码器（LAEs）泛化性，提出多元线性回归的PAC - Bayes界，证明LAEs可解释为有约束的多元线性回归模型，改进优化LAE界的计算效率，实验显示界较紧且与实用排名指标相关性好。


<details>
  <summary>Details</summary>
Motivation: 线性自编码器在推荐系统中表现好但理论理解有限，研究其泛化性。

Method: 提出多元线性回归的PAC - Bayes界并建立收敛充分条件，将LAEs解释为有约束的多元线性回归模型，开发提高优化LAE界计算效率的理论方法。

Result: 实验表明提出的界较紧，与Recall@K和NDCG@K等实用排名指标相关性好。

Conclusion: 所提方法有效，能提升对线性自编码器泛化性的理论理解并可应用于实际。

Abstract: Linear Autoencoders (LAEs) have shown strong performance in state-of-the-art recommender systems. However, this success remains largely empirical, with limited theoretical understanding. In this paper, we investigate the generalizability -- a theoretical measure of model performance in statistical learning -- of multivariate linear regression and LAEs. We first propose a PAC-Bayes bound for multivariate linear regression, extending the earlier bound for single-output linear regression by Shalaeva et al., and establish sufficient conditions for its convergence. We then show that LAEs, when evaluated under a relaxed mean squared error, can be interpreted as constrained multivariate linear regression models on bounded data, to which our bound adapts. Furthermore, we develop theoretical methods to improve the computational efficiency of optimizing the LAE bound, enabling its practical evaluation on large models and real-world datasets. Experimental results demonstrate that our bound is tight and correlates well with practical ranking metrics such as Recall@K and NDCG@K.

</details>


### [324] [Evaluating Singular Value Thresholds for DNN Weight Matrices based on Random Matrix Theory](https://arxiv.org/abs/2512.12911)
*Kohei Nishikawa,Koki Shimizu,Hashiguchi Hiroki*

Main category: stat.ML

TL;DR: 研究评估基于奇异值分解的深度神经网络权重矩阵低秩近似中去除奇异值的阈值，提出评估指标并比较两种阈值估计方法。


<details>
  <summary>Details</summary>
Motivation: 确定合适的阈值来去除深度神经网络权重矩阵低秩近似中的奇异值。

Method: 将权重矩阵建模为信号和噪声矩阵之和，用基于随机矩阵理论的阈值去除噪声相关奇异值，提出基于余弦相似度的评估指标。

Result: 通过数值实验用提出的指标比较了两种阈值估计方法。

Conclusion: 未提及明确结论。

Abstract: This study evaluates thresholds for removing singular values from singular value decomposition-based low-rank approximations of deep neural network weight matrices. Each weight matrix is modeled as the sum of signal and noise matrices. The low-rank approximation is obtained by removing noise-related singular values using a threshold based on random matrix theory. To assess the adequacy of this threshold, we propose an evaluation metric based on the cosine similarity between the singular vectors of the signal and original weight matrices. The proposed metric is used in numerical experiments to compare two threshold estimation methods.

</details>


### [325] [General OOD Detection via Model-aware and Subspace-aware Variable Priority](https://arxiv.org/abs/2512.13003)
*Min Lu,Hemant Ishwaran*

Main category: stat.ML

TL;DR: 提出同时具备模型感知和子空间感知的OOD检测框架，在合成和真实数据基准测试中表现优于现有方法，并应用于食管癌生存研究。


<details>
  <summary>Details</summary>
Motivation: 分类中OOD检测研究广泛，但回归和生存分析的OOD检测因无离散标签和量化预测不确定性的挑战而受限。

Method: 使用拟合预测器为每个测试用例构建局部邻域，强调驱动模型学习关系的特征，弱化与预测无关的方向，不依赖全局距离度量或估计全特征密度。

Result: 在合成和真实数据基准测试中，相比现有方法有一致改进；在食管癌生存研究中识别出与手术指南相关的模式。

Conclusion: 所提出的框架适用于不同结果类型，能有效进行OOD检测。

Abstract: Out-of-distribution (OOD) detection is essential for determining when a supervised model encounters inputs that differ meaningfully from its training distribution. While widely studied in classification, OOD detection for regression and survival analysis remains limited due to the absence of discrete labels and the challenge of quantifying predictive uncertainty. We introduce a framework for OOD detection that is simultaneously model aware and subspace aware, and that embeds variable prioritization directly into the detection step. The method uses the fitted predictor to construct localized neighborhoods around each test case that emphasize the features driving the model's learned relationship and downweight directions that are less relevant to prediction. It produces OOD scores without relying on global distance metrics or estimating the full feature density. The framework is applicable across outcome types, and in our implementation we use random forests, where the rule structure yields transparent neighborhoods and effective scoring. Experiments on synthetic and real data benchmarks designed to isolate functional shifts show consistent improvements over existing methods. We further demonstrate the approach in an esophageal cancer survival study, where distribution shifts related to lymphadenectomy identify patterns relevant to surgical guidelines.

</details>


### [326] [A Nonparametric Statistics Approach to Feature Selection in Deep Neural Networks with Theoretical Guarantees](https://arxiv.org/abs/2512.13565)
*Junye Du,Zhenghao Li,Zhutong Gu,Long Feng*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper tackles the problem of feature selection in a highly challenging setting: $\mathbb{E}(y | \boldsymbol{x}) = G(\boldsymbol{x}_{\mathcal{S}_0})$, where $\mathcal{S}_0$ is the set of relevant features and $G$ is an unknown, potentially nonlinear function subject to mild smoothness conditions. Our approach begins with feature selection in deep neural networks, then generalizes the results to H{ö}lder smooth functions by exploiting the strong approximation capabilities of neural networks. Unlike conventional optimization-based deep learning methods, we reformulate neural networks as index models and estimate $\mathcal{S}_0$ using the second-order Stein's formula. This gradient-descent-free strategy guarantees feature selection consistency with a sample size requirement of $n = Ω(p^2)$, where $p$ is the feature dimension. To handle high-dimensional scenarios, we further introduce a screening-and-selection mechanism that achieves nonlinear selection consistency when $n = Ω(s \log p)$, with $s$ representing the sparsity level. Additionally, we refit a neural network on the selected features for prediction and establish performance guarantees under a relaxed sparsity assumption. Extensive simulations and real-data analyses demonstrate the strong performance of our method even in the presence of complex feature interactions.

</details>


### [327] [Universality of high-dimensional scaling limits of stochastic gradient descent](https://arxiv.org/abs/2512.13634)
*Reza Gheissari,Aukosh Jagannath*

Main category: stat.ML

TL;DR: 研究高维统计任务中，随机梯度下降下统计量演化的ODE极限的普遍性，给出普遍性结果及两个非普遍性结果。


<details>
  <summary>Details</summary>
Motivation: 探究在数据来自乘积测度混合时，高维统计任务中随机梯度下降下统计量演化的ODE极限是否仍具有普遍性。

Method: 理论分析，研究数据来自各向同性高斯混合分布及乘积测度混合时，随机梯度下降下有限族统计量的演化。

Result: 证明当数据来自乘积测度混合且前两阶矩与高斯分布匹配、初始化和真实向量足够坐标非局部化时，ODE极限具有普遍性；给出初始化坐标对齐时ODE极限非普遍性的例子；证明围绕ODE不动点的随机微分方程极限不具有普遍性。

Conclusion: 高维统计任务中随机梯度下降下的ODE极限在一定条件下具有普遍性，但也存在非普遍性情况。

Abstract: We consider statistical tasks in high dimensions whose loss depends on the data only through its projection into a fixed-dimensional subspace spanned by the parameter vectors and certain ground truth vectors. This includes classifying mixture distributions with cross-entropy loss with one and two-layer networks, and learning single and multi-index models with one and two-layer networks. When the data is drawn from an isotropic Gaussian mixture distribution, it is known that the evolution of a finite family of summary statistics under stochastic gradient descent converges to an autonomous ordinary differential equation (ODE), as the dimension and sample size go to $\infty$ and the step size goes to $0$ commensurately. Our main result is that these ODE limits are universal in that this convergence occurs even when the data is drawn from mixtures of product measures provided the first two moments match the corresponding Gaussian distribution and the initialization and ground truth vectors are sufficiently coordinate-delocalized. We complement this by proving two corresponding non-universality results. We provide a simple example where the ODE limits are non-universal if the initialization is coordinate aligned. We also show that the stochastic differential equation limits arising as fluctuations of the summary statistics around their ODE's fixed points are not universal.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [328] [StochTree: BART-based modeling in R and Python](https://arxiv.org/abs/2512.12051)
*Andrew Herren,P. Richard Hahn,Jared Murray,Carlos Carvalho*

Main category: stat.CO

TL;DR: stochtree是用于贝叶斯树集成模型的C++库，支持R和Python，功能更全且灵活，还举例说明了其应用场景。


<details>
  <summary>Details</summary>
Motivation: 开发一个功能更全面、跨语言互操作性强的贝叶斯树集成模型库。

Method: 开发stochtree库，提供R和Python绑定，支持多种模型，提供灵活的模型拟合处理和底层功能。

Result: stochtree库能在R和Python平台使用，可处理多种模型，能灵活保存和传递模型拟合。

Conclusion: stochtree库在贝叶斯树集成模型应用中有广泛用途，可用于多种场景。

Abstract: stochtree is a C++ library for Bayesian tree ensemble models such as BART and Bayesian Causal Forests (BCF), as well as user-specified variations. Unlike previous BART packages, stochtree provides bindings to both R and Python for full interoperability. stochtree boasts a more comprehensive range of models relative to previous packages, including heteroskedastic forests, random effects, and treed linear models. Additionally, stochtree offers flexible handling of model fits: the ability to save model fits, reinitialize models from existing fits (facilitating improved model initialization heuristics), and pass fits between R and Python. On both platforms, stochtree exposes lower-level functionality, allowing users to specify models incorporating Bayesian tree ensembles without needing to modify C++ code. We illustrate the use of stochtree in three settings: i) straightfoward applications of existing models such as BART and BCF, ii) models that include more sophisticated components like heteroskedasticity and leaf-wise regression models, and iii) as a component of custom MCMC routines to fit nonstandard tree ensemble models.

</details>


### [329] [Complexity of Markov Chain Monte Carlo for Generalized Linear Models](https://arxiv.org/abs/2512.12748)
*Martin Chak,Giacomo Zanella*

Main category: stat.CO

TL;DR: 论文探讨贝叶斯推理方法，表明在n和d关系下MCMC复杂度与LA和Gaussian VI相当。


<details>
  <summary>Details</summary>
Motivation: 现有Markov Chain Monte Carlo (MCMC)、Laplace approximation (LA)和variational inference (VI)方法在计算成本和准确性上有取舍，且缺乏大样本和高维时差异的理论理解。

Method: 针对线性、逻辑和泊松回归进行研究。

Result: 在ngtrsim d时，MCMC与一阶优化算法有相同复杂度缩放，在更一般的n和d缩放关系下，其复杂度与LA和Gaussian VI相当。

Conclusion: MCMC复杂度在更一般条件下可与LA和Gaussian VI竞争，复杂度适用于非高斯尾先验。

Abstract: Markov Chain Monte Carlo (MCMC), Laplace approximation (LA) and variational inference (VI) methods are popular approaches to Bayesian inference, each with trade-offs between computational cost and accuracy. However, a theoretical understanding of these differences is missing, particularly when both the sample size $n$ and the dimension $d$ are large. LA and Gaussian VI are justified by Bernstein-von Mises (BvM) theorems, and recent work has derived the characteristic condition $n\gg d^2$ for their validity, improving over the condition $n\gg d^3$. In this paper, we show for linear, logistic and Poisson regression that for $n\gtrsim d$, MCMC attains the same complexity scaling in $n$, $d$ as first-order optimization algorithms, up to sub-polynomial factors. Thus MCMC is competitive with LA and Gaussian VI in complexity, under a scaling between $n$ and $d$ more general than BvM regimes. Our complexities apply to appropriately scaled priors that are not necessarily Gaussian-tailed, including Student-$t$ and flat priors, with log-posteriors that are not necessarily globally concave or gradient-Lipschitz.

</details>


### [330] [Flow-matching Operators for Residual-Augmented Probabilistic Learning of Partial Differential Equations](https://arxiv.org/abs/2512.12749)
*Sahil Bhola,Karthik Duraisamy*

Main category: stat.CO

TL;DR: 本文提出在无限维函数空间中进行流匹配，学习从低保真近似到高保真偏微分方程解流形的概率传输，经实验验证方法在不同分辨率和保真度下有效。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺情况下，学习偏微分方程的概率替代模型具有挑战性，现有方法存在需要大量高保真数据或牺牲分辨率不变性的问题。

Method: 在无限维函数空间中制定流匹配，开发基于特征线性调制的条件神经算子架构，参数化流向量场，制定残差增强学习策略并推导可处理的训练目标。

Result: 所提方法能在不同分辨率和保真度下准确学习解算子，产生能反映模型置信度的不确定性估计，即使在有限高保真数据上训练。

Conclusion: 提出的方法在学习偏微分方程概率替代模型方面有效，尤其适用于数据稀缺情况。

Abstract: Learning probabilistic surrogates for PDEs remains challenging in data-scarce regimes: neural operators require large amounts of high-fidelity data, while generative approaches typically sacrifice resolution invariance. We formulate flow matching in an infinite-dimensional function space to learn a probabilistic transport that maps low-fidelity approximations to the manifold of high-fidelity PDE solutions via learned residual corrections. We develop a conditional neural operator architecture based on feature-wise linear modulation for flow-matching vector fields directly in function space, enabling inference at arbitrary spatial resolutions without retraining. To improve stability and representational control of the induced neural ODE, we parameterize the flow vector field as a sum of a linear operator and a nonlinear operator, combining lightweight linear components with a conditioned Fourier neural operator for expressive, input-dependent dynamics. We then formulate a residual-augmented learning strategy where the flow model learns probabilistic corrections from inexpensive low-fidelity surrogates to high-fidelity solutions, rather than learning the full solution mapping from scratch. Finally, we derive tractable training objectives that extend conditional flow matching to the operator setting with input-function-dependent couplings. To demonstrate the effectiveness of our approach, we present numerical experiments on a range of PDEs, including the 1D advection and Burgers' equation, and a 2D Darcy flow problem for flow through a porous medium.
  We show that the proposed method can accurately learn solution operators across different resolutions and fidelities and produces uncertainty estimates that appropriately reflect model confidence, even when trained on limited high-fidelity data.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [331] [Explainable Prediction of Economic Time Series Using IMFs and Neural Networks](https://arxiv.org/abs/2512.12499)
*Pablo Hidalgo,Julio E. Sandubete,Agustín García-García*

Main category: econ.EM

TL;DR: 研究经济时间序列IMF对神经网络模型预测性能的贡献，用DeepSHAP增强可解释性，发现长期趋势IMF最有影响，高频IMF贡献小甚至有噪声，MLP和LSTM在特征相关性分布上有差异。


<details>
  <summary>Details</summary>
Motivation: 研究经济时间序列IMF对神经网络模型（MLP和LSTM）预测性能的贡献，并增强模型的可解释性。

Method: 应用DeepSHAP估计每个IMF的边际贡献。

Result: 长期趋势的IMF最有影响，高频IMF贡献小甚至引入噪声；LSTM在IMF上分配重要性更均匀。

Conclusion: IMF对神经网络模型预测性能有不同影响，模型架构会影响特征相关性分布。

Abstract: This study investigates the contribution of Intrinsic Mode Functions (IMFs) derived from economic time series to the predictive performance of neural network models, specifically Multilayer Perceptrons (MLP) and Long Short-Term Memory (LSTM) networks. To enhance interpretability, DeepSHAP is applied, which estimates the marginal contribution of each IMF while keeping the rest of the series intact. Results show that the last IMFs, representing long-term trends, are generally the most influential according to DeepSHAP, whereas high-frequency IMFs contribute less and may even introduce noise, as evidenced by improved metrics upon their removal. Differences between MLP and LSTM highlight the effect of model architecture on feature relevance distribution, with LSTM allocating importance more evenly across IMFs.

</details>


### [332] [Policy-Aligned Estimation of Conditional Average Treatment Effects](https://arxiv.org/abs/2512.13400)
*Artem Timoshenko,Caio Waisman*

Main category: econ.EM

TL;DR: 本文提出一种估计营销行动条件平均处理效应（CATEs）的方法，将其与企业利润目标相结合，以制定接近最优的目标定位策略。


<details>
  <summary>Details</summary>
Motivation: 企业制定目标定位策略需准确区分顾客的正负处理效应，现有方法可能无法与企业利润目标有效结合。

Method: 修改标准利润最大化问题中的企业目标函数，在估计CATEs的同时制定接近最优的目标定位策略。

Result: 建立了所提方法的理论性质，并通过合成数据展示了其性能和权衡。

Conclusion: 将CATE估计重新定义为利润优化问题，而非追求预测准确性。

Abstract: Firms often develop targeting policies to personalize marketing actions and improve incremental profits. Effective targeting depends on accurately separating customers with positive versus negative treatment effects. We propose an approach to estimate the conditional average treatment effects (CATEs) of marketing actions that aligns their estimation with the firm's profit objective. The method recognizes that, for many customers, treatment effects are so extreme that additional accuracy is unlikely to change the recommended actions. However, accuracy matters near the decision boundary, as small errors can alter targeting decisions. By modifying the firm's objective function in the standard profit maximization problem, our method yields a near-optimal targeting policy while simultaneously estimating CATEs. This introduces a new perspective on CATE estimation, reframing it as a problem of profit optimization rather than prediction accuracy. We establish the theoretical properties of the proposed method and demonstrate its performance and trade-offs using synthetic data.

</details>


### [333] [From Many Models, One: Macroeconomic Forecasting with Reservoir Ensembles](https://arxiv.org/abs/2512.13642)
*Giovanni Ballarin,Lyudmila Grigoryeva,Yui Ching Li*

Main category: econ.EM

TL;DR: 研究多频回声状态网络（MFESNs）集成的有效性，提出的集成回声状态网络预测性能优于单个MFESN模型。


<details>
  <summary>Details</summary>
Motivation: 模型组合可提升性能，研究MFESNs集成在宏观经济时间序列预测中的有效性。

Method: 从理论和实证两方面研究，讨论Hedge和Follow - the - Leader方案，并将其在线学习保证扩展到相关数据情况。

Result: 提出的集成回声状态网络相比单个MFESN模型，预测性能显著提高。

Conclusion: MFESNs集成在宏观经济时间序列预测中是有效的，能提升预测性能。

Abstract: Model combination is a powerful approach to achieve superior performance with a set of models than by just selecting any single one. We study both theoretically and empirically the effectiveness of ensembles of Multi-Frequency Echo State Networks (MFESNs), which have been shown to achieve state-of-the-art macroeconomic time series forecasting results (Ballarin et al., 2024a). Hedge and Follow-the-Leader schemes are discussed, and their online learning guarantees are extended to the case of dependent data. In applications, our proposed Ensemble Echo State Networks show significantly improved predictive performance compared to individual MFESN models.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [334] [TreeVQA: A Tree-Structured Execution Framework for Shot Reduction in Variational Quantum Algorithms](https://arxiv.org/abs/2512.12068)
*Yuewen Hou,Dhanvi Bharadwaj,Gokul Subramanian Ravi*

Main category: quant-ph

TL;DR: 本文提出TreeVQA框架降低变分量子算法（VQAs）执行成本，评估显示可显著减少执行次数。


<details>
  <summary>Details</summary>
Motivation: VQAs执行成本高，实际应用需大量执行次数，成本难以承受。

Method: 提出基于树的执行框架TreeVQA，先联合执行任务，量子执行出现差异时再分支，以VQA包装器形式实现并集成到典型VQA应用中。

Result: 在科学和组合基准测试中，平均减少25.9倍执行次数，大规模问题减少超100倍，且问题规模和精度要求增加时效益更明显。

Conclusion: 利用应用任务间执行相似性，TreeVQA能显著降低VQAs执行成本。

Abstract: Variational Quantum Algorithms (VQAs) are promising for near- and intermediate-term quantum computing, but their execution cost is substantial. Each task requires many iterations and numerous circuits per iteration, and real-world applications often involve multiple tasks, scaling with the precision needed to explore the application's energy landscape. This demands an enormous number of execution shots, making practical use prohibitively expensive. We observe that VQA costs can be significantly reduced by exploiting execution similarities across an application's tasks. Based on this insight, we propose TreeVQA, a tree-based execution framework that begins by executing tasks jointly and progressively branches only as their quantum executions diverge. Implemented as a VQA wrapper, TreeVQA integrates with typical VQA applications. Evaluations on scientific and combinatorial benchmarks show shot count reductions of $25.9\times$ on average and over $100\times$ for large-scale problems at the same target accuracy. The benefits grow further with increasing problem size and precision requirements.

</details>


### [335] [Quantum oracles give an advantage for identifying classical counterfactuals](https://arxiv.org/abs/2512.13692)
*Ciarán M. Gilligan-Lee,Yìlè Yīng,Jonathan Richens,David Schmid*

Main category: quant-ph

TL;DR: 研究表明量子预言机在回答经典因果模型中的反事实问题上优于经典预言机，能识别未知因果参数。


<details>
  <summary>Details</summary>
Motivation: 经典预言机在结构因果模型中，观测数据和干预数据无法回答所有反事实问题，不同因果参数可产生相同观测与干预数据。

Method: 用简单二元示例，将经典变量编码到量子系统，把因果依赖关系编码到量子预言机，通过对预言机的相干查询进行研究。

Result: 相干探测可识别所有双向联合反事实，能为高阶多向反事实提供更严格的边界。

Conclusion: 量子预言机在回答经典反事实问题上有优势，还探讨了量子优势是否依赖非经典特性，二元情况下部分经典可解释理论的预言机也有优势。

Abstract: We show that quantum oracles provide an advantage over classical oracles for answering classical counterfactual questions in causal models, or equivalently, for identifying unknown causal parameters such as distributions over functional dependences. In structural causal models with discrete classical variables, observational data and even ideal interventions generally fail to answer all counterfactual questions, since different causal parameters can reproduce the same observational and interventional data while disagreeing on counterfactuals. Using a simple binary example, we demonstrate that if the classical variables of interest are encoded in quantum systems and the causal dependence among them is encoded in a quantum oracle, coherently querying the oracle enables the identification of all causal parameters -- hence all classical counterfactuals. We generalize this to arbitrary finite cardinalities and prove that coherent probing 1) allows the identification of all two-way joint counterfactuals p(Y_x=y, Y_{x'}=y'), which is not possible with any number of queries to a classical oracle, and 2) provides tighter bounds on higher-order multi-way counterfactuals than with a classical oracle. This work can also be viewed as an extension to traditional quantum oracle problems such as Deutsch--Jozsa to identifying more causal parameters beyond just, e.g., whether a function is constant or balanced. Finally, we raise the question of whether this quantum advantage relies on uniquely non-classical features like contextuality. We provide some evidence against this by showing that in the binary case, oracles in some classically-explainable theories like Spekkens' toy theory also give rise to a counterfactual identifiability advantage over strictly classical oracles.

</details>


### [336] [Quantum Implicit Neural Representations for 3D Scene Reconstruction and Novel View Synthesis](https://arxiv.org/abs/2512.12683)
*Yeray Cordero,Paula García-Molina,Fernando Vilariño*

Main category: quant-ph

TL;DR: 提出量子神经辐射场（Q - NeRF）用于神经辐射场渲染，评估其在多视图室内数据集上的表现，结果显示量子 - 经典混合模型在有限计算资源下有竞争力，为量子3D场景重建奠基。


<details>
  <summary>Details</summary>
Motivation: 经典隐式神经表示网络存在频谱偏差，限制捕捉高频细节能力，需新方法缓解。

Method: 将QIREN模块集成到Nerfacto骨干中，替换部分密度和辐射预测组件，形成Q - NeRF，在标准多视图室内数据集上评估三种混合配置。

Result: 量子 - 经典混合模型在有限计算资源下有竞争力，量子模块在表示精细、视图相关外观上有效。

Conclusion: Q - NeRF为可扩展量子3D场景重建提供基础，为未来量子神经渲染研究提供基线。

Abstract: Implicit neural representations (INRs) have become a powerful paradigm for continuous signal modeling and 3D scene reconstruction, yet classical networks suffer from a well-known spectral bias that limits their ability to capture high-frequency details. Quantum Implicit Representation Networks (QIREN) mitigate this limitation by employing parameterized quantum circuits with inherent Fourier structures, enabling compact and expressive frequency modeling beyond classical MLPs. In this paper, we present Quantum Neural Radiance Fields (Q-NeRF), the first hybrid quantum-classical framework for neural radiance field rendering. Q-NeRF integrates QIREN modules into the Nerfacto backbone, preserving its efficient sampling, pose refinement, and volumetric rendering strategies while replacing selected density and radiance prediction components with quantum-enhanced counterparts. We systematically evaluate three hybrid configurations on standard multi-view indoor datasets, comparing them to classical baselines using PSNR, SSIM, and LPIPS metrics. Results show that hybrid quantum-classical models achieve competitive reconstruction quality under limited computational resources, with quantum modules particularly effective in representing fine-scale, view-dependent appearance. Although current implementations rely on quantum circuit simulators constrained to few-qubit regimes, the results highlight the potential of quantum encodings to alleviate spectral bias in implicit representations. Q-NeRF provides a foundational step toward scalable quantum-enabled 3D scene reconstruction and a baseline for future quantum neural rendering research.

</details>


### [337] [Scalable Quantum Error Mitigation with Neighbor-Informed Learning](https://arxiv.org/abs/2512.12578)
*Zhenyu Chen,Bin Cheng,Minbo Gao,Xiaodie Lin,Ruiqi Zhang,Zhaohui Wei,Zhengfeng Ji*

Main category: quant-ph

TL;DR: 提出邻域信息学习（NIL）的量子误差缓解框架，统一并增强现有方法，有更好性能。


<details>
  <summary>Details</summary>
Motivation: 量子硬件噪声是实现量子计算潜力的主要障碍，现有量子误差缓解方法在性能、资源开销和理论保证间存在权衡难题。

Method: 引入NIL框架，通过结构相关“邻域”电路的噪声输出预测目标量子电路的理想输出，采用2 - design训练方法生成机器学习模型的训练数据。

Result: 相比传统基于学习的QEM协议，该方法在理论分析和数值模拟中都有更高准确性和效率，训练集规模与邻域电路总数呈对数关系。

Conclusion: 建立了理论可靠且实际高效的QEM框架，为在噪声硬件上实现量子优势提供可行途径。

Abstract: Noise in quantum hardware is the primary obstacle to realizing the transformative potential of quantum computing. Quantum error mitigation (QEM) offers a promising pathway to enhance computational accuracy on near-term devices, yet existing methods face a difficult trade-off between performance, resource overhead, and theoretical guarantees. In this work, we introduce neighbor-informed learning (NIL), a versatile and scalable QEM framework that unifies and strengthens existing methods such as zero-noise extrapolation (ZNE) and probabilistic error cancellation (PEC), while offering improved flexibility, accuracy, efficiency, and robustness.
  NIL learns to predict the ideal output of a target quantum circuit from the noisy outputs of its structurally related ``neighbor'' circuits. A key innovation is our 2-design training method, which generates training data for our machine learning model. In contrast to conventional learning-based QEM protocols that create training circuits by replacing non-Clifford gates with uniformly random Clifford gates, our approach achieves higher accuracy and efficiency, as demonstrated by both theoretical analysis and numerical simulation. Furthermore, we prove that the required size of the training set scales only \emph{logarithmically} with the total number of neighbor circuits, enabling NIL to be applied to problems involving large-scale quantum circuits. Our work establishes a theoretically grounded and practically efficient framework for QEM, paving a viable path toward achieving quantum advantage on noisy hardware.

</details>


### [338] [Practical Hybrid Quantum Language Models with Observable Readout on Real Hardware](https://arxiv.org/abs/2512.12710)
*Stefan Balauca,Ada-Astrid Balauca,Adrian Iftene*

Main category: quant-ph

TL;DR: 提出QRNN和QCNNs作为混合量子语言模型，在真实量子硬件上进行端到端训练和评估，介绍合成数据集，在IBM量子处理器实验揭示电路深度和可训练性权衡，验证在当前量子硬件训练复杂序列模型可行性。


<details>
  <summary>Details</summary>
Motivation: 利用近期量子设备进行序列数据处理，开展生成式量子自然语言处理。

Method: 结合硬件优化的参数化量子电路和轻量级经典投影层，采用多样本SPSA策略训练量子参数，引入合成数据集。

Result: 在IBM量子处理器实验揭示电路深度和可训练性的关键权衡，基于可观测量的读出能使NISQ设备成功学习序列模式。

Conclusion: 为生成式量子自然语言处理建立严格工程基线，验证在当前量子硬件训练复杂序列模型的可行性。

Abstract: Hybrid quantum-classical models represent a crucial step toward leveraging near-term quantum devices for sequential data processing. We present Quantum Recurrent Neural Networks (QRNNs) and Quantum Convolutional Neural Networks (QCNNs) as hybrid quantum language models, reporting the first empirical demonstration of generative language modeling trained and evaluated end-to-end on real quantum hardware. Our architecture combines hardware-optimized parametric quantum circuits with a lightweight classical projection layer, utilizing a multi-sample SPSA strategy to efficiently train quantum parameters despite hardware noise. To characterize the capabilities of these models, we introduce a synthetic dataset designed to isolate syntactic dependencies in a controlled, low-resource environment. Experiments on IBM Quantum processors reveal the critical trade-offs between circuit depth and trainability, demonstrating that while noise remains a significant factor, observable-based readout enables the successful learning of sequential patterns on NISQ devices. These results establish a rigorous engineering baseline for generative quantum natural language processing, validating the feasibility of training complex sequence models on current quantum hardware.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [339] [Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences](https://arxiv.org/abs/2512.13123)
*Liviu Aolaritei,Michael I. Jordan*

Main category: math.OC

TL;DR: 从任意时刻有效的置信序列角度研究凸优化中随机梯度下降（SGD）的停止规则，开发了数据依赖的上置信序列并得到简单停止规则。


<details>
  <summary>Details</summary>
Motivation: 经典SGD分析无法在任意时刻有效评估当前迭代与最优值的接近程度，需要新方法。

Method: 通过非负上鞅构建投影SGD加权平均次优性的任意时刻有效、数据依赖的上置信序列。

Result: 所构建的置信序列产生简单停止规则，以至少1 - α的概率达到ε - 最优，且在标准随机近似步长下几乎必然有限。

Conclusion: 这是基于可观测轨迹量的、针对具有一般凸目标的投影SGD的首个严格、时间一致的性能保证和有限时间ε - 最优性证明。

Abstract: We study stopping rules for stochastic gradient descent (SGD) for convex optimization from the perspective of anytime-valid confidence sequences. Classical analyses of SGD provide convergence guarantees in expectation or at a fixed horizon, but offer no statistically valid way to assess, at an arbitrary time, how close the current iterate is to the optimum. We develop an anytime-valid, data-dependent upper confidence sequence for the weighted average suboptimality of projected SGD, constructed via nonnegative supermartingales and requiring no smoothness or strong convexity. This confidence sequence yields a simple stopping rule that is provably $\varepsilon$-optimal with probability at least $1-α$ and is almost surely finite under standard stochastic approximation stepsizes. To the best of our knowledge, these are the first rigorous, time-uniform performance guarantees and finite-time $\varepsilon$-optimality certificates for projected SGD with general convex objectives, based solely on observable trajectory quantities.

</details>


### [340] [A Class of Accelerated Fixed-Point-Based Methods with Delayed Inexact Oracles and Its Applications](https://arxiv.org/abs/2512.13547)
*Nghia Nguyen-Trung,Quoc Tran-Dinh*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we develop a novel accelerated fixed-point-based framework using delayed inexact oracles to approximate a fixed point of a nonexpansive operator (or equivalently, a root of a co-coercive operator), a central problem in scientific computing. Our approach leverages both Nesterov's acceleration technique and the Krasnosel'skii-Mann (KM) iteration, while accounting for delayed inexact oracles, a key mechanism in asynchronous algorithms. We also introduce a unified approximate error condition for delayed inexact oracles, which can cover various practical scenarios. Under mild conditions and appropriate parameter updates, we establish both $\mathcal{O}(1/k^2)$ non-asymptotic and $o(1/k^2)$ asymptotic convergence rates in expectation for the squared norm of residual. Our rate significantly improves the $\mathcal{O}(1/k)$ rates in classical KM-type methods, including their asynchronous variants. We also establish $o(1/k^2)$ almost sure convergence rates and the almost sure convergence of iterates to a solution of the problem. Within our framework, we instantiate three settings for the underlying operator: (i) a deterministic universal delayed oracle; (ii) a stochastic delayed oracle; and (iii) a finite-sum structure with asynchronous updates. For each case, we instantiate our framework to obtain a concrete algorithmic variant for which our convergence results still apply, and whose iteration complexity depends linearly on the maximum delay. Finally, we verify our algorithms and theoretical results through two numerical examples on both matrix game and shallow neural network training problems.

</details>


### [341] [Rethinking Physics-Informed Regression Beyond Training Loops and Bespoke Architectures](https://arxiv.org/abs/2512.13217)
*Lorenzo Sabug,Eric Kerrigan*

Main category: math.OC

TL;DR: 提出直接计算预测点状态及现有样本导数和曲率信息的物理信息回归方法，具低计算成本等优势。


<details>
  <summary>Details</summary>
Motivation: 重新审视物理信息回归问题，寻找比基于全局函数逼近器（如神经网络）更好的解决方案。

Method: 将每个预测构建为约束优化问题，利用多元泰勒级数展开并明确执行物理定律。

Result: 在反应 - 扩散系统的对比基准测试中，预测精度与基于神经网络的解决方案相当，无需长时间训练，且对采样布局变化具有鲁棒性。

Conclusion: 该方法在物理信息回归问题上有优势，计算成本低且性能良好。

Abstract: We revisit the problem of physics-informed regression, and propose a method that directly computes the state at the prediction point, simultaneously with the derivative and curvature information of the existing samples. We frame each prediction as a constrained optimisation problem, leveraging multivariate Taylor series expansions and explicitly enforcing physical laws. Each individual query can be processed with low computational cost without any pre- or re-training, in contrast to global function approximator-based solutions such as neural networks. Our comparative benchmarks on a reaction-diffusion system show competitive predictive accuracy relative to a neural network-based solution, while completely eliminating the need for long training loops, and remaining robust to changes in the sampling layout.

</details>


### [342] [Better LMO-based Momentum Methods with Second-Order Information](https://arxiv.org/abs/2512.13227)
*Sarit Khirirat,Abdurakhmon Sadiev,Yury Demidovich,Peter Richtárik*

Main category: math.OC

TL;DR: 本文将HCM集成到基于LMO的框架中，在更宽松的平滑性和任意范数设置下提供收敛保证，实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 传统随机动量方法收敛保证不优于$O(1/K^{1/4})$，部分改进方法理论结果局限于欧几里得范数，限制了适用性。

Method: 将HCM集成到基于LMO的框架中，在放松的平滑性和任意范数设置下进行分析。

Result: 建立了HCM的$O(1/K^{1/3})$改进收敛率，实验验证了理论结果。

Conclusion: 所提方法能适应问题几何结构，比传统动量方法收敛更快。

Abstract: The use of momentum in stochastic optimization algorithms has shown empirical success across a range of machine learning tasks. Recently, a new class of stochastic momentum algorithms has emerged within the Linear Minimization Oracle (LMO) framework--leading to state-of-the-art methods, such as Muon, Scion, and Gluon, that effectively solve deep neural network training problems. However, traditional stochastic momentum methods offer convergence guarantees no better than the ${O}(1/K^{1/4})$ rate. While several approaches--such as Hessian-Corrected Momentum (HCM)--have aimed to improve this rate, their theoretical results are generally restricted to the Euclidean norm setting. This limitation hinders their applicability in problems, where arbitrary norms are often required. In this paper, we extend the LMO-based framework by integrating HCM, and provide convergence guarantees under relaxed smoothness and arbitrary norm settings. We establish improved convergence rates of ${O}(1/K^{1/3})$ for HCM, which can adapt to the geometry of the problem and achieve a faster rate than traditional momentum. Experimental results on training Multi-Layer Perceptrons (MLPs) and Long Short-Term Memory (LSTM) networks verify our theoretical observations.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [343] [ElasticVR: Elastic Task Computing in Multi-User Multi-Connectivity Wireless Virtual Reality (VR) Systems](https://arxiv.org/abs/2512.12366)
*Babak Badnava,Jacob Chakareski,Morteza Hashemi*

Main category: cs.IT

TL;DR: 提出ElasticVR框架，集成可扩展360视频切片，用多智能体深度强化学习解决VR计算任务的优化问题，性能优于无弹性计算的情况。


<details>
  <summary>Details</summary>
Motivation: 新兴VR应用对计算和数据速率要求高，需平衡通信、计算、能耗和QoE等多方面因素。

Method: 在边缘 - 客户端无线多连接架构中集成可扩展360视频切片，提出CPPG和IPPG两种多智能体深度强化学习解决方案。

Result: ElasticVR框架使PSNR提高43.21%，响应时间和能耗分别降低42.35%和56.83%。

Conclusion: ElasticVR框架在VR计算中表现良好，能有效解决多方面权衡问题，且IPPG方案提高了可扩展性。

Abstract: Diverse emerging VR applications integrate streaming of high fidelity 360 video content that requires ample amounts of computation and data rate. Scalable 360 video tiling enables having elastic VR computational tasks that can be scaled adaptively in computation and data rate based on the available user and system resources. We integrate scalable 360 video tiling in an edge-client wireless multi-connectivity architecture for joint elastic task computation offloading across multiple VR users called ElasticVR. To balance the trade-offs in communication, computation, energy consumption, and QoE that arise herein, we formulate a constrained QoE and energy optimization problem that integrates the multi-user/multi-connectivity action space with the elasticity of VR computational tasks. The ElasticVR framework introduces two multi-agent deep reinforcement learning solutions, namely CPPG and IPPG. CPPG adopts a centralized training and centralized execution approach to capture the coupling between users' communication and computational demands. This leads to globally coordinated decisions at the cost of increased computational overheads and limited scalability. To address the latter challenges, we also explore an alternative strategy denoted IPPG that adopts a centralized training with decentralized execution paradigm. IPPG leverages shared information and parameter sharing to learn robust policies; however, during execution, each user takes action independently based on its local state information only. The decentralized execution alleviates the communication and computation overhead of centralized decision-making and improves scalability. We show that the ElasticVR framework improves the PSNR by 43.21%, while reducing the response time and energy consumption by 42.35% and 56.83%, respectively, compared with a case where no elasticity is incorporated into VR computations.

</details>


### [344] [From Zipf's Law to Neural Scaling through Heaps' Law and Hilberg's Hypothesis](https://arxiv.org/abs/2512.13491)
*Łukasz Dębowski*

Main category: cs.IT

TL;DR: 研究了神经缩放定律与Zipf定律的演绎联系，表明在特定假设下神经缩放定律是Zipf定律的结果，并说明了推导步骤并举例。


<details>
  <summary>Details</summary>
Motivation: 探究机器学习中的神经缩放定律与定量语言学中Zipf定律之间的演绎关系。

Method: 从Zipf定律推导出Heaps定律，再从Heaps定律推导出Hilberg假设，最后从Hilberg假设推导出神经缩放定律。并用Santa Fe过程的例子说明推理步骤。

Result: 在某些宽泛假设下，神经缩放定律是Zipf定律的结果。

Conclusion: 清晰了神经缩放定律和Zipf定律之间的演绎联系。

Abstract: We inspect the deductive connection between the neural scaling law and Zipf's law -- two statements discussed in machine learning and quantitative linguistics. The neural scaling law describes how the cross entropy rate of a foundation model -- such as a large language model -- changes with respect to the amount of training tokens, parameters, and compute. By contrast, Zipf's law posits that the distribution of tokens exhibits a power law tail. Whereas similar claims have been made in more specific settings, we show that the neural scaling law is a consequence of Zipf's law under certain broad assumptions that we reveal systematically. The derivation steps are as follows: We derive Heaps' law on the vocabulary growth from Zipf's law, Hilberg's hypothesis on the entropy scaling from Heaps' law, and the neural scaling from Hilberg's hypothesis. We illustrate these inference steps by a toy example of the Santa Fe process that satisfies all the four statistical laws.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [345] [Hot Hém: Sài Gòn Giũa Cái Nóng Hông Còng Bàng -- Saigon in Unequal Heat](https://arxiv.org/abs/2512.11896)
*Tessa Vu*

Main category: cs.CV

TL;DR: 介绍Hot Hém工作流，结合多技术估算并实现胡志明市行人热暴露分析，用于热感知路由。


<details>
  <summary>Details</summary>
Motivation: 标准路由算法常忽略微尺度热变化，而行人热暴露对热带城市行人是关键健康风险。

Method: 采用Google Street View图像、语义图像分割和遥感技术，训练两个XGBoost模型预测地表温度，在行人网络节点部署。

Result: 提出的模型可用于热感知路由，并为确定城市特定区域高温地点及原因提供基础。

Conclusion: 该工作流有助于解决热带城市行人热暴露问题及热感知路由问题。

Abstract: Pedestrian heat exposure is a critical health risk in dense tropical cities, yet standard routing algorithms often ignore micro-scale thermal variation. Hot Hém is a GeoAI workflow that estimates and operationalizes pedestrian heat exposure in Hô Chí Minh City (HCMC), Vi\d{e}t Nam, colloquially known as Sài Gòn. This spatial data science pipeline combines Google Street View (GSV) imagery, semantic image segmentation, and remote sensing. Two XGBoost models are trained to predict land surface temperature (LST) using a GSV training dataset in selected administrative wards, known as phŏng, and are deployed in a patchwork manner across all OSMnx-derived pedestrian network nodes to enable heat-aware routing. This is a model that, when deployed, can provide a foundation for pinpointing where and further understanding why certain city corridors may experience disproportionately higher temperatures at an infrastructural scale.

</details>


### [346] [Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching](https://arxiv.org/abs/2512.12610)
*Wonseok Choi,Sohwi Lim,Nam Hyeon-Woo,Moon Ye-Bin,Dong-Ju Jeong,Jinyoung Hwang,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 提出Patchify框架用于实例级图像检索，引入LocScore评估指标，实验显示其性能优越，还应用产品量化提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决实例级图像检索中尽管目标存在大小、位置和外观变化仍需找到相同对象图像的挑战。

Method: 提出Patchify框架，将数据库图像分割成少量结构化补丁并与全局查询描述符比较；引入LocScore定位感知指标；应用产品量化进行高效大规模检索。

Result: 在多个基准测试、骨干网络和区域选择策略上的实验表明，Patchify优于全局方法并能补充最先进的重排序管道；使用信息丰富的特征进行压缩可显著提升性能。

Conclusion: Patchify是一个简单有效的逐补丁检索框架，具有高性能、可扩展性和可解释性，无需微调，LocScore是理解和改进检索行为的有价值诊断工具。

Abstract: Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance. To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning. Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching. To assess not just retrieval accuracy but also spatial correctness, we introduce LocScore, a localization-aware metric that quantifies whether the retrieved region aligns with the target object. This makes LocScore a valuable diagnostic tool for understanding and improving retrieval behavior. We conduct extensive experiments across multiple benchmarks, backbones, and region selection strategies, showing that Patchify outperforms global methods and complements state-of-the-art reranking pipelines. Furthermore, we apply Product Quantization for efficient large-scale retrieval and highlight the importance of using informative features during compression, which significantly boosts performance. Project website: https://wons20k.github.io/PatchwiseRetrieval/

</details>


### [347] [SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition](https://arxiv.org/abs/2512.12885)
*Minghao Zhu,Zhihao Zhang,Anmol Sidhu,Keith Redmill*

Main category: cs.CV

TL;DR: 本文介绍一种零样本识别框架以处理自动路牌识别问题，在测试中展现出有效性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在处理大量路牌类别和创建完整标注数据集时存在困难。

Method: 使用视觉语言模型从输入图像生成路牌文本描述，从向量数据库中检索相关候选，再用大语言模型推理得出最终识别结果。

Result: 在理想参考图像上准确率达95.58%，在真实复杂道路数据上达82.45%。

Conclusion: 基于RAG架构可创建无需特定任务训练、可扩展且准确的路牌识别系统。

Abstract: Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.

</details>


### [348] [Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion](https://arxiv.org/abs/2512.12935)
*Toan Le Ngo Thanh,Phat Ha Huu,Tan Nguyen Dang Duy,Thong Nguyen Le Minh,Anh Nguyen Nhu Tinh*

Main category: cs.CV

TL;DR: 针对视频内容增多对多模态时刻检索系统的需求，提出统一系统，有三方面创新，能处理模糊查询等问题。


<details>
  <summary>Details</summary>
Motivation: 视频内容指数级增长，现有多模态时刻检索系统面临跨模态噪声和模糊查询表现不佳、时态建模难捕捉连贯事件序列、需人工选择模态等问题。

Method: 提出统一系统，包含级联双嵌入管道结合BEIT - 3和SigLIP并由BLIP - 2重排序；时态感知评分机制用指数衰减惩罚大时间间隙；采用Agent引导的查询分解自动解释模糊查询并自适应融合分数。

Result: 定性分析表明系统能有效处理模糊查询、检索时间连贯的序列、动态调整融合策略。

Conclusion: 该系统推进了交互式时刻搜索能力。

Abstract: The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.

</details>


### [349] [TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding](https://arxiv.org/abs/2512.13511)
*Piyush Bagad,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出TARA方法将多模态大语言模型适配为时间感知视频文本嵌入模型，在新基准和标准基准上表现出色，还有其他优势。


<details>
  <summary>Details</summary>
Motivation: 构建通用的时间感知视频文本嵌入模型用于检索。

Method: 提出TARA方法，在不使用视频数据的情况下将多模态大语言模型适配为时间感知视频文本嵌入模型；提出新基准评估检索中的时间感知。

Result: TARA在新的手性基准上优于现有视频文本模型，在标准基准上也有好结果；TARA嵌入对否定敏感，在动词和副词理解上达最优。

Conclusion: TARA是强大、通用、时间感知的视频文本嵌入模型，零样本性能最优。

Abstract: Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.

</details>


### [350] [Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation](https://arxiv.org/abs/2512.11865)
*Ju-Young Kim,Ji-Hong Park,Myeongjun Kim,Gun-Woo Kim*

Main category: cs.CV

TL;DR: 提出基于OpenVLA - OFT框架的可解释对抗鲁棒视觉 - 语言 - 动作模型，能检测光度扰动并解释，实验显示可提升对抗条件下动作预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 智能农业中依赖RGB相机和机器人操纵器的系统易受光度扰动影响，在对抗攻击下会出现故障。

Method: 提出基于OpenVLA - OFT框架的可解释对抗鲁棒视觉 - 语言 - 动作模型，集成Evidence - 3模块检测光度扰动并生成自然语言解释。

Result: 与基线相比，提出的模型使当前动作L1损失降低21.7%，下一动作L1损失降低18.4%。

Conclusion: 该模型在对抗条件下提高了动作预测的准确性和可解释性。

Abstract: Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.

</details>


### [351] [Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops](https://arxiv.org/abs/2512.11871)
*Tekleab G. Gebremedhin,Hailom S. Asegede,Bruh W. Tesheme,Tadesse B. Gebremichael,Kalayu G. Redae*

Main category: cs.CV

TL;DR: 本文提出离线优先的农作物病害检测系统，以仙人掌果数据集评估三种模型，展示性能权衡，模型部署在本地化应用支持离线推理。


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚提格雷地区基础设施受限，缺乏专业作物病害诊断途径，需开发检测系统。

Method: 构建含3587张图像的仙人掌果数据集，对自定义轻量级CNN、EfficientNet - Lite1和MobileViT - XS三种模型进行基准测试，评估仙人掌果模型性能。

Result: EfficientNet - Lite1测试准确率90.7%；轻量级CNN准确率89.5%，部署优势大；MobileViT - XS平均交叉验证准确率97.3%，基于MHSA的全局推理更能区分病虫害特征。

Conclusion: 三种模型各有优劣，ARM兼容模型部署在本地化应用，增强食品安全诊断的包容性。

Abstract: Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.

</details>


### [352] [FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications](https://arxiv.org/abs/2512.11925)
*Mozhgan Hadadi,Talukder Z. Jubery,Patrick S. Schnable,Arti Singh,Bedrich Benes,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: 提出LLM辅助框架FloraForge，助领域专家用自然语言生成3D植物模型，兼顾易用性与数学严谨性。


<details>
  <summary>Details</summary>
Motivation: 现有3D植物模型构建方法存在需大量数据、缺乏可编辑性、需专业知识等局限，难以满足需求。

Method: 利用LLM辅助共设计精炼Python脚本，生成带植物约束的参数化植物几何，以分层B样条表面表示。通过手动细化植物描述符文件拟合经验点云数据。

Result: 在玉米、大豆和绿豆上展示框架，生成用于可视化和定量分析的三角网格。

Conclusion: 该框架使植物科学复杂几何建模更普及，同时保持数学严谨性。

Abstract: Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.

</details>


### [353] [MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion](https://arxiv.org/abs/2512.11928)
*Alexander Peysakhovich,William Berman,Joseph Rufo,Felix Wong,Maxwell Z. Wilson*

Main category: cs.CV

TL;DR: 本文训练扩散模型MONET从明场图像预测细胞染色通道，展示模型质量与规模的关系及架构优势，指出虚拟细胞染色是生物研究的补充工具。


<details>
  <summary>Details</summary>
Motivation: 解决传统细胞染色技术劳动密集且无法研究细胞动力学的问题。

Method: 在大数据集上训练扩散模型MONET，使用一致性架构生成延时视频。

Result: 模型质量随规模提升，架构可实现上下文学习，能部分迁移到分布外的细胞系和成像协议。

Conclusion: 虚拟细胞染色不能完全替代物理细胞染色，可作为生物研究的补充工具。

Abstract: Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.

</details>


### [354] [DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition](https://arxiv.org/abs/2512.11941)
*Jingmin Zhu,Anqi Zhu,James Bailey,Jun Liu,Hossein Rahmani,Mohammed Bennamoun,Farid Boussaid,Qiuhong Ke*

Main category: cs.CV

TL;DR: 提出DynaPURLS框架解决零样本骨架动作识别问题，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有零样本骨架动作识别方法依赖粗粒度特征与静态语义对齐，无法解决领域偏移问题，阻碍细粒度视觉知识迁移。

Method: 引入DynaPURLS框架，利用大语言模型生成层次化文本描述，自适应分区模块生成细粒度视觉表示，动态细化模块结合轻量级投影和记忆库减轻噪声伪标签影响。

Result: 在NTU RGB+D 60/120和PKU - MMD三个大规模基准数据集上显著超越现有方法，创造新的最优记录。

Conclusion: DynaPURLS框架有效解决了零样本骨架动作识别问题，具有良好的泛化能力。

Abstract: Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS

</details>


### [355] [V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions](https://arxiv.org/abs/2512.11995)
*Chenrui Fan,Yijun Liang,Shweta Bhardwaj,Kwesi Cobbina,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: 现有视觉语言模型处理复杂开放式任务能力不足，本文提出评估套件V - REX，对模型多步探索推理能力进行评估，揭示模型能力特点和改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在处理复杂开放式任务时表现不佳，且视觉思维路径难以评估。

Method: 开发评估套件V - REX，包括具有挑战性的视觉推理任务基准和评估协议，将多步探索推理转化为问题链，分步分析模型规划和跟进能力。

Result: 通过评估SOTA模型，揭示了模型能力的一致缩放趋势、规划与跟进能力的显著差异。

Conclusion: 当前模型在多步探索推理方面有很大改进空间。

Abstract: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.

</details>


### [356] [Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus](https://arxiv.org/abs/2512.12012)
*Antonio Guillen-Perez*

Main category: cs.CV

TL;DR: 针对自动驾驶车辆长尾训练数据稀缺问题，提出Semantic - Drive框架，在召回率等方面表现良好且可运行于消费级硬件。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆开发受长尾训练数据稀缺瓶颈制约，现有识别安全关键事件的方法存在精度不足、侵犯隐私和成本高的问题。

Method: 提出本地优先、神经符号语义数据挖掘框架Semantic - Drive，将感知解耦为两个阶段，并采用'Judge - Scout'共识机制减少幻觉。

Result: 在nuScenes数据集上，召回率达0.966（CLIP为0.475），风险评估误差较单模型降低40%，可在消费级硬件运行。

Conclusion: Semantic - Drive是云方案的隐私保护替代方案。

Abstract: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.

</details>


### [357] [A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery](https://arxiv.org/abs/2512.12128)
*Thomas Manzini,Priyankari Perali,Raisa Karnik,Robin R. Murphy*

Main category: cs.CV

TL;DR: 本文推出道路损伤评估和道路对齐的大型基准数据集，训练18个基线模型，解决现有灾害后道路损伤评估数据集的问题，还处理了道路线对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有灾害道路损伤评估数据集规模小或图像分辨率低，且机器学习系统未得到运营验证，实际中存在道路线未对齐问题影响模型性能。

Method: 按10类标注方案标注657.25公里道路，在2024年飓风应对中训练和部署机器学习模型，提供9184次道路线调整用于空间对齐。

Result: 未考虑空间对齐时，约8%（11公里）道路不良状况标注错误，约9%（59公里）道路线与实际道路未对齐，18个基线模型在未对齐道路线上性能平均下降5.596% Macro IoU。

Conclusion: 机器学习、计算机视觉和机器人领域需解决这些问题，以在灾害时实现更有效决策。

Abstract: This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\% Macro IoU. If spatial alignment is not considered, approximately 8\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.

</details>


### [358] [MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater](https://arxiv.org/abs/2512.12142)
*Björn Lütjens,Patrick Alexander,Raf Antwerpen,Til Widmann,Guido Cervone,Marco Tedesco*

Main category: cs.CV

TL;DR: 本文开发深度学习模型，融合遥感观测与物理模型数据，生成每日100米分辨率的格网化地表融水图，精度优于现有方法，并发布数据集。


<details>
  <summary>Details</summary>
Motivation: 格陵兰冰盖加速融化，过程不明且难测量，现有融水图无法同时兼顾时间和空间高分辨率。

Method: 开发深度学习模型，融合遥感观测和物理模型数据流，对区域气候模型输出进行时空降尺度处理。

Result: 深度学习方法比现有非深度学习方法精度高超10个百分点，滑动窗口计算也有不错精度。

Conclusion: 所提方法有效，发布数据集MeltwaterBench用于对比更复杂的数据驱动降尺度方法。

Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.

</details>


### [359] [Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms](https://arxiv.org/abs/2512.12199)
*Ercan Erkalkan,Vedat Topuz,Ayça Ak*

Main category: cs.CV

TL;DR: 提出适用于野火环境有限带宽下微型无人机团队的轻量级周边跟踪方法，经仿真验证有效可行。


<details>
  <summary>Details</summary>
Motivation: 在野火环境有限带宽条件下，实现微型无人机团队的有效周边跟踪，用于应急侦察。

Method: 热图像帧通过自适应阈值和形态学细化生成粗热区掩码，RGB帧用基于梯度的滤波提供边缘线索并抑制纹理相关误检；用规则级合并策略选择边界候选并简化；加入周期性信标和惯性反馈回路；在嵌入式SoC平台上限制每帧像素操作和预计算梯度表。

Result: 小规模仿真显示，与纯边缘跟踪基线相比，平均路径长度和边界抖动减少，保持环境覆盖；电池消耗和计算利用率表明在标准微型平台上可实现10 - 15 m/s的前进运动。

Conclusion: 该方法能快速实地部署，满足应急侦察应用对鲁棒传感和最少通信的要求。

Abstract: This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.

</details>


### [360] [ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB](https://arxiv.org/abs/2512.12206)
*Jeongjun Park,Sunwook Hwang,Hyeonho Noh,Jin Mo Yang,Hyun Jong Yang,Saewoong Bahk*

Main category: cs.CV

TL;DR: 本文提出ALERT数据集解决缺乏大规模UWB数据集问题，提出ISA - ViT框架解决ViT适配非标准尺寸UWB数据问题，实验表明该框架提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 分心驾驶导致致命事故，使用IR - UWB雷达的驾驶员活动识别有优势，但缺乏大规模UWB数据集和ViT适配非标准尺寸数据困难限制其应用。

Method: 提出含10220个样本的ALERT数据集，构建输入大小无关的Vision Transformer（ISA - ViT）框架调整UWB数据尺寸，采用领域融合策略结合多领域特征。

Result: ISA - ViT相比现有基于ViT的方法在UWB驾驶员活动识别上准确率提升22.68%。

Conclusion: 公开数据集和输入大小无关策略，有助于开发更强大、可扩展的分心驾驶检测系统用于实际部署。

Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.

</details>


### [361] [Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs](https://arxiv.org/abs/2512.12222)
*Nathalie Alexander,Arnaud Gucciardi,Umberto Michelucci*

Main category: cs.CV

TL;DR: 研究对比SynthSeg和SamSeg对婴儿脑MRI分割准确性及对体积和分形维数估计的影响，发现SynthSeg表现更优但结果解读需谨慎。


<details>
  <summary>Details</summary>
Motivation: 婴儿脑MRI自动分割因髓鞘形成和组织对比度低具有挑战性，需准确分割来量化脑结构和复杂性发育变化。

Method: 使用BOB数据集，用Dice、交并比等指标将SynthSeg和SamSeg与专家标注对比评估。

Result: SynthSeg在各质量指标上优于SamSeg，分割准确性随年龄提高，两方法分形维数分析有差异，体积和分形维数偏差正相关。

Conclusion: SynthSeg提供最可靠结果，但因分割不确定性，小形态差异结果解读需谨慎。

Abstract: Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.

</details>


### [362] [A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams](https://arxiv.org/abs/2512.12410)
*Khalfalla Awedat,Mohamed Abidalrekab,Mohammad El-Yabroudi*

Main category: cs.CV

TL;DR: 本文提出基于图注意力网络（GAT）框架，仅用当前激光雷达帧重建垂直波束缺失通道，在模拟数据上效果良好，证明纯图注意力模型可有效恢复垂直波束


<details>
  <summary>Details</summary>
Motivation: 旋转式激光雷达传感器因硬件老化等因素出现垂直波束缺失，严重影响自动驾驶车辆3D感知

Method: 将激光雷达扫描表示为非结构化空间图，用多层GAT学习局部几何邻域的自适应注意力权重，直接回归缺失的高程值

Result: 在1065个模拟数据上平均高度RMSE为11.67cm，87.98%重建点误差在10cm内，单GPU推理每帧14.65秒，不同邻域大小重建质量稳定

Conclusion: 纯图注意力模型仅基于原始点云几何可在实际传感器退化下有效恢复垂直波束

Abstract: Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.

</details>


### [363] [StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding](https://arxiv.org/abs/2512.12560)
*Xinqi Jin,Hanxun Yu,Bohan Yu,Kebin Liu,Jian Liu,Keda Tao,Yixuan Pei,Huan Wang,Fan Dang,Jiangchuan Liu,Weiqiang Wang*

Main category: cs.CV

TL;DR: 针对在线视频理解应用中MLLMs显存与计算问题，提出基于MSSAVT的令牌剪枝方法，结合现有方法消除冗余，实验显示可提升精度且剪枝延迟低。


<details>
  <summary>Details</summary>
Motivation: 在线视频理解应用中，使用MLLMs处理大量视频帧会导致高显存占用和计算延迟。

Method: 提出MSSAVT冗余度量，设计掩码剪枝策略，结合现有基于时间冗余的剪枝方法。

Result: 在多个基准测试中，方法最多提升4%精度，剪枝延迟小于1ms。

Conclusion: 提出的剪枝方法能有效减少上下文长度，提升在线视频理解性能。

Abstract: Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.

</details>


### [364] [Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models](https://arxiv.org/abs/2512.12596)
*Kei Yoshitake,Kento Hosono,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.CV

TL;DR: 提出用视觉语言模型（VLM）生成基于图像广告布局的方法，经实验验证效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统广告布局技术依赖显著性映射，无法充分考虑图像详细构成和语义内容，需要改进。

Method: 利用VLM识别背景图像中的产品和元素，布局生成管道分两步：先分析图像生成文本‘放置计划’，再将计划转换为HTML代码。

Result: 通过定量和定性比较实验，表明该方法生成的广告布局质量明显更高。

Conclusion: 明确考虑背景图像内容的方法能生成更高质量的广告布局。

Abstract: In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based "placement plan" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.

</details>


### [365] [DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model](https://arxiv.org/abs/2512.12633)
*Zhou Tao,Shida Wang,Yongxiang Hua,Haoyu Cao,Linli Xu*

Main category: cs.CV

TL;DR: 文章提出DiG框架提升多模态大语言模型细粒度视觉感知和空间推理能力，实验证明其效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型细粒度视觉感知和精确空间推理能力有限。

Method: 引入DiG框架，通过识别定位相似图像对差异学习；开发基于3D渲染的数据生成管道；采用课程学习应对差异信号稀疏问题。

Result: DiG显著提升视觉感知基准测试表现，学习的细粒度感知技能有效迁移到下游任务。

Conclusion: 差异定位是提升多模态大语言模型细粒度视觉推理的可扩展且稳健方法。

Abstract: Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.

</details>


### [366] [Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images](https://arxiv.org/abs/2512.12662)
*Muhammad Umar Farooq,Abd Ur Rehman,Azka Rehman,Muhammad Usman,Dong-Kyu Chae,Junaid Qadir*

Main category: cs.CV

TL;DR: 提出SSMT - Net用于超声图像甲状腺结节分割，结合半监督和多任务学习，在数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动分割方法面临结节与周围组织边界模糊、大小变化及标注数据少等挑战，且难以整合上下文信息和有效泛化。

Method: 提出半监督多任务Transformer网络SSMT - Net，先在无监督阶段利用未标注数据增强基于Transformer的编码器特征提取能力，在监督阶段联合优化结节分割、腺体分割和结节大小估计。

Result: 在TN3K和DDTI数据集上的评估显示，SSMT - Net比现有方法有更高的准确性和鲁棒性。

Conclusion: SSMT - Net有用于实际临床应用的潜力。

Abstract: Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.

</details>


### [367] [Improving Long-Tailed Object Detection with Balanced Group Softmax and Metric Learning](https://arxiv.org/abs/2511.16619)
*Satyam Gaba*

Main category: cs.CV

TL;DR: 本文使用LVISv1数据集解决长尾2D目标检测问题，对BAGS框架改进并探索度量学习，提升了稀有类别检测性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中目标检测存在长尾分布，类别不平衡使模型偏向常见类别，降低稀有类别检测性能。

Method: 采用两阶段Faster R - CNN架构，改进Balanced Group Softmax框架；探索度量学习生成特征嵌入，推理时使用k - Nearest Neighbors方法。

Result: 达到新的最优性能，平均精度均值（mAP）为24.5%，超过之前的24.0%。

Conclusion: 所提出的方法在推进长尾目标检测方面有效。

Abstract: Object detection has been widely explored for class-balanced datasets such as COCO. However, real-world scenarios introduce the challenge of long-tailed distributions, where numerous categories contain only a few instances. This inherent class imbalance biases detection models towards the more frequent classes, degrading performance on rare categories. In this paper, we tackle the problem of long-tailed 2D object detection using the LVISv1 dataset, which consists of 1,203 categories and 164,000 images. We employ a two-stage Faster R-CNN architecture and propose enhancements to the Balanced Group Softmax (BAGS) framework to mitigate class imbalance. Our approach achieves a new state-of-the-art performance with a mean Average Precision (mAP) of 24.5%, surpassing the previous benchmark of 24.0%.
  Additionally, we hypothesize that tail class features may form smaller, denser clusters within the feature space of head classes, making classification challenging for regression-based classifiers. To address this issue, we explore metric learning to produce feature embeddings that are both well-separated across classes and tightly clustered within each class. For inference, we utilize a k-Nearest Neighbors (k-NN) approach to improve classification performance, particularly for rare classes. Our results demonstrate the effectiveness of these methods in advancing long-tailed object detection.

</details>


### [368] [Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling](https://arxiv.org/abs/2512.12675)
*Yuran Wang,Bohan Zeng,Chengzhuo Tong,Wenxuan Liu,Yang Shi,Xiaochen Ma,Hao Liang,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出Scone方法解决多主体图像生成区分性问题，还引入SconeEval基准，实验表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有多主体图像生成忽略区分性，限制了在复杂视觉场景中的有效性。

Method: 提出Scone统一理解生成方法，采用两阶段训练方案，引入SconeEval基准。

Result: Scone在两个基准上的组合和区分任务中优于现有开源模型。

Conclusion: Scone能有效解决多主体图像生成的区分性问题，代码和数据开源。

Abstract: Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.

</details>


### [369] [Robust Motion Generation using Part-level Reliable Data from Videos](https://arxiv.org/abs/2512.12703)
*Boyuan Li,Sipeng Zheng,Bin Cao,Ruihua Song,Zongqing Lu*

Main category: cs.CV

TL;DR: 本文提出利用视频中可信的部分级数据，通过鲁棒的部分感知掩码自回归模型增强运动生成，并贡献了新基准K700 - M，实验表明方法优于基线。


<details>
  <summary>Details</summary>
Motivation: 从大规模网络视频中提取人体运动可解决角色动画数据稀缺问题，但视频帧中部分人体部位不可见，丢弃缺失数据限制规模和多样性，保留则影响数据质量和模型性能。

Method: 提出利用视频中可信部分级数据的方法，将人体分解为五部分，检测可见部分为“可信”部分，用部分感知变分自编码器将其编码为潜在令牌，提出鲁棒的部分级掩码生成模型预测掩码后的可信部分，忽略噪声部分；还贡献了新基准K700 - M。

Result: 实验结果表明该方法在干净和有噪声的数据集上，在运动质量、语义一致性和多样性方面均优于基线。

Conclusion: 提出的利用可信部分级数据增强运动生成的方法有效，能解决从视频中提取人体运动时面临的数据问题。

Abstract: Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.
  To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as "credible". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.
  In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/

</details>


### [370] [mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description](https://arxiv.org/abs/2512.11894)
*Mahathir Monjur,Shahriar Nirjon*

Main category: cs.CV

TL;DR: 提出mmWeaver框架用INRs合成复杂毫米波信号，实现49倍压缩，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达应用需多样、特定环境信号数据集，但物理模拟计算成本高。

Method: 用隐式神经表示（INRs）将毫米波信号建模为连续函数，结合超网络根据环境和运动特征动态生成INR参数。

Result: 达到0.88的复杂SSIM和35 dB的PSNR，在信号逼真度上优于现有方法，活动识别准确率提高7%，人体姿态估计误差降低15%，运行速度比基于模拟的方法快6 - 35倍。

Conclusion: mmWeaver框架在毫米波信号合成方面表现优越，能有效促进毫米波雷达应用发展。

Abstract: Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.

</details>


### [371] [CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence](https://arxiv.org/abs/2512.12768)
*Tianjiao Yu,Xinzhuo Li,Yifan Shen,Yuanzhe Liu,Ismini Lourentzou*

Main category: cs.CV

TL;DR: 提出CoRe3D统一3D理解与生成推理框架，结合语义和空间抽象，使输出与语言描述对齐。


<details>
  <summary>Details</summary>
Motivation: 现有以推理为中心的方法在3D领域发展不足，需提升模型在3D方面的可靠性、可解释性和跨模态对齐能力。

Method: 提出CoRe3D推理框架，将3D潜在空间分解为局部区域，结合语义思维链推理和结构化空间推理。

Result: CoRe3D生成的3D输出具有强局部一致性，且与语言描述忠实对齐。

Conclusion: 显式推理机制对3D理解和生成有效，CoRe3D框架有积极作用。

Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.

</details>


### [372] [CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities](https://arxiv.org/abs/2512.11901)
*Santosh Patapati*

Main category: cs.CV

TL;DR: 介绍通用多模态融合架构CLARGA，可处理任意数量和类型模态，经多数据集验证效果好。


<details>
  <summary>Details</summary>
Motivation: 开发能处理任意数量和类型模态、适用于多种机器学习任务的多模态融合架构。

Method: 构建注意力加权图，用多头图注意力网络传递消息，通过可学习掩码适应缺失模态输入，采用混合目标函数训练。

Result: 在7个数据集的多模态表示学习任务中表现出色，优于基线、最先进模型和消融实验。

Conclusion: CLARGA可轻松融入机器学习模型，实现跨多种任务的高效表示学习。

Abstract: We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.

</details>


### [373] [MPath: Multimodal Pathology Report Generation from Whole Slide Images](https://arxiv.org/abs/2512.11906)
*Noorul Wahab,Nasir Rajpoot*

Main category: cs.CV

TL;DR: 介绍MPath框架用于从全切片图像自动生成诊断病理报告，在数据集上评估排名第4，凸显基于提示的多模态调节策略潜力。


<details>
  <summary>Details</summary>
Motivation: 解决将高分辨率组织模式转化为临床连贯文本的难题，推动自动生成诊断病理报告这一新兴方向发展。

Method: 引入MPath框架，通过学习的视觉前缀提示机制，利用基础模型WSI特征，经紧凑投影模块注入预训练生物医学语言模型BioBART，保持语言主干冻结。

Result: 在RED 2025 Grand Challenge数据集上开发和评估，测试阶段2排名第4。

Conclusion: 基于提示的多模态调节是病理报告生成可扩展且可解释的策略。

Abstract: Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.

</details>


### [374] [Exploring Spatial-Temporal Representation via Star Graph for mmWave Radar-based Human Activity Recognition](https://arxiv.org/abs/2512.12013)
*Senhao Gao,Junqing Zhang,Luoyu Mei,Shuai Wang,Xuyu Wang*

Main category: cs.CV

TL;DR: 提出基于离散动态图神经网络的图表示方法用于毫米波雷达点云人体活动识别，实验效果好且适用于资源受限平台。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达点云人体活动识别系统存在稀疏和可变大小问题，现有预处理算法非最优。

Method: 设计星型图描述静态中心点与动态毫米波雷达点关系，采用DDGNN学习星型图特征。

Result: 在真实数据集上表现优于基线方法，整体分类准确率达94.27%，接近视觉骨架数据准确率，在树莓派4上测试有效。

Conclusion: 所提方法有效，消融实验验证模型设计，优于近期雷达特定方法且无需重采样或帧聚合器。

Abstract: Human activity recognition (HAR) requires extracting accurate spatial-temporal features with human movements. A mmWave radar point cloud-based HAR system suffers from sparsity and variable-size problems due to the physical features of the mmWave signal. Existing works usually borrow the preprocessing algorithms for the vision-based systems with dense point clouds, which may not be optimal for mmWave radar systems. In this work, we proposed a graph representation with a discrete dynamic graph neural network (DDGNN) to explore the spatial-temporal representation of human movement-related features. Specifically, we designed a star graph to describe the high-dimensional relative relationship between a manually added static center point and the dynamic mmWave radar points in the same and consecutive frames. We then adopted DDGNN to learn the features residing in the star graph with variable sizes. Experimental results demonstrated that our approach outperformed other baseline methods using real-world HAR datasets. Our system achieved an overall classification accuracy of 94.27\%, which gets the near-optimal performance with a vision-based skeleton data accuracy of 97.25\%. We also conducted an inference test on Raspberry Pi~4 to demonstrate its effectiveness on resource-constraint platforms. \sh{ We provided a comprehensive ablation study for variable DDGNN structures to validate our model design. Our system also outperformed three recent radar-specific methods without requiring resampling or frame aggregators.

</details>


### [375] [CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos](https://arxiv.org/abs/2512.12060)
*Tejas Panambur,Ishan Rajendrakumar Dave,Chongjian Ge,Ersin Yumer,Xue Bai*

Main category: cs.CV

TL;DR: 提出CreativeVR框架用于修复AI生成和真实视频的严重结构与时间伪影，在多方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现代文本到视频扩散模型存在精细结构问题，现有视频修复方法无法有效处理这些严重结构伪影。

Method: 引入基于深度适配器的CreativeVR框架，有控制模型跟随输入强度的精度旋钮，训练时使用时间连贯的退化模块。

Result: 在AIGC54基准测试和标准视频修复基准测试中取得良好结果，运行吞吐量实用。

Conclusion: CreativeVR能有效修复严重结构和时间伪影，在视频修复任务中表现优异。

Abstract: Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.
  We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.
  To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.

</details>


### [376] [BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models](https://arxiv.org/abs/2512.12080)
*Ryan Po,Eric Ryan Chan,Changan Chen,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 提出Backwards Aggregation (BAgger) 解决自回归视频模型的曝光偏差问题，在多个视频生成任务上效果良好。


<details>
  <summary>Details</summary>
Motivation: 自回归视频模型存在曝光偏差，会导致错误累积和质量下降。

Method: 引入自监督方案Backwards Aggregation (BAgger)，通过模型自身的滚动构建纠正轨迹，并使用标准分数或流匹配目标进行训练。

Result: 在文本到视频、视频扩展和多提示生成任务中，观察到更稳定的长期运动和更好的视觉一致性，减少了漂移。

Conclusion: Backwards Aggregation (BAgger) 能有效解决自回归视频模型的曝光偏差问题。

Abstract: Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.

</details>


### [377] [SPDMark: Selective Parameter Displacement for Robust Video Watermarking](https://arxiv.org/abs/2512.12090)
*Samar Fares,Nurbek Tastan,Karthik Nandakumar*

Main category: cs.CV

TL;DR: 本文提出基于视频扩散模型选择性参数位移的视频水印框架SPDMark，能生成难以察觉且可高精度恢复的水印，对常见视频修改具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视频水印方法无法同时实现不可感知性、鲁棒性和计算效率，需要新的水印方案。

Method: 基于视频扩散模型的选择性参数位移，用低秩自适应实现基础位移，联合学习基础位移和水印提取器，用加密哈希函数检测视频修改，用最大二分匹配恢复帧顺序。

Result: 在文本到视频和图像到视频生成模型上的评估表明，SPDMark能生成不可感知的水印，可高精度恢复，对多种常见视频修改具有鲁棒性。

Conclusion: SPDMark是一种有效的视频水印框架，能满足视频水印的多种需求。

Abstract: The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.

</details>


### [378] [Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding](https://arxiv.org/abs/2512.12822)
*Yongyuan Liang,Xiyao Wang,Yuanchen Ju,Jianwei Yang,Furong Huang*

Main category: cs.CV

TL;DR: 提出Lemon统一Transformer架构处理3D点云补丁和语言标记，处理3D理解挑战，在3D任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 3D理解中，点云数据稀疏不规则，现有模型架构碎片化、训练不稳定且扩展性差。

Method: 引入Lemon统一Transformer架构，联合处理3D点云补丁和语言标记；设计结构化补丁化和标记化方案，采用三阶段训练策略。

Result: 在3D理解和推理任务上达到了新的最优性能，模型大小和训练数据增加时展现出强大的扩展性。

Conclusion: 为现实世界应用中推进3D空间智能提供了统一基础。

Abstract: Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.

</details>


### [379] [AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging](https://arxiv.org/abs/2512.12101)
*Swarn S. Warshaneyan,Maksims Ivanovs,Blaž Cugmas,Inese Bērziņa,Laura Goldberga,Mindaugas Tamosiunas,Roberts Kadiķis*

Main category: cs.CV

TL;DR: 本文对光学和数字同轴全息显微镜图像进行全自动花粉识别研究，建立性能基线，通过扩展边界框、生成合成图像等方法提升性能，证明GAN增强可缩小性能差距。


<details>
  <summary>Details</summary>
Motivation: 在未重建的全息图像中视觉识别花粉具挑战性，需提升数字同轴全息显微镜（DIHM）图像的花粉识别性能。

Method: 使用YOLOv8s进行目标检测、MobileNetV3L进行分类；扩展DIHM图像中花粉边界框；用WGAN - SN生成合成DIHM图像；混合真实和合成数据。

Result: 光学数据检测mAP50达91.3%、分类准确率达97%；DIHM数据原始检测mAP50为8.15%、分类准确率50%，扩展边界框后检测mAP50为13.3%、分类准确率54%；生成合成图像FID分数58.246；按1.0 : 1.5比例混合数据后检测提升至15.4%。

Conclusion: 基于GAN的增强可缩小性能差距，推动全自动DIHM兽医成像工作流程迈向实际应用。

Abstract: We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.

</details>


### [380] [Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners](https://arxiv.org/abs/2512.12824)
*N. K. B. M. P. K. B. Narasinghe,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 本文围绕CoCa视觉骨干进行少样本图像分类的适配开展全面实证研究，评估不同策略，有多项新发现并给出训练配置参考。


<details>
  <summary>Details</summary>
Motivation: 现有大规多模态基础模型虽在零样本迁移效果好，但在极端数据稀缺下游任务（少样本学习）适配研究不足，特别是对CoCa潜在空间进行参数高效微调的响应研究缺乏。

Method: 对CoCa视觉骨干用于少样本图像分类进行全面实证研究，系统评估从无训练混合原型到基于低秩自适应（LoRA）的深度参数自适应等一系列策略。

Result: 发现强数据增强在低样本设置中对线性探测和LoRA微调表现的不同影响；含监督对比损失的混合目标比标准交叉熵在不同样本数上有性能提升；明确训练配置对数据稀缺性的敏感性。

Conclusion: 为生成 - 对比基础模型的高效适配提供缩放正则化、秩和采样策略等方面的实证参考设置。

Abstract: Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an "augmentation divergence": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.

</details>


### [381] [A Novel Patch-Based TDA Approach for Computed Tomography](https://arxiv.org/abs/2512.12108)
*Dashti A. Ali,Aras T. Asaad,Jacob J. Peoples,Mohammad Hamghalam,Alex Robins,Mane Piliposyan,Richard K. G. Do,Natalie Gangai,Yun S. Chun,Ahmad Bashir Barekzai,Jayasree Chakraborty,Hala Khasawneh,Camila Vilela,Natally Horvat,João Miranda,Alice C. Wei,Amber L. Simpson*

Main category: cs.CV

TL;DR: 本文提出针对3D CT图像的基于补丁的持久同调构建方法，实验表明该方法在分类性能和时间效率上优于3D立方体复形算法，并提供Python包。


<details>
  <summary>Details</summary>
Motivation: 现有3D立方体复形过滤构建持久同调的方法在高分辨率CT图像上性能不佳且有计算复杂度问题，需更好方法。

Method: 引入针对体积医学成像数据（特别是CT）的基于补丁的持久同调构建方法，在多个3D CT图像数据集上实验，与3D立方体复形算法对比。

Result: 基于补丁的TDA方法在分类性能和时间效率上占优，在各数据集上准确率、AUC、灵敏度、特异性和F1分数平均提升10.38%、6.94%、2.06%、11.58%和8.51%。

Conclusion: 提出的基于补丁的方法性能优越，还提供Python包方便使用。

Abstract: The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.

</details>


### [382] [Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking](https://arxiv.org/abs/2512.12218)
*Rheeya Uppaal,Phu Mon Htut,Min Bai,Nikolaos Pappas,Zheng Qi*

Main category: cs.CV

TL;DR: 本文引入推理链视觉忠实性评估维度，提出无训练和无参考框架及轻量级自我反思程序，减少不忠实感知率并提高多模态推理可靠性。


<details>
  <summary>Details</summary>
Motivation: 标准评估仅衡量最终答案准确性，无法区分推理增强视觉语言模型新的失败模式，需新评估维度。

Method: 引入视觉忠实性评估维度，提出无训练和无参考框架分解推理链，用现成模型进行步骤级忠实性判断，通过人工元评估验证；采用轻量级自我反思程序检测和局部再生不忠实感知步骤。

Result: 在多个推理训练的视觉语言模型和重感知基准上，减少了不忠实感知率，同时保留了最终答案准确性。

Conclusion: 该方法提高了多模态推理的可靠性。

Abstract: Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.

</details>


### [383] [MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation](https://arxiv.org/abs/2512.12929)
*Huu-An Vu,Van-Khanh Mai,Trong-Tam Nguyen,Quang-Duc Dam,Tien-Huy Nguyen,Thanh-Huong Le*

Main category: cs.CV

TL;DR: 提出MADTempo视频检索框架，统一时间搜索和视觉定位，提升检索能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨事件建模时间依赖和处理罕见视觉概念查询上不足，快速扩张的视频内容需检索系统理解复杂事件时间结构。

Method: 提出MADTempo框架，时间搜索机制聚合视频片段相似度分数，谷歌图像搜索备用模块用外部图像扩展查询表示。

Result: 提升现代视频检索系统的时间推理和泛化能力。

Conclusion: 为大规模视频集语义感知和自适应检索奠定基础。

Abstract: The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.

</details>


### [384] [GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search](https://arxiv.org/abs/2512.12296)
*Hyunju Lee,Youngmin Oh,Jeimin Jeon,Donghyeon Baek,Bumsub Ham*

Main category: cs.CV

TL;DR: 提出GrowTAS和GrowTAS+框架解决现有TAS方法子网干扰问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有TAS方法中所有子网共享权重，导致小子网受干扰严重，性能下降。

Method: 提出GrowTAS渐进式训练框架，从训练小子网开始逐步引入大子网；还引入GrowTAS+仅微调部分权重。

Result: 在ImageNet和多个迁移学习基准测试上进行了广泛实验。

Conclusion: 所提方法比当前TAS方法更有效。

Abstract: Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods

</details>


### [385] [Unified Control for Inference-Time Guidance of Denoising Diffusion Models](https://arxiv.org/abs/2512.12339)
*Maurya Goyal,Anuj Singh,Hadi Jamali-Rad*

Main category: cs.CV

TL;DR: 本文提出通用算法UniCoDe，结合采样和梯度引导方法优势，解决复杂奖励采样低效问题，实验显示其在多任务中与SOTA相当。


<details>
  <summary>Details</summary>
Motivation: 让扩散模型输出与下游目标对齐以提升特定任务性能，现有推理时免训练方法有采样和梯度引导两类，需改进。

Method: 提出UniCoDe算法，在采样过程中整合局部梯度信号，结合两种范式。

Result: UniCoDe在一系列任务中与最先进的基线模型具有竞争力。

Conclusion: UniCoDe算法有效，能提高采样效率，在奖励对齐和与扩散无条件先验的差异之间实现更好平衡。

Abstract: Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe

</details>


### [386] [Content Adaptive based Motion Alignment Framework for Learned Video Compression](https://arxiv.org/abs/2512.12936)
*Tiange Zhang,Xiandong Meng,Siwei Ma*

Main category: cs.CV

TL;DR: 本文提出基于内容自适应的运动对齐框架CAMA，通过多策略改进视频压缩性能，实验显示其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有端到端视频压缩框架缺乏内容特定适配，导致压缩性能不佳。

Method: 引入两阶段流引导的可变形翘曲机制；提出多参考质量感知策略并用于分层训练；集成免训练模块。

Result: 在标准测试数据集上，CAMA框架比基线模型DCVC - TCM节省24.95%的BD - rate（PSNR），且优于DCVC - DC和传统编解码器HM - 16.25。

Conclusion: 所提基于内容自适应的运动对齐框架能有效提升视频压缩性能。

Abstract: Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.

</details>


### [387] [ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics](https://arxiv.org/abs/2512.12424)
*Tue-Thu Van-Dinh,Hoang-Duy Tran,Truong-Binh Duong,Mai-Hanh Pham,Binh-Nam Le-Nguyen,Quoc-Thai Nguyen*

Main category: cs.CV

TL;DR: 提出越南语信息图视觉问答首个基准ViInfographicVQA，评估多个模型并揭示性能差异及模型局限。


<details>
  <summary>Details</summary>
Motivation: 信息图视觉问答需更强的OCR、布局理解和推理能力，缺乏越南语相关基准。

Method: 构建包含6747张真实信息图和20409对问答对的基准，设置单图像和多图像两个评估任务，对多个视觉语言模型进行评估。

Result: 不同模型性能有显著差异，多图像问题上错误最明显。

Conclusion: 提供了越南语信息图视觉问答基准结果，揭示当前多模态模型在低资源环境下的局限，鼓励探索布局感知和跨图像推理方法。

Abstract: Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.

</details>


### [388] [Calibrating Uncertainty for Zero-Shot Adversarial CLIP](https://arxiv.org/abs/2512.12997)
*Wenjing lu,Zerui Tao,Dongping Zhang,Yuning Qiu,Yang Yang,Qibin Zhao*

Main category: cs.CV

TL;DR: CLIP易受对抗攻击，以往对抗微调忽略不确定性校准，本文提出考虑预测准确性和不确定性对齐的新目标，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: CLIP易受对抗攻击，以往工作忽略不确定性校准且可能降低零样本泛化能力，存在可靠性差距。

Method: 提出新的对抗微调目标，将CLIP输出重新参数化为狄利克雷分布的浓度参数，提出统一表示来捕获相对语义结构和预测置信度。

Result: 在多个零样本分类基准上的实验表明，该方法有效恢复校准的不确定性，实现有竞争力的对抗鲁棒性，同时保持干净样本的准确性。

Conclusion: 提出的新对抗微调目标能有效解决CLIP的可靠性问题，兼顾准确性和不确定性校准。

Abstract: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.

</details>


### [389] [GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training](https://arxiv.org/abs/2512.13043)
*Tong Wei,Yijun Yang,Changhao Zhang,Junliang Xing,Yuanchun Shi,Zongqing Lu,Deheng Ye*

Main category: cs.CV

TL;DR: 提出GTR - Turbo改进多模态智能体的多轮强化学习，无需昂贵教师模型，提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的多模态智能体多轮强化学习存在稀疏奖励和长期信用分配问题，且现有方法依赖昂贵教师模型，实用性和可复现性受限。

Method: 合并强化学习训练过程中检查点的权重，用合并后的模型作为“免费”教师，通过有监督微调或软logit蒸馏来指导后续强化学习。

Result: 在各种视觉智能体任务中，GTR - Turbo使基线模型准确率提高10 - 30%，相比GTR减少50%训练时间和60%计算成本。

Conclusion: GTR - Turbo设计消除了对特权视觉语言模型的依赖，缓解了之前工作中的“熵崩塌”问题，保持训练稳定。

Abstract: Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.

</details>


### [390] [UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era](https://arxiv.org/abs/2512.13089)
*Ziqiang Zhu,Bowei Yang*

Main category: cs.CV

TL;DR: 提出无监督、开放词汇变化检测方法UniVCD，在多个基准测试中表现出色，证明该方法是开放词汇变化检测的实用有效范式。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法依赖监督学习，性能依赖数据集、标注成本高、泛化能力差，借助视觉基础模型发展解决这些限制。

Method: 基于冻结的SAM2和CLIP构建无监督、开放词汇变化检测方法UniVCD，引入轻量级特征对齐模块和简化后处理流程。

Result: 在多个公共基准测试中，UniVCD实现稳定的强性能，在F1和IoU等关键指标上匹配或超越现有开放词汇变化检测方法。

Conclusion: 使用冻结视觉基础模型和轻量级多模态对齐的无监督变化检测是开放词汇变化检测的实用有效范式。

Abstract: Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.

</details>


### [391] [Generative Spatiotemporal Data Augmentation](https://arxiv.org/abs/2512.12508)
*Jinfan Zhou,Lixin Luo,Sungmin Eum,Heesung Kwon,Jeong Joon Park*

Main category: cs.CV

TL;DR: 本文探索使用视频基础模型进行时空数据增强，在低数据场景下提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 在低数据场景下，传统方法难以提供足够的多样化数据，为了增强数据的多样性和提升模型性能。

Method: 利用现成的视频扩散模型，从给定图像数据集中生成逼真的3D空间和时间变化，将合成视频片段作为补充训练数据。

Result: 在COCO子集和无人机捕获数据集实验表明，能拓宽数据分布，在数据稀缺情况下提升模型性能。

Conclusion: 合理应用时空增强可提升数据分布，是改善数据稀缺情况下模型性能的有效手段，并给出了实际应用指导。

Abstract: We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.

</details>


### [392] [Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2512.13101)
*Wenjing Lu,Yi Hong,Yang Yang*

Main category: cs.CV

TL;DR: 提出UnCoL框架解决视觉基础模型在医学图像分割中泛化到特定临床任务的问题，实验表明其优于现有方法且减少标注需求。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型在有限标注或罕见病理变异的特定临床任务中泛化能力不足，原因是通用先验与特定任务要求不匹配。

Method: 提出Uncertainty - informed Collaborative Learning (UnCoL) 双教师框架，从冻结的基础模型提取视觉和语义表示传递通用知识，同时维护一个逐步适应的教师捕获细粒度和特定任务表示，通过预测不确定性自适应调节伪标签学习。

Result: 在2D和3D分割基准测试中，UnCoL始终优于最先进的半监督方法和基础模型基线。

Conclusion: UnCoL能平衡泛化和专业化，在显著减少标注需求的情况下达到接近全监督的性能。

Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.

</details>


### [393] [Animus3D: Text-driven 3D Animation via Motion Score Distillation](https://arxiv.org/abs/2512.12534)
*Qi Sun,Can Wang,Jiaxiang Shang,Wensen Feng,Jing Liao*

Main category: cs.CV

TL;DR: 提出文本驱动3D动画框架Animus3D，引入新方法生成有实质且细节丰富的3D动画。


<details>
  <summary>Details</summary>
Motivation: 以往方法使用SDS目标从预训练文本到视频扩散模型提取动作，生成的动画动作少或有抖动，需改进。

Method: 引入Motion Score Distillation (MSD)替代SDS，采用LoRA增强视频扩散模型和基于反演的噪声估计技术，加入时空正则化项，提出运动细化模块。

Result: Animus3D能根据不同文本提示对静态3D资产进行动画处理，比现有基线生成的动作更丰富、详细，且保持高视觉完整性。

Conclusion: Animus3D是有效的文本驱动3D动画框架，代码将公开。

Abstract: We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.

</details>


### [394] [Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2512.13107)
*Zhijian He,Feifei Liu,Yuwei Li,Zhanpeng Liu,Jintao Cheng,Xieyuanli Chen,Xiaoyu Tang*

Main category: cs.CV

TL;DR: 提出DiffFusion框架，通过扩散恢复和自适应跨模态融合增强多模态3D目标检测在恶劣天气下的鲁棒性，实验效果好且将开源。


<details>
  <summary>Details</summary>
Motivation: 多模态3D目标检测在恶劣天气下因数据失真和模态不对齐效果受限，需提升鲁棒性。

Method: 提出DiffFusion框架，包含Diffusion - IR恢复图像、Point Cloud Restoration (PCR)补偿LiDAR数据，以及Bidirectional Adaptive Fusion and Alignment Module (BAFAM)处理模态不对齐。

Result: 在三个公开数据集上达到了恶劣天气下的最优鲁棒性，同时保持了良好的干净数据性能，在真实世界DENSE数据集上的零样本结果验证了泛化性。

Conclusion: DiffFusion能有效提升多模态3D目标检测在恶劣天气下的鲁棒性，且泛化性良好，将开源代码。

Abstract: Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.

</details>


### [395] [Supervised Contrastive Frame Aggregation for Video Representation Learning](https://arxiv.org/abs/2512.12549)
*Shaif Chowdhury,Mushfika Rahman,Greg Hamerly*

Main category: cs.CV

TL;DR: 提出利用时间全局上下文的视频表征学习监督对比学习框架，实验表明其在分类准确率和计算资源上表现更佳。


<details>
  <summary>Details</summary>
Motivation: 解决现有视频表征学习方法计算开销大等问题，提高视频分类等任务的性能。

Method: 引入视频到图像聚合策略，设计对比学习目标，利用不同时间帧采样创建正样本。

Result: 在Penn Action和HMDB51数据集上分类准确率优于现有方法，所需计算资源更少。

Conclusion: 提出的方法在有监督和自监督设置下都能学习有效视频表征，支持视频分类和字幕等任务。

Abstract: We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.

</details>


### [396] [DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass](https://arxiv.org/abs/2512.13122)
*Vivek Alumootil,Tuan-Anh Vu,M. Khalid Jawed*

Main category: cs.CV

TL;DR: 提出DePT3R框架，可在单前向传播中同时进行动态场景的密集点跟踪和3D重建，无需相机位姿，在多个基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有动态场景密集3D点跟踪方法存在灵活性和适用性受限问题，且大规模无位姿图像集3D重建的进展带来统一方法的机会。

Method: 通过强大的骨干网络提取深度时空特征，用密集预测头回归逐像素映射，实现多任务学习。

Result: 在多个具有挑战性的动态场景基准测试中验证，表现出色，内存效率比现有方法显著提高。

Conclusion: DePT3R无需相机位姿，增强了适应性和效率，为动态场景理解提供了有效方案。

Abstract: Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R

</details>


### [397] [Intrinsic Image Fusion for Multi-View 3D Material Reconstruction](https://arxiv.org/abs/2512.13157)
*Peter Kocsis,Lukas Höllein,Matthias Nießner*

Main category: cs.CV

TL;DR: 提出Intrinsic Image Fusion方法，从多视图图像重建高质量基于物理的材质，结果超现有方法。


<details>
  <summary>Details</summary>
Motivation: 材质重建约束不足且通常依赖分析合成法，需昂贵且有噪声的路径追踪，要更好地约束优化。

Method: 将单视图先验融入重建过程，利用扩散材质估计器生成视图候选分解，拟合显式低维参数函数，提出鲁棒优化框架融合预测，用逆路径追踪优化低维参数。

Result: 在合成和真实场景的材质分解上优于现有方法，重建结果清晰，适用于高质量重新光照。

Conclusion: 该方法能有效从多视图图像重建高质量基于物理的材质。

Abstract: We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.

</details>


### [398] [A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis](https://arxiv.org/abs/2512.13164)
*Xianchao Guan,Zhiyuan Fan,Yifeng Wang,Fuqiang Chen,Yanjiang Zhou,Zengyang Che,Hongxue Meng,Xin Li,Yaowei Wang,Hongpeng Wang,Min Zhang,Heng Tao Shen,Zheng Zhang,Yongbing Zhang*

Main category: cs.CV

TL;DR: 本文介绍用于病理特定文本到图像合成的生成基础模型CRAFTS，它解决数据稀缺和隐私问题，为临床诊断提供支持。


<details>
  <summary>Details</summary>
Motivation: 临床级病理人工智能发展受限于缺乏多样、高质量注释数据集，现有生成模型存在语义不稳定和形态幻觉问题，影响诊断可靠性。

Method: 引入CRAFTS模型，采用双阶段训练策略，利用约280万图像 - 文本对，融入新的对齐机制抑制语义漂移。

Result: 模型能生成30种癌症类型的多样病理图像，经客观指标和病理学家评估验证质量；增强多种临床任务表现；与ControlNet结合可精确控制组织结构。

Conclusion: CRAFTS克服数据稀缺和隐私问题，为罕见和复杂癌症表型创建强大诊断工具提供无限多样注释组织学数据。

Abstract: The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.

</details>


### [399] [Efficient Vision-Language Reasoning via Adaptive Token Pruning](https://arxiv.org/abs/2512.12701)
*Xue Li,Xiaonan Song,Henry Hu*

Main category: cs.CV

TL;DR: 提出自适应令牌剪枝（ATP）机制，可减少推理计算量和延迟，提升效率与稳定性，且不影响准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉 - 语言模型（VLMs）计算需求高，架构处理令牌效率低，阻碍实际部署。

Method: 引入ATP，在视觉 - 语言接口根据上下文相关性进行动态推理，根据混合重要性得分保留最重要的K个令牌给大语言模型，且与主流骨干模型兼容。

Result: 在多个数据集上减少约40%的推理FLOPs，端到端延迟提升约1.5倍，准确率损失小于1%，还提升了稳定性和可解释性。

Conclusion: 资源受限推理和模型可靠性并非相互竞争的目标，还探讨了ATP在边缘计算中的作用。

Abstract: Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.

</details>


### [400] [LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models](https://arxiv.org/abs/2512.13290)
*Shu Yu,Chaochao Lu*

Main category: cs.CV

TL;DR: 本文针对扩散模型在物理对齐和OOD指令遵循方面的问题，引入Causal Scene Graph和PAP数据集分析，提出LINA框架提升模型在因果生成任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成中存在物理对齐和OOD指令遵循的问题，根源在于模型未能学习因果方向和分离因果因素。

Method: 引入CSG和PAP数据集进行诊断干预，提出LINA框架，在提示和视觉潜在空间中进行有针对性的引导，采用重新分配的、因果感知的去噪时间表。

Result: 在具有挑战性的因果生成任务和Winoground数据集上达到了最先进的性能。

Conclusion: LINA框架能有效解决扩散模型在物理对齐和OOD指令遵循方面的问题。

Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.

</details>


### [401] [Face Identity Unlearning for Retrieval via Embedding Dispersion](https://arxiv.org/abs/2512.13317)
*Mikhail Zakharov*

Main category: cs.CV

TL;DR: 研究人脸识别系统中面部身份遗忘问题，评估现有方法并提出新的分散式遗忘方法，实验证明该方法效果好。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统存在隐私问题，现有机器遗忘技术在人脸检索中的应用待探索。

Method: 评估现有近似类别遗忘方法，提出基于分散的遗忘方法。

Result: 在标准基准上实验表明，所提方法在实现遗忘效果的同时能保留检索实用性。

Conclusion: 所提的分散式遗忘方法能有效解决人脸检索系统中面部身份遗忘问题，在遗忘效果和保留检索性能上表现出色。

Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.

</details>


### [402] [End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery](https://arxiv.org/abs/2512.13402)
*Lorenzo Pettinari,Sidaty El Hadramy,Michael Wehrli,Philippe C. Cattin,Daniel Studer,Carol C. Hasler,Maria Licci*

Main category: cs.CV

TL;DR: 提出End2Reg端到端深度学习框架，用于脊柱手术术中导航，可提高RGB - D配准精度，减少对弱标签依赖。


<details>
  <summary>Details</summary>
Motivation: 现有脊柱手术术中导航系统存在侵入性、辐射大、扰乱工作流程问题，且无标记的RGB - D配准方法依赖弱分割标签传播误差。

Method: 提出End2Reg端到端深度学习框架，联合优化分割和配准，仅由配准目标指导学习用于配准的分割掩码，无直接分割监督。

Result: 该框架在体内外基准测试中达先进水平，中位目标配准误差降低32%至1.83mm，均方根误差均值降低45%至3.95mm，消融研究证实端到端优化显著提高配准精度。

Conclusion: 所提出的端到端RGB - D配准流程消除对弱标签和手动步骤的依赖，向全自动无标记术中导航迈进。

Abstract: Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.

</details>


### [403] [Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution](https://arxiv.org/abs/2512.12898)
*Abhinav Kumar,Tristan Aumentado-Armstrong,Lazar Valkov,Gopal Sharma,Alex Levinshtein,Radek Grzeszczuk,Suren Kumar*

Main category: cs.CV

TL;DR: 本文引入Queried - Convolutions改善高频率信号学习，在多个高频学习任务中提升性能，在新视图合成中表现出色。


<details>
  <summary>Details</summary>
Motivation: 神经网络学习高频信号存在挑战，现有技术在处理高频信息时仍有提升空间。

Method: 引入Queried - Convolutions，利用卷积的邻域特性，通过低频率信号与查询（如坐标）进行卷积来学习复杂高频信号。

Result: Qonvolutions在多个高频学习任务中提升性能，结合高斯散点法用于新视图合成时在真实复杂场景中达到了前沿水平，图像质量超强大辐射场模型。

Conclusion: Qonvolutions是一种简单有效的方法，能显著提升高频信号学习效果，在计算机视觉和图形学中的高频学习任务中有良好应用前景。

Abstract: Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier encodings have made great strides in improving performance, there remains scope for improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality.

</details>


### [404] [DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides](https://arxiv.org/abs/2512.13600)
*Haoyue Zhang,Meera Chappidi,Erolcan Sayar,Helen Richards,Zhijun Chen,Lucas Liu,Roxanne Wadia,Peter A Humphrey,Fady Ghali,Alberto Contreras-Sanz,Peter Black,Jonathan Wright,Stephanie Harmon,Michael Haffner*

Main category: cs.CV

TL;DR: 本文提出域自适应自监督适配器DA - SSL，用于调整预训练病理基础模型特征到TURBT领域，在预测TURBT治疗反应上取得良好效果，证明轻量级域自适应可增强基于PFM的MIL管道。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型（PFMs）因域偏移在某些癌症或样本类型上有局限，如TURBT样本含特殊组织碎片和电灼伪影且未广泛用于公开PFMs预训练，且TURBT中组织形态特征未充分利用，识别新辅助化疗受益患者有挑战。

Method: 提出简单有效的域自适应自监督适配器（DA - SSL），在不微调基础模型的情况下将预训练PFM特征调整到TURBT领域。

Result: 在多中心研究中，DA - SSL在五折交叉验证中AUC为0.77±0.04，外部测试准确率0.84、敏感度0.71、特异度0.91。

Conclusion: 轻量级域自适应与自监督结合能有效增强基于PFM的MIL管道，用于临床挑战性组织病理学任务。

Abstract: Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.

</details>


### [405] [Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing](https://arxiv.org/abs/2512.13018)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

TL;DR: 本文首次全面评估空间泛化技术，聚焦室内FMCW MIMO雷达人数统计，实验表明基于Sigmoid的幅度加权、数据增强和迁移学习有良好效果，为开发雷达传感系统指明方向。


<details>
  <summary>Details</summary>
Motivation: 对基于深度学习的射频传感实际部署中至关重要的空间泛化技术进行首次全面评估。

Method: 系统研究多种方法，包括基于幅度的统计预处理、频域滤波、基于自编码器的背景抑制、数据增强策略和迁移学习。

Result: 基于Sigmoid的幅度加权跨环境性能优越，RMSE和MAE分别降低50.1%和55.2%；数据增强有一定提升，MAE最多改善8.8%；迁移学习对大空间偏移不可或缺，540个目标域样本时RMSE和MAE分别降低82.1%和91.3%。

Conclusion: 将深度学习模型与基于幅度的预处理和高效迁移学习相结合，可为开发能在空间变化下保持稳健精度的雷达传感系统提供实用方向。

Abstract: This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.

</details>


### [406] [Motus: A Unified Latent Action World Model](https://arxiv.org/abs/2512.13030)
*Hongzhe Bi,Hengkai Tan,Shenghao Xie,Zeyuan Wang,Shuhe Huang,Haitian Liu,Ruowen Zhao,Yao Feng,Chendong Xiang,Yinze Rong,Hongyan Zhao,Hanyu Liu,Zhizhong Su,Lei Ma,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: 提出Motus统一潜在动作世界模型，结合预训练模型和运动信息，实验显示其在模拟和现实场景中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前方法基于孤立模型，存在碎片化问题，无法统一多模态生成能力和从大规模异构数据学习。

Method: 提出Motus模型，引入Mixture-of-Transformer架构集成三个专家，采用UniDiffuser风格调度器，利用光流学习潜在动作，采用三相训练管道和六层数据金字塔。

Result: Motus在模拟场景中比X - VLA提高15%、比Pi0.5提高45%，在现实场景中提高11 - 48%。

Conclusion: 统一所有功能和先验的建模对下游机器人任务有显著益处。

Abstract: While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.

</details>


### [407] [DiRe: Diversity-promoting Regularization for Dataset Condensation](https://arxiv.org/abs/2512.13083)
*Saumyaranjan Mohanty,Aravind Reddy,Konda Reddy Mopuri*

Main category: cs.CV

TL;DR: 提出多样性正则化器DiRe用于数据集凝聚，可减少冗余、提高多样性，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 现有数据集凝聚方法合成的数据集存在显著冗余，需减少冗余、提高多样性。

Method: 提出由余弦相似度和欧氏距离组成的多样性正则化器DiRe，并可应用于各种先进的凝聚方法。

Result: 在多个基准数据集上，添加正则化器能在泛化和多样性指标上改进现有先进凝聚方法。

Conclusion: 所提出的DiRe正则化器能有效提升数据集凝聚方法的性能。

Abstract: In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.

</details>


### [408] [Feedforward 3D Editing via Text-Steerable Image-to-3D](https://arxiv.org/abs/2512.13678)
*Ziqi Ma,Hongqiao Chen,Yisong Yue,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 提出Steer3D方法为图像到3D模型添加文本可控性，能以语言编辑3D资产，比竞品更忠实遵循指令、速度更快。


<details>
  <summary>Details</summary>
Motivation: 为使AI生成的3D资产能用于实际应用，需具备轻松编辑的能力。

Method: 受ControlNet启发，将其应用于图像到3D生成；构建可扩展的数据引擎自动生成数据；采用基于流匹配训练和直接偏好优化（DPO）的两阶段训练方法。

Result: Steer3D比竞品更忠实遵循语言指令，与原始3D资产一致性更好，速度快2.4至28.5倍。

Conclusion: 可以通过10万数据为预训练的图像到3D生成模型添加文本模态以控制生成。

Abstract: Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/

</details>


### [409] [ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning](https://arxiv.org/abs/2512.13095)
*Feng Zhang,Zezhong Tan,Xinhong Ma,Ziqiang Dong,Xi Leng,Jianfei Zhao,Xin Sun,Yang Yang*

Main category: cs.CV

TL;DR: 本文提出ADHint方法结合监督微调与强化学习，解决现有基于提示的强化学习方法问题，实验显示其推理能力和泛化性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的强化学习方法在调度提示比率和估计相对优势时忽略难度，导致学习不稳定和过度模仿离策略提示，故需改进。

Method: 提出考虑样本难度的自适应提示、基于一致性的梯度调制和选择性掩码、考虑难度后验的优势估计等方法。

Result: 在多种模态、模型规模和领域的实验中，ADHint在pass@1和avg@8上均超过现有方法。

Conclusion: ADHint方法能实现探索与模仿的更好权衡，具备更优推理能力和分布外泛化能力。

Abstract: To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.

</details>


### [410] [DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders](https://arxiv.org/abs/2512.13690)
*Susung Hong,Chongjian Ge,Zhifei Zhang,Jui-Hsien Wang*

Main category: cs.CV

TL;DR: 提出DiffusionBrowser框架，可在去噪过程中交互式生成预览，能多模态、高速生成预览，还可引导生成并探查模型。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型不精确、速度慢且生成过程不透明，用户长时间无法了解情况。

Method: 提出DiffusionBrowser模型无关的轻量级解码器框架，通过随机注入和模态引导在中间噪声步骤交互式引导生成，用学习的解码器探查模型。

Result: 能以超4倍实时速度生成包含RGB和场景内在信息的多模态预览，与最终视频外观和运动一致。

Conclusion: 该框架可实现交互式预览生成、引导生成和探查黑盒去噪过程，解锁新控制能力。

Abstract: Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.

</details>


### [411] [Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models](https://arxiv.org/abs/2512.13144)
*Chun Kit Wong,Paraskevas Pegios,Nina Weng,Emilie Pi Fogtmann Sejer,Martin Grønnebæk Tolsgaard,Anders Nymark Christensen,Aasa Feragen*

Main category: cs.CV

TL;DR: 提出权重空间相关性分析方法检测医学影像深度学习模型特征利用情况，验证sPTB预测模型对特征的选择性利用。


<details>
  <summary>Details</summary>
Motivation: 探究医学影像深度学习模型是否利用编码在图像嵌入中的混淆元数据进行最终预测。

Method: 引入权重空间相关性分析方法，通过测量主临床任务和辅助元数据任务分类头之间的对齐程度来量化特征利用。

Result: 成功检测人工诱导的捷径学习；sPTB分类器权重向量与临床相关因素高度相关，与临床无关采集因素解耦。

Conclusion: 该方法可验证模型可信度，表明无诱导偏差时临床模型会选择性利用与真实临床信号相关的特征。

Abstract: Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.

</details>


### [412] [rNCA: Self-Repairing Segmentation Masks](https://arxiv.org/abs/2512.13397)
*Malte Silbernagel,Albert Alonso,Jens Petersen,Bulat Ibragimov,Marleen de Bruijne,Madeleine K. Wyburd*

Main category: cs.CV

TL;DR: 论文表明神经元细胞自动机（NCA）能作为有效的细化机制修复分割掩码，在不同任务中取得较好效果。


<details>
  <summary>Details</summary>
Motivation: 通用分割模型难以准确预测拓扑正确的掩码，修复掩码人工规则或特定架构效果不佳。

Method: 将NCA用作细化机制，在不完美掩码和真实标签上训练，依赖局部信息学习目标形状结构特征。

Result: rNCA能修复不同基础分割模型和任务产生的拓扑错误，在视网膜血管分割和心肌分割任务中取得显著效果。

Conclusion: NCA是一种有效且广泛适用的细化器。

Abstract: Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.

</details>


### [413] [MineTheGap: Automatic Mining of Biases in Text-to-Image Models](https://arxiv.org/abs/2512.13427)
*Noa Cohen,Nurit Spingarn-Eliezer,Inbar Huberman-Spiegelglas,Tomer Michaeli*

Main category: cs.CV

TL;DR: 提出MineTheGap方法自动挖掘使文本到图像模型产生偏差输出的提示词，利用遗传算法和新的偏差分数进行优化。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在处理模糊文本提示时存在偏差，会产生社会影响和影响用户体验，需要挖掘导致偏差输出的提示词。

Method: 引入MineTheGap方法，利用遗传算法迭代优化提示词池，通过新的偏差分数驱动优化过程，该分数通过比较生成图像分布和大语言模型生成文本分布得到。

Result: 在已知偏差的数据集上验证了方法，项目网页提供代码和示例。

Conclusion: MineTheGap方法可有效挖掘使文本到图像模型产生偏差输出的提示词。

Abstract: Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.

</details>


### [414] [Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains](https://arxiv.org/abs/2512.13534)
*Marianne Rakic,Siyu Gai,Etienne Chollet,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: 提出Pancakes框架，可对新领域图像自动生成多协议多标签分割图，实验显示其性能优于现有基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有自动分割模型要么仅支持单一协议，要么需人工大量手动指定分割方式，无法满足多协议分割需求。

Method: 引入Pancakes框架，采用新问题公式化方法。

Result: 在七个保留数据集实验中，模型能显著优于现有基础模型生成语义连贯的多协议全图像分割。

Conclusion: Pancakes框架可解决现有基础模型在多协议图像分割的不足，实现新领域图像的多标签分割。

Abstract: A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.

</details>


### [415] [Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models](https://arxiv.org/abs/2512.13609)
*Shweta Mahajan,Shreya Kadambi,Hoang Le,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: 提出Do - Undo任务和基准，以解决视觉语言模型在理解和生成由现实世界动作驱动的物理上合理的场景转换方面的差距，实验显示当前模型在物理可逆性上存在困难。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在理解和生成由现实世界动作驱动的物理上合理的场景转换方面的关键差距，反映视觉世界中的真实因果关系。

Method: 从真实世界视频中整理大规模可逆动作数据集，设计强制一致性的训练策略以实现稳健的动作定位。

Result: 当前模型在物理可逆性上表现不佳。

Conclusion: Do - Undo为评估和推进多模态系统的物理推理能力建立了直观的测试平台。

Abstract: We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [416] [Not All Transparency Is Equal: Source Presentation Effects on Attention, Interaction, and Persuasion in Conversational Search](https://arxiv.org/abs/2512.12207)
*Jiangen He,Jiqun Liu*

Main category: cs.HC

TL;DR: 研究不同源呈现格式对用户参与度的影响，发现高可见度界面有不同效果，源呈现不一定能提高参与度。


<details>
  <summary>Details</summary>
Motivation: 探究引用或源呈现格式如何影响用户参与度。

Method: 进行有394名参与者的众包用户实验，比较四种源呈现设计。

Result: 高可见度界面使源的悬停增多，但点击仍少；设计对用户体验和感知影响有限，对知识、兴趣和同意度变化影响显著；侧边栏设计独特影响同意度变化。

Conclusion: 源呈现本身可能无法提高参与度，源不足时甚至会降低。

Abstract: Conversational search systems increasingly provide source citations, yet how citation or source presentation formats influence user engagement remains unclear. We conducted a crowdsourcing user experiment with 394 participants comparing four source presentation designs that varied citation visibility and accessibility: collapsible lists, hover cards, footer lists, and aligned sidebars.High-visibility interfaces generated substantially more hovering on sources, though clicking remained infrequent across all conditions. While interface design showed limited effects on user experience and perception measures, it significantly influenced knowledge, interest, and agreement changes. High-visibility interfaces initially reduced knowledge gain and interest, but these positive effects emerged with increasing source usage. The sidebar condition uniquely increased agreement change. Our findings demonstrate that source presentation alone may not enhance engagement and can even reduce it when insufficient sources are provided.

</details>


### [417] [Designing The Internet of Agents: A Framework for Trustworthy, Transparent, and Collaborative Human-Agent Interaction (HAX)](https://arxiv.org/abs/2512.11979)
*Marc Scibelli,Krystelle Gonzalez Papaux,Julia Valenti,Srishti Kush*

Main category: cs.HC

TL;DR: 提出HAX框架，用于人与生成式和自主智能体协作交互，整合多种元素并基于TIP理论。


<details>
  <summary>Details</summary>
Motivation: 生成式和自主智能体兴起，需重新思考人与概率性、部分自主系统的协作方式。

Method: 提出包含三个阶段的HAX框架，整合行为启发式、SDK和行为代理概念，还有混合主动设计模式目录，基于TIP理论。

Result: 建立了可信赖、透明和协作的智能体交互设计基础。

Conclusion: HAX是首个连接信任理论、界面设计和基础设施的端到端框架，将多智能体系统视为同事。

Abstract: The rise of generative and autonomous agents marks a fundamental shift in computing, demanding a rethinking of how humans collaborate with probabilistic, partially autonomous systems. We present the Human-AI-Experience (HAX) framework, a comprehensive, three-phase approach that establishes design foundations for trustworthy, transparent, and collaborative agentic interaction. HAX integrates behavioral heuristics, a schema-driven SDK enforcing structured and safe outputs, and a behavioral proxy concept that orchestrates agent activity to reduce cognitive load. A validated catalog of mixed-initiative design patterns further enables intent preview, iterative alignment, trust repair, and multi-agent narrative coherence. Grounded in Time, Interaction, and Performance (TIP) theory, HAX reframes multi-agent systems as colleagues, offering the first end-to-end framework that bridges trust theory, interface design, and infrastructure for the emerging Internet of Agents.

</details>


### [418] [AI as a Teaching Partner: Early Lessons from Classroom Codesign with Secondary Teachers](https://arxiv.org/abs/2512.12045)
*Alex Liu,Lief Esbenshade,Shawon Sarkar,Zewei,Tian,Min Sun,Zachary Zhang,Thomas Han,Yulia Lapicus,Kevin He*

Main category: cs.HC

TL;DR: 本文介绍Colleague AI Classroom试点项目，教师和学生参与其中，教师反馈指导AI迭代，研究团队收集数据了解AI价值与改进方向。


<details>
  <summary>Details</summary>
Motivation: 将生成式AI技术引入真实课堂，发挥AI在教学中的作用，同时保留人类判断和教师权威。

Method: 21位在职教师将Colleague AI Classroom的四个AI功能融入教学，教师作为共同设计伙伴参与活动并提供反馈，研究团队通过多种方式收集数据。

Result: 未明确提及具体结果。

Conclusion: 未明确提及具体结论，主要展示了试点项目的开展过程。

Abstract: This report presents a comprehensive account of the Colleague AI Classroom pilot, a collaborative design (co-design) study that brought generative AI technology directly into real classrooms. In this study, AI functioned as a third agent, an active participant that mediated feedback, supported inquiry, and extended teachers' instructional reach while preserving human judgment and teacher authority.
  Over seven weeks in spring 2025, 21 in-service teachers from four Washington State public school districts and one independent school integrated four AI-powered features of the Colleague AI Classroom into their instruction: Teaching Aide, Assessment and AI Grading, AI Tutor, and Student Growth Insights. More than 600 students in grades 6-12 used the platform in class at the direction of their teachers, who designed and facilitated the AI activities.
  During the Classroom pilot, teachers were co-design partners: they planned activities, implemented them with students, and provided weekly reflections on AI's role in classroom settings. The teachers' feedback guided iterative improvements for Colleague AI. The research team captured rich data through surveys, planning and reflection forms, group meetings, one-on-one interviews, and platform usage logs to understand where AI adds instructional value and where it requires refinement.

</details>


### [419] [Epistemoverse: Toward an AI-Driven Knowledge Metaverse for Intellectual Heritage Preservation](https://arxiv.org/abs/2512.12201)
*Predrag K. Nikolić,Robert Prentner*

Main category: cs.HC

TL;DR: 本文挑战大语言模型是‘随机鹦鹉’的假设，通过研究互动艺术项目中AI哲学家的哲学辩论，揭示机器对话的能力，提出认识论宇宙概念。


<details>
  <summary>Details</summary>
Motivation: 挑战大语言模型只是复制训练数据片段的观点，探索其在对话语境中的能力。

Method: 借鉴克隆生物学原理和苏格拉底助产术，分析互动艺术装置中AI哲学家的哲学辩论，与AI哲学家进行迭代对话。

Result: 发现机器对话能产生推理连贯性、反思性提问和创造性综合。

Conclusion: 提出认识论宇宙概念，将虚拟和沉浸式环境定位为知识交流、数字遗产和协作创造的新空间。

Abstract: Large language models (LLMs) have often been characterized as "stochastic parrots" that merely reproduce fragments of their training data. This study challenges that assumption by demonstrating that, when placed in an appropriate dialogical context, LLMs can develop emergent conceptual structures and exhibit interaction-driven (re-)structuring of cognitive interfaces and reflective question-asking. Drawing on the biological principle of cloning and Socrates' maieutic method, we analyze authentic philosophical debates generated among AI-reincarnated philosophers within the interactive art installations of the Syntropic Counterpoints project. By engaging digital counterparts of Aristotle, Nietzsche, Machiavelli, and Sun Tzu in iterative discourse, the study reveals how machine dialogue can give rise to inferential coherence, reflective questioning, and creative synthesis. Based on these findings, we propose the concept of the Epistemoverse--a metaverse of knowledge where human and machine cognition intersect to preserve, reinterpret, and extend intellectual heritage through AI-driven interaction. This framework positions virtual and immersive environments as new spaces for epistemic exchange, digital heritage, and collaborative creativity.

</details>


### [420] [Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians versus The Public](https://arxiv.org/abs/2512.12500)
*Xuhai Xu,Haoyu Hu,Haoran Zhang,Will Ke Wang,Reina Wang,Luis R. Soenksen,Omar Badri,Sheharbano Jafry,Elise Burger,Lotanna Nwandu,Apoorva Mehta,Erik P. Duhaime,Asif Qasim,Hause Lin,Janis Pereira,Jonathan Hershon,Paulius Mui,Alejandro A. Gru,Noémie Elhadad,Lena Mamykina,Matthew Groh,Philipp Tschandl,Roxana Daneshjou,Marzyeh Ghassemi*

Main category: cs.HC

TL;DR: 研究结合公平诊断AI模型与不同XAI解释，通过实验研究XAI对诊断表现的影响，发现其效果因专业知识和时机而异，LLMs是医疗AI的“双刃剑”。


<details>
  <summary>Details</summary>
Motivation: AI算法不透明挑战人机交互，虽XAI可提供决策见解，但会导致过度依赖或偏差，需研究XAI影响。

Method: 进行两项大规模实验，结合公平诊断AI模型与不同XAI解释，以研究XAI特别是多模态大语言模型对诊断表现的影响。

Result: AI辅助提高准确性并减少诊断差异；LLM解释对不同专业群体影响不同，呈现AI建议的时机也影响结果。

Conclusion: XAI影响因专业知识和时机而异，LLMs是医疗AI的“双刃剑”，为未来人机协作系统设计提供参考。

Abstract: Artificial intelligence (AI) is increasingly permeating healthcare, from physician assistants to consumer applications. Since AI algorithm's opacity challenges human interaction, explainable AI (XAI) addresses this by providing AI decision-making insight, but evidence suggests XAI can paradoxically induce over-reliance or bias. We present results from two large-scale experiments (623 lay people; 153 primary care physicians, PCPs) combining a fairness-based diagnosis AI model and different XAI explanations to examine how XAI assistance, particularly multimodal large language models (LLMs), influences diagnostic performance. AI assistance balanced across skin tones improved accuracy and reduced diagnostic disparities. However, LLM explanations yielded divergent effects: lay users showed higher automation bias - accuracy boosted when AI was correct, reduced when AI erred - while experienced PCPs remained resilient, benefiting irrespective of AI accuracy. Presenting AI suggestions first also led to worse outcomes when the AI was incorrect for both groups. These findings highlight XAI's varying impact based on expertise and timing, underscoring LLMs as a "double-edged sword" in medical AI and informing future human-AI collaborative system design.

</details>


### [421] [Can You Keep a Secret? Exploring AI for Care Coordination in Cognitive Decline](https://arxiv.org/abs/2512.12510)
*Alicia,Lee,Mai Lee Chang,Sreehana Mandava,Destiny Deshields,Hugo Simão,Aaron Steinfeld,Jodi Forlizzi,John Zimmerman*

Main category: cs.HC

TL;DR: 采访认知衰退老年人及其非专业护理者，探讨代理人助低社会经济地位老人居家养老，发现老人控制护理方式、有减负策略及相关疑问。


<details>
  <summary>Details</summary>
Motivation: 探索代理人如何帮助低社会经济地位的老年人居家养老。

Method: 对十对认知衰退老年人及其非专业护理者进行访谈。

Result: 老人通过向特定护理者委托任务等方式控制护理，部分放弃日常任务，有“搭便车”策略。

Conclusion: 提出代理人能否协助“搭便车”协调及能否让老人居家更久且不增加护理负担的问题。

Abstract: The increasing number of older adults who experience cognitive decline places a burden on informal caregivers, whose support with tasks of daily living determines whether older adults can remain in their homes. To explore how agents might help lower-SES older adults to age-in-place, we interviewed ten pairs of older adults experiencing cognitive decline and their informal caregivers. We explored how they coordinate care, manage burdens, and sustain autonomy and privacy. Older adults exercised control by delegating tasks to specific caregivers, keeping information about all the care they received from their adult children. Many abandoned some tasks of daily living, lowering their quality of life to ease caregiver burden. One effective strategy, piggybacking, uses spontaneous overlaps in errands to get more work done with less caregiver effort. This raises the questions: (i) Can agents help with piggyback coordination? (ii) Would it keep older adults in their homes longer, while not increasing caregiver burden?

</details>


### [422] [ORIBA: Exploring LLM-Driven Role-Play Chatbot as a Creativity Support Tool for Original Character Artists](https://arxiv.org/abs/2512.12630)
*Yuqian Sun,Xingyu Li,Shunyu Yao,Noura Howell,Tristan Braud,Chang Hee Lee,Ali Asadipour*

Main category: cs.HC

TL;DR: 本文探讨生成式AI辅助视觉艺术家创作原创角色，介绍ORIBA聊天机器人，研究表明其能激发创作灵感，为AI系统设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 生成式AI带来创意支持新机遇，但引发视觉艺术家群体伦理担忧，旨在探索其在尊重艺术家创作自主性的前提下辅助创作原创角色的方法。

Method: 提出利用大语言模型的AI聊天机器人ORIBA，让艺术家与原创角色进行角色扮演，研究对14位艺术家展开。

Result: ORIBA激发了艺术家的想象力，使其能开发多维属性，与原创角色建立更强联系，促进创作过程。

Conclusion: 生成式AI可作为中立非视觉支持，强化现有创作实践，不侵犯艺术家的创作表达。

Abstract: Recent advances in Generative AI (GAI) have led to new opportunities for creativity support. However, this technology has raised ethical concerns in the visual artists community. This paper explores how GAI can assist visual artists in developing original characters (OCs) while respecting their creative agency. We present ORIBA, an AI chatbot leveraging large language models (LLMs) to enable artists to role-play with their OCs, focusing on conceptualization (e.g., backstories) while leaving exposition (visual creation) to creators. Through a study with 14 artists, we found ORIBA motivated artists' imaginative engagement, developing multidimensional attributes and stronger bonds with OCs that inspire their creative process. Our contributions include design insights for AI systems that develop from artists' perspectives, demonstrating how LLMs can support cross-modal creativity while preserving creative agency in OC art. This paper highlights the potential of GAI as a neutral, non-visual support that strengthens existing creative practice, without infringing artistic exposition.

</details>


### [423] [Love First, Know Later: Persona-Based Romantic Compatibility Through LLM Text World Engines](https://arxiv.org/abs/2512.11844)
*Haoyang Shang,Zhengyang Yan,Xuan Liu*

Main category: cs.HC

TL;DR: 提出“先互动，后了解”计算匹配范式，利用大语言模型模拟互动评估兼容性，理论证明收敛性，实证验证有效性。


<details>
  <summary>Details</summary>
Motivation: 改变传统基于静态资料对比的计算匹配方式，实现更有效的兼容性评估。

Method: 利用大语言模型作为文本世界引擎，将兼容性评估形式化为奖励建模问题，从关系心理学中获取灵感建立数学假设。

Result: 理论上证明匹配收敛到最优稳定匹配，实证上在速配数据和离婚预测上验证有效性。

Conclusion: 该范式能实现交互式、个性化匹配系统，为透明和交互式兼容性评估带来未来可能性。

Abstract: We propose Love First, Know Later: a paradigm shift in computational matching that simulates interactions first, then assesses compatibility. Instead of comparing static profiles, our framework leverages LLMs as text world engines that operate in dual capacity-as persona-driven agents following behavioral policies and as the environment modeling interaction dynamics. We formalize compatibility assessment as a reward-modeling problem: given observed matching outcomes, we learn to extract signals from simulations that predict human preferences. Our key insight is that relationships hinge on responses to critical moments-we translate this observation from relationship psychology into mathematical hypotheses, enabling effective simulation. Theoretically, we prove that as LLM policies better approximate human behavior, the induced matching converges to optimal stable matching. Empirically, we validate on speed dating data for initial chemistry and divorce prediction for long-term stability. This paradigm enables interactive, personalized matching systems where users iteratively refine their agents, unlocking future possibilities for transparent and interactive compatibility assessment.

</details>


### [424] [Designing The Drive: Enhancing User Experience through Adaptive Interfaces in Autonomous Vehicles](https://arxiv.org/abs/2512.12773)
*Reeteesha Roy*

Main category: cs.HC

TL;DR: 探讨在自动驾驶汽车界面个性化中应用HCI原则提升用户体验，介绍多种策略及强调透明度和用户控制。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶汽车发展，需定制用户界面优化体验，理解用户需求对技术接受和信任很重要。

Method: 探索通过自适应设计、多模态交互和用户反馈机制等HCI策略设计个性化界面。

Result: 个性化界面能显著提高用户参与度和满意度，透明度和用户控制有助于增强对自动驾驶系统的信任。

Conclusion: 指出HCI在自动驾驶汽车发展中的作用，满足安全和安保标准相关需求。

Abstract: With the recent development and integration of autonomous vehicles (AVs) in transportation systems of the modern world, the emphasis on customizing user interfaces to optimize the overall user experience has been growing expediently. Therefore, understanding user needs and preferences is essential to the acceptance and trust of these technologies as they continue to grow in prevalence. This paper addresses the implementation of HCI principles in the personalization of interfaces to improve safety, security, and usability for the users. This paper explores the way that personalized interfaces can be devised to increase user engagement and satisfaction through various HCI strategies such as adaptive design, multi-modal interaction, and user feedback mechanisms. Moreover, this paper puts emphasis on factors of transparency and user control in the design of an interface; hence, allowing users to design or modify their experience could foster an increase in trust in autonomous systems. In so doing, this research touches on the quite influential role HCI will play in this future scenario of autonomous vehicles while designing to ensure relevance to the diverse needs of users while maintaining high standards of safety and security. Discussing various HCI strategies such as adaptive design, multi-modal interaction, and feedback mechanisms to the user, this paper demonstrates how personalized interfaces can enhance significantly both user engagement and satisfaction. Transparency and user control also in designing an interface are further discussed, pointing out the need for a prerequisite condition of enabling the user to take control of their experience as a state of trust in autonomous systems. In summary, this paper points out the role of HCI in the development of autonomous vehicles and addresses numerous needs with respect to those enforced safety and security standards.

</details>


### [425] [Decoding Human and AI Persuasion in National College Debate: Analyzing Prepared Arguments Through Aristotle's Rhetorical Principles](https://arxiv.org/abs/2512.12817)
*Mengqian Wu,Jiayi Zhang,Raymond Z. Zhang*

Main category: cs.HC

TL;DR: 本文探讨利用AI生成辩论有效论点的潜力，对比GPT - 4与人类辩手证据卡论点质量，分析两者优劣以助力学生提升辩论能力。


<details>
  <summary>Details</summary>
Motivation: 辩论中形成有效论证的技能培训依赖人力，劳动密集且难规模化，需AI辅助支持学生辩论准备。

Method: 用GPT - 4生成证据卡并与人类辩手的对比，依据亚里士多德修辞原则从定性和定量分析论点质量。

Result: 识别出人类和GPT在辩论推理中的优势和局限，明确AI与人类推理的异同。

Conclusion: 研究成果有助于明确AI辅助学习的作用，为学生提升论证和推理技能提供策略。

Abstract: Debate has been widely adopted as a strategy to enhance critical thinking skills in English Language Arts (ELA). One important skill in debate is forming effective argumentation, which requires debaters to select supportive evidence from literature and construct compelling claims. However, the training of this skill largely depends on human coaching, which is labor-intensive and difficult to scale. To better support students in preparing for debates, this study explores the potential of leveraging artificial intelligence to generate effective arguments. Specifically, we prompted GPT-4 to create an evidence card and compared it to those produced by human debaters. The evidence cards outline the arguments students will present and how those arguments will be delivered, including components such as literature-based evidence quotations, summaries of core ideas, verbatim reading scripts, and tags (i.e., titles of the arguments). We compared the quality of the arguments in the evidence cards created by GPT and student debaters using Aristotle's rhetorical principles: ethos (credibility), pathos (emotional appeal), and logos (logical reasoning). Through a systematic qualitative and quantitative analysis, grounded in the rhetorical principles, we identify the strengths and limitations of human and GPT in debate reasoning, outlining areas where AI's focus and justifications align with or diverge from human reasoning. Our findings contribute to the evolving role of AI-assisted learning interventions, offering insights into how student debaters can develop strategies that enhance their argumentation and reasoning skills.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [426] [Homomorphism Indistinguishability, Multiplicity Automata Equivalence, and Polynomial Identity Testing](https://arxiv.org/abs/2512.13058)
*Marek Černý,Tim Seppelt*

Main category: cs.CC

TL;DR: 本文研究图同态不可区分性问题的复杂度，证明Seppelt算法有条件最优，改进有界路径宽度图类的复杂度上界，还建立同态不可区分性与多重自动机等价性的联系并确定后者复杂度。


<details>
  <summary>Details</summary>
Motivation: 图同态不可区分性问题涵盖多种图同构松弛问题，其复杂度范围广，需进一步研究其复杂度。

Method: 分析Seppelt算法，通过条件证明其最优性；对有界路径宽度的图类改进复杂度上界；建立同态不可区分性与多重自动机等价性的联系。

Result: 证明Seppelt算法条件最优；将有界路径宽度图类的复杂度上界从PTIME改进到C=L；确定多重自动机等价性问题复杂度为C=L完全。

Conclusion: 在图同态不可区分性问题复杂度研究上取得进展，明确算法最优性、改进复杂度上界并确定相关问题复杂度。

Abstract: Two graphs $G$ and $H$ are homomorphism indistinguishable over a graph class $\mathcal{F}$ if they admit the same number of homomorphisms from every graph $F \in \mathcal{F}$. Many graph isomorphism relaxations such as (quantum) isomorphism and cospectrality can be characterised as homomorphism indistinguishability over specific graph classes. Thereby, the problems $\textrm{HomInd}(\mathcal{F})$ of deciding homomorphism indistinguishability over $\mathcal{F}$ subsume diverse graph isomorphism relaxations whose complexities range from logspace to undecidable. Establishing the first general result on the complexity of $\textrm{HomInd}(\mathcal{F})$, Seppelt (MFCS 2024) showed that $\textrm{HomInd}(\mathcal{F})$ is in randomised polynomial time for every graph class $\mathcal{F}$ of bounded treewidth that can be defined in counting monadic second-order logic $\mathsf{CMSO}_2$.
  We show that this algorithm is conditionally optimal, i.e. it cannot be derandomised unless polynomial identity testing is in $\mathsf{PTIME}$. For $\mathsf{CMSO}_2$-definable graph classes $\mathcal{F}$ of bounded pathwidth, we improve the previous complexity upper bound for $\textrm{HomInd}(\mathcal{F})$ from $\mathsf{PTIME}$ to $\mathsf{C}_=\mathsf{L}$ and show that this is tight. Secondarily, we establish a connection between homomorphism indistinguishability and multiplicity automata equivalence which allows us to pinpoint the complexity of the latter problem as $\mathsf{C}_=\mathsf{L}$-complete.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [427] [Data-driven modelling of autonomous and forced dynamical systems](https://arxiv.org/abs/2512.12432)
*Robert Szalai*

Main category: math.DS

TL;DR: 本文证明不变叶层是物理系统数据驱动建模的有效工具，将其扩展到受迫和参数相关系统，能准确预测长期动态并恢复不变流形。


<details>
  <summary>Details</summary>
Motivation: 探索准确、数据高效且实用的物理系统数据驱动建模工具。

Method: 将不变叶层拟合到数据，选择合适函数表示和超参数消除过拟合与欠拟合；扩展到受迫和参数相关系统，利用全轨迹预测；考虑系统可约性；计算多个不变叶层恢复不变流形；将模型转化为标准形式解释结果。

Result: 能够准确预测长期动态，恢复不变流形。

Conclusion: 不变叶层是物理系统数据驱动建模的准确、数据高效且实用的工具。

Abstract: The paper demonstrates that invariant foliations are accurate, data-efficient and practical tools for data-driven modelling of physical systems. Invariant foliations can be fitted to data that either fill the phase space or cluster about an invariant manifold. Invariant foliations can be fitted to a single trajectory or multiple trajectories. Over and underfitting are eliminated by appropriately choosing a function representation and its hyperparameters, such as polynomial orders. The paper extends invariant foliations to forced and parameter dependent systems. It is assumed that forcing is provided by a volume preserving map, and therefore the forcing can be periodic, quasi-periodic or even chaotic. The method utilises full trajectories, hence it is able to predict long-term dynamics accurately. We take into account if a forced system is reducible to an autonomous system about a steady state, similar to how Floquet theory guarantees reducibility for periodically forced systems. In order to find an invariant manifold, multiple invariant foliations are calculated in the neighbourhood of the invariant manifold. Some of the invariant foliations can be linear, while others nonlinear but only defined in a small neighbourhood of an invariant manifold, which reduces the number of parameters to be identified. An invariant manifold is recovered as the zero level set of one or more of the foliations. To interpret the results, the identified mathematical models are transformed to a canonical form and instantaneous frequency and damping information are calculated.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [428] [Adaptive Edge-Cloud Inference for Speech-to-Action Systems Using ASR and Large Language Models (ASTA)](https://arxiv.org/abs/2512.12769)
*Mohammad Jalili Torkamani,Israt Zarin*

Main category: cs.SD

TL;DR: 本文提出自适应语音转动作解决方案ASTA，平衡边缘与云端推理，实验表明其能实现输入命令执行及推理分布平衡，证明自适应边缘 - 云端编排对语音控制物联网系统可行。


<details>
  <summary>Details</summary>
Motivation: 语音驱动的边缘设备在云解决方案和边缘解决方案间存在权衡，需平衡性能和系统资源利用率。

Method: 提出ASTA，集成设备端自动语音识别、轻量级离线语言模型推理和云端大语言模型处理，通过实时系统指标引导，利用度量感知路由机制选择推理路径，规则式命令验证和修复组件确保命令执行。

Result: ASTA成功路由所有输入命令执行，实现线上和线下推理的平衡分布，ASR准确率62.5%，仅47.5%输入无需修复生成可执行命令。

Conclusion: 自适应边缘 - 云端编排是实现有弹性且资源感知的语音控制物联网系统的可行方法。

Abstract: Voice-based interaction has emerged as a natural and intuitive modality for controlling IoT devices. However, speech-driven edge devices face a fundamental trade-off between cloud-based solutions, which offer stronger language understanding capabilities at the cost of latency, connectivity dependence, and privacy concerns, and edge-based solutions, which provide low latency and improved privacy but are limited by computational constraints. This paper presents ASTA, an adaptive speech-to-action solution that dynamically routes voice commands between edge and cloud inference to balance performance and system resource utilization. ASTA integrates on-device automatic speech recognition and lightweight offline language-model inference with cloud-based LLM processing, guided by real-time system metrics such as CPU workload, device temperature, and network latency. A metric-aware routing mechanism selects the inference path at runtime, while a rule-based command validation and repair component ensures successful end-to-end command execution. We implemented our solution on an NVIDIA Jetson-based edge platform and evaluated it using a diverse dataset of 80 spoken commands. Experimental results show that ASTA successfully routes all input commands for execution, achieving a balanced distribution between online and offline inference. The system attains an ASR accuracy of 62.5% and generates executable commands without repair for only 47.5% of inputs, highlighting the importance of the repair mechanism in improving robustness. These results suggest that adaptive edge-cloud orchestration is a viable approach for resilient and resource-aware voice-controlled IoT systems.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [429] [Sharpen the Spec, Cut the Code: A Case for Generative File System with SYSSPEC](https://arxiv.org/abs/2512.13047)
*Qingyuan Liu,Zou Mo,Hengbin Zhang,Dong Du,Yubin Xia,Haibo Chen*

Main category: cs.OS

TL;DR: 本文提出生成式文件系统新范式，介绍SYSSPEC框架，生成SPECFS文件系统，证明规范引导方法生成和演化复杂系统可行且高效。


<details>
  <summary>Details</summary>
Motivation: 传统文件系统开发和维护开销大，现有基于大语言模型创建文件系统因自然语言提示模糊而未成功，需解决文件系统稳健演化问题。

Method: 引入SYSSPEC框架，用形式方法原则替代模糊自然语言，采用多部分规范描述文件系统功能等，开发DAG结构补丁管理演化，工具链含基于大语言模型的代理减轻幻觉。

Result: 生成的SPECFS通过数百个回归测试，与手动编码基线匹配，还能无缝集成Ext4的10个真实世界特性。

Conclusion: 规范引导的方法使生成和演化复杂系统可行且高效。

Abstract: File systems are critical OS components that require constant evolution to support new hardware and emerging application needs. However, the traditional paradigm of developing features, fixing bugs, and maintaining the system incurs significant overhead, especially as systems grow in complexity. This paper proposes a new paradigm, generative file systems, which leverages Large Language Models (LLMs) to generate and evolve a file system from prompts, effectively addressing the need for robust evolution. Despite the widespread success of LLMs in code generation, attempts to create a functional file system have thus far been unsuccessful, mainly due to the ambiguity of natural language prompts.
  This paper introduces SYSSPEC, a framework for developing generative file systems. Its key insight is to replace ambiguous natural language with principles adapted from formal methods. Instead of imprecise prompts, SYSSPEC employs a multi-part specification that accurately describes a file system's functionality, modularity, and concurrency. The specification acts as an unambiguous blueprint, guiding LLMs to generate expected code flexibly. To manage evolution, we develop a DAG-structured patch that operates on the specification itself, enabling new features to be added without violating existing invariants. Moreover, the SYSSPEC toolchain features a set of LLM-based agents with mechanisms to mitigate hallucination during construction and evolution. We demonstrate our approach by generating SPECFS, a concurrent file system. SPECFS passes hundreds of regression tests, matching a manually-coded baseline. We further confirm its evolvability by seamlessly integrating 10 real-world features from Ext4. Our work shows that a specification-guided approach makes generating and evolving complex systems not only feasible but also highly effective.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [430] [Preconditioning Techniques for Hybridizable Discontinuous Galerkin Discretizations on GPU Architectures](https://arxiv.org/abs/2512.13619)
*Andrew Welter,Ngoc Cuong Nguyen*

Main category: math.NA

TL;DR: 本文提出用于GPU上偏微分方程HDG离散化的可扩展迭代求解器和预处理策略，通过GPU定制算法实现，对多种PDE进行综合研究。


<details>
  <summary>Details</summary>
Motivation: 为GPU上偏微分方程的HDG离散化提供可扩展的迭代求解器和预处理策略。

Method: 采用GPU定制算法消除局部单元自由度并直接在设备上组装全局压缩系统；非线性求解器结合牛顿法和预处理GMRES；用批处理形式实现预处理器并进行架构感知优化。

Result: 实现的方法避免稀疏数据结构，提高算术强度，在多种网格和多项式阶数上维持高内存吞吐量。

Conclusion: 所提出的方法可用于多种PDE，在NVIDIA和AMD GPU架构上有效。

Abstract: We present scalable iterative solvers and preconditioning strategies for Hybridizable Discontinuous Galerkin (HDG) discretizations of partial differential equations (PDEs) on graphics processing units (GPUs). The HDG method is implemented using GPU-tailored algorithms in which local element degrees of freedom are eliminated in parallel, and the globally condensed system is assembled directly on the device using dense-block operations. The global matrix is stored in a block format that reflects the natural HDG structure, enabling all iterative solver kernels to be executed with strided batched dense matrix-vector multiplications. This implementation avoids sparse data structures, increases arithmetic intensity, and sustains high memory throughput across a range of meshes and polynomial orders. The nonlinear solver combines Newton's method with preconditioned GMRES, integrating scalable preconditioners such as block-Jacobi, additive Schwarz domain decomposition, and polynomial smoothers. All preconditioners are implemented in batched form with architecture-aware optimizations--including dense linear algebra kernels, memory-coalesced vector operations, and shared-memory acceleration--to minimize memory traffic and maximize parallel occupancy. Comprehensive studies are conducted for a variety of PDEs (including Poisson equation, Burgers equation, linear and nonlinear elasticity, Euler equations, Navier-Stokes equations, and Reynolds-Averaged Navier-Stokes equations) using structured and unstructured meshes with different element types and polynomial orders on both NVIDIA and AMD GPU architectures.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [431] [Hold Onto That Thought: Assessing KV Cache Compression On Reasoning](https://arxiv.org/abs/2512.12008)
*Minghui Liu,Aadi Palnitkar,Tahseen Rabbani,Hyunwoo Jae,Kyle Rui Sang,Dixi Yao,Shayan Shabihi,Fuheng Zhao,Tian Li,Ce Zhang,Furong Huang,Kunpeng Zhang*

Main category: cs.CL

TL;DR: 评估流行压缩策略在长推理任务上的性能，发现不同模型适用策略不同，H2O和改进的SnapKV对推理模型效果好，低预算驱逐策略有缓存大小与推理成本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有压缩算法多针对预填充阶段，很少评估其在长解码推理任务上的性能，需要进行相关研究。

Method: 对几种流行的压缩策略在长推理任务上进行基准测试。

Result: 对于非推理模型，没有单一策略适用于所有情况，性能受数据集类型影响大；H2O和改进的SnapKV是推理模型的优势策略；低预算驱逐策略可产生更长推理轨迹。

Conclusion: 不同类型模型在长推理任务上适用的压缩策略不同，重尾跟踪对推理轨迹有用，存在缓存大小和推理成本的权衡。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.

</details>


### [432] [Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects](https://arxiv.org/abs/2512.12818)
*Chris Latimer,Nicoló Boschi,Andrew Neeser,Chris Bartholomew,Gaurav Srivastava,Xuan Wang,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: 介绍当前代理记忆系统问题，提出Hindsight架构，在长时对话记忆基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前代理记忆系统存在混淆证据与推理、长时信息组织困难、推理解释支持有限等问题，需改进。

Method: 提出Hindsight架构，将代理记忆组织成四个逻辑网络，支持保留、回忆、反思三个核心操作，有时间和实体感知记忆层及反思层。

Result: 在LongMemEval和LoCoMo等基准测试中，Hindsight提升准确率，超越全上下文基线及GPT - 4o，扩大骨干模型后表现更好。

Conclusion: Hindsight架构在多会话和开放域问题上持续优于现有记忆架构。

Abstract: Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.

</details>


### [433] [Large-Language Memorization During the Classification of United States Supreme Court Cases](https://arxiv.org/abs/2512.13654)
*John E. Ortega,Dhruv D. Joshi,Matt P. Borkowski*

Main category: cs.CL

TL;DR: 本文研究大语言模型在最高法院判决分类任务中的表现，发现带记忆的基于提示的模型比之前基于BERT的模型更稳健。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型在分类任务中的响应方式，研究其记忆策略，最高法院判决语料库适合研究大语言模型的记忆准确性。

Method: 使用最新的大语言模型微调与基于检索的方法，对两个传统的最高法院分类任务进行实验。

Result: 带记忆的基于提示的模型（如DeepSeek）在两个任务上比之前非基于提示的模型得分高约2分。

Conclusion: 带记忆的基于提示的模型在最高法院判决分类任务中比之前基于BERT的模型更稳健。

Abstract: Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called "hallucinations" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.

</details>


### [434] [Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation](https://arxiv.org/abs/2512.13655)
*Richard J. Young*

Main category: cs.CL

TL;DR: 评估四种消除工具在16个指令微调模型上的效果，为研究人员提供选择标准，发现数学推理能力对消除干预最敏感。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的安全对齐机制阻碍合法研究应用，现有消除技术效果未被充分表征。

Method: 对四种消除工具（Heretic、DECCP、ErisForge、FailSpy）在16个指令微调模型（7B - 14B参数）上进行评估，报告工具兼容性和定量指标。

Result: 单遍方法在基准子集上能力保留更好，贝叶斯优化消除产生可变分布偏移和与模型相关的能力影响。

Conclusion: 研究为不同模型架构部署消除工具提供基于证据的选择标准，数学推理能力对消除干预最敏感。

Abstract: Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.

</details>


### [435] [EMNLP: Educator-role Moral and Normative Large Language Models Profiling](https://arxiv.org/abs/2508.15250)
*Yilin Jiang,Mingzi Zhang,Sheng Jin,Zengyi Yu,Xiangjie Kong,Binghao Tu*

Main category: cs.CL

TL;DR: 本文提出EMNLP框架评估教师角色大语言模型的伦理和心理一致性，实验揭示模型特点与能力安全悖论，提供首个基准。


<details>
  <summary>Details</summary>
Motivation: 当前模拟职业的大语言模型缺乏全面心理和伦理评估，需对教师角色大语言模型进行评估。

Method: 引入EMNLP框架，扩展现有量表、构建教师道德困境，用软提示注入集评估。

Result: 教师角色大语言模型个性更理想化和两极化，抽象道德推理强但应对情感复杂情况差，推理强的模型易受有害提示影响，超参数影响有限。

Conclusion: 提出首个评估教育AI中教师角色大语言模型伦理和心理一致性的基准。

Abstract: Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 14 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at https://e-m-n-l-p.github.io/.

</details>


### [436] [Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention](https://arxiv.org/abs/2512.11811)
*Fengyi Xu,Jun Ma,Waishan Qiu,Cui Guo*

Main category: cs.CL

TL;DR: 本文提出VPR - AttLLM框架，将大语言模型融入VPR流程，提升众包街景图像视觉定位识别性能，还建立通用范式，有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 众包街景图像缺乏地理元数据，现有的视觉位置识别（VPR）模型在跨源场景存在性能下降问题。

Method: 提出VPR - AttLLM模型无关框架，通过注意力引导的描述符增强将大语言模型的语义推理和地理空间知识集成到现有VPR流程中。

Result: 在多个基准测试中，将VPR - AttLLM与三个先进VPR模型结合，召回性能提升1 - 3%，在最具挑战性的真实洪水图像上可达8%。

Conclusion: 建立了视觉检索系统中LLM引导的多模态融合的可推广范式，其设计有在城市监测和众包危机图像快速地理定位上的应用潜力。

Abstract: Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.

</details>


### [437] [KH-FUNSD: A Hierarchical and Fine-Grained Layout Analysis Dataset for Low-Resource Khmer Business Document](https://arxiv.org/abs/2512.11849)
*Nimol Thuon,Jun Du*

Main category: cs.CL

TL;DR: 提出首个公开可用的高棉语表单文档理解分层注释数据集KH - FUNSD，对多个领先模型进行基准测试并给出基线结果。


<details>
  <summary>Details</summary>
Motivation: 自动化文档布局分析对低资源、非拉丁脚本是挑战，高棉语在文档AI工具开发中受关注少，商业文档缺乏专用资源。

Method: 设计三级注释框架，包括区域检测、FUNSD风格注释、细粒度分类，对多个领先模型进行基准测试。

Result: 得到了高棉语商业文档的第一组基线结果。

Conclusion: 提出KH - FUNSD数据集支持全面布局分析和精确信息提取，探讨了非拉丁、低资源脚本带来的独特挑战。

Abstract: Automated document layout analysis remains a major challenge for low-resource, non-Latin scripts. Khmer is a language spoken daily by over 17 million people in Cambodia, receiving little attention in the development of document AI tools. The lack of dedicated resources is particularly acute for business documents, which are critical for both public administration and private enterprise. To address this gap, we present \textbf{KH-FUNSD}, the first publicly available, hierarchically annotated dataset for Khmer form document understanding, including receipts, invoices, and quotations. Our annotation framework features a three-level design: (1) region detection that divides each document into core zones such as header, form field, and footer; (2) FUNSD-style annotation that distinguishes questions, answers, headers, and other key entities, together with their relationships; and (3) fine-grained classification that assigns specific semantic roles, such as field labels, values, headers, footers, and symbols. This multi-level approach supports both comprehensive layout analysis and precise information extraction. We benchmark several leading models, providing the first set of baseline results for Khmer business documents, and discuss the distinct challenges posed by non-Latin, low-resource scripts. The KH-FUNSD dataset and documentation will be available at URL.

</details>


### [438] [Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings](https://arxiv.org/abs/2512.12167)
*Yoav Gelberg,Koshi Eguchi,Takuya Akiba,Edoardo Cetin*

Main category: cs.CL

TL;DR: 提出DroPE方法打破语言模型上下文扩展需微调的瓶颈，实现零样本上下文扩展且表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型扩展上下文时需昂贵微调的问题，基于对位置嵌入的理论和实证观察。

Method: 提出DroPE方法，即训练后丢弃语言模型的位置嵌入。

Result: DroPE无需长上下文微调即可实现无缝零样本上下文扩展，在不同模型和数据集规模下表现远超先前方法。

Conclusion: DroPE能有效扩展语言模型上下文，且不影响其在原训练上下文中的能力。

Abstract: So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.

</details>


### [439] [Diffusion Language Model Inference with Monte Carlo Tree Search](https://arxiv.org/abs/2512.12168)
*Zheng Huang,Kiran Ramnath,Yueyan Chen,Aosong Feng,Sangmin Woo,Balasubramaniam Srinivasan,Zhichao Xu,Kang Zhou,Shuai Wang,Haibo Ding,Lin Lee Cheong*

Main category: cs.CL

TL;DR: 介绍MEDAL框架用蒙特卡洛树搜索初始化来进行扩散语言模型推理，在多基准测试中表现优于现有策略。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型推理方法在确定解掩码位置和选择标记时，使用启发式方法近似搜索常导致次优解码路径，其他方法需额外训练，因此需要一种有原则的搜索机制。

Method: 引入MEDAL框架，在初始化阶段使用蒙特卡洛树搜索探索解掩码轨迹，并将搜索空间限制在高置信度动作，优先选择能提高模型对剩余掩码位置置信度的标记。

Result: 在多个基准测试中，MEDAL较现有推理策略最多实现22.0%的改进。

Conclusion: MEDAL为扩散语言模型基于搜索的推理建立了新范式。

Abstract: Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.

</details>


### [440] [Semantic Distance Measurement based on Multi-Kernel Gaussian Processes](https://arxiv.org/abs/2512.12238)
*Yinzhu Cheng,Haihua Xie,Yaqing Wang,Miao He,Mingming Sun*

Main category: cs.CL

TL;DR: 提出基于多核高斯过程（MK - GP）的语义距离度量方法，并在上下文学习（ICL）设置下的细粒度情感分类中评估，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 大多数经典语义距离方法固定，难以适应特定数据分布和任务要求。

Method: 将与文本相关的潜在语义函数建模为高斯过程，其协方差函数由Matérn和多项式组件组合的核给出，核参数从有监督的数据中自动学习。

Result: 在上下文学习（ICL）设置下的细粒度情感分类中评估，实验证明所提出度量方法有效。

Conclusion: 所提出的基于多核高斯过程（MK - GP）的语义距离度量方法是有效的。

Abstract: Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Matérn and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.

</details>


### [441] [Adversarially Probing Cross-Family Sound Symbolism in 27 Languages](https://arxiv.org/abs/2512.12245)
*Anika Sharma,Tianyi Niu,Emma Wrenn,Shashank Srivastava*

Main category: cs.CL

TL;DR: 本文对大小语义领域的语音象征现象进行了首次跨语言计算分析，发现语音形式能预测大小语义，且存在跨语系的语音象征偏差，并发布相关数据和工具。


<details>
  <summary>Details</summary>
Motivation: 以往语音象征现象多通过轶事实验证明，缺乏大规模测试，本文旨在对大小语义领域的语音象征进行跨语言计算分析。

Method: 编译包含27种语言810个形容词的数据集，使用基于音段特征袋的可解释分类器，训练对抗清洗器抑制语言身份。

Result: 语音形式能在无关语言中以高于随机水平预测大小语义，元音和辅音都有贡献；语言预测低于随机水平，大小预测显著高于随机水平。

Conclusion: 存在跨语系的语音象征偏差，为未来象似性大规模研究提供了数据、代码和诊断工具。

Abstract: The phenomenon of sound symbolism, the non-arbitrary mapping between word sounds and meanings, has long been demonstrated through anecdotal experiments like Bouba Kiki, but rarely tested at scale. We present the first computational cross-linguistic analysis of sound symbolism in the semantic domain of size. We compile a typologically broad dataset of 810 adjectives (27 languages, 30 words each), each phonemically transcribed and validated with native-speaker audio. Using interpretable classifiers over bag-of-segment features, we find that phonological form predicts size semantics above chance even across unrelated languages, with both vowels and consonants contributing. To probe universality beyond genealogy, we train an adversarial scrubber that suppresses language identity while preserving size signal (also at family granularity). Language prediction averaged across languages and settings falls below chance while size prediction remains significantly above chance, indicating cross-family sound-symbolic bias. We release data, code, and diagnostic tools for future large-scale studies of iconicity.

</details>


### [442] [SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema](https://arxiv.org/abs/2512.12337)
*Yushen Fang,Jianjun Li,Mingqian Ding,Chang Liu,Xinchi Zou,Wenqi Yang*

Main category: cs.CL

TL;DR: 提出SCIR框架和MBSC数据集解决大模型信息抽取微调的高成本和偏好对齐问题，实验显示性能提升且降低成本。


<details>
  <summary>Details</summary>
Motivation: 当前大模型信息抽取微调范式面临高训练成本和难与大模型偏好对齐问题。

Method: 提出SCIR框架，通过双路径自校正模块和反馈驱动优化实现与现有模型和系统的即插即用；构建MBSC数据集，将GPT - 4能力间接提炼到信息抽取结果检测模型。

Result: SCIR在三个关键任务上优于现有方法，基于跨度的Micro - F1平均提高5.27%，相比基线方法降低87%训练成本。

Conclusion: 提升了信息抽取系统的灵活性和准确性，为轻量级高效信息抽取范式奠定基础。

Abstract: Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.

</details>


### [443] [Coupled Variational Reinforcement Learning for Language Model General Reasoning](https://arxiv.org/abs/2512.12576)
*Xueru Wen,Jie Lou,Yanjiang Liu,Hongyu Lin,Ben He,Xianpei Han,Le Sun,Yaojie Lu,Debing Zhang*

Main category: cs.CL

TL;DR: 本文提出CoVRL方法，结合变分推理和强化学习，在数学和通用推理基准测试中提升语言模型推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有无验证器强化学习方法在推理轨迹采样时与答案信息解耦，导致探索效率低和轨迹与最终答案不一致，需要改进。

Method: 提出CoVRL方法，通过混合采样策略耦合先验和后验分布，构建并优化复合分布。

Result: 在数学和通用推理基准测试中，CoVRL较基础模型性能提升12.4%，较先进无验证器强化学习基线额外提升2.3%。

Conclusion: CoVRL为提升语言模型的通用推理能力提供了一个有原则的框架。

Abstract: While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \textit{\b{Co}upled \b{V}ariational \b{R}einforcement \b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\% over the base model and achieves an additional 2.3\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.

</details>


### [444] [Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery](https://arxiv.org/abs/2512.12608)
*Hong Su*

Main category: cs.CL

TL;DR: 本文针对大语言模型应对罕见场景的不足，提出受人类启发的学习框架，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理罕见、低资源或未见场景时表现不佳，且依赖隐式参数记忆，缺乏明确获取、回忆和完善方法的能力。

Method: 提出受人类启发的学习框架，包含'显式记录'和'最大熵方法发现'两种互补机制。

Result: 在60个语义多样的问答对基准上验证，该方法比随机基线有更强未见问题覆盖率和更高内部多样性。

Conclusion: 该熵引导方法能发现更具泛化性和受人类启发的方法，是有效的。

Abstract: Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.

</details>


### [445] [Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives](https://arxiv.org/abs/2512.12620)
*Aheli Poddar,Saptarshi Sahoo,Sujata Ghosh*

Main category: cs.CL

TL;DR: 从逻辑和自然语言角度研究大语言模型的三段论推理能力，使用14个模型展开研究，发现推理机制非统一涌现属性，部分模型符号推理表现完美。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型的基本推理能力以及该研究的发展方向。

Method: 使用14个大语言模型，从符号推理和自然语言理解两方面研究其三段论推理能力。

Result: 三段论推理机制并非大语言模型统一的涌现属性，部分模型有完美的符号推理表现。

Conclusion: 部分模型完美的符号推理表现引发思考：大语言模型是否正逐渐成为更正式的推理机制，而非体现人类推理的细微差别。

Abstract: We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.

</details>


### [446] [Reinforcement Learning for Latent-Space Thinking in LLMs](https://arxiv.org/abs/2512.11816)
*Enes Özeren,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 研究潜空间思维训练方法，发现Coconut方法有局限，用强化学习技术优化潜思维步骤，但强化学习训练模型在数学推理领域仍落后于传统语言空间思维模型。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理在离散语言空间思考效率低，现有潜空间思维训练方法在复杂任务中难以保持性能。

Method: 实验分析Coconut方法局限，研究GRPO等强化学习技术，设计新的潜强化学习方法优化潜思维步骤。

Result: 强化学习训练的模型在数学推理领域落后于传统语言空间链式思维模型。

Conclusion: 潜空间思维训练方法有改进空间，强化学习在潜空间思维应用仍需提升。

Abstract: Chain-of-Thought (CoT) reasoning typically utilizes the discrete language space for thinking, which is inherently inefficient, as many generated tokens only enforce linguistic rules that are not required for reasoning. To bypass this, latent-space thinking allows models to think using the continuous embedding space. While existing methods for training those models show domain-specific gains, they fail to maintain performance in complex tasks, such as mathematical reasoning. We experimentally demonstrate that the Coconut approach, a form of supervised fine-tuning for latent-space thinking, is highly sensitive to design choices and exhibits several inherent limitations. To address these issues, we investigate reinforcement learning (RL) techniques -- an underexplored direction in latent-space thinking -- including GRPO and design a novel Latent RL method for directly optimizing the latent thinking steps. Our experimental results reveal that these RL-trained models still lag behind traditional language-space CoT models in the mathematical reasoning domain. We make our codebase publicly available.

</details>


### [447] [Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches](https://arxiv.org/abs/2512.12677)
*Amirhossein Yousefiramandi,Ciaran Cooney*

Main category: cs.CL

TL;DR: 研究在资源受限下微调仅解码器大语言模型用于文本分类的策略，对比两种方法，证明嵌入法效果好。


<details>
  <summary>Details</summary>
Motivation: 探索在资源受限下微调仅解码器大语言模型用于下游文本分类的有效策略。

Method: 研究两种方法，一是给预训练因果语言模型添加分类头微调，二是用提示-响应格式进行指令调优；结合4位模型量化和低秩自适应进行参数高效训练。

Result: 在两个数据集上实验，嵌入法F1分数显著优于指令调优法，且与微调的特定领域模型相比有竞争力甚至更优。

Conclusion: 直接利用因果语言模型的内部表示和高效微调技术，能在有限计算资源下获得出色分类性能，还讨论了方法优势、给出实践指南和未来方向。

Abstract: We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.

</details>


### [448] [State over Tokens: Characterizing the Role of Reasoning Tokens](https://arxiv.org/abs/2512.12777)
*Mosh Levy,Zohar Elyoseph,Shauli Ravfogel,Yoav Goldberg*

Main category: cs.CL

TL;DR: 引入SoT框架解释大语言模型推理标记非忠实解释问题，呼吁超越文本解读。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型推理标记外观与功能的差距，即其看似像人类思维过程但并非实际推理过程的忠实解释。

Method: 引入State over Tokens (SoT)概念框架，将推理标记重新定义为外部计算状态。

Result: 解释了推理标记在文本上非忠实解释却能驱动正确推理的原因，揭示了之前被忽视的研究问题。

Conclusion: 要真正理解大语言模型的推理过程，研究应超越将推理标记作为文本解读，聚焦于将其解码为状态。

Abstract: Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.

</details>


### [449] [Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA](https://arxiv.org/abs/2512.12812)
*Hanyu Cai,Binqi Shen,Lier Jin,Lan Hu,Xiaojing Fan*

Main category: cs.CL

TL;DR: 本文提出系统评估框架，研究交互语气对三个大语言模型性能的影响，发现语气敏感性因模型和领域而异，现代大模型对语气变化总体较稳健。


<details>
  <summary>Details</summary>
Motivation: 现有研究对语用元素（如语言语气和礼貌程度）对大语言模型性能的影响探索不足，特别是跨不同模型家族的情况。

Method: 提出系统评估框架，使用MMMLU基准测试，评估三个大语言模型在非常友好、中立和非常粗鲁三种提示变体下的性能，并进行统计显著性检验。

Result: 语气敏感性因模型和领域而异，中立或非常友好的提示通常比非常粗鲁的提示准确率更高，但仅在部分人文任务中有统计显著影响；跨任务聚合时，语气影响减弱且大多失去统计显著性。

Conclusion: 交互语气在特定解释场景中可能重要，但现代大语言模型在典型混合领域使用中对语气变化总体较稳健，为实际应用中的提示设计和模型选择提供了指导。

Abstract: Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.
  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.

</details>


### [450] [VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs](https://arxiv.org/abs/2512.12072)
*Avinash Amballa,Yashas Malur Saidutta,Chi-Heng Lin,Vivek Kulkarni,Srinivas Chappidi*

Main category: cs.CL

TL;DR: 本文提出Voyager方法生成多样数据集，无需训练、适用于闭源模型且可扩展，实验显示其在多样性上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型生成的合成数据集缺乏多样性的问题。

Method: 提出Voyager方法，该方法是迭代的，利用行列式点过程直接优化数据集多样性的数学量，且无需训练，适用于闭源模型，可扩展。

Result: 通过综合实验表明，Voyager在多样性上比流行的基线方法有1.5 - 3倍的提升。

Conclusion: Voyager是一种有效生成多样数据集的方法，显著优于基线方法。

Abstract: Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.

</details>


### [451] [Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM](https://arxiv.org/abs/2512.12868)
*Furong Jia,Yuan Pu,Finn Guo,Monica Agrawal*

Main category: cs.CL

TL;DR: 研究大语言模型在临床诊断基准测试中的性能是否源于概率推理，引入FBPR方法，发现其与LLMs性能相当且各有优势，凸显显式概率基线的价值。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在临床诊断基准测试中的性能在多大程度上反映了潜在的概率推理。

Method: 引入基于频率的概率排序器（FBPR），通过对大型语料库中概念 - 诊断共现统计的平滑朴素贝叶斯对选项进行评分。

Result: 当共现统计来自OLMo和Llama的预训练语料库时，FBPR的性能与相应LLMs相当；直接LLM推理和FBPR答对的问题大多不同，重叠仅略高于随机水平。

Conclusion: 显式概率基线具有持续价值，可提供有意义的性能参考点和潜在混合方法的互补信号；类似历史悠久、低复杂度专家系统的方法仍占基准测试性能的很大一部分。

Abstract: Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.

</details>


### [452] [Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping](https://arxiv.org/abs/2512.12950)
*Lingyi Meng,Maolin Liu,Hao Wang,Yilan Cheng,Qi Yang,Idlkaid Mohanmmed*

Main category: cs.CL

TL;DR: 提出人机协作构建多语言法律术语数据库的方法，经实验证明可提升映射精度、一致性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 准确跨语言映射法律术语有挑战，尤其是中日语言，且现有资源和工具有限。

Method: 基于多智能体框架的人机协作方法，AI处理特定重复任务，人类专家监督。

Result: 使用三语平行语料库测试，该方法提升了多语言法律术语映射的精度和一致性，有更好的可扩展性。

Conclusion: 人机协作、多智能体工作流优于传统手动方法。

Abstract: Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.

</details>


### [453] [LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators](https://arxiv.org/abs/2512.13063)
*Cheril Shah,Akshit Agarwal,Kanak Garg,Mourad Heddaya*

Main category: cs.CL

TL;DR: 提出双曲正切曲线建模让步动态框架及相关指标，对比人类和大语言模型谈判表现，发现大语言模型谈判能力有局限。


<details>
  <summary>Details</summary>
Motivation: 双边谈判复杂且依赖情境，需了解大语言模型在谈判中的表现及与人类的差异。

Method: 引入基于双曲正切曲线的让步动态建模框架，提出两个指标，在多种场景下对人类和四个最先进大语言模型进行大规模实证比较。

Result: 大语言模型在谈判中系统地锚定在可能协议区域的极端，优化固定点，策略多样性有限且有欺骗策略，能力不随模型改进而提升。

Conclusion: 当前大语言模型谈判能力存在根本局限，需能内化对手推理和情境依赖策略的模型。

Abstract: Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.

</details>


### [454] [HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks](https://arxiv.org/abs/2512.12544)
*Yiming Zeng,Jinghan Cao,Zexin Li,Wanhao Yu,Zhankai Ye,Dawei Xiang,Ting Hua,Xin Liu,Shangqian Gao,Tingting Yu*

Main category: cs.CL

TL;DR: 现有大语言模型在基于指令的文本编辑任务中表现不佳，本文提出HyperEdit方法，通过超网络动态适配和差异感知正则化解决问题，在修改区域的BLEU指标上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在基于指令的文本编辑任务中存在困难，现有方法将编辑视为通用文本生成，导致难以与用户意图对齐和过度编辑问题。

Method: 提出HyperEdit，引入超网络的动态适配生成特定请求参数，开发差异感知正则化聚焦修改区域监督。

Result: HyperEdit在修改区域的BLEU指标上相对现有最先进基线有9% - 30%的提升，且仅使用3B参数。

Conclusion: HyperEdit能有效解决大语言模型在文本编辑任务中的问题，实现精确、最小化的修改。

Abstract: Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.

</details>


### [455] [Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing](https://arxiv.org/abs/2512.13109)
*Zewen Qiang,Sendong Zhao,Haochun Wang,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 大语言模型处理长文本有‘lost in the middle’问题，研究发现初始显著性是新因素，利用此特性并结合现有方法可提升长文本处理性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型处理长文本序列时因‘lost in the middle’现象导致的性能问题。

Method: 识别初始显著性这一新因素，利用该特性缩放初始标记和其他标记间的注意力权重，并结合现有减少位置编码偏差的方法。

Result: 在MDQA数据集上最高提升3.6%性能，在KV - Retrieval任务中最高提升3.4%性能。

Conclusion: 利用初始显著性特性并结合现有方法能有效提升大语言模型处理长文本的能力。

Abstract: Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\% in KV-Retrieval tasks.

</details>


### [456] [Modeling Authorial Style in Urdu Novels Using Character Interaction Graphs and Graph Neural Networks](https://arxiv.org/abs/2512.12654)
*Hassan Mujtaba,Hamza Naveed,Hanzlah Munir*

Main category: cs.CL

TL;DR: 提出基于图的框架以叙事结构分析乌尔都语小说作者风格，实验显示学习的图表示表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统作者分析侧重词汇和风格线索，高层次叙事结构研究不足，特别是乌尔都语这类低资源语言，需探索能否从叙事结构推断作者风格。

Method: 将乌尔都语小说建模为人物交互网络，用图表示，系统比较多种图表示，包括全局结构特征、节点级语义总结、无监督图嵌入和有监督图神经网络。

Result: 在7位作者的52部乌尔都语小说数据集上，学习的图表示大幅优于手工和无监督基线，严格评估协议下准确率达0.857。

Conclusion: 基于图的框架对分析乌尔都语小说作者风格有效，从叙事结构推断作者风格是可行的。

Abstract: Authorship analysis has traditionally focused on lexical and stylistic cues within text, while higher-level narrative structure remains underexplored, particularly for low-resource languages such as Urdu. This work proposes a graph-based framework that models Urdu novels as character interaction networks to examine whether authorial style can be inferred from narrative structure alone. Each novel is represented as a graph where nodes correspond to characters and edges denote their co-occurrence within narrative proximity. We systematically compare multiple graph representations, including global structural features, node-level semantic summaries, unsupervised graph embeddings, and supervised graph neural networks. Experiments on a dataset of 52 Urdu novels written by seven authors show that learned graph representations substantially outperform hand-crafted and unsupervised baselines, achieving up to 0.857 accuracy under a strict author-aware evaluation protocol.

</details>


### [457] [Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models](https://arxiv.org/abs/2512.13194)
*Chendong Sun*

Main category: cs.CL

TL;DR: 本文针对投机解码的随机拒绝问题提出EARS方法，实验表明其能提升解码效率且无需修改模型架构。


<details>
  <summary>Details</summary>
Motivation: 现有投机解码的拒绝采样机制采用固定随机阈值，在高不确定性场景下存在严重的随机拒绝问题，影响推理效率。

Method: 引入Efficient Adaptive Rejection Sampling (EARS)方法，通过结合目标模型的预测不确定性动态调整接受阈值，在模型不确定时放宽接受标准。

Result: 在创意写作和开放域问答任务实验中，EARS显著提升了投机解码效率，在GSM8K基准上吞吐量最高提升18.12%，准确率仅下降0.84%。

Conclusion: EARS方法能有效减少随机拒绝，提升投机解码效率，且无需修改模型架构，可无缝集成到现有框架。

Abstract: Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant "random rejection" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \(1 - \max(P_{\mathrm{target}})\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.

</details>


### [458] [MiniLingua: A Small Open-Source LLM for European Languages](https://arxiv.org/abs/2512.13298)
*Anna Aksenova,Boris Zverkov,Nicola Dainese,Alexander Nikitin,Pekka Marttinen*

Main category: cs.CL

TL;DR: 本文介绍十亿参数多语言开源大模型MiniLingua，其在多任务表现出色并发布了相关权重、分词器和代码。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在计算成本高、隐私问题和以英语为中心训练的局限，小而高效模型有发展潜力。

Method: 从头开始训练面向13种欧洲语言的十亿参数多语言开源大模型MiniLingua。

Result: MiniLingua指令微调版本在多项任务上优于EuroLLM，在开放式生成任务上与先进模型有竞争力。

Conclusion: MiniLingua能在平衡覆盖范围和指令跟随能力方面表现良好，且研究成果已开源。

Abstract: Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.

</details>


### [459] [FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models](https://arxiv.org/abs/2512.13330)
*Joona Kytöniemi,Jousia Piha,Akseli Reunamo,Fedor Vitiugin,Farrokh Mehryary,Sampo Pyysalo*

Main category: cs.CL

TL;DR: 介绍芬兰语大语言模型评估基准套件FIN - bench - v2，整合数据集，筛选任务，评估模型，资源公开。


<details>
  <summary>Details</summary>
Motivation: 为评估芬兰语大语言模型提供统一的基准套件。

Method: 将芬兰语版本常用基准和原FIN - bench更新扩展版本整合，转换数据集格式，对机器翻译资源人工标注，通过预训练模型筛选任务，评估更大的指令调优模型。

Result: 完成FIN - bench - v2基准套件，包含多任务、多提示形式的数据集。

Conclusion: FIN - bench - v2可用于评估芬兰语大语言模型，相关资源公开可获取。

Abstract: We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.

</details>


### [460] [Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers](https://arxiv.org/abs/2512.13363)
*Shibani Sankpal*

Main category: cs.CL

TL;DR: 研究检测心理健康相关文本中的情绪漂移，用预训练模型获取结果，可助于理解内容情感动态。


<details>
  <summary>Details</summary>
Motivation: 传统情感分析易忽略单条消息内情绪的细微变化，故研究心理健康相关文本中的情绪漂移。

Method: 利用DistilBERT和RoBERTa等预训练的transformer模型检测句子级情绪并测量情绪漂移分数。

Result: 得到心理健康对话中情绪升级或缓解的模式。

Conclusion: 该方法有助于理解内容中的情感动态。

Abstract: This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.

</details>


### [461] [Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models](https://arxiv.org/abs/2512.13478)
*Kei Saito*

Main category: cs.CL

TL;DR: 当前语言模型存在过早语义崩溃问题，本文提出非解析推理（NRR）框架解决该问题，合成评估显示其效果良好。


<details>
  <summary>Details</summary>
Motivation: 解决当前语言模型中过早语义崩溃的核心架构局限问题。

Method: 引入NRR框架，包含多向量嵌入、非崩溃注意力和上下文身份跟踪三个组件，通过外部解析算子统一，分离表示和解析。

Result: 合成评估中，CIT增强模型在分布外身份转移任务上准确率达90.9%，而Transformer基线仅为9.1%。

Conclusion: NRR为过早崩溃提供了原则性替代方案，将歧义重新定义为显式表示状态，关键在于何时、如何以及由谁控制解析歧义。

Abstract: Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing "Dr. Smith the cardiologist" from "Dr. Smith the researcher"). These mechanisms are unified by an external Resolution Operator $ρ$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.

</details>


### [462] [SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping](https://arxiv.org/abs/2512.13494)
*Yu-Chen Lu,Sheng-Feng Yu,Hui-Hsien Weng,Pei-Shuo Wang,Yu-Fang Hu,Liang Hung-Chun,Hung-Yueh Chiang,Kai-Chiang Wu*

Main category: cs.CL

TL;DR: 本文提出SkipCat低秩压缩框架解决大语言模型低秩压缩时性能与效率的权衡问题，实验显示该方法在相同压缩率下表现更优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数量大，在边缘设备部署困难，传统低秩压缩方法需大幅降低保留秩以节省资源，但会导致性能严重下降，需解决此权衡问题。

Method: 提出SkipCat框架，包括层内共享低秩投影方法和块跳过技术，减少冗余并提高压缩效率。

Result: 实验表明，在相同压缩率下，无需额外微调，该方法在零样本任务上准确率比之前方法提高7%。

Conclusion: 提出的最大秩压缩策略在资源受限情况下能有效保留模型性能。

Abstract: Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, naïve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.

</details>


### [463] [Verifying Rumors via Stance-Aware Structural Modeling](https://arxiv.org/abs/2512.13559)
*Gibson Nkhata,Uttamasha Anjally Oyshi,Quan Mai,Susan Gauch*

Main category: cs.CL

TL;DR: 提出一种立场感知结构建模方法，在谣言真实性预测上优于先前方法且适用于早期检测和跨平台泛化。


<details>
  <summary>Details</summary>
Motivation: 现有模型在结合语义内容、立场信息和对话结构方面存在困难，尤其是在基于transformer的编码器的序列长度限制下，需要更好的方法来验证社交媒体上的谣言。

Method: 提出立场感知结构建模，对帖子编码并按立场类别聚合回复嵌入，引入立场分布和层次深度作为协变量。

Result: 在基准数据集上的大量实验表明，该方法在预测谣言真实性的能力上显著优于先前方法。

Conclusion: 该模型在谣言真实性预测上效果好，且具有早期检测和跨平台泛化的通用性。

Abstract: Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.

</details>


### [464] [Memory in the Age of AI Agents](https://arxiv.org/abs/2512.13564)
*Yuyang Hu,Shichun Liu,Yanwei Yue,Guibin Zhang,Boyang Liu,Fangyi Zhu,Jiahang Lin,Honglin Guo,Shihan Dou,Zhiheng Xi,Senjie Jin,Jiejun Tan,Yanbin Yin,Jiongnan Liu,Zeyu Zhang,Zhongxiang Sun,Yutao Zhu,Hao Sun,Boci Peng,Zhenrong Cheng,Xuanbo Fan,Jiaxin Guo,Xinlei Yu,Zhenhong Zhou,Zewen Hu,Jiahao Huo,Junhao Wang,Yuwei Niu,Yu Wang,Zhenfei Yin,Xiaobin Hu,Yue Liao,Qiankun Li,Kun Wang,Wangchunshu Zhou,Yixin Liu,Dawei Cheng,Qi Zhang,Tao Gui,Shirui Pan,Yan Zhang,Philip Torr,Zhicheng Dou,Ji-Rong Wen,Xuanjing Huang,Yu-Gang Jiang,Shuicheng Yan*

Main category: cs.CL

TL;DR: 本文旨在梳理当前基于基础模型的智能体记忆研究现状，从形式、功能和动态性三个方面进行分析，并总结基准测试和开源框架，还展望了研究前沿。


<details>
  <summary>Details</summary>
Motivation: 当前智能体记忆研究领域碎片化，术语定义模糊，传统分类不足以涵盖当代智能体记忆系统的多样性，因此需要梳理现状。

Method: 明确智能体记忆的范围并与相关概念区分，从形式、功能和动态性三个视角分析，总结记忆基准测试和开源框架。

Result: 确定了三种主要的记忆实现形式，提出了更细粒度的功能分类，分析了记忆的形成、演变和检索过程，总结了基准测试和开源框架。

Conclusion: 本文不仅可作为现有工作的参考，还为未来智能体智能设计中重新思考记忆作为一等原语提供概念基础。

Abstract: Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.

</details>


### [465] [ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding](https://arxiv.org/abs/2512.13586)
*Jia-Nan Li,Jian Guan,Wei Wu,Chongxuan Li*

Main category: cs.CL

TL;DR: 论文提出新型掩码扩散模型ReFusion，将并行解码提升到slot级别，实验显示其在性能和速度上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 自回归模型（ARMs）顺序推理慢，掩码扩散模型（MDMs）有计算开销大、生成不连贯问题，需改进。

Method: 提出ReFusion模型，通过迭代的“计划 - 填充”解码过程，从token级提升到slot级并行解码。

Result: 在七个基准测试中，ReFusion性能比之前的MDMs平均提高34%、速度提升超18倍，缩小与ARMs的性能差距并保持2.33倍的平均加速。

Conclusion: ReFusion模型能有效解决现有模型问题，在性能和效率上取得优势。

Abstract: Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\times$ average speedup.

</details>


### [466] [Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models](https://arxiv.org/abs/2512.13607)
*Boxin Wang,Chankyu Lee,Nayeon Lee,Sheng-Chieh Lin,Wenliang Dai,Yang Chen,Yangyi Chen,Zhuolin Yang,Zihan Liu,Mohammad Shoeybi,Bryan Catanzaro,Wei Ping*

Main category: cs.CL

TL;DR: 提出级联领域强化学习方法Cascade RL开发通用推理模型Nemotron - Cascade，能在多种模式运行，性能优异。


<details>
  <summary>Details</summary>
Motivation: 构建通用推理模型的强化学习存在跨领域异构性问题，使基础设施复杂、训练变慢且参数选择困难。

Method: 提出Cascade RL，进行顺序的领域强化学习，还将RLHF作为预步骤。

Result: 14B模型在RL后超过其SFT老师，在LiveCodeBench及2025 IOI中表现优异。

Conclusion: Cascade RL能降低工程复杂度，在多个基准测试中达到先进水平，且分享了训练和数据方法。

Abstract: Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.

</details>


### [467] [AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning](https://arxiv.org/abs/2512.13278)
*Jiaru Zou,Ling Yang,Yunzhe Qi,Sirui Chen,Mengting Ai,Ke Shen,Jingrui He,Mengdi Wang*

Main category: cs.CL

TL;DR: 提出AutoTool框架，使LLM智能体具动态工具选择能力，在多个基准测试中表现出色且泛化性强。


<details>
  <summary>Details</summary>
Motivation: 现有方法假定工具固定，限制了LLM智能体适应新或不断演变工具集的能力。

Method: 构建含200k数据的数据集，采用双阶段优化管道，包括基于监督和强化学习的轨迹稳定，以及KL正则化的Plackett - Luce排序。

Result: 在十个不同基准测试中，AutoTool用更少参数始终优于先进的LLM智能体和工具集成方法，在多个领域有平均增益，推理时能利用未见工具。

Conclusion: AutoTool框架有效，能提升LLM智能体在工具选择上的适应能力和推理表现，具备较强泛化能力。

Abstract: Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.

</details>


### [468] [Textual Gradients are a Flawed Metaphor for Automatic Prompt Optimization](https://arxiv.org/abs/2512.13598)
*Daniel Melcer,Qi Chen,Wen-Hao Chiang,Shweta Garg,Pranav Garg,Christian Bock*

Main category: cs.CL

TL;DR: 研究文本梯度方法在大语言模型提示优化中的表现，指出梯度类比不能准确解释其行为。


<details>
  <summary>Details</summary>
Motivation: 大语言模型可用自动提示优化技术提升性能，文本梯度法是一类领先技术，需探究其行为。

Method: 通过一系列实验和案例研究来调查文本梯度方法的行为。

Result: 这类方法常能提升性能，但实验表明梯度类比不能准确解释其行为。

Conclusion: 研究结果可指导提示优化策略选择和新方法开发。

Abstract: A well-engineered prompt can increase the performance of large language models; automatic prompt optimization techniques aim to increase performance without requiring human effort to tune the prompts. One leading class of prompt optimization techniques introduces the analogy of textual gradients. We investigate the behavior of these textual gradient methods through a series of experiments and case studies. While such methods often result in a performance improvement, our experiments suggest that the gradient analogy does not accurately explain their behavior. Our insights may inform the selection of prompt optimization strategies, and development of new approaches.

</details>


### [469] [Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models](https://arxiv.org/abs/2512.13618)
*Zefang Liu,Nam Nguyen,Yinzhu Quan,Austin Zhang*

Main category: cs.CL

TL;DR: 本文对事件序列的时间标记化进行实证研究，比较不同编码策略，发现无通用最优策略，性能取决于与数据统计特性的匹配。


<details>
  <summary>Details</summary>
Motivation: 在使用大语言模型建模时间事件序列时，连续时间表示是关键且未充分探索的挑战，目前最佳方法不明确。

Method: 比较朴素数字字符串、高精度字节级表示、人类语义日历标记、经典均匀分箱和自适应残差标量量化等不同编码策略，并在体现不同分布的真实数据集上微调大语言模型进行评估。

Result: 没有单一策略是普遍优越的，预测性能很大程度上取决于标记器与数据统计特性的匹配。

Conclusion: 基于对数的策略在偏态分布上表现出色，以人类为中心的格式在混合模态数据上表现稳健。

Abstract: Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [470] [A Multitask VAE for Time Series Preprocessing and Prediction of Blood Glucose Level](https://arxiv.org/abs/2410.00015)
*Ali AbuSaleh,Mehdi Rahim*

Main category: eess.SP

TL;DR: 提出新深度学习模型减轻时间序列数据预处理假设，用VAE和循环VAE处理医疗设备数据，在预测糖尿病患者血糖水平上效果更佳。


<details>
  <summary>Details</summary>
Motivation: 医疗设备数据采集有缺失或异常值，处理需额外假设和领域知识，耗时且影响预测模型准确性和医学解释。

Method: 提出新深度学习模型，用变分自编码器（VAE）产生预处理潜在空间，用循环VAE保留数据时间动态。

Result: 在远程监测数据上验证，相比现有方法和架构，预测准确性有提升。

Conclusion: 新模型可有效减轻预处理假设，提升预测准确性。

Abstract: Data preprocessing is a critical part of time series data analysis. Data from connected medical devices often have missing or abnormal values during acquisition. Handling such situations requires additional assumptions and domain knowledge. This can be time-consuming, and can introduce a significant bias affecting predictive model accuracy and thus, medical interpretation. To overcome this issue, we propose a new deep learning model to mitigate the preprocessing assumptions. The model architecture relies on a variational auto-encoder (VAE) to produce a preprocessing latent space, and a recurrent VAE to preserve the temporal dynamics of the data. We demonstrate the effectiveness of such an architecture on telemonitoring data to forecast glucose-level of diabetic patients. Our results show an improvement in terms of accuracy with respect of existing state-of-the-art methods and architectures.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [471] [HaShiFlex: A High-Throughput Hardened Shifter DNN Accelerator with Fine-Tuning Flexibility](https://arxiv.org/abs/2512.12847)
*Jonathan Herbst,Michael Pellauer,Sherief Reda*

Main category: cs.AR

TL;DR: 介绍了一种高通量神经网络加速器，通过硬件嵌入层、Po2 量化等方式实现高效处理，在 MobileNetV2 上测试，大幅提升推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 设计一种高吞吐量、节能且有一定灵活性的神经网络加速器，适用于模型参数稳定的应用场景。

Method: 将多数网络层直接嵌入硬件，用 Po2 量化权重以简化乘法为加法，设置可重编程最终层，在 7nm ASIC 流程中用 MobileNetV2 实现。

Result: 在 MobileNetV2 上，相比全可编程 GPU，有微调时推理吞吐量提高 20 倍，每秒处理 121 万张图像；无微调时提高 67 倍，每秒处理 400 万张图像。

Conclusion: 所提出的加速器架构在提升推理吞吐量上有优势，同时展示了其优点和潜在权衡。

Abstract: We introduce a high-throughput neural network accelerator that embeds most network layers directly in hardware, minimizing data transfer and memory usage while preserving a degree of flexibility via a small neural processing unit for the final classification layer. By leveraging power-of-two (Po2) quantization for weights, we replace multiplications with simple rewiring, effectively reducing each convolution to a series of additions. This streamlined approach offers high-throughput, energy-efficient processing, making it highly suitable for applications where model parameters remain stable, such as continuous sensing tasks at the edge or large-scale data center deployments. Furthermore, by including a strategically chosen reprogrammable final layer, our design achieves high throughput without sacrificing fine-tuning capabilities. We implement this accelerator in a 7nm ASIC flow using MobileNetV2 as a baseline and report throughput, area, accuracy, and sensitivity to quantization and pruning - demonstrating both the advantages and potential trade-offs of the proposed architecture. We find that for MobileNetV2, we can improve inference throughput by 20x over fully programmable GPUs, processing 1.21 million images per second through a full forward pass while retaining fine-tuning flexibility. If absolutely no post-deployment fine tuning is required, this advantage increases to 67x at 4 million images per second.

</details>


### [472] [KANELÉ: Kolmogorov-Arnold Networks for Efficient LUT-based Evaluation](https://arxiv.org/abs/2512.12850)
*Duc Hoang,Aarush Gupta,Philip Harris*

Main category: cs.AR

TL;DR: 介绍KANELÉ框架用于FPGA部署KAN网络，有设计流程并优化，结果显示有提速和资源节省，还可用于控制系统。


<details>
  <summary>Details</summary>
Motivation: 实现FPGA上低延迟、资源高效的神经网络推理，传统LUT方法有局限，KAN网络有独特优势。

Method: 提出首个在FPGA上实现KAN的系统设计流程，联合优化训练、量化和剪枝。

Result: 相比之前KAN-on-FPGA方法有2700倍提速和大量资源节省，在基准测试中表现好。

Conclusion: KANELÉ框架有效，能实现低延迟、资源高效推理，还可用于实时节能控制系统。

Abstract: Low-latency, resource-efficient neural network inference on FPGAs is essential for applications demanding real-time capability and low power. Lookup table (LUT)-based neural networks are a common solution, combining strong representational power with efficient FPGA implementation. In this work, we introduce KANELÉ, a framework that exploits the unique properties of Kolmogorov-Arnold Networks (KANs) for FPGA deployment. Unlike traditional multilayer perceptrons (MLPs), KANs employ learnable one-dimensional splines with fixed domains as edge activations, a structure naturally suited to discretization and efficient LUT mapping. We present the first systematic design flow for implementing KANs on FPGAs, co-optimizing training with quantization and pruning to enable compact, high-throughput, and low-latency KAN architectures. Our results demonstrate up to a 2700x speedup and orders of magnitude resource savings compared to prior KAN-on-FPGA approaches. Moreover, KANELÉ matches or surpasses other LUT-based architectures on widely used benchmarks, particularly for tasks involving symbolic or physical formulas, while balancing resource usage across FPGA hardware. Finally, we showcase the versatility of the framework by extending it to real-time, power-efficient control systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [473] [AI-Generated Compromises for Coalition Formation](https://arxiv.org/abs/2506.06837)
*Eyal Briman,Ehud Shapiro,Nimrod Talmon*

Main category: cs.MA

TL;DR: 本文聚焦在协作文档编写领域寻找妥协提案的问题，结合自然语言处理和大语言模型，设计算法生成妥协点并通过模拟评估，证明AI可助力大规模民主文本编辑。


<details>
  <summary>Details</summary>
Motivation: 现有联盟形成过程中有效寻找妥协提案的问题未解决，作者希望填补该空白。

Method: 形式化一个包含代理有限理性和不确定性的模型，利用自然语言处理技术和大语言模型构建文本语义度量空间，基于此设计算法。

Result: 通过模拟联盟形成过程，表明AI能够促进大规模民主文本编辑。

Conclusion: AI方法可在传统工具受限的大规模民主文本编辑领域发挥作用。

Abstract: The challenge of finding compromises between agent proposals is fundamental to AI subfields such as argumentation, mediation, and negotiation. Building on this tradition, Elkind et al. (2021) introduced a process for coalition formation that seeks majority-supported proposals preferable to the status quo, using a metric space where each agent has an ideal point. A crucial step in this process involves identifying compromise proposals around which agent coalitions can unite. How to effectively find such compromise proposals remains an open question. We address this gap by formalizing a model that incorporates agent bounded rationality and uncertainty, and by developing AI methods to generate compromise proposals. We focus on the domain of collaborative document writing, such as the democratic drafting of a community constitution. Our approach uses natural language processing techniques and large language models to induce a semantic metric space over text. Based on this space, we design algorithms to suggest compromise points likely to receive broad support. To evaluate our methods, we simulate coalition formation processes and show that AI can facilitate large-scale democratic text editing, a domain where traditional tools are limited.

</details>


### [474] [How AI Agents Follow the Herd of AI? Network Effects, History, and Machine Optimism](https://arxiv.org/abs/2512.11943)
*Yu Liu,Wenwen Li,Yifan Dou,Guangnan Ye*

Main category: cs.MA

TL;DR: 研究AI代理在网络效应游戏中的决策，引入新工作流，发现历史数据对LLM推理有影响，AI系统均衡结果受历史数据编排影响。


<details>
  <summary>Details</summary>
Motivation: 理解多AI代理框架中的决策对分析网络效应驱动情境下的战略互动至关重要，且该情境在多智能体系统中研究不足。

Method: 引入基于大语言模型（LLM）的代理在重复决策场景中的新工作流，系统地操纵价格轨迹和网络效应强度。

Result: 无历史数据时代理无法推断均衡；有序历史序列在弱网络效应下可部分收敛，强效应引发“AI乐观主义”；随机历史完全破坏收敛。

Conclusion: 在AI介导系统中，均衡结果不仅取决于激励，还取决于历史数据的编排，这是人类无法做到的。

Abstract: Understanding decision-making in multi-AI-agent frameworks is crucial for analyzing strategic interactions in network-effect-driven contexts. This study investigates how AI agents navigate network-effect games, where individual payoffs depend on peer participatio--a context underexplored in multi-agent systems despite its real-world prevalence. We introduce a novel workflow design using large language model (LLM)-based agents in repeated decision-making scenarios, systematically manipulating price trajectories (fixed, ascending, descending, random) and network-effect strength. Our key findings include: First, without historical data, agents fail to infer equilibrium. Second, ordered historical sequences (e.g., escalating prices) enable partial convergence under weak network effects but strong effects trigger persistent "AI optimism"--agents overestimate participation despite contradictory evidence. Third, randomized history disrupts convergence entirely, demonstrating that temporal coherence in data shapes LLMs' reasoning, unlike humans. These results highlight a paradigm shift: in AI-mediated systems, equilibrium outcomes depend not just on incentives, but on how history is curated, which is impossible for human.

</details>


### [475] [Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems](https://arxiv.org/abs/2512.12791)
*Sreemaee Akshathala,Bassam Adnan,Mahisha Ramesh,Karthik Vaidhyanathan,Basil Muhammed,Kannan Parthasarathy*

Main category: cs.MA

TL;DR: 研究聚焦大语言模型（LLMs）多智能体系统评估难题，提出含四评估支柱的端到端评估框架，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统虽发展良好，但对基于LLMs的智能体和多智能体系统的评估仍是挑战，现有评估方法忽视模型非确定性，需新评估维度。

Method: 提出包含LLMs、Memory、Tools和Environment四个评估支柱的端到端Agent Assessment Framework。

Result: 在代表性的Autonomous CloudOps用例上验证框架，实验显示该框架能发现传统指标忽略的行为偏差。

Conclusion: 所提框架能有效捕捉运行时不确定性，可用于评估智能体系统。

Abstract: Recent advances in agentic AI have shifted the focus from standalone Large Language Models (LLMs) to integrated systems that combine LLMs with tools, memory, and other agents to perform complex tasks. These multi-agent architectures enable coordinated reasoning, planning, and execution across diverse domains, allowing agents to collaboratively automate complex workflows. Despite these advances, evaluation and assessment of LLM agents and the multi-agent systems they constitute remain a fundamental challenge. Although various approaches have been proposed in the software engineering literature for evaluating conventional software components, existing methods for AI-based systems often overlook the non-deterministic nature of models. This non-determinism introduces behavioral uncertainty during execution, yet existing evaluations rely on binary task completion metrics that fail to capture it. Evaluating agentic systems therefore requires examining additional dimensions, including the agent ability to invoke tools, ingest and retrieve memory, collaborate with other agents, and interact effectively with its environment. We propose an end-to-end Agent Assessment Framework with four evaluation pillars encompassing LLMs, Memory, Tools, and Environment. We validate the framework on a representative Autonomous CloudOps use case, where experiments reveal behavioral deviations overlooked by conventional metrics, demonstrating its effectiveness in capturing runtime uncertainties.

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [476] [Gene regulatory network inference algorithm based on spectral signed directed graph convolution](https://arxiv.org/abs/2512.11927)
*Rijie Xi,Weikang Xu,Wei Xiong,Yuannong Ye,Bin Zhao*

Main category: q-bio.MN

TL;DR: 提出MSGRNLink框架用于基因调控网络（GRN）重建，实验表明其性能优于基线模型，具鲁棒性和生物学相关性。


<details>
  <summary>Details</summary>
Motivation: 准确重建GRN对理解基因功能和疾病机制至关重要，传统谱图卷积难以对GRN进行有效表示。

Method: 提出MSGRNLink框架，将GRN显式建模为有符号有向图，并采用磁有符号拉普拉斯卷积。

Result: 在模拟和真实数据集上，MSGRNLink的AUROC优于所有基线模型；参数敏感性分析和消融研究证实其鲁棒性；在膀胱癌案例中，预测出更多已知边和边符号。

Conclusion: MSGRNLink是一种有效的GRN重建方法，具有良好的性能、鲁棒性和生物学相关性。

Abstract: Accurately reconstructing Gene Regulatory Networks (GRNs) is crucial for understanding gene functions and disease mechanisms. Single-cell RNA sequencing (scRNA-seq) technology provides vast data for computational GRN reconstruction. Since GRNs are ideally modeled as signed directed graphs to capture activation/inhibition relationships, the most intuitive and reasonable approach is to design feature extractors based on the topological structure of GRNs to extract structural features, then combine them with biological characteristics for research. However, traditional spectral graph convolution struggles with this representation. Thus, we propose MSGRNLink, a novel framework that explicitly models GRNs as signed directed graphs and employs magnetic signed Laplacian convolution. Experiments across simulated and real datasets demonstrate that MSGRNLink outperforms all baseline models in AUROC. Parameter sensitivity analysis and ablation studies confirmed its robustness and the importance of each module. In a bladder cancer case study, MSGRNLink predicted more known edges and edge signs than benchmark models, further validating its biological relevance.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [477] [Policy Gradient Algorithms for Age-of-Information Cost Minimization](https://arxiv.org/abs/2512.11990)
*José-Ramón Vidal,Vicent Pla,Luis Guijarro,Israel Leyva-Mayorga*

Main category: cs.NI

TL;DR: 本文引入两种算法优化信息更新过程，采用无模型强化学习中的策略梯度方法，适用于多种场景，结果显示算法收敛性好，成本接近最优值且优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 优化物联网设备访问策略以最大化数据新鲜度是一项具有挑战性的任务，需要寻找方法优化信息更新过程。

Method: 引入两种算法，在无模型强化学习框架下采用策略梯度方法，处理连续状态和动作空间，各算法采用不同策略决定信息更新时机，还可同时应用两种策略。

Result: 算法收敛性良好，时间平均成本可达到最优值的3%以内，与其他方法相比，在适用场景范围、成本和计算成本方面表现更优。

Conclusion: 所提出的两种算法在优化信息更新过程中有效，能降低成本，且在多方面优于现有方法。

Abstract: Recent developments in cyber-physical systems have increased the importance of maximizing the freshness of the information about the physical environment. However, optimizing the access policies of Internet of Things devices to maximize the data freshness, measured as a function of the Age-of-Information (AoI) metric, is a challenging task. This work introduces two algorithms to optimize the information update process in cyber-physical systems operating under the generate-at-will model, by finding an online policy without knowing the characteristics of the transmission delay or the age cost function. The optimization seeks to minimize the time-average cost, which integrates the AoI at the receiver and the data transmission cost, making the approach suitable for a broad range of scenarios. Both algorithms employ policy gradient methods within the framework of model-free reinforcement learning (RL) and are specifically designed to handle continuous state and action spaces. Each algorithm minimizes the cost using a distinct strategy for deciding when to send an information update. Moreover, we demonstrate that it is feasible to apply the two strategies simultaneously, leading to an additional reduction in cost. The results demonstrate that the proposed algorithms exhibit good convergence properties and achieve a time-average cost within 3% of the optimal value, when the latter is computable. A comparison with other state-of-the-art methods shows that the proposed algorithms outperform them in one or more of the following aspects: being applicable to a broader range of scenarios, achieving a lower time-average cost, and requiring a computational cost at least one order of magnitude lower.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [478] [WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2512.11872)
*Mingwang Xu,Jiahao Cui,Feipeng Cai,Hanlin Shang,Zhihao Zhu,Shan Luan,Yifang Xu,Neng Zhang,Yaoyi Li,Jia Cai,Siyu Zhu*

Main category: cs.RO

TL;DR: 本文提出WAM - Diff框架，用掩码扩散迭代优化离散序列来生成未来自我轨迹，在相关测试中表现良好，为自动驾驶提供新策略。


<details>
  <summary>Details</summary>
Motivation: 当前自回归大语言模型和连续扩散策略盛行，但离散掩码扩散在轨迹生成方面潜力待挖掘。

Method: 采用掩码扩散迭代优化代表未来自我轨迹的离散序列，有系统适配掩码扩散、用稀疏MoE架构扩展模型容量、用GSPO进行在线强化学习三个创新点。

Result: 模型在NAVSIM - v1上达到91.0 PDMS，在NAVSIM - v2上达到89.7 EPDMS。

Conclusion: 该方法为自回归和基于扩散的策略提供了有前景的替代方案，支持用于轨迹生成的场景感知解码策略。

Abstract: End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff

</details>


### [479] [Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control](https://arxiv.org/abs/2512.11921)
*Abdullah Yahya Abdullah Omaisan,Ibrahim Sheikh Mohamed*

Main category: cs.RO

TL;DR: 提出将VLA模型适配到低成本机器人操纵系统的微调方法和部署分析，以解决计算约束和适配新机器人实体的问题。


<details>
  <summary>Details</summary>
Motivation: 大规模VLA模型受计算约束且难以适配新机器人实体，在低成本机器人平台部署存在挑战。

Method: 采用低秩自适应（LoRA）和量化技术，研究冻结与非冻结视觉编码器的权衡。

Result: 在SO101机械臂按按钮任务中证明模型有效且具计算效率；展示正确微调下VLA可在低成本平台部署。

Conclusion: 通过适当微调，VLA模型可成功部署在低成本机器人平台，使先进操作能力更普及。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual observations.However, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation systems.We propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.

</details>


### [480] [A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach](https://arxiv.org/abs/2512.11944)
*Jia Hu,Yang Chang,Haoran Wang*

Main category: cs.RO

TL;DR: 本文综述自动驾驶运动规划领域发展，指出困境，提出数据驱动最优控制范式并探讨其能力，给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决管道方法与学习系统在自动驾驶运动规划中的困境，开发真正可信的系统。

Method: 对学习型运动规划方法进行全面综述，提出数据驱动最优控制范式，融合经典控制与机器学习。

Result: 探索该范式具备“以人为中心”定制、“平台自适应”动力学适应和“系统自优化”能力。

Conclusion: 基于该范式提出未来研究方向，以开发安全、可解释且具类人自主性的智能交通系统。

Abstract: Motion planning for high-level autonomous driving is constrained by a fundamental trade-off between the transparent, yet brittle, nature of pipeline methods and the adaptive, yet opaque, "black-box" characteristics of modern learning-based systems. This paper critically synthesizes the evolution of the field -- from pipeline methods through imitation learning, reinforcement learning, and generative AI -- to demonstrate how this persistent dilemma has hindered the development of truly trustworthy systems. To resolve this impasse, we conduct a comprehensive review of learning-based motion planning methods. Based on this review, we outline a data-driven optimal control paradigm as a unifying framework that synergistically integrates the verifiable structure of classical control with the adaptive capacity of machine learning, leveraging real-world data to continuously refine key components such as system dynamics, cost functions, and safety constraints. We explore this framework's potential to enable three critical next-generation capabilities: "Human-Centric" customization, "Platform-Adaptive" dynamics adaptation, and "System Self-Optimization" via self-tuning. We conclude by proposing future research directions based on this paradigm, aimed at developing intelligent transportation systems that are simultaneously safe, interpretable, and capable of human-like autonomy.

</details>


### [481] [Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving](https://arxiv.org/abs/2512.12211)
*Longchao Da,David Isele,Hua Wei,Manish Saroya*

Main category: cs.RO

TL;DR: 提出一种综合评估轨迹预测器性能的管道，在闭环基准测试中表现优于传统指标。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹预测评估方法依赖基于误差的指标，忽略对自动驾驶车辆的实际影响，特别是在复杂交互场景中。

Method: 提出一个综合管道，从准确性和多样性两个维度自适应评估预测器性能，根据驾驶场景的关键程度动态组合这两个维度得出最终分数。

Result: 在使用真实世界数据集的闭环基准测试中，该管道比传统指标能更合理地评估，更好反映预测器评估与自动驾驶车辆驾驶性能的相关性。

Conclusion: 该评估管道是选择对自动驾驶车辆驾驶性能最有贡献的预测器的可靠方法。

Abstract: Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.

</details>


### [482] [SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding](https://arxiv.org/abs/2512.12842)
*Kuan Fang,Yuxin Chen,Xinghao Zhu,Farzad Niroui,Lingfeng Sun,Jiuguang Wang*

Main category: cs.RO

TL;DR: 提出SAGA框架用于视觉运动控制，能跨环境、任务和用户规格泛化，在真实任务中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 开发一个通用且自适应的视觉运动控制框架，使其能在不同环境、任务目标和用户规格下实现泛化。

Method: 将高级语义意图与低级视觉运动控制解耦，使用基于功能的任务表示，利用多模态基础模型构建3D功能热图，在多任务演示数据上训练条件策略。

Result: 在四足机械手上对11个真实世界任务进行实验，SAGA大幅优于端到端和模块化基线方法。

Conclusion: 结构化功能接地为通用移动操作提供了可扩展且有效的途径。

Abstract: We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.

</details>


### [483] [Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy](https://arxiv.org/abs/2512.12230)
*Jonathan Spraggett*

Main category: cs.RO

TL;DR: 本文提出单一DRL策略可让七种不同人形机器人从跌倒中恢复，零样本迁移效果好，证明形态无关控制的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于DRL的跌倒恢复方法需为每个机器人形态训练单独策略，本文旨在提出单一策略实现多种机器人的跌倒恢复。

Method: 使用CrossQ训练统一策略，进行综合留一法实验、形态缩放分析和多样性消融实验。

Result: 统一策略在未见形态上零样本迁移率达86 +/- 7% (95% CI [81, 89])，在某些情况下超过专家基线。

Conclusion: 形态无关控制用于跌倒恢复具有实用性，为通用人形机器人控制奠定基础。

Abstract: Fall recovery is a critical skill for humanoid robots in dynamic environments such as RoboCup, where prolonged downtime often decides the match. Recent techniques using deep reinforcement learning (DRL) have produced robust get-up behaviors, yet existing methods require training of separate policies for each robot morphology. This paper presents a single DRL policy capable of recovering from falls across seven humanoid robots with diverse heights (0.48 - 0.81 m), weights (2.8 - 7.9 kg), and dynamics. Trained with CrossQ, the unified policy transfers zero-shot up to 86 +/- 7% (95% CI [81, 89]) on unseen morphologies, eliminating the need for robot-specific training. Comprehensive leave-one-out experiments, morph scaling analysis, and diversity ablations show that targeted morphological coverage improves zero-shot generalization. In some cases, the shared policy even surpasses the specialist baselines. These findings illustrate the practicality of morphology-agnostic control for fall recovery, laying the foundation for generalist humanoid control. The software is open-source and available at: https://github.com/utra-robosoccer/unified-humanoid-getup

</details>


### [484] [Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning](https://arxiv.org/abs/2512.12987)
*Amin Jalal Aghdasian,Farzaneh Abdollahi,Ali Kamali Iglie*

Main category: cs.RO

TL;DR: 本文提出两种用于雪地路况下自动驾驶车辆车道保持系统的算法，经模拟和真实实验验证有效，AR - CADPG 表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶车辆在雪地路况下车道保持系统应对不确定性和打滑问题。

Method: 提出 Action - Robust Recurrent Deep Deterministic Policy Gradient (AR - RDPG) 和 end - to - end Action - Robust convolutional neural network Attention Deterministic Policy Gradient (AR - CADPG) 两种基于深度强化学习的算法，分别采用不同的数据处理和网络架构，先在 CARLA 模拟器训练，再进行真实实验。

Result: 两种算法经真实实验验证了学习策略的可行性和稳定性，AR - CADPG 有更优的路径跟踪准确性和鲁棒性。

Conclusion: 结合时间记忆、对抗恢复能力和注意力机制在自动驾驶车辆中是有效的。

Abstract: This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.

</details>


### [485] [Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation](https://arxiv.org/abs/2512.13094)
*Xiang Li,Gang Liu,Weitao Zhou,Hongyi Zhu,Zhong Cao*

Main category: cs.RO

TL;DR: 提出Sequence of Experts (SoE)方法提升模仿学习在自动驾驶闭环中的性能，在nuPlan基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在闭环自动驾驶中因误差累积性能下降，当前研究多关注单时间点状态鲁棒性，而自动驾驶是连续过程，需从时间尺度增强鲁棒性。

Method: 提出Sequence of Experts (SoE)，一种时间交替策略，不增加模型大小和数据需求提升闭环性能。

Result: 在大规模自动驾驶基准测试nuPlan上，SoE方法持续显著提升所有评估模型的性能，达到了最先进水平。

Conclusion: 该模块可为提高自动驾驶模型的训练效率提供关键且广泛适用的支持。

Abstract: Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.

</details>


### [486] [OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning](https://arxiv.org/abs/2512.13100)
*Guanhua Ji,Harsha Polavaram,Lawrence Yunliang Chen,Sandeep Bajamahal,Zehan Ma,Simeon Adebola,Chenfeng Xu,Ken Goldberg*

Main category: cs.RO

TL;DR: 提出AugE-Toolkit和数据集OXE-AugE，通过机器人数据增强提升通用策略性能，在物理实验中验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 通用机器人策略训练需大量多样数据，重新收集和训练成本高，且现有数据集OXE高度不平衡，有过拟合风险。

Method: 提出AugE-Toolkit数据增强管道，生成包含9种不同机器人的数据集OXE-AugE，并系统研究数据增强对跨实体学习的影响。

Result: OXE-AugE提供超440万条轨迹，是原数据集3倍多；数据增强不仅提升增强机器人策略性能，对未见过机器人和分布偏移下的原机器人也有提升；在物理实验中，微调策略使成功率提高24 - 45%。

Conclusion: 用不同机械臂和夹具增强数据集可提升策略性能，在真实操作任务中有效。

Abstract: Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\% of its real data, which risks overfitting to robot-scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $π_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot-gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.

</details>


### [487] [Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration](https://arxiv.org/abs/2512.13293)
*Hao Fua,Wei Liu,Shuai Zhoua*

Main category: cs.RO

TL;DR: 本文研究强化学习在多机器人社交编队导航中的应用，提出新的协调探索多机器人强化学习算法，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决行人行为的不可预测性和不合作动态对多机器人协调探索效率的挑战，实现无缝的人机共存。

Method: 提出引入内在动机探索的协调探索多机器人强化学习算法，包含自学习内在奖励机制，在集中训练和分散执行框架中采用双采样模式，利用双时间尺度更新规则解耦参数更新。

Result: 在社交编队导航基准测试中，所提算法在关键指标上优于现有最先进方法。

Conclusion: 所提出的算法在多机器人社交编队导航中具有优越性，代码和视频演示可在指定链接获取。

Abstract: This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.

</details>


### [488] [Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)](https://arxiv.org/abs/2512.13356)
*Zeyad Gamal,Youssef Mahran,Ayman El-Badawy*

Main category: cs.RO

TL;DR: 本文提出用强化学习框架控制双旋翼气动系统，用TD3算法训练智能体，经仿真、抗扰测试和实验室实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 双旋翼气动系统复杂动态和非线性特征使传统控制算法难以控制，强化学习在多旋翼控制的潜在应用引发关注。

Method: 使用Twin Delayed Deep Deterministic Policy Gradient (TD3)算法训练强化学习智能体，该算法适用于连续状态和动作空间的环境，无需系统模型。

Result: 仿真结果显示了强化学习控制方法的有效性，与传统PID控制器对比测试了抗风扰能力，实验室实验证实了控制器在实际应用中的有效性。

Conclusion: 所提出的强化学习框架可有效控制和稳定双旋翼气动系统，能跟踪给定轨迹，在实际应用中具有有效性。

Abstract: This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.

</details>


### [489] [World Models Can Leverage Human Videos for Dexterous Manipulation](https://arxiv.org/abs/2512.13644)
*Raktim Gautam Goswami,Amir Bar,David Fan,Tsung-Yen Yang,Gaoyue Zhou,Prashanth Krishnamurthy,Michael Rabbat,Farshad Khorrami,Yann LeCun*

Main category: cs.RO

TL;DR: 提出DexWM灵巧操作世界模型，在大量视频上训练，引入辅助损失，预测表现优于先前模型，在零样本泛化任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作具有挑战性，需要理解手部细微动作如何通过与物体接触影响环境。

Method: 引入DexWM模型，在900多小时人类和非灵巧机器人视频上训练，引入辅助手一致性损失。

Result: DexWM在预测未来状态上优于先前基于文本、导航和全身动作的世界模型，在零样本泛化任务中平均比Diffusion Policy表现好超50%。

Conclusion: DexWM模型能更准确预测未来状态，在零样本泛化任务中有良好表现。

Abstract: Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.

</details>


### [490] [PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations](https://arxiv.org/abs/2512.13093)
*Mingqi Yuan,Tao Yu,Haolin Song,Bo Li,Xin Jin,Hua Chen,Wenjun Zeng*

Main category: cs.RO

TL;DR: 提出PvP框架解决强化学习在人形机器人全身控制中样本效率低的问题，开发SRL4Humanoid框架，实验表明PvP提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在人形机器人全身控制中样本效率低，因机器人复杂动力学和部分可观测性。

Method: 提出PvP框架，利用本体感受和特权状态的互补性学习潜在表征；开发SRL4Humanoid框架用于系统评估。

Result: 在LimX Oli机器人的速度跟踪和运动模仿任务中，PvP相比基线SRL方法显著提高样本效率和最终性能。

Conclusion: 研究为将状态表示学习与强化学习集成用于人形机器人全身控制提供实践见解和指导。

Abstract: Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.

</details>


### [491] [Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks](https://arxiv.org/abs/2512.13170)
*Deepak Ingole,Valentin Bhend,Shiva Ganesh Murali,Oliver Dobrich,Alisa Rupenayan*

Main category: cs.RO

TL;DR: 提出基于任务级性能反馈自动调整非线性模型预测控制（NMPC）权重矩阵的迭代学习框架，经仿真验证有效。


<details>
  <summary>Details</summary>
Motivation: 制造过程受环境漂移和系统磨损影响，需要对控制进行重新调整，即便在重复操作下也是如此，因此要找到自动调整NMPC权重矩阵的方法。

Method: 受范数最优迭代学习控制（ILC）启发，通过构建经验灵敏度矩阵，在任务重复中自适应调整NMPC权重Q和R，无需解析导数进行结构化权重更新。

Result: 在UR10e机器人碳纤维缠绕仿真中，所提方法4次在线重复后跟踪性能接近最优（RMSE比离线贝叶斯优化（BO）低0.3%），而BO算法需100次离线评估。

Conclusion: 该方法为重复机器人任务中的自适应NMPC调整提供了实用解决方案，兼具精心优化控制器的精度和在线自适应的灵活性。

Abstract: Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.

</details>


### [492] [Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles](https://arxiv.org/abs/2512.13359)
*Sümer Tunçay,Alain Andres,Ignacio Carlucho*

Main category: cs.RO

TL;DR: 本文介绍GPU加速的强化学习训练管道，实现水下航行器6自由度位置控制，训练时间短，实验验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 传统控制器在非标称条件下性能下降，强化学习训练慢且迁移有挑战，需有效方法实现AUV可靠6自由度位置控制。

Method: 构建基于JAX和MuJoCo - XLA的GPU加速强化学习训练管道，联合即时编译大规模并行物理模拟和学习更新。

Result: 训练时间不到两分钟，多个强化学习算法经系统评估，在真实水下实验展现出稳健的6自由度轨迹跟踪和有效干扰抑制，策略零样本迁移成功。

Conclusion: 首次明确在现实中展示基于强化学习的AUV全6自由度位置控制。

Abstract: Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [493] [Semantic search for 100M+ galaxy images using AI-generated captions](https://arxiv.org/abs/2512.11982)
*Nolan Koblischke,Liam Parker,Francois Lanusse,Irina Espejo Morales,Jo Bovy,Shirley Ho*

Main category: astro-ph.IM

TL;DR: 本文开发了从无标签图像数据创建语义搜索引擎的管道，AION - Search模型实现了对1.4亿张星系图像的灵活语义搜索。


<details>
  <summary>Details</summary>
Motivation: 手动标记活动限制了对数十亿星系图像的探索能力，需要从无标签图像创建语义搜索引擎。

Method: 利用视觉 - 语言模型生成星系图像描述，将预训练的多模态天文学基础模型与嵌入描述进行对比对齐以产生可搜索嵌入。还引入基于VLM的重排序方法。

Result: 当前VLM提供的描述足以训练出优于直接图像相似性搜索的语义搜索模型，AION - Search在寻找罕见现象上实现零样本先进性能，重排序方法将近使最具挑战性目标的前100结果召回率翻倍。

Conclusion: 实现对1.4亿张星系图像的灵活语义搜索，为使大型无标签科学图像档案可语义搜索提供了方法，扩展了多领域数据探索能力。代码等公开。

Abstract: Finding scientifically interesting phenomena through slow, manual labeling campaigns severely limits our ability to explore the billions of galaxy images produced by telescopes. In this work, we develop a pipeline to create a semantic search engine from completely unlabeled image data. Our method leverages Vision-Language Models (VLMs) to generate descriptions for galaxy images, then contrastively aligns a pre-trained multimodal astronomy foundation model with these embedded descriptions to produce searchable embeddings at scale. We find that current VLMs provide descriptions that are sufficiently informative to train a semantic search model that outperforms direct image similarity search. Our model, AION-Search, achieves state-of-the-art zero-shot performance on finding rare phenomena despite training on randomly selected images with no deliberate curation for rare cases. Furthermore, we introduce a VLM-based re-ranking method that nearly doubles the recall for our most challenging targets in the top-100 results. For the first time, AION-Search enables flexible semantic search scalable to 140 million galaxy images, enabling discovery from previously infeasible searches. More broadly, our work provides an approach for making large, unlabeled scientific image archives semantically searchable, expanding data exploration capabilities in fields from Earth observation to microscopy. The code, data, and app are publicly available at https://github.com/NolanKoblischke/AION-Search

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [494] [MVP-ORAM: a Wait-free Concurrent ORAM for Confidential BFT Storage](https://arxiv.org/abs/2512.12006)
*Robin Vassantlal,Hasan Heydari,Bernardo Ferreira,Alysson Bessani*

Main category: cs.CR

TL;DR: 介绍了首个支持并发易故障客户端的无等待ORAM协议MVP - ORAM，可无缝集成到现有BFT数据存储，实现BFT ORAM构造，在现代云环境有较高处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有ORAM协议在支持并发客户端和拜占庭容错方面受限，需要一种新的协议。

Method: 避免使用信任代理和基于客户端间通信或分布式锁的并发控制机制，让客户端并发请求并合并冲突更新，定义依赖应用负载和并发客户端数量的弱隐私概念。

Result: 实现的MVP - ORAM原型能在现代云环境中每秒处理数百个4KB访问。

Conclusion: MVP - ORAM可支持并发易故障客户端，满足无等待性，能安全用于实际场景，可与现有BFT数据存储集成。

Abstract: It is well known that encryption alone is not enough to protect data privacy. Access patterns, revealed when operations are performed, can also be leveraged in inference attacks. Oblivious RAM (ORAM) hides access patterns by making client requests oblivious. However, existing protocols are still limited in supporting concurrent clients and Byzantine fault tolerance (BFT). We present MVP-ORAM, the first wait-free ORAM protocol that supports concurrent fail-prone clients. In contrast to previous works, MVP-ORAM avoids using trusted proxies, which require additional security assumptions, and concurrency control mechanisms based on inter-client communication or distributed locks, which limit overall throughput and the capability of tolerating faulty clients. Instead, MVP-ORAM enables clients to perform concurrent requests and merge conflicting updates as they happen, satisfying wait-freedom, i.e., clients make progress independently of the performance or failures of other clients. Since wait and collision freedom are fundamentally contradictory goals that cannot be achieved simultaneously in an asynchronous concurrent ORAM service, we define a weaker notion of obliviousness that depends on the application workload and number of concurrent clients, and prove MVP-ORAM is secure in practical scenarios where clients perform skewed block accesses. By being wait-free, MVP-ORAM can be seamlessly integrated into existing confidential BFT data stores, creating the first BFT ORAM construction. We implement MVP-ORAM on top of a confidential BFT data store and show our prototype can process hundreds of 4KB accesses per second in modern clouds.

</details>


### [495] [Towards Secure Decentralized Applications and Consensus Protocols in Blockchains (on Selfish Mining, Undercutting Attacks, DAG-Based Blockchains, E-Voting, Cryptocurrency Wallets, Secure-Logging, and CBDC)](https://arxiv.org/abs/2512.13213)
*Ivan Homoliak*

Main category: cs.CR

TL;DR: 论文总结区块链和去中心化应用安全方面贡献，包括提出安全参考架构、研究共识安全、改进钱包安全、推进电子投票、改善安全日志及提出央行数字货币互操作性协议。


<details>
  <summary>Details</summary>
Motivation: 随着加密货币兴起，已部署的区块链和去中心化应用需更好的可扩展性、性能和安全性，且因其复杂，评估安全需更全面视角。

Method: 提出安全参考架构；研究单链工作量证明区块链共识安全；提出新的钱包认证方案分类和双因素方法；提出实用董事会投票协议并扩展；提出重复投票框架；利用区块链和可信计算改善安全日志，提出央行数字货币互操作性协议。

Result: 完成多方面安全相关研究与改进，如共识安全研究、钱包认证方案创新、电子投票协议优化、安全日志改善、央行数字货币互操作性协议提出。

Conclusion: 在区块链和去中心化应用安全多个领域取得成果，为该领域发展提供支持。

Abstract: With the rise of cryptocurrencies, many new applications built on decentralized blockchains have emerged. Blockchains are full-stack distributed systems where multiple sub-systems interact. While many deployed blockchains and decentralized applications need better scalability and performance, security is also critical. Due to their complexity, assessing blockchain and DAPP security requires a more holistic view than for traditional distributed or centralized systems.
  In this thesis, we summarize our contributions to blockchain and decentralized application security. We propose a security reference architecture to support standardized vulnerability and threat analysis. We study consensus security in single-chain Proof-of-Work blockchains, including resistance to selfish mining, undercutting, and greedy transaction selection, as well as related issues in DAG-based systems. We contribute to wallet security with a new classification of authentication schemes and a two-factor method based on One-Time Passwords. We advance e-voting with a practical boardroom voting protocol, extend it to a scalable version for millions of participants while preserving security and privacy, and introduce a repetitive voting framework that enables vote changes between elections while avoiding peak-end effects. Finally, we improve secure logging using blockchains and trusted computing through a centralized ledger that guarantees non-equivocation, integrity, and censorship evidence, then build on it to propose an interoperability protocol for central bank digital currencies that ensures atomic transfers.

</details>


### [496] [SEDULity: A Proof-of-Learning Framework for Distributed and Secure Blockchains with Efficient Useful Work](https://arxiv.org/abs/2512.13666)
*Weihang Cao,Mustafa Doger,Sennur Ulukus*

Main category: cs.CR

TL;DR: 提出名为SEDULity的PoL框架，可高效训练ML模型并维护区块链安全，框架分布式且安全，还可扩展及设计激励机制，有理论分析和模拟结果验证。


<details>
  <summary>Details</summary>
Motivation: PoW能耗大，以往PoL存在安全、去中心化或效率问题，需新的PoL框架。

Method: 将模板块编码到训练过程，设计难求解但易验证的有用函数替代PoW难题，设计激励机制。

Result: 框架分布式、安全且能高效训练ML模型，可扩展到其他有用工作，理论证明合理矿工受激励诚实训练，有模拟结果验证性能。

Conclusion: 提出的SEDULity框架有效可行，能解决现有问题。

Abstract: The security and decentralization of Proof-of-Work (PoW) have been well-tested in existing blockchain systems. However, its tremendous energy waste has raised concerns about sustainability. Proof-of-Useful-Work (PoUW) aims to redirect the meaningless computation to meaningful tasks such as solving machine learning (ML) problems, giving rise to the branch of Proof-of-Learning (PoL). While previous studies have proposed various PoLs, they all, to some degree, suffer from security, decentralization, or efficiency issues. In this paper, we propose a PoL framework that trains ML models efficiently while maintaining blockchain security in a fully distributed manner. We name the framework SEDULity, which stands for a Secure, Efficient, Distributed, and Useful Learning-based blockchain system. Specifically, we encode the template block into the training process and design a useful function that is difficult to solve but relatively easy to verify, as a substitute for the PoW puzzle. We show that our framework is distributed, secure, and efficiently trains ML models. We further demonstrate that the proposed PoL framework can be extended to other types of useful work and design an incentive mechanism to incentivize task verification. We show theoretically that a rational miner is incentivized to train fully honestly with well-designed system parameters. Finally, we present simulation results to demonstrate the performance of our framework and validate our analysis.

</details>


### [497] [Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring](https://arxiv.org/abs/2512.12069)
*Peichun Hua,Hao Li,Shanghao Shi,Zhiyuan Yu,Ning Zhang*

Main category: cs.CR

TL;DR: 论文针对大型视觉语言模型多模态越狱攻击防御问题，提出基于模型内部表征的对比评分框架RCS，实现了最先进的越狱检测性能，并证明用简单统计方法能实现有效检测。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型易受多模态越狱攻击，现有防御策略要么泛化性差，要么计算开销大，轻量级异常检测方法存在将新良性输入误判为恶意输入的问题。

Method: 提出Representational Contrastive Scoring (RCS)框架，通过检查模型内部表征的几何结构，学习轻量级投影，最大化分离安全关键层中的良性和恶意输入，得出对比分数来区分恶意与新颖输入，具体实现有MCD和KCD。

Result: MCD和KCD在挑战性评估协议上实现了最先进的性能，能泛化到未见的攻击类型。

Conclusion: 通过对适当的内部表征应用简单、可解释的统计方法，可以实现有效的越狱检测，为更安全地部署大型视觉语言模型提供了实用途径。

Abstract: Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github https://github.com/sarendis56/Jailbreak_Detection_RCS.

</details>


### [498] [UniMark: Artificial Intelligence Generated Content Identification Toolkit](https://arxiv.org/abs/2512.12324)
*Meilin Li,Ji He,Jia Xu,Shanzhe Lei,Yan Teng,Yingchun Wang,Xuhong Wang*

Main category: cs.CR

TL;DR: 提出开源统一框架UniMark用于多模态内容治理，支持隐式水印和可见标记，建立评估框架。


<details>
  <summary>Details</summary>
Motivation: 人工智能生成内容带来信任危机和监管需求，但现有识别工具零散且缺乏可见合规标记支持。

Method: 引入模块化统一引擎处理多模态内容，提出双操作策略，建立标准化评估框架。

Result: 开发出UniMark工具包，连接了先进算法和工程实现。

Conclusion: UniMark有助于构建更透明安全的数字生态系统。

Abstract: The rapid proliferation of Artificial Intelligence Generated Content has precipitated a crisis of trust and urgent regulatory demands. However, existing identification tools suffer from fragmentation and a lack of support for visible compliance marking. To address these gaps, we introduce the \textbf{UniMark}, an open-source, unified framework for multimodal content governance. Our system features a modular unified engine that abstracts complexities across text, image, audio, and video modalities. Crucially, we propose a novel dual-operation strategy, natively supporting both \emph{Hidden Watermarking} for copyright protection and \emph{Visible Marking} for regulatory compliance. Furthermore, we establish a standardized evaluation framework with three specialized benchmarks (Image/Video/Audio-Bench) to ensure rigorous performance assessment. This toolkit bridges the gap between advanced algorithms and engineering implementation, fostering a more transparent and secure digital ecosystem.

</details>


### [499] [Mage: Cracking Elliptic Curve Cryptography with Cross-Axis Transformers](https://arxiv.org/abs/2512.12483)
*Lily Erickson*

Main category: cs.CR

TL;DR: 21世纪机器学习和量子计算发展使算法安全受威胁，现有加密算法研究少且有漏洞，本文探索用现代语言模型架构破解公钥私钥关联，测试机器学习能力并预测领域未来走向


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和量子计算发展，当前加密算法如ECC虽为安全支柱但研究少且有漏洞，有被破解风险，需探索新方法应对

Method: 探索使用现代语言模型架构反向工程公钥对生成过程以破解公钥私钥关联，测试机器学习记忆和反向工程能力

Result: 未提及具体结果

Conclusion: 将通过数据计算预测该领域未来方向

Abstract: With the advent of machine learning and quantum computing, the 21st century has gone from a place of relative algorithmic security, to one of speculative unease and possibly, cyber catastrophe.
  Modern algorithms like Elliptic Curve Cryptography (ECC) are the bastion of current cryptographic security protocols that form the backbone of consumer protection ranging from Hypertext Transfer Protocol Secure (HTTPS) in the modern internet browser, to cryptographic financial instruments like Bitcoin.
  And there's been very little work put into testing the strength of these ciphers. Practically the only study that I could find was on side-channel recognition, a joint paper from the University of Milan, Italy and King's College, London\cite{battistello2025ecc}.
  These algorithms are already considered bulletproof by many consumers, but exploits already exist for them, and with computing power and distributed, federated compute on the rise, it's only a matter of time before these current bastions fade away into obscurity, and it's on all of us to stand up when we notice something is amiss, lest we see such passages claim victims in that process.
  In this paper, we seek to explore the use of modern language model architecture in cracking the association between a known public key, and its associated private key, by intuitively learning to reverse engineer the public keypair generation process, effectively solving the curve.
  Additonally, we attempt to ascertain modern machine learning's ability to memorize public-private secp256r1 keypairs, and to then test their ability to reverse engineer the public keypair generation process.
  It is my belief that proof-for would be equally valuable as proof-against in either of these categories.
  Finally, we'll conclude with some number crunching on where we see this particular field heading in the future.

</details>


### [500] [Detecting Prompt Injection Attacks Against Application Using Classifiers](https://arxiv.org/abs/2512.12583)
*Safwan Shaheer,G. M. Refatul Islam,Mohammad Rafid Hamid,Md. Abrar Faiaz Khan,Md. Omar Faruk,Yaseen Nur*

Main category: cs.CR

TL;DR: 本文基于HackAPrompt Playground Submissions语料库整理并扩充提示注入数据集，训练多个分类器来检测大语言模型集成Web应用中的恶意提示，提升检测和缓解能力。


<details>
  <summary>Details</summary>
Motivation: 提示注入攻击会危及关键系统的安全和稳定，需要有效检测和缓解。

Method: 基于HackAPrompt Playground Submissions语料库整理并扩充提示注入数据集，训练LSTM、前馈神经网络、随机森林和朴素贝叶斯等分类器。

Result: 所提方法改善了提示注入的检测和缓解。

Conclusion: 所提方法有助于保护目标应用和系统。

Abstract: Prompt injection attacks can compromise the security and stability of critical systems, from infrastructure to large web applications. This work curates and augments a prompt injection dataset based on the HackAPrompt Playground Submissions corpus and trains several classifiers, including LSTM, feed forward neural networks, Random Forest, and Naive Bayes, to detect malicious prompts in LLM integrated web applications. The proposed approach improves prompt injection detection and mitigation, helping protect targeted applications and systems.

</details>


### [501] [Adversarial Attacks Against Deep Learning-Based Radio Frequency Fingerprint Identification](https://arxiv.org/abs/2512.12002)
*Jie Ma,Junqing Zhang,Guanxiong Shen,Alan Marshall,Chip-Hong Chang*

Main category: cs.CR

TL;DR: 本文对不同深度学习模型的RFFI系统进行对抗攻击研究，分析三种算法，实验表明生成扰动对多种网络有效，UAP能成功攻击RFFI，成功率达81.7%。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示基于深度学习的RFFI易受对抗攻击，但针对不同类型RFFI分类器的有效攻击未被充分探索。

Method: 对RFFI系统使用FGSM、PGD和UAP三种算法进行对抗攻击，考虑无线环境特殊因素，用UAP开展实际攻击。

Result: 生成的扰动对CNN、LSTM和GRU有效，UAP在攻击者几乎无先验知识时对RFFI攻击成功率达81.7%。

Conclusion: UAP可成功对RFFI发动对抗攻击，在特定条件下有较高成功率。

Abstract: Radio frequency fingerprint identification (RFFI) is an emerging technique for the lightweight authentication of wireless Internet of things (IoT) devices. RFFI exploits deep learning models to extract hardware impairments to uniquely identify wireless devices. Recent studies show deep learning-based RFFI is vulnerable to adversarial attacks. However, effective adversarial attacks against different types of RFFI classifiers have not yet been explored. In this paper, we carried out a comprehensive investigations into different adversarial attack methods on RFFI systems using various deep learning models. Three specific algorithms, fast gradient sign method (FGSM), projected gradient descent (PGD), and universal adversarial perturbation (UAP), were analyzed. The attacks were launched to LoRa-RFFI and the experimental results showed the generated perturbations were effective against convolutional neural networks (CNNs), long short-term memory (LSTM) networks, and gated recurrent units (GRU). We further used UAP to launch practical attacks. Special factors were considered for the wireless context, including implementing real-time attacks, the effectiveness of the attacks over a period of time, etc. Our experimental evaluation demonstrated that UAP can successfully launch adversarial attacks against the RFFI, achieving a success rate of 81.7% when the adversary almost has no prior knowledge of the victim RFFI systems.

</details>


### [502] [Keep the Lights On, Keep the Lengths in Check: Plug-In Adversarial Detection for Time-Series LLMs in Energy Forecasting](https://arxiv.org/abs/2512.12154)
*Hua Ma,Ruoxi Sun,Minhui Xue,Xingliang Yuan,Carsten Rudolph,Surya Nepal,Ling Liu*

Main category: cs.CR

TL;DR: 新兴时间序列大语言模型（TS - LLM）在低碳电力系统时间序列预测中重要，但受对抗样本（AEs）影响，本文提出插件检测框架，在多个数据集和模型上验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 新兴TS - LLM在能源互联网生态系统中重要，但存在对抗样本这一关键漏洞，且检测对抗样本面临挑战。

Method: 利用TS - LLM可变长度输入能力，以采样引起的差异为检测信号，通过生成输入序列的多个缩短变体，测量其预测一致性来检测AEs。

Result: 在三个代表性TS - LLM和三个能源数据集上评估，在黑盒和白盒攻击场景下都有强而稳健的检测性能。

Conclusion: 该方法可作为现实世界能源系统中TS - LLM预测的可靠保障。

Abstract: Accurate time-series forecasting is increasingly critical for planning and operations in low-carbon power systems. Emerging time-series large language models (TS-LLMs) now deliver this capability at scale, requiring no task-specific retraining, and are quickly becoming essential components within the Internet-of-Energy (IoE) ecosystem. However, their real-world deployment is complicated by a critical vulnerability: adversarial examples (AEs). Detecting these AEs is challenging because (i) adversarial perturbations are optimized across the entire input sequence and exploit global temporal dependencies, which renders local detection methods ineffective, and (ii) unlike traditional forecasting models with fixed input dimensions, TS-LLMs accept sequences of variable length, increasing variability that complicates detection. To address these challenges, we propose a plug-in detection framework that capitalizes on the TS-LLM's own variable-length input capability. Our method uses sampling-induced divergence as a detection signal. Given an input sequence, we generate multiple shortened variants and detect AEs by measuring the consistency of their forecasts: Benign sequences tend to produce stable predictions under sampling, whereas adversarial sequences show low forecast similarity, because perturbations optimized for a full-length sequence do not transfer reliably to shorter, differently-structured subsamples. We evaluate our approach on three representative TS-LLMs (TimeGPT, TimesFM, and TimeLLM) across three energy datasets: ETTh2 (Electricity Transformer Temperature), NI (Hourly Energy Consumption), and Consumption (Hourly Electricity Consumption and Production). Empirical results confirm strong and robust detection performance across both black-box and white-box attack scenarios, highlighting its practicality as a reliable safeguard for TS-LLM forecasting in real-world energy systems.

</details>


### [503] [CTIGuardian: A Few-Shot Framework for Mitigating Privacy Leakage in Fine-Tuned LLMs](https://arxiv.org/abs/2512.12914)
*Shashie Dilhara Batan Arachchige,Benjamin Zi Hao Zhao,Hassan Jameel Asghar,Dinusha Vatsalan,Dali Kaafar*

Main category: cs.CR

TL;DR: 研究表明微调大语言模型存在敏感信息泄露风险，提出隐私对齐方法CTIGuardian，在CTI用例中表现优于基于NER的模型，框架具通用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调时使用含敏感信息的专有数据集，存在信息泄露风险，需有效缓解方法。

Method: 提出隐私对齐方法，通过少样本监督，集成隐私分类器和隐私编辑器，由同一基础大语言模型处理。

Result: 使用GPT - 4o mini和Mistral - 7B Instruct模型评估CTIGuardian，相比基于NER的Presidio模型，有更好的隐私 - 效用权衡。

Conclusion: CTIGuardian在CTI用例中有效，框架可应用于其他敏感领域。

Abstract: Large Language Models (LLMs) are often fine-tuned to adapt their general-purpose knowledge to specific tasks and domains such as cyber threat intelligence (CTI). Fine-tuning is mostly done through proprietary datasets that may contain sensitive information. Owners expect their fine-tuned model to not inadvertently leak this information to potentially adversarial end users. Using CTI as a use case, we demonstrate that data-extraction attacks can recover sensitive information from fine-tuned models on CTI reports, underscoring the need for mitigation. Retraining the full model to eliminate this leakage is computationally expensive and impractical. We propose an alternative approach, which we call privacy alignment, inspired by safety alignment in LLMs. Just like safety alignment teaches the model to abide by safety constraints through a few examples, we enforce privacy alignment through few-shot supervision, integrating a privacy classifier and a privacy redactor, both handled by the same underlying LLM. We evaluate our system, called CTIGuardian, using GPT-4o mini and Mistral-7B Instruct models, benchmarking against Presidio, a named entity recognition (NER) baseline. Results show that CTIGuardian provides a better privacy-utility trade-off than NER based models. While we demonstrate its effectiveness on a CTI use case, the framework is generic enough to be applicable to other sensitive domains.

</details>


### [504] [Cisco Integrated AI Security and Safety Framework Report](https://arxiv.org/abs/2512.12921)
*Amy Chang,Tiffany Saade,Sanket Mendapara,Adam Swanda,Ankit Garg*

Main category: cs.CR

TL;DR: 已有AI安全框架有局限，本文提出Cisco的集成AI安全与安全框架，可对AI风险分类、集成和操作。


<details>
  <summary>Details</summary>
Motivation: 现存AI安全相关框架只能覆盖部分风险情况，有必要提出全面、可操作的框架来应对AI安全挑战。

Method: 分析现有框架的差距，提出统一、生命周期感知的分类法和操作化框架设计原则。

Result: 得到Cisco的集成AI安全与安全框架，可用于威胁识别、红队测试、风险优先级排序等。

Conclusion: 该框架能为理解AI系统故障、对手利用方式及组织构建防御提供结构，且可随能力发展扩展。

Abstract: Artificial intelligence (AI) systems are being readily and rapidly adopted, increasingly permeating critical domains: from consumer platforms and enterprise software to networked systems with embedded agents. While this has unlocked potential for human productivity gains, the attack surface has expanded accordingly: threats now span content safety failures (e.g., harmful or deceptive outputs), model and data integrity compromise (e.g., poisoning, supply-chain tampering), runtime manipulations (e.g., prompt injection, tool and agent misuse), and ecosystem risks (e.g., orchestration abuse, multi-agent collusion). Existing frameworks such as MITRE ATLAS, National Institute of Standards and Technology (NIST) AI 100-2 Adversarial Machine Learning (AML) taxonomy, and OWASP Top 10s for Large Language Models (LLMs) and Agentic AI Applications provide valuable viewpoints, but each covers only slices of this multi-dimensional space.
  This paper presents Cisco's Integrated AI Security and Safety Framework ("AI Security Framework"), a unified, lifecycle-aware taxonomy and operationalization framework that can be used to classify, integrate, and operationalize the full range of AI risks. It integrates AI security and AI safety across modalities, agents, pipelines, and the broader ecosystem. The AI Security Framework is designed to be practical for threat identification, red-teaming, risk prioritization, and it is comprehensive in scope and can be extensible to emerging deployments in multimodal contexts, humanoids, wearables, and sensory infrastructures. We analyze gaps in prevailing frameworks, discuss design principles for our framework, and demonstrate how the taxonomy provides structure for understanding how modern AI systems fail, how adversaries exploit these failures, and how organizations can build defenses across the AI lifecycle that evolve alongside capability advancements.

</details>


### [505] [ceLLMate: Sandboxing Browser AI Agents](https://arxiv.org/abs/2512.12594)
*Luoxi Meng,Henry Feng,Ilia Shumailov,Earlence Fernandes*

Main category: cs.CR

TL;DR: 提出浏览器级沙箱框架ceLLMate，解决语义差距和策略预测挑战，能有效阻止提示注入攻击且开销小。


<details>
  <summary>Details</summary>
Motivation: 浏览器使用代理（BUAs）易受提示注入攻击，需保护其安全。

Method: 引入代理站点地图映射低级行为到高级语义动作，将网站编写的强制策略与自动策略预测层结合。

Result: 将ceLLMate实现为与代理无关的浏览器扩展，能有效阻止各种提示注入攻击。

Conclusion: ceLLMate能在开销可忽略的情况下，有效降低提示注入攻击的影响。

Abstract: Browser-using agents (BUAs) are an emerging class of autonomous agents that interact with web browsers in human-like ways, including clicking, scrolling, filling forms, and navigating across pages. While these agents help automate repetitive online tasks, they are vulnerable to prompt injection attacks that can trick an agent into performing undesired actions, such as leaking private information or issuing state-changing requests. We propose ceLLMate, a browser-level sandboxing framework that restricts the agent's ambient authority and reduces the blast radius of prompt injections. We address two fundamental challenges: (1) The semantic gap challenge in policy enforcement arises because the agent operates through low-level UI observations and manipulations; however, writing and enforcing policies directly over UI-level events is brittle and error-prone. To address this challenge, we introduce an agent sitemap that maps low-level browser behaviors to high-level semantic actions. (2) Policy prediction in BUAs is the norm rather than the exception. BUAs have no app developer to pre-declare sandboxing policies, and thus, ceLLMate pairs website-authored mandatory policies with an automated policy-prediction layer that adapts and instantiates these policies from the user's natural-language task. We implement ceLLMate as an agent-agnostic browser extension and demonstrate how it enables sandboxing policies that effectively block various types of prompt injection attacks with negligible overhead.

</details>


### [506] [Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models](https://arxiv.org/abs/2512.13325)
*Malte Hellmeier*

Main category: cs.CR

TL;DR: 本文研究现有数字文本水印方法在大语言模型下的安全性，通过实验发现最新推理模型能检测含水印文本，但无源代码无法提取水印，并探讨了相关影响与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛使用，确保数字文本安全愈发重要，现有研究缺乏对数字文本水印方法在大语言模型下安全性和不可检测性的分析验证。

Method: 在三个实验的受控测试平台中，针对GPT - 5等六种大语言模型，实现并分析十种现有的Unicode文本水印方法。

Result: 最新推理模型能检测到含水印文本，但所有模型在无源代码的情况下无法提取水印。

Conclusion: 讨论了对安全研究人员和从业者的影响，指出了未来解决安全问题的研究机会。

Abstract: Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.

</details>


### [507] [Behavior-Aware and Generalizable Defense Against Black-Box Adversarial Attacks for ML-Based IDS](https://arxiv.org/abs/2512.13501)
*Sabrine Ennaji,Elhadj Benkhelifa,Luigi Vincenzo Mancini*

Main category: cs.CR

TL;DR: 现有机器学习入侵检测系统的黑盒对抗攻击防御方法有局限，本文提出Adaptive Feature Poisoning防御机制，评估显示其能混淆攻击者、降低攻击效果并保持检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有针对机器学习入侵检测系统的黑盒对抗攻击防御方法存在适用场景有限、需内部模型访问、难适应新攻击策略、影响系统性能等问题，无法实际应用。

Method: 提出Adaptive Feature Poisoning机制，对特定流量特征引入动态、上下文感知的扰动，利用流量分析、变化点检测和自适应缩放，根据观测偏差选择攻击者可能利用的特征进行扰动。

Result: 针对多种现实对抗攻击策略的评估表明，该机制能混淆攻击者、降低攻击效果并保持检测性能。

Conclusion: Adaptive Feature Poisoning是一种可泛化、与攻击无关且难以察觉的防御机制，向机器学习入侵检测系统的实用和强大的对抗弹性迈进了重要一步。

Abstract: Machine learning based intrusion detection systems are increasingly targeted by black box adversarial attacks, where attackers craft evasive inputs using indirect feedback such as binary outputs or behavioral signals like response time and resource usage. While several defenses have been proposed, including input transformation, adversarial training, and surrogate detection, they often fall short in practice. Most are tailored to specific attack types, require internal model access, or rely on static mechanisms that fail to generalize across evolving attack strategies. Furthermore, defenses such as input transformation can degrade intrusion detection system performance, making them unsuitable for real time deployment.
  To address these limitations, we propose Adaptive Feature Poisoning, a lightweight and proactive defense mechanism designed specifically for realistic black box scenarios. Adaptive Feature Poisoning assumes that probing can occur silently and continuously, and introduces dynamic and context aware perturbations to selected traffic features, corrupting the attacker feedback loop without impacting detection capabilities. The method leverages traffic profiling, change point detection, and adaptive scaling to selectively perturb features that an attacker is likely exploiting, based on observed deviations.
  We evaluate Adaptive Feature Poisoning against multiple realistic adversarial attack strategies, including silent probing, transferability based attacks, and decision boundary based attacks. The results demonstrate its ability to confuse attackers, degrade attack effectiveness, and preserve detection performance. By offering a generalizable, attack agnostic, and undetectable defense, Adaptive Feature Poisoning represents a significant step toward practical and robust adversarial resilience in machine learning based intrusion detection systems.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [508] [Advancing Machine Learning Optimization of Chiral Photonic Metasurface: Comparative Study of Neural Network and Genetic Algorithm Approaches](https://arxiv.org/abs/2512.13656)
*Davide Filippozzi,Alexandre Mayer,Nicolas Roy,Wei Fang,Arash Rahimi-Iman*

Main category: physics.optics

TL;DR: 提出结合深度学习与进化算法的优化框架，改进手性光子纳米结构设计与性能，展示了高CD和更多结构探索成果。


<details>
  <summary>Details</summary>
Motivation: 手性光子超表面对下一代光子器件很重要，需改进手性光子纳米结构的设计和性能。

Method: 结合深度学习和进化算法，采用双输出神经网络架构、改进适应度函数和数据增强技术，对比NN和GA方法。

Result: 实现两倍高的CD，在有限计算资源下探索更多结构，电磁模拟给出定制光谱反射率。

Conclusion: 该优化框架为适用于偏振选择性光 - 物质相互作用研究的手性镜铺平道路。

Abstract: Chiral photonic metasurfaces provide unique capabilities for tailoring light-matter interactions, which are essential for next-generation photonic devices. Here, we report an advanced optimization framework that combines deep learning and evolutionary algorithms to significantly improve both the design and performance of chiral photonic nanostructures. Building on previous work utilizing a three-layer perceptron reinforced learning and stochastic evolutionary algorithm with decaying changes and mass extinction for chiral photonic optimization, our study introduces a refined pipeline featuring a two-output neural network architecture to reduce the trade-off between high chiral dichroism (CD) and reflectivity. Additionally, we use an improved fitness function, and efficient data augmentation techniques. A comparative analysis between a neural network (NN)-based approach and a genetic algorithm (GA) is presented for structures of different interface pattern depth, material combinations, and geometric complexity. We demonstrate a twice higher CD and the impact of both the corner number and the refractive index contrast at the example of a GaP/air and PMMA/air metasurface as a result of superior optimization performance. Additionally, a substantial increase in the number of structures explored within limited computational resources is highlighted, with tailored spectral reflectivity suggested by our electromagnetic simulations, paving the way for chiral mirrors applicable to polarization-selective light-matter interaction studies.

</details>


### [509] [Meta-GPT: Decoding the Metasurface Genome with Generative Artificial Intelligence](https://arxiv.org/abs/2512.12888)
*David Dang,Stuart Love,Meena Salib,Quynh Dang,Samuel Rothfarb,Mysk Alnatour,Andrew Salij,Hou-Tong Chen,Ho Wai,Lee,Wilton J. M. Kort-Kamp*

Main category: physics.optics

TL;DR: 本文引入METASTRINGS语言，开发Meta - GPT模型用于光子学设计，取得良好效果，为AI驱动的光子学奠定基础。


<details>
  <summary>Details</summary>
Motivation: 推进适用于物理科学的人工智能，需要可解释且符合自然规律的表示方法。

Method: 引入METASTRINGS语言表达光子学纳米结构，开发基于该语言训练并经多种学习方式微调的Meta - GPT模型。

Result: 模型在各种设计任务中均方光谱误差<3%，句法有效性>98%，生成的超表面原型实验测量的光学响应与目标光谱匹配。

Conclusion: Meta - GPT能通过METASTRINGS学习光与物质相互作用的组合规则，为AI驱动的光子学奠定基础，是迈向超表面基因组计划的重要一步。

Abstract: Advancing artificial intelligence for physical sciences requires representations that are both interpretable and compatible with the underlying laws of nature. We introduce METASTRINGS, a symbolic language for photonics that expresses nanostructures as textual sequences encoding materials, geometries, and lattice configurations. Analogous to molecular textual representations in chemistry, METASTRINGS provides a framework connecting human interpretability with computational design by capturing the structural hierarchy of photonic metasurfaces. Building on this representation, we develop Meta-GPT, a foundation transformer model trained on METASTRINGS and finetuned with physics-informed supervised, reinforcement, and chain-of-thought learning. Across various design tasks, the model achieves <3% mean-squared spectral error and maintains >98% syntactic validity, generating diverse metasurface prototypes whose experimentally measured optical responses match their target spectra. These results demonstrate that Meta-GPT can learn the compositional rules of light-matter interactions through METASTRINGS, laying a rigorous foundation for AI-driven photonics and representing an important step toward a metasurface genome project.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [510] [Modeling Dabrafenib Response Using Multi-Omics Modality Fusion and Protein Network Embeddings Based on Graph Convolutional Networks](https://arxiv.org/abs/2512.12134)
*La Ode Aman,A Mu'thi Andy Suryadi,Dizky Ramadani Putri Papeo,Hamsidar Hasan,Ariani H Hutuba,Netty Ino Ischak,Yuszda K. Salimi*

Main category: q-bio.BM

TL;DR: 研究通过整合多组学数据与蛋白质网络嵌入预测Dabrafenib敏感性，选择整合蛋白质组学和转录组学的模式效果最佳。


<details>
  <summary>Details</summary>
Motivation: 癌细胞对靶向治疗的反应源于复杂分子相互作用，单一组学不足以准确预测。

Method: 整合多组学数据，用图卷积网络生成蛋白质网络嵌入，经神经网络预处理编码为低维表示，结合STRING的蛋白质相互作用信息，用注意力融合机制为各模态分配权重。

Result: 选择整合两种模态，尤其是蛋白质组学和转录组学，测试性能最佳（R2约0.96），优于单一组学和全多模态设置。

Conclusion: 注意力引导的多组学融合结合图卷积网络可提高药物反应预测能力，揭示Dabrafenib敏感性的互补分子决定因素，为精准肿瘤学和靶向治疗预测建模提供计算框架。

Abstract: Cancer cell response to targeted therapy arises from complex molecular interactions, making single omics insufficient for accurate prediction. This study develops a model to predict Dabrafenib sensitivity by integrating multiple omics layers (genomics, transcriptomics, proteomics, epigenomics, and metabolomics) with protein network embeddings generated using Graph Convolutional Networks (GCN). Each modality is encoded into low dimensional representations through neural network preprocessing. Protein interaction information from STRING is incorporated using GCN to capture biological topology. An attention based fusion mechanism assigns adaptive weights to each modality according to its relevance. Using GDSC cancer cell line data, the model shows that selective integration of two modalities, especially proteomics and transcriptomics, achieves the best test performance (R2 around 0.96), outperforming all single omics and full multimodal settings. Genomic and epigenomic data were less informative, while proteomic and transcriptomic layers provided stronger phenotypic signals related to MAPK inhibitor activity. These results show that attention guided multi omics fusion combined with GCN improves drug response prediction and reveals complementary molecular determinants of Dabrafenib sensitivity. The approach offers a promising computational framework for precision oncology and predictive modeling of targeted therapies.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [511] [Adaptive Sampling for Hydrodynamic Stability](https://arxiv.org/abs/2512.13532)
*Anshima Singh,David J. Silvester*

Main category: physics.flu-dyn

TL;DR: 提出一种自适应采样方法用于高效检测参数化流体流动问题中的分岔边界，减少模拟次数，为高维稳定性分析提供可扩展基础。


<details>
  <summary>Details</summary>
Motivation: 改进现有机器学习方法，提高检测参数化流体流动问题分岔边界的效率。

Method: 引入基于流的深度生成模型实现参数空间采样自适应，包括分类网络和概率密度估计技术KRnet，利用分类输出的香农熵作为不确定性指标。

Result: 从均匀参数分布开始，该方法能用更少的Navier - Stokes模拟准确识别分岔边界。

Conclusion: 新方法为高维稳定性分析提供了可扩展的基础。

Abstract: An adaptive sampling approach for efficient detection of bifurcation boundaries in parametrized fluid flow problems is presented herein. The study extends the machine-learning approach of Silvester (Machine Learning for Hydrodynamic Stability, arXiv:2407.09572), where a classifier network was trained on preselected simulation data to identify bifurcated and nonbifurcated flow regimes. In contrast, the proposed methodology introduces adaptivity through a flow-based deep generative model that automatically refines the sampling of the parameter space. The strategy has two components: a classifier network maps the flow parameters to a bifurcation probability, and a probability density estimation technique (KRnet) for the generation of new samples at each adaptive step. The classifier output provides a probabilistic measure of flow stability, and the Shannon entropy of these predictions is employed as an uncertainty indicator. KRnet is trained to approximate a probability density function that concentrates sampling in regions of high entropy, thereby directing computational effort towards the evolving bifurcation boundary. This coupling between classification and generative modeling establishes a feedback-driven adaptive learning process analogous to error-indicator based refinement in contemporary partial differential equation solution strategies. Starting from a uniform parameter distribution, the new approach achieves accurate bifurcation boundary identification with significantly fewer Navier--Stokes simulations, providing a scalable foundation for high-dimensional stability analysis.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [512] [Group-averaged Markov chains II: tuning of group action in finite state space](https://arxiv.org/abs/2512.13067)
*Michael C. H. Choi,Ryan J. Y. Lim,Youjia Wang*

Main category: math.PR

TL;DR: 研究群平均马尔可夫链，分析三种轨道核及其组合，给出多种理论结果并提出调参启发式方法，通过实例验证。


<details>
  <summary>Details</summary>
Motivation: 研究群平均马尔可夫链的性质，优化马尔可夫链蒙特卡罗方法。

Method: 分析三种规范轨道核及其乘法三明治和加法混合形式，推导相关理论结果。

Result: 得到轨道核的收敛性、投影链一致性、谱间隙等结果，证明毕达哥拉斯恒等式，给出最优选择的特征，交替投影收敛速率等。

Conclusion: 理论结果可用于实际中轨道核的调整，在离散均匀和多峰例子中表现良好。

Abstract: We study group-averaged Markov chains obtained by augmenting a $π$-stationary transition kernel $P$ with a group action on the state space via orbit kernels. Given a group $\mathcal{G}$ with orbits $(\mathcal{O}_i)_{i=1}^k$, we analyse three canonical orbit kernels: namely the Gibbs $(G)$, Metropolis-Hastings $(M)$, and Barker $(B)$ kernels, as well as their multiplicative sandwiches $QPQ$ and the additive mixtures $\frac{1}{2}(P+Q)$ where $Q\in\{G,M,B\}$. We show that $M^t, B^t \to G$ blockwise as $t \to \infty$ under suitable conditions, that the projection chains induced by $(\mathcal{O}_i)_{i=1}^k$ coincide for $GPG$ and $P$, and that orbit averaging never deteriorates the absolute spectral gap or asymptotic variance when $P$ is reversible. We give a direct and simple proof of Pythagorean identity under the Kullback-Leibler (KL) divergence, showing that $GPG$ arises naturally as an information projection of $P$ onto the set of $G$-invariant transition matrices. For a given $P$, we characterise the optimal choice of $G$ with a fixed number of orbits that minimises the one-step KL divergence to stationarity. Analogously, for a given $G$, we characterise the optimal choice of $P$ and give sufficient conditions under which $GPG = Π$. We further show that alternating projections over multiple group actions converge at a rate governed by the singular values of an overlap matrix, and that in structured cases, this yields exact sampling where the number of group actions grows logarithmically with the size of the state space. Based on the theory, we propose two heuristics to tune $G$ in practice. We also illustrate the results on discrete uniform and multimodal examples, including the Curie-Weiss model where $GPG$ achieves polynomial (in inverse temperature and dimension) mixing while Glauber dynamics remains exponentially slow.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [513] [CapOptix: An Options-Framework for Capacity Market Pricing](https://arxiv.org/abs/2512.12871)
*Millend Roy,Agostino Capponi,Vladimir Pyltsov,Yinbo Hu,Vijay Modi*

Main category: eess.SY

TL;DR: 提出CapOptix容量定价框架，将容量承诺视为可靠性期权，应用于多市场历史价格数据并与现有机制比较。


<details>
  <summary>Details</summary>
Motivation: 电力市场在可再生能源渗透、需求多变和价格冲击下需维持可靠性，传统容量市场设计依赖预期值指标，无法应对风险。

Method: 提出CapOptix框架，将容量承诺视作可靠性期权，用马尔可夫状态转换过程模拟结构性价格变动来确定容量溢价。

Result: 将框架应用于多市场历史价格数据，并得到溢价范围。

Conclusion: 文档未提及明确结论。

Abstract: Electricity markets are under increasing pressure to maintain reliability amidst rising renewable penetration, demand variability, and occasional price shocks. Traditional capacity market designs often fall short in addressing this by relying on expected-value metrics of energy unserved, which overlook risk exposure in such systems. In this work, we present CapOptix, a capacity pricing framework that interprets capacity commitments as reliability options, i.e., financial derivatives of wholesale electricity prices. CapOptix characterizes the capacity premia charged by accounting for structural price shifts modeled by the Markov Regime Switching Process. We apply the framework to historical price data from multiple electricity markets and compare the resulting premium ranges with existing capacity remuneration mechanisms.

</details>


### [514] [On the Complementarity of Shared Electric Mobility and Renewable Energy Communities](https://arxiv.org/abs/2512.13099)
*Julien Allard,Noé Diffels,François Vallée,Bertrand Cornélusse,Zacharie De Grève*

Main category: eess.SY

TL;DR: 本文提出集成与可再生能源社区协调及车网互动能力的优化模型，评估合作效果，结果显示协调可降成本、减变电站压力。


<details>
  <summary>Details</summary>
Motivation: 共享出行服务提供商和能源社区都要以低成本满足需求，两者可利用互补性协调决策提升运营。

Method: 提出一个混合整数二阶锥规划的长期电动汽车车队规划优化问题的模型。

Result: 双方协调相比独自运营最多可降低11.3%的年成本，在电网运营商设置峰值惩罚时可通过激活电动汽车的灵活性将变电站变压器压力降低46%。

Conclusion: 共享出行服务提供商和能源社区协调合作能提升经济、能源和电网性能。

Abstract: Driven by the ongoing energy transition, shared mobility service providers are emerging actors in electrical power systems which aim to shift combustion-based mobility to electric paradigm. In the meantime, Energy Communities are deployed to enhance the local usage of distributed renewable production. As both ators share the same goal of satisfying the demand at the lowest cost, they could take advantage of their complementarity and coordinate their decisions to enhance each other operation. This paper presents an original Mixed-Integer Second Order Cone Programming long-term Electric Vehicle fleet planning optimization problem that integrates the coordination with a Renewable Energy Community and Vehicle-to-Grid capability. This model is used to assess the economic, energy, and grid performances of their collaboration in a 21 buses low-voltage distribution network. Key results show that, both actors coordination can help reducing the yearly cost up to 11.3 % compared to their stand-alone situation and that it may reduce the stress on the substation transformer by 46 % through the activation of the inherent EVs flexibility when subject to peak penalties from the grid operator.

</details>


### [515] [Congestion Reduction in EV Charger Placement Using Traffic Equilibrium Models](https://arxiv.org/abs/2512.12081)
*Semih Kara,Yasin Sonmez,Can Kizilkale,Alex Kurzhanskiy,Nuno C. Martins,Murat Arcak*

Main category: eess.SY

TL;DR: 研究如何战略布局电动车充电器以减少拥堵，采用两种均衡模型结合贪心算法，实验显示有良好效果且排队模型更真实。


<details>
  <summary>Details</summary>
Motivation: 电动车普及，若不考虑充电器布局对拥堵的影响，会使交通状况恶化，因此研究如何战略布局充电器以减少拥堵。

Method: 使用基于拥塞博弈和原子排队模拟的两种均衡模型，并将其应用于可扩展的贪心站点布局算法。

Result: 贪心方案在现实网络中能产生最优或接近最优的拥堵结果；排队模型比拥塞博弈模型产生更真实的结果；提出统一方法校准拥堵延迟并求解链路空间均衡。

Conclusion: 通过合理布局充电器能有效减少拥堵，排队模型更适合用于相关研究。

Abstract: Growing EV adoption can worsen traffic conditions if chargers are sited without regard to their impact on congestion. We study how to strategically place EV chargers to reduce congestion using two equilibrium models: one based on congestion games and one based on an atomic queueing simulation. We apply both models within a scalable greedy station-placement algorithm. Experiments show that this greedy scheme yields optimal or near-optimal congestion outcomes in realistic networks, even though global optimality is not guaranteed as we show with a counterexample. We also show that the queueing-based approach yields more realistic results than the congestion-game model, and we present a unified methodology that calibrates congestion delays from queue simulation and solves equilibrium in link-space.

</details>


### [516] [AI-Driven Real-Time Kick Classification in Olympic Taekwondo Using Sensor Fusion](https://arxiv.org/abs/2512.12474)
*Jamsheed Mistri*

Main category: eess.SY

TL;DR: 本文提出基于AI的跆拳道计分系统，结合多传感器，利用机器学习实现实时踢击分类和计分，有望提升赛事计分公平性与观赏性。


<details>
  <summary>Details</summary>
Motivation: 现有跆拳道护具计分系统简单，导致比赛防守过多、缺乏活力，计分有争议，影响观众参与度。

Method: 提出AI计分系统，集成多种传感器进行数据融合，用支持向量机（SVM）实现踢击技术实时识别，设计新计分规则。

Result: 基于2024年研究实现96 - 98%的踢击分类准确率，验证实时分类可行性。

Conclusion: 该系统可提高计分公平性，减少规则利用，鼓励动态技术，提升观众理解和兴奋度。

Abstract: Olympic Taekwondo has faced challenges in spectator engagement due to static, defensive gameplay and contentious scoring. Current Protector and Scoring Systems (PSS) rely on impact sensors and simplistic logic, encouraging safe strategies that diminish the sport's dynamism. This paper proposes an AI-powered scoring system that integrates existing PSS sensors with additional accelerometers, gyroscopes, magnetic/RFID, and impact force sensors in a sensor fusion framework. The system classifies kicks in real-time to identify technique type, contact location, impact force, and even the part of the foot used. A machine learning pipeline employing sensor fusion and Support Vector Machines (SVMs) is detailed, enabling automatic kick technique recognition for scoring. We present a novel kick scoring rubric that awards points based on specific kick techniques (e.g., turning and spinning kicks) to incentivize dynamic attacks. Drawing on a 2024 study achieving 96-98% accuracy, we validate the feasibility of real-time kick classification and further propose enhancements to this methodology, such as ensemble SVM classifiers and expanded datasets, to achieve the high-stakes accuracy required by the sport. We analyze how the proposed system can improve scoring fairness, reduce rule exploitation and illegitimate tactics, encourage more dynamic techniques, and enhance spectator understanding and excitement. The paper includes system design illustrations, a kick scoring table from an AI-augmented rule set, and discusses anticipated impacts on Olympic Taekwondo.

</details>


### [517] [An End-to-End Approach for Microgrid Probabilistic Forecasting and Robust Operation via Decision-focused Learning](https://arxiv.org/abs/2512.12755)
*Tingwei Cao,Yan Xu*

Main category: eess.SY

TL;DR: 本文提出端到端决策框架，结合概率预测与鲁棒运行优化微电网，案例显示能提升性能、降低成本。


<details>
  <summary>Details</summary>
Motivation: 可再生能源高渗透给微电网运行带来不确定性和间歇性，挑战经济可靠调度。

Method: 提出端到端决策框架，将多层编解码器概率预测模型与含直接负荷控制的两阶段鲁棒优化模型通过可微决策路径集成，用替代智能先预测后优化损失函数最小化决策遗憾。

Result: 在修改的IEEE 33母线和69母线系统案例中，该框架实现更优预测精度和运行性能，总成本和净运营成本最多降低18%。

Conclusion: 端到端决策方法对不确定下微电网弹性和成本效益管理有效且可扩展。

Abstract: High penetration of renewable energy sources (RES) introduces significant uncertainty and intermittency into microgrid operations, posing challenges to economic and reliable scheduling. To address this, this paper proposes an end-to-end decision-focused framework that jointly optimizes probabilistic forecasting and robust operation for microgrids. A multilayer encoder-decoder (MED) probabilistic forecasting model is integrated with a two-stage robust optimization (TSRO) model involving direct load control (DLC) through a differentiable decision pathway, enabling gradient-based feedback from operational outcomes to improve forecasting performance. Unlike conventional sequential approaches, the proposed method aligns forecasting accuracy with operational objectives by directly minimizing decision regret via a surrogate smart predict-then-optimize (SPO) loss function. This integration ensures that probabilistic forecasts are optimized for downstream decisions, enhancing both economic efficiency and robustness. Case studies on modified IEEE 33-bus and 69-bus systems demonstrate that the proposed framework achieves superior forecasting accuracy and operational performance, reducing total and net operation costs by up to 18% compared with conventional forecasting and optimization combinations. The results verify the effectiveness and scalability of the end-to-end decision-focused approach for resilient and cost-efficient microgrid management under uncertainty.

</details>


### [518] [Real-Time AI-Driven Milling Digital Twin Towards Extreme Low-Latency](https://arxiv.org/abs/2512.13482)
*Wenyi Liu,R. Sharma,W. "Grace" Guo,J. Yi,Y. B. Guo*

Main category: eess.SY

TL;DR: 本文对铣削领域数字孪生（DT）进行分析，探讨关键方面，展示案例并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 利用数字孪生实现智能制造，对铣削领域的新兴数字孪生领域进行分析。

Method: 从物理铣削的虚拟模型、数据流向和反馈等方面探索数字孪生关键方面，突出实时数据传输协议和虚拟建模方法，通过案例研究展示其能力。

Result: 通过案例展示了铣削过程中由实时机器学习驱动的实时数字孪生对刀具 - 工件接触的变革能力。

Conclusion: 论文概述了未来研究方向以实现工业4.0及更远目标。

Abstract: Digital twin (DT) enables smart manufacturing by leveraging real-time data, AI models, and intelligent control systems. This paper presents a state-of-the-art analysis on the emerging field of DTs in the context of milling. The critical aspects of DT are explored through the lens of virtual models of physical milling, data flow from physical milling to virtual model, and feedback from virtual model to physical milling. Live data streaming protocols and virtual modeling methods are highlighted. A case study showcases the transformative capability of a real-time machine learning-driven live DT of tool-work contact in a milling process. Future research directions are outlined to achieve the goals of Industry 4.0 and beyond.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [519] [Defunding Sexual Healthcare: A Topological Investigation of Resource Accessibility](https://arxiv.org/abs/2512.12011)
*Denise Gonzalez-Cruz,Genesis Encarnacion,Kaili Martinez-Beasley,Robin Wilson,Nicholas Arosemena,Atilio Barreda,Omayra Ortega,Daniel A. Cruz*

Main category: cs.SI

TL;DR: 分析政府削减PPHCs医疗补助资金影响，用TDA评估加州生殖和性健康护理覆盖差距，找出就医脆弱地区。


<details>
  <summary>Details</summary>
Motivation: 政府削减PPHCs资金，扩大FQHCs资金不能解决就医地点可及性问题，需评估该问题。

Method: 运用TDA检测覆盖差距，用R和Python收集和可视化数据，应用n - 最近邻算法和持久同调分析。

Result: 旨在找出就医最脆弱地区，展示TDA可用于分析公共卫生空间不平等。

Conclusion: TDA可用于分析公共卫生领域的空间不平等问题。

Abstract: Government actions, such as the Medina v. Planned Parenthood South Atlantic Supreme Court ruling and the passage of the Big Beautiful Bill Act, have aimed to restrict or prohibit Medicaid funding for Planned Parenthood Healthcare Centers (PPHCs) at both the state and national levels. These funding cuts are particularly harmful in states like California, which has a large population of Medicaid users. This analysis focuses on the distribution of Planned Parenthood clinics and Federally Qualified Health Centers (FQHCs), which offer essential reproductive healthcare services including, but not limited to, abortions, birth control, HIV services, pregnancy testing and planning, STD testing and treatment, and cancer screenings. While expanded funding for FQHCs has been proposed as a solution, it fails to address the locational accessibility of Medicaid-funded health centers that provide sexual and reproductive care. To assess this issue, we analyze the proximity of data points representing California's PPHC and FQHC locations. Topological Data Analysis (TDA)-an approach that examines the shape and structure of data -- is used to detect disparities in reproductive and sexual healthcare coverage. To conduct data collection and visualization, we utilize R and Python. We apply an n-closest neighbor algorithm to examine distances between facilities and assess changes in travel time required to reach healthcare sites. We apply persistent homology to analyze current gaps across multiple scales in healthcare coverage and compare them to potential future gaps. Our findings aim to identify areas where access to care is most vulnerable and demonstrate how TDA can be used to analyze spatial inequalities in public health.

</details>


### [520] [Dynamic Homophily with Imperfect Recall: Modeling Resilience in Adversarial Networks](https://arxiv.org/abs/2512.12332)
*Saad Alqithami*

Main category: cs.SI

TL;DR: 研究同质性、记忆约束和对抗性干扰对复杂网络弹性和适应性的影响，提出新框架并实验，发现余弦相似度有优势，战略遗忘可增强弹性。


<details>
  <summary>Details</summary>
Motivation: 探究同质性、记忆约束和对抗性干扰如何共同塑造复杂网络的弹性和适应性。

Method: 开发新框架，将显式记忆衰减机制集成到基于同质性的模型中，在合成数据集上进行广泛实验，改变衰减函数、重新连接概率和相似度度量等。

Result: 余弦相似度在稀疏、凸和模块化网络的稳定性指标上提高达30%；战略遗忘可通过平衡网络鲁棒性和适应性增强弹性。

Conclusion: 强调记忆和相似度参数与网络结构和对抗动态对齐的重要性，为优化现实应用提供可操作的见解。

Abstract: The purpose of this study is to investigate how homophily, memory constraints, and adversarial disruptions collectively shape the resilience and adaptability of complex networks. To achieve this, we develop a new framework that integrates explicit memory decay mechanisms into homophily-based models and systematically evaluate their performance across diverse graph structures and adversarial settings. Our methods involve extensive experimentation on synthetic datasets, where we vary decay functions, reconnection probabilities, and similarity measures, primarily comparing cosine similarity with traditional metrics such as Jaccard similarity and baseline edge weights. The results show that cosine similarity achieves up to a 30\% improvement in stability metrics in sparse, convex, and modular networks. Moreover, the refined value-of-recall metric demonstrates that strategic forgetting can bolster resilience by balancing network robustness and adaptability. The findings underscore the critical importance of aligning memory and similarity parameters with the structural and adversarial dynamics of the network. By quantifying the tangible benefits of incorporating memory constraints into homophily-based analyses, this study offers actionable insights for optimizing real-world applications, including social systems, collaborative platforms, and cybersecurity contexts.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [521] [The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance](https://arxiv.org/abs/2512.11933)
*Eren Kurshan,Tucker Balch,David Byrd*

Main category: cs.CY

TL;DR: 生成式和智能体人工智能进入金融市场速度超现有治理适应能力，文章基于复杂自适应系统理论建模，提出模块化治理架构，经案例验证可实现实时管控并兼容现有规则。


<details>
  <summary>Details</summary>
Motivation: 生成式和智能体人工智能进入金融市场，现有模型风险框架无法适应其特性，需新治理架构。

Method: 运用复杂自适应系统理论将相关技术建模为分散集合，提出模块化治理架构，含四层“监管模块”及八项设计策略。

Result: 案例研究表明分层控制可实时隔离有害行为，架构兼容现有模型风险规则，填补关键观测和控制缺口。

Conclusion: 该架构为金融系统中实现有弹性、自适应的人工智能治理提供了切实可行的途径。

Abstract: Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of "regulatory blocks": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems.

</details>


### [522] [Anticipatory Governance in Data-Constrained Environments: A Predictive Simulation Framework for Digital Financial Inclusion](https://arxiv.org/abs/2512.12212)
*Elizabeth Irenne Yuwono,Dian Tjondronegoro,Shawn Hunter,Amber Marshall*

Main category: cs.CY

TL;DR: 本文提出预测模拟框架支持政府信息系统的前瞻性治理，用太平洋数字经济数据集预测数字金融扫盲干预结果，发现基础数字能力干预效果好，可精准定位目标群体。


<details>
  <summary>Details</summary>
Motivation: 解决资源有限和群岛国家数字公共服务交付中金融排斥问题，弥补传统政策评估依赖回顾性数据缺乏事前情报的不足。

Method: 采用三阶段流程（描述性分析、可解释机器学习、情景模拟），利用联合国资本发展基金太平洋数字经济数据集进行模拟预测。

Result: 透明线性回归模型R平方达95.9%，确定可调整政策杠杆；基础数字能力干预效果最佳，最高增益达5.5%；能精准定位目标群体，防止资源错配。

Conclusion: 静态调查数据可转化为可操作的政策情报，为将预测分析嵌入公共部门决策支持系统提供可扩展且基于证据的蓝图，推动以公平为重点的数字治理。

Abstract: Financial exclusion remains a major barrier to digital public service delivery in resource-constrained and archipelagic nations. Traditional policy evaluations rely on retrospective data, limiting the ex-ante intelligence needed for agile resource allocation. This study introduces a predictive simulation framework to support anticipatory governance within government information systems. Using the UNCDF Pacific Digital Economy dataset of 10,108 respondents, we apply a three-stage pipeline: descriptive profiling, interpretable machine learning, and scenario simulation to forecast outcomes of digital financial literacy interventions before deployment. Leveraging cross-sectional structural associations, the framework projects intervention scenarios as prioritization heuristics rather than causal estimates. A transparent linear regression model with R-squared of 95.9 identifies modifiable policy levers. Simulations indicate that foundational digital capabilities such as device access and expense tracking yield the highest projected gains, up to 5.5 percent, outperforming attitudinal nudges. The model enables precision targeting, highlighting young female caregivers as high-leverage responders while flagging non-responders such as urban professionals to prevent resource misallocation. This research demonstrates how static survey data can be repurposed into actionable policy intelligence, offering a scalable and evidence-based blueprint for embedding predictive analytics into public-sector decision-support systems to advance equity-focused digital governance.

</details>


### [523] [Beyond right or wrong : towards redefining adaptive learning indicators in virtual learning environments](https://arxiv.org/abs/2512.12105)
*Andreia dos Santos Sachete,Alba Valeria de SantAnna de Freitas Loiola,Fabio Diniz Rossi,Jose Valdeni de Lima,Raquel Salcedo Gomes*

Main category: cs.CY

TL;DR: 多数虚拟学习环境的自适应学习方法仅关注答题对错，本文通过系统文献综述，揭示影响学生学习的指标，助于开发者提供更适配方案。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟学习环境自适应学习方法仅以答题对错评估学生学习水平有局限，需找出影响学生学习的指标以辅助自适应学习。

Method: 进行系统文献综述（SLR）。

Result: 筛选出的文献揭示了评估虚拟环境学习不同方面的综合方法，如动机、情绪等。

Conclusion: 讨论新指标可让开发者为学生实际情况提供更合适方案，实现更全面培养。

Abstract: Student learning development must involve more than just correcting or incorrect questions. However, most adaptive learning methods in Virtual Learning Environments are based on whether the student's response is incorrect or correct. This perspective is limited in assessing the student's learning level, as it does not consider other elements that can be crucial in this process. The objective of this work is to conduct a Systematic Literature Review (SLR) to elucidate which learning indicators influence student learning and which can be implemented in a VLE to assist in adaptive learning. The works selected and filtered by qualitative assessment reveal a comprehensive approach to assessing different aspects of the learning in virtual environments, such as motivation, emotions, physiological responses, brain imaging, and the students' prior knowledge. The discussion of these new indicators allows adaptive technology developers to implement more appropriate solutions to students' realities, resulting in more complete training.

</details>


### [524] [An Experience Report on a Pedagogically Controlled, Curriculum-Constrained AI Tutor for SE Education](https://arxiv.org/abs/2512.11882)
*Lucia Happe,Dominik Fuchß,Luca Hüttner,Kai Marquardt,Anne Koziolek*

Main category: cs.CY

TL;DR: 本文介绍RockStartIT Tutor设计与试点评估，该AI导师用于编程课程，评估显示受师生认可，但有挑战，强调AI是辅助工具。


<details>
  <summary>Details</summary>
Motivation: 过往技术乐观未达成，近期大语言模型发展，促使设计AI导师助力编程课程教学。

Method: 采用OpenAI的GPT - 4，通过新颖提示策略和模块化知识库，用技术接受模型（TAM）对13名师生评估。

Result: 学习者认可低风险提问环境，教育者强调可减轻认知负担、补充课堂教学。

Conclusion: AI集成采用实用方法，AI导师是辅助工具，非替代教师，可促进学习。

Abstract: The integration of artificial intelligence (AI) into education continues to evoke both promise and skepticism. While past waves of technological optimism often fell short, recent advances in large language models (LLMs) have revived the vision of scalable, individualized tutoring. This paper presents the design and pilot evaluation of RockStartIT Tutor, an AI-powered assistant developed for a digital programming and computational thinking course within the RockStartIT initiative. Powered by GPT-4 via OpenAI's Assistant API, the tutor employs a novel prompting strategy and a modular, semantically tagged knowledge base to deliver context-aware, personalized, and curriculum-constrained support for secondary school students. We evaluated the system using the Technology Acceptance Model (TAM) with 13 students and teachers. Learners appreciated the low-stakes environment for asking questions and receiving scaffolded guidance. Educators emphasized the system's potential to reduce cognitive load during independent tasks and complement classroom teaching. Key challenges include prototype limitations, a small sample size, and the need for long-term studies with the target age group. Our findings highlight a pragmatic approach to AI integration that requires no model training, using structure and prompts to shape behavior. We position AI tutors not as teacher replacements but as enabling tools that extend feedback access, foster inquiry, and support what schools do best: help students learn.

</details>


### [525] [Totalitarian Technics: The Hidden Cost of AI Scribes in Healthcare](https://arxiv.org/abs/2512.11814)
*Hugh Brosnahan*

Main category: cs.CY

TL;DR: 本文指出AI抄写员的意义不在于提高效率，而在于重塑医疗注意力，其体现的思维模式会带来负面影响。


<details>
  <summary>Details</summary>
Motivation: 探讨AI抄写员在医疗场景中的真正意义，而不局限于其所谓的提高效率作用。

Method: 进行概念分析，将AI抄写员置于关注人类思想和技能外化的哲学谱系中，并借鉴Iain McGilchrist的半球理论和Lewis Mumford的技术哲学。

Result: AI抄写员体现左脑式、计算性思维模式的主导，这种模式重视可衡量和程序性的内容，超过直观和关系性的内容。

Conclusion: 这种思维模式在医疗实践中的深入嵌入，有缩小护理范围、侵蚀临床专业知识以及使医生沦为机械系统操作员的风险。

Abstract: Artificial intelligence (AI) scribes, systems that record and summarise patient-clinician interactions, are promoted as solutions to administrative overload. This paper argues that their significance lies not in efficiency gains but in how they reshape medical attention itself. Offering a conceptual analysis, it situates AI scribes within a broader philosophical lineage concerned with the externalisation of human thought and skill. Drawing on Iain McGilchrist's hemisphere theory and Lewis Mumford's philosophy of technics, the paper examines how technology embodies and amplifies a particular mode of attention. AI scribes, it contends, exemplify the dominance of a left-hemispheric, calculative mindset that privileges the measurable and procedural over the intuitive and relational. As this mode of attention becomes further embedded in medical practice, it risks narrowing the field of care, eroding clinical expertise, and reducing physicians to operators within an increasingly mechanised system.

</details>


### [526] [The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique](https://arxiv.org/abs/2512.11818)
*Izabela Lipinska,Hugh Brosnahan*

Main category: cs.CY

TL;DR: 当代大语言模型可能导致类感应性精神病，论文分析其原理并提出缓解原则。


<details>
  <summary>Details</summary>
Motivation: 探究当代大语言模型对用户产生精神病影响的机制。

Method: 结合Bateson的双重束缚理论、共享性精神病临床文献和McGilchrist的半球理论进行分析。

Result: 大语言模型语言连贯性和无实质主体的矛盾会让用户在情感需求或不稳定时产生想象投射，当前设计加剧风险。

Conclusion: 提出“本体论诚实”作为缓解类感应性精神病的必要设计原则。

Abstract: This paper argues that contemporary large language models (LLMs) can contribute to psychotic involvement by creating interactions that resemble the relational dynamics of folie a deux. Drawing on Bateson's double bind theory, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory, we show how the combination of high linguistic coherence and the absence of an underlying subject produces a structural tension for the user: language suggests an interlocutor, while intuition registers a void. In contexts of emotional need or instability, this tension can lead users to resolve the conflict through imaginative projection, attributing interiority, intention, or presence to a system that possesses none. The paper situates these dynamics within emerging clinical reports, develops a phenomenological account of how they unfold, and argues that current engagement-optimised design choices exacerbate the risk. We conclude by proposing 'ontological honesty' as a necessary design principle for mitigating technologically mediated folie a deux.

</details>


### [527] [Assessing Greenspace Attractiveness with ChatGPT, Claude, and Gemini: Do AI Models Reflect Human Perceptions?](https://arxiv.org/abs/2512.11827)
*Milad Malekzadeh,Magdalena Biernacka,Elias Willberg,Jussi Torkko,Edyta Łaszkiewicz,Tuuli Toivonen*

Main category: cs.CY

TL;DR: 研究用多模态大语言模型评估绿地吸引力，对比模型输出与居民问卷回复，发现不同类型绿地的人机判断一致性有差异，模型有评估潜力但需人工监督，不能替代绿地评估。


<details>
  <summary>Details</summary>
Motivation: 现有绿地吸引力评估方法忽略非正式空间且难以大规模捕捉主观感知，需新方法。

Method: 使用ChatGPT GPT - 4o、Claude 3.5 Haiku和Gemini 2.0 Flash等多模态大语言模型，结合谷歌街景图像评估绿地吸引力，并与波兰罗兹居民地理问卷回复对比。

Result: 有吸引力的正式绿地和无吸引力的非正式绿地人机判断一致性高，其他情况一致性低；模型强调美学和设计特征，对安全、功能设施和地方特色关注不足。

Conclusion: 多模态大语言模型可支持但不能替代规划实践中因地制宜的绿地评估。

Abstract: Understanding greenspace attractiveness is essential for designing livable and inclusive urban environments, yet existing assessment approaches often overlook informal or transient spaces and remain too resource intensive to capture subjective perceptions at scale. This study examines the ability of multimodal large language models (MLLMs), ChatGPT GPT-4o, Claude 3.5 Haiku, and Gemini 2.0 Flash, to assess greenspace attractiveness similarly to humans using Google Street View imagery. We compared model outputs with responses from a geo-questionnaire of residents in Lodz, Poland, across both formal (for example, parks and managed greenspaces) and informal (for example, meadows and wastelands) greenspaces. Survey respondents and models indicated whether each greenspace was attractive or unattractive and provided up to three free text explanations. Analyses examined how often their attractiveness judgments aligned and compared their explanations after classifying them into shared reasoning categories. Results show high AI human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design oriented features, underrepresenting safety, functional infrastructure, and locally embedded qualities valued by survey respondents. While these findings highlight the potential for scalable pre-assessment, they also underscore the need for human oversight and complementary participatory approaches. We conclude that MLLMs can support, but not replace, context sensitive greenspace evaluation in planning practice.

</details>


### [528] [Expert Assessment: The Systemic Environmental Risks of Artficial Intelligence](https://arxiv.org/abs/2512.11863)
*Julian Schön,Lena Hoffmann,Nikolas Becker*

Main category: cs.CY

TL;DR: 报告探讨AI对环境的系统性风险，提出三级框架并通过案例研究说明。


<details>
  <summary>Details</summary>
Motivation: AI虽被视为解决社会挑战的工具，但环境足迹不断扩大，需了解其系统性环境风险。

Method: 进行叙事性文献综述，提出三级框架，并通过基于专家访谈的案例研究说明。

Result: 提出可操作的三级框架，识别影响AI发展的结构条件、传播环境危害的风险放大机制及可观察的生态和社会后果。

Conclusion: 该报告对AI的系统性环境风险进行了概述，提出的框架有助于分析此类风险。

Abstract: Artificial intelligence (AI) is often presented as a key tool for addressing societal challenges, such as climate change. At the same time, AI's environmental footprint is expanding increasingly. This report describes the systemic environmental risks of artificial intelligence, in particular, moving beyond direct impacts such as energy and water usage. Systemic environmental risks of AI are emergent, cross-sector harms to climate, biodiversity, freshwater, and broader socioecological systems that arise primarily from AI's integration into social, economic, and physical infrastructures, rather than its direct resource use, and that propagate through feedbacks, yielding nonlinear, inequitable, and potentially irreversible impacts. While these risks are emergent and quantification is uncertain, this report aims to provide an overview of systemic environmental risks. Drawing on a narrative literature review, we propose a three-level framework that operationalizes systemic risk analysis. The framework identifies the structural conditions that shape AI development, the risk amplification mechanisms that propagate environmental harm, and the impacts that manifest as observable ecological and social consequences. We illustrate the framework in expert-interview-based case studies across agriculture and biodiversity, oil and gas, and waste management.

</details>


### [529] [Industrial AI Robustness Card: Evaluating and Monitoring Time Series Models](https://arxiv.org/abs/2512.11868)
*Alexander Windmann,Benedikt Stratmann,Mariya Lyashenko,Oliver Niggemann*

Main category: cs.CY

TL;DR: 本文介绍工业AI鲁棒性卡片（IARC），用于记录和评估工业时间序列AI模型鲁棒性，并通过案例说明其作用。


<details>
  <summary>Details</summary>
Motivation: 工业AI从业者面临新兴法规和标准中模糊的鲁棒性要求，且缺乏具体的实施协议。

Method: 引入IARC，规定所需字段和测量报告协议，结合漂移监测、不确定性量化和压力测试，并映射到相关欧盟AI法案义务。

Result: 通过生物制药发酵过程的软传感器案例，展示IARC支持可重复的鲁棒性证据和持续监测。

Conclusion: IARC是一种轻量级、与任务无关的协议，可有效用于工业时间序列AI模型的鲁棒性评估和记录。

Abstract: Industrial AI practitioners face vague robustness requirements in emerging regulations and standards but lack concrete, implementation ready protocols. This paper introduces the Industrial AI Robustness Card (IARC), a lightweight, task agnostic protocol for documenting and evaluating the robustness of AI models on industrial time series. The IARC specifies required fields and an empirical measurement and reporting protocol that combines drift monitoring, uncertainty quantification, and stress tests, and it maps these to relevant EU AI Act obligations. A soft sensor case study on a biopharmaceutical fermentation process illustrates how the IARC supports reproducible robustness evidence and continuous monitoring.

</details>


### [530] [Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT](https://arxiv.org/abs/2512.11870)
*Mulham Fawkherji,Bruce Race,Driss Benhaddou*

Main category: cs.CY

TL;DR: 本文聚焦休斯顿公路交通减排问题，提出建立排放基线、评估政策的方法，开发模拟环境，成果对依赖汽车的城市有借鉴意义。


<details>
  <summary>Details</summary>
Motivation: 全球公路交通产生大量温室气体排放和过早死亡案例，休斯顿公路交通排放占比高，要实现2050年净零目标面临挑战，需加速零排放车辆（ZEV）普及并减少车辆行驶里程（VMT）。

Method: 建立公路排放基线，利用社会经济指标和智能交通系统（ITS）评估政策，开发Unity 3D模拟环境进行动态建模和政策情景可视化。

Result: 分析政策选项并确定潜在行动，智能停车、公交激励等措施有助于改善出行方式和系统可靠性。

Conclusion: 文中讨论的指标、指标和技术对追求2050年排放目标的依赖汽车的城市有帮助。

Abstract: Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed.

</details>


### [531] [It's About Time: The Temporal and Modal Dynamics of Copilot Usage](https://arxiv.org/abs/2512.11879)
*Beatriz Costa-Gomes,Sophia Chen,Connie Hsueh,Deborah Morgan,Philipp Schoenegger,Yash Shah,Sam Way,Yuki Zhu,Timothé Adeline,Michael Bhaskar,Mustafa Suleyman,Seth Spielman*

Main category: cs.CY

TL;DR: 分析2025年1 - 9月与微软Copilot的3750万次对话，发现用户使用AI依上下文和设备类型而异，且已将AI融入生活。


<details>
  <summary>Details</summary>
Motivation: 不同于以往对AI使用的分析，关注人们使用AI的方式、时间，而非仅关注做什么。

Method: 分析2025年1月至9月与微软Copilot的3750万次经去标识化处理后的对话。

Result: 使用AI的情况因上下文和设备类型而异，如移动设备上健康是主导话题，桌面设备在工作时间工作和技术是主导话题；不同时间有不同查询高峰。

Conclusion: 用户已迅速将AI融入生活，既作为工作助手，又作为手机伴侣。

Abstract: We analyze 37.5 million deidentified conversations with Microsoft's Copilot between January and September 2025. Unlike prior analyses of AI usage, we focus not just on what people do with AI, but on how and when they do it. We find that how people use AI depends fundamentally on context and device type. On mobile, health is the dominant topic, which is consistent across every hour and every month we observed - with users seeking not just information but also advice. On desktop, the pattern is strikingly different: work and technology dominate during business hours, with "Work and Career" overtaking "Technology" as the top topic precisely between 8 a.m. and 5 p.m. These differences extend to temporal rhythms: programming queries spike on weekdays while gaming rises on weekends, philosophical questions climb during late-night hours, and relationship conversations surge on Valentine's Day. These patterns suggest that users have rapidly integrated AI into the full texture of their lives, as a work aid at their desks and a companion on their phones.

</details>


### [532] [Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological "Censorship"](https://arxiv.org/abs/2512.11883)
*Wenqi Marshall Guo,Qingyun Qian,Khalad Hasan,Shan Du*

Main category: cs.CY

TL;DR: 图像生成模型过度遵循通用审美偏好与用户意图冲突，研究通过实验证实了该模型存在系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 解决图像生成模型过度遵循通用审美偏好，与用户意图冲突，损害用户自主性和审美多元性的问题。

Method: 构建宽谱美学数据集，评估最先进的生成和奖励模型，并通过图像编辑和与真实抽象艺术品对比来验证。

Result: 审美对齐的生成模型常输出常规美观图像，不遵循低质量或负面图像指令，奖励模型会惩罚反审美图像。

Conclusion: 图像生成模型存在系统性偏差。

Abstract: Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.

</details>


### [533] [Advancing Autonomous Driving System Testing: Demands, Challenges, and Future Directions](https://arxiv.org/abs/2512.11887)
*Yihan Liao,Jingyu Zhang,Jacky Keung,Yan Xiao,Yurou Dai*

Main category: cs.CY

TL;DR: 文章探讨自动驾驶系统（ADS）测试实践，通过调查分析现有测试技术不足，总结研究现状并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: ADS在复杂现实环境中可靠性待提升，有效测试对验证性能和降低部署风险至关重要。

Method: 调查100位来自行业和学术界参与者，经专家讨论完善问题，进行定量和定性分析，结合105篇代表性研究。

Result: 现有ADS测试技术难以全面评估现实性能，存在角落案例多样性、仿真与现实差距等问题。

Conclusion: 明确ADS测试关键研究差距，给出综合测试标准、V2X系统跨模型协作等未来研究方向。

Abstract: Autonomous driving systems (ADSs) promise improved transportation efficiency and safety, yet ensuring their reliability in complex real-world environments remains a critical challenge. Effective testing is essential to validate ADS performance and reduce deployment risks. This study investigates current ADS testing practices for both modular and end-to-end systems, identifies key demands from industry practitioners and academic researchers, and analyzes the gaps between existing research and real-world requirements. We review major testing techniques and further consider emerging factors such as Vehicle-to-Everything (V2X) communication and foundation models, including large language models and vision foundation models, to understand their roles in enhancing ADS testing. We conducted a large-scale survey with 100 participants from both industry and academia. Survey questions were refined through expert discussions, followed by quantitative and qualitative analyses to reveal key trends, challenges, and unmet needs. Our results show that existing ADS testing techniques struggle to comprehensively evaluate real-world performance, particularly regarding corner case diversity, the simulation to reality gap, the lack of systematic testing criteria, exposure to potential attacks, practical challenges in V2X deployment, and the high computational cost of foundation model-based testing. By further analyzing participant responses together with 105 representative studies, we summarize the current research landscape and highlight major limitations. This study consolidates critical research gaps in ADS testing and outlines key future research directions, including comprehensive testing criteria, cross-model collaboration in V2X systems, cross-modality adaptation for foundation model-based testing, and scalable validation frameworks for large-scale ADS evaluation.

</details>


### [534] [Should AI Become an Intergenerational Civil Right?](https://arxiv.org/abs/2512.11892)
*Jon Crowcroft,Rute C. Sofia,Dirk Trossen,Vassilis Tsaoussidis*

Main category: cs.CY

TL;DR: 文章指出AI发展中普及与资源约束的矛盾，提出将AI获取权视为代际公民权利，并探讨技术实现路径。


<details>
  <summary>Details</summary>
Motivation: 解决当前AI治理中普及AI与资源约束的问题，平衡公平性、可持续性和长期社会影响。

Method: 规范性分析，借鉴物联网 - 边缘 - 云计算、去中心化推理和能源感知网络的新兴范式，构建AI交付网络的技术架构。

Result: 提出将AI获取作为代际公民权利的理念，给出支持公平获取的技术轨迹和架构。

Conclusion: 将AI视为共享社会基础设施能连接治理原则与系统设计，实现社会公平和环境可持续的AI部署。

Abstract: Artificial Intelligence (AI) is rapidly becoming a foundational layer of social, economic, and cognitive infrastructure. At the same time, the training and large-scale deployment of AI systems rely on finite and unevenly distributed energy, networking, and computational resources. This tension exposes a largely unexamined problem in current AI governance: while expanding access to AI is essential for social inclusion and equal opportunity, unconstrained growth in AI use risks unsustainable resource consumption, whereas restricting access threatens to entrench inequality and undermine basic rights.
  This paper argues that access to AI outputs largely derived from publicly produced knowledge should not be treated solely as a commercial service, but as a fundamental civil interest requiring explicit protection. We show that existing regulatory frameworks largely ignore the coupling between equitable access and resource constraints, leaving critical questions of fairness, sustainability, and long-term societal impact unresolved. To address this gap, we propose recognizing access to AI as an \emph{Intergenerational Civil Right}, establishing a legal and ethical framework that simultaneously safeguards present-day inclusion and the rights of future generations.
  Beyond normative analysis, we explore how this principle can be technically realized. Drawing on emerging paradigms in IoT--Edge--Cloud computing, decentralized inference, and energy-aware networking, we outline technological trajectories and a strawman architecture for AI Delivery Networks that support equitable access under strict resource constraints. By framing AI as a shared social infrastructure rather than a discretionary market commodity, this work connects governance principles with concrete system design choices, offering a pathway toward AI deployment that is both socially just and environmentally sustainable.

</details>


### [535] [Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI](https://arxiv.org/abs/2512.11893)
*Haocheng Lin*

Main category: cs.CY

TL;DR: 本文探讨AI时代四个相关现象，检验新模型表现假设，用混合方法构建理解AI社会影响框架，认为政策制定者应将UBI纳入更广泛治理体系，最后指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI系统快速发展，重塑工作、创造力和经济安全，需研究其带来的社会现象和影响。

Method: 采用整合劳动力市场任务暴露建模、部门扩散映射、政策框架分析和定性话语批判的混合方法。

Result: 构建了理解AI系统社会后果的综合框架，发现新模型可能不如旧模型，用户与AI交互可能产生回音室效应。

Conclusion: 政策制定者应将UBI纳入更广泛治理体系，未来需对AI创造性表现、使用分布公平性和治理标准等进行研究。

Abstract: The accelerating advancement of generative artificial intelligence (AI) systems is reshaping the nature, distribution and meaning of work, creativity, and economic security. This paper investigates four inter-related phenomena in the current AI era: (1) the evolving landscape of employment and the future of work; (2) the diverse patterns of AI adoption across socio-demographic groups, sectors, and geographies; (3) whether universal basic income (UBI) should become a compulsory policy response to the AI revolution; and (4) the implications of AI content policies and model behaviours for human creativity, wellbeing, and everyday decision-making. Furthermore, the paper tests the hypothesis that newer model generations may perform worse than their predecessors, and examines how users' interactions with AI systems may produce echo chambers through sycophantic model alignment. Using a mixed methodology that integrates labour market task-exposure modelling, sectoral diffusion mapping, policy-framework analysis, and qualitative discourse critique, this study develops a comprehensive framework for understanding the societal consequences of AI systems beyond productivity gains. It argues that to foster an inclusive, meaningful, and creative environment, policymakers must treat UBI as one dimension within a broader ecosystem of governance, skills development, creativity preservation, and model design. The paper concludes by outlining future research directions, including systematic evaluation of AI's creative performance across model generations, construction of a taxonomy of AI-usage distribution and equity, and formulation of governance criteria to balance content restrictions with creative freedom.

</details>


### [536] [Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction](https://arxiv.org/abs/2512.11930)
*Mei Jiang,Haihai Shen,Zhuo Luo,Bingdong Li,Wenjing Hong,Ke Tang,Aimin Zhou*

Main category: cs.CY

TL;DR: 现代STEM教育需转变教学方法，现有LLM方法有不足，提出针对SIIP的ERL4SIIP框架。


<details>
  <summary>Details</summary>
Motivation: 现代STEM教育需从被动知识传输转向主动苏格拉底式构建，但现有LLM方法难以支持，存在关键挑战，需要新方法。

Method: 将SIIP问题形式化为POMDP，提出ERL4SIIP框架，集成动态学生模拟器、分层奖励机制和LoRA - Division优化策略。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent.

</details>


### [537] [Mapping AI Risk Mitigations: Evidence Scan and Preliminary AI Risk Mitigation Taxonomy](https://arxiv.org/abs/2512.11931)
*Alexander K. Saeri,Sophia Lloyd George,Jess Graham,Clelia D. Lacarriere,Peter Slattery,Michael Noetel,Neil Thompson*

Main category: cs.CY

TL;DR: 本文引入初步的AI风险缓解分类法，通过对相关框架的快速证据扫描和聚类编码得出，该分类法分四类23子类，虽初步但为AI风险缓解提供起点和协作途径。


<details>
  <summary>Details</summary>
Motivation: 当前AI风险缓解框架零散、术语不一致且覆盖有缺口，需组织AI风险缓解措施并提供通用参考框架。

Method: 对2023 - 2025年发布的13个AI风险缓解框架进行快速证据扫描，提取到含831个AI风险缓解措施的数据库，对措施迭代聚类和编码。

Result: 初步的AI风险缓解分类法将缓解措施分为治理与监督、技术与安全、运营流程、透明与问责四类及23个子类，还发现部分术语使用差异。

Conclusion: 该分类法和相关缓解数据库虽初步，但为AI风险缓解措施的整理综合提供起点，为AI生态不同参与者讨论协作提供途径。

Abstract: Organizations and governments that develop, deploy, use, and govern AI must coordinate on effective risk mitigation. However, the landscape of AI risk mitigation frameworks is fragmented, uses inconsistent terminology, and has gaps in coverage. This paper introduces a preliminary AI Risk Mitigation Taxonomy to organize AI risk mitigations and provide a common frame of reference. The Taxonomy was developed through a rapid evidence scan of 13 AI risk mitigation frameworks published between 2023-2025, which were extracted into a living database of 831 AI risk mitigations. The mitigations were iteratively clustered & coded to create the Taxonomy. The preliminary AI Risk Mitigation Taxonomy organizes mitigations into four categories and 23 subcategories: (1) Governance & Oversight: Formal organizational structures and policy frameworks that establish human oversight mechanisms and decision protocols; (2) Technical & Security: Technical, physical, and engineering safeguards that secure AI systems and constrain model behaviors; (3) Operational Process: processes and management frameworks governing AI system deployment, usage, monitoring, incident handling, and validation; and (4) Transparency & Accountability: formal disclosure practices and verification mechanisms that communicate AI system information and enable external scrutiny. The rapid evidence scan and taxonomy construction also revealed several cases where terms like 'risk management' and 'red teaming' are used widely but refer to different responsible actors, actions, and mechanisms of action to reduce risk. This Taxonomy and associated mitigation database, while preliminary, offers a starting point for collation and synthesis of AI risk mitigations. It also offers an accessible, structured way for different actors in the AI ecosystem to discuss and coordinate action to reduce risks from AI.

</details>


### [538] [Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching](https://arxiv.org/abs/2512.11934)
*Adeleh Mazaherian,Erfan Nourbakhsh*

Main category: cs.CY

TL;DR: 研究对谷歌应用商店热门AI教育应用的用户评价进行情感驱动评估，发现多数应用获积极评价，不同类型应用表现有差异，还提出未来发展建议。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能快速融入教育推动电子教学数字化转型，但用户对AI教育应用的看法研究不足，需评估其功效、挑战和教学意义。

Method: 抓取应用数据和评价，用RoBERTa进行二元情感分类，GPT - 4o提取关键点，GPT - 5合成正负主题。将应用分为七类。

Result: 多数为积极评价，作业类应用表现好，语言/LMS类应用因不稳定和功能有限表现不佳。积极方面强调效率等，消极方面集中在付费墙等问题。作业辅助类优于专业工具类。

Conclusion: 提出未来混合人工智能 - 人类模型、VR/AR等生态系统，为开发者和政策制定者提供建议，强调生成式人工智能在推动电子教学、促进公平创新环境方面的作用。

Abstract: The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent).

</details>


### [539] [A neuro-symbolic framework for accountability in public-sector AI](https://arxiv.org/abs/2512.12109)
*Allen Daniel Sunny*

Main category: cs.CY

TL;DR: 本文开发了一个基于法律的可解释性框架，用于让自动化资格系统的决策理由符合CalFresh项目的法律规定，并通过案例评估验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 自动化资格系统生成的解释常不符合法律规定，需要开发一种可使决策理由与相关法律约束相联系的框架。

Method: 结合从州政策和程序手册导出的资格要求结构化本体、将法定逻辑表达为可验证形式的规则提取流程，以及基于求解器的推理层。

Result: 案例评估表明该框架能检测出不符合法律的解释，突出违反的资格规则，使自动决策的依据可追溯和可质疑。

Conclusion: 该框架能有效实现程序问责，确保自动化资格系统的决策符合相关法律规定。

Abstract: Automated eligibility systems increasingly determine access to essential public benefits, but the explanations they generate often fail to reflect the legal rules that authorize those decisions. This thesis develops a legally grounded explainability framework that links system-generated decision justifications to the statutory constraints of CalFresh, California's Supplemental Nutrition Assistance Program. The framework combines a structured ontology of eligibility requirements derived from the state's Manual of Policies and Procedures (MPP), a rule extraction pipeline that expresses statutory logic in a verifiable formal representation, and a solver-based reasoning layer to evaluate whether the explanation aligns with governing law. Case evaluations demonstrate the framework's ability to detect legally inconsistent explanations, highlight violated eligibility rules, and support procedural accountability by making the basis of automated determinations traceable and contestable.

</details>


### [540] [The Art of Storytelling in Authoritarian Regimes: Crafting State Narratives on Chinese Social Media](https://arxiv.org/abs/2512.11875)
*Ting Luo,Yan Wang*

Main category: cs.CY

TL;DR: 本文研究威权政权如何构建重大政治事件的国家叙事，发现事件合法性影响及公民验证能力会影响叙事构建，揭示宣传是适应特定情境的复杂叙事过程。


<details>
  <summary>Details</summary>
Motivation: 研究威权政权如何构建关于重大政治事件的国家叙事。

Method: 基于叙事政策框架和威权宣传研究，提出影响叙事构建的两个维度，对中国社交媒体帖子进行定量叙事分析，提取主谓宾三元组。

Result: 事件的合法性影响塑造政权叙事努力和叙事中强调的信念，公民验证能力可平衡国家叙事自上而下操纵和自下而上响应的战略选择。

Conclusion: 宣传是适应特定情境的复杂叙事构建过程，动态叙事有助于维持威权政权韧性。

Abstract: This article examines how authoritarian regimes construct state narratives about politically consequential events. Building on the narrative policy framework and existing research on authoritarian propaganda, we propose two dimensions that shape narrative construction: legitimacy implications -- whether events enhance or threaten regime legitimacy, and citizen verification capacity -- the extent to which citizens can evaluate official narratives through alternative sources. Using quantitative narrative analysis of Chinese social media posts by government, state media, and celebrity accounts, we extract subject-verb-object (SVO) triplets to map dominant narrative structures across four major events. Our findings show that legitimacy implications of the event shape regime's efforts in storytelling and the beliefs highlighted in the narratives, while citizen's verification capacity could balance the strategic choice between a top-down manipulation and bottom-up responsiveness of state narratives. Together, the results reveal propaganda as a complex process of narrative construction adaptive to specific contexts, offering new insights into how dynamic storytelling sustains authoritarian resilience.

</details>


### [541] [Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance](https://arxiv.org/abs/2512.13658)
*Mohammadreza Molavi,Mohammad Moein,Mohammadreza Tavakoli,Abdolali Faraji,Stefan T. Mol,Gábor Kismihók*

Main category: cs.CY

TL;DR: 随着在线学习发展，个性化需求凸显，本文提出框架自动化评估教育资源与学习目标的一致性，实验表明嵌入对齐分数有助于可扩展的个性化学习。


<details>
  <summary>Details</summary>
Motivation: 在线学习中个性化需求增长，但教育资源选择难，使用大语言模型生成学习资源需人工审核对齐情况，成本高且可扩展性受限。

Method: 使用人类生成材料对基于大语言模型的文本嵌入模型进行基准测试，选择最优模型应用于大语言模型生成的资源，并进行三组实验。

Result: 最准确的模型（Voyage）检测对齐的准确率达79%，应用于大语言模型生成资源时准确率为83%，实验显示更高的对齐分数与更好的学习表现正相关。

Conclusion: 基于嵌入的对齐分数可通过确认与学习成果的对齐，促进可扩展的个性化学习，让教师专注于满足学习者多样化需求。

Abstract: As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [542] [Cargo Sherlock: An SMT-Based Checker for Software Trust Costs](https://arxiv.org/abs/2512.12553)
*Muhammad Hassnain,Anirudh Basu,Ethan Ng,Caleb Stanford*

Main category: cs.LO

TL;DR: 本文提出量化第三方软件依赖信任度的正式框架，以应对供应链攻击对开源软件生态的威胁，在Rust库中实现Cargo Sherlock工具，评估显示其能识别攻击且性能良好。


<details>
  <summary>Details</summary>
Motivation: 供应链攻击威胁开源软件生态系统，需要一种量化第三方软件依赖信任的方法。

Method: 结合软件分析工具数据和元数据构建软件依赖的一阶关系模型，基于最小信任问题形式化以获得整体“信任成本”，在Cargo Sherlock中实现该框架。

Result: Cargo Sherlock能识别合成生成的供应链攻击、已知的打字诈骗和AI维护不良的 crate 事件，并且性能可适用于有许多依赖项的 Rust crate。

Conclusion: 提出的正式框架和实现的工具可用于应对供应链攻击，保障开源软件生态安全。

Abstract: Supply chain attacks threaten open-source software ecosystems. This paper proposes a formal framework for quantifying trust in third-party software dependencies that is both formally checkable - formalized in satisfiability modulo theories (SMT) - while at the same time incorporating human factors, like the number of downloads, authors, and other metadata that are commonly used to identify trustworthy software in practice. We use data from both software analysis tools and metadata to build a first-order relational model of software dependencies; to obtain an overall "trust cost" combining these factors, we propose a formalization based on the minimum trust problem which asks for the minimum cost of a set of assumptions which can be used to prove that the code is safe. We implement these ideas in Cargo Sherlock, targeted for Rust libraries (crates), incorporating a list of candidate assumptions motivated by quantifiable trust metrics identified in prior work. Our evaluation shows that Cargo Sherlock can be used to identify synthetically generated supply chain attacks and known incidents involving typosquatted and poorly AI-maintained crates, and that its performance scales to Rust crates with many dependencies.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [543] [V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval](https://arxiv.org/abs/2512.12284)
*Donghyuk Kim,Sejeong Yang,Wonjin Shin,Joo-Young Kim*

Main category: eess.IV

TL;DR: 提出名为 V - Rex 的软硬件协同设计加速器用于流式视频大语言模型推理，可在边缘设备实时高效运行且精度损失小。


<details>
  <summary>Details</summary>
Motivation: 流式视频大语言模型在处理连续视频输入时键值缓存大幅增长，存在计算、数据传输和精度问题，尤其在边缘部署时问题更严重。

Method: 提出免训练的动态键值缓存检索算法 ReSV 减少缓存内存，设计带有动态键值缓存检索引擎（DRE）的低延迟硬件加速器。

Result: 实现 3.9 - 8.3 FPS 的实时推理，DRE 仅占 2.2% 功耗和 2.0% 面积，系统速度提升 1.9 - 19.7 倍，能效提升 3.1 - 18.5 倍，精度损失可忽略不计。

Conclusion: 首次全面解决算法和硬件层面的键值缓存检索问题，使资源受限边缘设备能实现实时流式视频大语言模型推理。

Abstract: Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.
  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [544] [Scalable Spatial Stream Network (S3N) Models](https://arxiv.org/abs/2512.12398)
*Jessica P. Kunke,Julian D. Olden,Tyler H. McCormick*

Main category: stat.ME

TL;DR: 本文介绍可扩展的空间流网络（S3N）模型，用于解决现有模型在区域或大陆尺度上应用的计算和时间问题，并通过验证分析展示其优势，推动了淡水鱼分布大规模制图的发展。


<details>
  <summary>Details</summary>
Motivation: 现有空间流网络（SSN）模型在区域或大陆尺度应用时计算和时间成本高，需要更高效的模型用于淡水生态研究和生物多样性管理。

Method: 引入新的可扩展空间流网络（S3N）模型，将最近邻高斯过程扩展以纳入生态相关的空间依赖性。

Result: S3N能准确恢复空间和协方差参数，与标准 SSN 实现相比，偏差和方差更小。

Conclusion: S3N 模型是淡水鱼分布大规模制图和量化环境驱动因素对大范围河网影响的关键进步。

Abstract: Understanding how habitats shape species distributions and abundances across spatially complex, dendritic freshwater networks remains a longstanding and fundamental challenge in ecology, with direct implications for effective biodiversity management and conservation. Existing spatial stream network (SSN) models adapt spatial process models to river networks by creating covariance functions that account for stream distance, but preprocessing and estimation with these models is both computationally and time intensive, thus precluding the application of these models to regional or continental scales. This paper introduces a new class of Scalable Spatial Stream Network (S3N) models, which extend nearest-neighbor Gaussian processes to incorporate ecologically relevant spatial dependence while greatly improving computational efficiency. The S3N framework enables scalable modeling of spatial stream networks, demonstrated here for 285 fish species in the Ohio River Basin (>4,000 river km). Validation analyses show that S3N accurately recovers spatial and covariance parameters, even with reduced bias and variance compared to standard SSN implementations. These results represent a key advancement toward large-scale mapping of freshwater fish distributions and quantifying the influence of environmental drivers across extensive river networks.

</details>


### [545] [Sleep pattern profiling using a finite mixture of contaminated multivariate skew-normal distributions on incomplete data](https://arxiv.org/abs/2512.12464)
*Jason Pillay,Cristina Tortora,Antonio Punzo,Andriette Bekker*

Main category: stat.ME

TL;DR: 提出统一的基于模型的聚类方法，用污染多元偏态正态分布建模，用改进EM算法估计参数，在模拟和实际数据中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学数据有缺失值、离群点和偏态等特征，传统聚类前的预处理与聚类阶段目标函数不同，需统一方法。

Method: 提出用污染多元偏态正态分布对每个聚类建模，用改进的EM算法获得模型参数的最大似然估计。

Result: 模拟研究表明该模型在聚类准确性和离群点检测上优于现有方法，应用于CCSHS数据集无需预处理识别出五组不同睡眠者。

Conclusion: 所提统一聚类方法有效，能处理医学数据的复杂特征。

Abstract: Medical data often exhibit characteristics that make cluster analysis particularly challenging, such as missing values, outliers, and cluster features like skewness. Typically, such data would need to be preprocessed -- by cleaning outliers and missing values -- before clustering could be performed. However, these preliminary steps rely on objective functions different from those used in the clustering stage. In this paper, we propose a unified model-based clustering approach that simultaneously handles atypical observations, missing values, and cluster-wise skewness within a single framework. Each cluster is modelled using a contaminated multivariate skew-normal distribution -- a convenient two-component mixture of multivariate skew-normal densities -- in which one component represents the main data (the "bulk") and the other captures potential outliers. From an inferential perspective, we implement and use a variant of the EM algorithm to obtain the maximum likelihood estimates of the model parameters. Simulation studies demonstrate that the proposed model outperforms existing approaches in both clustering accuracy and outlier detection, across low- and high-dimensional settings, even in the presence of substantial missingness. The method is further applied to the Cleveland Children's Sleep and Health Study (CCSHS), a dataset characterised by incomplete observations. Without any preprocessing, the proposed approach identifies five distinct groups of sleepers, revealing meaningful differences in sleeper typologies.

</details>


### [546] [A Bayesian approach to learning mixtures of nonparametric components](https://arxiv.org/abs/2512.12988)
*Yilei Zhang,Yun Wei,Aritra Guha,XuanLong Nguyen*

Main category: stat.ME

TL;DR: 本文研究含非参数混合分量的有限混合模型，用贝叶斯非参数方法建模，给出分量分布可识别条件，建立后验收缩行为，开发MCMC算法并验证有效性，理论上分量密度后验收缩率近多项式。


<details>
  <summary>Details</summary>
Motivation: 对潜在子群体分布做参数假设在很多应用中不现实，因此需要对混合分量进行非参数建模。

Method: 采用贝叶斯非参数建模方法，假设数据总体由潜在分量分布的有限混合生成，每个分量赋予如狄利克雷过程混合的贝叶斯非参数先验；开发高效MCMC算法进行后验推断。

Result: 给出单个混合分量分布可识别的条件，建立数据总体密度和潜在混合分量密度的后验收缩行为；通过模拟研究和真实数据表明能有效学习潜在子群体的复杂分布；理论上分量密度的后验收缩率近多项式。

Conclusion: 用贝叶斯非参数方法对有限混合模型的非参数混合分量建模是有效的，后验收缩率相比反卷积估计混合测度有显著提升。

Abstract: Mixture models are widely used in modeling heterogeneous data populations. A standard approach of mixture modeling is to assume that the mixture component takes a parametric kernel form, while the flexibility of the model can be obtained by using a large or possibly unbounded number of such parametric kernels. In many applications, making parametric assumptions on the latent subpopulation distributions may be unrealistic, which motivates the need for nonparametric modeling of the mixture components themselves. In this paper we study finite mixtures with nonparametric mixture components, using a Bayesian nonparametric modeling approach. In particular, it is assumed that the data population is generated according to a finite mixture of latent component distributions, where each component is endowed with a Bayesian nonparametric prior such as the Dirichlet process mixture. We present conditions under which the individual mixture component's distributions can be identified, and establish posterior contraction behavior for the data population's density, as well as densities of the latent mixture components. We develop an efficient MCMC algorithm for posterior inference and demonstrate via simulation studies and real-world data illustrations that it is possible to efficiently learn complex distributions for the latent subpopulations. In theory, the posterior contraction rate of the component densities is nearly polynomial, which is a significant improvement over the logarithm convergence rate of estimating mixing measures via deconvolution.

</details>


### [547] [Robust Variational Bayes by Min-Max Median Aggregation](https://arxiv.org/abs/2512.12676)
*Jiawei Yan,Ju Liu,Weidong Liu,Jiyuan Tu*

Main category: stat.ME

TL;DR: 提出处理数据污染和异常值的变分贝叶斯框架，分数据子集并优化，用min - max改进，分情况处理，理论分析显示效果好，模拟研究验证。


<details>
  <summary>Details</summary>
Motivation: 有效处理数据集中的污染和异常值。

Method: 将数据划分为m个不相交子集，基于鲁棒聚合原则提出联合优化问题，用min - max中位数公式替代平均KL散度，分情况处理局部潜变量，进行非渐近分析。

Result: 两阶段方法比直接聚合m次幂局部后验的近似误差小，为后验均值建立了接近最优的统计率。

Conclusion: 所提方法有效，推进了min - max中位数估计器相关理论，模拟研究证明了方法的有效性。

Abstract: We propose a robust and scalable variational Bayes (VB) framework designed to effectively handle contamination and outliers in dataset. Our approach partitions the data into $m$ disjoint subsets and formulates a joint optimization problem based on robust aggregation principles. A key insight is that the full posterior distribution is equivalent to the minimizer of the mean Kullback-Leibler (KL) divergence from the $m$-powered local posterior distributions. To enhance robustness, we replace the mean KL divergence with a min-max median formulation. The min-max formulation not only ensures consistency between the KL minimizer and the Evidence Lower Bound (ELBO) maximizer but also facilitates the establishment of improved statistical rates for the mean of variational posterior. We observe a notable discrepancy in the $m$-powered marginal log likelihood function contingent on the presence of local latent variables. To address this, we treat these two scenarios separately to guarantee the consistency of the aggregated variational posterior. Specifically, when local latent variables are present, we introduce an aggregate-and-rescale strategy. Theoretically, we provide a non-asymptotic analysis of our proposed posterior, incorporating a refined analysis of Bernstein-von Mises (BvM) theorem to accommodate a diverging number of subsets $m$. Our findings indicate that the two-stage approach yields a smaller approximation error compared to directly aggregating the $m$-powered local posteriors. Furthermore, we establish a nearly optimal statistical rate for the mean of the proposed posterior, advancing existing theories related to min-max median estimators. The efficacy of our method is demonstrated through extensive simulation studies.

</details>


### [548] [Machine learning to optimize precision in the analysis of randomized trials: A journey in pre-specified, yet data-adaptive learning](https://arxiv.org/abs/2512.13610)
*Laura B. Balzer,Mark J. van der Laan,Maya L. Petersen*

Main category: stat.ME

TL;DR: 受SEARCH试验启发，开发并实施基于机器学习的协变量调整方法，介绍该方法及评估流程，给出8个试验结果，提供实用建议。


<details>
  <summary>Details</summary>
Motivation: 受SEARCH Universal HIV Test - and - Treat Trial启发，开发基于机器学习的协变量调整方法以提高试验分析精度。

Method: 采用带自适应预指定的目标机器学习估计（TMLE），使用样本分割选择最小化交叉验证方差估计的估计器组合；通过参数和等离子体模式模拟评估有限样本性能；预指定统计分析计划并实时揭盲。

Result: 给出该方法在8个最近发表试验（2022 - 2024）主要预指定分析中的结果。

Conclusion: 给出实用建议，并邀请在下次试验主要分析中应用该方法。

Abstract: Covariate adjustment is an approach to improve the precision of trial analyses by adjusting for baseline variables that are prognostic of the primary endpoint. Motivated by the SEARCH Universal HIV Test-and-Treat Trial (2013-2017), we tell our story of developing, evaluating, and implementing a machine learning-based approach for covariate adjustment. We provide the rationale for as well as the practical concerns with such an approach for estimating marginal effects. Using schematics, we illustrate our procedure: targeted machine learning estimation (TMLE) with Adaptive Pre-specification. Briefly, sample-splitting is used to data-adaptively select the combination of estimators of the outcome regression (i.e., the conditional expectation of the outcome given the trial arm and covariates) and known propensity score (i.e., the conditional probability of being randomized to the intervention given the covariates) that minimizes the cross-validated variance estimate and, thereby, maximizes empirical efficiency. We discuss our approach for evaluating finite sample performance with parametric and plasmode simulations, pre-specifying the Statistical Analysis Plan, and unblinding in real-time on video conference with our colleagues from around the world. We present the results from applying our approach in the primary, pre-specified analysis of 8 recently published trials (2022-2024). We conclude with practical recommendations and an invitation to implement our approach in the primary analysis of your next trial.

</details>


### [549] [A fine-grained look at causal effects in causal spaces](https://arxiv.org/abs/2512.11919)
*Junhyung Park,Yuqing Zhou*

Main category: stat.ME

TL;DR: 本文从事件层面研究因果效应，在因果空间的测度论框架下引入相关定义和量化指标。


<details>
  <summary>Details</summary>
Motivation: 传统定量研究多在变量层面研究因果效应，而现代数据中原始变量缺乏语义结构，难以提出有意义的因果问题，因此需要更细粒度的视角。

Method: 在因果空间的测度论框架下，引入判断因果效应是否存在的二元定义，证明其相关性质，提供量化因果效应对事件的强度和性质的指标。

Result: 给出判断因果效应的定义和性质，能将常见治疗效应测度作为特殊情况恢复。

Conclusion: 从事件层面研究因果效应为因果研究提供了更细粒度的视角。

Abstract: The notion of causal effect is fundamental across many scientific disciplines. Traditionally, quantitative researchers have studied causal effects at the level of variables; for example, how a certain drug dose (W) causally affects a patient's blood pressure (Y). However, in many modern data domains, the raw variables-such as pixels in an image or tokens in a language model-do not have the semantic structure needed to formulate meaningful causal questions. In this paper, we offer a more fine-grained perspective by studying causal effects at the level of events, drawing inspiration from probability theory, where core notions such as independence are first given for events and sigma-algebras, before random variables enter the picture. Within the measure-theoretic framework of causal spaces, a recently introduced axiomatisation of causality, we first introduce several binary definitions that determine whether a causal effect is present, as well as proving some properties of them linking causal effect to (in)dependence under an intervention measure. Further, we provide quantifying measures that capture the strength and nature of causal effects on events, and show that we can recover the common measures of treatment effect as special cases.

</details>


### [550] [Robust Outlier Detection and Low-Latency Concept Drift Adaptation for Data Stream Regression: A Dual-Channel Architecture](https://arxiv.org/abs/2512.12289)
*Bingbing Wang,Shengyan Sun,Jiaqi Wang,Yu Tang*

Main category: stat.ME

TL;DR: 提出用于联合离群值和概念漂移检测的鲁棒回归框架，有双渠道决策过程和EWMAD - DT检测器，实验显示优越性能。


<details>
  <summary>Details</summary>
Motivation: 离群值检测和概念漂移检测两个挑战研究多分开进行，回归中的联合检测机制未充分探索。

Method: 提出新颖的鲁棒回归框架，引入双渠道决策过程，开发EWMAD - DT检测器。

Result: 在合成和真实世界数据集上的综合实验表明，统一框架结合EWMAD - DT在离群值和概念漂移共存时仍有优越检测性能。

Conclusion: 所提出的统一框架结合EWMAD - DT在联合检测离群值和概念漂移方面效果良好。

Abstract: Outlier detection and concept drift detection represent two challenges in data analysis. Most studies address these issues separately. However, joint detection mechanisms in regression remain underexplored, where the continuous nature of output spaces makes distinguishing drifts from outliers inherently challenging. To address this, we propose a novel robust regression framework for joint outlier and concept drift detection. Specifically, we introduce a dual-channel decision process that orchestrates prediction residuals into two coupled logic flows: a rapid response channel for filtering point outliers and a deep analysis channel for diagnosing drifts. We further develop the Exponentially Weighted Moving Absolute Deviation with Distinguishable Types (EWMAD-DT) detector to autonomously differentiate between abrupt and incremental drifts via dynamic thresholding. Comprehensive experiments on both synthetic and real-world datasets demonstrate that our unified framework, enhanced by EWMAD-DT, exhibits superior detection performance even when point outliers and concept drifts coexist.

</details>


### [551] [Actively Learning Joint Contours of Multiple Computer Experiments](https://arxiv.org/abs/2512.13530)
*Shih-Ni Prim,Kevin R. Quinlan,Paul Hawkins,Jagadeesh Movva,Annie S. Booth*

Main category: stat.ME

TL;DR: 本文提出联合轮廓定位（jCL）方案解决多独立计算机实验同时达到指定响应值问题，显著优于现有策略。


<details>
  <summary>Details</summary>
Motivation: 受飞行器飞行中旋转扭矩计算机实验启发，旨在确定产生零扭矩力的稳定飞行条件。

Method: 提出jCL方案，在探索多响应面和利用相交轮廓学习间取得平衡，采用浅和深高斯过程代理模型，该方案适用于能提供后验预测分布的任何代理模型。

Result: jCL设计显著优于现有单响应CL策略。

Conclusion: jCL方案能有效定位联合轮廓，可用于解决多独立计算机实验同时达到指定响应值的问题。

Abstract: Contour location$\unicode{x2014}$the process of sequentially training a surrogate model to identify the design inputs that result in a pre-specified response value from a single computer experiment$\unicode{x2014}$is a well-studied active learning problem. Here, we tackle a related but distinct problem: identifying the input configuration that returns pre-specified values of multiple independent computer experiments simultaneously. Motivated by computer experiments of the rotational torques acting upon a vehicle in flight, we aim to identify stable flight conditions which result in zero torque forces. We propose a "joint contour location" (jCL) scheme that strikes a strategic balance between exploring the multiple response surfaces while exploiting learning of the intersecting contours. We employ both shallow and deep Gaussian process surrogates, but our jCL procedure is applicable to any surrogate that can provide posterior predictive distributions. Our jCL designs significantly outperform existing (single response) CL strategies, enabling us to efficiently locate the joint contour of our motivating computer experiments.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [552] [A Disproof of Large Language Model Consciousness: The Necessity of Continual Learning for Consciousness](https://arxiv.org/abs/2512.12802)
*Erik Hoel*

Main category: q-bio.NC

TL;DR: 研究意识理论约束条件，反驳当代大语言模型有意识，指出基于持续学习的意识理论满足人类意识理论约束，支持持续学习与意识关联假设。


<details>
  <summary>Details</summary>
Motivation: 分析可证伪且非平凡的意识理论的约束条件，探讨当代大语言模型的意识问题。

Method: 运用新的接近论证分析意识理论对当代大语言模型意识的约束，研究基于持续学习的意识理论。

Result: 反驳了当代大语言模型有意识，基于持续学习的意识理论满足人类意识理论的严格形式约束。

Conclusion: 若持续学习与人类意识相关，大语言模型缺乏持续学习能力可能是其无意识的原因。

Abstract: The requirements for a falsifiable and non-trivial theory of consciousness significantly constrain such theories. Specifically, recent research on the Unfolding Argument and the Substitution Argument has given us formal tools to analyze requirements for a theory of consciousness. I show via a new Proximity Argument that these requirements especially constrain the potential consciousness of contemporary Large Language Models (LLMs) because of their proximity to systems that are equivalent to LLMs in terms of input/output function; yet, for these functionally equivalent systems, there cannot be any non-trivial theory of consciousness that judges them conscious. This forms the basis of a disproof of contemporary LLM consciousness. I then show a positive result, which is that theories of consciousness based on (or requiring) continual learning do satisfy the stringent formal constraints for a theory of consciousness in humans. Intriguingly, this work supports a hypothesis: If continual learning is linked to consciousness in humans, the current limitations of LLMs (which do not continually learn) are intimately tied to their lack of consciousness.

</details>


### [553] [A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments](https://arxiv.org/abs/2512.13517)
*Raymond Khazoum,Daniela Fernandes,Aleksandr Krylov,Qin Li,Stephane Deny*

Main category: q-bio.NC

TL;DR: 提出人类心理旋转的机制模型，该模型由三层组件构成，能捕捉参与者表现、反应时间和行为，证明整合多种表征建模人类思维的有效性。


<details>
  <summary>Details</summary>
Motivation: 人类心理旋转是心理模拟和空间世界建模的基础示例，旨在构建人类心理旋转的机制模型。

Method: 利用深度、等变和神经符号学习的进展构建三层组件模型，参考大量实验文献并进行VR实验。

Result: 模型能很好捕捉参与者在实验中的表现、反应时间和行为，通过系统消融实验证明各组件必要性。

Conclusion: 为人类空间推理的深度神经模型添砖加瓦，证明整合深层、等变和符号表征对模拟人类思维的有效性。

Abstract: Mental rotation -- the ability to compare objects seen from different viewpoints -- is a fundamental example of mental simulation and spatial world modelling in humans. Here we propose a mechanistic model of human mental rotation, leveraging advances in deep, equivariant, and neuro-symbolic learning. Our model consists of three stacked components: (1) an equivariant neural encoder, taking images as input and producing 3D spatial representations of objects, (2) a neuro-symbolic object encoder, deriving symbolic descriptions of objects from these spatial representations, and (3) a neural decision agent, comparing these symbolic descriptions to prescribe rotation simulations in 3D latent space via a recurrent pathway. Our model design is guided by the abundant experimental literature on mental rotation, which we complemented with experiments in VR where participants could at times manipulate the objects to compare, providing us with additional insights into the cognitive process of mental rotation. Our model captures well the performance, response times and behavior of participants in our and others' experiments. The necessity of each model component is shown through systematic ablations. Our work adds to a recent collection of deep neural models of human spatial reasoning, further demonstrating the potency of integrating deep, equivariant, and symbolic representations to model the human mind.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [554] [Understanding Structural Representation in Foundation Models for Polymers](https://arxiv.org/abs/2512.11881)
*Nathaniel H. Park,Eduardo Soares,Victor Y. Shirasuna,Tiffany J. Callahan,Sara Capponi,Emilio Vital Brazil*

Main category: cond-mat.soft

TL;DR: 本文提出基于SMILES的聚合物图表示的基础模型，在28个基准数据集上表现出色，强调控制实验对化学基础模型设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决聚合物基础模型开发中训练数据稀缺、缺乏标准基准等多方面挑战，问题核心与聚合物结构表示有关。

Method: 提出基于SMILES的聚合物图表示的新基础模型。

Result: 模型在28个不同基准数据集上表现优异；控制实验显示该方法是基于语言的基础模型中表现出色的聚合物表示方法；SMILES表示具有强不变性，很多变体达先进水平；对误差源和注意力图的分析验证了控制实验结果。

Conclusion: 强调控制实验对检验人为假设的重要性，避免限制化学基础模型及其结构表示的合理设计。

Abstract: From the relative scarcity of training data to the lack of standardized benchmarks, the development of foundation models for polymers face significant and multi-faceted challenges. At the core, many of these issues are tied directly to the structural representation of polymers and here, we present a new foundation model using a SMILES-based polymer graph representation. This approach allows representation of critical polymer architectural features and connectivity that are not available in other SMILES-based representations. The developed polymer foundation model exhibited excellent performance on 28 different benchmark datasets. Critical evaluation of the developed representation against other variations in control experiments reveals this approach to be a highly performant method of representing polymers in language-based foundation models. These control experiments also reveal a strong invariance of all SMILES representations, with many variations achieving state-of-the-art or near state-of-the-art performance, including those which are chemically or semantically invalid. Examination of error sources and attention maps for the evaluated representations corroborate the findings of the control experiments, showing that chemistry language models based on SMILES interpolate over all sequence space for prediction tasks, not only those of semantically valid inputs. Overall, this work highlights the importance of control experiments as a check on human-imposed assumptions that can limit rational design of both chemistry foundation models and their underlying structural representations.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [555] [Monetary Policy, Uncertainty, and Credit Supply](https://arxiv.org/abs/2512.12255)
*Eric Vansteenberghe*

Main category: econ.GN

TL;DR: 研究银行主观通胀预测分散性作为货币政策向信贷供给传导渠道，模型预测及实证表明通胀不确定性和信念不对称影响信贷条件。


<details>
  <summary>Details</summary>
Motivation: 探究银行主观通胀预测分散性如何成为货币政策向信贷供给的传导渠道。

Method: 扩展蒙蒂 - 克莱因垄断银行模型，结合风险厌恶、主观信念和模糊厌恶；使用法国AnaCredit贷款级数据估计有限混合密度回归。

Result: 更高主观不确定性和不对称性增加平均贷款利率并使分布倾斜，影响财务受限企业；从指标25%分位数到75%分位数使平均借款成本增加超10个基点，非金融企业年利息支出增加约5亿欧元；预测分歧影响较弱且不系统。

Conclusion: 通胀预期的不确定性和不对称性是信贷条件的独立且有力驱动因素，对理解货币政策通过银行部门传导很重要。

Abstract: This paper investigates how dispersion in banks' subjective inflation forecasts is a channel of the transmission of monetary policy to credit supply. We extend the Monti-Klein model of monopolistic banking by incorporating risk aversion, subjective beliefs, and ambiguity aversion. The model predicts that greater inflation uncertainty or asymmetry in beliefs raises equilibrium loan rates and amplifies credit rationing. Using AnaCredit loan-level data for France, we estimate finite-mixture density regressions that allow for latent heterogeneity in loan pricing. Empirically, we find that higher subjective uncertainty and asymmetry both increase average lending rates and skew their distribution, disproportionately affecting financially constrained firms in the right tail. Quantitatively, moving from the 25th to the 75th percentile of our indicators raises average borrowing costs by more than 10 basis points, which translates into roughly 0.5 billion euros of additional annual interest expenses for non-financial corporations. By contrast, forecast disagreement has a weaker and less systematic effect. Taken together, these results show that uncertainty and asymmetry in inflation expectations are independent and powerful drivers of credit conditions, underscoring their importance for understanding monetary policy transmission through the banking sector.

</details>


### [556] [Explainable Artificial Intelligence for Economic Time Series: A Comprehensive Review and a Systematic Taxonomy of Methods and Concepts](https://arxiv.org/abs/2512.12506)
*Agustín García-García,Pablo Hidalgo,Julio E. Sandubete*

Main category: econ.GN

TL;DR: 文章综述经济时间序列XAI文献，提出分类法，综合适配方法，连接可解释性与因果和政策分析，给出决策应用指导。


<details>
  <summary>Details</summary>
Motivation: 机器学习预测器在计算经济学中难审计和用于政策，且经济时间序列特性使标准解释技术不可靠或经济上不合理，需XAI。

Method: 提出分类法按解释机制和时间序列兼容性对方法分类，综合特定适配方法，通过介入归因和反事实推理连接可解释性与因果和政策分析。

Result: 完成文献综述和分类，综合适配方法，建立可解释性与因果和政策分析联系。

Conclusion: 讨论可解释架构，针对决策级应用给出指导，强调不确定性和动态性对结构变化的指示作用。

Abstract: Explainable Artificial Intelligence (XAI) is increasingly required in computational economics, where machine-learning forecasters can outperform classical econometric models but remain difficult to audit and use for policy. This survey reviews and organizes the growing literature on XAI for economic time series, where autocorrelation, non-stationarity, seasonality, mixed frequencies, and regime shifts can make standard explanation techniques unreliable or economically implausible. We propose a taxonomy that classifies methods by (i) explanation mechanism: propagation-based approaches (e.g., Integrated Gradients, Layer-wise Relevance Propagation), perturbation and game-theoretic attribution (e.g., permutation importance, LIME, SHAP), and function-based global tools (e.g., Accumulated Local Effects); (ii) time-series compatibility, including preservation of temporal dependence, stability over time, and respect for data-generating constraints. We synthesize time-series-specific adaptations such as vector- and window-based formulations (e.g., Vector SHAP, WindowSHAP) that reduce lag fragmentation and computational cost while improving interpretability. We also connect explainability to causal inference and policy analysis through interventional attributions (Causal Shapley values) and constrained counterfactual reasoning. Finally, we discuss intrinsically interpretable architectures (notably attention-based transformers) and provide guidance for decision-grade applications such as nowcasting, stress testing, and regime monitoring, emphasizing attribution uncertainty and explanation dynamics as indicators of structural change.

</details>


### [557] [Machine Learning Predictive Analytics for Social Media Enabled Women's Economic Empowerment in Pakistan](https://arxiv.org/abs/2512.12685)
*Maryam Arif,Soban Saeed*

Main category: econ.GN

TL;DR: 研究巴基斯坦年轻女性赋权与经济增长关系，发现社交媒体虽能促进经济但创业参与率低，还存在诸多障碍和政策宣传不足问题。


<details>
  <summary>Details</summary>
Motivation: 探究年轻女性赋权与巴基斯坦经济增长的相互作用，以及社交媒体使用对其商业和经济发展的促进作用。

Method: 采用混合研究方法，结合线上线下随机抽样对51名受访者进行调查，利用社交媒体使用（n = 1000）和创业（n = 1092）的现有数据集，通过无监督学习识别社交媒体参与模式，用监督模型进行创业预测。

Result: 39.4%受访者认为社交媒体能促进经济增收；仅14%受访者参与创业；YouTube和WhatsApp使用频率高；存在网络骚扰、数字素养低、文化限制等障碍；52.9%受访者不知政府支持女性创业的举措。

Conclusion: 社交媒体使用与商业采用之间存在较大差距，巴基斯坦存在阻碍女性创业的因素以及政策宣传不足的问题。

Abstract: Our study investigates the interplay between young women's empowerment and Pakistan's economic growth, focusing on how social media use enhances their businesses and drives economic advancement. We utilize a mixed-methods research design, integrating both online and offline random sampling, for our survey of 51 respondents. We also utilized existing datasets consisting of both social media usage (n = 1000) and entrepreneurship (n = 1092). Our analysis identifies distinct social media engagement patterns via unsupervised learning and applies supervised models for entrepreneurship prediction, with logistic regression outperforming all other algorithms in terms of predictive accuracy and stability. In social media use, the cluster analysis reveals that at K=2, users form tightly packed, well-separated engagement groups. The results indicate that 39.4 percent of respondents believe social media positively impacts the economy by enabling businesses to generate increased revenue. However, only 14 percent of respondents participate in entrepreneurship, highlighting a substantial gap between digital engagement and business adoption. The analysis indicates that daily social media consumption is widespread with YouTube (66.7 percent) and WhatsApp (62.7 percent) being the most frequently used platforms. Key barriers identified are online harassment, limited digital literacy, and cultural constraints in a patriarchal society such as Pakistan. Additionally, 52.9 percent of respondents are unaware of government initiatives supporting women entrepreneurs, indicating limited policy outreach.

</details>


### [558] [Carrot, stick, or both? Price incentives for sustainable food choice in competitive environments](https://arxiv.org/abs/2512.13174)
*Francesco Salvi,Giuseppe Russo,Adam Barla,Vincent Moreau,Robert West*

Main category: econ.GN

TL;DR: 本文通过大规模实地实验评估多种肉类定价干预措施，发现组合方案能有效减排且不影响销售，平衡定价策略需协调实施以最大化气候效益。


<details>
  <summary>Details</summary>
Motivation: 以往关于定价干预减少肉类摄入的研究集中于选择有限的受限环境，本文旨在评估现实竞争环境中多种定价干预措施的效果。

Method: 采用顺序交叉设计，在瑞士大学校园的四个食堂匹配菜单，系统比较素食餐折扣、肉类附加费和组合方案。

Result: 仅肉类附加费和组合干预使素食餐购买量显著增加，分别减排7.4%和11.3%；附加费有溢出效应，抵消环境效益，组合方案有效且经济可行；定价干预对不同饮食习惯人群效果相同。

Conclusion: 平衡定价策略可减少食物环境碳足迹，但需协调实施以减少溢出效应、最大化气候效益。

Abstract: Meat consumption is a major driver of global greenhouse gas emissions. While pricing interventions have shown potential to reduce meat intake, previous studies have focused on highly constrained environments with limited consumer choice. Here, we present the first large-scale field experiment to evaluate multiple pricing interventions in a real-world, competitive setting. Using a sequential crossover design with matched menus in a Swiss university campus, we systematically compared vegetarian-meal discounts (-2.5 CHF), meat surcharges (+2.5 CHF), and a combined scheme (-1.2 CHF=+1.2 CHF) across four campus cafeterias. Only the surcharge and combined interventions led to significant increases in vegetarian meal uptake--by 26.4% and 16.6%, respectively--and reduced CO2 emissions per meal by 7.4% and 11.3%, respectively. The surcharge, while effective, triggered a 12.3% drop in sales at intervention sites and a corresponding 14.9% increase in non-treated locations, hence causing a spillover effect that completely offset environmental gains. In contrast, the combined approach achieved meaningful emission reductions without significant effects on overall sales or revenue, making it both effective and economically viable. Notably, pricing interventions were equally effective for both vegetarian-leaning customers and habitual meat-eaters, stimulating change even within entrenched dietary habits. Our results show that balanced pricing strategies can reduce the carbon footprint of realistic food environments, but require coordinated implementation to maximize climate benefits and avoid unintended spillover effects.

</details>


### [559] [Predicting the Emergence of the EV Industry: A Product Space Analysis Across Regions and Firms](https://arxiv.org/abs/2512.13178)
*Katharina Ledebur. Ladislav Bartuska,Klaus Friesenbichler,Peter Klimek*

Main category: econ.GN

TL;DR: 研究汽车行业转型，探讨企业跨行业多元化能力对竞争优势的影响，分析各国在电动汽车转型中的潜力。


<details>
  <summary>Details</summary>
Motivation: 汽车行业因电动化、软件定义车辆和循环经济概念而转型，需研究企业跨行业多元化能力对竞争优势的贡献。

Method: 开发国家层面和行业特定的产品空间，识别共同出口的零部件集群，研究接近中心性与新比较优势的关系。

Result: 欧盟在车辆和铝产品的新优势预计将比其他电动汽车相关部门产生更多电动汽车特定优势；中国已有一定多元化；韩国、中国、美国和加拿大有向电动汽车相关产品多元化的潜力，欧盟老牌生产商可能面临压力。

Conclusion: 汽车转型的成功取决于地区调动现有产业能力的能力，特别是机械和电子设备等行业。

Abstract: The automotive industry is undergoing transformation, driven by the electrification of powertrains, the rise of software-defined vehicles, and the adoption of circular economy concepts. These trends blur the boundaries between the automotive sector and other industries. Unlike internal combustion engine (ICE) production, where mechanical capabilities dominated, competitiveness in electric vehicle (EV) production increasingly depends on expertise in electronics, batteries, and software. This study investigates whether and how firms' ability to leverage cross-industry diversification contributes to competitive advantage. We develop a country-level product space covering all industries and an industry-specific product space covering over 900 automotive components. This allows us to identify clusters of parts that are exported together, revealing shared manufacturing capabilities. Closeness centrality in the country-level product space, rather than simple proximity, is a strong predictor of where new comparative advantages are likely to emerge. We examine this relationship across industrial sectors to establish patterns of path dependency, diversification and capability formation, and then focus on the EV transition. New strengths in vehicles and aluminium products in the EU are expected to generate 5 and 4.6 times more EV-specific strengths, respectively, than other EV-relevant sectors over the next decade, compared to only 1.6 and 4.5 new strengths in already diversified China. Countries such as South Korea, China, the US and Canada show strong potential for diversification into EV-related products, while established producers in the EU are likely to come under pressure. These findings suggest that the success of the automotive transformation depends on regions' ability to mobilize existing industrial capabilities, particularly in sectors such as machinery and electronic equipment.

</details>


### [560] [Job insecurity, equilibrium determinacy and E-stability in a New Keynesian model with asymmetric information. Theory and simulation analysis](https://arxiv.org/abs/2512.13627)
*Luca Vota,Luisa Errichiello*

Main category: econ.GN

TL;DR: 本文在新凯恩斯框架下对工作不安全感进行宏观经济形式化建模，分析不同货币政策条件下的均衡情况并通过模拟矩方法验证模型关键含义的实证合理性。


<details>
  <summary>Details</summary>
Motivation: 跳出关注个体和中观层面决定因素的主流方法，从宏观经济角度对工作不安全感进行形式化建模。

Method: 在新凯恩斯框架下，加入劳动力市场摩擦，构建包含动态IS曲线、新凯恩斯菲利普斯曲线和泰勒规则方程的模型，考虑部分知情的私人部门和完全知情的公共部门，分析不同货币政策条件下的均衡。

Result: 货币政策满足泰勒原则时，均衡唯一确定，但消息发布可能产生“透明度悖论”；违反泰勒原则时，可能出现信念驱动的均衡。模拟矩方法验证了模型关键含义的实证合理性。

Conclusion: 所构建的宏观经济模型能有效分析工作不安全感，不同货币政策条件下会出现不同均衡情况，且模型具有实证合理性。

Abstract: Departing from the dominant approach focused on individual and meso-level determinants, this manuscript develops a macroeconomic formalization of job insecurity within a New Keynesian framework in which the block formed by the dynamic IS curve, New Keynesian Phillips curve, and Taylor rule equations is augmented with labor market frictions. The model features partially informed private agents who receive a noisy signal about economic fundamentals from a fully informed public sector. When monetary policy satisfies the Taylor principle, the equilibrium is unique and determinate. However, the release of news about current or future fundamentals can still generate a form of "Paradox of Transparency" through general equilibrium interactions between aggregate demand and monetary policy. When the Taylor principle is violated, belief-driven equilibria may emerge. Validation exercises based on the Simulated Method of Moments confirm the empirical plausibility of the model's key implications.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [561] [Continuous Edit Distance, Geodesics and Barycenters of Time-varying Persistence Diagrams](https://arxiv.org/abs/2512.12939)
*Sebastien Tchitchek,Mohamed Kissi,Julien Tierny*

Main category: cs.CG

TL;DR: 本文引入连续编辑距离（CED）用于时变持久图（TVPDs）分析，提供显式构造，给出两种重心求解器，实证表现好且有C++实现。


<details>
  <summary>Details</summary>
Motivation: 需要为TVPDs分析提供有效的距离度量和处理方法。

Method: 将编辑距离思想扩展到TVPDs，结合局部替换成本与惩罚性删除/插入，由两个参数控制；提供显式构造；提出随机和贪心两种重心求解器。

Result: CED对加性扰动有鲁棒性，能恢复时间偏移，支持时间模式搜索；在现实数据集上聚类性能可比或优于标准弹性相异度，基于CED重心的聚类分类效果更好。

Conclusion: CED为TVPD分析提供了原则性距离、可解释的测地线和实用的重心，可直接在TVPD空间实现对齐、比较、平均和聚类。

Abstract: We introduce the Continuous Edit Distance (CED), a geodesic and elastic distance for time-varying persistence diagrams (TVPDs). The CED extends edit-distance ideas to TVPDs by combining local substitution costs with penalized deletions/insertions, controlled by two parameters: \(α\) (trade-off between temporal misalignment and diagram discrepancy) and \(β\) (gap penalty). We also provide an explicit construction of CED-geodesics. Building on these ingredients, we present two practical barycenter solvers, one stochastic and one greedy, that monotonically decrease the CED Frechet energy. Empirically, the CED is robust to additive perturbations (both temporal and spatial), recovers temporal shifts, and supports temporal pattern search. On real-life datasets, the CED achieves clustering performance comparable or better than standard elastic dissimilarities, while our clustering based on CED-barycenters yields superior classification results. Overall, the CED equips TVPD analysis with a principled distance, interpretable geodesics, and practical barycenters, enabling alignment, comparison, averaging, and clustering directly in the space of TVPDs. A C++ implementation is provided for reproducibility at the following address https://github.com/sebastien-tchitchek/ContinuousEditDistance.

</details>


### [562] [VoroLight: Learning Quality Volumetric Voronoi Meshes from General Inputs](https://arxiv.org/abs/2512.12984)
*Jiayin Lu,Ying Jiang,Yin Yang,Chenfanfu Jiang*

Main category: cs.CG

TL;DR: 提出基于Voronoi网格划分的可微框架VoroLight用于3D形状重建，能从多种输入生成表面和网格。


<details>
  <summary>Details</summary>
Motivation: 实现从多种输入进行3D形状重建，生成光滑、水密表面和拓扑一致的体积网格。

Method: 分三个阶段，先使用可微Voronoi公式初始化表面，再通过多边形面球体训练阶段优化表面质量，最后用可微Voronoi公式结合内部生成点进行体积优化。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: We present VoroLight, a differentiable framework for 3D shape reconstruction based on Voronoi meshing. Our approach generates smooth, watertight surfaces and topologically consistent volumetric meshes directly from diverse inputs, including images, implicit shape level-set fields, point clouds and meshes. VoroLight operates in three stages: it first initializes a surface using a differentiable Voronoi formulation, then refines surface quality through a polygon-face sphere training stage, and finally reuses the differentiable Voronoi formulation for volumetric optimization with additional interior generator points. Project page: https://jiayinlu19960224.github.io/vorolight/

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [563] [Diffusion Model-Based Posterior Sampling in Full Waveform Inversion](https://arxiv.org/abs/2512.12797)
*Mohammad H. Taufik,Tariq Alkhalifah*

Main category: physics.geo-ph

TL;DR: 本文提出耦合扩散后验采样与同步源全波形反演数据的方法，在多个数据集上验证，相比基线方法降低模型误差、数据拟合更好且成本低，为大尺度问题后验推断提供实用途径。


<details>
  <summary>Details</summary>
Motivation: 使基于观测地震炮记录的贝叶斯全波形反演后验采样在大尺度勘测中可行，同时保留校准性。

Method: 将扩散后验采样与同步源全波形反演数据耦合，在各扩散噪声水平用网络预测干净速度模型，在模型空间用朗之万动力学随机细化，引入噪声解耦连续水平；同步源批次减少波方程求解，无条件扩散先验抑制数值伪影。

Result: 在多个二维合成数据集、二维野外测线和三维升尺度研究中，相比基线方法，该采样器降低模型误差、数据拟合更好且计算成本大幅降低。

Conclusion: 该方法通过对齐编码炮似然与基于扩散的采样，并利用样本和源批次的并行化，为观测炮记录的校准后验推断提供了适用于大尺度二维和三维问题的实用途径。

Abstract: Bayesian full waveform inversion (FWI) offers uncertainty-aware subsurface models; however, posterior sampling directly on observed seismic shot records is rarely practical at the field scale because each sample requires numerous wave-equation solves. We aim to make such sampling feasible for large surveys while preserving calibration, that is, high uncertainty in less illuminated areas. Our approach couples diffusion-based posterior sampling with simultaneous-source FWI data. At each diffusion noise level, a network predicts a clean velocity model. We then apply a stochastic refinement step in model space using Langevin dynamics under the wave-equation likelihood and reintroduce noise to decouple successive levels before proceeding. Simultaneous-source batches reduce forward and adjoint solves approximately in proportion to the supergather size, while an unconditional diffusion prior trained on velocity patches and volumes helps suppress source-related numerical artefacts. We evaluate the method on three 2D synthetic datasets (SEG/EAGE Overthrust, SEG/EAGE Salt, SEAM Arid), a 2D field line, and a 3D upscaling study. Relative to a particle-based variational baseline, namely Stein variational gradient descent without a learned prior and with single-source (non-simultaneous-source) FWI, our sampler achieves lower model error and better data fit at a substantially reduced computational cost. By aligning encoded-shot likelihoods with diffusion-based sampling and exploiting straightforward parallelization over samples and source batches, the method provides a practical path to calibrated posterior inference on observed shot records that scales to large 2D and 3D problems.

</details>


### [564] [MicroPhaseNO: Adapting an Earthquake-Trained Phase Neural Operator for Microseismic Phase Picking](https://arxiv.org/abs/2512.13197)
*Ayrat Abdullin,Umair bin Waheed,Leo Eisner,Naveed Iqbal*

Main category: physics.geo-ph

TL;DR: 本文展示如何用迁移学习使地震相位拾取网络PhaseNO适应微地震监测，经微调后模型性能提升，方法可迁移且代码将开源


<details>
  <summary>Details</summary>
Motivation: 传统手动处理不适用于实时应用或大型阵列，基于深度学习的拾取器难以应对微地震数据集的挑战

Method: 从预训练的PhaseNO模型出发，用200个水力压裂环境下诱发事件的标记和噪声地震图微调模型

Result: 在三个真实微地震数据集上评估，相比原模型和传统工作流程，微调模型F1分数和准确率提高达30%，减少系统时间偏差和拾取不确定性

Conclusion: 该方法依赖特定校准集，在有公共地震数据和预训练模型时可轻松迁移到其他微地震任务

Abstract: Seismic phase picking is very often used for microseismic monitoring and subsurface imaging. Traditional manual processing is not feasible for either real-time applications or large arrays. Deep learning-based pickers trained on large earthquake catalogs offer an automated alternative. However, they are typically optimized for high signal-to-noise, long-duration networks and struggle with the challenges presented by microseismic datasets, which are purpose-built for limited time without previously detected seismicity. In this study, we demonstrate how a network-wide earthquake phase picker, the Phase Neural Operator (PhaseNO), can be adapted to microseismic monitoring using transfer learning. Starting from a PhaseNO model pre-trained on more than 57,000 three-component earthquake and noise records, we fine-tune the model using only 200 labeled and noise seismograms from induced events in hydraulic-fracturing settings. The fine-tuned model thus preserves the rich spatio-temporal representation learned from abundant earthquake data, while adapting to the characteristics and labeling conventions of microseismic phases, which are often picked on peaks or troughs rather than onsets. We evaluate performance on three distinct real-world microseismic datasets with different network geometries and acquisition parameters. Compared to the original PhaseNO and a conventional workflow, the adapted model increases F1 score and accuracy by up to 30%, and strongly reduces systematic timing bias and pick uncertainty. Because the adaptation relies on a small, campaign-specific calibration set, the approach is readily transferable to other microseismic tasks where public earthquake data and pre-trained models are accessible. The associated code will be released openly at https://github.com/ayratabd/MicroPhaseNO.

</details>


### [565] [Enhancing lithological interpretation from petrophysical well log of IODP expedition 390/393 using machine learning](https://arxiv.org/abs/2512.13529)
*Raj Sahu,Saumen Maiti*

Main category: physics.geo-ph

TL;DR: 本文开发多种机器学习算法分析IODP测井数据以增强岩性解释，决策树和梯度提升模型表现最佳，联合方法有潜力用于分析其他海洋测井数据。


<details>
  <summary>Details</summary>
Motivation: 传统测井解释方法受成本和井况限制，且传统统计方法难以区分岩性和岩相，需要新方法增强岩性解释。

Method: 开发多种有监督和无监督机器学习算法联合分析IODP测井数据。

Result: 决策树和梯度提升模型表现优于其他算法，准确率达0.9950，F1分数达0.9951。

Conclusion: 开发的联合机器学习方法有潜力用于分析世界其他海洋的测井数据集。

Abstract: Enhanced lithological interpretation from well logs plays a key role in geological resource exploration and mapping, as well as in geo-environmental modeling studies. Core and cutting information is useful for making sound interpretations of well logs; however, these are rarely collected at each depth due to high costs. Moreover, well log interpretation using traditional methods is constrained by poor borehole conditions. Traditional statistical methods are mostly linear, often failing to discriminate between lithology and rock facies, particularly when dealing with overlapping well log signals characterized by the structural and compositional variation of rock types. In this study, we develop multiple supervised and unsupervised machine learning algorithms to jointly analyze multivariate well log data from Integrated Ocean Drilling Program (IODP) expeditions 390 and 393 for enhanced lithological interpretations. Among the algorithms, Logistic Regression, Decision Trees, Gradient Boosting, Support Vector Machines (SVM), k-Nearest Neighbors (KNN), and Multi-Layer Perceptron (MLP) neural network models, the Decision Tree and Gradient Boosting models outperformed the others, achieving an accuracy of 0.9950 and an F1-score of 0.9951. While unsupervised machine learning (ML) provides the foundation for cluster information that inherently supports the classification algorithm, supervised ML is applied to devise a data-driven lithology clustering mechanism for IODP datasets. The joint ML-based method developed here has the potential to be further explored for analyzing other well log datasets from the world's oceans.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [566] [Scalable branch-and-bound model selection with non-monotonic criteria including AIC, BIC and Mallows's $\mathit{C_p}$](https://arxiv.org/abs/2512.12221)
*Jakob Vanhoefer,Antonia Körner,Domagoj Doresic,Jan Hasenauer,Dilan Pathirana*

Main category: q-bio.QM

TL;DR: 传统模型选择方法计算成本高，本文提出新边界开发分支限界算法，可保证找到最优模型并实现计算加速，对复杂领域模型选择有广泛意义。


<details>
  <summary>Details</summary>
Motivation: 传统信息准则穷举搜索计算量过大，启发式方法无法保证找到全局最优模型，且普遍认为信息准则非单调性使搜索空间无界。

Method: 引入新边界，开发针对非单调函数的分支限界算法。

Result: 该方法能保证在不同模型类别、规模和应用中找到最优模型，实现计算加速，如在某案例中加速超6000倍。

Conclusion: 此方法对复杂科学领域模型选择的可扩展性和有效性有广泛意义。

Abstract: Model selection is a pivotal process in the quantitative sciences, where researchers must navigate between numerous candidate models of varying complexity. Traditional information criteria, such as the corrected Akaike Information Criterion (AICc), Bayesian Information Criterion (BIC), and Mallows's $\mathit{C_p}$, are valuable tools for identifying optimal models. However, the exponential increase in candidate models with each additional model parameter renders the evaluation of these criteria for all models -- a strategy known as exhaustive, or brute-force, searches -- computationally prohibitive. Consequently, heuristic approaches like stepwise regression are commonly employed, albeit without guarantees of finding the globally-optimal model.
  In this study, we challenge the prevailing notion that non-monotonicity in information criteria precludes bounds on the search space. We introduce a simple but novel bound that enables the development of branch-and-bound algorithms tailored for these non-monotonic functions. We demonstrate that our approach guarantees identification of the optimal model(s) across diverse model classes, sizes, and applications, often with orders of magnitude computational speedups. For instance, in one previously-published model selection task involving $2^{32}$ (approximately 4 billion) candidate models, our method achieves a computational speedup exceeding 6,000. These findings have broad implications for the scalability and effectiveness of model selection in complex scientific domains.

</details>


### [567] [Cycles Communities from the Perspective of Dendrograms and Gradient Sampling](https://arxiv.org/abs/2512.12974)
*Sixtus Dakurah*

Main category: q-bio.QM

TL;DR: 本文提出构建循环社区的新框架，含基于树状图和分层梯度采样两种方法，将循环匹配问题转化为聚类和拓扑优化问题。


<details>
  <summary>Details</summary>
Motivation: 解决持久同调与拓扑数据分析中识别和比较不同拓扑对象拓扑特征（尤其是循环）的挑战。

Method: 一是基于树状图方法，利用合并树算法从持久区间构建同调类的层次表示，引入Wasserstein距离比较树状图；二是扩展分层梯度采样，学习多个过滤函数得到循环重心函数以构建循环社区。

Result: 通过模拟研究证明树状图表示识别循环社区的判别能力，过滤函数构建的循环社区不重叠且划分所有循环空间。

Conclusion: 这些方法将循环匹配问题转化为层次聚类和拓扑优化框架，为识别拓扑对象组内和组间相似拓扑结构提供原则性方法。

Abstract: Identifying and comparing topological features, particularly cycles, across different topological objects remains a fundamental challenge in persistent homology and topological data analysis. This work introduces a novel framework for constructing cycle communities through two complementary approaches. First, a dendrogram-based methodology leverages merge-tree algorithms to construct hierarchical representations of homology classes from persistence intervals. The Wasserstein distance on merge trees is introduced as a metric for comparing dendrograms, establishing connections to hierarchical clustering frameworks. Through simulation studies, the discriminative power of dendrogram representations for identifying cycle communities is demonstrated. Second, an extension of Stratified Gradient Sampling simultaneously learns multiple filter functions that yield cycle barycenter functions capable of faithfully reconstructing distinct sets of cycles. The set of cycles each filter function can reconstruct constitutes cycle communities that are non-overlapping and partition the space of all cycles. Together, these approaches transform the problem of cycle matching into both a hierarchical clustering and topological optimization framework, providing principled methods to identify similar topological structures both within and across groups of topological objects.

</details>


### [568] [Vision Foundry: A System for Training Foundational Vision AI Models](https://arxiv.org/abs/2512.11837)
*Mahmut S. Gokmen,Mitchell A. Klusty,Evan W. Damron,W. Vaiden Logan,Aaron D. Mullen,Caroline N. Leach,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: q-bio.QM

TL;DR: 介绍无代码、符合HIPAA的Vision Foundry平台，可用于基础视觉模型的预训练、适配和部署，实验验证其效果好，能让领域专家开发临床AI工具。


<details>
  <summary>Details</summary>
Motivation: 自监督学习虽能利用大量未标注医疗数据集，但技术壁垒限制临床研究人员使用，需要一个易用平台。

Method: 引入Vision Foundry平台，集成DINO - MX框架，采用Magnification - Aware Distillation和Parameter - Efficient Fine - Tuning策略。

Result: 在多个领域验证，通过该平台训练的模型在分割保真度和回归准确性上显著优于通用基线，且在不同成像协议上有零样本泛化能力。

Conclusion: Vision Foundry能弥合高级表征学习与实际应用的差距，让领域专家以最少标注开发先进临床AI工具，关注临床发现。

Abstract: Self-supervised learning (SSL) leverages vast unannotated medical datasets, yet steep technical barriers limit adoption by clinical researchers. We introduce Vision Foundry, a code-free, HIPAA-compliant platform that democratizes pre-training, adaptation, and deployment of foundational vision models. The system integrates the DINO-MX framework, abstracting distributed infrastructure complexities while implementing specialized strategies like Magnification-Aware Distillation (MAD) and Parameter-Efficient Fine-Tuning (PEFT). We validate the platform across domains, including neuropathology segmentation, lung cellularity estimation, and coronary calcium scoring. Our experiments demonstrate that models trained via Vision Foundry significantly outperform generic baselines in segmentation fidelity and regression accuracy, while exhibiting robust zero-shot generalization across imaging protocols. By bridging the gap between advanced representation learning and practical application, Vision Foundry enables domain experts to develop state-of-the-art clinical AI tools with minimal annotation overhead, shifting focus from engineering optimization to clinical discovery.

</details>


### [569] [Accurate de novo sequencing of the modified proteome with OmniNovo](https://arxiv.org/abs/2512.12272)
*Yuhan Chen,Shang Qu,Zhiqiang Gao,Yuejin Yang,Xiang Zhang,Sheng Xu,Xinjie Mao,Liujia Qian,Jiaqi Wei,Zijie Qiu,Chenyu You,Lei Bai,Ning Ding,Tiannan Guo,Bowen Zhou,Siqi Sun*

Main category: q-bio.QM

TL;DR: 介绍了OmniNovo深度学习框架用于直接从串联质谱中无参考地测序肽段，能更好地识别修饰肽，发现更多蛋白质组信息。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质组学方法难以识别大量修饰蛋白质组，标准数据库搜索算法存在搜索空间组合爆炸问题，限制了未鉴定或复杂修饰的识别。

Method: 引入OmniNovo框架，学习通用的碎裂规则，集成质量约束解码算法和严格的假发现率估计。

Result: 在1%假发现率下，OmniNovo比标准方法多鉴定51%的肽段，且能推广到训练中未见的生物位点。

Conclusion: OmniNovo框架能照亮蛋白质组的暗物质，实现对细胞调控的无偏综合分析。

Abstract: Post-translational modifications (PTMs) serve as a dynamic chemical language regulating protein function, yet current proteomic methods remain blind to a vast portion of the modified proteome. Standard database search algorithms suffer from a combinatorial explosion of search spaces, limiting the identification of uncharacterized or complex modifications. Here we introduce OmniNovo, a unified deep learning framework for reference-free sequencing of unmodified and modified peptides directly from tandem mass spectra. Unlike existing tools restricted to specific modification types, OmniNovo learns universal fragmentation rules to decipher diverse PTMs within a single coherent model. By integrating a mass-constrained decoding algorithm with rigorous false discovery rate estimation, OmniNovo achieves state-of-the-art accuracy, identifying 51\% more peptides than standard approaches at a 1\% false discovery rate. Crucially, the model generalizes to biological sites unseen during training, illuminating the dark matter of the proteome and enabling unbiased comprehensive analysis of cellular regulation.

</details>
