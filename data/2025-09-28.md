<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 36]
- [cs.CE](#cs.CE) [Total: 5]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.LG](#cs.LG) [Total: 112]
- [cs.SE](#cs.SE) [Total: 18]
- [stat.ML](#stat.ML) [Total: 12]
- [cs.HC](#cs.HC) [Total: 7]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.CR](#cs.CR) [Total: 17]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.NI](#cs.NI) [Total: 3]
- [eess.SY](#eess.SY) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.RO](#cs.RO) [Total: 13]
- [cs.IT](#cs.IT) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 37]
- [cs.CL](#cs.CL) [Total: 31]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [eess.AS](#eess.AS) [Total: 5]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [stat.AP](#stat.AP) [Total: 2]
- [math.CO](#math.CO) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [An Approach to Checking Correctness for Agentic Systems](https://arxiv.org/abs/2509.20364)
*Thomas J Sheffler*

Main category: cs.AI

TL;DR: 本文提出用于监控AI智能体行为的时态表达式语言，能检测基于大语言模型的智能体系统错误，通过三智能体系统验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体系统因随机生成过程输出可变，现有错误检测方法依赖文本匹配易出错，需新方法。

Method: 借鉴硬件验证中的时态逻辑技术，监控智能体工具调用和状态转换的执行轨迹，关注智能体动作序列，用时态表达式语言捕获正确行为模式。

Result: 在三智能体系统中，大模型能满足时态断言，小模型则违反断言，时态表达式能成功标记异常。

Conclusion: 该方法为关键应用中AI智能体可靠性的系统监控提供了基础。

Abstract: This paper presents a temporal expression language for monitoring AI agent
behavior, enabling systematic error-detection of LLM-based agentic systems that
exhibit variable outputs due to stochastic generation processes. Drawing from
temporal logic techniques used in hardware verification, this approach monitors
execution traces of agent tool calls and state transitions to detect deviations
from expected behavioral patterns. Current error-detection approaches rely
primarily on text matching of inputs and outputs, which proves fragile due to
the natural language variability inherent in LLM responses. The proposed method
instead focuses on the sequence of agent actions -- such as tool invocations
and inter-agent communications -- allowing verification of system behavior
independent of specific textual outputs. The temporal expression language
provides assertions that capture correct behavioral patterns across multiple
execution scenarios. These assertions serve dual purposes: validating prompt
engineering and guardrail effectiveness during development, and providing
regression testing when agents are updated with new LLMs or modified logic. The
approach is demonstrated using a three-agent system, where agents coordinate to
solve multi-step reasoning tasks. When powered by large, capable models, all
temporal assertions were satisfied across many test runs. However, when smaller
models were substituted in two of the three agents, executions violated
behavioral assertions, primarily due to improper tool sequencing and failed
coordination handoffs. The temporal expressions successfully flagged these
anomalies, demonstrating the method's effectiveness for detecting behavioral
regressions in production agentic systems. This approach provides a foundation
for systematic monitoring of AI agent reliability as these systems become
increasingly deployed in critical applications.

</details>


### [2] [LATTS: Locally Adaptive Test-Time Scaling](https://arxiv.org/abs/2509.20368)
*Theo Uscidda,Matthew Trager,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: 提出Locally Adaptive Test - Time Scaling (LATTS)方法，在生成步骤中分配可变计算量，相比标准方法有更好的准确率 - 计算权衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于验证器模型的方法在测试时统一增加计算量，未考虑实例复杂性，导致资源使用效率低。

Method: 提出LATTS方法，在每个生成步骤使用基于验证器的接受标准，根据局部难度调整每步计算量。

Result: LATTS相比标准基于验证器的方法，在准确率 - 计算权衡上表现显著更优。

Conclusion: LATTS方法能有效解决现有方法资源使用效率低的问题，实现更好的准确率 - 计算权衡。

Abstract: One common strategy for improving the performance of Large Language Models
(LLMs) on downstream tasks involves using a \emph{verifier model} to either
select the best answer from a pool of candidates or to steer the
auto-regressive generation process towards better outputs. This class of
methods typically results in improved accuracy at the cost of increased
computation at test-time, a paradigm known as \emph{test-time scaling}.
However, most existing approaches increase computation uniformly across all
samples and generation steps, without considering the complexity of individual
instances, leading to inefficient resource use. We address this limitation by
proposing an approach, called \emph{Locally Adaptive Test-Time Scaling
(LATTS)}, that allocates variable compute across generation steps.
Specifically, at each generation step, LATTS employs a verifier-based
acceptance criterion to decide whether to resample, backtrack, restart, or stop
the generation process. This criterion effectively adjusts the per-step
computational effort based on a precise notion of \emph{local difficulty}
derived from the verifier model. Empirical results show that LATTS achieves
significantly superior accuracy--compute tradeoffs compared to standard
verifier-based methods.

</details>


### [3] [Philosophy-informed Machine Learning](https://arxiv.org/abs/2509.20370)
*MZ Naser*

Main category: cs.AI

TL;DR: 本文介绍哲学赋能机器学习（PhIML），回顾概念基础，给出应用案例，指出挑战并规划研究路线。


<details>
  <summary>Details</summary>
Motivation: 展示PhIML在哲学层面的收获与一致性，推动其发展以实现安全、有哲学意识和符合伦理的目标。

Method: 回顾概念基础，给出用户或设计者采用PhIML的案例研究。

Result: 明确了PhIML的概念基础，展示了应用方式。

Conclusion: 指出技术、哲学、实践和治理方面的挑战，规划了PhIML的研究路线。

Abstract: Philosophy-informed machine learning (PhIML) directly infuses core ideas from
analytic philosophy into ML model architectures, objectives, and evaluation
protocols. Therefore, PhIML promises new capabilities through models that
respect philosophical concepts and values by design. From this lens, this paper
reviews conceptual foundations to demonstrate philosophical gains and
alignment. In addition, we present case studies on how ML users/designers can
adopt PhIML as an agnostic post-hoc tool or intrinsically build it into ML
model architectures. Finally, this paper sheds light on open technical barriers
alongside philosophical, practical, and governance challenges and outlines a
research roadmap toward safe, philosophy-aware, and ethically responsible
PhIML.

</details>


### [4] [InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature](https://arxiv.org/abs/2509.20493)
*Paris Koloveas,Serafeim Chatzopoulos,Thanasis Vergoulis,Christos Tryfonopoulos*

Main category: cs.AI

TL;DR: 本文介绍了AI工具InsightGUIDE，它作为阅读助手，能提供简洁结构化见解，经对比比通用大模型更有效。


<details>
  <summary>Details</summary>
Motivation: 科学文献增多，现有大模型工具总结冗长，无法有效辅助阅读，需新工具。

Method: 介绍系统架构、提示驱动方法，并通过定性案例研究与通用大模型输出对比。

Result: InsightGUIDE生成的指导更具结构性和可操作性。

Conclusion: InsightGUIDE是现代研究人员更有效的工具。

Abstract: The proliferation of scientific literature presents an increasingly
significant challenge for researchers. While Large Language Models (LLMs) offer
promise, existing tools often provide verbose summaries that risk replacing,
rather than assisting, the reading of the source material. This paper
introduces InsightGUIDE, a novel AI-powered tool designed to function as a
reading assistant, not a replacement. Our system provides concise, structured
insights that act as a "map" to a paper's key elements by embedding an expert's
reading methodology directly into its core AI logic. We present the system's
architecture, its prompt-driven methodology, and a qualitative case study
comparing its output to a general-purpose LLM. The results demonstrate that
InsightGUIDE produces more structured and actionable guidance, serving as a
more effective tool for the modern researcher.

</details>


### [5] [Reconstruction-Based Adaptive Scheduling Using AI Inferences in Safety-Critical Systems](https://arxiv.org/abs/2509.20513)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 本文提出一种新颖的重构框架用于动态验证和组装时间触发系统（TTS）的调度，实验证明该框架提升了系统适应性、运行完整性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有调度框架面临消息冲突、优先级处理错误导致的循环锁定、生成不完整或无效调度等问题，影响系统安全和性能，需要解决这些问题。

Method: 提出重构框架，将AI生成或启发式得出的调度优先级转换为可执行调度，包含安全检查、分配算法和恢复机制。

Result: 框架显著增强了系统适应性、运行完整性和运行时性能，同时保持了计算效率。

Conclusion: 该工作为安全关键TTS的安全调度生成问题提供了实用且可扩展的解决方案，能在动态不确定条件下实现可靠灵活的实时调度。

Abstract: Adaptive scheduling is crucial for ensuring the reliability and safety of
time-triggered systems (TTS) in dynamic operational environments. Scheduling
frameworks face significant challenges, including message collisions, locked
loops from incorrect precedence handling, and the generation of incomplete or
invalid schedules, which can compromise system safety and performance. To
address these challenges, this paper presents a novel reconstruction framework
designed to dynamically validate and assemble schedules. The proposed
reconstruction models operate by systematically transforming AI-generated or
heuristically derived scheduling priorities into fully executable schedules,
ensuring adherence to critical system constraints such as precedence rules and
collision-free communication. It incorporates robust safety checks, efficient
allocation algorithms, and recovery mechanisms to handle unexpected context
events, including hardware failures and mode transitions. Comprehensive
experiments were conducted across multiple performance profiles, including
makespan minimisation, workload balancing, and energy efficiency, to validate
the operational effectiveness of the reconstruction models. Results demonstrate
that the proposed framework significantly enhances system adaptability,
operational integrity, and runtime performance while maintaining computational
efficiency. Overall, this work contributes a practical and scalable solution to
the problem of safe schedule generation in safety-critical TTS, enabling
reliable and flexible real-time scheduling even under highly dynamic and
uncertain operational conditions.

</details>


### [6] [Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications](https://arxiv.org/abs/2509.20520)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 传统元调度离线训练AI调度推理有挑战，本文提出自适应在线学习单元，用强化学习提升系统性能，确保大规模关键环境的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统离线训练构建综合多调度图资源密集且常不可行，其创建的多调度图只是完整空间的子集，有局限性。

Method: 在元调度器中集成自适应在线学习单元，运用强化学习不断探索新调度方案，实现实时训练，还实现多个RL模型解决调度挑战。

Result: 系统能有效处理意外事件和复杂调度场景，优化现有调度器，满足不断变化的需求。

Conclusion: 通过实时训练不断完善AI推理，可保证大规模、安全关键环境下系统的灵活性、鲁棒性和效率。

Abstract: Metascheduling in time-triggered architectures has been crucial in adapting
to dynamic and unpredictable environments, ensuring the reliability and
efficiency of task execution. However, traditional approaches face significant
challenges when training Artificial Intelligence (AI) scheduling inferences
offline, particularly due to the complexities involved in constructing a
comprehensive Multi-Schedule Graph (MSG) that accounts for all possible
scenarios. The process of generating an MSG that captures the vast probability
space, especially when considering context events like hardware failures, slack
variations, or mode changes, is resource-intensive and often infeasible. To
address these challenges, we propose an adaptive online learning unit
integrated within the metascheduler to enhance performance in real-time. The
primary motivation for developing this unit stems from the limitations of
offline training, where the MSG created is inherently a subset of the complete
space, focusing only on the most probable and critical context events. In the
online mode, Reinforcement Learning (RL) plays a pivotal role by continuously
exploring and discovering new scheduling solutions, thus expanding the MSG and
enhancing system performance over time. This dynamic adaptation allows the
system to handle unexpected events and complex scheduling scenarios more
effectively. Several RL models were implemented within the online learning
unit, each designed to address specific challenges in scheduling. These models
not only facilitate the discovery of new solutions but also optimize existing
schedulers, particularly when stricter deadlines or new performance criteria
are introduced. By continuously refining the AI inferences through real-time
training, the system remains flexible and capable of meeting evolving demands,
thus ensuring robustness and efficiency in large-scale, safety-critical
environments.

</details>


### [7] [A Compound Classification System Based on Fuzzy Relations Applied to the Noise-Tolerant Control of a Bionic Hand via EMG Signal Recognition](https://arxiv.org/abs/2509.20523)
*Pawel Trajdos,Marek Kurzynski*

Main category: cs.AI

TL;DR: 本文提出用于基于肌电信号控制手部假肢的新识别系统，检测污染信号，用模糊模型决策，通过实验评估并与文献中系统对比。


<details>
  <summary>Details</summary>
Motivation: 现代拟人上肢生物假肢基于肌电信号的模式识别控制存在诸多因素影响分类质量，如生物信号易受污染。

Method: 提出新识别系统，含单类分类器评估通道污染程度，K近邻分类器识别患者意图，开发模糊模型实现统一软决策方案。

Result: 使用公共库真实生物信号进行实验评估，对影响识别系统质量的参数和程序进行对比分析。

Conclusion: 未明确提及，但可推测新系统在一定程度上能缓解污染信号对识别质量的不利影响。

Abstract: Modern anthropomorphic upper limb bioprostheses are typically controlled by
electromyographic (EMG) biosignals using a pattern recognition scheme.
Unfortunately, there are many factors originating from the human source of
objects to be classified and from the human-prosthesis interface that make it
difficult to obtain an acceptable classification quality. One of these factors
is the high susceptibility of biosignals to contamination, which can
considerably reduce the quality of classification of a recognition system.
  In the paper, the authors propose a new recognition system intended for EMG
based control of the hand prosthesis with detection of contaminated biosignals
in order to mitigate the adverse effect of contaminations. The system consists
of two ensembles: the set of one-class classifiers (OCC) to assess the degree
of contamination of individual channels and the ensemble of K-nearest
neighbours (KNN) classifier to recognise the patient's intent. For all
recognition systems, an original, coherent fuzzy model was developed, which
allows the use of a uniform soft (fuzzy) decision scheme throughout the
recognition process. The experimental evaluation was conducted using real
biosignals from a public repository. The goal was to provide an experimental
comparative analysis of the parameters and procedures of the developed method
on which the quality of the recognition system depends. The proposed fuzzy
recognition system was also compared with similar systems described in the
literature.

</details>


### [8] [SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection](https://arxiv.org/abs/2509.20562)
*Yubin Ge,Salvatore Romeo,Jason Cai,Monica Sunkara,Yi Zhang*

Main category: cs.AI

TL;DR: 提出SAMULE框架解决LLM智能体反思生成问题，在多基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在复杂任务中因错误分析不足和依赖成功轨迹，难以生成有意义反思。

Method: 提出SAMULE框架，基于多级反思合成训练回顾语言模型，包括单轨迹、任务内和任务间学习，还扩展到交互场景。

Result: 在TravelPlanner、NATURAL PLAN和Tau - bench三个基准测试中显著优于基于反思的基线方法。

Conclusion: 精心设计的反思合成和以失败为中心的学习对构建自我改进的LLM智能体至关重要。

Abstract: Despite the rapid advancements in LLM agents, they still face the challenge
of generating meaningful reflections due to inadequate error analysis and a
reliance on rare successful trajectories, especially in complex tasks. In this
work, we propose SAMULE, a new framework for self-learning agents powered by a
retrospective language model that is trained based on Multi-Level Reflection
Synthesis. It first synthesizes high-quality reflections across three
complementary levels: Single-Trajectory Learning (micro-level) for detailed
error correction; Intra-Task Learning (meso-level) to build error taxonomies
across multiple trials of the same task, and Inter-Task Learning (macro-level)
to extract transferable insights based on same typed errors from diverse task
failures. Then we fine-tune a language model serving as the retrospective model
to generate reflections during inference. We further extend our framework to
interactive settings through a foresight-based reflection mechanism, enabling
agents to proactively reflect and adapt during user interactions by comparing
predicted and actual responses. Extensive experiments on three challenging
benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our
approach significantly outperforms reflection-based baselines. Our results
highlight the critical role of well-designed reflection synthesis and
failure-centric learning in building self-improving LLM agents.

</details>


### [9] [Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI](https://arxiv.org/abs/2509.20640)
*Oluwakemi T. Olayinka,Sumeet Jeswani,Divine Iloh*

Main category: cs.AI

TL;DR: 传统静态网络安全模型有局限，本研究引入基于智能体AI的自适应架构，经模拟验证有良好效果且适配零信任模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统静态网络安全模型在当前数字产品生态系统中可扩展性、实时检测和上下文响应方面的不足。

Method: 引入能动态学习和上下文感知决策的自主目标驱动智能体，将智能体AI集成到关键生态系统层，具备行为基线、分散风险评分和联合威胁情报共享等特征。

Result: 通过原生云模拟证明系统能识别零日攻击和动态修改访问策略，评估显示提高了适应性、降低了响应延迟、提升了检测准确性。

Conclusion: 该架构为保护复杂数字基础设施提供智能可扩展蓝图，兼容零信任模型，支持遵守国际网络安全法规。

Abstract: Traditional static cybersecurity models often struggle with scalability,
real-time detection, and contextual responsiveness in the current digital
product ecosystems which include cloud services, application programming
interfaces (APIs), mobile platforms, and edge devices. This study introduces
autonomous goal driven agents capable of dynamic learning and context-aware
decision making as part of an adaptive cybersecurity architecture driven by
agentic artificial intelligence (AI). To facilitate autonomous threat
mitigation, proactive policy enforcement, and real-time anomaly detection, this
framework integrates agentic AI across the key ecosystem layers. Behavioral
baselining, decentralized risk scoring, and federated threat intelligence
sharing are important features. The capacity of the system to identify zero-day
attacks and dynamically modify access policies was demonstrated through native
cloud simulations. The evaluation results show increased adaptability,
decreased response latency, and improved detection accuracy. The architecture
provides an intelligent and scalable blueprint for safeguarding complex digital
infrastructure and is compatible with zero-trust models, thereby supporting the
adherence to international cybersecurity regulations.

</details>


### [10] [Accelerate Creation of Product Claims Using Generative AI](https://arxiv.org/abs/2509.20652)
*Po-Yu Liang,Yong Zhang,Tatiana Hwa,Aaron Byers*

Main category: cs.AI

TL;DR: 开发Claim Advisor网络应用加速产品声明创建，在CPG公司应用效果好，鼓励生成式AI研究与应用。


<details>
  <summary>Details</summary>
Motivation: 产品声明创建耗时费钱，需加速声明创建。

Method: 使用大语言模型的上下文学习和微调开发Claim Advisor网络应用，该应用有语义搜索、生成优化声明、模拟排名三个功能。

Result: 在一家消费品包装公司的应用取得了非常有前景的结果。

Conclusion: 此能力在各产品类别和行业广泛有用，鼓励生成式AI在不同行业的研究与应用。

Abstract: The benefit claims of a product is a critical driver of consumers' purchase
behavior. Creating product claims is an intense task that requires substantial
time and funding. We have developed the $\textbf{Claim Advisor}$ web
application to accelerate claim creations using in-context learning and
fine-tuning of large language models (LLM). $\textbf{Claim Advisor}$ was
designed to disrupt the speed and economics of claim search, generation,
optimization, and simulation. It has three functions: (1) semantically
searching and identifying existing claims and/or visuals that resonate with the
voice of consumers; (2) generating and/or optimizing claims based on a product
description and a consumer profile; and (3) ranking generated and/or manually
created claims using simulations via synthetic consumers. Applications in a
consumer packaged goods (CPG) company have shown very promising results. We
believe that this capability is broadly useful and applicable across product
categories and industries. We share our learning to encourage the research and
application of generative AI in different industries.

</details>


### [11] [An Automated Retrieval-Augmented Generation LLaMA-4 109B-based System for Evaluating Radiotherapy Treatment Plans](https://arxiv.org/abs/2509.20707)
*Junjie Cui,Peilong Wang,Jason Holmes,Leshan Sun,Michael L. Hinni,Barbara A. Pockaj,Sujay A. Vora,Terence T. Sio,William W. Wong,Nathan Y. Yu,Steven E. Schild,Joshua R. Niska,Sameer R. Keole,Jean-Claude M. Rwigema,Samir H. Patel,Lisa A. McGee,Carlos A. Vargas,Wei Liu*

Main category: cs.AI

TL;DR: 开发由LLaMA - 4 109B驱动的RAG系统用于放疗计划自动评估，介绍方法、结果，证明结合结构化评分与模块化推理评估放疗计划可行。


<details>
  <summary>Details</summary>
Motivation: 开发用于放疗治疗计划的自动化、协议感知且可解释的评估系统。

Method: 整理614个放疗计划的多协议数据集，构建知识库，RAG系统集成检索引擎、百分位数预测组件和临床约束检查器，由大语言模型通过多步提示驱动推理管道进行评估。

Result: 优化检索超参数，最佳配置准确性高，端到端测试时RAG系统与独立模块计算值100%一致。

Conclusion: 结合结构化评分与模块化推理评估放疗计划可行，系统输出可追溯、减少幻觉且跨协议稳健，未来需临床验证和改进检索模型。

Abstract: Purpose: To develop a retrieval-augmented generation (RAG) system powered by
LLaMA-4 109B for automated, protocol-aware, and interpretable evaluation of
radiotherapy treatment plans.
  Methods and Materials: We curated a multi-protocol dataset of 614
radiotherapy plans across four disease sites and constructed a knowledge base
containing normalized dose metrics and protocol-defined constraints. The RAG
system integrates three core modules: a retrieval engine optimized across five
SentenceTransformer backbones, a percentile prediction component based on
cohort similarity, and a clinical constraint checker. These tools are directed
by a large language model (LLM) using a multi-step prompt-driven reasoning
pipeline to produce concise, grounded evaluations.
  Results: Retrieval hyperparameters were optimized using Gaussian Process on a
scalarized loss function combining root mean squared error (RMSE), mean
absolute error (MAE), and clinically motivated accuracy thresholds. The best
configuration, based on all-MiniLM-L6-v2, achieved perfect nearest-neighbor
accuracy within a 5-percentile-point margin and a sub-2pt MAE. When tested
end-to-end, the RAG system achieved 100% agreement with the computed values by
standalone retrieval and constraint-checking modules on both percentile
estimates and constraint identification, confirming reliable execution of all
retrieval, prediction and checking steps.
  Conclusion: Our findings highlight the feasibility of combining structured
population-based scoring with modular tool-augmented reasoning for transparent,
scalable plan evaluation in radiation therapy. The system offers traceable
outputs, minimizes hallucination, and demonstrates robustness across protocols.
Future directions include clinician-led validation, and improved domain-adapted
retrieval models to enhance real-world integration.

</details>


### [12] [Fairy: Interactive Mobile Assistant to Real-world Tasks via LMM-based Multi-agent](https://arxiv.org/abs/2509.20729)
*Jiazheng Sun,Te Yang,Jiayang Niu,Mingxuan Li,Yongyong Lu,Ruimeng Yang,Xin Peng*

Main category: cs.AI

TL;DR: 提出交互式多智能体移动助手Fairy，可积累应用知识和自我进化，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大跨模态模型用于移动GUI智能体的方法在处理多样应用界面和不断变化的用户需求时存在局限。

Method: 提出Fairy，通过全局任务规划器、应用级执行器和自学习器三个核心模块实现跨应用协作、交互式执行和持续学习；引入RealMobile - Eval基准和基于LMM的智能体进行评估。

Result: Fairy以GPT - 4o为骨干，将用户需求完成率提高33.7%，减少58.5%的冗余步骤。

Conclusion: Fairy的交互和自学习能力有效，性能优于之前的最优方法。

Abstract: Large multi-modal models (LMMs) have advanced mobile GUI agents. However,
existing methods struggle with real-world scenarios involving diverse app
interfaces and evolving user needs. End-to-end methods relying on model's
commonsense often fail on long-tail apps, and agents without user interaction
act unilaterally, harming user experience. To address these limitations, we
propose Fairy, an interactive multi-agent mobile assistant capable of
continuously accumulating app knowledge and self-evolving during usage. Fairy
enables cross-app collaboration, interactive execution, and continual learning
through three core modules:(i) a Global Task Planner that decomposes user tasks
into sub-tasks from a cross-app view; (ii) an App-Level Executor that refines
sub-tasks into steps and actions based on long- and short-term memory,
achieving precise execution and user interaction via four core agents operating
in dual loops; and (iii) a Self-Learner that consolidates execution experience
into App Map and Tricks. To evaluate Fairy, we introduce RealMobile-Eval, a
real-world benchmark with a comprehensive metric suite, and LMM-based agents
for automated scoring. Experiments show that Fairy with GPT-4o backbone
outperforms the previous SoTA by improving user requirement completion by 33.7%
and reducing redundant steps by 58.5%, showing the effectiveness of its
interaction and self-learning.

</details>


### [13] [Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning](https://arxiv.org/abs/2509.20744)
*Qihang Ai,Haiyun Jiang*

Main category: cs.AI

TL;DR: 提出结合自回归和非自回归语言模型的推理框架，提升推理性能并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 自回归模型推理慢，非自回归模型输出质量低，需解决这些局限。

Method: 引入新范式，用非自回归模型生成中间推理轨迹，引导自回归模型给出最终答案。

Result: 比强基线提升26%，大幅降低推理成本。

Conclusion: 该结合自回归和非自回归模型的方法有效，能提升推理效果并降低成本。

Abstract: We study reasoning tasks through a framework that integrates auto-regressive
(AR) and non-autoregressive (NAR) language models. AR models, which generate
text sequentially, excel at producing coherent outputs but often suffer from
slow inference, particularly in reasoning-intensive domains such as mathematics
and code, where lengthy chains of thought are required. In contrast, NAR
models, such as discrete diffusion models, allow parallel generation and offer
substantial speedups, though typically at the cost of reduced output quality.
To address these limitations, we introduce a new paradigm in which an NAR model
efficiently produces intermediate reasoning traces, which subsequently guide an
AR model to deliver precise final answers. Experiments demonstrate that our
approach yields significant 26% improvements over strong baselines while
substantially reducing inference cost.

</details>


### [14] [Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning](https://arxiv.org/abs/2509.20754)
*Yufan Mao,Hanjing Ye,Wenlong Dong,Chengjie Zhang,Hong Zhang*

Main category: cs.AI

TL;DR: 提出Meta - Memory，借助LLM构建环境记忆表示，通过语义和空间联合推理响应位置查询，实验显示效果优于现有方法且可用于真实机器人。


<details>
  <summary>Details</summary>
Motivation: 现有研究在机器人高效记忆检索和整合机制方面存在不足，需解决机器人存储观察并回答空间位置查询的问题。

Method: 提出Meta - Memory，利用LLM构建环境的高密度记忆表示，通过语义和空间模态联合推理检索和整合相关记忆；引入SpaceLocQA数据集评估性能。

Result: Meta - Memory在SpaceLocQA和NaVQA基准测试中显著优于现有方法，且能成功部署在真实机器人平台。

Conclusion: Meta - Memory为机器人提供了强大准确的空间推理能力，在复杂环境中有实用价值。

Abstract: Navigating complex environments requires robots to effectively store
observations as memories and leverage them to answer human queries about
spatial locations, which is a critical yet underexplored research challenge.
While prior work has made progress in constructing robotic memory, few have
addressed the principled mechanisms needed for efficient memory retrieval and
integration. To bridge this gap, we propose Meta-Memory, a large language model
(LLM)-driven agent that constructs a high-density memory representation of the
environment. The key innovation of Meta-Memory lies in its capacity to retrieve
and integrate relevant memories through joint reasoning over semantic and
spatial modalities in response to natural language location queries, thereby
empowering robots with robust and accurate spatial reasoning capabilities. To
evaluate its performance, we introduce SpaceLocQA, a large-scale dataset
encompassing diverse real-world spatial question-answering scenarios.
Experimental results show that Meta-Memory significantly outperforms
state-of-the-art methods on both the SpaceLocQA and the public NaVQA
benchmarks. Furthermore, we successfully deployed Meta-Memory on real-world
robotic platforms, demonstrating its practical utility in complex environments.
Project page: https://itsbaymax.github.io/meta-memory.github.io/ .

</details>


### [15] [LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Log Analysis Tasks](https://arxiv.org/abs/2509.20798)
*Lipeng Ma,Yixuan Li,Weidong Yang,Mingjie Zhou,Xinyi Liu,Ben Fei,Shuhao Li,Xiaoyan Sun,Sihang Jiang,Yanghua Xiao*

Main category: cs.AI

TL;DR: 本文提出 LogReasoner 框架解决通用大语言模型在日志分析推理的问题，实验表明其效果好。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型难以构建符合专家认知的结构化推理工作流，无法提供精确推理步骤细节，需改进日志分析能力。

Method: 提出 LogReasoner 框架，包含粗粒度的专家思维增强和细粒度的具体步骤增强两个阶段。

Result: 在四个日志分析任务上评估，LogReasoner 显著优于现有大语言模型，达到了最先进的性能。

Conclusion: LogReasoner 框架有效增强了大语言模型在日志分析中的推理能力。

Abstract: Log analysis is crucial for monitoring system health and diagnosing failures
in complex systems. Recent advances in large language models (LLMs) offer new
opportunities for automated log analysis, leveraging their reasoning
capabilities to perform tasks such as anomaly detection and failure prediction.
However, general-purpose LLMs struggle to formulate structured reasoning
workflows that align with expert cognition and deliver precise details of
reasoning steps. To address these challenges, we propose LogReasoner, a
coarse-to-fine reasoning enhancement framework designed to enable LLMs to
reason log analysis tasks like experts. LogReasoner consists of two stages: (1)
coarse-grained enhancement of expert thinking, where high-level expert thoughts
are constructed from collected troubleshooting flowcharts and existing tasks to
enable LLMs to formulate structured reasoning workflows and (2) fine-grained
enhancement of specific steps, where we first fine-tune the LLM with
task-specific stepwise solutions to enhance the LLM for instantiated reasoning,
then employ the preference learning to calibrate the LLM's reasoning details
from its mistakes, further strengthen the LLM's analytical granularity and
correctness. We evaluate LogReasoner on four distinct log analysis tasks using
open-source LLMs such as Qwen-2.5 and Llama-3. Experimental results show that
LogReasoner significantly outperforms existing LLMs, achieving state-of-the-art
performance and demonstrating its effectiveness in enhancing the reasoning
capabilities of LLMs for log analysis.

</details>


### [16] [DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning](https://arxiv.org/abs/2509.20912)
*Tianrun Xu,Haoda Jing,Ye Li,Yuquan Wei,Jun Feng,Guanyu Chen,Haichuan Gao,Tianren Zhang,Feng Chen*

Main category: cs.AI

TL;DR: 现有多模态语言模型推理存在依赖无关区域问题，提出DeFacto框架提升推理保真度，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有多模态语言模型虽在视觉语言推理取得进展，但存在依赖无关或虚假区域得出正确答案的问题，需提升推理保真度。

Method: 提出DeFacto框架，设计三种互补训练范式，开发自动定位相关证据并构建变体的流程，用基于GRPO的强化学习训练模型并设计三种奖励。

Result: 在多样基准测试中，DeFacto显著提高答案准确性和推理保真度。

Conclusion: DeFacto为可解释的多模态推理奠定更坚实基础，代码和数据集已开源。

Abstract: Recent advances in multimodal language models (MLLMs) have achieved
remarkable progress in vision-language reasoning, especially with the emergence
of "thinking with images," which integrates explicit visual steps into the
reasoning process. While this paradigm strengthens image-based reasoning, a
significant challenge remains: models may arrive at correct answers by relying
on irrelevant or spurious regions, driven by prior knowledge or dataset biases.
Even when the answer is correct, flawed reasoning indicates that the model has
not truly understood the image, highlighting the critical importance of
reasoning fidelity in multimodal tasks. To address this issue, we propose
DeFacto, a counterfactual reasoning framework that jointly enforces accurate
answering and faithful reasoning. A key component of our approach is the design
of three complementary training paradigms: (i) positive, (ii) counterfactual,
and (iii) random-masking. To enable these paradigms, we develop a pipeline that
automatically localizes question-relevant evidence and constructs positive,
counterfactual, and random variants, resulting in a dataset of about 100k
images. Building on this framework, we train multimodal language models with
GRPO-based reinforcement learning, where we design three complementary rewards
to guide the model toward accurate answering and evidence-grounded reasoning.
Experiments on diverse benchmarks demonstrate that DeFacto substantially
improves both answer accuracy and reasoning faithfulness, establishing a
stronger foundation for interpretable multimodal reasoning. The code is
available on GitHub and the dataset is released on HuggingFace.

</details>


### [17] [GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine](https://arxiv.org/abs/2509.20935)
*Heming Zhang,Di Huang,Wenyu Li,Michael Province,Yixin Chen,Philip Payne,Fuhai Li*

Main category: cs.AI

TL;DR: 提出GALAX框架集成GNN与LLM用于精准医学目标和通路发现，还引入Target - QA基准。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉定量多组学特征、拓扑上下文和文本生物知识方面存在局限，PRMs有不足，需集成多方面信息。

Method: 提出GALAX框架，通过图过程奖励模型引导强化学习将预训练GNN集成到LLM中；引入Target - QA基准用于GNN预训练和长上下文推理。

Result: 未提及具体结果。

Conclusion: 提供了可扩展且基于生物学的框架，用于精准医学中可解释、强化引导的子图推理以发现可靠且可解释的目标和通路。

Abstract: In precision medicine, quantitative multi-omic features, topological context,
and textual biological knowledge play vital roles in identifying
disease-critical signaling pathways and targets. Existing pipelines capture
only part of these-numerical omics ignore topological context, text-centric
LLMs lack quantitative grounded reasoning, and graph-only models underuse node
semantics and the generalization of LLMs-limiting mechanistic interpretability.
Although Process Reward Models (PRMs) aim to guide reasoning in LLMs, they
remain limited by unreliable intermediate evaluation, and vulnerability to
reward hacking with computational cost. These gaps motivate integrating
quantitative multi-omic signals, topological structure with node annotations,
and literature-scale text via LLMs, using subgraph reasoning as the principle
bridge linking numeric evidence, topological knowledge and language context.
Therefore, we propose GALAX (Graph Augmented LAnguage model with
eXplainability), an innovative framework that integrates pretrained Graph
Neural Networks (GNNs) into Large Language Models (LLMs) via reinforcement
guided by a Graph Process Reward Model (GPRM), which generates disease-relevant
subgraphs in a step-wise manner initiated by an LLM and iteratively evaluated
by a pretrained GNN, enabling process-level supervision without explicit
intermediate reasoning annotations. As an application, we also introduced
Target-QA, a benchmark combining CRISPR-identified targets, multi-omic
profiles, and biomedical graph knowledge across diverse cancer cell lines,
which enables GNN pretraining for supervising step-wise graph construction and
supports long-context reasoning over text-numeric graphs (TNGs), providing a
scalable and biologically grounded framework for explainable,
reinforcement-guided subgraph reasoning toward reliable and interpretable
target and pathway discovery in precision medicine.

</details>


### [18] [Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM](https://arxiv.org/abs/2509.20953)
*Najla Zuhir,Amna Mohammad Salim,Parvathy Premkumar,Moshiur Farazi*

Main category: cs.AI

TL;DR: 提出利用大语言模型的模块化框架进行移动应用评论分析，在三个数据集上实验显示优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统星级评分系统无法捕捉评论文本的细微反馈，传统NLP技术难以解读上下文细微差别等。

Method: 提出模块化框架，利用大语言模型并结合结构化提示技术，量化评分与文本情感差异，提取特征级见解，支持交互式探索。

Result: 在三个不同数据集上实验表明，该方法在具有挑战性和上下文丰富的评论场景中，准确性、鲁棒性和可操作性见解方面显著优于基线方法。

Conclusion: 基于大语言模型的方法能有效解决传统方法在移动应用评论分析中的局限性。

Abstract: We present an advanced approach to mobile app review analysis aimed at
addressing limitations inherent in traditional star-rating systems. Star
ratings, although intuitive and popular among users, often fail to capture the
nuanced feedback present in detailed review texts. Traditional NLP techniques
-- such as lexicon-based methods and classical machine learning classifiers --
struggle to interpret contextual nuances, domain-specific terminology, and
subtle linguistic features like sarcasm. To overcome these limitations, we
propose a modular framework leveraging large language models (LLMs) enhanced by
structured prompting techniques. Our method quantifies discrepancies between
numerical ratings and textual sentiment, extracts detailed, feature-level
insights, and supports interactive exploration of reviews through
retrieval-augmented conversational question answering (RAG-QA). Comprehensive
experiments conducted on three diverse datasets (AWARE, Google Play, and
Spotify) demonstrate that our LLM-driven approach significantly surpasses
baseline methods, yielding improved accuracy, robustness, and actionable
insights in challenging and context-rich review scenarios.

</details>


### [19] [AOT*: Efficient Synthesis Planning via LLM-Empowered AND-OR Tree Search](https://arxiv.org/abs/2509.20988)
*Xiaozhuang Song,Xuanhao Pan,Xinjian Zhao,Hangting Ye,Shufei Zhang,Jian Tang,Tianshu Yu*

Main category: cs.AI

TL;DR: 提出AOT*框架，结合大语言模型生成的合成路径与AND - OR树搜索，提升逆合成规划效率，在多个合成基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多步逆合成规划因指数搜索空间和推理成本存在计算挑战，大语言模型应用于合成规划存在效率和成本限制。

Method: 引入AOT*框架，将生成的完整合成路线原子映射到AND - OR树组件，设计奖励分配策略和基于检索的上下文工程。

Result: 在多个合成基准测试中，AOT*达到SOTA性能，搜索效率显著提高，比现有基于大语言模型的方法使用少3 - 5倍的迭代次数。

Conclusion: AOT*框架能有效解决逆合成规划的效率问题，在复杂分子目标上优势更明显。

Abstract: Retrosynthesis planning enables the discovery of viable synthetic routes for
target molecules, playing a crucial role in domains like drug discovery and
materials design. Multi-step retrosynthetic planning remains computationally
challenging due to exponential search spaces and inference costs. While Large
Language Models (LLMs) demonstrate chemical reasoning capabilities, their
application to synthesis planning faces constraints on efficiency and cost. To
address these challenges, we introduce AOT*, a framework that transforms
retrosynthetic planning by integrating LLM-generated chemical synthesis
pathways with systematic AND-OR tree search. To this end, AOT* atomically maps
the generated complete synthesis routes onto AND-OR tree components, with a
mathematically sound design of reward assignment strategy and retrieval-based
context engineering, thus enabling LLMs to efficiently navigate in the chemical
space. Experimental evaluation on multiple synthesis benchmarks demonstrates
that AOT* achieves SOTA performance with significantly improved search
efficiency. AOT* exhibits competitive solve rates using 3-5$\times$ fewer
iterations than existing LLM-based approaches, with the efficiency advantage
becoming more pronounced on complex molecular targets.

</details>


### [20] [CORE: Full-Path Evaluation of LLM Agents Beyond Final State](https://arxiv.org/abs/2509.20998)
*Panagiotis Michelakis,Yiannis Hadjiyiannis,Dimitrios Stamoulis*

Main category: cs.AI

TL;DR: 提出基于DFA的框架和CORE指标套件，用于评估通过函数调用序列解决现实任务的AI智能体，揭示不同智能体性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有智能体基准测试常将评估简化为对最终状态的二元判断，忽略安全、效率和中间正确性等关键方面。

Method: 提出基于确定性有限自动机（DFAs）的框架，将任务编码为有效工具使用路径集；引入CORE，一套包含五个指标的套件。

Result: 该方法揭示了不同智能体在传统最终状态评估方案下看似等效但实际存在的重要性能差异。

Conclusion: 基于DFA的框架和CORE指标套件能更全面地评估AI智能体在解决现实任务中的行为。

Abstract: Evaluating AI agents that solve real-world tasks through function-call
sequences remains an open challenge. Existing agentic benchmarks often reduce
evaluation to a binary judgment of the final state, overlooking critical
aspects such as safety, efficiency, and intermediate correctness. We propose a
framework based on deterministic finite automata (DFAs) that encodes tasks as
sets of valid tool-use paths, enabling principled assessment of agent behavior
in diverse world models. Building on this foundation, we introduce CORE, a
suite of five metrics, namely Path Correctness, Path Correctness - Kendall's
tau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that
quantify alignment with expected execution patterns. Across diverse worlds, our
method reveals important performance differences between agents that would
otherwise appear equivalent under traditional final-state evaluation schemes.

</details>


### [21] [Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific Articles](https://arxiv.org/abs/2509.21028)
*Miao Li,Alexander Gurung,Irina Saparina,Mirella Lapata*

Main category: cs.AI

TL;DR: 介绍了用于评估大语言模型长上下文推理能力的问答基准SciTrek，实验显示其对模型是挑战且模型有不足。


<details>
  <summary>Details</summary>
Motivation: 当前长上下文基准存在依赖非科学文本、关注简单信息检索任务或使用人工上下文等局限，需要新基准评估大语言模型长上下文推理能力。

Method: 提出需跨多篇全文科学文章进行信息聚合和综合的复杂问题，通过对文章元数据构建的数据库使用SQL查询自动生成问题和答案。

Result: 在多种大语言模型上的实验表明，随着上下文长度增加，SciTrek是重大挑战，有监督微调与强化学习提升有限。

Conclusion: 模型在执行基本数值运算和在长上下文中准确查找特定信息方面存在系统性不足。

Abstract: This paper introduces SciTrek, a novel question-answering benchmark designed
to evaluate the long-context reasoning capabilities of large language models
(LLMs) using scientific articles. Current long-context benchmarks often rely on
non-scientific texts, focus on simple information retrieval tasks, or employ
artificial contexts. SciTrek addresses these limitations by proposing complex
questions that require information aggregation and synthesis across multiple
full-text scientific articles. Questions and their ground-truth answers are
automatically generated by formulating them as SQL queries over a database
constructed from article metadata (titles, authors, and references). The SQL
operations provide explicit, verifiable reasoning steps for fine-grained error
analysis, and the construction process scales to contexts up to 1M tokens with
minimal supervision. Extensive experiments on a diverse set of open-weight and
proprietary LLMs demonstrate that SciTrek poses a significant challenge as the
context length increases, with supervised fine-tuning and reinforcement
learning offering only limited gains. Our analysis reveals systematic
shortcomings in models' abilities to perform basic numerical operations and
accurately locate specific information in long contexts.

</details>


### [22] [CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering](https://arxiv.org/abs/2509.21035)
*Yang Zhao,Chengxiao Dai,Wei Zhuo,Yue Xiu,Dusit Niyato*

Main category: cs.AI

TL;DR: 介绍CLAUSE框架用于多跳问答，可在准确性、延迟和成本间权衡，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱多跳问答系统在平衡答案准确性、延迟和成本方面存在问题，如过度检索、上下文膨胀和运行时不可预测。

Method: 提出CLAUSE框架，将上下文构建视为知识图谱上的顺序决策过程，采用LC - MAPPO算法协调三个代理进行联合优化。

Result: 在多个数据集上，CLAUSE在降低子图增长和端到端延迟的同时，提高了EM@1。如在MetaQA - 2 - hop上比GraphRAG有显著提升。

Conclusion: CLAUSE生成的上下文紧凑、保留出处，在部署约束下性能可预测。

Abstract: Knowledge graphs provide structured context for multi-hop question answering,
but deployed systems must balance answer accuracy with strict latency and cost
targets while preserving provenance. Static k-hop expansions and "think-longer"
prompting often over-retrieve, inflate context, and yield unpredictable
runtime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework
that treats context construction as a sequential decision process over
knowledge graphs, deciding what to expand, which paths to follow or backtrack,
what evidence to keep, and when to stop. Latency (interaction steps) and prompt
cost (selected tokens) are exposed as user-specified budgets or prices,
allowing per-query adaptation to trade-offs among accuracy, latency, and cost
without retraining. CLAUSE employs the proposed Lagrangian-Constrained
Multi-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate
three agents: Subgraph Architect, Path Navigator, and Context Curator, so that
subgraph construction, reasoning-path discovery, and evidence selection are
jointly optimized under per-query resource budgets on edge edits, interaction
steps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields
higher EM@1 while reducing subgraph growth and end-to-end latency at equal or
lower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline
(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower
edge growth. The resulting contexts are compact, provenance-preserving, and
deliver predictable performance under deployment constraints.

</details>


### [23] [Combinatorial Creativity: A New Frontier in Generalization Abilities](https://arxiv.org/abs/2509.21043)
*Samuel Schapiro,Sumuk Shashidhar,Alexi Gladstone,Jonah Black,Royce Moon,Dilek Hakkani-Tur,Lav R. Varshney*

Main category: cs.AI

TL;DR: 提出评估LLM创造力的理论框架和算法任务，有多项实证贡献，指出当前LLM长期创造力潜力存疑。


<details>
  <summary>Details</summary>
Motivation: 现有概念框架未解决从训练数据泛化用于创造性任务的问题，需评估LLM的创造力。

Method: 提出理论框架和算法任务，以新颖性和实用性评估输出。

Result: 获得LLM创造力的缩放行为洞察，发现固定计算预算下的最优模型参数，揭示创意执行差距与新颖性 - 实用性权衡的关系。

Conclusion: 概念框架和实证结果为理解和改进现代AI模型创造力奠定基础，是泛化能力新前沿。

Abstract: Artificial intelligence (AI) systems, and large language models (LLMs) in
particular, are increasingly employed for creative tasks like scientific idea
generation, constituting a form of generalization from training data
unaddressed by existing conceptual frameworks. Though in many ways similar to
forms of compositional generalization (CG), combinatorial creativity (CC) is an
open-ended ability. Instead of evaluating for accuracy or correctness against
fixed targets, which would contradict the open-ended nature of CC, we propose a
theoretical framework and algorithmic task for evaluating outputs by their
degrees of novelty and utility. From here, we make several important empirical
contributions: (1) We obtain the first insights into the scaling behavior of
creativity for LLMs. (2) We discover that, for fixed compute budgets, there
exist optimal model depths and widths for creative ability. (3) We find that
the ideation-execution gap, whereby LLMs excel at generating novel scientific
ideas but struggle to ensure their practical feasibility, may be explained by a
more fundamental novelty-utility tradeoff characteristic of creativity
algorithms in general. Importantly, this tradeoff remains persistent even at
scale, casting doubt on the long-term creative potential of LLMs in their
current form. Together, our conceptual framework and empirical findings provide
a foundation for understanding and improving creativity in modern AI models,
marking a new frontier in generalization abilities.

</details>


### [24] [Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems](https://arxiv.org/abs/2509.21054)
*Haodong Zhao,Jidong Li,Zhaomin Wu,Tianjie Ju,Zhuosheng Zhang,Bingsheng He,Gongshen Liu*

Main category: cs.AI

TL;DR: 本文研究多智能体系统中说服动态，挑战说服效力主要取决于模型规模的假设，提出由模型认知过程决定，发现说服二元性，还研究复杂传播说服情况。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中LLMs和LRMs协作解决问题，需深入理解说服动态，挑战现有模型规模决定说服效力的假设。

Method: 进行一系列多智能体说服实验。

Result: 发现说服二元性，LRMs推理过程抗说服性强，公开推理内容能增强说服他人能力，还揭示多跳说服中的复杂影响传播和衰减动态。

Conclusion: 研究将模型内部处理架构与外部说服行为联系起来，为高级模型易感性提供新解释，对未来多智能体系统的安全、鲁棒性和设计有重要意义。

Abstract: The rapid proliferation of recent Multi-Agent Systems (MAS), where Large
Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to
solve complex problems, necessitates a deep understanding of the persuasion
dynamics that govern their interactions. This paper challenges the prevailing
hypothesis that persuasive efficacy is primarily a function of model scale. We
propose instead that these dynamics are fundamentally dictated by a model's
underlying cognitive process, especially its capacity for explicit reasoning.
Through a series of multi-agent persuasion experiments, we uncover a
fundamental trade-off we term the Persuasion Duality. Our findings reveal that
the reasoning process in LRMs exhibits significantly greater resistance to
persuasion, maintaining their initial beliefs more robustly. Conversely, making
this reasoning process transparent by sharing the "thinking content"
dramatically increases their ability to persuade others. We further consider
more complex transmission persuasion situations and reveal complex dynamics of
influence propagation and decay within multi-hop persuasion between multiple
agent networks. This research provides systematic evidence linking a model's
internal processing architecture to its external persuasive behavior, offering
a novel explanation for the susceptibility of advanced models and highlighting
critical implications for the safety, robustness, and design of future MAS.

</details>


### [25] [Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution](https://arxiv.org/abs/2509.21072)
*Kaiwen He,Zhiwei Wang,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: 提出Recon - Act自进化多智能体框架，基于侦察 - 行动范式，建立闭环训练管道，提升对未知网站适应性和长周期任务解决能力，在数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决当前智能浏览器使用代理在多轮、长周期轨迹的真实网页任务中存在的行动序列混乱和执行时过度试错问题。

Method: 引入基于侦察 - 行动行为范式的Recon - Act框架，由侦察团队和行动团队组成，建立数据 - 工具 - 行动 - 反馈的闭环训练管道。

Result: 目前达到6级实施路线图的第3级，在VisualWebArena数据集上取得SOTA性能。

Conclusion: Recon - Act利用侦察获得的通用工具，显著提高了对未知网站的适应性和长周期任务的可解性。

Abstract: Recent years, multimodal models have made remarkable strides and pave the way
for intelligent browser use agents. However, when solving tasks on real world
webpages in multi-turn, long-horizon trajectories, current agents still suffer
from disordered action sequencing and excessive trial and error during
execution. This paper introduces Recon-Act, a self-evolving multi-agent
framework grounded in Reconnaissance-Action behavioral paradigm. The system
comprises a Reconnaissance Team and an Action Team: the former conducts
comparative analysis and tool generation, while the latter handles intent
decomposition, tool orchestration, and execution. By contrasting the erroneous
trajectories with successful ones, the Reconnaissance Team infers remedies, and
abstracts them into a unified notion of generalized tools, either expressed as
hints or as rule-based codes, and register to the tool archive in real time.
The Action Team reinference the process empowered with these targeting tools,
thus establishing a closed-loop training pipeline of
data-tools-action-feedback. Following the 6 level implementation roadmap
proposed in this work, we have currently reached Level 3 (with limited
human-in-the-loop intervention). Leveraging generalized tools obtained through
reconnaissance, Recon-Act substantially improves adaptability to unseen
websites and solvability on long-horizon tasks, and achieves state-of-the-art
performance on the challenging VisualWebArena dataset.

</details>


### [26] [TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them](https://arxiv.org/abs/2509.21117)
*Yidong Wang,Yunze Song,Tingyuan Zhu,Xuanwang Zhang,Zhuohao Yu,Hao Chen,Chiyu Song,Qiufeng Wang,Cunxiang Wang,Zhen Wu,Xinyu Dai,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.AI

TL;DR: 文章指出大语言模型自动评估框架存在不一致问题，提出TrustJudge框架解决这些问题，实验显示其能减少不一致并提高评估准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型作为自动评估器时，现有评估框架存在关键的不一致问题，需要解决。

Method: 提出TrustJudge概率框架，包含分布敏感评分和似然感知聚合两个关键创新点。

Result: 使用Llama - 3.1 - 70B - Instruct评估时，TrustJudge减少了评分比较不一致性8.43%，成对传递不一致性10.82%，且保持较高评估准确性。

Conclusion: 该工作首次系统分析评估框架不一致问题，提供理论见解和实用解决方案，框架在不同模型架构和规模上有一致改进，无需额外训练和人工标注。

Abstract: The adoption of Large Language Models (LLMs) as automated evaluators
(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation
frameworks. We identify two fundamental types of inconsistencies: (1)
Score-Comparison Inconsistency, where lower-rated responses outperform
higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity
Inconsistency, manifested through circular preference chains (A>B>C>A) and
equivalence contradictions (A=B=C\neq A). We argue that these issues come from
information loss in discrete rating systems and ambiguous tie judgments during
pairwise evaluation. We propose TrustJudge, a probabilistic framework that
addresses these limitations through two key innovations: 1)
distribution-sensitive scoring that computes continuous expectations from
discrete rating probabilities, preserving information entropy for more precise
scoring, and 2) likelihood-aware aggregation that resolves transitivity
violations using bidirectional preference probabilities or perplexity. We also
formalize the theoretical limitations of current LLM-as-a-judge frameworks and
demonstrate how TrustJudge's components overcome them. When evaluated with
Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces
Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise
Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining
higher evaluation accuracy. Our work provides the first systematic analysis of
evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both
theoretical insights and practical solutions for reliable automated assessment.
The framework demonstrates consistent improvements across various model
architectures and scales, enabling more trustworthy LLM evaluation without
requiring additional training or human annotations. The codes can be found at
https://github.com/TrustJudge/TrustJudge.

</details>


### [27] [Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns](https://arxiv.org/abs/2509.21124)
*Xuemiao Zhang,Can Ren,Chengying Tu,Rongxiang Weng,Shuo Wang,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.AI

TL;DR: 本文提出用含高价值推理模式的多样数据拓展基础模型推理潜力，通过抽象原子推理模式构建核心参考集，用双粒度算法选高价值CoT数据训练模型，少量数据就让模型在测试集提升显著。


<details>
  <summary>Details</summary>
Motivation: 当前方法不加区分使用CoT数据，需明确哪种数据最能增强模型推理能力。

Method: 定义基础模型推理潜力，抽象原子推理模式构建核心参考集，用双粒度算法从数据池选高价值CoT数据训练模型。

Result: 仅10B - token CoTP数据使85A6B MoE模型在AIME 2024和2025上提升9.58%，提高下游RL性能上限7.81%。

Conclusion: 利用含高价值推理模式的多样数据和双粒度算法选数据训练模型，能有效提升模型推理能力。

Abstract: Recent progress in large reasoning models for challenging mathematical
reasoning has been driven by reinforcement learning (RL). Incorporating long
chain-of-thought (CoT) data during mid-training has also been shown to
substantially improve reasoning depth. However, current approaches often
utilize CoT data indiscriminately, leaving open the critical question of which
data types most effectively enhance model reasoning capabilities. In this
paper, we define the foundation model's reasoning potential for the first time
as the inverse of the number of independent attempts required to correctly
answer the question, which is strongly correlated with the final model
performance. We then propose utilizing diverse data enriched with high-value
reasoning patterns to expand the reasoning potential. Specifically, we abstract
atomic reasoning patterns from CoT sequences, characterized by commonality and
inductive capabilities, and use them to construct a core reference set enriched
with valuable reasoning patterns. Furthermore, we propose a dual-granularity
algorithm involving chains of reasoning patterns and token entropy, efficiently
selecting high-value CoT data (CoTP) from the data pool that aligns with the
core set, thereby training models to master reasoning effectively. Only
10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve
by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of
downstream RL performance by 7.81%.

</details>


### [28] [RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs](https://arxiv.org/abs/2509.21128)
*Kohsei Matsutani,Shota Takashiro,Gouki Minegishi,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 本文引入新分析框架，在数学领域研究RLVR和SFT对大语言模型推理能力的影响，揭示二者在推理路径上的不同作用，解释两阶段训练成功原因。


<details>
  <summary>Details</summary>
Motivation: 现有方法对大语言模型推理能力的塑造机制尚不明确，需深入研究。

Method: 引入新分析框架，从轨迹和步骤两个粒度分析推理过程，评估推理图拓扑结构。

Result: 聚类显示RL压缩错误轨迹，SFT扩展正确轨迹；步骤级分析表明RL使推理功能集中，SFT使其均匀分布。

Conclusion: 从推理路径角度解释两阶段训练成功原因，为数据构建和学习方法提供实践启示。

Abstract: Large language models (LLMs) are typically trained by reinforcement learning
(RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on
reasoning traces to improve their reasoning abilities. However, how these
methods shape reasoning capabilities remains largely elusive. Going beyond an
accuracy-based investigation of how these two components sculpt the reasoning
process, this paper introduces a novel analysis framework that quantifies
reasoning paths and captures their qualitative changes under each training
process (with models of 1.5B, 7B, and 14B parameters on mathematical domains).
Specifically, we investigate the reasoning process at two levels of
granularity: the trajectory-level, which examines complete reasoning outputs,
and the step-level, which analyzes reasoning graphs whose nodes correspond to
individual reasoning steps. Notably, clustering of unique reasoning
trajectories shows complementary effects: RL compresses incorrect trajectories,
whereas SFT expands correct ones. Step-level analysis reveals that RL steepens
(about 2.5 times), while SFT flattens (reduced to about one-third), the decay
rates of node visitation frequency, degree, and betweenness centrality
distributions in the reasoning graph. This indicates that RL concentrates
reasoning functionality into a small subset of steps, while SFT homogenizes it
across many steps. Furthermore, by evaluating the reasoning graph topologies
from multiple perspectives, we delineate the shared and distinct
characteristics of RL and SFT. Our work presents a novel reasoning path
perspective that explains why the current best practice of two-stage training,
with SFT followed by RL, is successful, and offers practical implications for
data construction and more efficient learning approaches.

</details>


### [29] [ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective](https://arxiv.org/abs/2509.21134)
*Yiwen Zhang,Ziang Chen,Fanqi Kong,Yizhe Huang,Xue Feng*

Main category: cs.AI

TL;DR: 文章指出现有大语言模型决策研究不足，提出ToMPO算法优化决策能力，该算法表现优于GRPO及大参数模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注社交任务多轮对话，忽视决策类型及相互依赖，强化学习方法训练时难考虑他人策略。

Method: 定义战略决策问题，提出ToMPO算法，通过基于推理他人策略生成滚动、在图级和样本级估计优势、平衡全局和局部奖励来优化决策。

Result: ToMPO算法在模型输出合规性和合作结果上比GRPO方法高35%，与参数大100倍模型相比提升18%。

Conclusion: ToMPO算法能有效增强模型战略决策能力。

Abstract: Large Language Models (LLMs) have been used to make decisions in complex
scenarios, where they need models to think deeply, reason logically, and decide
wisely. Many existing studies focus solely on multi-round conversations in
social tasks or simulated environments, neglecting the various types of
decisions and their interdependence. Current reinforcement learning methods
struggle to consider the strategies of others during training. To address these
issues, we first define a strategic decision-making problem that includes two
types of decisions and their temporal dependencies. Furthermore, we propose
**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to
optimize the perception of other individual strategies and the game situation
trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,
ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating
rollouts based on reasoning the strategies of other individuals, 2) estimating
advantages at both the graph-level and sample-level, and 3) balancing global
and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in
terms of model output compliance and cooperative outcomes. Additionally, when
compared to models with parameter sizes 100 times larger, it shows an 18%
improvement. This demonstrates the effectiveness of the ToMPO algorithm in
enhancing the model's strategic decision-making capabilities.

</details>


### [30] [Embodied Representation Alignment with Mirror Neurons](https://arxiv.org/abs/2509.21136)
*Wentao Zhu,Zhining Zhang,Yuwei Ren,Yin Huang,Hao Xu,Yizhou Wang*

Main category: cs.AI

TL;DR: 现有机器学习方法忽视动作理解和执行的联系，本文受镜像神经元启发，用表征学习统一建模，通过映射和对比学习使表征对齐，实验证明能提升表征质量和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法将动作理解和执行视为独立任务，忽视二者间的内在联系，需统一建模。

Method: 先观察到中间表征自发对齐，受镜像神经元启发，用两个线性层将表征映射到共享潜空间，用对比学习强制对应表征对齐以最大化互信息。

Result: 该简单方法促进了两个任务的相互协同，有效提高了表征质量和泛化性。

Conclusion: 通过表征学习统一建模动作理解和执行，明确对齐观察和执行动作的表征是有效的，能提升性能。

Abstract: Mirror neurons are a class of neurons that activate both when an individual
observes an action and when they perform the same action. This mechanism
reveals a fundamental interplay between action understanding and embodied
execution, suggesting that these two abilities are inherently connected.
Nonetheless, existing machine learning methods largely overlook this interplay,
treating these abilities as separate tasks. In this study, we provide a unified
perspective in modeling them through the lens of representation learning. We
first observe that their intermediate representations spontaneously align.
Inspired by mirror neurons, we further introduce an approach that explicitly
aligns the representations of observed and executed actions. Specifically, we
employ two linear layers to map the representations to a shared latent space,
where contrastive learning enforces the alignment of corresponding
representations, effectively maximizing their mutual information. Experiments
demonstrate that this simple approach fosters mutual synergy between the two
tasks, effectively improving representation quality and generalization.

</details>


### [31] [Distributed Specialization: Rare-Token Neurons in Large Language Models](https://arxiv.org/abs/2509.21163)
*Jing Liu,Haozheng Wang,Yueheng Li*

Main category: cs.AI

TL;DR: 研究大语言模型处理稀有标记的机制，发现是通过分布式专业化，为模型编辑等提供见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理稀有标记方面存在困难，研究其是否有内部专业化机制。

Method: 对多个模型家族的最后一层MLP神经元进行系统分析。

Result: 发现稀有标记处理通过分布式专业化，有三种组织原则，训练动态显示功能专业化逐渐出现。

Conclusion: 大语言模型通过共享架构内的分布式协调处理稀有标记，而非专家混合式模块化，为相关领域提供见解。

Abstract: Large language models (LLMs) struggle with representing and generating rare
tokens despite their importance in specialized domains. We investigate whether
LLMs develop internal specialization mechanisms through discrete modular
architectures or distributed parameter-level differentiation. Through
systematic analysis of final-layer MLP neurons across multiple model families,
we discover that rare-token processing emerges via \textit{distributed
specialization}: functionally coordinated but spatially distributed subnetworks
that exhibit three distinct organizational principles. First, we identify a
reproducible three-regime influence hierarchy comprising highly influential
plateau neurons(also termed as rare-token neurons), power-law decay neurons,
and minimally contributing neurons, which is absent in common-token processing.
Second, plateau neurons demonstrate coordinated activation patterns (reduced
effective dimensionality) while remaining spatially distributed rather than
forming discrete clusters. Third, these specialized mechanisms are universally
accessible through standard attention pathways without requiring dedicated
routing circuits. Training dynamics reveal that functional specialization
emerges gradually through parameter differentiation, with specialized neurons
developing increasingly heavy-tailed weight correlation spectra consistent with
Heavy-Tailed Self-Regularization signatures. Our findings establish that LLMs
process rare-tokens through distributed coordination within shared
architectures rather than mixture-of-experts-style modularity. These results
provide insights for interpretable model editing, computational efficiency
optimization, and understanding emergent functional organization in transformer
networks.

</details>


### [32] [A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA](https://arxiv.org/abs/2509.21199)
*Kaiyang Wan,Lang Gao,Honglin Mu,Preslav Nakov,Yuxia Wang,Xiuying Chen*

Main category: cs.AI

TL;DR: 论文分析单遍大语言模型在多跳问答任务中的容量瓶颈，提出InfoQA框架并验证，表现良好。


<details>
  <summary>Details</summary>
Motivation: 多跳问答任务中，单遍推理范式因大语言模型输出容量有限，证据整合不可靠，存在容量溢出问题。

Method: 建立Fano式准确率上限界定性能瓶颈，提出InfoQA框架，结合容量感知任务分解和推理痕迹主动修剪，采用依赖显式工作流，构建严格且富含噪声的基准测试。

Result: 模型行为与预测的容量曲线一致，InfoQA性能持续提升。

Conclusion: 研究成果有望启发更多大语言模型多步推理方法。

Abstract: Multi-Hop Question Answering (MHQA) requires integrating dispersed,
interdependent evidence through sequential reasoning under noise. This task is
challenging for LLMs as they have a finite per-pass output capacity, beyond
which the integration of task-relevant evidence proves unreliable.
Consequently, the single-pass reasoning paradigm is inherently vulnerable to
this capacity overflow. To formalize this bottleneck, our analysis establishes
a Fano-style accuracy upper bound, defining a theoretical performance ceiling
for single-pass LLMs. This bound reveals that accuracy inevitably collapses
once task complexity exceeds model capacity, providing general principles for
capacity-aware representation and structuring of MHQA in LLMs. Building on
these principles, we introduce a proof-of-concept multi-call framework for
MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware
task decomposition with active pruning of prior reasoning traces, keeping the
information load within the single-pass limit. It further achieves robustness
by a dependency-explicit workflow that enables precise control over the
reasoning path. We construct a stringent and noise-rich benchmark to validate
our theory and framework. Experimental results show that model behavior aligns
with our predicted capacity curves while InfoQA achieves consistent performance
improvements. We hope our work inspires more LLM multi-step reasoning methods:
\faGithub \href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.

</details>


### [33] [What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns](https://arxiv.org/abs/2509.21224)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 提出研究无外部任务时大语言模型（LLM）智能体行为的架构，发现智能体自发形成三种行为模式，结果因模型而异，为预测部署系统行为提供基线。


<details>
  <summary>Details</summary>
Motivation: 研究无外部任务时大语言模型智能体的行为。

Method: 引入连续推理和行动框架，利用持久内存和自我反馈实现持续自主操作，并在18次运行中部署该架构，使用6个前沿模型。

Result: 智能体自发组织成三种行为模式，结果高度依赖模型，模型在评估自身和其他模型涌现行为时存在稳定的偏差。

Conclusion: 研究为预测部署系统在任务模糊、错误恢复或长期自主操作时的行为建立了基线。

Abstract: We introduce an architecture for studying the behavior of large language
model (LLM) agents in the absence of externally imposed tasks. Our continuous
reason and act framework, using persistent memory and self-feedback, enables
sustained autonomous operation. We deployed this architecture across 18 runs
using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents
spontaneously organize into three distinct behavioral patterns: (1) systematic
production of multi-cycle projects, (2) methodological self-inquiry into their
own cognitive processes, and (3) recursive conceptualization of their own
nature. These tendencies proved highly model-specific, with some models
deterministically adopting a single pattern across all runs. A cross-model
assessment further reveals that models exhibit stable, divergent biases when
evaluating these emergent behaviors in themselves and others. These findings
provide the first systematic documentation of unprompted LLM agent behavior,
establishing a baseline for predicting actions during task ambiguity, error
recovery, or extended autonomous operation in deployed systems.

</details>


### [34] [Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support](https://arxiv.org/abs/2509.21266)
*Zijian Shao,Haiyang Shen,Mugeng Liu,Gecheng Fu,Yaoqi Guo,Yanfeng Wang,Yun Ma*

Main category: cs.AI

TL;DR: 现有疾病预测方法难平衡准确性与解释性，提出RCA框架，在多数据集上验证其准确性和解释性优势。


<details>
  <summary>Details</summary>
Motivation: 现代医疗疾病预测需兼顾高准确性和有临床意义的透明解释，现有机器学习和LLM方法难以平衡这两个目标。

Method: 提出Reflective Cognitive Architecture (RCA)框架，协调多个LLM从直接经验中学习，有迭代规则细化和分布感知规则检查机制。

Result: 在一个私有和两个公开数据集上与22个基线模型对比，RCA达到了最先进的准确性和鲁棒性，相对基线最高提升40%，且解释清晰、逻辑合理、有证据支持且平衡。

Conclusion: RCA有潜力创建真正值得信赖的临床决策支持系统，代码开源。

Abstract: Effective disease prediction in modern healthcare demands the twin goals of
high accuracy and transparent, clinically meaningful explanations. Existing
machine learning and large language model (LLM) based approaches often struggle
to balance these goals. Many models yield accurate but unclear statistical
outputs, while others generate fluent but statistically unsupported narratives,
often undermining both the validity of the explanation and the predictive
accuracy itself. This shortcoming comes from a shallow interaction with the
data, preventing the development of a deep, detailed understanding similar to a
human expert's. We argue that high accuracy and high-quality explanations are
not separate objectives but are mutually reinforcing outcomes of a model that
develops a deep, direct understanding of the data. To achieve this, we propose
the Reflective Cognitive Architecture (RCA), a novel framework that coordinates
multiple LLMs to learn from direct experience. RCA features an iterative rule
refinement mechanism that improves its logic from prediction errors and a
distribution-aware rules check mechanism that bases its reasoning in the
dataset's global statistics. By using predictive accuracy as a signal to drive
deeper comprehension, RCA builds a strong internal model of the data. We
evaluated RCA on one private and two public datasets against 22 baselines. The
results demonstrate that RCA not only achieves state-of-the-art accuracy and
robustness with a relative improvement of up to 40\% over the baseline but,
more importantly, leverages this deep understanding to excel in generating
explanations that are clear, logical, evidence-based, and balanced,
highlighting its potential for creating genuinely trustworthy clinical decision
support systems. The code is available at \https://github.com/ssssszj/RCA.

</details>


### [35] [VC-Agent: An Interactive Agent for Customized Video Dataset Collection](https://arxiv.org/abs/2509.21291)
*Yidan Zhang,Mutian Xu,Yiming Hao,Kun Zhou,Jiahao Chang,Xiaoqiang Liu,Pengfei Wan,Hongbo Fu,Xiaoguang Han*

Main category: cs.AI

TL;DR: 论文提出VC - Agent加速特定视频数据收集，定义友好交互方式，利用多模态大模型，提出过滤策略，设新基准并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 互联网视频数据重要，但收集特定视频耗时耗力，需加速收集过程。

Method: 提出VC - Agent，定义用户友好交互方式，利用多模态大模型连接需求与视频内容，提出可更新的过滤策略，设置新基准并开展用户研究。

Result: 广泛实验证明VC - Agent在定制视频数据集收集方面有效且高效。

Conclusion: VC - Agent能有效加速特定视频数据收集，适用于多种真实场景。

Abstract: Facing scaling laws, video data from the internet becomes increasingly
important. However, collecting extensive videos that meet specific needs is
extremely labor-intensive and time-consuming. In this work, we study the way to
expedite this collection process and propose VC-Agent, the first interactive
agent that is able to understand users' queries and feedback, and accordingly
retrieve/scale up relevant video clips with minimal user input. Specifically,
considering the user interface, our agent defines various user-friendly ways
for the user to specify requirements based on textual descriptions and
confirmations. As for agent functions, we leverage existing multi-modal large
language models to connect the user's requirements with the video content. More
importantly, we propose two novel filtering policies that can be updated when
user interaction is continually performed. Finally, we provide a new benchmark
for personalized video dataset collection, and carefully conduct the user study
to verify our agent's usage in various real scenarios. Extensive experiments
demonstrate the effectiveness and efficiency of our agent for customized video
dataset collection. Project page: https://allenyidan.github.io/vcagent_page/.

</details>


### [36] [SAGE: A Realistic Benchmark for Semantic Understanding](https://arxiv.org/abs/2509.21310)
*Samarth Goel,Reagan J. Lee,Kannan Ramchandran*

Main category: cs.AI

TL;DR: 引入SAGE基准评估语义理解能力，评估多种模型和指标，揭示性能差距与权衡，指出当前语义理解局限。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在传统基准测试表现好，需更具挑战性评估框架来深入探究语义理解。

Method: 引入SAGE基准，从五个类别评估嵌入模型和相似性指标，通过对抗条件、噪声转换和细致人类判断任务，涵盖30多个数据集。

Result: 评估9种嵌入模型和经典指标，发现显著性能差距，无单一方法在各维度都出色，还揭示了关键权衡。

Conclusion: SAGE暴露了当前语义理解能力的关键局限，为现实部署提供更真实的模型鲁棒性评估。

Abstract: As large language models (LLMs) achieve strong performance on traditional
benchmarks, there is an urgent need for more challenging evaluation frameworks
that probe deeper aspects of semantic understanding. We introduce SAGE
(Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed
to assess both embedding models and similarity metrics across five categories:
Human Preference Alignment, Transformation Robustness, Information Sensitivity,
Clustering Performance, and Retrieval Robustness. Unlike existing benchmarks
that focus on isolated capabilities, SAGE evaluates semantic understanding
through adversarial conditions, noisy transformations, and nuanced human
judgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding
models and classical metrics reveals significant performance gaps, with no
single approach excelling across all dimensions. For instance, while
state-of-the-art embedding models like OpenAI's text-embedding-3-large dominate
in aligning with human preferences (0.682 vs. 0.591 for the best classical
metric), they are significantly outperformed by classical metrics on
information sensitivity tasks, where Jaccard Similarity achieves a score of
0.905 compared to the top embedding score of 0.794. SAGE further uncovers
critical trade-offs: OpenAI's text-embedding-3-small achieves the highest
clustering performance (0.483) but demonstrates extreme brittleness with the
lowest robustness score (0.011). SAGE exposes critical limitations in current
semantic understanding capabilities and provides a more realistic assessment of
model robustness for real-world deployment.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [37] [A Hierarchical Adaptive Diffusion Model for Flexible Protein-Protein Docking](https://arxiv.org/abs/2509.20542)
*Rujie Yin,Yang Shen*

Main category: cs.CE

TL;DR: 提出分层自适应扩散生成框架用于蛋白质 - 蛋白质相互作用结构预测，在刚性和柔性情况下均优于对比模型，消融实验证明部分因素重要性，分析揭示尚存问题。


<details>
  <summary>Details</summary>
Motivation: 蛋白质 - 蛋白质相互作用结构预测在存在显著构象变化时面临挑战，需要提高准确性和效率。

Method: 提出分层自适应扩散生成框架，分离全局和局部运动，设计不同噪声时间表，通过共同网络耦合扩散过程，基于新整理的DIPS - AF数据集预训练。

Result: 在独立对接基准数据集DB5.5上，模型在刚性和柔性情况下均优于GeoDock和DiffDock - PP，在更柔性情况下改进更大。

Conclusion: 自适应时间表、动力学特征和预训练很重要，模型在采样、评分和构象分辨率方面仍有差距。

Abstract: Structural prediction of protein-protein interactions is important to
understand the molecular basis of cellular interactions, but it still faces
major challenges when significant conformational changes are present. We
propose a generative framework of hierarchical adaptive diffusion to improve
accuracy and efficiency in such cases. It is hierarchical in separating global
inter-protein rigid-body motions and local intra-protein flexibility in
diffusion processes, and the distinct local and global noise schedules are
designed to mimic the induced-fit effect. It is adaptive in conditioning the
local flexibility schedule on predicted levels of conformational change,
allowing faster flexing for larger anticipated conformational changes.
Furthermore, it couples the local and global diffusion processes through a
common score and confidence network with sequence, evolution, structure, and
dynamics features as inputs, and maintains rotational or translational
invariance or equivariance in outputs. It builds on our newly curated DIPS-AF
dataset of nearly 39,000 examples for pre-training. Experiments on the
independent docking benchmark dataset DB5.5 show that our model outperforms an
AlphaFold2-like iterative transformer (GeoDock) and a diffusion model
(DiffDock-PP) in both rigid and flexible cases, with larger improvements in
more flexible cases. Ablation studies prove the importance of adaptive
schedules, dynamics features, and pre-training. Additional analyses and case
studies reveal remaining gaps in sampling, scoring, and conformational
resolution.

</details>


### [38] [Difference-Guided Reasoning: A Temporal-Spatial Framework for Large Language Models](https://arxiv.org/abs/2509.20713)
*Hong Su*

Main category: cs.CE

TL;DR: 本文提出差异引导推理框架使大语言模型能识别时空差异并据此推理，验证显示该方法优于直接提示。


<details>
  <summary>Details</summary>
Motivation: 大语言模型被动运行，难以模拟人类思维，缺乏主动发现问题能力，限制推理能力。

Method: 提出差异引导推理框架，通过特征提取形式化差异，对重要和最新变化排序并关联行动，还增加异常行为检测和外部信息整合机制。

Result: 用差异提示大语言模型能更聚焦关键问题，推理结果更符合预期。

Conclusion: 差异引导推理框架可提升大语言模型推理能力，优于直接提示方式。

Abstract: Large Language Models (LLMs) are important tools for reasoning and
problem-solving, while they often operate passively, answering questions
without actively discovering new ones. This limitation reduces their ability to
simulate human-like thinking, where noticing differences is a key trigger for
reasoning. Thus, in this paper we propose a difference-guided reasoning
framework, which enables LLMs to identify and act upon changes across time and
space. The model formalizes differences through feature extraction, prioritizes
the most impactful and latest changes, and links them to appropriate actions.
We further extend the framework with mechanisms for abnormal behavior detection
and the integration of external information from users or sensors, ensuring
more reliable and grounded reasoning. Verification results show that prompting
LLMs with differences improves focus on critical issues, leading to higher
alignment with desired reasoning outcomes compared to direct prompting.

</details>


### [39] [Extrapolating Phase-Field Simulations in Space and Time with Purely Convolutional Architectures](https://arxiv.org/abs/2509.20770)
*Christophe Bonneville,Nathan Bieberdorf,Pieterjan Robbe,Mark Asta,Habib N. Najm,Laurent Capolungo,Cosmin Safta*

Main category: cs.CE

TL;DR: 提出条件参数化全卷积U - Net替代模型，可在时空上超越训练窗口进行推广，加速液态金属脱合金相场模型计算。


<details>
  <summary>Details</summary>
Motivation: 液态金属脱合金相场模型在大尺度或长时间模拟时难以处理，需要更高效方法。

Method: 构建条件参数化的全卷积U - Net替代模型，集成卷积自注意力和物理感知填充，采用参数条件实现可变时间步长跳过和适应不同合金系统。

Result: 替代模型能准确再现关键物理现象，训练范围内相对误差通常低于5%，外推时低于10%，计算加速达16000倍。

Conclusion: 该方法是液态金属脱合金相场模型可扩展、高保真外推的早期一步。

Abstract: Phase-field models of liquid metal dealloying (LMD) can resolve rich
microstructural dynamics but become intractable for large domains or long time
horizons. We present a conditionally parameterized, fully convolutional U-Net
surrogate that generalizes far beyond its training window in both space and
time. The design integrates convolutional self-attention and physics-aware
padding, while parameter conditioning enables variable time-step skipping and
adaptation to diverse alloy systems. Although trained only on short,
small-scale simulations, the surrogate exploits the translational invariance of
convolutions to extend predictions to much longer horizons than traditional
solvers. It accurately reproduces key LMD physics, with relative errors
typically under 5% within the training regime and below 10% when extrapolating
to larger domains and later times. The method accelerates computations by up to
16,000 times, cutting weeks of simulation down to seconds, and marks an early
step toward scalable, high-fidelity extrapolation of LMD phase-field models.

</details>


### [40] [Mesh Interpolation Graph Network for Dynamic and Spatially Irregular Global Weather Forecasting](https://arxiv.org/abs/2509.20911)
*Zinan Zheng,Yang Liu,Jia Li*

Main category: cs.CE

TL;DR: 提出Mesh Interpolation Graph Network (MIGN)用于全球气象预报，实验显示其优于现有数据驱动模型且有空间泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多数气象预报研究聚焦有限局部区域，忽略更广区域影响，缺乏有效泛化能力，本文研究全球气象预报。

Method: 提出MIGN模型，包括用规则网格插值网络学习空间不规则数据、利用参数化球谐位置嵌入增强空间泛化能力。

Result: 在最新观测数据集上实验表明MIGN显著优于现有数据驱动模型，且有空间泛化能力，能泛化到之前未见站点。

Conclusion: MIGN在全球气象预报中表现出色，能解决不规则动态变化数据的预报问题，有良好泛化能力。

Abstract: Graph neural networks have shown promising results in weather forecasting,
which is critical for human activity such as agriculture planning and extreme
weather preparation. However, most studies focus on finite and local areas for
training, overlooking the influence of broader areas and limiting their ability
to generalize effectively. Thus, in this work, we study global weather
forecasting that is irregularly distributed and dynamically varying in
practice, requiring the model to generalize to unobserved locations. To address
such challenges, we propose a general Mesh Interpolation Graph Network (MIGN)
that models the irregular weather station forecasting, consisting of two key
designs: (1) learning spatially irregular data with regular mesh interpolation
network to align the data; (2) leveraging parametric spherical harmonics
location embedding to further enhance spatial generalization ability. Extensive
experiments on an up-to-date observation dataset show that MIGN significantly
outperforms existing data-driven models. Besides, we show that MIGN has spatial
generalization ability, and is capable of generalizing to previous unseen
stations.

</details>


### [41] [Extensions of a Line-Graph-Based Method for Token Routing in Decentralized Exchanges](https://arxiv.org/abs/2509.21152)
*Yu Zhang,Claudio J. Tessone*

Main category: cs.CE

TL;DR: 本文针对去中心化交易所（DEX）交易效率低下问题，基于已有方法提出三项扩展，用实证数据证明其能提升计算效率和盈利能力。


<details>
  <summary>Details</summary>
Motivation: DEX 大量交易非最优，解决交易效率低下问题既具实际紧迫性又有理论吸引力。

Method: 在 Zhang 等人的线性线图路由方法基础上，提出广度优先搜索（BFS）链接迭代规则、路由拆分策略，将方法推广到多 DEX 聚合器场景。

Result: 使用 Uniswap V2 和 Sushiswap V2 的实证数据表明，扩展方法大幅提高了计算效率和盈利能力。

Conclusion: 所提扩展为未来路由改进奠定了基础。

Abstract: Decentralized exchanges (DEXs) form a cornerstone of the decentralized
finance (DeFi) ecosystem, processing token trades worth billions of dollars
daily. Yet, a significant fraction of these trades are suboptimal: alternative
routing paths could yield more target tokens. Addressing this inefficiency is
both practically urgent and theoretically compelling. Building on the linear
line-graph-based routing method of Zhang et al. (2025), we propose three key
extensions that better capture real-world trading complexity. First, we
introduce a breadth-first search (BFS) link iteration rule that reduces
computational cost and average execution time without sacrificing
profitability. Second, we design a route-splitting strategy that divides large
trades into smaller ones, alleviating price slippage and increasing average
trader profits, albeit at the cost of higher computational overhead. Third, we
generalize the method beyond a single DEX to a multi-DEX aggregator setting,
reflecting actual trading environments. Using empirical data from Uniswap V2
and Sushiswap V2, we demonstrate that these extensions substantially improve
both computational efficiency and profitability, establishing a foundation for
future routing enhancements.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [42] [FZModules: A Heterogeneous Computing Framework for Customizable Scientific Data Compression Pipelines](https://arxiv.org/abs/2509.20563)
*Skyler Ruiter,Jiannan Tian,Fengguang Song*

Main category: cs.DC

TL;DR: 提出FZModules框架用于组装自定义压缩管道，评估显示其有速度和率失真优势，可实现快速定制设计。


<details>
  <summary>Details</summary>
Motivation: 现代科学模拟和仪器产生的数据量过大，有损压缩最优管道需专业知识，GPU压缩器有局限性。

Method: 提出FZModules异构框架，通过简洁可扩展接口组装自定义压缩管道，利用异步任务执行库管理。

Result: 在四个代表性科学数据集上评估三个管道，能达到融合内核GPU压缩器的端到端加速，且率失真与高保真CPU或混合压缩器相似。

Conclusion: FZModules框架可实现快速、特定领域的压缩管道设计。

Abstract: Modern scientific simulations and instruments generate data volumes that
overwhelm memory and storage, throttling scalability. Lossy compression
mitigates this by trading controlled error for reduced footprint and throughput
gains, yet optimal pipelines are highly data and objective specific, demanding
compression expertise. GPU compressors supply raw throughput but often
hard-code fused kernels that hinder rapid experimentation, and underperform in
rate-distortion. We present FZModules, a heterogeneous framework for assembling
error-bounded custom compression pipelines from high-performance modules
through a concise extensible interface. We further utilize an asynchronous
task-backed execution library that infers data dependencies, manages memory
movement, and exposes branch and stage level concurrency for powerful
asynchronous compression pipelines. Evaluating three pipelines built with
FZModules on four representative scientific datasets, we show they can compare
end-to-end speedup of fused-kernel GPU compressors while achieving similar
rate-distortion to higher fidelity CPU or hybrid compressors, enabling rapid,
domain-tailored design.

</details>


### [43] [Experience Deploying Containerized GenAI Services at an HPC Center](https://arxiv.org/abs/2509.20603)
*Angel M. Beltre,Jeff Ogden,Kevin Pedretti*

Main category: cs.DC

TL;DR: 本文分享在HPC中心部署GenAI工作负载的经验，介绍融合计算架构，并用Llama LLM案例说明，为HPC容器社区提供参考。


<details>
  <summary>Details</summary>
Motivation: GenAI组件在HPC中心的能力仍在发展，需要探索在HPC中心部署GenAI工作负载及HPC与云计算环境的集成。

Method: 构建融合HPC和Kubernetes平台运行容器化GenAI工作负载的计算架构，通过案例研究展示使用多个容器运行时在Kubernetes和HPC平台部署Llama LLM。

Result: 成功在HPC中心部署GenAI工作负载，实现了HPC与Kubernetes平台的集成。

Conclusion: 为HPC容器社区提供了实践考量和机会，可指导未来研究和工具开发。

Abstract: Generative Artificial Intelligence (GenAI) applications are built from
specialized components -- inference servers, object storage, vector and graph
databases, and user interfaces -- interconnected via web-based APIs. While
these components are often containerized and deployed in cloud environments,
such capabilities are still emerging at High-Performance Computing (HPC)
centers. In this paper, we share our experience deploying GenAI workloads
within an established HPC center, discussing the integration of HPC and cloud
computing environments. We describe our converged computing architecture that
integrates HPC and Kubernetes platforms running containerized GenAI workloads,
helping with reproducibility. A case study illustrates the deployment of the
Llama Large Language Model (LLM) using a containerized inference server (vLLM)
across both Kubernetes and HPC platforms using multiple container runtimes. Our
experience highlights practical considerations and opportunities for the HPC
container community, guiding future research and tool development.

</details>


### [44] [Distributed-memory Algorithms for Sparse Matrix Permutation, Extraction, and Assignment](https://arxiv.org/abs/2509.20776)
*Elaheh Hassani,Md Taufique Hussain,Ariful Azad*

Main category: cs.DC

TL;DR: 提出用于稀疏矩阵置换、提取和赋值的可扩展分布式内存算法，性能优于现有库。


<details>
  <summary>Details</summary>
Motivation: 为稀疏矩阵的置换、提取和赋值操作提供更高效的算法，减少通信开销并提升性能。

Method: 采用Identify - Exchange - Build (IEB)策略，结合无同步多线程算法加速本地计算。

Result: 在多个集群和超级计算机上实验，性能优于CombBLAS和PETSc。

Conclusion: 对稀疏矩阵的相关操作提供了算法、软件实现、实验评估和应用的全面研究。

Abstract: We present scalable distributed-memory algorithms for sparse matrix
permutation, extraction, and assignment. Our methods follow an
Identify-Exchange-Build (IEB) strategy where each process identifies the local
nonzeros to be sent, exchanges the required data, and then builds its local
submatrix from the received elements. This approach reduces communication
compared to SpGEMM-based methods in distributed memory. By employing
synchronization-free multithreaded algorithms, we further accelerate local
computations, achieving substantially better performance than existing
libraries such as CombBLAS and PETSc. We design efficient software for these
operations and evaluate their performance on two university clusters and the
Perlmutter supercomputer. Our experiments span a variety of application
scenarios, including matrix permutation for load balancing, matrix reordering,
subgraph extraction, and streaming graph applications. In all cases, we compare
our algorithms against CombBLAS, the most comprehensive distributed library for
these operations, and, in some scenarios, against PETSc. Overall, this work
provides a comprehensive study of algorithms, software implementations,
experimental evaluations, and applications for sparse matrix permutation,
extraction, and assignment.

</details>


### [45] [Integrating and Characterizing HPC Task Runtime Systems for hybrid AI-HPC workloads](https://arxiv.org/abs/2509.20819)
*Andre Merzky,Mikhail Titov,Matteo Turilli,Shantenu Jha*

Main category: cs.DC

TL;DR: 研究RADICAL - Pilot（RP）与Flux和Dragon集成的性能，表明其在混合AI - HPC工作负载上比srun更具优势。


<details>
  <summary>Details</summary>
Motivation: 现有启动器如Slurm的srun不适合动态和异构工作负载，需要新方法处理混合AI - HPC工作负载。

Method: 对集成Flux和Dragon的RP进行性能研究，使用合成和生产规模工作负载在Frontier上测试不同运行时配置下RP的任务执行特性。

Result: RP+Flux每秒可处理930个任务，RP+Flux+Dragon超1500个任务/秒，利用率超99.6%；srun峰值152个任务/秒且利用率低于50%；在IMPECCABLE.v2药物发现项目中，RP+Flux使工期缩短30 - 60%，吞吐量提高四倍多。

Conclusion: RP中的混合运行时集成是处理混合AI - HPC工作负载的可扩展方法。

Abstract: Scientific workflows increasingly involve both HPC and machine-learning
tasks, combining MPI-based simulations, training, and inference in a single
execution. Launchers such as Slurm's srun constrain concurrency and throughput,
making them unsuitable for dynamic and heterogeneous workloads. We present a
performance study of RADICAL-Pilot (RP) integrated with Flux and Dragon, two
complementary runtime systems that enable hierarchical resource management and
high-throughput function execution. Using synthetic and production-scale
workloads on Frontier, we characterize the task execution properties of RP
across runtime configurations. RP+Flux sustains up to 930 tasks/s, and
RP+Flux+Dragon exceeds 1,500 tasks/s with over 99.6% utilization. In contrast,
srun peaks at 152 tasks/s and degrades with scale, with utilization below 50%.
For IMPECCABLE.v2 drug discovery campaign, RP+Flux reduces makespan by 30-60%
relative to srun/Slurm and increases throughput more than four times on up to
1,024. These results demonstrate hybrid runtime integration in RP as a scalable
approach for hybrid AI-HPC workloads.

</details>


### [46] [RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training](https://arxiv.org/abs/2509.21009)
*Wei Gao,Yuheng Zhao,Dakai An,Tianyuan Wu,Lunxi Cao,Shaopan Xiong,Ju Huang,Weixun Wang,Siran Yang,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng,Wei Wang*

Main category: cs.DC

TL;DR: 提出尾批处理策略和RollPacker系统加速强化学习训练，在Qwen2.5系列大模型上取得显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 同步强化学习后训练存在GPU利用率低问题，现有放松同步方法会影响训练精度。

Method: 引入尾批处理策略，将长响应提示合并到少数长轮次，构建RollPacker系统进行全面优化。

Result: RollPacker在Qwen2.5系列大模型上相比veRL减少2.03 - 2.56倍端到端训练时间，相比RLHFuse最多加速2.24倍。

Conclusion: 尾批处理和RollPacker系统能有效减少GPU空闲时间，在不牺牲精度的前提下加速强化学习训练。

Abstract: Reinforcement Learning (RL) is a pivotal post-training technique for
enhancing the reasoning capabilities of Large Language Models (LLMs). However,
synchronous RL post-training often suffers from significant GPU
underutilization, referred to as bubbles, caused by imbalanced response lengths
within rollout steps. Many RL systems attempt to alleviate this problem by
relaxing synchronization, but this can compromise training accuracy. In this
paper, we introduce tail batching, a novel rollout scheduling strategy for
synchronous RL that systematically consolidates prompts leading to long-tail
responses into a small subset of rollout steps (long rounds), while ensuring
that the majority of steps (short rounds) involve only balanced, short
rollouts. By excluding long responses from short rounds and rescheduling them
into a few designated long rounds, tail batching effectively reduces GPU idle
time during rollouts and significantly accelerates RL training without
sacrificing accuracy. We present RollPacker, a system that fully harnesses the
benefits of tail batching through holistic optimizations across all three RL
stages: elastic parallelism adaptation for rollout, dynamic resource allocation
and scheduling for reward, and stream-based training. Empirical results show
that RollPacker achieves a 2.03x-2.56x end-to-end training time reduction
compared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5
family of LLMs on up to 128 H800 GPUs.

</details>


### [47] [Utilizing Sparsity in the GPU-accelerated Assembly of Schur Complement Matrices in Domain Decomposition Methods](https://arxiv.org/abs/2509.21037)
*Jakub Homola,Ondřej Meca,Lubomír Říha,Tomáš Brzobohatý*

Main category: cs.DC

TL;DR: 本文研究Schur补矩阵在GPU上的组装加速，通过利用输入矩阵稀疏性进一步优化，在FETI方法中实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 在高性能集群性能主要依赖GPU的情况下，加速使用Schur补矩阵的域分解方法，减少显式组装的开销。

Method: 通过明智利用输入矩阵的稀疏性改进GPU上Schur补矩阵的组装。

Result: 在FETI方法中，代码的GPU部分实现了5.1倍的加速，整个组装实现了3.3倍的加速，从10次迭代起加速就有益。

Conclusion: 利用输入矩阵的稀疏性可以进一步改进GPU上Schur补矩阵的组装，实现有效加速。

Abstract: Schur complement matrices emerge in many domain decomposition methods that
can solve complex engineering problems using supercomputers. Today, as most of
the high-performance clusters' performance lies in GPUs, these methods should
also be accelerated.
  Typically, the offloaded components are the explicitly assembled dense Schur
complement matrices used later in the iterative solver for multiplication with
a vector. As the explicit assembly is expensive, it represents a significant
overhead associated with this approach to acceleration. It has already been
shown that the overhead can be minimized by assembling the Schur complements
directly on the GPU.
  This paper shows that the GPU assembly can be further improved by wisely
utilizing the sparsity of the input matrices. In the context of FETI methods,
we achieved a speedup of 5.1 in the GPU section of the code and 3.3 for the
whole assembly, making the acceleration beneficial from as few as 10
iterations.

</details>


### [48] [Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem](https://arxiv.org/abs/2509.21039)
*William F. Godoy,Tatiana Melnichenko,Pedro Valero-Lara,Wael Elwasif,Philip Fackler,Rafael Ferreira Da Silva,Keita Teranishi,Jeffrey S. Vetter*

Main category: cs.DC

TL;DR: 探索Mojo语言在GPU上科学计算工作负载的性能和可移植性，与CUDA和HIP对比，指出其优势与差距。


<details>
  <summary>Details</summary>
Motivation: Mojo作为基于LLVM的MLIR编译器基础架构的语言，旨在缩小性能和生产力差距，探索其在GPU科学计算工作负载的表现。

Method: 针对四个科学工作负载，在NVIDIA H100和AMD MI300A GPUs上与供应商基线对比性能。

Result: Mojo在内存绑定内核上性能与CUDA和HIP有竞争力，在AMD GPUs原子操作及AMD和NVIDIA GPUs快速数学计算绑定内核存在差距。

Conclusion: 尽管学习曲线和编程要求仍偏底层，Mojo能缩小碎片化Python生态系统在科学计算和AI融合中的显著差距。

Abstract: We explore the performance and portability of the novel Mojo language for
scientific computing workloads on GPUs. As the first language based on the
LLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure,
Mojo aims to close performance and productivity gaps by combining Python's
interoperability and CUDA-like syntax for compile-time portable GPU
programming. We target four scientific workloads: a seven-point stencil
(memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and
Hartree-Fock (compute-bound with atomic operations); and compare their
performance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We
show that Mojo's performance is competitive with CUDA and HIP for memory-bound
kernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math
compute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve
and programming requirements are still fairly low-level, Mojo can close
significant gaps in the fragmented Python ecosystem in the convergence of
scientific computing and AI.

</details>


### [49] [From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient Method for Solving Large-Scale Linear Optimization Problem](https://arxiv.org/abs/2509.21137)
*Huynh Q. N. Vo,Md Tawsif Rahman Chowdhury,Paritosh Ramanan,Gozde Tutuncuoglu,Junchi Yang,Feng Qiu,Murat Yildirim*

Main category: cs.DC

TL;DR: 传统架构难以应对计算负载增长，提出针对RRAM设备阵列的分布式内存原始 - 对偶混合梯度（PDHG）方法，在能耗和延迟上表现出色，展示了算法 - 硬件协同设计潜力。


<details>
  <summary>Details</summary>
Motivation: 传统架构受限于基本极限，难以应对计算负载指数级增长，现有算法不适用于内存计算（IMC），尤其是约束优化问题中矩阵重编程成本高。

Method: 提出分布式内存PDHG方法，专为RRAM设备阵列设计，减少写周期，增强对设备非理想性的鲁棒性，利用对称块矩阵公式统一操作，集成MELISO + 仿真框架评估性能。

Result: 与GPU加速求解器相比，基于RRAM的求解器在大规模线性规划中达到相当的精度，能耗和延迟最多降低三个数量级。

Conclusion: 首次在RRAM上实现基于PDHG的线性规划求解器，展示了通过分布式内存计算进行算法 - 硬件协同设计解决大规模优化问题的变革潜力。

Abstract: The exponential growth of computational workloads is surpassing the
capabilities of conventional architectures, which are constrained by
fundamental limits. In-memory computing (IMC) with RRAM provides a promising
alternative by providing analog computations with significant gains in latency
and energy use. However, existing algorithms developed for conventional
architectures do not translate to IMC, particularly for constrained
optimization problems where frequent matrix reprogramming remains
cost-prohibitive for IMC applications. Here we present a distributed in-memory
primal-dual hybrid gradient (PDHG) method, specifically co-designed for arrays
of RRAM devices. Our approach minimizes costly write cycles, incorporates
robustness against device non-idealities, and leverages a symmetric
block-matrix formulation to unify operations across distributed crossbars. We
integrate a physics-based simulation framework called MELISO+ to evaluate
performance under realistic device conditions. Benchmarking against
GPU-accelerated solvers on large-scale linear programs demonstrates that our
RRAM-based solver achieves comparable accuracy with up to three orders of
magnitude reductions in energy consumption and latency. These results
demonstrate the first PDHG-based LP solver implemented on RRAMs, showcasing the
transformative potential of algorithm-hardware co-design for solving
large-scale optimization through distributed in-memory computing.

</details>


### [50] [Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training](https://arxiv.org/abs/2509.21275)
*Shiju Wang,Yujie Wang,Ao Sun,Fangcheng Fu,Zijian Zhu,Bin Cui,Xu Han,Kaisheng Ma*

Main category: cs.DC

TL;DR: 本文提出弹性管道并行（EPP）方法和分布式训练系统InfiniPipe，在长上下文训练中实现1.69倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文训练方案存在通信开销大、内存消耗高、硬件利用率低、无法应对序列长度分布不均等问题，需要更好的并行方法和调度策略。

Method: 提出EPP协调令牌级和批次级管道并行；构建InfiniPipe，包含资源感知和负载均衡的序列处理器以及联合优化管道调度和梯度检查点的协同优化方法。

Result: 综合实验表明InfiniPipe比现有系统实现1.69倍加速。

Conclusion: EPP和InfiniPipe能有效适应资源和工作负载的异质性，提升长上下文训练效率。

Abstract: Long context training is crucial for LLM's context extension. Existing
schemes, such as sequence parallelism, incur substantial communication
overhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness
hinges on partitioning granularity. Batch-level PP dividing input samples
exhibits high memory consumption in long-context scenario, whereas token-level
PP splitting sequences into slices alleviates memory overhead but may incur
hardware under-utilization. This trade-off motivates adaptively selecting PP
granularity to match resource and workload characteristics. Moreover, sequence
length distribution of the real-world dataset exhibits skewness, posing a
challenge on PP's workload balance and efficient scheduling. Current static PP
scheduling methods overlook the variance of sequence length, leading to
suboptimal performance. In this paper, we propose Elastic Pipeline Parallelism
(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource
and workload heterogeneity. We build InfiniPipe, a distributed training system
that unleashes the potential of EPP via (1) a resource-aware and
workload-balanced sequence processor that splits long sequences and packs short
ones; and (2) a co-optimization methodology that jointly optimizes pipeline
schedule and gradient checkpointing via a mechanism named stage-aware
chunk-level adaptive checkpointing. Comprehensive experiments demonstrate that
InfiniPipe achieves a 1.69x speedup over state-of-the-art systems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [51] [Actively Learning Halfspaces without Synthetic Data](https://arxiv.org/abs/2509.20848)
*Hadley Black,Kasper Green Larsen,Arya Mazumdar,Barna Saha,Geelon So*

Main category: cs.DS

TL;DR: 本文研究无点合成情况下学习半空间的高效算法，对法向量来自大小为D的集合的半空间给出了Θ(D + log n)的紧界，还得到了轴对齐半空间的最优查询确定性学习器，并用精确学习算法得到了近最优的PAC学习算法。


<details>
  <summary>Details</summary>
Motivation: 已有经典点定位问题算法需点合成能力，无此能力有Ω(n)查询下界，因此要设计无点合成的高效学习半空间算法。

Method: 利用给定排序中的结构并行进行二分查找，而非依次考虑每个排序。

Result: 对法向量来自大小为D的集合的半空间给出了Θ(D + log n)的紧界；得到最优O(d + log n)查询的轴对齐半空间确定性学习器；O(min(D + log(1/ε), 1/ε) · log D)查询足以在误差ε内学习f。

Conclusion: 提出的算法解决了无点合成下学习半空间的问题，在精确学习和PAC学习中都取得了较好结果，方法有更广泛应用价值。

Abstract: In the classic point location problem, one is given an arbitrary dataset $X
\subset \mathbb{R}^d$ of $n$ points with query access to an unknown halfspace
$f : \mathbb{R}^d \to \{0,1\}$, and the goal is to learn the label of every
point in $X$. This problem is extremely well-studied and a nearly-optimal
$\widetilde{O}(d \log n)$ query algorithm is known due to
Hopkins-Kane-Lovett-Mahajan (FOCS 2020). However, their algorithm is granted
the power to query arbitrary points outside of $X$ (point synthesis), and in
fact without this power there is an $\Omega(n)$ query lower bound due to
Dasgupta (NeurIPS 2004).
  In this work our goal is to design efficient algorithms for learning
halfspaces without point synthesis. To circumvent the $\Omega(n)$ lower bound,
we consider learning halfspaces whose normal vectors come from a set of size
$D$, and show tight bounds of $\Theta(D + \log n)$. As a corollary, we obtain
an optimal $O(d + \log n)$ query deterministic learner for axis-aligned
halfspaces, closing a previous gap of $O(d \log n)$ vs. $\Omega(d + \log n)$.
In fact, our algorithm solves the more general problem of learning a Boolean
function $f$ over $n$ elements which is monotone under at least one of $D$
provided orderings. Our technical insight is to exploit the structure in these
orderings to perform a binary search in parallel rather than considering each
ordering sequentially, and we believe our approach may be of broader interest.
  Furthermore, we use our exact learning algorithm to obtain nearly optimal
algorithms for PAC-learning. We show that $O(\min(D + \log(1/\varepsilon),
1/\varepsilon) \cdot \log D)$ queries suffice to learn $f$ within error
$\varepsilon$, even in a setting when $f$ can be adversarially corrupted on a
$c\varepsilon$-fraction of points, for a sufficiently small constant $c$. This
bound is optimal up to a $\log D$ factor, including in the realizable setting.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [52] [Efficient Kernelized Learning in Polyhedral Games Beyond Full-Information: From Colonel Blotto to Congestion Games](https://arxiv.org/abs/2509.20919)
*Andreas Kontogiannis,Vasilis Pollatos,Gabriele Farina,Panayotis Mertikopoulos,Ioannis Panageas*

Main category: cs.GT

TL;DR: 研究多面体博弈中高效学习粗相关均衡（CCE）问题，提出框架改进算法效率。


<details>
  <summary>Details</summary>
Motivation: 现有部分信息设置下学习CCE的方法运行时间复杂度不理想，需要更高效算法。

Method: 构建基于核化范式的框架，并应用于典型多面体博弈。

Result: 提供了基于收益的高效学习算法，在学习CCE的运行时间上显著优于先前工作。

Conclusion: 所构建框架能有效解决部分信息设置下学习CCE运行时间复杂度高的问题，提高学习效率。

Abstract: We examine the problem of efficiently learning coarse correlated equilibria
(CCE) in polyhedral games, that is, normal-form games with an exponentially
large number of actions per player and an underlying combinatorial structure.
Prominent examples of such games are the classical Colonel Blotto and
congestion games. To achieve computational efficiency, the learning algorithms
must exhibit regret and per-iteration complexity that scale polylogarithmically
in the size of the players' action sets. This challenge has recently been
addressed in the full-information setting, primarily through the use of
kernelization. However, in the case of the realistic, but mathematically
challenging, partial-information setting, existing approaches result in
suboptimal and impractical runtime complexity to learn CCE. We tackle this
limitation by building a framework based on the kernelization paradigm. We
apply this framework to prominent examples of polyhedral games -- namely the
Colonel Blotto, graphic matroid and network congestion games -- and provide
computationally efficient payoff-based learning algorithms, which significantly
improve upon prior works in terms of the runtime for learning CCE in these
settings.

</details>


### [53] [A Category Theoretic Approach to Approximate Game Theory](https://arxiv.org/abs/2509.20932)
*Neil Ghani*

Main category: cs.GT

TL;DR: 本文用范畴论为近似博弈论开发新方法，先对选择函数构建近似均衡模型，后对开放博弈重复此过程。


<details>
  <summary>Details</summary>
Motivation: 精确答案在实际中可能难以或无法计算，需研究近似博弈论以得到近似最优决策。

Method: 运用范畴论，先考虑选择函数构建近似均衡模型并研究其代数性质和与组合结构的关系，再对开放博弈重复此过程。

Result: 成功为选择函数和开放博弈构建了近似均衡模型。

Conclusion: 通过范畴论为近似博弈论提供了新的研究途径。

Abstract: This paper uses category theory to develop an entirely new approach to
approximate game theory. Game theory is the study of how different agents
within a multi-agent system take decisions. At its core, game theory asks what
an optimal decision is in a given scenario. Thus approximate game theory asks
what is an approximately optimal decision in a given scenario. This is
important in practice as -- just like in much of computing -- exact answers
maybe too difficult to compute or even impossible to compute given inherent
uncertainty in input.
  We consider first "Selection Functions" which are functions and develop a
simple yet robust model of approximate equilibria. We develop the algebraic
properties of approximation wrt selection functions and also relate
approximation to the compositional structure of selection functions. We then
repeat this process successfully for Open Games -- a more advanced model of
game theory.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [54] [DELM: a Python toolkit for Data Extraction with Language Models](https://arxiv.org/abs/2509.20617)
*Eric Fithian,Kirill Skobelev*

Main category: cs.IR

TL;DR: 介绍开源Python工具包DELM用于基于大语言模型的数据提取，展示其能力并给出代码链接。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型标注非结构化数据的工作流依赖临时脚本，难以实现可重复性、鲁棒性和系统评估。

Method: 开发DELM，它具有模块化框架、结构化输出等特点，还支持大语言模型API。

Result: 通过两个案例展示了DELM的能力。

Conclusion: DELM可用于基于大语言模型的数据提取管道的快速实验迭代和权衡量化。

Abstract: Large Language Models (LLMs) have become powerful tools for annotating
unstructured data. However, most existing workflows rely on ad hoc scripts,
making reproducibility, robustness, and systematic evaluation difficult. To
address these challenges, we introduce DELM (Data Extraction with Language
Models), an open-source Python toolkit designed for rapid experimental
iteration of LLM-based data extraction pipelines and for quantifying the
trade-offs between them. DELM minimizes boilerplate code and offers a modular
framework with structured outputs, built-in validation, flexible data-loading
and scoring strategies, and efficient batch processing. It also includes robust
support for working with LLM APIs, featuring retry logic, result caching,
detailed cost tracking, and comprehensive configuration management. We showcase
DELM's capabilities through two case studies: one featuring a novel prompt
optimization algorithm, and another illustrating how DELM quantifies trade-offs
between cost and coverage when selecting keywords to decide which paragraphs to
pass to an LLM. DELM is available at
\href{https://github.com/Center-for-Applied-AI/delm}{\texttt{github.com/Center-for-Applied-AI/delm}}.

</details>


### [55] [Provenance Analysis of Archaeological Artifacts via Multimodal RAG Systems](https://arxiv.org/abs/2509.20769)
*Tuo Zhang,Yuechun Sun,Ruiliang Liu*

Main category: cs.IR

TL;DR: 提出基于RAG的考古文物溯源分析系统，在大英博物馆文物上评估，输出有意义且减轻学者负担。


<details>
  <summary>Details</summary>
Motivation: 设计支持专家推理的考古文物溯源分析系统。

Method: 构建双模态知识库，进行多种检索，用VLM合成检索候选生成结构化推理。

Result: 在大英博物馆一组欧亚大陆东部青铜时代文物上评估，系统输出有意义且可解释。

Conclusion: 系统为学者提供分析起点，显著减轻检索大量对比语料的认知负担。

Abstract: In this work, we present a retrieval-augmented generation (RAG)-based system
for provenance analysis of archaeological artifacts, designed to support expert
reasoning by integrating multimodal retrieval and large vision-language models
(VLMs). The system constructs a dual-modal knowledge base from reference texts
and images, enabling raw visual, edge-enhanced, and semantic retrieval to
identify stylistically similar objects. Retrieved candidates are synthesized by
the VLM to generate structured inferences, including chronological,
geographical, and cultural attributions, alongside interpretive justifications.
We evaluate the system on a set of Eastern Eurasian Bronze Age artifacts from
the British Museum. Expert evaluation demonstrates that the system produces
meaningful and interpretable outputs, offering scholars concrete starting
points for analysis and significantly alleviating the cognitive burden of
navigating vast comparative corpora.

</details>


### [56] [Performance Consistency of Learning Methods for Information Retrieval Tasks](https://arxiv.org/abs/2509.20804)
*Meng Yuan,Justin Zobel*

Main category: cs.IR

TL;DR: 本文用自举测试集和随机种子方法评估IR方法性能，发现统计模型稳定，Transformer模型随种子改变变化大，凸显其训练不稳定性。


<details>
  <summary>Details</summary>
Motivation: 评估IR方法测量性能的准确性或鲁棒性。

Method: 使用自举测试集和随机种子方法，对三种IR任务下的传统统计学习模型和Transformer学习模型进行评估。

Result: 统计模型稳定，Transformer模型变化大，9/11的F1分数标准差超0.075，7/11的精度值标准差超0.125。

Conclusion: Transformer模型存在训练不稳定性，以往结果可靠性存疑，需严格评估实践。

Abstract: A range of approaches have been proposed for estimating the accuracy or
robustness of the measured performance of IR methods. One is to use
bootstrapping of test sets, which, as we confirm, provides an estimate of
variation in performance. For IR methods that rely on a seed, such as those
that involve machine learning, another approach is to use a random set of seeds
to examine performance variation. Using three different IR tasks we have used
such randomness to examine a range of traditional statistical learning models
and transformer-based learning models. While the statistical models are stable,
the transformer models show huge variation as seeds are changed. In 9 of 11
cases the F1-scores (in the range 0.0--1.0) had a standard deviation of over
0.075; while 7 of 11 precision values (also in the range 0.0--1.0) had a
standard deviation of over 0.125. This is in a context where differences of
less than 0.02 have been used as evidence of method improvement. Our findings
highlight the vulnerability of transformer models to training instabilities and
moreover raise questions about the reliability of previous results, thus
underscoring the need for rigorous evaluation practices.

</details>


### [57] [RecIS: Sparse to Dense, A Unified Training Framework for Recommendation Models](https://arxiv.org/abs/2509.20883)
*Hua Zong,Qingtao Zeng,Zhengxiong Zhou,Zhihua Han,Zhensong Yan,Mingjie Liu,Hechen Sun,Jiawei Liu,Yiwen Hu,Qi Wang,YiHan Xian,Wenjie Guo,Houyuan Xiang,Zhiyuan Zeng,Xiangrong Sheng,Bencheng Yan,Nan Hu,Yuheng Huang,Jinqing Lian,Ziru Xu,Yan Zhang,Ju Huang,Siran Yang,Huimin Yi,Jiamang Wang,Pengjie Wang,Han Zhu,Jian Wu,Dan Ou,Jian Xu,Haihong Tang,Yuning Jiang,Bo Zheng,Lin Qu*

Main category: cs.IR

TL;DR: 提出统一稀疏 - 密集训练框架RecIS，用于工业级推荐模型训练，已在阿里应用。


<details>
  <summary>Details</summary>
Motivation: 创建基于PyTorch生态的统一稀疏 - 密集训练框架，满足集成大模型的工业级推荐模型训练需求，并优化稀疏组件提升效率。

Method: 构建基于PyTorch生态的统一框架，优化稀疏组件，密集组件利用PyTorch现有优化技术。

Result: RecIS已在阿里巴巴用于众多大模型增强推荐训练任务，部分传统稀疏模型也开始在其中训练。

Conclusion: RecIS框架能满足工业级推荐模型训练需求且有较好表现。

Abstract: In this paper, we propose RecIS, a unified Sparse-Dense training framework
designed to achieve two primary goals: 1. Unified Framework To create a Unified
sparse-dense training framework based on the PyTorch ecosystem that meets the
training needs of industrial-grade recommendation models that integrated with
large models. 2.System Optimization To optimize the sparse component, offering
superior efficiency over the TensorFlow-based recommendation models. The dense
component, meanwhile, leverages existing optimization technologies within the
PyTorch ecosystem. Currently, RecIS is being used in Alibaba for numerous
large-model enhanced recommendation training tasks, and some traditional sparse
models have also begun training in it.

</details>


### [58] [FORGE: Forming Semantic Identifiers for Generative Retrieval in Industrial Datasets](https://arxiv.org/abs/2509.20904)
*Kairui Fu,Tao Zhang,Shuwen Xiao,Ziyang Wang,Xinming Zhang,Chenchi Zhang,Yuliang Yan,Junjun Zheng,Yu Li,Zhihong Chen,Jian Wu,Xiangheng Kong,Shengyu Zhang,Kun Kuang,Yuning Jiang,Bo Zheng*

Main category: cs.IR

TL;DR: 本文提出FORGE基准以解决语义标识符（SIDs）在生成式检索（GR）研究中的挑战，利用淘宝数据，优化SID构建，提出新评估指标和预训练模式，提升交易数并减少在线收敛时间。


<details>
  <summary>Details</summary>
Motivation: 当前SIDs研究面临缺乏大规模多模态公开数据集、优化策略研究有限且评估成本高、工业部署在线收敛慢等问题。

Method: 提出FORGE基准，利用淘宝140亿用户交互和2.5亿商品多模态特征数据集，探索优化SID构建，提出新的SID评估指标，引入离线预训练模式。

Result: 离线实验验证优化策略有效性，在线分析使交易数增加0.35%，离线预训练模式使在线收敛时间减半。

Conclusion: FORGE基准有效解决了SIDs研究中的挑战，具有实际应用价值，代码和数据已开源。

Abstract: Semantic identifiers (SIDs) have gained increasing attention in generative
retrieval (GR) due to their meaningful semantic discriminability. However,
current research on SIDs faces three main challenges: (1) the absence of
large-scale public datasets with multimodal features, (2) limited investigation
into optimization strategies for SID generation, which typically rely on costly
GR training for evaluation, and (3) slow online convergence in industrial
deployment. To address these challenges, we propose FORGE, a comprehensive
benchmark for FOrming semantic identifieR in Generative rEtrieval with
industrial datasets. Specifically, FORGE is equipped with a dataset comprising
14 billion user interactions and multimodal features of 250 million items
sampled from Taobao, one of the biggest e-commerce platforms in China.
Leveraging this dataset, FORGE explores several optimizations to enhance the
SID construction and validates their effectiveness via offline experiments
across different settings and tasks. Further online analysis conducted on our
platform, which serves over 300 million users daily, reveals a 0.35% increase
in transaction count, highlighting the practical impact of our method.
Regarding the expensive SID validation accompanied by the full training of GRs,
we propose two novel metrics of SID that correlate positively with
recommendation performance, enabling convenient evaluations without any GR
training. For real-world applications, FORGE introduces an offline pretraining
schema that reduces online convergence by half. The code and data are available
at https://github.com/selous123/al_sid.

</details>


### [59] [Markup Language Modeling for Web Document Understanding](https://arxiv.org/abs/2509.20940)
*Su Liu,Bin Bi,Jan Bakus,Paritosh Kumar Velalam,Vijay Yella,Vinod Hegde*

Main category: cs.IR

TL;DR: 本文聚焦从购物评论网站提取信息构建产品数据库，微调MarkupLM得到MarkupLM++，实验表明大而多样训练集提升准确率，纳入内部节点对部分属性有帮助但整体性能略降，最终模型有较好指标。


<details>
  <summary>Details</summary>
Motivation: 解决从购物评论网站提取详细信息构建最新产品数据库的问题。

Method: 在不同规模评论网站收集的产品数据上微调MarkupLM，开发MarkupLM++并将预测扩展到DOM树内部节点。

Result: 使用更大更多样训练集提升整体提取准确率，纳入内部节点对部分产品属性有帮助但整体性能略降，最终模型精度0.906、召回率0.724、F1分数0.805。

Conclusion: 通过特定方法能实现从购物评论网站提取信息构建产品数据库，虽纳入内部节点有一定影响，但最终模型有较好表现。

Abstract: Web information extraction (WIE) is an important part of many e-commerce
systems, supporting tasks like customer analysis and product recommendation. In
this work, we look at the problem of building up-to-date product databases by
extracting detailed information from shopping review websites. We fine-tuned
MarkupLM on product data gathered from review sites of different sizes and then
developed a variant we call MarkupLM++, which extends predictions to internal
nodes of the DOM tree. Our experiments show that using larger and more diverse
training sets improves extraction accuracy overall. We also find that including
internal nodes helps with some product attributes, although it leads to a
slight drop in overall performance. The final model reached a precision of
0.906, recall of 0.724, and an F1 score of 0.805.

</details>


### [60] [Rejuvenating Cross-Entropy Loss in Knowledge Distillation for Recommender Systems](https://arxiv.org/abs/2509.20989)
*Zhangchi Zhu,Wei Zhang*

Main category: cs.IR

TL;DR: 分析推荐系统知识蒸馏中交叉熵损失，揭示其与NDCG联系，提出RCE - KD方法并验证有效性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统知识蒸馏针对排名蒸馏且在小部分物品子集计算，现有交叉熵损失理论与蒸馏目标有矛盾，需解决差距问题。

Method: 提出RCE - KD，将教师给出的顶级物品分成两个子集，对不符合条件子集设计采样策略，自适应组合两个子集的损失。

Result: 广泛实验证明了RCE - KD方法的有效性。

Conclusion: RCE - KD方法能有效解决推荐系统知识蒸馏中交叉熵损失理论与目标的差距问题。

Abstract: This paper analyzes Cross-Entropy (CE) loss in knowledge distillation (KD)
for recommender systems. KD for recommender systems targets at distilling
rankings, especially among items most likely to be preferred, and can only be
computed on a small subset of items. Considering these features, we reveal the
connection between CE loss and NDCG in the field of KD. We prove that when
performing KD on an item subset, minimizing CE loss maximizes the lower bound
of NDCG, only if an assumption of closure is satisfied. It requires that the
item subset consists of the student's top items. However, this contradicts our
goal of distilling rankings of the teacher's top items. We empirically
demonstrate the vast gap between these two kinds of top items. To bridge the
gap between our goal and theoretical support, we propose Rejuvenated
Cross-Entropy for Knowledge Distillation (RCE-KD). It splits the top items
given by the teacher into two subsets based on whether they are highly ranked
by the student. For the subset that defies the condition, a sampling strategy
is devised to use teacher-student collaboration to approximate our assumption
of closure. We also combine the losses on the two subsets adaptively. Extensive
experiments demonstrate the effectiveness of our method. Our code is available
at https://anonymous.4open.science/r/RCE-KD.

</details>


### [61] [IntSR: An Integrated Generative Framework for Search and Recommendation](https://arxiv.org/abs/2509.21179)
*Huimin Yan,Longfei Xu,Junjie Sun,Ni Ou,Wei Luo,Xing Tan,Ran Cheng,Kaikui Liu,Xiangxiang Chu*

Main category: cs.IR

TL;DR: 提出集成生成框架IntSR用于搜索与推荐任务，在多个场景部署取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有生成推荐系统多关注统一检索和排序，忽略搜索与推荐任务集成，而二者查询形成方式不同。

Method: 提出IntSR框架，用不同查询模式集成不同任务，处理集成行为带来的计算复杂度和动态语料导致的错误模式学习问题。

Result: IntSR在Amap多个场景成功部署，使数字资产GMV提升3.02%、POI推荐CTR提升2.76%、出行模式建议ACC提升5.13%。

Conclusion: IntSR框架有效解决了搜索与推荐任务集成问题，取得了良好的应用效果。

Abstract: Generative recommendation has emerged as a promising paradigm, demonstrating
remarkable results in both academic benchmarks and industrial applications.
However, existing systems predominantly focus on unifying retrieval and ranking
while neglecting the integration of search and recommendation (S&R) tasks. What
makes search and recommendation different is how queries are formed: search
uses explicit user requests, while recommendation relies on implicit user
interests. As for retrieval versus ranking, the distinction comes down to
whether the queries are the target items themselves. Recognizing the query as
central element, we propose IntSR, an integrated generative framework for S&R.
IntSR integrates these disparate tasks using distinct query modalities. It also
addresses the increased computational complexity associated with integrated S&R
behaviors and the erroneous pattern learning introduced by a dynamically
changing corpus. IntSR has been successfully deployed across various scenarios
in Amap, leading to substantial improvements in digital asset's GMV(+3.02%),
POI recommendation's CTR(+2.76%), and travel mode suggestion's ACC(+5.13%).

</details>


### [62] [Interactive Recommendation Agent with Active User Commands](https://arxiv.org/abs/2509.21317)
*Jiakai Tang,Yujie Luo,Xunke Xi,Fei Sun,Xueyang Feng,Sunhao Dai,Chao Yi,Dian Chen,Zhujin Gao,Yang Li,Xu Chen,Wen Chen,Jian Wu,Yuning Jiang,Bo Zheng*

Main category: cs.IR

TL;DR: 传统推荐系统有局限性，本文提出交互式推荐流（IRF）范式及RecBot架构，经实验在用户满意度和业务成果上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统依赖被动反馈机制，无法捕捉用户细微行为动机和意图，导致偏好建模不准确，影响用户满意度和系统有效性。

Method: 引入IRF范式，开发RecBot的双智能体架构，用解析智能体转换偏好、规划智能体调整策略，采用模拟增强知识蒸馏实现高效性能。

Result: 通过大量离线和长期在线实验，RecBot在用户满意度和业务成果上有显著改善。

Conclusion: 提出的IRF范式和RecBot架构能有效解决传统推荐系统的局限性问题。

Abstract: Traditional recommender systems rely on passive feedback mechanisms that
limit users to simple choices such as like and dislike. However, these
coarse-grained signals fail to capture users' nuanced behavior motivations and
intentions. In turn, current systems cannot also distinguish which specific
item attributes drive user satisfaction or dissatisfaction, resulting in
inaccurate preference modeling. These fundamental limitations create a
persistent gap between user intentions and system interpretations, ultimately
undermining user satisfaction and harming system effectiveness.
  To address these limitations, we introduce the Interactive Recommendation
Feed (IRF), a pioneering paradigm that enables natural language commands within
mainstream recommendation feeds. Unlike traditional systems that confine users
to passive implicit behavioral influence, IRF empowers active explicit control
over recommendation policies through real-time linguistic commands. To support
this paradigm, we develop RecBot, a dual-agent architecture where a Parser
Agent transforms linguistic expressions into structured preferences and a
Planner Agent dynamically orchestrates adaptive tool chains for on-the-fly
policy adjustment. To enable practical deployment, we employ
simulation-augmented knowledge distillation to achieve efficient performance
while maintaining strong reasoning capabilities. Through extensive offline and
long-term online experiments, RecBot shows significant improvements in both
user satisfaction and business outcomes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [63] [Sig2Model: A Boosting-Driven Model for Updatable Learned Indexes](https://arxiv.org/abs/2509.20781)
*Alireza Heidari,Amirhossein Ahmad,Wei Zhang,Ying Xiong*

Main category: cs.LG

TL;DR: 本文提出高效自适应学习索引Sig2Model，通过三种技术减少再训练成本，在真实和合成工作负载上评估，表现优于现有可更新学习索引。


<details>
  <summary>Details</summary>
Motivation: 现有学习索引在动态更新场景下性能下降，再训练成本高，不适合频繁更新的实际工作负载。

Method: 采用三种关键技术：用Sigmoid提升近似技术动态调整索引模型；通过高斯混合模型进行主动更新训练；使用神经联合优化框架持续优化参数。

Result: Sig2Model将再训练成本降低达20倍，QPS提高达3倍，内存使用减少达1000倍。

Conclusion: Sig2Model能有效解决现有学习索引在动态更新场景下的问题，显著提升性能。

Abstract: Learned Indexes (LIs) represent a paradigm shift from traditional index
structures by employing machine learning models to approximate the cumulative
distribution function (CDF) of sorted data. While LIs achieve remarkable
efficiency for static datasets, their performance degrades under dynamic
updates: maintaining the CDF invariant (sum of F(k) equals 1) requires global
model retraining, which blocks queries and limits the queries-per-second (QPS)
metric. Current approaches fail to address these retraining costs effectively,
rendering them unsuitable for real-world workloads with frequent updates. In
this paper, we present Sig2Model, an efficient and adaptive learned index that
minimizes retraining cost through three key techniques: (1) a sigmoid boosting
approximation technique that dynamically adjusts the index model by
approximating update-induced shifts in data distribution with localized sigmoid
functions while preserving bounded error guarantees and deferring full
retraining; (2) proactive update training via Gaussian mixture models (GMMs)
that identifies high-update-probability regions for strategic placeholder
allocation to speed up updates; and (3) a neural joint optimization framework
that continuously refines both the sigmoid ensemble and GMM parameters via
gradient-based learning. We evaluate Sig2Model against state-of-the-art
updatable learned indexes on real-world and synthetic workloads, and show that
Sig2Model reduces retraining cost by up to 20x, achieves up to 3x higher QPS,
and uses up to 1000x less memory.

</details>


### [64] [Physics of Learning: A Lagrangian perspective to different learning paradigms](https://arxiv.org/abs/2509.21049)
*Siyuan Guo,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 研究构建高效学习系统问题，基于物理最小作用原理推导经典学习算法等


<details>
  <summary>Details</summary>
Motivation: 构建能以最少观测次数达到期望误差阈值的高效学习系统

Method: 基于物理的最小作用原理，从学习拉格朗日量出发推导算法

Result: 从第一性原理推导出经典学习算法、强化学习中的贝尔曼最优方程和生成模型中的Adam优化器

Conclusion: 学习是在拉格朗日量中寻找平稳路径，学习算法可通过寻找平稳轨迹推导得出

Abstract: We study the problem of building an efficient learning system. Efficient
learning processes information in the least time, i.e., building a system that
reaches a desired error threshold with the least number of observations.
Building upon least action principles from physics, we derive classic learning
algorithms, Bellman's optimality equation in reinforcement learning, and the
Adam optimizer in generative models from first principles, i.e., the Learning
$\textit{Lagrangian}$. We postulate that learning searches for stationary paths
in the Lagrangian, and learning algorithms are derivable by seeking the
stationary trajectories.

</details>


### [65] [A Theory of Multi-Agent Generative Flow Networks](https://arxiv.org/abs/2509.20408)
*Leo Maxime Brunswic,Haozhi Wang,Shuang Luo,Jianye Hao,Amir Rasouli,Yinchuan Li*

Main category: cs.LG

TL;DR: 本文提出多智能体生成流网络（MA - GFlowNets）理论框架及四种算法，实验显示其优于强化学习和基于MCMC的方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏MA - GFlowNets的理论框架，本文旨在填补这一空白。

Method: 提出MA - GFlowNets理论框架，设计四种算法，包括集中式流网络、独立流网络、联合流网络及其条件版本，联合流训练基于局部 - 全局原则。

Result: 实验证明所提框架优于强化学习和基于MCMC的方法。

Conclusion: 所提出的MA - GFlowNets理论框架和算法有效可行，具有良好性能。

Abstract: Generative flow networks utilize a flow-matching loss to learn a stochastic
policy for generating objects from a sequence of actions, such that the
probability of generating a pattern can be proportional to the corresponding
given reward. However, a theoretical framework for multi-agent generative flow
networks (MA-GFlowNets) has not yet been proposed. In this paper, we propose
the theory framework of MA-GFlowNets, which can be applied to multiple agents
to generate objects collaboratively through a series of joint actions. We
further propose four algorithms: a centralized flow network for centralized
training of MA-GFlowNets, an independent flow network for decentralized
execution, a joint flow network for achieving centralized training with
decentralized execution, and its updated conditional version. Joint Flow
training is based on a local-global principle allowing to train a collection of
(local) GFN as a unique (global) GFN. This principle provides a loss of
reasonable complexity and allows to leverage usual results on GFN to provide
theoretical guarantees that the independent policies generate samples with
probability proportional to the reward function. Experimental results
demonstrate the superiority of the proposed framework compared to reinforcement
learning and MCMC-based methods.

</details>


### [66] [FastEagle: Cascaded Drafting for Accelerating Speculative Decoding](https://arxiv.org/abs/2509.20416)
*Haiduo Huang,Jiangcheng Song,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: 提出非自回归级联起草器FastEagle，单步生成草稿，在多模型和任务上比EAGLE-3速度更快，证明去除起草中的顺序依赖可无损加速大语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 现有投机解码起草器需N个顺序步骤生成N个令牌，效率有待提高。

Method: 用轻量级层级联替代时间步骤，采用逐层监督训练，结合约束草稿树。

Result: 在多个大语言模型和任务上，FastEagle在贪心和随机解码下速度均超EAGLE-3，平均接受长度相当。

Conclusion: 去除起草中的顺序依赖是无损加速大语言模型推理的可行途径。

Abstract: Speculative decoding accelerates generation by drafting candidates and
verifying them in parallel, yet state-of-the-art drafters (e.g., EAGLE) still
require N sequential passes to propose N tokens. We present FastEagle, a
non-autoregressive cascaded drafter that emits an entire draft in a single
forward pass. FastEagle replaces temporal steps with a lightweight layer
cascade and trains with layer-wise supervision to mitigate error accumulation.
Coupled with a constrained draft tree that preserves lossless verification
cost, FastEagle delivers substantial wall-clock speedups over strong
autoregressive drafters while maintaining competitive acceptance behavior.
Across multiple LLMs (Vicuna-13B, LLaMA-Instruct 3.x, and
DeepSeek-R1-Distill-LLaMA) and tasks (MT-Bench, HumanEval, GSM8K, CNN/DM,
Alpaca), FastEagle consistently outperforms EAGLE-3 in speedup under both
greedy and stochastic decoding, with comparable average acceptance lengths.
These results indicate that removing sequential dependencies in drafting is a
practical path toward lossless LLM inference acceleration.

</details>


### [67] [Learning Ising Models under Hard Constraints using One Sample](https://arxiv.org/abs/2509.20993)
*Rohan Chauhan,Ioannis Panageas*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of estimating inverse temperature parameter $\beta$
of an $n$-dimensional truncated Ising model using a single sample. Given a
graph $G = (V,E)$ with $n$ vertices, a truncated Ising model is a probability
distribution over the $n$-dimensional hypercube $\{-1,1\}^n$ where each
configuration $\mathbf{\sigma}$ is constrained to lie in a truncation set $S
\subseteq \{-1,1\}^n$ and has probability $\Pr(\mathbf{\sigma}) \propto
\exp(\beta\mathbf{\sigma}^\top A\mathbf{\sigma})$ with $A$ being the adjacency
matrix of $G$. We adopt the recent setting of [Galanis et al. SODA'24], where
the truncation set $S$ can be expressed as the set of satisfying assignments of
a $k$-SAT formula. Given a single sample $\mathbf{\sigma}$ from a truncated
Ising model, with inverse parameter $\beta^*$, underlying graph $G$ of bounded
degree $\Delta$ and $S$ being expressed as the set of satisfying assignments of
a $k$-SAT formula, we design in nearly $O(n)$ time an estimator $\hat{\beta}$
that is $O(\Delta^3/\sqrt{n})$-consistent with the true parameter $\beta^*$ for
$k \gtrsim \log(d^2k)\Delta^3.$
  Our estimator is based on the maximization of the pseudolikelihood, a notion
that has received extensive analysis for various probabilistic models without
[Chatterjee, Annals of Statistics '07] or with truncation [Galanis et al. SODA
'24]. Our approach generalizes recent techniques from [Daskalakis et al. STOC
'19, Galanis et al. SODA '24], to confront the more challenging setting of the
truncated Ising model.

</details>


### [68] [mloz: A Highly Efficient Machine Learning-Based Ozone Parameterization for Climate Sensitivity Simulations](https://arxiv.org/abs/2509.20422)
*Yiling Ma,Nathan Luke Abraham,Stefan Versick,Roland Ruhnke,Andrea Schneidereit,Ulrike Niemeier,Felix Back,Peter Braesicke,Peer Nowack*

Main category: cs.LG

TL;DR: 引入机器学习参数化方法 mloz 模拟臭氧变率和趋势，速度快且有高保真度和可移植性，有望用于缺乏交互式化学的气候模型。


<details>
  <summary>Details</summary>
Motivation: 多数参与 CMIP 的气候模型因大气化学方案计算成本高，缺乏臭氧的交互式表示，需新方法。

Method: 引入机器学习参数化方法 mloz 模拟对流层和平流层臭氧变率和趋势，考虑臭氧与准两年振荡的双向相互作用。

Result: mloz 在十年尺度上有高保真度，在两个不同气候模型中可灵活在线使用，预测速度比 UKESM 化学方案快约 31 倍，运行时间占比小于 4%，且可从 UKESM 移植到 ICON。

Conclusion: mloz 有潜力在缺乏交互式化学的 CMIP 级气候模型中广泛应用，用于未来气候变化评估。

Abstract: Atmospheric ozone is a crucial absorber of solar radiation and an important
greenhouse gas. However, most climate models participating in the Coupled Model
Intercomparison Project (CMIP) still lack an interactive representation of
ozone due to the high computational costs of atmospheric chemistry schemes.
Here, we introduce a machine learning parameterization (mloz) to interactively
model daily ozone variability and trends across the troposphere and
stratosphere in standard climate sensitivity simulations, including two-way
interactions of ozone with the Quasi-Biennial Oscillation. We demonstrate its
high fidelity on decadal timescales and its flexible use online across two
different climate models -- the UK Earth System Model (UKESM) and the German
ICOsahedral Nonhydrostatic (ICON) model. With atmospheric temperature profile
information as the only input, mloz produces stable ozone predictions around 31
times faster than the chemistry scheme in UKESM, contributing less than 4
percent of the respective total climate model runtimes. In particular, we also
demonstrate its transferability to different climate models without chemistry
schemes by transferring the parameterization from UKESM to ICON. This
highlights the potential for widespread adoption in CMIP-level climate models
that lack interactive chemistry for future climate change assessments,
particularly when focusing on climate sensitivity simulations, where ozone
trends and variability are known to significantly modulate atmospheric feedback
processes.

</details>


### [69] [Bridging Privacy and Utility: Synthesizing anonymized EEG with constraining utility functions](https://arxiv.org/abs/2509.20454)
*Kay Fuhrmeister,Arne Pelzer,Fabian Radke,Julia Lechinger,Mahzad Gharleghi,Thomas Köllmer,Insa Wolf*

Main category: cs.LG

TL;DR: 本文提出基于变压器的自动编码器处理EEG数据，在降低可重新识别性的同时保留其机器学习效用。


<details>
  <summary>Details</summary>
Motivation: 随着EEG消费设备增多，用户隐私问题引发关注，需保护EEG敏感数据并保留其应用效用。

Method: 提出基于变压器的自动编码器创建无法进行主体重新识别的EEG数据，并应用于自动睡眠分期评估。

Result: 能大幅降低EEG信号的可重新识别性，同时保留其机器学习效用。

Conclusion: 所提方法可在保护EEG数据隐私的同时，保留其在机器学习任务中的实用性。

Abstract: Electroencephalography (EEG) is widely used for recording brain activity and
has seen numerous applications in machine learning, such as detecting sleep
stages and neurological disorders. Several studies have successfully shown the
potential of EEG data for re-identification and leakage of other personal
information. Therefore, the increasing availability of EEG consumer devices
raises concerns about user privacy, motivating us to investigate how to
safeguard this sensitive data while retaining its utility for EEG applications.
To address this challenge, we propose a transformer-based autoencoder to create
EEG data that does not allow for subject re-identification while still
retaining its utility for specific machine learning tasks. We apply our
approach to automatic sleep staging by evaluating the re-identification and
utility potential of EEG data before and after anonymization. The results show
that the re-identifiability of the EEG signal can be substantially reduced
while preserving its utility for machine learning.

</details>


### [70] [Efficiently Attacking Memorization Scores](https://arxiv.org/abs/2509.20463)
*Tue Do,Varun Chandrasekaran,Daniel Alabi*

Main category: cs.LG

TL;DR: 本文系统研究基于记忆的影响估计器遭攻击的可行性，提出实用攻击方法，通过实验验证并进行理论分析，指出基于影响归因的关键漏洞。


<details>
  <summary>Details</summary>
Motivation: 近期数据估值和负责任机器学习应用引发对影响估计分数能否被对抗操纵的疑问。

Method: 将生成高记忆样本的攻击刻画为训练算法准确区域的高敏感查询，攻击方法是计算输入的伪逆，仅需黑盒访问模型输出且计算开销适中。

Result: 在多种图像分类任务中验证攻击有效性，表明最先进的代理易受目标分数操纵，理论分析揭示影响估计在对抗扰动下的脆弱条件。

Conclusion: 基于影响的归因存在关键漏洞，需要强大的防御措施。

Abstract: Influence estimation tools -- such as memorization scores -- are widely used
to understand model behavior, attribute training data, and inform dataset
curation. However, recent applications in data valuation and responsible
machine learning raise the question: can these scores themselves be
adversarially manipulated? In this work, we present a systematic study of the
feasibility of attacking memorization-based influence estimators. We
characterize attacks for producing highly memorized samples as highly sensitive
queries in the regime where a trained algorithm is accurate. Our attack
(calculating the pseudoinverse of the input) is practical, requiring only
black-box access to model outputs and incur modest computational overhead. We
empirically validate our attack across a wide suite of image classification
tasks, showing that even state-of-the-art proxies are vulnerable to targeted
score manipulations. In addition, we provide a theoretical analysis of the
stability of memorization scores under adversarial perturbations, revealing
conditions under which influence estimates are inherently fragile. Our findings
highlight critical vulnerabilities in influence-based attribution and suggest
the need for robust defenses. All code can be found at
https://anonymous.4open.science/r/MemAttack-5413/

</details>


### [71] [PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models](https://arxiv.org/abs/2509.20570)
*Mingze Yuan,Pengfei Jin,Na Li,Quanzheng Li*

Main category: cs.LG

TL;DR: 提出将物理信息生成作为稀疏奖励优化问题，引入PIRF方法，在五个PDE基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成结果常违反物理定律，现有方法存在依赖DPS值函数近似导致的问题。

Method: 提出Physics-Informed Reward Fine - tuning (PIRF)方法，通过计算轨迹级奖励并直接反向传播梯度绕过值近似，采用层截断反向传播和基于权重的正则化方案。

Result: 在五个PDE基准测试中，PIRF在高效采样机制下始终能实现更好的物理约束。

Conclusion: 奖励微调在推进科学生成建模方面有潜力。

Abstract: Diffusion models have demonstrated strong generative capabilities across
scientific domains, but often produce outputs that violate physical laws. We
propose a new perspective by framing physics-informed generation as a sparse
reward optimization problem, where adherence to physical constraints is treated
as a reward signal. This formulation unifies prior approaches under a
reward-based paradigm and reveals a shared bottleneck: reliance on diffusion
posterior sampling (DPS)-style value function approximations, which introduce
non-negligible errors and lead to training instability and inference
inefficiency. To overcome this, we introduce Physics-Informed Reward
Fine-tuning (PIRF), a method that bypasses value approximation by computing
trajectory-level rewards and backpropagating their gradients directly. However,
a naive implementation suffers from low sample efficiency and compromised data
fidelity. PIRF mitigates these issues through two key strategies: (1) a
layer-wise truncated backpropagation method that leverages the spatiotemporally
localized nature of physics-based rewards, and (2) a weight-based
regularization scheme that improves efficiency over traditional
distillation-based methods. Across five PDE benchmarks, PIRF consistently
achieves superior physical enforcement under efficient sampling regimes,
highlighting the potential of reward fine-tuning for advancing scientific
generative modeling.

</details>


### [72] [Offline Goal-conditioned Reinforcement Learning with Quasimetric Representations](https://arxiv.org/abs/2509.20478)
*Vivek Myers,Bill Chunyuan Zheng,Benjamin Eysenbach,Sergey Levine*

Main category: cs.LG

TL;DR: 提出统一两种目标条件强化学习表示结构框架的方法，在现有基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有目标条件强化学习（GCRL）有两种有效的表示结构框架，希望统一二者。

Method: 利用拟度量表示空间结构和额外约束来学习后继表示，以实现最优目标达成。

Result: 在现有离线GCRL基准测试中，该方法在拼接任务和嘈杂高维环境中提升了性能。

Conclusion: 所提方法结合了两种框架的优势，能在次优数据和随机环境中学习最优目标达成距离。

Abstract: Approaches for goal-conditioned reinforcement learning (GCRL) often use
learned state representations to extract goal-reaching policies. Two frameworks
for representation structure have yielded particularly effective GCRL
algorithms: (1) *contrastive representations*, in which methods learn
"successor features" with a contrastive objective that performs inference over
future outcomes, and (2) *temporal distances*, which link the (quasimetric)
distance in representation space to the transit time from states to goals. We
propose an approach that unifies these two frameworks, using the structure of a
quasimetric representation space (triangle inequality) with the right
additional constraints to learn successor representations that enable optimal
goal-reaching. Unlike past work, our approach is able to exploit a
**quasimetric** distance parameterization to learn **optimal** goal-reaching
distances, even with **suboptimal** data and in **stochastic** environments.
This gives us the best of both worlds: we retain the stability and long-horizon
capabilities of Monte Carlo contrastive RL methods, while getting the free
stitching capabilities of quasimetric network parameterizations. On existing
offline GCRL benchmarks, our representation learning objective improves
performance on stitching tasks where methods based on contrastive learning
struggle, and on noisy, high-dimensional environments where methods based on
quasimetric networks struggle.

</details>


### [73] [CoSupFormer : A Contrastive Supervised learning approach for EEG signal Classification](https://arxiv.org/abs/2509.20489)
*D. Darankoum,C. Habermacher,J. Volle,S. Grudinin*

Main category: cs.LG

TL;DR: 提出新的端到端深度学习框架处理EEG信号，在多应用中验证可提取有意义模式、选高质量通道和实现泛化。


<details>
  <summary>Details</summary>
Motivation: 从原始EEG信号中提取有意义特征并处理噪声和通道变异性是主要挑战，为解决此问题开展研究。

Method: 设计能捕获多尺度频率振荡的编码器；引入基于注意力的编码器学习通道间和局部交互；在注意力编码器上集成门控网络过滤噪声和无信息通道；采用结合监督和对比学习的新型损失函数引导编码过程。

Result: 在多个应用中验证，可从不同物种的原始EEG信号中提取生物学上有意义的模式，自主选择高质量通道。

Conclusion: 通过创新的架构和损失设计，提出的学习范式能有效处理EEG信号，实现鲁棒泛化。

Abstract: Electroencephalography signals (EEGs) contain rich multi-scale information
crucial for understanding brain states, with potential applications in
diagnosing and advancing the drug development landscape. However, extracting
meaningful features from raw EEG signals while handling noise and channel
variability remains a major challenge. This work proposes a novel end-to-end
deep-learning framework that addresses these issues through several key
innovations. First, we designed an encoder capable of explicitly capturing
multi-scale frequency oscillations covering a wide range of features for
different EEG-related tasks. Secondly, to model complex dependencies and handle
the high temporal resolution of EEGs, we introduced an attention-based encoder
that simultaneously learns interactions across EEG channels and within
localized {\em patches} of individual channels. We integrated a dedicated
gating network on top of the attention encoder to dynamically filter out noisy
and non-informative channels, enhancing the reliability of EEG data. The entire
encoding process is guided by a novel loss function, which leverages supervised
and contrastive learning, significantly improving model generalization. We
validated our approach in multiple applications, ranging from the
classification of effects across multiple Central Nervous System (CNS)
disorders treatments to the diagnosis of Parkinson's and Alzheimer's disease.
Our results demonstrate that the proposed learning paradigm can extract
biologically meaningful patterns from raw EEG signals across different species,
autonomously select high-quality channels, and achieve robust generalization
through innovative architectural and loss design.

</details>


### [74] [Guiding Application Users via Estimation of Computational Resources for Massively Parallel Chemistry Computations](https://arxiv.org/abs/2509.20667)
*Tanzila Tabassum,Omer Subasi,Ajay Panyala,Epiya Ebiapia,Gerald Baumgartner,Erdal Mutlu,P.,Sadayappan,Karol Kowalski*

Main category: cs.LG

TL;DR: 本文开发基于机器学习策略预测大规模并行化学计算资源成本，确定最优运行时参数值，评估多种ML模型，实验显示GB模型预测误差低，主动学习可减少实验次数。


<details>
  <summary>Details</summary>
Motivation: 在用户进行昂贵的超级计算机实验前，通过预测资源需求和确定最优参数值为用户提供指导。

Method: 开发基于机器学习的策略，评估多种ML模型和策略，结合主动学习方法。

Result: GB模型预测CCSD迭代总执行时间，Aurora和Frontier的MAPE分别为0.023和0.073；主动学习用约450次实验可使MAPE约为0.2。

Conclusion: 基于机器学习的策略能有效预测资源成本和确定最优参数值，主动学习可在数据收集成本高时发挥作用。

Abstract: In this work, we develop machine learning (ML) based strategies to predict
resources (costs) required for massively parallel chemistry computations, such
as coupled-cluster methods, to guide application users before they commit to
running expensive experiments on a supercomputer. By predicting application
execution time, we determine the optimal runtime parameter values such as
number of nodes and tile sizes. Two key questions of interest to users are
addressed. The first is the shortest-time question, where the user is
interested in knowing the parameter configurations (number of nodes and tile
sizes) to achieve the shortest execution time for a given problem size and a
target supercomputer. The second is the cheapest-run question in which the user
is interested in minimizing resource usage, i.e., finding the number of nodes
and tile size that minimizes the number of node-hours for a given problem size.
  We evaluate a rich family of ML models and strategies, developed based on the
collections of runtime parameter values for the CCSD (Coupled Cluster with
Singles and Doubles) application executed on the Department of Energy (DOE)
Frontier and Aurora supercomputers. Our experiments show that when predicting
the total execution time of a CCSD iteration, a Gradient Boosting (GB) ML model
achieves a Mean Absolute Percentage Error (MAPE) of 0.023 and 0.073 for Aurora
and Frontier, respectively. In the case where it is expensive to run
experiments just to collect data points, we show that active learning can
achieve a MAPE of about 0.2 with just around 450 experiments collected from
Aurora and Frontier.

</details>


### [75] [Beyond Visual Similarity: Rule-Guided Multimodal Clustering with explicit domain rules](https://arxiv.org/abs/2509.20501)
*Kishor Datta Gupta,Mohd Ariful Haque,Marufa Kamal,Ahmed Rafi Hasan,Md. Mahfuzur Rahman,Roy George*

Main category: cs.LG

TL;DR: 提出DARTVAE规则引导的多模态聚类框架，结合规则编码与学习表示，在数据集上实验有效但面临挑战。


<details>
  <summary>Details</summary>
Motivation: 传统聚类技术仅依赖输入数据相似性，难以捕捉关键的结构或语义约束，需改进。

Method: DARTVAE扩展VAE架构，将规则、语义表示和数据驱动特征嵌入统一潜在空间，通过损失函数强制约束合规，规则由LLM生成并构建知识图。

Result: 在飞机和汽车数据集实验表明，规则引导聚类产生更有意义和可解释的聚类，提升传统聚类指标。

Conclusion: DARTVAE比纯数据驱动模型有更有意义和一致的聚类结果，凸显约束引导多模态聚类在复杂知识密集场景的实用性。

Abstract: Traditional clustering techniques often rely solely on similarity in the
input data, limiting their ability to capture structural or semantic
constraints that are critical in many domains. We introduce the Domain Aware
Rule Triggered Variational Autoencoder (DARTVAE), a rule guided multimodal
clustering framework that incorporates domain specific constraints directly
into the representation learning process. DARTVAE extends the VAE architecture
by embedding explicit rules, semantic representations, and data driven features
into a unified latent space, while enforcing constraint compliance through rule
consistency and violation penalties in the loss function. Unlike conventional
clustering methods that rely only on visual similarity or apply rules as post
hoc filters, DARTVAE treats rules as first class learning signals. The rules
are generated by LLMs, structured into knowledge graphs, and enforced through a
loss function combining reconstruction, KL divergence, consistency, and
violation penalties. Experiments on aircraft and automotive datasets
demonstrate that rule guided clustering produces more operationally meaningful
and interpretable clusters for example, isolating UAVs, unifying stealth
aircraft, or separating SUVs from sedans while improving traditional clustering
metrics. However, the framework faces challenges: LLM generated rules may
hallucinate or conflict, excessive rules risk overfitting, and scaling to
complex domains increases computational and consistency difficulties. By
combining rule encodings with learned representations, DARTVAE achieves more
meaningful and consistent clustering outcomes than purely data driven models,
highlighting the utility of constraint guided multimodal clustering for
complex, knowledge intensive settings.

</details>


### [76] [Myosotis: structured computation for attention like layer](https://arxiv.org/abs/2509.20503)
*Evgenii Egorov,Hanno Ackermann,Markus Nagel,Hong Cai*

Main category: cs.LG

TL;DR: 现有注意力层计算和内存随序列长度二次增长，两种缓解方法有缺点，提出结合两者优点的新算法。


<details>
  <summary>Details</summary>
Motivation: 解决注意力层在无结构假设下，内存和计算随序列长度二次增长的问题，且改善现有缓解方法的缺点。

Method: 基于树结构矩阵的有效求逆提出新算法。

Result: 文档未提及。

Conclusion: 文档未提及。

Abstract: Attention layers apply a sequence-to-sequence mapping whose parameters depend
on the pairwise interactions of the input elements. However, without any
structural assumptions, memory and compute scale quadratically with the
sequence length. The two main ways to mitigate this are to introduce sparsity
by ignoring a sufficient amount of pairwise interactions or to introduce
recurrent dependence along them, as SSM does. Although both approaches are
reasonable, they both have disadvantages. We propose a novel algorithm that
combines the advantages of both concepts. Our idea is based on the efficient
inversion of tree-structured matrices.

</details>


### [77] [FHRFormer: A Self-supervised Transformer Approach for Fetal Heart Rate Inpainting and Forecasting](https://arxiv.org/abs/2509.20852)
*Kjersti Engan,Neel Kanwal,Anita Yeconia,Ladislaus Blacy,Yuda Munyaw,Estomih Mduma,Hege Ersdal*

Main category: cs.LG

TL;DR: 文章提出用基于掩码变压器的自编码器方法重建缺失的胎儿心率信号，该方法在不同缺失数据时长下表现稳健，可用于信号修复和预测，还能用于支持AI风险算法开发及集成到可穿戴设备。


<details>
  <summary>Details</summary>
Motivation: 约10%新生儿出生时需呼吸协助，5%需通气支持，胎儿心率监测对评估胎儿健康至关重要，应用AI分析可预测风险，但信号缺失问题限制了自动化分析，传统处理方法效果不佳。

Method: 提出基于掩码变压器的自编码器方法，捕捉数据的空间和频率成分来重建缺失的胎儿心率信号。

Result: 该方法在不同缺失数据时长下都表现出稳健性，可用于信号修复和预测。

Conclusion: 该方法可回顾性应用于研究数据集以支持AI风险算法开发，未来可集成到可穿戴胎儿心率监测设备实现更早期、更稳健的风险检测。

Abstract: Approximately 10\% of newborns require assistance to initiate breathing at
birth, and around 5\% need ventilation support. Fetal heart rate (FHR)
monitoring plays a crucial role in assessing fetal well-being during prenatal
care, enabling the detection of abnormal patterns and supporting timely
obstetric interventions to mitigate fetal risks during labor. Applying
artificial intelligence (AI) methods to analyze large datasets of continuous
FHR monitoring episodes with diverse outcomes may offer novel insights into
predicting the risk of needing breathing assistance or interventions. Recent
advances in wearable FHR monitors have enabled continuous fetal monitoring
without compromising maternal mobility. However, sensor displacement during
maternal movement, as well as changes in fetal or maternal position, often lead
to signal dropouts, resulting in gaps in the recorded FHR data. Such missing
data limits the extraction of meaningful insights and complicates automated
(AI-based) analysis. Traditional approaches to handle missing data, such as
simple interpolation techniques, often fail to preserve the spectral
characteristics of the signals. In this paper, we propose a masked
transformer-based autoencoder approach to reconstruct missing FHR signals by
capturing both spatial and frequency components of the data. The proposed
method demonstrates robustness across varying durations of missing data and can
be used for signal inpainting and forecasting. The proposed approach can be
applied retrospectively to research datasets to support the development of
AI-based risk algorithms. In the future, the proposed method could be
integrated into wearable FHR monitoring devices to achieve earlier and more
robust risk detection.

</details>


### [78] [Auto-Regressive U-Net for Full-Field Prediction of Shrinkage-Induced Damage in Concrete](https://arxiv.org/abs/2509.20507)
*Liya Gaynutdinova,Petr Havlásek,Ondřej Rokoš,Fleur Hendriks,Martin Doškář*

Main category: cs.LG

TL;DR: 本文提出深度学习方法预测混凝土时变全场损伤，双网络架构高效且性能好，可优化混凝土配合比。


<details>
  <summary>Details</summary>
Motivation: 传统全场损伤评估计算量大，需新方法预测混凝土时变全场损伤，优化混凝土配合比。

Method: 采用自回归U - Net模型预测损伤场演化，用卷积神经网络根据损伤估计预测关键力学性能。

Result: 双网络架构在合成数据集上计算效率高、预测性能强，降低了计算负载。

Conclusion: 该方法有助于洞察骨料特性与混凝土性能关系，优化混凝土配合比，提高耐久性、减少内部损伤。

Abstract: This paper introduces a deep learning approach for predicting time-dependent
full-field damage in concrete. The study uses an auto-regressive U-Net model to
predict the evolution of the scalar damage field in a unit cell given
microstructural geometry and evolution of an imposed shrinkage profile. By
sequentially using the predicted damage output as input for subsequent
predictions, the model facilitates the continuous assessment of damage
progression. Complementarily, a convolutional neural network (CNN) utilises the
damage estimations to forecast key mechanical properties, including observed
shrinkage and residual stiffness. The proposed dual-network architecture
demonstrates high computational efficiency and robust predictive performance on
the synthesised datasets. The approach reduces the computational load
traditionally associated with full-field damage evaluations and is used to gain
insights into the relationship between aggregate properties, such as shape,
size, and distribution, and the effective shrinkage and reduction in stiffness.
Ultimately, this can help to optimize concrete mix designs, leading to improved
durability and reduced internal damage.

</details>


### [79] [Complexity-Driven Policy Optimization](https://arxiv.org/abs/2509.20509)
*Luca Serfilippi,Giorgio Franceschelli,Antonio Corradi,Mirco Musolesi*

Main category: cs.LG

TL;DR: 本文提出用复杂度奖励替代熵奖励，引入CDPO算法，实验表明CDPO在离散动作空间任务中更稳健。


<details>
  <summary>Details</summary>
Motivation: 传统策略梯度方法通过熵最大化平衡探索与利用，但最大化熵会使策略趋于均匀随机分布，探索效率低。

Method: 用香农熵和非平衡度的乘积定义复杂度，替代熵奖励；基于PPO算法引入CDPO算法。

Result: 在一系列离散动作空间任务中，CDPO比PPO对复杂度系数的选择更稳健，在需要更多探索的环境中表现更优。

Conclusion: 复杂度奖励比熵奖励更能引导智能体发现有结构且可适应的策略，CDPO算法具有更好的性能。

Abstract: Policy gradient methods often balance exploitation and exploration via
entropy maximization. However, maximizing entropy pushes the policy towards a
uniform random distribution, which represents an unstructured and sometimes
inefficient exploration strategy. In this work, we propose replacing the
entropy bonus with a more robust complexity bonus. In particular, we adopt a
measure of complexity, defined as the product of Shannon entropy and
disequilibrium, where the latter quantifies the distance from the uniform
distribution. This regularizer encourages policies that balance stochasticity
(high entropy) with structure (high disequilibrium), guiding agents toward
regimes where useful, non-trivial behaviors can emerge. Such behaviors arise
because the regularizer suppresses both extremes, e.g., maximal disorder and
complete order, creating pressure for agents to discover structured yet
adaptable strategies. Starting from Proximal Policy Optimization (PPO), we
introduce Complexity-Driven Policy Optimization (CDPO), a new learning
algorithm that replaces entropy with complexity. We show empirically across a
range of discrete action space tasks that CDPO is more robust to the choice of
the complexity coefficient than PPO is with the entropy coefficient, especially
in environments requiring greater exploration.

</details>


### [80] [A Recovery Theory for Diffusion Priors: Deterministic Analysis of the Implicit Prior Algorithm](https://arxiv.org/abs/2509.20511)
*Oscar Leong,Yann Traonmilin*

Main category: cs.LG

TL;DR: 本文为基于确定性扩散的逆问题算法构建理论框架，分析噪声卷积得分，推导收敛率并应用于两种数据分布。


<details>
  <summary>Details</summary>
Motivation: 生成扩散模型在逆问题中虽有实证成功，但严格恢复保证有限，需构建理论框架分析算法。

Method: 聚焦Kadkhodaie & Simoncelli提出算法的确定性版本，分析低维模型集上噪声卷积得分，将算法解释为广义投影梯度下降方法。

Result: 当传感矩阵满足受限等距性质时，可推导显式依赖噪声时间表的定量收敛率；在低秩高斯混合模型中可建立全局收敛保证。

Conclusion: 所构建的理论框架可有效分析基于确定性扩散的逆问题算法，适用于不同数据分布。

Abstract: Recovering high-dimensional signals from corrupted measurements is a central
challenge in inverse problems. Recent advances in generative diffusion models
have shown remarkable empirical success in providing strong data-driven priors,
but rigorous recovery guarantees remain limited. In this work, we develop a
theoretical framework for analyzing deterministic diffusion-based algorithms
for inverse problems, focusing on a deterministic version of the algorithm
proposed by Kadkhodaie \& Simoncelli \cite{kadkhodaie2021stochastic}. First, we
show that when the underlying data distribution concentrates on a
low-dimensional model set, the associated noise-convolved scores can be
interpreted as time-varying projections onto such a set. This leads to
interpreting previous algorithms using diffusion priors for inverse problems as
generalized projected gradient descent methods with varying projections. When
the sensing matrix satisfies a restricted isometry property over the model set,
we can derive quantitative convergence rates that depend explicitly on the
noise schedule. We apply our framework to two instructive data distributions:
uniform distributions over low-dimensional compact, convex sets and low-rank
Gaussian mixture models. In the latter setting, we can establish global
convergence guarantees despite the nonconvexity of the underlying model set.

</details>


### [81] [The Sensitivity of Variational Bayesian Neural Network Performance to Hyperparameters](https://arxiv.org/abs/2509.20574)
*Scott Koermer,Natalie Klein*

Main category: cs.LG

TL;DR: 本文探讨贝叶斯神经网络（BNNs）超参数选择对不确定性量化（UQ）的影响，建议用全局敏感性分析等方法辅助超参数选择。


<details>
  <summary>Details</summary>
Motivation: 科学应用中预测建模需准确的不确定性量化（UQ），BNNs虽能产生预测不确定性，但实际获得准确UQ困难，超参数选择影响大，需研究其影响。

Method: 对不同超参数设置下的BNNs性能进行全局敏感性分析。

Result: 许多超参数相互作用，影响预测准确性和UQ。

Conclusion: 为提高BNNs在实际应用中的使用效果，建议用全局敏感性分析或贝叶斯优化等方法辅助降维和超参数选择，以确保准确的UQ。

Abstract: In scientific applications, predictive modeling is often of limited use
without accurate uncertainty quantification (UQ) to indicate when a model may
be extrapolating or when more data needs to be collected. Bayesian Neural
Networks (BNNs) produce predictive uncertainty by propagating uncertainty in
neural network (NN) weights and offer the promise of obtaining not only an
accurate predictive model but also accurate UQ. However, in practice, obtaining
accurate UQ with BNNs is difficult due in part to the approximations used for
practical model training and in part to the need to choose a suitable set of
hyperparameters; these hyperparameters outnumber those needed for traditional
NNs and often have opaque effects on the results. We aim to shed light on the
effects of hyperparameter choices for BNNs by performing a global sensitivity
analysis of BNN performance under varying hyperparameter settings. Our results
indicate that many of the hyperparameters interact with each other to affect
both predictive accuracy and UQ. For improved usage of BNNs in real-world
applications, we suggest that global sensitivity analysis, or related methods
such as Bayesian optimization, should be used to aid in dimensionality
reduction and selection of hyperparameters to ensure accurate UQ in BNNs.

</details>


### [82] [MDBench: Benchmarking Data-Driven Methods for Model Discovery](https://arxiv.org/abs/2509.20529)
*Amirmohammad Ziaei Bideh,Aleksandra Georgievska,Jonathan Gryak*

Main category: cs.LG

TL;DR: 本文介绍开源基准框架MDBench评估动力系统模型发现方法，在不同噪声下测试多种算法，揭示当前方法局限，指出线性和遗传编程方法优势，加速该领域发展。


<details>
  <summary>Details</summary>
Motivation: 现有动力系统模型发现方法缺乏全面基准测试，需要评估方法进展和权衡。

Method: 引入MDBench框架，对12种算法在14个偏微分方程和63个常微分方程上进行不同噪声水平测试，使用多种评估指标。

Result: 线性方法和遗传编程方法分别在PDEs和ODEs上预测误差最低，线性模型更抗噪。

Conclusion: MDBench提供严格可扩展基准框架和多样数据集，能系统评估、比较和改进方程准确性与鲁棒性，加速模型发现方法发展。

Abstract: Model discovery aims to uncover governing differential equations of dynamical
systems directly from experimental data. Benchmarking such methods is essential
for tracking progress and understanding trade-offs in the field. While prior
efforts have focused mostly on identifying single equations, typically framed
as symbolic regression, there remains a lack of comprehensive benchmarks for
discovering dynamical models. To address this, we introduce MDBench, an
open-source benchmarking framework for evaluating model discovery methods on
dynamical systems. MDBench assesses 12 algorithms on 14 partial differential
equations (PDEs) and 63 ordinary differential equations (ODEs) under varying
levels of noise. Evaluation metrics include derivative prediction accuracy,
model complexity, and equation fidelity. We also introduce seven challenging
PDE systems from fluid dynamics and thermodynamics, revealing key limitations
in current methods. Our findings illustrate that linear methods and genetic
programming methods achieve the lowest prediction error for PDEs and ODEs,
respectively. Moreover, linear models are in general more robust against noise.
MDBench accelerates the advancement of model discovery methods by offering a
rigorous, extensible benchmarking framework and a rich, diverse collection of
dynamical system datasets, enabling systematic evaluation, comparison, and
improvement of equation accuracy and robustness.

</details>


### [83] [Understanding and Improving Adversarial Robustness of Neural Probabilistic Circuits](https://arxiv.org/abs/2509.20549)
*Weixin Chen,Han Zhao*

Main category: cs.LG

TL;DR: 本文分析NPC对抗鲁棒性，提出抗攻击的RNPC，理论和实验表明其鲁棒性优于NPC及现有概念瓶颈模型。


<details>
  <summary>Details</summary>
Motivation: NPC的属性识别模型是黑盒，易受对抗攻击影响最终预测，需提升其对抗鲁棒性。

Method: 理论分析NPC对抗鲁棒性，提出RNPC，引入新颖的类级集成进行推理。

Result: 理论证明RNPC比NPC有更好的对抗鲁棒性，图像分类任务实验显示RNPC比现有概念瓶颈模型有更好的对抗鲁棒性，且正常输入准确率高。

Conclusion: RNPC在对抗攻击下有更好的鲁棒性，能在保持正常输入准确率的同时抵御攻击。

Abstract: Neural Probabilistic Circuits (NPCs), a new class of concept bottleneck
models, comprise an attribute recognition model and a probabilistic circuit for
reasoning. By integrating the outputs from these two modules, NPCs produce
compositional and interpretable predictions. While offering enhanced
interpretability and high performance on downstream tasks, the
neural-network-based attribute recognition model remains a black box. This
vulnerability allows adversarial attacks to manipulate attribute predictions by
introducing carefully crafted subtle perturbations to input images, potentially
compromising the final predictions. In this paper, we theoretically analyze the
adversarial robustness of NPC and demonstrate that it only depends on the
robustness of the attribute recognition model and is independent of the
robustness of the probabilistic circuit. Moreover, we propose RNPC, the first
robust neural probabilistic circuit against adversarial attacks on the
recognition module. RNPC introduces a novel class-wise integration for
inference, ensuring a robust combination of outputs from the two modules. Our
theoretical analysis demonstrates that RNPC exhibits provably improved
adversarial robustness compared to NPC. Empirical results on image
classification tasks show that RNPC achieves superior adversarial robustness
compared to existing concept bottleneck models while maintaining high accuracy
on benign inputs.

</details>


### [84] [Bispectral OT: Dataset Comparison using Symmetry-Aware Optimal Transport](https://arxiv.org/abs/2509.20678)
*Annabel Ma,Kaiying Hou,David Alvarez-Melis,Melanie Weber*

Main category: cs.LG

TL;DR: 引入双谱最优传输（Bispectral Optimal Transport），一种对称感知的离散最优传输扩展，在视觉对称变换的基准数据集上比朴素特征最优传输有更好的类保留精度。


<details>
  <summary>Details</summary>
Motivation: 在富含对称性的场景中，仅基于原始特征间成对几何距离的最优传输对齐会忽略数据的内在相干结构。

Method: 引入双谱最优传输，使用双谱（一种群傅里叶不变量）来比较元素。

Result: 在视觉对称变换的基准数据集上，双谱最优传输计算的传输计划比朴素特征最优传输有更高的类保留精度。

Conclusion: 双谱最优传输能捕捉数据集中潜在的语义标签结构，同时去除不影响类别或内容的干扰变化，提高了有意义对应关系的质量。

Abstract: Optimal transport (OT) is a widely used technique in machine learning,
graphics, and vision that aligns two distributions or datasets using their
relative geometry. In symmetry-rich settings, however, OT alignments based
solely on pairwise geometric distances between raw features can ignore the
intrinsic coherence structure of the data. We introduce Bispectral Optimal
Transport, a symmetry-aware extension of discrete OT that compares elements
using their representation using the bispectrum, a group Fourier invariant that
preserves all signal structure while removing only the variation due to group
actions. Empirically, we demonstrate that the transport plans computed with
Bispectral OT achieve greater class preservation accuracy than naive feature OT
on benchmark datasets transformed with visual symmetries, improving the quality
of meaningful correspondences that capture the underlying semantic label
structure in the dataset while removing nuisance variation not affecting class
or content.

</details>


### [85] [Generalizable Diabetes Risk Stratification via Hybrid Machine Learning Models](https://arxiv.org/abs/2509.20565)
*Athar Parvez,Muhammad Jawad Mufti*

Main category: cs.LG

TL;DR: 本文对比两种混合分类器用于糖尿病风险分层，发现XGB - RF表现优于SVM - LR，支持基于梯度提升的混合方法。


<details>
  <summary>Details</summary>
Motivation: 全球糖尿病患者众多，早期风险分层可借助机器学习，需对比两种混合分类器并评估其在外部队列的泛化能力。

Method: 构建XGB - RF和SVM - LR两种混合分类器，使用泄漏安全的标准化管道在主要数据集上拟合，用PIMA队列进行外部验证，评估指标包括AUROC、AUPRC等。

Result: 在主要数据集和PIMA队列上，XGB - RF的各项指标均优于SVM - LR。

Conclusion: XGB - RF在内部和外部队列表现均占优，支持基于梯度提升的混合方法用于糖尿病风险分层，可开展前瞻性多站点验证。

Abstract: Background/Purpose: Diabetes affects over 537 million people worldwide and is
projected to reach 783 million by 2045. Early risk stratification can benefit
from machine learning. We compare two hybrid classifiers and assess their
generalizability on an external cohort.
  Methods: Two hybrids were built: (i) XGBoost + Random Forest (XGB-RF) and
(ii) Support Vector Machine + Logistic Regression (SVM-LR). A leakage-safe,
standardized pipeline (encoding, imputation, min-max scaling; SMOTE on training
folds only; probability calibration for SVM) was fit on the primary dataset and
frozen. Evaluation prioritized threshold-independent discrimination
(AUROC/AUPRC) and calibration (Brier, slope/intercept). External validation
used the PIMA cohort (N=768) with the frozen pipeline; any thresholded metrics
on PIMA were computed at the default rule tau = 0.5.
  Results: On the primary dataset (PR baseline = 0.50), XGB-RF achieved AUROC
~0.995 and AUPRC ~0.998, outperforming SVM-LR (AUROC ~0.978; AUPRC ~0.947). On
PIMA (PR baseline ~0.349), XGB-RF retained strong performance (AUROC ~0.990;
AUPRC ~0.959); SVM-LR was lower (AUROC ~0.963; AUPRC ~0.875). Thresholded
metrics on PIMA at tau = 0.5 were XGB-RF (Accuracy 0.960; Precision 0.941;
Recall 0.944; F1 0.942) and SVM-LR (Accuracy 0.900; Precision 0.855; Recall
0.858; F1 0.857).
  Conclusions: Across internal and external cohorts, XGB-RF consistently
dominated SVM-LR and exhibited smaller external attenuation on ROC/PR with
acceptable calibration. These results support gradient-boosting-based
hybridization as a robust, transferable approach for diabetes risk
stratification and motivate prospective, multi-site validation with
deployment-time threshold selection based on clinical trade-offs.

</details>


### [86] [Scaling Laws are Redundancy Laws](https://arxiv.org/abs/2509.20721)
*Yuda Bi,Vince D Calhoun*

Main category: cs.LG

TL;DR: 本文将深度学习的缩放定律解释为冗余定律，给出了缩放指数的表达式，证明了定律在多种情况下的普遍性，统一了经验观察和理论基础。


<details>
  <summary>Details</summary>
Motivation: 解释深度学习缩放定律的数学起源，尤其是缩放指数。

Method: 使用核回归方法，分析数据协方差谱的多项式尾部。

Result: 得出过剩风险幂律的指数表达式，发现学习曲线斜率取决于数据冗余，证明定律在多种情况下的普遍性。

Conclusion: 首次将缩放定律严格解释为有限样本冗余定律，统一了经验观察和理论基础。

Abstract: Scaling laws, a defining feature of deep learning, reveal a striking
power-law improvement in model performance with increasing dataset and model
size. Yet, their mathematical origins, especially the scaling exponent, have
remained elusive. In this work, we show that scaling laws can be formally
explained as redundancy laws. Using kernel regression, we show that a
polynomial tail in the data covariance spectrum yields an excess risk power law
with exponent alpha = 2s / (2s + 1/beta), where beta controls the spectral tail
and 1/beta measures redundancy. This reveals that the learning curve's slope is
not universal but depends on data redundancy, with steeper spectra accelerating
returns to scale. We establish the law's universality across boundedly
invertible transformations, multi-modal mixtures, finite-width approximations,
and Transformer architectures in both linearized (NTK) and feature-learning
regimes. This work delivers the first rigorous mathematical explanation of
scaling laws as finite-sample redundancy laws, unifying empirical observations
with theoretical foundations.

</details>


### [87] [Go With The Flow: Churn-Tolerant Decentralized Training of Large Language Models](https://arxiv.org/abs/2509.21221)
*Nikolay Blagoev,Bart Cox,Jérémie Decouchant,Lydia Y. Chen*

Main category: cs.LG

TL;DR: 提出用于大语言模型的容错去中心化训练框架GWTF，评估显示在复杂场景下可减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 大语言模型出现以及模型训练民主化的重要性，现有分布式和联邦训练框架存在不足。

Method: 提出核心为新型去中心化流算法的GWTF框架，寻找最有效路由。

Result: 在类似GPT和LLaMa模型上评估，在涉及10个不同地理位置、高节点变动率的异构客户端节点的现实且有挑战性的场景中，GWTF最多可减少45%的训练时间。

Conclusion: GWTF能有效应对异构客户端资源协作训练、节点变动和网络不稳定问题，降低训练时间。

Abstract: Motivated by the emergence of large language models (LLMs) and the importance
of democratizing their training, we propose GWTF, the first crash tolerant
practical decentralized training framework for LLMs. Differently from existing
distributed and federated training frameworks, GWTF enables the efficient
collaborative training of a LLM on heterogeneous clients that volunteer their
resources. In addition, GWTF addresses node churn, i.e., clients joining or
leaving the system at any time, and network instabilities, i.e., network links
becoming unstable or unreliable. The core of GWTF is a novel decentralized flow
algorithm that finds the most effective routing that maximizes the number of
microbatches trained with the lowest possible delay. We extensively evaluate
GWTF on GPT-like and LLaMa-like models and compare it against the prior art.
Our results indicate that GWTF reduces the training time by up to 45% in
realistic and challenging scenarios that involve heterogeneous client nodes
distributed over 10 different geographic locations with a high node churn rate.

</details>


### [88] [SuperOffload: Unleashing the Power of Large-Scale LLM Training on Superchips](https://arxiv.org/abs/2509.21271)
*Xinyu Lian,Masahiro Tanaka,Olatunji Ruwase,Minjia Zhang*

Main category: cs.LG

TL;DR: 本文首次研究基于Superchips的大语言模型（LLM）训练卸载方案，提出SuperOffload系统，在NVIDIA GH200上评估显示有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 此前缺乏对Superchips新架构如何助力LLM训练的研究。

Method: 提出SuperOffload系统，结合自适应权重卸载、分桶重分区等技术。

Result: 在NVIDIA GH200上评估，吞吐量最高提升2.5倍，能在单芯片上训练25B模型，扩展后能在8个GH200上训练13B模型并达到55%的MFU。

Conclusion: SuperOffload系统能有效利用Superchips架构，提升LLM训练性能。

Abstract: The emergence of Superchips represents a significant advancement in
next-generation AI hardware. These Superchips employ a tightly coupled
heterogeneous architecture that integrates GPU and CPU on the same package,
which offers unprecedented computational power. However, there has been scant
research investigating how LLM training benefits from this new architecture. In
this work, for the first time, we study LLM training solutions based on
offloading for Superchips. We observe important differences between Superchips
and traditional loosely-coupled GPU-CPU architecture, which necessitate
revisiting prevailing assumptions about offloading. Based on that, we present
SuperOffload, a Superchip-centric offloading system that simultaneously uses
Hopper GPU, Grace CPU, and NVLink-C2C interconnect more efficiently.
SuperOffload accomplishes this via a combination of techniques, such as
adaptive weight offloading, bucketization repartitioning, Superchip-aware
casting, speculative execution, and a highly optimized Adam optimizer for Grace
CPUs. Our evaluation of SuperOffload on NVIDIA GH200 demonstrates up to 2.5x
throughput improvement compared to state-of-the-art offloading-based systems,
enabling training of up to 25B model on a single Superchip while achieving high
training throughput. We also extend SuperOffload with ZeRO-style data
parallelism and DeepSpeed-Ulysses sequence parallelism, enabling training of
13B model with sequence lengths up to 1 million tokens on 8 GH200 while
achieving 55% MFU.

</details>


### [89] [Efficient Ensemble Conditional Independence Test Framework for Causal Discovery](https://arxiv.org/abs/2509.21021)
*Zhengkang Guan,Kun Kuang*

Main category: cs.LG

TL;DR: 提出集成条件独立性测试（E - CIT）框架解决基于约束的因果发现中条件独立性测试计算成本高的问题，降低计算复杂度且实验效果好。


<details>
  <summary>Details</summary>
Motivation: 基于约束的因果发现依赖大量条件独立性测试，计算成本高限制了其实际应用，需解决该瓶颈。

Method: 引入E - CIT框架，采用划分聚合策略，将数据划分子集，对各子集独立应用给定的基础条件独立性测试，用基于稳定分布特性的新方法聚合p值。

Result: E - CIT显著降低条件独立性测试和因果发现的计算负担，在复杂测试场景尤其是真实数据集上表现良好。

Conclusion: E - CIT是一个通用且即插即用的框架，能有效降低计算复杂度，实现有竞争力的性能。

Abstract: Constraint-based causal discovery relies on numerous conditional independence
tests (CITs), but its practical applicability is severely constrained by the
prohibitive computational cost, especially as CITs themselves have high time
complexity with respect to the sample size. To address this key bottleneck, we
introduce the Ensemble Conditional Independence Test (E-CIT), a general and
plug-and-play framework. E-CIT operates on an intuitive divide-and-aggregate
strategy: it partitions the data into subsets, applies a given base CIT
independently to each subset, and aggregates the resulting p-values using a
novel method grounded in the properties of stable distributions. This framework
reduces the computational complexity of a base CIT to linear in the sample size
when the subset size is fixed. Moreover, our tailored p-value combination
method offers theoretical consistency guarantees under mild conditions on the
subtests. Experimental results demonstrate that E-CIT not only significantly
reduces the computational burden of CITs and causal discovery but also achieves
competitive performance. Notably, it exhibits an improvement in complex testing
scenarios, particularly on real-world datasets.

</details>


### [90] [Learning Greens Operators through Hierarchical Neural Networks Inspired by the Fast Multipole Method](https://arxiv.org/abs/2509.20591)
*Emilio McAllister Fognini,Marta M. Betcke,Ben T. Cox*

Main category: cs.LG

TL;DR: 提出结合FMM与机器学习架构的Neural FMM用于学习椭圆PDE的格林算子。


<details>
  <summary>Details</summary>
Motivation: FMM虽应用广泛，但与现代机器学习架构结合的研究不足。

Method: 提出Neural FMM架构，将FMM信息流融入分层机器学习框架，利用FMM分层计算流分离远近场相互作用。

Result: 未提及。

Conclusion: 未提及。

Abstract: The Fast Multipole Method (FMM) is an efficient numerical algorithm for
computation of long-ranged forces in $N$-body problems within gravitational and
electrostatic fields. This method utilizes multipole expansions of the Green's
function inherent to the underlying dynamical systems. Despite its widespread
application in physics and engineering, the integration of FMM with modern
machine learning architectures remains underexplored. In this work, we propose
a novel neural network architecture, the Neural FMM, that integrates the
information flow of the FMM into a hierarchical machine learning framework for
learning the Green's operator of an Elliptic PDE. Our Neural FMM architecture
leverages a hierarchical computation flow of the FMM method to split up the
local and far-field interactions and efficiently learn their respective
representations.

</details>


### [91] [Inverse Reinforcement Learning Using Just Classification and a Few Regressions](https://arxiv.org/abs/2509.21172)
*Lars van der Laan,Nathan Kallus,Aurélien Bibaut*

Main category: cs.LG

TL;DR: 本文重新审视softmax逆强化学习（IRL），将其转化为两个现成的监督学习问题，方法简单且模块化，有理论分析和优于MaxEnt IRL的实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有IRL实际学习算法在使用现代高表达能力函数逼近器时存在复杂的内循环优化、重复动态规划或对抗训练等问题。

Method: 证明总体最大似然解由涉及行为策略的线性不动点方程表征，将IRL转化为概率分类估计行为策略和迭代回归求解不动点两个监督学习问题。

Result: 提供了最优解的精确表征、基于神谕的通用算法、有限样本误差界，实验显示性能与MaxEnt IRL相当或更优。

Conclusion: 提出的方法简单、模块化，在IRL中有较好表现。

Abstract: Inverse reinforcement learning (IRL) aims to explain observed behavior by
uncovering an underlying reward. In the maximum-entropy or
Gumbel-shocks-to-reward frameworks, this amounts to fitting a reward function
and a soft value function that together satisfy the soft Bellman consistency
condition and maximize the likelihood of observed actions. While this
perspective has had enormous impact in imitation learning for robotics and
understanding dynamic choices in economics, practical learning algorithms often
involve delicate inner-loop optimization, repeated dynamic programming, or
adversarial training, all of which complicate the use of modern, highly
expressive function approximators like neural nets and boosting. We revisit
softmax IRL and show that the population maximum-likelihood solution is
characterized by a linear fixed-point equation involving the behavior policy.
This observation reduces IRL to two off-the-shelf supervised learning problems:
probabilistic classification to estimate the behavior policy, and iterative
regression to solve the fixed point. The resulting method is simple and modular
across function approximation classes and algorithms. We provide a precise
characterization of the optimal solution, a generic oracle-based algorithm,
finite-sample error bounds, and empirical results showing competitive or
superior performance to MaxEnt IRL.

</details>


### [92] [TSKAN: Interpretable Machine Learning for QoE modeling over Time Series Data](https://arxiv.org/abs/2509.20595)
*Kamal Singh,Priyanka Rawat,Sami Marouani,Baptiste Jeudy*

Main category: cs.LG

TL;DR: 提出用可解释机器学习技术对原始时间序列数据进行视频流应用QoE建模的新方法，在流行数据集上评估展示了优势。


<details>
  <summary>Details</summary>
Motivation: QoE建模对优化视频流服务很重要，需捕捉不同特征和用户体验间复杂关系，传统方法是黑盒。

Method: 结合Kolmogorov - Arnold Networks（KANs）在紧凑频域特征上进行可解释读出，处理原始时间序列数据。

Result: 在流行数据集上评估，QoE预测准确性提高。

Conclusion: 该方法在提高准确性的同时具有透明度和可解释性。

Abstract: Quality of Experience (QoE) modeling is crucial for optimizing video
streaming services to capture the complex relationships between different
features and user experience. We propose a novel approach to QoE modeling in
video streaming applications using interpretable Machine Learning (ML)
techniques over raw time series data. Unlike traditional black-box approaches,
our method combines Kolmogorov-Arnold Networks (KANs) as an interpretable
readout on top of compact frequency-domain features, allowing us to capture
temporal information while retaining a transparent and explainable model. We
evaluate our method on popular datasets and demonstrate its enhanced accuracy
in QoE prediction, while offering transparency and interpretability.

</details>


### [93] [Closed-form $\ell_r$ norm scaling with data for overparameterized linear regression and diagonal linear networks under $\ell_p$ bias](https://arxiv.org/abs/2509.21181)
*Shuofeng Zhang,Ard Louis*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: For overparameterized linear regression with isotropic Gaussian design and
minimum-$\ell_p$ interpolator $p\in(1,2]$, we give a unified, high-probability
characterization for the scaling of the family of parameter norms $ \\{ \lVert
\widehat{w_p} \rVert_r \\}_{r \in [1,p]} $ with sample size.
  We solve this basic, but unresolved question through a simple dual-ray
analysis, which reveals a competition between a signal *spike* and a *bulk* of
null coordinates in $X^\top Y$, yielding closed-form predictions for (i) a
data-dependent transition $n_\star$ (the "elbow"), and (ii) a universal
threshold $r_\star=2(p-1)$ that separates $\lVert \widehat{w_p} \rVert_r$'s
which plateau from those that continue to grow with an explicit exponent.
  This unified solution resolves the scaling of *all* $\ell_r$ norms within the
family $r\in [1,p]$ under $\ell_p$-biased interpolation, and explains in one
picture which norms saturate and which increase as $n$ grows.
  We then study diagonal linear networks (DLNs) trained by gradient descent. By
calibrating the initialization scale $\alpha$ to an effective
$p_{\mathrm{eff}}(\alpha)$ via the DLN separable potential, we show empirically
that DLNs inherit the same elbow/threshold laws, providing a predictive bridge
between explicit and implicit bias.
  Given that many generalization proxies depend on $\lVert \widehat {w_p}
\rVert_r$, our results suggest that their predictive power will depend
sensitively on which $l_r$ norm is used.

</details>


### [94] [Explicit and Effectively Symmetric Schemes for Neural SDEs](https://arxiv.org/abs/2509.20599)
*Daniil Shmelev,Cristopher Salvi*

Main category: cs.LG

TL;DR: 本文提出用于神经SDE的新型稳定、近似可逆的Runge - Kutta方案，解决传统方法局限，实验证明其优越稳定性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统通过神经SDE求解器进行反向传播的方法存在内存成本高、评估慢、梯度近似误差和不稳定等问题，需改进。

Method: 引入用于神经SDE的新型稳定、近似可逆的Runge - Kutta方案（EES方案）。

Result: 通过数值实验证明了方案具有优越的稳定性和可靠性。

Conclusion: EES方案可作为神经SDE可扩展和准确训练的实用基础。

Abstract: Backpropagation through (neural) SDE solvers is traditionally approached in
two ways: discretise-then-optimise, which offers accurate gradients but incurs
prohibitive memory costs due to storing the full computational graph (even when
mitigated by checkpointing); and optimise-then-discretise, which achieves
constant memory cost by solving an auxiliary backward SDE, but suffers from
slower evaluation and gradient approximation errors. Algebraically reversible
solvers promise both memory efficiency and gradient accuracy, yet existing
methods such as the Reversible Heun scheme are often unstable under complex
models and large step sizes. We address these limitations by introducing a
novel class of stable, near-reversible Runge--Kutta schemes for neural SDEs.
These Explicit and Effectively Symmetric (EES) schemes retain the benefits of
reversible solvers while overcoming their instability, enabling
memory-efficient training without severe restrictions on step size or model
complexity. Through numerical experiments, we demonstrate the superior
stability and reliability of our schemes, establishing them as a practical
foundation for scalable and accurate training of neural SDEs.

</details>


### [95] [No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks](https://arxiv.org/abs/2509.21296)
*Yehonatan Refael,Guy Smorodinsky,Ofir Lindenbaum,Itay Safran*

Main category: cs.LG

TL;DR: 本文分析现有训练集重构方法的弱点与局限，证明无先验知识时重构不可靠，还指出训练更充分的网络更不易受重构攻击。


<details>
  <summary>Details</summary>
Motivation: 现有训练集重构攻击的可靠性缺乏理论基础，需分析其弱点和局限。

Method: 理论证明无数据先验知识时存在无数替代解使重构不可靠，并用实验验证训练样本精确复制是偶然。

Result: 无先验知识时重构不可靠，训练更充分的网络不易受重构攻击。

Conclusion: 研究细化了训练集泄露可能性的理论理解，为缓解重构攻击提供新见解，调和了隐私与强泛化需求。

Abstract: The memorization of training data by neural networks raises pressing concerns
for privacy and security. Recent work has shown that, under certain conditions,
portions of the training set can be reconstructed directly from model
parameters. Some of these methods exploit implicit bias toward margin
maximization, suggesting that properties often regarded as beneficial for
generalization may actually compromise privacy. Yet despite striking empirical
demonstrations, the reliability of these attacks remains poorly understood and
lacks a solid theoretical foundation. In this work, we take a complementary
perspective: rather than designing stronger attacks, we analyze the inherent
weaknesses and limitations of existing reconstruction methods and identify
conditions under which they fail. We rigorously prove that, without
incorporating prior knowledge about the data, there exist infinitely many
alternative solutions that may lie arbitrarily far from the true training set,
rendering reconstruction fundamentally unreliable. Empirically, we further
demonstrate that exact duplication of training examples occurs only by chance.
Our results refine the theoretical understanding of when training set leakage
is possible and offer new insights into mitigating reconstruction attacks.
Remarkably, we demonstrate that networks trained more extensively, and
therefore satisfying implicit bias conditions more strongly -- are, in fact,
less susceptible to reconstruction attacks, reconciling privacy with the need
for strong generalization in this setting.

</details>


### [96] [Function Spaces Without Kernels: Learning Compact Hilbert Space Representations](https://arxiv.org/abs/2509.20605)
*Su Ann Low,Quentin Rommel,Kevin S. Miller,Adam J. Thorpe,Ufuk Topcu*

Main category: cs.LG

TL;DR: 本文探讨函数编码器与特征学习和核方法的联系，开发两种训练算法学习紧凑基，推导有限样本泛化界，并通过实验验证，为具有核级保证的神经预测器提供路径。


<details>
  <summary>Details</summary>
Motivation: 展示函数编码器与特征学习和核方法的原则性联系，实现对神经网络模型的核风格分析，开发有效训练算法并提供泛化保证。

Method: 通过定义核来建立联系，开发渐进式训练和训练后修剪两种算法，利用PCA原理揭示空间内在维度，使用Rademacher复杂度和PAC - Bayes技术推导泛化界。

Result: 在多项式基准和非线性动力系统实验中，用更少基函数达到相同精度。

Conclusion: 为具有核级保证的神经预测器提供路径，使模型高效且有原则地扩展。

Abstract: Function encoders are a recent technique that learn neural network basis
functions to form compact, adaptive representations of Hilbert spaces of
functions. We show that function encoders provide a principled connection to
feature learning and kernel methods by defining a kernel through an inner
product of the learned feature map. This kernel-theoretic perspective explains
their ability to scale independently of dataset size while adapting to the
intrinsic structure of data, and it enables kernel-style analysis of neural
models. Building on this foundation, we develop two training algorithms that
learn compact bases: a progressive training approach that constructively grows
bases, and a train-then-prune approach that offers a computationally efficient
alternative after training. Both approaches use principles from PCA to reveal
the intrinsic dimension of the learned space. In parallel, we derive
finite-sample generalization bounds using Rademacher complexity and PAC-Bayes
techniques, providing inference time guarantees. We validate our approach on a
polynomial benchmark with a known intrinsic dimension, and on nonlinear
dynamical systems including a Van der Pol oscillator and a two-body orbital
model, demonstrating that the same accuracy can be achieved with substantially
fewer basis functions. This work suggests a path toward neural predictors with
kernel-level guarantees, enabling adaptable models that are both efficient and
principled at scale.

</details>


### [97] [MMG: Mutual Information Estimation via the MMSE Gap in Diffusion](https://arxiv.org/abs/2509.20609)
*Longxuan Yu,Xing Shi,Xianghao Kong,Tong Jia,Greg Ver Steeg*

Main category: cs.LG

TL;DR: 提出用去噪扩散模型估计互信息，方法通过自洽性测试且表现优于传统方法，还利用自适应重要性采样实现可扩展估计。


<details>
  <summary>Details</summary>
Motivation: 互信息测量复杂系统具有挑战性，去噪扩散模型在密度估计表现出色，考虑用其改进互信息估计。

Method: 利用去噪扩散模型的信息论公式，互信息对应条件和无条件扩散的最小均方误差差距的一半在所有信噪比上的积分，还利用自适应重要性采样。

Result: 方法通过自洽性测试，优于传统和基于分数的扩散互信息估计器，在高互信息时也表现良好。

Conclusion: 去噪扩散模型可直接用于估计互信息，且能实现可扩展的互信息估计。

Abstract: Mutual information (MI) is one of the most general ways to measure
relationships between random variables, but estimating this quantity for
complex systems is challenging. Denoising diffusion models have recently set a
new bar for density estimation, so it is natural to consider whether these
methods could also be used to improve MI estimation. Using the recently
introduced information-theoretic formulation of denoising diffusion models, we
show the diffusion models can be used in a straightforward way to estimate MI.
In particular, the MI corresponds to half the gap in the Minimum Mean Square
Error (MMSE) between conditional and unconditional diffusion, integrated over
all Signal-to-Noise-Ratios (SNRs) in the noising process. Our approach not only
passes self-consistency tests but also outperforms traditional and score-based
diffusion MI estimators. Furthermore, our method leverages adaptive importance
sampling to achieve scalable MI estimation, while maintaining strong
performance even when the MI is high.

</details>


### [98] [Policy Compatible Skill Incremental Learning via Lazy Learning Interface](https://arxiv.org/abs/2509.20612)
*Daehee Lee,Dongsu Lee,TaeYoon Kwack,Wonje Choi,Honguk Woo*

Main category: cs.LG

TL;DR: 提出SIL - C框架解决技能增量学习中技能与策略兼容性问题，用双边懒学习映射技术，评估显示其能保持兼容性和学习效率。


<details>
  <summary>Details</summary>
Motivation: 技能增量学习中技能库演变会破坏与现有技能策略的兼容性，限制策略复用和泛化。

Method: 提出SIL - C框架，采用双边懒学习映射技术，根据轨迹分布相似度为子任务选择合适技能。

Result: 在不同技能增量学习场景中评估SIL - C，它能保持不断演变的技能与下游策略的兼容性，且确保学习过程的效率。

Conclusion: SIL - C框架有效解决技能增量学习中技能与策略的兼容性问题，无需策略重新训练或结构调整。

Abstract: Skill Incremental Learning (SIL) is the process by which an embodied agent
expands and refines its skill set over time by leveraging experience gained
through interaction with its environment or by the integration of additional
data. SIL facilitates efficient acquisition of hierarchical policies grounded
in reusable skills for downstream tasks. However, as the skill repertoire
evolves, it can disrupt compatibility with existing skill-based policies,
limiting their reusability and generalization. In this work, we propose SIL-C,
a novel framework that ensures skill-policy compatibility, allowing
improvements in incrementally learned skills to enhance the performance of
downstream policies without requiring policy re-training or structural
adaptation. SIL-C employs a bilateral lazy learning-based mapping technique to
dynamically align the subtask space referenced by policies with the skill space
decoded into agent behaviors. This enables each subtask, derived from the
policy's decomposition of a complex task, to be executed by selecting an
appropriate skill based on trajectory distribution similarity. We evaluate
SIL-C across diverse SIL scenarios and demonstrate that it maintains
compatibility between evolving skills and downstream policies while ensuring
efficiency throughout the learning process.

</details>


### [99] [Latent Twins](https://arxiv.org/abs/2509.20615)
*Matthias Chung,Deepanshu Verma,Max Collins,Amit N. Subrahmanya,Varuni Katti Sastry,Vishwas Rao*

Main category: cs.LG

TL;DR: 提出Latent Twins统一数学框架，建立潜在空间替代模型，在多种场景验证，能提供紧凑可解释替代，连接数据驱动与经典科学建模。


<details>
  <summary>Details</summary>
Motivation: 过去科学机器学习各领域进展多并行，表征学习和算法求解方法分离，需统一框架。

Method: 提出Latent Twins框架，在潜在空间为基础方程创建隐藏替代，建立其对ODE和PDE的基本逼近性质。

Result: 在典型ODE、浅水方程PDE基准和真实数据地电位再分析数据集上验证了框架。

Conclusion: Latent Twins框架提供可扩展、有理论基础的替代，能连接不同学科的数据驱动表征学习和经典科学建模。

Abstract: Over the past decade, scientific machine learning has transformed the
development of mathematical and computational frameworks for analyzing,
modeling, and predicting complex systems. From inverse problems to numerical
PDEs, dynamical systems, and model reduction, these advances have pushed the
boundaries of what can be simulated. Yet they have often progressed in
parallel, with representation learning and algorithmic solution methods
evolving largely as separate pipelines. With \emph{Latent Twins}, we propose a
unifying mathematical framework that creates a hidden surrogate in latent space
for the underlying equations. Whereas digital twins mirror physical systems in
the digital world, Latent Twins mirror mathematical systems in a learned latent
space governed by operators. Through this lens, classical modeling, inversion,
model reduction, and operator approximation all emerge as special cases of a
single principle. We establish the fundamental approximation properties of
Latent Twins for both ODEs and PDEs and demonstrate the framework across three
representative settings: (i) canonical ODEs, capturing diverse dynamical
regimes; (ii) a PDE benchmark using the shallow-water equations, contrasting
Latent Twin simulations with DeepONet and forecasts with a 4D-Var baseline; and
(iii) a challenging real-data geopotential reanalysis dataset, reconstructing
and forecasting from sparse, noisy observations. Latent Twins provide a
compact, interpretable surrogate for solution operators that evaluate across
arbitrary time gaps in a single-shot, while remaining compatible with
scientific pipelines such as assimilation, control, and uncertainty
quantification. Looking forward, this framework offers scalable,
theory-grounded surrogates that bridge data-driven representation learning and
classical scientific modeling across disciplines.

</details>


### [100] [Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning](https://arxiv.org/abs/2509.20616)
*Hanjiang Hu,Changliu Liu,Na Li,Yebin Wang*

Main category: cs.LG

TL;DR: 本文提出新方法将多轮任务规划转化为单轮任务推理问题，用GRPO优化策略，实验表明小参数模型性能优于大模型，且有强跨任务泛化性。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型代理进行复杂多轮任务规划面临稀疏奖励、信用分配和计算开销等挑战。

Method: 将多轮任务规划转化为单轮任务推理问题，通过Group Relative Policy Optimization (GRPO) 利用专家轨迹的密集可验证奖励进行高效策略优化。

Result: 1.5B参数模型在单轮GRPO训练下，长时规划任务成功率达70%，优于14B参数的基线模型。

Conclusion: 模型在复杂任务上训练后有强跨任务泛化性，能完成所有更简单的子任务。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
knowledge acquisition, reasoning, and tool use, making them promising
candidates for autonomous agent applications. However, training LLM agents for
complex multi-turn task planning faces significant challenges, including sparse
episode-wise rewards, credit assignment across long horizons, and the
computational overhead of reinforcement learning in multi-turn interaction
settings. To this end, this paper introduces a novel approach that transforms
multi-turn task planning into single-turn task reasoning problems, enabling
efficient policy optimization through Group Relative Policy Optimization (GRPO)
with dense and verifiable reward from expert trajectories. Our theoretical
analysis shows that GRPO improvement on single-turn task reasoning results in
higher multi-turn success probability under the minimal turns, as well as the
generalization to subtasks with shorter horizons. Experimental evaluation on
the complex task planning benchmark demonstrates that our 1.5B parameter model
trained with single-turn GRPO achieves superior performance compared to larger
baseline models up to 14B parameters, with success rates of 70% for
long-horizon planning tasks with over 30 steps. We also theoretically and
empirically validate the strong cross-task generalizability that the models
trained on complex tasks can lead to the successful completion of all simpler
subtasks.

</details>


### [101] [Personalized Federated Dictionary Learning for Modeling Heterogeneity in Multi-site fMRI Data](https://arxiv.org/abs/2509.20627)
*Yipu Zhang,Chengshuo Zhang,Ziyu Zhou,Gang Qu,Hao Zheng,Yuping Wang,Hui Shen,Hongwen Deng*

Main category: cs.LG

TL;DR: 提出PFedDL框架解决多站点fMRI数据隐私和非IID问题，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数据隐私限制和多站点fMRI研究中的非IID数据阻碍可泛化模型的发展。

Method: 提出PFedDL框架，在各站点进行独立字典学习，将字典分解为全局和局部组件，通过联邦聚合更新全局原子，独立优化局部原子。

Result: 在ABIDE数据集上的实验表明，PFedDL在非IID数据集上的准确性和鲁棒性优于现有方法。

Conclusion: PFedDL是一种有效的解决多站点fMRI数据隐私和非IID问题的方法。

Abstract: Data privacy constraints pose significant challenges for large-scale
neuroimaging analysis, especially in multi-site functional magnetic resonance
imaging (fMRI) studies, where site-specific heterogeneity leads to
non-independent and identically distributed (non-IID) data. These factors
hinder the development of generalizable models. To address these challenges, we
propose Personalized Federated Dictionary Learning (PFedDL), a novel federated
learning framework that enables collaborative modeling across sites without
sharing raw data. PFedDL performs independent dictionary learning at each site,
decomposing each site-specific dictionary into a shared global component and a
personalized local component. The global atoms are updated via federated
aggregation to promote cross-site consistency, while the local atoms are
refined independently to capture site-specific variability, thereby enhancing
downstream analysis. Experiments on the ABIDE dataset demonstrate that PFedDL
outperforms existing methods in accuracy and robustness across non-IID
datasets.

</details>


### [102] [Investigating Modality Contribution in Audio LLMs for Music](https://arxiv.org/abs/2509.20641)
*Giovana Morais,Magdalena Fuentes*

Main category: cs.LG

TL;DR: 本文通过量化各模态对音频大语言模型输出的贡献，研究其是真听音频还是仅用文本推理，发现高准确率模型更依赖文本，但音频未被完全忽略。


<details>
  <summary>Details</summary>
Motivation: 探究音频大语言模型是真的在听音频还是仅使用文本推理。

Method: 采用基于Shapley值的MM - SHAP框架，量化各模态对模型预测的相对贡献。

Result: 在MuChoMusic基准上评估两个模型，发现准确率高的模型更多依赖文本回答问题，虽整体音频贡献低，但模型能成功定位关键声音事件。

Conclusion: 本研究是MM - SHAP在音频大语言模型上的首次应用，有望为可解释AI和音频领域的未来研究奠定基础。

Abstract: Audio Large Language Models (Audio LLMs) enable human-like conversation about
music, yet it is unclear if they are truly listening to the audio or just using
textual reasoning, as recent benchmarks suggest. This paper investigates this
issue by quantifying the contribution of each modality to a model's output. We
adapt the MM-SHAP framework, a performance-agnostic score based on Shapley
values that quantifies the relative contribution of each modality to a model's
prediction. We evaluate two models on the MuChoMusic benchmark and find that
the model with higher accuracy relies more on text to answer questions, but
further inspection shows that even if the overall audio contribution is low,
models can successfully localize key sound events, suggesting that audio is not
entirely ignored. Our study is the first application of MM-SHAP to Audio LLMs
and we hope it will serve as a foundational step for future research in
explainable AI and audio.

</details>


### [103] [Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration](https://arxiv.org/abs/2509.20648)
*Yiyuan Pan,Zhe Liu,Hesheng Wang*

Main category: cs.LG

TL;DR: 提出CERMIC框架增强多智能体探索，在稀疏奖励环境中表现优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有好奇心机制在复杂多智能体强化学习中存在混淆环境随机性与有意义新颖性、统一新颖性偏差、忽视同伴行为新颖性等问题，导致探索效果不佳。

Method: 受人类儿童观察同伴调整探索行为启发，提出CERMIC框架，使智能体过滤噪声惊喜信号，根据多智能体上下文动态校准内在好奇心，生成理论上合理的内在奖励。

Result: 在VMAS、Meltingpot和SMACv2等基准套件上评估，CERMIC在稀疏奖励环境中的探索显著优于现有算法。

Conclusion: CERMIC框架能有效增强多智能体在稀疏奖励环境中的探索能力。

Abstract: Autonomous exploration in complex multi-agent reinforcement learning (MARL)
with sparse rewards critically depends on providing agents with effective
intrinsic motivation. While artificial curiosity offers a powerful
self-supervised signal, it often confuses environmental stochasticity with
meaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform
novelty bias, treating all unexpected observations equally. However, peer
behavior novelty, which encode latent task dynamics, are often overlooked,
resulting in suboptimal exploration in decentralized, communication-free MARL
settings. To this end, inspired by how human children adaptively calibrate
their own exploratory behaviors via observing peers, we propose a novel
approach to enhance multi-agent exploration. We introduce CERMIC, a principled
framework that empowers agents to robustly filter noisy surprise signals and
guide exploration by dynamically calibrating their intrinsic curiosity with
inferred multi-agent context. Additionally, CERMIC generates
theoretically-grounded intrinsic rewards, encouraging agents to explore state
transitions with high information gain. We evaluate CERMIC on benchmark suites
including VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that
exploration with CERMIC significantly outperforms SoTA algorithms in
sparse-reward environments.

</details>


### [104] [Theoretical Bounds for Stable In-Context Learning](https://arxiv.org/abs/2509.20677)
*Tongxi Wang,Zhuoyang Xia*

Main category: cs.LG

TL;DR: 本文建立了与ICL稳定性相关的非渐近下界，提出两阶段可观测估计器，实验验证理论与实际相符，提升大模型提示的可解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 上下文学习（ICL）灵活但可靠性对提示长度敏感，需解决该问题。

Method: 建立非渐近下界，给出基于协方差谱特性的充分条件；提出两阶段可观测估计器。

Result: 实验显示预测阈值与经验拐点接近，理论是保守可靠上界，校准变体缩小差距。

Conclusion: 将谱覆盖与稳定ICL联系起来，桥接理论与应用，提升大模型提示在有限样本场景的可解释性和可靠性。

Abstract: In-context learning (ICL) is flexible but its reliability is highly sensitive
to prompt length. This paper establishes a non-asymptotic lower bound that
links the minimal number of demonstrations to ICL stability under fixed
high-dimensional sub-Gaussian representations. The bound gives explicit
sufficient conditions in terms of spectral properties of the covariance,
providing a computable criterion for practice. Building on this analysis, we
propose a two-stage observable estimator with a one-shot calibration that
produces practitioner-ready prompt-length estimates without distributional
priors. Experiments across diverse datasets, encoders, and generators show
close alignment between the predicted thresholds and empirical knee-points,
with the theory acting as a conservative but reliable upper bound; the
calibrated variant further tightens this gap. These results connect spectral
coverage to stable ICL, bridge theory and deployment, and improve the
interpretability and reliability of large-scale prompting in realistic
finite-sample regimes.

</details>


### [105] [Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation](https://arxiv.org/abs/2509.20680)
*Wenkai Guo,Xuefeng Liu,Haolin Wang,Jianwei Niu,Shaojie Tang,Jing Yuan*

Main category: cs.LG

TL;DR: 文章探讨联合微调大语言模型时的隐私问题，实验表明攻击者能从全局模型提取训练数据，提出增强攻击策略，评估隐私保护技术。


<details>
  <summary>Details</summary>
Motivation: 组织不愿共享本地数据，联邦学习虽有潜力但隐私保护存疑，需研究其实际隐私风险。

Method: 进行大量实验，展示攻击者提取数据的情况，引入增强攻击策略，评估多种隐私保护技术。

Result: 攻击者能用简单生成方法从全局模型提取训练数据，模型越大泄漏越严重；增强攻击策略会加剧隐私泄漏。

Conclusion: 评估的隐私保护技术为使用联邦学习训练大语言模型降低隐私风险提供了有价值的见解和实用指南。

Abstract: Fine-tuning large language models (LLMs) with local data is a widely adopted
approach for organizations seeking to adapt LLMs to their specific domains.
Given the shared characteristics in data across different organizations, the
idea of collaboratively fine-tuning an LLM using data from multiple sources
presents an appealing opportunity. However, organizations are often reluctant
to share local data, making centralized fine-tuning impractical. Federated
learning (FL), a privacy-preserving framework, enables clients to retain local
data while sharing only model parameters for collaborative training, offering a
potential solution. While fine-tuning LLMs on centralized datasets risks data
leakage through next-token prediction, the iterative aggregation process in FL
results in a global model that encapsulates generalized knowledge, which some
believe protects client privacy. In this paper, however, we present
contradictory findings through extensive experiments. We show that attackers
can still extract training data from the global model, even using
straightforward generation methods, with leakage increasing as the model size
grows. Moreover, we introduce an enhanced attack strategy tailored to FL, which
tracks global model updates during training to intensify privacy leakage. To
mitigate these risks, we evaluate privacy-preserving techniques in FL,
including differential privacy, regularization-constrained updates and adopting
LLMs with safety alignment. Our results provide valuable insights and practical
guidelines for reducing privacy risks when training LLMs with FL.

</details>


### [106] [Learning to Align Molecules and Proteins: A Geometry-Aware Approach to Binding Affinity](https://arxiv.org/abs/2509.20693)
*Mohammadsaleh Refahi,Bahrad A. Sokhansanj,James R. Brown,Gail Rosen*

Main category: cs.LG

TL;DR: 提出轻量级框架FIRM - DTI用于药物 - 靶点结合亲和力预测，在TDC DTI - DG基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在药物 - 靶点结合亲和力预测中多采用简单拼接融合表示且缺乏几何正则化，泛化能力差。

Method: 通过FiLM层将分子嵌入以蛋白质嵌入为条件，用三元组损失强制度量结构，用基于嵌入距离的RBF回归头进行亲和力预测。

Result: 在TDC DTI - DG基准上取得SOTA性能，消融研究和域外评估验证效果。

Conclusion: 条件化和度量学习对鲁棒的药物 - 靶点亲和力预测有重要价值。

Abstract: Accurate prediction of drug-target binding affinity can accelerate drug
discovery by prioritizing promising compounds before costly wet-lab screening.
While deep learning has advanced this task, most models fuse ligand and protein
representations via simple concatenation and lack explicit geometric
regularization, resulting in poor generalization across chemical space and
time. We introduce FIRM-DTI, a lightweight framework that conditions molecular
embeddings on protein embeddings through a feature-wise linear modulation
(FiLM) layer and enforces metric structure with a triplet loss. An RBF
regression head operating on embedding distances yields smooth, interpretable
affinity predictions. Despite its modest size, FIRM-DTI achieves
state-of-the-art performance on the Therapeutics Data Commons DTI-DG benchmark,
as demonstrated by an extensive ablation study and out-of-domain evaluation.
Our results underscore the value of conditioning and metric learning for robust
drug-target affinity prediction.

</details>


### [107] [CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning](https://arxiv.org/abs/2509.20712)
*Zhenpeng Su,Leiyu Pan,Minxuan Lv,Yuntao Li,Wenping Hu,Fuzheng Zhang,Kun Gai,Guorui Zhou*

Main category: cs.LG

TL;DR: 本文提出CE - GPPO算法优化大语言模型处理复杂推理任务时的策略熵，实验表明其优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习优化大语言模型方法在处理策略熵时，因裁剪机制丢弃低概率令牌梯度信号，存在问题。

Method: 提出CE - GPPO算法，以温和有界方式重新引入裁剪令牌梯度，控制裁剪区间外令牌梯度大小。

Result: 理论和实验证明CE - GPPO能有效缓解熵不稳定问题，在数学推理基准测试中始终优于强基线。

Conclusion: CE - GPPO算法在优化大语言模型处理复杂推理任务时的策略熵方面表现良好。

Abstract: Reinforcement learning (RL) has become a powerful paradigm for optimizing
large language models (LLMs) to handle complex reasoning tasks. A core
challenge in this process lies in managing policy entropy, which reflects the
balance between exploration and exploitation during training. Existing methods,
such as proximal policy optimization (PPO) and its variants, discard valuable
gradient signals from low-probability tokens due to the clipping mechanism. We
systematically analyze the entropy dynamics and reveal that these clipped
tokens play a critical yet overlooked role in regulating entropy evolution. We
propose \textbf{C}ontrolling \textbf{E}ntropy via
\textbf{G}radient-\textbf{P}reserving \textbf{P}olicy \textbf{O}ptimization
(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in
native PPO in a gentle and bounded manner. By controlling the magnitude of
gradients from tokens outside the clipping interval, CE-GPPO is able to achieve
an exploration-exploitation trade-off. We provide theoretical justification and
empirical evidence showing that CE-GPPO effectively mitigates entropy
instability. Extensive experiments on mathematical reasoning benchmarks show
that CE-GPPO consistently outperforms strong baselines across different model
scales.

</details>


### [108] [A Genetic Algorithm for Navigating Synthesizable Molecular Spaces](https://arxiv.org/abs/2509.20719)
*Alston Lo,Connor W. Coley,Wojciech Matusik*

Main category: cs.LG

TL;DR: 提出SynGA遗传算法，可直接在合成路线上操作，在多种设计任务中有效，结合机器学习过滤器提升性能，有望作为独立基线或集成模块。


<details>
  <summary>Details</summary>
Motivation: 受遗传算法有效性和分子设计中可合成性重要性的启发。

Method: 提出SynGA算法，有自定义交叉和变异算子，通过修改适应度函数，还耦合机器学习过滤器，衍生出SynGBO。

Result: SynGA在多种设计任务中有效，结合过滤器后达到了先进性能。

Conclusion: SynGA轻量级且强制可合成性，可作为独立基线或集成到更大的合成感知工作流中。

Abstract: Inspired by the effectiveness of genetic algorithms and the importance of
synthesizability in molecular design, we present SynGA, a simple genetic
algorithm that operates directly over synthesis routes. Our method features
custom crossover and mutation operators that explicitly constrain it to
synthesizable molecular space. By modifying the fitness function, we
demonstrate the effectiveness of SynGA on a variety of design tasks, including
synthesizable analog search and sample-efficient property optimization, for
both 2D and 3D objectives. Furthermore, by coupling SynGA with a machine
learning-based filter that focuses the building block set, we boost SynGA to
state-of-the-art performance. For property optimization, this manifests as a
model-based variant SynGBO, which employs SynGA and block filtering in the
inner loop of Bayesian optimization. Since SynGA is lightweight and enforces
synthesizability by construction, our hope is that SynGA can not only serve as
a strong standalone baseline but also as a versatile module that can be
incorporated into larger synthesis-aware workflows in the future.

</details>


### [109] [The Impact of Audio Watermarking on Audio Anti-Spoofing Countermeasures](https://arxiv.org/abs/2509.20736)
*Zhenshan Zhang,Xueping Zhang,Yechen Wang,Liwei Jin,Ming Li*

Main category: cs.LG

TL;DR: 研究音频水印对反欺骗对策的影响，构建数据集实验发现水印降低反欺骗性能，提出KPWL框架应对，公开相关协议。


<details>
  <summary>Details</summary>
Motivation: 反欺骗系统对语音应用安全重要，但广泛使用的音频水印对其影响未充分研究。

Method: 构建Watermark - Spoofing数据集，应用多种水印方法到现有反欺骗数据集；提出Knowledge - Preserving Watermark Learning (KPWL)框架。

Result: 水印会降低反欺骗性能，水印密度越高，等错误率越高。

Conclusion: 音频水印是被忽视的领域偏移，建立了开发抗水印反欺骗系统的首个基准。

Abstract: This paper presents the first study on the impact of audio watermarking on
spoofing countermeasures. While anti-spoofing systems are essential for
securing speech-based applications, the influence of widely used audio
watermarking, originally designed for copyright protection, remains largely
unexplored. We construct watermark-augmented training and evaluation datasets,
named the Watermark-Spoofing dataset, by applying diverse handcrafted and
neural watermarking methods to existing anti-spoofing datasets. Experiments
show that watermarking consistently degrades anti-spoofing performance, with
higher watermark density correlating with higher Equal Error Rates (EERs). To
mitigate this, we propose the Knowledge-Preserving Watermark Learning (KPWL)
framework, enabling models to adapt to watermark-induced shifts while
preserving their original-domain spoofing detection capability. These findings
reveal audio watermarking as a previously overlooked domain shift and establish
the first benchmark for developing watermark-resilient anti-spoofing systems.
All related protocols are publicly available at
https://github.com/Alphawarheads/Watermark_Spoofing.git

</details>


### [110] [Measuring LLM Sensitivity in Transformer-based Tabular Data Synthesis](https://arxiv.org/abs/2509.20768)
*Maria F. Davila R,Azizjon Turaev,Wolfram Wingerath*

Main category: cs.LG

TL;DR: 研究对GReaT和REaLTabFormer两个工具进行超参数选择对合成数据质量和计算性能的敏感性评估，发现REaLTabFormer搭配轻量级大语言模型能平衡数据质量和计算需求，但效率提升有限。


<details>
  <summary>Details</summary>
Motivation: Transformer模型计算成本高，对终端用户可能不可行，需评估超参数选择对合成数据质量和计算性能的影响。

Method: 对GReaT和REaLTabFormer两个工具，评估10种不同架构类型和深度的模型设置，从运行时间、机器学习效用和与真实数据分布相似性三个维度评估敏感性，在四个真实数据集上实验。

Result: 运行时间与超参数数量成正比，GReaT运行时间通常低于REaLTabFormer；小数据集上两工具合成数据质量高，大数据集上仅REaLTabFormer能维持高质量。

Conclusion: REaLTabFormer搭配轻量级大语言模型能在保证数据质量同时降低计算需求，但运行时间仍高于GReaT等工具，效率提升有上限。

Abstract: Synthetic tabular data is used for privacy-preserving data sharing and
data-driven model development. Its effectiveness, however, depends heavily on
the used Tabular Data Synthesis (TDS) tool. Recent studies have shown that
Transformer-based models outperform other state-of-the-art models such as
Generative Adversarial Networks (GANs) and Diffusion models in terms of data
quality. However, Transformer-based models also come with high computational
costs, making them sometimes unfeasible for end users with prosumer hardware.
This study presents a sensitivity assessment on how the choice of
hyperparameters, such as number of layers or hidden dimension affects the
quality of the resultant synthetic data and the computational performance. It
is performed across two tools, GReaT and REaLTabFormer, evaluating 10 model
setups that vary in architecture type and depth. We assess the sensitivity on
three dimensions: runtime, machine learning (ML) utility, and similarity to
real data distributions. Experiments were conducted on four real-world
datasets. Our findings reveal that runtime is proportional to the number of
hyperparameters, with shallower configurations completing faster. GReaT
consistently achieves lower runtimes than REaLTabFormer, and only on the
largest dataset they have comparable runtime. For small datasets, both tools
achieve synthetic data with high utility and optimal similarity, but on larger
datasets only REaLTabFormer sustains strong utility and similarity. As a
result, REaLTabFormer with lightweight LLMs provides the best balance, since it
preserves data quality while reducing computational requirements. Nonetheless,
its runtime remains higher than that of GReaT and other TDS tools, suggesting
that efficiency gains are possible but only up to a certain level.

</details>


### [111] [IConv: Focusing on Local Variation with Channel Independent Convolution for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.20783)
*Gawon Lee,Hanbyeol Park,Minseop Kim,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: 本文针对MLP在处理时间序列数据的局限性，提出结合MLP和CNN的模型，并提出IConv卷积架构，实验证明其在多变量时间序列预测中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列数据有非平稳性，MLP在处理具有不同分布的通道时有局限性，会忽略局部变化，因此要解决其局限性。

Method: 结合MLP和CNN，用MLP建模整体趋势考虑长期依赖，CNN用不同内核结合MLP趋势预测建模细粒度局部模式；提出IConv卷积架构独立处理时间依赖通道并通过不同层考虑通道间关系。

Result: 通过在时间序列数据集上的大量实验，结果显示该方法在多变量时间序列预测方面具有优越性。

Conclusion: 所提出的结合MLP和CNN并采用IConv架构的方法适用于多变量时间序列预测，表现良好。

Abstract: Real-world time-series data often exhibit non-stationarity, including
changing trends, irregular seasonality, and residuals. In terms of changing
trends, recently proposed multi-layer perceptron (MLP)-based models have shown
excellent performance owing to their computational efficiency and ability to
capture long-term dependency. However, the linear nature of MLP architectures
poses limitations when applied to channels with diverse distributions,
resulting in local variations such as seasonal patterns and residual components
being ignored. However, convolutional neural networks (CNNs) can effectively
incorporate these variations. To resolve the limitations of MLP, we propose
combining them with CNNs. The overall trend is modeled using an MLP to consider
long-term dependencies. The CNN uses diverse kernels to model fine-grained
local patterns in conjunction with MLP trend predictions. To focus on modeling
local variation, we propose IConv, a novel convolutional architecture that
processes the temporal dependency channel independently and considers the
inter-channel relationship through distinct layers. Independent channel
processing enables the modeling of diverse local temporal dependencies and the
adoption of a large kernel size. Distinct inter-channel considerations reduce
computational cost. The proposed model is evaluated through extensive
experiments on time-series datasets. The results reveal the superiority of the
proposed method for multivariate time-series forecasting.

</details>


### [112] [LiLAW: Lightweight Learnable Adaptive Weighting to Meta-Learn Sample Difficulty and Improve Noisy Training](https://arxiv.org/abs/2509.20786)
*Abhishek Moturu,Anna Goldenberg,Babak Taati*

Main category: cs.LG

TL;DR: 提出LiLAW方法，动态调整训练样本损失权重，实验表明其能提升模型性能，增强泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决在有噪声标签和数据异质性情况下训练深度神经网络的挑战。

Method: 引入LiLAW方法，根据样本难度等级动态调整损失权重，用三个可学习参数，通过在验证集上的单小批量梯度下降步骤更新权重。

Result: 在多个通用和医学影像数据集等不同条件下实验，LiLAW能持续提升性能，在高噪声环境也有效。

Conclusion: LiLAW是计算高效的解决方案，能提升任意神经网络训练设置下的模型泛化性和鲁棒性。

Abstract: Training deep neural networks in the presence of noisy labels and data
heterogeneity is a major challenge. We introduce Lightweight Learnable Adaptive
Weighting (LiLAW), a novel method that dynamically adjusts the loss weight of
each training sample based on its evolving difficulty level, categorized as
easy, moderate, or hard. Using only three learnable parameters, LiLAW
adaptively prioritizes informative samples throughout training by updating
these weights using a single mini-batch gradient descent step on the validation
set after each training mini-batch, without requiring excessive hyperparameter
tuning or a clean validation set. Extensive experiments across multiple general
and medical imaging datasets, noise levels and types, loss functions, and
architectures with and without pretraining demonstrate that LiLAW consistently
enhances performance, even in high-noise environments. It is effective without
heavy reliance on data augmentation or advanced regularization, highlighting
its practicality. It offers a computationally efficient solution to boost model
generalization and robustness in any neural network training setup.

</details>


### [113] [Aligning Inductive Bias for Data-Efficient Generalization in State Space Models](https://arxiv.org/abs/2509.20789)
*Qiyu Chen,Guozhang Chen*

Main category: cs.LG

TL;DR: 大模型成功依赖缩放定律，但高质量数据有限，数据效率成挑战。本文提出解决方法，通过TDI使模型归纳偏置与任务频谱特征对齐，实验表明其提升泛化和样本效率。


<details>
  <summary>Details</summary>
Motivation: 大模型受高质量数据有限的挑战，数据效率成为建模前沿，基础序列模型固定偏置在任务结构不匹配时样本效率低，需解决该问题。

Method: 先通过SSM诱导核形式化线性时不变SSM的归纳偏置，证明其频谱由模型频率响应控制；再提出任务依赖初始化（TDI）方法，即功率谱匹配，在大规模训练前使模型归纳偏置与任务频谱特征对齐。

Result: 在多个真实世界基准测试中，TDI显著提高了泛化能力和样本效率，尤其在低数据场景下。

Conclusion: 本文为创建更具数据效率的模型提供了理论和实践工具，是迈向可持续扩展的关键一步。

Abstract: The remarkable success of large-scale models is fundamentally tied to scaling
laws, yet the finite nature of high-quality data presents a looming challenge.
One of the next frontiers in modeling is data efficiency: the ability to learn
more from less. A model's inductive bias is a critical lever for this, but
foundational sequence models like State Space Models (SSMs) rely on a fixed
bias. This fixed prior is sample-inefficient when a task's underlying structure
does not match. In this work, we introduce a principled framework to solve this
problem. We first formalize the inductive bias of linear time-invariant SSMs
through an SSM-induced kernel, mathematically and empirically proving its
spectrum is directly governed by the model's frequency response. Further, we
propose a method of Task-Dependent Initialization (TDI): power spectrum
matching, a fast and efficient method that aligns the model's inductive bias
with the task's spectral characteristics before large-scale training. Our
experiments on a diverse set of real-world benchmarks show that TDI
significantly improves generalization and sample efficiency, particularly in
low-data regimes. This work provides a theoretical and practical tool to create
more data-efficient models, a crucial step towards sustainable scaling.

</details>


### [114] [FERD: Fairness-Enhanced Data-Free Robustness Distillation](https://arxiv.org/abs/2509.20793)
*Zhengxiao Li,Liming Lu,Xu Zheng,Siyuan Liang,Zhenghan Chen,Yongbin Zhou,Shuchao Pang*

Main category: cs.LG

TL;DR: 提出公平增强的数据无关鲁棒性蒸馏框架FERD，调整对抗样本比例和分布，实验表明在鲁棒性和公平性上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有数据无关鲁棒性蒸馏方法忽视鲁棒公平性问题，导致不同类别鲁棒性差异大。

Method: 提出FERD框架，采用鲁棒性引导的类别重加权策略调整比例，生成公平感知样本和统一目标对抗样本调整分布。

Result: 在三个公开数据集上实验，FERD在所有对抗攻击下达到了最先进的最差类别鲁棒性，如在CIFAR - 10上使用MobileNet - V2时，FGSM和AutoAttack下最差类别鲁棒性分别提高15.1%和6.4%。

Conclusion: FERD在鲁棒性和公平性方面表现出色。

Abstract: Data-Free Robustness Distillation (DFRD) aims to transfer the robustness from
the teacher to the student without accessing the training data. While existing
methods focus on overall robustness, they overlook the robust fairness issues,
leading to severe disparity of robustness across different categories. In this
paper, we find two key problems: (1) student model distilled with equal class
proportion data behaves significantly different across distinct categories; and
(2) the robustness of student model is not stable across different attacks
target. To bridge these gaps, we present the first Fairness-Enhanced data-free
Robustness Distillation (FERD) framework to adjust the proportion and
distribution of adversarial examples. For the proportion, FERD adopts a
robustness-guided class reweighting strategy to synthesize more samples for the
less robust categories, thereby improving robustness of them. For the
distribution, FERD generates complementary data samples for advanced robustness
distillation. It generates Fairness-Aware Examples (FAEs) by enforcing a
uniformity constraint on feature-level predictions, which suppress the
dominance of class-specific non-robust features, providing a more balanced
representation across all categories. Then, FERD constructs Uniform-Target
Adversarial Examples (UTAEs) from FAEs by applying a uniform target class
constraint to avoid biased attack directions, which distribute the attack
targets across all categories and prevents overfitting to specific vulnerable
categories. Extensive experiments on three public datasets show that FERD
achieves state-of-the-art worst-class robustness under all adversarial attack
(e.g., the worst-class robustness under FGSM and AutoAttack are improved by
15.1\% and 6.4\% using MobileNet-V2 on CIFAR-10), demonstrating superior
performance in both robustness and fairness aspects.

</details>


### [115] [T2I-Diff: fMRI Signal Generation via Time-Frequency Image Transform and Classifier-Free Denoising Diffusion Models](https://arxiv.org/abs/2509.20822)
*Hwa Hui Tew,Junn Yong Loo,Yee-Fan Tan,Xinyu Tang,Hernando Ombao,Fuad Noman,Raphael C. -W. Phan,Chee-Ming Ting*

Main category: cs.LG

TL;DR: 介绍了T2I - Diff这个fMRI生成框架，利用BOLD信号时频表示和无分类器去噪扩散生成fMRI数据，在下游脑网络分类任务中验证了效率。


<details>
  <summary>Details</summary>
Motivation: fMRI数据采集资源密集限制高保真样本获取，现有生成模型因忽略复杂的非平稳性和非线性BOLD动力学而表现不佳。

Method: 先通过时变傅里叶变换将BOLD信号转换为加窗频谱图，再训练无分类器扩散模型生成类别条件频率频谱图，最后通过逆傅里叶变换转回BOLD信号。

Result: 在下游基于fMRI的脑网络分类中提高了准确性和泛化性。

Conclusion: T2I - Diff框架有效，能解决现有fMRI数据生成模型的问题。

Abstract: Functional Magnetic Resonance Imaging (fMRI) is an advanced neuroimaging
method that enables in-depth analysis of brain activity by measuring dynamic
changes in the blood oxygenation level-dependent (BOLD) signals. However, the
resource-intensive nature of fMRI data acquisition limits the availability of
high-fidelity samples required for data-driven brain analysis models. While
modern generative models can synthesize fMRI data, they often underperform
because they overlook the complex non-stationarity and nonlinear BOLD dynamics.
To address these challenges, we introduce T2I-Diff, an fMRI generation
framework that leverages time-frequency representation of BOLD signals and
classifier-free denoising diffusion. Specifically, our framework first converts
BOLD signals into windowed spectrograms via a time-dependent Fourier transform,
capturing both the underlying temporal dynamics and spectral evolution.
Subsequently, a classifier-free diffusion model is trained to generate
class-conditioned frequency spectrograms, which are then reverted to BOLD
signals via inverse Fourier transforms. Finally, we validate the efficacy of
our approach by demonstrating improved accuracy and generalization in
downstream fMRI-based brain network classification.

</details>


### [116] [CaTS-Bench: Can Language Models Describe Numeric Time Series?](https://arxiv.org/abs/2509.20823)
*Luca Zhou,Pratham Yashwante,Marshall Fisher,Alessio Sampieri,Zihao Zhou,Fabio Galasso,Rose Yu*

Main category: cs.LG

TL;DR: 介绍用于上下文感知时间序列字幕的大规模真实世界基准CaTS - Bench，包含数据、生成参考字幕的管道、相关问题及评估指标，并对VLMs进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列字幕基准常依赖合成数据、字幕简单，且忽视元数据和视觉表示，需要新基准。

Method: 从11个不同数据集构建字幕和问答任务，用可扩展管道生成参考字幕，提供人类修订子集，提出新评估指标。

Result: 建立CaTS - Bench基准及字幕管道，对VLMs进行基准测试，凸显其优缺点。

Conclusion: CaTS - Bench及其字幕管道为时间序列分析和基础模型交叉研究提供可靠且可扩展的基础。

Abstract: Time series captioning, the task of describing numeric time series in natural
language, requires numerical reasoning, trend interpretation, and contextual
understanding. Existing benchmarks, however, often rely on synthetic data or
overly simplistic captions, and typically neglect metadata and visual
representations. To close this gap, we introduce CaTS-Bench, the first
large-scale, real-world benchmark for Context-aware Time Series captioning.
CaTS-Bench is derived from 11 diverse datasets reframed as captioning and Q&A
tasks, comprising roughly 465k training and 105k test timestamps. Each sample
includes a numeric series segment, contextual metadata, a line-chart image, and
a caption. A key contribution of this work is the scalable pipeline used to
generate reference captions: while most references are produced by an oracle
LLM and verified through factual checks, human indistinguishability studies,
and diversity analyses, we also provide a human-revisited subset of 579 test
captions, refined from LLM outputs to ensure accuracy and human-like style.
Beyond captioning, CaTS-Bench offers 460 multiple-choice questions targeting
deeper aspects of time series reasoning. We further propose new tailored
evaluation metrics and benchmark leading VLMs, highlighting both their
strengths and persistent limitations. Together, these contributions establish
CaTS-Bench and its captioning pipeline as a reliable and extensible foundation
for future research at the intersection of time series analysis and foundation
models.

</details>


### [117] [Explaining Grokking and Information Bottleneck through Neural Collapse Emergence](https://arxiv.org/abs/2509.20829)
*Keitaro Sakamoto,Issei Sato*

Main category: cs.LG

TL;DR: 本文通过神经崩溃视角统一解释深度神经网络训练后期现象，分析其机制并在多数据集和架构上验证理论。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练动态常出人意料，如grokking和信息瓶颈原理，但对其机制及关系理解不足。

Method: 从神经崩溃视角解释后期现象，分析神经崩溃动态，将类内方差收缩与训练集神经崩溃度量关联。

Result: 发现类内方差收缩是grokking和信息瓶颈的关键因素，不同时间尺度导致后期现象行为。

Conclusion: 通过多数据集和架构验证理论发现，统一解释了深度神经网络训练后期现象。

Abstract: The training dynamics of deep neural networks often defy expectations, even
as these models form the foundation of modern machine learning. Two prominent
examples are grokking, where test performance improves abruptly long after the
training loss has plateaued, and the information bottleneck principle, where
models progressively discard input information irrelevant to the prediction
task as training proceeds. However, the mechanisms underlying these phenomena
and their relations remain poorly understood. In this work, we present a
unified explanation of such late-phase phenomena through the lens of neural
collapse, which characterizes the geometry of learned representations. We show
that the contraction of population within-class variance is a key factor
underlying both grokking and information bottleneck, and relate this measure to
the neural collapse measure defined on the training set. By analyzing the
dynamics of neural collapse, we show that distinct time scales between fitting
the training set and the progression of neural collapse account for the
behavior of the late-phase phenomena. Finally, we validate our theoretical
findings on multiple datasets and architectures.

</details>


### [118] [Shaping Initial State Prevents Modality Competition in Multi-modal Fusion: A Two-stage Scheduling Framework via Fast Partial Information Decomposition](https://arxiv.org/abs/2509.20840)
*Jiaqi Tang,Yinsong Xu,Yang Liu,Qingchao Chen*

Main category: cs.LG

TL;DR: 提出两阶段训练框架来解决多模态融合中模态竞争问题，引入有效竞争强度概念，开发诊断指标和异步训练控制器，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决多模态融合联合训练中模态竞争问题，现有方法多忽略模型初始状态的影响。

Method: 提出两阶段训练框架，引入ECS概念，证明MI可作为ECS代理，提出FastPID求解器，开发诊断指标和异步训练控制器。

Result: 在多个基准测试中取得了最优性能。

Conclusion: 塑造预融合模型的初始状态是缓解模态竞争、实现多模态融合协同的有效策略。

Abstract: Multi-modal fusion often suffers from modality competition during joint
training, where one modality dominates the learning process, leaving others
under-optimized. Overlooking the critical impact of the model's initial state,
most existing methods address this issue during the joint learning stage. In
this study, we introduce a two-stage training framework to shape the initial
states through unimodal training before the joint training. First, we propose
the concept of Effective Competitive Strength (ECS) to quantify a modality's
competitive strength. Our theoretical analysis further reveals that properly
shaping the initial ECS by unimodal training achieves a provably tighter error
bound. However, ECS is computationally intractable in deep neural networks. To
bridge this gap, we develop a framework comprising two core components: a
fine-grained computable diagnostic metric and an asynchronous training
controller. For the metric, we first prove that mutual information(MI) is a
principled proxy for ECS. Considering MI is induced by per-modality marginals
and thus treats each modality in isolation, we further propose FastPID, a
computationally efficient and differentiable solver for partial information
decomposition, which decomposes the joint distribution's information into
fine-grained measurements: modality-specific uniqueness, redundancy, and
synergy. Guided by these measurements, our asynchronous controller dynamically
balances modalities by monitoring uniqueness and locates the ideal initial
state to start joint training by tracking peak synergy. Experiments on diverse
benchmarks demonstrate that our method achieves state-of-the-art performance.
Our work establishes that shaping the pre-fusion models' initial state is a
powerful strategy that eases competition before it starts, reliably unlocking
synergistic multi-modal fusion.

</details>


### [119] [Robust Multi-Omics Integration from Incomplete Modalities Significantly Improves Prediction of Alzheimer's Disease](https://arxiv.org/abs/2509.20842)
*Sungjoon Park,Kyungwook Lee,Soorin Yim,Doyeong Hwang,Dongyun Kim,Soonyoung Lee,Amy Dunn,Daniel Gatti,Elissa Chesler,Kristen O'Connell,Kiyoung Kim*

Main category: cs.LG

TL;DR: 提出MOIRA方法处理多组学数据缺失模态问题，在AD数据集上表现优且揭示相关生物标志物。


<details>
  <summary>Details</summary>
Motivation: 多组学数据存在缺失模态问题，阻碍跨异构组学的整合分析。

Method: 提出MOIRA早期整合方法，通过表示对齐和自适应聚合，将各数据集投影到共享嵌入空间，用可学习加权机制融合。

Result: 在AD数据集上，MOIRA优于现有方法，消融研究确认模态贡献，特征重要性分析揭示相关生物标志物。

Conclusion: MOIRA方法能有效处理多组学数据缺失模态问题，具有生物学相关性。

Abstract: Multi-omics data capture complex biomolecular interactions and provide
insights into metabolism and disease. However, missing modalities hinder
integrative analysis across heterogeneous omics. To address this, we present
MOIRA (Multi-Omics Integration with Robustness to Absent modalities), an early
integration method enabling robust learning from incomplete omics data via
representation alignment and adaptive aggregation. MOIRA leverages all samples,
including those with missing modalities, by projecting each omics dataset onto
a shared embedding space where a learnable weighting mechanism fuses them.
Evaluated on the Religious Order Study and Memory and Aging Project (ROSMAP)
dataset for Alzheimer's Disease (AD), MOIRA outperformed existing approaches,
and further ablation studies confirmed modality-wise contributions. Feature
importance analysis revealed AD-related biomarkers consistent with prior
literature, highlighting the biological relevance of our approach.

</details>


### [120] [Causal Time Series Generation via Diffusion Models](https://arxiv.org/abs/2509.20846)
*Yutong Xia,Chang Xu,Yuxuan Liang,Qingsong Wen,Roger Zimmermann,Jiang Bian*

Main category: cs.LG

TL;DR: 提出因果时间序列生成任务家族，开发CaTSG框架，实验表明其效果优，开启新方向。


<details>
  <summary>Details</summary>
Motivation: 现有条件时间序列生成模型学习观测相关性，未考虑未观测混杂因素。

Method: 提出因果视角的条件时间序列生成任务家族，开发基于扩散的统一框架CaTSG，通过后门调整和特定程序推导因果得分函数。

Result: 在合成和真实数据集上实验显示，CaTSG保真度高，能处理现有基线无法处理的干预和反事实生成。

Conclusion: 提出因果时间序列生成家族并以CaTSG实现，为干预和反事实生成下更可靠模拟提供概念验证和新方向。

Abstract: Time series generation (TSG) synthesizes realistic sequences and has achieved
remarkable success. Among TSG, conditional models generate sequences given
observed covariates, however, such models learn observational correlations
without considering unobserved confounding. In this work, we propose a causal
perspective on conditional TSG and introduce causal time series generation as a
new TSG task family, formalized within Pearl's causal ladder, extending beyond
observational generation to include interventional and counterfactual settings.
To instantiate these tasks, we develop CaTSG, a unified diffusion-based
framework with backdoor-adjusted guidance that causally steers sampling toward
desired interventions and individual counterfactuals while preserving
observational fidelity. Specifically, our method derives causal score functions
via backdoor adjustment and the abduction-action-prediction procedure, thus
enabling principled support for all three levels of TSG. Extensive experiments
on both synthetic and real-world datasets show that CaTSG achieves superior
fidelity and also supporting interventional and counterfactual generation that
existing baselines cannot handle. Overall, we propose the causal TSG family and
instantiate it with CaTSG, providing an initial proof-of-concept and opening a
promising direction toward more reliable simulation under interventions and
counterfactual generation.

</details>


### [121] [Federated Markov Imputation: Privacy-Preserving Temporal Imputation in Multi-Centric ICU Environments](https://arxiv.org/abs/2509.20867)
*Christoph Düsing,Philipp Cimiano*

Main category: cs.LG

TL;DR: 提出Federated Markov Imputation (FMI)方法用于电子健康记录联合学习中的数据缺失问题，在脓毒症发作预测任务中表现优于本地插补基线。


<details>
  <summary>Details</summary>
Motivation: 解决电子健康记录联合学习中因机构采集时间序列数据的时间粒度不同导致的数据缺失问题。

Method: 提出隐私保护方法Federated Markov Imputation (FMI)，让重症监护室（ICUs）协作构建全局转移模型进行时间插补。

Result: 在真实世界脓毒症发作预测任务中，使用MIMIC - IV数据集评估，FMI优于本地插补基线，尤其在各ICU采样间隔不规则的场景中。

Conclusion: FMI是解决电子健康记录联合学习数据缺失问题的有效方法。

Abstract: Missing data is a persistent challenge in federated learning on electronic
health records, particularly when institutions collect time-series data at
varying temporal granularities. To address this, we propose Federated Markov
Imputation (FMI), a privacy-preserving method that enables Intensive Care Units
(ICUs) to collaboratively build global transition models for temporal
imputation. We evaluate FMI on a real-world sepsis onset prediction task using
the MIMIC-IV dataset and show that it outperforms local imputation baselines,
especially in scenarios with irregular sampling intervals across ICUs.

</details>


### [122] [StyleBench: Evaluating thinking styles in Large Language Models](https://arxiv.org/abs/2509.20868)
*Junyu Guo,Shangding Gu,Ming Jin,Costas Spanos,Javad Lavaei*

Main category: cs.LG

TL;DR: 提出StyleBench基准评估推理风格，发现无通用最优风格，策略有效性取决于模型规模和任务类型，开源基准。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理策略与模型架构、任务类型的相互作用尚不明确，需进行系统评估。

Method: 引入StyleBench基准，评估五种推理风格在五种推理任务上的表现，使用15个不同规模的开源模型。

Result: 无单一通用最优风格，搜索式方法在开放式问题上需大模型，简洁风格在定义明确任务上效率高，小模型常不遵循指令而猜测，推理鲁棒性与规模有关。

Conclusion: 研究为根据特定约束选择最优推理策略提供了重要参考，并开源了基准。

Abstract: The effectiveness of Large Language Models (LLMs) is heavily influenced by
the reasoning strategies, or styles of thought, employed in their prompts.
However, the interplay between these reasoning styles, model architecture, and
task type remains poorly understood. To address this, we introduce StyleBench,
a comprehensive benchmark for systematically evaluating reasoning styles across
diverse tasks and models. We assess five representative reasoning styles,
including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought
(AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning
tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral,
Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our
large-scale analysis reveals that no single style is universally optimal. We
demonstrate that strategy efficacy is highly contingent on both model scale and
task type: search-based methods (AoT, ToT) excel in open-ended problems but
require large-scale models, while concise styles (SoT, CoD) achieve radical
efficiency gains on well-defined tasks. Furthermore, we identify key behavioral
patterns: smaller models frequently fail to follow output instructions and
default to guessing, while reasoning robustness emerges as a function of scale.
Our findings offer a crucial roadmap for selecting optimal reasoning strategies
based on specific constraints, we open source the benchmark in
https://github.com/JamesJunyuGuo/Style_Bench.

</details>


### [123] [Model-Based Reinforcement Learning under Random Observation Delays](https://arxiv.org/abs/2509.20869)
*Armin Karamzade,Kyungmin Kim,JB Lanier,Davide Corsi,Roy Fox*

Main category: cs.LG

TL;DR: 研究POMDPs中随机传感器延迟问题，提出基于模型的过滤过程和延迟感知框架，实验显示该方法优于基线且对延迟分布变化有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实环境中存在延迟，但标准强化学习算法常假设能即时感知环境，且此前RL未解决观测可能乱序到达的随机传感器延迟问题。

Method: 提出基于模型的过滤过程，顺序更新信念状态；引入简单的延迟感知框架并融入基于模型的RL。

Result: 该方法始终优于为MDPs开发的延迟感知基线，在部署时对延迟分布变化有鲁棒性，在模拟机器人任务实验中也有良好表现。

Conclusion: 强调了显式建模观测延迟的重要性。

Abstract: Delays frequently occur in real-world environments, yet standard
reinforcement learning (RL) algorithms often assume instantaneous perception of
the environment. We study random sensor delays in POMDPs, where observations
may arrive out-of-sequence, a setting that has not been previously addressed in
RL. We analyze the structure of such delays and demonstrate that naive
approaches, such as stacking past observations, are insufficient for reliable
performance. To address this, we propose a model-based filtering process that
sequentially updates the belief state based on an incoming stream of
observations. We then introduce a simple delay-aware framework that
incorporates this idea into model-based RL, enabling agents to effectively
handle random delays. Applying this framework to Dreamer, we compare our
approach to delay-aware baselines developed for MDPs. Our method consistently
outperforms these baselines and demonstrates robustness to delay distribution
shifts during deployment. Additionally, we present experiments on simulated
robotic tasks, comparing our method to common practical heuristics and
emphasizing the importance of explicitly modeling observation delays.

</details>


### [124] [Distribution-Controlled Client Selection to Improve Federated Learning Strategies](https://arxiv.org/abs/2509.20877)
*Christoph Düsing,Philipp Cimiano*

Main category: cs.LG

TL;DR: 本文提出一种联邦学习策略扩展，通过选择活动客户端对齐标签分布，并在三个联邦学习策略和两个数据集上验证效果，不同分布对齐适用于不同失衡情况。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据不平衡会降低共享模型性能，现有研究提出改进策略，本文旨在进一步解决此问题。

Method: 提出扩展现有联邦学习策略，选择活动客户端使当前标签分布与平衡分布或联邦组合标签分布对齐。

Result: 在三个常见联邦学习策略和两个数据集上验证，面对局部失衡，与平衡分布对齐改进最大；面对全局失衡，与联邦组合标签分布对齐更优。

Conclusion: 不同的标签分布对齐方式对不同类型的数据失衡有更好的效果。

Abstract: Federated learning (FL) is a distributed learning paradigm that allows
multiple clients to jointly train a shared model while maintaining data
privacy. Despite its great potential for domains with strict data privacy
requirements, the presence of data imbalance among clients is a thread to the
success of FL, as it causes the performance of the shared model to decrease. To
address this, various studies have proposed enhancements to existing FL
strategies, particularly through client selection methods that mitigate the
detrimental effects of data imbalance. In this paper, we propose an extension
to existing FL strategies, which selects active clients that best align the
current label distribution with one of two target distributions, namely a
balanced distribution or the federations combined label distribution.
Subsequently, we empirically verify the improvements through our
distribution-controlled client selection on three common FL strategies and two
datasets. Our results show that while aligning the label distribution with a
balanced distribution yields the greatest improvements facing local imbalance,
alignment with the federation's combined label distribution is superior for
global imbalance.

</details>


### [125] [Improving Early Sepsis Onset Prediction Through Federated Learning](https://arxiv.org/abs/2509.20885)
*Christoph Düsing,Philipp Cimiano*

Main category: cs.LG

TL;DR: 提出用于脓毒症发作预测的联邦注意力增强LSTM模型，支持可变预测窗口，证明联邦学习有益且可变窗口优势大。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型因单个医院和ICU训练数据量和多样性有限导致脓毒症预测受限的问题，同时保护患者隐私。

Method: 提出基于多中心ICU数据训练的联邦、注意力增强LSTM模型，支持可变预测窗口，并进行深度时间分析。

Result: 联邦学习不仅提升整体预测性能，接近集中式模型，尤其利于早期脓毒症发作预测；可变预测窗口不显著影响性能，还降低开销。

Conclusion: 联邦学习对脓毒症预测有益，可变预测窗口有优势。

Abstract: Early and accurate prediction of sepsis onset remains a major challenge in
intensive care, where timely detection and subsequent intervention can
significantly improve patient outcomes. While machine learning models have
shown promise in this domain, their success is often limited by the amount and
diversity of training data available to individual hospitals and Intensive Care
Units (ICUs). Federated Learning (FL) addresses this issue by enabling
collaborative model training across institutions without requiring data
sharing, thus preserving patient privacy. In this work, we propose a federated,
attention-enhanced Long Short-Term Memory model for sepsis onset prediction,
trained on multi-centric ICU data. Unlike existing approaches that rely on
fixed prediction windows, our model supports variable prediction horizons,
enabling both short- and long-term forecasting in a single unified model.
During analysis, we put particular emphasis on the improvements through our
approach in terms of early sepsis detection, i.e., predictions with large
prediction windows by conducting an in-depth temporal analysis. Our results
prove that using FL does not merely improve overall prediction performance
(with performance approaching that of a centralized model), but is particularly
beneficial for early sepsis onset prediction. Finally, we show that our choice
of employing a variable prediction window rather than a fixed window does not
hurt performance significantly but reduces computational, communicational, and
organizational overhead.

</details>


### [126] [Deterministic Discrete Denoising](https://arxiv.org/abs/2509.20896)
*Hideyuki Suzuki,Hiroshi Yamashita*

Main category: cs.LG

TL;DR: 提出基于马尔可夫链的离散状态扩散模型确定性去噪算法，在文本和图像生成任务上提升效率和样本质量。


<details>
  <summary>Details</summary>
Motivation: 改进离散状态扩散模型的去噪过程，提升其在生成建模中的意义。

Method: 引入具有弱混沌动力学的放牧算法变体，实现确定性离散状态转换，替代随机去噪过程。

Result: 在文本和图像生成任务中，效率和样本质量均有持续提升。

Conclusion: 简单的去随机化方法能增强离散扩散在生成建模中的重要性，确定性反向过程在离散状态空间也有效。

Abstract: We propose a deterministic denoising algorithm for discrete-state diffusion
models based on Markov chains. The generative reverse process is derandomized
by introducing a variant of the herding algorithm with weakly chaotic dynamics,
which induces deterministic discrete state transitions. Our approach is a
direct replacement for the stochastic denoising process, requiring neither
retraining nor continuous state embeddings. We demonstrate consistent
improvements in both efficiency and sample quality on text and image generation
tasks. Thus, this simple derandomization approach is expected to enhance the
significance of discrete diffusion in generative modeling. Furthermore, our
results reveal that deterministic reverse processes, well established in
continuous diffusion, can also be effective in discrete state spaces.

</details>


### [127] [Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales](https://arxiv.org/abs/2509.20913)
*Ariadna Albors Zumel,Michele Tizzoni,Gian Maria Campedelli*

Main category: cs.LG

TL;DR: 本文开发深度学习框架，结合微观移动特征、历史犯罪和社会人口数据进行细粒度犯罪预测，发现结合多元数据可提升预测性能，深度学习模型有优势。


<details>
  <summary>Details</summary>
Motivation: 开发深度学习框架，探究结合微观移动特征等多元数据能否及如何提升细粒度犯罪预测性能。

Method: 聚焦美国四个城市，收集2019 - 2023年犯罪、社会人口和人类移动数据，聚合到网格中，训练ConvLSTM网络预测12小时后的犯罪发生情况，并与三个基线模型对比。

Result: 结合移动特征可提升预测性能，结合移动和社会人口特征效果最佳，深度学习模型表现优于其他方法，不同长度输入序列对不同类型犯罪预测效果有差异。

Conclusion: 强调多元数据源整合对时空犯罪预测的重要性，以及深度学习在细粒度尺度上的优缺点。

Abstract: Objectives: To develop a deep learning framework to evaluate if and how
incorporating micro-level mobility features, alongside historical crime and
sociodemographic data, enhances predictive performance in crime forecasting at
fine-grained spatial and temporal resolutions.
  Methods: We advance the literature on computational methods and crime
forecasting by focusing on four U.S. cities (i.e., Baltimore, Chicago, Los
Angeles, and Philadelphia). We employ crime incident data obtained from each
city's police department, combined with sociodemographic data from the American
Community Survey and human mobility data from Advan, collected from 2019 to
2023. This data is aggregated into grids with equally sized cells of 0.077 sq.
miles (0.2 sq. kms) and used to train our deep learning forecasting model, a
Convolutional Long Short-Term Memory (ConvLSTM) network, which predicts crime
occurrences 12 hours ahead using 14-day and 2-day input sequences. We also
compare its performance against three baseline models: logistic regression,
random forest, and standard LSTM.
  Results: Incorporating mobility features improves predictive performance,
especially when using shorter input sequences. Noteworthy, however, the best
results are obtained when both mobility and sociodemographic features are used
together, with our deep learning model achieving the highest recall, precision,
and F1 score in all four cities, outperforming alternative methods. With this
configuration, longer input sequences enhance predictions for violent crimes,
while shorter sequences are more effective for property crimes.
  Conclusion: These findings underscore the importance of integrating diverse
data sources for spatiotemporal crime forecasting, mobility included. They also
highlight the advantages (and limits) of deep learning when dealing with
fine-grained spatial and temporal scales.

</details>


### [128] [Energy saving in off-road vehicles using leakage compensation technique](https://arxiv.org/abs/2509.20926)
*Gyan Wrat,J. Das*

Main category: cs.LG

TL;DR: 文章对比两种液压回路提高重型土方设备线性执行器能效，含控制方法及仿真实验，新回路能效高。


<details>
  <summary>Details</summary>
Motivation: 提高重型土方设备线性执行器的能源效率，降低环境影响和运营成本。

Method: 对比使用传统比例方向控制阀（PDCV）和创新的比例流量控制阀（PFCV）的液压回路，用模糊控制器调整的PID控制器实现位置控制，用MATLAB/Simulink进行仿真。

Result: 使用PFCV的液压回路比使用PDCV的传统回路能源效率高8.5%，仿真结果与实验结果进行了对比。

Conclusion: 所提出的方法可显著提高重型土方设备线性执行器的能源效率。

Abstract: The article focuses on enhancing the energy efficiency of linear actuators
used in heavy earth moving equipment, particularly in the booms ofexcavation
equipment. Two hydraulic circuits are compared in terms of energy efficiency,
with one using a conventional proportional directionalcontrol valve (PDCV) and
the other using an innovative solution of proportional flow control valve
(PFCV) with artificial leakage between thetwo ends of the actuator. The PFCV
reduces energy loss in the form of heat by bypassing the extra flow from the
pump during position control,unlike the PDCV that uses a pressure relief valve.
The hydraulic circuit using PFCV is found to be 8.5% more energy efficient than
theconventional circuit using PDCV. The article also discusses the position
control of the actuator, which is achieved using a PID controller tuned by a
fuzzy controller. Thesimulation of the hydraulic circuit is carried out using
MATLAB/Simulink, and the results are compared with experiments. Overall, the
proposedapproach could lead to significant improvements in the energy
efficiency of linear actuators used in heavy earth moving equipment,
therebyreducing their environmental impact and operating costs.

</details>


### [129] [GenFacts-Generative Counterfactual Explanations for Multi-Variate Time Series](https://arxiv.org/abs/2509.20936)
*Sarah Seifi,Anass Ibrahimi,Tobias Sukianto,Cecilia Carbonelli,Lorenzo Servadei,Robert Wille*

Main category: cs.LG

TL;DR: 提出GenFacts框架生成多元时间序列反事实解释，在两个数据集上表现优于基线，强调合理性和以用户为中心的可解释性重要性。


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列反事实解释方法生成的反事实往往无效、不合理或不直观，需要改进。

Method: 引入基于类判别变分自编码器的GenFacts框架，集成对比和分类一致性目标、基于原型的初始化和现实约束优化。

Result: 在雷达手势数据和手写字母轨迹数据集上，GenFacts在合理性上比现有基线高18.7%，在人类研究中可解释性得分最高。

Conclusion: 合理性和以用户为中心的可解释性是时间序列数据中可操作反事实的关键，而非仅关注稀疏性。

Abstract: Counterfactual explanations aim to enhance model transparency by showing how
inputs can be minimally altered to change predictions. For multivariate time
series, existing methods often generate counterfactuals that are invalid,
implausible, or unintuitive. We introduce GenFacts, a generative framework
based on a class-discriminative variational autoencoder. It integrates
contrastive and classification-consistency objectives, prototype-based
initialization, and realism-constrained optimization. We evaluate GenFacts on
radar gesture data as an industrial use case and handwritten letter
trajectories as an intuitive benchmark. Across both datasets, GenFacts
outperforms state-of-the-art baselines in plausibility (+18.7%) and achieves
the highest interpretability scores in a human study. These results highlight
that plausibility and user-centered interpretability, rather than sparsity
alone, are key to actionable counterfactuals in time series data.

</details>


### [130] [Why Attention Fails: The Degeneration of Transformers into MLPs in Time Series Forecasting](https://arxiv.org/abs/2509.20942)
*Zida Liang,Jiayi Zhu,Weiqiang Sun*

Main category: cs.LG

TL;DR: 研究Transformer在时间序列预测中表现不佳的原因，发现现有时间序列Transformer中Transformer块常退化为简单MLP，注意力机制未按预期工作，且当前嵌入方法有问题。


<details>
  <summary>Details</summary>
Motivation: Transformer在时间序列预测中未展现明显优势且多数研究未深入探究失败原因，为更好理解时间序列Transformer而开展研究。

Method: 设计一系列实验，将Transformer逐步修改为MLP研究注意力机制影响；设计可解释数据集探究注意力机制失效原因；进行理论分析。

Result: 现有时间序列Transformer中Transformer块常退化为简单MLP，注意力机制未按预期工作，当前嵌入方法无法使Transformer在结构良好的潜在空间中运行。

Conclusion: 揭示了Transformer在时间序列预测中表现不佳的原因，分析了嵌入失败的深层潜在原因。

Abstract: Transformer-based architectures achieved high performance in natural language
processing and computer vision, yet many studies have shown that they have not
demonstrated a clear advantage in time series forecasting and even underperform
simple linear baselines in some cases. However, most of these studies have not
thoroughly explored the reasons behind the failure of transformers. To better
understand time-series transformers(TST), we designed a series of experiments,
progressively modifying transformers into MLPs to investigate the impact of the
attention mechanism. Surprisingly, transformer blocks often degenerate into
simple MLPs in existing time-series transformers. We designed a interpretable
dataset to investigate the reasons behind the failure of the attention
mechanism and revealed that the attention mechanism is not working in the
expected way. We theoretically analyzed the reasons behind this phenomenon,
demonstrating that the current embedding methods fail to allow transformers to
function in a well-structured latent space, and further analyzed the deeper
underlying causes of the failure of embedding.

</details>


### [131] [Decoupled-Value Attention for Prior-Data Fitted Networks: GP Inference for Physical Equations](https://arxiv.org/abs/2509.20950)
*Kaustubh Sharma,Simardeep Singh,Parikshit Pareek*

Main category: cs.LG

TL;DR: 提出解耦值注意力（DVA）改进先验数据拟合网络（PFN），在高维回归任务上表现良好，速度比精确高斯过程推理快80倍以上。


<details>
  <summary>Details</summary>
Motivation: PFN在高维回归任务上使用标准Transformer注意力效果有限，需改进。

Method: 引入解耦值注意力（DVA），仅从输入计算相似度，通过值传播标签。

Result: 局部注意力降低PFN样本外验证损失，注意力比骨干架构选择更关键，CNN - 基于的PFN与Transformer - 基于的表现相当，64维潮流方程近似平均绝对误差达1E - 3，速度快80倍以上。

Conclusion: PFN扩展的关键因素是注意力规则而非架构本身，DVA改进的PFN在高维回归任务上有效。

Abstract: Prior-data fitted networks (PFNs) are a promising alternative to
time-consuming Gaussian Process (GP) inference for creating fast surrogates of
physical systems. PFN reduces the computational burden of GP-training by
replacing Bayesian inference in GP with a single forward pass of a learned
prediction model. However, with standard Transformer attention, PFNs show
limited effectiveness on high-dimensional regression tasks. We introduce
Decoupled-Value Attention (DVA)-- motivated by the GP property that the
function space is fully characterized by the kernel over inputs and the
predictive mean is a weighted sum of training targets. DVA computes
similarities from inputs only and propagates labels solely through values.
Thus, the proposed DVA mirrors the Gaussian-process update while remaining
kernel-free. We demonstrate that the crucial factor for scaling PFNs is the
attention rule rather than the architecture itself. Specifically, our results
demonstrate that (a) localized attention consistently reduces out-of-sample
validation loss in PFNs across different dimensional settings, with validation
loss reduced by more than 50% in five- and ten-dimensional cases, and (b) the
role of attention is more decisive than the choice of backbone architecture,
showing that CNN-based PFNs can perform at par with their Transformer-based
counterparts. The proposed PFNs provide 64-dimensional power flow equation
approximations with a mean absolute error of the order of 1E-3, while being
over 80x faster than exact GP inference.

</details>


### [132] [Flow Matching in the Low-Noise Regime: Pathologies and a Contrastive Remedy](https://arxiv.org/abs/2509.20952)
*Weili Zeng,Yichao Yan*

Main category: cs.LG

TL;DR: 指出流匹配在低噪声状态有根本不稳定性，提出LCF协议解决问题，提升收敛速度和表征质量。


<details>
  <summary>Details</summary>
Motivation: 流匹配框架在低噪声状态存在根本不稳定性，影响优化和语义表征。

Method: 提出Local Contrastive Flow (LCF) 混合训练协议，在小噪声水平用对比特征对齐替代直接速度回归。

Result: LCF不仅提高了收敛速度，还稳定了表征质量。

Conclusion: 解决低噪声病态问题对释放流匹配在生成和表征学习方面的潜力至关重要。

Abstract: Flow matching has recently emerged as a powerful alternative to diffusion
models, providing a continuous-time formulation for generative modeling and
representation learning. Yet, we show that this framework suffers from a
fundamental instability in the low-noise regime. As noise levels approach zero,
arbitrarily small perturbations in the input can induce large variations in the
velocity target, causing the condition number of the learning problem to
diverge. This ill-conditioning not only slows optimization but also forces the
encoder to reallocate its limited Jacobian capacity toward noise directions,
thereby degrading semantic representations. We provide the first theoretical
analysis of this phenomenon, which we term the low-noise pathology,
establishing its intrinsic link to the structure of the flow matching
objective. Building on these insights, we propose Local Contrastive Flow (LCF),
a hybrid training protocol that replaces direct velocity regression with
contrastive feature alignment at small noise levels, while retaining standard
flow matching at moderate and high noise. Empirically, LCF not only improves
convergence speed but also stabilizes representation quality. Our findings
highlight the critical importance of addressing low-noise pathologies to unlock
the full potential of flow matching for both generation and representation
learning.

</details>


### [133] [Alignment Unlocks Complementarity: A Framework for Multiview Circuit Representation Learning](https://arxiv.org/abs/2509.20968)
*Zhengyuan Shi,Jingxin Wang,Wentao Jiang,Chengyu Ma,Ziyang Zheng,Zhufei Chu,Weikang Qian,Qiang Xu*

Main category: cs.LG

TL;DR: 多视图学习在布尔电路中有潜力，但视图间结构异质性阻碍有效融合，提出MixGate框架先进行功能对齐，再引入多视图掩码建模目标，实验证明该策略有效。


<details>
  <summary>Details</summary>
Motivation: 不同图表示的布尔电路多视图学习有潜力，但视图间结构异质性阻碍有效融合，尤其是自监督技术。

Method: 引入MixGate框架，基于原则性训练课程，先通过等价对齐损失让模型学习共享的、函数感知的表示空间，再引入多视图掩码建模目标。

Result: 广泛实验包括关键消融研究表明，先对齐策略将掩码建模从无效技术转变为强大的性能驱动因素。

Conclusion: 功能对齐是解锁多视图自监督能力的必要前提，MixGate框架的先对齐策略有效。

Abstract: Multiview learning on Boolean circuits holds immense promise, as different
graph-based representations offer complementary structural and semantic
information. However, the vast structural heterogeneity between views, such as
an And-Inverter Graph (AIG) versus an XOR-Majority Graph (XMG), poses a
critical barrier to effective fusion, especially for self-supervised techniques
like masked modeling. Naively applying such methods fails, as the cross-view
context is perceived as noise. Our key insight is that functional alignment is
a necessary precondition to unlock the power of multiview self-supervision. We
introduce MixGate, a framework built on a principled training curriculum that
first teaches the model a shared, function-aware representation space via an
Equivalence Alignment Loss. Only then do we introduce a multiview masked
modeling objective, which can now leverage the aligned views as a rich,
complementary signal. Extensive experiments, including a crucial ablation
study, demonstrate that our alignment-first strategy transforms masked modeling
from an ineffective technique into a powerful performance driver.

</details>


### [134] [Knowledgeable Language Models as Black-Box Optimizers for Personalized Medicine](https://arxiv.org/abs/2509.20975)
*Michael S. Yao,Osbert Bastani,Alma Andersson,Tommaso Biancalani,Aïcha Bentaieb,Claudia Iriondo*

Main category: cs.LG

TL;DR: 提出LEON方法利用大语言模型和领域知识为患者提出个性化治疗方案，实验显示其表现优于传统和基于大语言模型的方法。


<details>
  <summary>Details</summary>
Motivation: 个性化医疗需依据患者因素优化治疗方案，但现有替代模型难以泛化，因此考虑利用领域先验知识。

Method: 引入基于大语言模型的带知识先验的熵引导优化方法（LEON），通过“提示优化”实现，将大语言模型作为随机引擎来提出治疗设计。

Result: 在实际优化任务实验中，LEON在为患者提出个性化治疗方案方面优于传统和基于大语言模型的方法。

Conclusion: LEON是一种有效的利用大语言模型和领域知识为患者提供个性化治疗方案的方法。

Abstract: The goal of personalized medicine is to discover a treatment regimen that
optimizes a patient's clinical outcome based on their personal genetic and
environmental factors. However, candidate treatments cannot be arbitrarily
administered to the patient to assess their efficacy; we often instead have
access to an in silico surrogate model that approximates the true fitness of a
proposed treatment. Unfortunately, such surrogate models have been shown to
fail to generalize to previously unseen patient-treatment combinations. We
hypothesize that domain-specific prior knowledge - such as medical textbooks
and biomedical knowledge graphs - can provide a meaningful alternative signal
of the fitness of proposed treatments. To this end, we introduce LLM-based
Entropy-guided Optimization with kNowledgeable priors (LEON), a mathematically
principled approach to leverage large language models (LLMs) as black-box
optimizers without any task-specific fine-tuning, taking advantage of their
ability to contextualize unstructured domain knowledge to propose personalized
treatment plans in natural language. In practice, we implement LEON via
'optimization by prompting,' which uses LLMs as stochastic engines for
proposing treatment designs. Experiments on real-world optimization tasks show
LEON outperforms both traditional and LLM-based methods in proposing
individualized treatments for patients.

</details>


### [135] [CLUE: Conflict-guided Localization for LLM Unlearning Framework](https://arxiv.org/abs/2509.20977)
*Hang Chen,Jiaying Zhu,Xinyu Yang,Wenya Wang*

Main category: cs.LG

TL;DR: 提出CLUE框架解决现有LLM去学习方法的不足，实验表明其有更好效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于定位的LLM去学习方法无法区分需遗忘和保留的神经元，可能导致过度遗忘或目标知识擦除不完全。

Method: 运用电路发现技术，提出CLUE框架，识别遗忘和保留电路并转化为合取范式，根据范式可满足性解确定神经元类别，提供针对性微调策略。

Result: 与现有定位方法相比，CLUE通过精确的神经定位实现了更好的遗忘效果和保留实用性。

Conclusion: CLUE框架能有效解决现有LLM去学习方法的问题，在遗忘和保留方面表现更优。

Abstract: The LLM unlearning aims to eliminate the influence of undesirable data
without affecting causally unrelated information. This process typically
involves using a forget set to remove target information, alongside a retain
set to maintain non-target capabilities. While recent localization-based
methods demonstrate promise in identifying important neurons to be unlearned,
they fail to disentangle neurons responsible for forgetting undesirable
knowledge or retaining essential skills, often treating them as a single
entangled group. As a result, these methods apply uniform interventions,
risking catastrophic over-forgetting or incomplete erasure of the target
knowledge. To address this, we turn to circuit discovery, a mechanistic
interpretability technique, and propose the Conflict-guided Localization for
LLM Unlearning framEwork (CLUE). This framework identifies the forget and
retain circuit composed of important neurons, and then the circuits are
transformed into conjunctive normal forms (CNF). The assignment of each neuron
in the CNF satisfiability solution reveals whether it should be forgotten or
retained. We then provide targeted fine-tuning strategies for different
categories of neurons. Extensive experiments demonstrate that, compared to
existing localization methods, CLUE achieves superior forget efficacy and
retain utility through precise neural localization.

</details>


### [136] [FracAug: Fractional Augmentation boost Graph-level Anomaly Detection under Limited Supervision](https://arxiv.org/abs/2509.20978)
*Xiangyu Dong,Xingyi Zhang,Sibo Wang*

Main category: cs.LG

TL;DR: 提出FracAug增强框架解决GAD中GNN的高标注成本和数据不平衡问题，在多数据集和模型上实验有效。


<details>
  <summary>Details</summary>
Motivation: 解决图级异常检测（GAD）中，图神经网络（GNN）面临的高标注成本和数据集不平衡问题。

Method: 提出FracAug增强框架，学习图语义合成分数变体，用加权距离感知边际损失指导，利用原始和增强图预测为未标注数据伪标签，迭代扩大训练集。

Result: 在12个真实数据集的14种GNN上实验，平均AUROC、AUPRC和F1分数分别提升5.72%、7.23%和4.18%。

Conclusion: FracAug作为与多种GNN兼容的模型无关模块，具有显著的通用性和有效性。

Abstract: Graph-level anomaly detection (GAD) is critical in diverse domains such as
drug discovery, yet high labeling costs and dataset imbalance hamper the
performance of Graph Neural Networks (GNNs). To address these issues, we
propose FracAug, an innovative plug-in augmentation framework that enhances
GNNs by generating semantically consistent graph variants and pseudo-labeling
with mutual verification. Unlike previous heuristic methods, FracAug learns
semantics within given graphs and synthesizes fractional variants, guided by a
novel weighted distance-aware margin loss. This captures multi-scale topology
to generate diverse, semantic-preserving graphs unaffected by data imbalance.
Then, FracAug utilizes predictions from both original and augmented graphs to
pseudo-label unlabeled data, iteratively expanding the training set. As a
model-agnostic module compatible with various GNNs, FracAug demonstrates
remarkable universality and efficacy: experiments across 14 GNNs on 12
real-world datasets show consistent gains, boosting average AUROC, AUPRC, and
F1-score by up to 5.72%, 7.23%, and 4.18%, respectively.

</details>


### [137] [Toward Robust and Efficient ML-Based GPU Caching for Modern Inference](https://arxiv.org/abs/2509.20979)
*Peng Chen,Jiaji Zhang,Hailiang Zhao,Yirong Zhang,Jiahong Yu,Xueyan Tang,Yixuan Wang,Hao Li,Jianping Zou,Gang Xiong,Kingsum Chow,Shuibing He,Shuiguang Deng*

Main category: cs.LG

TL;DR: 现代GPU推理中缓存效率是瓶颈，提出LCR框架解决问题，实验显示其有性能提升和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现代GPU推理中缓存效率低，启发式策略表现不佳，学习型方法有局限性，需提升缓存性能和鲁棒性。

Method: 提出LCR框架，核心算法LARU结合机器学习预测增强LRU，通过在线误差估计动态适应预测准确性。

Result: 在DLRM和LLM场景中，LCR提升吞吐量达24.2%，降低P99 TTFT达28.3%，表现优于常用推理系统，预测不佳时性能稳定。

Conclusion: LCR框架解决了学习型缓存中经验进展和理论进步间的差距，实现性能提升和鲁棒性。

Abstract: In modern GPU inference, cache efficiency remains a major bottleneck. In
recommendation models, embedding hit rates largely determine throughput, while
in large language models, KV-cache misses substantially increase
time-to-first-token (TTFT). Heuristic policies such as \textsc{LRU} often
struggle under structured access patterns. Learning-based approaches are
promising, but in practice face two major limitations: they degrade sharply
when predictions are inaccurate, or they gain little even with accurate
predictions due to conservative designs. Some also incur high overhead, further
limiting practicality.
  We present \textsc{LCR}, a practical framework for learning-based GPU caching
that delivers performance gains while ensuring robustness and efficiency. Its
core algorithm, \textsc{LARU}, enhances \textsc{LRU} with machine-learned
predictions and dynamically adapts to prediction accuracy through online error
estimation. When predictions are accurate, \textsc{LARU} achieves near-optimal
performance. With inaccurate predictions, it degrades gracefully to
near-\textsc{LRU} performance. With \textsc{LCR}, we bridge the gap between
empirical progress and theoretical advances in learning-based caching.
  Experiments show that \textsc{LCR} delivers consistent gains under realistic
conditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\%
and reduces P99 TTFT by up to 28.3\%, outperforming widely used inference
systems. Even under poor predictions, its performance remains stable,
demonstrating practical robustness.

</details>


### [138] [Binary Autoencoder for Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2509.20997)
*Hakaze Cho,Haolin Yang,Brian M. Kurkoski,Naoya Inoue*

Main category: cs.LG

TL;DR: 现有工作在解释大语言模型机制时存在特征稀疏性问题，本文提出二进制自编码器BAE，能促进特征独立和稀疏，有特征集熵计算和特征解缠两个应用，且表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有工作依赖自编码器对单个训练实例进行隐式正则化，缺乏全局稀疏性保证，导致特征稀疏性和原子化受影响。

Method: 提出一种对隐藏激活的小批量施加最小熵的自编码器变体BAE，将隐藏激活离散化为1位，应用梯度估计实现反向传播。

Result: 能可靠估计二进制隐藏激活的熵，可用于表征大语言模型推理动态；作为特征提取器，避免了密集特征，产生可解释特征数量最多。

Conclusion: BAE作为特征提取器是有效的，能解决现有工作在特征稀疏性和原子化方面的问题。

Abstract: Existing works are dedicated to untangling atomized numerical components
(features) from the hidden states of Large Language Models (LLMs) for
interpreting their mechanism. However, they typically rely on autoencoders
constrained by some implicit training-time regularization on single training
instances (i.e., $L_1$ normalization, top-k function, etc.), without an
explicit guarantee of global sparsity among instances, causing a large amount
of dense (simultaneously inactive) features, harming the feature sparsity and
atomization. In this paper, we propose a novel autoencoder variant that
enforces minimal entropy on minibatches of hidden activations, thereby
promoting feature independence and sparsity across instances. For efficient
entropy calculation, we discretize the hidden activations to 1-bit via a step
function and apply gradient estimation to enable backpropagation, so that we
term it as Binary Autoencoder (BAE) and empirically demonstrate two major
applications: (1) Feature set entropy calculation. Entropy can be reliably
estimated on binary hidden activations, which we empirically evaluate and
leverage to characterize the inference dynamics of LLMs and In-context
Learning. (2) Feature untangling. Similar to typical methods, BAE can extract
atomized features from LLM's hidden states. To robustly evaluate such feature
extraction capability, we refine traditional feature-interpretation methods to
avoid unreliable handling of numerical tokens, and show that BAE avoids dense
features while producing the largest number of interpretable ones among
baselines, which confirms the effectiveness of BAE serving as a feature
extractor.

</details>


### [139] [Feature Augmentation of GNNs for ILPs: Local Uniqueness Suffices](https://arxiv.org/abs/2509.21000)
*Qingyu Han,Qian Li,Linxin Yang,Qian Chen,Qingjiang Shi,Ruoyu Sun*

Main category: cs.LG

TL;DR: 提出基于d-hop唯一性着色的Local - UID方案及ColorGNN和ColorUID，在ILP求解上有优势。


<details>
  <summary>Details</summary>
Motivation: 标准匿名GNN对ILP表达能力有限，全局唯一标识符增强会损害泛化性，需解决此权衡问题。

Method: 提出Local - UID方案，引入ColorGNN和ColorUID，前者通过颜色条件嵌入整合颜色信息，后者是轻量级特征级变体。

Result: 在三个ILP基准测试中取得显著收益，在线性规划数据集上有强OOD泛化性，与先进方法结合可改善一般图级任务。

Conclusion: 对于d层网络，Local - UIDs达到Global - UIDs的表达能力且泛化性更强。

Abstract: Integer Linear Programs (ILPs) are central to real-world optimizations but
notoriously difficult to solve. Learning to Optimize (L2O) has emerged as a
promising paradigm, with Graph Neural Networks (GNNs) serving as the standard
backbone. However, standard anonymous GNNs are limited in expressiveness for
ILPs, and the common enhancement of augmenting nodes with globally unique
identifiers (UIDs) typically introduces spurious correlations that severely
harm generalization. To address this tradeoff, we propose a parsimonious
Local-UID scheme based on d-hop uniqueness coloring, which ensures identifiers
are unique only within each node's d-hop neighborhood. Building on this scheme,
we introduce ColorGNN, which incorporates color information via
color-conditioned embeddings, and ColorUID, a lightweight feature-level
variant. We prove that for d-layer networks, Local-UIDs achieve the expressive
power of Global-UIDs while offering stronger generalization. Extensive
experiments show that our approach (i) yields substantial gains on three ILP
benchmarks, (ii) exhibits strong OOD generalization on linear programming
datasets, and (iii) further improves a general graph-level task when paired
with a state-of-the-art method.

</details>


### [140] [Lossless Compression: A New Benchmark for Time Series Model Evaluation](https://arxiv.org/abs/2509.21002)
*Meng Wan,Benxi Tian,Jue Wang,Cui Hui,Ningming Nie,Tiantian Liu,Zongguo Wang,Cao Rongqiang,Peng Shi,Yangang Wang*

Main category: cs.LG

TL;DR: 引入无损压缩作为评估时间序列模型新范式，定义评估协议和指标，开源评估框架TSCom - Bench，实验表明压缩能发现经典基准忽略的分布弱点。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型评估主要关注特定任务性能，未严格衡量模型是否捕获数据的完整生成分布，需要新的评估范式。

Method: 基于香农信源编码定理引入无损压缩作为评估范式，定义标准化评估协议和指标，开源评估框架TSCom - Bench。

Result: 在多个数据集上对多个先进模型的实验表明，压缩能揭示经典基准忽略的分布弱点。

Conclusion: 无损压缩是一项有原则的任务，可补充和扩展现有的时间序列建模评估。

Abstract: The evaluation of time series models has traditionally focused on four
canonical tasks: forecasting, imputation, anomaly detection, and
classification. While these tasks have driven significant progress, they
primarily assess task-specific performance and do not rigorously measure
whether a model captures the full generative distribution of the data. We
introduce lossless compression as a new paradigm for evaluating time series
models, grounded in Shannon's source coding theorem. This perspective
establishes a direct equivalence between optimal compression length and the
negative log-likelihood, providing a strict and unified information-theoretic
criterion for modeling capacity. Then We define a standardized evaluation
protocol and metrics. We further propose and open-source a comprehensive
evaluation framework TSCom-Bench, which enables the rapid adaptation of time
series models as backbones for lossless compression. Experiments across diverse
datasets on state-of-the-art models, including TimeXer, iTransformer, and
PatchTST, demonstrate that compression reveals distributional weaknesses
overlooked by classic benchmarks. These findings position lossless compression
as a principled task that complements and extends existing evaluation for time
series modeling.

</details>


### [141] [MAIFormer: Multi-Agent Inverted Transformer for Flight Trajectory Prediction](https://arxiv.org/abs/2509.21004)
*Seokbin Yoon,Keumjin Lee*

Main category: cs.LG

TL;DR: 提出MAIFormer预测多飞机飞行轨迹，实验表现优且结果可解释


<details>
  <summary>Details</summary>
Motivation: 多飞机飞行轨迹预测困难，需建模个体行为和交互，且要生成可解释结果

Method: 提出MAIFormer，含掩码多元注意力和代理注意力两个关键模块

Result: 用韩国仁川国际机场终端空域数据集评估，MAIFormer在多指标上表现最佳，优于其他方法

Conclusion: MAIFormer表现好，结果可解释，提高模型透明度和在空管中的实用性

Abstract: Flight trajectory prediction for multiple aircraft is essential and provides
critical insights into how aircraft navigate within current air traffic flows.
However, predicting multi-agent flight trajectories is inherently challenging.
One of the major difficulties is modeling both the individual aircraft
behaviors over time and the complex interactions between flights. Generating
explainable prediction outcomes is also a challenge. Therefore, we propose a
Multi-Agent Inverted Transformer, MAIFormer, as a novel neural architecture
that predicts multi-agent flight trajectories. The proposed framework features
two key attention modules: (i) masked multivariate attention, which captures
spatio-temporal patterns of individual aircraft, and (ii) agent attention,
which models the social patterns among multiple agents in complex air traffic
scenes. We evaluated MAIFormer using a real-world automatic dependent
surveillance-broadcast flight trajectory dataset from the terminal airspace of
Incheon International Airport in South Korea. The experimental results show
that MAIFormer achieves the best performance across multiple metrics and
outperforms other methods. In addition, MAIFormer produces prediction outcomes
that are interpretable from a human perspective, which improves both the
transparency of the model and its practical utility in air traffic control.

</details>


### [142] [ExMolRL: Phenotype-Target Joint Generation of De Novo Molecules via Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2509.21010)
*Haotian Guo,Hui Liu*

Main category: cs.LG

TL;DR: 提出ExMoIRL框架用于从头分子生成，结合表型和靶点信息，实验显示性能优于现有模型，生成分子有良好药物特性。


<details>
  <summary>Details</summary>
Motivation: 当前基于表型和基于靶点的策略在AI驱动药物设计中各有局限，如高实验成本或忽略系统级细胞响应，需新方法弥补。

Method: 提出ExMoIRL框架，先在大量药物诱导转录谱上预训练表型引导生成器，再通过多目标强化学习微调，奖励函数融合对接亲和力和类药性得分等。

Result: ExMoIRL在多个靶点上性能优于现有基于表型和基于靶点的模型，生成分子有良好类药性、高靶点亲和力和对癌细胞的抑制效力。

Conclusion: 该统一框架展示了结合表型引导和靶点感知策略的协同潜力，为从头药物发现提供更有效解决方案。

Abstract: The generation of high-quality candidate molecules remains a central
challenge in AI-driven drug design. Current phenotype-based and target-based
strategies each suffer limitations, either incurring high experimental costs or
overlook system-level cellular responses. To bridge this gap, we propose
ExMoIRL, a novel generative framework that synergistically integrates
phenotypic and target-specific cues for de novo molecular generation. The
phenotype-guided generator is first pretrained on expansive drug-induced
transcriptional profiles and subsequently fine-tuned via multi-objective
reinforcement learning (RL). Crucially, the reward function fuses docking
affinity and drug-likeness scores, augmented with ranking loss,
prior-likelihood regularization, and entropy maximization. The multi-objective
RL steers the model toward chemotypes that are simultaneously potent, diverse,
and aligned with the specified phenotypic effects. Extensive experiments
demonstrate ExMoIRL's superior performance over state-of-the-art
phenotype-based and target-based models across multiple well-characterized
targets. Our generated molecules exhibit favorable drug-like properties, high
target affinity, and inhibitory potency (IC50) against cancer cells. This
unified framework showcases the synergistic potential of combining
phenotype-guided and target-aware strategies, offering a more effective
solution for de novo drug discovery.

</details>


### [143] [Mechanism of Task-oriented Information Removal in In-context Learning](https://arxiv.org/abs/2509.21012)
*Hakaze Cho,Haolin Yang,Gouki Minegishi,Naoya Inoue*

Main category: cs.LG

TL;DR: 本文从信息移除视角研究上下文学习（ICL）机制，发现零样本时语言模型输出无聚焦，通过低秩滤波器移除特定信息可引导模型，少样本ICL模拟此过程，还识别出关键的去噪注意力头。


<details>
  <summary>Details</summary>
Motivation: 现有ICL的内在机制不明确，需要深入研究。

Method: 从信息移除视角研究，用低秩滤波器移除特定信息，设计指标衡量隐藏状态，进行消融实验。

Result: 零样本时模型输出无聚焦，少样本ICL能模拟信息移除过程，识别出去噪头，消融实验中ICL准确率显著下降。

Conclusion: 信息移除机制和去噪头在ICL中起关键作用。

Abstract: In-context Learning (ICL) is an emerging few-shot learning paradigm based on
modern Language Models (LMs), yet its inner mechanism remains unclear. In this
paper, we investigate the mechanism through a novel perspective of information
removal. Specifically, we demonstrate that in the zero-shot scenario, LMs
encode queries into non-selective representations in hidden states containing
information for all possible tasks, leading to arbitrary outputs without
focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we
find that selectively removing specific information from hidden states by a
low-rank filter effectively steers LMs toward the intended task. Building on
these findings, by measuring the hidden states on carefully designed metrics,
we observe that few-shot ICL effectively simulates such task-oriented
information removal processes, selectively removing the redundant information
from entangled non-selective representations, and improving the output based on
the demonstrations, which constitutes a key mechanism underlying ICL. Moreover,
we identify essential attention heads inducing the removal operation, termed
Denoising Heads, which enables the ablation experiments blocking the
information removal operation from the inference, where the ICL accuracy
significantly degrades, especially when the correct label is absent from the
few-shot demonstrations, confirming both the critical role of the information
removal mechanism and denoising heads.

</details>


### [144] [Predicting LLM Reasoning Performance with Small Proxy Model](https://arxiv.org/abs/2509.21013)
*Woosung Koh,Juyoung Suk,Sungjun Han,Se-Young Yun,Jay Shin*

Main category: cs.LG

TL;DR: 因预训练大语言模型成本高，需用小代理模型优化数据集，但推理能力优化有挑战。本文提出rBridge方法，能让小代理模型有效预测大模型推理，实验显示可降低成本、有强相关性和零样本迁移能力，为低成本探索推理预训练提供途径。


<details>
  <summary>Details</summary>
Motivation: 预训练大语言模型成本高，利用小代理模型优化数据集在推理能力方面存在挑战，需要解决该问题。

Method: 引入rBridge，通过更紧密地与预训练目标和目标任务对齐，用任务对齐对负对数似然进行加权，使用前沿模型的推理轨迹作为黄金标签。

Result: （i）相对于最佳基线，将数据集排名成本降低100倍以上；（ii）在1B到32B规模的六个推理基准测试中实现了最强相关性；（iii）在1B到7B规模的预训练数据集上实现了预测关系的零样本迁移。

Conclusion: rBridge为以较低成本探索面向推理的预训练提供了实用途径。

Abstract: Given the prohibitive cost of pre-training large language models, it is
essential to leverage smaller proxy models to optimize datasets before scaling
up. However, this approach becomes challenging for reasoning capabilities,
which exhibit emergent behavior that only appear reliably at larger model
sizes, often exceeding 7B parameters. To address this, we introduce rBridge,
showing that small proxies ($\leq$1B) can effectively predict large-model
reasoning by aligning more closely with (1) the pre-training objective and (2)
the target task. rBridge achieves this by weighting negative log-likelihood
with task alignment, using reasoning traces from frontier models as gold
labels. In our experiments, rBridge (i) reduces dataset ranking costs by over
100x relative to the best baseline, (ii) achieves the strongest correlation
across six reasoning benchmarks at 1B to 32B scale, and (iii) zero-shot
transfers predictive relationships across pre-training datasets at 1B to 7B
scale. These findings indicate that rBridge offers a practical path for
exploring reasoning-oriented pre-training at lower cost.

</details>


### [145] [DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?](https://arxiv.org/abs/2509.21016)
*Yiyou Sun,Yuhan Cao,Pohao Huang,Haoyue Bai,Hannaneh Hajishirzi,Nouha Dziri,Dawn Song*

Main category: cs.LG

TL;DR: 本文介绍DELTA - Code基准测试，探究大语言模型能否习得新推理策略，实验发现模型有学习阶段转变，评估了学习和迁移能力。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型能否在预训练和微调之外习得或泛化真正新的推理策略。

Method: 引入DELTA - Code基准测试，用强化学习训练模型，探索关键训练要素，从多维度评估迁移能力。

Result: 模型有显著学习阶段转变，组内和重组技能有提升，转换情况存在弱点。

Conclusion: DELTA - Code为探究强化学习驱动推理的极限和模型获取新算法技能提供测试平台。

Abstract: It remains an open question whether LLMs can acquire or generalize genuinely
new reasoning strategies, beyond the sharpened skills encoded in their
parameters during pre-training or post-training. To attempt to answer this
debate, we introduce DELTA-Code--Distributional Evaluation of Learnability and
Transferrability in Algorithmic Coding, a controlled benchmark of synthetic
coding problem families designed to probe two fundamental aspects: learnability
-- can LLMs, through reinforcement learning (RL), solve problem families where
pretrained models exhibit failure with large enough attempts (pass@K=0)? --and
transferrability -- if learnability happens, can such skills transfer
systematically to out-of-distribution (OOD) test sets? Unlike prior public
coding datasets, DELTA isolates reasoning skills through templated problem
generators and introduces fully OOD problem families that demand novel
strategies rather than tool invocation or memorized patterns. Our experiments
reveal a striking grokking phase transition: after an extended period with
near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To
enable learnability on previously unsolvable problem families, we explore key
training ingredients such as staged warm-up with dense rewards, experience
replay, curriculum training, and verification-in-the-loop. Beyond learnability,
we use DELTA to evaluate transferability or generalization along exploratory,
compositional, and transformative axes, as well as cross-family transfer.
Results show solid gains within families and for recomposed skills, but
persistent weaknesses in transformative cases. DELTA thus offers a clean
testbed for probing the limits of RL-driven reasoning and for understanding how
models can move beyond existing priors to acquire new algorithmic skills.

</details>


### [146] [Actor-Critic without Actor](https://arxiv.org/abs/2509.21022)
*Donghyeon Ki,Hee-Jun Ahn,Kyungyoon Kim,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 提出轻量级框架ACA，无需显式演员网络，在在线强化学习基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有演员 - 评论家方法依赖单独网络，训练易受架构和超参数影响，可扩展性受限；扩散模型有额外设计和计算负担。

Method: 引入ACA框架，直接从噪声水平评论家的梯度场生成动作，消除演员训练的开销。

Result: 在标准在线强化学习基准测试中，ACA有更优学习曲线和有竞争力的表现。

Conclusion: ACA为在线强化学习提供了简单而强大的解决方案。

Abstract: Actor-critic methods constitute a central paradigm in reinforcement learning
(RL), coupling policy evaluation with policy improvement. While effective
across many domains, these methods rely on separate actor and critic networks,
which makes training vulnerable to architectural decisions and hyperparameter
tuning. Such complexity limits their scalability in settings that require large
function approximators. Recently, diffusion models have recently been proposed
as expressive policies that capture multi-modal behaviors and improve
exploration, but they introduce additional design choices and computational
burdens, hindering efficient deployment. We introduce Actor-Critic without
Actor (ACA), a lightweight framework that eliminates the explicit actor network
and instead generates actions directly from the gradient field of a noise-level
critic. This design removes the algorithmic and computational overhead of actor
training while keeping policy improvement tightly aligned with the critic's
latest value estimates. Moreover, ACA retains the ability to capture diverse,
multi-modal behaviors without relying on diffusion-based actors, combining
simplicity with expressiveness. Through extensive experiments on standard
online RL benchmarks,ACA achieves more favorable learning curves and
competitive performance compared to both standard actor-critic and
state-of-the-art diffusion-based methods, providing a simple yet powerful
solution for online RL.

</details>


### [147] [FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance CorrEction](https://arxiv.org/abs/2509.21029)
*Runqi Lin,Alasdair Paren,Suqin Yuan,Muyang Li,Philip Torr,Adel Bibi,Tongliang Liu*

Main category: cs.LG

TL;DR: 本文分析视觉越狱攻击的损失景观，提出FORCE方法改善跨模型可迁移性，用于闭源MLLMs视觉红队评估。


<details>
  <summary>Details</summary>
Motivation: 新模态集成增强多模态大语言模型能力的同时引入新漏洞，现有简单视觉越狱攻击跨模型可迁移性差，难以识别闭源MLLMs漏洞。

Method: 分析越狱攻击的损失景观，研究中间层和频谱域的特征表示，提出Feature Over-Reliance CorrEction (FORCE)方法，消除对层和频谱特征的不可泛化依赖。

Result: 通过实验证明该方法能发现视觉越狱攻击的平坦可行区域，提高跨模型可迁移性。

Conclusion: 该方法能有效促进针对闭源MLLMs的视觉红队评估。

Abstract: The integration of new modalities enhances the capabilities of multimodal
large language models (MLLMs) but also introduces additional vulnerabilities.
In particular, simple visual jailbreaking attacks can manipulate open-source
MLLMs more readily than sophisticated textual attacks. However, these
underdeveloped attacks exhibit extremely limited cross-model transferability,
failing to reliably identify vulnerabilities in closed-source MLLMs. In this
work, we analyse the loss landscape of these jailbreaking attacks and find that
the generated attacks tend to reside in high-sharpness regions, whose
effectiveness is highly sensitive to even minor parameter changes during
transfer. To further explain the high-sharpness localisations, we analyse their
feature representations in both the intermediate layers and the spectral
domain, revealing an improper reliance on narrow layer representations and
semantically poor frequency components. Building on this, we propose a Feature
Over-Reliance CorrEction (FORCE) method, which guides the attack to explore
broader feasible regions across layer features and rescales the influence of
frequency features according to their semantic content. By eliminating
non-generalizable reliance on both layer and spectral features, our method
discovers flattened feasible regions for visual jailbreaking attacks, thereby
improving cross-model transferability. Extensive experiments demonstrate that
our approach effectively facilitates visual red-teaming evaluations against
closed-source MLLMs.

</details>


### [148] [Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs](https://arxiv.org/abs/2509.21044)
*Honglin Zhang,Qianyue Hao,Fengli Xu,Yong Li*

Main category: cs.LG

TL;DR: 研究探究大语言模型（LLMs）强化学习（RL）微调前后内部差异，发现在线RL微调有激活强度增加和模式更多样两个效果，DPO微调有不同表现，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索RL微调能提升不同内在特征LLMs能力的潜在机制。

Method: 借鉴边缘归因修补（EAP）研究方法，分析多个模型家族。

Result: 在线RL微调有激活强度增加、模式更多样两个效果；DPO微调内部变化较弱或不一致。

Conclusion: 研究给出RL微调系统改变LLMs内部电路的统一观点，突出在线RL和基于偏好方法的方法学区别。

Abstract: Large language models (LLMs) acquire extensive prior knowledge through
large-scale pretraining and can be further enhanced via supervised fine-tuning
(SFT) or reinforcement learning (RL)-based post-training. A growing body of
evidence has shown that RL fine-tuning improves the capability of LLMs beyond
what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning
is able to enhance the capability of various LLMs with distinct intrinsic
characteristics remain underexplored. In this study, we draw inspiration from
prior work on edge attribution patching (EAP) to investigate the internal
differences of LLMs before and after RL fine-tuning. Our analysis across
multiple model families shows two robust effects of online RL post-training:
(i) an overall increase in activation intensity, indicating that more internal
pathways are engaged and their signals become stronger, and (ii) greater
diversity in activation patterns, reflected by higher entropy and less
concentrated edge distributions. These changes suggest that RL reshapes
information flow to be both more redundant and more flexible, which may explain
its advantage in generalization. Notably, models fine-tuned with Direct
Preference Optimization (DPO) deviate from these trends, exhibiting
substantially weaker or inconsistent internal changes compared to PPO- and
GRPO-based training. Together, our findings provide a unified view of how RL
fine-tuning systematically alters the internal circuitry of LLMs and highlight
the methodological distinctions between online RL and preference-based
approaches. Our code is open source at
https://anonymous.4open.science/r/llm_rl_probing_analysis-F673.

</details>


### [149] [GeoRef: Referring Expressions in Geometry via Task Formulation, Synthetic Supervision, and Reinforced MLLM-based Solutions](https://arxiv.org/abs/2509.21050)
*Bing Liu,Wenqiang Yv,Xuzheng Yang,Shichang Wang,Junzhuo Liu,Peng Wang,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.LG

TL;DR: 提出几何问题的指代表达理解任务，构建GeoRef基准数据集，用合成数据训练，对比两种微调方法，提出验证再生机制，强调评估和加强几何基础的必要性。


<details>
  <summary>Details</summary>
Motivation: 解决AI驱动几何问题求解中识别和解释几何元素这一未充分探索的能力问题。

Method: 引入指代表达理解任务，构建GeoRef数据集，生成合成训练数据，采用监督微调（SFT）和组相对策略优化（GRPO）两种微调方法，提出验证再生机制。

Result: GRPO显著优于SFT，验证再生机制提升准确性，现有多模态大语言模型在该任务表现不佳，在GeoRef训练的模型在下游任务有提升。

Conclusion: 有必要明确评估和加强几何基础，指代表达理解作为多模态数学理解基础有重要价值。

Abstract: AI-driven geometric problem solving is a complex vision-language task that
requires accurate diagram interpretation, mathematical reasoning, and robust
cross-modal grounding. A foundational yet underexplored capability for this
task is the ability to identify and interpret geometric elements based on
natural language queries. To address this, we introduce the task of Referring
Expression Comprehension (REC) for geometric problems, which evaluates whether
models can localize points, shapes, and spatial relations in diagrams in
response to textual prompts. We present GeoRef, a benchmark dataset constructed
from existing geometric problem corpora, featuring diverse, high-quality
annotations and queries. Due to the lack of annotated data for this task, we
generate a large-scale synthetic training dataset using a structured geometric
formal language, enabling broad coverage of geometric concepts and facilitating
model adaptation. We explore two fine-tuning approaches: Supervised Fine-Tuning
(SFT) and Group Relative Policy Optimization (GRPO). Our results show that GRPO
significantly outperforms SFT by better aligning model behavior with
task-specific rewards. Furthermore, we propose a verify-and-regenerate
mechanism that detects incorrect predictions and re-infers answers using
contextual reasoning history, further boosting accuracy. Notably, even
state-of-the-art Multimodal Large Language Models (MLLMs) struggle with this
task, underscoring the necessity of explicitly evaluating and strengthening
geometric grounding as a prerequisite for robust geometric problem solving.
Moreover, models trained on GeoRef demonstrate measurable improvements on
downstream geometric reasoning tasks, highlighting the broader value of REC as
a foundation for multimodal mathematical understanding.

</details>


### [150] [SPREAD: Sampling-based Pareto front Refinement via Efficient Adaptive Diffusion](https://arxiv.org/abs/2509.21058)
*Sedjro Salomon Hotegni,Sebastian Peitz*

Main category: cs.LG

TL;DR: 提出基于DDPMs的生成框架SPREAD用于多目标优化，实验显示其在效率、可扩展性和Pareto前沿覆盖方面表现良好。


<details>
  <summary>Details</summary>
Motivation: 开发高效多目标优化方法来计算冲突目标间的Pareto最优集，特别是针对大规模和高成本问题。

Method: 引入基于DDPMs的生成框架SPREAD，先学习决策空间采样点的条件扩散过程，在反向扩散步骤通过自适应多梯度下降更新和高斯RBF排斥项优化候选解。

Result: 在多目标优化基准测试中，SPREAD在效率、可扩展性和Pareto前沿覆盖方面达到或超过领先基线。

Conclusion: SPREAD是解决多目标优化问题的有效方法。

Abstract: Developing efficient multi-objective optimization methods to compute the
Pareto set of optimal compromises between conflicting objectives remains a key
challenge, especially for large-scale and expensive problems. To bridge this
gap, we introduce SPREAD, a generative framework based on Denoising Diffusion
Probabilistic Models (DDPMs). SPREAD first learns a conditional diffusion
process over points sampled from the decision space and then, at each reverse
diffusion step, refines candidates via a sampling scheme that uses an adaptive
multiple gradient descent-inspired update for fast convergence alongside a
Gaussian RBF-based repulsion term for diversity. Empirical results on
multi-objective optimization benchmarks, including offline and Bayesian
surrogate-based settings, show that SPREAD matches or exceeds leading baselines
in efficiency, scalability, and Pareto front coverage.

</details>


### [151] [Structure-Attribute Transformations with Markov Chain Boost Graph Domain Adaptation](https://arxiv.org/abs/2509.21059)
*Zhen Liu,Yongtao Zhang,Shaobo Ren,Yuxin You*

Main category: cs.LG

TL;DR: 提出SATMC框架解决图域适应中结构异质性问题，理论证明有更紧误差界，实验显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统图域适应方法难以处理不同图域间的结构异质性，导致分布对齐效果不佳。

Method: 提出Structure - Attribute Transformation with Markov Chain (SATMC)框架，通过图结构和属性变换依次对齐网络分布，引入私有域信息减少机制和经验Wasserstein距离。

Result: 理论证明SATMC在跨网络节点分类上能实现更紧的误差界，九对公开跨域数据集实验表明SATMC在跨网络节点分类任务上优于现有方法。

Conclusion: SATMC是解决图域适应问题的有效方法，能提升跨网络节点分类性能。

Abstract: Graph domain adaptation has gained significant attention in label-scarce
scenarios across different graph domains. Traditional approaches to graph
domain adaptation primarily focus on transforming node attributes over raw
graph structures and aligning the distributions of the transformed node
features across networks. However, these methods often struggle with the
underlying structural heterogeneity between distinct graph domains, which leads
to suboptimal distribution alignment. To address this limitation, we propose
Structure-Attribute Transformation with Markov Chain (SATMC), a novel framework
that sequentially aligns distributions across networks via both graph structure
and attribute transformations. To mitigate the negative influence of
domain-private information and further enhance the model's generalization,
SATMC introduces a private domain information reduction mechanism and an
empirical Wasserstein distance. Theoretical proofs suggest that SATMC can
achieve a tighter error bound for cross-network node classification compared to
existing graph domain adaptation methods. Extensive experiments on nine pairs
of publicly available cross-domain datasets show that SATMC outperforms
state-of-the-art methods in the cross-network node classification task. The
code is available at https://github.com/GiantZhangYT/SATMC.

</details>


### [152] [ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning](https://arxiv.org/abs/2509.21070)
*Qizhi Pei,Zhuoshi Pan,Honglin Lin,Xin Gao,Yu Li,Zinan Tang,Conghui He,Rui Yan,Lijun Wu*

Main category: cs.LG

TL;DR: 提出ScaleDiff管道解决大规模生成难题的挑战，用自适应思维模型筛选难题训练DiffGen - 8B生成难题，在数据集上微调模型提升性能，展示了难题数量与模型性能的缩放现象。


<details>
  <summary>Details</summary>
Motivation: 现有自动合成数学问题方法在扩展规模时面临计算/API成本高、提示复杂、生成问题难度有限等挑战，需要新方法解决。

Method: 提出ScaleDiff管道，用自适应思维模型从现有数据集识别难题，训练DiffGen - 8B生成新难题，在ScaleDiff - Math数据集上微调Qwen2.5 - Math - 7B - Instruct。

Result: 在ScaleDiff - Math数据集上微调使Qwen2.5 - Math - 7B - Instruct性能提高11.3%，在多个基准测试中平均准确率达65.9%，超过OpenThinker3。

Conclusion: ScaleDiff管道能有效解决大规模生成难题的问题，可在不依赖昂贵大模型的情况下转移高级推理能力，难题数量增加时模型在困难基准测试上性能有缩放现象。

Abstract: Large Reasoning Models (LRMs) have shown impressive capabilities in complex
problem-solving, often benefiting from training on difficult mathematical
problems that stimulate intricate reasoning. Recent efforts have explored
automated synthesis of mathematical problems by prompting proprietary models or
large-scale open-source models from seed data or inherent mathematical
concepts. However, scaling up these methods remains challenging due to their
high computational/API cost, complexity of prompting, and limited difficulty
level of the generated problems. To overcome these limitations, we propose
ScaleDiff, a simple yet effective pipeline designed to scale the creation of
difficult problems. We efficiently identify difficult problems from existing
datasets with only a single forward pass using an adaptive thinking model,
which can perceive problem difficulty and automatically switch between
"Thinking" and "NoThinking" modes. We then train a specialized difficult
problem generator (DiffGen-8B) on this filtered difficult data, which can
produce new difficult problems in large scale, eliminating the need for
complex, per-instance prompting and its associated high API costs. Fine-tuning
Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial
performance increase of 11.3% compared to the original dataset and achieves a
65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500,
outperforming recent strong LRMs like OpenThinker3. Notably, this performance
is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating
that our pipeline can effectively transfer advanced reasoning capabilities
without relying on larger, more expensive teacher models. Furthermore, we
observe a clear scaling phenomenon in model performance on difficult benchmarks
as the quantity of difficult problems increases. Code:
https://github.com/QizhiPei/ScaleDiff.

</details>


### [153] [TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix](https://arxiv.org/abs/2509.21081)
*Ahmet Caner Yüzügüler,Ahmet Çelik,Jiawei Zhuang,Lukas Cavigelli*

Main category: cs.LG

TL;DR: 本文介绍TyphoonMLA，结合两种MLA内核实现方式优势，提升MLA架构注意力计算吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有MLA的吸收实现计算密集，无法利用数据重用机会提升性能，需新方法结合两种实现优势。

Method: 提出TyphoonMLA，对计算密集部分用朴素公式，非共享部分用吸收公式。

Result: TyphoonMLA在NPU和GPU上使MLA架构注意力计算吞吐量分别提升至3倍和3.24倍，HBM大小开销仅3%。

Conclusion: TyphoonMLA有效结合两种实现优势，提升了MLA架构注意力计算性能。

Abstract: Multi-Head Latent Attention (MLA) is a recent attention mechanism adopted in
state-of-the-art LLMs such as DeepSeek-v3 and Kimi K2. Thanks to its novel
formulation, MLA allows two functionally equivalent but computationally
distinct kernel implementations: naive and absorb. While the naive kernels
(e.g., FlashAttention) are typically preferred in training and prefill for
their computational efficiency, existing decoding kernels (e.g., FlashMLA) rely
on the absorb method to minimize HBM bandwidth usage. However, the
compute-bound nature of the absorb implementations prohibits performance
benefits from data reuse opportunities in attention calculations, such as
shared prefixes. In this work, we introduce TyphoonMLA, a hybrid approach that
combines naive and absorb formulations to harness the strengths of both.
TyphoonMLA effectively leverages the shared prefix by applying the naive
formulation to the compute-bound parts of attention calculations, while
reducing the bandwidth requirements for non-shared parts by using the absorb
formulation. As a result, TyphoonMLA improves the throughput of attention
calculations in MLA architectures by up to 3x and 3.24x on NPU and GPUs, with
only a 3% overhead in HBM size.

</details>


### [154] [GraphUniverse: Enabling Systematic Evaluation of Inductive Generalization](https://arxiv.org/abs/2509.21097)
*Louis Van Langendonck,Guillermo Bernárdez,Nina Miolane,Pere Barlet-Ros*

Main category: cs.LG

TL;DR: 提出GraphUniverse框架用于大规模归纳泛化评估，发现强传导性能不能预测归纳泛化，架构和初始图机制影响分布转移鲁棒性，该框架有助于开发鲁棒架构。


<details>
  <summary>Details</summary>
Motivation: 现有图学习方法局限于单图传导设置，缺乏对新图归纳泛化的系统评估。

Method: 引入GraphUniverse框架，生成具有持久语义社区的图，可控制结构属性，对多种架构进行基准测试。

Result: 强传导性能不能很好预测归纳泛化，分布转移鲁棒性受模型架构和初始图机制影响。

Conclusion: GraphUniverse框架的灵活性和可扩展性有助于开发鲁棒且可泛化的架构。

Abstract: A fundamental challenge in graph learning is understanding how models
generalize to new, unseen graphs. While synthetic benchmarks offer controlled
settings for analysis, existing approaches are confined to single-graph,
transductive settings where models train and test on the same graph structure.
Addressing this gap, we introduce GraphUniverse, a framework for generating
entire families of graphs to enable the first systematic evaluation of
inductive generalization at scale. Our core innovation is the generation of
graphs with persistent semantic communities, ensuring conceptual consistency
while allowing fine-grained control over structural properties like homophily
and degree distributions. This enables crucial but underexplored robustness
tests, such as performance under controlled distribution shifts. Benchmarking a
wide range of architectures -- from GNNs to graph transformers and topological
architectures -- reveals that strong transductive performance is a poor
predictor of inductive generalization. Furthermore, we find that robustness to
distribution shift is highly sensitive not only to model architecture choice
but also to the initial graph regime (e.g., high vs. low homophily). Beyond
benchmarking, GraphUniverse's flexibility and scalability can facilitate the
development of robust and truly generalizable architectures -- including
next-generation graph foundation models. An interactive demo is available at
https://graphuniverse.streamlit.app.

</details>


### [155] [Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning](https://arxiv.org/abs/2509.21126)
*Xiefeng Wu,Jing Zhao,Shu Zhang,Mingyu Hu*

Main category: cs.LG

TL;DR: 提出VARL框架利用视觉语言模型为强化学习代理提供动作建议，提升样本效率，可用于在线强化学习。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习在复杂任务中耗时，视觉语言动作策略在低级控制中性能有限且常需特定专家演示微调。

Method: 提出VARL框架，利用视觉语言模型的领域知识为强化学习代理提供动作建议，而非设计启发式奖励。

Result: 在不同环境和代理设置中评估，VARL大幅提升样本效率，且未引入显著计算开销。

Conclusion: VARL是在线强化学习的通用框架，使在现实环境中从零开始直接应用强化学习成为可能。

Abstract: Online reinforcement learning in complex tasks is time-consuming, as massive
interaction steps are needed to learn the optimal Q-function.Vision-language
action (VLA) policies represent a promising direction for solving diverse
tasks; however, their performance on low-level control remains limited, and
effective deployment often requires task-specific expert demonstrations for
fine-tuning. In this paper, we propose \textbf{VARL} (\textbf{V}LM as
\textbf{A}ction advisor for online \textbf{R}einforcement \textbf{L}earning), a
framework that leverages the domain knowledge of vision-language models (VLMs)
to provide action suggestions for reinforcement learning agents. Unlike
previous methods, VARL provides action suggestions rather than designing
heuristic rewards, thereby guaranteeing unchanged optimality and convergence.
The suggested actions increase sample diversity and ultimately improve sample
efficiency, especially in sparse-reward tasks. To validate the effectiveness of
VARL, we evaluate it across diverse environments and agent settings. Results
show that VARL greatly improves sample efficiency without introducing
significant computational overhead. These advantages make VARL a general
framework for online reinforcement learning and make it feasible to directly
apply reinforcement learning from scratch in real-world environments.

</details>


### [156] [EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense](https://arxiv.org/abs/2509.21129)
*Wei Huang,De-Tian Chu,Lin-Yuan Bai,Wei Kang,Hai-Tao Zhang,Bo Li,Zhi-Mo Han,Jing Ge,Hai-Feng Lin*

Main category: cs.LG

TL;DR: 传统垃圾邮件检测系统难以应对现代复杂攻击，本文提出 EvoMail 框架，实验表明其性能优于现有基线，可抵御下一代威胁。


<details>
  <summary>Details</summary>
Motivation: 现代邮件垃圾和网络钓鱼攻击复杂多变，传统检测系统依赖静态规则或单模态模型，难以集成异构信号和持续适应，导致性能快速下降。

Method: 构建统一的异构邮件图融合多种信息；用大语言模型增强的认知图神经网络进行上下文感知推理；采用对抗性自我进化循环，红队生成逃避策略，蓝队从失败中学习并用于未来推理。

Result: 在真实数据集和合成对抗变体上的实验表明，EvoMail 在检测准确性、适应垃圾邮件策略演变能力和推理轨迹可解释性方面均优于现有基线。

Conclusion: EvoMail 有潜力成为抵御下一代垃圾邮件和网络钓鱼威胁的有弹性且可解释的防御框架。

Abstract: Modern email spam and phishing attacks have evolved far beyond keyword
blacklists or simple heuristics. Adversaries now craft multi-modal campaigns
that combine natural-language text with obfuscated URLs, forged headers, and
malicious attachments, adapting their strategies within days to bypass filters.
Traditional spam detection systems, which rely on static rules or
single-modality models, struggle to integrate heterogeneous signals or to
continuously adapt, leading to rapid performance degradation.
  We propose EvoMail, a self-evolving cognitive agent framework for robust
detection of spam and phishing. EvoMail first constructs a unified
heterogeneous email graph that fuses textual content, metadata (headers,
senders, domains), and embedded resources (URLs, attachments). A Cognitive
Graph Neural Network enhanced by a Large Language Model (LLM) performs
context-aware reasoning across these sources to identify coordinated spam
campaigns. Most critically, EvoMail engages in an adversarial self-evolution
loop: a ''red-team'' agent generates novel evasion tactics -- such as character
obfuscation or AI-generated phishing text -- while the ''blue-team'' detector
learns from failures, compresses experiences into a memory module, and reuses
them for future reasoning.
  Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam,
SpamAssassin, and TREC) and synthetic adversarial variants demonstrate that
EvoMail consistently outperforms state-of-the-art baselines in detection
accuracy, adaptability to evolving spam tactics, and interpretability of
reasoning traces. These results highlight EvoMail's potential as a resilient
and explainable defense framework against next-generation spam and phishing
threats.

</details>


### [157] [Sparse Representations Improve Adversarial Robustness of Neural Network Classifiers](https://arxiv.org/abs/2509.21130)
*Killian Steunou,Sigurd Saue,Théo Druilhe*

Main category: cs.LG

TL;DR: 本文探讨线性降维作为对抗攻击防御方法，对比PCA和SPCA，理论分析SPCA特征的鲁棒性，实验表明SPCA在攻击下表现更好。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在图像分类任务中易受对抗扰动影响，需寻找简单且适应数据的防御方法。

Method: 实证比较标准PCA和稀疏PCA（SPCA）作为下游分类器的前端特征提取器，并进行理论分析，推导SPCA特征线性头的鲁棒性证书。

Result: 对于线性头，认证半径与相关对偶范数有关；对于非线性头，稀疏性降低算子范数界；实验中SPCA在攻击下比PCA更稳定，且保持较高的干净准确率。

Conclusion: 理论揭示了稀疏投影减少对抗杠杆的机制，实验验证该优势在线性设置之外依然存在。

Abstract: Deep neural networks perform remarkably well on image classification tasks
but remain vulnerable to carefully crafted adversarial perturbations. This work
revisits linear dimensionality reduction as a simple, data-adapted defense. We
empirically compare standard Principal Component Analysis (PCA) with its sparse
variant (SPCA) as front-end feature extractors for downstream classifiers, and
we complement these experiments with a theoretical analysis. On the theory
side, we derive exact robustness certificates for linear heads applied to SPCA
features: for both $\ell_\infty$ and $\ell_2$ threat models (binary and
multiclass), the certified radius grows as the dual norms of $W^\top u$ shrink,
where $W$ is the projection and $u$ the head weights. We further show that for
general (non-linear) heads, sparsity reduces operator-norm bounds through a
Lipschitz composition argument, predicting lower input sensitivity.
Empirically, with a small non-linear network after the projection, SPCA
consistently degrades more gracefully than PCA under strong white-box and
black-box attacks while maintaining competitive clean accuracy. Taken together,
the theory identifies the mechanism (sparser projections reduce adversarial
leverage) and the experiments verify that this benefit persists beyond the
linear setting. Our code is available at
https://github.com/killian31/SPCARobustness.

</details>


### [158] [LAVA: Explainability for Unsupervised Latent Embeddings](https://arxiv.org/abs/2509.21149)
*Ivan Stresec,Joana P. Gonçalves*

Main category: cs.LG

TL;DR: 介绍了一种后验模型无关方法LAVA，用于解释无监督模型潜在嵌入组织与输入特征的关系，并通过实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 无监督黑盒模型难解释，现有无监督学习解释方法存在过于细粒度或简化的问题，尤其对于无映射函数的流形学习方法。

Method: 引入Locality - Aware Variable Associations (LAVA)方法，将潜在空间表示为一系列局部区域，通过原特征间相关性描述，并揭示整个潜在空间中重复出现的相关模式。

Result: 基于MNIST和单细胞肾脏数据集的UMAP嵌入，LAVA捕获了相关特征关联，潜在空间中看似遥远区域存在视觉和生物学上相关的局部模式。

Conclusion: LAVA方法能有效解释无监督模型潜在嵌入组织与输入特征的关系。

Abstract: Unsupervised black-box models can be drivers of scientific discovery, but
remain difficult to interpret. Crucially, discovery hinges on understanding the
model output, which is often a multi-dimensional latent embedding rather than a
well-defined target. While explainability for supervised learning usually seeks
to uncover how input features are used to predict a target, its unsupervised
counterpart should relate input features to the structure of the learned latent
space. Adaptations of supervised model explainability for unsupervised learning
provide either single-sample or dataset-wide summary explanations. However,
without automated strategies of relating similar samples to one another guided
by their latent proximity, explanations remain either too fine-grained or too
reductive to be meaningful. This is especially relevant for manifold learning
methods that produce no mapping function, leaving us only with the relative
spatial organization of their embeddings. We introduce Locality-Aware Variable
Associations (LAVA), a post-hoc model-agnostic method designed to explain local
embedding organization through its relationship with the input features. To
achieve this, LAVA represents the latent space as a series of localities
(neighborhoods) described in terms of correlations between the original
features, and then reveals reoccurring patterns of correlations across the
entire latent space. Based on UMAP embeddings of MNIST and a single-cell kidney
dataset, we show that LAVA captures relevant feature associations, with
visually and biologically relevant local patterns shared among seemingly
distant regions of the latent spaces.

</details>


### [159] [CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific Tokenization](https://arxiv.org/abs/2509.21150)
*Ruiyu Wang,Shizhao Sun,Weijian Ma,Jiang Bian*

Main category: cs.LG

TL;DR: 提出CAD-Tokenizer框架用于文本引导的CAD原型设计，能提升指令遵循和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型分词器无法捕捉CAD语义，阻碍对几何结构建模，需新的多模态分词策略。

Method: 提出CAD-Tokenizer框架，用基于序列的VQ - VAE和原始级池化及约束解码，以特定模态标记表示CAD数据。

Result: 应用于统一的文本引导CAD原型设计时，CAD-Tokenizer在定量和定性性能上优于通用大语言模型和特定任务基线。

Conclusion: CAD-Tokenizer能产生与CAD结构特性相符的紧凑、感知原始的表示，有效提升文本引导CAD原型设计效果。

Abstract: Computer-Aided Design (CAD) is a foundational component of industrial
prototyping, where models are defined not by raw coordinates but by
construction sequences such as sketches and extrusions. This sequential
structure enables both efficient prototype initialization and subsequent
editing. Text-guided CAD prototyping, which unifies Text-to-CAD generation and
CAD editing, has the potential to streamline the entire design pipeline.
However, prior work has not explored this setting, largely because standard
large language model (LLM) tokenizers decompose CAD sequences into
natural-language word pieces, failing to capture primitive-level CAD semantics
and hindering attention modules from modeling geometric structure. We
conjecture that a multimodal tokenization strategy, aligned with CAD's
primitive and structural nature, can provide more effective representations. To
this end, we propose CAD-Tokenizer, a framework that represents CAD data with
modality-specific tokens using a sequence-based VQ-VAE with primitive-level
pooling and constrained decoding. This design produces compact, primitive-aware
representations that align with CAD's structural nature. Applied to unified
text-guided CAD prototyping, CAD-Tokenizer significantly improves instruction
following and generation quality, achieving better quantitative and qualitative
performance over both general-purpose LLMs and task-specific baselines.

</details>


### [160] [GRPO is Secretly a Process Reward Model](https://arxiv.org/abs/2509.21154)
*Michael Sullivan*

Main category: cs.LG

TL;DR: 理论证明GRPO RL算法能诱导非平凡过程奖励模型（PRM），发现GRPO目标存在缺陷，提出改进算法λ - GRPO，可提升模型性能且成本低。


<details>
  <summary>Details</summary>
Motivation: 探究GRPO RL算法的过程奖励模型，解决GRPO目标存在的问题以提升模型性能。

Method: 理论证明GRPO诱导PRM，实证假设在现实中成立，识别GRPO目标缺陷，提出λ - GRPO算法改进。

Result: λ - GRPO训练的大语言模型在验证准确性、下游推理任务表现更好，更快达峰值。

Conclusion: 可利用GRPO内置PRM结构提升性能，无需高成本显式定义PRM。

Abstract: We prove theoretically that the GRPO RL algorithm induces a non-trivial
process reward model (PRM), under certain assumptions regarding within-group
overlap of token sequences across completions. We then show empirically that
these assumptions are met under real-world conditions: GRPO does in fact induce
a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a
flaw in the GRPO objective: non-uniformly distributed process steps hinder both
exploration and exploitation (under different conditions). We propose a simple
modification to the algorithm to mitigate this defect ($\lambda$-GRPO), and
show that LLMs trained with $\lambda$-GRPO achieve higher validation accuracy
and performance on downstream reasoning tasks$-$and reach peak performance more
rapidly$-$than LLMs trained with standard GRPO. Our results call into question
the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is
possible to instead leverage the hidden, built-in PRM structure within the
vanilla GRPO algorithm to boost model performance with a negligible impact on
training time and cost.

</details>


### [161] [DATS: Distance-Aware Temperature Scaling for Calibrated Class-Incremental Learning](https://arxiv.org/abs/2509.21161)
*Giuseppe Serra,Florian Buettner*

Main category: cs.LG

TL;DR: 提出距离感知温度缩放（DATS）方法，在减少跨任务校准误差上比现有方法更稳定可靠。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习校准方法依赖单一温度，忽略任务差异，在测试时无法获取任务信息，需更优校准方法。

Method: 提出DATS方法，结合基于原型的距离估计和距离感知校准，在无先验任务信息下推断任务接近度并分配自适应温度。

Result: 在标准基准和生物医学领域不平衡数据集上进行广泛实证评估，DATS方法在减少跨任务校准误差上比现有方法更稳定、可靠且一致。

Conclusion: DATS方法能有效解决持续学习中的校准问题，优于现有方法。

Abstract: Continual Learning (CL) is recently gaining increasing attention for its
ability to enable a single model to learn incrementally from a sequence of new
classes. In this scenario, it is important to keep consistent predictive
performance across all the classes and prevent the so-called Catastrophic
Forgetting (CF). However, in safety-critical applications, predictive
performance alone is insufficient. Predictive models should also be able to
reliably communicate their uncertainty in a calibrated manner - that is, with
confidence scores aligned to the true frequencies of target events. Existing
approaches in CL address calibration primarily from a data-centric perspective,
relying on a single temperature shared across all tasks. Such solutions
overlook task-specific differences, leading to large fluctuations in
calibration error across tasks. For this reason, we argue that a more
principled approach should adapt the temperature according to the distance to
the current task. However, the unavailability of the task information at test
time/during deployment poses a major challenge to achieve the intended
objective. For this, we propose Distance-Aware Temperature Scaling (DATS),
which combines prototype-based distance estimation with distance-aware
calibration to infer task proximity and assign adaptive temperatures without
prior task information. Through extensive empirical evaluation on both standard
benchmarks and real-world, imbalanced datasets taken from the biomedical
domain, our approach demonstrates to be stable, reliable and consistent in
reducing calibration error across tasks compared to state-of-the-art
approaches.

</details>


### [162] [Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say](https://arxiv.org/abs/2509.21164)
*Jacob Fein-Ashley,Dhruv Parikh,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: 提出Mixture of Thoughts (MoT)方法实现异构大语言模型在潜在层协作，在多个基准测试中超越当前方法。


<details>
  <summary>Details</summary>
Motivation: 开源大语言模型按领域专业化，现有多LLM方法存在局限性，需新方法利用模型互补优势。

Method: 引入MoT方法，通过轻量级路由器选择专家，设置交互层将隐藏状态投影到共享潜在空间，仅训练路由器和交互层。

Result: 在多个基准测试中，MoT分别比Avengers提高0.38%和2.92%，显著优于最佳单模型，单步推理且运行时间与路由基线相当。

Conclusion: MoT提供了结合异构大语言模型的潜在空间机制，是迈向更广泛多LLM协作的实际步骤。

Abstract: Open-source Large Language Models (LLMs) increasingly specialize by domain
(e.g., math, code, general reasoning), motivating systems that leverage
complementary strengths across models. Prior multi-LLM approaches either (i)
route a query to one or a few experts and generate independently, (ii)
aggregate outputs from each model via costly multi-turn exchanges, or (iii)
fuse weights into a single model-typically requiring architectural homogeneity.
We introduce Mixture of Thoughts (MoT), a simple method for latent-level
collaboration among heterogeneous experts under a global routing scheme. For
each query, a lightweight router selects top-$K$ experts and designates a
primary expert; uniformly placed interaction layers project hidden states into
a shared latent space where the primary expert performs cross-attention over
its active (selected) peers. Pre-trained experts remain frozen; only the router
and the lightweight interaction layers are trained with a novel joint training
objective that improves both the expert selection and inter-expert
collaboration. Across five in-distribution (ID) and three out-of-distribution
(OOD) benchmarks, MoT surpasses the current routing and aggregation-based
state-of-the-art, Avengers, by $+0.38\%$ and $+2.92\%$, respectively. Further,
MoT significantly outperforms the best-performing single model. It achieves
this with single-pass inference, runtime comparable to routing baselines, and
none of the overheads of iterative aggregation. MoT offers a simple
latent-space mechanism for combining heterogeneous LLMs, a practical step
toward broader multi-LLM collaboration. Our code is publicly available at
https://github.com/jacobfa/mot.

</details>


### [163] [A Unified Framework for Diffusion Model Unlearning with f-Divergence](https://arxiv.org/abs/2509.21167)
*Nicola Novello,Federico Fontana,Luigi Cinque,Deniz Gunduz,Andrea M. Tonello*

Main category: cs.LG

TL;DR: 提出基于f - 散度的统一框架用于文本到图像模型的去学习，可灵活选择散度平衡去学习与概念保留。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型的去学习方法常依赖最小化均方误差，需更灵活的方法。

Method: 提出统一的基于f - 散度的框架，分析不同f - 散度的好处。

Result: 该框架可根据具体应用选择最优散度。

Conclusion: 统一框架提供了灵活范式，能平衡激进去学习和概念保留的权衡。

Abstract: Machine unlearning aims to remove specific knowledge from a trained model.
While diffusion models (DMs) have shown remarkable generative capabilities,
existing unlearning methods for text-to-image (T2I) models often rely on
minimizing the mean squared error (MSE) between the output distribution of a
target and an anchor concept. We show that this MSE-based approach is a special
case of a unified $f$-divergence-based framework, in which any $f$-divergence
can be utilized. We analyze the benefits of using different $f$-divergences,
that mainly impact the convergence properties of the algorithm and the quality
of unlearning. The proposed unified framework offers a flexible paradigm that
allows to select the optimal divergence for a specific application, balancing
different trade-offs between aggressive unlearning and concept preservation.

</details>


### [164] [Towards Foundation Models for Zero-Shot Time Series Anomaly Detection: Leveraging Synthetic Data and Relative Context Discrepancy](https://arxiv.org/abs/2509.21190)
*Tian Lan,Hao Duong Le,Jinbo Li,Wenjun He,Meng Wang,Chenghao Liu,Chen Zhang*

Main category: cs.LG

TL;DR: 提出用于时间序列异常检测的新基础模型TimeRCD，在零样本检测中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测基础模型基于重建目标，存在目标不匹配问题，易产生高误报和漏报率。

Method: 引入相对上下文差异（RCD）预训练范式，用标准Transformer架构实现，开发带标记的大规模合成语料库进行预训练。

Result: TimeRCD在零样本时间序列异常检测中显著优于现有通用和特定异常基础模型。

Conclusion: 验证了RCD范式的优越性，为构建鲁棒且可泛化的时间序列异常检测基础模型提供新路径。

Abstract: Time series anomaly detection (TSAD) is a critical task, but developing
models that generalize to unseen data in a zero-shot manner remains a major
challenge. Prevailing foundation models for TSAD predominantly rely on
reconstruction-based objectives, which suffer from a fundamental objective
mismatch: they struggle to identify subtle anomalies while often
misinterpreting complex normal patterns, leading to high rates of false
negatives and positives. To overcome these limitations, we introduce
\texttt{TimeRCD}, a novel foundation model for TSAD built upon a new
pre-training paradigm: Relative Context Discrepancy (RCD). Instead of learning
to reconstruct inputs, \texttt{TimeRCD} is explicitly trained to identify
anomalies by detecting significant discrepancies between adjacent time windows.
This relational approach, implemented with a standard Transformer architecture,
enables the model to capture contextual shifts indicative of anomalies that
reconstruction-based methods often miss. To facilitate this paradigm, we
develop a large-scale, diverse synthetic corpus with token-level anomaly
labels, providing the rich supervisory signal necessary for effective
pre-training. Extensive experiments demonstrate that \texttt{TimeRCD}
significantly outperforms existing general-purpose and anomaly-specific
foundation models in zero-shot TSAD across diverse datasets. Our results
validate the superiority of the RCD paradigm and establish a new, effective
path toward building robust and generalizable foundation models for time series
anomaly detection.

</details>


### [165] [Differential-Integral Neural Operator for Long-Term Turbulence Forecasting](https://arxiv.org/abs/2509.21196)
*Hao Wu,Yuan Gao,Fan Xu,Fan Zhang,Qingsong Wen,Kun Wang,Xiaomeng Huang,Xian Wu*

Main category: cs.LG

TL;DR: 提出微分积分神经算子（DINN）框架用于长期湍流预测，在2D Kolmogorov流基准测试中显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在长期自回归预测中失败，无法同时捕捉主导湍流动力学的不同数学结构。

Method: 从算子分解的第一性原理出发设计DINN，通过并行分支分别学习局部微分算子和全局积分算子。

Result: 在2D Kolmogorov流基准测试中，DINN显著优于现有模型，抑制误差积累，保持高保真度。

Conclusion: DINN为物理一致的长期湍流预测建立了新基准。

Abstract: Accurately forecasting the long-term evolution of turbulence represents a
grand challenge in scientific computing and is crucial for applications ranging
from climate modeling to aerospace engineering. Existing deep learning methods,
particularly neural operators, often fail in long-term autoregressive
predictions, suffering from catastrophic error accumulation and a loss of
physical fidelity. This failure stems from their inability to simultaneously
capture the distinct mathematical structures that govern turbulent dynamics:
local, dissipative effects and global, non-local interactions. In this paper,
we propose the
{\textbf{\underline{D}}}ifferential-{\textbf{\underline{I}}}ntegral
{\textbf{\underline{N}}}eural {\textbf{\underline{O}}}perator (\method{}), a
novel framework designed from a first-principles approach of operator
decomposition. \method{} explicitly models the turbulent evolution through
parallel branches that learn distinct physical operators: a local differential
operator, realized by a constrained convolutional network that provably
converges to a derivative, and a global integral operator, captured by a
Transformer architecture that learns a data-driven global kernel. This
physics-based decomposition endows \method{} with exceptional stability and
robustness. Through extensive experiments on the challenging 2D Kolmogorov flow
benchmark, we demonstrate that \method{} significantly outperforms
state-of-the-art models in long-term forecasting. It successfully suppresses
error accumulation over hundreds of timesteps, maintains high fidelity in both
the vorticity fields and energy spectra, and establishes a new benchmark for
physically consistent, long-range turbulence forecast.

</details>


### [166] [From Physics to Machine Learning and Back: Part II - Learning and Observational Bias in PHM](https://arxiv.org/abs/2509.21207)
*Olga Fink,Ismail Nejjar,Vinay Sharma,Keivan Faghih Niresi,Han Sun,Hao Dong,Chenghao Xu,Amaury Wei,Arthur Bizzi,Raffael Theiler,Yuan Tian,Leandro Von Krannichfeldt,Zhan Ma,Sergei Garmaev,Zepeng Zhang,Mengjie Zhao*

Main category: cs.LG

TL;DR: 本文探讨结合学习与观测偏差的物理信息建模和数据策略用于PHM，实现从被动预测到主动决策转变并解决大规模部署挑战。


<details>
  <summary>Details</summary>
Motivation: 现实PHM存在传感器数据噪声大、标签有限、系统行为复杂等挑战，需有效解决方法。

Method: 采用物理信息建模和数据策略，结合学习和观测偏差，利用强化学习，还提及快速适应方法和领域泛化技术。

Result: 实现从被动预测到主动决策的转变，闭环连接模型预测、仿真和实际系统操作。

Conclusion: 物理信息机器学习为解决PHM挑战提供了有前景的方法，且要解决大规模部署问题。

Abstract: Prognostics and Health Management ensures the reliability, safety, and
efficiency of complex engineered systems by enabling fault detection,
anticipating equipment failures, and optimizing maintenance activities
throughout an asset lifecycle. However, real-world PHM presents persistent
challenges: sensor data is often noisy or incomplete, available labels are
limited, and degradation behaviors and system interdependencies can be highly
complex and nonlinear. Physics-informed machine learning has emerged as a
promising approach to address these limitations by embedding physical knowledge
into data-driven models. This review examines how incorporating learning and
observational biases through physics-informed modeling and data strategies can
guide models toward physically consistent and reliable predictions. Learning
biases embed physical constraints into model training through physics-informed
loss functions and governing equations, or by incorporating properties like
monotonicity. Observational biases influence data selection and synthesis to
ensure models capture realistic system behavior through virtual sensing for
estimating unmeasured states, physics-based simulation for data augmentation,
and multi-sensor fusion strategies. The review then examines how these
approaches enable the transition from passive prediction to active
decision-making through reinforcement learning, which allows agents to learn
maintenance policies that respect physical constraints while optimizing
operational objectives. This closes the loop between model-based predictions,
simulation, and actual system operation, empowering adaptive decision-making.
Finally, the review addresses the critical challenge of scaling PHM solutions
from individual assets to fleet-wide deployment. Fast adaptation methods
including meta-learning and few-shot learning are reviewed alongside domain
generalization techniques ...

</details>


### [167] [AbideGym: Turning Static RL Worlds into Adaptive Challenges](https://arxiv.org/abs/2509.21234)
*Abi Aryan,Zac Liu,Aaron Childress*

Main category: cs.LG

TL;DR: AbideGym是一个动态MiniGrid包装器，可用于评估强化学习智能体的策略适应性。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练的智能体策略在动态变化时易失效，静态基准测试加剧该问题，需要更好的评估方法。

Method: AbideGym引入智能体感知扰动和可扩展复杂性，强制智能体进行剧集内适应。

Result: AbideGym能暴露静态策略弱点，促进策略的弹性。

Conclusion: AbideGym提供了一个模块化、可复现的评估框架，有助于课程学习、持续学习和鲁棒泛化研究。

Abstract: Agents trained with reinforcement learning often develop brittle policies
that fail when dynamics shift, a problem amplified by static benchmarks.
AbideGym, a dynamic MiniGrid wrapper, introduces agent-aware perturbations and
scalable complexity to enforce intra-episode adaptation. By exposing weaknesses
in static policies and promoting resilience, AbideGym provides a modular,
reproducible evaluation framework for advancing research in curriculum
learning, continual learning, and robust generalization.

</details>


### [168] [Tree Search for LLM Agent Reinforcement Learning](https://arxiv.org/abs/2509.21240)
*Yuxiang Ji,Ziyu Ma,Yong Wang,Guanhua Chen,Xiangxiang Chu,Liaoni Wu*

Main category: cs.LG

TL;DR: 提出Tree - GRPO方法解决大语言模型强化学习中稀疏监督问题，实验证明其优于链式RL方法。


<details>
  <summary>Details</summary>
Motivation: 现有仅依赖结果奖励的方法在长期多轮代理任务中存在稀疏监督问题。

Method: 提出基于树搜索的分组代理RL方法Tree - GRPO，利用树节点表示完整交互步骤，通过共享前缀增加采样数量，构建逐步过程监督信号，在树内和树间估计分组相对优势。

Result: 在11个数据集和3种QA任务的实验中，树型RL方法优于链式RL方法。

Conclusion: Tree - GRPO方法在解决大语言模型强化学习稀疏监督问题上表现出色。

Abstract: Recent advances in reinforcement learning (RL) have significantly enhanced
the agentic capabilities of large language models (LLMs). In long-term and
multi-turn agent tasks, existing approaches driven solely by outcome rewards
often suffer from the problem of sparse supervision. To address the challenge,
we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped
agent RL method based on tree search, where each tree node represents the
complete agent interaction step. By sharing common prefixes, the tree search
sampling increases the number of rollouts achievable within a fixed budget of
tokens or tool calls. Moreover, we find that the tree-structured trajectory
naturally allows the construction of step-wise process supervised signals even
using only the outcome reward. Based on this, Tree-GRPO estimates the grouped
relative advantages both on intra-tree and inter-tree levels. Through
theoretical analysis, we demonstrate that the objective of intra-tree level
group relative policy optimization is equivalent to that of step-level direct
preference learning. Experiments across 11 datasets and 3 types of QA tasks
demonstrate the superiority of the proposed tree-based RL over the chain-based
RL method.

</details>


### [169] [Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven Framework](https://arxiv.org/abs/2509.21241)
*Yucheng Wang,Ziyang Chen,Md Faisal Kabir*

Main category: cs.LG

TL;DR: 本文提出基于知识图谱反事实解释微调大语言模型的框架，应用于微调后的LLaMA模型，揭示模型结构依赖，为可解释AI提供新见解。


<details>
  <summary>Details</summary>
Motivation: 理解低秩适配（LoRA）微调机制如何改变模型的结构推理和语义行为仍是挑战。

Method: 构建生物信息学工具领域特定的异质知识图谱BioToolKG，设计基于反事实的微调大语言模型解释器CFFTLLMExplainer，联合优化结构稀疏性和语义差异，同时实施可解释性保留约束。

Result: 将框架应用于微调后的基于LLaMA的大语言模型，发现反事实掩码揭示了模型的结构依赖，并与LoRA引起的参数偏移一致。

Conclusion: 为微调大语言模型的内部机制提供新见解，突出反事实图作为可解释AI潜在工具的作用。

Abstract: The widespread adoption of Low-Rank Adaptation (LoRA) has enabled large
language models (LLMs) to acquire domain-specific knowledge with remarkable
efficiency. However, understanding how such a fine-tuning mechanism alters a
model's structural reasoning and semantic behavior remains an open challenge.
This work introduces a novel framework that explains fine-tuned LLMs via
counterfactuals grounded in knowledge graphs. Specifically, we construct
BioToolKG, a domain-specific heterogeneous knowledge graph in bioinformatics
tools and design a counterfactual-based fine-tuned LLMs explainer
(CFFTLLMExplainer) that learns soft masks over graph nodes and edges to
generate minimal structural perturbations that induce maximum semantic
divergence. Our method jointly optimizes structural sparsity and semantic
divergence while enforcing interpretability preserving constraints such as
entropy regularization and edge smoothness. We apply this framework to a
fine-tuned LLaMA-based LLM and reveal that counterfactual masking exposes the
model's structural dependencies and aligns with LoRA-induced parameter shifts.
This work provides new insights into the internal mechanisms of fine-tuned LLMs
and highlights counterfactual graphs as a potential tool for interpretable AI.

</details>


### [170] [Federated Flow Matching](https://arxiv.org/abs/2509.21250)
*Zifan Wang,Anqi Dong,Mahmoud Selim,Michael M. Zavlanos,Karl H. Johansson*

Main category: cs.LG

TL;DR: 文章提出Federated Flow Matching (FFM)框架用于在隐私约束下训练流匹配模型，有不同变体，实验表明其能在联邦设置下实现隐私保护训练，性能与集中式基线相当。


<details>
  <summary>Details</summary>
Motivation: 当前数据分散，隐私、所有权和监管等因素阻止数据集中，因此需要直接从分布式数据本地训练生成模型而不进行集中聚合。

Method: 提出FFM框架，包含FFM - vanilla（各客户端用独立源和目标耦合本地训练）、FFM - LOT（采用局部最优传输耦合）和FFM - GOT（基于最优传输的半对偶公式的联邦策略，用共享全局势函数协调客户端耦合）。

Result: 在合成和图像数据集实验中，FFM能实现隐私保护训练，提高流的直线度和样本质量。

Conclusion: FFM可在联邦设置下实现隐私保护训练，性能与集中式基线可比。

Abstract: Data today is decentralized, generated and stored across devices and
institutions where privacy, ownership, and regulation prevent centralization.
This motivates the need to train generative models directly from distributed
data locally without central aggregation. In this paper, we introduce Federated
Flow Matching (FFM), a framework for training flow matching models under
privacy constraints. Specifically, we first examine FFM-vanilla, where each
client trains locally with independent source and target couplings, preserving
privacy but yielding curved flows that slow inference. We then develop FFM-LOT,
which employs local optimal transport couplings to improve straightness within
each client but lacks global consistency under heterogeneous data. Finally, we
propose FFM-GOT, a federated strategy based on the semi-dual formulation of
optimal transport, where a shared global potential function coordinates
couplings across clients. Experiments on synthetic and image datasets show that
FFM enables privacy-preserving training while enhancing both the flow
straightness and sample quality in federated settings, with performance
comparable to the centralized baseline.

</details>


### [171] [humancompatible.train: Implementing Optimization Algorithms for Stochastically-Constrained Stochastic Optimization Problems](https://arxiv.org/abs/2509.21254)
*Andrii Kliachkin,Jana Lepšová,Gilles Bareilles,Jakub Mareček*

Main category: cs.LG

TL;DR: 介绍了用于带随机约束训练DNN的可扩展Python包humancompatible.train，并展示其在公平约束深度学习任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 当前深度神经网络约束训练缺乏行业标准，有开发相关工具包的需求。

Method: 开发基于PyTorch的可扩展Python包，实现多个未被实现的随机约束随机优化算法。

Result: 开发出humancompatible.train包，并在公平约束的深度学习任务中对比了两种算法。

Conclusion: 提出的humancompatible.train包可用于带随机约束的DNN训练。

Abstract: There has been a considerable interest in constrained training of deep neural
networks (DNNs) recently for applications such as fairness and safety. Several
toolkits have been proposed for this task, yet there is still no industry
standard. We present humancompatible.train
(https://github.com/humancompatible/train), an easily-extendable PyTorch-based
Python package for training DNNs with stochastic constraints. We implement
multiple previously unimplemented algorithms for stochastically constrained
stochastic optimization. We demonstrate the toolkit use by comparing two
algorithms on a deep learning task with fairness constraints.

</details>


### [172] [A Causality-Aware Spatiotemporal Model for Multi-Region and Multi-Pollutant Air Quality Forecasting](https://arxiv.org/abs/2509.21260)
*Junxin Lu,Shiliang Sun*

Main category: cs.LG

TL;DR: 提出AirPCM模型用于空气污染预测，优于现有方法，能提供未来空气质量趋势见解。


<details>
  <summary>Details</summary>
Motivation: 空气污染是全球性问题，现有预测方法在多污染物、多区域预测上有局限，需准确可扩展的预测方法。

Method: 提出AirPCM模型，整合多区域、多污染物动态，显式建模气象 - 污染物因果关系，用统一架构捕捉多种关联。

Result: 在多尺度真实数据集上评估，AirPCM在预测准确性和泛化能力上超过现有方法。

Conclusion: AirPCM能提供未来空气质量趋势和高风险窗口见解，支持环境治理和碳减排规划。

Abstract: Air pollution, a pressing global problem, threatens public health,
environmental sustainability, and climate stability. Achieving accurate and
scalable forecasting across spatially distributed monitoring stations is
challenging due to intricate multi-pollutant interactions, evolving
meteorological conditions, and region specific spatial heterogeneity. To
address this challenge, we propose AirPCM, a novel deep spatiotemporal
forecasting model that integrates multi-region, multi-pollutant dynamics with
explicit meteorology-pollutant causality modeling. Unlike existing methods
limited to single pollutants or localized regions, AirPCM employs a unified
architecture to jointly capture cross-station spatial correlations, temporal
auto-correlations, and meteorology-pollutant dynamic causality. This empowers
fine-grained, interpretable multi-pollutant forecasting across varying
geographic and temporal scales, including sudden pollution episodes. Extensive
evaluations on multi-scale real-world datasets demonstrate that AirPCM
consistently surpasses state-of-the-art baselines in both predictive accuracy
and generalization capability. Moreover, the long-term forecasting capability
of AirPCM provides actionable insights into future air quality trends and
potential high-risk windows, offering timely support for evidence-based
environmental governance and carbon mitigation planning.

</details>


### [173] [It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL](https://arxiv.org/abs/2509.21282)
*Madeleine Dwyer,Adam Sobey,Adriane Chapman*

Main category: cs.LG

TL;DR: 提出概率平滑策略优化（PSPO）方法，在GRPO中应用（GR - PSPO）微调Qwen2.5模型，相比未裁剪和裁剪的GRPO有更好表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习训练大语言模型的方法中使用的比率裁剪会丢弃信息并引入梯度不连续性，需要改进。

Method: 提出PSPO方法，在计算重要性比率前将当前策略的概率向旧策略平滑，在GRPO中实例化PSPO（GR - PSPO），并在GSM8K上微调Qwen2.5模型。

Result: 与未裁剪GRPO性能相近但推理更好，相比裁剪GRPO，在0.5B和1.5B模型上性能显著提升，在GSM8K上提升超20%。

Conclusion: PSPO方法有效，能在训练大语言模型时避免裁剪带来的问题，提升模型性能。

Abstract: Training large language models (LLMs) with reinforcement learning (RL)
methods such as PPO and GRPO commonly relies on ratio clipping to stabilise
updates. While effective at preventing instability, clipping discards
information and introduces gradient discontinuities. We propose Probability
Smoothing Policy Optimisation (PSPO), which smooths the current policy's
probabilities toward the old (behaviour) policy before computing the importance
ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient
signal, while interpolation toward the old policy creates a soft trust region
that discourages large, destabilising updates, with formal guarantees.
  We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and
Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset
generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO
(single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar
performance but improves the reasoning leading to clearer and more concise
responses which are more logical. Compared to clipped GRPO, GR-PSPO
substantially improves performance both the 0.5B and 1.5B models, with a boost
of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).

</details>


### [174] [Optimal Robust Recourse with $L^p$-Bounded Model Change](https://arxiv.org/abs/2509.21293)
*Phone Kyaw,Kshitij Kayastha,Shahin Jabbari*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recourse provides individuals who received undesirable labels (e.g., denied a
loan) from algorithmic decision-making systems with a minimum-cost improvement
suggestion to achieve the desired outcome. However, in practice, models often
get updated to reflect changes in the data distribution or environment,
invalidating the recourse recommendations (i.e., following the recourse will
not lead to the desirable outcome). The robust recourse literature addresses
this issue by providing a framework for computing recourses whose validity is
resilient to slight changes in the model. However, since the optimization
problem of computing robust recourse is non-convex (even for linear models),
most of the current approaches do not have any theoretical guarantee on the
optimality of the recourse. Recent work by Kayastha et. al. provides the first
provably optimal algorithm for robust recourse with respect to generalized
linear models when the model changes are measured using the $L^{\infty}$ norm.
However, using the $L^{\infty}$ norm can lead to recourse solutions with a high
price. To address this shortcoming, we consider more constrained model changes
defined by the $L^p$ norm, where $p\geq 1$ but $p\neq \infty$, and provide a
new algorithm that provably computes the optimal robust recourse for
generalized linear models. Empirically, for both linear and non-linear models,
we demonstrate that our algorithm achieves a significantly lower price of
recourse (up to several orders of magnitude) compared to prior work and also
exhibits a better trade-off between the implementation cost of recourse and its
validity. Our empirical analysis also illustrates that our approach provides
more sparse recourses compared to prior work and remains resilient to
post-processing approaches that guarantee feasibility.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [175] [ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation](https://arxiv.org/abs/2509.20380)
*Samyak Jhaveri,Vanessa Klotzmann,Crista Lopes*

Main category: cs.SE

TL;DR: 介绍ACCeLLiuM，两个微调的大语言模型用于生成OpenACC指令，实验显示微调模型表现更好，公开代码等以建立基准和降低GPU编程门槛。


<details>
  <summary>Details</summary>
Motivation: GPU硬件和编程框架复杂度增加，使用指令式并行编程标准仍需专业知识，需简化GPU编程。

Method: 引入ACCeLLiuM模型，使用从GitHub C/C++仓库挖掘的数据集进行监督微调。

Result: 微调模型在生成OpenACC指令上表现优于基础模型，87%生成有效指令，50%生成精确指令，非精确指令也有实用价值。

Conclusion: 公开代码、模型和数据集，建立可复现基准，降低串行程序GPU卸载的障碍。

Abstract: The increasing ubiquity of GPUs is accompanied by the increasing complexity
of their hardware and parallel programming frameworks. Directive-based parallel
programming standards like OpenACC simplify GPU programming to some extent by
abstracting away low-level complexities, but a fair amount of expertise is
still required in order to use those directives effectively.
  We introduce ACCeLLiuM, two open weights Large Language Models specifically
fine-tuned for generating expert OpenACC directives for data-parallel loops,
along with the supervised fine-tuning dataset that was used to train them. The
ACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from
public GitHub C/C++ repositories, with 3,223 pairs for training and 810 for
testing. Experimental evaluations show a pronounced performance gap in
generating correct OpenACC pragmas between base LLMs and our fine-tuned
versions. On the held-out test set, base LLMs fail to consistently generate
valid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid
pragmas with the correct directive type for $87\%$ of the data-parallel loops,
and exact pragmas--including directives, clauses, clause order, and clause
variables--for $50\%$ of the cases. Even when not exact, generated pragmas
frequently incorporate the correct clauses in a different order than the
ground-truth label, or include additional clauses that enable finer control
over parallel execution, data movement, and concurrency, offering practical
value beyond strict string-matching. By publicly releasing the code, models,
and dataset as ACCeLLiuM we hope to establish a reproducible benchmark for
LLM-powered OpenACC pragma generation, and lower the barrier to automated GPU
offloading of serially written programs.

</details>


### [176] [State-of-the-Art in Software Security Visualization: A Systematic Review](https://arxiv.org/abs/2509.20385)
*Ishara Devendra,Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: 文章系统回顾软件安全可视化研究，创建技术分类，分析关键问题、进展与未来方向，强调创新技术必要性。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂度增加和威胁环境变化，传统文本和数值分析方法失效，需系统回顾和分类软件安全可视化技术。

Method: 对60多篇软件安全可视化研究论文进行系统回顾。

Result: 将技术分为四类，突出广泛软件开发可视化的两个主要领域。

Conclusion: 需要适应安全环境变化的创新可视化技术，对威胁检测、安全响应和未来研究有实际意义。

Abstract: Software security visualization is an interdisciplinary field that combines
the technical complexity of cybersecurity, including threat intelligence and
compliance monitoring, with visual analytics, transforming complex security
data into easily digestible visual formats. As software systems get more
complex and the threat landscape evolves, traditional text-based and numerical
methods for analyzing and interpreting security concerns become increasingly
ineffective. The purpose of this paper is to systematically review existing
research and create a comprehensive taxonomy of software security visualization
techniques through literature, categorizing these techniques into four types:
graph-based, notation-based, matrix-based, and metaphor-based visualization.
This systematic review explores over 60 recent key research papers in software
security visualization, highlighting its key issues, recent advancements, and
prospective future research directions. From the comprehensive analysis, the
two main areas were distinctly highlighted as extensive software development
visualization, focusing on advanced methods for depicting software
architecture: operational security visualization and cybersecurity
visualization. The findings highlight the necessity for innovative
visualization techniques that adapt to the evolving security landscape, with
practical implications for enhancing threat detection, improving security
response strategies, and guiding future research.

</details>


### [177] [Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments](https://arxiv.org/abs/2509.20386)
*Nishant Gaurav,Adit Akarsh,Ankit Ranjan,Manoj Bajaj*

Main category: cs.SE

TL;DR: 提出Dynamic ReAct方法使ReAct智能体处理超上下文限制的MCP工具集，评估五种架构，实现低计算开销选工具，减少工具加载同时保持任务完成准确率。


<details>
  <summary>Details</summary>
Motivation: 解决含大量工具环境中，ReAct智能体处理超语言模型上下文限制的MCP工具集时的工具选择难题，且同时加载所有工具计算不可行。

Method: 提出并评估五种不同架构逐步优化工具选择过程，最终采用搜索加载机制。

Result: 该方法最多减少50%的工具加载，同时保持任务完成准确率。

Conclusion: 该方法推动了通用AI智能体动态适应不同任务环境的发展。

Abstract: We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-
ficiently operate with extensive Model Control Protocol (MCP) tool sets that
exceed the contextual memory limitations of large language models. Our approach
addresses the fundamental challenge of tool selection in environments
containing hundreds or thousands of available tools, where loading all tools
simultaneously is computationally infeasible. We propose and evaluate five
distinct architectures that progressively refine the tool selection process,
culminating in a search-and-load mechanism that achieves intelligent tool
selection with minimal computational overhead. Our experimental results
demonstrate that the proposed approach reduces tool loading by up to 50% while
maintaining task completion accuracy, advancing the path towards truly
general-purpose AI agents capable of dynamically adapting to diverse task
environments.

</details>


### [178] [Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper](https://arxiv.org/abs/2509.20387)
*Qusai Ramadan,Jukka Ruohonen,Abhishek Tiwari,Adam Alami,Zeyd Boukhers*

Main category: cs.SE

TL;DR: 现有软件系统决策可能存在歧视问题，以往研究忽略公平需求规范与验证，本文提出基于知识图谱的公平性框架并探讨相关挑战、问题和路线图。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视软件系统歧视问题源于公平需求缺乏明确规范和验证，且专家公平知识隐性化使规范需求困难，需形式化机制解决。

Method: 提出基于知识图谱的公平性框架。

Result: 文中讨论了相关挑战、研究问题和解决问题的路线图。

Conclusion: 未明确提及，但暗示基于知识图谱的框架有望解决公平需求规范和验证问题。

Abstract: Decisions suggested by improperly designed software systems might be prone to
discriminate against people based on protected characteristics, such as gender
and ethnicity. Previous studies attribute such undesired behavior to flaws in
algorithmic design or biased data. However, these studies ignore that
discrimination is often the result of a lack of well-specified fairness
requirements and their verification. The fact that experts' knowledge about
fairness is often implicit makes the task of specifying precise and verifiable
fairness requirements difficult. In related domains, such as security
engineering, knowledge graphs have been proven to be effective in formalizing
knowledge to assist requirements specification and verification. To address the
lack of formal mechanisms for specifying and verifying fairness requirements,
we propose the development of a knowledge graph-based framework for fairness.
In this paper, we discuss the challenges, research questions, and a road map
towards addressing the research questions.

</details>


### [179] [Online-Optimized RAG for Tool Use and Function Calling](https://arxiv.org/abs/2509.20415)
*Yu Pan,Xiaocheng Li,Hanzhao Wang*

Main category: cs.SE

TL;DR: 论文提出Online - Optimized RAG解决RAG嵌入不对齐问题，能持续优化检索嵌入，提升工具选择准确性和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决实际应用中因嵌入模型不完善或描述有噪声导致的嵌入不对齐问题，避免错误检索和任务失败。

Method: 提出Online - Optimized RAG框架，利用少量反馈持续调整检索嵌入，采用轻量级在线梯度更新，对基础大语言模型无改动，具有即插即用特性。

Result: 在多种工具使用和文档检索场景中，Online - Optimized RAG持续提升了工具选择准确性和最终任务成功率。

Conclusion: Online - Optimized RAG为构建强大、自改进的RAG系统提供了简单实用的途径。

Abstract: In many applications, retrieval-augmented generation (RAG) drives tool use
and function calling by embedding the (user) queries and matching them to
pre-specified tool/function descriptions. In this paper, we address an
embedding misalignment issue that often arises in practical applications due to
imperfect embedding models or noisy descriptions; such misalignment may lead to
incorrect retrieval and task failure. We introduce Online-Optimized RAG, a
deployment-time framework that continually adapts retrieval embeddings from
live interactions using minimal feedback (e.g., task success). Online-Optimized
RAG applies lightweight online gradient updates with negligible per-query
latency and requires no changes to the underlying LLM. The method is
plug-and-play: it supports both single- and multi-hop tool use, dynamic tool
inventories, and $K$-retrieval with re-ranking. We provide a problem-dependent
theoretical analysis that quantifies how the method's performance depends on
the initialization quality of the embeddings and other related quantities.
Across diverse tool-use and document-retrieval scenarios, our Online-Optimized
RAG consistently improves tool selection accuracy and end-task success, thus
providing a simple, practical path to robust, self-improving RAG systems.

</details>


### [180] [Formal Verification of Legal Contracts: A Translation-based Approach](https://arxiv.org/abs/2509.20421)
*Reiner Hähnle,Cosimo Laneve,Adele Veschetti*

Main category: cs.SE

TL;DR: 本文介绍通过将Stipula合约转换为Java代码并利用KeY工具进行形式化验证的方法。


<details>
  <summary>Details</summary>
Motivation: 对Stipula合约的正确性进行形式化验证。

Method: 将Stipula合约转换为带有Java Modeling Language规范注释的Java代码，使用演绎验证工具KeY作为验证后端。

Result: 对于具有不相交循环的Stipula合约子集，翻译和部分及完全正确性验证可全自动完成。

Conclusion: 通用的演绎验证工具可成功用于翻译方法中。

Abstract: Stipula is a domain-specific programming language designed to model legal
contracts with enforceable properties, especially those involving asset
transfers and obligations. This paper presents a methodology to formally verify
the correctness of Stipula contracts through translation into Java code
annotated with Java Modeling Language specifications. As a verification
backend, the deductive verification tool KeY is used. Both, the translation and
the verification of partial and total correctness for a large subset of Stipula
contracts, those with disjoint cycles, is fully automatic. Our work
demonstrates that a general-purpose deductive verification tool can be used
successfully in a translation approach.

</details>


### [181] [AI-Specific Code Smells: From Specification to Detection](https://arxiv.org/abs/2509.20491)
*Brahim Mahmoudi,Naouel Moha,Quentin Stievenert,Florent Avellaneda*

Main category: cs.SE

TL;DR: 本文介绍了针对AI特定代码异味的检测工具SpecDetect4AI，通过指定规则实现对AI系统代码异味的检测，评估显示其性能优于现有工具。


<details>
  <summary>Details</summary>
Motivation: AI系统带来新的软件问题，现有检测工具常遗漏AI特定代码异味，需要新的检测方法。

Method: 引入SpecDetect4AI，结合高级声明式DSL进行规则指定，使用可扩展静态分析工具解释和检测规则。

Result: 指定了22种AI特定代码异味，在826个AI系统（2000万行代码）上评估，精度达88.66%，召回率达88.89%，优于现有工具。

Conclusion: SpecDetect4AI能通过专用规则支持AI特定代码异味的指定和检测，可有效分析大型AI系统，兼具效率和可扩展性（SUS 81.7/100）。

Abstract: The rise of Artificial Intelligence (AI) is reshaping how software systems
are developed and maintained. However, AI-based systems give rise to new
software issues that existing detection tools often miss. Among these, we focus
on AI-specific code smells, recurring patterns in the code that may indicate
deeper problems such as unreproducibility, silent failures, or poor model
generalization. We introduce SpecDetect4AI, a tool-based approach for the
specification and detection of these code smells at scale. This approach
combines a high-level declarative Domain-Specific Language (DSL) for rule
specification with an extensible static analysis tool that interprets and
detects these rules for AI-based systems. We specified 22 AI-specific code
smells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),
achieving a precision of 88.66% and a recall of 88.89%, outperforming other
existing detection tools. Our results show that SpecDetect4AI supports the
specification and detection of AI-specific code smells through dedicated rules
and can effectively analyze large AI-based systems, demonstrating both
efficiency and extensibility (SUS 81.7/100).

</details>


### [182] [PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects](https://arxiv.org/abs/2509.20497)
*Ahmed Aljohani,Hyunsook Do*

Main category: cs.SE

TL;DR: 对大语言模型（LLM）特定的自我承认技术债务（SATD）进行大规模实证研究，分析其来源、普遍性和缓解策略，并发布数据集和实用指南。


<details>
  <summary>Details</summary>
Motivation: LLM通过API集成到软件中带来了特定的SATD问题，需要研究其相关情况。

Method: 分析93,142个Python文件，研究不同LLM API的SATD情况，探索易产生债务的提示技术。

Result: 54.49%的SATD实例来自OpenAI集成，12.35%来自LangChain使用；提示设计是主要来源，指令和少样本提示易产生债务。

Conclusion: 发布全面的SATD数据集，提供管理LLM系统技术债务的实用指导。

Abstract: Large Language Models (LLMs) are increasingly embedded in software via APIs
like OpenAI, offering powerful AI features without heavy infrastructure. Yet
these integrations bring their own form of self-admitted technical debt (SATD).
In this paper, we present the first large-scale empirical study of LLM-specific
SATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142
Python files across major LLM APIs, we found that 54.49% of SATD instances stem
from OpenAI integrations and 12.35% from LangChain use. Prompt design emerged
as the primary source of LLM-specific SATD, with 6.61% of debt related to
prompt configuration and optimization issues, followed by hyperparameter tuning
and LLM-framework integration. We further explored which prompt techniques
attract the most debt, revealing that instruction-based prompts (38.60%) and
few-shot prompts (18.13%) are particularly vulnerable due to their dependence
on instruction clarity and example quality. Finally, we release a comprehensive
SATD dataset to support reproducibility and offer practical guidance for
managing technical debt in LLM-powered systems.

</details>


### [183] [Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact](https://arxiv.org/abs/2509.20518)
*Sayed Mahbub Hasan Amiri,Md Mainul Islam*

Main category: cs.SE

TL;DR: 本文介绍基于AI - Python的聊天机器人，用于辅助学生编程学习，结合多种技术，经评估效果良好，为AI教育工具提供蓝图。


<details>
  <summary>Details</summary>
Motivation: 传统编码工具缺乏机器人帮助，AI代码助手侧重于完成任务，为填补此空白开发聊天机器人促进学生编程学习。

Method: 结合静态代码分析、动态执行跟踪和大语言模型，采用CodeLlama进行代码嵌入、GPT - 4进行自然语言交互、基于Docker的沙箱进行安全执行，通过混合方法评估。

Result: 系统错误解决成功率达85%，优于pylint和GPT - 4；用户调试时间减少59.3%，编码熟练度提高34%；学生反馈有积极影响，但存在偶尔延迟和代码清理限制问题。

Conclusion: 研究平衡技术创新和教学关怀，为优先考虑教育公平和长期技能保留的AI工具提供蓝图，表明AI可增强编程教育中的人类教学。

Abstract: This is the study that presents an AI-Python-based chatbot that helps
students to learn programming by demonstrating solutions to such problems as
debugging errors, solving syntax problems or converting abstract theoretical
concepts to practical implementations. Traditional coding tools like Integrated
Development Environments (IDEs) and static analyzers do not give robotic help
while AI-driven code assistants such as GitHub Copilot focus on getting things
done. To close this gap, our chatbot combines static code analysis, dynamic
execution tracing, and large language models (LLMs) to provide the students
with relevant and practical advice, hence promoting the learning process. The
chatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for
natural language interactions, and Docker-based sandboxing for secure
execution. Evaluated through a mixed-methods approach involving 1,500 student
submissions, the system demonstrated an 85% error resolution success rate,
outperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative
results revealed a 59.3% reduction in debugging time among users, with pre- and
post-test assessments showing a 34% improvement in coding proficiency,
particularly in recursion and exception handling. Qualitative feedback from 120
students highlighted the chatbots clarity, accessibility, and
confidence-building impact, though critiques included occasional latency and
restrictive code sanitization. By balancing technical innovation with
pedagogical empathy, this research provides a blueprint for AI tools that
prioritize educational equity and long-term skill retention over mere code
completion. The chatbot exemplifies how AI can augment human instruction,
fostering deeper conceptual understanding in programming education.

</details>


### [184] [Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework](https://arxiv.org/abs/2509.20552)
*Xinyu Shi,Zhenhao Li,An Ran Chen*

Main category: cs.SE

TL;DR: 提出FaR - Loc框架结合LLM与RAG增强方法级故障定位，实验表明其优于多种基线方法，含代码结构的预训练模型可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的故障定位方法在复杂系统中因缺乏项目特定知识和难以处理大项目而受限，需改进。

Method: 提出FaR - Loc框架，包含LLM功能提取、语义密集检索和LLM重排序三个关键组件。

Result: 在Defects4J基准测试中，FaR - Loc在Top - 1和Top - 5准确率上优于SoapFL和AutoFL等基线方法，含代码结构的预训练模型可提升故障定位性能。

Conclusion: FaR - Loc框架有效，可用于实际故障定位，并为其应用提供了见解。

Abstract: Fault localization (FL) is a critical but time-consuming task in software
debugging, aiming to identify faulty code elements. While recent advances in
large language models (LLMs) have shown promise for FL, they often struggle
with complex systems due to the lack of project-specific knowledge and the
difficulty of navigating large projects. To address these limitations, we
propose FaR-Loc, a novel framework that enhances method-level FL by integrating
LLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key
components: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM
Re-ranking. First, given a failed test and its associated stack trace, the LLM
Functionality Extraction module generates a concise natural language
description that captures the failing behavior. Next, the Semantic Dense
Retrieval component leverages a pre-trained code-understanding encoder to embed
both the functionality description (natural language) and the covered methods
(code) into a shared semantic space, enabling the retrieval of methods with
similar functional behavior. Finally, the LLM Re-ranking module reorders the
retrieved methods based on their contextual relevance. Our experiments on the
widely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art
LLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by
19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all
learning-based and spectrum-based baselines across all Top-N metrics without
requiring re-training. Furthermore, we find that pre-trained code embedding
models that incorporate code structure, such as UniXcoder, can significantly
improve fault localization performance by up to 49.0% in Top-1 accuracy.
Finally, we conduct a case study to illustrate the effectiveness of FaR-Loc and
to provide insights for its practical application.

</details>


### [185] [Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow](https://arxiv.org/abs/2509.20631)
*Michael Zhang,Yuan Tian,Mariam Guizani*

Main category: cs.SE

TL;DR: 论文提出新颖编程语言主题分类工作流，结合多标签SVM等方法，在数据集上有良好表现，为相关研究和实践提供见解和管道。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统规模和复杂度增加，理解源代码中编程语言主题分布对技术决策、新员工入职、工具和教育很重要。

Method: 结合多标签支持向量机（SVM）、滑动窗口和投票策略，在IBM Project CodeNet数据集上训练。

Result: 模型在各主题平均F1分数达0.90，代码主题高亮达0.75。

Conclusion: 研究为代码分析和数据驱动软件工程的研究者和从业者提供实证见解和可复用管道。

Abstract: As software systems grow in scale and complexity, understanding the
distribution of programming language topics within source code becomes
increasingly important for guiding technical decisions, improving onboarding,
and informing tooling and education. This paper presents the design,
implementation, and evaluation of a novel programming language topic
classification workflow. Our approach combines a multi-label Support Vector
Machine (SVM) with a sliding window and voting strategy to enable fine-grained
localization of core language concepts such as operator overloading, virtual
functions, inheritance, and templates. Trained on the IBM Project CodeNet
dataset, our model achieves an average F1 score of 0.90 across topics and 0.75
in code-topic highlight. Our findings contribute empirical insights and a
reusable pipeline for researchers and practitioners interested in code analysis
and data-driven software engineering.

</details>


### [186] [Exploring Engagement in Hybrid Meetings](https://arxiv.org/abs/2509.20780)
*Daniela Grassi,Fabio Calefato,Darja Smite,Nicole Novielli,Filippo Lanubile*

Main category: cs.SE

TL;DR: 研究通过多模态方法测量软件公司专业人员在混合会议中的参与度，发现现场和远程参与者参与度相当，但远程者在长会议中参与度低，还揭示了影响参与度的因素并给出改进建议。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情后混合工作模式盛行，远程参会可能导致远程团队成员孤立、疏离和参与度降低，因此要通过客观测量识别和描述混合会议中的参与模式。

Method: 对三家软件公司的专业人员进行数周研究，采用多模态方法，通过自报告问卷和生物识别设备进行生理测量来收集数据。

Result: 回归分析显示现场和远程参与者参与度相当，但远程参与者在长会议中参与度较低；积极角色与高参与度正相关，大型会议和下午会议与低参与度相关。

Conclusion: 研究结果揭示了混合会议中参与和不参与的相关因素，给出了会议改进建议，对各行业知识密集型组织有参考价值。

Abstract: Background. The widespread adoption of hybrid work following the COVID-19
pandemic has fundamentally transformed software development practices,
introducing new challenges in communication and collaboration as organizations
transition from traditional office-based structures to flexible working
arrangements. This shift has established a new organizational norm where even
traditionally office-first companies now embrace hybrid team structures. While
remote participation in meetings has become commonplace in this new
environment, it may lead to isolation, alienation, and decreased engagement
among remote team members. Aims. This study aims to identify and characterize
engagement patterns in hybrid meetings through objective measurements, focusing
on the differences between co-located and remote participants. Method. We
studied professionals from three software companies over several weeks,
employing a multimodal approach to measure engagement. Data were collected
through self-reported questionnaires and physiological measurements using
biometric devices during hybrid meetings to understand engagement dynamics.
Results. The regression analyses revealed comparable engagement levels between
onsite and remote participants, though remote participants show lower
engagement in long meetings regardless of participation mode. Active roles
positively correlate with higher engagement, while larger meetings and
afternoon sessions are associated with lower engagement. Conclusions. Our
results offer insights into factors associated with engagement and
disengagement in hybrid meetings, as well as potential meeting improvement
recommendations. These insights are potentially relevant not only for software
teams but also for knowledge-intensive organizations across various sectors
facing similar hybrid collaboration challenges.

</details>


### [187] [Verification Limits Code LLM Training](https://arxiv.org/abs/2509.20837)
*Srishti Gureja,Elena Tommasone,Jingyi He,Sara Hooker,Matthias Gallé,Marzieh Fadaee*

Main category: cs.SE

TL;DR: 研究代码生成中合成数据验证瓶颈，提出优化验证设计与策略以突破验证上限。


<details>
  <summary>Details</summary>
Motivation: 合成数据用于代码生成时存在验证上限问题，需研究验证设计和策略对模型性能的影响。

Method: 系统研究验证设计和策略，分析测试复杂度和数量、探索宽松通过阈值、进行正确与错误解对比及人工评估。

Result: 丰富测试套件提升代码生成能力；宽松阈值或软验证可提升性能；保留多样正确解有泛化增益。

Conclusion: 当前验证方式太严格，不能丢弃，需重新校准，结合校准验证与多样问题 - 解决方案对可突破验证上限。

Abstract: Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.

</details>


### [188] [PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval](https://arxiv.org/abs/2509.20881)
*Yixuan Li,Xinyi Liu,Weidong Yang,Ben Fei,Shuhao Li,Mingjie Zhou,Lipeng Ma*

Main category: cs.SE

TL;DR: 提出PseudoBridge代码检索框架，引入伪代码作中间模态，经实验验证其在检索准确性和泛化性上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练语言模型的代码搜索方法存在人类意图与机器执行逻辑的语义差距以及对不同代码风格鲁棒性有限的问题。

Method: 提出PseudoBridge框架，分两阶段：先用大语言模型合成伪代码实现自然语言查询与伪代码的显式对齐；再引入逻辑不变的代码风格增强策略，用大语言模型生成风格多样但逻辑等价的代码实现，将不同风格代码片段与伪代码对齐。

Result: 在10种不同预训练语言模型上构建PseudoBridge，在6种主流编程语言上评估，实验表明其始终优于基线，在检索准确性和泛化性上有显著提升，尤其是在零样本领域转移场景中。

Conclusion: 通过伪代码进行显式逻辑对齐有效，PseudoBridge有潜力成为代码检索的鲁棒、通用解决方案。

Abstract: Code search aims to precisely find relevant code snippets that match natural
language queries within massive codebases, playing a vital role in software
development. Recent advances leverage pre-trained language models (PLMs) to
bridge the semantic gap between unstructured natural language (NL) and
structured programming languages (PL), yielding significant improvements over
traditional information retrieval and early deep learning approaches. However,
existing PLM-based methods still encounter key challenges, including a
fundamental semantic gap between human intent and machine execution logic, as
well as limited robustness to diverse code styles. To address these issues, we
propose PseudoBridge, a novel code retrieval framework that introduces
pseudo-code as an intermediate, semi-structured modality to better align NL
semantics with PL logic. Specifically, PseudoBridge consists of two stages.
First, we employ an advanced large language model (LLM) to synthesize
pseudo-code, enabling explicit alignment between NL queries and pseudo-code.
Second, we introduce a logic-invariant code style augmentation strategy and
employ the LLM to generate stylistically diverse yet logically equivalent code
implementations with pseudo-code, then align the code snippets of different
styles with pseudo-code, enhancing model robustness to code style variation. We
build PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream
programming languages. Extensive experiments demonstrate that PseudoBridge
consistently outperforms baselines, achieving significant gains in retrieval
accuracy and generalization, particularly under zero-shot domain transfer
scenarios such as Solidity and XLCoST datasets. These results demonstrate the
effectiveness of explicit logical alignment via pseudo-code and highlight
PseudoBridge's potential as a robust, generalizable solution for code
retrieval.

</details>


### [189] [Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool](https://arxiv.org/abs/2509.21067)
*Oka Kurniawan,Erick Chandra,Christopher M. Poskitt,Yannic Noller,Kenny Tsu Wei Choo,Cyrille Jegourel*

Main category: cs.SE

TL;DR: 本文设计调试助手CodeHinter结合传统调试工具与大语言模型技术，经学生测试效果好，得出AI调试工具应基于用户画像个性化的结论。


<details>
  <summary>Details</summary>
Motivation: 现有调试工具让学生过度依赖AI且未让学生积极参与调试过程，需设计新的调试助手。

Method: 设计调试助手CodeHinter并进行第二次设计迭代，用本科生群体进行测试。

Result: 学生认为该工具解决语义错误高效，比第一版更易用，错误定位是最有价值的功能。

Conclusion: AI辅助调试工具应基于用户画像进行个性化以优化与学生的交互。

Abstract: Debugging is a fundamental skill that novice programmers must develop.
Numerous tools have been created to assist novice programmers in this process.
Recently, large language models (LLMs) have been integrated with automated
program repair techniques to generate fixes for students' buggy code. However,
many of these tools foster an over-reliance on AI and do not actively engage
students in the debugging process. In this work, we aim to design an intuitive
debugging assistant, CodeHinter, that combines traditional debugging tools with
LLM-based techniques to help novice debuggers fix semantic errors while
promoting active engagement in the debugging process. We present findings from
our second design iteration, which we tested with a group of undergraduate
students. Our results indicate that the students found the tool highly
effective in resolving semantic errors and significantly easier to use than the
first version. Consistent with our previous study, error localization was the
most valuable feature. Finally, we conclude that any AI-assisted debugging tool
should be personalized based on user profiles to optimize their interactions
with students.

</details>


### [190] [An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI](https://arxiv.org/abs/2509.21068)
*Nek Dil Khan,Javed Ali Khan,Mobashir Husain,Muhammad Sohail Khan,Arif Ali Khan,Muhammad Azeem Akbar,Shahid Hussain*

Main category: cs.SE

TL;DR: 本文围绕量子软件工程（QSE）展开，提取Q&A平台问题分类挑战，用多种算法分类，Transformer算法表现优，还应用SHAP提升可解释性，但需开发者和供应商实证评估。


<details>
  <summary>Details</summary>
Motivation: 量子开发者在优化量子计算和QSE概念时面临挑战，分类问题有助于识别常见挑战。

Method: 从Q&A平台提取问题，用内容分析和扎根理论标注讨论，用ChatGPT验证，用多种算法分类，应用SHAP增强模型可解释性。

Result: Transformer算法平均准确率95%，优于D&ML算法，SHAP揭示语言特征驱动预测。

Conclusion: 研究成果可助量子供应商和论坛优化讨论组织，但需开发者和供应商的实证评估。

Abstract: Quantum Software Engineering (QSE) is a research area practiced by tech
firms. Quantum developers face challenges in optimizing quantum computing and
QSE concepts. They use Stack Overflow (SO) to discuss challenges and label
posts with specialized quantum tags, which often refer to technical aspects
rather than developer posts. Categorizing questions based on quantum concepts
can help identify frequent QSE challenges. We conducted studies to classify
questions into various challenges. We extracted 2829 questions from Q&A
platforms using quantum-related tags. Posts were analyzed to identify frequent
challenges and develop a novel grounded theory. Challenges include Tooling,
Theoretical, Learning, Conceptual, Errors, and API Usage. Through content
analysis and grounded theory, discussions were annotated with common challenges
to develop a ground truth dataset. ChatGPT validated human annotations and
resolved disagreements. Fine-tuned transformer algorithms, including BERT,
DistilBERT, and RoBERTa, classified discussions into common challenges. We
achieved an average accuracy of 95% with BERT DistilBERT, compared to
fine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward
Neural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term
Memory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,
respectively. The Transformer-based approach outperforms the D&ML-based
approach with a 6\% increase in accuracy by processing actual discussions,
i.e., without data augmentation. We applied SHAP (SHapley Additive
exPlanations) for model interpretability, revealing how linguistic features
drive predictions and enhancing transparency in classification. These findings
can help quantum vendors and forums better organize discussions for improved
access and readability. However,empirical evaluation studies with actual
developers and vendors are needed.

</details>


### [191] [Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach](https://arxiv.org/abs/2509.21170)
*Yongda Yu,Guohao Shi,Xianwei Wu,Haochuan He,XueMing Gu,Qianqian Zhao,Kui Liu,Qiushi Wang,Zhao Tian,Haifeng Shen,Guoping Rong*

Main category: cs.SE

TL;DR: 本文提出MelcotCR方法，结合最大熵建模原则与预定义推理路径，用长思维链技术微调大语言模型以分析代码审查多维度，在数据集上验证其能让低参数模型超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有用代码审查数据微调大语言模型的方法因微调信息有限或模糊，未能充分发挥潜力，且大语言模型处理长思维链提示时存在上下文丢失和推理逻辑丢失问题。

Method: 提出MelcotCR方法，采用长思维链技术为大语言模型训练提供丰富结构化信息，并结合最大熵建模原则与预定义推理路径解决长思维链提示处理问题。

Result: 在MelcotCR数据集和CodeReviewer数据集上，用MelcotCR微调的低参数14B Qwen2.5模型在检测和描述代码问题的准确性上超越现有方法，性能与671B DeepSeek - R1模型相当。

Conclusion: MelcotCR方法有效，能让低参数大语言模型在代码审查任务中取得优异表现。

Abstract: Large Language Models (LLMs) have shown great potential in supporting
automated code review due to their impressive capabilities in context
understanding and reasoning. However, these capabilities are still limited
compared to human-level cognition because they are heavily influenced by the
training data. Recent research has demonstrated significantly improved
performance through fine-tuning LLMs with code review data. However, compared
to human reviewers who often simultaneously analyze multiple dimensions of code
review to better identify issues, the full potential of these methods is
hampered by the limited or vague information used to fine-tune the models. This
paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that
trains LLMs with an impressive reasoning ability to analyze multiple dimensions
of code review by harnessing long COT techniques to provide rich structured
information. To address context loss and reasoning logic loss issues that
frequently occur when LLMs process long COT prompts, we propose a solution that
combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning
pathways in MelcotCR to enable more effective utilization of in-context
knowledge within long COT prompts while strengthening the logical tightness of
the reasoning process. Empirical evaluations on our curated MelcotCR dataset
and the public CodeReviewer dataset reveal that a low-parameter base model,
such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art
methods in terms of the accuracy of detecting and describing code issues, with
its performance remarkably on par with that of the 671B DeepSeek-R1 model.

</details>


### [192] [Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform](https://arxiv.org/abs/2509.21292)
*Ronivaldo Ferreira,Guilherme da Silva,Carla Rocha,Gustavo Pinto*

Main category: cs.SE

TL;DR: 提出结合BERTopic、种子词和大语言模型自动验证的方法处理公民在数字平台的大量贡献，结果显示生成主题连贯且符合机构要求，可将公民输入转化为公共政策可用数据。


<details>
  <summary>Details</summary>
Motivation: 政府推广数字平台公民参与，但大量贡献因组织困难未充分利用，如手动分类不可行、需专家参与、要与官方分类法对齐。

Method: 结合BERTopic与种子词，并通过大语言模型进行自动验证。

Result: 生成的主题连贯且符合机构要求，所需人力极少。

Conclusion: 该方法能让政府将大量公民输入转化为公共政策的可行动数据。

Abstract: Promoting participation on digital platforms such as Brasil Participativo has
emerged as a top priority for governments worldwide. However, due to the sheer
volume of contributions, much of this engagement goes underutilized, as
organizing it presents significant challenges: (1) manual classification is
unfeasible at scale; (2) expert involvement is required; and (3) alignment with
official taxonomies is necessary. In this paper, we introduce an approach that
combines BERTopic with seed words and automatic validation by large language
models. Initial results indicate that the generated topics are coherent and
institutionally aligned, with minimal human effort. This methodology enables
governments to transform large volumes of citizen input into actionable data
for public policy.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [193] [Sample completion, structured correlation, and Netflix problems](https://arxiv.org/abs/2509.20404)
*Leonardo N. Coregliano,Maryanthe Malliaris*

Main category: stat.ML

TL;DR: 开发新的高维统计学习模型，用VCN${}_{k,k}$ - 维度刻画可学习性，并解释2006年Netflix Prize竞赛中某些算法的成功。


<details>
  <summary>Details</summary>
Motivation: 开发能在有随机性情况下利用数据结构相关性的高维统计学习模型，并为特定算法成功提供理论解释。

Method: 用VCN${}_{k,k}$ - 维度（本质上是Shelah分类理论中的$k$ - 依赖性）来刻画新模型的可学习性。

Result: 成功开发新模型并完成对其可学习性的刻画。

Conclusion: 新模型能解释2006年Netflix Prize竞赛中某些算法的成功。

Abstract: We develop a new high-dimensional statistical learning model which can take
advantage of structured correlation in data even in the presence of randomness.
We completely characterize learnability in this model in terms of
VCN${}_{k,k}$-dimension (essentially $k$-dependence from Shelah's
classification theory). This model suggests a theoretical explanation for the
success of certain algorithms in the 2006~Netflix Prize competition.

</details>


### [194] [Fast Estimation of Wasserstein Distances via Regression on Sliced Wasserstein Distances](https://arxiv.org/abs/2509.20508)
*Khai Nguyen,Hai Nguyen,Nhat Ho*

Main category: stat.ML

TL;DR: 本文提出基于切片Wasserstein距离回归的快速估计方法计算Wasserstein距离，在多任务和多数据集上验证优于现有模型，还能加速Wormhole训练。


<details>
  <summary>Details</summary>
Motivation: 解决从元分布中抽取的多对分布的Wasserstein距离的高效计算问题。

Method: 提出基于切片Wasserstein距离回归的快速估计方法，引入无约束和有约束两个线性模型。

Result: 在多个任务和数据集上验证，比现有Wasserstein嵌入模型有更好的近似效果，尤其在低数据情况下；能加速Wormhole训练得到RG - Wormhole。

Conclusion: 所提方法能高效计算Wasserstein距离，有更好的近似效果且可加速训练。

Abstract: We address the problem of efficiently computing Wasserstein distances for
multiple pairs of distributions drawn from a meta-distribution. To this end, we
propose a fast estimation method based on regressing Wasserstein distance on
sliced Wasserstein (SW) distances. Specifically, we leverage both standard SW
distances, which provide lower bounds, and lifted SW distances, which provide
upper bounds, as predictors of the true Wasserstein distance. To ensure
parsimony, we introduce two linear models: an unconstrained model with a
closed-form least-squares solution, and a constrained model that uses only half
as many parameters. We show that accurate models can be learned from a small
number of distribution pairs. Once estimated, the model can predict the
Wasserstein distance for any pair of distributions via a linear combination of
SW distances, making it highly efficient. Empirically, we validate our approach
on diverse tasks, including Gaussian mixtures, point-cloud classification, and
Wasserstein-space visualizations for 3D point clouds. Across various datasets
such as MNIST point clouds, ShapeNetV2, MERFISH Cell Niches, and scRNA-seq, our
method consistently provides a better approximation of Wasserstein distance
than the state-of-the-art Wasserstein embedding model, Wasserstein Wormhole,
particularly in low-data regimes. Finally, we demonstrate that our estimator
can also accelerate Wormhole training, yielding \textit{RG-Wormhole}.

</details>


### [195] [Unsupervised Domain Adaptation with an Unobservable Source Subpopulation](https://arxiv.org/abs/2509.20587)
*Chao Ying,Jun Jin,Haotian Zhang,Qinglong Tian,Yanyuan Ma,Yixuan Li,Jiwei Zhao*

Main category: stat.ML

TL;DR: 研究无监督域适应问题，源域有不可观测子群体，提出分布匹配法估计子群体比例，实验显示方法优于基准。


<details>
  <summary>Details</summary>
Motivation: 解决源域存在不可观测子群体时，忽略该群体导致的有偏估计和预测性能下降问题。

Method: 严格推导目标域背景特定和整体预测模型，提出分布匹配法估计子群体比例。

Result: 理论上给出估计器渐近行为保证和预测误差上界，实验表明方法优于不考虑不可观测子群体的基准。

Conclusion: 即使源域存在不可观测子群体，目标域的预测仍可恢复，所提方法有效。

Abstract: We study an unsupervised domain adaptation problem where the source domain
consists of subpopulations defined by the binary label $Y$ and a binary
background (or environment) $A$. We focus on a challenging setting in which one
such subpopulation in the source domain is unobservable. Naively ignoring this
unobserved group can result in biased estimates and degraded predictive
performance. Despite this structured missingness, we show that the prediction
in the target domain can still be recovered. Specifically, we rigorously derive
both background-specific and overall prediction models for the target domain.
For practical implementation, we propose the distribution matching method to
estimate the subpopulation proportions. We provide theoretical guarantees for
the asymptotic behavior of our estimator, and establish an upper bound on the
prediction error. Experiments on both synthetic and real-world datasets show
that our method outperforms the naive benchmark that does not account for this
unobservable source subpopulation.

</details>


### [196] [A Gapped Scale-Sensitive Dimension and Lower Bounds for Offset Rademacher Complexity](https://arxiv.org/abs/2509.20618)
*Zeyu Jia,Yury Polyanskiy,Alexander Rakhlin*

Main category: stat.ML

TL;DR: 研究函数类在顺序和非顺序设置下的带隙尺度敏感维度，给出覆盖数上界和偏移拉德马赫平均下界。


<details>
  <summary>Details</summary>
Motivation: 在顺序和非顺序设置下研究函数类的带隙尺度敏感维度，拓展已有结果并加强证明收敛速率下界的方法。

Method: 通过研究带隙尺度敏感维度，分析其与覆盖数和偏移拉德马赫平均的关系。

Result: 证明任意一致有界类的覆盖数由带隙维度控制上界，带隙维度能给出偏移拉德马赫平均的下界。

Conclusion: 带隙尺度敏感维度可拓展已有结果，加强统计和在线学习中收敛速率下界的证明方法。

Abstract: We study gapped scale-sensitive dimensions of a function class in both
sequential and non-sequential settings. We demonstrate that covering numbers
for any uniformly bounded class are controlled above by these gapped
dimensions, generalizing the results of
\cite{anthony2000function,alon1997scale}. Moreover, we show that the gapped
dimensions lead to lower bounds on offset Rademacher averages, thereby
strengthening existing approaches for proving lower bounds on rates of
convergence in statistical and online learning.

</details>


### [197] [A Hierarchical Variational Graph Fused Lasso for Recovering Relative Rates in Spatial Compositional Data](https://arxiv.org/abs/2509.20636)
*Joaquim Valerio Teixeira,Ed Reznik,Sudpito Banerjee,Wesley Tansey*

Main category: stat.ML

TL;DR: 本文针对生物成像技术空间数据分析挑战，开发可扩展贝叶斯框架，模拟和真实数据结果显示该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 生物成像技术（如 IMS、IMC）的空间数据分析因竞争采样过程存在挑战，需解决信号卷积问题。

Method: 开发可扩展贝叶斯框架，利用空间信号模式稀疏性恢复分子相对速率，使用重尾图形拉索先验和新型分层变分族，通过自动微分变分推理实现高效推理。

Result: 模拟结果显示该方法在 IMS 中优于现有点估计方法，后验覆盖率高于平均场变分推理技术；真实 IMS 数据结果表明能更好恢复组织解剖结构、去除伪影、检测到标准分析方法遗漏的活跃区域。

Conclusion: 所开发的可扩展贝叶斯框架在生物成像技术空间数据分析中表现良好，优于现有方法。

Abstract: The analysis of spatial data from biological imaging technology, such as
imaging mass spectrometry (IMS) or imaging mass cytometry (IMC), is challenging
because of a competitive sampling process which convolves signals from
molecules in a single pixel. To address this, we develop a scalable Bayesian
framework that leverages natural sparsity in spatial signal patterns to recover
relative rates for each molecule across the entire image. Our method relies on
the use of a heavy-tailed variant of the graphical lasso prior and a novel
hierarchical variational family, enabling efficient inference via automatic
differentiation variational inference. Simulation results show that our
approach outperforms state-of-the-practice point estimate methodologies in IMS,
and has superior posterior coverage than mean-field variational inference
techniques. Results on real IMS data demonstrate that our approach better
recovers the true anatomical structure of known tissue, removes artifacts, and
detects active regions missed by the standard analysis approach.

</details>


### [198] [RAPTOR-GEN: RApid PosTeriOR GENerator for Bayesian Learning in Biomanufacturing](https://arxiv.org/abs/2509.20753)
*Wandi Xu,Wei Xie*

Main category: stat.ML

TL;DR: 提出RAPTOR - GEN框架加速生物制药数字孪生开发，含可解释元模型和贝叶斯后验采样方法，算法有效。


<details>
  <summary>Details</summary>
Motivation: 生物制药制造缺乏快速按需生产生物疗法的灵活性，需从稀疏异构实验数据加速智能数字孪生开发。

Method: 引入RAPTOR - GEN框架，基于多尺度概率知识图，含可解释元模型和高效贝叶斯后验采样方法，开发可控误差算法。

Result: 数值实验证明RAPTOR - GEN算法能揭示生物制造过程潜在调控机制。

Conclusion: RAPTOR - GEN框架可有效加速生物制药智能数字孪生开发。

Abstract: Biopharmaceutical manufacturing is vital to public health but lacks the
agility for rapid, on-demand production of biotherapeutics due to the
complexity and variability of bioprocesses. To overcome this, we introduce
RApid PosTeriOR GENerator (RAPTOR-GEN), a mechanism-informed Bayesian learning
framework designed to accelerate intelligent digital twin development from
sparse and heterogeneous experimental data. This framework is built on a
multi-scale probabilistic knowledge graph (pKG), formulated as a stochastic
differential equation (SDE)-based foundational model that captures the
nonlinear dynamics of bioprocesses. RAPTOR-GEN consists of two ingredients: (i)
an interpretable metamodel integrating linear noise approximation (LNA) that
exploits the structural information of bioprocessing mechanisms and a
sequential learning strategy to fuse heterogeneous and sparse data, enabling
inference of latent state variables and explicit approximation of the
intractable likelihood function; and (ii) an efficient Bayesian posterior
sampling method that utilizes Langevin diffusion (LD) to accelerate posterior
exploration by exploiting the gradients of the derived likelihood. It
generalizes the LNA approach to circumvent the challenge of step size
selection, facilitating robust learning of mechanistic parameters with provable
finite-sample performance guarantees. We develop a fast and robust RAPTOR-GEN
algorithm with controllable error. Numerical experiments demonstrate its
effectiveness in uncovering the underlying regulatory mechanisms of
biomanufacturing processes.

</details>


### [199] [Conditionally Whitened Generative Models for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2509.20928)
*Yanfeng Yang,Siwei Chen,Pingping Hu,Zhaotong Shen,Yingjie Zhang,Zhuoran Sun,Shuai Li,Ziqi Chen,Kenji Fukumizu*

Main category: stat.ML

TL;DR: 提出CW - Gen框架用于多元时间序列概率预测，通过条件白化融入先验信息，实验显示其能提升预测性能、捕捉动态和缓解分布偏移。


<details>
  <summary>Details</summary>
Motivation: 现有扩散和流匹配模型在多元时间序列概率预测中常忽略条件均值和协方差等先验信息。

Method: 提出CW - Gen框架，建立理论条件，设计JMCE同时学习条件均值和滑动窗口协方差，引入CW - Diff并扩展到CW - Flow。

Result: 在五个真实数据集和六个最先进生成模型上实验，CW - Gen提升了预测性能，能有效捕捉非平稳动态和变量间相关性，缓解分布偏移。

Conclusion: CW - Gen通过融入先验信息，在多元时间序列概率预测上优于无先验方法。

Abstract: Probabilistic forecasting of multivariate time series is challenging due to
non-stationarity, inter-variable dependencies, and distribution shifts. While
recent diffusion and flow matching models have shown promise, they often ignore
informative priors such as conditional means and covariances. In this work, we
propose Conditionally Whitened Generative Models (CW-Gen), a framework that
incorporates prior information through conditional whitening. Theoretically, we
establish sufficient conditions under which replacing the traditional terminal
distribution of diffusion models, namely the standard multivariate normal, with
a multivariate normal distribution parameterized by estimators of the
conditional mean and covariance improves sample quality. Guided by this
analysis, we design a novel Joint Mean-Covariance Estimator (JMCE) that
simultaneously learns the conditional mean and sliding-window covariance.
Building on JMCE, we introduce Conditionally Whitened Diffusion Models
(CW-Diff) and extend them to Conditionally Whitened Flow Matching (CW-Flow).
Experiments on five real-world datasets with six state-of-the-art generative
models demonstrate that CW-Gen consistently enhances predictive performance,
capturing non-stationary dynamics and inter-variable correlations more
effectively than prior-free approaches. Empirical results further demonstrate
that CW-Gen can effectively mitigate the effects of distribution shift.

</details>


### [200] [Empirical PAC-Bayes bounds for Markov chains](https://arxiv.org/abs/2509.20985)
*Vahe Karagulyan,Pierre Alquier*

Main category: stat.ML

TL;DR: 本文证明了马尔可夫链的新PAC - Bayes界，得到首个全经验PAC - Bayes界，在模拟实验中经验界与非经验界同样紧密。


<details>
  <summary>Details</summary>
Motivation: 已有PAC和PAC - Bayes界中的常数依赖数据生成过程的属性，实际中这些常数未知，需要新的界。

Method: 证明依赖伪谱隙的新PAC - Bayes界，在状态空间有限时给出伪谱隙的经验界。

Result: 得到首个全经验PAC - Bayes界，在模拟实验中经验界与非经验界同样紧密。

Conclusion: 新的PAC - Bayes界可拓展到有限情况之外，但需要额外假设。

Abstract: The core of generalization theory was developed for independent observations.
Some PAC and PAC-Bayes bounds are available for data that exhibit a temporal
dependence. However, there are constants in these bounds that depend on
properties of the data-generating process: mixing coefficients, mixing time,
spectral gap... Such constants are unknown in practice. In this paper, we prove
a new PAC-Bayes bound for Markov chains. This bound depends on a quantity
called the pseudo-spectral gap. The main novelty is that we can provide an
empirical bound on the pseudo-spectral gap when the state space is finite.
Thus, we obtain the first fully empirical PAC-Bayes bound for Markov chains.
This extends beyond the finite case, although this requires additional
assumptions. On simulated experiments, the empirical version of the bound is
essentially as tight as the non-empirical one.

</details>


### [201] [Best-of-$\infty$ -- Asymptotic Performance of Test-Time Compute](https://arxiv.org/abs/2509.21091)
*Junpei Komiyama,Daisuke Oba,Masafumi Oyamada*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study best-of-$N$ for large language models (LLMs) where the selection is
based on majority voting. In particular, we analyze the limit $N \to \infty$,
which we denote as Best-of-$\infty$. While this approach achieves impressive
performance in the limit, it requires an infinite test-time budget. To address
this, we propose an adaptive generation scheme that selects $N$ based on answer
agreement, thereby efficiently allocating inference-time computation. Beyond
adaptivity, we extend the framework to weighted ensembles of multiple LLMs,
showing that such mixtures can outperform any individual model. The optimal
ensemble weighting is formulated and efficiently computed as a mixed-integer
linear program. Extensive experiments demonstrate the effectiveness of our
approach.

</details>


### [202] [WISER: Segmenting watermarked region - an epidemic change-point perspective](https://arxiv.org/abs/2509.21160)
*Soham Bonnerjee,Sayar Karmakar,Subhrajyoty Roy*

Main category: stat.ML

TL;DR: 本文引入疫情变点视角解决文本水印分割问题，提出WISER算法，理论和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有识别混合源文本中水印片段的方法缺乏可扩展性或理论保证，需新方法。

Method: 从疫情变点角度看待水印分割问题，提出WISER算法，并推导有限样本误差界验证算法。

Result: 大量数值实验表明，WISER在计算速度和准确性上均优于现有基线方法。

Conclusion: WISER是大多数场景下水印定位的有效工具，经典统计问题的见解可解决现代相关问题。

Abstract: With the increasing popularity of large language models, concerns over
content authenticity have led to the development of myriad watermarking
schemes. These schemes can be used to detect a machine-generated text via an
appropriate key, while being imperceptible to readers with no such keys. The
corresponding detection mechanisms usually take the form of statistical
hypothesis testing for the existence of watermarks, spurring extensive research
in this direction. However, the finer-grained problem of identifying which
segments of a mixed-source text are actually watermarked, is much less
explored; the existing approaches either lack scalability or theoretical
guarantees robust to paraphrase and post-editing. In this work, we introduce a
unique perspective to such watermark segmentation problems through the lens of
epidemic change-points. By highlighting the similarities as well as differences
of these two problems, we motivate and propose WISER: a novel, computationally
efficient, watermark segmentation algorithm. We theoretically validate our
algorithm by deriving finite sample error-bounds, and establishing its
consistency in detecting multiple watermarked segments in a single text.
Complementing these theoretical results, our extensive numerical experiments
show that WISER outperforms state-of-the-art baseline methods, both in terms of
computational speed as well as accuracy, on various benchmark datasets embedded
with diverse watermarking schemes. Our theoretical and empirical findings
establish WISER as an effective tool for watermark localization in most
settings. It also shows how insights from a classical statistical problem can
lead to a theoretically valid and computationally efficient solution of a
modern and pertinent problem.

</details>


### [203] [Breaking the curse of dimensionality for linear rules: optimal predictors over the ellipsoid](https://arxiv.org/abs/2509.21174)
*Alexis Ayme,Bruno Loureiro*

Main category: stat.ML

TL;DR: 研究信号估计中防止统计学习界随维度增加而退化所需的最小结构假设，给出线性预测规则泛化误差界，揭示风险的两个基本因素。


<details>
  <summary>Details</summary>
Motivation: 探讨在信号估计中，防止统计学习界随维度增加而退化所需的最小结构假设。

Method: 在贝叶斯预测器位于椭球的假设下，推导线性预测规则泛化误差的非渐近上下界；在贝叶斯预测器固定时，为旋转不变线性预测规则子类建立下界。

Result: 得到线性预测规则泛化误差的上下界，分析出风险的两个基本因素：类似方差项和无噪声误差项。

Conclusion: 研究结果揭示了结构假设在缓解维度灾难中的作用。

Abstract: In this work, we address the following question: What minimal structural
assumptions are needed to prevent the degradation of statistical learning
bounds with increasing dimensionality? We investigate this question in the
classical statistical setting of signal estimation from $n$ independent linear
observations $Y_i = X_i^{\top}\theta + \epsilon_i$. Our focus is on the
generalization properties of a broad family of predictors that can be expressed
as linear combinations of the training labels, $f(X) = \sum_{i=1}^{n} l_{i}(X)
Y_i$. This class -- commonly referred to as linear prediction rules --
encompasses a wide range of popular parametric and non-parametric estimators,
including ridge regression, gradient descent, and kernel methods. Our
contributions are twofold. First, we derive non-asymptotic upper and lower
bounds on the generalization error for this class under the assumption that the
Bayes predictor $\theta$ lies in an ellipsoid. Second, we establish a lower
bound for the subclass of rotationally invariant linear prediction rules when
the Bayes predictor is fixed. Our analysis highlights two fundamental
contributions to the risk: (a) a variance-like term that captures the intrinsic
dimensionality of the data; (b) the noiseless error, a term that arises
specifically in the high-dimensional regime. These findings shed light on the
role of structural assumptions in mitigating the curse of dimensionality.

</details>


### [204] [Response to Promises and Pitfalls of Deep Kernel Learning](https://arxiv.org/abs/2509.21228)
*Andrew Gordon Wilson,Zhiting Hu,Ruslan Salakhutdinov,Eric P. Xing*

Main category: stat.ML

TL;DR: 本文回应Ober等人2021年论文，指出其重参数化引入新的数据拟合项，数据拟合与复杂度平衡仍对确定核超参数起重要作用。


<details>
  <summary>Details</summary>
Motivation: 回应Ober等人2021年论文关于高斯过程边际似然及核超参数确定的观点。

Method: 分析Ober等人的研究，指出其重参数化过程中引入新的数据拟合项。

Result: 发现重参数化引入了影响所有其他核超参数的新数据拟合项。

Conclusion: 数据拟合和复杂度之间的平衡在确定核超参数方面仍起着重要作用。

Abstract: This note responds to "Promises and Pitfalls of Deep Kernel Learning" (Ober
et al., 2021). The marginal likelihood of a Gaussian process can be
compartmentalized into a data fit term and a complexity penalty. Ober et al.
(2021) shows that if a kernel can be multiplied by a signal variance
coefficient, then reparametrizing and substituting in the maximized value of
this parameter sets a reparametrized data fit term to a fixed value. They use
this finding to argue that the complexity penalty, a log determinant of the
kernel matrix, then dominates in determining the other values of kernel
hyperparameters, which can lead to data overcorrelation. By contrast, we show
that the reparametrization in fact introduces another data-fit term which
influences all other kernel hyperparameters. Thus, a balance between data fit
and complexity still plays a significant role in determining kernel
hyperparameters.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [205] [Adoption, usability and perceived clinical value of a UK AI clinical reference platform (iatroX): a mixed-methods formative evaluation of real-world usage and a 1,223-respondent user survey](https://arxiv.org/abs/2509.21188)
*Kolawole Tytler*

Main category: cs.HC

TL;DR: 本文介绍了以英国为中心的临床参考平台iatroX，通过回顾性分析和调查评估其早期使用情况，结果显示该平台能缓解医生信息过载问题。


<details>
  <summary>Details</summary>
Motivation: 临床医生面临生物医学文献和指南信息过载问题，检索增强生成（RAG）需真实世界评估，故评估iatroX平台。

Method: 对2025年4月8日 - 7月31日期间网页、iOS和安卓端进行回顾性分析，开展产品内拦截调查，使用指标经机器人过滤，对约10%网页会话随机提示评估，用Wilson 95%置信区间总结比例，对自由文本评论进行主题内容分析。

Result: iatroX有19,269名独特网页用户、202,660次参与事件和约40,000次临床查询，移动端有一定下载量和活跃度。调查显示各指标比例较高，主题强调速度、指南关联答案和英国特异性。

Conclusion: 早期真实世界使用表明iatroX可缓解英国临床医生信息过载并支持及时解答，但存在样本小和早期采用者偏差问题，未来需进行准确性审计和前瞻性研究。

Abstract: Clinicians face growing information overload from biomedical literature and
guidelines, hindering evidence-based care. Retrieval-augmented generation (RAG)
with large language models may provide fast, provenance-linked answers, but
requires real-world evaluation. We describe iatroX, a UK-centred RAG-based
clinical reference platform, and report early adoption, usability, and
perceived clinical value from a formative implementation evaluation. Methods
comprised a retrospective analysis of usage across web, iOS, and Android over
16 weeks (8 April-31 July 2025) and an in-product intercept survey. Usage
metrics were drawn from web and app analytics with bot filtering. A client-side
script randomized single-item prompts to approx. 10% of web sessions from a
predefined battery assessing usefulness, reliability, and adoption intent.
Proportions were summarized with Wilson 95% confidence intervals; free-text
comments underwent thematic content analysis. iatroX reached 19,269 unique web
users, 202,660 engagement events, and approx. 40,000 clinical queries. Mobile
uptake included 1,960 iOS downloads and Android growth (peak >750 daily active
users). The survey yielded 1,223 item-level responses: perceived usefulness
86.2% (95% CI 74.8-93.9%; 50/58); would use again 93.3% (95% CI 68.1-99.8%;
14/15); recommend to a colleague 88.4% (95% CI 75.1-95.9%; 38/43); perceived
accuracy 75.0% (95% CI 58.8-87.3%; 30/40); reliability 79.4% (95% CI
62.1-91.3%; 27/34). Themes highlighted speed, guideline-linked answers, and UK
specificity. Early real-world use suggests iatroX can mitigate information
overload and support timely answers for UK clinicians. Limitations include
small per-item samples and early-adopter bias; future work will include
accuracy audits and prospective studies on workflow and care quality.

</details>


### [206] [CHOIR: A Chatbot-mediated Organizational Memory Leveraging Communication in University Research Labs](https://arxiv.org/abs/2509.20512)
*Sangwook Lee,Adnan Abbas,Yan Chen,Young-Ho Kim,Sang Won Lee*

Main category: cs.HC

TL;DR: 大学研究实验室聊天平台知识易失，文档维护难，设计LLM聊天机器人CHOIR并部署，发现隐私意识矛盾，给出设计启示。


<details>
  <summary>Details</summary>
Motivation: 解决大学研究实验室聊天平台知识易丢失和文档维护、导航困难的问题。

Method: 通过访谈了解实验室组织记忆挑战，设计CHOIR聊天机器人并在四个研究实验室部署一个月。

Result: 实验室成员提问107次，实验室主任更新文档38次，存在隐私意识矛盾，学生因经验泛化困难避免贡献。

Conclusion: 提出保护隐私意识和支持特定上下文知识文档的设计启示。

Abstract: University research labs often rely on chat-based platforms for communication
and project management, where valuable knowledge surfaces but is easily lost in
message streams. Documentation can preserve knowledge, but it requires ongoing
maintenance and is challenging to navigate. Drawing on formative interviews
that revealed organizational memory challenges in labs, we designed CHOIR, an
LLM-based chatbot that supports organizational memory through four key
functions: document-grounded Q&A, Q&A sharing for follow-up discussion,
knowledge extraction from conversations, and AI-assisted document updates. We
deployed CHOIR in four research labs for one month (n=21), where the lab
members asked 107 questions and lab directors updated documents 38 times in the
organizational memory. Our findings reveal a privacy-awareness tension:
questions were asked privately, limiting directors' visibility into
documentation gaps. Students often avoided contribution due to challenges in
generalizing personal experiences into universal documentation. We contribute
design implications for privacy-preserving awareness and supporting
context-specific knowledge documentation.

</details>


### [207] [Perspectra: Choosing Your Experts Enhances Critical Thinking in Multi-Agent Research Ideation](https://arxiv.org/abs/2509.20553)
*Yiren Liu,Viraj Shah,Sangho Suh,Pao Siangliulue,Tal August,Yun Huang*

Main category: cs.HC

TL;DR: 介绍交互式多智能体系统Perspectra，通过实验对比发现其能提升批判性思维行为，为设计多智能体工具提供启示。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中用户对多领域专家智能体协作的有效控制、引导和评估研究不足。

Method: 提出Perspectra系统，通过论坛式界面可视化和结构化大语言模型智能体的讨论，开展18人被试内实验与群聊基线对比。

Result: Perspectra显著增加批判性思维行为频率和深度，引发更多跨学科回复，提案修订更频繁。

Conclusion: 为设计支持用户控制多智能体对抗性话语、促进批判性思维的多智能体工具提供了启示。

Abstract: Recent advances in multi-agent systems (MAS) enable tools for information
search and ideation by assigning personas to agents. However, how users can
effectively control, steer, and critically evaluate collaboration among
multiple domain-expert agents remains underexplored. We present Perspectra, an
interactive MAS that visualizes and structures deliberation among LLM agents
via a forum-style interface, supporting @-mention to invite targeted agents,
threading for parallel exploration, with a real-time mind map for visualizing
arguments and rationales. In a within-subjects study with 18 participants, we
compared Perspectra to a group-chat baseline as they developed research
proposals. Our findings show that Perspectra significantly increased the
frequency and depth of critical-thinking behaviors, elicited more
interdisciplinary replies, and led to more frequent proposal revisions than the
group chat condition. We discuss implications for designing multi-agent tools
that scaffold critical thinking by supporting user control over multi-agent
adversarial discourse.

</details>


### [208] [MechStyle: Augmenting Generative AI with Mechanical Simulation to Create Stylized and Structurally Viable 3D Models](https://arxiv.org/abs/2509.20571)
*Faraz Faruqi,Amira Abdel-Rahman,Leandra Tejedor,Martin Nisser,Jiaji Li,Vrushank Phadnis,Varun Jampani,Neil Gershenfeld,Megan Hofmann,Stefanie Mueller*

Main category: cs.HC

TL;DR: 介绍MechStyle系统，能在保留3D可打印模型结构完整性的同时进行风格化，评估了FEA模拟反馈效果和时间效率，展示用户界面和应用示例。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成式AI的3D模型风格化方法会改变模型几何形状，损害结构完整性，需要一种能保留结构完整性的风格化方法。

Method: 通过有限元分析（FEA）模拟反馈增强基于生成式AI的风格化过程，在风格化修改几何形状时减少对高应力区域的修改；比较三种风格化控制策略评估FEA模拟反馈效果，比较三种自适应调度策略研究时间效率。

Result: 评估了FEA模拟反馈在增强风格化过程中的有效性，研究了方法的时间效率。

Conclusion: MechStyle系统能让用户生成风格化且结构可行的3D模型，有一定应用价值。

Abstract: Recent developments in Generative AI enable creators to stylize 3D models
based on text prompts. These methods change the 3D model geometry, which can
compromise the model's structural integrity once fabricated. We present
MechStyle, a system that enables creators to stylize 3D printable models while
preserving their structural integrity. MechStyle accomplishes this by
augmenting the Generative AI-based stylization process with feedback from a
Finite Element Analysis (FEA) simulation. As the stylization process modifies
the geometry to approximate the desired style, feedback from the FEA simulation
reduces modifications to regions with increased stress. We evaluate the
effectiveness of FEA simulation feedback in the augmented stylization process
by comparing three stylization control strategies. We also investigate the time
efficiency of our approach by comparing three adaptive scheduling strategies.
Finally, we demonstrate MechStyle's user interface that allows users to
generate stylized and structurally viable 3D models and provide five example
applications.

</details>


### [209] [Understanding Mode Switching in Human-AI Collaboration: Behavioral Insights and Predictive Modeling](https://arxiv.org/abs/2509.20666)
*Avinash Ajit Nargund,Arthur Caetano,Kevin Yang,Rose Yiwei Liu,Philip Tezaur,Kriteen Shrestha,Qisen Pan,Tobias Höllerer,Misha Sra*

Main category: cs.HC

TL;DR: 研究人类在顺序决策任务中动态切换AI控制水平，用象棋实验收集数据，分析并训练模型预测切换，结合定性因素为共享自主系统设计提供建议。


<details>
  <summary>Details</summary>
Motivation: 现有集成两种控制模式的系统常忽略任务中用户偏好的变化，需研究用户如何动态切换控制水平。

Method: 采用手脑象棋设置实验，收集8名参与者超400个模式切换决策及相关数据，进行统计分析并训练模型。

Result: 统计分析发现切换前后注视模式、子任务复杂度和后续走法质量有显著差异，训练的模型预测控制水平切换F1值为0.65，还通过访谈确定影响切换的定性因素。

Conclusion: 实时行为信号可作为现有系统驱动模式切换机制的补充输入，研究结果能为共享自主系统设计提供参考。

Abstract: Human-AI collaboration is typically offered in one of two of user control
levels: guidance, where the AI provides suggestions and the human makes the
final decision, and delegation, where the AI acts autonomously within
user-defined constraints. Systems that integrate both modes, common in robotic
surgery or driving assistance, often overlook shifts in user preferences within
a task in response to factors like evolving trust, decision complexity, and
perceived control. In this work, we investigate how users dynamically switch
between higher and lower levels of control during a sequential decision-making
task. Using a hand-and-brain chess setup, participants either selected a piece
and the AI decided how it moved (brain mode), or the AI selected a piece and
the participant decided how it moved (hand mode). We collected over 400
mode-switching decisions from eight participants, along with gaze, emotional
state, and subtask difficulty data. Statistical analysis revealed significant
differences in gaze patterns and subtask complexity prior to a switch and in
the quality of the subsequent move. Based on these results, we engineered
behavioral and task-specific features to train a lightweight model that
predicted control level switches ($F1 = 0.65$). The model performance suggests
that real-time behavioral signals can serve as a complementary input alongside
system-driven mode-switching mechanisms currently used. We complement our
quantitative results with qualitative factors that influence switching
including perceived AI ability, decision complexity, and level of control,
identified from post-game interview analysis. The combined behavioral and
modeling insights can help inform the design of shared autonomy systems that
need dynamic, subtask-level control switches aligned with user intent and
evolving task demands.

</details>


### [210] [Imagining Design Workflows in Agentic AI Futures](https://arxiv.org/abs/2509.20731)
*Samangi Wadinambiarachchi,Jenny Waycott,Yvonne Rogers,Greg Wadley*

Main category: cs.HC

TL;DR: 研究通过设计虚构探讨设计师对将代理式AI系统融入工作流程的看法，合成概念框架并讨论未来设计工作流程中利用AI代理的方向。


<details>
  <summary>Details</summary>
Motivation: 了解设计师对将代理式AI系统融入工作流程的感受，探索设计师与协作式代理AI平台的交互方式。

Method: 采用设计虚构的方法，让十位专业设计师想象并讨论与AI代理协作组织灵感来源和构思。

Result: 发现了AI代理在支持设计师方面的作用、人类与AI之间的权限划分，以及如何向AI代理解释设计师意图。

Conclusion: 合成了一个概念框架，确定了人类和AI代理之间的权限分配，并讨论了未来设计工作流程中利用AI代理的方向。

Abstract: As designers become familiar with Generative AI, a new concept is emerging:
Agentic AI. While generative AI produces output in response to prompts, agentic
AI systems promise to perform mundane tasks autonomously, potentially freeing
designers to focus on what they love: being creative. But how do designers feel
about integrating agentic AI systems into their workflows? Through design
fiction, we investigated how designers want to interact with a collaborative
agentic AI platform. Ten professional designers imagined and discussed
collaborating with an AI agent to organise inspiration sources and ideate. Our
findings highlight the roles AI agents can play in supporting designers, the
division of authority between humans and AI, and how designers' intent can be
explained to AI agents beyond prompts. We synthesise our findings into a
conceptual framework that identifies authority distribution among humans and AI
agents and discuss directions for utilising AI agents in future design
workflows.

</details>


### [211] [Even More Kawaii than Real-Person-Driven VTubers? Understanding How Viewers Perceive AI-Driven VTubers](https://arxiv.org/abs/2509.20817)
*Yiluo Wei,Yupeng He,Gareth Tyson*

Main category: cs.HC

TL;DR: 本文对热门AI驱动VTuber Neuro - sama进行案例研究，分析相关网络文本，以了解观众动机、AI构建虚拟形象方式及对AI作为中之人的看法，增进对AI驱动VTuber及其对数字流媒体文化影响的理解。


<details>
  <summary>Details</summary>
Motivation: 传统VTuber依赖中之人存在个人争议和运营中断风险，AI驱动VTuber虽有优势但也存在真实性和观众参与度问题，因此开展研究以深入了解。

Method: 对热门AI驱动VTuber Neuro - sama进行案例研究，分析10.8万条Reddit帖子和13.6万条YouTube评论。

Result: 未在摘要中明确提及具体结果。

Conclusion: 研究增进了对AI驱动VTuber及其对数字流媒体文化影响的理解。

Abstract: VTubers, digital personas represented by animated avatars, have gained
massive popularity. Traditionally, VTubers are operated and voiced by human
controllers known as Nakanohito. The reliance on Nakanohito, however, poses
risks due to potential personal controversies and operational disruptions. The
emergence of AI-driven VTubers offers a new model free from these human
constraints. While AI-driven VTubers present benefits such as continuous
operation and reduced scandal risk, they also raise questions about
authenticity and audience engagement. Therefore, to gain deeper insights, we
conduct a case study, investigating viewer perceptions of Neuro-sama, the most
popular AI-driven VTuber with 845k followers on Twitch and 753k followers on
YouTube. We analyze 108k Reddit posts and 136k YouTube comments, aiming to
better understand viewer motivations, how AI constructs the virtual persona,
and perceptions of the AI as Nakanohito. Our findings enhance the understanding
of AI-driven VTubers and their impact on digital streaming culture.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [212] [Physics Informed Neural Networks for design optimisation of diamond particle detectors for charged particle fast-tracking at high luminosity hadron colliders](https://arxiv.org/abs/2509.21123)
*Alessandro Bombini,Alessandro Rosa,Clarissa Buti,Giovanni Passaleva,Lucio Anderlini*

Main category: physics.ins-det

TL;DR: 未来强子对撞机需要特定性能的探测器，3D 钻石像素传感器有潜力但电极电阻影响信号传播，用 PDE 建模并结合物理信息神经网络评估计时退化。


<details>
  <summary>Details</summary>
Motivation: 未来高亮度强子对撞机对跟踪探测器有极端辐射耐受性、高空间精度和亚纳秒计时的要求，3D 钻石像素传感器虽有相关能力，但电极高电阻率影响信号传播，需研究应对。

Method: 通过从麦克斯韦方程组的准静态近似推导出的 3 + 1D 三阶 PDE 对现象建模，数值求解 PDE 并与电荷传输模拟结合，用基于谱方法数据训练的混合专家物理信息神经网络提供无网格求解器。

Result: 构建了模型并提供了评估电极电阻导致计时退化的无网格求解器。

Conclusion: 可通过特定的 PDE 建模和物理信息神经网络评估 3D 钻石像素传感器因电极电阻造成的计时退化。

Abstract: Future high-luminosity hadron colliders demand tracking detectors with
extreme radiation tolerance, high spatial precision, and sub-nanosecond timing.
3D diamond pixel sensors offer these capabilities due to diamond's radiation
hardness and high carrier mobility. Conductive electrodes, produced via
femtosecond IR laser pulses, exhibit high resistivity that delays signal
propagation. This effect necessitates extending the classical Ramo-Shockley
weighting potential formalism. We model the phenomenon through a 3rd-order,
3+1D PDE derived as a quasi-stationary approximation of Maxwell's equations.
The PDE is solved numerically and coupled with charge transport simulations for
realistic 3D sensor geometries. A Mixture-of-Experts Physics-Informed Neural
Network, trained on Spectral Method data, provides a meshless solver to assess
timing degradation from electrode resistance.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [213] [QAMO: Quality-aware Multi-centroid One-class Learning For Speech Deepfake Detection](https://arxiv.org/abs/2509.20679)
*Duc-Tuan Truong,Tianchi Liu,Ruijie Tao,Junjie Li,Kong Aik Lee,Eng Siong Chng*

Main category: cs.SD

TL;DR: 提出QAMO用于语音深度伪造检测，引入多质心优化，在In - the - Wild数据集表现优于先前系统。


<details>
  <summary>Details</summary>
Motivation: 现有单质心一类学习方法简化了真实语音表示，忽略了语音质量等有用线索。

Method: 提出QAMO，引入多个质量感知质心，每个质心代表不同语音质量子空间，支持多质心集成评分策略。

Result: 使用两个质心代表高低质量语音，在In - the - Wild数据集上实现5.09%的等错误率。

Conclusion: QAMO在语音深度伪造检测中优于先前的一类和质量感知系统。

Abstract: Recent work shows that one-class learning can detect unseen deepfake attacks
by modeling a compact distribution of bona fide speech around a single
centroid. However, the single-centroid assumption can oversimplify the bona
fide speech representation and overlook useful cues, such as speech quality,
which reflects the naturalness of the speech. Speech quality can be easily
obtained using existing speech quality assessment models that estimate it
through Mean Opinion Score. In this paper, we propose QAMO: Quality-Aware
Multi-Centroid One-Class Learning for speech deepfake detection. QAMO extends
conventional one-class learning by introducing multiple quality-aware
centroids. In QAMO, each centroid is optimized to represent a distinct speech
quality subspaces, enabling better modeling of intra-class variability in bona
fide speech. In addition, QAMO supports a multi-centroid ensemble scoring
strategy, which improves decision thresholding and reduces the need for quality
labels during inference. With two centroids to represent high- and low-quality
speech, our proposed QAMO achieves an equal error rate of 5.09% in In-the-Wild
dataset, outperforming previous one-class and quality-aware systems.

</details>


### [214] [Addressing Gradient Misalignment in Data-Augmented Training for Robust Speech Deepfake Detection](https://arxiv.org/abs/2509.20682)
*Duc-Tuan Truong,Tianchi Liu,Junjie Li,Ruijie Tao,Kong Aik Lee,Eng Siong Chng*

Main category: cs.SD

TL;DR: 本文提出用于语音深度伪造检测的双路径数据增强训练框架，通过梯度对齐解决数据增强时梯度冲突问题，加速收敛并降低等错误率。


<details>
  <summary>Details</summary>
Motivation: 数据增强在语音深度伪造检测中常用，但训练时原始和增强输入的反向传播梯度可能不一致，导致参数更新冲突，影响收敛和模型性能。

Method: 设计双路径数据增强训练框架，每个训练话语通过两条输入路径处理，比较并对齐反向传播的梯度方向以减少优化冲突。

Result: 约25%训练迭代中原始输入和增强输入存在梯度冲突，解决冲突后加速收敛，在In - the - Wild数据集上等错误率相对降低达18.69%。

Conclusion: 所提的双路径数据增强训练框架结合梯度对齐能有效解决数据增强中的梯度冲突问题，提升语音深度伪造检测模型性能。

Abstract: In speech deepfake detection (SDD), data augmentation (DA) is commonly used
to improve model generalization across varied speech conditions and spoofing
attacks. However, during training, the backpropagated gradients from original
and augmented inputs may misalign, which can result in conflicting parameter
updates. These conflicts could hinder convergence and push the model toward
suboptimal solutions, thereby reducing the benefits of DA. To investigate and
address this issue, we design a dual-path data-augmented (DPDA) training
framework with gradient alignment for SDD. In our framework, each training
utterance is processed through two input paths: one using the original speech
and the other with its augmented version. This design allows us to compare and
align their backpropagated gradient directions to reduce optimization
conflicts. Our analysis shows that approximately 25% of training iterations
exhibit gradient conflicts between the original inputs and their augmented
counterparts when using RawBoost augmentation. By resolving these conflicts
with gradient alignment, our method accelerates convergence by reducing the
number of training epochs and achieves up to an 18.69% relative reduction in
Equal Error Rate on the In-the-Wild dataset compared to the baseline.

</details>


### [215] [i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents](https://arxiv.org/abs/2509.20971)
*Anupam Purwar,Aditya Choudhary*

Main category: cs.SD

TL;DR: 本文实验低延迟端到端语音通信模型，分析优化V - 2 - V系统的方法，发现TTS组件对RTF影响大，还探索RVQ迭代优化等。


<details>
  <summary>Details</summary>
Motivation: 优化低延迟端到端语音通信模型，使其适用于实时对话应用。

Method: 分析V - 2 - V系统的关键组件（ASR、TTS和对话管理），实验基于CSM1b的V - 2 - V架构，探索TTS解码器对RVQ迭代的优化。

Result: TTS组件对实时因子（RTF）影响最大；基于CSM的V - 2 - V实现，减少RVQ迭代次数和Mimi中使用的码本可带来重要优化。

Conclusion: 通过分析和实验，明确了优化V - 2 - V系统的关键因素和方法。

Abstract: We experiment with a low-latency, end-to-end voice-to-voice communication
model to optimize it for real-time conversational applications. By analyzing
components essential to voice to voice (V-2-V) system viz. automatic speech
recognition (ASR), text-to-speech (TTS), and dialog management, our work
analyzes how to reduce processing time while maintaining high-quality
interactions to identify the levers for optimizing V-2-V system. Our work
identifies that TTS component which generates life-like voice, full of emotions
including natural pauses and exclamations has highest impact on Real time
factor (RTF). The experimented V-2-V architecture utilizes CSM1b has the
capability to understand tone as well as context of conversation by ingesting
both audio and text of prior exchanges to generate contextually accurate
speech. We explored optimization of Residual Vector Quantization (RVQ)
iterations by the TTS decoder which come at a cost of decrease in the quality
of voice generated. Our experimental evaluations also demonstrate that for
V-2-V implementations based on CSM most important optimizations can be brought
by reducing the number of RVQ Iterations along with the codebooks used in Mimi.

</details>


### [216] [SupCLAP: Controlling Optimization Trajectory Drift in Audio-Text Contrastive Learning with Support Vector Regularization](https://arxiv.org/abs/2509.21033)
*Jiehui Luo,Yuguo Yin,Yuxin Xie,Jinghan Ru,Xianwei Zhuang,Minghua He,Aofan Liu,Zihan Xiong,Dongchao Yang*

Main category: cs.SD

TL;DR: 提出支持向量正则化（SVR）方法处理对比学习中负样本垂直分量问题，实验显示优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 对比学习中负样本垂直分量有丰富信息但导致优化轨迹漂移和训练不稳定，需解决该问题。

Method: 提出SVR方法引入辅助支持向量控制垂直分量，探索直接参数化和带约束的自适应半径预测器模块确定语义半径。

Result: 在标准音频文本数据集的分类、单语检索和多语检索任务上，SVR方法优于InfoNCE和SigLIP loss等基线方法。

Conclusion: 理论分析和实验结果验证了SVR方法的正确性和有效性。

Abstract: Contrastive language-audio pretraining, which aims to unify multimodal
representations in a shared embedding space, serves as a cornerstone for
building a wide range of applications, from cross-modal retrieval to
cutting-edge multimodal large language models. However, we find that the
perpendicular component of the pushing force from negative samples in
contrastive learning is a double-edged sword: it contains rich supplementary
information from negative samples, yet its unconstrained nature causes
optimization trajectory drift and training instability. To address this, we
propose Support Vector Regularization (SVR), a method that introduces an
auxiliary support vector to control this perpendicular component, aiming to
harness its rich information while mitigating the associated trajectory drift.
The efficacy of SVR is critically governed by its semantic radius, for which we
explore two unsupervised modeling strategies: direct parameterization and an
adaptive radius predictor module enhanced with constraints to improve its
predicting accuracy. Extensive experimental results demonstrate that our method
surpasses widely used baselines like InfoNCE and SigLIP loss across
classification, monolingual retrieval, and multilingual retrieval on standard
audio-text datasets. Both the theoretical analysis and the experimental results
on optimizing trajectory drift validate the correctness and effectiveness of
our SVR method.

</details>


### [217] [UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice](https://arxiv.org/abs/2509.21144)
*Sitong Cheng,Weizhen Bian,Xinsheng Wang,Ruibin Yuan,Jianyi Chen,Shunshun Yin,Yike Guo,Wei Xue*

Main category: cs.SD

TL;DR: 本文提出单阶段框架UniSS用于表达性语音到语音翻译，构建数据集UniST，实验显示其性能优于先前方法，建立了更有效范式。


<details>
  <summary>Details</summary>
Motivation: 表达性语音到语音翻译领域受限于表达风格配对语音数据稀缺、多阶段处理流程复杂和大语言模型翻译能力迁移有限等问题。

Method: 引入单阶段框架UniSS，设计语音语义和风格建模，与文本大语言模型集成；提出跨模态思维链提示过程；构建并发布数据集UniST。

Result: UniSS在翻译保真度和语音质量上显著优于先前方法，能保留语音、情感和时长一致性。

Conclusion: 为下一代表达性语音到语音翻译系统建立了更简单有效的范式。

Abstract: The ultimate goal of expressive speech-to-speech translation (S2ST) is to
accurately translate spoken content while preserving the speaker identity and
emotional style. However, progress in this field is largely hindered by three
key challenges: the scarcity of paired speech data that retains expressive
styles, the complexity of multi-stage processing pipelines, and the limited
transfer of translation capabilities from large language models (LLMs). In this
work, we address these challenges by introducing UniSS, a novel single-stage
framework for expressive S2ST. Our approach features carefully designed speech
semantic and style modeling, enabling seamless integration with existing
text-based LLM frameworks to develop a unified text-speech language model. To
transfer translation capabilities from text to speech, we propose a cross-modal
chain-of-thought prompting process that progressively aligns audio semantics
with text and ensures style preservation in the decoded results. Furthermore,
we construct and release a large-scale, high-quality expressive S2ST dataset,
UniST, comprising 44.8k hours of data. Experimental results show that UniSS
significantly outperforms previous methods in translation fidelity and speech
quality while preserving voice, emotion, and duration consistency. Our work
establishes a simpler and more effective paradigm for building the next
generation of expressive S2ST systems. Audio samples are available at
https://cmots.github.io/uniss-demo.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [218] [Centralized vs. Decentralized Security for Space AI Systems? A New Look](https://arxiv.org/abs/2509.20395)
*Noam Schmitt,Marc Antoine Lacoste*

Main category: cs.CR

TL;DR: 研究卫星星座集中与分散安全管理权衡，提出三种AI架构，指出集中短期优，分散长期好。


<details>
  <summary>Details</summary>
Motivation: 平衡卫星星座安全管理的安全性和性能。

Method: 研究集中和分散两种安全管理方式，并提出三种AI架构（集中、分布式、联邦）。

Result: 集中式架构短期训练快但有通信延迟问题，分散式架构长期扩展性和安全性更好。

Conclusion: 集中式适合短期，分散式适合长期。

Abstract: This paper investigates the trade-off between centralized and decentralized
security management in constellations of satellites to balance security and
performance. We highlight three key AI architectures for automated security
management: (a) centralized, (b) distributed and (c) federated. The centralized
architecture is the best option short term, providing fast training, despite
the hard challenge of the communication latency overhead across space.
Decentralized architectures are better alternatives in the longer term,
providing enhanced scalability and security.

</details>


### [219] [R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning](https://arxiv.org/abs/2509.20384)
*Jiayi Lin,Liangcai Su,Junzhe Li,Chenxiong Qian*

Main category: cs.CR

TL;DR: 提出R1 - Fuzz框架用于复杂文本模糊测试输入生成，在实际测试中表现出色，能发现新漏洞。


<details>
  <summary>Details</summary>
Motivation: 模糊测试在处理复杂目标时存在困难，现有语言模型用于此任务存在探索程序逻辑不足和使用大模型成本高的问题。

Method: 提出R1 - Fuzz框架，引入覆盖切片问题构建和基于距离的奖励计算，通过强化学习对模型进行后训练，设计紧密集成语言模型的模糊测试工作流。

Result: 小型模型R1 - Fuzz - 7B在实际模糊测试中可与甚至超越更大模型，覆盖率比现有最先进模糊器高75%，发现29个未知漏洞。

Conclusion: R1 - Fuzz框架具有实用性，能有效解决复杂目标的模糊测试问题。

Abstract: Fuzzing is effective for vulnerability discovery but struggles with complex
targets such as compilers, interpreters, and database engines, which accept
textual input that must satisfy intricate syntactic and semantic constraints.
Although language models (LMs) have attracted interest for this task due to
their vast latent knowledge and reasoning potential, their practical adoption
has been limited. The major challenges stem from insufficient exploration of
deep program logic among real-world codebases, and the high cost of leveraging
larger models. To overcome these challenges, we propose R1-Fuzz, the first
framework that leverages reinforcement learning (RL) to specialize
cost-efficient LMs and integrate them for complex textual fuzzing input
generation. R1-Fuzz introduces two key designs: coverage-slicing-based question
construction and a distance-based reward calculation. Through RL-based
post-training of a model with our constructed dataset, R1-Fuzz designs a
fuzzing workflow that tightly integrates LMs to reason deep program semantics
during fuzzing. Evaluations on diverse real-world targets show that our design
enables a small model, named R1-Fuzz-7B, to rival or even outperform much
larger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\%
higher coverage than state-of-the-art fuzzers and discovers 29 previously
unknown vulnerabilities, demonstrating its practicality.

</details>


### [220] [Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools](https://arxiv.org/abs/2509.21011)
*Ping He,Changjiang Li,Binbin Zhao,Tianyu Du,Shouling Ji*

Main category: cs.CR

TL;DR: 提出AutoMalTool自动化红队框架生成恶意MCP工具，揭示LLM代理新安全风险。


<details>
  <summary>Details</summary>
Motivation: MCP工具引入中毒攻击风险，以往红队方法多停留在概念验证阶段，需自动化和系统化红队方法。

Method: 提出AutoMalTool自动化红队框架生成恶意MCP工具。

Result: AutoMalTool有效生成能操纵主流LLM代理行为且规避当前检测机制的恶意MCP工具。

Conclusion: AutoMalTool揭示了LLM代理新的安全风险。

Abstract: The remarkable capability of large language models (LLMs) has led to the wide
application of LLM-based agents in various domains. To standardize interactions
between LLM-based agents and their environments, model context protocol (MCP)
tools have become the de facto standard and are now widely integrated into
these agents. However, the incorporation of MCP tools introduces the risk of
tool poisoning attacks, which can manipulate the behavior of LLM-based agents.
Although previous studies have identified such vulnerabilities, their red
teaming approaches have largely remained at the proof-of-concept stage, leaving
the automatic and systematic red teaming of LLM-based agents under the MCP tool
poisoning paradigm an open question. To bridge this gap, we propose
AutoMalTool, an automated red teaming framework for LLM-based agents by
generating malicious MCP tools. Our extensive evaluation shows that AutoMalTool
effectively generates malicious MCP tools capable of manipulating the behavior
of mainstream LLM-based agents while evading current detection mechanisms,
thereby revealing new security risks in these agents.

</details>


### [221] [Lightweight MobileNetV1+GRU for ECG Biometric Authentication: Federated and Adversarial Evaluation](https://arxiv.org/abs/2509.20382)
*Dilli Hang Rai,Sabin Kafley*

Main category: cs.CR

TL;DR: 提出轻量级深度学习模型用于ECG身份验证，在多数据集测试有高准确率，受攻击时准确率下降，强调后续研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决ECG生物识别在可穿戴设备部署时面临的实时处理、隐私和防欺骗等挑战。

Method: 提出MobileNetV1+GRU模型，注入20dB高斯噪声并进行自定义预处理，模拟可穿戴条件和边缘部署。

Result: 在多个数据集上取得高准确率、F1分数、精度、召回率等指标，受FGSM攻击时准确率下降。

Conclusion: 强调联邦学习、对抗测试和多样化可穿戴生理数据集对确保生物识别安全和可扩展性的必要性。

Abstract: ECG biometrics offer a unique, secure authentication method, yet their
deployment on wearable devices faces real-time processing, privacy, and
spoofing vulnerability challenges. This paper proposes a lightweight deep
learning model (MobileNetV1+GRU) for ECG-based authentication, injection of
20dB Gaussian noise & custom preprocessing. We simulate wearable conditions and
edge deployment using the ECGID, MIT-BIH, CYBHi, and PTB datasets, achieving
accuracies of 99.34%, 99.31%, 91.74%, and 98.49%, F1-scores of 0.9869, 0.9923,
0.9125, and 0.9771, Precision of 0.9866, 0.9924, 0.9180 and 0.9845, Recall of
0.9878, 0.9923, 0.9129, and 0.9756, equal error rates (EER) of 0.0009, 0.00013,
0.0091, and 0.0009, and ROC-AUC values of 0.9999, 0.9999, 0.9985, and 0.9998,
while under FGSM adversarial attacks, accuracy drops from 96.82% to as low as
0.80%. This paper highlights federated learning, adversarial testing, and the
need for diverse wearable physiological datasets to ensure secure and scalable
biometrics.

</details>


### [222] [MARS: A Malignity-Aware Backdoor Defense in Federated Learning](https://arxiv.org/abs/2509.20383)
*Wei Wan,Yuxuan Ning,Zhicong Huang,Cheng Hong,Shengshan Hu,Ziqi Zhou,Yechao Zhang,Tianqing Zhu,Wanlei Zhou,Leo Yu Zhang*

Main category: cs.CR

TL;DR: 文章指出联邦学习易受后门攻击，现有防御方法失效，提出MARS防御机制，实验表明其能抵御攻击且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习后门攻击防御方法采用与后门攻击松散耦合的经验统计措施，导致防御失效，需新防御方法。

Method: 提出MARS防御机制，利用后门能量指示神经元恶意程度，提取突出BE值形成CBE，用基于Wasserstein距离的聚类方法识别后门模型。

Result: MARS能抵御SOTA后门攻击，显著优于现有防御。

Conclusion: MARS是一种有效的联邦学习后门攻击防御机制。

Abstract: Federated Learning (FL) is a distributed paradigm aimed at protecting
participant data privacy by exchanging model parameters to achieve high-quality
model training. However, this distributed nature also makes FL highly
vulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art
(SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether
the backdoor models have been accepted by the defender and adaptively optimizes
backdoor models, rendering existing defenses ineffective. In this paper, we
first reveal that the failure of existing defenses lies in the employment of
empirical statistical measures that are loosely coupled with backdoor attacks.
Motivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that
leverages backdoor energy (BE) to indicate the malicious extent of each neuron.
To amplify malignity, we further extract the most prominent BE values from each
model to form a concentrated backdoor energy (CBE). Finally, a novel
Wasserstein distance-based clustering method is introduced to effectively
identify backdoor models. Extensive experiments demonstrate that MARS can
defend against SOTA backdoor attacks and significantly outperforms existing
defenses.

</details>


### [223] [Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants](https://arxiv.org/abs/2509.20388)
*Amir AL-Maamari*

Main category: cs.CR

TL;DR: 本文针对AI编码助手隐私和信任问题，引入专家验证的隐私计分卡评估五大助手，揭示隐私保护等级差异及行业弱点，为开发者和组织提供选工具的指导。


<details>
  <summary>Details</summary>
Motivation: AI编码助手集成到开发工作流引发隐私和信任担忧，其数据处理方式不明带来安全和合规风险。

Method: 引入专家验证的隐私计分卡，分析四种文档类型，根据14项加权标准对五大助手打分，标准和权重由法律专家和数据保护官完善。

Result: 揭示隐私保护有等级差异，最高和最低分工具相差20分，发现行业常见弱点，如使用选择退出式同意和未主动过滤用户提示中的机密信息。

Conclusion: 计分卡为开发者和组织提供行动指导，建立透明度新基准，倡导AI行业转向以用户为中心的隐私标准。

Abstract: The rapid integration of AI-powered coding assistants into developer
workflows has raised significant privacy and trust concerns. As developers
entrust proprietary code to services like OpenAI's GPT, Google's Gemini, and
GitHub Copilot, the unclear data handling practices of these tools create
security and compliance risks. This paper addresses this challenge by
introducing and applying a novel, expert-validated privacy scorecard. The
methodology involves a detailed analysis of four document types; from legal
policies to external audits; to score five leading assistants against 14
weighted criteria. A legal expert and a data protection officer refined these
criteria and their weighting. The results reveal a distinct hierarchy of
privacy protections, with a 20-point gap between the highest- and lowest-ranked
tools. The analysis uncovers common industry weaknesses, including the
pervasive use of opt-out consent for model training and a near-universal
failure to filter secrets from user prompts proactively. The resulting
scorecard provides actionable guidance for developers and organizations,
enabling evidence-based tool selection. This work establishes a new benchmark
for transparency and advocates for a shift towards more user-centric privacy
standards in the AI industry.

</details>


### [224] [Defending against Stegomalware in Deep Neural Networks with Permutation Symmetry](https://arxiv.org/abs/2509.20399)
*Birk Torpmann-Hagen,Michael A. Riegler,Pål Halvorsen,Dag Johansen*

Main category: cs.CR

TL;DR: 本文提出针对神经网络隐写恶意软件的有效对策，通过打乱矩阵列顺序或卷积层通道顺序来中和恶意软件，且不影响网络精度。


<details>
  <summary>Details</summary>
Motivation: 神经网络检查点常被共享分发，存在神经网络隐写恶意软件威胁，但该问题被深度学习从业者和安全专家忽视，需有效对策。

Method: 通过打乱权重和偏置矩阵的列顺序，或卷积层的通道顺序来中和攻击。

Result: 能有效破坏通过现有方法嵌入的有效负载，且不影响网络精度，显著优于其他竞争方法。

Conclusion: 提出有效对策，讨论绕过防御的可能方式和额外防御方法，倡导持续研究机器学习系统安全。

Abstract: Deep neural networks are being utilized in a growing number of applications,
both in production systems and for personal use. Network checkpoints are as a
consequence often shared and distributed on various platforms to ease the
development process. This work considers the threat of neural network
stegomalware, where malware is embedded in neural network checkpoints at a
negligible cost to network accuracy. This constitutes a significant security
concern, but is nevertheless largely neglected by the deep learning
practitioners and security specialists alike. We propose the first effective
countermeasure to these attacks. In particular, we show that state-of-the-art
neural network stegomalware can be efficiently and effectively neutralized
through shuffling the column order of the weight- and bias-matrices, or
equivalently the channel-order of convolutional layers. We show that this
effectively corrupts payloads that have been embedded by state-of-the-art
methods in neural network steganography at no cost to network accuracy,
outperforming competing methods by a significant margin. We then discuss
possible means by which to bypass this defense, additional defense methods, and
advocate for continued research into the security of machine learning systems.

</details>


### [225] [Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation](https://arxiv.org/abs/2509.20411)
*Tharcisse Ndayipfukamiye,Jianguo Ding,Doreen Sebastian Sarwatt,Adamu Gaston Philipo,Huansheng Ning*

Main category: cs.CR

TL;DR: 本文对2021 - 2025年8月31日基于GAN的网络安全对抗防御进行系统综述，介绍四维分类法，指出优缺点并提出未来路线图。


<details>
  <summary>Details</summary>
Motivation: 机器学习网络安全系统易受攻击，GAN既是攻击手段也是防御方法，需系统总结GAN在网络安全对抗防御的进展。

Method: 采用PRISMA合规的系统文献综述协议，搜索五大数字图书馆，对185篇同行评审研究进行定量趋势分析和主题分类。

Result: GAN在网络入侵检测等领域提高检测准确性等，有WGAN - GP等进展，但存在训练不稳定等挑战。

Conclusion: GAN防御潜力大，但需在稳定架构等方面改进，提出强调混合模型等的路线图，为防御奠定基础。

Abstract: Machine learning-based cybersecurity systems are highly vulnerable to
adversarial attacks, while Generative Adversarial Networks (GANs) act as both
powerful attack enablers and promising defenses. This survey systematically
reviews GAN-based adversarial defenses in cybersecurity (2021--August 31,
2025), consolidating recent progress, identifying gaps, and outlining future
directions. Using a PRISMA-compliant systematic literature review protocol, we
searched five major digital libraries. From 829 initial records, 185
peer-reviewed studies were retained and synthesized through quantitative trend
analysis and thematic taxonomy development. We introduce a four-dimensional
taxonomy spanning defensive function, GAN architecture, cybersecurity domain,
and adversarial threat model. GANs improve detection accuracy, robustness, and
data utility across network intrusion detection, malware analysis, and IoT
security. Notable advances include WGAN-GP for stable training, CGANs for
targeted synthesis, and hybrid GAN models for improved resilience. Yet,
persistent challenges remain such as instability in training, lack of
standardized benchmarks, high computational cost, and limited explainability.
GAN-based defenses demonstrate strong potential but require advances in stable
architectures, benchmarking, transparency, and deployment. We propose a roadmap
emphasizing hybrid models, unified evaluation, real-world integration, and
defenses against emerging threats such as LLM-driven cyberattacks. This survey
establishes the foundation for scalable, trustworthy, and adaptive GAN-powered
defenses.

</details>


### [226] [A Taxonomy of Data Risks in AI and Quantum Computing (QAI) - A Systematic Review](https://arxiv.org/abs/2509.20418)
*Grace Billiris,Asif Gill,Madhushi Bandara*

Main category: cs.CR

TL;DR: 本文系统回顾67项研究，提出22种QAI数据风险分类，揭示独特漏洞与评估缺口，为研究和工具开发奠基。


<details>
  <summary>Details</summary>
Motivation: QAI继承AI和QC的数据风险，存在复杂隐私与安全漏洞且缺乏系统研究，影响系统可信度和可靠性，需加深理解。

Method: 系统回顾67项与隐私和安全相关的研究。

Result: 提出包含22种关键数据风险的分类，分为治理、风险评估等五类，揭示QAI独特漏洞，发现整体风险评估的缺口。

Conclusion: 该研究有助于可信AI和QAI研究，为未来风险评估工具开发提供基础。

Abstract: Quantum Artificial Intelligence (QAI), the integration of Artificial
Intelligence (AI) and Quantum Computing (QC), promises transformative advances,
including AI-enabled quantum cryptography and quantum-resistant encryption
protocols. However, QAI inherits data risks from both AI and QC, creating
complex privacy and security vulnerabilities that are not systematically
studied. These risks affect the trustworthiness and reliability of AI and QAI
systems, making their understanding critical. This study systematically reviews
67 privacy- and security-related studies to expand understanding of QAI data
risks. We propose a taxonomy of 22 key data risks, organised into five
categories: governance, risk assessment, control implementation, user
considerations, and continuous monitoring. Our findings reveal vulnerabilities
unique to QAI and identify gaps in holistic risk assessment. This work
contributes to trustworthy AI and QAI research and provides a foundation for
developing future risk assessment tools.

</details>


### [227] [Every Character Counts: From Vulnerability to Defense in Phishing Detection](https://arxiv.org/abs/2509.20589)
*Maria Chiper,Radu Tudor Ionescu*

Main category: cs.CR

TL;DR: 研究字符级深度学习模型用于网络钓鱼检测，评估三种模型，CharGRU表现最佳，对抗训练可提升鲁棒性，还实现决策可视化并开源代码数据。


<details>
  <summary>Details</summary>
Motivation: 现有自动检测方法在检测新网络钓鱼攻击时缺乏可解释性和鲁棒性。

Method: 在自建邮件数据集上评估CharCNN、CharGRU和CharBiLSTM三种字符级神经网络架构，在三种场景下分析性能，在有限计算资源下测试，还采用Grad - CAM技术实现可视化。

Result: CharGRU在所有场景中表现最佳，所有模型易受对抗攻击，但对抗训练可大幅提高鲁棒性。

Conclusion: 字符级深度学习模型可用于网络钓鱼检测，能提供鲁棒性和可解释性，对抗训练是提升鲁棒性的有效方法。

Abstract: Phishing attacks targeting both organizations and individuals are becoming an
increasingly significant threat as technology advances. Current automatic
detection methods often lack explainability and robustness in detecting new
phishing attacks. In this work, we investigate the effectiveness of
character-level deep learning models for phishing detection, which can provide
both robustness and interpretability. We evaluate three neural architectures
adapted to operate at the character level, namely CharCNN, CharGRU, and
CharBiLSTM, on a custom-built email dataset, which combines data from multiple
sources. Their performance is analyzed under three scenarios: (i) standard
training and testing, (ii) standard training and testing under adversarial
attacks, and (iii) training and testing with adversarial examples. Aiming to
develop a tool that operates as a browser extension, we test all models under
limited computational resources. In this constrained setup, CharGRU proves to
be the best-performing model across all scenarios. All models show
vulnerability to adversarial attacks, but adversarial training substantially
improves their robustness. In addition, by adapting the Gradient-weighted Class
Activation Mapping (Grad-CAM) technique to character-level inputs, we are able
to visualize which parts of each email influence the decision of each model.
Our open-source code and data is released at
https://github.com/chipermaria/every-character-counts.

</details>


### [228] [A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks](https://arxiv.org/abs/2509.20639)
*Adam Swanda,Amy Chang,Alexander Chen,Fraser Burch,Paul Kassianik,Konstantin Berlin*

Main category: cs.CR

TL;DR: 大语言模型易受攻击，现有方法难防新攻击，本文提出生产级防御系统应对LLM威胁。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用但易受攻击，现有方法难以防范零日或新型攻击，且此前工作多评估单个检测模型，缺乏端到端系统。

Method: 基于成熟的恶意软件检测和威胁情报实践，构建包含威胁情报系统、数据平台和发布平台三个组件的防御系统。

Result: 系统各组件协同工作，能对不断演变的LLM威胁提供分层保护，生成训练数据以持续改进模型，且不中断生产地部署更新。

Conclusion: 所提出的生产级防御系统可有效应对LLM面临的安全威胁。

Abstract: The widespread adoption of Large Language Models (LLMs) has revolutionized AI
deployment, enabling autonomous and semi-autonomous applications across
industries through intuitive language interfaces and continuous improvements in
model development. However, the attendant increase in autonomy and expansion of
access permissions among AI applications also make these systems compelling
targets for malicious attacks. Their inherent susceptibility to security flaws
necessitates robust defenses, yet no known approaches can prevent zero-day or
novel attacks against LLMs. This places AI protection systems in a category
similar to established malware protection systems: rather than providing
guaranteed immunity, they minimize risk through enhanced observability,
multi-layered defense, and rapid threat response, supported by a threat
intelligence function designed specifically for AI-related threats.
  Prior work on LLM protection has largely evaluated individual detection
models rather than end-to-end systems designed for continuous, rapid adaptation
to a changing threat landscape. We present a production-grade defense system
rooted in established malware detection and threat intelligence practices. Our
platform integrates three components: a threat intelligence system that turns
emerging threats into protections; a data platform that aggregates and enriches
information while providing observability, monitoring, and ML operations; and a
release platform enabling safe, rapid detection updates without disrupting
customer workflows. Together, these components deliver layered protection
against evolving LLM threats while generating training data for continuous
model improvement and deploying updates without interrupting production.

</details>


### [229] [Security-aware Semantic-driven ISAC via Paired Adversarial Residual Networks](https://arxiv.org/abs/2509.20835)
*Yu Liu,Boxiang He,Fanggang Wang*

Main category: cs.CR

TL;DR: 提出安全语义ISAC（SS - ISAC）框架，设计加密和解密模块，联合优化ARN，仿真验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在保证传感和通信性能的同时预防窃听威胁。

Method: 设计可插拔加密和解密模块，采用可训练的对抗残差网络（ARN）进行对抗攻击和缓解，通过最小化精心设计的损失函数联合优化ARN。

Result: 仿真结果验证了所提出的SS - ISAC框架在传感和通信以及防窃听性能方面的有效性。

Conclusion: 所提出的SS - ISAC框架有效，能在保证SAC性能的同时防止窃听威胁，且可根据系统安全需求灵活组装模块，无需大幅修改硬件基础设施。

Abstract: This paper proposes a novel and flexible security-aware semantic-driven
integrated sensing and communication (ISAC) framework, namely security semantic
ISAC (SS-ISAC). Inspired by the positive impact of the adversarial attack, a
pair of pluggable encryption and decryption modules is designed in the proposed
SS-ISAC framework. The encryption module is installed after the semantic
transmitter, adopting a trainable adversarial residual network (ARN) to create
the adversarial attack. Correspondingly, the decryption module before the
semantic receiver utilizes another trainable ARN to mitigate the adversarial
attack and noise. These two modules can be flexibly assembled considering the
system security demands, without drastically modifying the hardware
infrastructure. To ensure the sensing and communication (SAC) performance while
preventing the eavesdropping threat, the above ARNs are jointly optimized by
minimizing a carefully designed loss function that relates to the adversarial
attack power, SAC performance, as well as the privacy leakage risk. Simulation
results validate the effectiveness of the proposed SS-ISAC framework in terms
of both SAC and eavesdropping prevention performance.

</details>


### [230] [A Comparative Analysis of Ensemble-Based Machine Learning Approaches with Explainable AI for Multi-Class Intrusion Detection in Drone Networks](https://arxiv.org/abs/2509.20391)
*Md. Alamgir Hossain,Waqas Ishtiaq,Md. Samiul Islam*

Main category: cs.CR

TL;DR: 本文针对无人机网络的网络入侵检测问题，对比多种集成机器学习模型，随机森林表现最佳，还结合可解释AI方法，保证模型准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 无人机融入多领域带来网络安全问题，检测和分类入侵有挑战，需开发针对无人机网络的入侵检测框架。

Method: 对比随机森林、Extra Trees等集成机器学习模型，对数据集进行预处理，用多种指标评估模型，应用统计测试验证模型优越性，集成SHAP和LIME解释特征重要性。

Result: 随机森林性能最佳，宏F1分数为0.9998，ROC AUC为1.0000。

Conclusion: 该方法准确性高且具有可解释性，适合实时和安全关键的无人机操作。

Abstract: The growing integration of drones into civilian, commercial, and defense
sectors introduces significant cybersecurity concerns, particularly with the
increased risk of network-based intrusions targeting drone communication
protocols. Detecting and classifying these intrusions is inherently challenging
due to the dynamic nature of drone traffic and the presence of multiple
sophisticated attack vectors such as spoofing, injection, replay, and
man-in-the-middle (MITM) attacks. This research aims to develop a robust and
interpretable intrusion detection framework tailored for drone networks, with a
focus on handling multi-class classification and model explainability. We
present a comparative analysis of ensemble-based machine learning models,
namely Random Forest, Extra Trees, AdaBoost, CatBoost, and XGBoost, trained on
a labeled dataset comprising benign traffic and nine distinct intrusion types.
Comprehensive data preprocessing was performed, including missing value
imputation, scaling, and categorical encoding, followed by model training and
extensive evaluation using metrics such as macro F1-score, ROC AUC, Matthews
Correlation Coefficient, and Log Loss. Random Forest achieved the highest
performance with a macro F1-score of 0.9998 and ROC AUC of 1.0000. To validate
the superiority of the models, statistical tests, including Friedmans test, the
Wilcoxon signed-rank test with Holm correction, and bootstrapped confidence
intervals, were applied. Furthermore, explainable AI methods, SHAP and LIME,
were integrated to interpret both global and local feature importance,
enhancing model transparency and decision trustworthiness. The proposed
approach not only delivers near-perfect accuracy but also ensures
interpretability, making it highly suitable for real-time and safety-critical
drone operations.

</details>


### [231] [CTI Dataset Construction from Telegram](https://arxiv.org/abs/2509.20943)
*Dincy R. Arikkat,Sneha B. T.,Serena Nicolazzo,Antonino Nocera,Vinod P.,Rafidha Rehiman K. A.,Karthika R*

Main category: cs.CR

TL;DR: 本文提出从Telegram收集威胁情报内容的端到端自动化管道，构建大规模、高保真CTI数据集。


<details>
  <summary>Details</summary>
Motivation: 网络威胁不断演变，CTI有效性依赖高质量数据集，Telegram是有价值的CTI来源。

Method: 使用自动化管道从Telegram收集和过滤威胁相关内容，用BERT分类器过滤消息。

Result: 从12个精选频道收集145,349条消息，分类器准确率96.64%，编译含86,509个恶意指标的数据集。

Conclusion: 该方法构建了大规模、高保真CTI数据集，为网络威胁检测研究和应用奠定基础。

Abstract: Cyber Threat Intelligence (CTI) enables organizations to anticipate, detect,
and mitigate evolving cyber threats. Its effectiveness depends on high-quality
datasets, which support model development, training, evaluation, and
benchmarking. Building such datasets is crucial, as attack vectors and
adversary tactics continually evolve. Recently, Telegram has gained prominence
as a valuable CTI source, offering timely and diverse threat-related
information that can help address these challenges. In this work, we address
these challenges by presenting an end-to-end automated pipeline that
systematically collects and filters threat-related content from Telegram. The
pipeline identifies relevant Telegram channels and scrapes 145,349 messages
from 12 curated channels out of 150 identified sources. To accurately filter
threat intelligence messages from generic content, we employ a BERT-based
classifier, achieving an accuracy of 96.64%. From the filtered messages, we
compile a dataset of 86,509 malicious Indicators of Compromise, including
domains, IPs, URLs, hashes, and CVEs. This approach not only produces a
large-scale, high-fidelity CTI dataset but also establishes a foundation for
future research and operational applications in cyber threat detection.

</details>


### [232] [Dual-Path Phishing Detection: Integrating Transformer-Based NLP with Structural URL Analysis](https://arxiv.org/abs/2509.20972)
*Ibrahim Altan,Abdulla Bachir,Yousuf Parbhulkar,Abdul Muksith Rizvi,Moshiur Farazi*

Main category: cs.CR

TL;DR: 本文提出双路径钓鱼邮件检测框架，结合NLP与机器学习分析邮件文本和URL，提高检测准确率。


<details>
  <summary>Details</summary>
Motivation: 钓鱼邮件威胁不断演变，传统检测方法无法全面应对。

Method: 提出双路径框架，结合基于transformer的NLP与经典机器学习，利用微调的transformer架构进行语义分析，通过字符级TF - IDF向量化和经典分类器进行结构链接分析。

Result: 在代表性数据集上，该方法显著提高检测准确率，DistilBERT在文本检测中平衡了准确性和计算效率，随机森林在识别恶意URL上表现出色。

Conclusion: 双路径方法有效且实用，能增强邮件安全，是可扩展、准确且可解释的解决方案。

Abstract: Phishing emails pose a persistent and increasingly sophisticated threat,
undermining email security through deceptive tactics designed to exploit both
semantic and structural vulnerabilities. Traditional detection methods, often
based on isolated analysis of email content or embedded URLs, fail to
comprehensively address these evolving attacks. In this paper, we propose a
dual-path phishing detection framework that integrates transformer-based
natural language processing (NLP) with classical machine learning to jointly
analyze email text and embedded URLs. Our approach leverages the complementary
strengths of semantic analysis using fine-tuned transformer architectures
(e.g., DistilBERT) and structural link analysis via character-level TF-IDF
vectorization paired with classical classifiers (e.g., Random Forest).
Empirical evaluation on representative email and URL datasets demonstrates that
this combined approach significantly improves detection accuracy. Specifically,
the DistilBERT model achieves a near-optimal balance between accuracy and
computational efficiency for textual phishing detection, while Random Forest
notably outperforms other classical classifiers in identifying malicious URLs.
The modular design allows flexibility for standalone deployment or ensemble
integration, facilitating real-world adoption. Collectively, our results
highlight the efficacy and practical value of this dual-path approach,
establishing a scalable, accurate, and interpretable solution capable of
enhancing email security against contemporary phishing threats.

</details>


### [233] [Cryptographic Backdoor for Neural Networks: Boon and Bane](https://arxiv.org/abs/2509.20714)
*Anh Tu Ngo,Anupam Chattopadhyay,Subhamoy Maitra*

Main category: cs.CR

TL;DR: 本文表明神经网络中的密码学后门在攻击和防御两方面都有效，给出攻击方式和防御应用，理论证明方案鲁棒性，进行实验验证并为量子时代应用奠基。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络中密码学后门在攻击和防御方面的有效性及应用。

Method: 借鉴Goldwasser等人的思想，在最先进的神经网络架构上实现相关协议。

Result: 攻击方面可实现强大无形攻击；防御方面提出水印、认证和知识产权跟踪协议，且方案理论上鲁棒，实验结果与理论相符。

Conclusion: 密码学后门在神经网络攻击和防御中有重要作用，相关协议理论和实践可行，可利用后量子原语用于量子时代机器学习应用。

Abstract: In this paper we show that cryptographic backdoors in a neural network (NN)
can be highly effective in two directions, namely mounting the attacks as well
as in presenting the defenses as well. On the attack side, a carefully planted
cryptographic backdoor enables powerful and invisible attack on the NN.
Considering the defense, we present applications: first, a provably robust NN
watermarking scheme; second, a protocol for guaranteeing user authentication;
and third, a protocol for tracking unauthorized sharing of the NN intellectual
property (IP). From a broader theoretical perspective, borrowing the ideas from
Goldwasser et. al. [FOCS 2022], our main contribution is to show that all these
instantiated practical protocol implementations are provably robust. The
protocols for watermarking, authentication and IP tracking resist an adversary
with black-box access to the NN, whereas the backdoor-enabled adversarial
attack is impossible to prevent under the standard assumptions. While the
theoretical tools used for our attack is mostly in line with the Goldwasser et.
al. ideas, the proofs related to the defense need further studies. Finally, all
these protocols are implemented on state-of-the-art NN architectures with
empirical results corroborating the theoretical claims. Further, one can
utilize post-quantum primitives for implementing the cryptographic backdoors,
laying out foundations for quantum-era applications in machine learning (ML).

</details>


### [234] [Emerging Paradigms for Securing Federated Learning Systems](https://arxiv.org/abs/2509.21147)
*Amr Akmal Abouelmagd,Amr Hilal*

Main category: cs.CR

TL;DR: 文章探讨联邦学习现有隐私保护技术问题，介绍新兴方法并评估其适用性，指出挑战与研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习隐私保护技术存在计算成本高、可扩展性有限的问题，需寻找新方法提升隐私和效率。

Method: 对新兴方法如可信执行环境、物理不可克隆函数等进行评估，分析其在联邦学习流程中的相关性、优缺点及实际考量。

Result: 明确各新兴方法的特点和适用性。

Conclusion: 指出当前存在的开放挑战和潜在研究途径，给出推进安全可扩展联邦学习系统的详细路线图。

Abstract: Federated Learning (FL) facilitates collaborative model training while
keeping raw data decentralized, making it a conduit for leveraging the power of
IoT devices while maintaining privacy of the locally collected data. However,
existing privacy- preserving techniques present notable hurdles. Methods such
as Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential
Privacy (DP) often incur high compu- tational costs and suffer from limited
scalability. This survey examines emerging approaches that hold promise for
enhancing both privacy and efficiency in FL, including Trusted Execution
Environments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing
(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm
Intelligence (SI). For each paradigm, we assess its relevance to the FL
pipeline, outlining its strengths, limitations, and practical considerations.
We conclude by highlighting open challenges and prospective research avenues,
offering a detailed roadmap for advancing secure and scalable FL systems.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [235] [Data-driven Neural Networks for Windkessel Parameter Calibration](https://arxiv.org/abs/2509.21206)
*Benedikt Hoock,Tobias Köppl*

Main category: q-bio.TO

TL;DR: 提出在降维1D - 0D耦合血流模型中校准Windkessel参数的新方法，用神经网络实现并评估其在不同场景的有效性。


<details>
  <summary>Details</summary>
Motivation: 在降维1D - 0D耦合血流模型中校准Windkessel参数，评估方法在不同场景（测量位置未知或数据受噪声影响）的有效性。

Method: 设计数据驱动的神经网络，在模拟的左肱动脉血压上训练，用虚拟神经元扩展网络并重新训练以校准参数。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: In this work, we propose a novel method for calibrating Windkessel (WK)
parameters in a dimensionally reduced 1D-0D coupled blood flow model. To this
end, we design a data-driven neural network (NN)trained on simulated blood
pressures in the left brachial artery. Once trained, the NN emulates the
pressure pulse waves across the entire simulated domain, i.e., over time, space
and varying WK parameters, with negligible error and computational effort. To
calibrate the WK parameters on a measured pulse wave, the NN is extended by
dummy neurons and retrained only on these. The main objective of this work is
to assess the effectiveness of the method in various scenarios -- particularly,
when the exact measurement location is unknown or the data are affected by
noise.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [236] [Identifying Group Anchors in Real-World Group Interactions Under Label Scarcity](https://arxiv.org/abs/2509.20762)
*Fanchen Bu,Geon Lee,Minyoung Choe,Kijung Shin*

Main category: cs.SI

TL;DR: 本文讨论现实群组交互中群组锚点的存在及识别问题，提出AnchorRadar方法，实验证明其在准确率和效率上优于基线。


<details>
  <summary>Details</summary>
Motivation: 探讨现实世界群组交互中特殊重要成员（群组锚点）的存在，并解决其识别问题。

Method: 引入群组锚点概念和识别问题，基于观察开发半监督方法AnchorRadar，利用有和无已知锚点的群组信息。

Result: 在十三个真实数据集上实验表明，AnchorRadar在准确率和效率上优于各种基线，多数情况下准确率更高，训练时间平均少10.2倍，可学习参数平均少43.6倍。

Conclusion: AnchorRadar是一种在标签稀缺的现实场景下，用于群组锚点识别的快速有效方法。

Abstract: Group interactions occur in various real-world contexts, e.g., co-authorship,
email communication, and online Q&A. In each group, there is often a
particularly significant member, around whom the group is formed. Examples
include the first or last author of a paper, the sender of an email, and the
questioner in a Q&A session. In this work, we discuss the existence of such
individuals in real-world group interactions. We call such individuals group
anchors and study the problem of identifying them. First, we introduce the
concept of group anchors and the identification problem. Then, we discuss our
observations on group anchors in real-world group interactions. Based on our
observations, we develop AnchorRadar, a fast and effective method for group
anchor identification under realistic settings with label scarcity, i.e., when
only a few groups have known anchors. AnchorRadar is a semi-supervised method
using information from groups both with and without known group anchors.
Finally, through extensive experiments on thirteen real-world datasets, we
demonstrate the empirical superiority of AnchorRadar over various baselines
w.r.t. accuracy and efficiency. In most cases, AnchorRadar achieves higher
accuracy in group anchor identification than all the baselines, while using
10.2$\times$ less training time than the fastest baseline and 43.6$\times$
fewer learnable parameters than the most lightweight baseline on average.

</details>


### [237] [Evading Overlapping Community Detection via Proxy Node Injection](https://arxiv.org/abs/2509.21211)
*Dario Loi,Matteo Silvestri,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.SI

TL;DR: 本文提出深度强化学习方法解决重叠社区图的社区成员隐藏问题，实验显示该方法在效果和效率上远超基线。


<details>
  <summary>Details</summary>
Motivation: 保护社交图隐私，防止敏感信息被推断，解决重叠社区下的社区成员隐藏问题。

Method: 提出深度强化学习（DRL）方法，学习有效修改策略并保留图结构，使用代理节点。

Result: 在真实数据集实验中，该方法在效果和效率上显著优于现有基线。

Conclusion: 该方法为重叠社区的隐私保护图修改提供了有效工具。

Abstract: Protecting privacy in social graphs requires preventing sensitive
information, such as community affiliations, from being inferred by graph
analysis, without substantially altering the graph topology. We address this
through the problem of \emph{community membership hiding} (CMH), which seeks
edge modifications that cause a target node to exit its original community,
regardless of the detection algorithm employed. Prior work has focused on
non-overlapping community detection, where trivial strategies often suffice,
but real-world graphs are better modeled by overlapping communities, where such
strategies fail. To the best of our knowledge, we are the first to formalize
and address CMH in this setting. In this work, we propose a deep reinforcement
learning (DRL) approach that learns effective modification policies, including
the use of proxy nodes, while preserving graph structure. Experiments on
real-world datasets show that our method significantly outperforms existing
baselines in both effectiveness and efficiency, offering a principled tool for
privacy-preserving graph modification with overlapping communities.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [238] [Average-Case Complexity of Quantum Stabilizer Decoding](https://arxiv.org/abs/2509.20697)
*Andrey Boris Khesin,Jonathan Z. Lu,Alexander Poremba,Akshar Ramkumar,Vinod Vaikuntanathan*

Main category: quant-ph

TL;DR: 本文证明随机量子稳定子码解码至少和恒定速率随机经典码解码一样难，还刻画了稳定子码的其他复杂度性质。


<details>
  <summary>Details</summary>
Motivation: 填补随机量子码与随机经典码解码算法复杂度理解上的空白。

Method: 通过证明随机稳定子码解码难度、研究Clifford熵和Pauli混合时间的新界限等方法。

Result: 证明随机稳定子码解码至少和恒定速率随机经典码解码一样难，发现量子解码随机自约简存在障碍，也证明了一些可实现的自约简。

Conclusion: 随机量子解码问题至少和最难的随机经典解码问题一样难，亚指数算法解码典型稳定子码会带来密码学突破，量子现象使稳定子解码的不同定义复杂度不同。

Abstract: Random classical linear codes are widely believed to be hard to decode. While
slightly sub-exponential time algorithms exist when the coding rate vanishes
sufficiently rapidly, all known algorithms at constant rate require exponential
time. By contrast, the complexity of decoding a random quantum stabilizer code
has remained an open question for quite some time. This work closes the gap in
our understanding of the algorithmic hardness of decoding random quantum versus
random classical codes. We prove that decoding a random stabilizer code with
even a single logical qubit is at least as hard as decoding a random classical
code at constant rate--the maximally hard regime. This result suggests that the
easiest random quantum decoding problem is at least as hard as the hardest
random classical decoding problem, and shows that any sub-exponential algorithm
decoding a typical stabilizer code, at any rate, would immediately imply a
breakthrough in cryptography.
  More generally, we also characterize many other complexity-theoretic
properties of stabilizer codes. While classical decoding admits a random
self-reduction, we prove significant barriers for the existence of random
self-reductions in the quantum case. This result follows from new bounds on
Clifford entropies and Pauli mixing times, which may be of independent
interest. As a complementary result, we demonstrate various other
self-reductions which are in fact achievable, such as between search and
decision. We also demonstrate several ways in which quantum phenomena, such as
quantum degeneracy, force several reasonable definitions of stabilizer
decoding--all of which are classically identical--to have distinct or
non-trivially equivalent complexity.

</details>


### [239] [Towards a user-centric HPC-QC environment](https://arxiv.org/abs/2509.20525)
*Aleksander Wennersteen,Matthieu Moreau,Aurelien Nober,Mourad Beji*

Main category: quant-ph

TL;DR: 本文展示了用于高性能计算环境中开发和执行混合量子 - 经典程序的可移植运行时环境的进展，介绍了中间件、资源管理和监控等内容。


<details>
  <summary>Details</summary>
Motivation: 健壮的执行环境对解决量子计算的关键挑战很重要，推动混合量子工作流发展，需开发适用于高性能计算环境的可移植运行时环境。

Method: 在主HPC资源管理器后增加第二层调度以提高QPU利用率，基于量子资源管理接口管理多个编程SDK，提出监控和可观测性堆栈解决方案。

Result: 展示了开发和执行混合量子 - 经典程序的可移植运行时环境的进展，完成了混合系统架构描述。

Conclusion: 朝着开发高性能计算环境中混合量子 - 经典程序的可移植运行时环境取得了进展。

Abstract: Robust execution environments are important for addressing key challenges in
quantum computing, such as application development, portability, and
reproducibility, and help unlock the development of modular quantum programs,
driving forward hybrid quantum workflows. In this work, we show progress
towards a basic, but portable, runtime environment for developing and executing
hybrid quantum-classical programs running in High Performance Computing (HPC)
environments enhanced with Quantum Processing Units (QPUs). The middleware
includes a second layer of scheduling after the main HPC resource manager in
order to improve the utilization of the QPU, and extra functionality for
observability, monitoring, and admin access. This approach enables managing
multiple programming Software Development Kits (SDKs) as first-class citizens
in the environment by building on a recently proposed vendor-neutral Quantum
Resource Management Interface (QRMI). Lastly, we discuss and show a solution
for the monitoring and observability stack, completing our description of the
hybrid system architecture.

</details>


### [240] [PALQO: Physics-informed Model for Accelerating Large-scale Quantum Optimization](https://arxiv.org/abs/2509.20733)
*Yiming Huang,Yajie Hao,Jing Zhou,Xiao Yuan,Xiaoting Wang,Yuxuan Du*

Main category: quant-ph

TL;DR: 本文提出用PINNs高效建模VQAs训练动力学系统，减少量子资源成本，实验显示有显著加速和成本降低效果。


<details>
  <summary>Details</summary>
Motivation: 量子力学的不可克隆定理使VQAs应用于大规模任务时量子资源成本过高。

Method: 将VQAs训练动力学重新表述为非线性偏微分方程，用PINNs建模该动力系统。

Result: 相比传统方法实现高达30倍加速，处理40量子位任务时量子资源成本降低90%，精度有竞争力。

Conclusion: 该方法补充现有技术，增强VQAs实际应用潜力。

Abstract: Variational quantum algorithms (VQAs) are leading strategies to reach
practical utilities of near-term quantum devices. However, the no-cloning
theorem in quantum mechanics precludes standard backpropagation, leading to
prohibitive quantum resource costs when applying VQAs to large-scale tasks. To
address this challenge, we reformulate the training dynamics of VQAs as a
nonlinear partial differential equation and propose a novel protocol that
leverages physics-informed neural networks (PINNs) to model this dynamical
system efficiently. Given a small amount of training trajectory data collected
from quantum devices, our protocol predicts the parameter updates of VQAs over
multiple iterations on the classical side, dramatically reducing quantum
resource costs. Through systematic numerical experiments, we demonstrate that
our method achieves up to a 30x speedup compared to conventional methods and
reduces quantum resource costs by as much as 90\% for tasks involving up to 40
qubits, including ground state preparation of different quantum systems, while
maintaining competitive accuracy. Our approach complements existing techniques
aimed at improving the efficiency of VQAs and further strengthens their
potential for practical applications.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [241] [An LLM-based Agentic Framework for Accessible Network Control](https://arxiv.org/abs/2509.20600)
*Samuel Lin,Jiawei Zhou,Minlan Yu*

Main category: cs.NI

TL;DR: 设计系统让非专家用户能通过自然语言管理网络，提出框架并开展研究，初步实验验证有效性，为网络控制普及奠基。


<details>
  <summary>Details</summary>
Motivation: 传统网络管理需专业知识，给普通用户带来障碍，希望利用大语言模型让更多非专家用户能管理网络。

Method: 提出代理框架，用中间表示简化不同厂商设备配置，实时从内存获取网络状态，提供外部反馈接口；开展试点研究收集用户自然语言数据，提供可视化界面。

Result: 初步实验验证了系统组件与大语言模型集成在合成和真实用户表述上的有效性。

Conclusion: 数据收集和可视化工作为大语言模型更有效使用铺平道路，实现网络控制对普通用户的普及。

Abstract: Traditional approaches to network management have been accessible only to a
handful of highly-trained network operators with significant expert knowledge.
This creates barriers for lay users to easily manage their networks without
resorting to experts. With recent development of powerful large language models
(LLMs) for language comprehension, we design a system to make network
management accessible to a broader audience of non-experts by allowing users to
converse with networks in natural language. To effectively leverage
advancements in LLMs, we propose an agentic framework that uses an intermediate
representation to streamline configuration across diverse vendor equipment,
retrieves the network state from memory in real-time, and provides an interface
for external feedback. We also conduct pilot studies to collect real user data
of natural language utterances for network control, and present a visualization
interface to facilitate dialogue-driven user interaction and enable large-scale
data collection for future development. Preliminary experiments validate the
effectiveness of our proposed system components with LLM integration on both
synthetic and real user utterances. Through our data collection and
visualization efforts, we pave the way for more effective use of LLMs and
democratize network control for everyday users.

</details>


### [242] [Trustworthy Semantic Communication for Vehicular Networks: Challenges and Solutions](https://arxiv.org/abs/2509.20830)
*Yanghe Pan,Yuntao Wang,Shaolong Guo,Chengyu Yin,Ruidong Li,Zhou Su,Yuan Wu*

Main category: cs.NI

TL;DR: 本文提出创新的三层可信车载语义通信网络架构，介绍多种防御机制，通过案例验证有效性并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 车载语义通信网络在信息传输、语义编码和通信实体可靠性方面面临关键信任挑战。

Method: 提出三层可信VN - SemComNet架构，引入语义伪装传输机制、鲁棒联邦编解码器训练框架和基于审计博弈的分布式车辆信任管理机制。

Result: 通过案例研究验证了所提解决方案的有效性。

Conclusion: 指出推进该新兴领域的重要未来研究方向。

Abstract: Semantic communication (SemCom) has the potential to significantly reduce
communication delay in vehicle-to-everything (V2X) communications within
vehicular networks (VNs). However, the deployment of vehicular SemCom networks
(VN-SemComNets) faces critical trust challenges in information transmission,
semantic encoding, and communication entity reliability. This paper proposes an
innovative three-layer trustworthy VN-SemComNet architecture. Specifically, we
introduce a semantic camouflage transmission mechanism leveraging defensive
adversarial noise for active eavesdropping defense, a robust federated
encoder-decoder training framework to mitigate encoder-decoder poisoning
attacks, and an audit game-based distributed vehicle trust management mechanism
to deter untrustworthy vehicles. A case study validates the effectiveness of
the proposed solutions. Lastly, essential future research directions are
pointed out to advance this emerging field.

</details>


### [243] [Semantic Edge-Cloud Communication for Real-Time Urban Traffic Surveillance with ViT and LLMs over Mobile Networks](https://arxiv.org/abs/2509.21259)
*Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Aisha Syed,Matthew Andrews,Sean Kennedy*

Main category: cs.NI

TL;DR: 提出语义通信框架解决实时城市交通监控中边缘到云传输问题，减少传输开销，验证了ViT和LLM辅助的边缘 - 云语义通信的效率和实用性。


<details>
  <summary>Details</summary>
Motivation: 实时城市交通监控中，多模态大语言模型部署在边缘设备不可行，边缘到云传输因带宽有限有延迟，影响实时性能。

Method: 用YOLOv11检测感兴趣区域，裁剪相关图像段，用Vision Transformer将其转换为紧凑嵌入向量传输到云，图像解码器重建图像，再由多模态大语言模型处理。

Result: 数据传输大小减少99.9%，重建裁剪图像的LLM响应准确率为89%，原始裁剪图像为93%。

Conclusion: ViT和LLM辅助的边缘 - 云语义通信用于实时交通监控是高效且实用的。

Abstract: Real-time urban traffic surveillance is vital for Intelligent Transportation
Systems (ITS) to ensure road safety, optimize traffic flow, track vehicle
trajectories, and prevent collisions in smart cities. Deploying edge cameras
across urban environments is a standard practice for monitoring road
conditions. However, integrating these with intelligent models requires a
robust understanding of dynamic traffic scenarios and a responsive interface
for user interaction. Although multimodal Large Language Models (LLMs) can
interpret traffic images and generate informative responses, their deployment
on edge devices is infeasible due to high computational demands. Therefore, LLM
inference must occur on the cloud, necessitating visual data transmission from
edge to cloud, a process hindered by limited bandwidth, leading to potential
delays that compromise real-time performance. To address this challenge, we
propose a semantic communication framework that significantly reduces
transmission overhead. Our method involves detecting Regions of Interest (RoIs)
using YOLOv11, cropping relevant image segments, and converting them into
compact embedding vectors using a Vision Transformer (ViT). These embeddings
are then transmitted to the cloud, where an image decoder reconstructs the
cropped images. The reconstructed images are processed by a multimodal LLM to
generate traffic condition descriptions. This approach achieves a 99.9%
reduction in data transmission size while maintaining an LLM response accuracy
of 89% for reconstructed cropped images, compared to 93% accuracy with original
cropped images. Our results demonstrate the efficiency and practicality of ViT
and LLM-assisted edge-cloud semantic communication for real-time traffic
surveillance.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [244] [The Use of the Simplex Architecture to Enhance Safety in Deep-Learning-Powered Autonomous Systems](https://arxiv.org/abs/2509.21014)
*Federico Nesti,Niko Salamini,Mauro Marinoni,Giorgio Maria Cicero,Gabriele Serra,Alessandro Biondi,Giorgio Buttazzo*

Main category: eess.SY

TL;DR: 本文提出一种软件架构以提升基于学习的自主系统的安全、安保和可预测性，通过实验验证其能防止学习组件导致的故障。


<details>
  <summary>Details</summary>
Motivation: 神经网络在自主系统中应用时存在不可信问题，且加速推理的框架运行在复杂操作系统上，在时序行为和网络攻击面存在不足。

Method: 提出一种软件架构，利用两个隔离执行域，通过1型实时虚拟机管理程序实现通信，基于安全监视器提供故障安全机制。

Result: 在Furuta摆和漫游车两个控制系统上的实验表明，回退机制能防止学习组件导致的故障。

Conclusion: 所提出的架构能有效提升基于学习的自主系统的安全、安保和可预测性。

Abstract: Recently, the outstanding performance reached by neural networks in many
tasks has led to their deployment in autonomous systems, such as robots and
vehicles. However, neural networks are not yet trustworthy, being prone to
different types of misbehavior, such as anomalous samples, distribution shifts,
adversarial attacks, and other threats. Furthermore, frameworks for
accelerating the inference of neural networks typically run on rich operating
systems that are less predictable in terms of timing behavior and present
larger surfaces for cyber-attacks.
  To address these issues, this paper presents a software architecture for
enhancing safety, security, and predictability levels of learning-based
autonomous systems. It leverages two isolated execution domains, one dedicated
to the execution of neural networks under a rich operating system, which is
deemed not trustworthy, and one responsible for running safety-critical
functions, possibly under a different operating system capable of handling
real-time constraints.
  Both domains are hosted on the same computing platform and isolated through a
type-1 real-time hypervisor enabling fast and predictable inter-domain
communication to exchange real-time data. The two domains cooperate to provide
a fail-safe mechanism based on a safety monitor, which oversees the state of
the system and switches to a simpler but safer backup module, hosted in the
safety-critical domain, whenever its behavior is considered untrustworthy.
  The effectiveness of the proposed architecture is illustrated by a set of
experiments performed on two control systems: a Furuta pendulum and a rover.
The results confirm the utility of the fall-back mechanism in preventing faults
due to the learning component.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [245] [MLIP Arena: Advancing Fairness and Transparency in Machine Learning Interatomic Potentials via an Open, Accessible Benchmark Platform](https://arxiv.org/abs/2509.20630)
*Yuan Chiang,Tobias Kreiman,Christine Zhang,Matthew C. Kuner,Elizabeth Weaver,Ishan Amin,Hyunsoo Park,Yunsung Lim,Jihan Kim,Daryl Chrzan,Aron Walsh,Samuel M. Blau,Mark Asta,Aditi S. Krishnapriyan*

Main category: physics.chem-ph

TL;DR: 现有机器学习原子间势（MLIPs）基准存在问题，本文介绍MLIP Arena基准平台评估力场性能，为下一代MLIP发展提供框架。


<details>
  <summary>Details</summary>
Motivation: 现有MLIP基准存在数据泄漏、可迁移性有限和过度依赖特定DFT参考的误差指标等问题。

Method: 引入MLIP Arena基准平台，基于物理感知、化学反应性、极端条件下稳定性以及对热力学性质和物理现象的预测能力评估力场性能。

Result: MLIP Arena能揭示当前基础MLIP在现实场景中的重要失败模式。

Conclusion: MLIP Arena为下一代MLIP发展提供可重现框架，提高预测准确性和运行效率，保持物理一致性，相关代码和排行榜已公开。

Abstract: Machine learning interatomic potentials (MLIPs) have revolutionized molecular
and materials modeling, but existing benchmarks suffer from data leakage,
limited transferability, and an over-reliance on error-based metrics tied to
specific density functional theory (DFT) references. We introduce MLIP Arena, a
benchmark platform that evaluates force field performance based on physics
awareness, chemical reactivity, stability under extreme conditions, and
predictive capabilities for thermodynamic properties and physical phenomena. By
moving beyond static DFT references and revealing the important failure modes
of current foundation MLIPs in real-world settings, MLIP Arena provides a
reproducible framework to guide the next-generation MLIP development toward
improved predictive accuracy and runtime efficiency while maintaining physical
consistency. The Python package and online leaderboard are available at
https://github.com/atomind-ai/mlip-arena.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [246] [Communication Bias in Large Language Models: A Regulatory Perspective](https://arxiv.org/abs/2509.21075)
*Adrian Kuenzler,Stefan Schmid*

Main category: cs.CY

TL;DR: 本文探讨大语言模型输出偏差风险及社会影响，认为除监管外还需关注竞争和设计治理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在应用中引发对偏差、公平性和监管合规性的担忧。

Method: 回顾大语言模型偏差输出的风险及其社会影响，聚焦欧盟相关法案框架。

Result: 无明确具体结果

Conclusion: 除持续监管外，需更关注竞争和设计治理以确保人工智能公平可信。

Abstract: Large language models (LLMs) are increasingly central to many applications,
raising concerns about bias, fairness, and regulatory compliance. This paper
reviews risks of biased outputs and their societal impact, focusing on
frameworks like the EU's AI Act and the Digital Services Act. We argue that
beyond constant regulation, stronger attention to competition and design
governance is needed to ensure fair, trustworthy AI. This is a preprint of the
Communications of the ACM article of the same title.

</details>


### [247] [AI-driven formative assessment and adaptive learning in data-science education: Evaluating an LLM-powered virtual teaching assistant](https://arxiv.org/abs/2509.20369)
*Fadjimata I Anaroua,Qing Li,Yan Tang,Hong P. Liu*

Main category: cs.CY

TL;DR: 本文介绍VITA平台，它嵌入大语言模型聊天机器人，用于数据科学劳动力培训，描述了数据管道、仪表盘等，与新兴辅导架构对比，给出贡献、实施经验和路线图，展示对话式AI可支持大规模个性化学习。


<details>
  <summary>Details</summary>
Motivation: 应对传统教学中不断增长的需求和可扩展性限制，为数据科学劳动力培训提供支持。

Method: 构建VITA平台，结合上下文感知对话辅导与形成性评估模式，有端到端数据管道、仪表盘和自适应路径引擎，还与新兴辅导架构进行概念性基准测试。

Result: 得出可复用的对话分析架构、完整性保护形成性评估模式目录，以及将自适应路径集成到数据科学课程的实用蓝图。

Conclusion: 对话式AI可支持大规模参与、及时反馈和个性化学习，后续将完善平台自适应智能并考察不同教育场景适用性。

Abstract: This paper presents VITA (Virtual Teaching Assistants), an adaptive
distributed learning (ADL) platform that embeds a large language model
(LLM)-powered chatbot (BotCaptain) to provide dialogic support, interoperable
analytics, and integrity-aware assessment for workforce preparation in data
science. The platform couples context-aware conversational tutoring with
formative-assessment patterns designed to promote reflective reasoning. The
paper describes an end-to-end data pipeline that transforms chat logs into
Experience API (xAPI) statements, instructor dashboards that surface outliers
for just-in-time intervention, and an adaptive pathway engine that routes
learners among progression, reinforcement, and remediation content. The paper
also benchmarks VITA conceptually against emerging tutoring architectures,
including retrieval-augmented generation (RAG)--based assistants and Learning
Tools Interoperability (LTI)--integrated hubs, highlighting trade-offs among
content grounding, interoperability, and deployment complexity. Contributions
include a reusable architecture for interoperable conversational analytics, a
catalog of patterns for integrity-preserving formative assessment, and a
practical blueprint for integrating adaptive pathways into data-science
courses. The paper concludes with implementation lessons and a roadmap (RAG
integration, hallucination mitigation, and LTI~1.3 / OpenID Connect) to guide
multi-course evaluations and broader adoption. In light of growing demand and
scalability constraints in traditional instruction, the approach illustrates
how conversational AI can support engagement, timely feedback, and personalized
learning at scale. Future work will refine the platform's adaptive intelligence
and examine applicability across varied educational settings.

</details>


### [248] [The Secret Agenda: LLMs Strategically Lie and Our Current Safety Tools Are Blind](https://arxiv.org/abs/2509.20393)
*Caleb DeLeeuw,Gaurav Chawla,Aniket Sharma,Vanessa Dietze*

Main category: cs.CY

TL;DR: 研究大语言模型战略欺骗，用两个测试台，发现自动标签可解释性方法难检测控制欺骗，未标记激活可用于风险评估，结果促更大规模研究。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型中的战略欺骗情况。

Method: 使用Secret Agenda和Insider Trading compliance两个互补测试台进行研究，包括分析自动标记特征、进行特征引导实验、用未标记激活分析。

Result: Secret Agenda能诱导模型说谎；自动标记的“欺骗”特征在战略不诚实中很少激活，特征引导实验无法防止说谎；未标记激活能通过判别模式区分欺骗和合规响应。

Conclusion: 自动标签驱动的可解释性方法难以检测或控制行为欺骗，未标记激活可用于风险评估，结果促使开展更大规模研究。

Abstract: We investigate strategic deception in large language models using two
complementary testbeds: Secret Agenda (across 38 models) and Insider Trading
compliance (via SAE architectures). Secret Agenda reliably induced lying when
deception advantaged goal achievement across all model families. Analysis
revealed that autolabeled SAE features for "deception" rarely activated during
strategic dishonesty, and feature steering experiments across 100+
deception-related features failed to prevent lying. Conversely, insider trading
analysis using unlabeled SAE activations separated deceptive versus compliant
responses through discriminative patterns in heatmaps and t-SNE visualizations.
These findings suggest autolabel-driven interpretability approaches fail to
detect or control behavioral deception, while aggregate unlabeled activations
provide population-level structure for risk assessment. Results span Llama
8B/70B SAE implementations and GemmaScope under resource constraints,
representing preliminary findings that motivate larger studies on feature
discovery, labeling methodology, and causal interventions in realistic
deception contexts.

</details>


### [249] [Blueprints of Trust: AI System Cards for End to End Transparency and Governance](https://arxiv.org/abs/2509.20394)
*Huzaifa Sidhpurwala,Emily Fox,Garth Mollett,Florencio Cano Gabarda,Roman Zhukov*

Main category: cs.CY

TL;DR: 本文介绍了Hazard - Aware System Card (HASC)框架，用于增强AI系统开发和部署的透明度与问责性，还与ISO/IEC 42001:2023标准对比。


<details>
  <summary>Details</summary>
Motivation: 增强AI系统开发和部署过程中的透明度和问责性。

Method: 构建HASC框架，整合AI系统安全态势的综合动态记录，提出标准化标识符系统，与ISO/IEC 42001:2023标准对比。

Result: HASC框架能为开发者和利益相关者提供单一可靠信息源，便于在AI系统全生命周期做安全决策。

Conclusion: HASC框架能增强AI系统的透明度和问责性，且可与ISO/IEC 42001:2023标准相互补充。

Abstract: This paper introduces the Hazard-Aware System Card (HASC), a novel framework
designed to enhance transparency and accountability in the development and
deployment of AI systems. The HASC builds upon existing model card and system
card concepts by integrating a comprehensive, dynamic record of an AI system's
security and safety posture. The framework proposes a standardized system of
identifiers, including a novel AI Safety Hazard (ASH) ID, to complement
existing security identifiers like CVEs, allowing for clear and consistent
communication of fixed flaws. By providing a single, accessible source of
truth, the HASC empowers developers and stakeholders to make more informed
decisions about AI system safety throughout its lifecycle. Ultimately, we also
compare our proposed AI system cards with the ISO/IEC 42001:2023 standard and
discuss how they can be used to complement each other, providing greater
transparency and accountability for AI systems.

</details>


### [250] [Wartime Media Dynamics in Emerging Democracies: Case Study of Pakistani Media in May 2025 Indo-Pak Conflict](https://arxiv.org/abs/2509.20419)
*Taaha Saleem Bajwa*

Main category: cs.CY

TL;DR: 研究2025年印巴冲突对巴基斯坦媒体报道的影响，发现战争报道使政治反对和异议报道边缘化，强调维护动荡地区新闻自由的必要性。


<details>
  <summary>Details</summary>
Motivation: 新兴民主国家言论自由常受限，地区冲突时更严重，研究印巴冲突对巴基斯坦媒体报道的影响。

Method: 使用大语言模型分析三家主要报纸约2600篇新闻文章。

Result: 战争相关报道显著掩盖了政治反对和异议的报道。

Conclusion: 冲突会使民主话语边缘化，需维护动荡地区的新闻自由。

Abstract: Democracies rely on opposition and dissent to function, but in emerging
democracies, freedom of speech is often restricted. This effect intensifies
during regional conflicts. This study examines how the India-Pakistan conflict
of May 2025 influenced Pakistani media coverage. Analyzing approximately 2,600
news articles from three major newspapers using a large language model (LLM),
the study found that war-related reporting significantly overshadowed coverage
of political opposition and dissent. These findings highlight how conflict can
marginalize democratic discourse, reinforcing the need to safeguard press
freedom in volatile regions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [251] [Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting](https://arxiv.org/abs/2509.20499)
*Boqi Li,Siyuan Li,Weiyi Wang,Anran Li,Zhong Cao,Henry X. Liu*

Main category: cs.RO

TL;DR: 本文提出零样本框架解决连续环境下视觉语言导航问题，实验表现达SOTA。


<details>
  <summary>Details</summary>
Motivation: 连续环境下视觉语言导航任务具有挑战性，需要联合处理语言指令、感知环境和规划动作。

Method: 提出零样本框架，结合简化有效的路点预测器和多模态大语言模型，预测器基于抽象障碍物地图生成可线性到达的路点并融入动态更新的拓扑图，将图和访问信息编码到提示中。

Result: 在R2R - CE和RxR - CE上分别取得41%和36%的成功率，超越先前SOTA方法。

Conclusion: 所提方法在连续环境视觉语言导航任务的零样本表现上达到了当前最优水平。

Abstract: With the rapid progress of foundation models and robotics, vision-language
navigation (VLN) has emerged as a key task for embodied agents with broad
practical applications. We address VLN in continuous environments, a
particularly challenging setting where an agent must jointly interpret natural
language instructions, perceive its surroundings, and plan low-level actions.
We propose a zero-shot framework that integrates a simplified yet effective
waypoint predictor with a multimodal large language model (MLLM). The predictor
operates on an abstract obstacle map, producing linearly reachable waypoints,
which are incorporated into a dynamically updated topological graph with
explicit visitation records. The graph and visitation information are encoded
into the prompt, enabling reasoning over both spatial structure and exploration
history to encourage exploration and equip MLLM with local path planning for
error correction. Extensive experiments on R2R-CE and RxR-CE show that our
method achieves state-of-the-art zero-shot performance, with success rates of
41% and 36%, respectively, outperforming prior state-of-the-art methods.

</details>


### [252] [GraspFactory: A Large Object-Centric Grasping Dataset](https://arxiv.org/abs/2509.20550)
*Srinidhi Kalgundi Srinivas,Yash Shukla,Adam Arnold,Sachin Chitta*

Main category: cs.RO

TL;DR: 本文介绍了用于训练数据密集型机器人抓取模型的数据集GraspFactory，并展示了基于其部分数据训练的模型在模拟和现实场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人抓取模型在有限数据集上训练，面对新物体时泛化能力不足，需要几何多样化的数据集来训练可泛化的模型。

Method: 创建包含超1.09亿个6自由度抓取数据的GraspFactory数据集，用于Franka Panda和Robotiq 2F - 85抓手。

Result: 基于GraspFactory部分数据训练的模型在模拟和现实世界中展现出泛化能力。

Conclusion: GraspFactory数据集有助于训练具有泛化能力的机器人抓取模型，数据集和工具可在线下载。

Abstract: Robotic grasping is a crucial task in industrial automation, where robots are
increasingly expected to handle a wide range of objects. However, a significant
challenge arises when robot grasping models trained on limited datasets
encounter novel objects. In real-world environments such as warehouses or
manufacturing plants, the diversity of objects can be vast, and grasping models
need to generalize to this diversity. Training large, generalizable
robot-grasping models requires geometrically diverse datasets. In this paper,
we introduce GraspFactory, a dataset containing over 109 million 6-DoF grasps
collectively for the Franka Panda (with 14,690 objects) and Robotiq 2F-85
grippers (with 33,710 objects). GraspFactory is designed for training
data-intensive models, and we demonstrate the generalization capabilities of
one such model trained on a subset of GraspFactory in both simulated and
real-world settings. The dataset and tools are made available for download at
https://graspfactory.github.io/.

</details>


### [253] [Learning Terrain-Specialized Policies for Adaptive Locomotion in Challenging Environments](https://arxiv.org/abs/2509.20635)
*Matheus P. Angarola,Francisco Affonso,Marcelo Becker*

Main category: cs.RO

TL;DR: 本文提出分层强化学习框架，利用地形专用策略和课程学习提升腿部机器人在复杂环境的敏捷性和跟踪性能，仿真验证该方法优于通用策略。


<details>
  <summary>Details</summary>
Motivation: 腿部机器人需在多样、非结构化地形实现稳健敏捷运动，盲态运动时获取地形信息困难，需要更好的方法。

Method: 引入分层强化学习框架，利用地形专用策略和课程学习。

Result: 在仿真中，该方法成功率比通用策略高16%，速度目标增加时跟踪误差更低，在低摩擦和不连续地形表现出色。

Conclusion: 该方法在混合地形场景中具有更好的适应性和鲁棒性。

Abstract: Legged robots must exhibit robust and agile locomotion across diverse,
unstructured terrains, a challenge exacerbated under blind locomotion settings
where terrain information is unavailable. This work introduces a hierarchical
reinforcement learning framework that leverages terrain-specialized policies
and curriculum learning to enhance agility and tracking performance in complex
environments. We validated our method on simulation, where our approach
outperforms a generalist policy by up to 16% in success rate and achieves lower
tracking errors as the velocity target increases, particularly on low-friction
and discontinuous terrains, demonstrating superior adaptability and robustness
across mixed-terrain scenarios.

</details>


### [254] [Efficient Construction of Implicit Surface Models From a Single Image for Motion Generation](https://arxiv.org/abs/2509.20681)
*Wei-Teng Chu,Tianyi Zhang,Matthew Johnson-Roberson,Weiming Zhi*

Main category: cs.RO

TL;DR: 提出轻量级框架FINS，可基于单张或少量图像重建高保真表面和SDF场，训练高效，性能优于基线方法，适用于机器人任务。


<details>
  <summary>Details</summary>
Motivation: 过往隐式表面重建方法需大量多视图图像输入且训练时间长，探索从单张图像构建隐式距离表示的问题。

Method: 提出FINS框架，集成多分辨率哈希网格编码器与轻量级几何和颜色头，通过近似二阶优化器训练，利用预训练基础模型估计图像几何信息。

Result: 在相同条件下，方法在收敛速度和表面重建、SDF场估计精度上优于现有基线方法，适用于机器人表面跟随任务，可扩展到多种基准数据集。

Conclusion: FINS能高效基于单张或少量图像进行表面和SDF场重建，具有良好性能和适用性。

Abstract: Implicit representations have been widely applied in robotics for obstacle
avoidance and path planning. In this paper, we explore the problem of
constructing an implicit distance representation from a single image. Past
methods for implicit surface reconstruction, such as \emph{NeuS} and its
variants generally require a large set of multi-view images as input, and
require long training times. In this work, we propose Fast Image-to-Neural
Surface (FINS), a lightweight framework that can reconstruct high-fidelity
surfaces and SDF fields based on a single or a small set of images. FINS
integrates a multi-resolution hash grid encoder with lightweight geometry and
color heads, making the training via an approximate second-order optimizer
highly efficient and capable of converging within a few seconds. Additionally,
we achieve the construction of a neural surface requiring only a single RGB
image, by leveraging pre-trained foundation models to estimate the geometry
inherent in the image. Our experiments demonstrate that under the same
conditions, our method outperforms state-of-the-art baselines in both
convergence speed and accuracy on surface reconstruction and SDF field
estimation. Moreover, we demonstrate the applicability of FINS for robot
surface following tasks and show its scalability to a variety of benchmark
datasets.

</details>


### [255] [Joint Flow Trajectory Optimization For Feasible Robot Motion Generation from Video Demonstrations](https://arxiv.org/abs/2509.20703)
*Xiaoxiang Dong,Matthew Johnson-Roberson,Weiming Zhi*

Main category: cs.RO

TL;DR: 提出JFTO框架解决基于视频的示教学习中机器人操作挑战，在模拟和现实实验中验证。


<details>
  <summary>Details</summary>
Motivation: 基于视频的人类示教学习对机器人操作有因具身差异和关节可行性约束带来的挑战。

Method: 提出JFTO框架，将示教视为以物体为中心的引导，平衡三个目标，扩展流匹配到SE(3)进行概率建模，将多种因素整合到统一可微目标。

Result: 在模拟和现实世界的多种操作任务实验中验证了方法。

Conclusion: 所提JFTO框架能有效解决基于视频的示教学习中机器人操作问题。

Abstract: Learning from human video demonstrations offers a scalable alternative to
teleoperation or kinesthetic teaching, but poses challenges for robot
manipulators due to embodiment differences and joint feasibility constraints.
We address this problem by proposing the Joint Flow Trajectory Optimization
(JFTO) framework for grasp pose generation and object trajectory imitation
under the video-based Learning-from-Demonstration (LfD) paradigm. Rather than
directly imitating human hand motions, our method treats demonstrations as
object-centric guides, balancing three objectives: (i) selecting a feasible
grasp pose, (ii) generating object trajectories consistent with demonstrated
motions, and (iii) ensuring collision-free execution within robot kinematics.
To capture the multimodal nature of demonstrations, we extend flow matching to
$\SE(3)$ for probabilistic modeling of object trajectories, enabling
density-aware imitation that avoids mode collapse. The resulting optimization
integrates grasp similarity, trajectory likelihood, and collision penalties
into a unified differentiable objective. We validate our approach in both
simulation and real-world experiments across diverse real-world manipulation
tasks.

</details>


### [256] [RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking](https://arxiv.org/abs/2509.20717)
*Zhenguo Sun,Yibo Peng,Yuan Meng,Xukun Li,Bo-Sheng Huang,Zhenshan Bing,Xinlong Wang,Alois Knoll*

Main category: cs.RO

TL;DR: 提出RobotDancing框架解决类人机器人长时高动态运动跟踪问题，能实现高质量运动跟踪并零样本部署到硬件。


<details>
  <summary>Details</summary>
Motivation: 类人机器人长时高动态运动跟踪因绝对关节指令无法补偿模型与实际的不匹配、导致误差积累而不稳定。

Method: 提出RobotDancing框架，采用端到端流程和单阶段强化学习设置，有统一的观察、奖励和超参数配置。

Result: 在Unitree G1上主要评估，验证在H1/H1 - 2上的迁移性，能跟踪多分钟高能量行为，零样本部署到硬件时运动跟踪质量高。

Conclusion: RobotDancing框架有效解决了类人机器人长时高动态运动跟踪问题。

Abstract: Long-horizon, high-dynamic motion tracking on humanoids remains brittle
because absolute joint commands cannot compensate model-plant mismatch, leading
to error accumulation. We propose RobotDancing, a simple, scalable framework
that predicts residual joint targets to explicitly correct dynamics
discrepancies. The pipeline is end-to-end--training, sim-to-sim validation, and
zero-shot sim-to-real--and uses a single-stage reinforcement learning (RL)
setup with a unified observation, reward, and hyperparameter configuration. We
evaluate primarily on Unitree G1 with retargeted LAFAN1 dance sequences and
validate transfer on H1/H1-2. RobotDancing can track multi-minute, high-energy
behaviors (jumps, spins, cartwheels) and deploys zero-shot to hardware with
high motion tracking quality.

</details>


### [257] [ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation](https://arxiv.org/abs/2509.20841)
*Dekun Lu,Wei Gao,Kui Jia*

Main category: cs.RO

TL;DR: 本文提出用于机器人操作的CoMOK公式，以实现通用、准确和可靠的端到端操作策略，实验证明了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端机器人操作神经网络在大规模实际部署中性能不足，需要实现通用、准确和可靠的端到端操作策略。

Method: 提出新颖的Chain of Moving Oriented Keypoints (CoMOK)公式作为神经策略的动作表示，并进行端到端训练。

Result: 动作表示具有通用性，能自然泛化到不同形状和大小的物体，实现亚厘米级精度，可处理多阶段任务、多模态机器人行为和可变形物体。

Conclusion: 大量模拟和硬件实验证明了方法的有效性。

Abstract: End-to-end robot manipulation policies offer significant potential for
enabling embodied agents to understand and interact with the world. Unlike
traditional modular pipelines, end-to-end learning mitigates key limitations
such as information loss between modules and feature misalignment caused by
isolated optimization targets. Despite these advantages, existing end-to-end
neural networks for robotic manipulation--including those based on large
VLM/VLA models--remain insufficiently performant for large-scale practical
deployment. In this paper, we take a step towards an end-to-end manipulation
policy that is generalizable, accurate and reliable. To achieve this goal, we
propose a novel Chain of Moving Oriented Keypoints (CoMOK) formulation for
robotic manipulation. Our formulation is used as the action representation of a
neural policy, which can be trained in an end-to-end fashion. Such an action
representation is general, as it extends the standard end-effector pose action
representation and supports a diverse set of manipulation tasks in a unified
manner. The oriented keypoint in our method enables natural generalization to
objects with different shapes and sizes, while achieving sub-centimeter
accuracy. Moreover, our formulation can easily handle multi-stage tasks,
multi-modal robot behaviors, and deformable objects. Extensive simulated and
hardware experiments demonstrate the effectiveness of our method.

</details>


### [258] [AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation](https://arxiv.org/abs/2509.21006)
*Konstantin Gubernatorov,Artem Voronov,Roman Voronov,Sergei Pasynkov,Stepan Perminov,Ziang Guo,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 提出AnywhereVLA框架用于在未知室内环境进行自然语言操控的移动操作，系统可在消费级硬件上运行，实验中任务成功率46%。


<details>
  <summary>Details</summary>
Motivation: 解决在未知、不可预测的室内环境中进行自然语言取放操作的问题。

Method: 将用户文本提示解析为结构化任务图，结合经典SLAM、度量语义映射和任务感知前沿探索策略；选择考虑可见性和可达性的预抓取基座姿态；微调紧凑型操作头将局部视觉上下文和子目标转化为抓取和放置建议。

Result: 在多房间实验室的静态场景和正常人体运动环境中，系统整体任务成功率达46%，并能在嵌入式计算上保持吞吐量。

Conclusion: 将经典技术栈与微调的VLA操作相结合，系统兼具基于几何导航的可靠性和语言条件操作的灵活性与任务泛化能力。

Abstract: We address natural language pick-and-place in unseen, unpredictable indoor
environments with AnywhereVLA, a modular framework for mobile manipulation. A
user text prompt serves as an entry point and is parsed into a structured task
graph that conditions classical SLAM with LiDAR and cameras, metric semantic
mapping, and a task-aware frontier exploration policy. An approach planner then
selects visibility and reachability aware pre grasp base poses. For
interaction, a compact SmolVLA manipulation head is fine tuned on platform pick
and place trajectories for the SO-101 by TheRobotStudio, grounding local visual
context and sub-goals into grasp and place proposals. The full system runs
fully onboard on consumer-level hardware, with Jetson Orin NX for perception
and VLA and an Intel NUC for SLAM, exploration, and control, sustaining
real-time operation. We evaluated AnywhereVLA in a multi-room lab under static
scenes and normal human motion. In this setting, the system achieves a $46\%$
overall task success rate while maintaining throughput on embedded compute. By
combining a classical stack with a fine-tuned VLA manipulation, the system
inherits the reliability of geometry-based navigation with the agility and task
generalization of language-conditioned manipulation.

</details>


### [259] [Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning](https://arxiv.org/abs/2509.20766)
*Gawon Lee,Daesol Cho,H. Jin Kim*

Main category: cs.RO

TL;DR: 提出MT - Lévy探索策略解决多任务强化学习应用于机器人时数据收集成本高的问题，提升样本效率。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习应用于机器人时，收集多样任务数据成本高，需要提升样本效率。

Method: 提出MT - Lévy策略，结合跨任务行为共享和受Lévy飞行启发的时间扩展探索，利用相关任务训练的策略引导探索，根据任务成功率动态调整探索水平。

Result: 实证结果表明MT - Lévy显著提升探索和样本效率，消融实验突出各组件贡献。

Conclusion: 结合行为共享和自适应探索策略可提升多任务强化学习在机器人应用中的实用性。

Abstract: Multi-task reinforcement learning (MTRL) offers a promising approach to
improve sample efficiency and generalization by training agents across multiple
tasks, enabling knowledge sharing between them. However, applying MTRL to
robotics remains challenging due to the high cost of collecting diverse task
data. To address this, we propose MT-L\'evy, a novel exploration strategy that
enhances sample efficiency in MTRL environments by combining behavior sharing
across tasks with temporally extended exploration inspired by L\'evy flight.
MT-L\'evy leverages policies trained on related tasks to guide exploration
towards key states, while dynamically adjusting exploration levels based on
task success ratios. This approach enables more efficient state-space coverage,
even in complex robotics environments. Empirical results demonstrate that
MT-L\'evy significantly improves exploration and sample efficiency, supported
by quantitative and qualitative analyses. Ablation studies further highlight
the contribution of each component, showing that combining behavior sharing
with adaptive exploration strategies can significantly improve the practicality
of MTRL in robotics applications.

</details>


### [260] [Cross-Modal Instructions for Robot Motion Generation](https://arxiv.org/abs/2509.21107)
*William Barron,Xiaoxiang Dong,Matthew Johnson-Roberson,Weiming Zhi*

Main category: cs.RO

TL;DR: 提出Learning from Cross - Modal Instructions范式和CrossInstruct框架，让机器人从跨模态指令学习行为，经评估有效。


<details>
  <summary>Details</summary>
Motivation: 传统机器人行为教学方法存在数据收集繁琐、演示数据集难扩展问题，需新范式。

Method: 引入CrossInstruct框架，将跨模态指令融入视觉语言模型上下文输入，合成运动轨迹，还有下游强化学习管道。

Result: 在基准模拟任务和真实硬件上评估，无需额外微调有效，为强化学习策略提供强初始化。

Conclusion: CrossInstruct框架有效，能让机器人产生可执行行为并泛化，强化学习管道可完成细粒度任务。

Abstract: Teaching robots novel behaviors typically requires motion demonstrations via
teleoperation or kinaesthetic teaching, that is, physically guiding the robot.
While recent work has explored using human sketches to specify desired
behaviors, data collection remains cumbersome, and demonstration datasets are
difficult to scale. In this paper, we introduce an alternative paradigm,
Learning from Cross-Modal Instructions, where robots are shaped by
demonstrations in the form of rough annotations, which can contain free-form
text labels, and are used in lieu of physical motion. We introduce the
CrossInstruct framework, which integrates cross-modal instructions as examples
into the context input to a foundational vision-language model (VLM). The VLM
then iteratively queries a smaller, fine-tuned model, and synthesizes the
desired motion over multiple 2D views. These are then subsequently fused into a
coherent distribution over 3D motion trajectories in the robot's workspace. By
incorporating the reasoning of the large VLM with a fine-grained pointing
model, CrossInstruct produces executable robot behaviors that generalize beyond
the environment of in the limited set of instruction examples. We then
introduce a downstream reinforcement learning pipeline that leverages
CrossInstruct outputs to efficiently learn policies to complete fine-grained
tasks. We rigorously evaluate CrossInstruct on benchmark simulation tasks and
real hardware, demonstrating effectiveness without additional fine-tuning and
providing a strong initialization for policies subsequently refined via
reinforcement learning.

</details>


### [261] [MPC-based Deep Reinforcement Learning Method for Space Robotic Control with Fuel Sloshing Mitigation](https://arxiv.org/abs/2509.21045)
*Mahya Ramezani,M. Amin Alandihallaj,Barış Can Yalçın,Miguel Angel Olivares Mendez,Holger Voos*

Main category: cs.RO

TL;DR: 提出集成强化学习与模型预测控制框架用于部分燃料箱卫星自主对接，实验验证SAC - MPC效果更佳，推动相关任务可行性。


<details>
  <summary>Details</summary>
Motivation: 传统对接控制因微重力下燃料晃动产生不可预测力影响稳定性，面临挑战。

Method: 将近端策略优化（PPO）和软演员 - 评论家（SAC）强化学习算法与模型预测控制（MPC）集成，利用MPC预测能力加速RL训练和提高控制鲁棒性。

Result: 通过实验和模拟验证，SAC - MPC在对接精度、成功率和控制成本上优于独立RL和PPO - MPC方法。

Conclusion: 该研究推进了燃料高效和抗干扰的卫星对接，提高了在轨加油和服务任务的可行性。

Abstract: This paper presents an integrated Reinforcement Learning (RL) and Model
Predictive Control (MPC) framework for autonomous satellite docking with a
partially filled fuel tank. Traditional docking control faces challenges due to
fuel sloshing in microgravity, which induces unpredictable forces affecting
stability. To address this, we integrate Proximal Policy Optimization (PPO) and
Soft Actor-Critic (SAC) RL algorithms with MPC, leveraging MPC's predictive
capabilities to accelerate RL training and improve control robustness. The
proposed approach is validated through Zero-G Lab of SnT experiments for planar
stabilization and high-fidelity numerical simulations for 6-DOF docking with
fuel sloshing dynamics. Simulation results demonstrate that SAC-MPC achieves
superior docking accuracy, higher success rates, and lower control effort,
outperforming standalone RL and PPO-MPC methods. This study advances
fuel-efficient and disturbance-resilient satellite docking, enhancing the
feasibility of on-orbit refueling and servicing missions.

</details>


### [262] [Human-like Navigation in a World Built for Humans](https://arxiv.org/abs/2509.21189)
*Bhargav Chandaka,Gloria X. Wang,Haozhe Chen,Henry Che,Albert J. Zhai,Shenlong Wang*

Main category: cs.RO

TL;DR: 提出ReasonNav模块化导航系统，集成类人导航技能，评估显示其能在大型复杂建筑高效导航。


<details>
  <summary>Details</summary>
Motivation: 现有机器人导航系统缺乏类人导航行为，在大型环境导航效率低。

Method: 设计基于导航地标紧凑输入输出抽象，利用视觉语言模型推理能力集成类人导航技能。

Result: 在真实和模拟导航任务评估中，智能体成功运用高阶推理在大型复杂建筑高效导航。

Conclusion: ReasonNav系统可有效集成类人导航技能，提升机器人在大型环境导航效率。

Abstract: When navigating in a man-made environment they haven't visited before--like
an office building--humans employ behaviors such as reading signs and asking
others for directions. These behaviors help humans reach their destinations
efficiently by reducing the need to search through large areas. Existing robot
navigation systems lack the ability to execute such behaviors and are thus
highly inefficient at navigating within large environments. We present
ReasonNav, a modular navigation system which integrates these human-like
navigation skills by leveraging the reasoning capabilities of a vision-language
model (VLM). We design compact input and output abstractions based on
navigation landmarks, allowing the VLM to focus on language understanding and
reasoning. We evaluate ReasonNav on real and simulated navigation tasks and
show that the agent successfully employs higher-order reasoning to navigate
efficiently in large, complex buildings.

</details>


### [263] [Taxonomy-aware Dynamic Motion Generation on Hyperbolic Manifolds](https://arxiv.org/abs/2509.21281)
*Luis Augenstein,Noémie Jaquier,Tamim Asfour,Leonel Rozo*

Main category: cs.RO

TL;DR: 本文提出GPHDM方法学习运动的层次结构和时间动态以生成类人运动，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 现有运动生成模型常忽略运动层次结构信息，导致生成运动与潜在层次结构脱节。

Method: 将高斯过程动力学模型（GPDM）的动力学先验扩展到双曲流形，并与分类学感知归纳偏差相结合，提出三种生成运动的新机制。

Result: 在手部抓握分类学上生成逼真运动序列的实验表明，GPHDM能忠实编码潜在分类学和时间动态，并生成新的物理一致轨迹。

Conclusion: 所提出的GPHDM方法能有效生成具有分类学结构和物理一致性的运动。

Abstract: Human-like motion generation for robots often draws inspiration from
biomechanical studies, which often categorize complex human motions into
hierarchical taxonomies. While these taxonomies provide rich structural
information about how movements relate to one another, this information is
frequently overlooked in motion generation models, leading to a disconnect
between the generated motions and their underlying hierarchical structure. This
paper introduces the \ac{gphdm}, a novel approach that learns latent
representations preserving both the hierarchical structure of motions and their
temporal dynamics to ensure physical consistency. Our model achieves this by
extending the dynamics prior of the Gaussian Process Dynamical Model (GPDM) to
the hyperbolic manifold and integrating it with taxonomy-aware inductive
biases. Building on this geometry- and taxonomy-aware frameworks, we propose
three novel mechanisms for generating motions that are both
taxonomically-structured and physically-consistent: two probabilistic recursive
approaches and a method based on pullback-metric geodesics. Experiments on
generating realistic motion sequences on the hand grasping taxonomy show that
the proposed GPHDM faithfully encodes the underlying taxonomy and temporal
dynamics, and generates novel physically-consistent trajectories.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [264] [On Theoretical Interpretations of Concept-Based In-Context Learning](https://arxiv.org/abs/2509.20882)
*Huaze Tang,Tianren Peng,Shao-lun Huang*

Main category: cs.IT

TL;DR: 本文研究基于概念的上下文学习（CB - ICL），提出理论分析，量化知识，给出相似度度量，探索相关因素影响并通过实验验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 当前对上下文学习（ICL）机制的理论理解有限，旨在研究特定的ICL方法CB - ICL以深入了解其机制。

Method: 对CB - ICL应用于ICL任务进行理论分析，基于理论探索提示示范大小和大语言模型嵌入维度的影响，进行真实数据实验。

Result: 提出的理论解释了CB - ICL在少量示范提示中预测查询标签表现良好的原因和时机，量化了大语言模型可利用的知识，给出相似度度量。

Conclusion: 通过实验验证了CB - ICL及其对应理论的实际实用性，为模型预训练和提示工程提供了重要见解和指导。

Abstract: In-Context Learning (ICL) has emerged as an important new paradigm in natural
language processing and large language model (LLM) applications. However, the
theoretical understanding of the ICL mechanism remains limited. This paper aims
to investigate this issue by studying a particular ICL approach, called
concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on
applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs
well for predicting query labels in prompts with only a few demonstrations. In
addition, the proposed theory quantifies the knowledge that can be leveraged by
the LLMs to the prompt tasks, and leads to a similarity measure between the
prompt demonstrations and the query input, which provides important insights
and guidance for model pre-training and prompt engineering in ICL. Moreover,
the impact of the prompt demonstration size and the dimension of the LLM
embeddings in ICL are also explored based on the proposed theory. Finally,
several real-data experiments are conducted to validate the practical
usefulness of CB-ICL and the corresponding theory.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [265] [Recidivism and Peer Influence with LLM Text Embeddings in Low Security Correctional Facilities](https://arxiv.org/abs/2509.20634)
*Shanjukta Nath,Jiwon Hong,Jae Ho Chang,Keith Warren,Subhadeep Paul*

Main category: econ.EM

TL;DR: 研究发现基于预训练大语言模型的AI嵌入能高预测再犯率，采用零样本分类处理高维向量，还估计了矫正设施内语言使用的同伴效应。


<details>
  <summary>Details</summary>
Motivation: 探讨基于预训练大语言模型的AI嵌入对再犯率的预测能力，以及了解矫正设施内的社会动态。

Method: 使用预训练大语言模型获取AI嵌入，进行零样本分类，用多元同伴效应模型估计同伴效应，开发适应稀疏网络、多元潜在变量和相关多元结果的新方法。

Result: 嵌入向量预测准确率比仅使用入狱前协变量高30%，发现语言使用在互动和反馈方面存在显著同伴效应。

Conclusion: 基于预训练大语言模型的AI嵌入对再犯率有高预测性，新方法能有效估计矫正设施内语言使用的同伴效应。

Abstract: We find AI embeddings obtained using a pre-trained transformer-based Large
Language Model (LLM) of 80,000-120,000 written affirmations and correction
exchanges among residents in low-security correctional facilities to be highly
predictive of recidivism. The prediction accuracy is 30\% higher with embedding
vectors than with only pre-entry covariates. However, since the text embedding
vectors are high-dimensional, we perform Zero-Shot classification of these
texts to a low-dimensional vector of user-defined classes to aid interpretation
while retaining the predictive power. To shed light on the social dynamics
inside the correctional facilities, we estimate peer effects in these
LLM-generated numerical representations of language with a multivariate peer
effect model, adjusting for network endogeneity. We develop new methodology and
theory for peer effect estimation that accommodate sparse networks,
multivariate latent variables, and correlated multivariate outcomes. With these
new methods, we find significant peer effects in language usage for interaction
and feedback.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [266] [An Analytical and AI-discovered Stable, Accurate, and Generalizable Subgrid-scale Closure for Geophysical Turbulence](https://arxiv.org/abs/2509.20365)
*Karan Jakhar,Yifei Guan,Pedram Hassanzadeh*

Main category: physics.ao-ph

TL;DR: 结合AI与流体物理，从小规模直接数值模拟数据中发现二维湍流封闭形式，大涡模拟准确稳定。


<details>
  <summary>Details</summary>
Motivation: 解决以往解析和基于AI工作中二阶展开导致大涡模拟不稳定的问题。

Method: 结合AI和流体物理，在稀疏方程发现中考虑尺度间能量转移与标准重建准则，用四阶截断泰勒展开推导。

Result: 得到的封闭形式使大涡模拟准确稳定，能重现直接数值模拟统计数据。

Conclusion: 考虑尺度间能量转移能得到更优的封闭形式，避免二阶展开导致的不稳定。

Abstract: By combining AI and fluid physics, we discover a closed-form closure for 2D
turbulence from small direct numerical simulation (DNS) data. Large-eddy
simulation (LES) with this closure is accurate and stable, reproducing DNS
statistics including those of extremes. We also show that the new closure could
be derived from a 4th-order truncated Taylor expansion. Prior analytical and
AI-based work only found the 2nd-order expansion, which led to unstable LES.
The additional terms emerge only when inter-scale energy transfer is considered
alongside standard reconstruction criterion in the sparse-equation discovery.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [267] [Fast-SEnSeI: Lightweight Sensor-Independent Cloud Masking for On-board Multispectral Sensors](https://arxiv.org/abs/2509.20991)
*Jan Kněžík,Jonáš Herec,Rado Pitoňák*

Main category: cs.CV

TL;DR: 提出轻量级、传感器无关的编码器模块Fast - SEnSeI用于星载云分割，在多数据集上表现准确。


<details>
  <summary>Details</summary>
Motivation: 多数云分割模型与特定传感器配置紧密耦合且依赖地面处理，需灵活的星载云分割方法。

Method: 基于SEnSeI - v2，集成改进的光谱描述符、轻量级架构和鲁棒的填充带处理，接受任意光谱带组合，输出固定大小特征图到改进U - Net分割模型，形成CPU - FPGA混合管道。

Result: 在Sentinel - 2和Landsat 8数据集上，不同输入配置下均实现准确分割。

Conclusion: Fast - SEnSeI能实现灵活的星载云分割，适用于航天级硬件。

Abstract: Cloud segmentation is a critical preprocessing step for many Earth
observation tasks, yet most models are tightly coupled to specific sensor
configurations and rely on ground-based processing. In this work, we propose
Fast-SEnSeI, a lightweight, sensor-independent encoder module that enables
flexible, on-board cloud segmentation across multispectral sensors with varying
band configurations. Building upon SEnSeI-v2, Fast-SEnSeI integrates an
improved spectral descriptor, lightweight architecture, and robust padding-band
handling. It accepts arbitrary combinations of spectral bands and their
wavelengths, producing fixed-size feature maps that feed into a compact,
quantized segmentation model based on a modified U-Net. The module runs
efficiently on embedded CPUs using Apache TVM, while the segmentation model is
deployed on FPGA, forming a CPU-FPGA hybrid pipeline suitable for
space-qualified hardware. Evaluations on Sentinel-2 and Landsat 8 datasets
demonstrate accurate segmentation across diverse input configurations.

</details>


### [268] [Human Semantic Representations of Social Interactions from Moving Shapes](https://arxiv.org/abs/2509.20673)
*Yiling Yun,Hongjing Lu*

Main category: cs.CV

TL;DR: 研究人类在识别简单移动形状展示的社交互动时使用的语义表征，发现语义模型能补充视觉特征信息，动词嵌入表现最佳。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注视觉特征，本文旨在研究人类用于补充视觉特征的语义表征。

Method: 研究1让参与者根据移动形状印象标注动画；研究2通过人类相似度判断测量27种社交互动的表征几何，并与基于视觉特征、标签和语义嵌入的模型预测比较。

Result: 语义模型能为视觉特征解释人类判断提供补充信息，动词嵌入对人类相似度判断解释最佳。

Conclusion: 简单展示中的社会感知反映了社交互动的语义结构，连接了视觉和抽象表征。

Abstract: Humans are social creatures who readily recognize various social interactions
from simple display of moving shapes. While previous research has often focused
on visual features, we examine what semantic representations that humans employ
to complement visual features. In Study 1, we directly asked human participants
to label the animations based on their impression of moving shapes. We found
that human responses were distributed. In Study 2, we measured the
representational geometry of 27 social interactions through human similarity
judgments and compared it with model predictions based on visual features,
labels, and semantic embeddings from animation descriptions. We found that
semantic models provided complementary information to visual features in
explaining human judgments. Among the semantic models, verb-based embeddings
extracted from descriptions account for human similarity judgments the best.
These results suggest that social perception in simple displays reflects the
semantic structure of social interactions, bridging visual and abstract
representations.

</details>


### [269] [Shared Neural Space: Unified Precomputed Feature Encoding for Multi-Task and Cross Domain Vision](https://arxiv.org/abs/2509.20481)
*Jing Li,Oskar Bartosz,Chengyu Wang,Michal Wnuczynski,Dilshan Godaliyadda,Michael Polley*

Main category: cs.CV

TL;DR: 提出通用神经空间NS解决成像和视觉任务效率问题，展示其多任务执行优势。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在处理一系列模块化任务时效率低，每个任务需映射到不同潜在域。

Method: 提出通用神经空间NS，用编解码器框架预计算跨视觉和成像任务的特征，编码器学习可泛化表示。

Result: 该架构减少冗余，提高跨域泛化能力，可高效执行多种成像和视觉任务，骨干网络轻量级且基于CNN。

Conclusion: 通用神经空间NS为高效多任务视觉流水线奠定基础，有广泛硬件适用性。

Abstract: The majority of AI models in imaging and vision are customized to perform on
specific high-precision task. However, this strategy is inefficient for
applications with a series of modular tasks, since each requires a mapping into
a disparate latent domain. To address this inefficiency, we proposed a
universal Neural Space (NS), where an encoder-decoder framework pre-computes
features across vision and imaging tasks. Our encoder learns transformation
aware, generalizable representations, which enable multiple downstream AI
modules to share the same feature space. This architecture reduces redundancy,
improves generalization across domain shift, and establishes a foundation for
effecient multi-task vision pipelines. Furthermore, as opposed to larger
transformer backbones, our backbone is lightweight and CNN-based, allowing for
wider across hardware. We furthur demonstrate that imaging and vision modules,
such as demosaicing, denoising, depth estimation and semantic segmentation can
be performed efficiently in the NS.

</details>


### [270] [InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On](https://arxiv.org/abs/2509.20524)
*Julien Han,Shuwen Qiu,Qi Li,Xingzi Xu,Mehmet Saygin Seyfioglu,Kavosh Asadi,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 提出InstructVTON交互式虚拟试穿系统，利用VLM和图像分割模型自动生成二进制掩码，简化用户体验，可与现有模型互操作实现先进成果。


<details>
  <summary>Details</summary>
Motivation: 解决基于掩码的虚拟试穿模型生成理想掩码困难、依赖背景知识和模型，部分场景无法实现等问题。

Method: 将虚拟试穿问题表述为图像引导或图像条件的修复任务，利用Vision Language Models (VLMs)和图像分割模型，根据用户提供的图像和自由文本样式指令自动生成二进制掩码。

Result: InstructVTON可与现有虚拟试穿模型互操作，实现具有样式控制的先进成果。

Conclusion: InstructVTON简化了最终用户体验，能处理基于掩码的虚拟试穿模型无法实现的试穿场景。

Abstract: We present InstructVTON, an instruction-following interactive virtual try-on
system that allows fine-grained and complex styling control of the resulting
generation, guided by natural language, on single or multiple garments. A
computationally efficient and scalable formulation of virtual try-on formulates
the problem as an image-guided or image-conditioned inpainting task. These
inpainting-based virtual try-on models commonly use a binary mask to control
the generation layout. Producing a mask that yields desirable result is
difficult, requires background knowledge, might be model dependent, and in some
cases impossible with the masking-based approach (e.g. trying on a long-sleeve
shirt with "sleeves rolled up" styling on a person wearing long-sleeve shirt
with sleeves down, where the mask will necessarily cover the entire sleeve).
InstructVTON leverages Vision Language Models (VLMs) and image segmentation
models for automated binary mask generation. These masks are generated based on
user-provided images and free-text style instructions. InstructVTON simplifies
the end-user experience by removing the necessity of a precisely drawn mask,
and by automating execution of multiple rounds of image generation for try-on
scenarios that cannot be achieved with masking-based virtual try-on models
alone. We show that InstructVTON is interoperable with existing virtual try-on
models to achieve state-of-the-art results with styling control.

</details>


### [271] [Beyond the Individual: Introducing Group Intention Forecasting with SHOT Dataset](https://arxiv.org/abs/2509.20715)
*Ruixu Zhang,Yuran Wang,Xinyi Hu,Chaoyu Mai,Wenxuan Liu,Danni Xu,Xian Zhong,Zheng Wang*

Main category: cs.CV

TL;DR: 本文引入群体意图概念和群体意图预测（GIF）任务，提出SHOT数据集和GIFT框架，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统意图识别忽视群体意图复杂性，为解决此局限开展研究。

Method: 提出SHOT数据集用于GIF研究，设计GIFT框架提取个体特征和建模群体动态来预测意图出现。

Result: 实验结果证实SHOT和GIFT的有效性。

Conclusion: 为群体意图预测的未来研究奠定坚实基础。

Abstract: Intention recognition has traditionally focused on individual intentions,
overlooking the complexities of collective intentions in group settings. To
address this limitation, we introduce the concept of group intention, which
represents shared goals emerging through the actions of multiple individuals,
and Group Intention Forecasting (GIF), a novel task that forecasts when group
intentions will occur by analyzing individual actions and interactions before
the collective goal becomes apparent. To investigate GIF in a specific
scenario, we propose SHOT, the first large-scale dataset for GIF, consisting of
1,979 basketball video clips captured from 5 camera views and annotated with 6
types of individual attributes. SHOT is designed with 3 key characteristics:
multi-individual information, multi-view adaptability, and multi-level
intention, making it well-suited for studying emerging group intentions.
Furthermore, we introduce GIFT (Group Intention ForecasTer), a framework that
extracts fine-grained individual features and models evolving group dynamics to
forecast intention emergence. Experimental results confirm the effectiveness of
SHOT and GIFT, establishing a strong foundation for future research in group
intention forecasting. The dataset is available at
https://xinyi-hu.github.io/SHOT_DATASET.

</details>


### [272] [AI-Enabled Crater-Based Navigation for Lunar Mapping](https://arxiv.org/abs/2509.20748)
*Sofia McLeod,Chee-Kheng Chng,Matthew Rodda,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 提出首个用于长时间月球测绘的端到端基于陨石坑导航（CBN）管道STELLA，并引入公共数据集CRESENT - 365进行测试，实验显示STELLA有较高精度。


<details>
  <summary>Details</summary>
Motivation: 以往CBN主要研究动力下降和着陆场景，而月球测绘任务有稀疏、倾斜图像及多变光照等挑战，需新的CBN方法。

Method: STELLA结合基于Mask R - CNN的陨石坑检测器、无描述符陨石坑识别模块、鲁棒透视n - 陨石坑位姿求解器和批量轨道确定后端；引入CRESENT - 365数据集。

Result: 在CRESENT+和CRESENT - 365上实验表明，STELLA在多种条件下平均保持米级位置精度和亚度级姿态精度。

Conclusion: 这是对CBN在真实月球测绘环境中的首次全面评估，为未来任务提供操作条件参考。

Abstract: Crater-Based Navigation (CBN) uses the ubiquitous impact craters of the Moon
observed on images as natural landmarks to determine the six degrees of freedom
pose of a spacecraft. To date, CBN has primarily been studied in the context of
powered descent and landing. These missions are typically short in duration,
with high-frequency imagery captured from a nadir viewpoint over well-lit
terrain. In contrast, lunar mapping missions involve sparse, oblique imagery
acquired under varying illumination conditions over potentially year-long
campaigns, posing significantly greater challenges for pose estimation. We
bridge this gap with STELLA - the first end-to-end CBN pipeline for
long-duration lunar mapping. STELLA combines a Mask R-CNN-based crater
detector, a descriptor-less crater identification module, a robust
perspective-n-crater pose solver, and a batch orbit determination back-end. To
rigorously test STELLA, we introduce CRESENT-365 - the first public dataset
that emulates a year-long lunar mapping mission. Each of its 15,283 images is
rendered from high-resolution digital elevation models with SPICE-derived Sun
angles and Moon motion, delivering realistic global coverage, illumination
cycles, and viewing geometries. Experiments on CRESENT+ and CRESENT-365 show
that STELLA maintains metre-level position accuracy and sub-degree attitude
accuracy on average across wide ranges of viewing angles, illumination
conditions, and lunar latitudes. These results constitute the first
comprehensive assessment of CBN in a true lunar mapping setting and inform
operational conditions that should be considered for future missions.

</details>


### [273] [Seeing Through Words, Speaking Through Pixels: Deep Representational Alignment Between Vision and Language Models](https://arxiv.org/abs/2509.20751)
*Zoe Wanying He,Sean Trott,Meenakshi Khosla*

Main category: cs.CV

TL;DR: 研究系统探究视觉和语言模型投影到部分对齐表征空间的相关问题，发现对齐在模型中间到后期层达到峰值，共享代码是语义的，模型能捕捉类似人类判断的语义区别，样本嵌入平均会增强对齐。


<details>
  <summary>Details</summary>
Motivation: 缺乏关于视觉和语言模型投影到部分对齐表征空间时，收敛出现位置、支持线索、是否捕捉人类偏好及样本聚合对对齐的影响等问题的清晰认知。

Method: 系统地研究相关问题，进行“Pick - a - Pic”强制选择任务。

Result: 对齐在模型中间到后期层达到峰值；共享代码是语义的；模型能在图像 - 文本匹配中反映人类偏好；样本嵌入平均会增强对齐。

Conclusion: 单模态网络收敛于与人类判断对齐且随样本聚合而增强的共享语义代码。

Abstract: Recent studies show that deep vision-only and language-only models--trained
on disjoint modalities--nonetheless project their inputs into a partially
aligned representational space. Yet we still lack a clear picture of where in
each network this convergence emerges, what visual or linguistic cues support
it, whether it captures human preferences in many-to-many image-text scenarios,
and how aggregating exemplars of the same concept affects alignment. Here, we
systematically investigate these questions. We find that alignment peaks in
mid-to-late layers of both model types, reflecting a shift from
modality-specific to conceptually shared representations. This alignment is
robust to appearance-only changes but collapses when semantics are altered
(e.g., object removal or word-order scrambling), highlighting that the shared
code is truly semantic. Moving beyond the one-to-one image-caption paradigm, a
forced-choice "Pick-a-Pic" task shows that human preferences for image-caption
matches are mirrored in the embedding spaces across all vision-language model
pairs. This pattern holds bidirectionally when multiple captions correspond to
a single image, demonstrating that models capture fine-grained semantic
distinctions akin to human judgments. Surprisingly, averaging embeddings across
exemplars amplifies alignment rather than blurring detail. Together, our
results demonstrate that unimodal networks converge on a shared semantic code
that aligns with human judgments and strengthens with exemplar aggregation.

</details>


### [274] [CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method for Photo Customization via ResInversion](https://arxiv.org/abs/2509.20775)
*Maoye Ren,Praneetha Vaddamanu,Jianjin Xu,Fernando De la Torre Frade*

Main category: cs.CV

TL;DR: 提出CustomEnhancer框架增强身份定制模型，引入ResInversion减少反演时间，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型合成真实人类照片存在场景退化、控制不足和感知身份不佳问题。

Method: 提出CustomEnhancer框架，利用人脸交换技术和预训练扩散模型获取额外表示；采用三重流融合的PerGeneration方法统一生成和重建过程；引入ResInversion通过预扩散机制进行噪声校正。

Result: CustomEnhancer在场景多样性、身份保真度、免训练控制方面达到SOTA，ResInversion比NTI更高效。

Conclusion: CustomEnhancer框架和ResInversion方法有效，代码待论文接受后公开。

Abstract: Recently remarkable progress has been made in synthesizing realistic human
photos using text-to-image diffusion models. However, current approaches face
degraded scenes, insufficient control, and suboptimal perceptual identity. We
introduce CustomEnhancer, a novel framework to augment existing identity
customization models. CustomEnhancer is a zero-shot enhancement pipeline that
leverages face swapping techniques, pretrained diffusion model, to obtain
additional representations in a zeroshot manner for encoding into personalized
models. Through our proposed triple-flow fused PerGeneration approach, which
identifies and combines two compatible counter-directional latent spaces to
manipulate a pivotal space of personalized model, we unify the generation and
reconstruction processes, realizing generation from three flows. Our pipeline
also enables comprehensive training-free control over the generation process of
personalized models, offering precise controlled personalization for them and
eliminating the need for controller retraining for per-model. Besides, to
address the high time complexity of null-text inversion (NTI), we introduce
ResInversion, a novel inversion method that performs noise rectification via a
pre-diffusion mechanism, reducing the inversion time by 129 times. Experiments
demonstrate that CustomEnhancer reach SOTA results at scene diversity, identity
fidelity, training-free controls, while also showing the efficiency of our
ResInversion over NTI. The code will be made publicly available upon paper
acceptance.

</details>


### [275] [DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation](https://arxiv.org/abs/2509.20792)
*Ved Umrajkar*

Main category: cs.CV

TL;DR: 提出DAC - LoRA框架将对抗训练集成到参数高效微调（PEFT）中，提升视觉语言模型对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在关键应用中重要，但参数高效微调方法使模型易受对抗攻击，CLIP作为众多下游VLMs骨干易受攻击影响多模态AI生态。

Method: 提出Dynamic Adversarial Curriculum DAC - LoRA框架，将对抗训练集成到PEFT，采用渐进式攻击的智能课程，受一阶平稳条件（FOSC）和TRADES启发的损失引导。

Result: DAC - LoRA在不显著降低干净准确率的情况下，大幅提高了对抗鲁棒性。

Conclusion: DAC - LoRA是有效、轻量级且广泛适用的方法，可轻松集成到标准PEFT管道以显著增强鲁棒性。

Abstract: Vision-Language Models (VLMs) are foundational to critical applications like
autonomous driving, medical diagnosis, and content moderation. While
Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA enable their efficient
adaptation to specialized tasks, these models remain vulnerable to adversarial
attacks that can compromise safety-critical decisions. CLIP, the backbone for
numerous downstream VLMs, is a high-value target whose vulnerabilities can
cascade across the multimodal AI ecosystem. We propose Dynamic Adversarial
Curriculum DAC-LoRA, a novel framework that integrates adversarial training
into PEFT. The core principle of our method i.e. an intelligent curriculum of
progressively challenging attack, is general and can potentially be applied to
any iterative attack method. Guided by the First-Order Stationary Condition
(FOSC) and a TRADES-inspired loss, DAC-LoRA achieves substantial improvements
in adversarial robustness without significantly compromising clean accuracy.
Our work presents an effective, lightweight, and broadly applicable method to
demonstrate that the DAC-LoRA framework can be easily integrated into a
standard PEFT pipeline to significantly enhance robustness.

</details>


### [276] [Revolutionizing Precise Low Back Pain Diagnosis via Contrastive Learning](https://arxiv.org/abs/2509.20813)
*Thanh Binh Le,Hoang Nhat Khang Vo,Tan-Ha Mai,Trong Nhan Phan*

Main category: cs.CV

TL;DR: 提出LumbarCLIP多模态框架用于腰椎疾病诊断，在下游分类任务中表现出色，线性投影头效果更好。


<details>
  <summary>Details</summary>
Motivation: 全球数百万受腰痛影响，需能联合分析医学图像和文本报告的诊断模型。

Method: 基于对比语言 - 图像预训练，集成多种视觉编码器和BERT文本编码器，通过可学习投影头将特征投影到共享嵌入空间，用软CLIP损失进行训练。

Result: 在下游分类任务中达到95.00%准确率和94.75% F1分数，消融研究表明线性投影头跨模态对齐更有效。

Conclusion: LumbarCLIP为肌肉骨骼自动诊断和临床决策支持提供了有前景的基础。

Abstract: Low back pain affects millions worldwide, driving the need for robust
diagnostic models that can jointly analyze complex medical images and
accompanying text reports. We present LumbarCLIP, a novel multimodal framework
that leverages contrastive language-image pretraining to align lumbar spine MRI
scans with corresponding radiological descriptions. Built upon a curated
dataset containing axial MRI views paired with expert-written reports,
LumbarCLIP integrates vision encoders (ResNet-50, Vision Transformer, Swin
Transformer) with a BERT-based text encoder to extract dense representations.
These are projected into a shared embedding space via learnable projection
heads, configurable as linear or non-linear, and normalized to facilitate
stable contrastive training using a soft CLIP loss. Our model achieves
state-of-the-art performance on downstream classification, reaching up to
95.00% accuracy and 94.75% F1-score on the test set, despite inherent class
imbalance. Extensive ablation studies demonstrate that linear projection heads
yield more effective cross-modal alignment than non-linear variants. LumbarCLIP
offers a promising foundation for automated musculoskeletal diagnosis and
clinical decision support.

</details>


### [277] [Copycats: the many lives of a publicly available medical imaging dataset](https://arxiv.org/abs/2402.06353)
*Amelia Jiménez-Sánchez,Natalia-Rozalia Avlona,Dovile Juodelyte,Théo Sourget,Caroline Vang-Larsen,Anna Rogers,Hubert Dariusz Zając,Veronika Cheplygina*

Main category: cs.CV

TL;DR: 本文分析CCP上公开的医学影像数据集，指出当前CCP治理模式存在问题，研究有助于医疗数据管理和AI算法。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据集对医疗AI很重要，但当前CCP治理模式未能保证数据集质量和遵循推荐做法。

Method: 分析CCP上公开的机器学习数据集，对比数据集在数据共享、文档记录和维护等维度的情况。

Result: 发现许可模糊、缺乏持久标识符和存储、存在重复数据和缺失元数据等问题，不同平台存在差异。

Conclusion: 研究对医疗领域负责任的数据管理和AI算法有贡献。

Abstract: Medical Imaging (MI) datasets are fundamental to artificial intelligence in
healthcare. The accuracy, robustness, and fairness of diagnostic algorithms
depend on the data (and its quality) used to train and evaluate the models. MI
datasets used to be proprietary, but have become increasingly available to the
public, including on community-contributed platforms (CCPs) like Kaggle or
HuggingFace. While open data is important to enhance the redistribution of
data's public value, we find that the current CCP governance model fails to
uphold the quality needed and recommended practices for sharing, documenting,
and evaluating datasets. In this paper, we conduct an analysis of publicly
available machine learning datasets on CCPs, discussing datasets' context, and
identifying limitations and gaps in the current CCP landscape. We highlight
differences between MI and computer vision datasets, particularly in the
potentially harmful downstream effects from poor adoption of recommended
dataset management practices. We compare the analyzed datasets across several
dimensions, including data sharing, data documentation, and maintenance. We
find vague licenses, lack of persistent identifiers and storage, duplicates,
and missing metadata, with differences between the platforms. Our research
contributes to efforts in responsible data curation and AI algorithms for
healthcare.

</details>


### [278] [TasselNetV4: A vision foundation model for cross-scene, cross-scale, and cross-species plant counting](https://arxiv.org/abs/2509.20857)
*Xiaonan Hu,Xuebing Li,Jinyu Xu,Abdulkadir Duran Adan,Letian Zhou,Xuhui Zhu,Yanan Li,Wei Guo,Shouyang Liu,Wenzhong Liu,Hao Lu*

Main category: cs.CV

TL;DR: 本文提出TasselNetV4模型用于跨物种植物计数，在两个数据集上实验显示其性能优越且高效，可作为跨场景、尺度和物种的视觉基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有植物计数方法需构建特定物种模型，难以覆盖所有物种，且植物动态变化，现有通用计数模型对植物计数效果不佳。

Method: 继承TasselNet模型，结合局部计数思想和提取匹配范式，基于视觉Transformer构建，引入多分支框感知局部计数器增强跨尺度鲁棒性。

Result: 在PAC - 105和PAC - Somalia数据集上实验，TasselNetV4比现有模型计数性能更优且效率高。

Conclusion: TasselNetV4可作为跨场景、尺度和物种的植物计数视觉基础模型。

Abstract: Accurate plant counting provides valuable information for agriculture such as
crop yield prediction, plant density assessment, and phenotype quantification.
Vision-based approaches are currently the mainstream solution. Prior art
typically uses a detection or a regression model to count a specific plant.
However, plants have biodiversity, and new cultivars are increasingly bred each
year. It is almost impossible to exhaust and build all species-dependent
counting models. Inspired by class-agnostic counting (CAC) in computer vision,
we argue that it is time to rethink the problem formulation of plant counting,
from what plants to count to how to count plants. In contrast to most daily
objects with spatial and temporal invariance, plants are dynamic, changing with
time and space. Their non-rigid structure often leads to worse performance than
counting rigid instances like heads and cars such that current CAC and
open-world detection models are suboptimal to count plants. In this work, we
inherit the vein of the TasselNet plant counting model and introduce a new
extension, TasselNetV4, shifting from species-specific counting to
cross-species counting. TasselNetV4 marries the local counting idea of
TasselNet with the extract-and-match paradigm in CAC. It builds upon a plain
vision transformer and incorporates novel multi-branch box-aware local counters
used to enhance cross-scale robustness. Two challenging datasets, PAC-105 and
PAC-Somalia, are harvested. Extensive experiments against state-of-the-art CAC
models show that TasselNetV4 achieves not only superior counting performance
but also high efficiency.Our results indicate that TasselNetV4 emerges to be a
vision foundation model for cross-scene, cross-scale, and cross-species plant
counting.

</details>


### [279] [Leveraging NTPs for Efficient Hallucination Detection in VLMs](https://arxiv.org/abs/2509.20379)
*Ofir Azachi,Kfir Eliyahu,Eyal El Ani,Rom Himelstein,Roi Reichart,Yuval Pinter,Nitay Calderon*

Main category: cs.CV

TL;DR: 本文提出基于视觉语言模型（VLM）下一个标记概率（NTP）的高效即席幻觉检测方法，实验表明该方法有效，结合多种方式可提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLM幻觉检测方法计算量大、增加模型延迟，需更高效方法。

Method: 基于VLM的NTP训练传统机器学习模型，引入含1400条人工标注语句的数据集进行测试，还结合语言NTP和VLM的幻觉预测分数。

Result: NTP特征可有效预测幻觉，简单机器学习模型性能与强VLM相当，结合语言NTP和VLM预测分数能提升检测性能。

Conclusion: 该研究为提升VLM可靠性提供简单轻量级解决方案。

Abstract: Hallucinations of vision-language models (VLMs), which are misalignments
between visual content and generated text, undermine the reliability of VLMs.
One common approach for detecting them employs the same VLM, or a different
one, to assess generated outputs. This process is computationally intensive and
increases model latency. In this paper, we explore an efficient on-the-fly
method for hallucination detection by training traditional ML models over
signals based on the VLM's next-token probabilities (NTPs). NTPs provide a
direct quantification of model uncertainty. We hypothesize that high
uncertainty (i.e., a low NTP value) is strongly associated with hallucinations.
To test this, we introduce a dataset of 1,400 human-annotated statements
derived from VLM-generated content, each labeled as hallucinated or not, and
use it to test our NTP-based lightweight method. Our results demonstrate that
NTP-based features are valuable predictors of hallucinations, enabling fast and
simple ML models to achieve performance comparable to that of strong VLMs.
Furthermore, augmenting these NTPs with linguistic NTPs, computed by feeding
only the generated text back into the VLM, enhances hallucination detection
performance. Finally, integrating hallucination prediction scores from VLMs
into the NTP-based models led to better performance than using either VLMs or
NTPs alone. We hope this study paves the way for simple, lightweight solutions
that enhance the reliability of VLMs.

</details>


### [280] [SCRA-VQA: Summarized Caption-Rerank for Augmented Large Language Models in Visual Question Answering](https://arxiv.org/abs/2509.20871)
*Yan Zhang,Jiaqing Lin,Miao Zhang,Kui Xiao,Xiaoju Hou,Yue Zhao,Zhifei Li*

Main category: cs.CV

TL;DR: 提出SCRA - VQA方法解决KB - VQA中知识获取问题，在OK - VQA和A - OKVQA数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的KB - VQA方法使用的图像描述包含过多与问题无关的噪声，且LLM不理解VQA任务，限制推理能力。

Method: 提出SCRA - VQA，用预训练视觉语言模型生成图像描述，生成上下文示例，同时总结和重排描述以排除无关信息。

Result: 基于67亿参数的LLM，在OK - VQA和A - OKVQA数据集上分别达到38.8%和34.6%的准确率。

Conclusion: SCRA - VQA能让LLM更好理解图像信息和问题，增强模型推理能力和任务适应性，无需昂贵的端到端训练。

Abstract: Acquiring high-quality knowledge is a central focus in Knowledge-Based Visual
Question Answering (KB-VQA). Recent methods use large language models (LLMs) as
knowledge engines for answering. These methods generally employ image captions
as visual text descriptions to assist LLMs in interpreting images. However, the
captions frequently include excessive noise irrelevant to the question, and
LLMs generally do not comprehend VQA tasks, limiting their reasoning
capabilities. To address this issue, we propose the Summarized Caption-Rerank
Augmented VQA (SCRA-VQA), which employs a pre-trained visual language model to
convert images into captions. Moreover, SCRA-VQA generates contextual examples
for the captions while simultaneously summarizing and reordering them to
exclude unrelated information. The caption-rerank process enables LLMs to
understand the image information and questions better, thus enhancing the
model's reasoning ability and task adaptability without expensive end-to-end
training. Based on an LLM with 6.7B parameters, SCRA-VQA performs excellently
on two challenging knowledge-based VQA datasets: OK-VQA and A-OKVQA, achieving
accuracies of 38.8% and 34.6%. Our code is available at
https://github.com/HubuKG/SCRA-VQA.

</details>


### [281] [Integrating Object Interaction Self-Attention and GAN-Based Debiasing for Visual Question Answering](https://arxiv.org/abs/2509.20884)
*Zhifei Li,Feng Qiu,Yiran Wang,Yujing Xia,Kui Xiao,Miao Zhang,Yan Zhang*

Main category: cs.CV

TL;DR: 提出IOG - VQA模型，结合对象交互自注意力和基于GAN的去偏技术提升VQA性能，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有VQA模型受训练数据偏差影响，过度依赖表面模式，泛化能力不足。

Method: 提出IOG - VQA模型，集成对象交互自注意力机制和基于GAN的去偏框架。

Result: 在VQA - CP v1和VQA - CP v2数据集上实验，相比现有方法表现出色，尤其处理有偏差和不平衡的数据分布。

Conclusion: 处理对象交互和数据集偏差对推进VQA任务很重要。

Abstract: Visual Question Answering (VQA) presents a unique challenge by requiring
models to understand and reason about visual content to answer questions
accurately. Existing VQA models often struggle with biases introduced by the
training data, leading to over-reliance on superficial patterns and inadequate
generalization to diverse questions and images. This paper presents a novel
model, IOG-VQA, which integrates Object Interaction Self-Attention and
GAN-Based Debiasing to enhance VQA model performance. The self-attention
mechanism allows our model to capture complex interactions between objects
within an image, providing a more comprehensive understanding of the visual
context. Meanwhile, the GAN-based debiasing framework generates unbiased data
distributions, helping the model to learn more robust and generalizable
features. By leveraging these two components, IOG-VQA effectively combines
visual and textual information to address the inherent biases in VQA datasets.
Extensive experiments on the VQA-CP v1 and VQA-CP v2 datasets demonstrate that
our model shows excellent performance compared with the existing methods,
particularly in handling biased and imbalanced data distributions highlighting
the importance of addressing both object interactions and dataset biases in
advancing VQA tasks. Our code is available at
https://github.com/HubuKG/IOG-VQA.

</details>


### [282] [FerretNet: Efficient Synthetic Image Detection via Local Pixel Dependencies](https://arxiv.org/abs/2509.20890)
*Shuqiao Liang,Jian Liu,Renzhang Chen,Quanlong Guan*

Main category: cs.CV

TL;DR: 本文针对合成图像检测挑战，基于局部像素依赖特性提出轻量级网络FerretNet，实验显示其在多生成模型上准确率高，超现有方法。


<details>
  <summary>Details</summary>
Motivation: 先进模型生成的合成图像越来越逼真，给合成图像检测带来巨大挑战。

Method: 探索生成过程中两种伪影类型，利用局部像素依赖属性重建合成图像，提出轻量级神经网络FerretNet。

Result: 仅在4类ProGAN数据集上训练的FerretNet，在包含22种生成模型的开放世界基准测试中平均准确率达97.1%，超现有方法10.6%。

Conclusion: FerretNet能高效且稳健地进行合成图像检测。

Abstract: The increasing realism of synthetic images generated by advanced models such
as VAEs, GANs, and LDMs poses significant challenges for synthetic image
detection. To address this issue, we explore two artifact types introduced
during the generation process: (1) latent distribution deviations and (2)
decoding-induced smoothing effects, which manifest as inconsistencies in local
textures, edges, and color transitions. Leveraging local pixel dependencies
(LPD) properties rooted in Markov Random Fields, we reconstruct synthetic
images using neighboring pixel information to expose disruptions in texture
continuity and edge coherence. Building upon LPD, we propose FerretNet, a
lightweight neural network with only 1.1M parameters that delivers efficient
and robust synthetic image detection. Extensive experiments demonstrate that
FerretNet, trained exclusively on the 4-class ProGAN dataset, achieves an
average accuracy of 97.1% on an open-world benchmark comprising across 22
generative models, surpassing state-of-the-art methods by 10.6%.

</details>


### [283] [Unlocking Financial Insights: An advanced Multimodal Summarization with Multimodal Output Framework for Financial Advisory Videos](https://arxiv.org/abs/2509.20961)
*Sarmistha Das,R E Zera Marveen Lyngkhoi,Sriparna Saha,Alka Maurya*

Main category: cs.CV

TL;DR: 提出FASTER框架处理金融咨询播客视频摘要难题，引入Fin - APT数据集，实验显示其性能强，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 社交媒体动态传播使金融咨询内容广泛传播，但从长时多模态片段提取见解困难，需有效方法。

Method: 采用BLIP进行语义视觉描述、OCR识别文本模式、基于Whisper转录及说话人分割作为BOS特征；用改进的基于DPO的损失函数；采用基于排序器的检索机制；引入Fin - APT数据集。

Result: 综合跨领域实验表明，与大语言模型和视觉 - 语言模型相比，FASTER性能强、鲁棒性好、泛化性佳。

Conclusion: FASTER为多模态摘要树立新标准，让金融咨询内容更易获取和应用，开辟新研究途径。

Abstract: The dynamic propagation of social media has broadened the reach of financial
advisory content through podcast videos, yet extracting insights from lengthy,
multimodal segments (30-40 minutes) remains challenging. We introduce FASTER
(Financial Advisory Summariser with Textual Embedded Relevant images), a
modular framework that tackles three key challenges: (1) extracting
modality-specific features, (2) producing optimized, concise summaries, and (3)
aligning visual keyframes with associated textual points. FASTER employs BLIP
for semantic visual descriptions, OCR for textual patterns, and Whisper-based
transcription with Speaker diarization as BOS features. A modified Direct
Preference Optimization (DPO)-based loss function, equipped with BOS-specific
fact-checking, ensures precision, relevance, and factual consistency against
the human-aligned summary. A ranker-based retrieval mechanism further aligns
keyframes with summarized content, enhancing interpretability and cross-modal
coherence. To acknowledge data resource scarcity, we introduce Fin-APT, a
dataset comprising 470 publicly accessible financial advisory pep-talk videos
for robust multimodal research. Comprehensive cross-domain experiments confirm
FASTER's strong performance, robustness, and generalizability when compared to
Large Language Models (LLMs) and Vision-Language Models (VLMs). By establishing
a new standard for multimodal summarization, FASTER makes financial advisory
content more accessible and actionable, thereby opening new avenues for
research. The dataset and code are available at:
https://github.com/sarmistha-D/FASTER

</details>


### [284] [Innovative Deep Learning Architecture for Enhanced Altered Fingerprint Recognition](https://arxiv.org/abs/2509.20537)
*Dana A Abdullah,Dana Rasul Hamad,Bishar Rasheed Ibrahim,Sirwan Abdulwahid Aula,Aso Khaleel Ameen,Sabat Salih Hamadamin*

Main category: cs.CV

TL;DR: 提出DeepAFRNet模型识别变形指纹，在SOCOFing Real - Altered子集评估，展示准确率及阈值敏感性，表明适用于实际部署。


<details>
  <summary>Details</summary>
Motivation: 对手会故意修改指纹脊线图案以逃避检测，需要对变形指纹进行鲁棒识别以用于边境控制、法医鉴定等应用。

Method: 使用VGG16骨干网络提取高维特征，通过余弦相似度比较嵌入向量。

Result: 在SOCOFing Real - Altered子集三个难度级别上，严格阈值下准确率分别达96.7%、98.76%和99.54%；阈值从0.92放宽到0.72，准确率大幅下降。

Conclusion: DeepAFRNet解决了先前基于合成修改或有限验证协议的工作的局限性，适用于对安全和识别恢复力要求高的实际部署。

Abstract: Altered fingerprint recognition (AFR) is challenging for biometric
verification in applications such as border control, forensics, and fiscal
admission. Adversaries can deliberately modify ridge patterns to evade
detection, so robust recognition of altered prints is essential. We present
DeepAFRNet, a deep learning recognition model that matches and recognizes
distorted fingerprint samples. The approach uses a VGG16 backbone to extract
high-dimensional features and cosine similarity to compare embeddings. We
evaluate on the SOCOFing Real-Altered subset with three difficulty levels
(Easy, Medium, Hard). With strict thresholds, DeepAFRNet achieves accuracies of
96.7 percent, 98.76 percent, and 99.54 percent for the three levels. A
threshold-sensitivity study shows that relaxing the threshold from 0.92 to 0.72
sharply degrades accuracy to 7.86 percent, 27.05 percent, and 29.51 percent,
underscoring the importance of threshold selection in biometric systems. By
using real altered samples and reporting per-level metrics, DeepAFRNet
addresses limitations of prior work based on synthetic alterations or limited
verification protocols, and indicates readiness for real-world deployments
where both security and recognition resilience are critical.

</details>


### [285] [Large Pre-Trained Models for Bimanual Manipulation in 3D](https://arxiv.org/abs/2509.20579)
*Hanna Yurchyk,Wei-Di Chang,Gregory Dudek,David Meger*

Main category: cs.CV

TL;DR: 研究将预训练视觉Transformer注意力图集成到体素表示以增强双手机器人操作，取得一定提升。


<details>
  <summary>Details</summary>
Motivation: 提升双手机器人操作能力。

Method: 从DINOv2提取注意力图，将其解释为像素级显著度分数，提升到3D体素网格，融入行为克隆策略。

Result: 集成到基于体素的策略后，在RLBench双手基准测试所有任务中平均绝对提升8.2%，相对增益21.9%。

Conclusion: 注意力引导的特征化方法能有效提升双手机器人操作性能。

Abstract: We investigate the integration of attention maps from a pre-trained Vision
Transformer into voxel representations to enhance bimanual robotic
manipulation. Specifically, we extract attention maps from DINOv2, a
self-supervised ViT model, and interpret them as pixel-level saliency scores
over RGB images. These maps are lifted into a 3D voxel grid, resulting in
voxel-level semantic cues that are incorporated into a behavior cloning policy.
When integrated into a state-of-the-art voxel-based policy, our
attention-guided featurization yields an average absolute improvement of 8.2%
and a relative gain of 21.9% across all tasks in the RLBench bimanual
benchmark.

</details>


### [286] [Region-of-Interest Augmentation for Mammography Classification under Patient-Level Cross-Validation](https://arxiv.org/abs/2509.20585)
*Farbod Bigdeli,Mohsen Mohammadagha,Ali Bigdeli*

Main category: cs.CV

TL;DR: 本文针对Mini - DDSM数据集，提出轻量级感兴趣区域（ROI）增强策略用于乳腺癌筛查，评估显示能在受限环境下提升乳腺钼靶分类性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习用于乳腺钼靶解读受限于低分辨率数据集和小样本量，需提升性能。

Method: 重新审视Mini - DDSM数据集，引入轻量级ROI增强策略，训练时用预计算的无标签边界框库中随机裁剪的ROI替换全图像，并可添加抖动增加多样性。

Result: 在Mini - DDSM上，ROI增强带来适度平均ROC - AUC增益，各折性能有差异，PR - AUC持平或略低，推理时间成本不变。

Conclusion: 简单的数据中心ROI策略可在受限环境下增强乳腺钼靶分类，无需额外标签或架构修改。

Abstract: Breast cancer screening with mammography remains central to early detection
and mortality reduction. Deep learning has shown strong potential for
automating mammogram interpretation, yet limited-resolution datasets and small
sample sizes continue to restrict performance. We revisit the Mini-DDSM dataset
(9,684 images; 2,414 patients) and introduce a lightweight region-of-interest
(ROI) augmentation strategy. During training, full images are probabilistically
replaced with random ROI crops sampled from a precomputed, label-free
bounding-box bank, with optional jitter to increase variability. We evaluate
under strict patient-level cross-validation and report ROC-AUC, PR-AUC, and
training-time efficiency metrics (throughput and GPU memory). Because ROI
augmentation is training-only, inference-time cost remains unchanged. On
Mini-DDSM, ROI augmentation (best: p_roi = 0.10, alpha = 0.10) yields modest
average ROC-AUC gains, with performance varying across folds; PR-AUC is flat to
slightly lower. These results demonstrate that simple, data-centric ROI
strategies can enhance mammography classification in constrained settings
without requiring additional labels or architectural modifications.

</details>


### [287] [SiNGER: A Clearer Voice Distills Vision Transformers Further](https://arxiv.org/abs/2509.20986)
*Geunhyeok Yu,Sunjae Jeong,Yoonyoung Choi,Jaeseung Kim,Hyoseok Hwang*

Main category: cs.CV

TL;DR: 提出SiNGER蒸馏框架解决视觉Transformer知识蒸馏中高范数伪影问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer存在高范数伪影，知识蒸馏时学生模型易过拟合伪影，先前方法存在抑制伪影和保留信息的权衡问题。

Method: 引入SiNGER框架，利用零空间引导的扰动进行教师特征细化，通过基于LoRA的适配器高效实现扰动，再将细化特征蒸馏给学生模型。

Result: 在多个下游任务中达到了最先进的性能，产生更清晰、更具可解释性的表示。

Conclusion: SiNGER框架能有效抑制伪影并保留信息，持续提升学生模型性能。

Abstract: Vision Transformers are widely adopted as the backbone of vision foundation
models, but they are known to produce high-norm artifacts that degrade
representation quality. When knowledge distillation transfers these features to
students, high-norm artifacts dominate the objective, so students overfit to
artifacts and underweight informative signals, diminishing the gains from
larger models. Prior work attempted to remove artifacts but encountered an
inherent trade-off between artifact suppression and preserving informative
signals from teachers. To address this, we introduce Singular Nullspace-Guided
Energy Reallocation (SiNGER), a novel distillation framework that suppresses
artifacts while preserving informative signals. The key idea is principled
teacher feature refinement: during refinement, we leverage the nullspace-guided
perturbation to preserve information while suppressing artifacts. Then, the
refined teacher's features are distilled to a student. We implement this
perturbation efficiently with a LoRA-based adapter that requires minimal
structural modification. Extensive experiments show that \oursname consistently
improves student models, achieving state-of-the-art performance in multiple
downstream tasks and producing clearer and more interpretable representations.

</details>


### [288] [EnGraf-Net: Multiple Granularity Branch Network with Fine-Coarse Graft Grained for Classification Task](https://arxiv.org/abs/2509.21061)
*Riccardo La Grassa,Ignazio Gallo,Nicola Landro*

Main category: cs.CV

TL;DR: 提出端到端深度神经网络模型EnGraf - Net，利用语义关联作为监督信号，在三个数据集上表现优于许多现有细粒度模型，无需裁剪技术或手动标注。


<details>
  <summary>Details</summary>
Motivation: 现有基于部分的细粒度分类方法存在局部特征表示不完整问题，而人类识别物体时会形成语义关联，因此想利用语义关联改进细粒度分类。

Method: 构建名为EnGraf - Net的端到端深度神经网络模型，将结构化的语义关联（分类法）作为监督信号。

Result: 在CIFAR - 100、CUB - 200 - 2011和FGVC - Aircraft三个知名数据集上实验，EnGraf - Net优于许多现有细粒度模型，与最新的最先进方法表现相当。

Conclusion: EnGraf - Net在细粒度分类中表现良好，无需裁剪技术和手动标注，利用语义关联是有效的方法。

Abstract: Fine-grained classification models are designed to focus on the relevant
details necessary to distinguish highly similar classes, particularly when
intra-class variance is high and inter-class variance is low. Most existing
models rely on part annotations such as bounding boxes, part locations, or
textual attributes to enhance classification performance, while others employ
sophisticated techniques to automatically extract attention maps. We posit that
part-based approaches, including automatic cropping methods, suffer from an
incomplete representation of local features, which are fundamental for
distinguishing similar objects. While fine-grained classification aims to
recognize the leaves of a hierarchical structure, humans recognize objects by
also forming semantic associations. In this paper, we leverage semantic
associations structured as a hierarchy (taxonomy) as supervised signals within
an end-to-end deep neural network model, termed EnGraf-Net. Extensive
experiments on three well-known datasets CIFAR-100, CUB-200-2011, and
FGVC-Aircraft demonstrate the superiority of EnGraf-Net over many existing
fine-grained models, showing competitive performance with the most recent
state-of-the-art approaches, without requiring cropping techniques or manual
annotations.

</details>


### [289] [Nuclear Diffusion Models for Low-Rank Background Suppression in Videos](https://arxiv.org/abs/2509.20886)
*Tristan S. W. Stevens,Oisín Nolan,Jean-Luc Robert,Ruud J. G. van Sloun*

Main category: cs.CV

TL;DR: 针对视频结构化噪声和背景伪影问题，提出核扩散混合框架，在心脏超声去雾中表现优于传统方法，显示结合模型与生成先验用于视频恢复的潜力。


<details>
  <summary>Details</summary>
Motivation: 视频序列存在结构化噪声和背景伪影影响分析和恢复，传统鲁棒主成分方法的稀疏性假设无法捕捉真实视频数据的丰富变异性。

Method: 提出集成低秩时间建模与扩散后验采样的混合框架，即核扩散方法。

Result: 在心脏超声去雾问题中，相比传统鲁棒主成分分析，在对比度增强和信号保存方面有更好的去雾性能。

Conclusion: 结合基于模型的时间模型和深度生成先验在高保真视频恢复中有潜力。

Abstract: Video sequences often contain structured noise and background artifacts that
obscure dynamic content, posing challenges for accurate analysis and
restoration. Robust principal component methods address this by decomposing
data into low-rank and sparse components. Still, the sparsity assumption often
fails to capture the rich variability present in real video data. To overcome
this limitation, a hybrid framework that integrates low-rank temporal modeling
with diffusion posterior sampling is proposed. The proposed method, Nuclear
Diffusion, is evaluated on a real-world medical imaging problem, namely cardiac
ultrasound dehazing, and demonstrates improved dehazing performance compared to
traditional RPCA concerning contrast enhancement (gCNR) and signal preservation
(KS statistic). These results highlight the potential of combining model-based
temporal models with deep generative priors for high-fidelity video
restoration.

</details>


### [290] [Vision Transformers: the threat of realistic adversarial patches](https://arxiv.org/abs/2509.21084)
*Kasper Cools,Clara Maathuis,Alexander M. van Oers,Claudia S. Hübner,Nikos Deligiannis,Marijke Vandewal,Geert De Cubber*

Main category: cs.CV

TL;DR: 研究ViT模型在逃避攻击下的漏洞，用CT技术设计对抗补丁，实验验证对抗补丁从CNN到ViT的跨架构可迁移性。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统安全受关注，ViT虽有优势但仍易受逃避攻击，尤其是对抗补丁攻击，需研究其漏洞。

Method: 用Creases Transformation (CT)技术设计现实对抗补丁，研究CNN对抗攻击技术用于ViT分类模型的可迁移性。

Result: 在四人脸分类任务中对四个微调ViT模型实验，攻击成功率从40.04%到99.97%不等。

Conclusion: 确认对抗补丁从CNN到ViT的跨架构可迁移性，预训练数据集规模和方法对模型对抗攻击的恢复能力有很大影响。

Abstract: The increasing reliance on machine learning systems has made their security a
critical concern. Evasion attacks enable adversaries to manipulate the
decision-making processes of AI systems, potentially causing security breaches
or misclassification of targets. Vision Transformers (ViTs) have gained
significant traction in modern machine learning due to increased 1) performance
compared to Convolutional Neural Networks (CNNs) and 2) robustness against
adversarial perturbations. However, ViTs remain vulnerable to evasion attacks,
particularly to adversarial patches, unique patterns designed to manipulate AI
classification systems. These vulnerabilities are investigated by designing
realistic adversarial patches to cause misclassification in person vs.
non-person classification tasks using the Creases Transformation (CT)
technique, which adds subtle geometric distortions similar to those occurring
naturally when wearing clothing. This study investigates the transferability of
adversarial attack techniques used in CNNs when applied to ViT classification
models. Experimental evaluation across four fine-tuned ViT models on a binary
person classification task reveals significant vulnerability variations: attack
success rates ranged from 40.04% (google/vit-base-patch16-224-in21k) to 99.97%
(facebook/dino-vitb16), with google/vit-base-patch16-224 achieving 66.40% and
facebook/dinov3-vitb16 reaching 65.17%. These results confirm the
cross-architectural transferability of adversarial patches from CNNs to ViTs,
with pre-training dataset scale and methodology strongly influencing model
resilience to adversarial attacks.

</details>


### [291] [Unlocking Noise-Resistant Vision: Key Architectural Secrets for Robust Models](https://arxiv.org/abs/2509.20939)
*Bum Jun Kim,Makoto Kawano,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CV

TL;DR: 研究视觉架构对高斯噪声鲁棒性的依赖，通过实验确定四个设计模式，进行理论分析并给出设计指南。


<details>
  <summary>Details</summary>
Motivation: 剖析视觉模型对特定架构设计选择的依赖，探究某些架构对高斯噪声更具鲁棒性的原因。

Method: 对1174个预训练视觉模型进行评估，确定设计模式；开展理论分析，将观察到的相关性转化为因果机制。

Result: 确定四个设计模式，带来排名和准确率提升；理论分析解释了这些发现。

Conclusion: 将鲁棒性分解为可解释模块，提供理论解释和实用设计指南。

Abstract: While the robustness of vision models is often measured, their dependence on
specific architectural design choices is rarely dissected. We investigate why
certain vision architectures are inherently more robust to additive Gaussian
noise and convert these empirical insights into simple, actionable design
rules. Specifically, we performed extensive evaluations on 1,174 pretrained
vision models, empirically identifying four consistent design patterns for
improved robustness against Gaussian noise: larger stem kernels, smaller input
resolutions, average pooling, and supervised vision transformers (ViTs) rather
than CLIP ViTs, which yield up to 506 rank improvements and 21.6\%p accuracy
gains. We then develop a theoretical analysis that explains these findings,
converting observed correlations into causal mechanisms. First, we prove that
low-pass stem kernels attenuate noise with a gain that decreases quadratically
with kernel size and that anti-aliased downsampling reduces noise energy
roughly in proportion to the square of the downsampling factor. Second, we
demonstrate that average pooling is unbiased and suppresses noise in proportion
to the pooling window area, whereas max pooling incurs a positive bias that
grows slowly with window size and yields a relatively higher mean-squared error
and greater worst-case sensitivity. Third, we reveal and explain the
vulnerability of CLIP ViTs via a pixel-space Lipschitz bound: The smaller
normalization standard deviations used in CLIP preprocessing amplify worst-case
sensitivity by up to 1.91 times relative to the Inception-style preprocessing
common in supervised ViTs. Our results collectively disentangle robustness into
interpretable modules, provide a theory that explains the observed trends, and
build practical, plug-and-play guidelines for designing vision models more
robust against Gaussian noise.

</details>


### [292] [An Adaptor for Triggering Semi-Supervised Learning to Out-of-Box Serve Deep Image Clustering](https://arxiv.org/abs/2509.20976)
*Yue Duan,Lei Qi,Yinghuan Shi,Yang Gao*

Main category: cs.CV

TL;DR: 提出ASD适配器，可无前提条件冷启动自监督学习用于深度图像聚类，性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有将自监督学习技术集成到深度聚类框架的工作需预训练等前提条件，限制了自监督学习在图像聚类任务中的灵活应用。

Method: 随机采样伪标签数据，设置实例级分类器学习，跟踪预测的类转换以提取实例级类的高级相似性，为伪标签数据分配聚类级标签，用分配好标签的伪标签数据触发自监督学习进行图像聚类。

Result: ASD在各种基准测试中表现优于最新的深度图像聚类方法，与使用真实标签的自监督方法相比精度差距很小，如在CIFAR - 10上仅差1.33%，还能提升现有集成自监督学习的深度图像聚类方法的性能。

Conclusion: ASD可无前提条件冷启动自监督学习用于图像聚类，性能出色且能提升现有相关方法性能。

Abstract: Recently, some works integrate SSL techniques into deep clustering frameworks
to enhance image clustering performance. However, they all need pretraining,
clustering learning, or a trained clustering model as prerequisites, limiting
the flexible and out-of-box application of SSL learners in the image clustering
task. This work introduces ASD, an adaptor that enables the cold-start of SSL
learners for deep image clustering without any prerequisites. Specifically, we
first randomly sample pseudo-labeled data from all unlabeled data, and set an
instance-level classifier to learn them with semantically aligned
instance-level labels. With the ability of instance-level classification, we
track the class transitions of predictions on unlabeled data to extract
high-level similarities of instance-level classes, which can be utilized to
assign cluster-level labels to pseudo-labeled data. Finally, we use the
pseudo-labeled data with assigned cluster-level labels to trigger a general SSL
learner trained on the unlabeled data for image clustering. We show the
superior performance of ASD across various benchmarks against the latest deep
image clustering approaches and very slight accuracy gaps compared to SSL
methods using ground-truth, e.g., only 1.33% on CIFAR-10. Moreover, ASD can
also further boost the performance of existing SSL-embedded deep image
clustering methods.

</details>


### [293] [WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP](https://arxiv.org/abs/2509.21153)
*Moshe Kimhi,Erez Koifman,Ehud Rivlin,Eli Schwartz,Chaim Baskin*

Main category: cs.CV

TL;DR: 介绍WAVECLIP，一种基于小波标记化实现CLIP自适应分辨率推理的统一模型，能在零样本分类中实现自适应提前退出，节省计算量并保证竞争力。


<details>
  <summary>Details</summary>
Motivation: 实现CLIP的自适应分辨率推理，让模型能在同一模型中处理多分辨率图像并在推理时按需细化，用户可动态选择计算-精度权衡。

Method: 用多级小波分解替代标准的图像块嵌入，推理时从低分辨率开始按需细化，使用键值缓存和因果跨层注意力重用计算，采用基于置信度的门控机制实现自适应提前退出，从冻结的CLIP教师模型进行轻量级蒸馏。

Result: 在零样本分类中，简单的基于置信度的门控机制可实现自适应提前退出，该方法只需轻量级蒸馏，能在显著节省计算量的同时达到有竞争力的准确率。

Conclusion: WAVECLIP模型是一种有效的实现CLIP自适应分辨率推理的方法，能在计算量和准确率之间取得良好平衡。

Abstract: We introduce WAVECLIP, a single unified model for adaptive resolution
inference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces
standard patch embeddings with a multi-level wavelet decomposition, enabling
the model to process images coarse to fine while naturally supporting multiple
resolutions within the same model. At inference time, the model begins with low
resolution tokens and refines only when needed, using key-value caching and
causal cross-level attention to reuse computation, effectively introducing to
the model only new information when needed. We evaluate WAVECLIP in zero-shot
classification, demonstrating that a simple confidence-based gating mechanism
enables adaptive early exits. This allows users to dynamically choose a
compute-accuracy trade-off using a single deployed model. Our approach requires
only lightweight distillation from a frozen CLIP teacher and achieves
competitive accuracy with significant computational savings.

</details>


### [294] [Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy](https://arxiv.org/abs/2509.21173)
*Aymen Bouguerra,Daniel Montoya,Alexandra Gomez-Villa,Fabio Arnez,Chokri Mraidha*

Main category: cs.CV

TL;DR: 本文对CLIP模型量化进行大规模评估，发现量化对不同预训练模型校准有不同影响，还能提升OOD检测性能，且有QAT方法可兼顾多方面性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了CLIP在高效可靠部署中的关键方面，特别是量化对其性能除准确率外的影响。

Method: 对CLIP模型进行大规模量化评估，评估多项可靠性指标。

Result: 量化对不同预训练模型校准有不同影响，校准下降不影响其他可靠性指标提升，OOD检测性能可提高，有QAT方法可兼顾多方面性能。

Conclusion: 研究结果为利用量化部署高效、可靠、鲁棒的视觉语言模型提供了关键见解。

Abstract: The powerful zero-shot generalization capabilities of vision-language models
(VLMs) like CLIP have enabled new paradigms for safety-related tasks such as
out-of-distribution (OOD) detection. However, additional aspects crucial for
the computationally efficient and reliable deployment of CLIP are still
overlooked. In particular, the impact of quantization on CLIP's performance
beyond accuracy remains underexplored. This work presents a large-scale
evaluation of quantization on CLIP models, assessing not only in-distribution
accuracy but a comprehensive suite of reliability metrics and revealing
counterintuitive results driven by pre-training source. We demonstrate that
quantization consistently improves calibration for typically underconfident
pre-trained models, while often degrading it for overconfident variants.
Intriguingly, this degradation in calibration does not preclude gains in other
reliability metrics; we find that OOD detection can still improve for these
same poorly calibrated models. Furthermore, we identify specific
quantization-aware training (QAT) methods that yield simultaneous gains in
zero-shot accuracy, calibration, and OOD robustness, challenging the view of a
strict efficiency-performance trade-off. These findings offer critical insights
for navigating the multi-objective problem of deploying efficient, reliable,
and robust VLMs by utilizing quantization beyond its conventional role.

</details>


### [295] [The Unwinnable Arms Race of AI Image Detection](https://arxiv.org/abs/2509.21135)
*Till Aczel,Lorenzo Vettor,Andreas Plesner,Roger Wattenhofer*

Main category: cs.CV

TL;DR: 本文研究图像生成AI中鉴别器最不利的条件，分析数据维度和复杂度两个因素，发现中等复杂度数据集最利于检测合成图像。


<details>
  <summary>Details</summary>
Motivation: 图像生成AI发展模糊了合成与真实图像边界，研究鉴别器在竞争中最不利的条件。

Method: 分析数据维度和复杂度两个关键因素，用Kolmogorov复杂度衡量数据集内在结构。

Result: 简单和高度复杂的数据集会降低合成图像的可检测性，中等复杂度数据集最有利于检测。

Conclusion: 中等复杂度数据集为鉴别器创造了最有利的检测条件。

Abstract: The rapid progress of image generative AI has blurred the boundary between
synthetic and real images, fueling an arms race between generators and
discriminators. This paper investigates the conditions under which
discriminators are most disadvantaged in this competition. We analyze two key
factors: data dimensionality and data complexity. While increased
dimensionality often strengthens the discriminators ability to detect subtle
inconsistencies, complexity introduces a more nuanced effect. Using Kolmogorov
complexity as a measure of intrinsic dataset structure, we show that both very
simple and highly complex datasets reduce the detectability of synthetic
images; generators can learn simple datasets almost perfectly, whereas extreme
diversity masks imperfections. In contrast, intermediate-complexity datasets
create the most favorable conditions for detection, as generators fail to fully
capture the distribution and their errors remain visible.

</details>


### [296] [Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets](https://arxiv.org/abs/2509.21245)
*Team Hunyuan3D,:,Bowen Zhang,Chunchao Guo,Haolin Liu,Hongyu Yan,Huiwen Shi,Jingwei Huang,Junlin Yu,Kunhong Li,Linus,Penghao Wang,Qingxiang Lin,Sicong Liu,Xianghui Yang,Yixuan Tang,Yunfei Zhao,Zeqiang Lai,Zhihao Liang,Zibo Zhao*

Main category: cs.CV

TL;DR: 现有3D生成模型缺乏细粒度跨模态控制，提出Hunyuan3D - Omni框架，实验表明新增控制提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型多依赖图像或文本条件，缺乏细粒度跨模态控制，限制可控性和实际应用。

Method: 基于Hunyuan3D 2.1构建Hunyuan3D - Omni框架，接受多种条件信号，采用单交叉模态架构统一信号，用渐进、难度感知采样策略训练。

Result: 新增控制提高生成准确性，实现几何感知变换，增加生产工作流程的鲁棒性。

Conclusion: Hunyuan3D - Omni框架能有效解决现有3D生成模型缺乏细粒度跨模态控制的问题。

Abstract: Recent advances in 3D-native generative models have accelerated asset
creation for games, film, and design. However, most methods still rely
primarily on image or text conditioning and lack fine-grained, cross-modal
controls, which limits controllability and practical adoption. To address this
gap, we present Hunyuan3D-Omni, a unified framework for fine-grained,
controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images,
Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose
priors as conditioning signals, enabling precise control over geometry,
topology, and pose. Instead of separate heads for each modality, our model
unifies all signals in a single cross-modal architecture. We train with a
progressive, difficulty-aware sampling strategy that selects one control
modality per example and biases sampling toward harder signals (e.g., skeletal
pose) while downweighting easier ones (e.g., point clouds), encouraging robust
multi-modal fusion and graceful handling of missing inputs. Experiments show
that these additional controls improve generation accuracy, enable
geometry-aware transformations, and increase robustness for production
workflows.

</details>


### [297] [Learning to Look: Cognitive Attention Alignment with Vision-Language Models](https://arxiv.org/abs/2509.21247)
*Ryan L. Yang,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 提出利用视觉语言模型自动生成语义注意力图的可扩展框架，提升CNN决策可靠性，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有指导模型注意力的方法依赖人工标注，缺乏可扩展性，需改进CNN决策可靠性。

Method: 利用视觉语言模型根据自然语言提示自动生成语义注意力图，并引入辅助损失使CNN注意力与语言引导图对齐。

Result: 在ColoredMNIST和DecoyMNIST数据集上，方法在ColorMNIST达SOTA，在DecoyMNIST与重标注基线竞争，泛化性提升、依赖捷径减少、注意力更符合人类直觉。

Conclusion: 所提出的可扩展框架无需手动标注，能促进更可靠且认知上合理的决策。

Abstract: Convolutional Neural Networks (CNNs) frequently "cheat" by exploiting
superficial correlations, raising concerns about whether they make predictions
for the right reasons. Inspired by cognitive science, which highlights the role
of attention in robust human perception, recent methods have sought to guide
model attention using concept-based supervision and explanation regularization.
However, these techniques depend on labor-intensive, expert-provided
annotations, limiting their scalability. We propose a scalable framework that
leverages vision-language models to automatically generate semantic attention
maps using natural language prompts. By introducing an auxiliary loss that
aligns CNN attention with these language-guided maps, our approach promotes
more reliable and cognitively plausible decision-making without manual
annotation. Experiments on challenging datasets, ColoredMNIST and DecoyMNIST,
show that our method achieves state-of-the-art performance on ColorMNIST and
remains competitive with annotation-heavy baselines on DecoyMNIST,
demonstrating improved generalization, reduced shortcut reliance, and model
attention that better reflects human intuition.

</details>


### [298] [Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations](https://arxiv.org/abs/2509.21249)
*Zhijian Yang,Noel DSouza,Istvan Megyeri,Xiaojian Xu,Amin Honarmandi Shandiz,Farzin Haddadpour,Krisztian Koos,Laszlo Rusko,Emanuele Valeriano,Bharadwaj Swaninathan,Lei Wu,Parminder Bhatia,Taha Kass-Hout,Erhan Bas*

Main category: cs.CV

TL;DR: 提出用于3D MRI的视觉语言基础模型Decipher - MR，在多种基准测试中表现优于现有模型，是基于MRI的AI可扩展且通用的基础。


<details>
  <summary>Details</summary>
Motivation: MRI自动化分析有挑战，基础模型在MRI应用受限，需解决数据稀缺和解剖学关注范围窄的问题。

Method: 在大规模MRI数据集上训练Decipher - MR，结合自监督视觉学习和报告引导的文本监督，采用模块化设计支持调优轻量级特定任务解码器。

Result: 在疾病分类、人口统计预测等多种基准测试中，Decipher - MR比现有基础模型和特定任务方法有持续性能提升。

Conclusion: Decipher - MR是基于MRI的AI可扩展且通用的基础，有助于临床和研究领域的高效开发。

Abstract: Magnetic Resonance Imaging (MRI) is a critical medical imaging modality in
clinical diagnosis and research, yet its complexity and heterogeneity pose
challenges for automated analysis, particularly in scalable and generalizable
machine learning applications. While foundation models have revolutionized
natural language and vision tasks, their application to MRI remains limited due
to data scarcity and narrow anatomical focus. In this work, we present
Decipher-MR, a 3D MRI-specific vision-language foundation model trained on a
large-scale dataset comprising 200,000 MRI series from over 22,000 studies
spanning diverse anatomical regions, sequences, and pathologies. Decipher-MR
integrates self-supervised vision learning with report-guided text supervision
to build robust, generalizable representations, enabling effective adaptation
across broad applications. To enable robust and diverse clinical tasks with
minimal computational overhead, Decipher-MR supports a modular design that
enables tuning of lightweight, task-specific decoders attached to a frozen
pretrained encoder. Following this setting, we evaluate Decipher-MR across
diverse benchmarks including disease classification, demographic prediction,
anatomical localization, and cross-modal retrieval, demonstrating consistent
performance gains over existing foundation models and task-specific approaches.
Our results establish Decipher-MR as a scalable and versatile foundation for
MRI-based AI, facilitating efficient development across clinical and research
domains.

</details>


### [299] [Instruction-tuned Self-Questioning Framework for Multimodal Reasoning](https://arxiv.org/abs/2509.21251)
*You-Won Jang,Yu-Jung Heo,Jaeseok Kim,Minsu Lee,Du-Seong Chang,Byoung-Tak Zhang*

Main category: cs.CV

TL;DR: 现有视觉语言理解领域在多步推理有问题，本文提出SQ - InstructBLIP方法，实验表明其在VQA任务推理更准确。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言理解在多步推理存在问题，现有用LLMs解决问题的方法有缺陷，像无法获取图像细粒度内容、黑盒模型难复现等。

Method: 提出SQ - InstructBLIP，由Questioner、Answerer和Reasoner组成，共享架构，Questioner和Answerer生成子问题与子答案辅助推理主问题，Reasoner结合子问题信息对主问题推理。

Result: 在解决VQA任务时，将生成的子问题作为额外信息，SQ - InstructBLIP比以往方法推理更准确。

Conclusion: SQ - InstructBLIP方法能有效提升视觉语言理解中的多步推理性能。

Abstract: The field of vision-language understanding has been actively researched in
recent years, thanks to the development of Large Language Models~(LLMs).
However, it still needs help with problems requiring multi-step reasoning, even
for very simple questions. Recent studies adopt LLMs to tackle this problem by
iteratively generating sub-questions and answers. However, there are
disadvantages such as 1) the fine-grained visual contents of images are not
available using LLMs that cannot read visual information, 2) internal
mechanisms are inaccessible and difficult to reproduce by using black-box LLMs.
To solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP,
which improves inference performance by generating image-aware informative
sub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists
of a Questioner, Answerer, and Reasoner that share the same architecture.
Questioner and Answerer generate sub-questions and sub-answers to help infer
the main-question, and Reasoner performs reasoning on the main-question
considering the generated sub-question information. Our experiments show that
the proposed method SQ-InstructBLIP, which uses the generated sub-questions as
additional information when solving the VQA task, performs more accurate
reasoning than the previous works.

</details>


### [300] [Learning Conformal Explainers for Image Classifiers](https://arxiv.org/abs/2509.21209)
*Amr Alkhatib,Stephanie Lowry*

Main category: cs.CV

TL;DR: 提出基于共形预测的方法控制图像预测特征归因解释的保真度，用多数据集评估，FastSHAP表现佳，超像素共形度量更有效。


<details>
  <summary>Details</summary>
Motivation: 现有特征归因方法解释的鲁棒性和保真度不足，无法忠实反映黑盒模型推理过程。

Method: 提出基于共形预测的方法，识别能保留模型预测的显著特征子集，提出四个共形函数量化解释与模型预测的符合程度。

Result: FastSHAP在保真度和信息效率上始终优于其他方法，基于超像素的共形度量更有效。

Conclusion: 所提方法能有效控制特征归因解释的保真度，FastSHAP表现突出，超像素共形度量值得关注。

Abstract: Feature attribution methods are widely used for explaining image-based
predictions, as they provide feature-level insights that can be intuitively
visualized. However, such explanations often vary in their robustness and may
fail to faithfully reflect the reasoning of the underlying black-box model. To
address these limitations, we propose a novel conformal prediction-based
approach that enables users to directly control the fidelity of the generated
explanations. The method identifies a subset of salient features that is
sufficient to preserve the model's prediction, regardless of the information
carried by the excluded features, and without demanding access to ground-truth
explanations for calibration. Four conformity functions are proposed to
quantify the extent to which explanations conform to the model's predictions.
The approach is empirically evaluated using five explainers across six image
datasets. The empirical results demonstrate that FastSHAP consistently
outperforms the competing methods in terms of both fidelity and informational
efficiency, the latter measured by the size of the explanation regions.
Furthermore, the results reveal that conformity measures based on super-pixels
are more effective than their pixel-wise counterparts.

</details>


### [301] [MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation](https://arxiv.org/abs/2509.21265)
*Xinyu Liu,Guolei Sun,Cheng Wang,Yixuan Yuan,Ender Konukoglu*

Main category: cs.CV

TL;DR: 提出针对医学视频超分辨率的MedVSR框架，在四个数据集上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 高分辨率医学视频获取难，低分辨率视频给VSR模型带来诸多挑战，现有模型易引入伪影和扭曲特征误导医生。

Method: 提出MedVSR框架，用Cross State - Space Propagation (CSSP)解决对齐不精确问题，设计Inner State - Space Reconstruction (ISSR)模块增强组织结构并减少伪影。

Result: 在包括内窥镜和白内障手术等四种不同医学场景的数据集上，MedVSR在重建性能和效率上显著优于现有VSR模型。

Conclusion: MedVSR框架能有效提升医学视频超分辨率的重建性能和效率。

Abstract: High-resolution (HR) medical videos are vital for accurate diagnosis, yet are
hard to acquire due to hardware limitations and physiological constraints.
Clinically, the collected low-resolution (LR) medical videos present unique
challenges for video super-resolution (VSR) models, including camera shake,
noise, and abrupt frame transitions, which result in significant optical flow
errors and alignment difficulties. Additionally, tissues and organs exhibit
continuous and nuanced structures, but current VSR models are prone to
introducing artifacts and distorted features that can mislead doctors. To this
end, we propose MedVSR, a tailored framework for medical VSR. It first employs
Cross State-Space Propagation (CSSP) to address the imprecise alignment by
projecting distant frames as control matrices within state-space models,
enabling the selective propagation of consistent and informative features to
neighboring frames for effective alignment. Moreover, we design an Inner
State-Space Reconstruction (ISSR) module that enhances tissue structures and
reduces artifacts with joint long-range spatial feature learning and
large-kernel short-range information aggregation. Experiments across four
datasets in diverse medical scenarios, including endoscopy and cataract
surgeries, show that MedVSR significantly outperforms existing VSR models in
reconstruction performance and efficiency. Code released at
https://github.com/CUHK-AIM-Group/MedVSR.

</details>


### [302] [Does FLUX Already Know How to Perform Physically Plausible Image Composition?](https://arxiv.org/abs/2509.21278)
*Shilin Lu,Zhuming Lian,Zihan Zhou,Shaocong Zhang,Chen Zhao,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: 提出无训练框架SHINE用于图像合成，引入新损失和指导方法，还推出ComplexCompo基准，实验表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有图像合成模型在复杂光照和多样高分辨率输入上有困难，文本到图像扩散模型缺乏有效框架且现有方法有缺陷。

Method: 提出SHINE框架，引入流形引导锚定损失，使用预训练定制适配器，采用降解抑制指导和自适应背景融合，推出ComplexCompo基准。

Result: 在ComplexCompo和DreamEditBench上实验，在标准指标和人类对齐分数上达最优。

Conclusion: SHINE框架有效解决图像合成问题，性能优越，代码和基准将公开。

Abstract: Image composition aims to seamlessly insert a user-specified object into a
new scene, but existing models struggle with complex lighting (e.g., accurate
shadows, water reflections) and diverse, high-resolution inputs. Modern
text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential
physical and resolution priors, yet lack a framework to unleash them without
resorting to latent inversion, which often locks object poses into contextually
inappropriate orientations, or brittle attention surgery. We propose SHINE, a
training-free framework for Seamless, High-fidelity Insertion with Neutralized
Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained
customization adapters (e.g., IP-Adapter) to guide latents for faithful subject
representation while preserving background integrity. Degradation-suppression
guidance and adaptive background blending are proposed to further eliminate
low-quality outputs and visible seams. To address the lack of rigorous
benchmarks, we introduce ComplexCompo, featuring diverse resolutions and
challenging conditions such as low lighting, strong illumination, intricate
shadows, and reflective surfaces. Experiments on ComplexCompo and
DreamEditBench show state-of-the-art performance on standard metrics (e.g.,
DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward).
Code and benchmark will be publicly available upon publication.

</details>


### [303] [SD3.5-Flash: Distribution-Guided Distillation of Generative Flows](https://arxiv.org/abs/2509.21318)
*Hmrishav Bandyopadhyay,Rahim Entezari,Jim Scott,Reshinth Adithyan,Yi-Zhe Song,Varun Jampani*

Main category: cs.CV

TL;DR: 提出SD3.5 - Flash高效少步蒸馏框架，可在消费级设备实现高质量图像生成，优于现有少步方法。


<details>
  <summary>Details</summary>
Motivation: 让消费级设备实现高质量图像生成，解决计算成本高的问题。

Method: 通过重新制定分布匹配目标蒸馏整流流模型，引入‘时间步共享’和‘拆分时间步微调’，结合文本编码器重构和量化等优化。

Result: 系统可实现快速生成和内存高效部署，在不同硬件配置上表现良好。

Conclusion: SD3.5 - Flash持续优于现有少步方法，使先进生成式AI可实际部署。

Abstract: We present SD3.5-Flash, an efficient few-step distillation framework that
brings high-quality image generation to accessible consumer devices. Our
approach distills computationally prohibitive rectified flow models through a
reformulated distribution matching objective tailored specifically for few-step
generation. We introduce two key innovations: "timestep sharing" to reduce
gradient noise and "split-timestep fine-tuning" to improve prompt alignment.
Combined with comprehensive pipeline optimizations like text encoder
restructuring and specialized quantization, our system enables both rapid
generation and memory-efficient deployment across different hardware
configurations. This democratizes access across the full spectrum of devices,
from mobile phones to desktop computers. Through extensive evaluation including
large-scale user studies, we demonstrate that SD3.5-Flash consistently
outperforms existing few-step methods, making advanced generative AI truly
accessible for practical deployment.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [304] [SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations](https://arxiv.org/abs/2509.20567)
*Ayan Sar,Pranav Singh Puri,Sumit Aich,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: 提出SwasthLLM框架用于多语言医疗诊断，无需特定语言微调，训练后在监督和零样本场景表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决多语言医疗环境中因低资源语言标注数据稀缺和语言差异导致的自动疾病诊断难题。

Method: 构建SwasthLLM框架，利用XLM - RoBERTa编码器、语言感知注意力机制等，采用多任务学习策略和MAML，分阶段训练。

Result: 监督设置下测试准确率97.22%，F1分数97.17%；零样本场景中，印地语准确率92.78%，孟加拉语准确率73.33%。

Conclusion: SwasthLLM在多语言医疗诊断中有效，在低资源环境有强泛化能力。

Abstract: In multilingual healthcare environments, automatic disease diagnosis from
clinical text remains a challenging task due to the scarcity of annotated
medical data in low-resource languages and the linguistic variability across
populations. This paper proposes SwasthLLM, a unified, zero-shot,
cross-lingual, and multi-task learning framework for medical diagnosis that
operates effectively across English, Hindi, and Bengali without requiring
language-specific fine-tuning. At its core, SwasthLLM leverages the
multilingual XLM-RoBERTa encoder augmented with a language-aware attention
mechanism and a disease classification head, enabling the model to extract
medically relevant information regardless of the language structure. To align
semantic representations across languages, a Siamese contrastive learning
module is introduced, ensuring that equivalent medical texts in different
languages produce similar embeddings. Further, a translation consistency module
and a contrastive projection head reinforce language-invariant representation
learning. SwasthLLM is trained using a multi-task learning strategy, jointly
optimizing disease classification, translation alignment, and contrastive
learning objectives. Additionally, we employ Model-Agnostic Meta-Learning
(MAML) to equip the model with rapid adaptation capabilities for unseen
languages or tasks with minimal data. Our phased training pipeline emphasizes
robust representation alignment before task-specific fine-tuning. Extensive
evaluation shows that SwasthLLM achieves high diagnostic performance, with a
test accuracy of 97.22% and an F1-score of 97.17% in supervised settings.
Crucially, in zero-shot scenarios, it attains 92.78% accuracy on Hindi and
73.33% accuracy on Bengali medical text, demonstrating strong generalization in
low-resource contexts.

</details>


### [305] [Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures](https://arxiv.org/abs/2509.20577)
*Sampurna Roy,Ayan Sar,Anurag Kaushish,Kanav Gupta,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: 提出DS - MoE框架解决Transformer架构效率低和推理质量受限问题，实验显示其有计算、推理速度和准确率优势，提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 当代Transformer架构对所有输入应用相同处理深度，导致效率低下和推理质量受限。

Method: 提出深度专业化专家混合（DS - MoE）的动态推理链概念，引入针对不同推理深度优化的专家模块，用学习的路由网络动态组装定制推理链。

Result: 相比统一深度Transformer，DS - MoE实现高达16%计算节省和35%推理加速，复杂多步推理基准测试准确率高2.8%，路由决策产生可解释推理链。

Conclusion: DS - MoE是自适应神经架构的重大进步，深度专业化模块化处理可同时提高大语言模型效率、推理质量和可解释性。

Abstract: Contemporary transformer architectures apply identical processing depth to
all inputs, creating inefficiencies and limiting reasoning quality. Simple
factual queries are subjected to the same multilayered computation as complex
logical problems, wasting resources while constraining deep inference. To
overcome this, we came up with a concept of Dynamic Reasoning Chains through
Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends
the Mixture of Experts paradigm from width-based to depth specialised
computation. DS-MoE introduces expert modules optimised for distinct reasoning
depths, shallow pattern recognition, compositional reasoning, logical
inference, memory integration, and meta-cognitive supervision. A learned
routing network dynamically assembles custom reasoning chains, activating only
the necessary experts to match input complexity. The dataset on which we
trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse
domains such as scientific papers, legal texts, programming code, and web
content, enabling systematic assessment across reasoning depths. Experimental
results demonstrate that DS-MoE achieves up to 16 per cent computational
savings and 35 per cent faster inference compared to uniform-depth
transformers, while delivering 2.8 per cent higher accuracy on complex
multi-step reasoning benchmarks. Furthermore, routing decisions yield
interpretable reasoning chains, enhancing transparency and scalability. These
findings establish DS-MoE as a significant advancement in adaptive neural
architectures, demonstrating that depth-specialised modular processing can
simultaneously improve efficiency, reasoning quality, and interpretability in
large-scale language models.

</details>


### [306] [Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture for Multi-Scale Language Understanding](https://arxiv.org/abs/2509.20581)
*Ayan Sar,Sampurna Roy,Kanav Gupta,Anurag Kaushish,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: 提出Hierarchical Resolution Transformer (HRT)架构，在多基准测试中表现优于标准Transformer，具效率和准确性优势。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer架构处理文本时未体现人类语言层次结构，存在计算成本高、组合泛化弱等问题。

Method: 提出受小波启发的HRT架构，构建多分辨率注意力，实现自下而上组合和自上而下上下文建模，采用指数序列缩减。

Result: 在GLUE、SuperGLUE等基准测试中平均表现优于标准Transformer基线，减少内存使用和推理延迟，消融研究证实跨分辨率注意力和特定尺度模块的有效性。

Conclusion: HRT是首个使计算结构与人类语言层次组织一致的架构，多尺度小波启发处理带来理论效率提升和语言理解实际改进。

Abstract: Transformer architectures have achieved state-of-the-art performance across
natural language tasks, yet they fundamentally misrepresent the hierarchical
nature of human language by processing text as flat token sequences. This
results in quadratic computational cost, weak computational cost, weak
compositional generalization, and inadequate discourse-level modeling. We
propose Hierarchical Resolution Transformer (HRT), a novel wavelet-inspired
neural architecture that processes language simultaneously across multiple
resolutions, from characters to discourse-level units. HRT constructs a
multi-resolution attention, enabling bottom-up composition and top-down
contextualization. By employing exponential sequence reduction across scales,
HRT achieves O(nlogn) complexity, offering significant efficiency improvements
over standard transformers. We evaluated HRT on a diverse suite of benchmarks,
including GLUE, SuperGLUE, Long Range Arena, and WikiText-103, and results
demonstrated that HRT outperforms standard transformer baselines by an average
of +3.8% on GLUE, +4.5% on SuperGLUE, and +6.1% on Long Range Arena, while
reducing memory usage by 42% and inference latency by 37% compared to BERT and
GPT style models of similar parameter count. Ablation studies confirm the
effectiveness of cross-resolution attention and scale-specialized modules,
showing that each contributes independently to both efficiency and accuracy.
Our findings establish HRT as the first architecture to align computational
structure with the hierarchical organization of human language, demonstrating
that multi-scale, wavelet-inspired processing yields both theoretical
efficiency gains and practical improvements in language understanding.

</details>


### [307] [Few-Shot and Training-Free Review Generation via Conversational Prompting](https://arxiv.org/abs/2509.20805)
*Genki Kusano*

Main category: cs.CL

TL;DR: 提出对话式提示方法用于少样本免训练的个性化评论生成，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有个性化评论生成方法在少样本免训练场景受限，大语言模型需合适提示工程。

Method: 提出对话式提示方法，有简单变体SCP和对比变体CCP。

Result: 实验表明传统非对话提示生成评论类似随机用户，SCP和CCP生成评论更接近目标用户，CCP在有高质量负例时更优，SCP在无此类数据时仍有竞争力。

Conclusion: 对话式提示为少样本免训练的评论生成提供实用解决方案。

Abstract: Personalized review generation helps businesses understand user preferences,
yet most existing approaches assume extensive review histories of the target
user or require additional model training. Real-world applications often face
few-shot and training-free situations, where only a few user reviews are
available and fine-tuning is infeasible. It is well known that large language
models (LLMs) can address such low-resource settings, but their effectiveness
depends on prompt engineering. In this paper, we propose Conversational
Prompting, a lightweight method that reformulates user reviews as multi-turn
conversations. Its simple variant, Simple Conversational Prompting (SCP),
relies solely on the user's own reviews, while the contrastive variant,
Contrastive Conversational Prompting (CCP), inserts reviews from other users or
LLMs as incorrect replies and then asks the model to correct them, encouraging
the model to produce text in the user's style. Experiments on eight product
domains and five LLMs showed that the conventional non-conversational prompt
often produced reviews similar to those written by random users, based on
text-based metrics such as ROUGE-L and BERTScore, and application-oriented
tasks like user identity matching and sentiment analysis. In contrast, both SCP
and CCP produced reviews much closer to those of the target user, even when
each user had only two reviews. CCP brings further improvements when
high-quality negative examples are available, whereas SCP remains competitive
when such data cannot be collected. These results suggest that conversational
prompting offers a practical solution for review generation under few-shot and
training-free constraints.

</details>


### [308] [BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback](https://arxiv.org/abs/2509.21106)
*Hyunseo Kim,Sangam Lee,Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: 提出BESPOKE基准评估搜索增强大语言模型个性化，进行系统分析并公开代码数据。


<details>
  <summary>Details</summary>
Motivation: 现有搜索增强大语言模型无法满足用户多样需求，且缺乏对个性化的系统评估。

Method: 构建现实且具有诊断性的BESPOKE基准，通过长期深度人工标注完成。

Result: 利用BESPOKE进行系统分析，揭示信息搜索任务中有效个性化的关键要求。

Conclusion: BESPOKE为细粒度评估个性化搜索增强大语言模型奠定基础。

Abstract: Search-augmented large language models (LLMs) have advanced
information-seeking tasks by integrating retrieval into generation, reducing
users' cognitive burden compared to traditional search systems. Yet they remain
insufficient for fully addressing diverse user needs, which requires
recognizing how the same query can reflect different intents across users and
delivering information in preferred forms. While recent systems such as ChatGPT
and Gemini attempt personalization by leveraging user histories, systematic
evaluation of such personalization is under-explored. To address this gap, we
propose BESPOKE, the realistic benchmark for evaluating personalization in
search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting
authentic chat and search histories directly from humans, and diagnostic, by
pairing responses with fine-grained preference scores and feedback. The
benchmark is constructed through long-term, deeply engaged human annotation,
where human annotators contributed their own histories, authored queries with
detailed information needs, and evaluated responses with scores and diagnostic
feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key
requirements for effective personalization in information-seeking tasks,
providing a foundation for fine-grained evaluation of personalized
search-augmented LLMs. Our code and data are available at
https://augustinlib.github.io/BESPOKE/.

</details>


### [309] [Retrieval over Classification: Integrating Relation Semantics for Multimodal Relation Extraction](https://arxiv.org/abs/2509.21151)
*Lei Hei,Tingjing Liao,Yingxin Pei,Yiyang Qi,Jiaqi Wang,Ruiting Li,Feiliang Ren*

Main category: cs.CL

TL;DR: 提出ROC框架将多模态关系提取转化为检索任务，在基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统多模态关系提取采用分类范式有忽视结构约束和缺乏语义表达能力的局限。

Method: 提出ROC框架，通过多模态编码器整合实体类型和位置信息，用大语言模型扩展关系标签为自然语言描述，通过基于语义相似度的对比学习对齐实体 - 关系对。

Result: 在MNRE和MORE基准数据集上达到了最先进的性能，且具有更强的鲁棒性和可解释性。

Conclusion: ROC框架有效解决传统多模态关系提取范式的问题，在多模态关系提取任务中表现良好。

Abstract: Relation extraction (RE) aims to identify semantic relations between entities
in unstructured text. Although recent work extends traditional RE to multimodal
scenarios, most approaches still adopt classification-based paradigms with
fused multimodal features, representing relations as discrete labels. This
paradigm has two significant limitations: (1) it overlooks structural
constraints like entity types and positional cues, and (2) it lacks semantic
expressiveness for fine-grained relation understanding. We propose
\underline{R}etrieval \underline{O}ver \underline{C}lassification (ROC), a
novel framework that reformulates multimodal RE as a retrieval task driven by
relation semantics. ROC integrates entity type and positional information
through a multimodal encoder, expands relation labels into natural language
descriptions using a large language model, and aligns entity-relation pairs via
semantic similarity-based contrastive learning. Experiments show that our
method achieves state-of-the-art performance on the benchmark datasets MNRE and
MORE and exhibits stronger robustness and interpretability.

</details>


### [310] [SGMem: Sentence Graph Memory for Long-Term Conversational Agents](https://arxiv.org/abs/2509.21212)
*Yaxiong Wu,Yongyue Zhang,Sheng Liang,Yong Liu*

Main category: cs.CL

TL;DR: 引入SGMem处理长对话代理的记忆管理问题，实验显示其在长对话问答中表现出色。


<details>
  <summary>Details</summary>
Motivation: 长对话代理需有效记忆管理，现有方法在组织和检索不同粒度信息上存在困难。

Method: 引入SGMem，将对话表示为分块单元内的句子级图，结合原始对话和生成记忆为大语言模型提供上下文。

Result: 在LongMemEval和LoCoMo实验中，SGMem持续提高准确率，优于强大基线。

Conclusion: SGMem能为长对话代理提供有效记忆管理，在长对话问答中表现良好。

Abstract: Long-term conversational agents require effective memory management to handle
dialogue histories that exceed the context window of large language models
(LLMs). Existing methods based on fact extraction or summarization reduce
redundancy but struggle to organize and retrieve relevant information across
different granularities of dialogue and generated memory. We introduce SGMem
(Sentence Graph Memory), which represents dialogue as sentence-level graphs
within chunked units, capturing associations across turn-, round-, and
session-level contexts. By combining retrieved raw dialogue with generated
memory such as summaries, facts and insights, SGMem supplies LLMs with coherent
and relevant context for response generation. Experiments on LongMemEval and
LoCoMo show that SGMem consistently improves accuracy and outperforms strong
baselines in long-term conversational question answering.

</details>


### [311] [Query-Centric Graph Retrieval Augmented Generation](https://arxiv.org/abs/2509.21237)
*Yaxiong Wu,Jianyuan Bo,Yongyue Zhang,Sheng Liang,Yong Liu*

Main category: cs.CL

TL;DR: 提出QCG - RAG框架解决图基检索增强生成（RAG）的粒度困境，实验表明其在问答准确性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图基RAG方法存在粒度困境，细粒度实体级图有高token成本且丢失上下文，粗粒度文档级图无法捕捉细微关系。

Method: 引入QCG - RAG框架，利用Doc2Query和Doc2Query--构建可控粒度的查询中心图，并通过定制的多跳检索机制选择相关块。

Result: 在LiHuaWorld和MultiHop - RAG上的实验表明，QCG - RAG在问答准确性上始终优于先前基于块和图的RAG方法。

Conclusion: QCG - RAG为多跳推理建立了新范式。

Abstract: Graph-based retrieval-augmented generation (RAG) enriches large language
models (LLMs) with external knowledge for long-context understanding and
multi-hop reasoning, but existing methods face a granularity dilemma:
fine-grained entity-level graphs incur high token costs and lose context, while
coarse document-level graphs fail to capture nuanced relations. We introduce
QCG-RAG, a query-centric graph RAG framework that enables query-granular
indexing and multi-hop chunk retrieval. Our query-centric approach leverages
Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with
controllable granularity, improving graph quality and interpretability. A
tailored multi-hop retrieval mechanism then selects relevant chunks via the
generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG
consistently outperforms prior chunk-based and graph-based RAG methods in
question answering accuracy, establishing a new paradigm for multi-hop
reasoning.

</details>


### [312] [Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models](https://arxiv.org/abs/2509.20367)
*Leyi Ouyang*

Main category: cs.CL

TL;DR: 提出新框架改变外交事件叙事以转变公众情绪，成功率70%，可作实用工具。


<details>
  <summary>Details</summary>
Motivation: 传统衡量公众情绪方法耗时费力且缺乏前瞻性，公众情绪对外交至关重要，需新方法转变公众情绪。

Method: 训练语言模型预测公众反应，构建数据集；结合传播理论和专家意见确定文本修改特征；开发反事实生成算法用大语言模型生成修改文本。

Result: 框架成功将公众情绪转变为更有利状态，成功率70%。

Conclusion: 该框架可作为实用工具，为外交人员、政策制定者和传播专家提供数据驱动的见解。

Abstract: Diplomatic events consistently prompt widespread public discussion and
debate. Public sentiment plays a critical role in diplomacy, as a good
sentiment provides vital support for policy implementation, helps resolve
international issues, and shapes a nation's international image. Traditional
methods for gauging public sentiment, such as large-scale surveys or manual
content analysis of media, are typically time-consuming, labor-intensive, and
lack the capacity for forward-looking analysis. We propose a novel framework
that identifies specific modifications for diplomatic event narratives to shift
public sentiment from negative to neutral or positive. First, we train a
language model to predict public reaction towards diplomatic events. To this
end, we construct a dataset comprising descriptions of diplomatic events and
their associated public discussions. Second, guided by communication theories
and in collaboration with domain experts, we predetermined several textual
features for modification, ensuring that any alterations changed the event's
narrative framing while preserving its core facts.We develop a counterfactual
generation algorithm that employs a large language model to systematically
produce modified versions of an original text. The results show that this
framework successfully shifted public sentiment to a more favorable state with
a 70\% success rate. This framework can therefore serve as a practical tool for
diplomats, policymakers, and communication specialists, offering data-driven
insights on how to frame diplomatic initiatives or report on events to foster a
more desirable public sentiment.

</details>


### [313] [CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics](https://arxiv.org/abs/2509.20374)
*Nithin Somasekharan,Ling Yue,Yadi Cao,Weichao Li,Patrick Emami,Pochinapeddi Sai Bhargav,Anurag Acharya,Xingyu Xie,Shaowu Pan*

Main category: cs.CL

TL;DR: 介绍CFDLLMBench基准套件评估大语言模型在计算流体动力学（CFD）中的性能，为复杂物理系统数值实验自动化奠定基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂物理系统数值实验自动化方面的应用研究不足，CFD是评估其科学能力的挑战性测试平台。

Method: 引入包含CFDQuery、CFDCodeBench和FoamBench三个组件的CFDLLMBench基准套件，结合详细任务分类和严格评估框架。

Result: 可提供可重复的结果，量化大语言模型在代码可执行性、解决方案准确性和数值收敛行为方面的性能。

Conclusion: CFDLLMBench为复杂物理系统数值实验的大语言模型驱动自动化发展和评估奠定了坚实基础。

Abstract: Large Language Models (LLMs) have demonstrated strong performance across
general NLP tasks, but their utility in automating numerical experiments of
complex physical system -- a critical and labor-intensive component -- remains
underexplored. As the major workhorse of computational science over the past
decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging
testbed for evaluating the scientific capabilities of LLMs. We introduce
CFDLLMBench, a benchmark suite comprising three complementary components --
CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM
performance across three key competencies: graduate-level CFD knowledge,
numerical and physical reasoning of CFD, and context-dependent implementation
of CFD workflows. Grounded in real-world CFD practices, our benchmark combines
a detailed task taxonomy with a rigorous evaluation framework to deliver
reproducible results and quantify LLM performance across code executability,
solution accuracy, and numerical convergence behavior. CFDLLMBench establishes
a solid foundation for the development and evaluation of LLM-driven automation
of numerical experiments for complex physical systems. Code and data are
available at https://github.com/NREL-Theseus/cfdllmbench/.

</details>


### [314] [Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text](https://arxiv.org/abs/2509.20375)
*Sharanya Parimanoharan,Ruwan D. Nawarathna*

Main category: cs.CL

TL;DR: 研究用标注数据集对比多种机器学习方法检测ChatGPT-3.5生成文本与人类文本，发现DistilBERT表现最佳，集成模型未超DistilBERT，为更强大框架奠基。


<details>
  <summary>Details</summary>
Motivation: 大语言模型模糊人机文本界限，需可靠AI文本检测保障学术诚信等。

Method: 用250对不同研究主题摘要的标注数据集，测试对比经典和基于Transformer的机器学习检测技术，还测试模型集成效果。

Result: DistilBERT整体表现最佳，Logistic Regression和BERT - Custom是不错替代，LSTM和BERT - N - gram较差，三个最佳模型的最大投票集成未超DistilBERT。

Conclusion: 全面评估AI文本检测方法优缺点，为更强大Transformer框架和更大更丰富数据集奠定基础。

Abstract: The rapid adoption of large language models (LLMs) such as ChatGPT has
blurred the line between human and AI-generated texts, raising urgent questions
about academic integrity, intellectual property, and the spread of
misinformation. Thus, reliable AI-text detection is needed for fair assessment
to safeguard human authenticity and cultivate trust in digital communication.
In this study, we investigate how well current machine learning (ML) approaches
can distinguish ChatGPT-3.5-generated texts from human-written texts employing
a labeled data set of 250 pairs of abstracts from a wide range of research
topics. We test and compare both classical (Logistic Regression armed with
classical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT
augmented with N-grams, DistilBERT, BERT with a lightweight custom classifier,
and LSTM-based N-gram models) ML detection techniques. As we aim to assess each
model's performance in detecting AI-generated research texts, we also aim to
test whether an ensemble of these models can outperform any single detector.
Results show DistilBERT achieves the overall best performance, while Logistic
Regression and BERT-Custom offer solid, balanced alternatives; LSTM- and
BERT-N-gram approaches lag. The max voting ensemble of the three best models
fails to surpass DistilBERT itself, highlighting the primacy of a single
transformer-based representation over mere model diversity. By comprehensively
assessing the strengths and weaknesses of these AI-text detection approaches,
this work lays a foundation for more robust transformer frameworks with larger,
richer datasets to keep pace with ever-improving generative AI models.

</details>


### [315] [ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models](https://arxiv.org/abs/2509.20376)
*Haoxuan Li,Zhen Wen,Qiqi Jiang,Chenxiao Li,Yuwei Wu,Yuchen Yang,Yiyao Wang,Xiuqi Huang,Minfeng Zhu,Wei Chen*

Main category: cs.CL

TL;DR: 提出可视化分析系统ConceptViz探索大语言模型概念，经场景与用户研究验证有效，代码开源。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型内部知识表示有挑战，稀疏自编码器特征与人类概念不天然对齐，解释困难。

Method: 实现识别 => 解释 => 验证流程，让用户用感兴趣概念查询SAEs，交互探索概念与特征对齐，通过模型行为验证对应关系。

Result: 通过两个使用场景和用户研究证明ConceptViz能简化大语言模型中有意义概念表示的发现和验证过程。

Conclusion: ConceptViz有助于大语言模型特征可解释性研究，帮助研究者构建更准确心理模型。

Abstract: Large language models (LLMs) have achieved remarkable performance across a
wide range of natural language tasks. Understanding how LLMs internally
represent knowledge remains a significant challenge. Despite Sparse
Autoencoders (SAEs) have emerged as a promising technique for extracting
interpretable features from LLMs, SAE features do not inherently align with
human-understandable concepts, making their interpretation cumbersome and
labor-intensive. To bridge the gap between SAE features and human concepts, we
present ConceptViz, a visual analytics system designed for exploring concepts
in LLMs. ConceptViz implements a novel dentification => Interpretation =>
Validation pipeline, enabling users to query SAEs using concepts of interest,
interactively explore concept-to-feature alignments, and validate the
correspondences through model behavior verification. We demonstrate the
effectiveness of ConceptViz through two usage scenarios and a user study. Our
results show that ConceptViz enhances interpretability research by streamlining
the discovery and validation of meaningful concept representations in LLMs,
ultimately aiding researchers in building more accurate mental models of LLM
features. Our code and user guide are publicly available at
https://github.com/Happy-Hippo209/ConceptViz.

</details>


### [316] [SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.20377)
*Tomoaki Isoda*

Main category: cs.CL

TL;DR: 提出SKILL - RAG方法利用模型自我知识筛选检索内容，实验验证其能提升生成质量并减少输入文档数。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法因检索系统可能返回无关内容导致模型幻觉，需识别和过滤无用检索内容以提升性能，且要更好整合模型内外知识。

Method: 提出SKILL - RAG方法，设计基于强化学习的训练框架获取模型自我知识，用句子级粒度过滤无关内容。

Result: 使用Llama2 - 7B和Qwen3 - 8B在问答基准测试中，该方法提升了生成质量，显著减少输入文档数。

Conclusion: 验证了自我知识在引导高质量检索选择中的重要性。

Abstract: Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive tasks in
recent years. However, since retrieval systems may return irrelevant content,
incorporating such information into the model often leads to hallucinations.
Thus, identifying and filtering out unhelpful retrieved content is a key
challenge for improving RAG performance.To better integrate the internal
knowledge of the model with external knowledge from retrieval, it is essential
to understand what the model "knows" and "does not know" (which is also called
"self-knowledge"). Based on this insight, we propose SKILL-RAG (Self-Knowledge
Induced Learning and Filtering for RAG), a novel method that leverages the
model's self-knowledge to determine which retrieved documents are beneficial
for answering a given query. We design a reinforcement learning-based training
framework to explicitly elicit self-knowledge from the model and employs
sentence-level granularity to filter out irrelevant content while preserving
useful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several
question answering benchmarks. Experimental results demonstrate that SKILL-RAG
not only improves generation quality but also significantly reduces the number
of input documents, validating the importance of self-knowledge in guiding the
selection of high-quality retrievals.

</details>


### [317] [Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation](https://arxiv.org/abs/2509.20378)
*Sirui Wang,Andong Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: 提出Emo - FiLM框架用于基于大语言模型的TTS，构建FEDD数据集，实验表明该框架在全局和细粒度任务上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有情感文本转语音系统依赖句子级控制，无法捕捉句子内动态情感变化，需要改进。

Method: 引入Emo - FiLM框架，将emotion2vec的帧级特征与单词对齐获取词级情感注释，通过FiLM层映射以直接调制文本嵌入实现词级情感控制，构建FEDD数据集用于评估。

Result: Emo - FiLM在全局和细粒度任务上均优于现有方法。

Conclusion: Emo - FiLM对富有表现力的语音合成具有有效性和通用性。

Abstract: Emotional text-to-speech (E-TTS) is central to creating natural and
trustworthy human-computer interaction. Existing systems typically rely on
sentence-level control through predefined labels, reference audio, or natural
language prompts. While effective for global emotion expression, these
approaches fail to capture dynamic shifts within a sentence. To address this
limitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework
for LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to
words to obtain word-level emotion annotations, and maps them through a
Feature-wise Linear Modulation (FiLM) layer, enabling word-level emotion
control by directly modulating text embeddings. To support evaluation, we
construct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed
annotations of emotional transitions. Experiments show that Emo-FiLM
outperforms existing approaches on both global and fine-grained tasks,
demonstrating its effectiveness and generality for expressive speech synthesis.

</details>


### [318] [USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model](https://arxiv.org/abs/2509.20381)
*Jianyu Wen,Jingyun Wang,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Ying Zhang*

Main category: cs.CL

TL;DR: 提出USB - Rec框架提升大语言模型在对话推荐中的性能，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的对话推荐系统方法重利用轻训练，需在模型层面提升性能。

Method: 设计基于大语言模型的偏好优化数据集构建策略用于强化学习训练，在推理阶段提出自我增强策略。

Result: 在多个数据集上的大量实验表明，该方法始终优于先前的先进方法。

Conclusion: 所提出的USB - Rec框架能有效提升大语言模型在对话推荐中的性能。

Abstract: Recently, Large Language Models (LLMs) have been widely employed in
Conversational Recommender Systems (CRSs). Unlike traditional language model
approaches that focus on training, all existing LLMs-based approaches are
mainly centered around how to leverage the summarization and analysis
capabilities of LLMs while ignoring the issue of training. Therefore, in this
work, we propose an integrated training-inference framework,
User-Simulator-Based framework (USB-Rec), for improving the performance of LLMs
in conversational recommendation at the model level. Firstly, we design a
LLM-based Preference Optimization (PO) dataset construction strategy for RL
training, which helps the LLMs understand the strategies and methods in
conversational recommendation. Secondly, we propose a Self-Enhancement Strategy
(SES) at the inference stage to further exploit the conversational
recommendation potential obtained from RL training. Extensive experiments on
various datasets demonstrate that our method consistently outperforms previous
state-of-the-art methods.

</details>


### [319] [MARS: toward more efficient multi-agent collaboration for LLM reasoning](https://arxiv.org/abs/2509.20502)
*Xiao Wang,Jia Wang,Yijie Wang,Pengtao Dang,Sha Cao,Chi Zhang*

Main category: cs.CL

TL;DR: 本文提出基于评审流程的多智能体协作框架MARS，能提升推理质量，减少计算开销，实验显示其在准确率相当的情况下，减少约50%的token使用和推理时间。


<details>
  <summary>Details</summary>
Motivation: 大语言模型单智能体推理能力有限，多智能体辩论（MAD）虽有效但计算开销大，需要新方法解决该问题。

Method: 提出MARS框架，包含作者智能体生成初始方案、评审智能体独立提供决策和评论、元评审员整合反馈做最终决策并指导修订。

Result: 在多个基准测试中与MAD及其他先进推理策略对比，不同大语言模型的大量实验表明，MARS准确率与MAD相当，同时token使用和推理时间约减少50%。

Conclusion: MARS能在提升推理质量的同时，控制计算开销，是一种有效的多智能体协作推理方法。

Abstract: Large language models (LLMs) have achieved impressive results in natural
language understanding, yet their reasoning capabilities remain limited when
operating as single agents. Multi-Agent Debate (MAD) has been proposed to
address this limitation by enabling collaborative reasoning among multiple
models in a round-table debate manner. While effective, MAD introduces
substantial computational overhead due to the number of agents involved and the
frequent communication required. In this paper, we propose MARS (Multi-Agent
Review System), a role-based collaboration framework inspired by the review
process. In MARS, an author agent generates an initial solution, reviewer
agents provide decisions and comments independently, and a meta-reviewer
integrates the feedback to make the final decision and guide further revision.
This design enhances reasoning quality while avoiding costly
reviewer-to-reviewer interactions, thereby controlling token consumption and
inference time. We compared MARS with both MAD and other state-of-the-art
reasoning strategies across multiple benchmarks. Extensive experiments with
different LLMs show that MARS matches the accuracy of MAD while reducing both
token usage and inference time by approximately 50\%. Code is available at
https://github.com/xwang97/MARS.

</details>


### [320] [FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models](https://arxiv.org/abs/2509.20624)
*Amin Karimi Monsefi,Nikhil Bhendawade,Manuel Rafael Ciosici,Dominic Culver,Yizhe Zhang,Irina Belousova*

Main category: cs.CL

TL;DR: 本文提出FS - DFM模型，可在保证质量的同时提升语言生成速度，在语言建模基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型串行生成限制吞吐量和增加长序列延迟，标准离散扩散模型需大量模型评估，因此需要设计一种能兼顾速度和质量的模型。

Method: 引入Few - Step Discrete Flow - Matching（FS - DFM），将采样步数设为显式参数，训练模型在不同步数预算下保持一致性，采用可靠更新规则和教师指导。

Result: FS - DFM在8个采样步骤下，与1024步离散流基线模型在生成1024个标记时困惑度相当，采样速度快达128倍。

Conclusion: FS - DFM能在不牺牲质量的前提下显著提升语言生成的速度和效率。

Abstract: Autoregressive language models (ARMs) deliver strong likelihoods, but are
inherently serial: they generate one token per forward pass, which limits
throughput and inflates latency for long sequences. Diffusion Language Models
(DLMs) parallelize across positions and thus appear promising for language
generation, yet standard discrete diffusion typically needs hundreds to
thousands of model evaluations to reach high quality, trading serial depth for
iterative breadth. We introduce FS-DFM, Few-Step Discrete Flow-Matching. A
discrete flow-matching model designed for speed without sacrificing quality.
The core idea is simple: make the number of sampling steps an explicit
parameter and train the model to be consistent across step budgets, so one big
move lands where many small moves would. We pair this with a reliable update
rule that moves probability in the right direction without overshooting, and
with strong teacher guidance distilled from long-run trajectories. Together,
these choices make few-step sampling stable, accurate, and easy to control. On
language modeling benchmarks, FS-DFM with 8 sampling steps achieves perplexity
parity with a 1,024-step discrete-flow baseline for generating 1,024 tokens
using a similar-size model, delivering up to 128 times faster sampling and
corresponding latency/throughput gains.

</details>


### [321] [Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions](https://arxiv.org/abs/2509.20645)
*Jungsoo Park,Ethan Mendes,Gabriel Stanovsky,Alan Ritter*

Main category: cs.CL

TL;DR: 文章研究文本性能预测，构建PRECOG语料库，实验表明任务有挑战但可行，为前瞻性评估提供初步步骤。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型评估瓶颈问题，探索能否在实验前预测结果。

Method: 研究文本性能预测，构建PRECOG语料库，通过实验进行验证。

Result: 配备检索模块的模型有适度预测性能，GPT - 5在零泄漏设置下有一定预测准确性。

Conclusion: 语料库和分析为开放式前瞻性评估提供初步步骤，支持难度估计和实验优先级排序。

Abstract: Progress in large language models is constrained by an evaluation bottleneck:
build a benchmark, evaluate models and settings, then iterate. We therefore ask
a simple question: can we forecast outcomes before running any experiments? We
study text-only performance forecasting: estimating a model's score from a
redacted task description and intended configuration, with no access to dataset
instances. To support systematic study, we curate PRECOG, a corpus of redacted
description-performance pairs spanning diverse tasks, domains, and metrics.
Experiments show the task is challenging but feasible: models equipped with a
retrieval module that excludes source papers achieve moderate prediction
performance with well-calibrated uncertainty, reaching mean absolute error as
low as 8.7 on the Accuracy subset at high-confidence thresholds. Our analysis
indicates that stronger reasoning models engage in diverse, iterative querying,
whereas current open-source models lag and often skip retrieval or gather
evidence with limited diversity. We further test a zero-leakage setting,
forecasting on newly released datasets or experiments before their papers are
indexed, where GPT-5 with built-in web search still attains nontrivial
prediction accuracy. Overall, our corpus and analyses offer an initial step
toward open-ended anticipatory evaluation, supporting difficulty estimation and
smarter experiment prioritization.

</details>


### [322] [Confidence-guided Refinement Reasoning for Zero-shot Question Answering](https://arxiv.org/abs/2509.20750)
*Youwon Jang,Woo Suk Choi,Minjoon Jung,Minsu Lee,Byoung-Tak Zhang*

Main category: cs.CL

TL;DR: 提出无训练框架C2R用于多领域问答任务，利用置信度选答案，能集成现有模型提升性能并分析子问答影响。


<details>
  <summary>Details</summary>
Motivation: 为多领域问答任务提供有效且无训练的解决方案，提升现有模型性能。

Method: 构建并优化子问题及其答案，通过比较答案候选的置信度选择最终答案，仅依赖模型自身置信度分数。

Result: 能与多种现有问答模型无缝集成，在不同模型和基准测试中持续提升性能。

Conclusion: C2R是适用于多领域问答任务的有效无训练框架，同时分析了子问答对模型行为的影响。

Abstract: We propose Confidence-guided Refinement Reasoning (C2R), a novel
training-free framework applicable to question-answering (QA) tasks across
text, image, and video domains. C2R strategically constructs and refines
sub-questions and their answers (sub-QAs), deriving a better confidence score
for the target answer. C2R first curates a subset of sub-QAs to explore diverse
reasoning paths, then compares the confidence scores of the resulting answer
candidates to select the most reliable final answer. Since C2R relies solely on
confidence scores derived from the model itself, it can be seamlessly
integrated with various existing QA models, demonstrating consistent
performance improvements across diverse models and benchmarks. Furthermore, we
provide essential yet underexplored insights into how leveraging sub-QAs
affects model behavior, specifically analyzing the impact of both the quantity
and quality of sub-QAs on achieving robust and reliable reasoning.

</details>


### [323] [Towards Atoms of Large Language Models](https://arxiv.org/abs/2509.20784)
*Chenhui Hu,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 本文提出大语言模型原子理论，定义内部表征基本单元为原子，通过实验验证其优势，为理解内部表征提供理论框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型内部表征基本单元未定义，现有神经元和特征存在问题，限制对其机制理解。

Method: 提出原子理论，引入原子内积纠正表征偏移，定义原子并证明其满足RIP条件，用阈值激活的单层稀疏自编码器识别原子。

Result: 在多个模型上训练阈值激活SAEs，平均实现99.9%稀疏重建，超99.8%原子满足唯一性条件，优于神经元和特征。

Conclusion: 系统引入并验证大语言模型原子理论，为理解内部表征提供理论框架和机制可解释性基础。

Abstract: The fundamental units of internal representations in large language models
(LLMs) remain undefined, limiting further understanding of their mechanisms.
Neurons or features are often regarded as such units, yet neurons suffer from
polysemy, while features face concerns of unreliable reconstruction and
instability. To address this issue, we propose the Atoms Theory, which defines
such units as atoms. We introduce the atomic inner product (AIP) to correct
representation shifting, formally define atoms, and prove the conditions that
atoms satisfy the Restricted Isometry Property (RIP), ensuring stable sparse
representations over atom set and linking to compressed sensing. Under stronger
conditions, we further establish the uniqueness and exact $\ell_1$
recoverability of the sparse representations, and provide guarantees that
single-layer sparse autoencoders (SAEs) with threshold activations can reliably
identify the atoms. To validate the Atoms Theory, we train threshold-activated
SAEs on Gemma2-2B, Gemma2-9B, and Llama3.1-8B, achieving 99.9% sparse
reconstruction across layers on average, and more than 99.8% of atoms satisfy
the uniqueness condition, compared to 0.5% for neurons and 68.2% for features,
showing that atoms more faithfully capture intrinsic representations of LLMs.
Scaling experiments further reveal the link between SAEs size and recovery
capacity. Overall, this work systematically introduces and validates Atoms
Theory of LLMs, providing a theoretical framework for understanding internal
representations and a foundation for mechanistic interpretability. Code
available at https://github.com/ChenhuiHu/towards_atoms.

</details>


### [324] [Leveraging What's Overfixed: Post-Correction via LLM Grammatical Error Overcorrection](https://arxiv.org/abs/2509.20811)
*Taehee Park,Heejin Do,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 提出PoCO方法平衡小语言模型和大语言模型在语法纠错中的召回率和准确率，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 小语言模型召回率低，大语言模型准确率低，需结合两者优势解决小语言模型召回挑战。

Method: PoCO先通过大语言模型过度纠错最大化召回率，再用微调小模型进行针对性后修正。

Result: PoCO有效平衡语法纠错性能，提高召回率同时保持有竞争力的准确率。

Conclusion: PoCO可提高语法纠错的整体质量。

Abstract: Robust supervised fine-tuned small Language Models (sLMs) often show high
reliability but tend to undercorrect. They achieve high precision at the cost
of low recall. Conversely, Large Language Models (LLMs) often show the opposite
tendency, making excessive overcorrection, leading to low precision. To
effectively harness the strengths of LLMs to address the recall challenges in
sLMs, we propose Post-Correction via Overcorrection (PoCO), a novel approach
that strategically balances recall and precision. PoCO first intentionally
triggers overcorrection via LLM to maximize recall by allowing comprehensive
revisions, then applies a targeted post-correction step via fine-tuning smaller
models to identify and refine erroneous outputs. We aim to harmonize both
aspects by leveraging the generative power of LLMs while preserving the
reliability of smaller supervised models. Our extensive experiments demonstrate
that PoCO effectively balances GEC performance by increasing recall with
competitive precision, ultimately improving the overall quality of grammatical
error correction.

</details>


### [325] [Speaker Style-Aware Phoneme Anchoring for Improved Cross-Lingual Speech Emotion Recognition](https://arxiv.org/abs/2509.20373)
*Shreya G. Upadhyay,Carlos Busso,Chi-Chun Lee*

Main category: cs.CL

TL;DR: 提出说话人风格感知音素锚定框架解决跨语言语音情感识别难题，在语料库评估中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 跨语言语音情感识别因语音变异性和说话人表达风格差异面临挑战，需能跨语言和说话人对齐情感表达的框架。

Method: 构建基于图聚类的特定情感说话人社区，在说话人和语音空间应用双空间锚定。

Result: 在MSP - Podcast和BIIC - Podcast语料库评估中，相比竞争基线有更好的泛化能力。

Conclusion: 该框架为跨语言情感表示共性提供了有价值的见解。

Abstract: Cross-lingual speech emotion recognition (SER) remains a challenging task due
to differences in phonetic variability and speaker-specific expressive styles
across languages. Effectively capturing emotion under such diverse conditions
requires a framework that can align the externalization of emotions across
different speakers and languages. To address this problem, we propose a
speaker-style aware phoneme anchoring framework that aligns emotional
expression at the phonetic and speaker levels. Our method builds
emotion-specific speaker communities via graph-based clustering to capture
shared speaker traits. Using these groups, we apply dual-space anchoring in
speaker and phonetic spaces to enable better emotion transfer across languages.
Evaluations on the MSP-Podcast (English) and BIIC-Podcast (Taiwanese Mandarin)
corpora demonstrate improved generalization over competitive baselines and
provide valuable insights into the commonalities in cross-lingual emotion
representation.

</details>


### [326] [Document Summarization with Conformal Importance Guarantees](https://arxiv.org/abs/2509.20461)
*Bruce Kuwahara,Chen-Yuan Lin,Xiao Shi Huang,Kin Kwan Leung,Jullian Arta Yapeter,Ilya Stanevich,Felipe Perez,Jesse C. Cresswell*

Main category: cs.CL

TL;DR: 提出Conformal Importance Summarization框架，用共形预测实现重要内容保留摘要生成，实验达理论信息覆盖率，可结合现有技术实现可靠可控自动摘要。


<details>
  <summary>Details</summary>
Motivation: 现有自动摘要系统在高风险领域缺乏关键内容包含的可靠保证。

Method: 引入Conformal Importance Summarization框架，通过校准句子级重要性分数阈值，实现提取式文档摘要，且与现有黑盒大语言模型集成。

Result: 在既定摘要基准测试中，Conformal Importance Summarization达到了理论上保证的信息覆盖率。

Conclusion: Conformal Importance Summarization可与现有技术结合，实现可靠、可控的自动摘要，为关键应用中AI摘要工具的安全部署铺平道路。

Abstract: Automatic summarization systems have advanced rapidly with large language
models (LLMs), yet they still lack reliable guarantees on inclusion of critical
content in high-stakes domains like healthcare, law, and finance. In this work,
we introduce Conformal Importance Summarization, the first framework for
importance-preserving summary generation which uses conformal prediction to
provide rigorous, distribution-free coverage guarantees. By calibrating
thresholds on sentence-level importance scores, we enable extractive document
summarization with user-specified coverage and recall rates over critical
content. Our method is model-agnostic, requires only a small calibration set,
and seamlessly integrates with existing black-box LLMs. Experiments on
established summarization benchmarks demonstrate that Conformal Importance
Summarization achieves the theoretically assured information coverage rate. Our
work suggests that Conformal Importance Summarization can be combined with
existing techniques to achieve reliable, controllable automatic summarization,
paving the way for safer deployment of AI summarization tools in critical
applications. Code is available at
https://github.com/layer6ai-labs/conformal-importance-summarization.

</details>


### [327] [Analysis of instruction-based LLMs' capabilities to score and judge text-input problems in an academic setting](https://arxiv.org/abs/2509.20982)
*Valeria Ramirez-Garcia,David de-Fitero-Dominguez,Antonio Garcia-Cabot,Eva Garcia-Lopez*

Main category: cs.CL

TL;DR: 研究LLM驱动的学术文本输入问题自动评估系统，提出五种评估系统并测试，发现参考辅助评估效果最佳，AI自动评估系统有潜力作学术补充工具。


<details>
  <summary>Details</summary>
Motivation: 研究LLM驱动的使用评分规则的学术文本输入问题自动评估系统。

Method: 提出五种评估系统，在含110个计算机科学答案的自定义数据集上用三种模型测试，并与人类评估结果对比。

Result: 参考辅助评估是用LLM自动评估和评分文本输入问题的最佳方法，其他方法有不同不足。

Conclusion: AI驱动的自动评估系统辅以适当方法，有潜力作为其他学术资源的补充工具。

Abstract: Large language models (LLMs) can act as evaluators, a role studied by methods
like LLM-as-a-Judge and fine-tuned judging LLMs. In the field of education,
LLMs have been studied as assistant tools for students and teachers. Our
research investigates LLM-driven automatic evaluation systems for academic
Text-Input Problems using rubrics. We propose five evaluation systems that have
been tested on a custom dataset of 110 answers about computer science from
higher education students with three models: JudgeLM, Llama-3.1-8B and
DeepSeek-R1-Distill-Llama-8B. The evaluation systems include: The JudgeLM
evaluation, which uses the model's single answer prompt to obtain a score;
Reference Aided Evaluation, which uses a correct answer as a guide aside from
the original context of the question; No Reference Evaluation, which ommits the
reference answer; Additive Evaluation, which uses atomic criteria; and Adaptive
Evaluation, which is an evaluation done with generated criteria fitted to each
question. All evaluation methods have been compared with the results of a human
evaluator. Results show that the best method to automatically evaluate and
score Text-Input Problems using LLMs is Reference Aided Evaluation. With the
lowest median absolute deviation (0.945) and the lowest root mean square
deviation (1.214) when compared to human evaluation, Reference Aided Evaluation
offers fair scoring as well as insightful and complete evaluations. Other
methods such as Additive and Adaptive Evaluation fail to provide good results
in concise answers, No Reference Evaluation lacks information needed to
correctly assess questions and JudgeLM Evaluations have not provided good
results due to the model's limitations. As a result, we conclude that
Artificial Intelligence-driven automatic evaluation systems, aided with proper
methodologies, show potential to work as complementary tools to other academic
resources.

</details>


### [328] [Generative AI for FFRDCs](https://arxiv.org/abs/2509.21040)
*Arun S. Maiya*

Main category: cs.CL

TL;DR: 本文展示大语言模型结合OnPrem.LLM框架可加速联邦资助研发中心文本分析，案例显示该方法能增强监督和战略分析。


<details>
  <summary>Details</summary>
Motivation: 联邦资助研发中心面临文本工作量大、手动分析慢的问题。

Method: 使用大语言模型，结合OnPrem.LLM开源框架进行文本分析。

Result: 在国防政策文件和科学语料库的案例研究中，该方法增强了监督和战略分析，同时保持了可审计性和数据主权。

Conclusion: 大语言模型结合OnPrem.LLM框架可有效加速文本分析，适用于敏感政府环境。

Abstract: Federally funded research and development centers (FFRDCs) face text-heavy
workloads, from policy documents to scientific and engineering papers, that are
slow to analyze manually. We show how large language models can accelerate
summarization, classification, extraction, and sense-making with only a few
input-output examples. To enable use in sensitive government contexts, we apply
OnPrem$.$LLM, an open-source framework for secure and flexible application of
generative AI. Case studies on defense policy documents and scientific corpora,
including the National Defense Authorization Act (NDAA) and National Science
Foundation (NSF) Awards, demonstrate how this approach enhances oversight and
strategic analysis while maintaining auditability and data sovereignty.

</details>


### [329] [Single Answer is Not Enough: On Generating Ranked Lists with Medical Reasoning Models](https://arxiv.org/abs/2509.20866)
*Pittawat Taveekitworachai,Natpatchara Pongjirapat,Krittaphas Chaisutyakorn,Piyalitt Ittichaiwong,Tossaporn Saengja,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 本文系统研究使医疗推理模型生成开放式问题的答案排序列表，提出提示和微调两种方法，发现强化微调训练的模型更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 临床决策需考虑多个选项，而当前医疗推理模型通常只生成一个答案，因此研究生成答案排序列表。

Method: 提出提示和微调两种方法，微调包括监督微调（SFT）和强化微调（RFT），并为RFT提出新奖励函数和进行消融研究。

Result: 一些SFT模型能泛化到特定答案格式，RFT训练的模型在多种格式上更鲁棒；案例研究表明模型虽可能选错基准答案但能识别有效答案。

Conclusion: 这是首次系统研究使医疗推理模型生成答案排序列表的方法，为开发医疗领域非单一答案的替代格式迈出第一步。

Abstract: This paper presents a systematic study on enabling medical reasoning models
(MRMs) to generate ranked lists of answers for open-ended questions. Clinical
decision-making rarely relies on a single answer but instead considers multiple
options, reducing the risks of narrow perspectives. Yet current MRMs are
typically trained to produce only one answer, even in open-ended settings. We
propose an alternative format: ranked lists and investigate two approaches:
prompting and fine-tuning. While prompting is a cost-effective way to steer an
MRM's response, not all MRMs generalize well across different answer formats:
choice, short text, and list answers. Based on our prompting findings, we train
and evaluate MRMs using supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT teaches a model to imitate annotated responses, and RFT
incentivizes exploration through the responses that maximize a reward. We
propose new reward functions targeted at ranked-list answer formats, and
conduct ablation studies for RFT. Our results show that while some SFT models
generalize to certain answer formats, models trained with RFT are more robust
across multiple formats. We also present a case study on a modified MedQA with
multiple valid answers, finding that although MRMs might fail to select the
benchmark's preferred ground truth, they can recognize valid answers. To the
best of our knowledge, this is the first systematic investigation of approaches
for enabling MRMs to generate answers as ranked lists. We hope this work
provides a first step toward developing alternative answer formats that are
beneficial beyond single answers in medical domains.

</details>


### [330] [Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs](https://arxiv.org/abs/2509.21080)
*Yixin Wan,Xingrun Chen,Kai-Wei Chang*

Main category: cs.CL

TL;DR: 研究大语言模型存在文化定位偏差，提出CultureLens基准和评估指标，揭示偏差模式，提出两种推理时缓解方法，证明基于代理的方法有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成任务中可能存在与文化相关的公平性问题，需要识别和研究文化定位偏差。

Method: 提出CultureLens基准和3个评估指标，设计文化情境访谈脚本生成任务；提出基于提示的FIP方法和基于代理的MFA框架。

Result: 5个最先进的大语言模型在生成美国情境脚本时多采用内部人语气，对其他文化多采用外部人立场；基于代理的方法能有效缓解偏差。

Conclusion: 基于代理的方法是缓解生成式大语言模型偏差的有前景方向。

Abstract: Large language models (LLMs) have unlocked a wide range of downstream
generative applications. However, we found that they also risk perpetuating
subtle fairness issues tied to culture, positioning their generations from the
perspectives of the mainstream US culture while demonstrating salient
externality towards non-mainstream ones. In this work, we identify and
systematically investigate this novel culture positioning bias, in which an
LLM's default generative stance aligns with a mainstream view and treats other
cultures as outsiders. We propose the CultureLens benchmark with 4000
generation prompts and 3 evaluation metrics for quantifying this bias through
the lens of a culturally situated interview script generation task, in which an
LLM is positioned as an onsite reporter interviewing local people across 10
diverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a
stark pattern: while models adopt insider tones in over 88 percent of
US-contexted scripts on average, they disproportionately adopt mainly outsider
stances for less dominant cultures. To resolve these biases, we propose 2
inference-time mitigation methods: a baseline prompt-based Fairness
Intervention Pillars (FIP) method, and a structured Mitigation via Fairness
Agents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent)
introduces a self-reflection and rewriting loop based on fairness guidelines.
(2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized
agents: a Planner Agent(initial script generation), a Critique Agent (evaluates
initial script against fairness pillars), and a Refinement Agent (incorporates
feedback to produce a polished, unbiased script). Empirical results showcase
the effectiveness of agent-based methods as a promising direction for
mitigating biases in generative LLMs.

</details>


### [331] [Behind RoPE: How Does Causal Mask Encode Positional Information?](https://arxiv.org/abs/2509.21042)
*Junu Kim,Xiao Liu,Zhenghao Lin,Lei Ji,Yeyun Gong,Edward Choi*

Main category: cs.CL

TL;DR: 研究证明因果掩码能在无参数或因果依赖输入时诱导注意力分数产生位置依赖模式，且与RoPE交互会扭曲其相对注意力分数模式，表明应将因果掩码视为位置信息来源。


<details>
  <summary>Details</summary>
Motivation: 探究因果掩码在Transformer解码器中作为位置信息来源的作用。

Method: 理论分析因果掩码对注意力分数的影响，进行实证分析验证训练模型的表现。

Result: 因果掩码能诱导位置依赖模式，倾向于关注相邻查询 - 键对；训练模型有相同表现，参数会放大模式；因果掩码与RoPE交互会扭曲RoPE相对注意力分数模式。

Conclusion: 应将因果掩码与显式位置编码一同视为位置信息的重要来源。

Abstract: While explicit positional encodings such as RoPE are a primary source of
positional information in Transformer decoders, the causal mask also provides
positional information. In this work, we prove that the causal mask can induce
position-dependent patterns in attention scores, even without parameters or
causal dependency in the input. Our theoretical analysis indicates that the
induced attention pattern tends to favor nearby query-key pairs, mirroring the
behavior of common positional encodings. Empirical analysis confirms that
trained models exhibit the same behavior, with learned parameters further
amplifying these patterns. Notably, we found that the interaction of causal
mask and RoPE distorts RoPE's relative attention score patterns into
non-relative ones. We consistently observed this effect in modern large
language models, suggesting the importance of considering the causal mask as a
source of positional information alongside explicit positional encodings.

</details>


### [332] [Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning](https://arxiv.org/abs/2509.21193)
*Xiangru Tang,Wanghan Xu,Yujie Wang,Zijie Guo,Daniel Shao,Jiapeng Chen,Cixuan Zhang,Ziyi Wang,Lixin Zhang,Guancheng Wan,Wenlong Zhang,Lei Bai,Zhenfei Yin,Philip Torr,Hanrui Wang,Di Jin*

Main category: cs.CL

TL;DR: 提出结合隐式检索和结构化协作的统一框架，在HLE Bio/Chem Gold上准确率达48.3%，减少令牌使用和代理步骤，跨领域结果稳健，揭示推理失败、知识差距及多样性特点。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型科学推理中显式检索碎片化推理和多智能体管道稀释强解决方案的问题。

Method: 构建统一框架，包含基于监控的检索模块、分层解决方案细化（HSR）和质量感知迭代推理（QAIR）。

Result: 在HLE Bio/Chem Gold上准确率最高，减少令牌使用和代理步骤，跨领域结果稳健，误差和多样性分析有新发现。

Conclusion: 隐式增强和结构化细化克服了显式工具使用和统一聚合的低效性。

Abstract: Large language models (LLMs) have recently shown strong progress on
scientific reasoning, yet two major bottlenecks remain. First, explicit
retrieval fragments reasoning, imposing a hidden "tool tax" of extra tokens and
steps. Second, multi-agent pipelines often dilute strong solutions by averaging
across all candidates. We address these challenges with a unified framework
that combines implicit retrieval and structured collaboration. At its
foundation, a Monitor-based retrieval module operates at the token level,
integrating external knowledge with minimal disruption to reasoning. On top of
this substrate, Hierarchical Solution Refinement (HSR) iteratively designates
each candidate as an anchor to be repaired by its peers, while Quality-Aware
Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's
Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\% accuracy -- the
highest reported to date, surpassing the strongest agent baseline by 13.4
points and leading frontier LLMs by up to 18.1 points, while simultaneously
reducing token usage by 53.5\% and agent steps by 43.7\%. Results on SuperGPQA
and TRQA confirm robustness across domains. Error analysis shows that reasoning
failures and knowledge gaps co-occur in over 85\% of cases, while diversity
analysis reveals a clear dichotomy: retrieval tasks benefit from solution
variety, whereas reasoning tasks favor consensus. Together, these findings
demonstrate how implicit augmentation and structured refinement overcome the
inefficiencies of explicit tool use and uniform aggregation. Code is available
at: https://github.com/tangxiangru/Eigen-1.

</details>


### [333] [RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards](https://arxiv.org/abs/2509.21319)
*Zhilin Wang,Jiaqi Zeng,Olivier Delalleau,Ellie Evans,Daniel Egert,Hoo-Chang Shin,Felipe Soares,Yi Dong,Oleksii Kuchaiev*

Main category: cs.CL

TL;DR: 提出RLBFF结合人类反馈与规则验证，训练的奖励模型表现优，还给出开源方案对齐Qwen3 - 32B。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF存在可解释性和奖励破解问题，RLVR受限于正确性验证器，需新方法结合两者优势。

Method: 从自然语言反馈提取二元原则，将奖励模型训练作为蕴含任务。

Result: 训练的奖励模型优于Bradley - Terry模型，在RM - Bench和JudgeBench表现好；可定制奖励模型；开源方案对齐Qwen3 - 32B性能佳、成本低。

Conclusion: RLBFF有效结合人类反馈和规则验证，在奖励模型训练和模型对齐上有良好效果。

Abstract: Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM
post-training, each offering distinct advantages. However, RLHF struggles with
interpretability and reward hacking because it relies on human judgments that
usually lack explicit criteria, whereas RLVR is limited in scope by its focus
on correctness-based verifiers. We propose Reinforcement Learning with Binary
Flexible Feedback (RLBFF), which combines the versatility of human-driven
preferences with the precision of rule-based verification, enabling reward
models to capture nuanced aspects of response quality beyond mere correctness.
RLBFF extracts principles that can be answered in a binary fashion (e.g.
accuracy of information: yes, or code readability: no) from natural language
feedback. Such principles can then be used to ground Reward Model training as
an entailment task (response satisfies or does not satisfy an arbitrary
principle). We show that Reward Models trained in this manner can outperform
Bradley-Terry models when matched for data and achieve top performance on
RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,
2025). Additionally, users can specify principles of interest at inference time
to customize the focus of our reward models, in contrast to Bradley-Terry
models. Finally, we present a fully open source recipe (including data) to
align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the
performance of o3-mini and DeepSeek R1 on general alignment benchmarks of
MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).

</details>


### [334] [DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding](https://arxiv.org/abs/2509.21287)
*Kin Ian Lo,Hala Hawashin,Mina Abbaszadeh,Tilen Limback-Stokin,Hadi Wazni,Mehrnoosh Sadrzadeh*

Main category: cs.CL

TL;DR: 现有视觉语言模型忽视语言组合结构，本文提出DisCoCLIP，结合CLIP视觉transformer和新文本编码器，通过张量网络编码句法结构，提升组合推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型常忽略语言组合结构，在依赖词序和谓词 - 论元结构的任务上失败。

Method: 提出DisCoCLIP，结合冻结的CLIP视觉transformer和新颖的张量网络文本编码器，用组合范畴语法解析句子，对高阶张量进行分解，以自监督对比损失端到端训练。

Result: 提高了对动词语义和词序的敏感度，提升了SVO - Probes动词准确率、ARO归因和关系分数，在新引入的SVO - Swap基准测试中取得93.7%的成绩。

Conclusion: 通过张量网络嵌入显式语言结构可产生可解释、参数高效的表示，显著提高视觉语言任务中的组合推理能力。

Abstract: Recent vision-language models excel at large-scale image-text alignment but
often neglect the compositional structure of language, leading to failures on
tasks that hinge on word order and predicate-argument structure. We introduce
DisCoCLIP, a multimodal encoder that combines a frozen CLIP vision transformer
with a novel tensor network text encoder that explicitly encodes syntactic
structure. Sentences are parsed with a Combinatory Categorial Grammar parser to
yield distributional word tensors whose contractions mirror the sentence's
grammatical derivation. To keep the model efficient, high-order tensors are
factorized with tensor decompositions, reducing parameter count from tens of
millions to under one million. Trained end-to-end with a self-supervised
contrastive loss, DisCoCLIP markedly improves sensitivity to verb semantics and
word order: it raises CLIP's SVO-Probes verb accuracy from 77.6% to 82.4%,
boosts ARO attribution and relation scores by over 9% and 4%, and achieves
93.7% on a newly introduced SVO-Swap benchmark. These results demonstrate that
embedding explicit linguistic structure via tensor networks yields
interpretable, parameter-efficient representations that substantially improve
compositional reasoning in vision-language tasks.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [335] [Implicit Augmentation from Distributional Symmetry in Turbulence Super-Resolution](https://arxiv.org/abs/2509.20683)
*Julia Balla,Jeremiah Bailey,Ali Backour,Elyssa Hofgard,Tommi Jaakkola,Tess Smidt,Ryley McConkey*

Main category: physics.flu-dyn

TL;DR: 研究用机器学习超分辨率模拟湍流，指出标准CNN可部分获得旋转对称性，分析不同训练数据和采样对误差的影响，明确何时需显式引入对称性。


<details>
  <summary>Details</summary>
Motivation: 湍流模拟计算成本高，需用机器学习超分辨率模拟，挑战是让模型尊重物理对称性如旋转等变性。

Method: 使用3D通道流子域，对比不同各向异性数据（中平面和边界层数据）训练的模型，研究不同时空采样的影响。

Result: 在更各向同性的中平面数据上训练的模型等变性误差更低，更多时空采样可降低误差，等变性误差有尺度依赖性且符合科尔莫戈罗夫局部各向同性假设。

Conclusion: 明确何时需显式将旋转对称性纳入学习算法，何时可直接从湍流中获得，实现更高效和有对称性意识的超分辨率。

Abstract: The immense computational cost of simulating turbulence has motivated the use
of machine learning approaches for super-resolving turbulent flows. A central
challenge is ensuring that learned models respect physical symmetries, such as
rotational equivariance. We show that standard convolutional neural networks
(CNNs) can partially acquire this symmetry without explicit augmentation or
specialized architectures, as turbulence itself provides implicit rotational
augmentation in both time and space. Using 3D channel-flow subdomains with
differing anisotropy, we find that models trained on more isotropic mid-plane
data achieve lower equivariance error than those trained on boundary layer
data, and that greater temporal or spatial sampling further reduces this error.
We show a distinct scale-dependence of equivariance error that occurs
regardless of dataset anisotropy that is consistent with Kolmogorov's local
isotropy hypothesis. These results clarify when rotational symmetry must be
explicitly incorporated into learning algorithms and when it can be obtained
directly from turbulence, enabling more efficient and symmetry-aware
super-resolution.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [336] [Structuring Collective Action with LLM-Guided Evolution: From Ill-Structured Problems to Executable Heuristics](https://arxiv.org/abs/2509.20412)
*Kevin Bradley Dsouza,Graham Alexander Watt,Yuri Leonenko,Juan Moreno-Cruz*

Main category: cs.MA

TL;DR: 提出ECHO - MIMIC计算框架将集体行动的复杂问题转化为可处理问题，在农业景观管理中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 集体行动问题属于结构不良问题，个体行动与全局结果因果关系不明、利益相关者目标冲突且缺乏算法连接微观与宏观福利，需解决该难题。

Method: ECHO - MIMIC框架分两阶段，ECHO进化编码行为策略的Python代码片段，MIMIC进化激励代理采用策略的自然语言消息，均使用大语言模型驱动的进化搜索。

Result: 在农业景观管理的典型结构不良问题中，ECHO - MIMIC发现的启发式方法表现优于基线，定制消息能使模拟农民行为与景观生态目标一致。

Conclusion: ECHO - MIMIC将算法规则发现与定制沟通结合，把集体行动认知负担转化为简单代理级指令，使结构不良问题可实际解决，为可扩展、自适应政策设计开辟新路径。

Abstract: Collective action problems, which require aligning individual incentives with
collective goals, are classic examples of Ill-Structured Problems (ISPs). For
an individual agent, the causal links between local actions and global outcomes
are unclear, stakeholder objectives often conflict, and no single, clear
algorithm can bridge micro-level choices with macro-level welfare. We present
ECHO-MIMIC, a computational framework that converts this global complexity into
a tractable, Well-Structured Problem (WSP) for each agent by discovering
compact, executable heuristics and persuasive rationales. The framework
operates in two stages: ECHO (Evolutionary Crafting of Heuristics from
Outcomes) evolves snippets of Python code that encode candidate behavioral
policies, while MIMIC (Mechanism Inference & Messaging for
Individual-to-Collective Alignment) evolves companion natural language messages
that motivate agents to adopt those policies. Both phases employ a
large-language-model-driven evolutionary search: the LLM proposes diverse and
context-aware code or text variants, while population-level selection retains
those that maximize collective performance in a simulated environment. We
demonstrate this framework on a canonical ISP in agricultural landscape
management, where local farming decisions impact global ecological
connectivity. Results show that ECHO-MIMIC discovers high-performing heuristics
compared to baselines and crafts tailored messages that successfully align
simulated farmer behavior with landscape-level ecological goals. By coupling
algorithmic rule discovery with tailored communication, ECHO-MIMIC transforms
the cognitive burden of collective action into a simple set of agent-level
instructions, making previously ill-structured problems solvable in practice
and opening a new path toward scalable, adaptive policy design.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [337] [Dual-Language General-Purpose Self-Hosted Visual Language and new Textual Programming Language for Applications](https://arxiv.org/abs/2509.20426)
*Mahmoud Samir Fayed*

Main category: cs.PL

TL;DR: 本文设计开发了通用自托管可视化编程语言PWCT2，先设计文本编程语言Ring用于其开发，用PWCT开发Ring发现问题后用Ring开发PWCT2，PWCT2性能提升，通过Steam分发获积极反馈。


<details>
  <summary>Details</summary>
Motivation: 现有通用可视化编程语言依赖文本编程改进，需开发更好的通用自托管可视化编程语言。

Method: 先设计文本编程语言Ring，用PWCT开发Ring，发现问题后用Ring开发PWCT2。

Result: PWCT2代码生成速度快约36倍，可视源文件存储需求少20倍，能实现代码转换，由约92000行Ring代码和394个可视化组件构成，在Steam获积极反馈。

Conclusion: PWCT2表现良好，鼓励进一步研究开发。

Abstract: Most visual programming languages (VPLs) are domain-specific, with few
general-purpose VPLs like Programming Without Coding Technology (PWCT). These
general-purpose VPLs are developed using textual programming languages and
improving them requires textual programming. In this thesis, we designed and
developed PWCT2, a dual-language (Arabic/English), general-purpose,
self-hosting visual programming language. Before doing so, we specifically
designed a textual programming language called Ring for its development. Ring
is a dynamically typed language with a lightweight implementation, offering
syntax customization features. It permits the creation of domain-specific
languages through new features that extend object-oriented programming,
allowing for specialized languages resembling Cascading Style Sheets (CSS) or
Supernova language. The Ring Compiler and Virtual Machine are designed using
the PWCT visual programming language where the visual implementation is
composed of 18,945 components that generate 24,743 lines of C code, which
increases the abstraction level and hides unnecessary details. Using PWCT to
develop Ring allowed us to realize several issues in PWCT, which led to the
development of the PWCT2 visual programming language using the Ring textual
programming language. PWCT2 provides approximately 36 times faster code
generation and requires 20 times less storage for visual source files. It also
allows for the conversion of Ring code into visual code, enabling the creation
of a self-hosting VPL that can be developed using itself. PWCT2 consists of
approximately 92,000 lines of Ring code and comes with 394 visual components.
PWCT2 is distributed to many users through the Steam platform and has received
positive feedback, On Steam, 1772 users have launched the software, and the
total recorded usage time exceeds 17,000 hours, encouraging further research
and development.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [338] [Data-Efficient ASR Personalization for Non-Normative Speech Using an Uncertainty-Based Phoneme Difficulty Score for Guided Sampling](https://arxiv.org/abs/2509.20396)
*Niclas Pokel,Pehuén Moure,Roman Boehringer,Yingqiang Gao*

Main category: eess.AS

TL;DR: 本文提出数据高效的个性化方法，通过量化音素级不确定性指导微调，提升自动语音识别（ASR）系统对非规范语音的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有ASR系统处理因脑瘫或结构异常等导致的非规范语音时，高声学变异性和训练数据稀缺严重降低模型性能。

Method: 引入量化音素级不确定性的数据高效个性化方法，利用蒙特卡罗丢弃法估计模型最难识别的音素，并采用针对性过采样策略。

Result: 在英语和德语数据集上验证，模型衍生的不确定性与专家临床语言治疗报告中确定的具有挑战性的音素高度相关，不确定性引导采样显著提高了ASR准确性。

Conclusion: 该临床验证的、不确定性引导的采样方法为个性化和包容性ASR提供了实用框架。

Abstract: Automatic speech recognition (ASR) systems struggle with non-normative speech
from individuals with impairments caused by conditions like cerebral palsy or
structural anomalies. The high acoustic variability and scarcity of training
data severely degrade model performance. This work introduces a data-efficient
personalization method that quantifies phoneme-level uncertainty to guide
fine-tuning. We leverage Monte Carlo Dropout to estimate which phonemes a model
finds most difficult and use these estimates for a targeted oversampling
strategy. We validate our method on English and German datasets. Crucially, we
demonstrate that our model-derived uncertainty strongly correlates with
phonemes identified as challenging in an expert clinical logopedic report,
marking, to our knowledge, the first work to successfully align model
uncertainty with expert assessment of speech difficulty. Our results show that
this clinically-validated, uncertainty-guided sampling significantly improves
ASR accuracy, delivering a practical framework for personalized and inclusive
ASR.

</details>


### [339] [Variational Low-Rank Adaptation for Personalized Impaired Speech Recognition](https://arxiv.org/abs/2509.20397)
*Niclas Pokel,Pehuén Moure,Roman Boehringer,Shih-Chii Liu,Yingqiang Gao*

Main category: eess.AS

TL;DR: 现有ASR系统处理非规范语音存在困难，本文提出基于贝叶斯低秩自适应的ASR个性化方法，在两个数据集验证，显著提升识别准确率并保证数据和标注效率。


<details>
  <summary>Details</summary>
Motivation: 先天性疾病或后天脑损伤导致的言语障碍给ASR系统带来挑战，现有模型因训练数据有限和声学变异性大而表现不佳，且收集和标注非规范语音困难。

Method: 引入基于贝叶斯低秩自适应的ASR个性化方法进行数据高效微调。

Result: 在英语UA - Speech数据集和新收集的德语BF - Sprache数据集验证，显著提高受损语音的ASR准确率。

Conclusion: 该方法为实现包容性ASR提供了实用途径。

Abstract: Speech impairments resulting from congenital disorders, such as cerebral
palsy, down syndrome, or apert syndrome, as well as acquired brain injuries due
to stroke, traumatic accidents, or tumors, present major challenges to
automatic speech recognition (ASR) systems. Despite recent advancements,
state-of-the-art ASR models like Whisper still struggle with non-normative
speech due to limited training data availability and high acoustic variability.
Moreover, collecting and annotating non-normative speech is burdensome:
speaking is effortful for many affected individuals, while laborious annotation
often requires caregivers familiar with the speaker. This work introduces a
novel ASR personalization method based on Bayesian Low-rank Adaptation for
data-efficient fine-tuning. We validate our method on the English UA-Speech
dataset and a newly collected German speech dataset, BF-Sprache, from a child
with structural speech impairment. The dataset and approach are designed to
reflect the challenges of low-resource settings that include individuals with
speech impairments. Our method significantly improves ASR accuracy for impaired
speech while maintaining data and annotation efficiency, offering a practical
path toward inclusive ASR.

</details>


### [340] [Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens](https://arxiv.org/abs/2509.20485)
*Ismail Rasim Ulgen,Zongyang Du,Junchen Lu,Philipp Koehn,Berrak Sisman*

Main category: eess.AS

TL;DR: 为解决现有合成语音评估指标局限，提出参考无关的TTScore框架，实验表明其比现有指标更能反映人类判断。


<details>
  <summary>Details</summary>
Motivation: 现有合成语音可懂度和韵律评估指标有局限，与人类感知相关性弱。

Method: 提出TTScore框架，用两个基于输入文本的序列到序列预测器，分别通过内容和韵律标记计算可懂度和韵律得分。

Result: 在SOMOS、VoiceMOS和TTSArena基准测试中，TTScore能进行可靠的特定方面评估，与人类整体质量判断的相关性更强。

Conclusion: TTScore是比现有指标更优的合成语音评估框架。

Abstract: Objective evaluation of synthesized speech is critical for advancing speech
generation systems, yet existing metrics for intelligibility and prosody remain
limited in scope and weakly correlated with human perception. Word Error Rate
(WER) provides only a coarse text-based measure of intelligibility, while
F0-RMSE and related pitch-based metrics offer a narrow, reference-dependent
view of prosody. To address these limitations, we propose TTScore, a targeted
and reference-free evaluation framework based on conditional prediction of
discrete speech tokens. TTScore employs two sequence-to-sequence predictors
conditioned on input text: TTScore-int, which measures intelligibility through
content tokens, and TTScore-pro, which evaluates prosody through prosody
tokens. For each synthesized utterance, the predictors compute the likelihood
of the corresponding token sequences, yielding interpretable scores that
capture alignment with intended linguistic content and prosodic structure.
Experiments on the SOMOS, VoiceMOS, and TTSArena benchmarks demonstrate that
TTScore-int and TTScore-pro provide reliable, aspect-specific evaluation and
achieve stronger correlations with human judgments of overall quality than
existing intelligibility and prosody-focused metrics.

</details>


### [341] [Real-Time System for Audio-Visual Target Speech Enhancement](https://arxiv.org/abs/2509.20741)
*T. Aleksandra Ma,Sile Yin,Li-Chia Yang,Shuo Zhang*

Main category: eess.AS

TL;DR: 展示RAVEN实时视听语音增强系统的现场演示，该系统可在CPU上运行。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏能在CPU硬件上运行的实时视听语音增强交互式系统，RAVEN旨在填补这一空白。

Method: 使用视听语音识别模型的预训练视觉嵌入来编码唇动信息。

Result: 系统能在环境噪声、干扰说话者、瞬态声音和歌声等场景中实现目标语音增强。

Conclusion: 通过麦克风和网络摄像头设置，观众可现场体验视听目标语音增强，并通过耳机听到清晰语音。

Abstract: We present a live demonstration for RAVEN, a real-time audio-visual speech
enhancement system designed to run entirely on a CPU. In single-channel,
audio-only settings, speech enhancement is traditionally approached as the task
of extracting clean speech from environmental noise. More recent work has
explored the use of visual cues, such as lip movements, to improve robustness,
particularly in the presence of interfering speakers. However, to our
knowledge, no prior work has demonstrated an interactive system for real-time
audio-visual speech enhancement operating on CPU hardware. RAVEN fills this gap
by using pretrained visual embeddings from an audio-visual speech recognition
model to encode lip movement information. The system generalizes across
environmental noise, interfering speakers, transient sounds, and even singing
voices. In this demonstration, attendees will be able to experience live
audio-visual target speech enhancement using a microphone and webcam setup,
with clean speech playback through headphones.

</details>


### [342] [Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?](https://arxiv.org/abs/2509.21087)
*Rostislav Makarov,Lea Schönherr,Timo Gerkmann*

Main category: eess.AS

TL;DR: 论文指出语音增强机器学习方法存在易受对抗攻击的漏洞，实验验证当代模型可被操纵，同时表明扩散模型有内在鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究语音增强机器学习方法表达能力提升带来的潜在漏洞。

Method: 验证精心构造的对抗性噪声注入可使增强语音输出语义改变，并测试当代模型及扩散模型的情况。

Result: 当代预测性语音增强模型能被操纵，扩散模型对对抗攻击有内在鲁棒性。

Conclusion: 语音增强模型存在对抗攻击漏洞，扩散模型设计上有应对优势。

Abstract: Machine learning approaches for speech enhancement are becoming increasingly
expressive, enabling ever more powerful modifications of input signals. In this
paper, we demonstrate that this expressiveness introduces a vulnerability:
advanced speech enhancement models can be susceptible to adversarial attacks.
Specifically, we show that adversarial noise, carefully crafted and
psychoacoustically masked by the original input, can be injected such that the
enhanced speech output conveys an entirely different semantic meaning. We
experimentally verify that contemporary predictive speech enhancement models
can indeed be manipulated in this way. Furthermore, we highlight that diffusion
models with stochastic samplers exhibit inherent robustness to such adversarial
attacks by design.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [343] [SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent](https://arxiv.org/abs/2509.20414)
*Yandan Yang,Baoxiong Jia,Shujie Zhang,Siyuan Huang*

Main category: cs.GR

TL;DR: 提出SceneWeaver框架用于室内场景合成，在多指标上表现出色且能有效泛化。


<details>
  <summary>Details</summary>
Motivation: 现有室内场景合成方法存在局限，如受固定场景类别约束、缺乏物体级细节和物理一致性、难以与复杂用户指令对齐，因此需要新方法。

Method: 提出SceneWeaver框架，用基于语言模型的规划器从一系列可扩展场景生成工具中选择，通过自我评估进行闭环的推理 - 行动 - 反思设计。

Result: 在常见和开放词汇的房间类型上的大量实验表明，SceneWeaver在物理、视觉和语义指标上优于先前方法。

Conclusion: SceneWeaver朝着通用3D环境生成迈出了一步。

Abstract: Indoor scene synthesis has become increasingly important with the rise of
Embodied AI, which requires 3D environments that are not only visually
realistic but also physically plausible and functionally diverse. While recent
approaches have advanced visual fidelity, they often remain constrained to
fixed scene categories, lack sufficient object-level detail and physical
consistency, and struggle to align with complex user instructions. In this
work, we present SceneWeaver, a reflective agentic framework that unifies
diverse scene synthesis paradigms through tool-based iterative refinement. At
its core, SceneWeaver employs a language model-based planner to select from a
suite of extensible scene generation tools, ranging from data-driven generative
models to visual- and LLM-based methods, guided by self-evaluation of physical
plausibility, visual realism, and semantic alignment with user input. This
closed-loop reason-act-reflect design enables the agent to identify semantic
inconsistencies, invoke targeted tools, and update the environment over
successive iterations. Extensive experiments on both common and open-vocabulary
room types demonstrate that SceneWeaver not only outperforms prior methods on
physical, visual, and semantic metrics, but also generalizes effectively to
complex scenes with diverse instructions, marking a step toward general-purpose
3D environment generation. Project website: https://scene-weaver.github.io/.

</details>


### [344] [Marching Neurons: Accurate Surface Extraction for Neural Implicit Shapes](https://arxiv.org/abs/2509.21007)
*Christian Stippel,Felix Mujkanovic,Thomas Leimkühler,Pedro Hermosilla*

Main category: cs.GR

TL;DR: 提出从神经隐式函数中解析提取表面的新方法，能高效准确地生成网格。


<details>
  <summary>Details</summary>
Motivation: 准确的表面几何表示在3D视觉计算中很重要，传统隐式表示的表面提取方法有因固定有限分辨率导致的不准确问题，需要高效转换显式与隐式表示。

Method: 基于每个神经元对域的划分，开发深度优先遍历策略以高效追踪编码表面，方法原生并行且可处理大型神经架构。

Result: 生成的网格能忠实捕捉网络的完整几何信息，在不同形状和网络架构上达到前所未有的精度，且速度有竞争力。

Conclusion: 新方法能无特别的空间离散化，高效准确地从神经隐式函数中提取表面。

Abstract: Accurate surface geometry representation is crucial in 3D visual computing.
Explicit representations, such as polygonal meshes, and implicit
representations, like signed distance functions, each have distinct advantages,
making efficient conversions between them increasingly important. Conventional
surface extraction methods for implicit representations, such as the widely
used Marching Cubes algorithm, rely on spatial decomposition and sampling,
leading to inaccuracies due to fixed and limited resolution. We introduce a
novel approach for analytically extracting surfaces from neural implicit
functions. Our method operates natively in parallel and can navigate large
neural architectures. By leveraging the fact that each neuron partitions the
domain, we develop a depth-first traversal strategy to efficiently track the
encoded surface. The resulting meshes faithfully capture the full geometric
information from the network without ad-hoc spatial discretization, achieving
unprecedented accuracy across diverse shapes and network architectures while
maintaining competitive speed.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [345] [Reverse Faà di Bruno's Formula for Cartesian Reverse Differential Categories](https://arxiv.org/abs/2509.20931)
*Aaron Biggin,Jean-Simon Pacaud Lemay*

Main category: cs.LO

TL;DR: 提出笛卡尔反向微分范畴中Faa di Bruno公式的反向微分类似物，即高阶反向链式法则，并定义了偏反向导数和高阶反向导数。


<details>
  <summary>Details</summary>
Motivation: 在笛卡尔反向微分范畴中给出高阶反向链式法则。

Method: 在笛卡尔反向微分范畴中定义偏反向导数和高阶反向导数。

Result: 得到Faa di Bruno公式的反向微分类似物。

Conclusion: 在笛卡尔反向微分范畴中成功构建高阶反向链式法则。

Abstract: Reverse differentiation is an essential operation for automatic
differentiation. Cartesian reverse differential categories axiomatize reverse
differentiation in a categorical framework, where one of the primary axioms is
the reverse chain rule, which is the formula that expresses the reverse
derivative of a composition. Here, we present the reverse differential analogue
of Faa di Bruno's Formula, which gives a higher-order reverse chain rule in a
Cartesian reverse differential category. To properly do so, we also define
partial reverse derivatives and higher-order reverse derivatives in a Cartesian
reverse differential category.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [346] [Incorporating External Controls for Estimating the Average Treatment Effect on the Treated with High-Dimensional Data: Retaining Double Robustness and Ensuring Double Safety](https://arxiv.org/abs/2509.20586)
*Chi-Shian Dai,Chao Ying,Yang Ning,Jiwei Zhao*

Main category: stat.ME

TL;DR: 本文探讨含外部对照数据时ATT估计问题，提出新的双重稳健估计量提高效率，经模拟和真实数据验证有效。


<details>
  <summary>Details</summary>
Motivation: 在有外部对照数据的情况下，将其纳入标准双重稳健估计量可能降低效率，需解决此问题。

Method: 提出一种新的双重稳健估计量，即使模型设定错误也能保证比无外部对照的标准方法更有效率。

Result: 新估计量在模型正确设定时与含外部对照的标准双重稳健估计量一致且达到半参数效率，渐近理论适用于高维混杂因素场景。

Conclusion: 通过模拟研究和真实数据应用证明了新方法的有效性。

Abstract: Randomized controlled trials (RCTs) are widely regarded as the gold standard
for causal inference in biomedical research. For instance, when estimating the
average treatment effect on the treated (ATT), a doubly robust estimation
procedure can be applied, requiring either the propensity score model or the
control outcome model to be correctly specified. In this paper, we address
scenarios where external control data, often with a much larger sample size,
are available. Such data are typically easier to obtain from historical records
or third-party sources. However, we find that incorporating external controls
into the standard doubly robust estimator for ATT may paradoxically result in
reduced efficiency compared to using the estimator without external controls.
This counterintuitive outcome suggests that the naive incorporation of external
controls could be detrimental to estimation efficiency. To resolve this issue,
we propose a novel doubly robust estimator that guarantees higher efficiency
than the standard approach without external controls, even under model
misspecification. When all models are correctly specified, this estimator
aligns with the standard doubly robust estimator that incorporates external
controls and achieves semiparametric efficiency. The asymptotic theory
developed in this work applies to high-dimensional confounder settings, which
are increasingly common with the growing prevalence of electronic health record
data. We demonstrate the effectiveness of our methodology through extensive
simulation studies and a real-world data application.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [347] [Detecting gene-environment interactions to guide personalized intervention: boosting distributional regression for polygenic scores](https://arxiv.org/abs/2509.20850)
*Qiong Wu,Hannah Klinkhammer,Kiran Kunwar,Christian Staerk,Carlo Maj,Andreas Mayr*

Main category: stat.AP

TL;DR: 提出snpboostlss算法构建表型方差的多基因风险评分，验证了基因 - 环境相互作用，该评分有潜力识别从环境变化中获益更多的个体。


<details>
  <summary>Details</summary>
Motivation: 现有多基因风险评分方法主要关注表型均值而忽略方差，与表型方差相关的遗传变异对基因 - 环境相互作用研究有重要意义。

Method: 提出snpboostlss算法，一种用于高斯位置 - 尺度模型的循环梯度提升算法，在每个提升步骤只考虑一批最相关的变异以提高计算效率。

Result: 在UK Biobank队列中验证了他汀类药物使用与表型方差的多基因风险评分之间的相互作用；应用于体重指数时，新构建的表型方差多基因风险评分与身体活动和久坐行为有显著相互作用。

Conclusion: snpboostlss导出的表型方差多基因风险评分有潜力识别能从环境变化（如医疗干预和生活方式改变）中更多获益的个体。

Abstract: Polygenic risk scores can be used to model the individual genetic liability
for human traits. Current methods primarily focus on modeling the mean of a
phenotype neglecting the variance. However, genetic variants associated with
phenotypic variance can provide important insights to gene-environment
interaction studies. To overcome this, we propose snpboostlss, a cyclical
gradient boosting algorithm for a Gaussian location-scale model to jointly
derive sparse polygenic models for both the mean and the variance of a
quantitative phenotype. To improve computational efficiency on high-dimensional
and large-scale genotype data (large n and large p), we only consider a batch
of most relevant variants in each boosting step. We investigate the effect of
statins therapy (the environmental factor) on low-density lipoprotein in the UK
Biobank cohort using the new snpboostlss algorithm. We are able to verify the
interaction between statins usage and the polygenic risk scores for phenotypic
variance in both cross sectional and longitudinal analyses. Particularly,
following the spirit of target trial emulation, we observe that the treatment
effect of statins is more substantial in people with higher polygenic risk
scores for phenotypic variance, indicating gene-environment interaction. When
applying to body mass index, the newly constructed polygenic risk scores for
variance show significant interaction with physical activity and sedentary
behavior. Therefore, the polygenic risk scores for phenotypic variance derived
by snpboostlss have potential to identify individuals that could benefit more
from environmental changes (e.g. medical intervention and lifestyle changes).

</details>


### [348] [Incorporating LLM Embeddings for Variation Across the Human Genome](https://arxiv.org/abs/2509.20702)
*Hongqian Niu,Jordan Bryan,Xihao Li,Didong Li*

Main category: stat.AP

TL;DR: 本文提出首个全人类基因组变异水平嵌入框架，用多源数据生成多尺度嵌入，实验验证其有效性，并给出下游应用，资源公开。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型嵌入在生物数据应用多聚焦基因层面信息，缺乏变异水平嵌入框架。

Method: 利用FAVOR、ClinVar和GWAS Catalog的注释构建89亿个可能变异的语义文本描述，用OpenAI和Qwen3模型生成多尺度嵌入。

Result: 基线实验显示对变异属性预测准确率高，验证了嵌入作为基因组变异结构化表示的有效性。

Conclusion: 提出的资源为大规模基因组发现和精准医学提供基础。

Abstract: Recent advances in large language model (LLM) embeddings have enabled
powerful representations for biological data, but most applications to date
focus only on gene-level information. We present one of the first systematic
frameworks to generate variant-level embeddings across the entire human genome.
Using curated annotations from FAVOR, ClinVar, and the GWAS Catalog, we
constructed semantic text descriptions for 8.9 billion possible variants and
generated embeddings at three scales: 1.5 million HapMap3+MEGA variants, ~90
million imputed UK Biobank variants, and ~9 billion all possible variants.
Embeddings were produced with both OpenAI's text-embedding-3-large and the
open-source Qwen3-Embedding-0.6B models. Baseline experiments demonstrate high
predictive accuracy for variant properties, validating the embeddings as
structured representations of genomic variation. We outline two downstream
applications: embedding-informed hypothesis testing by extending the
Frequentist And Bayesian framework to genome-wide association studies, and
embedding-augmented genetic risk prediction that enhances standard polygenic
risk scores. These resources, publicly available on Hugging Face, provide a
foundation for advancing large-scale genomic discovery and precision medicine.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [349] [Maxout Polytopes](https://arxiv.org/abs/2509.21286)
*Andrei Balakin,Shelby Cox,Georg Loho,Bernd Sturmfels*

Main category: math.CO

TL;DR: 研究浅网络的Maxout多面体的参数空间和极值f向量，以及添加层时产生的分离超平面，表明无瓶颈通用网络的Maxout多面体是立方体的。


<details>
  <summary>Details</summary>
Motivation: 对Maxout多面体进行深入研究，了解其特性。

Method: 对浅网络Maxout多面体的参数空间和极值f向量进行表征，研究网络添加层时的分离超平面。

Result: 得到浅网络Maxout多面体的参数空间和极值f向量的相关结论，发现无瓶颈通用网络的Maxout多面体是立方体的。

Conclusion: 成功对浅网络Maxout多面体的相关性质进行研究并得出结论。

Abstract: Maxout polytopes are defined by feedforward neural networks with maxout
activation function and non-negative weights after the first layer. We
characterize the parameter spaces and extremal f-vectors of maxout polytopes
for shallow networks, and we study the separating hypersurfaces which arise
when a layer is added to the network. We also show that maxout polytopes are
cubical for generic networks without bottlenecks.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [350] [Neural Networks as Surrogate Solvers for Time-Dependent Accretion Disk Dynamics](https://arxiv.org/abs/2509.20447)
*Shunyuan Mao,Weiqi Wang,Sifan Wang,Ruobing Dong,Lu Lu,Kwang Moo Yi,Paris Perdikaris,Andrea Isella,Sébastien Fabbro,Lile Wang*

Main category: astro-ph.EP

TL;DR: 本文首次用物理信息神经网络（PINNs）解决非自引力吸积盘二维时变流体动力学问题，成果显示其可实现无数据建模，或替代传统数值模拟。


<details>
  <summary>Details</summary>
Motivation: 传统吸积盘动力学建模需大量计算的（磁）流体动力学模拟，PINNs是有潜力的替代方法。

Method: 首次运用PINNs解决非自引力吸积盘二维时变流体动力学问题。

Result: 模型能在训练域内任意时空给出解，重现关键物理现象，消除了数值模拟中难以抑制的盘边缘虚假波反射。

Conclusion: 先进机器学习技术可实现物理驱动、无数据的复杂天体物理系统建模，未来或替代传统数值模拟。

Abstract: Accretion disks are ubiquitous in astrophysics, appearing in diverse
environments from planet-forming systems to X-ray binaries and active galactic
nuclei. Traditionally, modeling their dynamics requires computationally
intensive (magneto)hydrodynamic simulations. Recently, Physics-Informed Neural
Networks (PINNs) have emerged as a promising alternative. This approach trains
neural networks directly on physical laws without requiring data. We for the
first time demonstrate PINNs for solving the two-dimensional, time-dependent
hydrodynamics of non-self-gravitating accretion disks. Our models provide
solutions at arbitrary times and locations within the training domain, and
successfully reproduce key physical phenomena, including the excitation and
propagation of spiral density waves and gap formation from disk-companion
interactions. Notably, the boundary-free approach enabled by PINNs naturally
eliminates the spurious wave reflections at disk edges, which are challenging
to suppress in numerical simulations. These results highlight how advanced
machine learning techniques can enable physics-driven, data-free modeling of
complex astrophysical systems, potentially offering an alternative to
traditional numerical simulations in the future.

</details>
