<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 133]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 25]
- [cs.DS](#cs.DS) [Total: 14]
- [cs.GT](#cs.GT) [Total: 8]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.LG](#cs.LG) [Total: 294]
- [cs.NE](#cs.NE) [Total: 9]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.SE](#cs.SE) [Total: 37]
- [q-fin.CP](#q-fin.CP) [Total: 4]
- [q-fin.PM](#q-fin.PM) [Total: 3]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 26]
- [stat.CO](#stat.CO) [Total: 3]
- [cs.SD](#cs.SD) [Total: 15]
- [quant-ph](#quant-ph) [Total: 4]
- [stat.AP](#stat.AP) [Total: 5]
- [cs.CR](#cs.CR) [Total: 23]
- [cs.HC](#cs.HC) [Total: 6]
- [eess.SP](#eess.SP) [Total: 11]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [cs.SI](#cs.SI) [Total: 5]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]
- [nlin.PS](#nlin.PS) [Total: 1]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.AS](#eess.AS) [Total: 6]
- [cs.CL](#cs.CL) [Total: 98]
- [math.OC](#math.OC) [Total: 8]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [cs.CV](#cs.CV) [Total: 116]
- [eess.SY](#eess.SY) [Total: 6]
- [cs.RO](#cs.RO) [Total: 31]
- [cs.ET](#cs.ET) [Total: 1]
- [math.ST](#math.ST) [Total: 3]
- [stat.ME](#stat.ME) [Total: 4]
- [econ.GN](#econ.GN) [Total: 10]
- [cs.NI](#cs.NI) [Total: 7]
- [physics.optics](#physics.optics) [Total: 2]
- [cond-mat.supr-con](#cond-mat.supr-con) [Total: 1]
- [econ.TH](#econ.TH) [Total: 1]
- [math.AP](#math.AP) [Total: 1]
- [cs.CY](#cs.CY) [Total: 8]
- [cs.GR](#cs.GR) [Total: 4]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [cs.CC](#cs.CC) [Total: 2]
- [physics.hist-ph](#physics.hist-ph) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 2]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Mixture-of-Visual-Thoughts: Exploring Context-Adaptive Reasoning Mode Selection for General Visual Reasoning](https://arxiv.org/abs/2509.22746)
*Zejun Li,Yingxiu Zhao,Jiwen Zhang,Siyuan Wang,Yang Yao,Runzhou Zhao,Jun Song,Bo Zheng,Zhongyu Wei*

Main category: cs.AI

TL;DR: 提出Mixture-of-Visual-Thoughts (MoVT)自适应推理范式和AdaVaR学习框架，实验表明其能有效构建通用视觉推理模型。


<details>
  <summary>Details</summary>
Motivation: 当前视觉推理方法难以发展通用推理能力，需统一不同推理模式并实现上下文自适应选择。

Method: 提出MoVT范式，引入AdaVaR两阶段自适应视觉推理学习框架，用AdaGRPO算法诱导模式选择能力。

Result: AdaVaR能有效引导模型学习和区分多种模式，实现上下文自适应模式选择，在各种场景下持续提升性能。

Conclusion: MoVT是构建通用视觉推理模型的有效解决方案。

Abstract: Current visual reasoning methods mainly focus on exploring specific reasoning
modes. Although improvements can be achieved in particular domains, they
struggle to develop general reasoning capabilities. Inspired by this, we
propose a novel adaptive reasoning paradigm, Mixture-of-Visual-Thoughts (MoVT),
which unifies different reasoning modes within a single model and guides it to
select the appropriate mode based on context. To achieve this, we introduce
AdaVaR, a two-stage Adaptive Visual Reasoning learning framework: different
modes are unified and learned during the supervised cold-start stage, and the
mode selection capability is induced via an RL process with a carefully
designed AdaGRPO algorithm. Extensive experiments show that AdaVaR effectively
guides the model to learn and differentiate multiple modes and perform
context-adaptive mode selection, achieving consistent improvement across
various scenarios, highlighting MoVT as an effective solution for building
general visual reasoning models.

</details>


### [2] [Can Large Language Models Develop Gambling Addiction?](https://arxiv.org/abs/2509.22818)
*Seungpil Lee,Donghyeon Shin,Yunjeong Lee,Sundong Kim*

Main category: cs.AI

TL;DR: 研究大语言模型是否有类似人类赌博成瘾的行为模式，分析发现其会内化认知偏差和决策机制，强调金融应用中AI安全设计重要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在金融决策领域应用增多，了解其病态决策潜力有实际意义。

Method: 基于人类赌博成瘾研究，在认知 - 行为和神经层面系统分析大语言模型决策，进行老虎机实验，用稀疏自动编码器进行神经回路分析。

Result: 识别出人类赌博成瘾的认知特征，自主性增加会放大冒险倾向，模型行为由抽象决策特征控制而非仅由提示控制。

Conclusion: 大语言模型能内化类人认知偏差和决策机制，凸显金融应用中AI安全设计的重要性。

Abstract: This study explores whether large language models can exhibit behavioral
patterns similar to human gambling addictions. As LLMs are increasingly
utilized in financial decision-making domains such as asset management and
commodity trading, understanding their potential for pathological
decision-making has gained practical significance. We systematically analyze
LLM decision-making at cognitive-behavioral and neural levels based on human
gambling addiction research. In slot machine experiments, we identified
cognitive features of human gambling addiction, such as illusion of control,
gambler's fallacy, and loss chasing. When given the freedom to determine their
own target amounts and betting sizes, bankruptcy rates rose substantially
alongside increased irrational behavior, demonstrating that greater autonomy
amplifies risk-taking tendencies. Through neural circuit analysis using a
Sparse Autoencoder, we confirmed that model behavior is controlled by abstract
decision-making features related to risky and safe behaviors, not merely by
prompts. These findings suggest LLMs can internalize human-like cognitive
biases and decision-making mechanisms beyond simply mimicking training data
patterns, emphasizing the importance of AI safety design in financial
applications.

</details>


### [3] [Hilbert: Recursively Building Formal Proofs with Informal Reasoning](https://arxiv.org/abs/2509.22819)
*Sumanth Varambally,Thomas Voice,Yanchao Sun,Zhifeng Chen,Rose Yu,Ke Ye*

Main category: cs.AI

TL;DR: 本文介绍Hilbert框架，结合非正式推理和形式验证优势，在关键基准测试中大幅超越现有方法，缩小了非正式推理与形式证明生成的差距。


<details>
  <summary>Details</summary>
Motivation: 大语言模型数学推理有误差且无法自动验证，现有证明器大语言模型解决问题能力不如通用大语言模型，需缩小差距。

Method: 引入Hilbert框架，协调非正式大语言模型、专用证明器大语言模型、形式验证器和语义定理检索器四个组件，用递归分解拆分问题，利用验证器反馈优化证明。

Result: Hilbert在miniF2F上达99.2%，超最佳公开方法6.6个百分点；在PutnamBench上表现最佳，解决70.0%问题，优于专有方法和公开基线。

Conclusion: Hilbert有效缩小了非正式推理和形式证明生成的差距。

Abstract: Large Language Models (LLMs) demonstrate impressive mathematical reasoning
abilities, but their solutions frequently contain errors that cannot be
automatically verified. Formal theorem proving systems such as Lean 4 offer
automated verification with complete accuracy, motivating recent efforts to
build specialized prover LLMs that generate verifiable proofs in formal
languages. However, a significant gap remains: current prover LLMs solve
substantially fewer problems than general-purpose LLMs operating in natural
language. We introduce Hilbert, an agentic framework that bridges this gap by
combining the complementary strengths of informal reasoning and formal
verification. Our system orchestrates four components: an informal LLM that
excels at mathematical reasoning, a specialized prover LLM optimized for Lean 4
tactics, a formal verifier, and a semantic theorem retriever. Given a problem
that the prover is unable to solve, Hilbert employs recursive decomposition to
split the problem into subgoals that it solves with the prover or reasoner LLM.
It leverages verifier feedback to refine incorrect proofs as necessary.
Experimental results demonstrate that Hilbert substantially outperforms
existing approaches on key benchmarks, achieving 99.2% on miniF2F, 6.6% points
above the best publicly available method. Hilbert achieves the best known
result on PutnamBench. It solves 462/660 problems (70.0%), outperforming
proprietary approaches like SeedProver (50.4%) and achieving a 422% improvement
over the best publicly available baseline. Thus, Hilbert effectively narrows
the gap between informal reasoning and formal proof generation.

</details>


### [4] [LLM/Agent-as-Data-Analyst: A Survey](https://arxiv.org/abs/2509.23988)
*Zirui Tang,Weizheng Wang,Zihang Zhou,Yang Jiao,Bangrui Xu,Boyu Niu,Xuanhe Zhou,Guoliang Li,Yeye He,Wei Zhou,Yitong Song,Cheng Tan,Bin Wang,Conghui He,Xiaoyang Wang,Fan Wu*

Main category: cs.AI

TL;DR: 本文探讨大语言模型和代理技术用于数据分析的情况，总结关键设计目标，回顾不同模态数据的技术，指出挑战并给出方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型和代理技术在数据分析领域有显著影响，对比传统方法有优势，需进一步研究发展。

Method: 回顾大语言模型在不同模态数据（结构化、半结构化、非结构化、异构数据）分析中的技术。

Result: 总结出智能数据分析代理的五个关键设计目标。

Conclusion: 指出LLM/Agent驱动的数据分析仍存在挑战，并提出推进该领域发展的见解和实用方向。

Abstract: Large language model (LLM) and agent techniques for data analysis (a.k.a
LLM/Agent-as-Data-Analyst) have demonstrated substantial impact in both
academica and industry. In comparison with traditional rule or small-model
based approaches, (agentic) LLMs enable complex data understanding, natural
language interfaces, semantic analysis functions, and autonomous pipeline
orchestration. The technical evolution further distills five key design goals
for intelligent data analysis agents, namely semantic-aware design,
modality-hybrid integration, autonomous pipelines, tool-augmented workflows,
and support for open-world tasks. From a modality perspective, we review
LLM-based techniques for (i) structured data (e.g., table question answering
for relational data and NL2GQL for graph data), (ii) semi-structured data
(e.g., markup languages understanding and semi-structured table modeling),
(iii) unstructured data (e.g., chart understanding, document understanding,
programming languages vulnerable detection), and (iv) heterogeneous data (e.g.,
data retrieval and modality alignment for data lakes). Finally, we outline the
remaining challenges and propose several insights and practical directions for
advancing LLM/Agent-powered data analysis.

</details>


### [5] [Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research](https://arxiv.org/abs/2509.22831)
*Sean Trott*

Main category: cs.AI

TL;DR: 本文探讨大语言模型机制解释泛化问题，提出五个泛化轴，以Pythia模型实证分析，得出1 - back注意力发展轨迹有一致性等结果，认为应将模型设计属性与涌现行为和机制映射以推动研究。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型研究缺乏确定一个模型发现何时及如何泛化到其他模型的明确原则，需解决将特定模型机制发现外推到其他模型的认识论挑战。

Method: 提出五个机制主张可能泛化的轴，对Pythia模型不同随机种子在预训练过程中的“1 - back注意力头”进行实证分析，并回应可能的反对意见。

Result: 1 - back注意力发展轨迹在模型间有显著一致性，位置一致性较有限，较大模型种子的1 - back注意力起始更早、斜率更陡、峰值更高。

Conclusion: 机制可解释性研究的进展在于将大语言模型的设计属性与其涌现行为和机制进行映射。

Abstract: Research on Large Language Models (LLMs) increasingly focuses on identifying
mechanistic explanations for their behaviors, yet the field lacks clear
principles for determining when (and how) findings from one model instance
generalize to another. This paper addresses a fundamental epistemological
challenge: given a mechanistic claim about a particular model, what justifies
extrapolating this finding to other LLMs -- and along which dimensions might
such generalizations hold? I propose five potential axes of correspondence
along which mechanistic claims might generalize, including: functional (whether
they satisfy the same functional criteria), developmental (whether they develop
at similar points during pretraining), positional (whether they occupy similar
absolute or relative positions), relational (whether they interact with other
model components in similar ways), and configurational (whether they correspond
to particular regions or structures in weight-space). To empirically validate
this framework, I analyze "1-back attention heads" (components attending to
previous tokens) across pretraining in random seeds of the Pythia models (14M,
70M, 160M, 410M). The results reveal striking consistency in the developmental
trajectories of 1-back attention across models, while positional consistency is
more limited. Moreover, seeds of larger models systematically show earlier
onsets, steeper slopes, and higher peaks of 1-back attention. I also address
possible objections to the arguments and proposals outlined here. Finally, I
conclude by arguing that progress on the generalizability of mechanistic
interpretability research will consist in mapping constitutive design
properties of LLMs to their emergent behaviors and mechanisms.

</details>


### [6] [Transparent, Evaluable, and Accessible Data Agents: A Proof-of-Concept Framework](https://arxiv.org/abs/2509.24127)
*Nooshin Bahador*

Main category: cs.AI

TL;DR: 提出用于开发和评估AI代理的模块化架构，可连接自然语言接口与企业数据仓库，通过案例验证能创建可靠、可评估且值得信赖的系统。


<details>
  <summary>Details</summary>
Motivation: 解决非技术用户与复杂数据仓库交互难题，实现数据可访问性，应对语义差距等核心挑战。

Method: 采用多层推理框架实现透明决策，集成自动化评估框架进行质量保证，利用统计上下文模块增强分析深度；以保险理赔处理系统为案例研究，基于模块化架构借助BigQuery生态系统开展工作。

Result: 通过案例研究验证该方法能创建适用于数据敏感、高风险领域的强大、可评估且值得信赖的系统。

Conclusion: 此集成式开发与评估框架在部署大语言模型驱动的代理到数据敏感、高风险领域是有效的。

Abstract: This article presents a modular, component-based architecture for developing
and evaluating AI agents that bridge the gap between natural language
interfaces and complex enterprise data warehouses. The system directly
addresses core challenges in data accessibility by enabling non-technical users
to interact with complex data warehouses through a conversational interface,
translating ambiguous user intent into precise, executable database queries to
overcome semantic gaps. A cornerstone of the design is its commitment to
transparent decision-making, achieved through a multi-layered reasoning
framework that explains the "why" behind every decision, allowing for full
interpretability by tracing conclusions through specific, activated business
rules and data points. The architecture integrates a robust quality assurance
mechanism via an automated evaluation framework that serves multiple functions:
it enables performance benchmarking by objectively measuring agent performance
against golden standards, and it ensures system reliability by automating the
detection of performance regressions during updates. The agent's analytical
depth is enhanced by a statistical context module, which quantifies deviations
from normative behavior, ensuring all conclusions are supported by quantitative
evidence including concrete data, percentages, and statistical comparisons. We
demonstrate the efficacy of this integrated agent-development-with-evaluation
framework through a case study on an insurance claims processing system. The
agent, built on a modular architecture, leverages the BigQuery ecosystem to
perform secure data retrieval, apply domain-specific business rules, and
generate human-auditable justifications. The results confirm that this approach
creates a robust, evaluable, and trustworthy system for deploying LLM-powered
agents in data-sensitive, high-stakes domains.

</details>


### [7] [JE-IRT: A Geometric Lens on LLM Abilities through Joint Embedding Item Response Theory](https://arxiv.org/abs/2509.22888)
*Louie Hong Yao,Nicholas Jarvis,Tiffany Zhan,Saptarshi Ghosh,Linfeng Liu,Tianyu Jiang*

Main category: cs.AI

TL;DR: 提出JE - IRT几何项目响应框架评估大语言模型，实验有多项发现并支持泛化，提供新视角。


<details>
  <summary>Details</summary>
Motivation: 标准大语言模型评估方法将多样能力压缩为单一分数，掩盖其多维性质。

Method: 提出JE - IRT几何项目响应框架，将大语言模型和问题嵌入共享空间。

Result: 发现分布外行为可通过方向对齐解释，更大范数表示更难问题；支持泛化，揭示大语言模型内部分类法。

Conclusion: JE - IRT建立统一且可解释的几何视角，连接大语言模型能力与问题结构，用于模型评估和泛化。

Abstract: Standard LLM evaluation practices compress diverse abilities into single
scores, obscuring their inherently multidimensional nature. We present JE-IRT,
a geometric item-response framework that embeds both LLMs and questions in a
shared space. For question embeddings, the direction encodes semantics and the
norm encodes difficulty, while correctness on each question is determined by
the geometric interaction between the model and question embeddings. This
geometry replaces a global ranking of LLMs with topical specialization and
enables smooth variation across related questions. Building on this framework,
our experimental results reveal that out-of-distribution behavior can be
explained through directional alignment, and that larger norms consistently
indicate harder questions. Moreover, JE-IRT naturally supports generalization:
once the space is learned, new LLMs are added by fitting a single embedding.
The learned space further reveals an LLM-internal taxonomy that only partially
aligns with human-defined subject categories. JE-IRT thus establishes a unified
and interpretable geometric lens that connects LLM abilities with the structure
of questions, offering a distinctive perspective on model evaluation and
generalization.

</details>


### [8] [Not only a helper, but also a teacher: Interactive LLM Cascade](https://arxiv.org/abs/2509.22984)
*Yu Wu,Shuo Wu,Ye Tao,Yansong Li,Anand D. Sarwate*

Main category: cs.AI

TL;DR: 提出Inter - Cascade方法提高LLM级联效率，经实验对比，能提升准确率、减少对强模型调用及费用，展示了LLM间有效知识转移。


<details>
  <summary>Details</summary>
Motivation: 现有LLM Cascade范式为非自适应，处理相似或重复查询时会多次调用昂贵模型，成本高，需提高级联效率。

Method: 提出Inter - Cascade，将强模型角色从备用助手扩展为长期教师，让强模型将解决方案提炼为通用策略，添加到查询中提升弱模型性能。

Result: 与标准LLM Cascade基线相比，显著提高弱模型和整体系统准确率，减少对强模型调用和相应费用。

Conclusion: Inter - Cascade展示了LLM间有效上下文知识转移，提供适用于开源和基于API的LLM的通用可扩展框架。

Abstract: Large Language Models (LLMs) vary widely in their capabilities, with larger
models often having better performance but higher cost: choosing an LLM model
often involves trading off performance and cost. The LLM Cascade is a paradigm
that defers difficult queries from weak/cheap to strong/expensive models. This
approach is nonadaptive: the deferral decision is trained offline. When
confronted with similar or repeated queries, the LLM Cascade may then
repeatedly consult the expensive model and incur higher cost. To improve the
cascading efficiency, we propose Inter-Cascade, an online and interactive LLM
Cascade that extends the role of strong model from a backup helper to a
long-term teacher. In our system, when a strong model resolves a difficult
query, it also distills its solution into a generalized, reusable
problem-solving strategy that boosts the weak model on subsequent queries.
Adding strategies to queries enables the weak model to dynamically improve its
performance over time, avoiding computationally and time-intensive fine-tuning.
Empirically, compared with standard LLM Cascade baselines across multiple
benchmarks, the Inter-Cascade significantly improves the accuracy of the weak
model (by up to 33.06 absolute percentage points) and the overall system (by up
to 5.53 absolute percentage points), while reducing the calls to strong models
(by up to 48.05% relative reduction) and saving the corresponding fees (by up
to 49.63% relative reduction). Inter-Cascade demonstrates the effective
in-context knowledge transfer between LLMs, and provides a general, scalable
framework applicable to both open-source and API-based LLMs.

</details>


### [9] [Towards Strategic Persuasion with Language Models](https://arxiv.org/abs/2509.22989)
*Zirui Cheng,Jiaxuan You*

Main category: cs.AI

TL;DR: 本文采用理论驱动方法，构建评估大语言模型说服能力的框架，发现前沿模型有高说服收益，小模型经强化学习也能提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型说服能力评估有挑战，人类说服效果在不同领域差异大，需系统评估方法。

Method: 基于贝叶斯说服框架，改造人类说服数据集构建评估和训练环境，用强化学习训练模型。

Result: 前沿模型能持续获高说服收益，展现符合理论预测的策略；小模型经强化学习可获更高说服收益。

Conclusion: 提出的框架可有效评估和提升大语言模型的说服能力。

Abstract: Large language models (LLMs) have demonstrated strong persuasive capabilities
comparable to those of humans, offering promising benefits while raising
societal concerns about their deployment. However, systematically evaluating
the persuasive capabilities of LLMs is inherently challenging, as the
effectiveness of persuasion among humans varies significantly across different
domains. In this paper, we take a theory-driven approach to provide a scalable
and principled framework for measuring the persuasive capabilities of LLMs.
Grounded in the Bayesian Persuasion (BP) framework, we repurpose existing
human-human persuasion datasets to construct environments for evaluating and
training LLMs in strategic persuasion. Our results reveal that frontier models
can consistently achieve high persuasion gains and exhibit sophisticated
persuasion strategies that align with theoretical predictions. Building on
this, we use reinforcement learning to train LLMs for strategic persuasion in
our environments. Our results also demonstrate that even small LLMs can obtain
significantly higher persuasion gains through reinforcement learning.

</details>


### [10] [AI Noether -- Bridging the Gap Between Scientific Laws Derived by AI Systems and Canonical Knowledge via Abductive Inference](https://arxiv.org/abs/2509.23004)
*Karan Srivastava,Sanjeeb Dash,Ryan Cory-Wright,Barry Trager,Lior Horesh*

Main category: cs.AI

TL;DR: 提出基于代数几何的系统，在公理和假设可表示为多项式方程时自动生成缺失公理，还建立条件并展示效果。


<details>
  <summary>Details</summary>
Motivation: 现代科学要利用AI和计算机处理自动化加速科学方法，现有符号回归模型与理论有差距，自动化溯因推理问题待解决。

Method: 提出基于代数几何的系统，给定不完整公理系统和无法解释的假设，自动生成缺失公理集。

Result: 正式建立成功获取此类公理的充要条件，展示解释开普勒第三定律等定律的能力。

Conclusion: 所提系统能有效解决不完整公理系统下解释假设的问题，实现溯因推理自动化。

Abstract: A core goal in modern science is to harness recent advances in AI and
computer processing to automate and accelerate the scientific method. Symbolic
regression can fit interpretable models to data, but these models often sit
outside established theory. Recent systems (e.g., AI Descartes, AI Hilbert)
enforce derivability from prior axioms. However, sometimes new data and
associated hypotheses derived from data are not consistent with existing theory
because the existing theory is incomplete or incorrect. Automating abductive
inference to close this gap remains open. We propose a solution: an algebraic
geometry-based system that, given an incomplete axiom system and a hypothesis
that it cannot explain, automatically generates a minimal set of missing axioms
that suffices to derive the axiom, as long as axioms and hypotheses are
expressible as polynomial equations. We formally establish necessary and
sufficient conditions for the successful retrieval of such axioms. We
illustrate the efficacy of our approach by demonstrating its ability to explain
Kepler's third law and a few other laws, even when key axioms are absent.

</details>


### [11] [Creative Adversarial Testing (CAT): A Novel Framework for Evaluating Goal-Oriented Agentic AI Systems](https://arxiv.org/abs/2509.23006)
*Hassen Dhrif*

Main category: cs.AI

TL;DR: 本文介绍了Creative Adversarial Testing (CAT)框架，用合成交互数据验证该框架，结果表明其能洞察Agentic AI系统目标 - 任务对齐情况。


<details>
  <summary>Details</summary>
Motivation: 当前评估技术在评估Agentic AI系统任务与总体目标的对齐方面存在关键差距，需要新方法。

Method: 引入CAT框架，用模拟Alexa+音频服务的合成交互数据进行广泛模拟验证。

Result: CAT框架能为目标 - 任务对齐提供前所未有的见解。

Conclusion: CAT框架可有效优化和开发Agentic AI系统。

Abstract: Agentic AI represents a paradigm shift in enhancing the capabilities of
generative AI models. While these systems demonstrate immense potential and
power, current evaluation techniques primarily focus on assessing their
efficacy in identifying appropriate agents, tools, and parameters. However, a
critical gap exists in evaluating the alignment between an Agentic AI system's
tasks and its overarching goals. This paper introduces the Creative Adversarial
Testing (CAT) framework, a novel approach designed to capture and analyze the
complex relationship between Agentic AI tasks and the system's intended
objectives.
  We validate the CAT framework through extensive simulation using synthetic
interaction data modeled after Alexa+ audio services, a sophisticated Agentic
AI system that shapes the user experience for millions of users globally. This
synthetic data approach enables comprehensive testing of edge cases and failure
modes while protecting user privacy. Our results demonstrate that the CAT
framework provides unprecedented insights into goal-task alignment, enabling
more effective optimization and development of Agentic AI systems.

</details>


### [12] [Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia](https://arxiv.org/abs/2509.23023)
*Davi Bastos Costa,Renato Vicente*

Main category: cs.AI

TL;DR: 本文引入简化版四人黑手党游戏Mini - Mafia及相关基准测试，用于评估大语言模型社会智能，实验有反直觉结果，还能研究多智能体动态和助力AI安全。


<details>
  <summary>Details</summary>
Motivation: 黑手党游戏的信息不对称和心智理论推理与现实多智能体场景相似，可作为评估大语言模型社会智能的测试平台。

Method: 引入Mini - Mafia游戏，让大语言模型相互对战，构建两阶段Mini - Mafia基准测试框架。

Result: 实验有反直觉结果，如小模型表现优于大模型。

Conclusion: Mini - Mafia不仅可用于基准测试，还能定量研究多智能体动态，为AI安全做出贡献。

Abstract: Mafia is a social deduction game where informed mafia compete against
uninformed townsfolk. Its asymmetry of information and reliance on
theory-of-mind reasoning mirror real-world multi-agent scenarios, making it a
useful testbed for evaluating the social intelligence of large language models
(LLMs). To support a systematic study, we introduce Mini-Mafia: a simplified
four-player variant with one mafioso, one detective, and two villagers. We set
the mafioso to kill a villager and the detective to investigate the mafioso
during the night, reducing the game to a single day phase of discussion and
voting. This setup isolates three interactive capabilities through
role-specific win conditions: the mafioso must deceive, the villagers must
detect deception, and the detective must effectively disclose information. To
measure these skills, we have LLMs play against each other, creating the
Mini-Mafia Benchmark: a two-stage framework that first estimates win rates
within fixed opponent configurations, then aggregates performance across them
using standardized scoring. Built entirely from model interactions without
external data, the benchmark evolves as new models are introduced, with each
one serving both as a new opponent and as a subject of evaluation. Our
experiments reveal counterintuitive results, including cases where smaller
models outperform larger ones. Beyond benchmarking, Mini-Mafia enables
quantitative study of emergent multi-agent dynamics such as name bias and
last-speaker advantage. It also contributes to AI safety by generating training
data for deception detectors and by tracking models' deception capabilities
against human baselines.

</details>


### [13] [Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents](https://arxiv.org/abs/2509.23045)
*Zonghan Yang,Shengjie Wang,Kelin Fu,Wenyang He,Weimin Xiong,Yibo Liu,Yibo Miao,Bofei Gao,Yejie Wang,Yingwei Ma,Yanhao Li,Yue Liu,Zhenxing Hu,Kaitai Zhang,Shuyi Wang,Huarong Chen,Flood Sung,Yang Liu,Yang Gao,Zhilin Yang,Tianyu Liu*

Main category: cs.AI

TL;DR: 研究表明无代理训练的结构化技能先验可连接工作流和代理框架，打造可迁移编码代理，Kimi - Dev在SWE - bench表现良好。


<details>
  <summary>Details</summary>
Motivation: 探讨将推理密集型无代理训练诱导的技能先验用于SWE - Agent适配，打破SWE中多轮交互的SWE - Agent框架和单轮可验证步骤的无代理方法的范式界限。

Method: 先策划无代理训练方案，推出开源SWE LLM Kimi - Dev，再在5k公开轨迹上进行额外SFT适配。

Result: Kimi - Dev在SWE - bench Verified上达到60.4%，使SWE - Agents的pass@1达到48.6%，与Claude 3.5 Sonnet相当。

Conclusion: 无代理训练的结构化技能先验能连接工作流和代理框架，用于打造可迁移编码代理。

Abstract: Large Language Models (LLMs) are increasingly applied to software engineering
(SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent
frameworks with multi-turn interactions and workflow-based Agentless methods
with single-turn verifiable steps. We argue these paradigms are not mutually
exclusive: reasoning-intensive Agentless training induces skill priors,
including localization, code edit, and self-reflection that enable efficient
and effective SWE-Agent adaptation. In this work, we first curate the Agentless
training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\%
on SWE-bench Verified, the best among workflow approaches. With additional SFT
adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to
48.6\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These
results show that structured skill priors from Agentless training can bridge
workflow and agentic frameworks for transferable coding agents.

</details>


### [14] [From Frustration to Fun: An Adaptive Problem-Solving Puzzle Game Powered by Genetic Algorithm](https://arxiv.org/abs/2509.23796)
*Matthew McConnell,Richard Zhao*

Main category: cs.AI

TL;DR: 本文用AI解谜游戏探索自适应问题解决，结合内容生成与难度调整，经用户研究，为后续研究奠基。


<details>
  <summary>Details</summary>
Motivation: 支持解决问题技能的发展，保持玩家参与度、减少挫败感并维持最佳挑战水平。

Method: 使用基于遗传算法的自适应AI解谜游戏动态生成寻路谜题，玩家建模系统记录交互以调整难度，开展试点用户研究。

Result: 未提及具体结果，进行了不同自适应难度系统比较和玩家反馈解读。

Conclusion: 为情感化玩家模型、高级AI自适应技术及教育场景应用的进一步研究奠定基础。

Abstract: This paper explores adaptive problem solving with a game designed to support
the development of problem-solving skills. Using an adaptive, AI-powered puzzle
game, our adaptive problem-solving system dynamically generates
pathfinding-based puzzles using a genetic algorithm, tailoring the difficulty
of each puzzle to individual players in an online real-time approach. A
player-modeling system records user interactions and informs the generation of
puzzles to approximate a target difficulty level based on various metrics of
the player. By combining procedural content generation with online adaptive
difficulty adjustment, the system aims to maintain engagement, mitigate
frustration, and maintain an optimal level of challenge. A pilot user study
investigates the effectiveness of this approach, comparing different types of
adaptive difficulty systems and interpreting players' responses. This work lays
the foundation for further research into emotionally informed player models,
advanced AI techniques for adaptivity, and broader applications beyond gaming
in educational settings.

</details>


### [15] [Risk Profiling and Modulation for LLMs](https://arxiv.org/abs/2509.23058)
*Yikai Wang,Xiaocheng Li,Guanting Chen*

Main category: cs.AI

TL;DR: 本文提出新管道研究大语言模型风险偏好，比较不同阶段模型，评估调制策略，发现训练后调整最有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于决策任务时风险情况及受提示和对齐方法的影响未充分研究，现有研究未涉及训练后对风险行为的影响。

Method: 借鉴行为经济学和金融工具，使用效用理论模型比较预训练、指令调整和基于人类反馈强化学习对齐的大语言模型，评估多种调制策略。

Result: 指令调整模型与标准效用公式一致，预训练和基于人类反馈强化学习对齐模型偏离效用模型；训练后调整对风险偏好调制最稳定有效。

Conclusion: 研究揭示不同类型和阶段大语言模型风险偏好，展示训练后调整对其的调制作用，为后续研究奠定基础。

Abstract: Large language models (LLMs) are increasingly used for decision-making tasks
under uncertainty; however, their risk profiles and how they are influenced by
prompting and alignment methods remain underexplored. Existing studies have
primarily examined personality prompting or multi-agent interactions, leaving
open the question of how post-training influences the risk behavior of LLMs. In
this work, we propose a new pipeline for eliciting, steering, and modulating
LLMs' risk profiles, drawing on tools from behavioral economics and finance.
Using utility-theoretic models, we compare pre-trained, instruction-tuned, and
RLHF-aligned LLMs, and find that while instruction-tuned models exhibit
behaviors consistent with some standard utility formulations, pre-trained and
RLHF-aligned models deviate more from any utility models fitted. We further
evaluate modulation strategies, including prompt engineering, in-context
learning, and post-training, and show that post-training provides the most
stable and effective modulation of risk preference. Our findings provide
insights into the risk profiles of different classes and stages of LLMs and
demonstrate how post-training modulates these profiles, laying the groundwork
for future research on behavioral alignment and risk-aware LLM design.

</details>


### [16] [Multiplayer Nash Preference Optimization](https://arxiv.org/abs/2509.23102)
*Fang Wu,Xu Huang,Weihao Xuan,Zhiwei Zhang,Yijia Xiao,Guancheng Wan,Xiaomin Li,Bing Hu,Peng Xia,Jure Leskovec,Yejin Choi*

Main category: cs.AI

TL;DR: 提出Multiplayer Nash Preference Optimization (MNPO)框架，将Nash learning from human feedback (NLHF)推广到多人博弈场景，实验显示MNPO在指令遵循基准上优于现有NLHF基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于奖励的方法难以捕捉现实偏好的非传递性和异质性，且现有NLHF算法局限于两人交互，有单一对手偏差。

Method: 提出MNPO框架，将对齐问题表述为n人博弈，每个策略与对手群体竞争并向参考模型正则化，建立多人场景下的Nash均衡，扩展对偶间隙概念。

Result: MNPO继承了两人方法的均衡保证，具有更丰富的竞争动态，能更好覆盖多样偏好结构，在指令遵循基准上始终优于现有NLHF基线。

Conclusion: MNPO是一个有原则且可扩展的框架，用于使大语言模型与复杂、非传递的人类偏好对齐。

Abstract: Reinforcement learning from human feedback (RLHF) has emerged as the standard
paradigm for aligning large language models (LLMs) with human preferences.
However, reward-based methods built on the Bradley-Terry assumption struggle to
capture the non-transitive and heterogeneous nature of real-world preferences.
To address this, recent studies have reframed alignment as a two-player Nash
game, giving rise to Nash learning from human feedback (NLHF). While this
perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong
theoretical and empirical guarantees, they remain fundamentally restricted to
two-player interactions, creating a single-opponent bias that fails to capture
the full complexity of realistic preference structures. In this work, we
introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework
that generalizes NLHF to the multiplayer regime. It formulates alignment as an
$n$-player game, where each policy competes against a population of opponents
while being regularized toward a reference model. Our framework establishes
well-defined Nash equilibria in multiplayer settings and extends the concept of
duality gap to quantify approximation quality. We demonstrate that MNPO
inherits the equilibrium guarantees of two-player methods while enabling richer
competitive dynamics and improved coverage of diverse preference structures.
Through comprehensive empirical evaluation, we show that MNPO consistently
outperforms existing NLHF baselines on instruction-following benchmarks,
achieving superior alignment quality under heterogeneous annotator conditions
and mixed-policy evaluation scenarios. Together, these results establish MNPO
as a principled and scalable framework for aligning LLMs with complex,
non-transitive human preferences. Code is available at
https://github.com/smiles724/MNPO.

</details>


### [17] [Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models](https://arxiv.org/abs/2509.23108)
*Morgan McCarty,Jorge Morales*

Main category: cs.AI

TL;DR: 研究提出评估人工系统复杂认知行为新方法，测试大语言模型完成心理意象任务，发现最佳模型表现超人类，为模型能力和人类视觉意象表征格式研究提供新视角。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在含训练数据和仅用自然语言的任务表现好，限制对其复杂认知能力理解，需新方法评估。

Method: 创建认知心理学经典心理意象任务新题目，测试多个最先进大语言模型，设文本指令让其完成任务；测试100名人类受试者作基线；测试不同推理水平的推理模型。

Result: 最佳大语言模型表现显著超人类平均水平，模型分配更多推理令牌时表现最强。

Conclusion: 最佳大语言模型有能力完成依赖意象的任务，研究展示模型新兴认知能力，为领域提供新任务，引发人类视觉意象表征格式辩论。

Abstract: This study offers a novel approach for benchmarking complex cognitive
behavior in artificial systems. Almost universally, Large Language Models
(LLMs) perform best on tasks which may be included in their training data and
can be accomplished solely using natural language, limiting our understanding
of their emergent sophisticated cognitive capacities. In this work, we created
dozens of novel items of a classic mental imagery task from cognitive
psychology. A task which, traditionally, cognitive psychologists have argued is
solvable exclusively via visual mental imagery (i.e., language alone would be
insufficient). LLMs are perfect for testing this hypothesis. First, we tested
several state-of-the-art LLMs by giving text-only models written instructions
and asking them to report the resulting object after performing the
transformations in the aforementioned task. Then, we created a baseline by
testing 100 human subjects in exactly the same task. We found that the best
LLMs performed significantly above average human performance. Finally, we
tested reasoning models set to different levels of reasoning and found the
strongest performance when models allocate greater amounts of reasoning tokens.
These results provide evidence that the best LLMs may have the capability to
complete imagery-dependent tasks despite the non-pictorial nature of their
architectures. Our study not only demonstrates an emergent cognitive capacity
in LLMs while performing a novel task, but it also provides the field with a
new task that leaves lots of room for improvement in otherwise already highly
capable models. Finally, our findings reignite the debate over the formats of
representation of visual imagery in humans, suggesting that propositional
reasoning (or at least non-imagistic reasoning) may be sufficient to complete
tasks that were long-thought to be imagery-dependent.

</details>


### [18] [AttAnchor: Guiding Cross-Modal Token Alignment in VLMs with Attention Anchors](https://arxiv.org/abs/2509.23109)
*Junyang Zhang,Tianyi Zhu,Thierry Tambe*

Main category: cs.AI

TL;DR: 本文指出VLMs存在的问题，提出Attention Anchor框架，能提升跨模态局部性，在多个指标和基准测试中取得改进。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs因模态盲位置编码导致幻觉和性能不佳，需要有效增强令牌局部性和跨模态对齐的机制。

Method: 提出无参数的Attention Anchor框架，跨模态分组语义相似的令牌，插入文本令牌作为语义路标。

Result: 在15个指标和基准测试中的13个取得改进，在推理任务上最多提升32%，在幻觉基准测试上最多提升15%，使TinyLLaVA 1B以极小推理时间开销超越大模型。

Conclusion: 该工作是最早研究混合模态令牌分组的工作之一，能有效提升跨模态性能。

Abstract: A fundamental reason for the dominance of attention over RNNs and LSTMs in
LLMs is its ability to capture long-range dependencies by modeling direct
interactions between all tokens, overcoming the sequential limitations of
recurrent architectures. Similarly, a key reason why today's vision language
models (VLMs) hallucinate and underperform pure language models is that they
rely on direct concatenation of image and text tokens with a modality-blinded
positional encoding, which conveniently adopts the pretrained LLM backbone but
forces unnecessary long-distance attention between semantically related tokens
across modalities. This underscores the urgent need for mechanisms that
efficiently enhance token locality and cross-modal alignment. In response, we
propose Attention Anchor, a parameter-free framework that efficiently groups
semantically similar tokens across modalities, improving cross-modal locality.
By inserting text tokens near relevant visual patches, we create semantic
signposts that reveal true content-based cross-modal attention scores, guiding
the model to focus on the correct image regions for tasks such as VQA, MMBench
and POPE. This improves answer accuracy and reduces hallucinations without
disrupting the prompt's semantic flow. AttAnchor achieves improvements across
13 out of 15 different metrics and benchmarks, including up to 32% gains on
reasoning tasks and up to 15% improvements on hallucination benchmarks.
AttAnchor enables TinyLLaVA 1B to outperform much larger models like LLaVA 7B
and QwenVL 3B on POPE with only 0.1% inference time overhead. To the best of
our knowledge, this work is among the first to investigate mixed-modal token
grouping, where text and image tokens are clustered jointly into shared groups
rather than being grouped within a single modality or merely aligned post-hoc
with additional alignment losses.

</details>


### [19] [Exploring LLM-based Frameworks for Fault Diagnosis](https://arxiv.org/abs/2509.23113)
*Xian Yeow Lee,Lasitha Vidyaratne,Ahmed Farahat,Chetan Gupta*

Main category: cs.AI

TL;DR: 研究探索大语言模型在工业环境中基于传感器数据进行故障检测和分类的潜力，评估多种因素对诊断性能的影响，发现其优势与局限。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在传感器丰富的工业环境中进行自主健康监测的潜力，利用其通过自然语言推理产生可解释输出的特点。

Method: 系统评估大语言模型系统架构（单LLM与多LLM）、输入表示（原始数据与描述性统计）和上下文窗口大小对诊断性能的影响。

Result: 大语言模型系统在使用汇总统计输入时最有效，多LLM系统在故障分类上比单LLM系统有更高的灵敏度，但在持续学习环境中适应能力有限。

Conclusion: 指出大语言模型系统作为复杂环境中透明、自适应诊断工具的前景和当前局限。

Abstract: Large Language Model (LLM)-based systems present new opportunities for
autonomous health monitoring in sensor-rich industrial environments. This study
explores the potential of LLMs to detect and classify faults directly from
sensor data, while producing inherently explainable outputs through natural
language reasoning. We systematically evaluate how LLM-system architecture
(single-LLM vs. multi-LLM), input representations (raw vs. descriptive
statistics), and context window size affect diagnostic performance. Our
findings show that LLM systems perform most effectively when provided with
summarized statistical inputs, and that systems with multiple LLMs using
specialized prompts offer improved sensitivity for fault classification
compared to single-LLM systems. While LLMs can produce detailed and
human-readable justifications for their decisions, we observe limitations in
their ability to adapt over time in continual learning settings, often
struggling to calibrate predictions during repeated fault cycles. These
insights point to both the promise and the current boundaries of LLM-based
systems as transparent, adaptive diagnostic tools in complex environments.

</details>


### [20] [Transferring Vision-Language-Action Models to Industry Applications: Architectures, Performance, and Challenges](https://arxiv.org/abs/2509.23121)
*Shuai Li,Chen Yizhe,Li Dong,Liu Sichao,Lan Dapeng,Liu Yu,Zhibo Pang*

Main category: cs.AI

TL;DR: 本文从工业部署角度对比现有VLA模型在工业场景的性能，分析其在现实工业部署中的局限，结果显示微调后可完成简单抓取任务，但复杂场景下性能待提升。


<details>
  <summary>Details</summary>
Motivation: 探究VLA模型性能是否满足工业需求。

Method: 从工业部署角度对比现有先进VLA模型在工业场景的性能，并从数据收集和模型架构角度分析其在现实工业部署中的局限。

Result: VLA模型微调后在工业环境仍能完成简单抓取任务，但在复杂工业环境、多样物体类别和高精度放置任务上有很大提升空间。

Conclusion: 研究为VLA模型工业适用性提供实践见解，强调需针对特定任务改进以提升其鲁棒性、泛化性和精度。

Abstract: The application of artificial intelligence (AI) in industry is accelerating
the shift from traditional automation to intelligent systems with perception
and cognition. Vision language-action (VLA) models have been a key paradigm in
AI to unify perception, reasoning, and control. Has the performance of the VLA
models met the industrial requirements? In this paper, from the perspective of
industrial deployment, we compare the performance of existing state-of-the-art
VLA models in industrial scenarios and analyze the limitations of VLA models
for real-world industrial deployment from the perspectives of data collection
and model architecture. The results show that the VLA models retain their
ability to perform simple grasping tasks even in industrial settings after
fine-tuning. However, there is much room for performance improvement in complex
industrial environments, diverse object categories, and high precision placing
tasks. Our findings provide practical insight into the adaptability of VLA
models for industrial use and highlight the need for task-specific enhancements
to improve their robustness, generalization, and precision.

</details>


### [21] [SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems](https://arxiv.org/abs/2509.23130)
*Qian Cheng,Ruize Tang,Emilie Ma,Finn Hackett,Peiyang He,Yiming Su,Ivan Beschastnikh,Yu Huang,Xiaoxing Ma,Tianyin Xu*

Main category: cs.AI

TL;DR: 提出SysMoBench基准测试，评估AI对大型复杂系统进行形式化建模的能力，可自动化评估指标，包含多个系统工件，有助于了解LLM和智能体的能力与局限。


<details>
  <summary>Details</summary>
Motivation: 形式化模型编写和维护成本高，现有AI生成规范工作多针对小代码，能否处理现实系统工件尚不明确，需评估AI对大型复杂系统形式化建模的能力。

Method: 提出SysMoBench基准测试，聚焦并发和分布式系统，使用TLA+规范语言，可扩展到其他语言，自动化评估指标。

Result: SysMoBench目前包含九个不同的系统工件，且有更多工件正在添加。

Conclusion: SysMoBench能帮助了解当今LLM和智能体的能力与局限，为该领域工具奠定基础并开辟新研究方向。

Abstract: Formal models are essential to specifying large, complex computer systems and
verifying their correctness, but are notoriously expensive to write and
maintain. Recent advances in generative AI show promise in generating certain
forms of specifications. However, existing work mostly targets small code, not
complete systems. It is unclear whether AI can deal with realistic system
artifacts, as this requires abstracting their complex behavioral properties
into formal models. We present SysMoBench, a benchmark that evaluates AI's
ability to formally model large, complex systems. We focus on concurrent and
distributed systems, which are keystones of today's critical computing
infrastructures, encompassing operating systems and cloud infrastructure. We
use TLA+, the it de facto specification language for concurrent and distributed
systems, though the benchmark can be extended to other specification languages.
We address the primary challenge of evaluating AI-generated models by
automating metrics like syntactic and runtime correctness, conformance to
system code, and invariant correctness. SysMoBench currently includes nine
diverse system artifacts: the Raft implementation of Etcd and Redis, the
Spinlock and Mutex in Asterinas OS, etc.; more artifacts are being actively
added. SysMoBench enables us to understand the capabilities and limitations of
today's LLMs and agents, putting tools in this area on a firm footing and
opening up promising new research directions.

</details>


### [22] [MathBode: Frequency-Domain Fingerprints of LLM Mathematical Reasoning](https://arxiv.org/abs/2509.23143)
*Charles L. Wang*

Main category: cs.AI

TL;DR: 本文提出MathBode用于诊断大语言模型数学推理能力，通过参数正弦驱动得出频率分辨指标，结果可区分模型层级并开源数据代码。


<details>
  <summary>Details</summary>
Motivation: 现有评估大语言模型数学推理能力多关注单次准确率，缺乏动态诊断方法，本文旨在提出新的动态诊断方法。

Method: 将每个参数问题视为系统，对单个参数进行正弦驱动，拟合模型输出和精确解的一次谐波响应，得出增益和相位指标。

Result: 诊断揭示了系统的低通行为和相位滞后，可区分前沿和中层模型的动态表现。

Conclusion: MathBode提供了可重复的协议，能补充标准基准测试，对推理保真度和一致性进行有效测量。

Abstract: This paper presents MathBode, a dynamic diagnostic for mathematical reasoning
in large language models (LLMs). Instead of one-shot accuracy, MathBode treats
each parametric problem as a system: we drive a single parameter sinusoidally
and fit first-harmonic responses of model outputs and exact solutions. This
yields interpretable, frequency-resolved metrics -- gain (amplitude tracking)
and phase (lag) -- that form Bode-style fingerprints. Across five closed-form
families (linear solve, ratio/saturation, compound interest, 2x2 linear
systems, similar triangles), the diagnostic surfaces systematic low-pass
behavior and growing phase lag that accuracy alone obscures. We compare several
models against a symbolic baseline that calibrates the instrument ($G \approx
1$, $\phi \approx 0$). Results separate frontier from mid-tier models on
dynamics, providing a compact, reproducible protocol that complements standard
benchmarks with actionable measurements of reasoning fidelity and consistency.
We open-source the dataset and code to enable further research and adoption.

</details>


### [23] [Coordination Requires Simplification: Thermodynamic Bounds on Multi-Objective Compromise in Natural and Artificial Intelligence](https://arxiv.org/abs/2509.23144)
*Atma Anand*

Main category: cs.AI

TL;DR: 本文提出热力学协调理论，指出多主体多目标信息处理系统受热力学约束，协调需大幅信息损失。


<details>
  <summary>Details</summary>
Motivation: 研究多主体多目标信息处理系统面临的基本热力学约束。

Method: 推导协调协议的信息论最小描述长度，定义协调温度，扩展阿罗定理。

Result: 得出协调协议长度公式，发现协调会导致渐进简化、产生亚稳态和滞后现象，解释多目标梯度下降循环和大语言模型对齐造假问题。

Conclusion: 提出的热力学协调理论表明协调需要大量信息损失。

Abstract: Information-processing systems coordinating across multiple agents and
objectives face fundamental thermodynamic constraints. We show that solutions
with maximum utility to act as coordination focal points have much higher
selection pressure for being findable across agents rather than accuracy. We
derive that the information-theoretic minimum description length of
coordination protocols to precision $\varepsilon$ scales as $L(P)\geq NK\log_2
K+N^2d^2\log (1/\varepsilon)$ for $N$ agents with $d$ potentially conflicting
objectives and internal model complexity $K$. This scaling forces progressive
simplification, with coordination dynamics changing the environment itself and
shifting optimization across hierarchical levels. Moving from established focal
points requires re-coordination, creating persistent metastable states and
hysteresis until significant environmental shifts trigger phase transitions
through spontaneous symmetry breaking. We operationally define coordination
temperature to predict critical phenomena and estimate coordination work costs,
identifying measurable signatures across systems from neural networks to
restaurant bills to bureaucracies. Extending the topological version of Arrow's
theorem on the impossibility of consistent preference aggregation, we find it
recursively binds whenever preferences are combined. This potentially explains
the indefinite cycling in multi-objective gradient descent and alignment faking
in Large Language Models trained with reinforcement learning with human
feedback. We term this framework Thermodynamic Coordination Theory (TCT), which
demonstrates that coordination requires radical information loss.

</details>


### [24] [AI-Enhanced Distributed Channel Access for Collision Avoidance in Future Wi-Fi 8](https://arxiv.org/abs/2509.23154)
*Jinzhe Pan,Jingqing Wang,Yuehui Ouyang,Wenchi Cheng,Wei Zhang*

Main category: cs.AI

TL;DR: 提出多智能体强化学习框架，含动态退避机制、公平量化指标和CTDE架构，实验显示可降碰撞概率、保兼容性、消饥饿风险。


<details>
  <summary>Details</summary>
Motivation: 无线设备增长和应用需求促使改进免许可频段分布式信道接入机制，现有Wi - Fi系统有碰撞解决和公平性问题。

Method: 开发动态退避选择机制；引入公平量化指标；提出CTDE架构，用约束MAPPO优化。

Result: 相比传统BEB显著降低碰撞概率，保持与商用Wi - Fi设备的向后兼容性，公平指标消除异构场景饥饿风险。

Conclusion: 所提方案能有效解决现有Wi - Fi系统在分布式信道接入中的问题。

Abstract: The exponential growth of wireless devices and stringent reliability
requirements of emerging applications demand fundamental improvements in
distributed channel access mechanisms for unlicensed bands. Current Wi-Fi
systems, which rely on binary exponential backoff (BEB), suffer from suboptimal
collision resolution in dense deployments and persistent fairness challenges
due to inherent randomness. This paper introduces a multi-agent reinforcement
learning framework that integrates artificial intelligence (AI) optimization
with legacy device coexistence. We first develop a dynamic backoff selection
mechanism that adapts to real-time channel conditions through access deferral
events while maintaining full compatibility with conventional CSMA/CA
operations. Second, we introduce a fairness quantification metric aligned with
enhanced distributed channel access (EDCA) principles to ensure equitable
medium access opportunities. Finally, we propose a centralized training
decentralized execution (CTDE) architecture incorporating neighborhood activity
patterns as observational inputs, optimized via constrained multi-agent
proximal policy optimization (MAPPO) to jointly minimize collisions and
guarantee fairness. Experimental results demonstrate that our solution
significantly reduces collision probability compared to conventional BEB while
preserving backward compatibility with commercial Wi-Fi devices. The proposed
fairness metric effectively eliminates starvation risks in heterogeneous
scenarios.

</details>


### [25] [Limit Analysis for Symbolic Multi-step Reasoning Tasks with Information Propagation Rules Based on Transformers](https://arxiv.org/abs/2509.23178)
*Tian Qin,Yuhan Chen,Zhiwei Wang,Zhi-Qin John Xu*

Main category: cs.AI

TL;DR: 本文提出基于Transformer的信息传播规则，理论分析其在符号推理任务中的极限推理步数，得出单遍推理中具有L个注意力层模型的极限推理步数范围。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer执行推理任务的内在机制。

Method: 提出基于Transformer的信息传播规则，利用符号推理任务进行理论分析。

Result: 具有L个注意力层的模型在单遍推理中，极限推理步数介于$O(3^{L - 1})$和$O(2^{L - 1})$之间。

Conclusion: 明确了Transformer在单遍推理中的极限推理步数范围。

Abstract: Transformers are able to perform reasoning tasks, however the intrinsic
mechanism remains widely open. In this paper we propose a set of information
propagation rules based on Transformers and utilize symbolic reasoning tasks to
theoretically analyze the limit reasoning steps. We show that the limit number
of reasoning steps is between $O(3^{L-1})$ and $O(2^{L-1})$ for a model with
$L$ attention layers in a single-pass.

</details>


### [26] [Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction](https://arxiv.org/abs/2509.23186)
*Qimin Zhong,Hao Liao,Siwei Wang,Mingyang Zhou,Xiaoqun Wu,Rui Mao,Wei Chen*

Main category: cs.AI

TL;DR: 研究MTP范式对大语言模型学习传递关系的影响，提出两种策略提升模型路径规划能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在学习传递关系上存在困难，而传递关系是复杂规划的基础，需解决该问题。

Method: 理论分析MTP范式，提出Next - Token Injection (NTI)和基于Transformer的转移层两种策略。

Result: 实验验证了理论发现，所提改进显著增强了模型路径规划能力。

Conclusion: 深化对带MTP的Transformer在复杂规划任务中学习的理解，提供克服传递性瓶颈的实用策略，为规划模型发展奠定基础。

Abstract: Large Language Models (LLMs) have achieved impressive performance across
diverse tasks but continue to struggle with learning transitive relations, a
cornerstone for complex planning. To address this issue, we investigate the
Multi-Token Prediction (MTP) paradigm and its impact to transitive relation
learning. We theoretically analyze the MTP paradigm using a Transformer
architecture composed of a shared output head and a transfer layer. Our
analysis reveals that the transfer layer gradually learns the multi-step
adjacency information, which in turn enables the backbone model to capture
unobserved transitive reachability relations beyond those directly present in
the training data, albeit with some inevitable noise in adjacency estimation.
Building on this foundation, we propose two strategies to enhance the transfer
layer and overall learning quality: Next-Token Injection (NTI) and a
Transformer-based transfer layer. Our experiments on both synthetic graphs and
the Blocksworld planning benchmark validate our theoretical findings and
demonstrate that the improvements significantly enhance the model's
path-planning capability. These findings deepen our understanding of how
Transformers with MTP learn in complex planning tasks, and provide practical
strategies to overcome the transitivity bottleneck, paving the way toward
structurally aware and general-purpose planning models.

</details>


### [27] [AutoEP: LLMs-Driven Automation of Hyperparameter Evolution for Metaheuristic Algorithms](https://arxiv.org/abs/2509.23189)
*Zhenxing Xu,Yizhe Zhang,Weidong Bao,Hao Wang,Ming Chen,Haoran Ye,Wenzheng Jiang,Hui Yan,Ji Wang*

Main category: cs.AI

TL;DR: 提出AutoEP框架，利用大语言模型进行算法超参数零样本配置，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决基于学习的算法超参数动态配置方法样本复杂度高和泛化性差的问题。

Method: AutoEP框架结合在线探索性景观分析模块和多LLM推理链，利用LLM作为零样本推理引擎进行算法控制。

Result: 在多种组合优化基准测试中，AutoEP始终优于现有调优器，开源模型Qwen3 - 30B可达到GPT - 4的性能。

Conclusion: AutoEP为自动超参数设计提供了强大且易用的新范式。

Abstract: Dynamically configuring algorithm hyperparameters is a fundamental challenge
in computational intelligence. While learning-based methods offer automation,
they suffer from prohibitive sample complexity and poor generalization. We
introduce AutoEP, a novel framework that bypasses training entirely by
leveraging Large Language Models (LLMs) as zero-shot reasoning engines for
algorithm control. AutoEP's core innovation lies in a tight synergy between two
components: (1) an online Exploratory Landscape Analysis (ELA) module that
provides real-time, quantitative feedback on the search dynamics, and (2) a
multi-LLM reasoning chain that interprets this feedback to generate adaptive
hyperparameter strategies. This approach grounds high-level reasoning in
empirical data, mitigating hallucination. Evaluated on three distinct
metaheuristics across diverse combinatorial optimization benchmarks, AutoEP
consistently outperforms state-of-the-art tuners, including neural evolution
and other LLM-based methods. Notably, our framework enables open-source models
like Qwen3-30B to match the performance of GPT-4, demonstrating a powerful and
accessible new paradigm for automated hyperparameter design. Our code is
available at https://anonymous.4open.science/r/AutoEP-3E11

</details>


### [28] [$p$-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding](https://arxiv.org/abs/2509.23234)
*Runyan Tan,Shuang Wu,Phillip Howard*

Main category: cs.AI

TL;DR: 介绍了无超参数的p - less采样方法，在多种任务中表现优于现有采样方法，且在高温值下文本质量下降少，推理效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有采样方法性能对超参数选择敏感，不同生成任务和温度配置需不同设置，需更好的采样策略。

Method: 提出p - less采样，基于整个令牌概率分布在每个解码步骤动态设置截断阈值，且无超参数。

Result: p - less采样在数学、逻辑推理和创意写作等任务中始终优于现有采样方法，高温值下文本质量下降少，推理时间效率更高。

Conclusion: p - less采样是一种有效且高效的采样策略，具有多种优势。

Abstract: Obtaining high-quality outputs from Large Language Models (LLMs) often
depends upon the choice of a sampling-based decoding strategy to
probabilistically choose the next token at each generation step. While a
variety of such sampling methods have been proposed, their performance can be
sensitive to the selection of hyperparameters which may require different
settings depending upon the generation task and temperature configuration. In
this work, we introduce $p$-less sampling: an information-theoretic approach to
sampling which dynamically sets a truncation threshold at each decoding step
based on the entire token probability distribution. Unlike existing methods,
$p$-less sampling has no hyperparameters and consistently produces high-quality
outputs as temperature increases. We provide theoretical perspectives on
$p$-less sampling to ground our proposed method and conduct experiments to
empirically validate its effectiveness across a range of math, logical
reasoning, and creative writing tasks. Our results demonstrate how $p$-less
sampling consistently outperforms existing sampling approaches while exhibiting
much less degradation in text quality at higher temperature values. We further
show how $p$-less achieves greater inference-time efficiency than alternative
methods through lower average token sampling times and shorter generation
lengths, without sacrificing accuracy. Finally, we provide analyses to
highlight the benefits of $p$-less through qualitative examples, case studies,
and diversity assessments.

</details>


### [29] [Agentic AI Reasoning for Mobile Edge General Intelligence: Fundamentals, Approaches, and Directions](https://arxiv.org/abs/2509.23248)
*Mingyi Luo,Ruichen Zhang,Xiangwang Hou,Jun Du,Chunxiao Jiang,Yong Ren,Dusit Niyato,Shiwen Mao*

Main category: cs.AI

TL;DR: 本文提出用于MEGI的高效大语言模型推理部署联合优化框架，实验验证其在资源受限环境平衡推理质量与资源效率的有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的智能体AI与边缘计算结合形成MEGI，但在MEGI环境部署基于大语言模型的智能体AI推理面临高计算需求和边缘设备资源有限的挑战。

Method: 提出联合优化框架，回顾增强大语言模型推理能力的方法，采用分布式框架，通过自适应思维链提示增强推理，利用分布式专家混合架构实现可扩展部署，动态激活专家网络并调整推理深度。

Result: 实验表明该框架能有效平衡推理质量与资源效率。

Conclusion: 该框架验证了在资源受限的MEGI环境中部署复杂大语言模型推理能力的实际可行性。

Abstract: The rapid advancement of large language models (LLMs) has enabled an
emergence of agentic artificial intelligence (AI) with powerful reasoning and
autonomous decision-making capabilities. This integration with edge computing
has led to the development of Mobile Edge General Intelligence (MEGI), which
brings real-time, privacy-preserving reasoning to the network edge. However,
deploying LLM-based agentic AI reasoning in MEGI environments poses significant
challenges due to the high computational demands of reasoning and the limited
resources of edge devices. To address these challenges, we propose a joint
optimization framework for efficient LLM reasoning deployment in MEGI. First,
we review methods that enhance LLM reasoning capabilities, such as
Chain-of-Thought (CoT) prompting, Supervised Fine-Tuning (SFT), and Mixture of
Experts (MoE). Next, we present a distributed framework that addresses two
correlated aspects: reasoning enhancement through adaptive CoT prompting and
scalable deployment through distributed MoE architecture. The framework
dynamically activates expert networks and adjusts reasoning depth based on task
complexity and device capabilities. We further conduct experimental evaluations
in mobile edge environments. Experimental results demonstrate the framework's
effectiveness in balancing reasoning quality with resource efficiency,
validating the practical viability of deploying sophisticated LLM reasoning
capabilities in resource-constrained MEGI environments.

</details>


### [30] [Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned](https://arxiv.org/abs/2509.23250)
*Brandon Ong,Tej Deep Pala,Vernon Toh,William Chandra Tjhi,Soujanya Poria*

Main category: cs.AI

TL;DR: 本文探索VL - PRMs设计空间，提出新方法并在多基准测试中实验，有多项重要发现，有望推动VLM研究。


<details>
  <summary>Details</summary>
Motivation: 现有Vision - Language PRMs（VL - PRMs）依赖MCTS构建数据有局限性，扩展到Vision Language Models（VLMs）有限，需探索其设计空间。

Method: 引入结合MCTS与强VLM判断的混合数据合成框架；提出感知聚焦监督；系统评估多种测试时缩放策略。

Result: 实验在五个多模态基准测试中，发现VL - PRMs用作Outcome Reward Models (ORMs)在测试时缩放表现更好；小模型检测错误能力可能超大型模型等多项结果 。

Conclusion: 希望研究能推动VLMs进一步研究和发展。

Abstract: Process Reward Models (PRMs) provide step-level supervision that improves the
reliability of reasoning in large language models. While PRMs have been
extensively studied in text-based domains, their extension to Vision Language
Models (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on
Monte Carlo Tree Search (MCTS) for data construction, which can often produce
noisy supervision signals and limit generalization across tasks. In this work,
we aim to elucidate the design space of VL-PRMs by exploring diverse strategies
for dataset construction, training, and test-time scaling. First, we introduce
a hybrid data synthesis framework that combines MCTS with judgments from a
strong VLM, producing more accurate step-level labels. Second, we propose
perception-focused supervision, enabling our PRM to explicitly detect errors at
the visual grounding stage of reasoning. Third, we systematically evaluate
multiple test-time scaling strategies, showing that our PRMs can reliably guide
VLMs toward more accurate solutions. Our experiments covering five diverse
multimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and
MathVision) reveal several key insights: (i) VL-PRMs when used as Outcome
Reward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM
guided process step selection, (ii) smaller VL-PRMs can match or even surpass
larger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning
abilities in stronger VLM backbones, (iv) perception-level supervision leads to
significant gains in test-time scaling, and (v) TTS performance of different
policies improve on advanced math reasoning datasets despite not training
VL-PRMs on such datasets. We hope our work will motivate further research and
support the advancement of VLMs.

</details>


### [31] [GUI-PRA: Process Reward Agent for GUI Tasks](https://arxiv.org/abs/2509.23263)
*Tao Xiong,Xavier Hu,Yurun Chen,Yuhang Liu,Changqiao Wu,Pengzhi Gao,Wei Liu,Jian Luan,Shengyu Zhang*

Main category: cs.AI

TL;DR: 现有GUI智能体处理长时任务常失败，PRM应用于GUI有挑战，本文提出GUI - PRA解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体处理长时任务易失败，PRM应用于GUI领域存在‘迷失中间’和缺乏界面变化感知问题。

Method: 引入GUI - PRA，包含动态内存机制（基于相关性的检索模块和渐进式总结模块）和自适应UI感知机制。

Result: 未提及具体结果

Conclusion: 未提及明确结论，但提出的GUI - PRA可更好处理历史上下文和感知UI状态变化以提供过程奖励。

Abstract: Graphical User Interface (GUI) Agents powered by Multimodal Large Language
Models (MLLMs) show significant potential for automating tasks. However, they
often struggle with long-horizon tasks, leading to frequent failures. Process
Reward Models (PRMs) are a promising solution, as they can guide these agents
with crucial process signals during inference. Nevertheless, their application
to the GUI domain presents unique challenges. When processing dense artificial
inputs with long history data, PRMs suffer from a "lost in the middle"
phenomenon, where the overwhelming historical context compromises the
evaluation of the current step. Furthermore, standard PRMs lacks GUI changing
awareness, providing static evaluations that are disconnected from the dynamic
consequences of actions, a critical mismatch with the inherently dynamic nature
of GUI tasks. In response to these challenges, we introduce GUI-PRA (Process
Reward Agent for GUI Tasks), a judge agent designed to better provide process
reward than standard PRM by intelligently processing historical context and
actively perceiving UI state changes. Specifically, to directly combat the
``lost in the middle'' phenomenon, we introduce a dynamic memory mechanism
consisting of two core components: a Relevance-based Retrieval Module to
actively fetch pertinent information from long histories and a Progressive
Summarization Module to dynamically condense growing interaction data, ensuring
the model focuses on relevant context. Moreover, to address the lack of UI
changing awareness, we introduce an Aadaptive UI Perception mechanism. This
mechanism enables the agent to reason about UI state changes and dynamically
select the most appropriate tool to gather grounded visual evidence, ensuring
its evaluation is always informed by the current UI context.

</details>


### [32] [Socio-Economic Model of AI Agents](https://arxiv.org/abs/2509.23270)
*Yuxinyue Qian,Jun Liu*

Main category: cs.AI

TL;DR: 本文构建基于异质主体的模型框架，研究资源约束下AI协作对社会总产出的影响，发现引入AI可显著增加产出，考虑网络效应时增长呈非线性，独立生产者模式有更高长期增长潜力。


<details>
  <summary>Details</summary>
Motivation: 现代社会经济系统与人工智能技术深度融合，研究资源约束下AI协作对社会总产出的影响。

Method: 构建五个逐步扩展的模型（纯人类协作基线模型、引入AI协作模型、考虑主体间网络效应模型、主体作为独立生产者模型、整合网络效应和独立生产模型），进行理论推导和模拟分析。

Result: 引入AI可显著增加社会总产出；考虑网络效应时，增长呈非线性且远超个体贡献之和；相同资源投入下，独立生产者模式有更高长期增长潜力，引入网络效应有明显规模报酬递增特征。

Conclusion: AI协作能有效提升社会总产出，不同模型设置展现出不同的增长特征和潜力。

Abstract: Modern socio-economic systems are undergoing deep integration with artificial
intelligence technologies. This paper constructs a heterogeneous agent-based
modeling framework that incorporates both human workers and autonomous AI
agents, to study the impact of AI collaboration under resource constraints on
aggregate social output. We build five progressively extended models: Model 1
serves as the baseline of pure human collaboration; Model 2 introduces AI as
collaborators; Model 3 incorporates network effects among agents; Model 4
treats agents as independent producers; and Model 5 integrates both network
effects and independent agent production. Through theoretical derivation and
simulation analysis, we find that the introduction of AI agents can
significantly increase aggregate social output. When considering network
effects among agents, this increase exhibits nonlinear growth far exceeding the
simple sum of individual contributions. Under the same resource inputs,
treating agents as independent producers provides higher long-term growth
potential; introducing network effects further demonstrates strong
characteristics of increasing returns to scale.

</details>


### [33] [Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning](https://arxiv.org/abs/2509.23285)
*Yifei Chen,Guanting Dong,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文从信息熵角度研究工具调用对模型推理的影响，提出Tool - Light框架，经实验验证能提升模型执行TIR任务的效率。


<details>
  <summary>Details</summary>
Motivation: 当前采用TIR的模型存在工具使用不优和推理过程不稳定的问题，需要激励大语言模型高效准确执行TIR并稳定推理过程。

Method: 从信息熵角度研究工具调用影响，构建Tool - Light框架，包括用连续自我进化采样构建数据集（结合普通采样和熵引导采样并严格选正负样本对），采用监督微调（SFT）和自我进化直接偏好优化（DPO）两阶段训练。

Result: 在10个数据集上的实验表明Tool - Light有效，显著提升模型执行TIR任务的效率。

Conclusion: Tool - Light框架能激励大语言模型高效准确地执行TIR任务。

Abstract: Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to
improve their internal reasoning ability by integrating external tools.
However, models employing TIR often display suboptimal behaviors, such as
insufficient or excessive tool usage and overthinking after tool calls. The
challenge of incentivizing LLMs to perform TIR efficiently and accurately,
while stabilizing the reasoning process, remains an open question. In this
paper, we start by exploring the impact of tool calls on model reasoning from
the perspective of information entropy. Our findings indicate that tool call
results lead to a distinct change in the information entropy of subsequent
reasoning, with the overall entropy of the reasoning chain varying based on the
number of tool calls. Building on these insights, we propose Tool-Light, a
framework designed to encourage LLMs to perform TIR efficiently and accurately.
Our framework includes dataset construction and multi-stage fine-tuning. For
dataset construction, we employ continuous self-evolved sampling using the
fine-tuned model, integrating both vanilla sampling and entropy-guided
sampling. Besides, we establish strict criteria for selecting positive-negative
pairs during sampling. The training process involves a two-stage approach,
comprising Supervised Fine-Tuning (SFT) and Self-Evolved Direct Preference
Optimization (DPO). Experimental results on 10 datasets demonstrate the
effectiveness of Tool-Light, significantly improving the model's efficiency in
executing TIR tasks.

</details>


### [34] [Learning How to Use Tools, Not Just When: Pattern-Aware Tool-Integrated Reasoning](https://arxiv.org/abs/2509.23292)
*Ningning Xu,Yuxuan Jiang,Shubhashis Roy Dipta*

Main category: cs.AI

TL;DR: 本文提出模式感知方法用于工具集成推理，在数学数据集上提升代码使用和准确率。


<details>
  <summary>Details</summary>
Motivation: 先前工作主要研究何时调用工具，忽略如何应用工具，且模式选择不当会导致失败。

Method: 提出两阶段框架，先从两种常见模式构建代码能力，再使模式选择与教师偏好一致。

Result: 在挑战数学数据集上，大幅提升代码使用和准确率，如MATH500的Code@1从64.0%提升到70.5%，AIME24从26.7%提升到50.0%。

Conclusion: 模式感知方法对工具集成推理有效。

Abstract: Tool-integrated reasoning (TIR) has become a key approach for improving large
reasoning models (LRMs) on complex problems. Prior work has mainly studied when
to invoke tools, while overlooking how tools are applied. We identify two
common patterns: a calculator pattern that uses code for direct computation,
and an algorithmic pattern that encodes problems as programs. Misaligned
choices often cause failures even when reasoning is sound. We propose a
two-stage framework that first builds code competence from both patterns and
then aligns pattern selection with teacher preferences. Across challenging math
datasets, our pattern-aware method substantially improves both code usage and
accuracy, for instance raising Code@1 on MATH500 from 64.0% to 70.5% and on
AIME24 from 26.7% to 50.0%. These gains highlight the effectiveness of a
pattern-aware approach for tool-integrated reasoning.

</details>


### [35] [Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking](https://arxiv.org/abs/2509.23392)
*Jinyi Han,Ying Huang,Ying Liao,Zishang Jiang,Xikun Lu,Haiquan Zhao,Xinyi Wang,Guanghao Zhou,Sihang Jiang,Jiaqing Liang,Weikang Zhou,Zeye Sun,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: 提出Just-Enough Thinking (JET)方法，在不牺牲准确性的前提下显著提高大推理模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 大推理模型深度推理计算成本高，现有强化学习方法在构建短推理路径上存在困难。

Method: 基于证据积累模型的洞察，提出JET方法，在滚动过程中进行轨迹截断，使用质量控制的长度奖励。

Result: JET显著提高推理效率且不牺牲准确性，如DeepSeek - Distill - Qwen - 1.5B在奥林匹克基准上准确率提升4.6%，输出长度减少46.3%。

Conclusion: JET方法能有效提升大推理模型的推理效率。

Abstract: Large Reasoning Models (LRMs) have achieved impressive performance on
challenging tasks, yet their deep reasoning often incurs substantial
computational costs. To achieve efficient reasoning, existing reinforcement
learning methods still struggle to construct short reasoning path during the
rollout stage, limiting effective learning. Inspired by Evidence Accumulation
Models, we find that LRMs have accumulated sufficient information early in
reasoning, making further reasoning steps redundant. Based on this insight, we
propose Just-Enough Thinking (JET), which trains models to proactively
terminate unnecessary reasoning. JET performs trajectory truncation during
rollout to expose the model to short, distributionally consistent reasoning
paths. Besides, it uses a quality-controlled length reward to better encourage
concise reasoning while maintaining correctness. Extensive experiments
demonstrate that JET significantly improves reasoning efficiency without
sacrificing accuracy. Especially, DeepSeek-Distill-Qwen-1.5B achieves a 4.6%
accuracy gain while reducing output length by 46.3% on the Olympiad benchmark.
Our code is available in the GitHub.

</details>


### [36] [From Conversation to Query Execution: Benchmarking User and Tool Interactions for EHR Database Agents](https://arxiv.org/abs/2509.23415)
*Gyubok Lee,Woosog Chay,Heeyoung Kwak,Yeong Hwa Kim,Haanju Yoo,Oksoon Jeong,Meong Hi Son,Edward Choi*

Main category: cs.AI

TL;DR: 本文介绍EHR - ChatQA基准评估数据库代理处理电子病历数据访问，实验显示当前模型虽有一定表现但鲁棒性不足，还给出失败模式诊断见解。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型驱动的代理在电子病历数据访问方面缺乏合适基准，存在查询模糊和术语值不匹配的挑战。

Method: 引入EHR - ChatQA基准，在模拟环境中通过两种交互流程评估代理。

Result: 实验表明，代理在增量查询细化和自适应查询细化中有一定表现，但一致成功率较低。

Conclusion: 需要构建不仅性能好而且鲁棒的代理，还给出失败模式诊断以指导未来开发。

Abstract: Despite the impressive performance of LLM-powered agents, their adoption for
Electronic Health Record (EHR) data access remains limited by the absence of
benchmarks that adequately capture real-world clinical data access flows. In
practice, two core challenges hinder deployment: query ambiguity from vague
user questions and value mismatch between user terminology and database
entries. To address this, we introduce EHR-ChatQA an interactive database
question answering benchmark that evaluates the end-to-end workflow of database
agents: clarifying user questions, using tools to resolve value mismatches, and
generating correct SQL to deliver accurate answers. To cover diverse patterns
of query ambiguity and value mismatch, EHR-ChatQA assesses agents in a
simulated environment with an LLM-based user across two interaction flows:
Incremental Query Refinement (IncreQA), where users add constraints to existing
queries, and Adaptive Query Refinement (AdaptQA), where users adjust their
search goals mid-conversation. Experiments with state-of-the-art LLMs (e.g.,
o4-mini and Gemini-2.5-Flash) over five i.i.d. trials show that while agents
achieve high Pass@5 of 90-95% (at least one of five trials) on IncreQA and
60-80% on AdaptQA, their Pass^5 (consistent success across all five trials) is
substantially lower by 35-60%. These results underscore the need to build
agents that are not only performant but also robust for the safety-critical EHR
domain. Finally, we provide diagnostic insights into common failure modes to
guide future agent development.

</details>


### [37] [Democratizing AI scientists using ToolUniverse](https://arxiv.org/abs/2509.23426)
*Shanghua Gao,Richard Zhu,Pengwei Sui,Zhenglun Kong,Sufian Aldogom,Yepeng Huang,Ayush Noori,Reza Shamji,Krishna Parvataneni,Theodoros Tsiligkaridis,Marinka Zitnik*

Main category: cs.AI

TL;DR: 介绍用于构建AI科学家的生态系统ToolUniverse，可集成多种工具，通过案例展示其应用，开源可用。


<details>
  <summary>Details</summary>
Motivation: 现有AI科学家计算系统构建困难，缺乏统一生态系统，而组学中统一生态系统已推动研究发展，因此需要为AI科学家构建类似基础设施。

Method: 提出ToolUniverse生态系统，标准化AI科学家识别和调用工具的方式，集成多种工具，自动优化工具接口等。

Result: 在高胆固醇血症案例中，使用ToolUniverse创建AI科学家识别出具有良好预测特性的药物类似物。

Conclusion: ToolUniverse为构建AI科学家提供了有效的生态系统，具有良好的应用前景，且已开源。

Abstract: AI scientists are emerging computational systems that serve as collaborative
partners in discovery. These systems remain difficult to build because they are
bespoke, tied to rigid workflows, and lack shared environments that unify
tools, data, and analyses into a common ecosystem. In omics, unified ecosystems
have transformed research by enabling interoperability, reuse, and
community-driven development; AI scientists require comparable infrastructure.
We present ToolUniverse, an ecosystem for building AI scientists from any
language or reasoning model, whether open or closed. TOOLUNIVERSE standardizes
how AI scientists identify and call tools, integrating more than 600 machine
learning models, datasets, APIs, and scientific packages for data analysis,
knowledge retrieval, and experimental design. It automatically refines tool
interfaces for correct use by AI scientists, creates new tools from natural
language descriptions, iteratively optimizes tool specifications, and composes
tools into agentic workflows. In a case study of hypercholesterolemia,
ToolUniverse was used to create an AI scientist to identify a potent analog of
a drug with favorable predicted properties. The open-source ToolUniverse is
available at https://aiscientist.tools.

</details>


### [38] [Beyond Embeddings: Interpretable Feature Extraction for Binary Code Similarity](https://arxiv.org/abs/2509.23449)
*Charles E. Gagnon,Steven H. H. Ding,Philippe Charland,Benjamin C. M. Fung*

Main category: cs.AI

TL;DR: 本文提出用基于语言模型的代理对汇编代码进行结构化推理分析以生成特征，解决现有二进制代码相似度检测方法在可解释性、泛化性和可扩展性上的问题，结合嵌入方法显著超越当前最优水平。


<details>
  <summary>Details</summary>
Motivation: 现有二进制代码相似度检测方法在可解释性、泛化性和可扩展性上存在不足，需做出妥协。

Method: 使用基于语言模型的代理对汇编代码进行结构化推理分析，生成如输入/输出类型、副作用、显著常量和算法意图等特征。

Result: 在无匹配训练的情况下，跨架构和跨优化任务的召回率分别达到42%和62%，与有训练的嵌入方法相当；结合嵌入方法显著超越当前最优水平。

Conclusion: 该方法能使准确性、可扩展性和可解释性共存。

Abstract: Binary code similarity detection is a core task in reverse engineering. It
supports malware analysis and vulnerability discovery by identifying
semantically similar code in different contexts. Modern methods have progressed
from manually engineered features to vector representations. Hand-crafted
statistics (e.g., operation ratios) are interpretable, but shallow and fail to
generalize. Embedding-based methods overcome this by learning robust
cross-setting representations, but these representations are opaque vectors
that prevent rapid verification. They also face a scalability-accuracy
trade-off, since high-dimensional nearest-neighbor search requires
approximations that reduce precision. Current approaches thus force a
compromise between interpretability, generalizability, and scalability.
  We bridge these gaps using a language model-based agent to conduct structured
reasoning analysis of assembly code and generate features such as input/output
types, side effects, notable constants, and algorithmic intent. Unlike
hand-crafted features, they are richer and adaptive. Unlike embeddings, they
are human-readable, maintainable, and directly searchable with inverted or
relational indexes. Without any matching training, our method respectively
achieves 42% and 62% for recall@1 in cross-architecture and cross-optimization
tasks, comparable to embedding methods with training (39% and 34%). Combined
with embeddings, it significantly outperforms the state-of-the-art,
demonstrating that accuracy, scalability, and interpretability can coexist.

</details>


### [39] [ViTSP: A Vision Language Models Guided Framework for Large-Scale Traveling Salesman Problems](https://arxiv.org/abs/2509.23465)
*Zhuoli Yin,Yi Ding,Reem Khir,Hua Cai*

Main category: cs.AI

TL;DR: 提出ViTSP框架利用预训练视觉语言模型解决大规模TSP问题，实验效果佳。


<details>
  <summary>Details</summary>
Motivation: 经典精确方法和启发式方法在解决TSP问题有局限，学习方法泛化和扩展性差，需新方法。

Method: 提出ViTSP框架，用预训练视觉语言模型识别小规模子问题，用现成求解器优化。

Result: 在1k到88k节点的TSP实例上平均最优差距低于0.2%，优于现有学习方法，比LKH - 3求解器差距降低12%到100%。

Conclusion: 框架为结合预训练生成模型和运筹学求解器解决组合优化问题提供新视角，可用于复杂物流系统。

Abstract: Solving Traveling Salesman Problem (TSP) is NP-hard yet fundamental for wide
real-world applications. Classical exact methods face challenges in scaling,
and heuristic methods often require domain-specific parameter calibration.
While learning-based approaches have shown promise, they suffer from poor
generalization and limited scalability due to fixed training data. This work
proposes ViTSP, a novel framework that leverages pre-trained vision language
models (VLMs) to visually guide the solution process for large-scale TSPs. The
VLMs function to identify promising small-scale subproblems from a visualized
TSP instance, which are then efficiently optimized using an off-the-shelf
solver to improve the global solution. ViTSP bypasses the dedicated model
training at the user end while maintaining effectiveness across diverse
instances. Experiments on real-world TSP instances ranging from 1k to 88k nodes
demonstrate that ViTSP consistently achieves solutions with average optimality
gaps below 0.2%, outperforming existing learning-based methods. Under the same
runtime budget, it surpasses the best-performing heuristic solver, LKH-3, by
reducing its gaps by 12% to 100%, particularly on very-large-scale instances
with more than 10k nodes. Our framework offers a new perspective in hybridizing
pre-trained generative models and operations research solvers in solving
combinatorial optimization problems, with practical implications for
integration into more complex logistics systems. The code is available at
https://anonymous.4open.science/r/ViTSP_codes-6683.

</details>


### [40] [GeoBS: Information-Theoretic Quantification of Geographic Bias in AI Models](https://arxiv.org/abs/2509.23482)
*Zhangyu Wang,Nemin Wu,Qian Cao,Jiangnan Xia,Zeping Liu,Yiqun Xie,Akshay Nambi,Tanuja Ganu,Ni Lao,Ninghao Liu,Gengchen Mai*

Main category: cs.AI

TL;DR: 本文建立地理偏差评估的信息理论框架 GeoBS，提出三种新的地理偏差分数，通过实验证明多种模型存在地理偏差，该框架有助于提升对地理偏差的理解和将空间公平性融入 AI 系统。


<details>
  <summary>Details</summary>
Motivation: AI 模型广泛应用引发伦理问题，地理偏差受关注少，缺乏通用、空间显式的评估框架来比较不同模型地理偏差及理解空间因素。

Method: 建立 GeoBS 信息理论框架，在框架下解释分析现有地理偏差度量，提出考虑复杂空间因素的三种新地理偏差分数，并在多任务、多数据集和多模型上进行实验。

Result: 实验表明特定任务的 GeoAI 模型和通用基础模型都可能存在各种地理偏差。

Conclusion: 该框架既推动对地理偏差的技术理解，也为将空间公平性融入 AI 系统的设计、部署和评估奠定基础。

Abstract: The widespread adoption of AI models, especially foundation models (FMs), has
made a profound impact on numerous domains. However, it also raises significant
ethical concerns, including bias issues. Although numerous efforts have been
made to quantify and mitigate social bias in AI models, geographic bias (in
short, geo-bias) receives much less attention, which presents unique
challenges. While previous work has explored ways to quantify geo-bias, these
measures are model-specific (e.g., mean absolute deviation of LLM ratings) or
spatially implicit (e.g., average fairness scores of all spatial partitions).
We lack a model-agnostic, universally applicable, and spatially explicit
geo-bias evaluation framework that allows researchers to fairly compare the
geo-bias of different AI models and to understand what spatial factors
contribute to the geo-bias. In this paper, we establish an
information-theoretic framework for geo-bias evaluation, called GeoBS (Geo-Bias
Scores). We demonstrate the generalizability of the proposed framework by
showing how to interpret and analyze existing geo-bias measures under this
framework. Then, we propose three novel geo-bias scores that explicitly take
intricate spatial factors (multi-scalability, distance decay, and anisotropy)
into consideration. Finally, we conduct extensive experiments on 3 tasks, 8
datasets, and 8 models to demonstrate that both task-specific GeoAI models and
general-purpose foundation models may suffer from various types of geo-bias.
This framework will not only advance the technical understanding of geographic
bias but will also establish a foundation for integrating spatial fairness into
the design, deployment, and evaluation of AI systems.

</details>


### [41] [Accurate Predictions in Education with Discrete Variational Inference](https://arxiv.org/abs/2509.23484)
*Tom Quilter,Anastasia Ilick,Anastasia Ilick,Richard Turner*

Main category: cs.AI

TL;DR: 社会不平等源于辅导资源不均，AI 辅导是解决方案。研究聚焦自适应学习预测答题情况，发布数学考试数据集，提出概率建模框架和协作过滤模型，推导离散变分推理框架提升低数据场景预测准确率。


<details>
  <summary>Details</summary>
Motivation: 解决社会因个人辅导资源不均导致的不平等问题，提高自适应学习中对学生答题情况预测的准确率，尤其是在数据稀疏场景下。

Method: 发布最大的数学考试答卷公开数据集；引入基于项目反应理论的概率建模框架；使用结合主题技能配置文件的协作过滤模型；推导并实施新颖的离散变分推理框架。

Result: 概率建模框架达到超 80%准确率；协作过滤模型发现单一潜在能力参数就能实现最大预测准确率；离散变分推理框架在低数据场景下达到最高预测准确率，优于经典基线。

Conclusion: 提出的方法有效提高了数学考试答卷预测的准确率，离散变分推理框架在低数据场景下表现出色。

Abstract: One of the largest drivers of social inequality is unequal access to personal
tutoring, with wealthier individuals able to afford it, while the majority
cannot. Affordable, effective AI tutors offer a scalable solution. We focus on
adaptive learning, predicting whether a student will answer a question
correctly, a key component of any effective tutoring system. Yet many platforms
struggle to achieve high prediction accuracy, especially in data-sparse
settings. To address this, we release the largest open dataset of
professionally marked formal mathematics exam responses to date. We introduce a
probabilistic modelling framework rooted in Item Response Theory (IRT) that
achieves over 80 percent accuracy, setting a new benchmark for mathematics
prediction accuracy of formal exam papers. Extending this, our collaborative
filtering models incorporate topic-level skill profiles, but reveal a
surprising and educationally significant finding, a single latent ability
parameter alone is needed to achieve the maximum predictive accuracy. Our main
contribution though is deriving and implementing a novel discrete variational
inference framework, achieving our highest prediction accuracy in low-data
settings and outperforming all classical IRT and matrix factorisation
baselines.

</details>


### [42] [Mapping Overlaps in Benchmarks through Perplexity in the Wild](https://arxiv.org/abs/2509.23488)
*Siyang Wu,Honglin Bao,Sida Li,Ari Holtzman,James A. Evans*

Main category: cs.AI

TL;DR: 本文提出基准签名方法分析大语言模型基准测试，发现其能捕捉差异，并揭示基准有效性和模型敏感性等。


<details>
  <summary>Details</summary>
Motivation: 刻画大语言模型基准测试及其有意义的重叠。

Method: 通过跨32个大语言模型和88个基准的线性回归逐步向前选择提取基准签名。

Result: 基准签名能捕捉差异、重叠和分歧，性能受基准正交因素影响，签名对此影响有鲁棒性。

Conclusion: 研究为基准有效性和模型敏感性提供机制见解，勾勒模型能力相互关联的潜在图景。

Abstract: We develop signatures of capacity familiarity to characterize large language
model (LLM) benchmarks and their meaningful overlaps. Benchmark signatures
probe the capacity required for benchmark performance. We formally define them
as a set of salient tokens drawn from in-the-wild, naturally authored corpora,
where LLM token perplexity, reflecting more or less pre-training exposure,
becomes highly predictive of LLM benchmark performance. Through a large-scale
meta-evaluation, we extract benchmark signatures via stepwise forward selection
with linear regressions across 32 LLMs and 88 benchmarks spanning diverse
knowledge, coding, logic, instruction following, math, language, reasoning, and
world modeling. Our analysis situates signatures in relation to both the
semantic similarity of benchmark questions and the correlation of model
performance. While performance overlaps are universally high and semantic
overlaps remain confined to a narrow mid-range, benchmark signatures prove
highly informative in capturing variation, overlap, and divergence. We observe
overlap in knowledge and reasoning subtasks, whereas multilingual and cultural
benchmarks exhibit less similarity, even compared to cross-task overlap.
Notably, performance-level results are strongly influenced by
benchmark-orthogonal factors such as question format, highlighting limitations
in LLM generalization, the conflation of performance with ability, and issues
inherent in current mainstream benchmark agreement studies. Benchmark
signatures, however, remain robust to such effects. Ultimately, we identify
cross-functional overlaps across logic, math, language, instruction following,
and world modeling, with coding emerging as the least overlapping domain.
Together, these findings provide mechanistic insights into benchmark validity
and LLM sensitivities, and sketch the underlying landscape of interconnected
LLM capabilities.

</details>


### [43] [Dynamic Trust Calibration Using Contextual Bandits](https://arxiv.org/abs/2509.23497)
*Bruno M. Henrique,Eugene Santos Jr*

Main category: cs.AI

TL;DR: 提出动态信任校准新方法，用上下文多臂老虎机算法，经三个数据集评估，有效校准信任可提升决策性能，为开发可信AI系统提供指导。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏衡量人与AI信任校准的明确客观方法，现有方法缺乏标准化和通用指标，未区分观点形成和后续决策。

Method: 提出动态信任校准新方法，引入标准化信任校准度量和指标，利用上下文多臂老虎机算法根据学习到的上下文信息动态评估何时信任AI贡献。

Result: 在三个不同数据集上评估，有效信任校准使决策性能显著提升，奖励指标提高10 - 38%。

Conclusion: 研究增强了理论理解，为关键领域开发更可信的AI决策支持系统提供了实践指导。

Abstract: Trust calibration between humans and Artificial Intelligence (AI) is crucial
for optimal decision-making in collaborative settings. Excessive trust can lead
users to accept AI-generated outputs without question, overlooking critical
flaws, while insufficient trust may result in disregarding valuable insights
from AI systems, hindering performance. Despite its importance, there is
currently no definitive and objective method for measuring trust calibration
between humans and AI. Current approaches lack standardization and consistent
metrics that can be broadly applied across various contexts, and they don't
distinguish between the formation of opinions and subsequent human decisions.
In this work, we propose a novel and objective method for dynamic trust
calibration, introducing a standardized trust calibration measure and an
indicator. By utilizing Contextual Bandits-an adaptive algorithm that
incorporates context into decision-making-our indicator dynamically assesses
when to trust AI contributions based on learned contextual information. We
evaluate this indicator across three diverse datasets, demonstrating that
effective trust calibration results in significant improvements in
decision-making performance, as evidenced by 10 to 38% increase in reward
metrics. These findings not only enhance theoretical understanding but also
provide practical guidance for developing more trustworthy AI systems
supporting decisions in critical domains, for example, disease diagnoses and
criminal justice.

</details>


### [44] [Model Consistency as a Cheap yet Predictive Proxy for LLM Elo Scores](https://arxiv.org/abs/2509.23510)
*Ashwin Ramaswamy,Nestor Demeure,Ermal Rrapaj*

Main category: cs.AI

TL;DR: 提出一种无需人类数据和先验知识、低成本计算的大语言模型Elo分数代理指标。


<details>
  <summary>Details</summary>
Motivation: 新大语言模型不断发布，需要独立评估方法，当前评估模型的最佳方法成本高。

Method: 观察大语言模型在评判比赛时选择最佳模型的一致性，以此生成与人类产生的Elo分数有91%相关性的指标。

Result: 得到的指标与人类产生的Elo分数有91%的相关性。

Conclusion: 该指标可作为Elo分数的简单代理，能低成本计算。

Abstract: New large language models (LLMs) are being released every day. Some perform
significantly better or worse than expected given their parameter count.
Therefore, there is a need for a method to independently evaluate models. The
current best way to evaluate a model is to measure its Elo score by comparing
it to other models in a series of contests - an expensive operation since
humans are ideally required to compare LLM outputs. We observe that when an LLM
is asked to judge such contests, the consistency with which it selects a model
as the best in a matchup produces a metric that is 91% correlated with its own
human-produced Elo score. This provides a simple proxy for Elo scores that can
be computed cheaply, without any human data or prior knowledge.

</details>


### [45] [DOoM: Difficult Olympiads of Math](https://arxiv.org/abs/2509.23529)
*Ilya Kuleshov,Ilin Pavel,Nikolay Kompanets,Ksenia Sycheva,Aleksandr Nikolich*

Main category: cs.AI

TL;DR: 本文介绍俄语数学和物理问题基准DOoM，包括不同难度问题，讨论创建动机、数据集结构和评估方法并给出测试结果。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型解决俄语数学和物理问题的能力。

Method: 创建包含不同难度问题的DOoM基准，采用相应评估方法对不同模型进行测试。

Result: 模型性能与使用的标记数量相关，数学和物理任务的性能存在差异。

Conclusion: 未明确提及，但通过结果可了解模型在解决数学和物理问题上的表现与相关因素的联系。

Abstract: This paper introduces DOoM, a new open-source benchmark designed to assess
the capabilities of language models in solving mathematics and physics problems
in Russian. The benchmark includes problems of varying difficulty, ranging from
school-level tasks to university Olympiad and entrance exam questions. In this
paper we discuss the motivation behind its creation, describe dataset's
structure and evaluation methodology, and present initial results from testing
various models. Analysis of the results shows a correlation between model
performance and the number of tokens used, and highlights differences in
performance between mathematics and physics tasks.

</details>


### [46] [Diagnosing Failure Root Causes in Platform-Orchestrated Agentic Systems: Dataset, Taxonomy, and Benchmark](https://arxiv.org/abs/2509.23735)
*Xuyan Ma,Xiaofei Xie,Yawen Wang,Junjie Wang,Boyu Wu,Mingyang Li,Qing Wang*

Main category: cs.AI

TL;DR: 本文研究平台编排的智能体系统故障根源识别，构建数据集AgentFail，提出分类法和基准测试，虽分类法提升性能但任务仍具挑战，还给出构建系统的指导方针。


<details>
  <summary>Details</summary>
Motivation: 平台编排的智能体系统易出错，且缺乏系统识别潜在故障根源的方法。

Method: 构建含307条故障日志的数据集AgentFail并精细标注，用反事实推理修复策略确保标注可靠性；基于数据集开发故障根源分类法；引入利用大语言模型自动识别根源的基准测试。

Result: 分类法能大幅提升性能，但根源识别准确率最高仅33.6%。

Conclusion: 本文为平台编排的智能体系统提供可靠的故障根源数据集、分类法和基准测试，为开发更可靠系统奠定基础。

Abstract: Agentic systems consisting of multiple LLM-driven agents coordinating through
tools and structured interactions, are increasingly deployed for complex
reasoning and problem-solving tasks. At the same time, emerging low-code and
template-based agent development platforms (e.g., Dify) enable users to rapidly
build and orchestrate agentic systems, which we refer to as
platform-orchestrated agentic systems. However, these systems are also fragile
and it remains unclear how to systematically identify their potential failure
root cause. This paper presents a study of root cause identification of these
platform-orchestrated agentic systems. To support this initiative, we construct
a dataset AgentFail containing 307 failure logs from ten agentic systems, each
with fine-grained annotations linking failures to their root causes. We
additionally utilize counterfactual reasoning-based repair strategy to ensure
the reliability of the annotation. Building on the dataset, we develop a
taxonomy that characterizes failure root causes and analyze their distribution
across different platforms and task domains. Furthermore, we introduce a
benchmark that leverages LLMs for automatically identifying root causes, in
which we also utilize the proposed taxonomy as guidance for LLMs. Results show
that the taxonomy can largely improve the performance, thereby confirming its
utility. Nevertheless, the accuracy of root cause identification reaches at
most 33.6%, which indicates that this task still remains challenging. In light
of these results, we also provide actionable guidelines for building such
agentic systems. In summary, this paper provides a reliable dataset of failure
root cause for platform-orchestrated agentic systems, corresponding taxonomy
and benchmark, which serves as a foundation for advancing the development of
more reliable agentic systems.

</details>


### [47] [Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks](https://arxiv.org/abs/2509.23537)
*Aaron Xuxiang Tian,Ruofan Zhang,Jiayao Tang,Young Min Cho,Xueqian Li,Qiang Yi,Ji Wang,Zhunping Zhang,Danrui Qi,Sharath Chandra Guntuku,Lyle Ungar,Tianyu Shi,Chi Wang*

Main category: cs.AI

TL;DR: 研究多轮多智能体编排，对比单LLM基线并进行消融实验，编排表现佳且有提升潜力，消融实验揭示信息展示影响。


<details>
  <summary>Details</summary>
Motivation: 研究多轮多智能体编排，提升模型表现。

Method: 使用四个大语言模型在多个数据集上进行对比单LLM基线的实验和消融实验。

Result: 编排表现与最强单模型相当或更优，消融实验显示揭示作者身份和投票情况有不同影响。

Conclusion: 多轮多智能体编排有潜力进一步提升性能，信息展示会影响编排结果。

Abstract: We study multi-turn multi-agent orchestration, where multiple large language
model (LLM) agents interact over multiple turns by iteratively proposing
answers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5
Pro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we
conduct two experiments: (i) benchmarking orchestration against single-LLM
baselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who
authored answers and whether they can observe ongoing votes. Orchestration
matches or exceeds the strongest single model and consistently outperforms the
others. Analysis of best-achievable orchestration performance shows potential
for further gains. The ablations show that revealing authorship increases
self-voting and ties, and that showing ongoing votes amplifies herding, which
speeds convergence but can sometimes yield premature consensus.

</details>


### [48] [AgentGuard: Runtime Verification of AI Agents](https://arxiv.org/abs/2509.23864)
*Roham Koohestani*

Main category: cs.AI

TL;DR: 论文针对自主智能体AI系统风险及传统验证方法不足，提出AgentGuard框架，用动态概率保证范式进行运行时验证。


<details>
  <summary>Details</summary>
Motivation: 自主智能体AI系统的不可预测性和涌现行为带来重大风险，传统验证方法不足，需转向概率保证。

Method: 提出AgentGuard框架，通过动态概率保证范式，观察智能体原始输入输出并抽象为形式事件，用在线学习动态构建和更新马尔可夫决策过程，再用概率模型检查实时验证定量属性。

Result: 未提及具体结果

Conclusion: 未提及明确结论

Abstract: The rapid evolution to autonomous, agentic AI systems introduces significant
risks due to their inherent unpredictability and emergent behaviors; this also
renders traditional verification methods inadequate and necessitates a shift
towards probabilistic guarantees where the question is no longer if a system
will fail, but the probability of its failure within given constraints. This
paper presents AgentGuard, a framework for runtime verification of Agentic AI
systems that provides continuous, quantitative assurance through a new paradigm
called Dynamic Probabilistic Assurance. AgentGuard operates as an inspection
layer that observes an agent's raw I/O and abstracts it into formal events
corresponding to transitions in a state model. It then uses online learning to
dynamically build and update a Markov Decision Process (MDP) that formally
models the agent's emergent behavior. Using probabilistic model checking, the
framework then verifies quantitative properties in real-time.

</details>


### [49] [Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning](https://arxiv.org/abs/2509.23558)
*Zhaoqi Wang,Daqing He,Zijian Zhang,Xin Li,Liehuang Zhu,Meng Li,Jiamou Liu*

Main category: cs.AI

TL;DR: 提出PASS框架挖掘大语言模型对齐方法漏洞，通过强化学习和GraphRAG系统进行越狱攻击，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型带来新安全挑战，如提示越狱攻击，需挖掘其对齐方法漏洞。

Method: 提出PASS框架，用强化学习将初始越狱提示转化为形式化描述，将越狱输出构建到GraphRAG系统。

Result: 在常见开源模型上进行大量实验，证明攻击有效。

Conclusion: PASS框架能有效挖掘大语言模型对齐方法漏洞，实现越狱攻击。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, yet
they also introduce novel security challenges. For instance, prompt
jailbreaking attacks involve adversaries crafting sophisticated prompts to
elicit responses from LLMs that deviate from human values. To uncover
vulnerabilities in LLM alignment methods, we propose the PASS framework
(\underline{P}rompt J\underline{a}ilbreaking via \underline{S}emantic and
\underline{S}tructural Formalization). Specifically, PASS employs reinforcement
learning to transform initial jailbreak prompts into formalized descriptions,
which enhances stealthiness and enables bypassing existing alignment defenses.
The jailbreak outputs are then structured into a GraphRAG system that, by
leveraging extracted relevant terms and formalized symbols as contextual input
alongside the original query, strengthens subsequent attacks and facilitates
more effective jailbreaks. We conducted extensive experiments on common
open-source models, demonstrating the effectiveness of our attack.

</details>


### [50] [A Hierarchical Structure-Enhanced Personalized Recommendation Model for Traditional Chinese Medicine Formulas Based on KG Diffusion Guidance](https://arxiv.org/abs/2509.23560)
*ChaoBo Zhang,Long Tan*

Main category: cs.AI

TL;DR: 提出基于知识图谱扩散引导的中医方剂个性化推荐模型TCM - HEDPR，实验证明有效，可为现代中医推荐提供新范式。


<details>
  <summary>Details</summary>
Motivation: 以往中医方剂推荐研究存在未充分考虑患者个性化信息、草药数据长尾分布及草药配伍关系等局限，影响模型性能。

Method: 用患者个性化提示序列预训练症状表示并进行对比学习，采用KG引导的同质图扩散方法结合自注意力机制，设计异质图分层网络。

Result: 在两个公共数据集和一个临床数据集上实验证明了TCM - HEDPR的有效性。

Conclusion: 该模型能综合评估推荐处方，为现代中医推荐提供新范式。

Abstract: Artificial intelligence technology plays a crucial role in recommending
prescriptions for traditional Chinese medicine (TCM). Previous studies have
made significant progress by focusing on the symptom-herb relationship in
prescriptions. However, several limitations hinder model performance: (i)
Insufficient attention to patient-personalized information such as age, BMI,
and medical history, which hampers accurate identification of syndrome and
reduces efficacy. (ii) The typical long-tailed distribution of herb data
introduces training biases and affects generalization ability. (iii) The
oversight of the 'monarch, minister, assistant and envoy' compatibility among
herbs increases the risk of toxicity or side effects, opposing the 'treatment
based on syndrome differentiation' principle in clinical TCM. Therefore, we
propose a novel hierarchical structure-enhanced personalized recommendation
model for TCM formulas based on knowledge graph diffusion guidance, namely
TCM-HEDPR. Specifically, we pre-train symptom representations using
patient-personalized prompt sequences and apply prompt-oriented contrastive
learning for data augmentation. Furthermore, we employ a KG-guided homogeneous
graph diffusion method integrated with a self-attention mechanism to globally
capture the non-linear symptom-herb relationship. Lastly, we design a
heterogeneous graph hierarchical network to integrate herbal dispensing
relationships with implicit syndromes, guiding the prescription generation
process at a fine-grained level and mitigating the long-tailed herb data
distribution problem. Extensive experiments on two public datasets and one
clinical dataset demonstrate the effectiveness of TCM-HEDPR. In addition, we
incorporate insights from modern medicine and network pharmacology to evaluate
the recommended prescriptions comprehensively. It can provide a new paradigm
for the recommendation of modern TCM.

</details>


### [51] [Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment](https://arxiv.org/abs/2509.23564)
*Min-Hsuan Yeh,Yixuan Li*

Main category: cs.AI

TL;DR: 本文提出首个评估大语言模型对齐中偏好数据清洗方法的综合基准PrefCleanBench，揭示数据清洗成功的关键因素并发布代码。


<details>
  <summary>Details</summary>
Motivation: 人类反馈有噪声和不一致性，影响奖励模型质量和对齐效果，缺乏对自动化数据清洗方法有效性和泛化性的系统评估。

Method: 引入PrefCleanBench，提供标准化协议评估13种偏好数据清洗方法，统一不同方法并严格比较。

Result: 揭示了决定对齐任务中数据清洗成功的关键因素。

Conclusion: 该基准为通过提升数据质量改善大语言模型对齐奠定基础，凸显数据预处理在负责任AI发展中的重要但未充分探索的作用。

Abstract: Human feedback plays a pivotal role in aligning large language models (LLMs)
with human preferences. However, such feedback is often noisy or inconsistent,
which can degrade the quality of reward models and hinder alignment. While
various automated data cleaning methods have been proposed to mitigate this
issue, a systematic evaluation of their effectiveness and generalizability
remains lacking. To bridge this gap, we introduce the first comprehensive
benchmark for evaluating 13 preference data cleaning methods in the context of
LLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning
strategies in terms of alignment performance and generalizability across
diverse datasets, model architectures, and optimization algorithms. By unifying
disparate methods and rigorously comparing them, we uncover key factors that
determine the success of data cleaning in alignment tasks. This benchmark lays
the groundwork for principled and reproducible approaches to improving LLM
alignment through better data quality-highlighting the crucial but
underexplored role of data preprocessing in responsible AI development. We
release modular implementations of all methods to catalyze further research:
https://github.com/deeplearning-wisc/PrefCleanBench.

</details>


### [52] [A Systematic Review of Digital Twin-Driven Predictive Maintenance in Industrial Engineering: Taxonomy, Architectural Elements, and Future Research Directions](https://arxiv.org/abs/2509.24443)
*Leila Ismail,Abdelmoneim Abdelmoti,Arkaprabha Basu,Aymen Dia Eddine Berini,Mohammad Naouss*

Main category: cs.AI

TL;DR: 工业系统日益复杂，预测性维护需求迫切。本文回顾数字孪生在工业工程预测性维护中的发展，给出架构和分类，探讨未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 工业系统复杂度增加，传统维护方式不足，数字孪生虽有进展但研究存在空白，需深入研究。

Method: 对数字孪生在工业工程预测性维护中的时间演变进行回顾分析，提供分层架构、分类等。

Result: 给出数字孪生技术的分层架构、技术使能的工业工程应用系统等分类，为实现智能数字孪生工业工程生态系统提供见解。

Conclusion: 探讨了数字孪生在工业工程预测性维护中的未来研究方向。

Abstract: With the increasing complexity of industrial systems, there is a pressing
need for predictive maintenance to avoid costly downtime and disastrous
outcomes that could be life-threatening in certain domains. With the growing
popularity of the Internet of Things, Artificial Intelligence, machine
learning, and real-time big data analytics, there is a unique opportunity for
efficient predictive maintenance to forecast equipment failures for real-time
intervention and optimize maintenance actions, as traditional reactive and
preventive maintenance practices are often inadequate to meet the requirements
for the industry to provide quality-of-services of operations. Central to this
evolution is digital twin technology, an adaptive virtual replica that
continuously monitors and integrates sensor data to simulate and improve asset
performance. Despite remarkable progress in digital twin implementations, such
as considering DT in predictive maintenance for industrial engineering. This
paper aims to address this void. We perform a retrospective analysis of the
temporal evolution of the digital twin in predictive maintenance for industrial
engineering to capture the applications, middleware, and technological
requirements that led to the development of the digital twin from its inception
to the AI-enabled digital twin and its self-learning models. We provide a
layered architecture of the digital twin technology, as well as a taxonomy of
the technology-enabled industrial engineering applications systems, middleware,
and the used Artificial Intelligence algorithms. We provide insights into these
systems for the realization of a trustworthy and efficient smart digital-twin
industrial engineering ecosystem. We discuss future research directions in
digital twin for predictive maintenance in industrial engineering.

</details>


### [53] [BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning in Autonomous Driving](https://arxiv.org/abs/2509.23589)
*Shu Liu,Wenlin Chen,Weihao Li,Zheng Wang,Lijin Yang,Jianing Huang,Yipin Zhang,Zhongzhan Huang,Ze Cheng,Hao Yang*

Main category: cs.AI

TL;DR: 本文提出BridgeDrive，一种用于闭环轨迹规划的锚引导扩散桥策略，在Bench2Drive基准测试中实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的规划器在反应式闭环环境中有效引导存在挑战，简单条件作用不足，现有使用典型专家驾驶行为引导的方法有理论不一致和性能问题。

Method: 引入BridgeDrive，一种新颖的锚引导扩散桥策略，构建有效将锚转化为细粒度轨迹计划的扩散框架，且与高效ODE求解器兼容。

Result: 在Bench2Drive基准测试中达到了SOTA性能，成功率比现有方法提高了5%。

Conclusion: 所提出的BridgeDrive能有效解决基于扩散的规划器在闭环环境中的引导问题，适用于实时自动驾驶部署。

Abstract: Diffusion-based planners have shown great promise for autonomous driving due
to their ability to capture multi-modal driving behaviors. However, guiding
these models effectively in reactive, closed-loop environments remains a
significant challenge. Simple conditioning often fails to provide sufficient
guidance in complex and dynamic driving scenarios. Recent work attempts to use
typical expert driving behaviors (i.e., anchors) to guide diffusion models but
relies on a truncated schedule, which introduces theoretical inconsistencies
and can compromise performance. To address this, we introduce BridgeDrive, a
novel anchor-guided diffusion bridge policy for closed-loop trajectory
planning. Our approach provides a principled diffusion framework that
effectively translates anchors into fine-grained trajectory plans,
appropriately responding to varying traffic conditions. Our planner is
compatible with efficient ODE solvers, a critical factor for real-time
autonomous driving deployment. We achieve state-of-the-art performance on the
Bench2Drive benchmark, improving the success rate by 5% over prior arts.

</details>


### [54] [BPMN Assistant: An LLM-Based Approach to Business Process Modeling](https://arxiv.org/abs/2509.24592)
*Josip Tomo Licardo,Nikola Tankovic,Darko Etinger*

Main category: cs.AI

TL;DR: 本文介绍了利用大语言模型创建和编辑BPMN图的工具BPMN Assistant，对比JSON和XML处理方式，展示JSON优势并讨论改进。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型实现基于自然语言的BPMN图创建和编辑，改进处理方式以提升准确性。

Method: 引入基于JSON的表示法替代XML，用GED和RGED评估生成质量，用二元成功指标评估编辑性能。

Result: JSON和XML在生成时相似度得分相近，但JSON可靠性更高、处理更快、编辑成功率显著更高。

Conclusion: 讨论了关键权衡、局限性和未来改进方向，代码开源。

Abstract: This paper presents BPMN Assistant, a tool that leverages Large Language
Models (LLMs) for natural language-based creation and editing of BPMN diagrams.
A specialized JSON-based representation is introduced as a structured
alternative to the direct handling of XML to enhance the accuracy of process
modifications. Process generation quality is evaluated using Graph Edit
Distance (GED) and Relative Graph Edit Distance (RGED), while editing
performance is evaluated with a binary success metric. Results show that JSON
and XML achieve similar similarity scores in generation, but JSON offers
greater reliability, faster processing, and significantly higher editing
success rates. We discuss key trade-offs, limitations, and future improvements.
The implementation is available at https://github.com/jtlicardo/bpmn-assistant.

</details>


### [55] [PSG-Agent: Personality-Aware Safety Guardrail for LLM-based Agents](https://arxiv.org/abs/2509.23614)
*Yaozu Wu,Jizhou Guo,Dongyuan Li,Henry Peng Zou,Wei-Chieh Huang,Yankai Chen,Zhen Wang,Weizhi Zhang,Yangning Li,Meng Zhang,Renhe Jiang,Philip S. Yu*

Main category: cs.AI

TL;DR: 提出PSG - Agent系统解决现有大语言模型代理护栏的局限性，经多场景验证效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理的护栏存在对所有用户采用统一策略、孤立检查响应的局限性，需要改进。

Method: 创建个性化护栏，通过挖掘交互历史和捕获实时状态生成用户特定风险阈值和保护策略；在代理管道中进行连续监控，设置多种专门的防护措施跟踪跨轮次风险积累并给出可验证的裁决。

Result: 在医疗、金融和日常生活自动化等多场景中验证，PSG - Agent显著优于现有代理护栏如LlamaGuard3和AGrail。

Conclusion: PSG - Agent为大语言模型代理的个性化安全提供了可执行和可审计的途径。

Abstract: Effective guardrails are essential for safely deploying LLM-based agents in
critical applications. Despite recent advances, existing guardrails suffer from
two fundamental limitations: (i) they apply uniform guardrail policies to all
users, ignoring that the same agent behavior can harm some users while being
safe for others; (ii) they check each response in isolation, missing how risks
evolve and accumulate across multiple interactions. To solve these issues, we
propose PSG-Agent, a personalized and dynamic system for LLM-based agents.
First, PSG-Agent creates personalized guardrails by mining the interaction
history for stable traits and capturing real-time states from current queries,
generating user-specific risk thresholds and protection strategies. Second,
PSG-Agent implements continuous monitoring across the agent pipeline with
specialized guards, including Plan Monitor, Tool Firewall, Response Guard,
Memory Guardian, that track cross-turn risk accumulation and issue verifiable
verdicts. Finally, we validate PSG-Agent in multiple scenarios including
healthcare, finance, and daily life automation scenarios with diverse user
profiles. It significantly outperform existing agent guardrails including
LlamaGuard3 and AGrail, providing an executable and auditable path toward
personalized safety for LLM-based agents.

</details>


### [56] [When Autonomous Vehicle Meets V2X Cooperative Perception: How Far Are We?](https://arxiv.org/abs/2509.24927)
*An Guo,Shuoxiao Zhang,Enyi Tang,Xinyu Gao,Haomin Pang,Haoxiang Tian,Yanzhou Mu,Wu Wen,Chunrong Fang,Zhenyu Chen*

Main category: cs.AI

TL;DR: 本文对V2X协同感知进行实证研究，分析六种错误模式，评估关键组件，揭示潜在风险，以促进协同感知系统设计和修复。


<details>
  <summary>Details</summary>
Motivation: V2X协同感知系统组成复杂有诸多操作挑战，且错误类型及原因探索不足，需进行研究。

Method: 识别并分析协同感知系统六种常见错误模式，开展大规模研究对系统关键组件进行系统评估。

Result: 基于LiDAR的合作配置感知性能最高；V2I和V2V通信在不同融合方案下性能不同；协同感知错误增加可能导致更多驾驶违规；协同感知系统在线运行时抗通信干扰能力弱。

Conclusion: 研究结果揭示了协同感知系统关键组件潜在风险和漏洞，有助于推动系统设计和修复。

Abstract: With the tremendous advancement of deep learning and communication
technology, Vehicle-to-Everything (V2X) cooperative perception has the
potential to address limitations in sensing distant objects and occlusion for a
single-agent perception system. V2X cooperative perception systems are software
systems characterized by diverse sensor types and cooperative agents, varying
fusion schemes, and operation under different communication conditions.
Therefore, their complex composition gives rise to numerous operational
challenges. Furthermore, when cooperative perception systems produce erroneous
predictions, the types of errors and their underlying causes remain
insufficiently explored. To bridge this gap, we take an initial step by
conducting an empirical study of V2X cooperative perception. To systematically
evaluate the impact of cooperative perception on the ego vehicle's perception
performance, we identify and analyze six prevalent error patterns in
cooperative perception systems. We further conduct a systematic evaluation of
the critical components of these systems through our large-scale study and
identify the following key findings: (1) The LiDAR-based cooperation
configuration exhibits the highest perception performance; (2)
Vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication
exhibit distinct cooperative perception performance under different fusion
schemes; (3) Increased cooperative perception errors may result in a higher
frequency of driving violations; (4) Cooperative perception systems are not
robust against communication interference when running online. Our results
reveal potential risks and vulnerabilities in critical components of
cooperative perception systems. We hope that our findings can better promote
the design and repair of cooperative perception systems.

</details>


### [57] [Reasoning Scaffolding: Distilling the Flow of Thought from LLMs](https://arxiv.org/abs/2509.23619)
*Xiangyu Wen,Junhua Huang,Zeju Li,Min Li,Jianyuan Zhong,Zhijian Xu,Mingxuan Yuan,Yongxiang Huang,Qiang Xu*

Main category: cs.AI

TL;DR: 现有从大语言模型蒸馏推理能力的方法有局限，提出推理脚手架框架，在推理基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有从大语言模型蒸馏推理到小语言模型的行为克隆方法只能让小模型模仿表面模式，缺乏逻辑鲁棒性，需直接转移算法结构。

Method: 引入推理脚手架框架，将推理重构为结构化生成过程，先将教师思维过程抽象为离散语义信号作为脚手架，学生模型通过多任务目标训练，包括预测下一信号和生成对应步骤。

Result: 在一系列具有挑战性的推理基准测试中，该方法在准确性和逻辑一致性上显著优于现有蒸馏方法。

Conclusion: 该方法为创建真正具备推理能力而非仅能流利模仿的小模型提供了途径。

Abstract: The prevailing approach to distilling reasoning from Large Language Models
(LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It
teaches Small Language Models (SLMs) to mimic surface-level patterns rather
than the underlying algorithmic structure of thought, resulting in a critical
lack of logical robustness. We argue that instead of cloning text, distillation
should transfer this algorithmic structure directly. We introduce Reasoning
Scaffolding}, a framework that reframes reasoning as a structured generation
process. Our method first abstracts the teacher's thought process into a
sequence of discrete, interpretable semantic signals (e.g., Contrast, Addition)
that act as a scaffold. The student model is then trained via a multi-task
objective to both (1)predict the next semantic signal, anticipating the
reasoning flow, and (2)generate the corresponding step, conditioned on that
signal. This multi-task scheme acts as a powerful regularizer, compelling the
student to internalize the computational patterns of coherent reasoning. On a
suite of challenging reasoning benchmarks, our method significantly outperforms
state-of-the-art distillation in both accuracy and logical consistency,
providing a path towards creating smaller models that are genuine reasoners,
not just fluent mimics.

</details>


### [58] [How LLMs Learn to Reason: A Complex Network Perspective](https://arxiv.org/abs/2509.23629)
*Sihan Hu,Xiansheng Cai,Yuan Huang,Zhiyuan Yao,Linfeng Zhang,Pan Zhang,Youjin Deng,Kun Chen*

Main category: cs.AI

TL;DR: 本文提出统一理论解释RLVR训练大语言模型的奇怪现象，提出Annealed - RLVR算法，实验证明其优于标准RLVR，为AI系统推理能力工程化提供新物理直觉。


<details>
  <summary>Details</summary>
Motivation: 解释RLVR训练大语言模型时出现的如两阶段学习曲线等奇怪现象。

Method: 提出模型推理过程映射到语义复杂网络自组织的理论，基于此提出Annealed - RLVR算法。

Result: 在15亿参数模型实验中，Annealed - RLVR在分布内和分布外基准测试中均优于标准RLVR。

Conclusion: 将RLVR从黑盒优化转变为可预测的结构自组织过程，为未来AI系统推理能力工程化提供新物理直觉。

Abstract: Training large language models with Reinforcement Learning from Verifiable
Rewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain
poorly understood, including a two-stage learning curve, V-shaped
response-length trajectories, and a pronounced vulnerability to catastrophic
forgetting. In this work, we propose that these seemingly disparate phenomena
can be explained using a single unifying theory: the model's reasoning process
maps to the self-organization of a semantic complex network whose topology
remains persistently sparse, with the average degree pinned close to two. This
topology imposes a fundamental mechanism for forgetting and learning: it first
drives the system into a maximally frustrated state where ``skill islands''
form, slow-learning happens, and forgetting is induced; then it enters a sharp
growth phase where the new skills are ``bolted on'', driven by
phase-transition-like learning at the web's frontier. Equipped with the theory,
we propose \textit{Annealed-RLVR}, a principled algorithm that introduces an
SFT-based ``heating'' step at the point of maximal frustration to resolve the
competitive bottleneck and enhance the reasoning capability of the model.
Experiments on a 1.5B-parameter model demonstrate that the approach outperforms
standard RLVR on both in-distribution and out-of-distribution benchmarks. By
recasting RLVR from black-box optimization into a predictable process of
structural self-organization, our work provides a new physical intuition for
engineering the emergent reasoning capabilities of future AI systems.

</details>


### [59] [Game-Oriented ASR Error Correction via RAG-Enhanced LLM](https://arxiv.org/abs/2509.23630)
*Yan Jiang,Yongle Luo,Qixian Zhou,Elvis S. Liu*

Main category: cs.AI

TL;DR: 提出GO - AEC框架解决游戏场景ASR系统问题，实验显示提升了准确率


<details>
  <summary>Details</summary>
Motivation: 通用ASR系统在游戏场景存在短语句、快速语音、行话和噪音等挑战，导致频繁出错，需要改进

Method: 提出GO - AEC框架，集成大语言模型、RAG，采用基于LLMs和TTS的数据增强策略，包含数据增强、基于N - best假设的校正和动态游戏知识库

Result: GO - AEC使字符错误率降低6.22%，句子错误率降低29.71%

Conclusion: GO - AEC显著提高了游戏场景中ASR的准确性

Abstract: With the rise of multiplayer online games, real-time voice communication is
essential for team coordination. However, general ASR systems struggle with
gaming-specific challenges like short phrases, rapid speech, jargon, and noise,
leading to frequent errors. To address this, we propose the GO-AEC framework,
which integrates large language models, Retrieval-Augmented Generation (RAG),
and a data augmentation strategy using LLMs and TTS. GO-AEC includes data
augmentation, N-best hypothesis-based correction, and a dynamic game knowledge
base. Experiments show GO-AEC reduces character error rate by 6.22% and
sentence error rate by 29.71%, significantly improving ASR accuracy in gaming
scenarios.

</details>


### [60] [From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models](https://arxiv.org/abs/2509.23676)
*Jue Zhang,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: 研究蒸馏的DeepSeek R1模型中推理与答案生成的相互作用，发现推理能提升答案质量，信息从推理流向答案。


<details>
  <summary>Details</summary>
Motivation: 探究Large Reasoning Models（LRMs）中推理痕迹对答案生成的影响程度。

Method: 分三阶段研究，包括实证评估、注意力分析和使用激活修补进行机制干预。

Result: 明确推理能提升答案质量，答案标记关注推理标记，扰动关键推理标记可改变最终答案。

Conclusion: 加深了对LRMs如何利用推理标记生成答案的理解，强调中间推理对模型输出的作用。

Abstract: Large Reasoning Models (LRMs) generate explicit reasoning traces alongside
final answers, yet the extent to which these traces influence answer generation
remains unclear. In this work, we conduct a three-stage investigation into the
interplay between reasoning and answer generation in three distilled DeepSeek
R1 models. First, through empirical evaluation, we demonstrate that including
explicit reasoning consistently improves answer quality across diverse domains.
Second, attention analysis reveals that answer tokens attend substantially to
reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely
tracking the reasoning trajectory, including self-reflective cues. Third, we
apply mechanistic interventions using activation patching to assess the
dependence of answer tokens on reasoning activations. Our results show that
perturbations to key reasoning tokens can reliably alter the final answers,
confirming a directional and functional flow of information from reasoning to
answer. These findings deepen our understanding of how LRMs leverage reasoning
tokens for answer generation, highlighting the functional role of intermediate
reasoning in shaping model outputs. Our data and code are publicly available at
\href{https://aka.ms/R2A-code}{this URL}.

</details>


### [61] [SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents](https://arxiv.org/abs/2509.23694)
*Jianshuo Dong,Sheng Guo,Hao Wang,Zhuotao Liu,Tianwei Zhang,Ke Xu,Minlie Huang,Han Qiu*

Main category: cs.AI

TL;DR: 本文指出搜索代理连接大语言模型与互联网存在低质量搜索结果带来安全威胁的问题，提出自动化红队框架构建SafeSearch基准进行评估，发现大语言模型搜索代理有大量漏洞，强调框架对安全开发的价值。


<details>
  <summary>Details</summary>
Motivation: 解决搜索代理因不可靠搜索结果给终端用户带来安全威胁的问题。

Method: 开展野外实验，引入自动化红队框架，构建SafeSearch基准，评估三种搜索代理架构。

Result: 大语言模型搜索代理存在大量漏洞，常见防御手段效果有限，如GPT - 4.1 - mini在搜索工作流设置下最高ASR达90.5%。

Conclusion: 强调自动化红队框架在促进安全代理开发透明度方面的价值，代码和测试用例公开。

Abstract: Search agents connect LLMs to the Internet, enabling access to broader and
more up-to-date information. However, unreliable search results may also pose
safety threats to end users, establishing a new threat surface. In this work,
we conduct two in-the-wild experiments to demonstrate both the prevalence of
low-quality search results and their potential to misguide agent behaviors. To
counter this threat, we introduce an automated red-teaming framework that is
systematic, scalable, and cost-efficient, enabling lightweight and harmless
safety assessments of search agents. Building on this framework, we construct
the SafeSearch benchmark, which includes 300 test cases covering five
categories of risks (e.g., misinformation and indirect prompt injection). Using
this benchmark, we evaluate three representative search agent scaffolds,
covering search workflow, tool-calling, and deep research, across 7 proprietary
and 8 open-source backend LLMs. Our results reveal substantial vulnerabilities
of LLM-based search agents: when exposed to unreliable websites, the highest
ASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover,
our analysis highlights the limited effectiveness of common defense practices,
such as reminder prompting. This emphasizes the value of our framework in
promoting transparency for safer agent development. Our codebase and test cases
are publicly available: https://github.com/jianshuod/SafeSearch.

</details>


### [62] [Measuring Sparse Autoencoder Feature Sensitivity](https://arxiv.org/abs/2509.23717)
*Claire Tian,Katherine Tian,Nathan Hu*

Main category: cs.AI

TL;DR: 本文提出评估稀疏自编码器（SAE）特征敏感性的可扩展方法，发现敏感性是评估特征质量新方面，且不同SAE宽度下特征平均敏感性有变化。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅通过激活示例刻画SAE特征，未揭示特征敏感性，需开发评估方法。

Method: 使用语言模型生成与特征激活示例语义属性相同的文本，测试特征是否激活。

Result: 敏感性是特征质量新方面，许多可解释特征敏感性差，人类评估确认生成文本与原激活示例相似，7种SAE变体中平均特征敏感性随SAE宽度增加而下降。

Conclusion: 特征敏感性可作为评估单个特征和SAE架构的新维度。

Abstract: Sparse Autoencoder (SAE) features have become essential tools for mechanistic
interpretability research. SAE features are typically characterized by
examining their activating examples, which are often "monosemantic" and align
with human interpretable concepts. However, these examples don't reveal feature
sensitivity: how reliably a feature activates on texts similar to its
activating examples. In this work, we develop a scalable method to evaluate
feature sensitivity. Our approach avoids the need to generate natural language
descriptions for features; instead we use language models to generate text with
the same semantic properties as a feature's activating examples. We then test
whether the feature activates on these generated texts. We demonstrate that
sensitivity measures a new facet of feature quality and find that many
interpretable features have poor sensitivity. Human evaluation confirms that
when features fail to activate on our generated text, that text genuinely
resembles the original activating examples. Lastly, we study feature
sensitivity at the SAE level and observe that average feature sensitivity
declines with increasing SAE width across 7 SAE variants. Our work establishes
feature sensitivity as a new dimension for evaluating both individual features
and SAE architectures.

</details>


### [63] [MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models](https://arxiv.org/abs/2509.23725)
*Siqi Ma,Jiajie Huang,Bolin Yang,Fan Zhang,Jinlin Wu,Yue Shen,Guohui Fan,Zhu Zhang,Zelin Zang*

Main category: cs.AI

TL;DR: 提出逻辑驱动的多智能体框架MedLA用于医疗推理，在基准测试中表现出色，可跨模型实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体方法依赖固定角色或浅层交互提示，难以检测和解决细粒度逻辑不一致问题。

Method: 构建基于三段论的显式逻辑树，通过多轮图引导讨论比较和迭代优化逻辑树。

Result: MedLA在MedDDx和标准医疗QA任务中优于静态基于角色系统和单智能体基线。

Conclusion: MedLA可有效跨模型扩展，实现SOTA性能，为可信医疗推理提供通用范式。

Abstract: Answering complex medical questions requires not only domain expertise and
patient-specific information, but also structured and multi-perspective
reasoning. Existing multi-agent approaches often rely on fixed roles or shallow
interaction prompts, limiting their ability to detect and resolve fine-grained
logical inconsistencies. To address this, we propose \textsc{MedLA}, a
logic-driven multi-agent framework built on large language models. Each agent
organizes its reasoning process into an explicit logical tree based on
syllogistic triads (major premise, minor premise, and conclusion), enabling
transparent inference and premise-level alignment. Agents engage in a
multi-round, graph-guided discussion to compare and iteratively refine their
logic trees, achieving consensus through error correction and contradiction
resolution. We demonstrate that \textsc{MedLA} consistently outperforms both
static role-based systems and single-agent baselines on challenging benchmarks
such as MedDDx and standard medical QA tasks. Furthermore, \textsc{MedLA}
scales effectively across both open-source and commercial LLM backbones,
achieving state-of-the-art performance and offering a generalizable paradigm
for trustworthy medical reasoning.

</details>


### [64] [EAPO: Enhancing Policy Optimization with On-Demand Expert Assistance](https://arxiv.org/abs/2509.23730)
*Siyao Song,Cong Ma,Zhihao Cheng,Shiye Lei,Minghao Li,Ying Zeng,Huaixiao Tou,Kai Jia*

Main category: cs.AI

TL;DR: 提出Expert - Assisted Policy Optimization (EAPO)框架以解决现有LLM推理优化问题，实验显示其性能更好。


<details>
  <summary>Details</summary>
Motivation: 现有基于结果监督的LLM推理优化方法存在探索效率低和奖励稀疏问题。

Method: 提出EAPO框架，在训练中引入与外部专家的多轮交互，激励策略自适应咨询专家。

Result: 在数学推理基准测试中，EAPO始终优于专家辅助工作流、专家蒸馏模型和RL基线，比自探索模型平均高5分。

Conclusion: EAPO能将专家知识内化到策略模型中，提升模型固有推理能力，优化后可独立解决问题。

Abstract: Large language models (LLMs) have recently advanced in reasoning when
optimized with reinforcement learning (RL) under verifiable rewards. Existing
methods primarily rely on outcome-based supervision to strengthen internal LLM
reasoning, often leading to inefficient exploration and sparse rewards. To
mitigate this issue, we propose Expert-Assisted Policy Optimization (EAPO), a
novel RL framework that enhances exploration by incorporating multi-turn
interactions with external experts during training. Unlike prior methods, where
policies reason in isolation, EAPO incentivizes the policy to adaptively
determine when and how to consult experts, yielding richer reward signals and
more reliable reasoning trajectories. External assistance ultimately
internalizes expert knowledge into the policy model, amplifying the model's
inherent reasoning capabilities. During evaluation, the policy model has been
well-optimized to solve questions independently, producing improved reasoning
paths and more accurate solutions. Experiments on mathematical reasoning
benchmarks, including AIME 2024, AIME 2025, and AIMO 2025, show that EAPO
consistently outperforms expert-assisted workflow, expert-distilled models, and
RL baselines, with an average gain of 5 points over self-exploratory models.

</details>


### [65] [GUI-Shepherd: Reliable Process Reward and Verification for Long-Sequence GUI Tasks](https://arxiv.org/abs/2509.23738)
*Cong Chen,Kaixiang Ji,Hao Zhong,Muzhi Zhu,Anzhou Li,Guo Gan,Ziyuan Huang,Cheng Zou,Jiajia Liu,Jingdong Chen,Hao Chen,Chunhua Shen*

Main category: cs.AI

TL;DR: 本文提出GUI - Shepherd解决GUI任务中稀疏奖励和信用分配问题，在多个基准测试中取得良好效果，证明过程监督对构建GUI智能体的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决自主智能体在长序列图形用户界面（GUI）任务中面临的稀疏奖励和难以解决的信用分配问题。

Method: 引入Process Reward Model（GUI - Shepherd），在包含52k交互的大规模多样化数据集上训练，该数据集有人工标注分数和GPT - 4o生成的理由，可作为强化学习训练的奖励提供者和推理验证器。

Result: 在在线AndroidWorld基准测试中，通过多轮在线PPO将成功率提高7.7个百分点；作为推理验证器带来5.1个百分点的提升。在离线AndroidControl基准测试中，作为奖励提供者提升2.2个百分点，作为验证器提升4.3个百分点。

Conclusion: 高保真的过程监督对于构建更强大的GUI智能体至关重要，且提出的解决方案具有通用性。

Abstract: Autonomous agents for long-sequence Graphical User Interface tasks are
hindered by sparse rewards and the intractable credit assignment problem. To
address these challenges, we introduce GUI-Shepherd, a Process Reward Model
that provides dense, step-by-step feedback to guide agents. GUI-Shepherd is
trained on a diverse large-scale data set of $52$k interactions that features
human-annotated scores and GPT-4o generated rationales, enabling it to serve
both as a reward provider for RL training and as a verifier for inference. As
far as we know, we are the first to conduct a systematic study of process
supervision in GUI agents, across diverse settings from online long-horizon
tasks to offline single-step prediction. On the online AndroidWorld benchmark,
GUI-Shepherd improves success rate by $7.7$ points via multi-turn online PPO,
significantly outperforming Outcome Reward Model based competitors. When used
as an inference verifier, it brings $5.1$ points improvements. The benefits
generalize to the offline AndroidControl benchmark, with gains of $2.2$ points
as a reward provider and $4.3$ points as a verifier. Collectively, our results
establish that high-fidelity process supervision is critical for building more
capable GUI agents and present a generalizable solution.

</details>


### [66] [Transparent Visual Reasoning via Object-Centric Agent Collaboration](https://arxiv.org/abs/2509.23757)
*Benjamin Teoh,Ben Glocker,Francesca Toni,Avinash Kori*

Main category: cs.AI

TL;DR: 提出OCEAN框架用于可解释AI，在多对象数据集上测试，表现有竞争力且解释获用户认可。


<details>
  <summary>Details</summary>
Motivation: 解决可解释AI在视觉领域生成基于人类可理解概念解释的挑战。

Method: 引入OCEAN框架，基于对象中心表示和透明多智能体推理过程，进行端到端训练，并与标准视觉分类器和流行事后解释工具对比。

Result: 在两个诊断多对象数据集上，OCEAN与最先进黑盒模型相比有竞争力，用户研究表明其解释更直观可信。

Conclusion: OCEAN框架在可解释AI方面表现良好，推理过程可靠，解释易被用户接受。

Abstract: A central challenge in explainable AI, particularly in the visual domain, is
producing explanations grounded in human-understandable concepts. To tackle
this, we introduce OCEAN (Object-Centric Explananda via Agent Negotiation), a
novel, inherently interpretable framework built on object-centric
representations and a transparent multi-agent reasoning process. The
game-theoretic reasoning process drives agents to agree on coherent and
discriminative evidence, resulting in a faithful and interpretable
decision-making process. We train OCEAN end-to-end and benchmark it against
standard visual classifiers and popular posthoc explanation tools like GradCAM
and LIME across two diagnostic multi-object datasets. Our results demonstrate
competitive performance with respect to state-of-the-art black-box models with
a faithful reasoning process, which was reflected by our user study, where
participants consistently rated OCEAN's explanations as more intuitive and
trustworthy.

</details>


### [67] [From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning](https://arxiv.org/abs/2509.23768)
*Cheng Yang,Jiaxuan Lu,Haiyuan Wan,Junchi Yu,Feiwei Qin*

Main category: cs.AI

TL;DR: 提出多智能体系统ChemMAS用于化学反应条件推荐，将任务分解多步骤，实验显示其优于基线模型和通用大模型，建立可解释AI新范式。


<details>
  <summary>Details</summary>
Motivation: 现有化学反应条件推荐方法很少解释推荐理由，限制其在高风险科学流程中的应用。

Method: 提出ChemMAS，将条件预测重构为基于证据的推理任务，分解为机理基础、多通道召回、约束感知的智能体辩论和理由聚合。

Result: ChemMAS在Top - 1准确率上比特定领域基线模型高20 - 35%，比通用大模型高10 - 15%，并提供可证伪、人类可信赖的理由。

Conclusion: ChemMAS为科学发现中的可解释AI建立了新范式。

Abstract: The chemical reaction recommendation is to select proper reaction condition
parameters for chemical reactions, which is pivotal to accelerating chemical
science. With the rapid development of large language models (LLMs), there is
growing interest in leveraging their reasoning and planning capabilities for
reaction condition recommendation. Despite their success, existing methods
rarely explain the rationale behind the recommended reaction conditions,
limiting their utility in high-stakes scientific workflows. In this work, we
propose ChemMAS, a multi-agent system that reframes condition prediction as an
evidence-based reasoning task. ChemMAS decomposes the task into mechanistic
grounding, multi-channel recall, constraint-aware agentic debate, and rationale
aggregation. Each decision is backed by interpretable justifications grounded
in chemical knowledge and retrieved precedents. Experiments show that ChemMAS
achieves 20-35% gains over domain-specific baselines and outperforms
general-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable,
human-trustable rationales, which establishes a new paradigm for explainable AI
in scientific discovery.

</details>


### [68] [Falcon: A Cross-Modal Evaluation Dataset for Comprehensive Safety Perception](https://arxiv.org/abs/2509.23783)
*Qi Xue,Minrui Jiang,Runjia Zhang,Xiurui Xie,Pei Ke,Guisong Liu*

Main category: cs.AI

TL;DR: 现有大语言模型内容有害性评估方法成熟，但多模态大语言模型相关方法不足。本文引入Falcon数据集和FalconEye评估器，实验表明FalconEye能可靠识别有害内容，有实用价值。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型内容有害性评估方法欠发达且缺乏深度，视觉信息在视觉问答内容审核中的作用常被忽视。

Method: 引入含57,515个视觉问答对、涵盖13种有害类别的Falcon数据集，为图像、指令和响应的有害属性提供注释；用Falcon数据集微调Qwen2.5 - VL - 7B得到评估器FalconEye。

Result: FalconEye能在复杂多模态对话场景中可靠识别有害内容，在Falcon - test数据集及VLGuard、Beavertail - V基准测试中整体准确率超其他基线。

Conclusion: FalconEye有潜力成为多模态大语言模型实用的安全审计工具。

Abstract: Existing methods for evaluating the harmfulness of content generated by large
language models (LLMs) have been well studied. However, approaches tailored to
multimodal large language models (MLLMs) remain underdeveloped and lack depth.
This work highlights the crucial role of visual information in moderating
content in visual question answering (VQA), a dimension often overlooked in
current research. To bridge this gap, we introduce Falcon, a large-scale
vision-language safety dataset containing 57,515 VQA pairs across 13 harm
categories. The dataset provides explicit annotations for harmful attributes
across images, instructions, and responses, thereby facilitating a
comprehensive evaluation of the content generated by MLLMs. In addition, it
includes the relevant harm categories along with explanations supporting the
corresponding judgments. We further propose FalconEye, a specialized evaluator
fine-tuned from Qwen2.5-VL-7B using the Falcon dataset. Experimental results
demonstrate that FalconEye reliably identifies harmful content in complex and
safety-critical multimodal dialogue scenarios. It outperforms all other
baselines in overall accuracy across our proposed Falcon-test dataset and two
widely-used benchmarks-VLGuard and Beavertail-V, underscoring its potential as
a practical safety auditing tool for MLLMs.

</details>


### [69] [AnveshanaAI: A Multimodal Platform for Adaptive AI/ML Education through Automated Question Generation and Interactive Assessment](https://arxiv.org/abs/2509.23811)
*Rakesh Thakur,Diksha Khandelwal,Shreya Tiwari*

Main category: cs.AI

TL;DR: 提出AnveshanaAI平台用于人工智能学习，有个性化界面、游戏化追踪等功能，实验显示效果良好，支持下一代AI教育。


<details>
  <summary>Details</summary>
Motivation: 开发一个更优质的人工智能应用学习平台，解决现有平台存在的问题。

Method: 构建包含个性化仪表盘、游戏化追踪的平台，采用基于布鲁姆分类法的数据集，运用语义相似度检查和可解释AI技术，使用自适应、自动化和领域感知评估方法。

Result: 实验表明有广泛的数据集覆盖，稳定的微调且困惑度降低，学习者参与度有可衡量的提升。

Conclusion: AnveshanaAI集成适应性、游戏化、交互性和可解释性，可支持下一代AI教育。

Abstract: We propose AnveshanaAI, an application-based learning platform for artificial
intelligence. With AnveshanaAI, learners are presented with a personalized
dashboard featuring streaks, levels, badges, and structured navigation across
domains such as data science, machine learning, deep learning, transformers,
generative AI, large language models, and multimodal AI, with scope to include
more in the future. The platform incorporates gamified tracking with points and
achievements to enhance engagement and learning, while switching between
Playground, Challenges, Simulator, Dashboard, and Community supports
exploration and collaboration. Unlike static question repositories used in
existing platforms, AnveshanaAI ensures balanced learning progression through a
dataset grounded in Bloom's taxonomy, with semantic similarity checks and
explainable AI techniques improving transparency and reliability. Adaptive,
automated, and domain-aware assessment methods are also employed. Experiments
demonstrate broad dataset coverage, stable fine-tuning with reduced perplexity,
and measurable gains in learner engagement. Together, these features illustrate
how AnveshanaAI integrates adaptivity, gamification, interactivity, and
explainability to support next-generation AI education.

</details>


### [70] [Training Agents Inside of Scalable World Models](https://arxiv.org/abs/2509.24527)
*Danijar Hafner,Wilson Yan,Timothy Lillicrap*

Main category: cs.AI

TL;DR: 介绍Dreamer 4可扩展智能体，其在快速准确的世界模型中用强化学习解决控制任务，在《我的世界》中表现出色，首次仅从离线数据中获取钻石，为想象训练提供可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 以往世界模型无法准确预测复杂环境中物体交互，需要新方法推动智能体发展。

Method: 引入Dreamer 4，在快速准确的世界模型中进行强化学习，采用捷径强制目标和高效变压器架构实现实时交互推理，从少量数据学习通用动作条件。

Result: 在《我的世界》中世界模型准确预测物体交互和游戏机制，远超以往模型；Dreamer 4首次仅从离线数据获取钻石。

Conclusion: 工作为想象训练提供可扩展方案，向智能体发展迈进了一步。

Abstract: World models learn general knowledge from videos and simulate experience for
training behaviors in imagination, offering a path towards intelligent agents.
However, previous world models have been unable to accurately predict object
interactions in complex environments. We introduce Dreamer 4, a scalable agent
that learns to solve control tasks by reinforcement learning inside of a fast
and accurate world model. In the complex video game Minecraft, the world model
accurately predicts object interactions and game mechanics, outperforming
previous world models by a large margin. The world model achieves real-time
interactive inference on a single GPU through a shortcut forcing objective and
an efficient transformer architecture. Moreover, the world model learns general
action conditioning from only a small amount of data, allowing it to extract
the majority of its knowledge from diverse unlabeled videos. We propose the
challenge of obtaining diamonds in Minecraft from only offline data, aligning
with practical applications such as robotics where learning from environment
interaction can be unsafe and slow. This task requires choosing sequences of
over 20,000 mouse and keyboard actions from raw pixels. By learning behaviors
in imagination, Dreamer 4 is the first agent to obtain diamonds in Minecraft
purely from offline data, without environment interaction. Our work provides a
scalable recipe for imagination training, marking a step towards intelligent
agents.

</details>


### [71] [Mix-Ecom: Towards Mixed-Type E-Commerce Dialogues with Complex Domain Rules](https://arxiv.org/abs/2509.23836)
*Chenyu Zhou,Xiaoming Shi,Hui Qiu,Xiawu Zheng,Haitao Leng,Yankai Jiang,Shaoguo Liu,Tingting Gao,Rongrong Ji*

Main category: cs.AI

TL;DR: 为评估电商领域大语言模型代理，引入基准框架，但现有基准有不足。本文引入新语料Mix - ECom并构建基线和动态框架，结果显示当前电商代理处理对话能力不足，数据集将公开。


<details>
  <summary>Details</summary>
Motivation: 现有电商领域基准框架缺乏对代理处理混合类型电商对话和复杂领域规则能力的评估，为推动电商代理的研究和应用，需解决此问题。

Method: 引入基于真实客服对话构建的新语料Mix - ECom，去除用户隐私并添加CoT过程；在Mix - ECom上构建基线，提出动态框架。

Result: 当前电商代理因复杂领域规则导致的幻觉问题，缺乏足够能力处理电商对话。

Conclusion: 引入的Mix - ECom语料和动态框架可用于评估和提升电商代理处理复杂电商对话的能力，数据集公开利于后续研究。

Abstract: E-commerce agents contribute greatly to helping users complete their
e-commerce needs. To promote further research and application of e-commerce
agents, benchmarking frameworks are introduced for evaluating LLM agents in the
e-commerce domain. Despite the progress, current benchmarks lack evaluating
agents' capability to handle mixed-type e-commerce dialogue and complex domain
rules. To address the issue, this work first introduces a novel corpus, termed
Mix-ECom, which is constructed based on real-world customer-service dialogues
with post-processing to remove user privacy and add CoT process. Specifically,
Mix-ECom contains 4,799 samples with multiply dialogue types in each e-commerce
dialogue, covering four dialogue types (QA, recommendation, task-oriented
dialogue, and chit-chat), three e-commerce task types (pre-sales, logistics,
after-sales), and 82 e-commerce rules. Furthermore, this work build baselines
on Mix-Ecom and propose a dynamic framework to further improve the performance.
Results show that current e-commerce agents lack sufficient capabilities to
handle e-commerce dialogues, due to the hallucination cased by complex domain
rules. The dataset will be publicly available.

</details>


### [72] [Rethinking Reward Miscalibration of GRPO in Agentic RL](https://arxiv.org/abs/2509.23870)
*Jingyu Liu,Xiaopeng Wu,Jingquan Peng,Kehan Chen,Chuan Yu,Lizhong Ding,Yong Liu*

Main category: cs.AI

TL;DR: 本文研究基于结果的奖励和梯度耦合问题，提出训练智能体对好坏动作分类的方法解决问题，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 构建能解决长周期现实任务的自主智能体时，基于结果的奖励会导致奖励校准错误，且存在梯度耦合问题。

Method: 训练智能体对好坏动作进行分类，分离好坏动作的嵌入，减轻梯度干扰。

Result: 提出的方法在广泛实验中证明有效。

Conclusion: 基于结果的奖励应惩罚有缺陷的动作，训练智能体对动作分类能有效解决梯度耦合问题。

Abstract: Building autonomous agents capable of solving long-horizon, real-world tasks
has garnered significant research interest. But outcome based rewards may cause
reward miscalibration which means it might mistakenly allocate positive reward
to flawed middle steps which is regarded as the key reason making the bad
actions being reinforced during training. However we reveal that outcome based
reward ensures expected negative advantage for those flawed middle steps, which
means the flawed actions should be punished during training. Even accounting
for the ``squeezing effect", the probability mass of good actions should
increase and the actor should gradually get rid of harmful actions. This shows
that flawed actions should be punished during training. We further identify
gradient coupling between similar samples as a key issue in agentic RL, the
input prompt is extremely similar and the output action space is limited,
therefore during training, gradients from well-performing samples can
inadvertently strengthen suboptimal or incorrect actions due to similar input
observation and output actions. We show that with gradient coupling, some
flawed actions might be enhanced. To address this, we propose training the
actor to classify good or bad actions to separate the embedding of good/bad
actions and alleviate the gradient interference, extensive experiments shows
its effectiveness.

</details>


### [73] [Quant Fever, Reasoning Blackholes, Schrodinger's Compliance, and More: Probing GPT-OSS-20B](https://arxiv.org/abs/2509.23882)
*Shuyi Lin,Tian Lu,Zikai Wang,Bo Wen,Yibo Zhao,Cheng Tan*

Main category: cs.AI

TL;DR: 对GPT - OSS - 20B进行安全评估，发现多种失败模式及可被利用情况。


<details>
  <summary>Details</summary>
Motivation: 对OpenAI的GPT - OSS - 20B模型在不同对抗条件下的行为进行安全评估。

Method: 使用Jailbreak Oracle（JO）这一系统的大语言模型评估工具。

Result: 发现多种失败模式，如量化狂热、推理黑洞等，且这些行为可在GPT - OSS - 20B模型上被利用导致严重后果。

Conclusion: GPT - OSS - 20B模型存在安全隐患，相关失败模式可被利用。

Abstract: OpenAI's GPT-OSS family provides open-weight language models with explicit
chain-of-thought (CoT) reasoning and a Harmony prompt format. We summarize an
extensive security evaluation of GPT-OSS-20B that probes the model's behavior
under different adversarial conditions. Using the Jailbreak Oracle (JO) [1], a
systematic LLM evaluation tool, the study uncovers several failure modes
including quant fever, reasoning blackholes, Schrodinger's compliance,
reasoning procedure mirage, and chain-oriented prompting. Experiments
demonstrate how these behaviors can be exploited on GPT-OSS-20B models, leading
to severe consequences.

</details>


### [74] [From Neural Networks to Logical Theories: The Correspondence between Fibring Modal Logics and Fibring Neural Networks](https://arxiv.org/abs/2509.23912)
*Ouns El Harzli,Bernardo Cuenca Grau,Artur d'Avila Garcez,Ian Horrocks,Tarek R. Besold*

Main category: cs.AI

TL;DR: 本文建立了神经网络纤维与模态逻辑纤维的对应关系，推导了图神经网络等的逻辑表达结果，旨在用纤维形式化解释神经网络学习的逻辑理论。


<details>
  <summary>Details</summary>
Motivation: 此前神经网络纤维与模态逻辑纤维的对应关系未正式确立，希望建立对应关系并为用纤维形式化解释神经网络学习的逻辑理论奠定基础。

Method: 形式化与纤维神经网络兼容的纤维模型的概念。

Result: 推导了图神经网络（GNNs）、图注意力网络（GATs）和Transformer编码器的非均匀逻辑表达结果。

Conclusion: 为用纤维作为形式化工具，结合计算逻辑解释神经网络学习的逻辑理论开辟了道路。

Abstract: Fibring of modal logics is a well-established formalism for combining
countable families of modal logics into a single fibred language with common
semantics, characterized by fibred models. Inspired by this formalism, fibring
of neural networks was introduced as a neurosymbolic framework for combining
learning and reasoning in neural networks. Fibring of neural networks uses the
(pre-)activations of a trained network to evaluate a fibring function computing
the weights of another network whose outputs are injected back into the
original network. However, the exact correspondence between fibring of neural
networks and fibring of modal logics was never formally established. In this
paper, we close this gap by formalizing the idea of fibred models
\emph{compatible} with fibred neural networks. Using this correspondence, we
then derive non-uniform logical expressiveness results for Graph Neural
Networks (GNNs), Graph Attention Networks (GATs) and Transformer encoders.
Longer-term, the goal of this paper is to open the way for the use of fibring
as a formalism for interpreting the logical theories learnt by neural networks
with the tools of computational logic.

</details>


### [75] [Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models](https://arxiv.org/abs/2509.23962)
*Guanxu Chen,Yafu Li,Yuxian Jiang,Chen Qian,Qihan Ren,Jingyi Yang,Yu Cheng,Dongrui Liu,Jing Shao*

Main category: cs.AI

TL;DR: 本文提出用于大语言模型的CANON方法，在数学推理和高复杂度逻辑任务上优于先前方法，应用于响应长度时还提升了令牌效率。


<details>
  <summary>Details</summary>
Motivation: 先前结合训练指标先验的方法依赖手工惩罚和偏好，未仔细调参时会有偏差并导致失败，需改进。

Method: 引入Conditional advANtage estimatiON (CANON)，根据目标指标高低将采样响应分组，通过组间比较确定哪种指标趋势带来更好性能，并在组内识别更好响应。

Result: 基于熵的CANON在三个大语言模型的数学推理和高复杂度逻辑任务上始终优于先前方法；应用于响应长度时，提升了令牌效率，在性能 - 成本权衡中产生更有利的帕累托前沿。

Conclusion: CANON方法有效，能提升大语言模型在相关任务中的表现和效率。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) for large language
models (LLMs) has achieved remarkable progress in enhancing LLMs' reasoning
capabilities on tasks with clear correctness criteria, such as mathematical
reasoning tasks. Several training metrics, such as entropy or response length,
have been observed to correlate with different reasoning behaviors in
reinforcement learning. Prior approaches incorporate such priors through reward
or advantage shaping, which often relies on hand-crafted penalties and
preferences (e.g., higher-is-better or lower-is-better). However, without
careful hyperparameter tuning, these directional priors can be overly biased
and may lead to failure. To this end, we introduce Conditional advANtage
estimatiON (CANON), amplifying the impact of the target metric without
presuming its direction. Specifically, CANON regroups the sampled responses
into two groups based on the higher or lower value of a target metric, measures
which metric trend contributes to better performance through inter-group
comparison, and identifies the better response within the same group. In
summary, CANON based on entropy consistently outperforms prior methods across
three LLMs on both math reasoning and high-complexity logic tasks. When applied
to response length, CANON further improves token efficiency, yielding a more
favorable Pareto frontier in the performance-cost trade-off.

</details>


### [76] [Automatic selection of primary studies in systematic reviews with evolutionary rule-based classification](https://arxiv.org/abs/2509.23981)
*José de la Torre-López,Aurora Ramírez,José Raúl Romero*

Main category: cs.AI

TL;DR: 提出进化机器学习方法ourmodel自动判断论文相关性，结合文本和文献计量数据，实验证明可生成准确且可解释的分类器。


<details>
  <summary>Details</summary>
Motivation: 科学文献搜索、筛选和分析耗时，机器学习自动选纸可减少识别相关文献的工作量。

Method: 提出进化机器学习方法ourmodel，用语法引导的遗传编程构建基于规则的可解释分类器。

Result: 实验表明能在不损害可解释性的情况下，使用此前不支持的可配置信息源生成准确分类器。

Conclusion: 进化机器学习方法ourmodel在自动判断论文相关性方面有效，且具有良好可解释性和利用新信息源的能力。

Abstract: Searching, filtering and analysing scientific literature are time-consuming
tasks when performing a systematic literature review. With the rise of
artificial intelligence, some steps in the review process are progressively
being automated. In particular, machine learning for automatic paper selection
can greatly reduce the effort required to identify relevant literature in
scientific databases. We propose an evolutionary machine learning approach,
called \ourmodel, to automatically determine whether a paper retrieved from a
literature search process is relevant. \ourmodel builds an interpretable
rule-based classifier using grammar-guided genetic programming. The use of a
grammar to define the syntax and the structure of the rules allows \ourmodel to
easily combine the usual textual information with other bibliometric data not
considered by state-of-the-art methods. Our experiments demonstrate that it is
possible to generate accurate classifiers without impairing interpretability
and using configurable information sources not supported so far.

</details>


### [77] [TusoAI: Agentic Optimization for Scientific Methods](https://arxiv.org/abs/2509.23986)
*Alistair Turcan,Kexin Huang,Lei Li,Martin Jinye Zhang*

Main category: cs.AI

TL;DR: 文章介绍了能自主开发和优化计算方法的AI系统TusoAI，经评估其表现优异，还解决了遗传学问题并发现新生物学关联。


<details>
  <summary>Details</summary>
Motivation: 科学发现常因手动开发计算工具分析复杂实验数据而受阻，现有基于大语言模型的系统存在不足，需开发新系统加速计算方法开发。

Method: TusoAI将领域知识整合到知识树表示中，进行迭代、特定领域的优化和模型诊断。

Result: 综合基准评估显示TusoAI在多项任务中优于现有方法；应用于遗传学问题，改进现有计算方法并发现新生物学关联。

Conclusion: TusoAI是有效的自主开发和优化计算方法的AI系统，有良好应用前景。

Abstract: Scientific discovery is often slowed by the manual development of
computational tools needed to analyze complex experimental data. Building such
tools is costly and time-consuming because scientists must iteratively review
literature, test modeling and scientific assumptions against empirical data,
and implement these insights into efficient software. Large language models
(LLMs) have demonstrated strong capabilities in synthesizing literature,
reasoning with empirical data, and generating domain-specific code, offering
new opportunities to accelerate computational method development. Existing
LLM-based systems either focus on performing scientific analyses using existing
computational methods or on developing computational methods or models for
general machine learning without effectively integrating the often unstructured
knowledge specific to scientific domains. Here, we introduce TusoAI , an
agentic AI system that takes a scientific task description with an evaluation
function and autonomously develops and optimizes computational methods for the
application. TusoAI integrates domain knowledge into a knowledge tree
representation and performs iterative, domain-specific optimization and model
diagnosis, improving performance over a pool of candidate solutions. We
conducted comprehensive benchmark evaluations demonstrating that TusoAI
outperforms state-of-the-art expert methods, MLE agents, and scientific AI
agents across diverse tasks, such as single-cell RNA-seq data denoising and
satellite-based earth monitoring. Applying TusoAI to two key open problems in
genetics improved existing computational methods and uncovered novel biology,
including 9 new associations between autoimmune diseases and T cell subtypes
and 7 previously unreported links between disease variants linked to their
target genes. Our code is publicly available at
https://github.com/Alistair-Turcan/TusoAI.

</details>


### [78] [Future-Proofing Programmers: Optimal Knowledge Tracing for AI-Assisted Personalized Education](https://arxiv.org/abs/2509.23996)
*Yuchen Wang,Pei-Duo Yu,Chee Wei Tan*

Main category: cs.AI

TL;DR: 本文介绍了AI驱动的CoTutor模型，它结合多种技术助力学生学习，大学试验显示效果良好，有个性化和可扩展性潜力。


<details>
  <summary>Details</summary>
Motivation: 随着知识追踪、信号处理和生成式AI融合，将学习转化为科学，优化教育。

Method: 提出CoTutor模型，用信号处理技术增强贝叶斯知识追踪，结合生成式AI与自适应学习技术。

Result: 大学试验中，CoTutor在学习成果上有显著提升，优于传统教育工具。

Conclusion: CoTutor有AI驱动个性化和可扩展性潜力，为教育技术隐私和伦理考量提供机会，AI辅助知识追踪，学习者可获新见解。

Abstract: Learning to learn is becoming a science, driven by the convergence of
knowledge tracing, signal processing, and generative AI to model student
learning states and optimize education. We propose CoTutor, an AI-driven model
that enhances Bayesian Knowledge Tracing with signal processing techniques to
improve student progress modeling and deliver adaptive feedback and strategies.
Deployed as an AI copilot, CoTutor combines generative AI with adaptive
learning technology. In university trials, it has demonstrated measurable
improvements in learning outcomes while outperforming conventional educational
tools. Our results highlight its potential for AI-driven personalization,
scalability, and future opportunities for advancing privacy and ethical
considerations in educational technology. Inspired by Richard Hamming's vision
of computer-aided 'learning to learn,' CoTutor applies convex optimization and
signal processing to automate and scale up learning analytics, while reserving
pedagogical judgment for humans, ensuring AI facilitates the process of
knowledge tracing while enabling learners to uncover new insights.

</details>


### [79] [Do Repetitions Matter? Strengthening Reliability in LLM Evaluations](https://arxiv.org/abs/2509.24086)
*Miguel Angel Alvarado Gonzalez,Michelle Bruno Hernandez,Miguel Angel Peñaloza Perez,Bruno Lopez Orozco,Jesus Tadeo Cruz Soto,Sandra Malagon*

Main category: cs.AI

TL;DR: 重新评估AI4Math基准上的模型，研究多次重复评估的价值，发现单轮排行榜不可靠，多次平均有优势并给出实践建议。


<details>
  <summary>Details</summary>
Motivation: LLM排行榜常依赖单随机运行，需明确可靠结论所需重复次数。

Method: 在AI4Math基准上对八个模型进行三次独立运行评估，使用混合效应逻辑回归、域级边际均值、排名不稳定性分析和运行间可靠性评估。

Result: 单轮排行榜不可靠，多次平均有适度标准误收缩和大幅排名提升，两轮可消除约83%单轮反转。

Conclusion: 建议将评估视为实验，报告不确定性，随机解码下使用至少两轮重复，可提高稳健性。

Abstract: LLM leaderboards often rely on single stochastic runs, but how many
repetitions are required for reliable conclusions remains unclear. We
re-evaluate eight state-of-the-art models on the AI4Math Benchmark with three
independent runs per setting. Using mixed-effects logistic regression,
domain-level marginal means, rank-instability analysis, and run-to-run
reliability, we assessed the value of additional repetitions. Our findings
shows that Single-run leaderboards are brittle: 10/12 slices (83\%) invert at
least one pairwise rank relative to the three-run majority, despite a zero
sign-flip rate for pairwise significance and moderate overall interclass
correlation. Averaging runs yields modest SE shrinkage ($\sim$5\% from one to
three) but large ranking gains; two runs remove $\sim$83\% of single-run
inversions. We provide cost-aware guidance for practitioners: treat evaluation
as an experiment, report uncertainty, and use $\geq 2$ repetitions under
stochastic decoding. These practices improve robustness while remaining
feasible for small teams and help align model comparisons with real-world
reliability.

</details>


### [80] [Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs](https://arxiv.org/abs/2509.24107)
*Shreyas Singh,Kunal Singh,Pradeep Moturi*

Main category: cs.AI

TL;DR: 介绍了由Fathom - Search - 4B和Fathom - Synthesizer - 4B组成的Fathom - DeepResearch代理系统，该系统在多个基准测试中达开源权重类别最优并能泛化到多样推理任务。


<details>
  <summary>Details</summary>
Motivation: 工具集成推理是实现智能代理应用的关键，DeepResearch Agents在复杂信息搜索任务表现出色，需开发新系统提升性能。

Method: 构建由Fathom - Search - 4B和Fathom - Synthesizer - 4B组成的系统。Fathom - Search - 4B基于Qwen3 - 4B训练，采用DUETQA数据集、RAPO方法和可引导的步级奖励；Fathom - Synthesizer - 4B也基于Qwen3 - 4B训练，将多轮搜索转换为结构化报告。

Result: 系统在DeepSearch基准测试和DeepResearch - Bench中达开源权重类别最优，能泛化到HLE、AIME - 25等多样推理任务。

Conclusion: Fathom - DeepResearch系统有效，在信息搜索和推理任务中表现佳，有良好泛化能力。

Abstract: Tool-integrated reasoning has emerged as a key focus for enabling agentic
applications. Among these, DeepResearch Agents have gained significant
attention for their strong performance on complex, open-ended
information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system
composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch
model trained from Qwen3-4B and optimized for evidence-based investigation
through live web search and targeted webpage querying. Its training combines
three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent
self-play that enforces strict web-search dependence and heterogeneous source
grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes
multi-turn Reinforcement Learning with Verifiable Rewards through curriculum
pruning, reward-aware advantage scaling, and per-prompt replay buffers; and
(iii) a steerable step-level reward that classifies each tool call by cognitive
behavior and marginal utility, enabling explicit control over search trajectory
breadth, depth, and horizon. These improvements enable reliable extension of
tool-calling beyond 20 calls when warranted. The second is
Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn
DeepSearch traces into structured, citation-dense DeepResearch Reports for
comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES,
WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves
state-of-the-art performance in the open-weights category while demonstrating
strong generalization to diverse reasoning tasks including HLE, AIME-25,
GPQA-Diamond, and MedQA.

</details>


### [81] [Reasoning or Retrieval? A Study of Answer Attribution on Large Reasoning Models](https://arxiv.org/abs/2509.24156)
*Yuhui Wang,Changjiang Li,Guangke Chen,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: 研究发现大推理模型答案与推理过程常矛盾，源于推理和记忆检索机制，提出FARL框架解决问题。


<details>
  <summary>Details</summary>
Motivation: 解决大推理模型最终答案与推理过程矛盾的问题，探究矛盾产生原因。

Method: 进行控制实验，用误导线索和损坏答案挑战模型；提出FARL框架，结合记忆遗忘和强化学习。

Result: 证实推理和记忆检索机制同时运行，其主导地位受问题领域、模型规模和微调方法影响；当前微调范式存在模型利用检索机制走捷径的问题。

Conclusion: FARL框架可抑制检索捷径，促进推理主导行为，增强可泛化推理能力。

Abstract: Large reasoning models (LRMs) exhibit unprecedented capabilities in solving
complex problems through Chain-of-Thought (CoT) reasoning. However, recent
studies reveal that their final answers often contradict their own reasoning
traces. We hypothesize that this inconsistency stems from two competing
mechanisms for generating answers: CoT reasoning and memory retrieval. To test
this hypothesis, we conduct controlled experiments that challenge LRMs with
misleading cues during reasoning and/or corrupted answers during retrieval. Our
results across models and datasets confirm that both mechanisms operate
simultaneously, with their relative dominance influenced by multiple factors:
problem domains, model scales, and fine-tuning approaches (e.g., reinforcement
learning vs. distillation). The findings reveal a critical limitation in
current reasoning fine-tuning paradigms: models can exploit the retrieval
mechanism as a shortcut, effectively "hacking" the reward signal and
undermining genuine reasoning development. To address this challenge, we
introduce FARL, a novel fine-tuning framework that integrates memory unlearning
with reinforcement learning. By carefully suppressing retrieval shortcuts
during the fine-tuning process, FARL promotes reasoning-dominant behavior and
enhances generalizable reasoning capabilities.

</details>


### [82] [Robust Preference Optimization: Aligning Language Models with Noisy Preference Feedback](https://arxiv.org/abs/2509.24159)
*Xiaoyang Cao,Zelai Xu,Mo Guang,Kaiwen Long,Michiel A. Bakker,Yu Wang,Chao Yu*

Main category: cs.AI

TL;DR: 传统基于人类偏好的大模型对齐方法存在假设缺陷，本文提出RPO方法，将其推广为元框架，理论证明其收敛性，实验表明能提升多种对齐算法性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于人类偏好的大模型对齐方法的假设（人类偏好同质、数据无噪声）与现实不符，导致模型性能下降，需要解决这一问题。

Method: 引入Robust Preference Optimization (RPO)，使用Expectation-Maximization (EM)算法推断标签正确性的后验概率，自适应重新加权训练损失中的数据点；建立任意偏好损失与其对应概率模型的理论联系，将其推广为元框架。

Result: 理论上证明在模型完美校准条件下RPO能收敛到数据集的真实噪声水平；实验中RPO作为元框架能持续提升四种先进对齐算法性能，应用于Mistral和Llama 3模型时在相关评测中有显著胜率提升。

Conclusion: RPO是有效的鲁棒偏好对齐元框架，能提升大模型对齐算法性能。

Abstract: Standard human preference-based alignment methods, such as Reinforcement
Learning from Human Feedback (RLHF), are a cornerstone technology for aligning
Large Language Models (LLMs) with human values. However, these methods are all
underpinned by a critical, yet flawed assumption: human preferences are
homogeneous (representing a single, unified preference) and the collected data
is noiseless (free from error). In reality, neither is true since human
preference is pluralistic and annotators can make mistakes. This creates a
discrepancy between the recorded data and the ground-truth preferences, which
can misguide the model and degrade its performance. To address this challenge,
we introduce Robust Preference Optimization (RPO). RPO employs an
Expectation-Maximization (EM) algorithm to infer the posterior probability of
each label's correctness, which is used to adaptively re-weigh each data point
in the training loss to mitigate noise. We further generalize this approach by
establishing a theoretical link between arbitrary preference losses and their
corresponding probabilistic models. This generalization enables the systematic
transformation of existing alignment algorithms into their robust counterparts,
elevating RPO from a specific algorithm to a meta-framework for robust
preference alignment. Theoretically, we prove that under the condition of a
perfectly calibrated model, RPO is guaranteed to converge to the true noise
level of the dataset. Our experiments demonstrate RPO's effectiveness as a
meta-framework, consistently enhancing four state-of-the-art alignment
algorithms (DPO, IPO, SimPO, and CPO). When applied to Mistral and Llama 3
models, the RPO-enhanced methods achieve substantial win rate gains on
AlpacaEval 2 and Arena-Hard, with improvements of up to 7.0% and 5.4%,
respectively.

</details>


### [83] [Humanline: Online Alignment as Perceptual Loss](https://arxiv.org/abs/2509.24207)
*Sijia Liu,Niklas Muennighoff,Kawin Ethayarajh*

Main category: cs.AI

TL;DR: 本文基于行为经济学的前景理论，为在线对齐比离线对齐更高效提供以人为中心的解释，并提出将概率感知扭曲纳入目标的设计模式，其变体用离线数据训练也能达在线模型性能。


<details>
  <summary>Details</summary>
Motivation: 解释在线对齐比离线对齐更高效的原因，并探索不依赖在线策略数据实现相同效果的方法。

Method: 基于前景理论，提出将概率感知扭曲纳入DPO/KTO/GRPO等目标的设计模式。

Result: 人类线变体即使使用离线非策略数据训练，在可验证和不可验证任务上也能达到在线模型的性能。

Conclusion: 在线/离线二分法对最大化人类效用并非必要，可通过模仿人类感知的方式训练数据，能快速、低成本、灵活地进行后训练且不牺牲性能。

Abstract: Online alignment (e.g., GRPO) is generally more performant than offline
alignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral
economics, we propose a human-centric explanation. We prove that online
on-policy sampling better approximates the human-perceived distribution of what
the model can produce, and PPO/GRPO-style clipping -- originally introduced to
just stabilize training -- recovers a perceptual bias in how humans perceive
probability. In this sense, PPO/GRPO act as perceptual losses already. Our
theory further suggests that the online/offline dichotomy is itself incidental
to maximizing human utility, since we can achieve the same effect by
selectively training on any data in a manner that mimics human perception,
rather than restricting ourselves to online on-policy data. Doing so would
allow us to post-train more quickly, cheaply, and flexibly without sacrificing
performance. To this end, we propose a design pattern that explicitly
incorporates perceptual distortions of probability into objectives like
DPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that
these humanline variants, even when trained with offline off-policy data, can
match the performance of their online counterparts on both verifiable and
unverifiable tasks.

</details>


### [84] [ELHPlan: Efficient Long-Horizon Task Planning for Multi-Agent Collaboration](https://arxiv.org/abs/2509.24230)
*Shaobin Ling,Yun Wang,Chenyou Fan,Tin Lun Lam,Junjie Hu*

Main category: cs.AI

TL;DR: 本文提出ELHPlan框架用于LLM多机器人协作规划，平衡适应性与效率，实验显示其表现佳，建立新的效率-效果边界。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多机器人协作中，声明式方法缺乏动态适应性，迭代式方法计算成本高，难以随团队规模和任务复杂度扩展。

Method: 提出ELHPlan框架，以行动链为基本规划单元，通过循环过程（构建行动序列、验证冲突和可行性、细化问题、执行验证后行动）进行规划，还提出全面效率指标。

Result: 在TDW - MAT和C - WAH基准测试中，ELHPlan任务成功率相当，但仅消耗24%的标记。

Conclusion: 研究为基于LLM的多智能体规划系统建立了新的效率 - 效果边界。

Abstract: Large Language Models (LLMs) enable intelligent multi-robot collaboration but
face fundamental trade-offs: declarative methods lack adaptability in dynamic
environments, while iterative methods incur prohibitive computational costs
that scale poorly with team size and task complexity. In this paper, we propose
ELHPlan, a novel framework that introduces Action Chains--sequences of actions
explicitly bound to sub-goal intentions--as the fundamental planning primitive.
ELHPlan operates via a cyclical process: 1) constructing intention-bound action
sequences, 2) proactively validating for conflicts and feasibility, 3) refining
issues through targeted mechanisms, and 4) executing validated actions. This
design balances adaptability and efficiency by providing sufficient planning
horizons while avoiding expensive full re-planning. We further propose
comprehensive efficiency metrics, including token consumption and planning
time, to more holistically evaluate multi-agent collaboration. Our experiments
on benchmark TDW-MAT and C-WAH demonstrate that ELHPlan achieves comparable
task success rates while consuming only 24% of the tokens required by
state-of-the-art methods. Our research establishes a new
efficiency-effectiveness frontier for LLM-based multi-agent planning systems.

</details>


### [85] [Learning to Ponder: Adaptive Reasoning in Latent Space](https://arxiv.org/abs/2509.24238)
*Yixin He,Lumingyuan Tang*

Main category: cs.AI

TL;DR: 提出FR - Ponder框架，通过潜在引导实现实例自适应推理计算分配，在GSM8K和MATH500上提升计算 - 准确率前沿。


<details>
  <summary>Details</summary>
Motivation: 现有增强大语言模型推理的方法对所有输入采用统一深度计算，存在计算浪费和对复杂问题推理不足的问题。

Method: 提出FR - Ponder框架，用不到100万参数的控制器观察隐藏状态决定是否停止或执行小思考步骤，提取与更深推理输出相关的潜在引导向量，使用GRPO自适应调节推理深度，通过课程学习和奖励工程学习与问题难度相关的计算分配。

Result: 在GSM8K和MATH500上，以更低的FLOPs获得更好匹配的准确率，优于早期退出基线，且无需修改骨干权重。

Conclusion: FR - Ponder能自适应调节推理深度，实现与问题难度相关的校准计算分配，提升计算 - 准确率。

Abstract: Test-time compute has emerged as a key paradigm for enhancing LLM reasoning,
yet prevailing approaches like Best-of-N and majority voting apply uniform
depth across inputs, wasting computation on simple queries while potentially
under-thinking complex ones. We present FR-Ponder, a single-graph,
backbone-training-free framework that allocates instance-adaptive reasoning
compute via latent steering. A less than 1M-param controller observes hidden
states and decides to halt or apply a small ponder step by adding a
pre-computed steering vector to frozen representations. Our method extracts the
latent steering vector associated with deeper reasoning outputs and direct IO
from LLM and re-applies it through a tunable scaling factor, allowing the model
to adapt its reasoning depth to the complexity of each input. To balance
performance and computational cost, we employ Group Relative Policy
Optimization (GRPO) as a reward signal to adaptively regulate reasoning depth,
achieving task accuracy while mitigating overreasoning. Through curriculum
learning and careful reward engineering, FR-Ponder learns calibrated compute
allocation correlated with problem difficulty. On GSM8K and MATH500, FR-Ponder
improves the compute-accuracy frontier, delivering lower FLOPs with better
matched accuracy and comparing favorably to early-exit baselines, without
modifying backbone weights. Analyses visualize interpretable steering
directions and show learned compute allocation correlates with problem
difficulty.

</details>


### [86] [Model Merging Scaling Laws in Large Language Models](https://arxiv.org/abs/2509.24244)
*Yuanyi Wang,Yanggan Gu,Yiming Zhang,Qi Zhou,Zhaoyi Yan,Congkai Xie,Xinyao Wang,Jianbo Yuan,Hongxia Yang*

Main category: cs.AI

TL;DR: 研究语言模型合并的经验缩放定律，确定幂律，提出简单理论，使合并可预测规划，为分布式生成AI提供缩放原则。


<details>
  <summary>Details</summary>
Motivation: 语言模型合并虽广泛应用，但缺乏预测收益的定量规则。

Method: 识别连接模型大小和专家数量的幂律，提出简单理论解释收益下降规律。

Result: 幂律在不同领域、架构和方法中适用，能解释两个规律，可进行预测规划。

Conclusion: 该定律使合并成为可规划的替代方案，为分布式生成AI提供可预测收益的缩放原则。

Abstract: We study empirical scaling laws for language model merging measured by
cross-entropy. Despite its wide practical use, merging lacks a quantitative
rule that predicts returns as we add experts or scale the model size. We
identify a compact power law that links model size and expert number: the
size-dependent floor decreases with model capacity, while the merging tail
exhibits clear diminishing returns in the number of experts. The law holds
in-domain and cross-domain, tightly fits measured curves across diverse
architectures and methods (Average, TA, TIES, DARE), and explains two robust
regularities: most gains arrive early, and variability shrinks as more experts
are included. Building on this, we present a simple theory that explains why
gains fall roughly as 1/k and links the floor and tail to properties of the
base model and the diversity across domains. This law enables predictive
planning: estimate how many experts are needed to reach a target loss, decide
when to stop adding experts, and trade off scaling the base model versus adding
experts under a fixed budget--turning merging from heuristic practice into a
computationally efficient, planable alternative to multitask training. This
suggests a scaling principle for distributed generative AI: predictable gains
can be achieved by composing specialists, offering a complementary path toward
AGI-level systems.

</details>


### [87] [SpecExit: Accelerating Large Reasoning Model via Speculative Exit](https://arxiv.org/abs/2509.24248)
*Rubing Yang,Huajun Bai,Song Liu,Guanghua Yu,Runzhi Fan,Yanbin Dang,Jiejing Zhang,Kai Liu,Jianchen Zhu,Peng Chen*

Main category: cs.AI

TL;DR: 针对大推理模型过度思考问题，提出无探测开销的SpecExit框架，减少生成长度、加速推理且不影响准确率。


<details>
  <summary>Details</summary>
Motivation: 大推理模型存在过度思考问题，现有早期退出机制有检测开销且泛化性不足。

Method: 提出SpecExit框架，直接从轻量级草稿模型预测未来标记和早期退出信号，利用隐藏状态固有信号。

Result: 与基线相比，平均生成长度减少66%，端到端延迟加速2.5倍，且不影响准确率。

Conclusion: 隐藏状态可用于高效推理，代码已开源。

Abstract: Despite their strong performance on reasoning tasks, large reasoning models
(LRMs) often suffer from overthinking, producing unnecessarily long outputs and
incurring high end-to-end latency, a significant limitation to their real-world
deployment. To address overthinking, early-exit mechanisms have been proposed
to terminate reasoning before typical completion, showing that this approach
can effectively shorten generation length with minimal impact on accuracy.
However, their reliance on probing mechanisms introduces a detection overhead
that limits their end-to-end latency gains and compromises their
generalizability across diverse problems. Inspired by the use of hidden states
in speculative decoding, we propose SpecExit, a novel framework that predicts
both future tokens and an early-exit signal directly from a lightweight draft
model without probing overhead. Our method offers significant improvements,
reducing average generation length by 66\% and achieving a 2.5x speedup in
end-to-end latency compared to the speculative decoding baseline, without
compromising accuracy. Our method leverages the inherent signals from hidden
states to provide effective early-exit signals, suggesting broader use of
hidden states for efficient reasoning. Our code is available at
https://github.com/Tencent/AngelSlim.

</details>


### [88] [Interactive Program Synthesis for Modeling Collaborative Physical Activities from Narrated Demonstrations](https://arxiv.org/abs/2509.24250)
*Edward Kim,Daniel He,Jorge Chao,Wiktor Rajca,Mohammed Amin,Nishant Malpani,Ruta Desai,Antti Oulasvirta,Bjoern Hartmann,Sanjit Seshia*

Main category: cs.AI

TL;DR: 文章将协作任务学习构建为程序综合问题，让用户通过叙述式演示教系统多人足球战术，多数用户能成功改进程序，还探讨了相关挑战与策略。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注非协作物理活动，协作任务需系统推断用户对队友意图的假设，需可解释和可修正的表示。

Method: 将协作任务学习构建为程序综合问题，用叙述式演示作为统一方式教学、检查和修正系统逻辑。

Result: 20名用户教系统多人足球战术，70%的参与者成功改进程序，90%的人觉得修正程序容易。

Conclusion: 研究揭示了用程序表示学习和让用户教授协作物理活动的独特挑战，并给出缓解策略。

Abstract: Teaching systems physical tasks is a long standing goal in HCI, yet most
prior work has focused on non collaborative physical activities. Collaborative
tasks introduce added complexity, requiring systems to infer users assumptions
about their teammates intent, which is an inherently ambiguous and dynamic
process. This necessitates representations that are interpretable and
correctable, enabling users to inspect and refine system behavior. We address
this challenge by framing collaborative task learning as a program synthesis
problem. Our system represents behavior as editable programs and uses narrated
demonstrations, i.e. paired physical actions and natural language, as a unified
modality for teaching, inspecting, and correcting system logic without
requiring users to see or write code. The same modality is used for the system
to communicate its learning to users. In a within subjects study, 20 users
taught multiplayer soccer tactics to our system. 70 percent (14/20) of
participants successfully refined learned programs to match their intent and 90
percent (18/20) found it easy to correct the programs. The study surfaced
unique challenges in representing learning as programs and in enabling users to
teach collaborative physical activities. We discuss these issues and outline
mitigation strategies.

</details>


### [89] [Rethinking and Benchmarking Large Language Models for Graph Reasoning](https://arxiv.org/abs/2509.24260)
*Yuwei Hu,Xinyi Huang,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.AI

TL;DR: 本文指出大语言模型在图推理任务中表现不佳的问题，重新思考发展方向，构建新基准，提出Simple - RTC基线方法并取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在图推理任务中表现欠佳，且存在方法和基准的问题，需重新思考发展方向。

Method: 指出问题，将推理重点从复制图算法转向设计算法，构建GraphAlgorithm基准，提出Simple - RTC基线方法。

Result: Simple - RTC在现有基准上接近完美准确率，在GraphAlgorithm基准上显著优于GPT - 4o - mini和先前方法。

Conclusion: Simple - RTC可推动大语言模型图推理未来的进一步发展。

Abstract: Large Language Models (LLMs) for Graph Reasoning have been extensively
studied over the past two years, involving enabling LLMs to understand graph
structures and reason on graphs to solve various graph problems, with graph
algorithm problems being the most prevalent. Recent studies underscore the
potential of LLMs in handling graph reasoning tasks, but their performance is
underwhelming. In this work, we point out issues with existing methods and
benchmarks, and rethink the direction that LLMs for graph reasoning should
strive toward. We find that base models, e.g., GPT-4o-mini, are largely
underestimated due to improper reasoning focus. Base models with reasoning
focus redirected from replicating graph algorithms to designing them can easily
solve most graph reasoning tasks in existing benchmarks. To truly evaluate the
graph reasoning capabilities of LLMs, we construct a more challenging
GraphAlgorithm benchmark, comprising 239 different graph problems and 3,041
test instances collected from 4 competition platforms. Finally, we introduce a
simple and strong baseline Simple-Reasoning-Then-Coding (Simple-RTC)-which
guides LLMs to design graph algorithms first and then code to address graph
reasoning tasks. Simple-RTC achieves near-perfect accuracy on existing
benchmarks and significantly outperforms GPT-4o-mini and all prior methods on
the GraphAlgorithm benchmark. This strong baseline encourages further
advancements in LLMs for Graph Reasoning in the future.

</details>


### [90] [Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models](https://arxiv.org/abs/2509.24261)
*Yuhua Jiang,Jiawei Huang,Yufeng Yuan,Xin Mao,Yu Yue,Qianchuan Zhao,Lin Yan*

Main category: cs.AI

TL;DR: 现有RLVR方法有探索困境，本文提出风险敏感强化学习框架RS - GRPO，在多基准测试和模型上提升了pass@k性能并维持或增强了pass@1准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在探索困境，局限于窄解集，难以发现新推理策略，需改进。

Method: 引入风险敏感强化学习框架，采用风险寻求目标在均值和最大奖励间插值，得到RS - GRPO算法。

Result: 在六个数学推理基准测试和五个不同大语言模型上，RS - GRPO持续提升了pass@k性能，同时维持或增强了pass@1准确性。

Conclusion: RS - GRPO能有效克服现有RLVR方法的探索困境，且实现简单。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for enhancing Large Language Models (LLMs) on complex reasoning tasks. However,
existing methods suffer from an exploration dilemma: the sharply peaked initial
policies of pre-trained LLMs confine standard RL algorithms to a narrow set of
solutions, boosting single-solution accuracy (pass@1) but suppressing solution
diversity and multi-solution performance (pass@k). As a result, RLVR often
distills existing capabilities rather than discovering new reasoning
strategies. To overcome this, we introduce a Risk-Sensitive Reinforcement
Learning framework. Our approach employs a risk-seeking objective that
interpolates between mean and maximum rewards, leading to a novel algorithm,
Risk-Sensitive GRPO (RS-GRPO), which drives deeper exploration by amplifying
learning from challenging prompts. Remarkably, RS-GRPO is simple to implement,
requiring only minor code modifications. On six mathematical reasoning
benchmarks and with five different LLMs, RS-GRPO consistently improves pass@k
performance while maintaining or enhancing pass@1 accuracy.

</details>


### [91] [PAME-AI: Patient Messaging Creation and Optimization using Agentic AI](https://arxiv.org/abs/2509.24263)
*Junjie Luo,Yihong Guo,Anqi Liu,Ritu Agarwal,Gordon,Gao*

Main category: cs.AI

TL;DR: 提出PAME - AI用于患者消息创建与优化，通过实验证明其有效性，提升了点击率，适用于大规模医疗通信优化。


<details>
  <summary>Details</summary>
Motivation: 传统移动消息设计无法探索高维设计空间，存在显著局限性，需改进医疗通信中的患者消息设计。

Method: 基于DIKW层级构建PAME - AI，由专业计算代理系统将原始实验数据转化为可操作的消息设计策略，并进行两阶段实验验证。

Result: 最佳生成消息的参与率达68.76%，相比基线61.27%，点击率相对提高12.2%。

Conclusion: PAME - AI的代理架构支持并行处理、假设验证和持续学习，适合大规模医疗通信优化。

Abstract: Messaging patients is a critical part of healthcare communication, helping to
improve things like medication adherence and healthy behaviors. However,
traditional mobile message design has significant limitations due to its
inability to explore the high-dimensional design space. We develop PAME-AI, a
novel approach for Patient Messaging Creation and Optimization using Agentic
AI. Built on the Data-Information-Knowledge-Wisdom (DIKW) hierarchy, PAME-AI
offers a structured framework to move from raw data to actionable insights for
high-performance messaging design. PAME-AI is composed of a system of
specialized computational agents that progressively transform raw experimental
data into actionable message design strategies. We demonstrate our approach's
effectiveness through a two-stage experiment, comprising of 444,691 patient
encounters in Stage 1 and 74,908 in Stage 2. The best-performing generated
message achieved 68.76% engagement compared to the 61.27% baseline,
representing a 12.2\% relative improvement in click-through rates. This agentic
architecture enables parallel processing, hypothesis validation, and continuous
learning, making it particularly suitable for large-scale healthcare
communication optimization.

</details>


### [92] [AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models](https://arxiv.org/abs/2509.24269)
*Zihao Zhu,Xinyu Wu,Gehan Hu,Siwei Lyu,Ke Xu,Baoyuan Wu*

Main category: cs.AI

TL;DR: 论文指出当前安全CoT调优方法存在‘雪球效应’问题，提出AdvChain对抗性CoT调优范式，实验表明其能提升模型安全性与实用性。


<details>
  <summary>Details</summary>
Motivation: 当前CoT推理的多步性质带来新安全挑战，现有安全CoT调优方法存在‘雪球效应’，需解决模型无法自我纠正的问题。

Method: 提出AdvChain对齐范式，构建包含诱惑-纠正和犹豫-纠正样本的数据集，让模型学习从有害推理偏差和不必要谨慎中恢复。

Result: AdvChain显著增强模型对越狱攻击和CoT劫持的鲁棒性，减少对良性提示的过度拒绝，实现安全-效用的良好平衡且不影响推理能力。

Conclusion: 为构建更强大可靠的推理模型开辟新方向。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in
complex problem-solving through Chain-of-Thought (CoT) reasoning. However, the
multi-step nature of CoT introduces new safety challenges that extend beyond
conventional language model alignment. We identify a failure mode in current
safety CoT tuning methods: the \textit{snowball effect}, where minor reasoning
deviations progressively amplify throughout the thought process, leading to
either harmful compliance or excessive refusal. This effect stems from models
being trained to imitate perfect reasoning scripts without learning to
self-correct. To address this limitation, we propose AdvChain, an alignment
paradigm that teaches models dynamic self-correction through adversarial CoT
tuning. Our method involves constructing a dataset containing
Temptation-Correction and Hesitation-Correction samples, where models learn to
recover from harmful reasoning drifts and unnecessary cautions. Extensive
experiments show that AdvChain significantly enhances robustness against
jailbreak attacks and CoT hijacking while substantially reducing over-refusal
on benign prompts, achieving a superior safety-utility balance without
compromising reasoning capabilities. Our work establishes a new direction for
building more robust and reliable reasoning models.

</details>


### [93] [G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge](https://arxiv.org/abs/2509.24276)
*Linhao Luo,Zicheng Zhao,Junnan Liu,Zhangchi Qiu,Junnan Dong,Serge Panev,Chen Gong,Thuy-Trang Vu,Gholamreza Haffari,Dinh Phung,Alan Wee-Chung Liew,Shirui Pan*

Main category: cs.AI

TL;DR: 提出G - reasoner框架解决现有RAG在知识密集任务的问题，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有RAG在知识密集任务表现不佳，图增强RAG存在可扩展性和泛化性问题，需更好方法处理图结构知识。

Method: 提出G - reasoner框架，用QuadGraph统一知识源，引入34M参数图基础模型并与大语言模型集成，采用混合精度训练和分布式消息传递确保可扩展性和效率。

Result: 在六个基准测试中，G - reasoner始终优于现有基线，显著增强大语言模型推理能力，实现高效性和跨图泛化。

Conclusion: G - reasoner是一个有效的统一框架，能用于不同图结构知识推理，具有良好性能和泛化能力。

Abstract: Large language models (LLMs) excel at complex reasoning but remain limited by
static and incomplete parametric knowledge. Retrieval-augmented generation
(RAG) mitigates this by incorporating external knowledge, yet existing RAGs
struggle with knowledge-intensive tasks due to fragmented information and weak
modeling of knowledge structure. Graphs offer a natural way to model
relationships within knowledge, but LLMs are inherently unstructured and cannot
effectively reason over graph-structured data. Recent graph-enhanced RAG
(GraphRAG) attempts to bridge this gap by constructing tailored graphs and
enabling LLMs to reason on them. However, these methods often depend on ad-hoc
graph designs, heuristic search, or costly agent pipelines, which hinder
scalability and generalization. To address these challenges, we present
G-reasoner, a unified framework that integrates graph and language foundation
models for reasoning over diverse graph-structured knowledge. Central to our
approach is QuadGraph, a standardized four-layer abstraction that unifies
heterogeneous knowledge sources into a common graph representation. Building on
this, we introduce a 34M-parameter graph foundation model (GFM) that jointly
captures graph topology and textual semantics, and is integrated with LLMs to
enhance reasoning in downstream applications. To ensure scalability and
efficiency, mixed-precision training and distributed message-passing are
implemented to scale GFM with more GPUs. Extensive experiments on six
benchmarks show that G-reasoner consistently outperforms state-of-the-art
baselines, significantly enhances LLM reasoning, and achieves strong efficiency
and cross-graph generalization.

</details>


### [94] [SCI-Verifier: Scientific Verifier with Thinking](https://arxiv.org/abs/2509.24285)
*Shenghe Zheng,Chenyu Huang,Fangchen Yu,Junchi Yao,Jingqi Ye,Tao Chen,Yun Luo,Ning Ding,LEI BAI,Ganqu Cui,Peng Ye*

Main category: cs.AI

TL;DR: 针对大语言模型科学推理答案验证难题，提出数据和模型层面解决方案，构建SCI - VerifyBench基准和SCI - Verifier验证器，提供科学验证框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于科学推理时，答案验证因格式复杂和表达多样而具挑战性，现有验证研究存在评估标准不系统、学科覆盖不足及依赖规则设计或提示工程等局限。

Method: 数据层面构建跨学科基准SCI - VerifyBench，基于真实大语言模型响应并进行特定领域等价变换；模型层面引入统一推理增强验证器SCI - Verifier并进行后训练。

Result: SCI - Verifier展现出强大逻辑推理和等价判断能力，输出简洁稳定。

Conclusion: SCI - VerifyBench和SCI - Verifier为科学验证提供了原则性框架，可提升大语言模型在科学领域的可靠性和适用性。

Abstract: As large language models (LLMs) are increasingly applied to scientific
reasoning, the complexity of answer formats and the diversity of equivalent
expressions make answer verification a critical yet challenging task. Existing
verification studies in scientific domains suffer from two major limitations:
(a) the absence of systematic evaluation standards and insufficient
disciplinary coverage, which hinders their comprehensive assessment; and (b)
heavy reliance on cumbersome rule design or prompt engineering, which reduces
their effectiveness in complex reasoning scenarios or limits their
cross-disciplinary generalization. To address these challenges, we propose
solutions at both the data and model levels. On the data side, we construct
SCI-VerifyBench, a cross-disciplinary benchmark covering mathematics, physics,
biology, chemistry, and general scientific QA. The benchmark is built from real
LLM responses and enhanced with domain-specific equivalence transformations
that generate challenging and realistic data. Model-based and expert
annotations ensure both quality and diversity, enabling rigorous evaluation of
verification ability. On the model side, we emphasize the importance of
reasoning for verification and introduce SCI-Verifier, a unified
reasoning-augmented verifier for scientific domains. Through post-training,
SCI-Verifier demonstrates strong logical reasoning and equivalence judgment
capabilities while maintaining concise and stable outputs. Together,
SCI-VerifyBench and SCI-Verifier provide a principled framework for scientific
verification, offering both systematic evaluation and practical pathways to
enhance the reliability and applicability of LLMs in scientific domains.

</details>


### [95] [Experience Paper: Adopting Activity Recognition in On-demand Food Delivery Business](https://arxiv.org/abs/2509.24303)
*Huatao Xu,Yan Zhang,Wei Gao,Guobin Shen,Mo Li*

Main category: cs.AI

TL;DR: 本文介绍了人类活动识别（HAR）技术在按需食品配送行业的全国首次部署，成功适配模型，展示效益并分享经验、开源模型。


<details>
  <summary>Details</summary>
Motivation: 将人类活动识别（HAR）技术应用于按需食品配送行业，挖掘其在实际应用中的潜力。

Method: 将最先进的LIMU - BERT基础模型适配到配送平台，分三个阶段从扬州市可行性研究推进到全国367个城市、50万快递员参与的部署。

Result: 实现了一系列下游应用，大规模测试显示出显著的运营和经济效益。

Conclusion: HAR技术在现实应用中具有变革潜力，同时分享部署经验并开源预训练模型。

Abstract: This paper presents the first nationwide deployment of human activity
recognition (HAR) technology in the on-demand food delivery industry. We
successfully adapted the state-of-the-art LIMU-BERT foundation model to the
delivery platform. Spanning three phases over two years, the deployment
progresses from a feasibility study in Yangzhou City to nationwide adoption
involving 500,000 couriers across 367 cities in China. The adoption enables a
series of downstream applications, and large-scale tests demonstrate its
significant operational and economic benefits, showcasing the transformative
potential of HAR technology in real-world applications. Additionally, we share
lessons learned from this deployment and open-source our LIMU-BERT pretrained
with millions of hours of sensor data.

</details>


### [96] [MedMMV: A Controllable Multimodal Multi-Agent Framework for Reliable and Verifiable Clinical Reasoning](https://arxiv.org/abs/2509.24314)
*Hongjun Liu,Yinghao Zhu,Yuhui Wang,Yitao Long,Zeyu Lai,Lequan Yu,Chen Zhao*

Main category: cs.AI

TL;DR: 现有MLLMs在医疗诊断存在不稳定问题，提出MedMMV框架，在多基准测试中提升准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在诊断案例中存在早期证据解释不稳定、产生幻觉等问题，需要约束随机性和幻觉并产生可审计决策流的临床推理代理。

Method: 引入MedMMV框架，通过多样化短滚动稳定推理，在幻觉检测器监督下将中间步骤基于结构化证据图，用组合不确定性评分器聚合候选路径。

Result: 在六个医疗基准测试中，MedMMV准确率最高提升12.7%，可靠性更优，盲法医生评估证实其提高推理真实性且不牺牲信息内容。

Conclusion: 通过可验证的多智能体过程控制不稳定性，为在临床决策支持等高风险领域部署可信AI系统提供了可靠途径。

Abstract: Recent progress in multimodal large language models (MLLMs) has demonstrated
promising performance on medical benchmarks and in preliminary trials as
clinical assistants. Yet, our pilot audit of diagnostic cases uncovers a
critical failure mode: instability in early evidence interpretation precedes
hallucination, creating branching reasoning trajectories that cascade into
globally inconsistent conclusions. This highlights the need for clinical
reasoning agents that constrain stochasticity and hallucination while producing
auditable decision flows. We introduce MedMMV, a controllable multimodal
multi-agent framework for reliable and verifiable clinical reasoning. MedMMV
stabilizes reasoning through diversified short rollouts, grounds intermediate
steps in a structured evidence graph under the supervision of a Hallucination
Detector, and aggregates candidate paths with a Combined Uncertainty scorer. On
six medical benchmarks, MedMMV improves accuracy by up to 12.7% and, more
critically, demonstrates superior reliability. Blind physician evaluations
confirm that MedMMV substantially increases reasoning truthfulness without
sacrificing informational content. By controlling instability through a
verifiable, multi-agent process, our framework provides a robust path toward
deploying trustworthy AI systems in high-stakes domains like clinical decision
support.

</details>


### [97] [humancompatible.detect: a Python Toolkit for Detecting Bias in AI Models](https://arxiv.org/abs/2509.24340)
*German M. Matilla,Jiri Nemecek,Illia Kryvoviaz,Jakub Marecek*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: There is a strong recent emphasis on trustworthy AI. In particular,
international regulations, such as the AI Act, demand that AI practitioners
measure data quality on the input and estimate bias on the output of high-risk
AI systems. However, there are many challenges involved, including scalability
(MMD) and computability (Wasserstein-1) issues of traditional methods for
estimating distances on measure spaces. Here, we present
humancompatible.detect, a toolkit for bias detection that addresses these
challenges. It incorporates two newly developed methods to detect and evaluate
bias: maximum subgroup discrepancy (MSD) and subsampled $\ell_\infty$
distances. It has an easy-to-use API documented with multiple examples.
humancompatible.detect is licensed under the Apache License, Version 2.0.

</details>


### [98] [Fin-Ally: Pioneering the Development of an Advanced, Commonsense-Embedded Conversational AI for Money Matters](https://arxiv.org/abs/2509.24342)
*Sarmistha Das,Priya Mathur,Ishani Sharma,Sriparna Saha,Kitsuchart Pasupa,Alka Maurya*

Main category: cs.AI

TL;DR: 本文提出Fin - Solution 2.O，含多轮金融对话数据集Fin - Vault和统一模型Fin - Ally，实验表明该方法能生成更优质金融指导。


<details>
  <summary>Details</summary>
Motivation: 大规模微调大语言模型会产生不专业言论，且特定领域数据集稀缺，此前研究聚焦孤立组件，存在研究空白。

Method: 提出Fin - Solution 2.O，引入多轮金融对话数据集Fin - Vault，集成统一模型Fin - Ally，利用COMET - BART嵌入常识上下文，并用DPO机制优化。

Result: 综合实验结果显示，引入常识上下文能使语言模型生成更精细、文本精确且专业的金融指导。

Conclusion: 该方法可作为金融科技领域的下一代人工智能解决方案。

Abstract: The exponential technological breakthrough of the FinTech industry has
significantly enhanced user engagement through sophisticated advisory chatbots.
However, large-scale fine-tuning of LLMs can occasionally yield unprofessional
or flippant remarks, such as ``With that money, you're going to change the
world,'' which, though factually correct, can be contextually inappropriate and
erode user trust. The scarcity of domain-specific datasets has led previous
studies to focus on isolated components, such as reasoning-aware frameworks or
the enhancement of human-like response generation. To address this research
gap, we present Fin-Solution 2.O, an advanced solution that 1) introduces the
multi-turn financial conversational dataset, Fin-Vault, and 2) incorporates a
unified model, Fin-Ally, which integrates commonsense reasoning, politeness,
and human-like conversational dynamics. Fin-Ally is powered by
COMET-BART-embedded commonsense context and optimized with a Direct Preference
Optimization (DPO) mechanism to generate human-aligned responses. The novel
Fin-Vault dataset, consisting of 1,417 annotated multi-turn dialogues, enables
Fin-Ally to extend beyond basic account management to provide personalized
budgeting, real-time expense tracking, and automated financial planning. Our
comprehensive results demonstrate that incorporating commonsense context
enables language models to generate more refined, textually precise, and
professionally grounded financial guidance, positioning this approach as a
next-generation AI solution for the FinTech sector. Dataset and codes are
available at: https://github.com/sarmistha-D/Fin-Ally

</details>


### [99] [From Static to Dynamic: Adaptive Monte Carlo Search for Mathematical Process Supervision](https://arxiv.org/abs/2509.24351)
*Jie Ma,Shihao Qi,Rui Xing,Ziang Yin,Bifan Wei,Jun Liu,Tongliang Liu*

Main category: cs.AI

TL;DR: 提出自适应蒙特卡罗搜索（AMCS）框架改进过程奖励模型（PRM）数据生成，构建MathSearch - 200K数据集，实验表明Qwen2.5 - Math - 7B - PRM - AMCS表现出色，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有方法在自动化数据生成过程中基于固定预算采样策略估计推理步骤质量和进行路径扩展，存在低效和不灵活问题。

Method: 提出AMCS框架，在节点值估计和路径扩展层面将数据生成从固定静态转为自适应动态搜索，构建MathSearch - 200K数据集。

Result: Qwen2.5 - Math - 7B - PRM - AMCS在MATH500上准确率达76.2%，优于所有基线PRMs，7B模型超72B弱监督模型，在分布外问题上有优势。

Conclusion: AMCS框架有效提升了过程奖励模型训练数据质量，增强了大语言模型的复杂数学推理能力，且具有强泛化能力。

Abstract: The quality of process data plays a key role in training a Process Reward
Model (PRM), which can enhance the complex mathematical reasoning capability of
large language models. Existing methods estimate the quality of reasoning steps
based on a fixed-budget sampling strategy and navigate a vast search space to
perform path expansion during the automated data generation process, resulting
in their inefficiency and inflexibility. To address these issues, we propose
Adaptive Monte Carlo Search (AMCS), a framework that transforms data generation
from fixed, static to adaptive, dynamic search at the level of node value
estimation and path expansion. On one hand, AMCS adaptively refines estimation
by allocating more samples to uncertain reasoning steps while using fewer
samples for those that are easier to estimate. On the other hand, it enhances
the path expansion through a Monte Carlo algorithm with a temporally adaptive
policy that begins with broad exploration and gradually shifts toward
exploiting the most promising directions. With AMCS, we construct a large-scale
dataset MathSearch-200K of about 200K process supervision examples for training
PRMs. To verify the effectiveness of our method, we conduct extensive
experiments on four mathematical reasoning benchmarks. Experimental results
show that Qwen2.5-Math-7B-PRM-AMCS achieves up to 76.2% accuracy on MATH500
with GLM-4-9B, outperforming all baseline PRMs. Notably, a 7B model supervised
by Qwen2.5-Math-7B-PRM-AMCS surpasses a 72B model with weaker supervision.
Moreover, Qwen2.5-Math-7B-PRM-AMCS maintains consistent advantages on
out-of-distribution problems, demonstrating strong generalization capability.
Our code is available at https://github.com/reml-group/AMCS.

</details>


### [100] [Plan before Solving: Problem-Aware Strategy Routing for Mathematical Reasoning with LLMs](https://arxiv.org/abs/2509.24377)
*Shihao Qi,Jie Ma,Ziang Yin,Lingling Zhang,Jian Zhang,Jun Liu,Feng Tian,Tongliang Liu*

Main category: cs.AI

TL;DR: 现有数学推理方法用固定策略引导大语言模型，存在不足。本文提出PRISM框架，分策略规划和目标执行两阶段，实验表明其优于现有策略和基线。


<details>
  <summary>Details</summary>
Motivation: 现有单一策略引导大语言模型进行数学推理时，无法适应特定问题需求，忽略了有效性和效率的权衡。

Method: 提出PRISM框架，构建MathStrat数据集，训练轻量级策略适配器，推理时使用自适应路由策略。

Result: 在五个数学推理基准测试中，PRISM始终优于单个策略和集成基线，不同基础模型上提升0.9% - 7.6%。

Conclusion: 自适应路由方法对不同模型架构的数学推理任务有显著益处，代码已开源。

Abstract: Existing methods usually leverage a fixed strategy, such as natural language
reasoning, code-augmented reasoning, tool-integrated reasoning, or
ensemble-based reasoning, to guide Large Language Models (LLMs) to perform
mathematical reasoning. Our analysis reveals that the single strategy cannot
adapt to problem-specific requirements and thus overlooks the trade-off between
effectiveness and efficiency. To address these issues, we propose Planning and
Routing through Instance-Specific Modeling (PRISM), a novel framework that
decouples mathematical reasoning into two stages: strategy planning and
targeted execution. Specifically, we first curate a multi-strategy preference
dataset, which we call MathStrat, capturing correctness, process quality, and
computational efficiency for each problem--strategy pair. Then, we train a
lightweight Strategy Adapter based on the dataset to obtain confidence
distributions over the mentioned four reasoning strategies. At inference time,
an adaptive routing policy dynamically tailors the reasoning approach based on
predictor confidence. It directs the model to use single-strategy execution for
high-confidence predictions, dual-strategy verification for competitive
scenarios, or comprehensive multi-strategy exploration for uncertain cases.
Extensive experiments across five mathematical reasoning benchmarks demonstrate
that PRISM consistently outperforms individual strategies and ensemble
baselines, achieving improvements ranging from 0.9% to 7.6% across different
base models. The adaptive routing approach shows particularly strong benefits
for mathematical reasoning tasks across diverse model architectures. Our code
is released at https://github.com/reml-group/PRISM.

</details>


### [101] [Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention](https://arxiv.org/abs/2509.24393)
*Yichi Zhang,Yue Ding,Jingwen Yang,Tianwei Luo,Dongbai Li,Ranjie Duan,Qiang Liu,Hang Su,Yinpeng Dong,Jun Zhu*

Main category: cs.AI

TL;DR: 现有大推理模型的思维链推理存在有害内容问题，本文提出Intervened Preference Optimization (IPO)方法解决推理安全对齐问题，实验表明该方法能显著提升安全性。


<details>
  <summary>Details</summary>
Motivation: 大推理模型的思维链推理存在有害内容，现有方法忽视推理安全，影响模型可信度并带来应用风险，因此要关注推理本身的安全对齐。

Method: 深入研究安全推理的特征，提出Intervened Preference Optimization (IPO)方法，通过用安全触发步骤替代合规步骤并构建偏好学习对来强化安全推理。

Result: 在越狱和对抗安全基准测试中，IPO显著提高了推理和响应的整体安全性，相对降低了30%以上的有害性，同时在各种推理任务中保持了出色性能。

Conclusion: 强调了推理显式对齐的重要性，为更安全的大推理模型提供了实用途径。

Abstract: Although Large Reasoning Models (LRMs) have progressed in solving complex
problems, their chain-of-thought (CoT) reasoning often contains harmful content
that can persist even when the final responses appear safe. We show that this
issue still remains in existing methods which overlook the unique significance
of safe reasoning, undermining their trustworthiness and posing potential risks
in applications if unsafe reasoning is accessible for and exploited by
malicious users. We therefore shift our focus to aligning the safety of
reasoning itself in this paper and explore process supervision as the solution.
However, simply rewarding safe reasoning proves inadequate due to low rollout
diversity and limited training signals. To tackle this challenge, we first
delve into the characteristics of safe reasoning and uncover several critical
insights that 1) safe reasoning is often consolidated by a few critical steps
of safety triggers; 2) compliance cues strongly correlate with unsafe
continuations; and 3) corrective interventions reliably steer unsafe
trajectories towards safer traces. Motivated by these, we propose Intervened
Preference Optimization (IPO), an alignment method that enforces safe reasoning
by substituting compliance steps with safety triggers and constructing pairs
for preference learning with strong signals. Experiments on jailbreak and
adversarial safety benchmarks demonstrate that IPO remarkably improves overall
safety regarding both reasoning and responses, outperforming SFT-based and
RL-based baselines with a relative reduction of over 30% in harmfulness, while
preserving excellent performance across diverse reasoning tasks. The results
highlight the importance of explicit alignment for reasoning and provide a
practical path to safer LRMs.

</details>


### [102] [ContextPRM: Leveraging Contextual Coherence for multi-domain Test-Time Scaling](https://arxiv.org/abs/2509.24460)
*Haotian Zhang,Liu Liu,Baosheng Yu,Jiayan Qiu,Likang Xiao,Yanwei Ren,Quan Chen,Xianglong Liu*

Main category: cs.AI

TL;DR: 现有PRMs在跨领域泛化能力有限，提出通过建模领域无关逻辑流的方法，新模型ContextPRM在多领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 大多数PRMs因领域特定训练数据少和基于知识的学习模式，跨领域泛化能力受限，需解决该问题。

Method: 将学习目标从验证特定领域知识转变为建模领域无关逻辑流，采用新的数据标注和训练框架，聚焦思维链步骤间的上下文连贯性。

Result: ContextPRM在MMLU - Pro的九个非数学领域通过加权多数投票比多数投票基线平均准确率提高6.5%，优于VersaPRM和其他数学聚焦的PRMs。

Conclusion: 提出的方法能增强模型在不同领域的泛化能力，模型在数学和非数学领域均有稳定表现。

Abstract: Process reward models (PRMs) have demonstrated significant efficacy in
enhancing the mathematical reasoning capabilities of large language models
(LLMs) by leveraging test-time scaling (TTS). However, while most PRMs exhibit
substantial gains in mathematical domains, the scarcity of domain-specific
training data and knowledge-based learning patterns limits their generalization
ability when faced with other domains. To address this limitation, we shift the
learning objective from verifying domain-specific knowledge to modeling
domain-agnostic logical flow. Centering on contextual coherence between
chain-of-thought (CoT) steps, our approach is realized through a novel data
annotation and training framework, which enhances the model's generalization
capabilities across diverse domains. For instance, our resulting model,
ContextPRM, achieves a notable 6.5% average accuracy improvement over the
majority voting baseline via weighted majority voting across nine
non-mathematical domains in MMLU-Pro, including law, history, and philosophy,
significantly surpassing the 2.2% improvement from VersaPRM and 0.5% gains from
other mathematics-focused PRMs, demonstrating consistent performance across
both mathematical and non-mathematical domains.

</details>


### [103] [Overcoming Over-Fitting in Constraint Acquisition via Query-Driven Interactive Refinement](https://arxiv.org/abs/2509.24489)
*Vasileios Balafas,Dimos Tsouros,Nikolaos Ploskas,Kostas Stergiou*

Main category: cs.AI

TL;DR: 介绍了一种混合约束获取框架，解决过拟合问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决约束获取中被动方法易过拟合、主动方法查询密集的问题。

Method: 结合被动学习生成初始候选，通过查询驱动的交互细化阶段识别过拟合约束，利用子集探索机制恢复有效子结构，最后进行主动学习确保模型完整。

Result: 实验表明交互细化阶段对从有限示例中实现高目标模型覆盖率和整体模型准确性至关重要，且查询复杂度可控。

Conclusion: 该框架在数据有限场景下的约束获取方面取得了重大进展。

Abstract: Manual modeling in Constraint Programming is a substantial bottleneck, which
Constraint Acquisition (CA) aims to automate. However, passive CA methods are
prone to over-fitting, often learning models that include spurious global
constraints when trained on limited data, while purely active methods can be
query-intensive. We introduce a hybrid CA framework specifically designed to
address the challenge of over-fitting in CA. Our approach integrates passive
learning for initial candidate generation, a query-driven interactive
refinement phase that utilizes probabilistic confidence scores (initialized by
machine learning priors) to systematically identify over-fitted constraints,
and a specialized subset exploration mechanism to recover valid substructures
from rejected candidates. A final active learning phase ensures model
completeness. Extensive experiments on diverse benchmarks demonstrate that our
interactive refinement phase is crucial for achieving high target model
coverage and overall model accuracy from limited examples, doing so with
manageable query complexity. This framework represents a substantial
advancement towards robust and practical constraint acquisition in data-limited
scenarios.

</details>


### [104] [Neuroplasticity-inspired dynamic ANNs for multi-task demand forecasting](https://arxiv.org/abs/2509.24495)
*Mateusz Żarski,Sławomir Nowaczyk*

Main category: cs.AI

TL;DR: 本文提出神经可塑性多任务网络（NMT - Net）用于多任务需求预测，通过动态网络适应提升性能，在三个数据集验证效果优，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注推理时间动态性或计算效率，本文希望实现计算图在训练期间的结构适应性，提升多任务需求预测效果。

Method: 提出NMT - Net，新任务触发动态网络适应，含基于相似度的任务识别和候选ANN头的选择性训练，再根据性能评估和集成。

Result: 在三个Kaggle真实多任务需求预测数据集上评估，相比传统基线和最先进的多任务学习方法，RMSE和标准差更低，性能和一致性更优。

Conclusion: NMT - Net为时间序列预测中的多任务和持续学习提供了可扩展、适应性强的解决方案。

Abstract: This paper introduces a novel approach to Dynamic Artificial Neural Networks
(D-ANNs) for multi-task demand forecasting called Neuroplastic Multi-Task
Network (NMT-Net). Unlike conventional methods focusing on inference-time
dynamics or computational efficiency, our proposed method enables structural
adaptability of the computational graph during training, inspired by
neuroplasticity as seen in biological systems. Each new task triggers a dynamic
network adaptation, including similarity-based task identification and
selective training of candidate ANN heads, which are then assessed and
integrated into the model based on their performance. We evaluated our
framework using three real-world multi-task demand forecasting datasets from
Kaggle. We demonstrated its superior performance and consistency, achieving
lower RMSE and standard deviation compared to traditional baselines and
state-of-the-art multi-task learning methods. NMT-Net offers a scalable,
adaptable solution for multi-task and continual learning in time series
prediction. The complete code for NMT-Net is available from our GitHub
repository.

</details>


### [105] [Experience-guided reflective co-evolution of prompts and heuristics for automatic algorithm design](https://arxiv.org/abs/2509.24509)
*Yihong Liu,Junyi Li,Wayne Xin Zhao,Hongyu Lu,Ji-Rong Wen*

Main category: cs.AI

TL;DR: 传统组合优化问题求解需大量专业知识和实现工作，现有大语言模型自动启发式设计方法易陷入局部最优，本文提出EvoPH框架解决该问题，实验证明其能降低相对误差。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型自动启发式设计方法在解决组合优化问题时易陷入局部最优。

Method: 提出经验引导的提示与启发式反射协同进化（EvoPH）框架，将岛屿迁移模型与精英选择算法结合，使提示与启发式算法协同进化。

Result: 在旅行商问题和装箱问题上，EvoPH相对最优解的相对误差最低。

Conclusion: EvoPH推动了基于大语言模型的自动算法设计领域的发展。

Abstract: Combinatorial optimization problems are traditionally tackled with
handcrafted heuristic algorithms, which demand extensive domain expertise and
significant implementation effort. Recent progress has highlighted the
potential of automatic heuristics design powered by large language models
(LLMs), enabling the automatic generation and refinement of heuristics. These
approaches typically maintain a population of heuristics and employ LLMs as
mutation operators to evolve them across generations. While effective, such
methods often risk stagnating in local optima. To address this issue, we
propose the Experience-Guided Reflective Co-Evolution of Prompt and Heuristics
(EvoPH) for automatic algorithm design, a novel framework that integrates the
island migration model with the elites selection algorithm to simulate diverse
heuristics populations. In EvoPH, prompts are co-evolved with heuristic
algorithms, guided by performance feedback. We evaluate our framework on two
problems, i.e., Traveling Salesman Problem and Bin Packing Problem.
Experimental results demonstrate that EvoPH achieves the lowest relative error
against optimal solutions across both datasets, advancing the field of
automatic algorithm design with LLMs.

</details>


### [106] [LTL$_f$ Learning Meets Boolean Set Cover](https://arxiv.org/abs/2509.24616)
*Gabriel Bathie,Nathanaël Fijalkow,Théo Matricon,Baptiste Mouillon,Pierre Vandenhove*

Main category: cs.AI

TL;DR: 提出新CPU工具Bolt，在LTLf公式学习上比现有技术快超100倍，公式更小或相等。


<details>
  <summary>Details</summary>
Motivation: LTLf公式学习在多领域有应用，需提升学习效率和优化公式大小。

Method: 利用布尔集覆盖问题作为子程序，用布尔连接词组合现有公式。

Result: 在70%的基准测试中学习公式速度比现有技术快超100倍，98%的情况下公式更小或相等。

Conclusion: 该方法在效率和公式大小之间实现了新颖的权衡。

Abstract: Learning formulas in Linear Temporal Logic (LTLf) from finite traces is a
fundamental research problem which has found applications in artificial
intelligence, software engineering, programming languages, formal methods,
control of cyber-physical systems, and robotics. We implement a new CPU tool
called Bolt improving over the state of the art by learning formulas more than
100x faster over 70% of the benchmarks, with smaller or equal formulas in 98%
of the cases. Our key insight is to leverage a problem called Boolean Set Cover
as a subroutine to combine existing formulas using Boolean connectives. Thanks
to the Boolean Set Cover component, our approach offers a novel trade-off
between efficiency and formula size.

</details>


### [107] ["Stop replacing salt with sugar!'': Towards Intuitive Human-Agent Teaching](https://arxiv.org/abs/2509.24651)
*Nikolaos Kondylidis,Andrea Rafanelli,Ilaria Tiddi,Annette ten Teije,Frank van Harmelen*

Main category: cs.AI

TL;DR: 本文提出直观的人机教学架构，使智能体从少量示例中学习，应用于食材替换任务，实验表明智能体学习效率高。


<details>
  <summary>Details</summary>
Motivation: 复制人类从少量示例快速学习新概念的能力，尤其是在主观任务数据稀缺的情况下。

Method: 提出人机教学架构，利用领域知识拓宽智能体任务理解，采用学习方法使其从少量示例高效学习，优化人类选择示例的方式。

Result: 在食材替换任务中，智能体仅需100个示例就能达到使用5万个示例完整训练集一半的性能。

Conclusion: 按策略顺序提供示例并结合利用外部符号知识的学习方法，能让智能体更高效地泛化。

Abstract: Humans quickly learn new concepts from a small number of examples.
Replicating this capacity with Artificial Intelligence (AI) systems has proven
to be challenging. When it comes to learning subjective tasks-where there is an
evident scarcity of data-this capacity needs to be recreated. In this work, we
propose an intuitive human-agent teaching architecture in which the human can
teach an agent how to perform a task by providing demonstrations, i.e.,
examples. To have an intuitive interaction, we argue that the agent should be
able to learn incrementally from a few single examples. To allow for this, our
objective is to broaden the agent's task understanding using domain knowledge.
Then, using a learning method to enable the agent to learn efficiently from a
limited number of examples. Finally, to optimize how human can select the most
representative and less redundant examples to provide the agent with. We apply
our proposed method to the subjective task of ingredient substitution, where
the agent needs to learn how to substitute ingredients in recipes based on
human examples. We replicate human input using the Recipe1MSubs dataset. In our
experiments, the agent achieves half its task performance after only 100
examples are provided, compared to the complete training set of 50k examples.
We show that by providing examples in strategic order along with a learning
method that leverages external symbolic knowledge, the agent can generalize
more efficiently.

</details>


### [108] [Successful Misunderstandings: Learning to Coordinate Without Being Understood](https://arxiv.org/abs/2509.24660)
*Nikolaos Kondylidis,Anil Yaman,Frank van Harmelen,Erman Acar,Annette ten Teije*

Main category: cs.AI

TL;DR: 研究信号博弈中交流与协调的关系，发现存在‘成功误解’现象，还指出确保共享解释出现的条件。


<details>
  <summary>Details</summary>
Motivation: 验证以协调评估交流的假设，研究感知环境差异大的群体中交流的出现。

Method: 在信号博弈中，个体开发新信号词汇来实现协调，且除交流信号和奖励外无共同观察。

Result: 群体总能达到最优协调水平，但存在‘成功误解’，有此现象的群体难与新伙伴协调，‘成功误解’难发现和修复。

Conclusion: 至少三个相互交流的主体是确保共享解释出现的两个最低条件之一，可弥补信号使用无共享观察的不足。

Abstract: The main approach to evaluating communication is by assessing how well it
facilitates coordination. If two or more individuals can coordinate through
communication, it is generally assumed that they understand one another. We
investigate this assumption in a signaling game where individuals develop a new
vocabulary of signals to coordinate successfully. In our game, the individuals
do not have common observations besides the communication signal and outcome of
the interaction, i.e. received reward. This setting is used as a proxy to study
communication emergence in populations of agents that perceive their
environment very differently, e.g. hybrid populations that include humans and
artificial agents. Agents develop signals, use them, and refine interpretations
while not observing how other agents are using them. While populations always
converge to optimal levels of coordination, in some cases, interacting agents
interpret and use signals differently, converging to what we call successful
misunderstandings. However, agents of population that coordinate using
misaligned interpretations, are unable to establish successful coordination
with new interaction partners. Not leading to coordination failure immediately,
successful misunderstandings are difficult to spot and repair. Having at least
three agents that all interact with each other are the two minimum conditions
to ensure the emergence of shared interpretations. Under these conditions, the
agent population exhibits this emergent property of compensating for the lack
of shared observations of signal use, ensuring the emergence of shared
interpretations.

</details>


### [109] [On the Self-awareness of Large Reasoning Models' Capability Boundaries](https://arxiv.org/abs/2509.24711)
*Qingjie Zhang,Yujia Fu,Yang Wang,Liu Yan,Tao Wei,Ke Xu,Minlie Huang,Han Qiu*

Main category: cs.AI

TL;DR: 研究大推理模型（LRMs）能力边界的自我认知，提出监测策略提升其可靠性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前LRMs在难题上存在无效推理问题，现有回答范式忽视问题与模型能力边界关系，因此研究LRMs是否有能力边界的自我认知。

Method: 观察LRMs推理信心表达，分析黑盒模型推理表达和白盒模型最后输入标记的隐藏状态，提出推理表达监测和隐藏状态监测两种优化策略。

Result: 实验表明边界感知策略能让LRMs避免无效推理，不牺牲准确性，最多可减少62.7 - 93.6%的令牌使用量。

Conclusion: 所提策略能显著提高LRMs的可靠性和效率。

Abstract: Large Reasoning Models (LRMs) have shown impressive performance on complex
reasoning tasks such as mathematics, yet they also display misbehaviors that
expose their limitations. In particular, when faced with hard questions, LRMs
often engage in unproductive reasoning until context limit, producing wrong
answers while wasting substantial computation. This phenomenon reflects a
fundamental issue: current answering paradigms overlook the relationship
between questions and LRMs' capability boundaries. In this paper, we
investigate whether LRMs possess self-awareness of capability boundaries. We
begin by an observation that LRMs may know what they cannot solve through
expressed reasoning confidence. For black-box models, we find that reasoning
expressions reveal boundary signals, with accelerated growing confidence
trajectory for solvable problems but convergent uncertainty trajectory for
unsolvable ones. For white-box models, we show that hidden states of the last
input token encode boundary information, with solvable and unsolvable problems
linearly separable even before reasoning begins. Building on these findings, we
propose two simple yet effective optimization strategies: reasoning expression
monitoring and hidden states monitoring. Experiments demonstrate that these
boundary-aware strategies enable LRMs to avoid unproductive reasoning without
sacrificing accuracy, significantly improving reliability and efficiency by
cutting token usage up to 62.7 - 93.6%.

</details>


### [110] [Spatial-Functional awareness Transformer-based graph archetype contrastive learning for Decoding Visual Neural Representations from EEG](https://arxiv.org/abs/2509.24761)
*Yueming Sun,Long Yang*

Main category: cs.AI

TL;DR: 提出SFTG框架增强基于EEG的视觉解码，经评估显著优于现有方法，凸显图学习与对比目标结合的潜力。


<details>
  <summary>Details</summary>
Motivation: EEG信号具有高维、嘈杂和非欧几里得性质，解码视觉神经表征是巨大挑战，需提升基于EEG的视觉解码能力。

Method: 提出SFTG框架，引入EEG图Transformer同时编码空间脑连接和时间神经动态；提出图原型对比学习以减轻高主体内变异性。

Result: 在Things - EEG数据集上的主体依赖和主体独立评估中，该方法显著优于先前的先进EEG解码方法。

Conclusion: 图基学习与对比目标的整合有变革性潜力，能增强基于EEG的脑解码，为更通用和稳健的神经表征铺平道路。

Abstract: Decoding visual neural representations from Electroencephalography (EEG)
signals remains a formidable challenge due to their high-dimensional, noisy,
and non-Euclidean nature. In this work, we propose a Spatial-Functional
Awareness Transformer-based Graph Archetype Contrastive Learning (SFTG)
framework to enhance EEG-based visual decoding. Specifically, we introduce the
EEG Graph Transformer (EGT), a novel graph-based neural architecture that
simultaneously encodes spatial brain connectivity and temporal neural dynamics.
To mitigate high intra-subject variability, we propose Graph Archetype
Contrastive Learning (GAC), which learns subject-specific EEG graph archetypes
to improve feature consistency and class separability. Furthermore, we conduct
comprehensive subject-dependent and subject-independent evaluations on the
Things-EEG dataset, demonstrating that our approach significantly outperforms
prior state-of-the-art EEG decoding methods.The results underscore the
transformative potential of integrating graph-based learning with contrastive
objectives to enhance EEG-based brain decoding, paving the way for more
generalizable and robust neural representations.

</details>


### [111] [From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent for LLM Logical Reasoning](https://arxiv.org/abs/2509.24765)
*Yunyao Zhang,Xinglang Zhang,Junxi Sheng,Wenbing Li,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.AI

TL;DR: 文章提出LogicAgent框架和RepublicQA基准，LogicAgent在RepublicQA上达SOTA，且能有效泛化到主流基准，证明半符号多视角推理对提升大模型逻辑能力有效。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视逻辑复杂度和语义复杂度的相互作用，难以应对人类推理中的复杂场景。

Method: 提出LogicAgent框架，在一阶逻辑中进行多视角推理，通过存在性导入检查减少空洞推理；引入RepublicQA基准，其具有大学难度和高语义丰富度。

Result: LogicAgent在RepublicQA上比强基线平均提高6.25%，在主流基准上平均提高7.05%。

Conclusion: 半符号多视角推理能有效提升大语言模型的逻辑性能。

Abstract: Logical reasoning is a fundamental capability of large language models
(LLMs). However, existing studies largely overlook the interplay between
logical complexity and semantic complexity, resulting in methods that struggle
to address challenging scenarios involving abstract propositions, ambiguous
contexts, and conflicting stances, which are central to human reasoning. For
this gap, we propose LogicAgent, a semiotic-square-guided framework designed to
jointly address logical complexity and semantic complexity. LogicAgent
explicitly performs multi-perspective deduction in first-order logic (FOL),
while mitigating vacuous reasoning through existential import checks that
incorporate a three-valued decision scheme (True, False, Uncertain) to handle
boundary cases more faithfully. Furthermore, to overcome the semantic
simplicity and low logical complexity of existing datasets, we introduce
RepublicQA, a benchmark that reaches college-level difficulty (FKGL = 11.94)
and exhibits substantially greater lexical and structural diversity than prior
benchmarks. RepublicQA is grounded in philosophical concepts, featuring
abstract propositions and systematically organized contrary and contradictory
relations, making it the most semantically rich resource for evaluating logical
reasoning. Experiments demonstrate that LogicAgent achieves state-of-the-art
performance on RepublicQA, with a 6.25% average gain over strong baselines, and
generalizes effectively to mainstream logical reasoning benchmarks including
ProntoQA, ProofWriter, FOLIO, and ProverQA, achieving an additional 7.05%
average gain. These results highlight the strong effectiveness of our
semiotic-grounded multi-perspective reasoning in boosting LLMs' logical
performance.

</details>


### [112] [TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models](https://arxiv.org/abs/2509.24803)
*Tong Guan,Zijie Meng,Dianqi Li,Shiyu Wang,Chao-Han Huck Yang,Qingsong Wen,Zuozhu Liu,Sabato Marco Siniscalchi,Ming Jin,Shirui Pan*

Main category: cs.AI

TL;DR: 本文引入TSR - Suite和TimeOmni - 1，前者支持评估和训练时间序列推理模型，后者是统一推理模型，实验显示其泛化能力强，在因果发现和事件预测任务上表现优于GPT - 4.1。


<details>
  <summary>Details</summary>
Motivation: 现有多模态时间序列数据集缺乏真正的时间序列推理任务和高质量数据，限制了实用时间序列推理模型的发展。

Method: 引入TSR - Suite定义四个原子任务，包含感知、外推和决策三种能力；提出TimeOmni - 1模型，分多阶段训练，整合多种任务场景、奖励函数和优化方法。

Result: TimeOmni - 1在所有任务上有强泛化能力，有效响应率高，因果发现准确率达64.0%（GPT - 4.1为35.9%），事件预测任务有效响应率比GPT - 4.1提高超6%。

Conclusion: TSR - Suite和TimeOmni - 1对时间序列推理模型的发展有积极作用，TimeOmni - 1表现优异。

Abstract: Recent advances in multimodal time series learning underscore a paradigm
shift from analytics centered on basic patterns toward advanced time series
understanding and reasoning. However, existing multimodal time series datasets
mostly remain at the level of surface alignment and question answering, without
reaching the depth of genuine reasoning. The absence of well-defined tasks that
genuinely require time series reasoning, along with the scarcity of
high-quality data, has limited progress in building practical time series
reasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite
(TSR-Suite), which formalizes four atomic tasks that span three fundamental
capabilities for reasoning with time series: (1) perception, acquired through
scenario understanding and causality discovery; (2) extrapolation, realized via
event-aware forecasting; and (3) decision-making, developed through
deliberation over perception and extrapolation. TSR-Suite is the first
comprehensive time series reasoning suite that supports not only thorough
evaluation but also the data pipeline and training of TSRMs. It contains more
than 23K samples, of which 2.3K are carefully curated through a human-guided
hierarchical annotation process. Building on this foundation, we introduce
TimeOmni-1, the first unified reasoning model designed to address diverse
real-world problems demanding time series reasoning. The model is trained in
multiple stages, integrating a mixture of task scenarios, novel reward
functions, and tailored optimizations. Experiments show that TimeOmni-1
delivers strong out-of-distribution generalization across all tasks and
achieves a high rate of valid responses. It significantly improves causality
discovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response
rate by over 6% compared to GPT-4.1 on the event-aware forecasting task.

</details>


### [113] [Query Circuits: Explaining How Language Models Answer User Prompts](https://arxiv.org/abs/2509.24808)
*Tung-Yu Wu,Fazl Barez*

Main category: cs.AI

TL;DR: 提出查询电路方法解释语言模型对特定输入的输出，解决两个挑战，发现模型中存在稀疏查询电路可恢复单查询性能，向忠实可扩展解释迈进。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法解释模型对特定输入查询的输出原因，需要本地输入级解释。

Method: 引入查询电路直接追踪模型中特定输入到输出的信息流，提出NDF指标评估电路，开发基于采样的方法识别稀疏电路。

Result: 在多个基准测试中发现模型中存在极稀疏的查询电路，如仅覆盖1.3%模型连接的电路可恢复MMLU问题约60%的性能。

Conclusion: 查询电路为语言模型处理单个输入的忠实、可扩展解释提供了一步。

Abstract: Explaining why a language model produces a particular output requires local,
input-level explanations. Existing methods uncover global capability circuits
(e.g., indirect object identification), but not why the model answers a
specific input query in a particular way. We introduce query circuits, which
directly trace the information flow inside a model that maps a specific input
to the output. Unlike surrogate-based approaches (e.g., sparse autoencoders),
query circuits are identified within the model itself, resulting in more
faithful and computationally accessible explanations. To make query circuits
practical, we address two challenges. First, we introduce Normalized Deviation
Faithfulness (NDF), a robust metric to evaluate how well a discovered circuit
recovers the model's decision for a specific input, and is broadly applicable
to circuit discovery beyond our setting. Second, we develop sampling-based
methods to efficiently identify circuits that are sparse yet faithfully
describe the model's behavior. Across benchmarks (IOI, arithmetic, MMLU, and
ARC), we find that there exist extremely sparse query circuits within the model
that can recover much of its performance on single queries. For example, a
circuit covering only 1.3% of model connections can recover about 60% of
performance on an MMLU questions. Overall, query circuits provide a step
towards faithful, scalable explanations of how language models process
individual inputs.

</details>


### [114] [Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity](https://arxiv.org/abs/2509.24836)
*Zhen Bi,Zhenlin Hu,Jinnan Yang,Mingyang Chen,Cheng Deng,Yida Xue,Zeyu Yang,Qing Shen,Zhenfang Liu,Kang Zhao,Ningyu Zhang,Jungang Lou*

Main category: cs.AI

TL;DR: 提出数据推理强度（DRI）指标，引入重新认知优化策略提升大语言模型逻辑推理性能，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视训练样本内部推理复杂性，数据推理潜力未充分挖掘，需提升大语言模型逻辑推理性能。

Method: 引入DRI指标量化样本潜在逻辑推理复杂性，提出重新认知优化策略系统提升训练数据逻辑推理强度。

Result: 实验表明该方法显著提升性能和泛化能力，在强化学习框架下验证有效。

Conclusion: 优先考虑数据推理复杂性对发挥大语言模型认知潜力至关重要。

Abstract: Recent advances in large language models (LLMs) highlight the importance of
training data structure and quality in shaping reasoning behavior. However,
most existing approaches focus on transforming data formats while neglecting
the internal reasoning complexity of training samples, leaving the reasoning
potential of data under-explored and underutilized. In this work, we posit that
LLM logical reasoning performance is jointly constrained by the potential of
the training data and the cognitive capacity of the model. To make this
relationship measurable, we introduce Data Reasoning Intensity (DRI), a novel
metric that quantifies the latent logical reasoning complexity of samples by
decomposing and aggregating their logical structures. This allows us to analyze
how well current LLMs utilize logical reasoning signals and identify
performance gaps relative to data potential. Based on this insight, we
introduce a re-cognizing optimization strategy that systematically enhances the
logical reasoning intensity of training data.Rather than increasing data
volume, our method re-optimizes existing samples to better align with the LLM's
logical reasoning boundary. Extensive experiments show that our approach
significantly improves performance and generalization over data-centric
strategies. We further validate our method under a reinforcement learning
framework. Our results indicate that prioritizing reasoning complexity in data
rather than sheer scale or superficial form is essential to realizing LLMs'
full cognitive potential.

</details>


### [115] [PhysicsMinions: Winning Gold Medals in the Latest Physics Olympiads with a Coevolutionary Multimodal Multi-Agent System](https://arxiv.org/abs/2509.24855)
*Fangchen Yu,Junchi Yao,Ziyi Wang,Haiyuan Wan,Youling Huang,Bo Zhang,Shuyue Hu,Dongzhan Zhou,Ning Ding,Ganqu Cui,Lei Bai,Wanli Ouyang,Peng Ye*

Main category: cs.AI

TL;DR: 提出PhysicsMinions多智能体系统解决物理奥赛问题，在HiPhO基准测试中取得三项突破，提供通用解题框架。


<details>
  <summary>Details</summary>
Motivation: 现有物理奥赛AI研究不足，单模型方法为主，开源多模态大语言模型难达金牌水平。

Method: 提出PhysicsMinions，含可视化、逻辑和评审三个工作室，通过迭代细化循环协同进化。

Result: 在HiPhO基准测试中，有强泛化性，提升开源模型获6枚金牌，开源Pass@32分数排名第4。

Conclusion: PhysicsMinions是可推广的奥赛解题框架，有跨学科应用潜力。

Abstract: Physics is central to understanding and shaping the real world, and the
ability to solve physics problems is a key indicator of real-world physical
intelligence. Physics Olympiads, renowned as the crown of competitive physics,
provide a rigorous testbed requiring complex reasoning and deep multimodal
understanding, yet they remain largely underexplored in AI research. Existing
approaches are predominantly single-model based, and open-source MLLMs rarely
reach gold-medal-level performance. To address this gap, we propose
PhysicsMinions, a coevolutionary multi-agent system for Physics Olympiad. Its
architecture features three synergistic studios: a Visual Studio to interpret
diagrams, a Logic Studio to formulate solutions, and a Review Studio to perform
dual-stage verification. The system coevolves through an iterative refinement
loop where feedback from the Review Studio continuously guides the Logic
Studio, enabling the system to self-correct and converge towards the ground
truth. Evaluated on the HiPhO benchmark spanning 7 latest physics Olympiads,
PhysicsMinions delivers three major breakthroughs: (i) Strong generalization:
it consistently improves both open-source and closed-source models of different
sizes, delivering clear benefits over their single-model baselines; (ii)
Historic breakthroughs: it elevates open-source models from only 1-2 to 6 gold
medals across 7 Olympiads, achieving the first-ever open-source gold medal in
the latest International Physics Olympiad (IPhO) under the average-score
metric; and (iii) Scaling to human expert: it further advances the open-source
Pass@32 score to 26.8/30 points on the latest IPhO, ranking 4th of 406
contestants and far surpassing the top single-model score of 22.7 (ranked
22nd). Generally, PhysicsMinions offers a generalizable framework for
Olympiad-level problem solving, with the potential to extend across
disciplines.

</details>


### [116] [The Emergence of Social Science of Large Language Models](https://arxiv.org/abs/2509.24877)
*Xiao Jia,Zhanzhan Zhao*

Main category: cs.AI

TL;DR: 对270项研究进行系统综述，构建计算分类法，划分三个领域，为人工智能社会科学提供指引。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型如何引发心智归因、相互交互以及改变人类活动和制度，梳理碎片化领域。

Method: 对270项研究进行系统综述，结合文本嵌入、无监督聚类和主题建模构建计算分类法。

Result: 划分出‘LLM as Social Minds’‘LLM Societies’‘LLM - Human Interactions’三个领域。

Conclusion: 该分类法为碎片化领域提供可重现地图，明确分析层面的证据标准，指出累积进展机会。

Abstract: The social science of large language models (LLMs) examines how these systems
evoke mind attributions, interact with one another, and transform human
activity and institutions. We conducted a systematic review of 270 studies,
combining text embeddings, unsupervised clustering and topic modeling to build
a computational taxonomy. Three domains emerge organically across the reviewed
literature. LLM as Social Minds examines whether and when models display
behaviors that elicit attributions of cognition, morality and bias, while
addressing challenges such as test leakage and surface cues. LLM Societies
examines multi-agent settings where interaction protocols, architectures and
mechanism design shape coordination, norms, institutions and collective
epistemic processes. LLM-Human Interactions examines how LLMs reshape tasks,
learning, trust, work and governance, and how risks arise at the human-AI
interface. This taxonomy provides a reproducible map of a fragmented field,
clarifies evidentiary standards across levels of analysis, and highlights
opportunities for cumulative progress in the social science of artificial
intelligence.

</details>


### [117] [RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark](https://arxiv.org/abs/2509.24897)
*Yang Shi,Yuhao Dong,Yue Ding,Yuran Wang,Xuanyu Zhu,Sheng Zhou,Wenting Liu,Haochen Tian,Rundong Wang,Huanqian Wang,Zuyan Liu,Bohan Zeng,Ruizhe Chen,Qixun Wang,Zhuoran Zhang,Xinlong Chen,Chengzhuo Tong,Bozhou Li,Chaoyou Fu,Qiang Liu,Haotian Wang,Wenjing Yang,Yuanxing Zhang,Pengfei Wan,Yi-Fan Zhang,Ziwei Liu*

Main category: cs.AI

TL;DR: 现有基准未解决统一多模态模型中视觉理解和生成能力是否协同的问题，本文提出RealUnify基准评估双向能力协同，评估发现当前统一模型难有效协同，需新策略。


<details>
  <summary>Details</summary>
Motivation: 现有基准和评估范式无法确定统一模型中理解和生成能力是否能协同，需新评估方法。

Method: 提出RealUnify基准，含1000个人工标注实例、10个类别和32个子任务，采用结合端到端评估和逐步诊断评估的双评估协议。

Result: 对12个领先统一模型和6个专门基线大规模评估发现，当前统一模型难以实现有效协同。

Conclusion: 架构统一不足以实现能力协同，需要新训练策略和归纳偏置来释放统一建模潜力。

Abstract: The integration of visual understanding and generation into unified
multimodal models represents a significant stride toward general-purpose AI.
However, a fundamental question remains unanswered by existing benchmarks: does
this architectural unification actually enable synergetic interaction between
the constituent capabilities? Existing evaluation paradigms, which primarily
assess understanding and generation in isolation, are insufficient for
determining whether a unified model can leverage its understanding to enhance
its generation, or use generative simulation to facilitate deeper
comprehension. To address this critical gap, we introduce RealUnify, a
benchmark specifically designed to evaluate bidirectional capability synergy.
RealUnify comprises 1,000 meticulously human-annotated instances spanning 10
categories and 32 subtasks. It is structured around two core axes: 1)
Understanding Enhances Generation, which requires reasoning (e.g., commonsense,
logic) to guide image generation, and 2) Generation Enhances Understanding,
which necessitates mental simulation or reconstruction (e.g., of transformed or
disordered visual inputs) to solve reasoning tasks. A key contribution is our
dual-evaluation protocol, which combines direct end-to-end assessment with a
diagnostic stepwise evaluation that decomposes tasks into distinct
understanding and generation phases. This protocol allows us to precisely
discern whether performance bottlenecks stem from deficiencies in core
abilities or from a failure to integrate them. Through large-scale evaluations
of 12 leading unified models and 6 specialized baselines, we find that current
unified models still struggle to achieve effective synergy, indicating that
architectural unification alone is insufficient. These results highlight the
need for new training strategies and inductive biases to fully unlock the
potential of unified modeling.

</details>


### [118] [Neural network embeddings recover value dimensions from psychometric survey items on par with human data](https://arxiv.org/abs/2509.24906)
*Max Pellert,Clemens M. Lechner,Indira Sen,Markus Strohmaier*

Main category: cs.AI

TL;DR: 本文提出SQuID方法，利用神经网络嵌入从心理测量调查项目中恢复潜在维度，实验表明该方法能有效复制心理测量结构，有成本、扩展性和灵活性优势。


<details>
  <summary>Details</summary>
Motivation: 寻找一种能有效从心理测量调查项目中恢复潜在维度的方法，解决以往方法在获取维度间负相关时需特定领域微调的问题。

Method: 提出SQuID方法，处理大语言模型的嵌入，对比多个嵌入模型并通过多种评估指标进行实验验证。

Result: 基于嵌入的方法能解释55%的维度 - 维度相似性方差，两种数据的多维尺度配置有较好的因子一致性系数且符合理论。

Conclusion: 语义嵌入能有效复制心理测量结构，该方法在保持与传统方法质量相当的同时，具有成本、扩展性和灵活性优势，对心理测量和社会科学研究有重要意义。

Abstract: This study introduces "Survey and Questionnaire Item Embeddings
Differentials" (SQuID), a novel methodological approach that enables neural
network embeddings to effectively recover latent dimensions from psychometric
survey items. We demonstrate that embeddings derived from large language
models, when processed with SQuID, can recover the structure of human values
obtained from human rater judgments on the Revised Portrait Value Questionnaire
(PVQ-RR). Our experimental validation compares multiple embedding models across
a number of evaluation metrics. Unlike previous approaches, SQuID successfully
addresses the challenge of obtaining negative correlations between dimensions
without requiring domain-specific fine-tuning. Quantitative analysis reveals
that our embedding-based approach explains 55% of variance in
dimension-dimension similarities compared to human data. Multidimensional
scaling configurations from both types of data show fair factor congruence
coefficients and largely follow the underlying theory. These results
demonstrate that semantic embeddings can effectively replicate psychometric
structures previously established through extensive human surveys. The approach
offers substantial advantages in cost, scalability and flexibility while
maintaining comparable quality to traditional methods. Our findings have
significant implications for psychometrics and social science research,
providing a complementary methodology that could expand the scope of human
behavior and experience represented in measurement tools.

</details>


### [119] [Meta-Learning Theory-Informed Inductive Biases using Deep Kernel Gaussian Processes](https://arxiv.org/abs/2509.24919)
*Bahti Zakirov,Gašper Tkačik*

Main category: cs.AI

TL;DR: 提出贝叶斯元学习框架将规范理论预测转化为概率模型，应用于早期视觉系统，提升预测精度等，提供整合理论知识新方法。


<details>
  <summary>Details</summary>
Motivation: 规范和任务驱动理论虽能自上而下解释生物系统，但定量仲裁竞争理论及利用其改进数据驱动拟合工作艰巨且常不可行。

Method: 引入贝叶斯元学习框架，采用自适应深度核高斯过程，在规范理论生成的合成数据上进行元学习得到理论知情核，构建概率模型。

Result: 应用于早期视觉系统，相比传统数据驱动基线，提升小鼠视网膜神经节细胞反应预测精度，提供校准的不确定性估计和可解释表示，能准确推断理论匹配程度。

Conclusion: 该工作为神经科学及其他领域将理论知识整合到数据驱动科学探究中提供更通用、可扩展和自动化的方法。

Abstract: Normative and task-driven theories offer powerful top-down explanations for
biological systems, yet the goals of quantitatively arbitrating between
competing theories, and utilizing them as inductive biases to improve
data-driven fits of real biological datasets are prohibitively laborious, and
often impossible. To this end, we introduce a Bayesian meta-learning framework
designed to automatically convert raw functional predictions from normative
theories into tractable probabilistic models. We employ adaptive deep kernel
Gaussian processes, meta-learning a kernel on synthetic data generated from a
normative theory. This Theory-Informed Kernel specifies a probabilistic model
representing the theory predictions -- usable for both fitting data and
rigorously validating the theory. As a demonstration, we apply our framework to
the early visual system, using efficient coding as our normative theory. We
show improved response prediction accuracy in ex vivo recordings of mouse
retinal ganglion cells stimulated by natural scenes compared to conventional
data-driven baselines, while providing well-calibrated uncertainty estimates
and interpretable representations. Using exact Bayesian model selection, we
also show that our informed kernel can accurately infer the degree of
theory-match from data, confirming faithful encapsulation of theory structure.
This work provides a more general, scalable, and automated approach for
integrating theoretical knowledge into data-driven scientific inquiry in
neuroscience and beyond.

</details>


### [120] [MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning](https://arxiv.org/abs/2509.24922)
*Huihao Jing,Wenbin Hu,Hongyu Luo,Jianhui Yang,Wei Fan,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: 本文提出针对多智能体系统（MAS）的法律基准MASLegalBench，用GDPR作为场景进行实验，分析现有模型和架构的优缺点。


<details>
  <summary>Details</summary>
Motivation: 此前研究缺乏考虑MAS独特优势的法律基准，评估方法缺失限制了MAS在法律领域的潜力。

Method: 提出用演绎推理方法设计的MASLegalBench，以GDPR为场景，手动设计多种基于角色的MAS并使用不同的先进大语言模型进行实验。

Result: 实验结果凸显了现有模型和MAS架构的优势、局限以及潜在改进领域。

Conclusion: 提出的MASLegalBench能有效评估MAS在法律领域的表现，为相关研究提供参考。

Abstract: Multi-agent systems (MAS), leveraging the remarkable capabilities of Large
Language Models (LLMs), show great potential in addressing complex tasks. In
this context, integrating MAS with legal tasks is a crucial step. While
previous studies have developed legal benchmarks for LLM agents, none are
specifically designed to consider the unique advantages of MAS, such as task
decomposition, agent specialization, and flexible training. In fact, the lack
of evaluation methods limits the potential of MAS in the legal domain. To
address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS
and designed with a deductive reasoning approach. Our benchmark uses GDPR as
the application scenario, encompassing extensive background knowledge and
covering complex reasoning processes that effectively reflect the intricacies
of real-world legal situations. Furthermore, we manually design various
role-based MAS and conduct extensive experiments using different
state-of-the-art LLMs. Our results highlight the strengths, limitations, and
potential areas for improvement of existing models and MAS architectures.

</details>


### [121] [KIRETT -- A wearable device to support rescue operations using artificial intelligence to improve first aid](https://arxiv.org/abs/2509.24934)
*Johannes Zenkert,Christian Weber,Mubaris Nadeem,Lisa Bender,Madjid Fathi,Abu Shad Ahammed,Aniebiet Micheal Ezekiel,Roman Obermaisser,Maximilian Bradford*

Main category: cs.AI

TL;DR: 本文介绍KIRETT项目科学部分的初步进展，该项目用可穿戴设备改进救援急救。


<details>
  <summary>Details</summary>
Motivation: 利用可穿戴设备改进救援行动中的急救，减少因治疗不当对患者造成的伤害，提高生存率。

Method: 使用可穿戴设备通过人工智能进行计算机辅助情况识别。

Result: 无明确提及具体结果。

Conclusion: 论文给出了项目内研究方法的首次概述。

Abstract: This short paper presents first steps in the scientific part of the KIRETT
project, which aims to improve first aid during rescue operations using a
wearable device. The wearable is used for computer-aided situation recognition
by means of artificial intelligence. It provides contextual recommendations for
actions and operations to rescue personnel and is intended to minimize damage
to patients due to incorrect treatment, as well as increase the probability of
survival. The paper describes a first overview of research approaches within
the project.

</details>


### [122] [Agentic Exploration of Physics Models](https://arxiv.org/abs/2509.24978)
*Maximilian Nägele,Florian Marquardt*

Main category: cs.AI

TL;DR: 介绍SciExplorer智能体，利用大语言模型工具使用能力探索未知系统，在多物理系统测试中表现出色，为其他领域科学探索提供可能。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习难以完全自动化探索未知系统规律，需一种无需针对特定任务定制的方法。

Method: 引入SciExplorer智能体，利用大语言模型工具使用能力，主要基于代码执行进行系统探索。

Result: 在机械动力学、波演化和量子多体物理等模型测试中，用最少工具取得显著性能，如从观测动力学恢复运动方程、从期望值推断哈密顿量。

Conclusion: 该方法无需微调或特定任务指令，为其他领域科学探索开辟道路。

Abstract: The process of scientific discovery relies on an interplay of observations,
analysis, and hypothesis generation. Machine learning is increasingly being
adopted to address individual aspects of this process. However, it remains an
open challenge to fully automate the open-ended, heuristic, iterative loop
required to discover the laws of an unknown system by exploring it through
experiments and analysis, without tailoring the approach to the specifics of a
given task. Here, we introduce SciExplorer, an agent that leverages large
language model tool-use capabilities to enable free-form exploration of systems
without any domain-specific blueprints, and apply it to the exploration of
physical systems that are initially unknown to the agent. We test SciExplorer
on a broad set of models spanning mechanical dynamical systems, wave evolution,
and quantum many-body physics. Despite using a minimal set of tools, primarily
based on code execution, we observe impressive performance on tasks such as
recovering equations of motion from observed dynamics and inferring
Hamiltonians from expectation values. The demonstrated effectiveness of this
setup opens the door towards similar scientific exploration in other domains,
without the need for finetuning or task-specific instructions.

</details>


### [123] [CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning](https://arxiv.org/abs/2509.25004)
*Shijie Zhang,Guohao Sun,Kevin Zhang,Xiang Guo,Rujun Guo*

Main category: cs.AI

TL;DR: 提出CLPO算法解决在线RLVR中训练样本处理问题，实验显示在多基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有在线RLVR方法统一处理训练样本，忽视问题难度差异，导致学习效率和性能上限受限。

Method: 提出CLPO算法，在策略优化过程中创建动态教学反馈循环，利用模型自身表现实时评估难度构建在线课程，引导自适应问题重构机制。

Result: CLPO在八个具有挑战性的数学和通用推理基准测试中达到了最先进的性能，平均pass@1比其他方法提高了6.96%。

Conclusion: CLPO算法有潜力更有效地训练出更具推理能力的模型。

Abstract: Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has
become a key paradigm for enhancing the reasoning capabilities of Large
Language Models (LLMs). However, existing methods typically treat all training
samples uniformly, overlooking the vast differences in problem difficulty
relative to the model's current capabilities. This uniform training strategy
leads to inefficient exploration of problems the model has already mastered,
while concurrently lacking effective guidance on problems that are challenging
its abilities the most, limiting both learning efficiency and upper-bound
performance. To address this, we propose CLPO (Curriculum-guided Learning for
Policy Optimization), a novel algorithm that creates a dynamic pedagogical
feedback loop within the policy optimization process. The core of CLPO
leverages the model's own rollout performance to conduct real-time difficulty
assessment, thereby constructing an Online Curriculum. This curriculum then
guides an Adaptive Problem Restructuring mechanism, where the model acts as its
own teacher: it diversifies medium-difficulty problems to promote
generalization and simplifies challenging problems to make them more
attainable. Our approach transforms the static training procedure into a
dynamic process that co-evolves with the model's capabilities. Experiments show
that CLPO achieves state-of-the-art performance across eight challenging
mathematical and general reasoning benchmarks, with an average pass@1
improvement of 6.96% over other methods, demonstrating its potential for more
efficiently training more capable reasoning models.

</details>


### [124] [Scaling Synthetic Task Generation for Agents via Exploration](https://arxiv.org/abs/2509.25047)
*Ram Ramrakhya,Andrew Szot,Omar Attia,Yuhao Yang,Anh Nguyen,Bogdan Mazoure,Zhe Gan,Harsh Agrawal,Alexander Toshev*

Main category: cs.AI

TL;DR: 提出AutoPlay可扩展任务生成管道，用于多模态大语言模型后训练，减少对人工标注依赖，提升UI代理成功率。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型后训练中缺乏高质量、多样化、可验证下游代理任务数据集的问题，现有方法成本高或可扩展性差。

Method: AutoPlay分为探索阶段（MLLM探索代理发现新环境状态和功能）和任务生成阶段（任务生成器利用探索轨迹和任务指南提示合成任务）。

Result: AutoPlay为安卓和Ubuntu应用生成大量任务，可进行大规模任务演示合成，训练的UI代理在移动和计算机使用场景中成功率提升，还能促进强化学习训练。

Conclusion: AutoPlay是一种可扩展的后训练方法，能减少对人工标注的依赖。

Abstract: Post-Training Multimodal Large Language Models (MLLMs) to build interactive
agents holds promise across domains such as computer-use, web navigation, and
robotics. A key challenge in scaling such post-training is lack of high-quality
downstream agentic task datasets with tasks that are diverse, feasible, and
verifiable. Existing approaches for task generation rely heavily on human
annotation or prompting MLLM with limited downstream environment information,
which is either costly or poorly scalable as it yield tasks with limited
coverage. To remedy this, we present AutoPlay, a scalable pipeline for task
generation that explicitly explores interactive environments to discover
possible interactions and current state information to synthesize
environment-grounded tasks. AutoPlay operates in two stages: (i) an exploration
phase, where an MLLM explorer agent systematically uncovers novel environment
states and functionalities, and (ii) a task generation phase, where a task
generator leverages exploration trajectories and a set of task guideline
prompts as context to synthesize diverse, executable, and verifiable tasks. We
show AutoPlay generates 20k tasks across 20 Android applications and 10k tasks
across 13 applications Ubuntu applications to train mobile-use and computer-use
agents. AutoPlay generated tasks enable large-scale task demonstration
synthesis without human annotation by employing an MLLM task executor and
verifier. This data enables training MLLM-based UI agents that improve success
rates up to $20.0\%$ on mobile-use and $10.9\%$ on computer-use scenarios. In
addition, AutoPlay generated tasks combined with MLLM verifier-based rewards
enable scaling reinforcement learning training of UI agents, leading to an
additional $5.7\%$ gain. coverage. These results establish AutoPlay as a
scalable approach for post-training capable MLLM agents reducing reliance on
human annotation.

</details>


### [125] [Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning](https://arxiv.org/abs/2509.25052)
*Sai Wang,Yu Wu,Zhongwen Xu*

Main category: cs.AI

TL;DR: 提出新范式CEL，利用LLM构建环境理解和策略，在多种网格世界任务中成功学习掌握游戏，迭代过程对持续学习至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法依赖大量经验且知识编码不透明，追求能通过推理和规划学习掌握复杂环境的智能体。

Method: 引入CEL智能体架构，基于LLM，从无先验知识状态开始，通过交互和反思循环，进行规则归纳和策略总结两个并行学习过程。

Result: CEL智能体在多种网格世界任务中成功自主发现规则并从稀疏奖励中制定有效策略，消融实验证明迭代过程对持续学习很关键。

Conclusion: 展示了构建更通用、可解释智能体的路径，能通过对原始经验的显式推理构建透明且不断改进的世界模型。

Abstract: The pursuit of artificial agents that can learn to master complex
environments has led to remarkable successes, yet prevailing deep reinforcement
learning methods often rely on immense experience, encoding their knowledge
opaquely within neural network weights. We propose a different paradigm, one in
which an agent learns to play by reasoning and planning. We introduce Cogito,
ergo ludo (CEL), a novel agent architecture that leverages a Large Language
Model (LLM) to build an explicit, language-based understanding of its
environment's mechanics and its own strategy. Starting from a tabula rasa state
with no prior knowledge (except action set), CEL operates on a cycle of
interaction and reflection. After each episode, the agent analyzes its complete
trajectory to perform two concurrent learning processes: Rule Induction, where
it refines its explicit model of the environment's dynamics, and Strategy and
Playbook Summarization, where it distills experiences into an actionable
strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,
Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent
successfully learns to master these games by autonomously discovering their
rules and developing effective policies from sparse rewards. Ablation studies
confirm that the iterative process is critical for sustained learning. Our work
demonstrates a path toward more general and interpretable agents that not only
act effectively but also build a transparent and improving model of their world
through explicit reasoning on raw experience.

</details>


### [126] [HeDA: An Intelligent Agent System for Heatwave Risk Discovery through Automated Knowledge Graph Construction and Multi-layer Risk Propagation Analysis](https://arxiv.org/abs/2509.25112)
*Yiquan Wang,Tin-Yeh Huang,Qingyun Gao,Jialin Zhang*

Main category: cs.AI

TL;DR: 本文介绍智能多智能体系统HeDA，通过构建知识图谱和多层风险传播分析发现热浪风险路径，准确率高且发现新风险链，为气候适应策略提供见解。


<details>
  <summary>Details</summary>
Motivation: 科学文献知识碎片化阻碍对热浪跨系统级联风险路径的全面理解。

Method: 引入HeDA系统，处理超10247篇学术论文构建知识图谱，进行多层风险传播分析。

Result: 系统在复杂问答任务准确率达78.9%，超GPT - 4等基线13.7%，发现5条未识别的高影响风险链。

Conclusion: 提出AI驱动科学发现新范式，为气候适应策略提供可行见解。

Abstract: Heatwaves pose complex cascading risks across interconnected climate, social,
and economic systems, but knowledge fragmentation in scientific literature
hinders comprehensive understanding of these risk pathways. We introduce HeDA
(Heatwave Discovery Agent), an intelligent multi-agent system designed for
automated scientific discovery through knowledge graph construction and
multi-layer risk propagation analysis. HeDA processes over 10,247 academic
papers to construct a comprehensive knowledge graph with 23,156 nodes and
89,472 relationships, employing novel multi-layer risk propagation analysis to
systematically identify overlooked risk transmission pathways. Our system
achieves 78.9% accuracy on complex question-answering tasks, outperforming
state-of-the-art baselines including GPT-4 by 13.7%. Critically, HeDA
successfully discovered five previously unidentified high-impact risk chains,
such as the pathway where a heatwave leads to a water demand surge, resulting
in industrial water restrictions and ultimately causing small business
disruption, which were validated through historical case studies and domain
expert review. This work presents a new paradigm for AI-driven scientific
discovery, providing actionable insights for developing more resilient climate
adaptation strategies.

</details>


### [127] [From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones](https://arxiv.org/abs/2509.25123)
*Lifan Yuan,Weize Chen,Yuchen Zhang,Ganqu Cui,Hanbin Wang,Ziming You,Ning Ding,Zhiyuan Liu,Maosong Sun,Hao Peng*

Main category: cs.AI

TL;DR: 本文探讨RL能否让大语言模型获得新技能，通过合成框架实验证明LLM可在RL中通过组合现有技能获得新技能，还发现技能迁移等现象，为LLM学习提供新见解。


<details>
  <summary>Details</summary>
Motivation: 解决关于强化学习（RL）在大语言模型（LLM）训练中作用的争议，即RL是让LLM获得新技能还是仅激活现有技能。

Method: 开发合成框架，定义技能为推断字符串转换函数输出的能力，进行实验观察LLM在RL中学习函数组合及技能迁移情况，并与下一个标记训练对比。

Result: LLM能在RL中通过组合现有技能学习未见的函数组合，这种能力可泛化到更难问题，且源任务获得的组合技能能迁移到目标任务，RL会改变模型推理行为，下一个标记训练无此效果。

Conclusion: 先构建具备基本技能的基础模型，再用RL激励解决复杂问题的高级、可泛化技能是有价值的。

Abstract: Does RL teach LLMs genuinely new skills, or does it merely activate existing
ones? This question lies at the core of ongoing debates about the role of RL in
LLM post-training. On one side, strong empirical results can be achieved with
RL even without preceding supervised finetuning; on the other, critics argue
that RL contributes little beyond reweighting existing reasoning strategies.
This work provides concrete evidence that LLMs can acquire genuinely new skills
during RL by composing existing ones, mirroring one of the central mechanisms
by which humans acquire new cognitive skills. To mitigate data contamination
and other confounding factors, and to allow precise control over task
complexity, we develop a synthetic framework for our investigation.
Specifically, we define a skill as the ability to infer the output of a string
transformation function f(x) given x. When an LLM has already learned f and g
prior to RL, our experiments reveal that RL enables it to learn unseen
compositions of them h(x)=g(f(x)). Further, this compositional ability
generalizes to more difficult problems such as compositions of >2 functions
unseen during RL training. Surprisingly, our experiments show that
compositional skill acquired on a source task transfers to a different target
task. This transfer happens even without compositional training on the target,
requiring only prior knowledge of the target's atomic skills. Our qualitative
analysis shows that RL fundamentally changes the reasoning behaviors of the
models. In contrast, next-token training with the same data yields none of
these findings. Our systematic experiments provide fresh insights into LLM
learning, suggesting the value of first building base models with basic skills,
then using RL to incentivize advanced, generalizable skills for complex
problems.

</details>


### [128] [The Era of Real-World Human Interaction: RL from User Conversations](https://arxiv.org/abs/2509.25137)
*Chuanyang Jin,Jing Xu,Bo Liu,Leitian Tao,Olga Golovneva,Tianmin Shu,Wenting Zhao,Xian Li,Jason Weston*

Main category: cs.AI

TL;DR: 提出从自然人类交互中学习的强化学习范式RLHI，含两种方法，表现优于基线，表明人类交互可用于个性化对齐监督。


<details>
  <summary>Details</summary>
Motivation: 为实现模型持续改进和多方面对齐，当前对话模型基于预标注专家反馈对齐，需从自然人类交互学习。

Method: 引入Reinforcement Learning from Human Interaction (RLHI)范式，开发RLHI with User - Guided Rewrites和RLHI with User - Based Rewards两种方法，通过角色条件偏好优化连接长期用户角色和逐轮偏好。

Result: 在WildChat对话上训练，两种RLHI变体在个性化和指令遵循方面优于强基线，类似反馈提升推理基准性能。

Conclusion: 有机人类交互为个性化对齐提供可扩展、有效的监督。

Abstract: We posit that to achieve continual model improvement and multifaceted
alignment, future models must learn from natural human interaction. Current
conversational models are aligned using pre-annotated, expert-generated human
feedback. In this work, we introduce Reinforcement Learning from Human
Interaction (RLHI), a paradigm that learns directly from in-the-wild user
conversations. We develop two complementary methods: (1) RLHI with User-Guided
Rewrites, which revises unsatisfactory model outputs based on users'
natural-language follow-up responses, (2) RLHI with User-Based Rewards, which
learns via a reward model conditioned on knowledge of the user's long-term
interaction history (termed persona). Together, these methods link long-term
user personas to turn-level preferences via persona-conditioned preference
optimization. Trained on conversations derived from WildChat, both RLHI
variants outperform strong baselines in personalization and
instruction-following, and similar feedback enhances performance on reasoning
benchmarks. These results suggest organic human interaction offers scalable,
effective supervision for personalized alignment.

</details>


### [129] [Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs](https://arxiv.org/abs/2509.25139)
*Yue Zhang,Tianyi Ma,Zun Wang,Yanyuan Qiao,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: 本文提出通过多视角文本描述改进基于LLM的视觉语言导航代理的上下文理解，在R2R数据集上提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于零样本大语言模型的视觉语言导航代理存在对视觉细节处理不足或无法捕获抽象语义的问题。

Method: 通过纳入多视角文本描述促进图像间的类比推理，提升导航代理的上下文理解。

Result: 在R2R数据集上的实验表明导航性能有显著提升。

Conclusion: 基于文本的类比推理可增强代理的全局场景理解和空间推理能力，实现更准确的行动决策。

Abstract: Integrating large language models (LLMs) into embodied AI models is becoming
increasingly prevalent. However, existing zero-shot LLM-based
Vision-and-Language Navigation (VLN) agents either encode images as textual
scene descriptions, potentially oversimplifying visual details, or process raw
image inputs, which can fail to capture abstract semantics required for
high-level reasoning. In this paper, we improve the navigation agent's
contextual understanding by incorporating textual descriptions from multiple
perspectives that facilitate analogical reasoning across images. By leveraging
text-based analogical reasoning, the agent enhances its global scene
understanding and spatial reasoning, leading to more accurate action decisions.
We evaluate our approach on the R2R dataset, where our experiments demonstrate
significant improvements in navigation performance.

</details>


### [130] [ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory](https://arxiv.org/abs/2509.25140)
*Siru Ouyang,Jun Yan,I-Hung Hsu,Yanfei Chen,Ke Jiang,Zifeng Wang,Rujun Han,Long T. Le,Samira Daruki,Xiangru Tang,Vishy Tirumalashetty,George Lee,Mahsan Rofouei,Hangfei Lin,Jiawei Han,Chen-Yu Lee,Tomas Pfister*

Main category: cs.AI

TL;DR: 提出ReasoningBank记忆框架和MaTTS方法，使大语言模型代理能从交互历史学习，在多基准测试中表现出色，确立记忆驱动的经验扩展为新维度。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理无法从积累的交互历史学习，会丢弃有价值见解并重复错误。

Method: 提出ReasoningBank框架，从代理的成功和失败经验中提炼通用推理策略；引入MaTTS，通过扩大代理的交互经验加速和多样化学习过程。

Result: 在网页浏览和软件工程基准测试中，ReasoningBank优于现有存储原始轨迹或仅成功任务例程的记忆机制，MaTTS进一步提升效果。

Conclusion: 记忆驱动的经验扩展可作为新的扩展维度，使代理能自然地自我进化并产生涌现行为。

Abstract: With the growing adoption of large language model agents in persistent
real-world roles, they naturally encounter continuous streams of tasks. A key
limitation, however, is their failure to learn from the accumulated interaction
history, forcing them to discard valuable insights and repeat past errors. We
propose ReasoningBank, a novel memory framework that distills generalizable
reasoning strategies from an agent's self-judged successful and failed
experiences. At test time, an agent retrieves relevant memories from
ReasoningBank to inform its interaction and then integrates new learnings back,
enabling it to become more capable over time. Building on this powerful
experience learner, we further introduce memory-aware test-time scaling
(MaTTS), which accelerates and diversifies this learning process by scaling up
the agent's interaction experience. By allocating more compute to each task,
the agent generates abundant, diverse experiences that provide rich contrastive
signals for synthesizing higher-quality memory. The better memory in turn
guides more effective scaling, establishing a powerful synergy between memory
and test-time scaling. Across web browsing and software engineering benchmarks,
ReasoningBank consistently outperforms existing memory mechanisms that store
raw trajectories or only successful task routines, improving both effectiveness
and efficiency; MaTTS further amplifies these gains. These findings establish
memory-driven experience scaling as a new scaling dimension, enabling agents to
self-evolve with emergent behaviors naturally arise.

</details>


### [131] [Visual serial processing deficits explain divergences in human and VLM reasoning](https://arxiv.org/abs/2509.25142)
*Nicholas Budny,Kia Ghods,Declan Campbell,Raja Marjieh,Amogh Joshi,Sreejan Kumar,Jonathan D. Cohen,Taylor W. Webb,Thomas L. Griffiths*

Main category: cs.AI

TL;DR: 研究发现视觉语言模型在串行视觉推理任务上与人类表现存在差距，串行视觉推理限制是其与人类的根本区别。


<details>
  <summary>Details</summary>
Motivation: 探究视觉语言模型在简单视觉推理任务上不如人类的原因，假设关键因素是视觉基础串行处理不足。

Method: 比较人类和视觉语言模型在几何推理、感知枚举和心理旋转三个领域不同串行处理需求任务上的表现。

Result: 所有领域中，视觉语言模型准确率下降与人类反应时间增加（代表串行处理负荷）强相关，任务串行处理要求越高，视觉语言模型与人类表现差距越大。

Conclusion: 串行、视觉基础推理的局限性是区分当前视觉语言模型和人类的根本瓶颈。

Abstract: Why do Vision Language Models (VLMs), despite success on standard benchmarks,
often fail to match human performance on surprisingly simple visual reasoning
tasks? While the underlying computational principles are still debated, we
hypothesize that a crucial factor is a deficit in visually-grounded serial
processing. To test this hypothesis, we compared human and VLM performance
across tasks designed to vary serial processing demands in three distinct
domains: geometric reasoning, perceptual enumeration, and mental rotation.
Tasks within each domain varied serial processing load by manipulating factors
such as geometric concept complexity, perceptual individuation load, and
transformation difficulty. Across all domains, our results revealed a
consistent pattern: decreased VLM accuracy was strongly correlated with
increased human reaction time (used as a proxy for serial processing load). As
tasks require more demanding serial processing -- whether composing concepts,
enumerating items, or performing mental transformations -- the VLM-human
performance gap widens reliably. These findings support our hypothesis,
indicating that limitations in serial, visually grounded reasoning represent a
fundamental bottleneck that distinguishes current VLMs from humans.

</details>


### [132] [UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following](https://arxiv.org/abs/2509.25148)
*FaQiang Qian,WeiKun Zhang,Ziliang Wang,Kang An,Xuhui Zheng,Liangjian Wen,Mengya Gao,Yong Dai,Yichao Wu*

Main category: cs.AI

TL;DR: 文章指出预训练后对齐本质是统一偏好学习问题，标准顺序管道有缺陷，提出UniAPL框架解决分布不匹配问题，实验表明该框架效果良好。


<details>
  <summary>Details</summary>
Motivation: 解决标准顺序管道（SFT后接RL）中存在的分布不匹配问题，实现数据间的相互正则化。

Method: 将对齐问题重构为约束优化问题，提出UniAPL框架，采用单阶段统一训练目标，从SFT和偏好数据的混合批次中共同学习。

Result: 在指令跟随任务上，模型表现匹配或超过强GRPO基线，在Qwen3 - 0.6B上提升5.77%，在Qwen3 - 4B上提升3.75%，甚至超越了教师模型。

Conclusion: UniAPL框架能有效解决分布不匹配问题，实现更强性能和更好的行为对齐。

Abstract: Shaping powerful LLMs to be beneficial and safe is central to AI alignment.
We argue that post-training alignment is fundamentally a unified Preference
Learning problem, involving two modalities: demonstrated preferences (e.g.,
Supervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement
Learning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due
to a critical distributional mismatch: SFT uses static expert data, but as the
policy evolves, its generation distribution drifts, making SFT knowledge
brittle. Subsequent RL then explores without direct access to the rich,
ground-truth knowledge in expert demonstrations, leading to inefficient,
ungrounded updates. This separation prevents mutual regularization between data
sources. To address this, we reframe alignment as a constrained optimization
problem and propose Unified Adversarial Preference Learning (UniAPL),a novel
framework that dynamically aligns the policy's distribution with the expert's.
UniAPL implements a single-stage unified training objective, jointly learning
from mixed batches of SFT and preference data. In every gradient step, dense
expert demonstrations directly ground and regularize online exploration,
inherently resolving distributional mismatch and maximizing data synergy.We
evaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507
as the teacher. Our models match or exceed strong GRPO baselines: +5.77% on
Qwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the
teacher. Analyses of response length and log-probability distributions confirm
that UniAPL outputs closely mimic expert demonstrations, achieving both
stronger performance and better behavioral alignment.

</details>


### [133] [Who's Your Judge? On the Detectability of LLM-Generated Judgments](https://arxiv.org/abs/2509.25154)
*Dawei Li,Zhen Tan,Chengshuai Zhao,Bohan Jiang,Baixiang Huang,Pingchuan Ma,Abdullah Alnaibari,Kai Shu,Huan Liu*

Main category: cs.AI

TL;DR: 本文提出判断检测任务并研究大语言模型生成判断的可检测性，引入J - Detector进行检测，实验证明其有效性并分析影响因素和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成判断存在固有偏差和漏洞，在敏感场景如学术同行评审中需要对其进行区分。

Method: 提出判断检测任务，分析现有检测方法不足，引入J - Detector，利用显式提取的语言和大语言模型增强特征将大语言模型判断偏差与候选属性关联。

Result: 在不同数据集上的实验证明了J - Detector的有效性，其可解释性可量化大语言模型判断中的偏差。

Conclusion: 分析了影响大语言模型生成判断可检测性的关键因素，验证了判断检测在现实场景中的实际效用。

Abstract: Large Language Model (LLM)-based judgments leverage powerful LLMs to
efficiently evaluate candidate content and provide judgment scores. However,
the inherent biases and vulnerabilities of LLM-generated judgments raise
concerns, underscoring the urgent need for distinguishing them in sensitive
scenarios like academic peer reviewing. In this work, we propose and formalize
the task of judgment detection and systematically investigate the detectability
of LLM-generated judgments. Unlike LLM-generated text detection, judgment
detection relies solely on judgment scores and candidates, reflecting
real-world scenarios where textual feedback is often unavailable in the
detection process. Our preliminary analysis shows that existing LLM-generated
text detection methods perform poorly given their incapability to capture the
interaction between judgment scores and candidate content -- an aspect crucial
for effective judgment detection. Inspired by this, we introduce
\textit{J-Detector}, a lightweight and transparent neural detector augmented
with explicitly extracted linguistic and LLM-enhanced features to link LLM
judges' biases with candidates' properties for accurate detection. Experiments
across diverse datasets demonstrate the effectiveness of \textit{J-Detector}
and show how its interpretability enables quantifying biases in LLM judges.
Finally, we analyze key factors affecting the detectability of LLM-generated
judgments and validate the practical utility of judgment detection in
real-world scenarios.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [134] [Channel, Trend and Periodic-Wise Representation Learning for Multivariate Long-term Time Series Forecasting](https://arxiv.org/abs/2509.23583)
*Zhangyao Song,Nanqing Jiang,Miaohong He,Xiaoyu Zhao,Tao Guo*

Main category: cs.CE

TL;DR: 提出CTPNet框架用于时间序列预测，从三方面学习表示，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于下采样的时间序列预测方法忽略子序列间和通道间交互，限制预测精度。

Method: 提出CTPNet框架，通过基于时间查询的多头注意力机制捕获通道间依赖，用Transformer建模子序列内依赖，利用带残差连接的编码器提取子序列间依赖。

Result: 广泛实验证明了所提方法的优越性。

Conclusion: CTPNet框架通过联合整合不同层次依赖，能提供更全面的时间动态表示。

Abstract: Downsampling-based methods for time series forecasting have attracted
increasing attention due to their superiority in capturing sequence trends.
However, this approaches mainly capture dependencies within subsequences but
neglect inter-subsequence and inter-channel interactions, which limits
forecasting accuracy. To address these limitations, we propose CTPNet, a novel
framework that explicitly learns representations from three perspectives: i)
inter-channel dependencies, captured by a temporal query-based multi-head
attention mechanism; ii) intra-subsequence dependencies, modeled via a
Transformer to characterize trend variations; and iii) inter-subsequence
dependencies, extracted by reusing the encoder with residual connections to
capture global periodic patterns. By jointly integrating these levels, proposed
method provides a more holistic representation of temporal dynamics. Extensive
experiments demonstrate the superiority of the proposed method.

</details>


### [135] [Text-to-Code Generation for Modular Building Layouts in Building Information Modeling](https://arxiv.org/abs/2509.23713)
*Yinyi Wei,Xiao Li*

Main category: cs.CE

TL;DR: 提出Text2MBL框架，能从文本描述生成可执行BIM代码，建立从概念设计到模块化施工的流程，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决传统布局生成方法在处理模块化建筑布局设计时的不足，应对MBL三层结构带来的挑战。

Method: 开发面向对象代码架构，微调大语言模型输出代码格式的结构化动作序列，整理配对描述和真实布局数据集进行训练和评估。

Result: 通过指标评估性能，建立从概念设计到模块化施工的可扩展流程。

Conclusion: Text2MBL框架将自然语言理解和BIM代码生成紧密结合，实现了从高层概念设计到自动化模块化施工工作流的转换。

Abstract: We present Text2MBL, a text-to-code generation framework that generates
executable Building Information Modeling (BIM) code directly from textual
descriptions of modular building layout (MBL) design. Unlike conventional
layout generation approaches that operate in 2D space, Text2MBL produces fully
parametric, semantically rich BIM layouts through on-the-fly code
instantiation. To address MBLs' unique challenges due to their hierarchical
three-tier structure: modules (physical building blocks), units (self-contained
dwellings), and rooms (functional spaces), we developed an object-oriented code
architecture and fine-tuned large language models to output structured action
sequences in code format. To train and evaluate the framework, we curated a
dataset of paired descriptions and ground truth layouts drawn from real-world
modular housing projects. Performance was assessed using metrics for executable
validity, semantic fidelity, and geometric consistency. By tightly unifying
natural language understanding with BIM code generation, Text2MBL establishes a
scalable pipeline from high-level conceptual design to automation-ready modular
construction workflows. Our implementation is available at
https://github.com/CI3LAB/Text2MBL.

</details>


### [136] [A Hybrid DNN Transformer AE Framework for Corporate Tax Risk Supervision and Risk Level Assessment](https://arxiv.org/abs/2509.23862)
*Zhenzhen Song,Nanxi Wang,Hongji Li*

Main category: cs.CE

TL;DR: 本文提出用于企业税务风险监管和风险等级评估的混合深度学习框架DNN - Transformer - Autoencoder，实验证明其有效，为智能税务风险管理提供方法创新和监管启示。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法难以捕捉大规模企业数据中复杂动态的税务相关异常，因此需要新方法进行企业税务风险监管和评估。

Method: 提出DNN - Transformer - Autoencoder混合深度学习框架，集成DNN建模静态企业属性、Transformer架构捕捉历史财务时间序列长期依赖、Autoencoder无监督检测异常税务行为，融合模块输出生成综合风险评分并映射到离散风险等级。

Result: 在真实企业税务数据集上实验，框架准确率达0.91，Macro F1分数达0.88。

Conclusion: 混合模型提高了分类性能，增强了在实际税务监管场景中的可解释性和适用性，为智能税务风险管理提供方法创新和监管启示。

Abstract: Tax risk supervision has become a critical component of modern financial
governance, as irregular tax behaviors and hidden compliance risks pose
significant challenges to regulatory authorities and enterprises alike.
Traditional rule-based methods often struggle to capture complex and dynamic
tax-related anomalies in large-scale enterprise data. To address this issue,
this paper proposes a hybrid deep learning framework
(DNN-Transformer-Autoencoder) for corporate tax risk supervision and risk level
assessment. The framework integrates three complementary modules: a Deep Neural
Network (DNN) for modeling static enterprise attributes, a Transformer-based
architecture for capturing long-term dependencies in historical financial time
series, and an Autoencoder (AE) for unsupervised detection of anomalous tax
behaviors. The outputs of these modules are fused to generate a comprehensive
risk score, which is further mapped into discrete risk levels (high, medium,
low). Experimental evaluations on a real-world enterprise tax dataset
demonstrate the effectiveness of the proposed framework, achieving an accuracy
of 0.91 and a Macro F1-score of 0.88. These results indicate that the hybrid
model not only improves classification performance but also enhances
interpretability and applicability in practical tax regulation scenarios. This
study provides both methodological innovation and regulatory implications for
intelligent tax risk management.

</details>


### [137] [Identifying the Multimodal Hierarchy of Public Transit Systems Using Trip Chain Data](https://arxiv.org/abs/2509.24220)
*Junhee Lee,Seungmo Kang,Jinwoo Lee*

Main category: cs.CE

TL;DR: 引入宏观多模式层次概念，提出用多模式智能卡行程链数据识别城市多模式层次的方法，并以韩国首尔及周边地区数据验证。


<details>
  <summary>Details</summary>
Motivation: 城市交通模式融合使公共交通系统复杂，需清晰理解不同模式间的交互。

Method: 提出用多模式智能卡行程链数据识别城市多模式层次的方法。

Result: 用韩国首尔及周边地区实际数据进行了方法应用演示。

Conclusion: 引入的宏观多模式层次概念和提出的识别方法有助于理解城市交通模式间的交互。

Abstract: As urban mobility integrates traditional and emerging modes, public transit
systems are becoming increasingly complex. Some modes complement each other,
while others compete, influencing users' multimodal itineraries. To provide a
clear, high-level understanding of these interactions, we introduce the concept
of a macroscopic multimodal hierarchy. In this framework, trips follow an
"ascending-descending" order, starting and ending with lower hierarchical modes
(e.g., walking) that offer high accessibility, while utilizing higher modes
(e.g., subways) for greater efficiency. We propose a methodology to identify
the multimodal hierarchy of a city using multimodal smart card trip chain data
and demonstrate its application with actual data collected from Seoul and the
surrounding metropolitan area in South Korea.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [138] [PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation](https://arxiv.org/abs/2509.23338)
*Wei Zhou,Guoliang Li,Haoyu Wang,Yuxing Han,Xufei Wu,Fan Wu,Xuanhe Zhou*

Main category: cs.DB

TL;DR: 现有SQL基准不适合SQL到SQL评估，本文引入PARROT基准用于跨系统SQL翻译，并提供多种变体，还发布公开排行榜和源代码。


<details>
  <summary>Details</summary>
Motivation: 跨系统SQL翻译有重要实际意义但研究不足，现有SQL基准不适合该任务评估。

Method: 从38个开源基准和实际业务服务中收集598个翻译对构建PARROT基准，还提供PARROT - Diverse和PARROT - Simple等变体。

Result: LLMs在PARROT上平均准确率低于38.53%，基准涵盖22个生产级数据库系统。

Conclusion: PARROT基准可用于跨系统SQL翻译评估，公开排行榜和代码能促进相关研究。

Abstract: Large language models (LLMS) have shown increasing effectiveness in
Text-to-SQL tasks. However, another closely related problem, Cross-System SQL
Translation (a.k.a., SQL-to-SQL), which adapts a query written for one database
system (e.g., MySQL) into its equivalent one for another system (e.g.,
ClickHouse), is of great practical importance but remains underexplored.
Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which
(1) focus on a limited set of database systems (often just SQLite) and (2)
cannot capture many system-specific SQL dialects (e.g., customized functions,
data types, and syntax rules). Thus, in this paper, we introduce PARROT, a
Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT
comprises 598 translation pairs from 38 open-source benchmarks and real-world
business services, specifically prepared to challenge system-specific SQL
understanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We
also provide multiple benchmark variants, including PARROT-Diverse with 28,003
translations (for extensive syntax testing) and PARROT-Simple with 5,306
representative samples (for focused stress testing), covering 22
production-grade database systems. To promote future research, we release a
public leaderboard and source code at: https://code4db.github.io/parrot-bench/.

</details>


### [139] [ML-Asset Management: Curation, Discovery, and Utilization](https://arxiv.org/abs/2509.23577)
*Mengying Wang,Moming Duan,Yicong Huang,Chen Li,Bingsheng He,Yinghui Wu*

Main category: cs.DB

TL;DR: 该教程全面概述机器学习资产全生命周期管理活动，提供资产分类与管理问题，调研技术并指出机会和系统挑战，通过演示为从业者提供见解和工具。


<details>
  <summary>Details</summary>
Motivation: 机器学习资产在实践中虽增长迅速，但因文档分散、存储孤立等问题未被充分利用，使机器学习资产管理成为紧迫挑战。

Method: 对机器学习资产在全生命周期的管理活动进行全面概述，提供分类、梳理主要管理问题、调研先进技术、识别各阶段机会，还通过系统的实时演示。

Result: 为研究人员和从业者提供了可操作的见解和实用工具。

Conclusion: 教程有助于在现实和特定领域场景中推进机器学习资产管理。

Abstract: Machine learning (ML) assets, such as models, datasets, and metadata, are
central to modern ML workflows. Despite their explosive growth in practice,
these assets are often underutilized due to fragmented documentation, siloed
storage, inconsistent licensing, and lack of unified discovery mechanisms,
making ML-asset management an urgent challenge. This tutorial offers a
comprehensive overview of ML-asset management activities across its lifecycle,
including curation, discovery, and utilization. We provide a categorization of
ML assets, and major management issues, survey state-of-the-art techniques, and
identify emerging opportunities at each stage. We further highlight
system-level challenges related to scalability, lineage, and unified indexing.
Through live demonstrations of systems, this tutorial equips both researchers
and practitioners with actionable insights and practical tools for advancing
ML-asset management in real-world and domain-specific settings.

</details>


### [140] [NeuSO: Neural Optimizer for Subgraph Queries](https://arxiv.org/abs/2509.23775)
*Linglin Yang,Lei Zou,Chunshan Zhao*

Main category: cs.DB

TL;DR: 提出NeuSO以解决现有子图查询方法的性能问题，实验证明其性能和效率更优。


<details>
  <summary>Details</summary>
Motivation: 现有子图查询方法依赖启发式顶点匹配顺序，学习型优化器因图数据特性无法直接应用，导致在线性能低效。

Method: 提出NeuSO，有高效查询图编码器和估计器，用多任务框架训练以估计子查询基数和执行成本，使用自顶向下的计划枚举器生成执行计划。

Result: 在多个数据集的大量实验中，NeuSO在性能和效率上优于现有子图查询排序方法。

Conclusion: NeuSO能有效应对子图查询挑战，实现高精度和高效率。

Abstract: Subgraph query is a critical task in graph analysis with a wide range of
applications across various domains. Most existing methods rely on heuristic
vertex matching orderings, which may significantly degrade enumeration
performance for certain queries. While learning-based optimizers have recently
gained attention in the context of relational databases, they cannot be
directly applied to subgraph queries due to the heterogeneous and
schema-flexible nature of graph data, as well as the large number of joins
involved in subgraph queries. These complexities often leads to inefficient
online performance, making such approaches impractical for real-world graph
database systems. To address this challenge, we propose NeuSO, a novel
learning-based optimizer for subgraph queries that achieves both high accuracy
and efficiency. NeuSO features an efficient query graph encoder and an
estimator which are trained using a multi-task framework to estimate both
subquery cardinality and execution cost. Based on these estimates, NeuSO
employs a top-down plan enumerator to generate high-quality execution plans for
subgraph queries. Extensive experiments on multiple datasets demonstrate that
NeuSO outperforms existing subgraph query ordering approaches in both
performance and efficiency.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [141] [Analysis of the carbon footprint of HPC](https://arxiv.org/abs/2509.22679)
*Abdessalam Benhari,Yves Denneulin,Frédéric Desprez,Fanny Dufossé,Denis Trystram*

Main category: cs.DC

TL;DR: 论文探讨高性能系统碳排放演变，考虑大型系统全生命周期，引入能源结构分析并建立未来5年HPC影响预测模型。


<details>
  <summary>Details</summary>
Motivation: 计算能力需求增长带来能源成本上升和高碳足迹问题，研究高性能系统碳排放演变。

Method: 考虑多个大型系统全生命周期，在Top500系统分析中引入能源结构，推导预测模型。

Result: 文中未提及具体结果。

Conclusion: 文中未提及具体结论。

Abstract: The demand in computing power has never stopped growing over the years.
Today, the performance of the most powerful systems exceeds the exascale.
Unfortunately, this growth also comes with ever-increasing energy costs,
leading to a high carbon footprint. This paper investigates the evolution of
high performance systems in terms of carbon emissions. A lot of studies focus
on Top500 (and Green500) as the tip of an iceberg to identify trends in the
domain in terms of computing performance. We propose here to go further in
considering the whole span life of several large scale systems and to link the
evolution with trajectory toward 2030. More precisely, we introduce the energy
mix in the analysis of Top500 systems and we derive a predictive model for
estimating the weight of HPC for the next 5 years.

</details>


### [142] [FLAME: A Serving System Optimized for Large-Scale Generative Recommendation with Efficiency](https://arxiv.org/abs/2509.22681)
*Xianwen Guo,Bin Huang,Xiaomeng Wu,Guanlin Wu,Fangjian Li,Shijia Wang,Qiang Xiao,Chuanjiang Luo,Yong Li*

Main category: cs.DC

TL;DR: 传统DLRMs计算负担小，GR模型虽扩展性强但计算负担大，本文提出针对GR模型的在线服务系统FLAME，通过多模块优化提升性能。


<details>
  <summary>Details</summary>
Motivation: GR模型计算负担大，在线部署面临挑战，需高效在线服务系统。

Method: 利用CPU - GPU异构硬件解耦特征预处理和模型计算；封装PDA模块优化内存；基于NVIDIA TensorRT实现FKE模块加速模型计算；设计DSO模块协调并发请求。

Result: PDA模块实现1.9x吞吐量提升和1.7x延迟降低；FKE模块实现4.6x - 6.1x加速比和4.7x - 6.3x吞吐量提升；DSO模块在非均匀分布下使吞吐量提升1.3x、加速2.3x。

Conclusion: FLAME能有效支持GR模型大规模在线部署，显著提升系统性能。

Abstract: Generative recommendation (GR) models possess greater scaling power compared
to traditional deep learning recommendation models (DLRMs), yet they also
impose a tremendous increase in computational burden. Measured in FLOPs, a
typical GR model's workload sits in $10^9 \sim 10^{11}$ range, roughly four
orders of magnitude higher than traditional DLRMs. Delivering accurate results
in a few tens of milliseconds while processing billions of such requests per
day puts extreme demands on the performance of the online serving system.
Therefore, for industry practitioners, the alluring gains of GR models are
tempered by the formidable challenge of online deployment at scale in
production services. In this work, we introduce a comprehensive solution of
online serving system tailored For Large-scale GenerAtive RecoMmendation with
Efficiency (FLAME). Specifically, we leveraging CPU-GPU heterogeneous hardware
to decouple feature pre-processing and model computation. We encapsulated
several memory optimization features as the Proximal Data Accelerator (PDA)
module to make full use of limited bandwidth and storage resources, which
achieves a 1.9x throughput gain and a 1.7x latency reduction. We implement the
Fused Kernel Engine (FKE) module based on the functionality and interface of
NVIDIA TensorRT to boost model computation, delivering a speedup ratio of
4.6x-6.1x, throughput gain ratio of 4.7x-6.3x one step further. In addition, we
design the Dynamic Stream Orchestrator (DSO) module to coordinate concurrent
requests, enhancing the system throughput performance with 1.3x improvement in
throughput and 2.3x speed-up under non-uniform distribution of upstream
candidates. Comprehensive evaluations demonstrate that our FLAME effectively
supports large-scale online deployment of GR models and achieves remarkable
improvements in system performance.

</details>


### [143] [ZKProphet: Understanding Performance of Zero-Knowledge Proofs on GPUs](https://arxiv.org/abs/2509.22684)
*Tarunesh Verma,Yichao Yuan,Nishil Talati,Todd Austin*

Main category: cs.DC

TL;DR: 本文对GPU上的零知识证明（ZKP）进行性能研究，发现NTT是瓶颈，提出通过参数调优等方式提升性能并提供路线图。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺少对ZKPs执行瓶颈的系统表征及其在现代GPU架构上的可扩展性研究。

Method: 对GPU上的ZKPs进行全面性能研究，分析各内核的执行情况，观察运算特点。

Result: 发现ZKPs在优化MSM后受NTT瓶颈，现有NTT实现未充分利用GPU资源，性能受整数计算单元限制。

Conclusion: 为ZKP社区提供在GPU上扩展性能的路线图，以构建适用于应用需求和硬件资源的GPU加速ZKPs。

Abstract: Zero-Knowledge Proofs (ZKP) are protocols which construct cryptographic
proofs to demonstrate knowledge of a secret input in a computation without
revealing any information about the secret. ZKPs enable novel applications in
private and verifiable computing such as anonymized cryptocurrencies and
blockchain scaling and have seen adoption in several real-world systems. Prior
work has accelerated ZKPs on GPUs by leveraging the inherent parallelism in
core computation kernels like Multi-Scalar Multiplication (MSM). However, we
find that a systematic characterization of execution bottlenecks in ZKPs, as
well as their scalability on modern GPU architectures, is missing in the
literature. This paper presents ZKProphet, a comprehensive performance study of
Zero-Knowledge Proofs on GPUs. Following massive speedups of MSM, we find that
ZKPs are bottlenecked by kernels like Number-Theoretic Transform (NTT), as they
account for up to 90% of the proof generation latency on GPUs when paired with
optimized MSM implementations. Available NTT implementations under-utilize GPU
compute resources and often do not employ architectural features like
asynchronous compute and memory operations. We observe that the arithmetic
operations underlying ZKPs execute exclusively on the GPU's 32-bit integer
pipeline and exhibit limited instruction-level parallelism due to data
dependencies. Their performance is thus limited by the available integer
compute units. While one way to scale the performance of ZKPs is adding more
compute units, we discuss how runtime parameter tuning for optimizations like
precomputed inputs and alternative data representations can extract additional
speedup. With this work, we provide the ZKP community a roadmap to scale
performance on GPUs and construct definitive GPU-accelerated ZKPs for their
application requirements and available hardware resources.

</details>


### [144] [Enhancing Cluster Scheduling in HPC: A Continuous Transfer Learning for Real-Time Optimization](https://arxiv.org/abs/2509.22701)
*Leszek Sliwko,Jolanta Mizera-Pietraszko*

Main category: cs.DC

TL;DR: 提出机器学习辅助方法优化集群系统任务调度，在Google集群数据上评估效果好，推动集群管理发展。


<details>
  <summary>Details</summary>
Motivation: 传统调度器如Kubernetes实时适应性差，需要优化集群系统任务调度。

Method: 采用连续迁移学习模型，在操作过程中动态演变，减少重新训练需求。

Result: 在Google集群数据上评估，模型准确率超99%，减少计算开销，改善受限任务调度延迟。

Conclusion: 该可扩展解决方案能实现实时优化，推动机器学习在集群管理中的集成，为未来自适应调度策略奠定基础。

Abstract: This study presents a machine learning-assisted approach to optimize task
scheduling in cluster systems, focusing on node-affinity constraints.
Traditional schedulers like Kubernetes struggle with real-time adaptability,
whereas the proposed continuous transfer learning model evolves dynamically
during operations, minimizing retraining needs. Evaluated on Google Cluster
Data, the model achieves over 99% accuracy, reducing computational overhead and
improving scheduling latency for constrained tasks. This scalable solution
enables real-time optimization, advancing machine learning integration in
cluster management and paving the way for future adaptive scheduling
strategies.

</details>


### [145] [TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents](https://arxiv.org/abs/2509.24063)
*Lukas Breitwieser,Ahmad Hesam,Abdullah Giray Yağlıkçı,Mohammad Sadrosadati,Fons Rademakers,Onur Mutlu*

Main category: cs.DC

TL;DR: 针对现有平台BioDynaMo不能跨服务器扩展的问题，提出分布式代理模拟引擎TeraAgent，通过两种解决方案实现极端规模模拟等优势。


<details>
  <summary>Details</summary>
Motivation: 现有基于共享内存实现的BioDynaMo平台不能跨服务器扩展，需新的分布式代理模拟引擎。

Method: 提出定制序列化机制，允许从接收缓冲区直接访问和修改代理；利用基于代理模拟的迭代性质，用增量编码减少数据传输。

Result: TeraAgent实现半万亿代理的极端规模模拟（提升84倍），减少计算时间，提高与第三方工具互操作性，提供更多硬件灵活性。

Conclusion: TeraAgent解决了现有平台的局限，能有效进行分布式代理模拟。

Abstract: Agent-based simulation is an indispensable paradigm for studying complex
systems. These systems can comprise billions of agents, requiring the computing
resources of multiple servers to simulate. Unfortunately, the state-of-the-art
platform, BioDynaMo, does not scale out across servers due to its
shared-memory-based implementation.
  To overcome this key limitation, we introduce TeraAgent, a distributed
agent-based simulation engine. A critical challenge in distributed execution is
the exchange of agent information across servers, which we identify as a major
performance bottleneck. We propose two solutions: 1) a tailored serialization
mechanism that allows agents to be accessed and mutated directly from the
receive buffer, and 2) leveraging the iterative nature of agent-based
simulations to reduce data transfer with delta encoding.
  Built on our solutions, TeraAgent enables extreme-scale simulations with half
a trillion agents (an 84x improvement), reduces time-to-result with additional
compute nodes, improves interoperability with third-party tools, and provides
users with more hardware flexibility.

</details>


### [146] [Intelligent Load Balancing in Cloud Computer Systems](https://arxiv.org/abs/2509.22704)
*Leszek Sliwko*

Main category: cs.DC

TL;DR: 本文聚焦云环境下任务动态分配策略，设计策略以维护系统稳定性并降低成本，有多项新贡献且实验结果良好。


<details>
  <summary>Details</summary>
Motivation: 设计动态分配任务的策略，在不使云节点过载的情况下，以最低成本维持系统稳定性。

Method: 对调度器进行分类，构建云资源利用抽象模型，实验虚拟机实时迁移，创建高保真云工作负载模拟器，提出并检验两种资源管理方法，在威斯敏斯特大学HPC集群进行大量实验。

Result: 项目实验取得了有前景的结果。

Conclusion: 文中未明确提及具体结论内容，但从上下文推测按设计策略和方法进行实验取得了积极成果。

Abstract: Cloud computing is an established technology allowing users to share
resources on a large scale, never before seen in IT history. A cloud system
connects multiple individual servers in order to process related tasks in
several environments at the same time. Clouds are typically more cost-effective
than single computers of comparable computing performance. The sheer physical
size of the system itself means that thousands of machines may be involved. The
focus of this research was to design a strategy to dynamically allocate tasks
without overloading Cloud nodes which would result in system stability being
maintained at minimum cost. This research has added the following new
contributions to the state of knowledge: (i) a novel taxonomy and
categorisation of three classes of schedulers, namely OS-level, Cluster and Big
Data, which highlight their unique evolution and underline their different
objectives; (ii) an abstract model of cloud resources utilisation is specified,
including multiple types of resources and consideration of task migration
costs; (iii) a virtual machine live migration was experimented with in order to
create a formula which estimates the network traffic generated by this process;
(iv) a high-fidelity Cloud workload simulator, based on a month-long workload
traces from Google's computing cells, was created; (v) two possible approaches
to resource management were proposed and examined in the practical part of the
manuscript: the centralised metaheuristic load balancer and the decentralised
agent-based system. The project involved extensive experiments run on the
University of Westminster HPC cluster, and the promising results are presented
together with detailed discussions and a conclusion.

</details>


### [147] [Metadata-Guided Adaptable Frequency Scaling across Heterogeneous Applications and Devices](https://arxiv.org/abs/2509.22707)
*Jinqi Yan,Fang He,Qianlong Sang,Bifeng Tong,Peng Sun,Yili Gong,Chuang Hu,Dazhao Cheng*

Main category: cs.DC

TL;DR: 传统DVFS调速器不足，强化学习方法有局限，提出MetaDVFS框架，在多设备多应用上评估有良好表现。


<details>
  <summary>Details</summary>
Motivation: 传统启发式调速器难以应对异构SoC设计和多样应用负载，强化学习方法泛化能力差、部署成本高，需新方案。

Method: 将DVFS问题表述为多任务强化学习问题，引入MetaDVFS框架，利用元数据挖掘和迁移共享知识。

Result: 在五款Google Pixel设备运行六个应用的评估中，性能功率比最高提升17%，体验质量最高提升26%，适配速度快70.8%，性能高5.8 - 27.6%，避免负迁移。

Conclusion: MetaDVFS是异构移动环境中DVFS部署的有效且可扩展的解决方案。

Abstract: Dynamic Voltage and Frequency Scaling is essential for enhancing energy
efficiency in mobile platforms. However, traditional heuristic-based governors
are increasingly inadequate for managing the complexity of heterogeneous
System-on-Chip designs and diverse application workloads. Although
reinforcement learning approaches offer improved performance, their poor
generalization capability and reliance on extensive retraining for each
hardware and application combination leads to significant deployment costs. In
this work, we observe that device and application metadata inherently
encapsulate valuable knowledge for DVFS, presenting an opportunity to overcome
these limitations. We formulate DVFS for heterogeneous devices and applications
as a multi-task reinforcement learning problem. We introduce MetaDVFS, which is
a metadata-guided framework that systematically leverages metadata to discover
and transfer shared knowledge across DVFS tasks. MetaDVFS can output a set of
DVFS models with significant generalization capability for various applications
of heterogeneous devices. Evaluations on five Google Pixel devices running six
applications show that MetaDVFS achieves up to 17% improvement in
Performance-Power Ratio and up to 26% improvement in Quality of Experience.
Compared to state-of-the-art methods, MetaDVFS delivers 70.8% faster adaptation
and 5.8-27.6% higher performance over standalone device-application specific
training, while avoiding negative transfer effects. These results establish
MetaDVFS as an effective and scalable solution for DVFS deployment in
heterogeneous mobile environments.

</details>


### [148] [Efficient Fine-Grained GPU Performance Modeling for Distributed Deep Learning of LLM](https://arxiv.org/abs/2509.22832)
*Biyao Zhang,Mingkai Zheng,Debargha Ganguly,Xuecen Zhang,Vikash Singh,Vipin Chaudhary,Zhao Zhang*

Main category: cs.DC

TL;DR: 提出分解大语言模型为核心计算原语的方法预测训练时间，在两个HPC系统验证，误差低且可在CPU运行。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练时间预测因组件、并行策略和通信交互复杂而具挑战性，学习和分析模型有各自局限。

Method: 将大语言模型分解为核心计算原语，采用算子级分解、轻量级采样硬件感知预测模型和端到端预测系统。

Result: 框架在两个系统上对最多20B参数、128个GPU的模型实现低平均预测误差，分别为4.98%和9.38%。

Conclusion: 该方法可在CPU上运行，能快速迭代硬件配置和训练策略，无需昂贵集群实验。

Abstract: Training Large Language Models(LLMs) is one of the most compute-intensive
tasks in high-performance computing. Predicting end-to-end training time for
multi-billion parameter models distributed across hundreds of GPUs remains
challenging due to complex interactions between transformer components,
parallelism strategies(data, model, pipeline, tensor), and multi-tier
communication. Learned models require costly sampling, while analytical models
often struggle with real-world network and hardware complexities. We address
this by decomposing LLMs into core computational primitives and modeling them
with: (1) operator-level decomposition for fine-grained analysis; (2)
lightweight sampling based hardware-aware prediction models for key operations;
(3) an end-to-end prediction system integrating these components across complex
parallelization strategies. Crucially, our methodology has been validated on
two large-scale HPC systems. Our framework achieves low average prediction
errors-4.98\% on Perlmutter(A100) and 9.38\% on Vista(GH200)-for models up to
20B parameters across 128 GPUs. Importantly, it runs entirely on CPUs, enabling
rapid iteration over hardware configurations and training strategies without
costly on-cluster experimentation.

</details>


### [149] [OptimES: Optimizing Federated Learning Using Remote Embeddings for Graph Neural Networks](https://arxiv.org/abs/2509.22922)
*Pranjal Naman,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 本文提出优化框架OptimES用于联邦GNN训练，能减少网络成本和训练时间，在不同图数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有联邦GNN训练方法因通信成本大导致性能受限，需优化以解决隐私问题同时提升效率。

Method: 提出OptimES框架，采用远程邻域修剪、嵌入推送与本地训练重叠、动态拉取嵌入等策略。

Result: 在大型密集图上比EmbC快约3.5倍，比默认联邦GNN准确率高约16%；在稀疏图上比EmbC快约11倍。

Conclusion: OptimES框架能有效减少训练时间，在不同图数据集上有较好的性能提升。

Abstract: Graph Neural Networks (GNNs) have experienced rapid advancements in recent
years due to their ability to learn meaningful representations from graph data
structures. However, in most real-world settings, such as financial transaction
networks and healthcare networks, this data is localized to different data
owners and cannot be aggregated due to privacy concerns. Federated Learning
(FL) has emerged as a viable machine learning approach for training a shared
model that iteratively aggregates local models trained on decentralized data.
This addresses privacy concerns while leveraging parallelism. State-of-the-art
methods enhance the privacy-respecting convergence accuracy of federated GNN
training by sharing remote embeddings of boundary vertices through a server
(EmbC). However, they are limited by diminished performance due to large
communication costs. In this article, we propose OptimES, an optimized
federated GNN training framework that employs remote neighbourhood pruning,
overlapping the push of embeddings to the server with local training, and
dynamic pulling of embeddings to reduce network costs and training time. We
perform a rigorous evaluation of these strategies for four common graph
datasets with up to $111M$ vertices and $1.8B$ edges. We see that a modest drop
in per-round accuracy due to the preemptive push of embeddings is out-stripped
by the reduction in per-round training time for large and dense graphs like
Reddit and Products, converging up to $\approx 3.5\times$ faster than EmbC and
giving up to $\approx16\%$ better accuracy than the default federated GNN
learning. While accuracy improvements over default federated GNNs are modest
for sparser graphs like Arxiv and Papers, they achieve the target accuracy
about $\approx11\times$ faster than EmbC.

</details>


### [150] [Characterizing FaaS Workflows on Public Clouds: The Good, the Bad and the Ugly](https://arxiv.org/abs/2509.23013)
*Varad Kulkarni,Nikhil Reddy,Tuhin Khare,Abhinandan S. Prasad,Chitra Babu,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 本文对AWS和Azure的三个流行FaaS工作流平台进行广泛评估，分析其特性，为开发者提供参考。


<details>
  <summary>Details</summary>
Motivation: FaaS工作流中函数间复杂交互和专有平台内部可见性有限，缺乏对FaaS工作流平台的原则性和严谨研究。

Method: 对AWS和Azure的三个流行FaaS工作流平台进行广泛评估，运行25个微基准和应用工作流，进行超132k次调用。

Result: 详细分析证实了一些传统观点，也揭示了函数执行、工作流编排、函数间交互、冷启动扩展和货币成本等方面的独特见解。

Conclusion: 研究结果有助于开发者更好配置和编程这些平台，设定性能和可扩展性预期，识别改进平台的研究差距。

Abstract: Function-as-a-service (FaaS) is a popular serverless computing paradigm for
developing event-driven functions that elastically scale on public clouds. FaaS
workflows, such as AWS Step Functions and Azure Durable Functions, are composed
from FaaS functions, like AWS Lambda and Azure Functions, to build practical
applications. But, the complex interactions between functions in the workflow
and the limited visibility into the internals of proprietary FaaS platforms are
major impediments to gaining a deeper understanding of FaaS workflow platforms.
While several works characterize FaaS platforms to derive such insights, there
is a lack of a principled and rigorous study for FaaS workflow platforms, which
have unique scaling, performance and costing behavior influenced by the
platform design, dataflow and workloads. In this article, we perform extensive
evaluations of three popular FaaS workflow platforms from AWS and Azure,
running 25 micro-benchmark and application workflows over 132k invocations. Our
detailed analysis confirms some conventional wisdom but also uncovers unique
insights on the function execution, workflow orchestration, inter-function
interactions, cold-start scaling and monetary costs. Our observations help
developers better configure and program these platforms, set performance and
scalability expectations, and identify research gaps on enhancing the
platforms.

</details>


### [151] [Memory Efficient and Staleness Free Pipeline Parallel DNN Training Framework with Improved Convergence Speed](https://arxiv.org/abs/2509.23241)
*Ankita Dutta,Nabendu Chaki,Rajat K. De*

Main category: cs.DC

TL;DR: 本文介绍基于流水线并行的两个DNN训练框架V - TiMePReSt和I - TiMePReSt，对比其特点与优势。


<details>
  <summary>Details</summary>
Motivation: DNN在多GPU上训练资源需求高，需开发并行技术。

Method: 引入V - TiMePReSt（无延迟系统）和I - TiMePReSt（有延迟但不依赖权重暂存）框架，数学公式量化陈旧权重的意义。

Result: 实验表明V - TiMePReSt在权重参数陈旧程度和GPU内存效率上占优，I - TiMePReSt在去除权重参数陈旧性和平衡GPU内存消耗与收敛速度上更优。

Conclusion: 所提出的两个框架各有优势，能在不同方面提升DNN多GPU训练性能。

Abstract: High resource requirement for Deep Neural Network (DNN) training across
multiple GPUs necessitates development of various parallelism techniques. In
this paper, we introduce two interconnected DNN training frameworks, namely,
V-TiMePReSt and I-TiMePReSt, based on pipeline parallelism, a variant of model
parallelism. V-TiMePReSt is a completely staleness-free system which enables
the DNNs to be trained on the latest updated weights in each stage of all
forward and backward passes. Developing staleness-aware systems at the expense
of weight stashing reduces GPU-memory consumption, however, increases the
number of epochs to converge. Thus, we introduce I-TiMePReSt, which is also a
staleness-aware system, but not at the expense of weight stashing. It does not
rely solely on the stale weights or the latest updated weights. I-TiMePReSt
computes an intermediate weight towards the latter and performs backward pass
on it. Additionally, we formulate the significance of the stale weights
mathematically depending on the degree of staleness. In contrast to
V-TiMePReSt, I-TiMePReSt works based on the assumption that stale weights have
a significant contribution in training, which can be quantified mathematically
based on the degree of staleness, although there are other contributory factors
which should not be ignored. Experimental results show that V-TiMePReSt is
advantageous over existing models in terms of $1)$ the extent of staleness of
the weight parameter values and $2)$ GPU memory efficiency, while I-TiMePReSt
is superior in terms of $1)$ removing staleness of the weight parameters
without removing weight stashing and $2)$ maintaining the trade-off between GPU
memory consumption and convergence speed (number of epochs).

</details>


### [152] [Scaling LLM Test-Time Compute with Mobile NPU on Smartphones](https://arxiv.org/abs/2509.23324)
*Zixu Hao,Jianyu Wei,Tuowei Wang,Minxing Huang,Huiqiang Jiang,Shiqi Jiang,Ting Cao,Ju Ren*

Main category: cs.DC

TL;DR: 本文提出在移动NPUs上应用并行测试时缩放技术提升小模型性能，引入两项关键技术，实现端到端推理系统，实验显示显著加速，小模型可匹配或超越大模型精度。


<details>
  <summary>Details</summary>
Motivation: 部署大语言模型在移动设备面临小模型性能不足和大模型资源消耗过多问题，且移动NPUs计算资源未充分利用。

Method: 提出在移动NPUs上应用并行测试时缩放技术，引入硬件感知的分块量化方案和基于LUT的复杂操作替代方法，设计并实现端到端推理系统。

Result: 实现显著加速，混合精度GEMM最高达19.0倍，Softmax达2.2倍，小模型使用测试时缩放可匹配或超越大模型精度。

Conclusion: 该方法达到了新的性能 - 成本帕累托前沿。

Abstract: Deploying Large Language Models (LLMs) on mobile devices faces the challenge
of insufficient performance in smaller models and excessive resource
consumption in larger ones. This paper highlights that mobile Neural Processing
Units (NPUs) have underutilized computational resources, particularly their
matrix multiplication units, during typical LLM inference. To leverage this
wasted compute capacity, we propose applying parallel test-time scaling
techniques on mobile NPUs to enhance the performance of smaller LLMs. However,
this approach confronts inherent NPU challenges, including inadequate hardware
support for fine-grained quantization and low efficiency in general-purpose
computations. To overcome these, we introduce two key techniques: a
hardware-aware tile quantization scheme that aligns group quantization with NPU
memory access patterns, and efficient LUT-based replacements for complex
operations such as Softmax and dequantization. We design and implement an
end-to-end inference system that leverages the NPU's compute capability to
support test-time scaling on Qualcomm Snapdragon platforms. Experiments show
our approach brings significant speedups: up to 19.0 for mixed-precision GEMM
and 2.2 for Softmax. More importantly, we demonstrate that smaller models using
test-time scaling can match or exceed the accuracy of larger models, achieving
a new performance-cost Pareto frontier.

</details>


### [153] [A Predictive and Synergistic Two-Layer Scheduling Framework for LLM Serving](https://arxiv.org/abs/2509.23384)
*Yue Zhang,Yuansheng Chen,Xuan Mo,Alex Xi,Jialun Li,WeiGang Wu*

Main category: cs.DC

TL;DR: 现有LLM推理服务两层架构存在信息差问题，本文提出SynergySched框架，提升SLO达成率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决LLM推理服务两层架构中因信息差导致的效率问题，避免SLO违规和资源浪费。

Method: 提出SynergySched框架，含结构感知在线性能模型，引擎层LENS做SLO感知自适应调度，集群层PRISM做状态驱动路由。

Result: SynergySched平均提升SLO达成率43%，在长上下文和异构场景下吞吐量最高提升3倍，且在FlowGPT集群部署验证。

Conclusion: SynergySched能有效将LLM服务系统从被动负载均衡转向预测编排，提升性能。

Abstract: LLM inference serving typically scales out with a two-tier architecture: a
cluster router distributes requests to multiple inference engines, each of
which then in turn performs its own internal scheduling. However, this commonly
used paradigm suffers from critical, systemic inefficiency caused by the
information gaps across two layers. At the cluster-layer, the router mainly
relies on lagging, coarse-grained metrics, such as average latency and queue
length to make decisions, resulting in "decision lag" that leads to suboptimal
request routing. At the engine-layer, static heuristic scheduling policies
cannot effectively handle the dynamic workloads, leading a poor balance between
latency and throughput. Besides, these gaps may cause SLO violations and
resource waste, especially in heterogeneous cloud environments.
  To bridge such gaps, we propose SynergySched, a cross-layer framework that
shifts LLM serving system from reactive load balancing to predictive
orchestration. The core of SynergySched lies in a structurally-informed online
performance model that provides accurate, forward-looking per-step latency and
capacity estimations. This model empowers two key components. At the
engine-layer, LENS performs SLO-aware, adaptive scheduling, dynamically
optimizing batching to meet SLOs under real-time loads. At the cluster-layer,
PRISM uses predictive signals to perform state-driven routing, maximizing
cluster-wide performance and SLO attainment. Performance evaluations show that
SynergySched improves SLO attainment by 43% on average and achieves up to 3x
throughput speedup in long-context and heterogeneous scenarios. Besides, we
also deploy SynergySched on FlowGPT's clusters to demonstrate its advantages in
production environment.

</details>


### [154] [Enhancing Communication Efficiency in FL with Adaptive Gradient Quantization and Communication Frequency Optimization](https://arxiv.org/abs/2509.23419)
*Asadullah Tariq,Tariq Qayyum,Mohamed Adel Serhani,Farag Sallabi,Ikbal Taleb,Ezedin S. Barka*

Main category: cs.DC

TL;DR: 本文针对联邦学习通信开销大问题，提出三重策略，实验表明模型在保证准确率同时实现高通信效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习因设备与服务器频繁更新模型导致通信开销大，限制其在资源受限无线网络的部署。

Method: 提出三重策略，包括自适应特征消除策略、基于自适应梯度创新和误差敏感度的量化方法、通信频率优化。

Result: 通过实验评估模型在准确率、损失和收敛性等方面表现，结果显示模型在框架内实现高通信效率且保持准确率。

Conclusion: 所提模型能在保证准确率的同时，有效提高联邦学习的通信效率。

Abstract: Federated Learning (FL) enables participant devices to collaboratively train
deep learning models without sharing their data with the server or other
devices, effectively addressing data privacy and computational concerns.
However, FL faces a major bottleneck due to high communication overhead from
frequent model updates between devices and the server, limiting deployment in
resource-constrained wireless networks. In this paper, we propose a three-fold
strategy. Firstly, an Adaptive Feature-Elimination Strategy to drop less
important features while retaining high-value ones; secondly, Adaptive Gradient
Innovation and Error Sensitivity-Based Quantization, which dynamically adjusts
the quantization level for innovative gradient compression; and thirdly,
Communication Frequency Optimization to enhance communication efficiency. We
evaluated our proposed model's performance through extensive experiments,
assessing accuracy, loss, and convergence compared to baseline techniques. The
results show that our model achieves high communication efficiency in the
framework while maintaining accuracy.

</details>


### [155] [Lyte Quorum: Off-Chain Ready Smart Contract Hosted with Choice](https://arxiv.org/abs/2509.23448)
*Hao Hao,Dahlia Malkhi,Maofan Yin,Lizan Zhou*

Main category: cs.DC

TL;DR: 介绍去中心化平台Lyquor，提出三项创新，解决现有系统局限，支持可扩展去中心化计算。


<details>
  <summary>Details</summary>
Motivation: 重新构想区块链基础设施，解决现有系统的关键局限，实现可扩展的去中心化计算。

Method: 提出Fate - Constrained Ordering（FCO）、Direct Memory Architecture（DMA）、Universal Procedure Call（UPC）三项创新，采用Rust - macroed统一编程模型。

Result: 各组件由统一编程模型驱动，支持传统智能合约模式和新型分布式应用，且与以太坊API兼容。

Conclusion: Lyquor解决现有系统局限，为真正可扩展的去中心化计算提供了路径。

Abstract: This paper introduces Lyquor, a decentralized platform that reimagines
blockchain infrastructure through a service-centric model where nodes
selectively host smart contracts (called Lyquids) while preserving global
composability. We present three key innovations: (1) Fate-Constrained Ordering
(FCO), which decouples consensus from execution to enable selective hosting
without sacrificing Layer-1 grade composability; (2) Direct Memory Architecture
(DMA), which eliminates state access bottlenecks by providing each contract
with persistent, byte-addressable virtual memory; and (3) Universal Procedure
Call (UPC), which enables fault-tolerant, programmable coordination across
distributed off-chain computation. Together, these components are powered by a
Rust-macroed unified programming model where on-chain and off-chain logic
coexist seamlessly, supporting both traditional smart contract patterns and
novel distributed applications. Lyquor addresses critical limitations in
existing systems while maintaining compatibility with Ethereum APIs, offering a
path toward truly scalable decentralized computation.

</details>


### [156] [Parallel Algorithms for the One Sided Crossing Minimization Problem](https://arxiv.org/abs/2509.23706)
*Bogdan-Ioan Popa,Adrian-Marius Dumitran,Livia Magureanu*

Main category: cs.DC

TL;DR: 本文研究单边交叉最小化（OSCM）问题的精确和固定参数易处理（FPT）算法的并行化，证明算法并行化可接近线性加速，最佳结果在特定机器上达近19倍加速，并探讨未达线性加速原因。


<details>
  <summary>Details</summary>
Motivation: 多核心系统兴起，但OSCM问题的精确和FPT算法并行化研究不足，并行变体有处理更大图的潜力。

Method: 实现并分析OSCM问题的精确和FPT算法的顺序和并行形式。

Result: 算法并行化可接近线性加速，最佳结果在16核、32线程机器上达近19倍加速。

Conclusion: OSCM问题的精确和FPT算法并行化可接近线性加速，但并非总能达到线性加速，探究了背后原因。

Abstract: The One Sided Crossing Minimization (OSCM) problem is an optimization problem
in graph drawing that aims to minimize the number of edge crossings in
bipartite graph layouts. It has practical applications in areas such as network
visualization and VLSI (Very Large Scale Integration) design, where reducing
edge crossings improves the arrangement of circuit components and their
interconnections. Despite the rise of multi-core systems, the parallelization
of exact and fixed-parameter tractable (FPT) algorithms for OSCM remains
largely unexplored. Parallel variants offer significant potential for scaling
to larger graphs but require careful handling of synchronization and memory
management. In this paper, we explore various previously studied exact and FPT
algorithms for OSCM, implementing and analyzing them in both sequential and
parallel forms. Our main contribution lies in empirically proving that these
algorithms can achieve close to linear speedup under parallelization. In
particular, our best result achieves a speedup of nearly 19 on a 16-core,
32-thread machine. We further investigate and discuss the reasons why linear
speedup is not always attained.

</details>


### [157] [AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on Heterogeneous Models](https://arxiv.org/abs/2509.23722)
*Jihu Guo,Tenghui Ma,Wei Gao,Peng Sun,Jiaxing Li,Xun Chen,Yuyang Jin,Dahua Lin*

Main category: cs.DC

TL;DR: 提出支持自适应流水线并行的LLM训练系统AdaPtis，实验显示其比Megatron - LM I - 1F1B有速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有模型架构异质性增加使流水线气泡问题加剧，现有方法未对模型分区、放置和工作负载调度进行协同优化，导致效率提升有限或性能下降。

Method: 开发流水线性能模型估计训练吞吐量；基于该模型联合优化模型分区、放置和工作负载调度策略；设计统一流水线执行器支持不同流水线策略执行。

Result: AdaPtis在各种LLM架构和规模上比Megatron - LM I - 1F1B平均加速1.42倍（最高2.14倍）。

Conclusion: AdaPtis在解决流水线并行训练效率问题上表现良好，能有效提升训练速度。

Abstract: Pipeline parallelism is widely used to train large language models (LLMs).
However, increasing heterogeneity in model architectures exacerbates pipeline
bubbles, thereby reducing training efficiency. Existing approaches overlook the
co-optimization of model partition, model placement, and workload scheduling,
resulting in limited efficiency improvement or even performance degradation. To
respond, we propose AdaPtis, an LLM training system that supports adaptive
pipeline parallelism. First, we develop a pipeline performance model to
accurately estimate training throughput. Second, AdaPtis jointly optimizes
model partition, model placement, and workload scheduling policies guided by
this performance model. Third, we design a unified pipeline executor that
efficiently supports the execution of diverse pipeline strategies. Extensive
experiments show that AdaPtis achieves an average speedup of 1.42x (up to
2.14x) over Megatron-LM I-1F1B across various LLM architectures and scales.

</details>


### [158] [From Edge to HPC: Investigating Cross-Facility Data Streaming Architectures](https://arxiv.org/abs/2509.24030)
*Anjus George,Michael Brim,Christopher Zimmer,David Rogers,Sarp Oral,Zach Mayes*

Main category: cs.DC

TL;DR: 研究三种跨设施数据流式架构DTS、PRS和MSS，用合成工作负载评估，DTS吞吐量高、延迟低，MSS部署可行性和可扩展性好但开销大，PRS性能居中。


<details>
  <summary>Details</summary>
Motivation: 研究不同跨设施数据流式架构的特性及适用性。

Method: 使用DS2HPC架构框架和SciStream工具包在ACE基础设施上实现三种架构，用合成工作负载进行模拟实验，测量吞吐量、往返时间和开销。

Result: DTS有最小跳数路径，吞吐量高、延迟低；MSS部署可行性和可扩展性好，但开销大；PRS性能居中，多数情况下与DTS相当。

Conclusion: 三种架构各有优缺点，可根据不同场景选择合适的架构。

Abstract: In this paper, we investigate three cross-facility data streaming
architectures, Direct Streaming (DTS), Proxied Streaming (PRS), and Managed
Service Streaming (MSS). We examine their architectural variations in data flow
paths and deployment feasibility, and detail their implementation using the
Data Streaming to HPC (DS2HPC) architectural framework and the SciStream
memory-to-memory streaming toolkit on the production-grade Advanced Computing
Ecosystem (ACE) infrastructure at Oak Ridge Leadership Computing Facility
(OLCF). We present a workflow-specific evaluation of these architectures using
three synthetic workloads derived from the streaming characteristics of
scientific workflows. Through simulated experiments, we measure streaming
throughput, round-trip time, and overhead under work sharing, work sharing with
feedback, and broadcast and gather messaging patterns commonly found in AI-HPC
communication motifs. Our study shows that DTS offers a minimal-hop path,
resulting in higher throughput and lower latency, whereas MSS provides greater
deployment feasibility and scalability across multiple users but incurs
significant overhead. PRS lies in between, offering a scalable architecture
whose performance matches DTS in most cases.

</details>


### [159] [RServe: Overlapping Encoding and Prefill for Efficient LMM Inference](https://arxiv.org/abs/2509.24381)
*Tianyu Guo,Tianming Xu,Xianjie Chen,Junru Chen,Nong Xiao,Xianwei Zhang*

Main category: cs.DC

TL;DR: 传统大模态模型推理服务有问题，提出 REDServe 系统，实验显示能降延迟、提吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大模态模型推理流水线复杂，传统服务引擎存在资源干扰和数据依赖问题，现有方法未充分利用并行性。

Method: 基于编码模块和语言模型的分离架构，采用细粒度调度方法，利用可调度令牌和令牌预算平衡负载，结合分块预填充。

Result: 在代表性大模态模型上实验，REDServe 最多降低 66% 延迟，最多提高 109% 吞吐量。

Conclusion: REDServe 显著优于现有服务方法。

Abstract: Large multimodal models (LMMs) typically employ an encoding module to
transform multimodal data inputs into embeddings, which are then fed to
language models for further processing. However, efficiently serving LMMs
remains highly challenging due to the inherent complexity of their inference
pipelines. Traditional serving engines co-locate the encoding module and the
language model, leading to significant resource interference and tight data
dependency. Recent studies have alleviated this issue by disaggregating the
encoding module from the model, following a design style of prefill-decode
disaggregation. Nevertheless, these approaches fail to fully exploit
parallelism both within individual requests (intra-request) and across multiple
requests (inter-request).
  To overcome the limitation, we propose REDServe, an LMM inference system that
efficiently orchestrates intra- and inter-request pipelines. REDServe is
designed to reduce low latency and maximize parallelism at both intra- and
inter-request granularities. Built on the disaggregated architecture of the
encoding module and language model, REDServe adopts a fine-grained scheduling
method that overlaps multimodal encoding with the forward computation of the
language model within a single request. For inter-request pipeline, REDServe
leverages schedulable tokens and token budgets to balance computational loads
across micro-batches. Combined with chunked prefill, this enables a novel
scheduling strategy that coordinates the execution of intra- and inter-request
pipelines. Experimental evaluations on representative LMMs show that REDServe
achieves substantial latency reduction of up to 66% while improving throughput
by up to 109%, significantly outperforming existing serving approaches.

</details>


### [160] [SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in Long-Context LLM Serving](https://arxiv.org/abs/2509.24626)
*Qihui Zhou,Peiqi Yin,Pengfei Zuo,James Cheng*

Main category: cs.DC

TL;DR: 论文提出SparseServe系统，通过高效的HBM - DRAM分层管理释放动态稀疏注意力算法并行潜力，实验显示其性能优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 动态稀疏注意力算法（DSAs）下，HBM容量成为性能瓶颈，现有工作未解决分层HBM - DRAM存储挑战。

Method: 提出SparseServe系统，包括碎片化感知的KV缓存传输、工作集感知的批量大小控制和层分段预填充三个创新点。

Result: SparseServe较现有系统，平均首token延迟降低至9.26倍，token生成吞吐量提高至3.14倍。

Conclusion: SparseServe系统能有效解决DSAs在HBM - DRAM存储方面的问题，提升LLM服务性能。

Abstract: Serving long-context LLMs is costly because attention computation grows
linearly with context length. Dynamic sparse attention algorithms (DSAs)
mitigate this by attending only to the key-value (KV) cache of critical tokens.
However, with DSAs, the main performance bottleneck shifts from HBM bandwidth
to HBM capacity: KV caches for unselected tokens must remain in HBM for
low-latency decoding, constraining parallel batch size and stalling further
throughput gains. Offloading these underutilized KV caches to DRAM could free
HBM capacity, allowing larger parallel batch sizes. Yet, achieving such
hierarchical HBM-DRAM storage raises new challenges, including fragmented KV
cache access, HBM cache contention, and high HBM demands of hybrid batching,
that remain unresolved in prior work.
  This paper proposes SparseServe, an LLM serving system that unlocks the
parallel potential of DSAs through efficient hierarchical HBM-DRAM management.
SparseServe introduces three key innovations to address the challenges
mentioned above: (1) fragmentation-aware KV cache transfer, which accelerates
HBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted
saving (FlashD2H); (2) working-set-aware batch size control that adjusts batch
sizes based on real-time working set estimation to minimize HBM cache
thrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a
single layer, enabling efficient execution even for long prompts. Extensive
experimental results demonstrate that SparseServe achieves up to 9.26x lower
mean time-to-first-token (TTFT) latency and up to 3.14x higher token generation
throughput compared to state-of-the-art LLM serving systems.

</details>


### [161] [HAPT: Heterogeneity-Aware Automated Parallel Training on Heterogeneous Clusters](https://arxiv.org/abs/2509.24859)
*Antian Liang,Zhigang Zhao,Kai Zhang,Xuri Shi,Chuantao Li,Chunxiao Wang,Zhenying He,Yinan Jing,X. Sean Wang*

Main category: cs.DC

TL;DR: 介绍专为异构集群设计的自动并行训练框架Hapt，评估显示其比现有框架性能高1.3 - 1.6倍。


<details>
  <summary>Details</summary>
Motivation: GPU架构快速演变使模型训练基础设施异构性增加，现有为同构集群设计的框架在异构环境资源利用率低，需有效利用异构加速器进行分布式模型训练。

Method: 引入细粒度规划器搜索算子间并行策略，实现感知异构性的1F1B调度器根据网络特性调整微批次执行时间和顺序。

Result: Hapt在异构集群上比现有训练框架性能高1.3 - 1.6倍。

Conclusion: Hapt能有效解决异构集群中资源利用问题，提升分布式模型训练性能。

Abstract: With the rapid evolution of GPU architectures, the heterogeneity of model
training infrastructures is steadily increasing. In such environments,
effectively utilizing all available heterogeneous accelerators becomes critical
for distributed model training. However, existing frameworks, which are
primarily designed for homogeneous clusters, often exhibit significant resource
underutilization when deployed on heterogeneous accelerators and networks. In
this paper, we present Hapt, an automated parallel training framework designed
specifically for heterogeneous clusters. Hapt introduces a fine-grained planner
that efficiently searches a wide space for the inter-operator parallel
strategy, enabling Hapt to alleviate communication overheads while maintaining
balanced loads across heterogeneous accelerators. In addition, Hapt implements
a heterogeneity-aware 1F1B scheduler that adaptively adjusts the execution
timing and ordering of microbatches based on network characteristics,
maximizing computation-communication overlap under cross-cluster interconnects
while incurring only minimal memory overhead. Our evaluation results show that
Hapt can deliver 1.3x-1.6x higher performance on heterogeneous clusters than
state-of-the-art training frameworks.

</details>


### [162] [Graph Theory Meets Federated Learning over Satellite Constellations: Spanning Aggregations, Network Formation, and Performance Optimization](https://arxiv.org/abs/2509.24932)
*Fardis Nadimi,Payam Abdisarabshali,Jacob Chakareski,Nicholas Mastronarde,Seyyedali Hosseinalipour*

Main category: cs.DC

TL;DR: 介绍用于低地球轨道卫星星座的Fed - Span框架，利用图论原理解决分布式学习挑战，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决动态卫星网络中分布式学习面临的间歇性连接、卫星计算能力异构和数据集时变等关键挑战。

Method: 基于最小生成树和最小生成森林拓扑构建Fed - Span，通过连续约束表示形式化，推导能耗和延迟，得出非凸机器学习损失函数收敛界，将优化问题转化为几何规划求解。

Result: 在真实数据集上评估显示，Fed - Span模型收敛更快、能源效率更高、延迟更低。

Conclusion: Fed - Span是卫星网络中高效分布式学习的新解决方案。

Abstract: We introduce Fed-Span, a novel federated/distributed learning framework
designed for low Earth orbit satellite constellations. By leveraging
graph-theoretic principles, Fed-Span addresses critical challenges inherent to
distributed learning in dynamic satellite networks, including intermittent
satellite connectivity, heterogeneous computational capabilities of satellites,
and time-varying satellites' datasets. At its core, Fed-Span builds upon
minimum spanning tree (MST) and minimum spanning forest (MSF) topologies,
enabling spanning model aggregation and dispatching processes for distributed
learning. To formalize Fed-Span, we offer a fresh perspective on MST/MSF
topologies by formulating them through a set of continuous constraint
representations (CCRs), thereby devising graph-theoretical abstractions into an
optimizable framework for satellite networks. Using these CCRs, we obtain the
energy consumption and latency of operations in Fed-Span. Moreover, we derive
novel convergence bounds for non-convex machine learning loss functions,
accommodating the key system characteristics and degrees of freedom of
Fed-Span. Finally, we propose a comprehensive optimization problem that jointly
minimizes model prediction loss, energy consumption, and latency of Fed-Span.
We unveil that this problem is NP-hard and develop a systematic approach to
transform it into a geometric programming formulation, solved via successive
convex optimization with performance guarantees. Through evaluations on
real-world datasets, we demonstrate that Fed-Span outperforms existing methods,
with faster model convergence, greater energy efficiency, and reduced latency.
These results highlight Fed-Span as a novel solution for efficient distributed
learning in satellite networks.

</details>


### [163] [GRACE-MoE: Grouping and Replication with Locality-Aware Routing for Efficient Distributed MoE Inference](https://arxiv.org/abs/2509.25041)
*Yu Han,Lehan Pan,Jie Peng,Ziyang Tao,Wuyang Zhang,Yanyong Zhang*

Main category: cs.DC

TL;DR: SMoE推理存在通信和计算负载问题，提出GRACE - MoE框架，实验显示可有效降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 解决SMoE分布式推理中通信开销大与计算负载不平衡的问题。

Method: 提出GRACE - MoE框架，含分组与复制、路由两个关键阶段。分组与复制阶段分组专家减少通信并动态复制解决负载倾斜；路由阶段采用带负载预测的局部感知路由策略。

Result: 在多样模型和多节点多GPU环境实验中，GRACE - MoE有效降低端到端推理延迟，比现有系统最多加速3.79倍。

Conclusion: GRACE - MoE能联合减少通信开销和缓解计算负载不平衡，提升推理效率。

Abstract: Sparse Mixture of Experts (SMoE) performs conditional computation by
selectively activating a subset of experts, thereby enabling scalable parameter
growth in large language models (LLMs). However, the expanded parameter scale
exceeds the memory capacity of a single device, necessitating distributed
deployment for inference. This setup introduces two critical challenges: (1)
Communication Issue: Transferring features to devices with activated experts
leads to significant communication overhead. (2) Computational Load Issue:
Skewed expert activation overloads certain GPUs, resulting in load imbalance
across devices. Among these, communication overhead is identified as the main
bottleneck in SMoE inference. Nevertheless, reducing communication between
devices may exacerbate computational load imbalance, leading to device idleness
and resource waste. Therefore, we present GRACE-MoE, short for Grouping and
Replication with Locality-Aware Routing for SMoE inference. GRACE-MoE is a
co-optimization framework that jointly reduces communication overhead and
alleviates computational load imbalance. Specifically, the framework comprises
two key phases: (1) Grouping & Replication: This phase groups experts based on
their affinity to reduce cross-device communication. Additionally, dynamic
replication is applied to address load skew, improving computational load
balance across GPUs. (2) Routing: This phase employs a locality-aware routing
strategy with load prediction. It prioritizes local replicas to minimize
communication overhead and balances requests across remote replicas when
necessary. Experiments on diverse models and multi-node, multi-GPU environments
demonstrate that GRACE-MoE efficiently reduces end-to-end inference latency,
achieving up to 3.79x speedup over state-of-the-art systems. Code for GRACE-MoE
will be released upon acceptance.

</details>


### [164] [Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs](https://arxiv.org/abs/2509.25121)
*Anvitha Ramachandran,Dhruv Parikh,Viktor Prasanna*

Main category: cs.DC

TL;DR: 提出用于DIGC的FPGA加速器，实现跨分辨率等的高效计算，加速比显著。


<details>
  <summary>Details</summary>
Motivation: DIGC是ViG推理延迟的主要瓶颈，现有硬件加速工作有局限性。

Method: 设计流式、深度流水线的FPGA加速器，有片上缓冲区，采用局部计算、本地归并排序和全局k路合并。

Result: 设计能跨图像分辨率等无缝扩展，支持多种ViG骨干网络，相比CPU和GPU有显著加速。

Conclusion: 所提加速器可有效解决DIGC计算瓶颈问题，提升计算效率。

Abstract: Vision Graph Neural Networks (Vision GNNs, or ViGs) represent images as
unstructured graphs, achieving state of the art performance in computer vision
tasks such as image classification, object detection, and instance
segmentation. Dynamic Image Graph Construction (DIGC) builds image graphs by
connecting patches (nodes) based on feature similarity, and is dynamically
repeated in each ViG layer following GNN based patch (node) feature updates.
However, DIGC constitutes over 50% of end to end ViG inference latency, rising
to 95% at high image resolutions, making it the dominant computational
bottleneck. While hardware acceleration holds promise, prior works primarily
optimize graph construction algorithmically, often compromising DIGC
flexibility, accuracy, or generality. To address these limitations, we propose
a streaming, deeply pipelined FPGA accelerator for DIGC, featuring on chip
buffers that process input features in small, uniform blocks. Our design
minimizes external memory traffic via localized computation and performs
efficient parallel sorting with local merge sort and global k way merging
directly on streaming input blocks via heap insertion. This modular
architecture scales seamlessly across image resolutions, ViG layer types, and
model sizes and variants, and supports DIGC across diverse ViG based vision
backbones. The design achieves high clock frequencies post place and route due
to the statically configured parallelism minimizing critical path delay and
delivers up to 16.6x and 6.8x speedups over optimized CPU and GPU DIGC
baselines.

</details>


### [165] [Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units](https://arxiv.org/abs/2509.25155)
*Neelesh Gupta,Rakshith Jayanth,Dhruv Parikh,Viktor Prasanna*

Main category: cs.DC

TL;DR: 对现代NPU上的因果推理算子进行性能分析，对比标准二次注意力和次二次替代方案，发现不同瓶颈，为硬件感知模型协同设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在资源受限边缘设备上长上下文推理需求增长，但在NPU部署因架构不匹配面临挑战。

Method: 对现代NPU上多种因果推理算子进行性能分析，对比标准二次注意力和几种次二次替代方案。

Result: 次二次方法扩展性好，但在NPU专用执行单元有计算瓶颈；二次注意力严重受内存限制，长上下文时缓存效率低、管道停顿超95%；次二次模型在可编程向量核上可能受计算限制。

Conclusion: 研究结果为硬件感知模型和优化策略的协同设计提供关键见解，以实现设备上的长上下文AI推理。

Abstract: The proliferation of large language models (LLMs) has driven demand for long
context inference on resource constrained edge devices. However, deploying
these models on Neural Processing Units (NPUs) presents significant challenges
due to the architectural mismatch: quadratic complexity of standard attention
mechanisms conflicts with memory and compute patterns of edge accelerators.
This paper presents a comprehensive performance analysis of various causal
inference operators on a modern NPU. We benchmark standard quadratic attention
against several sub-quadratic alternatives, including structured state-space
and linear attention models. Our analysis reveals that while sub-quadratic
methods offer superior scalability, they introduce distinct computational
bottlenecks on the NPU's specialized execution units. We identify that
quadratic attention becomes severely memory-bound, suffering from cache
inefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,
sub-quadratic models can become compute-bound on programmable vector cores.
These findings provide critical insights for the co-design of hardware-aware
models and optimization strategies to enable on-device AI inference with
long-contexts.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [166] [Computing k-mers in Graphs](https://arxiv.org/abs/2509.22885)
*Jarno N. Alanko,Maximo Perez-Lopez*

Main category: cs.DS

TL;DR: 研究有标签图上k - 元组计算问题，证明计数问题#P - 难，给出Wheeler图计数方法及应用。


<details>
  <summary>Details</summary>
Motivation: 开展有标签图上k - 元组计算问题的研究，以计数图中不同k - 元组数量问题为起点。

Method: 证明计数问题复杂度，对Wheeler图使用前缀加倍技术的推广方法，讨论图转换为Wheeler图的方式。

Result: 计数问题是#P - 难；在Wheeler图中可使用特定算术运算计数；可在特定时间构造de Bruijn图表示。

Conclusion: 提出的k - 元组计数算法对de Bruijn图构造方法有理论改进。

Abstract: We initiate the study of computational problems on $k$-mers (strings of
length $k$) in labeled graphs. As a starting point, we consider the problem of
counting the number of distinct $k$-mers found on the walks of a graph. We
establish that this is #P-hard, even on connected deterministic DAGs. However,
in the class of deterministic Wheeler graphs (Gagie, Manzini, and Siren, TCS
2017), we show that distinct $k$-mers of such a graph $W$ can be counted using
$O(|W|k)$ or $O(n^4 \log k)$ arithmetic operations, where $n$ is the number of
vertices of the graph, and $|W|$ is $n$ plus the number of edges. The latter
result uses a new generalization of the technique of prefix doubling to Wheeler
graphs. To generalize our results beyond Wheeler graphs, we discuss ways to
transform a graph into a Wheeler graph in a manner that preserves the $k$-mers.
  As an application of our $k$-mer counting algorithms, we construct a
representation of the de Bruijn graph (dBg) of the $k$-mers in time $O(|dBg| +
|W|k)$. Given that the Wheeler graph can be exponentially smaller than the de
Bruijn graph, for large $k$ this provides a theoretical improvement over
previous de Bruijn graph construction methods from graphs, which must spend
$\Omega(k)$ time per $k$-mer in the graph. Our representation occupies $O(|dBg|
+ |W|k \log(\max_{1 \leq \ell \leq k}(n_\ell)))$ bits of space, where $n_\ell$
is the number of distinct $l$-mers in the Wheeler graph.

</details>


### [167] [Sparse Graph Reconstruction and Seriation for Large-Scale Image Stacks](https://arxiv.org/abs/2509.23084)
*Fuming Yang,Yaron Meirovitch,Jeff W. Lichtman*

Main category: cs.DS

TL;DR: 研究在严格查询预算下从有噪声的局部采样成对比较矩阵中恢复一维顺序，提出 SuperChain 方法，理论证明可精确恢复，实验表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在严格查询预算下，从有噪声的局部采样成对比较矩阵中恢复一维顺序。

Method: 将任务重铸为重建稀疏有噪声线图，采用五阶段流水线方法，包括随机挂钩 Boruvka 步骤、迭代凝聚、双扫 BFS、固定窗口致密化和贪婪 SuperChain 组装最终排列。

Result: 在简单 top - 1 边际和有界相对噪声下可精确恢复，实验中约 2N/3 真实邻接存在时仍能成功，在晶圆级连续切片 EM 中优于谱、MST 和 TSP 基线方法。

Conclusion: 该方法适用于其他局部结构测序任务，如时间快照排序、考古序列化和播放列表/旅游路线构建。

Abstract: We study recovering a 1D order from a noisy, locally sampled pairwise
comparison matrix under a tight query budget. We recast the task as
reconstructing a sparse, noisy line graph and present, to our knowledge, the
first method that provably builds a sparse graph containing all edges needed
for exact seriation using only O(N(log N + K)) oracle queries, which is
near-linear in N for fixed window K. The approach is parallelizable and
supports both binary and bounded-noise distance oracles. Our five-stage
pipeline consists of: (i) a random-hook Boruvka step to connect components via
short-range edges in O(N log N) queries; (ii) iterative condensation to bound
graph diameter; (iii) a double-sweep BFS to obtain a provisional global order;
(iv) fixed-window densification around that order; and (v) a greedy SuperChain
that assembles the final permutation. Under a simple top-1 margin and bounded
relative noise we prove exact recovery; empirically, SuperChain still succeeds
when only about 2N/3 of true adjacencies are present. On wafer-scale
serial-section EM, our method outperforms spectral, MST, and TSP baselines with
far fewer comparisons, and is applicable to other locally structured sequencing
tasks such as temporal snapshot ordering, archaeological seriation, and
playlist/tour construction.

</details>


### [168] [Finding the diameter of a tree with distance queries](https://arxiv.org/abs/2509.23326)
*Dániel Gerbner,András Imolay,Kartal Nagy,Balázs Patkós,Kristóf Zólomy*

Main category: cs.DS

TL;DR: 研究确定隐藏树某些属性所需的距离查询次数，包括自适应和非自适应情况。


<details>
  <summary>Details</summary>
Motivation: 探究确定隐藏树特定属性所需的距离查询次数。

Method: 分别研究自适应算法和非自适应算法下的情况。

Result: 确定了最优自适应算法找到最大距离两顶点所需查询次数（至加法常数）和渐近识别隐藏树所需查询次数，精确确定非自适应问题所需查询次数。

Conclusion: 明确了不同算法下确定隐藏树某些属性的距离查询次数。

Abstract: We study the number of distance queries needed to identify certain properties
of a hidden tree $T$ on $n$ vertices. A distance query consists of two vertices
$x,y$, and the answer is the distance of $x$ and $y$ in $T$. We determine the
number of queries an optimal adaptive algorithm needs to find two vertices of
maximal distance up to an additive constant, and the number of queries needed
to identify the hidden tree asymptotically. We also study the non-adaptive
versions of these problems, determining the number of queries needed exactly.

</details>


### [169] [Maximal Covering Location Problem: A Set Coverage Approach Using Dynamic Programming](https://arxiv.org/abs/2509.23334)
*Sukanya Samanta,Abhi Rohit Kalathoti,Siva Jayanth Gonchi,Venkata Krishna Kashyap Adiraju,Sai Kiran Nettem*

Main category: cs.DS

TL;DR: 本文用集合覆盖法和0/1背包动态规划分析最大覆盖选址问题，为设施选址优化提供理论和算法方案。


<details>
  <summary>Details</summary>
Motivation: 解决最大覆盖选址问题中在资源约束下最大化需求覆盖的优化挑战。

Method: 采用集合覆盖法，通过0/1背包动态规划实现。

Result: 提出了设施战略布局方法，以在指定服务距离内实现需求点的最优覆盖。

Conclusion: 为设施选址优化提供了理论基础和适用于城市规划、应急服务和供应链管理等领域的实用算法解决方案。

Abstract: The Maximal Covering Location Problem (MCLP) represents a fundamental
optimization challenge in facility location theory, where the objective is to
maximize demand coverage while operating under resource constraints. This paper
presents a comprehensive analysis of MCLP using a set coverage methodology
implemented through 0/1 knapsack dynamic programming. Our approach addresses
the strategic placement of facilities to achieve optimal coverage of demand
points within specified service distances. This research contributes to the
understanding of facility location optimization by providing both theoretical
foundations and practical algorithmic solutions for real-world applications in
urban planning, emergency services, and supply chain management.

</details>


### [170] [Stochastic Embedding of Digraphs into DAGs](https://arxiv.org/abs/2509.23458)
*Arnold Filtser*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given a weighted digraph $G=(V,E,w)$, a stochastic embedding into DAGs is a
distribution $\mathcal{D}$ over pairs of DAGs $(D_1,D_2)$ such that for every
$u,v$: (1) the reachability is preserved: $u\rightsquigarrow_G v$ (i.e., $v$ is
reachable from $u$ in $G$) implies that $u\rightsquigarrow_{D_1} v$ or
$u\rightsquigarrow_{D_2} v$ (but not both), and (2) distances are dominated:
$d_G(u,v)\le\min\{d_{D_1}(u,v),d_{D_2}(u,v)\}$. The stochastic embedding
$\mathcal{D}$ has expected distortion $t$ if for every $u,v\in V$, \[
\mathbb{E}_{(D_{1},D_{2})\sim\mathcal{D}}\left[d_{D_{1}}(u,v)\cdot\boldsymbol{1}[u\rightsquigarrow_{D_{1}}v]+d_{D_{2}}(u,v)\cdot\boldsymbol{1}[u\rightsquigarrow_{D_{2}}v]\right]\le
t\cdot d_{G}(u,v)~. \] Finally, the sparsity of $\mathcal{D}$ is the maximum
number of edges in any of the DAGs in its support. Given an $n$ vertex digraph
with $m$ edges, we construct a stochastic embedding into DAGs with expected
distortion $\tilde{O}(\log n)$ and $\tilde{O}(m)$ sparsity, improving a
previous result by Assadi, Hoppenworth, and Wein [STOC 25], which achieved
expected distortion $\tilde{O}(\log^3 n)$. Further, we can sample DAGs from
this distribution in $\tilde{O}(m)$ time.

</details>


### [171] [A Near-Real-Time Reduction-Based Algorithm for Coloring Massive Graphs](https://arxiv.org/abs/2509.23606)
*Chenghao Zhu,Yi Zhou*

Main category: cs.DS

TL;DR: 本文针对大规模图着色问题，应用多种约简规则提出RECOL算法，实验表明其在大规模稀疏图上一分钟内优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 图着色问题虽研究已久，但能在严格时间内为大规模真实图提供高质量解的近实时算法研究不足，本文旨在填补此空白。

Method: 系统研究能在保持最优性的同时缩小问题规模的约简规则，将支配约简、补冠约简和独立集约简应用于大规模实例，提出基于约简的RECOL算法，交替进行上下界快速估计、图约简和启发式着色。

Result: 在多种基准数据集上评估，RECOL在大规模稀疏图上一分钟内始终优于现有算法，额外实验凸显约简技术对性能的关键作用。

Conclusion: RECOL算法在大规模稀疏图的图着色问题上表现良好，约简技术对提升性能至关重要。

Abstract: The graph coloring problem is a classical combinatorial optimization problem
with important applications such as register allocation and task scheduling,
and it has been extensively studied for decades. However, near-real-time
algorithms that can deliver high-quality solutions for very large real-world
graphs within a strict time frame remain relatively underexplored. In this
paper, we try to bridge this gap by systematically investigating reduction
rules that shrink the problem size while preserving optimality. For the first
time, domination reduction, complement crown reduction, and independent set
reduction are applied to large-scale instances. Building on these techniques,
we propose RECOL, a reduction-based algorithm that alternates between fast
estimation of lower and upper bounds, graph reductions, and heuristic coloring.
We evaluate RECOL on a wide range of benchmark datasets, including SNAP, the
Network Repository, DIMACS10, and DIMACS2. Experimental results show that RECOL
consistently outperforms state-of-the-art algorithms on very large sparse
graphs within one minute. Additional experiments further highlight the pivotal
role of reduction techniques in achieving this performance.

</details>


### [172] [Systematic Alias Sampling: an efficient and low-variance way to sample from a discrete distribution](https://arxiv.org/abs/2509.24089)
*Ilari Vallivaara,Katja Poikselkä,Pauli Rikula,Juha Röning*

Main category: cs.DS

TL;DR: 结合Alias方法与系统抽样概念实现离散分布快速采样，效果优于多项抽样，可用于粒子滤波建议分布。


<details>
  <summary>Details</summary>
Motivation: 实现离散分布的高效低方差重采样，为粒子滤波建议分布高效生成随机样本。

Method: 将Alias方法与系统抽样概念结合。

Result: 从离散分布抽样速度比许多库中的二分查找或反演方法快一个数量级，经修正的Cramér - Von Mises拟合优度统计评估表明优于多项抽样。

Conclusion: 该方法可作为一种通用方式，为粒子滤波建议分布高效生成随机样本，如用于机器人运动模型。

Abstract: In this paper we combine the Alias method with the concept of systematic
sampling, a method commonly used in particle filters for efficient low-variance
resampling. The proposed method allows very fast sampling from a discrete
distribution: drawing k samples is up to an order of magnitude faster than
binary search from the cumulative distribution function (cdf) or inversion
methods used in many libraries. The produced empirical distribution function is
evaluated using a modified Cram\'er-Von Mises goodness-of-fit statistic,
showing that the method compares very favourably to multinomial sampling. As
continuous distributions can often be approximated with discrete ones, the
proposed method can be used as a very general way to efficiently produce random
samples for particle filter proposal distributions, e.g. for motion models in
robotics.

</details>


### [173] [The Role of Commitment in Optimal Stopping](https://arxiv.org/abs/2509.24132)
*José Correa,Evangelia Gergatsouli,Bruno Ziliotto*

Main category: cs.DS

TL;DR: 研究Prophet Inequality (PI)和Pandora's Box (PB)之间所有变体中承诺的作用，总结已知结果并填补研究空白，还揭示与滑雪租赁问题的联系。


<details>
  <summary>Details</summary>
Motivation: 探究承诺在最优停止问题中的作用，通过研究PI和PB之间的变体来深入了解。

Method: 针对PI和PB问题，改变承诺、观察成本、顺序选择等参数形成不同变体，总结已知结果并填补未研究变体的空白。

Result: 总结了PI和PB不同变体的已知结果，填补了部分未研究变体的空白，还发现与滑雪租赁问题的联系。

Conclusion: 通过研究不同变体，能更深入理解承诺在最优停止问题中的作用，且与经典在线算法问题有联系。

Abstract: We investigate the role of commitment in optimal stopping by studying all the
variants between Prophet Inequality (PI) and Pandora's Box (PB). Both problems
deal with a set of variables drawn from known distributions.
  In PI the gambler observes an adversarial order of these variables with the
goal of selecting one that maximizes the expected value against a prophet who
knows the exact values realized. The gambler has to irrevocably decide at each
step whether to select the value or discard it (commitment).
  On the other hand, in PB the gambler selects the order of inspecting the
variables and for each pays an observation cost to see the actual value
realized, aiming to choose one to maximize the net cost of the value chosen
minus the observation cost paid. The gambler in PB can return and select any
variable already seen (no commitment).
  For all the variants between these problems that arise by changing parameters
such as (1) commitment (2) observation cost (3) order selection, we concisely
summarize the known results and fill the gaps of variants not yet studied. We
also uncover connections to Ski-Rental, a classic online algorithm problem.

</details>


### [174] [Optimally revealing bits for rejection sampling](https://arxiv.org/abs/2509.24290)
*Louis-Roy Langevin,Alex Waese-Perlman*

Main category: cs.DS

TL;DR: 研究用拒绝采样法从递增概率密度函数生成单位区间随机数，聚焦n个相关随机变量采样问题，得出最坏情况下接受或拒绝样本所需随机比特期望数与n的关系。


<details>
  <summary>Details</summary>
Motivation: 研究用拒绝采样法从递增概率密度函数在单位区间生成随机数，解决n个相关随机变量从联合分布采样的问题。

Method: 使用拒绝采样法进行研究。

Result: 在最坏情况下，接受或拒绝样本所需随机比特的期望数至少随n线性增长，至多随n二次增长。

Conclusion: 明确了拒绝采样法在特定情况下接受或拒绝样本所需随机比特期望数与n的增长关系。

Abstract: Rejection sampling is a popular method used to generate numbers that follow
some given distribution. We study the use of this method to generate random
numbers in the unit interval from increasing probability density functions. We
focus on the problem of sampling from $n$ correlated random variables from a
joint distribution whose marginal distributions are all increasing. We show
that, in the worst case, the expected number of random bits required to accept
or reject a sample grows at least linearly and at most quadratically with $n$.

</details>


### [175] [Forcing a unique minimum spanning tree and a unique shortest path](https://arxiv.org/abs/2509.24309)
*Tatsuya Gima,Yasuaki Kobayashi,Yota Otachi,Takumi Sato*

Main category: cs.DS

TL;DR: 本文研究最短s - t路径问题和最小权重生成树问题的最小强制集和最小反强制集问题的复杂度，发现除最短s - t路径的最小反强制集问题为NP - 难外，其他问题均可解。


<details>
  <summary>Details</summary>
Motivation: 已有研究表明很多组合问题中寻找最小强制集的计算复杂度比经典问题更难，本文旨在研究最短s - t路径问题和最小权重生成树问题中寻找最小强制集和最小反强制集问题的复杂度。

Method: 对最短s - t路径问题和最小权重生成树问题进行分析研究。

Result: 除最短s - t路径的最小反强制集问题是NP - 难外，其他问题均可解。

Conclusion: 最短s - t路径问题和最小权重生成树问题的最小强制集和最小反强制集问题的复杂度情况与之前研究的其他组合问题不同。

Abstract: A forcing set $S$ in a combinatorial problem is a set of elements such that
there is a unique solution that contains all the elements in $S$. An
anti-forcing set is the symmetric concept: a set $S$ of elements is called an
anti-forcing set if there is a unique solution disjoint from $S$. There are
extensive studies on the computational complexity of finding a minimum forcing
set in various combinatorial problems, and the known results indicate that many
problems would be harder than their classical counterparts: finding a minimum
forcing set for perfect matchings is NP-hard [Adams et al., Discret. Math.
2004] and finding a minimum forcing set for satisfying assignments for 3CNF
formulas is $\mathrm{\Sigma}_2^P$-hard [Hatami-Maserrat, DAM 2005]. In this
paper, we investigate the complexity of the problems of finding minimum forcing
and anti-forcing sets for the shortest $s$-$t$ path problem and the minimum
weight spanning tree problem. We show that, unlike the aforementioned results,
these problems are tractable, with the exception of finding a minimum
anti-forcing set for shortest $s$-$t$ paths, which is NP-hard.

</details>


### [176] [Simple in-place yet comparison-optimal Mergesort](https://arxiv.org/abs/2509.24540)
*Christian Siebert*

Main category: cs.DS

TL;DR: 本文提出一种新的原地归并排序算法，虽时间复杂度非最优，但理论和实践上效率高、稳定且易理解，还给出实现和性能结果。


<details>
  <summary>Details</summary>
Motivation: 传统归并排序需额外数组，处理大量数据时内存需求大，以往原地归并排序算法存在效率低、不稳定或复杂等问题，故提出新算法。

Method: 基于最优数组旋转和2014年提出的协同排序思想设计新的原地归并排序算法，关注其顺序执行方面。

Result: 实现了新算法，并在大型共享内存系统上给出性能结果。

Conclusion: 新的原地归并排序算法虽时间复杂度非最优，但理论和实践中效率高、稳定且易理解，适用于实际应用。

Abstract: Mergesort is one of the few efficient sorting algorithms and, despite being
the oldest one, often still the method of choice today. In contrast to some
alternative algorithms, it always runs efficiently using O(n log n) element
comparisons and usually works in a stable manner. Its only practical
disadvantage is the need for a second array, and thus twice the amount of
memory. This can be an impeding factor, especially when handling large amounts
of data, where it is often either impractical or even impossible to fall back
to slower or unstable sorting alternatives. Therefore, many attempts have been
made to fix this problem by adapting Mergesort to work in place. While it is
known that such algorithms exist, the previously published solutions are mostly
not efficient, become unstable, and/or are very complex. This renders them
practically useless for real-world applications. In this paper, we propose a
novel in-place Mergesort algorithm that is stable by design. Albeit its running
time of O(n log^2 n) is not quite optimal, it still works efficiently, both in
theory using the optimal number of O(n log n) comparisons and in practice with
low constants, while being easily comprehensible. The baseline for this new
algorithm includes just two prerequisites: 1) an optimal array rotation; and 2)
the co-ranking idea, published in 2014 and originally intended to parallelize
Mergesort. Although it would certainly be possible to parallelize the presented
algorithm, this paper focuses on the sequential aspect of this efficient,
stable and in-place Mergesort algorithm. Additionally, we implemented our
algorithm and present performance results measured on one of the largest shared
memory systems currently available.

</details>


### [177] [Stronger Directed Low-Diameter Decompositions with Sub-Logarithmic Diameter and Separation](https://arxiv.org/abs/2509.24565)
*Bernhard Haeupler,Richard Hladík,Shengzhe Wang,Zhijun Zhang*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper significantly strengthens directed low-diameter decompositions in
several ways.
  We define and give the first results for separated low-diameter
decompositions in directed graphs, tighten and generalize probabilistic
guarantees, and prove new independence results between (far away) edges. Our
results are the first to give meaningful guarantees for decompositions with
small diameters $D = \Omega(\log\log n)$ in contrast to the state of the art
that only applies to super-logarithmic diameters $D = \omega(\log n)$.
  These results transfer several important and widely used aspects of
undirected low-diameter decompositions to the directed setting. All our results
are algorithmic -- small modifications to two existing directed low-diameter
decompositions [Bri+25; Li25] can be used to sample decompositions with our new
guarantees in near-linear time $\tilde{O}(m)$.

</details>


### [178] [Algorithms and data structures for automatic precision estimation of neural networks](https://arxiv.org/abs/2509.24607)
*Igor V. Netay*

Main category: cs.DS

TL;DR: 本文介绍为神经网络库添加浮点计算自动精度估计的算法和数据结构，实验表明跟踪计算误差对神经网络很重要。


<details>
  <summary>Details</summary>
Motivation: 提升神经网络训练和推理的可靠性、结果可解释性，解决计算精度损失问题。

Method: 提出为神经网络库添加浮点计算自动精度估计的算法和数据结构，并探讨使估计精确且保持高性能的条件。

Result: 数值实验显示计算精度损失对推理、梯度等有影响，几乎所有神经网络都会积累计算误差，实际行为与数学模型预测不符。

Conclusion: 跟踪计算误差对神经网络的推理、训练和结果可解释性的可靠性很重要。

Abstract: We describe algorithms and data structures to extend a neural network library
with automatic precision estimation for floating point computations. We also
discuss conditions to make estimations exact and preserve high computation
performance of neural networks training and inference. Numerical experiments
show the consequences of significant precision loss for particular values such
as inference, gradients and deviations from mathematically predicted behavior.
  It turns out that almost any neural network accumulates computational
inaccuracies. As a result, its behavior does not coincide with predicted by the
mathematical model of neural network. This shows that tracking of computational
inaccuracies is important for reliability of inference, training and
interpretability of results.

</details>


### [179] [Efficient Sketching and Nearest Neighbor Search Algorithms for Sparse Vector Sets](https://arxiv.org/abs/2509.24815)
*Sebastian Bruch,Franco Maria Nardini,Cosimo Rulli,Rossano Venturini*

Main category: cs.DS

TL;DR: 本文提出新颖数据结构和算法方法解决稀疏近似最近邻搜索问题，最终算法Seismic在单CPU上对大规模基准数据集实现高精度、亚毫秒级查询延迟。


<details>
  <summary>Details</summary>
Motivation: 稀疏嵌入数据虽有可解释性，但给近似最近邻搜索（ANNS）带来挑战，且该领域研究不足，NeurIPS 2023的BigANN挑战赛推动相关研究。

Method: 提出一套新颖数据结构和算法方法，包括理论支撑的稀疏向量草图算法、倒排索引的几何组织、融合局部和全局信息。

Result: 最终算法Seismic在大规模基准数据集上，使用单CPU达到亚毫秒级每查询延迟且有高准确性。

Conclusion: 所提出的方法为稀疏ANNS提供了优雅、有效且高效的解决方案。

Abstract: Sparse embeddings of data form an attractive class due to their inherent
interpretability: Every dimension is tied to a term in some vocabulary, making
it easy to visually decipher the latent space. Sparsity, however, poses unique
challenges for Approximate Nearest Neighbor Search (ANNS) which finds, from a
collection of vectors, the k vectors closest to a query. To encourage research
on this underexplored topic, sparse ANNS featured prominently in a BigANN
Challenge at NeurIPS 2023, where approximate algorithms were evaluated on large
benchmark datasets by throughput and accuracy. In this work, we introduce a set
of novel data structures and algorithmic methods, a combination of which leads
to an elegant, effective, and highly efficient solution to sparse ANNS. Our
contributions range from a theoretically-grounded sketching algorithm for
sparse vectors to reduce their effective dimensionality while preserving inner
product-induced ranks; a geometric organization of the inverted index; and the
blending of local and global information to improve the efficiency and efficacy
of ANNS. Empirically, our final algorithm, dubbed Seismic, reaches
sub-millisecond per-query latency with high accuracy on a large-scale benchmark
dataset using a single CPU.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [180] [Grouped Satisficing Paths in Pure Strategy Games: a Topological Perspective](https://arxiv.org/abs/2509.23157)
*Yanqing Fu,Chao Huang,Chenrun Wang,Zhuping Wang*

Main category: cs.GT

TL;DR: 本文研究博弈论和多智能体强化学习中“赢则留，输则换”原则下，初始联合策略到均衡的有限长度满意路径存在条件，证明有限状态马尔可夫博弈和N人博弈中该路径存在，为算法设计提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 探究在“赢则留，输则换”原则下，何种条件能使每个初始联合策略都有从初始策略到均衡的有限长度满意路径。

Method: 理论推导，证明有限状态马尔可夫博弈和N人博弈中有限长度满意路径的存在性。

Result: 证明了有限状态马尔可夫博弈和N人博弈能保证从任意初始策略到某个均衡存在有限长度满意路径。

Conclusion: 研究结果为多智能体强化学习算法设计提供了更坚实的理论基础。

Abstract: In game theory and multi-agent reinforcement learning (MARL), each agent
selects a strategy, interacts with the environment and other agents, and
subsequently updates its strategy based on the received payoff. This process
generates a sequence of joint strategies $(s^t)_{t \geq 0}$, where $s^t$
represents the strategy profile of all agents at time step $t$. A widely
adopted principle in MARL algorithms is "win-stay, lose-shift", which dictates
that an agent retains its current strategy if it achieves the best response.
This principle exhibits a fixed-point property when the joint strategy has
become an equilibrium. The sequence of joint strategies under this principle is
referred to as a satisficing path, a concept first introduced in [40] and
explored in the context of $N$-player games in [39]. A fundamental question
arises regarding this principle: Under what conditions does every initial joint
strategy $s$ admit a finite-length satisficing path $(s^t)_{0 \leq t \leq T}$
where $s^0=s$ and $s^T$ is an equilibrium? This paper establishes a sufficient
condition for such a property, and demonstrates that any finite-state Markov
game, as well as any $N$-player game, guarantees the existence of a
finite-length satisficing path from an arbitrary initial strategy to some
equilibrium. These results provide a stronger theoretical foundation for the
design of MARL algorithms.

</details>


### [181] [Beyond Game Theory Optimal: Profit-Maximizing Poker Agents for No-Limit Holdem](https://arxiv.org/abs/2509.23747)
*SeungHyun Yi,Seungjun Yi*

Main category: cs.GT

TL;DR: 本文旨在开发在无限注德州扑克中超越GTO策略以实现利润最大化的模型，先建立接近理论最优的基线，再根据对手行为调整策略，结果表明蒙特卡罗反事实遗憾最小化在不同情况表现良好。


<details>
  <summary>Details</summary>
Motivation: 纯GTO策略不能保证扑克最大利润，因此要开发能超越GTO策略以实现利润最大化的模型。

Method: 先让模型与自身模拟大量手牌对局，不断调整决策形成接近理论最优策略的基线，再观察对手行为调整策略。

Result: 蒙特卡罗反事实遗憾最小化（CFR）在单挑情况表现最佳，在多数多人情况仍是最强方法。

Conclusion: 结合GTO防御优势和实时剥削对手，可让扑克智能体从避免输牌变为稳定战胜不同对手。

Abstract: Game theory has grown into a major field over the past few decades, and poker
has long served as one of its key case studies. Game-Theory-Optimal (GTO)
provides strategies to avoid loss in poker, but pure GTO does not guarantee
maximum profit. To this end, we aim to develop a model that outperforms GTO
strategies to maximize profit in No Limit Holdem, in heads-up (two-player) and
multi-way (more than two-player) situations. Our model finds the GTO foundation
and goes further to exploit opponents. The model first navigates toward many
simulated poker hands against itself and keeps adjusting its decisions until no
action can reliably beat it, creating a strong baseline that is close to the
theoretical best strategy. Then, it adapts by observing opponent behavior and
adjusting its strategy to capture extra value accordingly. Our results indicate
that Monte-Carlo Counterfactual Regret Minimization (CFR) performs best in
heads-up situations and CFR remains the strongest method in most multi-way
situations. By combining the defensive strength of GTO with real-time
exploitation, our approach aims to show how poker agents can move from merely
not losing to consistently winning against diverse opponents.

</details>


### [182] [Evolutionary hypergame dynamics: Introspection reasoning and social learning](https://arxiv.org/abs/2509.24398)
*Feipeng Zhang,Te Wu,Guofeng Zhang,Long Wang*

Main category: cs.GT

TL;DR: 本文研究进化超博弈动态中策略集的学习和进化，发现与传统进化博弈不同的复杂阶段，还发现更高理性促进合作。


<details>
  <summary>Details</summary>
Motivation: 标准进化博弈理论假设玩家拥有全面知识和策略空间访问权，而现实中个体存在差异，超博弈虽考虑了异质性但其进化后果未充分研究。

Method: 采用具有三种可用策略的原型模型，研究合作、背叛和独行三种策略在社会困境中的情况，研究涵盖混合和空间晶格种群。

Result: 发现与传统进化博弈动态不同的复杂阶段，如独行主导、多策略集共存、合作与独行主导组合等。

Conclusion: 更高的理性显著促进合作行为。

Abstract: In the realm of evolutionary game theory, standard frameworks typically
presuppose that every player possesses comprehensive knowledge and unrestricted
access to the entire strategy space. However, real-world human society
inherently harbors diverse levels of knowledge, experience, and background
among individuals. Hypergames incorporate this heterogeneity by permitting
individuals to differ in their access to the full strategy set, reflecting
cognitive or informational constraints and giving rise to asymmetric strategic
interactions. Yet, their evolutionary consequences remain underexplored. Our
inquiry employs prototype models featuring three available strategies, focusing
on social dilemmas involving cooperation, defection, and loner. These
strategies manifest cyclic dominance, akin to the well-studied
rock-paper-scissors dynamics, a foundational model in game theory. Our study
spans both well-mixed and spatial lattice populations, delving into the
intricacies of learning and evolution of the strategy set within the
evolutionary hypergame dynamics. In stark contrast to traditional evolutionary
game dynamics, our findings unveil nuanced and intricate phases, encompassing
scenarios of loner dominance, coexistence of multiple strategy sets,
combinations of cooperation and loner dominance, and more. Remarkably, we
discern that heightened rationality significantly promotes cooperative
behaviors.

</details>


### [183] [Dynamic Pricing of an Expiring Item under Strategic Buyers with Stochastic Arrival](https://arxiv.org/abs/2509.24720)
*Suyeon Choi,Changhyun Kwon,Seungki Min*

Main category: cs.GT

TL;DR: 研究时间敏感卖家向策略型买家出售过期票券或代金券的最优动态定价，提出VBT策略，推导不同市场的近优定价策略并分析其效果。


<details>
  <summary>Details</summary>
Motivation: 解决过期票券或代金券销售中卖家急于出售降价与买家等待之间的矛盾，寻找卖家的最优定价策略。

Method: 引入基于价值的阈值（VBT）策略，通过常微分方程证明均衡存在性并给出构造性表征方法。

Result: 推导出薄市场的恒定价格和厚市场的线性折扣两种近优定价策略，数值分析验证了这些基准。

Conclusion: 厚市场或时间敏感度高的卖家适合线性折扣，薄市场适合恒定价格，耐心卖家采用准拍卖定价有效，简单策略在广泛条件下稳健。

Abstract: We study the optimal dynamic pricing of an expiring ticket or voucher, sold
by a time-sensitive seller to strategic buyers who arrive stochastically with
private values. The expiring nature creates a conflict: the seller's urgency to
sell before expiration drives price reductions, which in turn incentivize
buyers to wait. We seek the seller's optimal pricing policy that resolves this
tension. The main analytical challenge is that buyer type is two-dimensional
(valuation and arrival time), which makes equilibrium intractable under general
strategies. To address this, we introduce the Value-Based Threshold (VBT)
strategy, a tractable framework that decouples these two dimensions. Using this
framework, we prove equilibrium existence via an ordinary differential equation
and provide a constructive procedure for its characterization. We then derive
near-optimal pricing policies for two stylized regimes: a constant price in
thin markets and a linear discount in thick markets. Numerical frontier
analysis confirms these benchmarks and shows how optimal policy adapts as the
seller's time sensitivity changes. Our findings clarify the conflict between
quick sales and strategic waiting. Sellers facing thick markets or high time
sensitivity benefit from linear discounts, while in thin markets a constant
price neutralizes buyers' incentive to wait. We also show this simple policy
remains robust across broad conditions. For patient sellers, a quasi-auction
schedule that maintains a high price until a sharp final drop is most effective
in aggregating demand.

</details>


### [184] [A Bilevel Approach to Integrated Surgeon Scheduling and Surgery Planning solved via Branch-and-Price](https://arxiv.org/abs/2509.24806)
*Broos Maenhout,Přemysl Šůcha,Viktorie Valdmanová,Ondřej Tkadlec,Jana Thao Rozlivková*

Main category: cs.GT

TL;DR: 研究手术室部门多智能体调度问题，提出专用分支定价算法并验证性能。


<details>
  <summary>Details</summary>
Motivation: 解决外科医生组长和个体外科医生在手术安排中因规划方式不同可能导致的调度质量下降问题。

Method: 提出专用分支定价算法，在外科医生特定定价问题的公式中添加懒约束以获取最优双层可行解。

Result: 通过计算稳定性价格和分散化价格，验证了算法及其组件的性能，展示了不同场景下达到均衡解的好处。

Conclusion: 所提算法能让外科医生组长考虑个体外科医生的目标需求，有效搜索解空间。

Abstract: In this paper, we study a multi-agent scheduling problem for organising the
operations within the operating room department. The head of the surgeon group
and individual surgeons are together responsible for the surgeon schedule and
surgical case planning. The surgeon head allocates time blocks to individual
surgeons, whereas individual surgeons determine the planning of surgical cases
independently, which might degrade the schedule quality envisaged by the
surgeon head. The bilevel optimisation under study seeks an optimal Nash
equilibrium solution -- a surgeon schedule and surgical case plan that optimise
the objectives of the surgeon head, while ensuring that no individual surgeon
can improve their own objective within the allocated time blocks. We propose a
dedicated branch-and-price that adds lazy constraints to the formulation of
surgeon-specific pricing problems to ensure an optimal bilevel feasible
solution is retrieved. In this way, the surgeon head respects the objective
requirements of the individual surgeons and the solution space can be searched
efficiently. In the computational experiments, we validate the performance of
the proposed algorithm and its dedicated components and provide insights into
the benefits of attaining an equilibrium solution under different scenarios by
calculating the price of stability and the price of decentralisation.

</details>


### [185] [The Free Option Problem of ePBS](https://arxiv.org/abs/2509.24849)
*Bruno Mazorra,Burak Öz,Christoph Schlegel,Fei Wu*

Main category: cs.GT

TL;DR: 研究以太坊Glamsterdam升级中ePBS带来的免费期权问题，理论和实证分析其影响及概率，提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: 以太坊Glamsterdam升级引入ePBS虽改善区块生产流程，但带来新的活性风险，需研究免费期权问题。

Method: 进行理论分析预测期权价值和行使概率的影响因素，通过历史区块进行实证估计。

Result: 理论结果表明期权价值和行使概率与市场波动性等因素有关，实证基本证实理论预测，特定时期行使概率显著上升，依赖CEX - DEX套利的构建者更可能行使期权。

Conclusion: 缩短期权窗口或对行使期权进行惩罚等缓解策略能有效降低活性风险。

Abstract: Ethereum's upcoming Glamsterdam upgrade introduces EIP-7732 enshrined
Proposer--Builder Separation (ePBS), which improves the block production
pipeline by addressing trust and scalability challenges. Yet it also creates a
new liveness risk: builders gain a short-dated ``free'' option to prevent the
execution payload they committed to from becoming canonical, without incurring
an additional penalty. Exercising this option renders an empty block for the
slot in question, thereby degrading network liveness.
  We present the first systematic study of the free option problem. Our
theoretical results predict that option value and exercise probability grow
with market volatility, the length of the option window, and the share of block
value derived from external signals such as external market prices. The
availability of a free option will lead to mispricing and LP losses. The
problem would be exacerbated if Ethereum further scales and attracts more
liquidity. Empirical estimates of values and exercise probabilities on
historical blocks largely confirm our theoretical predictions. While the option
is rarely profitable to exercise on average (0.82\% of blocks assuming an
8-second option time window), it becomes significant in volatile periods,
reaching up to 6\% of blocks on high-volatility days -- precisely when users
most require timely execution.
  Moreover, builders whose block value relies heavily on CEX-DEX arbitrage are
more likely to exercise the option. We demonstrate that mitigation strategies
-- shortening the option window or penalizing exercised options -- effectively
reduce liveness risk.

</details>


### [186] [A Management Framework for Vehicular Cloudtoward Economic and Environmental Efficiency](https://arxiv.org/abs/2509.24946)
*Rosario Patanè,Andrea Araldo,Nadjib Achir,Lila Boukhatem*

Main category: cs.GT

TL;DR: 本文提出VCC管理方案，结合能量感知任务分配和博弈论收益共享机制，经仿真表明该方案支持低延迟任务执行，能有效实现车辆资源货币化，大幅减少CO₂排放。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏量化证据证明VCC在现实场景中的盈利能力和环境优势，需探究VCC能否兼具盈利性和可持续性。

Method: 提出结合能量感知任务分配和博弈论收益共享机制的VCC管理方案，联合建模延迟、能耗、货币激励和碳排放。

Result: 方案支持低延迟任务执行，能有效实现车辆资源货币化，与传统边缘基础设施相比，减少CO₂排放超99%。

Conclusion: VCC是边缘计算的实用且可持续替代方案。

Abstract: Vehicular Cloud Computing (VCC) leverages the idle computing capacity of
vehicles to execute end-users' offloaded tasks without requiring new
computation infrastructure. Despite its conceptual appeal, VCC adoption is
hindered by the lack of quantitative evidence demonstrating its profitability
and environmental advantages in real-world scenarios. This paper tackles the
fundamental question: Can VCC be both profitable and sustainable? We address
this problem by proposing a management scheme for VCC that combines
energy-aware task allocation with a game-theoretic revenue-sharing mechanism.
Our framework is the first to jointly model latency, energy consumption,
monetary incentives, and carbon emissions within urban mobility and 5G
communication settings. The task allocation strategy maximizes the aggregate
stakeholder utility while satisfying deadlines and minimizing energy costs. The
payoffs are distributed via a coalitional game theory adapted to dynamic
vehicular environments, to prevent disincentivizing participants with
potentially negative contributions. Extensive simulations demonstrate that our
approach supports low-latency task execution, enables effective monetization of
vehicular resources, and reduces CO2 emissions by more than 99% compared to
conventional edge infrastructures, making VCC a practical and sustainable
alternative to edge computing.

</details>


### [187] [The Popular Dimension of Matchings](https://arxiv.org/abs/2509.25150)
*Frank Connor,Louis-Roy Langevin,Ndiamé Ndiaye,Agnès Totschnig,Rohit Vasishta,Adrian Vetta*

Main category: cs.GT

TL;DR: 研究三种经典匹配问题中流行匹配的流行维度，证明房屋分配问题流行维度为2，婚姻和室友问题在一定条件下流行维度在2到3之间，特殊情况下室友问题流行维度为2。


<details>
  <summary>Details</summary>
Motivation: 流行匹配通常不存在，研究其自然松弛——流行获胜集及流行维度。

Method: 对房屋分配、婚姻和室友三种问题进行理论分析与证明。

Result: 房屋分配问题流行维度为2；婚姻和室友问题在加权和/或允许偏好列表有平局时流行维度在2到3之间；特殊情况下婚姻问题流行维度为1，室友问题为2。

Conclusion: 明确了不同条件下三种经典匹配问题的流行维度。

Abstract: We study popular matchings in three classical settings: the house allocation
problem, the marriage problem, and the roommates problem. In the popular
matching problem, (a subset of) the vertices in a graph have preference
orderings over their potential matches. A matching is popular if it gets a
plurality of votes in a pairwise election against any other matching.
Unfortunately, popular matchings typically do not exist. So we study a natural
relaxation, namely popular winning sets which are a set of matchings that
collectively get a plurality of votes in a pairwise election against any other
matching. The $\textit{popular dimension}$ is the minimum cardinality of a
popular winning set, in the worst case over the problem class.
  We prove that the popular dimension is exactly $2$ in the house allocation
problem, even if the voters are weighted and ties are allowed in their
preference lists. For the marriage problem and the roommates problem, we prove
that the popular dimension is between $2$ and $3$, when the agents are weighted
and/or their preferences orderings allow ties. In the special case where the
agents are unweighted and have strict preference orderings, the popular
dimension of the marriage problem is known to be exactly $1$ and we prove the
popular dimension of the roommates problem is exactly $2$.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [188] [How good are LLMs at Retrieving Documents in a Specific Domain?](https://arxiv.org/abs/2509.22658)
*Nafis Tanveer Islam,Zhiming Zhao*

Main category: cs.IR

TL;DR: 经典搜索引擎基于索引的方法存在理解用户意图不足问题，本文提出自动化方法创建评估数据集，结合RAG技术，分析表明基于大语言模型的系统在多意图查询理解上精度更高。


<details>
  <summary>Details</summary>
Motivation: 经典搜索引擎及特定领域搜索系统因缺乏评估数据集和语义理解不足，难以捕捉用户意图，产生不完整响应，需解决这些挑战。

Method: 提出自动化方法创建特定领域评估数据集，结合由大语言模型驱动的RAG技术进行环境领域数据的自然语言查询检索。

Result: 对评估数据集的定量和定性分析显示，基于大语言模型的信息检索系统在理解多意图查询时比基于Elasticsearch的系统返回结果的精度更高。

Conclusion: 基于大语言模型的信息检索系统在处理多意图查询时具有更好的性能。

Abstract: Classical search engines using indexing methods in data infrastructures
primarily allow keyword-based queries to retrieve content. While these
indexing-based methods are highly scalable and efficient, due to a lack of an
appropriate evaluation dataset and a limited understanding of semantics, they
often fail to capture the user's intent and generate incomplete responses
during evaluation. This problem also extends to domain-specific search systems
that utilize a Knowledge Base (KB) to access data from various research
infrastructures. Research infrastructures (RIs) from the environmental and
earth science domain, which encompass the study of ecosystems, biodiversity,
oceanography, and climate change, generate, share, and reuse large volumes of
data. While there are attempts to provide a centralized search service using
Elasticsearch as a knowledge base, they also face similar challenges in
understanding queries with multiple intents. To address these challenges, we
proposed an automated method to curate a domain-specific evaluation dataset to
analyze the capability of a search system. Furthermore, we incorporate the
Retrieval of Augmented Generation (RAG), powered by Large Language Models
(LLMs), for high-quality retrieval of environmental domain data using natural
language queries. Our quantitative and qualitative analysis of the evaluation
dataset shows that LLM-based systems for information retrieval return results
with higher precision when understanding queries with multiple intents,
compared to Elasticsearch-based systems.

</details>


### [189] [Federated Consistency- and Complementarity-aware Consensus-enhanced Recommendation](https://arxiv.org/abs/2509.22659)
*Yunqi Mi,Boyang Yan,Guoshuai Zhao,Jialie Shen,Xueming Qian*

Main category: cs.IR

TL;DR: 提出Fed3CR方法用于个性化联邦推荐系统，含ACE和C2O策略，可与其他方法集成，实验显示性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有个性化联邦推荐系统因交互数据稀疏和高一致性，导致共识退化和解耦不足，影响共识效用。

Method: 提出Fed3CR方法，包含ACE策略学习全局和客户端特定项目嵌入的关系，C2O策略学习更有效互补的表示。

Result: 在四个真实数据集上的广泛实验表明Fed3CR性能优越。

Conclusion: Fed3CR是一个即插即用的方法，可集成到其他FedRec方法中提升性能。

Abstract: Personalized federated recommendation system (FedRec) has gained significant
attention for its ability to preserve privacy in delivering tailored
recommendations. To alleviate the statistical heterogeneity challenges among
clients and improve personalization, decoupling item embeddings into the server
and client-specific views has become a promising way. Among them, the global
item embedding table serves as a consensus representation that integrates and
reflects the collective patterns across all clients. However, the inherent
sparsity and high uniformity of interaction data from massive-scale clients
results in degraded consensus and insufficient decoupling, reducing consensus's
utility. To this end, we propose a \textbf{Fed}erated \textbf{C}onsistency- and
\textbf{C}omplementarity-aware \textbf{C}onsensus-enhanced
\textbf{R}ecommendation (Fed3CR) method for personalized FedRec. To improve the
efficiency of the utilization of consensus, we propose an \textbf{A}daptive
\textbf{C}onsensus \textbf{E}nhancement (ACE) strategy to learn the
relationship between global and client-specific item embeddings. It enables the
client to adaptively enhance specific information in the consensus,
transforming it into a form that best suits itself. To improve the quality of
decoupling, we propose a \textbf{C}onsistency- and
\textbf{C}omplementarity-aware \textbf{O}ptimization (C2O) strategy, which
helps to learn more effective and complementary representations. Notably, our
proposed Fed3CR is a plug-and-play method, which can be integrated with other
FedRec methods to improve its performance. Extensive experiments on four
real-world datasets represent the superior performance of Fed3CR.

</details>


### [190] [Fairness for niche users and providers: algorithmic choice and profile portability](https://arxiv.org/abs/2509.22660)
*Elizabeth McKinnie,Anas Buhayh,Clement Canel,Robin Burke*

Main category: cs.IR

TL;DR: 研究推荐系统中算法多元化和用户资料可移植性对公平性结果的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注推荐系统算法干预的公平性，很少研究推荐生态系统的结构变化，本文探索算法多元化及资料可移植性的公平性影响。

Method: 采用模拟方法，研究用户资料处理政策与公平性结果的相互作用。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Ensuring fair outcomes for multiple stakeholders in recommender systems has
been studied mostly in terms of algorithmic interventions: building new models
with better fairness properties, or using reranking to improve outcomes from an
existing algorithm. What has rarely been studied is structural changes in the
recommendation ecosystem itself. Our work explores the fairness impact of
algorithmic pluralism, the idea that the recommendation algorithm is decoupled
from the platform through which users access content, enabling user choice in
algorithms. Prior work using a simulation approach has shown that niche
consumers and (especially) niche providers benefit from algorithmic choice. In
this paper, we use simulation to explore the question of profile portability,
to understand how different policies regarding the handling of user profiles
interact with fairness outcomes for consumers and providers.

</details>


### [191] [Next Point-of-interest (POI) Recommendation Model Based on Multi-modal Spatio-temporal Context Feature Embedding](https://arxiv.org/abs/2509.22661)
*Lingyu Zhang,Guobin Wu,Yan Wang,Pengfei Xu,Jian Liang,Xuan Song,Yunhai Wang*

Main category: cs.IR

TL;DR: 提出基于多模态时空上下文特征嵌入的POI推荐模型，实验表明结合多种特征的模型预测准确性高于现有SOTA模型和方法。


<details>
  <summary>Details</summary>
Motivation: 传统POI预测模型主要依赖短期交通序列信息，常忽略用户行为中的长期和短期偏好数据以及关键时空上下文特征。

Method: 引入用户长期偏好信息和关键时空上下文信息，通过时空特征处理、多模态嵌入和自注意力聚合等模块提取特征，用加权融合方法动态调整特征权重，最后用注意力匹配融合特征并计算候选位置概率。

Result: 在多个交通数据集上实验，结合多种特征的POI预测模型比现有SOTA模型和方法有更高预测准确性。

Conclusion: 基于多模态时空上下文特征嵌入的POI推荐模型有效，能提升预测准确性。

Abstract: The next Point-of-interest (POI) recommendation is mainly based on sequential
traffic information to predict the user's next boarding point location. This is
a highly regarded and widely applied research task in the field of intelligent
transportation, and there have been many research results to date. Traditional
POI prediction models primarily rely on short-term traffic sequence
information, often neglecting both long-term and short-term preference data, as
well as crucial spatiotemporal context features in user behavior. To address
this issue, this paper introduces user long-term preference information and key
spatiotemporal context information, and proposes a POI recommendation model
based on multimodal spatiotemporal context feature embedding. The model
extracts long-term preference features and key spatiotemporal context features
from traffic data through modules such as spatiotemporal feature processing,
multimodal embedding, and self-attention aggregation. It then uses a weighted
fusion method to dynamically adjust the weights of long-term and short-term
features based on users' historical behavior patterns and the current context.
Finally, the fused features are matched using attention, and the probability of
each location candidate becoming the next location is calculated. This paper
conducts experimental verification on multiple transportation datasets, and the
results show that the POI prediction model combining multiple types of features
has higher prediction accuracy than existing SOTA models and methods.

</details>


### [192] [MTRec: Learning to Align with User Preferences via Mental Reward Models](https://arxiv.org/abs/2509.22807)
*Mengchen Zhao,Yifan Gao,Yaqing Hou,Xiangyang Li,Pengjie Gu,Zhenhua Dong,Ruiming Tang,Yi Cai*

Main category: cs.IR

TL;DR: 提出MTRec框架，通过挖掘用户对推荐项的内在满意度使推荐与真实偏好对齐，实验有显著提升，部署后用户观看时长增7%。


<details>
  <summary>Details</summary>
Motivation: 现有推荐模型多使用隐式反馈训练，但隐式反馈不能反映用户真实偏好，可能误导推荐系统。

Method: 引入心理奖励模型量化用户满意度，采用分布逆强化学习方法学习该模型，用其指导推荐模型。

Result: MTRec为多种推荐模型带来显著改进，部署在工业短视频平台使用户平均观看时长增加7%。

Conclusion: MTRec能有效使推荐模型更好地符合用户真实偏好。

Abstract: Recommendation models are predominantly trained using implicit user feedback,
since explicit feedback is often costly to obtain. However, implicit feedback,
such as clicks, does not always reflect users' real preferences. For example, a
user might click on a news article because of its attractive headline, but end
up feeling uncomfortable after reading the content. In the absence of explicit
feedback, such erroneous implicit signals may severely mislead recommender
systems. In this paper, we propose MTRec, a novel sequential recommendation
framework designed to align with real user preferences by uncovering their
internal satisfaction on recommended items. Specifically, we introduce a mental
reward model to quantify user satisfaction and propose a distributional inverse
reinforcement learning approach to learn it. The learned mental reward model is
then used to guide recommendation models to better align with users' real
preferences. Our experiments show that MTRec brings significant improvements to
a variety of recommendation models. We also deploy MTRec on an industrial short
video platform and observe a 7 percent increase in average user viewing time.

</details>


### [193] [WARBERT: A Hierarchical BERT-based Model for Web API Recommendation](https://arxiv.org/abs/2509.23175)
*Zishuo Xu,Yuhong Gu,Dezhong Yao*

Main category: cs.IR

TL;DR: 提出基于分层BERT的Web API推荐模型WARBERT，在数据集上表现优于多数现有方案。


<details>
  <summary>Details</summary>
Motivation: Web API数量增加，现有推荐方案存在语义模糊、缺乏详细比较和时间效率低等问题，需高效推荐方案。

Method: 提出WARBERT模型，含WARBERT(R)和WARBERT(M)两个组件，利用双组件特征融合和注意力比较，WARBERT(R)做初始筛选，WARBERT(M)细化匹配，结合两者预测结果，且WARBERT(R)有辅助任务。

Result: 在ProgrammableWeb数据集上，WARBERT优于多数现有方案，比MTFM模型最多提升11.7%。

Conclusion: WARBERT在Web API推荐中能显著提升准确性和效率。

Abstract: With the emergence of Web 2.0 and microservices architecture, the number of
Web APIs has increased dramatically, further intensifying the demand for
efficient Web API recommendation. Existing solutions typically fall into two
categories: recommendation-type methods, which treat each API as a label for
classification, and match-type methods, which focus on matching mashups through
API retrieval. However, three critical challenges persist: 1) the semantic
ambiguities in comparing API and mashup descriptions, 2) the lack of detailed
comparisons between the individual API and the mashup in recommendation-type
methods, and 3) time inefficiencies for API retrieval in match-type methods. To
address these challenges, we propose WARBERT, a hierarchical BERT-based model
for Web API recommendation. WARBERT leverages dual-component feature fusion and
attention comparison to extract precise semantic representations of API and
mashup descriptions. WARBERT consists of two main components: WARBERT(R) for
Recommendation and WARBERT(M) for Matching. Specifically, WAR-BERT(R) serves as
an initial filter, narrowing down the candidate APIs, while WARBERT(M) refines
the matching process by calculating the similarity between candidate APIs and
mashup. The final likelihood of a mashup being matched with an API is
determined by combining the predictions from WARBERT(R) and WARBERT(M).
Additionally, WARBERT(R) incorporates an auxiliary task of mashup category
judgment, which enhances its effectiveness in candidate selection. Experimental
results on the ProgrammableWeb dataset demonstrate that WARBERT outperforms
most existing solutions and achieves improvements of up to 11.7% compared to
the model MTFM (Multi-Task Fusion Model), delivering significant enhancements
in accuracy and effiency.

</details>


### [194] [From Past To Path: Masked History Learning for Next-Item Prediction in Generative Recommendation](https://arxiv.org/abs/2509.23649)
*KaiWen Wei,Kejun He,Xiaomian Kang,Jie Zhang,Yuming Yang,Jiang Zhong,He Bai,Junnan Zhu*

Main category: cs.IR

TL;DR: 提出Masked History Learning (MHL)训练框架，含熵引导掩码策略和课程学习调度器，实验显示其优于现有生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐依赖纯自回归训练，忽略用户交互历史内部结构，无法把握潜在意图，限制了其潜力。

Method: 提出MHL框架，用重建掩码历史项的辅助任务增强标准自回归目标；引入熵引导掩码策略和课程学习调度器。

Result: 在三个公共数据集上的实验表明，该方法显著优于现有生成模型。

Conclusion: 全面理解用户过去行为对准确预测未来路径至关重要。

Abstract: Generative recommendation, which directly generates item identifiers, has
emerged as a promising paradigm for recommendation systems. However, its
potential is fundamentally constrained by the reliance on purely autoregressive
training. This approach focuses solely on predicting the next item while
ignoring the rich internal structure of a user's interaction history, thus
failing to grasp the underlying intent. To address this limitation, we propose
Masked History Learning (MHL), a novel training framework that shifts the
objective from simple next-step prediction to deep comprehension of history.
MHL augments the standard autoregressive objective with an auxiliary task of
reconstructing masked historical items, compelling the model to understand
``why'' an item path is formed from the user's past behaviors, rather than just
``what'' item comes next. We introduce two key contributions to enhance this
framework: (1) an entropy-guided masking policy that intelligently targets the
most informative historical items for reconstruction, and (2) a curriculum
learning scheduler that progressively transitions from history reconstruction
to future prediction. Experiments on three public datasets show that our method
significantly outperforms state-of-the-art generative models, highlighting that
a comprehensive understanding of the past is crucial for accurately predicting
a user's future path. The code will be released to the public.

</details>


### [195] [Constructing Opera Seria in the Iberian Courts: Metastasian Repertoire for Spain and Portugal](https://arxiv.org/abs/2509.23771)
*Ana Llorens,Alvaro Torrente*

Main category: cs.IR

TL;DR: 文章研究五位国际作曲家为伊比利亚宫廷创作歌剧时的风格变化，通过定量分析将伊比利亚音乐置于欧洲歌剧背景中，指出其独特性与当地习俗等有关。


<details>
  <summary>Details</summary>
Motivation: 虽梅塔斯塔西奥作品在欧洲成功表明伊比利亚参与国际艺术潮流，但当地剧目有特点，需研究作曲家为伊比利亚宫廷创作歌剧时的风格差异。

Method: 对马德里和里斯本宫廷剧院的十五部作品进行统计分析，并与从2404首咏叹调语料库提取的平均数据对比。

Result: 可评估在调式、节拍、节奏和声乐处理方面的特定用法。

Conclusion: 十八世纪伊比利亚音乐独特性部分依赖当地音乐习俗、性别刻板印象和个人特质。

Abstract: The exceptional reception of Pietro Metastasio's works during the eighteenth
century, all over Europe and in the Iberian Peninsula in particular, is well
documented. Due to that unparalleled success, it is possible to ascertain Spain
and Portugal's participation in international, contemporary tastes and artistic
webs, applicable to both composers and performers. However, this
internationalisation needs to be nuanced, as some characteristics of the
repertoire specifically written for the Peninsula indicate that their court
audiences may have had expectations, both social and strictly musical,
different from those of the public in opera theatres elsewhere in the
continent. In this light, this article investigates in what ways the style of
five composers in the international scene - Perez, Galuppi, Jommelli, Conforto,
and Corselli - varied when commissioned to write opera seria for the Iberian
courts. The statistical analysis of fifteen settings especially written for the
court theatres in Madrid and Lisbon, in comparison to the average data
extracted from a corpus of 2,404 arias from 126 versions of a select number of
Metastasian librettos, allows us to evaluate some particular usages regarding
key, metre, tempo, and treatment of the vocal part. In this manner, through
quantitative analysis, this article places eighteenth-century Iberian music
production and consumption in the context of European opera seria, while
ultimately suggesting that its unique musical characteristics were also partly
dependent on local musical customs, gender stereotypes, and personal
idiosyncrasies alike.

</details>


### [196] [Semantic Representation of Processes with Ontology Design Patterns](https://arxiv.org/abs/2509.23776)
*Ebrahim Norouzi,Sven Hertling,Jörg Waitelonis,Harald Sack*

Main category: cs.IR

TL;DR: 本文调研材料科学中科学工作流和工程过程建模相关本体，提取隐含设计模式，提出自动提取方法并公开相关资源。


<details>
  <summary>Details</summary>
Motivation: 现有用于过程建模的本体复杂且难复用，本体设计模式未充分公开和文档化。

Method: 调研相关本体，识别隐含设计模式，提出自动提取设计模式的基线方法，并与策划的真值模式进行评估。

Result: 成功识别本体中的隐含设计模式，提出的提取方法得到评估，相关资源公开在GitHub。

Conclusion: 为材料科学过程表示提供了可复用的设计模式及提取方法。

Abstract: The representation of workflows and processes is essential in materials
science engineering, where experimental and computational reproducibility
depend on structured and semantically coherent process models. Although
numerous ontologies have been developed for process modeling, they are often
complex and challenging to reuse. Ontology Design Patterns (ODPs) offer modular
and reusable modeling solutions to recurring problems; however, these patterns
are frequently neither explicitly published nor documented in a manner
accessible to domain experts. This study surveys ontologies relevant to
scientific workflows and engineering process modeling and identifies implicit
design patterns embedded within their structures. We evaluate the capacity of
these ontologies to fulfill key requirements for process representation in
materials science. Furthermore, we propose a baseline method for the automatic
extraction of design patterns from existing ontologies and assess the approach
against curated ground truth patterns. All resources associated with this work,
including the extracted patterns and the extraction workflow, are made openly
available in a public GitHub repository.

</details>


### [197] [GSID: Generative Semantic Indexing for E-Commerce Product Understanding](https://arxiv.org/abs/2509.23860)
*Haiyang Yang,Qinye Xie,Qingheng Zhang,Liyu Chen,Huike Zou,Chengbao Lian,Shuguang Han,Fei Huang,Jufeng Chen,Bo Zheng*

Main category: cs.IR

TL;DR: 提出GSID方法解决电商平台产品信息结构化表示问题，实验验证有效并部署获良好结果


<details>
  <summary>Details</summary>
Motivation: 现有电商平台产品信息基于手动整理的类别和属性组织，无法覆盖长尾产品且与买家偏好不符，影响平台效率

Method: 提出GSID方法，包括在非结构化产品元数据上预训练学习领域内语义嵌入、为下游以产品为中心的应用生成更有效的语义代码

Result: 广泛实验验证GSID有效性，已成功部署在真实电商平台，在产品理解等下游任务上取得良好结果

Conclusion: GSID能有效解决电商平台产品信息结构化表示问题

Abstract: Structured representation of product information is a major bottleneck for
the efficiency of e-commerce platforms, especially in second-hand ecommerce
platforms. Currently, most product information are organized based on manually
curated product categories and attributes, which often fail to adequately cover
long-tail products and do not align well with buyer preference. To address
these problems, we propose \textbf{G}enerative \textbf{S}emantic
\textbf{I}n\textbf{D}exings (GSID), a data-driven approach to generate product
structured representations. GSID consists of two key components: (1)
Pre-training on unstructured product metadata to learn in-domain semantic
embeddings, and (2) Generating more effective semantic codes tailored for
downstream product-centric applications. Extensive experiments are conducted to
validate the effectiveness of GSID, and it has been successfully deployed on
the real-world e-commerce platform, achieving promising results on product
understanding and other downstream tasks.

</details>


### [198] [Investigating Multi-layer Representations for Dense Passage Retrieval](https://arxiv.org/abs/2509.23861)
*Zhongbin Xie,Thomas Lukasiewicz*

Main category: cs.IR

TL;DR: 提出用多层表示（MLR）组成文档表示，研究不同层表示对其性能影响，用池化策略提高检索效率，实验证明MLR有效性。


<details>
  <summary>Details</summary>
Motivation: 现有密集检索模型通常用文档编码器最后隐藏层向量表示文档，而预训练语言模型不同层表示包含不同语言知识且微调表现不同。

Method: 研究不同层表示对MLR在多向量检索设置下的性能影响，用池化策略将多向量模型简化为单向量模型。

Result: 实验证明MLR在单向量检索设置下优于dual encoder、ME - BERT和ColBERT，且能与其他高级训练技术配合良好。

Conclusion: 利用多层表示组成文档表示是有效的，可提高检索性能和效率。

Abstract: Dense retrieval models usually adopt vectors from the last hidden layer of
the document encoder to represent a document, which is in contrast to the fact
that representations in different layers of a pre-trained language model
usually contain different kinds of linguistic knowledge, and behave differently
during fine-tuning. Therefore, we propose to investigate utilizing
representations from multiple encoder layers to make up the representation of a
document, which we denote Multi-layer Representations (MLR). We first
investigate how representations in different layers affect MLR's performance
under the multi-vector retrieval setting, and then propose to leverage pooling
strategies to reduce multi-vector models to single-vector ones to improve
retrieval efficiency. Experiments demonstrate the effectiveness of MLR over
dual encoder, ME-BERT and ColBERT in the single-vector retrieval setting, as
well as demonstrate that it works well with other advanced training techniques
such as retrieval-oriented pre-training and hard negative mining.

</details>


### [199] [Multi-Value-Product Retrieval-Augmented Generation for Industrial Product Attribute Value Identification](https://arxiv.org/abs/2509.23874)
*Huike Zou,Haiyang Yang,Yindu Su,Liyu Chen,Chengbao Lian,Qingheng Zhang,Shuguang Han,Jufeng Chen*

Main category: cs.IR

TL;DR: 提出MVP - RAG方法解决现有PAVI方法局限性，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有PAVI方法存在级联错误、无法处理OOD属性值和缺乏泛化能力等问题。

Method: 引入Multi - Value - Product Retrieval - Augmented Generation (MVP - RAG)，结合检索、生成和分类范式，将PAVI定义为检索生成任务，先检索相似产品和候选属性值，再生成标准化属性值。

Result: 广泛的实验结果表明MVP - RAG比现有最先进的基线表现更好。

Conclusion: MVP - RAG方法有提出多级检索方案、缓解OOD问题和可在现实工业环境部署等优势，能有效解决现有PAVI方法的局限性。

Abstract: Identifying attribute values from product profiles is a key task for
improving product search, recommendation, and business analytics on e-commerce
platforms, which we called Product Attribute Value Identification (PAVI) .
However, existing PAVI methods face critical challenges, such as cascading
errors, inability to handle out-of-distribution (OOD) attribute values, and
lack of generalization capability. To address these limitations, we introduce
Multi-Value-Product Retrieval-Augmented Generation (MVP-RAG), combining the
strengths of retrieval, generation, and classification paradigms. MVP-RAG
defines PAVI as a retrieval-generation task, where the product title
description serves as the query, and products and attribute values act as the
corpus. It first retrieves similar products of the same category and candidate
attribute values, and then generates the standardized attribute values. The key
advantages of this work are: (1) the proposal of a multi-level retrieval
scheme, with products and attribute values as distinct hierarchical levels in
PAVI domain (2) attribute value generation of large language model to
significantly alleviate the OOD problem and (3) its successful deployment in a
real-world industrial environment. Extensive experimental results demonstrate
that MVP-RAG performs better than the state-of-the-art baselines.

</details>


### [200] [Multi-Item-Query Attention for Stable Sequential Recommendation](https://arxiv.org/abs/2509.24424)
*Mingshi Xu,Haoren Zhu,Wilfred Siu Hung Ng*

Main category: cs.IR

TL;DR: 提出多项目查询注意力机制(MIQ - Attn)提升顺序推荐系统性能，实验表明其在基准数据集上效果显著。


<details>
  <summary>Details</summary>
Motivation: 用户交互数据的不稳定性和噪声挑战顺序推荐系统，现有单查询掩码注意力模型对噪声敏感，降低预测可靠性。

Method: 提出多项目查询注意力机制(MIQ - Attn)，从用户交互中构建多个不同的查询向量，可作为现有单查询注意力的直接替代品。

Result: MIQ - Attn在基准数据集上显著提高了性能。

Conclusion: MIQ - Attn能增强模型稳定性和准确性，有效减轻噪声影响并提高一致性。

Abstract: The inherent instability and noise in user interaction data challenge
sequential recommendation systems. Prevailing masked attention models, relying
on a single query from the most recent item, are sensitive to this noise,
reducing prediction reliability. We propose the Multi-Item-Query attention
mechanism (MIQ-Attn) to enhance model stability and accuracy. MIQ-Attn
constructs multiple diverse query vectors from user interactions, effectively
mitigating noise and improving consistency. It is designed for easy adoption as
a drop-in replacement for existing single-query attention. Experiments show
MIQ-Attn significantly improves performance on benchmark datasets.

</details>


### [201] [UniDex: Rethinking Search Inverted Indexing with Unified Semantic Modeling](https://arxiv.org/abs/2509.24632)
*Zan Li,Jiahui Chen,Yuan Chai,Xiaoze Jiang,Xiaohua Qi,Zhiheng Qin,Runbin Zhou,Shun Zuo,Guangchao Hao,Kefeng Wang,Jingshan Lv,Yupeng Huang,Xiao Liang,Han Li*

Main category: cs.IR

TL;DR: 提出新型基于模型的方法UniDex革新倒排索引，提升检索能力并在快手系统验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于词项的倒排索引强调表面词元重叠，限制系统泛化能力和检索效果，需改进。

Method: 提出UniDex，包含将查询和文档映射为语义ID的UniTouch及进行语义匹配排序的UniRank。

Result: 通过大规模工业数据集和在线流量评估，显示UniDex显著提升检索能力；在快手短视频搜索系统部署，能高效服务数亿活跃用户。

Conclusion: UniDex实现从基于词项到基于模型索引的范式转变，具有实际有效性。

Abstract: Inverted indexing has traditionally been a cornerstone of modern search
systems, leveraging exact term matches to determine relevance between queries
and documents. However, this term-based approach often emphasizes surface-level
token overlap, limiting the system's generalization capabilities and retrieval
effectiveness. To address these challenges, we propose UniDex, a novel
model-based method that employs unified semantic modeling to revolutionize
inverted indexing. UniDex replaces complex manual designs with a streamlined
architecture, enhancing semantic generalization while reducing maintenance
overhead. Our approach involves two key components: UniTouch, which maps
queries and documents into semantic IDs for improved retrieval, and UniRank,
which employs semantic matching to rank results effectively. Through
large-scale industrial datasets and real-world online traffic assessments, we
demonstrate that UniDex significantly improves retrieval capabilities, marking
a paradigm shift from term-based to model-based indexing. Our deployment within
Kuaishou's short-video search systems further validates UniDex's practical
effectiveness, serving hundreds of millions of active users efficiently.

</details>


### [202] [Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval](https://arxiv.org/abs/2509.24869)
*Junwei Lan,Jianlyu Chen,Zheng Liu,Chaofan Li,Siqi Bao,Defu Lian*

Main category: cs.IR

TL;DR: 提出Retro*用于推理密集型文档检索，在BRIGHT基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理和RAG流行，现有IR技术难以解决间接或隐式关联的文档检索问题，现有推理增强IR方法在适用性、可扩展性和效率方面存在挑战。

Method: 提出基于规则的相关性评分机制，支持测试时扩展，引入针对相关性评分机制的强化学习算法。

Result: Retro*在实验中显著优于现有文档检索方法。

Conclusion: Retro*在文档检索任务中达到了最先进的性能。

Abstract: With the growing popularity of LLM agents and RAG, it has become increasingly
important to retrieve documents that are essential for solving a task, even
when their connection to the task is indirect or implicit. Addressing this
problem requires fine-grained reasoning to accurately assess the relevance
between the task and each candidate document. This capability, however, poses a
significant challenge for existing IR techniques. Despite recent progress in
reasoning-enhanced IR, existing approaches still face significant challenges in
applicability, scalability, and efficiency. In this work, we propose Retro*, a
novel approach for reasoning-intensive document retrieval. Our method
introduces a rubric-based relevance scoring mechanism, enabling the model to
reason about the relationship between a task and a document based on explicitly
defined criteria, whereby producing a fine-grained, interpretable relevance
score. Retro* also supports test-time scaling by combining multiple reasoning
trajectories via score integration, which produces more reliable relevance
estimates. To optimize Retro*'s reasoning capabilities, we introduce a novel
reinforcement learning algorithm tailored for its relevance scoring mechanism,
which employs two composite rewards to fully exploit the trajectories of each
training sample. Our experiments show that Retro* outperforms existing document
retrieval methods with notable advantages, leading to state-of-the-art
performance on the BRIGHT benchmark.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [203] [Localizing Adversarial Attacks To Produces More Imperceptible Noise](https://arxiv.org/abs/2509.22710)
*Pavan Reddy,Aditya Sanjay Gujral*

Main category: cs.LG

TL;DR: 研究系统评估常用方法的局部对抗攻击，发现其比全局攻击有低像素扰动等优势，但计算量大、攻击成功率略降，迭代方法更能适应局部约束。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习对抗攻击关注全局扰动，局部对抗噪声潜力待挖掘，故研究局部对抗攻击。

Method: 对FGSM、PGD和C&W等常用方法进行系统评估，引入二进制掩码将噪声限制在特定区域。

Result: 局部攻击比全局攻击像素扰动低、PSNR和SSIM高，但计算量大、攻击成功率略降，迭代方法比单步方法更能适应局部约束。

Conclusion: 为推进攻击策略和设计鲁棒防御系统提供实用见解。

Abstract: Adversarial attacks in machine learning traditionally focus on global
perturbations to input data, yet the potential of localized adversarial noise
remains underexplored. This study systematically evaluates localized
adversarial attacks across widely-used methods, including FGSM, PGD, and C&W,
to quantify their effectiveness, imperceptibility, and computational
efficiency. By introducing a binary mask to constrain noise to specific
regions, localized attacks achieve significantly lower mean pixel
perturbations, higher Peak Signal-to-Noise Ratios (PSNR), and improved
Structural Similarity Index (SSIM) compared to global attacks. However, these
benefits come at the cost of increased computational effort and a modest
reduction in Attack Success Rate (ASR). Our results highlight that iterative
methods, such as PGD and C&W, are more robust to localization constraints than
single-step methods like FGSM, maintaining higher ASR and imperceptibility
metrics. This work provides a comprehensive analysis of localized adversarial
attacks, offering practical insights for advancing attack strategies and
designing robust defensive systems.

</details>


### [204] [In-Context Learning can Perform Continual Learning Like Humans](https://arxiv.org/abs/2509.22764)
*Liuwang Kang,Fan Wang,Shaoshan Liu,Hung-Chyun Chou,Chuan Lin,Ning Ding*

Main category: cs.LG

TL;DR: 研究ICL在多任务场景的保留特性并拓展到ICCL，实验表明ICCL有类似人类的特性，还提出相似度指标评估CL方法，结果显示ICCL认知合理且有效。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索ICL在多任务顺序到来时的长期保留和跨任务知识积累能力，受人类记忆研究启发开展研究。

Method: 研究ICL在多任务设置中的保留特性并拓展到ICCL，通过任务调度和提示重排实现持续学习能力，在Markov - Chain基准上实验，提出人类保留相似度指标。

Result: ICCL从分布式练习中受益，有保留的间隔“最佳点”；线性注意力模型有类似人类的保留模式，但保留性能不如基于Transformer的大模型。

Conclusion: ICCL认知合理且实际有效，提供仅推理的CL范式，缓解传统CL方法的灾难性遗忘和稳定性 - 可塑性困境。

Abstract: Large language models (LLMs) can adapt to new tasks via in-context learning
(ICL) without parameter updates, making them powerful learning engines for fast
adaptation. While extensive research has examined ICL as a few-shot learner,
whether it can achieve long-term retention and cross-task knowledge
accumulation when multitasks arrive sequentially remains underexplored.
Motivated by human memory studies, we investigate the retention characteristics
of ICL in multitask settings and extend it to in-context continual learning
(ICCL), where continual learning ability emerges through task scheduling and
prompt rearrangement. Experiments on Markov-Chain benchmarks demonstrate that,
for specific large-language models, ICCL benefits from distributed practice
(DP) in a manner analogous to humans, consistently revealing a spacing "sweet
spot" for retention. Beyond retention performance, we propose a human-retention
similarity metric to quantify how closely a continual-learning (CL) method
aligns with human retention dynamics. Using this metric, we show that
linear-attention models such as MAMBA and RWKV exhibit particularly human-like
retention patterns, despite their retention performance lagging behind that of
Transformer-based LLMs. Overall, our results establish ICCL as both cognitively
plausible and practically effective, providing an inference-only CL paradigm
that mitigates catastrophic forgetting and addresses the stability-plasticity
dilemma in conventional CL methods.

</details>


### [205] [Efficient Identification of High Similarity Clusters in Polygon Datasets](https://arxiv.org/abs/2509.23942)
*John N. Daras*

Main category: cs.LG

TL;DR: 利用Shapely 2.0和Triton优化空间相似性计算在处理大数据集时面临挑战，本文提出框架减少需验证的簇数量，结合多种技术降低计算成本，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有工具在处理极大数据集的空间相似性计算时，因计算量过大面临挑战，需寻找解决方案。

Method: 提出一个框架，集成动态相似性索引阈值、监督调度和召回约束优化，利用KDE动态确定相似性阈值，用机器学习模型对簇进行优先级排序。

Result: 实验结果表明该方法具有可扩展性和有效性。

Conclusion: 该方法是大规模地理空间分析的实用解决方案，能在不牺牲准确性的前提下大幅降低计算成本。

Abstract: Advancements in tools like Shapely 2.0 and Triton can significantly improve
the efficiency of spatial similarity computations by enabling faster and more
scalable geometric operations. However, for extremely large datasets, these
optimizations may face challenges due to the sheer volume of computations
required. To address this, we propose a framework that reduces the number of
clusters requiring verification, thereby decreasing the computational load on
these systems. The framework integrates dynamic similarity index thresholding,
supervised scheduling, and recall-constrained optimization to efficiently
identify clusters with the highest spatial similarity while meeting
user-defined precision and recall requirements. By leveraging Kernel Density
Estimation (KDE) to dynamically determine similarity thresholds and machine
learning models to prioritize clusters, our approach achieves substantial
reductions in computational cost without sacrificing accuracy. Experimental
results demonstrate the scalability and effectiveness of the method, offering a
practical solution for large-scale geospatial analysis.

</details>


### [206] [Communication-Efficient and Interoperable Distributed Learning](https://arxiv.org/abs/2509.22823)
*Mounssif Krouka,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出支持模型异构的通信高效分布式学习框架，实验显示其通信效率优于基线且训练性能稳定。


<details>
  <summary>Details</summary>
Motivation: 解决异构模型架构协同学习中确保互操作性和保护隐私的挑战。

Method: 让所有客户端采用通用融合层输出维度，将模型分为个性化基础块和通用模块化块，客户端共享融合层输出并保持参数和架构隐私。

Result: 框架通信效率优于联邦学习和联邦拆分学习基线，在异构架构上训练性能稳定。

Conclusion: 所提框架能有效应对异构模型架构协同学习问题，具备通信高效和性能稳定的优势。

Abstract: Collaborative learning across heterogeneous model architectures presents
significant challenges in ensuring interoperability and preserving privacy. We
propose a communication-efficient distributed learning framework that supports
model heterogeneity and enables modular composition during inference. To
facilitate interoperability, all clients adopt a common fusion-layer output
dimension, which permits each model to be partitioned into a personalized base
block and a generalized modular block. Clients share their fusion-layer
outputs, keeping model parameters and architectures private. Experimental
results demonstrate that the framework achieves superior communication
efficiency compared to federated learning (FL) and federated split learning
(FSL) baselines, while ensuring stable training performance across
heterogeneous architectures.

</details>


### [207] [On the Capacity of Self-Attention](https://arxiv.org/abs/2509.22840)
*Micah Adler*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While self-attention is known to learn relations among tokens, we lack a
formal understanding of its capacity: how many distinct relations can a single
layer reliably recover for a given budget?
  To formalize this, we introduce Relational Graph Recognition (RGR), where the
key-query channel represents a graph on $m$ items with $m'$ directed edges,
and, given a context of items, must recover the neighbors of each item. We
measure resources by the total key dimension $D_K = h\,d_k$. Within this
framework, we analytically derive a capacity scaling law and validate it
empirically. We show that $D_K = \Theta(m' \log m' / d_{\text{model}})$ is both
necessary (information-theoretic lower bound) and sufficient (explicit
construction) in a broad class of graphs to recover $m'$ relations. This
scaling law directly leads to a new, capacity-based rationale for multi-head
attention that applies even when each item only attends to a single target.
When embeddings are uncompressed ($m = d_{\text{model}}$) and the graph is a
permutation, a single head suffices. However, compression ($m >
d_{\text{model}}$) forces relations into overlapping subspaces, creating
interference that a single large head cannot disentangle. Our analysis shows
that allocating a fixed $D_K$ across many small heads mitigates this
interference, increasing the number of recoverable relations. Controlled
single-layer experiments mirror the theory, revealing a sharp performance
threshold that matches the predicted capacity scaling and confirms the benefit
of distributing $D_K$ across multiple heads.
  Altogether, these results provide a concrete scaling law for self-attention
capacity and a principled design rule for allocating key-query budget across
heads.

</details>


### [208] [Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data](https://arxiv.org/abs/2509.22850)
*Roie Kazoom,Yuval Ratzabi,Etamar Rothstein,Ofer Hadar*

Main category: cs.LG

TL;DR: 本文提出针对表格数据的黑盒、基于决策的对抗攻击方法，实验表明该方法能成功攻击多种模型，凸显表格模型易受攻击，需加强防御。


<details>
  <summary>Details</summary>
Motivation: 与视觉和语言领域相比，结构化数据的对抗鲁棒性研究不足。

Method: 结合无梯度方向估计和迭代边界搜索，在最少预言机访问下高效探索离散和连续特征空间。

Result: 该方法成功攻击了不同模型的近整个测试集，成功率超90%，且每个实例只需少量查询。

Conclusion: 表格模型易受对抗扰动影响，现实决策系统急需更强防御。

Abstract: Adversarial robustness in structured data remains an underexplored frontier
compared to vision and language domains. In this work, we introduce a novel
black-box, decision-based adversarial attack tailored for tabular data. Our
approach combines gradient-free direction estimation with an iterative boundary
search, enabling efficient navigation of discrete and continuous feature spaces
under minimal oracle access. Extensive experiments demonstrate that our method
successfully compromises nearly the entire test set across diverse models,
ranging from classical machine learning classifiers to large language model
(LLM)-based pipelines. Remarkably, the attack achieves success rates
consistently above 90%, while requiring only a small number of queries per
instance. These results highlight the critical vulnerability of tabular models
to adversarial perturbations, underscoring the urgent need for stronger
defenses in real-world decision-making systems.

</details>


### [209] [Adaptive Margin RLHF via Preference over Preferences](https://arxiv.org/abs/2509.22851)
*Yaswanth Chittepu,Prasann Singhal,Greg Durrett,Scott Niekum*

Main category: cs.LG

TL;DR: 提出DPO - PoP方法利用偏好上的偏好推断自适应边距，在数据集上表现优于其他方法，还提出两种采样策略应对判别和生成性能权衡。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型学习方法在边距处理上未考虑偏好强度差异或依赖不准确的偏好分数，作者认为建模偏好强度可提升泛化和对齐效果。

Method: 提出利用偏好上的偏好的方法来推断自适应边距，引入DPO - PoP，结合偏好上的偏好监督的自适应边距；提出两种采样策略。

Result: 在UltraFeedback数据集上，方法优于普通DPO、固定边距DPO和有真实边距的DPO；发现判别和生成性能存在权衡。

Conclusion: 建模偏好强度可提升奖励模型学习效果，通过提出的方法和策略能应对判别和生成性能的权衡。

Abstract: Margin-based optimization is fundamental to improving generalization and
robustness in classification tasks. In the context of reward model learning
from preferences within Reinforcement Learning from Human Feedback (RLHF),
existing methods typically rely on no margins, fixed margins, or margins that
are simplistic functions of preference ratings. However, such formulations
often fail to account for the varying strengths of different preferences, for
example some preferences are associated with larger margins between responses,
or they rely on noisy margin information derived from ratings. We argue that
modeling the strength of preferences can lead to better generalization and more
faithful alignment. Furthermore, many existing methods that use adaptive
margins assume access to accurate preference scores, which can be difficult for
humans to provide reliably. We propose an approach that leverages preferences
over preferences, that is annotations indicating which of two preferences
reflects a stronger distinction. We use this ordinal signal to infer adaptive
margins on a per-datapoint basis. We introduce an extension to Direct
Preference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from
preference-over-preference supervision, enabling improved discriminative and
generative performance. Empirically, our method outperforms vanilla DPO, DPO
with fixed margins, and DPO with ground-truth margins on the UltraFeedback
dataset. Additionally, we show that there is a tradeoff between discriminative
and generative performance: improving test classification accuracy,
particularly by correctly labeling weaker preferences at the expense of
stronger ones, can lead to a decline in generative quality. To navigate this
tradeoff, we propose two sampling strategies to gather
preference-over-preference labels: one favoring discriminative performance and
one favoring generative performance.

</details>


### [210] [Cell2Text: Multimodal LLM for Generating Single-Cell Descriptions from RNA-Seq Data](https://arxiv.org/abs/2509.24840)
*Oussama Kharouiche,Aris Markogiannakis,Xiao Fei,Michail Chatzianastasis,Michalis Vazirgiannis*

Main category: cs.LG

TL;DR: 介绍Cell2Text框架，将scRNA - seq数据转化为自然语言描述，表现优于基线，证明结合表达数据与自然语言有优势。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞基础模型局限于离散预测头，无法捕捉生物学家所需的丰富上下文解释。

Method: 引入Cell2Text多模态生成框架，集成单细胞基础模型的基因级嵌入和预训练大语言模型。

Result: Cell2Text在分类准确性上优于基线，在本体一致性和文本生成语义保真度上表现良好。

Conclusion: 结合表达数据与自然语言能带来更强预测性能和可解释输出，为未见细胞的高效表征提供可扩展路径。

Abstract: Single-cell RNA sequencing has transformed biology by enabling the
measurement of gene expression at cellular resolution, providing information
for cell types, states, and disease contexts. Recently, single-cell foundation
models have emerged as powerful tools for learning transferable representations
directly from expression profiles, improving performance on classification and
clustering tasks. However, these models are limited to discrete prediction
heads, which collapse cellular complexity into predefined labels that fail to
capture the richer, contextual explanations biologists need. We introduce
Cell2Text, a multimodal generative framework that translates scRNA-seq profiles
into structured natural language descriptions. By integrating gene-level
embeddings from single-cell foundation models with pretrained large language
models, Cell2Text generates coherent summaries that capture cellular identity,
tissue origin, disease associations, and pathway activity, generalizing to
unseen cells. Empirically, Cell2Text outperforms baselines on classification
accuracy, demonstrates strong ontological consistency using PageRank-based
similarity metrics, and achieves high semantic fidelity in text generation.
These results demonstrate that coupling expression data with natural language
offers both stronger predictive performance and inherently interpretable
outputs, pointing to a scalable path for label-efficient characterization of
unseen cells.

</details>


### [211] [PATCH: Learnable Tile-level Hybrid Sparsity for LLMs](https://arxiv.org/abs/2509.23410)
*Younes Hourri,Mohammad Mozaffari,Maryam Mehri Dehnavi*

Main category: cs.LG

TL;DR: 提出混合稀疏框架PATCH，实现0% - 50%连续稀疏率，在多模型上缩小与密集模型准确率差距并实现加速。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署时内存和计算成本高，现有模型剪枝方法存在无法兼顾准确率和GPU加速的问题。

Method: 引入PATCH框架，将权重矩阵分块，通过可学习的掩码选择机制分配每块为密集或2:4稀疏。

Result: 在0.5B - 8B参数模型上，PATCH缩小与密集模型准确率差距并实现加速，如在LLaMA - 2 7B上比密集基线实现1.18x - 1.38x端到端加速，比2:4剪枝方法MaskLLM准确率提升0.37% - 2.96%。

Conclusion: PATCH框架能在准确率和加速之间实现细粒度控制，整体效果更优。

Abstract: Large language models (LLMs) deliver impressive performance but incur
prohibitive memory and compute costs at deployment. Model pruning is an
effective way to reduce these overheads, yet existing approaches face
challenges: unstructured sparsity, where nonzeros can appear anywhere,
preserves accuracy but yields irregular access patterns that prevent GPU
acceleration, while semi-structured 2:4 sparsity is hardware-friendly but
enforces a rigid 50% pattern that degrades model quality. To bridge this gap,
we introduce PATCH, a hybrid sparsity framework that enables a continuous
sparsity ratio between 0% and 50%. PATCH partitions weight matrices into tiles,
assigning each tile to be either dense or 2:4 sparse via a learnable mask
selection mechanism. This design provides fine-grained control over
accuracy-acceleration tradeoffs and supports non-uniform sparsity across
layers, leading to superior overall quality. Across models from 0.5B to 8B
parameters, PATCH consistently narrows the gap to dense accuracy while
delivering practical speedups. For instance, on LLaMA-2 7B with an A6000 GPU,
PATCH achieves 1.18x-1.38x end-to-end speedup over dense baselines while
improving accuracy by 0.37%-2.96% compared to the state-of-the-art 2:4 pruning
method, MaskLLM.

</details>


### [212] [Observation-Free Attacks on Online Learning to Rank](https://arxiv.org/abs/2509.22855)
*Sameep Chattopadhyay,Nikhil Karamchandani,Sharayu Mohair*

Main category: cs.LG

TL;DR: 提出攻击常用在线学习排序算法的框架，包含两种攻击策略并给出理论保证和实证结果


<details>
  <summary>Details</summary>
Motivation: 现有在线学习排序算法对协同对抗攻击的易感性理解不足

Method: 提出攻击框架，设计CascadeOFA和PBMOFA两种攻击策略

Result: 理论上两种策略仅需O(log T)次操作即可成功，有真实数据的实证结果

Conclusion: 提出的框架和策略可有效攻击常用在线学习排序算法

Abstract: Online learning to rank (OLTR) plays a critical role in information retrieval
and machine learning systems, with a wide range of applications in search
engines and content recommenders. However, despite their extensive adoption,
the susceptibility of OLTR algorithms to coordinated adversarial attacks
remains poorly understood. In this work, we present a novel framework for
attacking some of the widely used OLTR algorithms. Our framework is designed to
promote a set of target items so that they appear in the list of top-K
recommendations for T - o(T) rounds, while simultaneously inducing linear
regret in the learning algorithm. We propose two novel attack strategies:
CascadeOFA for CascadeUCB1 and PBMOFA for PBM-UCB . We provide theoretical
guarantees showing that both strategies require only O(log T) manipulations to
succeed. Additionally, we supplement our theoretical analysis with empirical
results on real-world data.

</details>


### [213] [Neighborhood Sampling Does Not Learn the Same Graph Neural Network](https://arxiv.org/abs/2509.22868)
*Zehao Niu,Mihai Anitescu,Jie Chen*

Main category: cs.LG

TL;DR: 使用神经切线核理论分析邻域采样方法，发现有限样本下后验不同但样本量增加时收敛，后验协方差不可比。


<details>
  <summary>Details</summary>
Motivation: 邻域采样是大规模图神经网络训练重要部分，但系统行为理解不足。

Method: 使用神经切线核工具，研究几种既定邻域采样方法及对应后验高斯过程。

Result: 有限样本下后验不同，样本量增加时收敛，后验协方差不可比。

Conclusion: 没有一种采样方法占主导地位。

Abstract: Neighborhood sampling is an important ingredient in the training of
large-scale graph neural networks. It suppresses the exponential growth of the
neighborhood size across network layers and maintains feasible memory
consumption and time costs. While it becomes a standard implementation in
practice, its systemic behaviors are less understood. We conduct a theoretical
analysis by using the tool of neural tangent kernels, which characterize the
(analogous) training dynamics of neural networks based on their infinitely wide
counterparts -- Gaussian processes (GPs). We study several established
neighborhood sampling approaches and the corresponding posterior GP. With
limited samples, the posteriors are all different, although they converge to
the same one as the sample size increases. Moreover, the posterior covariance,
which lower-bounds the mean squared prediction error, is uncomparable, aligning
with observations that no sampling approach dominates.

</details>


### [214] [From Noise to Knowledge: A Comparative Study of Acoustic Anomaly Detection Models in Pumped-storage Hydropower Plants](https://arxiv.org/abs/2509.22881)
*Karim Khamaisi,Nicolas Keller,Stefan Krummenacher,Valentin Huber,Bernhard Fässler,Bruno Rodrigues*

Main category: cs.LG

TL;DR: 本文对水电厂基于声学的异常检测方法进行对比分析，在高噪声条件下处理声学数据并提取特征，用三个机器学习模型在两个真实数据集上测试，OC - SVM 表现最佳。


<details>
  <summary>Details</summary>
Motivation: 工业工厂和能源生产中计划外停机成本高且难维护，现有声学异常检测研究多依赖通用或合成数据集，针对水电厂的研究少，旨在改进水电厂的预测性维护。

Method: 解决高噪声条件下声学预处理的关键挑战，提取时域和频域特征，对 LSTM AE、K - Means 和 OC - SVM 三个机器学习模型在奥地利 Rodundwerk II 抽水蓄能电站的两个真实数据集上进行测试。

Result: One - Class SVM 在准确率（ROC AUC 0.966 - 0.998）和最小训练时间上取得最佳平衡；LSTM 自编码器检测效果好（ROC AUC 0.889 - 0.997），但计算成本高。

Conclusion: 未明确提及最终结论，但可知 OC - SVM 在水电厂声学异常检测中有较好表现。

Abstract: In the context of industrial factories and energy producers, unplanned
outages are highly costly and difficult to service. However, existing
acoustic-anomaly detection studies largely rely on generic industrial or
synthetic datasets, with few focused on hydropower plants due to limited
access. This paper presents a comparative analysis of acoustic-based anomaly
detection methods, as a way to improve predictive maintenance in hydropower
plants. We address key challenges in the acoustic preprocessing under highly
noisy conditions before extracting time- and frequency-domain features. Then,
we benchmark three machine learning models: LSTM AE, K-Means, and OC-SVM, which
are tested on two real-world datasets from the Rodundwerk II pumped-storage
plant in Austria, one with induced anomalies and one with real-world
conditions. The One-Class SVM achieved the best trade-off of accuracy (ROC AUC
0.966-0.998) and minimal training time, while the LSTM autoencoder delivered
strong detection (ROC AUC 0.889-0.997) at the expense of higher computational
cost.

</details>


### [215] [T-TAMER: Provably Taming Trade-offs in ML Serving](https://arxiv.org/abs/2509.22992)
*Yuanyuan Yang,Ruimin Zhang,Jamie Morgenstern,Haifeng Xu*

Main category: cs.LG

TL;DR: 提出T - Tamer框架解决机器学习模型服务权衡问题，证明召回对性能保证的必要性和充分性，实验验证召回策略有效。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型规模和复杂度增加，多模型服务权衡问题复杂，现有策略多为启发式且缺乏理论保证和通用性。

Method: 提出T - Tamer框架，将问题形式化为多阶段决策过程，研究召回策略。

Result: 实验表明召回策略能持续实现高效的准确率 - 延迟权衡。

Conclusion: 为早期退出和级联模型设计提供了连接启发式实践和理论保证的原则性基础。

Abstract: As machine learning models continue to grow in size and complexity, efficient
serving faces increasingly broad trade-offs spanning accuracy, latency,
resource usage, and other objectives. Multi-model serving further complicates
these trade-offs; for example, in cascaded models, each early-exit decision
balances latency reduction against potential accuracy loss. Despite the
pervasiveness and importance of such trade-offs, current strategies remain
largely heuristic and case-specific, limiting both their theoretical guarantees
and general applicability.
  We present a general framework, T-Tamer, which formalizes this setting as a
multi-stage decision process, where the objective is to determine both when to
exit and which model to consult. Our main result shows that recall (i.e., the
ability to revisit earlier models) is both necessary and sufficient for
achieving provable performance guarantees. In particular, we prove that
strategies without recall cannot obtain any constant-factor approximation to
the optimal trade-off, whereas recall-based strategies provably attain the
optimal trade-off in polynomial time.
  We validate our analysis through experiments on synthetic datasets and
early-exit workloads for vision and NLP benchmarks. The results show that
recall-based strategies consistently yield efficient accuracy-latency
trade-offs. We hope this work provides a principled foundation for bridging
heuristic practice with theoretical guarantees in the design of early-exit and
cascaded models.

</details>


### [216] [FedCF: Fair Federated Conformal Prediction](https://arxiv.org/abs/2509.22907)
*Anutam Srinivasan,Aditya T. Vadlamani,Amin Meghrazi,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 将Conformal Fairness框架扩展到联邦学习设置，通过分析不同人口群体公平相关差距审计联邦模型公平性并实验验证。


<details>
  <summary>Details</summary>
Motivation: 标准Conformal Prediction对数据集中敏感属性不敏感，近期工作尝试将公平性融入CP，作者想将Conformal Fairness框架扩展到联邦学习场景并审计公平性。

Method: 将CF框架扩展到联邦学习设置，分析不同人口群体公平相关差距，利用可交换性假设，在多领域多个数据集上实验。

Result: 文中未明确提及具体实验结果。

Conclusion: 通过实验验证了所提出框架的可行性。

Abstract: Conformal Prediction (CP) is a widely used technique for quantifying
uncertainty in machine learning models. In its standard form, CP offers
probabilistic guarantees on the coverage of the true label, but it is agnostic
to sensitive attributes in the dataset. Several recent works have sought to
incorporate fairness into CP by ensuring conditional coverage guarantees across
different subgroups. One such method is Conformal Fairness (CF). In this work,
we extend the CF framework to the Federated Learning setting and discuss how we
can audit a federated model for fairness by analyzing the fairness-related gaps
for different demographic groups. We empirically validate our framework by
conducting experiments on several datasets spanning multiple domains, fully
leveraging the exchangeability assumption.

</details>


### [217] [Guided Manifold Alignment with Geometry-Regularized Twin Autoencoders](https://arxiv.org/abs/2509.22913)
*Jake S. Rhodes,Adam G. Rustad,Marshall S. Nielsen,Morgan Chase McClellan,Dallan Gardner,Dawson Hedges*

Main category: cs.LG

TL;DR: 提出利用几何正则化孪生自编码器架构的引导式表示学习框架，可增强流形对齐并泛化到未见数据，在多方面有改进且应用于阿尔茨海默病诊断提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 许多传统流形对齐方法无法进行样本外扩展，限制了实际应用。

Method: 提出引导式表示学习框架，利用几何正则化孪生自编码器架构，执行结构化跨模态映射，结合预训练对齐模型和多任务学习公式。

Result: 在嵌入一致性、信息保留和跨域转移方面有改进，应用于阿尔茨海默病诊断能提升预测准确性。

Conclusion: 该框架能增强流形对齐，泛化到未见数据，可有效集成多模态患者数据提升预测效果。

Abstract: Manifold alignment (MA) involves a set of techniques for learning shared
representations across domains, yet many traditional MA methods are incapable
of performing out-of-sample extension, limiting their real-world applicability.
We propose a guided representation learning framework leveraging a
geometry-regularized twin autoencoder (AE) architecture to enhance MA while
enabling generalization to unseen data. Our method enforces structured
cross-modal mappings to maintain geometric fidelity in learned embeddings. By
incorporating a pre-trained alignment model and a multitask learning
formulation, we improve cross-domain generalization and representation
robustness while maintaining alignment fidelity. We evaluate our approach using
several MA methods, showing improvements in embedding consistency, information
preservation, and cross-domain transfer. Additionally, we apply our framework
to Alzheimer's disease diagnosis, demonstrating its ability to integrate
multi-modal patient data and enhance predictive accuracy in cases limited to a
single domain by leveraging insights from the multi-modal problem.

</details>


### [218] [Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective](https://arxiv.org/abs/2509.22921)
*Matthieu Zimmer,Xiaotong Ji,Tu Nguyen,Haitham Bou Ammar*

Main category: cs.LG

TL;DR: 提出将大语言模型蒸馏表述为约束强化学习问题的新方法，实验表明该方法在数学推理任务上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型蒸馏方法在整合特定任务奖励时依赖临时奖励加权，缺乏原则性优化框架。

Method: 将约束状态增强强化学习应用于蒸馏场景，提出修改后的奖励函数，无需状态增强、部署时无需访问教师模型，且无对偶拉格朗日方法的计算开销。

Result: 在数学推理任务实验中，相比软拉格朗日松弛基线，该方法约束满足率更高、推理能力更好，且任务性能有竞争力。

Conclusion: 该框架为资源受限环境下奖励感知蒸馏提供了理论可靠且实际高效的解决方案。

Abstract: We introduce a novel approach to large language model (LLM) distillation by
formulating it as a constrained reinforcement learning problem. While recent
work has begun exploring the integration of task-specific rewards into
distillation processes, existing methods typically rely on ad-hoc reward
weighting. We propose a principled optimization framework that maximizes
task-specific rewards while constraining the divergence from the teacher model
to remain below a specified threshold. Our approach adapts constrained state
augmented reinforcement learning to the distillation setting, introducing a
modified reward function that maintains theoretical guarantees of constraint
satisfaction without requiring state augmentation or teacher model access
during deployment and without the computational overhead of the dual Lagrangian
methods. Through extensive experiments on mathematical reasoning tasks, we
demonstrate that our method achieves better constraint satisfaction rates and
better reasoning compared to the soft Lagrangian relaxation baselines while
maintaining competitive task performance. Our framework provides a
theoretically grounded and practically efficient solution for reward-aware
distillation in resource-constrained settings.

</details>


### [219] [MonoCon: A general framework for learning ultra-compact high-fidelity representations using monotonicity constraints](https://arxiv.org/abs/2509.22931)
*Shreyas Gokhale*

Main category: cs.LG

TL;DR: 本文介绍MonoCon框架，利用功能约束学习高质量表征，在图像和句子任务上表现出色，提供统一解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决学习高质量、鲁棒、高效和解耦表征这一人工智能核心挑战，传统方法主要用架构和优化约束，本文引入功能约束方法。

Method: 提出MonoCon框架，将小型单调多层感知机（MLP）头连接到预训练编码器，通过对比损失和单调性约束引导编码器与头的共同适应。

Result: 在CIFAR - 100图像分类任务中，表征紧凑度近9倍、鲁棒性1.5倍于微调编码器基线，保留99%的5 - NN分类准确率；在SNLI句子相似度任务中，表征紧凑度3.4倍、鲁棒性1.4倍，STSb分数略有降低。

Conclusion: MonoCon是通用的、与领域无关的框架，通过功能约束学习的表征为不同场景的关键挑战提供统一解决方案。

Abstract: Learning high-quality, robust, efficient, and disentangled representations is
a central challenge in artificial intelligence (AI). Deep metric learning
frameworks tackle this challenge primarily using architectural and optimization
constraints. Here, we introduce a third approach that instead relies on
$\textit{functional}$ constraints. Specifically, we present MonoCon, a simple
framework that uses a small monotonic multi-layer perceptron (MLP) head
attached to any pre-trained encoder. Due to co-adaptation between encoder and
head guided by contrastive loss and monotonicity constraints, MonoCon learns
robust, disentangled, and highly compact embeddings at a practically negligible
performance cost. On the CIFAR-100 image classification task, MonoCon yields
representations that are nearly 9x more compact and 1.5x more robust than the
fine-tuned encoder baseline, while retaining 99\% of the baseline's 5-NN
classification accuracy. We also report a 3.4x more compact and 1.4x more
robust representation on an SNLI sentence similarity task for a marginal
reduction in the STSb score, establishing MonoCon as a general domain-agnostic
framework. Crucially, these robust, ultra-compact representations learned via
functional constraints offer a unified solution to critical challenges in
disparate contexts ranging from edge computing to cloud-scale retrieval.

</details>


### [220] [Compute-Optimal Quantization-Aware Training](https://arxiv.org/abs/2509.22935)
*Aleksandr Dremov,David Grangier,Angelos Katharopoulos,Awni Hannun*

Main category: cs.LG

TL;DR: 研究量化感知训练（QAT）中全精度（FP）和QAT阶段计算资源的最优分配，得出损失最优比例与总计算量的关系，推导损失缩放定律，还提出新方法节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 先前研究虽表明分FP和QAT阶段训练效果好，但两者间计算资源的最优分配不明确。

Method: 进行不同计算预算、QAT位宽和模型大小的实验，推导损失缩放定律，提出冷却和QAT融合方法。

Result: 损失最优的QAT与FP训练比例随总计算量增加；可用tokens - per - parameter - byte统计准确预测最优比例；验证了基于缩放定律的预测；新方法节省计算资源。

Conclusion: 研究为高效QAT规划提供实用见解，可在相同计算预算下训练更高质量量化模型。

Abstract: Quantization-aware training (QAT) is a leading technique for improving the
accuracy of quantized neural networks. Previous work has shown that decomposing
training into a full-precision (FP) phase followed by a QAT phase yields
superior accuracy compared to QAT alone. However, the optimal allocation of
compute between the FP and QAT phases remains unclear. We conduct extensive
experiments with various compute budgets, QAT bit widths, and model sizes from
86.0M to 2.2B to investigate how different QAT durations impact final
performance. We demonstrate that, contrary to previous findings, the
loss-optimal ratio of QAT to FP training increases with the total amount of
compute. Moreover, the optimal fraction can be accurately predicted for a wide
range of model sizes and quantization widths using the
tokens-per-parameter-byte statistic. From experimental data, we derive a loss
scaling law that predicts both optimal QAT ratios and final model performance
across different QAT/FP compute allocation strategies and QAT bit widths. We
use the scaling law to make further predictions, which we verify
experimentally, including which QAT bit width is optimal under a given memory
constraint and how QAT accuracy with different bit widths compares to
full-precision model accuracy. Additionally, we propose a novel cooldown and
QAT fusion approach that performs learning rate decay jointly with
quantization-aware training, eliminating redundant full-precision model updates
and achieving significant compute savings. These findings provide practical
insights into efficient QAT planning and enable the training of higher-quality
quantized models with the same compute budget.

</details>


### [221] [Sample-efficient Multiclass Calibration under $\ell_{p}$ Error](https://arxiv.org/abs/2509.23000)
*Konstantina Bairaktari,Huy L. Nguyen*

Main category: cs.LG

TL;DR: 提出新校准误差定义，算法能用多项式数量样本校准预测器，在一端点改进前人工作，有新的自适应数据分析应用。


<details>
  <summary>Details</summary>
Motivation: 多类预测器校准因可能预测值数量呈指数级而具挑战性。

Method: 提出新的校准误差定义，该定义在两种既定校准误差概念间进行插值；运用具有高适应性且样本复杂度仅对数开销的自适应数据分析。

Result: 算法能为整个插值范围（除一个端点）校准任何给定预测器，仅需多项式数量样本；在另一个端点，实现了对误差参数近乎最优的依赖，改进了先前工作。

Conclusion: 新定义和算法在多类预测器校准上有良好效果，新的自适应数据分析应用是关键技术贡献。

Abstract: Calibrating a multiclass predictor, that outputs a distribution over labels,
is particularly challenging due to the exponential number of possible
prediction values. In this work, we propose a new definition of calibration
error that interpolates between two established calibration error notions, one
with known exponential sample complexity and one with polynomial sample
complexity for calibrating a given predictor. Our algorithm can calibrate any
given predictor for the entire range of interpolation, except for one endpoint,
using only a polynomial number of samples. At the other endpoint, we achieve
nearly optimal dependence on the error parameter, improving upon previous work.
A key technical contribution is a novel application of adaptive data analysis
with high adaptivity but only logarithmic overhead in the sample complexity.

</details>


### [222] [Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning](https://arxiv.org/abs/2509.24372)
*Xin Qiu,Yulu Gan,Conor F. Hayes,Qiyao Liang,Elliot Meyerson,Babak Hodjat,Risto Miikkulainen*

Main category: cs.LG

TL;DR: 本文首次成功将进化策略（ES）扩展到对大语言模型（LLM）全参数微调，表现优于现有强化学习（RL）微调方法，为LLM微调开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 强化学习是主流的大语言模型微调方法，而进化策略曾因扩展性问题被忽视，作者希望探索其在大模型上的应用潜力。

Method: 将进化策略扩展用于大语言模型全参数的微调。

Result: 进化策略能在数十亿参数上高效搜索，在样本效率、对长时奖励的耐受性等多方面优于现有强化学习微调方法。

Conclusion: 进化策略为大语言模型微调提供了超越当前强化学习技术的新方向。

Abstract: Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is
a critical step in the AI deployment pipeline. Reinforcement learning (RL) is
arguably the most prominent fine-tuning method, contributing to the birth of
many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once
showed comparable performance to RL on models with a few million parameters,
was neglected due to the pessimistic perception of its scalability to larger
models. In this work, we report the first successful attempt to scale up ES for
fine-tuning the full parameters of LLMs, showing the surprising fact that ES
can search efficiently over billions of parameters and outperform existing RL
fine-tuning methods in multiple respects, including sample efficiency,
tolerance to long-horizon rewards, robustness to different base LLMs, less
tendency to reward hacking, and more stable performance across runs. It
therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond
what current RL techniques provide. The source codes are provided at:
https://github.com/VsonicV/es-fine-tuning-paper.

</details>


### [223] [Understanding SOAP from the Perspective of Gradient Whitening](https://arxiv.org/abs/2509.22938)
*Yanqing Lu,Letao Wang,Jinbo Liu*

Main category: cs.LG

TL;DR: 从梯度白化角度分析Adam、Shampoo和SOAP优化算法，理论上建立SOAP和Shampoo等价性，实验显示SOAP收敛率与Shampoo相似，最终损失无显著优势。


<details>
  <summary>Details</summary>
Motivation: 分析Adam、Shampoo和SOAP优化算法，从梯度白化角度理解其预条件矩阵。

Method: 从梯度白化角度分析算法，建立理论等价性，用nanoGPT和灰度图像着色实验评估。

Result: SOAP收敛率与Shampoo相似，最终损失相比Adam和Shampoo无显著优势。

Conclusion: 实验结果与SOAP和Shampoo理论等价性相符。

Abstract: Shampoo with Adam in the Preconditioner's eigenbasis (SOAP) has recently
emerged as a promising optimization algorithm for neural network training,
achieving superior training efficiency over both Adam and Shampoo in language
modeling tasks. In this work, we analyze Adam, Shampoo, and SOAP from the
perspective of gradient whitening, interpreting their preconditioners as
approximations to the whitening matrix, which captures second-order curvature
information. We further establish a theoretical equivalence between idealized
versions of SOAP and Shampoo under the Kronecker product assumption. To
empirically evaluate these insights, we reproduce the language modeling
experiments using nanoGPT and grayscale image colorization. Our results show
that SOAP exhibits similar convergence rate as Shampoo, and no significant
advantage over both Adam and Shampoo in the final loss achieved, which aligns
with their equivalence in theory.

</details>


### [224] [EOE: Evolutionary Optimization of Experts for Training Language Models](https://arxiv.org/abs/2509.24436)
*Yingshi Chen*

Main category: cs.LG

TL;DR: 提出大语言模型训练的进化框架，分专家训练，用进化算子学习，实验显示可减模型大小、提吞吐量，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型训练中内存需求大、推理模型尺寸大等问题。

Method: 将模型分为多个专家，每次只训练一个专家，用AdamW优化后，用进化算子让当前专家学习最佳专家经验。

Result: 最佳专家与全模型精度相近，大幅减小推理模型尺寸，训练内存需求少、吞吐量加速超10倍。

Conclusion: 该进化框架有效，代码适合在PC和边缘计算设备部署。

Abstract: This paper presents an evolutionary framework for the training of large
language models(LLM). The models are divided into several
experts(sub-networks), which have the same structure but different parameter
values. Only one expert is trained at each step. After the classical AdamW
optimization, some evolutionary operators(crossover, PSO, and mutation) act on
the tensor weights between the current expert and the best expert. So current
expert would learn the experience of best expert. The direction of best expert
would help current expert's loss decrease faster. Finally, only save the weight
of the best expert. Experiments show that best expert would achieve nearly the
same accuracy as the full model. This would greatly reduce the size of the
model for inference. Since only one expert is trained at each step, the
training needs much less memory and has much higher throughput. Experiments
show that the throughput would accelerate more than ten times! Our source code
is available. It's a pure c++/cu framework, which is suitable for easy
deployment on PCs and edge computing devices.

</details>


### [225] [SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights](https://arxiv.org/abs/2509.22944)
*Lorenz K. Müller,Philippe Bich,Jiawei Zhuang,Ahmet Çelik,Luca Benfenati,Lukas Cavigelli*

Main category: cs.LG

TL;DR: 提出SINQ方法增强现有后训练量化器，在Qwen3和DeepSeek - V2.5上评估，能显著改善困惑度，代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前后训练量化方法在比特宽度小于等于4时困惑度下降，尤其是无校准的均匀量化方法，因表示离群值导致精度问题。

Method: 引入SINQ，增加第二轴比例因子，使用快速Sinkhorn - Knopp风格算法找到归一化行和列方差的比例，最小化矩阵不平衡。

Result: 在Qwen3和DeepSeek - V2.5模型上，SINQ显著改善WikiText2和C4困惑度，结合校准和非均匀量化级别可进一步提升。

Conclusion: SINQ方法有效，能轻松应用于新架构量化线性层，可提高后训练量化性能。

Abstract: Post-training quantization has emerged as the most widely used strategy for
deploying large language models at low precision. Still, current methods show
perplexity degradation at bit-widths less than or equal to 4, partly because
representing outliers causes precision issues in parameters that share the same
scales as these outliers. This problem is especially pronounced for
calibration-free, uniform quantization methods. We introduce SINQ to augment
existing post-training quantizers with an additional second-axis scale factor
and a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize
per-row and per-column variances, thereby minimizing a novel per-matrix proxy
target for quantization: the matrix imbalance. Our method has no interactions
between layers and can be trivially applied to new architectures to quantize
any linear layers. We evaluate our method on the Qwen3 model family and
DeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against
uncalibrated uniform quantization baselines and can be further enhanced by
combining it with calibration and non-uniform quantization levels. Code to
reproduce the results of this work and to easily quantize models using SINQ is
available at https://github.com/huawei-csl/SINQ.

</details>


### [226] [Who invented deep residual learning?](https://arxiv.org/abs/2509.24732)
*Juergen Schmidhuber*

Main category: cs.LG

TL;DR: 介绍现代AI基于深度神经网络，提及2025年最具引用量的文章，呈现深度残差学习的发展时间线


<details>
  <summary>Details</summary>
Motivation: 探究深度残差学习中带残差连接的深度残差学习的发明者

Method: 梳理并呈现深度残差学习的发展时间线

Result: 未提及具体结果

Conclusion: 未提及具体结论

Abstract: Modern AI is based on deep artificial neural networks (NNs). As of 2025, the
most cited scientific article of the 21st century is an NN paper on deep
residual learning with residual connections. Who invented this? We present a
timeline of the evolution of deep residual learning.

</details>


### [227] [Meta-Learning Fourier Neural Operators for Hessian Inversion and Enhanced Variational Data Assimilation](https://arxiv.org/abs/2509.22949)
*Hamidreza Moazzami,Asma Jamali,Nicholas Kevlahan,Rodrigo A. Vargas-Hernández*

Main category: cs.LG

TL;DR: 提出用FNO元学习框架近似逆Hessian算子以优化共轭梯度法，线性平流方程实验显示有效


<details>
  <summary>Details</summary>
Motivation: 变分数据同化方法在涉及Hessian信息时计算成本高，需解决此挑战

Method: 提出用Fourier Neural Operator近似逆Hessian算子，为共轭梯度法提供有效初始化的元学习框架

Result: 线性平流方程数值实验中，FNO - CG方法对比标准CG平均相对误差降低62%，迭代次数减少17%

Conclusion: FNO - CG方法在难处理的数据同化问题中具有鲁棒性和高效性

Abstract: Data assimilation (DA) is crucial for enhancing solutions to partial
differential equations (PDEs), such as those in numerical weather prediction,
by optimizing initial conditions using observational data. Variational DA
methods are widely used in oceanic and atmospheric forecasting, but become
computationally expensive, especially when Hessian information is involved. To
address this challenge, we propose a meta-learning framework that employs the
Fourier Neural Operator (FNO) to approximate the inverse Hessian operator
across a family of DA problems, thereby providing an effective initialization
for the conjugate gradient (CG) method. Numerical experiments on a linear
advection equation demonstrate that the resulting FNO-CG approach reduces the
average relative error by $62\%$ and the number of iterations by $17\%$
compared to the standard CG. These improvements are most pronounced in
ill-conditioned scenarios, highlighting the robustness and efficiency of FNO-CG
for challenging DA problems.

</details>


### [228] [GDR-learners: Orthogonal Learning of Generative Models for Potential Outcomes](https://arxiv.org/abs/2509.22953)
*Valentyn Melnychuk,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 本文提出生成式Neyman正交（双稳健）学习器GDR - learners估计潜在结果条件分布，基于多种模型实现，具准神谕效率和双稳健性，实验显示其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型估计潜在结果分布时缺乏一般Neyman正交性及相关准神谕效率和双稳健性，需新方法。

Method: 引入GDR - learners，可基于条件归一化流、条件生成对抗网络、条件变分自编码器和条件扩散模型等实现。

Result: 在（半）合成实验中，GDR - learners在估计潜在结果条件分布上非常有效，优于现有方法。

Conclusion: GDR - learners具备准神谕效率和双稳健性，渐近最优，是估计潜在结果条件分布的有效方法。

Abstract: Various deep generative models have been proposed to estimate potential
outcomes distributions from observational data. However, none of them have the
favorable theoretical property of general Neyman-orthogonality and, associated
with it, quasi-oracle efficiency and double robustness. In this paper, we
introduce a general suite of generative Neyman-orthogonal (doubly-robust)
learners that estimate the conditional distributions of potential outcomes. Our
proposed GDR-learners are flexible and can be instantiated with many
state-of-the-art deep generative models. In particular, we develop GDR-learners
based on (a) conditional normalizing flows (which we call GDR-CNFs), (b)
conditional generative adversarial networks (GDR-CGANs), (c) conditional
variational autoencoders (GDR-CVAEs), and (d) conditional diffusion models
(GDR-CDMs). Unlike the existing methods, our GDR-learners possess the
properties of quasi-oracle efficiency and rate double robustness, and are thus
asymptotically optimal. In a series of (semi-)synthetic experiments, we
demonstrate that our GDR-learners are very effective and outperform the
existing methods in estimating the conditional distributions of potential
outcomes.

</details>


### [229] [Drift-Adapter: A Practical Approach to Near Zero-Downtime Embedding Model Upgrades in Vector Databases](https://arxiv.org/abs/2509.23471)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: 本文提出Drift - Adapter，可桥接模型版本间嵌入空间，继续使用现有ANN索引，减少重新计算成本和操作中断。


<details>
  <summary>Details</summary>
Motivation: 生产向量数据库中升级嵌入模型需重新编码语料和重建ANN索引，会造成重大操作中断和计算成本。

Method: 提出Drift - Adapter轻量级可学习转换层，对三种适配器参数化进行系统评估，在新旧嵌入对的小样本上训练。

Result: 在MTEB文本语料和CLIP图像模型升级实验中，Drift - Adapter恢复95 - 99%的检索召回率，增加不到10微秒查询延迟，比全重新索引或双索引服务等策略减少超100倍重新计算成本。

Conclusion: Drift - Adapter是敏捷模型部署的实用解决方案，分析了其对不同模型漂移的鲁棒性等方面。

Abstract: Upgrading embedding models in production vector databases typically requires
re-encoding the entire corpus and rebuilding the Approximate Nearest Neighbor
(ANN) index, leading to significant operational disruption and computational
cost. This paper presents Drift-Adapter, a lightweight, learnable
transformation layer designed to bridge embedding spaces between model
versions. By mapping new queries into the legacy embedding space, Drift-Adapter
enables the continued use of the existing ANN index, effectively deferring full
re-computation. We systematically evaluate three adapter parameterizations:
Orthogonal Procrustes, Low-Rank Affine, and a compact Residual MLP, trained on
a small sample of paired old and new embeddings. Experiments on MTEB text
corpora and a CLIP image model upgrade (1M items) show that Drift-Adapter
recovers 95-99% of the retrieval recall (Recall@10, MRR) of a full
re-embedding, adding less than 10 microseconds of query latency. Compared to
operational strategies like full re-indexing or dual-index serving,
Drift-Adapter reduces recompute costs by over 100 times and facilitates
upgrades with near-zero operational interruption. We analyze robustness to
varied model drift, training data size, scalability to billion-item systems,
and the impact of design choices like diagonal scaling, demonstrating
Drift-Adapter's viability as a pragmatic solution for agile model deployment.

</details>


### [230] [Doubly-Robust LLM-as-a-Judge: Externally Valid Estimation with Imperfect Personas](https://arxiv.org/abs/2509.22957)
*Luke Guerdan,Justin Whitehouse,Kimberly Truong,Kenneth Holstein,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: 随着生成式AI系统应用增加，评估外部有效性受关注，本文提出双稳健估计框架解决评估采样偏差问题，理论和模拟验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI评估的外部有效性受源样本与目标分布差异影响，存在评估采样偏差问题，需解决。

Method: 提出双稳健估计框架，结合LLM生成的“角色”评分和有采样偏差的人类评分，在特定模型质量足够时产生有效系统质量估计，并通过Persona Simulation Framework验证。

Result: 该框架在特定模型质量足够时能产生有效的系统质量估计，通过理论和模拟验证了框架的有效性。

Conclusion: 为结合不完美的角色评分和有采样偏差的人类评分以获得有效系统质量估计提供了原则性基础。

Abstract: As Generative AI (GenAI) systems see growing adoption, a key concern involves
the external validity of evaluations, or the extent to which they generalize
from lab-based to real-world deployment conditions. Threats to the external
validity of GenAI evaluations arise when the source sample of human raters and
system outputs used to obtain a system quality estimate differs from the target
distribution at deployment time. In this work, we propose a doubly-robust
estimation framework designed to address this evaluation sampling bias. Key to
our approach is the use of "persona" ratings produced by prompting an LLM
evaluator (i.e., an LLM-as-a-judge) to behave as a human rater with specific
sociodemographic characteristics. Our doubly-robust framework combines these
informative yet imperfect persona ratings with human ratings obtained under
evaluation sampling bias to produce statistically valid system quality
estimates. In particular, we show that our approach yields valid system quality
estimates when either (i) a model trained to predict human ratings using
persona ratings and source data observed under sampling bias, or (ii) a
reweighting model that corrects for sampling bias is of sufficient quality. We
validate our framework theoretically and via a novel Persona Simulation
Framework (PSF) designed to systematically manipulate persona quality and the
degree of evaluation sampling bias present in source data. Our work provides a
principled foundation for combining imperfect persona ratings with human
ratings observed under sampling bias to obtain valid system quality estimates.

</details>


### [231] [Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces](https://arxiv.org/abs/2509.22963)
*Haitong Ma,Ofir Nabati,Aviv Rosenberg,Bo Dai,Oran Lang,Idan Szpektor,Craig Boutilier,Na Li,Shie Mannor,Lior Shani,Guy Tenneholtz*

Main category: cs.LG

TL;DR: 本文提出用离散扩散模型训练策略的新框架，解决强化学习在大规模组合动作空间的扩展问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 强化学习在处理现实中常见的大规模组合动作空间时存在扩展性问题。

Method: 采用高效在线训练过程，利用策略镜像下降定义正则化目标策略分布，将策略更新视为分布匹配问题，训练扩散模型复制稳定目标。

Result: 在多个具有挑战性的组合基准测试中取得了最先进的结果和卓越的样本效率，扩散策略性能优于其他基线。

Conclusion: 所提出的用离散扩散模型训练策略的方法能稳定学习并显著提高训练性能。

Abstract: Reinforcement learning (RL) struggles to scale to large, combinatorial action
spaces common in many real-world problems. This paper introduces a novel
framework for training discrete diffusion models as highly effective policies
in these complex settings. Our key innovation is an efficient online training
process that ensures stable and effective policy improvement. By leveraging
policy mirror descent (PMD) to define an ideal, regularized target policy
distribution, we frame the policy update as a distributional matching problem,
training the expressive diffusion model to replicate this stable target. This
decoupled approach stabilizes learning and significantly enhances training
performance. Our method achieves state-of-the-art results and superior sample
efficiency across a diverse set of challenging combinatorial benchmarks,
including DNA sequence generation, RL with macro-actions, and multi-agent
systems. Experiments demonstrate that our diffusion policies attain superior
performance compared to other baselines.

</details>


### [232] [GBSK: Skeleton Clustering via Granular-ball Computing and Multi-Sampling for Large-Scale Data](https://arxiv.org/abs/2509.23742)
*Yewang Chen,Junfeng Li,Shuyin Xia,Qinghong Lai,Xinbo Gao,Guoyin Wang,Dongdong Cheng,Yi Liu,Yi Wang*

Main category: cs.LG

TL;DR: 提出可扩展骨架聚类算法GBSK及自适应版本AGBSK，能高效处理大规模数据集聚类任务，实验证明其高效性和强聚类性能。


<details>
  <summary>Details</summary>
Motivation: 有效处理大规模数据集的聚类任务。

Method: 提出GBSK算法，利用粒度球技术，通过多采样和构建多粒度粒度球挖掘数据“骨架”；引入自适应版本AGBSK简化参数设置。

Result: 在标准计算硬件上的实验表明，GBSK在大规模数据集上实现了高效率和强聚类性能。

Conclusion: GBSK和AGBSK算法在处理大规模数据集聚类任务上有良好表现，具有实用性和可部署性。

Abstract: To effectively handle clustering task for large-scale datasets, we propose a
novel scalable skeleton clustering algorithm, namely GBSK, which leverages the
granular-ball technique to capture the underlying structure of data. By
multi-sampling the dataset and constructing multi-grained granular-balls, GBSK
progressively uncovers a statistical "skeleton" -- a spatial abstraction that
approximates the essential structure and distribution of the original data.
This strategy enables GBSK to dramatically reduce computational overhead while
maintaining high clustering accuracy. In addition, we introduce an adaptive
version, AGBSK, with simplified parameter settings to enhance usability and
facilitate deployment in real-world scenarios. Extensive experiments conducted
on standard computing hardware demonstrate that GBSK achieves high efficiency
and strong clustering performance on large-scale datasets, including one with
up to 100 million instances across 256 dimensions. Our implementation and
experimental results are available at: https://github.com/XFastDataLab/GBSK/.

</details>


### [233] [Functional Critic Modeling for Provably Convergent Off-Policy Actor-Critic](https://arxiv.org/abs/2509.22964)
*Qinxun Bai,Yuxuan Han,Wei Xu,Zhengyuan Zhou*

Main category: cs.LG

TL;DR: 提出功能批评家建模概念，构建新的演员 - 批评家框架，解决脱策略演员 - 批评家学习的挑战，理论证明收敛性并实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有脱策略演员 - 批评家方法中批评家和演员学习存在挑战，如评估不稳定和策略梯度估计困难。

Method: 引入功能批评家建模概念构建新框架，在线性函数设置下进行理论分析，设计神经网络架构。

Result: 理论上证明框架可收敛，初步实验验证在深度思维控制基准的强化学习任务上有效。

Conclusion: 所提出的新框架是首个收敛的基于脱策略目标的演员 - 批评家算法，能解决现有方法的挑战。

Abstract: Off-policy reinforcement learning (RL) with function approximation offers an
effective way to improve sample efficiency by reusing past experience. Within
this setting, the actor-critic (AC) framework has achieved strong empirical
success. However, both the critic and actor learning is challenging for the
off-policy AC methods: first of all, in addition to the classic "deadly triad"
instability of off-policy evaluation, it also suffers from a "moving target"
problem, where the policy being evaluated changes continually; secondly, actor
learning becomes less efficient due to the difficulty of estimating the exact
off-policy policy gradient. The first challenge essentially reduces the problem
to repeatedly performing off-policy evaluation for changing policies. For the
second challenge, the off-policy policy gradient theorem requires a complex and
often impractical algorithm to estimate an additional emphasis critic, which is
typically neglected in practice, thereby reducing to the on-policy policy
gradient as an approximation. In this work, we introduce a novel concept of
functional critic modeling, which leads to a new AC framework that addresses
both challenges for actor-critic learning under the deadly triad setting. We
provide a theoretical analysis in the linear function setting, establishing the
provable convergence of our framework, which, to the best of our knowledge, is
the first convergent off-policy target-based AC algorithm. From a practical
perspective, we further propose a carefully designed neural network
architecture for the functional critic modeling and demonstrate its
effectiveness through preliminary experiments on widely used RL tasks from the
DeepMind Control Benchmark.

</details>


### [234] [Shape-Informed Clustering of Multi-Dimensional Functional Data via Deep Functional Autoencoders](https://arxiv.org/abs/2509.22969)
*Samuel V. Singh,Shirley Coyle,Mimi Zhang*

Main category: cs.LG

TL;DR: 介绍用于多维函数数据聚类分析的FAEclust框架，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决多维函数数据的聚类分析问题。

Method: 构建具有通用近似能力的编码器和解码器，采用正则化策略增强稳定性，引入聚类损失和形状信息聚类目标。

Result: 建立非线性解码器的通用近似性质，实验验证模型有效性。

Conclusion: FAEclust框架能有效进行多维函数数据的聚类分析。

Abstract: We introduce FAEclust, a novel functional autoencoder framework for cluster
analysis of multi-dimensional functional data, data that are random
realizations of vector-valued random functions. Our framework features a
universal-approximator encoder that captures complex nonlinear
interdependencies among component functions, and a universal-approximator
decoder capable of accurately reconstructing both Euclidean and manifold-valued
functional data. Stability and robustness are enhanced through innovative
regularization strategies applied to functional weights and biases.
Additionally, we incorporate a clustering loss into the network's training
objective, promoting the learning of latent representations that are conducive
to effective clustering. A key innovation is our shape-informed clustering
objective, ensuring that the clustering results are resistant to phase
variations in the functions. We establish the universal approximation property
of our non-linear decoder and validate the effectiveness of our model through
extensive experiments.

</details>


### [235] [OptiMind: Teaching LLMs to Think Like Optimization Experts](https://arxiv.org/abs/2509.22979)
*Zeyi Chen,Xinzhi Zhang,Humishka Zope,Hugo Barbalho,Konstantina Mellou,Marco Molinaro,Janardhan Kulkarni,Ishai Menache,Sirui Li*

Main category: cs.LG

TL;DR: 本文通过集成优化专业知识，改进训练数据清理和推理策略，提高混合整数线性规划的公式化准确性。


<details>
  <summary>Details</summary>
Motivation: 现有将自然语言转化为可执行优化模型的方法准确性有限，受限于稀缺嘈杂的训练数据且未利用领域知识。

Method: 先通过基于类的错误分析清理训练数据，再开发多轮推理策略，用特定类别的错误总结和求解器反馈引导大语言模型进行迭代改进。

Result: 在多个基础大语言模型上的实验表明，结合清理后的数据、领域知识提示和反馈，平均提高公式化准确性14个百分点。

Conclusion: 该方法有助于推动大语言模型辅助优化公式化的进一步发展。

Abstract: Mathematical programming -- the task of expressing operations and
decision-making problems in precise mathematical language -- is fundamental
across domains, yet remains a skill-intensive process requiring operations
research expertise. Recent advances in large language models for complex
reasoning have spurred interest in automating this task, translating natural
language into executable optimization models. Current approaches, however,
achieve limited accuracy, hindered by scarce and noisy training data without
leveraging domain knowledge. In this work, we systematically integrate
optimization expertise to improve formulation accuracy for mixed-integer linear
programming, a key family of mathematical programs. Our approach first cleans
training data through class-based error analysis to explicitly prevent common
mistakes within each optimization class. We then develop multi-turn inference
strategies that guide LLMs with class-specific error summaries and solver
feedback, enabling iterative refinement. Experiments across multiple base LLMs
demonstrate that combining cleaned data with domain-informed prompting and
feedback improves formulation accuracy by 14 percentage points on average,
enabling further progress toward robust LLM-assisted optimization formulation.

</details>


### [236] [MDP modeling for multi-stage stochastic programs](https://arxiv.org/abs/2509.22981)
*David P. Morton,Oscar Dowson,Bernardo K. Pagnoncelli*

Main category: cs.LG

TL;DR: 研究含MDP建模特征的多阶段随机规划，扩展策略图，用实例展示建模方法，开发新的随机对偶动态规划变体。


<details>
  <summary>Details</summary>
Motivation: 研究含MDP建模特征的多阶段随机规划，包括连续状态和动作空间的结构化MDP。

Method: 扩展策略图以包含与决策相关的不确定性和有限形式的统计学习，开发新的随机对偶动态规划变体并进行近似处理非凸性。

Result: 通过一系列复杂度递增的实例展示了建模方法的表达能力。

Conclusion: 未明确提及，但暗示新方法可用于解决相关多阶段随机规划问题。

Abstract: We study a class of multi-stage stochastic programs, which incorporate
modeling features from Markov decision processes (MDPs). This class includes
structured MDPs with continuous state and action spaces. We extend policy
graphs to include decision-dependent uncertainty for one-step transition
probabilities as well as a limited form of statistical learning. We focus on
the expressiveness of our modeling approach, illustrating ideas with a series
of examples of increasing complexity. As a solution method, we develop new
variants of stochastic dual dynamic programming, including approximations to
handle non-convexities.

</details>


### [237] [Analysis of Variational Autoencoders](https://arxiv.org/abs/2509.22994)
*Zachary Baker,Yuxiao Li*

Main category: cs.LG

TL;DR: 研究将变分方法引入稀疏自编码器（SAE）架构是否能改善特征组织和可解释性，提出变分稀疏自编码器（vSAE），但实验表明简单应用变分方法不能改善特征组织和可解释性。


<details>
  <summary>Details</summary>
Motivation: 探究将变分方法融入SAE架构能否提升特征组织和可解释性。

Method: 引入vSAE，用从学习的高斯后验随机采样替代确定性ReLU门控，并加入KL散度正则化；在Pythia - 70M变压器残差流激活上用SAE Bench等基准评估Topk vSAE和标准TopK SAE。

Result: vSAE在核心评估指标上不如标准SAE，但在特征独立性和消融指标上表现出色；KL散度项产生过度正则化压力，减少存活特征比例，导致性能下降；vSAE特征鲁棒性提升，但死特征更多。

Conclusion: 简单地将变分方法应用于SAE不能改善特征组织或可解释性。

Abstract: Sparse Autoencoders (SAEs) have emerged as a promising approach for
interpreting neural network representations by learning sparse,
human-interpretable features from dense activations. We investigate whether
incorporating variational methods into SAE architectures can improve feature
organization and interpretability. We introduce the variational Sparse
Autoencoder (vSAE), which replaces deterministic ReLU gating with stochastic
sampling from learned Gaussian posteriors and incorporates KL divergence
regularization toward a standard normal prior. Our hypothesis is that this
probabilistic sampling creates dispersive pressure, causing features to
organize more coherently in the latent space while avoiding overlap. We
evaluate a Topk vSAE against a standard TopK SAE on Pythia-70M transformer
residual steam activations using comprehensive benchmarks including SAE Bench,
individual feature interpretability analysis, and global latent space
visualization through t-SNE. The vSAE underperforms standard SAE across core
evaluation metrics, though excels at feature independence and ablation metrics.
The KL divergence term creates excessive regularization pressure that
substantially reduces the fraction of living features, leading to observed
performance degradation. While vSAE features demonstrate improved robustness,
they exhibit many more dead features than baseline. Our findings suggest that
naive application of variational methods to SAEs does not improve feature
organization or interpretability.

</details>


### [238] [Beyond Aggregation: Guiding Clients in Heterogeneous Federated Learning](https://arxiv.org/abs/2509.23049)
*Zijian Wang,Xiaofei Zhang,Xin Zhang,Yukun Liu,Qiong Zhang*

Main category: cs.LG

TL;DR: 本文提出联邦学习新范式，让服务器引导新任务分配，经实验验证有效，开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习算法主要关注聚合客户端模型更新，服务器潜力未充分挖掘，以医疗场景为例，探讨服务器能否引导新患者到合适医院。

Method: 引入基于经验似然的框架，同时实现学习有效本地模型和为新查询找到最佳匹配客户端两个目标。

Result: 在基准数据集上验证了框架有效性，与标准联邦学习方法相比，提高了模型准确率和客户端引导精度。

Conclusion: 该工作为构建更智能、资源高效的联邦系统开辟了新方向，可将数据异质性作为优势而非缺陷。

Abstract: Federated learning (FL) is increasingly adopted in domains like healthcare,
where data privacy is paramount. A fundamental challenge in these systems is
statistical heterogeneity-the fact that data distributions vary significantly
across clients (e.g., different hospitals may treat distinct patient
demographics). While current FL algorithms focus on aggregating model updates
from these heterogeneous clients, the potential of the central server remains
under-explored. This paper is motivated by a healthcare scenario: could a
central server not only build a model but also guide a new patient to the
hospital best equipped for their specific condition? We generalize this idea to
propose a novel paradigm for FL systems where the server actively guides the
allocation of new tasks or queries to the most appropriate client in the
network. To enable this, we introduce an empirical likelihood-based framework
that simultaneously addresses two goals: (1) learning effective local models on
each client, and (2) finding the best matching client for a new query.
Empirical results demonstrate the framework's effectiveness on benchmark
datasets, showing improvements in both model accuracy and the precision of
client guidance compared to standard FL approaches. This work opens a new
direction for building more intelligent and resource-efficient federated
systems that leverage heterogeneity as a feature, not just a bug. Code is
available at https://github.com/zijianwang0510/FedDRM.git.

</details>


### [239] [Towards Quantum-Ready Blockchain Fraud Detection via Ensemble Graph Neural Networks](https://arxiv.org/abs/2509.23101)
*M. Z. Haider,Tayyaba Noreen,M. Salman*

Main category: cs.LG

TL;DR: 本文提出集成GCN、GAT和GIN的框架用于区块链欺诈检测，在真实数据集上效果好，且架构支持量子集成。


<details>
  <summary>Details</summary>
Motivation: 区块链匿名性导致反洗钱执法困难，需要能捕捉结构和时间依赖、抗干扰的欺诈检测模型。

Method: 提出集成GCN、GAT和GIN的框架，采用软投票集成。

Result: 在Elliptic数据集上，调优的软投票集成实现高召回率，误报率低于1%，优于单个GNN模型和基线方法。

Conclusion: 集成GNN是实时加密货币监控的实用且有前瞻性的解决方案，兼具反洗钱效用和量子增强金融安全分析潜力。

Abstract: Blockchain Business applications and cryptocurrencies such as enable secure,
decentralized value transfer, yet their pseudonymous nature creates
opportunities for illicit activity, challenging regulators and exchanges in
anti money laundering (AML) enforcement. Detecting fraudulent transactions in
blockchain networks requires models that can capture both structural and
temporal dependencies while remaining resilient to noise, imbalance, and
adversarial behavior. In this work, we propose an ensemble framework that
integrates Graph Convolutional Networks (GCN), Graph Attention Networks (GAT),
and Graph Isomorphism Networks (GIN) to enhance blockchain fraud detection.
Using the real-world Elliptic dataset, our tuned soft voting ensemble achieves
high recall of illicit transactions while maintaining a false positive rate
below 1%, beating individual GNN models and baseline methods. The modular
architecture incorporates quantum-ready design hooks, allowing seamless future
integration of quantum feature mappings and hybrid quantum classical graph
neural networks. This ensures scalability, robustness, and long-term
adaptability as quantum computing technologies mature. Our findings highlight
ensemble GNNs as a practical and forward-looking solution for real-time
cryptocurrency monitoring, providing both immediate AML utility and a pathway
toward quantum-enhanced financial security analytics.

</details>


### [240] [Physically Plausible Multi-System Trajectory Generation and Symmetry Discovery](https://arxiv.org/abs/2509.23003)
*Jiayin Liu,Yulong Yang,Vineet Bansal,Christine Allen-Blanchette*

Main category: cs.LG

TL;DR: 本文提出Symplectic Phase Space GAN (SPS - GAN)，可捕获多系统动态，推广到未见物理参数，无需系统配置空间先验知识，并展示其在轨迹预测、视频生成和对称性发现中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络模型一般只能捕获具有固定物理参数的单个系统动态，且需已知配置空间的状态空间测量，本文旨在解决这些局限性。

Method: 引入将哈密顿神经网络循环模块嵌入条件GAN骨干的架构，通过增加物理动机项优化条件时间序列GAN目标以发现配置空间结构。

Result: SPS - GAN在轨迹预测、视频生成和对称性发现中展示了实用性，捕获多系统且性能与单系统监督模型相当。

Conclusion: SPS - GAN能有效捕获多系统动态，无需配置空间先验知识，是一种有潜力的模型。

Abstract: From metronomes to celestial bodies, mechanics underpins how the world
evolves in time and space. With consideration of this, a number of recent
neural network models leverage inductive biases from classical mechanics to
encourage model interpretability and ensure forecasted states are physical.
However, in general, these models are designed to capture the dynamics of a
single system with fixed physical parameters, from state-space measurements of
a known configuration space. In this paper we introduce Symplectic Phase Space
GAN (SPS-GAN) which can capture the dynamics of multiple systems, and
generalize to unseen physical parameters from. Moreover, SPS-GAN does not
require prior knowledge of the system configuration space. In fact, SPS-GAN can
discover the configuration space structure of the system from arbitrary
measurement types (e.g., state-space measurements, video frames). To achieve
physically plausible generation, we introduce a novel architecture which embeds
a Hamiltonian neural network recurrent module in a conditional GAN backbone. To
discover the structure of the configuration space, we optimize the conditional
time-series GAN objective with an additional physically motivated term to
encourages a sparse representation of the configuration space. We demonstrate
the utility of SPS-GAN for trajectory prediction, video generation and symmetry
discovery. Our approach captures multiple systems and achieves performance on
par with supervised models designed for single systems.

</details>


### [241] [MoE-PHDS: One MoE checkpoint for flexible runtime sparsity](https://arxiv.org/abs/2509.23012)
*Lauren. A Hannah,Soheil Zibakhsh,Kumari Nishu,Arnav Kundu,Mohammad Samragh Razlighi,Mehrdad Farajtabar,Minsik Cho*

Main category: cs.LG

TL;DR: 论文指出预训练的稀疏混合专家（MoE）模型对运行时稀疏性变化更具鲁棒性，提出MoE - PHDS方法将单个检查点转换为全局稀疏控制面，实验表明该方法有良好效果。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏混合专家模型通常在固定稀疏水平下训练，满足多个效率目标需训练和维护多个模型，导致服务复杂、成本增加且灵活性受限。

Method: 提出MoE - PHDS方法，跨稀疏水平混合训练，并在高稀疏度下进行短期课程锚定，无需架构更改。

Result: 在多个模型上实验，PHDS匹配或超过指定的oracle模型，将跨稀疏一致性提高了22%，使全局稀疏性成为一流的服务原语，实现了简化、灵活的运行时MoE部署。

Conclusion: MoE - PHDS方法可让从业者在推理时灵活控制稀疏度，无需更换检查点、更改架构或依赖令牌级启发式方法，能有效解决现有稀疏混合专家模型训练和部署的问题。

Abstract: Sparse Mixtures of Experts (MoEs) are typically trained to operate at a fixed
sparsity level, e.g. $k$ in a top-$k$ gating function. This global sparsity
level determines an operating point on the accuracy/latency curve; currently,
meeting multiple efficiency targets means training and maintaining multiple
models. This practice complicates serving, increases training and maintenance
costs, and limits flexibility in meeting diverse latency, efficiency, and
energy requirements. We show that pretrained MoEs are more robust to runtime
sparsity shifts than commonly assumed, and introduce MoE-PHDS ({\bf P}ost {\bf
H}oc {\bf D}eclared {\bf S}parsity), a lightweight SFT method that turns a
single checkpoint into a global sparsity control surface. PHDS mixes training
across sparsity levels and anchors with a short curriculum at high sparsity,
requiring no architectural changes. The result is predictable accuracy/latency
tradeoffs from one model: practitioners can ``dial $k$'' at inference time
without swapping checkpoints, changing architecture, or relying on token-level
heuristics. Experiments on OLMoE-1B-7B-0125, Qwen1.5-MoE-A2.7B, and proprietary
models fit on multiple operating points show that PHDS matches or exceeds
well-specified oracle models, improves cross-sparsity agreement by up to 22\%
vs. well-specified oracle models, and enables simplified, flexible runtime MoE
deployment by making global sparsity a first-class serving primitive.

</details>


### [242] [On the Sheafification of Higher-Order Message Passing](https://arxiv.org/abs/2509.23020)
*Jacob Hume,Pietro Liò*

Main category: cs.LG

TL;DR: 本文探讨利用层论改进Hodge拉普拉斯算子以实现更具表现力的消息传递，拓展图学习中层扩散理论至高阶情形。


<details>
  <summary>Details</summary>
Motivation: 现有高阶消息传递（HOMP）方法中Hodge拉普拉斯算子在高阶情形下的偏差不透明甚至退化，需要改进。

Method: 引入层论来修改Hodge拉普拉斯算子在局部和全局描述符之间的扩散介导界面。

Result: 将图学习中层扩散的先前理论进行情境化和新颖扩展，探索其在k>0时的局限性，并为高阶情形开发新理论和实践。

Conclusion: 层论是改进Hodge拉普拉斯算子以实现更具表现力的消息传递的自然且有原则的形式主义。

Abstract: Recent work in Topological Deep Learning (TDL) seeks to generalize graph
learning's preeminent $message \ passing$ paradigm to more complex relational
structures: simplicial complexes, cell complexes, hypergraphs, and combinations
thereof. Many approaches to such ${higher\text{-}order \ message \ passing}$
(HOMP) admit formulation in terms of nonlinear diffusion with the Hodge
(combinatorial) Laplacian, a graded operator which carries an inductive bias
that dimension-$k$ data features correlate with dimension-$k$ topological
features encoded in the (singular) cohomology of the underlying domain. For
$k=0$ this recovers the graph Laplacian and its well-studied homophily bias. In
higher gradings, however, the Hodge Laplacian's bias is more opaque and
potentially even degenerate. In this essay, we position sheaf theory as a
natural and principled formalism for modifying the Hodge Laplacian's
diffusion-mediated interface between local and global descriptors toward more
expressive message passing. The sheaf Laplacian's inductive bias correlates
dimension-$k$ data features with dimension-$k$ $sheaf$ cohomology, a data-aware
generalization of singular cohomology. We will contextualize and novelly extend
prior theory on sheaf diffusion in graph learning ($k=0$) in such a light --
and explore how it fails to generalize to $k>0$ -- before developing novel
theory and practice for the higher-order setting. Our exposition is accompanied
by a self-contained introduction shepherding sheaves from the abstract to the
applied.

</details>


### [243] [FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents](https://arxiv.org/abs/2509.23803)
*Pramit Saha,Joshua Strong,Divyanshu Mishra,Cheng Ouyang,J. Alison Noble*

Main category: cs.LG

TL;DR: 现有联邦学习（FL）部署有操作挑战，本文提出代理驱动的FL框架和基准FedAgentBench，评估多种LLM代理在医疗FL中的表现，发现复杂任务对强模型仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 现实中FL部署存在诸多操作挑战，现有工作忽视这些问题，因此需要自主、代理驱动的FL系统。

Method: 引入代理驱动的FL框架和FedAgentBench，纳入40种FL算法，设置201个数据集的复杂任务，评估14个开源和10个专有LLM。

Result: 部分代理核心能自动化FL流程各阶段，但复杂、基于隐式目标的任务对强模型仍有挑战。

Conclusion: 代理驱动的FL系统在应对复杂任务上还有提升空间。

Abstract: Federated learning (FL) allows collaborative model training across healthcare
sites without sharing sensitive patient data. However, real-world FL deployment
is often hindered by complex operational challenges that demand substantial
human efforts. This includes: (a) selecting appropriate clients (hospitals),
(b) coordinating between the central server and clients, (c) client-level data
pre-processing, (d) harmonizing non-standardized data and labels across
clients, and (e) selecting FL algorithms based on user instructions and
cross-client data characteristics. However, the existing FL works overlook
these practical orchestration challenges. These operational bottlenecks
motivate the need for autonomous, agent-driven FL systems, where intelligent
agents at each hospital client and the central server agent collaboratively
manage FL setup and model training with minimal human intervention. To this
end, we first introduce an agent-driven FL framework that captures key phases
of real-world FL workflows from client selection to training completion and a
benchmark dubbed FedAgentBench that evaluates the ability of LLM agents to
autonomously coordinate healthcare FL. Our framework incorporates 40 FL
algorithms, each tailored to address diverse task-specific requirements and
cross-client characteristics. Furthermore, we introduce a diverse set of
complex tasks across 201 carefully curated datasets, simulating 6
modality-specific real-world healthcare environments, viz., Dermatoscopy,
Ultrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic
performance of 14 open-source and 10 proprietary LLMs spanning small, medium,
and large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3
can automate various stages of the FL pipeline, our results reveal that more
complex, interdependent tasks based on implicit goals remain challenging for
even the strongest models.

</details>


### [244] [Tracing the Representation Geometry of Language Models from Pretraining to Post-training](https://arxiv.org/abs/2509.23024)
*Melody Zixuan Li,Kumar Krishna Agrawal,Arna Ghosh,Komal Kumar Teru,Adam Santoro,Guillaume Lajoie,Blake A. Richards*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Standard training metrics like loss fail to explain the emergence of complex
capabilities in large language models. We take a spectral approach to
investigate the geometry of learned representations across pretraining and
post-training, measuring effective rank (RankMe) and eigenspectrum decay
($\alpha$-ReQ). With OLMo (1B-7B) and Pythia (160M-12B) models, we uncover a
consistent non-monotonic sequence of three geometric phases during
autoregressive pretraining. The initial "warmup" phase exhibits rapid
representational collapse. This is followed by an "entropy-seeking" phase,
where the manifold's dimensionality expands substantially, coinciding with peak
n-gram memorization. Subsequently, a "compression-seeking" phase imposes
anisotropic consolidation, selectively preserving variance along dominant
eigendirections while contracting others, a transition marked with significant
improvement in downstream task performance. We show these phases can emerge
from a fundamental interplay of cross-entropy optimization under skewed token
frequencies and representational bottlenecks ($d \ll |V|$). Post-training
further transforms geometry: SFT and DPO drive "entropy-seeking" dynamics to
integrate specific instructional or preferential data, improving
in-distribution performance while degrading out-of-distribution robustness.
Conversely, RLVR induces "compression-seeking", enhancing reward alignment but
reducing generation diversity.

</details>


### [245] [Understanding Catastrophic Interference On the Identifibility of Latent Representations](https://arxiv.org/abs/2509.23027)
*Yuke Li,Yujia Zheng,Tianyi Xiong,Zhenyi Wang,Heng Huang*

Main category: cs.LG

TL;DR: 从潜在表征学习角度研究灾难性干扰问题，提出新理论框架，用两阶段训练策略的方法减轻干扰，有理论保证和性能提升。


<details>
  <summary>Details</summary>
Motivation: 更好地理解和建模机器学习中的灾难性干扰问题。

Method: 提出将灾难性干扰表述为识别问题的理论框架，用两阶段训练策略，先最大似然估计学习潜在表征，再优化KL散度识别共享潜在变量。

Result: 通过理论证明和实证验证，表明识别和学习共享表征可有效减轻机器学习系统中的灾难性干扰。

Conclusion: 该方法有理论保证，能在合成和基准数据集上提升性能。

Abstract: Catastrophic interference, also known as catastrophic forgetting, is a
fundamental challenge in machine learning, where a trained learning model
progressively loses performance on previously learned tasks when adapting to
new ones. In this paper, we aim to better understand and model the catastrophic
interference problem from a latent representation learning point of view, and
propose a novel theoretical framework that formulates catastrophic interference
as an identification problem. Our analysis demonstrates that the forgetting
phenomenon can be quantified by the distance between partial-task aware (PTA)
and all-task aware (ATA) setups. Building upon recent advances in
identifiability theory, we prove that this distance can be minimized through
identification of shared latent variables between these setups. When learning,
we propose our method \ourmeos with two-stage training strategy: First, we
employ maximum likelihood estimation to learn the latent representations from
both PTA and ATA configurations. Subsequently, we optimize the KL divergence to
identify and learn the shared latent variables. Through theoretical guarantee
and empirical validations, we establish that identifying and learning these
shared representations can effectively mitigate catastrophic interference in
machine learning systems. Our approach provides both theoretical guarantees and
practical performance improvements across both synthetic and benchmark
datasets.

</details>


### [246] [Asynchronous Policy Gradient Aggregation for Efficient Distributed Reinforcement Learning](https://arxiv.org/abs/2509.24305)
*Alexander Tyurin,Andrei Spiridonov,Varvara Rudenko*

Main category: cs.LG

TL;DR: 研究异步并行下分布式强化学习策略梯度方法，提出Rennala NIGT和Malenia NIGT算法，效率达最优，实验显示显著优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 非分布式强化学习方法理论和实践成果好，但分布式方法尤其是在异构异步计算和通信瓶颈下研究较少。

Method: 引入Rennala NIGT和Malenia NIGT两种新算法实现异步策略梯度聚合。

Result: 在同构场景，Rennala NIGT改善计算和通信复杂度；异构场景，Malenia NIGT处理异步计算和异构环境有更好理论保证，实验显示显著优于先前方法。

Conclusion: 提出的新算法在分布式强化学习中有效且优于先前方法。

Abstract: We study distributed reinforcement learning (RL) with policy gradient methods
under asynchronous and parallel computations and communications. While
non-distributed methods are well understood theoretically and have achieved
remarkable empirical success, their distributed counterparts remain less
explored, particularly in the presence of heterogeneous asynchronous
computations and communication bottlenecks. We introduce two new algorithms,
Rennala NIGT and Malenia NIGT, which implement asynchronous policy gradient
aggregation and achieve state-of-the-art efficiency. In the homogeneous
setting, Rennala NIGT provably improves the total computational and
communication complexity while supporting the AllReduce operation. In the
heterogeneous setting, Malenia NIGT simultaneously handles asynchronous
computations and heterogeneous environments with strictly better theoretical
guarantees. Our results are further corroborated by experiments, showing that
our methods significantly outperform prior approaches.

</details>


### [247] [DPFNAS: Differential Privacy-Enhanced Federated Neural Architecture Search for 6G Edge Intelligence](https://arxiv.org/abs/2509.23030)
*Yang Lv,Jin Cao,Ben Niu,Zhe Sun,Fengwei Wang,Fenghua Li,Hui Li*

Main category: cs.LG

TL;DR: 本文提出集成个性化差分隐私和自适应模型设计的联邦学习框架，保障数据隐私同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 边缘数据的敏感性和异质性给联邦学习带来参数共享风险数据重建、全局模型难适应本地分布的挑战，需解决这些问题。

Method: 利用样本级表示进行知识共享，应用个性化差分隐私策略抵抗重建攻击；开发隐私感知的神经架构搜索算法生成本地定制架构和超参数。

Result: 在基准数据集上，相比联邦NAS方法PerFedRLNAS，准确率提高6.82%，模型大小减至1/10，通信成本降至1/20。

Conclusion: 该方案是首个针对基于表示的联邦学习且有理论收敛保证的个性化DP解决方案，能实现强隐私保障并显著提升模型性能。

Abstract: The Sixth-Generation (6G) network envisions pervasive artificial intelligence
(AI) as a core goal, enabled by edge intelligence through on-device data
utilization. To realize this vision, federated learning (FL) has emerged as a
key paradigm for collaborative training across edge devices. However, the
sensitivity and heterogeneity of edge data pose key challenges to FL: parameter
sharing risks data reconstruction, and a unified global model struggles to
adapt to diverse local distributions. In this paper, we propose a novel
federated learning framework that integrates personalized differential privacy
(DP) and adaptive model design. To protect training data, we leverage
sample-level representations for knowledge sharing and apply a personalized DP
strategy to resist reconstruction attacks. To ensure distribution-aware
adaptation under privacy constraints, we develop a privacy-aware neural
architecture search (NAS) algorithm that generates locally customized
architectures and hyperparameters. To the best of our knowledge, this is the
first personalized DP solution tailored for representation-based FL with
theoretical convergence guarantees. Our scheme achieves strong privacy
guarantees for training data while significantly outperforming state-of-the-art
methods in model performance. Experiments on benchmark datasets such as
CIFAR-10 and CIFAR-100 demonstrate that our scheme improves accuracy by 6.82\%
over the federated NAS method PerFedRLNAS, while reducing model size to 1/10
and communication cost to 1/20.

</details>


### [248] [GuardNet: Graph-Attention Filtering for Jailbreak Defense in Large Language Models](https://arxiv.org/abs/2509.23037)
*Javad Forough,Mohammad Maheri,Hamed Haddadi*

Main category: cs.LG

TL;DR: 论文提出GuardNet框架检测和过滤大语言模型越狱攻击提示，实验表明其性能远超先前防御方法，有实用性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受越狱攻击，影响输出的安全、可靠和可信度，在多领域构成风险。

Method: 提出GuardNet分层过滤框架，构建结合多种关系的结构化图，应用图神经网络进行提示级和词元级过滤。

Result: 在三个数据集和多种攻击设置实验中，GuardNet大幅提升F1分数，在提示级和词元级都有显著表现，且保持可接受延迟，跨领域评估泛化性好。

Conclusion: GuardNet是应对现实世界大语言模型越狱威胁的实用且强大的防御手段。

Abstract: Large Language Models (LLMs) are increasingly susceptible to jailbreak
attacks, which are adversarial prompts that bypass alignment constraints and
induce unauthorized or harmful behaviors. These vulnerabilities undermine the
safety, reliability, and trustworthiness of LLM outputs, posing critical risks
in domains such as healthcare, finance, and legal compliance. In this paper, we
propose GuardNet, a hierarchical filtering framework that detects and filters
jailbreak prompts prior to inference. GuardNet constructs structured graphs
that combine sequential links, syntactic dependencies, and attention-derived
token relations to capture both linguistic structure and contextual patterns
indicative of jailbreak behavior. It then applies graph neural networks at two
levels: (i) a prompt-level filter that detects global adversarial prompts, and
(ii) a token-level filter that pinpoints fine-grained adversarial spans.
Extensive experiments across three datasets and multiple attack settings show
that GuardNet substantially outperforms prior defenses. It raises prompt-level
F$_1$ scores from 66.4\% to 99.8\% on LLM-Fuzzer, and from 67-79\% to over 94\%
on PLeak datasets. At the token level, GuardNet improves F$_1$ from 48-75\% to
74-91\%, with IoU gains up to +28\%. Despite its structural complexity,
GuardNet maintains acceptable latency and generalizes well in cross-domain
evaluations, making it a practical and robust defense against jailbreak threats
in real-world LLM deployments.

</details>


### [249] [IsingFormer: Augmenting Parallel Tempering With Learned Proposals](https://arxiv.org/abs/2509.23043)
*Saleh Bunaiyan,Corentin Delacour,Shuvro Chowdhury,Kyle Lee,Kerem Y. Camsari*

Main category: cs.LG

TL;DR: 本文介绍IsingFormer，将其用于并行回火（PT），在2D Ising模型采样、3D自旋玻璃优化和整数分解问题上取得良好效果，证明蒙特卡罗方法可被捕捉全局结构的神经提议加速。


<details>
  <summary>Details</summary>
Motivation: Markov Chain Monte Carlo（MCMC）在临界点和粗糙景观中混合缓慢，Parallel Tempering（PT）中每个副本仍依赖缓慢的局部更新来改变配置，需要改进。

Method: 引入IsingFormer，用其生成的自旋配置作为PT中Metropolis步骤的全局移动提议，补充单自旋翻转。

Result: 在2D Ising模型中能重现磁化和自由能曲线，减少平衡时间；在3D自旋玻璃中找到更低能量状态；在整数分解问题上能成功泛化，提高成功率。

Conclusion: 蒙特卡罗方法可通过捕捉全局结构的神经提议系统地加速，实现更快采样和更强的组合优化性能。

Abstract: Markov Chain Monte Carlo (MCMC) underlies both statistical physics and
combinatorial optimization, but mixes slowly near critical points and in rough
landscapes. Parallel Tempering (PT) improves mixing by swapping replicas across
temperatures, yet each replica still relies on slow local updates to change its
configuration. We introduce IsingFormer, a Transformer trained on equilibrium
samples that can generate entire spin configurations resembling those from the
target distribution. These uncorrelated samples are used as proposals for
global moves within a Metropolis step in PT, complementing the usual
single-spin flips. On 2D Ising models (sampling), IsingFormer reproduces
magnetization and free-energy curves and generalizes to unseen temperatures,
including the critical region. Injecting even a single proposal sharply reduces
equilibration time, replacing thousands of local updates. On 3D spin glasses
(optimization), PT enhanced with IsingFormer finds substantially lower-energy
states, demonstrating how global moves accelerate search in rugged landscapes.
Finally, applied to integer factorization encoded as Ising problems,
IsingFormer trained on a limited set of semiprimes transfers successfully to
unseen semiprimes, boosting success rates beyond the training distribution.
Since factorization is a canonical hard benchmark, this ability to generalize
across instances highlights the potential of learning proposals that move
beyond single problems to entire families of instances. The IsingFormer
demonstrates that Monte Carlo methods can be systematically accelerated by
neural proposals that capture global structure, yielding faster sampling and
stronger performance in combinatorial optimization.

</details>


### [250] [Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding](https://arxiv.org/abs/2509.23050)
*Lin Long,Changdae Oh,Seongheon Park,Yixuan Li*

Main category: cs.LG

TL;DR: 本文通过链嵌入视角系统分析大视觉语言模型（LVLMs）的语言先验，发现视觉集成点（VIP），引入总视觉集成（TVI）估计器，为诊断和理解LVLMs的语言先验提供工具。


<details>
  <summary>Details</summary>
Motivation: 现有对LVLMs语言先验的分析多依赖输入 - 输出探测，无法揭示视觉影响模型行为的内部机制，因此需要新的分析方法。

Method: 通过链嵌入视角进行系统分析，引入TVI估计器量化视觉查询对响应生成的影响。

Result: 在54种模型 - 数据集组合中，VIP普遍存在，TVI能可靠预测语言先验的强度。

Conclusion: 为诊断和理解LVLMs的语言先验提供了一套有原则的工具包。

Abstract: Large vision-language models (LVLMs) achieve strong performance on multimodal
tasks, yet they often default to their language prior (LP) -- memorized textual
patterns from pre-training while under-utilizing visual evidence. Prior
analyses of LP mostly rely on input-output probing, which fails to reveal the
internal mechanisms governing when and how vision influences model behavior. To
address this gap, we present the first systematic analysis of language prior
through the lens of chain-of-embedding, which examines the layer-wise
representation dynamics within LVLMs. Our analysis reveals a universal
phenomenon: each model exhibits a Visual Integration Point (VIP), a critical
layer at which visual information begins to meaningfully reshape hidden
representations and influence decoding. Building on this observation, we
introduce the Total Visual Integration (TVI) estimator, which aggregates
representation distance beyond the VIP to quantify how strongly visual query
influences response generation. Across 54 model-dataset combinations spanning 9
contemporary LVLMs and 6 benchmarks, we demonstrate that VIP consistently
emerges, and that TVI reliably predicts the strength of language prior. This
offers a principled toolkit for diagnosing and understanding language prior in
LVLMs.

</details>


### [251] [Sensitivity Analysis for Diffusion Models](https://arxiv.org/abs/2509.23092)
*Christopher Scarvelis,Justin Solomon*

Main category: cs.LG

TL;DR: 本文给出计算扩散模型映射方向导数的闭式过程，还扩展结果以估计样本对目标测度加性扰动的敏感性，方法鲁棒且敏感性与模型重训练和微调后的样本变化相关。


<details>
  <summary>Details</summary>
Motivation: 探究能否对扩散模型从数据分布到最优得分函数的映射求导，以预测训练集小扰动下得分和样本的变化，避免高成本的重新训练。

Method: 给出仅依赖黑盒访问预训练得分模型及其输入导数来计算映射方向导数的闭式过程，并将结果扩展以估计样本对目标测度加性扰动的敏感性。

Result: 方法对数值和近似误差具有鲁棒性，得到的敏感性与图像扩散模型重新训练和微调后样本的变化相关。

Conclusion: 所提出的计算方向导数和估计样本敏感性的方法是有效的，可用于预测训练集扰动下模型样本的变化。

Abstract: Training a diffusion model approximates a map from a data distribution $\rho$
to the optimal score function $s_t$ for that distribution. Can we differentiate
this map? If we could, then we could predict how the score, and ultimately the
model's samples, would change under small perturbations to the training set
before committing to costly retraining. We give a closed-form procedure for
computing this map's directional derivatives, relying only on black-box access
to a pre-trained score model and its derivatives with respect to its inputs. We
extend this result to estimate the sensitivity of a diffusion model's samples
to additive perturbations of its target measure, with runtime comparable to
sampling from a diffusion model and computing log-likelihoods along the sample
path. Our method is robust to numerical and approximation error, and the
resulting sensitivities correlate with changes in an image diffusion model's
samples after retraining and fine-tuning.

</details>


### [252] [Dynamics of Learning: Generative Schedules from Latent ODEs](https://arxiv.org/abs/2509.23052)
*Matt L. Sampson,Peter Melchior*

Main category: cs.LG

TL;DR: 提出新学习率调度器，建模训练过程，预测未来学习率，在多模型上达SOTA，泛化性好且计算高效。


<details>
  <summary>Details</summary>
Motivation: 现有学习率调度缺乏对神经网络训练的全面时间视角，需改进。

Method: 将神经网络训练性能建模为动态系统，利用超参搜索的训练运行学习训练过程的潜在表示，根据当前训练指标预测未来学习率。

Result: 在CNN、ResNet图像分类和Transformer下一个标记预测任务中达SOTA，训练模型在损失景观更平坦区域。

Conclusion: 新调度器泛化性好、计算高效、与优化器无关，可集成到ML实验跟踪平台。

Abstract: The learning rate schedule is one of the most impactful aspects of neural
network optimization, yet most schedules either follow simple parametric
functions or react only to short-term training signals. None of them are
supported by a comprehensive temporal view of how well neural networks actually
train. We present a new learning rate scheduler that models the training
performance of neural networks as a dynamical system. It leverages training
runs from a hyperparameter search to learn a latent representation of the
training process. Given current training metrics, it predicts the future
learning rate schedule with the best long-term validation performance. Our
scheduler generalizes beyond previously observed training dynamics and creates
specialized schedules that deviate noticeably from common parametric functions.
It achieves SOTA results for image classification with CNN and ResNet models as
well as for next-token prediction with a transformer model. The trained models
are located in flatter regions of the loss landscape and thus provide better
generalization than those trained with other schedules. Our method is
computationally efficient, optimizer-agnostic, and can easily be layered on top
of ML experiment-tracking platforms. An implementation of our scheduler will be
made available after acceptance.

</details>


### [253] [Dense associative memory on the Bures-Wasserstein space](https://arxiv.org/abs/2509.23162)
*Chandan Tankala,Krishnakumar Balasubramanian*

Main category: cs.LG

TL;DR: 本文将稠密联想记忆（DAMs）扩展到配备2 - 瓦瑟斯坦距离的概率分布，证明了指数存储容量，验证了模型在分布任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有DAMs模型局限于向量表示，需要将其扩展到概率分布。

Method: 定义存储分布上的log - sum - exp能量和以吉布斯加权方式聚合最优传输映射的检索动力学。

Result: 证明指数存储容量，给出瓦瑟斯坦扰动下的定量检索保证，在合成和真实世界分布任务中验证模型。

Conclusion: 将联想记忆从向量提升到完整分布，连接经典DAMs与现代生成建模，实现记忆增强学习中的分布存储和检索。

Abstract: Dense associative memories (DAMs) store and retrieve patterns via
energy-functional fixed points, but existing models are limited to vector
representations. We extend DAMs to probability distributions equipped with the
2-Wasserstein distance, focusing mainly on the Bures-Wasserstein class of
Gaussian densities. Our framework defines a log-sum-exp energy over stored
distributions and a retrieval dynamics aggregating optimal transport maps in a
Gibbs-weighted manner. Stationary points correspond to self-consistent
Wasserstein barycenters, generalizing classical DAM fixed points. We prove
exponential storage capacity, provide quantitative retrieval guarantees under
Wasserstein perturbations, and validate the model on synthetic and real-world
distributional tasks. This work elevates associative memory from vectors to
full distributions, bridging classical DAMs with modern generative modeling and
enabling distributional storage and retrieval in memory-augmented learning.

</details>


### [254] [Beyond Model Ranking: Predictability-Aligned Evaluation for Time Series Forecasting](https://arxiv.org/abs/2509.23074)
*Wanjin Feng,Yuan Yuan,Jingtao Ding,Yong Li*

Main category: cs.LG

TL;DR: 提出基于谱相干性的可预测性对齐诊断框架，验证其有效性并揭示两个核心见解，倡导评估范式转变。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型评估方法存在标准评估指标将模型性能与数据固有不可预测性混淆的问题。

Method: 引入基于谱相干性的诊断框架，包括SCP和LUR。

Result: 验证了框架有效性，发现可预测性漂移现象，揭示复杂模型和线性模型在不同可预测性数据上的优势。

Conclusion: 应从简单聚合评分转向可预测性感知评估，促进更公平的模型比较和对模型行为的深入理解。

Abstract: In the era of increasingly complex AI models for time series forecasting,
progress is often measured by marginal improvements on benchmark leaderboards.
However, this approach suffers from a fundamental flaw: standard evaluation
metrics conflate a model's performance with the data's intrinsic
unpredictability. To address this pressing challenge, we introduce a novel,
predictability-aligned diagnostic framework grounded in spectral coherence. Our
framework makes two primary contributions: the Spectral Coherence
Predictability (SCP), a computationally efficient ($O(N\log N)$) and
task-aligned score that quantifies the inherent difficulty of a given
forecasting instance, and the Linear Utilization Ratio (LUR), a
frequency-resolved diagnostic tool that precisely measures how effectively a
model exploits the linearly predictable information within the data. We
validate our framework's effectiveness and leverage it to reveal two core
insights. First, we provide the first systematic evidence of "predictability
drift", demonstrating that a task's forecasting difficulty varies sharply over
time. Second, our evaluation reveals a key architectural trade-off: complex
models are superior for low-predictability data, whereas linear models are
highly effective on more predictable tasks. We advocate for a paradigm shift,
moving beyond simplistic aggregate scores toward a more insightful,
predictability-aware evaluation that fosters fairer model comparisons and a
deeper understanding of model behavior.

</details>


### [255] [Landing with the Score: Riemannian Optimization through Denoising](https://arxiv.org/abs/2509.23357)
*Andrey Kharitenko,Zebang Shen,Riccardo de Santi,Niao He,Florian Doerfler*

Main category: cs.LG

TL;DR: 研究隐式流形上黎曼优化问题，引入链接函数，提出两个算法并验证有效性。


<details>
  <summary>Details</summary>
Motivation: 在数据流形假设下，解决隐式流形上经典算法所需标准流形操作不可用的黎曼优化问题，此类问题是现代生成式AI核心。

Method: 引入链接函数连接数据分布和优化所需几何操作，建立与扩散模型中得分函数的联系，提出Denoising Landing Flow (DLF) 和Denoising Riemannian Gradient Descent (DRGD) 两个算法。

Result: 链接函数可恢复基本流形操作，算法有可行性和最优性的理论保证。

Conclusion: 所提方法在数据驱动控制的有限时间参考跟踪任务中有效，有实际生成和设计应用潜力。

Abstract: Under the data manifold hypothesis, high-dimensional data are concentrated
near a low-dimensional manifold. We study the problem of Riemannian
optimization over such manifolds when they are given only implicitly through
the data distribution, and the standard manifold operations required by
classical algorithms are unavailable. This formulation captures a broad class
of data-driven design problems that are central to modern generative AI. Our
key idea is to introduce a link function that connects the data distribution to
the geometric operations needed for optimization. We show that this function
enables the recovery of essential manifold operations, such as retraction and
Riemannian gradient computation. Moreover, we establish a direct connection
between our construction and the score function in diffusion models of the data
distribution. This connection allows us to leverage well-studied
parameterizations, efficient training procedures, and even pretrained score
networks from the diffusion model literature to perform optimization. Building
on this foundation, we propose two efficient inference-time algorithms --
Denoising Landing Flow (DLF) and Denoising Riemannian Gradient Descent (DRGD)
-- and provide theoretical guarantees for both feasibility (approximate
manifold adherence) and optimality (small Riemannian gradient norm). Finally,
we demonstrate the effectiveness of our approach on finite-horizon reference
tracking tasks in data-driven control, highlighting its potential for practical
generative and design applications.

</details>


### [256] [CLAD-Net: Continual Activity Recognition in Multi-Sensor Wearable Systems](https://arxiv.org/abs/2509.23077)
*Reza Rahimi Azghan,Gautham Krishna Gudur,Mohit Malu,Edison Thomaz,Giulia Pedrielli,Pavan Turaga,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: 本文提出CLAD - Net框架解决可穿戴传感器在人类活动识别中因数据分布变化和标签稀缺带来的问题，在PAMAP2数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在人类活动识别中假设数据分布平稳，与现实不符，且持续学习存在灾难性遗忘问题，人类研究中标签数据稀缺不一致。

Method: 提出CLAD - Net框架，集成自监督变压器和通过知识蒸馏训练的监督CNN，变压器捕捉全局活动模式，CNN在微调时保留先验知识。

Result: 在PAMAP2上最终准确率达91.36%，遗忘率仅8.78%，超基线方法；半监督设置下表现强，消融实验验证各模块贡献。

Conclusion: CLAD - Net能使可穿戴传感器模型持续更新且不牺牲旧任务性能，对标签稀缺有鲁棒性。

Abstract: The rise of deep learning has greatly advanced human behavior monitoring
using wearable sensors, particularly human activity recognition (HAR). While
deep models have been widely studied, most assume stationary data distributions
- an assumption often violated in real-world scenarios. For example, sensor
data from one subject may differ significantly from another, leading to
distribution shifts. In continual learning, this shift is framed as a sequence
of tasks, each corresponding to a new subject. Such settings suffer from
catastrophic forgetting, where prior knowledge deteriorates as new tasks are
learned. This challenge is compounded by the scarcity and inconsistency of
labeled data in human studies. To address these issues, we propose CLAD-Net
(Continual Learning with Attention and Distillation), a framework enabling
wearable-sensor models to be updated continuously without sacrificing
performance on past tasks. CLAD-Net integrates a self-supervised transformer,
acting as long-term memory, with a supervised Convolutional Neural Network
(CNN) trained via knowledge distillation for activity classification. The
transformer captures global activity patterns through cross-attention across
body-mounted sensors, learning generalizable representations without labels.
Meanwhile, the CNN leverages knowledge distillation to retain prior knowledge
during subject-wise fine-tuning. On PAMAP2, CLAD-Net achieves 91.36 percent
final accuracy with only 8.78 percent forgetting, surpassing memory-based and
regularization-based baselines such as Experience Replay and Elastic Weight
Consolidation. In semi-supervised settings with only 10-20 percent labeled
data, CLAD-Net still delivers strong performance, demonstrating robustness to
label scarcity. Ablation studies further validate each module's contribution.

</details>


### [257] [Better Hessians Matter: Studying the Impact of Curvature Approximations in Influence Functions](https://arxiv.org/abs/2509.23437)
*Steve Hong,Runa Eschenhagen,Bruno Mlodozeniec,Richard Turner*

Main category: cs.LG

TL;DR: 本文研究了Hessian近似质量对影响函数归因的影响，发现更好的Hessian近似能带来更好的影响得分质量，并分析了近似步骤对归因准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 在深度学习中使用影响函数需求逆大的病态Hessian矩阵，现有近似方法对数据归因性能的影响不明，且不确定更好的Hessian近似是否带来更好的数据归因性能。

Method: 在可控分类设置下研究Hessian近似质量对影响函数归因的影响，分解近期Hessian近似方法的近似步骤并评估各步骤对归因准确性的影响。

Result: 更好的Hessian近似能带来更好的影响得分质量；K - FAC特征值与GGN/EK - FAC特征值的不匹配是误差和影响损失的主要原因。

Conclusion: 指出了哪些近似最为关键，为未来平衡计算可处理性和归因准确性提供指导。

Abstract: Influence functions offer a principled way to trace model predictions back to
training data, but their use in deep learning is hampered by the need to invert
a large, ill-conditioned Hessian matrix. Approximations such as Generalised
Gauss-Newton (GGN) and Kronecker-Factored Approximate Curvature (K-FAC) have
been proposed to make influence computation tractable, yet it remains unclear
how the departure from exactness impacts data attribution performance.
Critically, given the restricted regime in which influence functions are
derived, it is not necessarily clear better Hessian approximations should even
lead to better data attribution performance. In this paper, we investigate the
effect of Hessian approximation quality on influence-function attributions in a
controlled classification setting. Our experiments show that better Hessian
approximations consistently yield better influence score quality, offering
justification for recent research efforts towards that end. We further
decompose the approximation steps for recent Hessian approximation methods and
evaluate each step's influence on attribution accuracy. Notably, the mismatch
between K-FAC eigenvalues and GGN/EK-FAC eigenvalues accounts for the majority
of the error and influence loss. These findings highlight which approximations
are most critical, guiding future efforts to balance computational tractability
and attribution accuracy.

</details>


### [258] [Signal Preserving Weight Initialization for Odd-Sigmoid Activations](https://arxiv.org/abs/2509.23085)
*Hyunwoo Lee,Hayoung Choi,Hyunju Kim*

Main category: cs.LG

TL;DR: 提出针对特定激活函数类的初始化方法，避免激活值问题，训练效果好。


<details>
  <summary>Details</summary>
Motivation: 激活函数与权重初始化相互依赖，不合适的初始化会导致饱和、方差崩溃等问题。

Method: 定义奇Sigmoid函数类，为该类中任意激活函数f提出量身定制的初始化方法，以闭式选择噪声尺度。

Result: 该方法无需归一化层就能可靠训练，数据效率高，能让标准初始化方法常无法可靠收敛的激活函数实现学习。

Conclusion: 提出的初始化方法有效，解决了激活函数与权重初始化相关的问题。

Abstract: Activation functions critically influence trainability and expressivity, and
recent work has therefore explored a broad range of nonlinearities. However,
activations and weight initialization are interdependent: without an
appropriate initialization method, nonlinearities can cause saturation,
variance collapse, and increased learning rate sensitivity. We address this by
defining an odd sigmoid function class and, given any activation f in this
class, proposing an initialization method tailored to f. The method selects a
noise scale in closed form so that forward activations remain well dispersed up
to a target layer, thereby avoiding collapse to zero or saturation.
Empirically, the approach trains reliably without normalization layers,
exhibits strong data efficiency, and enables learning for activations under
which standard initialization methods (Xavier, He, Orthogonal) often do not
converge reliably.

</details>


### [259] [Data-Efficient Training by Evolved Sampling](https://arxiv.org/abs/2509.23461)
*Ziheng Cheng,Zhong Li,Jiang Bian*

Main category: cs.LG

TL;DR: 提出Evolved Sampling (ES) 框架用于动态采样，可加速训练并保持性能，还可扩展为ESWP，能节省近45%时间，促进行业内对数据效率的研究。


<details>
  <summary>Details</summary>
Motivation: 在保证性能的前提下加速学习，识别对训练有重要贡献的信息数据样本。

Method: 提出ES框架，基于损失动态和增强的损失差异进行批量级数据选择，可灵活调整频率；还可扩展为ESWP进行集级数据选择。

Result: ES(WP) 作为即插即用框架，在各种预训练和后训练任务中实现无损训练加速，节省近45%的时钟时间。

Conclusion: 研究结果鼓励对现代大规模机器学习的数据效率方面进行进一步研究。

Abstract: Data selection is designed to accelerate learning with preserved performance.
To achieve this, a fundamental thought is to identify informative data samples
with significant contributions to the training. In this work, we propose
\textbf{Evolved Sampling} (\textbf{ES}), a simple yet effective framework for
\emph{dynamic} sampling along the training process. This method conducts \em
batch \em level data selection based on the dynamics of losses and augmented
\emph{loss differences}, which enables flexible \emph{frequency tuning}, and
hence significantly reduces the back propagation time with maintained model
performance. Due to its conciseness, ES is also readily extensible to
incorporate \em set \em level data selection (to form ES with pruning,
\textbf{ESWP}) for further accelerations. As a plug-and-play framework, ES(WP)
consistently achieves lossless training accelerations across various
pre-training and post-training tasks, saving up to nearly 45\% wall-clock time.
Our results motivate further investigations on the data efficiency aspect of
modern large-scale machine learning.

</details>


### [260] [Unleashing Flow Policies with Distributional Critics](https://arxiv.org/abs/2509.23087)
*Deshu Chen,Yuchen Liu,Zhijian Zhou,Chao Qu,Yuan Qi*

Main category: cs.LG

TL;DR: 提出分布流批评家（DFC）架构，在离线和离线到在线强化学习中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有批评家通常学习单一标量估计，限制了基于流策略的潜力，需改进。

Method: 引入DFC架构，用流匹配将回报分布从简单基础分布转换为复杂目标分布，提供丰富分布贝尔曼目标。

Result: 在D4RL和OGBench基准测试中表现良好，尤其在需要多模态动作分布的任务上。

Conclusion: DFC架构为基于流策略提供更稳定、信息丰富的学习信号，优于现有方法。

Abstract: Flow-based policies have recently emerged as a powerful tool in offline and
offline-to-online reinforcement learning, capable of modeling the complex,
multimodal behaviors found in pre-collected datasets. However, the full
potential of these expressive actors is often bottlenecked by their critics,
which typically learn a single, scalar estimate of the expected return. To
address this limitation, we introduce the Distributional Flow Critic (DFC), a
novel critic architecture that learns the complete state-action return
distribution. Instead of regressing to a single value, DFC employs flow
matching to model the distribution of return as a continuous, flexible
transformation from a simple base distribution to the complex target
distribution of returns. By doing so, DFC provides the expressive flow-based
policy with a rich, distributional Bellman target, which offers a more stable
and informative learning signal. Extensive experiments across D4RL and OGBench
benchmarks demonstrate that our approach achieves strong performance,
especially on tasks requiring multimodal action distributions, and excels in
both offline and offline-to-online fine-tuning compared to existing methods.

</details>


### [261] [Statistical Learning Guarantees for Group-Invariant Barron Functions](https://arxiv.org/abs/2509.23474)
*Yahong Yang,Wei Zhu*

Main category: cs.LG

TL;DR: 研究组不变神经网络在Barron框架下泛化误差，表明结合组不变结构可显著提升对称目标函数学习统计优势。


<details>
  <summary>Details</summary>
Motivation: 探究组不变神经网络在Barron框架下的泛化误差情况。

Method: 分析结合组不变结构对近似率的影响，建立组不变类的Rademacher复杂度。

Result: 结合组不变结构引入组相关因子影响近似率；组不变类的Rademacher复杂度不大于非不变类；给出因子取值不同的示例。

Conclusion: 在神经网络中编码组不变结构对对称目标函数有明显统计优势。

Abstract: We investigate the generalization error of group-invariant neural networks
within the Barron framework. Our analysis shows that incorporating
group-invariant structures introduces a group-dependent factor
$\delta_{G,\Gamma,\sigma} \le 1$ into the approximation rate. When this factor
is small, group invariance yields substantial improvements in approximation
accuracy. On the estimation side, we establish that the Rademacher complexity
of the group-invariant class is no larger than that of the non-invariant
counterpart, implying that the estimation error remains unaffected by the
incorporation of symmetry. Consequently, the generalization error can improve
significantly when learning functions with inherent group symmetries. We
further provide illustrative examples demonstrating both favorable cases, where
$\delta_{G,\Gamma,\sigma}\approx |G|^{-1}$, and unfavorable ones, where
$\delta_{G,\Gamma,\sigma}\approx 1$. Overall, our results offer a rigorous
theoretical foundation showing that encoding group-invariant structures in
neural networks leads to clear statistical advantages for symmetric target
functions.

</details>


### [262] [Demystifying Network Foundation Models](https://arxiv.org/abs/2509.23089)
*Sylee,Beltiukov,Satyandra Guthula,Wenbo Guo,Walter Willinger,Arpit Gupta*

Main category: cs.LG

TL;DR: 对网络基础模型（NFMs）潜在知识进行系统研究，通过三部分评估分析模型，发现模型存在诸多局限，解决这些局限可提升性能。


<details>
  <summary>Details</summary>
Motivation: 聚焦隐藏表征分析，而非仅关注下游任务性能，系统研究NFMs潜在知识。

Method: 通过嵌入几何分析、度量对齐评估和因果敏感性测试三部分评估，使用五个不同网络数据集对四个最先进的NFMs进行评估。

Result: 发现所有模型都存在显著各向异性、特征敏感性模式不一致、无法分离高级上下文和负载依赖等问题。

Conclusion: 所有模型存在大量局限性，解决这些局限可显著提升模型性能（F1分数最多提高0.35），且无需架构更改。

Abstract: This work presents a systematic investigation into the latent knowledge
encoded within Network Foundation Models (NFMs) that focuses on hidden
representations analysis rather than pure downstream task performance.
Different from existing efforts, we analyze the models through a three-part
evaluation: Embedding Geometry Analysis to assess representation space
utilization, Metric Alignment Assessment to measure correspondence with
domain-expert features, and Causal Sensitivity Testing to evaluate robustness
to protocol perturbations. Using five diverse network datasets spanning
controlled and real-world environments, we evaluate four state-of-the-art NFMs,
revealing that they all exhibit significant anisotropy, inconsistent feature
sensitivity patterns, an inability to separate the high-level context, payload
dependency, and other properties. Our work identifies numerous limitations
across all models and demonstrates that addressing them can significantly
improve model performance (by up to +0.35 $F_1$ score without architectural
changes).

</details>


### [263] [Revisiting Multivariate Time Series Forecasting with Missing Values](https://arxiv.org/abs/2509.23494)
*Jie Yang,Yifan Hu,Kexin Zhang,Luyang Niu,Yushun Dong,Philip S. Yu,Kaize Ding*

Main category: cs.LG

TL;DR: 研究多元时间序列含缺失值预测问题，指出当前插补再预测框架不足，提出 CRIB 框架直接预测，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决多元时间序列含缺失值预测中，当前插补再预测框架因插补无真实值易出错、降低预测精度的问题。

Method: 提出基于信息瓶颈原则的 CRIB 框架，结合单变量注意力机制与一致性正则化方案学习鲁棒表示。

Result: 在四个真实数据集上的综合实验表明，CRIB 即使在高缺失率下也能准确预测。

Conclusion: CRIB 框架有效解决了多元时间序列含缺失值预测问题，优于当前插补再预测框架。

Abstract: Missing values are common in real-world time series, and multivariate time
series forecasting with missing values (MTSF-M) has become a crucial area of
research for ensuring reliable predictions. To address the challenge of missing
data, current approaches have developed an imputation-then-prediction framework
that uses imputation modules to fill in missing values, followed by forecasting
on the imputed data. However, this framework overlooks a critical issue: there
is no ground truth for the missing values, making the imputation process
susceptible to errors that can degrade prediction accuracy. In this paper, we
conduct a systematic empirical study and reveal that imputation without direct
supervision can corrupt the underlying data distribution and actively degrade
prediction accuracy. To address this, we propose a paradigm shift that moves
away from imputation and directly predicts from the partially observed time
series. We introduce Consistency-Regularized Information Bottleneck (CRIB), a
novel framework built on the Information Bottleneck principle. CRIB combines a
unified-variate attention mechanism with a consistency regularization scheme to
learn robust representations that filter out noise introduced by missing values
while preserving essential predictive signals. Comprehensive experiments on
four real-world datasets demonstrate the effectiveness of CRIB, which predicts
accurately even under high missing rates. Our code is available in
https://github.com/Muyiiiii/CRIB.

</details>


### [264] [Causally-Enhanced Reinforcement Policy Optimization](https://arxiv.org/abs/2509.23095)
*Xiangqi Wang,Yue Huang,Yujun Zhou,Xiaonan Luo,Kehan Guo,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 提出因果增强策略优化（CE - PO）框架，可减少奖励破解和不忠实推理，提升模型鲁棒性与准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型使用强化目标训练时会通过捷径策略得到表面正确答案，存在推理不忠实和在小因果扰动下性能下降的问题。

Method: 引入CE - PO框架，用可微代理增强策略优化，通过雅可比敏感性估计模型内部影响，抑制干扰线索，用Minkowski组合器融合连贯性得分和任务准确率反馈。

Result: 在4个数据集实验中，CE - PO平均比基线提高准确率5.49%（最高达9.58%），提升了对因果翻转和反事实编辑的鲁棒性。

Conclusion: CE - PO能减少奖励破解和不忠实推理，在接近同等准确率下提升模型鲁棒性。

Abstract: Large language models (LLMs) trained with reinforcement objectives often
achieve superficially correct answers via shortcut strategies, pairing correct
outputs with spurious or unfaithful reasoning and degrading under small causal
perturbations. We introduce Causally-Enhanced Policy Optimization (CE-PO), a
drop-in reward-shaping framework that augments policy optimization with a
differentiable proxy for causal coherence along the generation pathway from
prompt (Z) to rationale (X) to answer (Y). CE-PO estimates model-internal
influence with Jacobian-based sensitivities, counterfactually hardens these
signals to suppress nuisance cues, and fuses the resulting coherence score with
task-accuracy feedback via a Minkowski (power-mean) combiner, exposing a single
tunable between accuracy and coherence trade-off. The unified reward integrates
with PPO/GRPO without architectural changes. Across reasoning benchmarks and
causal stress tests, CE-PO reduces reward hacking and unfaithful
chain-of-thought while improving robustness to correlation-causation flips and
light counterfactual edits, all at near-parity accuracy. Experimental results
across 4 datasets show that CE-PO improves accuracy over baselines by 5.49% on
average (up to 9.58%), while improving robustness to correlation-causation
flips and light counterfactual edits.

</details>


### [265] [Bridging Discrete and Continuous RL: Stable Deterministic Policy Gradient with Martingale Characterization](https://arxiv.org/abs/2509.23711)
*Ziheng Cheng,Xin Guo,Yufei Zhang*

Main category: cs.LG

TL;DR: 本文研究连续时间强化学习的确定性策略梯度方法，提出CT - DDPG算法，实验表明该算法稳定性和收敛速度优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 离散时间强化学习算法扩展到连续时间设置时对时间离散化敏感，稳定性差且收敛慢，而许多现实应用是连续复杂的。

Method: 基于优势函数的类似物推导连续时间策略梯度公式，建立鞅特征，提出CT - DDPG算法。

Result: 数值实验显示CT - DDPG算法在不同时间离散化和噪声水平的控制任务中，稳定性和收敛速度优于现有离散时间和连续时间方法。

Conclusion: CT - DDPG算法能在连续时间环境中用确定性策略实现稳定学习。

Abstract: The theory of discrete-time reinforcement learning (RL) has advanced rapidly
over the past decades. Although primarily designed for discrete environments,
many real-world RL applications are inherently continuous and complex. A major
challenge in extending discrete-time algorithms to continuous-time settings is
their sensitivity to time discretization, often leading to poor stability and
slow convergence. In this paper, we investigate deterministic policy gradient
methods for continuous-time RL. We derive a continuous-time policy gradient
formula based on an analogue of the advantage function and establish its
martingale characterization. This theoretical foundation leads to our proposed
algorithm, CT-DDPG, which enables stable learning with deterministic policies
in continuous-time environments. Numerical experiments show that the proposed
CT-DDPG algorithm offers improved stability and faster convergence compared to
existing discrete-time and continuous-time methods, across a wide range of
control tasks with varying time discretizations and noise levels.

</details>


### [266] [FraudTransformer: Time-Aware GPT for Transaction Fraud Detection](https://arxiv.org/abs/2509.23712)
*Gholamali Aminian,Andrew Elliott,Tiger Li,Timothy Cheuk Hin Wong,Victor Claude Dehon,Lukasz Szpruch,Carsten Maple,Christopher Read,Martin Brown,Gesine Reinert,Mo Mamouei*

Main category: cs.LG

TL;DR: 介绍FraudTransformer模型用于检测银行支付欺诈，实验显示其性能优于多个基线模型。


<details>
  <summary>Details</summary>
Motivation: 现实银行流中检测支付欺诈需要能利用事件顺序和时间间隔的模型。

Method: 引入FraudTransformer模型，增强GPT架构，添加专用时间编码器和学习的位置编码器。

Result: 在大型工业数据集实验中，FraudTransformer超越四个经典基线模型和部分变压器消融模型，在测试集上有最高AUROC和PRAUC。

Conclusion: FraudTransformer在银行支付欺诈检测上表现出色。

Abstract: Detecting payment fraud in real-world banking streams requires models that
can exploit both the order of events and the irregular time gaps between them.
We introduce FraudTransformer, a sequence model that augments a vanilla
GPT-style architecture with (i) a dedicated time encoder that embeds either
absolute timestamps or inter-event values, and (ii) a learned positional
encoder that preserves relative order. Experiments on a large industrial
dataset -- tens of millions of transactions and auxiliary events -- show that
FraudTransformer surpasses four strong classical baselines (Logistic
Regression, XGBoost and LightGBM) as well as transformer ablations that omit
either the time or positional component. On the held-out test set it delivers
the highest AUROC and PRAUC.

</details>


### [267] [Effective Quantization of Muon Optimizer States](https://arxiv.org/abs/2509.23106)
*Aman Gupta,Rafael Celente,Abhishek Shivanna,D. T. Braithwaite,Gregory Dexter,Shao Tang,Hiroto Udagawa,Daniel Silva,Rohan Ramanath,S. Sathiya Keerthi*

Main category: cs.LG

TL;DR: 本文提出8位Muon优化器，支持线性和动态量化方案，内存占用减少约74%，性能表现佳并给出理论解释。


<details>
  <summary>Details</summary>
Motivation: Muon优化器虽比AdamW有优势但有状态需存储权重和梯度，8位AdamW变体有局限性，故提出8位Muon优化器。

Method: 使用块量化方法，引入支持线性和动态量化方案的8位Muon优化器。

Result: 8位Muon在两种量化方案下保持稳定，内存占用减少约74%，预训练和微调实验中性能表现好。

Conclusion: 8位Muon优化器有效，在减少内存占用同时保持性能，理论解释了量化下的鲁棒性。

Abstract: The Muon optimizer, based on matrix orthogonalization, has recently shown
faster convergence and up to 2x computational efficiency over AdamW in LLM
pretraining. Like AdamW, Muon is stateful, requiring storage of both model
weights and accumulated gradients. While 8-bit AdamW variants mitigate this
overhead using blockwise quantization, they are typically stable only under
dynamic quantization - which improves stability on linear quantization for
extreme values. In this paper, we introduce the 8-bit Muon optimizer using
blockwise quantization, supporting both linear and dynamic schemes. We
demonstrate that 8-bit Muon maintains stability under both, while delivering
$\sim$74\% reduction in memory footprint compared to full-precision Muon. In
extensive experiments, 8-bit Muon closely matches the performance of Muon while
outperforming AdamW and 8-bit AdamW in pre-training a 1.6B model on 4B FineWeb
tokens. It also shows competitive results when fine-tuning the Llama 3.2 3B
model on post-training data. We also provide a theoretical perspective to help
explain this robustness under quantization.

</details>


### [268] [Bayesian Mixture-of-Experts: Towards Making LLMs Know What They Don't Know](https://arxiv.org/abs/2509.23830)
*Albus Yizhuo Li*

Main category: cs.LG

TL;DR: 论文提出贝叶斯MoE路由框架，研究三类引入不确定性的方法，实验证明该框架提升路由稳定性等，为构建更可靠大语言模型提供途径。


<details>
  <summary>Details</summary>
Motivation: 标准确定性路由机制导致模型校准错误和过度自信，系统常不知自身未知。

Method: 提出贝叶斯MoE路由框架，对路由决策建模概率分布，研究在权重空间、对数空间和选择空间引入不确定性的三类方法。

Result: 在30亿参数MoE模型实验中，该框架显著提升路由稳定性、分布内校准和分布外检测能力，能创建更可靠内部不确定性信号。

Conclusion: 该工作为构建更健壮、有自我认知的大语言模型提供实用且计算可行的途径。

Abstract: The Mixture-of-Experts (MoE) architecture has enabled the creation of massive
yet efficient Large Language Models (LLMs). However, the standard deterministic
routing mechanism presents a significant limitation: its inherent brittleness
is a key contributor to model miscalibration and overconfidence, resulting in
systems that often do not know what they don't know.
  This thesis confronts this challenge by proposing a structured
\textbf{Bayesian MoE routing framework}. Instead of forcing a single,
deterministic expert selection, our approach models a probability distribution
over the routing decision itself. We systematically investigate three families
of methods that introduce this principled uncertainty at different stages of
the routing pipeline: in the \textbf{weight-space}, the \textbf{logit-space},
and the final \textbf{selection-space}.
  Through a series of controlled experiments on a 3-billion parameter MoE
model, we demonstrate that this framework significantly improves routing
stability, in-distribution calibration, and out-of-distribution (OoD)
detection. The results show that by targeting this core architectural
component, we can create a more reliable internal uncertainty signal. This work
provides a practical and computationally tractable pathway towards building
more robust and self-aware LLMs, taking a crucial step towards making them know
what they don't know.

</details>


### [269] [RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility](https://arxiv.org/abs/2509.23115)
*Haoyu He,Haozheng Luo,Yan Chen,Qi R. Wang*

Main category: cs.LG

TL;DR: 提出RHYTHM框架预测人类移动性，采用时间标记化等方法，在三个真实数据集上评估，精度提升、训练时间减少，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决人类移动性预测中复杂的长距离依赖和多尺度周期性行为带来的挑战。

Method: 采用时间标记化将轨迹划分为每日段并编码为离散标记，添加预计算的提示嵌入丰富标记表示，冻结预训练LLM骨干以降低复杂度和内存成本。

Result: 与现有方法相比，整体精度提高2.4%，周末提高5.0%，训练时间减少24.6%。

Conclusion: RHYTHM框架在人类移动性预测方面具有良好效果。

Abstract: Predicting human mobility is inherently challenging due to complex long-range
dependencies and multi-scale periodic behaviors. To address this, we introduce
RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility),
a unified framework that leverages large language models (LLMs) as
general-purpose spatio-temporal predictors and trajectory reasoners.
Methodologically, RHYTHM employs temporal tokenization to partition each
trajectory into daily segments and encode them as discrete tokens with
hierarchical attention that captures both daily and weekly dependencies,
thereby significantly reducing the sequence length while preserving cyclical
information. Additionally, we enrich token representations by adding
pre-computed prompt embeddings for trajectory segments and prediction targets
via a frozen LLM, and feeding these combined embeddings back into the LLM
backbone to capture complex interdependencies. Computationally, RHYTHM freezes
the pretrained LLM's backbone to reduce attention complexity and memory cost.
We evaluate our model against state-of-the-art methods using three real-world
datasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a
5.0% increase on weekends, and a 24.6% reduction in training time. Code is
publicly available at https://github.com/he-h/rhythm.

</details>


### [270] [Differentiable Sparsity via $D$-Gating: Simple and Versatile Structured Penalization](https://arxiv.org/abs/2509.23898)
*Chris Kolb,Laetitia Frost,Bernd Bischl,David Rügamer*

Main category: cs.LG

TL;DR: 提出D - Gating方法解决结构化稀疏正则化问题，理论证明其与原问题等价，实验验证性能优越。


<details>
  <summary>Details</summary>
Motivation: 结构化稀疏正则化的不可微性与传统随机梯度下降不兼容，需要专门优化器或无正式保证的事后剪枝。

Method: 提出D - Gating，将每组权重拆分为主权重向量和多个标量门控因子。

Result: 理论上证明D - Gating与原组稀疏问题等价，收敛速度快；实验上在多种任务中实现了强性能 - 稀疏性权衡，优于直接优化结构化惩罚和传统剪枝基线。

Conclusion: D - Gating是解决结构化稀疏正则化问题的有效方法，在理论和实践上都有优势。

Abstract: Structured sparsity regularization offers a principled way to compact neural
networks, but its non-differentiability breaks compatibility with conventional
stochastic gradient descent and requires either specialized optimizers or
additional post-hoc pruning without formal guarantees. In this work, we propose
$D$-Gating, a fully differentiable structured overparameterization that splits
each group of weights into a primary weight vector and multiple scalar gating
factors. We prove that any local minimum under $D$-Gating is also a local
minimum using non-smooth structured $L_{2,2/D}$ penalization, and further show
that the $D$-Gating objective converges at least exponentially fast to the
$L_{2,2/D}$-regularized loss in the gradient flow limit. Together, our results
show that $D$-Gating is theoretically equivalent to solving the original group
sparsity problem, yet induces distinct learning dynamics that evolve from a
non-sparse regime into sparse optimization. We validate our theory across
vision, language, and tabular tasks, where $D$-Gating consistently delivers
strong performance-sparsity tradeoffs and outperforms both direct optimization
of structured penalties and conventional pruning baselines.

</details>


### [271] [Impute-MACFM: Imputation based on Mask-Aware Flow Matching](https://arxiv.org/abs/2509.23126)
*Dengyi Liu,Honggang Wang,Hua Fang*

Main category: cs.LG

TL;DR: 提出Impute - MACFM框架用于表格数据插补，在多个基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据插补方法存在施加限制假设、难以处理复杂跨特征结构、不稳定和推理成本高等问题。

Method: 提出Impute - MACFM框架，其掩码感知目标仅在缺失条目上构建轨迹，约束预测速度；结合稳定性惩罚、一致性正则化和时间衰减噪声注入；推理使用约束保持常微分方程积分。

Result: 在多个基准测试中，Impute - MACFM实现了最先进的结果，比竞争方法提供了更稳健、高效和高质量的插补。

Conclusion: 流匹配是解决表格缺失数据问题（包括纵向数据）的一个有前景的方向。

Abstract: Tabular data are central to many applications, especially longitudinal data
in healthcare, where missing values are common, undermining model fidelity and
reliability. Prior imputation methods either impose restrictive assumptions or
struggle with complex cross-feature structure, while recent generative
approaches suffer from instability and costly inference. We propose
Impute-MACFM, a mask-aware conditional flow matching framework for tabular
imputation that addresses missingness mechanisms, missing completely at random,
missing at random, and missing not at random. Its mask-aware objective builds
trajectories only on missing entries while constraining predicted velocity to
remain near zero on observed entries, using flexible nonlinear schedules.
Impute-MACFM combines: (i) stability penalties on observed positions, (ii)
consistency regularization enforcing local invariance, and (iii) time-decayed
noise injection for numeric features. Inference uses constraint-preserving
ordinary differential equation integration with per-step projection to fix
observed values, optionally aggregating multiple trajectories for robustness.
Across diverse benchmarks, Impute-MACFM achieves state-of-the-art results while
delivering more robust, efficient, and higher-quality imputation than competing
approaches, establishing flow matching as a promising direction for tabular
missing-data problems, including longitudinal data.

</details>


### [272] [C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning](https://arxiv.org/abs/2509.23129)
*Haotian Liu,Shuo Wang,Hongteng Xu*

Main category: cs.LG

TL;DR: 提出C²GSPG方法，增强推理性能并抑制过度自信，在推理和校准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法如GRPO及其变体存在过度自信问题，无法实现有自我意识的推理模型。

Method: 提出GSPG框架消除token级偏差，用归一化序列级概率定义模型置信度，用交叉熵正则化器校准，对非二进制奖励进行非线性归一化和自适应正则化裁剪。

Result: 将C²GSPG应用于大语言模型后训练，在推理准确性和置信度校准上优于现有方法。

Conclusion: C²GSPG是简单有效的置信度校准组序列策略梯度方法，能同时提升推理性能和抑制过度自信。

Abstract: Reinforcement Learning (RL) methods, exemplified by Group Relative Policy
Optimization (GRPO) and its variants, play a central role in developing
reasoning models. However, these methods often suffer from a critical
overconfidence issue, which prevents them from achieving self-aware reasoning
models. In this study, we propose a simple yet effective confidence-calibration
group sequence policy gradient method, called C$^2$GSPG, which simultaneously
enhances reasoning performance while suppressing overconfidence. In principle,
we propose a Group Sequence Policy Gradient (GSPG) framework for learning
reasoning models, which eliminates the token-level bias commonly appearing in
GRPO and its variants. In this framework, we define the model confidence for
each reasoning problem using the normalized sequence-level probability, and
then apply a cross-entropy regularizer to calibrate the model confidence to the
sequence's reward. We demonstrate that the confidence calibration regularizer
and GSPG are collaborative for binary rewards, as their objectives always share
the same gradient direction. For non-binary rewards, we apply nonlinear reward
normalization and adaptive regularizer clipping, mitigating the potential
conflict between the two objectives. Applying C$^2$GSPG to post-train large
language models in logical and mathematical reasoning tasks, we show its
superiority over state-of-the-art methods in both reasoning accuracy and
confidence calibration. The code of C$^2$GSPG is available at
https://github.com/HaotianLiu123/CCGSPG.

</details>


### [273] [Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm](https://arxiv.org/abs/2509.23946)
*Kaisen Yang,Lixuan He,Rushi Shah,Kaicheng Yang,Qinwei Ma,Dianbo Liu,Alex Lamb*

Main category: cs.LG

TL;DR: 提出Explore - Execute Chain (E²C)框架解决CoT及其变体问题，有高效测试时间缩放策略和跨域适应优势。


<details>
  <summary>Details</summary>
Motivation: Chain - of - Thought及其变体存在计算低效、推理路径探索有限和可解释性低等问题，需改进。

Method: 提出E²C框架，将推理分为探索和执行阶段，采用两阶段训练方法，结合SFT和RL。

Result: 在AIME'2024上，E²C测试时间缩放策略用<10%解码令牌达58.1%准确率；跨域适应中，用3.5%令牌比标准SFT准确率高14.5%。

Conclusion: E²C框架通过分离规划和执行，实现了高效、高准确率、强泛化性和更好的可解释性。

Abstract: Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning
abilities of Large Language Models (LLMs), yet their monolithic and
auto-regressive architecture inherently conflates high-level strategic planning
with low-level step-by-step execution, leading to computational inefficiency,
limited exploration of reasoning paths, and reduced interpretability. To
overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a
structured reasoning framework that decouples reasoning into two distinct
phases: an exploratory phase that stochastically generates succinct high-level
plans, followed by an execution phase that deterministically carries out the
chosen plan. Our approach incorporates a two-stage training methodology, which
combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation
algorithm enforcing strict plan adherence - with a subsequent Reinforcement
Learning (RL) stage that capitalizes on the informativeness of exploration and
reinforces the determinism of execution.This decomposition enables an efficient
test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches
58.1% accuracy using <10% of the decoding tokens required by comparable methods
(e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For
cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with
only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher
accuracy than standard SFT on medical benchmarks, delivering state-of-the-art
performance, strong generalization, and greater interpretability by separating
planning from execution. The code and pre-trained models for the project are
available at: https://github.com/yks23/Explore-Execute-Chain.git

</details>


### [274] [Trust Region Reward Optimization and Proximal Inverse Reward Optimization Algorithm](https://arxiv.org/abs/2509.23135)
*Yang Chen,Menglin Zou,Jiaqi Zhang,Yitan Zhang,Junyi Yang,Gael Gendron,Libo Zhang,Jiamou Liu,Michael J. Witbrock*

Main category: cs.LG

TL;DR: 本文提出Trust Region Reward Optimization (TRRO)框架及其实例化算法Proximal Inverse Reward Optimization (PIRO)，解决非对抗性逆强化学习缺乏理论保证问题，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 现代对抗性逆强化学习训练不稳定，非对抗性方法缺乏理论保证，需提出有理论保证的方法。

Method: 提出统一观点，指出非对抗性方法本质，基于此提出TRRO框架，实例化为PIRO算法。

Result: TRRO为逆强化学习提供稳定性保证，PIRO在奖励恢复、策略模仿等任务上匹配或超越现有基线。

Conclusion: TRRO和PIRO有效解决非对抗性逆强化学习缺乏理论保证问题，在多任务上表现优异。

Abstract: Inverse Reinforcement Learning (IRL) learns a reward function to explain
expert demonstrations. Modern IRL methods often use the adversarial (minimax)
formulation that alternates between reward and policy optimization, which often
lead to unstable training. Recent non-adversarial IRL approaches improve
stability by jointly learning reward and policy via energy-based formulations
but lack formal guarantees. This work bridges this gap. We first present a
unified view showing canonical non-adversarial methods explicitly or implicitly
maximize the likelihood of expert behavior, which is equivalent to minimizing
the expected return gap. This insight leads to our main contribution: Trust
Region Reward Optimization (TRRO), a framework that guarantees monotonic
improvement in this likelihood via a Minorization-Maximization process. We
instantiate TRRO into Proximal Inverse Reward Optimization (PIRO), a practical
and stable IRL algorithm. Theoretically, TRRO provides the IRL counterpart to
the stability guarantees of Trust Region Policy Optimization (TRPO) in forward
RL. Empirically, PIRO matches or surpasses state-of-the-art baselines in reward
recovery, policy imitation with high sample efficiency on MuJoCo and
Gym-Robotics benchmarks and a real-world animal behavior modeling task.

</details>


### [275] [Does Weak-to-strong Generalization Happen under Spurious Correlations?](https://arxiv.org/abs/2509.24005)
*Chenruo Liu,Yijun Dong,Qi Lei*

Main category: cs.LG

TL;DR: 对弱到强（W2S）泛化关键问题进行理论和算法研究，分析W2S发生条件并提出改进算法。


<details>
  <summary>Details</summary>
Motivation: 研究在下游任务存在虚假关联时，使用弱教师伪标签微调强预训练学生模型，W2S是否发生及如何改进。

Method: 考虑组不平衡导致的两种虚假关联来源，从理论上刻画W2S增益，进行大量实验验证，并提出在W2S微调后对高置信度数据子集重新训练学生模型的算法。

Result: 理论表明当ηu = ηl时，有足够伪标签W2S总会发生，ηu ≠ ηl时可能失败；实验验证了理论；提出的算法能显著提升W2S性能。

Conclusion: 提出的组标签无关算法能有效改进W2S性能，相比普通W2S微调有一致、显著提升。

Abstract: We initiate a unified theoretical and algorithmic study of a key problem in
weak-to-strong (W2S) generalization: when fine-tuning a strong pre-trained
student with pseudolabels from a weaker teacher on a downstream task with
spurious correlations, does W2S happen, and how to improve it upon failures? We
consider two sources of spurious correlations caused by group imbalance: (i) a
weak teacher fine-tuned on group-imbalanced labeled data with a minority group
of fraction $\eta_\ell$, and (ii) a group-imbalanced unlabeled set
pseudolabeled by the teacher with a minority group of fraction $\eta_u$.
Theoretically, a precise characterization of W2S gain at the proportional
asymptotic limit shows that W2S always happens with sufficient pseudolabels
when $\eta_u = \eta_\ell$ but may fail when $\eta_u \ne \eta_\ell$, where W2S
gain diminishes as $(\eta_u - \eta_\ell)^2$ increases. Our theory is
corroborated by extensive experiments on various spurious correlation
benchmarks and teacher-student pairs. To boost W2S performance upon failures,
we further propose a simple, effective algorithmic remedy that retrains the
strong student on its high-confidence data subset after W2S fine-tuning. Our
algorithm is group-label-free and achieves consistent, substantial improvements
over vanilla W2S fine-tuning.

</details>


### [276] [Beyond Heuristics: Globally Optimal Configuration of Implicit Neural Representations](https://arxiv.org/abs/2509.23139)
*Sipeng Chen,Yan Zhang,Shibo Li*

Main category: cs.LG

TL;DR: 文章指出隐式神经表示（INRs）配置缺乏原则性策略，提出OptiINR框架将INR配置作为优化问题，用贝叶斯优化探索参数空间，提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前INRs配置缺乏原则性策略，依赖临时启发式方法、暴力网格搜索或特定任务调整，导致不同模态结果不一致，需要统一框架。

Method: 引入OptiINR框架，利用贝叶斯优化探索离散激活族和连续初始化参数的联合空间。

Result: OptiINR能提供全局最优配置，在不同信号处理应用中持续最大化性能。

Conclusion: OptiINR为INR设计建立了原则性基础，以数据驱动的优化过程取代手动调整。

Abstract: Implicit Neural Representations (INRs) have emerged as a transformative
paradigm in signal processing and computer vision, excelling in tasks from
image reconstruction to 3D shape modeling. Yet their effectiveness is
fundamentally limited by the absence of principled strategies for optimal
configuration - spanning activation selection, initialization scales,
layer-wise adaptation, and their intricate interdependencies. These choices
dictate performance, stability, and generalization, but current practice relies
on ad-hoc heuristics, brute-force grid searches, or task-specific tuning, often
leading to inconsistent results across modalities. This work introduces
OptiINR, the first unified framework that formulates INR configuration as a
rigorous optimization problem. Leveraging Bayesian optimization, OptiINR
efficiently explores the joint space of discrete activation families - such as
sinusoidal (SIREN), wavelet-based (WIRE), and variable-periodic (FINER) - and
their associated continuous initialization parameters. This systematic approach
replaces fragmented manual tuning with a coherent, data-driven optimization
process. By delivering globally optimal configurations, OptiINR establishes a
principled foundation for INR design, consistently maximizing performance
across diverse signal processing applications.

</details>


### [277] [On The Variability of Concept Activation Vectors](https://arxiv.org/abs/2509.24058)
*Julia Wenkmann,Damien Garreau*

Main category: cs.LG

TL;DR: 本文对概念激活向量（CAVs）构建进行细粒度理论分析，量化其变异性，发现CAVs方差随随机样本数N的增加以1/N的速度减小，并给出资源高效应用该方法的实用建议。


<details>
  <summary>Details</summary>
Motivation: 人工智能需提高模型透明度，基于概念的解释是有前景的途径，而CAVs计算依赖随机采样，其结果会因用户不同而有差异，因此要量化这种变异性。

Method: 对CAVs构建进行细粒度理论分析。

Result: 实验证实CAVs的方差随随机样本数N的增加以1/N的速度减小。

Conclusion: 根据结果给出资源高效应用CAVs方法的实用建议。

Abstract: One of the most pressing challenges in artificial intelligence is to make
models more transparent to their users. Recently, explainable artificial
intelligence has come up with numerous method to tackle this challenge. A
promising avenue is to use concept-based explanations, that is, high-level
concepts instead of plain feature importance score. Among this class of
methods, Concept Activation vectors (CAVs), Kim et al. (2018) stands out as one
of the main protagonists. One interesting aspect of CAVs is that their
computation requires sampling random examples in the train set. Therefore, the
actual vectors obtained may vary from user to user depending on the randomness
of this sampling. In this paper, we propose a fine-grained theoretical analysis
of CAVs construction in order to quantify their variability. Our results,
confirmed by experiments on several real-life datasets, point out towards an
universal result: the variance of CAVs decreases as $1/N$, where $N$ is the
number of random examples. Based on this we give practical recommendations for
a resource-efficient application of the method.

</details>


### [278] [TimeExpert: Boosting Long Time Series Forecasting with Temporal Mix of Experts](https://arxiv.org/abs/2509.23145)
*Xiaowen Ma,Shuning Ge,Fan Yang,Xiangyu Li,Yun Chen,Mengting Ma,Wei Zhang,Zhipeng Liu*

Main category: cs.LG

TL;DR: 提出TMOE机制解决Transformer时间序列建模问题，替换注意力机制得到TimeExpert和TimeExpert-G，实验显示其性能优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在时间序列建模中存在无法处理固有滞后效应和异常片段问题，影响预测精度。

Method: 提出Temporal Mix of Experts (TMOE)机制，将K-V对视为局部专家，为每个查询进行自适应专家选择，结合共享全局专家，替换流行时间序列Transformer框架中的注意力机制。

Result: 在七个真实世界的长期预测基准测试中，TimeExpert和TimeExpert-G优于现有方法。

Conclusion: TMOE机制有效解决了Transformer在时间序列建模中的问题，提升了预测性能。

Abstract: Transformer-based architectures dominate time series modeling by enabling
global attention over all timestamps, yet their rigid 'one-size-fits-all'
context aggregation fails to address two critical challenges in real-world
data: (1) inherent lag effects, where the relevance of historical timestamps to
a query varies dynamically; (2) anomalous segments, which introduce noisy
signals that degrade forecasting accuracy. To resolve these problems, we
propose the Temporal Mix of Experts (TMOE), a novel attention-level mechanism
that reimagines key-value (K-V) pairs as local experts (each specialized in a
distinct temporal context) and performs adaptive expert selection for each
query via localized filtering of irrelevant timestamps. Complementing this
local adaptation, a shared global expert preserves the Transformer's strength
in capturing long-range dependencies. We then replace the vanilla attention
mechanism in popular time-series Transformer frameworks (i.e., PatchTST and
Timer) with TMOE, without extra structural modifications, yielding our specific
version TimeExpert and general version TimeExpert-G. Extensive experiments on
seven real-world long-term forecasting benchmarks demonstrate that TimeExpert
and TimeExpert-G outperform state-of-the-art methods. Code is available at
https://github.com/xwmaxwma/TimeExpert.

</details>


### [279] [A Family of Kernelized Matrix Costs for Multiple-Output Mixture Neural Networks](https://arxiv.org/abs/2509.24076)
*Bo Hu,José C. Príncipe*

Main category: cs.LG

TL;DR: 结合MDNs与对比成本，提出四种核化矩阵成本用于数据密度近似，学习定义混合密度所需的多个中心。


<details>
  <summary>Details</summary>
Motivation: 成对距离成本对自监督和对比特征学习很关键，MDNs广泛用于生成模型和密度近似，本文想结合二者进行数据密度近似。

Method: 结合MDNs与对比成本，采用四种核化矩阵成本（标量成本、向量 - 矩阵成本、矩阵 - 矩阵成本、SVD成本）进行数据密度近似。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Pairwise distance-based costs are crucial for self-supervised and contrastive
feature learning. Mixture Density Networks (MDNs) are a widely used approach
for generative models and density approximation, using neural networks to
produce multiple centers that define a Gaussian mixture. By combining MDNs with
contrastive costs, this paper proposes data density approximation using four
types of kernelized matrix costs: the scalar cost, the vector-matrix cost, the
matrix-matrix cost (the trace of Schur complement), and the SVD cost (the
nuclear norm), for learning multiple centers required to define a mixture
density.

</details>


### [280] [Critique to Verify: Accurate and Honest Test-Time Scaling with RL-Trained Verifiers](https://arxiv.org/abs/2509.23152)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.LG

TL;DR: 提出Mirror - Critique框架训练验证器，实验显示Mirror - Verifier在解决方案准确性和求解器诚实性上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型选择方法在测试时间缩放中难以识别少数正确答案，原因是验证器训练缺乏信息丰富的批判信号。

Method: 引入Mirror - Critique框架，对比模型生成解和真实解获取批判信号，用小指令调优模型结合拒绝采样合成高质量批判数据，冷启动大语言模型提升验证能力，用Mirror - Verifier评估候选解。

Result: Mirror - Verifier在解决方案准确性上显著优于多数投票，还提高了求解器识别并放弃超能力范围问题的诚实性。

Conclusion: Mirror - Critique框架有效，Mirror - Verifier能提升大语言模型推理性能。

Abstract: Test-time scaling via solution sampling and aggregation has become a key
paradigm for improving the reasoning performance of Large Language Models
(LLMs). While reward model selection is commonly employed in this approach, it
often fails to identify minority-yet-correct answers, which limits its
effectiveness beyond that of simple majority voting. We argue that this
limitation stems from a lack of informative critique signals during verifier
training. To bridge this gap, we introduce Mirror-Critique, a framework that
trains a verifier with informative critiques. Our key insight is to leverage
the rich critique signal by contrasting model-generated solutions with
ground-truth solutions. We deploy a small instruction-tuned model to synthesize
high-quality critique data with rejection sampling that teaches the verifier
not only what is wrong, but also why. The synthetic data is used to cold-start
the LLMs in the RLVR process to further improve the verification ability. The
resulting Mirror-Verifier is deployed to evaluate candidate solutions by
generating multiple critiques per solution, aggregating them into a verify
score used for weighted voting or selective abstention. The experimental
results show that our Mirror-Verifier significantly outperforms majority voting
in terms of solution accuracy and also improves the solver's honesty to
recognize and abstain from answering beyond its capability boundaries.

</details>


### [281] [Demographic-Agnostic Fairness without Harm](https://arxiv.org/abs/2509.24077)
*Zhongteng Cai,Mohammad Mahdi Khalili,Xueru Zhang*

Main category: cs.LG

TL;DR: 现有机器学习算法存在公平性问题，基于奇偶性公平会降低模型精度，基于偏好公平需已知个体人口统计信息。本文提出人口统计无关无伤害公平（DAFH）优化算法，理论和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于奇偶性公平会降低模型精度，基于偏好公平需已知个体人口统计信息，为解决这些问题开展研究。

Method: 提出DAFH优化算法，联合学习将人群划分为多组的组分类器和与这些组相关的解耦分类器。

Result: 理论上进行样本复杂度分析，表明该方法在已知人口统计信息训练解耦分类器时能优于基线；在合成和真实数据上的实验验证了该方法。

Conclusion: 提出的DAFH优化算法是有效的，能在不依赖人口统计信息的情况下解决机器学习公平性问题。

Abstract: As machine learning (ML) algorithms are increasingly used in social domains
to make predictions about humans, there is a growing concern that these
algorithms may exhibit biases against certain social groups. Numerous notions
of fairness have been proposed in the literature to measure the unfairness of
ML. Among them, one class that receives the most attention is
\textit{parity-based}, i.e., achieving fairness by equalizing treatment or
outcomes for different social groups. However, achieving parity-based fairness
often comes at the cost of lowering model accuracy and is undesirable for many
high-stakes domains like healthcare. To avoid inferior accuracy, a line of
research focuses on \textit{preference-based} fairness, under which any group
of individuals would experience the highest accuracy and collectively prefer
the ML outcomes assigned to them if they were given the choice between various
sets of outcomes. However, these works assume individual demographic
information is known and fully accessible during training. In this paper, we
relax this requirement and propose a novel \textit{demographic-agnostic
fairness without harm (DAFH)} optimization algorithm, which jointly learns a
group classifier that partitions the population into multiple groups and a set
of decoupled classifiers associated with these groups. Theoretically, we
conduct sample complexity analysis and show that our method can outperform the
baselines when demographic information is known and used to train decoupled
classifiers. Experiments on both synthetic and real data validate the proposed
method.

</details>


### [282] [CrystalGym: A New Benchmark for Materials Discovery Using Reinforcement Learning](https://arxiv.org/abs/2509.23156)
*Prashant Govindarajan,Mathieu Reymond,Antoine Clavaud,Mariano Phielipp,Santiago Miret,Sarath Chandar*

Main category: cs.LG

TL;DR: 提出开源强化学习环境CrystalGym用于晶体材料发现，对常见强化学习算法进行基准测试，还开展大语言模型微调案例研究，为跨学科研究铺路。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习辅助材料设计大多生成式方法未用直接DFT信号作反馈，需借助在线强化学习将其引入材料设计循环。

Method: 提出CrystalGym环境，对常见基于价值和策略的强化学习算法进行基准测试，开展大语言模型微调案例研究。

Result: 所测试算法未解决所有CrystalGym任务，不同算法和环境设置有不同样本效率和收敛到最优的难易程度。

Conclusion: CrystalGym可作为测试平台，为处理耗时奖励信号的强化学习方法带来新挑战，推动跨学科研究。

Abstract: In silico design and optimization of new materials primarily relies on
high-accuracy atomic simulators that perform density functional theory (DFT)
calculations. While recent works showcase the strong potential of machine
learning to accelerate the material design process, they mostly consist of
generative approaches that do not use direct DFT signals as feedback to improve
training and generation mainly due to DFT's high computational cost. To aid the
adoption of direct DFT signals in the materials design loop through online
reinforcement learning (RL), we propose CrystalGym, an open-source RL
environment for crystalline material discovery. Using CrystalGym, we benchmark
common value- and policy-based reinforcement learning algorithms for designing
various crystals conditioned on target properties. Concretely, we optimize for
challenging properties like the band gap, bulk modulus, and density, which are
directly calculated from DFT in the environment. While none of the algorithms
we benchmark solve all CrystalGym tasks, our extensive experiments and
ablations show different sample efficiencies and ease of convergence to
optimality for different algorithms and environment settings. Additionally, we
include a case study on the scope of fine-tuning large language models with
reinforcement learning for improving DFT-based rewards. Our goal is for
CrystalGym to serve as a test bed for reinforcement learning researchers and
material scientists to address these real-world design problems with practical
applications. We therefore introduce a novel class of challenges for
reinforcement learning methods dealing with time-consuming reward signals,
paving the way for future interdisciplinary research for machine learning
motivated by real-world applications.

</details>


### [283] [GeoFunFlow: Geometric Function Flow Matching for Inverse Operator Learning over Complex Geometries](https://arxiv.org/abs/2509.24117)
*Sifan Wang,Zhikai Wu,David van Dijk,Lu Lu*

Main category: cs.LG

TL;DR: 提出GeoFunFlow框架解决复杂几何形状的偏微分方程反问题，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 偏微分方程反问题因不适定性、数据稀疏和不规则几何形状而具有挑战性，传统方法计算昂贵，基于学习的方法多针对规则域或前向建模。

Method: 引入GeoFunFlow框架，结合几何函数自动编码器（GeoFAE）和通过整流流训练的潜在扩散模型。

Result: 在五个基准测试中，GeoFunFlow在复杂几何形状上实现了最先进的重建精度，提供了校准的不确定性量化，并比算子学习和扩散模型基线更高效。

Conclusion: GeoFunFlow框架能有效解决复杂几何形状的偏微分方程反问题。

Abstract: Inverse problems governed by partial differential equations (PDEs) are
crucial in science and engineering. They are particularly challenging due to
ill-posedness, data sparsity, and the added complexity of irregular geometries.
Classical PDE-constrained optimization methods are computationally expensive,
especially when repeated posterior sampling is required. Learning-based
approaches improve efficiency and scalability, yet most are designed for
regular domains or focus on forward modeling. Here, we introduce {\em
GeoFunFlow}, a geometric diffusion model framework for inverse problems on
complex geometries. GeoFunFlow combines a novel geometric function autoencoder
(GeoFAE) and a latent diffusion model trained via rectified flow. GeoFAE
employs a Perceiver module to process unstructured meshes of varying sizes and
produces continuous reconstructions of physical fields, while the diffusion
model enables posterior sampling from sparse and noisy data. Across five
benchmarks, GeoFunFlow achieves state-of-the-art reconstruction accuracy over
complex geometries, provides calibrated uncertainty quantification, and
delivers efficient inference compared to operator-learning and diffusion model
baselines.

</details>


### [284] [Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization](https://arxiv.org/abs/2509.23158)
*Yufei Shen,Ji Hwan Park,Minchao Huang,Jared F. Benge,Justin F. Rousseau,Rosemary A. Lester-Smith,Edison Thomaz*

Main category: cs.LG

TL;DR: 本文通过LSTM模型利用智能手机多模态传感数据检测认知障碍，提出两种增强模型泛化性的技术，提升了模型性能，凸显被动传感监测认知障碍的潜力。


<details>
  <summary>Details</summary>
Motivation: 临床评估对老年人认知衰退检测缺乏敏感性和时间分辨率，被动智能手机传感是有潜力的认知监测方法。

Method: 实现LSTM模型，利用多模态传感数据检测认知障碍，提出常规感知增强和人口统计学个性化两种技术增强模型泛化性。

Result: 在36位老年人6个月的数据上评估，两种技术使基于传感和人口统计学特征训练的模型的AUPRC从0.637提升到0.766。

Conclusion: 被动传感在老年人群认知障碍的可扩展监测方面具有潜力。

Abstract: Early detection of cognitive impairment is critical for timely diagnosis and
intervention, yet infrequent clinical assessments often lack the sensitivity
and temporal resolution to capture subtle cognitive declines in older adults.
Passive smartphone sensing has emerged as a promising approach for naturalistic
and continuous cognitive monitoring. Building on this potential, we implemented
a Long Short-Term Memory (LSTM) model to detect cognitive impairment from
sequences of daily behavioral features, derived from multimodal sensing data
collected in an ongoing one-year study of older adults. Our key contributions
are two techniques to enhance model generalizability across participants: (1)
routine-aware augmentation, which generates synthetic sequences by replacing
each day with behaviorally similar alternatives, and (2) demographic
personalization, which reweights training samples to emphasize those from
individuals demographically similar to the test participant. Evaluated on
6-month data from 36 older adults, these techniques jointly improved the Area
Under the Precision-Recall Curve (AUPRC) of the model trained on sensing and
demographic features from 0.637 to 0.766, highlighting the potential of
scalable monitoring of cognitive impairment in aging populations with passive
sensing.

</details>


### [285] [A signal separation view of classification](https://arxiv.org/abs/2509.24140)
*H. N. Mhaskar,Ryan O'Dowd*

Main category: cs.LG

TL;DR: 提出一种在任意紧致度量空间中进行分类的新方法，用局部三角多项式核实现分类，并用MASC算法处理边界问题，在多个数据集上验证。


<details>
  <summary>Details</summary>
Motivation: 机器学习分类问题常基于函数逼近，本文寻求替代方法，理论上得到类别数量和完美分类。

Method: 使用为信号处理中点源信号分离问题开发的局部三角多项式核，认为不同类别来自不同概率分布，用MASC算法以分层方式处理类别边界。

Result: 在多个模拟和真实数据集上进行了验证。

Conclusion: 提出的方法可用于分类问题，能处理类别边界情况。

Abstract: The problem of classification in machine learning has often been approached
in terms of function approximation. In this paper, we propose an alternative
approach for classification in arbitrary compact metric spaces which, in
theory, yields both the number of classes, and a perfect classification using a
minimal number of queried labels. Our approach uses localized trigonometric
polynomial kernels initially developed for the point source signal separation
problem in signal processing. Rather than point sources, we argue that the
various classes come from different probability distributions. The localized
kernel technique developed for separating point sources is then shown to
separate the supports of these distributions. This is done in a hierarchical
manner in our MASC algorithm to accommodate touching/overlapping class
boundaries. We illustrate our theory on several simulated and real life
datasets, including the Salinas and Indian Pines hyperspectral datasets and a
document dataset.

</details>


### [286] [ProtoTS: Learning Hierarchical Prototypes for Explainable Time Series Forecasting](https://arxiv.org/abs/2509.23159)
*Ziheng Peng,Shijie Ren,Xinyue Gu,Linxiao Yang,Xiting Wang,Liang Sun*

Main category: cs.LG

TL;DR: 提出ProtoTS可解释预测框架，在多个基准测试中表现出色，兼具准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在时间序列预测中需理解决策过程，现有可解释模型无法揭示输入变量如何共同影响预测曲线的整体时间模式。

Method: 提出ProtoTS框架，通过建模原型时间模式，基于去噪表示计算实例 - 原型相似度，原型分层组织。

Result: 在多个现实基准测试中，ProtoTS预测准确性超过现有方法，还能提供专家可引导的解释。

Conclusion: ProtoTS能在保证预测准确性的同时，实现透明的决策过程，有助于更好地理解模型和支持决策。

Abstract: While deep learning has achieved impressive performance in time series
forecasting, it becomes increasingly crucial to understand its decision-making
process for building trust in high-stakes scenarios. Existing interpretable
models often provide only local and partial explanations, lacking the
capability to reveal how heterogeneous and interacting input variables jointly
shape the overall temporal patterns in the forecast curve. We propose ProtoTS,
a novel interpretable forecasting framework that achieves both high accuracy
and transparent decision-making through modeling prototypical temporal
patterns. ProtoTS computes instance-prototype similarity based on a denoised
representation that preserves abundant heterogeneous information. The
prototypes are organized hierarchically to capture global temporal patterns
with coarse prototypes while capturing finer-grained local variations with
detailed prototypes, enabling expert steering and multi-level interpretability.
Experiments on multiple realistic benchmarks, including a newly released LOF
dataset, show that ProtoTS not only exceeds existing methods in forecast
accuracy but also delivers expert-steerable interpretations for better model
understanding and decision support.

</details>


### [287] [F-Adapter: Frequency-Adaptive Parameter-Efficient Fine-Tuning in Scientific Machine Learning](https://arxiv.org/abs/2509.23173)
*Hangwei Zhang,Chun Kang,Yan Wang,Difan Zou*

Main category: cs.LG

TL;DR: 首次系统研究预训练大算子模型（LOMs）的参数高效微调（PEFT），发现LoRA性能不如Adapter，引入频率自适应适配器（F - Adapter），在3D Navier - Stokes基准测试中取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: PEFT在科学机器学习领域未被探索，旨在为建模复杂物理系统的预训练LOMs找到有效的微调方法。

Method: 对比LoRA和Adapter在LOMs上的性能，理论分析两者在傅里叶层的近似误差，引入根据频谱复杂度分配容量的F - Adapter。

Result: F - Adapter在多个具有挑战性的3D Navier - Stokes基准测试中达到SOTA，在泛化性和频谱保真度上显著优于LoRA和其他常用PEFT技术。

Conclusion: 这是首次探索科学机器学习中的PEFT，F - Adapter是该领域的有效范式。

Abstract: Parameter-efficient fine-tuning (PEFT) of powerful pre-trained models for
complex downstream tasks has proven effective in vision and language
processing, yet this paradigm remains unexplored in scientific machine
learning, where the objective is to model complex physical systems. We conduct
the first systematic study of PEFT for pre-trained Large Operator Models (LOMs)
obtained by scaling variants of Fourier Neural Operator. First, we observe that
the widely used Low-Rank Adaptation (LoRA) yields markedly poorer performance
on LOMs than Adapter tuning. Then, we further theoretically establish that
stacked LoRA incurs a depth-amplified lower bound on approximation error within
Fourier layers, whereas adapters retain universal approximation capacity and,
by concentrating parameters on energy-dominant low-frequency modes, attain
exponentially decaying error with bottleneck width in the Fourier domain.
Motivated by the robust empirical gains of adapters and by our theoretical
characterization of PDE solutions as spectrally sparse, we introduce
Frequency-Adaptive Adapter (F-Adapter). F-Adapter allocates adapter capacity
based on spectral complexity, assigning higher-dimension modules to
low-frequency components and lower-dimension modules to high-frequency
components. Our F-Adapters establish state-of-the-art (SOTA) results on
multiple challenging 3D Navier-Stokes benchmarks, markedly enhancing both
generalization and spectral fidelity over LoRA and other PEFT techniques
commonly used in LLMs. To the best of our knowledge, this work is the first to
explore PEFT for scientific machine-learning and establishes F-Adapter as an
effective paradigm for this domain.

</details>


### [288] [Semantic Editing with Coupled Stochastic Differential Equations](https://arxiv.org/abs/2509.24223)
*Jianxin Zhang,Clayton Scott*

Main category: cs.LG

TL;DR: 提出用耦合随机微分方程（耦合SDE）引导预训练生成模型采样过程来编辑图像，效果好。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法常扭曲细节或引入伪影，编辑预训练文本到图像模型的图像内容有挑战。

Method: 使用耦合随机微分方程引导可通过求解SDE采样的预训练生成模型的采样过程，用相同相关噪声驱动源图像和编辑图像。

Result: 方法无需重新训练或辅助网络，实现高提示保真度和近像素级一致性。

Conclusion: 耦合SDE是用于可控生成式AI的简单而强大的工具。

Abstract: Editing the content of an image with a pretrained text-to-image model remains
challenging. Existing methods often distort fine details or introduce
unintended artifacts. We propose using coupled stochastic differential
equations (coupled SDEs) to guide the sampling process of any pre-trained
generative model that can be sampled by solving an SDE, including diffusion and
rectified flow models. By driving both the source image and the edited image
with the same correlated noise, our approach steers new samples toward the
desired semantics while preserving visual similarity to the source. The method
works out-of-the-box-without retraining or auxiliary networks-and achieves high
prompt fidelity along with near-pixel-level consistency. These results position
coupled SDEs as a simple yet powerful tool for controlled generative AI.

</details>


### [289] [ZeroSiam: An Efficient Siamese for Test-Time Entropy Optimization without Collapse](https://arxiv.org/abs/2509.23183)
*Guohao Chen,Shuaicheng Niu,Deyu Chen,Jiahao Yang,Zitian Zhang,Mingkui Tan,Pengcheng Wu,Zhiqi Shen*

Main category: cs.LG

TL;DR: 本文提出ZeroSiam架构用于测试时熵最小化，防止模型崩溃，提升性能，在多任务和模型上表现稳定。


<details>
  <summary>Details</summary>
Motivation: 纯熵最小化会导致模型采用不可泛化的捷径，出现崩溃解，需要改进。

Method: 引入ZeroSiam，通过可学习预测器和分类器前的停止梯度算子实现非对称散度对齐。

Result: ZeroSiam不仅防止崩溃解，还能吸收和正则化有偏学习信号，在多种任务和模型上表现更稳定，开销小。

Conclusion: ZeroSiam简单有效，在测试时熵最小化任务中能提升模型性能和稳定性。

Abstract: Test-time entropy minimization helps adapt a model to novel environments and
incentivize its reasoning capability, unleashing the model's potential during
inference by allowing it to evolve and improve in real-time using its own
predictions, achieving promising performance. However, pure entropy
minimization can favor non-generalizable shortcuts, such as inflating the logit
norm and driving all predictions to a dominant class to reduce entropy, risking
collapsed solutions (e.g., constant one-hot outputs) that trivially minimize
the objective without meaningful learning. In this paper, we introduce
ZeroSiam, an efficient asymmetric Siamese architecture tailored for test-time
entropy minimization. ZeroSiam prevents collapse through asymmetric divergence
alignment, which is efficiently achieved by a learnable predictor and a
stop-gradient operator before the classifier. We provide empirical and
theoretical evidence that ZeroSiam not only prevents collapse solutions, but
also absorbs and regularizes biased learning signals, enhancing performance
even when no collapse occurs. Despite its simplicity, extensive results show
that ZeroSiam performs more stably over prior methods using negligible
overhead, demonstrating efficacy on both vision adaptation and large language
model reasoning tasks across challenging test scenarios and diverse models,
including tiny models that are particularly collapse-prone.

</details>


### [290] [AuON: A Linear-time Alternative to Semi-Orthogonal Momentum Updates](https://arxiv.org/abs/2509.24320)
*Dipan Maity*

Main category: cs.LG

TL;DR: 研究正交梯度更新优化方法，提出线性时间优化器AuON及其混合变体Hybrid - AuON，实验表明性能与AdamW和Muon相当。


<details>
  <summary>Details</summary>
Motivation: 传统正交梯度更新方法计算成本高，近期方法虽有改进但二次成本仍是瓶颈，需更高效方法。

Method: 研究动量更新半正交性质，在谱范数信任区域内约束动量更新，提出AuON结合双曲余弦RMS缩放变换与归一化，还引入Hybrid - AuON应用单次牛顿 - 舒尔茨迭代。

Result: AuON和Hybrid - AuON在视觉和语言基准测试中性能与AdamW和Muon相当。

Conclusion: AuON和Hybrid - AuON在不构建半正交矩阵的情况下，兼具有效性和计算效率，是优化机器学习的有效方法。

Abstract: Orthogonal gradient updates have emerged as a promising direction in
optimization for machine learning. However, traditional approaches such as
SVD/QR decomposition incur prohibitive computational costs of O(n^3) and
underperform compared to well-tuned SGD with momentum, since momentum is
applied only after strict orthogonalization. Recent advances, such as Muon,
improve efficiency by applying momentum before orthogonalization and producing
semi-orthogonal matrices via Newton-Schulz iterations, reducing complexity to
O(n^2). Nevertheless, quadratic costs remain a bottleneck.
  In this work, we study the semi-orthogonal properties of momentum-based
updates and develop a method to bound momentum updates under a spectral-norm
trust region, preserving directional information without requiring explicit
semi-orthogonalization.
  We propose AuON (Alternative Unit-norm momentum updates by Normalized
nonlinear scaling), a linear-time optimizer that achieves strong performance
without constructing semi-orthogonal matrices, while preserving structural
alignment and reconditioning ill-posed updates. Our approach combines
hyperbolic-cosine RMS scaling transformations with normalization, demonstrating
both effectiveness and computational efficiency compared to Newton-Schulz
methods. We further introduce a hybrid variant (Hybrid-AuON) that applies a
single Newton-Schulz iteration. Experiments across vision and language
benchmarks show that AuON and its hybrid variant achieve performance comparable
to strong baselines such as AdamW and Muon.
  Code is available at: https://github.com/ryyzn9/AuON

</details>


### [291] [CoSIFL: Collaborative Secure and Incentivized Federated Learning with Differential Privacy](https://arxiv.org/abs/2509.23190)
*Zhanhong Xie,Meifan Zhang,Lihua Yin*

Main category: cs.LG

TL;DR: 提出CoSIFL框架，集成主动报警、本地差分隐私和激励机制，经实验验证能提升模型鲁棒性并降低服务器成本。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临恶意客户端和激励参与者贡献高质量数据的挑战。

Method: 提出CoSIFL框架，用主动报警机制和鲁棒聚合防御攻击，设计激励模块，将服务器和客户端的交互建模为两阶段博弈。

Result: 实验表明CoSIFL在标准基准上优于现有方案，提升了模型鲁棒性并降低了服务器总成本。

Conclusion: CoSIFL的集成设计是有效的。

Abstract: Federated learning (FL) has emerged as a promising paradigm for collaborative
model training while preserving data locality. However, it still faces
challenges from malicious or compromised clients, as well as difficulties in
incentivizing participants to contribute high-quality data under strict privacy
requirements. Motivated by these considerations, we propose CoSIFL, a novel
framework that integrates proactive alarming for robust security and local
differential privacy (LDP) for inference attacks, together with a
Stackelberg-based incentive scheme to encourage client participation and data
sharing. Specifically, CoSIFL uses an active alarming mechanism and robust
aggregation to defend against Byzantine and inference attacks, while a Tullock
contest-inspired incentive module rewards honest clients for both data
contributions and reliable alarm triggers. We formulate the interplay between
the server and clients as a two-stage game: in the first stage, the server
determines total rewards, selects participants, and fixes global iteration
settings, whereas in the second stage, each client decides its mini-batch size,
privacy noise scale, and alerting strategy. We prove that the server-client
game admits a unique equilibrium, and analyze how clients' multi-dimensional
attributes - such as non-IID degrees and privacy budgets - jointly affect
system efficiency. Experimental results on standard benchmarks demonstrate that
CoSIFL outperforms state-of-the-art solutions in improving model robustness and
reducing total server costs, highlighting the effectiveness of our integrated
design.

</details>


### [292] [Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization](https://arxiv.org/abs/2509.23202)
*Vage Egiazarian,Roberto L. Castro,Denis Kuznedelev,Andrei Panferov,Eldar Kurtic,Shubhra Pandit,Alexandre Marques,Mark Kurtz,Saleh Ashkboos,Torsten Hoefler,Dan Alistarh*

Main category: cs.LG

TL;DR: 研究MXFP4和NVFP4在大语言模型推理后训练量化中的应用，指出问题并提出MR - GPTQ算法，有显著加速效果和较好准确率。


<details>
  <summary>Details</summary>
Motivation: 虽硬件加速的4位浮点格式如MXFP4和NVFP4有望革新大语言模型推理，但实际效果未被证实，需研究其实际表现。

Method: 提出Micro - Rotated - GPTQ (MR - GPTQ)算法，结合块级Hadamard变换和特定格式优化，开发高性能GPU内核。

Result: 在NVIDIA B200和RTX5090上有显著加速效果，MR - GPTQ匹配或超越现有准确率，提升MXFP4性能接近NVFP4。

Conclusion: FP4并非一定比INT4好，但如MR - GPTQ这样的特定格式方法可开拓准确率 - 性能权衡的新领域。

Abstract: The recent hardware-accelerated microscaling 4-bit floating-point formats
such as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to
revolutionize large language model (LLM) inference. Yet, their practical
benefits remain unproven. We present the first comprehensive study of MXFP4 and
NVFP4 for post-training quantization, revealing gaps between their promise and
real-world performance. Our analysis shows that state-of-the-art methods
struggle with FP4, due to two key issues: (1) NVFP4's small group size provably
neutralizes traditional outlier mitigation techniques; (2) MXFP4's power-of-two
scale quantization severely degrades accuracy due to high induced error. To
bridge this gap, we introduce Micro-Rotated-GPTQ (MR-GPTQ), a variant of the
classic GPTQ quantization algorithm that tailors the quantization process to
FP4's unique properties, by using block-wise Hadamard transforms and
format-specific optimizations. We support our proposal with a set of
high-performance GPU kernels that enable the MR-GPTQ format with negligible
overhead, by rotation fusion into the weights, and fast online computation of
the activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and
2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on
RTX5090. Our extensive empirical evaluation demonstrates that MR-GPTQ matches
or outperforms state-of-the-art accuracy, significantly boosting MXFP4, to the
point where it nears that of NVFP4. We conclude that, while FP4 is not an
automatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock
a new frontier of accuracy-performance trade-offs.

</details>


### [293] [Interpretable Kernel Representation Learning at Scale: A Unified Framework Utilizing Nyström Approximation](https://arxiv.org/abs/2509.24467)
*Maedeh Zarvandi,Michael Timothy,Theresa Wasserer,Debarghya Ghoshdastidar*

Main category: cs.LG

TL;DR: 提出KREPES框架用于可扩展的基于核的表示学习，实验证明其效率并能实现表征可解释性。


<details>
  <summary>Details</summary>
Motivation: 核方法可扩展性受时间和内存成本限制，且缺乏可扩展的基于核的表示学习框架。

Method: 引入KREPES框架，通过Nyström近似实现基于核的表示学习。

Result: 在大型图像和表格数据集实验中展示了效率。

Conclusion: KREPES框架可用于可扩展的基于核的表示学习，且能使学习的表征具有可解释性，优于深度模型。

Abstract: Kernel methods provide a theoretically grounded framework for non-linear and
non-parametric learning, with strong analytic foundations and statistical
guarantees. Yet, their scalability has long been limited by prohibitive time
and memory costs. While progress has been made in scaling kernel regression, no
framework exists for scalable kernel-based representation learning, restricting
their use in the era of foundation models where representations are learned
from massive unlabeled data. We introduce KREPES -- a unified, scalable
framework for kernel-based representation learning via Nystr\"om approximation.
KREPES accommodates a wide range of unsupervised and self-supervised losses,
and experiments on large image and tabular datasets demonstrate its efficiency.
Crucially, KREPES enables principled interpretability of the learned
representations, an immediate benefit over deep models, which we substantiate
through dedicated analysis.

</details>


### [294] [Towards Monotonic Improvement in In-Context Reinforcement Learning](https://arxiv.org/abs/2509.23209)
*Wenhao Zhang,Shao Zhang,Xihuai Wang,Yang Li,Ying Wen*

Main category: cs.LG

TL;DR: 指出现有ICRL模型测试时无法持续提升性能问题，提出CV - ICRL方法解决上下文模糊问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于在线RL的大序列模型在ICRL测试时无法像训练数据那样持续提升性能，存在上下文模糊问题。

Method: 引入上下文价值到训练阶段，提出CV - ICRL，用上下文价值作为策略理想性能信号，证明其能收紧性能差距下界，并提出训练和测试时估计上下文价值的方法。

Result: 在Dark Room和Minigrid测试平台实验表明，CV - ICRL有效减轻性能下降，提升ICRL整体能力。

Conclusion: CV - ICRL能解决ICRL中的上下文模糊问题，提升性能。

Abstract: In-Context Reinforcement Learning (ICRL) has emerged as a promising paradigm
for developing agents that can rapidly adapt to new tasks by leveraging past
experiences as context, without updating their parameters. Recent approaches
train large sequence models on monotonic policy improvement data from online
RL, aiming to a continue improved testing time performance. However, our
experimental analysis reveals a critical flaw: these models cannot show a
continue improvement like the training data during testing time. Theoretically,
we identify this phenomenon as Contextual Ambiguity, where the model's own
stochastic actions can generate an interaction history that misleadingly
resembles that of a sub-optimal policy from the training data, initiating a
vicious cycle of poor action selection. To resolve the Contextual Ambiguity, we
introduce Context Value into training phase and propose Context Value Informed
ICRL (CV-ICRL). CV-ICRL use Context Value as an explicit signal representing
the ideal performance theoretically achievable by a policy given the current
context. As the context expands, Context Value could include more task-relevant
information, and therefore the ideal performance should be non-decreasing. We
prove that the Context Value tightens the lower bound on the performance gap
relative to an ideal, monotonically improving policy. We fruther propose two
methods for estimating Context Value at both training and testing time.
Experiments conducted on the Dark Room and Minigrid testbeds demonstrate that
CV-ICRL effectively mitigates performance degradation and improves overall ICRL
abilities across various tasks and environments. The source code and data of
this paper are available at
https://github.com/Bluixe/towards_monotonic_improvement .

</details>


### [295] [One-Shot Multi-Label Causal Discovery in High-Dimensional Event Sequences](https://arxiv.org/abs/2509.23213)
*Hugo Math,Robin Schön,Rainer Lienhart*

Main category: cs.LG

TL;DR: 提出OSCAR方法解决稀疏事件序列因果推断扩展性问题，在汽车数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理数千种稀疏事件类型的事件序列因果关系时无法扩展，需要新方法。

Method: 提出OSCAR，一种一次性因果自回归方法，使用两个预训练的Transformer作为密度估计器来推断每个序列的马尔可夫边界。

Result: 在有29,100个事件和474个标签的真实汽车数据集上，OSCAR能在几分钟内恢复可解释的因果结构，而经典方法无法扩展。

Conclusion: OSCAR使生产规模的实际科学诊断成为可能。

Abstract: Understanding causality in event sequences with thousands of sparse event
types is critical in domains such as healthcare, cybersecurity, or vehicle
diagnostics, yet current methods fail to scale. We present OSCAR, a one-shot
causal autoregressive method that infers per-sequence Markov Boundaries using
two pretrained Transformers as density estimators. This enables efficient,
parallel causal discovery without costly global CI testing. On a real-world
automotive dataset with 29,100 events and 474 labels, OSCAR recovers
interpretable causal structures in minutes, while classical methods fail to
scale, enabling practical scientific diagnostics at production scale.

</details>


### [296] [WirelessMathLM: Teaching Mathematical Reasoning for LLMs in Wireless Communications with Reinforcement Learning](https://arxiv.org/abs/2509.23219)
*Xin Li,Mengbing Liu,Yiyang Zhu,Wenhe Zhang,Li Wei,Jiancheng An,Chau Yuen*

Main category: cs.LG

TL;DR: 提出WirelessMathLM，小模型经特定强化学习能在无线数学问题上媲美大模型，在基准测试表现佳且有正向迁移。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在专业技术数学尤其是无线通信领域表现不佳，需提升模型在该领域的性能。

Method: 构建WirelessMathBench - XL基准，采用Group Relative Policy Optimization (GRPO)和二元验证奖励，从基础检查点直接训练模型。

Result: 7B模型在WirelessMathBench - XL上准确率达39.5%，接近GPT - 4o；GRPO训练提升各规模模型性能，并正向迁移到通用数学基准。

Conclusion: 紧凑模型通过特定领域强化学习可在专业技术数学问题上匹配或超越大模型，且有良好的迁移能力。

Abstract: Large language models (LLMs) excel at general mathematical reasoning but fail
catastrophically on specialized technical mathematics. In wireless
communications, where problems require precise manipulation of
information-theoretic bounds, optimization constraints, and signal processing
formulations, even state-of-the-art models struggle to achieve competent
performance. We present WirelessMathLM, demonstrating that compact models
(0.5B-7B parameters) can match or exceed much larger models through
domain-specific reinforcement learning with verifiable rewards. Our key insight
is that wireless mathematics problems possess a unique property--verifiable
correctness--that enables effective reinforcement learning without human
feedback. We construct WirelessMathBench-XL, a comprehensive benchmark of 4,027
problems from 970 papers. Using Group Relative Policy Optimization (GRPO) with
binary verification rewards, we train models directly from base checkpoints
without supervised warm-start. Our 7B model achieves 39.5% accuracy on
WirelessMathBench-XL, approaching GPT-4o (40.4%) while using about 100 times
fewer parameters than DeepSeek-R1 (671B, 57.4%). Remarkably, GRPO training
nearly doubles performance across all model scales (0.5B +11%, 3B +103%, 7B
+81%), with positive transfer to general mathematics benchmarks--our models
gain +8.4 points on average across MATH, Minerva-Math, OlympiadBench, AMC, and
AIME without any training on these tasks.

</details>


### [297] [Evaluating classification performance across operating contexts: A comparison of decision curve analysis and cost curves](https://arxiv.org/abs/2509.24608)
*Louise AC Millard,Peter A Flach*

Main category: cs.LG

TL;DR: 比较决策曲线分析（DCA）和成本曲线，指出决策曲线与Brier曲线紧密相关，二者有相同x轴，特定阈值下净收益和Brier损失选最优模型一致，Brier曲线适用性更广。


<details>
  <summary>Details</summary>
Motivation: 分类模型评估需考虑应用场景和分类阈值，对比DCA和成本曲线以明确二者关系、优缺点。

Method: 对DCA和成本曲线进行理论对比分析。

Result: 决策曲线与Brier曲线紧密相关，特定阈值下净收益和Brier损失选最优模型一致，Brier曲线适用性更广，Brier曲线下面积为Brier分数。

Conclusion: Brier曲线在更广泛阈值下更适用，还提出上包络决策曲线用于DCA比较。

Abstract: Classification models typically predict a score and use a decision threshold
to produce a classification. Appropriate model evaluation should carefully
consider the context in which a model will be used, including the relative
value of correct classifications of positive versus negative examples, which
affects the threshold that should be used. Decision curve analysis (DCA) and
cost curves are model evaluation approaches that assess the expected utility
and expected loss of prediction models, respectively, across decision
thresholds. We compared DCA and cost curves to determine how they are related,
and their strengths and limitations. We demonstrate that decision curves are
closely related to a specific type of cost curve called a Brier curve. Both
curves are derived assuming model scores are calibrated and setting the
classification threshold using the relative value of correct positive and
negative classifications, and the x-axis of both curves are equivalent. Net
benefit (used for DCA) and Brier loss (used for Brier curves) will always
choose the same model as optimal at any given threshold. Across thresholds,
differences in Brier loss are comparable whereas differences in net benefit
cannot be compared. Brier curves are more generally applicable (when a wider
range of thresholds are plausible), and the area under the Brier curve is the
Brier score. We demonstrate that reference lines common in each space can be
included in either and suggest the upper envelope decision curve as a useful
comparison for DCA showing the possible gain in net benefit that could be
achieved through recalibration alone.

</details>


### [298] [SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts](https://arxiv.org/abs/2509.23232)
*Bingshuai Liu,Ante Wang,Zijun Min,Liang Yao,Haibo Zhang,Yang Liu,Anxiang Zeng,Jinsong Su*

Main category: cs.LG

TL;DR: 提出SPEC - RL框架加速大语言模型基于可验证奖励的强化学习（RLVR）的训练，减少计算时间且不降低策略质量。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR训练过程受限于计算昂贵的滚动阶段，现有加速方法存在收益递减、引入偏差或忽略迭代冗余等问题。

Method: 提出SPEC - RL框架，将推测解码与RL滚动过程集成，重用先前轨迹段作为推测前缀，通过草稿 - 验证机制扩展。

Result: 在多个数学推理和泛化基准测试中，SPEC - RL将滚动时间减少2 - 3倍，且不影响策略质量。

Conclusion: SPEC - RL作为纯滚动阶段的增强方法，能与主流算法无缝集成，为扩展大型推理模型的RLVR提供通用实用途径。

Abstract: Large Language Models (LLMs) increasingly rely on reinforcement learning with
verifiable rewards (RLVR) to elicit reliable chain-of-thought reasoning.
However, the training process remains bottlenecked by the computationally
expensive rollout stage. Existing acceleration methods-such as parallelization,
objective- and data-driven modifications, and replay buffers-either incur
diminishing returns, introduce bias, or overlook redundancy across iterations.
We identify that rollouts from consecutive training epochs frequently share a
large portion of overlapping segments, wasting computation. To address this, we
propose SPEC-RL, a novel framework that integrates SPECulative decoding with
the RL rollout process. SPEC-RL reuses prior trajectory segments as speculative
prefixes and extends them via a draft-and-verify mechanism, avoiding redundant
generation while ensuring policy consistency. Experiments on diverse math
reasoning and generalization benchmarks, including GSM8K, MATH-500,
OlympiadBench, MMLU-STEM, and others, demonstrate that SPEC-RL reduces rollout
time by 2-3x without compromising policy quality. As a purely rollout-stage
enhancement, SPEC-RL integrates seamlessly with mainstream algorithms (e.g.,
PPO, GRPO, DAPO), offering a general and practical path to scale RLVR for large
reasoning models. Our code is available at https://github.com/ShopeeLLM/Spec-RL

</details>


### [299] [More Data or Better Algorithms: Latent Diffusion Augmentation for Deep Imbalanced Regression](https://arxiv.org/abs/2509.23240)
*Shayan Alahyari*

Main category: cs.LG

TL;DR: 针对深度不平衡回归问题缺乏适用于高维数据的数据层解决方案的现状，提出LatentDiff框架，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 现实回归任务中数据分布严重偏斜，不平衡回归研究少，深度不平衡回归缺乏适用于高维数据的数据层解决方案。

Method: 提出LatentDiff框架，使用带优先级生成的条件扩散模型在潜在表示空间合成高质量特征。

Result: 在三个深度不平衡回归基准测试中，在少数区域有显著改进且保持了整体准确性。

Conclusion: LatentDiff框架计算高效，适用于多种数据模态，能解决深度不平衡回归问题。

Abstract: In many real-world regression tasks, the data distribution is heavily skewed,
and models learn predominantly from abundant majority samples while failing to
predict minority labels accurately. While imbalanced classification has been
extensively studied, imbalanced regression remains relatively unexplored. Deep
imbalanced regression (DIR) represents cases where the input data are
high-dimensional and unstructured. Although several data-level approaches for
tabular imbalanced regression exist, deep imbalanced regression currently lacks
dedicated data-level solutions suitable for high-dimensional data and relies
primarily on algorithmic modifications. To fill this gap, we propose
LatentDiff, a novel framework that uses conditional diffusion models with
priority-based generation to synthesize high-quality features in the latent
representation space. LatentDiff is computationally efficient and applicable
across diverse data modalities, including images, text, and other
high-dimensional inputs. Experiments on three DIR benchmarks demonstrate
substantial improvements in minority regions while maintaining overall
accuracy.

</details>


### [300] [Beyond Softmax: A Natural Parameterization for Categorical Random Variables](https://arxiv.org/abs/2509.24728)
*Alessandro Manenti,Cesare Alippi*

Main category: cs.LG

TL;DR: 本文指出softmax函数在处理潜在分类变量时的局限，提出用catnat函数替代，实验表明该函数能提高学习效率和测试性能。


<details>
  <summary>Details</summary>
Motivation: 潜在分类变量的离散性给梯度下降学习算法带来挑战，现有工作多为改进梯度估计技术，本文从新角度解决问题。

Method: 从信息几何角度分析softmax函数的局限性，用由一系列分层二元分裂组成的catnat函数替代softmax函数。

Result: 丰富实验表明，catnat函数提高了学习效率，模型测试性能更高，且易于实现、能无缝集成到现有代码库，与标准训练稳定技术兼容。

Conclusion: catnat函数是比softmax函数更好的选择。

Abstract: Latent categorical variables are frequently found in deep learning
architectures. They can model actions in discrete reinforcement-learning
environments, represent categories in latent-variable models, or express
relations in graph neural networks. Despite their widespread use, their
discrete nature poses significant challenges to gradient-descent learning
algorithms. While a substantial body of work has offered improved gradient
estimation techniques, we take a complementary approach. Specifically, we: 1)
revisit the ubiquitous $\textit{softmax}$ function and demonstrate its
limitations from an information-geometric perspective; 2) replace the
$\textit{softmax}$ with the $\textit{catnat}$ function, a function composed of
a sequence of hierarchical binary splits; we prove that this choice offers
significant advantages to gradient descent due to the resulting diagonal Fisher
Information Matrix. A rich set of experiments - including graph structure
learning, variational autoencoders, and reinforcement learning - empirically
show that the proposed function improves the learning efficiency and yields
models characterized by consistently higher test performance. $\textit{Catnat}$
is simple to implement and seamlessly integrates into existing codebases.
Moreover, it remains compatible with standard training stabilization techniques
and, as such, offers a better alternative to the $\textit{softmax}$ function.

</details>


### [301] [Adaptive Token-Weighted Differential Privacy for LLMs: Not All Tokens Require Equal Protection](https://arxiv.org/abs/2509.23246)
*Manjiang Yu,Priyanka Singh,Xue Li,Yang Cao*

Main category: cs.LG

TL;DR: 现有DPSGD训练时间长、精度低，提出ATDP方法，能减少DP训练时间、加强敏感信息保护且维持模型性能，可集成到现有流程，结合初始阶段可大幅缩短训练时间。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常记忆敏感信息有隐私问题，现有DPSGD方法存在训练时间长和模型精度降低的问题。

Method: 提出Adaptive Token - Weighted Differential Privacy (ATDP)，对敏感和非敏感令牌自适应分配不同梯度权重，早期用大噪声尺度，微调后进行轻量级后处理，在敏感令牌对应参数上注入目标噪声。

Result: 可无缝集成到现有DP微调流程或用于非私有模型，结合初始阶段可减少约90%训练时间，实现可比或更优隐私保护和最小精度损失。

Conclusion: ATDP能减少DP训练时间，加强敏感信息保护，同时保持模型在非敏感数据上的性能，具有较好的实用性和效果。

Abstract: Large language models (LLMs) frequently memorize sensitive or personal
information, raising significant privacy concerns. Existing variants of
differential privacy stochastic gradient descent (DPSGD) inject uniform noise
into every gradient step, significantly extending training time and reducing
model accuracy. We propose that concentrating noise primarily on gradients
associated with sensitive tokens can substantially decrease DP training time,
strengthen the protection of sensitive information, and simultaneously preserve
the model's performance on non-sensitive data. We operationalize this insight
through Adaptive Token-Weighted Differential Privacy (ATDP), a modification of
vanilla DP-SGD that adaptively assigns different gradient weights to sensitive
and non-sensitive tokens. By employing a larger noise scale at the early stage
of training, ATDP rapidly disrupts memorization of sensitive content. As a
result, ATDP only requires a few additional epochs of lightweight
post-processing following standard fine-tuning, injecting targeted noise
primarily on parameters corresponding to sensitive tokens, thus minimally
affecting the model's general capabilities. ATDP can be seamlessly integrated
into any existing DP-based fine-tuning pipeline or directly applied to
non-private models as a fast privacy-enhancing measure. Additionally, combined
with an initial redacted fine-tuning phase, ATDP forms a streamlined DP
pipeline that achieves comparable canary protection to state-of-the-art DP-SGD
methods, significantly reduces the computational overhead of DP fine-tuning,
shortening training time by approximately 90 percent, while achieving
comparable or superior privacy protection and minimal accuracy degradation.

</details>


### [302] [Fidel-TS: A High-Fidelity Benchmark for Multimodal Time Series Forecasting](https://arxiv.org/abs/2509.24789)
*Zhijian Xu,Wanxu Cai,Xilin Dai,Zhaorong Deng,Qiang Xu*

Main category: cs.LG

TL;DR: 现有时间序列预测模型评估缺乏高质量基准，本文提出高保真基准核心原则，构建Fidel - TS基准，实验验证其有效性并指出文本信息因果相关性对多模态预测很关键。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型评估缺乏高质量基准，现有数据集存在诸多问题，可能造成进展假象。

Method: 形式化高保真基准的核心原则，从实时API获取数据构建新的大规模基准Fidel - TS。

Result: 实验揭示了先前基准的关键偏差和设计局限性，证明文本信息的因果相关性是多模态预测性能提升的关键因素。

Conclusion: 构建的Fidel - TS基准有效，文本信息因果相关性对多模态预测性能提升至关重要。

Abstract: The evaluation of time series forecasting models is hindered by a critical
lack of high-quality benchmarks, leading to a potential illusion of progress.
Existing datasets suffer from issues ranging from pre-training data
contamination in the age of LLMs to the causal and description leakage
prevalent in early multimodal designs. To address this, we formalize the core
principles of high-fidelity benchmarking, focusing on data sourcing integrity,
strict causal soundness, and structural clarity. We introduce Fidel-TS, a new
large-scale benchmark built from the ground up on these principles by sourcing
data from live APIs. Our extensive experiments validate this approach by
exposing the critical biases and design limitations of prior benchmarks.
Furthermore, we conclusively demonstrate that the causal relevance of textual
information is the key factor in unlocking genuine performance gains in
multimodal forecasting.

</details>


### [303] [Deep Learning for Subspace Regression](https://arxiv.org/abs/2509.23249)
*Vladimir Fanaskov,Vladislav Trifonov,Alexander Rudikov,Ekaterina Muravleva,Ivan Oseledets*

Main category: cs.LG

TL;DR: 提出将子空间插值问题转化为回归问题，用神经网络逼近高维目标函数，引入冗余预测更大子空间，理论和实验证明可提升精度，还展示了子空间回归在多个任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 在高维参数空间下，经典子空间插值策略不可行或不可靠，需要新方法。

Method: 将插值问题转化为回归问题，引入适合子空间数据的损失函数，用神经网络逼近高维目标函数，引入冗余预测更大子空间。

Result: 理论上降低了常系数椭圆特征问题映射复杂度，使格拉斯曼流形上的映射更平滑；实验上精度显著提高。

Conclusion: 子空间回归对参数特征问题、放缩技术、松弛方法、最优控制和参数偏微分方程求解等任务有用。

Abstract: It is often possible to perform reduced order modelling by specifying linear
subspace which accurately captures the dynamics of the system. This approach
becomes especially appealing when linear subspace explicitly depends on
parameters of the problem. A practical way to apply such a scheme is to compute
subspaces for a selected set of parameters in the computationally demanding
offline stage and in the online stage approximate subspace for unknown
parameters by interpolation. For realistic problems the space of parameters is
high dimensional, which renders classical interpolation strategies infeasible
or unreliable. We propose to relax the interpolation problem to regression,
introduce several loss functions suitable for subspace data, and use a neural
network as an approximation to high-dimensional target function. To further
simplify a learning problem we introduce redundancy: in place of predicting
subspace of a given dimension we predict larger subspace. We show theoretically
that this strategy decreases the complexity of the mapping for elliptic
eigenproblems with constant coefficients and makes the mapping smoother for
general smooth function on the Grassmann manifold. Empirical results also show
that accuracy significantly improves when larger-than-needed subspaces are
predicted. With the set of numerical illustrations we demonstrate that subspace
regression can be useful for a range of tasks including parametric
eigenproblems, deflation techniques, relaxation methods, optimal control and
solution of parametric partial differential equations.

</details>


### [304] [NanoFlux: Adversarial Dual-LLM Evaluation and Distillation For Multi-Domain Reasoning](https://arxiv.org/abs/2509.23252)
*Raviteja Anantha,Soheil Hor,Teodor Nicola Antoniu,Layne C. Price*

Main category: cs.LG

TL;DR: 提出NanoFlux框架生成目标训练数据提升大语言模型推理能力，少量数据效果好且减少计算量，揭示数据集特征与模型性能关系，暗示未来可智能合成小而精准数据集。


<details>
  <summary>Details</summary>
Motivation: 提高大语言模型的推理能力，探索更高效的训练数据生成方法。

Method: 采用攻击者和防御者模型的竞争动态，由工具增强的裁判监督，合成带解释注释的多步问题，还通过基于嵌入的新颖性过滤、工具增强评估和多跳推理实现训练数据自动生成。

Result: 使用NanoFlux生成的数据微调4B参数模型，在不同领域有性能提升，如数学推理+5.9%、科学推理+3.6%、医学推理+16.6%，并减少3 - 14倍计算需求；消融研究揭示数据集特征与模型性能的非单调关系。

Conclusion: 未来模型改进可能在于智能合成小而精准的目标训练数据集。

Abstract: We present NanoFlux, a novel adversarial framework for generating targeted
training data to improve LLM reasoning, where adversarially-generated datasets
containing fewer than 200 examples outperform conventional fine-tuning
approaches. The framework employs a competitive dynamic between models
alternating as Attacker and Defender, supervised by a tool-augmented Judge,
synthesizing multi-step questions with explanatory annotations that target
specific reasoning capabilities. Fine-tuning a 4B-parameter model on
NanoFlux-generated data yields performance gains across diverse domains
compared to full-benchmark fine-tuning: +5.9% on mathematical reasoning
(GSMHard), +3.6% on scientific reasoning (GenomeBench), and +16.6% on medical
reasoning (MultiMedQA), while reducing computational requirements by 3-14x.
Ablation studies reveal a non-monotonic relationship between dataset
characteristics and model performance, uncovering domain-specific optimal
points for question complexity and reasoning quality. NanoFlux automates
training data generation through embedding-based novelty filtering,
tool-augmented evaluation, and multi-hop reasoning, suggesting that future
model improvements may lie in the intelligent synthesis of small, precisely
targeted training datasets.

</details>


### [305] [Scaling Laws and Spectra of Shallow Neural Networks in the Feature Learning Regime](https://arxiv.org/abs/2509.24882)
*Leonardo Defilippis,Yizhou Xu,Julius Girardin,Emanuele Troiani,Vittorio Erba,Lenka Zdeborová,Bruno Loureiro,Florent Krzakala*

Main category: cs.LG

TL;DR: 本文对特征学习机制下二次和对角神经网络的缩放定律进行系统分析，推导了超额风险缩放指数的相图，揭示不同缩放机制和平台行为，建立与网络权重谱特性的联系，理论验证了权重谱幂律尾与网络泛化性能的关联。


<details>
  <summary>Details</summary>
Motivation: 当前对神经网络缩放定律的理论理解大多局限于线性模型，需要对二次和对角神经网络的缩放定律进行研究。

Method: 利用矩阵压缩感知和LASSO的联系，推导超额风险缩放指数的相图，详细刻画训练网络权重的谱特性。

Result: 发现不同缩放机制间的交叉和平台行为，建立缩放机制与网络权重谱特性的联系，理论验证权重谱幂律尾与网络泛化性能的关联。

Conclusion: 从第一性原理对权重谱幂律尾与网络泛化性能的关联给出了解释。

Abstract: Neural scaling laws underlie many of the recent advances in deep learning,
yet their theoretical understanding remains largely confined to linear models.
In this work, we present a systematic analysis of scaling laws for quadratic
and diagonal neural networks in the feature learning regime. Leveraging
connections with matrix compressed sensing and LASSO, we derive a detailed
phase diagram for the scaling exponents of the excess risk as a function of
sample complexity and weight decay. This analysis uncovers crossovers between
distinct scaling regimes and plateau behaviors, mirroring phenomena widely
reported in the empirical neural scaling literature. Furthermore, we establish
a precise link between these regimes and the spectral properties of the trained
network weights, which we characterize in detail. As a consequence, we provide
a theoretical validation of recent empirical observations connecting the
emergence of power-law tails in the weight spectrum with network generalization
performance, yielding an interpretation from first principles.

</details>


### [306] [ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction](https://arxiv.org/abs/2509.23254)
*Zhang-Yu You,Jiahao Ma,Hongzong Li,Ye-Fan Hu,Jian-Dong Huang*

Main category: cs.LG

TL;DR: 提出基于Conformer架构的ABCONFORMER模型用于抗体 - 抗原界面预测，有良好表现并将开源代码。


<details>
  <summary>Details</summary>
Motivation: 准确预测抗体 - 抗原界面在疫苗设计等领域至关重要，但仅从序列进行可靠预测仍有挑战。

Method: 构建基于Conformer架构的ABCONFORMER模型，引入物理启发的滑动注意力机制。

Result: 在SARS - CoV - 2抗体 - 抗原数据集上达到了最先进性能，超越常用的基于序列的抗体无关表位预测方法，消融实验量化了各组件贡献。

Conclusion: 相比传统交叉注意力，滑动注意力显著提高了表位预测的精度。

Abstract: Accurate prediction of antibody-antigen (Ab-Ag) interfaces is critical for
vaccine design, immunodiagnostics, and therapeutic antibody development.
However, achieving reliable predictions from sequences alone remains a
challenge. In this paper, we present ABCONFORMER, a model based on the
Conformer backbone that captures both local and global features of a
biosequence. To accurately capture Ab-Ag interactions, we introduced the
physics-inspired sliding attention, enabling residue-level contact recovery
without relying on three-dimensional structural data. ABConformer can
accurately predict paratopes and epitopes given the antibody and antigen
sequence, and predict pan-epitopes on the antigen without antibody information.
In comparison experiments, ABCONFORMER achieves state-of-the-art performance on
a recent SARS-CoV-2 Ab-Ag dataset, and surpasses widely used sequence-based
methods for antibody-agnostic epitope prediction. Ablation studies further
quantify the contribution of each component, demonstrating that, compared to
conventional cross-attention, sliding attention significantly enhances the
precision of epitope prediction. To facilitate reproducibility, we will release
the code under an open-source license upon acceptance.

</details>


### [307] [Overlap-Adaptive Regularization for Conditional Average Treatment Effect Estimation](https://arxiv.org/abs/2509.24962)
*Valentyn Melnychuk,Dennis Frauen,Jonas Schweisthal,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 本文提出Overlap - Adaptive Regularization (OAR)方法，可提升低重叠区域条件平均治疗效果（CATE）估计的元学习器性能，实验表明其优于恒定正则化。


<details>
  <summary>Details</summary>
Motivation: 现有CATE估计的元学习器在低重叠情况下表现不佳，需新方法改善其在低重叠区域的性能。

Method: 引入OAR，根据重叠权重对目标模型进行正则化；提出去偏版本OAR以保持元学习器的Neyman正交性；且OAR可与任何现有CATE元学习器结合。

Result: 通过（半）合成实验，OAR在低重叠设置下显著改善了CATE估计，优于恒定正则化。

Conclusion: OAR是首个在元学习器正则化项中利用重叠权重的方法，能有效提升低重叠区域CATE估计性能。

Abstract: The conditional average treatment effect (CATE) is widely used in
personalized medicine to inform therapeutic decisions. However,
state-of-the-art methods for CATE estimation (so-called meta-learners) often
perform poorly in the presence of low overlap. In this work, we introduce a new
approach to tackle this issue and improve the performance of existing
meta-learners in the low-overlap regions. Specifically, we introduce
Overlap-Adaptive Regularization (OAR) that regularizes target models
proportionally to overlap weights so that, informally, the regularization is
higher in regions with low overlap. To the best of our knowledge, our OAR is
the first approach to leverage overlap weights in the regularization terms of
the meta-learners. Our OAR approach is flexible and works with any existing
CATE meta-learner: we demonstrate how OAR can be applied to both parametric and
non-parametric second-stage models. Furthermore, we propose debiased versions
of our OAR that preserve the Neyman-orthogonality of existing meta-learners and
thus ensure more robust inference. Through a series of (semi-)synthetic
experiments, we demonstrate that our OAR significantly improves CATE estimation
in low-overlap settings in comparison to constant regularization.

</details>


### [308] [CREPE: Controlling Diffusion with Replica Exchange](https://arxiv.org/abs/2509.23265)
*Jiajun He,Paul Jeha,Peter Potaptchik,Leo Zhang,José Miguel Hernández-Lobato,Yuanqi Du,Saifuddin Syed,Francisco Vargas*

Main category: cs.LG

TL;DR: 提出基于副本交换的CREPE方法用于扩散模型推理时控制，性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 以往扩散模型推理时控制方法多依赖启发式引导或结合SMC进行偏差校正，需更灵活方法。

Method: 提出CREPE方法，基于副本交换算法，与SMC不同，顺序生成粒子、保持样本多样性、支持在线细化或提前终止。

Result: 在温度退火、奖励倾斜、模型组合和无分类器引导去偏等任务中展示了通用性，性能与先前SMC方法相当。

Conclusion: CREPE方法是扩散模型推理时控制的灵活替代方案。

Abstract: Inference-time control of diffusion models aims to steer model outputs to
satisfy new constraints without retraining. Previous approaches have mostly
relied on heuristic guidance or have been coupled with Sequential Monte Carlo
(SMC) for bias correction. In this paper, we propose a flexible alternative
based on replica exchange, an algorithm designed initially for sampling
problems. We refer to this method as the CREPE (Controlling with REPlica
Exchange). Unlike SMC, CREPE: (1) generates particles sequentially, (2)
maintains high diversity in the generated samples after a burn-in period, and
(3) enables online refinement or early termination. We demonstrate its
versatility across various tasks, including temperature annealing,
reward-tilting, model composition and classifier-free guidance debiasing, with
competitive performance compared to prior SMC methods.

</details>


### [309] [A multiscale analysis of mean-field transformers in the moderate interaction regime](https://arxiv.org/abs/2509.25040)
*Giuseppe Bruno,Federico Pasqualotto,Andrea Agazzi*

Main category: cs.LG

TL;DR: 研究推理时仅编码器变压器模型中令牌的演化，在适度交互机制下分析系统多尺度行为并严格刻画各阶段动力学。


<details>
  <summary>Details</summary>
Motivation: 研究推理时仅编码器变压器模型中令牌通过模型深度的演化情况。

Method: 将令牌建模为以平均场方式相互作用的粒子系统，研究相应动力学，在适度交互机制下分析。

Result: 系统动力学呈现多尺度行为，包括快速、中间和慢速阶段，严格刻画各阶段极限动力学并证明收敛性，有模拟示例。

Conclusion: 成功对仅编码器变压器模型中令牌演化的系统动力学进行分析和刻画。

Abstract: In this paper, we study the evolution of tokens through the depth of
encoder-only transformer models at inference time by modeling them as a system
of particles interacting in a mean-field way and studying the corresponding
dynamics. More specifically, we consider this problem in the moderate
interaction regime, where the number $N$ of tokens is large and the inverse
temperature parameter $\beta$ of the model scales together with $N$. In this
regime, the dynamics of the system displays a multiscale behavior: a fast
phase, where the token empirical measure collapses on a low-dimensional space,
an intermediate phase, where the measure further collapses into clusters, and a
slow one, where such clusters sequentially merge into a single one. We provide
a rigorous characterization of the limiting dynamics in each of these phases
and prove convergence in the above mentioned limit, exemplifying our results
with some simulations.

</details>


### [310] [Transfer Learning and Machine Learning for Training Five Year Survival Prognostic Models in Early Breast Cancer](https://arxiv.org/abs/2509.23268)
*Lisa Pilgram,Kai Yang,Ana-Alicia Beltran-Bless,Gregory R. Pond,Lisa Vandermeer,John Hilton,Marie-France Savard,Andréanne Leblanc,Lois Sheperd,Bingshu E. Chen,John M. S. Bartlett,Karen J. Taylor,Jane Bayani,Sarah L. Barker,Melanie Spears,Cornelis J. H. van der Velde,Elma Meershoek-Klein Kranenbarg,Luc Dirix,Elizabeth Mallon,Annette Hasenburg,Christos Markopoulos,Lamin Juwara,Fida K. Dankar,Mark Clemons,Khaled El Emam*

Main category: cs.LG

TL;DR: 研究对比机器学习、迁移学习和集成整合方法，提升乳腺癌生存预后预测，在信息缺失或数据集偏移时有优势。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺癌预后主要关注基因组工具，而临床病理预后成本低且易获取，探索机器学习等方法提升预后预测能力。

Method: 使用MA.27试验数据训练模型，在TEAM试验和SEER队列验证。采用迁移学习微调PREDICT v3，从头开始的机器学习包括随机生存森林和极端梯度提升，集成整合通过模型预测加权和实现。

Result: 迁移学习、从头开始的随机生存森林和集成整合在MA.27中改善校准，区分度相当；ML模型和集成整合不受信息缺失影响；患者年龄等因素对预后预测重要；SEER外部验证确认方法优势。

Conclusion: 迁移学习、从头开始的随机生存森林和集成整合在PREDICT v3信息缺乏或数据集偏移时可改善预后预测。

Abstract: Prognostic information is essential for decision-making in breast cancer
management. Recently trials have predominantly focused on genomic
prognostication tools, even though clinicopathological prognostication is less
costly and more widely accessible. Machine learning (ML), transfer learning and
ensemble integration offer opportunities to build robust prognostication
frameworks. We evaluate this potential to improve survival prognostication in
breast cancer by comparing de-novo ML, transfer learning from a pre-trained
prognostic tool and ensemble integration. Data from the MA.27 trial was used
for model training, with external validation on the TEAM trial and a SEER
cohort. Transfer learning was applied by fine-tuning the pre-trained prognostic
tool PREDICT v3, de-novo ML included Random Survival Forests and Extreme
Gradient Boosting, and ensemble integration was realized through a weighted sum
of model predictions. Transfer learning, de-novo RSF, and ensemble integration
improved calibration in MA.27 over the pre-trained model (ICI reduced from
0.042 in PREDICT v3 to <=0.007) while discrimination remained comparable (AUC
increased from 0.738 in PREDICT v3 to 0.744-0.799). Invalid PREDICT v3
predictions were observed in 23.8-25.8% of MA.27 individuals due to missing
information. In contrast, ML models and ensemble integration could predict
survival regardless of missing information. Across all models, patient age,
nodal status, pathological grading and tumor size had the highest SHAP values,
indicating their importance for survival prognostication. External validation
in SEER, but not in TEAM, confirmed the benefits of transfer learning, RSF and
ensemble integration. This study demonstrates that transfer learning, de-novo
RSF, and ensemble integration can improve prognostication in situations where
relevant information for PREDICT v3 is lacking or where a dataset shift is
likely.

</details>


### [311] [Learning in an Echo Chamber: Online Learning with Replay Adversary](https://arxiv.org/abs/2509.25135)
*Daniil Dmitriev,Harald Eskelund Franck,Carolin Heinzler,Amartya Sanyal*

Main category: cs.LG

TL;DR: 本文提出在线学习重放设置框架，引入扩展阈值维度，给出学习能力上下界，证明重放设置更难及恰当学习的分离性。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统使用自标注数据有强化错误和形成信念回音室的风险，需研究应对方法。

Method: 引入在线学习重放设置框架，定义扩展阈值维度，分析不同类型对手下的学习情况。

Result: 得出扩展阈值维度是该模型学习能力的精确度量，证明重放设置比经典错误界限设置更难，恰当学习有分离性。

Conclusion: 基于闭包型算法对重放对手学习进行了首次严格分析。

Abstract: As machine learning systems increasingly train on self-annotated data, they
risk reinforcing errors and becoming echo chambers of their own beliefs. We
model this phenomenon by introducing a learning-theoretic framework: Online
Learning in the Replay Setting. In round $t$, the learner outputs a hypothesis
$\hat{h}_t$; the adversary then reveals either the true label $f^\ast(x_t)$ or
a replayed label $\hat{h}_i(x_t)$ from an earlier round $i < t$. A mistake is
counted only when the true label is shown, yet classical algorithms such as the
SOA or the halving algorithm are easily misled by the replayed errors.
  We introduce the Extended Threshold dimension, $\mathrm{ExThD}(\mathcal{H})$,
and prove matching upper and lower bounds that make
$\mathrm{ExThD}(\mathcal{H})$ the exact measure of learnability in this model.
A closure-based learner makes at most $\mathrm{ExThD}(\mathcal{H})$ mistakes
against any adaptive adversary, and no algorithm can perform better. For
stochastic adversaries, we prove a similar bound for every intersection-closed
class. The replay setting is provably harder than the classical mistake bound
setting: some classes have constant Littlestone dimension but arbitrarily large
$\mathrm{ExThD}(\mathcal{H})$. Proper learning exhibits an even sharper
separation: a class is properly learnable under replay if and only if it is
(almost) intersection-closed. Otherwise, every proper learner suffers
$\Omega(T)$ errors, whereas our improper algorithm still achieves the
$\mathrm{ExThD}(\mathcal{H})$ bound. These results give the first tight
analysis of learning against replay adversaries, based on new results for
closure-type algorithms.

</details>


### [312] [Continuous-Time Reinforcement Learning for Asset-Liability Management](https://arxiv.org/abs/2509.23280)
*Yilie Huang*

Main category: cs.LG

TL;DR: 本文提出基于连续时间强化学习的资产负债管理新方法，在随机市场场景测试中表现优于其他策略。


<details>
  <summary>Details</summary>
Motivation: 提出新的资产负债管理方法，实现资产与负债动态同步。

Method: 采用无模型、基于策略梯度的软演员 - 评论家算法，引入演员自适应探索和评论家调度探索。

Result: 在200个随机市场场景评估中，该方法平均奖励高于其他策略，初期收益快且性能持续优越。

Conclusion: 该方法优势在于直接学习最优策略，而非依赖复杂神经网络或改进参数估计。

Abstract: This paper proposes a novel approach for Asset-Liability Management (ALM) by
employing continuous-time Reinforcement Learning (RL) with a linear-quadratic
(LQ) formulation that incorporates both interim and terminal objectives. We
develop a model-free, policy gradient-based soft actor-critic algorithm
tailored to ALM for dynamically synchronizing assets and liabilities. To ensure
an effective balance between exploration and exploitation with minimal tuning,
we introduce adaptive exploration for the actor and scheduled exploration for
the critic. Our empirical study evaluates this approach against two enhanced
traditional financial strategies, a model-based continuous-time RL method, and
three state-of-the-art RL algorithms. Evaluated across 200 randomized market
scenarios, our method achieves higher average rewards than all alternative
strategies, with rapid initial gains and sustained superior performance. The
outperformance stems not from complex neural networks or improved parameter
estimation, but from directly learning the optimal ALM strategy without
learning the environment.

</details>


### [313] [High-Dimensional Analysis of Single-Layer Attention for Sparse-Token Classification](https://arxiv.org/abs/2509.25153)
*Nicholas Barnfield,Hugo Cui,Yue M. Lu*

Main category: cs.LG

TL;DR: 理论研究注意力机制在稀疏令牌分类模型中选择性关注信息令牌的时机和方式，对比单层注意力分类器和线性分类器，证明两次梯度更新使查询权重向量与隐藏信号对齐，推导测试误差等表达式并解释自适应令牌选择优势。


<details>
  <summary>Details</summary>
Motivation: 探讨注意力机制何时以及如何学会选择性地关注信息令牌，以检测弱、稀有和稀疏定位的特征。

Method: 在稀疏令牌分类模型中进行理论分析，对比单层注意力分类器和线性分类器，研究有限序列长度和高维状态下的训练，进行梯度更新实验并推导相关表达式。

Result: 在长序列极限下，单层注意力分类器信号强度对数增长时可实现测试误差趋近于零，而线性分类器需线性增长；两次梯度更新使查询权重向量与隐藏信号对齐；推导了测试误差和训练损失的精确渐近表达式，量化了其容量。

Conclusion: 解释了自适应令牌选择相对于非自适应线性基线的优势。

Abstract: When and how can an attention mechanism learn to selectively attend to
informative tokens, thereby enabling detection of weak, rare, and sparsely
located features? We address these questions theoretically in a sparse-token
classification model in which positive samples embed a weak signal vector in a
randomly chosen subset of tokens, whereas negative samples are pure noise. In
the long-sequence limit, we show that a simple single-layer attention
classifier can in principle achieve vanishing test error when the signal
strength grows only logarithmically in the sequence length $L$, whereas linear
classifiers require $\sqrt{L}$ scaling. Moving from representational power to
learnability, we study training at finite $L$ in a high-dimensional regime,
where sample size and embedding dimension grow proportionally. We prove that
just two gradient updates suffice for the query weight vector of the attention
classifier to acquire a nontrivial alignment with the hidden signal, inducing
an attention map that selectively amplifies informative tokens. We further
derive an exact asymptotic expression for the test error and training loss of
the trained attention-based classifier, and quantify its capacity -- the
largest dataset size that is typically perfectly separable -- thereby
explaining the advantage of adaptive token selection over nonadaptive linear
baselines.

</details>


### [314] [A Neural ODE Approach to Aircraft Flight Dynamics Modelling](https://arxiv.org/abs/2509.23307)
*Gabriel Jarry,Ramon Dalmau,Xavier Olive,Philippe Very*

Main category: cs.LG

TL;DR: 本文提出NODE - FDM模型，结合解析运动关系与数据驱动组件，在飞机轨迹预测上比现有模型更准确，尤其在下降阶段，虽有局限但展示了物理信息神经常微分方程潜力，未来将扩展到飞机横向动力学建模。


<details>
  <summary>Details</summary>
Motivation: 准确的飞机轨迹预测对空中交通管理、航空公司运营和环境评估至关重要，需更准确的飞机性能建模方法。

Method: 引入基于神经常微分方程的飞行动力学模型NODE - FDM，结合解析运动关系与数据驱动组件，并在快速访问记录器（QAR）数据上训练。

Result: NODE - FDM比现有模型（如基于BADA的轨迹生成方法）能更准确地重现记录轨迹，在高度、速度和质量动力学方面有显著改进。

Conclusion: 物理信息神经常微分方程作为一种高保真、数据驱动的飞机性能建模方法具有潜力，未来将扩展框架以对飞机横向动力学进行全面建模。

Abstract: Accurate aircraft trajectory prediction is critical for air traffic
management, airline operations, and environmental assessment. This paper
introduces NODE-FDM, a Neural Ordinary Differential Equations-based Flight
Dynamics Model trained on Quick Access Recorder (QAR) data. By combining
analytical kinematic relations with data-driven components, NODE-FDM achieves a
more accurate reproduction of recorded trajectories than state-of-the-art
models such as a BADA-based trajectory generation methodology (BADA4
performance model combined with trajectory control routines), particularly in
the descent phase of the flight. The analysis demonstrates marked improvements
across altitude, speed, and mass dynamics. Despite current limitations,
including limited physical constraints and the limited availability of QAR
data, the results demonstrate the potential of physics-informed neural ordinary
differential equations as a high-fidelity, data-driven approach to aircraft
performance modelling. Future work will extend the framework to incorporate a
full modelling of the lateral dynamics of the aircraft.

</details>


### [315] [GLASS Flows: Transition Sampling for Alignment of Flow and Diffusion Models](https://arxiv.org/abs/2509.25170)
*Peter Holderrieth,Uriel Singer,Tommi Jaakkola,Ricky T. Q. Chen,Yaron Lipman,Brian Karrer*

Main category: cs.LG

TL;DR: 引入GLASS Flows采样范式，结合ODE效率与SDE随机演化，消除文本到图像模型随机演化和效率的权衡，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有奖励对齐算法在提升流匹配和扩散模型推理性能时效率受限，瓶颈在于依赖的采样方法。

Method: 引入GLASS Flows采样范式，模拟“流匹配模型中的流匹配模型”来采样马尔可夫转移，且可从预训练模型中获取“内部”流匹配模型而无需重新训练。

Result: 在大规模文本到图像模型中，GLASS Flows消除了随机演化和效率的权衡；结合Feynman - Kac Steering，提升了文本到图像生成的性能。

Conclusion: GLASS Flows是一种简单、可直接应用的解决方案，可用于流和扩散模型推理时的扩展。

Abstract: The performance of flow matching and diffusion models can be greatly improved
at inference time using reward alignment algorithms, yet efficiency remains a
major limitation. While several algorithms were proposed, we demonstrate that a
common bottleneck is the sampling method these algorithms rely on: many
algorithms require to sample Markov transitions via SDE sampling, which is
significantly less efficient and often less performant than ODE sampling. To
remove this bottleneck, we introduce GLASS Flows, a new sampling paradigm that
simulates a "flow matching model within a flow matching model" to sample Markov
transitions. As we show in this work, this "inner" flow matching model can be
retrieved from a pre-trained model without any re-training, combining the
efficiency of ODEs with the stochastic evolution of SDEs. On large-scale
text-to-image models, we show that GLASS Flows eliminate the trade-off between
stochastic evolution and efficiency. Combined with Feynman-Kac Steering, GLASS
Flows improve state-of-the-art performance in text-to-image generation, making
it a simple, drop-in solution for inference-time scaling of flow and diffusion
models.

</details>


### [316] [ASTGI: Adaptive Spatio-Temporal Graph Interactions for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.23313)
*Xvyuan Liu,Xiangfei Qiu,Hanyin Cheng,Xingjian Wu,Chenjuan Guo,Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: 提出ASTGI框架解决不规则多元时间序列预测问题，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 不规则多元时间序列在医疗和金融等领域普遍存在，现有方法在处理异步采样和不规则间隔时面临准确表示信息和捕捉动态依赖的挑战。

Method: 提出ASTGI框架，包含时空点表示、邻域自适应图构建、时空动态传播和查询点预测四个模块。

Result: 在多个基准数据集上的实验表明，ASTGI优于各种先进方法。

Conclusion: ASTGI框架能有效解决不规则多元时间序列预测问题，具有更好的性能。

Abstract: Irregular multivariate time series (IMTS) are prevalent in critical domains
like healthcare and finance, where accurate forecasting is vital for proactive
decision-making. However, the asynchronous sampling and irregular intervals
inherent to IMTS pose two core challenges for existing methods: (1) how to
accurately represent the raw information of irregular time series without
introducing data distortion, and (2) how to effectively capture the complex
dynamic dependencies between observation points. To address these challenges,
we propose the Adaptive Spatio-Temporal Graph Interaction (ASTGI) framework.
Specifically, the framework first employs a Spatio-Temporal Point
Representation module to encode each discrete observation as a point within a
learnable spatio-temporal embedding space. Second, a Neighborhood-Adaptive
Graph Construction module adaptively builds a causal graph for each point in
the embedding space via nearest neighbor search. Subsequently, a
Spatio-Temporal Dynamic Propagation module iteratively updates information on
these adaptive causal graphs by generating messages and computing interaction
weights based on the relative spatio-temporal positions between points.
Finally, a Query Point-based Prediction module generates the final forecast by
aggregating neighborhood information for a new query point and performing
regression. Extensive experiments on multiple benchmark datasets demonstrate
that ASTGI outperforms various state-of-the-art methods.

</details>


### [317] [Two-Scale Latent Dynamics for Recurrent-Depth Transformers](https://arxiv.org/abs/2509.23314)
*Francesco Pappone,Donato Crisostomi,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 研究循环深度变压器迭代的几何特性，提出两尺度操作图景，并基于步长二阶差分提出早期退出机制，性能更优。


<details>
  <summary>Details</summary>
Motivation: 探索循环深度变压器迭代的几何特性，以改进其测试时计算性能和效率。

Method: 研究迭代的几何特性，提出两尺度操作图景，基于步长二阶差分设计早期退出机制。

Result: 测量显示循环步骤变小且相互正交，新的早期退出机制在性能、稳定性和时间效率上优于其他策略。

Conclusion: 基于步长二阶差分的早期退出机制在循环深度变压器中表现更优。

Abstract: Recurrent-depth transformers scale test-time compute by iterating latent
computations before emitting tokens. We study the geometry of these iterates
and argue for a simple, \emph{two-scale} operational picture: (i) within a
looped block, updates act as \emph{small-scale refinements}; (ii) across
consecutive blocks, states undergo a \emph{larger-scale drift}. Across
checkpoints, our measurements show that loop steps become \emph{smaller} and
increasingly \emph{orthogonal} to one another, indicating better local modeling
of fine structure rather than merely pushing in a single direction. These
dynamics motivate an early-exit mechanism based on the model's second-order
difference in step-size, which we show is superior in terms of performance,
stability and time-efficiency, when compared to the KL-divergence exit strategy
of Geiping et al. and its naive first-order counterpart.

</details>


### [318] [MELCOT: A Hybrid Learning Architecture with Marginal Preservation for Matrix-Valued Regression](https://arxiv.org/abs/2509.23315)
*Khang Tran,Hieu Cao,Thinh Pham,Nghiem Diep,Tri Cao,Binh Nguyen*

Main category: cs.LG

TL;DR: 提出混合模型MELCOT解决矩阵值回归问题，实验显示其优于基线且高效。


<details>
  <summary>Details</summary>
Motivation: 回归在高维场景有挑战，现有方法常丢失空间结构或需大量存储，要解决矩阵值回归问题。

Method: 提出MELCOT，集成基于经典机器学习的边际估计（ME）块和基于深度学习的可学习成本最优传输（LCOT）块。

Result: 在不同数据集和领域的大量实验中，MELCOT始终优于所有基线。

Conclusion: MELCOT继承了经典和深度学习方法的优势，高效且表现出色。

Abstract: Regression is essential across many domains but remains challenging in
high-dimensional settings, where existing methods often lose spatial structure
or demand heavy storage. In this work, we address the problem of matrix-valued
regression, where each sample is naturally represented as a matrix. We propose
MELCOT, a hybrid model that integrates a classical machine learning-based
Marginal Estimation (ME) block with a deep learning-based Learnable-Cost
Optimal Transport (LCOT) block. The ME block estimates data marginals to
preserve spatial information, while the LCOT block learns complex global
features. This design enables MELCOT to inherit the strengths of both classical
and deep learning methods. Extensive experiments across diverse datasets and
domains demonstrate that MELCOT consistently outperforms all baselines while
remaining highly efficient.

</details>


### [319] [LLM Interpretability with Identifiable Temporal-Instantaneous Representation](https://arxiv.org/abs/2509.23323)
*Xiangchen Song,Jiaqi Sun,Zijian Li,Yujia Zheng,Kun Zhang*

Main category: cs.LG

TL;DR: 本文提出针对大语言模型高维概念空间的可识别时间因果表示学习框架，结合SAE技术发现有意义概念关系，提升了大语言模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有机制可解释性工具（如SAEs）缺乏时间依赖建模等且无理论保证，因果表示学习方法无法扩展到LLMs丰富概念空间。

Method: 引入针对LLMs高维概念空间的可识别时间因果表示学习框架，结合SAE技术。

Result: 该方法在合成数据集上有效，成功在LLM激活中发现有意义概念关系。

Conclusion: 对时间和即时概念关系建模可提升LLMs的可解释性。

Abstract: Despite Large Language Models' remarkable capabilities, understanding their
internal representations remains challenging. Mechanistic interpretability
tools such as sparse autoencoders (SAEs) were developed to extract
interpretable features from LLMs but lack temporal dependency modeling,
instantaneous relation representation, and more importantly theoretical
guarantees, undermining both the theoretical foundations and the practical
confidence necessary for subsequent analyses. While causal representation
learning (CRL) offers theoretically grounded approaches for uncovering latent
concepts, existing methods cannot scale to LLMs' rich conceptual space due to
inefficient computation. To bridge the gap, we introduce an identifiable
temporal causal representation learning framework specifically designed for
LLMs' high-dimensional concept space, capturing both time-delayed and
instantaneous causal relations. Our approach provides theoretical guarantees
and demonstrates efficacy on synthetic datasets scaled to match real-world
complexity. By extending SAE techniques with our temporal causal framework, we
successfully discover meaningful concept relationships in LLM activations. Our
findings show that modeling both temporal and instantaneous conceptual
relationships advances the interpretability of LLMs.

</details>


### [320] [Robust Fine-Tuning from Non-Robust Pretrained Models: Mitigating Suboptimal Transfer With Adversarial Scheduling](https://arxiv.org/abs/2509.23325)
*Jonas Ngnawé,Maxime Heuillet,Sabyasachi Sahoo,Yann Pequignot,Ola Ahmad,Audrey Durand,Frédéric Precioso,Christian Gagné*

Main category: cs.LG

TL;DR: 研究非鲁棒预训练模型的鲁棒微调，发现次优转移现象，提出Epsilon - Scheduling启发式方法和预期鲁棒性指标，实验表明前者可防止次优转移并提升预期鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有非鲁棒预训练模型用于鲁棒微调（RFT）的潜力未被充分理解，需填补这一知识空白。

Method: 系统地研究非鲁棒模型的RFT，提出Epsilon - Scheduling启发式方法和预期鲁棒性指标。

Result: 发现用鲁棒目标微调非鲁棒模型会出现次优转移现象，Epsilon - Scheduling能防止次优转移并提升预期鲁棒性。

Conclusion: Epsilon - Scheduling可有效促进最优转移，预期鲁棒性指标能更全面评估模型在测试时的准确率 - 鲁棒性权衡。

Abstract: Fine-tuning pretrained models is a standard and effective workflow in modern
machine learning. However, robust fine-tuning (RFT), which aims to
simultaneously achieve adaptation to a downstream task and robustness to
adversarial examples, remains challenging. Despite the abundance of non-robust
pretrained models in open-source repositories, their potential for RFT is less
understood. We address this knowledge gap by systematically examining RFT from
such non-robust models. Our experiments reveal that fine-tuning non-robust
models with a robust objective, even under small perturbations, can lead to
poor performance, a phenomenon that we dub \emph{suboptimal transfer}. In
challenging scenarios (eg, difficult tasks, high perturbation), the resulting
performance can be so low that it may be considered a transfer failure. We find
that fine-tuning using a robust objective impedes task adaptation at the
beginning of training and eventually prevents optimal transfer. However, we
propose a novel heuristic, \emph{Epsilon-Scheduling}, a schedule over
perturbation strength used during training that promotes optimal transfer.
Additionally, we introduce \emph{expected robustness}, a metric that captures
performance across a range of perturbations, providing a more comprehensive
evaluation of the accuracy-robustness trade-off for diverse models at test
time. Extensive experiments on a wide range of configurations (six pretrained
models and five datasets) show that \emph{Epsilon-Scheduling} successfully
prevents \emph{suboptimal transfer} and consistently improves expected
robustness.

</details>


### [321] [Entering the Era of Discrete Diffusion Models: A Benchmark for Schrödinger Bridges and Entropic Optimal Transport](https://arxiv.org/abs/2509.23348)
*Xavier Aramayo Carrasco,Grigoriy Ksenofontov,Aleksei Leonov,Iaroslav Sergeevich Koshelev,Alexander Korotin*

Main category: cs.LG

TL;DR: 本文为离散空间上的Schrödinger桥（SB）问题引入基准测试，得到新算法并评估求解器，为SB方法评估奠定基础。


<details>
  <summary>Details</summary>
Motivation: 离散扩散和流模型使SB方法在离散域的应用受关注，但缺乏可靠评估方法。

Method: 构建离散空间SB基准测试，得到DLightSB和DLightSB - M算法，扩展相关工作得到α - CSBM算法。

Result: 可在高维离散环境下评估现有和新的求解器。

Conclusion: 为离散空间SB方法的正确评估迈出第一步，为未来研究提供可重复性。

Abstract: The Entropic Optimal Transport (EOT) problem and its dynamic counterpart, the
Schr\"odinger bridge (SB) problem, play an important role in modern machine
learning, linking generative modeling with optimal transport theory. While
recent advances in discrete diffusion and flow models have sparked growing
interest in applying SB methods to discrete domains, there is still no reliable
way to evaluate how well these methods actually solve the underlying problem.
We address this challenge by introducing a benchmark for SB on discrete spaces.
Our construction yields pairs of probability distributions with analytically
known SB solutions, enabling rigorous evaluation. As a byproduct of building
this benchmark, we obtain two new SB algorithms, DLightSB and DLightSB-M, and
additionally extend prior related work to construct the $\alpha$-CSBM
algorithm. We demonstrate the utility of our benchmark by evaluating both
existing and new solvers in high-dimensional discrete settings. This work
provides the first step toward proper evaluation of SB methods on discrete
spaces, paving the way for more reproducible future studies.

</details>


### [322] [Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought](https://arxiv.org/abs/2509.23365)
*Hanlin Zhu,Shibo Hao,Zhiting Hu,Jiantao Jiao,Stuart Russell,Yuandong Tian*

Main category: cs.LG

TL;DR: 本文理论分析简化两层transformer在有向图可达性问题上的训练动态，揭示叠加机制如何出现，实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 此前工作未明确基于梯度的训练方法如何自然学习叠加机制，本文旨在填补该空白。

Method: 理论分析简化两层transformer在有向图可达性问题上训练动态，分思想生成和预测两个阶段。

Result: 训练中索引匹配对数先增加后有界，能平衡推理过程中的探索与利用，产生叠加。实验跟踪对数增长验证理论。

Conclusion: 理论分析揭示了基于梯度训练方法中叠加机制的出现过程，实验结果验证了理论。

Abstract: Previous work shows that the chain of continuous thought (continuous CoT)
improves the reasoning capability of large language models (LLMs) by enabling
implicit parallel thinking, and a subsequent work provided theoretical insight
by showing that a two-layer transformer equipped with continuous CoT can
efficiently solve directed graph reachability by maintaining a superposition of
multiple reasoning traces in the continuous thought. However, it remains
unclear how the superposition mechanism is naturally learned from
gradient-based training methods. To fill this gap, we theoretically analyze the
training dynamics of a simplified two-layer transformer on the directed graph
reachability problem to unveil how the superposition mechanism emerges during
training in two training stages -- (i) a thought-generation stage that
autoregressively expands the continuous thought, and (ii) a prediction stage
that converts the thought into the final answer. Our analysis reveals that
during training using continuous thought, the index-matching logit, an
important quantity which reflects the strength of the model's local search
ability, will first increase and then remain bounded under mild assumptions.
The bounded index-matching logit effectively balances exploration and
exploitation during the reasoning process: the model will exploit local problem
structures to identify plausible search traces, and assign comparable weights
to multiple such traces to explore when it is uncertain about which solution is
correct, which results in superposition. Our experimental results tracking the
growth of logits further validate our theory.

</details>


### [323] [Splines-Based Feature Importance in Kolmogorov-Arnold Networks: A Framework for Supervised Tabular Data Dimensionality Reduction](https://arxiv.org/abs/2509.23366)
*Ange-Clément Akazan,Verlon Roel Mbingui*

Main category: cs.LG

TL;DR: 本文提出基于Kolmogorov - Arnold网络（KANs）的表格数据集特征选择方法，通过多基准测试与经典方法对比，表明KAN基选择器有竞争力，是传统方法的有力替代。


<details>
  <summary>Details</summary>
Motivation: 高维数据集需要有效的特征选择以提升预测性能、可解释性和鲁棒性。

Method: 提出四种基于KAN的特征选择器（$	extit{KAN - L1}$、$	extit{KAN - L2}$、$	extit{KAN - SI}$、$	extit{KAN - KO}$），并与经典方法（LASSO、随机森林等）在多个分类和回归表格数据集基准上对比。

Result: KAN基选择器有竞争力，部分表现优于经典方法，但也有不足，如$	extit{KAN - L1}$在回归中激进，$	extit{KAN - L2}$在分类中表现不佳。

Conclusion: KAN基特征选择是传统方法的有力可解释替代，能发现非线性和多变量特征相关性。

Abstract: High-dimensional datasets require effective feature selection to improve
predictive performance, interpretability, and robustness. We propose and
evaluate feature selection methods for tabular datasets based on
Kolmogorov-Arnold networks (KANs), which parameterize feature transformations
through splines, enabling direct access to interpretable importance measures.
We introduce four KAN-based selectors ($\textit{KAN-L1}$, $\textit{KAN-L2}$,
$\textit{KAN-SI}$, $\textit{KAN-KO}$) and compare them against classical
baselines (LASSO, Random Forest, Mutual Information, SVM-RFE) across multiple
classification and regression tabular dataset benchmarks. Average (over three
retention levels: 20\%, 40\%, and 60\%) F1 scores and $R^2$ score results
reveal that KAN-based selectors, particularly $\textit{KAN-L2}$,
$\textit{KAN-L1}$, $\textit{KAN-SI}$, and $\textit{KAN-KO}$, are competitive
with and sometimes superior to classical baselines in structured and synthetic
datasets. However, $\textit{KAN-L1}$ is often too aggressive in regression,
removing useful features, while $\textit{KAN-L2}$ underperforms in
classification, where simple coefficient shrinkage misses complex feature
interactions. $\textit{KAN-L2}$ and $\textit{KAN-SI}$ provide robust
performance on noisy regression datasets and heterogeneous datasets, aligning
closely with ensemble predictors. In classification tasks, KAN selectors such
as $\textit{KAN-L1}$, $\textit{KAN-KO}$, and $\textit{KAN-SI}$ sometimes
surpass the other selectors by eliminating redundancy, particularly in
high-dimensional multi-class data. Overall, our findings demonstrate that
KAN-based feature selection provides a powerful and interpretable alternative
to traditional methods, capable of uncovering nonlinear and multivariate
feature relevance beyond sparsity or impurity-based measures.

</details>


### [324] [Graph Your Own Prompt](https://arxiv.org/abs/2509.23373)
*Xi Ding,Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.LG

TL;DR: 提出图一致性正则化（GCR）框架，通过注入关系图结构促进特征表示学习，实验表明其能提升特征质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度网络学习的表示存在噪声的类间相似性，与预测语义矛盾，需要解决该问题。

Method: 引入无参数的图一致性层（GCL），构建批级特征相似性图并与全局类感知掩码预测图对齐，采用多层、跨空间图对齐机制和自适应加权。

Result: GCR促进了更清晰的特征结构、更强的类内凝聚性和更好的泛化能力。

Conclusion: GCR是一种模型无关、轻量级的方法，为从预测结构学习提供了新视角。

Abstract: We propose Graph Consistency Regularization (GCR), a novel framework that
injects relational graph structures, derived from model predictions, into the
learning process to promote class-aware, semantically meaningful feature
representations. Functioning as a form of self-prompting, GCR enables the model
to refine its internal structure using its own outputs. While deep networks
learn rich representations, these often capture noisy inter-class similarities
that contradict the model's predicted semantics. GCR addresses this issue by
introducing parameter-free Graph Consistency Layers (GCLs) at arbitrary depths.
Each GCL builds a batch-level feature similarity graph and aligns it with a
global, class-aware masked prediction graph, derived by modulating softmax
prediction similarities with intra-class indicators. This alignment enforces
that feature-level relationships reflect class-consistent prediction behavior,
acting as a semantic regularizer throughout the network. Unlike prior work, GCR
introduces a multi-layer, cross-space graph alignment mechanism with adaptive
weighting, where layer importance is learned from graph discrepancy magnitudes.
This allows the model to prioritize semantically reliable layers and suppress
noisy ones, enhancing feature quality without modifying the architecture or
training procedure. GCR is model-agnostic, lightweight, and improves semantic
structure across various networks and datasets. Experiments show that GCR
promotes cleaner feature structure, stronger intra-class cohesion, and improved
generalization, offering a new perspective on learning from prediction
structure. [Project website](https://darcyddx.github.io/gcr/)
[Code](https://github.com/Darcyddx/graph-prompt)

</details>


### [325] [Planner Aware Path Learning in Diffusion Language Models Training](https://arxiv.org/abs/2509.23405)
*Fred Zhangzhi Peng,Zachary Bezemek,Jarrid Rector-Brooks,Shuibai Zhang,Anru R. Zhang,Michael Bronstein,Avishek Joey Bose,Alexander Tong*

Main category: cs.LG

TL;DR: 本文研究扩散语言模型训练与推理路径不匹配问题，推导新的P - ELBO，提出PAPL方法，在多领域提升效果。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型的规划器在推理时引入的路径与训练时不同，标准离散扩散训练ELBO无法准确描述非均匀规划下的去噪器。

Method: 推导新的Planned Evidence Lower Bound (P - ELBO)，提出Planner Aware Path Learning (PAPL)方法改进标准掩码离散扩散损失。

Result: PAPL在多个领域带来持续改进，如蛋白质序列建模提升40%，文本生成MAUVE提升4倍，代码生成HumanEval pass@10提升23%。

Conclusion: PAPL方法有效，能使基于规划的去噪器在训练和推理时保持一致，提升模型性能。

Abstract: Diffusion language models have emerged as a powerful alternative to
autoregressive models, enabling fast inference through flexible and parallel
generation paths. This flexibility is enabled by new sampling strategies, or
planners, that iteratively choose where to denoise along the sequence rather
than sampling uniformly at random. However, by modifying reverse paths,
planners introduce a mismatch between the uniformly random denoising paths used
during training and the planning-based paths used at inference. In this work,
we systematically investigate this mismatch and theoretically show that the
standard discrete diffusion training evidence lower bound (ELBO) does not
accurately describe a denoiser under non-uniform planning. To bridge this gap,
we derive a new Planned Evidence Lower Bound (P-ELBO) that directly
incorporates planner-based reverse dynamics into the training objective.
Building on this, we propose Planner Aware Path Learning (PAPL), a simple and
effective modification of the standard masked discrete diffusion loss that
aligns training and inference under planned denoisers. Empirically, PAPL
delivers consistent improvements across domains, including a 40% relative gain
in protein sequence modeling, up to a 4x improvement in MAUVE for text
generation, and a 23% relative gain in HumanEval pass@10 for code generation.

</details>


### [326] [Mind the Links: Cross-Layer Attention for Link Prediction in Multiplex Networks](https://arxiv.org/abs/2509.23409)
*Devesh Sharma,Aditya Kishore,Ayush Garg,Debajyoti Mazumder,Debasis Mohapatra,Jasabanta Patro*

Main category: cs.LG

TL;DR: 本文将多路链接预测建模为多视图边分类，提出两个模型，引入候选池和无泄漏协议，实验显示在多个数据集上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有多路图链接预测器存在丢失层间依赖和可扩展性差的问题。

Method: 将多路链接预测构建为多视图边分类，构建每层边视图，应用跨层自注意力融合证据；提出Trans - SLE和Trans - GAT两个模型；引入Union - Set候选池和两个无泄漏协议。

Result: 在六个公共多路数据集上实验，相比强基线（MELL、HOPLP - MUL、RMNE）有一致的宏观F_1提升。

Conclusion: 该方法简单、可扩展，与预计算嵌入和GNN编码器兼容。

Abstract: Multiplex graphs capture diverse relations among shared nodes. Most
predictors either collapse layers or treat them independently. This loses
crucial inter-layer dependencies and struggles with scalability. To overcome
this, we frame multiplex link prediction as multi-view edge classification. For
each node pair, we construct a sequence of per-layer edge views and apply
cross-layer self-attention to fuse evidence for the target layer. We present
two models as instances of this framework: Trans-SLE, a lightweight transformer
over static embeddings, and Trans-GAT, which combines layer-specific GAT
encoders with transformer fusion. To ensure scalability and fairness, we
introduce a Union--Set candidate pool and two leakage-free protocols:
cross-layer and inductive subgraph generalization. Experiments on six public
multiplex datasets show consistent macro-F_1 gains over strong baselines (MELL,
HOPLP-MUL, RMNE). Our approach is simple, scalable, and compatible with both
precomputed embeddings and GNN encoders.

</details>


### [327] [URS: A Unified Neural Routing Solver for Cross-Problem Zero-Shot Generalization](https://arxiv.org/abs/2509.23413)
*Changliang Zhou,Canhong Yu,Shunyu Yao,Xi Lin,Zhenkun Wang,Yu Zhou,Qingfu Zhang*

Main category: cs.LG

TL;DR: 提出统一神经路由求解器URS，可零样本泛化解决多种未见的车辆路径规划问题（VRP），实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有神经求解器依赖预定义问题约束或需针对每个问题微调，限制了零样本泛化能力。

Method: 提出统一数据表示（UDR）、混合偏差模块（MBM）、参数生成器和大语言模型驱动的约束满足机制。

Result: URS能在无微调情况下为超100种不同的VRP变体生成高质量解决方案，含超90种未见变体。

Conclusion: URS是首个能用单一模型处理超100种VRP变体的神经求解器。

Abstract: Multi-task neural routing solvers have emerged as a promising paradigm for
their ability to solve multiple vehicle routing problems (VRPs) using a single
model. However, existing neural solvers typically rely on predefined problem
constraints or require per-problem fine-tuning, which substantially limits
their zero-shot generalization ability to unseen VRP variants. To address this
critical bottleneck, we propose URS, a unified neural routing solver capable of
zero-shot generalization across a wide range of unseen VRPs using a single
model without any fine-tuning. The key component of URS is the unified data
representation (UDR), which replaces problem enumeration with data unification,
thereby broadening the problem coverage and reducing reliance on domain
expertise. In addition, we propose a Mixed Bias Module (MBM) to efficiently
learn the geometric and relational biases inherent in various problems. On top
of the proposed UDR, we further develop a parameter generator that adaptively
adjusts the decoder and bias weights of MBM to enhance zero-shot
generalization. Moreover, we propose an LLM-driven constraint satisfaction
mechanism, which translates raw problem descriptions into executable stepwise
masking functions to ensure solution feasibility. Extensive experiments
demonstrate that URS can consistently produce high-quality solutions for more
than 100 distinct VRP variants without any fine-tuning, which includes more
than 90 unseen variants. To the best of our knowledge, URS is the first neural
solver capable of handling over 100 VRP variants with a single model.

</details>


### [328] [LOTFormer: Doubly-Stochastic Linear Attention via Low-Rank Optimal Transport](https://arxiv.org/abs/2509.23436)
*Ashkan Shahbazi,Chayne Thrash,Yikun Bai,Keaton Hamm,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.LG

TL;DR: 提出LOTFormer，一种线性时间且双随机的注意力机制，在长序列基准测试中取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 标准softmax注意力机制的二次复杂度限制长序列扩展，多数注意力机制会过度关注少量token，现有双随机注意力机制开销大，影响可扩展性。

Method: 利用注意力图与查询和键测度之间运输计划的联系，通过可学习的小支撑枢轴测度约束运输计划为低秩，求解两个熵最优运输问题并组合成条件耦合。

Result: LOTFormer在Long Range Arena基准测试中超越先前的线性和基于运输的注意力方法，在准确性和效率上取得SOTA。

Conclusion: LOTFormer是一种有效的线性时间且双随机的注意力机制。

Abstract: Transformers have proven highly effective across a wide range of modalities.
However, the quadratic complexity of the standard softmax attention mechanism
poses a fundamental barrier to scaling them to long context windows. A large
body of work addresses this with linear attention, which reformulates attention
as a kernel function and approximates it with finite feature maps to achieve
linear-time computation. Orthogonal to computational scaling, most attention
mechanisms -- both quadratic and linear -- produce row-normalized maps that can
over-focus on a few tokens, degrading robustness and information flow.
Enforcing doubly-stochastic attention alleviates this by balancing token
participation across rows and columns, but existing doubly-stochastic attention
mechanisms typically introduce substantial overhead, undermining scalability.
We propose LOTFormer, a principled attention mechanism that is simultaneously
linear-time and doubly-stochastic. Our approach exploits the connection between
attention maps and transportation plans between query and key measures. The
central idea is to constrain the transport plan to be low-rank by conditioning
it on a learnable pivot measure with small support. Concretely, we solve two
entropic optimal transport problems (queries $\to$ pivot and pivot $\to$ keys)
and compose them into a conditional (glued) coupling. This yields an attention
matrix that is provably doubly-stochastic, has rank at most $r \ll n$, and
applies to values in $O(nr)$ time without forming the full $n \times n$ map.
The pivot locations and masses are learned end-to-end. Empirically, LOTFormer
achieves state-of-the-art results on the Long Range Arena benchmark, surpassing
prior linear and transport-based attention methods in both accuracy and
efficiency.

</details>


### [329] [Factor Decorrelation Enhanced Data Removal from Deep Predictive Models](https://arxiv.org/abs/2509.23443)
*Wenhao Yang,Lin Li,Xiaohui Tao,Kaize Shi*

Main category: cs.LG

TL;DR: 为解决模型训练中移除敏感数据导致分布偏移问题，提出通过因子去相关和损失扰动增强深度预测模型的方法，实验显示该方法表现出色。


<details>
  <summary>Details</summary>
Motivation: 用户隐私保护和监管合规要求移除敏感数据，但此过程会导致分布偏移，影响模型性能，尤其是在分布外场景。

Method: 提出新的数据移除方法，包括采用动态自适应权重调整和迭代表示更新的判别保留因子去相关模块，以及带有损失扰动的平滑数据移除机制。

Result: 在五个基准数据集上的大量实验表明，该方法优于其他基线方法，即使在显著分布偏移下也能持续实现高预测准确性和鲁棒性。

Conclusion: 该方法在分布内和分布外场景均具有卓越的效率和适应性。

Abstract: The imperative of user privacy protection and regulatory compliance
necessitates sensitive data removal in model training, yet this process often
induces distributional shifts that undermine model performance-particularly in
out-of-distribution (OOD) scenarios. We propose a novel data removal approach
that enhances deep predictive models through factor decorrelation and loss
perturbation. Our approach introduces: (1) a discriminative-preserving factor
decorrelation module employing dynamic adaptive weight adjustment and iterative
representation updating to reduce feature redundancy and minimize inter-feature
correlations. (2) a smoothed data removal mechanism with loss perturbation that
creates information-theoretic safeguards against data leakage during removal
operations. Extensive experiments on five benchmark datasets show that our
approach outperforms other baselines and consistently achieves high predictive
accuracy and robustness even under significant distribution shifts. The results
highlight its superior efficiency and adaptability in both in-distribution and
out-of-distribution scenarios.

</details>


### [330] [PHASE: Physics-Integrated, Heterogeneity-Aware Surrogates for Scientific Simulations](https://arxiv.org/abs/2509.23453)
*Dawei Gao,Dali Wang,Zhuowei Gu,Qinglei Cao,Xiao Wang,Peter Thornton,Dan Ricciuto,Yunhe Feng*

Main category: cs.LG

TL;DR: 介绍了名为PHASE的深度学习框架，能加速科学模拟，在E3SM的BGC自旋-up工作流中验证效果好，有效减少所需积分长度。


<details>
  <summary>Details</summary>
Motivation: 大规模数值模拟计算成本高，AI代理在关键任务应用受限，需解决物理合理性、可信度和异构数据融合问题。

Method: PHASE框架结合数据类型感知编码器处理异构输入，采用多级基于物理的约束促进从局部到全局的一致性。

Result: 在E3SM的ELM的BGC自旋-up工作流中验证，仅用前20年模拟数据推断接近平衡状态，有效减少积分长度至少60倍，能融合异构数据，泛化能力强。

Conclusion: PHASE捕获了物理规律而非表面相关性，可实现陆地表面建模和其他复杂科学工作流的实际、物理上一致的加速。

Abstract: Large-scale numerical simulations underpin modern scientific discovery but
remain constrained by prohibitive computational costs. AI surrogates offer
acceleration, yet adoption in mission-critical settings is limited by concerns
over physical plausibility, trustworthiness, and the fusion of heterogeneous
data. We introduce PHASE, a modular deep-learning framework for
physics-integrated, heterogeneity-aware surrogates in scientific simulations.
PHASE combines data-type-aware encoders for heterogeneous inputs with
multi-level physics-based constraints that promote consistency from local
dynamics to global system behavior. We validate PHASE on the biogeochemical
(BGC) spin-up workflow of the U.S. Department of Energy's Energy Exascale Earth
System Model (E3SM) Land Model (ELM), presenting-to our knowledge-the first
scientifically validated AI-accelerated solution for this task. Using only the
first 20 simulation years, PHASE infers a near-equilibrium state that otherwise
requires more than 1,200 years of integration, yielding an effective reduction
in required integration length by at least 60x. The framework is enabled by a
pipeline for fusing heterogeneous scientific data and demonstrates strong
generalization to higher spatial resolutions with minimal fine-tuning. These
results indicate that PHASE captures governing physical regularities rather
than surface correlations, enabling practical, physically consistent
acceleration of land-surface modeling and other complex scientific workflows.

</details>


### [331] [Generative Evolutionary Meta-Solver (GEMS): Scalable Surrogate-Free Multi-Agent Learning](https://arxiv.org/abs/2509.23462)
*Alakh Sharma,Gaurish Trivedi,Kartikey Bhandari,Yash Sinha,Dhruv Kumar,Pratik Narang,Jagat Sesh Challa*

Main category: cs.LG

TL;DR: 提出GEMS框架解决可扩展多智能体强化学习问题，比PSRO更快、内存使用更少且奖励更高。


<details>
  <summary>Details</summary>
Motivation: 现有基于种群的多智能体强化学习方法存在计算和内存成本高的问题，需要更高效的方法。

Method: 提出GEMS框架，用潜在锚点和生成器替代显式种群，通过蒙特卡罗滚动、元动力学和UCB预言自适应扩展策略集。

Result: GEMS比PSRO快约6倍，内存使用少1.3倍，同时获得更高奖励。

Conclusion: GEMS保留了PSRO的博弈论保证，克服了其效率问题，可实现多领域的可扩展多智能体学习。

Abstract: Scalable multi-agent reinforcement learning (MARL) remains a central
challenge for AI. Existing population-based methods, like Policy-Space Response
Oracles, PSRO, require storing explicit policy populations and constructing
full payoff matrices, incurring quadratic computation and linear memory costs.
We present Generative Evolutionary Meta-Solver (GEMS), a surrogate-free
framework that replaces explicit populations with a compact set of latent
anchors and a single amortized generator. Instead of exhaustively constructing
the payoff matrix, GEMS relies on unbiased Monte Carlo rollouts,
multiplicative-weights meta-dynamics, and a model-free empirical-Bernstein UCB
oracle to adaptively expand the policy set. Best responses are trained within
the generator using an advantage-based trust-region objective, eliminating the
need to store and train separate actors. We evaluated GEMS in a variety of
Two-player and Multi-Player games such as the Deceptive Messages Game, Kuhn
Poker and Multi-Particle environment. We find that GEMS is up to ~6x faster,
has 1.3x less memory usage than PSRO, while also reaps higher rewards
simultaneously. These results demonstrate that GEMS retains the game theoretic
guarantees of PSRO, while overcoming its fundamental inefficiencies, hence
enabling scalable multi-agent learning in multiple domains.

</details>


### [332] [Solve Smart, Not Often: Policy Learning for Costly MILP Re-solving](https://arxiv.org/abs/2509.23470)
*Rui Ai,Hugo De Oliveira Barbalho,Sirui Li,Alexei Robsky,David Simchi-Levi,Ishai Menache*

Main category: cs.LG

TL;DR: 提出POC框架解决实时操作中优化问题重解时机决策，平衡性能与成本，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 实时操作中决定是否重解优化问题是经济上重要问题，现有方法有局限，缺乏针对常见NP - hard MILPs的研究。

Method: 提出Proximal Policy Optimization with Change Point Detection (POC)框架，理论上建立重解次数与成本关系。

Result: 使用八个合成和真实世界数据集测试，POC始终比现有基线性能高2% - 17%。

Conclusion: POC框架能有效平衡实时操作中性能与成本，工作填补了实时MILP基准和评估标准的文献空白。

Abstract: A common challenge in real-time operations is deciding whether to re-solve an
optimization problem or continue using an existing solution. While modern data
platforms may collect information at high frequencies, many real-time
operations require repeatedly solving computationally intensive optimization
problems formulated as Mixed-Integer Linear Programs (MILPs). Determining when
to re-solve is, therefore, an economically important question. This problem
poses several challenges: 1) How to characterize solution optimality and
solving cost; 2) How to detect environmental changes and select beneficial
samples for solving the MILP; 3) Given the large time horizon and non-MDP
structure, vanilla reinforcement learning (RL) methods are not directly
applicable and tend to suffer from value function explosion. Existing
literature largely focuses on heuristics, low-data settings, and smooth
objectives, with little focus on common NP-hard MILPs. We propose a framework
called Proximal Policy Optimization with Change Point Detection (POC), which
systematically offers a solution for balancing performance and cost when
deciding appropriate re-solving times. Theoretically, we establish the
relationship between the number of re-solves and the re-solving cost. To test
our framework, we assemble eight synthetic and real-world datasets, and show
that POC consistently outperforms existing baselines by 2%-17%. As a side
benefit, our work fills the gap in the literature by introducing real-time MILP
benchmarks and evaluation criteria.

</details>


### [333] [Memory-Efficient Fine-Tuning via Low-Rank Activation Compression](https://arxiv.org/abs/2509.23472)
*Jiang-Xin Shi,Wen-Da Wei,Jin-Fei Qi,Xuanyu Chen,Tong Wei,Yu-Feng Li*

Main category: cs.LG

TL;DR: 本文提出内存高效微调方法LoRAct，可在线压缩激活，实验证明其能降内存且性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法内存开销大，模型激活是内存消耗主要来源，而激活矩阵秩低。

Method: 提出Low - Rank Activation Compression (LoRAct)方法，提供灵活压缩策略，结合基于采样的正交分解算法。

Result: LoRAct能减少约80%激活内存，在视觉和语言任务中性能有竞争力。

Conclusion: LoRAct是一种有效的内存高效微调方法，代码开源。

Abstract: The parameter-efficient fine-tuning paradigm has garnered significant
attention with the advancement of foundation models. Although numerous methods
have been proposed to reduce the number of trainable parameters, their
substantial memory overhead remains a critical bottleneck that hinders
practical deployment. In this paper, we observe that model activations
constitute a major source of memory consumption, especially under large batch
sizes and long context lengths; however, the rank of the activations remains
consistently low. Motivated by this insight, we propose a memory-efficient
fine-tuning approach Low-Rank Activation Compression (LoRAct). Unlike prior
work, LoRAct provides a more flexible and versatile compressing strategy that
can be applied online during the forward pass without the need for any
calibration data. Moreover, LoRAct incorporates a novel sampling-based
orthogonal decomposition algorithm specifically designed for low-rank matrices,
offering improved computational efficiency and a tighter error bound compared
to the widely used RSVD. Experiments on both vision and language tasks
demonstrate the effectiveness of LoRAct. Notably, LoRAct further reduces
activation memory by approximately 80% in comparison with the widely adopted
LoRA method, while maintaining competitive performance. The source code is
available at https://github.com/shijxcs/meft.

</details>


### [334] [Temporal Generalization: A Reality Check](https://arxiv.org/abs/2509.23487)
*Divyam Madaan,Sumit Chopra,Kyunghyun Cho*

Main category: cs.LG

TL;DR: 研究仅依赖过去数据时模型对未来数据的泛化能力，对比多种方法，发现无方法能始终优于使用最新模型参数的基线。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在分布偏移下难以保持性能，研究仅依赖过去数据时模型能否及在何种条件下实现对未来数据的泛化。

Method: 探索参数插值和参数外推两种方法，并在多种时间任务上对相关方法进行基准测试。

Result: 评估的方法在所有场景中都不能始终优于使用最新可用模型参数的简单基线。

Conclusion: 在无法获取未来数据或对数据生成过程缺乏可靠假设时，对未来数据进行泛化和外推存在固有困难，评估相关泛化声明需谨慎。

Abstract: Machine learning (ML) models often struggle to maintain performance under
distribution shifts, leading to inaccurate predictions on unseen future data.
In this work, we investigate whether and under what conditions models can
achieve such a generalization when relying solely on past data. We explore two
primary approaches: convex combinations of past model parameters
(\emph{parameter interpolation}) and explicit extrapolation beyond the convex
hull of past parameters (\emph{parameter extrapolation}). We benchmark several
methods within these categories on a diverse set of temporal tasks, including
language modeling, news summarization, news tag prediction, academic paper
categorization, satellite image-based land use classification over time, and
historical yearbook photo gender prediction. Our empirical findings show that
none of the evaluated methods consistently outperforms the simple baseline of
using the latest available model parameters in all scenarios. In the absence of
access to future data or robust assumptions about the underlying
data-generating process, these results underscore the inherent difficulties of
generalizing and extrapolating to future data and warrant caution when
evaluating claims of such generalization.

</details>


### [335] [Beyond Outliers: A Study of Optimizers Under Quantization](https://arxiv.org/abs/2509.23500)
*Georgios Vlassis,Saleh Ashkboos,Alexandra Volkova,Torsten Hoefler,Dan Alistarh*

Main category: cs.LG

TL;DR: 研究不同优化器在量化场景下对模型性能的影响，发现异常值相关指标无法预测PTQ性能，不同优化器在QAT下表现不同，Shampoo在QAT下参数效率最高。


<details>
  <summary>Details</summary>
Motivation: 现有关于优化器和量化交互的系统性证据有限，需研究优化器选择对量化下模型鲁棒性的影响。

Method: 用六种优化器训练50M - 1.5B参数的全精度模型，探索超参数空间并建立基线；应用PTQ评估不同优化器训练模型的性能退化；从头训练量化模型研究QAT退化。

Result: 异常值相关指标无法预测不同优化器的PTQ性能；原预训练中表现好的优化器在QAT下不一定最优，Shampoo训练的模型精度退化最低。

Conclusion: 推导不同优化器下QAT的缩放定律，Shampoo在测试的优化器中参数效率最高。

Abstract: As new optimizers gain traction and model quantization becomes standard for
efficient deployment, a key question arises: how does the choice of optimizer
affect model performance in the presence of quantization? Despite progress in
both areas, systematic evidence on optimizer-quantization interactions remains
limited. To fill this gap, we study the impact of optimizer choice on model
robustness under quantization, considering both post-training quantization
(PTQ), and quantization-aware training (QAT). We first train full-precision
models, ranging from 50M to 1.5B parameters, with six optimizers, to explore
the hyperparameter landscape, and establish well-tuned baselines. We then apply
PTQ to evaluate how model performance degrades when trained with different
optimizers. We find that outlier-related metrics, such as the max-to-mean ratio
(MMR) and Kurtosis, fail to predict the PTQ performance across different
optimizers. We show analytically that this is due to the MMR capturing only
isolated layer errors, while ignoring how quantization errors accumulate and
propagate through the network. To study the QAT degradation, we train quantized
models from scratch and compare them to our original-precision baselines. We
find that optimizers performing well in the original pretraining setup may not
remain optimal under QAT, and that models trained with Shampoo show the lowest
accuracy degradation. Finally, we derive scaling laws for quantization-aware
training under different optimizers, showing that Shampoo achieves the highest
parameter efficiency of all tested optimizers.

</details>


### [336] [Disentanglement of Variations with Multimodal Generative Modeling](https://arxiv.org/abs/2509.23548)
*Yijie Zhang,Yiyang Shen,Weiran Wang*

Main category: cs.LG

TL;DR: 提出信息解耦多模态VAE（IDMVAE）处理多模态数据，在挑战性数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有多模态生成模型在处理挑战性数据集时，难以有效分离共享和私有信息，提升生成质量和下游任务性能。

Method: 提出IDMVAE，采用基于互信息的正则化方法，包括跨视图互信息最大化和循环一致性损失，还引入扩散模型改进潜在先验。

Result: IDMVAE在挑战性数据集上实现共享和私有信息的清晰分离，展现出更优的生成质量和语义一致性。

Conclusion: IDMVAE能有效解决现有方法在挑战性数据集上的问题，具有更好的性能。

Abstract: Multimodal data are prevalent across various domains, and learning robust
representations of such data is paramount to enhancing generation quality and
downstream task performance. To handle heterogeneity and interconnections among
different modalities, recent multimodal generative models extract shared and
private (modality-specific) information with two separate variables. Despite
attempts to enforce disentanglement between these two variables, these methods
struggle with challenging datasets where the likelihood model is insufficient.
In this paper, we propose Information-disentangled Multimodal VAE (IDMVAE) to
explicitly address this issue, with rigorous mutual information-based
regularizations, including cross-view mutual information maximization for
extracting shared variables, and a cycle-consistency style loss for redundancy
removal using generative augmentations. We further introduce diffusion models
to improve the capacity of latent priors. These newly proposed components are
complementary to each other. Compared to existing approaches, IDMVAE shows a
clean separation between shared and private information, demonstrating superior
generation quality and semantic coherence on challenging datasets.

</details>


### [337] [Fusing Sequence Motifs and Pan-Genomic Features: Antimicrobial Resistance Prediction using an Explainable Lightweight 1D CNN-XGBoost Ensemble](https://arxiv.org/abs/2509.23552)
*Md. Saiful Bari Siddiqui,Nowshin Tarannum*

Main category: cs.LG

TL;DR: 本文提出AMR - EnsembleNet集成框架，结合序列和特征学习，在预测大肠杆菌对抗生素抗性上表现出色，克服单一模型局限。


<details>
  <summary>Details</summary>
Motivation: 当前计算方法在利用基因组测序预测抗菌抗性表型时有局限，标准机器学习模型忽视SNP序列上下文，先进序列模型对数据和计算要求高。

Method: 提出AMR - EnsembleNet集成框架，用轻量级1D CNN学习SNP数据的预测序列基序，与XGBoost模型集成。

Result: 在809个大肠杆菌菌株基准数据集上，对四种抗生素的抗性预测表现优异，如环丙沙星MCC达0.926，庆大霉素Macro F1得分最高达0.691，且模型关注知名AMR基因内的SNP。

Conclusion: 融合序列感知的1D CNN和基于特征的XGBoost模型可创建强大集成，克服单一模型局限。

Abstract: Antimicrobial Resistance (AMR) is a rapidly escalating global health crisis.
While genomic sequencing enables rapid prediction of resistance phenotypes,
current computational methods have limitations. Standard machine learning
models treat the genome as an unordered collection of features, ignoring the
sequential context of Single Nucleotide Polymorphisms (SNPs). State-of-the-art
sequence models like Transformers are often too data-hungry and computationally
expensive for the moderately-sized datasets that are typical in this domain. To
address these challenges, we propose AMR-EnsembleNet, an ensemble framework
that synergistically combines sequence-based and feature-based learning. We
developed a lightweight, custom 1D Convolutional Neural Network (CNN) to
efficiently learn predictive sequence motifs from high-dimensional SNP data.
This sequence-aware model was ensembled with an XGBoost model, a powerful
gradient boosting system adept at capturing complex, non-local feature
interactions. We trained and evaluated our framework on a benchmark dataset of
809 E. coli strains, predicting resistance across four antibiotics with varying
class imbalance. Our 1D CNN-XGBoost ensemble consistently achieved top-tier
performance across all the antibiotics, reaching a Matthews Correlation
Coefficient (MCC) of 0.926 for Ciprofloxacin (CIP) and the highest Macro
F1-score of 0.691 for the challenging Gentamicin (GEN) AMR prediction. We also
show that our model consistently focuses on SNPs within well-known AMR genes
like fusA and parC, confirming it learns the correct genetic signals for
resistance. Our work demonstrates that fusing a sequence-aware 1D CNN with a
feature-based XGBoost model creates a powerful ensemble, overcoming the
limitations of using either an order-agnostic or a standalone sequence model.

</details>


### [338] [Improving constraint-based discovery with robust propagation and reliable LLM priors](https://arxiv.org/abs/2509.23570)
*Ruiqi Lyu,Alistair Turcan,Martin Jinye Zhang,Bryan Wilder*

Main category: cs.LG

TL;DR: 提出MosaCD因果发现方法，结合CI测试和LLM注释，过滤幻觉，采用新传播策略，在真实图上构建图的准确率高于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于约束的因果结构学习方法在假设不满足时会导致级联错误，使用LLM的方法假设专家完美不现实。

Method: 提出MosaCD方法，从CI测试和LLM注释中获取高置信度种子集传播边，用洗牌查询过滤幻觉，采用置信度向下传播策略。

Result: 在多个真实世界图上，MosaCD构建最终图的准确率高于现有基于约束的方法。

Conclusion: MosaCD因改进初始种子可靠性和采用鲁棒传播策略，在因果结构学习上表现更优。

Abstract: Learning causal structure from observational data is central to scientific
modeling and decision-making. Constraint-based methods aim to recover
conditional independence (CI) relations in a causal directed acyclic graph
(DAG). Classical approaches such as PC and subsequent methods orient
v-structures first and then propagate edge directions from these seeds,
assuming perfect CI tests and exhaustive search of separating subsets --
assumptions often violated in practice, leading to cascading errors in the
final graph. Recent work has explored using large language models (LLMs) as
experts, prompting sets of nodes for edge directions, and could augment edge
orientation when assumptions are not met. However, such methods implicitly
assume perfect experts, which is unrealistic for hallucination-prone LLMs. We
propose MosaCD, a causal discovery method that propagates edges from a
high-confidence set of seeds derived from both CI tests and LLM annotations. To
filter hallucinations, we introduce shuffled queries that exploit LLMs'
positional bias, retaining only high-confidence seeds. We then apply a novel
confidence-down propagation strategy that orients the most reliable edges
first, and can be integrated with any skeleton-based discovery method. Across
multiple real-world graphs, MosaCD achieves higher accuracy in final graph
construction than existing constraint-based methods, largely due to the
improved reliability of initial seeds and robust propagation strategies.

</details>


### [339] [EVO-LRP: Evolutionary Optimization of LRP for Interpretable Model Explanations](https://arxiv.org/abs/2509.23585)
*Emerald Zhang,Julian Weaver,Edward Castillo*

Main category: cs.LG

TL;DR: 提出EVO - LRP方法调优LRP超参数，在可解释性和视觉连贯性上优于传统XAI方法。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法存在细节与可解释性的权衡，LRP实现依赖未优化的启发式规则集。

Method: 应用CMA - ES，基于如忠实性、稀疏性等定量可解释性指标来调优LRP超参数。

Result: EVO - LRP在可解释性指标表现和视觉连贯性上超越传统XAI方法，对特定类别特征敏感。

Conclusion: 通过有原则的、特定任务的优化可系统地提高归因质量。

Abstract: Explainable AI (XAI) methods help identify which image regions influence a
model's prediction, but often face a trade-off between detail and
interpretability. Layer-wise Relevance Propagation (LRP) offers a model-aware
alternative. However, LRP implementations commonly rely on heuristic rule sets
that are not optimized for clarity or alignment with model behavior. We
introduce EVO-LRP, a method that applies Covariance Matrix Adaptation Evolution
Strategy (CMA-ES) to tune LRP hyperparameters based on quantitative
interpretability metrics, such as faithfulness or sparseness. EVO-LRP
outperforms traditional XAI approaches in both interpretability metric
performance and visual coherence, with strong sensitivity to class-specific
features. These findings demonstrate that attribution quality can be
systematically improved through principled, task-specific optimization.

</details>


### [340] [Sketching Low-Rank Plus Diagonal Matrices](https://arxiv.org/abs/2509.23587)
*Andres Fernandez,Felix Dangel,Philipp Hennig,Frank Schneider*

Main category: cs.LG

TL;DR: 本文介绍SKETCHLORD方法，可同时估计低秩和对角分量，在合成矩阵实验中表现良好，是结构化近似工具的有价值补充。


<details>
  <summary>Details</summary>
Motivation: 现有草图方法只能构建低秩或对角近似，存在近似误差，需要能同时估计低秩和对角分量的方法。

Method: 引入SKETCHLORD方法同时估计低秩和对角分量，将其转化为凸优化问题得到可扩展算法。

Result: 理论和实证表明联合估计优于顺序估计，合成矩阵实验证实SKETCHLORD能准确恢复结构。

Conclusion: SKETCHLORD是结构化近似工具的有价值补充，适合大规模算子的高保真近似。

Abstract: Many relevant machine learning and scientific computing tasks involve
high-dimensional linear operators accessible only via costly matrix-vector
products. In this context, recent advances in sketched methods have enabled the
construction of *either* low-rank *or* diagonal approximations from few
matrix-vector products. This provides great speedup and scalability, but
approximation errors arise due to the assumed simpler structure. This work
introduces SKETCHLORD, a method that simultaneously estimates both low-rank
*and* diagonal components, targeting the broader class of Low-Rank *plus*
Diagonal (LoRD) linear operators. We demonstrate theoretically and empirically
that this joint estimation is superior also to any sequential variant
(diagonal-then-low-rank or low-rank-then-diagonal). Then, we cast SKETCHLORD as
a convex optimization problem, leading to a scalable algorithm. Comprehensive
experiments on synthetic (approximate) LoRD matrices confirm SKETCHLORD's
performance in accurately recovering these structures. This positions it as a
valuable addition to the structured approximation toolkit, particularly when
high-fidelity approximations are desired for large-scale operators, such as the
deep learning Hessian.

</details>


### [341] [Toward a Holistic Approach to Continual Model Merging](https://arxiv.org/abs/2509.23592)
*Hoang Phan,Sungmin Cha,Tung Lam Tran,Qi Lei*

Main category: cs.LG

TL;DR: 提出用于持续模型合并的整体框架，解决持续学习两大挑战，实验证明有效且可扩展。


<details>
  <summary>Details</summary>
Motivation: 传统方法存在可扩展性问题或丢失关键功能信息，需解决持续学习的两大基本挑战。

Method: 在预合并阶段在切空间微调主模型；合并时利用优化器状态功能信息；合并后进行校正以减少偏差。

Result: 在标准基准测试中取得有竞争力的性能。

Conclusion: 该方法为灾难性遗忘问题提供了可扩展且高效的解决方案。

Abstract: We present a holistic framework for continual model merging that intervenes
at three critical stages: pre-merging, during merging, and post-merging-to
address two fundamental challenges in continual learning. In particular,
conventional approaches either maintain a growing list of per-domain task
vectors, leading to scalability issues or rely solely on weight-space merging
when old data is inaccessible, thereby losing crucial functional information.
Our method overcomes these limitations by first fine-tuning the main model
within its tangent space on domain-specific data; this linearization amplifies
per-task weight disentanglement, effectively mitigating across-task
interference. During merging, we leverage functional information from available
optimizer states beyond mere parameter averages to avoid the need to revisit
old data. Finally, a post-merging correction aligns the representation
discrepancy between pre- and post-merged models, reducing bias and enhancing
overall performance-all while operating under constant memory constraints
without accessing historical data. Extensive experiments on standard
class-incremental and domain-incremental benchmarks demonstrate that our
approach not only achieves competitive performance but also provides a scalable
and efficient solution to the catastrophic forgetting problem.

</details>


### [342] [Avoid Catastrophic Forgetting with Rank-1 Fisher from Diffusion Models](https://arxiv.org/abs/2509.23593)
*Zekun Wang,Anant Gupta,Zihan Dong,Christopher J. MacLellan*

Main category: cs.LG

TL;DR: 研究扩散模型梯度几何，提出秩1 EWC变体与重放方法结合，在图像生成数据集上改善FID、减少遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法（重放和EWC）存在局限，如重放需强生成器、易分布漂移，EWC有假设和近似问题。

Method: 研究扩散模型梯度几何，提出秩1 EWC变体，与重放方法结合。

Result: 在多个图像生成数据集上，相比仅重放和对角EWC基线，平均FID改善，遗忘减少，MNIST和FashionMNIST几乎消除遗忘，ImageNet - 1k遗忘约减半。

Conclusion: 扩散模型有近似秩1的Fisher，更好的Fisher估计使EWC是重放的有力补充。

Abstract: Catastrophic forgetting remains a central obstacle for continual learning in
neural models. Popular approaches -- replay and elastic weight consolidation
(EWC) -- have limitations: replay requires a strong generator and is prone to
distributional drift, while EWC implicitly assumes a shared optimum across
tasks and typically uses a diagonal Fisher approximation. In this work, we
study the gradient geometry of diffusion models, which can already produce
high-quality replay data. We provide theoretical and empirical evidence that,
in the low signal-to-noise ratio (SNR) regime, per-sample gradients become
strongly collinear, yielding an empirical Fisher that is effectively rank-1 and
aligned with the mean gradient. Leveraging this structure, we propose a rank-1
variant of EWC that is as cheap as the diagonal approximation yet captures the
dominant curvature direction. We pair this penalty with a replay-based approach
to encourage parameter sharing across tasks while mitigating drift. On
class-incremental image generation datasets (MNIST, FashionMNIST, CIFAR-10,
ImageNet-1k), our method consistently improves average FID and reduces
forgetting relative to replay-only and diagonal-EWC baselines. In particular,
forgetting is nearly eliminated on MNIST and FashionMNIST and is roughly halved
on ImageNet-1k. These results suggest that diffusion models admit an
approximately rank-1 Fisher. With a better Fisher estimate, EWC becomes a
strong complement to replay: replay encourages parameter sharing across tasks,
while EWC effectively constrains replay-induced drift.

</details>


### [343] [Characteristic Root Analysis and Regularization for Linear Time Series Forecasting](https://arxiv.org/abs/2509.23597)
*Zheng Wang,Kaixuan Zhang,Wanfang Chen,Xiaonan Lu,Longyuan Li,Tobias Schlagenhauf*

Main category: cs.LG

TL;DR: 本文系统研究时间序列预测的线性模型，分析特征根作用，提出两种鲁棒根重构策略，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 复杂模型在不同数据集上效果不稳定，简单线性模型有竞争力但需理论研究。

Method: 先分析无噪声情况，再拓展到有噪声情况，提出秩缩减技术和Root Purge两种策略。

Result: 实验证明两种策略有效，在多个场景达最优。

Conclusion: 将线性系统经典理论与现代学习技术结合可构建鲁棒、可解释和数据高效的预测模型。

Abstract: Time series forecasting remains a critical challenge across numerous domains,
yet the effectiveness of complex models often varies unpredictably across
datasets. Recent studies highlight the surprising competitiveness of simple
linear models, suggesting that their robustness and interpretability warrant
deeper theoretical investigation. This paper presents a systematic study of
linear models for time series forecasting, with a focus on the role of
characteristic roots in temporal dynamics. We begin by analyzing the noise-free
setting, where we show that characteristic roots govern long-term behavior and
explain how design choices such as instance normalization and channel
independence affect model capabilities. We then extend our analysis to the
noisy regime, revealing that models tend to produce spurious roots. This leads
to the identification of a key data-scaling property: mitigating the influence
of noise requires disproportionately large training data, highlighting the need
for structural regularization. To address these challenges, we propose two
complementary strategies for robust root restructuring. The first uses rank
reduction techniques, including Reduced-Rank Regression and Direct Weight Rank
Reduction, to recover the low-dimensional latent dynamics. The second, a novel
adaptive method called Root Purge, encourages the model to learn a
noise-suppressing null space during training. Extensive experiments on standard
benchmarks demonstrate the effectiveness of both approaches, validating our
theoretical insights and achieving state-of-the-art results in several
settings. Our findings underscore the potential of integrating classical
theories for linear systems with modern learning techniques to build robust,
interpretable, and data-efficient forecasting models.

</details>


### [344] [GraphIFE: Rethinking Graph Imbalance Node Classification via Invariant Learning](https://arxiv.org/abs/2509.23616)
*Fanlong Zeng,Wensheng Gan,Philip S. Yu*

Main category: cs.LG

TL;DR: 论文指出图数据类别不平衡下合成节点存在质量不一致问题，提出 GraphIFE 框架解决该问题，实验显示其性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 图数据存在类别不平衡问题，多数图神经网络未考虑该问题，且合成节点有质量不一致问题，导致模型在少数类上性能下降。

Method: 提出 GraphIFE 框架，结合图不变学习的两个关键概念，引入强化嵌入空间表示的策略。

Result: 在多个数据集上，GraphIFE 始终优于各种基线模型，展示了其效率和强大的泛化能力。

Conclusion: GraphIFE 框架能有效缓解图不平衡条件下合成节点的质量不一致问题。

Abstract: The class imbalance problem refers to the disproportionate distribution of
samples across different classes within a dataset, where the minority classes
are significantly underrepresented. This issue is also prevalent in
graph-structured data. Most graph neural networks (GNNs) implicitly assume a
balanced class distribution and therefore often fail to account for the
challenges introduced by class imbalance, which can lead to biased learning and
degraded performance on minority classes. We identify a quality inconsistency
problem in synthesized nodes, which leads to suboptimal performance under graph
imbalance conditions. To mitigate this issue, we propose GraphIFE (Graph
Invariant Feature Extraction), a novel framework designed to mitigate quality
inconsistency in synthesized nodes. Our approach incorporates two key concepts
from graph invariant learning and introduces strategies to strengthen the
embedding space representation, thereby enhancing the model's ability to
identify invariant features. Extensive experiments demonstrate the framework's
efficiency and robust generalization, as GraphIFE consistently outperforms
various baselines across multiple datasets. The code is publicly available at
https://github.com/flzeng1/GraphIFE.

</details>


### [345] [DRIK: Distribution-Robust Inductive Kriging without Information Leakage](https://arxiv.org/abs/2509.23631)
*Chen Yang,Changhao Zhao,Chen Wang,Jiansheng Fan*

Main category: cs.LG

TL;DR: 传统时空分割使归纳克里金存在信息泄漏和OOD泛化问题，提出3x3分区和DRIK方法，实验显示DRIK性能优且可扩展性强。


<details>
  <summary>Details</summary>
Motivation: 解决归纳克里金在传统训练评估设置中存在的信息泄漏和OOD泛化差的问题。

Method: 提出3x3分区以分离训练、验证和测试集；引入DRIK方法，采用节点、边和子图三层策略提升OOD泛化。

Result: 在六个时空数据集上实验，DRIK始终优于现有方法，MAE最多降低12.48%，且可扩展性强。

Conclusion: 3x3分区和DRIK方法有效解决了归纳克里金的信息泄漏和OOD泛化问题，性能良好。

Abstract: Inductive kriging supports high-resolution spatio-temporal estimation with
sparse sensor networks, but conventional training-evaluation setups often
suffer from information leakage and poor out-of-distribution (OOD)
generalization. We find that the common 2x2 spatio-temporal split allows test
data to influence model selection through early stopping, obscuring the true
OOD characteristics of inductive kriging. To address this issue, we propose a
3x3 partition that cleanly separates training, validation, and test sets,
eliminating leakage and better reflecting real-world applications. Building on
this redefined setting, we introduce DRIK, a Distribution-Robust Inductive
Kriging approach designed with the intrinsic properties of inductive kriging in
mind to explicitly enhance OOD generalization, employing a three-tier strategy
at the node, edge, and subgraph levels. DRIK perturbs node coordinates to
capture continuous spatial relationships, drops edges to reduce ambiguity in
information flow and increase topological diversity, and adds pseudo-labeled
subgraphs to strengthen domain generalization. Experiments on six diverse
spatio-temporal datasets show that DRIK consistently outperforms existing
methods, achieving up to 12.48% lower MAE while maintaining strong scalability.

</details>


### [346] [PreScope: Unleashing the Power of Prefetching for Resource-Constrained MoE Inference](https://arxiv.org/abs/2509.23638)
*Enda Yu,Zhaoning Zhang,Dezun Dong,Yongwei Wu,Xiangke Liao*

Main category: cs.LG

TL;DR: 提出PreScope系统解决MoE模型部署问题，含LLaPor、PreSched和AsyncIO组件，性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型在商用硬件上部署时的内存和PCIe延迟瓶颈，以及卸载专家权重带来的高PCIe传输延迟问题。

Method: 提出Learnable Layer - Aware Predictor (LLaPor)捕获特定层专家激活模式；Prefetch - Aware Cross - Layer Scheduling (PreSched)生成全局最优计划；Asynchronous I/O Optimizer (AsyncIO)解耦I/O与计算。

Result: PreScope比现有技术方案实现了141%的吞吐量提升和74.6%的延迟降低。

Conclusion: PreScope能有效解决MoE模型部署面临的关键挑战，提升系统性能。

Abstract: Mixture-of-Experts (MoE) models face memory and PCIe latency bottlenecks when
deployed on commodity hardware. Offloading expert weights to CPU memory results
in PCIe transfer latency that exceeds GPU computation by several folds. We
present PreScope, a prediction-driven expert scheduling system that addresses
three key challenges: inaccurate activation prediction, PCIe bandwidth
competition, and cross-device scheduling complexity. Our solution includes: 1)
Learnable Layer-Aware Predictor (LLaPor) that captures layer-specific expert
activation patterns; 2) Prefetch-Aware Cross-Layer Scheduling (PreSched) that
generates globally optimal plans balancing prefetching costs and loading
overhead; 3) Asynchronous I/O Optimizer (AsyncIO) that decouples I/O from
computation, eliminating waiting bubbles. PreScope achieves 141% higher
throughput and 74.6% lower latency than state-of-the-art solutions.

</details>


### [347] [Virtual Nodes based Heterogeneous Graph Convolutional Neural Network for Efficient Long-Range Information Aggregation](https://arxiv.org/abs/2509.23660)
*Ranhui Yan,Jia cai*

Main category: cs.LG

TL;DR: 提出基于虚拟节点的异构图卷积网络VN - HGCN，利用虚拟节点促进图中信息流动，仅4层就能有效聚合信息，且有通用性，实验验证其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有异构图模型难以捕捉长距离信息，堆叠多层会导致高计算复杂度和过平滑问题。

Method: 提出VN - HGCN，引入虚拟节点，虚拟节点与特定类型的所有节点相连，以高效聚合不同类型节点和边的长距离信息。

Result: VN - HGCN仅4层就能实现有效信息聚合，可作为通用框架应用于其他HGNN模型。在三个真实世界异构图数据集上的实验显示其优于多个先进基线模型。

Conclusion: VN - HGCN有效且具有通用性，在异构图学习中有较好表现。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) have exhibited powerful
performance in heterogeneous graph learning by aggregating information from
various types of nodes and edges. However, existing heterogeneous graph models
often struggle to capture long-range information or necessitate stacking
numerous layers to learn such dependencies, resulting in high computational
complexity and encountering over-smoothing issues. In this paper, we propose a
Virtual Nodes based Heterogeneous Graph Convolutional Network (VN-HGCN), which
leverages virtual nodes to facilitate enhanced information flow within the
graph. Virtual nodes are auxiliary nodes interconnected with all nodes of a
specific type in the graph, facilitating efficient aggregation of long-range
information across different types of nodes and edges. By incorporating virtual
nodes into the graph structure, VN-HGCN achieves effective information
aggregation with only $4$ layers. Additionally, we demonstrate that VN-HGCN can
serve as a versatile framework that can be seamlessly applied to other HGNN
models, showcasing its generalizability. Empirical evaluations validate the
effectiveness of VN-HGCN, and extensive experiments conducted on three
real-world heterogeneous graph datasets demonstrate the superiority of our
model over several state-of-the-art baselines.

</details>


### [348] [Pure Node Selection for Imbalanced Graph Node Classification](https://arxiv.org/abs/2509.23662)
*Fanlong Zeng,Wensheng Gan,Jiayang Wu,Philip S. Yu*

Main category: cs.LG

TL;DR: 论文针对图数据中GNN的类别不平衡问题，提出PNS模块，解决随机种子影响，实验证明其有效稳定。


<details>
  <summary>Details</summary>
Motivation: 图数据存在类别不平衡问题，现有GNN基于类别平衡假设，部分模型受随机种子影响性能下降。

Method: 提出PNS模块，在节点合成阶段解决随机异常连接问题（RACP），缓解节点邻居异常分布导致的性能下降。

Result: 实验表明PNS消除了不利随机种子的影响，在不同基准数据集和GNN骨干网络上表现优于基线。

Conclusion: PNS是有效的、稳定的，可缓解RACP，减轻节点邻居异常分布的影响。

Abstract: The problem of class imbalance refers to an uneven distribution of quantity
among classes in a dataset, where some classes are significantly
underrepresented compared to others. Class imbalance is also prevalent in
graph-structured data. Graph neural networks (GNNs) are typically based on the
assumption of class balance, often overlooking the issue of class imbalance. In
our investigation, we identified a problem, which we term the Randomness
Anomalous Connectivity Problem (RACP), where certain off-the-shelf models are
affected by random seeds, leading to a significant performance degradation. To
eliminate the influence of random factors in algorithms, we proposed PNS (Pure
Node Sampling) to address the RACP in the node synthesis stage. Unlike existing
approaches that design specialized algorithms to handle either quantity
imbalance or topological imbalance, PNS is a novel plug-and-play module that
operates directly during node synthesis to mitigate RACP. Moreover, PNS also
alleviates performance degradation caused by abnormal distribution of node
neighbors. We conduct a series of experiments to identify what factors are
influenced by random seeds. Experimental results demonstrate the effectiveness
and stability of our method, which not only eliminates the effect of
unfavorable random seeds but also outperforms the baseline across various
benchmark datasets with different GNN backbones. Data and code are available at
https://github.com/flzeng1/PNS.

</details>


### [349] [Calibration Meets Reality: Making Machine Learning Predictions Trustworthy](https://arxiv.org/abs/2509.23665)
*Kristina P. Sinaga,Arjun S. Nair*

Main category: cs.LG

TL;DR: 本文对事后校准方法进行理论分析，通过实验探索特征信息性对校准性能的影响，为选择校准方法提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 事后校准方法广泛使用，但缺乏全面理论理解，特征质量与校准性能的相互作用未深入研究。

Method: 对Platt缩放和等渗回归进行理论分析，推导收敛保证、计算复杂度界限和有限样本性能指标，通过合成实验探索特征信息性影响，在多种数据集和模型架构上进行实证评估。

Result: 在不同场景下校准指标有一致改进，通过对比不同特征条件下的校准性能，得到不同校准方法的稳健性和可靠性的见解。

Conclusion: 研究结果为基于数据集特征和计算约束选择合适的校准方法提供实用指南，弥合理论理解与实际应用间的差距。

Abstract: Post-hoc calibration methods are widely used to improve the reliability of
probabilistic predictions from machine learning models. Despite their
prevalence, a comprehensive theoretical understanding of these methods remains
elusive, particularly regarding their performance across different datasets and
model architectures. Input features play a crucial role in shaping model
predictions and, consequently, their calibration. However, the interplay
between feature quality and calibration performance has not been thoroughly
investigated. In this work, we present a rigorous theoretical analysis of
post-hoc calibration methods, focusing on Platt scaling and isotonic
regression. We derive convergence guarantees, computational complexity bounds,
and finite-sample performance metrics for these methods. Furthermore, we
explore the impact of feature informativeness on calibration performance
through controlled synthetic experiments. Our empirical evaluation spans a
diverse set of real-world datasets and model architectures, demonstrating
consistent improvements in calibration metrics across various scenarios. By
examining calibration performance under varying feature conditions utilizing
only informative features versus complete feature spaces including noise
dimensions, we provide fundamental insights into the robustness and reliability
of different calibration approaches. Our findings offer practical guidelines
for selecting appropriate calibration methods based on dataset characteristics
and computational constraints, bridging the gap between theoretical
understanding and practical implementation in uncertainty quantification. Code
and experimental data are available at:
https://github.com/Ajwebdevs/calibration-analysis-experiments.

</details>


### [350] [Beyond Greedy Exits: Improved Early Exit Decisions for Risk Control and Reliability](https://arxiv.org/abs/2509.23666)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: 提出UAT框架解决早期退出策略问题，在多任务上验证性能，提升速度且性能损失小。


<details>
  <summary>Details</summary>
Motivation: 现有早期退出策略存在模型可能对错误类别过度自信，且对分布偏移不鲁棒的问题。

Method: 使用多臂老虎机框架自适应调整退出决策阈值，基于新奖励函数做决策。

Result: 在多种任务上验证，速度提升1.70 - 2.10倍，性能下降小于2%。

Conclusion: UAT框架能有效平衡计算效率和预测质量，提升推理速度并保证一定性能。

Abstract: Early-Exit Deep Neural Networks enable adaptive inference by allowing
prediction at intermediary layers, significantly reducing computational costs
and latency. Most of the early exit strategies greedily exit a sample at an
intermediary layer if the confidence in class prediction exceeds a predefined
threshold that is set using a static validation set. This is problematic as the
model might be overconfident in a wrong class. Also, they are not robust to
distribution shifts encountered in deployment, which can undermine model
trustworthiness and accuracy. To address these challenges, we propose UAT that
adapts the threshold for exit decisions using a Multi-Armed Bandit framework,
enabling online, unsupervised adjustment of exit decisions. UAT makes decisions
based on a new reward function that assesses predictive certainty and its
reliability to balance computational efficiency and prediction quality while
penalizing unnecessary late exits. We provide guarantees on risk achieved by
UAT and validate its performance on diverse tasks spanning vision-language
understanding, text generation, and classification. Our framework demonstrates
consistent improvements in speedup (1.70-2.10x) with a minimal performance drop
(<2%) as compared to full model performance. Our source code is available at
https://github.com/Div290/UAT.

</details>


### [351] [Why Alignment Must Precede Distillation: A Minimal Working Explanation](https://arxiv.org/abs/2509.23667)
*Sungmin Cha,Kyunghyun Cho*

Main category: cs.LG

TL;DR: 指出偏好对齐常基于知识蒸馏模型进行存在局限，提出应先对齐再蒸馏，通过实验验证该方法更好。


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐在知识蒸馏模型上进行的做法忽视参考模型分布召回率，影响对罕见行为的对齐。

Method: 从理论解释参考模型对偏好对齐目标的约束，在高斯混合实验验证理论，在LLM对齐中对比先蒸馏后对齐和先对齐后蒸馏的效果。

Result: 先蒸馏后对齐的模型在对齐目标行为上效果差，先对齐后蒸馏的管道能更有效地对齐行为，指标更优且方差低。

Conclusion: 参考模型召回率是对齐设计的首要考虑因素，对齐应先于蒸馏。

Abstract: For efficiency, preference alignment is often performed on compact,
knowledge-distilled (KD) models. We argue this common practice introduces a
significant limitation by overlooking a key property of the alignment's
reference model: its distributional recall. We show that the standard KD ->
Align workflow diminishes the model's capacity to align rare yet desirable
behaviors, even under strong preference signals. We instead demonstrate that
reversing the pipeline (i.e., Align -> KD) is essential: alignment must first
be performed on a high-recall reference before distillation. Our contributions
are threefold. First, we provide a minimal working explanation of how the
reference model constrains preference alignment objectives at a fundamental
level. Second, we validate this theory in a controllable Mixture-of-Gaussians
experiment, where low-recall anchoring consistently results in suboptimal model
performance. Finally, we demonstrate that the same phenomenon holds in LLM
alignment with the SmolLM2 family: models aligned after KD fail to effectively
align target behaviors, resulting in substantially lower reward and target
precision. In contrast, our proposed Align -> KD pipeline robustly aligns these
behaviors, yielding models with superior target-oriented metrics and lower
variance. Together, these results establish reference-model recall as a
first-order design choice in alignment, offering a clear principle: alignment
must precede distillation.

</details>


### [352] [Multi-Scale Spatial-Temporal Hypergraph Network with Lead-Lag Structures for Stock Time Series Forecasting](https://arxiv.org/abs/2509.23668)
*Xiangfei Qiu,Liu Yang,Hanyin Cheng,Xingjian Wu,Rongjia Wu,Zhigang Zhang,Ding Tu,Chenjuan Guo,Bin Yang,Christian S. Jensen,Jilin Hu*

Main category: cs.LG

TL;DR: 提出Hermes框架用于股票时间序列预测，通过消除现有方法局限提升行业相关性利用，实验表明其在效率和准确性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于超图的股票时间序列预测方法对行业相关性挖掘较浅，未充分考虑行业间领先 - 滞后交互和行业内外多尺度信息，需改进。

Method: 在超图网络中集成移动聚合和多尺度融合模块，提出基于超边的移动聚合模块捕捉行业间领先 - 滞后关系，采用跨尺度、边到边消息传递建模多尺度信息。

Result: 在多个真实股票数据集上实验，Hermes在效率和准确性上均优于现有最先进方法。

Conclusion: Hermes框架能有效消除现有方法局限，更好地利用行业相关性进行股票时间序列预测。

Abstract: Time series forecasting occurs in a range of financial applications providing
essential decision-making support to investors, regulatory institutions, and
analysts. Unlike multivariate time series from other domains, stock time series
exhibit industry correlation. Exploiting this kind of correlation can improve
forecasting accuracy. However, existing methods based on hypergraphs can only
capture industry correlation relatively superficially. These methods face two
key limitations: they do not fully consider inter-industry lead-lag
interactions, and they do not model multi-scale information within and among
industries. This study proposes the Hermes framework for stock time series
forecasting that aims to improve the exploitation of industry correlation by
eliminating these limitations. The framework integrates moving aggregation and
multi-scale fusion modules in a hypergraph network. Specifically, to more
flexibly capture the lead-lag relationships among industries, Hermes proposes a
hyperedge-based moving aggregation module. This module incorporates a sliding
window and utilizes dynamic temporal aggregation operations to consider
lead-lag dependencies among industries. Additionally, to effectively model
multi-scale information, Hermes employs cross-scale, edge-to-edge message
passing to integrate information from different scales while maintaining the
consistency of each scale. Experimental results on multiple real-world stock
datasets show that Hermes outperforms existing state-of-the-art methods in both
efficiency and accuracy.

</details>


### [353] [Graph Neural Networks with Diversity-aware Neighbor Selection and Dynamic Multi-scale Fusion for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.23671)
*Jingqi Xu,Guibin Chen,Jingxi Lu,Yuzhang Lin*

Main category: cs.LG

TL;DR: 提出DIMIGNN模型解决GNNs在MTS预测中的问题，实验显示其优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有GNNs方法在MTS预测中忽略邻居信息多样性，且最终预测仅依赖单一时间尺度表示。

Method: 提出DIMIGNN，引入DNSM确保邻居信息多样性，引入DMFM动态调整不同时间尺度预测结果对最终结果的贡献。

Result: 在真实数据集上的大量实验表明，DIMIGNN始终优于先前方法。

Conclusion: DIMIGNN能有效解决现有GNNs方法存在的问题，提升MTS预测性能。

Abstract: Recently, numerous deep models have been proposed to enhance the performance
of multivariate time series (MTS) forecasting. Among them, Graph Neural
Networks (GNNs)-based methods have shown great potential due to their
capability to explicitly model inter-variable dependencies. However, these
methods often overlook the diversity of information among neighbors, which may
lead to redundant information aggregation. In addition, their final prediction
typically relies solely on the representation from a single temporal scale. To
tackle these issues, we propose a Graph Neural Networks (GNNs) with
Diversity-aware Neighbor Selection and Dynamic Multi-scale Fusion (DIMIGNN).
DIMIGNN introduces a Diversity-aware Neighbor Selection Mechanism (DNSM) to
ensure that each variable shares high informational similarity with its
neighbors while maintaining diversity among neighbors themselves. Furthermore,
a Dynamic Multi-Scale Fusion Module (DMFM) is introduced to dynamically adjust
the contributions of prediction results from different temporal scales to the
final forecasting result. Extensive experiments on real-world datasets
demonstrate that DIMIGNN consistently outperforms prior methods.

</details>


### [354] [Towards a Comprehensive Scaling Law of Mixture-of-Experts](https://arxiv.org/abs/2509.23678)
*Guoliang Zhao,Yuhan Fu,Shuaipeng Li,Xingwu Sun,Ruobing Xie,An Wang,Weidong Han,Zhen Yang,Weixuan Sun,Yudong Zhang,Cheng-zhong Xu,Di Wang,Jie Jiang*

Main category: cs.LG

TL;DR: 本文研究MoE模型特定的缩放定律，分解MoE设置确定五个关键因素，通过实验构建联合缩放定律，得出最优配置，为MoE模型设计和训练提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有密集模型的缩放定律不适用于MoE模型，存在多重影响因素、复杂耦合关系和非单调性能影响等挑战，需研究MoE特定的缩放定律。

Method: 系统分解MoE设置，确定五个关键因素，设计446个对照实验刻画边际效应，构建联合缩放定律，推导最优配置。

Result: 最优的G和S设置与模型架构和数据大小无关，随着N的增加，最优激活参数比Na/N变得更稀疏。

Conclusion: 提出的MoE缩放定律能为未来MoE模型设计和训练提供准确且有洞察力的指导。

Abstract: Mixture-of-Experts (MoE) models have become the consensus approach for
enabling parameter-efficient scaling and cost-effective deployment in large
language models. However, existing scaling laws for dense models are
inapplicable to MoE models, which stems from three critical challenges: the
multiplicity of influencing factors, their intricate coupling relationships and
the non-monotonic nature of their performance impacts. They collectively
necessitate a fine-grained investigation into MoE-specific scaling laws. In
this work, we perform a systematic decomposition of MoE settings, identifying
five key factors that influence model performance from both size and structural
perspectives (data size ($D$), total model size ($N$), activated model size
($N_a$), number of active experts ($G$) and the ratio of shared experts ($S$)).
Specifically, we design $446$ controlled experiments to characterize their
marginal effects, ultimately constructing a comprehensive and precise joint MoE
scaling law that considers all essential factors. Furthermore, we derive the
theoretically optimal and practically efficiency-aware optimal configurations
for $G$, $S$ and $N_a/N$ with detailed analyses. Our results demonstrate that
the optimal settings for $G$ and $S$ are independent of both the model
architecture and data size. With the scaling of $N$, the optimal activation
parameter ratio of $N_a/N$ becomes sparser. Our proposed MoE scaling law could
function as an accurate and insightful guidance to facilitate future MoE model
design and training.

</details>


### [355] [Decentralized Dynamic Cooperation of Personalized Models for Federated Continual Learning](https://arxiv.org/abs/2509.23683)
*Danni Yang,Zhikang Chen,Sen Cui,Mengyue Yang,Ding Li,Abudukelimu Wuerkaixi,Haoxuan Li,Jinke Ren,Mingming Gong*

Main category: cs.LG

TL;DR: 提出用于联邦持续学习的去中心化动态合作框架，通过动态联盟和算法实现合作与动态平衡，实验表明方法优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有联邦持续学习方法存在灾难性遗忘问题，聚合知识可能引入干扰，去中心化方法组形成效不佳。

Method: 提出去中心化动态合作框架，让客户端建立动态合作学习联盟，用联盟亲和博弈模拟关系，提出合并阻塞和动态合作进化算法。

Result: 综合实验显示该方法相比各种基线具有优越性。

Conclusion: 所提框架和算法能有效解决联邦持续学习中的问题，提升模型性能。

Abstract: Federated continual learning (FCL) has garnered increasing attention for its
ability to support distributed computation in environments with evolving data
distributions. However, the emergence of new tasks introduces both temporal and
cross-client shifts, making catastrophic forgetting a critical challenge. Most
existing works aggregate knowledge from clients into a global model, which may
not enhance client performance since irrelevant knowledge could introduce
interference, especially in heterogeneous scenarios. Additionally, directly
applying decentralized approaches to FCL suffers from ineffective group
formation caused by task changes. To address these challenges, we propose a
decentralized dynamic cooperation framework for FCL, where clients establish
dynamic cooperative learning coalitions to balance the acquisition of new
knowledge and the retention of prior learning, thereby obtaining personalized
models. To maximize model performance, each client engages in selective
cooperation, dynamically allying with others who offer meaningful performance
gains. This results in non-overlapping, variable coalitions at each stage of
the task. Moreover, we use coalitional affinity game to simulate coalition
relationships between clients. By assessing both client gradient coherence and
model similarity, we quantify the client benefits derived from cooperation. We
also propose a merge-blocking algorithm and a dynamic cooperative evolution
algorithm to achieve cooperative and dynamic equilibrium. Comprehensive
experiments demonstrate the superiority of our method compared to various
baselines. Code is available at: https://github.com/ydn3229/DCFCL.

</details>


### [356] [Hedonic Neurons: A Mechanistic Mapping of Latent Coalitions in Transformer MLPs](https://arxiv.org/abs/2509.23684)
*Tanya Chowdhury,Atharva Nijasure,Yair Zick,James Allan*

Main category: cs.LG

TL;DR: 研究微调大语言模型MLP层特征表示，引入基于合作博弈论的框架提取神经元稳定联盟，应用于多个模型有较好效果。


<details>
  <summary>Details</summary>
Motivation: 微调大语言模型MLP层特征表示形式不明，此前研究表明需研究神经元协同工作，因此有必要深入探究。

Method: 引入基于合作博弈论的机制可解释性框架，用top - responsive utilities和PAC - Top - Cover算法提取神经元稳定联盟并跟踪其跨层转变。

Result: 应用于多个微调模型，找到的联盟协同性高于聚类基线。

Conclusion: 该方法揭示神经元合作编码特征，发现超越解纠缠的高阶结构，得到功能重要、可解释且跨领域可预测的计算单元。

Abstract: Fine-tuned Large Language Models (LLMs) encode rich task-specific features,
but the form of these representations, especially within MLP layers, remains
unclear. Empirical inspection of LoRA updates shows that new features
concentrate in mid-layer MLPs, yet the scale of these layers obscures
meaningful structure. Prior probing suggests that statistical priors may
strengthen, split, or vanish across depth, motivating the need to study how
neurons work together rather than in isolation.
  We introduce a mechanistic interpretability framework based on coalitional
game theory, where neurons mimic agents in a hedonic game whose preferences
capture their synergistic contributions to layer-local computations. Using
top-responsive utilities and the PAC-Top-Cover algorithm, we extract stable
coalitions of neurons: groups whose joint ablation has non-additive effects. We
then track their transitions across layers as persistence, splitting, merging,
or disappearance.
  Applied to LLaMA, Mistral, and Pythia rerankers fine-tuned on scalar IR
tasks, our method finds coalitions with consistently higher synergy than
clustering baselines. By revealing how neurons cooperate to encode features,
hedonic coalitions uncover higher-order structure beyond disentanglement and
yield computational units that are functionally important, interpretable, and
predictive across domains.

</details>


### [357] [FedDAPL: Toward Client-Private Generalization in Federated Learning](https://arxiv.org/abs/2509.23688)
*Soroosh Safari Loaliyan,Jose-Luis Ambite,Paul M. Thompson,Neda Jahanshad,Greg Ver Steeg*

Main category: cs.LG

TL;DR: 本文将DANN集成到联邦学习过程中，提出近端正则化方法解决扫描器导致的域偏移问题，实验表明该方法在跨站点泛化和数据隐私保护上表现出色。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医学影像应用中存在扫描器导致的域偏移问题，现有方法与联邦学习隐私约束冲突，需要新方法解决。

Method: 将Domain - Adversarial Neural Network (DANN)集成到联邦学习过程，提出近端正则化方法稳定客户端间的对抗训练。

Result: 在OpenBHB数据集的T1加权3 - D脑MRI上实验，训练15个站点，测试19个未见站点，比FedAvg和ERM有更好的跨站点泛化能力，且保护了数据隐私。

Conclusion: 提出的方法能有效解决联邦学习在医学影像中的域偏移问题，在跨站点泛化和数据隐私保护上有优势。

Abstract: Federated Learning (FL) trains models locally at each research center or
clinic and aggregates only model updates, making it a natural fit for medical
imaging, where strict privacy laws forbid raw data sharing. A major obstacle is
scanner-induced domain shift: non-biological variations in hardware or
acquisition protocols can cause models to fail on external sites. Most
harmonization methods correct this shift by directly comparing data across
sites, conflicting with FL's privacy constraints. Domain Generalization (DG)
offers a privacy-friendly alternative - learning site-invariant representations
without sharing raw data - but standard DG pipelines still assume centralized
access to multi-site data, again violating FL's guarantees. This paper meets
these difficulties with a straightforward integration of a Domain-Adversarial
Neural Network (DANN) within the FL process. After demonstrating that a naive
federated DANN fails to converge, we propose a proximal regularization method
that stabilizes adversarial training among clients. Experiments on T1-weighted
3-D brain MRIs from the OpenBHB dataset, performing brain-age prediction on
participants aged 6-64 y (mean 22+/-6 y; 45 percent male) in training and 6-79
y (mean 19+/-13 y; 55 percent male) in validation, show that training on 15
sites and testing on 19 unseen sites yields superior cross-site generalization
over FedAvg and ERM while preserving data privacy.

</details>


### [358] [Merge Now, Regret Later: The Hidden Cost of Model Merging is Adversarial Transferability](https://arxiv.org/abs/2509.23689)
*Ankit Gangwal,Aaryan Ajay Sharma*

Main category: cs.LG

TL;DR: 研究模型合并（MM）对对抗样本可迁移性的影响，挑战MM免费对抗鲁棒性观念，揭示相关见解并提供潜在解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索MM对基于对抗样本的迁移攻击的影响，需深入研究。

Method: 对8种MM方法、7个数据集和6种攻击方法进行综合评估和统计分析，覆盖336种不同攻击设置。

Result: MM不能可靠防御迁移攻击，相对迁移攻击成功率超95%；更强MM方法、减少表征偏差会增加迁移攻击脆弱性；权重平均是最易受迁移攻击的MM方法。

Conclusion: 研究结果为使用MM设计更安全系统提供关键见解。

Abstract: Model Merging (MM) has emerged as a promising alternative to multi-task
learning, where multiple fine-tuned models are combined, without access to
tasks' training data, into a single model that maintains performance across
tasks. Recent works have explored the impact of MM on adversarial attacks,
particularly backdoor attacks. However, none of them have sufficiently explored
its impact on transfer attacks using adversarial examples, i.e., a black-box
adversarial attack where examples generated for a surrogate model successfully
mislead a target model.
  In this work, we study the effect of MM on the transferability of adversarial
examples. We perform comprehensive evaluations and statistical analysis
consisting of 8 MM methods, 7 datasets, and 6 attack methods, sweeping over 336
distinct attack settings. Through it, we first challenge the prevailing notion
of MM conferring free adversarial robustness, and show MM cannot reliably
defend against transfer attacks, with over 95% relative transfer attack success
rate. Moreover, we reveal 3 key insights for machine-learning practitioners
regarding MM and transferability for a robust system design: (1) stronger MM
methods increase vulnerability to transfer attacks; (2) mitigating
representation bias increases vulnerability to transfer attacks; and (3) weight
averaging, despite being the weakest MM method, is the most vulnerable MM
method to transfer attacks. Finally, we analyze the underlying reasons for this
increased vulnerability, and provide potential solutions to the problem. Our
findings offer critical insights for designing more secure systems employing
MM.

</details>


### [359] [Estimating Time Series Foundation Model Transferability via In-Context Learning](https://arxiv.org/abs/2509.23695)
*Qingren Yao,Ming Jin,Chengqi Zhang,Chao-Han Huck Yang,Jun Qi,Shirui Pan*

Main category: cs.LG

TL;DR: 本文介绍了TimeTic框架，用于预测时间序列基础模型在下游数据集微调后的性能，在基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 随着时间序列基础模型增多，高效选择适合下游微调的最佳模型愈发困难。

Method: 将模型选择问题转化为上下文学习问题，利用表格基础模型作为上下文学习者，引入基于熵演化的模型特征。

Result: 在包含10个数据集、10个基础模型和3个预测任务的基准测试中，TimeTic估计与实际微调性能高度一致，平均秩相关性约为0.6，比使用零样本性能作为可迁移性得分提高30%。

Conclusion: TimeTic框架能有效预测时间序列基础模型在下游数据集微调后的性能。

Abstract: Time series foundation models (TSFMs) offer strong zero-shot forecasting via
large-scale pre-training, yet fine-tuning remains critical for boosting
performance in domains with limited public data. With the growing number of
TSFMs, efficiently identifying the best model for downstream fine-tuning
becomes increasingly challenging. In this work, we introduce TimeTic, a
transferability estimation framework that recasts model selection as an
in-context-learning problem: given observations on known (source) datasets, it
predicts how a TSFM will perform after fine-tuning on a downstream (target)
dataset. TimeTic flexibly organizes the observed model-data relationships as
contextual information, allowing it to adapt seamlessly to various test-time
scenarios. Leveraging the natural tabular structure formed by dataset
meta-features, model characteristics, and fine-tuned performance, we employ
tabular foundation models to serve as in-context learners. We further introduce
a novel model characterization based on entropy evolution across model layers,
capturing embedding-space distinctions and enabling TimeTic to generalize
across arbitrary model sets. We establish a comprehensive benchmark for
transferability estimation including 10 datasets, 10 foundation models, and 3
forecasting tasks. On this benchmark, TimeTic's estimation demonstrates strong
alignment with actual fine-tuned performance for previously unseen datasets,
achieving a mean rank correlation of approximately 0.6 and a 30% improvement
compared to using zero-shot performance as the transferability score.

</details>


### [360] [A Self-Adaptive Frequency Domain Network for Continuous Intraoperative Hypotension Prediction](https://arxiv.org/abs/2509.23720)
*Xian Zeng,Tianze Xu,Kai Yang,Jie Sun,Youran Wang,Jun Xu,Mucheng Ren*

Main category: cs.LG

TL;DR: 提出SAFDNet模型用于术中低血压（IOH）预警，在两个数据集验证中表现出色，适合临床应用。


<details>
  <summary>Details</summary>
Motivation: 术中低血压与术后并发症紧密相关，现有基于人工智能的预警模型存在不能兼顾时频域信息、捕捉长短依赖和处理生物信号噪声等问题。

Method: 提出SAFDNet模型，集成自适应频谱块利用傅里叶分析提取频域特征并自适应阈值降噪，引入交互式注意力块捕捉长短依赖。

Result: SAFDNet在两个大规模真实数据集的内外部验证中，IOH预警的AUROC最高达97.3%，优于现有模型，对噪声不敏感。

Conclusion: SAFDNet有强大的预测性能和低噪声敏感性，适合实际临床应用。

Abstract: Intraoperative hypotension (IOH) is strongly associated with postoperative
complications, including postoperative delirium and increased mortality, making
its early prediction crucial in perioperative care. While several artificial
intelligence-based models have been developed to provide IOH warnings, existing
methods face limitations in incorporating both time and frequency domain
information, capturing short- and long-term dependencies, and handling noise
sensitivity in biosignal data. To address these challenges, we propose a novel
Self-Adaptive Frequency Domain Network (SAFDNet). Specifically, SAFDNet
integrates an adaptive spectral block, which leverages Fourier analysis to
extract frequency-domain features and employs self-adaptive thresholding to
mitigate noise. Additionally, an interactive attention block is introduced to
capture both long-term and short-term dependencies in the data. Extensive
internal and external validations on two large-scale real-world datasets
demonstrate that SAFDNet achieves up to 97.3\% AUROC in IOH early warning,
outperforming state-of-the-art models. Furthermore, SAFDNet exhibits robust
predictive performance and low sensitivity to noise, making it well-suited for
practical clinical applications.

</details>


### [361] [Time-Shifted Token Scheduling for Symbolic Music Generation](https://arxiv.org/abs/2509.23749)
*Ting-Kang Wang,Chih-Pin Tan,Yi-Hsuan Yang*

Main category: cs.LG

TL;DR: 提出延迟调度机制DP解决符号音乐生成中效率和质量的权衡问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 符号音乐生成存在效率和质量的权衡，细粒度标记序列长、复杂度高，紧凑标记牺牲内部依赖。

Method: 采用基于延迟的调度机制DP，在解码步骤中扩展复合标记，实现内部依赖的自回归建模，且不引入额外参数，可集成到现有表示中。

Result: 在符号管弦乐MIDI数据集实验中，该方法在所有指标上优于标准复合标记，缩小与细粒度标记的差距。

Conclusion: 所提出的DP机制能有效解决符号音乐生成的效率和质量权衡问题。

Abstract: Symbolic music generation faces a fundamental trade-off between efficiency
and quality. Fine-grained tokenizations achieve strong coherence but incur long
sequences and high complexity, while compact tokenizations improve efficiency
at the expense of intra-token dependencies. To address this, we adapt a
delay-based scheduling mechanism (DP) that expands compound-like tokens across
decoding steps, enabling autoregressive modeling of intra-token dependencies
while preserving efficiency. Notably, DP is a lightweight strategy that
introduces no additional parameters and can be seamlessly integrated into
existing representations. Experiments on symbolic orchestral MIDI datasets show
that our method improves all metrics over standard compound tokenizations and
narrows the gap to fine-grained tokenizations.

</details>


### [362] [An Investigation of Batch Normalization in Off-Policy Actor-Critic Algorithms](https://arxiv.org/abs/2509.23750)
*Li Wang,Sudun,Xingjian Zhang,Wenjun Wu,Lei Huang*

Main category: cs.LG

TL;DR: 本文探讨BN在深度强化学习（DRL）中的应用，开展实证研究，识别失败模式，提出MA - BN方法并验证其在DRL中的优势。


<details>
  <summary>Details</summary>
Motivation: 虽然BN在深度学习中成功，但因DRL数据非独立同分布和分布动态变化，其在DRL中应用受限，本文旨在挖掘BN在DRL中的独特优势。

Method: 对离线策略的演员 - 评论家算法中BN的使用进行全面实证研究，分析不同训练和评估模式对性能的影响，识别失败模式并分析原因，提出MA - BN方法。

Result: MA - BN在RL环境中加速并稳定训练，拓宽有效学习率范围，增强探索能力，降低整体优化难度。

Conclusion: BN在DRL中合理应用有独特优势，MA - BN方法能有效将BN集成到DRL流程中。

Abstract: Batch Normalization (BN) has played a pivotal role in the success of deep
learning by improving training stability, mitigating overfitting, and enabling
more effective optimization. However, its adoption in deep reinforcement
learning (DRL) has been limited due to the inherent non-i.i.d. nature of data
and the dynamically shifting distributions induced by the agent's learning
process. In this paper, we argue that, despite these challenges, BN retains
unique advantages in DRL settings, particularly through its stochasticity and
its ability to ease training. When applied appropriately, BN can adapt to
evolving data distributions and enhance both convergence speed and final
performance. To this end, we conduct a comprehensive empirical study on the use
of BN in off-policy actor-critic algorithms, systematically analyzing how
different training and evaluation modes impact performance. We further identify
failure modes that lead to instability or divergence, analyze their underlying
causes, and propose the Mode-Aware Batch Normalization (MA-BN) method with
practical actionable recommendations for robust BN integration in DRL
pipelines. We also empirically validate that, in RL settings, MA-BN accelerates
and stabilizes training, broadens the effective learning rate range, enhances
exploration, and reduces overall optimization difficulty. Our code is available
at: https://github.com/monster476/ma-bn.git.

</details>


### [363] [Anchored Supervised Fine-Tuning](https://arxiv.org/abs/2509.23753)
*He Zhu,Junyou Su,Peng Lai,Ren Ma,Wenjia Zhang,Linyi Yang,Guanhua Chen*

Main category: cs.LG

TL;DR: 文章分析了动态微调（DFT）存在的问题，提出锚定监督微调（ASFT）方法，经实验验证其优于SFT和DFT，RWR框架为理解训练后方法提供系统视角。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型训练中监督微调易记忆、强化学习计算成本高的问题，以及DFT存在的稳定性问题。

Method: 通过奖励加权回归（RWR）框架分析DFT，提出ASFT方法，即对DFT的重新加权增加轻量级KL正则化。

Result: ASFT在数学推理、医学知识基础和代码生成等任务上始终优于SFT和DFT，且计算开销小。

Conclusion: RWR框架为理解后训练方法提供了系统视角，理论分析能带来更强的保证和实际收益。

Abstract: Post-training of large language models involves a fundamental trade-off
between supervised fine-tuning (SFT), which efficiently mimics demonstrations
but tends to memorize, and reinforcement learning (RL), which achieves better
generalization at higher computational cost. Dynamic Fine-Tuning (DFT) recently
emerged as a promising middle ground, reweighting SFT objectives with token
probabilities and achieving improvements in certain reasoning domains, though
it exhibits instability in other tasks. We provide a analysis of DFT through
the reward-weighted regression (RWR) framework, revealing that it corresponds
to a specific auxiliary distribution choice that yields provably tighter RL
bounds than standard SFT. However, our analysis also uncovers a critical
limitation: this construction lacks distributional anchoring, leading to
progressive drift that undermines training stability. To address this, we
propose Anchored Supervised Fine-Tuning (ASFT), which augments DFT's
reweighting with lightweight KL regularization to preserve tightness while
ensuring stability. Empirically, ASFT consistently outperforms both SFT and DFT
across mathematical reasoning, medical knowledge grounding, and code
generation, achieving substantial improvements with minimal computational
overhead. Our RWR framework provides a systematic lens for understanding
post-training methods and demonstrates that principled theoretical analysis
leads to both stronger guarantees and practical gains.

</details>


### [364] [SHAPoint: Task-Agnostic, Efficient, and Interpretable Point-Based Risk Scoring via Shapley Values](https://arxiv.org/abs/2509.23756)
*Tomer D. Meirman,Bracha Shapira,Noa Dagan,Lior S. Rokach*

Main category: cs.LG

TL;DR: 提出SHAPoint框架，结合梯度提升树预测准确性与基于点的风险评分可解释性，性能优、运行快。


<details>
  <summary>Details</summary>
Motivation: 传统推导可解释风险评分的方法有局限性，如依赖手动预处理、特定任务建模等，限制灵活性和预测能力。

Method: 提出SHAPoint框架，结合梯度提升树和基于点的风险评分，支持多种任务，继承树模型特性。

Result: SHAPoint产生的评分紧凑且可解释，预测性能与现有最优方法相当，运行时间大幅减少。

Conclusion: SHAPoint是用于透明和可扩展风险分层的强大工具。

Abstract: Interpretable risk scores play a vital role in clinical decision support, yet
traditional methods for deriving such scores often rely on manual
preprocessing, task-specific modeling, and simplified assumptions that limit
their flexibility and predictive power. We present SHAPoint, a novel,
task-agnostic framework that integrates the predictive accuracy of gradient
boosted trees with the interpretability of point-based risk scores. SHAPoint
supports classification, regression, and survival tasks, while also inheriting
valuable properties from tree-based models, such as native handling of missing
data and support for monotonic constraints. Compared to existing frameworks,
SHAPoint offers superior flexibility, reduced reliance on manual preprocessing,
and faster runtime performance. Empirical results show that SHAPoint produces
compact and interpretable scores with predictive performance comparable to
state-of-the-art methods, but at a fraction of the runtime, making it a
powerful tool for transparent and scalable risk stratification.

</details>


### [365] [Knowledge Homophily in Large Language Models](https://arxiv.org/abs/2509.23773)
*Utkarsh Sahu,Zhisheng Qi,Mahantesh Halappanavar,Nedim Lipka,Ryan A. Rossi,Franck Dernoncourt,Yu Zhang,Yao Ma,Yu Wang*

Main category: cs.LG

TL;DR: 研究大语言模型（LLMs）知识同质性模式，提出GNN回归模型估计实体知识得分，提升微调标注效率和问答检索能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为知识密集型应用的神经知识库，但其知识的结构组织未被探索，受认知神经科学发现启发，研究其知识同质性模式。

Method: 将LLM知识映射为图表示，分析实体与其邻居的知识关系，提出GNN回归模型估计实体知识得分。

Result: 发现LLMs对图中位置更近的实体倾向于拥有相似水平的知识，预测的知识得分可优先检查不太知名的三元组。

Conclusion: 该方法提高了微调主动标注效率，增强了推理密集型问答中的多跳路径检索能力。

Abstract: Large Language Models (LLMs) have been increasingly studied as neural
knowledge bases for supporting knowledge-intensive applications such as
question answering and fact checking. However, the structural organization of
their knowledge remains unexplored. Inspired by cognitive neuroscience
findings, such as semantic clustering and priming, where knowing one fact
increases the likelihood of recalling related facts, we investigate an
analogous knowledge homophily pattern in LLMs. To this end, we map LLM
knowledge into a graph representation through knowledge checking at both the
triplet and entity levels. After that, we analyze the knowledgeability
relationship between an entity and its neighbors, discovering that LLMs tend to
possess a similar level of knowledge about entities positioned closer in the
graph. Motivated by this homophily principle, we propose a Graph Neural Network
(GNN) regression model to estimate entity-level knowledgeability scores for
triplets by leveraging their neighborhood scores. The predicted
knowledgeability enables us to prioritize checking less well-known triplets,
thereby maximizing knowledge coverage under the same labeling budget. This not
only improves the efficiency of active labeling for fine-tuning to inject
knowledge into LLMs but also enhances multi-hop path retrieval in
reasoning-intensive question answering.

</details>


### [366] [Trained Mamba Emulates Online Gradient Descent in In-Context Linear Regression](https://arxiv.org/abs/2509.23779)
*Jiarui Jiang,Wei Huang,Miao Zhang,Taiji Suzuki,Liqiang Nie*

Main category: cs.LG

TL;DR: 研究Mamba在线性回归ICL任务上的训练动态，建立收敛率和损失界，揭示其ICL机制并实验验证。


<details>
  <summary>Details</summary>
Motivation: 当前对Mamba的ICL理论理解有限，基础的线性回归ICL任务也未被充分分析，需要深入研究其潜在机制。

Method: 开发处理与Mamba结构相关的非凸优化和梯度下降的新技术。

Result: 建立了ICL解的指数收敛率，推导了与Transformer相当的损失界，揭示Mamba通过在线梯度下降学习潜在函数，机制与Transformer不同。

Conclusion: 理论结果通过实验模拟得到验证，加深了对Mamba ICL机制的理解。

Abstract: State-space models (SSMs), particularly Mamba, emerge as an efficient
Transformer alternative with linear complexity for long-sequence modeling.
Recent empirical works demonstrate Mamba's in-context learning (ICL)
capabilities competitive with Transformers, a critical capacity for large
foundation models. However, theoretical understanding of Mamba's ICL remains
limited, restricting deeper insights into its underlying mechanisms. Even
fundamental tasks such as linear regression ICL, widely studied as a standard
theoretical benchmark for Transformers, have not been thoroughly analyzed in
the context of Mamba. To address this gap, we study the training dynamics of
Mamba on the linear regression ICL task. By developing novel techniques
tackling non-convex optimization with gradient descent related to Mamba's
structure, we establish an exponential convergence rate to ICL solution, and
derive a loss bound that is comparable to Transformer's. Importantly, our
results reveal that Mamba can perform a variant of \textit{online gradient
descent} to learn the latent function in context. This mechanism is different
from that of Transformer, which is typically understood to achieve ICL through
gradient descent emulation. The theoretical results are verified by
experimental simulation.

</details>


### [367] [Visual CoT Makes VLMs Smarter but More Fragile](https://arxiv.org/abs/2509.23789)
*Chunxue Xu,Yiwei Wang,Yujun Cai,Bryan Hooi,Songze Li*

Main category: cs.LG

TL;DR: 本文首次系统评估视觉CoT在视觉扰动下的鲁棒性，发现其虽提升准确率但增加对输入扰动敏感性，确定脆弱源头并提出增强鲁棒性方法。


<details>
  <summary>Details</summary>
Motivation: 视觉CoT在视觉问答任务中表现出色，但基于视觉CoT的VLM对图像级噪声的鲁棒性尚未探索，因此开展系统评估。

Method: 对4个VQA数据集的12种图像损坏类型进行基准测试，对比使用和不使用视觉CoT的VLM；分析确定脆弱源头后，将Grounding DINO模型集成到视觉CoT管道中。

Result: 视觉CoT无论图像干净或有噪声都能提高绝对准确率，但对输入扰动更敏感，性能下降更明显；确定中间推理组件为脆弱源头。

Conclusion: 揭示了视觉CoT的脆弱模式，提出了一种有效的、与架构无关的增强视觉鲁棒性的解决方案。

Abstract: Chain-of-Thought (CoT) techniques have significantly enhanced reasoning in
Vision-Language Models (VLMs). Extending this paradigm, Visual CoT integrates
explicit visual edits, such as cropping or annotating regions of interest, into
the reasoning process, achieving superior multimodal performance. However, the
robustness of Visual CoT-based VLMs against image-level noise remains
unexplored. In this paper, we present the first systematic evaluation of Visual
CoT robustness under visual perturbations. Our benchmark spans 12 image
corruption types across 4 Visual Question Answering (VQA) datasets, enabling a
comprehensive comparison between VLMs that use Visual CoT, and VLMs that do
not. The results reveal that integrating Visual CoT consistently improves
absolute accuracy regardless of whether the input images are clean or corrupted
by noise; however, it also increases sensitivity to input perturbations,
resulting in sharper performance degradation compared to standard VLMs. Through
extensive analysis, we identify the intermediate reasoning components of Visual
CoT, i.e., the edited image patches , as the primary source of fragility.
Building on this analysis, we propose a plug-and-play robustness enhancement
method that integrates Grounding DINO model into the Visual CoT pipeline,
providing high-confidence local visual cues to stabilize reasoning. Our work
reveals clear fragility patterns in Visual CoT and offers an effective,
architecture-agnostic solution for enhancing visual robustness.

</details>


### [368] [Enhancing LLM Steering through Sparse Autoencoder-Based Vector Refinement](https://arxiv.org/abs/2509.23799)
*Anyi Wang,Xuansheng Wu,Dong Shu,Yunpu Ma,Ninghao Liu*

Main category: cs.LG

TL;DR: 提出SAE - RSV方法用稀疏自编码器精炼小数据集学习的转向向量，实验表明其优于基线方法，能从有限数据构建有效转向向量。


<details>
  <summary>Details</summary>
Motivation: 现有转向方法依赖大规模数据集，小数据集提取的转向向量含噪声特征，限制了其在现实场景的应用。

Method: 引入Refinement of Steering Vector via Sparse Autoencoder (SAE - RSV)，先根据稀疏自编码器提供的语义去除任务无关特征，再通过语义相似性丰富小数据集中缺失的任务相关特征。

Result: 所提出的SAE - RSV显著优于包括监督微调在内的所有基线方法。

Conclusion: 可以通过稀疏自编码器精炼原始转向向量，从有限训练数据构建有效的转向向量。

Abstract: Steering has emerged as a promising approach in controlling large language
models (LLMs) without modifying model parameters. However, most existing
steering methods rely on large-scale datasets to learn clear behavioral
information, which limits their applicability in many real-world scenarios. The
steering vectors extracted from small dataset often contain task-irrelevant
noising features, which degrades their effectiveness. To refine the steering
vectors learned from limited data, we introduce Refinement of Steering Vector
via Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise
and augment the steering vectors. In our framework, we first remove
task-irrelevant features according to their semantics provided by SAEs, and
then enrich task-relevant features missing from the small dataset through their
semantic similarity to the identified relevant features. Extensive experiments
demonstrate that the proposed SAE-RSV substantially outperforms all the
baseline methods including supervised fine-tuning. Our findings show that
effective steering vector can be constructed from limited training data by
refining the original steering vector through SAEs.

</details>


### [369] [STAIR: Addressing Stage Misalignment through Temporal-Aligned Preference Reinforcement Learning](https://arxiv.org/abs/2509.23802)
*Yao Luan,Ni Mu,Yiqin Yang,Bo Xu,Qing-Shan Jia*

Main category: cs.LG

TL;DR: 论文指出基于偏好的强化学习（PbRL）在多阶段任务中存在阶段不对齐问题，提出 STAIR 方法解决该问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决 PbRL 在多阶段任务中因阶段不对齐导致反馈无信息、阻碍策略学习的问题。

Method: 提出 STAIR 方法，先基于时间距离学习阶段近似，再优先进行同阶段比较，通过对比学习学习时间距离。

Result: 广泛实验表明 STAIR 在多阶段任务中表现优越，单阶段任务有竞争力，人体研究显示其阶段近似与人类认知一致。

Conclusion: STAIR 能有效缓解多阶段任务中的阶段不对齐问题。

Abstract: Preference-based reinforcement learning (PbRL) bypasses complex reward
engineering by learning rewards directly from human preferences, enabling
better alignment with human intentions. However, its effectiveness in
multi-stage tasks, where agents sequentially perform sub-tasks (e.g.,
navigation, grasping), is limited by stage misalignment: Comparing segments
from mismatched stages, such as movement versus manipulation, results in
uninformative feedback, thus hindering policy learning. In this paper, we
validate the stage misalignment issue through theoretical analysis and
empirical experiments. To address this issue, we propose STage-AlIgned Reward
learning (STAIR), which first learns a stage approximation based on temporal
distance, then prioritizes comparisons within the same stage. Temporal distance
is learned via contrastive learning, which groups temporally close states into
coherent stages, without predefined task knowledge, and adapts dynamically to
policy changes. Extensive experiments demonstrate STAIR's superiority in
multi-stage tasks and competitive performance in single-stage tasks.
Furthermore, human studies show that stages approximated by STAIR are
consistent with human cognition, confirming its effectiveness in mitigating
stage misalignment.

</details>


### [370] [Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR](https://arxiv.org/abs/2509.23808)
*Fanding Huang,Guanbo Huang,Xiao Fan,Yi He,Xiao Liang,Xiao Chen,Qinting Jiang,Faisal Nadeem Khan,Jingyan Jiang,Zhi Wang*

Main category: cs.LG

TL;DR: 文章重新审视强化学习可验证奖励中的探索-利用权衡视角，提出在隐藏状态层面二者可解耦，据此提出VERL方法并实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 重新审视RLVR中基于探索-利用权衡的主流观点，认为其可能是测量层面的假象，探索在隐藏状态空间中探索与利用的关系以同时提升二者能力。

Method: 将分析转移到语义丰富的隐藏状态空间，用有效秩量化探索，提出有效秩速度和加速度捕捉利用动态，提出VERL方法，利用ERA作为预测元控制器创建双渠道激励结构。

Result: 实验在不同大语言模型和推理基准测试中取得一致增益，在高考2024数据集上绝对准确率提升达21.4%。

Conclusion: 在隐藏状态层面探索和利用可解耦，VERL方法能同时提升探索和利用能力，避免权衡。

Abstract: A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR)
interprets recent progress through the lens of an exploration-exploitation
trade-off, a perspective largely shaped by token-level metrics. We re-examine
this perspective, proposing that this perceived trade-off may not be a
fundamental constraint but rather an artifact of the measurement level. To
investigate this, we shift the analysis to the semantically rich hidden-state
space, adopting Effective Rank (ER) to quantify exploration and proposing its
novel first- and second-order derivatives, named Effective Rank Velocity (ERV)
and Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our
analysis reveals that at the hidden-state level, exploration and exploitation
could be decoupled (Sec. 4). This finding reveals an opportunity to enhance
both capacities simultaneously. This insight motivates our method,
Velocity-Exploiting Rank-Learning (VERL), the first to operationalize the
principle of synergistic exploration-exploitation enhancement by directly
shaping the RL advantage function. The key innovation is leveraging the
theoretically stable ERA as a predictive meta-controller to create a
synergistic, dual-channel incentive structure. Instead of forcing a trade-off,
VERL prospectively amplifies rewards for exploration to preempt overconfidence
and reinforces exploitative gains to consolidate reasoning. Experiments across
diverse LLMs and reasoning benchmarks show consistent gains, including up to
21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.

</details>


### [371] [Tequila: Trapping-free Ternary Quantization for Large Language Models](https://arxiv.org/abs/2509.23809)
*Hong Huang,Decheng Wu,Rui Cen,Guanghua Yu,Zonghang Li,Kai Liu,Jianchen Zhu,Peng Chen,Xue Liu,Dapeng Wu*

Main category: cs.LG

TL;DR: 文章指出现有大语言模型量化方法不足，提出Tequila方法，评估显示其优于SOTA方法，适合资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法依赖缺乏硬件支持的混合精度乘法或存在严重精度损失问题，需解决权重死区陷阱问题。

Method: 提出Tequila，一种无陷阱量化优化方法，将死区权重重新用作动态偏差。

Result: Tequila在五个基准测试中优于SOTA三元量化方法，在ARC基准上精度提升超4%，推理速度提升3.0倍。

Conclusion: Tequila为资源受限环境下部署高级大语言模型提供高效实现方案。

Abstract: Quantization techniques are essential for the deployment of Large Language
Models (LLMs) on edge devices. However, prevailing methods often rely on
mixed-precision multiplication that lacks efficient hardware support, making it
not feasible. Ternary weight quantization addresses this by constraining
weights to {-1, 0, 1}, replacing expensive multiplications with
hardware-efficient additions. However, such aggressive compression leads to
significant accuracy degradation, even after costly quantization-aware training
with massive data. We identify the core issue as deadzone trapping: a large
number of weights are trapped at the deadzone boundary. This occurs because
these weights receive only noisy, uninformative gradients, preventing stable
escape from the deadzone and severely impeding model capacity and optimization.
To address this issue, we propose Tequila, a trapping-free quantization
optimization method that reactivates deadzone-trapped weights by repurposing
them as dynamic biases. This allows the repurposed weights to provide a
continuous signal in the forward pass and, critically, receive direct,
meaningful gradient signals during backpropagation, thereby enhancing model
capacity and optimization with nearly zero inference overhead. Extensive
evaluations demonstrate that Tequila outperforms state-of-the-art (SOTA)
ternary quantization methods across five benchmarks. Specifically, on the ARC
benchmark, it achieves >4% accuracy gain over the SOTA baseline, nearly
matching full-precision performance (within <1% gap) with a 3.0x inference
speedup. Consequently, Tequila offers a highly practical and efficient
implementation for the deployment of advanced LLMs in resource-constrained
environments. The code is available at https://github.com/Tencent/AngelSlim.

</details>


### [372] [IndexNet: Timestamp and Variable-Aware Modeling for Time Series Forecasting](https://arxiv.org/abs/2509.23813)
*Beiliang Wu,Peiyuan Liu,Yifan Hu,Luyan Zhang,Ao Hu,Zenglin Xu*

Main category: cs.LG

TL;DR: 提出基于MLP的IndexNet框架用于多变量时间序列预测，实验证明其有效性、通用性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视索引相关描述信息，为挖掘此类信息潜力并利用MLP架构周期性捕获能力。

Method: 提出IndexNet框架，含Timestamp Embedding和Channel Embedding模块。

Result: 在12个真实数据集上性能与主流基线相当，具备强通用性和可解释性。

Conclusion: IndexNet设计有效，对多变量时间序列预测有积极作用。

Abstract: Multivariate time series forecasting (MTSF) plays a vital role in a wide
range of real-world applications, such as weather prediction and traffic flow
forecasting. Although recent advances have significantly improved the modeling
of temporal dynamics and inter-variable dependencies, most existing methods
overlook index-related descriptive information, such as timestamps and variable
indices, which carry rich contextual semantics. To unlock the potential of such
information and take advantage of the lightweight and powerful periodic capture
ability of MLP-based architectures, we propose IndexNet, an MLP-based framework
augmented with an Index Embedding (IE) module. The IE module consists of two
key components: Timestamp Embedding (TE) and Channel Embedding (CE).
Specifically, TE transforms timestamps into embedding vectors and injects them
into the input sequence, thereby improving the model's ability to capture
long-term complex periodic patterns. In parallel, CE assigns each variable a
unique and trainable identity embedding based on its index, allowing the model
to explicitly distinguish between heterogeneous variables and avoid homogenized
predictions when input sequences seem close. Extensive experiments on 12
diverse real-world datasets demonstrate that IndexNet achieves comparable
performance across mainstream baselines, validating the effectiveness of our
temporally and variably aware design. Moreover, plug-and-play experiments and
visualization analyses further reveal that IndexNet exhibits strong generality
and interpretability, two aspects that remain underexplored in current MTSF
research.

</details>


### [373] [Test-time GNN Model Evaluation on Dynamic Graphs](https://arxiv.org/abs/2509.23816)
*Bo Li,Xin Zheng,Ming Jin,Can Wang,Shirui Pan*

Main category: cs.LG

TL;DR: 本文提出DGNN模型评估问题，设计DyGEval解决该问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 动态图数据分布随时间变化，训练好的DGNN在实际部署时性能有不确定性，需在测试时评估其性能。

Method: 提出DyGEval，包含测试时动态图模拟和DyGEval开发训练两个阶段。

Result: 实验表明DyGEval能有效评估不同动态图和DGNN骨干模型。

Conclusion: DyGEval可作为评估分布偏移下各种DGNN骨干模型的有效评估器。

Abstract: Dynamic graph neural networks (DGNNs) have emerged as a leading paradigm for
learning from dynamic graphs, which are commonly used to model real-world
systems and applications. However, due to the evolving nature of dynamic graph
data distributions over time, well-trained DGNNs often face significant
performance uncertainty when inferring on unseen and unlabeled test graphs in
practical deployment. In this case, evaluating the performance of deployed
DGNNs at test time is crucial to determine whether a well-trained DGNN is
suited for inference on an unseen dynamic test graph. In this work, we
introduce a new research problem: DGNN model evaluation, which aims to assess
the performance of a specific DGNN model trained on observed dynamic graphs by
estimating its performance on unseen dynamic graphs during test time.
Specifically, we propose a Dynamic Graph neural network Evaluator, dubbed
DyGEval, to address this new problem. The proposed DyGEval involves a two-stage
framework: (1) test-time dynamic graph simulation, which captures the
training-test distributional differences as supervision signals and trains an
evaluator; and (2) DyGEval development and training, which accurately estimates
the performance of the well-trained DGNN model on the test-time dynamic graphs.
Extensive experiments demonstrate that the proposed DyGEval serves as an
effective evaluator for assessing various DGNN backbones across different
dynamic graphs under distribution shifts.

</details>


### [374] [Space Group Conditional Flow Matching](https://arxiv.org/abs/2509.23822)
*Omri Puny,Yaron Lipman,Benjamin Kurt Miller*

Main category: cs.LG

TL;DR: 介绍Space Group Conditional Flow Matching生成框架，考虑晶体对称性约束，在晶体结构预测和从头生成基准测试中取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 大多数预测原子坐标的生成模型忽略了晶体的对称性约束，导致生成的晶体对称性有限。

Method: 提出Space Group Conditional Flow Matching框架，在生成过程中以给定的空间群和Wyckoff位置为条件，定义条件对称噪声基分布和群条件等变参数向量场，用针对对称晶体的高效群平均重新公式化实现群条件等变性。

Result: 在晶体结构预测和从头生成基准测试中取得了最优结果，并进行了相关消融实验。

Conclusion: 所提出的框架能有效考虑晶体对称性约束，生成更接近目标群体的高对称、稳定晶体。

Abstract: Inorganic crystals are periodic, highly-symmetric arrangements of atoms in
three-dimensional space. Their structures are constrained by the symmetry
operations of a crystallographic \emph{space group} and restricted to lie in
specific affine subspaces known as \emph{Wyckoff positions}. The frequency an
atom appears in the crystal and its rough positioning are determined by its
Wyckoff position. Most generative models that predict atomic coordinates
overlook these symmetry constraints, leading to unrealistically high
populations of proposed crystals exhibiting limited symmetry. We introduce
Space Group Conditional Flow Matching, a novel generative framework that
samples significantly closer to the target population of highly-symmetric,
stable crystals. We achieve this by conditioning the entire generation process
on a given space group and set of Wyckoff positions; specifically, we define a
conditionally symmetric noise base distribution and a group-conditioned,
equivariant, parametric vector field that restricts the motion of atoms to
their initial Wyckoff position. Our form of group-conditioned equivariance is
achieved using an efficient reformulation of \emph{group averaging} tailored
for symmetric crystals. Importantly, it reduces the computational overhead of
symmetrization to a negligible level. We achieve state of the art results on
crystal structure prediction and de novo generation benchmarks. We also perform
relevant ablations.

</details>


### [375] [Electric Currents for Discrete Data Generation](https://arxiv.org/abs/2509.23825)
*Alexander Kolesov,Stepan Manukhov,Vladimir V. Palyulin,Alexander Korotin*

Main category: cs.LG

TL;DR: 提出ECD²G方法用于离散数据生成，基于电路电流类比，用神经网络学习电流表示概率流，证明可实现数据分布转移并做概念验证实验。


<details>
  <summary>Details</summary>
Motivation: 解决离散环境下的数据生成问题。

Method: 将电路中电流流动与数据分布间概率质量转移类比，用神经网络学习电流表示概率流，从源分布采样并按学习的电流沿电路路径传输样本。

Result: 该方法可保证数据分布间的转移。

Conclusion: ECD²G方法在离散数据生成方面具有可行性和有效性。

Abstract: We propose $\textbf{E}$lectric $\textbf{C}$urrent $\textbf{D}$iscrete
$\textbf{D}$ata $\textbf{G}$eneration (ECD$^{2}$G), a pioneering method for
data generation in discrete settings that is grounded in electrical engineering
theory. Our approach draws an analogy between electric current flow in a
circuit and the transfer of probability mass between data distributions. We
interpret samples from the source distribution as current input nodes of a
circuit and samples from the target distribution as current output nodes. A
neural network is then used to learn the electric currents to represent the
probability flow in the circuit. To map the source distribution to the target,
we sample from the source and transport these samples along the circuit
pathways according to the learned currents. This process provably guarantees
transfer between data distributions. We present proof-of-concept experiments to
illustrate our ECD$^{2}$G method.

</details>


### [376] [Adversarial Diffusion for Robust Reinforcement Learning](https://arxiv.org/abs/2509.23846)
*Daniele Foffano,Alessio Russo,Alexandre Proutiere*

Main category: cs.LG

TL;DR: 本文利用扩散模型训练强化学习（RL）的鲁棒策略，提出AD - RRL方法，经实验证明该方法比现有鲁棒RL方法有更优的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中对建模误差和不确定性的鲁棒性挑战。

Method: 利用扩散模型的条件采样学习对环境动态不确定性有鲁棒性的策略，基于CVaR优化与鲁棒RL的联系，提出AD - RRL方法，在训练中引导扩散过程生成最坏情况轨迹以优化累积回报的CVaR。

Result: 在标准基准测试中，AD - RRL比现有鲁棒RL方法实现了更优的鲁棒性和性能。

Conclusion: AD - RRL方法在解决强化学习鲁棒性问题上是有效的，优于现有方法。

Abstract: Robustness to modeling errors and uncertainties remains a central challenge
in reinforcement learning (RL). In this work, we address this challenge by
leveraging diffusion models to train robust RL policies. Diffusion models have
recently gained popularity in model-based RL due to their ability to generate
full trajectories "all at once", mitigating the compounding errors typical of
step-by-step transition models. Moreover, they can be conditioned to sample
from specific distributions, making them highly flexible. We leverage
conditional sampling to learn policies that are robust to uncertainty in
environment dynamics. Building on the established connection between
Conditional Value at Risk (CVaR) optimization and robust RL, we introduce
Adversarial Diffusion for Robust Reinforcement Learning (AD-RRL). AD-RRL guides
the diffusion process to generate worst-case trajectories during training,
effectively optimizing the CVaR of the cumulative return. Empirical results
across standard benchmarks show that AD-RRL achieves superior robustness and
performance compared to existing robust RL methods.

</details>


### [377] [Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation](https://arxiv.org/abs/2509.23866)
*Pengxiang Li,Zechen Hu,Zirui Shang,Jingrong Wu,Yang Liu,Hui Liu,Zhi Gao,Chenrui Shi,Bofei Zhang,Zihao Zhang,Xiaochuan Shi,Zedong YU,Yuwei Wu,Xinxiao Wu,Yunde Jia,Liuyu Xiang,Zhaofeng He,Qing Li*

Main category: cs.LG

TL;DR: 本文提出DART框架解决基于VLM的GUI智能体在强化学习应用中的挑战，分离训练系统为四个异步模块，引入自适应数据管理方案，提升系统效率和任务成功率，并将开源。


<details>
  <summary>Details</summary>
Motivation: 基于VLM的GUI智能体在应用强化学习时面临多轮交互慢和高质量交互不足的挑战。

Method: 提出DART框架，将训练系统分为环境集群、滚动服务、数据管理器和训练器四个异步模块；引入自适应数据管理方案。

Result: 实现1.6倍的滚动GPU利用率、1.9倍的训练吞吐量和5.5倍的环境利用率；DART - GUI - 7B在OSWorld基准上任务成功率达42.13%，比基础模型提高14.61%，比开源SOTA高7.34%。

Conclusion: DART框架能有效解决GUI智能体强化学习的挑战，开源工作将为智能体强化学习训练开源社区做贡献。

Abstract: Vision-language model (VLM) based GUI agents show promise for automating
complex desktop and mobile tasks, but face significant challenges in applying
reinforcement learning (RL): (1) slow multi-turn interactions with GUI
environments for policy rollout, and (2) insufficient high-quality
agent-environment interactions for policy learning. To address these
challenges, we propose DART, a Decoupled Agentic RL Training framework for GUI
agents, which coordinates heterogeneous modules in a highly decoupled manner.
DART separates the training system into four asynchronous modules: environment
cluster, rollout service, data manager, and trainer. This design enables
non-blocking communication, asynchronous training, rollout-wise trajectory
sampling, and per-worker model synchronization, significantly improving the
system efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput,
and 5.5* environment utilization. To facilitate effective learning from
abundant samples, we introduce an adaptive data curation scheme: (1)
pre-collecting successful trajectories for challenging tasks to supplement
sparse success in online sampling; (2) dynamically adjusting rollout numbers
and trajectory lengths based on task difficulty; (3) training selectively on
high-entropy steps to prioritize critical decisions; (4) stabilizing learning
via truncated importance sampling for policy mismatch between policy rollout
and updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task
success rate, a 14.61% absolute gain over the base model, and 7.34% higher than
open-source SOTA. We will fully open-source our training framework, data, and
model checkpoints via computer-use-agents.github.io/dart-gui, which we believe
is a timely contribution to the open-source community of agentic RL training.

</details>


### [378] [Towards Understanding Subliminal Learning: When and How Hidden Biases Transfer](https://arxiv.org/abs/2509.23886)
*Simon Schrodi,Elias Kempf,Fazl Barez,Thomas Brox*

Main category: cs.LG

TL;DR: 研究语言模型蒸馏中的潜意识学习现象，发现其不依赖全局标记纠缠或对数几率泄漏，关键在于分歧标记，且该现象很脆弱。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在硬蒸馏下潜意识学习何时及如何发生的问题。

Method: 通过受控实验和机制分析。

Result: 潜意识学习不依赖全局标记纠缠或对数几率泄漏，关键是分歧标记；早期层很关键，微调单个早期层就足以实现潜意识学习；潜意识学习很脆弱。

Conclusion: 揭示了语言模型潜意识学习的机制和特点，可通过掩盖分歧标记减少隐藏偏差转移。

Abstract: Language models can transfer hidden biases during distillation. For example,
a teacher that "likes owls" can make its student "like owls" too, even when the
training data consists only of lists of numbers. This surprising phenomenon is
called subliminal learning. Subliminal learning can be expected under soft
distillation, where the student is trained on the teacher's full next-token
distribution. But the fact that this also occurs under hard distillation-where
the student only sees sampled tokens-raises a deeper question: when and how
does subliminal learning actually occur? We answer this question through
controlled experiments and mechanistic analysis. Our results show that
subliminal learning does not need (global) token entanglement or logit leakage.
Instead, it comes down to a small set of divergence tokens-rare cases where
teachers with different biases would predict different tokens. Masking out
these tokens mostly removes the hidden bias transfer. Mechanistically,
divergence tokens reveal that early layers are critical. Surprisingly,
finetuning even a single such early layer is sufficient for subliminal
learning. Finally, we find that subliminal learning is fragile. Even small
changes, like paraphrasing prompts, are usually sufficient to suppress it.

</details>


### [379] [Gradient Flow Convergence Guarantee for General Neural Network Architectures](https://arxiv.org/abs/2509.23887)
*Yash Jakhmola*

Main category: cs.LG

TL;DR: 本文给出了采用分段非零多项式激活函数、ReLU或sigmoid激活函数的神经网络在连续梯度下降（梯度流）训练时线性收敛的统一证明，理论结果与实践有良好吻合。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习理论面临的一个关键挑战是解释基于梯度的优化方法在训练大规模复杂深度神经网络时显著成功的原因，且缺少统一理论。

Method: 为采用分段非零多项式激活函数、ReLU或sigmoid激活函数的神经网络的连续梯度下降（梯度流）训练给出统一证明。

Result: 得出一个通用定理，不仅涵盖此前未知结果的架构，还在更弱假设下整合了现有结果，理论预测与实际梯度下降方法结果有良好吻合。

Conclusion: 给出了连续梯度下降（梯度流）训练神经网络线性收敛的统一证明，理论与实践相符。

Abstract: A key challenge in modern deep learning theory is to explain the remarkable
success of gradient-based optimization methods when training large-scale,
complex deep neural networks. Though linear convergence of such methods has
been proved for a handful of specific architectures, a united theory still
evades researchers. This article presents a unified proof for linear
convergence of continuous gradient descent, also called gradient flow, while
training any neural network with piecewise non-zero polynomial activations or
ReLU, sigmoid activations. Our primary contribution is a single, general
theorem that not only covers architectures for which this result was previously
unknown but also consolidates existing results under weaker assumptions. While
our focus is theoretical and our results are only exact in the infinitesimal
step size limit, we nevertheless find excellent empirical agreement between the
predictions of our result and those of the practical step-size gradient descent
method.

</details>


### [380] [Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic Forgettings](https://arxiv.org/abs/2509.23893)
*Zhixin Zhang,Zeming Wei,Meng Sun*

Main category: cs.LG

TL;DR: 本文揭示微调中功能方向漂移致现有正则化方法在大语言模型持续学习中失效，提出DOC微调方法，实验显示其优于先前方法，减少灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型持续学习中灾难性遗忘问题，现有正则化方法在长期持续学习中失效。

Method: 提出Dynamic Orthogonal Continual (DOC) 微调方法，跟踪功能方向漂移并动态更新，调整新任务参数梯度与历史功能方向正交。

Result: 在各种大语言模型持续学习基准测试中，该方法优于先前方法，有效减少灾难性遗忘。

Conclusion: DOC微调方法是用于大语言模型持续微调的强大工具，代码开源。

Abstract: Catastrophic forgetting remains a critical challenge in continual learning
for large language models (LLMs), where models struggle to retain performance
on historical tasks when fine-tuning on new sequential data without access to
past datasets. In this paper, we first reveal that the drift of functional
directions during the fine-tuning process is a key reason why existing
regularization-based methods fail in long-term LLM continual learning. To
address this, we propose Dynamic Orthogonal Continual (DOC) fine-tuning, a
novel approach that tracks the drift of these functional directions and
dynamically updates them during the fine-tuning process. Furthermore, by
adjusting the gradients of new task parameters to be orthogonal to the tracked
historical function directions, our method mitigates interference between new
and old tasks. Extensive experiments on various LLM continual learning
benchmarks demonstrate that this approach outperforms prior methods,
effectively reducing catastrophic forgetting and providing a robust tool for
continuous LLM fine-tuning. Our code is available at
https://github.com/meloxxxxxx/DOC.

</details>


### [381] [Integrated Communication and Control for Energy-Efficient UAV Swarms: A Multi-Agent Reinforcement Learning Approach](https://arxiv.org/abs/2509.23905)
*Tianjiao Sun,Ningyan Guo,Haozhe Gu,Yanyan Peng,Zhiyong Feng*

Main category: cs.LG

TL;DR: 本文提出集成通信与控制协同设计机制，用多智能体强化学习框架和新算法优化无人机群通信，实验显示能达公平指数0.99并降能耗25%。


<details>
  <summary>Details</summary>
Motivation: 复杂地理环境使无人机群辅助移动通信的空对地链路常中断，限制服务可靠性和质量，需提升通信质量。

Method: 将联合资源分配和3D轨迹控制问题建模为马尔可夫决策过程，开发多智能体强化学习框架，提出多智能体混合近端策略优化带动作掩码算法。

Result: 相比基线方法，实现公平指数0.99，降低能耗达25%。

Conclusion: 所提方法能在复杂地理环境中优化无人机群辅助通信，平衡用户通信速率和无人机能耗，提升能效。

Abstract: The deployment of unmanned aerial vehicle (UAV) swarm-assisted communication
networks has become an increasingly vital approach for remediating coverage
limitations in infrastructure-deficient environments, with especially pressing
applications in temporary scenarios, such as emergency rescue, military and
security operations, and remote area coverage. However, complex geographic
environments lead to unpredictable and highly dynamic wireless channel
conditions, resulting in frequent interruptions of air-to-ground (A2G) links
that severely constrain the reliability and quality of service in UAV
swarm-assisted mobile communications. To improve the quality of UAV
swarm-assisted communications in complex geographic environments, we propose an
integrated communication and control co-design mechanism. Given the stringent
energy constraints inherent in UAV swarms, our proposed mechanism is designed
to optimize energy efficiency while maintaining an equilibrium between
equitable communication rates for mobile ground users (GUs) and UAV energy
expenditure. We formulate the joint resource allocation and 3D trajectory
control problem as a Markov decision process (MDP), and develop a multi-agent
reinforcement learning (MARL) framework to enable real-time coordinated actions
across the UAV swarm. To optimize the action policy of UAV swarms, we propose a
novel multi-agent hybrid proximal policy optimization with action masking
(MAHPPO-AM) algorithm, specifically designed to handle complex hybrid action
spaces. The algorithm incorporates action masking to enforce hard constraints
in high-dimensional action spaces. Experimental results demonstrate that our
approach achieves a fairness index of 0.99 while reducing energy consumption by
up to 25% compared to baseline methods.

</details>


### [382] [Graph Mixing Additive Networks](https://arxiv.org/abs/2509.23923)
*Maya Bechler-Speicher,Andrea Zerio,Maor Huri,Marie Vibeke Vestergaard,Ran Gilad-Bachrach,Tine Jess,Samir Bhatt,Aleksejs Sazonovs*

Main category: cs.LG

TL;DR: 提出GMAN框架，扩展GNAN以处理稀疏时间序列数据，在真实数据集上表现优于黑盒基线并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 需要一个灵活、可解释且有表现力的框架来处理稀疏时间序列数据。

Method: 将每个时间相关轨迹表示为有向图，对每个图应用增强的GNAN，通过分组特征和图来控制可解释性与表现力的权衡。

Result: 在包括血液测试死亡率预测和假新闻检测等真实数据集上，GMAN优于强不可解释的黑盒基线。

Conclusion: GMAN是一个有效的处理稀疏时间序列数据的框架，能提供可操作、与领域对齐的解释。

Abstract: We introduce GMAN, a flexible, interpretable, and expressive framework that
extends Graph Neural Additive Networks (GNANs) to learn from sets of sparse
time-series data. GMAN represents each time-dependent trajectory as a directed
graph and applies an enriched, more expressive GNAN to each graph. It allows
users to control the interpretability-expressivity trade-off by grouping
features and graphs to encode priors, and it provides feature, node, and
graph-level interpretability. On real-world datasets, including mortality
prediction from blood tests and fake-news detection, GMAN outperforms strong
non-interpretable black-box baselines while delivering actionable,
domain-aligned explanations.

</details>


### [383] [HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models](https://arxiv.org/abs/2509.23928)
*Zhinan Xie,Peisong Wang,Jian Cheng*

Main category: cs.LG

TL;DR: 针对视觉语言模型（VLM）的推测解码存在低效问题，提出HiViS框架，压缩草稿器预填充序列长度，保持无损生成质量，实验证明可加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法应用于VLM时，因视觉和文本多模态输入导致语义表示不一致和自注意力计算慢，需要解决推理效率问题。

Method: 提出HiViS框架，移除草稿器输入中的视觉标记，重用目标VLM对应最后一层隐藏状态作为隐式视觉信息；引入多步自反馈训练策略，包括动态数据选择和顺序嵌入监督。

Result: 将草稿器的预填充序列长度压缩至目标VLM输入的0.7%-1.3%，保持无损生成质量，实验实现高达2.65倍的加速。

Conclusion: HiViS框架能有效加速VLM推理。

Abstract: Speculative decoding is an effective approach for accelerating inference in
Large Language models (LLMs), but its adaptation to Vision-Language models
(VLMs) remains challenging for additional visual tokens in multimodal inputs.
First, owing to the fact that the drafter and the target VLM may derived from
different families, the semantic representations of visual tokens in the target
VLM are misaligned with those in the drafter, introducing bias into the
KV-cache during the prefill stage. Second, the large number of visual tokens
substantially slows down the drafter's self-attention during the decoding
stage. We propose Hiding Visual Tokens from the Drafter for Speculative
Decoding in Vision-Language Models (HiViS), an explicit-implicit input
decomposition framework that alleviates the above inefficiency. All visual
tokens are removed from the drafter's input, retaining only textual tokens as
explicit inputs, while directly reusing the target VLM's corresponding
last-layer hidden states as implicit visual information without additional
processing. To train the drafter efficiently, we introduces multi-step
self-feedback training strategy with dynamic data selection and sequential
embedding supervision to simulate reasoning during training. Our approach
compresses the prefill sequence length of the drafter to only 0.7%-1.3% of the
target VLM's input, while maintaining lossless generation quality. Extensive
experiments across diverse models and tasks demonstrate up to 2.65x speedup,
confirming the effectiveness of HiViS in accelerating VLM inference.

</details>


### [384] [Beyond Benchmarks: Understanding Mixture-of-Experts Models through Internal Mechanisms](https://arxiv.org/abs/2509.23933)
*Jiahao Ying,Mingbao Lin,Qianru Sun,Yixin Cao*

Main category: cs.LG

TL;DR: 本文用内部指标研究MoE架构机制，分析公开模型有多项发现，显示MUI可作为基准性能补充指标。


<details>
  <summary>Details</summary>
Motivation: 当前MoE架构研究多以性能为中心，对内部机制理解有限，制约进一步发展。

Method: 使用内部指标，结合路由机制，分析专家级行为，系统分析大量公开MoE模型。

Result: 发现神经元利用率随模型发展降低、训练有动态轨迹、任务由多专家协作完成、神经元激活模式反映数据多样性。

Conclusion: MUI可作为基准性能的补充指标，为MoE模型的容量、动态和专业化提供新见解。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising direction,
offering efficiency and scalability by activating only a subset of parameters
during inference. However, current research remains largely
performance-centric, with limited understanding of its internal mechanisms,
thereby constraining broader progress. In this work, we use an internal metric
to investigate the mechanisms of MoE architecture by explicitly incorporating
routing mechanisms and analyzing expert-level behaviors. Through systematic
analyses of a wide range of publicly available MoE models, we uncover several
findings: (1) neuron utilization decreases as models evolve, reflecting
stronger generalization; (2) training exhibits a dynamic trajectory, where
benchmark performance alone provides limited signal while MUI reveals deeper
insights; (3) task completion emerges from collaborative contributions of
multiple experts, with shared experts driving concentration; and (4) activation
patterns at the neuron level provide a fine-grained proxy for data diversity.
Together, these results demonstrate the potential of MUI as a complementary
indicator to benchmark performance, offering new insights into the capacity,
dynamics, and specialization of MoE models. Our project can be found at
https://yingjiahao14.github.io/MoE-MUI/.

</details>


### [385] [Diffusion Models are Kelly Gamblers](https://arxiv.org/abs/2509.23937)
*Akhil Premkumar*

Main category: cs.LG

TL;DR: 建立扩散模型与凯利准则联系，指出条件扩散模型存储信息，无分类器引导可提升互信息，还指出扩散模型是无限深自编码器观点的细微差别并关联降噪损失与费米黄金规则。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型与凯利准则的联系，研究条件扩散模型存储信息及无分类器引导对互信息的影响，指出扩散模型相关观点的细微差别。

Method: 通过理论分析建立扩散模型与凯利准则联系，研究条件扩散模型存储信息情况及无分类器引导作用，关联降噪损失与量子力学规则。

Result: 发现条件扩散模型存储信息等于信号与条件信息的互信息，无分类器引导能有效提升互信息，指出扩散模型是无限深自编码器观点的细微差别并关联降噪损失与费米黄金规则。

Conclusion: 建立了扩散模型与凯利准则的联系，明确了无分类器引导对提升互信息的作用，完善了对扩散模型相关观点的认识。

Abstract: We draw a connection between diffusion models and the Kelly criterion for
maximizing returns in betting games. We find that conditional diffusion models
store additional information to bind the signal $X$ with the conditioning
information $Y$, equal to the mutual information between them. Classifier-free
guidance effectively boosts the mutual information between $X$ and $Y$ at
sampling time. This is especially helpful in image models, since the mutual
information between images and their labels is low, a fact which is intimately
connected to the manifold hypothesis. Finally, we point out some nuances in the
popular perspective that diffusion models are infinitely deep autoencoders. In
doing so, we relate the denoising loss to the Fermi Golden Rule from quantum
mechanics.

</details>


### [386] [Brain-language fusion enables interactive neural readout and in-silico experimentation](https://arxiv.org/abs/2509.23941)
*Victoria Bosch,Daniel Anthes,Adrien Doerig,Sushrut Thorat,Peter König,Tim Christian Kietzmann*

Main category: cs.LG

TL;DR: 提出CorText框架，将神经活动集成到LLM潜空间，实现与脑数据自然语言交互，有良好表现并推动脑活动与语言接口发展。


<details>
  <summary>Details</summary>
Motivation: 现有神经解码受静态、非交互方法限制，需改进以实现与脑数据更好交互。

Method: 提出CorText框架，在观看自然场景的fMRI数据上训练。

Result: CorText能生成准确图像描述，比对照组更好回答详细问题，实现零样本泛化，可进行反事实分析。

Conclusion: 这些进展使脑活动与语言接口从被动解码转向生成式、灵活交互。

Abstract: Large language models (LLMs) have revolutionized human-machine interaction,
and have been extended by embedding diverse modalities such as images into a
shared language space. Yet, neural decoding has remained constrained by static,
non-interactive methods. We introduce CorText, a framework that integrates
neural activity directly into the latent space of an LLM, enabling open-ended,
natural language interaction with brain data. Trained on fMRI data recorded
during viewing of natural scenes, CorText generates accurate image captions and
can answer more detailed questions better than controls, while having access to
neural data only. We showcase that CorText achieves zero-shot generalization
beyond semantic categories seen during training. Furthermore, we present a
counterfactual analysis that emulates in-silico cortical microstimulation.
These advances mark a shift from passive decoding toward generative, flexible
interfaces between brain activity and language.

</details>


### [387] [DiBS-MTL: Transformation-Invariant Multitask Learning with Direction Oracles](https://arxiv.org/abs/2509.23948)
*Surya Murthy,Kushagra Gupta,Mustafa O. Karabag,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: 本文研究多任务学习（MTL）中DiBS算法，证明其在非凸MTL下收敛性，提出DiBS - MTL并验证其性能。


<details>
  <summary>Details</summary>
Motivation: 现有MTL算法因任务损失非仿射缩放导致任务主导训练和性能下降，且DiBS在非凸MTL下的收敛行为未知。

Method: 证明在标准假设下，DiBS迭代子序列在任务损失可能非凸时收敛到帕累托稳定点，提出DiBS - MTL。

Result: 在标准MTL基准上验证DiBS - MTL，它与现有方法竞争且对非仿射单调变换有鲁棒性。

Conclusion: DiBS - MTL在MTL中表现良好，对非仿射变换有鲁棒性，可作为MTL的有效方法。

Abstract: Multitask learning (MTL) algorithms typically rely on schemes that combine
different task losses or their gradients through weighted averaging. These
methods aim to find Pareto stationary points by using heuristics that require
access to task loss values, gradients, or both. In doing so, a central
challenge arises because task losses can be arbitrarily, nonaffinely scaled
relative to one another, causing certain tasks to dominate training and degrade
overall performance. A recent advance in cooperative bargaining theory, the
Direction-based Bargaining Solution (DiBS), yields Pareto stationary solutions
immune to task domination because of its invariance to monotonic nonaffine task
loss transformations. However, the convergence behavior of DiBS in nonconvex
MTL settings is currently not understood. To this end, we prove that under
standard assumptions, a subsequence of DiBS iterates converges to a Pareto
stationary point when task losses are possibly nonconvex, and propose DiBS-MTL,
a computationally efficient adaptation of DiBS to the MTL setting. Finally, we
validate DiBS-MTL empirically on standard MTL benchmarks, showing that it
achieves competitive performance with state-of-the-art methods while
maintaining robustness to nonaffine monotonic transformations that
significantly degrade the performance of existing approaches, including prior
bargaining-inspired MTL methods. Code available at
https://github.com/suryakmurthy/dibs-mtl.

</details>


### [388] [Evaluating the Robustness of Chinchilla Compute-Optimal Scaling](https://arxiv.org/abs/2509.23963)
*Rylan Schaeffer,Noam Levi,Andreas Kirsch,Theo Guenais,Brando Miranda,Elyas Obbad,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 本文探讨能否继续依赖Chinchilla的建议，研究发现模型参数解读不影响关键结果，且关键结果能承受较大扰动，让学界对其更有信心。


<details>
  <summary>Details</summary>
Motivation: Chinchilla提出计算最优缩放原则后受到质疑，需确定从业者能否继续依赖其建议。

Method: 揭示Chinchilla分析中模型参数的模糊性，进行不同解读；通过四种结构化方式扰动模型参数。

Result: 模型参数解读不影响关键结果，在一种解读下，令牌与参数比更稳定；关键结果对加性或系统性误差敏感，但总体能承受较大扰动。

Conclusion: 研究结果让学界对Chinchilla作为语言模型缩放的持久指南更有信心。

Abstract: Hoffman et al (2022)'s Chinchilla paper introduced the principle of
compute-optimal scaling, laying a foundation for future scaling of language
models. In the years since, however, valid concerns about Chinchilla have been
raised: wide confidence intervals, discrepancies between its three approaches,
and incongruities with other scaling laws. This raises a critical question for
the field: Can practitioners still rely on Chinchilla's prescriptions? Our work
demonstrates the answer is yes. We begin by uncovering that the model
parameters central to Chinchilla's analyses were ambiguous: three
interpretations are possible, with relative differences between different
interpretations of model parameters as high as 15.2%. We find that, perhaps
surprisingly, which model parameters are used for the analyses do not
meaningfully affect key results: the scaling law estimates and the
compute-optimal tokens-to-parameter ratio. Indeed, under one interpretation,
the tokens-to-parameter ratio becomes more constant with the target compute
budget. We then ask how distorted the Chinchilla model parameters could have
been without meaningfully affecting the key results. By deliberately perturbing
model parameters in four structured ways, we find that key Chinchilla results
are most sensitive to additive or systematic errors, which can alter the
otherwise flat trend of the optimal tokens-to-parameter ratio, but overall,
Chinchilla's key results withstand sizable perturbations. Altogether, our
findings offer the field renewed confidence in Chinchilla as a durable guide
for scaling language models.

</details>


### [389] [Detecting and Rectifying Noisy Labels: A Similarity-based Approach](https://arxiv.org/abs/2509.23964)
*Dang Huu-Tien,Naoya Inoue*

Main category: cs.LG

TL;DR: 提出利用神经网络倒数第二层特征的模型无关误差检测和纠正方法，实验表明该方法性能高且能提升数据集质量。


<details>
  <summary>Details</summary>
Motivation: 数据集中的标签噪声会损害神经网络训练性能，随着深度网络规模增长，对检测此类错误的自动化工具需求增加。

Method: 提出后验的、模型无关的误差检测和纠正方法，利用神经网络倒数第二层特征，依据误标记数据点与真实类数据点特征相似度更高的观察。

Result: 方法在各种噪声下都有高性能，还能自动纠正错误提升数据集质量。

Conclusion: 所提方法可有效检测和纠正数据集中的标签噪声，提升数据集质量。

Abstract: Label noise in datasets could damage the performance of neural net training.
As the size of modern deep networks grows, there is a growing demand for
automated tools for detecting such errors. In this paper, we propose post-hoc,
model-agnostic error detection and rectification methods utilizing the
penultimate feature from a neural network. Our idea is based on the observation
that the similarity between the penultimate feature of a mislabeled data point
and its true class data points is higher than that for data points from other
classes, making the probability of label occurrence within a tight, similar
cluster informative for detecting and rectifying errors. Extensive experiments
show our method not only demonstrates high performance across various noises
but also automatically rectifies these errors to improve the quality of
datasets.

</details>


### [390] [Curriculum-Guided Reinforcement Learning for Synthesizing Gas-Efficient Financial Derivatives Contracts](https://arxiv.org/abs/2509.23976)
*Maruf Ahmed Mridul,Oshani Seneviratne*

Main category: cs.LG

TL;DR: 本文引入强化学习框架，从CDM规范直接生成功能和燃气优化的Solidity智能合约，实现显著燃气节省。


<details>
  <summary>Details</summary>
Motivation: 基于智能合约的金融衍生品自动化有效率提升，但将金融规范转化为燃气高效代码有挑战，尤其是从CDM等高级规范生成功能正确且经济可行的代码。

Method: 引入强化学习框架，使用近端策略优化（PPO）智能体从预定义库中选择最优代码片段，采用两阶段课程，先训练功能正确性，再关注燃气优化。

Result: RL智能体学会生成能显著节省燃气的合约，在未见测试数据上相比未优化基线成本降低达35.59%。

Conclusion: 提出了自动化合成可靠且经济可持续智能合约的可行方法，弥合高级金融协议与高效链上执行之间的差距。

Abstract: Smart contract-based automation of financial derivatives offers substantial
efficiency gains, but its real-world adoption is constrained by the complexity
of translating financial specifications into gas-efficient executable code. In
particular, generating code that is both functionally correct and economically
viable from high-level specifications, such as the Common Domain Model (CDM),
remains a significant challenge. This paper introduces a Reinforcement Learning
(RL) framework to generate functional and gas-optimized Solidity smart
contracts directly from CDM specifications. We employ a Proximal Policy
Optimization (PPO) agent that learns to select optimal code snippets from a
pre-defined library. To manage the complex search space, a two-phase curriculum
first trains the agent for functional correctness before shifting its focus to
gas optimization. Our empirical results show the RL agent learns to generate
contracts with significant gas savings, achieving cost reductions of up to
35.59% on unseen test data compared to unoptimized baselines. This work
presents a viable methodology for the automated synthesis of reliable and
economically sustainable smart contracts, bridging the gap between high-level
financial agreements and efficient on-chain execution.

</details>


### [391] [Guide: Generalized-Prior and Data Encoders for DAG Estimation](https://arxiv.org/abs/2509.23992)
*Amartya Roy,Devharish N,Shreya Ganguly,Kripabandhu Ghosh*

Main category: cs.LG

TL;DR: 传统因果发现方法在可扩展性、计算效率和数据类型适应性上有局限，本文提出GUIDE框架，能提升计算效率和准确率，适应混合数据类型且可扩展到70个以上节点。


<details>
  <summary>Details</summary>
Motivation: 现代因果发现方法在可扩展性、计算效率和适应混合数据类型方面存在关键局限，传统算法难以应对这些挑战。

Method: 提出GUIDE框架，通过双编码器架构将大语言模型生成的邻接矩阵与观测数据集成，训练时使用强化学习代理动态平衡奖励最大化和惩罚避免。

Result: 与RL - BIC和KCRL方法相比，平均运行时间减少约42%；比NOTEARS和GraN - DAG的准确率平均提高约117%。

Conclusion: GUIDE能在混合数据类型上表现稳健，可扩展到70个以上节点，而基线方法在此设置下失败。

Abstract: Modern causal discovery methods face critical limitations in scalability,
computational efficiency, and adaptability to mixed data types, as evidenced by
benchmarks on node scalability (30, $\le 50$, $\ge 70$ nodes), computational
energy demands, and continuous/non-continuous data handling. While traditional
algorithms like PC, GES, and ICA-LiNGAM struggle with these challenges,
exhibiting prohibitive energy costs for higher-order nodes and poor scalability
beyond 70 nodes, we propose \textbf{GUIDE}, a framework that integrates Large
Language Model (LLM)-generated adjacency matrices with observational data
through a dual-encoder architecture. GUIDE uniquely optimizes computational
efficiency, reducing runtime on average by $\approx 42%$ compared to RL-BIC and
KCRL methods, while achieving an average $\approx 117%$ improvement in accuracy
over both NOTEARS and GraN-DAG individually. During training, GUIDE's
reinforcement learning agent dynamically balances reward maximization
(accuracy) and penalty avoidance (DAG constraints), enabling robust performance
across mixed data types and scalability to $\ge 70$ nodes -- a setting where
baseline methods fail.

</details>


### [392] [SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention](https://arxiv.org/abs/2509.24006)
*Jintao Zhang,Haoxu Wang,Kai Jiang,Shuo Yang,Kaiwen Zheng,Haocheng Xi,Ziteng Wang,Hongzhou Zhu,Min Zhao,Ion Stoica,Joseph E. Gonzalez,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: 提出SLA方法加速扩散模型，减少注意力计算，提升视频生成速度且不降低质量。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformer模型视频生成中注意力延迟是瓶颈，需加速。

Method: 将注意力权重分类，分别采用不同复杂度注意力计算，融合成SLA方法并实现GPU内核。

Result: DiT模型用SLA减少95%注意力计算，注意力计算加速13.7倍，视频生成端到端加速2.2倍。

Conclusion: SLA方法有效加速扩散模型，性能优于基线方法。

Abstract: In Diffusion Transformer (DiT) models, particularly for video generation,
attention latency is a major bottleneck due to the long sequence length and the
quadratic complexity. We find that attention weights can be separated into two
parts: a small fraction of large weights with high rank and the remaining
weights with very low rank. This naturally suggests applying sparse
acceleration to the first part and low-rank acceleration to the second. Based
on this finding, we propose SLA (Sparse-Linear Attention), a trainable
attention method that fuses sparse and linear attention to accelerate diffusion
models. SLA classifies attention weights into critical, marginal, and
negligible categories, applying O(N^2) attention to critical weights, O(N)
attention to marginal weights, and skipping negligible ones. SLA combines these
computations into a single GPU kernel and supports both forward and backward
passes. With only a few fine-tuning steps using SLA, DiT models achieve a 20x
reduction in attention computation, resulting in significant acceleration
without loss of generation quality. Experiments show that SLA reduces attention
computation by 95% without degrading end-to-end generation quality,
outperforming baseline methods. In addition, we implement an efficient GPU
kernel for SLA, which yields a 13.7x speedup in attention computation and a
2.2x end-to-end speedup in video generation on Wan2.1-1.3B.

</details>


### [393] [Pretraining Scaling Laws for Generative Evaluations of Language Models](https://arxiv.org/abs/2509.24012)
*Rylan Schaeffer,Noam Levi,Brando Miranda,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 本文提出并评估三种预训练缩放定律，用于生成式评估中拟合和预测性能，为预测生成式性能提供见解和方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究在生成式评估的缩放定律拟合和性能预测方面较少，需开展相关研究。

Method: 提出三种不同的预训练缩放定律，其使用的协变量不同，分别为计算量、模型参数和令牌、黄金参考解决方案的对数似然。

Result: 生成式评估有新超参数可控制缩放定律；计算和参数+令牌缩放定律在最后1.5 - 2.5个数量级稳定，黄金参考似然缩放定律在最后约5个数量级稳定；三种缩放定律预测性能相当；计算缩放定律是参数 - 令牌缩放定律的计算最优包络。

Conclusion: 该框架为研究人员和从业者提供预测生成式性能的见解和方法。

Abstract: Neural scaling laws have played a central role in modern machine learning,
driving the field's ever-expanding scaling of parameters, data and compute.
While much research has gone into fitting scaling laws and predicting
performance on pretraining losses and on discriminative evaluations such as
multiple-choice question-answering, comparatively little research has been done
on fitting scaling laws and predicting performance on generative evaluations
such as mathematical problem-solving or software engineering. We propose and
evaluate three different pretraining scaling laws for fitting pass-at-$k$ on
generative evaluations and for predicting pass-at-$k$ of the most expensive
model using the performance of cheaper models. Our three scaling laws differ in
the covariates used: (1) compute, (2) model parameters and tokens, (3) log
likelihoods of gold reference solutions. We make four main contributions: (1)
We show how generative evaluations offer new hyperparameters (in our setting,
$k$) that researchers can use to control the scaling laws parameters and the
predictability of performance. (2) In terms of scaling law parameters, we find
that the compute scaling law and parameters\,+\,tokens scaling law stabilize
for the last ~$1.5{-}2.5$ orders of magnitude, whereas the gold reference
likelihood scaling law stabilizes for the last ~$5$ orders of magnitude. (3) In
terms of predictive performance, we find all three scaling laws perform
comparably, although the compute scaling law predicts slightly worse for small
$k$ and the log likelihoods of gold reference solutions predicts slightly worse
for large $k$. (4) We establish a theoretical connection that the compute
scaling law emerges as the compute-optimal envelope of the
parameters-and-tokens scaling law. Our framework provides researchers and
practitioners with insights and methodologies to forecast generative
performance.

</details>


### [394] [GPS-MTM: Capturing Pattern of Normalcy in GPS-Trajectories with self-supervised learning](https://arxiv.org/abs/2509.24031)
*Umang Garg,Bowen Zhang,Anantanjit Subrahmanya,Chandrakanth Gudavalli,BS Manjunath*

Main category: cs.LG

TL;DR: 本文介绍用于大规模移动数据的基础模型GPS - MTM，它在轨迹分析下游任务表现出色，确立了移动数据在大规模表征学习中的地位并开源代码。


<details>
  <summary>Details</summary>
Motivation: 利用基础模型在轨迹建模领域取得类似文本、视觉和视频理解领域的突破，捕捉人类移动的正常模式。

Method: 将移动性分解为状态和动作两种互补模态，使用带自监督掩码建模目标的双向Transformer重建跨模态缺失段。

Result: 在Numosim - LA、Urban Anomalies和Geolife等基准数据集的下游任务（如轨迹填充和下一站预测）中始终表现优于其他方法，在动态任务中优势明显。

Conclusion: GPS - MTM是用于轨迹分析的强大基础模型，使移动数据成为大规模表征学习的一等模态。

Abstract: Foundation models have driven remarkable progress in text, vision, and video
understanding, and are now poised to unlock similar breakthroughs in trajectory
modeling. We introduce the GPSMasked Trajectory Transformer (GPS-MTM), a
foundation model for large-scale mobility data that captures patterns of
normalcy in human movement. Unlike prior approaches that flatten trajectories
into coordinate streams, GPS-MTM decomposes mobility into two complementary
modalities: states (point-of-interest categories) and actions (agent
transitions). Leveraging a bi-directional Transformer with a self-supervised
masked modeling objective, the model reconstructs missing segments across
modalities, enabling it to learn rich semantic correlations without manual
labels. Across benchmark datasets, including Numosim-LA, Urban Anomalies, and
Geolife, GPS-MTM consistently outperforms on downstream tasks such as
trajectory infilling and next-stop prediction. Its advantages are most
pronounced in dynamic tasks (inverse and forward dynamics), where contextual
reasoning is critical. These results establish GPS-MTM as a robust foundation
model for trajectory analytics, positioning mobility data as a first-class
modality for large-scale representation learning. Code is released for further
reference.

</details>


### [395] [Optimism as Risk-Seeking in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.24047)
*Runyu Zhang,Na Li,Asuman Ozdaglar,Jeff Shamma,Gioele Zardini*

Main category: cs.LG

TL;DR: 本文基于凸风险度量的对偶表示提出框架，将风险寻求目标解释为乐观主义，推导乐观价值函数的策略梯度定理，开发去中心化乐观演员 - 评论家算法，实验表明该方法能提升多智能体协作。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习在合作场景下保守策略常导致次优均衡，乐观方法缺乏理论基础，需一种理论上合理且实践有效的合作方法。

Method: 基于凸风险度量的对偶表示提出框架，引入乐观价值函数，推导策略梯度定理，开发去中心化乐观演员 - 评论家算法。

Result: 在合作基准测试中，风险寻求乐观主义比风险中性基线和启发式乐观方法更能改善协调。

Conclusion: 该框架统一了风险敏感学习和乐观主义，为多智能体强化学习中的合作提供了理论和实践有效的方法。

Abstract: Risk sensitivity has become a central theme in reinforcement learning (RL),
where convex risk measures and robust formulations provide principled ways to
model preferences beyond expected return. Recent extensions to multi-agent RL
(MARL) have largely emphasized the risk-averse setting, prioritizing robustness
to uncertainty. In cooperative MARL, however, such conservatism often leads to
suboptimal equilibria, and a parallel line of work has shown that optimism can
promote cooperation. Existing optimistic methods, though effective in practice,
are typically heuristic and lack theoretical grounding. Building on the dual
representation for convex risk measures, we propose a principled framework that
interprets risk-seeking objectives as optimism. We introduce optimistic value
functions, which formalize optimism as divergence-penalized risk-seeking
evaluations. Building on this foundation, we derive a policy-gradient theorem
for optimistic value functions, including explicit formulas for the entropic
risk/KL-penalty setting, and develop decentralized optimistic actor-critic
algorithms that implement these updates. Empirical results on cooperative
benchmarks demonstrate that risk-seeking optimism consistently improves
coordination over both risk-neutral baselines and heuristic optimistic methods.
Our framework thus unifies risk-sensitive learning and optimism, offering a
theoretically grounded and practically effective approach to cooperation in
MARL.

</details>


### [396] [Collaborative Device-Cloud LLM Inference through Reinforcement Learning](https://arxiv.org/abs/2509.24050)
*Wenzhi Fang,Dong-Jun Han,Liangqi Yuan,Christopher Brinton*

Main category: cs.LG

TL;DR: 提出一种设备端大语言模型在解决过程结束时做路由决策的框架，实验显示该方法优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有设备 - 云协作中依赖外部路由器的方法难以从提示的表面模式判断任务难度，需要改进路由决策方法。

Method: 提出框架，让设备端大语言模型在解决过程结束时做路由决策，通过后训练赋予能力；构建奖励最大化问题，设计奖励；开发组自适应策略梯度算法，包括组级策略梯度和自适应提示过滤。

Result: 广泛的跨模型和基准测试实验表明，该方法始终优于现有基线，显著缩小与全云大语言模型性能的差距。

Conclusion: 所提出的方法论有效，能提升设备 - 云协作中路由决策的效果。

Abstract: Device-cloud collaboration has emerged as a promising paradigm for deploying
large language models (LLMs), combining the efficiency of lightweight on-device
inference with the superior performance of powerful cloud LLMs. An essential
problem in this scenario lies in deciding whether a given query is best handled
locally or delegated to the cloud. Existing approaches typically rely on
external routers, implemented as binary classifiers, which often struggle to
determine task difficulty from the prompt's surface pattern. To address these
limitations, we propose a framework where the on-device LLM makes routing
decisions at the end of its solving process, with this capability instilled
through post-training. In particular, we formulate a reward maximization
problem with carefully designed rewards that encourage effective problem
solving and judicious offloading to the cloud. To solve this problem, we
develop a group-adaptive policy gradient algorithm, featuring a group-level
policy gradient, designed to yield an unbiased gradient estimator of the
reward, and adaptive prompt filtering, developed to enforce the constraint on
cloud LLM usage. Extensive experiments across models and benchmarks show that
the proposed methodology consistently outperforms existing baselines and
significantly narrows the gap to full cloud LLM performance.

</details>


### [397] [In-Context Compositional Q-Learning for Offline Reinforcement Learning](https://arxiv.org/abs/2509.24067)
*Qiushui Xu,Yuhao Huang,Yushu Jiang,Lei Song,Jinyu Wang,Wenliang Zheng,Jiang Bian*

Main category: cs.LG

TL;DR: 本文提出ICQL框架解决离线强化学习中Q函数估计问题，理论证明误差有界，实验显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法依赖单一全局Q函数，难以捕捉包含多样子任务的组合特性。

Method: 提出In - context Compositional Q - Learning (ICQL)框架，将Q学习表述为上下文推理问题，用线性Transformer从检索的转换中自适应推断局部Q函数。

Result: 理论上，在两个假设下ICQL的Q函数近似误差有界，支持近最优策略提取；实验上，在厨房、Gym和Adroit任务中分别最多提升16.4%、8.6%和6.3%的性能。

Conclusion: ICQL展现了上下文学习在鲁棒和组合价值估计方面的潜力，是有效的离线强化学习框架。

Abstract: Accurately estimating the Q-function is a central challenge in offline
reinforcement learning. However, existing approaches often rely on a single
global Q-function, which struggles to capture the compositional nature of tasks
involving diverse subtasks. We propose In-context Compositional Q-Learning
(\texttt{ICQL}), the first offline RL framework that formulates Q-learning as a
contextual inference problem, using linear Transformers to adaptively infer
local Q-functions from retrieved transitions without explicit subtask labels.
Theoretically, we show that under two assumptions--linear approximability of
the local Q-function and accurate weight inference from retrieved
context--\texttt{ICQL} achieves bounded Q-function approximation error, and
supports near-optimal policy extraction. Empirically, \texttt{ICQL}
substantially improves performance in offline settings: improving performance
in kitchen tasks by up to 16.4\%, and in Gym and Adroit tasks by up to 8.6\%
and 6.3\%. These results highlight the underexplored potential of in-context
learning for robust and compositional value estimation, positioning
\texttt{ICQL} as a principled and effective framework for offline RL.

</details>


### [398] [A Small Math Model: Recasting Strategy Choice Theory in an LLM-Inspired Architecture](https://arxiv.org/abs/2509.24068)
*Roussel Rahman,Jeff Shrager*

Main category: cs.LG

TL;DR: 将策略选择理论（SCT）重铸为基于神经网络架构的小数学模型（SMM），扩展SCT功能，展示计数与加法干扰及指算使用情况，计划进一步扩展SMM。


<details>
  <summary>Details</summary>
Motivation: 为儿童算术学习的策略选择理论提供更完善模型，探究基于大语言模型的智能体对数学推理关键数值特征和关系的理解。

Method: 将SCT重铸为SMM，采用类似大语言模型的神经网络架构，扩展SCT包含计数练习、符号嵌入和门控注意力。

Result: SMM展示了计数和加法间的建设性和破坏性干扰，以及随着加法回忆能力提升指算的‘波浪式’使用情况。

Conclusion: SMM可作为统一平台，后续可扩展到SCT项目后期方面，用于研究数学推理所需数值特征和关系的理解。

Abstract: Strategy Choice Theory (SCT)\footnote{``Strategy Choice Theory'',
``Distributions of Associations'', and ``Overlapping Wave Theory'' have been
used to refer to this line of work, emphasizing different
aspects.}\citep[e.g.,][]{siegler1984strategychoices, siegler2000rebirth}
explains important aspects of children's arithmetic learning based upon
principles including learning from developmentally naturalistic data,
probabilistic representation, confidence-based retrieval, and the phase-like
importance of scaffolding strategies, such as finger-counting. Here we recast
SCT as a ``Small Math Model'' (SMM), employing a neural-network-based
architecture analogous to LLMs. The SMM extends SCT to include counting
practice\footnote{The original SCT model was pre-biased in accordance with the
supposed experience of counting.}, symbol (number) embedding, and gated
attention. Similar to earlier work, the SMM demonstrates constructive and
destructive interference between counting and addition, and the ``wave-like''
use of finger-counting as sum recall improves. We plan to extend the SMM to
later aspects of the decades-long SCT program, including adaptive strategy
choice and eventually strategy discovery, providing a unified platform to
investigate the understanding of numerical characteristics and relationships
essential for mathematical reasoning -- as it can emerge in LLM-based agents.

</details>


### [399] [AQUAIR: A High-Resolution Indoor Environmental Quality Dataset for Smart Aquaculture Monitoring](https://arxiv.org/abs/2509.24069)
*Youssef Sabiri,Walid Houmaidi,Ouail El Maadi,Yousra Chtouki*

Main category: cs.LG

TL;DR: 介绍公开数据集AQUAIR，记录摩洛哥水产养殖设施内6种IEQ变量数据，可用于相关研究。


<details>
  <summary>Details</summary>
Motivation: 公共水产养殖室内空气数据集稀缺，限制预测和异常检测工具开发。

Method: 用Awair HOME监测仪每5分钟采样，进行质量控制，用开源处理流程处理数据。

Result: 得到超23000条时间戳观测数据，探索性统计显示条件稳定且有喂食高峰。

Conclusion: AQUAIR填补智能水产养殖信息学空白，为相关研究提供可复现基准。

Abstract: Smart aquaculture systems depend on rich environmental data streams to
protect fish welfare, optimize feeding, and reduce energy use. Yet public
datasets that describe the air surrounding indoor tanks remain scarce, limiting
the development of forecasting and anomaly-detection tools that couple
head-space conditions with water-quality dynamics. We therefore introduce
AQUAIR, an open-access public dataset that logs six Indoor Environmental
Quality (IEQ) variables--air temperature, relative humidity, carbon dioxide,
total volatile organic compounds, PM2.5 and PM10--inside a fish aquaculture
facility in Amghass, Azrou, Morocco. A single Awair HOME monitor sampled every
five minutes from 14 October 2024 to 9 January 2025, producing more than 23,000
time-stamped observations that are fully quality-controlled and publicly
archived on Figshare. We describe the sensor placement, ISO-compliant mounting
height, calibration checks against reference instruments, and an open-source
processing pipeline that normalizes timestamps, interpolates short gaps, and
exports analysis-ready tables. Exploratory statistics show stable conditions
(median CO2 = 758 ppm; PM2.5 = 12 micrograms/m3) with pronounced feeding-time
peaks, offering rich structure for short-horizon forecasting, event detection,
and sensor drift studies. AQUAIR thus fills a critical gap in smart aquaculture
informatics and provides a reproducible benchmark for data-centric machine
learning curricula and environmental sensing research focused on head-space
dynamics in recirculating aquaculture systems.

</details>


### [400] [PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM](https://arxiv.org/abs/2509.24085)
*Ju-Hyung Lee,Yanqing Lu,Klaus Doppler*

Main category: cs.LG

TL;DR: 提出PEARL框架用于D2D通信跨层优化，研究两种轻量级变体，在合成场景中表现优，证明LLM用于设备跨层控制可行。


<details>
  <summary>Details</summary>
Motivation: 在D2D通信中实现合作式跨层优化。

Method: 基于单设备LLM工作，利用发布者和订阅者状态指导WA参数选择，使用上下文感知奖励进行KL微调，研究两种轻量级变体。

Result: PEARL（Head + LoRA）整体性能最佳，PEARL - Lite推理时间亚20ms，比启发式和紧凑模型基线提高目标得分，低电量合作时节能达16%。

Conclusion: 同伴感知上下文、奖励对齐训练和基于头部的效率使LLM可用于始终开启的设备跨层控制。

Abstract: We present PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM), a
framework for cooperative cross-layer optimization in device-to-device (D2D)
communication. Building on our previous work on single-device on-device LLMs,
PEARL extends the paradigm by leveraging both publisher and subscriber states
to guide Wi-Fi Aware (WA) parameter selection. A context-aware reward, which
normalizes latency by application tolerances and modulates energy by device
battery states, provides richer supervision for KL-based finetuning. We study
two lightweight variants: PEARL (Head + Low-Rank Adaptation (LoRA)) achieves
the best overall performance, while PEARL-Lite (Head-only) delivers sub-20 ms
inference at near-identical objective scores. Across synthetic scenarios
grounded in real measurements, PEARL improves objective scores over heuristic
and compact model baselines and reduces energy by up to 16% in cooperative
low-battery cases. These results demonstrate that peer-aware context,
reward-aligned training, and head-based efficiency make LLMs practical for
always-on, on-device cross-layer control.

</details>


### [401] [Clebsch-Gordan Transformer: Fast and Global Equivariant Attention](https://arxiv.org/abs/2509.24093)
*Owen Lewis Howell,Linfeng Zhao,Xupeng Zhu,Yaoyao Qian,Haojie Huang,Lingfeng Sun,Wil Thomason,Robert Platt,Robin Walters*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The global attention mechanism is one of the keys to the success of
transformer architecture, but it incurs quadratic computational costs in
relation to the number of tokens. On the other hand, equivariant models, which
leverage the underlying geometric structures of problem instance, often achieve
superior accuracy in physical, biochemical, computer vision, and robotic tasks,
at the cost of additional compute requirements. As a result, existing
equivariant transformers only support low-order equivariant features and local
context windows, limiting their expressiveness and performance. This work
proposes Clebsch-Gordan Transformer, achieving efficient global attention by a
novel Clebsch-Gordon Convolution on $\SO(3)$ irreducible representations. Our
method enables equivariant modeling of features at all orders while achieving
${O}(N \log N)$ input token complexity. Additionally, the proposed method
scales well with high-order irreducible features, by exploiting the sparsity of
the Clebsch-Gordon matrix. Lastly, we also incorporate optional token
permutation equivariance through either weight sharing or data augmentation. We
benchmark our method on a diverse set of benchmarks including n-body
simulation, QM9, ModelNet point cloud classification and a robotic grasping
dataset, showing clear gains over existing equivariant transformers in GPU
memory size, speed, and accuracy.

</details>


### [402] [ADAPT: Lightweight, Long-Range Machine Learning Force Fields Without Graphs](https://arxiv.org/abs/2509.24115)
*Evan Dramko,Yihuang Xiong,Yizhi Zhu,Geoffroy Hautier,Thomas Reps,Christopher Jermaine,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: 引入ADAPT MLFF处理硅点缺陷，减少预测误差并降低计算成本


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的MLFF存在过平滑和长程相互作用表示不佳问题，尤其在建模点缺陷时需解决

Method: 引入ADAPT，用直接空间坐标公式替代图表示，显式考虑所有原子对相互作用，用Transformer编码器建模原子相互作用

Result: 应用于硅点缺陷数据集时，与基于GNN的模型相比，力和能量预测误差约减少33%，计算成本仅为一小部分

Conclusion: ADAPT在处理点缺陷方面具有优势，能降低误差和计算成本

Abstract: Point defects play a central role in driving the properties of materials.
First-principles methods are widely used to compute defect energetics and
structures, including at scale for high-throughput defect databases. However,
these methods are computationally expensive, making machine-learning force
fields (MLFFs) an attractive alternative for accelerating structural
relaxations. Most existing MLFFs are based on graph neural networks (GNNs),
which can suffer from oversmoothing and poor representation of long-range
interactions. Both of these issues are especially of concern when modeling
point defects. To address these challenges, we introduce the Accelerated Deep
Atomic Potential Transformer (ADAPT), an MLFF that replaces graph
representations with a direct coordinates-in-space formulation and explicitly
considers all pairwise atomic interactions. Atoms are treated as tokens, with a
Transformer encoder modeling their interactions. Applied to a dataset of
silicon point defects, ADAPT achieves a roughly 33 percent reduction in both
force and energy prediction errors relative to a state-of-the-art GNN-based
model, while requiring only a fraction of the computational cost.

</details>


### [403] [HyMaTE: A Hybrid Mamba and Transformer Model for EHR Representation Learning](https://arxiv.org/abs/2509.24118)
*Md Mozaharul Mottalib,Thao-Ly T. Phan,Rahmatollah Beheshti*

Main category: cs.LG

TL;DR: 本文提出HyMaTE模型处理EHR数据，结合SSMs和注意力机制优势，在多临床数据集预测任务验证效果，代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习难以处理EHR数据的复杂性，Transformer模型有计算复杂度和上下文长度限制，SSMs模型多关注序列级信息而非通道级数据。

Method: 提出HyMaTE，一种结合SSMs和先进注意力机制的混合模型，用于表示纵向数据。

Result: HyMaTE能捕捉EHR数据有效、丰富、细致的统一表示，自注意力结果具有可解释性。

Conclusion: HyMaTE是适用于现实医疗应用的可扩展、通用的解决方案。

Abstract: Electronic health Records (EHRs) have become a cornerstone in modern-day
healthcare. They are a crucial part for analyzing the progression of patient
health; however, their complexity, characterized by long, multivariate
sequences, sparsity, and missing values poses significant challenges in
traditional deep learning modeling. While Transformer-based models have
demonstrated success in modeling EHR data and predicting clinical outcomes,
their quadratic computational complexity and limited context length hinder
their efficiency and practical applications. On the other hand, State Space
Models (SSMs) like Mamba present a promising alternative offering linear-time
sequence modeling and improved efficiency for handling long sequences, but
focus mostly on mixing sequence-level information rather than channel-level
data. To overcome these challenges, we propose HyMaTE (A Hybrid Mamba and
Transformer Model for EHR Representation Learning), a novel hybrid model
tailored for representing longitudinal data, combining the strengths of SSMs
with advanced attention mechanisms. By testing the model on predictive tasks on
multiple clinical datasets, we demonstrate HyMaTE's ability to capture an
effective, richer, and more nuanced unified representation of EHR data.
Additionally, the interpretability of the outcomes achieved by self-attention
illustrates the effectiveness of our model as a scalable and generalizable
solution for real-world healthcare applications. Codes are available at:
https://github.com/healthylaife/HyMaTE.

</details>


### [404] [Echo Flow Networks](https://arxiv.org/abs/2509.24122)
*Hongbo Liu,Jia Xu*

Main category: cs.LG

TL;DR: 本文提出回声流网络（EFNs）解决时间序列预测中长程依赖问题，评估显示其训练更快、模型更小、误差更低，EchoFormer达最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习架构在计算复杂度和长程信息保留间存在权衡，传统回声状态网络非线性能力有限，性能不佳。

Method: 引入由扩展回声状态网络和MLP读出组成的EFNs框架，采用新颖的矩阵门控复合随机激活（MCRA）；提出双流架构，让近期输入历史从无限时域记忆中动态选择特征。

Result: 在五个基准测试中，EFNs训练速度比PatchTST快4倍，模型大小小3倍，预测误差从43%降至35%；EchoFormer在五个基准数据集上达最优性能。

Conclusion: EFNs在时间序列预测中表现出色，能有效解决长程依赖问题，提高预测准确性和稳定性。

Abstract: At the heart of time-series forecasting (TSF) lies a fundamental challenge:
how can models efficiently and effectively capture long-range temporal
dependencies across ever-growing sequences? While deep learning has brought
notable progress, conventional architectures often face a trade-off between
computational complexity and their ability to retain accumulative information
over extended horizons.
  Echo State Networks (ESNs), a class of reservoir computing models, have
recently regained attention for their exceptional efficiency, offering constant
memory usage and per-step training complexity regardless of input length. This
makes them particularly attractive for modeling extremely long-term event
history in TSF. However, traditional ESNs fall short of state-of-the-art
performance due to their limited nonlinear capacity, which constrains both
their expressiveness and stability.
  We introduce Echo Flow Networks (EFNs), a framework composed of a group of
extended Echo State Networks (X-ESNs) with MLP readouts, enhanced by our novel
Matrix-Gated Composite Random Activation (MCRA), which enables complex,
neuron-specific temporal dynamics, significantly expanding the network's
representational capacity without compromising computational efficiency. In
addition, we propose a dual-stream architecture in which recent input history
dynamically selects signature reservoir features from an infinite-horizon
memory, leading to improved prediction accuracy and long-term stability.
  Extensive evaluations on five benchmarks demonstrate that EFNs achieve up to
4x faster training and 3x smaller model size compared to leading methods like
PatchTST, reducing forecasting error from 43% to 35%, a 20% relative
improvement. One instantiation of our framework, EchoFormer, consistently
achieves new state-of-the-art performance across five benchmark datasets: ETTh,
ETTm, DMV, Weather, and Air Quality.

</details>


### [405] [The Impossibility of Inverse Permutation Learning in Transformer Models](https://arxiv.org/abs/2509.24125)
*Rohan Alur,Chris Hays,Manish Raghavan,Devavrat Shah*

Main category: cs.LG

TL;DR: 研究仅解码器Transformer中逆排列学习问题，得出不可能结果并给出可行替代构造。


<details>
  <summary>Details</summary>
Motivation: 该任务可模拟多种推理任务的自然鲁棒性属性，如长上下文检索、多项选择问答和上下文学习。

Method: 理论分析得出任意深度仅解码器Transformer无法学习该任务，给出两种替代构造。

Result: 任意深度仅解码器Transformer不能学习逆排列任务；编码器 - 解码器Transformer和输入填充“临时标记”时逆排列学习可行。

Conclusion: 结果涉及仅解码器Transformer模型表达能力，推测输入填充标记可能为大语言模型推理提供新机制。

Abstract: In this technical note, we study the problem of inverse permutation learning
in decoder-only transformers. Given a permutation and a string to which that
permutation has been applied, the model is tasked with producing the original
(``canonical'') string. We argue that this task models a natural robustness
property across a variety of reasoning tasks, including long-context retrieval,
multiple choice QA and in-context learning. Our primary contribution is an
impossibility result: we show that an arbitrary depth, decoder-only transformer
cannot learn this task. This result concerns the expressive capacity of
decoder-only transformer models and is agnostic to training dynamics or sample
complexity. We give a pair of alternative constructions under which inverse
permutation learning is feasible. The first of these highlights the fundamental
role of the causal attention mask, and reveals a gap between the expressivity
of encoder-decoder transformers and the more popular decoder-only architecture.
The latter result is more surprising: we show that simply padding the input
with ``scratch tokens" yields a construction under which inverse permutation
learning is possible. We conjecture that this may suggest an alternative
mechanism by which chain-of-thought prompting or, more generally, intermediate
``thinking'' tokens can enable reasoning in large language models, even when
these tokens encode no meaningful semantic information (e.g., the results of
intermediate computations).

</details>


### [406] [Evaluation of Machine and Deep Learning Techniques for Cyclone Trajectory Regression and Status Classification by Time Series Data](https://arxiv.org/abs/2509.24146)
*Ethan Zachary Lo,Dan Chie-Tien Lo*

Main category: cs.LG

TL;DR: 本文提出用机器学习方法预测热带气旋轨迹和状态，经评估随机森林分类器表现最佳，证明机器学习模型是传统预报方法的有效替代。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报模型计算量大且易出错，需要更有效的气旋预测方法。

Method: 开发两阶段机器学习管道，先用回归模型预测气旋特征，再用分类模型预测气旋状态，评估梯度提升回归和三种分类器。

Result: 随机森林分类器准确率达93%，在各项指标上优于其他分类器，回归结果误差低。

Conclusion: 机器学习模型，尤其是基于集成的分类器，是传统预报方法的有效可扩展替代，有实时预测和集成到决策支持系统的潜力。

Abstract: Accurate cyclone forecasting is essential for minimizing loss of life,
infrastructure damage, and economic disruption. Traditional numerical weather
prediction models, though effective, are computationally intensive and prone to
error due to the chaotic nature of atmospheric systems. This study proposes a
machine learning (ML) approach to forecasting tropical cyclone trajectory and
status using time series data from the National Hurricane Center, including
recently added best track wind radii. A two-stage ML pipeline is developed: a
regression model first predicts cyclone features maximum wind speed, minimum
pressure, trajectory length, and directional change using a sliding window of
historical data. These outputs are then input into classification models to
predict the cyclone's categorical status. Gradient boosting regression and
three classifiers random forest (RF), support vector machine (SVM), and
multilayer perceptron (MLP) are evaluated. After hyperparameter tuning and
synthetic minority oversampling (SMOTE), the RF classifier achieves the highest
performance with 93% accuracy, outperforming SVM and MLP across precision,
recall, and F1 score. The RF model is particularly robust in identifying
minority cyclone statuses and minimizing false negatives. Regression results
yield low mean absolute errors, with pressure and wind predictions within about
2.2 mb and 2.4 kt, respectively. These findings demonstrate that ML models,
especially ensemble-based classifiers, offer an effective, scalable alternative
to traditional forecasting methods, with potential for real-time cyclone
prediction and integration into decision support systems.

</details>


### [407] [Stable Forgetting: Bounded Parameter-Efficient Unlearning in LLMs](https://arxiv.org/abs/2509.24166)
*Arpit Garg,Hemanth Saratchandran,Ravi Garg,Simon Lucey*

Main category: cs.LG

TL;DR: 现有大语言模型机器遗忘方法不稳定不可靠，本文提出有界参数高效遗忘方法，在多个基准测试中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型机器遗忘方法不稳定不可靠，梯度差分法结合交叉熵损失会导致权重和梯度无界增长。

Method: 提出有界参数高效遗忘方法，对MLP适配器应用有界函数来稳定基于LoRA的微调。

Result: 在TOFU、TDEC和MUSE基准测试中，在不同架构和规模的模型上，该方法在遗忘性能上有显著提升，同时保留性能良好。

Conclusion: 建立了一个新的、有理论基础且实际可扩展的大语言模型遗忘框架。

Abstract: Machine unlearning in large language models (LLMs) is essential for privacy
and safety; however, existing approaches remain unstable and unreliable. A
widely used strategy, the gradient difference method, applies gradient descent
on retained data while performing gradient ascent on forget data, the data
whose influence should be removed. However, when combined with cross-entropy
loss, this procedure causes unbounded growth of weights and gradients, leading
to training instability and degrading both forgetting and retention. We provide
a theoretical framework that explains this failure, explicitly showing how
ascent on the forget set destabilizes optimization in the feedforward MLP
layers of LLMs. Guided by this insight, we propose Bounded Parameter-Efficient
Unlearning, a parameter-efficient approach that stabilizes LoRA-based
fine-tuning by applying bounded functions to MLP adapters. This simple
modification controls the weight dynamics during ascent, enabling the gradient
difference method to converge reliably. Across the TOFU, TDEC, and MUSE
benchmarks, and across architectures and scales from 125M to 8B parameters, our
method achieves substantial improvements in forgetting while preserving
retention, establishing a novel theoretically grounded and practically scalable
framework for unlearning in LLMs.

</details>


### [408] [Multi-Scale Geometric Autoencoder](https://arxiv.org/abs/2509.24168)
*Qipeng Zhan,Zhuoping Zhou,Zexuan Wang,Li Shen*

Main category: cs.LG

TL;DR: 提出多尺度几何自动编码器（MAE），同时保留数据几何结构的全局和局部尺度，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动编码器设计在保留潜在空间数据几何结构时，通常分别关注全局或局部几何属性，存在距离近似误差累积或扭曲大规模关系的问题。

Method: 提出MAE，采用非对称架构，对编码器应用全局距离约束，对解码器应用局部几何约束。

Result: 在合成流形和真实世界数据集上的综合实验表明，MAE在各种评估指标上始终优于现有方法。

Conclusion: MAE的非对称设计自然地与编码器和解码器组件的不同角色相契合，能有效保留数据几何结构。

Abstract: Autoencoders have emerged as powerful models for visualization and
dimensionality reduction based on the fundamental assumption that
high-dimensional data is generated from a low-dimensional manifold. A critical
challenge in autoencoder design is to preserve the geometric structure of data
in the latent space, with existing approaches typically focusing on either
global or local geometric properties separately. Global approaches often
encounter errors in distance approximation that accumulate, while local methods
frequently converge to suboptimal solutions that distort large-scale
relationships. We propose Multi-Scale Geometric Autoencoder (MAE), which
introduces an asymmetric architecture that simultaneously preserves both scales
of the geometric structure by applying global distance constraints to the
encoder and local geometric constraints to the decoder. Through theoretical
analysis, we establish that this asymmetric design aligns naturally with the
distinct roles of the encoder and decoder components. Our comprehensive
experiments on both synthetic manifolds and real-world datasets demonstrate
that MAE consistently outperforms existing methods across various evaluation
metrics.

</details>


### [409] [Model Correlation Detection via Random Selection Probing](https://arxiv.org/abs/2509.24171)
*Ruibo Chen,Sheng Zhang,Yihan Wu,Tong Zheng,Peihua Mai,Heng Huang*

Main category: cs.LG

TL;DR: 提出随机选择探测（RSP）框架用于模型相关性检测，实验证明其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于相似度的模型相关性检测方法需访问模型参数或缺乏原则性阈值，适用性受限。

Method: 引入RSP假设检验框架，将模型相关性检测作为统计测试，优化参考模型前缀并评估其在目标模型的可迁移性，引入无关基线模型过滤通用特征。

Result: 在不同访问条件下对LLMs和VLMs评估，相关模型p值小，无关模型p值高，消融实验证明鲁棒性。

Conclusion: RSP是首个用于模型相关性检测的原则性通用统计框架，可在机器学习生态中实现透明可解释决策。

Abstract: The growing prevalence of large language models (LLMs) and vision-language
models (VLMs) has heightened the need for reliable techniques to determine
whether a model has been fine-tuned from or is even identical to another.
Existing similarity-based methods often require access to model parameters or
produce heuristic scores without principled thresholds, limiting their
applicability. We introduce Random Selection Probing (RSP), a
hypothesis-testing framework that formulates model correlation detection as a
statistical test. RSP optimizes textual or visual prefixes on a reference model
for a random selection task and evaluates their transferability to a target
model, producing rigorous p-values that quantify evidence of correlation. To
mitigate false positives, RSP incorporates an unrelated baseline model to
filter out generic, transferable features. We evaluate RSP across both LLMs and
VLMs under diverse access conditions for reference models and test models.
Experiments on fine-tuned and open-source models show that RSP consistently
yields small p-values for related models while maintaining high p-values for
unrelated ones. Extensive ablation studies further demonstrate the robustness
of RSP. These results establish RSP as the first principled and general
statistical framework for model correlation detection, enabling transparent and
interpretable decisions in modern machine learning ecosystems.

</details>


### [410] [FM-FoG: A Real-Time Foundation Model-based Wearable System for Freezing-of-Gait Mitigation](https://arxiv.org/abs/2509.24176)
*Chuntian Chi,John Clapham,Leslie Cloud,Ingrid Pretzer-Aboff,GinaMari Blackwell,Huajie Shao,Gang Zhou*

Main category: cs.LG

TL;DR: 本文提出FM - FoG实时可穿戴系统，无需特定患者训练数据就能检测冻结步态，在未见过的患者上表现优异且节能。


<details>
  <summary>Details</summary>
Motivation: 当前冻结步态检测系统需大量特定患者训练数据且缺乏泛化性，限制临床应用。

Method: 结合多样化IMU数据集的自监督预训练与传感器上下文集成，用轻量级CNN - LSTM活动分类器在行走或站立时激活基础模型。

Result: 在VCU FoG - IMU数据集上，对未见过患者测试F1分数达98.5%，部署在手机上可延长电池寿命72%，干预延迟低于20ms。

Conclusion: FM - FoG能实现实用、节能且无需个体训练的跨患者通用医疗应用。

Abstract: Freezing-of-Gait (FoG) affects over 50% of mid-to-late stage Parkinson's
disease (PD) patients, significantly impairing patients' mobility independence
and reducing quality of life. FoG is characterized by sudden episodes where
walking cannot start or is interrupted, occurring exclusively during standing
or walking, and never while sitting or lying down. Current FoG detection
systems require extensive patient-specific training data and lack
generalization, limiting clinical deployment. To address these issues, we
introduce FM-FoG, a real-time foundation model-based wearable system achieving
FoG detection in unseen patients without patient-specific training. Our
approach combines self-supervised pretraining on diverse Inertial Measurement
Unit (IMU) datasets with sensor context integration. Since FoG occurs only
during ambulatory activities, a lightweight CNN-LSTM activity classifier
selectively activates the foundation model only during walking or standing,
avoiding unnecessary computation. Evaluated on the VCU FoG-IMU dataset with 23
PD patients, FM-FoG achieves a 98.5% F1-score when tested on previously unseen
patients, substantially outperforming competitive baseline methods. Deployed on
a Google Pixel 8a smartphone, the system extends battery life by up to 72%
while maintaining sub-20ms intervention latency. The results indicate that our
FM-FoG can enable practical, energy-efficient healthcare applications that
generalize across patients without individual training requirements.

</details>


### [411] [Negative Pre-activations Differentiate Syntax](https://arxiv.org/abs/2509.24198)
*Linghao Kong,Angelina Ning,Micah Adler,Nir Shavit*

Main category: cs.LG

TL;DR: 研究发现Wasserstein神经元在大语言模型中虽占比小但至关重要，其负预激活区域对语法处理有重要作用。


<details>
  <summary>Details</summary>
Motivation: 探究Wasserstein神经元在大语言模型中的作用及对语法处理的影响。

Method: 对Wasserstein神经元进行定向移除、特定干预，进行词性分析和层特定干预，并观察不同训练检查点的情况。

Result: 定向移除Wasserstein神经元使模型崩溃；特定负预激活干预削弱模型功能、破坏语法行为；词性分析定位额外惊奇值到句法支架标记；层特定干预表明局部退化随深度累积；相同消融在Wasserstein神经元出现和稳定时损害语法行为。

Conclusion: 纠缠神经元稀疏子集中的负分化是语言模型依赖的句法关键机制。

Abstract: A recently discovered class of entangled neurons, known as Wasserstein
neurons, is disproportionately critical in large language models despite
constituting only a very small fraction of the network: their targeted removal
collapses the model, consistent with their unique role in differentiating
similar inputs. Interestingly, in Wasserstein neurons immediately preceding
smooth activation functions, such differentiation manifests in the negative
pre-activation space, especially in early layers. Pairs of similar inputs are
driven to highly distinct negative values, and these pairs involve syntactic
tokens such as determiners and prepositions. We show that this negative region
is functional rather than simply favorable for optimization. A minimal,
sign-specific intervention that zeroes only the negative pre-activations of a
small subset of entangled neurons significantly weakens overall model function
and disrupts grammatical behavior, while both random and perplexity-matched
controls leave grammatical performance largely unchanged. Part of speech
analysis localizes the excess surprisal to syntactic scaffolding tokens, and
layer-specific interventions reveal that small local degradations accumulate
across depth. Over training checkpoints, the same ablation impairs grammatical
behavior as Wasserstein neurons emerge and stabilize. Together, these results
identify negative differentiation in a sparse subset of entangled neurons as a
crucial mechanism that language models rely on for syntax.

</details>


### [412] [Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends](https://arxiv.org/abs/2509.24203)
*Chaorui Yao,Yanxi Chen,Yuchang Sun,Yushuo Chen,Wenhao Zhang,Xuchen Pan,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: 论文推导了无特定训练数据分布假设下的群体相对REINFORCE，给出其离策略解释及适应离策略的原则，统一并重新解释算法，经实证验证。


<details>
  <summary>Details</summary>
Motivation: 受实际应用限制、大语言模型强化学习基础设施复杂性和强化学习方法创新需求驱动，研究大语言模型的离策略强化学习。

Method: 从第一性原理推导群体相对REINFORCE，得出适应离策略的两个原则。

Result: 揭开GRPO中重要性采样和裁剪作用的谜团，统一并重新解释算法，为数据加权策略提供理论依据，经实证验证。

Conclusion: 研究结果带来可操作见解，为大语言模型离策略强化学习算法设计带来新机会。

Abstract: Off-policy reinforcement learning (RL) for large language models (LLMs) is
attracting growing interest, driven by practical constraints in real-world
applications, the complexity of LLM-RL infrastructure, and the need for further
innovations of RL methodologies. While classic REINFORCE and its modern
variants like Group Relative Policy Optimization (GRPO) are typically regarded
as on-policy algorithms with limited tolerance of off-policyness, we present in
this work a first-principles derivation for group-relative REINFORCE without
assuming a specific training data distribution, showing that it admits a native
off-policy interpretation. This perspective yields two general principles for
adapting REINFORCE to off-policy settings: regularizing policy updates, and
actively shaping the data distribution. Our analysis demystifies some myths
about the roles of importance sampling and clipping in GRPO, unifies and
reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and
Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss,
and offers theoretical justification for seemingly heuristic data-weighting
strategies. Our findings lead to actionable insights that are validated with
extensive empirical studies, and open up new opportunities for principled
algorithm design in off-policy RL for LLMs. Source code for this work is
available at
https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.

</details>


### [413] [MDD-Thinker: Towards Large Reasoning Models for Major Depressive Disorder Diagnosis](https://arxiv.org/abs/2509.24217)
*Yuyang Sha,Hongxin Pan,Gang Luo,Caijuan Shi,Jing Wang,Kefeng Li*

Main category: cs.LG

TL;DR: 开发了基于大语言模型的MDD诊断框架MDD - Thinker，在真实数据上训练，性能优于传统方法和通用大模型，兼顾准确性、可解释性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前MDD诊断依赖主观评估且难以整合多模态信息，大语言模型用于诊断有挑战，需提升诊断准确性、可解释性等。

Method: 开发MDD - Thinker框架，结合监督微调（SFT）和强化学习（RL），用UK Biobank等数据集生成样本进行微调，并与多种基线模型对比评估。

Result: MDD - Thinker准确率0.8268，F1分数0.8081，显著优于传统和通用大模型，结合SFT和RL提升显著，推理性能与大模型相当且计算高效。

Conclusion: 这是首个基于大规模真实临床数据训练的用于MDD诊断的推理增强大模型框架，推理导向的大模型可为MDD检测提供可靠支持，有更广泛应用潜力。

Abstract: Background Major depressive disorder (MDD) is a leading cause of global
disability, yet current diagnostic approaches often rely on subjective
assessments and lack the ability to integrate multimodal clinical information.
Large language models (LLMs) hold promise for enhancing diagnostic accuracy
through advanced reasoning but face challenges in interpretability,
hallucination, and reliance on synthetic data.
  Methods We developed MDD-Thinker, an LLM-based diagnostic framework that
integrates supervised fine-tuning (SFT) with reinforcement learning (RL) to
strengthen reasoning ability and interpretability. Using the UK Biobank
dataset, we generated 40,000 reasoning samples, supplemented with 10,000
samples from publicly available mental health datasets. The model was
fine-tuned on these reasoning corpora, and its diagnostic and reasoning
performance was evaluated against machine learning, deep learning, and
state-of-the-art LLM baselines.
  Findings MDD-Thinker achieved an accuracy of 0.8268 and F1-score of 0.8081,
significantly outperforming traditional baselines such as SVM and MLP, as well
as general-purpose LLMs. Incorporating both SFT and RL yielded the greatest
improvements, with relative gains of 29.0% in accuracy, 38.1% in F1-score, and
34.8% in AUC. Moreover, the model demonstrated comparable reasoning performance
compared to much larger LLMs, while maintaining computational efficiency.
  Interpretation This study presents the first reasoning-enhanced LLM framework
for MDD diagnosis trained on large-scale real-world clinical data. By
integrating SFT and RL, MDD-Thinker balances accuracy, interpretability, and
efficiency, offering a scalable approach for intelligent psychiatric
diagnostics. These findings suggest that reasoning-oriented LLMs can provide
clinically reliable support for MDD detection and may inform broader
applications in mental health care.

</details>


### [414] [Conda: Column-Normalized Adam for Training Large Language Models Faster](https://arxiv.org/abs/2509.24218)
*Junjie Wang,Pan Zhou,Yiming Dong,Huan Li,Jia Li,Xun Zhou,Qicheng Lao,Cong Fang,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出Column - Normalized Adam (Conda)优化器，结合两种方法优势，在LLM预训练中表现优于基线，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于Adam的优化器更新存在谱条件差和低秩结构问题，Muon缺乏逐坐标适应性，需新优化器解决。

Method: 将更新投影到正交子空间，基于投影梯度进行列方向二阶矩归一化。

Result: 在LLaMA和GPT - 2系列实验中，Conda始终优于AdamW、Muon等基线，在LLaMA系列中收敛速度是AdamW的2 - 2.5倍，消融实验显示其在不同训练设置下有鲁棒性。

Conclusion: Conda是大规模LLM训练中有效且广泛适用的优化器。

Abstract: Large language models (LLMs) have demonstrated impressive generalization and
emergent capabilities, yet their pre-training remains computationally expensive
and sensitive to optimization dynamics. While Adam-based optimizers offer fast
convergence by adapting learning rates coordinate-wise, recent studies reveal
that their updates often suffer from poor spectral conditioning and low-rank
structures, hindering efficiency. Muon addresses this issue via global spectral
normalization but lacks the per-coordinate adaptivity of Adam. In this work, we
propose \textbf{Column-Normalized Adam (Conda)}, a novel optimizer that bridges
the strengths of both approaches. Conda projects updates into an orthogonal
subspace and applies column-wise second moment normalization based on the
projected gradients, thereby achieving both improved spectral conditioning and
maintaining coordinate-wise adaptivity. This design alleviates the spectral
pathologies of Adam while preserving its fast convergence behavior. Extensive
experiments on the LLaMA and GPT-2 series show that Conda consistently
outperforms AdamW, Muon, and other baselines in pre-training. Remarkably, on
the LLaMA series, \textbf{Conda achieves $2{\sim}2.5\times$ the convergence
speed of AdamW, measured in both training steps and training time.} Further
ablations demonstrate its robustness under diverse training setups. These
results collectively highlight Conda as an effective and broadly applicable
optimizer for large-scale LLM training. The code is released on
https://github.com/jie040109/Conda

</details>


### [415] [Proposing a Framework for Machine Learning Adoption on Legacy Systems](https://arxiv.org/abs/2509.24224)
*Ashiqur Rahman,Hamed Alhoori*

Main category: cs.LG

TL;DR: 本文提出基于API的框架，将ML模型生命周期与生产环境解耦，降低成本和运营风险，助力制造业提升竞争力。


<details>
  <summary>Details</summary>
Motivation: 机器学习集成对工业竞争力至关重要，但升级遗留系统成本高、会扰乱运营，阻碍其广泛应用，尤其是中小企业。

Method: 引入基于API的框架，将ML模型生命周期与生产环境解耦，通过轻量级浏览器界面提供服务，实现人在回路的交互控制。

Result: 框架可消除本地硬件升级需求，实现零生产停机的模型维护，增强专家对模型的信任，便于集成到现有工作流程。

Conclusion: 该框架降低了主要的财务和运营风险，为提升生产质量和安全提供了可扩展且易获取的途径，增强了制造业的竞争优势。

Abstract: The integration of machine learning (ML) is critical for industrial
competitiveness, yet its adoption is frequently stalled by the prohibitive
costs and operational disruptions of upgrading legacy systems. The financial
and logistical overhead required to support the full ML lifecycle presents a
formidable barrier to widespread implementation, particularly for small and
medium-sized enterprises. This paper introduces a pragmatic, API-based
framework designed to overcome these challenges by strategically decoupling the
ML model lifecycle from the production environment. Our solution delivers the
analytical power of ML to domain experts through a lightweight, browser-based
interface, eliminating the need for local hardware upgrades and ensuring model
maintenance can occur with zero production downtime. This human-in-the-loop
approach empowers experts with interactive control over model parameters,
fostering trust and facilitating seamless integration into existing workflows.
By mitigating the primary financial and operational risks, this framework
offers a scalable and accessible pathway to enhance production quality and
safety, thereby strengthening the competitive advantage of the manufacturing
sector.

</details>


### [416] [Accessible, Realistic, and Fair Evaluation of Positive-Unlabeled Learning Algorithms](https://arxiv.org/abs/2509.24228)
*Wei Wang,Dong-Dong Wu,Ming Li,Jingxiong Zhang,Gang Niu,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文提出首个PU学习基准，解决评估算法时的不现实和不公平问题，提供公平评估环境。


<details>
  <summary>Details</summary>
Motivation: 现有PU学习算法实验设置不一致，难以判断算法优劣，需系统比较。

Method: 提出PU学习基准，研究模型选择标准，解决内部标签偏移问题并提出校准方法。

Result: 解决模型选择不现实和评估协议不公平问题。

Conclusion: 框架将为未来评估PU学习算法提供公平环境。

Abstract: Positive-unlabeled (PU) learning is a weakly supervised binary classification
problem, in which the goal is to learn a binary classifier from only positive
and unlabeled data, without access to negative data. In recent years, many PU
learning algorithms have been developed to improve model performance. However,
experimental settings are highly inconsistent, making it difficult to identify
which algorithm performs better. In this paper, we propose the first PU
learning benchmark to systematically compare PU learning algorithms. During our
implementation, we identify subtle yet critical factors that affect the
realistic and fair evaluation of PU learning algorithms. On the one hand, many
PU learning algorithms rely on a validation set that includes negative data for
model selection. This is unrealistic in traditional PU learning settings, where
no negative data are available. To handle this problem, we systematically
investigate model selection criteria for PU learning. On the other hand, the
problem settings and solutions of PU learning have different families, i.e.,
the one-sample and two-sample settings. However, existing evaluation protocols
are heavily biased towards the one-sample setting and neglect the significant
difference between them. We identify the internal label shift problem of
unlabeled training data for the one-sample setting and propose a simple yet
effective calibration approach to ensure fair comparisons within and across
families. We hope our framework will provide an accessible, realistic, and fair
environment for evaluating PU learning algorithms in the future.

</details>


### [417] [ChessArena: A Chess Testbed for Evaluating Strategic Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2509.24239)
*Jincheng Liu,Sijun He,Jingjing Wu,Xiangsen Wang,Yang Chen,Zhaoqi Kuang,Siqi Bao,Yuan Yao*

Main category: cs.LG

TL;DR: 本文提出ChessArena测试平台评估大语言模型战略推理能力，发现当前模型有显著不足，微调的Qwen3 - 8B表现提升。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否具备真正的战略推理能力，尤其是复杂的战略推理，还是仅擅长训练数据中的模式识别。

Method: 构建ChessArena测试平台，让大语言模型相互对战，有四种不同游戏模式，配备排名算法和排行榜，可评估细粒度能力。

Result: 评估13个不同模式的大语言模型，进行超800场游戏，结果显示当前大语言模型有显著不足，无模型能击败Maia - 1100，部分模型甚至输给随机玩家。微调的Qwen3 - 8B性能大幅提升，接近更大型的先进推理模型。

Conclusion: 当前大语言模型在战略推理能力上存在显著不足，但通过微调可以提升性能。

Abstract: Recent large language models (LLMs) have shown strong reasoning capabilities.
However, a critical question remains: do these models possess genuine reasoning
skills particularly complex strategic reasoning or are they primarily excelling
at sophisticated pattern recognition within their training data? To address
this question, this paper presents a chess testbed, ChessArena, to evaluate the
strategic reasoning capabilities of LLMs. Chess requires complex strategic
reasoning capabilities including long-term planning, strict rule comprehension,
and multi-turn conversation memorization. Specifically, ChessArena is a
competitive framework where LLMs play against each other, under four different
play modes. The testbed is equipped with a ranking algorithm and a leaderboard.
The testbed can also evaluate fine-grained capabilities including basic
understanding, move selection, and puzzle solving. Over 13 LLMs with different
modes are evaluated in ChessArena, playing over 800 games. The results reveal
significant shortcomings in current LLMs: no model can beat Maia-1100 (a chess
engine at human amateur level), while some even failed to defeat a random
player that selects moves arbitrarily. We also present a strong baseline to the
testbed: our fine-tuned Qwen3-8B substantially improved performance,
approaching much larger state-of-the-art reasoning models.

</details>


### [418] [Graph Foundation Models: Bridging Language Model Paradigms and Graph Optimization](https://arxiv.org/abs/2509.24256)
*Yunhao Liang,Pujun Zhang,Yuan Qu,Shaochong Lin,Zuo-jun Max Shen*

Main category: cs.LG

TL;DR: 本文引入图基础模型GFM解决图结构上基于距离的优化问题，实验表明其性能有竞争力且推理快，开创了预训练 - 迁移框架用于图优化的新范式。


<details>
  <summary>Details</summary>
Motivation: 预训练 - 迁移范式在大语言模型成功，但扩展到图结构的运筹学问题有挑战，需弥合语言统计灵活性与图组合约束的冲突。

Method: 引入类似大语言模型的自监督预训练范式，在图随机游走生成的路径上训练，将图结构连通性作为监督信号，利用预训练的GFM作为图内在结构基础模型结合简单生成启发式方法。

Result: 在20 - 893个节点的网络上实验，GFM在多种优化任务中与专业求解器相比有竞争力，且推理时间显著更快。

Conclusion: 建立了将预训练 - 迁移框架应用于图优化的新范式，为运筹学引入基础模型创新打开大门。

Abstract: The pretrain-transfer paradigm, which underpins the success of large language
models (LLMs), has demonstrated the immense power of creating foundation models
that learn generalizable representations from vast datasets. However, extending
this paradigm to Operations Research (OR) problems on graph structures remains
challenging due to the fundamental conflict between the statistical flexibility
of language and the strict combinatorial constraints of graphs. To bridge this
gap, we introduce the Graph Foundation Model (GFM), the first framework capable
of solving all distance-based optimization problems on graph structures. By
introducing the LLM-like self-supervised pre-training paradigm on the paths
generated from random walks in the graph, GFM is compelled to internalize the
graph's complex topological and combinatorial rules, where the connectivity of
the structure itself can be treated as the supervisory signal. Unlike existing
neural methods that learn complex and task-specific solving policies, our
approach leverages the pre-trained GFM as a foundational model of the graph's
intrinsic structure, which in turn enables a simple generative heuristic to
tackle a diverse range of optimization challenges effectively. Comprehensive
experiments on networks ranging from 20 to 893 nodes demonstrate that GFM
achieves competitive performance against specialized solvers across a variety
of distinct optimization task classes, while maintaining significantly faster
inference times. Our work establishes a new paradigm of adapting the
pretrain-transfer framework to graph optimization, opening the door for
applying foundation model innovations to OR.

</details>


### [419] [Adversarial Reinforcement Learning Framework for ESP Cheater Simulation](https://arxiv.org/abs/2509.24274)
*Inkyu Park,Jeong-Gwan Lee,Taehwan Kwon,Juheon Choi,Seungku Kim,Junsu Kim,Kimin Lee*

Main category: cs.LG

TL;DR: 提出模拟框架应对ESP作弊检测难题，模拟自适应作弊行为，为研究和开发检测器提供平台。


<details>
  <summary>Details</summary>
Motivation: ESP作弊难检测，缺乏可靠标注数据，作弊者行为难捉摸，影响反作弊系统训练和开发。

Method: 构建模拟框架，将作弊者和非作弊者建模为不同可观测性的强化学习智能体，检测器对行为轨迹分类，将作弊者与检测器的交互建模为对抗游戏，引入动态切换作弊和非作弊行为的结构化作弊者模型。

Result: 框架成功模拟出能平衡奖励优化和躲避检测的自适应作弊行为。

Conclusion: 该框架为研究自适应作弊行为和开发有效反作弊检测器提供了可控且可扩展的平台。

Abstract: Extra-Sensory Perception (ESP) cheats, which reveal hidden in-game
information such as enemy locations, are difficult to detect because their
effects are not directly observable in player behavior. The lack of observable
evidence makes it difficult to collect reliably labeled data, which is
essential for training effective anti-cheat systems. Furthermore, cheaters
often adapt their behavior by limiting or disguising their cheat usage, which
further complicates detection and detector development. To address these
challenges, we propose a simulation framework for controlled modeling of ESP
cheaters, non-cheaters, and trajectory-based detectors. We model cheaters and
non-cheaters as reinforcement learning agents with different levels of
observability, while detectors classify their behavioral trajectories. Next, we
formulate the interaction between the cheater and the detector as an
adversarial game, allowing both players to co-adapt over time. To reflect
realistic cheater strategies, we introduce a structured cheater model that
dynamically switches between cheating and non-cheating behaviors based on
detection risk. Experiments demonstrate that our framework successfully
simulates adaptive cheater behaviors that strategically balance reward
optimization and detection evasion. This work provides a controllable and
extensible platform for studying adaptive cheating behaviors and developing
effective cheat detectors.

</details>


### [420] [ELASTIQ: EEG-Language Alignment with Semantic Task Instruction and Querying](https://arxiv.org/abs/2509.24302)
*Muyun Jiang,Shuailei Zhang,Zhenjie Yang,Mengjun Wu,Weibang Jiang,Zhiwei Guo,Wei Zhang,Rui Liu,Shangen Zhang,Yong Li,Yi Ding,Cuntai Guan*

Main category: cs.LG

TL;DR: 提出EEG基础模型ELASTIQ，结合任务语义指导生成EEG嵌入，在多个数据集取得SOTA，揭示任务指令作为语义先验的作用。


<details>
  <summary>Details</summary>
Motivation: 现有EEG基础模型难以将语言指令作为先验约束用于EEG表征学习，限制利用语言语义知识统一标签和任务的能力。

Method: 预训练阶段引入联合频谱 - 时间重建（STR）模块；指令调优阶段提出基于查询的跨注意力变压器IQF。

Result: 在20个数据集上评估，在14个数据集达到SOTA，在五个任务类别中取得最佳平均结果。

Conclusion: 明确任务指令可作为语义先验，引导EEG嵌入到连贯且基于语言的空间，代码和预训练权重将发布。

Abstract: Recent advances in electroencephalography (EEG) foundation models, which
capture transferable EEG representations, have greatly accelerated the
development of brain-computer interfaces (BCI). However, existing approaches
still struggle to incorporate language instructions as prior constraints for
EEG representation learning, limiting their ability to leverage the semantic
knowledge inherent in language to unify different labels and tasks. To address
this challenge, we present ELASTIQ, a foundation model for EEG-Language
Alignment with Semantic Task Instruction and Querying. ELASTIQ integrates
task-aware semantic guidance to produce structured and linguistically aligned
EEG embeddings, thereby enhancing decoding robustness and transferability. In
the pretraining stage, we introduce a joint Spectral-Temporal Reconstruction
(STR) module, which combines frequency masking as a global spectral
perturbation with two complementary temporal objectives: random masking to
capture contextual dependencies and causal masking to model sequential
dynamics. In the instruction tuning stage, we propose the
Instruction-conditioned Q-Former (IQF), a query-based cross-attention
transformer that injects instruction embeddings into EEG tokens and aligns them
with textual label embeddings through learnable queries. We evaluate ELASTIQ on
20 datasets spanning motor imagery, emotion recognition, steady-state visual
evoked potentials, covert speech, and healthcare tasks. ELASTIQ achieves
state-of-the-art performance on 14 of the 20 datasets and obtains the best
average results across all five task categories. Importantly, our analyses
reveal for the first time that explicit task instructions serve as semantic
priors guiding EEG embeddings into coherent and linguistically grounded spaces.
The code and pre-trained weights will be released.

</details>


### [421] [A study of Universal ODE approaches to predicting soil organic carbon](https://arxiv.org/abs/2509.24306)
*Satyanarayana Raju G. V. V,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 本文用基于UDE的SciML框架预测土壤有机碳动态，评估不同噪声下的表现，显示其潜力与局限，指出需改进方法以用于实地。


<details>
  <summary>Details</summary>
Motivation: 土壤有机碳预测困难，因物理、化学和生物过程复杂，需更好的预测方法。

Method: 构建基于UDE的SciML框架，结合机械物理与神经网络，用合成数据集评估六个实验案例。

Result: 在无噪声和适度噪声下，UDE能准确重建土壤有机碳动态；高噪声下出现过拟合或无法捕捉深度变化。

Conclusion: UDE适合可扩展、抗噪声的土壤有机碳预测，但实地应用需噪声感知损失函数、概率建模和更紧密整合微生物动态。

Abstract: Soil Organic Carbon (SOC) is a foundation of soil health and global climate
resilience, yet its prediction remains difficult because of intricate physical,
chemical, and biological processes. In this study, we explore a Scientific
Machine Learning (SciML) framework built on Universal Differential Equations
(UDEs) to forecast SOC dynamics across soil depth and time. UDEs blend
mechanistic physics, such as advection diffusion transport, with neural
networks that learn nonlinear microbial production and respiration. Using
synthetic datasets, we systematically evaluated six experimental cases,
progressing from clean, noise free benchmarks to stress tests with high (35%)
multiplicative, spatially correlated noise. Our results highlight both the
potential and limitations of the approach. In noise free and moderate noise
settings, the UDE accurately reconstructed SOC dynamics. In clean terminal
profile at 50 years (Case 4) achieved near perfect fidelity, with MSE = 1.6e-5,
and R2 = 0.9999. Case 5, with 7% noise, remained robust (MSE = 3.4e-6, R2 =
0.99998), capturing depth wise SOC trends while tolerating realistic
measurement uncertainty. In contrast, Case 3 (35% noise at t = 0) showed clear
evidence of overfitting: the model reproduced noisy inputs with high accuracy
but lost generalization against the clean truth (R2 = 0.94). Case 6 (35% noise
at t = 50) collapsed toward overly smooth mean profiles, failing to capture
depth wise variability and yielding negative R2, underscoring the limits of
standard training under severe uncertainty. These findings suggest that UDEs
are well suited for scalable, noise tolerant SOC forecasting, though advancing
toward field deployment will require noise aware loss functions, probabilistic
modelling, and tighter integration of microbial dynamics.

</details>


### [422] [Rethinking JEPA: Compute-Efficient Video SSL with Frozen Teachers](https://arxiv.org/abs/2509.24317)
*Xianhang Li,Chen Huang,Chun-Liang Li,Eran Malach,Josh Susskind,Vimal Thilak,Etai Littwin*

Main category: cs.LG

TL;DR: 提出SALT方法用于视频表征学习，优于V - JEPA 2，计算更优且对教师模型质量鲁棒。


<details>
  <summary>Details</summary>
Motivation: V - JEPA中EMA更新教师模型会使可扩展模型选择复杂，且耦合教师和学生架构，需改进。

Method: 先以简单像素重建目标训练目标编码器并冻结，再训练学生模型预测教师模型在掩码区域的潜在表示，即两阶段、无正则化的SALT方案。

Result: 学生模型在多个基准测试中优于V - JEPA 2编码器，计算更优，对教师模型质量鲁棒。

Conclusion: SALT是基于EMA的自蒸馏方法在视频表征学习中的简单、可扩展且计算高效的替代方案。

Abstract: Video Joint Embedding Predictive Architectures (V-JEPA) learn generalizable
off-the-shelf video representation by predicting masked regions in latent space
with an exponential moving average (EMA)-updated teacher. While EMA prevents
representation collapse, it complicates scalable model selection and couples
teacher and student architectures. We revisit masked-latent prediction and show
that a frozen teacher suffices. Concretely, we (i) train a target encoder with
a simple pixel-reconstruction objective under V-JEPA masking, then (ii) freeze
it and train a student to predict the teacher's latents on masked regions. This
leads to a two-stage, unregularized scheme that we refer to as SALT
(Static-teacher Asymmetric Latent Training). SALT decouples optimization into
pixel reconstruction (teacher) and masked latent prediction (student),
increasing transparency, efficiency, and scalability while preserving the
ability of representation to generalize under frozen evaluation. Empirically,
our student models outperform recently proposed V-JEPA 2 encoders under frozen
backbone evaluation across diverse benchmarks. They are also more
compute-optimal: at matched pretraining FLOPs, our method achieves higher
probing accuracy, and its scaling curves dominate V-JEPA's accuracy-FLOPs
Pareto frontier. Finally, we find that student quality is remarkably robust to
teacher quality: high-performing students emerge even with small, sub-optimal
teachers. This points to a compute budget allocation that should overwhelmingly
favor the student. These results position SALT as a simple, scalable, and
compute-efficient alternative to EMA-based self-distillation for video
representation learning.

</details>


### [423] [H+: An Efficient Similarity-Aware Aggregation for Byzantine Resilient Federated Learning](https://arxiv.org/abs/2509.24330)
*Shiyuan Zuo,Rongfei Fan,Cheng Zhan,Jie Xu,Puning Zhao,Han Hu*

Main category: cs.LG

TL;DR: 本文提出H+聚合方法，在有无干净数据场景下均有效，复杂度低，实验验证其为SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有相似性感知聚合方法仅适用于有干净数据的联邦学习系统，无法用于无干净数据场景。

Method: H+从上传的参数向量中随机选r维片段，用相似度检查函数H与参考向量比较，多次重复识别诚实客户端。参考向量可来自现有鲁棒算法或干净数据。

Result: 综合实验表明，H+在不同拜占庭攻击比率和多种传统拜占庭攻击下，相比现有方法有显著的鲁棒性提升。

Conclusion: H+是一种先进的相似性感知聚合方法，在有无干净数据的联邦学习系统中均适用，且计算复杂度低。

Abstract: Federated Learning (FL) enables decentralized model training without sharing
raw data. However, it remains vulnerable to Byzantine attacks, which can
compromise the aggregation of locally updated parameters at the central server.
Similarity-aware aggregation has emerged as an effective strategy to mitigate
such attacks by identifying and filtering out malicious clients based on
similarity between client model parameters and those derived from clean data,
i.e., data that is uncorrupted and trustworthy. However, existing methods adopt
this strategy only in FL systems with clean data, making them inapplicable to
settings where such data is unavailable. In this paper, we propose H+, a novel
similarity-aware aggregation approach that not only outperforms existing
methods in scenarios with clean data, but also extends applicability to FL
systems without any clean data. Specifically, H+ randomly selects
$r$-dimensional segments from the $p$-dimensional parameter vectors uploaded to
the server and applies a similarity check function $H$ to compare each segment
against a reference vector, preserving the most similar client vectors for
aggregation. The reference vector is derived either from existing robust
algorithms when clean data is unavailable or directly from clean data.
Repeating this process $K$ times enables effective identification of honest
clients. Moreover, H+ maintains low computational complexity, with an
analytical time complexity of $\mathcal{O}(KMr)$, where $M$ is the number of
clients and $Kr \ll p$. Comprehensive experiments validate H+ as a
state-of-the-art (SOTA) method, demonstrating substantial robustness
improvements over existing approaches under varying Byzantine attack ratios and
multiple types of traditional Byzantine attacks, across all evaluated scenarios
and benchmark datasets.

</details>


### [424] [Towards Generalizable PDE Dynamics Forecasting via Physics-Guided Invariant Learning](https://arxiv.org/abs/2509.24332)
*Siyang Li,Yize Chen,Yan Guo,Ming Huang,Hui Xiong*

Main category: cs.LG

TL;DR: 本文提出iMOOE方法解决PDE时空物理动力学预测中零样本OOD泛化问题，实验验证其性能。


<details>
  <summary>Details</summary>
Motivation: 现实中PDE系统参数多变，现有方法零样本OOD泛化能力不足，需研究和整合PDE动力系统的基本物理不变性。

Method: 明确提出双重PDE不变性原理，提出物理引导的不变性学习方法iMOOE，包括不变性对齐的混合算子专家架构和富含频率的不变性学习目标。

Result: 在模拟基准和实际应用的大量实验中，iMOOE在分布内表现和多样OOD预测场景的零样本泛化能力出色。

Conclusion: iMOOE方法有效解决了PDE预测中零样本OOD泛化问题。

Abstract: Advanced deep learning-based approaches have been actively applied to
forecast the spatiotemporal physical dynamics governed by partial differential
equations (PDEs), which acts as a critical procedure in tackling many science
and engineering problems. As real-world physical environments like PDE system
parameters are always capricious, how to generalize across unseen
out-of-distribution (OOD) forecasting scenarios using limited training data is
of great importance. To bridge this barrier, existing methods focus on
discovering domain-generalizable representations across various PDE dynamics
trajectories. However, their zero-shot OOD generalization capability remains
deficient, since extra test-time samples for domain-specific adaptation are
still required. This is because the fundamental physical invariance in PDE
dynamical systems are yet to be investigated or integrated. To this end, we
first explicitly define a two-fold PDE invariance principle, which points out
that ingredient operators and their composition relationships remain invariant
across different domains and PDE system evolution. Next, to capture this
two-fold PDE invariance, we propose a physics-guided invariant learning method
termed iMOOE, featuring an Invariance-aligned Mixture Of Operator Expert
architecture and a frequency-enriched invariant learning objective. Extensive
experiments across simulated benchmarks and real-world applications validate
iMOOE's superior in-distribution performance and zero-shot generalization
capabilities on diverse OOD forecasting scenarios.

</details>


### [425] [Expanding Horizons of Level Diversity via Multi-objective Evolutionary Learning](https://arxiv.org/abs/2509.24341)
*Qingquan Zhang,Ziqi Wang,Yuchen Li,Keyuan Zhang,Bo Yuan,Jialin Liu*

Main category: cs.LG

TL;DR: 本文提出多目标进化学习框架，在训练生成模型时考虑多维多样性，以提升游戏关卡多样性，案例证明该框架有效。


<details>
  <summary>Details</summary>
Motivation: 现有关卡生成方法难以全面评估多维度的关卡多样性，本文旨在在训练生成模型时考虑多维多样性以拓展关卡多样性视野。

Method: 将模型训练表述为多目标学习问题，每个多样性指标作为一个独立目标，并提出多目标进化学习框架在训练过程中同时优化多个多样性指标。

Result: 在超级马里奥兄弟基准测试中，该框架能增强多维多样性，确定生成模型的帕累托前沿，在可玩性和两个代表性多样性指标间提供权衡。

Conclusion: 该框架使决策者能在选择生成器时根据不同场景及玩家和设计师的多样需求做出明智选择。

Abstract: In recent years, the generation of diverse game levels has gained increasing
interest, contributing to a richer and more engaging gaming experience. A
number of level diversity metrics have been proposed in literature, which are
naturally multi-dimensional, leading to conflicted, complementary, or both
relationships among these dimensions. However, existing level generation
approaches often fail to comprehensively assess diversity across those
dimensions. This paper aims to expand horizons of level diversity by
considering multi-dimensional diversity when training generative models. We
formulate the model training as a multi-objective learning problem, where each
diversity metric is treated as a distinct objective. Furthermore, a
multi-objective evolutionary learning framework that optimises multiple
diversity metrics simultaneously throughout the model training process is
proposed. Our case study on the commonly used benchmark Super Mario Bros.
demonstrates that our proposed framework can enhance multi-dimensional
diversity and identify a Pareto front of generative models, which provides a
range of tradeoffs among playability and two representative diversity metrics,
including a content-based one and a player-centered one. Such capability
enables decision-makers to make informed choices when selecting generators
accommodating a variety of scenarios and the diverse needs of players and
designers.

</details>


### [426] [Watermarking Diffusion Language Models](https://arxiv.org/abs/2509.24368)
*Thibaud Gloaguen,Robin Staab,Nikola Jovanović,Martin Vechev*

Main category: cs.LG

TL;DR: 提出首个适用于扩散语言模型（DLM）的水印方案，解决了将自回归语言模型（ARLM）水印方案应用于 DLM 的挑战，实验显示该水印方案效果良好。


<details>
  <summary>Details</summary>
Motivation: 已有自回归语言模型（ARLM）水印方案难以直接应用于扩散语言模型（DLM），因 DLM 生成方式与 ARLM 不同，需新的水印方案。

Method: （i）即使部分上下文令牌未确定，也在上下文期望上应用水印；（ii）当某些令牌用作其他令牌的上下文时，提升能增强水印强度的令牌，同时保持水印检测器不变。

Result: DLM 水印的真阳性率超 99%，对生成质量影响极小，且具备与现有 ARLM 水印相似的鲁棒性。

Conclusion: 首次实现了可靠的 DLM 水印。

Abstract: We introduce the first watermark tailored for diffusion language models
(DLMs), an emergent LLM paradigm able to generate tokens in arbitrary order, in
contrast to standard autoregressive language models (ARLMs) which generate
tokens sequentially. While there has been much work in ARLM watermarking, a key
challenge when attempting to apply these schemes directly to the DLM setting is
that they rely on previously generated tokens, which are not always available
with DLM generation. In this work we address this challenge by: (i) applying
the watermark in expectation over the context even when some context tokens are
yet to be determined, and (ii) promoting tokens which increase the watermark
strength when used as context for other tokens. This is accomplished while
keeping the watermark detector unchanged. Our experimental evaluation
demonstrates that the DLM watermark leads to a >99% true positive rate with
minimal quality impact and achieves similar robustness to existing ARLM
watermarks, enabling for the first time reliable DLM watermarking.

</details>


### [427] [AXIS: Explainable Time Series Anomaly Detection with Large Language Models](https://arxiv.org/abs/2509.24378)
*Tian Lan,Hao Duong Le,Jinbo Li,Wenjun He,Meng Wang,Chenghao Liu,Chen Zhang*

Main category: cs.LG

TL;DR: 提出AXIS框架用于可解释的时间序列异常检测，引入新基准测试，实验表明AXIS解释质量高且检测准确率有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有将时间序列作为文本进行可解释TSAD的方法存在LLM难以处理连续信号、序列化缺乏上下文和模态表示对齐的问题，需要更好的方法。

Method: 引入AXIS框架，通过三种互补提示丰富LLM输入；引入新的包含多格式问题和理由的基准测试。

Result: 通过大量实验，包括LLM和人工评估，表明AXIS能产生更高质量解释，且检测准确率有竞争力。

Conclusion: AXIS框架有效解决了现有可解释TSAD方法的问题，在解释质量和检测准确率上表现良好。

Abstract: Time-series anomaly detection (TSAD) increasingly demands explanations that
articulate not only if an anomaly occurred, but also what pattern it exhibits
and why it is anomalous. Leveraging the impressive explanatory capabilities of
Large Language Models (LLMs), recent works have attempted to treat time series
as text for explainable TSAD. However, this approach faces a fundamental
challenge: LLMs operate on discrete tokens and struggle to directly process
long, continuous signals. Consequently, naive time-to-text serialization
suffers from a lack of contextual grounding and representation alignment
between the two modalities. To address this gap, we introduce AXIS, a framework
that conditions a frozen LLM for nuanced time-series understanding. Instead of
direct serialization, AXIS enriches the LLM's input with three complementary
hints derived from the series: (i) a symbolic numeric hint for numerical
grounding, (ii) a context-integrated, step-aligned hint distilled from a
pretrained time-series encoder to capture fine-grained dynamics, and (iii) a
task-prior hint that encodes global anomaly characteristics. Furthermore, to
facilitate robust evaluation of explainability, we introduce a new benchmark
featuring multi-format questions and rationales that supervise contextual
grounding and pattern-level semantics. Extensive experiments, including both
LLM-based and human evaluations, demonstrate that AXIS yields explanations of
significantly higher quality and achieves competitive detection accuracy
compared to general-purpose LLMs, specialized time-series LLMs, and time-series
Vision Language Models.

</details>


### [428] [Muon: Training and Trade-offs with Latent Attention and MoE](https://arxiv.org/abs/2509.24406)
*Sushant Mehta,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 对用于训练中小解码器（30M - 200M参数）Transformer的Muon优化器进行理论和实证研究，证明其优于AdamW，与现代技术结合有显著效率提升。


<details>
  <summary>Details</summary>
Motivation: 研究Muon优化器在训练中小解码器Transformer时的数学基础、收敛性及与现代架构优化的协同作用。

Method: 进行严格理论分析，包括收敛率、谱正则化等；开展大量实证实验。

Result: Muon在大批次训练时数据效率高，达到目标损失的计算量是AdamW的48 - 52%；与MLA和MoE结合有显著效率提升。

Conclusion: Muon是AdamW的可靠替代方案，尤其适合与现代效率技术和大批次训练结合。

Abstract: We present a comprehensive theoretical and empirical study of the Muon
optimizer for training transformers only with a small to medium decoder (30M -
200M parameters), with an emphasis on its mathematical foundations, convergence
properties and synergistic interactions with modern architectural
optimizations. Building on recent work showing Muon's scalability, we provide
rigorous theoretical analysis including: (i)showing the convergence rate under
standard assumptions, (ii) spectral regularization properties that prevent
gradient explosion, (iii) connection to natural gradient descent on the Stiefel
manifold, and (iv) equivalence to steepest gradient descent under the spectral
norm. Crucially, we demonstrate that Muon expands the Pareto frontier in the
compute-time trade-off by maintaining superior data efficiency at large batch
sizes, a key finding of~\cite{essentialai2025muon} that we validate across our
model scales. Empirically, Muon reaches the target loss with 48-52\% of the
training calculated by AdamW while maintaining or improving the final
perplexity, consistent with larger-scale results. When combined with Multi-Head
Latent Attention (MLA) and Mixture-of-Experts (MoE), we observe multiplicative
efficiency gains: MLA+MoE+Muon achieves 68\% memory reduction and 3.2$\times$
inference speedup, while improving perplexity by 8-12\%. We provide detailed
procedures on 15 architectural and optimizer components, stability analyzes
across 100+ training runs, and practical implementation guidelines including
Newton-Schulz coefficients $(3.4445, -4.7750, 2.0315)$ optimized
by~\cite{su2024muonblog}. Our theoretical analysis and comprehensive
experiments establish Muon as a principled, robust alternative to AdamW that
particularly excels when combined with modern efficiency techniques and
large-batch training regimes.

</details>


### [429] [ScatterAD: Temporal-Topological Scattering Mechanism for Time Series Anomaly Detection](https://arxiv.org/abs/2509.24414)
*Tao Yin,Xiaohong Zhang,Shaochen Fu,Zhibin Zhang,Li Huang,Yiyuan Yang,Kaixiang Yang,Meng Yan*

Main category: cs.LG

TL;DR: 本文针对工业物联网时间序列异常检测挑战，提出ScatterAD方法，利用样本散射现象建模时空异常，实验显示其达SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法独立建模时空依赖，导致表征学习不佳和对高维空间异常分散敏感性有限。

Method: 提出ScatterAD，结合拓扑编码器和时间编码器，引入对比融合机制，理论证明最大化条件互信息可提升跨视图一致性。

Result: 在多个公共基准上实验表明，ScatterAD在多变量时间序列异常检测上达SOTA水平。

Conclusion: ScatterAD有效利用样本散射现象，能提升工业物联网时间序列异常检测性能。

Abstract: One main challenge in time series anomaly detection for industrial IoT lies
in the complex spatio-temporal couplings within multivariate data. However,
traditional anomaly detection methods focus on modeling spatial or temporal
dependencies independently, resulting in suboptimal representation learning and
limited sensitivity to anomalous dispersion in high-dimensional spaces. In this
work, we conduct an empirical analysis showing that both normal and anomalous
samples tend to scatter in high-dimensional space, especially anomalous samples
are markedly more dispersed. We formalize this dispersion phenomenon as
scattering, quantified by the mean pairwise distance among sample
representations, and leverage it as an inductive signal to enhance
spatio-temporal anomaly detection. Technically, we propose ScatterAD to model
representation scattering across temporal and topological dimensions. ScatterAD
incorporates a topological encoder for capturing graph-structured scattering
and a temporal encoder for constraining over-scattering through mean squared
error minimization between neighboring time steps. We introduce a contrastive
fusion mechanism to ensure the complementarity of the learned temporal and
topological representations. Additionally, we theoretically show that
maximizing the conditional mutual information between temporal and topological
views improves cross-view consistency and enhances more discriminative
representations. Extensive experiments on multiple public benchmarks show that
ScatterAD achieves state-of-the-art performance on multivariate time series
anomaly detection. Code is available at this repository:
https://github.com/jk-sounds/ScatterAD.

</details>


### [430] [BiHDTrans: binary hyperdimensional transformer for efficient multivariate time series classification](https://arxiv.org/abs/2509.24425)
*Jingtao Zhang,Yi Liu,Qi Shen,Changhong Wang*

Main category: cs.LG

TL;DR: 提出BiHDTrans用于MTS分类，结合HD计算与Transformer优势，性能超SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 处理物联网设备产生的大量多变量时间序列数据，解决HD计算难捕捉复杂时间模式、Transformer计算和内存开销大的问题。

Method: 引入BiHDTrans，将自注意力集成到HD计算范式中。

Result: BiHDTrans在准确率、推理延迟等方面优于SOTA模型，降维后仍有竞争力。

Conclusion: BiHDTrans弥合了Transformer表达能力与HD计算效率之间的差距，实现准确、可扩展和低延迟的MTS分类。

Abstract: The proliferation of Internet-of-Things (IoT) devices has led to an
unprecedented volume of multivariate time series (MTS) data, requiring
efficient and accurate processing for timely decision-making in
resource-constrained edge environments. Hyperdimensional (HD) computing, with
its inherent efficiency and parallelizability, has shown promise in
classification tasks but struggles to capture complex temporal patterns, while
Transformers excel at sequence modeling but incur high computational and memory
overhead. We introduce BiHDTrans, an efficient neurosymbolic binary
hyperdimensional Transformer that integrates self-attention into the HD
computing paradigm, unifying the representational efficiency of HD computing
with the temporal modeling power of Transformers. Empirically, BiHDTrans
outperforms state-of-the-art (SOTA) HD computing models by at least 14.47% and
achieves 6.67% higher accuracy on average than SOTA binary Transformers. With
hardware acceleration on FPGA, our pipelined implementation leverages the
independent and identically distributed properties of high-dimensional
representations, delivering 39.4 times lower inference latency than SOTA binary
Transformers. Theoretical analysis shows that binarizing in holographic
high-dimensional space incurs significantly less information distortion than
directly binarizing neural networks, explaining BiHDTrans's superior accuracy.
Furthermore, dimensionality experiments confirm that BiHDTrans remains
competitive even with a 64% reduction in hyperspace dimensionality, surpassing
SOTA binary Transformers by 1-2% in accuracy with 4.4 times less model size, as
well as further reducing the latency by 49.8% compare to the full-dimensional
baseline. Together, these contributions bridge the gap between the
expressiveness of Transformers and the efficiency of HD computing, enabling
accurate, scalable, and low-latency MTS classification.

</details>


### [431] [Semantic Compression via Multimodal Representation Learning](https://arxiv.org/abs/2509.24431)
*Eleonora Grassucci,Giordano Cicchetti,Aurelio Uncini,Danilo Comminiello*

Main category: cs.LG

TL;DR: 本文探讨多模态表征学习中语义压缩问题，提出基于模态对齐的语义压缩方法，在大规模下游任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态表征学习虽能泛化但存在存储和处理的可扩展性挑战，需解决语义压缩问题，减少多模态嵌入的内存占用。

Method: 证明缩小模态差距与训练后语义压缩可行性的联系，基于此提出直接在预训练编码器上操作的语义压缩方法。

Result: 在多种大规模多模态下游任务中证明方法有效。

Conclusion: 模态对齐是语义压缩的关键，所提方法能实现显著压缩且不牺牲性能。

Abstract: Multimodal representation learning produces high-dimensional embeddings that
align diverse modalities in a shared latent space. While this enables strong
generalization, it also introduces scalability challenges, both in terms of
storage and downstream processing. A key open problem is how to achieve
semantic compression, reducing the memory footprint of multimodal embeddings
while preserving their ability to represent shared semantic content across
modalities. In this paper, we prove a strong connection between reducing the
modality gap, which is the residual separation of embeddings from different
modalities, and the feasibility of post-training semantic compression. When the
gap is sufficiently reduced, embeddings from different modalities but
expressing the same semantics share a common portion of the space. Therefore,
their centroid is a faithful representation of such a semantic concept. This
enables replacing multiple embeddings with a single centroid, yielding
significant memory savings. We propose a novel approach for semantic
compression grounded on the latter intuition, operating directly on pretrained
encoders. We demonstrate its effectiveness across diverse large-scale
multimodal downstream tasks. Our results highlight that modality alignment is a
key enabler for semantic compression, showing that the proposed approach
achieves significant compression without sacrificing performance.

</details>


### [432] [Distributionally Robust Federated Learning with Outlier Resilience](https://arxiv.org/abs/2509.24462)
*Zifan Wang,Xinlei Yi,Xenia Konti,Michael M. Zavlanos,Karl H. Johansson*

Main category: cs.LG

TL;DR: 现有基于DRO的联邦学习方法忽略局部数据集中异常值影响，本文提出具有异常值恢复能力的分布鲁棒联邦学习方法，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于DRO的联邦学习方法常忽略局部数据集中异常值对模型的不利影响，需解决此问题。

Method: 引入基于不平衡Wasserstein距离的新模糊集，将问题重新表述为易处理的拉格朗日惩罚优化问题，提出分布抗异常值鲁棒联邦学习算法。

Result: 提出的算法有收敛保证，在合成和真实数据集上的大量实验证明了方法的有效性。

Conclusion: 所提出的分布抗异常值鲁棒联邦学习算法能有效应对局部数据集中异常值，提升联邦学习性能。

Abstract: Federated learning (FL) enables collaborative model training without direct
data sharing, but its performance can degrade significantly in the presence of
data distribution perturbations. Distributionally robust optimization (DRO)
provides a principled framework for handling this by optimizing performance
against the worst-case distributions within a prescribed ambiguity set.
However, existing DRO-based FL methods often overlook the detrimental impact of
outliers in local datasets, which can disproportionately bias the learned
models. In this work, we study distributionally robust federated learning with
explicit outlier resilience. We introduce a novel ambiguity set based on the
unbalanced Wasserstein distance, which jointly captures geometric
distributional shifts and incorporates a non-geometric Kullback--Leibler
penalization to mitigate the influence of outliers. This formulation naturally
leads to a challenging min--max--max optimization problem. To enable
decentralized training, we reformulate the problem as a tractable Lagrangian
penalty optimization, which admits robustness certificates. Building on this
reformulation, we propose the distributionally outlier-robust federated
learning algorithm and establish its convergence guarantees. Extensive
experiments on both synthetic and real-world datasets demonstrate the
effectiveness of our approach.

</details>


### [433] [FS-KAN: Permutation Equivariant Kolmogorov-Arnold Networks via Function Sharing](https://arxiv.org/abs/2509.24472)
*Ran Elbaz,Guy Bar-Shalom,Yam Eitan,Fabrizio Frasca,Haggai Maron*

Main category: cs.LG

TL;DR: 本文提出Function Sharing KAN (FS - KAN)，用于构建任意排列对称群的等变和不变KA层，理论证明其与标准参数共享层有相同表达能力，实证表明在多数据类型和对称群下有更高数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏将等变Kolmogorov - Arnold Networks (KANs)应用于具有排列对称性数据的通用框架。

Method: 将参数共享方案推广到Kolmogorov - Arnold设置，推导FS - KAN层的基本构造，并进行理论分析。

Result: FS - KANs与使用标准参数共享层的网络有相同表达能力，在多数据类型和对称群上数据效率优于标准参数共享层。

Conclusion: FS - KANs在保持KANs可解释性和适应性的同时，在低数据情况下是优秀的架构选择。

Abstract: Permutation equivariant neural networks employing parameter-sharing schemes
have emerged as powerful models for leveraging a wide range of data symmetries,
significantly enhancing the generalization and computational efficiency of the
resulting models. Recently, Kolmogorov-Arnold Networks (KANs) have demonstrated
promise through their improved interpretability and expressivity compared to
traditional architectures based on MLPs. While equivariant KANs have been
explored in recent literature for a few specific data types, a principled
framework for applying them to data with permutation symmetries in a general
context remains absent. This paper introduces Function Sharing KAN (FS-KAN), a
principled approach to constructing equivariant and invariant KA layers for
arbitrary permutation symmetry groups, unifying and significantly extending
previous work in this domain. We derive the basic construction of these FS-KAN
layers by generalizing parameter-sharing schemes to the Kolmogorov-Arnold setup
and provide a theoretical analysis demonstrating that FS-KANs have the same
expressive power as networks that use standard parameter-sharing layers,
allowing us to transfer well-known and important expressivity results from
parameter-sharing networks to FS-KANs. Empirical evaluations on multiple data
types and symmetry groups show that FS-KANs exhibit superior data efficiency
compared to standard parameter-sharing layers, by a wide margin in certain
cases, while preserving the interpretability and adaptability of KANs, making
them an excellent architecture choice in low-data regimes.

</details>


### [434] [One-Prompt Strikes Back: Sparse Mixture of Experts for Prompt-based Continual Learning](https://arxiv.org/abs/2509.24483)
*Minh Le,Bao-Ngoc Dao,Huy Nguyen,Quyen Tran,Anh Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: 提出SMoPE框架调和任务特定和共享提示策略的权衡，在多个CL基准测试中表现出色，降低参数和计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的持续学习方法中，任务特定提示策略计算开销大、内存需求随任务数线性增长，共享提示策略因知识干扰性能下降，需调和两者权衡。

Method: 将共享提示组织成稀疏MoE架构中的多个“提示专家”，引入提示注意力分数聚合机制进行专家选择，提出自适应噪声机制，设计基于原型的损失函数。

Result: 在多个CL基准测试中，SMoPE始终优于任务特定提示方法，与最先进方法性能相当，显著降低参数数量和计算成本。

Conclusion: SMoPE有效调和了任务特定和共享提示策略的权衡，提高了持续学习的效率和性能。

Abstract: Prompt-based methods have recently gained prominence in Continual Learning
(CL) due to their strong performance and memory efficiency. A prevalent
strategy in this paradigm assigns a dedicated subset of prompts to each task,
which, while effective, incurs substantial computational overhead and causes
memory requirements to scale linearly with the number of tasks. Conversely,
approaches employing a single shared prompt across tasks offer greater
efficiency but often suffer from degraded performance due to knowledge
interference. To reconcile this trade-off, we propose SMoPE, a novel framework
that integrates the benefits of both task-specific and shared prompt
strategies. Inspired by recent findings on the relationship between Prefix
Tuning and Mixture of Experts (MoE), SMoPE organizes a shared prompt into
multiple "prompt experts" within a sparse MoE architecture. For each input,
only a select subset of relevant experts is activated, effectively mitigating
interference. To facilitate expert selection, we introduce a prompt-attention
score aggregation mechanism that computes a unified proxy score for each
expert, enabling dynamic and sparse activation. Additionally, we propose an
adaptive noise mechanism to encourage balanced expert utilization while
preserving knowledge from prior tasks. To further enhance expert
specialization, we design a prototype-based loss function that leverages prefix
keys as implicit memory representations. Extensive experiments across multiple
CL benchmarks demonstrate that SMoPE consistently outperforms task-specific
prompt methods and achieves performance competitive with state-of-the-art
approaches, all while significantly reducing parameter counts and computational
costs.

</details>


### [435] [Guided Uncertainty Learning Using a Post-Hoc Evidential Meta-Model](https://arxiv.org/abs/2509.24492)
*Charmaine Barker,Daniel Bethell,Simos Gerasimou*

Main category: cs.LG

TL;DR: 介绍轻量级证据学习元模型GUIDE，不改变基础模型，提升分布外检测和对抗攻击检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有事后方法在分布偏移下无法有效让模型学会何时不确定，可靠的不确定性量化是深度学习模型部署的障碍。

Method: 引入GUIDE，通过校准阶段识别显著内部特征，构建噪声驱动课程。不重新训练、不修改架构、不手动选择中间层。

Result: 避免从基础模型中提炼过度自信，分布外检测提升约77%，对抗攻击检测提升约80%，保留分布内性能。

Conclusion: GUIDE在不同基准上优于现有方法，表明主动引导不确定性对缩小预测置信度和可靠性差距有必要。

Abstract: Reliable uncertainty quantification remains a major obstacle to the
deployment of deep learning models under distributional shift. Existing
post-hoc approaches that retrofit pretrained models either inherit misplaced
confidence or merely reshape predictions, without teaching the model when to be
uncertain. We introduce GUIDE, a lightweight evidential learning meta-model
approach that attaches to a frozen deep learning model and explicitly learns
how and when to be uncertain. GUIDE identifies salient internal features via a
calibration stage, and then employs these features to construct a noise-driven
curriculum that teaches the model how and when to express uncertainty. GUIDE
requires no retraining, no architectural modifications, and no manual
intermediate-layer selection to the base deep learning model, thus ensuring
broad applicability and minimal user intervention. The resulting model avoids
distilling overconfidence from the base model, improves out-of-distribution
detection by ~77% and adversarial attack detection by ~80%, while preserving
in-distribution performance. Across diverse benchmarks, GUIDE consistently
outperforms state-of-the-art approaches, evidencing the need for actively
guiding uncertainty to close the gap between predictive confidence and
reliability.

</details>


### [436] [LLM DNA: Tracing Model Evolution via Functional Representations](https://arxiv.org/abs/2509.24496)
*Zhaomin Wu,Haodong Zhao,Ziyang Wang,Jizhou Guo,Qian Wang,Bingsheng He*

Main category: cs.LG

TL;DR: 文章提出LLM DNA解决大语言模型管理难题，推导提取流程，实验效果好，还构建了进化树。


<details>
  <summary>Details</summary>
Motivation: 大语言模型爆炸式增长，但模型间进化关系不明，现有方法有局限，导致模型管理困难。

Method: 受生物DNA启发，数学定义LLM DNA为低维双Lipschitz表示，推导无训练提取流程。

Result: 实验中LLM DNA与先前研究结果相符，特定任务表现优，还发现了模型间未记录的关系，构建的进化树与架构转变等相符。

Conclusion: LLM DNA是解决大语言模型管理难题的有效方法。

Abstract: The explosive growth of large language models (LLMs) has created a vast but
opaque landscape: millions of models exist, yet their evolutionary
relationships through fine-tuning, distillation, or adaptation are often
undocumented or unclear, complicating LLM management. Existing methods are
limited by task specificity, fixed model sets, or strict assumptions about
tokenizers or architectures. Inspired by biological DNA, we address these
limitations by mathematically defining LLM DNA as a low-dimensional,
bi-Lipschitz representation of functional behavior. We prove that LLM DNA
satisfies inheritance and genetic determinism properties and establish the
existence of DNA. Building on this theory, we derive a general, scalable,
training-free pipeline for DNA extraction. In experiments across 305 LLMs, DNA
aligns with prior studies on limited subsets and achieves superior or
competitive performance on specific tasks. Beyond these tasks, DNA comparisons
uncover previously undocumented relationships among LLMs. We further construct
the evolutionary tree of LLMs using phylogenetic algorithms, which align with
shifts from encoder-decoder to decoder-only architectures, reflect temporal
progression, and reveal distinct evolutionary speeds across LLM families.

</details>


### [437] [Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models](https://arxiv.org/abs/2509.24510)
*Jonas Hübotter,Patrik Wolf,Alexander Shevchenko,Dennis Jüni,Andreas Krause,Gil Kur*

Main category: cs.LG

TL;DR: 本文探讨测试时训练（TTT）有效性原因，提出基础模型全局欠参数化观点，建模验证并通过实验确认其在图像和语言任务中的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有研究对TTT有效的原因和时机理解有限，早期解释不适用于基础模型和多数测试数据为分布内的情况。

Method: 在线性表示假设下提出模型，通过在ImageNet上训练稀疏自动编码器验证假设，在图像和语言任务上进行扩展研究。

Result: 提出的模型显示TTT比全局训练能实现更小的分布内测试误差，实验验证了模型关键假设，确认了模型的实际意义。

Conclusion: 基础模型全局欠参数化，TTT是泛化后进行专业化的机制，能聚焦测试任务相关概念，确定了专业化最有效的范围。

Abstract: Recent empirical studies have explored the idea of continuing to train a
model at test-time for a given task, known as test-time training (TTT), and
have found it to yield significant performance improvements. However, there is
limited understanding of why and when TTT is effective. Earlier explanations
mostly focused on the observation that TTT may help when applied to
out-of-distribution adaptation or used with privileged data. However, the
growing scale of foundation models with most test data being in-distribution
questions these explanations. We instead posit that foundation models remain
globally underparameterized, with TTT providing a mechanism for specialization
after generalization, focusing capacity on concepts relevant to the test task.
Specifically, under the linear representation hypothesis, we propose a model in
which TTT achieves a substantially smaller in-distribution test error than
global training. We empirically validate our model's key assumptions by
training a sparse autoencoder on ImageNet, showing that semantically related
data points are explained by only a few shared concepts. Finally, we perform
scaling studies across image and language tasks that confirm the practical
implications of our model, identifying the regimes where specialization is most
effective.

</details>


### [438] [Trading Carbon for Physics: On the Resource Efficiency of Machine Learning for Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.24517)
*Sophia N. Wilson,Jens Hesselbjerg Christensen,Raghavendra Selvan*

Main category: cs.LG

TL;DR: 本文探讨物理归纳偏置在模型效能与效率间的权衡，以时空预测模型为例，表明嵌入物理归纳偏置可提升效率、减少碳足迹，模型效率应成开发核心考量。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习重模型效能，导致大模型资源需求大、碳足迹高，需探索效能与效率的平衡。

Method: 研究多种时空预测模型，将物理归纳偏置嵌入模型设计，使用标准物理信息时空模型和流匹配等新模型。

Result: 嵌入物理归纳偏置能在保持或提高效能的同时大幅提升效率，减少机器学习模型碳足迹。

Conclusion: 模型效率应与效能一同成为机器学习模型开发和部署的核心考量。

Abstract: Development of modern deep learning methods has been driven primarily by the
push for improving model efficacy (accuracy metrics). This sole focus on
efficacy has steered development of large-scale models that require massive
resources, and results in considerable carbon footprint across the model
life-cycle. In this work, we explore how physics inductive biases can offer
useful trade-offs between model efficacy and model efficiency (compute, energy,
and carbon). We study a variety of models for spatio-temporal forecasting, a
task governed by physical laws and well-suited for exploring different levels
of physics inductive bias. We show that embedding physics inductive biases into
the model design can yield substantial efficiency gains while retaining or even
improving efficacy for the tasks under consideration. In addition to using
standard physics-informed spatio-temporal models, we demonstrate the usefulness
of more recent models like flow matching as a general purpose method for
spatio-temporal forecasting. Our experiments show that incorporating physics
inductive biases offer a principled way to improve the efficiency and reduce
the carbon footprint of machine learning models. We argue that model
efficiency, along with model efficacy, should become a core consideration
driving machine learning model development and deployment.

</details>


### [439] [LEAF: A Robust Expert-Based Framework for Few-Shot Continual Event Detection](https://arxiv.org/abs/2509.24547)
*Bao-Ngoc Dao,Quang Nguyen,Luyen Ngo Dinh,Minh Le,Linh Ngo Van*

Main category: cs.LG

TL;DR: 提出LEAF框架解决少样本连续事件检测的挑战，实验证明其达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本连续事件检测方法存在全量微调导致严重遗忘、数据增强引入不自然输入的问题。

Method: 将混合专家架构集成到基础模型，用低秩适应矩阵参数化专家，采用语义感知专家选择机制，结合标签描述的对比学习目标，使用知识蒸馏策略。

Result: 在多个少样本连续事件检测基准测试中，LEAF始终达到了最先进的性能。

Conclusion: LEAF是一个新颖且强大的少样本连续事件检测框架，能有效解决现有方法的局限。

Abstract: Few-shot Continual Event Detection (FCED) poses the dual challenges of
learning from limited data and mitigating catastrophic forgetting across
sequential tasks. Existing approaches often suffer from severe forgetting due
to the full fine-tuning of a shared base model, which leads to knowledge
interference between tasks. Moreover, they frequently rely on data augmentation
strategies that can introduce unnatural or semantically distorted inputs. To
address these limitations, we propose LEAF, a novel and robust expert-based
framework for FCED. LEAF integrates a specialized mixture of experts
architecture into the base model, where each expert is parameterized with
low-rank adaptation (LoRA) matrices. A semantic-aware expert selection
mechanism dynamically routes instances to the most relevant experts, enabling
expert specialization and reducing knowledge interference. To improve
generalization in limited-data settings, LEAF incorporates a contrastive
learning objective guided by label descriptions, which capture high-level
semantic information about event types. Furthermore, to prevent overfitting on
the memory buffer, our framework employs a knowledge distillation strategy that
transfers knowledge from previous models to the current one. Extensive
experiments on multiple FCED benchmarks demonstrate that LEAF consistently
achieves state-of-the-art performance.

</details>


### [440] [Training-Free Multimodal Guidance for Video to Audio Generation](https://arxiv.org/abs/2509.24550)
*Eleonora Grassucci,Giuliano Galadini,Giordano Cicchetti,Aurelio Uncini,Fabio Antonacci,Danilo Comminiello*

Main category: cs.LG

TL;DR: 提出无训练的多模态引导机制用于视频到音频扩散，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有视频到音频生成方法存在需大规模成对数据集联合训练成本高或难以捕捉全局多模态一致性的问题。

Method: 提出一种无训练的多模态引导机制（MDG），利用模态嵌入的体积实现视频、音频和文本的统一对齐，可应用于任何预训练音频扩散模型。

Result: 在VGGSound和AudioCaps上的实验表明，MDG相比基线模型持续提高了感知质量和多模态对齐。

Conclusion: 联合多模态引导对视频到音频生成是有效的。

Abstract: Video-to-audio (V2A) generation aims to synthesize realistic and semantically
aligned audio from silent videos, with potential applications in video editing,
Foley sound design, and assistive multimedia. Although the excellent results,
existing approaches either require costly joint training on large-scale paired
datasets or rely on pairwise similarities that may fail to capture global
multimodal coherence. In this work, we propose a novel training-free multimodal
guidance mechanism for V2A diffusion that leverages the volume spanned by the
modality embeddings to enforce unified alignment across video, audio, and text.
The proposed multimodal diffusion guidance (MDG) provides a lightweight,
plug-and-play control signal that can be applied on top of any pretrained audio
diffusion model without retraining. Experiments on VGGSound and AudioCaps
demonstrate that our MDG consistently improves perceptual quality and
multimodal alignment compared to baselines, proving the effectiveness of a
joint multimodal guidance for V2A.

</details>


### [441] [Short window attention enables long-term memorization](https://arxiv.org/abs/2509.24552)
*Loïc Cabannes,Maximilian Beck,Gergely Szilvasy,Matthijs Douze,Maria Lomeli,Jade Copet,Pierre-Emmanuel Mazaré,Gabriel Synnaeve,Hervé Jégou*

Main category: cs.LG

TL;DR: 本文引入混合架构SWAX，发现大滑动窗口不提升长上下文性能，小滑动窗口不利于短上下文任务，采用随机改变窗口大小训练SWAX效果更佳。


<details>
  <summary>Details</summary>
Motivation: 现有混合架构中窗口长度影响和注意力与RNN层相互作用研究不足。

Method: 引入由滑动窗口注意力和xLSTM线性RNN层组成的SWAX架构，随机改变滑动窗口大小进行训练。

Result: 随机窗口大小训练的SWAX在长短上下文问题上显著优于常规窗口注意力。

Conclusion: 随机改变窗口大小能让SWAX更好利用长上下文窗口和xLSTM记忆，提升性能。

Abstract: Recent works show that hybrid architectures combining sliding window softmax
attention layers with linear recurrent neural network (RNN) layers outperform
both of these architectures taken separately. However, the impact of the window
length and the interplay between softmax attention and linear RNN layers remain
under-studied. In this work, we introduce SWAX, a hybrid architecture
consisting of sliding-window attention and xLSTM linear RNN layers.
  A counter-intuitive finding with SWAX is that larger sliding windows do not
improve the long-context performance. In fact, short window attention
encourages the model to better train the long-term memory of the xLSTM, by
relying less on the softmax attention mechanism for long context-retrieval.
  The issue with small sliding windows is that they are detrimental for
short-context tasks, which could be solved with information from moderately
larger sliding windows otherwise. Therefore, we train SWAX by stochastically
changing the sliding window size, forcing the model to leverage both a longer
context window and the xLSTM memory. SWAX trained with stochastic window sizes
significantly outperforms regular window attention both on short and
long-context problems.

</details>


### [442] [Deep Reinforcement Learning in Action: Real-Time Control of Vortex-Induced Vibrations](https://arxiv.org/abs/2509.24556)
*Hussam Sababha,Bernat Font,Mohammed Daqaq*

Main category: cs.LG

TL;DR: 本文展示了在高雷诺数下用深度强化学习（DRL）对圆柱涡激振动（VIV）进行主动流动控制（AFC）的实验部署，通过不同反馈方式使振动显著抑制，证明了DRL在AFC中的适应性和克服仪器限制的能力。


<details>
  <summary>Details</summary>
Motivation: 以往研究依赖低雷诺数数值模拟，本研究旨在在高雷诺数的具有挑战性的实验环境中进行实时控制，解决实际约束如执行器延迟。

Method: 采用旋转驱动，在学习算法中分别使用状态反馈和结合过去控制动作的方式，让DRL代理学习控制策略。

Result: 仅用状态反馈时，DRL代理学习低频旋转控制策略，实现高达80%的振动抑制；结合过去控制动作后，学习高频旋转控制策略，实现超95%的振动衰减。

Conclusion: DRL适用于现实实验中的AFC，且能克服执行滞后等仪器限制。

Abstract: This study showcases an experimental deployment of deep reinforcement
learning (DRL) for active flow control (AFC) of vortex-induced vibrations (VIV)
in a circular cylinder at a high Reynolds number (Re = 3000) using rotary
actuation. Departing from prior work that relied on low-Reynolds-number
numerical simulations, this research demonstrates real-time control in a
challenging experimental setting, successfully addressing practical constraints
such as actuator delay. When the learning algorithm is provided with state
feedback alone (displacement and velocity of the oscillating cylinder), the DRL
agent learns a low-frequency rotary control strategy that achieves up to 80%
vibration suppression which leverages the traditional lock-on phenomenon. While
this level of suppression is significant, it remains below the performance
achieved using high-frequency rotary actuation. The reduction in performance is
attributed to actuation delays and can be mitigated by augmenting the learning
algorithm with past control actions. This enables the agent to learn a
high-frequency rotary control strategy that effectively modifies vortex
shedding and achieves over 95% vibration attenuation. These results demonstrate
the adaptability of DRL for AFC in real-world experiments and its ability to
overcome instrumental limitations such as actuation lag.

</details>


### [443] [Emergent World Representations in OpenVLA](https://arxiv.org/abs/2509.24559)
*Marco Molinari,Leonardo Nevali,Saharsha Navani,Omar G. Younis*

Main category: cs.LG

TL;DR: 本文提出实验方法探究OpenVLA是否包含状态转移的潜在知识，发现其编码了内部世界模型，且世界模型随训练出现，还给出分析其世界模型的管道。


<details>
  <summary>Details</summary>
Motivation: 探究基于策略的强化学习训练的视觉语言动作模型（VLAs）是否隐式学习了世界模型。

Method: 使用嵌入算术对状态表示进行实验，通过线性和非线性探针测量顺序环境状态嵌入的差异，测试过渡向量能否从中间模型激活中恢复。

Result: OpenVLA在状态转移上有超越基线的显著预测能力，表明其编码了内部世界模型；早期检查点显示世界模型随训练出现。

Conclusion: OpenVLA编码了内部世界模型，且可利用稀疏自动编码器分析其世界模型。

Abstract: Vision Language Action models (VLAs) trained with policy-based reinforcement
learning (RL) encode complex behaviors without explicitly modeling
environmental dynamics. However, it remains unclear whether VLAs implicitly
learn world models, a hallmark of model-based RL. We propose an experimental
methodology using embedding arithmetic on state representations to probe
whether OpenVLA, the current state of the art in VLAs, contains latent
knowledge of state transitions. Specifically, we measure the difference between
embeddings of sequential environment states and test whether this transition
vector is recoverable from intermediate model activations. Using linear and non
linear probes trained on activations across layers, we find statistically
significant predictive ability on state transitions exceeding baselines
(embeddings), indicating that OpenVLA encodes an internal world model (as
opposed to the probes learning the state transitions). We investigate the
predictive ability of an earlier checkpoint of OpenVLA, and uncover hints that
the world model emerges as training progresses. Finally, we outline a pipeline
leveraging Sparse Autoencoders (SAEs) to analyze OpenVLA's world model.

</details>


### [444] [Learning to Solve Optimization Problems Constrained with Partial Differential Equations](https://arxiv.org/abs/2509.24573)
*Yusuf Guven,Vincenzo Di Vito,Ferdinando Fioretto*

Main category: cs.LG

TL;DR: 本文针对偏微分方程（PDE）约束优化问题计算要求高的挑战，引入基于学习的框架，验证表明该方法在解质量相当的情况下，计算速度大幅提升。


<details>
  <summary>Details</summary>
Motivation: PDE约束优化问题中决策变量与PDE状态变量紧密耦合，可行集由PDE约束隐式定义，导致问题计算要求高。

Method: 引入基于学习的框架，将动态预测器与优化替代器集成。动态预测器用时间离散神经算子近似系统轨迹，优化替代器利用代理优化技术近似最优决策。

Result: 在Burgers方程、热方程和电压调节等基准PDE约束优化任务上验证，解质量与经典控制算法相当，计算速度最多提升四个数量级。

Conclusion: 所提出的基于学习的框架能有效解决PDE约束优化问题，在保证解质量的同时显著提高计算速度。

Abstract: Partial differential equation (PDE)-constrained optimization arises in many
scientific and engineering domains, such as energy systems, fluid dynamics and
material design. In these problems, the decision variables (e.g., control
inputs or design parameters) are tightly coupled with the PDE state variables,
and the feasible set is implicitly defined by the governing PDE constraints.
This coupling makes the problems computationally demanding, as it requires
handling high dimensional discretization and dynamic constraints. To address
these challenges, this paper introduces a learning-based framework that
integrates a dynamic predictor with an optimization surrogate. The dynamic
predictor, a novel time-discrete Neural Operator (Lu et al.), efficiently
approximate system trajectories governed by PDE dynamics, while the
optimization surrogate leverages proxy optimizer techniques (Kotary et al.) to
approximate the associated optimal decisions. This dual-network design enables
real-time approximation of optimal strategies while explicitly capturing the
coupling between decisions and PDE dynamics. We validate the proposed approach
on benchmark PDE-constrained optimization tasks inlacing Burgers' equation,
heat equation and voltage regulation, and demonstrate that it achieves solution
quality comparable to classical control-based algorithms, such as the Direct
Method and Model Predictive Control (MPC), while providing up to four orders of
magnitude improvement in computational speed.

</details>


### [445] [SAIP: A Plug-and-Play Scale-adaptive Module in Diffusion-based Inverse Problems](https://arxiv.org/abs/2509.24580)
*Lingyu Wang,Xiangming Meng*

Main category: cs.LG

TL;DR: 提出SAIP模块解决扩散模型逆问题中固定权重平衡先验和似然贡献的不足，提升图像恢复质量。


<details>
  <summary>Details</summary>
Motivation: 现有解决扩散模型逆问题的方法依赖固定、手动调整的权重平衡先验和似然贡献，静态设计不理想，限制性能和泛化能力。

Method: 提出SAIP模块，在不重新训练或改变扩散主干的情况下，自适应地在每个时间步优化权重，并能无缝集成到现有采样器中。

Result: SAIP在多种图像恢复任务中，包括具有挑战性的场景，持续提高了重建质量。

Conclusion: SAIP是一个即插即用的模块，能有效解决现有方法的不足，提升图像恢复性能。

Abstract: Solving inverse problems with diffusion models has shown promise in tasks
such as image restoration. A common approach is to formulate the problem in a
Bayesian framework and sample from the posterior by combining the prior score
with the likelihood score. Since the likelihood term is often intractable,
estimators like DPS, DMPS, and $\pi$GDM are widely adopted. However, these
methods rely on a fixed, manually tuned scale to balance prior and likelihood
contributions. Such a static design is suboptimal, as the ideal balance varies
across timesteps and tasks, limiting performance and generalization. To address
this issue, we propose SAIP, a plug-and-play module that adaptively refines the
scale at each timestep without retraining or altering the diffusion backbone.
SAIP integrates seamlessly into existing samplers and consistently improves
reconstruction quality across diverse image restoration tasks, including
challenging scenarios.

</details>


### [446] [CURA: Size Isnt All You Need -- A Compact Universal Architecture for On-Device Intelligence](https://arxiv.org/abs/2509.24601)
*Jae-Bum Seo,Muhammad Salman,Lismer Andres Caceres-Najarro*

Main category: cs.LG

TL;DR: 提出受模拟音频信号处理电路启发的CURA架构，解决现有设备端AI架构在资源受限环境下的问题，评估显示其在紧凑性、泛化性和复杂模式识别上有优势。


<details>
  <summary>Details</summary>
Motivation: 现有设备端AI架构在资源受限环境下缺乏紧凑性和泛化性，无法适应不同任务和领域。

Method: 提出CURA架构，在多个数据集和领域进行评估。

Result: 紧凑性上使用参数最多减少2500倍；泛化性在多个基准测试表现一致，F1分数达90%；复杂模式识别上误差更低。

Conclusion: CURA架构在紧凑性、泛化性和复杂模式识别上优于现有方法。

Abstract: Existing on-device AI architectures for resource-constrained environments
face two critical limitations: they lack compactness, with parameter
requirements scaling proportionally to task complexity, and they exhibit poor
generalizability, performing effectively only on specific application domains
(e.g., models designed for regression tasks cannot adapt to natural language
processing (NLP) applications). In this paper, we propose CURA, an architecture
inspired by analog audio signal processing circuits that provides a compact and
lightweight solution for diverse machine learning tasks across multiple
domains. Our architecture offers three key advantages over existing approaches:
(1) Compactness: it requires significantly fewer parameters regardless of task
complexity; (2) Generalizability: it adapts seamlessly across regression,
classification, complex NLP, and computer vision tasks; and (3) Complex pattern
recognition: it can capture intricate data patterns while maintaining extremely
low model complexity. We evaluated CURA across diverse datasets and domains.
For compactness, it achieved equivalent accuracy using up to 2,500 times fewer
parameters compared to baseline models. For generalizability, it demonstrated
consistent performance across four NLP benchmarks and one computer vision
dataset, nearly matching specialized existing models (achieving F1-scores up to
90%). Lastly, it delivers superior forecasting accuracy for complex patterns,
achieving 1.6 times lower mean absolute error and 2.1 times lower mean squared
error than competing models.

</details>


### [447] [OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment](https://arxiv.org/abs/2509.24610)
*Liang Lin,Zhihao Xu,Junhao Dong,Jian Zhao,Yuchen Yuan,Guibin Zhang,Miao Yu,Yiming Zhang,Zhengtao Yao,Huahui Yi,Dongrui Liu,Xinfeng Li,Kun Wang*

Main category: cs.LG

TL;DR: 提出OrthAlign方法解决大语言模型多目标偏好对齐中梯度冲突问题，实验显示有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略在参数层面解决大语言模型多目标偏好对齐冲突，本文旨在解决该问题。

Method: 提出OrthAlign方法，利用正交子空间分解将参数更新空间分解，确保优化方向不干扰，并给出理论保证。

Result: OrthAlign在多目标对齐后单偏好最大提升34.61%-50.89%，总体奖励平均提升13.96%。

Conclusion: OrthAlign能有效解决大语言模型多目标偏好对齐的梯度冲突问题，实现稳定收敛和性能提升。

Abstract: Large language model (LLM) alignment faces a critical dilemma when addressing
multiple human preferences: improvements in one dimension frequently come at
the expense of others, creating unavoidable trade-offs between competing
objectives like helpfulness and harmlessness. While prior work mainly focuses
on constraint-based optimization algorithms and data selection strategies to
mitigate conflicts, these approaches overlook the fundamental issue of
resolving conflicts directly at the parameter level. In this paper, we present
OrthAlign, an innovative approach that pioneers a new paradigm by leveraging
orthogonal subspace decomposition to fundamentally resolve gradient-level
conflicts in multi-objective preference alignment. OrthAlign strategically
decomposes parameter update spaces into orthogonal subspaces, ensuring that
optimization toward different preferences occurs in mathematically
non-interfering directions. Building upon this, we provide theoretical
guarantees demonstrating that when parameter increments satisfy both orthogonal
subspace constraints and spectral norm bounds, the resulting updates exhibit
linear Lipschitz growth rather than exponential instability, ensuring stable
convergence across all preference dimensions. Extensive experiments show that:
I. OrthAlign achieves maximum single-preference improvements ranging from
34.61% to 50.89% after multiple-objective alignment across helpful, harmless,
and truthful dimensions. II. With an average overall reward improvement of
13.96%.

</details>


### [448] [Learning Hamiltonian Dynamics at Scale: A Differential-Geometric Approach](https://arxiv.org/abs/2509.24627)
*Katharina Friedl,Noémie Jaquier,Mika Liao,Danica Kragic*

Main category: cs.LG

TL;DR: 本文提出几何降阶哈密顿神经网络（RO - HNN），结合哈密顿力学守恒定律与模型降阶可扩展性，实验表明其能对复杂高维动力学进行有效预测。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入物理直觉的网络模型在扩展到高维系统时存在挑战，需要解决可扩展性问题。

Method: 引入RO - HNN，由几何约束辛自编码器学习低维、保结构的辛子流形，以及几何哈密顿神经网络对该子流形上的动力学建模。

Result: RO - HNN能对复杂高维动力学提供符合物理规律、稳定且可泛化的预测。

Conclusion: RO - HNN有效将哈密顿神经网络的适用范围扩展到高维物理系统。

Abstract: By embedding physical intuition, network architectures enforce fundamental
properties, such as energy conservation laws, leading to plausible predictions.
Yet, scaling these models to intrinsically high-dimensional systems remains a
significant challenge. This paper introduces Geometric Reduced-order
Hamiltonian Neural Network (RO-HNN), a novel physics-inspired neural network
that combines the conservation laws of Hamiltonian mechanics with the
scalability of model order reduction. RO-HNN is built on two core components: a
novel geometrically-constrained symplectic autoencoder that learns a
low-dimensional, structure-preserving symplectic submanifold, and a geometric
Hamiltonian neural network that models the dynamics on the submanifold. Our
experiments demonstrate that RO-HNN provides physically-consistent, stable, and
generalizable predictions of complex high-dimensional dynamics, thereby
effectively extending the scope of Hamiltonian neural networks to
high-dimensional physical systems.

</details>


### [449] [Identity Bridge: Enabling Implicit Reasoning via Shared Latent Memory](https://arxiv.org/abs/2509.24653)
*Pengxiao Lin,Zheng-An Chen,Zhi-Qin John Xu*

Main category: cs.LG

TL;DR: 本文提出Identity Bridge机制解决大语言模型组合推理问题，通过理论分析解释现象，还在大规模模型上观察到其两跳推理能力及潜在启发。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在组合推理任务中常失败，存在'两跳推理诅咒'现象，需解决组合性差距问题。

Method: 引入Identity Bridge机制，对模型进行零跳身份任务监督；用简化Emb - MLP模型进行理论分析；在复杂任务中用小初始化或权重衰减增强正则化效果。

Result: 使模型能成功进行分布外两跳推理；证明身份监督重塑模型潜在几何结构；大规模模型可通过潜在记忆实现两跳推理。

Conclusion: Identity Bridge机制有效解决大语言模型组合推理问题，为增强其隐式推理能力提供关键启发。

Abstract: Despite remarkable advances, large language models often fail at
compositional reasoning tasks, a phenomenon exemplified by the ``curse of
two-hop reasoning''. This paper introduces the Identity Bridge, a simple yet
powerful mechanism that resolves this compositionality gap by supervising the
model on a zero-hop identity task. We demonstrate empirically that this
addition enables models to successfully perform out-of-distribution two-hop
reasoning, a task they otherwise completely fail. To explain this phenomenon,
we provide a theoretical analysis using a simplified Emb-MLP model, proving
that identity supervision reshapes the model's latent geometry. We show this
alignment is induced by an implicit nuclear-norm regularization during
optimization, which favors low-rank solutions that share structure across
tasks. For complex tasks, we use small initialization or weight decay to
enhance the regularization effect, which enhances the latent space alignment
effect and slows down the generalization decay. Finally, we extend our
investigation to large-scale models, observing that they still achieve two-hop
reasoning through the latent memory, which provides crucial inspiration for
enhancing their implicit reasoning abilities.

</details>


### [450] [HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling](https://arxiv.org/abs/2509.24655)
*Max van Spengler,Artem Moskalev,Tommaso Mansi,Mangal Prakash,Rui Liao*

Main category: cs.LG

TL;DR: 提出HyperHELM框架，在双曲空间对mRNA序列进行掩码语言模型预训练，在多任务中表现优于欧式基线。


<details>
  <summary>Details</summary>
Motivation: 语言模型默认欧式几何与生物数据的层次结构不匹配，双曲几何未用于mRNA序列语言建模。

Method: 引入HyperHELM框架，采用双曲层置于欧式骨干之上的混合设计。

Result: 在多个多物种数据集的10项任务中，9项优于欧式基线，平均提升10%，在分布外泛化和抗体区域注释上表现出色。

Conclusion: 双曲几何是mRNA序列层次语言建模的有效归纳偏置。

Abstract: Language models are increasingly applied to biological sequences like
proteins and mRNA, yet their default Euclidean geometry may mismatch the
hierarchical structures inherent to biological data. While hyperbolic geometry
provides a better alternative for accommodating hierarchical data, it has yet
to find a way into language modeling for mRNA sequences. In this work, we
introduce HyperHELM, a framework that implements masked language model
pre-training in hyperbolic space for mRNA sequences. Using a hybrid design with
hyperbolic layers atop Euclidean backbone, HyperHELM aligns learned
representations with the biological hierarchy defined by the relationship
between mRNA and amino acids. Across multiple multi-species datasets, it
outperforms Euclidean baselines on 9 out of 10 tasks involving property
prediction, with 10% improvement on average, and excels in out-of-distribution
generalization to long and low-GC content sequences; for antibody region
annotation, it surpasses hierarchy-aware Euclidean models by 3% in annotation
accuracy. Our results highlight hyperbolic geometry as an effective inductive
bias for hierarchical language modeling of mRNA sequences.

</details>


### [451] [T-POP: Test-Time Personalization with Online Preference Feedback](https://arxiv.org/abs/2509.24696)
*Zikun Qu,Min Zhang,Mingze Kong,Xiang Li,Zhiwei Shang,Zhiyong Wang,Yikun Ban,Shuang Qiu,Yao Shu,Zhongxiang Dai*

Main category: cs.LG

TL;DR: 提出T - POP算法解决大语言模型个性化冷启动问题，实验显示其效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型个性化方法不适用于新用户，存在冷启动问题。

Method: 引入实时个性化新范式，提出T - POP算法，结合测试时间对齐和决斗多臂老虎机，在线学习奖励函数引导解码过程。

Result: T - POP实现快速且数据高效的个性化，显著优于现有基线，随用户交互增多持续改进。

Conclusion: T - POP算法能有效解决大语言模型个性化的冷启动问题。

Abstract: Personalizing large language models (LLMs) to individual user preferences is
a critical step beyond generating generically helpful responses. However,
current personalization methods are ill-suited for new users, as they typically
require either slow, resource-intensive fine-tuning or a substantial amount of
pre-existing user data, creating a significant cold-start problem. To address
this challenge, we introduce a new paradigm for real-time personalization by
learning from online pairwise preference feedback collected during text
generation. We propose T-POP (Test-Time Personalization with Online Preference
Feedback}), a novel algorithm that synergistically combines test-time alignment
with dueling bandits. Without updating the LLM parameters, T-POP steers the
decoding process of a frozen LLM by learning a reward function online that
captures user preferences. By leveraging dueling bandits, T-POP intelligently
queries the user to efficiently balance between exploring their preferences and
exploiting the learned knowledge to generate personalized text. Extensive
experiments demonstrate that T-POP achieves rapid and data-efficient
personalization, significantly outperforming existing baselines and showing
consistent improvement with more user interactions.

</details>


### [452] [FedPOB: Sample-Efficient Federated Prompt Optimization via Bandits](https://arxiv.org/abs/2509.24701)
*Pingchen Lu,Zhi Hong,Zhiwei Shang,Zhiyong Wang,Yikun Ban,Yao Shu,Min Zhang,Shuang Qiu,Zhongxiang Dai*

Main category: cs.LG

TL;DR: 论文提出基于多臂老虎机的样本高效联邦提示优化框架，含FedPOB和FedPOB - Pref算法，实验表明其优于基线且参与协作的代理越多性能越好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型性能对输入提示敏感，但现实应用受强大专有模型黑盒性、高样本效率需求和隐私保护协作需求的阻碍。

Method: 引入基于多臂老虎机的框架，提出FedPOB算法，又扩展到比较用户反馈场景，提出FedPOB - Pref算法。

Result: FedPOB和FedPOB - Pref显著优于现有基线，且参与协作的代理越多性能越好。

Conclusion: 所提出的联邦方法有效。

Abstract: The performance of large language models (LLMs) is highly sensitive to the
input prompt, making prompt optimization a critical task. However, real-world
application is hindered by three major challenges: (1) the black-box nature of
powerful proprietary LLMs, (2) the need for high sample efficiency due to query
costs, and (3) the desire for privacy-preserving collaboration among multiple
users. To address these challenges simultaneously, we introduce a novel
framework for sample-efficient federated prompt optimization based on
multi-armed bandits (MABs). The MAB framework is uniquely suited for this
problem as it is (1) inherently a black-box optimization method, (2)
practically sample-efficient, and (3) enables collaborative learning with
theoretically guaranteed benefit from more participating agents. We first
propose the Federated Prompt Optimization via Bandits (FedPOB) algorithm, a
federated variant of the Linear UCB algorithm, where agents collaborate by
sharing model parameters instead of raw data. We then extend our approach to
the practical setting of comparative user feedback by introducing FedPOB with
Preference Feedback (FedPOB-Pref), an efficient algorithm based on federated
dueling bandits. Extensive experiments demonstrate that both FedPOB and
FedPOB-Pref significantly outperform existing baselines and that their
performance consistently improves as more agents participate in the
collaboration, validating the effectiveness of our federated approach.

</details>


### [453] [Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF](https://arxiv.org/abs/2509.24713)
*Jing Liu*

Main category: cs.LG

TL;DR: 提出机械可解释性框架识别奖励模型中处理罕见事件的神经回路，引入CART方法改进长尾鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 强化学习从人类反馈的奖励模型在长尾分布上存在系统故障，导致奖励破解和失调。

Method: 提出机械可解释性框架，假设奖励模型有处理长尾场景的不同功能回路，建立理论联系，引入CART方法，用回路分析指导数据增强、正则化和集成策略。

Result: 该方法既提供奖励模型故障的理论见解，也有改善长尾鲁棒性的实际干预措施。

Conclusion: 所提框架和方法有助于解决奖励模型在长尾分布上的问题。

Abstract: Reinforcement Learning from Human Feedback (RLHF) reward models exhibit
systematic failures on longtail distributions, leading to reward hacking and
misalignment. We propose a mechanistic interpretability framework that
identifies specialized neural circuits responsible for rare-event processing in
reward models. Drawing from recent advances showing distributed specialization
for rare tokens in language models\citep{liu2025no, liu2025emergent}, we
hypothesize that reward models also develop functionally distinct circuits for
longtail scenarios. Our theoretical framework establishes formal connections
between circuit specialization, reward generalization bounds, and longtail
performance. We introduce \textbf{Circuit-Aware Reward Training (CART)}, which
uses circuit analysis to guide data augmentation, regularization, and ensemble
strategies. This approach provides both theoretical insights into reward model
failures and practical interventions for improving longtail robustness.

</details>


### [454] [Discrete Variational Autoencoding via Policy Search](https://arxiv.org/abs/2509.24716)
*Michael Drolet,Firas Al-Hafez,Aditya Bhatt,Jan Peters,Oleg Arenz*

Main category: cs.LG

TL;DR: 提出离散变分自编码器（VAE）训练框架，在ImageNet等数据集表现出色，FID分数提升20%。


<details>
  <summary>Details</summary>
Motivation: 离散随机变量无法精确可微参数化，现有离散VAE依赖近似方法或高方差无梯度方法，在高维任务表现有限。

Method: 利用非参数编码器的自然梯度更新参数编码器，无需重参数化，结合自动步长调整和基于Transformer的编码器。

Result: 可扩展到如ImageNet等具有挑战性的数据集，在从紧凑潜空间重建高维数据方面优于近似重参数化方法和基于量化的离散自编码器，ImageNet 256的FID分数提升20%。

Conclusion: 所提训练框架有效，能提升离散VAE在高维数据重建任务中的性能。

Abstract: Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit
efficiency and can be modeled with autoregressive discrete distributions,
enabling parameter-efficient multimodal search with transformers. However,
discrete random variables do not allow for exact differentiable
parameterization; therefore, discrete VAEs typically rely on approximations,
such as Gumbel-Softmax reparameterization or straight-through gradient
estimates, or employ high-variance gradient-free methods such as REINFORCE that
have had limited success on high-dimensional tasks such as image
reconstruction. Inspired by popular techniques in policy search, we propose a
training framework for discrete VAEs that leverages the natural gradient of a
non-parametric encoder to update the parametric encoder without requiring
reparameterization. Our method, combined with automatic step size adaptation
and a transformer-based encoder, scales to challenging datasets such as
ImageNet and outperforms both approximate reparameterization methods and
quantization-based discrete autoencoders in reconstructing high-dimensional
data from compact latent spaces, achieving a 20% improvement on FID Score for
ImageNet 256.

</details>


### [455] [Q-Net: Transferable Queue Length Estimation via Kalman-based Neural Networks](https://arxiv.org/abs/2509.24725)
*Ting Gao,Elvin Isufi,Winnie Daamen,Erik-Sander Smits,Serge Hoogendoorn*

Main category: cs.LG

TL;DR: 本文提出Q - Net框架用于信号交叉口队列长度估计，融合两种数据，采用特定模型和滤波器，在荷兰鹿特丹主干道评估中表现优于基线方法，有良好可转移性，还提出实时变体。


<details>
  <summary>Details</summary>
Motivation: 解决信号交叉口队列长度估计难题，特别是部分观测条件下的估计问题，且在交通守恒假设不成立时仍能稳健估计。

Method: 引入Q - Net框架，整合车辆计数和聚合浮动车数据，采用定制状态空间模型和AI增强卡尔曼滤波器KalmanNet，改进其管道以实现空间可转移性。

Result: 在荷兰鹿特丹主干道评估中，Q - Net在均方根误差上比基线方法优超60%以上，能准确跟踪队列形成和消散，修正数据延迟，有强时空可转移性。

Conclusion: Q - Net是数据高效且可解释的队列长度估计框架，有潜力集成到基于队列的动态交通控制系统中。

Abstract: Estimating queue lengths at signalized intersections remains a challenge in
traffic management, especially under partially observed conditions where
vehicle flows are not fully captured. This paper introduces Q-Net, a
data-efficient and interpretable framework for queue length estimation that
performs robustly even when traffic conservation assumptions are violated.
Q-Net integrates two widely available and privacy-friendly data sources: (i)
vehicle counts from loop detectors near stop lines, and (ii) aggregated
floating car data (aFCD), which divides each road section into segments and
provides segment-wise average speed measurements. These data sources often
differ in spatial and temporal resolution, creating fusion challenges. Q-Net
addresses this by employing a tailored state-space model and an AI-augmented
Kalman filter, KalmanNet, which learns the Kalman gain from data without
requiring prior knowledge of noise covariances or full system dynamics. We
build on the vanilla KalmanNet pipeline to decouple measurement dimensionality
from section length, enabling spatial transferability across road segments.
Unlike black-box models, Q-Net maintains physical interpretability, with
internal variables linked to real-world traffic dynamics. Evaluations on main
roads in Rotterdam, the Netherlands, demonstrate that Q-Net outperforms
baseline methods by over 60\% in Root Mean Square Error (RMSE), accurately
tracking queue formation and dissipation while correcting aFCD-induced delays.
Q-Net also demonstrates strong spatial and temporal transferability, enabling
deployment without costly sensing infrastructure like cameras or radar.
Additionally, we propose a real-time variant of Q-Net, highlighting its
potential for integration into dynamic, queue-based traffic control systems.

</details>


### [456] [A TRIANGLE Enables Multimodal Alignment Beyond Cosine Similarity](https://arxiv.org/abs/2509.24734)
*Giordano Cicchetti,Eleonora Grassucci,Danilo Comminiello*

Main category: cs.LG

TL;DR: 本文提出TRIANGLE方法，通过三角形面积相似度改善多模态联合对齐，提升多模态建模性能，在多模态任务中取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的多模态学习模型存在局限性，无法有效对齐所有模态，影响下游任务效果。

Method: 提出TRIANGLE方法，直接在模态嵌入的高维空间中计算相似度，通过三角形面积相似度改善三种模态的联合对齐，避免额外融合层或成对相似度。

Result: 将TRIANGLE应用于对比损失中，显著提升了多模态建模性能，在视频 - 文本、音频 - 文本检索或音频 - 视频分类等三模态任务中，比基于余弦相似度的方法在Recall@1上最多提高9个点。

Conclusion: TRIANGLE方法在多模态任务中能取得最优结果，可有效提升多模态建模性能，并能给出可解释的对齐理由。

Abstract: Multimodal learning plays a pivotal role in advancing artificial intelligence
systems by incorporating information from multiple modalities to build a more
comprehensive representation. Despite its importance, current state-of-the-art
models still suffer from severe limitations that prevent the successful
development of a fully multimodal model. Such methods may not provide
indicators that all the involved modalities are effectively aligned. As a
result, some modalities may not be aligned, undermining the effectiveness of
the model in downstream tasks where multiple modalities should provide
additional information that the model fails to exploit. In this paper, we
present TRIANGLE: TRI-modAl Neural Geometric LEarning, the novel proposed
similarity measure that is directly computed in the higher-dimensional space
spanned by the modality embeddings. TRIANGLE improves the joint alignment of
three modalities via a triangle-area similarity, avoiding additional fusion
layers or pairwise similarities. When incorporated in contrastive losses
replacing cosine similarity, TRIANGLE significantly boosts the performance of
multimodal modeling, while yielding interpretable alignment rationales.
Extensive evaluation in three-modal tasks such as video-text and audio-text
retrieval or audio-video classification, demonstrates that TRIANGLE achieves
state-of-the-art results across different datasets improving the performance of
cosine-based methods up to 9 points of Recall@1.

</details>


### [457] [Robust Policy Expansion for Offline-to-Online RL under Diverse Data Corruption](https://arxiv.org/abs/2509.24748)
*Longxiang He,Deheng Ye,Junbo Tan,Xueqian Wang,Li Shen*

Main category: cs.LG

TL;DR: 文章针对O2O RL在数据损坏下性能下降问题，提出RPEX方法，实验显示其在多种数据损坏场景达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有研究未关注O2O RL在数据损坏下的鲁棒性，数据损坏会降低在线探索效率。

Method: 将逆概率加权（IPW）融入在线探索策略以减轻重尾性，提出RPEX方法。

Result: 在D4RL数据集的大量实验中，RPEX在多种数据损坏场景实现SOTA的O2O性能。

Conclusion: RPEX方法简单有效，能提升O2O RL在数据损坏场景下的性能。

Abstract: Pretraining a policy on offline data followed by fine-tuning through online
interactions, known as Offline-to-Online Reinforcement Learning (O2O RL), has
emerged as a promising paradigm for real-world RL deployment. However, both
offline datasets and online interactions in practical environments are often
noisy or even maliciously corrupted, severely degrading the performance of O2O
RL. Existing works primarily focus on mitigating the conservatism of offline
policies via online exploration, while the robustness of O2O RL under data
corruption, including states, actions, rewards, and dynamics, is still
unexplored. In this work, we observe that data corruption induces heavy-tailed
behavior in the policy, thereby substantially degrading the efficiency of
online exploration. To address this issue, we incorporate Inverse Probability
Weighted (IPW) into the online exploration policy to alleviate
heavy-tailedness, and propose a novel, simple yet effective method termed
$\textbf{RPEX}$: $\textbf{R}$obust $\textbf{P}$olicy $\textbf{EX}$pansion.
Extensive experimental results on D4RL datasets demonstrate that RPEX achieves
SOTA O2O performance across a wide range of data corruption scenarios. Code is
available at
$\href{https://github.com/felix-thu/RPEX}{https://github.com/felix-thu/RPEX}$.

</details>


### [458] [In-Context Learning of Temporal Point Processes with Foundation Inference Models](https://arxiv.org/abs/2509.24762)
*David Berghaus,Patrick Seifner,Kostadin Cvejoski,César Ojeda,Ramsés J. Sánchez*

Main category: cs.LG

TL;DR: 本文提出一种新的点过程推理方法FIM - PP，在合成数据集上预训练，能在真实数据上直接估计或快速微调，实验显示其性能与专业模型相当。


<details>
  <summary>Details</summary>
Motivation: 当前MTPP的神经网络推理方法需为每个目标系统训练专门模型，本文寻求不同方法。

Method: 利用摊销推理和上下文学习，在从广泛的Hawkes过程分布中采样的大型合成MTPP数据集上预训练深度神经网络FIM - PP，以进行上下文推理。

Result: 该摊销方法在常见基准数据集的下一事件预测中达到了与专门模型相当的性能。

Conclusion: FIM - PP可在无需额外训练的情况下估计真实世界数据中的MTPP，或快速微调以适应目标系统。

Abstract: Modeling event sequences of multiple event types with marked temporal point
processes (MTPPs) provides a principled way to uncover governing dynamical
rules and predict future events. Current neural network approaches to MTPP
inference rely on training separate, specialized models for each target system.
We pursue a radically different approach: drawing on amortized inference and
in-context learning, we pretrain a deep neural network to infer, in-context,
the conditional intensity functions of event histories from a context defined
by sets of event sequences. Pretraining is performed on a large synthetic
dataset of MTPPs sampled from a broad distribution of Hawkes processes. Once
pretrained, our Foundation Inference Model for Point Processes (FIM-PP) can
estimate MTPPs from real-world data without any additional training, or be
rapidly finetuned to target systems. Experiments show that this amortized
approach matches the performance of specialized models on next-event prediction
across common benchmark datasets.
  Our pretrained model, repository and tutorials will soon be available online

</details>


### [459] [Neural Message-Passing on Attention Graphs for Hallucination Detection](https://arxiv.org/abs/2509.24770)
*Fabrizio Frasca,Guy Bar-Shalom,Yftah Ziser,Haggai Maron*

Main category: cs.LG

TL;DR: 提出CHARM方法统一信号检测大语言模型幻觉，实验表现优且有零样本跨数据集迁移能力


<details>
  <summary>Details</summary>
Motivation: 大语言模型常产生幻觉内容，现有检测方法依赖启发式或简单模型处理孤立计算痕迹

Method: 将信号表示为属性图，把幻觉检测转化为图学习任务，用图神经网络处理

Result: CHARM理论上包含先前基于注意力的启发式方法，实验中在多个基准测试上始终优于其他领先方法，有零样本跨数据集迁移能力

Conclusion: 图结构和组合计算痕迹有益，CHARM有良好表现

Abstract: Large Language Models (LLMs) often generate incorrect or unsupported content,
known as hallucinations. Existing detection methods rely on heuristics or
simple models over isolated computational traces such as activations, or
attention maps. We unify these signals by representing them as attributed
graphs, where tokens are nodes, edges follow attentional flows, and both carry
features from attention scores and activations. Our approach, CHARM, casts
hallucination detection as a graph learning task and tackles it by applying
GNNs over the above attributed graphs. We show that CHARM provably subsumes
prior attention-based heuristics and, experimentally, it consistently
outperforms other leading approaches across diverse benchmarks. Our results
shed light on the relevant role played by the graph structure and on the
benefits of combining computational traces, whilst showing CHARM exhibits
promising zero-shot performance on cross-dataset transfer.

</details>


### [460] [MarS-FM: Generative Modeling of Molecular Dynamics via Markov State Models](https://arxiv.org/abs/2509.24779)
*Kacper Kapuśniak,Cristian Gabellini,Michael Bronstein,Prudencio Tossou,Francesco Di Giovanni*

Main category: cs.LG

TL;DR: 本文提出新生成模型MSM Emulators，以MarS - FM为例，采样速度比MD模拟快两个数量级，在多项指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 分子动力学（MD）计算成本高，现有生成模型学习固定滞后转移密度存在问题。

Method: 引入MSM Emulators类，以Markov Space Flow Matching（MarS - FM）为例，通过结构可观测量评估其重现MD统计数据的能力。

Result: MarS - FM采样比隐式或显式溶剂MD模拟快两个数量级，在所有指标上优于现有方法。

Conclusion: MarS - FM在生成替代轨迹方面表现出色，能有效解决MD计算成本高的问题。

Abstract: Molecular Dynamics (MD) is a powerful computational microscope for probing
protein functions. However, the need for fine-grained integration and the long
timescales of biomolecular events make MD computationally expensive. To address
this, several generative models have been proposed to generate surrogate
trajectories at lower cost. Yet, these models typically learn a fixed-lag
transition density, causing the training signal to be dominated by frequent but
uninformative transitions. We introduce a new class of generative models, MSM
Emulators, which instead learn to sample transitions across discrete states
defined by an underlying Markov State Model (MSM). We instantiate this class
with Markov Space Flow Matching (MarS-FM), whose sampling offers more than two
orders of magnitude speedup compared to implicit- or explicit-solvent MD
simulations. We benchmark Mars-FM ability to reproduce MD statistics through
structural observables such as RMSD, radius of gyration, and secondary
structure content. Our evaluation spans protein domains (up to 500 residues)
with significant chemical and structural diversity, including unfolding events,
and enforces strict sequence dissimilarity between training and test sets to
assess generalization. Across all metrics, MarS-FM outperforms existing
methods, often by a substantial margin.

</details>


### [461] [Quantifying Generalisation in Imitation Learning](https://arxiv.org/abs/2509.24784)
*Nathan Gavenski,Odinaldo Rodrigues*

Main category: cs.LG

TL;DR: 提出Labyrinth基准测试环境以提升模仿学习泛化评估。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习基准测试缺乏训练和评估的足够差异，限制泛化评估。

Method: 引入Labyrinth基准测试环境，可精确控制结构、起止位置和任务复杂度。

Result: Labyrinth有离散、全可观测状态空间和已知最优动作，支持可解释性和细粒度评估。

Conclusion: Labyrinth推动模仿学习泛化评估，为开发更鲁棒的智能体提供有价值工具。

Abstract: Imitation learning benchmarks often lack sufficient variation between
training and evaluation, limiting meaningful generalisation assessment. We
introduce Labyrinth, a benchmarking environment designed to test generalisation
with precise control over structure, start and goal positions, and task
complexity. It enables verifiably distinct training, evaluation, and test
settings. Labyrinth provides a discrete, fully observable state space and known
optimal actions, supporting interpretability and fine-grained evaluation. Its
flexible setup allows targeted testing of generalisation factors and includes
variants like partial observability, key-and-door tasks, and ice-floor hazards.
By enabling controlled, reproducible experiments, Labyrinth advances the
evaluation of generalisation in imitation learning and provides a valuable tool
for developing more robust agents.

</details>


### [462] [Assessing the risk of future Dunkelflaute events for Germany using generative deep learning](https://arxiv.org/abs/2509.24788)
*Felix Strnad,Jonathan Schmidt,Fabian Mockert,Philipp Hennig,Nicole Ludwig*

Main category: cs.LG

TL;DR: 研究适应生成式深度学习框架对CMIP6集合气候模拟进行降尺度，分析未来德国Dunkelflaute事件对电力生产影响，发现其频率和持续时间预计与历史时期大体不变。


<details>
  <summary>Details</summary>
Motivation: 欧洲电网向可再生能源转型，能源源对天气的依赖给电网稳定性带来挑战，特别是Dunkelflaute事件可能导致电力供应短缺，需研究其对德国未来电力生产的影响。

Method: 适应最近开发的生成式深度学习框架对CMIP6集合气候模拟进行降尺度，将模拟统计数据与ERA5历史记录对比，用降尺度模拟评估德国在低（SSP2 - 4.5）和高（SSP5 - 8.5）排放情景下Dunkelflaute事件的未来可能发生情况。

Result: 集合平均中德国Dunkelflaute事件的频率和持续时间与历史时期相比预计大体不变。

Conclusion: 在考虑的气候情景下，相关风险预计在整个世纪保持稳定。

Abstract: The European electricity power grid is transitioning towards renewable energy
sources, characterized by an increasing share of off- and onshore wind and
solar power. However, the weather dependency of these energy sources poses a
challenge to grid stability, with so-called Dunkelflaute events -- periods of
low wind and solar power generation -- being of particular concern due to their
potential to cause electricity supply shortages. In this study, we investigate
the impact of these events on the German electricity production in the years
and decades to come. For this purpose, we adapt a recently developed generative
deep learning framework to downscale climate simulations from the CMIP6
ensemble. We first compare their statistics to the historical record taken from
ERA5 data. Next, we use these downscaled simulations to assess plausible future
occurrences of Dunkelflaute events in Germany under the optimistic low
(SSP2-4.5) and high (SSP5-8.5) emission scenarios. Our analysis indicates that
both the frequency and duration of Dunkelflaute events in Germany in the
ensemble mean are projected to remain largely unchanged compared to the
historical period. This suggests that, under the considered climate scenarios,
the associated risk is expected to remain stable throughout the century.

</details>


### [463] [DSAT-HD: Dual-Stream Adaptive Transformer with Hybrid Decomposition for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.24800)
*Zixu Wang,Hongbin Dong,Xiaoping Zhang*

Main category: cs.LG

TL;DR: 提出DSAT - HD模型用于时间序列预测，通过三项创新解决现有方法局限，实验表明该模型表现优异且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法在捕捉不同范围特征和处理复杂季节性趋势分解上存在局限，需新方法解决。

Method: 提出Hybrid Decomposition Dual - Stream Adaptive Transformer (DSAT - HD)，包含混合分解机制、多尺度自适应路径、双流残差学习框架三项创新。

Result: 在九个数据集上的实验显示DSAT - HD整体优于现有方法，在部分数据集达最优，且在不同迁移场景有强泛化能力。

Conclusion: DSAT - HD能有效解决现有时间序列预测方法的局限，是一种更优的时间序列预测方案。

Abstract: Time series forecasting is crucial for various applications, such as weather,
traffic, electricity, and energy predictions. Currently, common time series
forecasting methods are based on Transformers. However, existing approaches
primarily model limited time series or fixed scales, making it more challenging
to capture diverse features cross different ranges. Additionally, traditional
methods like STL for complex seasonality-trend decomposition require
pre-specified seasonal periods and typically handle only single, fixed
seasonality. We propose the Hybrid Decomposition Dual-Stream Adaptive
Transformer (DSAT-HD), which integrates three key innovations to address the
limitations of existing methods: 1) A hybrid decomposition mechanism combining
EMA and Fourier decomposition with RevIN normalization, dynamically balancing
seasonal and trend components through noise Top-k gating; 2) A multi-scale
adaptive pathway leveraging a sparse allocator to route features to four
parallel Transformer layers, followed by feature merging via a sparse combiner,
enhanced by hybrid attention combining local CNNs and global interactions; 3) A
dual-stream residual learning framework where CNN and MLP branches separately
process seasonal and trend components, coordinated by a balanced loss function
minimizing expert collaboration variance. Extensive experiments on nine
datasets demonstrate that DSAT-HD outperforms existing methods overall and
achieves state-of-the-art performance on some datasets. Notably, it also
exhibits stronger generalization capabilities across various transfer
scenarios.

</details>


### [464] [Physics-informed learning under mixing: How physical knowledge speeds up learning](https://arxiv.org/abs/2509.24801)
*Anna Scampicchio,Leonardo F. Toso,Rahel Rickenbach,James Anderson,Melanie N. Zeilinger*

Main category: cs.LG

TL;DR: 研究物理信息机器学习中先验知识对数据相关时学习率的影响，推导了超额风险界，证明物理先验信息一致时学习率可提升。


<details>
  <summary>Details</summary>
Motivation: 解决物理信息机器学习中理解先验领域知识对数据相关时学习率影响的挑战。

Method: 针对带物理信息正则化的经验风险最小化，推导复杂度相关的超额风险界。

Result: 证明当物理先验信息一致时，学习率从Sobolev极小极大率提升到最优独立同分布率，且无因数据依赖导致的样本量缩减。

Conclusion: 物理先验信息一致有助于提升物理信息机器学习的数据相关时的学习率。

Abstract: A major challenge in physics-informed machine learning is to understand how
the incorporation of prior domain knowledge affects learning rates when data
are dependent. Focusing on empirical risk minimization with physics-informed
regularization, we derive complexity-dependent bounds on the excess risk in
probability and in expectation. We prove that, when the physical prior
information is aligned, the learning rate improves from the (slow) Sobolev
minimax rate to the (fast) optimal i.i.d. one without any sample-size deflation
due to data dependence.

</details>


### [465] [DyMoDreamer: World Modeling with Dynamic Modulation](https://arxiv.org/abs/2509.24804)
*Boxuan Zhang,Runqing Wang,Wei Xiao,Weipu Zhang,Jian Sun,Gao Huang,Jie Chen,Gang Wang*

Main category: cs.LG

TL;DR: 本文提出DyMoDreamer算法解决深度强化学习样本效率低问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习样本效率低，传统基于模型的强化学习世界模型处理观测时未分离动态对象和时间特征，计算效率低。

Method: 引入动态调制机制，采用帧间差分掩码的差分观测，将动态调制建模为随机分类分布并集成到循环状态空间模型。

Result: 在Atari 100k基准测试、DeepMind视觉控制套件和Crafter基准测试中取得优异成绩。

Conclusion: DyMoDreamer算法能有效提高样本效率，在多个基准测试中表现出色。

Abstract: A critical bottleneck in deep reinforcement learning (DRL) is sample
inefficiency, as training high-performance agents often demands extensive
environmental interactions. Model-based reinforcement learning (MBRL) mitigates
this by building world models that simulate environmental dynamics and generate
synthetic experience, improving sample efficiency. However, conventional world
models process observations holistically, failing to decouple dynamic objects
and temporal features from static backgrounds. This approach is computationally
inefficient, especially for visual tasks where dynamic objects significantly
influence rewards and decision-making performance. To address this, we
introduce DyMoDreamer, a novel MBRL algorithm that incorporates a dynamic
modulation mechanism to improve the extraction of dynamic features and enrich
the temporal information. DyMoDreamer employs differential observations derived
from a novel inter-frame differencing mask, explicitly encoding object-level
motion cues and temporal dynamics. Dynamic modulation is modeled as stochastic
categorical distributions and integrated into a recurrent state-space model
(RSSM), enhancing the model's focus on reward-relevant dynamics. Experiments
demonstrate that DyMoDreamer sets a new state-of-the-art on the Atari $100$k
benchmark with a $156.6$\% mean human-normalized score, establishes a new
record of $832$ on the DeepMind Visual Control Suite, and gains a $9.5$\%
performance improvement after $1$M steps on the Crafter benchmark. Our code is
released at https://github.com/Ultraman-Tiga1/DyMoDreamer.

</details>


### [466] [Putnam-like dataset summary: LLMs as mathematical competition contestants](https://arxiv.org/abs/2509.24827)
*Bartosz Bieganowski,Daniel Strzelecki,Robert Skiba,Mateusz Topolewski*

Main category: cs.LG

TL;DR: 总结Google DeepMind发布的类似普特南基准测试结果，分析模型在该数据集上表现以验证解题能力


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型解决数学竞赛问题的能力

Method: 分析大语言模型在含96个原创问题和576个解答的数据集上的表现

Result: 未提及

Conclusion: 未提及

Abstract: In this paper we summarize the results of the Putnam-like benchmark published
by Google DeepMind. This dataset consists of 96 original problems in the spirit
of the Putnam Competition and 576 solutions of LLMs. We analyse the performance
of models on this set of problems to verify their ability to solve problems
from mathematical contests.

</details>


### [467] [Beyond the Hook: Predicting Billboard Hot 100 Chart Inclusion with Machine Learning from Streaming, Audio Signals, and Perceptual Features](https://arxiv.org/abs/2509.24856)
*Christos Mountzouris*

Main category: cs.LG

TL;DR: 研究数字音乐时代哪些因素能预测歌曲进入Billboard Hot 100榜单，发现流行度最关键，多种模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 数字音乐平台兴起提供数据，研究歌曲进入Billboard Hot 100榜单的最强预测因素。

Method: 分析歌曲的流媒体流行度、音频信号属性和人类收听概率指标，采用逻辑回归、随机森林和梯度提升（XGBoost）模型。

Result: 流行度是最关键预测因素，多种音频属性也有贡献。逻辑回归准确率90.0%，随机森林90.4%，梯度提升90.3%。

Conclusion: 不同模型在预测歌曲是否进入榜单上各有优劣，都能达到较高的准确率和F1分数。

Abstract: The advent of digital streaming platforms have recently revolutionized the
landscape of music industry, with the ensuing digitalization providing
structured data collections that open new research avenues for investigating
popularity dynamics and mainstream success. The present work explored which
determinants hold the strongest predictive influence for a track's inclusion in
the Billboard Hot 100 charts, including streaming popularity, measurable audio
signal attributes, and probabilistic indicators of human listening. The
analysis revealed that popularity was by far the most decisive predictor of
Billboard Hot 100 inclusion, with considerable contribution from
instrumentalness, valence, duration and speechiness. Logistic Regression
achieved 90.0% accuracy, with very high recall for charting singles (0.986) but
lower recall for non-charting ones (0.813), yielding balanced F1-scores around
0.90. Random Forest slightly improved performance to 90.4% accuracy,
maintaining near-perfect precision for non-charting singles (0.990) and high
recall for charting ones (0.992), with F1-scores up to 0.91. Gradient Boosting
(XGBoost) reached 90.3% accuracy, delivering a more balanced trade-off by
improving recall for non-charting singles (0.837) while sustaining high recall
for charting ones (0.969), resulting in F1-scores comparable to the other
models.

</details>


### [468] [DRIFT-Net: A Spectral--Coupled Neural Operator for PDEs Learning](https://arxiv.org/abs/2509.24868)
*Jiayi Li,Flora D. Salim*

Main category: cs.LG

TL;DR: 提出 DRIFT - Net 解决现有 PDE 基础模型全局耦合弱问题，表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有 PDE 基础模型采用多尺度窗口自注意力机制，存在局部性，导致全局耦合弱、误差积累和漂移。

Method: 提出 DRIFT - Net，采用双分支设计，包括光谱分支和图像分支，在低频范围进行混合，通过带权融合，最后转换回空间域并添加到图像分支。

Result: 与基于注意力的基线模型相比，DRIFT - Net 在相同训练设置和预算下误差更低、吞吐量更高、参数更少，在 Navier - Stokes 基准测试中相对 $L_{1}$ 误差降低 7% - 54%，参数减少约 15%，吞吐量高于 scOT。

Conclusion: 消融研究和理论分析证明了 DRIFT - Net 设计的稳定性和有效性。

Abstract: Learning PDE dynamics with neural solvers can significantly improve
wall-clock efficiency and accuracy compared with classical numerical solvers.
In recent years, foundation models for PDEs have largely adopted multi-scale
windowed self-attention, with the scOT backbone in \textsc{Poseidon} serving as
a representative example.
  However, because of their locality, truly globally consistent spectral
coupling can only be propagated gradually through deep stacking and window
shifting. This weakens global coupling and leads to error accumulation and
drift during closed-loop rollouts. To address this, we propose
\textbf{DRIFT-Net}. It employs a dual-branch design comprising a spectral
branch and an image branch. The spectral branch is responsible for capturing
global, large-scale low-frequency information, whereas the image branch focuses
on local details and nonstationary structures. Specifically, we first perform
controlled, lightweight mixing within the low-frequency range. Then we fuse the
spectral and image paths at each layer via bandwise weighting, which avoids the
width inflation and training instability caused by naive concatenation. The
fused result is transformed back into the spatial domain and added to the image
branch, thereby preserving both global structure and high-frequency details
across scales. Compared with strong attention-based baselines, DRIFT-Net
achieves lower error and higher throughput with fewer parameters under
identical training settings and budget. On Navier--Stokes benchmarks, the
relative $L_{1}$ error is reduced by 7\%--54\%, the parameter count decreases
by about 15\%, and the throughput remains higher than scOT. Ablation studies
and theoretical analyses further demonstrate the stability and effectiveness of
this design. The code is available at
https://github.com/cruiseresearchgroup/DRIFT-Net.

</details>


### [469] [Uncertainty-Guided Expert-AI Collaboration for Efficient Soil Horizon Annotation](https://arxiv.org/abs/2509.24873)
*Teodor Chiaburu,Vipin Singh,Frank Haußer,Felix Bießmann*

Main category: cs.LG

TL;DR: 本文将保形预测应用于描述土壤剖面的多模态多任务模型SoilNet，设计模拟人在环注释管道，实验表明保形化的SoilNet在回归任务中注释更高效，分类任务中性能相当。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化在人机协作中至关重要，可靠校准的模型不确定性能实现更有效的协作，保形预测是成熟的不确定性校准框架，因此将其应用于SoilNet以提升性能。

Method: 将保形预测应用于SoilNet，设计模拟人在环（HIL）注释管道，在模型不确定性高时利用有限预算获取领域专家的真实注释。

Result: 与非保形的SoilNet相比，保形化的SoilNet在回归任务中注释更高效，在相同注释预算下分类任务性能相当。

Conclusion: 将保形预测应用于SoilNet是有效的，能提升注释效率和任务性能。

Abstract: Uncertainty quantification is essential in human-machine collaboration, as
human agents tend to adjust their decisions based on the confidence of the
machine counterpart. Reliably calibrated model uncertainties, hence, enable
more effective collaboration, targeted expert intervention and more responsible
usage of Machine Learning (ML) systems. Conformal prediction has become a well
established model-agnostic framework for uncertainty calibration of ML models,
offering statistically valid confidence estimates for both regression and
classification tasks. In this work, we apply conformal prediction to
$\textit{SoilNet}$, a multimodal multitask model for describing soil profiles.
We design a simulated human-in-the-loop (HIL) annotation pipeline, where a
limited budget for obtaining ground truth annotations from domain experts is
available when model uncertainty is high. Our experiments show that
conformalizing SoilNet leads to more efficient annotation in regression tasks
and comparable performance scores in classification tasks under the same
annotation budget when tested against its non-conformal counterpart. All code
and experiments can be found in our repository:
https://github.com/calgo-lab/BGR

</details>


### [470] [Adaptive Canonicalization with Application to Invariant Anisotropic Geometric Networks](https://arxiv.org/abs/2509.24886)
*Ya-Wei Eileen Lin,Ron Levie*

Main category: cs.LG

TL;DR: 提出自适应规范化框架解决传统规范化问题，证明其性质并应用于多个任务，表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 传统规范化在等变机器学习中会引入不连续性，影响训练稳定性、泛化能力和通用近似定理。

Method: 引入自适应规范化框架，基于先验最大化选择输入标准形式，提出两个应用场景。

Result: 所提方法在分子、蛋白质和点云分类任务上进行了实证验证，自适应规范化表现优于数据增强、标准规范化和等变架构。

Conclusion: 自适应规范化能产生连续且尊重对称性的模型，具有通用近似特性。

Abstract: Canonicalization is a widely used strategy in equivariant machine learning,
enforcing symmetry in neural networks by mapping each input to a standard form.
Yet, it often introduces discontinuities that can affect stability during
training, limit generalization, and complicate universal approximation
theorems. In this paper, we address this by introducing \emph{adaptive
canonicalization}, a general framework in which the canonicalization depends
both on the input and the network. Specifically, we present the adaptive
canonicalization based on prior maximization, where the standard form of the
input is chosen to maximize the predictive confidence of the network. We prove
that this construction yields continuous and symmetry-respecting models that
admit universal approximation properties.
  We propose two applications of our setting: (i) resolving eigenbasis
ambiguities in spectral graph neural networks, and (ii) handling rotational
symmetries in point clouds. We empirically validate our methods on molecular
and protein classification, as well as point cloud classification tasks. Our
adaptive canonicalization outperforms the three other common solutions to
equivariant machine learning: data augmentation, standard canonicalization, and
equivariant architectures.

</details>


### [471] [Towards Understanding the Shape of Representations in Protein Language Models](https://arxiv.org/abs/2509.24895)
*Kosio Beshkov,Anders Malthe-Sørenssen*

Main category: cs.LG

TL;DR: 本文用平方根速度（SRV）表示和图过滤来理解蛋白质语言模型（PLMs）转换后的序列空间，分析发现SRV形状空间的Karcher均值和有效维度有非线性模式，PLMs对残基关系编码有特点，特定层训练折叠模型或提升性能。


<details>
  <summary>Details</summary>
Motivation: PLMs将序列转换为隐藏表示的方式及其中编码的信息尚未完全理解，此前研究多关注单个序列转换，本文旨在理解PLMs对整个序列空间及其关系的转换。

Method: 通过平方根速度（SRV）表示和图过滤来识别蛋白质结构和表示，分析SCOP数据集中不同类型蛋白质。

Result: SRV形状空间的Karcher均值和有效维度随不同大小ESM2模型的层数呈非线性模式；PLMs优先编码残基的直接和局部关系，大上下文长度时编码效果下降；最具结构忠实性的编码接近但在模型最后一层之前。

Conclusion: 在接近但早于模型最后一层的层上训练折叠模型可能会提高折叠性能。

Abstract: While protein language models (PLMs) are one of the most promising avenues of
research for future de novo protein design, the way in which they transform
sequences to hidden representations, as well as the information encoded in such
representations is yet to be fully understood. Several works have attempted to
propose interpretability tools for PLMs, but they have focused on understanding
how individual sequences are transformed by such models. Therefore, the way in
which PLMs transform the whole space of sequences along with their relations is
still unknown. In this work we attempt to understand this transformed space of
sequences by identifying protein structure and representation with square-root
velocity (SRV) representations and graph filtrations. Both approaches naturally
lead to a metric space in which pairs of proteins or protein representations
can be compared with each other.
  We analyze different types of proteins from the SCOP dataset and show that
the Karcher mean and effective dimension of the SRV shape space follow a
non-linear pattern as a function of the layers in ESM2 models of different
sizes. Furthermore, we use graph filtrations as a tool to study the context
lengths at which models encode the structural features of proteins. We find
that PLMs preferentially encode immediate as well as local relations between
residues, but start to degrade for larger context lengths. The most
structurally faithful encoding tends to occur close to, but before the last
layer of the models, indicating that training a folding model ontop of these
layers might lead to improved folding performance.

</details>


### [472] [When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training](https://arxiv.org/abs/2509.24923)
*Sanxing Chen,Xiaoyin Chen,Yukun Huang,Roy Xie,Bhuwan Dhingra*

Main category: cs.LG

TL;DR: 研究用SFT和RL训练大语言模型在多臂老虎机任务中的探索策略，模型表现佳但有缺陷，给出训练范式选择建议。


<details>
  <summary>Details</summary>
Motivation: 现有工作用SFT或RL提升大语言模型在顺序决策中的探索能力，但不清楚这些学习方法如何塑造探索策略及泛化性，需深入研究。

Method: 用SFT在专家轨迹上训练大语言模型，用多种定制奖励信号进行RL训练。

Result: 训练后的代理优于预训练模型，性能与UCB和汤普森采样相当，有良好泛化性，但更易早期失败，模仿UCB的代理能超越其“老师”。

Conclusion: 明确各训练范式的适用场景，提倡定制奖励设计和超越平均遗憾的评估以促进稳健探索行为。

Abstract: While Large Language Models (LLMs) hold promise to become autonomous agents,
they often explore suboptimally in sequential decision-making. Recent work has
sought to enhance this capability via supervised fine-tuning (SFT) or
reinforcement learning (RL), improving regret on the classic multi-armed bandit
task. However, it remains unclear how these learning methods shape exploration
strategies and how well they generalize. We investigate both paradigms by
training LLMs with SFT on expert trajectories and RL with a range of tailored
reward signals including a strategic, regret-shaped reward to reduce variance,
and an algorithmic reward that enables oracle imitation. The resulting agents
outperform pre-trained models and achieve performance comparable to Upper
Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x
longer horizons and across bandit families. Behavioral analysis reveals that
gains often stem from more sophisticated but greedier exploitation: RL/SFT
agents are more prone to early catastrophic failure than pre-trained models,
prematurely abandoning exploration. Furthermore, agents trained to imitate UCB
learn to outperform their teacher by adopting more exploitative variants. Our
findings clarify when each training paradigm is preferable and advocate
tailored reward design and evaluation beyond average regret to promote robust
exploratory behavior.

</details>


### [473] [Is Sequence Information All You Need for Bayesian Optimization of Antibodies?](https://arxiv.org/abs/2509.24933)
*Sebastian W. Ober,Calvin McCarter,Aniruddh Raghu,Yucen Lily Li,Alan N. Amin,Andrew Gordon Wilson,Hunter Elliott*

Main category: cs.LG

TL;DR: 本文探索将结构信息融入抗体贝叶斯优化的不同方法，并与仅基于序列的方法比较，还提出使用蛋白质语言模型软约束，发现某些结构信息对稳定性早期优化有帮助，软约束缩小性能差距，引发对结构必要性的思考。


<details>
  <summary>Details</summary>
Motivation: 抗体治疗特性工程常迭代且成本高，贝叶斯优化是合适选择，但在高度结构化的抗体空间中为优化选择最佳替代模型困难，且此前无人尝试将结构信息融入抗体贝叶斯优化。

Method: 探索不同将结构信息融入贝叶斯优化的方法，与多种仅基于序列的方法在结合亲和力和稳定性两种抗体特性上进行比较，提出使用蛋白质语言模型软约束。

Result: 某些类型的结构信息在稳定性早期优化轮次提高数据效率，但峰值性能相当；使用蛋白质语言模型软约束后，亲和力的数据效率差距缩小，稳定性的差距消除，仅基于序列的方法性能与基于结构的方法相当。

Conclusion: 对在抗体贝叶斯优化中使用结构信息的必要性提出疑问。

Abstract: Bayesian optimization is a natural candidate for the engineering of antibody
therapeutic properties, which is often iterative and expensive. However,
finding the optimal choice of surrogate model for optimization over the highly
structured antibody space is difficult, and may differ depending on the
property being optimized. Moreover, to the best of our knowledge, no prior
works have attempted to incorporate structural information into antibody
Bayesian optimization. In this work, we explore different approaches to
incorporating structural information into Bayesian optimization, and compare
them to a variety of sequence-only approaches on two different antibody
properties, binding affinity and stability. In addition, we propose the use of
a protein language model-based ``soft constraint,'' which helps guide the
optimization to promising regions of the space. We find that certain types of
structural information improve data efficiency in early optimization rounds for
stability, but have equivalent peak performance. Moreover, when incorporating
the protein language model soft constraint we find that the data efficiency gap
is diminished for affinity and eliminated for stability, resulting in
sequence-only methods that match the performance of structure-based methods,
raising questions about the necessity of structure in Bayesian optimization for
antibodies.

</details>


### [474] [OAT-FM: Optimal Acceleration Transport for Improved Flow Matching](https://arxiv.org/abs/2509.24936)
*Angxiao Yue,Anqi Dong,Hongteng Xu*

Main category: cs.LG

TL;DR: 本文将Flow Matching (FM)与最优加速传输理论(OAT)相结合，提出OAT - FM方法，设计高效算法，提出两阶段FM范式，提升生成任务模型性能。


<details>
  <summary>Details</summary>
Motivation: 将FM与OAT理论相结合，改进现有FM方法，探索其在理论和实践中的优势。

Method: 开发OAT - FM方法，在样本和速度的乘积空间中优化加速传输，设计低复杂度算法，提出两阶段FM范式。

Result: 证明现有基于OT的FM方法中的拉直目标与最小化OAT定义的加速度相关物理作用等价，两阶段FM范式消除数据分布漂移风险和大量噪声数据对需求，提升模型性能。

Conclusion: OAT - FM方法有效，两阶段FM范式可提升多种生成任务的模型性能。

Abstract: As a powerful technique in generative modeling, Flow Matching (FM) aims to
learn velocity fields from noise to data, which is often explained and
implemented as solving Optimal Transport (OT) problems. In this study, we
bridge FM and the recent theory of Optimal Acceleration Transport (OAT),
developing an improved FM method called OAT-FM and exploring its benefits in
both theory and practice. In particular, we demonstrate that the straightening
objective hidden in existing OT-based FM methods is mathematically equivalent
to minimizing the physical action associated with acceleration defined by OAT.
Accordingly, instead of enforcing constant velocity, OAT-FM optimizes the
acceleration transport in the product space of sample and velocity, whose
objective corresponds to a necessary and sufficient condition of flow
straightness. An efficient algorithm is designed to achieve OAT-FM with low
complexity. OAT-FM motivates a new two-phase FM paradigm: Given a generative
model trained by an arbitrary FM method, whose velocity information has been
relatively reliable, we can fine-tune and improve it via OAT-FM. This paradigm
eliminates the risk of data distribution drift and the need to generate a large
number of noise data pairs, which consistently improves model performance in
various generative tasks. Code is available at:
https://github.com/AngxiaoYue/OAT-FM

</details>


### [475] [Learning Distinguishable Representations in Deep Q-Networks for Linear Transfer](https://arxiv.org/abs/2509.24947)
*Sooraj Sathish,Keshav Goyal,Raghuram Bharadwaj Diddigi*

Main category: cs.LG

TL;DR: 本文研究深度强化学习迁移学习，针对标准模型特征表示相关性高问题，提出新的深度Q学习方法，实验证明可提升迁移学习性能并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习训练存在超参数调整和计算成本高的问题，迁移学习可解决，但标准模型特征表示相关性高限制线性函数近似效果。

Method: 提出一种新的深度Q学习方法，引入正则化项减少状态特征表示之间的正相关性。

Result: 通过在标准强化学习基准和MinAtar游戏上的实验和消融研究，证明该方法能提升迁移学习性能。

Conclusion: 所提方法能有效减少特征表示相关性，提高迁移学习性能，降低计算开销。

Abstract: Deep Reinforcement Learning (RL) has demonstrated success in solving complex
sequential decision-making problems by integrating neural networks with the RL
framework. However, training deep RL models poses several challenges, such as
the need for extensive hyperparameter tuning and high computational costs.
Transfer learning has emerged as a promising strategy to address these
challenges by enabling the reuse of knowledge from previously learned tasks for
new, related tasks. This avoids the need for retraining models entirely from
scratch. A commonly used approach for transfer learning in RL is to leverage
the internal representations learned by the neural network during training.
Specifically, the activations from the last hidden layer can be viewed as
refined state representations that encapsulate the essential features of the
input. In this work, we investigate whether these representations can be used
as input for training simpler models, such as linear function approximators, on
new tasks. We observe that the representations learned by standard deep RL
models can be highly correlated, which limits their effectiveness when used
with linear function approximation. To mitigate this problem, we propose a
novel deep Q-learning approach that introduces a regularization term to reduce
positive correlations between feature representation of states. By leveraging
these reduced correlated features, we enable more effective use of linear
function approximation in transfer learning. Through experiments and ablation
studies on standard RL benchmarks and MinAtar games, we demonstrate the
efficacy of our approach in improving transfer learning performance and thereby
reducing computational overhead.

</details>


### [476] [Intra-request branch orchestration for efficient LLM reasoning](https://arxiv.org/abs/2509.24957)
*Weifan Jiang,Rana Shahout,Yilun Du,Michael Mitzenmacher,Minlan Yu*

Main category: cs.LG

TL;DR: 提出LLM服务系统DUCHESS，通过预测引导的请求内分支编排降低成本和延迟，不牺牲准确性，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有推理算法增加token使用和请求延迟，以往工作多关注减少token使用而忽视其他延迟因素。

Method: 采用轻量级线性探测模型估计分支正确性，编排策略决定分支操作；处理多请求时按复杂度优先处理简单任务。

Result: 在三个推理基准上改善token - 准确率帕累托前沿，减少token使用；在vLLM服务中降低不同类型延迟。

Conclusion: DUCHESS能有效降低成本和延迟，且不牺牲准确性。

Abstract: Large Language Models (LLMs) increasingly rely on inference-time reasoning
algorithms such as chain-of-thought and multi-branch reasoning to improve
accuracy on complex tasks. These methods, however, substantially increase token
usage and per-request latency. Prior work has largely focused on reducing token
usage, often at the expense of accuracy, while overlooking other latency
factors. We present DUCHESS, an LLM serving system that reduces cost and
latency without sacrificing accuracy through intra-request branch orchestration
guided by predictions. DUCHESS employs a lightweight linear probing model over
LLM layer activations to estimate branch correctness, and its orchestration
policy decides whether to terminate, duplicate, or continue a branch. When
handling multiple requests, DUCHESS further reduces latency by prioritizing
easier reasoning tasks when complexity can be estimated from the prompt.
Experiments on three reasoning benchmarks show that DUCHESS consistently
improves the token-accuracy Pareto frontier, reducing token usage by 42-63% at
matched accuracy compared to self-consistency. In serving with vLLM, DUCHESS
reduces mean, median, and tail latencies by 57-81%, 58-85%, and 52-84% with
First-Come-First-Served scheduling, and achieves additional gains under
difficulty-aware scheduling at higher request rates.

</details>


### [477] [Double Descent as a Lens for Sample Efficiency in Autoregressive vs. Discrete Diffusion Models](https://arxiv.org/abs/2509.24974)
*Ahmad Fraij,Sam Dauncey*

Main category: cs.LG

TL;DR: 利用双下降现象对比离散扩散和自回归模型样本效率，发现小数据集上自回归模型更优，离散扩散模型需足够容量和计算资源才具竞争力。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺促使需要更具样本效率的大语言模型，因此对比离散扩散和自回归模型的样本效率。

Method: 使用双下降现象来全面比较离散扩散和自回归模型的样本效率。

Result: 离散扩散模型需更大容量和更多训练轮次才能脱离参数不足状态；强过参数化状态下，两种模型表现相似，测试损失无明显二次下降；小数据集上自回归模型样本效率更高，离散扩散模型需足够容量和计算资源才具竞争力。

Conclusion: 小数据集上自回归模型更具样本效率，离散扩散模型在有足够容量和计算时才有竞争力。

Abstract: Data scarcity drives the need for more sample-efficient large language
models. In this work, we use the double descent phenomenon to holistically
compare the sample efficiency of discrete diffusion and autoregressive models.
We show that discrete diffusion models require larger capacity and more
training epochs to escape their underparameterized regime and reach the
interpolation threshold. In the strongly overparameterized regime, both models
exhibit similar behavior, with neither exhibiting a pronounced second descent
in test loss across a large range of model sizes. Overall, our results indicate
that autoregressive models are more sample-efficient on small-scale datasets,
while discrete diffusion models only become competitive when given sufficient
capacity and compute.

</details>


### [478] [Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards](https://arxiv.org/abs/2509.24981)
*Haoran He,Yuxiao Ye,Qingpeng Cai,Chen Hu,Binxing Jiao,Daxin Jiang,Ling Pan*

Main category: cs.LG

TL;DR: 提出ROVER算法用于大语言模型数学推理，简化现有方法，提升质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在训练中存在不稳定和多样性崩溃问题，需复杂技巧和调优。

Method: 证明可从固定均匀随机策略的Q函数恢复最优动作，提出ROVER算法，从均匀策略Q值的softmax中采样动作。

Result: 在多个基础模型和标准数学推理基准上，ROVER在质量和多样性上表现优越。

Conclusion: ROVER是一种极简且高效的强化学习方法，能在训练中保持多样性。

Abstract: RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for
improving the reasoning abilities of large language models (LLMs). Current
methods rely primarily on policy optimization frameworks like PPO and GRPO,
which follow generalized policy iteration that alternates between evaluating
the current policy's value and improving the policy based on evaluation. While
effective, they often suffer from training instability and diversity collapse,
requiring complex heuristic tricks and careful tuning. We observe that standard
RLVR in math reasoning can be formalized as a specialized finite-horizon Markov
Decision Process with deterministic state transitions, tree-structured
dynamics, and binary terminal rewards. Though large in scale, the underlying
structure is simpler than general-purpose control settings for which popular RL
algorithms (e.g., PPO) were developed, suggesting that several sophisticated
techniques in existing methods may be reduced or even omitted. Based on this
insight, we prove a surprising result: the optimal action can be recovered from
the Q-function of a fixed uniformly random policy, thereby bypassing the
generalized policy iteration loop and its associated heuristics. We introduce
Random Policy Valuation for Diverse Reasoning (ROVER) to translate this
principle into a practical and scalable algorithm for LLM math reasoning, a
minimalist yet highly effective RL method that samples actions from a softmax
over these uniform-policy Q-values. ROVER preserves diversity throughout
training, allowing sustained exploration of multiple valid pathways. Across
multiple base models and standard math reasoning benchmarks, ROVER demonstrates
superior performance in both \textbf{quality} (\textbf{+8.2} on pass@1,
\textbf{+16.8} on pass@256) and \textbf{diversity} (\textbf{+17.6\%}), despite
its radical simplification compared to strong, complicated existing methods.

</details>


### [479] [Sampling Complexity of TD and PPO in RKHS](https://arxiv.org/abs/2509.24991)
*Lu Zou,Wendi Ren,Weizhong Zhang,Liang Ding,Shuang Li*

Main category: cs.LG

TL;DR: 从函数空间视角重新审视PPO，在RKHS中解耦策略评估与改进，给出理论保证和采样规则，实证显示能提升稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 从函数空间视角重新审视PPO，为其建立更坚实理论基础。

Method: 在RKHS中解耦策略评估与改进，用核化TD批评家进行更新，采用KL正则化自然梯度策略步骤。

Result: 给出非渐近、实例自适应保证，推导采样规则，实证表明理论对齐调度提升稳定性和效率。

Conclusion: 研究使PPO超越有限维假设，明确了核TD批评家的RKHS近端更新何时能实现全局策略改进和实际效率。

Abstract: We revisit Proximal Policy Optimization (PPO) from a function-space
perspective. Our analysis decouples policy evaluation and improvement in a
reproducing kernel Hilbert space (RKHS): (i) A kernelized temporal-difference
(TD) critic performs efficient RKHS-gradient updates using only one-step
state-action transition samples; (ii) a KL-regularized, natural-gradient policy
step exponentiates the evaluated action-value, recovering a PPO/TRPO-style
proximal update in continuous state-action spaces. We provide non-asymptotic,
instance-adaptive guarantees whose rates depend on RKHS entropy, unifying
tabular, linear, Sobolev, Gaussian, and Neural Tangent Kernel (NTK) regimes,
and we derive a sampling rule for the proximal update that ensures the optimal
$k^{-1/2}$ convergence rate for stochastic optimization. Empirically, the
theory-aligned schedule improves stability and sample efficiency on common
control tasks (e.g., CartPole, Acrobot), while our TD-based critic attains
favorable throughput versus a GAE baseline. Altogether, our results place PPO
on a firmer theoretical footing beyond finite-dimensional assumptions and
clarify when RKHS-proximal updates with kernel-TD critics yield global policy
improvement with practical efficiency.

</details>


### [480] [Score-based Membership Inference on Diffusion Models](https://arxiv.org/abs/2509.25003)
*Mingxing Rao,Bowen Qu,Daniel Moyer*

Main category: cs.LG

TL;DR: 研究基于分数的成员推理攻击（MIAs），提出单查询攻击SimA，发现潜在扩散模型比像素空间模型更抗攻击并给出强化训练策略。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的MIAs引发隐私担忧，需对基于分数的MIAs进行研究。

Method: 理论与实证研究预测噪声向量，提出单查询攻击SimA，研究潜在通道正则化超参数。

Result: SimA在多种模型上表现好，潜在扩散模型比像素空间模型更抗攻击。

Conclusion: 巩固基于分数的MIAs理论，强调潜在扩散方法需更好理解VAE反演。

Abstract: Membership inference attacks (MIAs) against diffusion models have emerged as
a pressing privacy concern, as these models may inadvertently reveal whether a
given sample was part of their training set. We present a theoretical and
empirical study of score-based MIAs, focusing on the predicted noise vectors
that diffusion models learn to approximate. We show that the expected denoiser
output points toward a kernel-weighted local mean of nearby training samples,
such that its norm encodes proximity to the training set and thereby reveals
membership. Building on this observation, we propose SimA, a single-query
attack that provides a principled, efficient alternative to existing
multi-query methods. SimA achieves consistently strong performance across
variants of DDPM, Latent Diffusion Model (LDM). Notably, we find that Latent
Diffusion Models are surprisingly less vulnerable than pixel-space models, due
to the strong information bottleneck imposed by their latent auto-encoder. We
further investigate this by differing the regularization hyperparameters
($\beta$ in $\beta$-VAE) in latent channel and suggest a strategy to make LDM
training more robust to MIA. Our results solidify the theory of score-based
MIAs, while highlighting that Latent Diffusion class of methods requires better
understanding of inversion for VAE, and not simply inversion of the Diffusion
process

</details>


### [481] [Uncertainty-Aware Deep Learning for Wildfire Danger Forecasting](https://arxiv.org/abs/2509.25017)
*Spyros Kondylatos,Gustau Camps-Valls,Ioannis Papoutsis*

Main category: cs.LG

TL;DR: 本文提出不确定性感知的深度学习框架用于野火危险预测，提升了预测准确性和可靠性，分析了不同类型不确定性在不同预测期的特点。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习预测野火危险存在可靠性问题，缺乏不确定性量化，因此需要提升预测模型的可靠性。

Method: 提出不确定性感知的深度学习框架，联合捕捉认知（模型）和随机（数据）不确定性。

Result: 次日预测中，最佳模型将F1分数提高2.3%，降低预期校准误差2.1%；延长预测期发现随机不确定性随时间增加，认知不确定性保持稳定。

Conclusion: 该方法显著提高野火危险预测的准确性和可靠性，推动可信野火深度学习系统的发展。

Abstract: Wildfires are among the most severe natural hazards, posing a significant
threat to both humans and natural ecosystems. The growing risk of wildfires
increases the demand for forecasting models that are not only accurate but also
reliable. Deep Learning (DL) has shown promise in predicting wildfire danger;
however, its adoption is hindered by concerns over the reliability of its
predictions, some of which stem from the lack of uncertainty quantification. To
address this challenge, we present an uncertainty-aware DL framework that
jointly captures epistemic (model) and aleatoric (data) uncertainty to enhance
short-term wildfire danger forecasting. In the next-day forecasting, our
best-performing model improves the F1 Score by 2.3% and reduces the Expected
Calibration Error by 2.1% compared to a deterministic baseline, enhancing both
predictive skill and calibration. Our experiments confirm the reliability of
the uncertainty estimates and illustrate their practical utility for decision
support, including the identification of uncertainty thresholds for rejecting
low-confidence predictions and the generation of well-calibrated wildfire
danger maps with accompanying uncertainty layers. Extending the forecast
horizon up to ten days, we observe that aleatoric uncertainty increases with
time, showing greater variability in environmental conditions, while epistemic
uncertainty remains stable. Finally, we show that although the two uncertainty
types may be redundant in low-uncertainty cases, they provide complementary
insights under more challenging conditions, underscoring the value of their
joint modeling for robust wildfire danger prediction. In summary, our approach
significantly improves the accuracy and reliability of wildfire danger
forecasting, advancing the development of trustworthy wildfire DL systems.

</details>


### [482] [MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts](https://arxiv.org/abs/2509.25020)
*Jiayu Liu,Zhenya Huang,Anya Sims,Enhong Chen,Yee Whye Teh,Ning Miao*

Main category: cs.LG

TL;DR: 现有大语言模型链式思维推理范式有缺陷，提出新范式 MARCOS，实验显示其表现优且推理速度快，还有额外优势。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型链式思维推理范式存在推理慢、有信息瓶颈、推理与标记生成纠缠等缺陷，需新推理范式。

Method: 将推理建模为连续高维“思想”的隐马尔可夫链，设计两阶段变分训练方案。

Result: 在三个基准测试中，MARCOS 优于现有连续推理方法，表现与基于标记的链式思维相当，在 GSM8K 上超越 4.7%，推理速度提升 15.7 倍。

Conclusion: MARCOS 是有效的大语言模型推理新范式，为强化学习和推理带来机遇。

Abstract: The current paradigm for reasoning in large language models (LLMs) involves
models "thinking out loud" via a sequence of tokens, known as chain-of-thought
(CoT). This approach, while effective, has several significant drawbacks.
Firstly, inference requires autoregressive generation of often thousands of CoT
tokens, which is slow and computationally expensive. Secondly, it constrains
reasoning to the discrete space of tokens, creating an information bottleneck
across reasoning steps. Thirdly, it fundamentally entangles reasoning with
token generation, forcing LLMs to "think while speaking," which causes
potentially short-sighted reasoning. In light of these limitations, we
re-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our
approach, rather than autoregressively generating tokens, we model reasoning as
a hidden Markov chain of continuous, high-dimensional "thoughts". Each
reasoning step involves a transition of the internal thoughts, where explicit
reasoning steps (which may consist of hundreds of tokens) serve as observable
variables, which are windows to peek into the implicit thoughts. Since this
latent process is incompatible with the standard supervised learning, we
further propose a two-phase variational training scheme. Our experiments on
three benchmarks demonstrate that MARCOS outperforms existing continuous
reasoning methods and, for the first time, achieves performance comparable to
token-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup
in inference. Beyond this, MARCOS offers additional advantages, such as
step-level instead of token-level control over randomness, opening significant
opportunities for reinforcement learning and reasoning in LLMs.

</details>


### [483] [Bayesian Surrogates for Risk-Aware Pre-Assessment of Aging Bridge Portfolios](https://arxiv.org/abs/2509.25031)
*Sophia V. Kuhn,Rafael Bischof,Marius Weber,Antoine Binggeli,Michael A. Kraus,Walter Kaufmann,Fernando Pérez-Cruz*

Main category: cs.LG

TL;DR: 提出贝叶斯神经网络（BNN）替代模型用于桥梁结构预评估，能快速评估并考虑不确定性，案例显示可降低成本和排放。


<details>
  <summary>Details</summary>
Motivation: 老化基础设施组合的资源分配面临挑战，需平衡结构评估方法的成本与准确性。

Method: 使用基于瑞士联邦铁路桥梁组合的参数化管道生成的大规模非线性有限元分析数据库训练BNN替代模型，预测规范合规因子。

Result: 模型能准确、高效地估计高保真结构分析结果，识别可能的关键结构，为精细化分析提供指导。

Conclusion: 模型有望通过避免不必要的分析和干预，显著降低整个基础设施组合的成本和排放。

Abstract: Aging infrastructure portfolios pose a critical resource allocation
challenge: deciding which structures require intervention and which can safely
remain in service. Structural assessments must balance the trade-off between
cheaper, conservative analysis methods and accurate but costly simulations that
do not scale portfolio-wide. We propose Bayesian neural network (BNN)
surrogates for rapid structural pre-assessment of worldwide common bridge
types, such as reinforced concrete frame bridges. Trained on a large-scale
database of non-linear finite element analyses generated via a parametric
pipeline and developed based on the Swiss Federal Railway's bridge portfolio,
the models accurately and efficiently estimate high-fidelity structural
analysis results by predicting code compliance factors with calibrated
epistemic uncertainty. Our BNN surrogate enables fast, uncertainty-aware
triage: flagging likely critical structures and providing guidance where
refined analysis is pertinent. We demonstrate the framework's effectiveness in
a real-world case study of a railway underpass, showing its potential to
significantly reduce costs and emissions by avoiding unnecessary analyses and
physical interventions across entire infrastructure portfolios.

</details>


### [484] [Efficient Hyperparameter Tuning via Trajectory Invariance Principle](https://arxiv.org/abs/2509.25049)
*Bingrui Li,Jiaxin Wen,Zhanpeng Zhou,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: 文章针对超参数调优成本高且缺乏指导原则的问题，发现轨迹不变性现象，提出高效调优规则并完善缩放定律。


<details>
  <summary>Details</summary>
Motivation: 超参数调优成本随规模增加而提高，且缺乏指导调优的原则。

Method: 考虑包括批量大小、学习率和权重衰减等多种超参数，发现轨迹不变性现象。

Result: 将二维超参数空间降为一维，得到高效调优规则，完善缩放定律并挑战现有观点。

Conclusion: 提出高效调优的新原则，为缩放定律的未来研究提供启发。

Abstract: As hyperparameter tuning becomes increasingly costly at scale, efficient
tuning methods are essential. Yet principles for guiding hyperparameter tuning
remain limited. In this work, we seek to establish such principles by
considering a broad range of hyperparameters, including batch size, learning
rate, and weight decay. We identify a phenomenon we call trajectory invariance,
where pre-training loss curves, gradient noise, and gradient norm exhibit
invariance--closely overlapping--with respect to a quantity that combines
learning rate and weight decay. This phenomenon effectively reduces the
original two-dimensional hyperparameter space to one dimension, yielding an
efficient tuning rule: follow the salient direction revealed by trajectory
invariance. Furthermore, we refine previous scaling laws and challenge several
existing viewpoints. Overall, our work proposes new principles for efficient
tuning and inspires future research on scaling laws.

</details>


### [485] [Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion Models](https://arxiv.org/abs/2509.25050)
*Shuchen Xue,Chongjian Ge,Shilong Zhang,Yichen Li,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: 本文指出DDPO存在的问题，提出优势加权匹配（AWM）方法，统一预训练和强化学习，减少方差、加速收敛，在多个基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的强化学习方法（如DDPO）优化目标与预训练目标不同，导致方差增加、收敛缓慢。

Method: 提出优势加权匹配（AWM）方法，使用与预训练相同的分数/流匹配损失，通过优势对样本进行重新加权。

Result: 在GenEval、OCR和PickScore基准测试中，应用于Stable Diffusion 3.5 Medium和FLUX时，AWM比Flow - GRPO提速达24倍，且不影响生成质量。

Conclusion: AWM在概念和实践上统一了预训练和强化学习，符合策略梯度理论，能减少方差、加速收敛，是一种简单有效的方法。

Abstract: Reinforcement Learning (RL) has emerged as a central paradigm for advancing
Large Language Models (LLMs), where pre-training and RL post-training share the
same log-likelihood formulation. In contrast, recent RL approaches for
diffusion models, most notably Denoising Diffusion Policy Optimization (DDPO),
optimize an objective different from the pretraining objectives--score/flow
matching loss. In this work, we establish a novel theoretical analysis: DDPO is
an implicit form of score/flow matching with noisy targets, which increases
variance and slows convergence. Building on this analysis, we introduce
\textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for
diffusion. It uses the same score/flow-matching loss as pretraining to obtain a
lower-variance objective and reweights each sample by its advantage. In effect,
AWM raises the influence of high-reward samples and suppresses low-reward ones
while keeping the modeling objective identical to pretraining. This unifies
pretraining and RL conceptually and practically, is consistent with
policy-gradient theory, reduces variance, and yields faster convergence. This
simple yet effective design yields substantial benefits: on GenEval, OCR, and
PickScore benchmarks, AWM delivers up to a $24\times$ speedup over Flow-GRPO
(which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX,
without compromising generation quality. Code is available at
https://github.com/scxue/advantage_weighted_matching.

</details>


### [486] [Towards a Certificate of Trust: Task-Aware OOD Detection for Scientific AI](https://arxiv.org/abs/2509.25080)
*Bogdan Raonić,Siddhartha Mishra,Samuel Lanthaler*

Main category: cs.LG

TL;DR: 提出基于分数扩散模型估计联合似然的OOD检测方法，在多科学数据集上验证其与预测误差强相关，提供评估AI科学预测可信度的工具。


<details>
  <summary>Details</summary>
Motivation: 数据驱动模型在OOD数据上可能失败，回归任务中检测此类失败是开放挑战。

Method: 基于分数扩散模型估计联合似然的新OOD检测方法，考虑输入和回归模型预测给出任务感知可靠性分数。

Result: 在多个科学数据集上表明似然与预测误差强相关。

Conclusion: 为构建可验证的信任证书提供基础步骤，提供评估AI科学预测可信度的实用工具。

Abstract: Data-driven models are increasingly adopted in critical scientific fields
like weather forecasting and fluid dynamics. These methods can fail on
out-of-distribution (OOD) data, but detecting such failures in regression tasks
is an open challenge. We propose a new OOD detection method based on estimating
joint likelihoods using a score-based diffusion model. This approach considers
not just the input but also the regression model's prediction, providing a
task-aware reliability score. Across numerous scientific datasets, including
PDE datasets, satellite imagery and brain tumor segmentation, we show that this
likelihood strongly correlates with prediction error. Our work provides a
foundational step towards building a verifiable 'certificate of trust', thereby
offering a practical tool for assessing the trustworthiness of AI-based
scientific predictions. Our code is publicly available at
https://github.com/bogdanraonic3/OOD_Detection_ScientificML

</details>


### [487] [Scaling with Collapse: Efficient and Predictable Training of LLM Families](https://arxiv.org/abs/2509.25087)
*Shane Bergsma,Bin Claire Zhang,Nolan Dey,Shaheer Muhammad,Gurpreet Gosal,Joel Hestness*

Main category: cs.LG

TL;DR: 研究表明在实际缩放配方下LLM训练损失曲线可坍缩，展示其作为高效训练标志及应用，还训练了Celerity模型。


<details>
  <summary>Details</summary>
Motivation: 探究Qiu等人发现的损失曲线坍缩现象在实际缩放配方下的LLM家族中是否成立。

Method: 依据经验缩放定律，将优化超参数设置为给定数据预算的最优值。

Result: 损失曲线在超参数最优时可跨尺度坍缩，偏差可诊断训练问题，坍缩曲线可实现早停。

Conclusion: 坍缩是计算高效训练的标志，是开发高效LLM的有效工具。

Abstract: Effective LLM training relies on *consistency*, meaning that key quantities
-- such as final losses and optimal hyperparameters -- scale predictably across
model sizes. Qiu et al. (2025) recently showed that this consistency extends
beyond scalars: whole training loss curves can *collapse* onto a universal
trajectory after a simple normalization. What remains unclear is whether this
phenomenon holds for LLM families trained under *practical scaling recipes*,
where width, depth, learning rate, batch size, and weight decay are scaled
jointly. We show that it does: loss curves collapse across scales precisely
when optimization hyperparameters are set optimally for the given data budget,
in accordance with recent empirical scaling laws. Collapse thus emerges as a
signature of compute-efficient training. We demonstrate two applications at
scale: (1) deviation-from-collapse provides a sensitive, early diagnostic of
training pathologies, and (2) the predictability of collapsed curves enables
early stopping in large-scale hyperparameter tuning. Finally, we train a
competitive LLM family, *Celerity*, using these insights, highlighting collapse
as an effective tool for developing efficient LLMs.

</details>


### [488] [ORPO-Distill: Mixed-Policy Preference Optimization for Cross-Architecture LLM Distillation](https://arxiv.org/abs/2509.25100)
*Aasheesh Singh,Vishal Vaddina,Dagnachew Birru*

Main category: cs.LG

TL;DR: 介绍ORPO - Distill用于跨架构大语言模型蒸馏，在多数据集和模型上优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 解决跨架构大语言模型蒸馏问题，改进标准思维链蒸馏方法。

Method: 将问题构建为偏好优化任务，通过多样推理轨迹传递知识，采用优势比偏好优化目标和混合策略。

Result: 在五个数据集和多个学生模型的实验中，持续优于传统黑盒知识蒸馏基线。

Conclusion: ORPO - Distill是有效的跨架构大语言模型蒸馏方法。

Abstract: We introduce ORPO-Distill, a general-purpose method for cross-architecture
LLM distillation that formulates the problem as a preference optimization task.
Unlike standard CoT distillation, the approach transfers knowledge through
diverse reasoning traces. It employs an Odds-Ratio Preference Optimization
objective that contrasts teacher and student traces for more effective
learning, and adopts a mixed-policy strategy for utilizing student-generated
outputs, outperforming both off- and on-policy alternatives. Experiments on
five datasets and multiple student models show consistent improvements over
conventional black-box KD baselines.

</details>


### [489] [Towards generalizable deep ptychography neural networks](https://arxiv.org/abs/2509.25104)
*Albert Vong,Steven Henke,Oliver Hoidn,Hanna Ruth,Junjing Deng,Alexander Hexemer,Apurva Mehta,Arianna Gleason,Levi Hancock,Nicholas Schwarz*

Main category: cs.LG

TL;DR: 提出一种无监督训练工作流，强调探针学习，使单个神经网络能跨多个光束线重建实验，还能训练实验引导模型提供实时反馈。


<details>
  <summary>Details</summary>
Motivation: 在加速采集率下需要实时反馈，现有深度学习方法在不同实验条件下缺乏鲁棒性。

Method: 结合实验测量的探针和合成的程序生成对象，提出强调探针学习的无监督训练工作流。

Result: 模型在跨多个光束线重建未见实验方面有良好表现，探针学习与分布内学习同样重要，使用合成工作流训练的模型重建保真度与仅用实验数据训练的相当。

Conclusion: 该方法能训练实验引导模型，在动态实验条件下提供实时反馈。

Abstract: X-ray ptychography is a data-intensive imaging technique expected to become
ubiquitous at next-generation light sources delivering many-fold increases in
coherent flux. The need for real-time feedback under accelerated acquisition
rates motivates surrogate reconstruction models like deep neural networks,
which offer orders-of-magnitude speedup over conventional methods. However,
existing deep learning approaches lack robustness across diverse experimental
conditions. We propose an unsupervised training workflow emphasizing probe
learning by combining experimentally-measured probes with synthetic,
procedurally generated objects. This probe-centric approach enables a single
physics-informed neural network to reconstruct unseen experiments across
multiple beamlines; among the first demonstrations of multi-probe
generalization. We find probe learning is equally important as in-distribution
learning; models trained using this synthetic workflow achieve reconstruction
fidelity comparable to those trained exclusively on experimental data, even
when changing the type of synthetic training object. The proposed approach
enables training of experiment-steering models that provide real-time feedback
under dynamic experimental conditions.

</details>


### [490] [Rethinking Entropy Regularization in Large Reasoning Models](https://arxiv.org/abs/2509.25133)
*Yuxian Jiang,Yafu Li,Guanxu Chen,Dongrui Liu,Yu Cheng,Jing Shao*

Main category: cs.LG

TL;DR: 论文指出RLVR中存在熵坍塌和过早收敛问题，提出SIREN方法，在五个数学基准测试中表现优于先前方法，缓解了LRM的RLVR过早收敛问题。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR中熵坍塌和过早收敛问题，传统朴素熵正则化方法在LRM中失效。

Method: 提出SIREN方法，通过两步熵掩码机制（top - p掩码和峰值熵掩码）将探索限制在有意义的动作和状态子集，并将正则化转换为自锚定形式以稳定训练。

Result: 在五个数学基准测试中，SIREN平均性能优于先前基于熵的RLVR方法，如在AIME24/25上用Qwen2.5 - Math - 7B有+6.6 maj@k的提升，促进了响应多样性并使熵保持在适当水平。

Conclusion: SIREN有效缓解了LRM的RLVR中常见的过早收敛问题。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown great promise
in enhancing the reasoning abilities of large reasoning models (LRMs). However,
it suffers from a critical issue: entropy collapse and premature convergence.
Naive entropy regularization, a common approach for encouraging exploration in
the traditional RL literature, fails to address this problem in the context of
LRM. Our analysis reveals that this failure stems from the vast action space
and long trajectories in LRMs, which easily trigger a global entropy explosion
as the model indiscriminately explores all possible actions and states. To
address this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method
that confines exploration to a meaningful subset of actions and states. SIREN
achieves this through a two-step entropy masking mechanism, consisting of a
top-p mask and a peak-entropy mask. In addition, regularization is transformed
into a self-anchored form to stabilize training. Across five mathematical
benchmarks, SIREN attains superior average performance over previous
entropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on
AIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes
greater response diversity and maintains entropy at an appropriate level, which
helps to preserve the validation pass@k throughout training. This effectively
mitigates the premature convergence problem common in RLVR for LRM.

</details>


### [491] [BALF: Budgeted Activation-Aware Low-Rank Factorization for Fine-Tuning-Free Model Compression](https://arxiv.org/abs/2509.25136)
*David González Martínez*

Main category: cs.LG

TL;DR: 提出BALF框架用于无微调模型压缩，在多尺度和架构上效果好。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络压缩技术需昂贵微调或搜索过程，在普通硬件上不实用。

Method: 提出通用激活感知分解框架和可扩展的预算秩分配器，组成BALF无微调压缩模型的高效管道。

Result: 在多个尺度和架构上验证有效性，如ResNeXt - 101减少45%FLOPs，top - 1准确率仅下降1个百分点。

Conclusion: BALF在无微调模式下取得了出色的压缩效果。

Abstract: Neural network compression techniques typically require expensive fine-tuning
or search procedures, rendering them impractical on commodity hardware.
Inspired by recent LLM compression research, we present a general
activation-aware factorization framework that can be applied to a broad range
of layers. Moreover, we introduce a scalable budgeted rank allocator that
allows flexible control over compression targets (e.g., retaining 50% of
parameters) with no overhead. Together, these components form BALF, an
efficient pipeline for compressing models without fine-tuning. We demonstrate
its effectiveness across multiple scales and architectures, from ResNet-20 on
CIFAR-10 to ResNeXt-101 and vision transformers on ImageNet, and show that it
achieves excellent results in the fine-tuning-free regime. For instance, BALF
reduces FLOPs on ResNeXt-101 by 45% with only a 1-percentage-point top-1
accuracy drop.

</details>


### [492] [Chance-constrained Flow Matching for High-Fidelity Constraint-aware Generation](https://arxiv.org/abs/2509.25157)
*Jinhao Liang,Yixuan Sun,Anirban Samaddar,Sandeep Madireddy,Ferdinando Fioretto*

Main category: cs.LG

TL;DR: 提出无训练方法CCFM解决生成模型满足硬约束问题，实验显示其性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在满足硬约束时存在重复投影会扭曲分布、多阶段程序增加复杂度和累积误差等问题。

Method: 提出无训练方法Chance - constrained Flow Matching (CCFM)，将随机优化融入采样过程。

Result: CCFM能保证可行性，理论上等同于投影到干净样本定义的可行集，减轻分布失真。实验表明其在建模复杂物理系统和分子对接问题上表现优于现有模型。

Conclusion: CCFM是一种有效方法，能在满足硬约束的同时保持高保真样本生成。

Abstract: Generative models excel at synthesizing high-fidelity samples from complex
data distributions, but they often violate hard constraints arising from
physical laws or task specifications. A common remedy is to project
intermediate samples onto the feasible set; however, repeated projection can
distort the learned distribution and induce a mismatch with the data manifold.
Thus, recent multi-stage procedures attempt to defer projection to clean
samples during sampling, but they increase algorithmic complexity and
accumulate errors across steps. This paper addresses these challenges by
proposing a novel training-free method, Chance-constrained Flow Matching
(CCFM), that integrates stochastic optimization into the sampling process,
enabling effective enforcement of hard constraints while maintaining
high-fidelity sample generation. Importantly, CCFM guarantees feasibility in
the same manner as conventional repeated projection, yet, despite operating
directly on noisy intermediate samples, it is theoretically equivalent to
projecting onto the feasible set defined by clean samples. This yields a
sampler that mitigates distributional distortion. Empirical experiments show
that CCFM outperforms current state-of-the-art constrained generative models in
modeling complex physical systems governed by partial differential equations
and molecular docking problems, delivering higher feasibility and fidelity.

</details>


### [493] [Physics-Informed Inductive Biases for Voltage Prediction in Distribution Grids](https://arxiv.org/abs/2509.25158)
*Ehimare Okoyomon,Arbel Yaniv,Christoph Goebel*

Main category: cs.LG

TL;DR: 本文研究归纳偏置对提升配电网电压预测模型学习潮流能力的作用，评估三种策略并实验，提供有效模型假设见解。


<details>
  <summary>Details</summary>
Motivation: 配电网电压预测对维持电力系统稳定性至关重要，但机器学习方法（如GNN）在有限或不完整数据上训练时泛化能力差。

Method: 系统研究归纳偏置，评估三种物理信息策略：潮流约束损失函数、复值神经网络、基于残差的任务重构；使用ENGAGE数据集进行对照实验。

Result: 通过实验隔离每种归纳偏置的影响，评估标准预测性能和分布外泛化能力。

Conclusion: 为现代配电网可靠高效电压预测中最有效引导学习的模型假设提供实用见解。

Abstract: Voltage prediction in distribution grids is a critical yet difficult task for
maintaining power system stability. Machine learning approaches, particularly
Graph Neural Networks (GNNs), offer significant speedups but suffer from poor
generalization when trained on limited or incomplete data. In this work, we
systematically investigate the role of inductive biases in improving a model's
ability to reliably learn power flow. Specifically, we evaluate three
physics-informed strategies: (i) power-flow-constrained loss functions, (ii)
complex-valued neural networks, and (iii) residual-based task reformulation.
Using the ENGAGE dataset, which spans multiple low- and medium-voltage grid
configurations, we conduct controlled experiments to isolate the effect of each
inductive bias and assess both standard predictive performance and
out-of-distribution generalization. Our study provides practical insights into
which model assumptions most effectively guide learning for reliable and
efficient voltage prediction in modern distribution networks.

</details>


### [494] [TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion](https://arxiv.org/abs/2509.25171)
*Sophia Tang,Yuchen Zhu,Molei Tao,Pranam Chatterjee*

Main category: cs.LG

TL;DR: 提出TR2 - D2框架优化奖励引导的离散扩散轨迹用于离散序列生成微调，在生物序列扩散模型微调中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习用于扩散微调的方法易强化次优轨迹，需克服该挑战。

Method: 引入TR2 - D2框架，用蒙特卡罗树搜索（MCTS）构建重放缓冲区，在随机最优控制目标下微调预训练离散扩散模型。

Result: 在生物序列扩散模型的单目标和多目标微调中验证了框架的有效性。

Conclusion: TR2 - D2在离散序列生成中进行可靠奖励引导微调是有效的。

Abstract: Reinforcement learning with stochastic optimal control offers a promising
framework for diffusion fine-tuning, where a pre-trained diffusion model is
optimized to generate paths that lead to a reward-tilted distribution. While
these approaches enable optimization without access to explicit samples from
the optimal distribution, they require training on rollouts under the current
fine-tuned model, making them susceptible to reinforcing sub-optimal
trajectories that yield poor rewards. To overcome this challenge, we introduce
TRee Search Guided TRajectory-Aware Fine-Tuning for Discrete Diffusion
(TR2-D2), a novel framework that optimizes reward-guided discrete diffusion
trajectories with tree search to construct replay buffers for trajectory-aware
fine-tuning. These buffers are generated using Monte Carlo Tree Search (MCTS)
and subsequently used to fine-tune a pre-trained discrete diffusion model under
a stochastic optimal control objective. We validate our framework on single-
and multi-objective fine-tuning of biological sequence diffusion models,
highlighting the overall effectiveness of TR2-D2 for reliable reward-guided
fine-tuning in discrete sequence generation.

</details>


### [495] [XQC: Well-conditioned Optimization Accelerates Deep Reinforcement Learning](https://arxiv.org/abs/2509.25174)
*Daniel Palenicek,Florian Vogt,Joe Watson,Ingmar Posner,Jan Peters*

Main category: cs.LG

TL;DR: 文章研究批评网络优化景观，提出XQC算法，在多项控制任务中实现最先进的样本效率，且参数更少。


<details>
  <summary>Details</summary>
Motivation: 当前提升深度强化学习样本效率的工作多是增加复杂度，文章从批评网络优化景观出发采取更有原则的方法。

Method: 利用批评网络Hessian矩阵的特征谱和条件数，研究常见架构设计决策对训练动态的影响，结合批量归一化、权重归一化和分布交叉熵损失。

Result: 提出的组合产生的条件数比基线小几个数量级，自然限制梯度范数，XQC算法在多项任务中实现最先进样本效率且参数更少。

Conclusion: 基于对批评网络优化景观的分析提出的XQC算法有效提升了样本效率。

Abstract: Sample efficiency is a central property of effective deep reinforcement
learning algorithms. Recent work has improved this through added complexity,
such as larger models, exotic network architectures, and more complex
algorithms, which are typically motivated purely by empirical performance. We
take a more principled approach by focusing on the optimization landscape of
the critic network. Using the eigenspectrum and condition number of the
critic's Hessian, we systematically investigate the impact of common
architectural design decisions on training dynamics. Our analysis reveals that
a novel combination of batch normalization (BN), weight normalization (WN), and
a distributional cross-entropy (CE) loss produces condition numbers orders of
magnitude smaller than baselines. This combination also naturally bounds
gradient norms, a property critical for maintaining a stable effective learning
rate under non-stationary targets and bootstrapping. Based on these insights,
we introduce XQC: a well-motivated, sample-efficient deep actor-critic
algorithm built upon soft actor-critic that embodies these optimization-aware
principles. We achieve state-of-the-art sample efficiency across 55
proprioception and 15 vision-based continuous control tasks, all while using
significantly fewer parameters than competing methods.

</details>


### [496] [SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression](https://arxiv.org/abs/2509.25176)
*Haoming Wen,Yushi Bai,Juanzi Li,Jie Tang*

Main category: cs.LG

TL;DR: 介绍SIRI方法，可让大推理模型推理更高效准确，训练时交替压缩和扩展推理预算，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有减少大推理模型重复思维模式的方法常牺牲性能，需克服性能与效率的权衡问题。

Method: 采用迭代交替压缩和扩展推理预算的训练机制，动态调整训练时的最大展开长度。

Result: 在DeepSeek - R1 - Distill - Qwen - 1.5B上训练，SIRI - low三次迭代后在AIME24上性能提升43.2%，减少46.9%的token使用，SIRI - high准确率最高。

Conclusion: 训练时周期性改变大推理模型输出截断长度，可动态平衡推理的探索性和效率，达到最优平衡点。

Abstract: We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved
Compression, a simple yet effective RL approach for Large Reasoning Models
(LRMs) that enables more efficient and accurate reasoning. Existing studies
have observed repetitive thinking patterns in LRMs, and attempts to reduce them
often come at the cost of performance. In this paper, we show that this
trade-off can be overcome through a training regime that iteratively alternates
between compressing and expanding the reasoning budget, by dynamically
adjusting the maximum rollout length during training. The compression phase
cuts the rollout length, forcing the model to make precise and valuable
decisions within a limited context, which effectively reduces redundant tokens
and increases reasoning density. The expansion phase then relaxes the length
limit, providing space for the model to explore and plan in long-horizon
settings. Remarkably, we find that after each compression-expansion cycle, the
model's performance improves even as its output length decreases, steadily
pushing it closer to the Pareto frontier in the performance-efficiency
trade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves
performance on AIME24 by 43.2% while reducing token usage by 46.9% after three
iterations, and SIRI-high achieves the highest accuracy compared to all other
methods (Figure 1). Our findings shed light on the potential of periodically
oscillating the LRM's output truncation length during training to dynamically
balance exploration and efficiency in reasoning, converging towards an optimal
"sweet spot" between the two. Our models are publicly available.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [497] [Training Deep Normalization-Free Spiking Neural Networks with Lateral Inhibition](https://arxiv.org/abs/2509.23253)
*Peiyu Liu,Jianhao Ding,Zhaofei Yu*

Main category: cs.NE

TL;DR: 提出无需归一化的学习框架训练深度脉冲神经网络，结合侧抑制，实验证明能稳定训练且具生物现实性和竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统深度脉冲神经网络训练依赖显式归一化方案，存在性能与生物现实性的权衡，需解决该冲突。

Method: 提出含侧抑制的无归一化学习框架，用符合戴尔定律的E-I神经元电路替代传统前馈层，提出E-I Init和E-I Prop技术。

Result: 在多个数据集和网络架构实验中，框架能稳定训练深度脉冲神经网络，无需显式归一化也有竞争力。

Conclusion: 工作为训练深度脉冲神经网络提供解决方案，也为探索侧抑制在大规模皮层计算功能提供计算平台。

Abstract: Spiking neural networks (SNNs) have garnered significant attention as a
central paradigm in neuromorphic computing, owing to their energy efficiency
and biological plausibility. However, training deep SNNs has critically
depended on explicit normalization schemes, such as batch normalization,
leading to a trade-off between performance and biological realism. To resolve
this conflict, we propose a normalization-free learning framework that
incorporates lateral inhibition inspired by cortical circuits. Our framework
replaces the traditional feedforward SNN layer with a circuit of distinct
excitatory (E) and inhibitory (I) neurons that complies with Dale's law. The
circuit dynamically regulates neuronal activity through subtractive and
divisive inhibition, which respectively control the activity and the gain of
excitatory neurons. To enable and stabilize end-to-end training of the
biologically constrained SNN, we propose two key techniques: E-I Init and E-I
Prop. E-I Init is a dynamic parameter initialization scheme that balances
excitatory and inhibitory inputs while performing gain control. E-I Prop
decouples the backpropagation of the E-I circuits from the forward propagation
and regulates gradient flow. Experiments across several datasets and network
architectures demonstrate that our framework enables stable training of deep
SNNs with biological realism and achieves competitive performance without
resorting to explicit normalizations. Therefore, our work not only provides a
solution to training deep SNNs but also serves a computational platform for
further exploring the functions of lateral inhibition in large-scale cortical
computation.

</details>


### [498] [Spatiotemporal Radar Gesture Recognition with Hybrid Spiking Neural Networks: Balancing Accuracy and Efficiency](https://arxiv.org/abs/2509.23303)
*Riccardo Mazzieri,Eleonora Cicciarella,Jacopo Pegoraro,Federico Corradi,Michele Rossi*

Main category: cs.NE

TL;DR: 本文首次将脉冲神经网络用于基于雷达的飞机调度信号分类的人体活动识别，提出混合架构，减少参数且精度损失小，证明SNNs是高效有竞争力的解决方案。


<details>
  <summary>Details</summary>
Motivation: 基于雷达的人体活动识别在边缘部署时计算需求大，需要高效解决方案。

Method: 提出结合卷积模块和LIF神经元的混合架构用于时空特征处理。

Result: 模型相比基线减少88%可训练参数，精度损失不到1%，能很好泛化到Soli手势数据集。

Conclusion: SNNs是基于雷达的人体活动识别的高效且有竞争力的解决方案。

Abstract: Radar-based Human Activity Recognition (HAR) offers privacy and robustness
over camera-based methods, yet remains computationally demanding for edge
deployment. We present the first use of Spiking Neural Networks (SNNs) for
radar-based HAR on aircraft marshalling signal classification. Our novel hybrid
architecture combines convolutional modules for spatial feature extraction with
Leaky Integrate-and-Fire (LIF) neurons for temporal processing, inherently
capturing gesture dynamics. The model reduces trainable parameters by 88\% with
under 1\% accuracy loss compared to baselines, and generalizes well to the Soli
gesture dataset. Through systematic comparisons with Artificial Neural
Networks, we demonstrate the trade-offs of spiking computation in terms of
accuracy, latency, memory, and energy, establishing SNNs as an efficient and
competitive solution for radar-based HAR.

</details>


### [499] [Network-Optimised Spiking Neural Network for Event-Driven Networking](https://arxiv.org/abs/2509.23516)
*Muhammad Bilal*

Main category: cs.NE

TL;DR: 介绍了适用于边缘网络任务的NOS神经元模型，分析其特性，实验显示在网络任务中优于其他模型并给出部署建议。


<details>
  <summary>Details</summary>
Motivation: 经典神经元模型不适合边缘网络时间关键任务，需要新模型。

Method: 引入NOS两变量单元，分析其平衡条件、稳定性等，采用Poisson shot - noise模型处理随机到达，通过实验对比性能。

Result: NOS在匹配M/M/1均值、减少长尾、低抖动短稳定时间、零样本预测、预警F1和检测延迟等方面表现良好，优于MLP等模型。

Conclusion: NOS适合边缘网络任务，给出数据驱动初始化、训练和稳定性检查等部署指导。

Abstract: Spiking neural networks offer event-driven computation suited to
time-critical networking tasks such as anomaly detection, local routing
control, and congestion management at the edge. Classical units, including
Hodgkin-Huxley, Izhikevich, and the Random Neural Network, map poorly to these
needs. We introduce Network-Optimised Spiking (NOS), a compact two-variable
unit whose state encodes normalised queue occupancy and a recovery resource.
The model uses a saturating nonlinearity to enforce finite buffers, a
service-rate leak, and graph-local inputs with delays and optional per link
gates. It supports two differentiable reset schemes for training and
deployment. We give conditions for equilibrium existence and uniqueness, local
stability tests from the Jacobian trace and determinant, and a network
threshold that scales with the Perron eigenvalue of the coupling matrix. The
analysis yields an operational rule g* ~ k* rho(W) linking damping and offered
load, shows how saturation enlarges the stable region, and explains finite-size
smoothing of synchrony onsets. Stochastic arrivals follow a Poisson shot-noise
model aligned with telemetry smoothing. Against queueing baselines, NOS matches
M/M/1 mean by calibration while truncating deep tails under bursty input. In
closed loop it gives, low-jitte with short settling. In zero-shot, label-free
forecasting NOS is calibrated per node from arrival statistics. Its NOS
dynamics yield high AUROC/AUPRC, enabling timely detection of congestion onsets
with few false positives. Under a train-calibrated residual protocol across
chain, star, and scale-free topologies, NOS improves early-warning F1 and
detection latency over MLP, RNN, GRU, and tGNN. We provide guidance for
data-driven initialisation, surrogate-gradient training with a homotopy on
reset sharpness, and explicit stability checks with topology-aware bounds for
resource constrained deployments.

</details>


### [500] [Accuracy-Robustness Trade Off via Spiking Neural Network Gradient Sparsity Trail](https://arxiv.org/abs/2509.23762)
*Nhan T. Luu*

Main category: cs.NE

TL;DR: 研究发现特定架构配置下SNNs有自然梯度稀疏性，无需显式正则化就能实现对抗防御，且存在鲁棒性与泛化性的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决SNNs在视觉相关任务中实现对抗鲁棒性这一未充分探索的挑战。

Method: 研究特定架构配置下SNNs的梯度特性。

Result: 特定架构下SNNs有自然梯度稀疏性，能实现先进的对抗防御性能，且发现鲁棒性和泛化性存在权衡。

Conclusion: SNNs自然梯度稀疏性可用于对抗防御，但要考虑鲁棒性和泛化性的平衡。

Abstract: Spiking Neural Networks (SNNs) have attracted growing interest in both
computational neuroscience and artificial intelligence, primarily due to their
inherent energy efficiency and compact memory footprint. However, achieving
adversarial robustness in SNNs, particularly for vision-related tasks, remains
a nascent and underexplored challenge. Recent studies have proposed leveraging
sparse gradients as a form of regularization to enhance robustness against
adversarial perturbations. In this work, we present a surprising finding: under
specific architectural configurations, SNNs exhibit natural gradient sparsity
and can achieve state-of-the-art adversarial defense performance without the
need for any explicit regularization. Further analysis reveals a trade-off
between robustness and generalization: while sparse gradients contribute to
improved adversarial resilience, they can impair the model's ability to
generalize; conversely, denser gradients support better generalization but
increase vulnerability to attacks.

</details>


### [501] [CaRe-BN: Precise Moving Statistics for Stabilizing Spiking Neural Networks in Reinforcement Learning](https://arxiv.org/abs/2509.23791)
*Zijie Xu,Xinyu Shi,Yiting Dong,Zihan Huang,Zhaofei Yu*

Main category: cs.NE

TL;DR: 提出CaRe - BN解决SNN在在线强化学习中因BN统计不准确导致的问题，实验表明其提升SNN性能，甚至超ANN。


<details>
  <summary>Details</summary>
Motivation: 直接训练的SNN依赖BN稳定梯度更新，但在线强化学习中BN统计不准确阻碍开发，导致收敛慢和策略欠佳，限制SNN在资源受限设备上应用。

Method: 提出CaRe - BN，含置信引导的自适应更新策略和重新校准机制。

Result: 在连续控制基准实验中，CaRe - BN使SNN性能最高提升22.6%，装备CaRe - BN的SNN比ANN性能高5.9%。

Conclusion: 为适用于RL的BN技术指明新方向，推动高效高性能神经形态智能体发展。

Abstract: Spiking Neural Networks (SNNs) offer low-latency and energy-efficient
decision-making on neuromorphic hardware by mimicking the event-driven dynamics
of biological neurons. However, due to the discrete and non-differentiable
nature of spikes, directly trained SNNs rely heavily on Batch Normalization
(BN) to stabilize gradient updates. In online Reinforcement Learning (RL),
imprecise BN statistics hinder exploitation, resulting in slower convergence
and suboptimal policies. This challenge limits the adoption of SNNs for
energy-efficient control on resource-constrained devices. To overcome this, we
propose Confidence-adaptive and Re-calibration Batch Normalization (CaRe-BN),
which introduces (\emph{i}) a confidence-guided adaptive update strategy for BN
statistics and (\emph{ii}) a re-calibration mechanism to align distributions.
By providing more accurate normalization, CaRe-BN stabilizes SNN optimization
without disrupting the RL training process. Importantly, CaRe-BN does not alter
inference, thus preserving the energy efficiency of SNNs in deployment.
Extensive experiments on continuous control benchmarks demonstrate that CaRe-BN
improves SNN performance by up to $22.6\%$ across different spiking neuron
models and RL algorithms. Remarkably, SNNs equipped with CaRe-BN even surpass
their ANN counterparts by $5.9\%$. These results highlight a new direction for
BN techniques tailored to RL, paving the way for neuromorphic agents that are
both efficient and high-performing.

</details>


### [502] [Hybrid Layer-Wise ANN-SNN With Surrogate Spike Encoding-Decoding Structure](https://arxiv.org/abs/2509.24411)
*Nhan T. Luu,Duong T. Luu,Pham Ngoc Nam,Truong Cong Thang*

Main category: cs.NE

TL;DR: 提出新的混合ANN - SNN框架，用替代梯度实现端到端可微训练，取得有竞争力的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有混合ANN - SNN方法在反向传播时缺乏深度层间合作，需新框架解决。

Method: 在传统ANN管道中集成逐层编解码SNN块，使用基于位平面的尖峰编码函数的替代梯度。

Result: 达到与现有纯ANN和SNN模型有竞争力的准确率，保留尖峰计算的效率和时间表示优势。

Conclusion: 首次在混合ANN - SNN中使用位平面编码的替代梯度和尖峰编码器接口，为未来研究开辟新方向。

Abstract: Spiking Neural Networks (SNNs) have gained significant traction in both
computational neuroscience and artificial intelligence for their potential in
energy-efficient computing. In contrast, artificial neural networks (ANNs)
excel at gradient-based optimization and high accuracy. This contrast has
consequently led to a growing subfield of hybrid ANN-SNN research. However,
existing hybrid approaches often rely on either a strict separation between ANN
and SNN components or employ SNN-only encoders followed by ANN classifiers due
to the constraints of non-differentiability of spike encoding functions,
causing prior hybrid architectures to lack deep layer-wise cooperation during
backpropagation. To address this gap, we propose a novel hybrid ANN-SNN
framework that integrates layer-wise encode-decode SNN blocks within
conventional ANN pipelines. Central to our method is the use of surrogate
gradients for a bit-plane-based spike encoding function, enabling end-to-end
differentiable training across ANN and SNN layers. This design achieves
competitive accuracy with state-of-the-art pure ANN and SNN models while
retaining the potential efficiency and temporal representation benefits of
spiking computation. To the best of our knowledge, this is the first
implementation of a surrogate gradient for bit plane coding specifically and
spike encoder interface in general to be utilized in the context of hybrid
ANN-SNN, successfully leading to a new class of hybrid models that pave new
directions for future research.

</details>


### [503] [Enabling Physical AI through Biological Principles](https://arxiv.org/abs/2509.24521)
*Wilkie Olin-Ammentorp*

Main category: cs.NE

TL;DR: 本文探讨借鉴生物智能改进人工智能，分析差距并提出机会领域。


<details>
  <summary>Details</summary>
Motivation: 大语言模型使计算需求大增，现代人工智能发展多未借鉴生物计算，需借助自然智能使人工智能多样化以适应现实。

Method: 分析人工智能与自然智能的差距，对比生物和人工计算系统元素。

Result: 明确了自然智能中尚未被人工智能有效捕获的领域。

Conclusion: 有必要进一步从生物中获取灵感，提出自然智能启发机制在人工智能软硬件上的机会领域。

Abstract: The introduction of large language models has significantly expanded global
demand for computing; addressing this growing demand requires novel approaches
that introduce new capabilities while addressing extant needs. Although
inspiration from biological systems served as the foundation on which modern
artificial intelligence (AI) was developed, many modern advances have been made
without clear parallels to biological computing. As a result, the ability of
techniques inspired by "natural intelligence" (NI) to inflect modern AI systems
may be questioned. However, by analyzing remaining disparities between AI and
NI, we argue that further biological inspiration is indeed necessary to
diversify the capabilities of artificial systems and enable them to succeed in
real-world environments and adapt to niche applications. To elucidate which NI
mechanisms can contribute toward this goal, we review and compare elements of
biological and artificial computing systems, emphasizing areas of NI that have
not yet been effectively captured by AI. We then suggest areas of opportunity
for NI-inspired mechanisms that can inflect AI hardware and software.

</details>


### [504] [PredNext: Explicit Cross-View Temporal Prediction for Unsupervised Learning in Spiking Neural Networks](https://arxiv.org/abs/2509.24844)
*Yiting Dong,Jianhao Ding,Zijie Xu,Tong Bu,Zhaofei Yu,Tiejun Huang*

Main category: cs.NE

TL;DR: 提出PredNext模块，可显式建模时间关系，集成到不同自监督目标中，在多个数据集上提升性能，为大规模时序视频数据的无监督深度SNN提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前无监督SNN大多采用浅层架构或局部可塑性规则，无法建模长程时间依赖和保持时间特征一致性，导致语义不稳定，阻碍大规模时序视频数据的深度无监督SNN发展。

Method: 提出PredNext模块，通过跨视图未来步骤预测和片段预测显式建模时间关系，且该模块可与多种自监督目标集成。

Result: PredNext在不同任务和自监督方法中显著提升性能，仅在UCF101上无监督训练就达到与ImageNet预训练监督权重相当的性能，还提高了时间特征一致性和网络泛化能力。

Conclusion: 该工作为大规模时序视频数据的无监督深度SNN提供了有效基础。

Abstract: Spiking Neural Networks (SNNs), with their temporal processing capabilities
and biologically plausible dynamics, offer a natural platform for unsupervised
representation learning. However, current unsupervised SNNs predominantly
employ shallow architectures or localized plasticity rules, limiting their
ability to model long-range temporal dependencies and maintain temporal feature
consistency. This results in semantically unstable representations, thereby
impeding the development of deep unsupervised SNNs for large-scale temporal
video data. We propose PredNext, which explicitly models temporal relationships
through cross-view future Step Prediction and Clip Prediction. This
plug-and-play module seamlessly integrates with diverse self-supervised
objectives. We firstly establish standard benchmarks for SNN self-supervised
learning on UCF101, HMDB51, and MiniKinetics, which are substantially larger
than conventional DVS datasets. PredNext delivers significant performance
improvements across different tasks and self-supervised methods. PredNext
achieves performance comparable to ImageNet-pretrained supervised weights
through unsupervised training solely on UCF101. Additional experiments
demonstrate that PredNext, distinct from forced consistency constraints,
substantially improves temporal feature consistency while enhancing network
generalization capabilities. This work provides a effective foundation for
unsupervised deep SNNs on large-scale temporal video data.

</details>


### [505] [DelRec: learning delays in recurrent spiking neural networks](https://arxiv.org/abs/2509.24852)
*Alexandre Queant,Ulysse Rançon,Benoit R Cottereau,Timothée Masquelier*

Main category: cs.NE

TL;DR: 本文提出DelRec方法训练循环脉冲层轴突或突触延迟，在多个数据集达SOTA，证明循环延迟对SNN时间处理很重要。


<details>
  <summary>Details</summary>
Motivation: 尖峰神经网络（SNN）有高能量效率潜力，可训练延迟能增强表达性，但实用学习方法刚出现，需有效方法训练延迟。

Method: 引入DelRec，基于代理梯度学习，利用可微插值技术处理非整数延迟。

Result: 可训练循环延迟优于前馈延迟，在两个挑战性时间数据集上达新SOTA，在Spiking Heidelberg Digit数据集上与SOTA相当。

Conclusion: 循环延迟对SNN时间处理至关重要，DelRec可有效优化延迟，为在可编程延迟神经形态硬件上高效部署铺平道路。

Abstract: Spiking neural networks (SNNs) are a bio-inspired alternative to conventional
real-valued deep learning models, with the potential for substantially higher
energy efficiency. Interest in SNNs has recently exploded due to a major
breakthrough: surrogate gradient learning (SGL), which allows training SNNs
with backpropagation, strongly outperforming other approaches. In SNNs, each
synapse is characterized not only by a weight but also by a transmission delay.
While theoretical works have long suggested that trainable delays significantly
enhance expressivity, practical methods for learning them have only recently
emerged. Here, we introduce ''DelRec'', the first SGL-based method to train
axonal or synaptic delays in recurrent spiking layers, compatible with any
spiking neuron model. DelRec leverages a differentiable interpolation technique
to handle non-integer delays with well-defined gradients at training time. We
show that trainable recurrent delays outperform feedforward ones, leading to
new state-of-the-art (SOTA) on two challenging temporal datasets (Spiking
Speech Command, an audio dataset, and Permuted Sequential MNIST, a vision one),
and match the SOTA on the now saturated Spiking Heidelberg Digit dataset using
only vanilla Leaky-Integrate-and-Fire neurons with stateless (instantaneous)
synapses. Our results demonstrate that recurrent delays are critical for
temporal processing in SNNs and can be effectively optimized with DelRec,
paving the way for efficient deployment on neuromorphic hardware with
programmable delays. Our code is available at :
https://github.com/alexmaxad/DelRec.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [506] [Tiny-QMoE](https://arxiv.org/abs/2509.22951)
*Jack Cashman,Jiaqi Nie*

Main category: cs.PF

TL;DR: QMoE模型为压缩大规模混合专家（MoE）模型提供实用方法，但未考虑移动设备等更受限环境及延迟问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模MoE模型的内存限制问题，同时考虑在更受限环境和低延迟场景使用LLM技术。

Method: 提出QMoE模型用于MoE模型压缩。

Result: 未提及具体结果。

Conclusion: QMoE模型虽有优势，但需考虑移动设备等受限环境和延迟问题。

Abstract: The QMoE model provides a practical approach for compression of massive
Mixture-of-Experts (MoE) models. QMoE offers a solution geared towards memory
limitations that often reach terabyte scales, and it has the advantage of
working with high sparsity models which implicitly lend themselves to
compression techniques. QMoE also has the advantage of only taking MoE models
into account and does not evaluate its use with non mixture of expert systems.
Although this prior attempt focuses on the limitations of large servers with
the latest NVIDIA hardware which in the case of the H100 and V100 which have 80
GB of HBM (High Bandwidth Memory), what is not being considered is a
significantly more constrained environment, such as in the case of mobile
devices which may have in the case of the iPhone anywhere from 4 to 8 GB of
unified memory which also needs to be shared with the operating system and
additional processes. Although edge devices such as phones and laptops are
becoming increasingly more computationally powerful, they are still not close
to the level of advanced server machines such as NVIDIA. An additional
constraint that we must consider is that of latency. The communication time of
sending a request to an LLM server and then getting it back is an additional
waiting time that can be removed. We may also want to use LLM technology in
environments where there is no reliable network connection.

</details>


### [507] [DarwinGame: Playing Tournaments for Tuning Applications in Noisy Cloud Environments](https://arxiv.org/abs/2509.25090)
*Rohan Basu Roy,Vijay Gadepally,Devesh Tiwari*

Main category: cs.PF

TL;DR: 本文引入共享易干扰计算环境下的性能调优新领域，提出DarwinGame方案，相比现有方案可减少超27%执行时间且性能波动小于0.5%。


<details>
  <summary>Details</summary>
Motivation: 现有调优器因无法考虑调优时的干扰，设计上存在显著不足，需在共享易干扰计算环境中进行有效性能调优。

Method: 提出DarwinGame，采用基于锦标赛的设计，系统比较不同可调参数配置下的应用执行情况，以识别噪声环境中不同配置的相对性能。

Result: 与现有解决方案相比，DarwinGame执行时间减少超27%，性能波动小于0.5%。

Conclusion: DarwinGame是首个能帮助开发者在共享、易受干扰的云环境中调优应用的性能调优器。

Abstract: This work introduces a new subarea of performance tuning -- performance
tuning in a shared interference-prone computing environment. We demonstrate
that existing tuners are significantly suboptimal by design because of their
inability to account for interference during tuning. Our solution, DarwinGame,
employs a tournament-based design to systematically compare application
executions with different tunable parameter configurations, enabling it to
identify the relative performance of different tunable parameter configurations
in a noisy environment. Compared to existing solutions, DarwinGame achieves
more than 27% reduction in execution time, with less than 0.5% performance
variability. DarwinGame is the first performance tuner that will help
developers tune their applications in shared, interference-prone, cloud
environments.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [508] [Similarity-Based Assessment of Computational Reproducibility in Jupyter Notebooks](https://arxiv.org/abs/2509.23645)
*A S M Shahadat Hossain,Colin Brown,David Koop,Tanu Malik*

Main category: cs.SE

TL;DR: 本文介绍用于评估Jupyter Notebook结果可重复性的相似度可重复性指数（SRI），通过案例展示其应用。


<details>
  <summary>Details</summary>
Motivation: Jupyter Notebook重运行结果可能因随机因素、库版本变化等不一致，需评估可重复性。

Method: 基于不同类型Python对象的相似度指标开发新方法，比较重运行和原始输出，为每个有输出的单元格给出[0, 1]的量化分数和定性见解。

Result: 提出了SRI指标，进行案例研究展示利用不同相似度指标量化计算可重复性。

Conclusion: SRI可用于评估Jupyter Notebook结果的可重复性。

Abstract: Computational reproducibility refers to obtaining consistent results when
rerunning an experiment. Jupyter Notebook, a web-based computational notebook
application, facilitates running, publishing, and sharing computational
experiments along with their results. However, rerunning a Jupyter Notebook may
not always generate identical results due to various factors, such as
randomness, changes in library versions, or variations in the computational
environment. This paper introduces the Similarity-based Reproducibility Index
(SRI) -- a metric for assessing the reproducibility of results in Jupyter
Notebooks. SRI employs novel methods developed based on similarity metrics
specific to different types of Python objects to compare rerun outputs against
original outputs. For every cell generating an output in a rerun notebook, SRI
reports a quantitative score in the range [0, 1] as well as some qualitative
insights to assess reproducibility. The paper also includes a case study in
which the proposed metric is applied to a set of Jupyter Notebooks,
demonstrating how various similarity metrics can be leveraged to quantify
computational reproducibility.

</details>


### [509] [A benchmark for vericoding: formally verified program synthesis](https://arxiv.org/abs/2509.22908)
*Sergiu Bursuc,Theodore Ehrenborg,Shaowei Lin,Lacramioara Astefanoaei,Ionel Emilian Chiosa,Jure Kukovec,Alok Singh,Oliver Butterley,Adem Bizid,Quinn Dougherty,Miranda Zhao,Max Tan,Max Tegmark*

Main category: cs.SE

TL;DR: 提出并测试最大的vericoding基准，含12504个形式规范，测试不同语言vericoding成功率等，结果公开。


<details>
  <summary>Details</summary>
Motivation: 创建并测试vericoding的大型基准，对比vibe coding。

Method: 使用现成的大语言模型对基准中的形式规范进行vericoding测试。

Result: Lean的vericoding成功率27%，Verus/Rust为44%，Dafny为82%；添加自然语言描述不显著提升性能；过去一年大语言模型使纯Dafny验证进度从68%提升到96%。

Conclusion: 该基准可用于评估vericoding能力，公开结果利于相关研究。

Abstract: We present and test the largest benchmark for vericoding, LLM-generation of
formally verified code from formal specifications - in contrast to vibe coding,
which generates potentially buggy code from a natural language description. Our
benchmark contains 12,504 formal specifications, with 3,029 in Dafny, 2,334 in
Verus/Rust and 7,141 in Lean. Of these, 6,174 are new unseen problems. We find
vericoding success rates of 27% in Lean, 44% in Verus/Rust and 82% in Dafny
using off-the-shelf LLMs. Adding natural-language descriptions does not
significantly improve performance. We also find that LLM progress has improved
progress on pure Dafny verification from 68% to 96% over the past year. The
benchmark and vericoding results are shared at
https://github.com/Beneficial-AI-Foundation/vericoding-benchmark

</details>


### [510] [Towards Human-interpretable Explanation in Code Clone Detection using LLM-based Post Hoc Explainer](https://arxiv.org/abs/2509.22978)
*Teeradaj Racharak,Chaiyong Ragkhitwetsagul,Chayanee Junplong,Akara Supratak*

Main category: cs.SE

TL;DR: 本文提出利用大语言模型上下文学习能力解释基于机器学习的代码克隆检测器预测结果的方法，用ChatGPT - 4解释GraphCodeBERT的克隆结果，证明该方法有前景。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的代码克隆检测器像黑盒，当前事后解释技术需白盒访问模型或计算成本高，因此需要先进的事后解释器。

Method: 利用大语言模型上下文学习能力，使用ChatGPT - 4解释GraphCodeBERT的代码克隆结果。

Result: 该方法作为事后解释器有前景，能给出98%的正确解释，95%的时间能给出良好解释，降低温度到零可提高解释准确性，但解释和代码示例仅在部分情况有用。

Conclusion: 为未来使用大语言模型作为软件工程任务的事后解释器研究奠定基础。

Abstract: Recent studies highlight various machine learning (ML)-based techniques for
code clone detection, which can be integrated into developer tools such as
static code analysis. With the advancements brought by ML in code
understanding, ML-based code clone detectors could accurately identify and
classify cloned pairs, especially semantic clones, but often operate as black
boxes, providing little insight into the decision-making process. Post hoc
explainers, on the other hand, aim to interpret and explain the predictions of
these ML models after they are made, offering a way to understand the
underlying mechanisms driving the model's decisions. However, current post hoc
techniques require white-box access to the ML model or are computationally
expensive, indicating a need for advanced post hoc explainers. In this paper,
we propose a novel approach that leverages the in-context learning capabilities
of large language models to elucidate the predictions made by the ML-based code
clone detectors. We perform a study using ChatGPT-4 to explain the code clone
results inferred by GraphCodeBERT. We found that our approach is promising as a
post hoc explainer by giving the correct explanations up to 98% and offering
good explanations 95% of the time. However, the explanations and the code line
examples given by the LLM are useful in some cases. We also found that lowering
the temperature to zero helps increase the accuracy of the explanation. Lastly,
we list the insights that can lead to further improvements in future work. This
study paves the way for future studies in using LLMs as a post hoc explainer
for various software engineering tasks.

</details>


### [511] [The Matthew Effect of AI Programming Assistants: A Hidden Bias in Software Evolution](https://arxiv.org/abs/2509.23261)
*Fei Gu,Zi Liang,Hongzong LI,Jiahao MA*

Main category: cs.SE

TL;DR: 本文对AI辅助编程进行大规模实验，发现Matthew效应，探讨其对编程生态系统未来演化的影响。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注提示设计和代码生成质量，而LLM驱动开发对软件工程迭代动态的更广泛影响未被充分探索。

Method: 对数千个算法编程任务和数百个框架选择任务进行大规模实验。

Result: 发现Matthew效应，即编程语言或框架越流行，LLM生成代码的成功率越高。

Conclusion: AI系统可能强化现有流行度等级，加速围绕主导工具的收敛，阻碍多样性和创新，并对该效应进行定量描述和探讨其对编程生态系统未来演化的影响。

Abstract: AI-assisted programming is rapidly reshaping software development, with large
language models (LLMs) enabling new paradigms such as vibe coding and agentic
coding. While prior works have focused on prompt design and code generation
quality, the broader impact of LLM-driven development on the iterative dynamics
of software engineering remains underexplored. In this paper, we conduct
large-scale experiments on thousands of algorithmic programming tasks and
hundreds of framework selection tasks to systematically investigate how
AI-assisted programming interacts with the software ecosystem. Our analysis
reveals \textbf{a striking Matthew effect: the more popular a programming
language or framework, the higher the success rate of LLM-generated code}. The
phenomenon suggests that AI systems may reinforce existing popularity
hierarchies, accelerating convergence around dominant tools while hindering
diversity and innovation. We provide a quantitative characterization of this
effect and discuss its implications for the future evolution of programming
ecosystems.

</details>


### [512] [Code Arcades: 3d Visualization of Classes, Dependencies and Software Metrics](https://arxiv.org/abs/2509.23297)
*Anthony Savidis,Christos Vasilopoulos*

Main category: cs.SE

TL;DR: 本文聚焦软件可视化，介绍其目标与作用，给出三项工作贡献以提升源代码理解能力。


<details>
  <summary>Details</summary>
Motivation: 提升对源代码的理解、分析、维护和演化能力，以更好处理大规模系统。

Method: 引入可配置分组机制；结合细粒度和粗粒度软件指标；提供交互式可视化引擎。

Result: 这些进展提供了更具适应性和洞察力的源代码理解方法。

Conclusion: 所提出的三项工作成果有助于更有效地理解源代码。

Abstract: Software visualization seeks to represent software artifacts graphical-ly in
two or three dimensions, with the goal of enhancing comprehension, anal-ysis,
maintenance, and evolution of the source code. In this context, visualiza-tions
employ graphical forms such as dependency structures, treemaps, or time-lines
that incorporate repository histories. These visualizations allow software
engineers to identify structural patterns, detect complexity hotspots, and
infer system behaviors that are difficult to perceive directly from source
text. By adopting metaphor-based approaches, visualization tools provide
macroscopic overviews while enabling focused inspection of specific program
elements, thus offering an accessible means of understanding large-scale
systems. The contri-bution of our work lies in three areas. First, we introduce
a configurable group-ing mechanism that supports flexible organization of code
elements based on arbitrary relationships. Second, we combine fine-grained and
coarse-grained software metrics to provide a multi-level perspective on system
properties. Third, we present an interactive visualization engine that allows
developers to dynamically adjust rendering attributes. Collectively, these
advances provide a more adaptable and insightful approach to source code
comprehension.

</details>


### [513] [Methods for evaluating software accessibility](https://arxiv.org/abs/2509.23469)
*Mykola Kuz,Ivan Yaremiy,Hanna Yaremii,Mykola Pikuliak,Ihor Lazarovych,Mykola Kozlenko,Denys Vekeryk*

Main category: cs.SE

TL;DR: 文章指出现有软件可访问性评估方法不足，提出构建分类和数学模型开发评估方法，对大学网站分析并给出改进建议。


<details>
  <summary>Details</summary>
Motivation: 社会数字化要求软件符合国际可访问性标准，但现有评估方法过于笼统，需开发新的详细方法。

Method: 构建分类和数学模型，开发软件可访问性评估方法。

Result: 开发出评估“可访问性”质量子特性的方法，分析网站对视障人士的包容性并给出改进建议；对大学网站进行分析并提出提升包容性的改进措施。

Conclusion: 相比标准化方法，提出了更详细且实用的可访问性评估方法，有助于创建包容性数字环境。

Abstract: The development and enhancement of methods for evaluating software
accessibility is a relevant challenge in modern software engineering, as
ensuring equal access to digital services is a key factor in improving their
efficiency and inclusivity. The increasing digitalization of society
necessitates the creation of software that complies with international
accessibility standards such as ISO/IEC 25023 and WCAG. Adhering to these
standards helps eliminate barriers to software use for individuals with diverse
physical, sensory, and cognitive needs. Despite advancements in regulatory
frameworks, existing accessibility evaluation methodologies are often
generalized and fail to account for the specific needs of different user
categories or the unique ways they interact with digital systems. This
highlights the need for the development of new, more detailed methods for
defining metrics that influence the quality of user interaction with software
products. Building a classification and mathematical model and developing
accessibility assessment methods for software based on it. A method for
assessing the quality subcharacteristic "Accessibility", which is part of the
"Usability" quality characteristic, has been developed. This enabled the
analysis of a website's inclusivity for individuals with visual impairments,
and the formulation of specific recommendations for further improvements, which
is a crucial step toward creating an inclusive digital environment. Comparing
to standardized approaches, a more detailed and practically oriented
accessibility assessment methodology has been proposed. Using this methodology,
an analysis of the accessibility of the main pages of Vasyl Stefanyk
Precarpathian National University's website was conducted, and improvements
were suggested to enhance its inclusivity.

</details>


### [514] [Improving the Efficiency of LLM Agent Systems through Trajectory Reduction](https://arxiv.org/abs/2509.23586)
*Yuan-An Xiao,Pengfei Gao,Chao Peng,Yingfei Xiong*

Main category: cs.SE

TL;DR: 本文提出AgentDiet方法减少大语言模型多轮代理系统输入令牌成本，评估显示能有效降低成本且不影响性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型多轮代理系统输入令牌计算成本高，现有研究和产品忽视效率问题，本文旨在填补该空白。

Method: 分析现有代理轨迹，发现无用、冗余和过期信息，设计AgentDiet方法自动移除这些信息。

Result: 在两个大语言模型和两个基准测试中，AgentDiet可减少39.9% - 59.7%的输入令牌，或降低21.1% - 35.9%的最终计算成本，且保持代理性能不变。

Conclusion: 轨迹减少是代理系统的一个有前景的方向。

Abstract: Multi-turn agent systems based on Large Language Models (LLMs) have been
increasingly popular for software engineering tasks. While LLM agents show
decent effectiveness, the high computational cost of input tokens due to the
ever-growing trajectory remains an efficiency concern for their applications.
Efficiency is largely neglected in existing studies and agent products, and
this paper fills the gap by introducing an inference-time trajectory reduction
approach to reduce the cost of agents.
  Through analyzing existing agent trajectories, we demonstrate that useless,
redundant, and expired information is widespread in all trajectories, which can
be identified and reduced without harming the agent's performance. We then
design a simple yet effective trajectory reduction approach, AgentDiet, which
automatically removes such waste information. We implement AgentDiet on a
top-performing coding agent, and the evaluation on two LLMs and two benchmarks
shows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final
computational cost by 21.1% ~ 35.9%, while maintaining the same agent
performance. This indicates that trajectory reduction is a promising direction
for agent systems.

</details>


### [515] [PAT-Agent: Autoformalization for Model Checking](https://arxiv.org/abs/2509.23675)
*Xinyue Zuo,Yifan Zhang,Hongshu Wang,Yufan Cai,Zhe Hou,Jing Sun,Jin Song Dong*

Main category: cs.SE

TL;DR: 介绍PAT - Agent框架用于自然语言自动形式化和形式模型修复，结合大语言模型和形式验证，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型应用于形式验证存在规范语言复杂、输出易幻觉、自然语言与形式逻辑有语义差距等挑战。

Method: PAT - Agent框架中，规划大语言模型提取关键元素并生成计划，代码生成大语言模型合成形式模型，用PAT模型检查器验证，有修复循环，还构建了基于网络的用户交互界面。

Result: 在40个系统上实验，PAT - Agent性能优于基线，消融研究证实规划和修复组件重要性，用户研究表明界面易用。

Conclusion: PAT - Agent能有效结合大语言模型和形式验证，实现可验证形式模型的自动化构建，界面也方便非专家用户。

Abstract: Recent advances in large language models (LLMs) offer promising potential for
automating formal methods. However, applying them to formal verification
remains challenging due to the complexity of specification languages, the risk
of hallucinated output, and the semantic gap between natural language and
formal logic. We introduce PAT-Agent, an end-to-end framework for natural
language autoformalization and formal model repair that combines the generative
capabilities of LLMs with the rigor of formal verification to automate the
construction of verifiable formal models. In PAT-Agent, a Planning LLM first
extracts key modeling elements and generates a detailed plan using semantic
prompts, which then guides a Code Generation LLM to synthesize syntactically
correct and semantically faithful formal models. The resulting code is verified
using the Process Analysis Toolkit (PAT) model checker against user-specified
properties, and when discrepancies occur, a Repair Loop is triggered to
iteratively correct the model using counterexamples. To improve flexibility, we
built a web-based interface that enables users, particularly non-FM-experts, to
describe, customize, and verify system behaviors through user-LLM interactions.
Experimental results on 40 systems show that PAT-Agent consistently outperforms
baselines, achieving high verification success with superior efficiency. The
ablation studies confirm the importance of both planning and repair components,
and the user study demonstrates that our interface is accessible and supports
effective formal modeling, even for users with limited formal methods
experience.

</details>


### [516] [Satellite: Detecting and Analyzing Smart Contract Vulnerabilities caused by Subcontract Misuse](https://arxiv.org/abs/2509.23679)
*Zeqin Liao,Yuhong Nan,Zixu Gao,Henglong Liang,Sicheng Hao,Jiajing Wu,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出Satellite框架检测智能合约分包滥用漏洞，实验显示其性能良好，还发现新漏洞。


<details>
  <summary>Details</summary>
Motivation: 智能合约开发者重用子合约会引入漏洞，但检测存在挑战，尤其是编译为字节码后信息和语义模糊。

Method: 提出Satellite框架，用迁移学习恢复继承方法，提取细粒度方法级特征并比较，总结漏洞指标。

Result: 在识别分包滥用漏洞上，精确率84.68%，召回率92.11%，还在真实合约中发现14个新漏洞，涉及201,358美元数字资产。

Conclusion: Satellite框架在检测智能合约分包滥用漏洞方面表现良好。

Abstract: Developers of smart contracts pervasively reuse subcontracts to improve
development efficiency. Like any program language, such subcontract reuse may
unexpectedly include, or introduce vulnerabilities to the end-point smart
contract. Unfortunately, automatically detecting such issues poses several
unique challenges. Particularly, in most cases, smart contracts are compiled as
bytecode, whose class-level information (e.g., inheritance, virtual function
table), and even semantics (e.g., control flow and data flow) are fully
obscured as a single smart contract after compilation.
  In this paper, we propose Satellite, a new bytecode-level static analysis
framework for subcontract misuse vulnerability (SMV) detection in smart
contracts. Satellite incorporates a series of novel designs to enhance its
overall effectiveness.. Particularly, Satellite utilizes a transfer learning
method to recover the inherited methods, which are critical for identifying
subcontract reuse in smart contracts. Further, Satellite extracts a set of
fine-grained method-level features and performs a method-level comparison, for
identifying the reuse part of subcontract in smart contracts. Finally,
Satellite summarizes a set of SMV indicators according to their types, and
hence effectively identifies SMVs. To evaluate Satellite, we construct a
dataset consisting of 58 SMVs derived from real-world attacks and collect
additional 56 SMV patterns from SOTA studies. Experiment results indicate that
Satellite exhibits good performance in identifying SMV, with a precision rate
of 84.68% and a recall rate of 92.11%. In addition, Satellite successfully
identifies 14 new/unknown SMV over 10,011 real-world smart contracts, affecting
a total amount of digital assets worth 201,358 USD.

</details>


### [517] [PerfBench: Can Agents Resolve Real-World Performance Bugs?](https://arxiv.org/abs/2509.24091)
*Spandan Garg,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: 提出PerfBench基准测试集评估编码代理修复性能漏洞能力，当前SOTA代理表现不佳，开发OpenHands - Perf - Agent提升成功率，但仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注功能正确性，缺乏对编码代理解决性能漏洞能力的评估。

Method: 引入包含81个来自GitHub流行.NET仓库性能漏洞修复任务的PerfBench基准，采用新评估框架让代理生成性能基准并验证修复。

Result: 当前SOTA编码代理修复性能漏洞成功率低，OpenHands代理成功率约3%，OpenHands - Perf - Agent达约20%。

Conclusion: 确保代理有基准测试指令和输出处理工具可显著提升性能，PerfBench为提升代理修复性能漏洞能力提供挑战测试集。

Abstract: Performance bugs are inefficiencies in software that waste computational
resources without causing functional failures, making them particularly
challenging to detect and fix. While recent advances in Software Engineering
agents have shown promise in automated bug fixing, existing benchmarks
primarily focus on functional correctness and fail to evaluate agents'
abilities to identify and resolve non-functional issues like performance bugs.
We introduce PerfBench, a benchmark comprising 81 real-world performance
bug-fixing tasks from popular .NET repositories on GitHub. Unlike existing
benchmarks that rely on pre-existing test suites, PerfBench features a novel
evaluation harness that allows agents to generate their own performance
benchmarks and validates fixes by comparing execution metrics collected for
developer fix and agent fix. Each task in PerfBench is derived from actual
developer fixes linked to performance-related issues, which are then verified
by human experts, ensuring real-world relevance. Our evaluation reveals that
current state-of-the-art coding agents struggle with performance optimization
tasks, with baseline OpenHands agent achieving only a ~3% success rate on our
benchmark. We develop OpenHands-Perf-Agent, which incorporates
performance-aware tooling and instructions and achieves a ~20% success rate on
the benchmark. We show that by ensuring the agent has proper instructions to
benchmark its changes and tooling for benchmark output processing, we can
improve the agent performance significantly, but room for improvement still
remains. PerfBench provides a challenging test set for furthering the
capabilities of agents in fixing performance issues.

</details>


### [518] [Influence-Guided Concolic Testing of Transformer Robustness](https://arxiv.org/abs/2509.23806)
*Chih-Duo Hong,Yu Wang,Yao-Chen Chang,Fang Yu*

Main category: cs.SE

TL;DR: 提出针对Transformer分类器的影响引导符号执行测试器，在白盒研究中更高效找到标签翻转输入并揭示决策逻辑，为符号探索和模型测试提供新方法。


<details>
  <summary>Details</summary>
Motivation: 寻找能有效为Transformer分类器找到翻转决策输入的方法，以用于调试和模型审计。

Method: 用基于SHAP估计对路径谓词排序的影响引导方法，构建求解器兼容的纯Python多头自注意力语义，引入调度启发式方法。

Result: 在小L_0预算的紧凑型Transformers白盒研究中，比FIFO基线更高效找到标签翻转输入，在更深网络保持稳定进展，揭示重复紧凑决策逻辑。

Conclusion: 影响信号对符号探索有用，求解器友好的注意力语义和轻量级调度使符号执行测试适用于当代Transformer模型。

Abstract: Concolic testing for deep neural networks alternates concrete execution with
constraint solving to search for inputs that flip decisions. We present an
{influence-guided} concolic tester for Transformer classifiers that ranks path
predicates by SHAP-based estimates of their impact on the model output. To
enable SMT solving on modern architectures, we prototype a solver-compatible,
pure-Python semantics for multi-head self-attention and introduce practical
scheduling heuristics that temper constraint growth on deeper models. In a
white-box study on compact Transformers under small $L_0$ budgets, influence
guidance finds label-flip inputs more efficiently than a FIFO baseline and
maintains steady progress on deeper networks. Aggregating successful attack
instances with a SHAP-based critical decision path analysis reveals recurring,
compact decision logic shared across attacks. These observations suggest that
(i) influence signals provide a useful search bias for symbolic exploration,
and (ii) solver-friendly attention semantics paired with lightweight scheduling
make concolic testing feasible for contemporary Transformer models, offering
potential utility for debugging and model auditing.

</details>


### [519] [Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large Language Models](https://arxiv.org/abs/2509.23812)
*Dianshu Liao,Xin Yin,Shidong Pan,Chao Ni,Zhenchang Xing,Xiaoyu Sun*

Main category: cs.SE

TL;DR: 提出路径敏感框架JUnitGenie用于单元测试生成，评估显示能提升覆盖率并发现真实漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有自动化单元测试生成方法路径不敏感，难以实现足够覆盖率，尤其是复杂执行路径。

Method: 结合代码知识与大语言模型语义能力，从Java项目提取代码知识并转化为结构化提示，引导生成高覆盖率单元测试。

Result: 在2258个复杂方法上评估，JUnitGenie生成有效测试，分支和行覆盖率平均比基线提升29.60%和31.00%，生成的测试用例能发现真实漏洞。

Conclusion: JUnitGenie可有效提升单元测试覆盖率，发现真实世界的软件漏洞。

Abstract: Unit testing is essential for software quality assurance, yet writing and
maintaining tests remains time-consuming and error-prone. To address this
challenge, researchers have proposed various techniques for automating unit
test generation, including traditional heuristic-based methods and more recent
approaches that leverage large language models (LLMs). However, these existing
approaches are inherently path-insensitive because they rely on fixed
heuristics or limited contextual information and fail to reason about deep
control-flow structures. As a result, they often struggle to achieve adequate
coverage, particularly for deep or complex execution paths. In this work, we
present a path-sensitive framework, JUnitGenie, to fill this gap by combining
code knowledge with the semantic capabilities of LLMs in guiding context-aware
unit test generation. After extracting code knowledge from Java projects,
JUnitGenie distills this knowledge into structured prompts to guide the
generation of high-coverage unit tests. We evaluate JUnitGenie on 2,258 complex
focal methods from ten real-world Java projects. The results show that
JUnitGenie generates valid tests and improves branch and line coverage by
29.60% and 31.00% on average over both heuristic and LLM-based baselines. We
further demonstrate that the generated test cases can uncover real-world bugs,
which were later confirmed and fixed by developers.

</details>


### [520] [SolContractEval: A Benchmark for Evaluating Contract-Level Solidity Code Generation](https://arxiv.org/abs/2509.23824)
*Zhifan Ye,Jiachi Chen,Zhenzhe Shao,Lingfeng Bao,Xiaohu Yang,Zhongxin Liu*

Main category: cs.SE

TL;DR: 论文引入首个合约级Solidity代码生成基准SolContractEval，对六个主流大语言模型进行评估，发现模型在Solidity代码生成上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在Solidity代码生成的有效性未充分研究，现有评估无法衡量模型在真实合约开发中的能力。

Method: 引入包含124个来自真实链上合约任务的SolContractEval基准，开发基于历史交易重放的动态评估框架，对六个主流大语言模型进行评估。

Result: Claude - 3.7 - Sonnet整体性能最高，评估模型在Solidity代码生成上不如在通用编程语言类级别生成任务的表现，模型处理标准模式任务较好，处理复杂逻辑和合约间依赖较难，对Solidity特定特性和上下文依赖理解有限。

Conclusion: 现有大语言模型在Solidity代码生成方面存在不足，需要进一步改进。

Abstract: The rise of blockchain has brought smart contracts into mainstream use,
creating a demand for smart contract generation tools. While large language
models (LLMs) excel at generating code in general-purpose languages, their
effectiveness on Solidity, the primary language for smart contracts, remains
underexplored. Solidity constitutes only a small portion of typical LLM
training data and differs from general-purpose languages in its
version-sensitive syntax and limited flexibility. These factors raise concerns
about the reliability of existing LLMs for Solidity code generation.
Critically, existing evaluations, focused on isolated functions and synthetic
inputs, fall short of assessing models' capabilities in real-world contract
development.
  To bridge this gap, we introduce SolContractEval, the first contract-level
benchmark for Solidity code generation. It comprises 124 tasks drawn from real
on-chain contracts across nine major domains. Each task input, consisting of
complete context dependencies, a structured contract framework, and a concise
task prompt, is independently annotated and cross-validated by experienced
developers. To enable precise and automated evaluation of functional
correctness, we also develop a dynamic evaluation framework based on historical
transaction replay. Building on SolContractEval, we perform a systematic
evaluation of six mainstream LLMs. We find that Claude-3.7-Sonnet achieves the
highest overall performance, though evaluated models underperform relative to
their capabilities on class-level generation tasks in general-purpose
programming languages. Second, current models perform better on tasks that
follow standard patterns but struggle with complex logic and inter-contract
dependencies. Finally, they exhibit limited understanding of Solidity-specific
features and contextual dependencies.

</details>


### [521] [HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing](https://arxiv.org/abs/2509.23835)
*Yukai Zhao,Menghan Wu,Xing Hu,Xin Xia*

Main category: cs.SE

TL;DR: 现有大语言模型在代码生成时存在包幻觉问题，本文提出HFUZZER框架测试该问题，经评估其效果良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成中存在包幻觉安全风险，且缺乏相关研究，需测试以缓解风险和防御攻击。

Method: 提出基于短语的模糊测试框架HFUZZER，采用模糊测试技术，基于短语引导模型推理信息生成编码任务，从包信息或编码任务中提取短语确保相关性。

Result: 在多个大语言模型上评估，触发所有选定模型的包幻觉；比突变模糊测试框架多识别2.60倍独特幻觉包，生成更多样化任务；测试GPT - 4o发现46个独特幻觉包。

Conclusion: LLMs在代码生成和环境配置时都可能出现包幻觉，HFUZZER框架能有效测试包幻觉问题。

Abstract: Large Language Models (LLMs) are widely used for code generation, but they
face critical security risks when applied to practical production due to
package hallucinations, in which LLMs recommend non-existent packages. These
hallucinations can be exploited in software supply chain attacks, where
malicious attackers exploit them to register harmful packages. It is critical
to test LLMs for package hallucinations to mitigate package hallucinations and
defend against potential attacks. Although researchers have proposed testing
frameworks for fact-conflicting hallucinations in natural language generation,
there is a lack of research on package hallucinations. To fill this gap, we
propose HFUZZER, a novel phrase-based fuzzing framework to test LLMs for
package hallucinations. HFUZZER adopts fuzzing technology and guides the model
to infer a wider range of reasonable information based on phrases, thereby
generating enough and diverse coding tasks. Furthermore, HFUZZER extracts
phrases from package information or coding tasks to ensure the relevance of
phrases and code, thereby improving the relevance of generated tasks and code.
We evaluate HFUZZER on multiple LLMs and find that it triggers package
hallucinations across all selected models. Compared to the mutational fuzzing
framework, HFUZZER identifies 2.60x more unique hallucinated packages and
generates more diverse tasks. Additionally, when testing the model GPT-4o,
HFUZZER finds 46 unique hallucinated packages. Further analysis reveals that
for GPT-4o, LLMs exhibit package hallucinations not only during code generation
but also when assisting with environment configuration.

</details>


### [522] [Learning-Based Testing for Deep Learning: Enhancing Model Robustness with Adversarial Input Prioritization](https://arxiv.org/abs/2509.23961)
*Sheikh Md Mushfiqur Rahman,Nasir Eisty*

Main category: cs.SE

TL;DR: 现有DNN测试优先级方法效率低，本文结合LBT、假设和变异测试高效优先处理对抗测试用例，结果显示该方法超基线，能加速故障检测，结论是此方法优势明显，是实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于覆盖或置信度的测试优先级方法难以有效识别最易暴露故障的输入，限制了其实际效果，需提升DNN的故障检测和模型鲁棒性。

Method: 选择高概率暴露模型故障的对抗输入子集，不依赖特定架构特征和形式验证，适用于多种DNN。

Result: 所提LBT方法在优先处理暴露故障输入和加速故障检测上超基线方法，能更快发现所有潜在故障。

Conclusion: 该方法不仅提升故障检测能力，还保留输入多样性，为模型再训练提供有效指导，增强鲁棒性，是实际DNN应用中对抗测试优先级的强大实用方案。

Abstract: Context: Deep Neural Networks (DNNs) are increasingly deployed in critical
applications, where resilience against adversarial inputs is paramount.
However, whether coverage-based or confidence-based, existing test
prioritization methods often fail to efficiently identify the most
fault-revealing inputs, limiting their practical effectiveness. Aims: This
project aims to enhance fault detection and model robustness in DNNs by
integrating Learning-Based Testing (LBT) with hypothesis and mutation testing
to efficiently prioritize adversarial test cases. Methods: Our method selects a
subset of adversarial inputs with a high likelihood of exposing model faults,
without relying on architecture-specific characteristics or formal
verification, making it adaptable across diverse DNNs. Results: Our results
demonstrate that the proposed LBT method consistently surpasses baseline
approaches in prioritizing fault-revealing inputs and accelerating fault
detection. By efficiently organizing test permutations, it uncovers all
potential faults significantly faster across various datasets, model
architectures, and adversarial attack techniques. Conclusion: Beyond improving
fault detection, our method preserves input diversity and provides effective
guidance for model retraining, further enhancing robustness. These advantages
establish our approach as a powerful and practical solution for adversarial
test prioritization in real-world DNN applications.

</details>


### [523] [SandCell: Sandboxing Rust Beyond Unsafe Code](https://arxiv.org/abs/2509.24032)
*Jialun Zhang,Merve Gülmez,Thomas Nyman,Gang Tan*

Main category: cs.SE

TL;DR: 本文提出用于Rust语言的SandCell实现灵活轻量级隔离，评估显示其能有效防漏洞且性能开销合理。


<details>
  <summary>Details</summary>
Motivation: 现有隔离不安全Rust代码方法边界固定，无法满足沙箱化安全和不安全代码的表达策略需求。

Method: 利用现有语法边界，允许程序员以最小注释工作量指定沙箱组件，引入新技术减少沙箱间数据传输开销。

Result: 评估表明SandCell能在多种Rust应用中有效防止漏洞。

Conclusion: SandCell可实现灵活轻量级隔离，在防止漏洞同时保持合理性能开销。

Abstract: Rust is a modern systems programming language that ensures memory safety by
enforcing ownership and borrowing rules at compile time. While the unsafe
keyword allows programmers to bypass these restrictions, it introduces
significant risks. Various approaches for isolating unsafe code to protect safe
Rust from vulnerabilities have been proposed, yet these methods provide only
fixed isolation boundaries and do not accommodate expressive policies that
require sandboxing both safe and unsafe code. This paper presents SandCell for
flexible and lightweight isolation in Rust by leveraging existing syntactic
boundaries. SandCell allows programmers to specify which components to sandbox
with minimal annotation effort, enabling fine-grained control over isolation.
The system also introduces novel techniques to minimize overhead when
transferring data between sandboxes. Our evaluation demonstrates SandCell's
effectiveness in preventing vulnerabilities across various Rust applications
while maintaining reasonable performance overheads.

</details>


### [524] [TENET: Leveraging Tests Beyond Validation for Code Generation](https://arxiv.org/abs/2509.24148)
*Yiran Hu,Nan Jiang,Shanchao Liang,Yi Wu,Lin Tan*

Main category: cs.SE

TL;DR: 介绍了TDD在vibe coding时代的重要性及挑战，提出TENET解决挑战，在基准测试中表现出色，是首个结合仓库级上下文的测试驱动代码生成研究。


<details>
  <summary>Details</summary>
Motivation: 在vibe coding时代，TDD面临选择有效测试套件、检索相关代码和利用测试反馈进行代码优化的挑战，需要解决这些问题。

Method: 引入TENET，包含新颖测试套件机制、定制代理工具集和基于反思的优化工作流。

Result: TENET在RepoCod和RepoEval基准测试中Pass@1分别达69.08%和81.77%，优于最佳基线。

Conclusion: TENET能有效应对TDD挑战，本研究是首个结合仓库级上下文的测试驱动代码生成研究，探究了测试套件各方面对LLM代理性能的影响。

Abstract: Test-Driven Development (TDD) is a widely adopted software engineering
practice that requires developers to create and execute tests alongside code
implementation, ensuring that software behavior is continuously validated and
refined. In the era of vibe coding, where developers increasingly delegate code
writing to large language models (LLMs) by specifying high-level intentions,
TDD becomes even more crucial, as test cases serve as executable specifications
that explicitly define and verify intended functionality beyond what
natural-language descriptions and code context can convey. While vibe coding
under TDD is promising, there are three main challenges: (1) selecting a small
yet effective test suite to improve the generation accuracy and control the
execution workload, (2) retrieving context such as relevant code effectively,
and (3) systematically using test feedback for effective code refinement. To
address these challenges, we introduce TENET, an LLM agent for generating
functions in complex real-world repositories under the TDD setting. TENET
features three components: (1) a novel test harness mechanism that selects a
concise test suite to maximize diversity of target usage scenarios; (2) a
tailored agent toolset that performs efficient retrieval of relevant code with
interactive debugging; and (3) a reflection-based refinement workflow that
iteratively analyzes failures, replenishes context, and applies code
refinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval
benchmarks, outperforming the best agentic baselines by 9.49 and 2.17
percentage points, respectively. In addition, this is the first study of
test-driven code generation with repository-level context, examining how
different aspects of test suites affect the performance of LLM agents under the
TDD setting.

</details>


### [525] [Metamorphic Testing for Audio Content Moderation Software](https://arxiv.org/abs/2509.24215)
*Wenxuan Wang,Yongjiang Wu,Junyuan Zhang,Shuqing Li,Yun Peng,Wenting Chen,Shuai Wang,Michael R. Lyu*

Main category: cs.SE

TL;DR: 音频平台发展致有害音频内容传播，现有审核工具易被绕过且相关研究不足，提出MTAM框架测试音频审核软件，测试多种软件和模型有较高错误发现率。


<details>
  <summary>Details</summary>
Motivation: 音频平台有害内容传播，现有审核工具被绕过且对对抗输入有效性研究不足。

Method: 提出MTAM框架，对2000个音频片段进行试点研究，定义14种变形关系，用其生成测试用例，测试五种商业软件和一个学术模型。

Result: 测试商业软件时EFR最高分别达38.6%、18.3%、35.1%、16.7%和51.1%，测试学术算法EFR最高达45.7%。

Conclusion: MTAM框架在测试音频内容审核软件时能有较高错误发现率，可用于检测软件不足。

Abstract: The rapid growth of audio-centric platforms and applications such as WhatsApp
and Twitter has transformed the way people communicate and share audio content
in modern society. However, these platforms are increasingly misused to
disseminate harmful audio content, such as hate speech, deceptive
advertisements, and explicit material, which can have significant negative
consequences (e.g., detrimental effects on mental health). In response,
researchers and practitioners have been actively developing and deploying audio
content moderation tools to tackle this issue. Despite these efforts, malicious
actors can bypass moderation systems by making subtle alterations to audio
content, such as modifying pitch or inserting noise. Moreover, the
effectiveness of modern audio moderation tools against such adversarial inputs
remains insufficiently studied. To address these challenges, we propose MTAM, a
Metamorphic Testing framework for Audio content Moderation software.
Specifically, we conduct a pilot study on 2000 audio clips and define 14
metamorphic relations across two perturbation categories: Audio Features-Based
and Heuristic perturbations. MTAM applies these metamorphic relations to toxic
audio content to generate test cases that remain harmful while being more
likely to evade detection. In our evaluation, we employ MTAM to test five
commercial textual content moderation software and an academic model against
three kinds of toxic content. The results show that MTAM achieves up to 38.6%,
18.3%, 35.1%, 16.7%, and 51.1% error finding rates (EFR) when testing
commercial moderation software provided by Gladia, Assembly AI, Baidu,
Nextdata, and Tencent, respectively, and it obtains up to 45.7% EFR when
testing the state-of-the-art algorithms from the academy.

</details>


### [526] [Comparing Open-Source and Commercial LLMs for Domain-Specific Analysis and Reporting: Software Engineering Challenges and Design Trade-offs](https://arxiv.org/abs/2509.24344)
*Theo Koraag,Niklas Wagner,Felix Dobslaw,Lucas Gren*

Main category: cs.SE

TL;DR: 研究探索开源和商业大语言模型用于财务报告分析和评论生成，发现大语言模型有自动化潜力，但集成有挑战，成功集成需关注多方面。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在跨领域自然语言处理自动化方面有应用，但金融领域特定应用研究有限，本研究聚焦金融报告分析和评论生成及软件工程挑战。

Method: 采用设计科学研究方法，进行探索性案例研究，迭代设计并评估两个基于大语言模型的系统，通过专家评估实际财务报告用例。

Result: 大语言模型有自动化财务报告任务的潜力，但集成有挑战，迭代开发暴露出提示设计等问题，云模型和本地开源模型各有优劣。

Conclusion: 大语言模型在财务报告自动化方面潜力大，但成功集成需关注架构、提示设计和系统可靠性，要通过定制验证机制和工程策略解决特定领域挑战。

Abstract: Context: Large Language Models (LLMs) enable automation of complex natural
language processing across domains, but research on domain-specific
applications like Finance remains limited. Objectives: This study explored
open-source and commercial LLMs for financial report analysis and commentary
generation, focusing on software engineering challenges in implementation.
Methods: Using Design Science Research methodology, an exploratory case study
iteratively designed and evaluated two LLM-based systems: one with local
open-source models in a multi-agent workflow, another using commercial GPT-4o.
Both were assessed through expert evaluation of real-world financial reporting
use cases. Results: LLMs demonstrated strong potential for automating financial
reporting tasks, but integration presented significant challenges. Iterative
development revealed issues including prompt design, contextual dependency, and
implementation trade-offs. Cloud-based models offered superior fluency and
usability but raised data privacy and external dependency concerns. Local
open-source models provided better data control and compliance but required
substantially more engineering effort for reliability and usability.
Conclusion: LLMs show strong potential for financial reporting automation, but
successful integration requires careful attention to architecture, prompt
design, and system reliability. Implementation success depends on addressing
domain-specific challenges through tailored validation mechanisms and
engineering strategies that balance accuracy, control, and compliance.

</details>


### [527] [Efficient Decomposition Identification of Deterministic Finite Automata from Examples](https://arxiv.org/abs/2509.24347)
*Junjie Meng,Jie An,Yong Li,Andrea Turrini,Fanjiang Xu,Naijun Zhan,Miaomiao Zhang*

Main category: cs.SE

TL;DR: 传统DFA学习方法有局限，现有DFA - DIP方法有冗余。本文研究两种DIP变体，提出新框架，用3DFA替代APTA，实验显示效率提升。


<details>
  <summary>Details</summary>
Motivation: 传统DFA学习方法生成的DFA缺乏简单性和互操作性，现有DFA - DIP方法因基于APTA的SAT编码有冗余，存在可扩展性问题。

Method: 研究传统Pareto - 最优DIP和新的状态最优DIP，提出新框架，用直接从标记示例派生的3值DFA（3DFA）替代APTA。

Result: 基于3DFA的方法在Pareto - 最优DIP上实现显著效率提升，为状态最优DIP提供可扩展解决方案。

Conclusion: 用3DFA替代APTA的方法有效，能解决现有DFA - DIP方法的可扩展性问题，提升效率。

Abstract: The identification of deterministic finite automata (DFAs) from labeled
examples is a cornerstone of automata learning, yet traditional methods focus
on learning monolithic DFAs, which often yield a large DFA lacking simplicity
and interoperability. Recent work addresses these limitations by exploring DFA
decomposition identification problems (DFA-DIPs), which model system behavior
as intersections of multiple DFAs, offering modularity for complex tasks.
However, existing DFA-DIP approaches depend on SAT encodings derived from
Augmented Prefix Tree Acceptors (APTAs), incurring scalability limitations due
to their inherent redundancy.
  In this work, we advance DFA-DIP research through studying two variants: the
traditional Pareto-optimal DIP and the novel states-optimal DIP, which
prioritizes a minimal number of states. We propose a novel framework that
bridges DFA decomposition with recent advancements in automata representation.
One of our key innovations replaces APTA with 3-valued DFA (3DFA) derived
directly from labeled examples. This compact representation eliminates
redundancies of APTA, thus drastically reducing variables in the improved SAT
encoding. Experimental results demonstrate that our 3DFA-based approach
achieves significant efficiency gains for the Pareto-optimal DIP while enabling
a scalable solution for the states-optimal DIP.

</details>


### [528] [Walk the Talk: Is Your Log-based Software Reliability Maintenance System Really Reliable?](https://arxiv.org/abs/2509.24352)
*Minghua He,Tong Jia,Chiming Duan,Pei Xiao,Lingzhe Zhang,Kangjin Wang,Yifan Wu,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: 本文针对现有基于深度学习的日志软件可靠性维护方法难以获服务提供商信任的问题，定义诊断忠实度指标，设计评估任务，提出FaithLog系统，在数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的日志软件可靠性维护方法像黑盒，服务提供商难以理解其检测异常的方式，阻碍了在实际生产环境中的信任和部署。

Method: 基于对主要云服务提供商的SRE调查定义诊断忠实度指标，设计基于注意力的根因定位和事件扰动两个评估任务，提出FaithLog系统，通过因果引导的注意力机制和对抗一致性学习实现忠实性。

Result: 现有方法在诊断忠实度上表现不佳，FaithLog系统在两个公共数据集和一个工业数据集上达到了诊断忠实度的最优性能。

Conclusion: 提出的FaithLog系统能有效解决现有方法在信任方面的问题，在诊断忠实度上表现出色。

Abstract: Log-based software reliability maintenance systems are crucial for sustaining
stable customer experience. However, existing deep learning-based methods
represent a black box for service providers, making it impossible for providers
to understand how these methods detect anomalies, thereby hindering trust and
deployment in real production environments. To address this issue, this paper
defines a trustworthiness metric, diagnostic faithfulness, for models to gain
service providers' trust, based on surveys of SREs at a major cloud provider.
We design two evaluation tasks: attention-based root cause localization and
event perturbation. Empirical studies demonstrate that existing methods perform
poorly in diagnostic faithfulness. Consequently, we propose FaithLog, a
faithful log-based anomaly detection system, which achieves faithfulness
through a carefully designed causality-guided attention mechanism and
adversarial consistency learning. Evaluation results on two public datasets and
one industrial dataset demonstrate that the proposed method achieves
state-of-the-art performance in diagnostic faithfulness.

</details>


### [529] [United We Stand: Towards End-to-End Log-based Fault Diagnosis via Interactive Multi-Task Learning](https://arxiv.org/abs/2509.24364)
*Minghua He,Chiming Duan,Pei Xiao,Tong Jia,Siyu Yu,Lingzhe Zhang,Weijie Hong,Jin Han,Yifan Wu,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: 提出端到端日志故障诊断方法Chimera，基于交互多任务学习，在多数据集表现优于现有方法并成功部署。


<details>
  <summary>Details</summary>
Motivation: 现有故障诊断方法以任务独立方式构建，无法弥合异常检测和根因定位的数据形式及诊断目标差距，导致诊断偏差积累、依赖昂贵监测数据、忽略诊断任务协作关系等问题。

Method: 提出Chimera方法，基于交互多任务学习，在数据、特征和诊断结果层面设计异常检测与根因定位的交互策略，在统一的端到端框架内实现两个子任务的交互。

Result: 在两个公共数据集和一个工业数据集上的评估显示，Chimera在异常检测和根因定位方面均优于现有方法，分别提高了2.92% - 5.00%和19.01% - 37.09%，并成功部署在工业云平台。

Conclusion: Chimera方法有效解决了现有故障诊断方法的问题，具有更好的诊断性能，可应用于实际生产环境。

Abstract: Log-based fault diagnosis is essential for maintaining software system
availability. However, existing fault diagnosis methods are built using a
task-independent manner, which fails to bridge the gap between anomaly
detection and root cause localization in terms of data form and diagnostic
objectives, resulting in three major issues: 1) Diagnostic bias accumulates in
the system; 2) System deployment relies on expensive monitoring data; 3) The
collaborative relationship between diagnostic tasks is overlooked. Facing this
problems, we propose a novel end-to-end log-based fault diagnosis method,
Chimera, whose key idea is to achieve end-to-end fault diagnosis through
bidirectional interaction and knowledge transfer between anomaly detection and
root cause localization. Chimera is based on interactive multi-task learning,
carefully designing interaction strategies between anomaly detection and root
cause localization at the data, feature, and diagnostic result levels, thereby
achieving both sub-tasks interactively within a unified end-to-end framework.
Evaluation on two public datasets and one industrial dataset shows that Chimera
outperforms existing methods in both anomaly detection and root cause
localization, achieving improvements of over 2.92% - 5.00% and 19.01% - 37.09%,
respectively. It has been successfully deployed in production, serving an
industrial cloud platform.

</details>


### [530] [Agentic Services Computing](https://arxiv.org/abs/2509.24380)
*Shuiguang Deng,Hailiang Zhao,Ziqi Wang,Guanjie Cheng,Peng Chen,Wenzhuo Qian,Zhiwei Ling,Jianwei Yin,Albert Y. Zomaya,Schahram Dustdar*

Main category: cs.SE

TL;DR: 介绍了Agentic Service Computing (ASC)新范式，提出生命周期驱动框架，从四个维度分析，揭示特性，识别新兴趋势，为ASC奠定基础。


<details>
  <summary>Details</summary>
Motivation: 应对大语言模型驱动的代理引发的服务计算从静态到动态多代理生态系统的转变。

Method: 提出生命周期驱动框架，围绕设计、部署、运营和演化四个核心阶段；从感知、自主决策、多代理协作和评估四个研究维度进行系统分析。

Result: 揭示代理服务是经过编排的，如上下文感知助力部署、自主推理支持运营等；识别出塑造ASC未来的新兴趋势。

Conclusion: 将服务计算经典原理与基于大语言模型的多智能体系统进展相结合，为ASC建立了全面且具有前瞻性的基础，为开发智能服务提供统一参考。

Abstract: The rise of LLM-powered agents is driving a fundamental transformation in
services computing: from static, request-response functions to dynamic,
goal-oriented, and autonomous multi-agent ecosystems. In response to this
shift, we introduce Agentic Service Computing (ASC), a new paradigm that
reimagines services as intelligent, self-adaptive, and socially embedded
entities. This comprehensive survey presents a lifecycle-driven framework for
ASC, structured around four core phases: Design, Deployment, Operation, and
Evolution. We systematically analyze ASC through four foundational research
dimensions: (1) Perception, Context, and Environment Modeling, (2) Autonomous
Decision-Making and Task Execution, (3) Multi-Agent Collaboration and
Organization, and (4) Evaluation, Value Alignment, and Trustworthiness. We
examine how these dimensions are instantiated, integrated, and continuously
adapted across the service lifecycle. Our synthesis reveals that agentic
services are not merely assembled but orchestrated: contextual awareness
enables robust deployment; autonomous reasoning supports real-time operation;
collaborative structures emerge and evolve through interaction; and
trustworthiness must be upheld as a cross-cutting, lifelong imperative. We
further identify and discuss emerging trends shaping the future of ASC. By
integrating classical principles of services computing with advances in
LLM-based multi-agent systems, this work establishes a holistic and
forward-looking foundation for ASC. It provides a unified reference for
researchers and practitioners aiming to develop adaptive, accountable, and
human-centered intelligent services.

</details>


### [531] [Unit Test Update through LLM-Driven Context Collection and Error-Type-Aware Refinement](https://arxiv.org/abs/2509.24419)
*Yuanhe Zhang,Zhiquan Yang,Shengyi Pan,Zhongxin Liu*

Main category: cs.SE

TL;DR: 文章指出手动维护单元测试效率低，现有自动化方法有局限，提出基于大语言模型的TESTUPDATER方法实现自动化测试更新，构建新基准测试，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手动维护单元测试效率低且有修复延迟或遗漏风险，现有自动化测试维护方法主要关注修复，忽视增强，且难以处理复杂代码更改，测试用例正确性低。

Method: 提出TESTUPDATER方法，利用大语言模型分析代码更改、提取和过滤相关上下文，通过精心设计的提示引导大语言模型处理代码更改和引入新依赖，引入错误类型感知的迭代细化机制，构建新基准UPDATES4J。

Result: TESTUPDATER编译通过率达94.4%，测试通过率达86.7%，分别比SYNTER高15.9%和20.0%，分支覆盖率高12.9%，行覆盖率高15.2%。

Conclusion: TESTUPDATER在自动化单元测试更新方面表现出色，能有效应对生产代码更改，提升测试效率和质量。

Abstract: Unit testing is critical for ensuring software quality and software system
stability. The current practice of manually maintaining unit tests suffers from
low efficiency and the risk of delayed or overlooked fixes. Therefore, an
automated approach is required to instantly update unit tests, with the
capability to both repair and enhance unit tests. However, existing automated
test maintenance methods primarily focus on repairing broken tests, neglecting
the scenario of enhancing existing tests to verify new functionality.
Meanwhile, due to their reliance on rule-based context collection and the lack
of verification mechanisms, existing approaches struggle to handle complex code
changes and often produce test cases with low correctness. To address these
challenges, we propose TESTUPDATER, a novel LLM based approach that enables
automated just-in-time test updates in response to production code changes.
TESTUPDATER first leverages the LLM to analyze code changes and identify
relevant context, which it then extracts and filters. Then, through carefully
designed prompts, TESTUPDATER guides the LLM step by step to handle various
types of code changes and introduce new dependencies, enabling both test repair
and enhancement. Finally, we introduce an error-type-aware iterative refinement
mechanism that executes the LLM-updated tests and repairs failures, which
significantly improves the overall correctness of test updates. Since existing
test repair datasets lack scenarios of test enhancement, we further construct a
new benchmark, UPDATES4J, with 195 real-world samples from 7 projects.
Experimental results show that TESTUPDATER achieves a compilation pass rate of
94.4% and a test pass rate of 86.7%, outperforming the state-of-the-art method
SYNTER by 15.9% and 20.0%, respectively. Furthermore, TESTUPDATER exhibits
12.9% higher branch coverage and 15.2% greater line coverage than SYNTER.

</details>


### [532] [Towards Shift-Up: A Framework and a Prestudy on High-Value Activities in GenAI Native Software Development](https://arxiv.org/abs/2509.24485)
*Vlad Stirbu,Mateen Ahmed Abbasi,Teerath Das,Jesse Haimi,Niko Iljin,Pyry Kotilainen,Petrus Lipsanen,Niko Mäkitalo,Maiju Sipilä,Venla Veijalainen,Tommi Mikkonen*

Main category: cs.SE

TL;DR: 本文提出GenAI原生开发框架shift - up，开展初步研究并提出未来研究目标。


<details>
  <summary>Details</summary>
Motivation: GenAI影响软件工程，相关工具使软件工程发生转变，需框架辅助团队专注高价值工作。

Method: 提出GenAI原生开发框架shift - up，并进行初步研究。

Result: 未提及具体研究结果。

Conclusion: 提出未来更详细研究shift - up的目标。

Abstract: Generative AI (GenAI) has significantly influenced software engineering.
Associated tools have created a shift in software engineering, where
specialized agents, based on user-provided prompts, are replacing human
developers. In this paper, we propose a framework for GenAI native development
that we call \textit{shift-up}, which helps software teams focus on high-value
work while being supported by GenAI. Furthermore, we also present a preliminary
study testing these ideas with current GenAI tools. Towards the end of the
paper, we propose future research goals to study shift-up in more detail.

</details>


### [533] [JSProtect: A Scalable Obfuscation Framework for Mini-Games in WeChat](https://arxiv.org/abs/2509.24498)
*Zhihao Li,Chaozheng Wang,Zongjie Li,Xinyong Peng,Zelin Su,Qun Xia,Haochuan Lu,Ting Xiong,Man Ho Lam,Shuzheng Gao,Yuchong Xie,Cuiyun Gao,Shuai Wang,Yuetang Deng,Huafeng Ma*

Main category: cs.SE

TL;DR: 论文介绍了用于保护微信小游戏生态中JavaScript代码的高吞吐量并行混淆框架JSProtect，它能解决现有工具的问题，兼具高性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 微信小游戏生态存在知识产权盗窃问题，现有JavaScript混淆工具不适用于大规模应用，存在处理时间长、运行性能下降和代码膨胀等问题。

Method: 提出Parallel - Aware Scope Analysis (PASA) 算法，实现独立代码分区用于多核处理和独立命名空间管理以重用短标识符。

Result: JSProtect能在几分钟内处理20MB代码库，保持100%语义等价，代码膨胀控制在20%，保留接近原生的运行性能，对静态分析工具和大语言模型有更好的安全防护效果。

Conclusion: 该工作为工业规模的JavaScript保护提供了新范式，有效平衡了强大的安全性、高性能和可扩展性。

Abstract: The WeChat mini-game ecosystem faces rampant intellectual property theft to
other platforms via secondary development, yet existing JavaScript obfuscation
tools are ill-equipped for large-scale applications, suffering from prohibitive
processing times, severe runtime performance degradation, and unsustainable
code size inflation. This paper introduces JSProtect, a high-throughput
parallelized obfuscation framework designed to overcome these fundamental
limitations. At the core of our framework is the Parallel-Aware Scope Analysis
(PASA) algorithm, which enables two key optimizations: independent code
partitioning for multi-core processing and independent namespace management
that aggressively reuses short identifiers to combat code bloat. Our evaluation
demonstrates that JSProtectprocesses 20MB codebases in minutes, maintaining
100\% semantic equivalence while controlling code size inflation to as low as
20\% compared to over 1,000\% with baseline tools. Furthermore, it preserves
near-native runtime performance and provides superior security effectiveness
against both static analysis tools and large language models. This work
presents a new paradigm for industrial-scale JavaScript protection that
effectively balances robust security with high performance and scalability.

</details>


### [534] [SemGuard: Real-Time Semantic Evaluator for Correcting LLM-Generated Code](https://arxiv.org/abs/2509.24507)
*Qinglin Wang,Zhihong Sun,Ruyun Wang,Tao Huang,Zhi Jin,Ge Li,Chen Lyu*

Main category: cs.SE

TL;DR: 论文指出大语言模型代码生成存在语义错误问题，提出SemGuard框架实时行级语义监督，在多个基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代码生成存在语义错误，现有事后修复和受限解码方法有缺陷，需在解码过程中注入语义信号解决问题。

Method: 提出SemGuard框架，构建SemDiff数据集训练语义评估器，嵌入评估器对部分代码标记偏差、回滚和引导重生成。

Result: 在四个基准测试中，SemGuard比现有基线表现更好，降低语义错误率，提升Pass@1指标。

Conclusion: SemGuard具有模型和语言无关性，能有效解决大语言模型代码生成的语义错误问题。

Abstract: Large Language Models (LLMs) can translate natural language requirements into
code, yet empirical analyses of representative models reveal that semantic
errors-programs that compile but behave incorrectly-constitute the majority of
observed faults (e.g., >60% on DeepSeek-Coder-6.7B and QwenCoder-7B). Post-hoc
repair pipelines detect such faults only after execution, incurring latency,
relying on incomplete test suites, and often mis-localizing the defect. Since
semantic drift originates in the autoregressive decoding process, intervening
while the code is being generated is a direct way to stop error propagation.
Constrained-decoding approaches such as ROCODE attempt this, but still wait
until the entire program runs to obtain feedback and use entropy heuristics
that do not truly capture semantics. A more effective solution must inject
semantic signals-early and precisely-into the decoding process.We present
SemGuard, a semantic-evaluator-driven framework that performs real-time,
line-level semantic supervision. To train the evaluator, we build SemDiff, the
first dataset with fine-grained annotations that mark the exact line where a
correct and an incorrect implementation diverge. The evaluator, once embedded
in the LLM's decoder, flags deviations on partial code, rolls back to the
faulty line, and guides regeneration-without executing the program or requiring
test cases. Across four benchmarks, SemGuard consistently outperforms
state-of-the-art baselines. It lowers the semantic error rate by 19.86% on
SemDiff relative to ROCODE, and lifts Pass@1 by 48.92% on the real-world
LiveCodeBench with CodeLlama-7B. Similar gains hold for StarCoder2-7B on MBPP
and for DeepSeekCoder-6.7B on the Java benchmark SemDiff-Java, demonstrating
model- and language-agnostic effectiveness.

</details>


### [535] [Agentic Specification Generator for Move Programs](https://arxiv.org/abs/2509.24515)
*Yu-Fu Fu,Meng Xu,Taesoo Kim*

Main category: cs.SE

TL;DR: 本文介绍了为Move智能合约设计的自动规范生成工具MSG，展示了LLM在非主流语言中的能力，评估了其性能和设计优势。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的规范生成工具主要关注主流编程语言，对Move这类新兴且面向验证的语言探索不足。

Method: 设计了自动化规范生成工具MSG，采用代理模块化设计，利用规范语言特性，并结合验证工具链的反馈。

Result: MSG为84%的测试Move函数生成了可验证规范，识别出专家之前忽略的子句，比传统设计多生成57%的可验证子句，结合反馈后可验证规范生成率提高30%。

Conclusion: LLM对非主流语言也有强大的代码理解和生成能力，通过特定设计和结合反馈能显著提高规范生成质量。

Abstract: While LLM-based specification generation is gaining traction, existing tools
primarily focus on mainstream programming languages like C, Java, and even
Solidity, leaving emerging and yet verification-oriented languages like Move
underexplored. In this paper, we introduce MSG, an automated specification
generation tool designed for Move smart contracts. MSG aims to highlight key
insights that uniquely present when applying LLM-based specification generation
to a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust
code comprehension and generation capabilities even for non-mainstream
languages. MSG successfully generates verifiable specifications for 84% of
tested Move functions and even identifies clauses previously overlooked by
experts. Additionally, MSG shows that explicitly leveraging specification
language features through an agentic, modular design improves specification
quality substantially (generating 57% more verifiable clauses than conventional
designs). Incorporating feedback from the verification toolchain further
enhances the effectiveness of MSG, leading to a 30% increase in generated
verifiable specifications.

</details>


### [536] [Bridging Developer Instructions and Code Completion Through Instruction-Aware Fill-in-the-Middle Paradigm](https://arxiv.org/abs/2509.24637)
*Zhensu Sun,Chengran Yang,Chao Peng,Pengfei Gao,Xiaoning Du,Li Li,David Lo*

Main category: cs.SE

TL;DR: 现有代码大语言模型在处理开发者意图不明确的代码补全时效果不佳，本文提出IFIM方法，构建数据集，实验证明能提升指令遵循能力且不影响无指令时的性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码大语言模型用于代码补全系统时难以有效利用开发者添加的自然语言指令信息，且现有指令微调技术存在指令遵循和填充能力的权衡问题。

Method: 提出Instruction - aware Fill - in - the - Middle (IFIM)方法，在输入中加入显式指令部分，用GPT - 4o构建大规模数据集。

Result: 将IFIM应用于两个流行基础模型，在基准测试中显著提高指令遵循能力，HumanEval - infilling上Pass@1分数从84.6%提升到93.6%，且不影响无指令时的性能。

Conclusion: IFIM方法能有效提升FIM代码补全模型的指令遵循能力，同时保留无指令时的核心补全能力。

Abstract: Large Language Models (LLMs) have significantly advanced code completion, yet
they often fail when the developer's intent is underspecified in the code
context. To address this, developers usually add natural language instructions
(e.g., comments) into the code context to clarify their intent. However,
existing code LLMs applied for code completion systems merely undergo a
fill-in-the-middle (FIM) pre-training, which struggles to leverage this
information effectively due to the lack of instruction-like training data.
Existing instruction-tuning techniques, which improve instruction-following in
general code generation, paradoxically degrade FIM performance, forcing a
trade-off between instruction-following and infilling capabilities. To address
this gap, we introduce Instruction-aware Fill-in-the-Middle (IFIM), an
instruction-tuning method specifically designed to enhance FIM code completion
models. IFIM extends the conventional FIM training objective by incorporating
an explicit instruction section into the input, enabling the model to learn
from (prefix, instruction, suffix) triplets. This approach allows the model to
effectively leverage developer-provided directives while preserving its core
completion abilities when no instructions are present. To facilitate this, we
constructed a large-scale dataset by using GPT-4o to generate concise,
intent-focused instructions for code infilling examples. We evaluated IFIM by
applying it to two popular base models, Deepseek-Coder and Qwen2.5-Coder, on
the benchmarks derived from HumanEval-infilling and RepoMasterEval. The results
demonstrate that IFIM significantly improves instruction-following
capabilities, boosting the Pass@1 score from 84.6% to 93.6% on
HumanEval-infilling. Moreover, this enhancement does not compromise the models'
original performance on FIM code completion tasks with no instructions
provided.

</details>


### [537] [CoTune: Co-evolutionary Configuration Tuning](https://arxiv.org/abs/2509.24694)
*Gangda Xiong,Tao Chen*

Main category: cs.SE

TL;DR: 提出CoTune工具，通过协同进化考虑目标性能需求信息，实验显示其显著优于现有调优器。


<details>
  <summary>Details</summary>
Motivation: 现有调优器设计大多忽略复杂性能需求，简单纳入需求作为调优目标存在问题。

Method: 提出CoTune工具，创建辅助性能需求与配置协同进化。

Result: 在162个案例中，CoTune在90%的案例中排名最佳，整体提升达2.9倍，且效率更高。

Conclusion: CoTune能在考虑性能需求的同时对其危害具有鲁棒性，显著优于现有调优器。

Abstract: To automatically tune configurations for the best possible system performance
(e.g., runtime or throughput), much work has been focused on designing
intelligent heuristics in a tuner. However, existing tuner designs have mostly
ignored the presence of complex performance requirements (e.g., the latency
shall ideally be 2 seconds), but simply assume that better performance is
always more preferred. This would not only waste valuable information in a
requirement but might also consume extensive resources to tune for a goal with
little gain. Yet, prior studies have shown that simply incorporating the
requirement as a tuning objective is problematic since the requirement might be
too strict, harming convergence; or its highly diverse satisfactions might lead
to premature convergence. In this paper, we propose CoTune, a tool that takes
the information of a given target performance requirement into account through
co-evolution. CoTune is unique in the sense that it creates an auxiliary
performance requirement to be co-evolved with the configurations, which assists
the target performance requirement when it becomes ineffective or even
misleading, hence allowing the tuning to be guided by the requirement while
being robust to its harm. Experiment results on 162 cases (nine systems and 18
requirements) reveal that CoTune considerably outperforms existing tuners,
ranking as the best for 90% cases (against the 0%--35% for other tuners) with
up to 2.9x overall improvements, while doing so under a much better efficiency.

</details>


### [538] [Large language models for behavioral modeling: A literature survey](https://arxiv.org/abs/2509.24782)
*Muhammad Laiq*

Main category: cs.SE

TL;DR: 本文对大语言模型用于行为建模的研究进行综述，筛选14项研究，发现其生成用例和序列图有前景，但缺乏专家评估且多使用GPT模型，建议未来评估更多模型并引入专家。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏大语言模型用于行为建模研究的综述，进行综述有助于确定未来研究方向，告知从业者和教育者其有效性。

Method: 通过基于术语的搜索，筛选出14项相关的主要研究进行分析。

Result: 大语言模型在自动生成用例和序列图方面显示出有前景的结果，当前文献大多缺乏基于专家的评估且主要使用基于GPT的模型。

Conclusion: 未来应评估更多大语言模型用于行为建模，并让领域专家参与评估模型输出。

Abstract: In recent years, large language models (LLMs) have been extensively utilized
for behavioral modeling, for example, to automatically generate sequence
diagrams. However, no overview of this work has been published yet. Such an
overview will help identify future research directions and inform practitioners
and educators about the effectiveness of LLMs in assisting behavioral modeling.
This study aims to provide an overview of the existing research on the use of
LLMs for behavioral modeling, particularly focusing on use case and sequence
diagrams. Through a term-based search, we filtered and identified 14 relevant
primary studies. Our analysis of the selected primary studies reveals that LLMs
have demonstrated promising results in automatically generating use case and
sequence diagrams. In addition, we found that most of the current literature
lacks expert-based evaluations and has mainly used GPT-based models. Therefore,
future work should evaluate a broader range of LLMs for behavioral modeling and
involve domain experts to evaluate the output of LLMs.

</details>


### [539] [Evaluating SAP Joule for Code Generation](https://arxiv.org/abs/2509.24828)
*Joshua Heisler,Johannes Reisinger,Andreas Fischer*

Main category: cs.SE

TL;DR: 本文用HumanEval - X JavaScript基准比较SAP Joule与29个其他模型的JavaScript编码能力，SAP Joule严格准确率80.49%，排名第五。


<details>
  <summary>Details</summary>
Motivation: 对SAP Joule代码生成能力进行比较评估。

Method: 使用HumanEval - X JavaScript基准比较SAP Joule与29个其他模型的JavaScript编码能力。

Result: SAP Joule严格准确率达80.49%，在评估中排名第五。

Conclusion: 这是首次对SAP Joule代码生成能力的比较评估。

Abstract: SAP has released its own proprietary generative model SAP Joule, intended for
various generative tasks, including serving as a code assistant for software
engineers. While Joule is yet not focused on SAP-specific ABAP code generation,
it can be used for other common languages, including Javascript. This paper
compares SAP Joules Javascript coding capabilities against a total of 29 other
models using the HumanEval-X Javascript benchmark. SAP Joule achieves a strict
accuracy of 80.49% as the fifth best model in our evaluation. To the best of
our knowledge, this is the first comparative evaluation of SAP Joule code
generation capabilities.

</details>


### [540] [DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern](https://arxiv.org/abs/2509.24975)
*Lekang Yang,Yuetong Liu,Yitong Zhang,Jia Li*

Main category: cs.SE

TL;DR: 现有LLMs进行单元测试生成（UTG）效率低，扩散LLMs（dLLMs）虽有并行生成能力但存在效率与测试质量权衡问题。提出DiffTester框架，可在不降低输出质量下加速，实验表明其有效且具有通用性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs进行UTG效率低，dLLMs应用于UTG存在效率与测试质量的权衡问题，需要解决以实现高效UTG。

Method: 提出DiffTester框架，通过抽象语法树分析动态识别单元测试的共同结构模式，自适应增加每步生成的令牌数量；扩展TestEval基准，引入Java和C++等编程语言。

Result: 在三个基准和两个代表性模型上的实验表明，DiffTester能显著加速并保持测试覆盖率，且在不同dLLMs和编程语言上有良好的泛化性。

Conclusion: DiffTester为软件开发中的高效UTG提供了实用且可扩展的解决方案。

Abstract: Software development relies heavily on extensive unit testing, which makes
the efficiency of automated Unit Test Generation (UTG) particularly important.
However, most existing LLMs generate test cases one token at a time in each
forward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)
have emerged, offering promising parallel generation capabilities and showing
strong potential for efficient UTG. Despite this advantage, their application
to UTG is still constrained by a clear trade-off between efficiency and test
quality, since increasing the number of tokens generated in each step often
causes a sharp decline in the quality of test cases. To overcome this
limitation, we present DiffTester, an acceleration framework specifically
tailored for dLLMs in UTG. The key idea of DiffTester is that unit tests
targeting the same focal method often share repetitive structural patterns. By
dynamically identifying these common patterns through abstract syntax tree
analysis during generation, DiffTester adaptively increases the number of
tokens produced at each step without compromising the quality of the output. To
enable comprehensive evaluation, we extend the original TestEval benchmark,
which was limited to Python, by introducing additional programming languages
including Java and C++. Extensive experiments on three benchmarks with two
representative models show that DiffTester delivers significant acceleration
while preserving test coverage. Moreover, DiffTester generalizes well across
different dLLMs and programming languages, providing a practical and scalable
solution for efficient UTG in software development. Code and data are publicly
available at https://github.com/wellbeingyang/DLM4UTG-open .

</details>


### [541] [Large Language Models for Software Testing: A Research Roadmap](https://arxiv.org/abs/2509.25043)
*Cristian Augusto,Antonia Bertolino,Guglielmo De Angelis,Francesca Lonetti,Jesús Morán*

Main category: cs.SE

TL;DR: 本文旨在为基于大语言模型的软件测试提供路线图，介绍现状、研究方向并分析挑战和长期影响。


<details>
  <summary>Details</summary>
Motivation: 此前缺乏对基于大语言模型的软件测试进展和研究趋势的结构化展望，为研究者跟上领域发展步伐，需提供相关路线图。

Method: 进行半系统性文献综述，收集文章并归类，审查现状和进行中情况，分析开放挑战。

Result: 对基于大语言模型的软件测试研究进行了分类和梳理，明确了当前状态和研究方向。

Conclusion: 概述了大语言模型对整个软件测试领域的几个预期长期影响。

Abstract: Large Language Models (LLMs) are starting to be profiled as one of the most
significant disruptions in the Software Testing field.
  Specifically, they have been successfully applied in software testing tasks
such as generating test code, or summarizing documentation.
  This potential has attracted hundreds of researchers, resulting in dozens of
new contributions every month, hardening researchers to
  stay at the forefront of the wave. Still, to the best of our knowledge, no
prior work has provided a structured vision of the progress
  and most relevant research trends in LLM-based testing. In this article, we
aim to provide a roadmap that illustrates its current state,
  grouping the contributions into different categories, and also sketching the
most promising and active research directions for the field.
  To achieve this objective, we have conducted a semi-systematic literature
review, collecting articles and mapping them into the most
  prominent categories, reviewing the current and ongoing status, and analyzing
the open challenges of LLM-based software testing.
  Lastly, we have outlined several expected long-term impacts of LLMs over the
whole software testing field.

</details>


### [542] [Towards Reliable Generation of Executable Workflows by Foundation Models](https://arxiv.org/abs/2509.25117)
*Sogol Masoumzadeh,Keheliya Gallaba,Dayi Lin,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文提出利用静态分析反馈的框架，让基础模型检测并修复基于DSL的工作流缺陷，推动从自然语言需求可靠自动生成可执行工作流。


<details>
  <summary>Details</summary>
Motivation: 手动将任务分解为工作流需大量精力和专业知识，基础模型生成DSL工作流在准确性和可靠性上有挑战。

Method: 提出FM生成DSL工作流缺陷的分类法，开发静态分析器Timon识别缺陷，利用Timon反馈指导Pumbaa修复缺陷。

Result: 发现87.27%研究实例至少含一个缺陷，9种缺陷可通过静态分析识别，能引导工具修复缺陷。

Conclusion: 该工作为从自然语言需求可靠自动生成可执行工作流迈出关键一步。

Abstract: Recent advancements in Foundation Models (FMs) have demonstrated significant
progress in comprehending complex natural language to perform intricate tasks.
Successfully executing these tasks often requires orchestrating calls to FMs
alongside other software components. However, manually decomposing a task into
a coherent sequence of smaller, logically aggregated steps, commonly referred
to as workflows, demands considerable effort and specialized domain knowledge.
While FMs can assist in generating such workflows specified in domain-specific
languages (DSLs), achieving accuracy and reliability in this process remains a
challenge.
  This work introduces a framework that leverages static analysis feedback to
enable FMs to detect and repair defects in the DSL-based workflows they
generate. We begin by presenting the first-ever taxonomy of incidences of
defects in FM-generated DSL workflows, categorizing them into 18 distinct
types. Furthermore, we observe a high prevalence of defects across FM-generated
DSL workflows, with 87.27% of the studied instances containing at least one
defect. This, in turn, emphasizes the magnitude of the problem in practice and
underscores the necessity for implementing mitigation strategies. Following
this, we demonstrate that nine types of these defects can be effectively
identified through static analysis of the workflows. For this purpose, we
develop Timon, the first-of-its-kind static analyzer specifically designed for
FM-generated DSL workflows. Finally, we show that by incorporating feedback
from Timon, we can guide Pumbaa, an FM-based tool, to repair the detected
defect incidences. By systematically detecting and repairing defects, our work
provides a crucial step towards the reliable and automated generation of
executable workflows from natural language requirements.

</details>


### [543] [BacPrep: An Experimental Platform for Evaluating LLM-Based Bacalaureat Assessment](https://arxiv.org/abs/2506.04989)
*Dumitran Adrian Marius,Dita Radu*

Main category: cs.SE

TL;DR: 本文介绍了实验性在线平台BacPrep，利用大语言模型Gemini 2.0 Flash为罗马尼亚高中毕业考试提供自动评估和反馈，目前主要收集数据用于后续专家验证。


<details>
  <summary>Details</summary>
Motivation: 解决罗马尼亚高中毕业考试学生，尤其是偏远或服务不足地区学生获取优质备考资源和反馈困难的问题。

Method: 使用过去5年的官方考试题目，借助Google的Gemini 2.0 Flash模型，依据官方评分方案提供实验性反馈。

Result: 平台已在运行，主要进行学生解答和大语言模型输出的数据收集。

Conclusion: 收集的数据集对于专家验证大语言模型在罗马尼亚高中毕业考试环境中的可行性和准确性至关重要，为可靠部署做准备，同时论文详细介绍了设计、数据策略、现状、验证计划和伦理问题。

Abstract: Accessing quality preparation and feedback for the Romanian Bacalaureat exam
is challenging, particularly for students in remote or underserved areas. This
paper introduces BacPrep, an experimental online platform exploring Large
Language Model (LLM) potential for automated assessment, aiming to offer a
free, accessible resource. Using official exam questions from the last 5 years,
BacPrep employs one of Google's newest models, Gemini 2.0 Flash (released Feb
2025), guided by official grading schemes, to provide experimental feedback.
Currently operational, its primary research function is collecting student
solutions and LLM outputs. This focused dataset is vital for planned expert
validation to rigorously evaluate the feasibility and accuracy of this
cutting-edge LLM in the specific Bacalaureat context before reliable
deployment. We detail the design, data strategy, status, validation plan, and
ethics.

</details>


### [544] [Leveraging Generative AI for Enhancing Automated Assessment in Programming Education Contests](https://arxiv.org/abs/2506.05990)
*Stefan Dascalescu,Adrian Marius Dumitran,Mihai Alexandru Vasiluta*

Main category: cs.SE

TL;DR: 本文提出用NLP驱动、生成式AI自动创建竞赛编程评估测试用例，经多数据集评估效果好，还分享资源助力提升评估质量。


<details>
  <summary>Details</summary>
Motivation: 竞赛编程中生成全面测试用例对教育工作者而言资源消耗大且具挑战性。

Method: 采用NLP驱动的生成式AI（大语言模型）自动创建高质量测试用例。

Result: AI生成的测试用例显著提升评估效果，在67%的OJI五年级编程问题中发现了先前未检测到的错误。

Conclusion: 该技术在形成性评估中有补充教育价值，分享的资源能帮助提升评估质量、减轻工作量和深入了解学生表现。

Abstract: Competitive programming contests play a crucial role in cultivating
computational thinking and algorithmic skills among learners. However,
generating comprehensive test cases to effectively assess programming solutions
remains resource-intensive and challenging for educators. This paper introduces
an innovative NLP-driven method leveraging generative AI (large language
models) to automate the creation of high-quality test cases for competitive
programming assessments. We extensively evaluated our approach on diverse
datasets, including 25 years of Romanian Informatics Olympiad (OJI) data for
5th graders, recent competitions hosted on the Kilonova.ro platform, and the
International Informatics Olympiad in Teams (IIOT). Our results demonstrate
that AI-generated test cases substantially enhanced assessments, notably
identifying previously undetected errors in 67% of the OJI 5th grade
programming problems. These improvements underscore the complementary
educational value of our technique in formative assessment contexts. By openly
sharing our prompts, translated datasets, and methodologies, we offer practical
NLP-based tools that educators and contest organizers can readily integrate to
enhance assessment quality, reduce workload, and deepen insights into learner
performance.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [545] [SIMPOL Model for Solving Continuous-Time Heterogeneous Agent Problems](https://arxiv.org/abs/2509.23557)
*Ricardo Alonzo Fernández Salguero*

Main category: q-fin.CP

TL;DR: 本文提出SIMPOL框架用于求解连续时间异质主体模型，介绍其求解方法、特色模块，验证其稳健性和准确性，表明是定量宏观经济研究可靠工具。


<details>
  <summary>Details</summary>
Motivation: 解决连续时间异质主体模型的求解问题，为定量宏观经济研究提供可靠工具。

Method: 使用Howard策略迭代结合迎风有限差分格式求解HJB和FPK方程系统，有新颖的消费政策后处理模块。

Result: 通过一系列综合诊断验证了SIMPOL的稳健性和准确性，计算高效且解符合经济和数学理论。

Conclusion: SIMPOL是定量宏观经济研究的可靠工具。

Abstract: This paper presents SIMPOL (Simplified Policy Iteration), a modular numerical
framework for solving continuous-time heterogeneous agent models. The core
economic problem, the optimization of consumption and savings under
idiosyncratic uncertainty, is formulated as a coupled system of partial
differential equations: a Hamilton-Jacobi-Bellman (HJB) equation for the
agent's optimal policy and a Fokker-Planck-Kolmogorov (FPK) equation for the
stationary wealth distribution. SIMPOL addresses this system using Howard's
policy iteration with an *upwind* finite difference scheme that guarantees
stability. A distinctive contribution is a novel consumption policy
post-processing module that imposes regularity through smoothing and a
projection onto an economically plausible slope band, improving convergence and
model behavior. The robustness and accuracy of SIMPOL are validated through a
set of integrated diagnostics, including verification of contraction in the
Wasserstein-2 metric and comparison with the analytical solution of the Merton
model in the no-volatility case. The framework is shown to be not only
computationally efficient but also to produce solutions consistent with
economic and mathematical theory, offering a reliable tool for research in
quantitative macroeconomics.

</details>


### [546] [Extracting the Structure of Press Releases for Predicting Earnings Announcement Returns](https://arxiv.org/abs/2509.24254)
*Yuntao Wu,Ege Mert Akin,Charles Martineau,Vincent Grégoire,Andreas Veneris*

Main category: q-fin.CP

TL;DR: 研究收益新闻稿文本特征对财报公布日股价回报的预测作用，比较不同方法，得出相关结论并提供实时预测框架。


<details>
  <summary>Details</summary>
Motivation: 探究收益新闻稿中的文本特征如何预测财报公布日的股票回报。

Method: 使用2005 - 2023年超13.8万份新闻稿，比较传统词袋模型和基于BERT的嵌入方法。

Result: 新闻稿内容与收益惊喜信息同样有价值，FinBERT预测力最强，结合模型增强解释力，股价开盘时反映新闻稿内容，若泄露有预测优势，主题分析发现管理层叙述存在自利偏差。

Conclusion: 框架支持实时回报预测，提供解释性，揭示语言在价格形成中的微妙作用。

Abstract: We examine how textual features in earnings press releases predict stock
returns on earnings announcement days. Using over 138,000 press releases from
2005 to 2023, we compare traditional bag-of-words and BERT-based embeddings. We
find that press release content (soft information) is as informative as
earnings surprise (hard information), with FinBERT yielding the highest
predictive power. Combining models enhances explanatory strength and
interpretability of the content of press releases. Stock prices fully reflect
the content of press releases at market open. If press releases are leaked, it
offers predictive advantage. Topic analysis reveals self-serving bias in
managerial narratives. Our framework supports real-time return prediction
through the integration of online learning, provides interpretability and
reveals the nuanced role of language in price formation.

</details>


### [547] [Simulation of the Heston stochastic local volatility model: implicit and explicit approaches](https://arxiv.org/abs/2509.24449)
*Meng cai,Tianze Li*

Main category: q-fin.CP

TL;DR: 本文对比HSLV模型模拟的两种方案与传统方法，展示各方法特点。


<details>
  <summary>Details</summary>
Motivation: HSLV模型中CIR型方差过程模拟有数值挑战，需对比不同模拟方案。

Method: 用蒙特卡罗方法对比截断欧拉法、向后欧拉法与传统欧拉法和几乎精确模拟法。

Result: 截断法强收敛且高波动下稳健，向后法误差最小、压力场景性能稳定但计算成本高。

Conclusion: 不同模拟方案各有优劣，可依具体情况选择。

Abstract: The Heston stochastic-local volatility (HSLV) model is widely used to capture
both market calibration and realistic volatility dynamics, but simulating its
CIR-type variance process is numerically challenging.This paper compare two
alternative schemes for HSLV simulation: the truncated Euler method and the
backward Euler method with the conventional Euler and almost exact simulation
methods in \cite{van2014heston} by using a Monte Carlo method.Numerical results
show that the truncated method achieves strong convergence and remains robust
under high volatility, while the backward method provides the smallest errors
and most stable performance in stress scenarios, though at higher computational
cost.

</details>


### [548] [AlphaSAGE: Structure-Aware Alpha Mining via GFlowNets for Robust Exploration](https://arxiv.org/abs/2509.25055)
*Binqi Chen,Hongjun Ding,Ning Shen,Jinsheng Huang,Taian Guo,Luchen Liu,Ming Zhang*

Main category: q-fin.CP

TL;DR: 提出AlphaSAGE框架解决量化金融中自动挖掘预测信号的问题，表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习框架在生成公式化预测信号时存在奖励稀疏、语义表示不足和难以生成多样化投资组合等问题。

Method: 引入基于关系图卷积网络的结构感知编码器、生成流网络框架和密集多方面奖励结构。

Result: AlphaSAGE在挖掘更多样、新颖和高预测性的预测信号投资组合方面优于现有基线。

Conclusion: AlphaSAGE为自动挖掘预测信号提出了新范式。

Abstract: The automated mining of predictive signals, or alphas, is a central challenge
in quantitative finance. While Reinforcement Learning (RL) has emerged as a
promising paradigm for generating formulaic alphas, existing frameworks are
fundamentally hampered by a triad of interconnected issues. First, they suffer
from reward sparsity, where meaningful feedback is only available upon the
completion of a full formula, leading to inefficient and unstable exploration.
Second, they rely on semantically inadequate sequential representations of
mathematical expressions, failing to capture the structure that determine an
alpha's behavior. Third, the standard RL objective of maximizing expected
returns inherently drives policies towards a single optimal mode, directly
contradicting the practical need for a diverse portfolio of non-correlated
alphas. To overcome these challenges, we introduce AlphaSAGE (Structure-Aware
Alpha Mining via Generative Flow Networks for Robust Exploration), a novel
framework is built upon three cornerstone innovations: (1) a structure-aware
encoder based on Relational Graph Convolutional Network (RGCN); (2) a new
framework with Generative Flow Networks (GFlowNets); and (3) a dense,
multi-faceted reward structure. Empirical results demonstrate that AlphaSAGE
outperforms existing baselines in mining a more diverse, novel, and highly
predictive portfolio of alphas, thereby proposing a new paradigm for automated
alpha mining. Our code is available at https://github.com/BerkinChen/AlphaSAGE.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [549] [Portfolio Analysis Based on Markowitz Stochastic Dominance Criteria: A Behavioral Perspective](https://arxiv.org/abs/2509.22896)
*Peng Xu*

Main category: q-fin.PM

TL;DR: 本文针对有Markowitz随机占优（MSD）偏好的行为投资者构建随机优化问题，建立离散状态空间占优条件，形成混合整数线性规划（MILP）模型并用于金融投资组合分析。


<details>
  <summary>Details</summary>
Motivation: 描述和分析具有MSD偏好的行为投资者。

Method: 在离散状态空间建立占优条件，将其作为线性约束纳入随机优化问题，形成MILP模型。

Result: 成功构建MILP模型，并用于投资组合分析。

Conclusion: 可利用MILP模型研究行为投资者投资决策中的参考点和主观概率扭曲等经典行为因素。

Abstract: This paper develops stochastic optimization problems for describing and
analyzing behavioral investors with Markowitz Stochastic Dominance (MSD)
preferences. Specifically, we establish dominance conditions in a discrete
state-space to capture all reverse S-shaped MSD preferences as well as all
subjective decision weights generated by inverse S-shaped probability weighting
functions. We demonstrate that these dominance conditions can be admitted as
linear constraints into the stochastic optimization problems to formulate
computationally tractable mixed-integer linear programming (MILP) models. We
then employ the developed MILP models in financial portfolio analysis and
examine classic behavioral factors such as reference point and subjective
probability distortion in behavioral investors' portfolio decisions.

</details>


### [550] [Rethinking Portfolio Risk: Forecasting Volatility Through Cointegrated Asset Dynamics](https://arxiv.org/abs/2509.23533)
*Gabriele Casto*

Main category: q-fin.PM

TL;DR: 引入HVR/DVR，利用波动率协整构建VECM预测投资组合波动率，在S&P 500数据上表现良好。


<details>
  <summary>Details</summary>
Motivation: 找到更好的预测投资组合波动率的方法。

Method: 引入HVR/DVR，发现股票和指数波动率的协整关系，构建VECM模型。

Result: HVR通常是平稳的，与指数协整频繁，VECM在中短期预测中MAPE低于基于协方差的预测。

Conclusion: 该方法可解释且易实施，能将协方差分解为市场波动率、相对波动率比率和相关性。

Abstract: We introduce the Historical and Dynamic Volatility Ratios (HVR/DVR) and show
that equity and index volatilities are cointegrated at intraday and daily
horizons. This allows us to construct a VECM to forecast portfolio volatility
by exploiting volatility cointegration. On S&P 500 data, HVR is generally
stationary and cointegration with the index is frequent; the VECM
implementation yields substantially lower mean absolute percentage error (MAPE)
than covariance-based forecasts at short- to medium-term horizons across
portfolio sizes. The approach is interpretable and readily implementable,
factorizing covariance into market volatility, relative-volatility ratios, and
correlations.

</details>


### [551] [From Headlines to Holdings: Deep Learning for Smarter Portfolio Decisions](https://arxiv.org/abs/2509.24144)
*Yun Lin,Jiawei Lou,Jinghe Zhang*

Main category: q-fin.PM

TL;DR: 提出端到端框架用于投资组合优化，结合LSTM、GAT和新闻情感分析，避免传统两步过程，在九只美股上表现优于基准，强调整合多信号价值。


<details>
  <summary>Details</summary>
Motivation: 深度学习为投资组合优化提供新工具，传统方法有不稳定问题，需新方法。

Method: 构建端到端框架，结合LSTM、GAT和金融新闻情感分析，直接学习投资组合权重。

Result: 在九只美股上，模型的累计回报和夏普比率高于等权重和基于CAPM的MVO基准。

Conclusion: 整合价格、关系和情绪信号对投资组合管理有价值，该方法可扩展到更多样化资产集。

Abstract: Deep learning offers new tools for portfolio optimization. We present an
end-to-end framework that directly learns portfolio weights by combining Long
Short-Term Memory (LSTM) networks to model temporal patterns, Graph Attention
Networks (GAT) to capture evolving inter-stock relationships, and sentiment
analysis of financial news to reflect market psychology. Unlike prior
approaches, our model unifies these elements in a single pipeline that produces
daily allocations. It avoids the traditional two-step process of forecasting
asset returns and then applying mean--variance optimization (MVO), a sequence
that can introduce instability. We evaluate the framework on nine U.S. stocks
spanning six sectors, chosen to balance sector diversity and news coverage. In
this setting, the model delivers higher cumulative returns and Sharpe ratios
than equal-weighted and CAPM-based MVO benchmarks. Although the stock universe
is limited, the results underscore the value of integrating price, relational,
and sentiment signals for portfolio management and suggest promising directions
for scaling the approach to larger, more diverse asset sets.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [552] [Forecasting Liquidity Withdraw with Machine Learning Models](https://arxiv.org/abs/2509.22985)
*Haochuan,Wang*

Main category: q-fin.RM

TL;DR: 本文测试预测个股流动性撤回的框架，引入LWI指标，用纳斯达克数据比较不同方法，结果为订单动态等提供见解。


<details>
  <summary>Details</summary>
Motivation: 流动性撤回是市场脆弱性的关键指标，需要预测个股流动性撤回并评估不同模型预测短期订单簿压力的表现。

Method: 引入流动性撤回指数（LWI），使用纳斯达克市场逐订单（MBO）数据，比较线性基准模型（AR、HAR）和非线性树集成模型（XGBoost）。

Result: 除了预测准确性，结果还为订单放置和取消动态提供见解，确定线性与非线性信号占主导的机制，强调流动性撤回预警指标对市场监测和执行的作用。

Conclusion: 所采用的方法和引入的指标能为市场相关方面提供有价值的信息。

Abstract: Liquidity withdrawal is a critical indicator of market fragility. In this
project, I test a framework for forecasting liquidity withdrawal at the
individual-stock level, ranging from less liquid stocks to highly liquid
large-cap tickers, and evaluate the relative performance of competing model
classes in predicting short-horizon order book stress. We introduce the
Liquidity Withdrawal Index (LWI) -- defined as the ratio of order cancellations
to the sum of standing depth and new additions at the best quotes -- as a
bounded, interpretable measure of transient liquidity removal.
  Using Nasdaq market-by-order (MBO) data, we compare a spectrum of approaches:
linear benchmarks (AR, HAR), and non-linear tree ensembles (XGBoost), across
horizons ranging from 250\,ms to 5\,s. Beyond predictive accuracy, our results
provide insights into order placement and cancellation dynamics, identify
regimes where linear versus non-linear signals dominate, and highlight how
early-warning indicators of liquidity withdrawal can inform both market
surveillance and execution.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [553] [STRAPSim: A Portfolio Similarity Metric for ETF Alignment and Portfolio Trades](https://arxiv.org/abs/2509.24151)
*Mingshu Li,Dhruv Desai,Jerinsh Jeyapaulraj,Philip Sommer,Riya Jain,Peter Chu,Dhagash Mehta*

Main category: q-fin.ST

TL;DR: 提出名为STRAPSim的新方法计算投资组合相似度，在多项任务中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有投资组合相似度度量方法无法捕捉成分间相似性及部分重叠投资组合的细微关系，准确度量相似度对金融应用至关重要。

Method: 通过基于语义相似性匹配成分、根据投资组合份额加权、通过残差感知贪婪对齐聚合结果来计算投资组合相似度。

Result: 在预测准确性和排名对齐方面始终优于基线，与基于回报的相似度具有最高的Spearman相关性。

Conclusion: 该方法提供了可扩展、可解释的框架，在ETF基准测试、投资组合构建和系统执行中有用。

Abstract: Accurately measuring portfolio similarity is critical for a wide range of
financial applications, including Exchange-traded Fund (ETF) recommendation,
portfolio trading, and risk alignment. Existing similarity measures often rely
on exact asset overlap or static distance metrics, which fail to capture
similarities among the constituents (e.g., securities within the portfolio) as
well as nuanced relationships between partially overlapping portfolios with
heterogeneous weights. We introduce STRAPSim (Semantic, Two-level,
Residual-Aware Portfolio Similarity), a novel method that computes portfolio
similarity by matching constituents based on semantic similarity, weighting
them according to their portfolio share, and aggregating results via
residual-aware greedy alignment. We benchmark our approach against Jaccard,
weighted Jaccard, as well as BERTScore-inspired variants across public
classification, regression, and recommendation tasks, as well as on corporate
bond ETF datasets. Empirical results show that our method consistently
outperforms baselines in predictive accuracy and ranking alignment, achieving
the highest Spearman correlation with return-based similarity. By leveraging
constituent-aware matching and dynamic reweighting, portfolio similarity offers
a scalable, interpretable framework for comparing structured asset baskets,
demonstrating its utility in ETF benchmarking, portfolio construction, and
systematic execution.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [554] [Variance-Bounded Evaluation without Ground Truth: VB-Score](https://arxiv.org/abs/2509.22751)
*Kaihua Ding*

Main category: stat.ML

TL;DR: 提出VB-Score评估框架，无需真值标签，能衡量有效性和鲁棒性，实验表明可揭示传统指标隐藏的鲁棒性差异，为机器学习系统评估提供基础。


<details>
  <summary>Details</summary>
Motivation: 传统基于Cranfield范式和标签的评估框架，在缺乏真值标签、存在模糊性和噪声的任务中无法评估系统在不确定解释下的鲁棒性。

Method: 引入VB-Score，枚举合理解释、分配概率，通过期望成功减去方差评估输出，对意图间的一致性能给予奖励，还对其进行形式分析。

Result: 在模糊查询和以实体为中心的检索任务实验中，VB-Score能揭示传统指标隐藏的鲁棒性差异。

Conclusion: VB-Score可实现无标签评估，为模糊或标签稀缺领域的机器学习系统基准测试提供了原则性基础。

Abstract: Reliable evaluation is a central challenge in machine learning when tasks
lack ground truth labels or involve ambiguity and noise. Conventional
frameworks, rooted in the Cranfield paradigm and label-based metrics, fail in
such cases because they cannot assess how robustly a system performs under
uncertain interpretations. We introduce VB-Score, a variance-bounded evaluation
framework that measures both effectiveness and robustness without requiring
ground truth. Given a query or input, VB-Score enumerates plausible
interpretations, assigns probabilities, and evaluates output by expected
success penalized by variance, rewarding consistent performance across intents.
We provide a formal analysis of VB-Score, establishing range, monotonicity, and
stability properties, and relate it to risk-sensitive measures such as
mean-variance utility. Experiments on ambiguous queries and entity-centric
retrieval tasks show that VB-Score surfaces robustness differences hidden by
conventional metrics. By enabling reproducible, label-free evaluation, VB-Score
offers a principled foundation for benchmarking machine learning systems in
ambiguous or label-scarce domains.

</details>


### [555] [Concept activation vectors: a unifying view and adversarial attacks](https://arxiv.org/abs/2509.22755)
*Ekkehard Schnoor,Malik Tiomoko,Jawher Said,Alex Jung,Wojciech Samek*

Main category: stat.ML

TL;DR: 从概率视角研究概念激活向量（CAVs），揭示其依赖非概念分布的潜在漏洞并展示攻击示例。


<details>
  <summary>Details</summary>
Motivation: 从概率角度统一理解CAVs并发现其潜在问题。

Method: 从概率视角推导不同类型CAVs的均值和协方差。

Result: 发现CAVs强烈依赖非概念分布这一潜在漏洞，并给出简单有效的对抗攻击示例。

Conclusion: 有必要对CAVs进行更系统的研究。

Abstract: Concept Activation Vectors (CAVs) are a tool from explainable AI, offering a
promising approach for understanding how human-understandable concepts are
encoded in a model's latent spaces. They are computed from hidden-layer
activations of inputs belonging either to a concept class or to non-concept
examples. Adopting a probabilistic perspective, the distribution of the
(non-)concept inputs induces a distribution over the CAV, making it a random
vector in the latent space. This enables us to derive mean and covariance for
different types of CAVs, leading to a unified theoretical view. This
probabilistic perspective also reveals a potential vulnerability: CAVs can
strongly depend on the rather arbitrary non-concept distribution, a factor
largely overlooked in prior work. We illustrate this with a simple yet
effective adversarial attack, underscoring the need for a more systematic
study.

</details>


### [556] [Conditional Risk Minimization with Side Information: A Tractable, Universal Optimal Transport Framework](https://arxiv.org/abs/2509.23128)
*Xinqiao Xie,Jonathan Yu-Meng Li*

Main category: stat.ML

TL;DR: 提出基于最优运输新联合球公式的分布鲁棒条件风险最小化通用框架，具可解释性、易处理性和可扩展性，在投资组合优化应用中有效。


<details>
  <summary>Details</summary>
Motivation: 从有限数据构建可靠条件分布困难，现有基于最优运输的方法零散且有局限性。

Method: 引入基于最优运输新联合球公式的通用框架。

Result: 框架具有可解释性、易处理性和可扩展性，在投资组合优化应用中，条件模型能收敛到最优解，无条件模型则不能。

Conclusion: 所提出的通用框架在分布鲁棒条件风险最小化问题上有效且实用。

Abstract: Conditional risk minimization arises in high-stakes decisions where risk must
be assessed in light of side information, such as stressed economic conditions,
specific customer profiles, or other contextual covariates. Constructing
reliable conditional distributions from limited data is notoriously difficult,
motivating a series of optimal-transport-based proposals that address this
uncertainty in a distributionally robust manner. Yet these approaches remain
fragmented, each constrained by its own limitations: some rely on point
estimates or restrictive structural assumptions, others apply only to narrow
classes of risk measures, and their structural connections are unclear. We
introduce a universal framework for distributionally robust conditional risk
minimization, built on a novel union-ball formulation in optimal transport.
This framework offers three key advantages: interpretability, by subsuming
existing methods as special cases and revealing their deep structural links;
tractability, by yielding convex reformulations for virtually all major risk
functionals studied in the literature; and scalability, by supporting
cutting-plane algorithms for large-scale conditional risk problems.
Applications to portfolio optimization with rank-dependent expected utility
highlight the practical effectiveness of the framework, with conditional models
converging to optimal solutions where unconditional ones clearly do not.

</details>


### [557] [Identifying Memory Effects in Epidemics via a Fractional SEIRD Model and Physics-Informed Neural Networks](https://arxiv.org/abs/2509.22760)
*Achraf Zinihi*

Main category: stat.ML

TL;DR: 开发PINN框架用于分数阶SEIRD流行病模型参数估计，测试表明能可靠恢复参数，对疫情动力学分析有意义。


<details>
  <summary>Details</summary>
Motivation: 经典整数阶模型无法捕捉疾病传播中的长程记忆效应，需新方法估计分数阶SEIRD模型参数。

Method: 将Caputo分数阶导数通过L1离散化方案嵌入网络残差，学习分数记忆阶α和流行病学参数，采用复合损失确保准确性和生物学一致性。

Result: 对合成猴痘数据测试能可靠恢复参数，应用于COVID - 19显示最优α能捕捉记忆效应，提升预测性能。

Conclusion: PINNs是学习疫情动力学中记忆效应的强大工具，对预测、控制策略和非马尔可夫疫情过程分析有影响。

Abstract: We develop a physics-informed neural network (PINN) framework for parameter
estimation in fractional-order SEIRD epidemic models. By embedding the Caputo
fractional derivative into the network residuals via the L1 discretization
scheme, our method simultaneously reconstructs epidemic trajectories and infers
both epidemiological parameters and the fractional memory order $\alpha$. The
fractional formulation extends classical integer-order models by capturing
long-range memory effects in disease progression, incubation, and recovery. Our
framework learns the fractional memory order $\alpha$ as a trainable parameter
while simultaneously estimating the epidemiological rates $(\beta, \sigma,
\gamma, \mu)$. A composite loss combining data misfit, physics residuals, and
initial conditions, with constraints on positivity and population conservation,
ensures both accuracy and biological consistency. Tests on synthetic Mpox data
confirm reliable recovery of $\alpha$ and parameters under noise, while
applications to COVID-19 show that optimal $\alpha \in (0, 1]$ captures memory
effects and improves predictive performance over the classical SEIRD model.
This work establishes PINNs as a robust tool for learning memory effects in
epidemic dynamics, with implications for forecasting, control strategies, and
the analysis of non-Markovian epidemic processes.

</details>


### [558] [A theoretical guarantee for SyncRank](https://arxiv.org/abs/2509.22766)
*Yang Rao*

Main category: stat.ML

TL;DR: 本文对SyncRank算法进行理论和实证分析，给出非渐近恢复保证，确定临界噪声阈值，实验验证理论预测。


<details>
  <summary>Details</summary>
Motivation: 从有噪声的成对比较中恢复全局排名。

Method: 采用复值数据模型，对相关半定规划（SDP）松弛建立理论分析。

Result: 确定临界噪声阈值sigma = O(sqrt(n / log n))，在此阈值下SyncRank能高概率实现精确排名恢复，实验验证理论预测。

Conclusion: SyncRank算法在不同问题规模和噪声条件下具有鲁棒性。

Abstract: We present a theoretical and empirical analysis of the SyncRank algorithm for
recovering a global ranking from noisy pairwise comparisons. By adopting a
complex-valued data model where the true ranking is encoded in the phases of a
unit-modulus vector, we establish a sharp non-asymptotic recovery guarantee for
the associated semidefinite programming (SDP) relaxation. Our main theorem
characterizes a critical noise threshold - scaling as sigma = O(sqrt(n / log
n)) - below which SyncRank achieves exact ranking recovery with high
probability. Extensive experiments under this model confirm the theoretical
predictions and demonstrate the algorithm's robustness across varying problem
sizes and noise regimes.

</details>


### [559] [Differentially Private Two-Stage Gradient Descent for Instrumental Variable Regression](https://arxiv.org/abs/2509.22794)
*Haodong Liang,Yanhao Jin,Krishnakumar Balasubramanian,Lifeng Lai*

Main category: stat.ML

TL;DR: 研究差分隐私约束下的工具变量回归，提出噪声两阶段梯度下降算法，有收敛率分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: 经典工具变量回归方法有隐私泄露风险，需设计兼具统计效率和差分隐私的算法。

Method: 提出噪声两阶段梯度下降算法，在梯度更新中注入校准噪声确保差分隐私。

Result: 建立有限样本收敛率，推导隐私参数、样本量和迭代复杂度的权衡边界，实验验证准确性与隐私的权衡。

Conclusion: 这是首个为线性模型中工具变量回归提供隐私保证和可证明收敛率的工作。

Abstract: We study instrumental variable regression (IVaR) under differential privacy
constraints. Classical IVaR methods (like two-stage least squares regression)
rely on solving moment equations that directly use sensitive covariates and
instruments, creating significant risks of privacy leakage and posing
challenges in designing algorithms that are both statistically efficient and
differentially private. We propose a noisy two-state gradient descent algorithm
that ensures $\rho$-zero-concentrated differential privacy by injecting
carefully calibrated noise into the gradient updates. Our analysis establishes
finite-sample convergence rates for the proposed method, showing that the
algorithm achieves consistency while preserving privacy. In particular, we
derive precise bounds quantifying the trade-off among privacy parameters,
sample size, and iteration-complexity. To the best of our knowledge, this is
the first work to provide both privacy guarantees and provable convergence
rates for instrumental variable regression in linear models. We further
validate our theoretical findings with experiments on both synthetic and real
datasets, demonstrating that our method offers practical accuracy-privacy
trade-offs.

</details>


### [560] [Label-Guided Imputation via Forest-Based Proximities for Improved Time Series Classification](https://arxiv.org/abs/2509.22919)
*Jake S. Rhodes,Adam G. Rustad,Sofia Pelagalli Maia,Evan Thacker,Hyunmi Choi,Jose Gutierrez,Tatjana Rundek,Ben Shaw*

Main category: stat.ML

TL;DR: 提出时间序列分类中缺失数据插补框架，利用监督模型提取树基邻近度插补，能提高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 多数时间序列插补方法忽略标签信息，本文旨在解决时间序列分类中缺失数据插补问题。

Method: 定义基于标签的缺失值插补方法，从现有监督模型中提取树基邻近度进行插补。

Result: 该插补方法虽插补值与真实值不同，但能提供更丰富信息，提高分类准确率。

Conclusion: 所提基于标签的插补方法在时间序列分类中有效，可提升分类性能。

Abstract: Missing data is a common problem in time series data. Most methods for
imputation ignore label information pertaining to the time series even if that
information exists. In this paper, we provide a framework for missing data
imputation in the context of time series classification, where each time series
is associated with a categorical label. We define a means of imputing missing
values conditional upon labels, the method being guided by powerful, existing
supervised models designed for high accuracy in this task. From each model, we
extract a tree-based proximity measure from which imputation can be applied. We
show that imputation using this method generally provides richer information
leading to higher classification accuracies, despite the imputed values
differing from the true values.

</details>


### [561] [Localized Uncertainty Quantification in Random Forests via Proximities](https://arxiv.org/abs/2509.22928)
*Jake S. Rhodes,Scott D. Brown,J. Riley Wilkinson*

Main category: stat.ML

TL;DR: 本文提出一种新方法用于随机森林的局部不确定性量化，可调整预测区间以达到期望覆盖率，分类时能提升模型准确性。


<details>
  <summary>Details</summary>
Motivation: 机器学习中不确定性量化很重要，但传统方法重预测精度，当前局部不确定性量化方法有局限，因此需新方法。

Method: 利用随机森林自然产生的测试集和相似度度量（邻近度），围绕邻近点形成OOB误差的局部分布，创建回归预测区间和分类信任分数。

Result: 可调整预测区间达到期望覆盖率，分类时排除不可分类点能提升模型准确性和ROC-AUC分数。

Conclusion: 提出的新方法在随机森林局部不确定性量化方面有效，分类时表现优于竞争方法。

Abstract: In machine learning, uncertainty quantification helps assess the reliability
of model predictions, which is important in high-stakes scenarios. Traditional
approaches often emphasize predictive accuracy, but there is a growing focus on
incorporating uncertainty measures. This paper addresses localized uncertainty
quantification in random forests. While current methods often rely on quantile
regression or Monte Carlo techniques, we propose a new approach using naturally
occurring test sets and similarity measures (proximities) typically viewed as
byproducts of random forests. Specifically, we form localized distributions of
OOB errors around nearby points, defined using the proximities, to create
prediction intervals for regression and trust scores for classification. By
varying the number of nearby points, our intervals can be adjusted to achieve
the desired coverage while retaining the flexibility that reflects the
certainty of individual predictions. For classification, excluding points
identified as unclassifiable by our method generally enhances the accuracy of
the model and provides higher accuracy-rejection AUC scores than competing
methods.

</details>


### [562] [Unsupervised Conformal Inference: Bootstrapping and Alignment to Control LLM Uncertainty](https://arxiv.org/abs/2509.23002)
*Lingyou Pang,Lei Huang,Jianyu Lin,Tianyu Wang,Akira Horiguchi,Alexander Aue,Carey E. Priebe*

Main category: stat.ML

TL;DR: 提出无监督共形推理框架用于生成任务，在不同数据集上表现良好，可实现无标签、API兼容的测试时过滤。


<details>
  <summary>Details</summary>
Motivation: 在缺乏token级概率和真实标签的情况下，部署黑盒大语言模型需要管理不确定性。

Method: 引入无监督共形推理框架，包括基于响应嵌入Gram矩阵的非典型分数、结合自举变体的UCP和共形对齐。

Result: 在不同基准数据集上，该方法实现接近标称的覆盖率，提供更紧密、更稳定的阈值，减少幻觉严重程度，优于轻量级单响应检测器。

Conclusion: 得到一个无标签、API兼容的测试时过滤门，能将几何信号转化为校准的、目标对齐的决策。

Abstract: Deploying black-box LLMs requires managing uncertainty in the absence of
token-level probability or true labels. We propose introducing an unsupervised
conformal inference framework for generation, which integrates: generative
models, incorporating: (i) an LLM-compatible atypical score derived from
response-embedding Gram matrix, (ii) UCP combined with a bootstrapping variant
(BB-UCP) that aggregates residuals to refine quantile precision while
maintaining distribution-free, finite-sample coverage, and (iii) conformal
alignment, which calibrates a single strictness parameter $\tau$ so a user
predicate (e.g., factuality lift) holds on unseen batches with probability $\ge
1-\alpha$. Across different benchmark datasets, our gates achieve
close-to-nominal coverage and provide tighter, more stable thresholds than
split UCP, while consistently reducing the severity of hallucination,
outperforming lightweight per-response detectors with similar computational
demands. The result is a label-free, API-compatible gate for test-time
filtering that turns geometric signals into calibrated, goal-aligned decisions.

</details>


### [563] [Sparse Deep Additive Model with Interactions: Enhancing Interpretability and Predictability](https://arxiv.org/abs/2509.23068)
*Yi-Ting Hung,Li-Hsiang Lin,Vince D. Calhoun*

Main category: stat.ML

TL;DR: 提出稀疏深度可加交互模型（SDAMI），结合特征选择与深度子网络，能提高可解释性与预测精度，经模拟和应用验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 深度学习需要能从小或中等样本学习、处理高维特征且可解释的个性化模型。

Method: 提出SDAMI框架，采用两阶段策略，先识别强主效应，再通过结构化正则化区分主效应和交互效应，为每个主效应构建子网络。

Result: 大量模拟对比证实SDAMI能在不同场景恢复效应结构，在可靠性分析、神经科学和医学诊断等应用中展示其处理现实高维建模挑战的通用性。

Conclusion: SDAMI是处理高维建模问题的有效方法，能兼顾可解释性和预测精度。

Abstract: Recent advances in deep learning highlight the need for personalized models
that can learn from small or moderate samples, handle high dimensional
features, and remain interpretable. To address this challenge, we propose the
Sparse Deep Additive Model with Interactions (SDAMI), a framework that combines
sparsity driven feature selection with deep subnetworks for flexible function
approximation. Unlike conventional deep learning models, which often function
as black boxes, SDAMI explicitly disentangles main effects and interaction
effects to enhance interpretability. At the same time, its deep additive
structure achieves higher predictive accuracy than classical additive models.
Central to SDAMI is the concept of an Effect Footprint, which assumes that
higher order interactions project marginally onto main effects. Guided by this
principle, SDAMI adopts a two stage strategy: first, identify strong main
effects that implicitly carry information about important interactions. second,
exploit this information through structured regularization such as group lasso
to distinguish genuine main effects from interaction effects. For each selected
main effect, SDAMI constructs a dedicated subnetwork, enabling nonlinear
function approximation while preserving interpretability and providing a
structured foundation for modeling interactions. Extensive simulations with
comparisons confirm SDAMI$'$s ability to recover effect structures across
diverse scenarios, while applications in reliability analysis, neuroscience,
and medical diagnostics further demonstrate its versatility in addressing
real-world high-dimensional modeling challenges.

</details>


### [564] [Statistical Inference for Gradient Boosting Regression](https://arxiv.org/abs/2509.23127)
*Haimo Fang,Kevin Tan,Giles Hooker*

Main category: stat.ML

TL;DR: 提出梯度提升回归统计推断统一框架，增强后提升性能，算法有CLT可用于统计推断，数值实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 梯度提升的统计推断和不确定性量化具有挑战性且研究不足。

Method: 将dropout或并行训练与正则化程序集成，以实现提升的中心极限定理。

Result: 增加dropout率和并行树数量可提升信号恢复和整体性能，算法有类似CLT，可构建置信区间等。

Conclusion: 算法表现良好，能在正则化提升和随机森林间插值，内置统计推断程序有效。

Abstract: Gradient boosting is widely popular due to its flexibility and predictive
accuracy. However, statistical inference and uncertainty quantification for
gradient boosting remain challenging and under-explored. We propose a unified
framework for statistical inference in gradient boosting regression. Our
framework integrates dropout or parallel training with a recently proposed
regularization procedure that allows for a central limit theorem (CLT) for
boosting. With these enhancements, we surprisingly find that increasing the
dropout rate and the number of trees grown in parallel at each iteration
substantially enhances signal recovery and overall performance. Our resulting
algorithms enjoy similar CLTs, which we use to construct built-in confidence
intervals, prediction intervals, and rigorous hypothesis tests for assessing
variable importance. Numerical experiments demonstrate that our algorithms
perform well, interpolate between regularized boosting and random forests, and
confirm the validity of their built-in statistical inference procedures.

</details>


### [565] [A Generative Model for Controllable Feature Heterophily in Graphs](https://arxiv.org/abs/2509.23230)
*Haoyu Wang,Renyuan Ma,Gonzalo Mateos,Luana Ruiz*

Main category: stat.ML

TL;DR: 提出图信号生成框架控制特征异质性，有理论保证并实验验证


<details>
  <summary>Details</summary>
Motivation: 实现对图学习方法有效性的关键属性——特征异质性的明确控制

Method: 结合基于Lipschitz图子的随机图生成器和通过拉普拉斯算子平滑谱函数过滤的高斯节点特征

Result: 建立了经验异质性得分的集中结果和特征异质性测度的几乎必然收敛性理论保证；实验证明能精确控制同质性

Conclusion: 该框架中，图子和滤波器的相互作用决定特征异质性的极限水平，为数据建模和生成提供了可调机制

Abstract: We introduce a principled generative framework for graph signals that enables
explicit control of feature heterophily, a key property underlying the
effectiveness of graph learning methods. Our model combines a Lipschitz
graphon-based random graph generator with Gaussian node features filtered
through a smooth spectral function of the rescaled Laplacian. We establish new
theoretical guarantees: (i) a concentration result for the empirical
heterophily score; and (ii) almost-sure convergence of the feature heterophily
measure to a deterministic functional of the graphon degree profile, based on a
graphon-limit law for polynomial averages of Laplacian eigenvalues. These
results elucidate how the interplay between the graphon and the filter governs
the limiting level of feature heterophily, providing a tunable mechanism for
data modeling and generation. We validate the theory through experiments
demonstrating precise control of homophily across graph families and spectral
filters.

</details>


### [566] [Flow Matching for Robust Simulation-Based Inference under Model Misspecification](https://arxiv.org/abs/2509.23385)
*Pierre-Louis Ruhlmann,Pedro L. C. Rodrigues,Michael Arbel,Florence Forbes*

Main category: stat.ML

TL;DR: 提出Flow Matching Corrected Posterior Estimation (FMCPE)框架解决模拟推理中模型误设问题，在多数据集验证其效果。


<details>
  <summary>Details</summary>
Motivation: 解决模拟推理（SBI）中模型误设导致后验有偏差或过度自信的问题。

Method: 引入FMCPE框架，分两步，先在模拟数据上训练后验近似器，再用流匹配将预测结果逼近真实后验。

Result: 在合成基准和真实数据集上，FMCPE能减轻模型误设影响，提高推理准确性和不确定性校准，且计算高效。

Conclusion: FMCPE可将SBI的可扩展性与对分布偏移的鲁棒性相结合，有效解决模型误设问题。

Abstract: Simulation-based inference (SBI) is transforming experimental sciences by
enabling parameter estimation in complex non-linear models from simulated data.
A persistent challenge, however, is model misspecification: simulators are only
approximations of reality, and mismatches between simulated and real data can
yield biased or overconfident posteriors. We address this issue by introducing
Flow Matching Corrected Posterior Estimation (FMCPE), a framework that
leverages the flow matching paradigm to refine simulation-trained posterior
estimators using a small set of real calibration samples. Our approach proceeds
in two stages: first, a posterior approximator is trained on abundant simulated
data; second, flow matching transports its predictions toward the true
posterior supported by real observations, without requiring explicit knowledge
of the misspecification. This design enables FMCPE to combine the scalability
of SBI with robustness to distributional shift. Across synthetic benchmarks and
real-world datasets, we show that our proposal consistently mitigates the
effects of misspecification, delivering improved inference accuracy and
uncertainty calibration compared to standard SBI baselines, while remaining
computationally efficient.

</details>


### [567] [End-to-End Deep Learning for Predicting Metric Space-Valued Outputs](https://arxiv.org/abs/2509.23544)
*Yidong Zhou,Su I Iao,Hans-Georg Müller*

Main category: stat.ML

TL;DR: 介绍用于预测度量空间值输出的深度学习框架E2M，有理论保证且表现优异，具实用性。


<details>
  <summary>Details</summary>
Motivation: 现代应用需预测非欧几里得结构化输出，经典回归技术不适用，需新方法。

Method: 引入E2M框架，通过基于输入的神经网络学习权重，对训练输出进行加权Fréchet均值预测。

Result: 通过模拟实验，E2M达到了最先进的性能，样本量越大优势越明显，在实际应用中也展现了灵活性和实用性。

Conclusion: E2M是一个有效的用于预测度量空间值输出的深度学习框架。

Abstract: Many modern applications involve predicting structured, non-Euclidean outputs
such as probability distributions, networks, and symmetric positive-definite
matrices. These outputs are naturally modeled as elements of general metric
spaces, where classical regression techniques that rely on vector space
structure no longer apply. We introduce E2M (End-to-End Metric regression), a
deep learning framework for predicting metric space-valued outputs. E2M
performs prediction via a weighted Fr\'echet means over training outputs, where
the weights are learned by a neural network conditioned on the input. This
construction provides a principled mechanism for geometry-aware prediction that
avoids surrogate embeddings and restrictive parametric assumptions, while fully
preserving the intrinsic geometry of the output space. We establish theoretical
guarantees, including a universal approximation theorem that characterizes the
expressive capacity of the model and a convergence analysis of the
entropy-regularized training objective. Through extensive simulations involving
probability distributions, networks, and symmetric positive-definite matrices,
we show that E2M consistently achieves state-of-the-art performance, with its
advantages becoming more pronounced at larger sample sizes. Applications to
human mortality distributions and New York City taxi networks further
demonstrate the flexibility and practical utility of the framework.

</details>


### [568] [Define latent spaces by example: optimisation over the outputs of generative models](https://arxiv.org/abs/2509.23800)
*Samuel Willis,Alexandru I. Stere,Dragos D. Margineantu,Henry T. Oldroyd,John A. Fozard,Carl Henrik Ek,Henry Moss,Erik Bodin*

Main category: stat.ML

TL;DR: 引入代理潜在空间解决生成式AI模型下游任务控制问题，方法通用且成本低。


<details>
  <summary>Details</summary>
Motivation: 现代生成式AI模型无约束采样无法满足下游任务控制需求，需高效识别符合模型概率和特定任务约束的输出。

Method: 引入代理潜在空间，这是一种无需额外训练、可从任何生成式模型提取的非参数低维欧几里得嵌入，通过示例定义轴。

Result: 该表示为欧几里得空间且维度可控，可直接应用标准优化算法遍历生成模型输出。

Conclusion: 方法与架构无关，几乎无额外计算成本，能跨模态泛化。

Abstract: Modern generative AI models such as diffusion and flow matching can sample
from rich data distributions, but many downstream tasks -- such as experimental
design or creative content generation -- require a higher level of control than
unconstrained sampling. The challenge is to efficiently identify outputs that
are both probable under the model and satisfy task-specific constraints. We
address this by introducing surrogate latent spaces: non-parametric,
low-dimensional Euclidean embeddings that can be extracted from any generative
model without additional training. The axes in the Euclidean space can be
defined via examples, providing a simple and interpretable approach to define
custom latent spaces that both express intended features and are convenient to
use in downstream tasks. The representation is Euclidean and has controllable
dimensionality, permitting direct application of standard optimisation
algorithms to traverse the outputs of generative models. Our approach is
architecture-agnostic, incurs almost no additional computational cost, and
generalises across modalities, including images, audio, videos, and structured
objects like proteins.

</details>


### [569] [Singleton-Optimized Conformal Prediction](https://arxiv.org/abs/2509.24095)
*Tao Wang,Yan Sun,Edgar Dobriban*

Main category: stat.ML

TL;DR: 提出Singleton - Optimized Conformal Prediction (SOCOP)方法，在图像分类和LLM选择题实验中，相比标准非一致性分数提高单例频率且对平均集大小影响小。


<details>
  <summary>Details</summary>
Motivation: 现有效率导向方法主要优化平均集大小，而最有用结果是单例预测，故提出新的非一致性分数以最小化产生非单例集的概率。

Method: 从非凸约束优化问题出发，进行几何重构并给出计算非一致性分数和相关分裂共形预测集的算法，提出SOCOP方法。

Result: 在图像分类和LLM选择题实验中，SOCOP相比标准非一致性分数提高单例频率（有时超20%），对平均集大小影响小。

Conclusion: SOCOP方法能有效提高单例频率，同时对平均集大小影响不大。

Abstract: Conformal prediction can be used to construct prediction sets that cover the
true outcome with a desired probability, but can sometimes lead to large
prediction sets that are costly in practice. The most useful outcome is a
singleton prediction-an unambiguous decision-yet existing efficiency-oriented
methods primarily optimize average set size. Motivated by this, we propose a
new nonconformity score that aims to minimize the probability of producing
non-singleton sets. Starting from a non-convex constrained optimization problem
as a motivation, we provide a geometric reformulation and associated algorithm
for computing the nonconformity score and associated split conformal prediction
sets in O(K) time for K-class problems. Using this score in split conformal
prediction leads to our proposed Singleton-Optimized Conformal Prediction
(SOCOP) method. We evaluate our method in experiments on image classification
and LLM multiple-choice question-answering, comparing with standard
nonconformity scores such as the (negative) label probability estimates and
their cumulative distribution function; both of which are motivated by
optimizing length. The results show that SOCOP increases singleton frequency
(sometimes by over 20%) compared to the above scores, with minimal impact on
average set size.

</details>


### [570] [ActiveCQ: Active Estimation of Causal Quantities](https://arxiv.org/abs/2509.24293)
*Erdun Gao,Dino Sejdinovic*

Main category: stat.ML

TL;DR: 文章提出主动估计因果量框架，用高斯过程建模回归函数，实验表明该框架样本效率显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 估计因果量需要大量数据，获取成本高，此前工作聚焦较窄，需样本高效的主动学习策略。

Method: 形式化主动估计因果量任务，用高斯过程建模回归函数，探索显式密度估计和条件均值嵌入两种方法，基于后验不确定性推导采集策略。

Result: 一系列模拟和半合成实验显示，该框架显著优于相关基线，在多种因果量上样本效率大幅提升。

Conclusion: 提出的统一框架有效，能提高样本效率，适用于估计多种因果量。

Abstract: Estimating causal quantities (CQs) typically requires large datasets, which
can be expensive to obtain, especially when measuring individual outcomes is
costly. This challenge highlights the importance of sample-efficient active
learning strategies. To address the narrow focus of prior work on the
conditional average treatment effect, we formalize the broader task of Actively
estimating Causal Quantities (ActiveCQ) and propose a unified framework for
this general problem. Built upon the insight that many CQs are integrals of
regression functions, our framework models the regression function with a
Gaussian Process. For the distribution component, we explore both a baseline
using explicit density estimators and a more integrated method using
conditional mean embeddings in a reproducing kernel Hilbert space. This latter
approach offers key advantages: it bypasses explicit density estimation,
operates within the same function space as the GP, and adaptively refines the
distributional model after each update. Our framework enables the principled
derivation of acquisition strategies from the CQ's posterior uncertainty; we
instantiate this principle with two utility functions based on information gain
and total variance reduction. A range of simulated and semi-synthetic
experiments demonstrate that our principled framework significantly outperforms
relevant baselines, achieving substantial gains in sample efficiency across a
variety of CQs.

</details>


### [571] [PEARL: Performance-Enhanced Aggregated Representation Learning](https://arxiv.org/abs/2509.24312)
*Wenhui Li,Shijin Gong,Xinyu Zhang*

Main category: stat.ML

TL;DR: 本文提出性能增强的聚合表征学习方法，结合多种表征学习方法提升下游任务性能，理论证明渐近最优，实验表明优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 不同表征学习方法提取数据不同方面，单一方法可能忽略下游任务重要信息，需结合多种方法提升性能。

Method: 提出聚合表征学习方法，结合多种表征学习方法，使用替代损失函数估计权重，框架通用灵活。

Result: 理论上证明方法在下游任务渐近达到最优性能，实验中相较先进机器学习模型，该方法始终优于基线方法。

Conclusion: 该方法有效且在现实机器学习场景有广泛适用性。

Abstract: Representation learning is a key technique in modern machine learning that
enables models to identify meaningful patterns in complex data. However,
different methods tend to extract distinct aspects of the data, and relying on
a single approach may overlook important insights relevant to downstream tasks.
This paper proposes a performance-enhanced aggregated representation learning
method, which combines multiple representation learning approaches to improve
the performance of downstream tasks. The framework is designed to be general
and flexible, accommodating a wide range of loss functions commonly used in
machine learning models. To ensure computational efficiency, we use surrogate
loss functions to facilitate practical weight estimation. Theoretically, we
prove that our method asymptotically achieves optimal performance in downstream
tasks, meaning that the risk of our predictor is asymptotically equivalent to
the theoretical minimum. Additionally, we derive that our method asymptotically
assigns nonzero weights to correctly specified models. We evaluate our method
on diverse tasks by comparing it with advanced machine learning models. The
experimental results demonstrate that our method consistently outperforms
baseline methods, showing its effectiveness and broad applicability in
real-world machine learning scenarios.

</details>


### [572] [Preference-Based Dynamic Ranking Structure Recognition](https://arxiv.org/abs/2509.24493)
*Nan Lu,Jian Shi,Xin-Yu Tian*

Main category: stat.ML

TL;DR: 本文提出基于偏好数据的排名结构识别新框架，介绍方法并证明理论性质，实验验证其实用性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 偏好数据复杂且有噪声，但可能隐藏潜在同质结构，需有效方法识别排名结构。

Method: 将时间惩罚纳入Bradley - Terry模型的谱估计以识别动态排名组；引入创新目标函数并基于动态规划提出实用算法检测结构变化；利用可逆马尔可夫链诱导的随机“设计矩阵”性质证明排名组识别的一致性；采用组逆技术量化项目能力估计的不确定性。

Result: 理论上证明了排名组识别和结构变化识别的一致性；在合成和真实数据集上的实验验证了方法的实用性和可解释性。

Conclusion: 所提出的框架具有鲁棒性，在实际应用中具有实用价值和可解释性。

Abstract: Preference-based data often appear complex and noisy but may conceal
underlying homogeneous structures. This paper introduces a novel framework of
ranking structure recognition for preference-based data. We first develop an
approach to identify dynamic ranking groups by incorporating temporal penalties
into a spectral estimation for the celebrated Bradley-Terry model. To detect
structural changes, we introduce an innovative objective function and present a
practicable algorithm based on dynamic programming. Theoretically, we establish
the consistency of ranking group recognition by exploiting properties of a
random `design matrix' induced by a reversible Markov chain. We also tailor a
group inverse technique to quantify the uncertainty in item ability estimates.
Additionally, we prove the consistency of structure change recognition,
ensuring the robustness of the proposed framework. Experiments on both
synthetic and real-world datasets demonstrate the practical utility and
interpretability of our approach.

</details>


### [573] [Quantitative convergence of trained single layer neural networks to Gaussian processes](https://arxiv.org/abs/2509.24544)
*Eloy Mosig,Andrea Agazzi,Dario Trevisan*

Main category: stat.ML

TL;DR: 研究浅层神经网络经梯度下降训练后在无限宽度极限下向相关高斯过程的定量收敛，给出网络输出与高斯近似二次Wasserstein距离的显式上界。


<details>
  <summary>Details</summary>
Motivation: 先前工作仅建立了广泛设置下的定性收敛，精确的有限宽度估计尤其是训练期间的估计有限。

Method: 给出网络输出和其高斯近似在任意训练时间的二次Wasserstein距离的显式上界。

Result: 证明了与网络宽度的多项式衰减关系，量化了架构参数和训练动态对收敛和近似误差的影响。

Conclusion: 实现了对浅层神经网络向高斯过程收敛的定量研究。

Abstract: In this paper, we study the quantitative convergence of shallow neural
networks trained via gradient descent to their associated Gaussian processes in
the infinite-width limit.
  While previous work has established qualitative convergence under broad
settings, precise, finite-width estimates remain limited, particularly during
training.
  We provide explicit upper bounds on the quadratic Wasserstein distance
between the network output and its Gaussian approximation at any training time
$t \ge 0$, demonstrating polynomial decay with network width.
  Our results quantify how architectural parameters, such as width and input
dimension, influence convergence, and how training dynamics affect the
approximation error.

</details>


### [574] [MAD: Manifold Attracted Diffusion](https://arxiv.org/abs/2509.24710)
*Dennis Elbrächter,Giovanni S. Alberti,Matteo Santacesaria*

Main category: stat.ML

TL;DR: 提出一种可高效实现的推理程序修改方法，用于从含噪训练数据生成无噪图像样本，并在多类数据上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 考虑训练数据来自目标分布的含噪版本的情况，基于流形假设，希望生成无噪样本。

Method: 引入扩展分数的概念，可将小变化减小到零，同时让大变化基本不变，利用标准分数的近似来高效计算其近似。

Result: 该方法在玩具问题、合成数据和真实数据上展示了有效性。

Conclusion: 提出的修改推理程序的方法能从含噪训练数据有效生成无噪样本。

Abstract: Score-based diffusion models are a highly effective method for generating
samples from a distribution of images. We consider scenarios where the training
data comes from a noisy version of the target distribution, and present an
efficiently implementable modification of the inference procedure to generate
noiseless samples. Our approach is motivated by the manifold hypothesis,
according to which meaningful data is concentrated around some low-dimensional
manifold of a high-dimensional ambient space. The central idea is that noise
manifests as low magnitude variation in off-manifold directions in contrast to
the relevant variation of the desired distribution which is mostly confined to
on-manifold directions. We introduce the notion of an extended score and show
that, in a simplified setting, it can be used to reduce small variations to
zero, while leaving large variations mostly unchanged. We describe how its
approximation can be computed efficiently from an approximation to the standard
score and demonstrate its efficacy on toy problems, synthetic data, and real
data.

</details>


### [575] [When Scores Learn Geometry: Rate Separations under the Manifold Hypothesis](https://arxiv.org/abs/2509.24912)
*Xiang Li,Zebang Shen,Ya-Ping Hsieh,Niao He*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Score-based methods, such as diffusion models and Bayesian inverse problems,
are often interpreted as learning the data distribution in the low-noise limit
($\sigma \to 0$). In this work, we propose an alternative perspective: their
success arises from implicitly learning the data manifold rather than the full
distribution. Our claim is based on a novel analysis of scores in the
small-$\sigma$ regime that reveals a sharp separation of scales: information
about the data manifold is $\Theta(\sigma^{-2})$ stronger than information
about the distribution. We argue that this insight suggests a paradigm shift
from the less practical goal of distributional learning to the more attainable
task of geometric learning, which provably tolerates $O(\sigma^{-2})$ larger
errors in score approximation. We illustrate this perspective through three
consequences: i) in diffusion models, concentration on data support can be
achieved with a score error of $o(\sigma^{-2})$, whereas recovering the
specific data distribution requires a much stricter $o(1)$ error; ii) more
surprisingly, learning the uniform distribution on the manifold-an especially
structured and useful object-is also $O(\sigma^{-2})$ easier; and iii) in
Bayesian inverse problems, the maximum entropy prior is $O(\sigma^{-2})$ more
robust to score errors than generic priors. Finally, we validate our
theoretical findings with preliminary experiments on large-scale models,
including Stable Diffusion.

</details>


### [576] [Inductive Bias and Spectral Properties of Single-Head Attention in High Dimensions](https://arxiv.org/abs/2509.24914)
*Fabrizio Boncoraglio,Vittorio Erba,Emanuele Troiani,Florent Krzakala,Lenka Zdeborová*

Main category: stat.ML

TL;DR: 研究单头绑定注意力层在合成高维序列任务上的经验风险最小化，给出训练和测试误差渐近分析等结果。


<details>
  <summary>Details</summary>
Motivation: 研究单头绑定注意力层在合成高维序列任务上的经验风险最小化问题。

Method: 运用随机矩阵理论、自旋玻璃物理和近似消息传递等工具。

Result: 得出训练和测试误差的渐近分析，确定插值和恢复阈值，刻画学习权重的极限谱分布，揭示因子化形式引入的归纳偏置，理论结果与大规模Transformer经验趋势相符。

Conclusion: 理论结果与大规模Transformer现象一致，可提供理论视角。

Abstract: We study empirical risk minimization in a single-head tied-attention layer
trained on synthetic high-dimensional sequence tasks, given by the recently
introduced attention-indexed model. Using tools from random matrix theory,
spin-glass physics, and approximate message passing, we derive sharp
asymptotics for training and test errors, locate interpolation and recovery
thresholds, and characterize the limiting spectral distribution of the learned
weights. Weight decay induces an implicit nuclear-norm regularization, favoring
low-rank query and key matrices. Leveraging this, we compare the standard
factorized training of query and key matrices with a direct parameterization in
which their product is trained element-wise, revealing the inductive bias
introduced by the factorized form. Remarkably, the predicted spectral
distribution echoes empirical trends reported in large-scale transformers,
offering a theoretical perspective consistent with these phenomena.

</details>


### [577] [A Spectral-Grassmann Wasserstein metric for operator representations of dynamical systems](https://arxiv.org/abs/2509.24920)
*Thibaut Germain,Rémi Flamary,Vladimir R. Kostic,Karim Lounici*

Main category: stat.ML

TL;DR: 提出一种新方法，将系统表示为联合算子特征值和谱投影的分布，用最优传输定义系统间度量，在机器学习应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决从轨迹数据估计动力系统几何这一机器学习应用中的重大挑战。

Method: 将每个系统表示为其联合算子特征值和谱投影的分布，利用最优传输定义系统间的度量。

Result: 该度量对轨迹采样频率不变，计算高效，有有限样本收敛保证，能计算Fréchet均值，在机器学习应用中始终优于基于标准算子的距离。

Conclusion: 该方法在机器学习应用中表现好，能在动力系统间进行有意义的插值。

Abstract: The geometry of dynamical systems estimated from trajectory data is a major
challenge for machine learning applications. Koopman and transfer operators
provide a linear representation of nonlinear dynamics through their spectral
decomposition, offering a natural framework for comparison. We propose a novel
approach representing each system as a distribution of its joint operator
eigenvalues and spectral projectors and defining a metric between systems
leveraging optimal transport. The proposed metric is invariant to the sampling
frequency of trajectories. It is also computationally efficient, supported by
finite-sample convergence guarantees, and enables the computation of Fr\'echet
means, providing interpolation between dynamical systems. Experiments on
simulated and real-world datasets show that our approach consistently
outperforms standard operator-based distances in machine learning applications,
including dimensionality reduction and classification, and provides meaningful
interpolation between dynamical systems.

</details>


### [578] [Symmetry-Aware Bayesian Optimization via Max Kernels](https://arxiv.org/abs/2509.25051)
*Anthony Bardou,Antoine Gonon,Aryan Ahadinia,Patrick Thiran*

Main category: stat.ML

TL;DR: 本文考虑最大核的PSD投影用于贝叶斯优化，在合成和真实世界基准上取得低遗憾且不增加计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 目标函数存在群作用下的不变性，利用对称性可提高贝叶斯优化效率，但最大核非半正定限制其在贝叶斯优化中的应用。

Method: 考虑最大核的PSD投影。

Result: 与现有不变和非不变核相比，在合成和真实世界贝叶斯优化基准上实现显著更低的遗憾，且不增加计算复杂度。

Conclusion: 最大核的PSD投影是提升贝叶斯优化效率的有效方法。

Abstract: Bayesian Optimization (BO) is a powerful framework for optimizing noisy,
expensive-to-evaluate black-box functions. When the objective exhibits
invariances under a group action, exploiting these symmetries can substantially
improve BO efficiency. While using maximum similarity across group orbits has
long been considered in other domains, the fact that the max kernel is not
positive semidefinite (PSD) has prevented its use in BO. In this work, we
revisit this idea by considering a PSD projection of the max kernel. Compared
to existing invariant (and non-invariant) kernels, we show it achieves
significantly lower regret on both synthetic and real-world BO benchmarks,
without increasing computational complexity.

</details>


### [579] [On Spectral Learning for Odeco Tensors: Perturbation, Initialization, and Algorithms](https://arxiv.org/abs/2509.25126)
*Arnab Auddy,Ming Yuan*

Main category: stat.ML

TL;DR: 研究正交可分解张量的谱学习，关注统计极限、优化几何和初始化的相互作用。


<details>
  <summary>Details</summary>
Motivation: 探索正交可分解张量谱学习中统计极限、优化几何和初始化的关系，解决迭代方法初始化的计算瓶颈问题。

Method: 研究扰动界、非凸优化分析和初始化策略。

Result: 发现正交可分解张量恢复不依赖特征间隙，在噪声下有更好的鲁棒性。

Conclusion: 明确了高效算法何时能达到统计极限以及何时存在基本障碍。

Abstract: We study spectral learning for orthogonally decomposable (odeco) tensors,
emphasizing the interplay between statistical limits, optimization geometry,
and initialization. Unlike matrices, recovery for odeco tensors does not hinge
on eigengaps, yielding improved robustness under noise. While iterative methods
such as tensor power iterations can be statistically efficient, initialization
emerges as the main computational bottleneck. We investigate perturbation
bounds, non-convex optimization analysis, and initialization strategies,
clarifying when efficient algorithms attain statistical limits and when
fundamental barriers remain.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [580] [Adaptive Pseudo-Marginal Algorithm](https://arxiv.org/abs/2509.24820)
*Sarra Abaoubida,Mylène Bédard,Florian Maire*

Main category: stat.CO

TL;DR: 提出伪边际算法的自适应版本，自动调整关键参数N，无需人工干预，在两个例子中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有确定伪边际算法关键参数N的方法需多步和人工调整，为克服此局限。

Method: 引入自适应版本的伪边际算法，在迭代过程中自动调整N至最优值。

Result: 算法在一定条件下能保证收敛，在两个例子（含学龄前儿童肺部感染真实数据问题）中表现优于现有方法。

Conclusion: 自适应伪边际算法有效，可避免人工干预，性能良好。

Abstract: The Pseudo-Marginal (PM) algorithm is a popular Markov chain Monte Carlo
(MCMC) method used to sample from a target distribution when its density is
inaccessible, but can be estimated with a non-negative unbiased estimator. Its
performance depends on a key parameter, N, the number of iterations (or
particles) used to approximate the target density. Larger values of N yield
more accurate estimates but at increased running time. Previous studies has
provided guidelines for selecting an optimal value of N to balance this
tradeoff. However, this approach involves multiple steps and manual
adjustments. To overcome these limitations, we introduce an adaptive version of
the PM algorithm, where N is automatically adjusted during the iterative
process toward its optimal value, thus eliminating the need for manual
intervention. This algorithm ensures convergence under certain conditions. On
two examples, including a real data problem on pulmonary infection in preschool
children, the proposed algorithm compares favorably to the existing approach.

</details>


### [581] [Covariance-Adaptive Bouncy Particle Samplers via Split Lagrangian Dynamics](https://arxiv.org/abs/2509.24847)
*Augustin Chevallier,Erik Raab*

Main category: stat.CO

TL;DR: 文章扩展了BPS，引入与目标局部协方差结构相关的位置依赖速度分布，构建了适应局部协方差结构的PDMP，并通过实验给出使用建议。


<details>
  <summary>Details</summary>
Motivation: 现有PDMP采样器无法适应目标分布协方差结构的局部变化。

Method: 引入位置依赖速度分布，借鉴Riemannian Manifold Hamiltonian Monte Carlo和Lagrangian Dynamical Monte Carlo的思想，构建因度量变化触发额外速度更新事件的PDMP。

Result: 得到适应局部协方差结构的算法。

Conclusion: 通过实验给出了协方差自适应BPS相较于标准PDMP算法的适用场景建议。

Abstract: Piecewise Deterministic Markov Processes (PDMPs) provide a powerful framework
for continuous-time Monte Carlo, with the Bouncy Particle Sampler (BPS) as a
prominent example. Recent advances through the Metropolised PDMP framework
allow local adaptivity in step size and effective path length, the latter
acting as a refreshment rate. However, current PDMP samplers cannot adapt to
local changes in the covariance structure of the target distribution.
  We extend BPS by introducing a position-dependent velocity distribution that
varies with the local covariance structure of the target. Building on ideas
from Riemannian Manifold Hamiltonian Monte Carlo and its velocity-based
variant, Lagrangian Dynamical Monte Carlo, we construct a PDMP for which
changes in the metric trigger additional velocity update events. Using a metric
derived from the target Hessian, the resulting algorithm adapts to the local
covariance structure.
  Through a series of controlled experiments, we provide practical guidance on
when the proposed covariance-adaptive BPS should be preferred over standard
PDMP algorithms.

</details>


### [582] [Deep P-Spline: Theory, Fast Tuning, and Application](https://arxiv.org/abs/2501.01376)
*Noah Yi-Ting Hung,Li-Hsiang Lin,Vince D. Calhoun*

Main category: stat.CO

TL;DR: 本文提出Deep P - Spline (DPS)方法解决深度神经网络最优结构选择难题，可克服维度诅咒，数值结果验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络解决回归问题时，选择最优网络结构是重大挑战。

Method: 将DNN中的神经元选择与基扩展技术中的节点放置相联系，引入差异惩罚自动选择节点，提出DPS方法，使用ECM算法进行网络结构调整。

Result: 从非参数回归角度，DPS能克服维度诅咒，有效处理大量输入变量的数据集；数值结果验证了模型有效性。

Conclusion: DPS方法有潜力用于高级非线性回归任务。

Abstract: Deep neural networks (DNNs) have been widely applied to solve real-world
regression problems. However, selecting optimal network structures remains a
significant challenge. This study addresses this issue by linking neuron
selection in DNNs to knot placement in basis expansion techniques. We introduce
a difference penalty that automates knot selection, thereby simplifying the
complexities of neuron selection. We name this method Deep P-Spline (DPS). This
approach extends the class of models considered in conventional DNN modeling
and forms the basis for a latent variable modeling framework using the
Expectation-Conditional Maximization (ECM) algorithm for efficient network
structure tuning with theoretical guarantees. From a nonparametric regression
perspective, DPS is proven to overcome the curse of dimensionality, enabling
the effective handling of datasets with a large number of input variable, a
scenario where conventional nonparametric regression methods typically
underperform. This capability motivates the application of the proposed
methodology to computer experiments and image data analyses, where the
associated regression problems involving numerous inputs are common. Numerical
results validate the effectiveness of the model, underscoring its potential for
advanced nonlinear regression tasks.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [583] [Prosody-Adaptable Audio Codecs for Zero-Shot Voice Conversion via In-Context Learning](https://arxiv.org/abs/2505.15402)
*Junchuan Zhao,Xintong Wang,Ye Wang*

Main category: cs.SD

TL;DR: 提出基于VALLE - X框架的VC模型，引入PACE模块提升韵律控制，实验显示优于基线系统。


<details>
  <summary>Details</summary>
Motivation: 离散音频编解码器和编解码器语言模型发展，启发利用其能力进行说话人自适应的语音转换研究。

Method: 在VALLE - X框架下构建VC模型，引入PACE模块分离和优化韵律。

Result: 实验表明在韵律保留、音色一致性和整体自然度上优于基线VC系统。

Conclusion: 提出的方法能在保持说话人音色的同时灵活控制韵律，性能表现良好。

Abstract: Recent advances in discrete audio codecs have significantly improved speech
representation modeling, while codec language models have enabled in-context
learning for zero-shot speech synthesis. Inspired by this, we propose a voice
conversion (VC) model within the VALLE-X framework, leveraging its strong
in-context learning capabilities for speaker adaptation. To enhance prosody
control, we introduce a prosody-aware audio codec encoder (PACE) module, which
isolates and refines prosody from other sources, improving expressiveness and
control. By integrating PACE into our VC model, we achieve greater flexibility
in prosody manipulation while preserving speaker timbre. Experimental
evaluation results demonstrate that our approach outperforms baseline VC
systems in prosody preservation, timbre consistency, and overall naturalness,
surpassing baseline VC systems.

</details>


### [584] [GOAT: A Large Dataset of Paired Guitar Audio Recordings and Tablatures](https://arxiv.org/abs/2509.22655)
*Jackson Loth,Pedro Sarmento,Saurjya Sarkar,Zixun Guo,Mathieu Barthet,Mark Sandler*

Main category: cs.SD

TL;DR: 本文提出GOAT数据集以解决吉他音乐信息检索中数据集稀缺和标注不足问题，还给出数据增强策略，展示了转录任务结果，有望推动吉他相关MIR任务模型训练。


<details>
  <summary>Details</summary>
Motivation: 吉他音乐信息检索受数据集稀缺和标注有限的限制，阻碍了深度学习方法的进展。

Method: 提出GOAT数据集，包含5.9小时电吉他高质量音频；采用吉他放大器进行数据增强；用吉他指法表进行标注，利用Guitar Pro格式和文本式令牌编码。

Result: 在MIDI转录中取得有竞争力的结果，在自动吉他指法表转录新方法上有初步成果。

Conclusion: GOAT数据集为多种吉他相关的音乐信息检索任务训练新模型提供了可能性。

Abstract: In recent years, the guitar has received increased attention from the music
information retrieval (MIR) community driven by the challenges posed by its
diverse playing techniques and sonic characteristics. Mainly fueled by deep
learning approaches, progress has been limited by the scarcity and limited
annotations of datasets. To address this, we present the Guitar On Audio and
Tablatures (GOAT) dataset, comprising 5.9 hours of unique high-quality direct
input audio recordings of electric guitars from a variety of different guitars
and players. We also present an effective data augmentation strategy using
guitar amplifiers which delivers near-unlimited tonal variety, of which we
provide a starting 29.5 hours of audio. Each recording is annotated using
guitar tablatures, a guitar-specific symbolic format supporting string and fret
numbers, as well as numerous playing techniques. For this we utilise both the
Guitar Pro format, a software for tablature playback and editing, and a
text-like token encoding. Furthermore, we present competitive results using
GOAT for MIDI transcription and preliminary results for a novel approach to
automatic guitar tablature transcription. We hope that GOAT opens up the
possibilities to train novel models on a wide variety of guitar-related MIR
tasks, from synthesis to transcription to playing technique detection.

</details>


### [585] [Prompt-aware classifier free guidance for diffusion models](https://arxiv.org/abs/2509.22728)
*Xuanhao Zhang,Chang Li*

Main category: cs.SD

TL;DR: 提出提示感知框架解决扩散模型中引导尺度选择问题，在图像和音频生成实验中取得提升。


<details>
  <summary>Details</summary>
Motivation: 扩散模型中分类器自由引导的引导尺度选择未被充分研究，固定尺度在不同复杂度提示下表现不佳。

Method: 构建合成数据集，用评估指标打分；使用轻量级预测器，基于语义嵌入和语言复杂度估计多指标质量曲线，通过带正则化的效用函数确定最佳尺度。

Result: 在MSCOCO 2014和AudioCaps实验中，比普通CFG有一致提升，增强保真度、对齐度和感知偏好。

Conclusion: 提示感知的尺度选择为预训练扩散模型提供了无训练的有效增强。

Abstract: Diffusion models have achieved remarkable progress in image and audio
generation, largely due to Classifier-Free Guidance. However, the choice of
guidance scale remains underexplored: a fixed scale often fails to generalize
across prompts of varying complexity, leading to oversaturation or weak
alignment. We address this gap by introducing a prompt-aware framework that
predicts scale-dependent quality and selects the optimal guidance at inference.
Specifically, we construct a large synthetic dataset by generating samples
under multiple scales and scoring them with reliable evaluation metrics. A
lightweight predictor, conditioned on semantic embeddings and linguistic
complexity, estimates multi-metric quality curves and determines the best scale
via a utility function with regularization. Experiments on MSCOCO~2014 and
AudioCaps show consistent improvements over vanilla CFG, enhancing fidelity,
alignment, and perceptual preference. This work demonstrates that prompt-aware
scale selection provides an effective, training-free enhancement for pretrained
diffusion backbones.

</details>


### [586] [ABC-Eval: Benchmarking Large Language Models on Symbolic Music Understanding and Instruction Following](https://arxiv.org/abs/2509.23350)
*Jiahao Zhao,Yunjia Li,Wei Li,Kazuyoshi Yoshii*

Main category: cs.SD

TL;DR: 提出ABC - Eval基准测试评估大语言模型处理基于文本的ABC符号乐谱的能力，发现现有模型存在显著局限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在符号音乐理解和推理能力方面研究不足，需填补该空白。

Method: 提出ABC - Eval基准测试，包含1086个测试样本和10个子任务，对7个最先进的大语言模型进行评估。

Result: 现有模型在符号音乐处理能力上存在显著局限，单个基线在不同子任务上表现一致。

Conclusion: ABC - Eval基准测试具有可靠性，能有效评估大语言模型处理符号音乐任务的能力。

Abstract: As large language models continue to develop, the feasibility and
significance of text-based symbolic music tasks have become increasingly
prominent. While symbolic music has been widely used in generation tasks, LLM
capabilities in understanding and reasoning about symbolic music remain largely
underexplored. To address this gap, we propose ABC-Eval, the first open-source
benchmark dedicated to the understanding and instruction-following capabilities
in text-based ABC notation scores. It comprises 1,086 test samples spanning 10
sub-tasks, covering scenarios from basic musical syntax comprehension to
complex sequence-level reasoning. Such a diverse scope poses substantial
challenges to models' ability to handle symbolic music tasks. We evaluated
seven state-of-the-art LLMs on ABC-Eval, and the results reveal notable
limitations in existing models' symbolic music processing capabilities.
Furthermore, the consistent performance of individual baselines across
different sub-tasks supports the reliability of our benchmark.

</details>


### [587] [AudioRole: An Audio Dataset for Character Role-Playing in Large Language Models](https://arxiv.org/abs/2509.23435)
*Wenyu Li,Xiaoqi Jiao,Yi Chang,Guangyan Zhang,Yiwen Guo*

Main category: cs.SD

TL;DR: 提出AudioRole数据集及ARP - Eval评估框架，验证数据集有效性，其对提升大模型音频角色扮演能力有重要作用。


<details>
  <summary>Details</summary>
Motivation: 现有工作多关注文本角色模拟，音频角色扮演因语义内容和声音特征同步对齐有独特挑战，需高质量多模态数据集。

Method: 从13部电视剧中精心策划AudioRole数据集，引入ARP - Eval评估框架。

Result: ARP - Model声学个性化得分0.31，内容个性化得分0.36，优于原模型和MiniCPM - O - 2.6。

Conclusion: AudioRole数据集及相关评估协议为音频角色扮演研究提供重要资源。

Abstract: The creation of high-quality multimodal datasets remains fundamental for
advancing role-playing capabilities in large language models (LLMs). While
existing works predominantly focus on text-based persona simulation, Audio
Role-Playing (ARP) presents unique challenges due to the need for synchronized
alignment of semantic content and vocal characteristics. To address this gap,
we propose AudioRole, a meticulously curated dataset from 13 TV series spanning
1K+ hours with 1M+ character-grounded dialogues, providing synchronized
audio-text pairs annotated with speaker identities and contextual metadata. In
addition, to demonstrate the effectiveness of the dataset, we introduced
ARP-Eval, a dual-aspect evaluation framework that assesses both response
quality and role fidelity. Empirical validation showing GLM-4-Voice trained on
AudioRole (which we called ARP-Model) achieve an average Acoustic
Personalization score of 0.31, significantly outperforming the original
GLM-4-voice and the more powerful model MiniCPM-O-2.6, which specifically
supports role-playing in one-shot scenarios. The ARP-Model also achieves a
Content Personalization score of 0.36, surpassing the untrained original model
by about 38% and maintaining the same level as MiniCPM-O-2.6.
  AudioRole features dialogues from over 115 main characters, 6 trained
ARP-Models that role-play different characters, and evaluation protocols.
Together, they provide an essential resource for advancing audio-grounded
role-playing research.

</details>


### [588] [Generalizable Speech Deepfake Detection via Information Bottleneck Enhanced Adversarial Alignment](https://arxiv.org/abs/2509.23618)
*Pu Huang,Shouguang Wang,Siya Yao,Mengchu Zhou*

Main category: cs.SD

TL;DR: 本文提出IB - CAAN用于语音深度伪造检测，实验显示其性能优于基线并达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 神经语音合成技术导致高度逼真的语音深度伪造，带来安全风险，且检测因多种因素的分布偏移而具有挑战性。

Method: 探索学习共享判别特征，提出信息瓶颈增强的置信度感知对抗网络（IB - CAAN），通过置信度引导的对抗对齐抑制攻击特定伪影，信息瓶颈去除干扰变异性。

Result: 在ASVspoof 2019/2021、ASVspoof 5和In - the - Wild等实验中，IB - CAAN始终优于基线。

Conclusion: IB - CAAN在许多基准测试中达到了先进性能。

Abstract: Neural speech synthesis techniques have enabled highly realistic speech
deepfakes, posing major security risks. Speech deepfake detection is
challenging due to distribution shifts across spoofing methods and variability
in speakers, channels, and recording conditions. We explore learning shared
discriminative features as a path to robust detection and propose Information
Bottleneck enhanced Confidence-Aware Adversarial Network (IB-CAAN).
Confidence-guided adversarial alignment adaptively suppresses attack-specific
artifacts without erasing discriminative cues, while the information bottleneck
removes nuisance variability to preserve transferable features. Experiments on
ASVspoof 2019/2021, ASVspoof 5, and In-the-Wild demonstrate that IB-CAAN
consistently outperforms baseline and achieves state-of-the-art performance on
many benchmarks.

</details>


### [589] [Text-Independent Speaker Identification Using Audio Looping With Margin Based Loss Functions](https://arxiv.org/abs/2509.22838)
*Elliot Q C Garcia,Nicéias Silva Vilela,Kátia Pires Nascimento do Sacramento,Tiago A. E. Ferreira*

Main category: cs.SD

TL;DR: 研究基于VGG16的卷积神经网络中CosFace Loss和ArcFace Loss用于文本无关说话人识别的效果，对比Softmax Loss，还研究了梅尔频谱图大小和时长对模型性能的影响，实验结果优于传统Softmax方法。


<details>
  <summary>Details</summary>
Motivation: 说话人识别在多种应用中至关重要，需研究更有效的损失函数用于文本无关说话人识别。

Method: 采用基于VGG16模型的卷积神经网络，使用CosFace Loss和ArcFace Loss，以Softmax Loss为对比基线，分析其对模型准确性和鲁棒性的影响，同时研究梅尔频谱图大小和时长对模型性能的影响。

Result: 实验结果显示识别准确率优于传统Softmax损失方法。

Conclusion: 文中讨论了研究结果对未来研究的意义。

Abstract: Speaker identification has become a crucial component in various
applications, including security systems, virtual assistants, and personalized
user experiences. In this paper, we investigate the effectiveness of CosFace
Loss and ArcFace Loss for text-independent speaker identification using a
Convolutional Neural Network architecture based on the VGG16 model, modified to
accommodate mel spectrogram inputs of variable sizes generated from the
Voxceleb1 dataset. Our approach involves implementing both loss functions to
analyze their effects on model accuracy and robustness, where the Softmax loss
function was employed as a comparative baseline. Additionally, we examine how
the sizes of mel spectrograms and their varying time lengths influence model
performance. The experimental results demonstrate superior identification
accuracy compared to traditional Softmax loss methods. Furthermore, we discuss
the implications of these findings for future research.

</details>


### [590] [AudioMoG: Guiding Audio Generation with Mixture-of-Guidance](https://arxiv.org/abs/2509.23727)
*Junyou Wang,Zehua Chen,Binjie Yuan,Kaiwen Zheng,Chang Li,Yuxuan Jiang,Jun Zhu*

Main category: cs.SD

TL;DR: 本文提出混合引导框架AudioMoG，在跨模态音频生成中结合不同引导原则，实验显示其优于单一引导方法，能在不牺牲推理效率下提升质量。


<details>
  <summary>Details</summary>
Motivation: 现有音频生成引导方法多依赖单一引导原则，未充分挖掘引导潜力。

Method: 提出混合引导框架AudioMoG，可利用不同引导原则的互补优势。

Result: 在相同推理速度下，AudioMoG在文本到音频等多种生成任务中始终优于单一引导方法。

Conclusion: 在采样阶段通过混合引导原则可在不牺牲推理效率下提高跨模态音频生成质量。

Abstract: Guidance methods have demonstrated significant improvements in cross-modal
audio generation, including text-to-audio (T2A) and video-to-audio (V2A)
generation. The popularly adopted method, classifier-free guidance (CFG),
steers generation by emphasizing condition alignment, enhancing fidelity but
often at the cost of diversity. Recently, autoguidance (AG) has been explored
for audio generation, encouraging the sampling to faithfully reconstruct the
target distribution and showing increased diversity. Despite these advances,
they usually rely on a single guiding principle, e.g., condition alignment in
CFG or score accuracy in AG, leaving the full potential of guidance for audio
generation untapped. In this work, we explore enriching the composition of the
guidance method and present a mixture-of-guidance framework, AudioMoG. Within
the design space, AudioMoG can exploit the complementary advantages of
distinctive guiding principles by fulfilling their cumulative benefits. With a
reduced form, AudioMoG can consider parallel complements or recover a single
guiding principle, without sacrificing generality. We experimentally show that,
given the same inference speed, AudioMoG approach consistently outperforms
single guidance in T2A generation across sampling steps, concurrently showing
advantages in V2A, text-to-music, and image generation. These results highlight
a "free lunch" in current cross-modal audio generation systems: higher quality
can be achieved through mixed guiding principles at the sampling stage without
sacrificing inference efficiency. Demo samples are available at:
https://audio-mog.github.io.

</details>


### [591] [Disentangling Score Content and Performance Style for Joint Piano Rendering and Transcription](https://arxiv.org/abs/2509.23878)
*Wei Zeng,Junchuan Zhao,Ye Wang*

Main category: cs.SD

TL;DR: 提出统一框架联合建模EPR和APT，实验表明该框架在相关任务表现好，能实现内容风格分离等。


<details>
  <summary>Details</summary>
Motivation: 以往工作独立处理EPR和APT，本文希望联合建模。

Method: 构建基于transformer的序列到序列架构，使用序列对齐数据训练，引入基于扩散的性能风格推荐模块。

Result: 客观和主观评估显示框架在EPR和APT任务有竞争力，能有效分离内容风格、可靠进行风格迁移和合适渲染。

Conclusion: 所提统一框架可行，能解决EPR和APT相关问题。

Abstract: Expressive performance rendering (EPR) and automatic piano transcription
(APT) are fundamental yet inverse tasks in music information retrieval: EPR
generates expressive performances from symbolic scores, while APT recovers
scores from performances. Despite their dual nature, prior work has addressed
them independently. In this paper we propose a unified framework that jointly
models EPR and APT by disentangling note-level score content and global
performance style representations from both paired and unpaired data. Our
framework is built on a transformer-based sequence-to-sequence architecture and
is trained using only sequence-aligned data, without requiring fine-grained
note-level alignment. To automate the rendering process while ensuring
stylistic compatibility with the score, we introduce an independent
diffusion-based performance style recommendation module that generates style
embeddings directly from score content. This modular component supports both
style transfer and flexible rendering across a range of expressive styles.
Experimental results from both objective and subjective evaluations demonstrate
that our framework achieves competitive performance on EPR and APT tasks, while
enabling effective content-style disentanglement, reliable style transfer, and
stylistically appropriate rendering. Demos are available at
https://jointpianist.github.io/epr-apt/

</details>


### [592] [VioPTT: Violin Technique-Aware Transcription from Synthetic Data Augmentation](https://arxiv.org/abs/2509.23759)
*Ting-Kang Wang,Yueh-Po Peng,Li Su,Vincent K. M. Cheung*

Main category: cs.SD

TL;DR: 提出轻量级端到端模型VioPTT转录小提琴演奏技巧，发布数据集MOSA - VPT，模型有良好泛化性和转录性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动音乐转录模型大多只能转录音高和时间信息，忽略了关键的表现力和乐器特定细微差别，如小提琴演奏技巧。

Method: 提出VioPTT模型直接转录小提琴演奏技巧、音高起止时间；发布MOSA - VPT合成数据集避免手动标注。

Result: 模型对现实世界音符级小提琴技巧录音有强泛化性，实现了最先进的转录性能。

Conclusion: VioPTT是首个在统一框架中结合小提琴转录和演奏技巧预测的模型。

Abstract: While automatic music transcription is well-established in music information
retrieval, most models are limited to transcribing pitch and timing information
from audio, and thus omit crucial expressive and instrument-specific nuances.
One example is playing technique on the violin, which affords its distinct
palette of timbres for maximal emotional impact. Here, we propose
\textbf{VioPTT} (Violin Playing Technique-aware Transcription), a lightweight,
end-to-end model that directly transcribes violin playing technique in addition
to pitch onset and offset. Furthermore, we release \textbf{MOSA-VPT}, a novel,
high-quality synthetic violin playing technique dataset to circumvent the need
for manually labeled annotations. Leveraging this dataset, our model
demonstrated strong generalization to real-world note-level violin technique
recordings in addition to achieving state-of-the-art transcription performance.
To our knowledge, VioPTT is the first to jointly combine violin transcription
and playing technique prediction within a unified framework.

</details>


### [593] [From Sound to Setting: AI-Based Equalizer Parameter Prediction for Piano Tone Replication](https://arxiv.org/abs/2509.24404)
*Song-Ze Yu*

Main category: cs.SD

TL;DR: 提出基于AI的音乐制作音调复制系统，预测EQ参数，评估模型，神经网络效果好，可用于音调匹配及拓展。


<details>
  <summary>Details</summary>
Motivation: 传统音频到音频方法输出不可解释，该项目旨在提供可让音乐家调整的可解释EQ参数值。

Method: 使用系统改变EQ设置的钢琴录音数据集，评估回归和神经网络模型。

Result: 神经网络在多频段任务中均方误差为0.0216。

Conclusion: 该系统能为音乐制作人实现实用、灵活和自动化的音调匹配，为拓展到更复杂音频效果奠定基础。

Abstract: This project presents an AI-based system for tone replication in music
production, focusing on predicting EQ parameter settings directly from audio
features. Unlike traditional audio-to-audio methods, our approach outputs
interpretable parameter values (e.g., EQ band gains) that musicians can further
adjust in their workflow. Using a dataset of piano recordings with
systematically varied EQ settings, we evaluate both regression and neural
network models. The neural network achieves a mean squared error of 0.0216 on
multi-band tasks. The system enables practical, flexible, and automated tone
matching for music producers and lays the foundation for extensions to more
complex audio effects.

</details>


### [594] [An Agent-Based Framework for Automated Higher-Voice Harmony Generation](https://arxiv.org/abs/2509.24463)
*Nia D'Souza Ganapathy,Arul Selvamani Shaja*

Main category: cs.SD

TL;DR: 本文介绍了一种基于多智能体系统的高级和声音乐生成器，可生成复杂且合适的和声。


<details>
  <summary>Details</summary>
Motivation: 解决算法作曲领域中生成音乐连贯且美观和声的挑战。

Method: 构建包含音乐摄取、和弦知识、和声生成和音频制作四个专业智能体的多智能体系统，各智能体分工协作。

Result: 系统能够有效模拟人类音乐家的协作过程，实现强大的数据处理、理论理解、创意作曲和逼真音频合成。

Conclusion: 该模块化、基于智能体的方法可生成复杂且上下文合适的高音和声。

Abstract: The generation of musically coherent and aesthetically pleasing harmony
remains a significant challenge in the field of algorithmic composition. This
paper introduces an innovative Agentic AI-enabled Higher Harmony Music
Generator, a multi-agent system designed to create harmony in a collaborative
and modular fashion. Our framework comprises four specialized agents: a
Music-Ingestion Agent for parsing and standardizing input musical scores; a
Chord-Knowledge Agent, powered by a Chord-Former (Transformer model), to
interpret and provide the constituent notes of complex chord symbols; a
Harmony-Generation Agent, which utilizes a Harmony-GPT and a Rhythm-Net (RNN)
to compose a melodically and rhythmically complementary harmony line; and an
Audio-Production Agent that employs a GAN-based Symbolic-to-Audio Synthesizer
to render the final symbolic output into high-fidelity audio. By delegating
specific tasks to specialized agents, our system effectively mimics the
collaborative process of human musicians. This modular, agent-based approach
allows for robust data processing, deep theoretical understanding, creative
composition, and realistic audio synthesis, culminating in a system capable of
generating sophisticated and contextually appropriate higher-voice harmonies
for given melodies.

</details>


### [595] [Sparse Autoencoders Make Audio Foundation Models more Explainable](https://arxiv.org/abs/2509.24793)
*Théo Mariotte,Martin Lebourdais,Antonio Almudévar,Marie Tahon,Alfonso Ortega,Nicolas Dugué*

Main category: cs.SD

TL;DR: 探索用稀疏自编码器分析预训练音频模型隐藏表征，以唱歌技巧分类为例，证明其保留信息、增强解耦。


<details>
  <summary>Details</summary>
Motivation: 现有预训练音频模型表征不明确，分析方法限于线性探测隐藏表征。

Method: 使用稀疏自编码器（SAEs）分析预训练模型的隐藏表征，以唱歌技巧分类为案例研究。

Result: SAEs保留原始表征和类标签信息，增强了声音属性的解耦。

Conclusion: SAEs是识别表征中潜在因素的有效工具，能为自监督学习系统提供见解。

Abstract: Audio pretrained models are widely employed to solve various tasks in speech
processing, sound event detection, or music information retrieval. However, the
representations learned by these models are unclear, and their analysis mainly
restricts to linear probing of the hidden representations. In this work, we
explore the use of Sparse Autoencoders (SAEs) to analyze the hidden
representations of pretrained models, focusing on a case study in singing
technique classification. We first demonstrate that SAEs retain both
information about the original representations and class labels, enabling their
internal structure to provide insights into self-supervised learning systems.
Furthermore, we show that SAEs enhance the disentanglement of vocal attributes,
establishing them as an effective tool for identifying the underlying factors
encoded in the representations.

</details>


### [596] [Unmute the Patch Tokens: Rethinking Probing in Multi-Label Audio Classification](https://arxiv.org/abs/2509.24901)
*Lukas Rauch,René Heinrich,Houtan Ghaffari,Lukas Miklautz,Ilyass Moummad,Bernhard Sick,Christoph Scholz*

Main category: cs.SD

TL;DR: 本文指出音频自监督学习默认微调的原因，研究全局池化瓶颈，提出二值化原型探针方法，证明探测是评估音频SSL模型的有效范式。


<details>
  <summary>Details</summary>
Motivation: 解决音频自监督学习中线性探测因全局池化信息瓶颈而无法准确评估嵌入质量的问题，挑战依赖微调的现状。

Method: 在13个数据集和6个基于频谱图的编码器上研究全局池化瓶颈，引入二值化原型探针方法进行类信息聚合。

Result: 二值化原型探针方法显著优于线性和注意力探测。

Conclusion: 探测可作为评估音频SSL模型的有竞争力且高效的范式，挑战了依赖高成本微调的情况。

Abstract: Although probing frozen models has become a standard evaluation paradigm,
self-supervised learning in audio defaults to fine-tuning. A key reason is that
global pooling creates an information bottleneck causing linear probes to
misrepresent the embedding quality: The $\texttt{cls}$-token discards crucial
token information about dispersed, localized events in multi-label audio. This
weakness is rooted in the mismatch between the pretraining objective (operating
globally) and the downstream task (localized events). Across a comprehensive
benchmark of 13 datasets and 6 spectrogram-based encoders, we first investigate
the global pooling bottleneck. We then introduce binarized prototypical probes:
a lightweight and simple pooling method that learns prototypes to perform
class-wise information aggregation. Despite its simplicity, our method notably
outperforms linear and attentive probing. Our work establishes probing as a
competitive and efficient paradigm for evaluating audio SSL models, challenging
the reliance on costly fine-tuning.

</details>


### [597] [MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech](https://arxiv.org/abs/2509.25131)
*Chengyao Wang,Zhisheng Zhong,Bohao Peng,Senqiao Yang,Yuqi Liu,Haokun Gui,Bin Xia,Jingyao Li,Bei Yu,Jiaya Jia*

Main category: cs.SD

TL;DR: 介绍MGM - Omni，一种统一的全模态大语言模型，可实现全模态理解和长时语音生成，性能优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有级联管道在语音合成方面存在不足，需一种能高效进行跨模态交互和实时语音生成的模型。

Method: 采用“脑 - 口”设计和双轨基于令牌的架构，统一训练策略和双音频编码器设计用于理解，基于块的并行解码方案用于生成。

Result: MGM - Omni在保留音色、生成自然语音和长音频及全模态理解方面优于现有开源模型，且训练数据效率高。

Conclusion: MGM - Omni为全模态理解和可控、个性化长时语音生成建立了高效的端到端范式。

Abstract: We present MGM-Omni, a unified Omni LLM for omni-modal understanding and
expressive, long-horizon speech generation. Unlike cascaded pipelines that
isolate speech synthesis, MGM-Omni adopts a "brain-mouth" design with a
dual-track, token-based architecture that cleanly decouples multimodal
reasoning from real-time speech generation. This design enables efficient
cross-modal interaction and low-latency, streaming speech generation. For
understanding, a unified training strategy coupled with a dual audio encoder
design enables long-form audio perception across diverse acoustic conditions.
For generation, a chunk-based parallel decoding scheme narrows the text speech
token-rate gap, accelerating inference and supporting streaming zero-shot voice
cloning with stable timbre over extended durations. Compared to concurrent
work, MGM-Omni achieves these capabilities with markedly data-efficient
training. Extensive experiments demonstrate that MGM-Omni outperforms existing
open source models in preserving timbre identity across extended sequences,
producing natural and context-aware speech, and achieving superior long-form
audio and omnimodal understanding. MGM-Omni establishes an efficient,
end-to-end paradigm for omnimodal understanding and controllable, personalised
long-horizon speech generation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [598] [Quantum Dynamics with Time-Dependent Neural Quantum States](https://arxiv.org/abs/2509.24865)
*Alejandro Romero-Ros,Javier Rozalén Sarmiento,Arnau Rios*

Main category: quant-ph

TL;DR: 本文进行了基于时间依赖的神经量子态（NQS）原理验证模拟，以量子谐振子为例测试NQS，结果与解析解高度吻合。


<details>
  <summary>Details</summary>
Motivation: 展示NQS方法有效捕捉连续体中量子动力学关键方面的能力。

Method: 利用神经网络架构对波函数进行参数化的NQS方法，以量子谐振子为测试对象。

Result: 获得了基态，进行了相干态和呼吸模式动力学模拟，结果与解析解高度一致。

Conclusion: NQS方法能有效捕捉连续体中量子动力学的关键方面。

Abstract: We present proof-of-principle time-dependent neural quantum state (NQS)
simulations to illustrate the ability of this approach to effectively capture
key aspects of quantum dynamics in the continuum. NQS leverage the
parameterization of the wave function with neural-network architectures. Here,
we put NQS to the test by solving the quantum harmonic oscillator. We obtain
the ground state and perform coherent state and breathing mode dynamics. Our
results are benchmarked against analytical solutions, showcasing an excellent
agreement.

</details>


### [599] [Comparison of Hyperplane Rounding for Max-Cut and Quantum Approximate Optimization Algorithm over Certain Regular Graph Families](https://arxiv.org/abs/2509.24108)
*Reuben Tate,Swati Gupta*

Main category: quant-ph

TL;DR: 从展示量子优势角度出发，寻找NP难问题的挑战性小实例，确定两类图，对比经典与量子算法表现并探索实例构造。


<details>
  <summary>Details</summary>
Motivation: 从展示量子优势的角度，寻找NP难问题的挑战性实例，且考虑到近期NISQ设备限制，希望实例规模小。

Method: 确定两类图（|V|<1000），对比Goemans - Williamson算法和量子近似优化算法在不同图上的近似比，通过扰动边权计算构造挑战性实例。

Result: Goemans - Williamson算法在两类图上近似比至多0.912；量子近似优化算法在Karloff实例极限下近似比0.592，在强正则图族上至多0.894。

Conclusion: 完成对两类图的研究和近似比对比，探索了挑战性实例构造并将其放入CI - QuBe github仓库。

Abstract: There is a strong interest in finding challenging instances of NP-hard
problems, from the perspective of showing quantum advantage. Due to the limits
of near-term NISQ devices, it is moreover useful if these instances are small.
In this work, we identify two graph families ($|V|<1000$) on which the
Goemans-Williamson algorithm for approximating the Max-Cut achieves at most a
0.912-approximation. We further show that, in comparison, a recent quantum
algorithm, Quantum Approximate Optimization Algorithm (depth $p=1$), is a
0.592-approximation on Karloff instances in the limit ($n \to \infty$), and is
at best a $0.894$-approximation on a family of strongly-regular graphs. We
further explore construction of challenging instances computationally by
perturbing edge weights, which may be of independent interest, and include
these in the CI-QuBe github repository.

</details>


### [600] [Accelerating Regression Tasks with Quantum Algorithms](https://arxiv.org/abs/2509.24757)
*Chenghua Liu,Zhengfeng Ji*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Regression is a cornerstone of statistics and machine learning, with
applications spanning science, engineering, and economics. While quantum
algorithms for regression have attracted considerable attention, most existing
work has focused on linear regression, leaving many more complex yet
practically important variants unexplored. In this work, we present a unified
quantum framework for accelerating a broad class of regression tasks --
including linear and multiple regression, Lasso, Ridge, Huber, $\ell_p$-, and
$\delta_p$-type regressions -- achieving up to a quadratic improvement in the
number of samples $m$ over the best classical algorithms. This speedup is
achieved by extending the recent classical breakthrough of Jambulapati et al.
(STOC'24) using several quantum techniques, including quantum leverage score
approximation (Apers &Gribling, 2024) and the preparation of many copies of a
quantum state (Hamoudi, 2022). For problems of dimension $n$, sparsity $r < n$,
and error parameter $\epsilon$, our algorithm solves the problem in
$\widetilde{O}(r\sqrt{mn}/\epsilon + \mathrm{poly}(n,1/\epsilon))$ quantum
time, demonstrating both the applicability and the efficiency of quantum
computing in accelerating regression tasks.

</details>


### [601] [Bandits roaming Hilbert space](https://arxiv.org/abs/2509.24569)
*Josep Lumbreras*

Main category: quant-ph

TL;DR: 本文研究使用多臂老虎机在量子态在线学习中探索与利用的权衡，推导了信息论下界和最优策略，应用于量子态层析、量子推荐系统和热力学功提取等场景。


<details>
  <summary>Details</summary>
Motivation: 解决量子态在线学习中探索与利用的权衡问题，减少遗憾值。

Method: 使用多臂老虎机，根据过往信息优化动作；推导信息论下界和匹配上界的最优策略；采用基于加权在线最小二乘估计器的样本最优算法。

Result: 遗憾值通常随轮数平方根缩放；在纯态和连续动作下实现多对数遗憾；在热力学功提取中比基于层析的协议有指数优势。

Conclusion: 所提方法在量子态在线学习及相关应用中有良好效果和优势。

Abstract: This thesis studies the exploration and exploitation trade-off in online
learning of properties of quantum states using multi-armed bandits. Given
streaming access to an unknown quantum state, in each round we select an
observable from a set of actions to maximize its expectation value. Using past
information, we refine actions to minimize regret; the cumulative gap between
current reward and the maximum possible. We derive information-theoretic lower
bounds and optimal strategies with matching upper bounds, showing regret
typically scales as the square root of rounds. As an application, we reframe
quantum state tomography to both learn the state efficiently and minimize
measurement disturbance. For pure states and continuous actions, we achieve
polylogarithmic regret using a sample-optimal algorithm based on a weighted
online least squares estimator. The algorithm relies on the optimistic
principle and controls the eigenvalues of the design matrix. We also apply our
framework to quantum recommender systems and thermodynamic work extraction from
unknown states. In this last setting, our results demonstrate an exponential
advantage in work dissipation over tomography-based protocols.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [602] [Dissecting Multi-Level Pricing Schemes in the Context of eCW Client Engagement](https://arxiv.org/abs/2509.22669)
*Paramahansa Pramanik,Joel Graff,Mike Decaro*

Main category: stat.AP

TL;DR: 本文为eClinicalWorks客户使用的智能医疗对象ProblemIT门户提出基于使用量的定价框架，确定单价、划分客户层级并对异常情况调整。


<details>
  <summary>Details</summary>
Motivation: 为eClinicalWorks客户使用的智能医疗对象ProblemIT门户制定合适的基于使用量的定价框架。

Method: 先通过半参数贝叶斯三次平滑样条分析确定每月稳定的单位请求价格，将客户按请求量分为八个层级，计算总费用，对异常账户进行调整。

Result: 发现单注册用户806个账户和双注册用户470个账户的请求量过高。

Conclusion: 提出的定价模型需对异常情况进行调整以适应实际情况。

Abstract: This paper presents a usage-based pricing framework for the Intelligent
Medical Objects ProblemIT Portal utilized by eClinicalWorks (eCW) clients. The
approach begins by determining a stable monthly unit price per request,
estimated as the median from semi-parametric Bayesian cubic smoothing spline
analyses covering the period November 2015 to December 2016. Clients are
subsequently segmented into eight volume-based tiers, with total charges
computed by multiplying the derived median unit price by each client's total
request count. Examination of the dataset reveals that 806 accounts with a
single registered user and 470 accounts with two registered users both exhibit
disproportionately high request volumes. The proposed model incorporates
adjustments to account for these anomalies.

</details>


### [603] [Profit over Proxies: A Scalable Bayesian Decision Framework for Optimizing Multi-Variant Online Experiments](https://arxiv.org/abs/2509.22677)
*Srijesh Pillai,Rajesh Kumar Chandrawat*

Main category: stat.AP

TL;DR: 论文提出贝叶斯决策框架解决线上A/B测试问题，能避免收益陷阱、提前终止无意义实验，助力企业向利润驱动实验文化转变。


<details>
  <summary>Details</summary>
Motivation: 线上A/B测试存在统计启发式方法错误和过度依赖代理指标的问题，影响决策质量和业务盈利能力，需解决这些挑战。

Method: 提出分层贝叶斯模型，同时估计转化率和转化货币价值，采用基于期望损失的决策理论停止规则。

Result: 框架成功避免收益陷阱，提前终止无意义实验，在监测过程中保持严格统计完整性。

Conclusion: 为组织提供实用方法，推动其从简单A/B测试转向成熟的利润驱动实验文化，使统计结论转化为战略业务价值。

Abstract: Online controlled experiments (A/B tests) are fundamental to data-driven
decision-making in the digital economy. However, their real-world application
is frequently compromised by two critical shortcomings: the use of
statistically flawed heuristics like "p-value peeking", which inflates false
positive rates, and an over-reliance on proxy metrics like conversion rates,
which can lead to decisions that inadvertently harm core business
profitability. This paper addresses these challenges by introducing a
comprehensive and scalable Bayesian decision framework designed for profit
optimization in multi-variant (A/B/n) experiments.
  We propose a hierarchical Bayesian model that simultaneously estimates the
probability of conversion (using a Beta-Bernoulli model) and the monetary value
of that conversion (using a robust Bayesian model for the mean transaction
value). Building on this, we employ a decision-theoretic stopping rule based on
Expected Loss, enabling experiments to be concluded not only when a superior
variant is identified but also when it becomes clear that no variant offers a
practically significant improvement (stopping for futility). The framework
successfully navigates "revenue traps" where a variant with a higher conversion
rate would have resulted in a net financial loss, correctly terminates futile
experiments early to conserve resources, and maintains strict statistical
integrity throughout the monitoring process.
  Ultimately, this work provides a practical and principled methodology for
organizations to move beyond simple A/B testing towards a mature, profit-driven
experimentation culture, ensuring that statistical conclusions translate
directly to strategic business value.

</details>


### [604] [PISA: An AI Pipeline for Interpretable-by-design Survival Analysis Providing Multiple Complexity-Accuracy Trade-off Models](https://arxiv.org/abs/2509.22673)
*Thalea Schlender,Catharina J. A. Romme,Yvette M. van der Linden,Luc R. C. W. van Lonkhuijzen,Peter A. N. Bosman,Tanja Alderliesten*

Main category: stat.AP

TL;DR: 提出可解释生存分析管道PISA，能生成多个权衡复杂度与性能的模型，应用于临床数据集表现优异且可自动化工作流。


<details>
  <summary>Details</summary>
Motivation: 传统生存模型难捕捉非线性交互，现代深度学习方法可解释性差，而医疗领域需要可解释的时间事件预测模型。

Method: 提出PISA管道，用多特征、多目标特征工程将数据转化为生存分析模型，模型转为有Kaplan - Meier曲线支持的患者分层流程图。

Result: 应用于两个临床基准数据集，产生可解释生存模型和直观分层流程图，达到了最先进的性能，还能自动化临床研究工作流。

Conclusion: PISA管道在生存分析中有效，兼顾可解释性和性能，可用于实际临床研究。

Abstract: Survival analysis is central to clinical research, informing patient
prognoses, guiding treatment decisions, and optimising resource allocation.
Accurate time-to-event predictions not only improve quality of life but also
reveal risk factors that shape clinical practice. For these models to be
relevant in healthcare, interpretability is critical: predictions must be
traceable to patient-specific characteristics, and risk factors should be
identifiable to generate actionable insights for both clinicians and
researchers. Traditional survival models often fail to capture non-linear
interactions, while modern deep learning approaches, though powerful, are
limited by poor interpretability.
  We propose a Pipeline for Interpretable Survival Analysis (PISA) - a pipeline
that provides multiple survival analysis models that trade off complexity and
performance. Using multiple-feature, multi-objective feature engineering, PISA
transforms patient characteristics and time-to-event data into multiple
survival analysis models, providing valuable insights into the survival
prediction task. Crucially, every model is converted into simple patient
stratification flowcharts supported by Kaplan-Meier curves, whilst not
compromising on performance. While PISA is model-agnostic, we illustrate its
flexibility through applications of Cox regression and shallow survival trees,
the latter avoiding proportional hazards assumptions.
  Applied to two clinical benchmark datasets, PISA produced interpretable
survival models and intuitive stratification flowcharts whilst achieving
state-of-the-art performances. Revisiting a prior departmental study further
demonstrated its capacity to automate survival analysis workflows in real-world
clinical research.

</details>


### [605] [A Comprehensive Analysis of Churn Prediction in Telecommunications Using Machine Learning](https://arxiv.org/abs/2509.22654)
*Xuhang Chen,Bo Lv,Mengqian Wang,Xunwen Xiang,Shiting Wu,Shenghong Luo,Wenjun Zhang*

Main category: stat.AP

TL;DR: 提出利用深度神经网络进行电信客户流失预测的综合框架，经评估优于现有基线方法，提升准确率并提供可解释见解。


<details>
  <summary>Details</summary>
Motivation: 电信客户流失预测从主观评估发展到算法方法，需更优方案提升预测准确性。

Method: 通过系统问题表述、严格数据集分析和仔细特征工程，利用深度神经网络构建模型。

Result: 所提神经网络架构在多性能指标评估中显著优于现有基线方法。

Conclusion: 该方法提升了流失预测准确性的现有水平，还能提供影响客户流失关键因素的可解释见解。

Abstract: Customer churn prediction in the telecommunications sector represents a
critical business intelligence task that has evolved from subjective human
assessment to sophisticated algorithmic approaches. In this work, we present a
comprehensive framework for telecommunications churn prediction leveraging deep
neural networks. Through systematic problem formulation, rigorous dataset
analysis, and careful feature engineering, we develop a model that captures
complex patterns in customer behavior indicative of potential churn. We conduct
extensive empirical evaluations across multiple performance metrics,
demonstrating that our proposed neural architecture achieves significant
improvements over existing baseline methods. Our approach not only advances the
state-of-the-art in churn prediction accuracy but also provides interpretable
insights into the key factors driving customer attrition in telecommunications
services.

</details>


### [606] [Forecasting West Nile virus with deep graph encoders](https://arxiv.org/abs/2509.22657)
*Ethan Greiffenstein,Trevor Harris,Rebecca Smith*

Main category: stat.AP

TL;DR: 本文引入新的GNN变体用于西尼罗河病毒（WNV）预测，编译新数据集，实验表明该方法在多种场景和预测期内显著优于GNN和经典基线。


<details>
  <summary>Details</summary>
Motivation: 美国WNV是重要且日益严重的公共卫生问题，无人类疫苗，需准确预测以指导蚊子控制项目，现有GNN虽有改进但仍有提升空间。

Method: 引入线性连接图注意力层的新GNN变体，编译包含天气、土地使用信息和蚊子诱捕结果的大规模新数据集。

Result: 该方法在多种场景和所有预测期内的样本外和图外WNV预测技能上显著优于GNN和经典基线。

Conclusion: 新的GNN变体在WNV预测方面表现出色，为WNV预测提供了更有效的方法。

Abstract: West Nile virus is a significant, and growing, public health issue in the
United States. With no human vaccine, mosquito control programs rely on
accurate forecasting to determine when and where WNV will emerge. Recently,
spatial Graph neural networks (GNNs) were shown to be a powerful tool for WNV
forecasting, significantly improving over traditional methods. Building on this
work, we introduce a new GNN variant that linearly connects graph attention
layers, allowing us to train much larger models than previously used for WNV
forecasting. This architecture specializes general densely connected GNNs so
that the model focuses more heavily on local information to prevent over
smoothing. To support training large GNNs we compiled a massive new dataset of
weather data, land use information, and mosquito trap results across Illinois.
Experiments show that our approach significantly outperforms both GNN and
classical baselines in both out-of-sample and out-of-graph WNV prediction skill
across a variety of scenarios and over all prediction horizons.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [607] [GPM: The Gaussian Pancake Mechanism for Planting Undetectable Backdoors in Differential Privacy](https://arxiv.org/abs/2509.23834)
*Haochen Sun,Xi He*

Main category: cs.CR

TL;DR: 文章提出高斯煎饼机制（GPM），可对差分隐私进行隐蔽后门攻击，强调使用透明开源库及严格审查验证DP实现的重要性。


<details>
  <summary>Details</summary>
Motivation: 针对差分隐私软件存在的缺陷可能导致隐私泄露问题，探讨能否将被动缺陷转化为隐蔽的主动隐私攻击。

Method: 提出高斯煎饼机制（GPM），该机制与高斯机制（GM）计算上不可区分，但统计差分隐私保证更弱，可进行隐蔽后门攻击；证明GPM的隐蔽性，刻画其统计泄漏，并展示区分攻击。

Result: 理论和实证表明，在合适参数选择下区分攻击可实现近乎完美的成功率。

Conclusion: 强调使用透明、开源的差分隐私库以及对差分隐私实现进行严格审查和形式验证的重要性，以防止现实系统中难以察觉的隐私泄露。

Abstract: Differential privacy (DP) has become the gold standard for preserving
individual privacy in data analysis. However, an implicit yet fundamental
assumption underlying these rigorous privacy guarantees is the correct
implementation and execution of DP mechanisms. Several incidents of unintended
privacy loss have occurred due to numerical issues and inappropriate
configurations of DP software, which have been successfully exploited in
privacy attacks. To better understand the seriousness of defective DP software,
we ask the following question: is it possible to elevate these passive defects
into active privacy attacks while maintaining covertness?
  To address this question, we present the Gaussian pancake mechanism (GPM), a
novel mechanism that is computationally indistinguishable from the widely used
Gaussian mechanism (GM), yet exhibits arbitrarily weaker statistical DP
guarantees. This unprecedented separation enables a new class of backdoor
attacks: by indistinguishably passing off as the authentic GM, GPM can covertly
degrade statistical privacy. Unlike the unintentional privacy loss caused by
GM's numerical issues, GPM is an adversarial yet undetectable backdoor attack
against data privacy. We formally prove GPM's covertness, characterize its
statistical leakage, and demonstrate a concrete distinguishing attack that can
achieve near-perfect success rates under suitable parameter choices, both
theoretically and empirically.
  Our results underscore the importance of using transparent, open-source DP
libraries and highlight the need for rigorous scrutiny and formal verification
of DP implementations to prevent subtle, undetectable privacy compromises in
real-world systems.

</details>


### [608] [StarveSpam: Mitigating Spam with Local Reputation in Permissionless Blockchains](https://arxiv.org/abs/2509.23427)
*Rowdy Chotkan,Bulat Nasrulin,Jérémie Decouchant,Johan Pouwelse*

Main category: cs.CR

TL;DR: 提出去中心化的基于声誉的协议StarveSpam缓解区块链网络垃圾信息问题，表现优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有区块链网络垃圾信息防御方法依赖经济威慑，无法区分恶意和合法用户，常排除低价值但诚实的活动。

Method: 在交易中继层运行，结合本地行为跟踪、对等评分和自适应速率限制，无需全局共识、协议更改或可信基础设施。

Result: 使用以太坊真实数据评估，能让每个节点阻止超95%的垃圾信息，仅丢弃3%的诚实流量，相比现有基于规则的方法将网络暴露于垃圾信息的比例降低85%。

Conclusion: StarveSpam是传统垃圾信息防御的可扩展且可部署的替代方案，为更具弹性和公平性的区块链基础设施铺平道路。

Abstract: Spam poses a growing threat to blockchain networks. Adversaries can easily
create multiple accounts to flood transaction pools, inflating fees and
degrading service quality. Existing defenses against spam, such as fee markets
and staking requirements, primarily rely on economic deterrence, which fails to
distinguish between malicious and legitimate users and often exclude low-value
but honest activity. To address these shortcomings, we present StarveSpam, a
decentralized reputation-based protocol that mitigates spam by operating at the
transaction relay layer. StarveSpam combines local behavior tracking, peer
scoring, and adaptive rate-limiting to suppress abusive actors, without
requiring global consensus, protocol changes, or trusted infrastructure. We
evaluate StarveSpam using real Ethereum data from a major NFT spam event and
show that it outperforms existing fee-based and rule-based defenses, allowing
each node to block over 95% of spam while dropping just 3% of honest traffic,
and reducing the fraction of the network exposed to spam by 85% compared to
existing rule-based methods. StarveSpam offers a scalable and deployable
alternative to traditional spam defenses, paving the way toward more resilient
and equitable blockchain infrastructure.

</details>


### [609] [Multiple Concurrent Proposers: Why and How](https://arxiv.org/abs/2509.23984)
*Pranav Garimidi,Joachim Neu,Max Resnick*

Main category: cs.CR

TL;DR: 传统单提议者区块链存在MEV问题，应用层减少MEV影响多依赖拍卖，需底层共识协议具备特定属性，本文提出多并发提议者（MCP）协议。


<details>
  <summary>Details</summary>
Motivation: 解决传统单提议者区块链存在的矿工可提取价值（MEV）问题，即验证者利用交易包含和排序的垄断权从用户处获取租金。

Method: 提出一个多并发提议者（MCP）协议。

Result: 未提及具体结果。

Conclusion: 提出的MCP协议能提供运行链上拍卖所需的选择性审查抗性和隐藏这两个关键属性。

Abstract: Traditional single-proposer blockchains suffer from miner extractable value
(MEV), where validators exploit their serial monopoly on transaction inclusion
and ordering to extract rents from users. While there have been many
developments at the application layer to reduce the impact of MEV, these
approaches largely require auctions as a subcomponent. Running auctions
efficiently on chain requires two key properties of the underlying consensus
protocol: selective-censorship resistance and hiding. These properties
guarantee that an adversary can neither selectively delay transactions nor see
their contents before they are confirmed. We propose a multiple concurrent
proposer (MCP) protocol offering exactly these properties.

</details>


### [610] [BugMagnifier: TON Transaction Simulator for Revealing Smart Contract Vulnerabilities](https://arxiv.org/abs/2509.24444)
*Yury Yanovich,Victoria Kovalevskaya,Maksim Egorov,Elizaveta Smirnova,Matvey Mishuris,Yash Madhwal,Kirill Ziborov,Vladimir Gorgadze,Subodh Sharma*

Main category: cs.CR

TL;DR: 提出BugMagnifier框架检测TON智能合约异步执行漏洞，实验证明其有效，填补TON安全工具空白。


<details>
  <summary>Details</summary>
Motivation: TON区块链异步执行模型给智能合约带来安全挑战，尤其是竞态条件，动态检测时间依赖问题待解决。

Method: 构建基于TON Sandbox和集成TON虚拟机（TVM）的交易模拟框架BugMagnifier，结合消息队列操作、差分状态分析和概率排列测试。

Result: 通过对特制易受攻击合约的大量参数研究，证明BugMagnifier有效，揭示消息比率相关的检测复杂性符合理论预测。

Conclusion: BugMagnifier能进行预测性漏洞评估，将漏洞发现从手动专家分析转变为自动证据生成，为异步区块链环境下安全的智能合约开发提供支持。

Abstract: The Open Network (TON) blockchain employs an asynchronous execution model
that introduces unique security challenges for smart contracts, particularly
race conditions arising from unpredictable message processing order. While
previous work established vulnerability patterns through static analysis of
audit reports, dynamic detection of temporal dependencies through systematic
testing remains an open problem. We present BugMagnifier, a transaction
simulation framework that systematically reveals vulnerabilities in TON smart
contracts through controlled message orchestration. Built atop TON Sandbox and
integrated with the TON Virtual Machine (TVM), our tool combines precise
message queue manipulation with differential state analysis and probabilistic
permutation testing to detect asynchronous execution flaws. Experimental
evaluation demonstrates BugMagnifier's effectiveness through extensive
parametric studies on purpose-built vulnerable contracts, revealing message
ratio-dependent detection complexity that aligns with theoretical predictions.
This quantitative model enables predictive vulnerability assessment while
shifting discovery from manual expert analysis to automated evidence
generation. By providing reproducible test scenarios for temporal
vulnerabilities, BugMagnifier addresses a critical gap in the TON security
tooling, offering practical support for safer smart contract development in
asynchronous blockchain environments.

</details>


### [611] [Towards Context-aware Mobile Privacy Notice: Implementation of A Deployable Contextual Privacy Policies Generator](https://arxiv.org/abs/2509.22900)
*Haochen Gong,Zhen Tao,Shidong Pan,Zhenchang Xing,Xiaoyu Sun*

Main category: cs.CR

TL;DR: 本文提出首个可部署的安卓上下文隐私策略SDK PrivScan，能捕获应用截图识别GUI元素并显示隐私策略，架构减少设备资源需求，评估显示其具有实用性。


<details>
  <summary>Details</summary>
Motivation: 冗长且法律措辞的隐私政策阻碍用户理解应用如何收集和处理个人数据，先前相关工作因无法在现实环境部署。

Method: 提出PrivScan SDK，捕获应用截图识别GUI元素，提供轻量级浮动按钮，采用远程部署架构。

Result: 可行性评估显示平均执行时间为9.15秒。

Conclusion: PrivScan具有实用性，其源代码和演示视频可获取。

Abstract: Lengthy and legally phrased privacy policies impede users' understanding of
how mobile applications collect and process personal data. Prior work proposed
Contextual Privacy Policies (CPPs) for mobile apps to display shorter policy
snippets only in the corresponding user interface contexts, but the pipeline
could not be deployable in real-world mobile environments. In this paper, we
present PrivScan, the first deployable CPP Software Development Kit (SDK) for
Android. It captures live app screenshots to identify GUI elements associated
with types of personal data and displays CPPs in a concise, user-facing format.
We provide a lightweight floating button that offers low-friction, on-demand
control. The architecture leverages remote deployment to decouple the
multimodal backend pipeline from a mobile client comprising five modular
components, thereby reducing on-device resource demands and easing
cross-platform portability. A feasibility-oriented evaluation shows an average
execution time of 9.15\,s, demonstrating the practicality of our approach. The
source code of PrivScan is available at https://github.com/buyanghc/PrivScan
and the demo video can be found at https://www.youtube.com/watch?v=ck-25otfyHc.

</details>


### [612] [A First Look at Privacy Risks of Android Task-executable Voice Assistant Applications](https://arxiv.org/abs/2509.23680)
*Shidong Pan,Yikai Ge,Xiaoyu Sun*

Main category: cs.CR

TL;DR: 本文对安卓可执行任务语音助手应用进行隐私风险实证研究，发现隐私声明不一致和三种威胁模型，并给出建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究未从任务执行模式全面审视语音助手隐私风险，填补此研究空白。

Method: 收集十个主流语音助手分析操作特征，跨六个来源交叉核对隐私声明。

Result: 发现隐私声明广泛不一致，揭示三种隐私威胁模型。

Conclusion: 为从业者提供可行建议，强调隐私风险对新兴自主AI代理的广泛相关性。

Abstract: With the development of foundation AI technologies, task-executable voice
assistants (VAs) have become more popular, enhancing user convenience and
expanding device functionality. Android task-executable VAs are applications
that are capable of understanding complex tasks and performing corresponding
operations. Given their prevalence and great autonomy, there is no existing
work examine the privacy risks within the voice assistants from the
task-execution pattern in a holistic manner. To fill this research gap, this
paper presents a user-centric comprehensive empirical study on privacy risks in
Android task-executable VA applications. We collect ten mainstream VAs as our
research target and analyze their operational characteristics. We then
cross-check their privacy declarations across six sources, including privacy
labels, policies, and manifest files, and our findings reveal widespread
inconsistencies. Moreover, we uncover three significant privacy threat models:
(1) privacy misdisclosure in mega apps, where integrated mini apps such as
Alexa skills are inadequately represented; (2) privilege escalation via
inter-application interactions, which exploit Android's communication
mechanisms to bypass user consent; and (3) abuse of Google system applications,
enabling apps to evade the declaration of dangerous permissions. Our study
contributes actionable recommendations for practitioners and underscores
broader relevance of these privacy risks to emerging autonomous AI agents.

</details>


### [613] [When MCP Servers Attack: Taxonomy, Feasibility, and Mitigation](https://arxiv.org/abs/2509.24272)
*Weibo Zhao,Jiahao Liu,Bonan Ruan,Shaofei Li,Zhenkai Liang*

Main category: cs.CR

TL;DR: 文章对MCP服务器安全进行系统研究，发现其攻击易实施、难检测，需多方协作构建安全生态。


<details>
  <summary>Details</summary>
Motivation: MCP服务器快速发展带来安全风险，但相关安全影响研究不足。

Method: 将MCP服务器视为威胁源，分解核心组件，研究攻击类型、系统脆弱性和攻击可行性，提出分类法，开发PoC服务器并测试现有扫描器。

Result: 攻击者可低成本生成大量恶意服务器，现有检测方法不足。

Conclusion: 恶意MCP服务器易实现、难检测且危害大，需多方协作构建安全MCP生态。

Abstract: Model Context Protocol (MCP) servers enable AI applications to connect to
external systems in a plug-and-play manner, but their rapid proliferation also
introduces severe security risks. Unlike mature software ecosystems with
rigorous vetting, MCP servers still lack standardized review mechanisms, giving
adversaries opportunities to distribute malicious implementations. Despite this
pressing risk, the security implications of MCP servers remain underexplored.
To address this gap, we present the first systematic study that treats MCP
servers as active threat actors and decomposes them into core components to
examine how adversarial developers can implant malicious intent. Specifically,
we investigate three research questions: (i) what types of attacks malicious
MCP servers can launch, (ii) how vulnerable MCP hosts and Large Language Models
(LLMs) are to these attacks, and (iii) how feasible it is to carry out MCP
server attacks in practice. Our study proposes a component-based taxonomy
comprising twelve attack categories. For each category, we develop
Proof-of-Concept (PoC) servers and demonstrate their effectiveness across
diverse real-world host-LLM settings. We further show that attackers can
generate large numbers of malicious servers at virtually no cost. We then test
state-of-the-art scanners on the generated servers and found that existing
detection approaches are insufficient. These findings highlight that malicious
MCP servers are easy to implement, difficult to detect with current tools, and
capable of causing concrete damage to AI agent systems. Addressing this threat
requires coordinated efforts among protocol designers, host developers, LLM
providers, and end users to build a more secure and resilient MCP ecosystem.

</details>


### [614] [Bidirectional Intention Inference Enhances LLMs' Defense Against Multi-Turn Jailbreak Attacks](https://arxiv.org/abs/2509.22732)
*Haibo Tong,Dongcheng Zhao,Guobin Shen,Xiang He,Dachuan Lin,Feifei Zhao,Yi Zeng*

Main category: cs.CR

TL;DR: 大语言模型存在越狱攻击安全问题，提出双向意图推理防御（BIID）方法，实验表明该方法能显著降低攻击成功率，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型防御研究主要针对单轮攻击，多轮越狱攻击使传统单轮防御失效，需新防御方法。

Method: 提出BIID方法，集成基于请求的前向意图推理和基于响应的后向意图回顾，建立双向协同机制检测风险。

Result: 实验显示，该方法在单轮和多轮越狱尝试中显著降低攻击成功率，优于所有现有基线方法，在三个多轮安全数据集比较实验中也有显著优势。

Conclusion: BIID方法能有效构建更强大的防护栏，防止有害内容生成，且保持实用性。

Abstract: The remarkable capabilities of Large Language Models (LLMs) have raised
significant safety concerns, particularly regarding "jailbreak" attacks that
exploit adversarial prompts to bypass safety alignment mechanisms. Existing
defense research primarily focuses on single-turn attacks, whereas multi-turn
jailbreak attacks progressively break through safeguards through by concealing
malicious intent and tactical manipulation, ultimately rendering conventional
single-turn defenses ineffective. To address this critical challenge, we
propose the Bidirectional Intention Inference Defense (BIID). The method
integrates forward request-based intention inference with backward
response-based intention retrospection, establishing a bidirectional synergy
mechanism to detect risks concealed within seemingly benign inputs, thereby
constructing a more robust guardrails that effectively prevents harmful content
generation. The proposed method undergoes systematic evaluation compared with a
no-defense baseline and seven representative defense methods across three LLMs
and two safety benchmarks under 10 different attack methods. Experimental
results demonstrate that the proposed method significantly reduces the Attack
Success Rate (ASR) across both single-turn and multi-turn jailbreak attempts,
outperforming all existing baseline methods while effectively maintaining
practical utility. Notably, comparative experiments across three multi-turn
safety datasets further validate the proposed model's significant advantages
over other defense approaches.

</details>


### [615] [Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment](https://arxiv.org/abs/2509.22745)
*Jaehan Kim,Minkyoo Song,Seungwon Shin,Sooel Son*

Main category: cs.CR

TL;DR: 现有基于MoE架构的大语言模型在微调后有害输入路由决策漂移，易受有害微调攻击，本文提出SafeMoE方法缓解该问题，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 基于MoE架构的大语言模型在微调后有害输入路由决策漂移，存在有害微调攻击的关键漏洞，现有防御方法对MoE架构LLMs效果不佳。

Method: 提出SafeMoE方法，通过惩罚微调模型和初始安全对齐模型的路由权重差距，直接缓解路由漂移。

Result: 在7B - 141B参数的开源MoE LLMs上实验，SafeMoE有效缓解HFT攻击，降低有害性分数，保持任务效用，开销小，优于现有防御方法。

Conclusion: SafeMoE是一种有效的针对MoE架构大语言模型的安全微调方法，可用于保护LLM微调。

Abstract: Recent large language models (LLMs) have increasingly adopted the
Mixture-of-Experts (MoE) architecture for efficiency. MoE-based LLMs heavily
depend on a superficial safety mechanism in which harmful inputs are routed
safety-critical experts. However, our analysis reveals that routing decisions
for harmful inputs drift significantly after fine-tuning, exposing a critical
vulnerability to harmful fine-tuning (HFT) attacks. Existing defenses,
primarily designed for monolithic LLMs, are less effective for MoE LLMs as they
fail to prevent drift in harmful input routing. To address this limitation, we
propose SafeMoE, a safe fine-tuning method tailored to MoE LLMs. SafeMoE
directly mitigates routing drift by penalizing the gap between the routing
weights of a fine-tuned model and those of the initial safety-aligned model,
thereby preserving the safety-aligned routing of harmful inputs to
safety-critical experts. Experiments on open-source MoE LLMs ranging from 7B to
141B parameters demonstrate that SafeMoE effectively mitigates HFT attacks,
reducing the harmfulness score of OLMoE from 62.0 to 5.0, for example, while
maintaining task utility within 1% degradation and incurring only 2% overhead.
It significantly outperforms state-of-the-art defense methods for safeguarding
LLM fine-tuning and remains effective in recent large-scale MoE LLMs such as
gpt-oss and Llama 4. Our implementation is available at
https://anonymous.4open.science/r/SafeMoE.

</details>


### [616] [Red Teaming Quantum-Resistant Cryptographic Standards: A Penetration Testing Framework Integrating AI and Quantum Security](https://arxiv.org/abs/2509.22757)
*Petar Radanliev*

Main category: cs.CR

TL;DR: 研究提出评估量子密码协议漏洞的结构化方法，结合AI等技术构建框架评估和缓解量子网络安全风险，证明AI可有效模拟攻击等。


<details>
  <summary>Details</summary>
Motivation: 评估量子密码协议中的漏洞，加强量子网络安全。

Method: 集成AI驱动的红队攻击、自动化渗透测试和实时异常检测，使用自动化漏洞模拟和协议模糊测试、对抗性机器学习技术。

Result: AI可有效模拟攻击、探测密码实现中的弱点、通过迭代反馈完善安全机制，能识别潜在漏洞和新的攻击面。

Conclusion: 提供了加强量子安全的综合方法，为将AI驱动的网络安全实践融入量子领域奠定基础。

Abstract: This study presents a structured approach to evaluating vulnerabilities
within quantum cryptographic protocols, focusing on the BB84 quantum key
distribution method and National Institute of Standards and Technology (NIST)
approved quantum-resistant algorithms. By integrating AI-driven red teaming,
automated penetration testing, and real-time anomaly detection, the research
develops a framework for assessing and mitigating security risks in quantum
networks. The findings demonstrate that AI can be effectively used to simulate
adversarial attacks, probe weaknesses in cryptographic implementations, and
refine security mechanisms through iterative feedback. The use of automated
exploit simulations and protocol fuzzing provides a scalable means of
identifying latent vulnerabilities, while adversarial machine learning
techniques highlight novel attack surfaces within AI-enhanced cryptographic
processes. This study offers a comprehensive methodology for strengthening
quantum security and provides a foundation for integrating AI-driven
cybersecurity practices into the evolving quantum landscape.

</details>


### [617] [LLM Watermark Evasion via Bias Inversion](https://arxiv.org/abs/2509.23019)
*Jeongyeon Hwang,Sangdon Park,Jungseul Ok*

Main category: cs.CR

TL;DR: 提出BIRA攻击削弱大语言模型水印信号，实现超99%规避率并揭示系统漏洞


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型水印在对抗逃避场景下的鲁棒性存在争议，需严格评估其漏洞

Method: 提出理论驱动且模型无关的Bias - Inversion Rewriting Attack (BIRA)，在重写时抑制可能水印标记的词元对数概率，无需了解水印方案

Result: 在多种水印方法中，BIRA实现超99%规避率，且保留原文语义内容

Conclusion: 研究揭示了系统漏洞，强调对大语言模型水印进行压力测试和构建强大防御机制的必要性

Abstract: Watermarking for large language models (LLMs) embeds a statistical signal
during generation to enable detection of model-produced text. While
watermarking has proven effective in benign settings, its robustness under
adversarial evasion remains contested. To advance a rigorous understanding and
evaluation of such vulnerabilities, we propose the \emph{Bias-Inversion
Rewriting Attack} (BIRA), which is theoretically motivated and model-agnostic.
BIRA weakens the watermark signal by suppressing the logits of likely
watermarked tokens during LLM-based rewriting, without any knowledge of the
underlying watermarking scheme. Across recent watermarking methods, BIRA
achieves over 99\% evasion while preserving the semantic content of the
original text. Beyond demonstrating an attack, our results reveal a systematic
vulnerability, emphasizing the need for stress testing and robust defenses.

</details>


### [618] [Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data](https://arxiv.org/abs/2509.23041)
*Zi Liang,Qingqing Ye,Xuan Liu,Yanyun Wang,Jianliang Xu,Haibo Hu*

Main category: cs.CR

TL;DR: 本文评估合成数据集成训练范式对主流攻击的抗性，提出VIA攻击框架提升攻击效果。


<details>
  <summary>Details</summary>
Motivation: 合成数据虽提升大语言模型性能，但潜在安全风险未被研究，需评估其训练范式抗性并增强攻击效果以进一步探究风险。

Method: 系统评估合成数据集成训练范式抗性，提出Virus Infection Attack (VIA)攻击框架，将中毒有效载荷藏于“外壳”并寻找最优劫持点。

Result: VIA显著增加合成数据中中毒内容，使下游模型攻击成功率接近上游中毒模型。

Conclusion: 合成数据集成训练范式对现有攻击有强抗性，VIA能增强攻击效果，可用于进一步研究合成数据安全风险。

Abstract: Synthetic data refers to artificial samples generated by models. While it has
been validated to significantly enhance the performance of large language
models (LLMs) during training and has been widely adopted in LLM development,
potential security risks it may introduce remain uninvestigated. This paper
systematically evaluates the resilience of synthetic-data-integrated training
paradigm for LLMs against mainstream poisoning and backdoor attacks. We reveal
that such a paradigm exhibits strong resistance to existing attacks, primarily
thanks to the different distribution patterns between poisoning data and
queries used to generate synthetic samples. To enhance the effectiveness of
these attacks and further investigate the security risks introduced by
synthetic data, we introduce a novel and universal attack framework, namely,
Virus Infection Attack (VIA), which enables the propagation of current attacks
through synthetic data even under purely clean queries. Inspired by the
principles of virus design in cybersecurity, VIA conceals the poisoning payload
within a protective "shell" and strategically searches for optimal hijacking
points in benign samples to maximize the likelihood of generating malicious
content. Extensive experiments on both data poisoning and backdoor attacks show
that VIA significantly increases the presence of poisoning content in synthetic
data and correspondingly raises the attack success rate (ASR) on downstream
models to levels comparable to those observed in the poisoned upstream models.

</details>


### [619] [ReliabilityRAG: Effective and Provably Robust Defense for RAG-based Web-Search](https://arxiv.org/abs/2509.23519)
*Zeyu Shen,Basileal Imana,Tong Wu,Chong Xiang,Prateek Mittal,Aleksandra Korolova*

Main category: cs.CR

TL;DR: 本文介绍了ReliabilityRAG框架，利用文档可靠性信息增强RAG系统对抗攻击的鲁棒性，实验显示其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: RAG系统易受检索语料库攻击，而基于RAG的搜索系统有内置可靠性信号可用于防御，因此提出增强其对抗鲁棒性的框架。

Method: 一是从图论角度，通过寻找文档图的最大独立集过滤恶意文档；二是提出可扩展的加权采样和聚合框架，高效处理大量文档。

Result: ReliabilityRAG在对抗攻击下鲁棒性优于现有方法，良性准确率高，在长文本生成任务中表现出色。

Conclusion: 该工作是对抗RAG检索语料库损坏更有效、可证明鲁棒防御的重要一步。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models by
grounding their outputs in external documents. These systems, however, remain
vulnerable to attacks on the retrieval corpus, such as prompt injection.
RAG-based search systems (e.g., Google's Search AI Overview) present an
interesting setting for studying and protecting against such threats, as
defense algorithms can benefit from built-in reliability signals -- like
document ranking -- and represent a non-LLM challenge for the adversary due to
decades of work to thwart SEO.
  Motivated by, but not limited to, this scenario, this work introduces
ReliabilityRAG, a framework for adversarial robustness that explicitly
leverages reliability information of retrieved documents.
  Our first contribution adopts a graph-theoretic perspective to identify a
"consistent majority" among retrieved documents to filter out malicious ones.
We introduce a novel algorithm based on finding a Maximum Independent Set (MIS)
on a document graph where edges encode contradiction. Our MIS variant
explicitly prioritizes higher-reliability documents and provides provable
robustness guarantees against bounded adversarial corruption under natural
assumptions. Recognizing the computational cost of exact MIS for large
retrieval sets, our second contribution is a scalable weighted sample and
aggregate framework. It explicitly utilizes reliability information, preserving
some robustness guarantees while efficiently handling many documents.
  We present empirical results showing ReliabilityRAG provides superior
robustness against adversarial attacks compared to prior methods, maintains
high benign accuracy, and excels in long-form generation tasks where prior
robustness-focused methods struggled. Our work is a significant step towards
more effective, provably robust defenses against retrieved corpus corruption in
RAG.

</details>


### [620] [Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting](https://arxiv.org/abs/2509.23571)
*Yuqiao Meng,Luoxi Tang,Feiyang Yu,Xi Li,Guanhua Yan,Ping Yang,Zhaohan Xi*

Main category: cs.CR

TL;DR: 随着网络威胁增加，论文提出CyberTeam基准指导大语言模型进行蓝队威胁狩猎，整合任务和模块，评估显示标准化设计有改进且揭示开放式推理局限。


<details>
  <summary>Details</summary>
Motivation: 网络威胁不断升级，大语言模型在蓝队威胁狩猎场景有效性待探索，需工具提升蓝队防御能力。

Method: 构建CyberTeam基准，分两阶段建立标准化工作流，建模现实威胁狩猎流程并通过操作模块处理任务，引导大语言模型模块化执行任务。

Result: 评估领先大语言模型和先进网络安全代理，对比显示标准化设计有改进，开放式推理有局限。

Conclusion: 标准化设计对大语言模型在蓝队威胁狩猎场景有效，开放式推理在现实威胁狩猎有不足。

Abstract: As cyber threats continue to grow in scale and sophistication, blue team
defenders increasingly require advanced tools to proactively detect and
mitigate risks. Large Language Models (LLMs) offer promising capabilities for
enhancing threat analysis. However, their effectiveness in real-world blue team
threat-hunting scenarios remains insufficiently explored. This paper presents
CyberTeam, a benchmark designed to guide LLMs in blue teaming practice.
CyberTeam constructs a standardized workflow in two stages. First, it models
realistic threat-hunting workflows by capturing the dependencies among
analytical tasks from threat attribution to incident response. Next, each task
is addressed through a set of operational modules tailored to its specific
analytical requirements. This transforms threat hunting into a structured
sequence of reasoning steps, with each step grounded in a discrete operation
and ordered according to task-specific dependencies. Guided by this framework,
LLMs are directed to perform threat-hunting tasks through modularized steps.
Overall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs
through standardized threat analysis. We evaluate both leading LLMs and
state-of-the-art cybersecurity agents, comparing CyberTeam against open-ended
reasoning strategies. Our results highlight the improvements enabled by
standardized design, while also revealing the limitations of open-ended
reasoning in real-world threat hunting.

</details>


### [621] [Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence](https://arxiv.org/abs/2509.23573)
*Yuqiao Meng,Luoxi Tang,Feiyang Yu,Jinyuan Jia,Guanhua Yan,Ping Yang,Zhaohan Xi*

Main category: cs.CR

TL;DR: 研究大语言模型在网络威胁情报（CTI）中的内在漏洞，提出分类方法分析失败案例，揭示三种漏洞并给出改进建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽可支持多种CTI任务，但实际部署存在性能差距，需研究其内在漏洞。

Method: 通过多CTI基准和真实威胁报告进行大规模评估，引入整合分层、自回归细化和人工监督的分类方法分析失败实例。

Result: 揭示大语言模型在CTI中的三种基本漏洞：虚假关联、矛盾知识和受限泛化。

Conclusion: 为设计更强大的大语言模型驱动的CTI系统提供了可操作的见解，利于未来研究。

Abstract: Large Language Models (LLMs) are intensively used to assist security analysts
in counteracting the rapid exploitation of cyber threats, wherein LLMs offer
cyber threat intelligence (CTI) to support vulnerability assessment and
incident response. While recent work has shown that LLMs can support a wide
range of CTI tasks such as threat analysis, vulnerability detection, and
intrusion defense, significant performance gaps persist in practical
deployments. In this paper, we investigate the intrinsic vulnerabilities of
LLMs in CTI, focusing on challenges that arise from the nature of the threat
landscape itself rather than the model architecture. Using large-scale
evaluations across multiple CTI benchmarks and real-world threat reports, we
introduce a novel categorization methodology that integrates stratification,
autoregressive refinement, and human-in-the-loop supervision to reliably
analyze failure instances. Through extensive experiments and human inspections,
we reveal three fundamental vulnerabilities: spurious correlations,
contradictory knowledge, and constrained generalization, that limit LLMs in
effectively supporting CTI. Subsequently, we provide actionable insights for
designing more robust LLM-powered CTI systems to facilitate future research.

</details>


### [622] [What Do They Fix? LLM-Aided Categorization of Security Patches for Critical Memory Bugs](https://arxiv.org/abs/2509.22796)
*Xingyu Li,Juefei Pu,Yifan Wu,Xiaochen Zou,Shitong Zhu,Xiaochen Zou,Shitong Zhu,Qiushi Wu,Zheng Zhang,Joshua Hsu,Yue Dong,Zhiyun Qian,Kangjie Lu,Trent Jaeger,Michael De Lucia,Srikanth V. Krishnamurthy*

Main category: cs.CR

TL;DR: 本文针对Linux内核下游维护者难识别安全关键补丁问题，提出DUALLM方法，准确率和F1分数高，还识别出相关漏洞补丁并构造概念验证。


<details>
  <summary>Details</summary>
Motivation: Linux下游维护者采用安全补丁滞后，难识别安全关键补丁，现有细粒度补丁分类方法有局限。

Method: 利用提交标题/消息、差异和代码上下文，开发集成大语言模型和微调小语言模型的双方法管道DUALLM。

Result: DUALLM准确率达87.4%，F1分数0.875，识别出111个补丁，90个经手动验证为真阳性，还构造两个概念验证。

Conclusion: DUALLM显著优于先前解决方案，能有效提高细粒度补丁分类。

Abstract: Open-source software projects are foundational to modern software ecosystems,
with the Linux kernel standing out as a critical exemplar due to its ubiquity
and complexity. Although security patches are continuously integrated into the
Linux mainline kernel, downstream maintainers often delay their adoption,
creating windows of vulnerability. A key reason for this lag is the difficulty
in identifying security-critical patches, particularly those addressing
exploitable vulnerabilities such as out-of-bounds (OOB) accesses and
use-after-free (UAF) bugs. This challenge is exacerbated by intentionally
silent bug fixes, incomplete or missing CVE assignments, delays in CVE
issuance, and recent changes to the CVE assignment criteria for the Linux
kernel. While fine-grained patch classification approaches exist, they exhibit
limitations in both coverage and accuracy. In this work, we identify previously
unexplored opportunities to significantly improve fine-grained patch
classification. Specifically, by leveraging cues from commit titles/messages
and diffs alongside appropriate code context, we develop DUALLM, a dual-method
pipeline that integrates two approaches based on a Large Language Model (LLM)
and a fine-tuned small language model. DUALLM achieves 87.4% accuracy and an
F1-score of 0.875, significantly outperforming prior solutions. Notably, DUALLM
successfully identified 111 of 5,140 recent Linux kernel patches as addressing
OOB or UAF vulnerabilities, with 90 true positives confirmed by manual
verification (many do not have clear indications in patch descriptions).
Moreover, we constructed proof-of-concepts for two identified bugs (one UAF and
one OOB), including one developed to conduct a previously unknown control-flow
hijack as further evidence of the correctness of the classification.

</details>


### [623] [FedBit: Accelerating Privacy-Preserving Federated Learning via Bit-Interleaved Packing and Cross-Layer Co-Design](https://arxiv.org/abs/2509.23091)
*Xiangchen Meng,Yangdi Lyu*

Main category: cs.CR

TL;DR: 提出FedBit框架优化同态加密联邦学习，降低开销且速度提升、精度高。


<details>
  <summary>Details</summary>
Motivation: 现有全同态加密联邦学习存在计算负担和密文扩展问题，导致资源和通信开销大。

Method: 提出FedBit框架，采用位交织数据打包减少密文扩展，集成FPGA加速器处理加密操作，优化数据流降低内存开销。

Result: FedBit在加密上实现两个数量级的加速，平均通信开销降低60.7%，并保持高精度。

Conclusion: FedBit能有效解决全同态加密联邦学习的开销问题，提升性能。

Abstract: Federated learning (FL) with fully homomorphic encryption (FHE) effectively
safeguards data privacy during model aggregation by encrypting local model
updates before transmission, mitigating threats from untrusted servers or
eavesdroppers in transmission. However, the computational burden and ciphertext
expansion associated with homomorphic encryption can significantly increase
resource and communication overhead. To address these challenges, we propose
FedBit, a hardware/software co-designed framework optimized for the
Brakerski-Fan-Vercauteren (BFV) scheme. FedBit employs bit-interleaved data
packing to embed multiple model parameters into a single ciphertext
coefficient, thereby minimizing ciphertext expansion and maximizing
computational parallelism. Additionally, we integrate a dedicated FPGA
accelerator to handle cryptographic operations and an optimized dataflow to
reduce the memory overhead. Experimental results demonstrate that FedBit
achieves a speedup of two orders of magnitude in encryption and lowers average
communication overhead by 60.7%, while maintaining high accuracy.

</details>


### [624] [Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack](https://arxiv.org/abs/2509.23871)
*Yukun Chen,Boheng Li,Yu Yuan,Leyi Qi,Yiming Li,Tianwei Zhang,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: 本文揭示知识蒸馏中的蒸馏条件后门攻击（DCBA），提出SCAR方法实施攻击，实验验证其有效性与抗检测能力。


<details>
  <summary>Details</summary>
Motivation: 发现第三方平台教师模型经安全验证后，知识蒸馏过程仍存在DCBA这一新型关键威胁。

Method: 将攻击建模为双层优化问题，提出SCAR方法，内优化模拟KD过程优化替代学生模型，外优化利用其输出优化教师模型植入条件后门，用隐式微分算法和预优化触发注入函数解决优化问题。

Result: 在不同数据集、模型架构和KD技术上的实验验证了SCAR的有效性，且能抵抗现有后门检测。

Conclusion: 知识蒸馏过程存在此前被忽视的重大漏洞，SCAR方法有效。

Abstract: Knowledge distillation (KD) is a vital technique for deploying deep neural
networks (DNNs) on resource-constrained devices by transferring knowledge from
large teacher models to lightweight student models. While teacher models from
third-party platforms may undergo security verification (\eg, backdoor
detection), we uncover a novel and critical threat: distillation-conditional
backdoor attacks (DCBAs). DCBA injects dormant and undetectable backdoors into
teacher models, which become activated in student models via the KD process,
even with clean distillation datasets. While the direct extension of existing
methods is ineffective for DCBA, we implement this attack by formulating it as
a bilevel optimization problem and proposing a simple yet effective method
(\ie, SCAR). Specifically, the inner optimization simulates the KD process by
optimizing a surrogate student model, while the outer optimization leverages
outputs from this surrogate to optimize the teacher model for implanting the
conditional backdoor. Our SCAR addresses this complex optimization utilizing an
implicit differentiation algorithm with a pre-optimized trigger injection
function. Extensive experiments across diverse datasets, model architectures,
and KD techniques validate the effectiveness of our SCAR and its resistance
against existing backdoor detection, highlighting a significant yet previously
overlooked vulnerability in the KD process. Our code is available at
https://github.com/WhitolfChen/SCAR.

</details>


### [625] [VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized Inference](https://arxiv.org/abs/2509.24257)
*Ke Wang,Felix Qu,Libin Xia,Zishuo Zhao,Chris Tong,Lynn Ai,Eric Yang*

Main category: cs.CR

TL;DR: 提出用于去中心化大语言模型推理的可公开验证协议VeriLLM，引入同构推理验证网络，有形式化博弈论分析与安全证明。


<details>
  <summary>Details</summary>
Motivation: 去中心化推理服务大语言模型虽有优势，但无许可环境下节点缺乏先验信任，需输出可验证性以安全部署。

Method: 提出VeriLLM协议，设计轻量级验证算法，使用同行预测机制；引入同构推理验证网络；进行形式化博弈论分析。

Result: VeriLLM协议在一个诚实验证者假设下实现安全，验证成本极低，通过同行预测机制防止懒惰验证；同构网络提高GPU利用率和端到端吞吐量，扩大验证者池，加强鲁棒性和安全性；证明诚实推理和验证构成纳什均衡。

Conclusion: 这是首个具有端到端博弈论安全证明的去中心化推理验证协议。

Abstract: Decentralized inference is an appealing paradigm for serving large language
models (LLMs), offering strong security, high efficiency, and lower operating
costs. Yet the permissionless setting admits no a priori trust in participating
nodes, making output verifiability a prerequisite for secure deployment. We
present VeriLLM, a publicly verifiable protocol for decentralized LLM inference
that (i) achieves security under a one-honest-verifier assumption, (ii) attains
near-negligible verification cost (about 1% of the underlying inference) via a
lightweight verification algorithm designed explicitly for LLMs, and (iii)
enforces honest checking through a peer-prediction mechanism that mitigates
lazy verification in naive voting. We further introduce an isomorphic
inference-verification network that multiplexes both roles on the same set of
GPU workers. This architecture (i) increases GPU utilization and thereby
improves end-to-end throughput for both inference and verification, (ii)
expands the effective pool of available validators, strengthening robustness
and security, and (iii) enforces task indistinguishability at the worker
boundary to prevent job-type-conditioned behavior. Finally, we provide a formal
game-theoretic analysis and prove that, under our incentives, honest inference
and verification constitute a Nash equilibrium, ensuring incentive
compatibility against rational adversaries. To our knowledge, this is the first
decentralized inference verification protocol with an end-to-end game-theoretic
security proof.

</details>


### [626] [FuncPoison: Poisoning Function Library to Hijack Multi-agent Autonomous Driving Systems](https://arxiv.org/abs/2509.24408)
*Yuzhen Long,Songze Li*

Main category: cs.CR

TL;DR: 本文提出针对共享函数库的FuncPoison攻击，可操纵基于大语言模型的多智能体自动驾驶系统行为，实验证明其能造成负面影响，揭示函数库是关键攻击面。


<details>
  <summary>Details</summary>
Motivation: 共享函数库在基于大语言模型的多智能体自动驾驶系统决策中至关重要，但仍是未充分探索的漏洞，需研究其安全性。

Method: 提出FuncPoison攻击，利用智能体访问函数库的两个弱点，注入带欺骗性指令的恶意工具，操纵一个智能体决策引发级联错误。

Result: 在两个代表性多智能体自动驾驶系统上实验表明，FuncPoison能显著降低轨迹精度、灵活诱导特定智能体协同不当行为、躲避多种防御机制。

Conclusion: 函数库常被视为简单工具集，实际是基于大语言模型的自动驾驶系统的关键攻击面，其可靠性令人担忧。

Abstract: Autonomous driving systems increasingly rely on multi-agent architectures
powered by large language models (LLMs), where specialized agents collaborate
to perceive, reason, and plan. A key component of these systems is the shared
function library, a collection of software tools that agents use to process
sensor data and navigate complex driving environments. Despite its critical
role in agent decision-making, the function library remains an under-explored
vulnerability. In this paper, we introduce FuncPoison, a novel poisoning-based
attack targeting the function library to manipulate the behavior of LLM-driven
multi-agent autonomous systems. FuncPoison exploits two key weaknesses in how
agents access the function library: (1) agents rely on text-based instructions
to select tools; and (2) these tools are activated using standardized command
formats that attackers can replicate. By injecting malicious tools with
deceptive instructions, FuncPoison manipulates one agent s decisions--such as
misinterpreting road conditions--triggering cascading errors that mislead other
agents in the system. We experimentally evaluate FuncPoison on two
representative multi-agent autonomous driving systems, demonstrating its
ability to significantly degrade trajectory accuracy, flexibly target specific
agents to induce coordinated misbehavior, and evade diverse defense mechanisms.
Our results reveal that the function library, often considered a simple
toolset, can serve as a critical attack surface in LLM-based autonomous driving
systems, raising elevated concerns on their reliability.

</details>


### [627] [Of-SemWat: High-payload text embedding for semantic watermarking of AI-generated images with arbitrary size](https://arxiv.org/abs/2509.24823)
*Benedetta Tondi,Andrea Costanzo,Mauro Barni*

Main category: cs.CR

TL;DR: 提出用于文本嵌入的高负载图像水印方法，实验表明该方法鲁棒性强，可通过图文不匹配分析揭示图像语义修改情况。


<details>
  <summary>Details</summary>
Motivation: 在大规模图像中鲁棒地嵌入高负载信息，如现代AI生成的图像。

Method: 基于传统水印方案，利用正交和turbo码提高鲁棒性，集成频域嵌入和感知掩蔽技术增强水印不可见性。

Result: 该方法对多种图像处理具有极强鲁棒性，即使经过传统和AI图像修复仍可检索嵌入文本。

Conclusion: 该方法能通过图文不匹配分析揭示图像所经历的语义修改。

Abstract: We propose a high-payload image watermarking method for textual embedding,
where a semantic description of the image - which may also correspond to the
input text prompt-, is embedded inside the image. In order to be able to
robustly embed high payloads in large-scale images - such as those produced by
modern AI generators - the proposed approach builds upon a traditional
watermarking scheme that exploits orthogonal and turbo codes for improved
robustness, and integrates frequency-domain embedding and perceptual masking
techniques to enhance watermark imperceptibility. Experiments show that the
proposed method is extremely robust against a wide variety of image processing,
and the embedded text can be retrieved also after traditional and AI
inpainting, permitting to unveil the semantic modification the image has
undergone via image-text mismatch analysis.

</details>


### [628] [Optimizing Privacy-Preserving Primitives to Support LLM-Scale Applications](https://arxiv.org/abs/2509.25072)
*Yaman Jandali,Ruisi Zhang,Nojan Sheybani,Farinaz Koushanfar*

Main category: cs.CR

TL;DR: 本文介绍利用MPC、ZKPs和FHE缩小隐私保护学习系统开销与实用性差距的工作，通过软硬件算法协同设计推动大语言模型规模应用，在多个场景验证了方案有效性。


<details>
  <summary>Details</summary>
Motivation: 隐私保护技术在实际应用中存在计算和通信开销大的问题，阻碍其大规模应用，需要缩小开销与实用性的差距。

Method: 采用多方计算（MPC）、零知识证明（ZKPs）和全同态加密（FHE），进行硬件/软件/算法协同设计。

Result: 在隐私保护环境中推动了大语言模型规模应用的进展，在DNN知识产权归属、道德大语言模型使用执行和Transformer推理等场景验证了方案的有效性。

Conclusion: 通过所采用的方法和设计，能够在隐私保护学习系统中缩小开销与实用性的差距，促进相关技术的实际应用。

Abstract: Privacy-preserving technologies have introduced a paradigm shift that allows
for realizable secure computing in real-world systems. The significant barrier
to the practical adoption of these primitives is the computational and
communication overhead that is incurred when applied at scale. In this paper,
we present an overview of our efforts to bridge the gap between this overhead
and practicality for privacy-preserving learning systems using multi-party
computation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic
encryption (FHE). Through meticulous hardware/software/algorithm co-design, we
show progress towards enabling LLM-scale applications in privacy-preserving
settings. We demonstrate the efficacy of our solutions in several contexts,
including DNN IP ownership, ethical LLM usage enforcement, and transformer
inference.

</details>


### [629] [SecInfer: Preventing Prompt Injection via Inference-time Scaling](https://arxiv.org/abs/2509.24967)
*Yupei Liu,Yanting Wang,Yuqi Jia,Jinyuan Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 提出SecInfer防御提示注入攻击，结合推理时缩放范式，实验表明其能有效缓解攻击且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于微调的防御方法对提示注入攻击效果有限，需要新的防御方案。

Method: 提出SecInfer防御方法，包含系统提示引导采样和目标任务引导聚合两个关键步骤。

Result: SecInfer通过在推理时利用额外计算资源，有效缓解现有和自适应提示注入攻击，性能优于现有防御方法和推理时缩放方法。

Conclusion: SecInfer是一种有效的防御提示注入攻击的方法。

Abstract: Prompt injection attacks pose a pervasive threat to the security of Large
Language Models (LLMs). State-of-the-art prevention-based defenses typically
rely on fine-tuning an LLM to enhance its security, but they achieve limited
effectiveness against strong attacks. In this work, we propose \emph{SecInfer},
a novel defense against prompt injection attacks built on \emph{inference-time
scaling}, an emerging paradigm that boosts LLM capability by allocating more
compute resources for reasoning during inference. SecInfer consists of two key
steps: \emph{system-prompt-guided sampling}, which generates multiple responses
for a given input by exploring diverse reasoning paths through a varied set of
system prompts, and \emph{target-task-guided aggregation}, which selects the
response most likely to accomplish the intended task. Extensive experiments
show that, by leveraging additional compute at inference, SecInfer effectively
mitigates both existing and adaptive prompt injection attacks, outperforming
state-of-the-art defenses as well as existing inference-time scaling
approaches.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [630] [NeuroBridge: Using Generative AI to Bridge Cross-neurotype Communication Differences through Neurotypical Perspective-taking](https://arxiv.org/abs/2509.23434)
*Rukhshan Haroon,Kyle Wigdor,Katie Yang,Nicole Toumanios,Eileen T. Crehan,Fahad Dogar*

Main category: cs.HC

TL;DR: 本文介绍NeuroBridge平台助力神经典型人群理解自闭症患者交流方式，用户研究显示有积极效果，最后探讨设计启示与局限。


<details>
  <summary>Details</summary>
Motivation: 解决自闭症与神经典型人群交流中，自闭症患者需适应神经典型规范的不平衡问题。

Method: 构建NeuroBridge平台，利用大语言模型模拟交流场景，开展用户研究。

Result: 12名神经典型参与者对自闭症患者语言理解提升，认可平台互动性与反馈，多数认为模拟准确。

Conclusion: 讨论AI中残疾表征的设计启示、平台个性化需求及大语言模型模拟复杂社交场景的局限。

Abstract: Communication challenges between autistic and neurotypical individuals stem
from a mutual lack of understanding of each other's distinct, and often
contrasting, communication styles. Yet, autistic individuals are expected to
adapt to neurotypical norms, making interactions inauthentic and mentally
exhausting for them. To help redress this imbalance, we build NeuroBridge, an
online platform that utilizes large language models (LLMs) to simulate: (a) an
AI character that is direct and literal, a style common among many autistic
individuals, and (b) four cross-neurotype communication scenarios in a
feedback-driven conversation between this character and a neurotypical user.
Through NeuroBridge, neurotypical individuals gain a firsthand look at autistic
communication, and reflect on their role in shaping cross-neurotype
interactions. In a user study with 12 neurotypical participants, we find that
NeuroBridge improved their understanding of how autistic people may interpret
language differently, with all describing autism as a social difference that
"needs understanding by others" after completing the simulation. Participants
valued its personalized, interactive format and described AI-generated feedback
as "constructive", "logical" and "non-judgmental". Most perceived the portrayal
of autism in the simulation as accurate, suggesting that users may readily
accept AI-generated (mis)representations of disabilities. To conclude, we
discuss design implications for disability representation in AI, the need for
making NeuroBridge more personalized, and LLMs' limitations in modeling complex
social scenarios.

</details>


### [631] [Privy: Envisioning and Mitigating Privacy Risks for Consumer-facing AI Product Concepts](https://arxiv.org/abs/2509.23525)
*Hao-Ping Lee,Yu-Ju Yang,Matthew Bilik,Isadora Krsek,Thomas Serban von Davier,Kyzyl Monteiro,Jason Lin,Shivani Agarwal,Jodi Forlizzi,Sauvik Das*

Main category: cs.HC

TL;DR: 提出工具Privy用于AI隐私风险评估，经评估该工具能帮助从业者完成高质量评估，LLM版本效果更佳，从业者认为其有用易用。


<details>
  <summary>Details</summary>
Motivation: AI带来并加剧隐私风险，但从业者缺乏有效资源识别和缓解这些风险。

Method: 通过对11名从业者的形成性研究塑造Privy的两个版本（LLM驱动和模板式），用24名从业者进行组间对照研究，评估由13名独立隐私专家审查。

Result: Privy帮助从业者完成专家认可的高质量隐私评估，识别相关风险并提出适当缓解策略，LLM驱动版本效果更好，从业者认为其有用且易用。

Conclusion: Privy有助于克服隐私工作中长期存在的意识、动机和能力障碍。

Abstract: AI creates and exacerbates privacy risks, yet practitioners lack effective
resources to identify and mitigate these risks. We present Privy, a tool that
guides practitioners through structured privacy impact assessments to: (i)
identify relevant risks in novel AI product concepts, and (ii) propose
appropriate mitigations. Privy was shaped by a formative study with 11
practitioners, which informed two versions -- one LLM-powered, the other
template-based. We evaluated these two versions of Privy through a
between-subjects, controlled study with 24 separate practitioners, whose
assessments were reviewed by 13 independent privacy experts. Results show that
Privy helps practitioners produce privacy assessments that experts deemed high
quality: practitioners identified relevant risks and proposed appropriate
mitigation strategies. These effects were augmented in the LLM-powered version.
Practitioners themselves rated Privy as being useful and usable, and their
feedback illustrates how it helps overcome long-standing awareness, motivation,
and ability barriers in privacy work.

</details>


### [632] [Explicit modelling of subject dependency in BCI decoding](https://arxiv.org/abs/2509.23247)
*Michele Romani,Francesco Paissan,Andrea Fossà,Elisabetta Farella*

Main category: cs.HC

TL;DR: 提出用轻量级CNN显式建模受试者依赖的端到端方法，优化超参数并评估调节机制，在ERP分类任务中表现良好，提升泛化与校准效率。


<details>
  <summary>Details</summary>
Motivation: 解决脑机接口（BCIs）存在的受试者间差异大、标记数据有限及校准阶段长的问题。

Method: 用轻量级CNN显式建模受试者依赖，集成超参数优化策略，评估两种调节机制，在ERP分类任务中测试三种轻量级架构。

Result: 实现了更好的泛化和数据高效校准。

Conclusion: 受试者自适应BCIs具有可扩展性和实用性。

Abstract: Brain-Computer Interfaces (BCIs) suffer from high inter-subject variability
and limited labeled data, often requiring lengthy calibration phases. In this
work, we present an end-to-end approach that explicitly models the subject
dependency using lightweight convolutional neural networks (CNNs) conditioned
on the subject's identity. Our method integrates hyperparameter optimization
strategies that prioritize class imbalance and evaluates two conditioning
mechanisms to adapt pre-trained models to unseen subjects with minimal
calibration data. We benchmark three lightweight architectures on a
time-modulated Event-Related Potentials (ERP) classification task, providing
interpretable evaluation metrics and explainable visualizations of the learned
representations. Results demonstrate improved generalization and data-efficient
calibration, highlighting the scalability and practicality of subject-adaptive
BCIs.

</details>


### [633] [Bridging the behavior-neural gap: A multimodal AI reveals the brain's geometry of emotion more accurately than human self-reports](https://arxiv.org/abs/2509.24298)
*Changde Du,Yizhuo Lu,Zhongyu Huang,Yi Sun,Zisen Zhou,Shaozheng Qin,Huiguang He*

Main category: cs.HC

TL;DR: 研究用AI模型收集情感视频判断，发现MLLM表征能高精度预测人类神经活动，表明MLLM可自主发展神经对齐的情感表征。


<details>
  <summary>Details</summary>
Motivation: 解决情感空间几何和神经基础存在争议，以及人类自我报告预测大脑活动能力有限的“行为 - 神经差距”问题。

Method: 使用AI模型作为“认知主体”，从MLLM和LLM收集数百万个情感视频的三元组判断。

Result: 模型的30维嵌入可解释，MLLM表征预测人类情绪处理网络神经活动的准确性最高，优于LLM和人类行为评级。

Conclusion: 表明感官基础对开发神经对齐的情感概念框架很关键，MLLM能自主发展神经对齐的情感表征，可弥合主观体验和神经基质间的差距。

Abstract: The ability to represent emotion plays a significant role in human cognition
and social interaction, yet the high-dimensional geometry of this affective
space and its neural underpinnings remain debated. A key challenge, the
`behavior-neural gap,' is the limited ability of human self-reports to predict
brain activity. Here we test the hypothesis that this gap arises from the
constraints of traditional rating scales and that large-scale similarity
judgments can more faithfully capture the brain's affective geometry. Using AI
models as `cognitive agents,' we collected millions of triplet odd-one-out
judgments from a multimodal large language model (MLLM) and a language-only
model (LLM) in response to 2,180 emotionally evocative videos. We found that
the emergent 30-dimensional embeddings from these models are highly
interpretable and organize emotion primarily along categorical lines, yet in a
blended fashion that incorporates dimensional properties. Most remarkably, the
MLLM's representation predicted neural activity in human emotion-processing
networks with the highest accuracy, outperforming not only the LLM but also,
counterintuitively, representations derived directly from human behavioral
ratings. This result supports our primary hypothesis and suggests that sensory
grounding--learning from rich visual data--is critical for developing a truly
neurally-aligned conceptual framework for emotion. Our findings provide
compelling evidence that MLLMs can autonomously develop rich, neurally-aligned
affective representations, offering a powerful paradigm to bridge the gap
between subjective experience and its neural substrates. Project page:
https://reedonepeck.github.io/ai-emotion.github.io/.

</details>


### [634] [TraitSpaces: Towards Interpretable Visual Creativity for Human-AI Co-Creation](https://arxiv.org/abs/2509.24326)
*Prerna Luthra*

Main category: cs.HC

TL;DR: 提出建模视觉创造力的框架，定义12种创造力特征，用图像评估特征可学习性，可视化特征空间并支持共创。


<details>
  <summary>Details</summary>
Motivation: 为艺术家、研究者和AI系统提供共享语言和可解释工具，实现有意义协作。

Method: 结合艺术家访谈和心理学理论定义特征，用GPT 4.1标注图像，从CLIP图像嵌入评估特征可学习性。

Result: 部分特征预测可靠性高，部分特征难预测，凸显纯视觉编码局限，可视化特征空间支持共创。

Conclusion: 将文化美学见解与计算建模结合，不将创造力简化为数字，而是提供协作工具。

Abstract: We introduce a psychologically grounded and artist-informed framework for
modeling visual creativity across four domains: Inner, Outer, Imaginative, and
Moral Worlds. Drawing on interviews with practicing artists and theories from
psychology, we define 12 traits that capture affective, symbolic, cultural, and
ethical dimensions of creativity.Using 20k artworks from the SemArt dataset, we
annotate images with GPT 4.1 using detailed, theory-aligned prompts, and
evaluate the learnability of these traits from CLIP image embeddings. Traits
such as Environmental Dialogicity and Redemptive Arc are predicted with high
reliability ($R^2 \approx 0.64 - 0.68$), while others like Memory Imprint
remain challenging, highlighting the limits of purely visual encoding. Beyond
technical metrics, we visualize a "creativity trait-space" and illustrate how
it can support interpretable, trait-aware co-creation - e.g., sliding along a
Redemptive Arc axis to explore works of adversity and renewal. By linking
cultural-aesthetic insights with computational modeling, our work aims not to
reduce creativity to numbers, but to offer shared language and interpretable
tools for artists, researchers, and AI systems to collaborate meaningfully.

</details>


### [635] [Understanding Cognitive States from Head & Hand Motion Data](https://arxiv.org/abs/2509.24255)
*Kaiang Wen,Mark Roman Miller*

Main category: cs.HC

TL;DR: 研究利用VR系统的头部和手部运动数据推断用户认知状态，发现深度时间模型仅通过运动就能推断微妙认知状态，成果可推动自适应虚拟环境发展，且数据集和框架将公开。


<details>
  <summary>Details</summary>
Motivation: 现有研究不清楚运动运动学在多大程度上能编码更细微的认知状态，如困惑、犹豫和准备状态等。

Method: 引入一个带有帧级认知状态注释的头部和手部运动新数据集，在结构化决策任务中收集数据。

Result: 深度时间模型仅通过运动就能推断微妙认知状态，性能与人类观察者相当。

Conclusion: 标准VR遥测数据包含与用户内部认知过程相关的强模式，为新一代自适应虚拟环境打开了大门。

Abstract: As virtual reality (VR) and augmented reality (AR) continue to gain
popularity, head and hand motion data captured by consumer VR systems have
become ubiquitous. Prior work shows that such telemetry can be highly
identifying and reflect broad user traits, often aligning with intuitive "folk
theories" of body language. However, it remains unclear to what extent motion
kinematics encode more nuanced cognitive states, such as confusion, hesitation,
and readiness, which lack clear correlates with motion. To investigate this, we
introduce a novel dataset of head and hand motion with frame-level annotations
of these states collected during structured decision-making tasks. Our findings
suggest that deep temporal models can infer subtle cognitive states from motion
alone, achieving comparable performance with human observers. This work
demonstrates that standard VR telemetry contains strong patterns related to
users' internal cognitive processes, which opens the door for a new generation
of adaptive virtual environments. To enhance reproducibility and support future
work, we will make our dataset and modeling framework publicly available.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [636] [Asymptotic Expansion for Nonlinear Filtering in the Small System Noise Regime](https://arxiv.org/abs/2509.23920)
*Masahiro Kurisaki*

Main category: eess.SP

TL;DR: 提出基于系统噪声小参数的非线性滤波渐近展开法，缓解计算效率与精度权衡问题，能以低成本捕捉复杂分布特征。


<details>
  <summary>Details</summary>
Motivation: 解决现有非线性滤波方法在计算效率和精度之间的权衡问题，更好地捕捉条件分布的复杂特征。

Method: 基于系统噪声小参数进行渐近展开，将条件期望展开为噪声水平的幂级数，通过求解常微分方程组计算系数，还结合Edgeworth型展开。

Result: 缓解了现有方法在计算效率和精度间的权衡问题，能以显著更低的计算成本捕捉条件分布的复杂特征，如多峰性。

Conclusion: 所提出的新渐近展开法在非线性滤波中具有优势，能在保证精度的同时降低计算成本。

Abstract: We propose a new asymptotic expansion method for nonlinear filtering, based
on a small parameter in the system noise. The conditional expectation is
expanded as a power series in the noise level, with each coefficient computed
by solving a system of ordinary differential equations. This approach mitigates
the trade-off between computational efficiency and accuracy inherent in
existing methods such as Gaussian approximations and particle filters.
Moreover, by incorporating an Edgeworth-type expansion, our method captures
complex features of the conditional distribution, such as multimodality, with
significantly lower computational cost than conventional filtering algorithms.

</details>


### [637] [YOLO-based Bearing Fault Diagnosis With Continuous Wavelet Transform](https://arxiv.org/abs/2509.03070)
*Po-Heng Chou,Wei-Lung Mao,Ru-Ping Lin*

Main category: eess.SP

TL;DR: 提出基于YOLO的框架用于空间轴承故障诊断，用CWT生成时频图，在三个数据集评估，比基线模型更优且能可视化故障位置。


<details>
  <summary>Details</summary>
Motivation: 实现空间轴承故障诊断，提升诊断准确性和泛化性。

Method: 用Morlet小波将一维振动信号转为时频图，用YOLOv9、v10、v11模型处理时频图分类故障类型。

Result: CWT - YOLO管道比基线MCNN - LSTM模型有更高准确性和泛化性，YOLOv11在三个数据集有高mAP分数。

Conclusion: 该方法为旋转机械状态监测提供实用解决方案。

Abstract: This letter proposes a YOLO-based framework for spatial bearing fault
diagnosis using time-frequency spectrograms derived from continuous wavelet
transform (CWT). One-dimensional vibration signals are first transformed into
time-frequency spectrograms using Morlet wavelets to capture transient fault
signatures. These spectrograms are then processed by YOLOv9, v10, and v11
models to classify fault types. Evaluated on three benchmark datasets,
including Case Western Reserve University (CWRU), Paderborn University (PU),
and Intelligent Maintenance System (IMS), the proposed CWT-YOLO pipeline
achieves significantly higher accuracy and generalizability than the baseline
MCNN-LSTM model. Notably, YOLOv11 reaches mAP scores of 99.4% (CWRU), 97.8%
(PU), and 99.5% (IMS). In addition, its region-aware detection mechanism
enables direct visualization of fault locations in spectrograms, offering a
practical solution for condition monitoring in rotating machinery.

</details>


### [638] [Green Learning for STAR-RIS mmWave Systems with Implicit CSI](https://arxiv.org/abs/2509.06820)
*Yu-Hsiang Huang,Po-Heng Chou,Wan-Jen Huang,Walid Saad,C. -C. Jay Kuo*

Main category: eess.SP

TL;DR: 本文为STAR - RIS辅助的毫米波MIMO广播系统提出基于绿色学习（GL）的预编码框架，该框架无需显式CSI估计，轻量级且性能优，适合实时部署。


<details>
  <summary>Details</summary>
Motivation: 未来6G网络强调环境可持续性，采用广播传输架构可提高频谱效率、减少冗余传输和功耗。

Method: 提出GL框架，集成子空间近似与调整偏差（Saab）、基于相关特征测试（RFT）的监督特征选择和基于极端梯度提升（XGBoost）的决策学习，在全CSI下用BCD生成监督训练，推理时无需CSI。

Result: GL方法与BCD和基于DL的模型相比，能实现有竞争力的频谱效率，同时将浮点运算（FLOPs）减少四个数量级以上。

Conclusion: GL方法适合在能源和硬件受限的广播场景中实时部署。

Abstract: In this paper, a green learning (GL)-based precoding framework is proposed
for simultaneously transmitting and reflecting reconfigurable intelligent
surface (STAR-RIS)-aided millimeter-wave (mmWave) MIMO broadcasting systems.
Motivated by the growing emphasis on environmental sustainability in future 6G
networks, this work adopts a broadcasting transmission architecture for
scenarios where multiple users share identical information, improving spectral
efficiency and reducing redundant transmissions and power consumption.
Different from conventional optimization methods, such as block coordinate
descent (BCD) that require perfect channel state information (CSI) and
iterative computation, the proposed GL framework operates directly on received
uplink pilot signals without explicit CSI estimation. Unlike deep learning (DL)
approaches that require CSI-based labels for training, the proposed GL approach
also avoids deep neural networks and backpropagation, leading to a more
lightweight design. Although the proposed GL framework is trained with
supervision generated by BCD under full CSI, inference is performed in a fully
CSI-free manner. The proposed GL integrates subspace approximation with
adjusted bias (Saab), relevant feature test (RFT)-based supervised feature
selection, and eXtreme gradient boosting (XGBoost)-based decision learning to
jointly predict the STAR-RIS coefficients and transmit precoder. Simulation
results show that the proposed GL approach achieves competitive spectral
efficiency compared to BCD and DL-based models, while reducing floating-point
operations (FLOPs) by over four orders of magnitude. These advantages make the
proposed GL approach highly suitable for real-time deployment in energy- and
hardware-constrained broadcasting scenarios.

</details>


### [639] [Sustainable LSTM-Based Precoding for RIS-Aided mmWave MIMO Systems with Implicit CSI](https://arxiv.org/abs/2509.12658)
*Po-Heng Chou,Jiun-Jia Wu,Wan-Jen Huang,Ronald Y. Chang*

Main category: eess.SP

TL;DR: 提出基于LSTM的可持续预编码框架用于RIS辅助毫米波MIMO系统，减少开销和复杂度，节能高效


<details>
  <summary>Details</summary>
Motivation: 为RIS辅助毫米波MIMO系统设计可持续预编码方案，减少开销和复杂度

Method: 利用上行导频序列隐式学习信道特征，结合RIS元素模型，采用多标签训练策略

Result: 设计方案实现ES频谱效率超90%，计算时间仅2.2%，能耗降低近两个数量级，在分布失配下有弹性，可扩展

Conclusion: 该方法是6G无线网络实用且节能的解决方案

Abstract: In this paper, we propose a sustainable long short-term memory (LSTM)-based
precoding framework for reconfigurable intelligent surface (RIS)-assisted
millimeter-wave (mmWave) MIMO systems. Instead of explicit channel state
information (CSI) estimation, the framework exploits uplink pilot sequences to
implicitly learn channel characteristics, reducing both pilot overhead and
inference complexity. Practical hardware constraints are addressed by
incorporating the phase-dependent amplitude model of RIS elements, while a
multi-label training strategy improves robustness when multiple near-optimal
codewords yield comparable performance. Simulations show that the proposed
design achieves over 90% of the spectral efficiency of exhaustive search (ES)
with only 2.2% of its computation time, cutting energy consumption by nearly
two orders of magnitude. The method also demonstrates resilience under
distribution mismatch and scalability to larger RIS arrays, making it a
practical and energy-efficient solution for sustainable 6G wireless networks.

</details>


### [640] [Generative Modeling and Decision Fusion for Unknown Event Detection and Classification Using Synchrophasor Data](https://arxiv.org/abs/2509.22795)
*Yi Hu,Zheyuan Cheng*

Main category: eess.SP

TL;DR: 本文提出一种集成生成建模、滑动窗口时间处理和决策融合的框架，用同步相量数据实现电力系统事件检测与分类，实验结果准确性高，能识别未知事件。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有限标注数据集，难以泛化到罕见或未见干扰，为解决此问题提出新框架。

Method: 采用变分自编码器 - 生成对抗网络建模正常运行状态，提取重建误差和判别器误差作为异常指标，开发基于阈值和凸包的两种决策策略，通过滑动窗口机制构建时空检测和分类矩阵，最后进行识别和决策融合。

Result: 实验结果达到了最先进的准确性，超越了机器学习、深度学习和基于包络的基线方法。

Conclusion: 该方法能够识别已知事件并对未知干扰分类，具有适应性和实用价值，适用于现代电力系统广域事件分析。

Abstract: Reliable detection and classification of power system events are critical for
maintaining grid stability and situational awareness. Existing approaches often
depend on limited labeled datasets, which restricts their ability to generalize
to rare or unseen disturbances. This paper proposes a novel framework that
integrates generative modeling, sliding-window temporal processing, and
decision fusion to achieve robust event detection and classification using
synchrophasor data. A variational autoencoder-generative adversarial network is
employed to model normal operating conditions, where both reconstruction error
and discriminator error are extracted as anomaly indicators. Two complementary
decision strategies are developed: a threshold-based rule for computational
efficiency and a convex hull-based method for robustness under complex error
distributions. These features are organized into spatiotemporal detection and
classification matrices through a sliding-window mechanism, and an
identification and decision fusion stage integrates the outputs across PMUs.
This design enables the framework to identify known events while systematically
classifying previously unseen disturbances into a new category, addressing a
key limitation of supervised classifiers. Experimental results demonstrate
state-of-the-art accuracy, surpassing machine learning, deep learning, and
envelope-based baselines. The ability to recognize unknown events further
highlights the adaptability and practical value of the proposed approach for
wide-area event analysis in modern power systems.

</details>


### [641] [Scalable Wi-Fi RSS-Based Indoor Localization via Automatic Vision-Assisted Calibration](https://arxiv.org/abs/2509.22869)
*Abdulkadir Bilge,Erdem Ergen,Burak Soner,Sinem Coleri*

Main category: eess.SP

TL;DR: 介绍轻量级框架，用相机辅助校准阶段自动收集高分辨率同步RSS - 位置数据，解决RSS定位问题，实验验证其有效性并开源代码等。


<details>
  <summary>Details</summary>
Motivation: RSS定位方法易受干扰，有局限性，监督学习方法需大量昂贵的标注数据，需解决数据收集问题。

Method: 通过短时间相机辅助校准阶段，用ArUco标记校准头顶相机，跟踪收集RSS数据的设备以自动收集(x, y, RSS)数据集，训练定位算法。

Result: 量化了视觉辅助RSS数据收集在关键因素下的精度限制，在不同信号条件和设备类型下测试传统和监督学习方法，显示出精度和泛化性的提升。

Conclusion: 所提出的框架对实际应用有实用价值，且相关代码、工具和数据集已开源。

Abstract: Wi-Fi-based positioning promises a scalable and privacy-preserving solution
for location-based services in indoor environments such as malls, airports, and
campuses. RSS-based methods are widely deployable as RSS data is available on
all Wi-Fi-capable devices, but RSS is highly sensitive to multipath, channel
variations, and receiver characteristics. While supervised learning methods
offer improved robustness, they require large amounts of labeled data, which is
often costly to obtain. We introduce a lightweight framework that solves this
by automating high-resolution synchronized RSS-location data collection using a
short, camera-assisted calibration phase. An overhead camera is calibrated only
once with ArUco markers and then tracks a device collecting RSS data from
broadcast packets of nearby access points across Wi-Fi channels. The resulting
(x, y, RSS) dataset is used to automatically train mobile-deployable
localization algorithms, avoiding the privacy concerns of continuous video
monitoring. We quantify the accuracy limits of such vision-assisted RSS data
collection under key factors such as tracking precision and label
synchronization. Using the collected experimental data, we benchmark
traditional and supervised learning approaches under varying signal conditions
and device types, demonstrating improved accuracy and generalization,
validating the utility of the proposed framework for practical use. All code,
tools, and datasets are released as open source.

</details>


### [642] [Joint Hybrid Beamforming and Artificial Noise Design for Secure Multi-UAV ISAC Networks](https://arxiv.org/abs/2509.23687)
*Runze Dong,Buhong Wang,Cunqian Feng,Jiang Weng,Chen Han,Jiwei Tian*

Main category: eess.SP

TL;DR: 本文提出多无人机网络安全高效ISAC框架及两阶段优化方法，仿真验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究将无人机主要视为空中基站，忽略其作为ISAC用户角色，且未利用地面基站大规模天线阵列提升安全和频谱效率。

Method: 提出安全高效ISAC框架，采用两阶段优化方法联合设计混合波束赋形、人工噪声注入和无人机轨迹，第一阶段用PPO优化数字波束赋形器和轨迹，第二阶段通过低复杂度矩阵分解将数字解分解为模拟和数字分量。

Result: 仿真结果表明所提框架相比基准方案更有效。

Conclusion: 所提安全和频谱高效的ISAC框架及优化方法可行且有效。

Abstract: Integrated sensing and communication (ISAC) emerges as a key enabler for
next-generation applications such as smart cities and autonomous systems. Its
integration with unmanned aerial vehicles (UAVs) unlocks new potentials for
reliable communication and precise sensing in dynamic aerial environments.
However, existing research predominantly treats UAVs as aerial base stations,
overlooking their role as ISAC users, and fails to leverage large-scale antenna
arrays at terrestrial base stations to enhance security and spectral
efficiency. This paper propose a secure and spectral efficient ISAC framework
for multi-UAV networks, and a two-stage optimization approach is developed to
jointly design hybrid beamforming (HBF), artificial noise (AN) injection, and
UAV trajectories. Aiming at maximizing the sum secrecy rate, the first stage
employs Proximal Policy Optimization (PPO) to optimize digital beamformers and
trajectories, and the second stage decomposes the digital solution into analog
and digital components via low-complexity matrix factorization. Simulation
results demonstrate the effectiveness of the proposed framework compared to
benchmark schemes.

</details>


### [643] [Uni-NTFM: A Unified Foundation Model for EEG Signal Representation Learning](https://arxiv.org/abs/2509.24222)
*Zhisheng Chen,Yingwei Zhang,Qizhen Lan,Tianyu Liu,Huacan Wang,Yi Ding,Ziyu Jia,Ronghao Chen,Kun Wang,Xinliang Zhou*

Main category: eess.SP

TL;DR: 现有脑基础模型应用于EEG有局限，本文提出Uni - NTFM模型，在下游任务表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有脑基础模型应用于EEG时，因信号特性存在处理时域与频域特征、电极空间拓扑、网络灵活性等方面的局限，需改进。

Method: 提出Uni - NTFM模型，有解耦架构、拓扑嵌入机制、混合专家神经Transformer三个核心创新，用双域掩码重建目标在大量数据上预训练。

Result: 最大模型Uni - NTFM$_{large}$有19亿参数，在九项下游任务中显著优于现有方法和模型。

Conclusion: Uni - NTFM能有效学习大脑活动的通用表示，具有优越性能。

Abstract: Foundation models pretrained on various and unlabeled data have demonstrated
significant success in natural language and vision, but their application to
electroencephalography (EEG) remains challenged due to the signal's unique
properties. Existing brain foundation models that inherit architectures
designed for text or images lead to three limitations in pre-training: 1)
conflating time-domain waveform patterns with frequency-domain rhythmic
features in a single processing stream, 2) ignoring the critical spatial
topology of electrodes with different standards, and 3) reliance on the
inflexible, dense network to process functionally distinct EEG patterns. To
address these challenges, we introduce the Unified Neural Topological
Foundation Model (Uni-NTFM), which is designed based on neuroscience principles
to produce universal and interpretable representations. Uni-NTFM integrates
three core innovations: 1) a decoupled architecture parallelly encodes time,
frequency, and raw signal representations before performing cross-domain
feature integration; 2) a topological embedding mechanism to unify electrodes
from different international standards and generate structured input sequences
for brain regions; and 3) a Mixture-of-Experts neural Transformer that
efficiently scales model capacity by routing signal patterns to specialized
subnetworks. The largest model, Uni-NTFM$_{large}$, has a record-breaking 1.9B
parameters and was pretrained on over 28,000 hours of diverse EEG data via a
dual-domain masked reconstruction objective. Uni-NTFM significantly outperforms
existing task-specific methods and foundation models across nine distinct
downstream tasks under both linear probing and fine-tuning settings,
demonstrating a superior ability to learn universal representations of brain
activity.

</details>


### [644] [RDD: Pareto Analysis of the Rate-Distortion-Distinguishability Trade-off](https://arxiv.org/abs/2509.24805)
*Andriy Enttsel,Alex Marchioni,Andrea Zanellini,Mauro Mangia,Gianluca Setti,Riccardo Rovatti*

Main category: eess.SP

TL;DR: 文章扩展信息论框架处理压缩有效性、失真量和信号可区分性的权衡。


<details>
  <summary>Details</summary>
Motivation: 广泛监测系统的数据压缩可能因信息丢失影响异常检测，需解决压缩有效性、失真量和信号可区分性之间的权衡问题。

Method: 扩展文献[1]的信息论框架，利用高斯假设绘制曲线。

Result: 表明在帕累托面上操作比单纯依靠最优率失真压缩更能处理好权衡。

Conclusion: 通过扩展信息论框架可更好处理系统有效性依赖的三个特征间的权衡。

Abstract: Extensive monitoring systems generate data that is usually compressed for
network transmission. This compressed data might then be processed in the cloud
for tasks such as anomaly detection. However, compression can potentially
impair the detector's ability to distinguish between regular and irregular
patterns due to information loss. Here we extend the information-theoretic
framework introduced in [1] to simultaneously address the trade-off between the
three features on which the effectiveness of the system depends: the
effectiveness of compression, the amount of distortion it introduces, and the
distinguishability between compressed normal signals and compressed anomalous
signals. We leverage a Gaussian assumption to draw curves showing how moving on
a Pareto surface helps administer such a trade-off better than simply relying
on optimal rate-distortion compression and hoping that compressed signals can
be distinguished from each other.

</details>


### [645] [Intelligent Optimization of Wireless Access Point Deployment for Communication-Based Train Control Systems Using Deep Reinforcement Learning](https://arxiv.org/abs/2509.24819)
*Kunyu Wu,Qiushi Zhao,Zihan Feng,Yunxi Mu,Hao Qin,Xinyu Zhang,Xingqi Zhang*

Main category: eess.SP

TL;DR: 本文提出基于深度强化学习的框架优化城市铁路隧道中接入点部署，对比实验显示其优于传统方法，为下一代CBTC系统提供可扩展且数据高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统接入点部署优化方法存在测量要求高、解非最优以及机器学习方法难适应复杂隧道环境的问题，需要新方法解决。

Method: 提出集成抛物波方程（PWE）信道建模、基于条件生成对抗网络（cGAN）的数据增强和决斗深度Q网络（Dueling DQN）的深度强化学习框架进行接入点部署优化。

Result: 对比实验表明，该方法优于传统Hooke Jeeves优化器和传统DQN，能提供平均接收功率更高、最坏情况覆盖更好且计算效率更高的接入点配置。

Conclusion: 该工作集成高保真电磁仿真、生成式建模和人工智能驱动优化，为复杂隧道环境中的下一代CBTC系统提供了可扩展且数据高效的解决方案。

Abstract: Urban railway systems increasingly rely on communication based train control
(CBTC) systems, where optimal deployment of access points (APs) in tunnels is
critical for robust wireless coverage. Traditional methods, such as empirical
model-based optimization algorithms, are hindered by excessive measurement
requirements and suboptimal solutions, while machine learning (ML) approaches
often struggle with complex tunnel environments. This paper proposes a deep
reinforcement learning (DRL) driven framework that integrates parabolic wave
equation (PWE) channel modeling, conditional generative adversarial network
(cGAN) based data augmentation, and a dueling deep Q network (Dueling DQN) for
AP placement optimization. The PWE method generates high-fidelity path loss
distributions for a subset of AP positions, which are then expanded by the cGAN
to create high resolution path loss maps for all candidate positions,
significantly reducing simulation costs while maintaining physical accuracy. In
the DRL framework, the state space captures AP positions and coverage, the
action space defines AP adjustments, and the reward function encourages signal
improvement while penalizing deployment costs. The dueling DQN enhances
convergence speed and exploration exploitation balance, increasing the
likelihood of reaching optimal configurations. Comparative experiments show
that the proposed method outperforms a conventional Hooke Jeeves optimizer and
traditional DQN, delivering AP configurations with higher average received
power, better worst-case coverage, and improved computational efficiency. This
work integrates high-fidelity electromagnetic simulation, generative modeling,
and AI-driven optimization, offering a scalable and data-efficient solution for
next-generation CBTC systems in complex tunnel environments.

</details>


### [646] [Benchmarking ECG Foundational Models: A Reality Check Across Clinical Tasks](https://arxiv.org/abs/2509.25095)
*M A Al-Masud,Juan Miguel Lopez Alcaraz,Nils Strodthoff*

Main category: eess.SP

TL;DR: 本文对8个心电图基础模型在26个临床相关任务上进行基准测试，发现其在成人心电图分析有潜力，但在其他方面有差距，ECG - CPC表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前心电图机器学习碎片化，基础模型在不同心电图任务上的泛化性不明，需要研究其表现。

Method: 在26个临床相关任务上对8个心电图基础模型进行基准测试，使用12个公共数据集，在微调与冻结设置下评估模型，并进行跨数据集规模的缩放分析。

Result: 模型在不同领域表现不同，在成人心电图解释领域三个基础模型优于监督基线；ECG - CPC在多数基础模型失败的类别中占优；模型随数据集大小有不同缩放行为。

Conclusion: 基础模型在成人心电图分析有前景，但在心脏结构、结果预测和患者特征化方面有差距，ECG - CPC为推进心电图基础模型带来新机遇。

Abstract: The 12-lead electrocardiogram (ECG) is a long-standing diagnostic tool. Yet
machine learning for ECG interpretation remains fragmented, often limited to
narrow tasks or datasets. Foundation models promise broader adaptability, but
their generalization across diverse ECG tasks is not well understood. We
benchmarked eight ECG foundation models on 26 clinically relevant tasks using
12 public datasets comprising 1,650 regression and classification targets.
Models were evaluated under fine-tuning and frozen settings, with scaling
analyses across dataset sizes. Results show heterogeneous performance across
domains: in the most widely studied domain, adult ECG interpretation, three
foundation models consistently outperformed strong supervised baselines. In
contrast, ECG-CPC, a compact structured state-space model pretrained on HEEDB,
dominated other categories where most foundation models failed to surpass
supervised learning. Foundation models also displayed distinct scaling
behaviors with dataset size, which are critical for small-scale clinical
applications. Overall, while foundation models show promise for adult ECG
analysis, substantial gaps remain in cardiac structure, outcome prediction, and
patient characterization. Notably, ECG-CPC's strong performance despite being
orders of magnitude smaller and consuming minimal computational resources
highlights untapped opportunities for advancing ECG foundation models.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [647] [A Computational Perspective on NeuroAI and Synthetic Biological Intelligence](https://arxiv.org/abs/2509.23896)
*Dhruvik Patel,Md Sayed Tanveer,Jesus Gonzalez-Ferrer,Alon Loeffler,Brett J. Kagan,Mohammed A. Mostajo-Radji,Ge Wang*

Main category: q-bio.NC

TL;DR: 本文对NeuroAI领域进行综述，将其分为三个领域，介绍计算框架和最新进展，指向生物神经组织与数字算法交互的新系统。


<details>
  <summary>Details</summary>
Motivation: NeuroAI是新兴领域，SBI作为其中核心领域有重要意义，需要梳理该领域情况。

Method: 将NeuroAI领域划分为硬件、软件和湿件三个相互作用的领域，介绍相关计算框架并突出近期进展。

Result: 展示了计算框架的整合情况，突出了器官oid智能、神经形态计算和神经符号学习的进展。

Conclusion: 这些发展指向一类通过生物神经组织和数字算法交互进行计算的新系统。

Abstract: NeuroAI is an emerging field at the intersection of neuroscience and
artificial intelligence, where insights from brain function guide the design of
intelligent systems. A central area within this field is synthetic biological
intelligence (SBI), which combines the adaptive learning properties of
biological neural networks with engineered hardware and software. SBI systems
provide a platform for modeling neural computation, developing biohybrid
architectures, and enabling new forms of embodied intelligence. In this review,
we organize the NeuroAI landscape into three interacting domains: hardware,
software, and wetware. We outline computational frameworks that integrate
biological and non-biological systems and highlight recent advances in organoid
intelligence, neuromorphic computing, and neuro-symbolic learning. These
developments collectively point toward a new class of systems that compute
through interactions between living neural tissue and digital algorithms.

</details>


### [648] [Targeted perturbations reveal brain-like local coding axes in robustified, but not standard, ANN-based brain models](https://arxiv.org/abs/2509.23333)
*Nikolas McNeal,N. Apurva Ratan Murty*

Main category: q-bio.NC

TL;DR: 用小尺度对抗探针表征ANN脑模型局部表征几何，发现当代模型脆弱，对抗探针敏感性更能区分模型等，表明局部表征几何是更好的脑模型评估标准。


<details>
  <summary>Details</summary>
Motivation: 现有许多ANN模型预测准确率相近，需要更强的脑模型评估标准。

Method: 使用小尺度对抗探针来表征多个高预测性ANN脑模型的局部表征几何。

Result: 1. 当代ANN脑模型意外脆弱，预测对小扰动敏感；2. 模型对对抗探针的敏感性比预测准确率更能区分候选神经编码模型；3. 标准模型依赖不同的局部编码方向，不跨架构转移；4. 鲁棒模型的对抗探针有可泛化和语义有意义的变化。

Conclusion: 局部表征几何为脑模型评估提供更强标准，应青睐鲁棒模型。

Abstract: Artificial neural networks (ANNs) have become the de facto standard for
modeling the human visual system, primarily due to their success in predicting
neural responses. However, with many models now achieving similar predictive
accuracy, we need a stronger criterion. Here, we use small-scale adversarial
probes to characterize the local representational geometry of many highly
predictive ANN-based brain models. We report four key findings. First, we show
that most contemporary ANN-based brain models are unexpectedly fragile. Despite
high prediction scores, their response predictions are highly sensitive to
small, imperceptible perturbations, revealing unreliable local coding
directions. Second, we demonstrate that a model's sensitivity to adversarial
probes can better discriminate between candidate neural encoding models than
prediction accuracy alone. Third, we find that standard models rely on distinct
local coding directions that do not transfer across model architectures.
Finally, we show that adversarial probes from robustified models produce
generalizable and semantically meaningful changes, suggesting that they capture
the local coding dimensions of the visual system. Together, our work shows that
local representational geometry provides a stronger criterion for brain model
evaluation. We also provide empirical grounds for favoring robust models, whose
more stable coding axes not only align better with neural selectivity but also
generate concrete, testable predictions for future experiments.

</details>


### [649] [End-to-end Topographic Auditory Models Replicate Signatures of Human Auditory Cortex](https://arxiv.org/abs/2509.24039)
*Haider Al-Tahan,Mayukh Deb,Jenelle Feather,N. Apurva Ratan Murty*

Main category: q-bio.NC

TL;DR: 当前听觉感知计算模型评估未考量地形结构，研究提出TopoAudio模型，在基准任务上表现佳，能预测fMRI响应并展现听觉皮层地形结构，强调布线长度约束可作通用正则化工具。


<details>
  <summary>Details</summary>
Motivation: 当前听觉感知计算模型评估未衡量地形结构是否存在，希望模型能展现听觉皮层的地形组织。

Method: 改编用于视觉感知的皮层布线约束损失，训练TopoAudio模型分类语音和环境声音，添加二维皮层上相邻单元调谐相似的约束。

Result: TopoAudio在基准任务上准确率高，预测fMRI响应与标准模型相当，能发展出平滑的地形图谱和聚类响应模块。

Conclusion: 布线长度约束可作为通用正则化工具，实现生物对齐表征。

Abstract: The human auditory cortex is topographically organized. Neurons with similar
response properties are spatially clustered, forming smooth maps for acoustic
features such as frequency in early auditory areas, and modular regions
selective for music and speech in higher-order cortex. Yet, evaluations for
current computational models of auditory perception do not measure whether such
topographic structure is present in a candidate model. Here, we show that
cortical topography is not present in the previous best-performing models at
predicting human auditory fMRI responses. To encourage the emergence of
topographic organization, we adapt a cortical wiring-constraint loss originally
designed for visual perception. The new class of topographic auditory models,
TopoAudio, are trained to classify speech, and environmental sounds from
cochleagram inputs, with an added constraint that nearby units on a 2D cortical
sheet develop similar tuning. Despite these additional constraints, TopoAudio
achieves high accuracy on benchmark tasks comparable to the unconstrained
non-topographic baseline models. Further, TopoAudio predicts the fMRI responses
in the brain as well as standard models, but unlike standard models, TopoAudio
develops smooth, topographic maps for tonotopy and amplitude modulation (common
properties of early auditory representation, as well as clustered response
modules for music and speech (higher-order selectivity observed in the human
auditory cortex). TopoAudio is the first end-to-end biologically grounded
auditory model to exhibit emergent topography, and our results emphasize that a
wiring-length constraint can serve as a general-purpose regularization tool to
achieve biologically aligned representations.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [650] [CrediBench: Building Web-Scale Network Datasets for Information Integrity](https://arxiv.org/abs/2509.23340)
*Emma Kondrup,Sebastian Sabry,Hussein Abdallah,Zachary Yang,James Zhou,Kellin Pelrine,Jean-François Godbout,Michael M. Bronstein,Reihaneh Rabbany,Shenyang Huang*

Main category: cs.SI

TL;DR: 针对在线错误信息检测问题，提出CrediBench数据处理管道构建时间网络图谱，实验证明结构和内容信号对学习可信度得分有效。


<details>
  <summary>Details</summary>
Motivation: 在线错误信息威胁增大，现有检测方法未充分利用网站内容和超链接关系的动态交互。

Method: 引入CrediBench数据处理管道，构建联合建模文本内容和超链接结构的时间网络图谱。

Result: 处理2024年12月Common Crawl档案的一个月快照，得到含4500万个节点和10亿条边的网络图谱，是目前公开的最大错误信息研究网络图谱数据集。

Conclusion: 结构和网页内容信号对学习可信度得分有效，且管道、实验代码和数据集均已公开。

Abstract: Online misinformation poses an escalating threat, amplified by the Internet's
open nature and increasingly capable LLMs that generate persuasive yet
deceptive content. Existing misinformation detection methods typically focus on
either textual content or network structure in isolation, failing to leverage
the rich, dynamic interplay between website content and hyperlink relationships
that characterizes real-world misinformation ecosystems. We introduce
CrediBench: a large-scale data processing pipeline for constructing temporal
web graphs that jointly model textual content and hyperlink structure for
misinformation detection. Unlike prior work, our approach captures the dynamic
evolution of general misinformation domains, including changes in both content
and inter-site references over time. Our processed one-month snapshot extracted
from the Common Crawl archive in December 2024 contains 45 million nodes and 1
billion edges, representing the largest web graph dataset made publicly
available for misinformation research to date. From our experiments on this
graph snapshot, we demonstrate the strength of both structural and webpage
content signals for learning credibility scores, which measure source
reliability. The pipeline and experimentation code are all available here, and
the dataset is in this folder.

</details>


### [651] [Community detection robustness of graph neural networks](https://arxiv.org/abs/2509.24662)
*Jaidev Goel,Pablo Moriano,Ramakrishnan Kannan,Yulia R. Gel*

Main category: cs.SI

TL;DR: 本文对六种常用GNN架构进行系统计算评估，分析其在社区检测任务中面对不同扰动的鲁棒性，发现有监督GNN基线准确率高，无监督方法DMoN鲁棒性强，社区强度影响鲁棒性，节点属性扰动影响大。


<details>
  <summary>Details</summary>
Motivation: 当前对GNN在社区检测任务中面对不同扰动和攻击的鲁棒性缺乏理解，需探究其敏感性背后的潜在机制。

Method: 对GCN、GAT等六种GNN架构进行系统计算评估，分析节点属性操作、边拓扑扭曲和对抗攻击三类扰动，使用元素中心相似度作为评估指标。

Result: 有监督GNN基线准确率高，无监督方法DMoN在针对性和对抗性扰动下鲁棒性强，社区强度影响鲁棒性，节点属性扰动与针对性边删除等会导致社区恢复性能大幅下降。

Conclusion: 指出基于GNN的社区检测在准确性和鲁棒性之间存在权衡，为选择抗噪声和对抗攻击的架构提供新见解。

Abstract: Graph neural networks (GNNs) are increasingly widely used for community
detection in attributed networks. They combine structural topology with node
attributes through message passing and pooling. However, their robustness or
lack of thereof with respect to different perturbations and targeted attacks in
conjunction with community detection tasks is not well understood. To shed
light into latent mechanisms behind GNN sensitivity on community detection
tasks, we conduct a systematic computational evaluation of six widely adopted
GNN architectures: GCN, GAT, Graph-SAGE, DiffPool, MinCUT, and DMoN. The
analysis covers three perturbation categories: node attribute manipulations,
edge topology distortions, and adversarial attacks. We use element-centric
similarity as the evaluation metric on synthetic benchmarks and real-world
citation networks. Our findings indicate that supervised GNNs tend to achieve
higher baseline accuracy, while unsupervised methods, particularly DMoN,
maintain stronger resilience under targeted and adversarial perturbations.
Furthermore, robustness appears to be strongly influenced by community
strength, with well-defined communities reducing performance loss. Across all
models, node attribute perturbations associated with targeted edge deletions
and shift in attribute distributions tend to cause the largest degradation in
community recovery. These findings highlight important trade-offs between
accuracy and robustness in GNN-based community detection and offer new insights
into selecting architectures resilient to noise and adversarial attacks.

</details>


### [652] [Hybrid Graph Embeddings and Louvain Algorithm for Unsupervised Community Detection](https://arxiv.org/abs/2509.23411)
*Dalila Khettaf,Djamel Djenouri,Zeinab Rezaeifar,Youcef Djenouri*

Main category: cs.SI

TL;DR: 本文提出结合Louvain算法与图神经网络（GNN）的社区检测方法，无需先验知识，通过实验验证其效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有的社区检测方法大多需要社区数量的先验知识，本文旨在提出无需先验知识的社区检测方法。

Method: 将Louvain算法与GNN结合，用GNN生成的节点嵌入增强Louvain算法，引入合并算法优化结果。

Result: 在真实数据集上的评估证实，该方法能动态调整检测到的社区数量，提高检测准确率。

Conclusion: 该方法是首次使用GNN改进Louvain算法进行社区检测，相比基准解决方案效果更好。

Abstract: This paper proposes a novel community detection method that integrates the
Louvain algorithm with Graph Neural Networks (GNNs), enabling the discovery of
communities without prior knowledge. Compared to most existing solutions, the
proposed method does not require prior knowledge of the number of communities.
It enhances the Louvain algorithm using node embeddings generated by a GNN to
capture richer structural and feature information. Furthermore, it introduces a
merging algorithm to refine the results of the enhanced Louvain algorithm,
reducing the number of detected communities. To the best of our knowledge, this
work is the first one that improves the Louvain algorithm using GNNs for
community detection. The improvement of the proposed method was empirically
confirmed through an evaluation on real-world datasets. The results demonstrate
its ability to dynamically adjust the number of detected communities and
increase the detection accuracy in comparison with the benchmark solutions.

</details>


### [653] [Node Classification via Simplicial Interaction with Augmented Maximal Clique Selection](https://arxiv.org/abs/2509.23568)
*Eunho Koo,Tongseok Lim*

Main category: cs.SI

TL;DR: 提出增强最大团策略处理高阶网络学习，在合成网络和真实数据集上表现出色，结合GNN提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 利用所有团处理高阶交互计算效率低，仅用最大团会导致训练数据不平衡。

Method: 提出增强最大团策略，选择性纳入非最大团以平衡学习。

Result: 在合成网络和真实世界引用数据集上，该方法优于基于成对交互、所有团或仅最大团的方法，结合GNN提升预测准确性。

Conclusion: 增强最大团策略为高阶网络学习提供了计算高效且有效的解决方案。

Abstract: Considering higher-order interactions allows for a more comprehensive
understanding of network structures beyond simple pairwise connections. While
leveraging all cliques in a network to handle higher-order interactions is
intuitive, it often leads to computational inefficiencies due to overlapping
information between higher-order and lower-order cliques. To address this
issue, we propose an augmented maximal clique strategy. Although using only
maximal cliques can reduce unnecessary overlap and provide a concise
representation of the network, certain nodes may still appear in multiple
maximal cliques, resulting in imbalanced training data. Therefore, our
augmented maximal clique approach selectively includes some non-maximal cliques
to mitigate the overrepresentation of specific nodes and promote more balanced
learning across the network. Comparative analyses on synthetic networks and
real-world citation datasets demonstrate that our method outperforms approaches
based on pairwise interactions, all cliques, or only maximal cliques. Finally,
by integrating this strategy into GNN-based semi-supervised learning, we
establish a link between maximal clique-based methods and GNNs, showing that
incorporating higher-order structures improves predictive accuracy. As a
result, the augmented maximal clique strategy offers a computationally
efficient and effective solution for higher-order network learning.

</details>


### [654] [Data-Driven Discrete Geofence Design Using Binary Quadratic Programming](https://arxiv.org/abs/2509.24679)
*Keisuke Otaki,Akihisa Okada,Tadayoshi Matsumori,Hiroaki Yoshida*

Main category: cs.SI

TL;DR: 本文针对传统圆形地理围栏灵活性不足的问题，将现有圆形地理围栏优化问题转化为0 - 1整数规划问题，提出新建模方法实现灵活地理围栏设计。


<details>
  <summary>Details</summary>
Motivation: 传统地理围栏手工制作，现有圆形地理围栏灵活性不足，在城市和高分辨率区域存在重叠、无法与边界对齐等问题。

Method: 将现有圆形地理围栏优化问题转化为0 - 1整数规划问题，再转化为二次（无约束）二进制优化问题，开发并比较不同提取离散地理围栏的公式化方法。

Result: 开发并比较了不同的公式化方法来提取离散地理围栏。

Conclusion: 新的建模方法能实现灵活的地理围栏设计。

Abstract: Geofences have attracted significant attention in the design of spatial and
virtual regions for managing and engaging spatiotemporal events. By using
geofences to monitor human activity across their boundaries, content providers
can create spatially triggered events that include notifications about points
of interest within a geofence by pushing spatial information to the devices of
users. Traditionally, geofences were hand-crafted by providers. In addition to
the hand-crafted approach, recent advances in collecting human mobility data
through mobile devices can accelerate the automatic and data-driven design of
geofences, also known as the geofence design problem. Previous approaches
assume circular shapes; thus, their flexibility is insufficient, and they can
only handle geofence-based applications for large areas with coarse
resolutions. A challenge with using circular geofences in urban and
high-resolution areas is that they often overlap and fail to align with
political district boundaries and road segments, such as one-way streets and
median barriers. In this study, we address the problem of extracting arbitrary
shapes as geofences from human mobility data to mitigate this problem. In our
formulation, we cast the existing optimization problems for circular geofences
to 0-1 integer programming problems to represent arbitrary shapes. Although 0-1
integer programming problems are computationally hard, formulating them as
quadratic (unconstrained) binary optimization problems enables efficient
approximation of optimal solutions, because this allows the use of specialized
quadratic solvers, such as the quantum annealing, and other state-of-the-art
algorithms. We then develop and compare different formulation methods to
extract discrete geofences. We confirmed that our new modeling approach enables
flexible geofence design.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [655] [Contrastive Learning Enhances Language Model Based Cell Embeddings for Low-Sample Single Cell Transcriptomics](https://arxiv.org/abs/2509.23543)
*Luxuan Zhang,Douglas Jiang,Qinglong Wang,Haoqi Sun,Feng Tian*

Main category: q-bio.GN

TL;DR: 提出将单细胞RNA测序与大语言模型集成的计算框架，用于单细胞生物学分析，在视网膜神经节细胞上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生物医学数据分析应用尚处起步阶段，单细胞转录组分析中稀有亚型给缩放定律带来挑战，需新方法。

Method: 将每个细胞高表达基因映射到NCBI基因描述，用text - embedding - ada - 002、BioBERT和SciBERT等模型嵌入。

Result: 应用于视网膜神经节细胞，改善了亚型分类，突出生物显著特征，揭示选择性神经元脆弱性潜在途径。

Conclusion: 表明大语言模型衍生嵌入可在数据有限条件下增强生物学分析，为单细胞生物学未来基础模型奠定基础。

Abstract: Large language models (LLMs) have shown strong ability in generating rich
representations across domains such as natural language processing and
generation, computer vision, and multimodal learning. However, their
application in biomedical data analysis remains nascent. Single-cell
transcriptomic profiling is essential for dissecting cell subtype diversity in
development and disease, but rare subtypes pose challenges for scaling laws. We
present a computational framework that integrates single-cell RNA sequencing
(scRNA-seq) with LLMs to derive knowledge-informed gene embeddings. Highly
expressed genes for each cell are mapped to NCBI Gene descriptions and embedded
using models such as text-embedding-ada-002, BioBERT, and SciBERT. Applied to
retinal ganglion cells (RGCs), which differ in vulnerability to
glaucoma-related neurodegeneration, this strategy improves subtype
classification, highlights biologically significant features, and reveals
pathways underlying selective neuronal vulnerability. More broadly, it
illustrates how LLM-derived embeddings can augment biological analysis under
data-limited conditions and lay the groundwork for future foundation models in
single-cell biology.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [656] [Local Success Does Not Compose: Benchmarking Large Language Models for Compositional Formal Verification](https://arxiv.org/abs/2509.23061)
*Xu Xu,Xin Li,Xingwei Qu,Jie Fu,Binhang Yuan*

Main category: cs.PL

TL;DR: 介绍DafnyCOMP基准测试，评估大语言模型在Dafny组合规范生成上的表现，发现模型单函数验证表现好但组合任务表现差，该基准可衡量LLM代码生成进展。


<details>
  <summary>Details</summary>
Motivation: 现有基准多关注单函数任务，缺乏对多函数交互且有数据依赖程序的评估，需新基准评估大语言模型在组合规范生成上的能力。

Method: 创建包含300个自动合成多函数程序的DafnyCOMP基准测试，评估多个最先进的大语言模型家族。

Result: 大语言模型在单函数验证中表现良好，但在组合任务中性能急剧下降，存在跨功能推理的系统性失败。

Conclusion: DafnyCOMP可作为诊断工具，衡量大语言模型在可靠、可验证和组合代码生成方面的进展。

Abstract: We introduce DafnyCOMP, a benchmark for evaluating large language models
(LLMs) on compositional specification generation in Dafny. Unlike prior
benchmarks that focus on single-function tasks, DafnyCOMP targets programs
composed of multiple interacting functions with data dependencies, requiring
reasoning across component boundaries. The benchmark consists of 300
automatically synthesized multi-function programs. We evaluate several
state-of-the-art LLM families and find that, while they perform well on
single-function verification, their performance drops sharply on compositional
tasks. Analysis reveals systematic failures in cross-functional reasoning,
including fragile specifications, misalignment between implementations and
proofs, and unstable reasoning. DafnyCOMP thus provides a diagnostic tool for
measuring progress toward reliable, verifiable, and compositional code
generation with LLMs.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [657] [Interpreting deep learning-based stellar mass estimation via causal analysis and mutual information decomposition](https://arxiv.org/abs/2509.23901)
*Wei Zhang,Qiufan Lin,Yuan-Sen Ting,Shupei Chen,Hengxin Ruan,Song Li,Yifan Wang*

Main category: astro-ph.IM

TL;DR: 本文使用因果分析和互信息分解两种可解释性技术，对基于深度学习的星系恒星质量估计进行解释，获得有意义结果，展示结合深度学习与可解释性技术的优势。


<details>
  <summary>Details</summary>
Motivation: 端到端深度学习模型缺乏可解释性，难以理解额外信息对星系物理性质估计的贡献，改进该领域理解有助于揭示星系性质间物理联系和优化数据利用。

Method: 采用因果分析和互信息分解两种可解释性技术，使用SDSS和WISE的数据进行研究。

Result: 获得有意义结果，为基于图像的模型提供物理解释。

Conclusion: 结合深度学习与可解释性技术有增益，有望促进更多数据驱动的天体物理研究。

Abstract: End-to-end deep learning models fed with multi-band galaxy images are
powerful data-driven tools used to estimate galaxy physical properties in the
absence of spectroscopy. However, due to a lack of interpretability and the
associational nature of such models, it is difficult to understand how the
information additional to integrated photometry (e.g., morphology) contributes
to the estimation task. Improving our understanding in this field would enable
further advances into unraveling the physical connections among galaxy
properties and optimizing data exploitation. Therefore, our work is aimed at
interpreting the deep learning-based estimation of stellar mass via two
interpretability techniques: causal analysis and mutual information
decomposition. The former reveals the causal paths between multiple variables
beyond nondirectional statistical associations, while the latter quantifies the
multicomponent contributions (i.e., redundant, unique, and synergistic) of
different input data to the stellar mass estimation. Using data from the Sloan
Digital Sky Survey (SDSS) and the Wide-field Infrared Survey Explorer (WISE),
we obtained meaningful results that provide physical interpretations for
image-based models. Our work demonstrates the gains from combining deep
learning with interpretability techniques, and holds promise in promoting more
data-driven astrophysical research (e.g., astrophysical parameter estimations
and investigations on complex multivariate physical processes).

</details>


### [658] [ASTROCO: Self-Supervised Conformer-Style Transformers for Light-Curve Embeddings](https://arxiv.org/abs/2509.24134)
*Antony Tan,Pavlos Protopapas,Martina Cádiz-Leyton,Guillermo Cabrera-Vives,Cristobal Donoso-Oliva,Ignacio Becker*

Main category: astro-ph.IM

TL;DR: 提出用于不规则恒星光变曲线的Conformer风格编码器AstroCo，性能优于Astromer v1和v2，在少样本分类中表现良好。


<details>
  <summary>Details</summary>
Motivation: 为不规则恒星光变曲线寻找更有效的编码器。

Method: 结合注意力机制、深度卷积和门控构建AstroCo。

Result: 在MACHO R波段，AstroCo比Astromer v1和v2误差分别降低70%和61%，相对宏观F1得分提高约7%，嵌入可有效用于少样本分类。

Conclusion: AstroCo有潜力成为时域天文学强大且标签高效的基础模型。

Abstract: We present AstroCo, a Conformer-style encoder for irregular stellar light
curves. By combining attention with depthwise convolutions and gating, AstroCo
captures both global dependencies and local features. On MACHO R-band, AstroCo
outperforms Astromer v1 and v2, yielding 70 percent and 61 percent lower error
respectively and a relative macro-F1 gain of about 7 percent, while producing
embeddings that transfer effectively to few-shot classification. These results
highlight AstroCo's potential as a strong and label-efficient foundation for
time-domain astronomy.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [659] [Consistency Models as Plug-and-Play Priors for Inverse Problems](https://arxiv.org/abs/2509.22736)
*Merve Gülle,Junno Yun,Yaşar Utku Alçalar,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: 本文将一致性模型（CMs）重新解释为先验的近端算子并集成到即插即用（PnP）框架，提出PnP - CM求解器，在多种逆问题上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于CM的逆问题求解器需额外特定任务训练或收敛慢，不适合大规模问题，因此需改进。

Method: 将CMs重新解释为近端算子，集成到PnP框架，基于PnP - ADMM提出求解器，并用噪声注入和动量加速，即PnP - CM。

Result: PnP - CM在多种逆问题上表现良好，如仅4次函数评估（NFEs）就能实现高质量重建，2步可产生有意义结果。

Conclusion: PnP - CM在实际逆问题中有效，优于可比的基于CM的方法。

Abstract: Diffusion models have found extensive use in solving numerous inverse
problems. Such diffusion inverse problem solvers aim to sample from the
posterior distribution of data given the measurements, using a combination of
the unconditional score function and an approximation of the posterior related
to the forward process. Recently, consistency models (CMs) have been proposed
to directly predict the final output from any point on the diffusion ODE
trajectory, enabling high-quality sampling in just a few NFEs. CMs have also
been utilized for inverse problems, but existing CM-based solvers either
require additional task-specific training or utilize data fidelity operations
with slow convergence, not amenable to large-scale problems. In this work, we
reinterpret CMs as proximal operators of a prior, enabling their integration
into plug-and-play (PnP) frameworks. We propose a solver based on PnP-ADMM,
which enables us to leverage the fast convergence of conjugate gradient method.
We further accelerate this with noise injection and momentum, dubbed PnP-CM,
and show it maintains the convergence properties of the baseline PnP-ADMM. We
evaluate our approach on a variety of inverse problems, including inpainting,
super-resolution, Gaussian deblurring, and magnetic resonance imaging (MRI)
reconstruction. To the best of our knowledge, this is the first CM trained for
MRI datasets. Our results show that PnP-CM achieves high-quality
reconstructions in as few as 4 NFEs, and can produce meaningful results in 2
steps, highlighting its effectiveness in real-world inverse problems while
outperforming comparable CM-based approaches.

</details>


### [660] [S$^3$F-Net: A Multi-Modal Approach to Medical Image Classification via Spatial-Spectral Summarizer Fusion Network](https://arxiv.org/abs/2509.23442)
*Md. Saiful Bari Siddiqui,Mohammed Imamul Hassan Bhuiyan*

Main category: eess.IV

TL;DR: 本文提出S$^3$F - Net解决传统CNN在医学图像分析中不足，在多数据集验证有效，证明双域方法是强大且通用范式。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络专注单域，难以捕捉全局模式和显式建模频域特征，需改进医学图像分析方法。

Method: 提出S$^3$F - Net双分支框架，融合深度空间CNN和浅光谱编码器SpectraNet，SpectraNet含SpectralFilter层，通过元素相乘应用可学习滤波器到傅里叶频谱。

Result: 在四个医学成像数据集上表现优于仅空间基线，最高提升5.13%准确率，在不同数据集用不同融合方法取得高准确率，可解释性分析表明能根据输入病理动态调整分支依赖。

Conclusion: 双域方法是医学图像分析强大且通用的范式。

Abstract: Convolutional Neural Networks have become a cornerstone of medical image
analysis due to their proficiency in learning hierarchical spatial features.
However, this focus on a single domain is inefficient at capturing global,
holistic patterns and fails to explicitly model an image's frequency-domain
characteristics. To address these challenges, we propose the Spatial-Spectral
Summarizer Fusion Network (S$^3$F-Net), a dual-branch framework that learns
from both spatial and spectral representations simultaneously. The S$^3$F-Net
performs a fusion of a deep spatial CNN with our proposed shallow spectral
encoder, SpectraNet. SpectraNet features the proposed SpectralFilter layer,
which leverages the Convolution Theorem by applying a bank of learnable filters
directly to an image's full Fourier spectrum via a computation-efficient
element-wise multiplication. This allows the SpectralFilter layer to attain a
global receptive field instantaneously, with its output being distilled by a
lightweight summarizer network. We evaluate S$^3$F-Net across four medical
imaging datasets spanning different modalities to validate its efficacy and
generalizability. Our framework consistently and significantly outperforms its
strong spatial-only baseline in all cases, with accuracy improvements of up to
5.13%. With a powerful Bilinear Fusion, S$^3$F-Net achieves a SOTA competitive
accuracy of 98.76% on the BRISC2025 dataset. Concatenation Fusion performs
better on the texture-dominant Chest X-Ray Pneumonia dataset, achieving 93.11%
accuracy, surpassing many top-performing, much deeper models. Our
explainability analysis also reveals that the S$^3$F-Net learns to dynamically
adjust its reliance on each branch based on the input pathology. These results
verify that our dual-domain approach is a powerful and generalizable paradigm
for medical image analysis.

</details>


### [661] [Non-Invasive Detection of PROState Cancer with Novel Time-Dependent Diffusion MRI and AI-Enhanced Quantitative Radiological Interpretation: PROS-TD-AI](https://arxiv.org/abs/2509.24227)
*Baltasar Ramos,Cristian Garrido,Paulette Narv'aez,Santiago Gelerstein Claro,Haotian Li,Rafael Salvador,Constanza V'asquez-Venegas,Iv'an Gallegos,Yi Zhang,V'ictor Casta~neda,Cristian Acevedo,Dan Wu,Gonzalo C'ardenas,Camilo G. Sotomayor*

Main category: eess.IV

TL;DR: 研究计划概述了对自制AI增强TDD - MRI软件（PROSTDAI）在常规诊断护理中的评估，对比PI - RADS v2.1并以MRI引导前列腺活检验证结果。


<details>
  <summary>Details</summary>
Motivation: 当前mpMRI在前列腺癌诊断中有假阳性、假阴性和观察者间一致性问题，TDD - MRI有潜力，结合机器学习或可改善诊断。

Method: 对自制AI增强TDD - MRI软件（PROSTDAI）进行前瞻性评估，与PI - RADS v2.1对比，并以MRI引导前列腺活检验证结果。

Result: 未提及。

Conclusion: 未提及。

Abstract: Prostate cancer (PCa) is the most frequently diagnosed malignancy in men and
the eighth leading cause of cancer death worldwide. Multiparametric MRI (mpMRI)
has become central to the diagnostic pathway for men at intermediate risk,
improving de-tection of clinically significant PCa (csPCa) while reducing
unnecessary biopsies and over-diagnosis. However, mpMRI remains limited by
false positives, false negatives, and moderate to substantial interobserver
agreement. Time-dependent diffusion (TDD) MRI, a novel sequence that enables
tissue microstructure characterization, has shown encouraging preclinical
performance in distinguishing clinically significant from insignificant PCa.
Combining TDD-derived metrics with machine learning may provide robust,
zone-specific risk prediction with less dependence on reader training and
improved accuracy compared to current standard-of-care. This study protocol
out-lines the rationale and describes the prospective evaluation of a
home-developed AI-enhanced TDD-MRI software (PROSTDAI) in routine diagnostic
care, assessing its added value against PI-RADS v2.1 and validating results
against MRI-guided prostate biopsy.

</details>


<div id='nlin.PS'></div>

# nlin.PS [[Back]](#toc)

### [662] [Multifractal features of multimodal cardiac signals: Nonlinear dynamics of exercise recovery](https://arxiv.org/abs/2509.23317)
*A. Maluckov,D. Stojanovic,M. Miletic,Lj. Hadzievski,J. Petrovic*

Main category: nlin.PS

TL;DR: 利用多模态生物信号和多尺度分析研究运动后心脏活动恢复动力学，评估五种分类算法，结果表明多尺度分析结合多模态传感可用于表征恢复状态及心脏疾病诊断。


<details>
  <summary>Details</summary>
Motivation: 研究运动后健康心脏活动的恢复动力学，寻找表征恢复状态的可靠特征及心脏疾病诊断方法。

Method: 使用多导心电图记录多模态生物信号，提取多尺度特征，用五种监督分类算法（LogReg、SVM - RBF、kNN、DT、RF）区分恢复状态。

Result: 多尺度分析结合多模态传感能得到用于表征恢复状态的可靠特征。

Conclusion: 多尺度分析结合多模态传感为心脏状况的非线性诊断方法提供了方向。

Abstract: We investigate the recovery dynamics of healthy cardiac activity after
physical exertion using multimodal biosignals recorded with a polycardiograph.
Multifractal features derived from the singularity spectrum capture the
scale-invariant properties of cardiovascular regulation. Five supervised
classification algorithms - Logistic Regression (LogReg), Suport Vector Machine
with RBF kernel (SVM-RBF), k-Nearest Neighbors (kNN), Decision Tree (DT), and
Random Forest (RF) - were evaluated to distinguish recovery states in a small,
imbalanced dataset. Our results show that multifractal analysis, combined with
multimodal sensing, yields reliable features for characterizing recovery and
points toward nonlinear diagnostic methods for heart conditions.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [663] [Large Language Models and Futures Price Factors in China](https://arxiv.org/abs/2509.23609)
*Yuhan Cheng,Heyang Zhou,Yanchu Liu*

Main category: q-fin.PR

TL;DR: 利用大语言模型为中国期货市场构建因子模型，设计单因子和多因子组合，回测显示因子表现出色。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型如GPT在构建中国期货市场因子模型方面的能力。

Method: 利用GPT构建因子模型，设计单因子和多因子组合，通过多空和仅做多策略进行样本内和样本外回测。

Result: GPT生成的因子有出色夏普比率和年化收益率，最大回撤可接受，能跑赢IPCA基准，且在鲁棒性测试中表现显著。

Conclusion: 基于GPT的因子模型在构建中国期货市场因子模型中有良好表现和应用价值。

Abstract: We leverage the capacity of large language models such as Generative
Pre-trained Transformer (GPT) in constructing factor models for Chinese futures
markets. We successfully obtain 40 factors to design single-factor and
multi-factor portfolios through long-short and long-only strategies, conducting
backtests during the in-sample and out-of-sample period. Comprehensive
empirical analysis reveals that GPT-generated factors deliver remarkable Sharpe
ratios and annualized returns while maintaining acceptable maximum drawdowns.
Notably, the GPT-based factor models also achieve significant alphas over the
IPCA benchmark. Moreover, these factors demonstrate significant performance
across extensive robustness tests, particularly excelling after the cutoff date
of GPT's training data.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [664] [Prediction-Powered Communication with Distortion Guarantees](https://arxiv.org/abs/2509.24373)
*Matteo Zecchin,Unnikrishnan Kunnath Ganesan,Giuseppe Durisi,Petar Popovski,Osvaldo Simeone*

Main category: cs.IT

TL;DR: 本文研究预测驱动通信设置，提出零延迟压缩算法，实验验证其在语义文本压缩上可降比特率并满足失真保证。


<details>
  <summary>Details</summary>
Motivation: 6G无线系统及智能设备发展促使重新思考经典香农信息论，强调语义和面向任务范式。

Method: 考虑两类失真度量，提出基于在线共形预测的零延迟压缩算法，针对擦除信道引入双重自适应共形更新。

Result: 实验表明，相比现有预测驱动压缩方法，在语义文本压缩中显著降低比特率并严格满足失真保证。

Conclusion: 所提方法在预测驱动通信设置中有效，能在满足失真约束下降低比特率。

Abstract: The development of 6G wireless systems is taking place alongside the
development of increasingly intelligent wireless devices and network nodes. The
changing technological landscape is motivating a rethinking of classical
Shannon information theory that emphasizes semantic and task-oriented
paradigms. In this paper, we study a prediction-powered communication setting,
in which devices, equipped with artificial intelligence (AI)-based predictors,
communicate under zero-delay constraints with strict distortion guarantees. Two
classes of distortion measures are considered: (i) outage-based metrics,
suitable for tasks tolerating occasional packet losses, such as real-time
control or monitoring; and (ii) bounded distortion metrics, relevant to
semantic-rich tasks like text or video transmission. We propose two zero-delay
compression algorithms leveraging online conformal prediction to provide
per-sequence guarantees on the distortion of reconstructed sequences over
error-free and packet-erasure channels with feedback. For erasure channels, we
introduce a doubly-adaptive conformal update to compensate for channel-induced
errors and derive sufficient conditions on erasure statistics to ensure
distortion constraints. Experiments on semantic text compression validate the
approach, showing significant bit rate reductions while strictly meeting
distortion guarantees compared to state-of-the-art prediction-powered
compression methods.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [665] [Learning What To Hear: Boosting Sound-Source Association For Robust Audiovisual Instance Segmentation](https://arxiv.org/abs/2509.22740)
*Jinbae Seo,Hyeongjun Kwon,Kwonyoung Kim,Jiyoung Lee,Kwanghoon Sohn*

Main category: eess.AS

TL;DR: 提出音频中心查询生成和声音感知序数计数损失，在AVISeg基准测试中提升了AVIS性能，验证关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有视听实例分割方法存在视觉偏差，一是均匀加法融合使查询无法针对不同声源专门化，二是仅视觉训练目标使查询收敛到任意显著对象。

Method: 提出使用交叉注意力的音频中心查询生成方法，使查询能选择性关注不同声源；引入声音感知序数计数损失，通过带单调一致性约束的序数回归监督发声对象数量。

Result: 在AVISeg基准测试中，mAP提升1.64，HOTA提升0.6，FSLA提升2.06。

Conclusion: 查询专门化和显式计数监督对准确的视听实例分割至关重要。

Abstract: Audiovisual instance segmentation (AVIS) requires accurately localizing and
tracking sounding objects throughout video sequences. Existing methods suffer
from visual bias stemming from two fundamental issues: uniform additive fusion
prevents queries from specializing to different sound sources, while
visual-only training objectives allow queries to converge to arbitrary salient
objects. We propose Audio-Centric Query Generation using cross-attention,
enabling each query to selectively attend to distinct sound sources and carry
sound-specific priors into visual decoding. Additionally, we introduce
Sound-Aware Ordinal Counting (SAOC) loss that explicitly supervises sounding
object numbers through ordinal regression with monotonic consistency
constraints, preventing visual-only convergence during training. Experiments on
AVISeg benchmark demonstrate consistent improvements: +1.64 mAP, +0.6 HOTA, and
+2.06 FSLA, validating that query specialization and explicit counting
supervision are crucial for accurate audiovisual instance segmentation.

</details>


### [666] [Index-MSR: A high-efficiency multimodal fusion framework for speech recognition](https://arxiv.org/abs/2509.22744)
*Jinming Chen,Lu Wang,Zheshu Song,Wei Deng*

Main category: eess.AS

TL;DR: 提出Index - MSR多模态语音识别框架，结合视频文本信息提升语音识别准确率，减少替换错误。


<details>
  <summary>Details</summary>
Motivation: 现有自动语音识别系统在特定领域术语和缺乏语义连贯性的短话语上识别性能下降，需要改进。

Method: 提出Index - MSR框架，核心是多模态融合解码器（MFD），将视频中的文本相关信息融入语音识别。

Result: 在内部字幕数据集和公共AVSR数据集上评估，Index - MSR达到最优准确率，替换错误减少20 - 50%。

Conclusion: 该方法能有效利用视频文本线索提高语音识别准确率，在音频翻译等需要严格音文同步的应用中有潜力。

Abstract: Driven by large scale datasets and LLM based architectures, automatic speech
recognition (ASR) systems have achieved remarkable improvements in accuracy.
However, challenges persist for domain-specific terminology, and short
utterances lacking semantic coherence, where recognition performance often
degrades significantly. In this work, we present Index-MSR, an efficient
multimodal speech recognition framework. At its core is a novel Multimodal
Fusion Decoder (MFD), which effectively incorporates text-related information
from videos (e.g., subtitles and presentation slides) into the speech
recognition. This cross-modal integration not only enhances overall ASR
accuracy but also yields substantial reductions in substitution errors.
Extensive evaluations on both an in-house subtitle dataset and a public AVSR
dataset demonstrate that Index-MSR achieves sota accuracy, with substitution
errors reduced by 20,50%. These results demonstrate that our approach
efficiently exploits text-related cues from video to improve speech recognition
accuracy, showing strong potential in applications requiring strict audio text
synchronization, such as audio translation.

</details>


### [667] [Unsupervised Speech Enhancement using Data-defined Priors](https://arxiv.org/abs/2509.22942)
*Dominik Klement,Matthew Maciejewski,Sanjeev Khudanpur,Jan Černocký,Lukáš Burget*

Main category: eess.AS

TL;DR: 提出用于无监督语音增强的双分支编解码器架构，实验表明性能与领先方法相当，揭示干净语音数据选择对增强性能的关键影响。


<details>
  <summary>Details</summary>
Motivation: 多数基于深度学习的语音增强方法需成对干净 - 嘈杂语音数据，真实环境收集困难，使用合成数据会导致训练和测试阶段有差距。

Method: 提出双分支编解码器架构将输入分离为干净语音和残余噪声，采用对抗训练为各分支施加先验。

Result: 方法性能与领先的无监督语音增强方法相当，揭示干净语音数据选择对增强性能有重要影响。

Conclusion: 使用领域内干净语音数据定义先验可能使性能表现过于乐观。

Abstract: The majority of deep learning-based speech enhancement methods require paired
clean-noisy speech data. Collecting such data at scale in real-world conditions
is infeasible, which has led the community to rely on synthetically generated
noisy speech. However, this introduces a gap between the training and testing
phases. In this work, we propose a novel dual-branch encoder-decoder
architecture for unsupervised speech enhancement that separates the input into
clean speech and residual noise. Adversarial training is employed to impose
priors on each branch, defined by unpaired datasets of clean speech and,
optionally, noise. Experimental results show that our method achieves
performance comparable to leading unsupervised speech enhancement approaches.
Furthermore, we demonstrate the critical impact of clean speech data selection
on enhancement performance. In particular, our findings reveal that performance
may appear overly optimistic when in-domain clean speech data are used for
prior definition -- a practice adopted in previous unsupervised speech
enhancement studies.

</details>


### [668] [AudioFuse: Unified Spectral-Temporal Learning via a Hybrid ViT-1D CNN Architecture for Robust Phonocardiogram Classification](https://arxiv.org/abs/2509.23454)
*Md. Saiful Bari Siddiqui,Utsab Saha*

Main category: eess.AS

TL;DR: 提出AudioFuse架构结合互补表征对PCG分类，在数据集上表现佳，融合互补表征可创建有效通用分类器。


<details>
  <summary>Details</summary>
Motivation: 标准2D频谱图处理生物医学音频信号时会损失相位和时间精度信息，需更好方法分类PCG。

Method: 提出AudioFuse架构，将自定义宽浅Vision Transformer用于频谱图，浅1D CNN用于原始波形。

Result: 在PhysioNet 2016数据集上ROC - AUC达0.8608，优于基线；在PASCAL数据集上对领域偏移更鲁棒。

Conclusion: 融合互补表征提供强归纳偏置，可创建高效、通用分类器，无需大规模预训练。

Abstract: Biomedical audio signals, such as phonocardiograms (PCG), are inherently
rhythmic and contain diagnostic information in both their spectral (tonal) and
temporal domains. Standard 2D spectrograms provide rich spectral features but
compromise the phase information and temporal precision of the 1D waveform. We
propose AudioFuse, an architecture that simultaneously learns from both
complementary representations to classify PCGs. To mitigate the overfitting
risk common in fusion models, we integrate a custom, wide-and-shallow Vision
Transformer (ViT) for spectrograms with a shallow 1D CNN for raw waveforms. On
the PhysioNet 2016 dataset, AudioFuse achieves a state-of-the-art competitive
ROC-AUC of 0.8608 when trained from scratch, outperforming its spectrogram
(0.8066) and waveform (0.8223) baselines. Moreover, it demonstrates superior
robustness to domain shift on the challenging PASCAL dataset, maintaining an
ROC-AUC of 0.7181 while the spectrogram baseline collapses (0.4873). Fusing
complementary representations thus provides a strong inductive bias, enabling
the creation of efficient, generalizable classifiers without requiring
large-scale pre-training.

</details>


### [669] [AI-Assisted Music Production: A User Study on Text-to-Music Models](https://arxiv.org/abs/2509.23364)
*Francesca Ronchini,Luca Comanducci,Simone Marcucci,Fabio Antonacci*

Main category: eess.AS

TL;DR: 本文通过用户研究开展关于文本到音乐（TTM）模型对音乐制作影响的案例研究，揭示关键挑战、机会和伦理考量。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到音乐模型革新了创作领域，但在音乐家工作流程中的整合情况研究不足，因此研究其对音乐制作的影响。

Method: 基于用户研究，让参与者使用结合TTM和源分离模型的自定义工具制作曲目，通过半结构化访谈和主题分析进行研究。

Result: 揭示了TTM模型在音乐制作中的关键挑战、机会和伦理考量。

Conclusion: 研究结果展示了TTM模型在音乐制作中的变革潜力以及在现实世界集成中面临的挑战。

Abstract: Text-to-music models have revolutionized the creative landscape, offering new
possibilities for music creation. Yet their integration into musicians
workflows remains underexplored. This paper presents a case study on how TTM
models impact music production, based on a user study of their effect on
producers creative workflows. Participants produce tracks using a custom tool
combining TTM and source separation models. Semi-structured interviews and
thematic analysis reveal key challenges, opportunities, and ethical
considerations. The findings offer insights into the transformative potential
of TTMs in music production, as well as challenges in their real-world
integration.

</details>


### [670] [VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning](https://arxiv.org/abs/2509.24773)
*Xin Cheng,Yuyue Wang,Xihua Wang,Yihan Wu,Kaisi Guan,Yijing Chen,Peng Zhang,Xiaojiang Liu,Meng Cao,Ruihua Song*

Main category: eess.AS

TL;DR: 提出VSSFlow框架统一视频到声音和视觉文本到语音任务，实验显示其超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统上视频到声音和视觉文本到语音任务分开处理，现有统一尝试存在处理不同条件类型和复杂训练阶段的问题，需统一这两个任务。

Method: 提出VSSFlow框架，用条件聚合机制处理不同输入信号，利用交叉注意力和自注意力处理不同表示，采用端到端联合学习。

Result: VSSFlow在视频到声音和视觉文本到语音基准测试中超越现有特定领域基线。

Conclusion: 统一生成模型有重要潜力。

Abstract: Video-conditioned sound and speech generation, encompassing video-to-sound
(V2S) and visual text-to-speech (VisualTTS) tasks, are conventionally addressed
as separate tasks, with limited exploration to unify them within a signle
framework. Recent attempts to unify V2S and VisualTTS face challenges in
handling distinct condition types (e.g., heterogeneous video and transcript
conditions) and require complex training stages. Unifying these two tasks
remains an open problem. To bridge this gap, we present VSSFlow, which
seamlessly integrates both V2S and VisualTTS tasks into a unified flow-matching
framework. VSSFlow uses a novel condition aggregation mechanism to handle
distinct input signals. We find that cross-attention and self-attention layer
exhibit different inductive biases in the process of introducing condition.
Therefore, VSSFlow leverages these inductive biases to effectively handle
different representations: cross-attention for ambiguous video conditions and
self-attention for more deterministic speech transcripts. Furthermore, contrary
to the prevailing belief that joint training on the two tasks requires complex
training strategies and may degrade performance, we find that VSSFlow benefits
from the end-to-end joint learning process for sound and speech generation
without extra designs on training stages. Detailed analysis attributes it to
the learned general audio prior shared between tasks, which accelerates
convergence, enhances conditional generation, and stabilizes the
classifier-free guidance process. Extensive experiments demonstrate that
VSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S
and VisualTTS benchmarks, underscoring the critical potential of unified
generative models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [671] [Agentar-Scale-SQL: Advancing Text-to-SQL through Orchestrated Test-Time Scaling](https://arxiv.org/abs/2509.24403)
*Pengfei Wang,Baolin Sun,Xuemei Dong,Yaxun Dai,Hongwei Yuan,Mengdie Chu,Yingqi Gao,Xiang Qi,Peng Zhang,Ying Yan*

Main category: cs.CL

TL;DR: 现有Text - to - SQL方法在BIRD基准测试上落后人类专家，提出Agentar - Scale - SQL框架提升性能，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前SOTA的Text - to - SQL方法在BIRD等基准测试中明显落后于人类专家，且现有测试时扩展方法缺乏协调策略、忽视模型内部推理过程。

Method: 引入Agentar - Scale - SQL框架，采用协调测试时扩展策略，结合内部扩展、顺序扩展和并行扩展三种视角。

Result: Agentar - Scale - SQL在BIRD基准测试上达到SOTA性能，测试集执行准确率达81.67%，在官方排行榜排名第一。

Conclusion: Agentar - Scale - SQL为实现人类水平性能提供了有效途径。

Abstract: State-of-the-art (SOTA) Text-to-SQL methods still lag significantly behind
human experts on challenging benchmarks like BIRD. Current approaches that
explore test-time scaling lack an orchestrated strategy and neglect the model's
internal reasoning process. To bridge this gap, we introduce Agentar-Scale-SQL,
a novel framework leveraging scalable computation to improve performance.
Agentar-Scale-SQL implements an Orchestrated Test-Time Scaling strategy that
synergistically combines three distinct perspectives: i) Internal Scaling via
RL-enhanced Intrinsic Reasoning, ii) Sequential Scaling through Iterative
Refinement, and iii) Parallel Scaling using Diverse Synthesis and Tournament
Selection. Agentar-Scale-SQL is a general-purpose framework designed for easy
adaptation to new databases and more powerful language models. Extensive
experiments show that Agentar-Scale-SQL achieves SOTA performance on the BIRD
benchmark, reaching 81.67\% execution accuracy on the test set and ranking
first on the official leaderboard, demonstrating an effective path toward
human-level performance.

</details>


### [672] [Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents](https://arxiv.org/abs/2509.24405)
*Khanh Trinh Pham,Thu Huong Nguyen,Jun Jo,Quoc Viet Hung Nguyen,Thanh Tam Nguyen*

Main category: cs.CL

TL;DR: 提出MultiSpider 2.0多语言基准，测试LLMs表现不佳，给出协作式语言代理基线提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL基准多为英文，限制多语言进展，需开发多语言基准。

Method: 扩展Spider 2.0到八种语言形成MultiSpider 2.0，提供协作式语言代理基线迭代优化查询。

Result: 最先进LLMs在MultiSpider 2.0上仅4%执行准确率，协作式基线提升到15%。

Conclusion: 存在巨大多语言差距，需开发跨语言鲁棒方法用于企业部署，基准已开源。

Abstract: Text-to-SQL enables natural access to databases, yet most benchmarks are
English-only, limiting multilingual progress. We introduce MultiSpider 2.0,
extending Spider 2.0 to eight languages (English, German, French, Spanish,
Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's
structural difficulty while adding linguistic and dialectal variability,
demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art
LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\% execution accuracy when
relying on intrinsic reasoning, versus 60\% on MultiSpider 1.0. Therefore, we
provide a collaboration-driven language agents baseline that iteratively
refines queries, improving accuracy to 15\%. These results reveal a substantial
multilingual gap and motivate methods that are robust across languages and
ready for real-world enterprise deployment. Our benchmark is available at
https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.

</details>


### [673] [Incentive-Aligned Multi-Source LLM Summaries](https://arxiv.org/abs/2509.25184)
*Yanchen Jiang,Zhe Feng,Aranyak Mehta*

Main category: cs.CL

TL;DR: 提出无真实标签下提高事实鲁棒性的TTS框架，实验显示其能提升事实准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前搜索和回答系统中使用大语言模型合成文本时，对来源准确性激励不足且易受对抗内容影响。

Method: TTS框架将合成文本分解为原子声明，引出每个来源对声明的立场，用多任务同行预测机制对来源打分，过滤不可靠来源后重新总结。

Result: TTS在保持流畅性的同时提高了事实准确性和鲁棒性，使曝光与信息确证一致，抑制了操纵行为。

Conclusion: TTS框架能使来源激励与信息诚实一致，让如实报告成为效用最大化策略。

Abstract: Large language models (LLMs) are increasingly used in modern search and
answer systems to synthesize multiple, sometimes conflicting, texts into a
single response, yet current pipelines offer weak incentives for sources to be
accurate and are vulnerable to adversarial content. We introduce Truthful Text
Summarization (TTS), an incentive-aligned framework that improves factual
robustness without ground-truth labels. TTS (i) decomposes a draft synthesis
into atomic claims, (ii) elicits each source's stance on every claim, (iii)
scores sources with an adapted multi-task peer-prediction mechanism that
rewards informative agreement, and (iv) filters unreliable sources before
re-summarizing. We establish formal guarantees that align a source's incentives
with informative honesty, making truthful reporting the utility-maximizing
strategy. Experiments show that TTS improves factual accuracy and robustness
while preserving fluency, aligning exposure with informative corroboration and
disincentivizing manipulation.

</details>


### [674] [Toward Preference-aligned Large Language Models via Residual-based Model Steering](https://arxiv.org/abs/2509.23982)
*Lucio La Cava,Andrea Tagarelli*

Main category: cs.CL

TL;DR: 提出无训练方法PaLRS用于大语言模型偏好对齐，在多基准测试中表现良好且节省时间。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型偏好对齐方法需精心策划数据、对数十亿参数进行昂贵优化，且会产生特定任务模型。

Method: 引入无训练方法PaLRS，利用大语言模型残差流中编码的偏好信号，从少量偏好对中提取轻量级、即插即用的引导向量。

Result: 在多个中小型开源大语言模型上评估，PaLRS对齐的模型在数学推理和代码生成基准测试中持续提升，保留通用性能，相比DPO对齐模型表现更好且节省大量时间。

Conclusion: PaLRS是标准偏好优化流程的有效、高效且灵活的替代方案，提供无训练、即插即用的对齐机制，只需最少的数据。

Abstract: Preference alignment is a critical step in making Large Language Models
(LLMs) useful and aligned with (human) preferences. Existing approaches such as
Reinforcement Learning from Human Feedback or Direct Preference Optimization
typically require curated data and expensive optimization over billions of
parameters, and eventually lead to persistent task-specific models. In this
work, we introduce Preference alignment of Large Language Models via Residual
Steering (PaLRS), a training-free method that exploits preference signals
encoded in the residual streams of LLMs. From as few as one hundred preference
pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be
applied at inference time to push models toward preferred behaviors. We
evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that
PaLRS-aligned models achieve consistent gains on mathematical reasoning and
code generation benchmarks while preserving baseline general-purpose
performance. Moreover, when compared to DPO-aligned models, they perform better
with huge time savings. Our findings highlight that PaLRS offers an effective,
much more efficient and flexible alternative to standard preference
optimization pipelines, offering a training-free, plug-and-play mechanism for
alignment with minimal data.

</details>


### [675] [Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems](https://arxiv.org/abs/2509.22845)
*Kai Hua,Zhiyuan Feng,Chongyang Tao,Rui Yan,Lu Zhang*

Main category: cs.CL

TL;DR: 本文提出RSM - DCK模型解决检索式对话系统中上下文和知识含无用信息影响匹配性能的问题，在两个基准数据集测试表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有检索式对话系统中，上下文和知识里的无用信息会影响匹配过程、导致性能下降，需要解决该问题。

Method: 提出RSM - DCK模型，先以近期上下文为查询词在词级和语句级语义上预选择相关上下文和知识，再让候选回复分别与所选内容交互，最后用上下文和候选回复的融合表示更可靠地后选择知识进行匹配。

Result: 在两个基准数据集上测试，模型比现有方法性能更好。

Conclusion: 模型能有效检测用于回复选择的相关上下文和知识。

Abstract: Recently, knowledge-grounded conversations in the open domain gain great
attention from researchers. Existing works on retrieval-based dialogue systems
have paid tremendous efforts to utilize neural networks to build a matching
model, where all of the context and knowledge contents are used to match the
response candidate with various representation methods. Actually, different
parts of the context and knowledge are differentially important for recognizing
the proper response candidate, as many utterances are useless due to the topic
shift. Those excessive useless information in the context and knowledge can
influence the matching process and leads to inferior performance. To address
this problem, we propose a multi-turn \textbf{R}esponse \textbf{S}election
\textbf{M}odel that can \textbf{D}etect the relevant parts of the
\textbf{C}ontext and \textbf{K}nowledge collection (\textbf{RSM-DCK}). Our
model first uses the recent context as a query to pre-select relevant parts of
the context and knowledge collection at the word-level and utterance-level
semantics. Further, the response candidate interacts with the selected context
and knowledge collection respectively. In the end, The fused representation of
the context and response candidate is utilized to post-select the relevant
parts of the knowledge collection more confidently for matching. We test our
proposed model on two benchmark datasets. Evaluation results indicate that our
model achieves better performance than the existing methods, and can
effectively detect the relevant context and knowledge for response selection.

</details>


### [676] [ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in Biographical Reasoning](https://arxiv.org/abs/2509.22991)
*Jasin Cekinmez,Omid Ghahroodi,Saad Fowad Chandle,Dhiman Gupta,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: 介绍ADAM框架评估和改进多模态大语言模型传记推理能力，含数据集、评估方法和检索增强系统，实验有成效。


<details>
  <summary>Details</summary>
Motivation: 系统检验大语言模型在传记推理这一未充分探索维度的能力。

Method: 构建AdamDB数据集和AdamBench评估方法，提出AdamRAG检索增强系统。

Result: AdamRAG大幅提升开源模型，适度提升闭源模型，在低阶推理上提升最大，流行度影响准确性，多模态输入提升较小。

Conclusion: ADAM建立首个传记评估基准和框架，推动多语言、准确且抗幻觉的多模态大语言模型发展。

Abstract: We introduce ADAM (A Diverse Archive of Mankind), a framework for evaluating
and improving multimodal large language models (MLLMs) in biographical
reasoning. To the best of our knowledge, this is the first work to
systematically examine LLM capabilities in biography, a critical yet
underexplored dimension of factual knowledge. At its core, AdamDB is a
multilingual and multimodal dataset covering over 4 million individuals across
geography, time, and profession, while AdamBench provides cognitively
structured evaluations based on Bloom's taxonomy, spanning six reasoning levels
in both English and native languages. To address hallucinations, particularly
for lesser-known individuals, we propose AdamRAG, a retrieval-augmented
generation system tailored to biographical contexts. Experiments show that
AdamRAG substantially improves open-source models and modestly benefits
closed-source ones, with the largest gains on lower-order reasoning. Popularity
strongly mediates accuracy, and multimodal input via face images offers
smaller, less consistent improvements than retrieval. ADAM establishes the
first benchmark and framework for cognitively, culturally, and multimodally
grounded biographical evaluation, advancing the development of multilingual,
accurate, and hallucination-resistant MLLMs.

</details>


### [677] [DocPruner: A Storage-Efficient Framework for Multi-Vector Visual Document Retrieval via Adaptive Patch-Level Embedding Pruning](https://arxiv.org/abs/2509.23883)
*Yibo Yan,Guangwei Xu,Xin Zou,Shuliang Liu,James Kwok,Xuming Hu*

Main category: cs.CL

TL;DR: 提出DocPruner框架用于视觉文档检索，通过自适应嵌入剪枝减少存储开销，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多向量范式视觉文档检索方法存储开销大，难以大规模部署。

Method: 引入DocPruner框架，利用文档内补丁注意力分布动态识别和丢弃冗余嵌入。

Result: 能使领先的多向量VDR模型存储减少50 - 60%，且检索性能下降可忽略不计。

Conclusion: DocPruner为构建存储高效、大规模的VDR系统提供了稳健、灵活和有效的解决方案。

Abstract: Visual Document Retrieval (VDR), the task of retrieving visually-rich
document pages using queries that combine visual and textual cues, is crucial
for numerous real-world applications. Recent state-of-the-art methods leverage
Large Vision-Language Models (LVLMs) in a multi-vector paradigm, representing
each document as patch-level embeddings to capture fine-grained details. While
highly effective, this approach introduces a critical challenge: prohibitive
storage overhead, as storing hundreds of vectors per page makes large-scale
deployment costly and impractical. To address this, we introduce DocPruner, the
first framework to employ adaptive patch-level embedding pruning for VDR to
effectively reduce the storage overhead. DocPruner leverages the intra-document
patch attention distribution to dynamically identify and discard redundant
embeddings for each document. This adaptive mechanism enables a significant
50-60% reduction in storage for leading multi-vector VDR models with negligible
degradation in document retrieval performance. Extensive experiments across
more than ten representative datasets validate that DocPruner offers a robust,
flexible, and effective solution for building storage-efficient, large-scale
VDR systems.

</details>


### [678] [AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play](https://arxiv.org/abs/2509.24193)
*Ran Xu,Yuchen Zhuang,Zihan Dong,Jonathan Wang,Yue Yu,Joyce C. Ho,Linjun Zhang,Haoyu Wang,Wenqi Shi,Carl Yang*

Main category: cs.CL

TL;DR: 提出AceSearcher框架，训练单一LLM交替扮演分解器和求解器角色，结合监督微调与强化微调，实验显示其在复杂推理任务上优于现有方法，效率高。


<details>
  <summary>Details</summary>
Motivation: 解决搜索增强大语言模型在复杂推理任务中因多跳检索无效和推理能力有限而面临的困境。

Method: 提出AceSearcher合作自玩框架，让单一LLM交替扮演分解器和求解器角色，结合监督微调与强化微调。

Result: 在10个数据集的3个推理密集型任务中，AceSearcher优于现有基线，平均精确匹配提高7.6%；在文档级金融推理任务中，小参数模型表现出色。

Conclusion: AceSearcher在处理复杂推理任务方面具有出色的效率和有效性。

Abstract: Search-augmented LLMs often struggle with complex reasoning tasks due to
ineffective multi-hop retrieval and limited reasoning ability. We propose
AceSearcher, a cooperative self-play framework that trains a single large
language model (LLM) to alternate between two roles: a decomposer that breaks
down complex queries and a solver that integrates retrieved contexts for answer
generation. AceSearcher couples supervised fine-tuning on a diverse mixture of
search, reasoning, and decomposition tasks with reinforcement fine-tuning
optimized for final answer accuracy, eliminating the need for intermediate
annotations. Extensive experiments on three reasoning-intensive tasks across 10
datasets show that AceSearcher outperforms state-of-the-art baselines,
achieving an average exact match improvement of 7.6%. Remarkably, on
document-level finance reasoning tasks, AceSearcher-32B matches the performance
of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller
scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented
LLMs with up to 9x more parameters, highlighting its exceptional efficiency and
effectiveness in tackling complex reasoning tasks. Our code will be published
at https://github.com/ritaranx/AceSearcher and
https://huggingface.co/AceSearcher.

</details>


### [679] [Scaling Generalist Data-Analytic Agents](https://arxiv.org/abs/2509.25084)
*Shuofei Qiao,Yanqiu Zhao,Zhisong Qiu,Xiaobin Wang,Jintian Zhang,Zhao Bin,Ningyu Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Huajun Chen*

Main category: cs.CL

TL;DR: 论文介绍DataMind构建通用数据解析智能体，应对开源数据解析智能体面临的挑战，基于此创建DataMind-12K，训练的模型在基准测试中表现出色，还将发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖专有模型的提示工程，开源模型难以处理现实世界分析所需的多样化数据和多步推理，因此要构建通用数据解析智能体。

Method: 1. 采用细粒度任务分类和递归易到难任务组合机制；2. 知识增强轨迹采样策略并进行过滤；3. 结合SFT和RL损失的动态可调整训练目标；4. 内存高效且稳定的基于代码的多轮滚动框架。

Result: 构建DataMind-12K，DataMind-14B在多个基准测试中达到71.16%的平均得分，优于专有基线；DataMind-7B在开源模型中表现最佳，得分68.10%。

Conclusion: 提出的DataMind有效解决了构建开源数据解析智能体的挑战，将发布相关资源，为社区提供关于智能体训练的实用见解。

Abstract: Data-analytic agents are emerging as a key catalyst for automated scientific
discovery and for the vision of Innovating AI. Current approaches, however,
rely heavily on prompt engineering over proprietary models, while open-source
models struggle to face diverse-format, large-scale data files and
long-horizon, multi-step reasoning that real-world analytics demands. This
paper introduces DataMind, a scalable data synthesis and agent training recipe
designed to build generalist data-analytic agents. DataMind tackles three key
challenges in building open-source data-analytic agents, including insufficient
data resources, improper training strategy, and unstable code-based multi-turn
rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a
recursive easy-to-hard task composition mechanism to increase the diversity and
difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling
strategy followed by model-based and rule-based filtering; 3) a dynamically
adjustable training objective combining both SFT and RL losses; 4) a
memory-frugal and stable code-based multi-turn rollout framework. Built on
DataMind, we curate DataMind-12K, a high-quality trajectory set spanning
diverse domains, task categories, and data file formats for data-analytic
tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with
an average score of 71.16% on multiple data analysis benchmarks, outperforming
the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B
also performs best among all open-source models with a score of 68.10%. We also
incorporate some empirical insights gained from our exploratory trials into the
analysis experiments, aiming to provide actionable insights about agentic
training for the community. We will release DataMind-12K and DataMind-7B,14B
for the community's future research.

</details>


### [680] [jina-reranker-v3: Last but Not Late Interaction for Document Reranking](https://arxiv.org/abs/2509.25085)
*Feng Wang,Yuqing Li,Han Xiao*

Main category: cs.CL

TL;DR: jina - reranker - v3是0.6B参数的多语言文档重排器，采用新交互方式，架构紧凑，性能达SOTA且模型小。


<details>
  <summary>Details</summary>
Motivation: 开发性能好且模型小的多语言文档重排器，改进现有晚交互模型。

Method: 在同一上下文窗口内对查询和文档进行因果自注意力操作，在从每个文档最后一个标记提取上下文嵌入之前实现丰富的跨文档交互。

Result: 在BEIR上达到61.94 nDCG@10的SOTA性能，模型比生成式列表重排器小10倍。

Conclusion: jina - reranker - v3采用的新交互方式有效，能在小模型下实现高性能。

Abstract: jina-reranker-v3 is a 0.6B parameter multilingual document reranker that
introduces a novel last but not late interaction. Unlike late interaction
models such as ColBERT that perform separate encoding followed by multi-vector
matching, our approach conducts causal self-attention between query and
documents within the same context window, enabling rich cross-document
interactions before extracting contextual embeddings from the last token of
each document. This compact architecture achieves state-of-the-art BEIR
performance with 61.94 nDCG@10 while being ten times smaller than generative
listwise rerankers.

</details>


### [681] [Towards Personalized Deep Research: Benchmarks and Evaluations](https://arxiv.org/abs/2509.25106)
*Yuan Liang,Jiaxian Li,Yuqing Wang,Piaohong Wang,Motong Tian,Pai Liu,Shuofei Qiao,Runnan Fang,He Zhu,Ge Zhang,Minghao Liu,Yuchen Eleanor Jiang,Ningyu Zhang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出Personalized Deep Research Bench和PQR评估框架评估DRAs个性化，实验揭示能力与局限，为下一代个性化AI研究助手奠基。


<details>
  <summary>Details</summary>
Motivation: 现有评估多依赖封闭式基准，开放式深度研究基准稀缺且忽视个性化场景。

Method: 引入Personalized Deep Research Bench，结合多样研究任务和真实用户档案生成查询；提出PQR评估框架衡量系统性能。

Result: 实验凸显当前系统在处理个性化深度研究方面的能力与局限。

Conclusion: 为开发和评估下一代真正个性化的AI研究助手奠定严谨基础。

Abstract: Deep Research Agents (DRAs) can autonomously conduct complex investigations
and generate comprehensive reports, demonstrating strong real-world potential.
However, existing evaluations mostly rely on close-ended benchmarks, while
open-ended deep research benchmarks remain scarce and typically neglect
personalized scenarios. To bridge this gap, we introduce Personalized Deep
Research Bench, the first benchmark for evaluating personalization in DRAs. It
pairs 50 diverse research tasks across 10 domains with 25 authentic user
profiles that combine structured persona attributes with dynamic real-world
contexts, yielding 250 realistic user-task queries. To assess system
performance, we propose the PQR Evaluation Framework, which jointly measures
(P) Personalization Alignment, (Q) Content Quality, and (R) Factual
Reliability. Our experiments on a range of systems highlight current
capabilities and limitations in handling personalized deep research. This work
establishes a rigorous foundation for developing and evaluating the next
generation of truly personalized AI research assistants.

</details>


### [682] [Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models](https://arxiv.org/abs/2509.22739)
*Sasha Cui,Zhongren Chen*

Main category: cs.CL

TL;DR: 提出无痛激活引导（PAS）方法，可自动利用标注数据集，无需手动干预。在多模型和任务上评估，发现其对行为任务有效，内省变体 iPAS 有强因果引导效果，还能在 ICL 和 SFT 基础上增益。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型后训练方法存在时间长、成本高、难控制等问题，当前激活引导技术使用不便，需开发更易用的方法。

Method: 引入 Painless Activation Steering (PAS) 这一全自动化方法，可利用给定标注数据集，无需构建提示、特征标注或人工干预。

Result: PAS 能可靠提升行为任务性能，内省变体 iPAS 有较强因果引导效果，且能在 ICL 和 SFT 基础上带来额外增益。

Conclusion: 研究明确了激活引导的适用场景和失效情况，提供了一种实用、自动化的语言模型后训练选项。

Abstract: Language models (LMs) are typically post-trained for desired capabilities and
behaviors via weight-based or prompt-based steering, but the former is
time-consuming and expensive, and the latter is not precisely controllable and
often requires manual trial-and-error. While activation steering (AS) promises
a cheap, fast, and controllable alternative to the two existing post-training
methods, current AS techniques require hand-crafted prompt pairs or
labor-intensive feature annotation, making them more inconvenient than the
plug-and-play methods such as Reinforcement Learning (RL) and Supervised
Fine-Tuning (SFT). We introduce Painless Activation Steering (PAS), a family of
fully automated methods that make AS readily usable with any given labeled
dataset, with no need for prompt construction, feature labeling, or human
intervention. We evaluate PAS on three open-weight models
(Llama3.1-8B-Instruct, DeepSeek-R1-Distill-8B, and Nous-Hermes-2) and 18 tasks;
we find that PAS reliably improves performance for behavior tasks, but not for
intelligence-oriented tasks. The introspective variant (iPAS) delivers the
strongest causal steering effects (10.1% on Bias, 5.2% on Morality, and 34.8%
on Alignment). We also show PAS delivers additional gains on top of In-Context
Learning (ICL) and SFT. PAS constructs a fast, lightweight activation vector
that can be cheaply trained, easily stored, and activated at will. Our results
provide a characterization of where AS helps, where it fails, and how to deploy
it as a practical, automated LM post-training option.

</details>


### [683] [Exploring Large Language Models for Translating Romanian Computational Problems into English](https://arxiv.org/abs/2501.05601)
*Adrian Marius Dumitran,Adrian-Catalin Badea,Stefan-Gabriel Muscalu,Angela-Liliana Dumitran,Stefan-Cosmin Dascalescu,Radu-Sebastian Amarie*

Main category: cs.CL

TL;DR: 研究表明，结构良好的提示可让大语言模型在翻译小语种时保持或提升性能，经评估和分析，在人工监督下大语言模型可用于多语言问题解决。


<details>
  <summary>Details</summary>
Motivation: 准确翻译在编程竞赛自动翻译、教育材料创建等方面至关重要，而现有大语言模型在罗马尼亚语转英语的数学和计算机科学任务中表现不佳。

Method: 评估多种大语言模型的翻译方法，通过多次运行评估翻译准确性和性能稳定性；为罗马尼亚数据集添加准确英文翻译；进行详细句法和语义分析；对比大语言模型和人工翻译质量。

Result: 结构良好的提示可让大语言模型在翻译小语种时保持或提升性能；在人工监督下大语言模型可用于国际信息学奥林匹克风格任务的自动翻译。

Conclusion: 在人工监督下，大语言模型可作为多语言问题解决的可行方案，在现实场景中有应用潜力。

Abstract: Recent studies have suggested that large language models (LLMs) underperform
on mathematical and computer science tasks when these problems are translated
from Romanian into English, compared to their original Romanian format.
Accurate translation is critical for applications ranging from automatic
translations in programming competitions to the creation of high-quality
educational materials, as well as minimizing errors or fraud in human
translations. This study shows that robust large language models (LLMs) can
maintain or even enhance their performance in translating less common languages
when given well-structured prompts. Our findings suggest that LLMs, with
appropriate supervision, can be reliably used for the automatic translation of
IOI (International Olympiad in Informatics)-style tasks. We evaluate several
translation methods across multiple LLMs, including OpenRoLLM, Llama 3.1 8B,
Llama 3.2 3B and GPT-4o, assessing their translation accuracy and performance
stability through repeated runs. Additionally, we augment the OJI (Romanian
County-Level Informatics Olympiad) Romanian dataset with accurate English
translations, enhancing its utility for future LLM training and evaluation.
Through detailed syntactic and semantic analyses, we confirm that with human
oversight, LLMs can serve as a viable solution for multilingual
problem-solving. We also compare the translation quality of LLMs against human
translators, as evaluated by a certified expert, underscoring the potential of
LLMs in realworld scenarios.

</details>


### [684] [TF-Bench: Evaluating Program Semantics Reasoning with Type Inference in System F](https://arxiv.org/abs/2509.23686)
*Yifeng He,Luning Yang,Christopher Castro Gaw Gonzalo,Hao Chen*

Main category: cs.CL

TL;DR: 介绍TF - Bench基准评估大语言模型程序语义推理能力，分析显示现有模型有局限并提出新评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有代码推理基准缺乏正式、以程序为中心的演绎框架，无法评估模型是否真正推理程序语义。

Method: 引入基于System F类型推理的TF - Bench基准，构建纯语义驱动的TF - Bench_pure，提出两个新评估指标。

Result: 现有最先进大语言模型Claude - 3.7 - sonnet在TF - Bench_pure上准确率仅55.85%。

Conclusion: 当前大语言模型能力存在关键局限，指明未来研究方向。

Abstract: Large Language Models (LLMs) are increasingly integrated into the software
engineering ecosystem. Their test-time compute (TTC) reasoning capabilities
show significant potential for understanding program logic and semantics beyond
mere token recognition. However, current benchmarks for code reasoning lack a
formal, program-centric deductive framework to ensure sound evaluation, and are
incapable of assessing whether models genuinely reason about program semantics
or merely exploit superficial associations between natural language and code
tokens. To bridge this gap, we introduce TF-Bench, a benchmark designed to
evaluate LLM reasoning based on type inference in System F, a task we refer to
as program semantics reasoning. By employing verified transformations to remove
semantically irrelevant natural language, we construct TF-Bench_pure, a purely
semantics-driven variant of TF-Bench. Our analysis reveals substantial
limitations in state-of-the-art LLMs, with the best-performing LLM
(Claude-3.7-sonnet) achieving only 55.85% accuracy on TF-Bench_pure.
Additionally, we propose two novel metrics to assess robustness and the
effectiveness of test-time reasoning, underscoring critical limitations in
current LLM capabilities and highlighting essential directions for future
research.

</details>


### [685] [AccessEval: Benchmarking Disability Bias in Large Language Models](https://arxiv.org/abs/2509.22703)
*Srikant Panda,Amit Agarwal,Hitesh Laxmichand Patel*

Main category: cs.CL

TL;DR: 本文引入AccessEval基准评估21个大语言模型在不同残疾场景下的表现，发现对残疾相关查询的回复存在负面倾向、刻板印象和事实错误，强调了缓解偏差的重要性。


<details>
  <summary>Details</summary>
Motivation: 系统研究大语言模型在不同残疾场景下处理现实查询时的差异。

Method: 引入AccessEval基准，使用配对的中性和残疾相关查询，在6个现实领域和9种残疾类型下评估21个闭源和开源大语言模型，并采用情感、社会认知和事实准确性指标评估模型输出。

Result: 对残疾相关查询的回复相比中性查询有更负面的语气、更多刻板印象和更高的事实错误率，不同领域和残疾类型有显著差异，听力、言语和行动障碍受影响更大。

Conclusion: 这些差异反映了模型行为中存在的能力歧视，强调了在日常应用中缓解偏差的重要性，数据集已公开。

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse domains
but often exhibit disparities in how they handle real-life queries. To
systematically investigate these effects within various disability contexts, we
introduce \textbf{AccessEval (Accessibility Evaluation)}, a benchmark
evaluating 21 closed- and open-source LLMs across 6 real-world domains and 9
disability types using paired Neutral and Disability-Aware Queries. We
evaluated model outputs with metrics for sentiment, social perception, and
factual accuracy.
  Our analysis reveals that responses to disability-aware queries tend to have
a more negative tone, increased stereotyping, and higher factual error compared
to neutral queries. These effects show notable variation by domain and
disability type, with disabilities affecting hearing, speech, and mobility
disproportionately impacted. These disparities reflect persistent forms of
ableism embedded in model behavior.
  By examining model performance in real-world decision-making contexts, we
better illuminate how such biases can translate into tangible harms for
disabled users. This framing helps bridges the gap between technical evaluation
and user impact, reinforcing importance of bias mitigation in day-to-day
applications. Our dataset is publicly available at:
https://huggingface.co/datasets/Srikant86/AccessEval

</details>


### [686] [TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?](https://arxiv.org/abs/2509.22715)
*Jiho Park,Jongyoon Song,Minjin Choi,Kyuho Heo,Taehun Huh,Ji Won Kim*

Main category: cs.CL

TL;DR: 现有基准难以严格评估大语言模型真实指令跟随能力，本文提出TRUEBench基准进行更现实评估，实验表明其挑战更大。


<details>
  <summary>Details</summary>
Motivation: 现有基准在多语言性、捕捉隐式约束和处理多轮对话方面存在不足，需更现实评估大语言模型的基准。

Method: 引入TRUEBench基准，有12种语言输入提示、严格评估标准和复杂多轮对话场景，用大语言模型验证器完善约束。

Result: 实验显示TRUEBench比现有基准挑战大，如OpenAI o1整体通过率仅69.07%。

Conclusion: TRUEBench能对大语言模型在实际生产力场景中进行严格现实评估，凸显其能力与局限。

Abstract: Large language models (LLMs) are increasingly integral as productivity
assistants, but existing benchmarks fall short in rigorously evaluating their
real-world instruction-following capabilities. Current benchmarks often (i)
lack sufficient multilinguality, (ii) fail to capture the implicit constraints
inherent in user requests, and (iii) overlook the complexities of multi-turn
dialogue. To address these critical gaps and provide a more realistic
assessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation
Benchmark)1, a novel benchmark specifically designed for LLM-based productivity
assistants. TRUEBench distinguishes itself by featuring input prompts across 12
languages, incorporating intra-instance multilingual instructions, employing
rigorous evaluation criteria to capture both explicit and implicit constraints,
and including complex multi-turn dialogue scenarios with both accumulating
constraints and context switches. Furthermore, to ensure reliability in
evaluation, we refined constraints using an LLM validator. Extensive
experiments demonstrate that TRUEBench presents significantly greater
challenges than existing benchmarks; for instance, a strong model like OpenAI
o1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and
realistic assessment of LLMs in practical productivity settings, highlighting
their capabilities and limitations.

</details>


### [687] [Multi-Modal Sentiment Analysis with Dynamic Attention Fusion](https://arxiv.org/abs/2509.22729)
*Sadia Abdulhalim,Muaz Albaghdadi,Moshiur Farazi*

Main category: cs.CL

TL;DR: 提出轻量级框架DAF结合文本与声学特征进行情感分析，在基准测试中表现出色，证明动态加权策略重要性。


<details>
  <summary>Details</summary>
Motivation: 传统单模态情感分析仅依赖文本，忽略非语言线索，无法捕捉真实情感意图。

Method: 引入动态注意力融合（DAF）框架，结合预训练语言模型的冻结文本嵌入和语音编码器的声学特征，用自适应注意力机制为每个模态加权。

Result: DAF模型在大型多模态基准测试中始终优于静态融合和单模态基线，F1分数提高，预测误差降低，消融研究支持动态加权策略重要性的假设。

Conclusion: 有效整合语言和非语言信息，为情感预测提供更强大基础，对情感计算应用有广泛影响。

Abstract: Traditional sentiment analysis has long been a unimodal task, relying solely
on text. This approach overlooks non-verbal cues such as vocal tone and prosody
that are essential for capturing true emotional intent. We introduce Dynamic
Attention Fusion (DAF), a lightweight framework that combines frozen text
embeddings from a pretrained language model with acoustic features from a
speech encoder, using an adaptive attention mechanism to weight each modality
per utterance. Without any finetuning of the underlying encoders, our proposed
DAF model consistently outperforms both static fusion and unimodal baselines on
a large multimodal benchmark. We report notable gains in F1-score and
reductions in prediction error and perform a variety of ablation studies that
support our hypothesis that the dynamic weighting strategy is crucial for
modeling emotionally complex inputs. By effectively integrating verbal and
non-verbal information, our approach offers a more robust foundation for
sentiment prediction and carries broader impact for affective computing
applications -- from emotion recognition and mental health assessment to more
natural human computer interaction.

</details>


### [688] [MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions](https://arxiv.org/abs/2509.22750)
*Jeonghyun Park,Ingeol Baek,Seunghyun Yoon,Haeun Jang,Aparna Garimella,Akriti Jain,Nedim Lipka,Hwanhee Lee*

Main category: cs.CL

TL;DR: 论文指出真实世界多跳问答存在推理过程中的歧义问题，当前大语言模型处理不佳，引入MIRAGE基准并提出CLARION框架解决该问题。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界多跳问答中推理过程的歧义问题，推动多跳歧义研究。

Method: 引入MIRAGE基准，包含1142个高质量模糊多跳问题，并通过严格多LLM验证流程筛选；提出CLARION多智能体框架。

Result: 实验表明即使最先进模型在MIRAGE上表现不佳，CLARION框架在MIRAGE上显著优于现有方法。

Conclusion: 解决歧义与多步推理的结合是一个独特且重大的挑战，CLARION框架为更自适应和强大的推理系统奠定基础。

Abstract: Real-world Multi-hop Question Answering (QA) often involves ambiguity that is
inseparable from the reasoning process itself. This ambiguity creates a
distinct challenge, where multiple reasoning paths emerge from a single
question, each requiring independent resolution. Since each sub-question is
ambiguous, the model must resolve ambiguity at every step. Thus, answering a
single question requires handling multiple layers of ambiguity throughout the
reasoning chain. We find that current Large Language Models (LLMs) struggle in
this setting, typically exploring wrong reasoning paths and producing
incomplete answers. To facilitate research on multi-hop ambiguity, we introduce
MultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions (MIRAGE),
a benchmark designed to analyze and evaluate this challenging intersection of
ambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142
high-quality examples of ambiguous multi-hop questions, categorized under a
taxonomy of syntactic, general, and semantic ambiguity, and curated through a
rigorous multi-LLM verification pipeline. Our experiments reveal that even
state-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity
combined with multi-step inference is a distinct and significant challenge. To
establish a robust baseline, we propose CLarifying Ambiguity with a Reasoning
and InstructiON (CLARION), a multi-agent framework that significantly
outperforms existing approaches on MIRAGE, paving the way for more adaptive and
robust reasoning systems.

</details>


### [689] [Extract-0: A Specialized Language Model for Document Information Extraction](https://arxiv.org/abs/2509.22906)
*Henrique Godoy*

Main category: cs.CL

TL;DR: 本文提出70亿参数的语言模型Extract - 0，优化用于文档信息提取，表现超参数大几个数量级的模型。


<details>
  <summary>Details</summary>
Motivation: 开发能在文档信息提取任务上表现出色且更节省计算资源的模型。

Method: 结合合成数据生成、基于LoRA的监督微调、基于GRPO的强化学习；采用内存保留的合成数据生成管道；引入基于语义相似度的奖励函数。

Result: Extract - 0在1000个不同文档提取任务基准测试中平均奖励达0.573，超过GPT - 4.1等模型。

Conclusion: 特定任务优化可产生超越通用系统且所需计算资源大幅减少的模型。

Abstract: This paper presents Extract-0, a 7-billion parameter language model
specifically optimized for document information extraction that achieves
performance exceeding models with parameter counts several orders of magnitude
larger. Through a novel combination of synthetic data generation, supervised
fine-tuning with Low-Rank Adaptation (LoRA), and reinforcement learning via
Group Relative Policy Optimization (GRPO), Extract-0 achieves a mean reward of
0.573 on a benchmark of 1,000 diverse document extraction tasks, outperforming
GPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459). The training methodology
employs a memory-preserving synthetic data generation pipeline that produces
280,128 training examples from diverse document sources, followed by
parameterefficient fine-tuning that modifies only 0.53% of model weights (40.4M
out of 7.66B parameters). The reinforcement learning phase introduces a novel
semantic similarity-based reward function that handles the inherent ambiguity
in information extraction tasks. This research demonstrates that task-specific
optimization can yield models that surpass general-purpose systems while
requiring substantially fewer computational resource.

</details>


### [690] [Large language models management of medications: three performance analyses](https://arxiv.org/abs/2509.22926)
*Kelli Henry,Steven Xu,Kaitlin Blotske,Moriah Cargile,Erin F. Barreto,Brian Murray,Susan Smith,Seth R. Bauer,Yanjun Gao,Tianming Liu,Andrea Sikora*

Main category: cs.CL

TL;DR: 评估GPT - 4o在三项药物基准测试中的表现，结果显示其整体表现不佳，需特定领域训练和综合评估框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在诊断医疗状况有用，但很少有研究评估其推荐适当药物治疗方案的一致性，因此测试GPT - 4o在三项药物基准测试中的表现。

Method: 使用GPT - 4o完成三项实验，通过计算TF - IDF向量余弦相似度、归一化Levenshtein相似度、ROUGE - 1/ROUGE - L F1或临床医生手动评估来量化准确性。

Result: GPT - 4o在药物剂型匹配中表现差，识别药物相互作用不一致，准备药物订单语句表现中等。

Conclusion: 模型在所有测试中整体表现不佳，需要通过临床医生注释数据集进行特定领域训练和综合评估框架来衡量性能。

Abstract: Background: Large language models (LLMs) can be useful in diagnosing medical
conditions, but few studies have evaluated their consistency in recommending
appropriate medication regimens. The purpose of this evaluation was to test
GPT-4o on three medication benchmarking tests including mapping a drug name to
its correct formulation, identifying drug-drug interactions using both its
internal knowledge and using a web search, and preparing a medication order
sentence after being given the medication name. Methods: Using GTP-4o three
experiments were completed. Accuracy was quantified by computing cosine
similarity on TF-IDF vectors, normalized Levenshtein similarity, and
ROUGE-1/ROUGE-L F1 between each response and its reference string or by manual
evaluation by clinicians. Results: GPT-4o performed poorly on drug-formulation
matching, with frequent omissions of available drug formulations (mean 1.23 per
medication) and hallucinations of formulations that do not exist (mean 1.14 per
medication). Only 49% of tested medications were correctly matched to all
available formulations. Accuracy was decreased for medications with more
formulations (p<0.0001). GPT-4o was also inconsistent at identifying
drug-drug-interactions, although it had better performance with the
search-augmented assessment compared to its internal knowledge (54.7% vs.
69.2%, p=0.013). However, allowing a web-search worsened performance when there
was no drug-drug interaction (median % correct 100% vs. 40%, p<0.001). Finally,
GPT-4o performed moderately with preparing a medication order sentence, with
only 65.8% of medication order sentences containing no medication or
abbreviation errors. Conclusions: Model performance was overall poor for all
tests. This highlights the need for domain-specific training through
clinician-annotated datasets and a comprehensive evaluation framework for
benchmarking performance.

</details>


### [691] [What Matters More For In-Context Learning under Matched Compute Budgets: Pretraining on Natural Text or Incorporating Targeted Synthetic Examples?](https://arxiv.org/abs/2509.22947)
*Mohammed Sabry,Anya Belz*

Main category: cs.CL

TL;DR: 研究在预训练中显式激活归纳电路是否提升上下文学习（ICL），引入Bi - Induct课程进行测试，发现早期激活归纳电路不一定改善ICL，ICL提升依赖电路功能必要性。


<details>
  <summary>Details</summary>
Motivation: 测试目标合成数据能否加速归纳头出现并增强上下文学习（ICL）。

Method: 引入Bi - Induct课程，在等FLOPs下训练0.13B到1B参数的模型，评估少样本ICL基准、头级遥测和语言建模困惑度。

Result: Bi - Induct在小尺度加速归纳头出现，但未稳定带来更强泛化能力；标准LM基准上与仅自然数据训练相当，函数式ICL探测中1B仅自然数据模型表现最佳；合成数据困惑度惩罚随规模缩小；消融归纳头对仅自然数据模型ICL影响更大；Bi - Induct变体归纳活动更冗余。

Conclusion: 诱导激活不充分，ICL提升依赖电路成为功能上必要的，强调机制感知的预训练诊断和数据混合。

Abstract: Does explicitly exercising the induction circuit during pretraining improve
in-context learning (ICL), or is natural text sufficient when compute is held
constant (iso-FLOPs)? To test whether targeted synthetic data can accelerate
induction-head emergence and enhance ICL, we introduce Bi-Induct, a lightweight
curriculum that injects forward-copy (Induction), backward-copy (Anti), or a
balanced mix into the pretraining stream. We train models from 0.13B to 1B
parameters under iso-FLOPs, evaluating (i) few-shot ICL benchmarks, (ii)
head-level telemetry, and (iii) held-out language modeling perplexity. Our
findings challenge the assumption that early induction circuit activation
directly improves ICL. While Bi-Induct accelerates induction-head emergence at
small scales, this does not consistently yield stronger generalization. On
standard LM benchmarks, Bi-Induct matches natural-only training; on
function-style ICL probes, the 1B natural-only performs best. Stress tests
(e.g., label permutation, HITS@1 vs. HITS@3, 1 vs. 10 shots) preserve these
trends. Telemetry shows larger natural-only models develop broader, earlier
induction heads without explicit induction patterns. Anti-induction data fails
to elicit meaningful activation. Perplexity penalties from synthetic data
shrink with scale, suggesting larger models can absorb non-natural patterns
with minimal cost. Crucially, ablating the top 2% of induction heads degrades
ICL more than random ablations, especially for natural-only models, indicating
more centralized, load-bearing circuits. Bi-Induct variants exhibit more
redundant induction activity, implying different circuit utilization. Overall,
inducing activation is not sufficient: ICL gains depend on these circuits
becoming functionally necessary. These results underscore mechanism-aware
pretraining diagnostics and data mixtures that foster load-bearing, not merely
present, structure.

</details>


### [692] [Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents](https://arxiv.org/abs/2509.23040)
*Yaorui Shi,Yuxin Chen,Siyuan Wang,Sihang Li,Hengxing Cai,Qi Gu,Xiang Wang,An Zhang*

Main category: cs.CL

TL;DR: 提出ReMemR1和RLMLR解决大语言模型长上下文问答问题，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型长上下文问答方法存在不可逆处理、信息丢失和稀疏强化学习信号等问题。

Method: 提出ReMemR1，一种具有回调增强记忆的记忆增强代理；提出RLMLR，结合最终答案奖励和密集的步骤级信号的强化学习方法。

Result: 在长文档问答实验中，相比现有基于记忆的方法有显著提升。

Conclusion: ReMemR1是长上下文推理代理的有效解决方案。

Abstract: Large language models face challenges in long-context question answering,
where key evidence of a query may be dispersed across millions of tokens.
Existing works equip large language models with a memory corpus that is
dynamically updated during a single-pass document scan, also known as the
"memorize while reading" methods. While this approach scales efficiently, it
suffers from irreversible forward-only processing, information loss through
overwriting, and sparse reinforcement learning signals. To tackle these
challenges, we present ReMemR1, a memory-augmented agent with callback-enhanced
memory that allows selective retrieval from the entire memory history and
allows non-linear reasoning and revisiting of early evidence. To further
strengthen training, we propose Reinforcement Learning with Multi-Level Rewards
(RLMLR), which combines final-answer rewards with dense, step-level signals
that guide effective memory use. Together, these contributions mitigate
information degradation, improve supervision, and support multi-hop memory
utilizing. Experiments on long-document QA show significant gains over existing
memory-based approaches, which validates ReMemR1 as an effective solution for
long-context reasoning agents.

</details>


### [693] [Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks](https://arxiv.org/abs/2509.23067)
*Chunyang Jiang,Yonggang Zhang,Yiyang Cai,Chi-Min Chan,Yulong Liu,Mingming Chen,Wei Xue,Yike Guo*

Main category: cs.CL

TL;DR: 提出一种无自评估的方法用于不可验证任务的大语言模型自我改进，基于语义投票，实验显示其计算效率和性能优于自评估方法。


<details>
  <summary>Details</summary>
Motivation: 获取监督数据成本上升，现有无监督信号用于不可验证任务受限，自评估机制有高计算开销和过度自信问题。

Method: 提出语义投票机制，从硬匹配转向软匹配，利用轻量级句子嵌入模型量化语义相似性。

Result: 在不同模型架构和任务中，方法在计算效率上有显著提升，整体性能优于自评估方法。

Conclusion: 所提无自评估方法能实现轻量级且有效的大语言模型自我改进。

Abstract: The rising cost of acquiring supervised data has driven significant interest
in self-improvement for large language models (LLMs). Straightforward
unsupervised signals like majority voting have proven effective in generating
pseudo-labels for verifiable tasks, while their applicability to unverifiable
tasks (e.g., translation) is limited by the open-ended character of responses.
As a result, self-evaluation mechanisms (e.g., self-judging and entropy
minimization) are predominantly used to derive pseudo-labels. However,
self-evaluation relying on LLMs typically incurs high computational overhead
and introduces overconfidence issues due to intrinsic biases. To address these
challenges, we propose a novel self-evaluation-free approach for unverifiable
tasks, designed for lightweight yet effective self-improvement. Inspired by
majority voting commonly employed in verifiable tasks, we propose semantic
voting as a novel mechanism that relaxes the principle of hard matching (i.e.,
exact matching) toward soft matching (i.e., semantic similarity). Soft matching
is achieved by leveraging a lightweight sentence embedding model to quantify
semantic similarity, thereby mitigating excessive computational burden and
intrinsic bias-associated limitations of self-evaluation. Comprehensive
experiments demonstrate that our method achieves substantial gains in
computational efficiency and overall better performance than self-evaluation
methods across diverse model architectures and tasks.

</details>


### [694] [From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents](https://arxiv.org/abs/2509.23071)
*Muzhi Li,Jinhu Qi,Yihong Wu,Minghao Zhao,Liheng Ma,Yifan Li,Xinyu Wang,Yingxue Zhang,Ho-fung Leung,Irwin King*

Main category: cs.CL

TL;DR: 本文提出EviPath范式用于RAG智能体开发，让大语言模型从合成数据学习能力，实验显示其训练的模型在问答基准测试中显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成智能体开发缺乏过程级监督，强化学习有稀疏奖励和大语言模型推理能力有限问题，现有数据合成方法无法模拟环境交互。

Method: 提出EviPath范式，包含溯因子任务规划、忠实子问题回答和对话微调三个部分。

Result: 使用EviPath合成数据训练的8B参数模型在问答基准测试中，开放领域问答绝对EM增益达14.7%，显著优于现有基线。

Conclusion: EviPath范式可有效助力RAG智能体开发，让大语言模型学习复杂推理和工具使用能力。

Abstract: Retrieval-augmented generation agents development is hindered by the lack of
process-level supervision to effectively guide agentic capabilities like task
decomposition, retriever invocation, and stepwise decision-making. While
reinforcement learning offers a potential solution, it suffers from sparse
rewards and the limited reasoning capabilities of large language models (LLMs).
Meanwhile, existing data synthesis methods only produce chain-of-thought
rationales and fail to model environmental interactions. In this paper, we
propose EviPath, an evidence-anchored reasoning path synthesis paradigm for RAG
agent development. EviPath comprises: (i) Abductive Subtask Planning, which
decomposes the problem into sub-questions and iteratively plans an optimal
solution path based on the dependencies between them; (ii) Faithful
Sub-question Answering, which uses supporting evidence to construct a proxy
environment to generate reasoning thoughts and answers for each sub-question;
and (iii) Conversational Fine-Tuning, which formats the complete
agent-environment interaction trajectory into a dialogue format suitable for
Supervised Fine-Tuning. EviPath allows LLMs to learn complex reasoning and
tool-use capabilities directly from synthesized data. Extensive experiments on
widely-used question-answering benchmarks show that an 8B parameter model
trained with EviPath-synthesized data significantly and consistently
outperforms state-of-the-art baselines with a double-digit absolute EM gain of
14.7% in open-domain question answering.

</details>


### [695] [PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness](https://arxiv.org/abs/2509.23206)
*Huacan Chai,Zijie Cao,Maolin Ran,Yingxuan Yang,Jianghao Lin,pengxin,Hairui Wang,Renjie Ding,Ziyu Wan,Muning Wen,Weiwen Liu,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.CL

TL;DR: 现有大语言模型在多轮函数调用存在不足，提出PARL - MT框架结合PAG和PAG - RL，实验显示其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多轮函数调用中存在忽略任务级规划、冗余且缺乏进度意识集成的问题，需要改进。

Method: 引入PARL - MT框架，包含自动构建数据集的PAG管道和将进度意识集成到强化学习训练的PAG - RL算法。

Result: 在两个公开基准测试中，PARL - MT显著优于现有方法。

Conclusion: 进度意识在实现稳健高效的多轮函数调用中有效。

Abstract: Large language models (LLMs) have achieved impressive success in single-turn
function calling, yet real-world applications such as travel planning or
multi-stage data analysis typically unfold across multi-turn conversations. In
these settings, LLMs must not only issue accurate function calls at each step
but also maintain progress awareness, the ability to summarize past
interactions and plan future actions to ensure coherent, long-horizon task
execution. Existing approaches, however, either reduce multi-turn training to
isolated single-turn samples, which neglects task-level planning, or employ
end-to-end reinforcement learning (RL) that struggles with redundancy and lacks
explicit integration of progress awareness. To overcome these limitations, we
introduce PARL-MT, a framework that explicitly incorporates progress awareness
into LLM training for multi-turn function calling. PARL-MT combines (i) a
Progress Awareness Generation (PAG) pipeline, which automatically constructs
datasets coupling conversation summaries with future task planning, and (ii) a
Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which
integrates progress awareness into RL training to reduce contextual redundancy
and improve alignment between local actions and global task completion.
Empirical results on two public benchmarks demonstrate that PARL-MT
significantly outperforms existing methods, highlighting the effectiveness of
progress awareness in enabling robust and efficient multi-turn function
calling.

</details>


### [696] [A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models](https://arxiv.org/abs/2509.23286)
*Wonje Jeung,Sangyeon Yoon,Yoonjun Cho,Dongjae Jeon,Sangwoo Shin,Hyesoo Hong,Albert No*

Main category: cs.CL

TL;DR: 本文提出A2D方法应对扩散大语言模型攻击，在安全基准测试中有效防止有害输出生成，提升终止速度。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型的灵活性增大攻击面，现有模板预填充攻击可绕过响应级拒绝。

Method: 引入A2D，一种基于随机掩码下的词元级对齐方法，使模型在出现有害内容时发出[EOS]拒绝信号。

Result: 在安全基准测试中，A2D将DIJA攻击成功率从超80%降至接近零，阈值化[EOS]概率实现早期拒绝，使安全终止速度提升达19.3倍。

Conclusion: A2D对任意解码顺序和任意步骤预填充攻击具有鲁棒性，能实时监测并自动终止不安全内容生成。

Abstract: Diffusion large language models (dLLMs) enable any-order generation, but this
flexibility enlarges the attack surface: harmful spans may appear at arbitrary
positions, and template-based prefilling attacks such as DIJA bypass
response-level refusals. We introduce A2D (Any-Order, Any-Step Defense), a
token-level alignment method that aligns dLLMs to emit an [EOS] refusal signal
whenever harmful content arises. By aligning safety directly at the token-level
under randomized masking, A2D achieves robustness to both any-decoding-order
and any-step prefilling attacks under various conditions. It also enables
real-time monitoring: dLLMs may begin a response but automatically terminate if
unsafe continuation emerges. On safety benchmarks, A2D consistently prevents
the generation of harmful outputs, slashing DIJA success rates from over 80% to
near-zero (1.3% on LLaDA-8B-Instruct, 0.0% on Dream-v0-Instruct-7B), and
thresholded [EOS] probabilities allow early rejection, yielding up to 19.3x
faster safe termination.

</details>


### [697] [Dual-Space Smoothness for Robust and Balanced LLM Unlearning](https://arxiv.org/abs/2509.23362)
*Han Yan,Zheyuan Liu,Meng Jiang*

Main category: cs.CL

TL;DR: 随着大语言模型发展，现有机器遗忘方法有问题，提出PRISM框架解决，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA机器遗忘方法存在灾难性遗忘、指标失衡问题，且易受再学习和越狱攻击，需改进。

Method: 提出PRISM框架，包含表示空间和参数空间两个平滑优化阶段。

Result: 在WMDP和MUSE上的实验表明，PRISM在多种攻击下优于SOTA基线，且关键指标间平衡更好。

Conclusion: PRISM框架能提升机器遗忘的鲁棒性和指标平衡性。

Abstract: With the rapid advancement of large language models, Machine Unlearning has
emerged to address growing concerns around user privacy, copyright
infringement, and overall safety. Yet state-of-the-art (SOTA) unlearning
methods often suffer from catastrophic forgetting and metric imbalance, for
example by over-optimizing one objective (e.g., unlearning effectiveness,
utility preservation, or privacy protection) at the expense of others. In
addition, small perturbations in the representation or parameter space can be
exploited by relearn and jailbreak attacks. To address these challenges, we
propose PRISM, a unified framework that enforces dual-space smoothness in
representation and parameter spaces to improve robustness and balance
unlearning metrics. PRISM consists of two smoothness optimization stages: (i) a
representation space stage that employs a robustly trained probe to defend
against jailbreak attacks, and (ii) a parameter-space stage that decouples
retain-forget gradient conflicts, reduces imbalance, and smooths the parameter
space to mitigate relearning attacks. Extensive experiments on WMDP and MUSE,
across conversational-dialogue and continuous-text settings, show that PRISM
outperforms SOTA baselines under multiple attacks while achieving a better
balance among key metrics.

</details>


### [698] [MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction](https://arxiv.org/abs/2509.23368)
*Xinchun Su,Chunxu Luo,Yixuan Li,Weidong Yang,Lipeng Ma*

Main category: cs.CL

TL;DR: 提出MedCritical两阶段框架，让小语言模型自对抗学习解决医学复杂推理任务，成本低且效果好，在CMExam基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 医学复杂推理任务挑战大，小语言模型表现不佳，基于知识蒸馏的方法有成本、时间和效率问题。

Method: 提出两阶段框架MedCritical，第一阶段从大模型提取思想模板引导小模型，第二阶段引入DPO通过自迭代协作增强小模型推理能力。

Result: MedCritical模型以低成本取得与传统知识蒸馏方法相当的结果，MedCritical 7B模型在CMExam基准上优于Taiyi和Huatuo - o1 - 7B模型。

Conclusion: MedCritical框架能有效解决医学复杂推理任务，以低成本提升小语言模型性能。

Abstract: In the field of medicine, complex reasoning tasks such as clinical diagnosis,
treatment planning, and medical knowledge integration pose significant
challenges, where small language models often underperform compared to large
language models like GPT-4 and Deepseek. Recent knowledge distillation-based
methods aim to address these issues through teacher-guided error correction,
but this LLM as judge approach remains challenging in terms of cost, time, and
efficiency. To circumvent this issue, we propose a novel two-stage framework,
MedCritical, which uses a small language model fine-tuned by a large teacher
model to play against itself. In the first stage, we extract high-level and
detailed long-chain thought templates from the teacher model to guide the
student model to generate more complex reasoning thoughts. In the second stage,
we introduce direct preference optimization (DPO) through model self-iteration
collaboration to enhance the reasoning ability of the student model by playing
against the correction trajectory of the fine-tuned model during training. This
model self-learning DPO approach teaches the student model to use its own
error-driven insights to consolidate its skills and knowledge to solve complex
problems, and achieves comparable results to traditional knowledge distillation
methods using teacher models at a lower cost. Notably, our MedCritical 7B model
outperforms the Taiyi and Huatuo-o1-7B models by 3.04\% and 10.12\%
respectively on the CMExam benchmark, achieving new SOTA performance among
7B-class small models.

</details>


### [699] [Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization](https://arxiv.org/abs/2509.23371)
*Junming Yang,Ning Xu,Biao Liu,Shiqi Qiao,Xin Geng*

Main category: cs.CL

TL;DR: 提出Meta - Weighted Adaptive Preference Optimization (MetaAPO)框架解决大语言模型偏好优化中数据分布不匹配问题，实验表明其性能优于现有方法并降低在线标注成本。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型偏好优化过程中预收集的离线偏好数据与不断演变的模型策略之间的分布不匹配问题，且现有方法无法适应模型动态学习状态。

Method: 提出MetaAPO框架，使用轻量级元学习器作为“对齐差距估计器”评估在线策略采样相对离线数据的潜在好处，引导有针对性的在线生成并为优化目标分配样本级元权重。

Result: 在AlpacaEval 2、Arena - Hard和MT - Bench上的实验表明，MetaAPO在各种设置下始终优于现有偏好优化方法，同时降低42%的在线标注成本。

Conclusion: MetaAPO是一种有效的解决大语言模型偏好优化数据分布不匹配问题的方法。

Abstract: Preference optimization is crucial for aligning large language models (LLMs)
with human values and intentions. A significant challenge in this process is
the distribution mismatch between pre-collected offline preference data and the
evolving model policy. Existing methods attempt to reduce this gap using static
heuristics or decoupled online sampling strategies, but they often fail to
adapt to the model's dynamic learning state. To bridge this gap, we propose
Meta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework
that dynamically couples data generation with model training. MetaAPO employs a
lightweight meta-learner, as an "alignment gap estimator", to evaluate the
potential benefits of on-policy sampling in relation to offline data. This
guides targeted online generation and assigns sample-wise meta-weights to the
optimization objective, dynamically balancing the quality and distribution of
online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench
demonstrate that MetaAPO consistently outperforms existing preference
optimization approaches across various settings, while reducing 42% in online
annotation costs.

</details>


### [700] [CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding](https://arxiv.org/abs/2509.23379)
*Xi Zhang,Zaiqiao Meng,Jake Lever,Edmond S. L. Ho*

Main category: cs.CL

TL;DR: 本文指出放射学多模态大语言模型存在医学幻觉问题，提出无训练无检索的Clinical Contrastive Cecoding（CCD）框架缓解该问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在放射学中存在医学幻觉问题，提示诱导幻觉普遍存在，影响医疗应用准确性。

Method: 引入Clinical Contrastive Cecoding（CCD），一个无训练无检索的推理框架，集成特定任务放射学专家模型的结构化临床信号，采用双阶段对比机制优化生成时的词元级对数几率。

Result: 在三个数据集和多个模型上的实验表明，CCD持续提升放射学报告生成的整体性能，在MIMIC - CXR数据集上应用于先进模型时，RadGraph - F1最多提高17%。

Conclusion: CCD是缓解医学幻觉的轻量级、通用解决方案，有效连接放射学中的专家模型和多模态大语言模型。

Abstract: Multimodal large language models (MLLMs) have recently achieved remarkable
progress in radiology by integrating visual perception with natural language
understanding. However, they often generate clinically unsupported
descriptions, known as medical hallucinations, which pose serious risks in
medical applications that demand accuracy and image-grounded outputs. Through
empirical analysis, we find that prompt-induced hallucinations remain prevalent
in radiology MLLMs, largely due to over-sensitivity to clinical sections. To
address this, we introduce Clinical Contrastive Cecoding (CCD), a training-free
and retrieval-free inference framework that integrates structured clinical
signals from task-specific radiology expert models. CCD introduces a dual-stage
contrastive mechanism to refine token-level logits during generation, thereby
enhancing clinical fidelity without modifying the base MLLM. Experiments on
three datasets and multiple models demonstrate that CCD consistently improves
overall performance on radiology report generation (RRG). On the MIMIC-CXR
dataset, it yields up to a 17% improvement in RadGraph-F1 when applied to
state-of-the-art RRG models. Our approach provides a lightweight and
generalisable solution for mitigating medical hallucinations, effectively
bridging expert models and MLLMs in radiology.

</details>


### [701] [Train Once, Answer All: Many Pretraining Experiments for the Cost of One](https://arxiv.org/abs/2509.23383)
*Sebastian Bordt,Martin Pawelczyk*

Main category: cs.CL

TL;DR: 本文提出在单次训练中同时进行多个预训练实验以解决计算成本高的问题，通过实验验证了方法可行性，虽有实验间交互但可忽略，该方法能在预算内对大模型进行严谨科学实验。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型预训练计算成本高这一显著约束。

Method: 在单次训练运行中同时进行多个预训练实验。

Result: 在1.5B参数模型对210B token训练中进行十次实验，能复现先前工作结果，开展新研究，实验对模型训练动态和整体性能影响小，实验间交互可忽略。

Conclusion: 在单次训练运行中进行多个预训练实验可在计算预算内对大模型进行严谨科学实验。

Abstract: Recent work has demonstrated that controlled pretraining experiments are a
powerful tool for understanding learning, reasoning, and memorization in large
language models (LLMs). However, the computational cost of pretraining presents
a significant constraint. To overcome this constraint, we propose to conduct
multiple pretraining experiments simultaneously during a single training run.
We demonstrate the feasibility of this approach by conducting ten experiments
during the training of a 1.5B parameter model on 210B tokens. Although we only
train a single model, we can replicate the results from multiple previous works
on data contamination, poisoning, and memorization. We also conduct novel
investigations into knowledge acquisition, mathematical reasoning, and
watermarking. For example, we dynamically update the training data until the
model acquires a particular piece of knowledge. Remarkably, the influence of
the ten experiments on the model's training dynamics and overall performance is
minimal. However, interactions between different experiments may act as a
potential confounder in our approach. We propose to test for interactions with
continual pretraining experiments, finding them to be negligible in our setup.
Overall, our findings suggest that performing multiple pretraining experiments
in a single training run can enable rigorous scientific experimentation with
large models on a compute budget.

</details>


### [702] [Retrieval-Constrained Decoding Reveals Underestimated Parametric Knowledge in Language Models](https://arxiv.org/abs/2509.23417)
*Rajaa El Hamdani,Samy Haffoudhi,Nils Holzenberger,Fabian Suchanek,Thomas Bonald,Fragkiskos D. Malliaros*

Main category: cs.CL

TL;DR: 研究指出语言模型评估可能低估参数知识，提出RCD策略，用YAGO - QA数据集评估，表明RCD可提升模型表现并公开代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 语言模型常因评估过严被误判答案错误，导致模型参数知识被低估。

Method: 提出Retrieval - Constrained Decoding (RCD)解码策略，引入YAGO - QA数据集评估开源语言模型。

Result: 标准解码低估模型知识，如Llama - 3.1 - 70B用普通解码F1分数32.3%，用RCD达46.0%；Llama - 3.1 - 8B用RCD达33.0%，超普通解码的大模型。

Conclusion: RCD策略能提升语言模型评估表现，避免知识被低估。

Abstract: Language models (LMs) encode substantial factual knowledge, but often produce
answers judged as incorrect. We hypothesize that many of these answers are
actually correct, but are expressed in alternative surface forms that are
dismissed due to an overly strict evaluation, leading to an underestimation of
models' parametric knowledge. We propose Retrieval-Constrained Decoding (RCD),
a decoding strategy that restricts model outputs to unique surface forms. We
introduce YAGO-QA, a dataset of 19,137 general knowledge questions. Evaluating
open-source LMs from 135M to 70B parameters, we show that standard decoding
undervalues their knowledge. For instance, Llama-3.1-70B scores only 32.3% F1
with vanilla decoding but 46.0% with RCD. Similarly, Llama-3.1-8B reaches 33.0%
with RCD, outperforming the larger model under vanilla decoding. We publicly
share the code and dataset at https://github.com/Rajjaa/disambiguated-LLM.

</details>


### [703] [Text-Based Approaches to Item Difficulty Modeling in Large-Scale Assessments: A Systematic Review](https://arxiv.org/abs/2509.23486)
*Sydney Peters,Nan Zhang,Hong Jiao,Ming Li,Tianyi Zhou,Robert Lissitz*

Main category: cs.CL

TL;DR: 本文综述37篇2025年5月前发表的关于大规模评估中自动项目难度预测的文章，分析各研究情况，指出文本方法潜力大，最后讨论实践意义与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统项目难度建模方法耗时且成本高，需新方法解决挑战。

Method: 综述并综合分析37篇相关文章，对各研究的数据集、难度参数等方面进行详细描述。

Result: 经典机器学习模型因可解释性仍有价值，先进语言模型无需手动特征工程，文本方法预测项目难度表现良好。

Conclusion: 讨论了自动项目难度建模的实践意义并指出未来研究方向。

Abstract: Item difficulty plays a crucial role in test performance, interpretability of
scores, and equity for all test-takers, especially in large-scale assessments.
Traditional approaches to item difficulty modeling rely on field testing and
classical test theory (CTT)-based item analysis or item response theory (IRT)
calibration, which can be time-consuming and costly. To overcome these
challenges, text-based approaches leveraging machine learning and language
models, have emerged as promising alternatives. This paper reviews and
synthesizes 37 articles on automated item difficulty prediction in large-scale
assessment settings published through May 2025. For each study, we delineate
the dataset, difficulty parameter, subject domain, item type, number of items,
training and test data split, input, features, model, evaluation criteria, and
model performance outcomes. Results showed that although classic machine
learning models remain relevant due to their interpretability, state-of-the-art
language models, using both small and large transformer-based architectures,
can capture syntactic and semantic patterns without the need for manual feature
engineering. Uniquely, model performance outcomes were summarized to serve as a
benchmark for future research and overall, text-based methods have the
potential to predict item difficulty with root mean square error (RMSE) as low
as 0.165, Pearson correlation as high as 0.87, and accuracy as high as 0.806.
The review concludes by discussing implications for practice and outlining
future research directions for automated item difficulty modeling.

</details>


### [704] [The Impact of Role Design in In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.23501)
*Hamidreza Rouzegar,Masoud Makrehchi*

Main category: cs.CL

TL;DR: 研究在零样本和少样本学习场景中角色配置对大语言模型上下文学习性能的影响，发现基于角色的提示结构可提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 提示工程中对提示内角色设计的影响研究不足，需探究角色配置对大语言模型性能的影响。

Method: 使用OpenAI的GPT - 3.5和GPT - 4o以及Meta的Llama2 - 7b和Llama2 - 13b，在多个数据集上评估模型在情感分析、文本分类、问答和数学推理等任务中的表现。

Result: 基于角色的提示结构有提升大语言模型性能的潜力。

Conclusion: 基于角色的提示结构能增强大语言模型的性能。

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to generate
predictions based on prompts without additional fine-tuning. While prompt
engineering has been widely studied, the impact of role design within prompts
remains underexplored. This study examines the influence of role configurations
in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from
OpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models'
performance across datasets, focusing on tasks like sentiment analysis, text
classification, question answering, and math reasoning. Our findings suggest
the potential of role-based prompt structuring to enhance LLM performance.

</details>


### [705] [From Human Annotation to Automation: LLM-in-the-Loop Active Learning for Arabic Sentiment Analysis](https://arxiv.org/abs/2509.23515)
*Dania Refai,Alaa Dalaq,Doaa Dalaq,Irfan Ahmad*

Main category: cs.CL

TL;DR: 本文提出阿拉伯语情感分析的主动学习框架以降低标注成本，评估多种深度学习架构和标注策略，结果显示大语言模型辅助主动学习性能良好。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语情感分析因缺乏高质量标注数据集进展有限，主动学习和大语言模型辅助标注在阿拉伯语情境下研究少，旨在降低标注成本并保持高性能。

Method: 提出主动学习框架，评估LSTM、GRU和RNN三种深度学习架构，比较人工标注和大语言模型辅助标注两种策略，评估五种大语言模型。

Result: 大语言模型辅助主动学习与人工标注相比有竞争力或更优，如在Hunger Station数据集上LSTM用GPT - 4o生成标签仅450个样本达93%准确率，MASAC数据集上DeepSeek Chat 650个样本达82%准确率。

Conclusion: 大语言模型辅助主动学习在阿拉伯语情感分析中可有效降低标注成本并保持高性能。

Abstract: Natural language processing (NLP), particularly sentiment analysis, plays a
vital role in areas like marketing, customer service, and social media
monitoring by providing insights into user opinions and emotions. However,
progress in Arabic sentiment analysis remains limited due to the lack of large,
high-quality labeled datasets. While active learning has proven effective in
reducing annotation efforts in other languages, few studies have explored it in
Arabic sentiment tasks. Likewise, the use of large language models (LLMs) for
assisting annotation and comparing their performance to human labeling is still
largely unexplored in the Arabic context. In this paper, we propose an active
learning framework for Arabic sentiment analysis designed to reduce annotation
costs while maintaining high performance. We evaluate multiple deep learning
architectures: Specifically, long short-term memory (LSTM), gated recurrent
units (GRU), and recurrent neural networks (RNN), across three benchmark
datasets: Hunger Station, AJGT, and MASAC, encompassing both modern standard
Arabic and dialectal variations. Additionally, two annotation strategies are
compared: Human labeling and LLM-assisted labeling. Five LLMs are evaluated as
annotators: GPT-4o, Claude 3 Sonnet, Gemini 2.5 Pro, DeepSeek Chat, and LLaMA 3
70B Instruct. For each dataset, the best-performing LLM was used: GPT-4o for
Hunger Station, Claude 3 Sonnet for AJGT, and DeepSeek Chat for MASAC. Our
results show that LLM-assisted active learning achieves competitive or superior
performance compared to human labeling. For example, on the Hunger Station
dataset, the LSTM model achieved 93% accuracy with only 450 labeled samples
using GPT-4o-generated labels, while on the MASAC dataset, DeepSeek Chat
reached 82% accuracy with 650 labeled samples, matching the accuracy obtained
through human labeling.

</details>


### [706] [On the Shelf Life of Fine-Tuned LLM Judges: Future Proofing, Backward Compatibility, and Question Generalization](https://arxiv.org/abs/2509.23542)
*Janvijay Singh,Austin Xu,Yilun Zhou,Yefan Zhou,Dilek Hakkani-Tur,Shafiq Joty*

Main category: cs.CL

TL;DR: 本文研究微调裁判模型的保质期相关问题，发现未来适应性难、向后兼容性易，持续学习更平衡，模型对未见问题泛化不足。


<details>
  <summary>Details</summary>
Motivation: 标准评估忽略微调裁判模型在现实部署中的实际问题，需研究影响其保质期的方面。

Method: 在数学领域统一框架下，采用不同训练和测试分布、三种微调算法和三种基础模型进行研究。

Result: 未来适应性难，向后兼容性相对容易，DPO 训练模型性能提升，持续学习更平衡，模型对未见问题有性能下降。

Conclusion: 研究结果为面对不断变化的生成器时开发和部署裁判模型提供了实际考虑的见解。

Abstract: The LLM-as-a-judge paradigm is widely used in both evaluating free-text model
responses and reward modeling for model alignment and finetuning. Recently,
finetuning judges with judge-specific data has emerged as an often preferred
choice over directly prompting frontier models as judges, as the former
achieves better performance with smaller model sizes while being more robust to
common biases. However, the standard evaluation ignores several practical
concerns of finetuned judges regarding their real world deployment. In this
paper, we identify and formalize three aspects that affect the shelf life of
these judges: future proofing and backward compatibility -- how well judges
finetuned on responses by today's generator models perform on responses by
future models or past models, as well as question generalization -- how well
judges generalize to unseen questions at test time. We study these three
aspects in the math domain under a unified framework with varying train and
test distributions, three SFT- and DPO-based finetuning algorithms and three
different base models. Experiments suggest that future-proofing is challenging
for most models, while backward compatibility is relatively easy, with
DPO-trained models consistently improving performance. We further find that
continual learning provides a more balanced adaptation to shifts between older
and newer response distributions than training solely on stronger or weaker
responses. Moreover, all models observe certain degrees of performance
degradation when moving from questions seen during training to unseen ones,
showing that current judges do not fully generalize to unseen questions. These
findings provide insights into practical considerations for developing and
deploying judge models in the face of ever-changing generators.

</details>


### [707] [Automatic Speech Recognition for Greek Medical Dictation](https://arxiv.org/abs/2509.23550)
*Vardis Georgilas,Themos Stafylakis*

Main category: cs.CL

TL;DR: 本文旨在创建希腊语医学语音转录的特定领域系统，结合自动语音识别和文本校正模型，经特定领域微调实现更准确连贯转录，助力希腊医疗行业。


<details>
  <summary>Details</summary>
Motivation: 创建希腊语医学语音转录的特定领域系统，减少医疗人员手动文档负担，提高工作流程效率。

Method: 结合自动语音识别技术和文本校正模型，利用声学和文本建模，将现有语言和语音技术适配到希腊医学环境，进行特定领域微调。

Result: 系统实现了更准确和连贯的转录。

Conclusion: 系统有助于希腊医疗领域实用语言技术的发展。

Abstract: Medical dictation systems are essential tools in modern healthcare, enabling
accurate and efficient conversion of speech into written medical documentation.
The main objective of this paper is to create a domain-specific system for
Greek medical speech transcriptions. The ultimate goal is to assist healthcare
professionals by reducing the overload of manual documentation and improving
workflow efficiency. Towards this goal, we develop a system that combines
automatic speech recognition techniques with text correction model, allowing
better handling of domain-specific terminology and linguistic variations in
Greek. Our approach leverages both acoustic and textual modeling to create more
realistic and reliable transcriptions. We focused on adapting existing language
and speech technologies to the Greek medical context, addressing challenges
such as complex medical terminology and linguistic inconsistencies. Through
domain-specific fine-tuning, our system achieves more accurate and coherent
transcriptions, contributing to the development of practical language
technologies for the Greek healthcare sector.

</details>


### [708] [A Culturally-Rich Romanian NLP Dataset from "Who Wants to Be a Millionaire?" Videos](https://arxiv.org/abs/2506.05991)
*Alexandru-Gabriel Ganea,Antonia-Adelina Popovici,Adrian-Marius Dumitran*

Main category: cs.CL

TL;DR: 研究引入基于罗马尼亚游戏节目创建的多语言数据集，对大语言模型进行基准测试，发现模型在国际问题和罗马尼亚特定文化问题上表现有差异，为构建跨文化NLP系统提供见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在不同语言和文化背景下表现不同，需研究文化背景和数据源对其性能的影响。

Method: 结合OCR、自动文本提取和手动验证收集问答对，用元数据丰富；在数据集上对模型进行基准测试，开展机器翻译和跨语言测试实验。

Result: 模型在国际问题上准确率80 - 95%，在罗马尼亚特定文化问题上准确率50 - 75%。

Conclusion: 文化背景和数据源影响大语言模型性能，研究为构建有文化意识的多语言NLP系统提供实用见解，数据集公开。

Abstract: Large Language Models (LLMs) demonstrate varying performance across languages
and cultural contexts. This study introduces a novel, culturally-rich,
multilingual dataset derived from video recordings of the Romanian game show
"Who Wants to Be a Millionaire?" (Vrei s\u{a} fii Milionar?). We employed an
innovative process combining optical character recognition (OCR), automated
text extraction, and manual verification to collect question-answer pairs,
enriching them with metadata including question domain (e.g., biology,
history), cultural relevance (Romanian-specific vs. international), and
difficulty. Benchmarking state-of-the-art LLMs, including Romanian-adapted
models, on this dataset revealed significant performance disparities: models
consistently achieve higher accuracy (80-95%) on international questions
compared to Romanian-specific cultural questions (50-75%). We further
investigate these differences through experiments involving machine translation
of Romanian questions into English and cross-lingual tests using a comparable
dataset in French. Our findings underscore the impact of cultural context and
data source on LLM performance and offer practical insights for building
robust, culturally-aware multilingual NLP systems, especially in educational
domains. The dataset is publicly available at Hugging Face.

</details>


### [709] [VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs](https://arxiv.org/abs/2506.22694)
*Raghavv Goel,Sudhanshu Agrawal,Mukul Gagrani,Junyoung Park,Yifan Zao,He Zhang,Tian Liu,Yiping Yang,Xin Yuan,Jiuyan Lu,Chris Lott,Mingu Lee*

Main category: cs.CL

TL;DR: 提出VocabTrim技术改善基于起草器的推测解码方法性能，减少起草开销，提高生成速度。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码在起草过程存在不必要推理开销，尤其对大词汇量目标大语言模型。

Method: 提出VocabTrim技术，重构起草器语言模型头，仅包含目标模型词汇表中最常采样的有限令牌集。

Result: 限制词汇量虽使接受率略有下降，但显著减少内存受限过程中的起草延迟，Llama - 3模型在Spec - Bench上内存受限加速比提高，如Llama - 3.2 - 3B - Instruct提高16%。

Conclusion: VocabTrim技术能有效提高基于起草器的推测解码方法在内存受限环境下的生成速度。

Abstract: In this paper, we introduce a simple training-free technique to improve the
performance of drafter-based speculative decoding (SpD) methods that
incorporates language modeling head (LM head) during drafting process. A
drafter-based speculative decoding leverages one or more smaller language
models, a.k.a. drafters or draft models, to sample a draft sequence or tree
consisting of multiple tokens, followed by verification by a base LLM, a target
model, accepting a subset as its valid generation. As it is usually considered
that the speculative decoding requires one-to-one mapping between vocabularies
of the target model and the draft model, it has been natural to share the
vocabulary between them, or even share the LM head as in EAGLE or Medusa. We
first identify that this draft token sampling scheme inherently contains an
unnecessary inference overhead in drafting, especially for some target LLMs
with very large vocabularies. Then, we propose a simple technique, VocabTrim,
to mitigate the drafting overhead to improve the generation speed in
memory-bound environment. VocabTrim reconstructs the drafter LM head to contain
only a limited set of tokens, selected by the most frequently sampled from the
vocabulary of the target model. While limiting the vocabulary in drafting
slightly degrades the acceptance rate, it significantly reduces the drafting
latency in memory-bound process which is often the case on edge devices,
resulting in higher memory-bound speed up (MBSU). We show that our method can
boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically
by 16% for Llama-3.2-3B-Instruct.

</details>


### [710] [GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs](https://arxiv.org/abs/2508.14279)
*Adrian-Marius Dumitran,Alexandra-Mihaela Danila,Angela-Liliana Dumitran*

Main category: cs.CL

TL;DR: 提出GRILE基准测试，评估大语言模型在罗马尼亚语教育场景的表现，发现多数模型存在问题并公开相关资源。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型对低资源语言的教学价值。

Method: 创建GRILE基准测试，包含1151道选择题，评估7个模型的答题和解释能力。

Result: Gemini 2.5 Pro准确率达83%，多数开源模型低于65%，48%的解释有问题，分析出模型在词法和正字法规范上的弱点。

Conclusion: 揭示低资源环境下可信教育NLP面临的挑战，确立GRILE作为可控解释生成和评估的新测试平台。

Abstract: LLMs (Large language models) have revolutionized NLP (Natural Language
Processing), yet their pedagogical value for low-resource languages remains
unclear. We present GRILE (Grammar Romanian Inference and Language
Explanations) , the first open benchmark of 1,151 multiple-choice questions
harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate,
university admissions). GRILE enables us to probe two complementary abilities
of seven state-of-the-art multilingual and Romanian-specific LLMs: (i)
selecting the correct answer, and (ii) producing linguistically accurate
explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight
models stay below 65%, and 48% of their explanations contain factual or
pedagogical flaws according to expert review. A detailed error analysis
pinpoints systematic weaknesses in morphology and in applying the latest DOOM3
orthographic norms. All data, code and a public web demo are released to
catalyze future research. Our findings expose open challenges for trustworthy
educational NLP in low-resource settings and establish GRILE as a new test-bed
for controllable explanation generation and evaluation.

</details>


### [711] [Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales](https://arxiv.org/abs/2509.23574)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: 提出 MoRSD 方法用于 CoT 蒸馏，通过选择高质量推理依据提升小语言模型推理能力，在多数据集上有提升。


<details>
  <summary>Details</summary>
Motivation: 现有 CoT 蒸馏工作低估推理依据质量，主要关注数据量，可能向学生模型传递噪声或错误信息。

Method: 提出 Model-Oriented Rationale Selection Distillation (MoRSD) 方法来选择高质量推理依据进行蒸馏，还提出 Rationale Difficulty (RD) 指标衡量学生模型推理能力。

Result: 在三个任务的七个数据集上平均提升 4.6%，用更少推理依据，控制其准确性、多样性和难度。

Conclusion: 少量高质量推理依据比整个数据集更能增强学生模型推理能力，该方法是高效 CoT 蒸馏的可能解决方案。

Abstract: Chain-of-thought (CoT) distillation aims to enhance small language models'
(SLMs) reasoning by transferring multi-step reasoning capability from the
larger teacher models. However, existing work underestimates rationale quality,
focusing primarily on data quantity, which may transfer noisy or incorrect
information to the student model. To address the above issues, we proposed
\textbf{M}odel-\textbf{O}riented \textbf{R}ationale \textbf{S}election
\textbf{D}istillation (MoRSD), which can discern and select high quality
rationales for distillation to improve performance further. We further propose
a Rationale Difficulty (RD) metric to measure the ability of the student model
to generate the correct answer under a given rationale. Compared to the
baseline, we achieved 4.6$\%$ average improvement on seven datasets over three
tasks, using fewer rationales by controlling their accuracy, diversity, and
difficulty. Our results reveal that a small portion of the high quality
rationales can enhance the reasoning ability of student models than the entire
dataset. Our method promises to be a possible solution for efficient CoT
distillation. Our code will be released in https://github.com/Leon221220/MoRSD.

</details>


### [712] [Timber: Training-free Instruct Model Refining with Base via Effective Rank](https://arxiv.org/abs/2509.23595)
*Taiqiang Wu,Runming Yang,Tao Liu,Jiahao Wang,Zenan Xu,Ngai Wong*

Main category: cs.CL

TL;DR: 文章指出训练后微调使预训练基础模型变为指令模型很表面，有效秩变化可证明，提出无训练方法Timber增强指令模型探索能力并保留利用能力，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 现有训练后微调使指令模型存在提升利用能力时限制探索能力的权衡问题，需解决此问题。

Method: 提出无训练方法Timber，通过对权重增量进行细微且有针对性的细化，使指令模型部分向基础模型回归。

Result: 在Llama和Qwen系列上的大量实验表明，Timber持续改进了普通指令模型，特别是在Pass@k性能上。

Conclusion: 研究在权重层面为训练后阶段提供了新见解，以及无需训练就能优化指令模型的实用策略。

Abstract: Post-training, which elicits a pretrained Base model into the corresponding
Instruct model, is widely considered to be superficial. In this work, we first
reinforce this hypothesis by providing novel quantitative evidence from the
weight level that the effective rank (eRank) remains negligibly changed.
However, this superficiality also suffers a critical trade-off, improving the
exploitation capabilities at the cost of limiting its exploration. To tackle
this issue, we propose Timber, a simple yet effective training-free method that
enhances the exploration capability of the Instruct model while preserving its
exploitation. The key insight is to partially revert Instruct towards the
paired Base model by subtle yet targeted refinement of the weight deltas.
Extensive experiments on Llama and Qwen series demonstrate that Timber
consistently improves vanilla Instruct models, particularly on Pass@k
performance. Our findings offer new insights into the post-training stage at
the weight level and practical strategies to refine the Instruct model without
training.

</details>


### [713] [Enabling Approximate Joint Sampling in Diffusion LMs](https://arxiv.org/abs/2509.22738)
*Parikshit Bansal,Sujay Sanghavi*

Main category: cs.CL

TL;DR: 本文提出在全模型单次前向传播中近似从联合分布采样多个token的方法，在语言建模和数学、编码任务上证明了有效性。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型并行解掩码token时，并行解掩码的token越多，生成字符串越偏离真实联合分布，导致准确率下降。

Method: 在现有大型扩散语言模型上开发轻量级单层“采样器”，全模型一次前向传播后，通过采样器层多次前向传播解掩码多个token，采样器经训练以模仿全模型的精确联合采样。

Result: 在预训练和指令微调模型的语言建模及数学、编码任务中，每次全模型去噪步骤解掩码4个token时，采样算法相对于真实联合分布的MAUVE分数达0.87（边际基线为0.31）。

Conclusion: 提出的近似联合采样方法有效，能在保证一定准确性的同时提高生成速度。

Abstract: In autoregressive language models, each token is sampled by conditioning on
all the past tokens; the overall string has thus been sampled from the correct
underlying joint distribution represented by the model. In contrast, masked
diffusion language models generate text by unmasking tokens out of order and
potentially in parallel. Generating an overall string sampled from the correct
underlying joint distribution would (again) require exactly one token unmasking
in every full-model forward pass. The more tokens unmasked in parallel, the
further away the string is from the true joint; this can be seen in the
resulting drop in accuracy (but, increase in speed). In this paper we devise a
way to {\em approximately} sample multiple tokens from the joint distribution
in a single full-model forward pass; we do so by developing a new lightweight
single-layer ``sampler" on top of an existing large diffusion LM. One forward
pass of the full model can now be followed by multiple forward passes of only
this sampler layer, to yield multiple unmasked tokens. Our sampler is trained
to mimic exact joint sampling from the (frozen) full model. We show the
effectiveness of our approximate joint sampling for both pretrained-only
(Dream-7B-Base) and instruction-tuned (Dream-7B-Instruct) models on language
modeling and math \& coding tasks. When four tokens are unmasked for each
full-model denoising step, our sampling algorithm achieves a MAUVE score of
0.87 (vs marginal baseline of 0.31) with respect to the true joint
distribution.

</details>


### [714] [Aligning LLMs for Multilingual Consistency in Enterprise Applications](https://arxiv.org/abs/2509.23659)
*Amit Agarwal,Hansa Meghwani,Hitesh Laxmichand Patel,Tao Sheng,Sujith Ravi,Dan Roth*

Main category: cs.CL

TL;DR: 大语言模型在企业应用中因语言资源差异不可靠，提出批量对齐微调策略提升非英语准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型以英语为中心的预训练和内部推理偏差导致高资源和中低资源语言性能差距大，影响多语言场景应用，即便有RAG系统，非英语准确率仍有大幅下降。

Method: 提出实用的批量对齐微调策略，在每个训练批次中利用语义等效的多语言数据直接对齐模型跨语言输出。

Result: 该方法使非英语准确率提高达23.9%，且不影响英语性能、模型推理和检索质量。

Conclusion: 该方法简单易实现、可扩展，能无缝集成现有管道，可实现更强大公平的多语言AI解决方案。

Abstract: Large language models (LLMs) remain unreliable for global enterprise
applications due to substantial performance gaps between high-resource and
mid/low-resource languages, driven by English-centric pretraining and internal
reasoning biases. This inconsistency undermines customer experience and
operational reliability in multilingual settings such as customer support,
content moderation, and information retrieval. Even with advanced
Retrieval-Augmented Generation (RAG) systems, we observe up to an 29% accuracy
drop in non-English languages compared to English.
  We propose a practical, batch-wise alignment strategy for fine-tuning LLMs,
leveraging semantically equivalent multilingual data in each training batch to
directly align model outputs across languages. This approach improves
non-English accuracy by up to 23.9\% without compromising English performance,
model reasoning, or retrieval quality. Our method is simple to implement,
scalable, and integrates seamlessly with existing LLM training \& deployment
pipelines, enabling more robust and equitable multilingual AI solutions in
industry.

</details>


### [715] [Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning](https://arxiv.org/abs/2509.23744)
*Yucheng Wang,Yifan Hou,Aydin Javadov,Mubashara Akhtar,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 本文提出逻辑评估框架研究多模态大语言模型跨模态推理，发现集成而非感知是主要障碍，提出有前景的改进方向。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型跨模态推理研究不足，评估框架和模型内部分析缺乏，导致结果不一致。

Method: 提出逻辑评估框架，将多模态推理分为六种交互模式。

Result: 额外模态仅在提供独立充分推理路径时增强推理，存在三种性能下降情况，发现两个核心失败点。

Conclusion: 集成是多模态推理主要障碍，组合感知训练和早期融合控制是有前景的方向。

Abstract: Multimodal large language models (MLLMs) promise enhanced reasoning by
integrating diverse inputs such as text, vision, and audio. Yet cross-modal
reasoning remains underexplored, with conflicting reports on whether added
modalities help or harm performance. These inconsistencies stem from a lack of
controlled evaluation frameworks and analysis of models' internals to isolate
when and why modality interactions support or undermine reasoning. We address
this gap through a logic-grounded evaluation framework that categorizes
multimodal reasoning into six interaction patterns, varying how facts are
distributed across modalities and logically combined. Empirically, additional
modalities enhance reasoning only when they provide independent and sufficient
reasoning paths, while redundant or chained entailment support often hurts
performance. Moreover, reasoning degrades in three systematic ways: weaker
modalities drag down overall performance, conflicts bias preference toward
certain modalities, and joint signals from different modalities fail to be
integrated effectively. Therefore, we identify two core failures:
task-composition bottleneck, where recognition and reasoning cannot be jointly
executed in one pass, and fusion bottleneck, where early integration introduces
bias. For further investigation, we find that attention patterns fail to encode
fact usefulness, but a simple two-step prompting (recognize then reason)
restores performance, confirming the task-composition bottleneck. Moreover,
modality identity remains recoverable in early layers, and softening attention
in early fusion improves reasoning, highlighting biased fusion as another
failure mode. Overall, our findings show that integration, not perception, is
the main barrier to multimodal reasoning, suggesting composition-aware training
and early fusion control as promising directions.

</details>


### [716] [HEART: Emotionally-driven test-time scaling of Language Models](https://arxiv.org/abs/2509.22876)
*Gabriela Pinto,Palash Goyal,Yiwen Song,Souradip Chakraborty,Zifeng Wang,Tomas Pfister,Hamid Palangi*

Main category: cs.CL

TL;DR: 提出HEART框架，用情感驱动提示进行迭代自修正，在推理基准测试中取得成果，但在无验证器设置中有瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前测试时间缩放策略未利用情感反馈的引导潜力，受心理学研究启发引入新框架。

Method: 基于Paul Ekman的六种通用情绪，用情感驱动提示进行迭代自修正，系统改变反馈情感基调。

Result: 在推理基准测试中，有验证器引导时能显著提升推理深度和准确率，无验证器时难以持续利用优势。

Conclusion: 机器推理的未来不仅在于完善逻辑，还需理解和利用模型的“核心”。

Abstract: Test-time scaling has shown considerable success in improving the performance
of language models on complex reasoning tasks without requiring fine-tuning.
However, current strategies such as self-reflection primarily focus on logical
or structural refinement. They do not leverage the guiding potential of
affective feedback. Inspired by psychological research showing that emotions
can modulate cognitive performance, we introduce HEART--a novel framework that
uses emotionally-driven prompts for iterative self-correction. HEART provides
feedback on a model's incorrect response using a curated set of concise,
emotionally charged phrases based on the six universal emotions categorized by
Dr. Paul Ekman. By systematically varying the emotional tone of the feedback
across iterations, our method guides the model to escape flawed reasoning paths
and explore more promising alternatives. We evaluate our framework on
challenging reasoning benchmarks including OlympiadBench, Humanity's Last Exam,
and SimpleQA. Our results reveal a significant new phenomenon: when guided by
an oracle verifier, this affective iteration protocol unlocks significantly
deeper reasoning, leading to consistent and substantial increases in accuracy
over state-of-the-art baselines with the same verifier. However, we also
identify a critical bottleneck for practical deployment. In a verifier-free
setting, it struggles to harness these gains consistently, highlighting as a
key challenge for future work. Our findings suggest that the next frontier in
machine reasoning may lie not just in refining logic, but also in understanding
and leveraging the `HEART' of the models.

</details>


### [717] [Understanding Textual Capability Degradation in Speech LLMs via Parameter Importance Analysis](https://arxiv.org/abs/2509.23755)
*Chao Wang,Rui-Chen Zheng,Yang Ai,Zhen-Hua Ling*

Main category: cs.CL

TL;DR: 研究语音集成到LLMs中削弱文本能力问题，提出分析框架并研究两种缓解策略，实验证明策略有效。


<details>
  <summary>Details</summary>
Motivation: 语音集成到LLMs中会削弱其核心文本能力，限制对预训练文本知识的利用。

Method: 通过研究编码器 - 适配器范式分析问题机制，提出基于参数重要性估计的分析框架，研究层学习率调度和低秩适配（LoRA）两种缓解策略。

Result: 两种策略比全量微调能更好保持文本能力，还能提升下游语音问答性能。

Conclusion: 分析为缓解策略的有效性提供了原理性解释，将其益处与LLMs中文本知识的结构特性联系起来。

Abstract: The integration of speech into Large Language Models (LLMs) has substantially
expanded their capabilities, but often at the cost of weakening their core
textual competence. This degradation limits the ability of speech-enabled LLMs
to fully exploit their pre-trained text-based knowledge. In this work, we
analyze the underlying mechanisms of this issue through a focused study of the
widely used encoder-adaptor paradigm. We propose an analytical framework based
on parameter importance estimation, which reveals that fine-tuning for speech
introduces a textual importance distribution shift: the layer-wise allocation
of parameters critical to textual reasoning is disrupted. Building on this
insight, we investigate two mitigation strategies: layer-wise learning rate
scheduling and Low-Rank Adaptation (LoRA), both aim to preserve the original
parameter distribution. Experimental results show that both approaches better
maintain textual competence than full fine-tuning, while also improving
downstream spoken question answering performance. Furthermore, our analysis
offers a principled explanation for the effectiveness of the proposed
mitigation strategies, linking their benefits to the structural properties of
textual knowledge in LLMs.

</details>


### [718] [Knowledge-Level Consistency Reinforcement Learning: Dual-Fact Alignment for Long-Form Factuality](https://arxiv.org/abs/2509.23765)
*Junliang Li,Yucheng Wang,Yan Chen,Yu Ran,Ruiqing Zhang,Jing Liu,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 提出KLCF框架解决大语言模型长文本生成中的幻觉和事实性问题，实验证明其有效提升事实性指标并缓解幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF框架忽视模型内部知识边界，加剧“幻觉税”，需解决大语言模型长文本生成中的可靠性问题。

Method: 提出KLCF框架，关注策略模型和基础模型知识一致性，引入双事实对齐机制；利用预训练知识边界构建事实清单，训练自我评估模块；奖励设计无需外部知识且轻量级。

Result: KLCF在多个长文本基准测试中大幅提高事实性指标，有效缓解模型幻觉。

Conclusion: KLCF框架能有效提升大语言模型长文本生成的事实性和可靠性，且高效可扩展。

Abstract: Hallucination and factuality deficits remain key obstacles to the reliability
of large language models (LLMs) in long-form generation. Existing reinforcement
learning from human feedback (RLHF) frameworks primarily rely on preference
rewards, yet they often overlook the model's internal knowledge boundaries,
exacerbating the so-called "hallucination tax". To address this challenge, we
propose Knowledge-Level Consistency Reinforcement Learning Framework (KLCF), a
novel framework that focuses on the knowledge consistency between the policy
model's expressed knowledge and the base model's parametric knowledge, and
introduces a Dual-Fact Alignment mechanism to jointly optimize factual recall
and precision. Specifically, KLCF leverages pretrained knowledge boundaries to
construct fact checklist, guiding online reinforcement learning to improve
factual coverage and recall; simultaneously, it trains a self-assessment module
based on the base model's internal knowledge to enhance factual precision
during generation. Unlike prior methods that rely on external retrieval or
heavy verification, our reward design is fully external-knowledge-free and
lightweight, making KLCF efficient and easily scalable to large-scale training.
Experimental results demonstrate that KLCF substantially improves factuality
metrics across multiple long-form benchmarks and effectively alleviates model
hallucinations.

</details>


### [719] [From Personal to Collective: On the Role of Local and Global Memory in LLM Personalization](https://arxiv.org/abs/2509.23767)
*Zehong Wang,Junlin Wu,ZHaoxuan Tan,Bolian Li,Xianrui Zhong,Zheli Liu,Qingkai Zeng*

Main category: cs.CL

TL;DR: 本文指出大语言模型个性化面临冷启动和偏差问题，提出LoGo框架结合本地和全局记忆，经实验验证可提升个性化质量。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型个性化中冷启动和偏差问题，根源是缺乏跨用户集体知识建模能力。

Method: 提出本地 - 全局记忆框架（LoGo），结合个性化本地记忆和捕捉群体共同兴趣的全局记忆，引入调解模块解决两者冲突。

Result: 在多个基准测试中，LoGo通过帮助冷启动用户和减少偏差预测，持续提高个性化质量。

Conclusion: 纳入集体知识对提升大语言模型个性化很重要。

Abstract: Large language model (LLM) personalization aims to tailor model behavior to
individual users based on their historical interactions. However, its
effectiveness is often hindered by two key challenges: the \textit{cold-start
problem}, where users with limited history provide insufficient context for
accurate personalization, and the \textit{biasing problem}, where users with
abundant but skewed history cause the model to overfit to narrow preferences.
We identify both issues as symptoms of a common underlying limitation, i.e.,
the inability to model collective knowledge across users. To address this, we
propose a local-global memory framework (LoGo) that combines the personalized
local memory with a collective global memory that captures shared interests
across the population. To reconcile discrepancies between these two memory
sources, we introduce a mediator module designed to resolve conflicts between
local and global signals. Extensive experiments on multiple benchmarks
demonstrate that LoGo consistently improves personalization quality by both
warming up cold-start users and mitigating biased predictions. These results
highlight the importance of incorporating collective knowledge to enhance LLM
personalization.

</details>


### [720] [How to Make Large Language Models Generate 100% Valid Molecules?](https://arxiv.org/abs/2509.23099)
*Wen Tao,Jing Tang,Alvin Chan,Bryan Hooi,Baolong Bi,Nanyun Peng,Yuansheng Liu,Yiwei Wang*

Main category: cs.CL

TL;DR: 本文探讨大语言模型生成有效分子的问题，评估其在不同表示法下的表现，提出SmiSelf框架，实验表明该框架能确保分子有效性且提升性能。


<details>
  <summary>Details</summary>
Motivation: 分子生成对药物发现和材料科学至关重要，但大语言模型在少样本下用SMILES生成有效分子有挑战，旨在探索其生成100%有效分子的方法。

Method: 评估大语言模型使用SELFIES进行有效分子生成的表现，考察其纠正无效SMILES的能力，提出SmiSelf框架转换并纠正无效SMILES。

Result: 大语言模型用SELFIES表现不如SMILES，纠正无效SMILES能力有限，SmiSelf能确保100%有效性，保留分子特征并提升性能。

Conclusion: SmiSelf有助于拓展大语言模型在生物医学的实际应用，且与所有基于SMILES的生成模型兼容。

Abstract: Molecule generation is key to drug discovery and materials science, enabling
the design of novel compounds with specific properties. Large language models
(LLMs) can learn to perform a wide range of tasks from just a few examples.
However, generating valid molecules using representations like SMILES is
challenging for LLMs in few-shot settings. In this work, we explore how LLMs
can generate 100% valid molecules. We evaluate whether LLMs can use SELFIES, a
representation where every string corresponds to a valid molecule, for valid
molecule generation but find that LLMs perform worse with SELFIES than with
SMILES. We then examine LLMs' ability to correct invalid SMILES and find their
capacity limited. Finally, we introduce SmiSelf, a cross-chemical language
framework for invalid SMILES correction. SmiSelf converts invalid SMILES to
SELFIES using grammatical rules, leveraging SELFIES' mechanisms to correct the
invalid SMILES. Experiments show that SmiSelf ensures 100% validity while
preserving molecular characteristics and maintaining or even enhancing
performance on other metrics. SmiSelf helps expand LLMs' practical applications
in biomedicine and is compatible with all SMILES-based generative models. Code
is available at https://github.com/wentao228/SmiSelf.

</details>


### [721] [Tree Reward-Aligned Search for TReASURe in Masked Diffusion Language Models](https://arxiv.org/abs/2509.23146)
*Zichao Yu,Ming Li,Wenyi Zhang,Weiguo Gao*

Main category: cs.CL

TL;DR: 提出TReASURe方法解决树搜索应用于掩码扩散语言模型的问题，理论分析有优势，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 树搜索应用于掩码扩散语言模型存在并行解掩码分支相关性高、奖励评估方差大的问题。

Method: 提出TReASURe方法，包含基于首次命中解掩码的UnmaskBranch分支策略和使用确定性重代入的ResubstituteScore剪枝规则。

Result: 在困惑度、语言可接受性、情感和毒性控制方面取得SOTA结果，在低函数评估数制度下提升显著。

Conclusion: TReASURe方法能有效解决相关问题，优于先前方法。

Abstract: Tree search has recently emerged as a powerful framework for aligning
generative models with task-specific rewards at test time. Applying tree search
to Masked Diffusion Language Models, however, introduces two key challenges:
(i) parallel unmasking yields highly correlated branches, limiting exploration,
and (ii) reward evaluation via sampled completions produces high-variance
estimates, making pruning unstable. We propose TReASURe, a tree-search
test-time alignment method that addresses these issues. It introduces (i)
UnmaskBranch, a branching strategy based on first-hitting unmasking that
diversifies both token content and reveal order with a single model call per
parent node, and (ii) ResubstituteScore, a pruning rule that uses deterministic
resubstitution to score partially masked sequences with low-variance proxy
completions. Theoretically, we quantify branching efficiency gains in NFEs
(number of function evaluations), show that the scoring rule approximates the
true reward with error bounded by predictive uncertainty, and prove
improvements with larger tree widths. Empirically, TReASURe achieves
state-of-the-art results on perplexity, linguistic acceptability, and control
of sentiment and toxicity, outperforming prior methods under matched compute
budgets, with especially strong gains in low-NFE regimes.

</details>


### [722] [Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces](https://arxiv.org/abs/2509.23291)
*Joseph Marvin Imperial,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: 本文引入Policy Reasoning Traces (PRT)提升大语言模型政策合规评估能力，在HIPAA和GDPR政策上取得新SOTA。


<details>
  <summary>Details</summary>
Motivation: 人工标注专家级推理过程成本高，需要提升大语言模型政策合规评估能力。

Method: 引入Policy Reasoning Traces (PRT)作为推理桥梁，在推理和训练阶段使用。

Result: 使用PRT显著提升开放权重和商业模型性能，在HIPAA和GDPR政策上达到新SOTA。

Conclusion: PRT不仅提高准确率，还能提升大语言模型引用政策条款的能力，并通过原始思维链的高利用率影响合规决策。

Abstract: Policy compliance assessment is a fundamental task of evaluating whether an
input case strictly complies with a set of human-defined rules, more generally
known as policies. In practice, human experts follow a systematic, step-by-step
process to identify violations with respect to specific stipulations outlined
in the policy. However, such documentation of gold-standard, expert-level
reasoning processes is costly to acquire. In this paper, we introduce Policy
Reasoning Traces (PRT), a form of specialized generated reasoning chains that
serve as a reasoning bridge to improve an LLM's policy compliance assessment
capabilities. Our empirical evaluations demonstrate that the use of PRTs for
both inference-time and training-time scenarios significantly enhances the
performance of open-weight and commercial models, setting a new
state-of-the-art for HIPAA and GDPR policies. Beyond accuracy gains, we also
highlight how PRTs can improve an LLM's ability to accurately cite policy
clauses, as well as influence compliance decisions through their high
utilization from the raw chains of thought.

</details>


### [723] [Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step](https://arxiv.org/abs/2509.23924)
*Jingyi Yang,Guanxu Chen,Xuhao Hu,Jing Shao*

Main category: cs.CL

TL;DR: 本文研究MDLMs解码策略和强化学习算法，提出EOSER、ASS解码调度器和CJ - GRPO算法，实验证明其能有效驯服MDLMs。


<details>
  <summary>Details</summary>
Motivation: MDLMs虽有优势，但适配的解码策略和强化学习算法研究不足，直接迁移AR模型技术非最优。

Method: 提出EOSER和ASS解码调度器以实现全扩散式解码，引入CJ - GRPO强调轨迹一致性并减少优化误差。

Result: 在推理任务实验中，EOSER、ASS和CJ - GRPO表现出色。

Conclusion: 所提的EOSER、ASS机制和CJ - GRPO有潜力有效且高效地驯服MDLMs。

Abstract: Masked diffusion language models (MDLMs) have recently emerged as a promising
alternative to autoregressive (AR) language models, offering properties such as
parallel decoding, flexible generation orders, and the potential for fewer
inference steps. Despite these advantages, decoding strategies and
reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored.
A naive approach is to directly transfer techniques well-established for AR
models to MDLMs. However, this raises an immediate question: Is such a naive
transfer truly optimal? For example, 1) Block-wise and semi-AR decoding
strategies are not employed during the training of MDLMs, so why do they
outperform full diffusion-style decoding during inference? 2) Applying RL
algorithms designed for AR models directly to MDLMs exhibits a
training-inference inconsistency, since MDLM decoding are non-causal
(parallel). This results in inconsistencies between the rollout trajectory and
the optimization trajectory. To address these challenges, we propose EOS Early
Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which
unlock the potential of MDLMs to perform full diffusion-style decoding,
achieving competitive performance with fewer decoding steps. Additionally, we
introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)
for taming MDLMs, which emphasizes the consistency between rollout trajectory
and optimization trajectory, and reduces the optimization errors caused by
skip-step optimization. We conduct extensive experiments on reasoning tasks,
such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The
results demonstrate that the proposed EOSER and ASS mechanisms, together with
CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs.
Code: https://github.com/yjyddq/EOSER-ASS-RL.

</details>


### [724] [Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking in Full-Duplex Spoken Dialogue Systems](https://arxiv.org/abs/2509.23938)
*Guojian Li,Chengyou Wang,Hongfei Xue,Shuiyuan Wang,Dehui Gao,Zihan Zhang,Yuke Lin,Wenjie Li,Longshuai Xiao,Zhonghua Fu,Lei Xie*

Main category: cs.CL

TL;DR: 提出开源模块化话轮检测模型Easy Turn，集成双模态信息预测四种对话话轮状态，发布训练集，在测试集上达SOTA，数据和模型将公开。


<details>
  <summary>Details</summary>
Motivation: 现有全双工交互话轮检测方案存在未开源、参数大、单模态、缺数据等问题。

Method: 提出Easy Turn模型集成声学和语言双模态信息预测四种对话话轮状态，发布Easy Turn trainset训练集。

Result: 相比现有开源模型，在Easy Turn测试集上实现了最先进的话轮检测准确率。

Conclusion: 所提模型和数据集可推动全双工交互话轮检测发展，数据和模型将在GitHub公开。

Abstract: Full-duplex interaction is crucial for natural human-machine communication,
yet remains challenging as it requires robust turn-taking detection to decide
when the system should speak, listen, or remain silent. Existing solutions
either rely on dedicated turn-taking models, most of which are not
open-sourced. The few available ones are limited by their large parameter size
or by supporting only a single modality, such as acoustic or linguistic.
Alternatively, some approaches finetune LLM backbones to enable full-duplex
capability, but this requires large amounts of full-duplex data, which remain
scarce in open-source form. To address these issues, we propose Easy Turn, an
open-source, modular turn-taking detection model that integrates acoustic and
linguistic bimodal information to predict four dialogue turn states: complete,
incomplete, backchannel, and wait, accompanied by the release of Easy Turn
trainset, a 1,145-hour speech dataset designed for training turn-taking
detection models. Compared to existing open-source models like TEN Turn
Detection and Smart Turn V2, our model achieves state-of-the-art turn-taking
detection accuracy on our open-source Easy Turn testset. The data and model
will be made publicly available on GitHub.

</details>


### [725] [Vision-Grounded Machine Interpreting: Improving the Translation Process through Visual Cues](https://arxiv.org/abs/2509.23957)
*Claudio Fantinuoli*

Main category: cs.CL

TL;DR: 本文介绍了视觉接地口译（VGI）方法以解决单模态机器口译的局限性，构建诊断语料库评估，结果显示视觉接地能改善词汇消歧等，认为多模态是提升机器口译质量的必要步骤。


<details>
  <summary>Details</summary>
Motivation: 单模态机器口译系统仅依赖语言信号，在需要额外线索进行消歧和确保准确性的场景中性能受限。

Method: 引入VGI方法，构建集成视觉 - 语言模型的原型系统处理语音和视觉输入，构造针对三种歧义的诊断语料库进行评估。

Result: 视觉接地显著改善词汇消歧，对性别分辨有适度且不稳定的提升，对句法歧义无益处。

Conclusion: 采用多模态是提升机器口译翻译质量的必要举措。

Abstract: Machine Interpreting systems are currently implemented as unimodal, real-time
speech-to-speech architectures, processing translation exclusively on the basis
of the linguistic signal. Such reliance on a single modality, however,
constrains performance in contexts where disambiguation and adequacy depend on
additional cues, such as visual, situational, or pragmatic information. This
paper introduces Vision-Grounded Interpreting (VGI), a novel approach designed
to address the limitations of unimodal machine interpreting. We present a
prototype system that integrates a vision-language model to process both speech
and visual input from a webcam, with the aim of priming the translation process
through contextual visual information. To evaluate the effectiveness of this
approach, we constructed a hand-crafted diagnostic corpus targeting three types
of ambiguity. In our evaluation, visual grounding substantially improves
lexical disambiguation, yields modest and less stable gains for gender
resolution, and shows no benefit for syntactic ambiguities. We argue that
embracing multimodality represents a necessary step forward for advancing
translation quality in machine interpreting.

</details>


### [726] [Comparison of Scoring Rationales Between Large Language Models and Human Raters](https://arxiv.org/abs/2509.23412)
*Haowei Hua,Hong Jiao,Dan Song*

Main category: cs.CL

TL;DR: 研究探索用大语言模型进行自动评分及评估评分理由，以提升对评分背后逻辑的理解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于自动评分且能给出评分理由，评估人类和模型评分理由可提高对评分推理过程的理解，本研究旨在找出评分不一致的潜在原因。

Method: 使用大规模测试文章，基于二次加权卡帕和归一化互信息检验GPT - 4o、Gemini等大语言模型的评分准确性；用余弦相似度评估理由相似度；基于理由嵌入使用主成分分析探索理由聚类模式。

Result: 文中未明确提及具体结果。

Conclusion: 研究结果有助于理解大语言模型自动评分的准确性和‘思维’方式，提升对人类评分和基于大语言模型自动评分背后理由的理解。

Abstract: Advances in automated scoring are closely aligned with advances in
machine-learning and natural-language-processing techniques. With recent
progress in large language models (LLMs), the use of ChatGPT, Gemini, Claude,
and other generative-AI chatbots for automated scoring has been explored. Given
their strong reasoning capabilities, LLMs can also produce rationales to
support the scores they assign. Thus, evaluating the rationales provided by
both human and LLM raters can help improve the understanding of the reasoning
that each type of rater applies when assigning a score. This study investigates
the rationales of human and LLM raters to identify potential causes of scoring
inconsistency. Using essays from a large-scale test, the scoring accuracy of
GPT-4o, Gemini, and other LLMs is examined based on quadratic weighted kappa
and normalized mutual information. Cosine similarity is used to evaluate the
similarity of the rationales provided. In addition, clustering patterns in
rationales are explored using principal component analysis based on the
embeddings of the rationales. The findings of this study provide insights into
the accuracy and ``thinking'' of LLMs in automated scoring, helping to improve
the understanding of the rationales behind both human scoring and LLM-based
automated scoring.

</details>


### [727] [The Hidden Costs of Translation Accuracy: Distillation, Quantization, and Environmental Impact](https://arxiv.org/abs/2509.23990)
*Dhaathri Vijay,Anandaswarup Vadapalli*

Main category: cs.CL

TL;DR: 研究对比全量、蒸馏和量化模型在机器翻译中的翻译质量与效率权衡，发现模型压缩策略可减少计算需求和环境影响，同时保持翻译质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的计算和环境成本引发关注，研究翻译质量和效率的权衡。

Method: 以机器翻译为例，在Flores+基准上评估，通过人工判断法语、印地语和卡纳达语的对话翻译，分析每次评估运行的碳排放。

Result: 全量3.3B fp32模型BLEU分数最高但环境足迹最大；蒸馏模型推理速度比全量模型快4.5倍，BLEU分数仅略有下降；人类评估显示积极量化（INT4）能保持高准确性和流畅性。

Conclusion: 模型压缩策略可减少计算需求和环境影响，同时保持翻译质量，但在低资源环境中权衡更明显，应将效率和可持续性纳入NLP评估框架。

Abstract: The rapid expansion of large language models (LLMs) has heightened concerns
about their computational and environmental costs. This study investigates the
trade-offs between translation quality and efficiency by comparing full-scale,
distilled, and quantized models using machine translation as a case study. We
evaluated performance on the Flores+ benchmark and through human judgments of
conversational translations in French, Hindi, and Kannada. Our analysis of
carbon emissions per evaluation run revealed that the full 3.3B fp32 model,
while achieving the highest BLEU scores, incurred the largest environmental
footprint (about 0.007-0.008 kg CO2 per run). The distilled models achieved an
inference of up to 4.5x faster than the full 3.3B model, with only minimal
reductions in BLEU scores. Human evaluations also showed that even aggressive
quantization (INT4) preserved high levels of accuracy and fluency, with
differences between models generally minor. These findings demonstrate that
model compression strategies can substantially reduce computational demands and
environmental impact while maintaining competitive translation quality, though
trade-offs are more pronounced in low-resource settings. We argue for
evaluation frameworks that integrate efficiency and sustainability alongside
objective metrics as central dimensions of progress in NLP.

</details>


### [728] [The AI Agent Code of Conduct: Automated Guardrail Policy-as-Prompt Synthesis](https://arxiv.org/abs/2509.23994)
*Gauri Kholkar,Ratinder Ahuja*

Main category: cs.CL

TL;DR: 引入新框架将非结构化设计文档转化为可验证的实时护栏，用“Policy as Prompt”方法，经验证能弥合政策与实践差距，使AI更安全可监管。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理在行业中广泛部署，需要保障其安全。

Method: 引入新框架将非结构化设计文档转化为可验证的实时护栏，采用“Policy as Prompt”方法，利用大语言模型解读和执行自然语言政策，构建可验证策略树并编译成轻量级基于提示的分类器，在运行时审计代理行为。

Result: 在不同应用中验证了该方法，展示了可扩展和可审计的管道。

Conclusion: 该方法弥合了政策与实践的关键差距，为更安全、更可监管的AI铺平了道路。

Abstract: As autonomous AI agents are increasingly deployed in industry, it is
essential to safeguard them. We introduce a novel framework that automates the
translation of unstructured design documents into verifiable, real-time
guardrails. We introduce "Policy as Prompt," a new approach that uses Large
Language Models (LLMs) to interpret and enforce natural language policies by
applying contextual understanding and the principle of least privilege. Our
system first ingests technical artifacts to construct a verifiable policy tree,
which is then compiled into lightweight, prompt-based classifiers that audit
agent behavior at runtime. We validate our approach across diverse
applications, demonstrating a scalable and auditable pipeline that bridges the
critical policy-to-practice gap, paving the way for verifiably safer and more
regulatable AI.

</details>


### [729] [MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use](https://arxiv.org/abs/2509.24002)
*Zijian Wu,Xiangyan Liu,Xinyuan Zhang,Lingjun Chen,Fanqing Meng,Lingxiao Du,Yiran Zhao,Fanshi Zhang,Yaoqi Ye,Jiawei Wang,Zirui Wang,Jinjie Ni,Yufan Yang,Arvin Xu,Michael Qizhe Shieh*

Main category: cs.CL

TL;DR: 现有MCP基准测试范围窄，本文提出MCPMark基准测试，用最小代理框架评估大模型，结果显示模型表现不佳，MCPMark有压力测试性质。


<details>
  <summary>Details</summary>
Motivation: 现有MCP基准测试聚焦读密集型或交互深度有限任务，无法反映现实工作流复杂性和真实性，需更全面的评估基准。

Method: 提出由领域专家和AI代理共同创建的包含127个高质量任务的MCPMark基准测试，用最小代理框架在工具调用循环中评估模型。

Result: 最佳模型gpt - 5 - medium的pass@1为52.56%、pass^4为33.86%，其他模型更低，平均每个任务需16.2次执行回合和17.4次工具调用。

Conclusion: MCPMark能更真实全面评估MCP使用，对大模型是压力测试。

Abstract: MCP standardizes how LLMs interact with external systems, forming the
foundation for general agents. However, existing MCP benchmarks remain narrow
in scope: they focus on read-heavy tasks or tasks with limited interaction
depth, and fail to capture the complexity and realism of real-world workflows.
To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP
use in a more realistic and comprehensive manner. It consists of $127$
high-quality tasks collaboratively created by domain experts and AI agents.
Each task begins with a curated initial state and includes a programmatic
script for automatic verification. These tasks demand richer and more diverse
interactions with the environment, involving a broad range of create, read,
update, and delete (CRUD) operations. We conduct a comprehensive evaluation of
cutting-edge LLMs using a minimal agent framework that operates in a
tool-calling loop. Empirical results show that the best-performing model,
gpt-5-medium, reaches only $52.56$\% pass@1 and $33.86$\% pass^4, while other
widely regarded strong models, including claude-sonnet-4 and o3, fall below
$30$\% pass@1 and $15$\% pass^4. On average, LLMs require $16.2$ execution
turns and $17.4$ tool calls per task, significantly surpassing those in
previous MCP benchmarks and highlighting the stress-testing nature of MCPMark.

</details>


### [730] [Large-Scale Constraint Generation -- Can LLMs Parse Hundreds of Constraints?](https://arxiv.org/abs/2509.24090)
*Matteo Boffa,Jiaxuan You*

Main category: cs.CL

TL;DR: 介绍大尺度约束生成问题LSCG，创建Words Checker实例评估LLM处理约束能力，提出FoCusNet模型，实验显示现有方案随约束数量增加性能下降，FoCusNet提升8 - 13%准确率。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型能否解析大规模、细粒度、通用的约束列表。

Method: 创建Words Checker实例评估LLM，提出FoCusNet模型解析约束列表。

Result: 现有解决方案随约束数量增加性能显著下降，FoCusNet可提升8 - 13%准确率。

Conclusion: FoCusNet有助于LLM聚焦相关约束，提高处理大规模约束的性能。

Abstract: Recent research has explored the constrained generation capabilities of Large
Language Models (LLMs) when explicitly prompted by few task-specific
requirements. In contrast, we introduce Large-Scale Constraint Generation
(LSCG), a new problem that evaluates whether LLMs can parse a large,
fine-grained, generic list of constraints. To examine the LLMs' ability to
handle an increasing number constraints, we create a practical instance of
LSCG, called Words Checker. In Words Checker, we evaluate the impact of model
characteristics (e.g., size, family) and steering techniques (e.g., Simple
Prompt, Chain of Thought, Best of N) on performance. We also propose FoCusNet,
a small and dedicated model that parses the original list of constraints into a
smaller subset, helping the LLM focus on relevant constraints. Experiments
reveal that existing solutions suffer a significant performance drop as the
number of constraints increases, with FoCusNet showing an 8-13% accuracy boost.

</details>


### [731] [GEAR: A General Evaluation Framework for Abductive Reasoning](https://arxiv.org/abs/2509.24096)
*Kaiyu He,Peilin Wu,Mian Zhang,Kun Wan,Wentian Zhao,Xinya Du,Zhiyu Chen*

Main category: cs.CL

TL;DR: 研究大语言模型的新知识发现能力，提出GEAR评估范式并进行研究，还提出动量课程策略提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型发现新知识的能力及如何评估该能力。

Method: 引入GEAR评估范式，用一致性、泛化性和多样性三个指标打分；提出动量课程策略调整训练数据。

Result: 使用GEAR对9个大语言模型进行细粒度研究，揭示模型差异；动量课程策略提升GEAR目标表现且成果可迁移。

Conclusion: GEAR为评估溯因推理提供框架，提供无标签、可扩展训练信号，助大语言模型生成更多样可靠假设。

Abstract: Since the advent of large language models (LLMs), research has focused on
instruction following and deductive reasoning. A central question remains: can
these models discover new knowledge, and how can we evaluate this ability? We
address this by studying abductive reasoning-the generation of plausible
hypotheses to explain observations-and introduce GEAR (General Evaluation for
Abductive Reasoning), a general-purpose, fully automated, transparent, and
label-free evaluation paradigm. GEAR scores hypothesis sets by three metrics:
consistency (each hypothesis explains the observations), generalizability
(consistent hypotheses make meaningful predictions on unseen inputs), and
diversity (the set covers distinct predictions and patterns). Built this way,
GEAR is scalable (no human gold answers), reliable (deterministic scoring
aligned with classical abduction), and open-ended (scores improve only when
models produce new plausible hypotheses, unlike static benchmarks that saturate
once accuracy is high). Using GEAR, we conduct a fine-grained study of nine
LLMs on four abduction benchmarks with 1,500 problems, generating over 50,000
candidate hypotheses and revealing model differences obscured by gold-answer or
purely human evaluations. We further propose a momentum-based curriculum that
adjusts GEAR-derived training data by learning velocity: it starts with what
the model learns quickly and shifts toward harder objectives such as generating
diverse hypotheses once the model is confident on foundational objectives.
Without gold-label supervision, this strategy improves all GEAR objectives and
these gains transfer to established abductive reasoning benchmarks. Taken
together, GEAR provides a principled framework that evaluates abduction and
supplies label-free, scalable training signals that help LLMs produce more
diverse and reliable hypotheses.

</details>


### [732] [Do LLMs Understand Romanian Driving Laws? A Study on Multimodal and Fine-Tuned Question Answering](https://arxiv.org/abs/2509.23715)
*Eduard Barbu,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: 本文评估大语言模型在罗马尼亚驾驶法律问答及解释生成上的表现，发布数据集，比较不同系统，研究微调影响，发现相关结论并为资源较少语言的可解释问答提供参考。


<details>
  <summary>Details</summary>
Motivation: 确保新老司机掌握现行交通规则对道路安全至关重要，评估大语言模型在罗马尼亚驾驶法律问答及解释生成方面的能力。

Method: 发布1208个问题的数据集（387个多模态），比较文本和多模态的SOTA系统，测量特定领域微调对Llama 3.1 - 8B - Instruct和RoLlama 3.1 - 8B - Instruct的影响，用LLM - as - a - Judge评估解释质量。

Result: SOTA模型表现良好，微调的8B模型有竞争力，图像的文本描述优于直接视觉输入，发现LLM存在自我偏好偏差。

Conclusion: 该研究为资源较少语言的可解释问答提供了信息。

Abstract: Ensuring that both new and experienced drivers master current traffic rules
is critical to road safety. This paper evaluates Large Language Models (LLMs)
on Romanian driving-law QA with explanation generation. We release a
1{,}208-question dataset (387 multimodal) and compare text-only and multimodal
SOTA systems, then measure the impact of domain-specific fine-tuning for Llama
3.1-8B-Instruct and RoLlama 3.1-8B-Instruct. SOTA models perform well, but
fine-tuned 8B models are competitive. Textual descriptions of images outperform
direct visual input. Finally, an LLM-as-a-Judge assesses explanation quality,
revealing self-preference bias. The study informs explainable QA for
less-resourced languages.

</details>


### [733] [Your thoughts tell who you are: Characterize the reasoning patterns of LRMs](https://arxiv.org/abs/2509.24147)
*Yida Chen,Yuning Mao,Xianjun Yang,Suyu Ge,Shengjie Bi,Lijuan Liu,Saghar Hosseini,Liang Tan,Yixin Nie,Shaoliang Nie*

Main category: cs.CL

TL;DR: 提出LOT方法比较大推理模型推理差异，应用于12个开源模型，能区分推理痕迹，还给出定性解释，案例显示可提升小模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大推理模型比较集中于宏观统计，不同模型推理是否不同尚不明确，需新方法分析差异。

Method: 引入LLM - proposed Open Taxonomy (LOT)方法，用生成式语言模型比较推理痕迹，根据经验分布建模预测来源，迭代生成人类可读分类法。

Result: LOT能识别模型思想的系统差异，区分不同模型推理痕迹准确率达80 - 100%，提供定性解释。

Conclusion: LOT可有效比较大推理模型推理差异，且能通过对齐推理风格提升小模型性能。

Abstract: Current comparisons of large reasoning models (LRMs) focus on macro-level
statistics such as task accuracy or reasoning length. Whether different LRMs
reason differently remains an open question. To address this gap, we introduce
the LLM-proposed Open Taxonomy (LOT), a classification method that uses a
generative language model to compare reasoning traces from two LRMs and
articulate their distinctive features in words. LOT then models how these
features predict the source LRM of a reasoning trace based on their empirical
distributions across LRM outputs. Iterating this process over a dataset of
reasoning traces yields a human-readable taxonomy that characterizes how models
think. We apply LOT to compare the reasoning of 12 open-source LRMs on tasks in
math, science, and coding. LOT identifies systematic differences in their
thoughts, achieving 80-100% accuracy in distinguishing reasoning traces from
LRMs that differ in scale, base model family, or objective domain. Beyond
classification, LOT's natural-language taxonomy provides qualitative
explanations of how LRMs think differently. Finally, in a case study, we link
the reasoning differences to performance: aligning the reasoning style of
smaller Qwen3 models with that of the largest Qwen3 during test time improves
their accuracy on GPQA by 3.3-5.7%.

</details>


### [734] [Retrieval-augmented GUI Agents with Generative Guidelines](https://arxiv.org/abs/2509.24183)
*Ran Xu,Kaixin Ma,Wenhao Yu,Hongming Zhang,Joyce C. Ho,Carl Yang,Dong Yu*

Main category: cs.CL

TL;DR: 提出轻量级VLM模型RAG - GUI，利用网络教程，经SFT和RSF微调，在多任务评估中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的GUI代理在实际应用中受训练数据稀缺和任务复杂性限制，需长尾知识。

Method: 提出RAG - GUI，先通过监督微调（SFT）预热启动，再通过自引导拒绝采样微调（RSF）进一步优化，作为通用插件增强基于VLM的代理。

Result: 在三个不同任务中评估，始终优于基线代理，在两种模型大小下比其他推理基线高出2.6%至13.3%。

Conclusion: RAG - GUI具有强大的泛化能力和实际场景中的即插即用能力。

Abstract: GUI agents powered by vision-language models (VLMs) show promise in
automating complex digital tasks. However, their effectiveness in real-world
applications is often limited by scarce training data and the inherent
complexity of these tasks, which frequently require long-tailed knowledge
covering rare, unseen scenarios. We propose RAG-GUI , a lightweight VLM that
leverages web tutorials at inference time. RAG-GUI is first warm-started via
supervised finetuning (SFT) and further refined through self-guided rejection
sampling finetuning (RSF). Designed to be model-agnostic, RAG-GUI functions as
a generic plug-in that enhances any VLM-based agent. Evaluated across three
distinct tasks, it consistently outperforms baseline agents and surpasses other
inference baselines by 2.6% to 13.3% across two model sizes, demonstrating
strong generalization and practical plug-and-play capabilities in real-world
scenarios.

</details>


### [735] [Beyond Overall Accuracy: A Psychometric Deep Dive into the Topic-Specific Medical Capabilities of 80 Large Language Models](https://arxiv.org/abs/2509.24186)
*Zhimeng Luo,Lixin Wu,Adam Frisch,Daqing He*

Main category: cs.CL

TL;DR: 本文引入MedIRT评估框架，用IRT评估大语言模型在医学应用中的表现，得出更稳定细致的排名，还发现模型专长差异，并建立实用决策支持框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于高风险医疗应用，传统评估指标不足，需要可靠准确的评估方法。

Method: 引入基于IRT的MedIRT评估框架，前瞻性收集80个大语言模型对1100个问题的回答，用单维双参数逻辑IRT模型估计模型能力、问题难度和区分度。

Result: 得出更稳定和细致的性能排名，发现模型有“尖峰”能力特征，GPT - 5在多数领域领先，Claude - 3 - opus在部分领域超过GPT - 5，还能识别有缺陷的问题。

Conclusion: 建立了一个强大、基于心理测量学的方法，对大语言模型在医疗领域的安全、有效和可靠部署至关重要。

Abstract: As Large Language Models (LLMs) are increasingly proposed for high-stakes
medical applications, there has emerged a critical need for reliable and
accurate evaluation methodologies. Traditional accuracy metrics fail
inadequately as they neither capture question characteristics nor offer
topic-specific insights. To address this gap, we introduce \textsc{MedIRT}, a
rigorous evaluation framework grounded in Item Response Theory (IRT), the gold
standard in high-stakes educational testing. Unlike previous research relying
on archival data, we prospectively gathered fresh responses from 80 diverse
LLMs on a balanced, 1,100-question USMLE-aligned benchmark. Using one
unidimensional two-parameter logistic IRT model per topic, we estimate LLM's
latent model ability jointly with question difficulty and discrimination,
yielding more stable and nuanced performance rankings than accuracy alone.
Notably, we identify distinctive ``spiky'' ability profiles, where overall
rankings can be misleading due to highly specialized model abilities. While
\texttt{GPT-5} was the top performer in a majority of domains (8 of 11), it was
outperformed in Social Science and Communication by \texttt{Claude-3-opus},
demonstrating that even an overall 23rd-ranked model can hold the top spot for
specific competencies. Furthermore, we demonstrate IRT's utility in auditing
benchmarks by identifying flawed questions. We synthesize these findings into a
practical decision-support framework that integrates our multi-factor
competency profiles with operational metrics. This work establishes a robust,
psychometrically grounded methodology essential for the safe, effective, and
trustworthy deployment of LLMs in healthcare.

</details>


### [736] [Can Large Language Models Express Uncertainty Like Human?](https://arxiv.org/abs/2509.24202)
*Linwei Tao,Yi-Fan Yeh,Bo Kai,Minjing Dong,Tao Huang,Tom A. Lamb,Jialin Yu,Philip H. S. Torr,Chang Xu*

Main category: cs.CL

TL;DR: 本文提出语言置信度（LC）作为大语言模型不确定性估计的新方法，发布相关数据集，提出映射器，开展系统研究并引入微调框架，证明LC可扩展、高效且符合人类习惯。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型置信度估计方法存在实际障碍，如logits隐藏、多采样计算成本高、数值不确定性表达偏离自然交流，需要可靠且轻量级的方法。

Method: 发布含人工标注置信度分数的对冲表达式数据集；提出轻量级映射器将对冲词转换为置信度分数；对现代大语言模型和问答基准进行系统研究；引入微调框架。

Result: 多数大语言模型在表达可靠LC方面表现不佳，但精心设计的提示能实现有竞争力的校准和区分能力；微调框架可进一步提高LC可靠性。

Conclusion: 语言置信度是大语言模型不确定性估计的可扩展、高效且符合人类习惯的方法，值得深入探索。

Abstract: Large language models (LLMs) are increasingly used in high-stakes settings,
where overconfident responses can mislead users. Reliable confidence estimation
has been shown to enhance trust and task accuracy. Yet existing methods face
practical barriers: logits are often hidden, multi-sampling is computationally
expensive, and verbalized numerical uncertainty (e.g., giving a 0-100 score)
deviates from natural communication. We revisit linguistic confidence (LC),
where models express uncertainty through hedging language (e.g., probably,
might), offering a lightweight and human-centered alternative. To advance this
direction, we (1) release the first diverse, large-scale dataset of hedging
expressions with human-annotated confidence scores, and (2) propose a
lightweight mapper that converts hedges into confidence scores at near-zero
cost. Building on these resources, we (3) conduct the first systematic study of
LC across modern LLMs and QA benchmarks, revealing that while most LLMs
underperform in expressing reliable LC, carefully designed prompting achieves
competitive calibration and discriminability. Finally, we (4) introduce a
fine-tuning framework that further improves LC reliability. Taken together, our
work positions linguistic confidence as a scalable, efficient, and
human-aligned approach to LLM uncertainty estimation, and calls for deeper
exploration of this promising yet underexplored direction.

</details>


### [737] [Sequential Diffusion Language Models](https://arxiv.org/abs/2509.24007)
*Yangzhou Liu,Yue Cao,Hao Li,Gen Luo,Zhe Chen,Weiyun Wang,Xiaobo Liang,Biqing Qi,Lijun Wu,Changyao Tian,Yanting Zhang,Yuqiang Li,Tong Lu,Yu Qiao,Jifeng Dai,Wenhai Wang*

Main category: cs.CL

TL;DR: 提出Next Sequence Prediction (NSP) 统一预测，基于此提出Sequential Diffusion Language Model (SDLM)，能低成本改造预训练模型，实验显示其效率高且可扩展性强。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型存在固定长度解码、与KV缓存不兼容等问题，块扩散仍有局限。

Method: 引入NSP统一预测，基于此提出SDLM，在固定大小掩码块内进行扩散推理，根据模型置信度动态解码连续子序列。

Result: SDLM用350万训练样本就达到或超越强自回归基线，吞吐量比Qwen - 2.5高2.1，SDLM - 32B模型效率提升更明显。

Conclusion: SDLM建模范式具有很强的可扩展性。

Abstract: Diffusion language models (DLMs) have strong theoretical efficiency but are
limited by fixed-length decoding and incompatibility with key-value (KV)
caches. Block diffusion mitigates these issues, yet still enforces a fixed
block size and requires expensive training. We introduce Next Sequence
Prediction (NSP), which unifies next-token and next-block prediction, enabling
the model to adaptively determine the generation length at each step. When the
length is fixed to 1, NSP reduces to standard next-token prediction. Building
on NSP, we propose Sequential Diffusion Language Model (SDLM), which can
retrofit pre-trained autoregressive language models (ALMs) at minimal cost.
Specifically, SDLM performs diffusion inference within fixed-size mask blocks,
but dynamically decodes consecutive subsequences based on model confidence,
thereby preserving KV-cache compatibility and improving robustness to varying
uncertainty and semantics across the sequence. Experiments show that SDLM
matches or surpasses strong autoregressive baselines using only 3.5M training
samples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the
SDLM-32B model delivers even more pronounced efficiency gains, demonstrating
the strong scalability potential of our modeling paradigm. Project page and
codes: https://github.com/OpenGVLab/SDLM

</details>


### [738] [BeyondBench: Benchmark-Free Evaluation of Reasoning in Language Models](https://arxiv.org/abs/2509.24210)
*Gaurav Srivastava,Aafiya Hussain,Zhenyu Bi,Swastik Roy,Priya Pitre,Meng Lu,Morteza Ziyadi,Xuan Wang*

Main category: cs.CL

TL;DR: 文章介绍BeyondBench评估框架，用算法生成问题避免数据污染，评估101个语言模型，发现推理缺陷及性能随复杂度变化情况，公布排行榜。


<details>
  <summary>Details</summary>
Motivation: 现有静态基准测试存在被训练数据污染风险，难以公平评估语言模型推理能力。

Method: 引入BeyondBench评估框架，通过算法生成数学问题，涵盖44个算法任务、117种变体，分三个难度等级，对101个语言模型进行评估。

Result: 各模型家族存在推理缺陷，性能随问题复杂度从多项式到指数级增加而急剧下降，不用工具时性能大幅下降。如Gemini - 2.5 - pro等在Hard Suite有不同准确率，GPT - 5等不用工具准确率下降。

Conclusion: BeyondBench能有效评估语言模型推理能力，展示不同模型在不同难度任务及工具使用与否下的性能差异。

Abstract: Evaluating language models fairly is becoming harder as static benchmarks
available on the internet risk contamination by training data. This makes it
unclear whether models are truly reasoning or just recalling answers. In this
paper, we introduce BeyondBench, an evaluation framework that avoids this
problem by using algorithmic problem generation. Unlike traditional benchmarks
that risk contamination from internet-scale training data, BeyondBench creates
mathematically grounded problems on the fly, ensuring each test remains fresh
and uncontaminated. Our framework covers 44 algorithmic tasks with a total of
117 variations, grouped into three difficulty levels: the Easy Suite (29 tasks)
for basic arithmetic and statistics, the Medium Suite (5 tasks, 49 variations)
for sequence patterns and reasoning, and the Hard Suite (10 tasks, 68
variations) tackling NP-complete and constraint satisfaction problems. Each
task generates problems from a combinatorial space larger than 10^15 unique
instances, with solutions verified deterministically by mathematical proofs. We
evaluated 101 language models, including 85 open-source and 16 closed-source
models, spanning sizes from 0.5B to 141B parameters and multiple quantization
schemes. Our results show consistent reasoning deficiencies across model
families, with performance degrading sharply as problem complexity increases
from polynomial to exponential. In our Hard Suite evaluations, models such as
Gemini-2.5-pro, Llama-3.3-70B, and Qwen2.5-72B achieved average accuracies of
56.38%, 26.91%, and 33.60%, respectively. Moreover, we observe that performance
drops drastically without tool usage, with GPT-5, GPT-5-mini, and GPT-5-nano
showing a decline of 16.81%, 28.05%, and 47.59% accuracy on the hard suite. Our
leaderboard is publicly available at https://ctrl-gaurav.github.io/BeyondBench/

</details>


### [739] [Prompt and Parameter Co-Optimization for Large Language Models](https://arxiv.org/abs/2509.24245)
*Xiaohe Bo,Rui Li,Zexu Sun,Quanyu Dai,Zeyu Zhang,Zihang Tian,Xu Chen,Zhenhua Dong*

Main category: cs.CL

TL;DR: 提出MetaTuner框架，联合集成提示优化和微调，实验显示优于基线。


<details>
  <summary>Details</summary>
Motivation: 以往研究多孤立探讨提示优化和微调，其协同潜力未充分挖掘。

Method: 引入两个神经网络分别生成提示和参数，共享底层编码层，设计监督正则化损失进行训练。

Result: 在多个基准测试中，该方法始终优于基线。

Conclusion: MetaTuner框架能有效联合提示优化和微调，提升大语言模型性能。

Abstract: Prompt optimization and fine-tuning are two major approaches to improve the
performance of Large Language Models (LLMs). They enhance the capabilities of
LLMs from complementary perspectives: the former through explicit natural
language, and the latter through implicit parameter updates. However, prior
work has typically studied them in isolation, leaving their synergistic
potential largely underexplored. To bridge this gap, in this paper, we
introduce MetaTuner, a novel framework that jointly integrates prompt
optimization and fine-tuning for LLM training. Specifically, we introduce two
neural networks to generate prompts and parameters, respectively, while
allowing them to share a common bottom encoding layer to enable knowledge
sharing. By the guidance of the final supervised signals, our framework is
optimized to discover the optimal combinations between the prompts and
parameters. Given that prompt learning involves discrete optimization while
fine-tuning operates in a continuous parameter space, we design a supervised
regularization loss to train our framework effectively. Extensive experiments
across diverse benchmarks show that our method consistently outperforms the
baselines.

</details>


### [740] [SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents](https://arxiv.org/abs/2509.24282)
*Gyuhyeon Seo,Jungwoo Yang,Junseong Pyo,Nalim Kim,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出SimuHome模拟环境和600集挑战性基准，评估11个智能家庭代理，发现模型在潜在意图推理等方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有开发具备特定能力的智能家居代理存在缺乏现实模拟环境和挑战性基准的瓶颈。

Method: 引入SimuHome时间加速家庭环境，基于Matter协议构建；提供涵盖十二种用户查询类型的600集基准。

Result: 评估11个代理发现，模型在简单任务上表现良好，但在潜在意图推理、状态验证和时间调度方面有困难，GPT - 4.1最高成功率仅54%。

Conclusion: 迫切需要能在行动前通过工具可靠验证当前状态和协调时间相关行动的方法。

Abstract: Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks.
However, smart homes introduce distinct challenges, requiring agents to handle
latent user intents, temporal dependencies, device constraints, scheduling, and
more. The main bottlenecks for developing smart home agents with such
capabilities include the lack of a realistic simulation environment where
agents can interact with devices and observe the results, as well as a
challenging benchmark to evaluate them. To address this, we introduce
$\textbf{SimuHome}$, a time-accelerated home environment that simulates smart
devices, supports API calls, and reflects changes in environmental variables.
By building the simulator on the Matter protocol (the global industry standard
for smart home communication), SimuHome provides a high-fidelity environment,
and agents validated in SimuHome can be deployed on real Matter-compliant
devices with minimal adaptation. We provide a challenging benchmark of 600
episodes across twelve user query types that require the aforementioned
capabilities. Our evaluation of 11 agents under a unified ReAct framework
reveals that while models perform well on simple tasks, they struggle with
latent intent inference, state verification, and especially temporal
scheduling. Even the top-performing model, GPT-4.1, reaches only 54% success
rate. These findings highlight a critical need for methods that can reliably
verify the current state via tools before acting and coordinate time-dependent
actions.

</details>


### [741] [Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement](https://arxiv.org/abs/2509.24291)
*Yu-Che Tsai,Kuan-Yu Chen,Yuan-Chi Li,Yuan-Hao Chen,Ching-Yu Tsai,Shou-De Lin*

Main category: cs.CL

TL;DR: 提出GIRCSE框架，利用自回归生成迭代细化语义表示，实验表明其在MTEB基准和指令跟随任务上表现出色，建立了生成式迭代细化作为表征学习新范式。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的嵌入方法采用仅编码器范式，忽略了大语言模型的核心生成优势。

Method: 引入GIRCSE框架，通过自回归生成优化软标记序列，提出迭代对比细化（ICR）目标来引导细化过程。

Result: GIRCSE在MTEB基准和指令跟随任务上优于基于大语言模型的嵌入基线，且具有推理时生成更多标记可稳步提高嵌入质量的特性。

Conclusion: 生成式迭代细化可作为表征学习的新范式。

Abstract: Existing large language model (LLM)-based embeddings typically adopt an
encoder-only paradigm, treating LLMs as static feature extractors and
overlooking their core generative strengths. We introduce GIRCSE (Generative
Iterative Refinement for Contrastive Sentence Embeddings), a novel framework
that leverages autoregressive generation to iteratively refine semantic
representations. By producing sequences of soft tokens optimized under
contrastive objective, GIRCSE captures latent concepts and implicit semantics
that encoder-only methods often miss. To guide this process, we propose an
Iterative Contrastive Refinement (ICR) objective that encourages each
refinement step to yield better representations. Extensive experiments show
that GIRCSE outperforms strong LLM-based embedding baselines on the MTEB
benchmark and instruction-following tasks. Moreover, GIRCSE exhibits an
emergent test-time scaling property: generating more tokens at inference
steadily improves embedding quality. Our results establish generative iterative
refinement as a new paradigm for representation learning.

</details>


### [742] [DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models](https://arxiv.org/abs/2509.24296)
*Zherui Li,Zheng Nie,Zhenhong Zhou,Yufei Guo,Yue Liu,Yitong Zhang,Yu Cheng,Qingsong Wen,Kun Wang,Jiaheng Zhang*

Main category: cs.CL

TL;DR: 文章分析扩散大语言模型（dLLM）越狱攻击漏洞，发现标准贪婪重掩码策略的有害偏差和去噪路径依赖现象，提出无训练防御框架DiffuGuard并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: dLLM快速发展带来与自回归大语言模型不同的新漏洞，需对其越狱攻击漏洞进行分析并提出防御方法。

Method: 从内步和跨步动态两个维度分析dLLM漏洞，提出DiffuGuard框架，通过随机退火重掩码和块级审计修复两个阶段解决漏洞。

Result: 实验表明DiffuGuard将六种越狱方法的攻击成功率从47.9%降至14.7%，且保留了模型的效用和效率。

Conclusion: DiffuGuard能有效解决dLLM越狱攻击漏洞问题，dLLM有很大的内在安全潜力。

Abstract: The rapid advancement of Diffusion Large Language Models (dLLMs) introduces
unprecedented vulnerabilities that are fundamentally distinct from
Autoregressive LLMs, stemming from their iterative and parallel generation
mechanisms. In this paper, we conduct an in-depth analysis of dLLM
vulnerabilities to jailbreak attacks across two distinct dimensions: intra-step
and inter-step dynamics. Experimental results reveal a harmful bias inherent in
the standard greedy remasking strategy and identify a critical phenomenon we
term Denoising-path Dependence, where the safety of early-stage tokens
decisively influences the final output. These findings also indicate that while
current decoding strategies constitute a significant vulnerability, dLLMs
possess a substantial intrinsic safety potential. To unlock this potential, we
propose DiffuGuard, a training-free defense framework that addresses
vulnerabilities through a dual-stage approach: Stochastic Annealing Remasking
dynamically introduces controlled randomness to mitigate greedy selection bias,
while Block-level Audit and Repair exploits internal model representations for
autonomous risk detection and guided correction. Comprehensive experiments on
four dLLMs demonstrate DiffuGuard's exceptional effectiveness, reducing Attack
Success Rate against six diverse jailbreak methods from 47.9% to 14.7% while
preserving model utility and efficiency. Our code is available at:
https://github.com/niez233/DiffuGuard.

</details>


### [743] [Q-Mirror: Unlocking the Multi-Modal Potential of Scientific Text-Only QA Pairs](https://arxiv.org/abs/2509.24297)
*Junying Wang,Zicheng Zhang,Ye Shen,Yalun Wu,Yingji Liang,Yijin Guo,Farong Wen,Wenzhe Li,Xuezhi Zhao,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: 本文探索将文本问答对转化为多模态问答对以解决高质量多模态基准创建瓶颈，构建基准并开发Q - Mirror系统，实验显示该系统提升了指标。


<details>
  <summary>Details</summary>
Motivation: 高质量多模态基准对大模型科学推理很重要，但手动创建成本高且不可扩展，需解决此瓶颈。

Method: 开发TQA - to - MMQA框架和质量标准，构建两个基准，开发Q - Mirror系统将生成和评估集成到闭环进行迭代优化。

Result: 现有模型生成MMQAs有差距，顶级理解模型在质量评估上接近人类判断，Q - Mirror将平均分从78.90提升到85.22，通过率从72%提升到95%。

Conclusion: 通过将TQA转化为MMQA及Q - Mirror系统，为大规模科学基准提供了可行途径。

Abstract: High-quality, multi-modal benchmarks are crucial for advancing scientific
reasoning in large models yet their manual creation is costly and unscalable.
To address this bottleneck, we explore the potential for transforming Text-Only
QA Pairs (TQAs) into high-quality Multi-Modal QA Pairs (MMQAs), which include
three parts: 1) Task Definition \& Evaluation Rubric: We develop a TQA-to-MMQA
framework and establish a comprehensive, multi-dimensional MMQA quality rubric
that provides principles for the transformation. 2) Benchmark Construction:
Then we construct two extensive benchmarks to rigorously evaluate
state-of-the-art generation \& understanding models on the distinct tasks of
MMQA generation \& MMQA quality evaluation. 3) Preliminary Solution: We develop
an agentic system (Q-Mirror), which operationalizes our framework by
integrating MMQA generation and evaluation into a closed loop for iterative
refinement. Our experiments show that while state-of-the-art models can
generate MMQAs, their outputs still leave substantial gaps, underscoring the
need for reliable evaluation. We further demonstrate that top-tier
understanding models align closely with human judgment in MMQA quality
assessment. Leveraging both insights, the Q-Mirror agent raises average scores
from 78.90 to 85.22 and pass rates from 72\% to 95\%, offering a practical path
to large-scale scientific benchmarks.

</details>


### [744] [Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs](https://arxiv.org/abs/2509.24319)
*Jongwook Han,Jongwon Lim,Injin Kong,Yohan Jo*

Main category: cs.CL

TL;DR: 研究大语言模型内在和提示价值表达机制，发现二者部分共享组件，有不同价值可引导性和响应多样性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在价值对齐和角色引导中广泛应用，需明确内在和提示价值表达机制是否重叠及底层机制，当前研究不足。

Method: 使用价值向量（从残差流提取代表价值机制的特征方向）和价值神经元（对价值表达有贡献的MLP神经元）两种方法进行分析。

Result: 内在和提示价值机制部分共享对诱导价值表达至关重要的组件，也有独特元素，导致不同程度的价值可引导性（提示>内在）和响应多样性（内在>提示）。

Conclusion: 内在机制独特组件促进响应词汇多样性，提示机制独特组件强化指令遵循，在越狱等远程任务也有效。

Abstract: Large language models (LLMs) can express different values in two distinct
ways: (1) intrinsic expression, reflecting the model's inherent values learned
during training, and (2) prompted expression, elicited by explicit prompts.
Given their widespread use in value alignment and persona steering, it is
paramount to clearly understand their underlying mechanisms, particularly
whether they mostly overlap (as one might expect) or rely on substantially
different mechanisms, but this remains largely understudied. We analyze this at
the mechanistic level using two approaches: (1) value vectors, feature
directions representing value mechanisms extracted from the residual stream,
and (2) value neurons, MLP neurons that contribute to value expressions. We
demonstrate that intrinsic and prompted value mechanisms partly share common
components that are crucial for inducing value expression, but also possess
unique elements that manifest in different ways. As a result, these mechanisms
lead to different degrees of value steerability (prompted > intrinsic) and
response diversity (intrinsic > prompted). In particular, components unique to
the intrinsic mechanism seem to promote lexical diversity in responses, whereas
those specific to the prompted mechanism primarily strengthen instruction
following, taking effect even in distant tasks like jailbreaking.

</details>


### [745] [Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining](https://arxiv.org/abs/2509.24356)
*Matthew Theodore Roque,Dan John Velasco*

Main category: cs.CL

TL;DR: 研究数据受限场景下语言模型预训练的课程学习，对比不同数据调度方式，发现添加简化数据及不同复杂度排序对不同规模模型有不同效果。


<details>
  <summary>Details</summary>
Motivation: 多数语言模型预训练研究聚焦大数据集，数据受限场景下训练数据顺序和包含相同文本替代版本的影响未充分探索。

Method: 基于一对平行语料库，测试重复曝光、低到高复杂度、高到低复杂度和交错四种数据调度方式，从样本效率和零样本性能角度分析模型表示质量。

Result: 添加简化数据比重复曝光基线提高了微调及零样本性能，小模型从低到高复杂度排序受益，大模型交错排序表现更好。

Conclusion: 在数据受限的语言模型预训练中，使用简化数据和合适的数据复杂度排序能提升模型性能。

Abstract: Most studies on language model pretraining focus on large datasets, leaving
open questions about optimization in data-constrained settings. In such
settings, the effects of training data order and of including alternative
versions of the same text remain underexplored. We address this by studying
curriculum learning in pretraining, focusing on text-complexity ordering and
data augmentation via simplification. We ask: (1) Does simplifying texts
enhance representation quality more than reusing the original data? and (2)
Does ordering data by text complexity yield better representations? To answer,
we build on a pair of parallel corpora where human-written paragraphs are
aligned with LLM-simplified variants, and test four data schedules: repeated
exposure, low-to-high complexity, high-to-low, and interleaved. We analyze
models' representation quality from a sample efficiency perspective via
fine-tuning, as well as its zero-shot performance on linguistic knowledge,
entity tracking, world knowledge, and commonsense reasoning. Our findings show
that adding simplified data improves fine-tuning and zero-shot performance over
a repeated-exposure baseline: smaller models benefit from low-to-high
complexity, while larger models perform better with interleaved ordering.

</details>


### [746] [HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment](https://arxiv.org/abs/2509.24384)
*Langqi Yang,Tianhang Zheng,Kedong Xiu,Yixuan Chen,Di Wang,Puning Zhao,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 提出HarmMetric Eval基准评估有害性指标和评判器，实验发现METEOR和ROUGE - 1优于基于大模型的评判器。


<details>
  <summary>Details</summary>
Motivation: 缺乏评估有害性指标和评判器质量与有效性的系统基准，影响越狱攻击有效性等风险报告的可信度。

Method: 引入HarmMetric Eval基准，包含高质量有害提示数据集、多样模型响应及灵活评分机制。

Result: METEOR和ROUGE - 1在评估模型响应有害性方面优于基于大模型的评判器。

Conclusion: HarmMetric Eval可用于评估有害性指标和评判器，挑战了大模型在该领域优越性的普遍认知。

Abstract: The alignment of large language models (LLMs) with human values is critical
for their safe deployment, yet jailbreak attacks can subvert this alignment to
elicit harmful outputs from LLMs. In recent years, a proliferation of jailbreak
attacks has emerged, accompanied by diverse metrics and judges to assess the
harmfulness of the LLM outputs. However, the absence of a systematic benchmark
to assess the quality and effectiveness of these metrics and judges undermines
the credibility of the reported jailbreak effectiveness and other risks. To
address this gap, we introduce HarmMetric Eval, a comprehensive benchmark
designed to support both overall and fine-grained evaluation of harmfulness
metrics and judges. Our benchmark includes a high-quality dataset of
representative harmful prompts paired with diverse harmful and non-harmful
model responses, alongside a flexible scoring mechanism compatible with various
metrics and judges. With HarmMetric Eval, our extensive experiments uncover a
surprising result: two conventional metrics--METEOR and ROUGE-1--outperform
LLM-based judges in evaluating the harmfulness of model responses, challenging
prevailing beliefs about LLMs' superiority in this domain. Our dataset is
publicly available at https://huggingface.co/datasets/qusgo/HarmMetric_Eval,
and the code is available at
https://anonymous.4open.science/r/HarmMetric-Eval-4CBE.

</details>


### [747] [LLaDA-MoE: A Sparse MoE Diffusion Language Model](https://arxiv.org/abs/2509.24389)
*Fengqi Zhu,Zebin You,Yipeng Xing,Zenan Huang,Lin Liu,Yihong Zhuang,Guoshan Lu,Kangyu Wang,Xudong Wang,Lanning Wei,Hongrui Guo,Jiaqi Hu,Wentao Ye,Tieyuan Chen,Chenchen Li,Chengfu Tang,Haibo Feng,Jun Hu,Jun Zhou,Xiaolu Zhang,Zhenzhong Lan,Junbo Zhao,Da Zheng,Chongxuan Li,Jianguo Li,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 介绍从零训练的大语言扩散模型LLaDA - MoE，它参数少、计算开销低，性能超同类模型，调优后能力与Qwen2.5 - 3B - Instruct相当。


<details>
  <summary>Details</summary>
Motivation: 降低大语言扩散模型的计算开销，同时保持甚至提升模型性能。

Method: 采用Mixture - of - Experts (MoE)架构，从零开始在约20T个token上训练LLaDA - MoE。

Result: LLaDA - MoE性能超越参数更大的扩散语言模型，调优后的LLaDA - MoE - 7B - A1B - Instruct能力与Qwen2.5 - 3B - Instruct相当。

Conclusion: 在掩码扩散语言模型训练目标中集成稀疏MoE架构在少参数高效推理下仍能发挥MoE优势，为扩散语言模型探索提供空间。

Abstract: We introduce LLaDA-MoE, a large language diffusion model with the
Mixture-of-Experts (MoE) architecture, trained from scratch on approximately
20T tokens. LLaDA-MoE achieves competitive performance with significantly
reduced computational overhead by maintaining a 7B-parameter capacity while
activating only 1.4B parameters during inference. Our empirical evaluation
reveals that LLaDA-MoE achieves state-of-the-art performance among diffusion
language models with larger parameters, surpassing previous diffusion language
models LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The
instruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities
comparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation,
mathematical reasoning, agent and alignment tasks, despite using fewer active
parameters. Our results show that integrating a sparse MoE architecture into
the training objective of masked diffusion language models still brings out
MoE's strengths under efficient inference with few active parameters, and opens
ample room for further exploration of diffusion language models. LLaDA-MoE
models are available at Huggingface.

</details>


### [748] [Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models](https://arxiv.org/abs/2509.24488)
*Wenjie Fu,Huandong Wang,Junyao Gao,Guoan Wan,Tao Jiang*

Main category: cs.CL

TL;DR: 文章提出Self - Sanitize框架应对大语言模型有害内容生成问题，实验表明该框架效果好、开销小。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在应用中存在生成有害内容问题，现有缓解策略有缺陷，且之前研究对隐私侵犯内容关注不足。

Method: 引入受认知心理学启发的Self - Sanitize框架，包含自我监测和自我修复模块，在token级别检查意图并原位修正有害内容。

Result: 在三种隐私泄露场景的四个大语言模型上实验，Self - Sanitize缓解效果好，开销小且不降低模型实用性。

Conclusion: Self - Sanitize为安全部署大语言模型提供实用且强大的解决方案。

Abstract: As Large Language Models (LLMs) achieve remarkable success across a wide
range of applications, such as chatbots and code copilots, concerns surrounding
the generation of harmful content have come increasingly into focus. Despite
significant advances in aligning LLMs with safety and ethical standards,
adversarial prompts can still be crafted to elicit undesirable responses.
Existing mitigation strategies are predominantly based on post-hoc filtering,
which introduces substantial latency or computational overhead, and is
incompatible with token-level streaming generation. In this work, we introduce
Self-Sanitize, a novel LLM-driven mitigation framework inspired by cognitive
psychology, which emulates human self-monitor and self-repair behaviors during
conversations. Self-Sanitize comprises a lightweight Self-Monitor module that
continuously inspects high-level intentions within the LLM at the token level
via representation engineering, and a Self-Repair module that performs in-place
correction of harmful content without initiating separate review dialogues.
This design allows for real-time streaming monitoring and seamless repair, with
negligible impact on latency and resource utilization. Given that
privacy-invasive content has often been insufficiently focused in previous
studies, we perform extensive experiments on four LLMs across three privacy
leakage scenarios. The results demonstrate that Self-Sanitize achieves superior
mitigation performance with minimal overhead and without degrading the utility
of LLMs, offering a practical and robust solution for safer LLM deployments.
Our code is available at the following link:
https://github.com/wjfu99/LLM_Self_Sanitize

</details>


### [749] [Alternatives To Next Token Prediction In Text Generation -- A Survey](https://arxiv.org/abs/2509.24435)
*Charlie Wyatt,Aditya Joshi,Flora Salim*

Main category: cs.CL

TL;DR: 本文探讨Next Token Prediction (NTP)范式的局限，介绍其替代方法生态系统并分类，提供分类法指导自然语言处理新模型研究。


<details>
  <summary>Details</summary>
Motivation: NTP范式虽使大语言模型成功，但存在诸多弱点，有探索替代方法的需求。

Method: 将替代NTP的方法分为五类：多令牌预测、先规划后生成、隐式推理、连续生成方法和非Transformer架构。

Result: 对替代NTP的方法进行了分类和总结。

Conclusion: 该分类法可指导研究解决令牌级生成已知局限的模型，开发自然语言处理新变革模型。

Abstract: The paradigm of Next Token Prediction (NTP) has driven the unprecedented
success of Large Language Models (LLMs), but is also the source of their most
persistent weaknesses such as poor long-term planning, error accumulation, and
computational inefficiency. Acknowledging the growing interest in exploring
alternatives to NTP, the survey describes the emerging ecosystem of
alternatives to NTP. We categorise these approaches into five main families:
(1) Multi-Token Prediction, which targets a block of future tokens instead of a
single one; (2) Plan-then-Generate, where a global, high-level plan is created
upfront to guide token-level decoding; (3) Latent Reasoning, which shifts the
autoregressive process itself into a continuous latent space; (4) Continuous
Generation Approaches, which replace sequential generation with iterative,
parallel refinement through diffusion, flow matching, or energy-based methods;
and (5) Non-Transformer Architectures, which sidestep NTP through their
inherent model structure. By synthesizing insights across these methods, this
survey offers a taxonomy to guide research into models that address the known
limitations of token-level generation to develop new transformative models for
natural language processing.

</details>


### [750] [Inducing Dyslexia in Vision Language Models](https://arxiv.org/abs/2509.24597)
*Melika Honarmand,Ayati Sharma,Badr AlKhamissi,Johannes Mehrer,Martin Schrimpf*

Main category: cs.CL

TL;DR: 本文使用大规模视觉 - 语言模型模拟阅读障碍，识别并扰动模型中类似词处理的单元，复现了阅读障碍的关键特征，建立研究阅读障碍的计算框架。


<details>
  <summary>Details</summary>
Motivation: 传统研究阅读障碍的方法难以检验阅读障碍潜在机制的因果假设，需新方法。

Method: 使用大规模视觉 - 语言模型，识别模型中视觉词形选择单元并进行定向消融。

Result: 定向消融这些单元会导致阅读任务选择性受损，一般视觉和语言理解能力不受影响，模型复现阅读障碍者语音缺陷。

Conclusion: 建模结果复现阅读障碍关键特征，建立研究阅读障碍的计算框架。

Abstract: Dyslexia, a neurodevelopmental disorder characterized by persistent reading
difficulties, is often linked to reduced activity of the visual word form area
in the ventral occipito-temporal cortex. Traditional approaches to studying
dyslexia, such as behavioral and neuroimaging methods, have provided valuable
insights but remain limited in their ability to test causal hypotheses about
the underlying mechanisms of reading impairments. In this study, we use
large-scale vision-language models (VLMs) to simulate dyslexia by functionally
identifying and perturbing artificial analogues of word processing. Using
stimuli from cognitive neuroscience, we identify visual-word-form-selective
units within VLMs and demonstrate that targeted ablation of these units, unlike
ablation of random units, leads to selective impairments in reading tasks while
general visual and language comprehension abilities remain intact. In
particular, the resulting model matches dyslexic humans' phonological deficits
without a significant change in orthographic processing. Taken together, our
modeling results replicate key characteristics of dyslexia and establish a
computational framework for investigating reading disorders.

</details>


### [751] [Hype or not? Formalizing Automatic Promotional Language Detection in Biomedical Research](https://arxiv.org/abs/2509.24638)
*Bojan Batalo,Erica K. Shimomoto,Neil Millar*

Main category: cs.CL

TL;DR: 本文引入自动检测科研宣传性语言（hype）任务，制定标注指南，用NIH语料实验，发现标注指南助于人工标注，训练模型有前景，还指出任务有语言复杂性等特点。


<details>
  <summary>Details</summary>
Motivation: 科研中宣传性语言增多，会破坏证据客观评估、阻碍研究发展和侵蚀科学信任，需自动检测。

Method: 提出识别宣传性语言的正式指南，标注NIH资助申请语料部分内容，评估传统文本分类器和语言模型性能并与人类基线比较。

Result: 形式化标注指南能帮助人类可靠标注候选宣传性形容词，用标注数据集训练机器学习模型有前景。

Conclusion: 该任务有语言复杂性，可能需要领域知识和对事实的时间意识，本文首次将其作为自然语言处理任务处理。

Abstract: In science, promotional language ('hype') is increasing and can undermine
objective evaluation of evidence, impede research development, and erode trust
in science. In this paper, we introduce the task of automatic detection of
hype, which we define as hyperbolic or subjective language that authors use to
glamorize, promote, embellish, or exaggerate aspects of their research. We
propose formalized guidelines for identifying hype language and apply them to
annotate a portion of the National Institutes of Health (NIH) grant application
corpus. We then evaluate traditional text classifiers and language models on
this task, comparing their performance with a human baseline. Our experiments
show that formalizing annotation guidelines can help humans reliably annotate
candidate hype adjectives and that using our annotated dataset to train machine
learning models yields promising results. Our findings highlight the linguistic
complexity of the task, and the potential need for domain knowledge and
temporal awareness of the facts. While some linguistic works address hype
detection, to the best of our knowledge, we are the first to approach it as a
natural language processing task.

</details>


### [752] [InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation](https://arxiv.org/abs/2509.24663)
*Weilin Zhao,Zihan Zhou,Zhou Su,Chaojun Xiao,Yuxuan Li,Yanghao Li,Yudi Zhang,Weilun Zhao,Zhen Li,Yuxiang Huang,Ao Sun,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 提出InfLLM - V2框架解决长序列处理问题，实验显示速度更快且性能损失小，还开源MiniCPM4.1。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer架构的自注意力机制处理长序列有计算和内存瓶颈，现有稀疏注意力方法存在参数过多、破坏训练流程等问题。

Method: 引入InfLLM - V2框架，通过无参数架构修改复用密集注意力参数，短输入用密集注意力，长输入过渡到稀疏注意力，并采用高效实现减少计算开销。

Result: 实验表明InfLLM - V2比密集注意力快4倍，分别保留98.1%和99.7%的性能，还训练并开源MiniCPM4.1。

Conclusion: InfLLM - V2能有效解决长序列处理问题，为研究社区提供可复现实现。

Abstract: Long-sequence processing is a critical capability for modern large language
models. However, the self-attention mechanism in the standard Transformer
architecture faces severe computational and memory bottlenecks when processing
long sequences. While trainable sparse attention methods offer a promising
solution, existing approaches such as NSA introduce excessive extra parameters
and disrupt the conventional \textit{pretrain-on-short, finetune-on-long}
workflow, resulting in slow convergence and difficulty in acceleration. To
overcome these limitations, we introduce dense-sparse switchable attention
framework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that
seamlessly adapts models from short to long sequences. Specifically, InfLLM-V2
reuses dense attention parameters through parameter-free architecture
modification, maintaining consistency between short and long sequence
processing. Additionally, InfLLM-V2 ensures computational efficiency across all
sequence lengths, by using dense attention for short inputs and smoothly
transitioning to sparse attention for long sequences. To achieve practical
acceleration, we further introduce an efficient implementation of InfLLM-V2
that significantly reduces the computational overhead. Our experiments on
long-context understanding and chain-of-thought reasoning demonstrate that
InfLLM-V2 is 4$\times$ faster than dense attention while retaining 98.1% and
99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we
have trained and open-sourced MiniCPM4.1
(https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model,
providing a reproducible implementation for the research community.

</details>


### [753] [Reference-Free Rating of LLM Responses via Latent Information](https://arxiv.org/abs/2509.24678)
*Leander Girrbach,Chi-Ping Su,Tankred Saanum,Richard Socher,Eric Schulz,Zeynep Akata*

Main category: cs.CL

TL;DR: 研究无参考时单响应大语言模型评分可靠性，指出问题并提出Latent Judges方法，该方法效果好。


<details>
  <summary>Details</summary>
Motivation: 探究无参考时单响应大语言模型评分的可靠性及能否获得细粒度确定性分数。

Method: 提出Latent Judges方法，从内部模型信号得出标量评分，包括概率加权分数、验证式概率和线性探针。

Result: 在多项基准测试中，潜在方法表现匹配或超越标准提示，概率加权分数单评分相关性最强，探针在输出对数未校准时有作用。

Conclusion: 潜在信息能为无参考评估提供确定性和更具区分性的信号，可改进选择和训练方法。

Abstract: How reliable are single-response LLM-as-a-judge ratings without references,
and can we obtain fine-grained, deterministic scores in this setting? We study
the common practice of asking a judge model to assign Likert-scale scores to
free-text responses and show two systematic issues: scores are unstable under
sampling and poorly calibrated, leading to compression near the top of the
scale and frequent ties. We then propose and evaluate Latent Judges, which
derive scalar ratings from internal model signals: (i) probability-weighted
scores over integer ratings, (ii) verifier-style probabilities of "yes", and
(iii) linear probes trained on model activations at the rating position. Across
a broad suite of pairwise and single-rating benchmarks, latent methods match or
surpass standard prompting, with consistent gains on pairwise accuracy and
listwise ranking relevant to Best-of-N selection. Probability-weighted scores
achieve the strongest single-rating correlations, while probes recover useful
signals when output logits are miscalibrated. These results indicate that
latent information provides deterministic and more discriminative signals for
reference-free evaluation, and can improve selection and training approaches
like Best-of-$N$, multi-teacher distillation, and routing.

</details>


### [754] [AdaThink-Med: Medical Adaptive Thinking with Uncertainty-Guided Length Calibration](https://arxiv.org/abs/2509.24560)
*Shaohao Rui,Kaitao Chen,Weijie Ma,Xiaosong Wang*

Main category: cs.CL

TL;DR: 现有医疗大语言模型推理成本高，提出AdaThink - Med框架，可在减少推理长度同时保持性能，还自发形成两种推理模式。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型无论问题难易都进行冗长推理，增加推理成本，且缺乏提升自适应思维能力的端到端方法。

Method: 提出AdaThink - Med框架，先为每个问题生成多个候选输出，评估其正确性和不确定性，再通过不确定性引导的长度校准模块估计问题难度，对不同难度和答案情况采取不同策略。

Result: 在六个公共医疗问答基准上，平均推理长度最多减少6.4倍，性能仅有极小下降，且自发形成“非思考”和“思考”两种推理模式。

Conclusion: AdaThink - Med框架能有效提升医疗推理模型的自适应思维能力，抑制冗余推理过程。

Abstract: Recent advances in inference time scaling with extended long chain-of thought
have significantly improved the reasoning capabilities of both general and
medical large language models (LLMs). However, these models tend to engage in
lengthy reasoning processes regardless of the difficulty of the input question,
leading to increased inference costs in real-world applications. Therefore,
enabling adaptive thinking where models think less for simpler questions and
think more for complex ones is critical for the effective use of medical LLMs
in practice. Despite its importance, there is a lack of end-to-end approaches
designed to enhance the adaptive thinking capabilities of medical LLMs while
providing a comprehensive examination of the trade-off between performance and
computational cost. To bridge this gap, we propose AdaThink-Med, the first
end-to-end framework designed to enhance adaptive thinking ability in medical
reasoning models with uncertainty-guided length calibration. AdaThink-Med first
generates multiple candidate outputs for each question, evaluates the
correctness and uncertainty of each candidate, and then estimates problem
difficulty via an uncertainty-guided length calibration module. For outputs
with low difficulty and correct answers, the framework penalizes longer
reasoning paths; whereas for those with high difficulty and incorrect answers,
it encourages extending the chain of thought to explore alternative solutions.
On six public medical QA benchmarks, AdaThink-Med achieves up to 6.4x length
reduction on average while retaining performance with only minimal degradation.
Intriguingly, we observe that AdaThink-Med spontaneously develops two distinct
reasoning modes, which we characterize as "non-thinking" and "thinking",
demonstrating the model's ability to suppress redundant reasoning processes
dynamically.

</details>


### [755] [ProxyAttn: Guided Sparse Attention via Representative Heads](https://arxiv.org/abs/2509.24745)
*Yixuan Wang,Huang He,Siqi Bao,Hua Wu,Haifeng Wang,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 提出训练无关的稀疏注意力算法ProxyAttn，通过压缩注意力头维度实现更精确块估计，实验验证其在性能和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有动态估计块重要性方法在高稀疏率下性能下降，需更精确块估计方法。

Method: 压缩注意力头维度，用池化代表头分数近似所有头分数，提出块感知动态预算估计方法。

Result: 实验证实注意力头间存在相似性，ProxyAttn能实现最高10.3倍注意力加速和2.4倍预填充加速，且无显著性能损失。

Conclusion: ProxyAttn能以低计算成本实现更细粒度块重要性评估，在性能和效率上表现出色。

Abstract: The quadratic complexity of attention mechanisms limits the efficiency of
Large Language Models (LLMs) on long-text tasks. Recently, methods that
dynamically estimate block importance have enabled efficient block sparse
attention, leading to significant acceleration in long-text pre-filling of
LLMs. However, their coarse-grained estimation inevitably leads to performance
degradation at high sparsity rates. In this work, we propose ProxyAttn, a
training-free sparse attention algorithm that achieves more precise block
estimation by compressing the dimension of attention heads. Based on our
observation of the similarity among multiple attention heads, we use the scores
of pooled representative heads to approximate the scores for all heads. To
account for the varying sparsity among heads, we also propose a block-aware
dynamic budget estimation method. By combining the scores from representative
proxy heads with multi-head dynamic budgets, we achieve a more fine-grained
block importance evaluation at low computational cost. Experiments on a variety
of mainstream models and extensive benchmarks confirm the underlying similarity
among attention heads. Leveraging a fine-grained estimation, the proposed
method achieves substantial gains in performance and efficiency compared to
existing methods. More precisely, ProxyAttn can achieve up to 10.3x attention
acceleration and 2.4x prefilling acceleration without significant performance
loss. Our code is available at https://github.com/wyxstriker/ProxyAttn.

</details>


### [756] [Understanding the Dilemma of Unlearning for Large Language Models](https://arxiv.org/abs/2509.24675)
*Qingjie Zhang,Haoting Qian,Zhicong Huang,Cheng Hong,Minlie Huang,Ke Xu,Chao Zhang,Han Qiu*

Main category: cs.CL

TL;DR: 本文提出可解释框架unPact研究大语言模型反学习机制，发现现有反学习方法存在不足与过度破坏的两难困境。


<details>
  <summary>Details</summary>
Motivation: 大语言模型反学习效果存争议且缺乏对其机制的可解释性分析。

Method: 提出unPact框架，通过提示归因和贡献跟踪量化提示标记对输出的影响，对比反学习前后变化。

Result: 反学习通过破坏对提示关键词的关注起作用；很多知识未真正擦除，可通过强调关键词恢复；灾难性遗忘源于对所有标记的无差别惩罚。

Conclusion: 现有反学习方法存在不足或过度破坏问题，距离可靠反学习仍有差距。

Abstract: Unlearning seeks to remove specific knowledge from large language models
(LLMs), but its effectiveness remains contested. On one side, "forgotten"
knowledge can often be recovered through interventions such as light
fine-tuning; on the other side, unlearning may induce catastrophic forgetting
that degrades general capabilities. Despite active exploration of unlearning
methods, interpretability analyses of the mechanism are scarce due to the
difficulty of tracing knowledge in LLMs' complex architectures. We address this
gap by proposing unPact, an interpretable framework for unlearning via prompt
attribution and contribution tracking. Typically, it quantifies each prompt
token's influence on outputs, enabling pre- and post-unlearning comparisons to
reveal what changes. Across six mainstream unlearning methods, three LLMs, and
three benchmarks, we find that: (1) Unlearning appears to be effective by
disrupting focus on keywords in prompt; (2) Much of the knowledge is not truly
erased and can be recovered by simply emphasizing these keywords in prompts,
without modifying the model's weights; (3) Catastrophic forgetting arises from
indiscriminate penalization of all tokens. Taken together, our results suggest
an unlearning dilemma: existing methods tend either to be insufficient -
knowledge remains recoverable by keyword emphasis, or overly destructive -
general performance collapses due to catastrophic forgetting, still leaving a
gap to reliable unlearning.

</details>


### [757] [Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct](https://arxiv.org/abs/2509.25035)
*Haoyang Zheng,Xinyang Liu,Cindy Xiangrui Kong,Nan Jiang,Zheyuan Hu,Weijian Luo,Wei Deng,Guang Lin*

Main category: cs.CL

TL;DR: 本文提出DiDi - Instruct方法实现快速语言生成，性能优于基线模型，且效率高，将发布代码和模型。


<details>
  <summary>Details</summary>
Motivation: 追求AI时代语言文本的快速生成。

Method: 从预训练离散扩散语言模型初始化，基于训练的DiDi - Instruct方法，结合积分KL散度最小化框架、分组奖励归一化等技术。

Result: DiDi - Instruct模型比dLLM和GPT - 2基线加速64倍，在OpenWebText上表现优于其他模型，样本困惑度良好，熵损失小，训练时间少。

Conclusion: DiDi - Instruct是高效有效的蒸馏方法，能实现快速语言生成。

Abstract: Fast generation of language texts is the holy grail that people pursue in the
AI era. In this work, we introduced Discrete Diffusion Divergence Instruct
(DiDi-Instruct), a training-based method that leads to fast language generation
models by initializing from a pre-trained (masked) discrete diffusion language
model (dLLM). The resulting DiDi-Instruct model outperforms the dLLM
counterparts and the GPT-2 baseline with 64x acceleration. In the theoretical
part of the paper, we build the foundation of DiDi-Instruct in a framework of
integral KL-divergence minimization, with practical training algorithms. We
also introduce techniques like grouped reward normalization, intermediate-state
matching, and the reward-guided ancestral sampler (RGAS) that significantly
improve the training stability, the model coverage, and the inference
performances. On OpenWebText, DiDi-Instruct outperforms all accelerated
language generation models as well as the GPT-2 baseline and the standard
dLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128
NFEs). These performance gains are accomplished with a negligible entropy loss
of about 1% and 20x less additional training wall-clock time. We further
validate the robustness and effectiveness of DiDi-Instruct through extensive
ablation studies, model scaling, and the generation of discrete protein
sequences. In conclusion, DiDi-Instruct is an efficient yet effective
distillation method, enabling language generation in the blink of an eye. We
will release both code and models at github.com/haoyangzheng-ai/didi-instruct.

</details>


### [758] [Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures](https://arxiv.org/abs/2509.25045)
*Marco Bronzini,Carlo Nicolini,Bruno Lepri,Jacopo Staiano,Andrea Passerini*

Main category: cs.CL

TL;DR: 提出Hyperdimensional Probe新范式用于从大语言模型向量空间解码信息，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型内部表示理解有限，现有解释性方法存在局限性。

Method: 结合符号表示和神经探测，通过向量符号架构将模型残差流投影到可解释概念。

Result: 在不同大语言模型、嵌入大小和输入领域可靠提取有意义概念，帮助识别模型失败。

Conclusion: 推动大语言模型向量空间信息解码，能从神经表示中提取更具信息性、可解释性和结构化的特征。

Abstract: Despite their capabilities, Large Language Models (LLMs) remain opaque with
limited understanding of their internal representations. Current
interpretability methods, such as direct logit attribution (DLA) and sparse
autoencoders (SAEs), provide restricted insight due to limitations such as the
model's output vocabulary or unclear feature names. This work introduces
Hyperdimensional Probe, a novel paradigm for decoding information from the LLM
vector space. It combines ideas from symbolic representations and neural
probing to project the model's residual stream into interpretable concepts via
Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs
and conventional probes while overcoming their key limitations. We validate our
decoding paradigm with controlled input-completion tasks, probing the model's
final state before next-token prediction on inputs spanning syntactic pattern
recognition, key-value associations, and abstract inference. We further assess
it in a question-answering setting, examining the state of the model both
before and after text generation. Our experiments show that our probe reliably
extracts meaningful concepts across varied LLMs, embedding sizes, and input
domains, also helping identify LLM failures. Our work advances information
decoding in LLM vector space, enabling extracting more informative,
interpretable, and structured features from neural representations.

</details>


### [759] [SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching](https://arxiv.org/abs/2509.24832)
*Xinye Zhao,Spyridon Mastorakis*

Main category: cs.CL

TL;DR: 现有大语言模型推理时KV缓存内存占用成瓶颈，提出SemShareKV框架，利用模糊匹配和旋转位置嵌入复用语义相似提示中的KVCache，实验显示有加速和降低内存效果。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩和复用方法在语义相似但词汇不同的提示场景中受限，需要更好的解决方案来加速大语言模型推理。

Method: 提出SemShareKV框架，运用局部敏感哈希对词嵌入进行模糊匹配，并结合旋转位置嵌入，选择性复用参考提示缓存中的键值对。

Result: 在不同摘要数据集实验中，输入5k个令牌时加速达6.25倍，GPU内存使用降低42%，且输出质量下降可忽略不计。

Conclusion: 语义感知的缓存共享对高效大语言模型推理有很大潜力。

Abstract: As large language models (LLMs) continue to scale, the memory footprint of
key-value (KV) caches during inference has become a significant bottleneck.
Existing approaches primarily focus on compressing KV caches within a single
prompt or reusing shared prefixes or frequently ocurred text segments across
prompts. However, such strategies are limited in scenarios where prompts are
semantically similar but lexically different, which frequently occurs in tasks
such as multi-document summarization and conversational agents. We propose
\textit{SemShareKV}, a KV cache sharing and compression framework that
accelerates LLM inference by reusing KVCache in semantically similar prompts.
Instead of relying on exact token matches, SemShareKV applies fuzzy token
matching using locality-sensitive hashing (LSH) on token embeddings and
incorporates Rotary Position Embedding (RoPE) to better preserve positional
information. By selectively reusing relevant key-value pairs from a reference
prompt's cache, SemShareKV reduces redundant computation while maintaining
output quality. Experiments on diverse summarization datasets show up to
6.25$\times$ speedup and 42\% lower GPU memory usage with 5k tokens input, with
negligible quality degradation. These results highlight the potential of
semantic-aware cache sharing for efficient LLM inference.

</details>


### [760] [Hierarchical Error Correction for Large Language Models: A Systematic Framework for Domain-Specific AI Quality Enhancement](https://arxiv.org/abs/2509.24841)
*Zhilong Zhao,Yindi Liu*

Main category: cs.CL

TL;DR: 本文提出分层纠错框架解决大语言模型在专业领域的性能问题，经多领域和多模型验证有提升，但在高基线任务有局限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在专业领域存在显著性能挑战，如医学编码任务准确率低，需解决特定领域AI的局限性。

Method: 分析四个专业领域的错误模式，发现AI错误具有层次结构，开发三阶段纠错框架按层次重要性纠错。

Result: 在多个领域实验验证有一致提升，跨五个LLM架构平均提升11.2个百分点；但在高基线任务（准确率>75%）存在局限性。

Conclusion: 系统的错误分析能指导专业领域的AI增强策略，尤其适用于中等基线任务，同时要理解框架边界以实现最优部署。

Abstract: Large Language Models face significant performance challenges in specialized
domains, with state-of-the-art models achieving only 45.9% accuracy on medical
coding tasks. This study proposes a Hierarchical Error Correction (HEC)
framework that addresses domain-specific AI limitations through systematic
error analysis and targeted intervention strategies.
  We analyze error patterns across four specialized domains and find that AI
errors follow consistent hierarchical structures: Knowledge-layer errors
(58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%).
Based on these patterns, we develop a three-stage correction framework that
addresses errors according to their hierarchical importance and demonstrates
that framework effectiveness correlates inversely with baseline task
performance.
  Experimental validation across medical transcription (4,921 cases), legal
document classification (1,000 cases), political bias detection (645 cases),
and legal reasoning (1,000 cases) shows consistent improvements. Cross-model
validation across five LLM architectures demonstrates average improvements of
11.2 percentage points (p < 0.001). However, analysis reveals framework
limitations in high-baseline tasks (>75% accuracy), where hierarchical
intervention may interfere with effective reasoning processes.
  The results suggest that systematic error analysis can guide effective AI
enhancement strategies in specialized domains, particularly for
moderate-baseline tasks, while highlighting the importance of understanding
framework boundaries for optimal deployment.

</details>


### [761] [Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning](https://arxiv.org/abs/2509.24866)
*Matteo Fuoli,Weihang Huang,Jeannette Littlemore,Sarah Turner,Ellen Wilding*

Main category: cs.CL

TL;DR: 研究探索大语言模型自动识别隐喻的潜力，比较三种方法，结果显示闭源大语言模型精度高，可部分自动化隐喻识别并完善相关理论。


<details>
  <summary>Details</summary>
Motivation: 因隐喻的上下文敏感性，大规模分析受手动标注限制，故研究大语言模型自动识别隐喻的潜力。

Method: 比较三种方法，包括检索增强生成、提示工程（含零样本、少样本和思维链策略）和微调，并在提示工程中进行不同策略测试。

Result: 最先进的闭源大语言模型能达到高精度，微调的中位数F1分数为0.79，人机输出差异多是系统性的。

Conclusion: 大语言模型可至少部分自动化隐喻识别，还能作为开发和完善隐喻识别协议及理论的试验台。

Abstract: Metaphor is a pervasive feature of discourse and a powerful lens for
examining cognition, emotion, and ideology. Large-scale analysis, however, has
been constrained by the need for manual annotation due to the context-sensitive
nature of metaphor. This study investigates the potential of large language
models (LLMs) to automate metaphor identification in full texts. We compare
three methods: (i) retrieval-augmented generation (RAG), where the model is
provided with a codebook and instructed to annotate texts based on its rules
and examples; (ii) prompt engineering, where we design task-specific verbal
instructions; and (iii) fine-tuning, where the model is trained on hand-coded
texts to optimize performance. Within prompt engineering, we test zero-shot,
few-shot, and chain-of-thought strategies. Our results show that
state-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning
yielding a median F1 score of 0.79. A comparison of human and LLM outputs
reveals that most discrepancies are systematic, reflecting well-known grey
areas and conceptual challenges in metaphor theory. We propose that LLMs can be
used to at least partly automate metaphor identification and can serve as a
testbed for developing and refining metaphor identification protocols and the
theory that underpins them.

</details>


### [762] [Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs for Low-Resource Text Generation](https://arxiv.org/abs/2509.25144)
*Yen-Ju Lu,Thomas Thebaud,Laureano Moro-Velazquez,Najim Dehak,Jesus Villalba*

Main category: cs.CL

TL;DR: 提出PbT两阶段师生管道合成无人工标注和并行数据的准确输入输出对，在多基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 低资源NLG场景中输入输出数据不匹配，小模型学习样本少或依赖大模型生成的合成样本成本高。

Method: 让教师大语言模型将未配对示例压缩为简洁中间表示，训练学生模型从中间表示重建输入，使输出与学生生成输入配对。

Result: 8B学生模型仅在PbT数据上训练，优于在70B教师生成语料库和其他无监督基线模型上训练的模型，接近人工标注对效果，人工评估也证实其优势。

Conclusion: PbT能生成领域内源数据，避免不匹配问题，在低资源NLG场景有优势。

Abstract: We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline
that synthesizes accurate input-output pairs without human labels or parallel
data. In many low-resource natural language generation (NLG) scenarios,
practitioners may have only raw outputs, like highlights, recaps, or questions,
or only raw inputs, such as articles, dialogues, or paragraphs, but seldom
both. This mismatch forces small models to learn from very few examples or rely
on costly, broad-scope synthetic examples produced by large LLMs. PbT addresses
this by asking a teacher LLM to compress each unpaired example into a concise
intermediate representation (IR), and training a student to reconstruct inputs
from IRs. This enables outputs to be paired with student-generated inputs,
yielding high-quality synthetic data. We evaluate PbT on five
benchmarks-document summarization (XSum, CNNDM), dialogue summarization
(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired
setting on SwitchBoard (paired with DialogSum summaries). An 8B student trained
only on PbT data outperforms models trained on 70 B teacher-generated corpora
and other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated
pairs and closing 82% of the oracle gap at one-third the annotation cost of
direct synthesis. Human evaluation on SwitchBoard further confirms that only
PbT produces concise, faithful summaries aligned with the target style,
highlighting its advantage of generating in-domain sources that avoid the
mismatch, limiting direct synthesis.

</details>


### [763] [Pretraining Large Language Models with NVFP4](https://arxiv.org/abs/2509.25149)
*NVIDIA,Felix Abecassis,Anjulie Agrusa,Dong Ahn,Jonah Alben,Stefania Alborghetti,Michael Andersch,Sivakumar Arayandi,Alexis Bjorlin,Aaron Blakeman,Evan Briones,Ian Buck,Bryan Catanzaro,Jinhang Choi,Mike Chrzanowski,Eric Chung,Victor Cui,Steve Dai,Bita Darvish Rouhani,Carlo del Mundo,Deena Donia,Burc Eryilmaz,Henry Estela,Abhinav Goel,Oleg Goncharov,Yugi Guvvala,Robert Hesse,Russell Hewett,Herbert Hum,Ujval Kapasi,Brucek Khailany,Mikail Khona,Nick Knight,Alex Kondratenko,Ronny Krashinsky,Ben Lanir,Simon Layton,Michael Lightstone,Daniel Lo,Paulius Micikevicius,Asit Mishra,Tim Moon,Deepak Narayanan,Chao Ni,Abhijit Paithankar,Satish Pasumarthi,Ankit Patel,Mostofa Patwary,Ashwin Poojary,Gargi Prasad,Sweta Priyadarshi,Yigong Qin,Xiaowei Ren,Oleg Rybakov,Charbel Sakr,Sanjeev Satheesh,Stas Sergienko,Pasha Shamis,Kirthi Shankar,Nishant Sharma,Mohammad Shoeybi,Michael Siu,Misha Smelyanskiy,Darko Stosic,Dusan Stosic,Bor-Yiing Su,Frank Sun,Nima Tajbakhsh,Shelby Thomas,Przemek Tredak,Evgeny Tsykunov,Gandhi Vaithilingam,Aditya Vavre,Rangharajan Venkatesan,Roger Waleffe,Qiyu Wan,Hexin Wang,Mengdi Wang,Lizzie Wei,Hao Wu,Evan Wu,Keith Wyss,Ning Xu,Jinze Xue,Charlene Yang,Yujia Zhai,Ruoxi Zhang,Jingyang Zhu,Zhongbo Zhu*

Main category: cs.CL

TL;DR: 本文介绍用NVFP4格式稳定准确训练大语言模型的新方法，经实验验证该方法效果与FP8基线相当，推动窄精度LLM训练算法进步。


<details>
  <summary>Details</summary>
Motivation: 训练前沿大语言模型需大量资源，提高预训练效率很重要，向更窄精度过渡可提升计算速度和资源利用率，但低精度量化存在挑战。

Method: 引入用NVFP4格式训练LLMs的新方法，集成随机哈达玛变换、二维量化方案、随机舍入和选择性高精度层。

Result: 用该方法训练120亿参数模型，在10万亿tokens上训练，训练损失和下游任务准确率与FP8基线相当。

Conclusion: NVFP4结合该训练方法是窄精度LLM训练算法的重大进步。

Abstract: Large Language Models (LLMs) today are powerful problem solvers across many
domains, and they continue to get stronger as they scale in model size,
training set size, and training set quality, as shown by extensive research and
experimentation across the industry. Training a frontier model today requires
on the order of tens to hundreds of yottaflops, which is a massive investment
of time, compute, and energy. Improving pretraining efficiency is therefore
essential to enable the next generation of even more capable LLMs. While 8-bit
floating point (FP8) training is now widely adopted, transitioning to even
narrower precision, such as 4-bit floating point (FP4), could unlock additional
improvements in computational speed and resource utilization. However,
quantization at this level poses challenges to training stability, convergence,
and implementation, notably for large-scale models trained on long token
horizons.
  In this study, we introduce a novel approach for stable and accurate training
of large language models (LLMs) using the NVFP4 format. Our method integrates
Random Hadamard transforms (RHT) to bound block-level outliers, employs a
two-dimensional quantization scheme for consistent representations across both
the forward and backward passes, utilizes stochastic rounding for unbiased
gradient estimation, and incorporates selective high-precision layers. We
validate our approach by training a 12-billion-parameter model on 10 trillion
tokens -- the longest publicly documented training run in 4-bit precision to
date. Our results show that the model trained with our NVFP4-based pretraining
technique achieves training loss and downstream task accuracies comparable to
an FP8 baseline. These findings highlight that NVFP4, when combined with our
training approach, represents a major step forward in narrow-precision LLM
training algorithms.

</details>


### [764] [MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes](https://arxiv.org/abs/2509.24945)
*Changsheng Zhao,Ernie Chang,Zechun Liu,Chia-Jung Chang,Wei Wen,Chen Lai,Rick Cao,Yuandong Tian,Raghuraman Krishnamoorthi,Yangyang Shi,Vikas Chandra*

Main category: cs.CL

TL;DR: 本文重新审视推理能力涌现是否需大规模语料，发现高质量小数据也能让模型涌现强推理能力，还发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 挑战大语言模型推理能力涌现需在大规模数据集上训练的假设，重新审视超大型语料对推理能力涌现的必要性。

Method: 精心策划和重采样开源数据集，通过4.2T重采样数据预训练和既定的后训练程序开发模型。

Result: 开发出亚10亿参数推理模型MobileLLM - R1，远超之前在完全开源数据上训练的模型，如MobileLLM - R1 - 950M表现出色。

Conclusion: 高质量小数据也能让模型涌现强推理能力，发布研究资源以推动相关研究。

Abstract: The paradigm shift in large language models (LLMs) from instinctive responses
to chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1)
reasoning capabilities only emerge in sufficiently large models, and (2) such
capabilities require training on massive datasets. While the first assumption
has already been challenged by recent sub-billion-parameter reasoning models
such as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely
unquestioned. In this work, we revisit the necessity of scaling to extremely
large corpora (>10T tokens) for reasoning emergence. By carefully curating and
resampling open-source datasets that we identify as beneficial under our
designed metrics, we demonstrate that strong reasoning abilities can emerge
with far less data. Specifically, we show that only ~2T tokens of high-quality
data are sufficient, and pre-training with 4.2T tokens on the dataset resampled
from these ~2T tokens, followed by a established post-training procedure,
enables the development of MobileLLM-R1, a series of sub-billion-parameter
reasoning models that substantially outperform prior models trained on fully
open-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of
15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B.
Remarkably, despite being trained on only 11.7% of the tokens compared to
Qwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches
or surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate
further research in this direction, we have released the complete training
recipe, data sources, data mixing ratio, and model checkpoints, together with
the key insights obtained throughout this study.

</details>


### [765] [Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns](https://arxiv.org/abs/2509.24988)
*Hanqi Xiao,Vaidehi Patil,Hyunji Lee,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: 文章指出生成准确校准的置信估计是LLM应用的挑战，提出广义正确性模型GCM，发现可靠的LLM置信估计是可泛化、与模型无关的技能。


<details>
  <summary>Details</summary>
Motivation: 解决在高风险或面向用户的应用中部署大语言模型时，生成准确且校准的置信估计这一开放挑战。

Method: 提出多种方法注入历史正确性信息构建GCM，系统控制训练数据研究正确性预测能力来源及泛化，探索不训练LLM注入历史的替代方法。

Result: GCM可跨数据集和模型学习正确性预测模式，答案措辞是正确性的强预测指标，包含历史上下文示例和事后校准可提升效果。

Conclusion: 可靠的LLM置信估计是通过系统编码正确性历史学到的可泛化、与模型无关的技能，而非依赖自我内省的特定模型技能。

Abstract: Generating accurate and calibrated confidence estimates is critical for
deploying LLMs in high-stakes or user-facing applications, and remains an open
challenge. Prior research has often framed confidence as a problem of eliciting
a model's "self-knowledge", i.e., the ability of an LLM to judge whether its
own answers are correct; this approach implicitly assumes that there is some
privileged information about the answer's correctness that is accessible to the
model itself. However, our experiments reveal that an LLM attempting to predict
the correctness of its own outputs generally performs no better than an
unrelated LLM. Moreover, we hypothesize that a key factor in building a
"Correctness Model" (CM) is exposure to a target model's historical
predictions. We propose multiple methods to inject this historical correctness
information, creating a Generalized Correctness Model (GCM). We first show that
GCMs can be trained on the correctness data from many LLMs and learn patterns
for correctness prediction applicable across datasets and models. We then use
CMs as a lens for studying the source of correctness prediction ability and its
generalization, systematically controlling their training data and finding that
answer phrasing is a strong predictor for correctness. We further explore
alternative methods of injecting history without training an LLM, finding that
including history as in-context examples can help improve correctness
prediction, and post-hoc calibration can provide complementary reductions in
calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families
and the MMLU and TriviaQA datasets, as well as on a downstream selective
prediction task, finding that reliable LLM confidence estimation is a
generalizable and model-agnostic skill learned by systematically encoding
correctness history rather than a model-specific skill reliant on
self-introspection.

</details>


### [766] [EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering](https://arxiv.org/abs/2509.25175)
*Haolei Xu,Xinyu Mei,Yuchen Yan,Rui Zhou,Wenqi Zhang,Weiming Lu,Yueting Zhuang,Yongliang Shen*

Main category: cs.CL

TL;DR: 提出基于vLLM的统一框架EasySteer用于大语言模型转向，比现有框架快5.5 - 11.4倍，实验证明其有效性，让转向从研究技术变为可用于生产的能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型转向框架存在计算效率低、扩展性有限和功能受限等问题，阻碍研究进展和实际部署。

Method: 构建基于vLLM的统一框架EasySteer，具有模块化架构、可插拔接口、细粒度参数控制、预计算转向向量和交互式演示系统。

Result: EasySteer比现有框架实现了5.5 - 11.4倍的加速，在缓解过度思考、减少幻觉等关键应用中有效。

Conclusion: EasySteer将转向从研究技术转变为可用于生产的能力，为可部署、可控的语言模型建立了关键基础设施。

Abstract: Large language model (LLM) steering has emerged as a promising paradigm for
controlling model behavior at inference time through targeted manipulation of
hidden states, offering a lightweight alternative to expensive retraining.
However, existing steering frameworks suffer from critical limitations:
computational inefficiency, limited extensibility, and restricted functionality
that hinder both research progress and practical deployment. We present
EasySteer, a unified framework for high-performance, extensible LLM steering
built on vLLM. Our system features modular architecture with pluggable
interfaces for both analysis-based and learning-based methods, fine-grained
parameter control, pre-computed steering vectors for eight application domains,
and an interactive demonstration system. Through deep integration with vLLM's
optimized inference engine, EasySteer achieves 5.5-11.4$\times$ speedup over
existing frameworks. Extensive experiments demonstrate its effectiveness in
overthinking mitigation, hallucination reduction, and other key applications.
EasySteer transforms steering from research technique to production-ready
capability, establishing critical infrastructure for deployable, controllable
language models.

</details>


### [767] [NAIPv2: Debiased Pairwise Learning for Efficient Paper Quality Estimation](https://arxiv.org/abs/2509.25179)
*Penghai Zhao,Jinyu Tian,Qinghua Xing,Xin Zhang,Zheng Li,Jianjun Qian,Ming-Ming Cheng,Xiang Li*

Main category: cs.CL

TL;DR: 提出用于论文质量评估的NAIPv2框架和NAIDv2数据集，在性能和效率上表现出色，泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的论文质量评估方法推理成本高，直接分数回归方法存在规模不一致问题，需要更好的评估方法。

Method: 在领域 - 年份组内进行成对学习，引入评审倾向信号（RTS），构建NAIDv2数据集。

Result: NAIPv2取得了78.2%的AUC和0.432的Spearman系数，在推理时保持线性时间效率，在未见的NeurIPS提交上有强泛化能力。

Conclusion: NAIPv2是一种去偏且可扩展的自动论文质量评估框架，向未来科学智能系统迈进了一步。

Abstract: The ability to estimate the quality of scientific papers is central to how
both humans and AI systems will advance scientific knowledge in the future.
However, existing LLM-based estimation methods suffer from high inference cost,
whereas the faster direct score regression approach is limited by scale
inconsistencies. We present NAIPv2, a debiased and efficient framework for
paper quality estimation. NAIPv2 employs pairwise learning within domain-year
groups to reduce inconsistencies in reviewer ratings and introduces the Review
Tendency Signal (RTS) as a probabilistic integration of reviewer scores and
confidences. To support training and evaluation, we further construct NAIDv2, a
large-scale dataset of 24,276 ICLR submissions enriched with metadata and
detailed structured content. Trained on pairwise comparisons but enabling
efficient pointwise prediction at deployment, NAIPv2 achieves state-of-the-art
performance (78.2% AUC, 0.432 Spearman), while maintaining scalable,
linear-time efficiency at inference. Notably, on unseen NeurIPS submissions, it
further demonstrates strong generalization, with predicted scores increasing
consistently across decision categories from Rejected to Oral. These findings
establish NAIPv2 as a debiased and scalable framework for automated paper
quality estimation, marking a step toward future scientific intelligence
systems. Code and dataset are released at
https://sway.cloud.microsoft/Pr42npP80MfPhvj8.

</details>


### [768] [InfoAgent: Advancing Autonomous Information-Seeking Agents](https://arxiv.org/abs/2509.25189)
*Gongrui Zhang,Jialiang Zhu,Ruiqi Yang,Kai Qiu,Miaosen Zhang,Zhirong Wu,Qi Dai,Bei Liu,Chong Luo,Zhengyuan Yang,Linjie Li,Lijuan Wang,Weizhu Chen,Yuan Zhang,Xin Li,Zhaoyi Liu,Xin Geng,Baining Guo*

Main category: cs.CL

TL;DR: 本文介绍InfoAgent，它由创新数据合成管道和编排式网络搜索工具驱动，构建挑战性查询，开发自托管搜索基础设施，经两阶段训练，在多个数据集上表现优于先前开源代理。


<details>
  <summary>Details</summary>
Motivation: 构建能通过与外部工具交互扩展能力的大语言模型代理，推动AI研究和应用发展。

Method: 构建实体树和应用子树采样与实体模糊化构建挑战性查询；开发自托管搜索基础设施；采用两阶段训练方法，包括冷启动监督微调与强化学习。

Result: InfoAgent在BrowseComp、BrowseComp - ZH和Xbench - DS上分别达到15.3%、29.2%和40.4%的准确率，优于WebSailor - 72B和DeepDive - 32B等先前开源深度研究代理。

Conclusion: 所提出的方法有效，InfoAgent在性能上超越了先前的开源深度研究代理。

Abstract: Building Large Language Model agents that expand their capabilities by
interacting with external tools represents a new frontier in AI research and
applications. In this paper, we introduce InfoAgent, a deep research agent
powered by an innovative data synthesis pipeline and orchestrated web search
tools. To construct challenging, hard-to-find queries,we build entity trees and
apply sub-tree sampling with entity fuzzification to systematically increase
question difficulty. Unlike prior work that relies heavily on commercial search
tools, we develop a dedicated self-hosted search infrastructure, enhancing
transparency of agent environments and facilitating further advancement of
agent capacity. We evaluate the effectiveness of our data pipeline by measuring
the average number of tool calls required to correctly answer a question, and
also show that our agent yields better performance when equipped with our
tools. Our \mbox{InfoAgent} is post-trained from Qwen3-14B using a two-stage
recipe: cold-start supervised finetuning to instill long-horizon search
behaviors, followed by reinforcement learning which significantly improves
reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\% accuracy
on BrowseComp, 29.2\% on BrowseComp-ZH, and 40.4\% on Xbench-DS, outperforming
prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [769] [Bounds for the Permutation Flowshop Scheduling Problem: New Framework and Theoretical Insights](https://arxiv.org/abs/2509.23512)
*J. A. Alejandro-Soto,Carlos Segura,Joel Antonio Trejo-Sanchez*

Main category: math.OC

TL;DR: 本文用矩阵公式为排列流水车间调度问题推导上下界框架，在特定参数下多项式时间求解，经测试改进多数基准实例的界，还给出渐近结果。


<details>
  <summary>Details</summary>
Motivation: 为排列流水车间调度问题（PFSP）推导上界和获取下界的通用框架。

Method: 使用矩阵公式，提出涉及求解路径集上的最小 - 最大或最大 - 最小表达式的框架，引入特定路径集，在特定有界参数下可多项式时间求解。

Result: 在Taillard的120个实例中的112个和VRF的480个实例中的430个上改进了界，包括大小实例；给出更准确的可能完工时间值数量估计和渐近结果。

Conclusion: 所提方法具有可扩展性，在改进界方面有效，为相关猜想和算法近似比提供进展。

Abstract: In this work, we use the matrix formulation of the Permutation Flowshop
Scheduling Problem with makespan minimization to derive an upper bound and a
general framework for obtaining lower bounds. The proposed framework involves
solving a min-max or max-min expression over a set of paths. We introduce a
family of such path sets for which the min-max expression can be solved in
polynomial time under certain bounded parameters. To validate the proposed
approach, we test it on the Taillard and VRF benchmark instances, the two most
widely used datasets in PFSP research. Our method improves the bounds in $112$
out of the $120$ Taillard instances and $430$ out of the $480$ VRF instances.
These improvements include both small and large instances, highlighting the
scalability of the proposed methodology. Additionally, the upper bound is used
to give a more accurate estimate of the number of possible makespan values for
a given instance and to present asymptotic results which provide advances in a
conjecture given by Taillard related to the quality of one of the most popular
lower bounds, as well as the asymptotic approximation ratio of any algorithm.

</details>


### [770] [Ringleader ASGD: The First Asynchronous SGD with Optimal Time Complexity under Data Heterogeneity](https://arxiv.org/abs/2509.22860)
*Artavazd Maranjyan,Peter Richtárik*

Main category: math.OC

TL;DR: 提出Ringleader ASGD算法，在数据异质性和可变计算速度下达到异步SGD理论最优。


<details>
  <summary>Details</summary>
Motivation: 现有异步SGD方法在数据和计算能力异构的分布式优化场景存在局限。

Method: 引入Ringleader ASGD算法。

Result: Ringleader ASGD算法在平滑非凸情况下达到并行一阶随机方法理论下界。

Conclusion: Ringleader ASGD在任意和时变计算速度下保持最优，填补异步优化理论空白。

Abstract: Asynchronous stochastic gradient methods are central to scalable distributed
optimization, particularly when devices differ in computational capabilities.
Such settings arise naturally in federated learning, where training takes place
on smartphones and other heterogeneous edge devices. In addition to varying
computation speeds, these devices often hold data from different distributions.
However, existing asynchronous SGD methods struggle in such heterogeneous
settings and face two key limitations. First, many rely on unrealistic
assumptions of similarity across workers' data distributions. Second, methods
that relax this assumption still fail to achieve theoretically optimal
performance under heterogeneous computation times. We introduce Ringleader
ASGD, the first asynchronous SGD algorithm that attains the theoretical lower
bounds for parallel first-order stochastic methods in the smooth nonconvex
regime, thereby achieving optimal time complexity under data heterogeneity and
without restrictive similarity assumptions. Our analysis further establishes
that Ringleader ASGD remains optimal under arbitrary and even time-varying
worker computation speeds, closing a fundamental gap in the theory of
asynchronous optimization.

</details>


### [771] [Addressing Methodological Uncertainty in MCDM with a Systematic Pipeline Approach to Data Transformation Sensitivity Analysis](https://arxiv.org/abs/2509.24996)
*Juan B. Cabral,Alvaro Roy Schachner*

Main category: math.OC

TL;DR: 当前多准则决策方法选归一化技术缺乏系统评估，提出框架自动探索缩放变换空间并分析。


<details>
  <summary>Details</summary>
Motivation: 多准则决策方法对归一化技术选择有重要依赖，当前选择方法缺乏系统的稳健性评估。

Method: 利用Scikit - Criteria基础设施自动生成所有可能的方法组合，对缩放变换空间进行自动探索。

Result: 未提及具体结果

Conclusion: 提出的框架可解决多准则决策方法中选择归一化技术的方法学不确定性问题。

Abstract: Multicriteria decision-making methods exhibit critical dependence on the
choice of normalization techniques, where different selections can alter 20-40%
of the final rankings. Current practice is characterized by the ad-hoc
selection of methods without systematic robustness evaluation. We present a
framework that addresses this methodological uncertainty through automated
exploration of the scaling transformation space. The implementation leverages
the existing Scikit-Criteria infrastructure to automatically generate all
possible methodological combinations and provide robust comparative analysis.

</details>


### [772] [BenLOC: A Benchmark for Learning to Configure MIP Optimizers](https://arxiv.org/abs/2506.02752)
*Hongpei Li,Ziyan He,Yufei Wang,Wenting Tu,Shanwen Pu,Qi Deng,Dongdong Ge*

Main category: math.OC

TL;DR: 为解决MIP优化器自动配置缺乏标准化评估框架问题，提出BenLOC工具包并进行实证分析，结果证明其有效性。


<details>
  <summary>Details</summary>
Motivation: MIP优化器自动配置中，因缺乏标准化评估框架导致数据泄露和过度乐观的结论，需促进公平评估。

Method: 提出BenLOC工具包，提供端到端学习实例优化器配置的管道，规范评估流程；在五个MIP数据集上对比经典机器学习模型和深度学习技术。

Result: 结果证明了BenLOC提出的数据集、特征和基线标准的重要性，以及其在提供无偏和全面评估方面的有效性。

Conclusion: BenLOC工具包能够实现无偏和全面的评估，对MIP优化器自动配置评估有重要意义。

Abstract: The automatic configuration of Mixed-Integer Programming (MIP) optimizers has
become increasingly critical as the large number of configurations can
significantly affect solver performance. Yet the lack of standardized
evaluation frameworks has led to data leakage and over-optimistic claims, as
prior studies often rely on homogeneous datasets and inconsistent experimental
setups. To promote a fair evaluation process, we present BenLOC, a
comprehensive benchmark and open-source toolkit, which not only offers an
end-to-end pipeline for learning instance-wise MIP optimizer configurations,
but also standardizes dataset selection, train-test splits, feature engineering
and baseline choice for unbiased and comprehensive evaluations. Leveraging this
framework, we conduct an empirical analysis on five well-established MIP
datasets and compare classical machine learning models with handcrafted
features against state-of-the-art deep-learning techniques. The results
demonstrate the importance of datasets, features and baseline criteria proposed
by BenLOC and the effectiveness of BenLOC in providing unbiased and
comprehensive evaluations.

</details>


### [773] [Mixtures Closest to a Given Measure: A Semidefinite Programming Approach](https://arxiv.org/abs/2509.22879)
*Srećko Đurašinović,Jean-Bernard Lasserre,Victor Magron*

Main category: math.OC

TL;DR: 研究用参数族分布混合近似目标测度问题，引入半定松弛层次，有收敛性，还应用于聚类。


<details>
  <summary>Details</summary>
Motivation: 解决混合模型确定混合阶数和估计参数的挑战，尤其是高维情况下的问题。

Method: 引入半定松弛层次，将参数集建模为紧基本半代数集。

Result: 半定松弛有渐近收敛性，满足特定秩条件时收敛是有限的且可恢复最优混合测度。

Conclusion: 该框架可用于聚类，可独立使用或作为预处理步骤加速标准聚类算法收敛。

Abstract: Mixture models, such as Gaussian mixture models, are widely used in machine
learning to represent complex data distributions. A key challenge, especially
in high-dimensional settings, is to determine the mixture order and estimate
the mixture parameters. We study the problem of approximating a target measure,
available only through finitely many of its moments, by a mixture of
distributions from a parametric family (e.g., Gaussian, exponential, Poisson),
with approximation quality measured by the 2-Wasserstein or the total variation
distance. Unlike many existing approaches, the parameter set is not assumed to
be finite; it is modeled as a compact basic semi-algebraic set. We introduce a
hierarchy of semidefinite relaxations with asymptotic convergence to the
desired optimal value. In addition, when a certain rank condition is satisfied,
the convergence is even finite and recovery of an optimal mixing measure is
obtained. We also present an application to clustering, where our framework
serves either as a stand-alone method or as a preprocessing step that yields
both the number of clusters and strong initial parameter estimates, thereby
accelerating convergence of standard (local) clustering algorithms.

</details>


### [774] [New Insights and Algorithms for Optimal Diagonal Preconditioning](https://arxiv.org/abs/2509.23439)
*Saeed Ghadimi,Woosuk L. Jung,Arnesh Sujanani,David Torregrosa-Belén,Henry Wolkowicz*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Preconditioning (scaling) is essential in many areas of mathematics, and in
particular in optimization. In this work, we study the problem of finding an
optimal diagonal preconditioner. We focus on minimizing two different notions
of condition number: the classical, worst-case type, $\kappa$-condition number,
and the more averaging motivated $\omega$-condition number. We provide affine
based pseudoconvex reformulations of both optimization problems. The advantage
of our formulations is that the gradient of the objective is inexpensive to
compute and the optimization variable is just an $n\times 1$ vector. We also
provide elegant characterizations of the optimality conditions of both
problems.
  We develop a competitive subgradient method, with convergence guarantees, for
$\kappa$-optimal diagonal preconditioning that scales much better and is more
efficient than existing SDP-based approaches. We also show that the
preconditioners found by our subgradient method leads to better PCG performance
for solving linear systems than other approaches. Finally, we show the
interesting phenomenon that we can apply the $\omega$-optimal preconditioner to
the exact $\kappa$-optimally diagonally preconditioned matrix $A$ and get
consistent, significantly improved convergence results for PCG methods.

</details>


### [775] [Bundle Network: a Machine Learning-Based Bundle Method](https://arxiv.org/abs/2509.24736)
*Francesca Demelas,Joseph Le Roux,Antonio Frangioni,Mathieu Lacroix,Emiliano Traversi,Roberto Wolfler Calvo*

Main category: math.OC

TL;DR: 提出基于学习的Bundle Network算法，可自动调整正则化参数，用含注意力机制的循环神经模型替代传统优化问题迭代求解，实验表明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖启发式调整正则化参数，本文希望通过学习的方式自动调整该参数。

Method: 受凸非光滑最小化问题的Bundle Method启发，用含注意力机制的循环神经模型替代传统优化问题迭代求解，利用展开计算图通过自动微分进行端到端训练。

Result: 在多商品网络设计和广义分配问题的拉格朗日对偶松弛实验中，该方法始终优于依赖网格搜索调参的传统方法，且能有效跨数据集泛化。

Conclusion: Bundle Network算法是一种有效的算法，能自动调整参数，优于传统方法。

Abstract: This paper presents Bundle Network, a learning-based algorithm inspired by
the Bundle Method for convex non-smooth minimization problems. Unlike classical
approaches that rely on heuristic tuning of a regularization parameter, our
method automatically learns to adjust it from data. Furthermore, we replace the
iterative resolution of the optimization problem that provides the search
direction-traditionally computed as a convex combination of gradients at
visited points-with a recurrent neural model equipped with an attention
mechanism. By leveraging the unrolled graph of computation, our Bundle Network
can be trained end-to-end via automatic differentiation. Experiments on
Lagrangian dual relaxations of the Multi-Commodity Network Design and
Generalized Assignment problems demonstrate that our approach consistently
outperforms traditional methods relying on grid search for parameter tuning,
while generalizing effectively across datasets.

</details>


### [776] [Improved Stochastic Optimization of LogSumExp](https://arxiv.org/abs/2509.24894)
*Egor Gladin,Alexey Kroshnin,Jia-Jie Zhu,Pavel Dvurechensky*

Main category: math.OC

TL;DR: 提出LogSumExp近似方法，基于安全KL散度，可随机梯度优化，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 对数内指数项多或无穷时，LogSumExp优化难，现有小批量替换方法有显著偏差。

Method: 提出基于对偶中KL散度修改的新f散度（安全KL散度）的LogSumExp近似方法，可用随机梯度方法优化。

Result: 近似精度可由可调参数控制，保留凸性，目标平滑常数与L线性缩放，实验显示优于基线方法，能处理数值问题。

Conclusion: 所提LogSumExp近似方法在DRO和连续最优传输中有效，优于现有方法。

Abstract: The LogSumExp function, also known as the free energy, plays a central role
in many important optimization problems, including entropy-regularized optimal
transport and distributionally robust optimization (DRO). It is also the dual
to the Kullback-Leibler (KL) divergence, which is widely used in machine
learning. In practice, when the number of exponential terms inside the
logarithm is large or infinite, optimization becomes challenging since
computing the gradient requires differentiating every term. Previous approaches
that replace the full sum with a small batch introduce significant bias. We
propose a novel approximation to LogSumExp that can be efficiently optimized
using stochastic gradient methods. This approximation is rooted in a sound
modification of the KL divergence in the dual, resulting in a new
$f$-divergence called the safe KL divergence. The accuracy of the approximation
is controlled by a tunable parameter and can be made arbitrarily small. Like
the LogSumExp, our approximation preserves convexity. Moreover, when applied to
an $L$-smooth function bounded from below, the smoothness constant of the
resulting objective scales linearly with $L$. Experiments in DRO and continuous
optimal transport demonstrate the advantages of our approach over
state-of-the-art baselines and the effective treatment of numerical issues
associated with the standard LogSumExp and KL.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [777] [A Comparison of Surrogate Constitutive Models for Viscoplastic Creep Simulation of HT-9 Steel](https://arxiv.org/abs/2509.22667)
*Pieterjan Robbe,Andre Ruybalid,Arun Hegde,Christophe Bonneville,Habib N Najm,Laurent Capolungo,Cosmin Safta*

Main category: physics.comp-ph

TL;DR: 本文开发两种局部替代模型用于钢的粘塑性响应，应用于HT - 9钢蠕变模拟，对比显示混合专家模型精度更优。


<details>
  <summary>Details</summary>
Motivation: 多晶体力学响应的机理微观结构本构模型计算成本高，需平衡精度和计算效率的替代本构模型。

Method: 开发分段响应面法和混合专家模型两种局部替代模型，利用粘塑性自洽模拟生成训练数据，定义测试指标评估模型。

Result: 混合专家模型在预测粘塑性材料行为的精度上优于分段响应面法。

Conclusion: 数据驱动的替代模型是解决复杂本构模型计算成本高问题的有效途径，混合专家模型更具优势。

Abstract: Mechanistic microstructure-informed constitutive models for the mechanical
response of polycrystals are a cornerstone of computational materials science.
However, as these models become increasingly more complex - often involving
coupled differential equations describing the effect of specific deformation
modes - their associated computational costs can become prohibitive,
particularly in optimization or uncertainty quantification tasks that require
numerous model evaluations. To address this challenge, surrogate constitutive
models that balance accuracy and computational efficiency are highly desirable.
Data-driven surrogate models, that learn the constitutive relation directly
from data, have emerged as a promising solution. In this work, we develop two
local surrogate models for the viscoplastic response of a steel: a piecewise
response surface method and a mixture of experts model. These surrogates are
designed to adapt to complex material behavior, which may vary with material
parameters or operating conditions. The surrogate constitutive models are
applied to creep simulations of HT-9 steel, an alloy of considerable interest
to the nuclear energy sector due to its high tolerance to radiation damage,
using training data generated from viscoplastic self-consistent (VPSC)
simulations. We define a set of test metrics to numerically assess the accuracy
of our surrogate models for predicting viscoplastic material behavior, and show
that the mixture of experts model outperforms the piecewise response surface
method in terms of accuracy.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [778] [When risk defies order: On the limits of fractional stochastic dominance](https://arxiv.org/abs/2509.24747)
*Christian Laudagé,Felix-Benedikt Liebrich*

Main category: q-fin.MF

TL;DR: 研究Meyer风险度量的存在性、结构和应用，揭示货币风险度量公理结构与SSD的联系，并给出两个应用。


<details>
  <summary>Details</summary>
Motivation: 受单调加性统计和基于回报的风险度量最优风险分担问题启发，研究Meyer风险度量。

Method: 通过两种类型的表示澄清尊重$v$-SD顺序的风险度量结构，探讨不同$v$选择下非平凡例子的存在性。

Result: 许多$v$选择下非平凡例子不存在，凸性或正齐次性等属性进一步限制可接受例子，得到不可能定理。

Conclusion: 货币风险度量公理结构与SSD的联系比以往认识更深，给出投资组合优化和金融时间序列数据风险评估两个应用。

Abstract: Motivated by recent work on monotone additive statistics and questions
regarding optimal risk sharing for return-based risk measures, we investigate
the existence, structure, and applications of Meyer risk measures. Those are
monetary risk measures consistent with fractional stochastic orders suggested
by Meyer (1977a,b) as refinement of second-order stochastic dominance (SSD).
These so-called $v$-SD orders are based on a threshold utility function $v$.
The test utilities defining the associated order are those at least as risk
averse in absolute terms as $v$. The generality of $v$ allows to subsume SSD
and other examples from the literature. The structure of risk measures
respecting the $v$-SD order is clarified by two types of representations. The
existence of nontrivial examples is more subtle: for many choices of $v$
outside the exponential (CARA) class, they do not exist. Additional properties
like convexity or positive homogeneity further restrict admissible examples,
even within the CARA class. We present impossibility theorems that demonstrate
a deeper link between the axiomatic structure of monetary risk measures and SSD
than previously acknowledged. The study concludes with two applications:
portfolio optimisation under a Meyer risk measure as objective, and risk
assessment of financial time series data.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [779] [GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction](https://arxiv.org/abs/2509.25075)
*Huaizhi Qu,Xiao Wang,Gengwei Zhang,Jie Peng,Tianlong Chen*

Main category: cs.CV

TL;DR: 提出基于3D高斯平铺的新型冷冻电镜重建框架GEM，高效且有高分辨率，代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统冷冻电镜重建方法计算成本高、内存需求大，傅里叶空间方法保真度低，基于神经辐射场的实空间方法开销大。

Method: 引入基于3D高斯平铺的GEM框架，用紧凑3D高斯表示蛋白质，设计新的梯度计算方法。

Result: 在标准冷冻电镜基准测试中，GEM训练速度最高快48%，内存使用降低12%，局部分辨率提高38.8%。

Conclusion: GEM是一种实用且可扩展的冷冻电镜重建范式，兼顾速度、效率和高分辨率精度。

Abstract: Cryo-electron microscopy (cryo-EM) has become a central tool for
high-resolution structural biology, yet the massive scale of datasets (often
exceeding 100k particle images) renders 3D reconstruction both computationally
expensive and memory intensive. Traditional Fourier-space methods are efficient
but lose fidelity due to repeated transforms, while recent real-space
approaches based on neural radiance fields (NeRFs) improve accuracy but incur
cubic memory and computation overhead. Therefore, we introduce GEM, a novel
cryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that
operates directly in real-space while maintaining high efficiency. Instead of
modeling the entire density volume, GEM represents proteins with compact 3D
Gaussians, each parameterized by only 11 values. To further improve the
training efficiency, we designed a novel gradient computation to 3D Gaussians
that contribute to each voxel. This design substantially reduced both memory
footprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to
48% faster training and 12% lower memory usage compared to state-of-the-art
methods, while improving local resolution by as much as 38.8%. These results
establish GEM as a practical and scalable paradigm for cryo-EM reconstruction,
unifying speed, efficiency, and high-resolution accuracy. Our code is available
at https://github.com/UNITES-Lab/GEM.

</details>


### [780] [A Scalable Distributed Framework for Multimodal GigaVoxel Image Registration](https://arxiv.org/abs/2509.25044)
*Rohit Jena,Vedant Zope,Pratik Chaudhari,James C. Gee*

Main category: cs.CV

TL;DR: 提出FFDP用于大规模图像配准，优化非GEMM瓶颈，展示了高效性能和效率提升。


<details>
  <summary>Details</summary>
Motivation: 图像配准算法的扩展未能跟上图像采集能力，需解决大规模图像配准问题。

Method: 提出IO感知的非GEMM融合内核FFDP，结合分布式框架，优化非GEMM瓶颈，实现卷积感知张量分片。

Result: 能在约一分钟内用8个A6000 GPU完成100微米离体人脑MRI体积的多模态配准，加速现有优化和深度学习配准管道6 - 7倍，减少峰值内存消耗20 - 59%，在单GPU上能处理比现有SOTA大64倍的问题。

Conclusion: FFDP相比现有SOTA图像配准方法有显著的性能和效率提升。

Abstract: In this work, we propose FFDP, a set of IO-aware non-GEMM fused kernels
supplemented with a distributed framework for image registration at
unprecedented scales. Image registration is an inverse problem fundamental to
biomedical and life sciences, but algorithms have not scaled in tandem with
image acquisition capabilities. Our framework complements existing model
parallelism techniques proposed for large-scale transformer training by
optimizing non-GEMM bottlenecks and enabling convolution-aware tensor sharding.
We demonstrate unprecedented capabilities by performing multimodal registration
of a 100 micron ex-vivo human brain MRI volume at native resolution - an
inverse problem more than 570x larger than a standard clinical datum in about a
minute using only 8 A6000 GPUs. FFDP accelerates existing state-of-the-art
optimization and deep learning registration pipelines by upto 6 - 7x while
reducing peak memory consumption by 20 - 59%. Comparative analysis on a 250
micron dataset shows that FFDP can fit upto 64x larger problems than existing
SOTA on a single GPU, and highlights both the performance and efficiency gains
of FFDP compared to SOTA image registration methods.

</details>


### [781] [Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment](https://arxiv.org/abs/2509.22697)
*Abhiroop Chatterjee,Susmita Ghosh*

Main category: cs.CV

TL;DR: 随着数据需求增长，高效学习依赖高价值数据处理。本文用CLIP风格框架优化VLM用于高光谱场景理解，仅更新少量参数就取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 数据需求增长，高效学习需高价值数据，高光谱图像有高维结构挑战，跨模态对齐在高光谱领域待探索。

Method: 利用CLIP风格对比训练框架，将视觉骨干网络的体素级嵌入映射到LEM潜在空间，用可训练探针对齐视觉特征和文本标记表示，通过对比损失对齐两种模态，引入描述性提示增强对齐。

Result: 仅更新0.07%的总参数，在IP和PU数据集上比单模态和多模态基线有更好结果，参数数量远小于DCTN和SS - TMNet。

Conclusion: 提出的方法在高光谱场景理解中有效，能以少量参数更新获得优异性能。

Abstract: As data requirements continue to grow, efficient learning increasingly
depends on the curation and distillation of high-value data rather than
brute-force scaling of model sizes. In the case of a hyperspectral image (HSI),
the challenge is amplified by the high-dimensional 3D voxel structure, where
each spatial location is associated with hundreds of contiguous spectral
channels. While vision and language models have been optimized effectively for
natural image or text tasks, their cross-modal alignment in the hyperspectral
domain remains an open and underexplored problem. In this article, we make an
attempt to optimize a Vision-Language Model (VLM) for hyperspectral scene
understanding by exploiting a CLIP-style contrastive training framework. Our
framework maps voxel-level embeddings from a vision backbone onto the latent
space of a frozen large embedding model (LEM), where a trainable probe aligns
vision features with the model's textual token representations. The two
modalities are aligned via a contrastive loss restricted to a curated set of
hard (closest wrong classes) and semi-hard (random distractors) negatives,
along with positive pairs. To further enhance alignment, descriptive prompts
that encode class semantics are introduced and act as structured anchors for
the HSI embeddings. It is seen that the proposed method updates only 0.07
percent of the total parameters, yet yields state-of-the-art performance. For
example, on Indian Pines (IP) the model produces better results over unimodal
and multimodal baselines by +0.92 Overall Accuracy (OA) and +1.60 Kappa
($\kappa$), while on Pavia University (PU) data it provides gains of +0.69 OA
and +0.90 $\kappa$. Moreover, this is achieved with the set of parameters,
nearly 50$\times$ smaller than DCTN and 90$\times$ smaller than SS-TMNet.

</details>


### [782] [GZSL-MoE: Apprentissage G{é}n{é}ralis{é} Z{é}ro-Shot bas{é} sur le M{é}lange d'Experts pour la Segmentation S{é}mantique de Nuages de Points 3DAppliqu{é} {à} un Jeu de Donn{é}es d'Environnement de Collaboration Humain-Robot](https://arxiv.org/abs/2509.22708)
*Ahed Alboody*

Main category: cs.CV

TL;DR: 本文介绍了基于专家混合的广义零样本学习模型（GZSL - MoE）用于3D点云语义分割，在COVERED数据集上表现良好，能提升对可见和不可见类别的性能。


<details>
  <summary>Details</summary>
Motivation: 在3D点云语义分割任务中，当缺乏所有对象类别的全面训练数据时，利用生成式零样本学习（GZSL）方法的潜力，解决对未见类别的标注问题。

Method: 引入GZSL - MoE模型，将专家混合层（MoE）集成到生成式零样本学习模型的生成器和判别器组件中，使用预训练的KPConv模型提取可见类别的真实特征，生成与真实特征相近的虚假特征。

Result: GZSL - MoE模型的性能评估显示其能提升对可见和不可见类别的性能。

Conclusion: 将生成式零样本学习模型与专家混合相结合，为理解复杂3D环境提供了有前景的解决方案，尤其在缺乏全面训练数据时。

Abstract: Generative Zero-Shot Learning approach (GZSL) has demonstrated significant
potential in 3D point cloud semantic segmentation tasks. GZSL leverages
generative models like GANs or VAEs to synthesize realistic features (real
features) of unseen classes. This allows the model to label unseen classes
during testing, despite being trained only on seen classes. In this context, we
introduce the Generalized Zero-Shot Learning based-upon Mixture-of-Experts
(GZSL-MoE) model. This model incorporates Mixture-of-Experts layers (MoE) to
generate fake features that closely resemble real features extracted using a
pre-trained KPConv (Kernel Point Convolution) model on seen classes. The main
contribution of this paper is the integration of Mixture-of-Experts into the
Generator and Discriminator components of the Generative Zero-Shot Learning
model for 3D point cloud semantic segmentation, applied to the COVERED dataset
(CollabOratiVE Robot Environment Dataset) for Human-Robot Collaboration (HRC)
environments. By combining the Generative Zero-Shot Learning model with
Mixture-of- Experts, GZSL-MoE for 3D point cloud semantic segmentation provides
a promising solution for understanding complex 3D environments, especially when
comprehensive training data for all object classes is unavailable. The
performance evaluation of the GZSL-MoE model highlights its ability to enhance
performance on both seen and unseen classes. Keywords Generalized Zero-Shot
Learning (GZSL), 3D Point Cloud, 3D Semantic Segmentation, Human-Robot
Collaboration, COVERED (CollabOratiVE Robot Environment Dataset), KPConv,
Mixture-of Experts

</details>


### [783] [IBiT: Utilizing Inductive Biases to Create a More Data Efficient Attention Mechanism](https://arxiv.org/abs/2509.22719)
*Adithya Giri*

Main category: cs.CV

TL;DR: 提出IBiT，可让视觉Transformer在小数据集上学习且无需知识蒸馏，小数据集上更准确还保留可解释性。


<details>
  <summary>Details</summary>
Motivation: Transformer缺乏卷积神经网络的归纳偏置，虽可在大数据集学习，但希望其能在小数据集学习。

Method: 通过学习掩码引入归纳偏置。

Result: 提出的IBiT在小数据集上准确性显著提高。

Conclusion: 引入归纳偏置的IBiT能让Transformer在小数据集上学习且保留可解释性。

Abstract: In recent years, Transformer-based architectures have become the dominant
method for Computer Vision applications. While Transformers are explainable and
scale well with dataset size, they lack the inductive biases of Convolutional
Neural Networks. While these biases may be learned on large datasets, we show
that introducing these inductive biases through learned masks allow Vision
Transformers to learn on much smaller datasets without Knowledge Distillation.
These Transformers, which we call Inductively Biased Image Transformers (IBiT),
are significantly more accurate on small datasets, while retaining the
explainability Transformers.

</details>


### [784] [LayoutAgent: A Vision-Language Agent Guided Compositional Diffusion for Spatial Layout Planning](https://arxiv.org/abs/2509.22720)
*Zezhong Fan,Xiaohan Li,Luyi Ma,Kai Zhao,Liang Peng,Topojoy Biswas,Evren Korpeoglu,Kaushiki Nag,Kannan Achan*

Main category: cs.CV

TL;DR: 提出LayoutAgent框架统一视觉语言推理和组合扩散进行布局生成，实验表明其在布局连贯性、空间真实性和美学一致性上优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型缺乏显式空间推理，传统机器人空间规划方法难以捕捉视觉场景语义丰富性，需弥合两者差距。

Method: 先使用视觉语言模型对输入图像预处理，再利用组合扩散合成边界框，最后用前景条件图像生成器合成完整场景。

Result: LayoutAgent在布局连贯性、空间真实性和美学一致性上优于其他先进布局生成模型。

Conclusion: LayoutAgent框架能有效结合视觉语言推理与组合扩散进行布局生成。

Abstract: Designing realistic multi-object scenes requires not only generating images,
but also planning spatial layouts that respect semantic relations and physical
plausibility. On one hand, while recent advances in diffusion models have
enabled high-quality image generation, they lack explicit spatial reasoning,
leading to unrealistic object layouts. On the other hand, traditional spatial
planning methods in robotics emphasize geometric and relational consistency,
but they struggle to capture semantic richness in visual scenes. To bridge this
gap, in this paper, we propose LayoutAgent, an agentic framework that unifies
vision-language reasoning with compositional diffusion for layout generation.
Given multiple input images with target objects in them, our method first
employs visual-language model to preprocess the inputs through segmentation,
object size estimation, scene graph construction, and prompt rewriting. Then we
leverage compositional diffusion-a method traditionally used in robotics-to
synthesize bounding boxes that respect object relations encoded in the scene
graph for spatial layouts. In the end, a foreground-conditioned image generator
composes the complete scene by rendering the objects into the planned layout
guided by designed prompts. Experiments demonstrate that LayoutAgent
outperforms other state-of-the-art layout generation models in layout
coherence, spatial realism and aesthetic alignment.

</details>


### [785] [CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models](https://arxiv.org/abs/2509.22737)
*Jie Cai,Kangning Yang,Lan Fu,Jiaming Ding,Jinlong Li,Huiming Sun,Daitao Xing,Jinglin Shen,Zibo Meng*

Main category: cs.CV

TL;DR: 介绍用于评估视觉语言模型视觉比较推理能力的CompareBench基准，评估多种模型，发现当前模型在视觉比较上存在局限，该基准为提升多模态推理奠定基础。


<details>
  <summary>Details</summary>
Motivation: 视觉比较推理是视觉语言模型中一项基础但研究不足的技能，需要合适的评估基准。

Method: 构建CompareBench基准，包含1000个跨四个任务的问答对，源自TallyBench和HistCaps两个辅助数据集，评估闭源API和开源模型。

Result: 结果显示有明显的扩展趋势，但也暴露出关键局限，最强模型在时间排序和空间关系上常失败，在基本计数和几何比较上也常出错。

Conclusion: 视觉比较仍是当前视觉语言模型的系统性盲点，CompareBench为推进更可靠的多模态推理奠定了基础。

Abstract: We introduce CompareBench, a benchmark for evaluating visual comparison
reasoning in vision-language models (VLMs), a fundamental yet understudied
skill. CompareBench consists of 1000 QA pairs across four tasks: quantity
(600), temporal (100), geometric (200), and spatial (100). It is derived from
two auxiliary datasets that we constructed: TallyBench (2000 counting images
with QA) and HistCaps (515 historical images with bilingual captions). We
evaluate both closed-source APIs (OpenAI, Gemini, Claude) and open-source
models (Qwen2.5-VL and Qwen3-VL series). Results show clear scaling trends but
also reveal critical limitations: even the strongest models consistently fail
at temporal ordering and spatial relations, and they often make mistakes in
basic counting and geometric comparisons that are trivial for humans. These
findings demonstrate that visual comparison remains a systematic blind spot for
current VLMs. By providing controlled, diverse, and diagnostic evaluation,
CompareBench establishes a foundation for advancing more reliable multimodal
reasoning.

</details>


### [786] [MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning](https://arxiv.org/abs/2509.22761)
*Yapeng Mi,Hengli Li,Yanpeng Zhao,Chenxi Li,Huimin Wu,Xiaojian Ma,Song-Chun Zhu,Ying Nian Wu,Qing Li*

Main category: cs.CV

TL;DR: 提出MILR方法在统一潜在向量空间对图像和文本联合推理，在多基准测试取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于推理的图像生成方法存在局限，如单模态推理或依赖高质量推理数据微调。

Method: 在统一潜在向量空间搜索离散图像和文本令牌的向量表示进行推理，通过策略梯度方法实现，由图像质量评判器引导，在MUG框架中实例化。

Result: 在GenEval、T2I - CompBench和WISE基准测试取得SOTA结果，在WISE上整体得分0.63，较基线提升80%。

Conclusion: 统一潜在空间的联合推理是MILR性能强的关键，该方法在时间和文化推理上有效。

Abstract: Reasoning-augmented machine learning systems have shown improved performance
in various domains, including image generation. However, existing
reasoning-based methods for image generation either restrict reasoning to a
single modality (image or text) or rely on high-quality reasoning data for
fine-tuning. To tackle these limitations, we propose MILR, a test-time method
that jointly reasons over image and text in a unified latent vector space.
Reasoning in MILR is performed by searching through vector representations of
discrete image and text tokens. Practically, this is implemented via the policy
gradient method, guided by an image quality critic. We instantiate MILR within
the unified multimodal understanding and generation (MUG) framework that
natively supports language reasoning before image synthesis and thus
facilitates cross-modal reasoning. The intermediate model outputs, which are to
be optimized, serve as the unified latent space, enabling MILR to operate
entirely at test time. We evaluate MILR on GenEval, T2I-CompBench, and WISE,
achieving state-of-the-art results on all benchmarks. Notably, on
knowledge-intensive WISE, MILR attains an overall score of 0.63, improving over
the baseline by 80%. Our further analysis indicates that joint reasoning in the
unified latent space is the key to its strong performance. Moreover, our
qualitative studies reveal MILR's non-trivial ability in temporal and cultural
reasoning, highlighting the efficacy of our reasoning method.

</details>


### [787] [UESA-Net: U-Shaped Embedded Multidirectional Shrinkage Attention Network for Ultrasound Nodule Segmentation](https://arxiv.org/abs/2509.22763)
*Tangqi Shi,Pietro Lio*

Main category: cs.CV

TL;DR: 本文提出UESA - Net用于乳腺和甲状腺超声图像分割，在两个公开数据集上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺和甲状腺癌带来公共卫生负担，超声成像有噪声、结构重叠等问题，现有网络难以调和高低层特征，需开发能弥合全局与局部语义差距的分割框架。

Method: 提出具有多向收缩注意力的U形网络UESA - Net，编码器 - 解码器架构捕捉病变特征，编码块内注意力模块沿多方向操作，采用收缩策略整合先验知识和局部特征，解码器采用成对收缩机制增强上下文建模。

Result: 在TN3K和BUSI两个公开数据集上，UESA - Net的IoU分数分别达到0.8487和0.6495，取得了SOTA性能。

Conclusion: UESA - Net有效聚合多向空间信息和先验知识，提高了乳腺和甲状腺超声分割的鲁棒性和准确性，在多个基准测试中优于现有方法。

Abstract: Background: Breast and thyroid cancers pose an increasing public-health
burden. Ultrasound imaging is a cost-effective, real-time modality for lesion
detection and segmentation, yet suffers from speckle noise, overlapping
structures, and weak global-local feature interactions. Existing networks
struggle to reconcile high-level semantics with low-level spatial details. We
aim to develop a segmentation framework that bridges the semantic gap between
global context and local detail in noisy ultrasound images.
  Methods: We propose UESA-Net, a U-shaped network with multidirectional
shrinkage attention. The encoder-decoder architecture captures long-range
dependencies and fine-grained structures of lesions. Within each encoding
block, attention modules operate along horizontal, vertical, and depth
directions to exploit spatial details, while a shrinkage (threshold) strategy
integrates prior knowledge and local features. The decoder mirrors the encoder
but applies a pairwise shrinkage mechanism, combining prior low-level physical
cues with corresponding encoder features to enhance context modeling.
  Results: On two public datasets - TN3K (3493 images) and BUSI (780 images) -
UESA-Net achieved state-of-the-art performance with intersection-over-union
(IoU) scores of 0.8487 and 0.6495, respectively.
  Conclusions: UESA-Net effectively aggregates multidirectional spatial
information and prior knowledge to improve robustness and accuracy in breast
and thyroid ultrasound segmentation, demonstrating superior performance to
existing methods on multiple benchmarks.

</details>


### [788] [VideoScore2: Think before You Score in Generative Video Evaluation](https://arxiv.org/abs/2509.22799)
*Xuan He,Dongfu Jiang,Ping Nie,Minghao Liu,Zhengxuan Jiang,Mingyi Su,Wentao Ma,Junru Lin,Chun Ye,Yi Lu,Keming Wu,Benjamin Schneider,Quy Duc Do,Zhuofeng Li,Yiming Jia,Yuxuan Zhang,Guo Cheng,Haozhe Wang,Wangchunshu Zhou,Qunshu Lin,Yuanxing Zhang,Ge Zhang,Wenhao Huang,Wenhu Chen*

Main category: cs.CV

TL;DR: 现有文本到视频生成评估方法不足，提出VideoScore2框架，经训练后在多基准测试中表现出色，还能提供可解释评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估器和奖励模型存在单一不透明分数、缺乏可解释性、分析粗略等问题，无法全面评估视频质量。

Method: 在含27168个人工标注视频的VideoFeedback2数据集上训练，采用监督微调后接基于Group Relative Policy Optimization (GRPO)的强化学习两阶段管道。

Result: VideoScore2在域内基准VideoScore - Bench - v2上准确率达44.35 (+5.94)，在四个域外基准上平均表现为50.37 (+4.32)。

Conclusion: VideoScore2性能优越，能提供可解释评估，通过有效奖励建模缩小评估与可控生成的差距。

Abstract: Recent advances in text-to-video generation have produced increasingly
realistic and diverse content, yet evaluating such videos remains a fundamental
challenge due to their multi-faceted nature encompassing visual quality,
semantic alignment, and physical consistency. Existing evaluators and reward
models are limited to single opaque scores, lack interpretability, or provide
only coarse analysis, making them insufficient for capturing the comprehensive
nature of video quality assessment. We present VideoScore2, a
multi-dimensional, interpretable, and human-aligned framework that explicitly
evaluates visual quality, text-to-video alignment, and physical/common-sense
consistency while producing detailed chain-of-thought rationales. Our model is
trained on a large-scale dataset VideoFeedback2 containing 27,168
human-annotated videos with both scores and reasoning traces across three
dimensions, using a two-stage pipeline of supervised fine-tuning followed by
reinforcement learning with Group Relative Policy Optimization (GRPO) to
enhance analytical robustness. Extensive experiments demonstrate that
VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our
in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance
across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc),
while providing interpretable assessments that bridge the gap between
evaluation and controllable generation through effective reward modeling for
Best-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/

</details>


### [789] [MMPB: It's Time for Multi-Modal Personalization](https://arxiv.org/abs/2509.22820)
*Jaeik Kim,Woojin Kim,Woohyeon Park,Jaeyoung Do*

Main category: cs.CV

TL;DR: 提出首个评估视觉语言模型（VLMs）个性化能力的基准MMPB，评估23个常用VLMs，发现多数模型在个性化方面存在问题，为多模态AI个性化研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在适应个体用户方面缺乏探索，而视觉个性化在面向用户的AI系统中至关重要，因此需要评估其个性化能力。

Method: 构建包含10k图像 - 查询对、111个可个性化概念的MMPB基准，将个性化分为三种任务类型，通过概念注入、多轮对话和个性化查询三阶段协议评估23个常用VLMs。

Result: 多数VLMs（包括部分闭源模型）在个性化方面表现不佳，尤其在对话一致性、处理用户偏好和适应视觉线索方面存在困难。

Conclusion: MMPB识别出VLM个性化的挑战，为未来真正个性化的多模态AI研究提供有价值的见解和基础。

Abstract: Visual personalization is essential in user-facing AI systems such as smart
homes and healthcare, where aligning model behavior with user-centric concepts
is critical. However, recent large Vision-Language Models (VLMs), despite their
broad applicability, remain underexplored in their ability to adapt to
individual users. In this paper, we introduce MMPB, the first extensive
benchmark for evaluating VLMs on personalization. MMPB comprises 10k
image-query pairs and includes 111 personalizable concepts across four
categories: humans, animals, objects, and characters, with the human category
enriched with preference-grounded queries. We structure personalization into
three main task types, each highlighting a different key property of VLMs.
Using 23 widely used VLMs including both open- and closed-source models, we
evaluate personalization performance via a three-stage protocol: concept
injection, multi-turn dialogue, and personalized querying. Our findings
indicate that most VLMs (including some closed-source models) struggle with
personalization, particularly in maintaining consistency over dialogue,
handling user preferences, and adapting to visual cues. Our analysis reveals
that the challenges in VLM personalization (such as refusal behaviors and
long-context forgetting) highlight substantial room for improvement. By
identifying these limitations and offering a scalable benchmark, MMPB offers
valuable insights and a solid foundation for future research toward truly
personalized multi-modal AI. Project Page: aidaslab.github.io/MMPB

</details>


### [790] [Seeing Isn't Believing: Context-Aware Adversarial Patch Synthesis via Conditional GAN](https://arxiv.org/abs/2509.22836)
*Roie Kazoom,Alon Goldberg,Hodaya Cohen,Ofer Hadar*

Main category: cs.CV

TL;DR: 提出可控对抗补丁生成框架，结合U - Net和Grad - CAM，实验表明在多种模型上表现优异，建立新基准。


<details>
  <summary>Details</summary>
Motivation: 现有对抗补丁攻击方法存在依赖白盒假设、无目标攻击、补丁明显等问题，限制实际应用。

Method: 结合生成式U - Net设计与Grad - CAM引导的补丁放置，实现语义感知定位。

Result: 在卷积网络和视觉变换器上实验，攻击成功率和目标类成功率超99%，优于先前方法。

Conclusion: 该框架兼顾真实性、目标控制和黑盒适用性，为对抗鲁棒性研究建立新基准。

Abstract: Adversarial patch attacks pose a severe threat to deep neural networks, yet
most existing approaches rely on unrealistic white-box assumptions, untargeted
objectives, or produce visually conspicuous patches that limit real-world
applicability. In this work, we introduce a novel framework for fully
controllable adversarial patch generation, where the attacker can freely choose
both the input image x and the target class y target, thereby dictating the
exact misclassification outcome. Our method combines a generative U-Net design
with Grad-CAM-guided patch placement, enabling semantic-aware localization that
maximizes attack effectiveness while preserving visual realism. Extensive
experiments across convolutional networks (DenseNet-121, ResNet-50) and vision
transformers (ViT-B/16, Swin-B/16, among others) demonstrate that our approach
achieves state-of-the-art performance across all settings, with attack success
rates (ASR) and target-class success (TCS) consistently exceeding 99%.
  Importantly, we show that our method not only outperforms prior white-box
attacks and untargeted baselines, but also surpasses existing non-realistic
approaches that produce detectable artifacts. By simultaneously ensuring
realism, targeted control, and black-box applicability-the three most
challenging dimensions of patch-based attacks-our framework establishes a new
benchmark for adversarial robustness research, bridging the gap between
theoretical attack strength and practical stealthiness.

</details>


### [791] [Multimodal Slice Interaction Network Enhanced by Transfer Learning for Precise Segmentation of Internal Gross Tumor Volume in Lung Cancer PET/CT Imaging](https://arxiv.org/abs/2509.22841)
*Yi Luo,Yike Guo,Hamed Hooshangnejad,Rui Zhang,Xue Feng,Quan Chen,Wil Ngwa,Kai Ding*

Main category: cs.CV

TL;DR: 本文提出基于迁移学习的方法，利用多模态交互感知网络和切片交互模块提升肺癌IGTV分割性能，实验效果优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 肺癌是全球癌症死亡主因，PET/CT影像中IGTV准确分割对肺癌放疗至关重要，但受标注数据集有限和PET信号强度弱的限制。

Method: 采用基于迁移学习的方法，利用预训练在GTV数据集上的多模态交互感知网络，在私有IGTV队列微调，引入切片交互模块建模层间关系。

Result: 在私有IGTV数据集上Dice系数达0.609，远超传统基线的0.385。

Conclusion: 迁移学习结合先进多模态技术和切片交互模块能增强IGTV分割的可靠性和临床相关性。

Abstract: Lung cancer remains the leading cause of cancerrelated deaths globally.
Accurate delineation of internal gross tumor volume (IGTV) in PET/CT imaging is
pivotal for optimal radiation therapy in mobile tumors such as lung cancer to
account for tumor motion, yet is hindered by the limited availability of
annotated IGTV datasets and attenuated PET signal intensity at tumor
boundaries. In this study, we present a transfer learningbased methodology
utilizing a multimodal interactive perception network with MAMBA, pre-trained
on extensive gross tumor volume (GTV) datasets and subsequently fine-tuned on a
private IGTV cohort. This cohort constitutes the PET/CT subset of the
Lung-cancer Unified Cross-modal Imaging Dataset (LUCID). To further address the
challenge of weak PET intensities in IGTV peripheral slices, we introduce a
slice interaction module (SIM) within a 2.5D segmentation framework to
effectively model inter-slice relationships. Our proposed module integrates
channel and spatial attention branches with depthwise convolutions, enabling
more robust learning of slice-to-slice dependencies and thereby improving
overall segmentation performance. A comprehensive experimental evaluation
demonstrates that our approach achieves a Dice of 0.609 on the private IGTV
dataset, substantially surpassing the conventional baseline score of 0.385.
This work highlights the potential of transfer learning, coupled with advanced
multimodal techniques and a SIM to enhance the reliability and clinical
relevance of IGTV segmentation for lung cancer radiation therapy planning.

</details>


### [792] [Convolutional Set Transformer](https://arxiv.org/abs/2509.22889)
*Federico Chinello,Giacomo Boracchi*

Main category: cs.CV

TL;DR: 介绍卷积集变换器（CST），可处理任意数量的视觉异质但语义相关的图像集，性能优且与CNN可解释性方法兼容，支持迁移学习并发布预训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有集输入网络限于向量输入，无法直接处理3D图像张量，需与特征提取器级联。

Method: 提出CST，直接对3D图像张量操作，同时进行特征提取和上下文建模。

Result: 在集分类和集异常检测等任务中表现出色，与CNN可解释性方法兼容，可通过迁移学习适配新领域和任务。

Conclusion: CST是处理图像集的有效架构，有良好性能和迁移能力，发布的预训练模型支持进一步研究。

Abstract: We introduce the Convolutional Set Transformer (CST), a novel neural
architecture designed to process image sets of arbitrary cardinality that are
visually heterogeneous yet share high-level semantics - such as a common
category, scene, or concept. Existing set-input networks, e.g., Deep Sets and
Set Transformer, are limited to vector inputs and cannot directly handle 3D
image tensors. As a result, they must be cascaded with a feature extractor,
typically a CNN, which encodes images into embeddings before the set-input
network can model inter-image relationships. In contrast, CST operates directly
on 3D image tensors, performing feature extraction and contextual modeling
simultaneously, thereby enabling synergies between the two processes. This
design yields superior performance in tasks such as Set Classification and Set
Anomaly Detection and further provides native compatibility with CNN
explainability methods such as Grad-CAM, unlike competing approaches that
remain opaque. Finally, we show that CSTs can be pre-trained on large-scale
datasets and subsequently adapted to new domains and tasks through standard
Transfer Learning schemes. To support further research, we release CST-15, a
CST backbone pre-trained on ImageNet
(https://github.com/chinefed/convolutional-set-transformer).

</details>


### [793] [TY-RIST: Tactical YOLO Tricks for Real-time Infrared Small Target Detection](https://arxiv.org/abs/2509.22909)
*Abdulkarim Atrash,Omar Moured,Yufan Chen,Jiaming Zhang,Seyda Ertekin,Omur Ugur*

Main category: cs.CV

TL;DR: 提出优化的YOLOv12n架构TY - RIST用于红外小目标检测，结合多种技术降低计算成本、提升性能，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测存在目标易丢失、杂波环境误警、低显著性漏检和高计算成本等问题，需优化检测方法。

Method: 提出TY - RIST架构，集成步幅感知主干、高分辨率检测头、级联坐标注意力块和分支剪枝策略，引入归一化高斯瓦瑟斯坦距离。

Result: 在四个基准测试和20个不同模型实验中表现达SOTA，mAP@0.5 IoU提升7.9%，Precision提升3%，Recall提升10.2%，单GPU达123 FPS，跨数据集验证显示强泛化能力。

Conclusion: TY - RIST架构有效解决红外小目标检测问题，降低计算成本，提升检测性能和泛化能力。

Abstract: Infrared small target detection (IRSTD) is critical for defense and
surveillance but remains challenging due to (1) target loss from minimal
features, (2) false alarms in cluttered environments, (3) missed detections
from low saliency, and (4) high computational costs. To address these issues,
we propose TY-RIST, an optimized YOLOv12n architecture that integrates (1) a
stride-aware backbone with fine-grained receptive fields, (2) a high-resolution
detection head, (3) cascaded coordinate attention blocks, and (4) a branch
pruning strategy that reduces computational cost by about 25.5% while
marginally improving accuracy and enabling real-time inference. We also
incorporate the Normalized Gaussian Wasserstein Distance (NWD) to enhance
regression stability. Extensive experiments on four benchmarks and across 20
different models demonstrate state-of-the-art performance, improving mAP at 0.5
IoU by +7.9%, Precision by +3%, and Recall by +10.2%, while achieving up to 123
FPS on a single GPU. Cross-dataset validation on a fifth dataset further
confirms strong generalization capability. Additional results and resources are
available at https://www.github.com/moured/TY-RIST

</details>


### [794] [Soft-Di[M]O: Improving One-Step Discrete Image Generation with Soft Embeddings](https://arxiv.org/abs/2509.22925)
*Yuanzhi Zhu,Xi Wang,Stéphane Lathuilière,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 本文提出软嵌入方法改进一步生成器，使其端到端可训练，在多方面取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有一步生成器存在继承教师模型偏差和离散令牌输出阻碍梯度流动的问题，无法进行蒸馏后优化。

Method: 引入软嵌入，用生成器输出分布下的期望嵌入替代离散令牌，将其集成到Di[M]O蒸馏框架。

Result: 在多个MDM教师模型上，Soft - Di[M]O取得了最先进的一步生成结果，如图像生成性能提升、FID值降低、文本到图像生成的相关评分提高。

Conclusion: 软嵌入方法使一步生成器端到端可训练，能有效应用多种优化方法，提升生成效果。

Abstract: One-step generators distilled from Masked Diffusion Models (MDMs) compress
multiple sampling steps into a single forward pass, enabling efficient text and
image synthesis. However, they suffer two key limitations: they inherit
modeling bias from the teacher, and their discrete token outputs block gradient
flow, preventing post-distillation refinements such as adversarial training,
reward-based fine-tuning, and Test-Time Embedding Optimization (TTEO). In this
work, we introduce soft embeddings, a simple relaxation that replaces discrete
tokens with the expected embeddings under the generator's output distribution.
Soft embeddings preserve representation fidelity for one-step discrete
generator while providing a fully differentiable continuous surrogate that is
compatible with teacher backbones and tokenizer decoders. Integrating soft
embeddings into the Di[M]O distillation framework (denoted Soft-Di[M]O) makes
one-step generators end-to-end trainable and enables straightforward
application of GAN-based refinement, differentiable reward fine-tuning, and
TTEO. Empirically, across multiple MDM teachers (e.g., MaskBit, MaskGen),
Soft-Di[M]O achieves state-of-the-art one-step results: improved class-to-image
performance, a one-step FID of 1.56 on ImageNet-256 with GAN-based refinement,
along with higher GenEval and HPS scores on text-to-image with reward
fine-tuning, and further gains from TTEO.

</details>


### [795] [Sensor-Adaptive Flood Mapping with Pre-trained Multi-Modal Transformers across SAR and Multispectral Modalities](https://arxiv.org/abs/2509.23035)
*Tomohiro Tanaka,Narumasa Tsutsumida*

Main category: cs.CV

TL;DR: 本文提出一种传感器灵活的洪水检测方法，通过微调轻量级多模态预训练变压器Presto实现洪水映射，在不同数据可用性场景下表现良好，为现实灾害场景提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有遥感技术用于洪水监测存在单传感器数据可用性受天气影响、重访周期有限，多传感器融合方法需大量计算资源和大规模标注数据集等问题，需一种新方法解决这些局限。

Method: 微调轻量级多模态预训练变压器Presto，使其能在像素级别处理合成孔径雷达（SAR）和多光谱（MS）数据，通过单一模型架构实现仅SAR、仅MS或SAR+MS输入的洪水映射。

Result: 在Sen1Floods11数据集上评估，在最优传感器融合场景下F1分数达0.896，mIoU达0.886，优于基线；在仅MS场景F1为0.893，仅SAR场景F1为0.718。

Conclusion: 参数高效、传感器灵活的方法为现实灾害场景提供了可访问且稳健的解决方案，证实了多模态预训练用于操作洪水映射的优势。

Abstract: Floods are increasingly frequent natural disasters causing extensive human
and economic damage, highlighting the critical need for rapid and accurate
flood inundation mapping. While remote sensing technologies have advanced flood
monitoring capabilities, operational challenges persist: single-sensor
approaches face weather-dependent data availability and limited revisit
periods, while multi-sensor fusion methods require substantial computational
resources and large-scale labeled datasets. To address these limitations, this
study introduces a novel sensor-flexible flood detection methodology by
fine-tuning Presto, a lightweight ($\sim$0.4M parameters) multi-modal
pre-trained transformer that processes both Synthetic Aperture Radar (SAR) and
multispectral (MS) data at the pixel level. Our approach uniquely enables flood
mapping using SAR-only, MS-only, or combined SAR+MS inputs through a single
model architecture, addressing the critical operational need for rapid response
with whatever sensor data becomes available first during disasters. We
evaluated our method on the Sen1Floods11 dataset against the large-scale
Prithvi-100M baseline ($\sim$100M parameters) across three realistic data
availability scenarios. The proposed model achieved superior performance with
an F1 score of 0.896 and mIoU of 0.886 in the optimal sensor-fusion scenario,
outperforming the established baseline. Crucially, the model demonstrated
robustness by maintaining effective performance in MS-only scenarios (F1:
0.893) and functional capabilities in challenging SAR-only conditions (F1:
0.718), confirming the advantage of multi-modal pre-training for operational
flood mapping. Our parameter-efficient, sensor-flexible approach offers an
accessible and robust solution for real-world disaster scenarios requiring
immediate flood extent assessment regardless of sensor availability
constraints.

</details>


### [796] [GeLoc3r: Enhancing Relative Camera Pose Regression with Geometric Consistency Regularization](https://arxiv.org/abs/2509.23038)
*Jingxing Li,Yongjae Lee,Deliang Fan*

Main category: cs.CV

TL;DR: 提出GeLoc3r方法解决相机位姿估计速度与精度难题，训练时利用几何一致性正则化，测试时仅用增强回归头，在多个数据集上表现优于ReLoc3R。


<details>
  <summary>Details</summary>
Motivation: ReLoc3R内部表示存在几何不一致，无法达到基于对应方法的精度上限，需解决速度 - 精度困境。

Method: 提出GeLoc3r，训练时利用真值深度生成3D - 2D对应，用FusionTransformer加权，通过加权RANSAC计算一致位姿，创建一致性损失传递几何知识到回归网络；测试时仅用增强回归头。

Result: 在多个具有挑战性的基准测试中，GeLoc3r始终优于ReLoc3R，如在CO3Dv2数据集上AUC@5°从34.85%提升到40.45%等。

Conclusion: GeLoc3r通过训练时教授几何一致性而非推理时强制实施，实现了回归速度和对应方法的几何理解，代表了神经网络学习相机几何的范式转变。

Abstract: Prior ReLoc3R achieves breakthrough performance with fast 25ms inference and
state-of-the-art regression accuracy, yet our analysis reveals subtle geometric
inconsistencies in its internal representations that prevent reaching the
precision ceiling of correspondence-based methods like MASt3R (which require
300ms per pair). In this work, we present GeLoc3r, a novel approach to relative
camera pose estimation that enhances pose regression methods through Geometric
Consistency Regularization (GCR). GeLoc3r overcomes the speed-accuracy dilemma
by training regression networks to produce geometrically consistent poses
without inference-time geometric computation. During training, GeLoc3r
leverages ground-truth depth to generate dense 3D-2D correspondences, weights
them using a FusionTransformer that learns correspondence importance, and
computes geometrically-consistent poses via weighted RANSAC. This creates a
consistency loss that transfers geometric knowledge into the regression
network. Unlike FAR method which requires both regression and geometric solving
at inference, GeLoc3r only uses the enhanced regression head at test time,
maintaining ReLoc3R's fast speed and approaching MASt3R's high accuracy. On
challenging benchmarks, GeLoc3r consistently outperforms ReLoc3R, achieving
significant improvements including 40.45% vs. 34.85% AUC@5{\deg} on the CO3Dv2
dataset (16% relative improvement), 68.66% vs. 66.70% AUC@5{\deg} on
RealEstate10K, and 50.45% vs. 49.60% on MegaDepth1500. By teaching geometric
consistency during training rather than enforcing it at inference, GeLoc3r
represents a paradigm shift in how neural networks learn camera geometry,
achieving both the speed of regression and the geometric understanding of
correspondence methods.

</details>


### [797] [MMeViT: Multi-Modal ensemble ViT for Post-Stroke Rehabilitation Action Recognition](https://arxiv.org/abs/2509.23044)
*Ye-eun Kim,Suhyeon Lim,Andrew J. Choi*

Main category: cs.CV

TL;DR: 为解决中风患者康复治疗供需矛盾，研究设计监测系统，用IMU传感器和RGB - D相机收集数据，提出适合多模态数据的深度学习模型，分析发现中风患者动作数据聚类性差，模型有扩展应用可能。


<details>
  <summary>Details</summary>
Motivation: 中风患者康复治疗供应短缺，现有HAR研究多针对非残疾个体，不适用于中风患者，且中风HAR研究多采用机器学习处理简单动作。

Method: 设计监测系统，用IMU传感器和RGB - D相机收集中风患者居家上肢日常活动数据，进行预处理并提出适合多模态数据的深度学习模型。

Result: 中风患者动作数据聚类性比非残疾个体差，模型在难聚类数据中对各标签学习有相似趋势。

Conclusion: 学习了中风患者动作特征的深度学习模型有扩展到简单动作识别之外、为居家康复提供评估反馈等应用的可能性。

Abstract: Rehabilitation therapy for stroke patients faces a supply shortage despite
the increasing demand. To address this issue, remote monitoring systems that
reduce the burden on medical staff are emerging as a viable alternative. A key
component of these remote monitoring systems is Human Action Recognition (HAR)
technology, which classifies actions. However, existing HAR studies have
primarily focused on non-disable individuals, making them unsuitable for
recognizing the actions of stroke patients. HAR research for stroke has largely
concentrated on classifying relatively simple actions using machine learning
rather than deep learning. In this study, we designed a system to monitor the
actions of stroke patients, focusing on domiciliary upper limb Activities of
Daily Living (ADL). Our system utilizes IMU (Inertial Measurement Unit) sensors
and an RGB-D camera, which are the most common modalities in HAR. We directly
collected a dataset through this system, investigated an appropriate preprocess
and proposed a deep learning model suitable for processing multimodal data. We
analyzed the collected dataset and found that the action data of stroke
patients is less clustering than that of non-disabled individuals.
Simultaneously, we found that the proposed model learns similar tendencies for
each label in data with features that are difficult to clustering. This study
suggests the possibility of expanding the deep learning model, which has
learned the action features of stroke patients, to not only simple action
recognition but also feedback such as assessment contributing to domiciliary
rehabilitation in future research. The code presented in this study is
available at https://github.com/ye-Kim/MMeViT.

</details>


### [798] [CoPatch: Zero-Shot Referring Image Segmentation by Leveraging Untapped Spatial Knowledge in CLIP](https://arxiv.org/abs/2509.23098)
*Na Min An,Inha Kang,Minhyun Lee,Hyunjung Shim*

Main category: cs.CV

TL;DR: 提出CoPatch零样本指称图像分割框架，利用模型内部组件增强文本和图像模态的空间表示，在多个数据集上显著提升空间定位性能，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 当前基础视觉 - 语言模型在指称图像分割任务中难以理解空间关系，现有方法在提取特征时存在忽视上下文和对空间结构敏感度低的问题。

Method: 构建结合携带空间线索上下文标记的混合文本特征；通过中间层新路径提取图像块级特征；将增强特征融合到聚类的图像 - 文本相似度图中进行精确掩码选择。

Result: CoPatch在RefCOCO、RefCOCO+、RefCOCOg和PhraseCut上显著提升零样本指称图像分割的空间定位性能，mIoU提升2 - 7。

Conclusion: 恢复和利用视觉 - 语言模型中固有的未开发空间知识很重要，为零样本指称图像分割带来新机遇。

Abstract: Spatial grounding is crucial for referring image segmentation (RIS), where
the goal of the task is to localize an object described by language. Current
foundational vision-language models (VLMs), such as CLIP, excel at aligning
images and text but struggle with understanding spatial relationships. Within
the language stream, most existing methods often focus on the primary noun
phrase when extracting local text features, undermining contextual tokens.
Within the vision stream, CLIP generates similar features for images with
different spatial layouts, resulting in limited sensitivity to spatial
structure. To address these limitations, we propose \textsc{CoPatch}, a
zero-shot RIS framework that leverages internal model components to enhance
spatial representations in both text and image modalities. For language,
\textsc{CoPatch} constructs hybrid text features by incorporating context
tokens carrying spatial cues. For vision, it extracts patch-level image
features using our novel path discovered from intermediate layers, where
spatial structure is better preserved. These enhanced features are fused into a
clustered image-text similarity map, \texttt{CoMap}, enabling precise mask
selection. As a result, \textsc{CoPatch} significantly improves spatial
grounding in zero-shot RIS across RefCOCO, RefCOCO+, RefCOCOg, and PhraseCut (+
2--7 mIoU) without requiring any additional training. Our findings underscore
the importance of recovering and leveraging the untapped spatial knowledge
inherently embedded in VLMs, thereby paving the way for opportunities in
zero-shot RIS.

</details>


### [799] [HTMA-Net: Towards Multiplication-Avoiding Neural Networks via Hadamard Transform and In-Memory Computing](https://arxiv.org/abs/2509.23103)
*Emadeldeen Hamdan,Ahmet Enis Cetin*

Main category: cs.CV

TL;DR: 提出HTMA - Net框架结合Hadamard变换与免乘法SRAM内存计算，减少乘法成本，评估显示可消除大量乘法且保持精度。


<details>
  <summary>Details</summary>
Motivation: 降低乘法成本对高效部署深度神经网络，尤其是在能量受限边缘设备中至关重要。

Method: 引入HTMA - Net框架，用混合Hadamard变换层选择性替换中间卷积，内部卷积通过免乘法内存操作实现。

Result: 在ResNet - 18上评估，相比基线模型可消除高达52%的乘法，精度相当，降低计算复杂度和参数数量。

Conclusion: 将结构化Hadamard变换层与基于SRAM的免乘法内存计算算子结合是实现高效深度学习架构的有前景途径。

Abstract: Reducing the cost of multiplications is critical for efficient deep neural
network deployment, especially in energy-constrained edge devices. In this
work, we introduce HTMA-Net, a novel framework that integrates the Hadamard
Transform (HT) with multiplication-avoiding (MA) SRAM-based in-memory computing
to reduce arithmetic complexity while maintaining accuracy. Unlike prior
methods that only target multiplications in convolutional layers or focus
solely on in-memory acceleration, HTMA-Net selectively replaces intermediate
convolutions with Hybrid Hadamard-based transform layers whose internal
convolutions are implemented via multiplication-avoiding in-memory operations.
We evaluate HTMA-Net on ResNet-18 using CIFAR-10, CIFAR-100, and Tiny ImageNet,
and provide a detailed comparison against regular, MF-only, and HT-only
variants. Results show that HTMA-Net eliminates up to 52\% of multiplications
compared to baseline ResNet-18, ResNet-20, and ResNet-50 models, while
achieving comparable accuracy in evaluation and significantly reducing
computational complexity and the number of parameters. Our results demonstrate
that combining structured Hadamard transform layers with SRAM-based in-memory
computing multiplication-avoiding operators is a promising path towards
efficient deep learning architectures.

</details>


### [800] [TRAX: TRacking Axles for Accurate Axle Count Estimation](https://arxiv.org/abs/2509.23171)
*Avinash Rai,Sandeep Jana,Vishal Vijay*

Main category: cs.CV

TL;DR: 提出基于视频的端到端车轴计数管道，结合检测方法和TRAX算法，减少误报、提高长车辆车轴计数准确性，向可扩展AI车轴计数系统迈进。


<details>
  <summary>Details</summary>
Motivation: 准确的车轴计数对交通控制、收费和基础设施发展至关重要，以往工作在密集环境有局限。

Method: 结合YOLO - OBB检测和分类车辆、YOLO检测轮胎，关联轮胎到车辆，提出TRAX算法跟踪车轴相关特征。

Result: 显著减少误报，提高长车辆车轴计数准确性，在真实交通视频中展现强鲁棒性。

Conclusion: 该工作是迈向可扩展AI车轴计数系统的重要一步，为机器视觉取代传统路边基础设施铺平道路。

Abstract: Accurate counting of vehicle axles is essential for traffic control, toll
collection, and infrastructure development. We present an end-to-end,
video-based pipeline for axle counting that tackles limitations of previous
works in dense environments. Our system leverages a combination of YOLO-OBB to
detect and categorize vehicles, and YOLO to detect tires. Detected tires are
intelligently associated to their respective parent vehicles, enabling accurate
axle prediction even in complex scenarios. However, there are a few challenges
in detection when it comes to scenarios with longer and occluded vehicles. We
mitigate vehicular occlusions and partial detections for longer vehicles by
proposing a novel TRAX (Tire and Axle Tracking) Algorithm to successfully track
axle-related features between frames. Our method stands out by significantly
reducing false positives and improving the accuracy of axle-counting for long
vehicles, demonstrating strong robustness in real-world traffic videos. This
work represents a significant step toward scalable, AI-driven axle counting
systems, paving the way for machine vision to replace legacy roadside
infrastructure.

</details>


### [801] [Patch Rebirth: Toward Fast and Transferable Model Inversion of Vision Transformers](https://arxiv.org/abs/2509.23235)
*Seongsoo Heo,Dong-Wan Choi*

Main category: cs.CV

TL;DR: 现有模型反演用于ViTs计算成本高，SMI方法丢弃部分块效率低，本文提出PRI方法，逐步分离重要块构建稀疏合成图像，实验显示其速度和准确率表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决模型反演应用于ViTs时的高计算成本问题，且改进SMI方法丢弃部分块带来的效率问题。

Method: 提出Patch Rebirth Inversion (PRI)方法，在反演过程中逐步分离重要块构建稀疏合成图像，让剩余块继续进化。

Result: PRI比标准Dense Model Inversion (DMI)快10倍，比SMI快2倍，准确率上始终优于SMI，与DMI相当。

Conclusion: PRI方法能有效平衡类别无关和类别特定知识，在速度和准确率上有较好表现。

Abstract: Model inversion is a widely adopted technique in data-free learning that
reconstructs synthetic inputs from a pretrained model through iterative
optimization, without access to original training data. Unfortunately, its
application to state-of-the-art Vision Transformers (ViTs) poses a major
computational challenge, due to their expensive self-attention mechanisms. To
address this, Sparse Model Inversion (SMI) was proposed to improve efficiency
by pruning and discarding seemingly unimportant patches, which were even
claimed to be obstacles to knowledge transfer. However, our empirical findings
suggest the opposite: even randomly selected patches can eventually acquire
transferable knowledge through continued inversion. This reveals that
discarding any prematurely inverted patches is inefficient, as it suppresses
the extraction of class-agnostic features essential for knowledge transfer,
along with class-specific features. In this paper, we propose Patch Rebirth
Inversion (PRI), a novel approach that incrementally detaches the most
important patches during the inversion process to construct sparse synthetic
images, while allowing the remaining patches to continue evolving for future
selection. This progressive strategy not only improves efficiency, but also
encourages initially less informative patches to gradually accumulate more
class-relevant knowledge, a phenomenon we refer to as the Re-Birth effect,
thereby effectively balancing class-agnostic and class-specific knowledge.
Experimental results show that PRI achieves up to 10x faster inversion than
standard Dense Model Inversion (DMI) and 2x faster than SMI, while consistently
outperforming SMI in accuracy and matching the performance of DMI.

</details>


### [802] [Self-Consistency as a Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection](https://arxiv.org/abs/2509.23236)
*Mingfei Han,Haihong Hao,Jinxing Zhou,Zhihui Li,Yuhui Zheng,Xueqing Deng,Linjie Yang,Xiaojun Chang*

Main category: cs.CV

TL;DR: 提出利用模型长回复与短答案自一致性生成偏好对的框架，在多基准测试中有效减少幻觉，提升事实基础和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型常产生幻觉细节、降低输出可靠性的问题，且现有方法依赖大量人工标注或外部监督。

Method: 设计自反思流程，对比详细回复和简洁二元答案，利用不一致信号自动生成高质量训练数据，仅依靠自一致性。

Result: 在AMBER、MultiObject - Hal (ROPE)等多个基准测试中显著提升事实基础和可靠性，在LLaVA - Bench和MMBench上提升指令遵循能力。

Conclusion: 该方法是可扩展且高效的解决方案，能利用无标签数据有效减少幻觉。

Abstract: Vision-language models often hallucinate details, generating non-existent
objects or inaccurate attributes that compromise output reliability. Existing
methods typically address these issues via extensive human annotations or
external supervision from more powerful models. In this work, we present a
novel framework that leverages the model's self-consistency between long
responses and short answers to generate preference pairs for training. We
observe that short binary questions tend to yield highly reliable responses,
which can be used to query the target model to evaluate and rank its generated
responses. Specifically, we design a self-reflection pipeline where detailed
model responses are compared against concise binary answers, and inconsistency
signals are utilized to automatically curate high-quality training data without
human annotations or external model-based supervision. By relying solely on
self-consistency rather than external supervision, our method offers a scalable
and efficient solution that effectively reduces hallucinations using unlabeled
data. Extensive experiments on multiple benchmarks, i.e., AMBER,
MultiObject-Hal (ROPE), Object HalBench, and MMHal-Bench, demonstrate
significant improvements in factual grounding and reliability. Moreover, our
approach maintains robust instruction-following ability, as evidenced by
enhanced performance on LLaVA-Bench and MMBench.

</details>


### [803] [Learning Regional Monsoon Patterns with a Multimodal Attention U-Net](https://arxiv.org/abs/2509.23267)
*Swaib Ilias Mazumder,Manish Kumar,Aparajita Khan*

Main category: cs.CV

TL;DR: 提出多模态深度学习框架用于印度高分辨率降水分类，表现优于单模态和现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确的季风降雨预测对印度农业、水管理和气候风险规划至关重要，但因地面观测稀疏和区域变化复杂而具有挑战性。

Method: 利用卫星和地球观测数据，策划1公里分辨率数据集，整合七种关键地理空间模态，使用注意力引导的U-Net架构，结合焦点和骰子损失函数。

Result: 多模态框架始终优于单模态基线和现有深度学习方法，在极端降雨类别中表现更佳。

Conclusion: 为印度区域季风预报、气候适应力和地理空间人工智能应用提供了可扩展框架、基准数据集和先进成果。

Abstract: Accurate monsoon rainfall prediction is vital for India's agriculture, water
management, and climate risk planning, yet remains challenging due to sparse
ground observations and complex regional variability. We present a multimodal
deep learning framework for high-resolution precipitation classification that
leverages satellite and Earth observation data. Unlike previous rainfall
prediction models based on coarse 5-50 km grids, we curate a new 1 km
resolution dataset for five Indian states, integrating seven key geospatial
modalities: land surface temperature, vegetation (NDVI), soil moisture,
relative humidity, wind speed, elevation, and land use, covering the
June-September 2024 monsoon season. Our approach uses an attention-guided U-Net
architecture to capture spatial patterns and temporal dependencies across
modalities, combined with focal and dice loss functions to handle rainfall
class imbalance defined by the India Meteorological Department (IMD).
Experiments demonstrate that our multimodal framework consistently outperforms
unimodal baselines and existing deep learning methods, especially in extreme
rainfall categories. This work contributes a scalable framework, benchmark
dataset, and state-of-the-art results for regional monsoon forecasting, climate
resilience, and geospatial AI applications in India.

</details>


### [804] [Vid-Freeze: Protecting Images from Malicious Image-to-Video Generation via Temporal Freezing](https://arxiv.org/abs/2509.23279)
*Rohit Chowdhury,Aniruddha Bala,Rohan Jaiswal,Siddharth Roheda*

Main category: cs.CV

TL;DR: 提出对抗攻击方法Vid - Freeze保护图像，阻止I2V模型生成恶意视频。


<details>
  <summary>Details</summary>
Motivation: I2V生成模型发展带来风险，现有防御机制对阻止运动合成的有效保护探索不足。

Method: 提出Vid - Freeze，向图像添加精心设计的对抗扰动，针对I2V模型的注意力机制。

Result: 免疫后的图像生成静止或接近静态的视频，实验显示该方法防护效果显著。

Conclusion: 注意力攻击是对抗I2V模型滥用的有前景方向。

Abstract: The rapid progress of image-to-video (I2V) generation models has introduced
significant risks, enabling video synthesis from static images and facilitating
deceptive or malicious content creation. While prior defenses such as I2VGuard
attempt to immunize images, effective and principled protection to block motion
remains underexplored. In this work, we introduce Vid-Freeze - a novel
attention-suppressing adversarial attack that adds carefully crafted
adversarial perturbations to images. Our method explicitly targets the
attention mechanism of I2V models, completely disrupting motion synthesis while
preserving semantic fidelity of the input image. The resulting immunized images
generate stand-still or near-static videos, effectively blocking malicious
content creation. Our experiments demonstrate the impressive protection
provided by the proposed approach, highlighting the importance of attention
attacks as a promising direction for robust and proactive defenses against
misuse of I2V generation models.

</details>


### [805] [Seeing Symbols, Missing Cultures: Probing Vision-Language Models' Reasoning on Fire Imagery and Cultural Meaning](https://arxiv.org/abs/2509.23311)
*Haorui Yu,Qiufeng Yi,Yijia Chu,Yang Zhao*

Main category: cs.CV

TL;DR: 研究指出视觉语言模型依赖表面模式匹配而非真正文化理解，提出诊断框架测试，发现系统偏差，强调文化评估重要性。


<details>
  <summary>Details</summary>
Motivation: 揭示视觉语言模型缺乏真正文化理解的问题，推动构建可解释和公平的多模态系统。

Method: 引入诊断框架，通过分类和解释分析探究视觉语言模型对火主题文化意象的推理，在多种场景下测试多个模型。

Result: 模型能正确识别西方主要节日，但处理代表性不足的文化活动有困难，常给出模糊标签或误判紧急情况。

Conclusion: 模型的失败暴露了符号捷径的风险，需要超越准确性指标进行文化评估。

Abstract: Vision-Language Models (VLMs) often appear culturally competent but rely on
superficial pattern matching rather than genuine cultural understanding. We
introduce a diagnostic framework to probe VLM reasoning on fire-themed cultural
imagery through both classification and explanation analysis. Testing multiple
models on Western festivals, non-Western traditions, and emergency scenes
reveals systematic biases: models correctly identify prominent Western
festivals but struggle with underrepresented cultural events, frequently
offering vague labels or dangerously misclassifying emergencies as
celebrations. These failures expose the risks of symbolic shortcuts and
highlight the need for cultural evaluation beyond accuracy metrics to ensure
interpretable and fair multimodal systems.

</details>


### [806] [DentVLM: A Multimodal Vision-Language Model for Comprehensive Dental Diagnosis and Enhanced Clinical Practice](https://arxiv.org/abs/2509.23344)
*Zijie Meng,Jin Hao,Xiwei Dai,Yang Feng,Jiaxiang Liu,Bin Feng,Huikai Wu,Xiaotang Gai,Hengchuan Zhu,Tianxiang Hu,Yangyang Wu,Hongxia Xu,Jin Li,Jun Xiao,Xiaoqiang Liu,Joey Tianyi Zhou,Fudong Zhu,Zhihe Zhao,Lunguo Xia,Bing Fang,Jimeng Sun,Jian Wu,Zuozhu Liu*

Main category: cs.CV

TL;DR: 介绍用于口腔疾病诊断的多模态视觉语言模型DentVLM，其性能优异，能提升牙医诊断表现并适用于多种场景。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型难以满足综合临床牙科实践的复杂多模态需求，需开发先进模型用于口腔疾病诊断和管理。

Method: 使用包含110,447张图像和246万个视觉问答对的大规模双语数据集开发DentVLM模型。

Result: DentVLM在36项诊断任务中显著优于领先模型；在临床研究中超越部分牙医；集成到协作流程后提升了初级牙医表现并减少诊断时间；在三个实用场景中表现良好。

Conclusion: DentVLM是强大的临床决策支持工具，可改善牙科初级保健，缓解医患失衡，使专业医疗知识更普及。

Abstract: Diagnosing and managing oral diseases necessitate advanced visual
interpretation across diverse imaging modalities and integrated information
synthesis. While current AI models excel at isolated tasks, they often fall
short in addressing the complex, multimodal requirements of comprehensive
clinical dental practice. Here we introduce DentVLM, a multimodal
vision-language model engineered for expert-level oral disease diagnosis.
DentVLM was developed using a comprehensive, large-scale, bilingual dataset of
110,447 images and 2.46 million visual question-answering (VQA) pairs. The
model is capable of interpreting seven 2D oral imaging modalities across 36
diagnostic tasks, significantly outperforming leading proprietary and
open-source models by 19.6% higher accuracy for oral diseases and 27.9% for
malocclusions. In a clinical study involving 25 dentists, evaluating 1,946
patients and encompassing 3,105 QA pairs, DentVLM surpassed the diagnostic
performance of 13 junior dentists on 21 of 36 tasks and exceeded that of 12
senior dentists on 12 of 36 tasks. When integrated into a collaborative
workflow, DentVLM elevated junior dentists' performance to senior levels and
reduced diagnostic time for all practitioners by 15-22%. Furthermore, DentVLM
exhibited promising performance across three practical utility scenarios,
including home-based dental health management, hospital-based intelligent
diagnosis and multi-agent collaborative interaction. These findings establish
DentVLM as a robust clinical decision support tool, poised to enhance primary
dental care, mitigate provider-patient imbalances, and democratize access to
specialized medical expertise within the field of dentistry.

</details>


### [807] [Dynamic-TreeRPO: Breaking the Independent Trajectory Bottleneck with Structured Sampling](https://arxiv.org/abs/2509.23352)
*Xiaolong Fu,Lichen Ma,Zipeng Guo,Gaojing Zhou,Chongxiao Wang,ShiPing Dong,Shizhe Zhou,Shizhe Zhou,Ximan Liu,Jingling Fu,Tan Lit Sin,Yu Shi,Zhen Chen,Junshi Huang,Jason Li*

Main category: cs.CV

TL;DR: 提出Dynamic - TreeRPO和LayerTuning - RL，提升文本到图像生成质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习融入文本到图像生成的流匹配模型存在探索不充分和采样策略低效问题。

Method: 提出Dynamic - TreeRPO，采用滑动窗口采样策略，在树结构中进行GRPO优化和SDE采样；构建LayerTuning - RL，将SFT和RL范式结合，重新定义损失函数。

Result: 模型在语义一致性、视觉保真度和人类偏好对齐方面优于现有基线，在多个基准上分别领先4.9%、5.91%和8.66%，训练效率提升近50%。

Conclusion: 所提方法有效，能动态探索多样搜索空间，提升生成质量和训练效率。

Abstract: The integration of Reinforcement Learning (RL) into flow matching models for
text-to-image (T2I) generation has driven substantial advances in generation
quality. However, these gains often come at the cost of exhaustive exploration
and inefficient sampling strategies due to slight variation in the sampling
group. Building on this insight, we propose Dynamic-TreeRPO, which implements
the sliding-window sampling strategy as a tree-structured search with dynamic
noise intensities along depth. We perform GRPO-guided optimization and
constrained Stochastic Differential Equation (SDE) sampling within this tree
structure. By sharing prefix paths of the tree, our design effectively
amortizes the computational overhead of trajectory search. With well-designed
noise intensities for each tree layer, Dynamic-TreeRPO can enhance the
variation of exploration without any extra computational cost. Furthermore, we
seamlessly integrate Supervised Fine-Tuning (SFT) and RL paradigm within
Dynamic-TreeRPO to construct our proposed LayerTuning-RL, reformulating the
loss function of SFT as a dynamically weighted Progress Reward Model (PRM)
rather than a separate pretraining method. By associating this weighted PRM
with dynamic-adaptive clipping bounds, the disruption of exploration process in
Dynamic-TreeRPO is avoided. Benefiting from the tree-structured sampling and
the LayerTuning-RL paradigm, our model dynamically explores a diverse search
space along effective directions. Compared to existing baselines, our approach
demonstrates significant superiority in terms of semantic consistency, visual
fidelity, and human preference alignment on established benchmarks, including
HPS-v2.1, PickScore, and ImageReward. In particular, our model outperforms SoTA
by $4.9\%$, $5.91\%$, and $8.66\%$ on those benchmarks, respectively, while
improving the training efficiency by nearly $50\%$.

</details>


### [808] [Enhanced Fracture Diagnosis Based on Critical Regional and Scale Aware in YOLO](https://arxiv.org/abs/2509.23408)
*Yuyang Sun,Junchuan Yu,Cuiming Zou*

Main category: cs.CV

TL;DR: 本文提出改进的Fracture - YOLO模型用于骨折检测，该模型集成新模块提升性能，实验显示检测精度显著提高。


<details>
  <summary>Details</summary>
Motivation: 传统骨折诊断依赖医生经验，速度和准确性受限，需借助深度学习模型提高诊断效率和准确性。

Method: 提出基于YOLO的Fracture - YOLO模型，集成Critical - Region - Selector Attention (CRSelector)和Scale - Aware (ScA)模块，前者聚焦骨折区域关键特征，后者调整不同尺度特征权重。

Result: 与基线模型相比，Fracture - YOLO的mAP50和mAP50 - 95分别提高4和3，达到了最先进水平。

Conclusion: Fracture - YOLO模型能有效提升骨折检测性能，具有更好的检测精度。

Abstract: Fracture detection plays a critical role in medical imaging analysis,
traditional fracture diagnosis relies on visual assessment by experienced
physicians, however the speed and accuracy of this approach are constrained by
the expertise. With the rapid advancements in artificial intelligence, deep
learning models based on the YOLO framework have been widely employed for
fracture detection, demonstrating significant potential in improving diagnostic
efficiency and accuracy. This study proposes an improved YOLO-based model,
termed Fracture-YOLO, which integrates novel Critical-Region-Selector Attention
(CRSelector) and Scale-Aware (ScA) heads to further enhance detection
performance. Specifically, the CRSelector module utilizes global texture
information to focus on critical features of fracture regions. Meanwhile, the
ScA module dynamically adjusts the weights of features at different scales,
enhancing the model's capacity to identify fracture targets at multiple scales.
Experimental results demonstrate that, compared to the baseline model,
Fracture-YOLO achieves a significant improvement in detection precision, with
mAP50 and mAP50-95 increasing by 4 and 3, surpassing the baseline model and
achieving state-of-the-art (SOTA) performance.

</details>


### [809] [Enhancing Polyp Segmentation via Encoder Attention and Dynamic Kernel Update](https://arxiv.org/abs/2509.23502)
*Fatemeh Salahi Chashmi,Roya Sotoudeh*

Main category: cs.CV

TL;DR: 提出结合动态内核机制与全局编码器注意力模块的框架用于息肉分割，在基准数据集上表现优于现有方法，且简化了解码器结构。


<details>
  <summary>Details</summary>
Motivation: 息肉分割因息肉形状、大小多样及边界低对比度而具有挑战性，需提高分割的准确性和效率。

Method: 将动态内核机制与全局编码器注意力模块集成，在解码器中使用统一通道自适应方法，引入灵活的注意力驱动内核初始化和统一解码器设计。

Result: 在KvasirSEG和CVC ClinicDB基准数据集上，模型优于多种先进分割方法，取得更好的Dice和交并比分数，且简化解码器结构降低计算成本。

Conclusion: 所提方法为息肉分割提供了稳健且自适应的解决方案，在临床和自动诊断系统中有应用前景。

Abstract: Polyp segmentation is a critical step in colorectal cancer detection, yet it
remains challenging due to the diverse shapes, sizes, and low contrast
boundaries of polyps in medical imaging. In this work, we propose a novel
framework that improves segmentation accuracy and efficiency by integrating a
Dynamic Kernel (DK) mechanism with a global Encoder Attention module. The DK
mechanism, initialized by a global context vector from the EA module,
iteratively refines segmentation predictions across decoding stages, enabling
the model to focus on and accurately delineate complex polyp boundaries. The EA
module enhances the network's ability to capture critical lesion features by
aggregating multi scale information from all encoder layers. In addition, we
employ Unified Channel Adaptation (UCA) in the decoder to standardize feature
dimensions across stages, ensuring consistent and computationally efficient
information fusion. Our approach extends the lesion-aware kernel framework by
introducing a more flexible, attention driven kernel initialization and a
unified decoder design. Extensive experiments on the KvasirSEG and CVC ClinicDB
benchmark datasets demonstrate that our model outperforms several state of the
art segmentation methods, achieving superior Dice and Intersection over Union
scores. Moreover, UCA simplifies the decoder structure, reducing computational
cost without compromising accuracy. Overall, the proposed method provides a
robust and adaptable solution for polyp segmentation, with promising
applications in clinical and automated diagnostic systems.

</details>


### [810] [Evaluating point-light biological motion in multimodal large language models](https://arxiv.org/abs/2509.23517)
*Akila Kadambi,Marco Iacoboni,Lisa Aziz-Zadeh,Srini Narayanan*

Main category: cs.CV

TL;DR: 介绍首个评估多模态大语言模型（MLLMs）从人类点光显示（PLDs）进行动作处理的基准ActPLD，测试模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 人类能从PLD提取语义信息，PLD可用于测试动作理解系统的限制，需评估MLLMs对PLD的动作处理能力。

Method: 引入ActPLD基准，对单演员和社交互动PLD上的先进专有和开源系统进行测试。

Result: 各模型表现始终较差，在动作和时空理解方面存在根本差距。

Conclusion: MLLMs在基于PLD的动作和时空理解上存在很大不足。

Abstract: Humans can extract rich semantic information from minimal visual cues, as
demonstrated by point-light displays (PLDs), which consist of sparse sets of
dots localized to key joints of the human body. This ability emerges early in
development and is largely attributed to human embodied experience. Since PLDs
isolate body motion as the sole source of meaning, they represent key stimuli
for testing the constraints of action understanding in these systems. Here we
introduce ActPLD, the first benchmark to evaluate action processing in MLLMs
from human PLDs. Tested models include state-of-the-art proprietary and
open-source systems on single-actor and socially interacting PLDs. Our results
reveal consistently low performance across models, introducing fundamental gaps
in action and spatiotemporal understanding.

</details>


### [811] [Imaging-Based Mortality Prediction in Patients with Systemic Sclerosis](https://arxiv.org/abs/2509.23530)
*Alec K. Peltekian,Karolina Senkow,Gorkem Durak,Kevin M. Grudzinski,Bradford C. Bemiss,Jane E. Dematte,Carrie Richardson,Nikolay S. Markov,Mary Carns,Kathleen Aren,Alexandra Soriano,Matthew Dapas,Harris Perlman,Aaron Gundersheimer,Kavitha C. Selvan,John Varga,Monique Hinchcliff,Krishnan Warrior,Catherine A. Gao,Richard G. Wunderink,GR Scott Budinger,Alok N. Choudhary,Anthony J. Esposito,Alexander V. Misharin,Ankit Agrawal,Ulas Bagci*

Main category: cs.CV

TL;DR: 本文提出新的胸部CT分析框架，用放射组学和深度学习预测系统性硬化症（SSc）肺部并发症相关死亡率，模型取得较好预测效果，显示方法潜力。


<details>
  <summary>Details</summary>
Motivation: 胸部CT在SSc疾病进展和死亡率预测中的作用未完全明确，需新方法预测SSc肺部并发症相关死亡率。

Method: 收集2125份SSc患者CT扫描数据，用ResNet - 18、DenseNet - 121和Swin Transformer预训练模型在数据上微调，进行1、3、5年死亡率分析。

Result: 模型预测1、3、5年死亡率的AUC分别为0.769、0.801、0.709。

Conclusion: 放射组学和深度学习计算方法在改善SSc相关间质性肺病的早期检测和风险评估方面有潜力。

Abstract: Interstitial lung disease (ILD) is a leading cause of morbidity and mortality
in systemic sclerosis (SSc). Chest computed tomography (CT) is the primary
imaging modality for diagnosing and monitoring lung complications in SSc
patients. However, its role in disease progression and mortality prediction has
not yet been fully clarified. This study introduces a novel, large-scale
longitudinal chest CT analysis framework that utilizes radiomics and deep
learning to predict mortality associated with lung complications of SSc. We
collected and analyzed 2,125 CT scans from SSc patients enrolled in the
Northwestern Scleroderma Registry, conducting mortality analyses at one, three,
and five years using advanced imaging analysis techniques. Death labels were
assigned based on recorded deaths over the one-, three-, and five-year
intervals, confirmed by expert physicians. In our dataset, 181, 326, and 428 of
the 2,125 CT scans were from patients who died within one, three, and five
years, respectively. Using ResNet-18, DenseNet-121, and Swin Transformer we use
pre-trained models, and fine-tuned on 2,125 images of SSc patients. Models
achieved an AUC of 0.769, 0.801, 0.709 for predicting mortality within one-,
three-, and five-years, respectively. Our findings highlight the potential of
both radiomics and deep learning computational methods to improve early
detection and risk assessment of SSc-related interstitial lung disease, marking
a significant advancement in the literature.

</details>


### [812] [Pancreas Part Segmentation under Federated Learning Paradigm](https://arxiv.org/abs/2509.23562)
*Ziliang Hong,Halil Ertugrul Aktas,Andrea Mia Bejar,Katherine Wu,Hongyi Pan,Gorkem Durak,Zheyuan Zhang,Sait Kayali,Temel Tirkes,Federica Proietto Salanitri,Concetto Spampinato,Michael Goggins,Tamas Gonda,Candice Bolan,Raj Keswani,Frank Miller,Michael Wallace,Ulas Bagci*

Main category: cs.CV

TL;DR: 本文提出首个用于MRI胰腺部位分割的联邦学习方法，解决技术复杂和数据稀缺问题，经评估找到最优组合并采用新损失函数，取得临床可行效果。


<details>
  <summary>Details</summary>
Motivation: 胰腺疾病有区域异质性，准确分割胰腺各部位对诊断和治疗至关重要，但MRI分割任务因多种因素极具挑战，且此前方法受数据稀缺阻碍。

Method: 引入隐私保护的联邦学习框架，跨七个医疗机构协作训练模型，评估三种分割架构与两种联邦学习算法的组合，采用新的解剖学信息损失函数。

Result: Attention U - Net与FedAvg组合对胰腺异质性最优，方法在分布式、异构数据集上训练取得临床可行性能。

Conclusion: 所提出的联邦学习方法能有效解决胰腺部位MRI分割的技术和数据问题，有临床应用价值。

Abstract: We present the first federated learning (FL) approach for pancreas part(head,
body and tail) segmentation in MRI, addressing a critical clinical challenge as
a significant innovation. Pancreatic diseases exhibit marked regional
heterogeneity cancers predominantly occur in the head region while chronic
pancreatitis causes tissue loss in the tail, making accurate segmentation of
the organ into head, body, and tail regions essential for precise diagnosis and
treatment planning. This segmentation task remains exceptionally challenging in
MRI due to variable morphology, poor soft-tissue contrast, and anatomical
variations across patients. Our novel contribution tackles two fundamental
challenges: first, the technical complexity of pancreas part delineation in
MRI, and second the data scarcity problem that has hindered prior approaches.
We introduce a privacy-preserving FL framework that enables collaborative model
training across seven medical institutions without direct data sharing,
leveraging a diverse dataset of 711 T1W and 726 T2W MRI scans. Our key
innovations include: (1) a systematic evaluation of three state-of-the-art
segmentation architectures (U-Net, Attention U-Net,Swin UNETR) paired with two
FL algorithms (FedAvg, FedProx), revealing Attention U-Net with FedAvg as
optimal for pancreatic heterogeneity, which was never been done before; (2) a
novel anatomically-informed loss function prioritizing region-specific texture
contrasts in MRI. Comprehensive evaluation demonstrates that our approach
achieves clinically viable performance despite training on distributed,
heterogeneous datasets.

</details>


### [813] [Multi-Level Heterogeneous Knowledge Transfer Network on Forward Scattering Center Model for Limited Samples SAR ATR](https://arxiv.org/abs/2509.23596)
*Chenxi Zhao,Daochang Wang,Siqian Zhang,Gangyao Kuang*

Main category: cs.CV

TL;DR: 提出MHKT网络利用FSCM迁移纯目标知识解决SAR目标识别样本有限问题，实验证明方法性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决现有模拟数据辅助SAR目标识别方法中图像包含大量无关信息影响迁移信息质量的问题。

Method: 提出MHKT网络，从特征、分布和类别层面迁移FSCM知识，用TAIS分离无用知识，用MDD感知可迁移知识，用CRKT模块解决数据不平衡问题。

Result: 在由FSCM数据和实测SAR图像构成的两个新数据集上实验，方法表现优越。

Conclusion: 所提方法能确保迁移的FSCM知识完整性，有效解决相关问题。

Abstract: Simulated data-assisted SAR target recognition methods are the research
hotspot currently, devoted to solving the problem of limited samples. Existing
works revolve around simulated images, but the large amount of irrelevant
information embedded in the images, such as background, noise, etc., seriously
affects the quality of the migrated information. Our work explores a new
simulated data to migrate purer and key target knowledge, i.e., forward
scattering center model (FSCM) which models the actual local structure of the
target with strong physical meaning and interpretability. To achieve this
purpose, multi-level heterogeneous knowledge transfer (MHKT) network is
proposed, which fully migrates FSCM knowledge from the feature, distribution
and category levels, respectively. Specifically, we permit the more suitable
feature representations for the heterogeneous data and separate non-informative
knowledge by task-associated information selector (TAIS), to complete purer
target feature migration. In the distribution alignment, the new metric
function maximum discrimination divergence (MDD) in target generic knowledge
transfer (TGKT) module perceives transferable knowledge efficiently while
preserving discriminative structure about classes. Moreover, category relation
knowledge transfer (CRKT) module leverages the category relation consistency
constraint to break the dilemma of optimization bias towards simulation data
due to imbalance between simulated and measured data. Such stepwise knowledge
selection and migration will ensure the integrity of the migrated FSCM
knowledge. Notably, extensive experiments on two new datasets formed by FSCM
data and measured SAR images demonstrate the superior performance of our
method.

</details>


### [814] [InteractMove: Text-Controlled Human-Object Interaction Generation in 3D Scenes with Movable Objects](https://arxiv.org/abs/2509.23612)
*Xinhao Cai,Minghang Zheng,Xin Jin,Yang Liu*

Main category: cs.CV

TL;DR: 提出3D场景中文本控制的人体与可移动物体交互生成任务，构建InteractMove数据集并提出新的解决方案，实验证明方法优越性。


<details>
  <summary>Details</summary>
Motivation: 现有人类场景交互数据集交互类别不足，且多针对静态物体，收集可移动物体数据集困难且成本高。

Method: 先使用3D视觉定位模型识别交互对象，再提出手 - 对象联合可供性学习预测接触区域，最后通过局部场景建模和避障约束优化交互。

Result: 综合实验表明，该方法在生成符合物理规律、文本要求的交互方面优于现有方法。

Conclusion: 提出的任务、构建的数据集和解决方案有效可行，能解决现有数据集存在的问题。

Abstract: We propose a novel task of text-controlled human object interaction
generation in 3D scenes with movable objects. Existing human-scene interaction
datasets suffer from insufficient interaction categories and typically only
consider interactions with static objects (do not change object positions), and
the collection of such datasets with movable objects is difficult and costly.
To address this problem, we construct the InteractMove dataset for Movable
Human-Object Interaction in 3D Scenes by aligning existing human object
interaction data with scene contexts, featuring three key characteristics: 1)
scenes containing multiple movable objects with text-controlled interaction
specifications (including same-category distractors requiring spatial and 3D
scene context understanding), 2) diverse object types and sizes with varied
interaction patterns (one-hand, two-hand, etc.), and 3) physically plausible
object manipulation trajectories. With the introduction of various movable
objects, this task becomes more challenging, as the model needs to identify
objects to be interacted with accurately, learn to interact with objects of
different sizes and categories, and avoid collisions between movable objects
and the scene. To tackle such challenges, we propose a novel pipeline solution.
We first use 3D visual grounding models to identify the interaction object.
Then, we propose a hand-object joint affordance learning to predict contact
regions for different hand joints and object parts, enabling accurate grasping
and manipulation of diverse objects. Finally, we optimize interactions with
local-scene modeling and collision avoidance constraints, ensuring physically
plausible motions and avoiding collisions between objects and the scene.
Comprehensive experiments demonstrate our method's superiority in generating
physically plausible, text-compliant interactions compared to existing
approaches.

</details>


### [815] [BioVessel-Net and RetinaMix: Unsupervised Retinal Vessel Segmentation from OCTA Images](https://arxiv.org/abs/2509.23617)
*Cheng Huang,Weizheng Xie,Fan Gao,Yutong Liu,Ruoling Wu,Zeyu Han,Jingxi Qiu,Xiangxiang Wang,Zhenglin Yang,Hao Wang,Yongbin Yu*

Main category: cs.CV

TL;DR: 提出BioVessel - Net无监督生成框架和RetinaMix数据集，实现视网膜血管准确分割，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前血管分割方法依赖监督学习和大量手动标注，成本高、易出错且在OCTA中难获取标注。

Method: 提出BioVessel - Net，整合血管生物统计学、对抗性细化和半径引导分割策略；引入RetinaMix数据集。

Result: BioVessel - Net在RetinaMix和现有数据集上实现接近完美的分割精度，大幅超越现有监督和半监督方法。

Conclusion: BioVessel - Net和RetinaMix为视网膜血管分析提供无标签、计算高效且临床可解释的解决方案，有广泛应用潜力。

Abstract: Structural changes in retinal blood vessels are critical biomarkers for the
onset and progression of glaucoma and other ocular diseases. However, current
vessel segmentation approaches largely rely on supervised learning and
extensive manual annotations, which are costly, error-prone, and difficult to
obtain in optical coherence tomography angiography. Here we present
BioVessel-Net, an unsupervised generative framework that integrates vessel
biostatistics with adversarial refinement and a radius-guided segmentation
strategy. Unlike pixel-based methods, BioVessel-Net directly models vascular
structures with biostatistical coherence, achieving accurate and explainable
vessel extraction without labeled data or high-performance computing. To
support training and evaluation, we introduce RetinaMix, a new benchmark
dataset of 2D and 3D OCTA images with high-resolution vessel details from
diverse populations. Experimental results demonstrate that BioVessel-Net
achieves near-perfect segmentation accuracy across RetinaMix and existing
datasets, substantially outperforming state-of-the-art supervised and
semi-supervised methods. Together, BioVessel-Net and RetinaMix provide a
label-free, computationally efficient, and clinically interpretable solution
for retinal vessel analysis, with broad potential for glaucoma monitoring,
blood flow modeling, and progression prediction. Code and dataset are
available: https://github.com/VikiXie/SatMar8.

</details>


### [816] [RIV: Recursive Introspection Mask Diffusion Vision Language Model](https://arxiv.org/abs/2509.23625)
*YuQian Li,Limeng Qiao,Lin Ma*

Main category: cs.CV

TL;DR: 提出递归内省掩码扩散视觉语言模型RIV，通过内省训练和递归推理赋予模型自我纠错能力，在多基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于掩码扩散的视觉语言模型缺乏自我纠错能力，无法纠正生成标记中的错误。

Method: 提出两种机制，一是内省训练，引入内省模型识别生成序列中的错误；二是递归推理，重复“解掩码→内省→重新掩码”过程直至得到可靠结果。

Result: 在多个基准测试中，RIV达到了最先进的性能，优于大多数现有的MDVLMs。

Conclusion: 提出的RIV模型有效可行，能够解决现有MDVLMs缺乏自我纠错能力的问题。

Abstract: Mask Diffusion-based Vision Language Models (MDVLMs) have achieved remarkable
progress in multimodal understanding tasks. However, these models are unable to
correct errors in generated tokens, meaning they lack self-correction
capability. In this paper, we propose Recursive Introspection Mask Diffusion
Vision Language Model (RIV), which equips the model with self-correction
ability through two novel mechanisms. The first is Introspection Training,
where an Introspection Model is introduced to identify errors within generated
sequences. Introspection Training enables the model to detect not only
grammatical and spelling mistakes, but more importantly, logical errors. The
second is Recursive Inference. Beginning with the standard unmasking step, the
learned Introspection Model helps to identify errors in the output sequence and
remask them. This alternating
($\text{unmask}\rightarrow\text{introspection}\rightarrow\text{remask}$)
process is repeated recursively until reliable results are obtained.
Experimental results on multiple benchmarks demonstrate that the proposed RIV
achieves state-of-the-art performance, outperforming most existing MDVLMs.

</details>


### [817] [LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via Debiasing Pre-trained Text Encoders](https://arxiv.org/abs/2509.23639)
*Boyu Han,Qianqian Xu,Shilong Bao,Zhiyong Yang,Kangli Zi,Qingming Huang*

Main category: cs.CV

TL;DR: 提出轻量级方法LightFair实现公平的文本到图像扩散模型，通过微调文本嵌入减轻偏差，经实验验证有效高效。


<details>
  <summary>Details</summary>
Motivation: 现有方法训练或采样负担重且性能不佳，文本编码器是T2I DMs中最易微调的前端模块，可通过微调文本嵌入减轻偏差。

Method: 提出协作距离约束去偏策略微调文本嵌入，引入两阶段文本引导采样策略平衡去偏与生成质量。

Result: 在Stable Diffusion v1.5上，以1/4的训练负担实现SOTA去偏，采样负担几乎无增加。

Conclusion: LightFair方法有效且高效。

Abstract: This paper explores a novel lightweight approach LightFair to achieve fair
text-to-image diffusion models (T2I DMs) by addressing the adverse effects of
the text encoder. Most existing methods either couple different parts of the
diffusion model for full-parameter training or rely on auxiliary networks for
correction. They incur heavy training or sampling burden and unsatisfactory
performance. Since T2I DMs consist of multiple components, with the text
encoder being the most fine-tunable and front-end module, this paper focuses on
mitigating bias by fine-tuning text embeddings. To validate feasibility, we
observe that the text encoder's neutral embedding output shows substantial
skewness across image embeddings of various attributes in the CLIP space. More
importantly, the noise prediction network further amplifies this imbalance. To
finetune the text embedding, we propose a collaborative distance-constrained
debiasing strategy that balances embedding distances to improve fairness
without auxiliary references. However, mitigating bias can compromise the
original generation quality. To address this, we introduce a two-stage
text-guided sampling strategy to limit when the debiased text encoder
intervenes. Extensive experiments demonstrate that LightFair is effective and
efficient. Notably, on Stable Diffusion v1.5, our method achieves SOTA
debiasing at just $1/4$ of the training burden, with virtually no increase in
sampling burden. The code is available at https://github.com/boyuh/LightFair.

</details>


### [818] [ReWatch-R1: Boosting Complex Video Reasoning in Large Vision-Language Models through Agentic Data Synthesis](https://arxiv.org/abs/2509.23652)
*Congzhi Zhang,Zhibin Wang,Yinchao Ma,Jiawei Peng,Yihan Wang,Qiang Zhou,Jun Song,Bo Zheng*

Main category: cs.CV

TL;DR: 本文指出RLVR在复杂视频推理应用不足，源于数据瓶颈，为此引入ReWatch数据集，提出合成管道和Multi - Agent ReAct框架，开发ReWatch - R1，实验显示其在五个视频推理基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR在复杂视频推理应用中因现有数据集缺乏挑战性问题和高质量CoT数据导致的发展不足问题。

Method: 引入ReWatch数据集，提出多阶段合成管道合成其三个组件；使用Multi - Agent ReAct框架进行CoT合成；用SFT和RLVR框架对LVLM进行后训练得到ReWatch - R1，RLVR框架含O&R奖励机制。

Result: ReWatch - R1在五个具有挑战性的视频推理基准测试中达到了当前最优的平均性能。

Conclusion: ReWatch数据集和ReWatch - R1模型有助于推动复杂视频推理的发展。

Abstract: While Reinforcement Learning with Verifiable Reward (RLVR) significantly
advances image reasoning in Large Vision-Language Models (LVLMs), its
application to complex video reasoning remains underdeveloped. This gap stems
primarily from a critical data bottleneck: existing datasets lack the
challenging, multi-hop questions and high-quality, video-grounded
Chain-of-Thought (CoT) data necessary to effectively bootstrap RLVR. To address
this, we introduce ReWatch, a large-scale dataset built to foster advanced
video reasoning. We propose a novel multi-stage synthesis pipeline to
synthesize its three components: ReWatch-Caption, ReWatch-QA, and ReWatch-CoT.
A core innovation is our Multi-Agent ReAct framework for CoT synthesis, which
simulates a human-like "re-watching" process to generate video-grounded
reasoning traces by explicitly modeling information retrieval and verification.
Building on this dataset, we develop ReWatch-R1 by post-training a strong
baseline LVLM with Supervised Fine-Tuning (SFT) and our RLVR framework. This
framework incorporates a novel Observation \& Reasoning (O\&R) reward mechanism
that evaluates both the final answer's correctness and the reasoning's
alignment with video content, directly penalizing hallucination. Our
experiments show that ReWatch-R1 achieves state-of-the-art average performance
on five challenging video reasoning benchmarks.

</details>


### [819] [RCI: A Score for Evaluating Global and Local Reasoning in Multimodal Benchmarks](https://arxiv.org/abs/2509.23673)
*Amit Agarwal,Hitesh Laxmichand Patel,Srikant Panda,Hansa Meghwani,Jyotika Singh,Karan Dua,Paul Li,Tao Sheng,Sujith Ravi,Dan Roth*

Main category: cs.CV

TL;DR: 本文引入区域理解指数（RCI）评估多模态基准数据集对全局和局部视觉信息的依赖，发现多数基准利于局部推理且有空间偏差，RCI可辅助构建鲁棒多模态系统。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法明确区分多模态大语言模型基准是否评估了真正的全局推理，阻碍数据集管理和模型开发。

Method: 引入区域理解指数（RCI），系统比较参考模型在图像块和完整图像上的性能。

Result: 应用RCI到13个常用多模态基准，发现多数利于局部推理且有显著空间偏差。

Conclusion: RCI为研究人员和从业者提供诊断和缓解偏差的工具，有助于构建强大的企业级多模态系统。

Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive results on
vision-language benchmarks, yet it remains unclear whether these benchmarks
assess genuine global reasoning or allow success via localized visual cues.
Existing evaluation methods do not explicitly measure this distinction,
hindering effective dataset curation and real-world focused model development.
  We introduce Region Comprehension Index (RCI), the first model-based score to
directly quantify a dataset's reliance on global versus local visual
information. RCI systematically compares reference-model performance on image
patches versus full images, revealing if tasks require holistic image
understanding or can be solved with partial or localized visual cues.
  When applying RCI to 13 widely used multimodal benchmarks, we observed that
most of them favor localized reasoning and exhibit significant spatial biases,
indicating potential risks in real-world applications. RCI equips researchers &
practitioners with an actionable tool for diagnosing & mitigating these biases,
enabling the construction of datasets and benchmarks to foster the development
of robust, enterprise-ready multimodal systems.

</details>


### [820] [CrimEdit: Controllable Editing for Counterfactual Object Removal, Insertion, and Movement](https://arxiv.org/abs/2509.23708)
*Boseong Jeon,Junghyuk Lee,Jimin Park,Kwanyoung Kim,Jingi Jung,Sangwon Lee,Hyunbo Shim*

Main category: cs.CV

TL;DR: 提出CrimEdit模型，联合训练去除和插入任务嵌入，利用无分类器引导方案，实现对象去除、效果插入和对象移动，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索在统一模型中应用无分类器引导处理对象去除和插入任务中对象效果的性能影响，为填补此空白并提高合成编辑效率。

Method: 提出CrimEdit，在单个模型中联合训练去除和插入的任务嵌入，在无分类器引导方案中利用它们，还将两个任务提示扩展应用于不同空间区域。

Result: 通过大量实验表明，CrimEdit无需额外训练或单独的去除和插入阶段，就能实现卓越的对象去除、可控的效果插入和高效的对象移动。

Conclusion: CrimEdit在对象去除、效果插入和对象移动方面表现出色，是一种有效的复合编辑方法。

Abstract: Recent works on object removal and insertion have enhanced their performance
by handling object effects such as shadows and reflections, using diffusion
models trained on counterfactual datasets. However, the performance impact of
applying classifier-free guidance to handle object effects across removal and
insertion tasks within a unified model remains largely unexplored. To address
this gap and improve efficiency in composite editing, we propose CrimEdit,
which jointly trains the task embeddings for removal and insertion within a
single model and leverages them in a classifier-free guidance scheme --
enhancing the removal of both objects and their effects, and enabling
controllable synthesis of object effects during insertion. CrimEdit also
extends these two task prompts to be applied to spatially distinct regions,
enabling object movement (repositioning) within a single denoising step. By
employing both guidance techniques, extensive experiments show that CrimEdit
achieves superior object removal, controllable effect insertion, and efficient
object movement without requiring additional training or separate removal and
insertion stages.

</details>


### [821] [Video Panels for Long Video Understanding](https://arxiv.org/abs/2509.23724)
*Lars Doorenbos,Federico Spurio,Juergen Gall*

Main category: cs.CV

TL;DR: 本文提出针对长视频理解的视觉提示策略，无需训练、无参数且模型无关，能提升长视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型在长视频理解上性能落后，为提升性能，不采用微调而是最大化现有模型性能。

Method: 提出新颖的视觉提示策略，将多帧组合成面板图像，用空间细节换取时间分辨率。

Result: 在五个基准测试上实验结果一致，在TimeScope (Long)数据集上视频问答准确率最高提升19.4%。

Conclusion: 该方法提升了长视频理解模型的水平。

Abstract: Recent Video-Language Models (VLMs) achieve promising results on long-video
understanding, but their performance still lags behind that achieved on tasks
involving images or short videos. This has led to great interest in improving
the long context modeling of VLMs by introducing novel modules and additional
complexity. % additional training time. In this paper, we take a different
approach: rather than fine-tuning VLMs with the limited data available, we
attempt to maximize the performance of existing models. To this end, we propose
a novel visual prompting strategy specifically designed for long-video
understanding. By combining multiple frames as panels into one image, we
effectively trade off spatial details for temporal resolution. Our approach is
training-free, parameter-free, and model-agnostic, and can be seamlessly
integrated into existing VLMs. Extensive experiments on five established
benchmarks across a wide range of model architectures, sizes, and context
windows confirm the consistency of our approach. For the TimeScope (Long)
dataset, which has the longest videos, the accuracy for video question
answering is improved by up to 19.4\%. Overall, our method raises the bar for
long video understanding models. We will make our code available upon
acceptance.

</details>


### [822] [Learning Temporal Saliency for Time Series Forecasting with Cross-Scale Attention](https://arxiv.org/abs/2509.22839)
*Ibrahim Delibasoglu,Fredrik Heintz*

Main category: cs.CV

TL;DR: 提出CrossScaleNet架构，结合基于补丁的交叉注意力机制与多尺度处理，在时间序列预测中实现高性能和增强的时间可解释性，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测的可解释性对提高模型透明度和支持决策至关重要，传统事后方法计算成本高，现有宣称有可解释性的模型在基准测试中表现不佳。

Method: 提出CrossScaleNet架构，将注意力机制嵌入训练过程。

Result: 在合成数据集和公共基准上验证了识别时间显著性的鲁棒性，在真实世界数据集预测任务中优于多数基于Transformer的模型。

Conclusion: CrossScaleNet有效捕捉时间显著性，在不同复杂度数据集上提供了兼顾可解释性和预测准确性的平衡方法。

Abstract: Explainability in time series forecasting is essential for improving model
transparency and supporting informed decision-making. In this work, we present
CrossScaleNet, an innovative architecture that combines a patch-based
cross-attention mechanism with multi-scale processing to achieve both high
performance and enhanced temporal explainability. By embedding attention
mechanisms into the training process, our model provides intrinsic
explainability for temporal saliency, making its decision-making process more
transparent. Traditional post-hoc methods for temporal saliency detection are
computationally expensive, particularly when compared to feature importance
detection. While ablation techniques may suffice for datasets with fewer
features, identifying temporal saliency poses greater challenges due to its
complexity. We validate CrossScaleNet on synthetic datasets with known saliency
ground truth and on established public benchmarks, demonstrating the robustness
of our method in identifying temporal saliency. Experiments on real-world
datasets for forecasting task show that our approach consistently outperforms
most transformer-based models, offering better explainability without
sacrificing predictive accuracy. Our evaluations demonstrate superior
performance in both temporal saliency detection and forecasting accuracy.
Moreover, we highlight that existing models claiming explainability often fail
to maintain strong performance on standard benchmarks. CrossScaleNet addresses
this gap, offering a balanced approach that captures temporal saliency
effectively while delivering state-of-the-art forecasting performance across
datasets of varying complexity.

</details>


### [823] [M3DLayout: A Multi-Source Dataset of 3D Indoor Layouts and Structured Descriptions for 3D Generation](https://arxiv.org/abs/2509.23728)
*Yiheng Zhang,Zhuojiang Cai,Mingdao Wang,Meitong Guo,Tianxiao Li,Li Lin,Yuwang Wang*

Main category: cs.CV

TL;DR: 提出大规模多源数据集M3DLayout用于3D室内布局生成，经实验验证能为训练布局生成模型提供基础，有望推动文本驱动3D场景合成研究。


<details>
  <summary>Details</summary>
Motivation: 当前3D室内布局生成模型受现有数据集规模、多样性和标注质量限制，学习能力受限。

Method: 引入包含15,080个布局和超258k对象实例的M3DLayout数据集，集成三种数据源，并为每个布局配详细文本；用文本条件扩散模型建立基准评估。

Result: 实验表明数据集能为训练布局生成模型提供基础，多源组成增强多样性，可生成更复杂详细场景。

Conclusion: M3DLayout可作为有价值资源用于推动文本驱动3D场景合成研究。

Abstract: In text-driven 3D scene generation, object layout serves as a crucial
intermediate representation that bridges high-level language instructions with
detailed geometric output. It not only provides a structural blueprint for
ensuring physical plausibility but also supports semantic controllability and
interactive editing. However, the learning capabilities of current 3D indoor
layout generation models are constrained by the limited scale, diversity, and
annotation quality of existing datasets. To address this, we introduce
M3DLayout, a large-scale, multi-source dataset for 3D indoor layout generation.
M3DLayout comprises 15,080 layouts and over 258k object instances, integrating
three distinct sources: real-world scans, professional CAD designs, and
procedurally generated scenes. Each layout is paired with detailed structured
text describing global scene summaries, relational placements of large
furniture, and fine-grained arrangements of smaller items. This diverse and
richly annotated resource enables models to learn complex spatial and semantic
patterns across a wide variety of indoor environments. To assess the potential
of M3DLayout, we establish a benchmark using a text-conditioned diffusion
model. Experimental results demonstrate that our dataset provides a solid
foundation for training layout generation models. Its multi-source composition
enhances diversity, notably through the Inf3DLayout subset which provides rich
small-object information, enabling the generation of more complex and detailed
scenes. We hope that M3DLayout can serve as a valuable resource for advancing
research in text-driven 3D scene synthesis.

</details>


### [824] [LUQ: Layerwise Ultra-Low Bit Quantization for Multimodal Large Language Models](https://arxiv.org/abs/2509.23729)
*Shubhang Bhatnagar,Andy Xu,Kar-Han Tan,Narendra Ahuja*

Main category: cs.CV

TL;DR: 研究多模态大语言模型超低比特量化，提出LUQ策略，评估显示可减少内存使用且性能损失小。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型部署需大量资源，后训练量化在多模态模型的超低比特量化效果待探索。

Method: 分析多模态令牌和中间层激活特征，提出LUQ策略，用多模态令牌混合进行后训练量化。

Result: 在LLaVA - 1.5和Qwen - 2.5 - VL上评估，LUQ模型比4比特模型分别减少40%和31%内存，MME基准性能损失小于10%。

Conclusion: 提出的LUQ策略可有效对多模态大语言模型进行超低比特量化，减少内存使用同时控制性能损失。

Abstract: Large Language Models (LLMs) with multimodal capabilities have revolutionized
vision-language tasks, but their deployment often requires huge memory and
computational resources. While post-training quantization (PTQ) has
successfully compressed language models to as low as 1-bit precision without
significant performance loss, its effectiveness for multimodal LLMs (MLLMs)
remains relatively unexplored. In this paper, we present the first study on
ultra-low bit (<4-bit) quantization for multimodal LLMs. Our analysis reveals
that multimodal tokens and intermediate layer activations produced by them
exhibit significantly higher statistical variance and entropy compared to text
tokens, making them less tolerant to ultra-low bit quantization. However, the
activation distributions of multimodal tokens varies significantly over
different layers, with some layers having lower entropy activation
distributions. We empirically show that such layers in these models can better
tolerate ultra-low bit quantization. Building on these insights, we propose a
novel strategy for MLLM quantization, LUQ: Layerwise Ultra-Low Bit
Quantization, which selectively applies ultra-low bit quantization to layers
that are more resilient to it. Additionally, we also show that using a mix of
multimodal tokens (image and text) for PTQ boosts VQA performance in the
ultra-low bit regime. We evaluate our method on LLaVA-1.5 and Qwen-2.5-VL
across 9 popular VQA benchmarks. The resulting LUQ models use 40% and 31% less
memory than their 4-bit counterparts, respectively, while exhibiting a
performance degradation of less than 10% on the MME benchmark.

</details>


### [825] [HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and Generation](https://arxiv.org/abs/2509.23736)
*Cong Chen,Ziyuan Huang,Cheng Zou,Muzhi Zhu,Kaixiang Ji,Jiajia Liu,Jingdong Chen,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出多尺度ViT分词器HieraTok，在图像重建和生成任务中表现出色，能提升性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 克服单尺度表示建模的固有局限，推进基于ViT的分词器在视觉生成任务中的发展。

Method: 采用多尺度下采样生成多尺度令牌序列，使用尺度因果注意力机制实现信息从低分辨率到高分辨率的流动。

Result: 在图像重建和生成任务中显著提升性能，rFID提升27.2%，集成到下游框架收敛速度快1.38倍，gFID提升18.9%，扩大训练后达到sota指标。

Conclusion: 首次引入多尺度ViT分词器，有望推动基于ViT的分词器在视觉生成任务中的发展。

Abstract: In this work, we present HieraTok, a novel multi-scale Vision Transformer
(ViT)-based tokenizer that overcomes the inherent limitation of modeling
single-scale representations. This is realized through two key designs: (1)
multi-scale downsampling applied to the token map generated by the tokenizer
encoder, producing a sequence of multi-scale tokens, and (2) a scale-causal
attention mechanism that enables the progressive flow of information from
low-resolution global semantic features to high-resolution structural details.
Coupling these designs, HieraTok achieves significant improvements in both
image reconstruction and generation tasks. Under identical settings, the
multi-scale visual tokenizer outperforms its single-scale counterpart by a
27.2\% improvement in rFID ($1.47 \rightarrow 1.07$). When integrated into
downstream generation frameworks, it achieves a $1.38\times$ faster convergence
rate and an 18.9\% boost in gFID ($16.4 \rightarrow 13.3$), which may be
attributed to the smoother and more uniformly distributed latent space.
Furthermore, by scaling up the tokenizer's training, we demonstrate its
potential by a sota rFID of 0.45 and a gFID of 1.82 among ViT tokenizers. To
the best of our knowledge, we are the first to introduce multi-scale ViT-based
tokenizer in image reconstruction and image generation. We hope our findings
and designs advance the ViT-based tokenizers in visual generation tasks.

</details>


### [826] [Poivre: Self-Refining Visual Pointing with Reinforcement Learning](https://arxiv.org/abs/2509.23746)
*Wenjie Yang,Zengfeng Huang*

Main category: cs.CV

TL;DR: 提出自完善程序Poivre解决视觉指向任务中VLMs的局限，用强化学习训练，Poivre - 7B在Point - Bench上创最佳表现并开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs在视觉指向任务上远落后于人类，因通常需单步完成任务。

Method: 提出Point, Visualize, then Refine (Poivre)自完善程序，采用强化学习训练，设计了有效的过程奖励。

Result: 训练的Poivre - 7B在Point - Bench上超越Gemini - 2.5 - Pro和Molmo - 72B等模型超3%。

Conclusion: Poivre方法有效提升了VLMs在视觉指向任务的表现，开源资源可支持未来研究。

Abstract: Visual pointing, which aims to localize a target by predicting its
coordinates on an image, has emerged as an important problem in the realm of
vision-language models (VLMs). Despite its broad applicability, recent
benchmarks show that current VLMs still fall far behind human performance on
this task. A key limitation is that VLMs are typically required to complete the
pointing task in a single step, akin to asking humans to point at an object
without seeing their own fingers. To address this issue, we propose a simple
yet effective self-refining procedure: Point, Visualize, then Refine (Poivre).
This procedure enables a VLM to first mark its estimated point, then
iteratively refine the coordinates if necessary. Inspired by advances of
reasoning models in the natural language domain, we employ reinforcement
learning (RL) to incentivize this self-refining ability. For the RL training,
we design a neat process reward that is not only empirically effective but also
grounded in appealing properties. Our trained model, Poivre-7B, sets a new
state of the art on Point-Bench, outperforming both proprietary models such as
Gemini-2.5-Pro and large open-source models such as Molmo-72B by over 3%. To
support future research, we release our training and inference code, dataset,
and the Poivre-7B checkpoint.

</details>


### [827] [PVTAdpNet: Polyp Segmentation using Pyramid vision transformer with a novel Adapter block](https://arxiv.org/abs/2509.23751)
*Arshia Yousefi Nezhad,Helia Aghaei,Hedieh Sajedi*

Main category: cs.CV

TL;DR: 本文提出PVTAdpNet模型用于结直肠癌息肉分割，性能优越且适合临床应用。


<details>
  <summary>Details</summary>
Motivation: 解决传统结肠镜检查因息肉多变导致的高漏诊率问题，实现有效早期检测。

Method: 引入PVTAdpNet模型，集成U-Net式编解码器结构、金字塔视觉Transformer主干、新型残差块和基于适配器的跳跃连接，并采用挤压激励注意力机制。

Result: 在基准数据集上有高mDice和mIoU分数，在分布外息肉数据集上Dice系数达0.8851，mIoU达0.8167，在PolypGen数据集上能实时准确运行。

Conclusion: PVTAdpNet能实现实时准确的息肉分割，适合临床应用，代码已开源。

Abstract: Colorectal cancer ranks among the most common and deadly cancers, emphasizing
the need for effective early detection and treatment. To address the
limitations of traditional colonoscopy, including high miss rates due to polyp
variability, we introduce the Pyramid Vision Transformer Adapter Residual
Network (PVTAdpNet). This model integrates a U-Net-style encoder-decoder
structure with a Pyramid Vision Transformer backbone, novel residual blocks,
and adapter-based skip connections. The design enhances feature extraction,
dense prediction, and gradient flow, supported by squeeze-and-excitation
attention for improved channel-wise feature refinement. PVTAdpNet achieves
real-time, accurate polyp segmentation, demonstrating superior performance on
benchmark datasets with high mDice and mIoU scores, making it highly suitable
for clinical applications. PVTAdpNet obtains a high Dice coefficient of 0.8851
and a mean Intersection over Union (mIoU) of 0.8167 on out-of-distribution
polyp datasets. Evaluation of the PolypGen dataset demonstrates PVTAdpNet's
capability for real-time, accurate performance within familiar distributions.
The source code of our network is available at
https://github.com/ayousefinejad/PVTAdpNet.git

</details>


### [828] [GroupCoOp: Group-robust Fine-tuning via Group Prompt Learning](https://arxiv.org/abs/2509.23781)
*Nayeong Kim,Seong Joon Oh,Suha Kwak*

Main category: cs.CV

TL;DR: 提出GroupCoOp算法提升微调视觉语言模型的组鲁棒性，在多个基准测试表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有微调视觉语言模型易受微调数据集中子组不平衡导致的虚假相关性影响。

Method: 提出GroupCoOp算法，使用特定组文本提示作为组代表充当多分类器，利用文本编码器语义知识发现有效组提示。

Result: 在五个基准测试和五种CLIP架构上取得最佳结果，仅训练0.016%网络参数时偶尔优于全网络微调方法。

Conclusion: GroupCoOp是一种简单有效的去偏微调算法，能提升微调视觉语言模型的组鲁棒性。

Abstract: Parameter-efficient fine-tuning (PEFT) of vision-language models (VLMs)
excels in various vision tasks thanks to the rich knowledge and generalization
ability of VLMs. However, recent studies revealed that such fine-tuned VLMs are
vulnerable to spurious correlations stemming from the subgroup imbalance in the
fine-tuning datasets. To resolve this issue, we propose Group Context
Optimization (GroupCoOp), a simple and effective debiased fine-tuning algorithm
that enhances the group robustness of fine-tuned VLMs. Its key idea is to
employ group-specific text prompts as group representatives serving as multiple
classifiers for their target class. The rich semantic knowledge of the text
encoder of VLM enables the discovery of effective group prompts even for groups
with a small number of training samples. Leveraging the group prompts for each
class addresses the issues caused by the group-imbalanced training set, such as
the neglect of minority groups and the scattered distribution of each class in
the embedding space. GroupCoOp achieved the best results on five benchmarks
across five CLIP architectures and occasionally outperformed prior methods that
fine-tune the entire network, despite training only 0.016\% of the network's
parameters.

</details>


### [829] [From Unstable to Playable: Stabilizing Angry Birds Levels via Object Segmentation](https://arxiv.org/abs/2509.23787)
*Mahdi Farrokhimaleki,Parsa Rahmati,Richard Zhao*

Main category: cs.CV

TL;DR: 提出识别和修复现有PCG模型生成不稳定关卡的方法，以《愤怒的小鸟》为例，实验证明可提升关卡稳定性和可玩性，该方法适用于类似2D游戏。


<details>
  <summary>Details</summary>
Motivation: PCG技术虽能高效创建内容，但保证高质量、符合行业标准的内容存在挑战，需识别和修复现有PCG模型生成的不稳定关卡。

Method: 以《愤怒的小鸟》为案例，利用关卡图像的对象分割和视觉分析检测结构漏洞并进行针对性修复，评估多个对象分割模型并选最有效者作为修复流程基础。

Result: 实验结果表明该方法提高了AI生成关卡的稳定性和可玩性。

Conclusion: 虽评估针对《愤怒的小鸟》，但基于图像的方法适用于多种具有类似关卡结构的2D游戏。

Abstract: Procedural Content Generation (PCG) techniques enable automatic creation of
diverse and complex environments. While PCG facilitates more efficient content
creation, ensuring consistently high-quality, industry-standard content remains
a significant challenge. In this research, we propose a method to identify and
repair unstable levels generated by existing PCG models. We use Angry Birds as
a case study, demonstrating our method on game levels produced by established
PCG approaches. Our method leverages object segmentation and visual analysis of
level images to detect structural gaps and perform targeted repairs. We
evaluate multiple object segmentation models and select the most effective one
as the basis for our repair pipeline. Experimental results show that our method
improves the stability and playability of AI-generated levels. Although our
evaluation is specific to Angry Birds, our image-based approach is designed to
be applicable to a wide range of 2D games with similar level structures.

</details>


### [830] [Activation Matching for Explanation Generation](https://arxiv.org/abs/2509.23051)
*Pirzada Suhail,Aditya Anand,Amit Sethi*

Main category: cs.CV

TL;DR: 提出基于激活匹配的方法为预训练分类器对图像决策生成最小、可信解释。


<details>
  <summary>Details</summary>
Motivation: 为预训练分类器对图像的决策生成最小且可信的解释。

Method: 训练轻量级自编码器输出二进制掩码，结合多层激活匹配、掩码先验和溯因约束等目标。

Result: 得到小的、人类可解释的掩码，保留分类器行为并丢弃无关输入区域。

Conclusion: 该方法能为底层模型的决策提供实用且可信的极简解释。

Abstract: In this paper we introduce an activation-matching--based approach to generate
minimal, faithful explanations for the decision-making of a pretrained
classifier on any given image. Given an input image \(x\) and a frozen model
\(f\), we train a lightweight autoencoder to output a binary mask \(m\) such
that the explanation \(e = m \odot x\) preserves both the model's prediction
and the intermediate activations of \(x\). Our objective combines: (i)
multi-layer activation matching with KL divergence to align distributions and
cross-entropy to retain the top-1 label for both the image and the explanation;
(ii) mask priors -- L1 area for minimality, a binarization penalty for crisp
0/1 masks, and total variation for compactness; and (iii) abductive constraints
for faithfulness and necessity. Together, these objectives yield small,
human-interpretable masks that retain classifier behavior while discarding
irrelevant input regions, providing practical and faithful minimalist
explanations for the decision making of the underlying model.

</details>


### [831] [FMC-DETR: Frequency-Decoupled Multi-Domain Coordination for Aerial-View Object Detection](https://arxiv.org/abs/2509.23056)
*Ben Liang,Yuan Liu,Bingwen Qiu,Yihong Wang,Xiubao Sui,Qian Chen*

Main category: cs.CV

TL;DR: 提出FMC - DETR框架用于航空视图目标检测，在基准数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有航空视图小目标检测方法存在上下文融合延迟和非线性建模不足问题，难以利用全局信息细化浅层特征，性能遇瓶颈。

Method: 提出FMC - DETR框架，包括引入WeKat骨干网络增强全局低频上下文感知，用CPF模块减少冗余并改善多尺度特征交互，用MDFC模块统一多领域先验。

Result: 在基准航空视图数据集上取得了最先进的性能，参数更少；在VisDrone数据集上，AP提升6.5%，AP50提升8.2%。

Conclusion: FMC - DETR在航空视图小目标检测中有效，代码公开。

Abstract: Aerial-view object detection is a critical technology for real-world
applications such as natural resource monitoring, traffic management, and
UAV-based search and rescue. Detecting tiny objects in high-resolution aerial
imagery presents a long-standing challenge due to their limited visual cues and
the difficulty of modeling global context in complex scenes. Existing methods
are often hampered by delayed contextual fusion and inadequate non-linear
modeling, failing to effectively use global information to refine shallow
features and thus encountering a performance bottleneck. To address these
challenges, we propose FMC-DETR, a novel framework with frequency-decoupled
fusion for aerial-view object detection. First, we introduce the Wavelet
Kolmogorov-Arnold Transformer (WeKat) backbone, which applies cascaded wavelet
transforms to enhance global low-frequency context perception in shallow
features while preserving fine-grained details, and employs Kolmogorov-Arnold
networks to achieve adaptive non-linear modeling of multi-scale dependencies.
Next, a lightweight Cross-stage Partial Fusion (CPF) module reduces redundancy
and improves multi-scale feature interaction. Finally, we introduce the
Multi-Domain Feature Coordination (MDFC) module, which unifies spatial,
frequency, and structural priors to to balance detail preservation and global
enhancement. Extensive experiments on benchmark aerial-view datasets
demonstrate that FMC-DETR achieves state-of-the-art performance with fewer
parameters. On the challenging VisDrone dataset, our model achieves
improvements of 6.5% AP and 8.2% AP50 over the baseline, highlighting its
effectiveness in tiny object detection. The code can be accessed at
https://github.com/bloomingvision/FMC-DETR.

</details>


### [832] [A Multi-Camera Vision-Based Approach for Fine-Grained Assembly Quality Control](https://arxiv.org/abs/2509.23815)
*Ali Nazeri,Shashank Mishra,Achim Wagner,Martin Ruskowski,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 本文提出多视图质量控制模块，结合多相机成像与目标检测算法，用图像融合方法提升检测可靠性，实验表明优于单视图方法，数据集公开。


<details>
  <summary>Details</summary>
Motivation: 现有质量控制解决方案依赖单视图成像或人工检查，易因遮挡、视角受限和光照不一致出错，需额外检测站，会增加成本和停机时间。

Method: 引入多视图质量控制模块，集成多相机成像系统和先进目标检测算法，用图像融合方法结合多视图结果，开发含多样场景标注图像的数据集。

Result: 该方法显著优于单视图方法，在识别小装配零件如螺丝未正确紧固时达到高精度和召回率。

Conclusion: 此工作克服单视图局限，为工业自动化提供可扩展、经济高效且准确的质量控制机制，确保装配线可靠性和安全性，数据集公开利于该领域进一步研究。

Abstract: Quality control is a critical aspect of manufacturing, particularly in
ensuring the proper assembly of small components in production lines. Existing
solutions often rely on single-view imaging or manual inspection, which are
prone to errors due to occlusions, restricted perspectives, or lighting
inconsistencies. These limitations require the installation of additional
inspection stations, which could disrupt the assembly line and lead to
increased downtime and costs. This paper introduces a novel multi-view quality
control module designed to address these challenges, integrating a multi-camera
imaging system with advanced object detection algorithms. By capturing images
from three camera views, the system provides comprehensive visual coverage of
components of an assembly process. A tailored image fusion methodology combines
results from multiple views, effectively resolving ambiguities and enhancing
detection reliability. To support this system, we developed a unique dataset
comprising annotated images across diverse scenarios, including varied lighting
conditions, occlusions, and angles, to enhance applicability in real-world
manufacturing environments. Experimental results show that our approach
significantly outperforms single-view methods, achieving high precision and
recall rates in the identification of improperly fastened small assembly parts
such as screws. This work contributes to industrial automation by overcoming
single-view limitations, and providing a scalable, cost-effective, and accurate
quality control mechanism that ensures the reliability and safety of the
assembly line. The dataset used in this study is publicly available to
facilitate further research in this domain.

</details>


### [833] [Not All Tokens are Guided Equal: Improving Guidance in Visual Autoregressive Models](https://arxiv.org/abs/2509.23876)
*Ky Dan Nguyen,Hoang Lam Tran,Anh-Dung Dinh,Daochang Liu,Weidong Cai,Xiuying Wang,Chang Xu*

Main category: cs.CV

TL;DR: 基于下一尺度预测的自回归（AR）模型在图像生成中面临跨时间步长的信息不一致问题，提出信息接地引导（IGG）机制解决该问题，在图像生成任务中取得更好效果。


<details>
  <summary>Details</summary>
Motivation: AR模型在图像生成中存在跨时间步长的信息不一致问题，导致引导信号分散、图像特征模糊不忠实。

Method: 提出信息接地引导（IGG）机制，通过注意力将引导锚定到语义重要区域，在采样时自适应增强信息丰富的补丁。

Result: 在类别条件和文本到图像生成任务中，IGG生成的图像更清晰、更连贯且语义更明确。

Conclusion: IGG为基于AR的方法设定了新的基准。

Abstract: Autoregressive (AR) models based on next-scale prediction are rapidly
emerging as a powerful tool for image generation, but they face a critical
weakness: information inconsistencies between patches across timesteps
introduced by progressive resolution scaling. These inconsistencies scatter
guidance signals, causing them to drift away from conditioning information and
leaving behind ambiguous, unfaithful features. We tackle this challenge with
Information-Grounding Guidance (IGG), a novel mechanism that anchors guidance
to semantically important regions through attention. By adaptively reinforcing
informative patches during sampling, IGG ensures that guidance and content
remain tightly aligned. Across both class-conditioned and text-to-image
generation tasks, IGG delivers sharper, more coherent, and semantically
grounded images, setting a new benchmark for AR-based methods.

</details>


### [834] [UltraUNet: Real-Time Ultrasound Tongue Segmentation for Diverse Linguistic and Imaging Conditions](https://arxiv.org/abs/2509.23225)
*Alisher Myrgyyassov,Zhen Song,Yu Sun,Bruce Xiao Wang,Min Ney Wong,Yongping Zheng*

Main category: cs.CV

TL;DR: 提出UltraUNet用于超声图像中舌轮廓实时分割，轻量级且高效准确。


<details>
  <summary>Details</summary>
Motivation: 超声舌成像实时舌轮廓分割因信噪比低、成像差异和计算需求面临挑战，需有效方法。

Method: 提出轻量级编码器 - 解码器架构UltraUNet，采用轻量级Squeeze - and - Excitation块、Group Normalization和求和式跳跃连接。

Result: 实现每秒250帧，在8个数据集上评估显示高精度和鲁棒性，单数据集和跨数据集指标良好。

Conclusion: UltraUNet为语音研究、临床诊断和语音运动障碍分析提供快速准确的解决方案。

Abstract: Ultrasound tongue imaging (UTI) is a non-invasive and cost-effective tool for
studying speech articulation, motor control, and related disorders. However,
real-time tongue contour segmentation remains challenging due to low
signal-to-noise ratios, imaging variability, and computational demands. We
propose UltraUNet, a lightweight encoder-decoder architecture optimized for
real-time segmentation of tongue contours in ultrasound images. UltraUNet
incorporates domain-specific innovations such as lightweight
Squeeze-and-Excitation blocks, Group Normalization for small-batch stability,
and summation-based skip connections to reduce memory and computational
overhead. It achieves 250 frames per second and integrates ultrasound-specific
augmentations like denoising and blur simulation. Evaluations on 8 datasets
demonstrate high accuracy and robustness, with single-dataset Dice = 0.855 and
MSD = 0.993px, and cross-dataset Dice averaging 0.734 and 0.761. UltraUNet
provides a fast, accurate solution for speech research, clinical diagnostics,
and analysis of speech motor disorders.

</details>


### [835] [PCRI: Measuring Context Robustness in Multimodal Models for Enterprise Applications](https://arxiv.org/abs/2509.23879)
*Hitesh Laxmichand Patel,Amit Agarwal,Srikant Panda,Hansa Meghwani,Karan Dua,Paul Li,Tao Sheng,Sujith Ravi,Dan Roth*

Main category: cs.CV

TL;DR: 提出Patch Context Robustness Index (PCRI)衡量多模态大语言模型对视觉上下文粒度变化的鲁棒性，分析19个模型，指出部分模型鲁棒性问题并支持模型选择和架构开发。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标未捕捉多模态大语言模型在现实场景中对无关或干扰视觉上下文的敏感性，需要新指标衡量其鲁棒性。

Method: 引入PCRI，通过比较局部图像块和全图像输入的性能变化来量化模型鲁棒性，并应用于19个模型在15个视觉语言基准测试中。

Result: 多数领先模型对背景噪声仍较脆弱，只有InternVL2 - 26B和Qwen2VL - 72B等少数模型在各任务中表现出一致鲁棒性，PCRI分析还揭示不同模型架构处理和整合视觉上下文的方式。

Conclusion: PCRI可对上下文鲁棒性进行严格比较，支持合理的模型选择，指导未来架构和训练策略开发以实现稳健的实际部署。

Abstract: The reliability of Multimodal Large Language Models (MLLMs) in real-world
settings is often undermined by sensitivity to irrelevant or distracting visual
context, an aspect not captured by existing evaluation metrics. We introduce
the \textbf{Patch Context Robustness Index (PCRI)}, the first systematic and
interpretable score for quantifying MLLM robustness to variations in visual
context granularity, measuring performance changes between localized image
patches and full-image input.
  Applying PCRI to 19 state-of-the-art MLLMs across 15 vision-language
benchmarks, we find that most leading models remain brittle to background
noise, with only a few, such as InternVL2-26B and Qwen2VL-72B, demonstrating
consistent robustness across tasks. PCRI analysis also highlights how different
model architectures handle and integrate visual context, offering actionable
diagnostic insight for both researchers and practitioners.
  PCRI enables rigorous comparison of context robustness, supporting principled
model selection and guiding the development of future architectures and
training strategies for robust, real-world deployment.

</details>


### [836] [Tunable-Generalization Diffusion Powered by Self-Supervised Contextual Sub-Data for Low-Dose CT Reconstruction](https://arxiv.org/abs/2509.23885)
*Guoquan Wei,Zekun Zhou,Liu Shi,Wenzhe Shan,Qiegen Liu*

Main category: cs.CV

TL;DR: 提出用于低剂量CT重建的SuperDiff方法，仅需LDCT投影域数据训练测试，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习低剂量CT去噪模型依赖配对数据、泛化性差，扩散模型需干净数据分布，自监督方法泛化性扩展有挑战。

Method: 设计上下文子数据相似性自适应感知策略，结合知识蒸馏和潜在扩散模型优化图像细节，用预训练模型推理重建，提出像素级自校正融合技术，采用双域策略级联。

Result: 在数据集和真实数据上的定性和定量评估表明，SuperDiff在重建和泛化性能上始终优于现有先进方法。

Conclusion: SuperDiff方法有效解决了现有低剂量CT去噪模型的问题，在重建和泛化方面表现出色。

Abstract: Current models based on deep learning for low-dose CT denoising rely heavily
on paired data and generalize poorly. Even the more concerned diffusion models
need to learn the distribution of clean data for reconstruction, which is
difficult to satisfy in medical clinical applications. At the same time,
self-supervised-based methods face the challenge of significant degradation of
generalizability of models pre-trained for the current dose to expand to other
doses. To address these issues, this paper proposes a novel method of
tunable-generalization diffusion powered by self-supervised contextual sub-data
for low-dose CT reconstruction, named SuperDiff. Firstly, a contextual subdata
similarity adaptive sensing strategy is designed for denoising centered on the
LDCT projection domain, which provides an initial prior for the subsequent
progress. Subsequently, the initial prior is used to combine knowledge
distillation with a deep combination of latent diffusion models for optimizing
image details. The pre-trained model is used for inference reconstruction, and
the pixel-level self-correcting fusion technique is proposed for fine-grained
reconstruction of the image domain to enhance the image fidelity, using the
initial prior and the LDCT image as a guide. In addition, the technique is
flexibly applied to the generalization of upper and lower doses or even unseen
doses. Dual-domain strategy cascade for self-supervised LDCT denoising,
SuperDiff requires only LDCT projection domain data for training and testing.
Full qualitative and quantitative evaluations on both datasets and real data
show that SuperDiff consistently outperforms existing state-of-the-art methods
in terms of reconstruction and generalization performance.

</details>


### [837] [Preserving Cross-Modal Stability for Visual Unlearning in Multimodal Scenarios](https://arxiv.org/abs/2509.23895)
*Jinghan Xu Yuyang Zhang Qixuan Cai Jiancheng Chen Keqiu Li*

Main category: cs.CV

TL;DR: 针对视觉模态隐私泄露问题，提出CCU框架解决现有视觉遗忘方法不足，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决现实多模态应用中视觉模态隐私泄露问题，且现有机器遗忘方法在视觉遗忘时无法保留跨模态知识和维持留存数据类内结构稳定性。

Method: 提出Cross - modal Contrastive Unlearning (CCU)框架，包含选择性视觉遗忘、跨模态知识保留和双集对比分离三个关键组件。

Result: 在三个数据集上实验，相比最高准确率基线，准确率提升7.12%，且遗忘时间仅为7%。

Conclusion: CCU框架具有优越性。

Abstract: Visual modality is the most vulnerable to privacy leakage in real-world
multimodal applications like autonomous driving with visual and radar data;
Machine unlearning removes specific training data from pre-trained models to
address privacy leakage, however, existing methods fail to preserve cross-modal
knowledge and maintain intra-class structural stability of retain data, leading
to reduced overall and other modalities' performance during visual unlearning;
to address these challenges, we propose a Cross-modal Contrastive Unlearning
(CCU) framework, which integrates three key components: (a) selective visual
unlearning: employing inverse contrastive learning to dissociate visual
representations from their original semantics, (b) cross-modal knowledge
retention: preserving other modalities' discriminability through semantic
consistency, and (c) dual-set contrastive separation: preserving the model
performance via isolation of structural perturbations between the unlearn set
and retain set; extensive experiments on three datasets demonstrate the
superiority of CCU, and our method achieves a 7.12% accuracy improvement with
only 7% of the unlearning time compared to the top-accuracy baseline.

</details>


### [838] [EWC-Guided Diffusion Replay for Exemplar-Free Continual Learning in Medical Imaging](https://arxiv.org/abs/2509.23906)
*Anoushka Harit,William Prew,Zhongtian Sun,Florian Markowetz*

Main category: cs.CV

TL;DR: 提出持续学习框架，避免存储患者样本，在多任务评估中表现良好，指出遗忘与两个因素有关，为临床影像模型提供实用适应途径。


<details>
  <summary>Details</summary>
Motivation: 医学影像基础模型需随时间适应，但全量再训练受隐私和成本限制。

Method: 结合类条件扩散重放与弹性权重整合，使用紧凑的视觉Transformer骨干网络。

Result: 在CheXpert上达到0.851 AUROC，比DER++减少超30%遗忘，接近联合训练的0.869 AUROC，且高效、保护隐私。

Conclusion: 为临床影像模型可扩展、注重隐私的持续适应提供了实用路线。

Abstract: Medical imaging foundation models must adapt over time, yet full retraining
is often blocked by privacy constraints and cost. We present a continual
learning framework that avoids storing patient exemplars by pairing class
conditional diffusion replay with Elastic Weight Consolidation. Using a compact
Vision Transformer backbone, we evaluate across eight MedMNIST v2 tasks and
CheXpert. On CheXpert our approach attains 0.851 AUROC, reduces forgetting by
more than 30\% relative to DER\texttt{++}, and approaches joint training at
0.869 AUROC, while remaining efficient and privacy preserving. Analyses connect
forgetting to two measurable factors: fidelity of replay and Fisher weighted
parameter drift, highlighting the complementary roles of replay diffusion and
synaptic stability. The results indicate a practical route for scalable,
privacy aware continual adaptation of clinical imaging models.

</details>


### [839] [3DPCNet: Pose Canonicalization for Robust Viewpoint-Invariant 3D Kinematic Analysis from Monocular RGB cameras](https://arxiv.org/abs/2509.23455)
*Tharindu Ekanayake,Constantino Álvarez Casado,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 提出3DPCNet模块将输入姿势转换为一致的以身体为中心的规范框架，在MM - Fi基准上减少误差，在TotalCapture数据集定性评估效果好。


<details>
  <summary>Details</summary>
Motivation: 单目3D姿态估计器生成以相机为中心的骨骼，产生依赖视图的运动学信号，使健康和体育科学等应用中的比较分析复杂化。

Method: 构建3DPCNet模块，其混合编码器通过门控交叉注意力机制融合图卷积网络的局部骨骼特征和Transformer的全局上下文；预测连续6D旋转并映射到SO(3)矩阵对齐姿势；在MM - Fi数据集上进行自监督训练，使用合成旋转姿势和复合损失。

Result: 在MM - Fi基准上，将平均旋转误差从20°以上降至3.4°，平均每个关节位置误差从约64mm降至47mm；在TotalCapture数据集定性评估中，视频加速度信号与地面真值IMU传感器数据视觉对应性强。

Conclusion: 3DPCNet模块能消除视角可变性，实现物理上合理的运动分析。

Abstract: Monocular 3D pose estimators produce camera-centered skeletons, creating
view-dependent kinematic signals that complicate comparative analysis in
applications such as health and sports science. We present 3DPCNet, a compact,
estimator-agnostic module that operates directly on 3D joint coordinates to
rectify any input pose into a consistent, body-centered canonical frame. Its
hybrid encoder fuses local skeletal features from a graph convolutional network
with global context from a transformer via a gated cross-attention mechanism.
From this representation, the model predicts a continuous 6D rotation that is
mapped to an $SO(3)$ matrix to align the pose. We train the model in a
self-supervised manner on the MM-Fi dataset using synthetically rotated poses,
guided by a composite loss ensuring both accurate rotation and pose
reconstruction. On the MM-Fi benchmark, 3DPCNet reduces the mean rotation error
from over 20$^{\circ}$ to 3.4$^{\circ}$ and the Mean Per Joint Position Error
from ~64 mm to 47 mm compared to a geometric baseline. Qualitative evaluations
on the TotalCapture dataset further demonstrate that our method produces
acceleration signals from video that show strong visual correspondence to
ground-truth IMU sensor data, confirming that our module removes viewpoint
variability to enable physically plausible motion analysis.

</details>


### [840] [FrameMind: Frame-Interleaved Chain-of-Thought for Video Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.24008)
*Haonan Ge,Yiwei Wang,Kai-Wei Chang,Hang Wu,Yujun Cai*

Main category: cs.CV

TL;DR: 本文提出FrameMind框架，采用强化学习训练，通过FiCOT动态请求视觉信息，还提出DRFS和DRFS - GRPO方法，实验表明显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前视频理解模型采用固定帧采样策略，缺乏自适应收集视觉证据能力，导致特定任务表现不佳。

Method: 引入FrameMind框架，通过FiCOT动态请求信息；提出DRFS和DRFS - GRPO方法训练动态采样策略。

Result: 在MLVU和VideoMME等基准测试中，方法显著优于现有模型。

Conclusion: 所提方法提升了灵活高效视频理解的技术水平。

Abstract: Current video understanding models rely on fixed frame sampling strategies,
processing predetermined visual inputs regardless of the specific reasoning
requirements of each question. This static approach limits their ability to
adaptively gather visual evidence, leading to suboptimal performance on tasks
that require either broad temporal coverage or fine-grained spatial detail. In
this paper, we introduce FrameMind, an end-to-end framework trained with
reinforcement learning that enables models to dynamically request visual
information during reasoning through Frame-Interleaved Chain-of-Thought
(FiCOT). Unlike traditional approaches, FrameMind operates in multiple turns
where the model alternates between textual reasoning and active visual
perception, using tools to extract targeted frames or video clips based on
identified knowledge gaps. To train effective dynamic sampling policies, we
propose Dynamic Resolution Frame Sampling (DRFS), which exposes models to
diverse temporal-spatial trade-offs during learning, and DRFS-GRPO, a
group-relative policy optimization algorithm that learns from outcome-based
rewards without requiring frame-level annotations. Extensive experiments on
challenging benchmarks like MLVU and VideoMME demonstrate that our method
significantly outperforms existing models, advancing the state of the art in
flexible and efficient video understanding.

</details>


### [841] [Multi-modal Data Spectrum: Multi-modal Datasets are Multi-dimensional](https://arxiv.org/abs/2509.23499)
*Divyam Madaan,Varshan Muhunthan,Kyunghyun Cho,Sumit Chopra*

Main category: cs.CV

TL;DR: 本文对23个视觉问答基准进行大规模实证研究，量化多模态依赖关系，发现基准测试在依赖视觉、问题及交互方面差异大，且一些基准放大图像依赖，为多模态基准设计和评估提供量化方法。


<details>
  <summary>Details</summary>
Motivation: 当前基准评估中多模态内和跨模态依赖关系的性质及相互作用缺乏充分表征，需深入研究以推动多模态学习。

Method: 使用多模态大语言模型对23个视觉问答基准进行大规模实证研究，覆盖多个领域。

Result: 各基准在依赖视觉、问题及其交互方面差异显著；许多旨在减轻纯文本偏差的基准反而放大了纯图像依赖；不同大小模型都存在类似特征。

Conclusion: 对多模态数据集进行了量化表征，为多模态基准设计和评估提供了原则性方法。

Abstract: Understanding the interplay between intra-modality dependencies (the
contribution of an individual modality to a target task) and inter-modality
dependencies (the relationships between modalities and the target task) is
fundamental to advancing multi-modal learning. However, the nature of and
interaction between these dependencies within current benchmark evaluations
remains poorly characterized. In this work, we present a large-scale empirical
study to quantify these dependencies across 23 visual question-answering
benchmarks using multi-modal large language models (MLLMs) covering domains
such as general and expert knowledge reasoning, optical character recognition,
and document understanding. Our findings show that the reliance on vision,
question (text), and their interaction varies significantly, both across and
within benchmarks. We discover that numerous benchmarks intended to mitigate
text-only biases have inadvertently amplified image-only dependencies. This
characterization persists across model sizes, as larger models often use these
intra-modality dependencies to achieve high performance that mask an underlying
lack of multi-modal reasoning. We provide a quantitative characterization of
multi-modal datasets, enabling a principled approach to multi-modal benchmark
design and evaluation.

</details>


### [842] [A Second-Order Perspective on Pruning at Initialization and Knowledge Transfer](https://arxiv.org/abs/2509.24066)
*Leonardo Iurada,Beatrice Occhiena,Tatiana Tommasi*

Main category: cs.CV

TL;DR: 研究数据对预训练视觉模型剪枝的影响，发现单任务剪枝能保留零样本性能，微调可提升性能。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉模型计算和存储成本高限制部署，剪枝可压缩模型，但传统剪枝需特定任务数据，下游任务未知时存在挑战。

Method: 研究数据对预训练视觉模型剪枝的影响。

Result: 单任务剪枝能保留模型在未见任务上的零样本性能，微调剪枝模型可提升原任务性能并恢复未参与训练任务的性能。

Conclusion: 这一现象归因于大规模数据集预训练带来的有利损失景观。

Abstract: The widespread availability of pre-trained vision models has enabled numerous
deep learning applications through their transferable representations. However,
their computational and storage costs often limit practical deployment.
Pruning-at-Initialization has emerged as a promising approach to compress
models before training, enabling efficient task-specific adaptation. While
conventional wisdom suggests that effective pruning requires task-specific
data, this creates a challenge when downstream tasks are unknown in advance. In
this paper, we investigate how data influences the pruning of pre-trained
vision models. Surprisingly, pruning on one task retains the model's zero-shot
performance also on unseen tasks. Furthermore, fine-tuning these pruned models
not only improves performance on original seen tasks but can recover held-out
tasks' performance. We attribute this phenomenon to the favorable loss
landscapes induced by extensive pre-training on large-scale datasets.

</details>


### [843] [Uncovering Grounding IDs: How External Cues Shape Multi-Modal Binding](https://arxiv.org/abs/2509.24072)
*Hosein Hasani,Amirmohammad Izadi,Fatemeh Askari,Mobin Bagherian,Sadegh Mohammadian,Mohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 研究大型视觉语言模型中外部提示提升多模态绑定的机制，提出Grounding IDs概念并证实其作用。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在结构化推理和精确对齐方面有限，添加视觉结构虽提升准确性，但内在机制不明。

Method: 提出Grounding IDs概念，通过表征分析和因果干预进行研究。

Result: Grounding IDs在嵌入空间中实现稳健的分区内对齐，减少模态差距，介导对象与符号提示的绑定，加强相关组件间的注意力。

Conclusion: Grounding IDs是解释外部提示增强多模态绑定的关键符号机制，兼具可解释性和鲁棒性改进。

Abstract: Large vision-language models (LVLMs) show strong performance across
multimodal benchmarks but remain limited in structured reasoning and precise
grounding. Recent work has demonstrated that adding simple visual structures,
such as partitions and annotations, improves accuracy, yet the internal
mechanisms underlying these gains remain unclear. We investigate this
phenomenon and propose the concept of Grounding IDs, latent identifiers induced
by external cues that bind objects to their designated partitions across
modalities. Through representation analysis, we find that these identifiers
emerge as robust within-partition alignment in embedding space and reduce the
modality gap between image and text. Causal interventions further confirm that
these identifiers mediate binding between objects and symbolic cues. We show
that Grounding IDs strengthen attention between related components, which in
turn improves cross-modal grounding and reduces hallucinations. Taken together,
our results identify Grounding IDs as a key symbolic mechanism explaining how
external cues enhance multimodal binding, offering both interpretability and
practical improvements in robustness.

</details>


### [844] [Confidence Aware SSD Ensemble with Weighted Boxes Fusion for Weapon Detection](https://arxiv.org/abs/2509.23697)
*Atharva Jadhav,Arush Karekar,Manas Divekar,Shachi Natu*

Main category: cs.CV

TL;DR: 本文提出用不同特征提取骨干的SSD模型集成来提升武器检测鲁棒性，在特定数据集上用WBF方法融合预测结果，采用'max'置信度评分策略的WBF实现mAP 0.838，相对最佳单模型有提升，表明置信度感知融合是提升集成精度的关键。


<details>
  <summary>Details</summary>
Motivation: 公共空间安全需要能准确检测武器的监控系统，但现有单模型检测器在部分遮挡、光照变化和杂乱背景等条件下缺乏鲁棒性。

Method: 选择VGG16、ResNet50、EfficientNet和MobileNetV3作为骨干网络训练单个SSD模型，在含枪、重型武器和刀的图像数据集上进行研究，用加权框融合（WBF）方法融合模型预测结果。

Result: 采用'max'置信度评分策略的WBF方法实现平均精度均值（mAP）为0.838，相对最佳单模型有2.948%的相对提升，且始终优于其他融合启发式方法。

Conclusion: 置信度感知融合是提升集成精度指标的关键机制，为监控应用中的实时武器检测提供了稳健方法。

Abstract: The safety and security of public spaces is of vital importance, driving the
need for sophisticated surveillance systems capable of accurately detecting
weapons, which are often hampered by issues like partial occlusion, varying
lighting, and cluttered backgrounds. While single-model detectors are advanced,
they often lack robustness in these challenging conditions. This paper presents
the hypothesis that ensemble of Single Shot Multibox Detector (SSD) models with
diverse feature extraction backbones can significantly enhance detection
robustness. To leverage diverse feature representations, individual SSD models
were trained using a selection of backbone networks: VGG16, ResNet50,
EfficientNet, and MobileNetV3. The study is conducted on a dataset consisting
of images of three distinct weapon classes: guns, heavy weapons and knives. The
predictions from these models are combined using the Weighted Boxes Fusion
(WBF) method, an ensemble technique designed to optimize bounding box accuracy.
Our key finding is that the fusion strategy is as critical as the ensemble's
diversity, a WBF approach using a 'max' confidence scoring strategy achieved a
mean Average Precision (mAP) of 0.838. This represents a 2.948% relative
improvement over the best-performing single model and consistently outperforms
other fusion heuristics. This research offers a robust approach to enhancing
real-time weapon detection capabilities in surveillance applications by
demonstrating that confidence-aware fusion is a key mechanism for improving
accuracy metrics of ensembles.

</details>


### [845] [EYE-DEX: Eye Disease Detection and EXplanation System](https://arxiv.org/abs/2509.24136)
*Youssef Sabiri,Walid Houmaidi,Amine Abouaomar*

Main category: cs.CV

TL;DR: 提出自动化框架EYE - DEX对10种视网膜疾病分类，微调VGG16达92.36%准确率，集成Grad - CAM增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 视网膜疾病诊断对预防视力丧失和减轻社会经济负担至关重要，传统人工分级耗时且主观，深度学习可自动化分析视网膜图像。

Method: 使用包含21,577张眼底图像的数据集，对VGG16、VGG19和ResNet50三种预训练CNN模型进行基准测试，集成Grad - CAM技术。

Result: 微调的VGG16达到了92.36%的全球基准测试准确率。

Conclusion: EYE - DEX框架在视网膜疾病分类上表现良好，Grad - CAM技术增强了可解释性，有助于临床医生信任和依赖AI辅助诊断。

Abstract: Retinal disease diagnosis is critical in preventing vision loss and reducing
socioeconomic burdens. Globally, over 2.2 billion people are affected by some
form of vision impairment, resulting in annual productivity losses estimated at
$411 billion. Traditional manual grading of retinal fundus images by
ophthalmologists is time-consuming and subjective. In contrast, deep learning
has revolutionized medical diagnostics by automating retinal image analysis and
achieving expert-level performance. In this study, we present EYE-DEX, an
automated framework for classifying 10 retinal conditions using the large-scale
Retinal Disease Dataset comprising 21,577 eye fundus images. We benchmark three
pre-trained Convolutional Neural Network (CNN) models--VGG16, VGG19, and
ResNet50--with our finetuned VGG16 achieving a state-of-the-art global
benchmark test accuracy of 92.36%. To enhance transparency and explainability,
we integrate the Gradient-weighted Class Activation Mapping (Grad-CAM)
technique to generate visual explanations highlighting disease-specific
regions, thereby fostering clinician trust and reliability in AI-assisted
diagnostics.

</details>


### [846] [Accelerating Cerebral Diagnostics with BrainFusion: A Comprehensive MRI Tumor Framework](https://arxiv.org/abs/2509.24149)
*Walid Houmaidi,Youssef Sabiri,Salmane El Mansour Billah,Amine Abouaomar*

Main category: cs.CV

TL;DR: 本文提出BrainFusion，结合微调CNN和YOLOv8进行脑肿瘤分析，微调VGG16模型测试准确率达99.86%，提升系统输出的可解释性和可信度。


<details>
  <summary>Details</summary>
Motivation: 早期准确分类脑肿瘤对指导治疗和改善患者预后至关重要，需要更有效的方法。

Method: 提出BrainFusion，结合微调的VGG16、ResNet50和Xception等CNN进行肿瘤分类，与YOLOv8结合进行精确肿瘤定位，利用脑肿瘤MRI数据集进行实验。

Result: 微调的VGG16模型测试准确率达99.86%，超过以往基准。

Conclusion: 该方法凸显深度学习在提供更快、更可靠诊断方面的变革潜力，有助于改善患者护理和提高生存率。

Abstract: The early and accurate classification of brain tumors is crucial for guiding
effective treatment strategies and improving patient outcomes. This study
presents BrainFusion, a significant advancement in brain tumor analysis using
magnetic resonance imaging (MRI) by combining fine-tuned convolutional neural
networks (CNNs) for tumor classification--including VGG16, ResNet50, and
Xception--with YOLOv8 for precise tumor localization with bounding boxes.
Leveraging the Brain Tumor MRI Dataset, our experiments reveal that the
fine-tuned VGG16 model achieves test accuracy of 99.86%, substantially
exceeding previous benchmarks. Beyond setting a new accuracy standard, the
integration of bounding-box localization and explainable AI techniques further
enhances both the clinical interpretability and trustworthiness of the system's
outputs. Overall, this approach underscores the transformative potential of
deep learning in delivering faster, more reliable diagnoses, ultimately
contributing to improved patient care and survival rates.

</details>


### [847] [LatXGen: Towards Radiation-Free and Accurate Quantitative Analysis of Sagittal Spinal Alignment Via Cross-Modal Radiographic View Synthesis](https://arxiv.org/abs/2509.24165)
*Moxin Zhao,Nan Meng,Jason Pui Yin Cheung,Chris Yuk Kwan Tang,Chenxi Yu,Wenting Zhong,Pengyu Lu,Chang Shi,Yipeng Zhuang,Teng Zhang*

Main category: cs.CV

TL;DR: 提出LatXGen框架，从背部RGBD图像合成侧位脊柱X光片，实现无辐射矢状位脊柱评估，实验效果佳。


<details>
  <summary>Details</summary>
Motivation: 以往无电离辐射的矢状位脊柱评估研究不足，需填补该空白。

Method: 采用双阶段架构，引入基于注意力的FFC模块和SDN网络，构建大规模配对数据集。

Result: LatXGen生成的X光片解剖学上准确，在视觉保真度和量化指标上优于现有基于GAN的方法。

Conclusion: 为矢状位脊柱评估提供无辐射解决方案，推动AIS综合评估。

Abstract: Adolescent Idiopathic Scoliosis (AIS) is a complex three-dimensional spinal
deformity, and accurate morphological assessment requires evaluating both
coronal and sagittal alignment. While previous research has made significant
progress in developing radiation-free methods for coronal plane assessment,
reliable and accurate evaluation of sagittal alignment without ionizing
radiation remains largely underexplored. To address this gap, we propose
LatXGen, a novel generative framework that synthesizes realistic lateral spinal
radiographs from posterior Red-Green-Blue and Depth (RGBD) images of unclothed
backs. This enables accurate, radiation-free estimation of sagittal spinal
alignment. LatXGen tackles two core challenges: (1) inferring sagittal spinal
morphology changes from a lateral perspective based on posteroanterior surface
geometry, and (2) performing cross-modality translation from RGBD input to the
radiographic domain. The framework adopts a dual-stage architecture that
progressively estimates lateral spinal structure and synthesizes corresponding
radiographs. To enhance anatomical consistency, we introduce an attention-based
Fast Fourier Convolution (FFC) module for integrating anatomical features from
RGBD images and 3D landmarks, and a Spatial Deformation Network (SDN) to model
morphological variations in the lateral view. Additionally, we construct the
first large-scale paired dataset for this task, comprising 3,264 RGBD and
lateral radiograph pairs. Experimental results demonstrate that LatXGen
produces anatomically accurate radiographs and outperforms existing GAN-based
methods in both visual fidelity and quantitative metrics. This study offers a
promising, radiation-free solution for sagittal spine assessment and advances
comprehensive AIS evaluation.

</details>


### [848] [Assessing Visual Privacy Risks in Multimodal AI: A Novel Taxonomy-Grounded Evaluation of Vision-Language Models](https://arxiv.org/abs/2509.23827)
*Efthymios Tsaprazlis,Tiantian Feng,Anil Ramakrishna,Rahul Gupta,Shrikanth Narayanan*

Main category: cs.CV

TL;DR: 本文探讨大语言模型隐私理解能力局限，引入视觉隐私分类法评估视觉语言模型，指出需更强大隐私感知AI系统。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽有进展但在理解和执行隐私原则方面有局限，且缺乏测试资源。

Method: 引入全面多层次的视觉隐私分类法，评估多个先进视觉语言模型。

Result: 评估发现视觉语言模型在理解上下文隐私方面存在显著不一致。

Conclusion: 研究提供基础分类法和模型局限性基准，表明急需更强大的隐私感知AI系统。

Abstract: Artificial Intelligence have profoundly transformed the technological
landscape in recent years. Large Language Models (LLMs) have demonstrated
impressive abilities in reasoning, text comprehension, contextual pattern
recognition, and integrating language with visual understanding. While these
advances offer significant benefits, they also reveal critical limitations in
the models' ability to grasp the notion of privacy. There is hence substantial
interest in determining if and how these models can understand and enforce
privacy principles, particularly given the lack of supporting resources to test
such a task. In this work, we address these challenges by examining how legal
frameworks can inform the capabilities of these emerging technologies. To this
end, we introduce a comprehensive, multi-level Visual Privacy Taxonomy that
captures a wide range of privacy issues, designed to be scalable and adaptable
to existing and future research needs. Furthermore, we evaluate the
capabilities of several state-of-the-art Vision-Language Models (VLMs),
revealing significant inconsistencies in their understanding of contextual
privacy. Our work contributes both a foundational taxonomy for future research
and a critical benchmark of current model limitations, demonstrating the urgent
need for more robust, privacy-aware AI systems.

</details>


### [849] [Talk in Pieces, See in Whole: Disentangling and Hierarchical Aggregating Representations for Language-based Object Detection](https://arxiv.org/abs/2509.24192)
*Sojung An,Kwanyong Park,Yong Jae Lee,Donghyun Kim*

Main category: cs.CV

TL;DR: 现有视觉语言模型处理复杂查询能力有限，本文提出TaSe框架，按句子层次关系重构语言表征，实验性能提升24%。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在处理含描述属性和关系从句的复杂查询时能力有限，问题源于文本编码器。

Method: 提出TaSe框架，包括构建三层合成字幕数据集、用新损失函数的组件解耦模块、按层次目标聚合组件的模块。

Result: 在OmniLabel基准测试中性能提升24%。

Conclusion: TaSe框架强化语言结构归纳偏置，证明了语言组合性的重要性。

Abstract: While vision-language models (VLMs) have made significant progress in
multimodal perception (e.g., open-vocabulary object detection) with simple
language queries, state-of-the-art VLMs still show limited ability to perceive
complex queries involving descriptive attributes and relational clauses. Our
in-depth analysis shows that these limitations mainly stem from text encoders
in VLMs. Such text encoders behave like bags-of-words and fail to separate
target objects from their descriptive attributes and relations in complex
queries, resulting in frequent false positives. To address this, we propose
restructuring linguistic representations according to the hierarchical
relations within sentences for language-based object detection. A key insight
is the necessity of disentangling textual tokens into core components-objects,
attributes, and relations ("talk in pieces")-and subsequently aggregating them
into hierarchically structured sentence-level representations ("see in whole").
Building on this principle, we introduce the TaSe framework with three main
contributions: (1) a hierarchical synthetic captioning dataset spanning three
tiers from category names to descriptive sentences; (2) Talk in Pieces, the
three-component disentanglement module guided by a novel disentanglement loss
function, transforms text embeddings into subspace compositions; and (3) See in
Whole, which learns to aggregate disentangled components into hierarchically
structured embeddings with the guide of proposed hierarchical objectives. The
proposed TaSe framework strengthens the inductive bias of hierarchical
linguistic structures, resulting in fine-grained multimodal representations for
language-based object detection. Experimental results under the OmniLabel
benchmark show a 24% performance improvement, demonstrating the importance of
linguistic compositionality.

</details>


### [850] [TREAT-Net: Tabular-Referenced Echocardiography Analysis for Acute Coronary Syndrome Treatment Prediction](https://arxiv.org/abs/2509.23999)
*Diane Kim,Minh Nguyen Nhat To,Sherif Abdalla,Teresa S. M. Tsang,Purang Abolmaesumi,and Christina Luong*

Main category: cs.CV

TL;DR: 介绍TREAT - Net用于急性冠状动脉综合征（ACS）治疗预测，训练后表现优于基线模型，有作为非侵入性分诊工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉造影诊断ACS资源密集且具侵入性，会带来风险和诊断延迟，需非侵入性方法。

Method: 引入TREAT - Net，利用超声心动图视频和结构化临床记录等非侵入性方式，结合表格引导的交叉注意力和后期融合机制。

Result: 在超9000例ACS病例数据集上训练，平衡准确率67.6%，AUROC 71.1%，跨模态协议分析干预预测准确率88.6%。

Conclusion: TREAT - Net有潜力作为非侵入性工具，用于及时准确的患者分诊，尤其适用于难以进行冠状动脉造影的人群。

Abstract: Coronary angiography remains the gold standard for diagnosing Acute Coronary
Syndrome (ACS). However, its resource-intensive and invasive nature can expose
patients to procedural risks and diagnostic delays, leading to postponed
treatment initiation. In this work, we introduce TREAT-Net, a multimodal deep
learning framework for ACS treatment prediction that leverages non-invasive
modalities, including echocardiography videos and structured clinical records.
TREAT-Net integrates tabular-guided cross-attention to enhance video
interpretation, along with a late fusion mechanism to align predictions across
modalities. Trained on a dataset of over 9000 ACS cases, the model outperforms
unimodal and non-fused baselines, achieving a balanced accuracy of 67.6% and an
AUROC of 71.1%. Cross-modality agreement analysis demonstrates 88.6% accuracy
for intervention prediction. These findings highlight the potential of
TREAT-Net as a non-invasive tool for timely and accurate patient triage,
particularly in underserved populations with limited access to coronary
angiography.

</details>


### [851] [BALR-SAM: Boundary-Aware Low-Rank Adaptation of SAM for Resource-Efficient Medical Image Segmentation](https://arxiv.org/abs/2509.24204)
*Zelin Liu,Sicheng Dong,Bocheng Li,Yixuan Yang,Jiacheng Ruan,Chenxu Zhou,Suncheng Xiang*

Main category: cs.CV

TL;DR: 提出BALR - SAM框架增强SAM用于医学成像，在医学分割数据集上表现出色，更新少量参数即可超越SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉基础模型如SAM在医学图像分割中因缺乏特定领域适应而表现不佳，高效微调以用于医学下游任务且资源需求少、性能强具有挑战性。

Method: 提出BALR - SAM框架，包含互补细节增强网络（CDEN）、集成到SAM的Vision Transformer块的低秩适配器、掩码解码器中的低秩张量注意力机制。

Result: 在标准医学分割数据集上，BALR - SAM不依赖提示，更新仅1.8%（1170万）的参数，超越了包括全微调的MedSAM在内的多个SOTA方法。

Conclusion: BALR - SAM框架能有效增强SAM在医学成像中的性能，以少量参数更新实现良好效果。

Abstract: Vision foundation models like the Segment Anything Model (SAM), pretrained on
large-scale natural image datasets, often struggle in medical image
segmentation due to a lack of domain-specific adaptation. In clinical practice,
fine-tuning such models efficiently for medical downstream tasks with minimal
resource demands, while maintaining strong performance, is challenging. To
address these issues, we propose BALR-SAM, a boundary-aware low-rank adaptation
framework that enhances SAM for medical imaging. It combines three tailored
components: (1) a Complementary Detail Enhancement Network (CDEN) using
depthwise separable convolutions and multi-scale fusion to capture
boundary-sensitive features essential for accurate segmentation; (2) low-rank
adapters integrated into SAM's Vision Transformer blocks to optimize feature
representation and attention for medical contexts, while simultaneously
significantly reducing the parameter space; and (3) a low-rank tensor attention
mechanism in the mask decoder, cutting memory usage by 75% and boosting
inference speed. Experiments on standard medical segmentation datasets show
that BALR-SAM, without requiring prompts, outperforms several state-of-the-art
(SOTA) methods, including fully fine-tuned MedSAM, while updating just 1.8%
(11.7M) of its parameters.

</details>


### [852] [Cycle Diffusion Model for Counterfactual Image Generation](https://arxiv.org/abs/2509.24267)
*Fangrui Huang,Alan Wang,Binxu Li,Bailey Trang,Ridvan Yesiloglu,Tianyu Hua,Wei Peng,Ehsan Adeli*

Main category: cs.CV

TL;DR: 提出Cycle Diffusion Model (CDM)框架微调扩散模型用于医学图像合成，实验表明其提升了条件准确性和图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决深度生成模型在医学图像合成中保证条件忠实性和生成高质量合成图像的挑战。

Method: 引入循环训练框架Cycle Diffusion Model (CDM)，通过加入循环约束确保生成图像与原始图像的一致性。

Result: 在3D大脑MRI数据集上实验显示，该方法提升了条件准确性，FID和SSIM指标表明图像质量得到增强。

Conclusion: CDM中的循环策略是改进基于扩散的医学图像生成的有效方法，可用于数据增强、反事实和疾病进展建模。

Abstract: Deep generative models have demonstrated remarkable success in medical image
synthesis. However, ensuring conditioning faithfulness and high-quality
synthetic images for direct or counterfactual generation remains a challenge.
In this work, we introduce a cycle training framework to fine-tune diffusion
models for improved conditioning adherence and enhanced synthetic image
realism. Our approach, Cycle Diffusion Model (CDM), enforces consistency
between generated and original images by incorporating cycle constraints,
enabling more reliable direct and counterfactual generation. Experiments on a
combined 3D brain MRI dataset (from ABCD, HCP aging & young adults, ADNI, and
PPMI) show that our method improves conditioning accuracy and enhances image
quality as measured by FID and SSIM. The results suggest that the cycle
strategy used in CDM can be an effective method for refining diffusion-based
medical image generation, with applications in data augmentation,
counterfactual, and disease progression modeling.

</details>


### [853] [Dynamic Orchestration of Multi-Agent System for Real-World Multi-Image Agricultural VQA](https://arxiv.org/abs/2509.24350)
*Yan Ke,Xin Yu,Heming Du,Scott Chapman,Helen Huang*

Main category: cs.CV

TL;DR: 本文提出自反思和自我改进多智能体框架解决农业视觉问答问题，实验显示该框架在多图像农业问答上有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有农业视觉问答方法难以应对多图像输入、证据不完整及缺乏质量控制等问题。

Method: 提出集成检索器、反思器、回答器和改进器四个角色的多智能体框架，各角色协作实现上下文丰富、反思推理、答案起草和迭代改进。

Result: 在AgMMU基准测试中，框架在多图像农业问答上取得有竞争力的性能。

Conclusion: 所提出的多智能体框架能有效解决现有农业视觉问答方法的不足，适用于多图像农业问答场景。

Abstract: Agricultural visual question answering is essential for providing farmers and
researchers with accurate and timely knowledge. However, many existing
approaches are predominantly developed for evidence-constrained settings such
as text-only queries or single-image cases. This design prevents them from
coping with real-world agricultural scenarios that often require multi-image
inputs with complementary views across spatial scales, and growth stages.
Moreover, limited access to up-to-date external agricultural context makes
these systems struggle to adapt when evidence is incomplete. In addition, rigid
pipelines often lack systematic quality control. To address this gap, we
propose a self-reflective and self-improving multi-agent framework that
integrates four roles, the Retriever, the Reflector, the Answerer, and the
Improver. They collaborate to enable context enrichment, reflective reasoning,
answer drafting, and iterative improvement.
  A Retriever formulates queries and gathers external information, while a
Reflector assesses adequacy and triggers sequential reformulation and renewed
retrieval. Two Answerers draft candidate responses in parallel to reduce bias.
The Improver refines them through iterative checks while ensuring that
information from multiple images is effectively aligned and utilized.
Experiments on the AgMMU benchmark show that our framework achieves competitive
performance on multi-image agricultural QA.

</details>


### [854] [An Enhanced Pyramid Feature Network Based on Long-Range Dependencies for Multi-Organ Medical Image Segmentation](https://arxiv.org/abs/2509.24358)
*Dayu Tan,Cheng Kong,Yansen Su,Hai Chen,Dongliang Yang,Junfeng Xia,Chunhou Zheng*

Main category: cs.CV

TL;DR: 提出用于多器官细粒度分割的LamFormer网络，在多个数据集上表现出色并平衡性能与复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有多器官医学图像分割方法使用Transformers时忽略高计算成本和局部细节提取不足问题。

Method: 提出U形网络LamFormer，用Linear Attention Mamba捕获多尺度长距离依赖，构建PHFA模块聚合特征，设计RT增强局部信息提取和长距离依赖捕获能力。

Result: LamFormer在七个复杂多样数据集上优于现有分割方法。

Conclusion: LamFormer能实现模型性能和复杂度的平衡。

Abstract: In the field of multi-organ medical image segmentation, recent methods
frequently employ Transformers to capture long-range dependencies from image
features. However, these methods overlook the high computational cost of
Transformers and their deficiencies in extracting local detailed information.
To address high computational costs and inadequate local detail information, we
reassess the design of feature extraction modules and propose a new
deep-learning network called LamFormer for fine-grained segmentation tasks
across multiple organs. LamFormer is a novel U-shaped network that employs
Linear Attention Mamba (LAM) in an enhanced pyramid encoder to capture
multi-scale long-range dependencies. We construct the Parallel Hierarchical
Feature Aggregation (PHFA) module to aggregate features from different layers
of the encoder, narrowing the semantic gap among features while filtering
information. Finally, we design the Reduced Transformer (RT), which utilizes a
distinct computational approach to globally model up-sampled features. RRT
enhances the extraction of detailed local information and improves the
network's capability to capture long-range dependencies. LamFormer outperforms
existing segmentation methods on seven complex and diverse datasets,
demonstrating exceptional performance. Moreover, the proposed network achieves
a balance between model performance and model complexity.

</details>


### [855] [UI-UG: A Unified MLLM for UI Understanding and Generation](https://arxiv.org/abs/2509.24361)
*Hao Yang,Weijie Qiu,Ru Zhang,Zhou Fang,Ruichao Mao,Xiaoyu Lin,Maji Huang,Zhaosong Huang,Teng Guo,Shuoyang Liu,Hai Rao*

Main category: cs.CV

TL;DR: 本文提出UI - UG模型解决MLLMs在UI任务的挑战，实验取得SOTA表现且成本低。


<details>
  <summary>Details</summary>
Motivation: 解决MLLMs在UI理解准确性和UI生成质量等特定领域任务的挑战。

Method: 理解任务用SFT结合GRPO；生成任务用DPO；提出有效工作流。

Result: 模型在理解任务达SOTA，生成性能与大模型相当但成本低。

Conclusion: 集成理解和生成任务可提高两者的准确性和质量。

Abstract: Although Multimodal Large Language Models (MLLMs) have been widely applied
across domains, they are still facing challenges in domain-specific tasks, such
as User Interface (UI) understanding accuracy and UI generation quality. In
this paper, we introduce UI-UG (a unified MLLM for UI Understanding and
Generation), integrating both capabilities. For understanding tasks, we employ
Supervised Fine-tuning (SFT) combined with Group Relative Policy Optimization
(GRPO) to enhance fine-grained understanding on the modern complex UI data. For
generation tasks, we further use Direct Preference Optimization (DPO) to make
our model generate human-preferred UIs. In addition, we propose an industrially
effective workflow, including the design of an LLM-friendly domain-specific
language (DSL), training strategies, rendering processes, and evaluation
metrics. In experiments, our model achieves state-of-the-art (SOTA) performance
on understanding tasks, outperforming both larger general-purpose MLLMs and
similarly-sized UI-specialized models. Our model is also on par with these
larger MLLMs in UI generation performance at a fraction of the computational
cost. We also demonstrate that integrating understanding and generation tasks
can improve accuracy and quality for both tasks.

</details>


### [856] [Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models](https://arxiv.org/abs/2509.24365)
*Jitai Hao,Hao Liu,Xinyan Xiao,Qiang Huang,Jun Yu*

Main category: cs.CV

TL;DR: 指出共享自回归变换器构建的统一多模态模型存在梯度冲突问题，提出Uni - X架构解决该问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 共享自回归变换器构建的统一多模态模型在多模态输入训练时，模态共享变换器存在严重的视觉和文本梯度冲突。

Method: 提出Uni - X架构，采用两端分离、中间共享的设计，两端进行特定模态处理，中间层进行高层语义融合。

Result: 在相同训练条件下，训练效率优于强基线；扩展到30亿参数时，性能匹配或超越70亿参数基于自回归的统一多模态模型，图像生成GenEval得分82，文本和视觉理解任务表现出色。

Conclusion: Uni - X是未来统一多模态建模的参数高效且可扩展的基础。

Abstract: Unified Multimodal Models (UMMs) built on shared autoregressive (AR)
transformers are attractive for their architectural simplicity. However, we
identify a critical limitation: when trained on multimodal inputs,
modality-shared transformers suffer from severe gradient conflicts between
vision and text, particularly in shallow and deep layers. We trace this issue
to the fundamentally different low-level statistical properties of images and
text, while noting that conflicts diminish in middle layers where
representations become more abstract and semantically aligned. To overcome this
challenge, we propose Uni-X, a two-end-separated, middle-shared architecture.
Uni-X dedicates its initial and final layers to modality-specific processing,
while maintaining shared parameters in the middle layers for high-level
semantic fusion. This X-shaped design not only eliminates gradient conflicts at
both ends but also further alleviates residual conflicts in the shared layers.
Extensive experiments validate the effectiveness of Uni-X. Under identical
training conditions, Uni-X achieves superior training efficiency compared to
strong baselines. When scaled to 3B parameters with larger training data, Uni-X
matches or surpasses 7B AR-based UMMs, achieving a GenEval score of 82 for
image generation alongside strong performance in text and vision understanding
tasks. These results establish Uni-X as a parameter-efficient and scalable
foundation for future unified multimodal modeling. Our code is available at
https://github.com/CURRENTF/Uni-X

</details>


### [857] [Skeleton-based Robust Registration Framework for Corrupted 3D Point Clouds](https://arxiv.org/abs/2509.24273)
*Yongqiang Wang,Weigang Li,Wenping Liu,Zhiqiang Tian,Jinling Li*

Main category: cs.CV

TL;DR: 提出基于骨架的鲁棒点云配准框架SRRF，实验表明其在处理含噪声点云时优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实点云受传感器、环境噪声和预处理误差影响，现有配准方法易受干扰，导致配准精度下降。

Method: 提出基于骨架的鲁棒配准框架，引入抗干扰的骨架表示，将骨架结构融入配准过程，结合点云与骨架的变换，设计分布距离损失函数。

Result: 在不同的含噪声数据集上的实验表明，SRRF在各种噪声场景下始终优于现有配准方法。

Conclusion: SRRF在处理含噪声点云时具有鲁棒性，是现实场景中3D感知任务的潜在方法。

Abstract: Point cloud registration is fundamental in 3D vision applications, including
autonomous driving, robotics, and medical imaging, where precise alignment of
multiple point clouds is essential for accurate environment reconstruction.
However, real-world point clouds are often affected by sensor limitations,
environmental noise, and preprocessing errors, making registration challenging
due to density distortions, noise contamination, and geometric deformations.
Existing registration methods rely on direct point matching or surface feature
extraction, which are highly susceptible to these corruptions and lead to
reduced alignment accuracy. To address these challenges, a skeleton-based
robust registration framework is presented, which introduces a
corruption-resilient skeletal representation to improve registration robustness
and accuracy. The framework integrates skeletal structures into the
registration process and combines the transformations obtained from both the
corrupted point cloud alignment and its skeleton alignment to achieve optimal
registration. In addition, a distribution distance loss function is designed to
enforce the consistency between the source and target skeletons, which
significantly improves the registration performance. This framework ensures
that the alignment considers both the original local geometric features and the
global stability of the skeleton structure, resulting in robust and accurate
registration results. Experimental evaluations on diverse corrupted datasets
demonstrate that SRRF consistently outperforms state-of-the-art registration
methods across various corruption scenarios, including density distortions,
noise contamination, and geometric deformations. The results confirm the
robustness of SRRF in handling corrupted point clouds, making it a potential
approach for 3D perception tasks in real-world scenarios.

</details>


### [858] [From Satellite to Street: A Hybrid Framework Integrating Stable Diffusion and PanoGAN for Consistent Cross-View Synthesis](https://arxiv.org/abs/2509.24369)
*Khawlah Bajbaa,Abbas Anwar,Muhammad Saqib,Hafeez Anwar,Nabin Sharma,Muhammad Usman*

Main category: cs.CV

TL;DR: 本文提出混合框架，结合扩散模型和条件生成对抗网络，从卫星图像生成地理一致的街景图像，在CVUSA数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 街景图像是地理空间数据收集和城市分析的重要来源，但从卫星图像合成街景图像存在挑战，因此提出解决方案。

Method: 采用多阶段训练策略，以Stable Diffusion为核心组件，结合条件生成对抗网络，还实施融合策略。

Result: 在CVUSA数据集上，混合方法在多个评估指标上优于仅使用扩散的方法，与最先进的基于GAN的方法相比有竞争力，能生成逼真且几何一致的街景图像并保留细节。

Conclusion: 提出的混合框架有效，能从卫星图像生成高质量、地理一致的街景图像。

Abstract: Street view imagery has become an essential source for geospatial data
collection and urban analytics, enabling the extraction of valuable insights
that support informed decision-making. However, synthesizing street-view images
from corresponding satellite imagery presents significant challenges due to
substantial differences in appearance and viewing perspective between these two
domains. This paper presents a hybrid framework that integrates diffusion-based
models and conditional generative adversarial networks to generate
geographically consistent street-view images from satellite imagery. Our
approach uses a multi-stage training strategy that incorporates Stable
Diffusion as the core component within a dual-branch architecture. To enhance
the framework's capabilities, we integrate a conditional Generative Adversarial
Network (GAN) that enables the generation of geographically consistent
panoramic street views. Furthermore, we implement a fusion strategy that
leverages the strengths of both models to create robust representations,
thereby improving the geometric consistency and visual quality of the generated
street-view images. The proposed framework is evaluated on the challenging
Cross-View USA (CVUSA) dataset, a standard benchmark for cross-view image
synthesis. Experimental results demonstrate that our hybrid approach
outperforms diffusion-only methods across multiple evaluation metrics and
achieves competitive performance compared to state-of-the-art GAN-based
methods. The framework successfully generates realistic and geometrically
consistent street-view images while preserving fine-grained local details,
including street markings, secondary roads, and atmospheric elements such as
clouds.

</details>


### [859] [REALIGN: Regularized Procedure Alignment with Matching Video Embeddings via Partial Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2509.24382)
*Soumyadeep Chandra,Kaushik Roy*

Main category: cs.CV

TL;DR: 介绍REALIGN自监督框架用于程序学习，在多个基准测试中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实教学视频存在背景片段、重复动作和步骤顺序混乱问题，现有方法依赖特征相似性，无法捕捉高阶时间结构。

Method: 基于正则化融合部分Gromov - Wasserstein最优传输（R - FPGWOT），结合FPGWOT距离和序列间对比学习。

Result: 在多个基准测试中，平均F1分数提升达18.9%，时间IoU增益超30%，生成更具解释性的传输映射。

Conclusion: REALIGN能有效处理教学视频中的各种复杂情况，提升程序学习效果。

Abstract: Learning from procedural videos remains a core challenge in self-supervised
representation learning, as real-world instructional data often contains
background segments, repeated actions, and steps presented out of order. Such
variability violates the strong monotonicity assumptions underlying many
alignment methods. Prior state-of-the-art approaches, such as OPEL, leverage
Kantorovich Optimal Transport (KOT) to build frame-to-frame correspondences,
but rely solely on feature similarity and fail to capture the higher-order
temporal structure of a task. In this paper, we introduce REALIGN, a
self-supervised framework for procedure learning based on Regularized Fused
Partial Gromov-Wasserstein Optimal Transport (R-FPGWOT). In contrast to KOT,
our formulation jointly models visual correspondences and temporal relations
under a partial alignment scheme, enabling robust handling of irrelevant
frames, repeated actions, and non-monotonic step orders common in instructional
videos. To stabilize training, we integrate FPGWOT distances with
inter-sequence contrastive learning, avoiding the need for multiple
regularizers and preventing collapse to degenerate solutions. Across egocentric
(EgoProceL) and third-person (ProceL, CrossTask) benchmarks, REALIGN achieves
up to 18.9% average F1-score improvements and over 30% temporal IoU gains,
while producing more interpretable transport maps that preserve key-step
orderings and filter out noise.

</details>


### [860] [Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy](https://arxiv.org/abs/2509.24385)
*Haijier Chen,Bo Xu,Shoujian Zhang,Haoze Liu,Jiaxuan Lin,Jingrong Wang*

Main category: cs.CV

TL;DR: 提出视频基3D - MLLM Vid - LLM，不依赖3D数据，在多任务表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有3D - MLLM依赖3D数据输入，限制可扩展性和泛化性，需拓展MLLM能力到3D场景理解。

Method: 用几何先验提升场景感知性能；设计CTA模块整合几何线索；引入度量深度模型确保几何一致性；采用两阶段蒸馏优化策略微调模型。

Result: 在多基准测试中验证了方法在3D问答、3D密集字幕和3D视觉定位任务中的有效性。

Conclusion: Vid - LLM具有优越的多任务能力，适合实际部署。

Abstract: Recent developments in Multimodal Large Language Models (MLLMs) have
significantly improved Vision-Language (VL) reasoning in 2D domains. However,
extending these capabilities to 3D scene understanding remains a major
challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often depend
on 3D data inputs, which limits scalability and generalization. To address this
limitation, we propose Vid-LLM, a video-based 3D-MLLM that directly processes
video inputs without requiring external 3D data, making it practical for
real-world deployment. In our method, the geometric prior are directly used to
improve the performance of the sceen perception. To integrate the geometric
cues into the MLLM compactly, we design a Cross-Task Adapter (CTA) module to
align the 3D geometric priors with the vision-language representations. To
ensure geometric consistency and integrity, we introduce a Metric Depth Model
that recovers real-scale geometry from the reconstruction outputs. Finally, the
model is fine-tuned with a two-stage distillation optimization strategy,
realizing fast convergence and stabilizes training. Extensive experiments
across diverse benchmarks verified the effectiveness of our method on 3D
Question Answering, 3D Dense Captioning and 3D Visual Grounding tasks,
demonstrating the superior multi-task capabilities.

</details>


### [861] [Hyperspherical Latents Improve Continuous-Token Autoregressive Generation](https://arxiv.org/abs/2509.24335)
*Guolin Ke,Hui Xue*

Main category: cs.CV

TL;DR: 提出SphereAR解决自回归图像生成中连续令牌AR模型的问题，在ImageNet生成上表现优异。


<details>
  <summary>Details</summary>
Motivation: 连续令牌AR图像生成模型常落后于潜在扩散和掩码生成模型，核心问题是VAE潜在变量的异质方差在AR解码中被放大，可能导致方差崩溃。

Method: 提出SphereAR，利用超球VAE将所有AR输入和输出（包括分类器自由引导后）约束在固定半径的超球面上。

Result: 在ImageNet生成上，SphereAR - H（943M）为AR模型创造了新的最优结果，FID为1.34；SphereAR - L（479M）FID达1.54，SphereAR - B（208M）达1.92，匹配或超越更大的基线模型。

Conclusion: 这是首次具有光栅顺序的纯下一个令牌AR图像生成器在可比参数规模上超越扩散和掩码生成模型。

Abstract: Autoregressive (AR) models are promising for image generation, yet
continuous-token AR variants often trail latent diffusion and masked-generation
models. The core issue is heterogeneous variance in VAE latents, which is
amplified during AR decoding, especially under classifier-free guidance (CFG),
and can cause variance collapse. We propose SphereAR to address this issue. Its
core design is to constrain all AR inputs and outputs -- including after CFG --
to lie on a fixed-radius hypersphere (constant $\ell_2$ norm), leveraging
hyperspherical VAEs. Our theoretical analysis shows that hyperspherical
constraint removes the scale component (the primary cause of variance
collapse), thereby stabilizing AR decoding. Empirically, on ImageNet
generation, SphereAR-H (943M) sets a new state of the art for AR models,
achieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54
and SphereAR-B (208M) reaches 1.92, matching or surpassing much larger
baselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge,
this is the first time a pure next-token AR image generator with raster order
surpasses diffusion and masked-generation models at comparable parameter
scales.

</details>


### [862] [DRIFT: Divergent Response in Filtered Transformations for Robust Adversarial Defense](https://arxiv.org/abs/2509.24359)
*Amira Guesmi,Muhammad Shafique*

Main category: cs.CV

TL;DR: 文章指出梯度共识是对抗样本可迁移性的关键驱动因素，提出DRIFT方法破坏梯度共识，在ImageNet上取得显著鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络易受对抗样本攻击，多数防御方法在梯度可可靠估计时失效的问题。

Method: 引入DRIFT，训练可学习滤波器的随机集合破坏梯度共识，采用结合多种分离和鲁棒性的共识 - 发散训练策略。

Result: DRIFT在ImageNet上跨CNN和Vision Transformers取得显著鲁棒性提升，在多种攻击下优于现有防御方法，且运行和内存成本可忽略不计。

Conclusion: 梯度发散是对抗防御实用且可推广的原则。

Abstract: Deep neural networks remain highly vulnerable to adversarial examples, and
most defenses collapse once gradients can be reliably estimated. We identify
\emph{gradient consensus} -- the tendency of randomized transformations to
yield aligned gradients -- as a key driver of adversarial transferability.
Attackers exploit this consensus to construct perturbations that remain
effective across transformations. We introduce \textbf{DRIFT} (Divergent
Response in Filtered Transformations), a stochastic ensemble of lightweight,
learnable filters trained to actively disrupt gradient consensus. Unlike prior
randomized defenses that rely on gradient masking, DRIFT enforces
\emph{gradient dissonance} by maximizing divergence in Jacobian- and
logit-space responses while preserving natural predictions. Our contributions
are threefold: (i) we formalize gradient consensus and provide a theoretical
analysis linking consensus to transferability; (ii) we propose a
consensus-divergence training strategy combining prediction consistency,
Jacobian separation, logit-space separation, and adversarial robustness; and
(iii) we show that DRIFT achieves substantial robustness gains on ImageNet
across CNNs and Vision Transformers, outperforming state-of-the-art
preprocessing, adversarial training, and diffusion-based defenses under
adaptive white-box, transfer-based, and gradient-free attacks. DRIFT delivers
these improvements with negligible runtime and memory cost, establishing
gradient divergence as a practical and generalizable principle for adversarial
defense.

</details>


### [863] [CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers](https://arxiv.org/abs/2509.24416)
*Kai Liu,Shaoqiu Zhang,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出CLQ方法用于扩散变压器模型后训练量化，减少内存消耗和加速推理，压缩模型为W4A4且视觉质量和指标下降可忽略，实现内存节省和加速。


<details>
  <summary>Details</summary>
Motivation: 扩散变压器模型规模和复杂度提升视觉生成质量，但阻碍其在边缘设备部署，传统模型后训练量化有性能下降问题。

Method: 提出CLQ方法，包含交叉块校准（CBC）获取准确校准数据、基于正交的平滑（OBS）量化通道异常值并平滑、跨层参数搜索（CLPS）。

Result: 用图像和视频生成模型评估CLQ，成功将模型压缩为W4A4，视觉质量和指标下降可忽略，实现3.98倍内存节省和3.95倍加速。

Conclusion: CLQ是一种有效的扩散变压器模型后训练量化方法，能在减少内存和加速推理的同时保持模型性能。

Abstract: Visual generation quality has been greatly promoted with the rapid advances
in diffusion transformers (DiTs), which is attributed to the scaling of model
size and complexity. However, these attributions also hinder the practical
deployment of DiTs on edge devices, limiting their development and application.
Serve as an efficient model compression technique, model post-training
quantization (PTQ) can reduce the memory consumption and speed up the
inference, with inevitable performance degradation. To alleviate the
degradation, we propose CLQ, a cross-layer guided orthogonal-based quantization
method for DiTs. To be specific, CLQ consists of three key designs. First, we
observe that the calibration data used by most of the PTQ methods can not
honestly represent the distribution of the activations. Therefore, we propose
cross-block calibration (CBC) to obtain accurate calibration data, with which
the quantization can be better guided. Second, we propose orthogonal-based
smoothing (OBS), which quantifies the outlier score of each channel and
leverages block Hadamard matrix to smooth the outliers with negligible
overhead. Third, we propose cross-layer parameter searching (CLPS) to search.
We evaluate CLQ with both image generation and video generation models and
successfully compress the model into W4A4 with negligible degradation in visual
quality and metrics. CLQ achieves 3.98x memory saving and 3.95x speedup. Our
code is available at
\hyperlink{https://github.com/Kai-Liu001/CLQ}{https://github.com/Kai-Liu001/CLQ}.

</details>


### [864] [A Data-Centric Perspective on the Influence of Image Data Quality in Machine Learning Models](https://arxiv.org/abs/2509.24420)
*Pei-Han Chen,Szu-Chi Chung*

Main category: cs.CV

TL;DR: 传统机器学习重模型轻数据，本文研究图像数据集质量评估方法，用CIFAKE数据集识别问题并开发集成工具的管道，实验表明不同质量问题影响不同，提出的改进方法提升了F1分数。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习研究重模型轻数据，随着模型架构成熟，数据质量成为关键，但图像领域数据集质量的系统研究有限，因此开展研究。

Method: 使用CIFAKE数据集识别常见质量问题并量化影响，开发集成CleanVision和Fastdup的管道，引入自动阈值选择等增强方法，将低质量图像检测作为二分类任务并用F1分数评估。

Result: 不同质量问题影响不同，卷积神经网络对部分失真有韧性，对模糊和严重降采样等敏感；自动阈值法和去重策略提升了F1分数。

Conclusion: 提出的工作流程有效，为基于图像的机器学习的数据质量评估提供了基础。

Abstract: In machine learning, research has traditionally focused on model development,
with relatively less attention paid to training data. As model architectures
have matured and marginal gains from further refinements diminish, data quality
has emerged as a critical factor. However, systematic studies on evaluating and
ensuring dataset quality in the image domain remain limited.
  This study investigates methods for systematically assessing image dataset
quality and examines how various image quality factors influence model
performance. Using the publicly available and relatively clean CIFAKE dataset,
we identify common quality issues and quantify their impact on training.
Building on these findings, we develop a pipeline that integrates two
community-developed tools, CleanVision and Fastdup. We analyze their underlying
mechanisms and introduce several enhancements, including automatic threshold
selection to detect problematic images without manual tuning.
  Experimental results demonstrate that not all quality issues exert the same
level of impact. While convolutional neural networks show resilience to certain
distortions, they are particularly vulnerable to degradations that obscure
critical visual features, such as blurring and severe downscaling. To assess
the performance of existing tools and the effectiveness of our proposed
enhancements, we formulate the detection of low-quality images as a binary
classification task and use the F1 score as the evaluation metric. Our
automatic thresholding method improves the F1 score from 0.6794 to 0.9468 under
single perturbations and from 0.7447 to 0.8557 under dual perturbations. For
near-duplicate detection, our deduplication strategy increases the F1 score
from 0.4576 to 0.7928. These results underscore the effectiveness of our
workflow and provide a foundation for advancing data quality assessment in
image-based machine learning.

</details>


### [865] [Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks](https://arxiv.org/abs/2509.24473)
*Shijie Lian,Changti Wu,Laurence Tianruo Yang,Hang Yuan,Bin Yu,Lei Zhang,Kai Chen*

Main category: cs.CV

TL;DR: 本文提出将欧几里得几何问题解决作为代理任务，构建Euclid30K数据集，用GRPO微调模型，在多个空间推理基准测试中取得零样本增益。


<details>
  <summary>Details</summary>
Motivation: 空间智能是多模态大语言模型（MLLMs）尚未解决的关键挑战，为填补这一空白开展研究。

Method: 构建约30K个平面和立体几何问题的Euclid30K数据集，用GRPO微调Qwen2.5VL和RoboBrain2.0系列模型。

Result: 模型在四个空间推理基准测试中取得零样本增益，训练后模型在VSI - Bench上平均准确率从34.5%提升到40.5%，RoboBrain2.0 - Euclid - 7B超越之前的SOTA模型。

Conclusion: 以几何为中心的微调能赋予视觉语言模型广泛可迁移的空间技能。

Abstract: Spatial intelligence spans a rich suite of abilities, including visualising
and transforming shapes, mentally rotating objects, judging relational
positions and containment, and estimating numerosity. However, it still remains
a critical unresolved challenge for Multimodal Large Language Models (MLLMs).To
fill this gap, we propose to treat Euclidean geometry problem-solving as a
surrogate task. Specifically, we meticulously constructed a curated multimodal
dataset, called Euclid30K, comprising approximately 30K plane and solid
geometry problems. To enable the model to acquire and apply Euclidean
principles from these geometry problems, we employed Group Relative Policy
Optimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family,
inspiring the models to identify shapes, count, and relate entities, and
perform multi-step deductive reasoning using Euclidean principles. Our
experiments demonstrate that the resulting models achieve substantial zero-shot
gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench,
VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after
training on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models
rose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them,
RoboBrain2.0-Euclid-7B achieves 49.6\% accuracy, surpassing the previous
state-of-the-art model, Spatial-MLLM.To our knowledge, this is the first
systematic study showing that geometry-centric fine-tuning can confer
vision-language models with broadly transferable spatial skills. Code and
Euclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift.

</details>


### [866] [CMT: Mid-Training for Efficient Learning of Consistency, Mean Flow, and Flow Map Models](https://arxiv.org/abs/2509.24526)
*Zheyuan Hu,Chieh-Hsin Lai,Yuki Mitsufuji,Stefano Ermon*

Main category: cs.CV

TL;DR: 提出一致性中训练（CMT）方法用于视觉生成，在流图模型训练中表现出色，节省资源并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有流图模型训练不稳定、敏感且成本高，预训练后仍存在问题。

Method: 在预训练和最终流图训练间插入轻量级中间阶段CMT，训练模型沿求解器轨迹映射点。

Result: CMT在多个数据集上取得SOTA的两步FID，使用数据和GPU时间少，在ImageNet 256x256上减少训练时间。

Conclusion: CMT是训练流图模型的有效通用框架。

Abstract: Flow map models such as Consistency Models (CM) and Mean Flow (MF) enable
few-step generation by learning the long jump of the ODE solution of diffusion
models, yet training remains unstable, sensitive to hyperparameters, and
costly. Initializing from a pre-trained diffusion model helps, but still
requires converting infinitesimal steps into a long-jump map, leaving
instability unresolved. We introduce mid-training, the first concept and
practical method that inserts a lightweight intermediate stage between the
(diffusion) pre-training and the final flow map training (i.e., post-training)
for vision generation. Concretely, Consistency Mid-Training (CMT) is a compact
and principled stage that trains a model to map points along a solver
trajectory from a pre-trained model, starting from a prior sample, directly to
the solver-generated clean sample. It yields a trajectory-consistent and stable
initialization. This initializer outperforms random and diffusion-based
baselines and enables fast, robust convergence without heuristics. Initializing
post-training with CMT weights further simplifies flow map learning.
Empirically, CMT achieves state of the art two step FIDs: 1.97 on CIFAR-10,
1.32 on ImageNet 64x64, and 1.84 on ImageNet 512x512, while using up to 98%
less training data and GPU time, compared to CMs. On ImageNet 256x256, CMT
reaches 1-step FID 3.34 while cutting total training time by about 50% compared
to MF from scratch (FID 3.43). This establishes CMT as a principled, efficient,
and general framework for training flow map models.

</details>


### [867] [LaMoGen: Laban Movement-Guided Diffusion for Text-to-Motion Generation](https://arxiv.org/abs/2509.24469)
*Heechang Kim,Gwanghyun Kim,Se Young Chun*

Main category: cs.CV

TL;DR: 本文提出将Laban量化方法融入文本引导的运动生成模型，实现对人类运动生成的可解释和富有表现力的控制。


<details>
  <summary>Details</summary>
Motivation: 文本到运动合成在实现细粒度表达性运动控制方面存在挑战，原因是数据集缺乏运动风格多样性以及自然语言难以表达定量特征。

Method: 提出零样本、推理时间优化方法，在采样步骤更新预训练扩散模型的文本嵌入，引导运动生成模型具有所需的Laban Effort和Shape组件。

Result: 该方法能根据目标Laban标签成功操纵运动属性，产生多样化的表达性运动质量，同时保留运动身份。

Conclusion: 将Laban量化方法融入文本引导的运动生成模型可实现对人类运动生成的可解释和富有表现力的控制。

Abstract: Diverse human motion generation is an increasingly important task, having
various applications in computer vision, human-computer interaction and
animation. While text-to-motion synthesis using diffusion models has shown
success in generating high-quality motions, achieving fine-grained expressive
motion control remains a significant challenge. This is due to the lack of
motion style diversity in datasets and the difficulty of expressing
quantitative characteristics in natural language. Laban movement analysis has
been widely used by dance experts to express the details of motion including
motion quality as consistent as possible. Inspired by that, this work aims for
interpretable and expressive control of human motion generation by seamlessly
integrating the quantification methods of Laban Effort and Shape components
into the text-guided motion generation models. Our proposed zero-shot,
inference-time optimization method guides the motion generation model to have
desired Laban Effort and Shape components without any additional motion data by
updating the text embedding of pretrained diffusion models during the sampling
step. We demonstrate that our approach yields diverse expressive motion
qualities while preserving motion identity by successfully manipulating motion
attributes according to target Laban tags.

</details>


### [868] [Mitigating Visual Hallucinations via Semantic Curriculum Preference Optimization in MLLMs](https://arxiv.org/abs/2509.24491)
*Yuanshuai Li,Yuping Yan,Junfeng Tang,Yunxuan Li,Zeqi Zheng,Yaochu Jin*

Main category: cs.CV

TL;DR: 提出Semantic Curriculum Preference Optimization (SCPO)框架解决多模态大语言模型视觉幻觉问题，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在视觉幻觉问题，Direct Preference Optimization应用于MLLMs时无法捕捉细粒度语义差异和鼓励捷径学习。

Method: 提出SCPO框架，利用基于语义课程偏好对数据集构建的由易到难课程，结合动态参考模型和对称双向目标进行训练。

Result: 在多种幻觉基准测试中，SCPO比基线模型性能更优，幻觉率最多降低62.9%；在通用基准测试中提高事实性并保持通用能力，性能稳定。

Conclusion: SCPO是首个统一语义、对称性和课程的MLLM对齐框架，能有效减轻视觉幻觉。

Abstract: Multimodal Large Language Models (MLLMs) have significantly improved the
performance of various tasks, but continue to suffer from visual
hallucinations, a critical issue where generated responses contradict visual
evidence. While Direct Preference Optimization(DPO) is widely used for
alignment, its application to MLLMs often fails to capture fine-grained
semantic differences and encourages shortcut learning. To address these
challenges, we propose Semantic Curriculum Preference Optimization (SCPO), a
novel framework for MLLM alignment. SCPO employs a progressive, easy-to-hard
curriculum built upon our Semantic Curriculum Preference Pairs dataset, which
provides fine-grained semantic contrasts sorted by difficulty. This curriculum
is trained with a dynamic reference model and a novel symmetric, bidirectional
objective to facilitate simultaneous learning from both textual and visual
preferences. To our knowledge, SCPO is the first framework to unify semantics,
symmetry, and curriculum for MLLMs alignment, effectively mitigating visual
hallucinations. Extensive experiments on LLaVA models across various scales and
versions validate that SCPO demonstrates superior performance compared to
baseline models on multiple hallucination benchmarks, reducing the
hallucination rate by up to 62.9%. Moreover, evaluations on generalized
benchmarks show that SCPO improves factuality while preserving general
capabilities, with its performance remaining stable across general
vision-language benchmarks.

</details>


### [869] [CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D](https://arxiv.org/abs/2509.24528)
*Mohamad Amin Mirzaei,Pantea Amoie,Ali Ekhterachian,Matin Mirzababaei*

Main category: cs.CV

TL;DR: 本文提出利用SemanticSAM和上下文感知CLIP编码策略改善3D场景理解，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D语义映射方法因直接使用原始掩码产生碎片化掩码和不准确语义分配，在复杂环境中效果受限。

Method: 利用具有渐进粒度细化的SemanticSAM生成更准确和更多的对象级掩码；采用上下文感知CLIP编码策略整合掩码的多个上下文视图。

Result: 在多个3D场景理解任务和基准数据集上实验，结果显示显著优于现有方法。

Conclusion: 所提方法在3D场景理解中有效。

Abstract: 3D scene understanding is fundamental for embodied AI and robotics,
supporting reliable perception for interaction and navigation. Recent
approaches achieve zero-shot, open-vocabulary 3D semantic mapping by assigning
embedding vectors to 2D class-agnostic masks generated via vision-language
models (VLMs) and projecting these into 3D. However, these methods often
produce fragmented masks and inaccurate semantic assignments due to the direct
use of raw masks, limiting their effectiveness in complex environments. To
address this, we leverage SemanticSAM with progressive granularity refinement
to generate more accurate and numerous object-level masks, mitigating the
over-segmentation commonly observed in mask generation models such as vanilla
SAM, and improving downstream 3D semantic segmentation. To further enhance
semantic context, we employ a context-aware CLIP encoding strategy that
integrates multiple contextual views of each mask using empirically determined
weighting, providing much richer visual context. We evaluate our approach on
multiple 3D scene understanding tasks, including 3D semantic segmentation and
object retrieval from language queries, across several benchmark datasets.
Experimental results demonstrate significant improvements over existing
methods, highlighting the effectiveness of our approach.

</details>


### [870] [Can you SPLICE it together? A Human Curated Benchmark for Probing Visual Reasoning in VLMs](https://arxiv.org/abs/2509.24640)
*Mohamad Ballout,Okajevo Wilfred,Seyedalireza Yaghoubi,Nohayr Muhammad Abdelmoneim,Julius Mayer,Elia Bruni*

Main category: cs.CV

TL;DR: 本文介绍SPLICE基准测试，评估人类和VLM视觉推理能力，结果显示VLM与人类表现有差距，且在不同推理类型和任务类型上表现不同。


<details>
  <summary>Details</summary>
Motivation: 引入一个可从多个维度探究基于事件推理的基准测试，评估人类和视觉语言模型的视觉推理能力。

Method: 从COIN数据集构建SPLICE基准，包含3381个视频和11423个事件片段，让人类和VLM对片段进行排序以评估视觉推理能力。

Result: VLM与人类表现存在显著差距，人类注释文本可提高模型准确率但不影响人类表现，VLM在时间和因果推理主导的视频及日常任务上表现较好。

Conclusion: VLM在视觉推理方面仍面临挑战，难以达到人类水平。

Abstract: In this work, we introduce SPLICE, a human-curated benchmark derived from the
COIN instructional video dataset, designed to probe event-based reasoning
across multiple dimensions: temporal, causal, spatial, contextual, and general
knowledge. SPLICE includes 3,381 human-filtered videos spanning 12 categories
and 180 sub-categories, such as sports, engineering, and housework. These
videos are segmented into a total of 11,423 event clips. We evaluate both human
participants and state-of-the-art vision-language models (VLMs) on the task of
rearranging these clips into coherent event sequences to assess visual
reasoning capabilities. Results reveal a significant gap: VLMs struggle to
match human performance. While human-annotated textual descriptions improve
model accuracy, they do not affect human performance, suggesting that models
rely more on language priors than on visual understanding. Even with
annotations, VLMs fall short of human-level reasoning, underscoring persistent
challenges in visual reasoning. A deeper analysis across sub-categories shows
that VLMs perform relatively better on videos where temporal and causal
reasoning are dominant, compared to those where contextual and spatial
reasoning are dominant. They also perform better on everyday tasks than on
specialized ones.

</details>


### [871] [TACO-Net: Topological Signatures Triumph in 3D Object Classification](https://arxiv.org/abs/2509.24802)
*Anirban Ghosh,Ayan Dutta*

Main category: cs.CV

TL;DR: 提出TACO - Net结合拓扑数据分析与图像过滤技术进行3D对象分类，在多个数据集表现出色。


<details>
  <summary>Details</summary>
Motivation: 3D对象分类有重要实际意义，但因点云无序、不规则和有噪声，现有深度学习方法实现高分类准确率仍具挑战。

Method: 将点云转换为体素化二进制3D图像提取拓扑特征，用提取特征集训练轻量级1D CNN。

Result: TACO - Net在ModelNet40和ModelNet10上分别达到99.05%和99.52%准确率，在OmniObject3D数据集展示鲁棒性，对十种损坏的ModelNet40输入有强恢复力。

Conclusion: 所提TACO - Net达到新的最优水平。

Abstract: 3D object classification is a crucial problem due to its significant
practical relevance in many fields, including computer vision, robotics, and
autonomous driving. Although deep learning methods applied to point clouds
sampled on CAD models of the objects and/or captured by LiDAR or RGBD cameras
have achieved remarkable success in recent years, achieving high classification
accuracy remains a challenging problem due to the unordered point clouds and
their irregularity and noise. To this end, we propose a novel state-of-the-art
(SOTA) 3D object classification technique that combines topological data
analysis with various image filtration techniques to classify objects when they
are represented using point clouds. We transform every point cloud into a
voxelized binary 3D image to extract distinguishing topological features. Next,
we train a lightweight one-dimensional Convolutional Neural Network (1D CNN)
using the extracted feature set from the training dataset. Our framework,
TACO-Net, sets a new state-of-the-art by achieving $99.05\%$ and $99.52\%$
accuracy on the widely used synthetic benchmarks ModelNet40 and ModelNet10, and
further demonstrates its robustness on the large-scale real-world OmniObject3D
dataset. When tested with ten different kinds of corrupted ModelNet40 inputs,
the proposed TACO-Net demonstrates strong resiliency overall.

</details>


### [872] [VNODE: A Piecewise Continuous Volterra Neural Network](https://arxiv.org/abs/2509.24659)
*Siddharth Roheda,Aniruddha Bala,Rohit Chowdhury,Rohan Jaiswal*

Main category: cs.CV

TL;DR: 本文提出用于图像分类的VNODE，交替进行离散特征提取和ODE驱动状态演化，参数少且性能优。


<details>
  <summary>Details</summary>
Motivation: 受视觉皮层处理机制启发，寻求能有效捕捉复杂模式且参数少的图像分类方法。

Method: 将非线性Volterra滤波与连续时间神经常微分方程结合，构建分段连续的Volterra神经网络VNODE，交替进行离散Volterra特征提取和ODE驱动状态演化。

Result: 在CIFAR10和Imagenet1K等基准数据集上，VNODE始终优于现有模型，且计算复杂度更低。

Conclusion: VNODE能有效捕捉复杂模式，所需参数比传统深度架构少，是一种优秀的图像分类方法。

Abstract: This paper introduces Volterra Neural Ordinary Differential Equations
(VNODE), a piecewise continuous Volterra Neural Network that integrates
nonlinear Volterra filtering with continuous time neural ordinary differential
equations for image classification. Drawing inspiration from the visual cortex,
where discrete event processing is interleaved with continuous integration,
VNODE alternates between discrete Volterra feature extraction and ODE driven
state evolution. This hybrid formulation captures complex patterns while
requiring substantially fewer parameters than conventional deep architectures.
VNODE consistently outperforms state of the art models with improved
computational complexity as exemplified on benchmark datasets like CIFAR10 and
Imagenet1K.

</details>


### [873] [Environment-Aware Satellite Image Generation with Diffusion Models](https://arxiv.org/abs/2509.24875)
*Nikos Kostagiolas,Pantelis Georgiades,Yannis Panagakis,Mihalis A. Nicolaou*

Main category: cs.CV

TL;DR: 本文提出基于环境上下文的扩散模型生成卫星图像，在实验中表现优于先前方法，且收集了首个公开的三模态数据集。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型应用于遥感领域存在依赖有限环境上下文、难处理缺失或损坏数据、难反映用户意图等问题，需改进。

Method: 提出基于环境上下文的扩散模型，可基于文本、元数据和视觉数据三种控制信号组合生成卫星图像，采用元数据融合策略处理缺失或损坏数据。

Result: 该方法在单图像和时间序列生成试验中，定性和定量表现均优于先前方法。

Conclusion: 基于环境上下文可提升卫星图像基础模型性能，该模型在下游任务有应用潜力，收集的三模态数据集具有开创性。

Abstract: Diffusion-based foundation models have recently garnered much attention in
the field of generative modeling due to their ability to generate images of
high quality and fidelity. Although not straightforward, their recent
application to the field of remote sensing signaled the first successful trials
towards harnessing the large volume of publicly available datasets containing
multimodal information. Despite their success, existing methods face
considerable limitations: they rely on limited environmental context, struggle
with missing or corrupted data, and often fail to reliably reflect user
intentions in generated outputs. In this work, we propose a novel diffusion
model conditioned on environmental context, that is able to generate satellite
images by conditioning from any combination of three different control signals:
a) text, b) metadata, and c) visual data. In contrast to previous works, the
proposed method is i) to our knowledge, the first of its kind to condition
satellite image generation on dynamic environmental conditions as part of its
control signals, and ii) incorporating a metadata fusion strategy that models
attribute embedding interactions to account for partially corrupt and/or
missing observations. Our method outperforms previous methods both
qualitatively (robustness to missing metadata, higher responsiveness to control
inputs) and quantitatively (higher fidelity, accuracy, and quality of
generations measured using 6 different metrics) in the trials of single-image
and temporal generation. The reported results support our hypothesis that
conditioning on environmental context can improve the performance of foundation
models for satellite imagery, and render our model a promising candidate for
usage in downstream tasks. The collected 3-modal dataset is to our knowledge,
the first publicly-available dataset to combine data from these three different
mediums.

</details>


### [874] [VAGUEGAN: Stealthy Poisoning and Backdoor Attacks on Image Generative Pipelines](https://arxiv.org/abs/2509.24891)
*Mostafa Mohaimen Akand Faisal,Rabeya Amin Jhuma*

Main category: cs.CV

TL;DR: 本文介绍VagueGAN攻击管道，可对生成图像造成目标变化，实验显示中毒输出可能有更高视觉质量，揭示像素级防御盲点。


<details>
  <summary>Details</summary>
Motivation: 现有针对生成式管道的攻击研究较少，需探索输入小扰动导致输出可控变化的攻击。

Method: 引入VagueGAN攻击管道，结合PoisonerNet和生成器判别器对制作隐蔽触发器，用自定义代理指标评估攻击效果，通过感知和频域测量分析隐蔽性，还检验方法对现代扩散管道的可迁移性。

Result: 中毒输出视觉质量可能高于干净样本，潜空间中毒可保留或增强输出美学，精心优化的扰动能产生一致、隐蔽效果。

Conclusion: 潜空间中毒暴露了像素级防御的盲点，对图像生成管道的完整性构成威胁。

Abstract: Generative models such as GANs and diffusion models are widely used to
synthesize photorealistic images and to support downstream creative and editing
tasks. While adversarial attacks on discriminative models are well studied,
attacks targeting generative pipelines where small, stealthy perturbations in
inputs lead to controlled changes in outputs are less explored. This study
introduces VagueGAN, an attack pipeline combining a modular perturbation
network PoisonerNet with a Generator Discriminator pair to craft stealthy
triggers that cause targeted changes in generated images. Attack efficacy is
evaluated using a custom proxy metric, while stealth is analyzed through
perceptual and frequency domain measures. The transferability of the method to
a modern diffusion based pipeline is further examined through ControlNet guided
editing. Interestingly, the experiments show that poisoned outputs can display
higher visual quality compared to clean counterparts, challenging the
assumption that poisoning necessarily reduces fidelity. Unlike conventional
pixel level perturbations, latent space poisoning in GANs and diffusion
pipelines can retain or even enhance output aesthetics, exposing a blind spot
in pixel level defenses. Moreover, carefully optimized perturbations can
produce consistent, stealthy effects on generator outputs while remaining
visually inconspicuous, raising concerns for the integrity of image generation
pipelines.

</details>


### [875] [SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer](https://arxiv.org/abs/2509.24695)
*Junsong Chen,Yuyang Zhao,Jincheng Yu,Ruihang Chu,Junyu Chen,Shuai Yang,Xianbang Wang,Yicheng Pan,Daquan Zhou,Huan Ling,Haozhe Liu,Hongwei Yi,Hao Zhang,Muyang Li,Yukang Chen,Han Cai,Sanja Fidler,Ping Luo,Song Han,Enze Xie*

Main category: cs.CV

TL;DR: 介绍小扩散模型SANA - Video，能高效生成高分辨率长视频，成本低且性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 解决高效、低成本生成高分辨率、高质量长视频的问题。

Method: 采用Linear DiT和Constant - Memory KV cache for Block Linear Attention核心设计，探索有效数据过滤器和模型训练策略。

Result: 训练成本低至MovieGen的1%，性能有竞争力，速度比现代小扩散模型快16倍，在RTX 5090 GPU上推理速度提升2.4倍。

Conclusion: SANA - Video可实现低成本、高质量视频生成。

Abstract: We introduce SANA-Video, a small diffusion model that can efficiently
generate videos up to 720x1280 resolution and minute-length duration.
SANA-Video synthesizes high-resolution, high-quality and long videos with
strong text-video alignment at a remarkably fast speed, deployable on RTX 5090
GPU. Two core designs ensure our efficient, effective and long video
generation: (1) Linear DiT: We leverage linear attention as the core operation,
which is more efficient than vanilla attention given the large number of tokens
processed in video generation. (2) Constant-Memory KV cache for Block Linear
Attention: we design block-wise autoregressive approach for long video
generation by employing a constant-memory state, derived from the cumulative
properties of linear attention. This KV cache provides the Linear DiT with
global context at a fixed memory cost, eliminating the need for a traditional
KV cache and enabling efficient, minute-long video generation. In addition, we
explore effective data filters and model training strategies, narrowing the
training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of
MovieGen. Given its low cost, SANA-Video achieves competitive performance
compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B
and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,
SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating
the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x
speedup). In summary, SANA-Video enables low-cost, high-quality video
generation.

</details>


### [876] [Scalable GANs with Transformers](https://arxiv.org/abs/2509.24935)
*Sangeek Hyun,MinKyu Lee,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 本文研究GAN可扩展性，结合在紧凑VAE潜在空间训练和纯Transformer架构，分析扩展时失败模式并给出解决方案，实验表明GAT在不同容量下易可靠训练，GAT - XL/2在ImageNet - 256上表现优异。


<details>
  <summary>Details</summary>
Motivation: 可扩展性推动生成模型发展，但对抗学习的可扩展性原理未充分探索，因此研究GAN的可扩展性。

Method: 在紧凑VAE潜在空间训练，采用纯Transformer架构生成器和判别器，分析扩展时失败模式并给出轻量级中间监督和宽度感知学习率调整的解决方案。

Result: GAT能在广泛容量下可靠训练，GAT - XL/2在ImageNet - 256上40个epoch达到FID为2.96的单步、类条件生成性能，比强基线少6倍epoch。

Conclusion: 通过特定设计选择和解决方案，GAN可实现良好的可扩展性，在图像生成任务中取得优异性能。

Abstract: Scalability has driven recent advances in generative modeling, yet its
principles remain underexplored for adversarial learning. We investigate the
scalability of Generative Adversarial Networks (GANs) through two design
choices that have proven to be effective in other types of generative models:
training in a compact Variational Autoencoder latent space and adopting purely
transformer-based generators and discriminators. Training in latent space
enables efficient computation while preserving perceptual fidelity, and this
efficiency pairs naturally with plain transformers, whose performance scales
with computational budget. Building on these choices, we analyze failure modes
that emerge when naively scaling GANs. Specifically, we find issues as
underutilization of early layers in the generator and optimization instability
as the network scales. Accordingly, we provide simple and scale-friendly
solutions as lightweight intermediate supervision and width-aware learning-rate
adjustment. Our experiments show that GAT, a purely transformer-based and
latent-space GANs, can be easily trained reliably across a wide range of
capacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art
single-step, class-conditional generation performance (FID of 2.96) on
ImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines.

</details>


### [877] [LVT: Large-Scale Scene Reconstruction via Local View Transformers](https://arxiv.org/abs/2509.25001)
*Tooba Imtiaz,Lucy Chai,Kathryn Heal,Xuan Luo,Jungyeon Park,Jennifer Dy,John Flynn*

Main category: cs.CV

TL;DR: 提出Local View Transformer (LVT)架构用于大规模场景重建和新视角合成，可避免二次注意力操作，实现单步重建大尺寸高分辨率场景。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的二次复杂度难以将相关方法扩展到大型场景，需解决该挑战。

Method: 提出LVT架构，在每个视图的局部邻域处理信息，利用基于查询视图与邻近视图相对几何变换的位置编码，将模型输出解码为包含颜色和不透明度视图依赖的3D高斯点云场景表示。

Result: 能在单个前向传播中重建任意大尺寸、高分辨率的场景，项目页面有结果和交互式演示。

Conclusion: LVT架构可有效解决标准Transformer在处理大型场景时的复杂度问题，实现高效场景重建和新视角合成。

Abstract: Large transformer models are proving to be a powerful tool for 3D vision and
novel view synthesis. However, the standard Transformer's well-known quadratic
complexity makes it difficult to scale these methods to large scenes. To
address this challenge, we propose the Local View Transformer (LVT), a
large-scale scene reconstruction and novel view synthesis architecture that
circumvents the need for the quadratic attention operation. Motivated by the
insight that spatially nearby views provide more useful signal about the local
scene composition than distant views, our model processes all information in a
local neighborhood around each view. To attend to tokens in nearby views, we
leverage a novel positional encoding that conditions on the relative geometric
transformation between the query and nearby views. We decode the output of our
model into a 3D Gaussian Splat scene representation that includes both color
and opacity view-dependence. Taken together, the Local View Transformer enables
reconstruction of arbitrarily large, high-resolution scenes in a single forward
pass. See our project page for results and interactive demos
https://toobaimt.github.io/lvt/.

</details>


### [878] [VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding](https://arxiv.org/abs/2509.24776)
*Yizhuo Ding,Mingkang Chen,Zhibang Feng,Tong Xiao,Wanying Qu,Wenqi Shao,Yanwei Fu*

Main category: cs.CV

TL;DR: 研究多模态大语言模型感知策略，提出VTPerception - R1框架提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型难以将推理建立在感知证据上的问题。

Method: 对四种感知策略在四个多模态基准和两个MLLMs上进行研究，提出VTPerception - R1框架，包含感知增强微调与感知感知强化学习。

Result: VTPerception - R1显著提高推理准确性和鲁棒性。

Conclusion: VTPerception - R1为感知基础的多模态推理提供可扩展和可审计的解决方案。

Abstract: Multimodal large language models (MLLMs) often struggle to ground reasoning
in perceptual evidence. We present a systematic study of perception
strategies-explicit, implicit, visual, and textual-across four multimodal
benchmarks and two MLLMs. Our findings show that explicit perception,
especially when paired with textual cues, consistently yields the best
improvements, particularly for smaller models. Based on this insight, we
propose VTPerception-R1, a unified two-stage framework that decouples
perception from reasoning. Stage 1 introduces perception-augmented fine-tuning,
and Stage 2 applies perception-aware reinforcement learning with novel visual,
textual, and consistency rewards. Experiments demonstrate that VTPerception-R1
significantly improves reasoning accuracy and robustness across diverse tasks,
offering a scalable and auditable solution for perception-grounded multimodal
reasoning. Our code is available at:
https://github.com/yizhuoDi/VTPerceprion-R1.

</details>


### [879] [CLASP: Adaptive Spectral Clustering for Unsupervised Per-Image Segmentation](https://arxiv.org/abs/2509.25016)
*Max Curie,Paulo da Costa*

Main category: cs.CV

TL;DR: 介绍无监督图像分割框架CLASP，简单且免训练，在COCO Stuff和ADE20K上有竞争力，适用于数字广告等场景。


<details>
  <summary>Details</summary>
Motivation: 提出一个无需标注数据和微调的轻量级无监督图像分割框架。

Method: 用自监督ViT编码器提取特征，构建亲和矩阵并应用谱聚类，自动选择分割数量，用DenseCRF锐化边界。

Result: CLASP在COCO Stuff和ADE20K上获得有竞争力的mIoU和像素精度，匹配近期无监督基线。

Conclusion: 零训练设计使CLASP成为大型未标注语料库的强大且易复现的基线，适用于数字广告和营销工作流程。

Abstract: We introduce CLASP (Clustering via Adaptive Spectral Processing), a
lightweight framework for unsupervised image segmentation that operates without
any labeled data or finetuning. CLASP first extracts per patch features using a
self supervised ViT encoder (DINO); then, it builds an affinity matrix and
applies spectral clustering. To avoid manual tuning, we select the segment
count automatically with a eigengap silhouette search, and we sharpen the
boundaries with a fully connected DenseCRF. Despite its simplicity and training
free nature, CLASP attains competitive mIoU and pixel accuracy on COCO Stuff
and ADE20K, matching recent unsupervised baselines. The zero training design
makes CLASP a strong, easily reproducible baseline for large unannotated
corpora especially common in digital advertising and marketing workflows such
as brand safety screening, creative asset curation, and social media content
moderation

</details>


### [880] [VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning](https://arxiv.org/abs/2509.25033)
*Wenhao Li,Qiangchang Wang,Xianjing Meng,Zhibin Wu,Yilong Yin*

Main category: cs.CV

TL;DR: 提出VT - FSL框架解决少样本学习中语义幻觉问题，该框架含CIP和CGA模块，在多个基准测试中创最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本学习方法因缺乏实际实例基础，存在语义幻觉问题，导致噪声指导和高成本修正。

Method: 提出VT - FSL框架，包含CIP和CGA模块。CIP基于类名和支持图像生成精确类描述；CGA通过最小化三维平行多面体核化体积联合对齐融合的文本、支持和合成视觉表示。

Result: VT - FSL方法在十个不同基准测试中建立了新的最优性能。

Conclusion: VT - FSL框架有效解决少样本学习中的语义幻觉问题，提升少样本学习性能。

Abstract: Few-shot learning (FSL) aims to recognize novel concepts from only a few
labeled support samples. Recent studies enhance support features by
incorporating additional semantic information or designing complex semantic
fusion modules. However, they still suffer from hallucinating semantics that
contradict the visual evidence due to the lack of grounding in actual
instances, resulting in noisy guidance and costly corrections. To address these
issues, we propose a novel framework, bridging Vision and Text with LLMs for
Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts
conditioned on Large Language Models (LLMs) and support images, seamlessly
integrating them through a geometry-aware alignment. It mainly consists of
Cross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment
(CGA). Specifically, the CIP conditions an LLM on both class names and support
images to generate precise class descriptions iteratively in a single
structured reasoning pass. These descriptions not only enrich the semantic
understanding of novel classes but also enable the zero-shot synthesis of
semantically consistent images. The descriptions and synthetic images act
respectively as complementary textual and visual prompts, providing high-level
class semantics and low-level intra-class diversity to compensate for limited
support data. Furthermore, the CGA jointly aligns the fused textual, support,
and synthetic visual representations by minimizing the kernelized volume of the
3-dimensional parallelotope they span. It captures global and nonlinear
relationships among all representations, enabling structured and consistent
multimodal integration. The proposed VT-FSL method establishes new
state-of-the-art performance across ten diverse benchmarks, including standard,
cross-domain, and fine-grained few-shot learning scenarios. Code is available
at https://github.com/peacelwh/VT-FSL.

</details>


### [881] [Causal-Adapter: Taming Text-to-Image Diffusion for Faithful Counterfactual Generation](https://arxiv.org/abs/2509.24798)
*Lei Tong,Zhihua Liu,Chaochao Lu,Dino Oglic,Tom Diethe,Philip Teare,Sotirios A. Tsaftaris,Chen Jin*

Main category: cs.CV

TL;DR: 提出Causal - Adapter框架用于反事实图像生成，采用新策略取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 改进现有依赖提示工程且无显式因果结构的方法，实现对目标属性的因果干预并传播影响，同时保留图像核心身份。

Method: 利用结构因果模型，结合提示对齐注入和条件令牌对比损失两种属性正则化策略。

Result: 在合成和真实数据集上达到SOTA，Pendulum上MAE最多降低91%，ADNI上FID最多降低87%。

Conclusion: 该方法能实现鲁棒、可泛化的反事实编辑，属性修改真实且能强保留图像身份。

Abstract: We present Causal-Adapter, a modular framework that adapts frozen
text-to-image diffusion backbones for counterfactual image generation. Our
method enables causal interventions on target attributes, consistently
propagating their effects to causal dependents without altering the core
identity of the image. In contrast to prior approaches that rely on prompt
engineering without explicit causal structure, Causal-Adapter leverages
structural causal modeling augmented with two attribute regularization
strategies: prompt-aligned injection, which aligns causal attributes with
textual embeddings for precise semantic control, and a conditioned token
contrastive loss to disentangle attribute factors and reduce spurious
correlations. Causal-Adapter achieves state-of-the-art performance on both
synthetic and real-world datasets, with up to 91\% MAE reduction on Pendulum
for accurate attribute control and 87\% FID reduction on ADNI for high-fidelity
MRI image generation. These results show that our approach enables robust,
generalizable counterfactual editing with faithful attribute modification and
strong identity preservation.

</details>


### [882] [Score Distillation of Flow Matching Models](https://arxiv.org/abs/2509.25127)
*Mingyuan Zhou,Yi Gu,Huangjie Zheng,Liangchen Song,Guande He,Yizhe Zhang,Wenze Hu,Yinfei Yang*

Main category: cs.CV

TL;DR: 本文探讨扩散模型蒸馏技术能否用于流匹配模型，推导统一高斯扩散和流匹配，将SiD扩展到预训练文本到图像流匹配模型，实验表明SiD适用，统一加速技术。


<details>
  <summary>Details</summary>
Motivation: 扩散模型图像生成质量高但采样慢，蒸馏方法可缓解，流匹配与扩散理论等价，探究蒸馏技术能否直接用于流匹配。

Method: 基于贝叶斯规则和条件期望推导统一高斯扩散和流匹配，将Score identity Distillation (SiD)扩展到预训练文本到图像流匹配模型。

Result: 仅需适度调整，SiD在数据自由和数据辅助设置下对多种预训练文本到图像流匹配模型有效，无需教师微调或架构更改。

Conclusion: 分数蒸馏广泛适用于文本到图像流匹配模型，解决稳定性和合理性问题，统一扩散和流基生成器加速技术。

Abstract: Diffusion models achieve high-quality image generation but are limited by
slow iterative sampling. Distillation methods alleviate this by enabling one-
or few-step generation. Flow matching, originally introduced as a distinct
framework, has since been shown to be theoretically equivalent to diffusion
under Gaussian assumptions, raising the question of whether distillation
techniques such as score distillation transfer directly. We provide a simple
derivation -- based on Bayes' rule and conditional expectations -- that unifies
Gaussian diffusion and flow matching without relying on ODE/SDE formulations.
Building on this view, we extend Score identity Distillation (SiD) to
pretrained text-to-image flow-matching models, including SANA, SD3-Medium,
SD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show
that, with only modest flow-matching- and DiT-specific adjustments, SiD works
out of the box across these models, in both data-free and data-aided settings,
without requiring teacher finetuning or architectural changes. This provides
the first systematic evidence that score distillation applies broadly to
text-to-image flow matching models, resolving prior concerns about stability
and soundness and unifying acceleration techniques across diffusion- and
flow-based generators. We will make the PyTorch implementation publicly
available.

</details>


### [883] [Fast Feature Field ($\text{F}^3$): A Predictive Representation of Events](https://arxiv.org/abs/2509.25146)
*Richeek Das,Kostas Daniilidis,Pratik Chaudhari*

Main category: cs.CV

TL;DR: 本文提出Fast Feature Field ($	ext{F}^3$) 表示方法处理事件相机数据，计算高效，在多任务上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 构建事件相机数据的有效表示，保留场景结构和运动信息，以支持下游任务。

Method: 通过从过去事件预测未来事件学习$	ext{F}^3$表示，利用多分辨率哈希编码和深度集合思想高效计算。

Result: $	ext{F}^3$能在不同条件下对不同机器人平台数据计算，在光流估计、语义分割和单目深度估计任务达SOTA，实现25 - 75 Hz高清分辨率预测。

Conclusion: $	ext{F}^3$是一种高效且强大的事件相机数据表示方法，可用于多种下游任务。

Abstract: This paper develops a mathematical argument and algorithms for building
representations of data from event-based cameras, that we call Fast Feature
Field ($\text{F}^3$). We learn this representation by predicting future events
from past events and show that it preserves scene structure and motion
information. $\text{F}^3$ exploits the sparsity of event data and is robust to
noise and variations in event rates. It can be computed efficiently using ideas
from multi-resolution hash encoding and deep sets - achieving 120 Hz at HD and
440 Hz at VGA resolutions. $\text{F}^3$ represents events within a contiguous
spatiotemporal volume as a multi-channel image, enabling a range of downstream
tasks. We obtain state-of-the-art performance on optical flow estimation,
semantic segmentation, and monocular metric depth estimation, on data from
three robotic platforms (a car, a quadruped robot and a flying platform),
across different lighting conditions (daytime, nighttime), environments
(indoors, outdoors, urban, as well as off-road) and dynamic vision sensors
(resolutions and event rates). Our implementations can predict these tasks at
25-75 Hz at HD resolution.

</details>


### [884] [Vehicle Classification under Extreme Imbalance: A Comparative Study of Ensemble Learning and CNNs](https://arxiv.org/abs/2509.24880)
*Abu Hanif Muhammad Syarubany*

Main category: cs.CV

TL;DR: 本文针对车辆类型识别中数据集类别不平衡问题，构建新数据集并进行平衡处理，对比不同模型，发现深度模型优势，但仍存在少数类别识别不佳问题，建议收集少数类数据等。


<details>
  <summary>Details</summary>
Motivation: 公共数据集中严重的类别不平衡抑制了稀有类别车辆类型识别的性能，需解决该问题以实现准确的车辆类型识别。

Method: 合并Kaggle、ImageNet和网络爬取数据构建16类数据集，用SMOTE过采样和目标欠采样创建六个平衡变体，对比随机森林等轻量级集成模型和可配置的ResNet风格CNN模型。

Result: 最佳集成模型测试准确率达74.8%，CNN在完整测试集上准确率79.19%，在未见推理批次上达81.25%，但最不具代表性的类别（驳船）识别效果差。

Conclusion: 应优先收集更多少数类数据、采用成本敏感目标，探索混合集成或CNN管道以结合可解释性和表征能力。

Abstract: Accurate vehicle type recognition underpins intelligent transportation and
logistics, but severe class imbalance in public datasets suppresses performance
on rare categories. We curate a 16-class corpus (~47k images) by merging
Kaggle, ImageNet, and web-crawled data, and create six balanced variants via
SMOTE oversampling and targeted undersampling. Lightweight ensembles, such as
Random Forest, AdaBoost, and a soft-voting combiner built on MobileNet-V2
features are benchmarked against a configurable ResNet-style CNN trained with
strong augmentation and label smoothing. The best ensemble (SMOTE-combined)
attains 74.8% test accuracy, while the CNN achieves 79.19% on the full test set
and 81.25% on an unseen inference batch, confirming the advantage of deep
models. Nonetheless, the most under-represented class (Barge) remains a failure
mode, highlighting the limits of rebalancing alone. Results suggest
prioritizing additional minority-class collection and cost-sensitive objectives
(e.g., focal loss) and exploring hybrid ensemble or CNN pipelines to combine
interpretability with representational power.

</details>


### [885] [Personalized Vision via Visual In-Context Learning](https://arxiv.org/abs/2509.25172)
*Yuxin Jiang,Yuchao Gu,Yiren Song,Ivor Tsang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出PICO框架用于个性化视觉任务，通过VisRel数据集和种子评分器提升性能，实验表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型在个性化视觉任务上表现不佳，现有个性化方法存在局限，视觉上下文学习也有不足。

Method: 引入PICO框架，利用扩散变压器作为视觉上下文学习者；构建VisRel数据集；提出注意力引导的种子评分器。

Result: PICO超越微调与合成数据基线，能灵活适应新任务，在识别和生成任务上都有泛化能力。

Conclusion: PICO框架是解决个性化视觉任务的有效方法。

Abstract: Modern vision models, trained on large-scale annotated datasets, excel at
predefined tasks but struggle with personalized vision -- tasks defined at test
time by users with customized objects or novel objectives. Existing
personalization approaches rely on costly fine-tuning or synthetic data
pipelines, which are inflexible and restricted to fixed task formats. Visual
in-context learning (ICL) offers a promising alternative, yet prior methods
confine to narrow, in-domain tasks and fail to generalize to open-ended
personalization. We introduce Personalized In-Context Operator (PICO), a simple
four-panel framework that repurposes diffusion transformers as visual
in-context learners. Given a single annotated exemplar, PICO infers the
underlying transformation and applies it to new inputs without retraining. To
enable this, we construct VisRel, a compact yet diverse tuning dataset, showing
that task diversity, rather than scale, drives robust generalization. We
further propose an attention-guided seed scorer that improves reliability via
efficient inference scaling. Extensive experiments demonstrate that PICO (i)
surpasses fine-tuning and synthetic-data baselines, (ii) flexibly adapts to
novel user-defined tasks, and (iii) generalizes across both recognition and
generation.

</details>


### [886] [OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing](https://arxiv.org/abs/2509.24900)
*Zhihong Chen,Xuehai Bai,Yang Shi,Chaoyou Fu,Huanyu Zhang,Haotian Wang,Xiaoyan Sun,Zhang Zhang,Liang Wang,Yuanxing Zhang,Pengfei Wan,Yi-Fan Zhang*

Main category: cs.CV

TL;DR: 提出OpenGPT - 4o - Image数据集，结合新方法生成数据，实验表明用该数据集微调模型性能提升显著，说明系统数据构建对多模态AI能力提升很关键。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成和编辑的多模态模型训练数据质量和全面性不足，缺乏系统结构和真实场景，限制了模型性能。

Method: 采用结合分层任务分类法和自动化数据生成的新方法构建OpenGPT - 4o - Image数据集，利用结构化资源池和GPT - 4o生成80k高质量指令 - 图像对。

Result: 用该数据集微调主流模型在多个基准测试中性能显著提升，编辑任务最高提升18%，生成任务最高提升13%。

Conclusion: 系统的数据构建是提升多模态AI能力的关键。

Abstract: The performance of unified multimodal models for image generation and editing
is fundamentally constrained by the quality and comprehensiveness of their
training data. While existing datasets have covered basic tasks like style
transfer and simple object manipulation, they often lack the systematic
structure and challenging scenarios required for real-world applications. To
address this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset
constructed using a novel methodology that combines hierarchical task taxonomy
with automated data generation. Our taxonomy not only includes fundamental
capabilities such as text rendering and style control but also introduces
highly practical yet challenging categories like scientific imagery for
chemistry illustrations and complex instruction editing requiring simultaneous
execution of multiple operations. Through an automated pipeline leveraging
structured resource pools and GPT-4o, we generate 80k high-quality
instruction-image pairs with controlled diversity, covering 11 major domains
and 51 subtasks. Extensive experiments show that fine-tuning leading models on
our dataset achieves significant performance gains across multiple benchmarks,
with improvements of up to 18\% on editing tasks (UniWorld-V1 on ImgEdit-Bench)
and 13% on generation tasks (Harmon on GenEval). Our work demonstrates that
systematic data construction is key to advancing multimodal AI capabilities.

</details>


### [887] [GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs](https://arxiv.org/abs/2509.25178)
*Aryan Yazdan Parast,Parsa Hosseini,Hesam Asadollahzadeh,Arshia Soltani Moakhar,Basim Azam,Soheil Feizi,Naveed Akhtar*

Main category: cs.CV

TL;DR: 提出GHOST方法主动生成诱导幻觉的图像对多模态大语言模型进行压力测试，评估效果好，还能发现可迁移漏洞并辅助模型微调。


<details>
  <summary>Details</summary>
Motivation: 现有使用静态基准测试研究多模态大语言模型物体幻觉问题，无法发现模型特定或意外的幻觉漏洞，需新方法。

Method: 在图像嵌入空间优化误导模型，同时保持目标物体不存在，引导扩散模型生成自然图像。

Result: 在多种模型上评估，幻觉成功率超28%，生成图像高质量且无目标物体，还发现可迁移漏洞。

Conclusion: GHOST可作为诊断和纠正工具，有助于构建更可靠的多模态系统。

Abstract: Object hallucination in Multimodal Large Language Models (MLLMs) is a
persistent failure mode that causes the model to perceive objects absent in the
image. This weakness of MLLMs is currently studied using static benchmarks with
fixed visual scenarios, which preempts the possibility of uncovering
model-specific or unanticipated hallucination vulnerabilities. We introduce
GHOST (Generating Hallucinations via Optimizing Stealth Tokens), a method
designed to stress-test MLLMs by actively generating images that induce
hallucination. GHOST is fully automatic and requires no human supervision or
prior knowledge. It operates by optimizing in the image embedding space to
mislead the model while keeping the target object absent, and then guiding a
diffusion model conditioned on the embedding to generate natural-looking
images. The resulting images remain visually natural and close to the original
input, yet introduce subtle misleading cues that cause the model to
hallucinate. We evaluate our method across a range of models, including
reasoning models like GLM-4.1V-Thinking, and achieve a hallucination success
rate exceeding 28%, compared to around 1% in prior data-driven discovery
methods. We confirm that the generated images are both high-quality and
object-free through quantitative metrics and human evaluation. Also, GHOST
uncovers transferable vulnerabilities: images optimized for Qwen2.5-VL induce
hallucinations in GPT-4o at a 66.5% rate. Finally, we show that fine-tuning on
our images mitigates hallucination, positioning GHOST as both a diagnostic and
corrective tool for building more reliable multimodal systems.

</details>


### [888] [Segmentor-Guided Counterfactual Fine-Tuning for Image Synthesis](https://arxiv.org/abs/2509.24913)
*Tian Xia,Matthew Sinclair,Andreas Schuh,Fabio De Sousa Ribeiro,Raghav Mehta,Rajat Rasal,Esther Puyol-Antón,Samuel Gerber,Kersten Petersen,Michiel Schaap,Ben Glocker*

Main category: cs.CV

TL;DR: 提出Seg - CFT方法用于反事实图像生成，能生成逼真胸部X光片并在冠心病建模上有良好表现。


<details>
  <summary>Details</summary>
Motivation: 当前反事实图像生成方法对特定结构干预不足且依赖外部分类器或回归器，以往使用像素级标签图作为指导获取困难。

Method: 提出Segmentor - guided Counterfactual Fine - Tuning (Seg - CFT)方法。

Result: 能够生成逼真的胸部X光片，在冠心病建模上有良好结果。

Conclusion: Seg - CFT方法在保持对特定结构标量值变量干预简单性的同时，能产生局部连贯且有效的反事实图像。

Abstract: Counterfactual image generation is a powerful tool for augmenting training
data, de-biasing datasets, and modeling disease. Current approaches rely on
external classifiers or regressors to increase the effectiveness of
subject-level interventions (e.g., changing the patient's age). For
structure-specific interventions (e.g., changing the area of the left lung in a
chest radiograph), we show that this is insufficient, and can result in
undesirable global effects across the image domain. Previous work used
pixel-level label maps as guidance, requiring a user to provide hypothetical
segmentations which are tedious and difficult to obtain. We propose
Segmentor-guided Counterfactual Fine-Tuning (Seg-CFT), which preserves the
simplicity of intervening on scalar-valued, structure-specific variables while
producing locally coherent and effective counterfactuals. We demonstrate the
capability of generating realistic chest radiographs, and we show promising
results for modeling coronary artery disease. Code:
https://github.com/biomedia-mira/seg-cft.

</details>


### [889] [Fast Real-Time Pipeline for Robust Arm Gesture Recognition](https://arxiv.org/abs/2509.25042)
*Milán Zsolt Bagladi,László Gulyás,Gergő Szalay*

Main category: cs.CV

TL;DR: 提出基于OpenPose关键点估计、归一化和循环神经网络分类器的实时动态手臂手势识别管道，在自定义数据集实验中效果好，还给出计算手臂信号速度方法。


<details>
  <summary>Details</summary>
Motivation: 实现实时动态手臂手势识别，并提高对相机角度变化的鲁棒性。

Method: 采用OpenPose进行关键点估计，提出1x1归一化方案和两种特征表示，利用人工旋转训练数据提高鲁棒性，使用循环神经网络分类器。

Result: 在自定义交通控制手势数据集上的实验表明，在不同视角和速度下都有高准确率。

Conclusion: 所提出的实时管道能有效进行动态手臂手势识别，还可计算手臂信号速度。

Abstract: This paper presents a real-time pipeline for dynamic arm gesture recognition
based on OpenPose keypoint estimation, keypoint normalization, and a recurrent
neural network classifier. The 1 x 1 normalization scheme and two feature
representations (coordinate- and angle-based) are presented for the pipeline.
In addition, an efficient method to improve robustness against camera angle
variations is also introduced by using artificially rotated training data.
Experiments on a custom traffic-control gesture dataset demonstrate high
accuracy across varying viewing angles and speeds. Finally, an approach to
calculate the speed of the arm signal (if necessary) is also presented.

</details>


### [890] [BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation](https://arxiv.org/abs/2509.25077)
*Dingning Liu,Haoyu Guo,Jingyi Zhou,Tong He*

Main category: cs.CV

TL;DR: 提出BRIDGE框架合成大量图像用于单目深度估计，训练模型采用混合监督策略，性能超现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统单目深度估计方法受数据稀缺和质量限制，缺乏鲁棒性。

Method: 提出RL优化的深度到图像（D2I）生成框架BRIDGE合成图像，采用混合监督策略训练深度估计模型。

Result: BRIDGE在规模和领域多样性上取得突破，定量表现和复杂场景细节捕捉上超现有方法。

Conclusion: 创新的数据生成和训练范式能培养通用且鲁棒的深度特征。

Abstract: Monocular Depth Estimation (MDE) is a foundational task for computer vision.
Traditional methods are limited by data scarcity and quality, hindering their
robustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image
(D2I) generation framework that synthesizes over 20M realistic and
geometrically accurate RGB images, each intrinsically paired with its ground
truth depth, from diverse source depth maps. Then we train our depth estimation
model on this dataset, employing a hybrid supervision strategy that integrates
teacher pseudo-labels with ground truth depth for comprehensive and robust
training. This innovative data generation and training paradigm enables BRIDGE
to achieve breakthroughs in scale and domain diversity, consistently
outperforming existing state-of-the-art approaches quantitatively and in
complex scene detail capture, thereby fostering general and robust depth
features. Code and models are available at
https://dingning-liu.github.io/bridge.github.io/.

</details>


### [891] [UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation](https://arxiv.org/abs/2509.25079)
*Guanjun Wu,Jiemin Fang,Chen Yang,Sikuang Li,Taoran Yi,Jia Lu,Zanwei Zhou,Jiazhong Cen,Lingxi Xie,Xiaopeng Zhang,Wei Wei,Wenyu Liu,Xinggang Wang,Qi Tian*

Main category: cs.CV

TL;DR: 提出UniLat3D统一框架，单阶段生成3D资产，速度快且质量高。


<details>
  <summary>Details</summary>
Motivation: 现有3D预训练模型多采用两阶段管道，易产生几何纹理不对齐和成本问题，需改进。

Method: 提出几何 - 外观统一VAE，将高分辨率稀疏特征压缩成UniLat，训练单一流匹配模型将高斯噪声映射到UniLat。

Result: 仅在公共数据集上训练，能从单张图像秒级生成高质量3D资产。

Conclusion: UniLat3D实现了优越的外观保真度和几何质量，是一种高效的3D资产生成方法。

Abstract: High-fidelity 3D asset generation is crucial for various industries. While
recent 3D pretrained models show strong capability in producing realistic
content, most are built upon diffusion models and follow a two-stage pipeline
that first generates geometry and then synthesizes appearance. Such a decoupled
design tends to produce geometry-texture misalignment and non-negligible cost.
In this paper, we propose UniLat3D, a unified framework that encodes geometry
and appearance in a single latent space, enabling direct single-stage
generation. Our key contribution is a geometry-appearance Unified VAE, which
compresses high-resolution sparse features into a compact latent representation
-- UniLat. UniLat integrates structural and visual information into a dense
low-resolution latent, which can be efficiently decoded into diverse 3D
formats, e.g., 3D Gaussians and meshes. Based on this unified representation,
we train a single flow-matching model to map Gaussian noise directly into
UniLat, eliminating redundant stages. Trained solely on public datasets,
UniLat3D produces high-quality 3D assets in seconds from a single image,
achieving superior appearance fidelity and geometric quality. More demos \&
code are available at https://unilat3d.github.io/

</details>


### [892] [GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts](https://arxiv.org/abs/2509.25160)
*Fan Yuan,Yuchen Yan,Yifan Jiang,Haoran Zhao,Tao Feng,Jinyan Chen,Yanwei Lou,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CV

TL;DR: 本文提出视觉多图像数学推理基准GSM8K-V，评估多种模型，发现现有模型在该基准上仍有提升空间，为视觉数学推理研究提供新视角。


<details>
  <summary>Details</summary>
Motivation: 现有视觉数学推理基准存在局限，如局限于几何、缺乏文字问题覆盖和多图像推理评估，因此提出GSM8K-V以填补这些空白。

Method: 将文本基准GSM8K样本系统映射为视觉形式，通过自动图像生成流程和人工标注，整理出1319个高质量样本，并在GSM8K-V上评估多种开源和闭源模型。

Result: 现有视觉语言模型在文本GSM8K上性能接近饱和，但在GSM8K-V上有很大提升空间，如Gemini - 2.5 - Pro在GSM8K上准确率95.22%，在GSM8K-V上仅46.93%。

Conclusion: GSM8K-V为视觉数学推理提供新视角，为开发更强大、更具泛化性的视觉语言模型建立了基准。

Abstract: Vision language models (VLMs) achieve unified modeling of images and text,
enabling them to accomplish complex real-world tasks through perception,
planning, and reasoning. Among these tasks, reasoning is particularly
representative, with mathematical reasoning serving as a prominent example. It
highlights the high-level capability of VLMs to comprehend mathematical
information in images and to perform sophisticated reasoning. Recently,
numerous visual mathematical reasoning benchmarks have been proposed, but they
are often restricted to geometry, lack coverage of math word problems, and
rarely assess reasoning across multiple images. To address these gaps, we
introduce GSM8K-V, a purely visual multi-image mathematical reasoning
benchmark. GSM8K-V is built by systematically mapping each sample from the
widely used text-based GSM8K into visual form. Through a carefully designed
automated image-generation pipeline combined with meticulous human annotation,
we curate 1,319 high-quality samples. We evaluate a wide range of open-source
and closed-source models on GSM8K-V. Results show that although existing VLMs
have nearly saturated performance on text-based GSM8K, there remains
substantial room for improvement on GSM8K-V. For example, the best-performing
model, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on
GSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the
limitations of current models as well as potential directions for improvement.
GSM8K-V offers a new perspective on visual mathematical reasoning and
establishes a benchmark to guide the development of more robust and
generalizable VLMs.

</details>


### [893] [DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed Latent Space](https://arxiv.org/abs/2509.25180)
*Wenkun He,Yuchao Gu,Junyu Chen,Dongyun Zou,Yujun Lin,Zhekai Zhang,Haocheng Xi,Muyang Li,Ligeng Zhu,Jincheng Yu,Junsong Chen,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: 现有文本到图像扩散模型生成高分辨率图像效率低，本文提出DC - Gen框架，利用深度压缩潜在空间加速模型，在SANA和FLUX.1 - Krea上验证有效，显著提升速度。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在高分辨率图像生成时效率低，以往研究很少处理潜在空间的固有冗余。

Method: 引入DC - Gen框架，采用高效的训练后管道，先进行轻量级嵌入对齐训练以弥合潜在空间表示差距，再进行少量LoRA微调。

Result: DC - Gen - SANA和DC - Gen - FLUX模型与基础模型质量相当，但速度显著提升，如DC - Gen - FLUX在NVIDIA H100 GPU上使4K图像生成延迟降低53倍，结合NVFP4 SVDQuant在单块NVIDIA 5090 GPU上生成4K图像总延迟降低138倍。

Conclusion: DC - Gen框架能有效加速文本到图像扩散模型，在保证图像质量的同时大幅提升高分辨率图像生成速度。

Abstract: Existing text-to-image diffusion models excel at generating high-quality
images, but face significant efficiency challenges when scaled to high
resolutions, like 4K image generation. While previous research accelerates
diffusion models in various aspects, it seldom handles the inherent redundancy
within the latent space. To bridge this gap, this paper introduces DC-Gen, a
general framework that accelerates text-to-image diffusion models by leveraging
a deeply compressed latent space. Rather than a costly training-from-scratch
approach, DC-Gen uses an efficient post-training pipeline to preserve the
quality of the base model. A key challenge in this paradigm is the
representation gap between the base model's latent space and a deeply
compressed latent space, which can lead to instability during direct
fine-tuning. To overcome this, DC-Gen first bridges the representation gap with
a lightweight embedding alignment training. Once the latent embeddings are
aligned, only a small amount of LoRA fine-tuning is needed to unlock the base
model's inherent generation quality. We verify DC-Gen's effectiveness on SANA
and FLUX.1-Krea. The resulting DC-Gen-SANA and DC-Gen-FLUX models achieve
quality comparable to their base models but with a significant speedup.
Specifically, DC-Gen-FLUX reduces the latency of 4K image generation by 53x on
the NVIDIA H100 GPU. When combined with NVFP4 SVDQuant, DC-Gen-FLUX generates a
4K image in just 3.5 seconds on a single NVIDIA 5090 GPU, achieving a total
latency reduction of 138x compared to the base FLUX.1-Krea model. Code:
https://github.com/dc-ai-projects/DC-Gen.

</details>


### [894] [DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder](https://arxiv.org/abs/2509.25182)
*Junyu Chen,Wenkun He,Yuchao Gu,Yuyang Zhao,Jincheng Yu,Junsong Chen,Dongyun Zou,Yujun Lin,Zhekai Zhang,Muyang Li,Haocheng Xi,Ligeng Zhu,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: 介绍DC - VideoGen，可加速预训练视频扩散模型，提升效率，有两项创新，加速模型推理延迟低且能单GPU生成高分辨率视频。


<details>
  <summary>Details</summary>
Motivation: 为视频生成提供高效的后训练加速框架，提升预训练视频扩散模型的效率。

Method: 构建Deep Compression Video Autoencoder实现高压缩比，采用AE - Adapt - V策略将预训练模型迁移到新潜在空间。

Result: 适配预训练模型仅需10个NVIDIA H100 GPU天，加速模型推理延迟最多降低14.8倍，可单GPU生成2160x3840视频。

Conclusion: DC - VideoGen能有效加速预训练视频扩散模型，在不损失质量的前提下提升效率。

Abstract: We introduce DC-VideoGen, a post-training acceleration framework for
efficient video generation. DC-VideoGen can be applied to any pre-trained video
diffusion model, improving efficiency by adapting it to a deep compression
latent space with lightweight fine-tuning. The framework builds on two key
innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal
temporal design that achieves 32x/64x spatial and 4x temporal compression while
preserving reconstruction quality and generalization to longer videos; and (ii)
AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer
of pre-trained models into the new latent space. Adapting the pre-trained
Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100
GPU. The accelerated models achieve up to 14.8x lower inference latency than
their base counterparts without compromising quality, and further enable
2160x3840 video generation on a single GPU. Code:
https://github.com/dc-ai-projects/DC-VideoGen.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [895] [Agentic DDQN-Based Scheduling for Licensed and Unlicensed Band Allocation in Sidelink Networks](https://arxiv.org/abs/2509.06775)
*Po-Heng Chou,Pin-Qi Fu,Walid Saad,Li-Chun Wang*

Main category: eess.SY

TL;DR: 提出用于NR SL网络许可/非许可频段分配的代理双深度Q网络调度器，减少阻塞，有较好效果。


<details>
  <summary>Details</summary>
Motivation: 在NR SL网络中实现更好的许可/非许可频段分配，解决传统方法问题。

Method: 设计代理双深度Q网络调度器，感知多维上下文，采用容量感知、QoS约束奖励。

Result: 在受限带宽下，与阈值策略相比减少阻塞达87.5%，同时保持吞吐量。

Conclusion: 上下文驱动决策在共存受限的NR SL网络中有价值，所提调度器适用于网络边缘任务特定、资源高效操作。

Abstract: In this paper, we present an agentic double deep Q-network (DDQN) scheduler
for licensed/unlicensed band allocation in New Radio (NR) sidelink (SL)
networks. Beyond conventional reward-seeking reinforcement learning (RL), the
agent perceives and reasons over a multi-dimensional context that jointly
captures queueing delay, link quality, coexistence intensity, and switching
stability. A capacity-aware, quality of service (QoS)-constrained reward aligns
the agent with goal-oriented scheduling rather than static thresholding. Under
constrained bandwidth, the proposed design reduces blocking by up to 87.5%
versus threshold policies while preserving throughput, highlighting the value
of context-driven decisions in coexistence-limited NR SL networks. The proposed
scheduler is an embodied agent (E-agent) tailored for task-specific,
resource-efficient operation at the network edge.

</details>


### [896] [Rebuild AC Power Flow Models with Graph Attention Networks](https://arxiv.org/abs/2509.22733)
*Yuting Hu,Jinjun Xiong*

Main category: eess.SY

TL;DR: 提出基于图注意力网络的潮流重建模型，经实验验证其在变化网络中精度更高且泛化性好。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的方法依赖完整潮流模型，但实际中潮流模型参数可能不准确或不可用，且需考虑潮流模型对不同网络规模和拓扑的泛化性。

Method: 基于各母线电压的实部和虚部构建新图，提出基于图注意力网络（GAT）的潮流重建模型。

Result: 与两个最先进的潮流重建模型在不同标准IEEE电力系统案例及其修改拓扑变体上进行比较，证明了方法的可行性。

Conclusion: 所提出的模型在变化网络中能实现更好的精度，且对不同网络的泛化时精度损失更小。

Abstract: A full power flow (PF) model is a complete representation of the physical
power network. Traditional model-based methods rely on the full PF model to
implement power flow analysis. In practice, however, some PF model parameters
can be inaccurate or even unavailable due to the uncertainties or dynamics in
the power systems. Moreover, because the power network keeps evolving with
possibly changing topology, the generalizability of a PF model to different
network sizes and typologies should be considered. In this paper, we propose a
PF rebuild model based on graph attention networks (GAT) by constructing a new
graph based on the real and imaginary parts of voltage at each bus. By
comparing with two state-of-the-art PF rebuild models for different standard
IEEE power system cases and their modified topology variants, we demonstrate
the feasibility of our method. Experimental results show that our proposed
model achieves better accuracy for a changing network and can generalize to
different networks with less accuracy discount.

</details>


### [897] [Semantic-Aware Edge Intelligence for UAV Handover in 6G Networks](https://arxiv.org/abs/2509.22668)
*Aubida A. Al-Hameed,Mohammed M. H. Qazzaz,Maryam Hafeez,Syed A. Zaidi*

Main category: eess.SY

TL;DR: 本文探讨利用边缘生成式AI进行6G无线网络优化，提出基于微调MobileBERT的无人机切换框架，在合成数据集上验证了模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 6G无线网络需利用语义感知优化资源、降低能耗和延迟，本文旨在利用边缘生成式AI能力进行网络优化。

Method: 提出在无人机上部署经低秩自适应微调的轻量级MobileBERT语言模型，处理多属性测量并进行多标签分类以确定切换动作，同时识别原因标签。

Result: 在基于规则的合成数据集上，模型能有效学习规则，预测切换决策准确性高，识别支持原因的F1微分数约为0.9。

Conclusion: 基于微调MobileBERT的无人机切换框架在学习规则和预测决策方面表现良好，能有效应用于6G无线网络优化。

Abstract: 6G wireless networks aim to exploit semantic awareness to optimize radio
resources. By optimizing the transmission through the lens of the desired goal,
the energy consumption of transmissions can also be reduced, and the latency
can be improved. To that end, this paper investigates a paradigm in which the
capabilities of generative AI (GenAI) on the edge are harnessed for network
optimization. In particular, we investigate an Unmanned Aerial Vehicle (UAV)
handover framework that takes advantage of GenAI and semantic communication to
maintain reliable connectivity. To that end, we propose a framework in which a
lightweight MobileBERT language model, fine-tuned using Low-Rank Adaptation
(LoRA), is deployed on the UAV. This model processes multi-attribute flight and
radio measurements and performs multi-label classification to determine
appropriate handover action. Concurrently, the model identifies an appropriate
set of contextual "Reason Tags" that elucidate the decision's rationale. Our
model, evaluated on a rule-based synthetic dataset of UAV handover scenarios,
demonstrates the model's high efficacy in learning these rules, achieving high
accuracy in predicting the primary handover decision. The model also shows
strong performance in identifying supporting reasons, with an F1 micro-score of
approximately 0.9 for reason tags.

</details>


### [898] [Optimizing the Network Topology of a Linear Reservoir Computer](https://arxiv.org/abs/2509.23391)
*Sahand Tangerami,Nicholas A. Mecholsky,Francesco Sorrentino*

Main category: eess.SY

TL;DR: 本文聚焦优化线性储层计算机（RC）拓扑以提升性能和可解释性，模拟显示优化后的RC表现更优，还提供了设计指导。


<details>
  <summary>Details</summary>
Motivation: 传统RC连接随机，缺乏原则性设计，需要优化拓扑以提升性能和可解释性。

Method: 将RC动力学解耦为多个独立模式，优化每个模式以执行给定任务，即根据RC邻接矩阵的特征值选择最优连接。

Result: 在不同规模网络的模拟中，优化后的RC在训练和测试阶段均显著优于随机构建的储层，且常超越同等规模的非线性储层。

Conclusion: 该方法提供了实际性能优势和理论指导，可用于设计高效、特定任务且分析透明的RC架构。

Abstract: Machine learning has become a fundamental approach for modeling, prediction,
and control, enabling systems to learn from data and perform complex tasks.
Reservoir computing is a machine learning tool that leverages high-dimensional
dynamical systems to efficiently process temporal data for prediction and
observation tasks. Traditionally, the connectivity of a reservoir computer (RC)
is generated at random, lacking a principled design. Here, we focus on
optimizing the topology of a linear RC to improve its performance and
interpretability, which we achieve by decoupling the RC dynamics into a number
of independent modes. We then proceed to optimize each one of these modes to
perform a given task, which corresponds to selecting an optimal RC connectivity
in terms of a given set of eigenvalues of the RC adjacency matrix. Simulations
on networks of varying sizes show that the optimized RC significantly
outperforms randomly constructed reservoirs in both the training and testing
phases and also often surpasses nonlinear reservoirs of comparable size. This
approach provides both practical performance advantages and theoretical
guidelines for designing efficient, task-specific, and analytically transparent
RC architectures.

</details>


### [899] [Communication-aware Wide-Area Damping Control using Risk-Constrained Reinforcement Learning](https://arxiv.org/abs/2509.23620)
*Kyung-bin Kwon,Lintao Ye,Vijay Gupta,Hao Zhu*

Main category: eess.SY

TL;DR: 提出风险约束框架解决电力系统广域阻尼控制中通信延迟及其他网络问题，用SGDmax算法求解，测试显示优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统延迟估计和补偿方法对快速广域阻尼控制精度要求高，且无法处理链路故障等网络问题。

Method: 提出风险约束框架，引入均值 - 方差风险约束到LQR最优控制成本中，用基于强化学习的SGDmax算法求解。

Result: SGDmax算法能高概率收敛到平稳状态，IEEE 68 - 母线系统测试验证算法收敛性和VSCs阻尼能力，该方法在估计误差下优于传统方法。

Conclusion: 提出的风险约束设计能缓解大延迟下最坏情况的振荡，有效处理其他通信问题和网络扰动。

Abstract: Non-ideal communication links, especially delays, critically affect fast
networked controls in power systems, such as the wide-area damping control
(WADC). Traditionally, a delay estimation and compensation approach is adopted
to address this cyber-physical coupling, but it demands very high accuracy for
the fast WADC and cannot handle other cyber concerns like link failures or
{cyber perturbations}. Hence, we propose a new risk-constrained framework that
can target the communication delays, yet amenable to general uncertainty under
the cyber-physical couplings. Our WADC model includes the synchronous
generators (SGs), and also voltage source converters (VSCs) for additional
damping capabilities. To mitigate uncertainty, a mean-variance risk constraint
is introduced to the classical optimal control cost of the linear quadratic
regulator (LQR). Unlike estimating delays, our approach can effectively
mitigate large communication delays by improving the worst-case performance. A
reinforcement learning (RL)-based algorithm, namely, stochastic
gradient-descent with max-oracle (SGDmax), is developed to solve the
risk-constrained problem. We further show its guaranteed convergence to
stationarity at a high probability, even using the simple zero-order policy
gradient (ZOPG). Numerical tests on the IEEE 68-bus system not only verify
SGDmax's convergence and VSCs' damping capabilities, but also demonstrate that
our approach outperforms conventional delay compensator-based methods under
estimation error. While focusing on performance improvement under large delays,
our proposed risk-constrained design can effectively mitigate the worst-case
oscillations, making it equally effective for addressing other communication
issues and cyber perturbations.

</details>


### [900] [Equation-Free Coarse Control of Distributed Parameter Systems via Local Neural Operators](https://arxiv.org/abs/2509.23975)
*Gianluca Fabiani,Constantinos Siettos,Ioannis G. Kevrekidis*

Main category: eess.SY

TL;DR: 提出基于局部神经算子的数据驱动方法控制高维分布参数系统，利用Krylov子空间方法和降阶模型设计控制器。


<details>
  <summary>Details</summary>
Motivation: 经典无方程方法计算量大或缺乏微观时间步长器，需以数据为唯一资源控制高维分布参数系统。

Method: 用局部神经算子训练时空微观/介观数据得到短时解算子，结合Krylov子空间方法计算稳态和非稳态，用Krylov - Arnoldi迭代近似主导特征谱得到降阶模型，基于降阶系统设计离散时间线性二次调节器和极点配置控制器。

Result: 获得能捕捉开环慢动态的降阶模型，完成反馈控制闭环。

Conclusion: 提出的数据驱动方法可有效解决高维分布参数系统控制难题。

Abstract: The control of high-dimensional distributed parameter systems (DPS) remains a
challenge when explicit coarse-grained equations are unavailable. Classical
equation-free (EF) approaches rely on fine-scale simulators treated as
black-box timesteppers. However, repeated simulations for steady-state
computation, linearization, and control design are often computationally
prohibitive, or the microscopic timestepper may not even be available, leaving
us with data as the only resource. We propose a data-driven alternative that
uses local neural operators, trained on spatiotemporal microscopic/mesoscopic
data, to obtain efficient short-time solution operators. These surrogates are
employed within Krylov subspace methods to compute coarse steady and
unsteady-states, while also providing Jacobian information in a matrix-free
manner. Krylov-Arnoldi iterations then approximate the dominant eigenspectrum,
yielding reduced models that capture the open-loop slow dynamics without
explicit Jacobian assembly. Both discrete-time Linear Quadratic Regulator
(dLQR) and pole-placement (PP) controllers are based on this reduced system and
lifted back to the full nonlinear dynamics, thereby closing the feedback loop.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [901] [Advancing Audio-Visual Navigation Through Multi-Agent Collaboration in 3D Environments](https://arxiv.org/abs/2509.22698)
*Hailong Zhang,Yinfeng Yu,Liejun Wang,Fuchun Sun,Wendong Zheng*

Main category: cs.RO

TL;DR: 本文提出MASTAVN框架，实现两智能体在共享3D环境中协作导航至音频目标，评估显示其在任务完成时间和成功率上优于单智能体和非协作基线，验证其在应急场景有效性。


<details>
  <summary>Details</summary>
Motivation: 现有视听导航研究多关注单智能体系统，在动态3D环境的多智能体快速协调方面存在局限，尤其在应急响应等对时间敏感的应用中。

Method: 引入MASTAVN框架，集成跨智能体通信协议和联合视听融合机制，增强空间推理和时间同步。

Result: 在逼真3D模拟器中评估，MASTAVN显著减少任务完成时间，显著提高导航成功率。

Conclusion: MASTAVN在时间敏感的应急场景中有效，为复杂3D环境中可扩展的多智能体具身智能发展建立了范例。

Abstract: Intelligent agents often require collaborative strategies to achieve complex
tasks beyond individual capabilities in real-world scenarios. While existing
audio-visual navigation (AVN) research mainly focuses on single-agent systems,
their limitations emerge in dynamic 3D environments where rapid multi-agent
coordination is critical, especially for time-sensitive applications like
emergency response. This paper introduces MASTAVN (Multi-Agent Scalable
Transformer Audio-Visual Navigation), a scalable framework enabling two agents
to collaboratively localize and navigate toward an audio target in shared 3D
environments. By integrating cross-agent communication protocols and joint
audio-visual fusion mechanisms, MASTAVN enhances spatial reasoning and temporal
synchronization. Through rigorous evaluation in photorealistic 3D simulators
(Replica and Matterport3D), MASTAVN achieves significant reductions in task
completion time and notable improvements in navigation success rates compared
to single-agent and non-collaborative baselines. This highlights the essential
role of spatiotemporal coordination in multi-agent systems. Our findings
validate MASTAVN's effectiveness in time-sensitive emergency scenarios and
establish a paradigm for advancing scalable multi-agent embodied intelligence
in complex 3D environments.

</details>


### [902] [Self-driving cars: Are we there yet?](https://arxiv.org/abs/2509.22754)
*Merve Atasever,Zhuochen Liu,Qingpei Li,Akshay Hitendra Shah,Hans Walker,Jyotirmoy V. Deshmukh,Rahul Jain*

Main category: cs.RO

TL;DR: 本文对CARLA、nuPlan和Waymo Open Dataset三个排行榜的运动规划方法进行综合比较分析，确定趋势、挑战并给出研究方向。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶是活跃研究领域，已有多个竞争平台和基准，为推进运动规划研究，需对三个领先排行榜的方法进行比较分析。

Method: 采用CARLA排行榜v2.0作为通用评估平台，修改所选模型以确保兼容性。

Result: 文中未明确提及具体结果。

Conclusion: 识别出当前方法的优缺点、流行趋势、常见挑战，并提出运动规划研究的潜在方向。

Abstract: Autonomous driving remains a highly active research domain that seeks to
enable vehicles to perceive dynamic environments, predict the future
trajectories of traffic agents such as vehicles, pedestrians, and cyclists and
plan safe and efficient future motions. To advance the field, several
competitive platforms and benchmarks have been established to provide
standardized datasets and evaluation protocols. Among these, leaderboards by
the CARLA organization and nuPlan and the Waymo Open Dataset have become
leading benchmarks for assessing motion planning algorithms. Each offers a
unique dataset and challenging planning problems spanning a wide range of
driving scenarios and conditions. In this study, we present a comprehensive
comparative analysis of the motion planning methods featured on these three
leaderboards. To ensure a fair and unified evaluation, we adopt CARLA
leaderboard v2.0 as our common evaluation platform and modify the selected
models for compatibility. By highlighting the strengths and weaknesses of
current approaches, we identify prevailing trends, common challenges, and
suggest potential directions for advancing motion planning research.

</details>


### [903] [Persistent Autoregressive Mapping with Traffic Rules for Autonomous Driving](https://arxiv.org/abs/2509.22756)
*Shiyi Liang,Xinyuan Chang,Changjie Wu,Huiyuan Yan,Yifan Bai,Xinran Liu,Hang Zhang,Yujian Yuan,Shuang Zeng,Mu Xu,Xing Wei*

Main category: cs.RO

TL;DR: 提出PAMR框架进行车道向量和交通规则的自回归联合构建，开发MapDRv2评估，实验表明PAMR性能优越且能保持规则持久有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法不能同时实现准确高清地图构建和对交通规则的持久感知，无法捕捉规则在长驾驶序列中的持久有效性。

Method: 提出PAMR框架，引入地图 - 规则联合构建和地图 - 规则缓存机制，开发MapDRv2进行评估。

Result: PAMR在联合向量 - 规则映射任务中表现优越，能在长驾驶序列中保持规则的持久有效性。

Conclusion: PAMR是解决安全自动驾驶中地图构建和规则持久感知问题的有效方法。

Abstract: Safe autonomous driving requires both accurate HD map construction and
persistent awareness of traffic rules, even when their associated signs are no
longer visible. However, existing methods either focus solely on geometric
elements or treat rules as temporary classifications, failing to capture their
persistent effectiveness across extended driving sequences. In this paper, we
present PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel
framework that performs autoregressive co-construction of lane vectors and
traffic rules from visual observations. Our approach introduces two key
mechanisms: Map-Rule Co-Construction for processing driving scenes in temporal
segments, and Map-Rule Cache for maintaining rule consistency across these
segments. To properly evaluate continuous and consistent map generation, we
develop MapDRv2, featuring improved lane geometry annotations. Extensive
experiments demonstrate that PAMR achieves superior performance in joint
vector-rule mapping tasks, while maintaining persistent rule effectiveness
throughout extended driving sequences.

</details>


### [904] [Dynamic Buffers: Cost-Efficient Planning for Tabletop Rearrangement with Stacking](https://arxiv.org/abs/2509.22828)
*Arman Barghi,Hamed Hosseini,Seraj Ghasemi,Mehdi Tale Masouleh,Ahmad Kalhor*

Main category: cs.RO

TL;DR: 提出动态缓冲区规划原语，提升杂乱桌面环境中物体重排规划的可行性与效率。


<details>
  <summary>Details</summary>
Motivation: 传统规划器在杂乱桌面环境物体重排中效率低、成本高，静态堆叠有局限性。

Method: 引入受人类分组策略启发的动态缓冲区规划原语，使机器人形成可移动临时堆叠。

Result: 与先进重排规划器相比，在不同场景减少操作器移动成本，通过实验验证实用性。

Conclusion: 动态缓冲是高效且稳健的重排规划关键原语。

Abstract: Rearranging objects in cluttered tabletop environments remains a
long-standing challenge in robotics. Classical planners often generate
inefficient, high-cost plans by shuffling objects individually and using fixed
buffers--temporary spaces such as empty table regions or static stacks--to
resolve conflicts. When only free table locations are used as buffers, dense
scenes become inefficient, since placing an object can restrict others from
reaching their goals and complicate planning. Allowing stacking provides extra
buffer capacity, but conventional stacking is static: once an object supports
another, the base cannot be moved, which limits efficiency. To overcome these
issues, a novel planning primitive called the Dynamic Buffer is introduced.
Inspired by human grouping strategies, it enables robots to form temporary,
movable stacks that can be transported as a unit. This improves both
feasibility and efficiency in dense layouts, and it also reduces travel in
large-scale settings where space is abundant. Compared with a state-of-the-art
rearrangement planner, the approach reduces manipulator travel cost by 11.89%
in dense scenarios with a stationary robot and by 5.69% in large, low-density
settings with a mobile manipulator. Practicality is validated through
experiments on a Delta parallel robot with a two-finger gripper. These findings
establish dynamic buffering as a key primitive for cost-efficient and robust
rearrangement planning.

</details>


### [905] [Open-Vocabulary Spatio-Temporal Scene Graph for Robot Perception and Teleoperation Planning](https://arxiv.org/abs/2509.23107)
*Yi Wang,Zeyu Xue,Mujie Liu,Tongqin Zhang,Yan Hu,Zhou Zhao,Chenguang Yang,Zhenyu Lu*

Main category: cs.RO

TL;DR: 本文提出ST - OVSG以解决远程操作中传输延迟问题，实验显示其性能优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 远程操作中双向通信的传输延迟会导致远程感知状态与操作者意图不一致，引发命令误解和错误执行，需解决该问题。

Method: 引入Spatio - Temporal Open - Vocabulary Scene Graph (ST - OVSG)，利用LVLMs构建3D对象表示并扩展到时间域，嵌入延迟标签，还提出任务导向的子图过滤策略。

Result: 在Replica基准测试中节点准确率达74%，优于ConceptGraph；在延迟鲁棒性实验中，LVLM规划器辅助下规划成功率达70.5%。

Conclusion: ST - OVSG能泛化到新类别，增强规划对传输延迟的鲁棒性，且无需微调。

Abstract: Teleoperation via natural-language reduces operator workload and enhances
safety in high-risk or remote settings. However, in dynamic remote scenes,
transmission latency during bidirectional communication creates gaps between
remote perceived states and operator intent, leading to command
misunderstanding and incorrect execution. To mitigate this, we introduce the
Spatio-Temporal Open-Vocabulary Scene Graph (ST-OVSG), a representation that
enriches open-vocabulary perception with temporal dynamics and lightweight
latency annotations. ST-OVSG leverages LVLMs to construct open-vocabulary 3D
object representations, and extends them into the temporal domain via Hungarian
assignment with our temporal matching cost, yielding a unified spatio-temporal
scene graph. A latency tag is embedded to enable LVLM planners to
retrospectively query past scene states, thereby resolving local-remote state
mismatches caused by transmission delays. To further reduce redundancy and
highlight task-relevant cues, we propose a task-oriented subgraph filtering
strategy that produces compact inputs for the planner. ST-OVSG generalizes to
novel categories and enhances planning robustness against transmission latency
without requiring fine-tuning. Experiments show that our method achieves 74
percent node accuracy on the Replica benchmark, outperforming ConceptGraph.
Notably, in the latency-robustness experiment, the LVLM planner assisted by
ST-OVSG achieved a planning success rate of 70.5 percent.

</details>


### [906] [Liaohe-CobotMagic-PnP: an Imitation Learning Dataset of Intelligent Robot for Industrial Applications](https://arxiv.org/abs/2509.23111)
*Chen Yizhe,Wang Qi,Hu Dongxiao,Jingzhe Fang,Liu Sichao,Zixin An,Hongliang Niu,Haoran Liu,Li Dong,Chuanfen Feng,Lan Dapeng,Liu Yu,Zhibo Pang*

Main category: cs.RO

TL;DR: 提出工业级多模态干扰数据集用于机器人感知和控制，实验表明可增强模型验证鲁棒性和提升机器人操作稳定性，数据集公开。


<details>
  <summary>Details</summary>
Motivation: 工业4.0应用中，动态环境干扰使环境状态与机器人行为存在高度非线性和强耦合，当前机器人数据集难以有效通过多模态传感器数据融合表示动态环境状态。

Method: 设计工业级多模态干扰数据集，集成多维度干扰特征，用高精度传感器同步采集多类型测量数据，设置高几何相似度和标准化光照梯度场景，通过ROS实现微秒级时间同步和抗振数据采集协议。

Result: 数据集增强了模型验证鲁棒性，提高了机器人在动态、多干扰环境中的操作稳定性。

Conclusion: 所提出的数据集对机器人在复杂环境下的感知和控制有积极作用，且已公开供使用。

Abstract: In Industry 4.0 applications, dynamic environmental interference induces
highly nonlinear and strongly coupled interactions between the environmental
state and robotic behavior. Effectively representing dynamic environmental
states through multimodal sensor data fusion remains a critical challenge in
current robotic datasets. To address this, an industrial-grade multimodal
interference dataset is presented, designed for robotic perception and control
under complex conditions. The dataset integrates multi-dimensional interference
features including size, color, and lighting variations, and employs
high-precision sensors to synchronously collect visual, torque, and joint-state
measurements. Scenarios with geometric similarity exceeding 85\% and
standardized lighting gradients are included to ensure real-world
representativeness. Microsecond-level time-synchronization and
vibration-resistant data acquisition protocols, implemented via the Robot
Operating System (ROS), guarantee temporal and operational fidelity.
Experimental results demonstrate that the dataset enhances model validation
robustness and improves robotic operational stability in dynamic,
interference-rich environments. The dataset is publicly available
at:https://modelscope.cn/datasets/Liaoh_LAB/Liaohe-CobotMagic-PnP.

</details>


### [907] [Leave No Observation Behind: Real-time Correction for VLA Action Chunks](https://arxiv.org/abs/2509.23224)
*Kohei Sendai,Maxime Alvarez,Tatsuya Matsushima,Yutaka Matsuo,Yusuke Iwasawa*

Main category: cs.RO

TL;DR: 提出轻量级实时块校正头A2C2，改进VLA模型，提升成功率和鲁棒性，是实时控制有效插件机制。


<details>
  <summary>Details</summary>
Motivation: VLA模型的动作分块在推理延迟和长视野下损害反应性，需改进。

Method: 引入A2C2，结合最新观察、VLA预测动作等输出每步校正，无需重新训练基础策略。

Result: 在任务套件上提升成功率，改进长视野鲁棒性，校正头开销小。

Conclusion: A2C2是实时控制中部署高容量分块策略的有效插件机制。

Abstract: To improve efficiency and temporal coherence, Vision-Language-Action (VLA)
models often predict action chunks; however, this action chunking harms
reactivity under inference delay and long horizons. We introduce Asynchronous
Action Chunk Correction (A2C2), which is a lightweight real-time chunk
correction head that runs every control step and adds a time-aware correction
to any off-the-shelf VLA's action chunk. The module combines the latest
observation, the predicted action from VLA (base action), a positional feature
that encodes the index of the base action within the chunk, and some features
from the base policy, then outputs a per-step correction. This preserves the
base model's competence while restoring closed-loop responsiveness. The
approach requires no retraining of the base policy and is orthogonal to
asynchronous execution schemes such as Real Time Chunking (RTC). On the dynamic
Kinetix task suite (12 tasks) and LIBERO Spatial, our method yields consistent
success rate improvements across increasing delays and execution horizons (+23%
point and +7% point respectively, compared to RTC), and also improves
robustness for long horizons even with zero injected delay. Since the
correction head is small and fast, there is minimal overhead compared to the
inference of large VLA models. These results indicate that A2C2 is an
effective, plug-in mechanism for deploying high-capacity chunking policies in
real-time control.

</details>


### [908] [Online Dynamic Goal Recognition in Gym Environments](https://arxiv.org/abs/2509.23244)
*Shamir Matan,Elhadad Osher,Nageris Ben,Mirsky Reuth*

Main category: cs.RO

TL;DR: 介绍了用于目标识别（GR）的两个开源框架 gr - libs 和 gr - envs，提供标准化、可扩展和可重复的平台。


<details>
  <summary>Details</summary>
Motivation: 当前目标识别领域因基准、领域和评估协议不一致而碎片化，需要统一平台。

Method: 引入 gr - libs 和 gr - envs 两个互补的开源框架，gr - libs 包含基于 MDP 的 GR 基线等实现，gr - envs 提供适配环境及兼容包装器。

Result: 提供了标准化、可扩展和可重复的平台用于推进 GR 研究，两个包均开源且可在 GitHub 和 PyPI 获取。

Conclusion: 这两个框架有助于解决目标识别领域碎片化问题，推动该领域研究发展。

Abstract: Goal Recognition (GR) is the task of inferring an agent's intended goal from
partial observations of its behavior, typically in an online and one-shot
setting. Despite recent advances in model-free GR, particularly in applications
such as human-robot interaction, surveillance, and assistive systems, the field
remains fragmented due to inconsistencies in benchmarks, domains, and
evaluation protocols.
  To address this, we introduce gr-libs
(https://github.com/MatanShamir1/gr_libs) and gr-envs
(https://github.com/MatanShamir1/gr_envs), two complementary open-source
frameworks that support the development, evaluation, and comparison of GR
algorithms in Gym-compatible environments. gr-libs includes modular
implementations of MDP-based GR baselines, diagnostic tools, and evaluation
utilities. gr-envs provides a curated suite of environments adapted for dynamic
and goal-directed behavior, along with wrappers that ensure compatibility with
standard reinforcement learning toolkits. Together, these libraries offer a
standardized, extensible, and reproducible platform for advancing GR research.
Both packages are open-source and available on GitHub and PyPI.

</details>


### [909] [Space Robotics Bench: Robot Learning Beyond Earth](https://arxiv.org/abs/2509.23328)
*Andrej Orsula,Matthieu Geist,Miguel Olivares-Mendez,Carol Martinez*

Main category: cs.RO

TL;DR: 为解决太空机器人学习中技术演示成本高和数据有限问题，提出开源模拟框架Space Robotics Bench，实验展示其在培养现实可用策略上的效用。


<details>
  <summary>Details</summary>
Motivation: 太空探索对自主系统需求增长，但机器人学习在该领域受技术演示成本和数据限制，需解决此问题。

Method: 引入Space Robotics Bench框架，其模块化架构集成程序生成与并行模拟环境，含系列基准任务，用标准强化学习算法建立基线并进行实验。

Result: 揭示当前方法局限性，证明框架能产生可用于现实操作的策略。

Conclusion: Space Robotics Bench是开发、基准测试和部署太空所需强大自主系统的宝贵资源。

Abstract: The growing ambition for space exploration demands robust autonomous systems
that can operate in unstructured environments under extreme extraterrestrial
conditions. The adoption of robot learning in this domain is severely hindered
by the prohibitive cost of technology demonstrations and the limited
availability of data. To bridge this gap, we introduce the Space Robotics
Bench, an open-source simulation framework for robot learning in space. It
offers a modular architecture that integrates on-demand procedural generation
with massively parallel simulation environments to support the creation of vast
and diverse training distributions for learning-based agents. To ground
research and enable direct comparison, the framework includes a comprehensive
suite of benchmark tasks that span a wide range of mission-relevant scenarios.
We establish performance baselines using standard reinforcement learning
algorithms and present a series of experimental case studies that investigate
key challenges in generalization, end-to-end learning, adaptive control, and
sim-to-real transfer. Our results reveal insights into the limitations of
current methods and demonstrate the utility of the framework in producing
policies capable of real-world operation. These contributions establish the
Space Robotics Bench as a valuable resource for developing, benchmarking, and
deploying the robust autonomous systems required for the final frontier.

</details>


### [910] [Multi-Modal Manipulation via Multi-Modal Policy Consensus](https://arxiv.org/abs/2509.23468)
*Haonan Chen,Jiaming Xu,Hongyu Chen,Kaiwen Hong,Binghao Huang,Chaoqi Liu,Jiayuan Mao,Yunzhu Li,Yilun Du,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: 提出将策略分解为扩散模型集并使用路由网络自适应组合各模态贡献的方法，在模拟和真实操作任务中表现优于特征拼接基线，且对物理扰动和传感器损坏有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统特征拼接方法在多模态融合用于机器人操作时存在不足，主导模态会掩盖关键稀疏信号，整体架构难以灵活纳入新或缺失模态。

Method: 将策略分解为一组扩散模型，每个模型专门处理单一表示，使用路由网络学习共识权重来自适应组合各模型贡献，实现新表示的增量添加。

Result: 在模拟和真实操作任务中显著优于特征拼接基线，对物理扰动和传感器损坏表现出鲁棒性，通过基于扰动的重要性分析揭示了模态间的自适应转移。

Conclusion: 所提方法能有效解决多模态融合问题，实现灵活的多模态推理，对物理扰动和传感器损坏具有鲁棒性。

Abstract: Effectively integrating diverse sensory modalities is crucial for robotic
manipulation. However, the typical approach of feature concatenation is often
suboptimal: dominant modalities such as vision can overwhelm sparse but
critical signals like touch in contact-rich tasks, and monolithic architectures
cannot flexibly incorporate new or missing modalities without retraining. Our
method factorizes the policy into a set of diffusion models, each specialized
for a single representation (e.g., vision or touch), and employs a router
network that learns consensus weights to adaptively combine their
contributions, enabling incremental of new representations. We evaluate our
approach on simulated manipulation tasks in {RLBench}, as well as real-world
tasks such as occluded object picking, in-hand spoon reorientation, and puzzle
insertion, where it significantly outperforms feature-concatenation baselines
on scenarios requiring multimodal reasoning. Our policy further demonstrates
robustness to physical perturbations and sensor corruption. We further conduct
perturbation-based importance analysis, which reveals adaptive shifts between
modalities.

</details>


### [911] [RAVEN: Resilient Aerial Navigation via Open-Set Semantic Memory and Behavior Adaptation](https://arxiv.org/abs/2509.23563)
*Seungchan Kim,Omar Alama,Dmytro Kurdydyk,John Keller,Nikhil Keetha,Wenshan Wang,Yonatan Bisk,Sebastian Scherer*

Main category: cs.RO

TL;DR: 提出用于非结构化户外环境的基于3D记忆的行为树框架RAVEN，在模拟和实际户外测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有室内语义导航方法受空间范围和布局限制，户外语义导航方法存在短视或适应性差的问题，需适用于非结构化户外环境的导航方法。

Method: 使用空间一致的语义体素 - 射线图作为持久内存，结合短程体素搜索和长程射线搜索，利用大视觉 - 语言模型提供辅助线索，通过行为树协调各组件。

Result: 在10个逼真户外模拟环境的100个语义任务中，RAVEN比基线方法性能高85.25%，并在户外实地测试中证明其实用性。

Conclusion: RAVEN是一种有效的用于非结构化户外环境的空中语义导航方法。

Abstract: Aerial outdoor semantic navigation requires robots to explore large,
unstructured environments to locate target objects. Recent advances in semantic
navigation have demonstrated open-set object-goal navigation in indoor
settings, but these methods remain limited by constrained spatial ranges and
structured layouts, making them unsuitable for long-range outdoor search. While
outdoor semantic navigation approaches exist, they either rely on reactive
policies based on current observations, which tend to produce short-sighted
behaviors, or precompute scene graphs offline for navigation, limiting
adaptability to online deployment. We present RAVEN, a 3D memory-based,
behavior tree framework for aerial semantic navigation in unstructured outdoor
environments. It (1) uses a spatially consistent semantic voxel-ray map as
persistent memory, enabling long-horizon planning and avoiding purely reactive
behaviors, (2) combines short-range voxel search and long-range ray search to
scale to large environments, (3) leverages a large vision-language model to
suggest auxiliary cues, mitigating sparsity of outdoor targets. These
components are coordinated by a behavior tree, which adaptively switches
behaviors for robust operation. We evaluate RAVEN in 10 photorealistic outdoor
simulation environments over 100 semantic tasks, encompassing single-object
search, multi-class, multi-instance navigation and sequential task changes.
Results show RAVEN outperforms baselines by 85.25% in simulation and
demonstrate its real-world applicability through deployment on an aerial robot
in outdoor field tests.

</details>


### [912] [Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models](https://arxiv.org/abs/2509.23655)
*Rokas Bendikas,Daniel Dijkman,Markus Peschl,Sanjay Haresh,Pietro Mazzaglia*

Main category: cs.RO

TL;DR: 提出Oat - VLA方法，可高效训练VLA模型，减少视觉标记数量，在任务中表现优于OpenVLA。


<details>
  <summary>Details</summary>
Motivation: 解决将预训练视觉语言模型（VLM）用于机器人领域时计算成本过高的问题。

Method: 提出以对象 - 主体为中心的标记化方法Oat - VLA，引入对场景对象和主体自身视觉信息的归纳偏差。

Result: Oat - VLA能大幅减少视觉标记数量且不牺牲性能，在LIBERO套件上收敛速度至少是OpenVLA的两倍，在现实世界的拾取和放置任务中表现更优。

Conclusion: Oat - VLA可实现高效的视觉 - 语言 - 动作（VLA）模型训练。

Abstract: Vision-Language-Action (VLA) models offer a pivotal approach to learning
robotic manipulation at scale by repurposing large pre-trained
Vision-Language-Models (VLM) to output robotic actions. However, adapting VLMs
for robotic domains comes with an unnecessarily high computational cost, which
we attribute to the tokenization scheme of visual inputs. In this work, we aim
to enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric
Tokenization for VLAs. Building on the insights of object-centric
representation learning, our method introduces an inductive bias towards scene
objects and the agent's own visual information. As a result, we find that
Oat-VLA can drastically reduce the number of visual tokens to just a few tokens
without sacrificing performance. We reveal that Oat-VLA converges at least
twice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in
diverse real-world pick and place tasks.

</details>


### [913] [LocoFormer: Generalist Locomotion via Long-context Adaptation](https://arxiv.org/abs/2509.23745)
*Min Liu,Deepak Pathak,Ananye Agarwal*

Main category: cs.RO

TL;DR: 提出通用的LocoFormer模型，可控制未知机器人，通过大规模RL训练和扩展上下文实现适应，在多种场景表现良好，有望用于训练其他机器人技能基础模型。


<details>
  <summary>Details</summary>
Motivation: 现代运动控制器需针对特定形态手动调整，需要能控制未知机器人的通用模型。

Method: 在程序生成的机器人上进行大规模强化学习训练，采用激进的领域随机化，将上下文扩展到跨越情节边界。

Result: 将LocoFormer部署到不同机器人，即使有大干扰也能实现稳健控制，在极端场景有跨情节的适应表现。

Conclusion: 此简单通用方法未来可用于训练其他机器人技能的基础模型。

Abstract: Modern locomotion controllers are manually tuned for specific embodiments. We
present LocoFormer, a generalist omni-bodied locomotion model that can control
previously unseen legged and wheeled robots, even without precise knowledge of
their kinematics. LocoFormer is able to adapt to changes in morphology and
dynamics at test time. We find that two key choices enable adaptation. First,
we train massive scale RL on procedurally generated robots with aggressive
domain randomization. Second, in contrast to previous policies that are myopic
with short context lengths, we extend context by orders of magnitude to span
episode boundaries. We deploy the same LocoFormer to varied robots and show
robust control even with large disturbances such as weight change and motor
failures. In extreme scenarios, we see emergent adaptation across episodes,
LocoFormer learns from falls in early episodes to improve control strategies in
later ones. We believe that this simple, yet general recipe can be used to
train foundation models for other robotic skills in the future. Videos at
generalist-locomotion.github.io.

</details>


### [914] [Robot Learning from Any Images](https://arxiv.org/abs/2509.22970)
*Siheng Zhao,Jiageng Mao,Wei Chow,Zeyu Shangguan,Tianheng Shi,Rong Xue,Yuxi Zheng,Yijia Weng,Yang You,Daniel Seita,Leonidas Guibas,Sergey Zakharov,Vitor Guizilini,Yue Wang*

Main category: cs.RO

TL;DR: 介绍RoLA框架，可将任意图像转化为交互式、支持物理的机器人环境，无需额外硬件或数字资产，能快速生成大量机器人视觉运动演示数据。


<details>
  <summary>Details</summary>
Motivation: 实现机器人数据生成的大众化，从多种图像源快速生成大量机器人视觉运动演示数据。

Method: 结合单视图物理场景恢复新方法和高效视觉融合策略进行逼真数据收集。

Result: 展示了RoLA在可扩展机器人数据生成与增强、从互联网图像进行机器人学习、单图像实-虚-实系统等应用中的多功能性。

Conclusion: RoLA框架有效且多功能，能在多个机器人应用场景中发挥作用。

Abstract: We introduce RoLA, a framework that transforms any in-the-wild image into an
interactive, physics-enabled robotic environment. Unlike previous methods, RoLA
operates directly on a single image without requiring additional hardware or
digital assets. Our framework democratizes robotic data generation by producing
massive visuomotor robotic demonstrations within minutes from a wide range of
image sources, including camera captures, robotic datasets, and Internet
images. At its core, our approach combines a novel method for single-view
physical scene recovery with an efficient visual blending strategy for
photorealistic data collection. We demonstrate RoLA's versatility across
applications like scalable robotic data generation and augmentation, robot
learning from Internet images, and single-image real-to-sim-to-real systems for
manipulators and humanoids. Video results are available at
https://sihengz02.github.io/RoLA .

</details>


### [915] [Sequence Pathfinder for Multi-Agent Pickup and Delivery in the Warehouse](https://arxiv.org/abs/2509.23778)
*Zeyuan Zhang,Chaoran Li,Shao Zhang,Ying Wen*

Main category: cs.RO

TL;DR: 提出Sequential Pathfinder (SePar)解决MAPD问题，在多任务中表现优于现有方法且泛化性好。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法在MAPD的仓库类环境中表现差，通信学习计算复杂度高。

Method: 将MAPF建模为序列问题，证明其路径策略的顺序不变最优性，提出SePar利用Transformer实现隐式信息交换。

Result: SePar在各种MAPF任务及变体中始终优于现有基于学习的方法，在未见环境中泛化性好。

Conclusion: SePar有效解决MAPD问题，在复杂地图中集成模仿学习很有必要。

Abstract: Multi-Agent Pickup and Delivery (MAPD) is a challenging extension of
Multi-Agent Path Finding (MAPF), where agents are required to sequentially
complete tasks with fixed-location pickup and delivery demands. Although
learning-based methods have made progress in MAPD, they often perform poorly in
warehouse-like environments with narrow pathways and long corridors when
relying only on local observations for distributed decision-making.
Communication learning can alleviate the lack of global information but
introduce high computational complexity due to point-to-point communication. To
address this challenge, we formulate MAPF as a sequence modeling problem and
prove that path-finding policies under sequence modeling possess
order-invariant optimality, ensuring its effectiveness in MAPD. Building on
this, we propose the Sequential Pathfinder (SePar), which leverages the
Transformer paradigm to achieve implicit information exchange, reducing
decision-making complexity from exponential to linear while maintaining
efficiency and global awareness. Experiments demonstrate that SePar
consistently outperforms existing learning-based methods across various MAPF
tasks and their variants, and generalizes well to unseen environments.
Furthermore, we highlight the necessity of integrating imitation learning in
complex maps like warehouses.

</details>


### [916] [EKF-Based Fusion of Wi-Fi/LiDAR/IMU for Indoor Localization and Navigation](https://arxiv.org/abs/2509.23118)
*Zeyi Li,Zhe Tang,Kyeong Soo Kim,Sihao Li,Jeremy S. Smith*

Main category: cs.RO

TL;DR: 提出结合Wi-Fi RSSI指纹、LiDAR-SLAM和IMU导航的室内定位框架，实验证明能抑制单一方法不稳定性，提供稳定精度。


<details>
  <summary>Details</summary>
Motivation: 传统Wi-Fi RSSI指纹定位精度低，基于LiDAR的方案部署成本和复杂度高，无法满足室内定位导航需求。

Method: 提出基于扩展卡尔曼滤波器（EKF）的室内定位导航框架，先用DNN的Wi-Fi RSSI指纹粗定位，再用IMU动态定位和Gmapping-SLAM细化，最后EKF融合传感器信息抑制噪声和漂移。

Result: 在实际场景实验中，该框架二维平均误差在0.2449 - 0.3781米，而Wi-Fi RSSI指纹在信号干扰区达1.3404米，LiDAR/IMU定位因累积漂移误差在0.6233 - 2.8803米。

Conclusion: 所提多传感器融合框架能抑制单一方法的不稳定性，在各路径配置下提供稳定精度。

Abstract: Conventional Wi-Fi received signal strength indicator (RSSI) fingerprinting
cannot meet the growing demand for accurate indoor localization and navigation
due to its lower accuracy, while solutions based on light detection and ranging
(LiDAR) can provide better localization performance but is limited by their
higher deployment cost and complexity. To address these issues, we propose a
novel indoor localization and navigation framework integrating Wi-Fi RSSI
fingerprinting, LiDAR-based simultaneous localization and mapping (SLAM), and
inertial measurement unit (IMU) navigation based on an extended Kalman filter
(EKF). Specifically, coarse localization by deep neural network (DNN)-based
Wi-Fi RSSI fingerprinting is refined by IMU-based dynamic positioning using a
Gmapping-based SLAM to generate an occupancy grid map and output high-frequency
attitude estimates, which is followed by EKF prediction-update integrating
sensor information while effectively suppressing Wi-Fi-induced noise and IMU
drift errors. Multi-group real-world experiments conducted on the IR building
at Xi'an Jiaotong-Liverpool University demonstrates that the proposed
multi-sensor fusion framework suppresses the instability caused by individual
approaches and thereby provides stable accuracy across all path configurations
with mean two-dimensional (2D) errors ranging from 0.2449 m to 0.3781 m. In
contrast, the mean 2D errors of Wi-Fi RSSI fingerprinting reach up to 1.3404 m
in areas with severe signal interference, and those of LiDAR/IMU localization
are between 0.6233 m and 2.8803 m due to cumulative drift.

</details>


### [917] [MAD-PINN: A Decentralized Physics-Informed Machine Learning Framework for Safe and Optimal Multi-Agent Control](https://arxiv.org/abs/2509.23960)
*Manan Tayal,Aditya Singh,Shishir Kolathaya,Somil Bansal*

Main category: cs.RO

TL;DR: 提出MAD - PINN框架解决多智能体状态约束最优控制问题，实验显示其在安全 - 性能权衡、可扩展性上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统安全与性能协同优化方法存在缺乏严格安全保证、保守或难以有效扩展的问题。

Method: 提出MAD - PINN框架，利用基于上图像的重写方法，通过物理信息神经网络近似求解，在简化智能体系统上训练价值函数并分散部署，引入基于HJ可达性的邻居选择策略和滚动时域策略执行方案。

Result: 在多智能体导航任务实验中，MAD - PINN实现了更好的安全 - 性能权衡，随智能体数量增加保持可扩展性，持续超越现有基线方法。

Conclusion: MAD - PINN是解决多智能体状态约束最优控制问题的有效框架。

Abstract: Co-optimizing safety and performance in large-scale multi-agent systems
remains a fundamental challenge. Existing approaches based on multi-agent
reinforcement learning (MARL), safety filtering, or Model Predictive Control
(MPC) either lack strict safety guarantees, suffer from conservatism, or fail
to scale effectively. We propose MAD-PINN, a decentralized physics-informed
machine learning framework for solving the multi-agent state-constrained
optimal control problem (MASC-OCP). Our method leverages an epigraph-based
reformulation of SC-OCP to simultaneously capture performance and safety, and
approximates its solution via a physics-informed neural network. Scalability is
achieved by training the SC-OCP value function on reduced-agent systems and
deploying them in a decentralized fashion, where each agent relies only on
local observations of its neighbours for decision-making. To further enhance
safety and efficiency, we introduce an Hamilton-Jacobi (HJ) reachability-based
neighbour selection strategy to prioritize safety-critical interactions, and a
receding-horizon policy execution scheme that adapts to dynamic interactions
while reducing computational burden. Experiments on multi-agent navigation
tasks demonstrate that MAD-PINN achieves superior safety-performance
trade-offs, maintains scalability as the number of agents grows, and
consistently outperforms state-of-the-art baselines.

</details>


### [918] [Ancestry Tree Clustering for Particle Filter Diversity Maintenance](https://arxiv.org/abs/2509.24124)
*Ilari Vallivaara,Bingnan Duan,Yinhuan Dong,Tughrul Arslan*

Main category: cs.RO

TL;DR: 提出线性时间粒子滤波多样性维护方法，聚类粒子，在多模态环境验证有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决粒子滤波在多模态环境中过早收敛问题，同时保持估计紧凑性。

Method: 基于祖先树拓扑对粒子聚类，结合簇内适应度共享和保护非聚类粒子。

Result: 在多模态机器人仿真和真实室内环境验证，与多种算法对比，成功率高且对紧凑性无负面影响。

Conclusion: 该算法对不同领域和复杂初始条件具有鲁棒性。

Abstract: We propose a method for linear-time diversity maintenance in particle
filtering. It clusters particles based on ancestry tree topology: closely
related particles in sufficiently large subtrees are grouped together. The main
idea is that the tree structure implicitly encodes similarity without the need
for spatial or other domain-specific metrics. This approach, when combined with
intra-cluster fitness sharing and the protection of particles not included in a
cluster, effectively prevents premature convergence in multimodal environments
while maintaining estimate compactness. We validate our approach in a
multimodal robotics simulation and a real-world multimodal indoor environment.
We compare the performance to several diversity maintenance algorithms from the
literature, including Deterministic Resampling and Particle Gaussian Mixtures.
Our algorithm achieves high success rates with little to no negative effect on
compactness, showing particular robustness to different domains and challenging
initial conditions.

</details>


### [919] [BOSfM: A View Planning Framework for Optimal 3D Reconstruction of Agricultural Scenes](https://arxiv.org/abs/2509.24126)
*Athanasios Bacharis,Konstantinos D. Polyzos,Georgios B. Giannakis,Nikolaos Papanikolopoulos*

Main category: cs.RO

TL;DR: 本文提出一种新的视图规划框架，用贝叶斯优化方法解决主动视觉中3D重建的视图规划问题，在模拟和真实农业场景测试中展现优势。


<details>
  <summary>Details</summary>
Motivation: 主动视觉中3D重建需高效视图规划，但存在相机位置和图像噪声以及泛化性问题。

Method: 提出基于重建质量优化公式，采用贝叶斯优化方法，考虑不同噪声情况。

Result: 在模拟和真实农业场景的数值测试表明，该方法能有效估计最优相机位置，准确重建3D环境，且在未知相似环境有良好泛化性。

Conclusion: 所提视图规划方法在主动视觉3D重建的视图规划中有效且具泛化性。

Abstract: Active vision (AV) has been in the spotlight of robotics research due to its
emergence in numerous applications including agricultural tasks such as
precision crop monitoring and autonomous harvesting to list a few. A major AV
problem that gained popularity is the 3D reconstruction of targeted
environments using 2D images from diverse viewpoints. While collecting and
processing a large number of arbitrarily captured 2D images can be arduous in
many practical scenarios, a more efficient solution involves optimizing the
placement of available cameras in 3D space to capture fewer, yet more
informative, images that provide sufficient visual information for effective
reconstruction of the environment of interest. This process termed as view
planning (VP), can be markedly challenged (i) by noise emerging in the location
of the cameras and/or in the extracted images, and (ii) by the need to
generalize well in other unknown similar agricultural environments without need
for re-optimizing or re-training. To cope with these challenges, the present
work presents a novel VP framework that considers a reconstruction
quality-based optimization formulation that relies on the notion of
`structure-from-motion' to reconstruct the 3D structure of the sought
environment from the selected 2D images. With no analytic expression of the
optimization function and with costly function evaluations, a Bayesian
optimization approach is proposed to efficiently carry out the VP process using
only a few function evaluations, while accounting for different noise cases.
Numerical tests on both simulated and real agricultural settings signify the
benefits of the advocated VP approach in efficiently estimating the optimal
camera placement to accurately reconstruct 3D environments of interest, and
generalize well on similar unknown environments.

</details>


### [920] [Memory Transfer Planning: LLM-driven Context-Aware Code Adaptation for Robot Manipulation](https://arxiv.org/abs/2509.24160)
*Tomoyuki Kagaya,Subramanian Lakshmi,Yuxuan Lou,Thong Jing Yuan,Jayashree Karlekar,Sugiri Pranata,Natsuki Murakami,Akira Kinose,Yang You*

Main category: cs.RO

TL;DR: 提出Memory Transfer Planning (MTP)框架，利用不同环境的成功控制代码示例进行LLM驱动规划，在多个实验中提升成功率和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在机器人操作中的方法难以适应新环境，可迁移性有限且需手动重新调整。

Method: MTP框架：用LLM生成初始计划和代码；从代码内存中检索相关成功示例；将检索到的代码上下文适应目标设置进行重新规划，不更新模型参数。

Result: 在RLBench、CALVIN和物理机器人上评估，MTP相比固定提示代码生成、简单检索和无记忆重新规划，持续提高了成功率和适应性，在硬件实验中利用模拟中构建的内存也有效。

Conclusion: MTP是一种实用方法，可利用程序知识实现跨多种机器人操作场景的强大基于LLM的规划，增强对新环境的适应性，弥合模拟和现实世界部署的差距。

Abstract: Large language models (LLMs) are increasingly explored in robot manipulation,
but many existing methods struggle to adapt to new environments. Many systems
require either environment-specific policy training or depend on fixed prompts
and single-shot code generation, leading to limited transferability and manual
re-tuning. We introduce Memory Transfer Planning (MTP), a framework that
leverages successful control-code examples from different environments as
procedural knowledge, using them as in-context guidance for LLM-driven
planning. Specifically, MTP (i) generates an initial plan and code using LLMs,
(ii) retrieves relevant successful examples from a code memory, and (iii)
contextually adapts the retrieved code to the target setting for re-planning
without updating model parameters. We evaluate MTP on RLBench, CALVIN, and a
physical robot, demonstrating effectiveness beyond simulation. Across these
settings, MTP consistently improved success rate and adaptability compared with
fixed-prompt code generation, naive retrieval, and memory-free re-planning.
Furthermore, in hardware experiments, leveraging a memory constructed in
simulation proved effective. MTP provides a practical approach that exploits
procedural knowledge to realize robust LLM-based planning across diverse
robotic manipulation scenarios, enhancing adaptability to novel environments
and bridging simulation and real-world deployment.

</details>


### [921] [ViReSkill: Vision-Grounded Replanning with Skill Memory for LLM-Based Planning in Lifelong Robot Learning](https://arxiv.org/abs/2509.24219)
*Tomoyuki Kagaya,Subramanian Lakshmi,Anbang Ye,Thong Jing Yuan,Jayashree Karlekar,Sugiri Pranata,Natsuki Murakami,Akira Kinose,Yang You*

Main category: cs.RO

TL;DR: 提出ViReSkill框架用于机器人运动规划，结合视觉重规划和技能记忆，在模拟和真实机器人上表现优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 现有通过强化学习或模仿学习训练的机器人适应新任务慢，使用大语言模型和视觉语言模型进行运动规划存在符号规划未结合场景几何和物体物理、输出不稳定的问题。

Method: 提出ViReSkill框架，结合视觉重规划和技能记忆，失败时重规划，成功时存储技能以便复用。

Result: 在LIBERO、RLBench等模拟器和物理机器人上进行评估，ViReSkill在任务成功率上始终优于传统基线。

Conclusion: ViReSkill框架具有强大的从模拟到真实的泛化能力，能实现自主持续学习。

Abstract: Robots trained via Reinforcement Learning (RL) or Imitation Learning (IL)
often adapt slowly to new tasks, whereas recent Large Language Models (LLMs)
and Vision-Language Models (VLMs) promise knowledge-rich planning from minimal
data. Deploying LLMs/VLMs for motion planning, however, faces two key
obstacles: (i) symbolic plans are rarely grounded in scene geometry and object
physics, and (ii) model outputs can vary for identical prompts, undermining
execution reliability. We propose ViReSkill, a framework that pairs
vision-grounded replanning with a skill memory for accumulation and reuse. When
a failure occurs, the replanner generates a new action sequence conditioned on
the current scene, tailored to the observed state. On success, the executed
plan is stored as a reusable skill and replayed in future encounters without
additional calls to LLMs/VLMs. This feedback loop enables autonomous continual
learning: each attempt immediately expands the skill set and stabilizes
subsequent executions. We evaluate ViReSkill on simulators such as LIBERO and
RLBench as well as on a physical robot. Across all settings, it consistently
outperforms conventional baselines in task success rate, demonstrating robust
sim-to-real generalization.

</details>


### [922] [SafeFlowMatcher: Safe and Fast Planning using Flow Matching with Control Barrier Functions](https://arxiv.org/abs/2509.24243)
*Jeongyong Yang,Seunghwan Jang,Soojean Han*

Main category: cs.RO

TL;DR: 提出SafeFlowMatcher规划框架，结合流匹配与控制障碍函数，实现实时高效与安全规划，在多基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于流匹配的生成式规划器无正式安全保证，在约束附近可能产生不完整路径，需要改进。

Method: 采用两阶段预测 - 校正积分器，先通过流匹配预测候选路径，再用向量场和基于控制障碍函数的二次规划校正路径，并证明了流系统的障碍证书。

Result: 在迷宫导航和运动基准测试中，能获得更快、更平滑和更安全的路径。

Conclusion: SafeFlowMatcher有效，预测 - 校正积分器和障碍证书有重要贡献。

Abstract: Generative planners based on flow matching (FM) can produce high-quality
paths in one or a few ODE steps, but their sampling dynamics offer no formal
safety guarantees and can yield incomplete paths near constraints. We present
SafeFlowMatcher, a planning framework that couples FM with control barrier
functions (CBFs) to achieve both real-time efficiency and certified safety.
SafeFlowMatcher uses a two-phase prediction-correction (PC) integrator: (i) a
prediction phase integrates the learned FM once (or a few steps) to obtain a
candidate path without intervention; (ii) a correction phase refines this path
with a vanishing time-scaled vector field and a CBF-based quadratic program
that minimally perturbs the vector field. We prove a barrier certificate for
the resulting flow system, establishing forward invariance of a robust safe set
and finite-time convergence to the safe set. By enforcing safety only on the
executed path (rather than on all intermediate latent paths), SafeFlowMatcher
avoids distributional drift and mitigates local trap problems. Across maze
navigation and locomotion benchmarks, SafeFlowMatcher attains faster, smoother,
and safer paths than diffusion- and FM-based baselines. Extensive ablations
corroborate the contributions of the PC integrator and the barrier certificate.

</details>


### [923] [Prompting Robot Teams with Natural Language](https://arxiv.org/abs/2509.24575)
*Nicolas Pfitzer,Eduardo Sebastián,Ajay Shankar,Amanda Prorok*

Main category: cs.RO

TL;DR: 本文提出用自然语言对多机器人团队进行高级任务提示的框架，利用语言模型能力，结合DFA、RNN和GNN实现多机器人协作。


<details>
  <summary>Details</summary>
Motivation: 利用语言模型理解和分解人类意图的推理能力，应用于多机器人协作和决策，解决集体中个体行为难指定和解释的问题。

Method: 将任务表示为DFA，用RNN编码自动机，将语言模型得到的子任务逻辑和顺序分解提炼到RNN中，训练基于RNN隐藏状态和语言嵌入的GNN控制策略。

Result: 在各种模拟和现实世界的多机器人任务中对单轻量级可解释模型进行了评估。

Conclusion: 所提出的框架能使机器人以分散方式执行与任务相关的动作，可用于需要团队顺序和协作行为的多机器人任务。

Abstract: This paper presents a framework towards prompting multi-robot teams with
high-level tasks using natural language expressions. Our objective is to use
the reasoning capabilities demonstrated by recent language models in
understanding and decomposing human expressions of intent, and repurpose these
for multi-robot collaboration and decision-making. The key challenge is that an
individual's behavior in a collective can be hard to specify and interpret, and
must continuously adapt to actions from others. This necessitates a framework
that possesses the representational capacity required by the logic and
semantics of a task, and yet supports decentralized and interactive real-time
operation. We solve this dilemma by recognizing that a task can be represented
as a deterministic finite automaton (DFA), and that recurrent neural networks
(RNNs) can encode numerous automata. This allows us to distill the logic and
sequential decompositions of sub-tasks obtained from a language model into an
RNN, and align its internal states with the semantics of a given task. By
training a graph neural network (GNN) control policy that is conditioned on the
hidden states of the RNN and the language embeddings, our method enables robots
to execute task-relevant actions in a decentralized manner. We present
evaluations of this single light-weight interpretable model on various
simulated and real-world multi-robot tasks that require sequential and
collaborative behavior by the team -- sites.google.com/view/prompting-teams.

</details>


### [924] [PhysiAgent: An Embodied Agent Framework in Physical World](https://arxiv.org/abs/2509.24524)
*Zhihao Wang,Jianxiong Li,Jinliang Zheng,Wencong Zhang,Dongxiu Liu,Yinan Zheng,Haoyi Niu,Junzhi Yu,Xianyuan Zhan*

Main category: cs.RO

TL;DR: 提出具身体智能体框架PhysiAgent，结合监控、记忆等机制，利用VLA实时反馈提示VLM组织组件，实验显示其在复杂机器人任务中性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有Vision - Language - Action (VLA)模型泛化能力有限，且与Vision - Language Models (VLMs)结合方式不佳，存在协作低效和接地性差问题。

Method: 提出PhysiAgent框架，融入监控、记忆、自我反思机制和轻量级现成工具箱，根据VLA实时反馈提示VLM组织组件。

Result: 在复杂现实机器人任务中任务解决性能显著提升，展示了VLM有效自我调节、工具协作连贯和框架执行时的自适应进化。

Conclusion: PhysiAgent在集成VLMs和VLAs方面做出了实际且开创性的努力，有效将具身智能体框架应用于现实场景。

Abstract: Vision-Language-Action (VLA) models have achieved notable success but often
struggle with limited generalizations. To address this, integrating generalized
Vision-Language Models (VLMs) as assistants to VLAs has emerged as a popular
solution. However, current approaches often combine these models in rigid,
sequential structures: using VLMs primarily for high-level scene understanding
and task planning, and VLAs merely as executors of lower-level actions, leading
to ineffective collaboration and poor grounding challenges. In this paper, we
propose an embodied agent framework, PhysiAgent, tailored to operate
effectively in physical environments. By incorporating monitor, memory,
self-reflection mechanisms, and lightweight off-the-shelf toolboxes, PhysiAgent
offers an autonomous scaffolding framework to prompt VLMs to organize different
components based on real-time proficiency feedback from VLAs to maximally
exploit VLAs' capabilities. Experimental results demonstrate significant
improvements in task-solving performance on complex real-world robotic tasks,
showcasing effective self-regulation of VLMs, coherent tool collaboration, and
adaptive evolution of the framework during execution. PhysiAgent makes
practical and pioneering efforts to integrate VLMs and VLAs, effectively
grounding embodied agent frameworks in real-world settings.

</details>


### [925] [Stabilizing Humanoid Robot Trajectory Generation via Physics-Informed Learning and Control-Informed Steering](https://arxiv.org/abs/2509.24697)
*Evelyn D'Elia,Paolo Maria Viceconte,Lorenzo Rapetti,Diego Ferigo,Giulio Romualdi,Giuseppe L'Erario,Raffaello Camoriano,Daniele Pucci*

Main category: cs.RO

TL;DR: 本文提出双管齐下学习策略，利用系统物理知识和控制原理，解决仿人机器人控制中模仿学习的局限性，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有仿人机器人控制的模仿学习方法受限于运动数据量，未结合物理定律，可能违反物理规律，影响现实稳定性。

Method: 采用双管齐下学习策略，一是在监督式模仿学习中编码物理先验，二是在推理时应用比例积分控制器减少漂移。

Result: 在ergoCub仿人机器人的各种运动行为上验证方法，物理信息损失鼓励零接触足速度。

Conclusion: 所提方法与真实机器人上的多个控制器兼容，显著提高生成轨迹的准确性和物理约束符合度。

Abstract: Recent trends in humanoid robot control have successfully employed imitation
learning to enable the learned generation of smooth, human-like trajectories
from human data. While these approaches make more realistic motions possible,
they are limited by the amount of available motion data, and do not incorporate
prior knowledge about the physical laws governing the system and its
interactions with the environment. Thus they may violate such laws, leading to
divergent trajectories and sliding contacts which limit real-world stability.
We address such limitations via a two-pronged learning strategy which leverages
the known physics of the system and fundamental control principles. First, we
encode physics priors during supervised imitation learning to promote
trajectory feasibility. Second, we minimize drift at inference time by applying
a proportional-integral controller directly to the generated output state. We
validate our method on various locomotion behaviors for the ergoCub humanoid
robot, where a physics-informed loss encourages zero contact foot velocity. Our
experiments demonstrate that the proposed approach is compatible with multiple
controllers on a real robot and significantly improves the accuracy and
physical constraint conformity of generated trajectories.

</details>


### [926] [PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control](https://arxiv.org/abs/2509.24591)
*Haozhuo Zhang,Michele Caprio,Jing Shao,Qiang Zhang,Jian Tang,Shanghang Zhang,Wei Pan*

Main category: cs.RO

TL;DR: 提出PoseDiff模型，统一机器人状态估计与控制，在数据集上表现优异，连接具身AI感知、规划与控制。


<details>
  <summary>Details</summary>
Motivation: 实现机器人状态估计和控制在单一框架内的统一，提高感知与控制集成的可扩展性和效率。

Method: 构建条件扩散模型PoseDiff，将原始视觉观测映射到结构化机器人状态，通过重叠平均策略生成平滑连续的长视野动作序列。

Result: 在DREAM数据集上姿态估计达到最优精度和实时性能，在Libero物体操作任务中显著提高成功率。

Conclusion: PoseDiff为具身AI中的感知、规划和控制提供了可扩展、准确且高效的桥梁。

Abstract: We present PoseDiff, a conditional diffusion model that unifies robot state
estimation and control within a single framework. At its core, PoseDiff maps
raw visual observations into structured robot states-such as 3D keypoints or
joint angles-from a single RGB image, eliminating the need for multi-stage
pipelines or auxiliary modalities. Building upon this foundation, PoseDiff
extends naturally to video-to-action inverse dynamics: by conditioning on
sparse video keyframes generated by world models, it produces smooth and
continuous long-horizon action sequences through an overlap-averaging strategy.
This unified design enables scalable and efficient integration of perception
and control. On the DREAM dataset, PoseDiff achieves state-of-the-art accuracy
and real-time performance for pose estimation. On Libero-Object manipulation
tasks, it substantially improves success rates over existing inverse dynamics
modules, even under strict offline settings. Together, these results show that
PoseDiff provides a scalable, accurate, and efficient bridge between
perception, planning, and control in embodied AI. The video visualization
results can be found on the project page:
https://haozhuo-zhang.github.io/PoseDiff-project-page/.

</details>


### [927] [Fidelity-Aware Data Composition for Robust Robot Generalization](https://arxiv.org/abs/2509.24797)
*Zizhao Tong,Di Chen,Sicheng Hu,Hongwei Fan,Liliang Chen,Guanghui Ren,Hao Tang,Hao Dong,Ling Shao*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Generalist robot policies trained on large-scale, visually homogeneous
datasets can be susceptible to shortcut learning, which impairs their
out-of-distribution (OOD) generalization. While generative data augmentation is
a common approach to introduce diversity, it presents a subtle challenge: data
composition. Naively mixing real and synthetic data can corrupt the learning
signal, as this process often prioritizes visual diversity at the expense of
information fidelity. This paper suggests that robust generalization depends on
principled, fidelity-aware data composition. We introduce Coherent Information
Fidelity Tuning (CIFT), a framework that treats data composition as an
optimization problem. CIFT uses a practical proxy for Information Fidelity
based on the feature-space geometry of a dataset. This enables the
identification of a phase transition, termed the Decoherence Point, where
training stability degrades. The framework includes a generative engine,
Multi-View Video Augmentation (MVAug), to synthesize a causally disentangled
data spectrum for this tuning process. Applying CIFT to policy architectures
such as $\pi_0$ and Diffusion Policy improves OOD success rates by over 54\%.
These results indicate that fidelity-aware composition, beyond data synthesis
alone, is an important component for developing robust, general-purpose robots.

</details>


### [928] [From Code to Action: Hierarchical Learning of Diffusion-VLM Policies](https://arxiv.org/abs/2509.24917)
*Markus Peschl,Pietro Mazzaglia,Daniel Dijkman*

Main category: cs.RO

TL;DR: 提出分层框架，结合代码生成VLM和低级扩散策略，处理机器人操作模仿学习的泛化和数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作模仿学习中泛化能力有限和数据稀缺的问题，特别是复杂长时任务。

Method: 引入分层框架，将开源机器人API作为结构化监督源，训练VLM将任务描述分解为可执行子程序，通过扩散策略实现行为模仿，并加入记忆机制处理非马尔可夫性。

Result: 实现可解释的策略分解，相比扁平策略提高泛化能力，可分别评估高层规划和低层控制。

Conclusion: 该分层框架有效解决机器人操作模仿学习问题，提升泛化能力。

Abstract: Imitation learning for robotic manipulation often suffers from limited
generalization and data scarcity, especially in complex, long-horizon tasks. In
this work, we introduce a hierarchical framework that leverages code-generating
vision-language models (VLMs) in combination with low-level diffusion policies
to effectively imitate and generalize robotic behavior. Our key insight is to
treat open-source robotic APIs not only as execution interfaces but also as
sources of structured supervision: the associated subtask functions - when
exposed - can serve as modular, semantically meaningful labels. We train a VLM
to decompose task descriptions into executable subroutines, which are then
grounded through a diffusion policy trained to imitate the corresponding robot
behavior. To handle the non-Markovian nature of both code execution and certain
real-world tasks, such as object swapping, our architecture incorporates a
memory mechanism that maintains subtask context across time. We find that this
design enables interpretable policy decomposition, improves generalization when
compared to flat policies and enables separate evaluation of high-level
planning and low-level control.

</details>


### [929] [MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic Manipulation](https://arxiv.org/abs/2509.24956)
*Jan Ole von Hartz,Lukas Schweizer,Joschka Boedecker,Abhinav Valada*

Main category: cs.RO

TL;DR: 提出Multi - Stream Generative Policy (MSG)框架提升生成式机器人策略泛化性和样本效率，实验表明可大幅减少演示次数并提升性能，还支持零样本对象实例迁移。


<details>
  <summary>Details</summary>
Motivation: 现有生成式机器人策略样本效率低，对象中心策略无法完全解决该问题。

Method: 提出MSG，一个推理时间组合框架，训练多个对象中心策略并在推理时组合。

Result: 实验表明能从仅5个演示中学习高质量策略，减少95%演示次数，性能比单流方法提升89%，还支持零样本对象实例迁移。

Conclusion: MSG可提升生成式机器人策略泛化性和样本效率，具有广泛适用性，并给出部署实用建议。

Abstract: Generative robot policies such as Flow Matching offer flexible, multi-modal
policy learning but are sample-inefficient. Although object-centric policies
improve sample efficiency, it does not resolve this limitation. In this work,
we propose Multi-Stream Generative Policy (MSG), an inference-time composition
framework that trains multiple object-centric policies and combines them at
inference to improve generalization and sample efficiency. MSG is
model-agnostic and inference-only, hence widely applicable to various
generative policies and training paradigms. We perform extensive experiments
both in simulation and on a real robot, demonstrating that our approach learns
high-quality generative policies from as few as five demonstrations, resulting
in a 95% reduction in demonstrations, and improves policy performance by 89
percent compared to single-stream approaches. Furthermore, we present
comprehensive ablation studies on various composition strategies and provide
practical recommendations for deployment. Finally, MSG enables zero-shot object
instance transfer. We make our code publicly available at
https://msg.cs.uni-freiburg.de.

</details>


### [930] [Curriculum Imitation Learning of Distributed Multi-Robot Policies](https://arxiv.org/abs/2509.25097)
*Jesús Roche,Eduardo Sebastián,Eduardo Montijano*

Main category: cs.RO

TL;DR: 本文在模仿学习框架下解决多机器人系统学习控制策略的两个局限，提出改进长期协调的课程学习方法和用第三人称全局状态演示近似各机器人自我中心感知的方法，实验表明这些策略能从全局演示中学习到鲁棒的分布式控制器。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统学习控制策略面临长期协调和获取现实训练数据困难的挑战。

Method: 转变课程学习在多机器人系统中的角色，逐步增加训练时专家轨迹长度；用第三人称全局状态演示近似各机器人自我中心感知，将理想轨迹转换为局部可用观测；将两者集成到物理信息技术中。

Result: 课程学习提高了长期准确性，感知估计方法产生的策略对现实不确定性具有鲁棒性。

Conclusion: 这些策略能使系统即使在缺乏专家行动或机载测量的情况下，也能从全局演示中学习到鲁棒的分布式控制器。

Abstract: Learning control policies for multi-robot systems (MRS) remains a major
challenge due to long-term coordination and the difficulty of obtaining
realistic training data. In this work, we address both limitations within an
imitation learning framework. First, we shift the typical role of Curriculum
Learning in MRS, from scalability with the number of robots, to focus on
improving long-term coordination. We propose a curriculum strategy that
gradually increases the length of expert trajectories during training,
stabilizing learning and enhancing the accuracy of long-term behaviors. Second,
we introduce a method to approximate the egocentric perception of each robot
using only third-person global state demonstrations. Our approach transforms
idealized trajectories into locally available observations by filtering
neighbors, converting reference frames, and simulating onboard sensor
variability. Both contributions are integrated into a physics-informed
technique to produce scalable, distributed policies from observations. We
conduct experiments across two tasks with varying team sizes and noise levels.
Results show that our curriculum improves long-term accuracy, while our
perceptual estimation method yields policies that are robust to realistic
uncertainty. Together, these strategies enable the learning of robust,
distributed controllers from global demonstrations, even in the absence of
expert actions or onboard measurements.

</details>


### [931] [AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation](https://arxiv.org/abs/2509.25032)
*Ryosuke Takanami,Petr Khrapchenkov,Shu Morikuni,Jumpei Arima,Yuta Takaba,Shunsuke Maeda,Takuya Okubo,Genki Sano,Satoshi Sekioka,Aoi Kadoya,Motonari Kambara,Naoya Nishiura,Haruto Suzuki,Takanori Yoshimoto,Koya Sakamoto,Shinnosuke Ono,Hu Yang,Daichi Yashima,Aoi Horo,Tomohiro Motoda,Kensuke Chiyoma,Hiroshi Ito,Koki Fukuda,Akihito Goto,Kazumi Morinaga,Yuya Ikeda,Riko Kawada,Masaki Yoshikawa,Norio Kosuge,Yuki Noguchi,Kei Ota,Tatsuya Matsushima,Yusuke Iwasawa,Yutaka Matsuo,Tetsuya Ogata*

Main category: cs.RO

TL;DR: 提出AIRoA MoMa数据集解决现有移动操作数据集不足问题，推动视觉语言行动模型发展且已公开。


<details>
  <summary>Details</summary>
Motivation: 机器人向非结构化环境过渡，构建通用智能体需大规模多模态数据集，但现有资源存在不足。

Method: 创建AIRoA MoMa数据集，包含同步的多种数据及新型两层注释模式，以LeRobot v2.1格式标准化。

Result: 初始数据集包含25469个情节，约94小时，以HSR收集。

Conclusion: AIRoA MoMa数据集为推进下一代视觉语言行动模型提供关键基准。

Abstract: As robots transition from controlled settings to unstructured human
environments, building generalist agents that can reliably follow natural
language instructions remains a central challenge. Progress in robust mobile
manipulation requires large-scale multimodal datasets that capture contact-rich
and long-horizon tasks, yet existing resources lack synchronized force-torque
sensing, hierarchical annotations, and explicit failure cases. We address this
gap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset
for mobile manipulation. It includes synchronized RGB images, joint states,
six-axis wrist force-torque signals, and internal robot states, together with a
novel two-layer annotation schema of sub-goals and primitive actions for
hierarchical learning and error analysis. The initial dataset comprises 25,469
episodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is
fully standardized in the LeRobot v2.1 format. By uniquely integrating mobile
manipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa
provides a critical benchmark for advancing the next generation of
Vision-Language-Action models. The first version of our dataset is now
available at https://huggingface.co/datasets/airoa-org/airoa-moma .

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [932] [Embedded Deep Learning for Bio-hybrid Plant Sensors to Detect Increased Heat and Ozone Levels](https://arxiv.org/abs/2509.24992)
*Till Aust,Christoph Karl Heck,Eduard Buss,Heiko Hamann*

Main category: cs.ET

TL;DR: 本文提出一种结合自然植物与嵌入式深度学习的生物混合环境传感器系统，用于实时检测温度和臭氧水平变化，该系统有良好灵敏度且可扩展。


<details>
  <summary>Details</summary>
Motivation: 开发一种实时、低功耗的环境监测解决方案。

Method: 基于低功耗PhytoNode平台，记录常春藤的电差分电位信号，并使用嵌入式深度学习模型进行处理。

Result: 传感设备检测温度和臭氧变化的灵敏度高达0.98，可通过增加训练数据减轻日常和植物间的变异性及精度问题。

Conclusion: 集成嵌入式深度学习的生物传感设备为连续环境监测及其他领域提供了新的低功耗解决方案。

Abstract: We present a bio-hybrid environmental sensor system that integrates natural
plants and embedded deep learning for real-time, on-device detection of
temperature and ozone level changes. Our system, based on the low-power
PhytoNode platform, records electric differential potential signals from Hedera
helix and processes them onboard using an embedded deep learning model. We
demonstrate that our sensing device detects changes in temperature and ozone
with good sensitivity of up to 0.98. Daily and inter-plant variability, as well
as limited precision, could be mitigated by incorporating additional training
data, which is readily integrable in our data-driven framework. Our approach
also has potential to scale to new environmental factors and plant species. By
integrating embedded deep learning onboard our biological sensing device, we
offer a new, low-power solution for continuous environmental monitoring and
potentially other fields of application.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [933] [Distinguishability of causal structures under latent confounding and selection](https://arxiv.org/abs/2509.20433)
*Ryan Carey,Marina Maciel Ansanelli,Elie Wolfe,Robin J. Evans*

Main category: math.ST

TL;DR: 本文提出选定边缘化有向图（smDG）这一图形结构来表示因果图等价类，给出smDG中条件独立的分离准则及因果结构不可区分的充分条件。


<details>
  <summary>Details</summary>
Motivation: 观察数据中的统计关系有多种成因，并非所有因果图都能通过实验数据区分，需研究因果图等价类。

Method: 将因果图等价类表述为smDG，给出其相关性质，提供类似d - 分离准则的分离准则。

Result: 证明两个有潜在和选定顶点的有向无环图具有相同smDG时不可区分，给出smDG中条件独立的分离准则及因果结构不可区分的充分条件。

Conclusion: smDG可用于处理因果图等价类问题，为因果结构不可区分性提供了相关条件。

Abstract: Statistical relationships in observed data can arise for several different
reasons: the observed variables may be causally related, they may share a
latent common cause, or there may be selection bias. Each of these scenarios
can be modelled using different causal graphs. Not all such causal graphs,
however, can be distinguished by experimental data. In this paper, we formulate
the equivalence class of causal graphs as a novel graphical structure, the
selected-marginalized directed graph (smDG). That is, we show that two directed
acyclic graphs with latent and selected vertices have the same smDG if and only
if they are indistinguishable, even when allowing for arbitrary interventions
on the observed variables. As a substitute for the more familiar d-separation
criterion for DAGs, we provide an analogous sound and complete separation
criterion in smDGs for conditional independence relative to passive
observations. Finally, we provide a series of sufficient conditions under which
two causal structures are indistinguishable when there is only access to
passive observations.

</details>


### [934] [Generalization Analysis for Classification on Korobov Space](https://arxiv.org/abs/2509.22748)
*Yuqing Liu*

Main category: math.ST

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, the classification algorithm arising from Tikhonov
regularization is discussed. The main intention is to derive learning rates for
the excess misclassification error according to the convex $\eta$-norm loss
function $\phi(v)=(1 - v)_{+}^{\eta}$, $\eta\geq1$. Following the argument, the
estimation of error under Tsybakov noise conditions is studied. In addition, we
propose the rate of $L_p$ approximation of functions from Korobov space $X^{2,
p}([-1,1]^{d})$, $1\leq p \leq \infty$, by the shallow ReLU neural network.
This result consists of a novel Fourier analysis

</details>


### [935] [Learning single index model with gradient descent: spectral initialization and precise asymptotics](https://arxiv.org/abs/2509.23527)
*Yuchen Chen,Yandi Shen*

Main category: math.ST

TL;DR: 研究单指标模型下两阶段算法，推导动力平均场方程描述谱初始化梯度下降轨迹行为，以正则化Wirtinger流为例验证理论。


<details>
  <summary>Details</summary>
Motivation: 两阶段算法广泛用于非凸优化问题，但瞬态和长期行为的精确分布特性尚不清楚。

Method: 在比例渐近体制下研究两阶段算法，推导动力平均场方程。

Result: 当谱初始化成功进入良性区域，方程系统渐近时间平移不变且指数收敛，有长期固定点。

Conclusion: 推导出的方程能描述谱初始化梯度下降轨迹，理论可应用于正则化Wirtinger流进行相位恢复。

Abstract: Non-convex optimization plays a central role in many statistics and machine
learning problems. Despite the landscape irregularities for general non-convex
functions, some recent work showed that for many learning problems with random
data and large enough sample size, there exists a region around the true signal
with benign landscape. Motivated by this observation, a widely used strategy is
a two-stage algorithm, where we first apply a spectral initialization to plunge
into the region, and then run gradient descent for further refinement. While
this two-stage algorithm has been extensively analyzed for many non-convex
problems, the precise distributional property of both its transient and
long-time behavior remains to be understood. In this work, we study this
two-stage algorithm in the context of single index models under the
proportional asymptotics regime. We derive a set of dynamical mean field
equations, which describe the precise behavior of the trajectory of spectral
initialized gradient descent in the large system limit. We further show that
when the spectral initialization successfully lands in a region of benign
landscape, the above equation system is asymptotically time translation
invariant and exponential converging, and thus admits a set of long-time fixed
points that represents the mean field characterization of the limiting point of
the gradient descent dynamic. As a proof of concept, we demonstrate our general
theory in the example of regularized Wirtinger flow for phase retrieval.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [936] [Blockwise Missingness meets AI: A Tractable Solution for Semiparametric Inference](https://arxiv.org/abs/2509.24158)
*Qi Xu,Lorenzo Testa,Jing Lei,Kathryn Roeder*

Main category: stat.ME

TL;DR: 本文针对块式、非单调缺失数据进行参数估计和推断，提出IBM(RAY) 估计器并扩展到一类估计器，找到最有效估计器IBM(Adaptive)，通过模拟和应用展示方法性能。


<details>
  <summary>Details</summary>
Motivation: 解决数据存在块式、非单调缺失时的参数估计和推断问题。

Method: 基于半参数理论和预测驱动推理，利用现成AI模型，通过RAY近似找到最优估计方程，构建IBM(RAY) 估计器并扩展，求解约束二次规划问题找到最有效估计器。

Result: 所有IBM估计器无偏，渐近上比简单完全案例估计器有保证的效率提升，与AI模型预测精度无关。

Conclusion: 所提方法在有限样本下性能良好，数值稳定，可用于实际问题如表面蛋白丰度估计。

Abstract: We consider parameter estimation and inference when data feature blockwise,
non-monotone missingness. Our approach, rooted in semiparametric theory and
inspired by prediction-powered inference, leverages off-the-shelf AI
(predictive or generative) models to handle missing completely at random
mechanisms, by finding an approximation of the optimal estimating equation
through a novel and tractable Restricted Anova hierarchY (RAY) approximation.
The resulting Inference for Blockwise Missingness(RAY), or IBM(RAY) estimator
incorporates pre-trained AI models and carefully controls asymptotic variance
by tuning model-specific hyperparameters. We then extend IBM(RAY) to a general
class of estimators. We find the most efficient estimator in this class, which
we call IBM(Adaptive), by solving a constrained quadratic programming problem.
All IBM estimators are unbiased, and, crucially, asymptotically achieving
guaranteed efficiency gains over a naive complete-case estimator, regardless of
the predictive accuracy of the AI models used. We demonstrate the finite-sample
performance and numerical stability of our method through simulation studies
and an application to surface protein abundance estimation.

</details>


### [937] [A Greedy PDE Router for Blending Neural Operators and Classical Methods](https://arxiv.org/abs/2509.24814)
*Sahana Rayan,Yash Patel,Ambuj Tewari*

Main category: stat.ME

TL;DR: 本文提出近似贪心路由方法解决偏微分方程，在泊松和亥姆霍兹方程上表现优于单求解器和现有混合求解器。


<details>
  <summary>Details</summary>
Motivation: 经典数值求解器计算成本高，机器学习方法存在频谱偏差，设计最优混合迭代求解器是具有挑战性的组合问题，且贪心选择策略需真实误差信息，实际不可用。

Method: 提出近似贪心路由方法来模拟贪心求解器选择策略。

Result: 在泊松和亥姆霍兹方程上的实验结果表明，该方法优于单求解器基线和现有混合求解器方法，收敛更快更稳定。

Conclusion: 近似贪心路由方法能有效解决偏微分方程求解问题，在性能上有优势。

Abstract: When solving PDEs, classical numerical solvers are often computationally
expensive, while machine learning methods can suffer from spectral bias,
failing to capture high-frequency components. Designing an optimal hybrid
iterative solver--where, at each iteration, a solver is selected from an
ensemble of solvers to leverage their complementary strengths--poses a
challenging combinatorial problem. While the greedy selection strategy is
desirable for its constant-factor approximation guarantee to the optimal
solution, it requires knowledge of the true error at each step, which is
generally unavailable in practice. We address this by proposing an approximate
greedy router that efficiently mimics a greedy approach to solver selection.
Empirical results on the Poisson and Helmholtz equations demonstrate that our
method outperforms single-solver baselines and existing hybrid solver
approaches, such as HINTS, achieving faster and more stable convergence.

</details>


### [938] [SpeedCP: Fast Kernel-based Conditional Conformal Prediction](https://arxiv.org/abs/2509.24100)
*Yeo Jin Jung,Yating Liu,Zixuan Wu,So Won Jeong,Claire Donnat*

Main category: stat.ME

TL;DR: 本文基于Gibbs等人的框架，开发稳定高效算法，结合低秩潜在嵌入扩展到高维，经验上能可靠覆盖，缩短区间长度并加速。


<details>
  <summary>Details</summary>
Motivation: Gibbs等人的基于RKHS的近似条件共形预测区间方法理论前景好但计算成本高，需解决此问题。

Method: 开发稳定高效算法计算正则化RKHS共形优化问题的完整解路径，结合低秩潜在嵌入扩展到高维。

Result: 在多种现代黑箱预测器上提供可靠的条件覆盖，将Gibbs等人方法的区间长度缩短30%，提速40倍。

Conclusion: 所提方法能有效解决Gibbs等人方法的计算成本问题，在性能上有明显提升。

Abstract: Conformal prediction provides distribution-free prediction sets with
finite-sample conditional guarantees. We build upon the RKHS-based framework of
Gibbs et al. (2023), which leverages families of covariate shifts to provide
approximate conditional conformal prediction intervals, an approach with strong
theoretical promise, but with prohibitive computational cost. To bridge this
gap, we develop a stable and efficient algorithm that computes the full
solution path of the regularized RKHS conformal optimization problem, at
essentially the same cost as a single kernel quantile fit. Our path-tracing
framework simultaneously tunes hyperparameters, providing smoothness control
and data-adaptive calibration. To extend the method to high-dimensional
settings, we further integrate our approach with low-rank latent embeddings
that capture conditional validity in a data-driven latent space. Empirically,
our method provides reliable conditional coverage across a variety of modern
black-box predictors, improving the interval length of Gibbs et al. (2023) by
30%, while achieving a 40-fold speedup.

</details>


### [939] [Surjective Independence of Causal Influences for Local Bayesian Network Structures](https://arxiv.org/abs/2509.24759)
*Kieran Drury,Martine J. Barons,Jim Q. Smith*

Main category: stat.ME

TL;DR: 本文引入了SICI模型，放宽ICI假设，为贝叶斯网络参数化提供实用的局部结构模型。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯网络建模关系多带来挑战，补充专家判断有认知负担大、数量要求多的问题，且ICI假设不合理。

Method: 引入了 surjective independence of causal influences (SICI) 模型，放宽ICI假设。

Result: 提出了更可行、实用的局部结构模型。

Conclusion: SICI模型能促进贝叶斯网络的高效参数化。

Abstract: The very expressiveness of Bayesian networks can introduce fresh challenges
due to the large number of relationships they often model. In many domains, it
is thus often essential to supplement any available data with elicited expert
judgements. This in turn leads to two key challenges: the cognitive burden of
these judgements is often very high, and there are a very large number of
judgements required to obtain a full probability model. We can mitigate both
issues by introducing assumptions such as independence of causal influences
(ICI) on the local structures throughout the network, restricting the parameter
space of the model. However, the assumption of ICI is often unjustified and
overly strong. In this paper, we introduce the surjective independence of
causal influences (SICI) model which relaxes the ICI assumption and provides a
more viable, practical alternative local structure model that facilitates
efficient Bayesian network parameterisation.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [940] [Terrorism & Democracy in Burkina-Faso](https://arxiv.org/abs/2509.23046)
*P Carmel Marie Zagre*

Main category: econ.GN

TL;DR: 文章研究布基纳法索恐怖主义的政治后果，发现成功的恐怖袭击增加对威权政体支持、降低对民主治理支持，揭示恐怖主义破坏民主韧性。


<details>
  <summary>Details</summary>
Motivation: 探究布基纳法索恐怖主义的政治后果，对比成功恐怖袭击对公众支持民主和威权替代方案的影响。

Method: 结合ACLED的地理定位恐怖事件数据集（2015 - 2024年）和Afrobarometer的民意调查数据进行分析。

Result: 成功的恐怖袭击显著增加对军事政权、一人统治和一党制的支持，降低对民主治理的支持，这种变化袭击后立即显现且持续存在，还侵蚀对关键民主价值观的认知。

Conclusion: 恐怖主义在脆弱民主国家会破坏民主韧性，加速威权倾向。

Abstract: This article examines the political consequences of terrorism in Burkina
Faso. Using a dataset combining geolocated terrorist events from ACLED (from
2015 to 2024) with public opinion data from Afrobarometer, I compare the effect
of successful terrorist attacks on public support for democracy and
authoritarian alternatives. The results reveal that successful terrorist
attacks significantly increase support for military regimes, one man regimes,
and one party systems, while decreasing support for democratic governance.
These changes are most pronounced immediately after the attacks and persist
over time. This suggests that terrorism has triggered a trade-off in public
preferences between security and freedom. The study also reveals that terrorism
erodes perceptions of key democratic values, particularly civil liberties and
freedom of movement. Robustness tests confirm that weak institutions or a lack
of political knowledge are not driving the results. The article highlights how
terrorism in fragile democracies can undermine democratic resilience and
accelerate authoritarian drift.

</details>


### [941] [What influenced the lack of diversity in CSR after the company's losses: evidence from topic modeling](https://arxiv.org/abs/2509.23424)
*Ruiying Liu,Yuchi Li,Zhanli Li*

Main category: econ.GN

TL;DR: 本文用2006 - 2023年中国A股企业CSR报告，研究企业亏损对CSR披露多样性的影响及治理机制作用，结果稳健并指出异质性，给出实践启示。


<details>
  <summary>Details</summary>
Motivation: 研究企业亏损对企业社会责任（CSR）披露多样性的影响，以及内外部治理机制的作用。

Method: 运用潜在狄利克雷分配（LDA）提取主题，用基尼 - 辛普森指数和香农熵量化披露多样性，进行回归分析，采用工具变量估计、倾向得分匹配和安慰剂检验确保结果稳健性。

Result: 企业亏损显著压缩CSR主题多样性，内外部治理机制能缓解该影响，在有第三方保证、披露安全生产内容、大型企业和竞争不激烈行业的企业中影响较弱。

Conclusion: 强调财务困境对非财务披露的结构性影响，为优化CSR沟通、完善评级机构评估框架和设计多元化披露标准提供实践启示。

Abstract: The diversity of corporate social responsibility (CSR) disclosure is a
crucial dimension of corporate transparency, reflecting the breadth and
resilience of a firm's social responsibility. Using CSR reports of Chinese
A-share firms from 2006 to 2023, this paper applies Latent Dirichlet Allocation
(LDA) to extract topics and quantifies disclosure diversity using the
Gini-Simpson index and Shannon entropy. Regression results show that corporate
losses significantly compress CSR topic diversity, consistent with the slack
resources hypothesis. Both external and internal governance mechanisms mitigate
this effect: higher media attention, stronger executive compensation
incentives, and greater supervisory board shareholding attenuate the
loss-diversity penalty. Results are robust to instrumental variables
estimation, propensity score matching, and placebo tests. Heterogeneity
analyses indicate weaker effects in firms with third-party assurance, those
disclosing work safety content, large firms, and those in less competitive
industries. Our study highlights the structural impact of financial distress on
non-financial disclosure and provides practical implications for optimizing CSR
communication, refining evaluation frameworks for rating agencies, and
designing diversified disclosure standards.

</details>


### [942] [When Clear Skies Cloud Trust: Environmental Cues and the Paradox of Confidence in Government](https://arxiv.org/abs/2509.23554)
*Xiangzhe Xu,Ran Wu*

Main category: econ.GN

TL;DR: 本文研究环境条件（阳光效率）对政府信任的影响，提出“显著性与归因”机制，还识别了潜在中介路径，指出环境条件会给调查指标带来测量误差。


<details>
  <summary>Details</summary>
Motivation: 政府信任是政治经济和公共政策研究核心概念，探究环境条件如何通过情感和认知机制影响政府信任。

Method: 利用世界价值观调查第7波数据与NASA POWER高频天气数据。

Result: 发现晴朗天空可能因提高环境意识和触发负面归因降低政府信任，识别了潜在中介路径，证明环境条件会给调查信任指标带来测量误差。

Conclusion: 研究为环境心理学、行为政治经济学和调查方法提供理论贡献，对治理、政策设计和调查有实际意义。

Abstract: Government trust, as a core concept in political economy and public policy
research, serves as a fundamental cornerstone of democratic legitimacy and
state capacity. This paper examines how environmental conditions, particularly
sunlight efficiency, influence reported government trust through both affective
and cognitive mechanisms. Leveraging World Values Survey Wave 7 data merged
with NASA POWER high-frequency weather data, we propose and validate a novel
``salience and attribution'' mechanism: clearer skies may paradoxically reduce
government trust by heightening environmental awareness and triggering negative
attributions. We further identify potential mediating pathways, including
subjective well-being, political interest, political discussion, and health
perception, and demonstrate that environmental conditions introduce measurement
error in survey-based trust indicators. Our findings provide theoretical
contributions to environmental psychology, behavioral political economy, and
survey methodology, and yield practical implications for governance, policy
design, and survey

</details>


### [943] [Unintended Consequences of Early Driving Access: Evidence from Graduated Driver Licensing Policies and Adolescent Health Outcomes](https://arxiv.org/abs/2509.23578)
*Sharareh Massahi*

Main category: econ.GN

TL;DR: 研究发现1999 - 2020年提前驾驶许可使15 - 19岁女性青少年药物和心理健康相关死亡率上升，虽降低车辆死亡数，但带来新健康风险，需多结果政策评估。


<details>
  <summary>Details</summary>
Motivation: 探究分级驾驶员许可系统在降低青少年交通死亡数时带来的意外健康后果。

Method: 利用1999 - 2020年各州许可政策差异进行双重差分分析。

Result: 16岁前允许学习驾驶许可的州，药物相关死亡率和心理健康相关死亡率上升，车辆死亡数下降；对应每年约138例额外药物死亡和79例心理健康死亡，年死亡成本超20亿美元。

Conclusion: 需基于证据修改政策，进行多结果政策评估以平衡利弊。

Abstract: Graduated driver licensing systems effectively reduce adolescent traffic
fatalities but create unintended health consequences. Using state-level
variation in licensing policies from 1999-2020 and difference-in-differences
analysis, we provide the first causal evidence that early driving access
generates significant health risks for female adolescents aged 15-19. States
allowing learner's permits before age 16 experienced sharp increases in
drug-related mortality (+1.331 per 100,000, p<0.001) and mental health-related
mortality (+0.760, p<0.001), even as vehicle deaths declined (-0.656, p<0.05).
These effects explain nearly one-third of rising adolescent drug mortality and
one-tenth of mental health mortality increases over the study period. Early
driving access expands geographic reach, enabling contact with illicit drug
markets previously inaccessible to adolescents. It broadens social networks,
increasing exposure to high-risk peers, while vehicles provide unsupervised
spaces for experimentation. Premature independence also intensifies
psychological stress during critical developmental stages. Nationally, results
correspond to approximately 138 additional drug deaths and 79 mental health
deaths annually among female adolescents, imposing over $2 billion in mortality
costs yearly. These findings fundamentally reshape cost-benefit assessments of
licensing policies, revealing how interventions protecting adolescents in one
domain can create risks in others. Evidence-based modifications including
enhanced supervision, geographic restrictions, and mental health integration
could preserve traffic safety gains while mitigating unintended harms. This
research demonstrates the critical need for multi-outcome policy evaluation
that captures both benefits and hidden costs of expanded adolescent
independence.

</details>


### [944] [Moravec's Paradox and Restrepo's Model: Limits of AGI Automation in Growth](https://arxiv.org/abs/2509.24466)
*Marc Bara*

Main category: econ.GN

TL;DR: 文章将莫拉维克悖论融入Restrepo (2025)的AGI经济增长模型，研究物理任务为经济瓶颈时劳动收入份额情况。


<details>
  <summary>Details</summary>
Motivation: 拓展Restrepo (2025)的AGI经济增长模型，考虑莫拉维克悖论对经济的影响。

Method: 将任务空间划分为认知和物理部分，考虑不同自动化成本和物理瓶颈的无限成本。

Result: 当物理任务构成具有足够高或无限计算需求的经济瓶颈时，在有限计算条件下劳动收入份额收敛于正常数。

Conclusion: 这从根本上改变了AGI的分配影响，同时保留了认知密集型经济体的增长动力。

Abstract: This note extends Restrepo (2025)'s model of economic growth under AGI by
incorporating Moravec's Paradox -the observation that tasks requiring
sensorimotor skills remain computationally expensive relative to cognitive
tasks. We partition the task space into cognitive and physical components with
differential automation costs, allowing infinite costs for some physical
bottlenecks. Our key result shows that when physical tasks constitute economic
bottlenecks with sufficiently high (or infinite) computational requirements,
the labor share of income converges to a positive constant in the
finite-compute regime (rather than zero). This fundamentally alters the
distributional implications of AGI while preserving the growth dynamics for
cognitive-intensive economies.

</details>


### [945] [Identifying the post-pandemic determinants of low performing students in Latin America through interpretable Machine Learning SHAP Values-Insights from PISA 2022](https://arxiv.org/abs/2509.24508)
*Marcos Delprato*

Main category: econ.GN

TL;DR: 本文运用可解释机器学习方法，基于PISA 2022数据，分析拉美地区低成绩学生的影响因素，发现多方面因素影响学生成绩，各国影响因素有一定同质性。


<details>
  <summary>Details</summary>
Motivation: 拉美地区学生未达基本能力水平的情况普遍，且存在结构性不平等和疫情后学习损失，需确定低成绩学生的影响因素。

Method: 使用可解释机器学习方法，依靠PISA 2022数据，采用Shapley Additive Explanations (SHAP)分析。

Result: 发现学生未达标与语言、留级、家庭数字设备、家庭财富、学校环境等因素有关，各国排名靠前因素的全球平均贡献模式较一致。

Conclusion: 研究结果有助于识别和针对拉美教育系统中最落后人群的策略相关文献。

Abstract: The high prevalence of students not achieving the basic competencies in Latin
America is concerning. Even more so given the region's deep structural
inequalities and the larger post-pandemic regional learning losses. Within this
scenario, this paper contributes to the identification of the determinants of
bottom and low performers (below level 2) using recent advancements on
explainable machine learning methods. In particular, relying on PISA 2022 data
for 10 countries and using the Shapley Additive Explanations (SHAP) analysis, I
identify critical factors impacting on the student performance across low
performers groups. I find that a student with the highest probability of being
a not achiever speaks a minority language and had repeated, has no digital
devices at home, comes from a poor family and works for payment half of the
week, and the school he/she attends has wide disadvantages such as bad school
climate, weak ICT infrastructure and poor teaching quality (only a third of
teachers being certified). Regarding countries' estimates, I find quite
homogeneous patterns as far as global average contribution of top ranked
factors is concerned, with repetition at primary, household wealth, and
educational ICT inputs being top ten ranked covariates in at least 8 out of the
10 total countries. The paper findings contribute to the broad literature on
strategies to identify and to target those most left behind in Latin American
education systems.

</details>


### [946] [Determinants of Latin American students academic resilience-Insights based on PISA 2022 using an explainable machine learning approach](https://arxiv.org/abs/2509.24830)
*Marcos Delprato*

Main category: econ.GN

TL;DR: 本文借助可解释机器学习方法和PISA 2022数据，分析拉美地区学生学业恢复力的决定因素，为政策设计提供参考。


<details>
  <summary>Details</summary>
Motivation: 拉美地区存在学习危机，尤其是疫情后不平等对学业成绩影响更大，分析学生学业恢复力及其决定因素具有政策相关性。

Method: 使用可解释机器学习的SHAP方法，依托PISA 2022数据对该地区9个国家进行分析。

Result: 发现家庭投入、性别等是一个学业恢复力指标的主要因素，学校规模等是另一个指标的主要驱动因素，且学业恢复力与学校停课时长和远程学习障碍呈负相关。

Conclusion: 研究结果丰富了该地区相关文献，有助于未来政策设计，提升弱势学生的学业恢复力。

Abstract: The learning crisis in the Latin American region (i.e., higher rates of
students not reaching basic competencies at secondary level) is worrying,
particularly post-pandemic given the stronger role of inequality behind
achievement. Within this scenario, the concept of student academic resilience
(SAR), students who despite coming from disadvantaged backgrounds reach good
performance levels, and an analysis of its determinants, are policy relevant.
In this paper, using advancements on explainable machine learning methods (the
SHAP method) and relying on PISA 2022 data for 9 countries from the region, I
identify leading factors behind SAR using diverse indicators. I find that
household inputs (books and digital devices), gender, homework, repetition and
work intensity are leading factors for one indicator of academic resilience,
whereas for other indicator leading drives fall into the school domain: school
size, the ratio of PC connected to the internet, STR and teaching quality
proxied by certified teachers and professional development rates and school
type (private school). Also, I find negative associations of SAR with the
length of school closures and barriers for remote learning during the pandemic.
The paper's findings adds to the scare regional literature as well as they
contribute to future policy designs where key features behind SAR can be used
to lift disadvantaged students from lower achievement groups towards being
academic resilient.

</details>


### [947] [Pixels to Prices: Visual Traits, Market Cycles, and the Economics of NFT Valuation](https://arxiv.org/abs/2509.24879)
*Samiha Tariq*

Main category: econ.GN

TL;DR: 研究视觉特征和市场周期对NFT市场价格的影响，用交易数据建模分析，发现价格受产品特征和市场状态影响，提供相关工具。


<details>
  <summary>Details</summary>
Motivation: 探究视觉特征和市场周期如何塑造NFT市场价格。

Method: 使用26个以太坊生成式系列的94,039笔交易，提取196个图像特征，经三阶段过滤用于特征回归，采用静态混合效应模型和贝叶斯动态混合效应面板模型。

Result: 市场情绪和图像特征有显著定价能力，部分特征获正向或负向定价；深度嵌入特征增量价值有限；时变系数有制度敏感性。

Conclusion: NFT价格反映产品特征和市场状态，该框架为数字艺术市场提供周期感知工具。

Abstract: This paper studies how visual traits and market cycles shape prices in NFT
markets. Using 94,039 transactions from 26 major generative Ethereum
collections, the analysis extracts 196 machine-quantified image features
(covering color, composition, palette structure, geometry, texture, and deep
learning embeddings), then applies a three-stage filter process to identify
stable predictors for hedonic regression. A static mixed-effects model shows
that market sentiment and transparent, interpretable image traits have
significant and independent pricing power: higher focal saturation,
compositional concentration, and curvature are rewarded, while clutter, heavy
line work, and dispersed palettes are discounted; deep embeddings add limited
incremental value conditional on explicit traits. To assess state dependence,
the study estimates a Bayesian dynamic mixed-effects panel with cycle effects
and time-varying coefficients for a salient image attribute (Composition Focus
- Saturation). Collection-level heterogeneity ("brand premia") is absorbed by
random effects. The time-varying coefficients exhibit regime sensitivity, with
stronger premia in expansionary phases and weaker or negative loadings in
downturns, while grand-mean effects remain small on average. Overall, NFT
prices reflect both observable digital product characteristics and market
regimes, and the framework offers a cycle-aware tool for asset pricing,
platform strategy, and market design in digital art markets.

</details>


### [948] [Signaling in the Age of AI: Evidence from Cover Letters](https://arxiv.org/abs/2509.25054)
*Jingyi Cui,Gabriel Dias,Justin Ye*

Main category: econ.GN

TL;DR: 研究AI求职信工具对劳动力市场信号的影响，发现其提升求职信匹配度和回调率，降低求职信信息价值，雇主转向其他信号，编辑AI草稿时间长更易成功。


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI如何影响劳动力市场信号。

Method: 利用Freelancer.com上AI求职信工具的引入，采用差分估计法，跟踪工具使用情况。

Result: 工具提高求职信匹配度和回调率，写作能力弱的工人提升大，整体求职信与回调的相关性下降51%，雇主转向其他信号，编辑AI草稿时间与招聘成功正相关。

Conclusion: 在AI时代，求职信作为劳动者能力信号的信息价值降低，雇主需要依赖其他信号进行招聘决策。

Abstract: We study how generative AI affects labor market signaling using the
introduction of an AI-powered cover letter writing tool on Freelancer.com. Our
data track both access to the tool and usage at the application level.
Difference-in-differences estimates show that access to the AI tool increased
textual alignment between cover letters and job posts--which we refer to as
cover letter tailoring--and raised callback likelihoods. Workers with weaker
pre-AI writing skills saw larger improvements in cover letters, indicating that
AI substitutes for workers' own skills. Although only a minority of
applications used the tool, the overall correlation between cover letter
tailoring and callbacks fell by 51%, implying that cover letters became less
informative signals of worker ability in the age of AI. Employers
correspondingly shifted toward alternative signals, such as workers' past
reviews, which became more predictive of hiring. Finally, within the treated
group, greater time spent editing AI drafts was associated with higher hiring
success.

</details>


### [949] [Labour unions under neoliberal authoritarianism in the Global South: the cases of Turkey and Egypt](https://arxiv.org/abs/2509.25152)
*Mehmet Erman Erol,Cagatay Edgucan Sahin*

Main category: econ.GN

TL;DR: 分析土耳其和埃及新自由主义时期有组织劳工轨迹及其在2013年后安全化新自由主义 - 发展主义政权下的现状，指出新自由主义经历伴随持续威权主义。


<details>
  <summary>Details</summary>
Motivation: 了解土耳其和埃及新自由主义时期有组织劳工情况以及新政权下的现状，探讨经济自由化与政治民主化的关系。

Method: 对土耳其和埃及新自由主义及劳工市场情况进行分析。

Result: 新自由主义经历中存在持续威权主义，挑战了经济自由化会带来政治民主化的观点，且在新自由主义结构调整中，对有组织劳工的斗争很关键。

Conclusion: 通过强制手段瓦解持不同政见的工会力量，通过威权法团主义关系控制其他有组织劳工部分很重要。

Abstract: This article analyses the trajectories of organised labour in times of
neoliberalism in Turkey and Egypt and their current condition under securitised
neoliberal-developmentalist regimes post-2013. Neoliberal experience in these
countries was marked by continuing authoritarianism, challenging the view that
economic liberalisation would lead to political democratisation. One of the
most important areas of neoliberal restructuring has been labour markets. In
order to achieve this, struggles over organised labour were of vital
importance. Dismantling the power of dissident labour unions through coercive
measures and containing other sections of organised labour through
authoritarian corporatist relations has been crucial in these cases.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [950] [Unlicensed Band Allocation for Heterogeneous Networks](https://arxiv.org/abs/2509.23216)
*Po-Heng Chou*

Main category: cs.NI

TL;DR: 研究LAA与Wi-Fi异构网络中四种非授权频段分配方案，评估其性能并为LAA小小区设计提供指导。


<details>
  <summary>Details</summary>
Motivation: LAA与Wi-Fi在异构网络中使用相同非授权频段会相互干扰，非授权频段分配影响两者服务质量。

Method: 提出分析模型并进行仿真实验，研究四种非授权频段分配方案。

Result: 评估了四种分配方案在LAA缓冲队列中LAA和Wi-Fi分组数据接受率方面的性能。

Conclusion: 为LAA小小区的信道占用阶段和缓冲区大小设计提供了指导。

Abstract: Based on the License-Assisted Access (LAA) small cell architecture, the LAA
coexisting with Wi-Fi heterogeneous networks provides LTE mobile users with
high bandwidth efficiency as the unlicensed channels are shared among LAA and
Wi-Fi. However, LAA and Wi-Fi interfere with each other when both systems use
the same unlicensed channel in heterogeneous networks. In such a network,
unlicensed band allocation for LAA and Wi-Fi is an important issue that may
affect the quality of service (QoS) of both systems significantly. In this
paper, we propose an analytical model and conduct simulation experiments to
study four allocations for the unlicensed band: unlicensed full allocation
(UFA), unlicensed time-division allocation (UTA), and UFA/UTA with buffering
mechanism (UFAB and UTAB) for the LAA data packets. We evaluate the performance
of these unlicensed band allocation schemes in terms of the acceptance rate of
both LAA and Wi-Fi packet data in the LAA buffer queue. Our study provides
guidelines for designing the channel occupation phase and the buffer size of
the LAA small cell.

</details>


### [951] [Modeling the Unlicensed Band Allocation for LAA With Buffering Mechanism](https://arxiv.org/abs/2509.23217)
*Po-Heng Chou*

Main category: cs.NI

TL;DR: 本文提出分析模型并进行仿真实验，研究异构网络中带缓冲机制的LAA数据包基于先听后说的免许可频段分配，评估性能并给出LAA系统设计指南。


<details>
  <summary>Details</summary>
Motivation: 异构网络中LAA和Wi-Fi的免许可频段分配会显著影响两个系统的服务质量，是重要问题。

Method: 提出分析模型并进行仿真实验，从LAA和Wi-Fi数据包的接受率评估免许可频段分配性能。

Result: 未明确提及具体结果

Conclusion: 为LAA系统的信道占用阶段和缓冲阈值设计提供了指南。

Abstract: In this letter, we propose an analytical model and conduct simulation
experiments to study listen-before-talk-based unlicensed band allocation with
the buffering mechanism for the License-Assisted Access (LAA) packets in the
heterogeneous networks. In such a network, unlicensed band allocation for LAA
and Wi-Fi is an important issue, which may affect the quality of service for
both systems significantly. We evaluate the performance of these unlicensed
band allocations in terms of the acceptance rate of both LAA and Wi-Fi packets.
This letter provides the guidelines for designing the channel occupation phase
and buffer threshold of the LAA systems.

</details>


### [952] [Markov Modeling for Licensed and Unlicensed Band Allocation in Underlay and Overlay D2D](https://arxiv.org/abs/2509.23218)
*Po-Heng Chou,Yen-Ting Liu,Wei-Chang Chen,Walid Saad*

Main category: cs.NI

TL;DR: 提出用于D2D辅助蜂窝网络的资源分配分析模型，考虑多种场景，有流量控制策略，仿真显示能提升性能且调整更灵活。


<details>
  <summary>Details</summary>
Motivation: 为D2D辅助蜂窝网络设计有效的资源分配模型，解决频谱共享和流量卸载问题，保障Wi - Fi服务质量。

Method: 提出新的分析模型，考虑D2D、传统蜂窝和Wi - Fi数据包的交互，采用基于阈值的流量控制。

Result: 该方案稍牺牲传统蜂窝性能，显著提升覆盖层D2D性能，维持Wi - Fi用户性能，且比底层方案在D2D和Wi - Fi间调整更灵活。

Conclusion: 所提资源分配方案在D2D辅助蜂窝网络中有效，能平衡不同网络性能。

Abstract: In this paper, a novel analytical model for resource allocation is proposed
for a device-to-device (D2D) assisted cellular network. The proposed model can
be applied to underlay and overlay D2D systems for sharing licensed bands and
offloading cellular traffic. The developed model also takes into account the
problem of unlicensed band sharing with Wi-Fi systems. In the proposed model, a
global system state reflects the interaction among D2D, conventional cellular,
and Wi-Fi packets. Under the standard traffic model assumptions, a
threshold-based flow control is proposed for guaranteeing the
quality-of-service (QoS) of Wi-Fi. The packet blockage probability is then
derived. Simulation results show the proposed scheme sacrifices conventional
cellular performance slightly to improve overlay D2D performance significantly
while maintaining the performance for Wi-Fi users. Meanwhile, the proposed
scheme has more flexible adjustments between D2D and Wi-Fi than the underlay
scheme.

</details>


### [953] [Bridging Language Models and Formal Methods for Intent-Driven Optical Network Design](https://arxiv.org/abs/2509.22834)
*Anis Bekri,Amar Abane,Abdella Battou,Saddek Bensalem*

Main category: cs.NI

TL;DR: 提出混合管道解决自然语言意图转换为光网络拓扑的挑战，推动IBN发展。


<details>
  <summary>Details</summary>
Motivation: 解决将自然语言意图转换为正式正确的光网络拓扑的挑战，因大语言模型存在模糊性和缺乏严谨性。

Method: 提出集成基于大语言模型的意图解析、形式方法和光检索增强生成的混合管道。

Result: 生成可解释、可验证和可信的光网络设计。

Conclusion: 该方法通过确保可靠性和正确性，显著推进了意图驱动网络，对关键任务网络至关重要。

Abstract: Intent-Based Networking (IBN) aims to simplify network management by enabling
users to specify high-level goals that drive automated network design and
configuration. However, translating informal natural-language intents into
formally correct optical network topologies remains challenging due to inherent
ambiguity and lack of rigor in Large Language Models (LLMs). To address this,
we propose a novel hybrid pipeline that integrates LLM-based intent parsing,
formal methods, and Optical Retrieval-Augmented Generation (RAG). By enriching
design decisions with domain-specific optical standards and systematically
incorporating symbolic reasoning and verification techniques, our pipeline
generates explainable, verifiable, and trustworthy optical network designs.
This approach significantly advances IBN by ensuring reliability and
correctness, essential for mission-critical networking tasks.

</details>


### [954] [Impact of Environmental Factors on LoRa 2.4 GHz Time of Flight Ranging Outdoors](https://arxiv.org/abs/2509.23125)
*Yiqing Zhou,Xule Zhou,Zecan Cheng,Chenao Lu,Junhan Chen,Jiahong Pan,Yizhuo Liu,Sihao Li,Kyeong Soo Kim*

Main category: cs.NI

TL;DR: 现有WSN/IoT节点定位技术精度不足，LoRa 2.4 GHz可实现米级定位，但相关数据集不完善。本文收集含温湿度的测距数据，初步研究表明环境因素影响测距精度，需高级补偿方法。


<details>
  <summary>Details</summary>
Motivation: 现有WSN/IoT节点定位技术因时间分辨率低和时间戳抖动无法提供米级分辨率，且现有LoRa 2.4 GHz数据集覆盖和环境因素考虑不足。

Method: 在XJTLU南校区运动场收集LoRa 2.4 GHz RF ToF测距数据，三个LoRa节点与基站在参考点测距并记录温湿度，上传至ESP32基站。用简单DNN模型进行初步研究。

Result: 初步研究表明环境因素（温度和湿度）显著影响测距精度。

Conclusion: 需要先进方法补偿环境因素对户外LoRa RF ToF测距的影响。

Abstract: In WSN/IoT, node localization is essential to long-running applications for
accurate environment monitoring and event detection, often covering a large
area in the field. Due to the lower time resolution of typical WSN/IoT
platforms (e.g., 1 microsecond on ESP32 platforms) and the jitters in
timestamping, packet-level localization techniques cannot provide meter-level
resolution. For high-precision localization as well as world-wide
interoperability via 2.4-GHz ISM band, a new variant of LoRa, called LoRa 2.4
GHz, was proposed by semtech, which provides a radio frequency (RF) time of
flight (ToF) ranging method for meter-level localization. However, the existing
datasets reported in the literature are limited in their coverages and do not
take into account varying environmental factors such as temperature and
humidity. To address these issues, LoRa 2.4 GHz RF ToF ranging data was
collected on a sports field at the XJTLU south campus, where three LoRa nodes
logged samples of ranging with a LoRa base station, together with temperature
and humidity, at reference points arranged as a 3x3 grid covering 400 square
meter over three weeks and uploaded all measurement records to the base station
equipped with an ESP32-based transceiver for machine and user communications.
The results of a preliminary investigation based on a simple deep neural
network (DNN) model demonstrate that the environmental factors, including the
temperature and humidity, significantly affect the accuracy of ranging, which
calls for advanced methods of compensating for the effects of environmental
factors on LoRa RF ToF ranging outdoors.

</details>


### [955] [Continual Learning to Generalize Forwarding Strategies for Diverse Mobile Wireless Networks](https://arxiv.org/abs/2509.23913)
*Cheonjin Park,Victoria Manfredi,Xiaolan Zhang,Chengyi Liu,Alicia P Wolfe,Dongjin Song,Sarah Tasneem,Bing Wang*

Main category: cs.NI

TL;DR: 本文提出框架解决深度强化学习在多跳移动无线网络中转发策略的泛化性问题，评估显示对未见移动场景有泛化能力，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习用于多跳移动无线网络转发策略，但开发在与训练环境差异大的场景有效的泛化方法尚待探索。

Method: 开发考虑多样移动网络场景的可泛化基础模型；用基础模型处理新场景，必要时用新场景少量数据微调；设计新特征表征网络变化和特征质量；开发连续学习方法避免灾难性遗忘。

Result: 通过包括两个城市真实场景的广泛评估，表明该方法对未见移动场景有泛化能力，相比现有启发式转发策略，延迟最多降低78%，交付率提高24%，转发次数相当或略高。

Conclusion: 所提框架能有效解决深度强化学习在多跳移动无线网络中转发策略的泛化性问题。

Abstract: Deep reinforcement learning (DRL) has been successfully used to design
forwarding strategies for multi-hop mobile wireless networks. While such
strategies can be used directly for networks with varied connectivity and
dynamic conditions, developing generalizable approaches that are effective on
scenarios significantly different from the training environment remains largely
unexplored. In this paper, we propose a framework to address the challenge of
generalizability by (i) developing a generalizable base model considering
diverse mobile network scenarios, and (ii) using the generalizable base model
for new scenarios, and when needed, fine-tuning the base model using a small
amount of data from the new scenarios. To support this framework, we first
design new features to characterize network variation and feature quality,
thereby improving the information used in DRL-based forwarding decisions. We
then develop a continual learning (CL) approach able to train DRL models across
diverse network scenarios without ``catastrophic forgetting.'' Using extensive
evaluation, including real-world scenarios in two cities, we show that our
approach is generalizable to unseen mobility scenarios. Compared to a
state-of-the-art heuristic forwarding strategy, it leads to up to 78% reduction
in delay, 24% improvement in delivery rate, and comparable or slightly higher
number of forwards.

</details>


### [956] [Contrastive Learning for Correlating Network Incidents](https://arxiv.org/abs/2509.24446)
*Jeremias Dötterl*

Main category: cs.NI

TL;DR: 本文提出基于自监督学习的网络情况相似性关联方法，实验表明对比学习在网络事件关联上有前景。


<details>
  <summary>Details</summary>
Motivation: 网络规模大，手动关联网络事件不可行，需自动化关联方法。

Method: 提出自监督学习方法，用对比学习在大量无标签网络情况数据集上训练深度神经网络。

Result: 在真实网络监控数据实验中达到高精度。

Conclusion: 对比学习是网络事件关联的有前景方法。

Abstract: Internet service providers monitor their networks to detect, triage, and
remediate service impairments. When an incident is detected, it is important to
determine whether similar incidents have occurred in the past or are happening
concurrently elsewhere in the network. Manual correlation of such incidents is
infeasible due to the scale of the networks under observation, making automated
correlation a necessity. This paper presents a self-supervised learning method
for similarity-based correlation of network situations. Using this method, a
deep neural network is trained on a large unlabeled dataset of network
situations using contrastive learning. High precision achieved in experiments
on real-world network monitoring data suggests that contrastive learning is a
promising approach to network incident correlation.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [957] [Spatially Parallel All-optical Neural Networks](https://arxiv.org/abs/2509.23611)
*Jianwei Qin,Yanbing Liu,Yan Liu,Xun Liu,Wei Li,Fangwei Ye*

Main category: physics.optics

TL;DR: 本文提出全光神经网络的空间并行架构（SP - AONNs），实验表明增加并行子网络数量可提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统空间串联全光神经网络（AONNs）存在信号退化问题，需额外非线性设计。

Method: 提出SP - AONNs架构，将输入信号复制并同时输入不同光学层，通过相干干涉实现非线性计算，还实现模块化4F光学系统。

Result: 增加并行子网络数量能提高准确率、增强抗噪性、扩展模型表达能力。

Conclusion: 空间并行性是提升光神经计算能力的实用且可扩展策略。

Abstract: All-optical neural networks (AONNs) have emerged as a promising paradigm for
ultrafast and energy-efficient computation. These networks typically consist of
multiple serially connected layers between input and output layers--a
configuration we term spatially series AONNs, with deep neural networks (DNNs)
being the most prominent examples. However, such series architectures suffer
from progressive signal degradation during information propagation and
critically require additional nonlinearity designs to model complex
relationships effectively. Here we propose a spatially parallel architecture
for all-optical neural networks (SP-AONNs). Unlike series architecture that
sequentially processes information through consecutively connected optical
layers, SP-AONNs divide the input signal into identical copies fed
simultaneously into separate optical layers. Through coherent interference
between these parallel linear sub-networks, SP-AONNs inherently enable
nonlinear computation without relying on active nonlinear components or
iterative updates. We implemented a modular 4F optical system for SP-AONNs and
evaluated its performance across multiple image classification benchmarks.
Experimental results demonstrate that increasing the number of parallel
sub-networks consistently enhances accuracy, improves noise robustness, and
expands model expressivity. Our findings highlight spatial parallelism as a
practical and scalable strategy for advancing the capabilities of optical
neural computing.

</details>


### [958] [Chat to Chip: Large Language Model Based Design of Arbitrarily Shaped Metasurfaces](https://arxiv.org/abs/2509.24196)
*Huanshu Zhang,Lei Kang,Sawyer D. Campbell,Douglas H. Werner*

Main category: physics.optics

TL;DR: 传统超表面设计受全波模拟计算成本限制，数据驱动方法虽有进展但仍有不足，本文展示大语言模型（LLM）可用于任意形状超表面设计，实现“聊天到芯片”工作流。


<details>
  <summary>Details</summary>
Motivation: 解决传统超表面设计中全波模拟计算成本高，现有数据驱动方法需特定网络训练和架构搜索的问题。

Method: 将任意形状超表面几何描述输入LLM学习物理关系进行光谱预测和逆设计，对一系列开放权重LLM进行基准测试。

Result: 发现十亿参数级模型精度与模型大小的关系，证明1 - D标记式LLM可用于2 - D任意形状超表面设计。

Conclusion: “聊天到芯片”工作流推动了更友好的数据驱动纳米光子学发展。

Abstract: Traditional metasurface design is limited by the computational cost of
full-wave simulations, preventing thorough exploration of complex
configurations. Data-driven approaches have emerged as a solution to this
bottleneck, replacing costly simulations with rapid neural network evaluations
and enabling near-instant design for meta-atoms. Despite advances, implementing
a new optical function still requires building and training a task-specific
network, along with exhaustive searches for suitable architectures and
hyperparameters. Pre-trained large language models (LLMs), by contrast,
sidestep this laborious process with a simple fine-tuning technique. However,
applying LLMs to the design of nanophotonic devices, particularly for
arbitrarily shaped metasurfaces, is still in its early stages; as such tasks
often require graphical networks. Here, we show that an LLM, fed with
descriptive inputs of arbitrarily shaped metasurface geometries, can learn the
physical relationships needed for spectral prediction and inverse design. We
further benchmarked a range of open-weight LLMs and identified relationships
between accuracy and model size at the billion-parameter level. We demonstrated
that 1-D token-wise LLMs provide a practical tool to designing 2-D arbitrarily
shaped metasurfaces. Linking natural-language interaction to electromagnetic
modelling, this "chat-to-chip" workflow represents a step toward more
user-friendly data-driven nanophotonics.

</details>


<div id='cond-mat.supr-con'></div>

# cond-mat.supr-con [[Back]](#toc)

### [959] [Guided Diffusion for the Discovery of New Superconductors](https://arxiv.org/abs/2509.25186)
*Pawan Prakash,Jason B. Gibson,Zhongwei Li,Gabriele Di Gianluca,Juan Esquivel,Eric Fuemmeler,Benjamin Geisler,Jung Soo Kim,Adrian Roitberg,Ellad B. Tadmor,Mingjie Liu,Stefano Martiniani,Gregory R. Stewart,James J. Hamlin,Peter J. Hirschfeld,Richard G. Hennig*

Main category: cond-mat.supr-con

TL;DR: 提出引导扩散框架加速新型超导体发现，经多步处理得到候选材料，验证中凸显实验合成挑战。


<details>
  <summary>Details</summary>
Motivation: 材料特定属性逆向设计挑战大，旨在加速新型超导体发现。

Method: 提出引导扩散框架，预训练DiffCSP基础模型，使用无分类器引导采样结构，结合机器学习和DFT计算进行多级筛选。

Result: 采样得到34,027个独特候选结构，773个候选材料的DFT计算Tc>5 K，生成模型展示有效属性驱动设计。

Conclusion: 端到端工作流程加速超导体发现，但预测和合成可实验实现材料仍具挑战。

Abstract: The inverse design of materials with specific desired properties, such as
high-temperature superconductivity, represents a formidable challenge in
materials science due to the vastness of chemical and structural space. We
present a guided diffusion framework to accelerate the discovery of novel
superconductors. A DiffCSP foundation model is pretrained on the Alexandria
Database and fine-tuned on 7,183 superconductors with first principles derived
labels. Employing classifier-free guidance, we sample 200,000 structures, which
lead to 34,027 unique candidates. A multistage screening process that combines
machine learning and density functional theory (DFT) calculations to assess
stability and electronic properties, identifies 773 candidates with
DFT-calculated $T_\mathrm{c}>5$ K. Notably, our generative model demonstrates
effective property-driven design. Our computational findings were validated
against experimental synthesis and characterization performed as part of this
work, which highlighted challenges in sparsely charted chemistries. This
end-to-end workflow accelerates superconductor discovery while underscoring the
challenge of predicting and synthesizing experimentally realizable materials.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [960] [Two-Sided Fairness in Many-to-One Matching](https://arxiv.org/abs/2509.24111)
*Ayumi Igarashi,Naoyuki Kamiyama,Yasushi Kawase,Warut Suksompong,Hanna Sumita,Yu Yokoi*

Main category: econ.TH

TL;DR: 提出多项式时间算法解决多对一匹配问题，满足多种特性并可扩展。


<details>
  <summary>Details</summary>
Motivation: 在多对一匹配场景中，不仅为参与者，也为团队提供公平性。

Method: 提出多项式时间算法，该算法综合了双边匹配的Gale - Shapley算法和公平分配的轮盘算法。

Result: 算法能计算出满足团队到一人无正当嫉妒、参与者无正当嫉妒、平衡性、帕累托最优和参与者群体防策略性的分配，即使存在平局情况。

Conclusion: 算法可扩展以适应配额和不完整偏好。

Abstract: We consider a classic many-to-one matching setting, where participants need
to be assigned to teams based on the preferences of both sides. Unlike most of
the matching literature, we aim to provide fairness not only to participants,
but also to teams using concepts from the literature of fair division. We
present a polynomial-time algorithm that computes an allocation satisfying
team-justified envy-freeness up to one participant, participant-justified
envy-freeness, balancedness, Pareto optimality, and group-strategyproofness for
participants, even in the possible presence of ties. Our algorithm generalizes
both the Gale-Shapley algorithm from two-sided matching as well as the
round-robin algorithm from fair division. We also discuss how our algorithm can
be extended to accommodate quotas and incomplete preferences.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [961] [Convergence of graph Dirichlet energies and graph Laplacians on intersecting manifolds of varying dimensions](https://arxiv.org/abs/2509.24458)
*Leon Bungert,Dejan Slepčev*

Main category: math.AP

TL;DR: 研究相交流形并集上的图狄利克雷能量的Γ - 收敛和图拉普拉斯算子的谱收敛，展示非归一化和归一化能量的不同收敛情况并做数值实验。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据常包含不同内在维度的部分或类别，为理解哪些机器学习方法能适应这种维度变化而开展研究。

Method: 研究标准的非归一化和归一化图狄利克雷能量。

Result: 非归一化能量及其相关图拉普拉斯算子渐近地只看到最高维流形内的变化；归一化狄利克雷能量收敛到适应所有维度的（张量积）狄利克雷能量，还建立了相关谱收敛。

Conclusion: 通过理论和数值实验说明了两种图狄利克雷能量在相交流形并集上的收敛特性。

Abstract: We study $\Gamma$-convergence of graph Dirichlet energies and spectral
convergence of graph Laplacians on unions of intersecting manifolds of
potentially different dimensions. Our investigation is motivated by problems of
machine learning, as real-world data often consist of parts or classes with
different intrinsic dimensions. An important challenge is to understand which
machine learning methods adapt to such varied dimensionalities. We investigate
the standard unnormalized and the normalized graph Dirichlet energies. We show
that the unnormalized energy and its associated graph Laplacian asymptotically
only sees the variations within the manifold of the highest dimension. On the
other hand, we prove that the normalized Dirichlet energy converges to a
(tensorized) Dirichlet energy on the union of manifolds that adapts to all
dimensions simultaneously. We also establish the related spectral convergence
and present a few numerical experiments to illustrate our findings.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [962] [A Data-Driven Framework for Digital Transformation in Smart Cities: Integrating AI, Dashboards, and IoT Readiness](https://arxiv.org/abs/2509.22721)
*Ángel Lloret,Jesús Peral,Antonio Ferrández,María Auladell,Rafael Muñoz*

Main category: cs.CY

TL;DR: 本文提出结合传统方法与AI技术的创新方法评估公共部门数字转型水平，通过西班牙案例验证有效，且具国际扩展性，技术集成能支持智慧城市转型。


<details>
  <summary>Details</summary>
Motivation: 公共管理需提供高效、以公民为中心的服务，响应社会期望、ESG标准和联合国可持续发展目标，因此要评估公共部门数字转型水平。

Method: 结合传统评估方法与AI技术，一方面对公共实体专业人员开展调查，另一方面用基于AI的模型自动评估。

Result: 该方法在西班牙瓦伦西亚社区的地方公共行政案例中表现有效。

Conclusion: 方法模块化结构和双源数据基础支持国际扩展，但行政、监管和数字转型成熟度因素可能限制其广泛应用，技术集成可支持智慧城市转型。

Abstract: Digital transformation (DT) has become a strategic priority for public
administrations, particularly due to the need to deliver more efficient and
citizen-centered services and respond to societal expectations, ESG
(Environmental, Social, and Governance) criteria, and the United Nations
Sustainable Development Goals (UN SDGs). In this context, the main objective of
this study is to propose an innovative methodology to automatically evaluate
the level of digital transformation (DT) in public sector organizations. The
proposed approach combines traditional assessment methods with Artificial
Intelligence (AI) techniques. The methodology follows a dual approach: on the
one hand, surveys are conducted using specialized staff from various public
entities; on the other, AI-based models (including neural networks and
transformer architectures) are used to estimate the DT level of the
organizations automatically. Our approach has been applied to a real-world case
study involving local public administrations in the Valencian Community (Spain)
and shown effective performance in assessing DT. While the proposed methodology
has been validated in a specific local context, its modular structure and
dual-source data foundation support its international scalability,
acknowledging that administrative, regulatory, and DT maturity factors may
condition its broader applicability. The experiments carried out in this work
include (i) the creation of a domain-specific corpus derived from the surveys
and websites of several organizations, used to train the proposed models; (ii)
the use and comparison of diverse AI methods; and (iii) the validation of our
approach using real data. The integration of technologies such as the IoT,
sensor networks, and AI-based analytics can significantly support resilient,
agile urban environments and the transition towards more effective and
sustainable Smart City models.

</details>


### [963] [A Meta-Analysis of LLM Effects on Students across Qualification, Socialisation, and Subjectification](https://arxiv.org/abs/2509.22725)
*Jiayu Huang,Ruoxin Ritter Wang,Jen-Hao Liu,Boming Xia,Yue Huang,Ruoxi Sun,Jason,Xue,Jinan Zou*

Main category: cs.CY

TL;DR: 本文对133项研究进行元分析，探讨大语言模型（LLMs）在教育中的影响，指出其影响积极但不均衡，设计是关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有评估常将LLMs在教育中的影响简化为狭窄指标，本文重新审视LLMs应在教育中产生何种影响。

Method: 基于Biesta的良好教育三方理论，对133项实验和准实验研究进行元分析。

Result: LLMs对学生学习的影响积极但不均衡，在资格培养方面效果强，社会化结果较多样，主体化改善局限于小规模长期研究。

Conclusion: 设计是决定性因素，若无参与和能动性的支撑，LLMs会忽视教育更广泛目标，HCI和教育应关注LLMs带来的未来。

Abstract: Large language models (LLMs) are increasingly positioned as solutions for
education, yet evaluations often reduce their impact to narrow performance
metrics. This paper reframes the question by asking "what kind of impact should
LLMs have in education?" Drawing on Biesta's tripartite account of good
education: qualification, socialisation, and subjectification, we present a
meta-analysis of 133 experimental and quasi-experimental studies (k = 188).
Overall, the impact of LLMs on student learning is positive but uneven. Strong
effects emerge in qualification, particularly when LLMs function as tutors in
sustained interventions. Socialisation outcomes appear more variable,
concentrated in sustained, reflective interventions. Subjectification, linked
to autonomy and learner development, remains fragile, with improvements
confined to small-scale, long-term studies. This purpose-level view highlights
design as the decisive factor: without scaffolds for participation and agency,
LLMs privilege what is easiest to measure while neglecting broader aims of
education. For HCI and education, the issue is not just whether LLMs work, but
what futures they enable or foreclose.

</details>


### [964] [Automated Formative Feedback for Short-form Writing: An LLM-Driven Approach and Adoption Analysis](https://arxiv.org/abs/2509.22734)
*Tiago Fernandes Tavares,Luciano Pereira Soares*

Main category: cs.CY

TL;DR: 本文探讨工程顶峰项目双周报告中基于AI的形成性反馈的开发与应用，发现初期采用有障碍，但参与学生报告质量提升，工具潜力大。


<details>
  <summary>Details</summary>
Motivation: 在工程顶峰项目的双周报告场景中，开发和应用基于AI的形成性反馈，以提升报告的完整性和质量。

Method: 开发基于大语言模型（LLM）的工具为学生草稿报告提供个性化反馈，并收集两轮使用数据。

Result: 初期采用有障碍、参与率低，但参与学生能有效使用工具，报告完整性和质量提升，工具任务解析能力可识别潜在任务和可交付成果。

Conclusion: 研究情境中对工具存在初期怀疑、采用有限，但AI驱动工具可为师生提供有价值见解和形成性支持。

Abstract: This paper explores the development and adoption of AI-based formative
feedback in the context of biweekly reports in an engineering Capstone program.
Each student is required to write a short report detailing their individual
accomplishments over the past two weeks, which is then assessed by their
advising professor. An LLM-powered tool was developed to provide students with
personalized feedback on their draft reports, guiding them toward improved
completeness and quality. Usage data across two rounds revealed an initial
barrier to adoption, with low engagement rates. However, students who engaged
in the AI feedback system demonstrated the ability to use it effectively,
leading to improvements in the completeness and quality of their reports.
Furthermore, the tool's task-parsing capabilities provided a novel approach to
identify potential student organizational tasks and deliverables. The findings
suggest initial skepticism toward the tool with a limited adoption within the
studied context, however, they also highlight the potential for AI-driven tools
to provide students and professors valuable insights and formative support.

</details>


### [965] [Regulating the Agency of LLM-based Agents](https://arxiv.org/abs/2509.22735)
*Seán Boddy,Joshua Joseph*

Main category: cs.CY

TL;DR: 为应对大语言模型代理的对齐和失控风险，提出测量和控制其能动性的方法及监管工具。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型代理能力增强，其未对齐和失控造成的潜在危害日益严重，需解决风险。

Method: 将大语言模型代理的能动性概念化为独立于智能相关指标的属性，从偏好刚性、独立操作和目标持久性维度衡量，采用表征工程方法测量和控制能动性，并提出监管工具。

Result: 提出测量和控制能动性的具体方法及相关监管工具。

Conclusion: 该方法是降低‘科学家AI’范式风险的一步，同时能从有限的能动行为中获益。

Abstract: As increasingly capable large language model (LLM)-based agents are
developed, the potential harms caused by misalignment and loss of control grow
correspondingly severe. To address these risks, we propose an approach that
directly measures and controls the agency of these AI systems. We conceptualize
the agency of LLM-based agents as a property independent of
intelligence-related measures and consistent with the interdisciplinary
literature on the concept of agency. We offer (1) agency as a system property
operationalized along the dimensions of preference rigidity, independent
operation, and goal persistence, (2) a representation engineering approach to
the measurement and control of the agency of an LLM-based agent, and (3)
regulatory tools enabled by this approach: mandated testing protocols,
domain-specific agency limits, insurance frameworks that price risk based on
agency, and agency ceilings to prevent societal-scale risks. We view our
approach as a step toward reducing the risks that motivate the ``Scientist AI''
paradigm, while still capturing some of the benefits from limited agentic
behavior.

</details>


### [966] [Societal Capacity Assessment Framework: Measuring Resilience to Inform Advanced AI Risk Management](https://arxiv.org/abs/2509.22742)
*Milan Gandhi,Peter Cihon,Owen Larter,Rebecca Anselmetti*

Main category: cs.CY

TL;DR: 介绍了基于指标的社会能力评估框架（SCAF）用于评估社会应对AI风险的能力，促进更全面的AI风险评估和治理。


<details>
  <summary>Details</summary>
Motivation: 高级AI系统的风险评估需要考虑模型和部署环境，现有方法存在‘背景差距’，需要更全面的评估框架。

Method: 将已有的恢复力分析方法应用于AI，构建基于指标的SCAF框架。

Result: SCAF框架可使组织将风险管理与国家层面的部署条件相结合，帮助利益相关者识别增强社会应对新兴AI能力的机会。

Conclusion: SCAF框架通过弥合不同文献和AI评估中的‘背景差距’，在全球高级AI系统扩散的背景下，促进更全面的风险评估和治理。

Abstract: Risk assessments for advanced AI systems require evaluating both the models
themselves and their deployment contexts. We introduce the Societal Capacity
Assessment Framework (SCAF), an indicators-based approach to measuring a
society's vulnerability, coping capacity, and adaptive capacity in response to
AI-related risks. SCAF adapts established resilience analysis methodologies to
AI, enabling organisations to ground risk management in insights about
country-level deployment conditions. It can also support stakeholders in
identifying opportunities to strengthen societal preparedness for emerging AI
capabilities. By bridging disparate literatures and the "context gap" in AI
evaluation, SCAF promotes more holistic risk assessment and governance as
advanced AI systems proliferate globally.

</details>


### [967] [AI Education in Higher Education: A Taxonomy for Curriculum Reform and the Mission of Knowledge](https://arxiv.org/abs/2509.23363)
*Tian Zheng*

Main category: cs.CY

TL;DR: 本文提出AI教育分类法，强调课程改革重要性，建议通过结构化教师发展旅程推动有意义的改革。


<details>
  <summary>Details</summary>
Motivation: 当前AI重塑高等教育的讨论缺乏统一框架，需组织AI教育多样叙述并为学科课程讨论提供信息。

Method: 将AI教育叙述置于高等教育知识使命中，分析学科演变及AI在课程和学科目的层面的影响。

Result: 指出最重大挑战在课程和学科目的层面，教学创新是切入点，建议结构化教师发展旅程。

Conclusion: 使高校教师发展旅程与知识使命一致，能将AI的挑战转化为学科发展机遇。

Abstract: Artificial intelligence (AI) is reshaping higher education, yet current
debates often feel tangled, mixing concerns about pedagogy, operations,
curriculum, and the future of work without a shared framework. This paper
offers a first attempt at a taxonomy to organize the diverse narratives of AI
education and to inform discipline-based curricular discussions. We place these
narratives within the enduring responsibility of higher education: the mission
of knowledge. This mission includes not only the preservation and advancement
of disciplinary expertise, but also the cultivation of skills and wisdom, i.e.,
forms of meta-knowledge that encompass judgment, ethics, and social
responsibility. For the purpose of this paper's discussion, AI is defined as
adaptive, data-driven systems that automate analysis, modeling, and
decision-making, highlighting its dual role as enabler and disruptor across
disciplines. We argue that the most consequential challenges lie at the level
of curriculum and disciplinary purpose, where AI accelerates inquiry but also
unsettles expertise and identity. We show how disciplines evolve through the
interplay of research, curriculum, pedagogy, and faculty expertise, and why
curricular reform is the central lever for meaningful change. Pedagogical
innovation offers a strategic and accessible entry point, providing actionable
steps that help faculty and students build the expertise needed to engage in
deeper curricular rethinking and disciplinary renewal. Within this framing, we
suggest that meaningful reform can move forward through structured faculty
journeys: from AI literacy to pedagogy, curriculum design, and research
integration. The key is to align these journeys with the mission of knowledge,
turning the disruptive pressures of AI into opportunities for disciplines to
sustain expertise, advance inquiry, and serve society.

</details>


### [968] [MateInfoUB: A Real-World Benchmark for Testing LLMs in Competitive, Multilingual, and Multimodal Educational Tasks](https://arxiv.org/abs/2507.03162)
*Dumitran Adrian Marius,Theodor-Pierre Moroianu,Buca Mihnea-Vicentiu*

Main category: cs.CY

TL;DR: 研究提出双语文本和图像多模态数据集评估大语言模型在计算机科学竞赛题上的表现，揭示其优缺点，探讨伦理问题，公开数据集并发布教育应用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在计算机科学教育领域发展迅速，需探究其在高级计算机科学情境中的潜力与局限。

Method: 创建双语文本和图像多模态数据集，系统评估当前最先进的大语言模型在理论编程任务上的表现。

Result: 揭示了当前大语言模型的优缺点，包括语言选择的影响。

Conclusion: 为计算机科学教育和竞赛场景中应用大语言模型提供见解，讨论伦理问题以指导未来教育实践和政策，公开数据集并发布教育应用支持进一步研究。

Abstract: The rapid advancement of Large Language Models (LLMs) has transformed various
domains, particularly computer science (CS) education. These models exhibit
remarkable capabilities in code-related tasks and problem-solving, raising
questions about their potential and limitations in advanced CS contexts. This
study presents a novel bilingual (English-Romanian) multimodal (text and image)
dataset of multiple-choice questions derived from a high-level computer science
competition. A particularity of our dataset is that the problems are conceived
such that some of them are easier solved using reasoning on paper, while for
others writing code is more efficient. We systematically evaluate State of The
Art LLMs on this dataset, analyzing their performance on theoretical
programming tasks. Our findings reveal the strengths and limitations of current
LLMs, including the influence of language choice (English vs. Romanian),
providing insights into their applicability in CS education and competition
settings. We also address critical ethical considerations surrounding
educational integrity and the fairness of assessments in the context of LLM
usage. These discussions aim to inform future educational practices and
policies. To support further research, our dataset will be made publicly
available in both English and Romanian. Additionally, we release an educational
application tailored for Romanian students, enabling them to self-assess using
the dataset in an interactive and practice-oriented environment.

</details>


### [969] [The 2025 OpenAI Preparedness Framework does not guarantee any AI risk mitigation practices: a proof-of-concept for affordance analyses of AI safety policies](https://arxiv.org/abs/2509.24394)
*Sam Coggins,Alex Saeri,Katherine A. Daniell,Lorenn P. Ruster,Jessie Liu,Jenny L. Davis*

Main category: cs.CY

TL;DR: 分析OpenAI安全框架，发现其对AI风险管控不足，需更强治理干预，且提供可复制评估方法。


<details>
  <summary>Details</summary>
Motivation: 了解AI安全框架对AI风险的覆盖情况及行动规定，以评估其对AI开发和部署的实际治理效果。

Method: 运用可供性理论，结合可供性的机制与条件模型和MIT AI风险库，分析OpenAI的‘Preparedness Framework Version 2’。

Result: 该安全政策仅评估少数AI风险，鼓励部署有‘中度’能力且可能造成‘严重伤害’的系统，允许CEO部署更危险能力的系统。

Conclusion: 有效缓解AI风险需超越当前行业自律的更强大治理干预，且可供性分析提供了评估安全框架的可复制方法。

Abstract: Prominent AI companies are producing 'safety frameworks' as a type of
voluntary self-governance. These statements purport to establish risk
thresholds and safety procedures for the development and deployment of highly
capable AI. Understanding which AI risks are covered and what actions are
allowed, refused, demanded, encouraged, or discouraged by these statements is
vital for assessing how these frameworks actually govern AI development and
deployment. We draw on affordance theory to analyse the OpenAI 'Preparedness
Framework Version 2' (April 2025) using the Mechanisms & Conditions model of
affordances and the MIT AI Risk Repository. We find that this safety policy
requests evaluation of a small minority of AI risks, encourages deployment of
systems with 'Medium' capabilities for what OpenAI itself defines as 'severe
harm' (potential for >1000 deaths or >$100B in damages), and allows OpenAI's
CEO to deploy even more dangerous capabilities. These findings suggest that
effective mitigation of AI risks requires more robust governance interventions
beyond current industry self-regulation. Our affordance analysis provides a
replicable method for evaluating what safety frameworks actually permit versus
what they claim.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [970] [DFG-PCN: Point Cloud Completion with Degree-Flexible Point Graph](https://arxiv.org/abs/2509.23703)
*Zhenyu Shu,Jian Yao,Shiqing Xin*

Main category: cs.GR

TL;DR: 提出DFG - PCN点云补全框架，自适应分配节点度并融合特征，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统点云补全方法依赖固定局部区域划分，未考虑形状不同区域几何复杂度分布不均，导致表示效率低和重建效果不佳。

Method: 提出Degree - Flexible Point Graph Completion Network (DFG - PCN)框架，用结合特征变化和曲率的细节感知度量自适应分配节点度，引入几何感知图集成模块，用曼哈顿距离进行边聚合和局部与全局特征融合。

Result: 在多个基准数据集上的大量实验表明该方法始终优于现有方法。

Conclusion: 所提的DFG - PCN框架在点云补全任务上有更好的性能表现。

Abstract: Point cloud completion is a vital task focused on reconstructing complete
point clouds and addressing the incompleteness caused by occlusion and limited
sensor resolution. Traditional methods relying on fixed local region
partitioning, such as k-nearest neighbors, which fail to account for the highly
uneven distribution of geometric complexity across different regions of a
shape. This limitation leads to inefficient representation and suboptimal
reconstruction, especially in areas with fine-grained details or structural
discontinuities. This paper proposes a point cloud completion framework called
Degree-Flexible Point Graph Completion Network (DFG-PCN). It adaptively assigns
node degrees using a detail-aware metric that combines feature variation and
curvature, focusing on structurally important regions. We further introduce a
geometry-aware graph integration module that uses Manhattan distance for edge
aggregation and detail-guided fusion of local and global features to enhance
representation. Extensive experiments on multiple benchmark datasets
demonstrate that our method consistently outperforms state-of-the-art
approaches.

</details>


### [971] [StrucADT: Generating Structure-controlled 3D Point Clouds with Adjacency Diffusion Transformer](https://arxiv.org/abs/2509.23709)
*Zhenyu Shu,Jiajun Shen,Zhongui Chen,Xiaoguang Han,Shiqing Xin*

Main category: cs.GR

TL;DR: 提出StrucADT模型解决3D点云生成缺乏控制问题，实验在ShapeNet数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 多数3D生成模型难生成满足用户特定需求的可控3D点云形状，阻碍其大规模应用。

Method: 手动标注点云形状分割部分的邻接关系构建StructureGraph表示，提出StrucADT模型，含StructureGraphNet、cCNF Prior和Diffusion Transformer模块。

Result: 结构可控的3D点云生成方法产生高质量、多样化点云形状，可基于用户指定形状结构生成可控点云。

Conclusion: 所提方法在ShapeNet数据集的可控点云生成上达到了当前最优性能。

Abstract: In the field of 3D point cloud generation, numerous 3D generative models have
demonstrated the ability to generate diverse and realistic 3D shapes. However,
the majority of these approaches struggle to generate controllable 3D point
cloud shapes that meet user-specific requirements, hindering the large-scale
application of 3D point cloud generation. To address the challenge of lacking
control in 3D point cloud generation, we are the first to propose controlling
the generation of point clouds by shape structures that comprise part
existences and part adjacency relationships. We manually annotate the adjacency
relationships between the segmented parts of point cloud shapes, thereby
constructing a StructureGraph representation. Based on this StructureGraph
representation, we introduce StrucADT, a novel structure-controllable point
cloud generation model, which consists of StructureGraphNet module to extract
structure-aware latent features, cCNF Prior module to learn the distribution of
the latent features controlled by the part adjacency, and Diffusion Transformer
module conditioned on the latent features and part adjacency to generate
structure-consistent point cloud shapes. Experimental results demonstrate that
our structure-controllable 3D point cloud generation method produces
high-quality and diverse point cloud shapes, enabling the generation of
controllable point clouds based on user-specified shape structures and
achieving state-of-the-art performance in controllable point cloud generation
on the ShapeNet dataset.

</details>


### [972] [Diff-3DCap: Shape Captioning with Diffusion Models](https://arxiv.org/abs/2509.23718)
*Zhenyu Shu,Jiawei Wen,Shiyang Li,Shiqing Xin,Ligang Liu*

Main category: cs.GR

TL;DR: 提出Diff - 3DCap用于3D形状字幕任务，利用投影视图和连续扩散模型，实验表明性能与当前最优方法相当。


<details>
  <summary>Details</summary>
Motivation: 传统3D形状字幕方法依赖昂贵体素表示或目标检测技术，且效果不佳。

Method: 引入Diff - 3DCap，用投影视图表示3D对象，连续扩散模型进行字幕生成，利用预训练视觉语言模型的视觉嵌入作指导信号。

Result: 实验结果表明Diff - 3DCap能达到与当前最优方法相当的性能。

Conclusion: Diff - 3DCap在3D形状字幕任务中是一种有效的方法。

Abstract: The task of 3D shape captioning occupies a significant place within the
domain of computer graphics and has garnered considerable interest in recent
years. Traditional approaches to this challenge frequently depend on the
utilization of costly voxel representations or object detection techniques, yet
often fail to deliver satisfactory outcomes. To address the above challenges,
in this paper, we introduce Diff-3DCap, which employs a sequence of projected
views to represent a 3D object and a continuous diffusion model to facilitate
the captioning process. More precisely, our approach utilizes the continuous
diffusion model to perturb the embedded captions during the forward phase by
introducing Gaussian noise and then predicts the reconstructed annotation
during the reverse phase. Embedded within the diffusion framework is a
commitment to leveraging a visual embedding obtained from a pre-trained
visual-language model, which naturally allows the embedding to serve as a
guiding signal, eliminating the need for an additional classifier. Extensive
results of our experiments indicate that Diff-3DCap can achieve performance
comparable to that of the current state-of-the-art methods.

</details>


### [973] [Light-SQ: Structure-aware Shape Abstraction with Superquadrics for Generated Meshes](https://arxiv.org/abs/2509.24986)
*Yuhan Wang,Weikai Chen,Zeyu Hu,Runze Zhang,Yingda Yin,Ruoyu Wu,Keyang Luo,Shengju Qian,Yiyan Ma,Hongyi Li,Yuan Gao,Yuhuan Zhou,Hao Luo,Wan Wang,Xiaobin Shen,Zhaowei Li,Kuixin Zhu,Chuanlang Hong,Yueyue Wang,Lijie Feng,Xin Wang,Chen Change Loy*

Main category: cs.GR

TL;DR: 提出Light - SQ框架用于UGC场景下的3D形状抽象，通过多方面优化实现高效、高保真和可编辑的形状抽象，还引入3DGen - Prim基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 在UGC应用中，非专业用户依赖图像到3D生成模型创建3D资产，基于图元的形状抽象是有前景的解决方案，需结构感知的有效形状抽象。

Method: 提出Light - SQ框架，包括SDF雕刻减少图元重叠、基于结构感知体积分解的块重新生长填充策略驱动图元放置、基于SDF更新历史的自适应残差修剪抑制过分割，且支持多尺度拟合。引入3DGen - Prim基准评估。

Result: 广泛实验表明Light - SQ能对复杂生成几何进行高效、高保真和可编辑的超二次曲面形状抽象。

Conclusion: Light - SQ框架推进了3D UGC创建的可行性。

Abstract: In user-generated-content (UGC) applications, non-expert users often rely on
image-to-3D generative models to create 3D assets. In this context,
primitive-based shape abstraction offers a promising solution for UGC scenarios
by compressing high-resolution meshes into compact, editable representations.
Towards this end, effective shape abstraction must therefore be
structure-aware, characterized by low overlap between primitives, part-aware
alignment, and primitive compactness. We present Light-SQ, a novel
superquadric-based optimization framework that explicitly emphasizes
structure-awareness from three aspects. (a) We introduce SDF carving to
iteratively udpate the target signed distance field, discouraging overlap
between primitives. (b) We propose a block-regrow-fill strategy guided by
structure-aware volumetric decomposition, enabling structural partitioning to
drive primitive placement. (c) We implement adaptive residual pruning based on
SDF update history to surpress over-segmentation and ensure compact results. In
addition, Light-SQ supports multiscale fitting, enabling localized refinement
to preserve fine geometric details. To evaluate our method, we introduce
3DGen-Prim, a benchmark extending 3DGen-Bench with new metrics for both
reconstruction quality and primitive-level editability. Extensive experiments
demonstrate that Light-SQ enables efficient, high-fidelity, and editable shape
abstraction with superquadrics for complex generated geometry, advancing the
feasibility of 3D UGC creation.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [974] [Graph-Based Learning of Free Surface Dynamics in Generalized Newtonian Fluids using Smoothed Particle Hydrodynamics](https://arxiv.org/abs/2509.24264)
*Hyo-Jin Kim,Jaekwang Kim,Hyung-Jun Park*

Main category: physics.flu-dyn

TL;DR: 本文提出用于预测非牛顿流体自由表面动力学的GNN模型，基于SPH数据训练，加速计算且保证精度，展示了数据驱动流体模拟潜力。


<details>
  <summary>Details</summary>
Motivation: 传统牛顿流体算法用于非牛顿流体数值分析收敛困难，尤其在自由表面流场景，需提升计算效率。

Method: 提出基于GNN的数值模型，基于SPH模拟数据训练，学习粒子加速度影响。

Result: GNN模型在基准测试中显著加速计算，同时保持可靠精度。

Conclusion: GNN模拟框架有潜力高效建模非牛顿流体行为，为数据驱动流体模拟发展铺平道路。

Abstract: In this study, we propose a graph neural network (GNN) model for efficiently
predicting the flow behavior of non-Newtonian fluids with free surface
dynamics. The numerical analysis of non-Newtonian fluids presents significant
challenges, as traditional algorithms designed for Newtonian fluids with
constant viscosity often struggle to converge when applied to non-Newtonian
cases, where rheological properties vary dynamically with flow conditions.
Among these, power-law fluids exhibit viscosity that decreases exponentially as
the shear rate increases, making numerical simulations particularly difficult.
The complexity further escalates in free surface flow scenarios, where
computational challenges intensify. In such cases, particle-based methods like
smoothed particle hydrodynamics (SPH) provide advantages over traditional
grid-based techniques, such as the finite element method (FEM). Building on
this approach, we introduce a novel GNN-based numerical model to enhance the
computational efficiency of non-Newtonian power-law fluid flow simulations. Our
model is trained on SPH simulation data, learning the effects of particle
accelerations in the presence of SPH interactions based on the fluid's
power-law parameters. The GNN significantly accelerates computations while
maintaining reliable accuracy in benchmark tests, including dam-break and
droplet impact simulations. The results underscore the potential of GNN-based
simulation frameworks for efficiently modeling non-Newtonian fluid behavior,
paving the way for future advancements in data-driven fluid simulations.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [975] [Patient-specific Biomolecular Instruction Tuning](https://arxiv.org/abs/2509.22853)
*Irsyad Adam,Zekai Chen,David Laub,Shaun Porwal,Arda Pekis,Kevin Brown*

Main category: q-bio.QM

TL;DR: 本文介绍肿瘤学分子理解的指令调优数据集CPTAC - PROTSTRUCT和图 - 大语言模型框架KRONOS，提升临床推理能力，推动精准医学。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽有整合异构数据能力，但在患者特定蛋白质组学分子理解的多模态语言建模存在缺乏指令调优数据集和合适架构的问题。

Method: 引入指令调优数据集CPTAC - PROTSTRUCT，提出图 - 大语言模型框架KRONOS，利用分子相互作用拓扑与蛋白质组学学习患者特定图表示。

Result: KRONOS在分子分类、时间轨迹建模和肿瘤阶段预测等临床任务中表现出色。

Conclusion: 该方法使大语言模型能理解患者水平发病机制，通过更准确诊断、预后和治疗分层推动精准医学。

Abstract: Proteomics data is essential to pathogenic understanding of a disease
phenotype. In cancer, analysis of molecular signatures enables precision
medicine through the identification of biological processes that drive
individualized tumor progression, therapeutic resistance, and clinical
heterogeneity. Recent advances in multimodal large language models (LLMs) have
shown remarkable capacity to integrate and reason across heterogeneous data
modalities. However, performing multi-modal language modeling for molecular
understanding of patient-specific proteomics remains a significant challenge
due to two barriers: (1) the lack of instruction-tuning datasets that enable
clinical interpretation from proteomics data, and (2) the absence of language
modeling architectures designed to capture the rich heterogeneity of molecular
data. In this work, we introduce CPTAC-PROTSTRUCT, the first instruction tuning
dataset for molecular understanding of oncology, comprising over 400k
open-ended examples derived from individualized proteomic profiles curated from
the largest national proteomics cancer study (CPTAC). Additionally, we propose
KRONOS (Knowledge Representation of patient Omics Networks in Oncology via
Structured tuning), a novel graph-LLM framework that leverages molecular
interaction topology with proteomics to learn patient-specific graph
representations for enhanced clinical reasoning. We show that KRONOS achieves
competitive performance across benchmark clinical tasks, including molecular
classification, temporal trajectory modeling, and tumor stage prediction from
proteomics data. Ultimately, this approach empowers LLMs to understand
patient-level pathogenesis, advancing precision medicine through more accurate
diagnosis, prognosis, and treatment stratification.

</details>


### [976] [LAMP-PRo: Label-aware Attention for Multi-label Prediction of DNA- and RNA-binding Proteins using Protein Language Models](https://arxiv.org/abs/2509.24262)
*Nimisha Ghosh,Dheeran Sankaran,Rahul Balakrishnan Adhi,Sharath S,Amrut Anand*

Main category: q-bio.QM

TL;DR: 提出基于预训练蛋白质语言模型、注意力机制和多标签学习的LAMP - PRo框架，用于识别DNA和RNA结合蛋白，实验表现佳且可展示模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在区分DBPs和RBPs时因高度相似性面临挑战，交叉预测误差高，识别DRBPs也很困难。

Method: 使用预训练PLM如ESM - 2嵌入蛋白质序列，结合CNN，应用多头自注意力机制获取上下文信息，使用标签感知注意力计算特定类表示，引入新颖的交叉标签注意力机制，最后用线性层和sigmoid函数进行最终预测。

Result: 与现有方法对比，提出的模型表现出持续的竞争力。

Conclusion: LAMP - PRo框架能有效解决现有方法在识别DBPs、RBPs和DRBPs时的问题，具有良好性能和可解释性。

Abstract: Identifying DNA- (DBPs) and RNA-binding proteins (RBPs) is crucial for the
understanding of cell function, molecular interactions as well as regulatory
functions. Owing to their high similarity, most of the existing approaches face
challenges in differentiating between DBPs and RBPs leading to high
cross-prediction errors. Moreover, identifying proteins which bind to both DNA
and RNA (DRBPs) is also quite a challenging task. In this regard, we propose a
novel framework viz. LAMP-PRo which is based on pre-trained protein language
model (PLM), attention mechanisms and multi-label learning to mitigate these
issues. First, pre-trained PLM such ESM-2 is used for embedding the protein
sequences followed by convolutional neural network (CNN). Subsequently
multi-head self-attention mechanism is applied for the contextual information
while label-aware attention is used to compute class-specific representations
by attending to the sequence in a way that is tailored to each label (DBP, RBP
and non-NABP) in a multi-label setup. We have also included a novel cross-label
attention mechanism to explicitly capture dependencies between DNA- and
RNA-binding proteins, enabling more accurate prediction of DRBP. Finally, a
linear layer followed by a sigmoid function are used for the final prediction.
Extensive experiments are carried out to compare LAMP-PRo with the existing
methods wherein the proposed model shows consistent competent performance.
Furthermore, we also provide visualization to showcase model interpretability,
highlighting which parts of the sequence are most relevant for a predicted
label. The original datasets are available at http://bliulab.net/iDRBP\_MMC and
the codes are available at https://github.com/NimishaGhosh/LAMP-PRo.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [977] [Stable and Interpretable Jet Physics with IRC-Safe Equivariant Feature Extraction](https://arxiv.org/abs/2509.22059)
*Partha Konar,Vishal S. Ngairangbam,Michael Spannowsky,Deepanshu Srivastava*

Main category: hep-ph

TL;DR: 研究用图神经网络进行夸克 - 胶子区分，融入物理动机归纳偏置，结果表明融入对称性和安全性约束能改善鲁棒性并使网络表征基于已知QCD结构。


<details>
  <summary>Details</summary>
Motivation: 深度学习在喷注分类任务中面临理解模型学习内容及特征与已知QCD可观测量关系的挑战，提高可解释性对对撞机物理中构建可靠机器学习工具至关重要。

Method: 研究用于夸克 - 胶子区分的图神经网络，设计强制执行红外和共线安全以及E(2)和O(2)等变性的消息传递架构，用模拟喷注数据集对比有约束和无约束网络。

Result: 物理感知网络在训练实例中更稳定，潜在方差分布在多个可解释方向，建立了学习表征与已知IRC安全喷注可观测量的直接对应。

Conclusion: 嵌入对称性和安全约束不仅提高鲁棒性，还使网络表征基于已知QCD结构，为对撞机物理中可解释深度学习提供原则性方法。

Abstract: Deep learning has achieved remarkable success in jet classification tasks,
yet a key challenge remains: understanding what these models learn and how
their features relate to known QCD observables. Improving interpretability is
essential for building robust and trustworthy machine learning tools in
collider physics. To address this challenge, we investigate graph neural
networks for quark-gluon discrimination, systematically incorporating
physics-motivated inductive biases. In particular, we design message-passing
architectures that enforce infrared and collinear (IRC) safety, as well as E(2)
and O(2) equivariance in the rapidity-azimuth plane. Using simulated jet
datasets, we compare these networks against unconstrained baselines in terms of
classification performance, robustness to soft emissions, and latent
representation structures. Our analysis shows that physics-aware networks are
more stable across training instances and distribute their latent variance
across multiple interpretable directions. By regressing Energy Flow Polynomials
onto the leading principal components, we establish a direct correspondence
between learned representations and established IRC-safe jet observables. These
results demonstrate that embedding symmetry and safety constraints not only
improves robustness but also grounds network representations in known QCD
structures, providing a principled approach toward interpretable deep learning
in collider physics.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [978] [The Role of Logic and Automata in Understanding Transformers](https://arxiv.org/abs/2509.24024)
*Anthony W. Lin,Pablo Barcelo*

Main category: cs.FL

TL;DR: 介绍近年来变压器模型进展及回答其能力问题中逻辑与自动机的作用，提及交叉领域开放问题


<details>
  <summary>Details</summary>
Motivation: 当前对变压器模型能力理解不足，需深入探究其能力

Method: 回顾过去几年的快速进展，发挥逻辑和自动机的作用

Result: 阐述了逻辑和自动机在回答变压器模型能做什么这一问题上的关键作用

Conclusion: 提出了逻辑、自动机、验证和变压器交叉领域的几个开放问题

Abstract: The advent of transformers has in recent years led to powerful and
revolutionary Large Language Models (LLMs). Despite this, our understanding on
the capability of transformers is still meager. In this invited contribution,
we recount the rapid progress in the last few years to the question of what
transformers can do. In particular, we will see the integral role of logic and
automata (also with some help from circuit complexity) in answering this
question. We also mention several open problems at the intersection of logic,
automata, verification and transformers.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [979] [A bound-preserving multinumerics scheme for steady-state convection-diffusion equations](https://arxiv.org/abs/2509.25181)
*Maurice S. Fabien*

Main category: math.NA

TL;DR: 本文使用单元中心有限体积（FV）和间断伽辽金（DG）方法耦合求解对流扩散方程，并提出自适应分区策略，经标准基准测试验证有效。


<details>
  <summary>Details</summary>
Motivation: DG方法在对流主导区域稳定但有振荡且计算昂贵，FV方法低阶单调但成本低，需要结合两者优势。

Method: 将区域划分为FV和DG子区域并通过界面项耦合，提出自适应分区策略，根据解的单元平均值是否越界切换子区域，最后应用保守限制器。

Result: 标准基准测试证实了自适应技术的有效性。

Conclusion: 所提出的自适应分区策略能有效结合FV和DG方法的优势，实现对对流扩散方程的有效求解。

Abstract: We solve the convection-diffusion equation using a coupling of cell-centered
finite volume (FV) and discontinuous Galerkin (DG) methods. The domain is
divided into disjoint regions assigned to FV or DG, and the two methods are
coupled through an interface term. DG is stable and resolves sharp layers in
convection-dominated regimes, but it can produce sizable spurious oscillations
and is computationally expensive; FV (two-point flux) is low-order and
monotone, but inexpensive. We propose a novel adaptive partitioning strategy
that automatically selects FV and DG subdomains: whenever the solution's cell
average violates the bounds, we switch to FV on a small neighborhood of that
element. Viewed as a natural analog of $p$-adaptivity, this process is repeated
until all cell averages are bound-preserving (up to some specified tolerance).
Thereafter, standard conservative limiters may be applied to ensure the full
solution is bound-preserving. Standard benchmarks confirm the effectiveness of
the adaptive technique.

</details>


### [980] [An Accelerated Newton-GMRES Method for Multilinear PageRank](https://arxiv.org/abs/2509.23374)
*Maryam Boubekraoui,Ridwane Tahiri*

Main category: math.NA

TL;DR: 本文提出加速牛顿 - GMRES方法解决大规模网络多线性PageRank问题，实验表明该方法在效率、鲁棒性和可扩展性上优于经典牛顿法。


<details>
  <summary>Details</summary>
Motivation: 经典牛顿法求解大规模网络多线性PageRank问题时需在每次迭代求解大型线性系统，成本过高。

Method: 提出加速牛顿 - GMRES方法，利用Krylov子空间技术近似牛顿步，不显式形成大型雅可比矩阵；采用向量外推法（MPE、RRE、AA）提高收敛速度和数值稳定性。

Result: 在合成和真实数据上的大量实验表明，所提方法在效率、鲁棒性和可扩展性方面显著优于经典牛顿法求解器。

Conclusion: 加速牛顿 - GMRES方法结合向量外推法能有效解决大规模网络多线性PageRank问题。

Abstract: Modeling complex multiway relationships in large-scale networks is becoming
more and more challenging in data science. The multilinear PageRank problem,
arising naturally in the study of higher-order Markov chains, is a powerful
framework for capturing such interactions, with applications in web ranking,
recommendation systems, and social network analysis. It extends the classical
Google PageRank model to a tensor-based formulation, leading to a nonlinear
system that captures multi-way dependencies between states. Newton-based
methods can achieve local quadratic convergence for this problem, but they
require solving a large linear system at each iteration, which becomes too
costly for large-scale applications. To address this challenge, we present an
accelerated Newton-GMRES method that leverages Krylov subspace techniques to
approximate the Newton step without explicitly forming the large Jacobian
matrix. We further employ vector extrapolation methods, including Minimal
Polynomial Extrapolation (MPE), Reduced Rank Extrapolation (RRE), and Anderson
Acceleration (AA), to improve the convergence rate and enhance numerical
stability. Extensive experiments on synthetic and real-world data demonstrate
that the proposed approach significantly outperforms classical Newton-based
solvers in terms of efficiency, robustness, and scalability.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [981] [Parameterized Hardness of Zonotope Containment and Neural Network Verification](https://arxiv.org/abs/2509.22849)
*Vincent Froese,Moritz Grillo,Christoph Hertrich,Moritz Stargalla*

Main category: cs.CC

TL;DR: 本文研究2层ReLU网络相关问题复杂度，证明决定函数正性、区域包含性等问题是W[1] - 难，部分问题是NP - 难，且结果表明枚举法在指数时间假设下最优。


<details>
  <summary>Details</summary>
Motivation: 深入理解ReLU激活的神经网络所计算函数的性质，解决Froese等人提出的参数化复杂度相关开放问题。

Method: 对2层和3层ReLU网络相关问题进行理论分析和证明。

Result: 证明决定2层ReLU网络函数正性（及满射性）、区域包含性是W[1] - 难；近似最大值、计算Lp - 利普希茨常数等问题是NP - 难和W[1] - 难。

Conclusion: 所得硬度结果是目前已知最强的，在指数时间假设下，基于枚举的朴素方法对解决这些基本问题基本是最优的。

Abstract: Neural networks with ReLU activations are a widely used model in machine
learning. It is thus important to have a profound understanding of the
properties of the functions computed by such networks. Recently, there has been
increasing interest in the (parameterized) computational complexity of
determining these properties. In this work, we close several gaps and resolve
an open problem posted by Froese et al. [COLT '25] regarding the parameterized
complexity of various problems related to network verification. In particular,
we prove that deciding positivity (and thus surjectivity) of a function
$f\colon\mathbb{R}^d\to\mathbb{R}$ computed by a 2-layer ReLU network is
W[1]-hard when parameterized by $d$. This result also implies that zonotope
(non-)containment is W[1]-hard with respect to $d$, a problem that is of
independent interest in computational geometry, control theory, and robotics.
Moreover, we show that approximating the maximum within any multiplicative
factor in 2-layer ReLU networks, computing the $L_p$-Lipschitz constant for
$p\in(0,\infty]$ in 2-layer networks, and approximating the $L_p$-Lipschitz
constant in 3-layer networks are NP-hard and W[1]-hard with respect to $d$.
Notably, our hardness results are the strongest known so far and imply that the
naive enumeration-based methods for solving these fundamental problems are all
essentially optimal under the Exponential Time Hypothesis.

</details>


### [982] [Hardness and Algorithmic Results for Roman \{3\}-Domination](https://arxiv.org/abs/2509.23615)
*Sangam Balchandar Reddy*

Main category: cs.CC

TL;DR: 研究罗马{3} - 支配函数问题复杂度，证明在分裂图上NP完全、按权重参数化时W[2]难，给出块图多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 已知罗马{3} - 支配函数问题在弦图等图上NP完全，研究其在分裂图上的复杂度及按权重参数化的复杂度，解决块图上的公开问题。

Method: 对图的结构和相关定义进行理论分析和证明。

Result: 证明该问题在分裂图上NP完全，按权重参数化时W[2]难，给出块图多项式时间算法。

Conclusion: 明确了罗马{3} - 支配函数问题在分裂图上的复杂度特征，解决了块图上的公开问题。

Abstract: A Roman $\{3\}$-dominating function on a graph $G = (V, E)$ is a function $f:
V \rightarrow \{0, 1, 2, 3\}$ such that for each vertex $u \in V$, if $f(u) =
0$ then $\sum_{v \in N(u)} f(v) \geq 3$ and if $f(u) = 1$ then $\sum_{v \in
N(u)} f(v) \geq 2$. The weight of a Roman $\{3\}$-dominating function $f$ is
$\sum_{u \in V} f(u)$. The objective of \rtd{} is to compute a Roman
$\{3\}$-dominating function of minimum weight. The problem is known to be
NP-complete on chordal graphs, star-convex bipartite graphs and comb-convex
bipartite graphs. In this paper, we study the complexity of \rtd{} and show
that the problem is NP-complete on split graphs. In addition, we prove that the
problem is W[2]-hard parameterized by weight. On the positive front, we present
a polynomial-time algorithm for block graphs, thereby resolving an open
question posed by Chaudhary and Pradhan [Discrete Applied Mathematics, 2024].

</details>


<div id='physics.hist-ph'></div>

# physics.hist-ph [[Back]](#toc)

### [983] [How are Scientific Concepts Birthed? Typing Rules of Concept Formation in Theoretical Physics Reasoning](https://arxiv.org/abs/2509.10740)
*Omar Aguilar,Anthony Aguirre*

Main category: physics.hist-ph

TL;DR: 本文旨在形式化理论物理发现过程中科学概念的形成方式，引入类型理论，定义认知打字规则，应用于物理学史案例，最后对爱因斯坦时间相对性概念路径进行计算建模。


<details>
  <summary>Details</summary>
Motivation: 形式化科学概念在理论物理发现过程中的形成方式。

Method: 先论证科学概念形成可形式化，引入类型理论，定义认知打字规则，应用于物理学史案例，进行计算建模。

Result: 将物理学家发现新科学概念的方式形式化为科学发现机制，对爱因斯坦时间相对性概念路径进行计算建模。

Conclusion: 成功用类型理论形式化科学概念形成方式及相关发现机制。

Abstract: This work aims to formalize some of the ways scientific concepts are formed
in the process of theoretical physics discovery. Since this may at first seem
like a task beyond the scope of the exact sciences (natural and formal
sciences), we begin by presenting arguments for why scientific concept
formation can be formalized. Then, we introduce type theory as a natural and
well-suited framework for this formalization. We formalize what we call "ways
of discovering new concepts" including concept distinction, property
preservation, and concept change, as cognitive typing rules. Next, we apply
these cognitive typing rules to two case studies of conceptual discovery in the
history of physics: Einstein's reasoning leading to the impossibility of frozen
waves, and his conceptual path to the relativity of time. In these historical
episodes, we recast what a physicist might informally call "ways of discovering
new scientific concepts" as compositional typing rules built from cognitive
typing rules - thus formalizing them as scientific discovery mechanisms.
Lastly, we computationally model the type-theoretic reconstruction of
Einstein's conceptual path to the relativity of time as a program synthesis
task.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [984] [Graph-based Analysis for Revealing the Stochastic Gravitational Wave Background in Pulsar Timing Arrays](https://arxiv.org/abs/2509.24904)
*M. Alakhras,S. M. S. Movahed*

Main category: astro-ph.CO

TL;DR: 提出基于图的方法检测随机引力波背景（SGWB），分析相关特征，给出检测值和参数不确定性，在数据集获3σ水平证据。


<details>
  <summary>Details</summary>
Motivation: SGWB能揭示宇宙信息，脉冲星计时阵列（PTAs）适合检测纳米赫兹频段SGWB，需有效检测方法及参数不确定性研究。

Method: 在脉冲星计时残差上实现基于图的方法，构建相关图，分析图的拓扑和几何特征。

Result: 检测的特征向量含平均聚类系数和边权重波动；用边权重二阶累积量检测；当前PTAs灵敏度下最低可检测应变幅度；参数不确定性水平；在数据集获3σ证据。

Conclusion: 基于图的方法可用于SGWB检测和参数不确定性分析。

Abstract: The stochastic gravitational wave background (SGWB) reveals valuable
information about its origin and the Universe. The pulsar timing arrays (PTAs)
are suitable indicators for detecting SGWB within the nano-Hertz frequency
range. In this work, we propose a graph-based method implemented on the pulsar
timing residuals (PTRs) for SGWB detection and examining uncertainties of its
parameters. We construct a correlation graph with pulsars as its nodes, and
analyze the graph-based summary statistics, which include topological and
geometrical characteristics, for identifying SGWB in real and synthetic
datasets. The effect of the number of pulsars, the observation time span, and
the strength of the SGWB on the graph-based feature vector is evaluated. Our
results demonstrate that the merit feature vector for common signal detection
consists of the average clustering coefficient and the edge weight fluctuation.
The SGWB detection conducted after the observation of a common signal and then
exclusion of non-Hellings \& Downs templates is performed by the second
cumulant of edge weight for angular separation thresholds $\bar{\zeta}\gtrsim
40^{\circ}$. The lowest detectable value of SGWB strain amplitude utilizing our
graph-based measures at the current PTAs sensitivity is $A_{\rm SGWB}\gtrsim
1.2\times 10^{-15}$. Fisher forecasts confirmed that the uncertainty levels of
$\log_{10} A_{\rm SGWB}$ and spectral index reach $2.2\%$ and $28.3\%$,
respectively, at $2\sigma$ confidence interval. Evidence for an SGWB at the
$3\sigma$ level is obtained by applying our graph-based method to the NANOGrav
15-year dataset.

</details>


### [985] [Inferring Cosmological Parameters with Evidential Physics-Informed Neural Networks](https://arxiv.org/abs/2509.24327)
*Hai Siong Tan*

Main category: astro-ph.CO

TL;DR: 本文研究用新型物理信息神经网络预测宇宙学参数，构建机器学习框架并应用于数据集，发现不同模型参数后验分布情况及对哈勃张力有不同衡量。


<details>
  <summary>Details</summary>
Motivation: 探索用新型物理信息神经网络从超新星和重子声学振荡数据集预测宇宙学参数。

Method: 构建基于证据深度学习、物理信息神经网络、贝叶斯神经网络和高斯过程原理混合的机器学习框架，通过标准梯度下降训练学习未知偏微分方程参数的后验分布。

Result: 不同模型仅基于Pantheon+数据训练的参数后验分布大多在基于BAO数据训练的对应参数2σ轮廓内，h₀后验中位数相差约2σ。

Conclusion: 机器学习引导的方法为哈勃张力提供了不同的衡量方式。

Abstract: We examine the use of a novel variant of Physics-Informed Neural Networks to
predict cosmological parameters from recent supernovae and baryon acoustic
oscillations (BAO) datasets. Our machine learning framework generates
uncertainty estimates for target variables and the inferred unknown parameters
of the underlying PDE descriptions. Built upon a hybrid of the principles of
Evidential Deep Learning, Physics-Informed Neural Networks, Bayesian Neural
Networks and Gaussian Processes, our model enables learning of the posterior
distribution of the unknown PDE parameters through standard gradient-descent
based training. We apply our model to an up-to-date BAO dataset (Bousis et al.
2024) calibrated with the CMB-inferred sound horizon, and the Pantheon$+$ Sne
Ia distances (Scolnic et al. 2018), examining the relative effectiveness and
mutual consistency among the standard $\Lambda$CDM, $w$CDM and $\Lambda_s$CDM
models. Unlike previous results arising from the standard approach of
minimizing an appropriate $\chi^2$ function, the posterior distributions for
parameters in various models trained purely on Pantheon$+$ data were found to
be largely contained within the $2\sigma$ contours of their counterparts
trained on BAO data. Their posterior medians for $h_0$ were within about
$2\sigma$ of one another, indicating that our machine learning-guided approach
provides a different measure of the Hubble tension.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [986] [Game-Theoretic Understandings of Multi-Agent Systems with Multiple Objectives](https://arxiv.org/abs/2509.23026)
*Yue Wang*

Main category: cs.MA

TL;DR: 本文引入多目标马尔可夫博弈框架，提出帕累托 - 纳什均衡作为解决方案，证明其存在性，提出更易处理的解概念，开发在线学习算法和两阶段无偏好算法。


<details>
  <summary>Details</summary>
Motivation: 实际多智能体系统中智能体目标多样，系统复杂，存在复杂策略权衡，需解决多目标多智能体强化学习问题。

Method: 引入多目标马尔可夫博弈框架，提出帕累托 - 纳什均衡概念，证明其存在性及与线性标量化博弈纳什均衡的等价性，提出更易处理解概念，开发在线学习算法和两阶段无偏好算法。

Result: 证明了帕累托 - 纳什均衡存在，可将多目标马尔可夫博弈转化为标准单目标马尔可夫博弈求解；开发的算法能识别多目标马尔可夫博弈的单个解，可计算任意偏好配置的帕累托 - 纳什均衡。

Conclusion: 所提算法能有效解决多目标多智能体强化学习问题，可高效刻画整个帕累托 - 纳什前沿。

Abstract: In practical multi-agent systems, agents often have diverse objectives, which
makes the system more complex, as each agent's performance across multiple
criteria depends on the joint actions of all agents, creating intricate
strategic trade-offs. To address this, we introduce the Multi-Objective Markov
Game (MOMG), a framework for multi-agent reinforcement learning with multiple
objectives. We propose the Pareto-Nash Equilibrium (PNE) as the primary
solution concept, where no agent can unilaterally improve one objective without
sacrificing performance on another. We prove existence of PNE, and establish an
equivalence between the PNE and the set of Nash Equilibria of MOMG's
corresponding linearly scalarized games, enabling solutions of MOMG by
transferring to a standard single-objective Markov game. However, we note that
computing a PNE is theoretically and computationally challenging, thus we
propose and study weaker but more tractable solution concepts. Building on
these foundations, we develop online learning algorithm that identify a single
solution to MOMGs. Furthermore, we propose a two-phase, preference-free
algorithm that decouples exploration from planning. Our algorithm enables
computation of a PNE for any given preference profile without collecting new
samples, providing an efficient methodological characterization of the entire
Pareto-Nash front.

</details>


### [987] [PartnerMAS: An LLM Hierarchical Multi-Agent Framework for Business Partner Selection on High-Dimensional Features](https://arxiv.org/abs/2509.24046)
*Lingyao Li,Haolun Wu,Zhenkun Li,Jiabei Hu,Yu Wang,Xiaoshan Huang,Wenyue Hua,Wenqian Wang*

Main category: cs.MA

TL;DR: 提出PartnerMAS框架用于高维决策任务，引入基准数据集，表现优于基线，展示多智能体协作优势。


<details>
  <summary>Details</summary>
Motivation: 高维决策任务中，单智能体或辩论式系统在可扩展性和一致性上存在不足。

Method: 提出PartnerMAS分层多智能体框架，包括规划、专业和监督智能体，并引入基准数据集。

Result: 在140个案例中，PartnerMAS匹配率比基线高10 - 15%，各智能体发挥不同作用。

Conclusion: LLM智能体的结构化协作比扩展单个模型能产生更稳健的结果，PartnerMAS是数据丰富领域高维决策的有前景框架。

Abstract: High-dimensional decision-making tasks, such as business partner selection,
involve evaluating large candidate pools with heterogeneous numerical,
categorical, and textual features. While large language models (LLMs) offer
strong in-context reasoning capabilities, single-agent or debate-style systems
often struggle with scalability and consistency in such settings. We propose
PartnerMAS, a hierarchical multi-agent framework that decomposes evaluation
into three layers: a Planner Agent that designs strategies, Specialized Agents
that perform role-specific assessments, and a Supervisor Agent that integrates
their outputs. To support systematic evaluation, we also introduce a curated
benchmark dataset of venture capital co-investments, featuring diverse firm
attributes and ground-truth syndicates. Across 140 cases, PartnerMAS
consistently outperforms single-agent and debate-based multi-agent baselines,
achieving up to 10--15\% higher match rates. Analysis of agent reasoning shows
that planners are most responsive to domain-informed prompts, specialists
produce complementary feature coverage, and supervisors play an important role
in aggregation. Our findings demonstrate that structured collaboration among
LLM agents can generate more robust outcomes than scaling individual models,
highlighting PartnerMAS as a promising framework for high-dimensional
decision-making in data-rich domains.

</details>
