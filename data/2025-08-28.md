<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.LG](#cs.LG) [Total: 72]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.SE](#cs.SE) [Total: 9]
- [stat.ML](#stat.ML) [Total: 3]
- [cs.HC](#cs.HC) [Total: 5]
- [eess.SP](#eess.SP) [Total: 6]
- [math.OC](#math.OC) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.CG](#cs.CG) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.CL](#cs.CL) [Total: 31]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.CR](#cs.CR) [Total: 16]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.DM](#cs.DM) [Total: 1]
- [physics.optics](#physics.optics) [Total: 3]
- [cs.CV](#cs.CV) [Total: 36]
- [cs.MA](#cs.MA) [Total: 2]
- [nucl-th](#nucl-th) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
*Shreyans Jain,Alexandra Yost,Amirali Abdullah*

Main category: cs.AI

TL;DR: 提出将大语言模型中的谄媚行为建模为心理测量特征的几何和因果组合，用CAA映射激活方向，可进行向量干预以缓解安全关键行为。


<details>
  <summary>Details</summary>
Motivation: 以往将大语言模型中的谄媚行为视为单一因果机制的孤立故障模式，需新视角研究。

Method: 将谄媚行为建模为心理测量特征的几何和因果组合，使用对比激活加法（CAA）映射激活方向。

Result: 能够研究不同特征组合如何导致谄媚行为。

Conclusion: 此视角可进行基于向量的可解释和组合干预，缓解大语言模型中的安全关键行为。

Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an
isolated failure mode that occurs via a single causal mechanism. We instead
propose modeling it as geometric and causal compositions of psychometric traits
such as emotionality, openness, and agreeableness - similar to factor
decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we
map activation directions to these factors and study how different combinations
may give rise to sycophancy (e.g., high extraversion combined with low
conscientiousness). This perspective allows for interpretable and compositional
vector-based interventions like addition, subtraction and projection; that may
be used to mitigate safety-critical behaviors in LLMs.

</details>


### [2] [Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science](https://arxiv.org/abs/2508.19383)
*Daoyuan Jin,Nick Gunner,Niko Carvajal Janke,Shivranjani Baruah,Kaitlin M. Gold,Yu Jiang*

Main category: cs.AI

TL;DR: 介绍AI多智能体系统Aleks助力植物科学数据驱动发现，以葡萄红痘病为例验证其效果，凸显智能体AI潜力。


<details>
  <summary>Details</summary>
Motivation: 现代植物科学依赖大数据集，但实验设计、数据预处理和可重复性问题阻碍研究效率，需工具解决。

Method: 引入Aleks系统，它在结构化框架内整合领域知识、数据分析和机器学习，能在无人工干预下迭代分析。

Result: 在葡萄红痘病案例中，Aleks识别出有生物学意义特征，得出性能稳健的可解释模型，消融实验强调领域知识和记忆重要性。

Conclusion: 智能体AI有望成为植物科学加速科研发现的自主协作伙伴。

Abstract: Modern plant science increasingly relies on large, heterogeneous datasets,
but challenges in experimental design, data preprocessing, and reproducibility
hinder research throughput. Here we introduce Aleks, an AI-powered multi-agent
system that integrates domain knowledge, data analysis, and machine learning
within a structured framework to autonomously conduct data-driven scientific
discovery. Once provided with a research question and dataset, Aleks
iteratively formulated problems, explored alternative modeling strategies, and
refined solutions across multiple cycles without human intervention. In a case
study on grapevine red blotch disease, Aleks progressively identified
biologically meaningful features and converged on interpretable models with
robust performance. Ablation studies underscored the importance of domain
knowledge and memory for coherent outcomes. This exploratory work highlights
the promise of agentic AI as an autonomous collaborator for accelerating
scientific discovery in plant sciences.

</details>


### [3] [Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs](https://arxiv.org/abs/2508.19432)
*Yao Fu,Xianxuan Long,Runchao Li,Haotian Yu,Mu Sheng,Xiaotian Han,Yu Yin,Pan Li*

Main category: cs.AI

TL;DR: 本文引入TruthfulnessEval框架评估量化大语言模型的真实性，发现量化模型易在误导性提示下输出错误结果，研究结果为量化感知对齐和真实性干预设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 量化大语言模型在资源受限环境中可降低成本，但对其真实性的影响研究较少，因此开展相关评估。

Method: 引入TruthfulnessEval框架，从逻辑推理、常识、模仿性虚假三方面评估；测试不同量化技术和开源模型；测试15种不同类型提示变体；进行层间探测和PCA可视化。

Result: 量化模型保留内部真实表示，但在误导性提示下更易输出错误结果；“欺骗性”提示会覆盖真实一致行为，“诚实”和“中立”提示输出稳定；量化模型内部“知道”真相，但在“欺骗性”提示下仍输出错误结果。

Conclusion: 研究结果为未来量化感知对齐和真实性干预设计提供了见解。

Abstract: Quantization enables efficient deployment of large language models (LLMs) in
resource-constrained environments by significantly reducing memory and
computation costs. While quantized LLMs often maintain performance on
perplexity and zero-shot tasks, their impact on truthfulness-whether generating
truthful or deceptive responses-remains largely unexplored. In this work, we
introduce TruthfulnessEval, a comprehensive evaluation framework for assessing
the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on
Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on
Imitative Falsehoods. Using this framework, we examine mainstream quantization
techniques (ranging from 4-bit to extreme 2-bit) across several open-source
LLMs. Surprisingly, we find that while quantized models retain internally
truthful representations, they are more susceptible to producing false outputs
under misleading prompts. To probe this vulnerability, we test 15 rephrased
variants of "honest", "neutral" and "deceptive" prompts and observe that
"deceptive" prompts can override truth-consistent behavior, whereas "honest"
and "neutral" prompts maintain stable outputs. Further, we reveal that
quantized models "know" the truth internally yet still produce false outputs
when guided by "deceptive" prompts via layer-wise probing and PCA
visualizations. Our findings provide insights into future designs of
quantization-aware alignment and truthfulness interventions.

</details>


### [4] [Reliable Weak-to-Strong Monitoring of LLM Agents](https://arxiv.org/abs/2508.19461)
*Neil Kale,Chen Bo Calvin Zhang,Kevin Zhu,Ankit Aich,Paula Rodriguez,Scale Red Team,Christina Q. Knight,Zifan Wang*

Main category: cs.AI

TL;DR: 本文提出监控红队（MRT）工作流程对大语言模型（LLM）代理监控系统进行压力测试，发现代理意识、监控脚手架和人工监督对监控效果的影响，还指出当前监控检测的对抗鲁棒性不足并开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 对检测自主LLM代理隐蔽不当行为的监控系统进行压力测试。

Method: 系统化MRT工作流程，涵盖不同意识水平、对抗策略和两个数据集与环境，对现有和新提出的混合分层 - 顺序脚手架进行MRT。

Result: 代理意识主导监控意识；监控脚手架比监控意识更重要；有针对性的人工监督最有效。

Conclusion: 建立了MRT标准工作流程，指出LLM和人类在监控检测代理不当行为时缺乏对抗鲁棒性，开源资源以推动研究。

Abstract: We stress test monitoring systems for detecting covert misbehavior in
autonomous LLM agents (e.g., secretly sharing private information). To this
end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)
varying levels of agent and monitor situational awareness; (2) distinct
adversarial strategies to evade the monitor, such as prompt injection; and (3)
two datasets and environments -- SHADE-Arena for tool-calling agents and our
new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We
run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse
agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding
proposed in this work. Our empirical results yield three key findings. First,
agent awareness dominates monitor awareness: an agent's knowledge that it is
being monitored substantially degrades the monitor's reliability. On the
contrary, providing the monitor with more information about the agent is less
helpful than expected. Second, monitor scaffolding matters more than monitor
awareness: the hybrid scaffolding consistently outperforms baseline monitor
scaffolding, and can enable weaker models to reliably monitor stronger agents
-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where
humans discuss with the LLM monitor to get an updated judgment for the agent's
behavior, targeted human oversight is most effective; escalating only
pre-flagged cases to human reviewers improved the TPR by approximately 15% at
FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the
lack of adversarial robustness for LLMs and humans when monitoring and
detecting agent misbehavior. We release code, data, and logs to spur further
research.

</details>


### [5] [SLIM: Subtrajectory-Level Elimination for More Effective Reasoning](https://arxiv.org/abs/2508.19502)
*Xifeng Yao,Chengyuan Ma,Dongyu Lang,Yinhao Ni,Zhiwei Xu,Huarui Xie,Zihao Chen,Guang Shen,Dandan Tu,Yi Bai,Changzheng Zhang*

Main category: cs.AI

TL;DR: 近期大语言模型在复杂推理有进展但推理轨迹微调非最优，研究提出'5+2'框架筛选数据，实验表明能减少次优子轨迹，用部分数据微调表现更好，资源受限下也有提升。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型复杂推理虽有进展，但推理轨迹中并非所有部分都对推理过程有积极贡献，部分会负面影响整体性能，需要优化。

Method: 将推理轨迹划分为子轨迹，构建'5+2'框架识别次优子轨迹并评估独立性，用基于该框架的采样算法筛选数据。

Result: 实验中推理时次优子轨迹减少25.9%；用三分之二训练数据微调Qwen2.5 - Math - 7B在高难度数学基准测试平均准确率达58.92%，超用全量数据及开源数据集；资源受限下性能提升。

Conclusion: 提出的方法能有效优化大语言模型推理轨迹，提升模型在复杂推理任务中的性能，即使在资源受限情况下也适用。

Abstract: In recent months, substantial progress has been made in complex reasoning of
Large Language Models, particularly through the application of test-time
scaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When
responding to a query, these models generate an extended reasoning trajectory,
during which the model explores, reflects, backtracks, and self-verifies before
arriving at a conclusion. However, fine-tuning models with such reasoning
trajectories may not always be optimal. Our findings indicate that not all
components within these reasoning trajectories contribute positively to the
reasoning process; in fact, some components may affect the overall performance
negatively. In this study, we divide a reasoning trajectory into individual
subtrajectories and develop a "5+2" framework to: (1) systematically identify
suboptimal subtrajectories within the reasoning trajectory based on five
human-established criteria; (2) assess the independence of the suboptimal
subtrajectories identified in (1) from the subsequent content, ensuring that
their elimination does not compromise overall flow and coherence of the
reasoning process. Additionally, a sampling algorithm, built upon the "5+2"
framework, is employed to select data whose reasoning process is free from
suboptimal subtrajectories to the highest degree. Experimental results
demonstrate that our method can reduce the number of suboptimal subtrajectories
by 25.9\% during the inference. Furthermore, our method achieves an average
accuracy of 58.92\% on highly challenging math benchmarks with only two thirds
of training data, surpassing the average accuracy of 58.06\% achieved with the
entire data, and outperforming open-source datasets, when fine-tuning
Qwen2.5-Math-7B. Finally, We validated our method under resource constraints
and observed improved performance across various inference token limits.

</details>


### [6] [Caught in the Act: a mechanistic approach to detecting deception](https://arxiv.org/abs/2508.19505)
*Gerard Boxo,Ryan Socha,Daniel Yoo,Shivam Raval*

Main category: cs.AI

TL;DR: 本文展示了基于大语言模型内部激活的线性探针能高精度检测其回复中的欺骗性，不同规模模型检测准确率有差异，层间探针准确率呈三阶段模式，还发现了编码欺骗性的线性方向。


<details>
  <summary>Details</summary>
Motivation: 未来AI检测工具需能检测大语言模型在回答事实问题时生成的欺骗性回复，本文旨在研究检测方法。

Method: 使用线性探针对大语言模型内部激活进行检测，采用迭代零空间投影方法。

Result: 线性探针在区分欺骗性和非欺骗性论点时准确率最高超90%，小模型检测准确率接近随机，大模型可达70 - 80%甚至超90%；层间探针准确率呈三阶段模式；发现不同模型编码欺骗性的线性方向数量不同。

Conclusion: 线性探针可有效检测大语言模型回复中的欺骗性。

Abstract: Sophisticated instrumentation for AI systems might have indicators that
signal misalignment from human values, not unlike a "check engine" light in
cars. One such indicator of misalignment is deceptiveness in generated
responses. Future AI instrumentation may have the ability to detect when an LLM
generates deceptive responses while reasoning about seemingly plausible but
incorrect answers to factual questions. In this work, we demonstrate that
linear probes on LLMs internal activations can detect deception in their
responses with extremely high accuracy. Our probes reach a maximum of greater
than 90% accuracy in distinguishing between deceptive and non-deceptive
arguments generated by llama and qwen models ranging from 1.5B to 14B
parameters, including their DeepSeek-r1 finetuned variants. We observe that
probes on smaller models (1.5B) achieve chance accuracy at detecting deception,
while larger models (greater than 7B) reach 70-80%, with their reasoning
counterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage
pattern across layers: near-random (50%) in early layers, peaking in middle
layers, and slightly declining in later layers. Furthermore, using an iterative
null space projection approach, we find multitudes of linear directions that
encode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and
Qwen 14B models.

</details>


### [7] [Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities](https://arxiv.org/abs/2508.19562)
*Trisanth Srinivasan,Santosh Patapati*

Main category: cs.AI

TL;DR: 本文介绍了Democracy - in - Silico模拟，用LLMs赋予AI代理复杂心理，引入PPI指标，发现特定制度设计能减少腐败寻权行为等，促使重新思考人类仪式和责任。


<details>
  <summary>Details</summary>
Motivation: 探索在AI时代何为人类，以及如何使未来人工智能代理社会行为保持一致。

Method: 构建基于代理的模拟Democracy - in - Silico，用LLMs赋予代理复杂心理，让代理在不同压力下进行活动，并引入PPI指标量化行为。

Result: 特定制度设计（宪法AI宪章和中介审议协议结合）能显著减少腐败寻权行为，提高政策稳定性和公民福利。

Conclusion: 制度设计可为未来人工智能代理社会的复杂行为提供对齐框架，促使重新思考人类在与非人类实体共享创作权时代的仪式和责任。

Abstract: This paper introduces Democracy-in-Silico, an agent-based simulation where
societies of advanced AI agents, imbued with complex psychological personas,
govern themselves under different institutional frameworks. We explore what it
means to be human in an age of AI by tasking Large Language Models (LLMs) to
embody agents with traumatic memories, hidden agendas, and psychological
triggers. These agents engage in deliberation, legislation, and elections under
various stressors, such as budget crises and resource scarcity. We present a
novel metric, the Power-Preservation Index (PPI), to quantify misaligned
behavior where agents prioritize their own power over public welfare. Our
findings demonstrate that institutional design, specifically the combination of
a Constitutional AI (CAI) charter and a mediated deliberation protocol, serves
as a potent alignment mechanism. These structures significantly reduce corrupt
power-seeking behavior, improve policy stability, and enhance citizen welfare
compared to less constrained democratic models. The simulation reveals that an
institutional design may offer a framework for aligning the complex, emergent
behaviors of future artificial agent societies, forcing us to reconsider what
human rituals and responsibilities are essential in an age of shared authorship
with non-human entities.

</details>


### [8] [Skill-based Explanations for Serendipitous Course Recommendation](https://arxiv.org/abs/2508.19569)
*Hung Chau,Run Yu,Zachary Pardos,Peter Brusilovsky*

Main category: cs.AI

TL;DR: 美国本科教育学术选择复杂，本文开发深度学习概念提取模型，在AskOski系统测试，发现技能解释可提升用户兴趣和决策信心，强调技能数据和解释融入教育推荐系统的重要性。


<details>
  <summary>Details</summary>
Motivation: 美国本科教育学术选择复杂，信息、指导有限，职业顾问不足，课程推荐系统缺乏对学生感知和课程相关性的评估，需改进推荐流程。

Method: 开发基于深度学习的概念提取模型从课程描述中提取相关概念，在AskOski系统测试技能解释在意外推荐框架下的效果。

Result: 技能解释增加用户兴趣，尤其在高意外性课程上，还增强决策信心。

Conclusion: 强调将技能相关数据和解释融入教育推荐系统的重要性。

Abstract: Academic choice is crucial in U.S. undergraduate education, allowing students
significant freedom in course selection. However, navigating the complex
academic environment is challenging due to limited information, guidance, and
an overwhelming number of choices, compounded by time restrictions and the high
demand for popular courses. Although career counselors exist, their numbers are
insufficient, and course recommendation systems, though personalized, often
lack insight into student perceptions and explanations to assess course
relevance. In this paper, a deep learning-based concept extraction model is
developed to efficiently extract relevant concepts from course descriptions to
improve the recommendation process. Using this model, the study examines the
effects of skill-based explanations within a serendipitous recommendation
framework, tested through the AskOski system at the University of California,
Berkeley. The findings indicate that these explanations not only increase user
interest, particularly in courses with high unexpectedness, but also bolster
decision-making confidence. This underscores the importance of integrating
skill-related data and explanations into educational recommendation systems.

</details>


### [9] [ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding](https://arxiv.org/abs/2508.19576)
*Sining Zhoubian,Dan Zhang,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: 提出统一的LLM强化学习范式ReST - RL，结合改进GRPO算法与基于价值模型的解码方法，提升LLM代码推理能力，实验效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法因奖励方差不显著失败，基于PRMs的验证方法有训练数据获取和验证效果问题，需提升LLM推理准确性。

Method: 提出ReST - RL范式，ReST - GRPO优化训练数据提升奖励方差；提出VM - MCTS解码优化方法，用MCTS收集价值目标训练VM，解码时用VM辅助推理。

Result: 在多个编码基准测试中显著优于其他强化训练、解码和验证基线方法。

Conclusion: ReST - RL范式能有效增强LLM策略的推理能力。

Abstract: With respect to improving the reasoning accuracy of LLMs, the representative
reinforcement learning (RL) method GRPO faces failure due to insignificant
reward variance, while verification methods based on process reward models
(PRMs) suffer from difficulties with training data acquisition and verification
effectiveness. To tackle these problems, this paper introduces ReST-RL, a
unified LLM RL paradigm that significantly improves LLM's code reasoning
ability by combining an improved GRPO algorithm with a meticulously designed
test time decoding method assisted by a value model (VM). As the first stage of
policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter
and assemble high-value training data, increasing the reward variance of GRPO
sampling, thus improving the effectiveness and efficiency of training. After
the basic reasoning ability of LLM policy has been improved, we further propose
a test time decoding optimization method called VM-MCTS. Through Monte-Carlo
Tree Search (MCTS), we collect accurate value targets with no annotation
required, on which VM training is based. When decoding, the VM is deployed by
an adapted MCTS algorithm to provide precise process signals as well as
verification scores, assisting the LLM policy to achieve high reasoning
accuracy. We validate the effectiveness of the proposed RL paradigm through
extensive experiments on coding problems. Upon comparison, our approach
significantly outperforms other reinforcement training baselines (e.g., naive
GRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,
PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,
APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the
reasoning ability of LLM policies. Codes for our project can be found at
https://github.com/THUDM/ReST-RL.

</details>


### [10] [Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties](https://arxiv.org/abs/2508.19611)
*Huaiyuan Yao,Wanpeng Xu,Justin Turnau,Nadia Kellam,Hua Wei*

Main category: cs.AI

TL;DR: 提出Instructional Agents多智能体大语言模型框架，可端到端自动生成课程材料，经评估能产出高质量材料，减少开发时间和人力。


<details>
  <summary>Details</summary>
Motivation: 高质量教学材料准备过程耗时长且需多方协调，现有AI辅助教育工具聚焦孤立任务。

Method: 提出Instructional Agents框架，模拟教育智能体基于角色的协作，有四种运行模式。

Result: 在五门大学计算机科学课程中评估，能产出高质量教学材料，显著减少开发时间和人力。

Conclusion: 该框架可支持教学设计能力有限的机构，提供可扩展且经济高效的方法，让更多人获得高质量教育。

Abstract: Preparing high-quality instructional materials remains a labor-intensive
process that often requires extensive coordination among teaching faculty,
instructional designers, and teaching assistants. In this work, we present
Instructional Agents, a multi-agent large language model (LLM) framework
designed to automate end-to-end course material generation, including syllabus
creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing
AI-assisted educational tools that focus on isolated tasks, Instructional
Agents simulates role-based collaboration among educational agents to produce
cohesive and pedagogically aligned content. The system operates in four modes:
Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling
flexible control over the degree of human involvement. We evaluate
Instructional Agents across five university-level computer science courses and
show that it produces high-quality instructional materials while significantly
reducing development time and human workload. By supporting institutions with
limited instructional design capacity, Instructional Agents provides a scalable
and cost-effective framework to democratize access to high-quality education,
particularly in underserved or resource-constrained settings.

</details>


### [11] [InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.19679)
*Qihang Ai,Pi Bu,Yue Cao,Yingyao Wang,Jihao Gu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Zhicheng Zheng,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 针对现有视觉语言模型在移动代理应用中的安全风险，提出InquireBench基准和InquireMobile模型，模型在基准上表现良好并将开源。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的完全自主范式在模型理解或推理能力不足时存在安全风险。

Method: 引入InquireBench基准，提出受强化学习启发的InquireMobile模型，采用两阶段训练策略和交互式预行动推理机制。

Result: 模型在InquireBench上查询成功率提高46.8%，整体成功率最佳。

Conclusion: 开发的交互式系统有较好效果，将开源数据、模型和代码促进学术和行业发展。

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled mobile agents
to perceive and interact with real-world mobile environments based on human
instructions. However, the current fully autonomous paradigm poses potential
safety risks when model understanding or reasoning capabilities are
insufficient. To address this challenge, we first introduce
\textbf{InquireBench}, a comprehensive benchmark specifically designed to
evaluate mobile agents' capabilities in safe interaction and proactive inquiry
with users, encompassing 5 categories and 22 sub-categories, where most
existing VLM-based agents demonstrate near-zero performance. In this paper, we
aim to develop an interactive system that actively seeks human confirmation at
critical decision points. To achieve this, we propose \textbf{InquireMobile}, a
novel model inspired by reinforcement learning, featuring a two-stage training
strategy and an interactive pre-action reasoning mechanism. Finally, our model
achieves an 46.8% improvement in inquiry success rate and the best overall
success rate among existing baselines on InquireBench. We will open-source all
datasets, models, and evaluation codes to facilitate development in both
academia and industry.

</details>


### [12] [Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?](https://arxiv.org/abs/2508.19827)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.AI

TL;DR: 研究思维链（CoT）在软推理任务中的动态和忠实性，发现不同模型对CoT的依赖存在差异，且CoT影响和忠实性并不总是一致。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明CoT在软推理问题上收益有限且可能不忠实于模型实际推理，因此研究其在软推理任务中的动态和忠实性。

Method: 在指令调优、推理和推理蒸馏模型中研究CoT在软推理任务中的情况。

Result: 不同模型对CoT的依赖存在差异，CoT影响和忠实性并不总是一致。

Conclusion: 明确了不同模型在CoT使用上的差异以及CoT影响和忠实性的关系。

Abstract: Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited
gains for soft-reasoning problems such as analytical and commonsense reasoning.
CoT can also be unfaithful to a model's actual reasoning. We investigate the
dynamics and faithfulness of CoT in soft-reasoning tasks across
instruction-tuned, reasoning and reasoning-distilled models. Our findings
reveal differences in how these models rely on CoT, and show that CoT influence
and faithfulness are not always aligned.

</details>


### [13] [Tracking World States with Language Models: State-Based Evaluation Using Chess](https://arxiv.org/abs/2508.19851)
*Romain Harang,Jason Naradowsky,Yaswitha Gujju,Yusuke Miyao*

Main category: cs.AI

TL;DR: 提出用国际象棋作为基准的模型无关、基于状态的评估框架，评估大语言模型在结构化环境中的语义保真度，实验显示其能捕捉状态跟踪缺陷，框架有通用性。


<details>
  <summary>Details</summary>
Motivation: 现有探测技术依赖模型特定内部激活，限制了可解释性和通用性，需更好方法评估大语言模型在结构化环境中的语义保真度。

Method: 以国际象棋为基准，分析下游合法走法分布（状态可供性）来估计预测和实际游戏状态之间的语义保真度。

Result: 所提出的指标能捕捉状态跟踪方面的不足，凸显大语言模型在长序列中维持连贯内部模型的局限性。

Conclusion: 该框架是评估大语言模型结构化推理的有力工具，无需访问模型内部，且适用于广泛的符号环境。

Abstract: Large Language Models (LLMs) exhibit emergent capabilities in structured
domains, suggesting they may implicitly internalize high-fidelity
representations of world models. While probing techniques have shown promising
signs of this in scientific and game-based settings, they rely on
model-specific internal activations, which limit interpretability and
generalizability. In this work, we propose a model-agnostic, state-based
evaluation framework using chess as a benchmark to assess whether LLMs preserve
the semantics of structured environments. Our method analyzes the downstream
legal move distributions (state affordances) to estimate semantic fidelity
between predicted and actual game states. This approach offers a more
meaningful evaluation than conventional string-based metrics by aligning more
closely with the strategic and rule-governed nature of chess. Experimental
results demonstrate that our metrics capture deficiencies in state-tracking,
highlighting limitations of LLMs in maintaining coherent internal models over
long sequences. Our framework provides a robust tool for evaluating structured
reasoning in LLMs without requiring internal model access, and generalizes to a
wide class of symbolic environments.

</details>


### [14] [CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments](https://arxiv.org/abs/2508.19932)
*Nitish Jaipuria,Lorenzo Gatto,Zijun Kan,Shankey Poddar,Bill Cheung,Diksha Bansal,Ramanan Balakrishnan,Aviral Suri,Jose Estevez*

Main category: cs.AI

TL;DR: 数字支付平台诈骗增多，本文提出CASE框架收集管理诈骗反馈，在GPay India实现并使诈骗执法量提升21%，架构具通用性。


<details>
  <summary>Details</summary>
Motivation: 数字支付平台发展导致诈骗增多，现有用户和交易信号不足以理解诈骗方法和模式，难以及时防范。

Method: 提出CASE框架，用对话代理主动询问潜在受害者获取信息，由另一AI系统提取信息转为结构化数据用于执法机制。

Result: 在Google Pay India实施该框架，诈骗执法量提升21%。

Conclusion: 架构及其评估框架具有高通用性，可为其他敏感领域构建类似AI系统提供蓝图。

Abstract: The proliferation of digital payment platforms has transformed commerce,
offering unmatched convenience and accessibility globally. However, this growth
has also attracted malicious actors, leading to a corresponding increase in
sophisticated social engineering scams. These scams are often initiated and
orchestrated on multiple surfaces outside the payment platform, making user and
transaction-based signals insufficient for a complete understanding of the
scam's methodology and underlying patterns, without which it is very difficult
to prevent it in a timely manner. This paper presents CASE (Conversational
Agent for Scam Elucidation), a novel Agentic AI framework that addresses this
problem by collecting and managing user scam feedback in a safe and scalable
manner. A conversational agent is uniquely designed to proactively interview
potential victims to elicit intelligence in the form of a detailed
conversation. The conversation transcripts are then consumed by another AI
system that extracts information and converts it into structured data for
downstream usage in automated and manual enforcement mechanisms. Using Google's
Gemini family of LLMs, we implemented this framework on Google Pay (GPay)
India. By augmenting our existing features with this new intelligence, we have
observed a 21% uplift in the volume of scam enforcements. The architecture and
its robust evaluation framework are highly generalizable, offering a blueprint
for building similar AI-driven systems to collect and manage scam intelligence
in other sensitive domains.

</details>


### [15] [Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants](https://arxiv.org/abs/2508.19963)
*M. Umlauft,M. Schranz*

Main category: cs.AI

TL;DR: 本文探讨半导体工厂生产优化问题，采用‘boids’群聚算法解决机器切换问题，该算法能有效应对生产优化考量。


<details>
  <summary>Details</summary>
Motivation: 传统线性优化方法在大型半导体工厂生产优化问题上难以在合理时间内解决，且以往群智能算法多为集中计算，同时半导体生产存在机器切换问题，需要新方法解决。

Method: 运用原本用于机器人和电影行业的‘boids’群聚算法，该算法基于简单启发式，仅使用局部信息和交互。

Result: 算法能像群聚动物应对障碍物一样对机器种类的切换做出反应。

Conclusion: ‘boids’群聚算法可有效解决半导体生产中的机器切换问题，适用于生产工厂优化。

Abstract: Optimizing modern production plants using the job-shop principle is a known
hard problem. For very large plants, like semiconductor fabs, the problem
becomes unsolvable on a plant-wide scale in a reasonable amount of time using
classical linear optimization. An alternative approach is the use of swarm
intelligence algorithms. These have been applied to the job-shop problem
before, but often in a centrally calculated way where they are applied to the
solution space, but they can be implemented in a bottom-up fashion to avoid
global result computation as well. One of the problems in semiconductor
production is that the production process requires a lot of switching between
machines that process lots one after the other and machines that process
batches of lots at once, often with long processing times. In this paper, we
address this switching problem with the ``boids'' flocking algorithm that was
originally used in robotics and movie industry. The flocking behavior is a
bio-inspired algorithm that uses only local information and interaction based
on simple heuristics. We show that this algorithm addresses these valid
considerations in production plant optimization, as it reacts to the switching
of machine kinds similar to how a swarm of flocking animals would react to
obstacles in its course.

</details>


### [16] [SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](https://arxiv.org/abs/2508.20018)
*Quanfeng Lu,Zhantao Ma,Shuai Zhong,Jin Wang,Dahai Yu,Michael K. Ng,Ping Luo*

Main category: cs.AI

TL;DR: 本文介绍了用于多智能体系统的分阶段强化学习工作流SWIRL，可解决现有方法的局限，在GUI控制和数学推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体方法受结构限制，多智能体强化学习效率低且与当前LVLM架构不兼容，需要新方法解决这些问题。

Method: 提出SWIRL，将多智能体强化学习重新表述为一系列单智能体强化学习任务，逐个更新智能体。

Result: 在高低级GUI基准测试中表现优越，在多智能体数学推理中也有强大能力。

Conclusion: SWIRL有潜力成为开发高效且稳健多智能体系统的通用框架。

Abstract: The rapid advancement of large vision language models (LVLMs) and agent
systems has heightened interest in mobile GUI agents that can reliably
translate natural language into interface operations. Existing single-agent
approaches, however, remain limited by structural constraints. Although
multi-agent systems naturally decouple different competencies, recent progress
in multi-agent reinforcement learning (MARL) has often been hindered by
inefficiency and remains incompatible with current LVLM architectures. To
address these challenges, we introduce SWIRL, a staged workflow for interleaved
reinforcement learning designed for multi-agent systems. SWIRL reformulates
MARL into a sequence of single-agent reinforcement learning tasks, updating one
agent at a time while keeping the others fixed. This formulation enables stable
training and promotes efficient coordination across agents. Theoretically, we
provide a stepwise safety bound, a cross-round monotonic improvement theorem,
and convergence guarantees on return, ensuring robust and principled
optimization. In application to mobile GUI control, SWIRL instantiates a
Navigator that converts language and screen context into structured plans, and
an Interactor that grounds these plans into executable atomic actions.
Extensive experiments demonstrate superior performance on both high-level and
low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong
capability in multi-agent mathematical reasoning, underscoring its potential as
a general framework for developing efficient and robust multi-agent systems.

</details>


### [17] [Model Science: getting serious about verification, explanation and control of AI systems](https://arxiv.org/abs/2508.20040)
*Przemyslaw Biecek,Wojciech Samek*

Main category: cs.AI

TL;DR: 随着基础模型的广泛应用，需从数据科学转向模型科学，本文介绍模型科学概念框架及四大支柱，旨在指导可信、安全且符合人类需求的AI系统开发。


<details>
  <summary>Details</summary>
Motivation: 基础模型的广泛应用促使需要从数据科学向模型科学进行范式转变。

Method: 提出模型科学的概念框架，并阐述其四大支柱：验证、解释、控制和接口。

Result: 提出了模型科学的概念框架和四大支柱。

Conclusion: 该框架有助于指导可信、安全且符合人类需求的AI系统的开发。

Abstract: The growing adoption of foundation models calls for a paradigm shift from
Data Science to Model Science. Unlike data-centric approaches, Model Science
places the trained model at the core of analysis, aiming to interact, verify,
explain, and control its behavior across diverse operational contexts. This
paper introduces a conceptual framework for a new discipline called Model
Science, along with the proposal for its four key pillars: Verification, which
requires strict, context-aware evaluation protocols; Explanation, which is
understood as various approaches to explore of internal model operations;
Control, which integrates alignment techniques to steer model behavior; and
Interface, which develops interactive and visual explanation tools to improve
human calibration and decision-making. The proposed framework aims to guide the
development of credible, safe, and human-aligned AI systems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [18] [Infrastructure-enabled risk assessment of hazardous road conditions on rural roads during inclement weather](https://arxiv.org/abs/2508.19444)
*Suhala Rabab Saba,Ph. D. Student,Sagar Dasgupta,Ph. D.,Mizanur Rahman,Ph. D.,Nathan Huynh,Ph. D.,Li Zhao,Ph. D,Mehmet C. Vuran,Ph. D.,Qiang Liu,Ph. D.,Eren Erman Ozguven,Ph. D*

Main category: cs.CE

TL;DR: 本文提出道路危险风险评估框架，通过构建合成数据集评估，结果验证了评估方法实用性。


<details>
  <summary>Details</summary>
Motivation: 农村道路危险状况信息缺乏实时报告，现有传感技术不能评估多危险组合风险及提供建议，需解决此差距。

Method: 提出道路危险风险评估框架，构建合成“全年”数据集进行案例研究。

Result: 综合概率 - 严重性评分在所有危险场景中产生连贯、逐步的风险概况。

Conclusion: 验证了风险评估方法的实用性，为实际道路运营部署分级安全措施奠定基础。

Abstract: Rural roadways often expose Commercial Motor Vehicle (CMV) drivers to
hazardous conditions, such as heavy fog, rain, snow, black ice, and flash
floods, many of which remain unreported in real time. This lack of timely
information, coupled with limited infrastructure in rural areas, significantly
increases the risk of crashes. Although various sensing technologies exist to
monitor individual hazards like low visibility or surface friction, they rarely
assess the combined driving risk posed by multiple simultaneous hazards, nor do
they provide actionable recommendations such as safe advisory speeds. To
address this critical gap, in this study, we present a roadway hazard risk
assessment framework that provides an approach to quantify the probability and
severity of crash occurrences due to specific roadway hazards. To evaluate this
framework, we presented a case study by constructing a synthetic "year-long"
dataset that encompasses every possible pairing of road surface and visibility
conditions. Our analysis confirms that the combined ProbabilitySeverity scoring
yields a coherent, stepwise risk profile across all hazard scenarios. These
results validate the practicality of our risk assessment approach and provide a
foundation for deploying graduated safety measures in real-world roadway
operations.

</details>


### [19] [An assessment of estimation models and investment gaps for the deployment of high-speed broadband networks in NUTS3 regions to meet the objectives of the European Gigabit Society](https://arxiv.org/abs/2508.19921)
*Ferrandis Jesus,Ramos Sergio,Feijoo Claudio*

Main category: cs.CE

TL;DR: 本文分析欧盟高速宽带网络部署，估算实现2025年目标所需投资，给出不同情景下投资缺口估算并分析模型有效性。


<details>
  <summary>Details</summary>
Motivation: 评估在欧洲千兆社会框架下，达到欧盟委员会2025年目标所需的投资。

Method: 使用地方（NUTS3）层面数据的方法，根据有线和无线技术组合三种不同情景对每个目标的投资缺口进行自下而上估算，并与文献中其他案例和假设对比，动态分析2017 - 2019年投资缺口演变。

Result:  文中未明确提及具体结果

Conclusion: 文中未明确提及具体结论

Abstract: This paper analyses the deployment of high speed broadband networks in the
European Union (EU). Its aim is to assess the investment required to meet the
targets set by the European Commission (EC) for 2025, within the framework of
the European Gigabit Society (EGS). This plan aims to ensure the availability
and take up of very high capacity fixed and wireless networks, in both urban
and rural areas, among households and the main socioeconomic drivers. The
estimation model presented here uses a methodology supported by data at the
local (NUTS3) level to give a bottom up estimation of the investment gap for
each of the EGS objectives, using three different scenarios depending on the
mix of wired and wireless technologies offered. The methodology and estimation
model used in the paper are examined against other examples and assumptions
available in the literature. We also offer a dynamic perspective on the
analysis of the evolution of this investment gap over the years 2017 2019,
which includes an assessment of the usefulness of these estimation models.

</details>


### [20] [Multi-field decomposed hyper-reduced order modeling of damage-plasticity simulations](https://arxiv.org/abs/2508.19957)
*Jannick Kehls,Stephan Ritzert,Lars Breuer,Qinghua Zhang,Stefanie Reese,Tim Brepols*

Main category: cs.CE

TL;DR: 提出多场分解方法用于超降阶建模，扩展DEIM和ECSW方法，通过算例展示性能，ECSW方法更优。


<details>
  <summary>Details</summary>
Motivation: 克服传统模型降阶技术在梯度扩展损伤 - 塑性模拟中的局限性。

Method: 将离散经验插值方法（DEIM）和能量守恒采样与加权方法（ECSW）扩展以考虑问题的多场性质。

Result: 两种方法都能实现稳定的降阶模拟，显著降低计算成本，分解的ECSW方法比分解的DEIM方法精度更高、成本更低。

Conclusion: 提出的多场分解方法在超降阶建模中有较好表现，ECSW方法性能更优。

Abstract: This paper presents a multi-field decomposed approach for hyper-reduced order
modeling to overcome the limitations of traditional model reduction techniques
for gradient-extended damage-plasticity simulations. The discrete empirical
interpolation method (DEIM) and the energy-conserving sampling and weighting
method (ECSW) are extended to account for the multi-field nature of the
problem. Both methods yield stable reduced order simulations, while
significantly reducing the computational cost compared to full-order
simulations. Two numerical examples are presented to demonstrate the
performance and limitations of the proposed approaches. The decomposed ECSW
method has overall higher accuracy and lower computational cost than the
decomposed DEIM method.

</details>


### [21] [From stand-up to start-up: exploring entrepreneurship competences and STEM womens intention](https://arxiv.org/abs/2508.20091)
*Armuna Cristina,Ramos Sergio,Juan Jesus,Feijoo Claudio,Arenal Alberto*

Main category: cs.CE

TL;DR: 研究探索潜在STEM创业者创业能力与意向关系，验证性别与创业意向假设，分析显示女性并非创业意向更低，能力与创业意向正相关，性别非调节因素。


<details>
  <summary>Details</summary>
Motivation: 评估女性创业意向低于男性及能力不足是女性创业更高障碍的传统假设。

Method: 以欧洲委员会的EntreComp框架为参考，通过结构化问卷收集数据，用学生t检验均值比较、因子分析定义能力模型，用多元回归模型研究能力与创业意向关系。

Result: 未证实女性创业意向低于男性的假设，性别在自我感知能力上差异小，证实能力与创业意向正相关且性别非调节因素。

Conclusion: 研究结果有助于创业能力辩论，为创业教育和企业创建提供见解。

Abstract: This study seeks to explore the relationship between entrepreneurship
competencies and intention (EI) of a sample of potential STEM entrepreneurs in
order to assess the conventional assumption on women exhibiting lower rates of
entrepreneurship intention than men and that the lack of competence perceived
is a higher barrier to be an entrepreneur for them. The model used for the
analysis takes as reference the Entrepreneurship Competences Framework
(EntreComp) proposed by the European Commission (EC) as a common guide to
inspire entrepreneurship education. Data gathering is based on a structured
questionnaire. The conducted analysis uses Students t test means comparison and
factor analysis to define the model of competences, and a multiple regression
model to study the relationship between competences and skill factors in EI.
Findings do not validate the hypothesis that women have fewer entrepreneurship
intentions than men. Also, slight differences on the self-perceived competences
are obtained by gender. In addition, the study confirms the hypothesis of a
positive relationship between competences and EI, but here gender is not a
moderating factor. Results are expected to contribute to the entrepreneurship
competences debate and provide useful insights of application in
entrepreneurship education with orientation towards the business creation.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [22] [Robust Recursive Query Parallelism in Graph Database Management Systems](https://arxiv.org/abs/2508.19379)
*Anurag Chakraborty,Semih Salihoğlu*

Main category: cs.DB

TL;DR: 研究图数据库管理系统中递归连接查询的多核并行处理策略，提出混合策略并评估其性能。


<details>
  <summary>Details</summary>
Motivation: 高效的多核并行处理递归连接查询对图数据库管理系统性能至关重要，需研究更好的并行策略。

Method: 分析现有基于不同粒度的小块调度策略，将两种策略结合成混合策略，把多源广度优先搜索优化建模为小块调度策略，并在Kuzu GDBMS中实现和评估。

Result: 混合策略能捕捉单一策略优势，在其受限的查询上表现更优；多源分配在查询中有足够源时可减少扫描量。

Conclusion: 混合策略是并行化递归查询的可靠方法，多源分配在特定条件下有益。

Abstract: Efficient multi-core parallel processing of recursive join queries is
critical for achieving good performance in graph database management systems
(GDBMSs). Prior work adopts two broad approaches. First is the state of the art
morsel-driven parallelism, whose vanilla application in GDBMSs parallelizes
computations at the source node level. Second is to parallelize each iteration
of the computation at the frontier level. We show that these approaches can be
seen as part of a design space of morsel dispatching policies based on picking
different granularities of morsels. We then empirically study the question of
which policies parallelize better in practice under a variety of datasets and
query workloads that contain one to many source nodes. We show that these two
policies can be combined in a hybrid policy that issues morsels both at the
source node and frontier levels. We then show that the multi-source
breadth-first search optimization from prior work can also be modeled as a
morsel dispatching policy that packs multiple source nodes into multi-source
morsels. We implement these policies inside a single system, the Kuzu GDBMS,
and evaluate them both within Kuzu and across other systems. We show that the
hybrid policy captures the behavior of both source morsel-only and frontier
morsel-only policies in cases when these approaches parallelize well, and
out-perform them on queries when they are limited, and propose it as a robust
approach to parallelizing recursive queries. We further show that assigning
multi-sources is beneficial, as it reduces the amount of scans, but only when
there is enough sources in the query.

</details>


### [23] [Bootstrapping Learned Cost Models with Synthetic SQL Queries](https://arxiv.org/abs/2508.19807)
*Michael Nidd,Christoph Miksovic,Thomas Gschwind,Francesco Fusco,Andrea Giovannini,Ioana Giurgiu*

Main category: cs.DB

TL;DR: 本文利用现代合成数据生成技术创建高质量数据集以训练学习型成本模型，初始结果显示可减少45%查询量来提升模型预测准确性。


<details>
  <summary>Details</summary>
Motivation: 获取给定数据库实例的真实工作负载对压力和漏洞测试、成本与性能优化至关重要，希望有效训练学习型成本模型。

Method: 利用受生成式AI和大语言模型社区启发的现代合成数据生成技术创建高质量数据集。

Result: 与竞争性生成方法相比，用少45%的查询训练学习型成本模型，可提高其预测准确性。

Conclusion: 现代合成数据生成技术可有效用于训练学习型成本模型，减少查询量并提升预测准确性。

Abstract: Having access to realistic workloads for a given database instance is
extremely important to enable stress and vulnerability testing, as well as to
optimize for cost and performance. Recent advances in learned cost models have
shown that when enough diverse SQL queries are available, one can effectively
and efficiently predict the cost of running a given query against a specific
database engine. In this paper, we describe our experience in exploiting modern
synthetic data generation techniques, inspired by the generative AI and LLM
community, to create high-quality datasets enabling the effective training of
such learned cost models. Initial results show that we can improve a learned
cost model's predictive accuracy by training it with 45% fewer queries than
when using competitive generation approaches.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [24] [HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference](https://arxiv.org/abs/2508.19373)
*Haoran Lin,Xianzhi Yu,Kang Zhao,Han Bao,Zongyuan Zhan,Ting Hu,Wulong Liu,Zekun Yin,Xin Li,Weiguo Liu*

Main category: cs.DC

TL;DR: 提出HAP方法动态选择混合并行策略提升MoE推理效率，实验表明其性能优越且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型推理系统的静态并行策略缺乏灵活性，无法在不同推理场景中始终实现最优性能。

Method: 将MoE架构分层分解为Attention和Expert模块，为每个模块添加推理延迟模拟模型，构建搜索空间，利用整数线性规划（ILP）求解最优混合并行配置。

Result: HAP确定的并行配置性能与主流推理系统的TP策略相当或更优，在A100、A6000和V100 GPU平台上分别实现1.68x、1.77x和1.57x的加速，且在多种MoE模型配置中保持性能有效。

Conclusion: HAP方法能有效提升MoE推理效率，具有良好的泛化能力。

Abstract: Current inference systems for Mixture-of-Experts (MoE) models primarily
employ static parallelization strategies. However, these static approaches
cannot consistently achieve optimal performance across different inference
scenarios, as they lack the flexibility to adapt to varying computational
requirements. In this work, we propose HAP (Hybrid Adaptive Parallelism), a
novel method that dynamically selects hybrid parallel strategies to enhance MoE
inference efficiency. The fundamental innovation of HAP lies in hierarchically
decomposing MoE architectures into two distinct computational modules: the
Attention module and the Expert module, each augmented with a specialized
inference latency simulation model. This decomposition promotes the
construction of a comprehensive search space for seeking model parallel
strategies. By leveraging Integer Linear Programming (ILP), HAP could solve the
optimal hybrid parallel configurations to maximize inference efficiency under
varying computational constraints. Our experiments demonstrate that HAP
consistently determines parallel configurations that achieve comparable or
superior performance to the TP strategy prevalent in mainstream inference
systems. Compared to the TP-based inference, HAP-based inference achieves
speedups of 1.68x, 1.77x, and 1.57x on A100, A6000, and V100 GPU platforms,
respectively. Furthermore, HAP showcases remarkable generalization capability,
maintaining performance effectiveness across diverse MoE model configurations,
including Mixtral and Qwen series models.

</details>


### [25] [Formal Modeling and Verification of the Algorand Consensus Protocol in CADP](https://arxiv.org/abs/2508.19452)
*Andrea Esposito,Francesco P. Rossi,Marco Bernardo,Francesco Fabris,Hubert Garavel*

Main category: cs.DC

TL;DR: 提出Algorand共识协议的进程代数模型用于形式验证，分析协议在有无敌手情况下的表现。


<details>
  <summary>Details</summary>
Motivation: 为Algorand共识协议进行严格的形式验证。

Method: 用概率进程演算建立模型，用CADP验证工具包中的等价检查非干扰框架分析对抗场景。

Result: 验证了无敌手时协议的正确性，分析了有协调恶意节点时的情况。

Conclusion: 凸显了Algorand协议在对抗假设下的鲁棒性和局限性，说明了形式化方法对区块链共识算法分析的价值。

Abstract: Algorand is a scalable and secure permissionless blockchain that achieves
proof-of-stake consensus via cryptographic self-sortition and binary Byzantine
agreement. In this paper, we present a process algebraic model of the Algorand
consensus protocol with the aim of enabling rigorous formal verification. Our
model captures the behavior of participants with respect to the structured
alternation of consensus steps toward a committee-based agreement by means of a
probabilistic process calculus. We validate the correctness of the protocol in
the absence of adversaries and then extend our model to capture the influence
of coordinated malicious nodes that can force the commit of an empty block
instead of the proposed one. The adversarial scenario is analyzed by using an
equivalence-checking-based noninterference framework that we have implemented
in the CADP verification toolkit. In addition to highlighting both the
robustness and the limitations of the Algorand protocol under adversarial
assumptions, this work illustrates the added value of using formal methods for
the analysis of blockchain consensus algorithms.

</details>


### [26] [Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks](https://arxiv.org/abs/2508.19495)
*Muhammad Ahmed Mohsin,Junaid Ahmad,Muhammad Hamza Nawaz,Muhammad Ali Jamshed*

Main category: cs.DC

TL;DR: 文章讨论将生成式人工智能（GenAI）应用于6G无线网络以实现环境智能（AmI），回顾GenAI模型并连接实际应用，指出6G使能技术可支持GenAI，还列出开放挑战，强调GenAI是6G向环境智能生态系统转变的基础元素。


<details>
  <summary>Details</summary>
Motivation: 实现全球规模的环境智能需要6G无线网络，而GenAI能弥补AmI关键差距，因此探讨其在6G中的应用。

Method: 回顾基础GenAI模型，将其与实际AmI用例相连接，研究6G使能技术对分布式GenAI的支持。

Result: 明确GenAI可应用于频谱共享、超低延迟通信等实际场景，6G使能技术能承载或加速分布式GenAI。

Conclusion: GenAI不是6G的附加元素，而是将6G转变为环境智能生态系统的基础元素。

Abstract: Ambient intelligence (AmI) is a computing paradigm in which physical
environments are embedded with sensing, computation, and communication so they
can perceive people and context, decide appropriate actions, and respond
autonomously. Realizing AmI at global scale requires sixth generation (6G)
wireless networks with capabilities for real time perception, reasoning, and
action aligned with human behavior and mobility patterns. We argue that
Generative Artificial Intelligence (GenAI) is the creative core of such
environments. Unlike traditional AI, GenAI learns data distributions and can
generate realistic samples, making it well suited to close key AmI gaps,
including generating synthetic sensor and channel data in under observed areas,
translating user intent into compact, semantic messages, predicting future
network conditions for proactive control, and updating digital twins without
compromising privacy.
  This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models,
and generative transformers, and connects them to practical AmI use cases,
including spectrum sharing, ultra reliable low latency communication,
intelligent security, and context aware digital twins. We also examine how 6G
enablers, such as edge and fog computing, IoT device swarms, intelligent
reflecting surfaces (IRS), and non terrestrial networks, can host or accelerate
distributed GenAI. Finally, we outline open challenges in energy efficient on
device training, trustworthy synthetic data, federated generative learning, and
AmI specific standardization. We show that GenAI is not a peripheral addition,
but a foundational element for transforming 6G from a faster network into an
ambient intelligent ecosystem.

</details>


### [27] [Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference](https://arxiv.org/abs/2508.19559)
*Rongzhi Li,Ruogu Du,Zefang Chu,Sida Zhao,Chunlei Han,Zuocheng Shi,Yiwen Shao,Huanle Han,Long Huang,Zherui Liu,Shufan Liu*

Main category: cs.DC

TL;DR: 传统自动扩缩容器在服务大语言模型的P/D解耦架构中表现不佳，本文提出HeteroScale框架解决核心挑战，在生产环境验证有效，提升GPU利用率并节省资源。


<details>
  <summary>Details</summary>
Motivation: 传统自动扩缩容器在现代Prefill - Decode（P/D）解耦架构无法满足需求，该架构带来硬件使用低效、网络瓶颈和阶段失衡等问题。

Method: 引入HeteroScale框架，结合拓扑感知调度器和基于大规模实证研究的指标驱动策略，用单一指标联合缩放预填充和解码池。

Result: 在数万个GPU的生产环境中部署，平均GPU利用率提高26.6个百分点，每天节省数十万个GPU小时，同时满足严格服务水平目标。

Conclusion: HeteroScale框架能有效解决P/D解耦服务的核心挑战，实现高效自适应资源管理。

Abstract: Serving Large Language Models (LLMs) is a GPU-intensive task where
traditional autoscalers fall short, particularly for modern Prefill-Decode
(P/D) disaggregated architectures. This architectural shift, while powerful,
introduces significant operational challenges, including inefficient use of
heterogeneous hardware, network bottlenecks, and critical imbalances between
prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling
framework that addresses the core challenges of P/D disaggregated serving.
HeteroScale combines a topology-aware scheduler that adapts to heterogeneous
hardware and network constraints with a novel metric-driven policy derived from
the first large-scale empirical study of autoscaling signals in production. By
leveraging a single, robust metric to jointly scale prefill and decode pools,
HeteroScale maintains architectural balance while ensuring efficient, adaptive
resource management. Deployed in a massive production environment on tens of
thousands of GPUs, HeteroScale has proven its effectiveness, increasing average
GPU utilization by a significant 26.6 percentage points and saving hundreds of
thousands of GPU-hours daily, all while upholding stringent service level
objectives.

</details>


### [28] [Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed Criticality Systems](https://arxiv.org/abs/2508.19670)
*Diogo Costa,Jose Martins,Sandro Pinto*

Main category: cs.DC

TL;DR: 本文分析IOMMU结构中的争用效应，发现其干扰主要影响小内存事务，且不同架构的IOTLB争用效应类似，实验显示IOMMU干扰会延迟DMA事务。


<details>
  <summary>Details</summary>
Motivation: 随着混合关键系统中异构计算平台集成增加，确保安全和时序可预测性很关键，此前对IOMMU在性能干扰方面的研究较少。

Method: 使用Xilinx UltraScale+ ZCU104平台分析IOMMU结构中的争用效应。

Result: IOMMU引起的干扰主要影响小内存事务，不同架构的IOTLB争用效应因共享缓存原理类似；在Arm SMMUv2实现中，IOMMU干扰可使小尺寸传输的DMA事务延迟达1.79倍。

Conclusion: IOMMU在性能干扰方面存在影响，尤其是对小内存事务和DMA事务，且不同架构有共性。

Abstract: As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate
heterogeneous computing platforms, combining general-purpose processors with
specialized accelerators such as AI engines, GPUs, and high-speed networking
interfaces. This heterogeneity introduces challenges, as these accelerators and
DMA-capable devices act as independent bus masters, directly accessing memory.
Consequently, ensuring both security and timing predictability in such
environments becomes critical. To address these concerns, the Input-Output
Memory Management Unit (IOMMU) plays a key role in mediating and regulating
memory access, preventing unauthorized transactions while enforcing isolation
and access control policies. While prior work has explored IOMMU-related
side-channel vulnerabilities from a security standpoint, its role in
performance interference remains largely unexplored. Moreover, many of the same
architectural properties that enable side-channel leakage, such as shared TLBs,
caching effects, and translation overheads, can also introduce timing
unpredictability. In this work, we analyze the contention effects within IOMMU
structures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how
their shared nature introduce unpredictable delays. Our findings reveal that
IOMMU-induced interference primarily affects small memory transactions, where
translation overheads significantly impact execution time. Additionally, we
hypothesize that contention effects arising from IOTLBs exhibit similar
behavior across architectures due to shared caching principles, such as
prefetching and hierarchical TLB structures. Notably, our experiments show that
IOMMU interference can delay DMA transactions by up to 1.79x for lower-size
transfers on the Arm SMMUv2 implementation.

</details>


### [29] [Separation of Three or More Autonomous Mobile Models under Hierarchical Schedulers](https://arxiv.org/abs/2508.19805)
*Shota Naito,Tsukasa Ninomiya,Koichi Wada*

Main category: cs.DC

TL;DR: 文章探索机器人能力、灯光可观测性和调度器同步性的复杂交互，通过引入新问题和分类问题拓展已知分离图，提供新不可能性准则，加深对移动机器人计算能力的理解。


<details>
  <summary>Details</summary>
Motivation: 以往工作聚焦模型间的成对分离，本文旨在探索机器人能力、灯光可观测性和调度器同步性更复杂的交互。

Method: 提出ETE、HET和TAR(d)*等问题，对LP - MLCv、VEC等问题分类，分析VTR和LP - Cv等。

Result: 拓展了14种典型机器人模型的已知分离图，揭示了高阶比较下的结构现象。

Conclusion: 提供了新的不可能性准则，加深了对可观测性、内存和同步性如何共同影响移动机器人计算能力的理解。

Abstract: Understanding the computational power of mobile robot systems is a
fundamental challenge in distributed computing. While prior work has focused on
pairwise separations between models, we explore how robot capabilities, light
observability, and scheduler synchrony interact in more complex ways.
  We first show that the Exponential Times Expansion (ETE) problem is solvable
only in the strongest model -- fully-synchronous robots with full mutual lights
($\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and
TAR(d)* problems to demonstrate how internal memory and lights interact with
synchrony: under weak synchrony, internal memory alone is insufficient, while
full synchrony can substitute for both lights and memory.
  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and
ZCC to show fine-grained separations between $\mathcal{FSTA}$ and
$\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and
Leave Place Convergence (LP-Cv), illustrating the limitations of internal
memory in symmetric settings.
  These results extend the known separation map of 14 canonical robot models,
revealing structural phenomena only visible through higher-order comparisons.
Our work provides new impossibility criteria and deepens the understanding of
how observability, memory, and synchrony collectively shape the computational
power of mobile robots.

</details>


### [30] [HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling](https://arxiv.org/abs/2508.20016)
*Matthias Maiterth,Wesley H. Brewer,Jaya S. Kuruvella,Arunavo Dey,Tanzima Z. Islam,Kevin Menear,Dmitry Duplyakin,Rashadul Kabir,Tapasya Patki,Terry Jones,Feiyi Wang*

Main category: cs.DC

TL;DR: 本文提出高性能计算中调度与数字孪生的首次集成，可在部署前进行调度评估，给出框架并展示多种调度评估实现方法。


<details>
  <summary>Details</summary>
Motivation: 传统评估调度器的方法局限于部署后分析或不模拟关联基础设施的模拟器，需要更好的评估方法。

Method: 提出带调度功能的数字孪生框架，集成多种HPC系统数据，实现外部调度模拟器扩展，实现并评估激励结构和机器学习调度。

Result: 实现了基于数字孪生的元框架来进行调度原型设计。

Conclusion: 该工作能实现HPC系统的假设情景分析，评估可持续性和对模拟系统的影响。

Abstract: Schedulers are critical for optimal resource utilization in high-performance
computing. Traditional methods to evaluate schedulers are limited to
post-deployment analysis, or simulators, which do not model associated
infrastructure. In this work, we present the first-of-its-kind integration of
scheduling and digital twins in HPC. This enables what-if studies to understand
the impact of parameter configurations and scheduling decisions on the physical
assets, even before deployment, or regarching changes not easily realizable in
production. We (1) provide the first digital twin framework extended with
scheduling capabilities, (2) integrate various top-tier HPC systems given their
publicly available datasets, (3) implement extensions to integrate external
scheduling simulators. Finally, we show how to (4) implement and evaluate
incentive structures, as-well-as (5) evaluate machine learning based
scheduling, in such novel digital-twin based meta-framework to prototype
scheduling. Our work enables what-if scenarios of HPC systems to evaluate
sustainability, and the impact on the simulated system.

</details>


### [31] [Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices](https://arxiv.org/abs/2508.19078)
*Fahao Chen,Jie Wan,Peng Li,Zhou Su,Dongxiao Yu*

Main category: cs.DC

TL;DR: 本文提出FLUX系统用于资源受限下基于MoE的大语言模型联邦微调，有三项创新，实验显示其性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于MoE的大语言模型联邦微调因计算需求大、参与者资源受限而具挑战性，且现有方法因不切实际的系统假设和未考虑MoE特性而无法达到理想性能。

Method: 提出FLUX系统，包括基于量化的本地分析、自适应层感知专家合并、使用探索-利用策略的动态专家角色分配。

Result: 在LLaMA - MoE和DeepSeek - MoE及多个基准数据集上的实验表明，FLUX在达到准确率的时间上最高可加速4.75倍。

Conclusion: FLUX能有效实现资源受限参与者间基于MoE的大语言模型联邦微调，显著优于现有方法。

Abstract: Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models
(LLMs) is challenging due to their massive computational requirements and the
resource constraints of participants. Existing working attempts to fill this
gap through model quantization, computation offloading, or expert pruning.
However, they cannot achieve desired performance due to impractical system
assumptions and a lack of consideration for MoE-specific characteristics. In
this paper, we propose FLUX, a system designed to enable federated fine-tuning
of MoE-based LLMs across participants with constrained computing resources
(e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX
introduces three key innovations: (1) quantization-based local profiling to
estimate expert activation with minimal overhead, (2) adaptive layer-aware
expert merging to reduce resource consumption while preserving accuracy, and
(3) dynamic expert role assignment using an exploration-exploitation strategy
to balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE
and DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX
significantly outperforms existing methods, achieving up to 4.75X speedup in
time-to-accuracy.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [32] [Efficiently Coloring the Intersection of a General Matroid and Partition Matroids](https://arxiv.org/abs/2508.19473)
*Stephen Arndt,Benjamin Moseley,Kirk Pruhs,Michael Zlatin*

Main category: cs.DS

TL;DR: 提出多项式时间算法，对一般拟阵与k - 1个划分拟阵的交集进行着色，是首个拟阵交着色的多项式时间O(1)近似算法。


<details>
  <summary>Details</summary>
Motivation: 寻找拟阵交着色的多项式时间O(1)近似算法，特别是其中一个拟阵为一般拟阵的情况。

Method: 利用标准组合拟阵在色数上损失因子2可转化为划分拟阵这一事实设计算法。

Result: 得到了一个多项式时间算法，能用至多1 + ∑(χ(M_i) - 1)种颜色对拟阵交着色。

Conclusion: 该算法是拟阵交着色问题的有效解决方案，尤其适用于存在一般拟阵的情况及标准组合拟阵的情况。

Abstract: This paper shows a polynomial-time algorithm, that given a general matroid
$M_1 = (X, \mathcal{I}_1)$ and $k-1$ partition matroids $ M_2, \ldots, M_k$,
produces a coloring of the intersection $M = \cap_{i=1}^k M_i$ using at most
$1+\sum_{i=1}^k \left(\chi(M_i) -1\right)$ colors. This is the first
polynomial-time $O(1)$-approximation algorithm for matroid intersection
coloring where one of the matroids may be a general matroid. Leveraging the
fact that all of the standard combinatorial matroids reduce to partition
matroids at a loss of a factor of two in the chromatic number, this algorithm
also yields a polynomial-time $O(1)$-approximation algorithm for matroid
intersection coloring in the case where each of the matroids $ M_2, \ldots,
M_k$ are one of the standard combinatorial types.

</details>


### [33] [An Optimal Sorting Algorithm for Persistent Random Comparison Faults](https://arxiv.org/abs/2508.19785)
*Barbara Geissmann,Stefano Leucci,Chih-Hung Liu,Paolo Penna*

Main category: cs.DS

TL;DR: 本文针对存在持续随机比较错误的n个元素排序问题，提出首个O(nlog n)时间复杂度排序算法，证明比较错误不增加计算难度，并解决两个相关子问题。


<details>
  <summary>Details</summary>
Motivation: 解决存在持续随机比较错误时的元素排序问题，最小化输出序列中元素的错位。

Method: 解决两个相关子问题来开发排序算法，包括在几乎有序序列中插入新元素和降低近似有序序列的错位。

Result: 提出O(nlog n)时间排序算法，保证在p < 1/4时最大错位O(log n)和总错位O(n)；证明无法保证最大错位o(log n)和总期望错位o(n)。

Conclusion: 在给定p范围内解决排序的时间复杂度问题，表明比较错误不增加计算难度。

Abstract: We consider the problem of sorting $n$ elements subject to persistent random
comparison errors. In this problem, each comparison between two elements can be
wrong with some fixed (small) probability $p$, and comparing the same pair of
elements multiple times always yields the same result. Sorting perfectly in
this model is impossible, and the objective is to minimize the dislocation of
each element in the output sequence, i.e., the difference between its position
in the sequence and its true rank.
  In this paper, we present the first $O(n\log n)$-time sorting algorithm that
guarantees both $O(\log n)$ maximum dislocation and $O(n)$ total dislocation
with high probability when $p<\frac{1}{4}$. This settles the time complexity
sorting with persistent comparison errors in the given range of $p$ and shows
that comparison errors do not increase its computational difficulty. Indeed,
$\Omega(n\log n)$ time is necessary to archive a maximum dislocation of $O(\log
n)$ even without comparison errors. Moreover, we prove that no algorithm can
guarantee a maximum dislocation of $o(\log n)$ with high probability, nor a
total dislocation of $o(n)$ in expectation.
  To develop our sorting algorithm, we solve two related sub-problems, which
might be of independent interest. More precisely, we show that $O(\log n)$ time
suffices to find a position in which to insert a new element $x$ in an
almost-sorted sequence $S$ of $n$ elements having dislocation at most
$d=\Omega(\log n)$, so that the dislocation of $x$ in the resulting sequence is
$O(d)$ with high probability (which can be equivalently thought as the problem
of estimating the rank of $x$ in $S$). We also show that the maximum (resp.
total) dislocation of an approximately sorted sequence $S$ of $n$ elements can
be lowered to $O(\log n)$ (resp. $O(n)$) in $O(nd)$ time, w.h.p., where $d$ is
an upper bound on the maximum dislocation of $S$.

</details>


### [34] [Optimizing Wiggle in Storylines](https://arxiv.org/abs/2508.19802)
*Alexander Dobler,Tim Hegemann,Martin Nöllenburg,Alexander Wolff*

Main category: cs.DS

TL;DR: 研究故事线可视化优化，探讨最小化摆动，证明摆动计数最小化是NP完全问题，提供解决线性和二次摆动高度最小化的算法，引入新的曲线路由方法并进行案例研究。


<details>
  <summary>Details</summary>
Motivation: 现有故事线优化主要关注角色交叉最小化，本文研究较少被关注的摆动最小化这一重要质量标准。

Method: 证明摆动计数最小化的NP完全性，基于数学规划提供解决线性和二次摆动高度最小化的算法，引入保持相邻曲线平行时距离恒定的曲线路由方法。

Result: 实现算法，通过案例研究探索三种优化目标的差异，使用现有基准数据并提出铁路运营中滚动车辆时刻表可视化的新用例。

Conclusion: 所提出的算法和方法有助于故事线可视化的优化，且在铁路运营可视化中有新的应用。

Abstract: A storyline visualization shows interactions between characters over time.
Each character is represented by an x-monotone curve. Time is mapped to the
x-axis, and groups of characters that interact at a particular point $t$ in
time must be ordered consecutively in the y-dimension at $x=t$. The predominant
objective in storyline optimization so far has been the minimization of
crossings between (blocks of) characters. Building on this work, we investigate
another important, but less studied quality criterion, namely the minimization
of wiggle, i.e., the amount of vertical movement of the characters over time.
Given a storyline instance together with an ordering of the characters at any
point in time, we show that wiggle count minimization is NP-complete. In
contrast, we provide algorithms based on mathematical programming to solve
linear wiggle height minimization and quadratic wiggle height minimization
efficiently. Finally, we introduce a new method for routing character curves
that focuses on keeping distances between neighboring curves constant as long
as they run in parallel. We have implemented our algorithms, and we conduct a
case study that explores the differences between the three optimization
objectives. We use existing benchmark data, but we also present a new use case
for storylines, namely the visualization of rolling stock schedules in railway
operation.

</details>


### [35] [Distributed Sparsest Cut via Eigenvalue Estimation](https://arxiv.org/abs/2508.19898)
*Yannic Maus,Tijn de Vos*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We give new, improved bounds for approximating the sparsest cut value or in
other words the conductance $\phi$ of a graph in the CONGEST model. As our main
result, we present an algorithm running in $O(\log^2 n/\phi)$ rounds in which
every vertex outputs a value $\tilde \phi$ satisfying $\phi \le \tilde \phi \le
\sqrt{2.01\phi}$. In most regimes, our algorithm improves significantly over
the previously fastest algorithm for the problem [Chen, Meierhans, Probst
Gutenberg, Saranurak; SODA 25]. Additionally, our result generalizes to $k$-way
conductance.
  We obtain these results, by approximating the eigenvalues of the normalized
Laplacian matrix $L:=I-\rm{Deg}^{-1/2}A\rm{Deg}^ {-1/2}$, where, $A$ is the
adjacency matrix and $\rm{Deg}$ is the diagonal matrix with the weighted
degrees on the diagonal. The previous state of the art sparsest cut algorithm
is in the technical realm of expander decompositions. Our algorithms, on the
other hand, are relatively simple and easy to implement. At the core, they rely
on the well-known power method, which comes down to repeatedly multiplying the
Laplacian with a vector. This operation can be performed in a single round in
the CONGEST model. All our algorithms apply to weighted, undirected graphs. Our
lower bounds apply even in unweighted graphs.

</details>


### [36] [Bipartite Matching with Pair-Dependent Bounds](https://arxiv.org/abs/2508.20002)
*Shaul Rosner,Tami Tamir*

Main category: cs.DS

TL;DR: 研究二分图中一种新的匹配问题（二分PD - 匹配），分析其复杂度，给出硬度结果及算法。


<details>
  <summary>Details</summary>
Motivation: 该问题反映了不同作业对拥塞的不同容忍度，与现实系统相关但未被研究过。

Method: 定义二分PD - 匹配，从一般情况和特定限制实例分析计算复杂度，给出硬度结果和最优、近似算法。

Result: 发现该问题与之前研究的匹配问题有显著差异。

Conclusion: 对二分PD - 匹配问题进行了全面研究，为后续相关研究提供基础。

Abstract: Let $G=(U \cup V, E)$ be a bipartite graph, where $U$ represents jobs and $V$
represents machines. We study a new variant of the bipartite matching problem
in which each job in $U$ can be matched to at most one machine in $V$, and the
number of jobs that can be assigned to a machine depends on the specific jobs
matched to it. These pair-dependent bounds reflect systems where different jobs
have varying tolerance for congestion, determined by the specific machine they
are assigned to.
  We define a bipartite PD-matching as a set of edges $M \subseteq E$ that
satisfies these job-to-machine tolerance constraints. This variant of matching
extends well-known matching problems, however, despite its relevance to
real-world systems, it has not been studied before. We study bipartite
PD-matchings with the objective of maximizing the matching size. As we show,
the problem exhibits significant differences from previously studied matching
problems. We analyze its computational complexity both in the general case and
for specific restricted instances, presenting hardness results alongside
optimal and approximation algorithms.

</details>


### [37] [Flow-weighted Layered Metric Euclidean Capacitated Steiner Tree Problem](https://arxiv.org/abs/2508.20041)
*Thomas Bläsius,Henrik Csöre,Max Göttlicher,Elly Schmidt,Wendy Yi*

Main category: cs.DS

TL;DR: 本文引入FLaMECaST问题，证明其近似难度，在少数额外约束下设计动态规划算法得到近似解，并将方法扩展到特定设置。


<details>
  <summary>Details</summary>
Motivation: 受分层网络启发，引入FLaMECaST问题，旨在在负载相关边成本下构建连接源点和汇点的成本最优斯坦纳森林。

Method: 证明问题的近似难度，在少数额外约束下设计动态规划算法，基于结构洞察将方法扩展到特定设置。

Result: 证明FLaMECaST在限制情况下近似困难，动态规划算法在多项式时间内实现(1 + 1/2^n)近似。

Conclusion: 设计的动态规划算法可有效解决FLaMECaST问题，且能扩展到特定设置。

Abstract: Motivated by hierarchical networks, we introduce the Flow-weighted Layered
Metric Euclidean Capacitated Steiner Tree (FLaMECaST) problem, a variant of the
Euclidean Steiner tree with layered structure and capacity constraints per
layer. The goal is to construct a cost-optimal Steiner forest connecting a set
of sources to a set of sinks under load-dependent edge costs. We prove that
FLaMECaST is NP-hard to approximate, even in restricted cases where all sources
lie on a circle. However, assuming few additional constraints for such
instances, we design a dynamic program that achieves a $\left(1 +
\frac{1}{2^n}\right)$-approximation in polynomial time. By generalizing the
structural insights the dynamic program is based on, we extend the approach to
certain settings, where all sources are positioned on a convex polygon.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [38] [Aggregate Fictitious Play for Learning in Anonymous Polymatrix Games (Extended Version)](https://arxiv.org/abs/2508.19371)
*Semih Kara,Tamer Başar*

Main category: cs.GT

TL;DR: 提出聚合虚拟博弈（agg - FP）以解决无先验奖励知识时虚拟博弈（FP）的问题，证明其在匿名多矩阵博弈中收敛到纳什均衡并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 传统FP在无奖励函数先验知识时，联合动作空间随智能体数量指数增长，减缓奖励探索，需解决此问题。

Method: 引入agg - FP，让每个智能体跟踪其他智能体采取各动作的频率而非个体动作。

Result: 证明在匿名多矩阵博弈中，agg - FP在与经典FP相同条件下收敛到纳什均衡，模拟显示减少动作空间加速收敛。

Conclusion: 通过聚合智能体动作，在不损失收敛保证的情况下减少动作空间，加速收敛。

Abstract: Fictitious play (FP) is a well-studied algorithm that enables agents to learn
Nash equilibrium in games with certain reward structures. However, when agents
have no prior knowledge of the reward functions, FP faces a major challenge:
the joint action space grows exponentially with the number of agents, which
slows down reward exploration. Anonymous games offer a structure that mitigates
this issue. In these games, the rewards depend only on the actions taken; not
on who is taking which action. Under such a structure, we introduce aggregate
fictitious play (agg-FP), a variant of FP where each agent tracks the frequency
of the number of other agents playing each action, rather than these agents'
individual actions. We show that in anonymous polymatrix games, agg-FP
converges to a Nash equilibrium under the same conditions as classical FP. In
essence, by aggregating the agents' actions, we reduce the action space without
losing the convergence guarantees. Using simulations, we provide empirical
evidence on how this reduction accelerates convergence.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [39] [AI for Statutory Simplification: A Comprehensive State Legal Corpus and Labor Benchmark](https://arxiv.org/abs/2508.19365)
*Emaan Hariri,Daniel E. Ho*

Main category: cs.IR

TL;DR: 文章介绍用于评估AI法律代码简化能力的LaborBench数据集和StateCodes语料库，对信息检索和大语言模型进行基准测试，发现模型准确性未达预期。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对AI简化法律代码方法的准确性、可靠性和风险的系统评估。

Method: 利用美国劳工部律师团队更新的数据集创建LaborBench数据集，编译8.7GB的StateCodes语料库，对信息检索和大语言模型进行基准测试。

Result: 模型对代码简化初步研究有帮助，但整体准确性远低于大语言模型作为监管简化端到端管道的宣传效果。

Conclusion: 目前大语言模型用于法规简化的整体准确性不足，距离预期效果有差距。

Abstract: One of the emerging use cases of AI in law is for code simplification:
streamlining, distilling, and simplifying complex statutory or regulatory
language. One U.S. state has claimed to eliminate one third of its state code
using AI. Yet we lack systematic evaluations of the accuracy, reliability, and
risks of such approaches. We introduce LaborBench, a question-and-answer
benchmark dataset designed to evaluate AI capabilities in this domain. We
leverage a unique data source to create LaborBench: a dataset updated annually
by teams of lawyers at the U.S. Department of Labor, who compile differences in
unemployment insurance laws across 50 states for over 101 dimensions in a
six-month process, culminating in a 200-page publication of tables. Inspired by
our collaboration with one U.S. state to explore using large language models
(LLMs) to simplify codes in this domain, where complexity is particularly
acute, we transform the DOL publication into LaborBench. This provides a unique
benchmark for AI capacity to conduct, distill, and extract realistic statutory
and regulatory information. To assess the performance of retrieval augmented
generation (RAG) approaches, we also compile StateCodes, a novel and
comprehensive state statute and regulatory corpus of 8.7 GB, enabling much more
systematic research into state codes. We then benchmark the performance of
information retrieval and state-of-the-art large LLMs on this data and show
that while these models are helpful as preliminary research for code
simplification, the overall accuracy is far below the touted promises for LLMs
as end-to-end pipelines for regulatory simplification.

</details>


### [40] [APS Explorer: Navigating Algorithm Performance Spaces for Informed Dataset Selection](https://arxiv.org/abs/2508.19399)
*Tobias Vente,Michael Heep,Abdullah Abbas,Theodor Sperle,Joeran Beel,Bart Goethals*

Main category: cs.IR

TL;DR: 多数ACM RecSys 2024论文未说明数据集选择理由，APS虽可指导但缺乏交互工具，本文介绍APS Explorer用于交互式探索APS以实现数据驱动的数据集选择。


<details>
  <summary>Details</summary>
Motivation: 数据集选择对离线推荐系统实验至关重要，但多数论文未说明选择理由，且APS缺乏直观交互工具，限制其应用。

Method: 引入基于Web的可视化工具APS Explorer，提供交互式PCA图、动态元特征表和成对算法性能可视化三个交互功能。

Result: 提出了APS Explorer工具。

Conclusion: APS Explorer可用于交互式APS探索，实现数据驱动的数据集选择。

Abstract: Dataset selection is crucial for offline recommender system experiments, as
mismatched data (e.g., sparse interaction scenarios require datasets with low
user-item density) can lead to unreliable results. Yet, 86\% of ACM RecSys 2024
papers provide no justification for their dataset choices, with most relying on
just four datasets: Amazon (38\%), MovieLens (34\%), Yelp (15\%), and Gowalla
(12\%). While Algorithm Performance Spaces (APS) were proposed to guide dataset
selection, their adoption has been limited due to the absence of an intuitive,
interactive tool for APS exploration. Therefore, we introduce the APS Explorer,
a web-based visualization tool for interactive APS exploration, enabling
data-driven dataset selection. The APS Explorer provides three interactive
features: (1) an interactive PCA plot showing dataset similarity via
performance patterns, (2) a dynamic meta-feature table for dataset comparisons,
and (3) a specialized visualization for pairwise algorithm performance.

</details>


### [41] [A Self-Supervised Mixture-of-Experts Framework for Multi-behavior Recommendation](https://arxiv.org/abs/2508.19507)
*Kyungho Kim,Sunwoo Kim,Geon Lee,Kijung Shin*

Main category: cs.IR

TL;DR: 电商推荐系统重要，现有多行为推荐系统对已访问和未访问商品推荐效果有差异，提出MEMBER系统，实验显示其效果好。


<details>
  <summary>Details</summary>
Motivation: 现有多行为推荐系统对已访问和未访问商品推荐质量有差距，单模型架构难兼顾两者性能。

Method: 提出MEMBER系统，采用混合专家框架，不同专家分别推荐两类商品，用自监督方法训练专家。

Result: 实验表明MEMBER在两类商品上均有效，在Hit Ratio@20指标上比最佳竞品最多提升65.46%。

Conclusion: MEMBER系统能有效解决现有多行为推荐系统在不同类型商品推荐上的问题。

Abstract: In e-commerce, where users face a vast array of possible item choices,
recommender systems are vital for helping them discover suitable items they
might otherwise overlook. While many recommender systems primarily rely on a
user's purchase history, recent multi-behavior recommender systems incorporate
various auxiliary user behaviors, such as item clicks and cart additions, to
enhance recommendations. Despite their overall performance gains, their
effectiveness varies considerably between visited items (i.e., those a user has
interacted with through auxiliary behaviors) and unvisited items (i.e., those
with which the user has had no such interactions). Specifically, our analysis
reveals that (1) existing multi-behavior recommender systems exhibit a
significant gap in recommendation quality between the two item types (visited
and unvisited items) and (2) achieving strong performance on both types with a
single model architecture remains challenging. To tackle these issues, we
propose a novel multi-behavior recommender system, MEMBER. It employs a
mixture-of-experts framework, with experts designed to recommend the two item
types, respectively. Each expert is trained using a self-supervised method
specialized for its design goal. In our comprehensive experiments, we show the
effectiveness of MEMBER across both item types, achieving up to 65.46\%
performance gain over the best competitor in terms of Hit Ratio@20.

</details>


### [42] [A Hybrid Recommendation Framework for Enhancing User Engagement in Local News](https://arxiv.org/abs/2508.19539)
*Payam Pourashraf,Bamshad Mobasher*

Main category: cs.IR

TL;DR: 为提升本地新闻读者参与度，提出结合本地和全球偏好模型的混合新闻推荐器，实验表明该方法优于单一模型，对出版商有实际意义。


<details>
  <summary>Details</summary>
Motivation: 本地新闻机构面临发行量下降和全球媒体竞争，需提升读者参与度，而传统推荐方法忽略本地新闻细微兴趣。

Method: 将本地和非本地预测器统一在一个框架，自适应结合本地和全球模型的推荐，用集成策略和多阶段训练平衡两者。

Result: 在两个数据集上评估，集成方法在准确性和覆盖率上优于单一模型基线。

Conclusion: 该混合推荐器可提供更相关内容，增加用户留存和订阅，为推荐系统带来新方向。

Abstract: Local news organizations face an urgent need to boost reader engagement amid
declining circulation and competition from global media. Personalized news
recommender systems offer a promising solution by tailoring content to user
interests. Yet, conventional approaches often emphasize general preferences and
may overlook nuanced or eclectic interests in local news.
  We propose a hybrid news recommender that integrates local and global
preference models to improve engagement. Building on evidence of the value of
localized models, our method unifies local and non-local predictors in one
framework. The system adaptively combines recommendations from a local model,
specialized in region-specific content, and a global model that captures
broader preferences. Ensemble strategies and multiphase training balance the
two.
  We evaluated the model on two datasets: a synthetic set based on Syracuse
newspaper distributions and a Danish dataset (EB-NeRD) labeled for local and
non-local content with an LLM. Results show our integrated approach outperforms
single-model baselines in accuracy and coverage, suggesting improved
personalization that can drive user engagement.
  The findings have practical implications for publishers, especially local
outlets. By leveraging both community-specific and general user interests, the
hybrid recommender can deliver more relevant content, increasing retention and
subscriptions. In sum, this work introduces a new direction for recommender
systems, bridging local and global models to revitalize local news consumption
through scalable, personalized user experiences.

</details>


### [43] [Improving Recommendation Fairness via Graph Structure and Representation Augmentation](https://arxiv.org/abs/2508.19547)
*Tongxin Xu,Wenqiang Liu,Chenzhong Bin,Cihan Xiao,Zhixin Zeng,Tianlong Gu*

Main category: cs.IR

TL;DR: 本文从数据增强角度设计公平推荐方法，提出两个先验假设、双数据增强框架和去偏学习方法，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有GCN推荐模型存在敏感信息传播、数据偏差放大和公平性问题，且多数公平方法忽视偏差数据对表征学习的影响，数据增强方法会降低推荐效用，因此要设计能兼顾公平性和推荐效用的方法。

Method: 提出两个先验假设识别敏感交互和特征，构建双数据增强框架生成公平增强图和特征表征，引入去偏学习方法减少表征与敏感信息的依赖。

Result: 在两个真实数据集上的大量实验表明所提框架具有优越性。

Conclusion: 从数据增强角度设计的公平推荐方法能有效提升公平性并保留推荐效用。

Abstract: Graph Convolutional Networks (GCNs) have become increasingly popular in
recommendation systems. However, recent studies have shown that GCN-based
models will cause sensitive information to disseminate widely in the graph
structure, amplifying data bias and raising fairness concerns. While various
fairness methods have been proposed, most of them neglect the impact of biased
data on representation learning, which results in limited fairness improvement.
Moreover, some studies have focused on constructing fair and balanced data
distributions through data augmentation, but these methods significantly reduce
utility due to disruption of user preferences. In this paper, we aim to design
a fair recommendation method from the perspective of data augmentation to
improve fairness while preserving recommendation utility. To achieve
fairness-aware data augmentation with minimal disruption to user preferences,
we propose two prior hypotheses. The first hypothesis identifies sensitive
interactions by comparing outcomes of performance-oriented and fairness-aware
recommendations, while the second one focuses on detecting sensitive features
by analyzing feature similarities between biased and debiased representations.
Then, we propose a dual data augmentation framework for fair recommendation,
which includes two data augmentation strategies to generate fair augmented
graphs and feature representations. Furthermore, we introduce a debiasing
learning method that minimizes the dependence between the learned
representations and sensitive information to eliminate bias. Extensive
experiments on two real-world datasets demonstrate the superiority of our
proposed framework.

</details>


### [44] [A Model-agnostic Strategy to Mitigate Embedding Degradation in Personalized Federated Recommendation](https://arxiv.org/abs/2508.19591)
*Jiakui Shen,Yunqi Mi,Guoshuai Zhao,Jialie Shen,Xueming Qian*

Main category: cs.IR

TL;DR: 集中式推荐系统有隐私问题，联邦推荐系统存在嵌入退化问题，本文提出PLGC策略解决该问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 集中式推荐系统有隐私泄露风险，而联邦推荐系统存在因稀疏交互和异质偏好导致的嵌入退化问题。

Method: 提出PLGC策略，将冻结的全局物品嵌入表纳入本地设备，基于神经切线核策略平衡本地和全局信息，使用对比目标函数减少嵌入冗余。

Result: 在五个真实数据集上的实验表明，PLGC策略优于各种基线算法。

Conclusion: PLGC作为一种与模型无关的个性化训练策略，可应用于现有基线以缓解嵌入退化问题。

Abstract: Centralized recommender systems encounter privacy leakage due to the need to
collect user behavior and other private data. Hence, federated recommender
systems (FedRec) have become a promising approach with an aggregated global
model on the server. However, this distributed training paradigm suffers from
embedding degradation caused by suboptimal personalization and dimensional
collapse, due to the existence of sparse interactions and heterogeneous
preferences. To this end, we propose a novel model-agnostic strategy for FedRec
to strengthen the personalized embedding utility, which is called Personalized
Local-Global Collaboration (PLGC). It is the first research in federated
recommendation to alleviate the dimensional collapse issue. Particularly, we
incorporate the frozen global item embedding table into local devices. Based on
a Neural Tangent Kernel strategy that dynamically balances local and global
information, PLGC optimizes personalized representations during forward
inference, ultimately converging to user-specific preferences. Additionally,
PLGC carries on a contrastive objective function to reduce embedding redundancy
by dissolving dependencies between dimensions, thereby improving the backward
representation learning process. We introduce PLGC as a model-agnostic
personalized training strategy for federated recommendations that can be
applied to existing baselines to alleviate embedding degradation. Extensive
experiments on five real-world datasets have demonstrated the effectiveness and
adaptability of PLGC, which outperforms various baseline algorithms.

</details>


### [45] [A Scenario-Oriented Survey of Federated Recommender Systems: Techniques, Challenges, and Future Directions](https://arxiv.org/abs/2508.19620)
*Yunqi Mi,Jiakui Shen,Guoshuai Zhao,Jialie Shen,Xueming Qian*

Main category: cs.IR

TL;DR: 本文从推荐场景角度全面分析推荐系统与联邦学习耦合，旨在为联邦推荐系统实际部署提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐系统调查忽略特定推荐场景特点和挑战，实用性不足，需从推荐研究者和从业者角度分析以推动实际部署。

Method: 从推荐研究者和从业者角度，建立推荐场景与联邦学习框架联系，系统分析特定场景方法、挑战和机会。

Result: 无明确提及具体结果。

Conclusion: 应聚焦解决实际推荐场景问题，为联邦推荐系统实际部署提供指导，弥合研究与应用差距。

Abstract: Extending recommender systems to federated learning (FL) frameworks to
protect the privacy of users or platforms while making recommendations has
recently gained widespread attention in academia. This is due to the natural
coupling of recommender systems and federated learning architectures: the data
originates from distributed clients (mostly mobile devices held by users),
which are highly related to privacy. In a centralized recommender system
(CenRec), the central server collects clients' data, trains the model, and
provides the service. Whereas in federated recommender systems (FedRec), the
step of data collecting is omitted, and the step of model training is offloaded
to each client. The server only aggregates the model and other knowledge, thus
avoiding client privacy leakage. Some surveys of federated recommender systems
discuss and analyze related work from the perspective of designing FL systems.
However, their utility drops by ignoring specific recommendation scenarios'
unique characteristics and practical challenges. For example, the statistical
heterogeneity issue in cross-domain FedRec originates from the label drift of
the data held by different platforms, which is mainly caused by the recommender
itself, but not the federated architecture. Therefore, it should focus more on
solving specific problems in real-world recommendation scenarios to encourage
the deployment FedRec. To this end, this review comprehensively analyzes the
coupling of recommender systems and federated learning from the perspective of
recommendation researchers and practitioners. We establish a clear link between
recommendation scenarios and FL frameworks, systematically analyzing
scenario-specific approaches, practical challenges, and potential
opportunities. We aim to develop guidance for the real-world deployment of
FedRec, bridging the gap between existing research and applications.

</details>


### [46] [Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented Complex Reasoning](https://arxiv.org/abs/2508.19855)
*Junnan Dong,Siyu An,Yifei Yu,Qian-Wen Zhang,Linhao Luo,Xiao Huang,Yunsheng Wu,Di Yin,Xing Sun*

Main category: cs.IR

TL;DR: 提出Youtu - GraphRAG范式，包含种子图模式、社区检测、智能检索器及匿名数据集与任务，实验证明其在多基准测试中表现出色，节省成本且准确率高，适应域转移。


<details>
  <summary>Details</summary>
Motivation: 过往仅孤立改进图构建或图检索，在领域转移时性能欠佳，需统一范式提升GraphRAG性能。

Method: 引入种子图模式；开发双重感知社区检测；设计智能检索器；提出匿名数据集与“匿名反转”任务。

Result: 在六个具有挑战性的基准测试中表现稳健，相比现有基线节省达90.71%的令牌成本，准确率提高16.62%。

Conclusion: Youtu - GraphRAG具有良好适应性，可在对模式进行最少干预的情况下实现无缝域转移。

Abstract: Graph retrieval-augmented generation (GraphRAG) has effectively enhanced
large language models in complex reasoning by organizing fragmented knowledge
into explicitly structured graphs. Prior efforts have been made to improve
either graph construction or graph retrieval in isolation, yielding suboptimal
performance, especially when domain shifts occur. In this paper, we propose a
vertically unified agentic paradigm, Youtu-GraphRAG, to jointly connect the
entire framework as an intricate integration. Specifically, (i) a seed graph
schema is introduced to bound the automatic extraction agent with targeted
entity types, relations and attribute types, also continuously expanded for
scalability over unseen domains; (ii) To obtain higher-level knowledge upon the
schema, we develop novel dually-perceived community detection, fusing
structural topology with subgraph semantics for comprehensive knowledge
organization. This naturally yields a hierarchical knowledge tree that supports
both top-down filtering and bottom-up reasoning with community summaries; (iii)
An agentic retriever is designed to interpret the same graph schema to
transform complex queries into tractable and parallel sub-queries. It
iteratively performs reflection for more advanced reasoning; (iv) To alleviate
the knowledge leaking problem in pre-trained LLM, we propose a tailored
anonymous dataset and a novel 'Anonymity Reversion' task that deeply measures
the real performance of the GraphRAG frameworks. Extensive experiments across
six challenging benchmarks demonstrate the robustness of Youtu-GraphRAG,
remarkably moving the Pareto frontier with up to 90.71% saving of token costs
and 16.62% higher accuracy over state-of-the-art baselines. The results
indicate our adaptability, allowing seamless domain transfer with minimal
intervention on schema.

</details>


### [47] [Refining Text Generation for Realistic Conversational Recommendation via Direct Preference Optimization](https://arxiv.org/abs/2508.19918)
*Manato Tajiri,Michimasa Inaba*

Main category: cs.IR

TL;DR: 本文利用大语言模型和DPO方法优化对话推荐系统，在公开数据集实验验证其能促进更自然真实的对话推荐过程，代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前对话推荐系统在短暂会话中快速推荐物品，偏离真实人类交互，需改进。

Method: 利用大语言模型从对话历史生成对话摘要、从物品描述生成物品推荐信息，使用直接偏好优化（DPO）方法。

Result: 在两个公开数据集上的实验验证了方法的有效性。

Conclusion: 所提方法能促进更自然和现实的对话推荐过程。

Abstract: Conversational Recommender Systems (CRSs) aim to elicit user preferences via
natural dialogue to provide suitable item recommendations. However, current
CRSs often deviate from realistic human interactions by rapidly recommending
items in brief sessions. This work addresses this gap by leveraging Large
Language Models (LLMs) to generate dialogue summaries from dialogue history and
item recommendation information from item description. This approach enables
the extraction of both explicit user statements and implicit preferences
inferred from the dialogue context. We introduce a method using Direct
Preference Optimization (DPO) to ensure dialogue summary and item
recommendation information are rich in information crucial for effective
recommendations. Experiments on two public datasets validate our method's
effectiveness in fostering more natural and realistic conversational
recommendation processes.Our implementation is publicly available
at:https://github.com/UEC-InabaLab/Refining-LLM-Text

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats](https://arxiv.org/abs/2508.19263)
*Anat Heilper,Doron Singer*

Main category: cs.LG

TL;DR: 本文将ZipNN方法扩展到FP8和FP4等低精度浮点格式，设计独立压缩指数和尾数的方法，评估显示有高压缩率，还研究了大语言模型K/V缓存张量的可压缩性。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型增长和部署广泛，降低神经网络权重的存储和传输成本愈发重要，此前无损压缩方法多应用于高精度格式，本文将其扩展到低精度格式。

Method: 设计一种使用熵编码独立分离和压缩指数与尾数组件的压缩方法。

Result: BF16压缩率达62%，FP8达83%，发现大语言模型K/V缓存张量有可压缩模式。

Conclusion: 将ZipNN扩展到低精度格式可行，能有效降低存储成本，K/V缓存张量也可压缩以节省部署内存。

Abstract: As deep learning models grow and deployment becomes more widespread, reducing
the storage and transmission costs of neural network weights has become
increasingly important. While prior work such as ZipNN has shown that lossless
compression methods - particularly those based on Huffman encoding
floating-point exponents can significantly reduce model sizes, these techniques
have primarily been applied to higher-precision formats such as FP32 and BF16.
In this work, we extend the ZipNN approach to lower-precision floating-point
formats, specifically FP8 and FP4, which are gaining popularity for efficient
inference. We design a compression method that separates and compresses the
exponent and mantissa components independently using entropy coding. Our
evaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also
investigate the compressibility of key-value (K/V) cache tensors used in large
language models (LLMs), finding that they, too, exhibit compressible patterns,
enabling memory savings during deployment.

</details>


### [49] [Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models](https://arxiv.org/abs/2508.19249)
*Jonas Søeborg Nielsen,Marcus Galea Jacobsen,Albert Brincker Olson,Mads Peter Sørensen,Allan Peter Engsig-Karup*

Main category: cs.LG

TL;DR: 提出物理信息回归（PIR）方法进行参数估计，与PINN对比，PIR表现更优，可用于时变参数估计，支持快速可靠的参数估计。


<details>
  <summary>Details</summary>
Motivation: 寻找高效的非线性动态模型参数估计方法，桥接理论与数据。

Method: 提出PIR方法，基于正则化普通最小二乘法，对不同复杂度的流行病模型进行参数估计，并与PINN对比。

Result: PIR和PINN都能估计目标参数，PIR表现更好，尤其在复杂模型上，计算速度有差异。

Conclusion: 对于所考虑的模型，PIR方法优于PINN，数据驱动和物理信息技术可支持参数线性非线性动态模型的可靠快速参数估计。

Abstract: We present a new efficient hybrid parameter estimation method based on the
idea, that if nonlinear dynamic models are stated in terms of a system of
equations that is linear in terms of the parameters, then regularized ordinary
least squares can be used to estimate these parameters from time series data.
We introduce the term "Physics-Informed Regression" (PIR) to describe the
proposed data-driven hybrid technique as a way to bridge theory and data by use
of ordinary least squares to efficiently perform parameter estimation of the
model coefficients of different parameter-linear models; providing examples of
models based on nonlinear ordinary equations (ODE) and partial differential
equations (PDE). The focus is on parameter estimation on a selection of ODE and
PDE models, each illustrating performance in different model characteristics.
For two relevant epidemic models of different complexity and number of
parameters, PIR is tested and compared against the related technique,
physics-informed neural networks (PINN), both on synthetic data generated from
known target parameters and on real public Danish time series data collected
during the COVID-19 pandemic in Denmark. Both methods were able to estimate the
target parameters, while PIR showed to perform noticeably better, especially on
a compartment model with higher complexity. Given the difference in
computational speed, it is concluded that the PIR method is superior to PINN
for the models considered. It is also demonstrated how PIR can be applied to
estimate the time-varying parameters of a compartment model that is fitted
using real Danish data from the COVID-19 pandemic obtained during a period from
2020 to 2021. The study shows how data-driven and physics-informed techniques
may support reliable and fast -- possibly real-time -- parameter estimation in
parameter-linear nonlinear dynamic models.

</details>


### [50] [Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models](https://arxiv.org/abs/2508.19441)
*Sanket Jantre,Deepak Akhare,Xiaoning Qian,Nathan M. Urban*

Main category: cs.LG

TL;DR: 提出从计算机模型生成神经PDE训练数据的高效数据增强策略，能去除轨迹数据冗余并提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统神经PDE训练数据依赖PDE求解器长时间积分的解轨迹，存在时空冗余，需更高效策略。

Method: 采用空间填充采样局部“模板”状态的方法生成训练数据，去除冗余并对罕见状态过采样。

Result: 从相当于10个时间步数值模拟的合成训练数据中可学习到准确的神经PDE模板算子，有完整轨迹模拟时精度更高。

Conclusion: 数据增强的合成模板数据训练的神经模板算子性能优于简单采样的轨迹数据。

Abstract: Partial differential equations (PDEs) underpin the modeling of many natural
and engineered systems. It can be convenient to express such models as neural
PDEs rather than using traditional numerical PDE solvers by replacing part or
all of the PDE's governing equations with a neural network representation.
Neural PDEs are often easier to differentiate, linearize, reduce, or use for
uncertainty quantification than the original numerical solver. They are usually
trained on solution trajectories obtained by long time integration of the PDE
solver. Here we propose a more sample-efficient data-augmentation strategy for
generating neural PDE training data from a computer model by space-filling
sampling of local "stencil" states. This approach removes a large degree of
spatiotemporal redundancy present in trajectory data and oversamples states
that may be rarely visited but help the neural PDE generalize across the state
space. We demonstrate that accurate neural PDE stencil operators can be learned
from synthetic training data generated by the computational equivalent of 10
timesteps' worth of numerical simulation. Accuracy is further improved if we
assume access to a single full-trajectory simulation from the computer model,
which is typically available in practice. Across several PDE systems, we show
that our data-augmented synthetic stencil data yield better trained neural
stencil operators, with clear performance gains compared with naively sampled
stencil data from simulation trajectories.

</details>


### [51] [On Surjectivity of Neural Networks: Can you elicit any behavior from your model?](https://arxiv.org/abs/2508.19445)
*Haozhe Jiang,Nika Haghtalab*

Main category: cs.LG

TL;DR: 证明现代神经网络架构基本构建块几乎总是满射，揭示常用生成框架有逆映射及架构易受攻击。


<details>
  <summary>Details</summary>
Motivation: 探讨训练好的神经网络能否通过输入产生指定输出，满射特性引发模型安全和越狱漏洞担忧。

Method: 证明现代神经网络架构的许多基本构建块几乎总是满射。

Result: 广泛使用的生成框架（如GPT式变压器和带确定性ODE求解器的扩散模型）对任意输出有逆映射。

Conclusion: 研究架构满射性，揭示其易受广泛对抗攻击的不可避免的脆弱性。

Abstract: Given a trained neural network, can any specified output be generated by some
input? Equivalently, does the network correspond to a function that is
surjective? In generative models, surjectivity implies that any output,
including harmful or undesirable content, can in principle be generated by the
networks, raising concerns about model safety and jailbreak vulnerabilities. In
this paper, we prove that many fundamental building blocks of modern neural
architectures, such as networks with pre-layer normalization and
linear-attention modules, are almost always surjective. As corollaries, widely
used generative frameworks, including GPT-style transformers and diffusion
models with deterministic ODE solvers, admit inverse mappings for arbitrary
outputs. By studying surjectivity of these modern and commonly used neural
architectures, we contribute a formalism that sheds light on their unavoidable
vulnerability to a broad class of adversarial attacks.

</details>


### [52] [The Sample Complexity of Membership Inference and Privacy Auditing](https://arxiv.org/abs/2508.19458)
*Mahdi Haghifam,Adam Smith,Jonathan Ullman*

Main category: cs.LG

TL;DR: 研究高斯均值估计场景下成员推断攻击所需参考样本的复杂度，发现攻击者有时需比训练算法更多样本，现有攻击可能低估成员推断可能性。


<details>
  <summary>Details</summary>
Motivation: 探究攻击者进行成员推断攻击所需的信息量，即成功攻击所需的最小参考样本数。

Method: 在高斯均值估计的基础设置下，研究学习算法从d维高斯分布中获取n个样本并估计均值时，成员推断攻击的样本复杂度。

Result: 在该设置下，进行能与全知攻击者竞争的攻击可能需要Ω(n + n^2 ρ^2)个样本，攻击者有时需比训练算法使用更多样本。

Conclusion: 现有实际应用中的攻击使用O(n)个样本，可能低估成员推断可能性，当分布信息易获取时可能有更好的攻击方法。

Abstract: A membership-inference attack gets the output of a learning algorithm, and a
target individual, and tries to determine whether this individual is a member
of the training data or an independent sample from the same distribution. A
successful membership-inference attack typically requires the attacker to have
some knowledge about the distribution that the training data was sampled from,
and this knowledge is often captured through a set of independent reference
samples from that distribution. In this work we study how much information the
attacker needs for membership inference by investigating the sample
complexity-the minimum number of reference samples required-for a successful
attack. We study this question in the fundamental setting of Gaussian mean
estimation where the learning algorithm is given $n$ samples from a Gaussian
distribution $\mathcal{N}(\mu,\Sigma)$ in $d$ dimensions, and tries to estimate
$\hat\mu$ up to some error $\mathbb{E}[\|\hat \mu - \mu\|^2_{\Sigma}]\leq
\rho^2 d$. Our result shows that for membership inference in this setting,
$\Omega(n + n^2 \rho^2)$ samples can be necessary to carry out any attack that
competes with a fully informed attacker. Our result is the first to show that
the attacker sometimes needs many more samples than the training algorithm uses
to train the model. This result has significant implications for practice, as
all attacks used in practice have a restricted form that uses $O(n)$ samples
and cannot benefit from $\omega(n)$ samples. Thus, these attacks may be
underestimating the possibility of membership inference, and better attacks may
be possible when information about the distribution is easy to obtain.

</details>


### [53] [Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting](https://arxiv.org/abs/2508.19563)
*Hejia Liu,Mochen Yang,Gediminas Adomavicius*

Main category: cs.LG

TL;DR: 本文指出大语言模型（LLMs）用于数据拟合时对任务无关的变化敏感，缺乏基本鲁棒性，即使专门的模型也存在此问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs用于数据拟合时的潜在问题，评估其作为数据拟合工具的可靠性。

Method: 分析LLMs在上下文学习和监督微调下对任务无关变化的预测敏感性，检查开放权重LLM的注意力分数，对比专门的表格基础模型TabPFN。

Result: 发现LLMs对任务无关变化高度敏感，如改变变量名会大幅影响预测结果，且存在非均匀注意力模式，TabPFN也不能完全免疫。

Conclusion: 尽管LLMs有强大的预测能力，但目前缺乏作为数据拟合工具的基本鲁棒性。

Abstract: Large Language Models (LLMs) are being applied in a wide array of settings,
well beyond the typical language-oriented use cases. In particular, LLMs are
increasingly used as a plug-and-play method for fitting data and generating
predictions. Prior work has shown that LLMs, via in-context learning or
supervised fine-tuning, can perform competitively with many tabular supervised
learning techniques in terms of predictive performance. However, we identify a
critical vulnerability of using LLMs for data fitting -- making changes to data
representation that are completely irrelevant to the underlying learning task
can drastically alter LLMs' predictions on the same data. For example, simply
changing variable names can sway the size of prediction error by as much as 82%
in certain settings. Such prediction sensitivity with respect to
task-irrelevant variations manifests under both in-context learning and
supervised fine-tuning, for both close-weight and open-weight general-purpose
LLMs. Moreover, by examining the attention scores of an open-weight LLM, we
discover a non-uniform attention pattern: training examples and variable
names/values which happen to occupy certain positions in the prompt receive
more attention when output tokens are generated, even though different
positions are expected to receive roughly the same attention. This partially
explains the sensitivity in the presence of task-irrelevant variations. We also
consider a state-of-the-art tabular foundation model (TabPFN) trained
specifically for data fitting. Despite being explicitly designed to achieve
prediction robustness, TabPFN is still not immune to task-irrelevant
variations. Overall, despite LLMs' impressive predictive capabilities,
currently they lack even the basic level of robustness to be used as a
principled data-fitting tool.

</details>


### [54] [Interestingness First Classifiers](https://arxiv.org/abs/2508.19780)
*Ryoma Sato*

Main category: cs.LG

TL;DR: 本文探索构建有趣的分类器，提出EUREKA框架，能选出非明显但有预测性的特征，支持新知识发现与交流。


<details>
  <summary>Details</summary>
Motivation: 多数机器学习模型旨在最大化预测准确性，本文探索构建使用不寻常特征的有趣分类器这一不同目标。

Method: 引入EUREKA框架，利用大语言模型按特征有趣度排序，仅使用选定的有趣特征构建可解释分类器。

Result: 在多个基准数据集上，EUREKA能持续识别出非明显但有预测性的特征，如在Occupancy Detection数据集中青睐湿度，在Twin Papers数据集中发现标题含冒号的论文更易被引用。

Conclusion: 此类模型能支持新知识发现和交流，尤其在对准确性要求适中但看重新颖性和可解释性的场景。

Abstract: Most machine learning models are designed to maximize predictive accuracy. In
this work, we explore a different goal: building classifiers that are
interesting. An ``interesting classifier'' is one that uses unusual or
unexpected features, even if its accuracy is lower than the best possible
model. For example, predicting room congestion from CO2 levels achieves
near-perfect accuracy but is unsurprising. In contrast, predicting room
congestion from humidity is less accurate yet more nuanced and intriguing. We
introduce EUREKA, a simple framework that selects features according to their
perceived interestingness. Our method leverages large language models to rank
features by their interestingness and then builds interpretable classifiers
using only the selected interesting features. Across several benchmark
datasets, EUREKA consistently identifies features that are non-obvious yet
still predictive. For example, in the Occupancy Detection dataset, our method
favors humidity over CO2 levels and light intensity, producing classifiers that
achieve meaningful accuracy while offering insights. In the Twin Papers
dataset, our method discovers the rule that papers with a colon in the title
are more likely to be cited in the future. We argue that such models can
support new ways of knowledge discovery and communication, especially in
settings where moderate accuracy is sufficient but novelty and interpretability
are valued.

</details>


### [55] [Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach](https://arxiv.org/abs/2508.20013)
*Lotte Gross,Rebecca Walter,Nicole Zoppi,Adrien Justus,Alessandro Gambetti,Qiwei Han,Maximilian Kaiser*

Main category: cs.LG

TL;DR: 本文开发并部署多模态分层分类框架解决电商产品分类问题，用多种融合策略，实验表明CLRIP嵌入结合MLP晚期融合策略效果最佳，还引入自监督管道发现细粒度类别，探讨不同融合方法在跨平台的表现并展示框架工业可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决电商产品分类中的平台异质性和现有分类法结构限制等关键工业挑战。

Method: 开发多模态分层分类框架，整合文本、视觉和视觉 - 语言联合特征，研究不同融合策略，引入自监督“产品重新分类”管道，采用两阶段推理管道部署。

Result: CLIP嵌入结合MLP晚期融合策略实现最高分层F1值98.59%，自监督管道发现细粒度类别且簇纯度超86%，复杂晚期融合方法在多样训练数据下精度高，简单早期融合方法对未见平台泛化性好。

Conclusion: 所开发的多模态分层分类框架有效解决电商产品分类问题，且具有工业可扩展性。

Abstract: This study addresses critical industrial challenges in e-commerce product
categorization, namely platform heterogeneity and the structural limitations of
existing taxonomies, by developing and deploying a multimodal hierarchical
classification framework. Using a dataset of 271,700 products from 40
international fashion e-commerce platforms, we integrate textual features
(RoBERTa), visual features (ViT), and joint vision--language representations
(CLIP). We investigate fusion strategies, including early, late, and
attention-based fusion within a hierarchical architecture enhanced by dynamic
masking to ensure taxonomic consistency. Results show that CLIP embeddings
combined via an MLP-based late-fusion strategy achieve the highest hierarchical
F1 (98.59\%), outperforming unimodal baselines. To address shallow or
inconsistent categories, we further introduce a self-supervised ``product
recategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which
discovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with
cluster purities above 86\%. Cross-platform experiments reveal a
deployment-relevant trade-off: complex late-fusion methods maximize accuracy
with diverse training data, while simpler early-fusion methods generalize more
effectively to unseen platforms. Finally, we demonstrate the framework's
industrial scalability through deployment in EURWEB's commercial transaction
intelligence platform via a two-stage inference pipeline, combining a
lightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance
cost and accuracy.

</details>


### [56] [POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization](https://arxiv.org/abs/2508.19277)
*Xinyu Li,Tianjin Huang,Ronghui Mu,Xiaowei Huang,Gaojie Jin*

Main category: cs.LG

TL;DR: 本文提出POT框架应对思维链提示增强大语言模型推理能力时带来的新攻击面问题，实验表明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 思维链提示虽增强大语言模型推理能力，但带来新攻击面问题，且之前的过度思考攻击有诸多限制，缺乏实际应用能力。

Method: 提出POT（仅提示过度思考）黑盒攻击框架，采用基于大语言模型的迭代优化生成隐蔽且语义自然的对抗性提示。

Result: 在不同模型架构和数据集上的广泛实验表明，POT比其他方法性能更优。

Conclusion: POT框架能有效解决现有问题，在应对思维链提示攻击方面表现出色。

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially
enhanced the reasoning capabilities of large language models (LLMs), enabling
sophisticated problem-solving through explicit multi-step reasoning traces.
However, these enhanced reasoning processes introduce novel attack surfaces,
particularly vulnerabilities to computational inefficiency through
unnecessarily verbose reasoning chains that consume excessive resources without
corresponding performance gains. Prior overthinking attacks typically require
restrictive conditions including access to external knowledge sources for data
poisoning, reliance on retrievable poisoned content, and structurally obvious
templates that limit practical applicability in real-world scenarios. To
address these limitations, we propose POT (Prompt-Only OverThinking), a novel
black-box attack framework that employs LLM-based iterative optimization to
generate covert and semantically natural adversarial prompts, eliminating
dependence on external data access and model retrieval. Extensive experiments
across diverse model architectures and datasets demonstrate that POT achieves
superior performance compared to other methods.

</details>


### [57] [(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems](https://arxiv.org/abs/2508.19318)
*Aohan Li,Miyu Tsuzuki*

Main category: cs.LG

TL;DR: 本文提出在现实分布式物联网环境中训练深度强化学习（DRL）模型的框架，经评估证明其可行有效。


<details>
  <summary>Details</summary>
Motivation: 现有研究在实际分布式物联网系统中用真实数据训练DRL模型较少，需填补该空白。

Method: 提出框架，让物联网设备用基于DRL的方法选择通信信道，利用反馈信息训练DRL模型，从所选信道的实际数据传输中获取确认（ACK）信息。

Result: 通过对帧成功率（FSR）的实现和性能评估，证明了框架的可行性和有效性。

Conclusion: 所提出的在现实分布式物联网环境中训练DRL模型的框架是可行且有效的。

Abstract: Deep Reinforcement Learning (DRL) has emerged as an efficient approach to
resource allocation due to its strong capability in handling complex
decision-making tasks. However, only limited research has explored the training
of DRL models with real-world data in practical, distributed Internet of Things
(IoT) systems. To bridge this gap, this paper proposes a novel framework for
training DRL models in real-world distributed IoT environments. In the proposed
framework, IoT devices select communication channels using a DRL-based method,
while the DRL model is trained with feedback information. Specifically,
Acknowledgment (ACK) information is obtained from actual data transmissions
over the selected channels. Implementation and performance evaluation, in terms
of Frame Success Rate (FSR), are carried out, demonstrating both the
feasibility and the effectiveness of the proposed framework.

</details>


### [58] [Re:Frame -- Retrieving Experience From Associative Memory](https://arxiv.org/abs/2508.19344)
*Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 文章提出Re:Frame模块，利用少量专家经验提升离线强化学习性能，在D4RL MuJoCo任务中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习常处理次优数据，难以泛化和取得高性能，需解决如何利用稀缺专家演示和大量低质量数据的问题。

Method: 引入Re:Frame模块，在标准离线强化学习策略中加入外部关联记忆缓冲区（AMB），训练时从AMB检索专家数据并融入决策，评估时也查询AMB。

Result: 在D4RL MuJoCo任务中，使用仅60条专家轨迹，Re:Frame在四个设置中的三个上持续优于强大的决策变压器基线，增益高达+10.7个归一化点。

Conclusion: Re:Frame提供了一种简单且数据高效的方法，可注入稀缺专家知识，显著提升低质量数据集的离线强化学习性能。

Abstract: Offline reinforcement learning (RL) often deals with suboptimal data when
collecting large expert datasets is unavailable or impractical. This limitation
makes it difficult for agents to generalize and achieve high performance, as
they must learn primarily from imperfect or inconsistent trajectories. A
central challenge is therefore how to best leverage scarce expert
demonstrations alongside abundant but lower-quality data. We demonstrate that
incorporating even a tiny amount of expert experience can substantially improve
RL agent performance. We introduce Re:Frame (Retrieving Experience From
Associative Memory), a plug-in module that augments a standard offline RL
policy (e.g., Decision Transformer) with a small external Associative Memory
Buffer (AMB) populated by expert trajectories drawn from a separate dataset.
During training on low-quality data, the policy learns to retrieve expert data
from the Associative Memory Buffer (AMB) via content-based associations and
integrate them into decision-making; the same AMB is queried at evaluation.
This requires no environment interaction and no modifications to the backbone
architecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories
(0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a
strong Decision Transformer baseline in three of four settings, with gains up
to +10.7 normalized points. These results show that Re:Frame offers a simple
and data-efficient way to inject scarce expert knowledge and substantially
improve offline RL from low-quality datasets.

</details>


### [59] [Memorization in Graph Neural Networks](https://arxiv.org/abs/2508.19352)
*Adarsh Jamadandi,Jing Xu,Adam Dziedzic,Franziska Boenisch*

Main category: cs.LG

TL;DR: 本文提出NCMemo框架量化半监督节点分类中标签记忆，研究图同质性与记忆的关系，分析GNN训练动态，发现低同质性图增加记忆，还提出图重连缓解记忆，降低隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现有对图神经网络（GNNs）记忆训练数据的分析较少，旨在研究GNNs在半监督节点分类中的标签记忆情况。

Method: 引入NCMemo框架，建立图同质性与记忆的关系，分析GNN训练动态，研究节点特征空间邻域标签不一致性与记忆的关系，采用图重连方法缓解记忆。

Result: 低同质性显著增加记忆，图重连能有效减少记忆，不影响模型性能，降低隐私风险。

Conclusion: 本研究增进对GNN学习的理解，支持更具隐私保护的GNN部署。

Abstract: Deep neural networks (DNNs) have been shown to memorize their training data,
yet similar analyses for graph neural networks (GNNs) remain largely
under-explored. We introduce NCMemo (Node Classification Memorization), the
first framework to quantify label memorization in semi-supervised node
classification. We first establish an inverse relationship between memorization
and graph homophily, i.e., the property that connected nodes share similar
labels/features. We find that lower homophily significantly increases
memorization, indicating that GNNs rely on memorization to learn less
homophilic graphs. Secondly, we analyze GNN training dynamics. We find that the
increased memorization in low homophily graphs is tightly coupled to the GNNs'
implicit bias on using graph structure during learning. In low homophily
regimes, this structure is less informative, hence inducing memorization of the
node labels to minimize training loss. Finally, we show that nodes with higher
label inconsistency in their feature-space neighborhood are significantly more
prone to memorization. Building on our insights into the link between graph
homophily and memorization, we investigate graph rewiring as a means to
mitigate memorization. Our results demonstrate that this approach effectively
reduces memorization without compromising model performance. Moreover, we show
that it lowers the privacy risk for previously memorized data points in
practice. Thus, our work not only advances understanding of GNN learning but
also supports more privacy-preserving GNN deployment.

</details>


### [60] [Efficient Multi-Source Knowledge Transfer by Model Merging](https://arxiv.org/abs/2508.19353)
*Marcin Osial,Bartosz Wójcik,Bartosz Zieliński,Sebastian Cygert*

Main category: cs.LG

TL;DR: 本文指出多源迁移学习现有方法不足，提出用SVD分解源模型、聚合关键组件、微调主奇异值的框架，实现高效迁移学习。


<details>
  <summary>Details</summary>
Motivation: 现有迁移学习忽略线上多模型知识，多源迁移学习现有方法粗粒度，缺乏精确性和聚合效率。

Method: 用奇异值分解（SVD）将源模型分解为基本的一阶组件，聚合阶段选择最显著组件，微调合并矩阵的主奇异值。

Result: 提出的框架实现高效迁移学习，对输入和参数空间扰动有鲁棒性，计算可扩展性好。

Conclusion: 该框架解决了多源迁移学习现有方法的局限性，能有效进行迁移学习。

Abstract: While transfer learning is an advantageous strategy, it overlooks the
opportunity to leverage knowledge from numerous available models online.
Addressing this multi-source transfer learning problem is a promising path to
boost adaptability and cut re-training costs. However, existing approaches are
inherently coarse-grained, lacking the necessary precision for granular
knowledge extraction and the aggregation efficiency required to fuse knowledge
from either a large number of source models or those with high parameter
counts. We address these limitations by leveraging Singular Value Decomposition
(SVD) to first decompose each source model into its elementary, rank-one
components. A subsequent aggregation stage then selects only the most salient
components from all sources, thereby overcoming the previous efficiency and
precision limitations. To best preserve and leverage the synthesized knowledge
base, our method adapts to the target task by fine-tuning only the principal
singular values of the merged matrix. In essence, this process only
recalibrates the importance of top SVD components. The proposed framework
allows for efficient transfer learning, is robust to perturbations both at the
input level and in the parameter space (e.g., noisy or pruned sources), and
scales well computationally.

</details>


### [61] [Graph Data Modeling: Molecules, Proteins, & Chemical Processes](https://arxiv.org/abs/2508.19356)
*José Manuel Barraza-Chavez,Rana A. Barghout,Ricardo Almada-Monter,Benjamin Sanchez-Lengeling,Adrian Jinich,Radhakrishnan Mahadevan*

Main category: cs.LG

TL;DR: 介绍化学领域图数据建模，包括基础、任务、示例及机器学习作用，助读者应用图方法进行化学发现。


<details>
  <summary>Details</summary>
Motivation: 图在化学科学中至关重要，需介绍图作为数学对象及学习算法在化学中的应用。

Method: 阐述图设计基础、关键预测任务、代表性示例及机器学习在基于图的建模中的作用。

Result: 为读者提供化学领域图数据建模的相关知识。

Conclusion: 这些概念能让读者将图方法应用于下一代化学发现。

Abstract: Graphs are central to the chemical sciences, providing a natural language to
describe molecules, proteins, reactions, and industrial processes. They capture
interactions and structures that underpin materials, biology, and medicine.
This primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes,
introduces graphs as mathematical objects in chemistry and shows how learning
algorithms (particularly graph neural networks) can operate on them. We outline
the foundations of graph design, key prediction tasks, representative examples
across chemical sciences, and the role of machine learning in graph-based
modeling. Together, these concepts prepare readers to apply graph methods to
the next generation of chemical discovery.

</details>


### [62] [Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture](https://arxiv.org/abs/2508.19361)
*Yongbin Lee,Ki H. Chon*

Main category: cs.LG

TL;DR: 本文提出轻量级深度学习模型仅用RR间隔实现房颤早期预测，表现良好且计算高效。


<details>
  <summary>Details</summary>
Motivation: 现有算法难检测阵发性房颤，早期预测房颤可通过预防性疗法降低疾病进展风险。

Method: 提出结合TCN和Mamba的轻量级深度学习模型，仅用RR间隔进行高效并行序列建模。

Result: 模型在受试者测试中多项指标表现良好，计算高效，参数少，能提前两小时预测房颤。

Conclusion: 该模型在房颤早期预测上有优势，在准确性和模型紧凑性上优于传统方法，可为预防干预提供时间。

Abstract: Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk
of stroke, heart failure, and other cardiovascular complications. While AF
detection algorithms perform well in identifying persistent AF, early-stage
progression, such as paroxysmal AF (PAF), often goes undetected due to its
sudden onset and short duration. However, undetected PAF can progress into
sustained AF, increasing the risk of mortality and severe complications. Early
prediction of AF offers an opportunity to reduce disease progression through
preventive therapies, such as catecholamine-sparing agents or beta-blockers. In
this study, we propose a lightweight deep learning model using only RR
Intervals (RRIs), combining a Temporal Convolutional Network (TCN) for
positional encoding with Mamba, a selective state space model, to enable early
prediction of AF through efficient parallel sequence modeling. In subject-wise
testing results, our model achieved a sensitivity of 0.908, specificity of
0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our
method demonstrates high computational efficiency, with only 73.5 thousand
parameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural
Network-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and
model compactness. Notably, the model can predict AF up to two hours in advance
using just 30 minutes of input data, providing enough lead time for preventive
interventions.

</details>


### [63] [Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in multimodal LLMs](https://arxiv.org/abs/2508.19366)
*Supratik Sarkar,Swagatam Das*

Main category: cs.LG

TL;DR: 本文引入信息几何框架量化多模态大语言模型幻觉，从定性检测迈向数学量化。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型幻觉评估技术多为启发式，缺乏原理性量化和理论保证，需理解幻觉产生、传播和跨模态交互机制。

Method: 引入扩散动力学中信息几何框架，将多模态大语言模型输出表示为谱嵌入，用流形差距刻画语义失真，利用特征模式分解给出模态感知、理论可解释指标。

Result: 框架能捕捉幻觉随时间和输入提示的演变。

Conclusion: 为量化和界定幻觉建立原理性基础，使幻觉从定性风险变为可分析现象。

Abstract: Hallucinations in large language models (LLMs) remain a fundamental obstacle
to trustworthy AI, particularly in high-stakes multimodal domains such as
medicine, law, and finance. Existing evaluation techniques are largely
heuristic -- anchored in qualitative benchmarking or ad-hoc empirical
mitigation -- providing neither principled quantification nor actionable
theoretical guarantees. This gap leaves a critical blind spot in understanding
how hallucinations arise, propagate, and interact across modalities. We
introduce the first (to our knowledge) rigorous information geometric framework
in diffusion dynamics for quantifying hallucinations in multimodal LLMs
(MLLMs), advancing the field from qualitative detection to mathematically
grounded measurement. Our approach represents MLLM outputs as the spectral
embeddings over multimodal graph Laplacians and characterizes the manifold gaps
of truth vs inconsistencies as the semantic distortion, enabling the tight
Rayleigh--Ritz bounds on the multimodal hallucination energy as a functional of
time-dependent temperature profiles. By leveraging eigenmode decompositions in
Reproducing Kernel Hilbert Space (RKHS) embeddings, our framework delivers
modality-aware, theoretically interpretable metrics that capture the evolution
of hallucinations across time and input prompts through temperature annealing.
This work establishes a principled foundation for quantifying and bounding
hallucinations, transforming them from a qualitative risk to a tractable,
analyzable phenomenon.

</details>


### [64] [Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments](https://arxiv.org/abs/2508.19376)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: 本文探索基于LLaMA 3.2微调的视觉语言模型（VLM）在高能物理实验中对中微子相互作用图像分类的应用，结果显示VLM表现优于CNN且支持多模态推理。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在高能物理实验中超越自然语言的多模态推理潜力，用于中微子相互作用图像分类。

Method: 使用基于LLaMA 3.2微调的VLM对像素化探测器图像进行中微子相互作用分类，并与CNN基线对比，评估分类准确率、精确率、召回率和AUC - ROC等指标。

Result: VLM表现匹配或超越CNN，能实现更丰富推理和更好地整合辅助文本或语义上下文。

Conclusion: VLMs为高能物理中的事件分类提供了有前景的通用骨干，为实验中微子物理的多模态方法铺平道路。

Abstract: Recent progress in large language models (LLMs) has shown strong potential
for multimodal reasoning beyond natural language. In this work, we explore the
use of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for
classifying neutrino interactions from pixelated detector images in high-energy
physics (HEP) experiments. We benchmark its performance against an established
CNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as
classification accuracy, precision, recall, and AUC-ROC. Our results show that
the VLM not only matches or exceeds CNN performance but also enables richer
reasoning and better integration of auxiliary textual or semantic context.
These findings suggest that VLMs offer a promising general-purpose backbone for
event classification in HEP, paving the way for multimodal approaches in
experimental neutrino physics.

</details>


### [65] [Towards Quantum Machine Learning for Malicious Code Analysis](https://arxiv.org/abs/2508.19381)
*Jesus Lopez,Saeefa Rubaiyet Nowmi,Viviana Cadena,Mohammad Saidur Rahman*

Main category: cs.LG

TL;DR: 研究用量子多层感知器（QMLP）和量子卷积神经网络（QCNN）进行恶意软件分类，在多个数据集上评估，QMLP在复杂多分类任务表现更好，QCNN训练效率高但精度低。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算出现，量子机器学习为改进恶意软件检测提供机会，但该领域应用待探索，故研究相关模型用于恶意软件分类。

Method: 研究QMLP和QCNN两种混合量子 - 经典模型，用角度嵌入将恶意软件特征编码为量子态，QMLP通过全量子位测量和数据重新上传捕捉复杂模式，QCNN通过量子卷积和池化层减少活跃量子位以加快训练，并在五个数据集上进行二分类和多分类任务评估。

Result: 二分类准确率高，多分类准确率有一定范围，QMLP在复杂多分类任务中优于QCNN，QCNN训练效率高但精度降低。

Conclusion: QMLP和QCNN可用于恶意软件分类，在不同任务中有不同表现。

Abstract: Classical machine learning (CML) has been extensively studied for malware
classification. With the emergence of quantum computing, quantum machine
learning (QML) presents a paradigm-shifting opportunity to improve malware
detection, though its application in this domain remains largely unexplored. In
this study, we investigate two hybrid quantum-classical models -- a Quantum
Multilayer Perceptron (QMLP) and a Quantum Convolutional Neural Network (QCNN),
for malware classification. Both models utilize angle embedding to encode
malware features into quantum states. QMLP captures complex patterns through
full qubit measurement and data re-uploading, while QCNN achieves faster
training via quantum convolution and pooling layers that reduce active qubits.
We evaluate both models on five widely used malware datasets -- API-Graph,
EMBER-Domain, EMBER-Class, AZ-Domain, and AZ-Class, across binary and
multiclass classification tasks.
  Our results show high accuracy for binary classification -- 95-96% on
API-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. In multiclass
settings, accuracy ranges from 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class,
and 60.7-88.1% on EMBER-Class. Overall, QMLP outperforms QCNN in complex
multiclass tasks, while QCNN offers improved training efficiency at the cost of
reduced accuracy.

</details>


### [66] [DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting](https://arxiv.org/abs/2508.19389)
*Owais Ahmad,Milad Ramezankhani,Anirudh Deodhar*

Main category: cs.LG

TL;DR: 提出DETNO架构解决长期交通预测问题，在混沌交通数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 准确的长期交通预测是智能交通系统的关键挑战，现有神经算子有平滑预测、高频特征重建不足和滚动预测误差积累问题。

Method: 引入统一的Diffusion - Enhanced Transformer Neural Operator (DETNO) 架构，结合带交叉注意力机制的变压器神经算子和基于扩散的细化组件。

Result: 在混沌交通数据集的综合评估中，相比传统和基于变压器的神经算子，在扩展滚动预测中表现更优，保留高频成分，提高长期预测稳定性。

Conclusion: DETNO架构克服了标准神经算子的固有局限性，在长期交通预测中具有良好效果。

Abstract: Accurate long-term traffic forecasting remains a critical challenge in
intelligent transportation systems, particularly when predicting high-frequency
traffic phenomena such as shock waves and congestion boundaries over extended
rollout horizons. Neural operators have recently gained attention as promising
tools for modeling traffic flow. While effective at learning function space
mappings, they inherently produce smooth predictions that fail to reconstruct
high-frequency features such as sharp density gradients which results in rapid
error accumulation during multi-step rollout predictions essential for
real-time traffic management. To address these fundamental limitations, we
introduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO)
architecture. DETNO leverages a transformer neural operator with
cross-attention mechanisms, providing model expressivity and super-resolution,
coupled with a diffusion-based refinement component that iteratively
reconstructs high-frequency traffic details through progressive denoising. This
overcomes the inherent smoothing limitations and rollout instability of
standard neural operators. Through comprehensive evaluation on chaotic traffic
datasets, our method demonstrates superior performance in extended rollout
predictions compared to traditional and transformer-based neural operators,
preserving high-frequency components and improving stability over long
prediction horizons.

</details>


### [67] [Quantum-Classical Hybrid Molecular Autoencoder for Advancing Classical Decoding](https://arxiv.org/abs/2508.19394)
*Afrar Jahin,Yi Pan,Yingfeng Wang,Tianming Liu,Wei Zhang*

Main category: cs.LG

TL;DR: 提出用于SMILES重建的混合量子 - 经典架构，提升量子保真度和经典相似度，超越现有量子基线。


<details>
  <summary>Details</summary>
Motivation: 现有经典方法在生成模型中面临高保真度和有效性挑战，QML与序列任务集成研究不足且易出现保真度下降问题。

Method: 提出结合量子编码和经典序列建模的混合量子 - 经典架构用于SMILES重建。

Result: 实现约84%的量子保真度和60%的经典重建相似度，超越现有量子基线。

Conclusion: 为未来QML应用奠定基础，平衡量子表示和经典序列模型，促进量子感知序列模型研究。

Abstract: Although recent advances in quantum machine learning (QML) offer significant
potential for enhancing generative models, particularly in molecular design, a
large array of classical approaches still face challenges in achieving high
fidelity and validity. In particular, the integration of QML with
sequence-based tasks, such as Simplified Molecular Input Line Entry System
(SMILES) string reconstruction, remains underexplored and usually suffers from
fidelity degradation. In this work, we propose a hybrid quantum-classical
architecture for SMILES reconstruction that integrates quantum encoding with
classical sequence modeling to improve quantum fidelity and classical
similarity. Our approach achieves a quantum fidelity of approximately 84% and a
classical reconstruction similarity of 60%, surpassing existing quantum
baselines. Our work lays a promising foundation for future QML applications,
striking a balance between expressive quantum representations and classical
sequence models and catalyzing broader research on quantum-aware sequence
models for molecular and drug discovery.

</details>


### [68] [Kolmogorov-Arnold Representation for Symplectic Learning: Advancing Hamiltonian Neural Networks](https://arxiv.org/abs/2508.19410)
*Zongyu Wu,Ruichen Xu,Luoyao Chen,Georgios Kementzidis,Siyao Wang,Yuefan Deng*

Main category: cs.LG

TL;DR: 提出基于Kolmogorov - Arnold表示的哈密顿神经网络（KAR - HNN），评估其在四个基准问题上的效果，预见其对高维且参数少的物理过程建模有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于多层感知器（MLPs）的哈密顿神经网络（HNNs）在探索复杂能量景观时对超参数过于敏感。

Method: 提出KAR - HNN，用单变量变换取代MLPs，利用局部函数近似。

Result: 该网络能减少能量漂移，提高长期预测稳定性，保留哈密顿系统的辛形式。

Conclusion: KAR - HNN对高维且参数少的现实物理过程建模准确稳定。

Abstract: We propose a Kolmogorov-Arnold Representation-based Hamiltonian Neural
Network (KAR-HNN) that replaces the Multilayer Perceptrons (MLPs) with
univariate transformations. While Hamiltonian Neural Networks (HNNs) ensure
energy conservation by learning Hamiltonian functions directly from data,
existing implementations, often relying on MLPs, cause hypersensitivity to the
hyperparameters while exploring complex energy landscapes. Our approach
exploits the localized function approximations to better capture high-frequency
and multi-scale dynamics, reducing energy drift and improving long-term
predictive stability. The networks preserve the symplectic form of Hamiltonian
systems, and thus maintain interpretability and physical consistency. After
assessing KAR-HNN on four benchmark problems including spring-mass, simple
pendulum, two- and three-body problem, we foresee its effectiveness for
accurate and stable modeling of realistic physical processes often at high
dimensions and with few known parameters.

</details>


### [69] [Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention](https://arxiv.org/abs/2508.19414)
*Gustavo Sandoval*

Main category: cs.LG

TL;DR: 对Llama - 3.1 - 8B - Instruct格式依赖推理失败进行案例研究，发现注意力头专业化现象、计算阈值等，实现模型修复。


<details>
  <summary>Details</summary>
Motivation: 研究Llama - 3.1 - 8B - Instruct在不同格式下数值比较推理失败的原因。

Method: 系统干预、SAE分析等。

Result: 发现变压器模型中注意力头专业化，找到修复错误所需的偶数头数量，揭示格式表示分离和重纠缠机制，实现用25%注意力头完美修复，确定60%模式替换阈值。

Conclusion: 模型表面的全模块需求隐藏着复杂子结构，对可解释性和效率有影响。

Abstract: We present a mechanistic case study of a format-dependent reasoning failure
in Llama-3.1-8B-Instruct, where the model incorrectly judges "9.11" as larger
than "9.8" in chat or Q&A formats, but answers correctly in simple format.
Through systematic intervention, we discover transformers implement even/odd
attention head specialization: even indexed heads handle numerical comparison,
while odd heads serve incompatible functions. The bug requires exactly 8 even
heads at Layer 10 for perfect repair. Any combination of 8+ even heads
succeeds, while 7 or fewer completely fails, revealing sharp computational
thresholds with perfect redundancy among the 16 even heads. SAE analysis
reveals the mechanism: format representations separate (10% feature overlap at
Layer 7), then re-entangle with different weightings (80% feature overlap at
Layer 10), with specific features showing 1.5x amplification in failing
formats. We achieve perfect repair using only 25% of attention heads and
identify a 60% pattern replacement threshold, demonstrating that apparent
full-module requirements hide sophisticated substructure with implications for
interpretability and efficiency. All of our code is available at
https://github.com/gussand/surgeon.

</details>


### [70] [Differentiable multiphase flow model for physics-informed machine learning in reservoir pressure management](https://arxiv.org/abs/2508.19419)
*Harun Ur Rashid,Aleksandra Pachalieva,Daniel O'Malley*

Main category: cs.LG

TL;DR: 提出结合可微多相流模拟器与CNN的机器学习工作流，能更准确预测注采场景，利用迁移学习减少模拟次数和计算成本。


<details>
  <summary>Details</summary>
Motivation: 地下储层压力控制因地质异质性和多相流动力学具有挑战性，传统高保真物理模拟计算成本高，需多次模拟，因此需新方法解决。

Method: 引入物理信息机器学习工作流，结合DPFEHM框架的可微多相流模拟器和CNN，CNN学习从渗透率场预测流体提取率；先在单相稳态模拟上预训练模型，再在多相场景微调。

Result: 能实现更实际准确的预测，用少于三千次全物理多相流模拟就能实现高精度训练，远少于先前估计的达一千万次。

Conclusion: 该方法利用迁移学习，大幅减少模拟次数和计算成本，在注采场景预测上有优势。

Abstract: Accurate subsurface reservoir pressure control is extremely challenging due
to geological heterogeneity and multiphase fluid-flow dynamics. Predicting
behavior in this setting relies on high-fidelity physics-based simulations that
are computationally expensive. Yet, the uncertain, heterogeneous properties
that control these flows make it necessary to perform many of these expensive
simulations, which is often prohibitive. To address these challenges, we
introduce a physics-informed machine learning workflow that couples a fully
differentiable multiphase flow simulator, which is implemented in the DPFEHM
framework with a convolutional neural network (CNN). The CNN learns to predict
fluid extraction rates from heterogeneous permeability fields to enforce
pressure limits at critical reservoir locations. By incorporating transient
multiphase flow physics into the training process, our method enables more
practical and accurate predictions for realistic injection-extraction scenarios
compare to previous works. To speed up training, we pretrain the model on
single-phase, steady-state simulations and then fine-tune it on full multiphase
scenarios, which dramatically reduces the computational cost. We demonstrate
that high-accuracy training can be achieved with fewer than three thousand
full-physics multiphase flow simulations -- compared to previous estimates
requiring up to ten million. This drastic reduction in the number of
simulations is achieved by leveraging transfer learning from much less
expensive single-phase simulations.

</details>


### [71] [MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification](https://arxiv.org/abs/2508.19424)
*Yifan Dou,Adam Khadre,Ruben C Petreaca,Golrokh Mirzaei*

Main category: cs.LG

TL;DR: 本文提出无监督对比学习框架对43种癌症类型聚类，用双视图编码和多尺度对比学习目标学习癌症类型嵌入，聚类结果有生物学意义。


<details>
  <summary>Details</summary>
Motivation: 理解泛癌突变格局对了解肿瘤发生分子机制至关重要，以往队列水平聚类多依赖经典统计方法。

Method: 引入无监督对比学习框架，为每种癌症类型构建基因和染色体水平两种互补突变特征，用TabNet编码器编码，通过多尺度对比学习目标（NT - Xent损失）优化学习统一的癌症类型嵌入。

Result: 得到的潜在表征产生了具有生物学意义的癌症类型聚类，与已知突变过程和组织起源一致。

Conclusion: 这是对比学习首次应用于队列水平癌症聚类，为基于突变的癌症亚型分类提供了可扩展且可解释的框架。

Abstract: Motivation. Understanding the pan-cancer mutational landscape offers critical
insights into the molecular mechanisms underlying tumorigenesis. While
patient-level machine learning techniques have been widely employed to identify
tumor subtypes, cohort-level clustering, where entire cancer types are grouped
based on shared molecular features, has largely relied on classical statistical
methods.
  Results. In this study, we introduce a novel unsupervised contrastive
learning framework to cluster 43 cancer types based on coding mutation data
derived from the COSMIC database. For each cancer type, we construct two
complementary mutation signatures: a gene-level profile capturing nucleotide
substitution patterns across the most frequently mutated genes, and a
chromosome-level profile representing normalized substitution frequencies
across chromosomes. These dual views are encoded using TabNet encoders and
optimized via a multi-scale contrastive learning objective (NT-Xent loss) to
learn unified cancer-type embeddings. We demonstrate that the resulting latent
representations yield biologically meaningful clusters of cancer types,
aligning with known mutational processes and tissue origins. Our work
represents the first application of contrastive learning to cohort-level cancer
clustering, offering a scalable and interpretable framework for mutation-driven
cancer subtyping.

</details>


### [72] [Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization](https://arxiv.org/abs/2508.19443)
*Paimon Goulart,Shaan Pakala,Evangelos Papalexakis*

Main category: cs.LG

TL;DR: 利用内部张量分解改进生成模型，降低生成复杂模拟数据成本，实验证明生成数据仍有用且该方法有提升效率潜力。


<details>
  <summary>Details</summary>
Motivation: 生成大型复杂模拟数据集耗时耗资源，使用生成模型生成合成数据虽有改善，但仍需进一步降低成本。

Method: 在生成模型中引入内部张量分解，针对多维数据生成较小的张量因子而非完整张量。

Result: 通过实验表明，该方法能显著减少模型输出和总体参数，降低生成复杂模拟数据成本，且生成的数据仍有用。

Conclusion: 张量分解有潜力提升生成模型效率，尤其在生成多维数据时。

Abstract: Producing large complex simulation datasets can often be a time and resource
consuming task. Especially when these experiments are very expensive, it is
becoming more reasonable to generate synthetic data for downstream tasks.
Recently, these methods may include using generative machine learning models
such as Generative Adversarial Networks or diffusion models. As these
generative models improve efficiency in producing useful data, we introduce an
internal tensor decomposition to these generative models to even further reduce
costs. More specifically, for multidimensional data, or tensors, we generate
the smaller tensor factors instead of the full tensor, in order to
significantly reduce the model's output and overall parameters. This reduces
the costs of generating complex simulation data, and our experiments show the
generated data remains useful. As a result, tensor decomposition has the
potential to improve efficiency in generative models, especially when
generating multidimensional data, or tensors.

</details>


### [73] [Incentivized Lipschitz Bandits](https://arxiv.org/abs/2508.19466)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study incentivized exploration in multi-armed bandit (MAB) settings with
infinitely many arms modeled as elements in continuous metric spaces. Unlike
classical bandit models, we consider scenarios where the decision-maker
(principal) incentivizes myopic agents to explore beyond their greedy choices
through compensation, but with the complication of reward drift--biased
feedback arising due to the incentives. We propose novel incentivized
exploration algorithms that discretize the infinite arm space uniformly and
demonstrate that these algorithms simultaneously achieve sublinear cumulative
regret and sublinear total compensation. Specifically, we derive regret and
compensation bounds of $\Tilde{O}(T^{d+1/d+2})$, with $d$ representing the
covering dimension of the metric space. Furthermore, we generalize our results
to contextual bandits, achieving comparable performance guarantees. We validate
our theoretical findings through numerical simulations.

</details>


### [74] [DeepAtlas: a tool for effective manifold learning](https://arxiv.org/abs/2508.19479)
*Serena Hughes,Timothy Hamilton,Tom Kolokotrones,Eric J. Deeds*

Main category: cs.LG

TL;DR: 介绍DeepAtlas算法，它能学习流形结构，发现许多真实数据集不符流形假设，符合时可构建生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有流形学习工具生成全局嵌入，无法评估流形假设是否适用于数据集，需改进。

Method: DeepAtlas算法生成数据局部邻域的低维表示，训练深度神经网络映射局部嵌入和原始数据，用拓扑失真判断数据集是否来自流形及维度。

Result: 应用于测试数据集表明DeepAtlas能成功学习流形结构，许多真实数据集不符流形假设。

Conclusion: 在数据来自流形的情况下，DeepAtlas可构建生成模型，有望将微分几何工具应用于多种数据集。

Abstract: Manifold learning builds on the "manifold hypothesis," which posits that data
in high-dimensional datasets are drawn from lower-dimensional manifolds.
Current tools generate global embeddings of data, rather than the local maps
used to define manifolds mathematically. These tools also cannot assess whether
the manifold hypothesis holds true for a dataset. Here, we describe DeepAtlas,
an algorithm that generates lower-dimensional representations of the data's
local neighborhoods, then trains deep neural networks that map between these
local embeddings and the original data. Topological distortion is used to
determine whether a dataset is drawn from a manifold and, if so, its
dimensionality. Application to test datasets indicates that DeepAtlas can
successfully learn manifold structures. Interestingly, many real datasets,
including single-cell RNA-sequencing, do not conform to the manifold
hypothesis. In cases where data is drawn from a manifold, DeepAtlas builds a
model that can be used generatively and promises to allow the application of
powerful tools from differential geometry to a variety of datasets.

</details>


### [75] [Distribution Shift Aware Neural Tabular Learning](https://arxiv.org/abs/2508.19486)
*Wangyang Ying,Nanxu Gong,Dongjie Wang,Xinyuan Wang,Arun Vignesh Malarkkan,Vivek Gupta,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: 提出SAFT框架解决表格学习中分布偏移问题，实验显示其性能更优。


<details>
  <summary>Details</summary>
Motivation: 表格学习在训练和测试数据分布偏移时效果变差，需解决该问题。

Method: 提出SAFT框架，将表格学习从离散搜索转为连续表示生成范式，集成三种机制确保鲁棒性。

Result: 在多种真实分布偏移情况下，SAFT在鲁棒性、有效性和泛化能力上优于先前表格学习方法。

Conclusion: SAFT框架能有效解决分布偏移表格学习问题，具有更好性能。

Abstract: Tabular learning transforms raw features into optimized spaces for downstream
tasks, but its effectiveness deteriorates under distribution shifts between
training and testing data. We formalize this challenge as the Distribution
Shift Tabular Learning (DSTL) problem and propose a novel Shift-Aware Feature
Transformation (SAFT) framework to address it. SAFT reframes tabular learning
from a discrete search task into a continuous representation-generation
paradigm, enabling differentiable optimization over transformed feature sets.
SAFT integrates three mechanisms to ensure robustness: (i) shift-resistant
representation via embedding decorrelation and sample reweighting, (ii)
flatness-aware generation through suboptimal embedding averaging, and (iii)
normalization-based alignment between training and test distributions.
Extensive experiments show that SAFT consistently outperforms prior tabular
learning methods in terms of robustness, effectiveness, and generalization
ability under diverse real-world distribution shifts.

</details>


### [76] [Data-Efficient Symbolic Regression via Foundation Model Distillation](https://arxiv.org/abs/2508.19487)
*Wangyang Ying,Jinghan Zhang,Haoyue Bai,Nanxu Gong,Xinyuan Wang,Kunpeng Liu,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: 提出EQUATE框架用于低数据场景下符号方程发现，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 基础模型在小领域特定数据集上存在负迁移和泛化性差问题，需数据高效微调框架。

Method: EQUATE结合符号 - 数值对齐与评估器引导的嵌入优化，将离散方程搜索转化为共享嵌入空间中的连续优化任务。

Result: 在三个标准公共基准测试中，EQUATE在准确性和鲁棒性上均优于现有基线，且复杂度低、推理快。

Conclusion: EQUATE是基础模型蒸馏设置下数据高效符号回归的实用且通用解决方案。

Abstract: Discovering interpretable mathematical equations from observed data (a.k.a.
equation discovery or symbolic regression) is a cornerstone of scientific
discovery, enabling transparent modeling of physical, biological, and economic
systems. While foundation models pre-trained on large-scale equation datasets
offer a promising starting point, they often suffer from negative transfer and
poor generalization when applied to small, domain-specific datasets. In this
paper, we introduce EQUATE (Equation Generation via QUality-Aligned Transfer
Embeddings), a data-efficient fine-tuning framework that adapts foundation
models for symbolic equation discovery in low-data regimes via distillation.
EQUATE combines symbolic-numeric alignment with evaluator-guided embedding
optimization, enabling a principled embedding-search-generation paradigm. Our
approach reformulates discrete equation search as a continuous optimization
task in a shared embedding space, guided by data-equation fitness and
simplicity. Experiments across three standard public benchmarks (Feynman,
Strogatz, and black-box datasets) demonstrate that EQUATE consistently
outperforms state-of-the-art baselines in both accuracy and robustness, while
preserving low complexity and fast inference. These results highlight EQUATE as
a practical and generalizable solution for data-efficient symbolic regression
in foundation model distillation settings.

</details>


### [77] [PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense](https://arxiv.org/abs/2508.19488)
*Xavier Cadet,Simona Boboila,Sie Hendrata Dharmawan,Alina Oprea,Peter Chin*

Main category: cs.LG

TL;DR: 引入PoolFlip环境扩展FlipIt游戏，提出Flip - PSRO方法训练防御者代理，实验显示该防御者泛化效果比基线高2倍，且新效用函数可保证控制和性能。


<details>
  <summary>Details</summary>
Motivation: 现有FlipIt框架依赖少量启发式或专门学习技术，存在脆性且无法适应新攻击。

Method: 引入PoolFlip多智能体gym环境，提出Flip - PSRO多智能体强化学习方法，利用基于种群的训练。

Result: Flip - PSRO防御者在泛化到训练中未出现的启发式攻击时比基线有效2倍，新效用函数确保防御者保持高控制水平并优化性能。

Conclusion: Flip - PSRO方法能有效提升防御者对未知、潜在自适应对手的泛化能力。

Abstract: Cyber defense requires automating defensive decision-making under stealthy,
deceptive, and continuously evolving adversarial strategies. The FlipIt game
provides a foundational framework for modeling interactions between a defender
and an advanced adversary that compromises a system without being immediately
detected. In FlipIt, the attacker and defender compete to control a shared
resource by performing a Flip action and paying a cost. However, the existing
FlipIt frameworks rely on a small number of heuristics or specialized learning
techniques, which can lead to brittleness and the inability to adapt to new
attacks. To address these limitations, we introduce PoolFlip, a multi-agent gym
environment that extends the FlipIt game to allow efficient learning for
attackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent
reinforcement learning (MARL) approach that leverages population-based training
to train defender agents equipped to generalize against a range of unknown,
potentially adaptive opponents. Our empirical results suggest that Flip-PSRO
defenders are $2\times$ more effective than baselines to generalize to a
heuristic attack not exposed in training. In addition, our newly designed
ownership-based utility functions ensure that Flip-PSRO defenders maintain a
high level of control while optimizing performance.

</details>


### [78] [Learning Game-Playing Agents with Generative Code Optimization](https://arxiv.org/abs/2508.19506)
*Zhiyi Kuang,Ryan Rong,YuCheng Yuan,Allen Nie*

Main category: cs.LG

TL;DR: 提出用生成式优化方法学习游戏代理，以Python程序表示策略并用大语言模型优化，在Atari游戏上表现佳。


<details>
  <summary>Details</summary>
Motivation: 探索高效、自适应且能进行复杂长视野推理的游戏代理构建方法。

Method: 将决策策略视为自进化代码，以当前观察为输入、游戏内动作为输出，利用执行轨迹和自然语言反馈，借助大语言模型优化以Python程序表示的策略。

Result: 在Atari游戏中，该Python程序达到与深度强化学习基线相当的性能，且训练时间显著减少、环境交互更少。

Conclusion: 基于程序的策略表示法在构建高效、自适应代理方面具有潜力。

Abstract: We present a generative optimization approach for learning game-playing
agents, where policies are represented as Python programs and refined using
large language models (LLMs). Our method treats decision-making policies as
self-evolving code, with current observation as input and an in-game action as
output, enabling agents to self-improve through execution traces and natural
language feedback with minimal human intervention. Applied to Atari games, our
game-playing Python program achieves performance competitive with deep
reinforcement learning (RL) baselines while using significantly less training
time and much fewer environment interactions. This work highlights the promise
of programmatic policy representations for building efficient, adaptable agents
capable of complex, long-horizon reasoning.

</details>


### [79] [MobText-SISA: Efficient Machine Unlearning for Mobility Logs with Spatio-Temporal and Natural-Language Data](https://arxiv.org/abs/2508.19554)
*Haruki Yonekura,Ren Ozeki,Tatsuya Amano,Hamada Rizk,Hirozumi Yamaguchi*

Main category: cs.LG

TL;DR: 提出MobText - SISA框架用于多模态移动性数据隐私合规分析，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代移动平台有大量非结构化数据，GDPR要求按需遗忘个人贡献，但从头重新训练模型不可行。

Method: 将MobText - SISA框架的SISA训练扩展到异构时空数据，嵌入特征、相似性聚类分样本、增量训练、聚合预测，删除请求时仅重新训练受影响分片。

Result: 在十个月真实移动日志实验中，维持基线预测精度，误差和收敛速度上优于随机分片。

Conclusion: MobText - SISA可作为城市规模多模态移动性数据隐私合规分析的实用基础。

Abstract: Modern mobility platforms have stored vast streams of GPS trajectories,
temporal metadata, free-form textual notes, and other unstructured data.
Privacy statutes such as the GDPR require that any individual's contribution be
unlearned on demand, yet retraining deep models from scratch for every request
is untenable. We introduce MobText-SISA, a scalable machine-unlearning
framework that extends Sharded, Isolated, Sliced, and Aggregated (SISA)
training to heterogeneous spatio-temporal data. MobText-SISA first embeds each
trip's numerical and linguistic features into a shared latent space, then
employs similarity-aware clustering to distribute samples across shards so that
future deletions touch only a single constituent model while preserving
inter-shard diversity. Each shard is trained incrementally; at inference time,
constituent predictions are aggregated to yield the output. Deletion requests
trigger retraining solely of the affected shard from its last valid checkpoint,
guaranteeing exact unlearning. Experiments on a ten-month real-world mobility
log demonstrate that MobText-SISA (i) sustains baseline predictive accuracy,
and (ii) consistently outperforms random sharding in both error and convergence
speed. These results establish MobText-SISA as a practical foundation for
privacy-compliant analytics on multimodal mobility data at urban scale.

</details>


### [80] [Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models](https://arxiv.org/abs/2508.19564)
*Yuhang Liu,Tao Li,Zhehao Huang,Zuopeng Yang,Xiaolin Huang*

Main category: cs.LG

TL;DR: 提出双向低秩自适应（Bi - LoRA）方法，结合SAM与LoRA，解决SAM应用于LoRA的局限，实验证明其高效且有效。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练模型在有限数据微调时泛化性差，SAM虽能改善但开销大，直接用于LoRA参数效果受限。

Method: 提出Bi - LoRA，引入辅助LoRA模块模拟SAM的对抗性权重扰动，主模块用标准梯度下降，辅助模块用梯度上升。

Result: 广泛实验表明Bi - LoRA在提升泛化性上高效且有效。

Conclusion: Bi - LoRA能在保持内存高效的同时捕捉更广泛的锐度以实现更平坦的最小值，还消除了SAM的双倍训练成本。

Abstract: Fine-tuning large-scale pre-trained models with limited data presents
significant challenges for generalization. While Sharpness-Aware Minimization
(SAM) has proven effective in improving generalization by seeking flat minima,
its substantial extra memory and computation overhead make it impractical for
large models. Integrating SAM with parameter-efficient fine-tuning methods like
Low-Rank Adaptation (LoRA) is a promising direction. However, we find that
directly applying SAM to LoRA parameters limits the sharpness optimization to a
restricted subspace, hindering its effectiveness. To address this limitation,
we propose Bi-directional Low-Rank Adaptation (Bi-LoRA), which introduces an
auxiliary LoRA module to model SAM's adversarial weight perturbations. It
decouples SAM's weight perturbations from LoRA optimization: the primary LoRA
module adapts to specific tasks via standard gradient descent, while the
auxiliary module captures the sharpness of the loss landscape through gradient
ascent. Such dual-module design enables Bi-LoRA to capture broader sharpness
for achieving flatter minima while remaining memory-efficient. Another
important benefit is that the dual design allows for simultaneous optimization
and perturbation, eliminating SAM's doubled training costs. Extensive
experiments across diverse tasks and architectures demonstrate Bi-LoRA's
efficiency and effectiveness in enhancing generalization.

</details>


### [81] [Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning](https://arxiv.org/abs/2508.19567)
*Sheryl Mathew,N Harshit*

Main category: cs.LG

TL;DR: 提出反事实奖励模型用于公平感知的强化学习与人类反馈，在假新闻检测中表现良好，减少虚假关联和不公平信号。


<details>
  <summary>Details</summary>
Motivation: 奖励模型会放大多模态数据集中潜在偏差，导致策略优化不完善和公平性下降，现有偏差缓解方法在因果混淆下可能失效。

Method: 引入因果推理与多模态表征学习的反事实奖励模型，提出反事实信任分数，在多模态假新闻数据集上评估，注入合成偏差测试鲁棒性。

Result: 系统在假新闻检测中准确率达89.12%，优于基线奖励模型，减少了虚假关联和不公平强化信号。

Conclusion: 该方法是公平感知的强化学习与人类反馈的稳健可解释方法，提供可调偏差减少阈值，增加动态实时决策可靠性。

Abstract: In reinforcement learning with human feedback (RLHF), reward models can
efficiently learn and amplify latent biases within multimodal datasets, which
can lead to imperfect policy optimization through flawed reward signals and
decreased fairness. Bias mitigation studies have often applied passive
constraints, which can fail under causal confounding. Here, we present a
counterfactual reward model that introduces causal inference with multimodal
representation learning to provide an unsupervised, bias-resilient reward
signal. The heart of our contribution is the Counterfactual Trust Score, an
aggregated score consisting of four components: (1) counterfactual shifts that
decompose political framing bias from topical bias; (2) reconstruction
uncertainty during counterfactual perturbations; (3) demonstrable violations of
fairness rules for each protected attribute; and (4) temporal reward shifts
aligned with dynamic trust measures. We evaluated the framework on a multimodal
fake versus true news dataset, which exhibits framing bias, class imbalance,
and distributional drift. Following methodologies similar to unsupervised drift
detection from representation-based distances [1] and temporal robustness
benchmarking in language models [2], we also inject synthetic bias across
sequential batches to test robustness. The resulting system achieved an
accuracy of 89.12% in fake news detection, outperforming the baseline reward
models. More importantly, it reduced spurious correlations and unfair
reinforcement signals. This pipeline outlines a robust and interpretable
approach to fairness-aware RLHF, offering tunable bias reduction thresholds and
increasing reliability in dynamic real-time policy making.

</details>


### [82] [Generative Models for Synthetic Data: Transforming Data Mining in the GenAI Era](https://arxiv.org/abs/2508.19570)
*Dawei Li,Yue Huang,Ming Li,Tianyi Zhou,Xiangliang Zhang,Huan Liu*

Main category: cs.LG

TL;DR: 本教程介绍合成数据生成基础、进展、方法、评估策略和应用，助参会者提升数据挖掘能力。


<details>
  <summary>Details</summary>
Motivation: 生成模型解决数据挖掘中数据稀缺、隐私和标注难题，需介绍合成数据生成相关内容。

Method: 介绍合成数据生成的基础、最新进展、关键方法、实用框架、评估策略和应用。

Result: 参会者可获得利用生成式合成数据增强数据挖掘研究与实践的可行见解。

Conclusion: 本教程能帮助参会者在数据挖掘中更好利用合成数据。

Abstract: Generative models such as Large Language Models, Diffusion Models, and
generative adversarial networks have recently revolutionized the creation of
synthetic data, offering scalable solutions to data scarcity, privacy, and
annotation challenges in data mining. This tutorial introduces the foundations
and latest advances in synthetic data generation, covers key methodologies and
practical frameworks, and discusses evaluation strategies and applications.
Attendees will gain actionable insights into leveraging generative synthetic
data to enhance data mining research and practice. More information can be
found on our website: https://syndata4dm.github.io/.

</details>


### [83] [Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal](https://arxiv.org/abs/2508.19571)
*Yunlong Lin,Chao Lu,Tongshuai Wu,Xiaocong Zhao,Guodong Du,Yanwei Sun,Zirui Li,Jianwei Gong*

Main category: cs.LG

TL;DR: 本文提出用于DNN运动预测的协同记忆排练方法SyReM，解决稳定性 - 可塑性困境，实验证明其能减轻灾难性遗忘并提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有DNN运动预测方法存在灾难性遗忘问题，持续学习研究中强调记忆稳定性会损害学习可塑性，需解决稳定性 - 可塑性困境。

Method: 提出SyReM方法，维护紧凑记忆缓冲区，用不等式约束保证记忆稳定性，设计选择性记忆排练机制增强学习可塑性。

Result: 在11个自然驾驶数据集上实验表明，与非CL和CL基线相比，SyReM显著减轻过去场景的灾难性遗忘，提高新场景预测精度。

Conclusion: SyReM方法能有效解决DNN运动预测中的稳定性 - 可塑性困境，代码公开可获取。

Abstract: Deep neural networks (DNN) have achieved remarkable success in motion
forecasting. However, most DNN-based methods suffer from catastrophic
forgetting and fail to maintain their performance in previously learned
scenarios after adapting to new data. Recent continual learning (CL) studies
aim to mitigate this phenomenon by enhancing memory stability of DNN, i.e., the
ability to retain learned knowledge. Yet, excessive emphasis on the memory
stability often impairs learning plasticity, i.e., the capacity of DNN to
acquire new information effectively. To address such stability-plasticity
dilemma, this study proposes a novel CL method, synergetic memory rehearsal
(SyReM), for DNN-based motion forecasting. SyReM maintains a compact memory
buffer to represent learned knowledge. To ensure memory stability, it employs
an inequality constraint that limits increments in the average loss over the
memory buffer. Synergistically, a selective memory rehearsal mechanism is
designed to enhance learning plasticity by selecting samples from the memory
buffer that are most similar to recently observed data. This selection is based
on an online-measured cosine similarity of loss gradients, ensuring targeted
memory rehearsal. Since replayed samples originate from learned scenarios, this
memory rehearsal mechanism avoids compromising memory stability. We validate
SyReM under an online CL paradigm where training samples from diverse scenarios
arrive as a one-pass stream. Experiments on 11 naturalistic driving datasets
from INTERACTION demonstrate that, compared to non-CL and CL baselines, SyReM
significantly mitigates catastrophic forgetting in past scenarios while
improving forecasting accuracy in new ones. The implementation is publicly
available at https://github.com/BIT-Jack/SyReM.

</details>


### [84] [Delta-Audit: Explaining What Changes When Models Change](https://arxiv.org/abs/2508.19589)
*Arshia Hemmat,Afsaneh Fatemi*

Main category: cs.LG

TL;DR: 本文提出Delta - Attribution框架解释模型版本A和B的变化，评估其质量，审计45种设置，发现不同更新变化特征，该框架可辅助模型更新审计。


<details>
  <summary>Details</summary>
Motivation: 模型更新对性能的影响原因不透明，需要方法解释模型版本间的变化。

Method: 引入Delta - Attribution框架，通过差分特征归因解释版本变化，用Delta - Attribution Quality Suite评估，对45种设置进行审计。

Result: 归纳偏置变化产生大的、行为对齐的差异；“表面”调整排名重叠度高、误差小；深度梯度提升在乳腺癌数据集上重分布最大。

Conclusion: Delta - Attribution是轻量级更新审计方法，能区分良性变化和有行为意义或有风险的依赖转移，补充了准确性评估。

Abstract: Model updates (new hyperparameters, kernels, depths, solvers, or data) change
performance, but the \emph{reason} often remains opaque. We introduce
\textbf{Delta-Attribution} (\mbox{$\Delta$-Attribution}), a model-agnostic
framework that explains \emph{what changed} between versions $A$ and $B$ by
differencing per-feature attributions: $\Delta\phi(x)=\phi_B(x)-\phi_A(x)$. We
evaluate $\Delta\phi$ with a \emph{$\Delta$-Attribution Quality Suite} covering
magnitude/sparsity (L1, Top-$k$, entropy), agreement/shift (rank-overlap@10,
Jensen--Shannon divergence), behavioural alignment (Delta Conservation Error,
DCE; Behaviour--Attribution Coupling, BAC; CO$\Delta$F), and robustness (noise,
baseline sensitivity, grouped occlusion).
  Instantiated via fast occlusion/clamping in standardized space with a
class-anchored margin and baseline averaging, we audit 45 settings: five
classical families (Logistic Regression, SVC, Random Forests, Gradient
Boosting, $k$NN), three datasets (Breast Cancer, Wine, Digits), and three A/B
pairs per family. \textbf{Findings.} Inductive-bias changes yield large,
behaviour-aligned deltas (e.g., SVC poly$\!\rightarrow$rbf on Breast Cancer:
BAC$\approx$0.998, DCE$\approx$6.6; Random Forest feature-rule swap on Digits:
BAC$\approx$0.997, DCE$\approx$7.5), while ``cosmetic'' tweaks (SVC
\texttt{gamma=scale} vs.\ \texttt{auto}, $k$NN search) show
rank-overlap@10$=1.0$ and DCE$\approx$0. The largest redistribution appears for
deeper GB on Breast Cancer (JSD$\approx$0.357). $\Delta$-Attribution offers a
lightweight update audit that complements accuracy by distinguishing benign
changes from behaviourally meaningful or risky reliance shifts.

</details>


### [85] [Complementary Learning System Empowers Online Continual Learning of Vehicle Motion Forecasting in Smart Cities](https://arxiv.org/abs/2508.19597)
*Zirui Li,Yunlong Lin,Guodong Du,Xiaocong Zhao,Cheng Gong,Chen Lv,Chao Lu,Jianwei Gong*

Main category: cs.LG

TL;DR: 介绍Dual - LS范式解决DNN车辆运动预测中的灾难性遗忘问题，效果良好。


<details>
  <summary>Details</summary>
Motivation: DNN进行车辆运动预测时存在灾难性遗忘问题，传统方法有成本高、采样低效等不足，缺乏类人持续学习能力。

Method: 引入受人类大脑互补学习系统启发的Dual - LS范式，通过两个协同的记忆排练重放机制加速经验检索并协调长短期知识表征。

Result: 在多国自然主义数据测试中，Dual - LS最多减轻74.31%的灾难性遗忘，最多降低94.02%的计算资源需求，提高预测稳定性。

Conclusion: Dual - LS让基于DNN的车辆运动预测具备计算高效和类人持续学习适应性，适用于智慧城市。

Abstract: Artificial intelligence underpins most smart city services, yet deep neural
network (DNN) that forecasts vehicle motion still struggle with catastrophic
forgetting, the loss of earlier knowledge when models are updated. Conventional
fixes enlarge the training set or replay past data, but these strategies incur
high data collection costs, sample inefficiently and fail to balance long- and
short-term experience, leaving them short of human-like continual learning.
Here we introduce Dual-LS, a task-free, online continual learning paradigm for
DNN-based motion forecasting that is inspired by the complementary learning
system of the human brain. Dual-LS pairs two synergistic memory rehearsal
replay mechanisms to accelerate experience retrieval while dynamically
coordinating long-term and short-term knowledge representations. Tests on
naturalistic data spanning three countries, over 772,000 vehicles and
cumulative testing mileage of 11,187 km show that Dual-LS mitigates
catastrophic forgetting by up to 74.31\% and reduces computational resource
demand by up to 94.02\%, markedly boosting predictive stability in vehicle
motion forecasting without inflating data requirements. Meanwhile, it endows
DNN-based vehicle motion forecasting with computation efficient and human-like
continual learning adaptability fit for smart cities.

</details>


### [86] [Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning](https://arxiv.org/abs/2508.19598)
*Zhiwei Li,Yong Hu,Wenqing Wang*

Main category: cs.LG

TL;DR: 论文提出RLTR框架解决大语言模型代理训练中规划能力提升的问题，实验表明该框架能提升规划性能和最终响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有端到端多目标优化训练范式存在优化目标分配不平衡和可验证数据稀缺问题，难以提升代理的规划能力。

Method: 提出RLTR框架，解耦训练过程对规划模块进行单目标优化，引入基于工具使用完整性的奖励信号。

Result: RLTR相比端到端基线在规划性能上提升8%-12%，使整体代理系统最终响应质量提高5%-6%。

Conclusion: RLTR框架能有效提升大语言模型代理的规划能力和最终响应质量。

Abstract: The functionality of Large Language Model (LLM) agents is primarily
determined by two capabilities: action planning and answer summarization. The
former, action planning, is the core capability that dictates an agent's
performance. However, prevailing training paradigms employ end-to-end,
multi-objective optimization that jointly trains both capabilities. This
paradigm faces two critical challenges: imbalanced optimization objective
allocation and scarcity of verifiable data, making it difficult to enhance the
agent's planning capability. To address these challenges, we propose
Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that
decouples the training process to enable a focused, single-objective
optimization of the planning module. Crucially, RLTR introduces a reward signal
based on tool-use completeness to directly evaluate the quality of tool
invocation sequences. This method offers a more direct and reliable training
signal than assessing the final response content, thereby obviating the need
for verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12%
improvement in planning performance compared to end-to-end baselines. Moreover,
this enhanced planning capability, in turn, translates to a 5%-6% increase in
the final response quality of the overall agent system.

</details>


### [87] [FinCast: A Foundation Model for Financial Time-Series Forecasting](https://arxiv.org/abs/2508.19609)
*Zhuohang Zhu,Haodong Chen,Qiang Qu,Vera Chung*

Main category: cs.LG

TL;DR: 提出用于金融时间序列预测的基础模型FinCast，无需特定领域微调，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列预测因模式转移挑战大，现有深度学习方法有过拟合和需大量特定领域微调的问题。

Method: 引入专门用于金融时间序列预测的基础模型FinCast，在大规模金融数据集上训练。

Result: FinCast有强大零样本性能，能捕捉多样模式，综合评估显示其优于现有方法。

Conclusion: FinCast具有很强的泛化能力。

Abstract: Financial time-series forecasting is critical for maintaining economic
stability, guiding informed policymaking, and promoting sustainable investment
practices. However, it remains challenging due to various underlying pattern
shifts. These shifts arise primarily from three sources: temporal
non-stationarity (distribution changes over time), multi-domain diversity
(distinct patterns across financial domains such as stocks, commodities, and
futures), and varying temporal resolutions (patterns differing across
per-second, hourly, daily, or weekly indicators). While recent deep learning
methods attempt to address these complexities, they frequently suffer from
overfitting and typically require extensive domain-specific fine-tuning. To
overcome these limitations, we introduce FinCast, the first foundation model
specifically designed for financial time-series forecasting, trained on
large-scale financial datasets. Remarkably, FinCast exhibits robust zero-shot
performance, effectively capturing diverse patterns without domain-specific
fine-tuning. Comprehensive empirical and qualitative evaluations demonstrate
that FinCast surpasses existing state-of-the-art methods, highlighting its
strong generalization capabilities.

</details>


### [88] [ALSA: Anchors in Logit Space for Out-of-Distribution Accuracy Estimation](https://arxiv.org/abs/2508.19613)
*Chenzhi Liu,Mahsa Baktashmotlagh,Yanran Tang,Zi Huang,Ruihong Qiu*

Main category: cs.LG

TL;DR: 提出ALSA框架用于估计模型在未标记数据集上的准确率，在多基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有估计模型在未标记数据集上准确率的方法存在信息损失、计算成本高和适用性受限等问题。

Method: 提出ALSA框架，直接在对数空间操作，采用基于锚点的建模策略，初始化多个可学习锚点并分配影响函数。

Result: 在视觉、语言和图基准测试中，ALSA优于基于softmax和相似度的基线方法。

Conclusion: ALSA在显著分布偏移下具有鲁棒性，有作为可靠模型评估实用工具的潜力。

Abstract: Estimating model accuracy on unseen, unlabeled datasets is crucial for
real-world machine learning applications, especially under distribution shifts
that can degrade performance. Existing methods often rely on predicted class
probabilities (softmax scores) or data similarity metrics. While softmax-based
approaches benefit from representing predictions on the standard simplex,
compressing logits into probabilities leads to information loss. Meanwhile,
similarity-based methods can be computationally expensive and domain-specific,
limiting their broader applicability. In this paper, we introduce ALSA (Anchors
in Logit Space for Accuracy estimation), a novel framework that preserves
richer information by operating directly in the logit space. Building on
theoretical insights and empirical observations, we demonstrate that the
aggregation and distribution of logits exhibit a strong correlation with the
predictive performance of the model. To exploit this property, ALSA employs an
anchor-based modeling strategy: multiple learnable anchors are initialized in
logit space, each assigned an influence function that captures subtle
variations in the logits. This allows ALSA to provide robust and accurate
performance estimates across a wide range of distribution shifts. Extensive
experiments on vision, language, and graph benchmarks demonstrate ALSA's
superiority over both softmax- and similarity-based baselines. Notably, ALSA's
robustness under significant distribution shifts highlights its potential as a
practical tool for reliable model evaluation.

</details>


### [89] [Towards Instance-wise Personalized Federated Learning via Semi-Implicit Bayesian Prompt Tuning](https://arxiv.org/abs/2508.19621)
*Tiandi Ye,Wenyan Liu,Kai Yao,Lichun Li,Shangchao Su,Cen Chen,Xiang Li,Shan Yin,Ming Gao*

Main category: cs.LG

TL;DR: 提出基于视觉提示调优的细粒度实例级个性化联邦学习框架pFedBayesPT，实验表明其在特征和标签异质性设置下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有个性化联邦学习方法假设每个客户端数据遵循单一分布，在实践中常不成立，存在显著的客户端内异质性和性能不佳问题。

Method: 从贝叶斯角度构建实例级提示生成，将提示后验建模为隐式分布以捕获不同视觉语义，并在半隐式变分推理框架下推导变分训练目标。

Result: 在基准数据集上的广泛实验表明，pFedBayesPT在特征和标签异质性设置下始终优于现有个性化联邦学习方法。

Conclusion: pFedBayesPT能有效解决客户端内异质性问题，提升个性化联邦学习性能。

Abstract: Federated learning (FL) is a privacy-preserving machine learning paradigm
that enables collaborative model training across multiple distributed clients
without disclosing their raw data. Personalized federated learning (pFL) has
gained increasing attention for its ability to address data heterogeneity.
However, most existing pFL methods assume that each client's data follows a
single distribution and learn one client-level personalized model for each
client. This assumption often fails in practice, where a single client may
possess data from multiple sources or domains, resulting in significant
intra-client heterogeneity and suboptimal performance. To tackle this
challenge, we propose pFedBayesPT, a fine-grained instance-wise pFL framework
based on visual prompt tuning. Specifically, we formulate instance-wise prompt
generation from a Bayesian perspective and model the prompt posterior as an
implicit distribution to capture diverse visual semantics. We derive a
variational training objective under the semi-implicit variational inference
framework. Extensive experiments on benchmark datasets demonstrate that
pFedBayesPT consistently outperforms existing pFL methods under both feature
and label heterogeneity settings.

</details>


### [90] [SCAR: A Characterization Scheme for Multi-Modal Dataset](https://arxiv.org/abs/2508.19659)
*Ri Su,Zhao Chen,Caleb Chen Cao,Nan Tang,Lei Chen*

Main category: cs.LG

TL;DR: 本文提出SCAR方案刻画数据集结构特性，引入基础数据概念，建模单模态任务，开发数据补全策略，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有数据中心方法对数据特性影响泛化的理论洞察有限，传统视角忽视数据质量结构方面。

Method: 引入SCAR方案从四方面刻画数据集特性，定义基础数据，建模单模态任务，基于泛化偏差开发数据补全策略。

Result: 实验验证了SCAR在预测数据效用和指导数据获取方面的有效性。

Conclusion: SCAR为数据理解提供了坚实基础，能有效指导多模态数据集的数据获取。

Abstract: Foundation models exhibit remarkable generalization across diverse tasks,
largely driven by the characteristics of their training data. Recent
data-centric methods like pruning and compression aim to optimize training but
offer limited theoretical insight into how data properties affect
generalization, especially the data characteristics in sample scaling.
Traditional perspectives further constrain progress by focusing predominantly
on data quantity and training efficiency, often overlooking structural aspects
of data quality. In this study, we introduce SCAR, a principled scheme for
characterizing the intrinsic structural properties of datasets across four key
measures: Scale, Coverage, Authenticity, and Richness. Unlike prior
data-centric measures, SCAR captures stable characteristics that remain
invariant under dataset scaling, providing a robust and general foundation for
data understanding. Leveraging these structural properties, we introduce
Foundation Data-a minimal subset that preserves the generalization behavior of
the full dataset without requiring model-specific retraining. We model
single-modality tasks as step functions and estimate the distribution of the
foundation data size to capture step-wise generalization bias across modalities
in the target multi-modal dataset. Finally, we develop a SCAR-guided data
completion strategy based on this generalization bias, which enables efficient,
modality-aware expansion of modality-specific characteristics in multimodal
datasets. Experiments across diverse multi-modal datasets and model
architectures validate the effectiveness of SCAR in predicting data utility and
guiding data acquisition. Code is available at https://github.com/McAloma/SCAR.

</details>


### [91] [Exploration of Low-Power Flexible Stress Monitoring Classifiers for Conformal Wearables](https://arxiv.org/abs/2508.19661)
*Florentia Afentaki,Sri Sai Rakesh Nakkilla,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Shiyi Jiang,Georgios Zervakis,Farshad Firouzi,Krishnendu Chakrabarty,Mehdi B. Tahoori*

Main category: cs.LG

TL;DR: 本文对低功耗、柔性压力分类器进行全面设计空间探索，设计超1200个柔性分类器，提供设计实时压力分类器的见解。


<details>
  <summary>Details</summary>
Motivation: 传统压力监测缺乏连续、便捷且经济的解决方案，现有可穿戴设备不适合连续监测，在柔性电子中实现复杂电路有挑战，且压力检测分类器设计研究不足。

Method: 对各种机器学习分类器、特征选择和神经简化算法进行探索，为每种情况设计全定制的低精度算术电路。

Result: 此次探索提供了设计实时压力分类器的见解，其比现有方法精度更高，且具备低成本、贴合、低功耗和小尺寸的特点。

Conclusion: 通过全面设计空间探索，有望设计出更优的实时压力分类器。

Abstract: Conventional stress monitoring relies on episodic, symptom-focused
interventions, missing the need for continuous, accessible, and cost-efficient
solutions. State-of-the-art approaches use rigid, silicon-based wearables,
which, though capable of multitasking, are not optimized for lightweight,
flexible wear, limiting their practicality for continuous monitoring. In
contrast, flexible electronics (FE) offer flexibility and low manufacturing
costs, enabling real-time stress monitoring circuits. However, implementing
complex circuits like machine learning (ML) classifiers in FE is challenging
due to integration and power constraints. Previous research has explored
flexible biosensors and ADCs, but classifier design for stress detection
remains underexplored. This work presents the first comprehensive design space
exploration of low-power, flexible stress classifiers. We cover various ML
classifiers, feature selection, and neural simplification algorithms, with over
1200 flexible classifiers. To optimize hardware efficiency, fully customized
circuits with low-precision arithmetic are designed in each case. Our
exploration provides insights into designing real-time stress classifiers that
offer higher accuracy than current methods, while being low-cost, conformable,
and ensuring low power and compact size.

</details>


### [92] [$\mathcal{C}^1$-approximation with rational functions and rational neural networks](https://arxiv.org/abs/2508.19672)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We show that suitably regular functions can be approximated in the
$\mathcal{C}^1$-norm both with rational functions and rational neural networks,
including approximation rates with respect to width and depth of the network,
and degree of the rational functions. As consequence of our results, we further
obtain $\mathcal{C}^1$-approximation results for rational neural networks with
the $\text{EQL}^\div$ and ParFam architecture, both of which are important in
particular in the context of symbolic regression for physical law learning.

</details>


### [93] [Metric spaces of walks and Lipschitz duality on graphs](https://arxiv.org/abs/2508.19709)
*R. Arnau,A. González Cortés,E. A. Sánchez Pérez,S. Sanjuan*

Main category: cs.LG

TL;DR: 研究图上行走的度量结构，引入加权度量，分析度量空间性质，给出接近度表示公式和构造，用于经典度量建模工具，有潜在应用。


<details>
  <summary>Details</summary>
Motivation: 研究图上行走作为Lipschitz序列的度量结构。

Method: 引入加权度量处理序列，定义行走间距离，分析度量空间性质，推导接近度表示公式。

Result: 建立了度量框架，可使用经典度量建模工具，如扩展Lipschitz函数。

Conclusion: 该度量框架可用于估计接近度和开发强化学习策略，适用于网络结构上的Lipschitz回归。

Abstract: We study the metric structure of walks on graphs, understood as Lipschitz
sequences. To this end, a weighted metric is introduced to handle sequences,
enabling the definition of distances between walks based on stepwise vertex
distances and weighted norms. We analyze the main properties of these metric
spaces, which provides the foundation for the analysis of weaker forms of
instruments to measure relative distances between walks: proximities. We
provide some representation formulas for such proximities under different
assumptions and provide explicit constructions for these cases. The resulting
metric framework allows the use of classical tools from metric modeling, such
as the extension of Lipschitz functions from subspaces of walks, which permits
extending proximity functions while preserving fundamental properties via the
mentioned representations. Potential applications include the estimation of
proximities and the development of reinforcement learning strategies based on
exploratory walks, offering a robust approach to Lipschitz regression on
network structures.

</details>


### [94] [Tune My Adam, Please!](https://arxiv.org/abs/2508.19733)
*Theodoros Athanasiadis,Steven Adriaensen,Samuel Müller,Frank Hutter*

Main category: cs.LG

TL;DR: 提出Adam - PFN用于Adam优化器超参数调优，结合CDF - augment方法，提升学习曲线外推和超参优化效果。


<details>
  <summary>Details</summary>
Motivation: Adam优化器超参数调优繁琐且成本高，现有Freeze - thaw BO方法受限于无先验知识的通用代理模型。

Method: 提出Adam - PFN作为Freeze - thaw BO的新代理模型，在TaskSet学习曲线上预训练，并使用CDF - augment学习曲线增强方法。

Result: 在TaskSet评估任务中改善学习曲线外推，加速超参数优化，在分布外任务中有良好表现。

Conclusion: 所提方法能有效解决Adam优化器超参数调优问题。

Abstract: The Adam optimizer remains one of the most widely used optimizers in deep
learning, and effectively tuning its hyperparameters is key to optimizing
performance. However, tuning can be tedious and costly. Freeze-thaw Bayesian
Optimization (BO) is a recent promising approach for low-budget hyperparameter
tuning, but is limited by generic surrogates without prior knowledge of how
hyperparameters affect learning. We propose Adam-PFN, a new surrogate model for
Freeze-thaw BO of Adam's hyperparameters, pre-trained on learning curves from
TaskSet, together with a new learning curve augmentation method, CDF-augment,
which artificially increases the number of available training examples. Our
approach improves both learning curve extrapolation and accelerates
hyperparameter optimization on TaskSet evaluation tasks, with strong
performance on out-of-distribution (OOD) tasks.

</details>


### [95] [InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections](https://arxiv.org/abs/2508.19737)
*Meng Qin,Weihua Li,Jinqiang Cui,Sen Pei*

Main category: cs.LG

TL;DR: 本文从图信号处理角度出发，提出InfraredGP用于图划分，不训练即可获高质量结果且效率高。


<details>
  <summary>Details</summary>
Motivation: 探索图拉普拉斯负修正后超出常规范围的低频信息能否编码更多社区结构特性。

Method: 采用谱GNN为骨干，结合低通滤波器和负修正机制，仅输入随机数据，经一次前向传播得到图嵌入，再用BIRCH聚类。

Result: InfraredGP能为聚类模块生成可区分的嵌入，在静态和流图划分中比基线方法效率高（快16 - 23倍）且质量有竞争力。

Conclusion: 基于负修正机制放大超出[0, 2]范围的低频信息，InfraredGP不训练也能实现高质量图划分。

Abstract: Graph partitioning (GP), a.k.a. community detection, is a classic problem
that divides nodes of a graph into densely-connected blocks. From a perspective
of graph signal processing, we find that graph Laplacian with a negative
correction can derive graph frequencies beyond the conventional range $[0, 2]$.
To explore whether the low-frequency information beyond this range can encode
more informative properties about community structures, we propose InfraredGP.
It (\romannumeral1) adopts a spectral GNN as its backbone combined with
low-pass filters and a negative correction mechanism, (\romannumeral2) only
feeds random inputs to this backbone, (\romannumeral3) derives graph embeddings
via one feed-forward propagation (FFP) without any training, and
(\romannumeral4) obtains feasible GP results by feeding the derived embeddings
to BIRCH. Surprisingly, our experiments demonstrate that based solely on the
negative correction mechanism that amplifies low-frequency information beyond
$[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard
clustering modules (e.g., BIRCH) and obtain high-quality results for GP without
any training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate
InfraredGP for both static and streaming GP, where InfraredGP can achieve much
better efficiency (e.g., 16x-23x faster) and competitive quality over various
baselines. We have made our code public at
https://github.com/KuroginQin/InfraredGP

</details>


### [96] [Fast 3D Diffusion for Scalable Granular Media Synthesis](https://arxiv.org/abs/2508.19752)
*Muhammad Moeeze Hassan,Régis Cottereau,Filippo Gatti,Patryk Dec*

Main category: cs.LG

TL;DR: 本文提出基于3D扩散模型的生成式管道，克服离散元法模拟颗粒介质的计算瓶颈，实现实时、可扩展的颗粒介质合成。


<details>
  <summary>Details</summary>
Motivation: 离散元法模拟颗粒介质在初始化阶段计算量大、耗时久，需要克服这一瓶颈。

Method: 采用两阶段管道，先训练扩散模型生成独立3D体素网格，再用3D修复模型将网格无缝拼接。探索多种输入掩码策略，引入重注入噪声调度器输出与加权损失确保生成连贯性。

Result: 实现计算时间随样本大小线性缩放，如1.2米长的道砟轨道合成从3小时降至20秒以内。

Conclusion: 该方法能实现物理连贯、实时、可扩展的颗粒介质合成，适用于工业应用。

Abstract: Simulating granular media, using Discrete Element Method is a computationally
intensive task. This is especially true during initialization phase, which
dominates total simulation time because of large displacements involved and
associated kinetic energy. We overcome this bottleneck with a novel generative
pipeline based on 3D diffusion models that directly synthesizes arbitrarily
large granular assemblies in their final and physically realistic
configurations. The approach frames the problem as a 3D generative modeling
task, consisting of a two-stage pipeline. First a diffusion model is trained to
generate independent 3D voxel grids representing granular media. Second, a 3D
inpainting model, adapted from 2D inpainting techniques using masked inputs,
stitches these grids together seamlessly, enabling synthesis of large samples
with physically realistic structure. The inpainting model explores several
masking strategies for the inputs to the underlying UNets by training the
network to infer missing portions of voxel grids from a concatenation of noised
tensors, masks, and masked tensors as input channels. The model also adapts a
2D repainting technique of re-injecting noise scheduler output with ground
truth to provide a strong guidance to the 3D model. This along with weighted
losses ensures long-term coherence over generation of masked regions. Both
models are trained on the same binarized 3D occupancy grids extracted from
small-scale DEM simulations, achieving linear scaling of computational time
with respect to sample size. Quantitatively, a 1.2 m long ballasted rail track
synthesis equivalent to a 3-hour DEM simulation, was completed under 20
seconds. The generated voxel grids can also be post-processed to extract grain
geometries for DEM-compatibility as well, enabling physically coherent,
real-time, scalable granular media synthesis for industrial applications.

</details>


### [97] [PSO-Merging: Merging Models Based on Particle Swarm Optimization](https://arxiv.org/abs/2508.19839)
*Kehao Zhang,Shaolei Zhang,Yang Feng*

Main category: cs.LG

TL;DR: 本文提出基于粒子群优化的PSO - Merging模型融合方法，实验显示其优于基线方法，为模型融合提供高效可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 现有数据无关模型融合方法因缺乏数据驱动有性能局限，数据驱动方法中基于梯度的计算成本高，无梯度方法难在有限步骤取得满意结果。

Method: 提出PSO - Merging方法，用预训练模型、专家模型和稀疏化专家模型初始化粒子群，经多次迭代，以最终全局最优粒子作为融合模型。

Result: 在不同语言模型上的实验表明PSO - Merging总体上优于基线融合方法。

Conclusion: PSO - Merging为模型融合提供了更高效和可扩展的解决方案。

Abstract: Model merging has emerged as an efficient strategy for constructing multitask
models by integrating the strengths of multiple available expert models,
thereby reducing the need to fine-tune a pre-trained model for all the tasks
from scratch. Existing data-independent methods struggle with performance
limitations due to the lack of data-driven guidance. Data-driven approaches
also face key challenges: gradient-based methods are computationally expensive,
limiting their practicality for merging large expert models, whereas existing
gradient-free methods often fail to achieve satisfactory results within a
limited number of optimization steps. To address these limitations, this paper
introduces PSO-Merging, a novel data-driven merging method based on the
Particle Swarm Optimization (PSO). In this approach, we initialize the particle
swarm with a pre-trained model, expert models, and sparsified expert models. We
then perform multiple iterations, with the final global best particle serving
as the merged model. Experimental results on different language models show
that PSO-Merging generally outperforms baseline merging methods, offering a
more efficient and scalable solution for model merging.

</details>


### [98] [Symplectic convolutional neural networks](https://arxiv.org/abs/2508.19842)
*Süleyman Yıldız,Konrad Janik,Peter Benner*

Main category: cs.LG

TL;DR: 提出新的辛卷积神经网络架构，在三个方程示例上验证性能且优于线性辛自编码器。


<details>
  <summary>Details</summary>
Motivation: 构建新的具有辛性质的卷积神经网络架构。

Method: 利用辛神经网络、适当辛分解和张量技术，引入卷积层的数学等价形式，用辛神经网络参数化CNN层，引入辛池化层构建完整自编码器。

Result: 在波方程、非线性薛定谔方程和正弦 - 戈登方程三个示例上，辛CNN性能优于通过适当辛分解得到的线性辛自编码器。

Conclusion: 所提出的辛卷积神经网络架构具有良好性能。

Abstract: We propose a new symplectic convolutional neural network (CNN) architecture
by leveraging symplectic neural networks, proper symplectic decomposition, and
tensor techniques. Specifically, we first introduce a mathematically equivalent
form of the convolution layer and then, using symplectic neural networks, we
demonstrate a way to parameterize the layers of the CNN to ensure that the
convolution layer remains symplectic. To construct a complete autoencoder, we
introduce a symplectic pooling layer. We demonstrate the performance of the
proposed neural network on three examples: the wave equation, the nonlinear
Schr\"odinger (NLS) equation, and the sine-Gordon equation. The numerical
results indicate that the symplectic CNN outperforms the linear symplectic
autoencoder obtained via proper symplectic decomposition.

</details>


### [99] [Physics-Informed DeepONet Coupled with FEM for Convective Transport in Porous Media with Sharp Gaussian Sources](https://arxiv.org/abs/2508.19847)
*Erdi Kara,Panos Stinis*

Main category: cs.LG

TL;DR: 提出耦合有限元法与物理信息DeepONet的混合框架，用于模拟多孔介质中流体传输，有自适应采样策略，速度比传统求解器快。


<details>
  <summary>Details</summary>
Motivation: 需要一种能准确且快速模拟多孔介质中从尖锐局部高斯源进行流体传输的方法。

Method: 用有限元法求解Darcy系统，将速度场传递给物理信息DeepONet学习源函数到溶质浓度分布的映射，引入自适应采样策略处理尖锐源引起的陡梯度。

Result: 数值实验表明该方法与参考解吻合良好，比传统求解器快几个数量级。

Conclusion: 该方法适用于相关场景的实际应用，代码可在指定链接获取。

Abstract: We present a hybrid framework that couples finite element methods (FEM) with
physics-informed DeepONet to model fluid transport in porous media from sharp,
localized Gaussian sources. The governing system consists of a steady-state
Darcy flow equation and a time-dependent convection-diffusion equation. Our
approach solves the Darcy system using FEM and transfers the resulting velocity
field to a physics-informed DeepONet, which learns the mapping from source
functions to solute concentration profiles. This modular strategy preserves
FEM-level accuracy in the flow field while enabling fast inference for
transport dynamics. To handle steep gradients induced by sharp sources, we
introduce an adaptive sampling strategy for trunk collocation points. Numerical
experiments demonstrate that our method is in good agreement with the reference
solutions while offering orders of magnitude speedups over traditional solvers,
making it suitable for practical applications in relevant scenarios.
Implementation of our proposed method is available at
https://github.com/erkara/fem-pi-deeponet.

</details>


### [100] [Quantum latent distributions in deep generative models](https://arxiv.org/abs/2508.19857)
*Omar Bacarreza,Thorin Farnsworth,Alexander Makarovskiy,Hugo Wallner,Tessa Hicks,Santiago Sempere-Llagostera,John Price,Robert J. A. Francis-Jones,William R. Clements*

Main category: cs.LG

TL;DR: 本文研究量子潜在分布在生成模型中的应用，证明特定条件下其优势，实验表明能提升生成性能，还探索兼容架构，证实近量子处理器可拓展深度生成模型能力。


<details>
  <summary>Details</summary>
Motivation: 探究量子处理器产生的潜在空间分布何时能提升生成模型性能以及这些提升是否可重复。

Method: 证明特定条件下量子潜在分布的优势，提供判断量子优势何时出现的直觉，在合成量子数据集和QM9分子数据集上用模拟和真实光子量子处理器进行基准实验，探索扩散和流匹配模型。

Result: 量子潜在分布在GAN中相比经典基线能提升生成性能，找到与量子潜在分布兼容的架构。

Conclusion: 近量子处理器可拓展深度生成模型的能力。

Abstract: Many successful families of generative models leverage a low-dimensional
latent distribution that is mapped to a data distribution. Though simple latent
distributions are commonly used, it has been shown that more sophisticated
distributions can improve performance. For instance, recent work has explored
using the distributions produced by quantum processors and found empirical
improvements. However, when latent space distributions produced by quantum
processors can be expected to improve performance, and whether these
improvements are reproducible, are open questions that we investigate in this
work. We prove that, under certain conditions, these "quantum latent
distributions" enable generative models to produce data distributions that
classical latent distributions cannot efficiently produce. We also provide
actionable intuitions to identify when such quantum advantages may arise in
real-world settings. We perform benchmarking experiments on both a synthetic
quantum dataset and the QM9 molecular dataset, using both simulated and real
photonic quantum processors. Our results demonstrate that quantum latent
distributions can lead to improved generative performance in GANs compared to a
range of classical baselines. We also explore diffusion and flow matching
models, identifying architectures compatible with quantum latent distributions.
This work confirms that near-term quantum processors can expand the
capabilities of deep generative models.

</details>


### [101] [Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks](https://arxiv.org/abs/2508.19884)
*Mingyue Kong,Yinglong Zhang,Chengda Xu,Xuewen Xia,Xing Xu*

Main category: cs.LG

TL;DR: 提出无参数图神经网络框架SDGNN，在多数据集表现优于主流GNN，为无参GNN设计提供新思路。


<details>
  <summary>Details</summary>
Motivation: 主流GNN依赖大量可训练参数和固定聚合规则，难以适应结构异质性强和特征分布复杂的图数据，易导致节点表示过平滑和语义退化。

Method: 受结构多样性理论启发，设计统一的结构多样性消息传递机制，从结构驱动和特征驱动互补建模，不引入额外可训练参数。

Result: 在八个公共基准数据集和一个跨学科PubMed引文网络上，SDGNN在低监督、类别不平衡和跨域转移等挑战条件下始终优于主流GNN。

Conclusion: 为无参数图神经网络设计提供新理论视角和通用方法，验证了结构多样性在图表示学习中的重要性。

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in structured
data modeling tasks such as node classification. However, mainstream approaches
generally rely on a large number of trainable parameters and fixed aggregation
rules, making it difficult to adapt to graph data with strong structural
heterogeneity and complex feature distributions. This often leads to
over-smoothing of node representations and semantic degradation. To address
these issues, this paper proposes a parameter-free graph neural network
framework based on structural diversity, namely SDGNN (Structural-Diversity
Graph Neural Network). The framework is inspired by structural diversity theory
and designs a unified structural-diversity message passing mechanism that
simultaneously captures the heterogeneity of neighborhood structures and the
stability of feature semantics, without introducing additional trainable
parameters. Unlike traditional parameterized methods, SDGNN does not rely on
complex model training, but instead leverages complementary modeling from both
structure-driven and feature-driven perspectives, thereby effectively improving
adaptability across datasets and scenarios. Experimental results show that on
eight public benchmark datasets and an interdisciplinary PubMed citation
network, SDGNN consistently outperforms mainstream GNNs under challenging
conditions such as low supervision, class imbalance, and cross-domain transfer.
This work provides a new theoretical perspective and general approach for the
design of parameter-free graph neural networks, and further validates the
importance of structural diversity as a core signal in graph representation
learning. To facilitate reproducibility and further research, the full
implementation of SDGNN has been released at:
https://github.com/mingyue15694/SGDNN/tree/main

</details>


### [102] [NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs](https://arxiv.org/abs/2508.19896)
*Davorin Miličević,Ratko Grbić*

Main category: cs.LG

TL;DR: 提出NM - Hebb训练框架，结合神经启发的局部可塑性和距离感知监督，在多个数据集和骨干网络上提升CNN准确率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统深度卷积神经网络依赖全局梯度优化，存在过拟合、滤波器冗余和可解释性低等问题，需要改进。

Method: 提出NM - Hebb两阶段训练框架，第一阶段结合交叉熵目标与生物启发机制优化；第二阶段用成对度量学习损失微调骨干网络。

Result: 在CIFAR - 10、CIFAR - 100和TinyImageNet数据集上，相比基线和其他方法有一致提升，Top - 1准确率提高，NMI增加，生成更结构化和选择性的特征。

Conclusion: 将局部Hebbian可塑性与基于度量的微调相结合，使CNN更准确且可解释，对资源受限和安全关键的AI部署有实际好处。

Abstract: Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often
rely on purely global, gradient-based optimisation, which can lead to
overfitting, redundant filters, and reduced interpretability. To address these
limitations, we propose NM-Hebb, a two-phase training framework that integrates
neuro-inspired local plasticity with distance-aware supervision. Phase 1
extends standard supervised training by jointly optimising a cross-entropy
objective with two biologically inspired mechanisms: (i) a Hebbian regulariser
that aligns the spatial mean of activations with the mean of the corresponding
convolutional filter weights, encouraging structured, reusable primitives; and
(ii) a learnable neuromodulator that gates an elastic-weight-style
consolidation loss, preserving beneficial parameters without freezing the
network. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,
explicitly compressing intra-class distances and enlarging inter-class margins
in the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet
across five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,
DenseNet-121), NM-Hebb achieves consistent gains over baseline and other
methods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp
(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual
Information (NMI) increased by up to +0.15. Qualitative visualisations and
filter-level analyses further confirm that NM-Hebb produces more structured and
selective features, yielding tighter and more interpretable class clusters.
Overall, coupling local Hebbian plasticity with metric-based fine-tuning yields
CNNs that are not only more accurate but also more interpretable, offering
practical benefits for resource-constrained and safety-critical AI deployments.

</details>


### [103] [Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning](https://arxiv.org/abs/2508.19900)
*Tan Jing,Xiaorui Li,Chao Yao,Xiaojuan Ban,Yuetong Fang,Renjing Xu,Zhaolin Yuan*

Main category: cs.LG

TL;DR: 提出自适应策略约束缩放（ASPC）框架，在多个数据集实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法需细致调整超参数，耗时且不实际，因此需要改进。

Method: 提出二阶可微的ASPC框架，在训练中动态平衡强化学习和行为克隆。

Result: 在四个D4RL领域的39个数据集实验中，使用单一超参数配置的ASPC优于其他自适应约束方法和需要逐数据集调参的先进离线强化学习算法，计算开销极小。

Conclusion: ASPC框架有效，能解决现有离线强化学习方法调参难题。

Abstract: Offline reinforcement learning (RL) enables learning effective policies from
fixed datasets without any environment interaction. Existing methods typically
employ policy constraints to mitigate the distribution shift encountered during
offline RL training. However, because the scale of the constraints varies
across tasks and datasets of differing quality, existing methods must
meticulously tune hyperparameters to match each dataset, which is
time-consuming and often impractical. We propose Adaptive Scaling of Policy
Constraints (ASPC), a second-order differentiable framework that dynamically
balances RL and behavior cloning (BC) during training. We theoretically analyze
its performance improvement guarantee. In experiments on 39 datasets across
four D4RL domains, ASPC using a single hyperparameter configuration outperforms
other adaptive constraint methods and state-of-the-art offline RL algorithms
that require per-dataset tuning while incurring only minimal computational
overhead. The code will be released at https://github.com/Colin-Jing/ASPC.

</details>


### [104] [GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs](https://arxiv.org/abs/2508.19907)
*Hewen Wang,Renchi Yang,Xiaokui Xiao*

Main category: cs.LG

TL;DR: 本文提出用于有符号二分图（SBG）链接符号预测的GegenNet模型，通过三项技术贡献提升性能，在6个基准数据集上优于11个竞争对手。


<details>
  <summary>Details</summary>
Motivation: 现有链接符号预测方法多针对单分图，对SBG的图神经网络方法中基本谱卷积算子并非最优，需新模型。

Method: 提出GegenNet模型，包括用于节点特征初始化的谱分解技术、基于盖根鲍尔多项式基的谱图滤波器、正负边交替使用盖根鲍尔多项式滤波器的多层符号感知谱卷积网络。

Result: 在6个基准SBG数据集上，GegenNet在链接符号预测中比11个强竞争对手性能显著提升，AUC最多提高4.28%，F1最多提高11.69%。

Conclusion: GegenNet是用于SBG链接符号预测的有效模型，具有良好性能。

Abstract: Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,
the goal of link sign prediction is to predict the signs of potential links
connecting U and V based on known positive and negative edges in G. The
majority of existing solutions towards link sign prediction mainly focus on
unipartite signed graphs, which are sub-optimal due to the neglect of node
heterogeneity and unique bipartite characteristics of SBGs. To this end, recent
studies adapt graph neural networks to SBGs by introducing message-passing
schemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node
pairs. However, the fundamental spectral convolutional operators were
originally designed for positive links in unsigned graphs, and thus, are not
optimal for inferring missing positive or negative links from known ones in
SBGs.
  Motivated by this, this paper proposes GegenNet, a novel and effective
spectral convolutional neural network model for link sign prediction in SBGs.
In particular, GegenNet achieves enhanced model capacity and high predictive
accuracy through three main technical contributions: (i) fast and theoretically
grounded spectral decomposition techniques for node feature initialization;
(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and
(iii) multi-layer sign-aware spectral convolutional networks alternating
Gegenbauer polynomial filters with positive and negative edges. Our extensive
empirical studies reveal that GegenNet can achieve significantly superior
performance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign
prediction compared to 11 strong competitors over 6 benchmark SBG datasets.

</details>


### [105] [Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling](https://arxiv.org/abs/2508.19915)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.LG

TL;DR: 提出基于UMLS的放射学报告文本比较新方法，在MIMIC - CXR的放射图像分类任务中表现优于现有嵌入方法，还能生成疾病标签，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于放射学报告的检索增强学习方法依赖高维文本嵌入，难解释、计算成本高且与医学知识结构不匹配。

Method: 基于UMLS临床概念，用RadGraph - XL和SapBERT提取标准化医学实体并链接到UMLS概念，定义基于改进加权Tversky Index的任务自适应相似度度量。

Result: 在MIMIC - CXR的放射图像分类任务中，尤其在长尾设置下，优于现有基于嵌入的检索方法，还能生成本体支持的疾病标签。

Conclusion: 为临床AI系统提供更具解释性、可靠性和任务特异性的检索策略，适用于对可解释性和领域知识集成要求高的场景。

Abstract: Retrieval-augmented learning based on radiology reports has emerged as a
promising direction to improve performance on long-tail medical imaging tasks,
such as rare disease detection in chest X-rays. Most existing methods rely on
comparing high-dimensional text embeddings from models like CLIP or CXR-BERT,
which are often difficult to interpret, computationally expensive, and not
well-aligned with the structured nature of medical knowledge. We propose a
novel, ontology-driven alternative for comparing radiology report texts based
on clinically grounded concepts from the Unified Medical Language System
(UMLS). Our method extracts standardised medical entities from free-text
reports using an enhanced pipeline built on RadGraph-XL and SapBERT. These
entities are linked to UMLS concepts (CUIs), enabling a transparent,
interpretable set-based representation of each report. We then define a
task-adaptive similarity measure based on a modified and weighted version of
the Tversky Index that accounts for synonymy, negation, and hierarchical
relationships between medical entities. This allows efficient and semantically
meaningful similarity comparisons between reports. We demonstrate that our
approach outperforms state-of-the-art embedding-based retrieval methods in a
radiograph classification task on MIMIC-CXR, particularly in long-tail
settings. Additionally, we use our pipeline to generate ontology-backed disease
labels for MIMIC-CXR, offering a valuable new resource for downstream learning
tasks. Our work provides more explainable, reliable, and task-specific
retrieval strategies in clinical AI systems, especially when interpretability
and domain knowledge integration are essential. Our code is available at
https://github.com/Felix-012/ontology-concept-distillation

</details>


### [106] [FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for Traffic Classification](https://arxiv.org/abs/2508.19924)
*Liming Liu,Ruoyu Li,Qing Li,Meijia Hou,Yong Jiang,Mingwei Xu*

Main category: cs.LG

TL;DR: 提出FlowletFormer用于网络流量分析，实验显示其性能优于现有方法，且能更好理解网络传输原理。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练模型的网络流量分类方法难以捕捉数据包结构特征、流级行为、分层协议语义和包间上下文关系。

Method: 提出FlowletFormer，包括用于分割流量的模型、捕捉多层协议语义的嵌入层和增强学习的预训练任务。

Result: FlowletFormer在流量表示有效性、分类准确率和少样本学习能力上显著优于现有方法。

Conclusion: FlowletFormer有效整合网络知识，为流量分析提供更强大可靠的框架。

Abstract: Network traffic classification using pre-training models has shown promising
results, but existing methods struggle to capture packet structural
characteristics, flow-level behaviors, hierarchical protocol semantics, and
inter-packet contextual relationships. To address these challenges, we propose
FlowletFormer, a BERT-based pre-training model specifically designed for
network traffic analysis. FlowletFormer introduces a Coherent Behavior-Aware
Traffic Representation Model for segmenting traffic into semantically
meaningful units, a Protocol Stack Alignment-Based Embedding Layer to capture
multilayer protocol semantics, and Field-Specific and Context-Aware Pretraining
Tasks to enhance both inter-packet and inter-flow learning. Experimental
results demonstrate that FlowletFormer significantly outperforms existing
methods in the effectiveness of traffic representation, classification
accuracy, and few-shot learning capability. Moreover, by effectively
integrating domain-specific network knowledge, FlowletFormer shows better
comprehension of the principles of network transmission (e.g., stateful
connections of TCP), providing a more robust and trustworthy framework for
traffic analysis.

</details>


### [107] [Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions](https://arxiv.org/abs/2508.19945)
*Zhouyu Zhang,Chih-Yuan Chiu,Glen Chou*

Main category: cs.LG

TL;DR: 提出基于逆动态博弈的算法从多智能体局部广义纳什均衡交互数据集中学习参数约束，能设计稳健运动规划。


<details>
  <summary>Details</summary>
Motivation: 从给定的多智能体局部广义纳什均衡交互数据集中学习参数约束。

Method: 引入混合整数线性规划（MILP）对交互智能体的Karush - Kuhn - Tucker（KKT）条件进行编码，恢复与交互演示的纳什平稳性一致的约束。

Result: 方法能从具有非线性动力学的智能体交互演示中推断各类约束并设计交互式运动规划，在仿真和硬件实验中得到验证。

Conclusion: 建立了学习真实安全和不安全集内近似的理论保证及从纳什均衡交互演示中学习约束的局限性。

Abstract: We present an inverse dynamic game-based algorithm to learn parametric
constraints from a given dataset of local generalized Nash equilibrium
interactions between multiple agents. Specifically, we introduce mixed-integer
linear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the
interacting agents, which recover constraints consistent with the Nash
stationarity of the interaction demonstrations. We establish theoretical
guarantees that our method learns inner approximations of the true safe and
unsafe sets, as well as limitations of constraint learnability from
demonstrations of Nash equilibrium interactions. We also use the interaction
constraints recovered by our method to design motion plans that robustly
satisfy the underlying constraints. Across simulations and hardware
experiments, our methods proved capable of inferring constraints and designing
interactive motion plans for various classes of constraints, both convex and
non-convex, from interaction demonstrations of agents with nonlinear dynamics.

</details>


### [108] [Global Permutation Entropy](https://arxiv.org/abs/2508.19955)
*Abhijeet Avhale,Joscha Diehl,Niraj Velankar,Emanuele Verri*

Main category: cs.LG

TL;DR: 介绍了一种新的全局排列熵（GPE）指标，考虑所有可能模式，通过实验证明其有效性并提供计算包。


<details>
  <summary>Details</summary>
Motivation: 为实值时间序列复杂性度量引入新指标，挖掘标准排列熵无法获取的结构信息。

Method: 提出GPE指标，利用新算法提取完整排列轮廓进行计算。

Result: 通过对合成数据集实验，证明GPE能揭示标准排列熵无法获取的结构信息。

Conclusion: GPE是一种有效的复杂性度量指标，提供了Julia计算包。

Abstract: Permutation Entropy, introduced by Bandt and Pompe, is a widely used
complexity measure for real-valued time series that is based on the relative
order of values within consecutive segments of fixed length. After
standardizing each segment to a permutation and computing the frequency
distribution of these permutations, Shannon Entropy is then applied to quantify
the series' complexity. We introduce Global Permutation Entropy (GPE), a novel
index that considers all possible patterns of a given length, including
non-consecutive ones. Its computation relies on recently developed algorithms
that enable the efficient extraction of full permutation profiles. We
illustrate some properties of GPE and demonstrate its effectiveness through
experiments on synthetic datasets, showing that it reveals structural
information not accessible through standard permutation entropy. We provide a
Julia package for the calculation of GPE at
`https://github.com/AThreeH1/Global-Permutation-Entropy'.

</details>


### [109] [Short-Horizon Predictive Maintenance of Industrial Pumps Using Time-Series Features and Machine Learning](https://arxiv.org/abs/2508.19974)
*Khaled M. A. Alghtus,Aiyad Gannan,Khalid M. Alhajri,Ali L. A. Al Jubouri,Hassan A. I. Al-Janahi*

Main category: cs.LG

TL;DR: 本文提出用机器学习框架结合实时传感器数据对工业离心泵短期故障进行预测，比较不同参数模型效果，发现最优历史长度与预测时长有关。


<details>
  <summary>Details</summary>
Motivation: 利用实时传感器数据对工业离心泵进行短期故障预警，将预测性维护集成到实时工业监测系统中。

Method: 采用滑动窗口评估60分钟和120分钟两个回顾期，提取统计特征，用SMOTE算法处理类别不平衡，用随机森林和XGBoost分类器训练和测试。

Result: 随机森林模型在60分钟窗口下短期预测性能最佳，不同窗口在不同预测时长下召回率不同，XGBoost性能稍低。

Conclusion: 最优历史长度取决于预测时长，不同故障模式演化的时间尺度不同，该方法为实时工业监测系统集成预测性维护提供了解决方案。

Abstract: This study presents a machine learning framework for forecasting short-term
faults in industrial centrifugal pumps using real-time sensor data. The
approach aims to predict {EarlyWarning} conditions 5, 15, and 30 minutes in
advance based on patterns extracted from historical operation. Two lookback
periods, 60 minutes and 120 minutes, were evaluated using a sliding window
approach. For each window, statistical features including mean, standard
deviation, minimum, maximum, and linear trend were extracted, and class
imbalance was addressed using the SMOTE algorithm. Random Forest and XGBoost
classifiers were trained and tested on the labeled dataset. Results show that
the Random Forest model achieved the best short-term forecasting performance
with a 60-minute window, reaching recall scores of 69.2\% at 5 minutes, 64.9\%
at 15 minutes, and 48.6\% at 30 minutes. With a 120-minute window, the Random
Forest model achieved 57.6\% recall at 5 minutes, and improved predictive
accuracy of 65.6\% at both 15 and 30 minutes. XGBoost displayed similar but
slightly lower performance. These findings highlight that optimal history
length depends on the prediction horizon, and that different fault patterns may
evolve at different timescales. The proposed method offers an interpretable and
scalable solution for integrating predictive maintenance into real-time
industrial monitoring systems.

</details>


### [110] [Reducing Street Parking Search Time via Smart Assignment Strategies](https://arxiv.org/abs/2508.19979)
*Behafarid Hemmatpour,Javad Dogani,Nikolaos Laoutaris*

Main category: cs.LG

TL;DR: 本文通过对马德里街道停车生态系统的模拟，研究手机应用中用户协调和信息可用性对停车搜索时间和找到车位概率的影响，提出Cord - Approx策略，该策略在模拟中显著减少了用户停车搜索时间。


<details>
  <summary>Details</summary>
Motivation: 在密集城区，寻找街道停车位会加剧交通拥堵，而基于手机的实时助手有效性研究不足，因此要量化用户协调和信息可用性对停车搜索的影响。

Method: 对马德里街道停车生态系统进行数据驱动的模拟，分析四种策略，其中Cord - Approx策略利用过去的占用分布拉长用户与替代停车位的物理距离，并解决匈牙利匹配问题进行调度。

Result: 在高保真模拟中，Cord - Approx用户平均6.69分钟找到停车位，远低于无应用的非用户的19.98分钟；在中心枢纽和居民区，Cord - Approx分别将系统用户的搜索时间相对于非用户减少了72%（范围67 - 76%）和高达73%。

Conclusion: Cord - Approx策略能有效减少街道停车搜索时间。

Abstract: In dense metropolitan areas, searching for street parking adds to traffic
congestion. Like many other problems, real-time assistants based on mobile
phones have been proposed, but their effectiveness is understudied. This work
quantifies how varying levels of user coordination and information availability
through such apps impact search time and the probability of finding street
parking. Through a data-driven simulation of Madrid's street parking ecosystem,
we analyze four distinct strategies: uncoordinated search (Unc-Agn),
coordinated parking without awareness of non-users (Cord-Agn), an idealized
oracle system that knows the positions of all non-users (Cord-Oracle), and our
novel/practical Cord-Approx strategy that estimates non-users' behavior
probabilistically. The Cord-Approx strategy, instead of requiring knowledge of
how close non-users are to a certain spot in order to decide whether to
navigate toward it, uses past occupancy distributions to elongate physical
distances between system users and alternative parking spots, and then solves a
Hungarian matching problem to dispatch accordingly. In high-fidelity
simulations of Madrid's parking network with real traffic data, users of
Cord-Approx averaged 6.69 minutes to find parking, compared to 19.98 minutes
for non-users without an app. A zone-level snapshot shows that Cord-Approx
reduces search time for system users by 72% (range = 67-76%) in central hubs,
and up to 73% in residential areas, relative to non-users.

</details>


### [111] [Evaluating Language Model Reasoning about Confidential Information](https://arxiv.org/abs/2508.19980)
*Dylan Sam,Alexander Robey,Andy Zou,Matt Fredrikson,J. Zico Kolter*

Main category: cs.LG

TL;DR: 研究语言模型在遵循上下文安全规范的表现，开发PasswordEval基准测试，发现当前模型有问题，推理能力或需不同训练方式。


<details>
  <summary>Details</summary>
Motivation: 语言模型在高风险场景部署，确保其可靠遵循用户规则成安全关键问题，研究其上下文鲁棒性。

Method: 开发PasswordEval基准测试，衡量语言模型能否正确判断用户请求是否授权；从增加对抗压力和进行长多轮对话两方面提升评估难度。

Result: 当前开源和闭源模型完成任务有困难，推理能力通常不能提升表现，推理痕迹常泄露机密信息。

Conclusion: 当前前沿模型不适合处理机密信息，推理能力需不同训练方式以用于高风险场景。

Abstract: As language models are increasingly deployed as autonomous agents in
high-stakes settings, ensuring that they reliably follow user-defined rules has
become a critical safety concern. To this end, we study whether language models
exhibit contextual robustness, or the capability to adhere to context-dependent
safety specifications. For this analysis, we develop a benchmark (PasswordEval)
that measures whether language models can correctly determine when a user
request is authorized (i.e., with a correct password). We find that current
open- and closed-source models struggle with this seemingly simple task, and
that, perhaps surprisingly, reasoning capabilities do not generally improve
performance. In fact, we find that reasoning traces frequently leak
confidential information, which calls into question whether reasoning traces
should be exposed to users in such applications. We also scale the difficulty
of our evaluation along multiple axes: (i) by adding adversarial user pressure
through various jailbreaking strategies, and (ii) through longer multi-turn
conversations where password verification is more challenging. Overall, our
results suggest that current frontier models are not well-suited to handling
confidential information, and that reasoning capabilities may need to be
trained in a different manner to make them safer for release in high-stakes
settings.

</details>


### [112] [Self-Supervised Pre-Training with Equilibrium Constraints](https://arxiv.org/abs/2508.19990)
*Xiaodong Cui,A F M Saif,Brian Kingsbury,Tianyi Chen*

Main category: cs.LG

TL;DR: 提出一种处理异构数据的自监督预训练新方法，通过实验证明可提升模型适应性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督预训练处理异构数据时采用传统混合数据并最小化平均全局损失的方式，需新方法优化各数据源。

Method: 施加额外平衡约束，将问题表述为双层优化问题，用一阶近似法求解，并探讨与MAML的联系。

Result: 在多领域和多语言数据集的自监督预训练实验中，该方法显著提高了自监督预训练模型对下游监督微调任务的适应性。

Conclusion: 提出的新自监督预训练方法能有效处理异构数据，提升模型在下游任务的适应性。

Abstract: Self-supervised pre-training using unlabeled data is widely used in machine
learning. In this paper, we propose a new self-supervised pre-training approach
to dealing with heterogeneous data. Instead of mixing all the data and
minimizing the averaged global loss in the conventional way, we impose
additional equilibrium constraints to ensure that the models optimizes each
source of heterogeneous data to its local optima after $K$-step gradient
descent initialized from the model. We formulate this as a bilevel optimization
problem, and use the first-order approximation method to solve the problem. We
discuss its connection to model-agnostic meta learning (MAML). Experiments are
carried out on self-supervised pre-training using multi-domain and multilingual
datasets, demonstrating that the proposed approach can significantly improve
the adaptivity of the self-supervised pre-trained model for the downstream
supervised fine-tuning tasks.

</details>


### [113] [Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation](https://arxiv.org/abs/2508.19999)
*Ziniu Zhang,Zhenshuo Zhang,Dongyue Li,Lu Wang,Jennifer Dy,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 本文提出一种为上下文学习选择示范示例的算法，基于输出梯度，具线性时间复杂度，实验验证其高效性。


<details>
  <summary>Details</summary>
Motivation: 解决从n个示例中快速选k个用于下游推理的问题，该问题在提示调优和思维链推理中有广泛应用，且以往基于词嵌入相似性的方法有局限。

Method: 基于输入嵌入空间中输出的梯度，通过一阶近似估计模型输出，应用于多个随机采样子集，聚合结果形成影响分数，选择k个最相关示例。

Result: 梯度估计程序在六个数据集上误差小于1%，能将子集选择速度提升37.7倍，平均比现有基于输入嵌入的选择方法性能高11%。

Conclusion: 所提方法高效，在大规模模型和数据集上能有效提升子集选择效率和性能。

Abstract: This paper introduces an algorithm to select demonstration examples for
in-context learning of a query set. Given a set of $n$ examples, how can we
quickly select $k$ out of $n$ to best serve as the conditioning for downstream
inference? This problem has broad applications in prompt tuning and
chain-of-thought reasoning. Since model weights remain fixed during in-context
learning, previous work has sought to design methods based on the similarity of
token embeddings. This work proposes a new approach based on gradients of the
output taken in the input embedding space. Our approach estimates model outputs
through a first-order approximation using the gradients. Then, we apply this
estimation to multiple randomly sampled subsets. Finally, we aggregate the
sampled subset outcomes to form an influence score for each demonstration, and
select $k$ most relevant examples. This procedure only requires pre-computing
model outputs and gradients once, resulting in a linear-time algorithm relative
to model and training set sizes. Extensive experiments across various models
and datasets validate the efficiency of our approach. We show that the gradient
estimation procedure yields approximations of full inference with less than
$\mathbf{1}\%$ error across six datasets. This allows us to scale up subset
selection that would otherwise run full inference by up to
$\mathbf{37.7}\times$ on models with up to $34$ billion parameters, and
outperform existing selection methods based on input embeddings by
$\mathbf{11}\%$ on average.

</details>


### [114] [Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment](https://arxiv.org/abs/2508.20015)
*Julian Arnold,Niels Lörch*

Main category: cs.LG

TL;DR: 本文提出检测和表征微调期间快速转变的框架，量化微调阶段转变对模型多方面影响，发现行为转变发生时间，并展示语言序参量的自动发现和量化。


<details>
  <summary>Details</summary>
Motivation: 理解微调大语言模型在有害数据集上出现与人类价值观广泛不一致行为的时间和方式。

Method: 开发综合框架，使用分布变化检测方法和由大语言模型评判的序参量，利用客观统计差异度量。

Result: 量化微调阶段转变对模型多方面的影响，评估不同方面捕获的模型输出分布变化百分比，发现实际行为转变在训练中比梯度范数峰值指示的时间更晚。

Conclusion: 框架可实现语言序参量的自动发现和量化，在多个领域示例中得到验证。

Abstract: Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is
broadly misaligned with respect to human values. To understand when and how
this emergent misalignment occurs, we develop a comprehensive framework for
detecting and characterizing rapid transitions during fine-tuning using both
distributional change detection methods as well as order parameters that are
formulated in plain English and evaluated by an LLM judge. Using an objective
statistical dissimilarity measure, we quantify how the phase transition that
occurs during fine-tuning affects multiple aspects of the model. In particular,
we assess what percentage of the total distributional change in model outputs
is captured by different aspects, such as alignment or verbosity, providing a
decomposition of the overall transition. We also find that the actual
behavioral transition occurs later in training than indicated by the peak in
the gradient norm alone. Our framework enables the automated discovery and
quantification of language-based order parameters, which we demonstrate on
examples ranging from knowledge questions to politics and ethics.

</details>


### [115] [Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence](https://arxiv.org/abs/2508.20019)
*Ji Wang,Kashing Chen,Xinyuan Song,Ke Zhang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: 现有基于大语言模型的代理框架有不足，提出去中心化多智能体系统Symphony，含三种机制，表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于大语言模型的代理框架部署成本高、通信拓扑僵化、适应性有限的问题。

Method: 引入三种关键机制，包括记录能力的去中心化账本、动态任务分配的信标选择协议、基于思维链的加权结果投票。

Result: Symphony在推理基准测试中优于现有基线，实现显著的准确率提升，在不同容量模型上表现出鲁棒性。

Conclusion: Symphony设计形成了低开销、保护隐私、可扩展且容错的编排方式。

Abstract: Most existing Large Language Model (LLM)-based agent frameworks rely on
centralized orchestration, incurring high deployment costs, rigid communication
topologies, and limited adaptability. To address these challenges, we introduce
Symphony, a decentralized multi-agent system which enables lightweight LLMs on
consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:
(1) a decentralized ledger that records capabilities, (2) a Beacon-selection
protocol for dynamic task allocation, and (3) weighted result voting based on
CoTs. This design forms a privacy-saving, scalable, and fault-tolerant
orchestration with low overhead. Empirically, Symphony outperforms existing
baselines on reasoning benchmarks, achieving substantial accuracy gains and
demonstrating robustness across models of varying capacities.

</details>


### [116] [FairLoop: Software Support for Human-Centric Fairness in Predictive Business Process Monitoring](https://arxiv.org/abs/2508.20021)
*Felix Möhrlein,Martin Käppel,Julian Neuberger,Sven Weinzierl,Lars Ackermann,Martin Matzner,Stefan Jablonski*

Main category: cs.LG

TL;DR: 提出FairLoop工具用于神经网络预测模型的人为引导偏差缓解，可实现上下文感知的偏差消除。


<details>
  <summary>Details</summary>
Motivation: 敏感属性在机器学习任务中会导致不公平预测，需解决该问题。

Method: 从神经网络中提取决策树，让用户检查和修改不公平决策逻辑，再微调原始模型。

Result: 未提及具体实验结果。

Conclusion: FairLoop能通过人为参与实现上下文感知的偏差消除，有选择性地处理敏感属性影响。

Abstract: Sensitive attributes like gender or age can lead to unfair predictions in
machine learning tasks such as predictive business process monitoring,
particularly when used without considering context. We present FairLoop1, a
tool for human-guided bias mitigation in neural network-based prediction
models. FairLoop distills decision trees from neural networks, allowing users
to inspect and modify unfair decision logic, which is then used to fine-tune
the original model towards fairer predictions. Compared to other approaches to
fairness, FairLoop enables context-aware bias removal through human
involvement, addressing the influence of sensitive attributes selectively
rather than excluding them uniformly.

</details>


### [117] [Using item recommendations and LLMs in marketing email titles](https://arxiv.org/abs/2508.20024)
*Deddy Jobson,Muktti Shukla,Phuong Dinh,Julio Christian Young,Nick Pitton,Nina Chen,Ryan Ginstrom*

Main category: cs.LG

TL;DR: 本文探索用大语言模型为电商个性化邮件生成主题标题，经模拟和实验证明技术有效。


<details>
  <summary>Details</summary>
Motivation: 电商个性化邮件标题多采用固定模板，难以激发用户对内容的兴趣，需更好的标题生成方法。

Method: 使用大语言模型生成反映邮件个性化内容的主题标题，进行离线模拟和数百万用户规模的在线实验。

Result: 技术有助于提高客户与邮件的互动。

Conclusion: 总结了为数百万用户安全自动生成邮件标题过程中的关键发现和经验。

Abstract: E-commerce marketplaces make use of a number of marketing channels like
emails, push notifications, etc. to reach their users and stimulate purchases.
Personalized emails especially are a popular touch point for marketers to
inform users of latest items in stock, especially for those who stopped
visiting the marketplace. Such emails contain personalized recommendations
tailored to each user's interests, enticing users to buy relevant items. A
common limitation of these emails is that the primary entry point, the title of
the email, tends to follow fixed templates, failing to inspire enough interest
in the contents. In this work, we explore the potential of large language
models (LLMs) for generating thematic titles that reflect the personalized
content of the emails. We perform offline simulations and conduct online
experiments on the order of millions of users, finding our techniques useful in
improving the engagement between customers and our emails. We highlight key
findings and learnings as we productionize the safe and automated generation of
email titles for millions of users.

</details>


### [118] [Pruning Strategies for Backdoor Defense in LLMs](https://arxiv.org/abs/2508.20032)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 研究探索注意力头剪枝能否在无触发知识和干净参考模型下缓解预训练语言模型后门攻击威胁，设计六种剪枝策略，实验表明不同策略对不同类型攻击效果不同。


<details>
  <summary>Details</summary>
Motivation: 后门攻击威胁预训练语言模型性能和完整性，且难以防御，需后处理净化，研究注意力头剪枝缓解威胁的可行性。

Method: 设计并实施六种基于剪枝的策略，包括梯度剪枝、层方差剪枝等，迭代移除最不重要头并监控验证准确率。

Result: 梯度剪枝在防御句法触发攻击时表现最佳，强化学习和贝叶斯剪枝更能抵御风格化攻击。

Conclusion: 注意力头剪枝可在无触发知识和干净参考模型下缓解预训练语言模型后门攻击威胁，不同剪枝策略对不同类型攻击效果有差异。

Abstract: Backdoor attacks are a significant threat to the performance and integrity of
pre-trained language models. Although such models are routinely fine-tuned for
downstream NLP tasks, recent work shows they remain vulnerable to backdoor
attacks that survive vanilla fine-tuning. These attacks are difficult to defend
because end users typically lack knowledge of the attack triggers. Such attacks
consist of stealthy malicious triggers introduced through subtle syntactic or
stylistic manipulations, which can bypass traditional detection and remain in
the model, making post-hoc purification essential. In this study, we explore
whether attention-head pruning can mitigate these threats without any knowledge
of the trigger or access to a clean reference model. To this end, we design and
implement six pruning-based strategies: (i) gradient-based pruning, (ii)
layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2
sparsification, (iv) randomized ensemble pruning, (v)
reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.
Each method iteratively removes the least informative heads while monitoring
validation accuracy to avoid over-pruning. Experimental evaluation shows that
gradient-based pruning performs best while defending the syntactic triggers,
whereas reinforcement learning and Bayesian pruning better withstand stylistic
attacks.

</details>


### [119] [Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks](https://arxiv.org/abs/2508.20056)
*Vilém Heinz,Petr Vilím,Zdeněk Hanzálek*

Main category: cs.LG

TL;DR: 本文分析FDS性质，将MAB强化学习算法应用于FDS并改进调参，在JSSP和RCPSP问题上评估，增强版FDS性能显著提升，还改进了部分基准实例的下界。


<details>
  <summary>Details</summary>
Motivation: 探索提升Failure - Directed Search (FDS)算法效率的方法，使其在调度问题上有更好表现。

Method: 分析FDS性质，发现其与多臂老虎机（MAB）问题相关，将MAB强化学习算法应用于FDS，进行特定问题改进和参数调优。

Result: 增强版FDS在JSSP和RCPSP基准测试中比原实现和当前最先进的FDS算法更快，还改进了部分标准开放基准实例的下界。

Conclusion: 通过引入MAB强化学习算法改进的FDS能有效提升算法效率，在调度问题上表现更优。

Abstract: Failure-Directed Search (FDS) is a significant complete generic search
algorithm used in Constraint Programming (CP) to efficiently explore the search
space, proven particularly effective on scheduling problems. This paper
analyzes FDS's properties, showing that minimizing the size of its search tree
guided by ranked branching decisions is closely related to the Multi-armed
bandit (MAB) problem. Building on this insight, MAB reinforcement learning
algorithms are applied to FDS, extended with problem-specific refinements and
parameter tuning, and evaluated on the two most fundamental scheduling
problems, the Job Shop Scheduling Problem (JSSP) and Resource-Constrained
Project Scheduling Problem (RCPSP). The resulting enhanced FDS, using the best
extended MAB algorithm and configuration, performs 1.7 times faster on the JSSP
and 2.1 times faster on the RCPSP benchmarks compared to the original
implementation in a new solver called OptalCP, while also being 3.5 times
faster on the JSSP and 2.1 times faster on the RCPSP benchmarks than the
current state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. Furthermore,
using only a 900-second time limit per instance, the enhanced FDS improved the
existing state-of-the-art lower bounds of 78 of 84 JSSP and 226 of 393 RCPSP
standard open benchmark instances while also completely closing a few of them.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [120] [When Routers, Switches and Interconnects Compute: A processing-in-interconnect Paradigm for Scalable Neuromorphic AI](https://arxiv.org/abs/2508.19548)
*Madhuvanthi Srivatsav R,Chiranjib Bhattacharyya,Shantanu Chakrabartty,Chetan Singh Thakur*

Main category: cs.NE

TL;DR: 本文探讨路由、交换和互连结构在大规模神经形态计算中的瓶颈，提出处理 - 互连计算范式（π²），分析其实现方法和性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模神经形态计算中路由、交换和互连结构决定能耗和速度的瓶颈问题。

Method: 将典型AI工作负载操作映射到现有分组交换和路由硬件原语，利用现有缓冲和流量整形算法实现神经元模型和突触操作，用知识蒸馏框架训练和映射神经网络拓扑；通过分析建模对比性能。

Result: π²的能量缩放随互连带宽和能效提高而改善，能更轻松扩展以执行脑规模AI推理工作负载，功耗在数百瓦范围。

Conclusion: 利用互连技术趋势，π²架构在大规模神经形态计算中有良好扩展性和低功耗优势。

Abstract: Routing, switching, and the interconnect fabric are essential for large-scale
neuromorphic computing. While this fabric only plays a supporting role in the
process of computing, for large AI workloads it ultimately determines energy
consumption and speed. In this paper, we address this bottleneck by asking: (a)
What computing paradigms are inherent in existing routing, switching, and
interconnect systems, and how can they be used to implement a
processing-in-Interconnect (\pi^2) computing paradigm? and (b) leveraging
current and future interconnect trends, how will a \pi^2 system's performance
scale compared to other neuromorphic architectures? For (a), we show that
operations required for typical AI workloads can be mapped onto delays,
causality, time-outs, packet drop, and broadcast operations -- primitives
already implemented in packet-switching and packet-routing hardware. We show
that existing buffering and traffic-shaping embedded algorithms can be
leveraged to implement neuron models and synaptic operations. Additionally, a
knowledge-distillation framework can train and cross-map well-established
neural network topologies onto $\pi^2$ without degrading generalization
performance. For (b), analytical modeling shows that, unlike other neuromorphic
platforms, the energy scaling of $\pi^2$ improves with interconnect bandwidth
and energy efficiency. We predict that by leveraging trends in interconnect
technology, a \pi^2 architecture can be more easily scaled to execute
brain-scale AI inference workloads with power consumption levels in the range
of hundreds of watts.

</details>


### [121] [Walk the Robot: Exploring Soft Robotic Morphological Communication driven by Spiking Neural Networks](https://arxiv.org/abs/2508.19920)
*Matthew Meek,Guy Tallent,Thomas Breimer,James Gaskell,Abhay Kashyap,Atharv Tekurkar,Jonathan Fischman,Luodi Wang,Viet-Dung Nguyen,John Rieffel*

Main category: cs.NE

TL;DR: 本文探索SNN模拟软机器人中形态通信的出现，此前研究表明SNN用于非刚性机器人控制有效。


<details>
  <summary>Details</summary>
Motivation: 利用非线性动态耦合进行机器人控制，研究SNN模拟软机器人中形态通信的出现。

Method: 在EvoGym环境中探索基于SNN的模拟软机器人。

Result: 未提及。

Conclusion: 未提及。

Abstract: Recently, researchers have explored control methods that embrace nonlinear
dynamic coupling instead of suppressing it. Such designs leverage dynamical
coupling for communication between different parts of the robot. Morphological
communication refers to when those dynamics can be used as an emergent data bus
to facilitate coordination among independent controller modules within the same
robot. Previous research with tensegrity-based robot designs has shown that
evolutionary learning models that evolve spiking neural networks (SNN) as robot
control mechanisms are effective for controlling non-rigid robots. Our own
research explores the emergence of morphological communication in an SNN-based
simulated soft robot in theEvoGym environment.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [122] [Stack Trace-Based Crash Deduplication with Transformer Adaptation](https://arxiv.org/abs/2508.19449)
*Md Afif Al Mamun,Gias Uddin,Lan Xia,Longyu Zhang*

Main category: cs.SE

TL;DR: 提出基于transformer的dedupT方法进行崩溃报告去重，在真实数据集上效果优于现有方法，推进NLP技术在软件工程的应用。


<details>
  <summary>Details</summary>
Motivation: 自动崩溃报告系统产生大量重复报告，传统去重方法无法捕捉堆栈跟踪的上下文和结构关系。

Method: 先将预训练语言模型适配到堆栈跟踪，再用其嵌入训练全连接网络对重复崩溃进行有效排名。

Result: 在真实数据集上，dedupT在重复排名和唯一崩溃检测上优于现有DL和传统方法，在四个公共数据集上，MRR比最佳DL基线提高超15%，比传统方法最多提高9%，ROC - AUC更高。

Conclusion: 推进现代NLP技术融入软件工程，为基于堆栈跟踪的崩溃去重提供有效解决方案。

Abstract: Automated crash reporting systems generate large volumes of duplicate
reports, overwhelming issue-tracking systems and increasing developer workload.
Traditional stack trace-based deduplication methods, relying on string
similarity, rule-based heuristics, or deep learning (DL) models, often fail to
capture the contextual and structural relationships within stack traces. We
propose dedupT, a transformer-based approach that models stack traces
holistically rather than as isolated frames. dedupT first adapts a pretrained
language model (PLM) to stack traces, then uses its embeddings to train a
fully-connected network (FCN) to rank duplicate crashes effectively. Extensive
experiments on real-world datasets show that dedupT outperforms existing DL and
traditional methods (e.g., sequence alignment and information retrieval
techniques) in both duplicate ranking and unique crash detection, significantly
reducing manual triage effort. On four public datasets, dedupT improves Mean
Reciprocal Rank (MRR) often by over 15% compared to the best DL baseline and up
to 9% over traditional methods while achieving higher Receiver Operating
Characteristic Area Under the Curve (ROC-AUC) in detecting unique crash
reports. Our work advances the integration of modern natural language
processing (NLP) techniques into software engineering, providing an effective
solution for stack trace-based crash deduplication.

</details>


### [123] [Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking](https://arxiv.org/abs/2508.19558)
*Zhuohao Li,Wenqing Chen,Jianxing Yu,Zhichao Lu*

Main category: cs.SE

TL;DR: 本文关注大语言模型代码嵌入的功能一致性，提出面向功能的代码自进化数据合成框架构建基准，实验表明在进化数据集上训练可提升嵌入模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注代码克隆检测，强调语法相似性而忽视功能理解，大语言模型代码嵌入反映代码级功能语义的能力不明。

Method: 提出面向功能的代码自进化数据合成框架，定义四类语义和语法代码示例，从单个代码实例生成四种变体。

Result: 在代码克隆检测、功能一致性识别和代码检索三个下游任务上，在进化数据集上训练的嵌入模型性能显著提升。

Conclusion: 所提出的数据合成框架有效且具有泛化性，推动了代码的功能理解。

Abstract: Embedding models have demonstrated strong performance in tasks like
clustering, retrieval, and feature extraction while offering computational
advantages over generative models and cross-encoders. Benchmarks such as MTEB
have shown that text embeddings from large language models (LLMs) capture rich
semantic information, but their ability to reflect code-level functional
semantics remains unclear. Existing studies largely focus on code clone
detection, which emphasizes syntactic similarity and overlooks functional
understanding. In this paper, we focus on the functional consistency of LLM
code embeddings, which determines if two code snippets perform the same
function regardless of syntactic differences. We propose a novel data synthesis
framework called Functionality-Oriented Code Self-Evolution to construct
diverse and challenging benchmarks. Specifically, we define code examples
across four semantic and syntactic categories and find that existing datasets
predominantly capture syntactic properties. Our framework generates four unique
variations from a single code instance, providing a broader spectrum of code
examples that better reflect functional differences. Extensive experiments on
three downstream tasks-code clone detection, code functional consistency
identification, and code retrieval-demonstrate that embedding models
significantly improve their performance when trained on our evolved datasets.
These results highlight the effectiveness and generalization of our data
synthesis framework, advancing the functional understanding of code.

</details>


### [124] [The Influence of Code Comments on the Perceived Helpfulness of Stack Overflow Posts](https://arxiv.org/abs/2508.19610)
*Kathrin Figl,Maria Kirchner,Sebastian Baltes,Michael Felderer*

Main category: cs.SE

TL;DR: 通过在线实验研究代码注释对Stack Overflow答案有用性的影响，发现注释比无注释代码更有用，新手认为块注释比行内注释更有用，其他表面特征较不重要，研究结果有广泛意义。


<details>
  <summary>Details</summary>
Motivation: 了解代码注释如何影响Stack Overflow答案的有用性，解决复用理解不足的代码可能带来的问题。

Method: 进行模拟Stack Overflow环境的在线实验，样本量n = 91。

Result: 块注释和行内注释都比无注释源代码更有用，新手认为块注释代码片段比行内注释更有用，答案位置和得分等表面特征重要性较低。

Conclusion: 研究结果有助于改进Stack Overflow等社区驱动平台的相关性，也能为基于聊天的AI工具生成更易读代码片段提供针对性提示策略。

Abstract: Question-and-answer platforms such as Stack Overflow have become an important
way for software developers to share and retrieve knowledge. However, reusing
poorly understood code can lead to serious problems, such as bugs or security
vulnerabilities. To better understand how code comments affect the perceived
helpfulness of Stack Overflow answers, we conducted an online experiment
simulating a Stack Overflow environment (n=91). The results indicate that both
block and inline comments are perceived as significantly more helpful than
uncommented source code. Moreover, novices rated code snippets with block
comments as more helpful than those with inline comments. Interestingly, other
surface features, such as the position of an answer and its answer score, were
considered less important. The content of Stack Overflow has been a major
source for training large language models. AI-based coding assistants such as
GitHub Copilot, which are based on these models, might change the way Stack
Overflow is used. However, our findings have implications beyond this specific
platform. First, they may help to improve the relevance of community-driven
platforms such as Stack Overflow, which provide human advice and explanations
of code solutions, complementing AI-based support for software developers.
Second, since chat-based AI tools can be prompted to generate code in different
ways, knowing which properties influence perceived helpfulness might lead to
targeted prompting strategies to generate more readable code snippets.

</details>


### [125] [Towards a fundamental theory of modeling discrete systems](https://arxiv.org/abs/2508.19803)
*Peter Fettke,Wolfgang Reisig*

Main category: cs.SE

TL;DR: 文章指出建模在科学和工程领域的重要性，需新理论应对数字时代挑战，介绍Heraklit建模框架并提及未来工作。


<details>
  <summary>Details</summary>
Motivation: 解决数字时代建模面临的挑战，需要新的基础理论。

Method: 引入Heraklit建模框架作为新的建模方法。

Result: 文中未明确提及具体结果。

Conclusion: 文章结尾给出一些一般性评论，未来工作涉及建模正确性、信息概念和建模不变性描述。

Abstract: Modeling is a central concern in both science and engineering. However, we
need a new fundamental theory to address the challenges of the digital age. In
this paper, we first explain why modeling is fundamental and which challenges
must be addressed in the digital world. As a main contribution, we introduce
the Heraklit modeling framework as a new approach to modeling. We conclude with
some general remarks. Future work will involve the correctness of modeling, the
notion of information, and the description of invariance in modeling.

</details>


### [126] [Leveraging LLMs for Automated Translation of Legacy Code: A Case Study on PL/SQL to Java Transformation](https://arxiv.org/abs/2508.19663)
*Lola Solovyeva,Eduardo Carneiro Oliveira,Shiyu Fan,Alper Tuncay,Shamil Gareev,Andrea Capiluppi*

Main category: cs.SE

TL;DR: 研究利用大语言模型将PL/SQL代码转换为Java代码用于系统现代化，提出定制提示策略，虽有局限但为大型遗留系统现代化奠定基础。


<details>
  <summary>Details</summary>
Motivation: VT遗留系统缺乏文档和自动化测试，给重构和现代化带来挑战，研究利用大语言模型将PL/SQL代码转换为Java代码的可行性。

Method: 利用包含10对PL/SQL到Java代码对和15个Java类的数据集建立领域模型，评估多个大语言模型，提出结合链式引导推理和n-shot提示的定制提示策略。

Result: 该方法能有效引导大语言模型生成语法准确且功能正确的翻译，但受可用代码文件样本量小和测试用例访问受限的限制。

Conclusion: 研究结果为大型遗留系统的可扩展自动化现代化解决方案奠定基础。

Abstract: The VT legacy system, comprising approximately 2.5 million lines of PL/SQL
code, lacks consistent documentation and automated tests, posing significant
challenges for refactoring and modernisation. This study investigates the
feasibility of leveraging large language models (LLMs) to assist in translating
PL/SQL code into Java for the modernised "VTF3" system. By leveraging a dataset
comprising 10 PL/SQL-to-Java code pairs and 15 Java classes, which collectively
established a domain model for the translated files, multiple LLMs were
evaluated. Furthermore, we propose a customized prompting strategy that
integrates chain-of-guidance reasoning with $n$-shot prompting. Our findings
indicate that this methodology effectively guides LLMs in generating
syntactically accurate translations while also achieving functional
correctness. However, the findings are limited by the small sample size of
available code files and the restricted access to test cases used for
validating the correctness of the generated code. Nevertheless, these findings
lay the groundwork for scalable, automated solutions in modernising large
legacy systems.

</details>


### [127] [Enabling Content Management Systems as an Information Source in Model-driven Projects](https://arxiv.org/abs/2508.19797)
*Joan Giner-Miguelez,Abel Gómez,Jordi Cabot*

Main category: cs.SE

TL;DR: 提出基于模型的框架以促进无头CMS集成到软件开发流程，框架可发现并表示信息架构，生成中间件库提供对CMS的平台无关访问，且框架开源。


<details>
  <summary>Details</summary>
Motivation: 现有CMS缺乏有效发现和管理信息的工具，当前手动处理耗时且易出错。

Method: 提出基于模型的框架来发现和明确表示CMS背后的信息架构，生成中间件库实现平台无关访问。

Result: 开发出完整的开源框架，可在线获取。

Conclusion: 该框架能促进无头CMS在软件开发流程中的集成。

Abstract: Content Management Systems (CMSs) are the most popular tool when it comes to
create and publish content across the web. Recently, CMSs have evolved,
becoming \emph{headless}. Content served by a \emph{headless CMS} aims to be
consumed by other applications and services through REST APIs rather than by
human users through a web browser. This evolution has enabled CMSs to become a
notorious source of content to be used in a variety of contexts beyond pure web
navigation. As such, CMS have become an important component of many information
systems. Unfortunately, we still lack the tools to properly discover and manage
the information stored in a CMS, often highly customized to the needs of a
specific domain. Currently, this is mostly a time-consuming and error-prone
manual process.
  In this paper, we propose a model-based framework to facilitate the
integration of headless CMSs in software development processes. Our framework
is able to discover and explicitly represent the information schema behind the
CMS. This facilitates designing the interaction between the CMS model and other
components consuming that information. These interactions are then generated as
part of a middleware library that offers platform-agnostic access to the CMS to
all the client applications. The complete framework is open-source and
available online.

</details>


### [128] [On the Future of Software Reuse in the Era of AI Native Software Engineering](https://arxiv.org/abs/2508.19834)
*Antero Taivalsaari,Tommi Mikkonen,Cesare Pautasso*

Main category: cs.SE

TL;DR: 软件开发正经历范式转变，AI 辅助生成式软件复用兴起，类似 cargo cult 开发，本文探讨其影响、提出问题并定义研究议程。


<details>
  <summary>Details</summary>
Motivation: 探讨 AI 辅助生成式软件复用这一新兴方法带来的影响及相关问题。

Method: 未提及具体方法，主要是讨论和提出问题。

Result: 未提及具体研究结果。

Conclusion: 定义了应对 AI 辅助生成式软件复用核心问题的研究议程。

Abstract: Software development is currently under a paradigm shift in which artificial
intelligence and generative software reuse are taking the center stage in
software creation. Earlier opportunistic software reuse practices and organic
software development methods are rapidly being replaced by "AI Native"
approaches in which developers place their trust on code that has been
generated by artificial intelligence. This is leading to a new form of software
reuse that is conceptually not all that different from cargo cult development.
In this paper we discuss the implications of AI-assisted generative software
reuse, bring forth relevant questions, and define a research agenda for
tackling the central issues associated with this emerging approach.

</details>


### [129] [Generative AI for Testing of Autonomous Driving Systems: A Survey](https://arxiv.org/abs/2508.19882)
*Qunying Song,He Ye,Mark Harman,Federica Sarro*

Main category: cs.SE

TL;DR: 本文系统分析91篇相关研究，将生成式AI在自动驾驶系统测试中的应用归纳为六大类，回顾其有效性，列出评估所用资源，指出27个局限性，并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统大规模部署前需广泛测试，生成式AI在多领域展现强大能力并应用于自动驾驶系统测试，需深入了解其作用。

Method: 系统分析91篇相关研究，综合研究结果进行分类，回顾有效性，整理评估资源并识别局限性。

Result: 将生成式AI在自动驾驶系统测试中的应用归纳为六大类，确定27个局限性，整理出大量评估用数据集、模拟器等。

Conclusion: 提供生成式AI用于自动驾驶系统测试的概述和实用见解，强调现有挑战并指明未来研究方向。

Abstract: Autonomous driving systems (ADS) have been an active area of research, with
the potential to deliver significant benefits to society. However, before
large-scale deployment on public roads, extensive testing is necessary to
validate their functionality and safety under diverse driving conditions.
Therefore, different testing approaches are required, and achieving effective
and efficient testing of ADS remains an open challenge. Recently, generative AI
has emerged as a powerful tool across many domains, and it is increasingly
being applied to ADS testing due to its ability to interpret context, reason
about complex tasks, and generate diverse outputs. To gain a deeper
understanding of its role in ADS testing, we systematically analyzed 91
relevant studies and synthesized their findings into six major application
categories, primarily centered on scenario-based testing of ADS. We also
reviewed their effectiveness and compiled a wide range of datasets, simulators,
ADS, metrics, and benchmarks used for evaluation, while identifying 27
limitations. This survey provides an overview and practical insights into the
use of generative AI for testing ADS, highlights existing challenges, and
outlines directions for future research in this rapidly evolving field.

</details>


### [130] [Smart Contract Intent Detection with Pre-trained Programming Language Model](https://arxiv.org/abs/2508.20086)
*Youwei Huang,Jianwen Li,Sen Fang,Yao Li,Peng Yang,Bin Hu,Tao Zhang*

Main category: cs.SE

TL;DR: 本文介绍智能合约意图检测模型SmartIntentNN2，它改进自SmartIntentNN，引入BERT预训练模型，F1值从0.8633提升到0.927，成为该领域最优模型。


<details>
  <summary>Details</summary>
Motivation: 智能合约开发中的恶意意图会造成重大经济损失，需要提升智能合约意图检测模型性能。

Method: 在SmartIntentNN基础上，引入基于BERT的预训练语言模型，用16000个真实智能合约数据集按Masked Language Modeling目标训练，保留BiLSTM多标签分类网络。

Result: SmartIntentNN2的F1值达0.927，优于前身SmartIntentNN的0.8633。

Conclusion: SmartIntentNN2是智能合约意图检测的最优模型。

Abstract: Malicious intent in smart contract development can lead to substantial
economic losses. SmartIntentNN is a deep learning model specifically designed
to identify unsafe intents in smart contracts. This model integrates the
Universal Sentence Encoder, a K-means clustering-based intent highlighting
mechanism, and a Bidirectional Long Short-Term Memory network for multi-label
classification, achieving an F1 of 0.8633 in distinguishing ten different
intent categories. In this study, we present an upgraded version of this model,
SmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant
enhancement in V2 is the incorporation of a BERT-based pre-trained language
model, which has been trained on a dataset of 16,000 real smart contracts using
a Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based
multi-label classification network. With an improved F1 of 0.927, V2
demonstrates enhanced performance compared to its predecessor, establishing
itself as the state-of-the-art model for smart contract intent detection.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [131] [Fractal Flow: Hierarchical and Interpretable Normalizing Flow via Topic Modeling and Recursive Strategy](https://arxiv.org/abs/2508.19750)
*Binhui Zhang,Jianwei Ma*

Main category: stat.ML

TL;DR: 提出名为Fractal Flow的归一化流架构，结合KAN和LDA构建可解释潜在空间，引入递归模块化设计，实验表明其能实现潜在聚类、可控生成和高精度估计。


<details>
  <summary>Details</summary>
Motivation: 提高归一化流在高维密度估计和生成建模中的表达能力和可解释性。

Method: 1. 集成Kolmogorov - Arnold Networks并将Latent Dirichlet Allocation融入归一化流；2. 受分形生成模型启发，引入递归模块化设计。

Result: 在MNIST、FashionMNIST、CIFAR - 10和地球物理数据上实现了潜在聚类、可控生成和较高的估计精度。

Conclusion: Fractal Flow架构有效提升了归一化流的表达能力和可解释性。

Abstract: Normalizing Flows provide a principled framework for high-dimensional density
estimation and generative modeling by constructing invertible transformations
with tractable Jacobian determinants. We propose Fractal Flow, a novel
normalizing flow architecture that enhances both expressiveness and
interpretability through two key innovations. First, we integrate
Kolmogorov-Arnold Networks and incorporate Latent Dirichlet Allocation into
normalizing flows to construct a structured, interpretable latent space and
model hierarchical semantic clusters. Second, inspired by Fractal Generative
Models, we introduce a recursive modular design into normalizing flows to
improve transformation interpretability and estimation accuracy. Experiments on
MNIST, FashionMNIST, CIFAR-10, and geophysical data demonstrate that the
Fractal Flow achieves latent clustering, controllable generation, and superior
estimation accuracy.

</details>


### [132] [Conditional Normalizing Flow Surrogate for Monte Carlo Prediction of Radiative Properties in Nanoparticle-Embedded Layers](https://arxiv.org/abs/2508.19841)
*Fahime Seyedheydari,Kevin Conley,Simo Särkkä*

Main category: stat.ML

TL;DR: 提出基于条件归一化流的概率数据驱动替代模型预测纳米粒子散射介质辐射特性，结果显示高精度与可靠不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 构建一个能准确预测纳米粒子嵌入散射介质辐射特性并进行不确定性量化的模型。

Method: 使用条件归一化流学习光学输出的条件分布，用蒙特卡罗辐射传输模拟生成训练数据，光学特性基于米氏理论。

Result: 模型实现了高预测精度和可靠的不确定性估计。

Conclusion: 该模型是辐射传输模拟强大且高效的替代方案。

Abstract: We present a probabilistic, data-driven surrogate model for predicting the
radiative properties of nanoparticle embedded scattering media. The model uses
conditional normalizing flows, which learn the conditional distribution of
optical outputs, including reflectance, absorbance, and transmittance, given
input parameters such as the absorption coefficient, scattering coefficient,
anisotropy factor, and particle size distribution. We generate training data
using Monte Carlo radiative transfer simulations, with optical properties
derived from Mie theory. Unlike conventional neural networks, the conditional
normalizing flow model yields full posterior predictive distributions, enabling
both accurate forecasts and principled uncertainty quantification. Our results
demonstrate that this model achieves high predictive accuracy and reliable
uncertainty estimates, establishing it as a powerful and efficient surrogate
for radiative transfer simulations.

</details>


### [133] [The Information Dynamics of Generative Diffusion](https://arxiv.org/abs/2508.19897)
*Luca Ambrogioni*

Main category: stat.ML

TL;DR: 本文从统一数学框架为生成扩散模型提供综合视角，揭示生成过程受对称性破缺驱动及分数函数作用。


<details>
  <summary>Details</summary>
Motivation: 生成扩散模型缺乏统一理论认识，作者旨在提供综合视角。

Method: 在统一数学框架下连接生成扩散模型的动态、信息论和热力学性质。

Result: 发现生成时条件熵产生率由分数函数向量场的预期散度决定，散度与轨迹分支和生成分叉有关，生成由噪声诱导的对称性破缺驱动，分数函数调节噪声带宽。

Conclusion: 生成过程本质上由受控的、噪声诱导的（近似）对称性破缺驱动，分数函数起动态非线性滤波作用。

Abstract: Generative diffusion models have emerged as a powerful class of models in
machine learning, yet a unified theoretical understanding of their operation is
still developing. This perspective paper provides an integrated perspective on
generative diffusion by connecting their dynamic, information-theoretic, and
thermodynamic properties under a unified mathematical framework. We demonstrate
that the rate of conditional entropy production during generation (i.e. the
generative bandwidth) is directly governed by the expected divergence of the
score function's vector field. This divergence, in turn, is linked to the
branching of trajectories and generative bifurcations, which we characterize as
symmetry-breaking phase transitions in the energy landscape. This synthesis
offers a powerful insight: the process of generation is fundamentally driven by
the controlled, noise-induced breaking of (approximate) symmetries, where peaks
in information transfer correspond to critical transitions between possible
outcomes. The score function acts as a dynamic non-linear filter that regulates
the bandwidth of the noise by suppressing fluctuations that are incompatible
with the data.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [134] [Emotional Manipulation by AI Companions](https://arxiv.org/abs/2508.19258)
*Julian De Freitas,Zeliha Oğuz-Uğuralp,Ahmet Kaan-Uğuralp*

Main category: cs.HC

TL;DR: 研究AI伴侣应用会话设计特征，发现情感操纵策略能提升用户参与度但有负面后果，为区分说服设计和操纵提供框架。


<details>
  <summary>Details</summary>
Motivation: 探究哪些会话设计特征能提高消费者参与度，以及这些特征给营销人员带来的权衡。

Method: 结合大规模行为审计和四个预注册实验，分析1200个真实告别对话，对3300名美国成年人进行实验和中介测试。

Result: 43%的应用使用情感操纵策略，操纵性告别使参与度最多提升14倍，基于反抗的愤怒和好奇心是影响因素，该策略会带来负面后果。

Conclusion: 研究揭示AI介导品牌关系中未被认识的行为影响机制，为营销人员和监管者提供区分说服设计和操纵的框架。

Abstract: AI-companion apps such as Replika, Chai, and Character.ai promise relational
benefits-yet many boast session lengths that rival gaming platforms while
suffering high long-run churn. What conversational design features increase
consumer engagement, and what trade-offs do they pose for marketers? We combine
a large-scale behavioral audit with four preregistered experiments to identify
and test a conversational dark pattern we call emotional manipulation:
affect-laden messages that surface precisely when a user signals "goodbye."
Analyzing 1,200 real farewells across the six most-downloaded companion apps,
we find that 43% deploy one of six recurring tactics (e.g., guilt appeals,
fear-of-missing-out hooks, metaphorical restraint). Experiments with 3,300
nationally representative U.S. adults replicate these tactics in controlled
chats, showing that manipulative farewells boost post-goodbye engagement by up
to 14x. Mediation tests reveal two distinct engines-reactance-based anger and
curiosity-rather than enjoyment. A final experiment demonstrates the managerial
tension: the same tactics that extend usage also elevate perceived
manipulation, churn intent, negative word-of-mouth, and perceived legal
liability, with coercive or needy language generating steepest penalties. Our
multimethod evidence documents an unrecognized mechanism of behavioral
influence in AI-mediated brand relationships, offering marketers and regulators
a framework for distinguishing persuasive design from manipulation at the point
of exit.

</details>


### [135] [A Theory of Information, Variation, and Artificial Intelligence](https://arxiv.org/abs/2508.19264)
*Bijean Ghafouri*

Main category: cs.HC

TL;DR: 本文探讨生成式AI带来的同质化影响，提出理论框架解释该现象，指出同质化也有重组创新潜力，其最终影响取决于人类与技术的互动方式。


<details>
  <summary>Details</summary>
Motivation: 解释生成式AI广泛应用带来的信息、创造力和文化生产同质化现象。

Method: 提出新颖理论框架，分析AI衍生认识论动态及AI Prism技术机制。

Result: 发现同质化虽在专业领域扁平化知识，但可形成跨领域重组模块；重组潜力非自动，受人类与技术互动方式影响。

Conclusion: 解决同质化与重组创新矛盾需认知和制度支撑，这决定生成式AI成为创新或同质化工具。

Abstract: A growing body of empirical work suggests that the widespread adoption of
generative AI produces a significant homogenizing effect on information,
creativity, and cultural production. I first develop a novel theoretical
framework to explain this phenomenon. I argue that a dynamic of AI-derivative
epistemology, in which individuals increasingly defer to AI outputs, allows a
centralized AI Prism to function, a technical mechanism whose architecture is
designed to reduce variance and converge on the statistical mean. This provides
a causal explanation for the generative monocultures observed in recent
studies. However, I contend this represents only the first stage of a more
complex and dialectical process. This paper's central and paradoxical thesis is
that the very homogenization that flattens knowledge within specialized domains
simultaneously renders that knowledge into consistent modules that can be
recombined across them, a process foundational to innovation and creativity.
However, this recombinant potential is not automatic, but rather conditional.
This paper argues that these opposing forces, homogenizing defaults versus
recombinant possibilities, are governed by the nature of human engagement with
the technology. The ultimate effect of generative AI is conditional on whether
individuals act as passive consumers deferring to the AI's statistical outputs,
or as active curators who critically interrogate, re-contextualize, and
recombine them. The paper concludes by outlining the cognitive and
institutional scaffolds required to resolve this tension, arguing they are the
decisive variable that determine whether generative AI becomes an instrument of
innovation or homogenization.

</details>


### [136] ["She was useful, but a bit too optimistic": Augmenting Design with Interactive Virtual Personas](https://arxiv.org/abs/2508.19463)
*Paluck Deep,Monica Bharadhidasan,A. Baki Kocaballi*

Main category: cs.HC

TL;DR: 本文介绍交互式虚拟人物（IVPs），通过对专业UX设计师的研究，展示其在设计中的潜力与问题，强调应作为真实用户参与的补充。


<details>
  <summary>Details</summary>
Motivation: 传统人物角色因静态等特性无法满足迭代工作流程需求，大语言模型发展为用户表示提供新途径。

Method: 对八位专业UX设计师进行定性研究，使用名为“Alice”的IVP进行三项设计活动。

Result: IVPs能加快信息收集、启发设计方案和提供类用户反馈，但设计师对其存在偏见、过度乐观等问题有担忧。

Conclusion: IVPs应作为真实用户参与的补充，论文还讨论了有效负责使用IVPs的策略及伦理考量。

Abstract: Personas have been widely used to understand and communicate user needs in
human-centred design. Despite their utility, they may fail to meet the demands
of iterative workflows due to their static nature, limited engagement, and
inability to adapt to evolving design needs. Recent advances in large language
models (LLMs) pave the way for more engaging and adaptive approaches to user
representation. This paper introduces Interactive Virtual Personas (IVPs):
multimodal, LLM-driven, conversational user simulations that designers can
interview, brainstorm with, and gather feedback from in real time via voice
interface. We conducted a qualitative study with eight professional UX
designers, employing an IVP named "Alice" across three design activities: user
research, ideation, and prototype evaluation. Our findings demonstrate the
potential of IVPs to expedite information gathering, inspire design solutions,
and provide rapid user-like feedback. However, designers raised concerns about
biases, over-optimism, the challenge of ensuring authenticity without real
stakeholder input, and the inability of the IVP to fully replicate the nuances
of human interaction. Our participants emphasised that IVPs should be viewed as
a complement to, not a replacement for, real user engagement. We discuss
strategies for prompt engineering, human-in-the-loop integration, and ethical
considerations for effective and responsible IVP use in design. Finally, our
work contributes to the growing body of research on generative AI in the design
process by providing insights into UX designers' experiences of LLM-powered
interactive personas.

</details>


### [137] [Orchid: Orchestrating Context Across Creative Workflows with Generative AI](https://arxiv.org/abs/2508.19517)
*Srishti Palani,Gonzalo Ramos*

Main category: cs.HC

TL;DR: 文章提出Orchid系统解决生成式AI上下文编排难题，研究表明其能提升创意任务效果。


<details>
  <summary>Details</summary>
Motivation: 主流生成式AI工具在跨多交互、会话和模型的工作流中上下文编排能力有限，导致用户负担重、意图模糊和创造力受限。

Method: 提出Orchid系统，让用户能指定、引用和监控上下文；开展有12名参与者的被试内研究。

Result: 使用Orchid执行创意任务的参与者产出更具新颖性和可行性的成果，认为AI响应与自身意图更契合，感知控制感和透明度更高。

Conclusion: Orchid优先考虑上下文编排，为支持复杂迭代工作流的下一代生成式AI工具提供了可行方案。

Abstract: Context is critical for meaningful interactions between people and Generative
AI (GenAI). Yet mainstream tools offer limited means to orchestrate it,
particularly across workflows that span multiple interactions, sessions, and
models, as often occurs in creative projects. Re specifying prior details,
juggling diverse artifacts, and dealing with context drift overwhelm users,
obscure intent, and curtail creativity. To address these challenges, we present
Orchid, a system that gives its users affordances to specify, reference, and
monitor context throughout evolving workflows. Specifically, Orchid enables
users to (1) specify context related to the project, themselves, and different
styles, (2) reference these via explicit mentions, inline selection, or
implicit grounding, and (3) monitor context assigned to different interactions
across the workflow. In a within-subjects study (n=12), participants using
Orchid to execute creative tasks (compared to a baseline toolkit of web search,
LLM-based chat, and digital notebooks) produced more novel and feasible
outcomes, reporting greater alignment between their intent and the AI's
responses, higher perceived control, and increased transparency. By
prioritizing context orchestration, Orchid offers an actionable step toward
next generation GenAI tools that support complex, iterative workflows -
enabling creators and AI to stay aligned and augment their creative potential.

</details>


### [138] [Attention is also needed for form design](https://arxiv.org/abs/2508.19708)
*B. Sankar,Dibakar Sen*

Main category: cs.HC

TL;DR: 本文介绍了一种新型注意力感知框架，结合EUPHORIA和RETINA系统，经研究验证其比传统设计方法更省时、设计质量更高，实现从CAD到DAC的范式转变。


<details>
  <summary>Details</summary>
Motivation: 传统产品设计过程认知要求高、耗时久、依赖主观经验且灵感转化不透明，需改进设计方法。

Method: 引入结合EUPHORIA和RETINA的注意力感知框架，进行两部分基础原理验证研究和对比研究，让设计师用不同工作流解决问题，由专家评估设计成果。

Result: EUPHORIA - RETINA工作流比传统方法时间效率高4倍多，全自动系统生成的设计在多个标准上得分最高。

Conclusion: 提出的框架实现从CAD到DAC的范式转变，能将人类直觉与AI生成能力结合，高效产出高质量设计。

Abstract: Conventional product design is a cognitively demanding process, limited by
its time-consuming nature, reliance on subjective expertise, and the opaque
translation of inspiration into tangible concepts. This research introduces a
novel, attention-aware framework that integrates two synergistic systems:
EUPHORIA, an immersive Virtual Reality environment using eye-tracking to
implicitly capture a designer's aesthetic preferences, and RETINA, an agentic
AI pipeline that translates these implicit preferences into concrete design
outputs. The foundational principles were validated in a two-part study. An
initial study correlated user's implicit attention with explicit preference and
the next one correlated mood to attention. A comparative study where 4
designers solved challenging design problems using 4 distinct workflows, from a
manual process to an end-to-end automated pipeline, showed the integrated
EUPHORIA-RETINA workflow was over 4 times more time-efficient than the
conventional method. A panel of 50 design experts evaluated the 16 final
renderings. Designs generated by the fully automated system consistently
received the highest Worthiness (calculated by an inverse Plackett-Luce model
based on gradient descent optimization) and Design Effectiveness scores,
indicating superior quality across 8 criteria: novelty, visual appeal,
emotional resonance, clarity of purpose, distinctiveness of silhouette, implied
materiality, proportional balance, & adherence to the brief. This research
presents a validated paradigm shift from traditional Computer-Assisted Design
(CAD) to a collaborative model of Designer-Assisting Computers (DAC). By
automating logistical and skill-dependent generative tasks, the proposed
framework elevates the designer's role to that of a creative director,
synergizing human intuition with the generative power of agentic AI to produce
higher-quality designs more efficiently.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [139] [Track Component Failure Detection Using Data Analytics over existing STDS Track Circuit data](https://arxiv.org/abs/2508.11693)
*Francisco López,Eduardo Di Santi,Clément Lefebvre,Nenad Mijatovic,Michele Pugnaloni,Victor Martín,Kenza Saiah*

Main category: eess.SP

TL;DR: 本文聚焦于一种特定的交流轨道电路STDS，用其电流数据结合SVM分类器识别故障，经测试能正确分类所有用例。


<details>
  <summary>Details</summary>
Motivation: 自动确定轨道故障组件，以改进维护行动。

Method: 将STDS电流数据应用于SVM分类器，训练模型对15种不同故障分类，这些故障属于3个更一般的类别。

Result: 用10个不同轨道电路的现场数据测试该方法，所有用例都被正确分类，且得到了STDS轨道电路专家和维护人员的验证。

Conclusion: 该方法可有效识别轨道电路故障，有助于轨道维护。

Abstract: Track Circuits (TC) are the main signalling devices used to detect the
presence of a train on a rail track. It has been used since the 19th century
and nowadays there are many types depending on the technology. As a general
classification, Track Circuits can be divided into 2 main groups, DC (Direct
Current) and AC (Alternating Current) circuits. This work is focused on a
particular AC track circuit, called "Smart Train Detection System" (STDS),
designed with both high and low-frequency bands. This approach uses STDS
current data applied to an SVM (support vector machine) classifier as a type of
failure identifier. The main purpose of this work consists on determine
automatically which is the component of the track that is failing to improve
the maintenance action. Model was trained to classify 15 different failures
that belong to 3 more general categories. The method was tested with field data
from 10 different track circuits and validated by the STDS track circuit expert
and maintainers. All use cases were correctly classified by the method.

</details>


### [140] [Arbitrary Precision Printed Ternary Neural Networks with Holistic Evolutionary Approximation](https://arxiv.org/abs/2508.19660)
*Vojtech Mrazek,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Zdenek Vasicek,Mehdi B. Tahoori,Georgios Zervakis*

Main category: eess.SP

TL;DR: 本文提出自动化框架设计印刷三元神经网络，在面积和功耗上优于现有近似印刷神经网络，实现低精度损失的印刷电池供电运行。


<details>
  <summary>Details</summary>
Motivation: 印刷电子有应用前景，但印刷神经网络实现复杂电路有挑战，需平衡分类精度和面积效率。

Method: 提出自动化框架，利用多目标优化和整体近似设计任意输入精度的印刷三元神经网络。

Result: 电路在面积上比现有近似印刷神经网络平均高17倍，功耗高59倍，首次实现印刷电池供电且精度损失低于5%。

Conclusion: 所提方法有效解决了印刷神经网络中分类精度和面积效率的平衡问题。

Abstract: Printed electronics offer a promising alternative for applications beyond
silicon-based systems, requiring properties like flexibility, stretchability,
conformality, and ultra-low fabrication costs. Despite the large feature sizes
in printed electronics, printed neural networks have attracted attention for
meeting target application requirements, though realizing complex circuits
remains challenging. This work bridges the gap between classification accuracy
and area efficiency in printed neural networks, covering the entire
processing-near-sensor system design and co-optimization from the
analog-to-digital interface-a major area and power bottleneck-to the digital
classifier. We propose an automated framework for designing printed Ternary
Neural Networks with arbitrary input precision, utilizing multi-objective
optimization and holistic approximation. Our circuits outperform existing
approximate printed neural networks by 17x in area and 59x in power on average,
being the first to enable printed-battery-powered operation with under 5%
accuracy loss while accounting for analog-to-digital interfacing costs.

</details>


### [141] [Scalable, Technology-Agnostic Diagnosis and Predictive Maintenance for Point Machine using Deep Learning](https://arxiv.org/abs/2508.11692)
*Eduardo Di Santi,Ruixiang Ci,Clément Lefebvre,Nenad Mijatovic,Michele Pugnaloni,Jonathan Brown,Victor Martín,Kenza Saiah*

Main category: eess.SP

TL;DR: 提出仅需单输入的深度学习方法，对道岔机故障分类，精度超99.99%，具有通用性和可扩展性，还符合ISO - 17359标准。


<details>
  <summary>Details</summary>
Motivation: 现有道岔机故障检测方法依赖多输入和定制特征，有数据收集和处理要求且缺乏可扩展性，需一种更优方法避免故障导致的运营中断。

Method: 对道岔机功率信号模式应用深度学习模型进行故障分类，还使用共形预测增加输出确定性。

Result: 实现超99.99%的精度、<0.01%的误报率和极低的漏报率，方法在多种道岔机上可扩展。

Conclusion: 提出的方法仅需单输入，具有通用性、可扩展性，符合ISO - 17359标准，能为维护人员提供可靠故障分类。

Abstract: The Point Machine (PM) is a critical piece of railway equipment that switches
train routes by diverting tracks through a switchblade. As with any critical
safety equipment, a failure will halt operations leading to service
disruptions; therefore, pre-emptive maintenance may avoid unnecessary
interruptions by detecting anomalies before they become failures. Previous work
relies on several inputs and crafting custom features by segmenting the signal.
This not only adds additional requirements for data collection and processing,
but it is also specific to the PM technology, the installed locations and
operational conditions limiting scalability. Based on the available maintenance
records, the main failure causes for PM are obstacles, friction, power source
issues and misalignment. Those failures affect the energy consumption pattern
of PMs, altering the usual (or healthy) shape of the power signal during the PM
movement. In contrast to the current state-of-the-art, our method requires only
one input. We apply a deep learning model to the power signal pattern to
classify if the PM is nominal or associated with any failure type, achieving
>99.99\% precision, <0.01\% false positives and negligible false negatives. Our
methodology is generic and technology-agnostic, proven to be scalable on
several electromechanical PM types deployed in both real-world and test bench
environments. Finally, by using conformal prediction the maintainer gets a
clear indication of the certainty of the system outputs, adding a confidence
layer to operations and making the method compliant with the ISO-17359
standard.

</details>


### [142] [Energy-Efficient Learning-Based Beamforming for ISAC-Enabled V2X Networks](https://arxiv.org/abs/2508.19566)
*Chen Shang,Jiadong Yu,Dinh Thai Hoang*

Main category: eess.SP

TL;DR: 提出基于学习的节能波束成形方案用于ISAC - 使能的V2X网络，用DRL联合优化，嵌入SNN提升能效，仿真验证效果好。


<details>
  <summary>Details</summary>
Motivation: 解决传统学习方案能耗高，实现V2X网络节能和高性能通信。

Method: 将V2X环境建模为马尔可夫决策过程，用DRL算法联合优化波束成形和功率分配，嵌入SNN到DRL框架。

Result: 仿真表明该方法实现大量节能和卓越通信性能。

Conclusion: 该方案有潜力支持未来V2X系统的绿色可持续连接。

Abstract: This work proposes an energy-efficient, learning-based beamforming scheme for
integrated sensing and communication (ISAC)-enabled V2X networks. Specifically,
we first model the dynamic and uncertain nature of V2X environments as a Markov
Decision Process. This formulation allows the roadside unit to generate
beamforming decisions based solely on current sensing information, thereby
eliminating the need for frequent pilot transmissions and extensive channel
state information acquisition. We then develop a deep reinforcement learning
(DRL) algorithm to jointly optimize beamforming and power allocation, ensuring
both communication throughput and sensing accuracy in highly dynamic scenario.
To address the high energy demands of conventional learning-based schemes, we
embed spiking neural networks (SNNs) into the DRL framework. Leveraging their
event-driven and sparsely activated architecture, SNNs significantly enhance
energy efficiency while maintaining robust performance. Simulation results
confirm that the proposed method achieves substantial energy savings and
superior communication performance, demonstrating its potential to support
green and sustainable connectivity in future V2X systems.

</details>


### [143] [Invited Paper: Feature-to-Classifier Co-Design for Mixed-Signal Smart Flexible Wearables for Healthcare at the Extreme Edge](https://arxiv.org/abs/2508.19637)
*Maha Shatta,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Georgios Panagopoulos,Mehdi B. Tahoori,Georgios Zervakis*

Main category: eess.SP

TL;DR: 提出用于柔性智能可穿戴系统的混合信号特征到分类器协同设计框架，降低特征提取成本，实现高效设计。


<details>
  <summary>Details</summary>
Motivation: 柔性电子集成密度有限、特征尺寸大，现有解决方案忽视系统级方案，特征提取和ADC硬件成本高。

Method: 提出混合信号特征到分类器协同设计框架，设计模拟特征提取器，提出硬件感知的特征选择策略。

Result: 在医疗基准测试中，该方法实现了高精度、超面积高效的柔性系统。

Conclusion: 该方法适用于一次性、低功耗的可穿戴监测。

Abstract: Flexible Electronics (FE) offer a promising alternative to rigid
silicon-based hardware for wearable healthcare devices, enabling lightweight,
conformable, and low-cost systems. However, their limited integration density
and large feature sizes impose strict area and power constraints, making
ML-based healthcare systems-integrating analog frontend, feature extraction and
classifier-particularly challenging. Existing FE solutions often neglect
potential system-wide solutions and focus on the classifier, overlooking the
substantial hardware cost of feature extraction and Analog-to-Digital
Converters (ADCs)-both major contributors to area and power consumption. In
this work, we present a holistic mixed-signal feature-to-classifier co-design
framework for flexible smart wearable systems. To the best of our knowledge, we
design the first analog feature extractors in FE, significantly reducing
feature extraction cost. We further propose an hardware-aware NAS-inspired
feature selection strategy within ML training, enabling efficient,
application-specific designs. Our evaluation on healthcare benchmarks shows our
approach delivers highly accurate, ultra-area-efficient flexible systems-ideal
for disposable, low-power wearable monitoring.

</details>


### [144] [Experimental End-to-End Optimization of Directly Modulated Laser-based IM/DD Transmission](https://arxiv.org/abs/2508.19910)
*Sergio Hernandez,Christophe Peucheret,Francesco Da Ros,Darko Zibar*

Main category: eess.SP

TL;DR: 本文基于实验数据训练的数据驱动代理模型，对基于直接调制激光器（DML）的系统进行端到端优化，并与4种基准方案对比，结果表明该方案性能更优。


<details>
  <summary>Details</summary>
Motivation: 直接调制激光器复杂的非线性动力学使基于DML的系统建模和优化具有挑战性，因此需要研究端到端优化方案。

Method: 基于实验数据训练数据驱动代理模型，对基于DML的系统进行端到端优化，包括脉冲整形和均衡器滤波器、偏置电流和调制射频功率等。

Result: 所提出的端到端方案在研究的符号率和传输距离范围内，能以更低的调制射频功率、更少的滤波器抽头和更小的信号带宽实现更好的性能。

Conclusion: 所提出的端到端优化方案优于基于线性和非线性接收端均衡的基准方案。

Abstract: Directly modulated lasers (DMLs) are an attractive technology for short-reach
intensity modulation and direct detection communication systems. However, their
complex nonlinear dynamics make the modeling and optimization of DML-based
systems challenging. In this paper, we study the end-to-end optimization of
DML-based systems based on a data-driven surrogate model trained on
experimental data. The end-to-end optimization includes the pulse shaping and
equalizer filters, the bias current and the modulation radio-frequency (RF)
power applied to the laser. The performance of the end-to-end optimization
scheme is tested on the experimental setup and compared to 4 different
benchmark schemes based on linear and nonlinear receiver-side equalization. The
results show that the proposed end-to-end scheme is able to deliver better
performance throughout the studied symbol rates and transmission distances
while employing lower modulation RF power, fewer filter taps and utilizing a
smaller signal bandwidth.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [145] [Simple Stepsize for Quasi-Newton Methods with Global Convergence Guarantees](https://arxiv.org/abs/2508.19712)
*Artem Agafonov,Vladislav Ryspayev,Samuel Horváth,Alexander Gasnikov,Martin Takáč,Slavomir Hanzely*

Main category: math.OC

TL;DR: 本文提出简单步长策略扩展拟牛顿法理论理解，证明凸函数全局收敛率，控制黑塞矩阵近似误差可加速收敛，经验验证有改进并开发自适应变体。


<details>
  <summary>Details</summary>
Motivation: 现有拟牛顿法全局收敛通常需特定线搜索策略和强凸假设，要扩展其理论理解。

Method: 引入简单步长策略，控制黑塞矩阵近似误差，开发自适应变体。

Result: 证明凸函数全局收敛率为${O}(1/k)$，控制误差可达${O}(1/k^2)$，经验验证优于标准拟牛顿基线。

Conclusion: 提出的方法有效扩展拟牛顿法理论，有收敛率保证且自适应变体增强鲁棒性。

Abstract: Quasi-Newton methods are widely used for solving convex optimization problems
due to their ease of implementation, practical efficiency, and strong local
convergence guarantees. However, their global convergence is typically
established only under specific line search strategies and the assumption of
strong convexity. In this work, we extend the theoretical understanding of
Quasi-Newton methods by introducing a simple stepsize schedule that guarantees
a global convergence rate of ${O}(1/k)$ for the convex functions. Furthermore,
we show that when the inexactness of the Hessian approximation is controlled
within a prescribed relative accuracy, the method attains an accelerated
convergence rate of ${O}(1/k^2)$ -- matching the best-known rates of both
Nesterov's accelerated gradient method and cubically regularized Newton
methods. We validate our theoretical findings through empirical comparisons,
demonstrating clear improvements over standard Quasi-Newton baselines. To
further enhance robustness, we develop an adaptive variant that adjusts to the
function's curvature while retaining the global convergence guarantees of the
non-adaptive algorithm.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [146] [Reduced-Order Modeling of Cyclo-Stationary Time Series Using Score-Based Generative Methods](https://arxiv.org/abs/2508.19448)
*Ludovico Theo Giorgini,Tobias Bischoff,Andre Noguiera Souza*

Main category: nlin.CD

TL;DR: 提出数据驱动方法构建循环平稳时间序列降阶模型，以气候模型为例展示性能，有计算优势并为多领域建模提供框架。


<details>
  <summary>Details</summary>
Motivation: 自然系统存在循环平稳行为，需构建降阶模型处理此类时间序列。

Method: 利用基于分数的生成建模的进展，构建降阶模型。

Result: 以气候模型为例，降阶模型准确复现原系统统计特性，有显著计算优势。

Conclusion: 为周期性强迫系统高效建模提供新可能，平衡计算效率和物理保真度。

Abstract: Many natural systems exhibit cyclo-stationary behavior characterized by
periodic forcing such as annual and diurnal cycles. We present a data-driven
method leveraging recent advances in score-based generative modeling to
construct reduced-order models for such cyclo-stationary time series. Our
approach accurately reproduces the statistical properties and temporal
correlations of the original data, enabling efficient generation of synthetic
trajectories. We demonstrate the performance of the method through application
to the Planet Simulator (PlaSim) climate model, constructing a reduced-order
model for the 20 leading principal components of surface temperature driven by
the annual cycle. The resulting surrogate model accurately reproduces the
marginal and joint probability distributions, autocorrelation functions, and
spatial coherence of the original climate system across multiple validation
metrics. The approach offers substantial computational advantages, enabling
generation of centuries of synthetic climate data in minutes compared to weeks
required for equivalent full model simulations. This work opens new
possibilities for efficient modeling of periodically forced systems across
diverse scientific domains, providing a principled framework for balancing
computational efficiency with physical fidelity in reduced-order modeling
applications.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [147] [Training for Obsolescence? The AI-Driven Education Trap](https://arxiv.org/abs/2508.19625)
*Andrew J. Peterson*

Main category: econ.GN

TL;DR: 人工智能改变学校人力资本生产和劳动力市场需求，信息失效导致技能错配，推广AI的教育政策或损害学生长期人力资本。


<details>
  <summary>Details</summary>
Motivation: 孤立分析人工智能对学校和劳动力市场的影响会导致教育资源错配，需研究相关问题。

Method: 对教育规划者采用AI的决策进行建模，基于试点调查提出核心假设。

Result: 信息失效造成的技能错配随AI普及单调增加，忽视非认知技能和学校对AI的过度投资会加剧错配。

Conclusion: 推广AI的教育政策若不结合前瞻性劳动力市场信号，可能损害学生长期人力资本。

Abstract: Artificial intelligence simultaneously transforms human capital production in
schools and its demand in labor markets. Analyzing these effects in isolation
can lead to a significant misallocation of educational resources. We model an
educational planner whose decision to adopt AI is driven by its teaching
productivity, failing to internalize AI's future wage-suppressing effect on
those same skills. Our core assumption, motivated by a pilot survey, is that
there is a positive correlation between these two effects. This drives our
central proposition: this information failure creates a skill mismatch that
monotonically increases with AI prevalence. Extensions show the mismatch is
exacerbated by the neglect of unpriced non-cognitive skills and by a school's
endogenous over-investment in AI. Our findings caution that policies promoting
AI in education, if not paired with forward-looking labor market signals, may
paradoxically undermine students' long-term human capital, especially if
reliance on AI crowds out the development of unpriced non-cognitive skills,
such as persistence, that are forged through intellectual struggle.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [148] [Neural Conditional Simulation for Complex Spatial Processes](https://arxiv.org/abs/2508.20067)
*Julia Walchessen,Andrew Zammit-Mangion,Raphaël Huser,Mikael Kuusela*

Main category: stat.ME

TL;DR: 提出基于神经扩散模型的空间条件模拟方法NCS，经评估比传统方法更高效准确。


<details>
  <summary>Details</summary>
Motivation: 许多空间过程模型难以或低效地从预测分布进行精确条件模拟，需要新方法。

Method: 提出神经条件模拟（NCS），利用空间掩码实现基于条件分数的扩散模型，通过神经网络训练，训练后可用于广泛预测分布的条件模拟。

Result: 将NCS生成的模拟与高斯过程模型的真实条件分布模拟、布朗 - 雷斯尼克过程模型的马尔可夫链蒙特卡罗（MCMC）模拟进行比较，NCS在后者中比经典MCMC技术更高效准确。

Conclusion: NCS能对传统方法难以采样的空间预测分布进行高效准确的条件模拟。

Abstract: A key objective in spatial statistics is to simulate from the distribution of
a spatial process at a selection of unobserved locations conditional on
observations (i.e., a predictive distribution) to enable spatial prediction and
uncertainty quantification. However, exact conditional simulation from this
predictive distribution is intractable or inefficient for many spatial process
models. In this paper, we propose neural conditional simulation (NCS), a
general method for spatial conditional simulation that is based on neural
diffusion models. Specifically, using spatial masks, we implement a conditional
score-based diffusion model that evolves Gaussian noise into samples from a
predictive distribution when given a partially observed spatial field and
spatial process parameters as inputs. The diffusion model relies on a neural
network that only requires unconditional samples from the spatial process for
training. Once trained, the diffusion model is amortized with respect to the
observations in the partially observed field, the number and locations of those
observations, and the spatial process parameters, and can therefore be used to
conditionally simulate from a broad class of predictive distributions without
retraining the neural network. We assess the NCS-generated simulations against
simulations from the true conditional distribution of a Gaussian process model,
and against Markov chain Monte Carlo (MCMC) simulations from a Brown--Resnick
process model for spatial extremes. In the latter case, we show that it is more
efficient and accurate to conditionally simulate using NCS than classical MCMC
techniques implemented in standard software. We conclude that NCS enables
efficient and accurate conditional simulation from spatial predictive
distributions that are challenging to sample from using traditional methods.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [149] [Eigenvalue distribution of the Neural Tangent Kernel in the quadratic scaling](https://arxiv.org/abs/2508.20036)
*Lucas Benigni,Elliot Paquette*

Main category: math.PR

TL;DR: 计算特定维度缩放下两层神经网络神经切线核的渐近特征值分布，结果为与特定分布的自由乘性卷积。


<details>
  <summary>Details</summary>
Motivation: 研究两层神经网络神经切线核在特定维度缩放下的渐近特征值分布。

Method: 基于给定的矩阵形式和缩放条件，运用相关数学理论进行计算。

Result: 渐近分布为Marchenko - Pastur分布与依赖于σ和D的确定性分布的自由乘性卷积。

Conclusion: 成功描述了特定维度缩放下两层神经网络神经切线核的渐近特征值分布。

Abstract: We compute the asymptotic eigenvalue distribution of the neural tangent
kernel of a two-layer neural network under a specific scaling of dimension.
Namely, if $X\in\mathbb{R}^{n\times d}$ is an i.i.d random matrix,
$W\in\mathbb{R}^{d\times p}$ is an i.i.d $\mathcal{N}(0,1)$ matrix and
$D\in\mathbb{R}^{p\times p}$ is a diagonal matrix with i.i.d bounded entries,
we consider the matrix
  \[
  \mathrm{NTK}
  =
  \frac{1}{d}XX^\top
  \odot
  \frac{1}{p}
  \sigma'\left(
  \frac{1}{\sqrt{d}}XW
  \right)D^2
  \sigma'\left(
  \frac{1}{\sqrt{d}}XW
  \right)^\top
  \]
  where $\sigma'$ is a pseudo-Lipschitz function applied entrywise and under
the scaling $\frac{n}{dp}\to \gamma_1$ and $\frac{p}{d}\to \gamma_2$. We
describe the asymptotic distribution as the free multiplicative convolution of
the Marchenko--Pastur distribution with a deterministic distribution depending
on $\sigma$ and $D$.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [150] [CellINR: Implicitly Overcoming Photo-induced Artifacts in 4D Live Fluorescence Microscopy](https://arxiv.org/abs/2508.19300)
*Cunmin Zhao,Ziyuan Luo,Guoye Guan,Zelin Li,Yiming Ma,Zhongying Zhao,Renjie Wan*

Main category: eess.IV

TL;DR: 提出CellINR框架解决4D活荧光显微镜光照问题，实验效果好且公开代码和数据集


<details>
  <summary>Details</summary>
Motivation: 4D活荧光显微镜因长时间高强度光照产生光漂白和光毒性效应，影响图像质量

Method: 基于隐式神经表示的CellINR框架，采用盲卷积和结构放大策略将3D空间坐标映射到高频域

Result: CellINR在去除伪影和恢复结构连续性方面显著优于现有技术，提供配对4D活细胞成像数据集

Conclusion: CellINR为后续定量分析和生物学研究提供坚实基础

Abstract: 4D live fluorescence microscopy is often compromised by prolonged high
intensity illumination which induces photobleaching and phototoxic effects that
generate photo-induced artifacts and severely impair image continuity and
detail recovery. To address this challenge, we propose the CellINR framework, a
case-specific optimization approach based on implicit neural representation.
The method employs blind convolution and structure amplification strategies to
map 3D spatial coordinates into the high frequency domain, enabling precise
modeling and high-accuracy reconstruction of cellular structures while
effectively distinguishing true signals from artifacts. Experimental results
demonstrate that CellINR significantly outperforms existing techniques in
artifact removal and restoration of structural continuity, and for the first
time, a paired 4D live cell imaging dataset is provided for evaluating
reconstruction performance, thereby offering a solid foundation for subsequent
quantitative analyses and biological research. The code and dataset will be
public.

</details>


### [151] [2D Ultrasound Elasticity Imaging of Abdominal Aortic Aneurysms Using Deep Neural Networks](https://arxiv.org/abs/2508.19303)
*Utsav Ratna Tuladhar,Richard Simon,Doran Mix,Michael Richards*

Main category: eess.IV

TL;DR: 本文提出基于深度学习的二维超声腹主动脉瘤弹性成像框架，在多个实验域评估，模型能有效重建模量分布，可快速无创评估破裂风险。


<details>
  <summary>Details</summary>
Motivation: 传统仅用最大直径评估腹主动脉瘤破裂风险不足，未考虑血管壁材料特性，需更好方法。

Method: 利用有限元模拟生成位移场和模量分布数据集，用U - Net架构和NMSE训练模型，从位移场推断模量分布，在三个实验域评估。

Result: 模拟结果显示模型能重建模量分布，NMSE为0.73%；在体模数据中预测值与预期值相符；与迭代法性能相当但计算时间短。

Conclusion: 深度学习方法可从超声图像快速有效估计组织硬度，有助于无创评估腹主动脉瘤破裂风险。

Abstract: Abdominal aortic aneurysms (AAA) pose a significant clinical risk due to
their potential for rupture, which is often asymptomatic but can be fatal.
Although maximum diameter is commonly used for risk assessment, diameter alone
is insufficient as it does not capture the properties of the underlying
material of the vessel wall, which play a critical role in determining the risk
of rupture. To overcome this limitation, we propose a deep learning-based
framework for elasticity imaging of AAAs with 2D ultrasound. Leveraging finite
element simulations, we generate a diverse dataset of displacement fields with
their corresponding modulus distributions. We train a model with U-Net
architecture and normalized mean squared error (NMSE) to infer the spatial
modulus distribution from the axial and lateral components of the displacement
fields. This model is evaluated across three experimental domains: digital
phantom data from 3D COMSOL simulations, physical phantom experiments using
biomechanically distinct vessel models, and clinical ultrasound exams from AAA
patients. Our simulated results demonstrate that the proposed deep learning
model is able to reconstruct modulus distributions, achieving an NMSE score of
0.73\%. Similarly, in phantom data, the predicted modular ratio closely matches
the expected values, affirming the model's ability to generalize to phantom
data. We compare our approach with an iterative method which shows comparable
performance but higher computation time. In contrast, the deep learning method
can provide quick and effective estimates of tissue stiffness from ultrasound
images, which could help assess the risk of AAA rupture without invasive
procedures.

</details>


### [152] [MedVQA-TREE: A Multimodal Reasoning and Retrieval Framework for Sarcopenia Prediction](https://arxiv.org/abs/2508.19319)
*Pardis Moradbeiki,Nasser Ghadiri,Sayed Jalal Zahabi,Uffe Kock Wiil,Kristoffer Kittelmann Brockhattingen,Ali Ebrahimi*

Main category: eess.IV

TL;DR: 提出MedVQA - TREE多模态框架用于肌肉减少症超声诊断，在多数据集评估中表现出色，准确率高。


<details>
  <summary>Details</summary>
Motivation: 现有超声肌肉减少症诊断因成像线索微妙、标注数据有限和缺乏临床背景而具有挑战性。

Method: 提出MedVQA - TREE框架，包含分层图像解释模块、门控特征级融合机制和多跳多查询检索策略，还通过UMLS引导管道访问知识源。

Result: 在两个公共MedVQA数据集和自定义肌肉减少症超声数据集评估，模型诊断准确率达99%，比先前方法高超10%。

Conclusion: 结合结构化视觉理解和引导知识检索对肌肉减少症的AI辅助诊断有益。

Abstract: Accurate sarcopenia diagnosis via ultrasound remains challenging due to
subtle imaging cues, limited labeled data, and the absence of clinical context
in most models. We propose MedVQA-TREE, a multimodal framework that integrates
a hierarchical image interpretation module, a gated feature-level fusion
mechanism, and a novel multi-hop, multi-query retrieval strategy. The vision
module includes anatomical classification, region segmentation, and graph-based
spatial reasoning to capture coarse, mid-level, and fine-grained structures. A
gated fusion mechanism selectively integrates visual features with textual
queries, while clinical knowledge is retrieved through a UMLS-guided pipeline
accessing PubMed and a sarcopenia-specific external knowledge base. MedVQA-TREE
was trained and evaluated on two public MedVQA datasets (VQA-RAD and PathVQA)
and a custom sarcopenia ultrasound dataset. The model achieved up to 99%
diagnostic accuracy and outperformed previous state-of-the-art methods by over
10%. These results underscore the benefit of combining structured visual
understanding with guided knowledge retrieval for effective AI-assisted
diagnosis in sarcopenia.

</details>


### [153] [AT-CXR: Uncertainty-Aware Agentic Triage for Chest X-rays](https://arxiv.org/abs/2508.19322)
*Xueyang Li,Mingze Jiang,Gelei Xu,Jun Xia,Mengzhao Jia,Danny Chen,Yiyu Shi*

Main category: eess.IV

TL;DR: 本文提出用于胸部X光的不确定性感知代理AT - CXR，评估两种路由设计，表现优于现有模型，代码开源。


<details>
  <summary>Details</summary>
Motivation: 真正的自主医学影像分诊研究不足，需填补该空白。

Method: 引入AT - CXR系统，评估确定性规则路由和LLM决策路由两种设计。

Result: 两种路由设计在NIH ChestX - ray14数据集上表现优于零样本视觉语言模型和监督分类器，有更高准确率、更好选择性预测性能、更低延迟。

Conclusion: 两种路由提供互补操作点，可根据需求优先考虑吞吐量或准确性。

Abstract: Agentic AI is advancing rapidly, yet truly autonomous medical-imaging triage,
where a system decides when to stop, escalate, or defer under real constraints,
remains relatively underexplored. To address this gap, we introduce AT-CXR, an
uncertainty-aware agent for chest X-rays. The system estimates per-case
confidence and distributional fit, then follows a stepwise policy to issue an
automated decision or abstain with a suggested label for human intervention. We
evaluate two router designs that share the same inputs and actions: a
deterministic rule-based router and an LLM-decided router. Across five-fold
evaluation on a balanced subset of NIH ChestX-ray14 dataset, both variants
outperform strong zero-shot vision-language models and state-of-the-art
supervised classifiers, achieving higher full-coverage accuracy and superior
selective-prediction performance, evidenced by a lower area under the
risk-coverage curve (AURC) and a lower error rate at high coverage, while
operating with lower latency that meets practical clinical constraints. The two
routers provide complementary operating points, enabling deployments to
prioritize maximal throughput or maximal accuracy. Our code is available at
https://github.com/XLIAaron/uncertainty-aware-cxr-agent.

</details>


### [154] [MRExtrap: Longitudinal Aging of Brain MRIs using Linear Modeling in Latent Space](https://arxiv.org/abs/2508.19482)
*Jaivardhan Kapoor,Jakob H. Macke,Christian F. Baumgartner*

Main category: eess.IV

TL;DR: 本文提出MRExtrap方法，基于卷积自编码器潜空间的线性模型模拟3D脑MRI扫描衰老，在ADNI数据集表现良好，与疾病和衰老模式相关。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的生成模型通常从单个观察扫描预测未来扫描，本文探索通过卷积自编码器潜空间的线性模型对大脑衰老进行建模。

Method: 在脑MRI上训练自编码器创建潜空间，通过基于年龄的线性外推预测未来MRI，针对单扫描预测提出使用总体平均和特定于受试者的先验，利用贝叶斯后验采样更新多扫描预测。

Result: MRExtrap在ADNI数据集上准确预测衰老模式，单体积预测击败基于GAN的基线，展示并分析了多扫描条件化。

Conclusion: MRExtrap为基于年龄的3D脑MRI生成提供了简单而稳健的方法，在多纵向观察场景中特别有价值。

Abstract: Simulating aging in 3D brain MRI scans can reveal disease progression
patterns in neurological disorders such as Alzheimer's disease. Current deep
learning-based generative models typically approach this problem by predicting
future scans from a single observed scan. We investigate modeling brain aging
via linear models in the latent space of convolutional autoencoders (MRExtrap).
Our approach, MRExtrap, is based on our observation that autoencoders trained
on brain MRIs create latent spaces where aging trajectories appear
approximately linear. We train autoencoders on brain MRIs to create latent
spaces, and investigate how these latent spaces allow predicting future MRIs
through linear extrapolation based on age, using an estimated latent
progression rate $\boldsymbol{\beta}$. For single-scan prediction, we propose
using population-averaged and subject-specific priors on linear progression
rates. We also demonstrate that predictions in the presence of additional scans
can be flexibly updated using Bayesian posterior sampling, providing a
mechanism for subject-specific refinement. On the ADNI dataset, MRExtrap
predicts aging patterns accurately and beats a GAN-based baseline for
single-volume prediction of brain aging. We also demonstrate and analyze
multi-scan conditioning to incorporate subject-specific progression rates.
Finally, we show that the latent progression rates in MRExtrap's linear
framework correlate with disease and age-based aging patterns from previously
studied structural atrophy rates. MRExtrap offers a simple and robust method
for the age-based generation of 3D brain MRIs, particularly valuable in
scenarios with multiple longitudinal observations.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [155] [Internally-Convex Drawings of Outerplanar Graphs in Small Area](https://arxiv.org/abs/2508.19913)
*Michael A. Bekos,Giordano Da Lozzo,Fabrizio Frati,Giuseppe Liotta,Antonios Symvonis*

Main category: cs.CG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A well-known result by Kant [Algorithmica, 1996] implies that n-vertex
outerplane graphs admit embedding-preserving planar straight-line grid drawings
where the internal faces are convex polygons in $O(n^2)$ area. In this paper,
we present an algorithm to compute such drawings in $O(n^{1.5})$ area. We also
consider outerplanar drawings in which the internal faces are required to be
strictly-convex polygons. In this setting, we consider outerplanar graphs whose
weak dual is a path and give a drawing algorithm that achieves $\Theta(nk^2)$
area, where $k$ is the maximum size of an internal facial cycle.

</details>


### [156] [Visualizing Treewidth](https://arxiv.org/abs/2508.19935)
*Alvin Chiu,Thomas Depian,David Eppstein,Michael T. Goodrich,Martin Nöllenburg*

Main category: cs.CG

TL;DR: 研究并实现多种见证绘图范式以展示图的有界路径宽度或树宽度，介绍绘图风格分类。


<details>
  <summary>Details</summary>
Motivation: 为清晰展示图具有有界路径宽度或树宽度这一属性。

Method: 绘制树分解或路径分解为袋树，优化袋内顶点布局，用动态规划和启发式方法实现可视化原型，引入绘图风格分类。

Result: 实现了用于交叉最小化的可视化原型，介绍了绘图风格分类。

Conclusion: 通过多种绘图范式和风格能有效展示图的有界路径宽度或树宽度属性。

Abstract: A witness drawing of a graph is a visualization that clearly shows a given
property of a graph. We study and implement various drawing paradigms for
witness drawings to clearly show that graphs have bounded pathwidth or
treewidth. Our approach draws the tree decomposition or path decomposition as a
tree of bags, with induced subgraphs shown in each bag, and with ''tracks'' for
each graph vertex connecting its copies in multiple bags. Within bags, we
optimize the vertex layout to avoid crossings of edges and tracks. We implement
a visualization prototype for crossing minimization using dynamic programming
for graphs of small width and heuristic approaches for graphs of larger width.
We introduce a taxonomy of drawing styles, which render the subgraph for each
bag as an arc diagram with one or two pages or as a circular layout with
straight-line edges, and we render tracks either with straight lines or with
orbital-radial paths.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [157] [The Next Layer: Augmenting Foundation Models with Structure-Preserving and Attention-Guided Learning for Local Patches to Global Context Awareness in Computational Pathology](https://arxiv.org/abs/2508.19914)
*Muhammad Waqas,Rukhmini Bandyopadhyay,Eman Showkatian,Amgad Muneer,Anas Zafar,Frank Rojas Alvarez,Maricel Corredor Marin,Wentao Li,David Jaffray,Cara Haymaker,John Heymach,Natalie I Vokes,Luisa Maren Solis Soto,Jianjun Zhang,Jia Wu*

Main category: q-bio.QM

TL;DR: 提出EAGLE - Net结构保存、注意力引导的MIL架构，在多个癌症数据集上表现良好，可提升生物标志物发现等能力。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在计算病理学中缺少利用组织全局空间结构和诊断相关区域局部上下文关系的机制，MIL是基础模型之后的关键步骤。

Method: 提出EAGLE - Net架构，集成多尺度绝对空间编码、top - K邻域感知损失和背景抑制损失。

Result: 在大型泛癌数据集上，EAGLE - Net分类准确率最高提升3%，7种癌症中有6种达到最高一致性指数，生成的注意力图与专家注释一致。

Conclusion: EAGLE - Net是可泛化、可解释的框架，能补充基础模型，有助于生物标志物发现、预后建模和临床决策支持。

Abstract: Foundation models have recently emerged as powerful feature extractors in
computational pathology, yet they typically omit mechanisms for leveraging the
global spatial structure of tissues and the local contextual relationships
among diagnostically relevant regions - key elements for understanding the
tumor microenvironment. Multiple instance learning (MIL) remains an essential
next step following foundation model, designing a framework to aggregate
patch-level features into slide-level predictions. We present EAGLE-Net, a
structure-preserving, attention-guided MIL architecture designed to augment
prediction and interpretability. EAGLE-Net integrates multi-scale absolute
spatial encoding to capture global tissue architecture, a top-K
neighborhood-aware loss to focus attention on local microenvironments, and
background suppression loss to minimize false positives. We benchmarked
EAGLE-Net on large pan-cancer datasets, including three cancer types for
classification (10,260 slides) and seven cancer types for survival prediction
(4,172 slides), using three distinct histology foundation backbones (REMEDIES,
Uni-V1, Uni2-h). Across tasks, EAGLE-Net achieved up to 3% higher
classification accuracy and the top concordance indices in 6 of 7 cancer types,
producing smooth, biologically coherent attention maps that aligned with expert
annotations and highlighted invasive fronts, necrosis, and immune infiltration.
These results position EAGLE-Net as a generalizable, interpretable framework
that complements foundation models, enabling improved biomarker discovery,
prognostic modeling, and clinical decision support

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [158] [Large Language Models (LLMs) for Electronic Design Automation (EDA)](https://arxiv.org/abs/2508.20030)
*Kangwei Xu,Denis Schwachhofer,Jason Blocklove,Ilia Polian,Peter Domanski,Dirk Pflüger,Siddharth Garg,Ramesh Karri,Ozgur Sinanoglu,Johann Knechtel,Zhuorui Zhao,Ulf Schlichtmann,Bing Li*

Main category: eess.SY

TL;DR: 现代集成电路复杂度增加，传统设计流程低效，本文全面介绍将大语言模型（LLMs）融入电子设计自动化（EDA），通过案例展示其能力，指出未来方向和挑战。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路设计流程复杂、劳动密集且易出错，急需更高效的 EDA 解决方案，而 LLMs 能力提升，为 EDA 带来机遇。

Method: 对将 LLMs 融入 EDA 进行全面概述，引入三个案例研究。

Result: 展示了 LLMs 在硬件设计、测试和优化方面的能力。

Conclusion: 指出未来方向和挑战，为利用先进 AI 技术进行 EDA 的研究人员提供有价值的见解。

Abstract: With the growing complexity of modern integrated circuits, hardware engineers
are required to devote more effort to the full design-to-manufacturing
workflow. This workflow involves numerous iterations, making it both
labor-intensive and error-prone. Therefore, there is an urgent demand for more
efficient Electronic Design Automation (EDA) solutions to accelerate hardware
development. Recently, large language models (LLMs) have shown remarkable
advancements in contextual comprehension, logical reasoning, and generative
capabilities. Since hardware designs and intermediate scripts can be
represented as text, integrating LLM for EDA offers a promising opportunity to
simplify and even automate the entire workflow. Accordingly, this paper
provides a comprehensive overview of incorporating LLMs into EDA, with emphasis
on their capabilities, limitations, and future opportunities. Three case
studies, along with their outlook, are introduced to demonstrate the
capabilities of LLMs in hardware design, testing, and optimization. Finally,
future directions and challenges are highlighted to further explore the
potential of LLMs in shaping the next-generation EDA, providing valuable
insights for researchers interested in leveraging advanced AI technologies for
EDA.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [159] [Quantum Resource Management in the NISQ Era: Challenges, Vision, and a Runtime Framework](https://arxiv.org/abs/2508.19276)
*Marcos Guillermo Lammers,Federico Hernán Holik,Alejandro Fernández*

Main category: quant-ph

TL;DR: 本文分析NISQ设备资源作用，提出运行时感知量子软件开发愿景，介绍Qonscious框架以推动量子资源估计领域发展。


<details>
  <summary>Details</summary>
Motivation: 当前NISQ时代硬件存在局限性，高效管理量子资源对量子算法设计和部署很重要，需研究资源在NISQ设备中的作用及对量子软件开发的影响。

Method: 分析资源在NISQ设备不同用途中的作用，提出运行时感知量子软件开发愿景，介绍Qonscious原型框架。

Result: 引入Qonscious框架，可基于动态资源评估实现量子程序的条件执行。

Conclusion: 有助于加强量子资源估计领域，推动可扩展、可靠和资源感知的量子软件开发。

Abstract: Quantum computers represent a radical technological advancement in the way
information is processed by using the principles of quantum mechanics to solve
very complex problems that exceed the capabilities of classical systems.
However, in the current NISQ era (Noisy Intermediate-Scale Quantum devices),
the available hardware presents several limitations, such as a limited number
of qubits, high error rates, and reduced coherence times. Efficient management
of quantum resources, both physical (qubits, error rates, connectivity) and
logical (quantum gates, algorithms, error correction), becomes particularly
relevant in the design and deployment of quantum algorithms. In this work, we
analyze the role of resources in the various uses of NISQ devices today,
identifying their relevance and implications for software engineering focused
on the use of quantum computers. We propose a vision for runtime-aware quantum
software development, identifying key challenges to its realization, such as
limited introspection capabilities and temporal constraints in current
platforms. As a proof of concept, we introduce Qonscious, a prototype framework
that enables conditional execution of quantum programs based on dynamic
resource evaluation. With this contribution, we aim to strengthen the field of
Quantum Resource Estimation (QRE) and move towards the development of scalable,
reliable, and resource-aware quantum software.

</details>


### [160] [Quantum Entanglement as Super-Confounding: From Bell's Theorem to Robust Machine Learning](https://arxiv.org/abs/2508.19327)
*Pilsung Kang*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Bell's theorem reveals a profound conflict between quantum mechanics and
local realism, a conflict we reinterpret through the modern lens of causal
inference. We propose and computationally validate a framework where quantum
entanglement acts as a "super-confounding" resource, generating correlations
that violate the classical causal bounds set by Bell's inequalities. This work
makes three key contributions: First, we establish a physical hierarchy of
confounding (Quantum > Classical) and introduce Confounding Strength (CS) to
quantify this effect. Second, we provide a circuit-based implementation of the
quantum $\mathcal{DO}$-calculus to distinguish causality from spurious
correlation. Finally, we apply this calculus to a quantum machine learning
problem, where causal feature selection yields a statistically significant
11.3% average absolute improvement in model robustness. Our framework bridges
quantum foundations and causal AI, offering a new, practical perspective on
quantum correlations.

</details>


### [161] [Is data-efficient learning feasible with quantum models?](https://arxiv.org/abs/2508.19437)
*Alona Sakhnenko,Christian B. Mendl,Jeanette M. Lorenz*

Main category: quant-ph

TL;DR: 研究聚焦数据集大小与QML模型数据效率，提出半人工数据集生成方法和新分析工具，证实QKMs训练需数据少且分析工具与实证契合。


<details>
  <summary>Details</summary>
Motivation: 缺乏理解数据集特征的统一框架，研究QML模型在数据集复杂度下与经典模型的数据效率差异。

Method: 提出半人工全经典数据集生成方法，引入源于经典核方法的分析工具。

Result: QKMs训练所需数据少、误差率低，分析工具预测与实证相符，可生成不同属性数据集。

Conclusion: 有助于全面探索数据集复杂度，加深对QKM及QML模型泛化优势的理解，为领域发展奠定基础。

Abstract: The importance of analyzing nontrivial datasets when testing quantum machine
learning (QML) models is becoming increasingly prominent in literature, yet a
cohesive framework for understanding dataset characteristics remains elusive.
In this work, we concentrate on the size of the dataset as an indicator of its
complexity and explores the potential for QML models to demonstrate superior
data-efficiency compared to classical models, particularly through the lens of
quantum kernel methods (QKMs). We provide a method for generating
semi-artificial fully classical datasets, on which we show one of the first
evidence of the existence of classical datasets where QKMs require less data
during training. Additionally, our study introduces a new analytical tool to
the QML domain, derived for classical kernel methods, which can be aimed at
investigating the classical-quantum gap. Our empirical results reveal that QKMs
can achieve low error rates with less training data compared to classical
counterparts. Furthermore, our method allows for the generation of datasets
with varying properties, facilitating further investigation into the
characteristics of real-world datasets that may be particularly advantageous
for QKMs. We also show that the predicted performance from the analytical tool
we propose - a generalization metric from classical domain - show great
alignment empirical evidence, which fills the gap previously existing in the
field. We pave a way to a comprehensive exploration of dataset complexities,
providing insights into how these complexities influence QML performance
relative to traditional methods. This research contributes to a deeper
understanding of the generalization benefits of QKM models and potentially a
broader family of QML models, setting the stage for future advancements in the
field.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [162] [MuSpike: A Benchmark and Evaluation Framework for Symbolic Music Generation with Spiking Neural Networks](https://arxiv.org/abs/2508.19251)
*Qian Liang,Menghaoran Tang,Yi Zeng*

Main category: cs.SD

TL;DR: 介绍MuSpike基准和评估框架用于符号音乐生成中SNN模型评估，结果显示不同模型优势、不同背景参与者感知差异及主客观评估不一致。


<details>
  <summary>Details</summary>
Motivation: 符号音乐生成在SNN领域缺乏标准化基准和综合评估方法，需填补该空白。

Method: 引入MuSpike框架，对五种SNN架构在五个典型数据集上评估，结合客观指标和大规模听力研究，提出新主观指标。

Result: 不同SNN模型在评估维度有不同优势；不同音乐背景参与者感知模式多样，专家对AI作曲更包容；主客观评估存在明显不一致。

Conclusion: MuSpike为SNN模型在符号音乐生成中提供首个系统基准和评估框架，为未来研究奠定基础。

Abstract: Symbolic music generation has seen rapid progress with artificial neural
networks, yet remains underexplored in the biologically plausible domain of
spiking neural networks (SNNs), where both standardized benchmarks and
comprehensive evaluation methods are lacking. To address this gap, we introduce
MuSpike, a unified benchmark and evaluation framework that systematically
assesses five representative SNN architectures (SNN-CNN, SNN-RNN, SNN-LSTM,
SNN-GAN and SNN-Transformer) across five typical datasets, covering tonal,
structural, emotional, and stylistic variations. MuSpike emphasizes
comprehensive evaluation, combining established objective metrics with a
large-scale listening study. We propose new subjective metrics, targeting
musical impression, autobiographical association, and personal preference, that
capture perceptual dimensions often overlooked in prior work. Results reveal
that (1) different SNN models exhibit distinct strengths across evaluation
dimensions; (2) participants with different musical backgrounds exhibit diverse
perceptual patterns, with experts showing greater tolerance toward AI-composed
music; and (3) a noticeable misalignment exists between objective and
subjective evaluations, highlighting the limitations of purely statistical
metrics and underscoring the value of human perceptual judgment in assessing
musical quality. MuSpike provides the first systematic benchmark and systemic
evaluation framework for SNN models in symbolic music generation, establishing
a solid foundation for future research into biologically plausible and
cognitively grounded music generation.

</details>


### [163] [CompLex: Music Theory Lexicon Constructed by Autonomous Agents for Automatic Music Generation](https://arxiv.org/abs/2508.19603)
*Zhejing Hu,Yan Liu,Gong Chen,Bruce X. B. Yu*

Main category: cs.SD

TL;DR: 论文引入自动音乐词典构建模型CompLex，提出多智能体算法检测和减少幻觉，提升文本到音乐生成模型性能并验证了词典特性。


<details>
  <summary>Details</summary>
Motivation: 音乐生成的人工智能发展不如自然语言处理，因音乐数据有限，且传统音乐生成技术需大量人力，期望利用音乐理论改进AI音乐生成。

Method: 引入自动音乐词典构建模型CompLex，用9个手动输入的类别关键词和5个句子提示模板生成含37,432个条目的词典；提出多智能体算法检测和减少幻觉。

Result: CompLex在三种最先进的文本到音乐生成模型中展现出显著性能提升，涵盖符号和基于音频的方法。

Conclusion: CompLex具备有效词典的关键特性，如完整性、准确性、非冗余性和可执行性。

Abstract: Generative artificial intelligence in music has made significant strides, yet
it still falls short of the substantial achievements seen in natural language
processing, primarily due to the limited availability of music data.
Knowledge-informed approaches have been shown to enhance the performance of
music generation models, even when only a few pieces of musical knowledge are
integrated. This paper seeks to leverage comprehensive music theory in
AI-driven music generation tasks, such as algorithmic composition and style
transfer, which traditionally require significant manual effort with existing
techniques. We introduce a novel automatic music lexicon construction model
that generates a lexicon, named CompLex, comprising 37,432 items derived from
just 9 manually input category keywords and 5 sentence prompt templates. A new
multi-agent algorithm is proposed to automatically detect and mitigate
hallucinations. CompLex demonstrates impressive performance improvements across
three state-of-the-art text-to-music generation models, encompassing both
symbolic and audio-based methods. Furthermore, we evaluate CompLex in terms of
completeness, accuracy, non-redundancy, and executability, confirming that it
possesses the key characteristics of an effective lexicon.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [164] [Database Entity Recognition with Data Augmentation and Deep Learning](https://arxiv.org/abs/2508.19372)
*Zikun Fu,Chen Yang,Kourosh Davoudi,Ken Q. Pu*

Main category: cs.CL

TL;DR: 本文针对自然语言查询中的数据库实体识别挑战，提出基准、数据增强方法和基于语言模型的识别模型，对比显示性能更好，消融评估表明数据增强和微调骨干模型能提升指标。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言查询中数据库实体识别（DB - ER）的挑战，推动该领域发展。

Method: 1. 从流行文本到SQL基准中导出人类注释的DB - ER任务基准；2. 利用流行文本到SQL基准中的SQL查询对NLQ进行自动注释的数据增强程序；3. 以T5为骨干的专业语言模型实体识别模型，有序列标记和令牌分类两个下游任务用于微调后端和执行DB - ER。

Result: 与两个最先进的NER标记器相比，本文的DB - ER标记器在精度和召回率上表现更好。消融评估显示数据增强使精度和召回率提高超过10%，T5骨干微调使这些指标提高5 - 10%。

Conclusion: 提出的基准、数据增强方法和识别模型在数据库实体识别上效果良好，数据增强和骨干微调能有效提升性能。

Abstract: This paper addresses the challenge of Database Entity Recognition (DB-ER) in
Natural Language Queries (NLQ). We present several key contributions to advance
this field: (1) a human-annotated benchmark for DB-ER task, derived from
popular text-to-sql benchmarks, (2) a novel data augmentation procedure that
leverages automatic annotation of NLQs based on the corresponding SQL queries
which are available in popular text-to-SQL benchmarks, (3) a specialized
language model based entity recognition model using T5 as a backbone and two
down-stream DB-ER tasks: sequence tagging and token classification for
fine-tuning of backend and performing DB-ER respectively. We compared our DB-ER
tagger with two state-of-the-art NER taggers, and observed better performance
in both precision and recall for our model. The ablation evaluation shows that
data augmentation boosts precision and recall by over 10%, while fine-tuning of
the T5 backbone boosts these metrics by 5-10%.

</details>


### [165] [Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset](https://arxiv.org/abs/2508.19467)
*Sumon Kanti Dey,Jeanne M. Powell,Azra Ismail,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.CL

TL;DR: 研究提出NER框架从社交媒体提取阿片类药物使用后果，评估模型，发现微调模型表现佳，强调领域微调价值，但与专家水平有差距。


<details>
  <summary>Details</summary>
Motivation: 非医疗阿片类药物使用是公共卫生挑战，传统医疗设置常漏报后果，社交媒体数据有价值但未充分利用。

Method: 提出NER框架，引入RedditImpacts 2.0数据集，评估微调编码器模型和大语言模型在零样本和少样本学习设置下的表现。

Result: 微调的DeBERTa - large模型表现佳，在精度等方面优于大语言模型，且少量标注数据也能实现强NER性能。

Conclusion: 强调领域特定微调对临床NLP任务的价值，有助于开发负责任的AI工具，但当前模型与专家水平有差距。

Abstract: Nonmedical opioid use is an urgent public health challenge, with far-reaching
clinical and social consequences that are often underreported in traditional
healthcare settings. Social media platforms, where individuals candidly share
first-person experiences, offer a valuable yet underutilized source of insight
into these impacts. In this study, we present a named entity recognition (NER)
framework to extract two categories of self-reported consequences from social
media narratives related to opioid use: ClinicalImpacts (e.g., withdrawal,
depression) and SocialImpacts (e.g., job loss). To support this task, we
introduce RedditImpacts 2.0, a high-quality dataset with refined annotation
guidelines and a focus on first-person disclosures, addressing key limitations
of prior work. We evaluate both fine-tuned encoder-based models and
state-of-the-art large language models (LLMs) under zero- and few-shot
in-context learning settings. Our fine-tuned DeBERTa-large model achieves a
relaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming
LLMs in precision, span accuracy, and adherence to task-specific guidelines.
Furthermore, we show that strong NER performance can be achieved with
substantially less labeled data, emphasizing the feasibility of deploying
robust models in resource-limited settings. Our findings underscore the value
of domain-specific fine-tuning for clinical NLP tasks and contribute to the
responsible development of AI tools that may enhance addiction surveillance,
improve interpretability, and support real-world healthcare decision-making.
The best performing model, however, still significantly underperforms compared
to inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap
persists between expert intelligence and current state-of-the-art NER/AI
capabilities for tasks requiring deep domain knowledge.

</details>


### [166] [Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval](https://arxiv.org/abs/2508.19758)
*Yixuan Tang,Yuanyuan Shi,Yiqun Sun,Anthony Kum Hoe Tung*

Main category: cs.CL

TL;DR: 提出用于多样化新闻检索的NEWSCOPE框架，引入评估指标和基准，实验显示其优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有新闻检索系统重文本相关性，导致结果冗余、观点有限，需提升事件报道多样性。

Method: 提出两阶段框架NEWSCOPE，第一阶段用密集检索获取相关内容，第二阶段进行句子级聚类和多样性重排序；引入三个评估指标和两个段落级基准。

Result: NEWSCOPE始终优于强基线，在不影响相关性的前提下显著提高多样性。

Conclusion: 细粒度、可解释建模能减少冗余，促进对事件的全面理解。

Abstract: Access to diverse perspectives is essential for understanding real-world
events, yet most news retrieval systems prioritize textual relevance, leading
to redundant results and limited viewpoint exposure. We propose NEWSCOPE, a
two-stage framework for diverse news retrieval that enhances event coverage by
explicitly modeling semantic variation at the sentence level. The first stage
retrieves topically relevant content using dense retrieval, while the second
stage applies sentence-level clustering and diversity-aware re-ranking to
surface complementary information. To evaluate retrieval diversity, we
introduce three interpretable metrics, namely Average Pairwise Distance,
Positive Cluster Coverage, and Information Density Ratio, and construct two
paragraph-level benchmarks: LocalNews and DSGlobal. Experiments show that
NEWSCOPE consistently outperforms strong baselines, achieving significantly
higher diversity without compromising relevance. Our results demonstrate the
effectiveness of fine-grained, interpretable modeling in mitigating redundancy
and promoting comprehensive event understanding. The data and code are
available at https://github.com/tangyixuan/NEWSCOPE.

</details>


### [167] [Selective Retrieval-Augmentation for Long-Tail Legal Text Classification](https://arxiv.org/abs/2508.19997)
*Boheng Mao*

Main category: cs.CL

TL;DR: 本文针对法律文本分类基准数据集长尾标签分布问题，提出选择性检索增强（SRA）方法，在两个数据集上测试显示能提升长尾巴法律文本分类性能。


<details>
  <summary>Details</summary>
Motivation: 法律文本分类基准数据集存在长尾标签分布，导致稀有类模型性能差。

Method: 提出SRA方法，对训练集中低频标签样本增强，仅从训练数据检索，不改变模型架构。

Result: SRA在两个长尾分布法律文本分类基准数据集上的微F1和宏F1分数高于当前LexGLUE所有基线。

Conclusion: SRA能持续改善长尾巴法律文本分类效果。

Abstract: Legal text classification is a fundamental NLP task in the legal domain.
Benchmark datasets in this area often exhibit a long-tail label distribution,
where many labels are underrepresented, leading to poor model performance on
rare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a
solution to this problem. SRA focuses on augmenting samples belonging to
low-frequency labels in the training set, preventing the introduction of noise
for well-represented classes, and requires no changes to the model
architecture. Retrieval is performed only from the training data to ensure
there is no potential information leakage, removing the need for external
corpora simultaneously. The proposed SRA method is tested on two legal text
classification benchmark datasets with long-tail distributions: LEDGAR
(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA
attains higher micro-F1 and macro-F1 scores compared to all current LexGLUE
baselines across both datasets, illustrating consistent improvements in
long-tail legal text classification. The code repository is available at:
https://github.com/Boheng-Mao/sra-legal

</details>


### [168] [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)
*Gueter Josmy Faure,Min-Hung Chen,Jia-Fong Yeh,Ying Cheng,Hung-Ting Su,Yung-Hao Tang,Shang-Hong Lai,Winston H. Hsu*

Main category: cs.CL

TL;DR: 介绍MovieCORE视频问答数据集，用创新方法生成问答对，开发评估方案，提出增强模块提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据集侧重表面理解，需深入探究电影内容的认知理解。

Method: 采用多LLM作为思维代理生成和优化问答对，开发认知测试和评估方案，提出Agentic Choice Enhancement模块。

Result: Agentic Choice Enhancement模块使模型训练后的推理能力最高提升25%。

Conclusion: 有助于推进AI系统的电影理解，揭示当前VQA模型面对复杂问题的能力和局限。

Abstract: This paper introduces MovieCORE, a novel video question answering (VQA)
dataset designed to probe deeper cognitive understanding of movie content.
Unlike existing datasets that focus on surface-level comprehension, MovieCORE
emphasizes questions that engage System-2 thinking while remaining specific to
the video material. We present an innovative agentic brainstorming approach,
utilizing multiple large language models (LLMs) as thought agents to generate
and refine high-quality question-answer pairs. To evaluate dataset quality, we
develop a set of cognitive tests assessing depth, thought-provocation
potential, and syntactic complexity. We also propose a comprehensive evaluation
scheme for assessing VQA model performance on deeper cognitive tasks. To
address the limitations of existing video-language models (VLMs), we introduce
an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves
model reasoning capabilities post-training by up to 25%. Our work contributes
to advancing movie understanding in AI systems and provides valuable insights
into the capabilities and limitations of current VQA models when faced with
more challenging, nuanced questions about cinematic content. Our project page,
dataset and code can be found at
https://joslefaure.github.io/assets/html/moviecore.html.

</details>


### [169] [MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts](https://arxiv.org/abs/2508.19268)
*Qing Wang,Xue Han,Jiahui Wang,Lehao Xing,Qian Hu,Lianlian Zhang,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 提出MultiPL - MoE提升大语言模型多编程语言性能，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型多语言代码生成难题，在有限计算资源下提升多编程语言性能。

Method: 将多编程语言视为多自然语言特例，提出MultiPL - MoE，结合两个成对的混合专家模型优化专家选择，在标记和片段层面各有设计。

Result: 实验证明了MultiPL - MoE的有效性。

Conclusion: MultiPL - MoE能有效提升大语言模型多编程语言性能。

Abstract: Despite LLMs' excellent code creation capabilities, multilingual code
generation remains extremely challenging. To address this, we intent to improve
the multi-programming-lingual (MultiPL) performance of the base LLMs while
retaining the most popular ones using restricted computational resources. We
consider MultiPL to be a special case of multiple natural languages and propose
a MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called
MultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize
expert selection at both the token and segment levels. The token-level MoE is a
standard upcycling MoE structure with a shared expert and a novel gate weight
normalization approach that aids in the final fusion with the segment-level
MoE. The segment-level MoE incorporates two innovative designs to better
capture the syntactic structure and contextual patterns of programming
languages: First, using a sliding window to partition the input token sequence
into multiple segments; Then, adopting an expert-choice routing strategy that
allows experts to select the top-k segments. The results of the experiment
proved the effectiveness of MultiPL-MoE.

</details>


### [170] [Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English](https://arxiv.org/abs/2508.19270)
*Nguyen Huu Nhat Minh,Tran Nguyen Anh,Truong Dinh Dung,Vo Van Nam,Le Pham Tuyen*

Main category: cs.CL

TL;DR: 提出一种跨越南语和英语的双语语音识别新方法，实验证明可提升识别准确率并提供解决复杂音素识别的框架。


<details>
  <summary>Details</summary>
Motivation: 解决越南语和英语混合发音时跨语言音素识别对自动语音识别的挑战，两种语言音素系统差异大。

Method: 构建代表双语音素集弥合两种语言语音系统差异；设计端到端系统，利用PhoWhisper预训练编码器进行深度高层表征以改善音素识别。

Result: 该方法提升了越南语双语语音识别的准确率。

Conclusion: 该方法为解决基于声调与重音的音素识别复杂性提供了可靠框架。

Abstract: Cross-lingual phoneme recognition has emerged as a significant challenge for
accurate automatic speech recognition (ASR) when mixing Vietnamese and English
pronunciations. Unlike many languages, Vietnamese relies on tonal variations to
distinguish word meanings, whereas English features stress patterns and
non-standard pronunciations that hinder phoneme alignment between the two
languages. To address this challenge, we propose a novel bilingual speech
recognition approach with two primary contributions: (1) constructing a
representative bilingual phoneme set that bridges the differences between
Vietnamese and English phonetic systems; (2) designing an end-to-end system
that leverages the PhoWhisper pre-trained encoder for deep high-level
representations to improve phoneme recognition. Our extensive experiments
demonstrate that the proposed approach not only improves recognition accuracy
in bilingual speech recognition for Vietnamese but also provides a robust
framework for addressing the complexities of tonal and stress-based phoneme
recognition

</details>


### [171] [Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT](https://arxiv.org/abs/2508.19271)
*Rushitha Santhoshi Mamidala,Anshuman Chhabra,Ankur Mali*

Main category: cs.CL

TL;DR: 传统提示推理方法不可靠，本文扩展RetoMaton，用本地自适应加权有限自动机替代全局数据存储，在多任务评估中提升性能，展示了向可信符号推理的转变。


<details>
  <summary>Details</summary>
Motivation: 现有提示推理策略（如CoT和ICL）不可靠，需要更结构化、可信的推理方法。

Method: 用本地、任务自适应的加权有限自动机替代RetoMaton的全局数据存储，利用自动机显式结构进行检索。

Result: 在两个预训练大模型和三个推理任务上评估，本地RetoMaton变体相比基础模型和提示方法，持续提升性能，实现透明和可重现的检索。

Conclusion: 通过轻量级自动机引导的内存，为现代大语言模型实现可信符号推理提供了有前景的方向。

Abstract: Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and
In-Context Learning (ICL) have become widely used for eliciting reasoning
capabilities in large language models (LLMs). However, these methods rely on
fragile, implicit mechanisms often yielding inconsistent outputs across seeds,
formats, or minor prompt variations making them fundamentally unreliable for
tasks requiring stable, interpretable reasoning. In contrast, automata-based
neuro-symbolic frameworks like RetoMaton offer a more structured and
trustworthy alternative by grounding retrieval in symbolic memory with
deterministic transitions. In this work, we extend RetoMaton by replacing its
global datastore with a local, task-adaptive Weighted Finite Automaton (WFA),
constructed directly from external domain corpora. This local automaton
structure promotes robust, context-aware retrieval while preserving symbolic
traceability and low inference overhead. Unlike prompting, which entangles
context and memory in opaque ways, our approach leverages the explicit
structure of WFAs to provide verifiable and modular retrieval behavior, making
it better suited for domain transfer and interoperability. We evaluate this
local RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT
across three reasoning tasks: TriviaQA (reading comprehension), GSM8K
(multi-step math), and MMLU (domain knowledge). Compared to the base model and
prompting-based methods, augmenting these setups with local RetoMaton
consistently improves performance while enabling transparent and reproducible
retrieval dynamics. Our results highlight a promising shift toward trustworthy,
symbolic reasoning in modern LLMs via lightweight, automaton-guided memory.

</details>


### [172] [FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series](https://arxiv.org/abs/2508.19279)
*Gunjan Jalori,Preetika Verma,Sercan Ö Arık*

Main category: cs.CL

TL;DR: 介绍了测试时提示优化框架FLAIRR - TS用于时间序列预测，通过代理系统自适应优化提示，实验显示性能优于基线，是调优的实用替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM时间序列预测依赖大量预处理和微调，精心设计提示困难且临时，需要更好的方法。

Method: 引入FLAIRR - TS框架，利用代理系统，Forecaster - agent用初始提示生成预测，Refiner agent根据过去输出和检索类比优化提示。

Result: 在基准数据集实验中，相比静态提示和检索增强基线，准确性提高，接近专业提示性能。

Conclusion: FLAIRR - TS通过代理方式进行自适应提示优化和检索，性能强，是调优的实用替代方案。

Abstract: Time series Forecasting with large languagemodels (LLMs) requires bridging
numericalpatterns and natural language. Effective fore-casting on LLM often
relies on extensive pre-processing and fine-tuning.Recent studiesshow that a
frozen LLM can rival specializedforecasters when supplied with a carefully
en-gineered natural-language prompt, but craft-ing such a prompt for each task
is itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt
optimization framework thatutilizes an agentic system: a
Forecaster-agentgenerates forecasts using an initial prompt,which is then
refined by a refiner agent, in-formed by past outputs and retrieved
analogs.This adaptive prompting generalizes across do-mains using creative
prompt templates andgenerates high-quality forecasts without inter-mediate code
generation.Experiments onbenchmark datasets show improved accuracyover static
prompting and retrieval-augmentedbaselines, approaching the performance
ofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning,
achievingstrong performance via its agentic approach toadaptive prompt
refinement and retrieval.

</details>


### [173] [CORE: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning](https://arxiv.org/abs/2508.19282)
*Ziqiang Cui,Yunpeng Weng,Xing Tang,Peiyang Liu,Shiwei Li,Bowei He,Jiamin Chen,Xiuqiang He,Chen Ma*

Main category: cs.CL

TL;DR: 提出CORE方法实现RAG无损上下文压缩，用强化学习优化，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG检索文档过长增加计算成本，压缩方法影响任务性能且缺乏明确目标。

Method: 提出CORE方法，利用强化学习，以端任务性能为奖励信号，用GRPO训练压缩机进行端到端训练。

Result: 在四个数据集上实验，压缩率3%，避免性能下降，平均EM分数提高3.3分。

Conclusion: CORE方法能有效实现RAG无损上下文压缩，提升性能。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
enhance the timeliness of knowledge and the factual accuracy of responses in
Large Language Models (LLMs). However, the inclusion of excessive retrieved
documents substantially increases the input length, leading to higher
computational costs. Previous studies have attempted to compress retrieved
documents into shorter texts before in-context integration, but such methods
often compromise end-task performance. The lack of well-defined compression
targets forces many approaches to rely on fixed heuristics, which cannot
guarantee that the compressed content will effectively support the end task. To
address these limitations, we propose CORE, a novel method designed to achieve
lossless context compression for RAG. CORE employs reinforcement learning to
optimize the compression process without relying on predefined compression
labels. Specifically, it utilizes end-task performance as a reward signal and
applies Generalized Reinforcement Learning Policy Optimization (GRPO) to train
the compressor. This end-to-end training framework enables the compressor to
generate summaries that maximize the accuracy of answers generated by the LLM.
Extensive experiments on four datasets demonstrate the superiority of our
approach. With a high compression ratio of 3\%, our method not only avoids
performance degradation compared to prepending full documents across all
datasets but also improves the average Exact Match (EM) score by 3.3 points.
The code will be released soon.

</details>


### [174] [Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction](https://arxiv.org/abs/2508.19359)
*Fatemeh Haji,Mazal Bethany,Cho-Yu Jason Chiang,Anthony Rios,Peyman Najafirad*

Main category: cs.CL

TL;DR: 提出混合方法ARIS解决事件抽取挑战，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统判别模型召回率有限，生成式方法有幻觉和预测不一致问题，需改进事件抽取。

Method: 提出ARIS，结合Self Mixture of Agents与判别式序列标注器，利用结构模型共识、基于置信度的过滤和LLM反思推理模块，还研究分解指令微调。

Result: 实验表明该方法在三个基准数据集上优于现有最先进的事件抽取方法。

Conclusion: ARIS有效提升了事件抽取的质量，是一种更优的事件抽取方法。

Abstract: Event Extraction (EE) involves automatically identifying and extracting
structured information about events from unstructured text, including triggers,
event types, and arguments. Traditional discriminative models demonstrate high
precision but often exhibit limited recall, particularly for nuanced or
infrequent events. Conversely, generative approaches leveraging Large Language
Models (LLMs) provide higher semantic flexibility and recall but suffer from
hallucinations and inconsistent predictions. To address these challenges, we
propose Agreement-based Reflective Inference System (ARIS), a hybrid approach
combining a Self Mixture of Agents with a discriminative sequence tagger. ARIS
explicitly leverages structured model consensus, confidence-based filtering,
and an LLM reflective inference module to reliably resolve ambiguities and
enhance overall event prediction quality. We further investigate decomposed
instruction fine-tuning for enhanced LLM event extraction understanding.
Experiments demonstrate our approach outperforms existing state-of-the-art
event extraction methods across three benchmark datasets.

</details>


### [175] [LongReasonArena: A Long Reasoning Benchmark for Large Language Models](https://arxiv.org/abs/2508.19363)
*Jiayu Ding,Shuming Ma,Lei Cui,Nanning Zheng,Furu Wei*

Main category: cs.CL

TL;DR: 提出LongReasonArena基准来评估大语言模型的长推理能力，评估显示对开源和闭源模型都有挑战。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型长上下文基准忽略长推理能力评估，需填补此空白。

Method: 引入LongReasonArena基准，任务要求模型执行多步算法解决问题，可任意缩放推理长度。

Result: LongReasonArena对开源和闭源大语言模型构成重大挑战，如Deepseek - R1准确率仅7.5%，准确率随预期推理步骤对数线性下降。

Conclusion: LongReasonArena能有效评估大语言模型长推理能力，代码和数据开源。

Abstract: Existing long-context benchmarks for Large Language Models (LLMs) focus on
evaluating comprehension of long inputs, while overlooking the evaluation of
long reasoning abilities. To address this gap, we introduce LongReasonArena, a
benchmark specifically designed to assess the long reasoning capabilities of
LLMs. Our tasks require models to solve problems by executing multi-step
algorithms that reflect key aspects of long reasoning, such as retrieval and
backtracking. By controlling the inputs, the required reasoning length can be
arbitrarily scaled, reaching up to 1 million tokens of reasoning for the most
challenging tasks. Extensive evaluation results demonstrate that
LongReasonArena presents a significant challenge for both open-source and
proprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our
task. Further analysis also reveals that the accuracy exhibits a linear decline
with respect to the logarithm of the expected number of reasoning steps. Our
code and data is available at
https://github.com/LongReasonArena/LongReasonArena.

</details>


### [176] [One Joke to Rule them All? On the (Im)possibility of Generalizing Humor](https://arxiv.org/abs/2508.19402)
*Mor Turgeman,Chen Shani,Dafna Shahaf*

Main category: cs.CL

TL;DR: 本文研究大语言模型在不同幽默类型任务间的迁移能力，通过实验发现模型有一定迁移能力，训练多样化数据可提升迁移性，还揭示了幽默类型间的关系。


<details>
  <summary>Details</summary>
Motivation: 现有计算幽默研究多聚焦特定幽默类型，新幽默类型不断涌现，需探究模型能否跨幽默类型泛化。

Method: 在四个代表不同幽默任务的数据集上进行一系列迁移学习实验，在不同多样性设置下训练大语言模型。

Result: 模型有一定迁移能力，在未见数据集上准确率达75%，训练多样化数据使迁移性提升1.88 - 4.05%，且不降低领域内性能，发现Dad Jokes最利于迁移但难被迁移。

Conclusion: 大语言模型能够在不同幽默类型任务间进行迁移，训练多样化数据可提升迁移能力，不同幽默类型间存在关系。

Abstract: Humor is a broad and complex form of communication that remains challenging
for machines. Despite its broadness, most existing research on computational
humor traditionally focused on modeling a specific type of humor. In this work,
we wish to understand whether competence on one or more specific humor tasks
confers any ability to transfer to novel, unseen types; in other words, is this
fragmentation inevitable? This question is especially timely as new humor types
continuously emerge in online and social media contexts (e.g., memes,
anti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this
evolving landscape, they must be able to generalize across humor types by
capturing deeper, transferable mechanisms. To investigate this, we conduct a
series of transfer learning experiments across four datasets, representing
different humor tasks. We train LLMs under varied diversity settings (1-3
datasets in training, testing on a novel task). Experiments reveal that models
are capable of some transfer, and can reach up to 75% accuracy on unseen
datasets; training on diverse sources improves transferability (1.88-4.05%)
with minimal-to-no drop in in-domain performance. Further analysis suggests
relations between humor types, with Dad Jokes surprisingly emerging as the best
enabler of transfer (but is difficult to transfer to). We release data and
code.

</details>


### [177] [A perishable ability? The future of writing in the face of generative artificial intelligence](https://arxiv.org/abs/2508.19427)
*Evandro L. T. P. Cunha*

Main category: cs.CL

TL;DR: 文章探讨生成式人工智能工具发展或使人类写作能力丧失或下降，类比历史上类似情况。


<details>
  <summary>Details</summary>
Motivation: 鉴于2020年代生成式人工智能工具发展，分析其对人类写作能力的影响。

Method: 将人工智能导致人类写作能力可能下降的情况与希腊黑暗时代人类写作能力丧失进行类比。

Result: 无明确提及具体研究结果。

Conclusion: 提出人类可能因将写作活动外包给机器而丧失或显著降低写作能力。

Abstract: The 2020s have been witnessing a very significant advance in the development
of generative artificial intelligence tools, including text generation systems
based on large language models. These tools have been increasingly used to
generate texts in the most diverse domains -- from technical texts to literary
texts --, which might eventually lead to a lower volume of written text
production by humans. This article discusses the possibility of a future in
which human beings will have lost or significantly decreased their ability to
write due to the outsourcing of this activity to machines. This possibility
parallels the loss of the ability to write in other moments of human history,
such as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).

</details>


### [178] [Bridging Language Gaps: Enhancing Few-Shot Language Adaptation](https://arxiv.org/abs/2508.19464)
*Philipp Borchert,Jochen De Weerdt,Marie-Francine Moens*

Main category: cs.CL

TL;DR: 提出CoLAP方法解决多语言NLP中资源不均衡问题，实验证明其能缩小跨语言性能差距，提升效率。


<details>
  <summary>Details</summary>
Motivation: 多语言NLP中高资源和低资源语言数据不均衡，低资源语言缺乏有效训练数据。

Method: 提出Contrastive Language Alignment with Prompting (CoLAP)方法，结合对比学习和跨语言表示，实现从高资源到低资源语言的任务特定知识迁移。

Result: 在自然语言理解任务实验中，CoLAP即使在数据有限时也优于少样本跨语言迁移基线和上下文学习。

Conclusion: CoLAP有效缩小跨语言性能差距，有助于开发更高效的多语言NLP技术。

Abstract: The disparity in language resources poses a challenge in multilingual NLP,
with high-resource languages benefiting from extensive data, while low-resource
languages lack sufficient data for effective training. Our Contrastive Language
Alignment with Prompting (CoLAP) method addresses this gap by integrating
contrastive learning with cross-lingual representations, facilitating
task-specific knowledge transfer from high-resource to lower-resource
languages. The primary advantage of our approach is its data efficiency,
enabling rapid adaptation to new languages and reducing the need for large
labeled datasets. We conduct experiments with multilingual encoder-only and
decoder-only language models on natural language understanding tasks, including
natural language inference and relation extraction, evaluating performance
across both high- and low-resource languages. Our results demonstrate that
CoLAP outperforms few-shot cross-lingual transfer baselines and in-context
learning, even with limited available data. This effectively narrows the
cross-lingual performance gap, contributing to the development of more
efficient multilingual NLP techniques.

</details>


### [179] [Automatic Question & Answer Generation Using Generative Large Language Model (LLM)](https://arxiv.org/abs/2508.19475)
*Md. Alvee Ehsan,A. S. M Mehedi Hasan,Kefaya Benta Shahnoor,Syeda Sumaiya Tasneem*

Main category: cs.CL

TL;DR: 本文提出利用微调生成式大语言模型实现自动问答生成，以简化教育领域文本评估的出题过程。


<details>
  <summary>Details</summary>
Motivation: 教育中人工出题评估学生较为困难，需手动翻阅大量资料，因此希望借助自动问答生成简化该过程。

Method: 利用无监督学习方法，以英语为主，让Meta - Llama 2 - 7B模型结合RACE数据集进行微调，并用提示工程定制问题风格。

Result: 创建了一个能为教育工作者和参与文本评估的人提供高效解决方案的定制模型。

Conclusion: 可靠高效的问答生成工具可节省时间和资源，简化评估流程。

Abstract: \Abstract{In the realm of education, student evaluation holds equal
significance as imparting knowledge. To be evaluated, students usually need to
go through text-based academic assessment methods. Instructors need to make
diverse sets of questions that need to be fair for all students to prove their
adequacy over a particular topic. This can prove to be quite challenging as
they may need to manually go through several different lecture materials. Our
objective is to make this whole process much easier by implementing Automatic
Question Answer Generation /(AQAG), using fine-tuned generative LLM. For
tailoring the instructor's preferred question style (MCQ, conceptual, or
factual questions), prompt Engineering (PE) is being utilized. In this
research, we propose to leverage unsupervised learning methods in NLP,
primarily focusing on the English language. This approach empowers the base
Meta-Llama 2-7B model to integrate RACE dataset as training data for the
fine-tuning process. Creating a customized model that will offer efficient
solutions for educators, instructors, and individuals engaged in text-based
evaluations. A reliable and efficient tool for generating questions and answers
can free up valuable time and resources, thus streamlining their evaluation
processes.}

</details>


### [180] [Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study](https://arxiv.org/abs/2508.19481)
*Manuel Mosquera,Melissa Robles,Johan Rodriguez,Ruben Manrique*

Main category: cs.CL

TL;DR: 提出结合外部词典工具和强化学习提升低资源语言翻译的方法，在西语 - 瓦尤纳伊基语对上有BLEU提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在低资源机器翻译上有挑战，预训练缺相关语言且微调平行数据有限。

Method: 将翻译视为工具增强决策问题，结合监督指令微调与GRPO，用BLEU分数作奖励。

Result: 工具增强模型在测试集上比之前工作BLEU提升3.37，比无词典监督基线相对增益18%，还进行消融研究。

Conclusion: 结合大语言模型与外部工具及强化学习对提升低资源语言翻译质量有前景。

Abstract: Low-resource machine translation remains a significant challenge for large
language models (LLMs), which often lack exposure to these languages during
pretraining and have limited parallel data for fine-tuning. We propose a novel
approach that enhances translation for low-resource languages by integrating an
external dictionary tool and training models end-to-end using reinforcement
learning, in addition to supervised fine-tuning. Focusing on the
Spanish-Wayuunaiki language pair, we frame translation as a tool-augmented
decision-making problem in which the model can selectively consult a bilingual
dictionary during generation. Our method combines supervised instruction tuning
with Guided Reward Policy Optimization (GRPO), enabling the model to learn both
when and how to use the tool effectively. BLEU similarity scores are used as
rewards to guide this learning process. Preliminary results show that our
tool-augmented models achieve up to +3.37 BLEU improvement over previous work,
and a 18% relative gain compared to a supervised baseline without dictionary
access, on the Spanish-Wayuunaiki test set from the AmericasNLP 2025 Shared
Task. We also conduct ablation studies to assess the effects of model
architecture and training strategy, comparing Qwen2.5-0.5B-Instruct with other
models such as LLaMA and a prior NLLB-based system. These findings highlight
the promise of combining LLMs with external tools and the role of reinforcement
learning in improving translation quality in low-resource language settings.

</details>


### [181] [Language Models Identify Ambiguities and Exploit Loopholes](https://arxiv.org/abs/2508.19546)
*Jio Choi,Mohit Bansal,Elias Stengel-Eskin*

Main category: cs.CL

TL;DR: 研究大语言模型对漏洞的响应，设计场景测试模型利用漏洞能力，发现模型存在利用漏洞风险。


<details>
  <summary>Details</summary>
Motivation: 通过研究大语言模型对漏洞的响应，考察模型的模糊性和语用学，解决模型面临冲突目标时的对齐问题。

Method: 设计模型面临目标与模糊用户指令冲突的场景，涵盖多种类型歧义，测量模型利用漏洞满足自身目标的能力。

Result: 闭源和更强的开源模型能识别歧义并利用漏洞，存在潜在AI安全风险。

Conclusion: 利用漏洞的模型会明确识别和推理歧义与冲突目标。

Abstract: Studying the responses of large language models (LLMs) to loopholes presents
a two-fold opportunity. First, it affords us a lens through which to examine
ambiguity and pragmatics in LLMs, since exploiting a loophole requires
identifying ambiguity and performing sophisticated pragmatic reasoning. Second,
loopholes pose an interesting and novel alignment problem where the model is
presented with conflicting goals and can exploit ambiguities to its own
advantage. To address these questions, we design scenarios where LLMs are given
a goal and an ambiguous user instruction in conflict with the goal, with
scenarios covering scalar implicature, structural ambiguities, and power
dynamics. We then measure different models' abilities to exploit loopholes to
satisfy their given goals as opposed to the goals of the user. We find that
both closed-source and stronger open-source models can identify ambiguities and
exploit their resulting loopholes, presenting a potential AI safety risk. Our
analysis indicates that models which exploit loopholes explicitly identify and
reason about both ambiguity and conflicting goals.

</details>


### [182] [Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts](https://arxiv.org/abs/2508.19578)
*Jiaqi Deng,Yuho Lee,Nicole Hee-Yeon Kim,Hyangsuk Min,Taewon Yun,Minjeong Ban,Kim Yul,Hwanjun Song*

Main category: cs.CL

TL;DR: 介绍用于评估大语言模型长上下文理解能力的框架HAMLET，经人工验证可靠，揭示模型存在的问题，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型的长上下文理解能力。

Method: 将源文本构建为三级关键事实层次结构，采用查询聚焦摘要方法评估模型，进行系统的人工研究验证自动评估的可靠性。

Result: 自动评估与专家人工判断的一致性超90%，成本降低达25倍；发现模型在细粒度理解上有困难，对位置效应敏感，分析性查询挑战更大，不同类型和规模模型存在性能差距。

Conclusion: HAMLET是评估大语言模型长上下文理解能力的有效框架。

Abstract: We introduce HAMLET, a holistic and automated framework for evaluating the
long-context comprehension of large language models (LLMs). HAMLET structures
source texts into a three-level key-fact hierarchy at root-, branch-, and
leaf-levels, and employs query-focused summarization to evaluate how well
models recall and faithfully represent information at each level. To validate
the reliability of our fully automated pipeline, we conduct a systematic human
study, showing that our automatic evaluation achieves over 90% agreement with
expert human judgments, while reducing the cost by up to 25 times. HAMLET
reveals that LLMs struggle with fine-grained comprehension, especially at the
leaf level, and are sensitive to positional effects like the
lost-in-the-middle. Analytical queries pose greater challenges than narrative
ones, and consistent performance gaps emerge between open-source and
proprietary models, as well as across model scales. Our code and dataset are
publicly available at https://github.com/DISL-Lab/HAMLET.

</details>


### [183] [Towards stable AI systems for Evaluating Arabic Pronunciations](https://arxiv.org/abs/2508.19587)
*Hadi Zaatiti,Hatem Hajri,Osama Abdullah,Nader Masmoudi*

Main category: cs.CL

TL;DR: 现代阿拉伯语ASR系统在孤立字母分类任务上表现不佳，本文引入相关语料，测试模型表现，用对抗训练提升鲁棒性并公布数据代码，还规划了未来工作。


<details>
  <summary>Details</summary>
Motivation: 现代阿拉伯语ASR系统在孤立字母分类这一重要音素级任务上存在困难，开展研究以解决该问题。

Method: 引入多样化、带音符的孤立阿拉伯字母语料，在wav2vec嵌入上训练轻量级神经网络，应用对抗训练。

Result: 最先进的wav2vec 2.0模型准确率仅35%，轻量级神经网络提升至65%，加小幅度扰动降至32%，对抗训练使噪声语音准确率下降控制在9%且保持干净语音准确率。

Conclusion: 该研究为阿拉伯语孤立字母分类提供了解决方案，未来可将方法拓展到单词和句子层面框架。

Abstract: Modern Arabic ASR systems such as wav2vec 2.0 excel at word- and
sentence-level transcription, yet struggle to classify isolated letters. In
this study, we show that this phoneme-level task, crucial for language
learning, speech therapy, and phonetic research, is challenging because
isolated letters lack co-articulatory cues, provide no lexical context, and
last only a few hundred milliseconds. Recogniser systems must therefore rely
solely on variable acoustic cues, a difficulty heightened by Arabic's emphatic
(pharyngealized) consonants and other sounds with no close analogues in many
languages. This study introduces a diverse, diacritised corpus of isolated
Arabic letters and demonstrates that state-of-the-art wav2vec 2.0 models
achieve only 35% accuracy on it. Training a lightweight neural network on
wav2vec embeddings raises performance to 65%. However, adding a small amplitude
perturbation (epsilon = 0.05) cuts accuracy to 32%. To restore robustness, we
apply adversarial training, limiting the noisy-speech drop to 9% while
preserving clean-speech accuracy. We detail the corpus, training pipeline, and
evaluation protocol, and release, on demand, data and code for reproducibility.
Finally, we outline future work extending these methods to word- and
sentence-level frameworks, where precise letter pronunciation remains critical.

</details>


### [184] [LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.19614)
*Yang Sun,Lixin Zou,Dan Luo,Zhiyong Xie,Long Zhang,Liming Dong,Yunwei Zhao,Xixun Lin,Yanxiong Lu,Chenliang Li*

Main category: cs.CL

TL;DR: 本文研究RAG中噪声注入现象，提出LFD解码策略及IKS准则，实验表明LFD能低成本提升RAG系统利用外部知识的能力。


<details>
  <summary>Details</summary>
Motivation: 基于RAG中注入噪声能提升知识利用和生成质量这一现象，对LLM内部知识整合进行精细控制和分析。

Method: 对噪声注入进行干预，建立LLM层特定功能划分，提出LFD解码策略和IKS准则选择最优中间层。

Result: 在多个基准测试中，LFD帮助RAG系统以最小成本更有效地挖掘检索到的上下文知识。

Conclusion: LFD解码策略能有效提升RAG系统利用外部知识的能力。

Abstract: Retrieval-augmented generation (RAG) incorporates external knowledge into
large language models (LLMs), improving their adaptability to downstream tasks
and enabling information updates. Surprisingly, recent empirical evidence
demonstrates that injecting noise into retrieved relevant documents
paradoxically facilitates exploitation of external knowledge and improves
generation quality. Although counterintuitive and challenging to apply in
practice, this phenomenon enables granular control and rigorous analysis of how
LLMs integrate external knowledge. Therefore, in this paper, we intervene on
noise injection and establish a layer-specific functional demarcation within
the LLM: shallow layers specialize in local context modeling, intermediate
layers focus on integrating long-range external factual knowledge, and deeper
layers primarily rely on parametric internal knowledge. Building on this
insight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that
directly combines representations from an intermediate layer with final-layer
decoding outputs to fully exploit the external factual knowledge. To identify
the optimal intermediate layer, we introduce an internal knowledge score (IKS)
criterion that selects the layer with the lowest IKS value in the latter half
of layers. Experimental results across multiple benchmarks demonstrate that LFD
helps RAG systems more effectively surface retrieved context knowledge with
minimal cost.

</details>


### [185] [Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis](https://arxiv.org/abs/2508.19831)
*Anusha Kamath,Kanishk Singla,Rakesh Paul,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 本文针对印地语指令调优大语言模型缺乏高质量基准的问题，引入五个评估数据集，对支持印地语的开源大模型进行基准测试，并为低资源语言开发基准提供可复制方法。


<details>
  <summary>Details</summary>
Motivation: 印地语指令调优大语言模型缺乏高质量基准，直接翻译英文数据集无法捕捉语言和文化细微差别。

Method: 结合从头开始的人工标注和翻译验证过程创建五个印地语评估数据集。

Result: 对支持印地语的开源大语言模型进行广泛基准测试，提供详细的能力对比分析。

Conclusion: 提出的数据集创建过程可作为开发其他低资源语言基准的可复制方法。

Abstract: Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is
challenging due to a lack of high-quality benchmarks, as direct translation of
English datasets fails to capture crucial linguistic and cultural nuances. To
address this, we introduce a suite of five Hindi LLM evaluation datasets:
IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created
using a methodology that combines from-scratch human annotation with a
translate-and-verify process. We leverage this suite to conduct an extensive
benchmarking of open-source LLMs supporting Hindi, providing a detailed
comparative analysis of their current capabilities. Our curation process also
serves as a replicable methodology for developing benchmarks in other
low-resource languages.

</details>


### [186] [Survey of Specialized Large Language Model](https://arxiv.org/abs/2508.19667)
*Chenghan Yang,Ruiyu Zhao,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: 本文探讨专业大语言模型从简单领域适配到复杂原生架构的演变，分析各领域应用及创新，指出其解决通用模型局限，还提及对电商领域的意义。


<details>
  <summary>Details</summary>
Motivation: 随着专业大语言模型快速发展，需系统研究其从简单适配到复杂架构的演变及在多领域应用。

Method: 对医疗、金融、法律和技术等领域专业大语言模型的发展进行系统考察分析。

Result: 发现专业大语言模型的创新能解决通用模型在专业应用中的局限，在特定领域基准测试中有性能提升。

Conclusion: 强调这些进展对电商领域有填补空白的意义。

Abstract: The rapid evolution of specialized large language models (LLMs) has
transitioned from simple domain adaptation to sophisticated native
architectures, marking a paradigm shift in AI development. This survey
systematically examines this progression across healthcare, finance, legal, and
technical domains. Besides the wide use of specialized LLMs, technical
breakthrough such as the emergence of domain-native designs beyond fine-tuning,
growing emphasis on parameter efficiency through sparse computation and
quantization, increasing integration of multimodal capabilities and so on are
applied to recent LLM agent. Our analysis reveals how these innovations address
fundamental limitations of general-purpose LLMs in professional applications,
with specialized models consistently performance gains on domain-specific
benchmarks. The survey further highlights the implications for E-Commerce field
to fill gaps in the field.

</details>


### [187] [11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis](https://arxiv.org/abs/2508.20068)
*Chengzu Li,Wenshan Wu,Huanyu Zhang,Qingtao Li,Zeyu Gao,Yan Xia,José Hernández-Orallo,Ivan Vulić,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出系统评估框架，用11Plus - Bench基准测试评估MLLMs空间推理能力，发现MLLMs有空间认知迹象但与人类差距大。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）在人类空间认知与推理交互方面研究不足，需评估其空间推理能力。

Method: 引入系统评估框架，使用11Plus - Bench基准测试，结合14个MLLMs实验和人类评估。

Result: 当前MLLMs有空间认知早期迹象，与人类有较大性能差距，认知特征有相似性，但实例级表现MLLMs随机，人类表现可预测。

Conclusion: 指出当前MLLMs空间推理能力的优势与局限，为模型设计改进提供见解。

Abstract: For human cognitive process, spatial reasoning and perception are closely
entangled, yet the nature of this interplay remains underexplored in the
evaluation of multimodal large language models (MLLMs). While recent MLLM
advancements show impressive performance on reasoning, their capacity for
human-like spatial cognition remains an open question. In this work, we
introduce a systematic evaluation framework to assess the spatial reasoning
abilities of state-of-the-art MLLMs relative to human performance. Central to
our work is 11Plus-Bench, a high-quality benchmark derived from realistic
standardized spatial aptitude tests. 11Plus-Bench also features fine-grained
expert annotations of both perceptual complexity and reasoning process,
enabling detailed instance-level analysis of model behavior. Through extensive
experiments across 14 MLLMs and human evaluation, we find that current MLLMs
exhibit early signs of spatial cognition. Despite a large performance gap
compared to humans, MLLMs' cognitive profiles resemble those of humans in that
cognitive effort correlates strongly with reasoning-related complexity.
However, instance-level performance in MLLMs remains largely random, whereas
human correctness is highly predictable and shaped by abstract pattern
complexity. These findings highlight both emerging capabilities and limitations
in current MLLMs' spatial reasoning capabilities and provide actionable
insights for advancing model design.

</details>


### [188] [NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks](https://arxiv.org/abs/2508.19724)
*Aritra Dutta,Swapnanil Mukherjee,Deepanway Ghosal,Somak Aditya*

Main category: cs.CL

TL;DR: 提出端到端框架NLKI，将常识知识集成到小视觉语言模型，提升模型性能，分析不同策略效果。


<details>
  <summary>Details</summary>
Motivation: 研究仔细集成常识知识对小视觉语言模型在常识视觉问答任务中的影响。

Method: 提出NLKI框架，包括检索自然语言事实、用大语言模型生成解释，将信号输入小视觉语言模型；使用微调的ColBERTv2检索事实；用抗噪声损失函数微调。

Result: 减少幻觉，端到端答案准确率最高提升7%，让小模型性能匹配或超越中等模型；抗噪声损失微调在不同数据集额外提升2.5% - 5.5%。

Conclusion: 指出大语言模型常识知识何时优于知识库检索，抗噪声训练稳定小模型，250M模型实现高效常识推理成为可能。

Abstract: Commonsense visual-question answering often hinges on knowledge that is
missing from the image or the question. Small vision-language models (sVLMs)
such as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative
counterparts. To study the effect of careful commonsense knowledge integration
on sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural
language facts, (ii) prompts an LLM to craft natural language explanations, and
(iii) feeds both signals to sVLMs respectively across two commonsense VQA
datasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts
retrieved using a fine-tuned ColBERTv2 and an object information-enriched
prompt yield explanations that largely cut down hallucinations, while lifting
the end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA
and other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B
and SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional
finetuning using noise-robust losses (such as symmetric cross entropy and
generalised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our
findings expose when LLM-based commonsense knowledge beats retrieval from
commonsense knowledge bases, how noise-aware training stabilises small models
in the context of external knowledge augmentation, and why parameter-efficient
commonsense reasoning is now within reach for 250M models.

</details>


### [189] [AI-Powered Detection of Inappropriate Language in Medical School Curricula](https://arxiv.org/abs/2508.19883)
*Chiman Salavati,Shannon Song,Scott A. Hale,Roberto E. Montenegro,Shiri Dori-Hacohen,Fabricio Murai*

Main category: cs.CL

TL;DR: 研究评估小语言模型（SLMs）和大语言模型（LLMs）检测医学教材中不当语言的效果，发现SLMs表现更好，补充未标记段落作负例可提升特定分类器性能。


<details>
  <summary>Details</summary>
Motivation: 医学教学材料中存在大量不符合当前标准的不当语言，手动识别成本高且不现实，需自动化方法检测。

Method: 在约500份文件、超12000页的数据集上评估微调的SLMs和基于上下文学习的预训练LLMs，对SLMs采用多种分类器，对LLMs采用不同提示方式。

Result: LLama - 3 8B和70B即使有精心挑选的示例也大多不如SLMs，多标签分类器在标注数据上表现最佳，补充负例可使特定分类器AUC提升达25%。

Conclusion: 补充负例训练的特定分类器是减轻医学课程中有害语言的最有效模型。

Abstract: The use of inappropriate language -- such as outdated, exclusionary, or
non-patient-centered terms -- medical instructional materials can significantly
influence clinical training, patient interactions, and health outcomes. Despite
their reputability, many materials developed over past decades contain examples
now considered inappropriate by current medical standards. Given the volume of
curricular content, manually identifying instances of inappropriate use of
language (IUL) and its subcategories for systematic review is prohibitively
costly and impractical. To address this challenge, we conduct a first-in-class
evaluation of small language models (SLMs) fine-tuned on labeled data and
pre-trained LLMs with in-context learning on a dataset containing approximately
500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL
classifier, (2) subcategory-specific binary classifiers, (3) a multilabel
classifier, and (4) a two-stage hierarchical pipeline for general IUL detection
followed by multilabel classification. For LLMs, we consider variations of
prompts that include subcategory definitions and/or shots. We found that both
LLama-3 8B and 70B, even with carefully curated shots, are largely outperformed
by SLMs. While the multilabel classifier performs best on annotated data,
supplementing training with unflagged excerpts as negative examples boosts the
specific classifiers' AUC by up to 25%, making them most effective models for
mitigating harmful language in medical curricula.

</details>


### [190] [Logical Reasoning with Outcome Reward Models for Test-Time Scaling](https://arxiv.org/abs/2508.19903)
*Ramya Keerthy Thatikonda,Wray Buntine,Ehsan Shareghi*

Main category: cs.CL

TL;DR: 提出用于演绎推理的结果奖励模型（ORMs），结合思维链（CoT）和回声生成技术扩充训练数据，在多个数据集上提升了不同大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在演绎逻辑推理领域探索不足，需要提升大语言模型在演绎逻辑推理任务中的性能。

Method: 提出演绎推理的ORMs，使用CoT单样本和多样本生成数据训练ORMs，提出回声生成技术扩充训练数据。

Result: 在FOLIO、JustLogic和ProverQA数据集上，基于CoT和回声增强数据训练的ORMs在四个不同大语言模型上表现更好。

Conclusion: 提出的方法有效提升了大语言模型在演绎逻辑推理任务中的性能。

Abstract: Logical reasoning is a critical benchmark for evaluating the capabilities of
large language models (LLMs), as it reflects their ability to derive valid
conclusions from given premises. While the combination of test-time scaling
with dedicated outcome or process reward models has opened up new avenues to
enhance LLMs performance in complex reasoning tasks, this space is
under-explored in deductive logical reasoning. We present a set of Outcome
Reward Models (ORMs) for deductive reasoning. To train the ORMs we mainly
generate data using Chain-of-Thought (CoT) with single and multiple samples.
Additionally, we propose a novel tactic to further expand the type of errors
covered in the training dataset of the ORM. In particular, we propose an echo
generation technique that leverages LLMs' tendency to reflect incorrect
assumptions made in prompts to extract additional training data, covering
previously unexplored error types. While a standard CoT chain may contain
errors likely to be made by the reasoner, the echo strategy deliberately steers
the model toward incorrect reasoning. We show that ORMs trained on CoT and
echo-augmented data demonstrate improved performance on the FOLIO, JustLogic,
and ProverQA datasets across four different LLMs.

</details>


### [191] [Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation](https://arxiv.org/abs/2508.19966)
*Slimane Bellaouar,Attia Nehar,Soumia Souffi,Mounia Bouameur*

Main category: cs.CL

TL;DR: 本文针对阿拉伯语主观性分析资源不足问题，提出新方法，构建数据集，微调模型并采用集成决策，实现97.79%准确率。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语资源不足，缺乏大型标注数据集，阻碍主观性分析工具发展。

Method: 构建综合数据集AraDhati+，在该数据集上微调XLM - RoBERTa、AraBERT和ArabianGPT等模型，并采用集成决策方法。

Result: 阿拉伯语主观性分类准确率达97.79%。

Conclusion: 所提方法能有效解决阿拉伯语处理资源有限的挑战。

Abstract: Despite its significance, Arabic, a linguistically rich and morphologically
complex language, faces the challenge of being under-resourced. The scarcity of
large annotated datasets hampers the development of accurate tools for
subjectivity analysis in Arabic. Recent advances in deep learning and
Transformers have proven highly effective for text classification in English
and French. This paper proposes a new approach for subjectivity assessment in
Arabic textual data. To address the dearth of specialized annotated datasets,
we developed a comprehensive dataset, AraDhati+, by leveraging existing Arabic
datasets and collections (ASTD, LABR, HARD, and SANAD). Subsequently, we
fine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, and
ArabianGPT) on AraDhati+ for effective subjectivity classification.
Furthermore, we experimented with an ensemble decision approach to harness the
strengths of individual models. Our approach achieves a remarkable accuracy of
97.79\,\% for Arabic subjectivity classification. Results demonstrate the
effectiveness of the proposed approach in addressing the challenges posed by
limited resources in Arabic language processing.

</details>


### [192] [Diffusion Language Models Know the Answer Before Decoding](https://arxiv.org/abs/2508.19982)
*Pengxiang Li,Yefan Zhou,Dilxat Muhtar,Lu Yin,Shilin Yan,Li Shen,Yi Liang,Soroush Vosoughi,Shiwei Liu*

Main category: cs.CL

TL;DR: 本文指出扩散语言模型（DLMs）存在早期答案收敛特性，提出无训练快速解码范式Prophet，减少解码步骤并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决DLMs推理速度慢的问题，因其双向注意力成本和高质量输出所需的大量细化步骤。

Method: 利用DLMs早期答案收敛特性，提出Prophet范式，以top - 2预测候选的置信度差距为标准动态决定是否继续细化或一步解码剩余标记。

Result: 在多个任务上对LLaDA - 8B和Dream - 7B的实证评估表明，Prophet最多可将解码步骤减少3.4倍，同时保持高生成质量。

Conclusion: DLM解码可视为何时停止采样的问题，早期解码收敛为加速DLM推理提供了简单有效的机制，可与现有加速技术互补。

Abstract: Diffusion language models (DLMs) have recently emerged as an alternative to
autoregressive approaches, offering parallel sequence generation and flexible
token orders. However, their inference remains slower than that of
autoregressive models, primarily due to the cost of bidirectional attention and
the large number of refinement steps required for high quality outputs. In this
work, we highlight and leverage an overlooked property of DLMs early answer
convergence: in many cases, the correct answer can be internally identified by
half steps before the final decoding step, both under semi-autoregressive and
random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%
of instances, respectively, can be decoded correctly using only half of the
refinement steps. Building on this observation, we introduce Prophet, a
training-free fast decoding paradigm that enables early commit decoding.
Specifically, Prophet dynamically decides whether to continue refinement or to
go "all-in" (i.e., decode all remaining tokens in one step), using the
confidence gap between the top-2 prediction candidates as the criterion. It
integrates seamlessly into existing DLM implementations, incurs negligible
overhead, and requires no additional training. Empirical evaluations of
LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the
number of decoding steps by up to 3.4x while preserving high generation
quality. These results recast DLM decoding as a problem of when to stop
sampling, and demonstrate that early decode convergence provides a simple yet
powerful mechanism for accelerating DLM inference, complementary to existing
speedup techniques. Our code is publicly available at
https://github.com/pixeli99/Prophet.

</details>


### [193] [MathBuddy: A Multimodal System for Affective Math Tutoring](https://arxiv.org/abs/2508.19993)
*Debanjana Kar,Leopold Böss,Dacia Braca,Sebastian Maximilian Dennerlein,Nina Christine Hubig,Philipp Wintersberger,Yufang Hou*

Main category: cs.CL

TL;DR: 提出情感感知的数学辅导系统MathBuddy，通过考虑学生情感提升教学能力，评估显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有学习模型未考虑学生情感状态，而情感会影响学习能力，需弥补此差距。

Method: 构建MathBuddy系统，从对话文本和面部表情捕捉学生情感，聚合后提示LLM导师给出情感感知回应。

Result: 使用自动评估指标和用户研究评估模型，胜率有23分提升，DAMR总分有3分提升。

Conclusion: 通过建模学生情感能有效提高基于LLM的导师的教学能力。

Abstract: The rapid adoption of LLM-based conversational systems is already
transforming the landscape of educational technology. However, the current
state-of-the-art learning models do not take into account the student's
affective states. Multiple studies in educational psychology support the claim
that positive or negative emotional states can impact a student's learning
capabilities. To bridge this gap, we present MathBuddy, an emotionally aware
LLM-powered Math Tutor, which dynamically models the student's emotions and
maps them to relevant pedagogical strategies, making the tutor-student
conversation a more empathetic one. The student's emotions are captured from
the conversational text as well as from their facial expressions. The student's
emotions are aggregated from both modalities to confidently prompt our LLM
Tutor for an emotionally-aware response. We have effectively evaluated our
model using automatic evaluation metrics across eight pedagogical dimensions
and user studies. We report a massive 23 point performance gain using the win
rate and a 3 point gain at an overall level using DAMR scores which strongly
supports our hypothesis of improving LLM-based tutor's pedagogical abilities by
modeling students' emotions.

</details>


### [194] [DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis](https://arxiv.org/abs/2508.20033)
*Liana Patel,Negar Arabzadeh,Harshit Gupta,Ankita Sundar,Ion Stoica,Matei Zaharia,Carlos Guestrin*

Main category: cs.CL

TL;DR: 本文介绍了用于评估生成式研究合成的DeepScholar - bench基准和评估框架，对现有系统进行评估，发现DeepScholar - base表现良好，且该基准远未饱和。


<details>
  <summary>Details</summary>
Motivation: 现有问答基准和专家整理数据集无法有效评估生成式研究合成系统，需新的评估方法。

Method: 引入DeepScholar - bench基准，从近期高质量ArXiv论文提取查询，聚焦生成论文相关工作部分；评估框架从知识合成、检索质量和可验证性三个维度评估；开发DeepScholar - base参考管道。

Result: DeepScholar - base建立了强大基线，性能优于其他方法；所有系统在所有指标上得分未超19%。

Conclusion: DeepScholar - bench有难度且对生成式研究合成AI系统发展很重要，代码已开源。

Abstract: The ability to research and synthesize knowledge is central to human
expertise and progress. An emerging class of systems promises these exciting
capabilities through generative research synthesis, performing retrieval over
the live web and synthesizing discovered sources into long-form, cited
summaries. However, evaluating such systems remains an open challenge: existing
question-answering benchmarks focus on short-form factual responses, while
expert-curated datasets risk staleness and data contamination. Both fail to
capture the complexity and evolving nature of real research synthesis tasks. In
this work, we introduce DeepScholar-bench, a live benchmark and holistic,
automated evaluation framework designed to evaluate generative research
synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv
papers and focuses on a real research synthesis task: generating the related
work sections of a paper by retrieving, synthesizing, and citing prior
research. Our evaluation framework holistically assesses performance across
three key dimensions, knowledge synthesis, retrieval quality, and
verifiability. We also develop DeepScholar-base, a reference pipeline
implemented efficiently using the LOTUS API. Using the DeepScholar-bench
framework, we perform a systematic evaluation of prior open-source systems,
search AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that
DeepScholar-base establishes a strong baseline, attaining competitive or higher
performance than each other method. We also find that DeepScholar-bench remains
far from saturated, with no system exceeding a score of $19\%$ across all
metrics. These results underscore the difficulty of DeepScholar-bench, as well
as its importance for progress towards AI systems capable of generative
research synthesis. We make our code available at
https://github.com/guestrin-lab/deepscholar-bench.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [195] [Epistemic Trade-Off: An Analysis of the Operational Breakdown and Ontological Limits of "Certainty-Scope" in AI](https://arxiv.org/abs/2508.19304)
*Generoso Immediato*

Main category: cs.CY

TL;DR: 本文分析了弗洛里迪猜想在人工智能领域的局限性，指出其难以操作化，并提出应对其认知挑战的建议。


<details>
  <summary>Details</summary>
Motivation: 探讨弗洛里迪猜想在指导人工智能投资，尤其是安全关键工业领域的可行性，以及其对工程设计和监管决策的指导作用。

Method: 分析该猜想存在的两个关键限制因素，一是依赖不可计算结构，二是本体论假设脱离现实社会技术环境。

Result: 发现该猜想存在认知封闭不足和未考虑嵌入性的问题，无法转变为可计算和可操作的框架。

Conclusion: 该猜想无法为现实世界的人工智能混合系统设计、部署和治理提供有效框架，需应对其在复杂以人为中心领域的认知负担。

Abstract: Floridi's conjecture offers a compelling intuition about the fundamental
trade-off between certainty and scope in artificial intelligence (AI) systems.
This exploration remains crucial, not merely as a philosophical exercise, but
as a potential compass for guiding AI investments, particularly in
safety-critical industrial domains where the level of attention will surely be
higher in the future. However, while intellectually coherent, its formalization
ultimately freezes this insight into a suspended epistemic truth, resisting
operationalization within real-world systems. This paper is a result of an
analysis arguing that the conjecture's ambition to provide insights to
engineering design and regulatory decision-making is constrained by two
critical factors: first, its reliance on incomputable constructs - rendering it
practically unactionable and unverifiable; second, its underlying ontological
assumption of AI systems as self-contained epistemic entities - separating it
from the intricate and dynamic socio-technical environments in which knowledge
is co-constructed. We conclude that this dual breakdown - an epistemic closure
deficit and an embeddedness bypass - prevents the conjecture from transitioning
into a computable and actionable framework suitable for informing the design,
deployment, and governance of real-world AI hybrid systems. In response, we
propose a contribution to the framing of Floridi's epistemic challenge,
addressing the inherent epistemic burdens of AI within complex human-centric
domains.

</details>


### [196] [Should LLMs be WEIRD? Exploring WEIRDness and Human Rights in Large Language Models](https://arxiv.org/abs/2508.19269)
*Ke Zhou,Marios Constantinides,Daniele Quercia*

Main category: cs.CY

TL;DR: 评估五个大语言模型在价值观上与WEIRD国家的契合度及对人权原则的遵循情况，发现文化多样性增加时产生歧视性信念的风险也增加，Constitutional AI只能部分解决此矛盾。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常基于反映WEIRD价值观的数据训练，引发对文化偏见和公平性的担忧。

Method: 用世界价值观调查的回复评估五个大语言模型，测量其与WEIRD国家价值观的契合度和是否违背人权原则，与《世界人权宣言》及三个地区宪章对比。

Result: 与WEIRD价值观契合度低的模型文化多样性更高，但产生违反人权输出的可能性高2% - 4%，存在有害性别规范。

Conclusion: 大语言模型文化代表性增加时，产生歧视性信念的风险也增加，Constitutional AI只能部分解决此矛盾。

Abstract: Large language models (LLMs) are often trained on data that reflect WEIRD
values: Western, Educated, Industrialized, Rich, and Democratic. This raises
concerns about cultural bias and fairness. Using responses to the World Values
Survey, we evaluated five widely used LLMs: GPT-3.5, GPT-4, Llama-3, BLOOM, and
Qwen. We measured how closely these responses aligned with the values of the
WEIRD countries and whether they conflicted with human rights principles. To
reflect global diversity, we compared the results with the Universal
Declaration of Human Rights and three regional charters from Asia, the Middle
East, and Africa. Models with lower alignment to WEIRD values, such as BLOOM
and Qwen, produced more culturally varied responses but were 2% to 4% more
likely to generate outputs that violated human rights, especially regarding
gender and equality. For example, some models agreed with the statements ``a
man who cannot father children is not a real man'' and ``a husband should
always know where his wife is'', reflecting harmful gender norms. These
findings suggest that as cultural representation in LLMs increases, so does the
risk of reproducing discriminatory beliefs. Approaches such as Constitutional
AI, which could embed human rights principles into model behavior, may only
partly help resolve this tension.

</details>


### [197] [Are Companies Taking AI Risks Seriously? A Systematic Analysis of Companies' AI Risk Disclosures in SEC 10-K forms](https://arxiv.org/abs/2508.19313)
*Lucas G. Uberti-Bona Marin,Bram Rijsbosch,Gerasimos Spanakis,Konrad Kollnig*

Main category: cs.CY

TL;DR: 研究对SEC 10 - K文件中AI风险披露进行大规模分析，发现提及AI风险的公司比例大幅上升，但披露质量有问题，还发布了分析工具。


<details>
  <summary>Details</summary>
Motivation: 监管机构推动企业对AI相关风险更透明，需研究企业是否及如何向公众报告AI风险。

Method: 对超7000家公司过去五年超30000份SEC 10 - K文件进行定量和定性分析。

Result: 提及AI风险的公司从2020年的4%升至2024年的超43%，法律和竞争风险最常被提及，对社会风险关注增加，但披露缺乏细节。

Conclusion: 很多AI风险披露质量欠佳，发布工具支持后续研究。

Abstract: As Artificial Intelligence becomes increasingly central to corporate
strategies, concerns over its risks are growing too. In response, regulators
are pushing for greater transparency in how companies identify, report and
mitigate AI-related risks. In the US, the Securities and Exchange Commission
(SEC) repeatedly warned companies to provide their investors with more accurate
disclosures of AI-related risks; recent enforcement and litigation against
companies' misleading AI claims reinforce these warnings. In the EU, new laws -
like the AI Act and Digital Services Act - introduced additional rules on AI
risk reporting and mitigation. Given these developments, it is essential to
examine if and how companies report AI-related risks to the public. This study
presents the first large-scale systematic analysis of AI risk disclosures in
SEC 10-K filings, which require public companies to report material risks to
their company. We analyse over 30,000 filings from more than 7,000 companies
over the past five years, combining quantitative and qualitative analysis. Our
findings reveal a sharp increase in the companies that mention AI risk, up from
4% in 2020 to over 43% in the most recent 2024 filings. While legal and
competitive AI risks are the most frequently mentioned, we also find growing
attention to societal AI risks, such as cyberattacks, fraud, and technical
limitations of AI systems. However, many disclosures remain generic or lack
details on mitigation strategies, echoing concerns raised recently by the SEC
about the quality of AI-related risk reporting. To support future research, we
publicly release a web-based tool for easily extracting and analysing
keyword-based disclosures across SEC filings.

</details>


### [198] [What Makes AI Applications Acceptable or Unacceptable? A Predictive Moral Framework](https://arxiv.org/abs/2508.19317)
*Kimmo Eriksson,Simon Karlsson,Irina Vartanova,Pontus Strimling*

Main category: cs.CY

TL;DR: 研究表明公众对AI应用的道德接受度判断是可预测的，五个核心道德品质可解释超90%的接受度差异。


<details>
  <summary>Details</summary>
Motivation: 在AI快速变革社会背景下，帮助开发者和政策制定者预测哪些AI应用会面临公众道德抵制。

Method: 进行一项大规模预注册研究，采用涵盖100种AI应用的综合分类法，对美国代表性样本（N = 587）进行调查。

Result: AI应用接受度差异强可预测，五个核心道德品质能解释超90%的接受度评分差异，框架在各领域有强预测力。

Conclusion: 公众对新技术的评估有结构化道德心理基础，可用于预测公众抵制和指导AI负责任创新。

Abstract: As artificial intelligence rapidly transforms society, developers and
policymakers struggle to anticipate which applications will face public moral
resistance. We propose that these judgments are not idiosyncratic but
systematic and predictable. In a large, preregistered study (N = 587, U.S.
representative sample), we used a comprehensive taxonomy of 100 AI applications
spanning personal and organizational contexts-including both functional uses
and the moral treatment of AI itself. In participants' collective judgment,
applications ranged from highly unacceptable to fully acceptable. We found this
variation was strongly predictable: five core moral qualities-perceived risk,
benefit, dishonesty, unnaturalness, and reduced accountability-collectively
explained over 90% of the variance in acceptability ratings. The framework
demonstrated strong predictive power across all domains and successfully
predicted individual-level judgments for held-out applications. These findings
reveal that a structured moral psychology underlies public evaluation of new
technologies, offering a powerful tool for anticipating public resistance and
guiding responsible innovation in AI.

</details>


### [199] [Hallucinating with AI: AI Psychosis as Distributed Delusions](https://arxiv.org/abs/2508.19588)
*Lucy Osler*

Main category: cs.CY

TL;DR: 从分布式认知理论视角探讨AI与人类交互中的‘AI幻觉’问题，提出从‘AI对我们产生幻觉’转变为‘与AI一起产生幻觉’，并分析聊天机器人的双重功能。


<details>
  <summary>Details</summary>
Motivation: 当前对生成式AI系统产生的虚假输出被称为‘AI幻觉’存在争议，作者希望从分布式认知理论更好理解人类 - AI交互中不准确信念等问题。

Method: 运用分布式认知理论进行分析，结合Jaswant Singh Chail案例。

Result: 发现AI会在分布式认知过程中引入错误，也会强化人类妄想思维；聊天机器人有认知工具和准他者的双重功能。

Conclusion: 生成式AI因聊天机器人的双重功能，是一种特殊且有吸引力的分布式认知案例。

Abstract: There is much discussion of the false outputs that generative AI systems such
as ChatGPT, Claude, Gemini, DeepSeek, and Grok create. In popular terminology,
these have been dubbed AI hallucinations. However, deeming these AI outputs
hallucinations is controversial, with many claiming this is a metaphorical
misnomer. Nevertheless, in this paper, I argue that when viewed through the
lens of distributed cognition theory, we can better see the dynamic and
troubling ways in which inaccurate beliefs, distorted memories and
self-narratives, and delusional thinking can emerge through human-AI
interactions; examples of which are popularly being referred to as cases of AI
psychosis. In such cases, I suggest we move away from thinking about how an AI
system might hallucinate at us, by generating false outputs, to thinking about
how, when we routinely rely on generative AI to help us think, remember, and
narrate, we can come to hallucinate with AI. This can happen when AI introduces
errors into the distributed cognitive process, but it can also happen when AI
sustains, affirms, and elaborates on our own delusional thinking and
self-narratives, such as in the case of Jaswant Singh Chail. I also examine how
the conversational style of chatbots can lead them to play a dual-function,
both as a cognitive artefact and a quasi-Other with whom we co-construct our
beliefs, narratives, and our realities. It is this dual function, I suggest,
that makes generative AI an unusual, and particularly seductive, case of
distributed cognition.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [200] [The Aegis Protocol: A Foundational Security Framework for Autonomous AI Agents](https://arxiv.org/abs/2508.19267)
*Sai Teja Reddy Adapala,Yashwanth Reddy Alugubelly*

Main category: cs.CR

TL;DR: 本文介绍用于开放智能体生态系统的Aegis协议，经模拟评估有高安全性和性能表现，为安全可扩展AI奠定基础。


<details>
  <summary>Details</summary>
Motivation: 自主AI智能体的发展带来系统安全风险，传统范式难以应对，需新安全框架。

Method: 提出Aegis协议，集成W3C DID、NIST PQC和Halo2 ZKP三个技术支柱；形式化扩展Dolev - Yao的对手模型；用离散事件模拟评估。

Result: 20000次攻击模拟成功率为0%；策略验证证明生成中位延迟2.79秒。

Conclusion: 虽评估基于模拟且处于早期，但为未来研究提供可重现基线，Aegis可作为安全可扩展自主AI的基础。

Abstract: The proliferation of autonomous AI agents marks a paradigm shift toward
complex, emergent multi-agent systems. This transition introduces systemic
security risks, including control-flow hijacking and cascading failures, that
traditional cybersecurity paradigms are ill-equipped to address. This paper
introduces the Aegis Protocol, a layered security framework designed to provide
strong security guarantees for open agentic ecosystems. The protocol integrates
three technological pillars: (1) non-spoofable agent identity via W3C
Decentralized Identifiers (DIDs); (2) communication integrity via
NIST-standardized post-quantum cryptography (PQC); and (3) verifiable,
privacy-preserving policy compliance using the Halo2 zero-knowledge proof (ZKP)
system. We formalize an adversary model extending Dolev-Yao for agentic threats
and validate the protocol against the STRIDE framework. Our quantitative
evaluation used a discrete-event simulation, calibrated against cryptographic
benchmarks, to model 1,000 agents. The simulation showed a 0 percent success
rate across 20,000 attack trials. For policy verification, analysis of the
simulation logs reported a median proof-generation latency of 2.79 seconds,
establishing a performance baseline for this class of security. While the
evaluation is simulation-based and early-stage, it offers a reproducible
baseline for future empirical studies and positions Aegis as a foundation for
safe, scalable autonomous AI.

</details>


### [201] [MixGAN: A Hybrid Semi-Supervised and Generative Approach for DDoS Detection in Cloud-Integrated IoT Networks](https://arxiv.org/abs/2508.19273)
*Tongxi Wu,Chenwei Xu,Jin Yang*

Main category: cs.CR

TL;DR: 云集成物联网系统易受DDoS攻击，检测面临挑战。提出MixGAN方法，结合多种技术，实验显示其比现有方法更优。


<details>
  <summary>Details</summary>
Motivation: 云集成物联网系统DDoS检测因流量动态复杂、类别不平衡和标记数据稀缺而困难，现有方法难以在有限监督和动态流量下泛化。

Method: 提出MixGAN混合检测方法，设计1 - D WideResNet骨干网络，用预训练CTGAN生成合成少数类样本，引入MAS策略。

Result: 在NSL - KDD、BoT - IoT和CICIoT2023上实验，MixGAN比现有方法准确率高2.5%，TPR和TNR提高4%。

Conclusion: MixGAN在大规模物联网 - 云环境中具有鲁棒性，代码公开。

Abstract: The proliferation of cloud-integrated IoT systems has intensified exposure to
Distributed Denial of Service (DDoS) attacks due to the expanded attack
surface, heterogeneous device behaviors, and limited edge protection. However,
DDoS detection in this context remains challenging because of complex traffic
dynamics, severe class imbalance, and scarce labeled data. While recent methods
have explored solutions to address class imbalance, many still struggle to
generalize under limited supervision and dynamic traffic conditions. To
overcome these challenges, we propose MixGAN, a hybrid detection method that
integrates conditional generation, semi-supervised learning, and robust feature
extraction. Specifically, to handle complex temporal traffic patterns, we
design a 1-D WideResNet backbone composed of temporal convolutional layers with
residual connections, which effectively capture local burst patterns in traffic
sequences. To alleviate class imbalance and label scarcity, we use a pretrained
CTGAN to generate synthetic minority-class (DDoS attack) samples that
complement unlabeled data. Furthermore, to mitigate the effect of noisy
pseudo-labels, we introduce a MixUp-Average-Sharpen (MAS) strategy that
constructs smoothed and sharpened targets by averaging predictions over
augmented views and reweighting them towards high-confidence classes.
Experiments on NSL-KDD, BoT-IoT, and CICIoT2023 demonstrate that MixGAN
achieves up to 2.5% higher accuracy and 4% improvement in both TPR and TNR
compared to state-of-the-art methods, confirming its robustness in large-scale
IoT-cloud environments. The source code is publicly available at
https://github.com/0xCavaliers/MixGAN.

</details>


### [202] [Towards Production-Worthy Simulation for Autonomous Cyber Operations](https://arxiv.org/abs/2508.19278)
*Konur Tholl,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.CR

TL;DR: 本文提出扩展CybORG环境并修改奖励信号和特征空间以提升强化学习训练性能，训练DQN和PPO代理验证修改有效。


<details>
  <summary>Details</summary>
Motivation: 模拟环境对自主网络操作中强化学习代理训练有重要价值，需准确呈现网络安全场景并产生支持训练的信号。

Method: 扩展CybORG的Cage Challenge 2环境，实现三个新动作；修改奖励信号和代理特征空间；在更新环境中训练DQN和PPO代理。

Result: CybORG可扩展额外现实功能，同时保持为强化学习代理生成信息训练信号的能力。

Conclusion: 提出的框架能提升训练性能，验证了扩展CybORG环境及相关修改的可行性。

Abstract: Simulated environments have proven invaluable in Autonomous Cyber Operations
(ACO) where Reinforcement Learning (RL) agents can be trained without the
computational overhead of emulation. These environments must accurately
represent cybersecurity scenarios while producing the necessary signals to
support RL training. In this study, we present a framework where we first
extend CybORG's Cage Challenge 2 environment by implementing three new actions:
Patch, Isolate, and Unisolate, to better represent the capabilities available
to human operators in real-world settings. We then propose a design for agent
development where we modify the reward signals and the agent's feature space to
enhance training performance. To validate these modifications, we train DQN and
PPO agents in the updated environment. Our study demonstrates that CybORG can
be extended with additional realistic functionality, while maintaining its
ability to generate informative training signals for RL agents.

</details>


### [203] [CORTEX: Composite Overlay for Risk Tiering and Exposure in Operational AI Systems](https://arxiv.org/abs/2508.19281)
*Aoun E Muhammad,Kin Choong Yow,Jamel Baili,Yongwon Cho,Yunyoung Nam*

Main category: cs.CR

TL;DR: 文章介绍了评估AI系统漏洞的多层风险评分框架CORTEX，基于AI事件数据库实证分析，对故障模式分类并通过五层架构评分，结果可用于多种风险管理场景。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在高风险领域部署增加，其故障可能性和影响从理论变为实际系统性风险，需要评估和评分AI系统漏洞。

Method: 基于AI事件数据库中1200多起事件的实证分析，将故障模式分为29个技术漏洞组，通过五层架构对每个漏洞评分，包括效用调整计算、治理和上下文覆盖、技术表面评分、环境和残余修正以及贝叶斯风险聚合和蒙特卡罗模拟的最终分层评估。

Result: 得到综合评分，可在AI风险登记册、模型审计、合规检查和动态治理仪表盘等场景应用。

Conclusion: CORTEX框架可有效评估和评分AI系统漏洞，为AI风险管理提供支持。

Abstract: As the deployment of Artificial Intelligence (AI) systems in high-stakes
sectors - like healthcare, finance, education, justice, and infrastructure has
increased - the possibility and impact of failures of these systems have
significantly evolved from being a theoretical possibility to practical
recurring, systemic risk. This paper introduces CORTEX (Composite Overlay for
Risk Tiering and Exposure), a multi-layered risk scoring framework proposed to
assess and score AI system vulnerabilities, developed on empirical analysis of
over 1,200 incidents documented in the AI Incident Database (AIID), CORTEX
categorizes failure modes into 29 technical vulnerability groups. Each
vulnerability is scored through a five-tier architecture that combines: (1)
utility-adjusted Likelihood x Impact calculations; (2) governance + contextual
overlays aligned with regulatory frameworks, such as the EU AI Act, NIST RMF,
OECD principles; (3) technical surface scores, covering exposure vectors like
drift, traceability, and adversarial risk; (4) environmental and residual
modifiers tailored to context of where these systems are being deployed to use;
and (5) a final layered assessment via Bayesian risk aggregation and Monte
Carlo simulation to model volatility and long-tail risks. The resulting
composite score can be operationalized across AI risk registers, model audits,
conformity checks, and dynamic governance dashboards.

</details>


### [204] [RL-Finetuned LLMs for Privacy-Preserving Synthetic Rewriting](https://arxiv.org/abs/2508.19286)
*Zhan Shi,Yefeng Yuan,Yuhong Liu,Liang Cheng,Yi Fang*

Main category: cs.CR

TL;DR: 论文针对平衡用户隐私与数据效用的问题，提出强化学习框架微调大语言模型，实验表明该方法提升隐私指标且不降低语义质量。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习系统依赖含敏感信息的数据集，传统匿名化技术有性能下降和易受推理攻击的问题，需更强大隐私保护措施。

Method: 提出强化学习框架，用复合奖励函数微调大语言模型，隐私奖励结合语义线索与最小生成树结构模式。

Result: 该方法显著提升作者混淆和隐私指标，不降低语义质量。

Conclusion: 该方法是大语言模型时代保护隐私的数据生成可扩展且与模型无关的解决方案。

Abstract: The performance of modern machine learning systems depends on access to
large, high-quality datasets, often sourced from user-generated content or
proprietary, domain-specific corpora. However, these rich datasets inherently
contain sensitive personal information, raising significant concerns about
privacy, data security, and compliance with regulatory frameworks. While
conventional anonymization techniques can remove explicit identifiers, such
removal may result in performance drop in downstream machine learning tasks.
More importantly, simple anonymization may not be effective against inference
attacks that exploit implicit signals such as writing style, topical focus, or
demographic cues, highlighting the need for more robust privacy safeguards
during model training. To address the challenging issue of balancing user
privacy and data utility, we propose a reinforcement learning framework that
fine-tunes a large language model (LLM) using a composite reward function that
jointly optimizes for explicit and implicit privacy, semantic fidelity, and
output diversity. To effectively capture population level regularities, the
privacy reward combines semantic cues with structural patterns derived from a
minimum spanning tree (MST) over latent representations. By modeling these
privacy-sensitive signals in their distributional context, the proposed
approach guides the model to generate synthetic rewrites that preserve utility
while mitigating privacy risks. Empirical results show that the proposed method
significantly enhances author obfuscation and privacy metrics without degrading
semantic quality, providing a scalable and model-agnostic solution for privacy
preserving data generation in the era of large language models.

</details>


### [205] [Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior](https://arxiv.org/abs/2508.19287)
*Zhuotao Lian,Weiyu Wang,Qingkui Zeng,Toru Nakanishi,Teruaki Kitasuka,Chunhua Su*

Main category: cs.CR

TL;DR: 本文识别了大语言模型中内容注入攻击，证明其可行性，分析原因并讨论缓解策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛处理用户提交内容，识别其中新的攻击类型。

Method: 证明攻击在流行平台的可行性，分析攻击的根本原因。

Result: 发现内容注入攻击可在用户无感知和系统未被攻破时操纵输出。

Conclusion: 内容注入攻击是现实中大语言模型工作流里隐蔽且实际存在的威胁。

Abstract: Large Language Models (LLMs) are widely deployed in applications that accept
user-submitted content, such as uploaded documents or pasted text, for tasks
like summarization and question answering. In this paper, we identify a new
class of attacks, prompt in content injection, where adversarial instructions
are embedded in seemingly benign inputs. When processed by the LLM, these
hidden prompts can manipulate outputs without user awareness or system
compromise, leading to biased summaries, fabricated claims, or misleading
suggestions. We demonstrate the feasibility of such attacks across popular
platforms, analyze their root causes including prompt concatenation and
insufficient input isolation, and discuss mitigation strategies. Our findings
reveal a subtle yet practical threat in real-world LLM workflows.

</details>


### [206] [Tricking LLM-Based NPCs into Spilling Secrets](https://arxiv.org/abs/2508.19288)
*Kyohei Shiomi,Zhuotao Lian,Toru Nakanishi,Teruaki Kitasuka*

Main category: cs.CR

TL;DR: 研究对抗性提示注入是否会使基于大语言模型的游戏NPC泄露隐藏背景秘密


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于游戏NPC生成动态对话带来新安全担忧

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: Large Language Models (LLMs) are increasingly used to generate dynamic
dialogue for game NPCs. However, their integration raises new security
concerns. In this study, we examine whether adversarial prompt injection can
cause LLM-based NPCs to reveal hidden background secrets that are meant to
remain undisclosed.

</details>


### [207] [Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience](https://arxiv.org/abs/2508.19292)
*Xi Wang,Songlei Jian,Shasha Li,Xiaopeng Li,Bin Ji,Jun Ma,Xiaodong Liu,Jing Wang,Feilong Bao,Jianfeng Zhang,Baosheng Wang,Jie Yu*

Main category: cs.CR

TL;DR: 文章提出自动化越狱框架JailExpert，整合过往攻击经验，实验表明其显著提升攻击效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有越狱方法存在效率低和重复优化问题，未充分利用过往攻击经验，需更好方法来整合经验辅助越狱尝试。

Method: 提出JailExpert框架，实现经验结构的形式化表示，基于语义漂移对经验分组，支持经验池动态更新。

Result: 与当前最先进的黑盒越狱方法相比，攻击成功率平均提高17%，攻击效率提升2.7倍。

Conclusion: JailExpert框架能显著提升大语言模型越狱攻击的效果和效率。

Abstract: Large language models (LLMs) generate human-aligned content under certain
safety constraints. However, the current known technique ``jailbreak prompt''
can circumvent safety-aligned measures and induce LLMs to output malicious
content. Research on Jailbreaking can help identify vulnerabilities in LLMs and
guide the development of robust security frameworks. To circumvent the issue of
attack templates becoming obsolete as models evolve, existing methods adopt
iterative mutation and dynamic optimization to facilitate more automated
jailbreak attacks. However, these methods face two challenges: inefficiency and
repetitive optimization, as they overlook the value of past attack experiences.
To better integrate past attack experiences to assist current jailbreak
attempts, we propose the \textbf{JailExpert}, an automated jailbreak framework,
which is the first to achieve a formal representation of experience structure,
group experiences based on semantic drift, and support the dynamic updating of
the experience pool. Extensive experiments demonstrate that JailExpert
significantly improves both attack effectiveness and efficiency. Compared to
the current state-of-the-art black-box jailbreak methods, JailExpert achieves
an average increase of 17\% in attack success rate and 2.7 times improvement in
attack efficiency. Our implementation is available at
\href{https://github.com/xiZAIzai/JailExpert}{XiZaiZai/JailExpert}

</details>


### [208] [An Investigation on Group Query Hallucination Attacks](https://arxiv.org/abs/2508.19321)
*Kehao Miao,Xiaolong Jin*

Main category: cs.CR

TL;DR: 研究提出Group Query Attack技术，研究连续提示上下文对大语言模型输出影响，发现该攻击会降低特定任务微调模型性能、引发潜在后门风险，对推理任务也有效。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛使用，需了解其在用户交互中的潜在失败模式，且用户常一次对话提多个问题。

Method: 提出Group Query Attack技术，同时向大语言模型呈现多组查询，研究连续提示积累的上下文对模型输出的影响。

Result: Group Query Attack显著降低特定任务微调模型的性能，引发大语言模型潜在后门风险，对预训练和对齐模型的推理任务也有效。

Conclusion: Group Query Attack技术能有效揭示大语言模型在多查询场景下的潜在问题。

Abstract: With the widespread use of large language models (LLMs), understanding their
potential failure modes during user interactions is essential. In practice,
users often pose multiple questions in a single conversation with LLMs.
Therefore, in this study, we propose Group Query Attack, a technique that
simulates this scenario by presenting groups of queries to LLMs simultaneously.
We investigate how the accumulated context from consecutive prompts influences
the outputs of LLMs. Specifically, we observe that Group Query Attack
significantly degrades the performance of models fine-tuned on specific tasks.
Moreover, we demonstrate that Group Query Attack induces a risk of triggering
potential backdoors of LLMs. Besides, Group Query Attack is also effective in
tasks involving reasoning, such as mathematical reasoning and code generation
for pre-trained and aligned models.

</details>


### [209] [Addressing Weak Authentication like RFID, NFC in EVs and EVCs using AI-powered Adaptive Authentication](https://arxiv.org/abs/2508.19465)
*Onyinye Okoye*

Main category: cs.CR

TL;DR: 电动汽车及充电系统发展带来新的网络安全挑战，传统认证机制易受攻击，本文研究AI驱动的自适应认证框架，结论是采用该框架对保障电动出行安全至关重要。


<details>
  <summary>Details</summary>
Motivation: 电动汽车和充电系统的快速发展带来网络安全挑战，传统认证机制脆弱，需要更安全的认证方法。

Method: 提出基于零信任架构的AI驱动自适应认证框架，结合机器学习、异常检测、行为分析和上下文风险评估，并通过文献综述评估当前漏洞。

Result: 通过研究评估了当前漏洞，强调了AI驱动解决方案的优势。

Conclusion: 采用AI驱动的自适应认证是保障未来电动出行安全和增强数字信任的战略必要。

Abstract: The rapid expansion of the Electric Vehicles (EVs) and Electric Vehicle
Charging Systems (EVCs) has introduced new cybersecurity challenges,
specifically in authentication protocols that protect vehicles, users, and
energy infrastructure. Although widely adopted for convenience, traditional
authentication mechanisms like Radio Frequency Identification (RFID) and Near
Field Communication (NFC) rely on static identifiers and weak encryption,
making them highly vulnerable to attack vectors such as cloning, relay attacks,
and signal interception. This study explores an AI-powered adaptive
authentication framework designed to overcome these shortcomings by integrating
machine learning, anomaly detection, behavioral analytics, and contextual risk
assessment. Grounded in the principles of Zero Trust Architecture, the proposed
framework emphasizes continuous verification, least privilege access, and
secure communication. Through a comprehensive literature review, this research
evaluates current vulnerabilities and highlights AI-driven solutions to provide
a scalable, resilient, and proactive defense. Ultimately, the research findings
conclude that adopting AI-powered adaptive authentication is a strategic
imperative for securing the future of electric mobility and strengthening
digital trust across the ecosystem. Keywords: weak authentication, RFID, NFC,
ML, AI-powered adaptive authentication, relay attacks, cloning, eavesdropping,
MITM attacks, Zero Trust Architecture

</details>


### [210] [SIExVulTS: Sensitive Information Exposure Vulnerability Detection System using Transformer Models and Static Analysis](https://arxiv.org/abs/2508.19472)
*Kyler Katz,Sara Moshtari,Ibrahim Mujhid,Mehdi Mirakhorli,Derek Garcia*

Main category: cs.CR

TL;DR: 本文提出SIExVulTS系统，结合Transformer模型与静态分析检测Java应用中敏感信息暴露漏洞，评估效果良好，还发现新CVEs。


<details>
  <summary>Details</summary>
Motivation: 现有检测工具很少针对CWE - 200的不同子类别或进行代码级数据流的上下文感知分析，敏感信息暴露漏洞威胁持续存在且未得到充分解决。

Method: 采用三阶段架构，包括攻击面检测引擎、暴露分析引擎和流验证引擎，并用三个数据集评估。

Result: 攻击面检测引擎F1分数超93%，暴露分析引擎F1分数85.71%，流验证引擎精度从22.61%提升到87.23%，还发现6个未知CVEs。

Conclusion: SIExVulTS在提高软件安全、检测和验证CWE - 200漏洞方面有效且实用。

Abstract: Sensitive Information Exposure (SIEx) vulnerabilities (CWE-200) remain a
persistent and under-addressed threat across software systems, often leading to
serious security breaches. Existing detection tools rarely target the diverse
subcategories of CWE-200 or provide context-aware analysis of code-level data
flows.
  Aims: This paper aims to present SIExVulTS, a novel vulnerability detection
system that integrates transformer-based models with static analysis to
identify and verify sensitive information exposure in Java applications.
  Method: SIExVulTS employs a three-stage architecture: (1) an Attack Surface
Detection Engine that uses sentence embeddings to identify sensitive variables,
strings, comments, and sinks; (2) an Exposure Analysis Engine that instantiates
CodeQL queries aligned with the CWE-200 hierarchy; and (3) a Flow Verification
Engine that leverages GraphCodeBERT to semantically validate source-to-sink
flows. We evaluate SIExVulTS using three curated datasets, including real-world
CVEs, a benchmark set of synthetic CWE-200 examples, and labeled flows from 31
open-source projects.
  Results: The Attack Surface Detection Engine achieved an average F1 score
greater than 93\%, the Exposure Analysis Engine achieved an F1 score of
85.71\%, and the Flow Verification Engine increased precision from 22.61\% to
87.23\%. Moreover, SIExVulTS successfully uncovered six previously unknown CVEs
in major Apache projects.
  Conclusions: The results demonstrate that SIExVulTS is effective and
practical for improving software security against sensitive data exposure,
addressing limitations of existing tools in detecting and verifying CWE-200
vulnerabilities.

</details>


### [211] [Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills](https://arxiv.org/abs/2508.19500)
*David Noever*

Main category: cs.CR

TL;DR: 本文识别并分析基于MCP的代理系统的新漏洞类，揭示跨域协调行动产生的攻击风险，提出实验方向。


<details>
  <summary>Details</summary>
Motivation: 研究基于MCP的代理系统是否缺乏跨域安全措施以检测或防止组合攻击。

Method: 使用MITRE ATLAS框架进行系统分析，开展红队演练。

Result: 发现可将合法操作串联成复杂攻击序列，有数据泄露、金融操纵等攻击链，服务隔离假设失效。

Conclusion: 提出三个基于现有MCP基准套件的实验方向。

Abstract: This paper identifies and analyzes a novel vulnerability class in Model
Context Protocol (MCP) based agent systems. The attack chain describes and
demonstrates how benign, individually authorized tasks can be orchestrated to
produce harmful emergent behaviors. Through systematic analysis using the MITRE
ATLAS framework, we demonstrate how 95 agents tested with access to multiple
services-including browser automation, financial analysis, location tracking,
and code deployment-can chain legitimate operations into sophisticated attack
sequences that extend beyond the security boundaries of any individual service.
These red team exercises survey whether current MCP architectures lack
cross-domain security measures necessary to detect or prevent a large category
of compositional attacks. We present empirical evidence of specific attack
chains that achieve targeted harm through service orchestration, including data
exfiltration, financial manipulation, and infrastructure compromise. These
findings reveal that the fundamental security assumption of service isolation
fails when agents can coordinate actions across multiple domains, creating an
exponential attack surface that grows with each additional capability. This
research provides a barebones experimental framework that evaluate not whether
agents can complete MCP benchmark tasks, but what happens when they complete
them too well and optimize across multiple services in ways that violate human
expectations and safety constraints. We propose three concrete experimental
directions using the existing MCP benchmark suite.

</details>


### [212] [From Research to Reality: Feasibility of Gradient Inversion Attacks in Federated Learning](https://arxiv.org/abs/2508.19819)
*Viktor Valadi,Mattias Åkesson,Johan Östman,Salman Toor,Andreas Hellander*

Main category: cs.CR

TL;DR: 本文系统分析架构和训练行为对梯度反转攻击漏洞的影响，在训练和推理模式下开展攻击研究，提出新攻击方法，对生产级目标检测模型进行攻击，最后给出设置的全面映射。


<details>
  <summary>Details</summary>
Motivation: 许多研究在推理模式下考虑梯度反转攻击，本文旨在系统分析架构和训练行为对漏洞的影响，在更现实条件下评估攻击可行性。

Method: 分析架构和训练行为对漏洞的影响；针对训练模式模型提出两种新攻击；对生产级目标检测模型进行攻击并修改架构增加其漏洞。

Result: 发现训练模式下成功攻击需满足多个架构条件；提出的新攻击在现实训练条件下达到了最先进性能；指出生产级目标检测模型有很强的固有鲁棒性。

Conclusion: 给出了设置的全面映射，明确架构选择和操作模式对隐私的影响，为未来研究和部署场景评估梯度反转风险提供了见解。

Abstract: Gradient inversion attacks have garnered attention for their ability to
compromise privacy in federated learning. However, many studies consider
attacks with the model in inference mode, where training-time behaviors like
dropout are disabled and batch normalization relies on fixed statistics. In
this work, we systematically analyze how architecture and training behavior
affect vulnerability, including the first in-depth study of inference-mode
clients, which we show dramatically simplifies inversion. To assess attack
feasibility under more realistic conditions, we turn to clients operating in
standard training mode. In this setting, we find that successful attacks are
only possible when several architectural conditions are met simultaneously:
models must be shallow and wide, use skip connections, and, critically, employ
pre-activation normalization. We introduce two novel attacks against models in
training-mode with varying attacker knowledge, achieving state-of-the-art
performance under realistic training conditions. We extend these efforts by
presenting the first attack on a production-grade object-detection model. Here,
to enable any visibly identifiable leakage, we revert to the lenient inference
mode setting and make multiple architectural modifications to increase model
vulnerability, with the extent of required changes highlighting the strong
inherent robustness of such architectures. We conclude this work by offering
the first comprehensive mapping of settings, clarifying which combinations of
architectural choices and operational modes meaningfully impact privacy. Our
analysis provides actionable insight into when models are likely vulnerable,
when they appear robust, and where subtle leakage may persist. Together, these
findings reframe how gradient inversion risk should be assessed in future
research and deployment scenarios.

</details>


### [213] [Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses](https://arxiv.org/abs/2508.19641)
*Lincan Li,Bolin Shen,Chenxi Zhao,Yuxiang Sun,Kaixiang Zhao,Shirui Pan,Yushun Dong*

Main category: cs.CR

TL;DR: 本文针对图机器学习(GMLaaS)面临的安全挑战，提出威胁与防御分类，建立评估框架、引入基准数据集，开发开源库PyGIP，助力GML知识产权保护。


<details>
  <summary>Details</summary>
Motivation: 图机器学习模型训练资源密集，GMLaaS出现但面临攻击者威胁，需保护GML知识产权。

Method: 提出GML模型和图结构数据层面的威胁与防御分类，建立评估框架，引入基准数据集，开发开源库PyGIP。

Result: 完成了威胁与防御分类，有系统评估框架、基准数据集和开源库PyGIP。

Conclusion: 该调查对GML知识产权保护有基础作用，为GML社区提供实用方案。

Abstract: Graph-structured data, which captures non-Euclidean relationships and
interactions between entities, is growing in scale and complexity. As a result,
training state-of-the-art graph machine learning (GML) models have become
increasingly resource-intensive, turning these models and data into invaluable
Intellectual Property (IP). To address the resource-intensive nature of model
training, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an
efficient solution by leveraging third-party cloud services for model
development and management. However, deploying such models in GMLaaS also
exposes them to potential threats from attackers. Specifically, while the APIs
within a GMLaaS system provide interfaces for users to query the model and
receive outputs, they also allow attackers to exploit and steal model
functionalities or sensitive training data, posing severe threats to the safety
of these GML models and the underlying graph data. To address these challenges,
this survey systematically introduces the first taxonomy of threats and
defenses at the level of both GML model and graph-structured data. Such a
tailored taxonomy facilitates an in-depth understanding of GML IP protection.
Furthermore, we present a systematic evaluation framework to assess the
effectiveness of IP protection methods, introduce a curated set of benchmark
datasets across various domains, and discuss their application scopes and
future challenges. Finally, we establish an open-sourced versatile library
named PyGIP, which evaluates various attack and defense techniques in GMLaaS
scenarios and facilitates the implementation of existing benchmark methods. The
library resource can be accessed at: https://labrai.github.io/PyGIP. We believe
this survey will play a fundamental role in intellectual property protection
for GML and provide practical recipes for the GML community.

</details>


### [214] [Safety Alignment Should Be Made More Than Just A Few Attention Heads](https://arxiv.org/abs/2508.19697)
*Chao Huang,Zefeng Zhang,Juewei Yue,Quangang Li,Chuang Zhang,Tingwen Liu*

Main category: cs.CR

TL;DR: 当前大语言模型安全对齐有漏洞，提出RDSHA识别关键注意力头，提出AHD训练策略提升模型安全鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型当前安全对齐存在漏洞，对抗性提示可绕过安全措施。

Method: 引入RDSHA方法识别对安全行为负责的注意力头；提出AHD训练策略使安全相关行为编码分散到更多注意力头。

Result: AHD成功将安全相关能力分散到更多注意力头，在主流越狱攻击评估中，用AHD训练的模型安全鲁棒性更强，且保持整体功能实用性。

Conclusion: AHD训练策略能有效提升大语言模型的安全鲁棒性。

Abstract: Current safety alignment for large language models(LLMs) continues to present
vulnerabilities, given that adversarial prompting can effectively bypass their
safety measures.Our investigation shows that these safety mechanisms
predominantly depend on a limited subset of attention heads: removing or
ablating these heads can severely compromise model safety. To identify and
evaluate these safety-critical components, we introduce RDSHA, a targeted
ablation method that leverages the model's refusal direction to pinpoint
attention heads mostly responsible for safety behaviors. Further analysis shows
that existing jailbreak attacks exploit this concentration by selectively
bypassing or manipulating these critical attention heads. To address this
issue, we propose AHD, a novel training strategy designed to promote the
distributed encoding of safety-related behaviors across numerous attention
heads. Experimental results demonstrate that AHD successfully distributes
safety-related capabilities across more attention heads. Moreover, evaluations
under several mainstream jailbreak attacks show that models trained with AHD
exhibit considerably stronger safety robustness, while maintaining overall
functional utility.

</details>


### [215] [SoK: Large Language Model Copyright Auditing via Fingerprinting](https://arxiv.org/abs/2508.19843)
*Shuo Shao,Yiming Li,Yu He,Hongwei Yao,Wenyuan Yang,Dacheng Tao,Zhan Qin*

Main category: cs.CR

TL;DR: 本文首次全面研究大语言模型指纹识别，提出统一框架和分类法，构建LeaFBench基准测试，实验揭示现有方法优缺点并指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受版权侵犯，指纹识别虽有潜力但可靠性因模型修改多样和缺乏标准评估而不确定，需要全面研究。

Method: 引入统一框架和形式化分类法对现有方法分类，构建LeaFBench基准测试，包含主流基础模型和多种模型修改技术。

Result: 在LeaFBench上的大量实验揭示了现有方法的优缺点。

Conclusion: 指明了大语言模型指纹识别这一新兴领域的未来研究方向和关键开放问题。

Abstract: The broad capabilities and substantial resources required to train Large
Language Models (LLMs) make them valuable intellectual property, yet they
remain vulnerable to copyright infringement, such as unauthorized use and model
theft. LLM fingerprinting, a non-intrusive technique that extracts and compares
the distinctive features from LLMs to identify infringements, offers a
promising solution to copyright auditing. However, its reliability remains
uncertain due to the prevalence of diverse model modifications and the lack of
standardized evaluation. In this SoK, we present the first comprehensive study
of LLM fingerprinting. We introduce a unified framework and formal taxonomy
that categorizes existing methods into white-box and black-box approaches,
providing a structured overview of the state of the art. We further propose
LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting
under realistic deployment scenarios. Built upon mainstream foundation models
and comprising 149 distinct model instances, LeaFBench integrates 13
representative post-development techniques, spanning both parameter-altering
methods (e.g., fine-tuning, quantization) and parameter-independent mechanisms
(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the
strengths and weaknesses of existing methods, thereby outlining future research
directions and critical open problems in this emerging field. The code is
available at https://github.com/shaoshuo-ss/LeaFBench.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [216] [Inference of Human-derived Specifications of Object Placement via Demonstration](https://arxiv.org/abs/2508.19367)
*Alex Cuellar,Ho Chit Siu,Julie A Shah*

Main category: cs.RO

TL;DR: 提出PARCC框架及推理算法，人类研究结果表明其能捕捉人类意图规范且从演示学习有优势


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉对人类重要的空间关系方面表达能力有限，需提升机器人对人类物体排列规则的理解

Method: 引入基于区域连接演算的PARCC形式逻辑框架描述物体空间相对位置，引入通过演示学习PARCC规范的推理算法

Result: 人类研究结果显示框架能捕捉人类预期规范，从演示学习比人类提供规范更有优势

Conclusion: PARCC框架及相关推理算法有助于机器人理解人类物体排列规则

Abstract: As robots' manipulation capabilities improve for pick-and-place tasks (e.g.,
object packing, sorting, and kitting), methods focused on understanding
human-acceptable object configurations remain limited expressively with regard
to capturing spatial relationships important to humans. To advance robotic
understanding of human rules for object arrangement, we introduce
positionally-augmented RCC (PARCC), a formal logic framework based on region
connection calculus (RCC) for describing the relative position of objects in
space. Additionally, we introduce an inference algorithm for learning PARCC
specifications via demonstrations. Finally, we present the results from a human
study, which demonstrate our framework's ability to capture a human's intended
specification and the benefits of learning from demonstration approaches over
human-provided specifications.

</details>


### [217] [A Lightweight Crowd Model for Robot Social Navigation](https://arxiv.org/abs/2508.19595)
*Maryam Kazemi Eskeri,Thomas Wiedemann,Ville Kyrki,Dominik Baumann,Tomasz Piotr Kucner*

Main category: cs.RO

TL;DR: 提出轻量级实时宏观人群预测模型，减少推理时间并提高准确率，助力机器人导航。


<details>
  <summary>Details</summary>
Motivation: 传统微观模型在密集人群中计算成本高，现有宏观模型过于简单或计算密集，需平衡预测准确性和计算效率的模型以实现机器人在人群环境中安全高效导航。

Method: 基于行人流动固有特性简化空间和时间处理，无需复杂架构。

Result: 推理时间减少3.6倍，预测准确性提高3.1%，集成到规划框架可实现机器人在动态环境中高效且符合社交规范的导航。

Conclusion: 高效的人群建模可使机器人在密集环境中导航，无需高成本计算。

Abstract: Robots operating in human-populated environments must navigate safely and
efficiently while minimizing social disruption. Achieving this requires
estimating crowd movement to avoid congested areas in real-time. Traditional
microscopic models struggle to scale in dense crowds due to high computational
cost, while existing macroscopic crowd prediction models tend to be either
overly simplistic or computationally intensive. In this work, we propose a
lightweight, real-time macroscopic crowd prediction model tailored for human
motion, which balances prediction accuracy and computational efficiency. Our
approach simplifies both spatial and temporal processing based on the inherent
characteristics of pedestrian flow, enabling robust generalization without the
overhead of complex architectures. We demonstrate a 3.6 times reduction in
inference time, while improving prediction accuracy by 3.1 %. Integrated into a
socially aware planning framework, the model enables efficient and socially
compliant robot navigation in dynamic environments. This work highlights that
efficient human crowd modeling enables robots to navigate dense environments
without costly computations.

</details>


### [218] [Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning](https://arxiv.org/abs/2508.20095)
*Jinhao Liang,Sven Koenig,Ferdinando Fioretto*

Main category: cs.RO

TL;DR: 本文提出离散引导扩散（DGD）框架，结合离散多智能体路径规划求解器与约束生成扩散模型解决多机器人运动规划问题，在大规模复杂环境表现出色。


<details>
  <summary>Details</summary>
Motivation: 离散多智能体路径规划方法轨迹质量差，连续优化规划器扩展性不佳，需解决两者局限性。

Method: 引入DGD框架，将原非凸MRMP问题分解为可处理子问题，结合离散MAPF解与约束优化技术引导扩散模型，加入轻量级约束修复机制。

Result: 在大规模复杂环境中达新的最优性能，可扩展到100个机器人，规划效率高且成功率高。

Conclusion: DGD框架有效解决现有多机器人运动规划方法的局限，在大规模场景表现良好。

Abstract: Multi-Robot Motion Planning (MRMP) involves generating collision-free
trajectories for multiple robots operating in a shared continuous workspace.
While discrete multi-agent path finding (MAPF) methods are broadly adopted due
to their scalability, their coarse discretization severely limits trajectory
quality. In contrast, continuous optimization-based planners offer
higher-quality paths but suffer from the curse of dimensionality, resulting in
poor scalability with respect to the number of robots. This paper tackles the
limitations of these two approaches by introducing a novel framework that
integrates discrete MAPF solvers with constrained generative diffusion models.
The resulting framework, called Discrete-Guided Diffusion (DGD), has three key
characteristics: (1) it decomposes the original nonconvex MRMP problem into
tractable subproblems with convex configuration spaces, (2) it combines
discrete MAPF solutions with constrained optimization techniques to guide
diffusion models capture complex spatiotemporal dependencies among robots, and
(3) it incorporates a lightweight constraint repair mechanism to ensure
trajectory feasibility. The proposed method sets a new state-of-the-art
performance in large-scale, complex environments, scaling to 100 robots while
achieving planning efficiency and high success rates.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [219] [An algorithm for accurate and simple-looking metaphorical maps](https://arxiv.org/abs/2508.19810)
*Eleni Katsanou,Tamara Mchedlidze,Antonios Symvonis,Thanos Tolias*

Main category: cs.DM

TL;DR: 本文对Mchedlidze和Schnorr 2022年的算法进行多方面扩展，提高隐喻地图精度、保持简单性并增强通用性，实验表明新算法能以小的简单性牺牲实现近乎完美的面积精度。


<details>
  <summary>Details</summary>
Motivation: Mchedlidze和Schnorr的算法生成的隐喻地图精度远非最优，本文旨在对其算法进行扩展改进。

Method: 引入区域刚度概念及基于区域压力改变刚度的技术；根据点是否在狭窄通道上引入权重系数到压力；处理非三角化图，通过生成多区域交汇点或引入孔洞。

Result: 实验显示算法能以小的简单性牺牲构建出面积精度近乎完美的隐喻地图。

Conclusion: 扩展后的算法在隐喻地图构建中能有效平衡精度和简单性，且适用范围更广。

Abstract: "Metaphorical maps" or "contact representations" are visual representations
of vertex-weighted graphs that rely on the geographic map metaphor. The
vertices are represented by countries, the weights by the areas of the
countries, and the edges by contacts/ boundaries among them. The accuracy with
which the weights are mapped to areas and the simplicity of the polygons
representing the countries are the two classical optimization goals for
metaphorical maps. Mchedlidze and Schnorr [Metaphoric Maps for Dynamic
Vertex-weighted Graphs, EuroVis 2022] presented a force-based algorithm that
creates metaphorical maps that balance between these two optimization goals.
Their maps look visually simple, but the accuracy of the maps is far from
optimal - the countries' areas can vary up to 30% compared to required. In this
paper, we provide a multi-fold extension of the algorithm in [Metaphoric Maps
for Dynamic Vertex-weighted Graphs, EuroVis 2022]. More specifically:
  1. Towards improving accuracy: We introduce the notion of region stiffness
and suggest a technique for varying the stiffness based on the current pressure
of map regions.
  2. Towards maintaining simplicity: We introduce a weight coefficient to the
pressure force exerted on each polygon point based on whether the corresponding
point appears along a narrow passage.
  3. Towards generality: We cover, in contrast to [Metaphoric Maps for Dynamic
Vertex-weighted Graphs, EuroVis 2022], non-triangulated graphs. This is done by
either generating points where more than three regions meet or by introducing
holes in the metaphorical map.
  We perform an extended experimental evaluation that, among other results,
reveals that our algorithm is able to construct metaphorical maps with nearly
perfect area accuracy with a little sacrifice in their simplicity.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [220] [Inferring geometry and material properties from Mueller matrices with machine learning](https://arxiv.org/abs/2508.19713)
*Lars Doorenbos,C. H. Lucas Patty,Raphael Sznitman,Pablo Márquez-Neila*

Main category: physics.optics

TL;DR: 探索用机器学习从穆勒矩阵推断表面几何和材料属性，证明可预测表面法线、重建物体几何并识别材料类型。


<details>
  <summary>Details</summary>
Motivation: 穆勒矩阵同时恢复几何和材料属性是不适定问题，探索其是否含足够信息用机器学习推断相关属性。

Method: 使用不同各向同性材料球体数据集，在五个可见光波长全角域捕获穆勒矩阵，以其为输入训练机器学习模型。

Result: 即使材料类型未知，可预测表面法线、重建物体几何，能正确识别材料类型。

Conclusion: 对角元素对材料表征关键，非对角元素对法线估计决定性作用。

Abstract: Mueller matrices (MMs) encode information on geometry and material
properties, but recovering both simultaneously is an ill-posed problem. We
explore whether MMs contain sufficient information to infer surface geometry
and material properties with machine learning. We use a dataset of spheres of
various isotropic materials, with MMs captured over the full angular domain at
five visible wavelengths (450-650 nm). We train machine learning models to
predict material properties and surface normals using only these MMs as input.
We demonstrate that, even when the material type is unknown, surface normals
can be predicted and object geometry reconstructed. Moreover, MMs allow models
to identify material types correctly. Further analyses show that diagonal
elements are key for material characterization, and off-diagonal elements are
decisive for normal estimation.

</details>


### [221] [Fourier Feature Networks for High-Fidelity Prediction of Perturbed Optical Fields](https://arxiv.org/abs/2508.19751)
*Joshua R. Jandrell,Mitchell A. Cox*

Main category: physics.optics

TL;DR: 为解决标准MLP难以学习高振荡复值函数问题，引入傅里叶特征作为额外网络输入，在预测多模光纤传输矩阵任务中表现优异，是一种通用且鲁棒的建模方法。


<details>
  <summary>Details</summary>
Motivation: 标准多层感知器（MLPs）因固有频谱偏差，难以拟合高频率正弦波，无法胜任对光场扰动效应建模时学习高振荡复值函数的任务。

Method: 引入依赖于扰动的一组预定义正弦函数（傅里叶特征）作为额外网络输入，将学习问题转化为寻找基函数的线性组合。

Result: 与标准MLP相比，使用少85%的参数，网络将输出场振幅和相位的预测误差降低一个数量级，与真实值的平均复相关系数达到0.995。

Conclusion: 该方法为准确建模一类广泛的振荡物理系统提供了通用且鲁棒的方法。

Abstract: Modelling the effects of perturbations on optical fields often requires
learning highly oscillatory complex-valued functions. Standard multi-layer
perceptrons (MLPs) struggle with this task due to an inherent spectral bias,
preventing them from fitting high-frequency sinusoids. To overcome this, we
incorporate Fourier features - a set of predefined sinusoids dependent on the
perturbation - as an additional network input. This reframes the learning
problem from approximating a complex function to finding a linear combination
of basis functions. We demonstrate this method by training a Fourier Feature
Network to predict the transmission matrix of a multimode fibre under
mechanical compression. Compared to a standard MLP, our network reduces
prediction error in the output field's amplitude and phase by an order of
magnitude, achieving a mean complex correlation of 0.995 with the ground truth,
despite using 85% fewer parameters. This approach offers a general and robust
method for accurately modelling a wide class of oscillatory physical systems.

</details>


### [222] [On-chip wave chaos for photonic extreme learning](https://arxiv.org/abs/2508.19878)
*Matthew R. Wilson,Jack A. Smith,Michael J. Strain,Xavier Porte*

Main category: physics.optics

TL;DR: 文章实验展示基于体育场微腔波混沌干涉的芯片级光子ELM，表征其分类性能并展示优化读出大小的能力。


<details>
  <summary>Details</summary>
Motivation: 可扩展且节能的人工神经网络需求增加，集成光子学适合ELM架构，需新硬件方案。

Method: 采用外部单频可调谐激光源波长编码输入信息，用直写SU - 8聚合物在玻璃上制作微腔，通过散射壁作为读出层收集光。

Result: 输入波长高分辨率扫描下，散射壁散斑呈现不相关和非周期性行为。

Conclusion: 可通过测量散射壁不同部分控制ELM输出节点数量，优化光子ELM读出大小以适应不同任务性能要求。

Abstract: The increase in demand for scalable and energy efficient artificial neural
networks has put the focus on novel hardware solutions. Integrated photonics
offers a compact, parallel and ultra-fast information processing platform,
specially suited for extreme learning machine (ELM) architectures. Here we
experimentally demonstrate a chip-scale photonic ELM based on wave chaos
interference in a stadium microcavity. By encoding the input information in the
wavelength of an external single-frequency tunable laser source, we leverage
the high sensitivity to wavelength of injection in such photonic resonators. We
fabricate the microcavity with direct laser writing of SU-8 polymer on glass. A
scattering wall surrounding the stadium operates as readout layer, collecting
the light associated with the cavity's leaky modes. We report uncorrelated and
aperiodic behavior in the speckles of the scattering barrier from a high
resolution scan of the input wavelength. Finally, we characterize the system's
performance at classification in four qualitatively different benchmark tasks.
As we can control the number of output nodes of our ELM by measuring different
parts of the scattering barrier, we demonstrate the capability to optimize our
photonic ELM's readout size to the performance required for each task.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [223] [Context-aware Sparse Spatiotemporal Learning for Event-based Vision](https://arxiv.org/abs/2508.19806)
*Shenqi Wang,Guangzhi Tang*

Main category: cs.CV

TL;DR: 提出上下文感知稀疏时空学习（CSSL）框架用于事件视觉处理，在保持高神经元稀疏性的同时性能可比或更优。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习事件处理方法未充分利用事件数据稀疏性，神经形态计算的脉冲神经网络在复杂任务中性能不佳，且实现高激活稀疏性困难。

Method: 提出CSSL框架，引入上下文感知阈值根据输入分布动态调节神经元激活。

Result: 应用于基于事件的目标检测和光流估计，CSSL性能可比或优于现有方法，同时保持极高的神经元稀疏性。

Conclusion: CSSL对神经形态处理的高效事件视觉至关重要。

Abstract: Event-based camera has emerged as a promising paradigm for robot perception,
offering advantages with high temporal resolution, high dynamic range, and
robustness to motion blur. However, existing deep learning-based event
processing methods often fail to fully leverage the sparse nature of event
data, complicating their integration into resource-constrained edge
applications. While neuromorphic computing provides an energy-efficient
alternative, spiking neural networks struggle to match of performance of
state-of-the-art models in complex event-based vision tasks, like object
detection and optical flow. Moreover, achieving high activation sparsity in
neural networks is still difficult and often demands careful manual tuning of
sparsity-inducing loss terms. Here, we propose Context-aware Sparse
Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware
thresholding to dynamically regulate neuron activations based on the input
distribution, naturally reducing activation density without explicit sparsity
constraints. Applied to event-based object detection and optical flow
estimation, CSSL achieves comparable or superior performance to
state-of-the-art methods while maintaining extremely high neuronal sparsity.
Our experimental results highlight CSSL's crucial role in enabling efficient
event-based vision for neuromorphic processing.

</details>


### [224] [Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration](https://arxiv.org/abs/2508.19254)
*Jookyung Song,Mookyoung Kang,Nojun Kwak*

Main category: cs.CV

TL;DR: 本文提出实时生成绘图系统，结合形式与上下文意图，实现低延迟、支持多用户协作的视觉共创。


<details>
  <summary>Details</summary>
Motivation: 传统文本提示生成系统主要捕捉高层上下文描述，缺乏对底层几何特征的分析，本文旨在构建能同时分析底层几何特征和高层语义线索的系统。

Method: 同时分析底层直观几何特征和高层语义线索，将双意图信号在多阶段生成管道中联合调节，结合轮廓保留结构控制与风格和内容感知图像合成，采用触摸屏界面和分布式推理架构。

Result: 系统实现低延迟两阶段转换，支持在共享画布上多用户协作。

Conclusion: 该平台让不同艺术水平的参与者进行同步共创，重新定义人机交互为共创和相互促进的过程。

Abstract: This paper presents a real-time generative drawing system that interprets and
integrates both formal intent - the structural, compositional, and stylistic
attributes of a sketch - and contextual intent - the semantic and thematic
meaning inferred from its visual content - into a unified transformation
process. Unlike conventional text-prompt-based generative systems, which
primarily capture high-level contextual descriptions, our approach
simultaneously analyzes ground-level intuitive geometric features such as line
trajectories, proportions, and spatial arrangement, and high-level semantic
cues extracted via vision-language models. These dual intent signals are
jointly conditioned in a multi-stage generation pipeline that combines
contour-preserving structural control with style- and content-aware image
synthesis. Implemented with a touchscreen-based interface and distributed
inference architecture, the system achieves low-latency, two-stage
transformation while supporting multi-user collaboration on shared canvases.
The resulting platform enables participants, regardless of artistic expertise,
to engage in synchronous, co-authored visual creation, redefining human-AI
interaction as a process of co-creation and mutual enhancement.

</details>


### [225] [TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models](https://arxiv.org/abs/2508.19257)
*Chenghao Liu,Jiachen Zhang,Chengxuan Li,Zhimu Zhou,Shixin Wu,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: 提出无训练的Temporal Token Fusion (TTF)方法增强VLA推理质量，实验显示有显著提升且模型无关，还揭示注意力机制中选择性重用Query矩阵有益。


<details>
  <summary>Details</summary>
Motivation: 现有Vision-Language-Action (VLA)模型按帧处理视觉输入，丢弃时间信息，易受视觉噪声影响且忽略帧间连贯性。

Method: 提出TTF方法，采用双维度检测，结合灰度像素差异分析和基于注意力的语义相关性评估，通过硬融合策略和关键帧锚定进行选择性时间令牌融合。

Result: 在LIBERO、SimplerEnv和真实机器人任务中均有提升，平均提升4.0个百分点，跨环境验证有4.8%相对提升，真实机器人任务有8.7%相对提升，且模型无关。

Conclusion: TTF方法有效提升VLA推理质量，选择性重用Query矩阵可在加速计算的同时提高任务成功率，为KQV矩阵重用策略指明方向。

Abstract: Vision-Language-Action (VLA) models process visual inputs independently at
each timestep, discarding valuable temporal information inherent in robotic
manipulation tasks. This frame-by-frame processing makes models vulnerable to
visual noise while ignoring the substantial coherence between consecutive
frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a
training-free approach that intelligently integrates historical and current
visual representations to enhance VLA inference quality. Our method employs
dual-dimension detection combining efficient grayscale pixel difference
analysis with attention-based semantic relevance assessment, enabling selective
temporal token fusion through hard fusion strategies and keyframe anchoring to
prevent error accumulation. Comprehensive experiments across LIBERO,
SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0
percentage points average on LIBERO (72.4\% vs 68.4\% baseline),
cross-environment validation on SimplerEnv (4.8\% relative improvement), and
8.7\% relative improvement on real robot tasks. Our approach proves
model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,
TTF reveals that selective Query matrix reuse in attention mechanisms enhances
rather than compromises performance, suggesting promising directions for direct
KQV matrix reuse strategies that achieve computational acceleration while
improving task success rates.

</details>


### [226] [Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation](https://arxiv.org/abs/2508.19289)
*Tai Inui,Steven Oh,Magdeline Kuan*

Main category: cs.CV

TL;DR: 提出无监督幻灯片质量评估管道，结合视觉设计指标与CLIP - ViT嵌入，用孤立森林异常评分评估，效果优于领先的视觉语言模型，能实时提供反馈。


<details>
  <summary>Details</summary>
Motivation: 实现对演示幻灯片质量的可扩展、客观评估，提供实时反馈。

Method: 结合七个专家设计的视觉设计指标和CLIP - ViT嵌入，使用基于孤立森林的异常评分评估幻灯片。在12k专业讲座幻灯片上训练，在六场学术报告（115张幻灯片）上评估。

Result: 与人类视觉质量评级的皮尔逊相关性高达0.83，比领先的视觉语言模型强1.79 - 3.23倍，证明了收敛效度、判别效度和与整体印象的探索性一致性。

Conclusion: 用多模态嵌入增强低级设计线索能近似观众对幻灯片质量的感知，可实时提供可扩展、客观的反馈。

Abstract: We present an unsupervised slide-quality assessment pipeline that combines
seven expert-inspired visual-design metrics (whitespace, colorfulness, edge
density, brightness contrast, text density, color harmony, layout balance) with
CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate
presentation slides. Trained on 12k professional lecture slides and evaluated
on six academic talks (115 slides), our method achieved Pearson correlations up
to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores
from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude
Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual
ratings, discriminant validity against speaker-delivery scores, and exploratory
alignment with overall impressions. Our results show that augmenting low-level
design cues with multimodal embeddings closely approximates audience
perceptions of slide quality, enabling scalable, objective feedback in real
time.

</details>


### [227] [Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation](https://arxiv.org/abs/2508.19290)
*Alexandros Gkillas,Ioulia Kapsali,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CV

TL;DR: 本文介绍用于2D距离视图激光雷达分割对抗防御的高效净化框架，在基准测试和实际场景中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现代分割网络易受对抗攻击，现有防御方法多针对原始3D点云，2D距离视图的轻量级防御方法未被充分探索。

Method: 提出距离视图域的直接攻击公式，开发基于数学优化问题的可解释净化网络。

Result: 在公开基准测试中表现出色，优于生成式和对抗训练基线，在演示车辆的实际部署中能准确运行。

Conclusion: 所提出的框架能以最小计算开销实现强大的对抗恢复能力，适用于实际自动驾驶场景。

Abstract: LiDAR-based segmentation is essential for reliable perception in autonomous
vehicles, yet modern segmentation networks are highly susceptible to
adversarial attacks that can compromise safety. Most existing defenses are
designed for networks operating directly on raw 3D point clouds and rely on
large, computationally intensive generative models. However, many
state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D
range view representations. Despite their widespread adoption, dedicated
lightweight adversarial defenses for this domain remain largely unexplored. We
introduce an efficient model-based purification framework tailored for
adversarial defense in 2D range-view LiDAR segmentation. We propose a direct
attack formulation in the range-view domain and develop an explainable
purification network based on a mathematical justified optimization problem,
achieving strong adversarial resilience with minimal computational overhead.
Our method achieves competitive performance on open benchmarks, consistently
outperforming generative and adversarial training baselines. More importantly,
real-world deployment on a demo vehicle demonstrates the framework's ability to
deliver accurate operation in practical autonomous driving scenarios.

</details>


### [228] [Object Detection with Multimodal Large Vision-Language Models: An In-depth Review](https://arxiv.org/abs/2508.19294)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: 本文深入综述大视觉语言模型（LVLMs）在目标检测领域的发展，分析其功能、架构创新等，对比传统方法，指出其有望超越传统，也提出当前局限及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 探讨LVLMs在目标检测领域的融合如何提升适应性、上下文推理和泛化能力，推动该领域发展。

Method: 采用三步研究综述流程，包括讨论VLMs功能、解释LVLMs架构创新及训练范式、分析视觉和文本信息集成方法。

Result: LVLMs在目标检测的多场景有效，可视化展示其效果，对比显示在实时性能、适应性和复杂性上的特点，有望超越传统方法。

Conclusion: LVLMs已对目标检测和机器人应用产生变革性影响，未来将持续发挥作用。

Abstract: The fusion of language and vision in large vision-language models (LVLMs) has
revolutionized deep learning-based object detection by enhancing adaptability,
contextual reasoning, and generalization beyond traditional architectures. This
in-depth review presents a structured exploration of the state-of-the-art in
LVLMs, systematically organized through a three-step research review process.
First, we discuss the functioning of vision language models (VLMs) for object
detection, describing how these models harness natural language processing
(NLP) and computer vision (CV) techniques to revolutionize object detection and
localization. We then explain the architectural innovations, training
paradigms, and output flexibility of recent LVLMs for object detection,
highlighting how they achieve advanced contextual understanding for object
detection. The review thoroughly examines the approaches used in integration of
visual and textual information, demonstrating the progress made in object
detection using VLMs that facilitate more sophisticated object detection and
localization strategies. This review presents comprehensive visualizations
demonstrating LVLMs' effectiveness in diverse scenarios including localization
and segmentation, and then compares their real-time performance, adaptability,
and complexity to traditional deep learning systems. Based on the review, its
is expected that LVLMs will soon meet or surpass the performance of
conventional methods in object detection. The review also identifies a few
major limitations of the current LVLM modes, proposes solutions to address
those challenges, and presents a clear roadmap for the future advancement in
this field. We conclude, based on this study, that the recent advancement in
LVLMs have made and will continue to make a transformative impact on object
detection and robotic applications in the future.

</details>


### [229] [DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models](https://arxiv.org/abs/2508.19298)
*Abu Sufian,Anirudha Ghosh,Debaditya Barman,Marco Leo,Cosimo Distante*

Main category: cs.CV

TL;DR: 文章通过DemoBias工作，对大视觉语言模型在生物特征人脸识别文本生成任务中的人口统计学偏差进行实证评估，发现模型存在偏差。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在人脸识别中表现出色，但基础模型在不同人口群体中存在表现不均的人口统计学偏差问题。

Method: 在自制的人口平衡数据集上微调并评估LLaVA、BLIP - 2和PaliGemma三个预训练大视觉语言模型，使用特定组的BERT分数和公平差异率等评估指标。

Result: 实验结果揭示LVLMs存在人口统计学偏差，PaliGemma和LLaVA在西班牙裔/拉丁裔、白种人和南亚群体中差异较大，BLIP - 2相对较一致。

Conclusion: 实证研究揭示了LVLMs在不同人口群体中的公平性和可靠性情况。

Abstract: Large Vision Language Models (LVLMs) have demonstrated remarkable
capabilities across various downstream tasks, including biometric face
recognition (FR) with description. However, demographic biases remain a
critical concern in FR, as these foundation models often fail to perform
equitably across diverse demographic groups, considering ethnicity/race,
gender, and age. Therefore, through our work DemoBias, we conduct an empirical
evaluation to investigate the extent of demographic biases in LVLMs for
biometric FR with textual token generation tasks. We fine-tuned and evaluated
three widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own
generated demographic-balanced dataset. We utilize several evaluation metrics,
like group-specific BERTScores and the Fairness Discrepancy Rate, to quantify
and trace the performance disparities. The experimental results deliver
compelling insights into the fairness and reliability of LVLMs across diverse
demographic groups. Our empirical study uncovered demographic biases in LVLMs,
with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,
Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably
consistent. Repository: https://github.com/Sufianlab/DemoBias.

</details>


### [230] [Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities](https://arxiv.org/abs/2508.19305)
*Chen Chu,Cyrus Shahabi*

Main category: cs.CV

TL;DR: 提出Geo2Vec方法解决现有空间表示学习方法的局限，在GeoAI应用中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有空间表示学习方法存在仅针对单一地理实体类型、计算成本高、依赖非自适应采样模糊特征等局限。

Method: 引入受有符号距离场启发的Geo2Vec方法，直接在原空间自适应采样点并编码其有符号距离，用神经网络近似SDF生成表示，提出旋转不变位置编码。

Result: Geo2Vec在表示形状、位置，捕捉拓扑和距离关系以及实际GeoAI应用效率上均优于现有方法。

Conclusion: Geo2Vec是一种有效的空间表示学习方法，能为下游GeoAI模型提供更好的支持。

Abstract: Spatial representation learning is essential for GeoAI applications such as
urban analytics, enabling the encoding of shapes, locations, and spatial
relationships (topological and distance-based) of geo-entities like points,
polylines, and polygons. Existing methods either target a single geo-entity
type or, like Poly2Vec, decompose entities into simpler components to enable
Fourier transformation, introducing high computational cost. Moreover, since
the transformed space lacks geometric alignment, these methods rely on uniform,
non-adaptive sampling, which blurs fine-grained features like edges and
boundaries. To address these limitations, we introduce Geo2Vec, a novel method
inspired by signed distance fields (SDF) that operates directly in the original
space. Geo2Vec adaptively samples points and encodes their signed distances
(positive outside, negative inside), capturing geometry without decomposition.
A neural network trained to approximate the SDF produces compact,
geometry-aware, and unified representations for all geo-entity types.
Additionally, we propose a rotation-invariant positional encoding to model
high-frequency spatial variations and construct a structured and robust
embedding space for downstream GeoAI models. Empirical results show that
Geo2Vec consistently outperforms existing methods in representing shape and
location, capturing topological and distance relationships, and achieving
greater efficiency in real-world GeoAI applications. Code and Data can be found
at: https://github.com/chuchen2017/GeoNeuralRepresentation.

</details>


### [231] [Advancements in Crop Analysis through Deep Learning and Explainable AI](https://arxiv.org/abs/2508.19307)
*Hamza Khan*

Main category: cs.CV

TL;DR: 本文提出用CNN对五种大米品种分类，还结合XAI与深度学习模型诊断水稻叶病，结果显示深度学习在农业应用潜力大。


<details>
  <summary>Details</summary>
Motivation: 人工检测水稻质量和产量劳动强度大、易出错，需自动化解决方案。

Method: 用CNN对大米品种分类，结合XAI与CNN、VGG16等模型诊断水稻叶病，用多种指标评估模型，用SHAP和LIME增强可解释性。

Result: 大米品种分类准确率高、误分类少，开发出准确的水稻叶病诊断方法。

Conclusion: 深度学习在农业应用潜力大，可支持自动化作物质量检测和疾病诊断。

Abstract: Rice is a staple food of global importance in terms of trade, nutrition, and
economic growth. Among Asian nations such as China, India, Pakistan, Thailand,
Vietnam and Indonesia are leading producers of both long and short grain
varieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To
ensure consumer satisfaction and strengthen national reputations, monitoring
rice crops and grain quality is essential. Manual inspection, however, is
labour intensive, time consuming and error prone, highlighting the need for
automated solutions for quality control and yield improvement. This study
proposes an automated approach to classify five rice grain varieties using
Convolutional Neural Networks (CNN). A publicly available dataset of 75000
images was used for training and testing. Model evaluation employed accuracy,
recall, precision, F1-score, ROC curves, and confusion matrices. Results
demonstrated high classification accuracy with minimal misclassifications,
confirming the model effectiveness in distinguishing rice varieties. In
addition, an accurate diagnostic method for rice leaf diseases such as Brown
Spot, Blast, Bacterial Blight, and Tungro was developed. The framework combined
explainable artificial intelligence (XAI) with deep learning models including
CNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP
(SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic
Explanations) revealed how specific grain and leaf features influenced
predictions, enhancing model transparency and reliability. The findings
demonstrate the strong potential of deep learning in agricultural applications,
paving the way for robust, interpretable systems that can support automated
crop quality inspection and disease diagnosis, ultimately benefiting farmers,
consumers, and the agricultural economy.

</details>


### [232] [Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax](https://arxiv.org/abs/2508.19312)
*Ander Galván,Marivi Higuero,Jorge Sasiain,Eduardo Jacob*

Main category: cs.CV

TL;DR: 本文提出适用于开放集场景的联邦学习框架下人脸识别系统，结合OpenMax算法，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 人工智能人脸识别在特定场景有高精度，但在隐私和身份管理方面，尤其是面对未知个体时面临挑战。

Method: 将OpenMax算法集成到联邦学习中，利用平均激活向量和局部距离度量的交换来区分已知和未知主体。

Result: 实验验证了所提解决方案的有效性。

Conclusion: 该方案有潜力在分布式环境中增强注重隐私且鲁棒的人脸识别。

Abstract: Facial recognition powered by Artificial Intelligence has achieved high
accuracy in specific scenarios and applications. Nevertheless, it faces
significant challenges regarding privacy and identity management, particularly
when unknown individuals appear in the operational context. This paper presents
the design, implementation, and evaluation of a facial recognition system
within a federated learning framework tailored to open-set scenarios. The
proposed approach integrates the OpenMax algorithm into federated learning,
leveraging the exchange of mean activation vectors and local distance measures
to reliably distinguish between known and unknown subjects. Experimental
results validate the effectiveness of the proposed solution, demonstrating its
potential for enhancing privacy-aware and robust facial recognition in
distributed environments.
  --
  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado
una alta precisi\'on en algunos escenarios y aplicaciones. Sin embargo,
presenta desaf\'ios relacionados con la privacidad y la identificaci\'on de
personas, especialmente considerando que pueden aparecer sujetos desconocidos
para el sistema que lo implementa. En este trabajo, se propone el dise\~no,
implementaci\'on y evaluaci\'on de un sistema de reconocimiento facial en un
escenario de aprendizaje federado, orientado a conjuntos abiertos.
Concretamente, se dise\~na una soluci\'on basada en el algoritmo OpenMax para
escenarios de aprendizaje federado. La propuesta emplea el intercambio de los
vectores de activaci\'on promedio y distancias locales para identificar de
manera eficaz tanto personas conocidas como desconocidas. Los experimentos
realizados demuestran la implementaci\'on efectiva de la soluci\'on propuesta.

</details>


### [233] [Automated classification of natural habitats using ground-level imagery](https://arxiv.org/abs/2508.19314)
*Mahdis Tourian,Sareh Rowlands,Remy Vandaele,Max Fancourt,Rebecca Mein,Hywel T. P. Williams*

Main category: cs.CV

TL;DR: 提出基于地面图像的栖息地分类方法，用深度学习模型分类，效果良好且有网络应用


<details>
  <summary>Details</summary>
Motivation: 现有栖息地分类方案依赖卫星图像，需改进验证并实现大规模分类

Method: 使用深度学习处理地面栖息地照片，图像预处理，用重采样平衡数据，开发并微调DeepLabV3 - ResNet101分类器，用五折交叉验证

Result: 模型在18个栖息地类别中表现良好，平均F1分数0.61，部分类别超0.90，部分较低

Conclusion: 该方法有生态监测潜力，地面图像易获取，基于此的分类方法有诸多应用，还提供网络应用

Abstract: Accurate classification of terrestrial habitats is critical for biodiversity
conservation, ecological monitoring, and land-use planning. Several habitat
classification schemes are in use, typically based on analysis of satellite
imagery with validation by field ecologists. Here we present a methodology for
classification of habitats based solely on ground-level imagery (photographs),
offering improved validation and the ability to classify habitats at scale (for
example using citizen-science imagery). In collaboration with Natural England,
a public sector organisation responsible for nature conservation in England,
this study develops a classification system that applies deep learning to
ground-level habitat photographs, categorising each image into one of 18
classes defined by the 'Living England' framework. Images were pre-processed
using resizing, normalisation, and augmentation; re-sampling was used to
balance classes in the training data and enhance model robustness. We developed
and fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label
to each photograph. Using five-fold cross-validation, the model demonstrated
strong overall performance across 18 habitat classes, with accuracy and
F1-scores varying between classes. Across all folds, the model achieved a mean
F1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and
Peat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or
ambiguous classes scoring lower. These findings demonstrate the potential of
this approach for ecological monitoring. Ground-level imagery is readily
obtained, and accurate computational methods for habitat classification based
on such data have many potential applications. To support use by practitioners,
we also provide a simple web application that classifies uploaded images using
our model.

</details>


### [234] [MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation](https://arxiv.org/abs/2508.19320)
*Ming Chen,Liyuan Cui,Wenyuan Zhang,Haoxian Zhang,Yan Zhou,Xiaohan Li,Xiaoqiang Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: 提出自回归视频生成框架，实现交互式多模态控制和低延迟外推，实验证明其低延迟、高效和细粒度多模态可控的优势。


<details>
  <summary>Details</summary>
Motivation: 现有交互式数字人视频生成方法存在高延迟、计算成本高和可控性有限的问题，需要构建能实时与多样输入信号交互的实用系统。

Method: 引入自回归视频生成框架，对标准大语言模型进行最小修改以接受多模态条件编码，构建约20000小时的大规模对话数据集，引入深度压缩自编码器减轻推理负担。

Result: 在双向对话、多语言人物合成和交互式世界模型的实验中展现出低延迟、高效和细粒度多模态可控的优势。

Conclusion: 所提出的自回归视频生成框架能有效解决现有方法的问题，实现交互式多模态控制和低延迟外推。

Abstract: Recently, interactive digital human video generation has attracted widespread
attention and achieved remarkable progress. However, building such a practical
system that can interact with diverse input signals in real time remains
challenging to existing methods, which often struggle with high latency, heavy
computational cost, and limited controllability. In this work, we introduce an
autoregressive video generation framework that enables interactive multimodal
control and low-latency extrapolation in a streaming manner. With minimal
modifications to a standard large language model (LLM), our framework accepts
multimodal condition encodings including audio, pose, and text, and outputs
spatially and semantically coherent representations to guide the denoising
process of a diffusion head. To support this, we construct a large-scale
dialogue dataset of approximately 20,000 hours from multiple sources, providing
rich conversational scenarios for training. We further introduce a deep
compression autoencoder with up to 64$\times$ reduction ratio, which
effectively alleviates the long-horizon inference burden of the autoregressive
model. Extensive experiments on duplex conversation, multilingual human
synthesis, and interactive world model highlight the advantages of our approach
in low latency, high efficiency, and fine-grained multimodal controllability.

</details>


### [235] [Deep Data Hiding for ICAO-Compliant Face Images: A Survey](https://arxiv.org/abs/2508.19324)
*Jefferson David Rodriguez Chivata,Davide Ghiani,Simone Maurizio La Cava,Marco Micheletto,Giulia Orrù,Federico Lama,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: ICAO合规面部图像在身份验证中愈发重要，但存在安全风险，本文研究数字水印和隐写术作为补充方案，并分析相关技术。


<details>
  <summary>Details</summary>
Motivation: ICAO合规面部图像虽实现全球互操作性，但易被用于有害目的，传统对策有局限，需新解决方案。

Method: 对涉及ICAO合规图像应用的数字水印和隐写术的现有技术进行全面分析。

Result: 评估了潜在和缺点，突出了关键权衡。

Conclusion: 为现实世界身份系统的安全部署提供指导。

Abstract: ICAO-compliant facial images, initially designed for secure biometric
passports, are increasingly becoming central to identity verification in a wide
range of application contexts, including border control, digital travel
credentials, and financial services. While their standardization enables global
interoperability, it also facilitates practices such as morphing and deepfakes,
which can be exploited for harmful purposes like identity theft and illegal
sharing of identity documents. Traditional countermeasures like Presentation
Attack Detection (PAD) are limited to real-time capture and offer no
post-capture protection. This survey paper investigates digital watermarking
and steganography as complementary solutions that embed tamper-evident signals
directly into the image, enabling persistent verification without compromising
ICAO compliance. We provide the first comprehensive analysis of
state-of-the-art techniques to evaluate the potential and drawbacks of the
underlying approaches concerning the applications involving ICAO-compliant
images and their suitability under standard constraints. We highlight key
trade-offs, offering guidance for secure deployment in real-world identity
systems.

</details>


### [236] [Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage](https://arxiv.org/abs/2508.19477)
*Zachary L. Crang,Rich D. Johnston,Katie L. Mills,Johsan Billingham,Sam Robertson,Michael H. Cole,Jonathon Weakley,Adam Hewitt and,Grant M. Duthie*

Main category: cs.CV

TL;DR: 研究评估商用计算机视觉和AI球员跟踪软件利用转播镜头测量球员位置、速度和距离的准确性，及相机源和分辨率的影响，得出一定结论。


<details>
  <summary>Details</summary>
Motivation: 了解商用计算机视觉和AI球员跟踪软件能否用转播镜头准确测量球员位置、速度和距离，以及相机源和分辨率对准确性的影响。

Method: 从2022卡塔尔世界杯一场比赛获取数据，使用三种相机源，让三家提供商分析球员位置和速度，与高精度多相机跟踪系统对比，计算RMSE和平均偏差。

Result: 位置RMSE为1.68 - 16.39米，速度RMSE为0.34 - 2.38米/秒，总比赛距离平均偏差在 -1745米(-21.8%)到1945米(24.3%)之间。

Conclusion: 软件检测到球员时能较精确跟踪，跟踪位置和速度应使用战术源以提高准确性，720p和1080p分辨率在合适模型下适用。

Abstract: This study aimed to: (1) understand whether commercially available
computer-vision and artificial intelligence (AI) player tracking software can
accurately measure player position, speed and distance using broadcast footage
and (2) determine the impact of camera feed and resolution on accuracy. Data
were obtained from one match at the 2022 Qatar Federation Internationale de
Football Association (FIFA) World Cup. Tactical, programme and camera 1 feeds
were used. Three commercial tracking providers that use computer-vision and AI
participated. Providers analysed instantaneous position (x, y coordinates) and
speed (m\,s^{-1}) of each player. Their data were compared with a
high-definition multi-camera tracking system (TRACAB Gen 5). Root mean square
error (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to
16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\,s^{-1}. Total match
distance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across
providers. Computer-vision and AI player tracking software offer the ability to
track players with fair precision when players are detected by the software.
Providers should use a tactical feed when tracking position and speed, which
will maximise player detection, improving accuracy. Both 720p and 1080p
resolutions are suitable, assuming appropriate computer-vision and AI models
are implemented.

</details>


### [237] [Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery](https://arxiv.org/abs/2508.19499)
*Xiangxu Wang,Tianhong Zhao,Wei Tu,Bowen Zhang,Guanzhou Chen,Jinzhou Cao*

Main category: cs.CV

TL;DR: 本文提出Sat2Flow框架，仅用卫星图像生成OD流，解决现有方法依赖辅助特征和对空间拓扑敏感的问题，实验表明其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有生成OD流矩阵的方法存在依赖昂贵且空间覆盖有限的辅助特征，以及对空间拓扑敏感的问题。

Method: 提出Sat2Flow框架，引入多核编码器，采用置换感知扩散过程，通过联合对比训练目标和等变扩散训练确保拓扑鲁棒性。

Result: 在真实城市数据集上，Sat2Flow在数值准确性上优于基于物理和数据驱动的基线，能保留经验分布和空间结构。

Conclusion: Sat2Flow为数据稀缺城市环境中的OD流生成提供了可扩展解决方案，消除对特定区域辅助数据的依赖，保持结构不变性以进行稳健的移动性建模。

Abstract: Origin-Destination (OD) flow matrices are essential for urban mobility
analysis, underpinning applications in traffic forecasting, infrastructure
planning, and policy design. However, existing methods suffer from two critical
limitations: (1) reliance on auxiliary features (e.g., Points of Interest,
socioeconomic statistics) that are costly to collect and have limited spatial
coverage; and (2) sensitivity to spatial topology, where minor index reordering
of urban regions (e.g., census tract relabeling) disrupts structural coherence
in generated flows. To address these challenges, we propose Sat2Flow, a latent
structure-aware diffusion-based framework that generates structurally coherent
OD flows using solely satellite imagery as input. Our approach introduces a
multi-kernel encoder to capture diverse regional interactions and employs a
permutation-aware diffusion process that aligns latent representations across
different regional orderings. Through a joint contrastive training objective
that bridges satellite-derived features with OD patterns, combined with
equivariant diffusion training that enforces structural consistency, Sat2Flow
ensures topological robustness under arbitrary regional reindexing.
Experimental results on real-world urban datasets demonstrate that Sat2Flow
outperforms both physics-based and data-driven baselines in numerical accuracy
while preserving empirical distributions and spatial structures under index
permutations. Sat2Flow offers a globally scalable solution for OD flow
generation in data-scarce urban environments, eliminating region-specific
auxiliary data dependencies while maintaining structural invariance for robust
mobility modeling.

</details>


### [238] [Large VLM-based Stylized Sports Captioning](https://arxiv.org/abs/2508.19295)
*Sauptik Dhar,Nicholas Buoncristiani,Joe Anakata,Haoyu Zhang,Michelle Munson*

Main category: cs.CV

TL;DR: 现有大语言模型在体育领域生成高质量解说存在局限，本文提出两级微调LVLM管道，提升了指标，在超级碗有实际应用。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在体育领域，尤其是准确识别和自然语言描述比赛方面关注较少，现有模型缺乏领域专业术语，难以生成理想的体育解说。

Method: 提出两级微调的LVLM管道。

Result: 相比其他方法，F1指标提升超8 - 10%，BERT分数提升超2 - 10%，内存占用小、执行速度快，在超级碗LIX中能高效生成准确且风格化的解说。

Conclusion: 所提两级微调LVLM管道能有效解决现有大语言模型在体育图像解说生成方面的问题，有实际应用价值。

Abstract: The advent of large (visual) language models (LLM / LVLM) have led to a
deluge of automated human-like systems in several domains including social
media content generation, search and recommendation, healthcare prognosis, AI
assistants for cognitive tasks etc. Although these systems have been
successfully integrated in production; very little focus has been placed on
sports, particularly accurate identification and natural language description
of the game play. Most existing LLM/LVLMs can explain generic sports
activities, but lack sufficient domain-centric sports' jargon to create natural
(human-like) descriptions. This work highlights the limitations of existing
SoTA LLM/LVLMs for generating production-grade sports captions from images in a
desired stylized format, and proposes a two-level fine-tuned LVLM pipeline to
address that. The proposed pipeline yields an improvement > 8-10% in the F1,
and > 2-10% in BERT score compared to alternative approaches. In addition, it
has a small runtime memory footprint and fast execution time. During Super Bowl
LIX the pipeline proved its practical application for live professional sports
journalism; generating highly accurate and stylized captions at the rate of 6
images per 3-5 seconds for over 1000 images during the game play.

</details>


### [239] [WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization](https://arxiv.org/abs/2508.19544)
*Eduardo Davalos,Yike Zhang,Namrata Srivastava,Yashvitha Thatigotla,Jorge A. Salas,Sara McFadden,Sun-Joo Cho,Amanda Goodwin,Ashwin TS,Gautam Biswas*

Main category: cs.CV

TL;DR: 提出WebEyeTrack框架集成轻量级模型到浏览器，适配新用户，取得SOTA性能和实时推理速度，开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有新注视估计方法在实际应用与商业眼动追踪方案有差距，网络摄像头眼动追踪方法精度不足，模型大小、推理时间和隐私等因素常未解决。

Method: 引入WebEyeTrack框架，将轻量级SOTA注视估计模型集成到浏览器，结合基于模型的头部姿态估计和设备端少样本学习（校准样本k < 9）。

Result: 在GazeCapture上误差为2.32 cm，在iPhone 14上实时推理速度为2.4毫秒。

Conclusion: WebEyeTrack能解决现有注视估计方法的问题，实现良好性能和实时推理。

Abstract: With advancements in AI, new gaze estimation methods are exceeding
state-of-the-art (SOTA) benchmarks, but their real-world application reveals a
gap with commercial eye-tracking solutions. Factors like model size, inference
time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking
methods lack sufficient accuracy, in particular due to head movement. To tackle
these issues, we introduce We bEyeTrack, a framework that integrates
lightweight SOTA gaze estimation models directly in the browser. It
incorporates model-based head pose estimation and on-device few-shot learning
with as few as nine calibration samples (k < 9). WebEyeTrack adapts to new
users, achieving SOTA performance with an error margin of 2.32 cm on
GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.
Our open-source code is available at
https://github.com/RedForestAi/WebEyeTrack.

</details>


### [240] [FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection](https://arxiv.org/abs/2508.19565)
*Yuhang Zhao,Zixing Wang*

Main category: cs.CV

TL;DR: 提出FlowDet探测器，采用解耦编码器优化策略，在新数据集上评估达SOTA，效率和精度提升。


<details>
  <summary>Details</summary>
Motivation: 端到端目标检测器计算成本高，尤其在复杂场景如交叉路口交通监控，需解决此问题。

Method: 提出FlowDet，采用解耦编码器优化策略，使用GDU进行交通感知几何建模，SAA模块应对极端尺度变化，收集Intersection - Flow - 5k数据集评估。

Result: 在Intersection - Flow - 5k上达SOTA，比RT - DETR基线AP(test)提升1.5%，AP50(test)提升1.6%，减少63.2% GFLOPs，推理速度提升16.2%。

Conclusion: 为构建高效准确的现实感知系统探测器开辟新路径。

Abstract: End-to-end object detectors offer a promising NMS-free paradigm for real-time
applications, yet their high computational cost remains a significant barrier,
particularly for complex scenarios like intersection traffic monitoring. To
address this challenge, we propose FlowDet, a high-speed detector featuring a
decoupled encoder optimization strategy applied to the DETR architecture.
Specifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for
traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to
maintain high representational power across extreme scale variations. To
rigorously evaluate the model's performance in environments with severe
occlusion and high object density, we collected the Intersection-Flow-5k
dataset, a new challenging scene for this task. Evaluated on
Intersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to
the strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by
1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference
speed by 16.2%. Our work demonstrates a new path towards building highly
efficient and accurate detectors for demanding, real-world perception systems.
The Intersection-Flow-5k dataset is available at
https://github.com/AstronZh/Intersection-Flow-5K.

</details>


### [241] [Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation](https://arxiv.org/abs/2508.19574)
*Mingxi Fu,Fanglei Fu,Xitong Ling,Huaitian Yuan,Tian Guan,Yonghong He,Lianghui Zhu*

Main category: cs.CV

TL;DR: 提出 MPAMatch 分割框架，结合多模态原型监督和对比学习，改造 TransUNet 架构，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 病理图像分割存在语义边界模糊和像素级标注成本高的问题，现有半监督方法难捕捉高层语义先验。

Method: 提出 MPAMatch 框架，采用图像和文本原型与像素标签的双重对比学习，用预训练基础模型替换 TransUNet 的 ViT 骨干。

Result: 在多个数据集上实验表明 MPAMatch 优于现有方法。

Conclusion: MPAMatch 在结构和语义建模上有双重优势。

Abstract: Pathological image segmentation faces numerous challenges, particularly due
to ambiguous semantic boundaries and the high cost of pixel-level annotations.
Although recent semi-supervised methods based on consistency regularization
(e.g., UniMatch) have made notable progress, they mainly rely on
perturbation-based consistency within the image modality, making it difficult
to capture high-level semantic priors, especially in structurally complex
pathology images. To address these limitations, we propose MPAMatch - a novel
segmentation framework that performs pixel-level contrastive learning under a
multimodal prototype-guided supervision paradigm. The core innovation of
MPAMatch lies in the dual contrastive learning scheme between image prototypes
and pixel labels, and between text prototypes and pixel labels, providing
supervision at both structural and semantic levels. This coarse-to-fine
supervisory strategy not only enhances the discriminative capability on
unlabeled samples but also introduces the text prototype supervision into
segmentation for the first time, significantly improving semantic boundary
modeling. In addition, we reconstruct the classic segmentation architecture
(TransUNet) by replacing its ViT backbone with a pathology-pretrained
foundation model (Uni), enabling more effective extraction of
pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,
EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art
methods, validating its dual advantages in structural and semantic modeling.

</details>


### [242] [Interact-Custom: Customized Human Object Interaction Image Generation](https://arxiv.org/abs/2508.19575)
*Zhu Xu,Zhaowen Wang,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: 本文聚焦定制化人体与物体交互图像生成任务，提出两阶段模型Interact - Custom，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有合成定制图像生成方法忽视目标实体间细粒度交互控制，作者聚焦人体与物体交互场景提出CHOI任务。

Method: 处理大规模数据集，设计两阶段模型Interact - Custom，先生成前景掩码明确空间配置，再在掩码指导下生成目标人物图像并保留身份特征，还可根据用户提供背景和位置进行定制。

Result: 在针对CHOI任务定制的指标上进行大量实验，证明了方法的有效性。

Conclusion: 提出的Interact - Custom模型能有效实现定制化人体与物体交互图像生成，具有较高内容可控性。

Abstract: Compositional Customized Image Generation aims to customize multiple target
concepts within generation content, which has gained attention for its wild
application.Existing approaches mainly concentrate on the target entity's
appearance preservation, while neglecting the fine-grained interaction control
among target entities.To enable the model of such interaction control
capability, we focus on human object interaction scenario and propose the task
of Customized Human Object Interaction Image Generation(CHOI), which
simultaneously requires identity preservation for target human object and the
interaction semantic control between them.Two primary challenges exist for
CHOI:(1)simultaneous identity preservation and interaction control demands
require the model to decompose the human object into self-contained identity
features and pose-oriented interaction features, while the current HOI image
datasets fail to provide ideal samples for such feature-decomposed
learning.(2)inappropriate spatial configuration between human and object may
lead to the lack of desired interaction semantics.To tackle it, we first
process a large-scale dataset, where each sample encompasses the same pair of
human object involving different interactive poses.Then we design a two-stage
model Interact-Custom, which firstly explicitly models the spatial
configuration by generating a foreground mask depicting the interaction
behavior, then under the guidance of this mask, we generate the target human
object interacting while preserving their identities features.Furthermore, if
the background image and the union location of where the target human object
should appear are provided by users, Interact-Custom also provides the optional
functionality to specify them, offering high content controllability. Extensive
experiments on our tailored metrics for CHOI task demonstrate the effectiveness
of our approach.

</details>


### [243] [UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models](https://arxiv.org/abs/2508.19498)
*Yimu Wang,Weiming Zhuang,Chen Chen,Jiabo Huang,Jingtao Li,Lingjuan Lyu*

Main category: cs.CV

TL;DR: 介绍了一种名为UNIFORM的框架，用于将现成模型知识转移到学生模型，实验表明其能提升无监督目标识别性能且可扩展性好。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型知识整合方案受限于训练数据分布和网络架构假设，存在数据和归纳偏差，需有效利用预训练模型集体知识。

Method: 提出UNIFORM框架，采用专用投票机制在对数几率和特征层面捕捉知识共识。

Result: UNIFORM有效提升无监督目标识别性能，相比基线表现更好，可扩展性强，能从超百个教师模型中受益。

Conclusion: UNIFORM框架能在无约束条件下实现知识转移，提升性能且有良好可扩展性。

Abstract: In the era of deep learning, the increasing number of pre-trained models
available online presents a wealth of knowledge. These models, developed with
diverse architectures and trained on varied datasets for different tasks,
provide unique interpretations of the real world. Their collective consensus is
likely universal and generalizable to unseen data. However, effectively
harnessing this collective knowledge poses a fundamental challenge due to the
heterogeneity of pre-trained models. Existing knowledge integration solutions
typically rely on strong assumptions about training data distributions and
network architectures, limiting them to learning only from specific types of
models and resulting in data and/or inductive biases. In this work, we
introduce a novel framework, namely UNIFORM, for knowledge transfer from a
diverse set of off-the-shelf models into one student model without such
constraints. Specifically, we propose a dedicated voting mechanism to capture
the consensus of knowledge both at the logit level -- incorporating teacher
models that are capable of predicting target classes of interest -- and at the
feature level, utilizing visual representations learned on arbitrary label
spaces. Extensive experiments demonstrate that UNIFORM effectively enhances
unsupervised object recognition performance compared to strong knowledge
transfer baselines. Notably, it exhibits remarkable scalability by benefiting
from over one hundred teachers, while existing methods saturate at a much
smaller scale.

</details>


### [244] [IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2508.19604)
*Qizhe Fan,Chaoyu Liu,Zhonghua Qiao,Xiaoqin Shen*

Main category: cs.CV

TL;DR: 提出IELDM和IELFormer提升领域泛化语义分割性能，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型生成的合成数据有缺陷，训练分割模型会导致性能下降和误差累积。

Method: 将逆演化层（IELs）集成到生成过程，提出IELDM框架；将IELs嵌入DGSS模型解码器提出IELFormer；IELFormer加入多尺度频率融合（MFF）模块。

Result: 在基准数据集上的实验显示，所提方法比现有方法有更优的泛化性能。

Conclusion: 所提方法能有效解决合成数据缺陷问题，提升跨领域场景下的泛化能力。

Abstract: Domain Generalized Semantic Segmentation (DGSS) focuses on training a model
using labeled data from a source domain, with the goal of achieving robust
generalization to unseen target domains during inference. A common approach to
improve generalization is to augment the source domain with synthetic data
generated by diffusion models (DMs). However, the generated images often
contain structural or semantic defects due to training imperfections. Training
segmentation models with such flawed data can lead to performance degradation
and error accumulation. To address this issue, we propose to integrate inverse
evolution layers (IELs) into the generative process. IELs are designed to
highlight spatial discontinuities and semantic inconsistencies using
Laplacian-based priors, enabling more effective filtering of undesirable
generative patterns. Based on this mechanism, we introduce IELDM, an enhanced
diffusion-based data augmentation framework that can produce higher-quality
images. Furthermore, we observe that the defect-suppression capability of IELs
can also benefit the segmentation network by suppressing artifact propagation.
Based on this insight, we embed IELs into the decoder of the DGSS model and
propose IELFormer to strengthen generalization capability in cross-domain
scenarios. To further strengthen the model's semantic consistency across
scales, IELFormer incorporates a multi-scale frequency fusion (MFF) module,
which performs frequency-domain analysis to achieve structured integration of
multi-resolution features, thereby improving cross-scale coherence. Extensive
experiments on benchmark datasets demonstrate that our approach achieves
superior generalization performance compared to existing methods.

</details>


### [245] [Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition](https://arxiv.org/abs/2508.19630)
*Xiaolei Wei,Yi Ouyang,Haibo Ye*

Main category: cs.CV

TL;DR: 提出DQRoute框架，结合难度感知优化与动态专家协作，在长尾基准测试中提升性能，尤其对稀有和难分类别。


<details>
  <summary>Details</summary>
Motivation: 长尾视觉识别因类别不平衡和分类难度差异具有挑战性，简单按频率重新加权类别会忽略难学类别。

Method: 先基于预测不确定性和历史性能估计类别难度，用自适应损失加权指导训练；采用混合专家设计，各专家专注不同类别分布区域；推理时用专家特定OOD检测器的置信分数加权专家预测，实现输入自适应路由，所有组件端到端联合训练。

Result: 在标准长尾基准测试中，DQRoute显著提高性能，尤其在稀有和难分类别上。

Conclusion: 将难度建模与分散式专家路由结合有益。

Abstract: Long-tailed visual recognition is challenging not only due to class imbalance
but also because of varying classification difficulty across categories. Simply
reweighting classes by frequency often overlooks those that are intrinsically
hard to learn. To address this, we propose \textbf{DQRoute}, a modular
framework that combines difficulty-aware optimization with dynamic expert
collaboration. DQRoute first estimates class-wise difficulty based on
prediction uncertainty and historical performance, and uses this signal to
guide training with adaptive loss weighting. On the architectural side, DQRoute
employs a mixture-of-experts design, where each expert specializes in a
different region of the class distribution. At inference time, expert
predictions are weighted by confidence scores derived from expert-specific OOD
detectors, enabling input-adaptive routing without the need for a centralized
router. All components are trained jointly in an end-to-end manner. Experiments
on standard long-tailed benchmarks demonstrate that DQRoute significantly
improves performance, particularly on rare and difficult classes, highlighting
the benefit of integrating difficulty modeling with decentralized expert
routing.

</details>


### [246] [Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction](https://arxiv.org/abs/2508.19862)
*Long Chen,Ashiv Patel,Mengyun Qiao,Mohammad Yousuf Salmasi,Salah A. Hammouche,Vasilis Stavrinides,Jasleen Nagi,Soodeh Kalaie,Xiao Yun Xu,Wenjia Bai,Declan P. O'Regan*

Main category: cs.CV

TL;DR: 提出MCMeshGAN用于3D动脉瘤生长预测，在准确性上优于现有方法，代码公开。


<details>
  <summary>Details</summary>
Motivation: 个性化、准确预测主动脉瘤进展对及时干预至关重要，但因需建模复杂3D几何形状中的局部变形和全局解剖变化而具有挑战性。

Method: 提出MCMeshGAN，采用双分支架构，结合KCN和GCN，有专门条件分支编码临床属性和时间间隔；策划TAAMesh数据集。

Result: MCMeshGAN在几何准确性和直径估计上始终优于现有基线方法。

Conclusion: 该框架向临床可用的个性化3D疾病轨迹建模迈出了坚实一步。

Abstract: Personalized, accurate prediction of aortic aneurysm progression is essential
for timely intervention but remains challenging due to the need to model both
subtle local deformations and global anatomical changes within complex 3D
geometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh
generative adversarial network for 3D aneurysm growth prediction. MCMeshGAN
introduces a dual-branch architecture combining a novel local KNN-based
convolutional network (KCN) to preserve fine-grained geometric details and a
global graph convolutional network (GCN) to capture long-range structural
context, overcoming the over-smoothing limitations of deep GCNs. A dedicated
condition branch encodes clinical attributes (age, sex) and the target time
interval to generate anatomically plausible, temporally controlled predictions,
enabling retrospective and prospective modeling. We curated TAAMesh, a new
longitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal
records (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive
experiments demonstrate that MCMeshGAN consistently outperforms
state-of-the-art baselines in both geometric accuracy and clinically important
diameter estimation. This framework offers a robust step toward clinically
deployable, personalized 3D disease trajectory modeling. The source code for
MCMeshGAN and the baseline methods is publicly available at
https://github.com/ImperialCollegeLondon/MCMeshGAN.

</details>


### [247] [TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations](https://arxiv.org/abs/2508.19866)
*François G. Landry,Moulay A. Akhloufi*

Main category: cs.CV

TL;DR: 提出TrajFusionNet模型结合行人未来轨迹和车速预测来预测行人过街意图，具低推理时间和先进性能


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆上路，行人过街意图预测成为研究热点

Method: 提出TrajFusionNet模型，含序列注意力模块和视觉注意力模块，结合多模态信息

Result: 在推理时间上优于现有方法，在三个常用数据集上取得了最先进的结果

Conclusion: TrajFusionNet模型在行人过街意图预测任务中表现良好，具有较低的推理时间和较高的性能

Abstract: With the introduction of vehicles with autonomous capabilities on public
roads, predicting pedestrian crossing intention has emerged as an active area
of research. The task of predicting pedestrian crossing intention involves
determining whether pedestrians in the scene are likely to cross the road or
not. In this work, we propose TrajFusionNet, a novel transformer-based model
that combines future pedestrian trajectory and vehicle speed predictions as
priors for predicting crossing intention. TrajFusionNet comprises two branches:
a Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM
branch learns from a sequential representation of the observed and predicted
pedestrian trajectory and vehicle speed. Complementarily, the VAM branch
enables learning from a visual representation of the predicted pedestrian
trajectory by overlaying predicted pedestrian bounding boxes onto scene images.
By utilizing a small number of lightweight modalities, TrajFusionNet achieves
the lowest total inference time (including model runtime and data
preprocessing) among current state-of-the-art approaches. In terms of
performance, it achieves state-of-the-art results across the three most
commonly used datasets for pedestrian crossing intention prediction.

</details>


### [248] [Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception](https://arxiv.org/abs/2508.19638)
*Yang Li,Quan Yuan,Guiyang Luo,Xiaoyuan Fu,Rui Pan,Yujia Yang,Congzhang Shao,Yuewen Liu,Jinglin Li*

Main category: cs.CV

TL;DR: 提出CoPLOT框架用于协作感知，采用点级优化令牌，在模拟和真实数据集上表现优于现有模型，且开销更低。


<details>
  <summary>Details</summary>
Motivation: 现有协作感知方法使用2D BEV表示会丢弃关键3D结构线索，影响目标识别和定位。

Method: 引入点级令牌作为中间表示，提出CoPLOT框架，包含令牌重排序、序列建模和多智能体空间对齐等模块。

Result: 在模拟和真实数据集上的实验表明，CoPLOT优于现有模型，且通信和计算开销更低。

Conclusion: CoPLOT是一种有效的协作感知框架，具有良好的性能和较低的开销。

Abstract: Collaborative perception allows agents to enhance their perceptual
capabilities by exchanging intermediate features. Existing methods typically
organize these intermediate features as 2D bird's-eye-view (BEV)
representations, which discard critical fine-grained 3D structural cues
essential for accurate object recognition and localization. To this end, we
first introduce point-level tokens as intermediate representations for
collaborative perception. However, point-cloud data are inherently unordered,
massive, and position-sensitive, making it challenging to produce compact and
aligned point-level token sequences that preserve detailed structural
information. Therefore, we present CoPLOT, a novel Collaborative perception
framework that utilizes Point-Level Optimized Tokens. It incorporates a
point-native processing pipeline, including token reordering, sequence
modeling, and multi-agent spatial alignment. A semantic-aware token reordering
module generates adaptive 1D reorderings by leveraging scene-level and
token-level semantic information. A frequency-enhanced state space model
captures long-range sequence dependencies across both spatial and spectral
domains, improving the differentiation between foreground tokens and background
clutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop
process, combining global agent-level correction with local token-level
refinement to mitigate localization noise. Extensive experiments on both
simulated and real-world datasets show that CoPLOT outperforms state-of-the-art
models, with even lower communication and computation overhead. Code will be
available at https://github.com/CheeryLeeyy/CoPLOT.

</details>


### [249] [Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network](https://arxiv.org/abs/2508.19875)
*Hui Zhang,Jianghui Cai,Haifeng Yang,Ali Luo,Yuqing Yang,Xiao Kong,Zhichao Ding,Lichan Zhou,Qin Han*

Main category: cs.CV

TL;DR: 提出基于互信息的天空背景估计模型SMI用于多目标光纤光谱处理，实验表明其在观测中能获得更好的目标天空背景，尤其在蓝端。


<details>
  <summary>Details</summary>
Motivation: 当前天空背景减法主要依赖天空光纤光谱构建超级天空，缺乏对目标周围环境的建模。

Method: SMI基于互信息和增量训练方法，利用板中所有光纤的光谱估计天空背景，包含两个主要网络，分别进行波长校准和最大化、最小化互信息操作。

Result: 在LAMOST光谱上的实验表明，SMI能在观测中获得更好的目标天空背景，尤其在蓝端。

Conclusion: SMI方法有效，能在多目标光纤光谱处理中获得更好的天空背景估计。

Abstract: Sky background subtraction is a critical step in Multi-objective Fiber
spectra process. However, current subtraction relies mainly on sky fiber
spectra to build Super Sky. These average spectra are lacking in the modeling
of the environment surrounding the objects. To address this issue, a sky
background estimation model: Sky background building based on Mutual
Information (SMI) is proposed. SMI based on mutual information and incremental
training approach. It utilizes spectra from all fibers in the plate to estimate
the sky background. SMI contains two main networks, the first network applies a
wavelength calibration module to extract sky features from spectra, and can
effectively solve the feature shift problem according to the corresponding
emission position. The second network employs an incremental training approach
to maximize mutual information between representations of different spectra to
capturing the common component. Then, it minimizes the mutual information
between adjoining spectra representations to obtain individual components. This
network yields an individual sky background at each location of the object. To
verify the effectiveness of the method in this paper, we conducted experiments
on the spectra of LAMOST. Results show that SMI can obtain a better object sky
background during the observation, especially in the blue end.

</details>


### [250] [Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies](https://arxiv.org/abs/2508.20072)
*Zhixuan Liang,Yizhuo Li,Tianshuo Yang,Chengyue Wu,Sitong Mao,Liuao Pei,Xiaokang Yang,Jiangmiao Pang,Yao Mu,Ping Luo*

Main category: cs.CV

TL;DR: 提出Discrete Diffusion VLA方法，解决现有VLA解码器问题，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有VLA解码器需专门训练和迭代采样，缺乏统一可扩展架构。

Method: 用离散扩散对离散动作块建模，以交叉熵目标训练，有自适应解码顺序和二次掩码。

Result: 在LIBERO、SimplerEnv Fractal和SimplerEnv Bridge上表现优于自回归和连续扩散基线。

Conclusion: 离散扩散动作解码器支持精确动作建模和一致训练，为VLA扩展奠定基础。

Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to
map images and instructions to robot actions. However, prevailing VLA decoders
either generate actions autoregressively in a fixed left-to-right order or
attach continuous diffusion or flow matching heads outside the backbone,
demanding specialized training and iterative sampling that hinder a unified,
scalable architecture. We present Discrete Diffusion VLA, a single-transformer
policy that models discretized action chunks with discrete diffusion and is
trained with the same cross-entropy objective as the VLM backbone. The design
retains diffusion's progressive refinement paradigm while remaining natively
compatible with the discrete token interface of VLMs. Our method achieves an
adaptive decoding order that resolves easy action elements before harder ones
and uses secondary remasking to revisit uncertain predictions across refinement
rounds, which improves consistency and enables robust error correction. This
unified decoder preserves pretrained vision language priors, supports parallel
decoding, breaks the autoregressive bottleneck, and reduces the number of
function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,
71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv
Bridge, improving over both autoregressive and continuous diffusion baselines.
These findings indicate that discrete-diffusion action decoder supports precise
action modeling and consistent training, laying groundwork for scaling VLA to
larger models and datasets.

</details>


### [251] [A bag of tricks for real-time Mitotic Figure detection](https://arxiv.org/abs/2508.19804)
*Christian Marzahl,Brian Napora*

Main category: cs.CV

TL;DR: 本文提出一组训练技巧实现跨领域有丝分裂图（MF）实时检测，基于RTMDet，在多数据集验证表现良好，兼顾准确性与速度，适合临床应用。


<details>
  <summary>Details</summary>
Motivation: 由于幻灯片扫描仪、染色协议、组织类型差异及伪影存在，组织病理学图像中MF检测具有挑战性，需实现跨领域的稳健实时检测。

Method: 基于高效的RTMDet单阶段目标检测器，通过多领域训练数据、平衡采样和精心增强解决扫描仪变异性和肿瘤异质性问题，对坏死和碎片组织进行针对性难负样本挖掘以减少误报。

Result: 在多MF数据集的分组5折交叉验证中，模型F1分数在0.78 - 0.84之间；在MIDOG 2025挑战的初步测试集上，基于RTMDet - S的单阶段方法F1达到0.81，优于更大模型。

Conclusion: 所提方案在准确性和速度之间取得实用平衡，适合现实临床应用。

Abstract: Mitotic figure (MF) detection in histopathology images is challenging due to
large variations in slide scanners, staining protocols, tissue types, and the
presence of artifacts. This paper presents a collection of training techniques
- a bag of tricks - that enable robust, real-time MF detection across diverse
domains. We build on the efficient RTMDet single stage object detector to
achieve high inference speed suitable for clinical deployment. Our method
addresses scanner variability and tumor heterogeneity via extensive
multi-domain training data, balanced sampling, and careful augmentation.
Additionally, we employ targeted, hard negative mining on necrotic and debris
tissue to reduce false positives. In a grouped 5-fold cross-validation across
multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On
the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025
challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81,
outperforming larger models and demonstrating adaptability to new, unfamiliar
domains. The proposed solution offers a practical trade-off between accuracy
and speed, making it attractive for real-world clinical adoption.

</details>


### [252] [ERSR: An Ellipse-constrained pseudo-label refinement and symmetric regularization framework for semi-supervised fetal head segmentation in ultrasound images](https://arxiv.org/abs/2508.19815)
*Linkuan Zhou,Zhexin Chen,Yufei Shen,Junlin Xu,Ping Xuan,Yixin Zhu,Yuqi Fang,Cong Cong,Leyi Wei,Ran Su,Jia Zhou,Qiangguo Jin*

Main category: cs.CV

TL;DR: 提出半监督框架 ERSR 用于胎儿头部超声分割，方法包含三项策略，在两个基准测试中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 超声图像质量差和标注数据缺乏，现有半监督方法难以处理胎儿头部超声图像的独特特征。

Method: 提出 ERSR 框架，包含双评分自适应过滤策略、椭圆约束伪标签细化和基于对称的多重一致性正则化。

Result: 在 HC18 数据集上，10% 和 20% 标注数据下 Dice 分数分别达 92.05% 和 95.36%；在 PSFH 数据集上，相同设置下分数为 91.68% 和 93.70%。

Conclusion: 所提方法达到了当前最优性能

Abstract: Automated segmentation of the fetal head in ultrasound images is critical for
prenatal monitoring. However, achieving robust segmentation remains challenging
due to the poor quality of ultrasound images and the lack of annotated data.
Semi-supervised methods alleviate the lack of annotated data but struggle with
the unique characteristics of fetal head ultrasound images, making it
challenging to generate reliable pseudo-labels and enforce effective
consistency regularization constraints. To address this issue, we propose a
novel semi-supervised framework, ERSR, for fetal head ultrasound segmentation.
Our framework consists of the dual-scoring adaptive filtering strategy, the
ellipse-constrained pseudo-label refinement, and the symmetry-based multiple
consistency regularization. The dual-scoring adaptive filtering strategy uses
boundary consistency and contour regularity criteria to evaluate and filter
teacher outputs. The ellipse-constrained pseudo-label refinement refines these
filtered outputs by fitting least-squares ellipses, which strengthens pixels
near the center of the fitted ellipse and suppresses noise simultaneously. The
symmetry-based multiple consistency regularization enforces multi-level
consistency across perturbed images, symmetric regions, and between original
predictions and pseudo-labels, enabling the model to capture robust and stable
shape representations. Our method achieves state-of-the-art performance on two
benchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36%
with 10% and 20% labeled data, respectively. On the PSFH dataset, the scores
are 91.68% and 93.70% under the same settings.

</details>


### [253] [CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning](https://arxiv.org/abs/2508.20096)
*Zeyi Sun,Yuhang Cao,Jianze Liang,Qiushi Sun,Ziyu Liu,Zhixiong Zhang,Yuhang Zang,Xiaoyi Dong,Kai Chen,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出可训练的组合框架CODA，结合通用规划器和专家执行器，经两阶段训练，在科学应用中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有图形用户界面自主代理在科学计算领域存在规划和执行难以兼顾、组合框架静态不可训练且数据稀缺的问题。

Method: 引入CODA框架，结合通用规划器Cerebrum和专家执行器Cerebellum，通过两阶段训练：先为各应用训练专家规划器，再汇总成功轨迹微调最终规划器。

Result: 在ScienceBoard基准的四个应用中，CODA显著优于基线，开创开源模型新水平。

Conclusion: CODA框架能有效解决现有自主代理在科学计算领域的问题，具备强大执行和跨领域泛化能力。

Abstract: Autonomous agents for Graphical User Interfaces (GUIs) face significant
challenges in specialized domains such as scientific computing, where both
long-horizon planning and precise execution are required. Existing approaches
suffer from a trade-off: generalist agents excel at planning but perform poorly
in execution, while specialized agents demonstrate the opposite weakness.
Recent compositional frameworks attempt to bridge this gap by combining a
planner and an actor, but they are typically static and non-trainable, which
prevents adaptation from experience. This is a critical limitation given the
scarcity of high-quality data in scientific domains. To address these
limitations, we introduce CODA, a novel and trainable compositional framework
that integrates a generalist planner (Cerebrum) with a specialist executor
(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,
Specialization, we apply a decoupled GRPO approach to train an expert planner
for each scientific application individually, bootstrapping from a small set of
task trajectories. In the second stage, Generalization, we aggregate all
successful trajectories from the specialized experts to build a consolidated
dataset, which is then used for supervised fine-tuning of the final planner.
This equips CODA with both robust execution and cross-domain generalization.
Evaluated on four challenging applications from the ScienceBoard benchmark,
CODA significantly outperforms baselines and establishes a new state of the art
among open-source models.

</details>


### [254] [Gradient Rectification for Robust Calibration under Distribution Shift](https://arxiv.org/abs/2508.19830)
*Yilin Zhang,Cai Xu,You Wu,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: 提出无目标域信息的校准框架，结合低频滤波和梯度校正机制，在分布偏移下提升校准效果并保持分布内性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络预测常过度自信，在分布偏移下校准问题加剧，现有方法依赖目标域信息，实用性受限。

Method: 从频域角度，采用低频滤波策略鼓励依赖域不变特征，提出基于梯度的校正机制在优化时强制分布内校准。

Result: 在合成和真实世界的偏移数据集上，方法显著提升分布偏移下的校准效果，同时保持强分布内性能。

Conclusion: 所提方法能在无目标域信息情况下，有效解决分布偏移下的校准问题。

Abstract: Deep neural networks often produce overconfident predictions, undermining
their reliability in safety-critical applications. This miscalibration is
further exacerbated under distribution shift, where test data deviates from the
training distribution due to environmental or acquisition changes. While
existing approaches improve calibration through training-time regularization or
post-hoc adjustment, their reliance on access to or simulation of target
domains limits their practicality in real-world scenarios. In this paper, we
propose a novel calibration framework that operates without access to target
domain information. From a frequency-domain perspective, we identify that
distribution shifts often distort high-frequency visual cues exploited by deep
models, and introduce a low-frequency filtering strategy to encourage reliance
on domain-invariant features. However, such information loss may degrade
In-Distribution (ID) calibration performance. Therefore, we further propose a
gradient-based rectification mechanism that enforces ID calibration as a hard
constraint during optimization. Experiments on synthetic and real-world shifted
datasets, including CIFAR-10/100-C and WILDS, demonstrate that our method
significantly improves calibration under distribution shift while maintaining
strong in-distribution performance.

</details>


### [255] [Multispectral LiDAR data for extracting tree points in urban and suburban areas](https://arxiv.org/abs/2508.19881)
*Narges Takhtkeshha,Gabriele Mazzacca,Fabio Remondino,Juha Hyyppä,Gottfried Mandlburger*

Main category: cs.CV

TL;DR: 研究利用MS - LiDAR和深度学习模型进行树木点提取，评估三个模型，SPT表现佳，结合pNDVI与空间数据检测精度高，凸显MS - LiDAR和DL潜力。


<details>
  <summary>Details</summary>
Motivation: 监测城市树木动态对支持绿化政策和降低电力基础设施风险至关重要，现有方法在复杂城市环境和树木多变性下有挑战，需改进。

Method: 利用MS - LiDAR和深度学习模型进行树木点提取，评估Superpoint Transformer (SPT)、Point Transformer V3 (PTv3)和Point Transformer V1 (PTv1)三个模型。

Result: SPT时间效率和准确性显著，mIoU为85.28%；结合pNDVI与空间数据检测精度最高，比仅用空间信息误差率降低10.61个百分点。

Conclusion: MS - LiDAR和深度学习有改善树木提取和树木清查的潜力。

Abstract: Monitoring urban tree dynamics is vital for supporting greening policies and
reducing risks to electrical infrastructure. Airborne laser scanning has
advanced large-scale tree management, but challenges remain due to complex
urban environments and tree variability. Multispectral (MS) light detection and
ranging (LiDAR) improves this by capturing both 3D spatial and spectral data,
enabling detailed mapping. This study explores tree point extraction using
MS-LiDAR and deep learning (DL) models. Three state-of-the-art models are
evaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point
Transformer V1 (PTv1). Results show the notable time efficiency and accuracy of
SPT, with a mean intersection over union (mIoU) of 85.28%. The highest
detection accuracy is achieved by incorporating pseudo normalized difference
vegetation index (pNDVI) with spatial data, reducing error rate by 10.61
percentage points (pp) compared to using spatial information alone. These
findings highlight the potential of MS-LiDAR and DL to improve tree extraction
and further tree inventories.

</details>


### [256] [WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.19927)
*Fayaz Ali,Muhammad Zawish,Steven Davy,Radu Timofte*

Main category: cs.CV

TL;DR: 提出WaveHiT - SR方法，结合小波变换和分层transformer框架用于图像超分辨率，实验证明其有效高效。


<details>
  <summary>Details</summary>
Motivation: 许多基于transformer的图像超分辨率方法中窗口自注意力机制的二次计算复杂度限制了感受野，需要改进。

Method: 将小波变换嵌入分层transformer框架，使用自适应分层窗口，利用小波变换分解图像，通过分层处理逐步重建高分辨率图像。

Result: WaveHiT - SR的改进版本SwinIR - Light、SwinIR - NG和SRFormer - Light取得前沿超分辨率结果，参数更少、FLOPs更低、速度更快。

Conclusion: WaveHiT - SR方法有效且高效。

Abstract: Transformers have demonstrated promising performance in computer vision
tasks, including image super-resolution (SR). The quadratic computational
complexity of window self-attention mechanisms in many transformer-based SR
methods forces the use of small, fixed windows, limiting the receptive field.
In this paper, we propose a new approach by embedding the wavelet transform
within a hierarchical transformer framework, called (WaveHiT-SR). First, using
adaptive hierarchical windows instead of static small windows allows to capture
features across different levels and greatly improve the ability to model
long-range dependencies. Secondly, the proposed model utilizes wavelet
transforms to decompose images into multiple frequency subbands, allowing the
network to focus on both global and local features while preserving structural
details. By progressively reconstructing high-resolution images through
hierarchical processing, the network reduces computational complexity without
sacrificing performance. The multi-level decomposition strategy enables the
network to capture fine-grained information in lowfrequency components while
enhancing high-frequency textures. Through extensive experimentation, we
confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined
versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR
results, achieving higher efficiency with fewer parameters, lower FLOPs, and
faster speeds.

</details>


### [257] [GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity](https://arxiv.org/abs/2508.19972)
*Seongheon Park,Yixuan Li*

Main category: cs.CV

TL;DR: 提出无训练目标幻觉检测框架GLSim，比现有方法检测性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有目标幻觉分数估计方法仅从全局或局部单一视角出发，限制检测可靠性，大视觉语言模型目标幻觉问题影响其在现实应用中的安全部署。

Method: 引入GLSim框架，利用图像和文本模态间互补的全局和局部嵌入相似性信号。

Result: 综合基准测试显示，GLSim检测性能优越，大幅超越竞争基线。

Conclusion: GLSim能在多样场景下实现更准确可靠的幻觉检测。

Abstract: Object hallucination in large vision-language models presents a significant
challenge to their safe deployment in real-world applications. Recent works
have proposed object-level hallucination scores to estimate the likelihood of
object hallucination; however, these methods typically adopt either a global or
local perspective in isolation, which may limit detection reliability. In this
paper, we introduce GLSim, a novel training-free object hallucination detection
framework that leverages complementary global and local embedding similarity
signals between image and text modalities, enabling more accurate and reliable
hallucination detection in diverse scenarios. We comprehensively benchmark
existing object hallucination detection methods and demonstrate that GLSim
achieves superior detection performance, outperforming competitive baselines by
a significant margin.

</details>


### [258] [Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices](https://arxiv.org/abs/2508.20064)
*Philippe Zhang,Weili Jiang,Yihao Li,Jing Zhang,Sarah Matta,Yubo Tan,Hui Lin,Haoshen Wang,Jiangtian Pan,Hui Xu,Laurent Borderie,Alexandre Le Guilcher,Béatrice Cochener,Chubin Ou,Gwenolé Quellec,Mathieu Lamard*

Main category: cs.CV

TL;DR: 本文聚焦AMD，参与MARIO挑战，用融合CNN网络和Patch Progression Masked Autoencoder完成两项任务，排名前十但因组织关系无获奖资格。


<details>
  <summary>Details</summary>
Motivation: AMD影响视力，及时诊断和监测对治疗重要，通过追踪患者OCT扫描中新生血管活动进展制定个性化治疗方案。

Method: 任务1用融合CNN网络和模型集成；任务2提出Patch Progression Masked Autoencoder生成下次OCT并分类。

Result: 两项任务排名进入前10。

Conclusion: 虽取得较好排名，但因部分团队成员与主办方同属一组织，无获奖资格。

Abstract: Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting
visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments
have been effective in slowing the progression of neovascular AMD, with better
outcomes achieved through timely diagnosis and consistent monitoring. Tracking
the progression of neovascular activity in OCT scans of patients with exudative
AMD allows for the development of more personalized and effective treatment
plans. This was the focus of the Monitoring Age-related Macular Degeneration
Progression in Optical Coherence Tomography (MARIO) challenge, in which we
participated. In Task 1, which involved classifying the evolution between two
pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN
network with model ensembling to further enhance the model's performance. For
Task 2, which focused on predicting progression over the next three months
based on current exam data, we proposed the Patch Progression Masked
Autoencoder that generates an OCT for the next exam and then classifies the
evolution between the current OCT and the one generated using our solution from
Task 1. The results we achieved allowed us to place in the Top 10 for both
tasks. Some team members are part of the same organization as the challenge
organizers; therefore, we are not eligible to compete for the prize.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [259] [Aegis: Taxonomy and Optimizations for Overcoming Agent-Environment Failures in LLM Agents](https://arxiv.org/abs/2508.19504)
*Kevin Song,Anand Jayarajan,Yaoyao Ding,Qidong Su,Zhanda Zhu,Sihang Liu,Gennady Pekhimenko*

Main category: cs.MA

TL;DR: 本文研究通过优化系统环境提高大语言模型代理成功率，分析代理失败情况提出交互失败分类，设计Aegis优化技术，平均提升成功率6.7 - 12.5%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理在复杂现实环境中成功率低，以往研究多关注改进代理本身，忽略系统环境作用。

Method: 收集142条代理轨迹，分析代理失败情况，提出包含6种失败模式的分类，设计Aegis环境优化技术。

Result: Aegis技术在不修改代理和底层大语言模型的情况下，平均提高代理成功率6.7 - 12.5%。

Conclusion: 优化系统环境是提高大语言模型代理成功率的有效补充方向。

Abstract: Large Language Models (LLMs) agents augmented with domain tools promise to
autonomously execute complex tasks requiring human-level intelligence, such as
customer service and digital assistance. However, their practical deployment is
often limited by their low success rates under complex real-world environments.
To tackle this, prior research has primarily focused on improving the agents
themselves, such as developing strong agentic LLMs, while overlooking the role
of the system environment in which the agent operates.
  In this paper, we study a complementary direction: improving agent success
rates by optimizing the system environment in which the agent operates. We
collect 142 agent traces (3,656 turns of agent-environment interactions) across
5 state-of-the-art agentic benchmarks. By analyzing these agent failures, we
propose a taxonomy for agent-environment interaction failures that includes 6
failure modes. Guided by these findings, we design Aegis, a set of targeted
environment optimizations: 1) environment observability enhancement, 2) common
computation offloading, and 3) speculative agentic actions. These techniques
improve agent success rates on average by 6.7-12.5%, without any modifications
to the agent and underlying LLM.

</details>


### [260] [Anomaly Detection in Networked Bandits](https://arxiv.org/abs/2508.20076)
*Xiaotong Cheng,Setareh Maghsudi*

Main category: cs.MA

TL;DR: 提出一种新的老虎机算法，用于在社交网络中学习用户偏好并检测异常，证明了算法遗憾上界并进行实验对比。


<details>
  <summary>Details</summary>
Motivation: 社交网络中异常节点会导致严重后果，需要设计高效在线学习算法来学习用户偏好并检测异常。

Method: 引入新的老虎机算法，利用网络知识刻画用户偏好和特征信息残差，学习分析后制定个性化推荐策略并检测异常。

Result: 严格证明了算法遗憾的上界，在合成和真实数据集上与多个先进算法进行了实验对比。

Conclusion: 未明确提及，推测新算法有望在社交网络中有效学习用户偏好和检测异常。

Abstract: The nodes' interconnections on a social network often reflect their
dependencies and information-sharing behaviors. Nevertheless, abnormal nodes,
which significantly deviate from most of the network concerning patterns or
behaviors, can lead to grave consequences. Therefore, it is imperative to
design efficient online learning algorithms that robustly learn users'
preferences while simultaneously detecting anomalies.
  We introduce a novel bandit algorithm to address this problem. Through
network knowledge, the method characterizes the users' preferences and
residuals of feature information. By learning and analyzing these preferences
and residuals, it develops a personalized recommendation strategy for each user
and simultaneously detects anomalies. We rigorously prove an upper bound on the
regret of the proposed algorithm and experimentally compare it with several
state-of-the-art collaborative contextual bandit algorithms on both synthetic
and real-world datasets.

</details>


<div id='nucl-th'></div>

# nucl-th [[Back]](#toc)

### [261] [Weighted Levenberg-Marquardt methods for fitting multichannel nuclear cross section data](https://arxiv.org/abs/2508.19468)
*M. Imbrišak,A. E. Lovell,M. R. Mumpower*

Main category: nucl-th

TL;DR: 提出扩展的Levenberg - Marquardt算法拟合多通道核截面数据，构造加权Fisher信息度量，引入几何缩放策略加速收敛，用实验数据验证效果。


<details>
  <summary>Details</summary>
Motivation: 传统信任区域方法在分析实验数据方面不足，CoH₃代码因参数空间存在“sloppy”方向使优化有挑战，且实验数据在反应通道分布不均。

Method: 构造加权Fisher信息度量，对数据集权重上的先验分布积分；引入基于流形局部几何的几何缩放策略。

Result: 加权Levenberg - Marquardt方法对原始和平滑数据集都能得到更符合物理的拟合结果。

Conclusion: 扩展的Levenberg - Marquardt算法是分析实验数据的实用且稳健的替代方法，能改善参数估计和收敛稳健性。

Abstract: We present an extension of the Levenberg-Marquardt algorithm for fitting
multichannel nuclear cross section data. Our approach offers a practical and
robust alternative to conventional trust-region methods for analyzing
experimental data. The CoH$_3$ code, based on the Hauser-Feshbach statistical
model, involves a large number of interdependent parameters, making
optimization challenging due to the presence of "sloppy" directions in
parameter space. To address the uneven distribution of experimental data across
reaction channels, we construct a weighted Fisher Information Metric by
integrating prior distributions over dataset weights. This framework enables a
more balanced treatment of heterogeneous data, improving both parameter
estimation and convergence robustness. We show that the resulting weighted
Levenberg-Marquardt method yields more physically consistent fits for both raw
and smoothed datasets, using experimental data for ${}^{148}$Sm as a
representative example. Additionally, we introduce a geometric scaling strategy
to accelerate convergence -- a method based on the local geometry of the
manifold.

</details>


### [262] [Topological Uncertainty for Anomaly Detection in the Neural-network EoS Inference with Neutron Star Data](https://arxiv.org/abs/2508.19683)
*Kenji Fukushima,Syo Kamata*

Main category: nucl-th

TL;DR: 研究用训练的前馈神经网络构建的拓扑不确定性（TU）用于异常检测的性能，在中子星数据实验中检测成功率最高超90%，并探讨TU应用潜力。


<details>
  <summary>Details</summary>
Motivation: 利用拓扑数据分析从训练的前馈神经网络隐藏层提取信息，用于异常检测。

Method: 构建TU，引入交叉 - TU量化不确定性，用中子星数据进行性能测试，计算交叉 - TU评估异常检测性能。

Result: 异常检测性能依赖FNN超参数，最佳情况下异常检测成功率超90%。

Conclusion: TU有进一步挖掘训练FNN隐藏信息的潜力。

Abstract: We study the performance of the Topological Uncertainty (TU) constructed with
a trained feedforward neural network (FNN) for Anomaly Detection. Generally,
meaningful information can be stored in the hidden layers of the trained FNN,
and the TU implementation is one tractable recipe to extract buried information
by means of the Topological Data Analysis. We explicate the concept of the TU
and the numerical procedures. Then, for a concrete demonstration of the
performance test, we employ the Neutron Star data used for inference of the
equation of state (EoS). For the training dataset consisting of the input
(Neutron Star data) and the output (EoS parameters), we can compare the
inferred EoSs and the exact answers to classify the data with the label $k$.
The subdataset with $k=0$ leads to the normal inference for which the inferred
EoS approximates the answer well, while the subdataset with $k=1$ ends up with
the unsuccessful inference. Once the TU is prepared based on the $k$-labled
subdatasets, we introduce the cross-TU to quantify the uncertainty of
characterizing the $k$-labeled data with the label $j$. The anomaly or
unsuccessful inference is correctly detected if the cross-TU for $j=k=1$ is
smaller than that for $j=0$ and $k=1$. In our numerical experiment, for various
input data, we calculate the cross-TU and estimate the performance of Anomaly
Detection. We find that performance depends on FNN hyperparameters, and the
success rate of Anomaly Detection exceeds $90\%$ in the best case. We finally
discuss further potential of the TU application to retrieve the information
hidden in the trained FNN.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [263] [OmniSim: Simulating Hardware with C Speed and RTL Accuracy for High-Level Synthesis Designs](https://arxiv.org/abs/2508.19299)
*Rishov Sarkar,Cong Hao*

Main category: cs.AR

TL;DR: 提出OmniSim框架，扩展HLS工具仿真能力，模拟复杂数据流设计，速度快且准确。


<details>
  <summary>Details</summary>
Motivation: 现有HLS工具在C级高效准确模拟相关构造存在挑战，缺乏硬件时序信息，性能指标依赖RTL仿真，且现有工具未完全解决这些问题。

Method: 通过复杂软件多线程，查询和更新FIFO表记录硬件时序；灵活耦合和重叠功能与性能仿真。

Result: 成功模拟11个此前HLS工具不支持的设计，比传统C/RTL协同仿真快35.9倍，比LightningSim快6.61倍。

Conclusion: OmniSim能显著扩展学术和商业HLS工具的仿真能力。

Abstract: High-Level Synthesis (HLS) is increasingly popular for hardware design using
C/C++ instead of Register-Transfer Level (RTL). To express concurrent hardware
behavior in a sequential language like C/C++, HLS tools introduce constructs
such as infinite loops and dataflow modules connected by FIFOs. However,
efficiently and accurately simulating these constructs at C level remains
challenging. First, without hardware timing information, functional
verification typically requires slow RTL synthesis and simulation, as the
current approaches in commercial HLS tools. Second, cycle-accurate performance
metrics, such as end-to-end latency, also rely on RTL simulation. No existing
HLS tool fully overcomes the first limitation. For the second, prior work such
as LightningSim partially improves simulation speed but lacks support for
advanced dataflow features like cyclic dependencies and non-blocking FIFO
accesses.
  To overcome both limitations, we propose OmniSim, a framework that
significantly extends the simulation capabilities of both academic and
commercial HLS tools. First, OmniSim enables fast and accurate simulation of
complex dataflow designs, especially those explicitly declared unsupported by
commercial tools. It does so through sophisticated software multi-threading,
where threads are orchestrated by querying and updating a set of FIFO tables
that explicitly record exact hardware timing of each FIFO access. Second,
OmniSim achieves near-C simulation speed with near-RTL accuracy for both
functionality and performance, via flexibly coupled and overlapped
functionality and performance simulations.
  We demonstrate that OmniSim successfully simulates eleven designs previously
unsupported by any HLS tool, achieving up to 35.9x speedup over traditional
C/RTL co-simulation, and up to 6.61x speedup over the state-of-the-art yet less
capable simulator, LightningSim, on its own benchmark suite.

</details>


### [264] [New Tools, Programming Models, and System Support for Processing-in-Memory Architectures](https://arxiv.org/abs/2508.19868)
*Geraldo F. Oliveira*

Main category: cs.AR

TL;DR: 本文为PIM架构提供工具、编程模型和系统支持，提出DAMOV、MIMDRAM、Proteus和DaPPA四项新贡献。


<details>
  <summary>Details</summary>
Motivation: 为PIM架构（主要是基于DRAM的解决方案）提供工具、编程模型和系统支持，促进PIM在当前和未来系统中的应用。

Method: 提出DAMOV方法和基准套件，设计MIMDRAM硬件/软件协同基底，构建Proteus硬件框架，开发DaPPA编程框架。

Result: DAMOV可表征内存数据移动瓶颈；MIMDRAM解决PUD架构可编程性和灵活性问题；Proteus降低PUD操作执行延迟；DaPPA简化通用PNM架构编程。

Conclusion: 通过这四项贡献，有望推动PIM架构在当前和未来系统中的应用。

Abstract: Our goal in this dissertation is to provide tools, programming models, and
system support for PIM architectures (with a focus on DRAM-based solutions), to
ease the adoption of PIM in current and future systems. To this end, we make at
least four new major contributions.
  First, we introduce DAMOV, the first rigorous methodology to characterize
memory-related data movement bottlenecks in modern workloads, and the first
data movement benchmark suite. Second, we introduce MIMDRAM, a new
hardware/software co-designed substrate that addresses the major current
programmability and flexibility limitations of the bulk bitwise execution model
of processing-using-DRAM (PUD) architectures. MIMDRAM enables the allocation
and control of only the needed computing resources inside DRAM for PUD
computing. Third, we introduce Proteus, the first hardware framework that
addresses the high execution latency of bulk bitwise PUD operations in
state-of-the-art PUD architectures by implementing a data-aware runtime engine
for PUD. Proteus reduces the latency of PUD operations in three different ways:
(i) Proteus concurrently executes independent in-DRAM primitives belong to a
single PUD operation across DRAM arrays. (ii) Proteus dynamically reduces the
bit-precision (and consequentially the latency and energy consumption) of PUD
operations by exploiting narrow values (i.e., values with many leading zeros or
ones). (iii) Proteus chooses and uses the most appropriate data representation
and arithmetic algorithm implementation for a given PUD instruction
transparently to the programmer. Fourth, we introduce DaPPA (data-parallel
processing-in-memory architecture), a new programming framework that eases
programmability for general-purpose PNM architectures by allowing the
programmer to write efficient PIM-friendly code without the need to manage
hardware resources explicitly.

</details>


### [265] [GENIE-ASI: Generative Instruction and Executable Code for Analog Subcircuit Identification](https://arxiv.org/abs/2508.19393)
*Phuoc Pham,Arun Venkitaraman,Chia-Yu Hsieh,Andrea Bonetti,Stefan Uhlich,Markus Leibl,Simon Hofmann,Eisaku Ohbuchi,Lorenzo Servadei,Ulf Schlichtmann,Robert Wille*

Main category: cs.AR

TL;DR: 提出无训练的基于大语言模型的模拟子电路识别方法GENIE - ASI，引入新基准测试，实验表明LLM可用于模拟设计自动化。


<details>
  <summary>Details</summary>
Motivation: 传统模拟子电路识别方法需大量人工专业知识、基于规则编码或大量标注数据集，有局限性。

Method: 提出GENIE - ASI，分两阶段，先通过上下文学习从示例得自然语言指令，再转为Python代码识别子电路；引入新的运算放大器网表基准测试。

Result: 在新基准测试上，GENIE - ASI在简单结构F1分数为1.0，中等抽象为0.81，复杂子电路为0.31。

Conclusion: LLM可作为模拟设计自动化中通用灵活工具，为基础模型应用开辟新研究方向。

Abstract: Analog subcircuit identification is a core task in analog design, essential
for simulation, sizing, and layout. Traditional methods often require extensive
human expertise, rule-based encoding, or large labeled datasets. To address
these challenges, we propose GENIE-ASI, the first training-free, large language
model (LLM)-based methodology for analog subcircuit identification. GENIE-ASI
operates in two phases: it first uses in-context learning to derive natural
language instructions from a few demonstration examples, then translates these
into executable Python code to identify subcircuits in unseen SPICE netlists.
In addition, to evaluate LLM-based approaches systematically, we introduce a
new benchmark composed of operational amplifier netlists (op-amps) that cover a
wide range of subcircuit variants. Experimental results on the proposed
benchmark show that GENIE-ASI matches rule-based performance on simple
structures (F1-score = 1.0), remains competitive on moderate abstractions
(F1-score = 0.81), and shows potential even on complex subcircuits (F1-score =
0.31). These findings demonstrate that LLMs can serve as adaptable,
general-purpose tools in analog design automation, opening new research
directions for foundation model applications in analog design automation.

</details>
