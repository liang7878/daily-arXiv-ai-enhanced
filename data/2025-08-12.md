<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 65]
- [cs.CE](#cs.CE) [Total: 5]
- [cs.DB](#cs.DB) [Total: 11]
- [cs.DC](#cs.DC) [Total: 16]
- [cs.DS](#cs.DS) [Total: 10]
- [cs.GT](#cs.GT) [Total: 10]
- [cs.IR](#cs.IR) [Total: 19]
- [cs.LG](#cs.LG) [Total: 144]
- [cs.NE](#cs.NE) [Total: 7]
- [cs.SE](#cs.SE) [Total: 15]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 3]
- [stat.ML](#stat.ML) [Total: 9]
- [stat.CO](#stat.CO) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.LO](#cs.LO) [Total: 3]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [cs.HC](#cs.HC) [Total: 16]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [hep-ex](#hep-ex) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 5]
- [physics.soc-ph](#physics.soc-ph) [Total: 3]
- [eess.IV](#eess.IV) [Total: 8]
- [econ.GN](#econ.GN) [Total: 9]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [math.NT](#math.NT) [Total: 1]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.MA](#cs.MA) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.SI](#cs.SI) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.CR](#cs.CR) [Total: 7]
- [math.PR](#math.PR) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 11]
- [math.OC](#math.OC) [Total: 4]
- [cs.RO](#cs.RO) [Total: 13]
- [cs.CV](#cs.CV) [Total: 58]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.IT](#cs.IT) [Total: 5]
- [cs.CL](#cs.CL) [Total: 48]
- [cs.ET](#cs.ET) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [physics.data-an](#physics.data-an) [Total: 1]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [cs.SD](#cs.SD) [Total: 6]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [nlin.CG](#nlin.CG) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization](https://arxiv.org/abs/2508.06559)
*Sina Baghal*

Main category: cs.AI

TL;DR: 本文介绍用于模拟Pasur牌类游戏的CUDA加速计算框架，用CFR算法计算近纳什均衡，还训练模型预测策略并评估牌组价值，框架可扩展到其他强化学习场景。


<details>
  <summary>Details</summary>
Motivation: Pasur游戏规则复杂、游戏树规模大，求解近纳什均衡存在挑战，需要高效计算框架。

Method: 使用PyTorch CUDA张量处理规则复杂性，将游戏树分解为游戏状态和上轮得分，构建完整游戏树；采用逐轮反向训练策略；训练基于树的模型预测策略，用GPU加速大规模自我对弈。

Result: 构建平均含超$10^9$节点的完整游戏树，计算出近纳什均衡策略，可预测策略并评估牌组公平价值。

Conclusion: 该框架可扩展到其他行动树能自然分解为多轮的强化学习算法场景。

Abstract: Pasur is a fishing card game played over six rounds and is played similarly
to games such as Cassino and Scopa, and Bastra. This paper introduces a
CUDA-accelerated computational framework for simulating Pasur, emphasizing
efficient memory management. We use our framework to compute near-Nash
equilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm
for solving large imperfect-information games.
  Solving Pasur presents unique challenges due to its intricate rules and the
large size of its game tree. We handle rule complexity using PyTorch CUDA
tensors and to address the memory-intensive nature of the game, we decompose
the game tree into two key components: (1) actual game states, and (2)
inherited scores from previous rounds. We construct the Full Game Tree by
pairing card states with accumulated scores in the Unfolding Process. This
design reduces memory overhead by storing only essential strategy values and
node connections. To further manage computational complexity, we apply a
round-by-round backward training strategy, starting from the final round and
recursively propagating average utilities to earlier stages. Our approach
constructs the complete game tree, which on average consists of over $10^9$
nodes. We provide detailed implementation snippets.
  After computing a near-Nash equilibrium strategy, we train a tree-based model
to predict these strategies for use during gameplay. We then estimate the fair
value of each deck through large-scale self-play between equilibrium strategies
by simulating, for instance, 10,000 games per matchup, executed in parallel
using GPU acceleration.
  Similar frameworks can be extended to other reinforcement learning algorithms
where the action tree naturally decomposes into multiple rounds such as
turn-based strategy games or sequential trading decisions in financial markets.

</details>


### [2] [Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop](https://arxiv.org/abs/2508.06569)
*Lance Yao,Suman Samantray,Ayana Ghosh,Kevin Roccapriore,Libor Kovarik,Sarah Allec,Maxim Ziatdinov*

Main category: cs.AI

TL;DR: 引入开源多智能体AI框架SciLink，将实验观察、新颖性评估和理论模拟自动关联，在材料研究中实现意外发现，展示其多功能性，弥补自动化实验与开放性科学探索的差距。


<details>
  <summary>Details</summary>
Motivation: 现代自主实验室在加速假设检验时可能忽略意外发现，需解决此问题。

Method: 采用混合AI策略，专业机器学习模型分析实验数据，大语言模型进行高层推理，将原始数据转化为可证伪科学主张并评估新颖性。

Result: 展示了框架在不同研究场景的多功能性，如处理原子分辨率和高光谱数据、整合专家指导、提出后续实验。

Conclusion: SciLink为AI驱动的材料研究提供实用框架，既提高效率又促进意外发现。

Abstract: The history of science is punctuated by serendipitous discoveries, where
unexpected observations, rather than targeted hypotheses, opened new fields of
inquiry. While modern autonomous laboratories excel at accelerating hypothesis
testing, their optimization for efficiency risks overlooking these crucial,
unplanned findings. To address this gap, we introduce SciLink, an open-source,
multi-agent artificial intelligence framework designed to operationalize
serendipity in materials research by creating a direct, automated link between
experimental observation, novelty assessment, and theoretical simulations. The
framework employs a hybrid AI strategy where specialized machine learning
models perform quantitative analysis of experimental data, while large language
models handle higher-level reasoning. These agents autonomously convert raw
data from materials characterization techniques into falsifiable scientific
claims, which are then quantitatively scored for novelty against the published
literature. We demonstrate the framework's versatility across diverse research
scenarios, showcasing its application to atomic-resolution and hyperspectral
data, its capacity to integrate real-time human expert guidance, and its
ability to close the research loop by proposing targeted follow-up experiments.
By systematically analyzing all observations and contextualizing them, SciLink
provides a practical framework for AI-driven materials research that not only
enhances efficiency but also actively cultivates an environment ripe for
serendipitous discoveries, thereby bridging the gap between automated
experimentation and open-ended scientific exploration.

</details>


### [3] [Pushing the Envelope of LLM Inference on AI-PC](https://arxiv.org/abs/2508.06753)
*Evangelos Georganas,Dhiraj Kalamkar,Alexander Heinecke*

Main category: cs.AI

TL;DR: 本文聚焦超低比特LLM模型在资源受限环境推理，设计优化微内核并集成到框架，实现端到端推理性能提升。


<details>
  <summary>Details</summary>
Motivation: 超低比特LLM模型虽有优势，但现有推理运行时计算效率待研究，需提升资源受限环境推理效率。

Method: 采用自底向上方法，设计并实现针对现代CPU优化的1位和2位微内核，集成到PyTorch - TPP框架。

Result: 2位模型端到端推理结果比当前SOTA运行时bitnet.cpp快达2.2倍，比16位模型推理快达7倍。

Conclusion: 优化后的运行时推动了AI PC和边缘设备上LLM推理发展，为超低比特LLM模型高效部署奠定基础。

Abstract: The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the
perplexity and end-task performance of their full-precision counterparts using
the same model size, is ushering in a new era of LLM inference for
resource-constrained environments such as edge devices and AI PCs. While these
quantization advances promise models that are more cost-effective in terms of
latency, memory, throughput, and energy consumption, the computational
efficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)
used to deploy them remains underexplored. In this work, we take a bottom-up
approach: we first design and implement 1-bit and 2-bit microkernels optimized
for modern CPUs, achieving peak computational efficiency across a variety of
CPU platforms. We integrate these microkernels into a state-of-the-art LLM
inference framework, namely PyTorch-TPP, and present end-to-end inference
results with 2-bit models that outperform the current SOTA runtime bitnet.cpp
by up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model
inference. Our optimized runtime advances the state of LLM inference on AI PCs
and edge devices, paving the way for efficient deployment of ultra-low-bit LLM
models.

</details>


### [4] [IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model](https://arxiv.org/abs/2508.06571)
*Anqing Jiang,Yu Gao,Yiru Wang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun,Shichen Tang,Lijuan Zhu,Jinhao Chai,Jijun Wang,Zichong Gu,Hao Jiang,Li Sun*

Main category: cs.AI

TL;DR: 本文针对视觉-语言-动作（VLA）模型在自动驾驶发展中的挑战，提出IRL - VLA框架，经三阶段训练，在相关基准测试中取得优异成绩，有望加速闭环自动驾驶的VLA研究。


<details>
  <summary>Details</summary>
Motivation: 现有VLA架构基于开环模仿学习性能欠佳，闭环训练依赖高保真传感器模拟，存在领域差距和计算效率问题，阻碍VLA模型在自动驾驶中的发展。

Method: 提出IRL - VLA框架，分三阶段：第一阶段提出VLA架构并通过模仿学习预训练VLA策略；第二阶段通过逆强化学习构建轻量级奖励世界模型以实现高效闭环奖励计算；第三阶段通过PPO设计专门的奖励世界模型引导强化学习，平衡安全、舒适驾驶和交通效率。

Result: 在NAVSIM v2端到端驾驶基准测试中达到了最先进性能，在CVPR2025自动驾驶大挑战中获得亚军。

Conclusion: 该框架有望加速闭环自动驾驶的VLA研究。

Abstract: Vision-Language-Action (VLA) models have demonstrated potential in autonomous
driving. However, two critical challenges hinder their development: (1)
Existing VLA architectures are typically based on imitation learning in
open-loop setup which tends to capture the recorded behaviors in the dataset,
leading to suboptimal and constrained performance, (2) Close-loop training
relies heavily on high-fidelity sensor simulation, where domain gaps and
computational inefficiencies pose significant barriers. In this paper, we
introduce IRL-VLA, a novel close-loop Reinforcement Learning via
\textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world model
with a self-built VLA approach. Our framework proceeds in a three-stage
paradigm: In the first stage, we propose a VLA architecture and pretrain the
VLA policy via imitation learning. In the second stage, we construct a
lightweight reward world model via inverse reinforcement learning to enable
efficient close-loop reward computation. To further enhance planning
performance, finally, we design specialized reward world model guidence
reinforcement learning via PPO(Proximal Policy Optimization) to effectively
balance the safety incidents, comfortable driving, and traffic efficiency. Our
approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving
benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that
our framework will accelerate VLA research in close-loop autonomous driving.

</details>


### [5] [CountQA: How Well Do MLLMs Count in the Wild?](https://arxiv.org/abs/2508.06585)
*Jayant Sravan Tamarapalli,Rynaa Grover,Nilay Pande,Sahiti Yerramilli*

Main category: cs.AI

TL;DR: 现有多模态大语言模型（MLLMs）存在物体计数能力缺陷，本文引入CountQA基准测试该能力，评估15个模型，发现最佳模型准确率仅42.9%，后续将开源数据和代码。


<details>
  <summary>Details</summary>
Motivation: MLLMs在物体计数这一基本认知技能上存在缺陷，现有基准无法在复杂场景下评估该能力，限制了其在现实应用中的可靠性。

Method: 引入包含超1500个问答对、具有高物体密度等特点的CountQA基准，对15个著名MLLMs进行评估。

Result: 表现最佳的模型准确率仅42.9%，且随着物体数量增加，性能下降。

Conclusion: CountQA为诊断和纠正MLLMs的核心弱点提供了专门基准，有望推动新一代兼具描述流畅性、数值基础和空间感知能力的MLLMs发展。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in
understanding visual scenes, yet they exhibit a critical lack in a fundamental
cognitive skill: object counting. This blind spot severely limits their
reliability in real-world applications. To date, this capability has been
largely unevaluated in complex scenarios, as existing benchmarks either feature
sparse object densities or are confined to specific visual domains, failing to
test models under realistic conditions. Addressing this gap, we introduce
CountQA, a challenging new benchmark designed to probe this deficiency.
Comprising over 1,500 question-answer pairs, CountQA features real-world images
with high object density, clutter, and occlusion. We investigate this weakness
by evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the
top-performing model achieves a mere 42.9% accuracy, with performance declining
as object counts rise. By providing a dedicated benchmark to diagnose and
rectify this core weakness, CountQA paves the way for a new generation of MLLMs
that are not only descriptively fluent but also numerically grounded and
spatially aware. We will open-source the dataset and code upon paper acceptance
to foster further research.

</details>


### [6] [Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis](https://arxiv.org/abs/2508.06668)
*Jessie Galasso*

Main category: cs.AI

TL;DR: 本文聚焦FCA框架，收集对可变性分析至关重要的属性，并说明如何在概念结构中解释可变性信息，以弥补FCA在可变性相关任务应用上的差距。


<details>
  <summary>Details</summary>
Motivation: FCA虽适合可变性提取和分析，但因基础文献的数学导向，难以明确其哪些属性可用于可变性相关任务及如何使用，故有此研究。

Method: 收集FCA框架中对可变性分析至关重要的属性，并阐述如何利用这些属性在概念结构中解释不同的可变性信息。

Result: 文中未提及具体结果。

Conclusion: 文中未提及明确结论。

Abstract: Formal Concept Analysis (FCA) is a mathematical framework for knowledge
representation and discovery. It performs a hierarchical clustering over a set
of objects described by attributes, resulting in conceptual structures in which
objects are organized depending on the attributes they share. These conceptual
structures naturally highlight commonalities and variabilities among similar
objects by categorizing them into groups which are then arranged by similarity,
making it particularly appropriate for variability extraction and analysis.
Despite the potential of FCA, determining which of its properties can be
leveraged for variability-related tasks (and how) is not always
straightforward, partly due to the mathematical orientation of its foundational
literature. This paper attempts to bridge part of this gap by gathering a
selection of properties of the framework which are essential to variability
analysis, and how they can be used to interpret diverse variability information
within the resulting conceptual structures.

</details>


### [7] [Zero-Shot Cellular Trajectory Map Matching](https://arxiv.org/abs/2508.06674)
*Weijie Shi,Yue Cui,Hao Chen,Jiaming Li,Mengze Li,Jia Zhu,Jiajie Xu,Xiaofang Zhou*

Main category: cs.AI

TL;DR: 本文提出像素轨迹校准助手用于零样本CTMM，通过结合地理知识、高斯混合模型、时空感知模块和约束寻路算法，提升零样本CTMM性能，实验显示优于现有方法16.8%。


<details>
  <summary>Details</summary>
Motivation: 现有CTMM方法依赖特定区域数据，适应性差，需实现无额外训练的高精度零样本CTMM。

Method: 提出像素轨迹校准助手，结合可迁移地理知识校准轨迹；将高斯混合模型融入VAE进行软聚类；设计时空感知模块捕捉序列特征和位置不确定性；采用约束寻路算法重建道路ID序列。

Result: 模型在零样本CTMM中比现有方法性能提升16.8%。

Conclusion: 所提方法能有效解决零样本CTMM问题，优于现有方法。

Abstract: Cellular Trajectory Map-Matching (CTMM) aims to align cellular location
sequences to road networks, which is a necessary preprocessing in
location-based services on web platforms like Google Maps, including navigation
and route optimization. Current approaches mainly rely on ID-based features and
region-specific data to learn correlations between cell towers and roads,
limiting their adaptability to unexplored areas. To enable high-accuracy CTMM
without additional training in target regions, Zero-shot CTMM requires to
extract not only region-adaptive features, but also sequential and location
uncertainty to alleviate positioning errors in cellular data. In this paper, we
propose a pixel-based trajectory calibration assistant for zero-shot CTMM,
which takes advantage of transferable geospatial knowledge to calibrate
pixelated trajectory, and then guide the path-finding process at the road
network level. To enhance knowledge sharing across similar regions, a Gaussian
mixture model is incorporated into VAE, enabling the identification of
scenario-adaptive experts through soft clustering. To mitigate high positioning
errors, a spatial-temporal awareness module is designed to capture sequential
features and location uncertainty, thereby facilitating the inference of
approximate user positions. Finally, a constrained path-finding algorithm is
employed to reconstruct the road ID sequence, ensuring topological validity
within the road network. This process is guided by the calibrated trajectory
while optimizing for the shortest feasible path, thus minimizing unnecessary
detours. Extensive experiments demonstrate that our model outperforms existing
methods in zero-shot CTMM by 16.8\%.

</details>


### [8] [Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets](https://arxiv.org/abs/2508.06706)
*Jaikrishna Manojkumar Patil,Nathaniel Lee,Al Mehdi Saadat Chowdhury,YooJung Choi,Paulo Shakarian*

Main category: cs.AI

TL;DR: 提出从训练数据发现规则上下文，并利用概率电路提升基于规则的知识图谱补全性能，减少规则使用量。


<details>
  <summary>Details</summary>
Motivation: 基于规则的知识图谱补全方法需大量规则，庞大规则集影响可解释性，需提升效率。

Method: 从训练数据发现规则上下文，利用概率电路学习规则上下文概率分布。

Result: 规则使用量减少70 - 96%，同等最少规则下性能超基线31倍，与基线完整规则集相比保留91%峰值性能，在8个基准数据集表现良好。

Conclusion: 方法基于概率逻辑语义，无需独立性假设，推理过程可提供近似下界和精确概率，对规则集上的概率推理有进一步意义。

Abstract: Rule-based methods for knowledge graph completion provide explainable results
but often require a significantly large number of rules to achieve competitive
performance. This can hinder explainability due to overwhelmingly large rule
sets. We discover rule contexts (meaningful subsets of rules that work
together) from training data and use learned probability distribution (i.e.
probabilistic circuits) over these rule contexts to more rapidly achieve
performance of the full rule set. Our approach achieves a 70-96% reduction in
number of rules used while outperforming baseline by up to 31$\times$ when
using equivalent minimal number of rules and preserves 91% of peak baseline
performance even when comparing our minimal rule sets against baseline's full
rule sets. We show that our framework is grounded in well-known semantics of
probabilistic logic, does not require independence assumptions, and that our
tractable inference procedure provides both approximate lower bounds and exact
probability of a given query. The efficacy of our method is validated by
empirical studies on 8 standard benchmark datasets where we show competitive
performance by using only a fraction of the rules required by AnyBURL's
standard inference method, the current state-of-the-art for rule-based
knowledge graph completion. This work may have further implications for general
probabilistic reasoning over learned sets of rules.

</details>


### [9] [GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning](https://arxiv.org/abs/2508.06716)
*Blair Johnson,Clayton Kerce,Faramarz Fekri*

Main category: cs.AI

TL;DR: 介绍GLIDR方法，它比以往方法有更具表现力的规则语法，在知识图谱补全任务中表现优异，规则可提取，对噪声鲁棒，还能与深度神经网络结合。


<details>
  <summary>Details</summary>
Motivation: 现有可微归纳逻辑编程技术中常见的链状规则结构假设会影响性能和可解释性。

Method: 引入GLIDR，使用可微消息传递推理算法，推广链状规则学习方法，规则搜索空间简单且由最大自由变量数参数化，可从模型权重中提取规则。

Result: GLIDR在知识图谱补全任务中显著优于现有规则学习方法，提取的规则有显著预测性能，对训练数据噪声高度鲁棒，可与深度神经网络端到端优化。

Conclusion: GLIDR是一种有效的规则学习方法，具有良好的性能、可解释性和鲁棒性，还能与其他模型结合用于任意数据模态的规则学习。

Abstract: Differentiable inductive logic programming (ILP) techniques have proven
effective at finding approximate rule-based solutions to link prediction and
node classification problems on knowledge graphs; however, the common
assumption of chain-like rule structure can hamper the performance and
interpretability of existing approaches. We introduce GLIDR, a differentiable
rule learning method that models the inference of logic rules with more
expressive syntax than previous methods. GLIDR uses a differentiable message
passing inference algorithm that generalizes previous chain-like rule learning
methods to allow rules with features like branches and cycles. GLIDR has a
simple and expressive rule search space which is parameterized by a limit on
the maximum number of free variables that may be included in a rule. Explicit
logic rules can be extracted from the weights of a GLIDR model for use with
symbolic solvers. We demonstrate that GLIDR can significantly outperform
existing rule learning methods on knowledge graph completion tasks and even
compete with embedding methods despite the inherent disadvantage of being a
structure-only prediction method. We show that rules extracted from GLIDR
retain significant predictive performance, and that GLIDR is highly robust to
training data noise. Finally, we demonstrate that GLIDR can be chained with
deep neural networks and optimized end-to-end for rule learning on arbitrary
data modalities.

</details>


### [10] [ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search](https://arxiv.org/abs/2508.06736)
*Alican Yilmaz,Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 本文研究Balans并行化能力，提出扩展ParBalans，实验表明其在挑战性MIP实例上表现有竞争力。


<details>
  <summary>Details</summary>
Motivation: MIP问题求解需大量计算资源，Balans并行潜力未充分研究，要提升挑战性MIP实例求解性能。

Method: 引入ParBalans，利用求解器级和算法级并行。

Result: ParBalans与商业求解器Gurobi相比有竞争力，在硬优化基准上表现突出。

Conclusion: ParBalans能有效利用并行性，提升挑战性MIP实例求解性能。

Abstract: Solving Mixed-Integer Programming (MIP) problems often requires substantial
computational resources due to their combinatorial nature. Parallelization has
emerged as a critical strategy to accelerate solution times and enhance
scalability to tackle large, complex instances. This paper investigates the
parallelization capabilities of Balans, a recently proposed multi-armed
bandits-based adaptive large neighborhood search for MIPs. While Balans's
modular architecture inherently supports parallel exploration of diverse
parameter configurations, this potential has not been thoroughly examined. To
address this gap, we introduce ParBalans, an extension that leverages both
solver-level and algorithmic-level parallelism to improve performance on
challenging MIP instances. Our experimental results demonstrate that ParBalans
exhibits competitive performance compared to the state-of-the-art commercial
solver Gurobi, particularly on hard optimization benchmarks.

</details>


### [11] [Topology Generation of UAV Covert Communication Networks: A Graph Diffusion Approach with Incentive Mechanism](https://arxiv.org/abs/2508.06746)
*Xin Tang,Qian Chen,Fengshun Li,Youchun Gong,Yinqiu Liu,Wen Tian,Shaowen Qin,Xiaohuan Li*

Main category: cs.AI

TL;DR: 本文提出结合GDPO与SG激励机制的自组织无人机网络框架，经实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 无人机网络在敏感应用中需求增长，动态移动性和暴露风险带来确保可靠连接和隐蔽通信的挑战。

Method: 提出结合图扩散策略优化（GDPO）与基于斯塔克尔伯格博弈（SG）激励机制的自组织无人机网络框架，GDPO用生成式AI生成拓扑，SG激励机制引导无人机协作。

Result: 通过大量实验验证了所提框架在模型收敛、拓扑生成质量和隐蔽通信性能提升方面的有效性。

Conclusion: 所提结合GDPO与SG激励机制的框架能有效应对无人机网络面临的挑战。

Abstract: With the growing demand for Uncrewed Aerial Vehicle (UAV) networks in
sensitive applications, such as urban monitoring, emergency response, and
secure sensing, ensuring reliable connectivity and covert communication has
become increasingly vital. However, dynamic mobility and exposure risks pose
significant challenges. To tackle these challenges, this paper proposes a
self-organizing UAV network framework combining Graph Diffusion-based Policy
Optimization (GDPO) with a Stackelberg Game (SG)-based incentive mechanism. The
GDPO method uses generative AI to dynamically generate sparse but
well-connected topologies, enabling flexible adaptation to changing node
distributions and Ground User (GU) demands. Meanwhile, the Stackelberg Game
(SG)-based incentive mechanism guides self-interested UAVs to choose relay
behaviors and neighbor links that support cooperation and enhance covert
communication. Extensive experiments are conducted to validate the
effectiveness of the proposed framework in terms of model convergence, topology
generation quality, and enhancement of covert communication performance.

</details>


### [12] [A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks](https://arxiv.org/abs/2508.06754)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 提出模块化提示框架，支持在动态、以用户为中心的任务中更安全、自适应地使用大语言模型，在模拟辅导场景表现出色且有其他领域应用潜力。


<details>
  <summary>Details</summary>
Motivation: 实现大语言模型在动态、以用户为中心任务中更安全、自适应的使用。

Method: 结合自然语言边界提示、模糊支架逻辑和适应规则的控制模式，基于人类学习理论。

Result: 在模拟智能辅导场景中，提升了支架质量、适应性和教学一致性，优于标准提示基线。

Conclusion: 该框架为不确定或不断变化的环境中构建可解释、目标一致的大语言模型行为提供了可复用方法。

Abstract: We introduce a modular prompting framework that supports safer and more
adaptive use of large language models (LLMs) across dynamic, user-centered
tasks. Grounded in human learning theory, particularly the Zone of Proximal
Development (ZPD), our method combines a natural language boundary prompt with
a control schema encoded with fuzzy scaffolding logic and adaptation rules.
This architecture enables LLMs to modulate behavior in response to user state
without requiring fine-tuning or external orchestration. In a simulated
intelligent tutoring setting, the framework improves scaffolding quality,
adaptivity, and instructional alignment across multiple models, outperforming
standard prompting baselines. Evaluation is conducted using rubric-based LLM
graders at scale. While initially developed for education, the framework has
shown promise in other interaction-heavy domains, such as procedural content
generation for games. Designed for safe deployment, it provides a reusable
methodology for structuring interpretable, goal-aligned LLM behavior in
uncertain or evolving contexts.

</details>


### [13] [Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation](https://arxiv.org/abs/2508.06823)
*Xuan Zhao,Jun Tao*

Main category: cs.AI

TL;DR: 提出利用自然语言交互提升体数据探索效率的框架，可自动选视角，提高导航效率和现象可解释性。


<details>
  <summary>Details</summary>
Motivation: 选择有效导航的最佳视角有挑战，尤其对非专业和不熟悉3D导航的用户。

Method: 对体数据块编码，引入CLIP Score机制提供语义信息，用强化学习框架结合语义线索搜索视角，并用CLIP Score评估视角。

Result: 能自动选择符合用户意图的视角。

Conclusion: 该方法提高了体数据导航效率，增强了复杂科学现象的可解释性。

Abstract: Exploring volumetric data is crucial for interpreting scientific datasets.
However, selecting optimal viewpoints for effective navigation can be
challenging, particularly for users without extensive domain expertise or
familiarity with 3D navigation. In this paper, we propose a novel framework
that leverages natural language interaction to enhance volumetric data
exploration. Our approach encodes volumetric blocks to capture and
differentiate underlying structures. It further incorporates a CLIP Score
mechanism, which provides semantic information to the blocks to guide
navigation. The navigation is empowered by a reinforcement learning framework
that leverage these semantic cues to efficiently search for and identify
desired viewpoints that align with the user's intent. The selected viewpoints
are evaluated using CLIP Score to ensure that they best reflect the user
queries. By automating viewpoint selection, our method improves the efficiency
of volumetric data navigation and enhances the interpretability of complex
scientific phenomena.

</details>


### [14] [Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges](https://arxiv.org/abs/2508.06832)
*Haifeng Li,Wang Guo,Haiyang Wu,Mengwei Wu,Jipeng Zhang,Qing Zhu,Yu Liu,Xin Huang,Chao Tao*

Main category: cs.AI

TL;DR: 文章倡导从以视觉为中心转向以语言为中心的遥感影像解译范式，提出基于全球工作空间理论的语言中心框架，总结挑战、构建机制并指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统以视觉为中心的遥感影像解译模型在多模态推理等方面存在局限，现有引入大语言模型的研究缺乏统一理论框架解释语言的认知作用。

Method: 借鉴人类认知的全球工作空间理论，提出以语言为中心的遥感影像解译框架，将大语言模型作为认知中心枢纽，整合多个空间。

Result: 探讨了大语言模型在遥感解译中的潜力，总结核心技术挑战，构建全球工作空间驱动的解译机制。

Conclusion: 为下一代遥感影像解译系统提供概念基础，为认知驱动的智能地理空间分析制定路线图。

Abstract: The mainstream paradigm of remote sensing image interpretation has long been
dominated by vision-centered models, which rely on visual features for semantic
understanding. However, these models face inherent limitations in handling
multi-modal reasoning, semantic abstraction, and interactive decision-making.
While recent advances have introduced Large Language Models (LLMs) into remote
sensing workflows, existing studies primarily focus on downstream applications,
lacking a unified theoretical framework that explains the cognitive role of
language. This review advocates a paradigm shift from vision-centered to
language-centered remote sensing interpretation. Drawing inspiration from the
Global Workspace Theory (GWT) of human cognition, We propose a
language-centered framework for remote sensing interpretation that treats LLMs
as the cognitive central hub integrating perceptual, task, knowledge and action
spaces to enable unified understanding, reasoning, and decision-making. We
first explore the potential of LLMs as the central cognitive component in
remote sensing interpretation, and then summarize core technical challenges,
including unified multimodal representation, knowledge association, and
reasoning and decision-making. Furthermore, we construct a global
workspace-driven interpretation mechanism and review how language-centered
solutions address each challenge. Finally, we outline future research
directions from four perspectives: adaptive alignment of multimodal data, task
understanding under dynamic knowledge constraints, trustworthy reasoning, and
autonomous interaction. This work aims to provide a conceptual foundation for
the next generation of remote sensing interpretation systems and establish a
roadmap toward cognition-driven intelligent geospatial analysis.

</details>


### [15] [Efficient and Reliable Hitting-Set Computations for the Implicit Hitting Set Approach](https://arxiv.org/abs/2508.07015)
*Hannes Ihalainen,Dieter Vandesande,André Schidler,Jeremias Berg,Bart Bogaerts,Matti Järvisalo*

Main category: cs.AI

TL;DR: 本文探讨了基于伪布尔推理和随机局部搜索的隐式命中集优化替代算法，评估其实用性，指出商业IP求解器虽有效但有数值不稳定问题，伪布尔推理可使精确命中集计算具有竞争力并能提供正确性证明。


<details>
  <summary>Details</summary>
Motivation: 探索基于不同伪布尔推理和随机局部搜索方式的命中集优化替代算法，评估其在伪布尔优化中作为隐式命中集最新实例的实用性。

Method: 采用不同的伪布尔推理和随机局部搜索方式进行命中集优化，并进行广泛评估。

Result: 商业IP求解器仍是最有效的命中集计算方式，但存在数值不稳定导致的正确性问题，基于伪布尔推理的精确命中集计算可与数值精确的IP求解器竞争，且能为隐式命中集计算提供正确性证明。

Conclusion: 伪布尔推理可使隐式命中集计算更具竞争力并提供正确性证明，适用于能以基于伪布尔的证明格式捕获声明性语言推理的任何隐式命中集实例。

Abstract: The implicit hitting set (IHS) approach offers a general framework for
solving computationally hard combinatorial optimization problems declaratively.
IHS iterates between a decision oracle used for extracting sources of
inconsistency and an optimizer for computing so-called hitting sets (HSs) over
the accumulated sources of inconsistency. While the decision oracle is
language-specific, the optimizers is usually instantiated through integer
programming.
  We explore alternative algorithmic techniques for hitting set optimization
based on different ways of employing pseudo-Boolean (PB) reasoning as well as
stochastic local search. We extensively evaluate the practical feasibility of
the alternatives in particular in the context of pseudo-Boolean (0-1 IP)
optimization as one of the most recent instantiations of IHS. Highlighting a
trade-off between efficiency and reliability, while a commercial IP solver
turns out to remain the most effective way to instantiate HS computations, it
can cause correctness issues due to numerical instability; in fact, we show
that exact HS computations instantiated via PB reasoning can be made
competitive with a numerically exact IP solver. Furthermore, the use of PB
reasoning as a basis for HS computations allows for obtaining certificates for
the correctness of IHS computations, generally applicable to any IHS
instantiation in which reasoning in the declarative language at hand can be
captured in the PB-based proof format we employ.

</details>


### [16] [Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.06836)
*Xutong Zhao,Yaqi Xie*

Main category: cs.AI

TL;DR: 本文提出多级别优势信用分配方法MACA解决多智能体强化学习中的信用分配问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中信用分配是关键挑战，任务多样，奖励分配复杂，需处理多级别共存的场景。

Method: 形式化信用分配级别，引入多级别优势公式进行反事实推理，利用基于注意力的框架识别相关智能体关系，构建多级别优势指导策略学习。

Result: 在具有挑战性的星际争霸v1和v2任务上的综合实验表明MACA性能优越。

Conclusion: MACA在复杂信用分配场景中有效。

Abstract: Cooperative multi-agent reinforcement learning (MARL) aims to coordinate
multiple agents to achieve a common goal. A key challenge in MARL is credit
assignment, which involves assessing each agent's contribution to the shared
reward. Given the diversity of tasks, agents may perform different types of
coordination, with rewards attributed to diverse and often overlapping agent
subsets. In this work, we formalize the credit assignment level as the number
of agents cooperating to obtain a reward, and address scenarios with multiple
coexisting levels. We introduce a multi-level advantage formulation that
performs explicit counterfactual reasoning to infer credits across distinct
levels. Our method, Multi-level Advantage Credit Assignment (MACA), captures
agent contributions at multiple levels by integrating advantage functions that
reason about individual, joint, and correlated actions. Utilizing an
attention-based framework, MACA identifies correlated agent relationships and
constructs multi-level advantages to guide policy learning. Comprehensive
experiments on challenging Starcraft v1\&v2 tasks demonstrate MACA's superior
performance, underscoring its efficacy in complex credit assignment scenarios.

</details>


### [17] [MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams](https://arxiv.org/abs/2508.06851)
*Pengfei Zhou,Xiaopeng Peng,Fanrui Zhang,Zhaopan Xu,Jiaxin Ai,Yansheng Qiu,Chuanhao Li,Zhen Li,Ming Li,Yukang Feng,Jianwen Sun,Haoquan Zhang,Zizhen Li,Xiaofeng Mao,Zekai Li,Wangbo Zhao,Kai Wang,Xiaojun Chang,Wenqi Shao,Yang You,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 提出MDK12 - Bench基准和动态评估框架评估多模态大语言模型，揭示其局限并为改进提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型智能评估基准存在规模有限、覆盖窄和知识无结构等问题，需更好的评估方法。

Method: 引入MDK12 - Bench基准，提出动态评估框架，评估知识点参考增强生成。

Result: 发现当前多模态大语言模型在多方面存在局限。

Conclusion: 研究为增强模型鲁棒性、可解释性和人工智能辅助教育提供指导。

Abstract: Multimodal large language models (MLLMs), which integrate language and visual
cues for problem-solving, are crucial for advancing artificial general
intelligence (AGI). However, current benchmarks for measuring the intelligence
of MLLMs suffer from limited scale, narrow coverage, and unstructured
knowledge, offering only static and undifferentiated evaluations. To bridge
this gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark
built from real-world K-12 exams spanning six disciplines with 141K instances
and 6,225 knowledge points organized in a six-layer taxonomy. Covering five
question formats with difficulty and year annotations, it enables comprehensive
evaluation to capture the extent to which MLLMs perform over four dimensions:
1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts,
and 4) knowledge-driven reasoning. We propose a novel dynamic evaluation
framework that introduces unfamiliar visual, textual, and question form shifts
to challenge model generalization while improving benchmark objectivity and
longevity by mitigating data contamination. We further evaluate knowledge-point
reference-augmented generation (KP-RAG) to examine the role of knowledge in
problem-solving. Key findings reveal limitations in current MLLMs in multiple
aspects and provide guidance for enhancing model robustness, interpretability,
and AI-assisted education.

</details>


### [18] [DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning](https://arxiv.org/abs/2508.06972)
*Dan Ivanov,Tristan Freiberg,Haruna Isah*

Main category: cs.AI

TL;DR: DSperse是用于分布式机器学习推理的模块化框架，可进行策略性加密验证，能灵活支持可扩展、有针对性的验证策略。


<details>
  <summary>Details</summary>
Motivation: 避免分布式零知识机器学习中全模型电路化的高成本和僵化问题，实现信任最小化。

Method: 通过对策略性选择的子计算进行有针对性的验证，设置可验证的“切片”，并通过审计、复制或经济激励来确保全局一致性。

Result: 使用多种证明系统评估DSperse，报告了切片和未切片配置下的内存使用、运行时间和电路行为的实证结果。

Conclusion: DSperse允许证明边界与模型的逻辑结构灵活对齐，支持适合不同部署需求的可扩展、有针对性的验证策略。

Abstract: DSperse is a modular framework for distributed machine learning inference
with strategic cryptographic verification. Operating within the emerging
paradigm of distributed zero-knowledge machine learning, DSperse avoids the
high cost and rigidity of full-model circuitization by enabling targeted
verification of strategically chosen subcomputations. These verifiable
segments, or "slices", may cover part or all of the inference pipeline, with
global consistency enforced through audit, replication, or economic incentives.
This architecture supports a pragmatic form of trust minimization, localizing
zero-knowledge proofs to the components where they provide the greatest value.
We evaluate DSperse using multiple proving systems and report empirical results
on memory usage, runtime, and circuit behavior under sliced and unsliced
configurations. By allowing proof boundaries to align flexibly with the model's
logical structure, DSperse supports scalable, targeted verification strategies
suited to diverse deployment needs.

</details>


### [19] [MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction](https://arxiv.org/abs/2508.06859)
*Shuo Tang,Jian Xu,Jiadong Zhang,Yi Chen,Qizhao Jin,Lingdong Shen,Chenglin Liu,Shiming Xiang*

Main category: cs.AI

TL;DR: 本文提出大规模数据集MP - Bench和气象多模态大模型MMLM用于恶劣天气预测，实验证明MMLM效果好，将公开代码和数据。


<details>
  <summary>Details</summary>
Motivation: 当前恶劣天气预报依赖人工，存在主观性和操作负担，端到端AI恶劣天气系统发展面临样本稀缺、数据与预警文本对齐不佳、现有模型处理高维气象数据能力不足等挑战。

Method: 引入大规模数据集MP - Bench，基于该数据集开发气象多模态大模型MMLM，融入三个即插即用自适应融合模块。

Result: 在MP - Bench上的大量实验表明MMLM在多任务中表现出色。

Conclusion: MMLM在恶劣天气理解上有效，是实现自动化AI天气预报系统的关键一步。

Abstract: Timely and accurate severe weather warnings are critical for disaster
mitigation. However, current forecasting systems remain heavily reliant on
manual expert interpretation, introducing subjectivity and significant
operational burdens. With the rapid development of AI technologies, the
end-to-end "AI weather station" is gradually emerging as a new trend in
predicting severe weather events. Three core challenges impede the development
of end-to-end AI severe weather system: (1) scarcity of severe weather event
samples; (2) imperfect alignment between high-dimensional meteorological data
and textual warnings; (3) existing multimodal language models are unable to
handle high-dimensional meteorological data and struggle to fully capture the
complex dependencies across temporal sequences, vertical pressure levels, and
spatial dimensions. To address these challenges, we introduce MP-Bench, the
first large-scale temporal multimodal dataset for severe weather events
prediction, comprising 421,363 pairs of raw multi-year meteorological data and
corresponding text caption, covering a wide range of severe weather scenarios
across China. On top of this dataset, we develop a meteorology multimodal large
model (MMLM) that directly ingests 4D meteorological inputs. In addition, it is
designed to accommodate the unique characteristics of 4D meteorological data
flow, incorporating three plug-and-play adaptive fusion modules that enable
dynamic feature extraction and integration across temporal sequences, vertical
pressure layers, and spatial dimensions. Extensive experiments on MP-Bench
demonstrate that MMLM performs exceptionally well across multiple tasks,
highlighting its effectiveness in severe weather understanding and marking a
key step toward realizing automated, AI-driven weather forecasting systems. Our
source code and dataset will be made publicly available.

</details>


### [20] [Pushdown Reward Machines for Reinforcement Learning](https://arxiv.org/abs/2508.06894)
*Giovanni Varricchione,Toryn Q. Klassen,Natasha Alechina,Mehdi Dastani,Brian Logan,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 本文提出下推奖励机器（pdRMs），介绍基于pdRM的两种策略，给出检查策略获得相同最优期望奖励的方法，提供理论和实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有奖励机器（RMs）只能奖励正则语言表示的行为，为了能识别和奖励确定性上下文无关语言表示的行为，提出pdRMs。

Method: 提出基于确定性下推自动机的pdRMs，引入两种基于pdRM的策略，给出检查策略获得相同最优期望奖励的程序。

Result: 提供了pdRMs表达能力的理论结果和学习问题的空间复杂度结果，实验表明可使用pdRMs训练智能体完成确定性上下文无关语言表示的任务。

Conclusion: pdRMs比奖励机器更具表达能力，可用于训练智能体完成更复杂的任务。

Abstract: Reward machines (RMs) are automata structures that encode (non-Markovian)
reward functions for reinforcement learning (RL). RMs can reward any behaviour
representable in regular languages and, when paired with RL algorithms that
exploit RM structure, have been shown to significantly improve sample
efficiency in many domains. In this work, we present pushdown reward machines
(pdRMs), an extension of reward machines based on deterministic pushdown
automata. pdRMs can recognize and reward temporally extended behaviours
representable in deterministic context-free languages, making them more
expressive than reward machines. We introduce two variants of pdRM-based
policies, one which has access to the entire stack of the pdRM, and one which
can only access the top $k$ symbols (for a given constant $k$) of the stack. We
propose a procedure to check when the two kinds of policies (for a given
environment, pdRM, and constant $k$) achieve the same optimal expected reward.
We then provide theoretical results establishing the expressive power of pdRMs,
and space complexity results about the proposed learning problems. Finally, we
provide experimental results showing how agents can be trained to perform tasks
representable in deterministic context-free languages using pdRMs.

</details>


### [21] [CP-Agent: Agentic Constraint Programming](https://arxiv.org/abs/2508.07468)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 提出无固定流程的纯代理策略解决自然语言到约束模型的翻译问题，实现代码少且解决基准集所有问题。


<details>
  <summary>Details</summary>
Motivation: 以往自动化翻译方法采用固定工作流，在大量基准问题上失败，需新方法。

Method: 基于ReAct原则开发通用Python编码代理，通过精心设计的项目提示注入领域专业知识，结合文件操作和代码执行工具动态测试、调试和验证。

Result: 用几百行代码实现的架构成功解决CP - Bench约束编程基准集的101个问题。

Conclusion: 约束建模任务需结合通用编码工具和提示中编码的领域专业知识，而非专门的代理架构或预定义工作流。

Abstract: Translating natural language problem descriptions into formal constraint
models remains a fundamental challenge in constraint programming, requiring
deep expertise in both the problem domain and modeling frameworks. Previous
approaches to automating this translation have employed fixed workflows with
predetermined modeling steps, failing on a significant number of benchmark
problems. We present a new approach using a pure agentic strategy without any
fixed pipeline. We developed a general-purpose Python coding agent based on the
ReAct (Reason and Act) principle, utilizing a persistent IPython kernel for
stateful code execution and iterative development. Rather than embedding
constraint programming logic into the agent architecture, domain-specific
expertise is injected solely through a carefully crafted project prompt. The
agent combines this prompt-encoded knowledge with access to file operations and
code execution tools, enabling it to test hypotheses, debug failures, and
verify solutions dynamically. Implemented in just a few hundred lines of code,
this architecture successfully solves all 101 problems of the CP-Bench
constraint programming benchmark set. The results suggest that constraint
modeling tasks require the combination of general coding tools and domain
expertise encoded in prompts, rather than specialized agent architectures or
predefined workflows.

</details>


### [22] [GDBA Revisited: Unleashing the Power of Guided Local Search for Distributed Constraint Optimization](https://arxiv.org/abs/2508.06899)
*Yanchen Deng,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 本文指出GDBA在解决DCOPs问题时表现不佳的原因，提出DGLS框架，理论证明其特性，实验显示DGLS优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有局部搜索算法常收敛到较差局部最优，GDBA在一般值问题上改进效果有限，需改进算法性能。

Method: 提出DGLS框架，包含自适应违规条件、惩罚蒸发机制和同步方案。

Result: 理论证明DGLS中惩罚值有界，代理参与潜在博弈；实验表明DGLS在标准基准测试中优于现有基线，在结构化问题上显著超越Damped Max - sum。

Conclusion: DGLS是解决DCOPs问题的有效方法，具有很大优越性。

Abstract: Local search is an important class of incomplete algorithms for solving
Distributed Constraint Optimization Problems (DCOPs) but it often converges to
poor local optima. While GDBA provides a comprehensive rule set to escape
premature convergence, its empirical benefits remain marginal on general-valued
problems. In this work, we systematically examine GDBA and identify three
factors that potentially lead to its inferior performance, i.e.,
over-aggressive constraint violation conditions, unbounded penalty
accumulation, and uncoordinated penalty updates. To address these issues, we
propose Distributed Guided Local Search (DGLS), a novel GLS framework for DCOPs
that incorporates an adaptive violation condition to selectively penalize
constraints with high cost, a penalty evaporation mechanism to control the
magnitude of penalization, and a synchronization scheme for coordinated penalty
updates. We theoretically show that the penalty values are bounded, and agents
play a potential game in our DGLS. Our extensive empirical results on various
standard benchmarks demonstrate the great superiority of DGLS over
state-of-the-art baselines. Particularly, compared to Damped Max-sum with high
damping factors (e.g., 0.7 or 0.9), our DGLS achieves competitive performance
on general-valued problems, and outperforms it by significant margins
(\textbf{3.77\%--66.3\%}) on structured problems in terms of anytime results.

</details>


### [23] [Automated Formalization via Conceptual Retrieval-Augmented LLMs](https://arxiv.org/abs/2508.06931)
*Wangyue Lu,Lun Du,Sirui Li,Ke Weng,Haozhe Sun,Hengyu Liu,Minghe Yu,Tiancheng Zhang,Ge Yu*

Main category: cs.AI

TL;DR: 针对交互式定理证明器手动形式化难题，提出CRAMF框架，构建知识库、解决概念多态问题并设计检索策略，实验显示能提升翻译准确率。


<details>
  <summary>Details</summary>
Motivation: 交互式定理证明器手动形式化劳动密集且需专业知识，自动形式化存在模型幻觉和语义差距问题。

Method: 提出CRAMF框架，从Mathlib4构建概念 - 定义知识库，提出上下文查询增强方法，设计双通道混合检索重排策略。

Result: 在miniF2F、ProofNet和AdvancedMath基准测试中，CRAMF可集成到基于大语言模型的自动形式化器中，翻译准确率最高提升62.1%，平均提升29.9%。

Conclusion: CRAMF能有效解决自动形式化的挑战，提升形式化翻译的准确性。

Abstract: Interactive theorem provers (ITPs) require manual formalization, which is
labor-intensive and demands expert knowledge. While automated formalization
offers a potential solution, it faces two major challenges: model hallucination
(e.g., undefined predicates, symbol misuse, and version incompatibility) and
the semantic gap caused by ambiguous or missing premises in natural language
descriptions. To address these issues, we propose CRAMF, a Concept-driven
Retrieval-Augmented Mathematical Formalization framework. CRAMF enhances
LLM-based autoformalization by retrieving formal definitions of core
mathematical concepts, providing contextual grounding during code generation.
However, applying retrieval-augmented generation (RAG) in this setting is
non-trivial due to the lack of structured knowledge bases, the polymorphic
nature of mathematical concepts, and the high precision required in formal
retrieval. We introduce a framework for automatically constructing a
concept-definition knowledge base from Mathlib4, the standard mathematical
library for the Lean 4 theorem prover, indexing over 26,000 formal definitions
and 1,000+ core mathematical concepts. To address conceptual polymorphism, we
propose contextual query augmentation with domain- and application-level
signals. In addition, we design a dual-channel hybrid retrieval strategy with
reranking to ensure accurate and relevant definition retrieval. Experiments on
miniF2F, ProofNet, and our newly proposed AdvancedMath benchmark show that
CRAMF can be seamlessly integrated into LLM-based autoformalizers, yielding
consistent improvements in translation accuracy, achieving up to 62.1% and an
average of 29.9% relative improvement.

</details>


### [24] [Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction](https://arxiv.org/abs/2508.06939)
*Hiba Najjar,Deepak Pathak,Marlon Nuske,Andreas Dengel*

Main category: cs.AI

TL;DR: 本文利用Transformer模型解释多模态学习网络进行亚田块级作物产量预测，对比不同特征和模态归因方法，发现Transformer模型表现更优，AR归因更可靠。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中模型可解释性常被忽视，研究旨在利用Transformer模型的可解释性解释多模态学习网络进行作物产量预测。

Method: 使用大规模多模态数据集，基于自注意力机制用AR和GA估计特征归因，与SVS对比；提出WMA评估模态归因并与SVS对比。

Result: Transformer模型优于卷积和循环网络；AR比GA和SVS提供更可靠的时间归因；模态归因在两种方法中有不同模式。

Conclusion: Transformer模型在作物产量预测中有优势，AR方法在特征归因上更可靠。

Abstract: Multimodal learning enables various machine learning tasks to benefit from
diverse data sources, effectively mimicking the interplay of different factors
in real-world applications, particularly in agriculture. While the
heterogeneous nature of involved data modalities may necessitate the design of
complex architectures, the model interpretability is often overlooked. In this
study, we leverage the intrinsic explainability of Transformer-based models to
explain multimodal learning networks, focusing on the task of crop yield
prediction at the subfield level. The large datasets used cover various crops,
regions, and years, and include four different input modalities: multispectral
satellite and weather time series, terrain elevation maps and soil properties.
Based on the self-attention mechanism, we estimate feature attributions using
two methods, namely the Attention Rollout (AR) and Generic Attention (GA), and
evaluate their performance against Shapley-based model-agnostic estimations,
Shapley Value Sampling (SVS). Additionally, we propose the Weighted Modality
Activation (WMA) method to assess modality attributions and compare it with SVS
attributions. Our findings indicate that Transformer-based models outperform
other architectures, specifically convolutional and recurrent networks,
achieving R2 scores that are higher by 0.10 and 0.04 at the subfield and field
levels, respectively. AR is shown to provide more robust and reliable temporal
attributions, as confirmed through qualitative and quantitative evaluation,
compared to GA and SVS values. Information about crop phenology stages was
leveraged to interpret the explanation results in the light of established
agronomic knowledge. Furthermore, modality attributions revealed varying
patterns across the two methods compared.[...]

</details>


### [25] [Large Language Models Do Not Simulate Human Psychology](https://arxiv.org/abs/2508.06950)
*Sarah Schröder,Thekla Morgenroth,Ulrike Kuhl,Valerie Vaquet,Benjamin Paaßen*

Main category: cs.AI

TL;DR: 文章警示用大语言模型替代心理学研究中的人类参与者的做法，指出其不能模拟人类心理，建议研究者将其作为需验证的工具。


<details>
  <summary>Details</summary>
Motivation: 针对近期一些研究认为大语言模型能模拟人类心理、可替代心理学研究中人类参与者的观点，提出反对意见。

Method: 先给出概念性论证，再通过举例说明措辞变化导致大语言模型和人类反应的差异，以及不同大语言模型对新问题反应不同。

Result: 即使专门针对心理反应微调的CENTAUR模型，措辞变化也会使大语言模型和人类反应有显著差异，不同大语言模型对新问题反应不同，缺乏可靠性。

Conclusion: 大语言模型不能模拟人类心理，心理学研究者应将其视为需针对新应用对照人类反应进行验证的有用但不可靠工具。

Abstract: Large Language Models (LLMs),such as ChatGPT, are increasingly used in
research, ranging from simple writing assistance to complex data annotation
tasks. Recently, some research has suggested that LLMs may even be able to
simulate human psychology and can, hence, replace human participants in
psychological studies. We caution against this approach. We provide conceptual
arguments against the hypothesis that LLMs simulate human psychology. We then
present empiric evidence illustrating our arguments by demonstrating that
slight changes to wording that correspond to large changes in meaning lead to
notable discrepancies between LLMs' and human responses, even for the recent
CENTAUR model that was specifically fine-tuned on psychological responses.
Additionally, different LLMs show very different responses to novel items,
further illustrating their lack of reliability. We conclude that LLMs do not
simulate human psychology and recommend that psychological researchers should
treat LLMs as useful but fundamentally unreliable tools that need to be
validated against human responses for every new application.

</details>


### [26] [DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery](https://arxiv.org/abs/2508.06960)
*Keyu Li,Mohan Jiang,Dayuan Fu,Yunze Wu,Xiangkun Hu,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 介绍DatasetResearch基准评估AI代理发现和合成数据集的能力，揭示当前能力与完美数据集发现间的差距，为数据集发现代理建立基线。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，AI开发瓶颈转向数据可用性，需评估AI代理能否系统发现满足用户需求的数据集。

Method: 引入DatasetResearch基准，采用三维评估框架评估AI代理从208个现实需求中发现和合成数据集的能力。

Result: 先进的深度研究系统在DatasetResearch - pro子集上仅获22%的分数，搜索和合成代理各有优势但都在‘极端情况’下失败。

Conclusion: 为数据集发现代理建立了严格基线，为下一代自我改进AI系统提供基础，基准和分析结果公开。

Abstract: The rapid advancement of large language models has fundamentally shifted the
bottleneck in AI development from computational power to data availability-with
countless valuable datasets remaining hidden across specialized repositories,
research appendices, and domain platforms. As reasoning capabilities and deep
research methodologies continue to evolve, a critical question emerges: can AI
agents transcend conventional search to systematically discover any dataset
that meets specific user requirements, enabling truly autonomous demand-driven
data curation? We introduce DatasetResearch, the first comprehensive benchmark
evaluating AI agents' ability to discover and synthesize datasets from 208
real-world demands across knowledge-intensive and reasoning-intensive tasks.
Our tri-dimensional evaluation framework reveals a stark reality: even advanced
deep research systems achieve only 22% score on our challenging
DatasetResearch-pro subset, exposing the vast gap between current capabilities
and perfect dataset discovery. Our analysis uncovers a fundamental
dichotomy-search agents excel at knowledge tasks through retrieval breadth,
while synthesis agents dominate reasoning challenges via structured
generation-yet both catastrophically fail on "corner cases" outside existing
distributions. These findings establish the first rigorous baseline for dataset
discovery agents and illuminate the path toward AI systems capable of finding
any dataset in the digital universe. Our benchmark and comprehensive analysis
provide the foundation for the next generation of self-improving AI systems and
are publicly available at https://github.com/GAIR-NLP/DatasetResearch.

</details>


### [27] [MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair](https://arxiv.org/abs/2508.06963)
*Changqing Li,Tianlin Li,Xiaohan Zhang,Aishan Liu,Li Pan*

Main category: cs.AI

TL;DR: 提出基于表征工程的端到端框架MASteer修复大语言模型可信度问题，实验显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在可信度问题，现有修复方法有成本高、缺乏鲁棒性和可扩展性等缺点，当前表征工程方法自动化和适应性有限。

Method: 提出MASteer框架，包含生成定制样本的AutoTester和构建自适应转向策略的AutoRepairer。

Result: 在标准和定制可信度任务上，MASteer始终优于基线，在LLaMA - 3.1 - 8B - Chat和Qwen - 3 - 8B - Chat上分别提升指标15.36%和4.21%，并保持模型通用能力。

Conclusion: MASteer具有强大的鲁棒性、泛化性和实用价值，可用于可扩展、高效的可信度修复。

Abstract: Large Language Models (LLMs) face persistent and evolving trustworthiness
issues, motivating developers to seek automated and flexible repair methods
that enable convenient deployment across diverse scenarios. Existing repair
methods like supervised fine-tuning (SFT) and reinforcement learning with human
feedback (RLHF) are costly and slow, while prompt engineering lacks robustness
and scalability. Representation engineering, which steers model behavior by
injecting targeted concept vectors during inference, offers a lightweight,
training-free alternative. However, current approaches depend on manually
crafted samples and fixed steering strategies, limiting automation and
adaptability. To overcome these challenges, we propose MASteer, the first
end-to-end framework for trustworthiness repair in LLMs based on representation
engineering. MASteer integrates two core components: AutoTester, a multi-agent
system that generates diverse, high-quality steer samples tailored to developer
needs; and AutoRepairer, which constructs adaptive steering strategies with
anchor vectors for automated, context-aware strategy selection during
inference. Experiments on standard and customized trustworthiness tasks show
MASteer consistently outperforms baselines, improving metrics by 15.36% on
LLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat, while maintaining general model
capabilities. MASteer demonstrates strong robustness, generalization, and
practical value for scalable, efficient trustworthiness repair.

</details>


### [28] [Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model](https://arxiv.org/abs/2508.06980)
*Aswin Paul,Moein Khajehnejad,Forough Habibollahi,Brett J. Kagan,Adeel Razi*

Main category: cs.AI

TL;DR: 文章提出基于主动推理的框架来建模具身智能体决策，模拟游戏环境决策过程，结果展示了智能体学习情况，为可解释AI做贡献。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能发展，理解自主智能体有目的行为基础对开发安全高效系统至关重要，且生物系统有潜力。

Method: 提出基于主动推理的框架，使用实验信息生成模型，在模拟游戏环境中模拟决策过程。

Result: 展示了智能体的学习情况，揭示了基于记忆的学习和预测规划在智能决策中的作用。

Conclusion: 为可解释AI领域提供了一种基于生物学且可扩展的方法来理解智能体有目的的行为。

Abstract: With recent and rapid advancements in artificial intelligence (AI),
understanding the foundation of purposeful behaviour in autonomous agents is
crucial for developing safe and efficient systems. While artificial neural
networks have dominated the path to AI, recent studies are exploring the
potential of biologically based systems, such as networks of living biological
neuronal networks. Along with promises of high power and data efficiency, these
systems may also inform more explainable and biologically plausible models. In
this work, we propose a framework rooted in active inference, a general theory
of behaviour, to model decision-making in embodied agents. Using
experiment-informed generative models, we simulate decision-making processes in
a simulated game-play environment, mirroring experimental setups that use
biological neurons. Our results demonstrate learning in these agents, providing
insights into the role of memory-based learning and predictive planning in
intelligent decision-making. This work contributes to the growing field of
explainable AI by offering a biologically grounded and scalable approach to
understanding purposeful behaviour in agents.

</details>


### [29] [MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA](https://arxiv.org/abs/2508.07022)
*Shengtao Wen,Haodong Chen,Yadong Wang,Zhongying Pan,Xiang Chen,Yu Tian,Bo Qian,Dong Liang,Sheng-Jun Huang*

Main category: cs.AI

TL;DR: 提出首个用于评估临床多模态任务知识编辑的基准MultiMedEdit，揭示当前方法局限性并为未来技术发展提供基础。


<details>
  <summary>Details</summary>
Motivation: 以往研究少关注多模态医疗场景的知识编辑，医疗KE需结合更新知识与视觉推理支持临床决策，填补此空白。

Method: 提出MultiMedEdit基准，覆盖理解和推理任务类型，定义三维度量套件，支持跨范式比较，在单编辑和终身编辑设置下进行实验。

Result: 当前方法在泛化和长尾推理上有困难，尤其是复杂临床工作流程，效率分析揭示KE范式在实际部署中的权衡。

Conclusion: MultiMedEdit揭示当前方法局限，为未来开发临床鲁棒的知识编辑技术提供坚实基础。

Abstract: Knowledge editing (KE) provides a scalable approach for updating factual
knowledge in large language models without full retraining. While previous
studies have demonstrated effectiveness in general domains and medical QA
tasks, little attention has been paid to KE in multimodal medical scenarios.
Unlike text-only settings, medical KE demands integrating updated knowledge
with visual reasoning to support safe and interpretable clinical decisions. To
address this gap, we propose MultiMedEdit, the first benchmark tailored to
evaluating KE in clinical multimodal tasks. Our framework spans both
understanding and reasoning task types, defines a three-dimensional metric
suite (reliability, generality, and locality), and supports cross-paradigm
comparisons across general and domain-specific models. We conduct extensive
experiments under single-editing and lifelong-editing settings. Results suggest
that current methods struggle with generalization and long-tail reasoning,
particularly in complex clinical workflows. We further present an efficiency
analysis (e.g., edit latency, memory footprint), revealing practical trade-offs
in real-world deployment across KE paradigms. Overall, MultiMedEdit not only
reveals the limitations of current approaches but also provides a solid
foundation for developing clinically robust knowledge editing techniques in the
future.

</details>


### [30] [K-Dense Analyst: Towards Fully Automated Scientific Analysis](https://arxiv.org/abs/2508.07043)
*Orion Li,Vinayak Agarwal,Summer Zhou,Ashwin Gopinath,Timothy Kassis*

Main category: cs.AI

TL;DR: 提出K - Dense Analyst分层多智能体系统用于自主生物信息学分析，在BixBench上表现超GPT - 5，证明自主科学推理需专用系统。


<details>
  <summary>Details</summary>
Motivation: 现代生物信息学分析复杂，数据生成与科学洞察存在差距，大语言模型处理实际分析工作流有局限。

Method: 引入K - Dense Analyst，采用双循环架构，通过专业智能体将复杂目标分解为可执行、可验证任务。

Result: 在BixBench上，K - Dense Analyst准确率达29.2%，超GPT - 5 6.3个百分点，用Gemini 2.5 Pro实现远超其直接使用的性能。

Conclusion: 自主科学推理需专用系统，该成果向全自主计算生物学家迈进了重要一步。

Abstract: The complexity of modern bioinformatics analysis has created a critical gap
between data generation and developing scientific insights. While large
language models (LLMs) have shown promise in scientific reasoning, they remain
fundamentally limited when dealing with real-world analytical workflows that
demand iterative computation, tool integration and rigorous validation. We
introduce K-Dense Analyst, a hierarchical multi-agent system that achieves
autonomous bioinformatics analysis through a dual-loop architecture. K-Dense
Analyst, part of the broader K-Dense platform, couples planning with validated
execution using specialized agents to decompose complex objectives into
executable, verifiable tasks within secure computational environments. On
BixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense
Analyst achieves 29.2% accuracy, surpassing the best-performing language model
(GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what
is widely considered the most powerful LLM available. Remarkably, K-Dense
Analyst achieves this performance using Gemini 2.5 Pro, which attains only
18.3% accuracy when used directly, demonstrating that our architectural
innovations unlock capabilities far beyond the underlying model's baseline
performance. Our insights demonstrate that autonomous scientific reasoning
requires more than enhanced language models, it demands purpose-built systems
that can bridge the gap between high-level scientific objectives and low-level
computational execution. These results represent a significant advance toward
fully autonomous computational biologists capable of accelerating discovery
across the life sciences.

</details>


### [31] [Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach](https://arxiv.org/abs/2508.07063)
*Naseem Machlovi,Maryam Saleki,Innocent Ababio,Ruhul Amin*

Main category: cs.AI

TL;DR: 随着AI融入生活，对安全可靠审核的需求增加。研究指出LLMs存在不足，开发实验框架评估，推出SafePhi表现佳，强调需改进数据提升模型。


<details>
  <summary>Details</summary>
Motivation: AI融入生活后，LLMs虽有能力但在审核方面存在易出错、强化偏见等问题，需探索其在审核角色中的局限性。

Method: 开发基于SOTA模型的实验框架，引入包含49个类别的统一基准数据集，推出QLoRA微调版SafePhi。

Result: SafePhi的Macro F1分数达0.89，优于OpenAI Moderator和Llama Guard。

Conclusion: 强调应纳入更多异质和有代表性的数据并结合人工，提升模型的鲁棒性和可解释性。

Abstract: As AI systems become more integrated into daily life, the need for safer and
more reliable moderation has never been greater. Large Language Models (LLMs)
have demonstrated remarkable capabilities, surpassing earlier models in
complexity and performance. Their evaluation across diverse tasks has
consistently showcased their potential, enabling the development of adaptive
and personalized agents. However, despite these advancements, LLMs remain prone
to errors, particularly in areas requiring nuanced moral reasoning. They
struggle with detecting implicit hate, offensive language, and gender biases
due to the subjective and context-dependent nature of these issues. Moreover,
their reliance on training data can inadvertently reinforce societal biases,
leading to inconsistencies and ethical concerns in their outputs. To explore
the limitations of LLMs in this role, we developed an experimental framework
based on state-of-the-art (SOTA) models to assess human emotions and offensive
behaviors. The framework introduces a unified benchmark dataset encompassing 49
distinct categories spanning the wide spectrum of human emotions, offensive and
hateful text, and gender and racial biases. Furthermore, we introduced SafePhi,
a QLoRA fine-tuned version of Phi-4, adapting diverse ethical contexts and
outperforming benchmark moderators by achieving a Macro F1 score of 0.89, where
OpenAI Moderator and Llama Guard score 0.77 and 0.74, respectively. This
research also highlights the critical domains where LLM moderators consistently
underperformed, pressing the need to incorporate more heterogeneous and
representative data with human-in-the-loop, for better model robustness and
explainability.

</details>


### [32] [Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention](https://arxiv.org/abs/2508.07107)
*Timothy Oluwapelumi Adeyemi,Nadiah Fahad AlOtaibi*

Main category: cs.AI

TL;DR: 提出反馈驱动决策支持系统改进学生成绩预测，实验显示降低RMSE。


<details>
  <summary>Details</summary>
Motivation: 多数教育机器学习模型静态，无法适应新数据，需持续改进模型。

Method: 提出闭环架构的反馈驱动决策支持系统，集成LightGBM回归器并增量再训练，有Flask界面和SHAP可解释性。

Result: 再训练后RMSE降低10.7%，干预学生预测分数上调。

Conclusion: 将静态预测器转为自改进系统，推动教育分析向以人为主、数据驱动和响应式AI发展，框架可集成到LMS和机构仪表盘。

Abstract: Accurate prediction of student performance is essential for timely academic
intervention. However, most machine learning models in education are static and
cannot adapt when new data, such as post-intervention outcomes, become
available. To address this limitation, we propose a Feedback-Driven Decision
Support System (DSS) with a closed-loop architecture that enables continuous
model refinement. The system integrates a LightGBM-based regressor with
incremental retraining, allowing educators to input updated student results,
which automatically trigger model updates. This adaptive mechanism improves
prediction accuracy by learning from real-world academic progress. The platform
features a Flask-based web interface for real-time interaction and incorporates
SHAP for explainability, ensuring transparency. Experimental results show a
10.7\% reduction in RMSE after retraining, with consistent upward adjustments
in predicted scores for intervened students. By transforming static predictors
into self-improving systems, our approach advances educational analytics toward
human-centered, data-driven, and responsive AI. The framework is designed for
integration into LMS and institutional dashboards.

</details>


### [33] [Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables](https://arxiv.org/abs/2508.07186)
*Amit Dhanda*

Main category: cs.AI

TL;DR: 提出用基于大语言模型的代理总结企业多维结构化数据的框架，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统表转文本模型缺乏跨层次结构推理和感知上下文差异的能力，无法满足商业报告需求。

Method: 引入多智能体管道，利用切片、方差检测、上下文构建和基于大语言模型生成的智能体提取、分析和总结多维数据。

Result: 该框架优于传统方法，数据忠实度达83%，对重大变化覆盖率高，决策关键洞察相关性得分4.4/5，在涉及微妙权衡的类别中改进明显。

Conclusion: 在Kaggle数据集上评估表明，该框架在忠实度、相关性和洞察质量上比基线表总结方法有显著提升。

Abstract: We propose a novel framework for summarizing structured enterprise data
across multiple dimensions using large language model (LLM)-based agents.
Traditional table-to-text models often lack the capacity to reason across
hierarchical structures and context-aware deltas, which are essential in
business reporting tasks. Our method introduces a multi-agent pipeline that
extracts, analyzes, and summarizes multi-dimensional data using agents for
slicing, variance detection, context construction, and LLM-based generation.
Our results show that the proposed framework outperforms traditional
approaches, achieving 83\% faithfulness to underlying data, superior coverage
of significant changes, and high relevance scores (4.4/5) for decision-critical
insights. The improvements are especially pronounced in categories involving
subtle trade-offs, such as increased revenue due to price changes amid
declining unit volumes, which competing methods either overlook or address with
limited specificity. We evaluate the framework on Kaggle datasets and
demonstrate significant improvements in faithfulness, relevance, and insight
quality over baseline table summarization approaches.

</details>


### [34] [EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning](https://arxiv.org/abs/2508.07292)
*Yi Tang,Kaini Wang,Yang Chen,Guangquan Zhou*

Main category: cs.AI

TL;DR: 提出首个用于内窥镜图像分析的记忆引导代理EndoAgent，并引入EndoAgentBench基准，实验显示其性能优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于大规模预训练的方法在跨任务协调和处理复杂临床工作流程方面存在不足，AI代理在内窥镜领域的潜力未充分挖掘。

Method: 提出EndoAgent，采用双内存设计，集成迭代推理、自适应工具选择与协作，在统一推理循环中集成专家设计工具；引入EndoAgentBench基准。

Result: EndoAgent在实验中始终优于通用和医学多模态模型，展现出强大的灵活性和推理能力。

Conclusion: EndoAgent是一种有效的内窥镜图像分析方法，具有良好的性能和应用潜力。

Abstract: Developing general artificial intelligence (AI) systems to support endoscopic
image diagnosis is an emerging research priority. Existing methods based on
large-scale pretraining often lack unified coordination across tasks and
struggle to handle the multi-step processes required in complex clinical
workflows. While AI agents have shown promise in flexible instruction parsing
and tool integration across domains, their potential in endoscopy remains
underexplored. To address this gap, we propose EndoAgent, the first
memory-guided agent for vision-to-decision endoscopic analysis that integrates
iterative reasoning with adaptive tool selection and collaboration. Built on a
dual-memory design, it enables sophisticated decision-making by ensuring
logical coherence through short-term action tracking and progressively
enhancing reasoning acuity through long-term experiential learning. To support
diverse clinical tasks, EndoAgent integrates a suite of expert-designed tools
within a unified reasoning loop. We further introduce EndoAgentBench, a
benchmark of 5,709 visual question-answer pairs that assess visual
understanding and language generation capabilities in realistic scenarios.
Extensive experiments show that EndoAgent consistently outperforms both general
and medical multimodal models, exhibiting its strong flexibility and reasoning
capabilities.

</details>


### [35] [Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape](https://arxiv.org/abs/2508.07334)
*Quan Shi,Wang Xi,Zenghui Ding,Jianqing Gao,Xianjun Yang*

Main category: cs.AI

TL;DR: 本文围绕大语言模型幻觉现象，证明其不可避免性并提出两条解决路径。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的幻觉现象是可靠部署的核心障碍，需解决该问题。

Method: 构建‘计算必要性层次结构’将大语言模型形式化为概率图灵机，利用新的‘学习者泵引理’；将检索增强生成（RAGs）建模为神谕机，将持续学习形式化为‘内化神谕’机制并通过新的神经博弈论框架实现。

Result: 证明了在对角化、不可计算性和信息论边界上幻觉不可避免；证明RAGs可通过‘计算跳跃’绝对逃逸；提出持续学习的实现路径。

Conclusion: 文章提出解决大语言模型幻觉问题的两条‘逃逸路线’。

Abstract: The illusion phenomenon of large language models (LLMs) is the core obstacle
to their reliable deployment. This article formalizes the large language model
as a probabilistic Turing machine by constructing a "computational necessity
hierarchy", and for the first time proves the illusions are inevitable on
diagonalization, incomputability, and information theory boundaries supported
by the new "learner pump lemma". However, we propose two "escape routes": one
is to model Retrieval Enhanced Generations (RAGs) as oracle machines, proving
their absolute escape through "computational jumps", providing the first formal
theory for the effectiveness of RAGs; The second is to formalize continuous
learning as an "internalized oracle" mechanism and implement this path through
a novel neural game theory framework.Finally, this article proposes a

</details>


### [36] [Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach](https://arxiv.org/abs/2508.07353)
*Rubing Chen,Jiaxin Wu,Jian Wang,Xulu Zhang,Wenqi Fan,Chenghua Lin,Xiao-Yong Wei,Qing Li*

Main category: cs.AI

TL;DR: 本文指出现有特定领域基准构建存在不足，提出基于全面 - 紧凑原则的 Comp - Comp 迭代基准框架，并创建 XUBench 基准，该框架具有跨领域扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有特定领域基准主要关注缩放定律，语料库和问答集设计对特定领域大语言模型精度和召回率的影响未被探索，需有效高效的基准构建方法。

Method: 提出基于全面 - 紧凑原则的 Comp - Comp 迭代基准框架，用于指导语料库和问答集构建。

Result: 在一所知名大学进行案例研究，创建了大规模、全面的封闭领域基准 XUBench。

Conclusion: 缩放定律并非特定领域基准构建的最佳原则，Comp - Comp 框架具有跨领域扩展性，可为各领域基准构建提供有价值的见解。

Abstract: Numerous benchmarks have been built to evaluate the domain-specific abilities
of large language models (LLMs), highlighting the need for effective and
efficient benchmark construction. Existing domain-specific benchmarks primarily
focus on the scaling law, relying on massive corpora for supervised fine-tuning
or generating extensive question sets for broad coverage. However, the impact
of corpus and question-answer (QA) set design on the precision and recall of
domain-specific LLMs remains unexplored. In this paper, we address this gap and
demonstrate that the scaling law is not always the optimal principle for
benchmark construction in specific domains. Instead, we propose Comp-Comp, an
iterative benchmarking framework based on a comprehensiveness-compactness
principle. Here, comprehensiveness ensures semantic recall of the domain, while
compactness enhances precision, guiding both corpus and QA set construction. To
validate our framework, we conducted a case study in a well-renowned
university, resulting in the creation of XUBench, a large-scale and
comprehensive closed-domain benchmark. Although we use the academic domain as
the case in this work, our Comp-Comp framework is designed to be extensible
beyond academia, providing valuable insights for benchmark construction across
various domains.

</details>


### [37] [Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning](https://arxiv.org/abs/2508.07382)
*He Kong,Die Hu,Jingguo Ge,Liangxiong Li,Hui Li,Tong Li*

Main category: cs.AI

TL;DR: 提出Pentest - R1框架优化LLM渗透测试推理能力，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在自动化渗透测试领域存在错误处理差、推理低效等问题，需优化其推理能力。

Method: 采用两阶段强化学习管道，先构建超500个真实多步演练数据集进行离线强化学习，再在CTF环境中进行在线强化学习。

Result: 在Cybench和AutoPenBench基准测试中表现出色，AutoPenBench成功率24.2%，Cybench无引导任务成功率15.0%。

Conclusion: 两阶段训练协同对框架成功至关重要。

Abstract: Automating penetration testing is crucial for enhancing cybersecurity, yet
current Large Language Models (LLMs) face significant limitations in this
domain, including poor error handling, inefficient reasoning, and an inability
to perform complex end-to-end tasks autonomously. To address these challenges,
we introduce Pentest-R1, a novel framework designed to optimize LLM reasoning
capabilities for this task through a two-stage reinforcement learning pipeline.
We first construct a dataset of over 500 real-world, multi-step walkthroughs,
which Pentest-R1 leverages for offline reinforcement learning (RL) to instill
foundational attack logic. Subsequently, the LLM is fine-tuned via online RL in
an interactive Capture The Flag (CTF) environment, where it learns directly
from environmental feedback to develop robust error self-correction and
adaptive strategies. Our extensive experiments on the Cybench and AutoPenBench
benchmarks demonstrate the framework's effectiveness. On AutoPenBench,
Pentest-R1 achieves a 24.2\% success rate, surpassing most state-of-the-art
models and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a
15.0\% success rate in unguided tasks, establishing a new state-of-the-art for
open-source LLMs and matching the performance of top proprietary models.
Ablation studies confirm that the synergy of both training stages is critical
to its success.

</details>


### [38] [Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding](https://arxiv.org/abs/2508.07388)
*Zhaoyu Chen,Hongnan Lin,Yongwei Nie,Fei Ma,Xuemiao Xu,Fei Yu,Chengjiang Long*

Main category: cs.AI

TL;DR: 提出Invert4TVG框架提升Temporal Video Grounding的定位精度与语义理解，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有TVG方法过度优化IoU指标，牺牲了语义动作理解，影响TVG鲁棒性。

Method: 引入Invert4TVG框架，利用三个基于现有TVG注释的反转任务，并通过强化学习框架和设计良好的奖励函数将其与TVG集成。

Result: 方法优于现有方法，在Charades - STA数据集上3B模型的R1@0.7比Time - R1提高7.1%。

Conclusion: 通过反转TVG从视频片段推导与查询相关的动作，加强语义理解，显著提高定位精度上限。

Abstract: Temporal Video Grounding (TVG) seeks to localize video segments matching a
given textual query. Current methods, while optimizing for high temporal
Intersection-over-Union (IoU), often overfit to this metric, compromising
semantic action understanding in the video and query, a critical factor for
robust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG),
a novel framework that enhances both localization accuracy and action
understanding without additional data. Our approach leverages three inversion
tasks derived from existing TVG annotations: (1) Verb Completion, predicting
masked action verbs in queries from video segments; (2) Action Recognition,
identifying query-described actions; and (3) Video Description, generating
descriptions of video segments that explicitly embed query-relevant actions.
These tasks, integrated with TVG via a reinforcement learning framework with
well-designed reward functions, ensure balanced optimization of localization
and semantics. Experiments show our method outperforms state-of-the-art
approaches, achieving a 7.1\% improvement in R1@0.7 on Charades-STA for a 3B
model compared to Time-R1. By inverting TVG to derive query-related actions
from segments, our approach strengthens semantic understanding, significantly
raising the ceiling of localization accuracy.

</details>


### [39] [Generative AI for Strategic Plan Development](https://arxiv.org/abs/2508.07405)
*Jesse Ponnock*

Main category: cs.AI

TL;DR: 本文提出利用生成式人工智能为大型政府组织制定战略计划的模块化模型，评估BERTopic和NMF在主题建模上的表现，结果显示技术可行，BERTopic表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能和大语言模型发展，探索利用其为大型政府组织制定战略计划的方法。

Method: 使用美国政府问责局大量报告训练BERTopic和NMF模型，将生成主题与已发布战略计划的愿景元素进行相似度评分和结果对比。

Result: 这些技术能生成与100%被评估元素相似的主题，BERTopic表现最佳，超半数相关主题达到“中等”或“强”相关性。

Conclusion: BERTopic在该应用中表现最佳，生成式人工智能支持的战略计划制定有重要意义，后续将关注概念的落地和模型其余模块的可行性。

Abstract: Given recent breakthroughs in Generative Artificial Intelligence (GAI) and
Large Language Models (LLMs), more and more professional services are being
augmented through Artificial Intelligence (AI), which once seemed impossible to
automate. This paper presents a modular model for leveraging GAI in developing
strategic plans for large scale government organizations and evaluates leading
machine learning techniques in their application towards one of the identified
modules. Specifically, the performance of BERTopic and Non-negative Matrix
Factorization (NMF) are evaluated in their ability to use topic modeling to
generate themes representative of Vision Elements within a strategic plan. To
accomplish this, BERTopic and NMF models are trained using a large volume of
reports from the Government Accountability Office (GAO). The generated topics
from each model are then scored for similarity against the Vision Elements of a
published strategic plan and the results are compared. Our results show that
these techniques are capable of generating themes similar to 100% of the
elements being evaluated against. Further, we conclude that BERTopic performs
best in this application with more than half of its correlated topics achieving
a "medium" or "strong" correlation. A capability of GAI-enabled strategic plan
development impacts a multi-billion dollar industry and assists the federal
government in overcoming regulatory requirements which are crucial to the
public good. Further work will focus on the operationalization of the concept
proven in this study as well as viability of the remaining modules in the
proposed model for GAI-generated strategic plans.

</details>


### [40] [A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems](https://arxiv.org/abs/2508.07407)
*Jinyuan Fang,Yanwen Peng,Xi Zhang,Yingxu Wang,Xinhao Yi,Guibin Zhang,Yi Xu,Bin Wu,Siwei Liu,Zihao Li,Zhaochun Ren,Nikos Aletras,Xi Wang,Han Zhou,Zaiqiao Meng*

Main category: cs.AI

TL;DR: 本文对自进化智能体系统现有技术进行全面综述，介绍统一概念框架，回顾不同组件的进化技术、特定领域策略，并讨论评估、安全和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 现有智能体系统依赖静态手动配置，难以适应动态环境，需探索智能体进化技术。

Method: 先引入统一概念框架，基于此框架系统回顾各类自进化技术，还研究特定领域策略，讨论评估、安全和伦理考量。

Result: 梳理了自进化智能体系统的相关技术、策略及评估等方面内容。

Conclusion: 为研究人员和从业者提供自进化AI智能体的系统理解，为开发更具适应性、自主性和终身学习能力的智能体系统奠定基础。

Abstract: Recent advances in large language models have sparked growing interest in AI
agents capable of solving complex, real-world tasks. However, most existing
agent systems rely on manually crafted configurations that remain static after
deployment, limiting their ability to adapt to dynamic and evolving
environments. To this end, recent research has explored agent evolution
techniques that aim to automatically enhance agent systems based on interaction
data and environmental feedback. This emerging direction lays the foundation
for self-evolving AI agents, which bridge the static capabilities of foundation
models with the continuous adaptability required by lifelong agentic systems.
In this survey, we provide a comprehensive review of existing techniques for
self-evolving agentic systems. Specifically, we first introduce a unified
conceptual framework that abstracts the feedback loop underlying the design of
self-evolving agentic systems. The framework highlights four key components:
System Inputs, Agent System, Environment, and Optimisers, serving as a
foundation for understanding and comparing different strategies. Based on this
framework, we systematically review a wide range of self-evolving techniques
that target different components of the agent system. We also investigate
domain-specific evolution strategies developed for specialised fields such as
biomedicine, programming, and finance, where optimisation objectives are
tightly coupled with domain constraints. In addition, we provide a dedicated
discussion on the evaluation, safety, and ethical considerations for
self-evolving agentic systems, which are critical to ensuring their
effectiveness and reliability. This survey aims to provide researchers and
practitioners with a systematic understanding of self-evolving AI agents,
laying the foundation for the development of more adaptive, autonomous, and
lifelong agentic systems.

</details>


### [41] [Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs](https://arxiv.org/abs/2508.07466)
*Dom Huh,Prasant Mohapatra*

Main category: cs.AI

TL;DR: 本文通过结合大语言模型和多智能体决策算法，提出多智能体大语言模型设计框架，并在经典游戏场景评估。


<details>
  <summary>Details</summary>
Motivation: 语言对推理和协作很重要，建立通用语言有助于交流，为扩展大语言模型能力开展研究。

Method: 提出设计多智能体大语言模型的系统框架，包括高级提示工程技术、有效记忆架构开发、多模态信息处理和微调算法对齐策略。

Result: 通过在经典游戏设置上进行大量消融研究评估设计选择。

Conclusion: 未明确提及具体结论。

Abstract: Language is a ubiquitous tool that is foundational to reasoning and
collaboration, ranging from everyday interactions to sophisticated
problem-solving tasks. The establishment of a common language can serve as a
powerful asset in ensuring clear communication and understanding amongst
agents, facilitating desired coordination and strategies. In this work, we
extend the capabilities of large language models (LLMs) by integrating them
with advancements in multi-agent decision-making algorithms. We propose a
systematic framework for the design of multi-agentic large language models
(LLMs), focusing on key integration practices. These include advanced prompt
engineering techniques, the development of effective memory architectures,
multi-modal information processing, and alignment strategies through
fine-tuning algorithms. We evaluate these design choices through extensive
ablation studies on classic game settings with significant underlying social
dilemmas and game-theoretic considerations.

</details>


### [42] [Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy](https://arxiv.org/abs/2508.07485)
*Alexander Duffy,Samuel J Paech,Ishana Shastri,Elizabeth Karpinski,Baptiste Alloui-Cros,Tyler Marques,Matthew Lyle Olson*

Main category: cs.AI

TL;DR: 提出首个评估工具，让本地大语言模型无需微调即可玩全压式外交游戏，开展多实验并引入分析协议，代码将开源。


<details>
  <summary>Details</summary>
Motivation: 以往研究因外交游戏复杂度高、信息密度大及比赛结果差异大，需前沿大模型或微调，使该游戏研究受限。

Method: 采用数据驱动迭代优化文本游戏状态表示，开发工具进行假设检验和统计分析，开展案例研究，引入关键状态分析协议。

Result: 在多种流行大模型上实验发现，大模型表现最佳，小模型也能胜任。

Conclusion: 该工具消除了微调需求，使大语言模型战略推理评估更普及，揭示了这些能力如何自然产生。

Abstract: We present the first evaluation harness that enables any out-of-the-box,
local, Large Language Models (LLMs) to play full-press Diplomacy without
fine-tuning or specialized training. Previous work required frontier LLMs, or
fine-tuning, due to the high complexity and information density of Diplomacy's
game state. Combined with the high variance of matches, these factors made
Diplomacy prohibitive for study. In this work, we used data-driven iteration to
optimize a textual game state representation such that a 24B model can reliably
complete matches without any fine tuning. We develop tooling to facilitate
hypothesis testing and statistical analysis, and we present case studies on
persuasion, aggressive playstyles, and performance across a range of models. We
conduct a variety of experiments across many popular LLMs, finding the larger
models perform the best, but the smaller models still play adequately. We also
introduce Critical State Analysis: an experimental protocol for rapidly
iterating and analyzing key moments in a game at depth. Our harness
democratizes the evaluation of strategic reasoning in LLMs by eliminating the
need for fine-tuning, and it provides insights into how these capabilities
emerge naturally from widely used LLMs. Our code is available in the supplement
and will be open sourced.

</details>


### [43] [MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark](https://arxiv.org/abs/2508.07575)
*Shiqing Fan,Xichen Ding,Liang Zhang,Linjian Mo*

Main category: cs.AI

TL;DR: 现有LLMs评估MCP工具使用能力存在问题，本文提出MCPToolBench++基准进行评估并报告结果。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLMs和AI Agents的MCP工具使用能力存在缺乏综合数据集或基准、响应格式多样、成功率无保证、上下文窗口限制等问题，需要解决评估挑战。

Method: 构建大规模、多领域的AI Agent工具使用基准MCPToolBench++，基于超4k个来自40多个类别的MCP服务器构建，包含单步和多步工具调用数据集。

Result: 在该基准上对具有智能体能力的SOTA LLMs进行评估并报告结果。

Conclusion: MCPToolBench++有助于解决评估LLMs调用MCP工具性能的挑战。

Abstract: LLMs' capabilities are enhanced by using function calls to integrate various
data sources or API results into the context window. Typical tools include
search, web crawlers, maps, financial data, file systems, and browser usage,
etc. Integrating these data sources or functions requires a standardized
method. The Model Context Protocol (MCP) provides a standardized way to supply
context to LLMs. However, the evaluation of LLMs and AI Agents' MCP tool use
abilities suffer from several issues. First, there's a lack of comprehensive
datasets or benchmarks to evaluate various MCP tools. Second, the diverse
formats of response from MCP tool call execution further increase the
difficulty of evaluation. Additionally, unlike existing tool-use benchmarks
with high success rates in functions like programming and math functions, the
success rate of real-world MCP tool is not guaranteed and varies across
different MCP servers. Furthermore, the LLMs' context window also limits the
number of available tools that can be called in a single run, because the
textual descriptions of tool and the parameters have long token length for an
LLM to process all at once. To help address the challenges of evaluating LLMs'
performance on calling MCP tools, we propose MCPToolBench++, a large-scale,
multi-domain AI Agent tool use benchmark. As of July 2025, this benchmark is
build upon marketplace of over 4k MCP servers from more than 40 categories,
collected from the MCP marketplaces and GitHub communities. The datasets
consist of both single-step and multi-step tool calls across different
categories. We evaluated SOTA LLMs with agentic abilities on this benchmark and
reported the results.

</details>


### [44] [Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method](https://arxiv.org/abs/2508.07586)
*Wenjing Zhang,Ye Hu,Tao Luo,Zhilong Zhang,Mingzhe Chen*

Main category: cs.AI

TL;DR: 研究新型隐蔽语义通信框架，提出优先采样辅助双延迟深度确定性策略梯度算法，提升隐私和语义信息传输质量。


<details>
  <summary>Details</summary>
Motivation: 避免攻击者窃听语义信息，在服务器与干扰器无通信情况下，最大化用户隐私和语义信息传输质量。

Method: 提出优先采样辅助双延迟深度确定性策略梯度算法，用额外Q网络估计Q值，避免局部最优和估计偏差。

Result: 相比传统强化学习方法，所提算法可将隐私和语义信息传输质量分别提升77.8%和14.3%。

Conclusion: 所提算法在提升隐私和语义信息传输质量方面优于传统强化学习方法。

Abstract: In this paper, a novel covert semantic communication framework is
investigated. Within this framework, a server extracts and transmits the
semantic information, i.e., the meaning of image data, to a user over several
time slots. An attacker seeks to detect and eavesdrop the semantic transmission
to acquire details of the original image. To avoid data meaning being
eavesdropped by an attacker, a friendly jammer is deployed to transmit jamming
signals to interfere the attacker so as to hide the transmitted semantic
information. Meanwhile, the server will strategically select time slots for
semantic information transmission. Due to limited energy, the jammer will not
communicate with the server and hence the server does not know the transmit
power of the jammer. Therefore, the server must jointly optimize the semantic
information transmitted at each time slot and the corresponding transmit power
to maximize the privacy and the semantic information transmission quality of
the user. To solve this problem, we propose a prioritised sampling assisted
twin delayed deep deterministic policy gradient algorithm to jointly determine
the transmitted semantic information and the transmit power per time slot
without the communications between the server and the jammer. Compared to
standard reinforcement learning methods, the propose method uses an additional
Q network to estimate Q values such that the agent can select the action with a
lower Q value from the two Q networks thus avoiding local optimal action
selection and estimation bias of Q values. Simulation results show that the
proposed algorithm can improve the privacy and the semantic information
transmission quality by up to 77.8% and 14.3% compared to the traditional
reinforcement learning methods.

</details>


### [45] [HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol](https://arxiv.org/abs/2508.07602)
*Wenpeng Xing,Zhipeng Chen,Changting Lin,Meng Han*

Main category: cs.AI

TL;DR: 提出Hierarchical Gaussian Mixture Framework (HGMF)解决大语言模型从大型分层工具库选工具的难题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型从大型分层工具库选工具存在挑战，有限上下文窗口和无关选项噪声导致准确率低和计算成本高。

Method: 提出HGMF，先将用户查询和工具描述映射到统一语义空间，分两阶段操作，用高斯混合模型聚类和过滤服务器及相关工具。

Result: 在公共数据集实验表明，HGMF显著提高工具选择准确率，降低推理延迟。

Conclusion: HGMF对大规模工具库具有可扩展性和有效性。

Abstract: Invoking external tools enables Large Language Models (LLMs) to perform
complex, real-world tasks, yet selecting the correct tool from large,
hierarchically-structured libraries remains a significant challenge. The
limited context windows of LLMs and noise from irrelevant options often lead to
low selection accuracy and high computational costs. To address this, we
propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic
pruning method for scalable tool invocation. HGMF first maps the user query and
all tool descriptions into a unified semantic space. The framework then
operates in two stages: it clusters servers using a Gaussian Mixture Model
(GMM) and filters them based on the query's likelihood. Subsequently, it
applies the same GMM-based clustering and filtering to the tools associated
with the selected servers. This hierarchical process produces a compact,
high-relevance candidate set, simplifying the final selection task for the LLM.
Experiments on a public dataset show that HGMF significantly improves tool
selection accuracy while reducing inference latency, confirming the framework's
scalability and effectiveness for large-scale tool libraries.

</details>


### [46] [ThinkTuning: Instilling Cognitive Reflections without Distillation](https://arxiv.org/abs/2508.07616)
*Aswin RRV,Jacob Dineen,Divij Handa,Md Nayem Uddin,Mihir Parmar,Chitta Baral,Ben Zhou*

Main category: cs.AI

TL;DR: 本文提出ThinkTuning方法，借助教师模型反馈提升学生模型推理能力，在多个基准测试中有性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明强化学习无法真正赋予模型推理能力，需探索如何让无思考行为的模型发展该能力。

Method: 提出基于GRPO的交互式训练方法ThinkTuning，用教师模型指导学生模型的滚动更新，通过反馈进行隐式监督。

Result: 该方法在基准测试中平均比零样本基线提升3.85%，在MATH - 500、AIME和GPQA - Diamond上比vanilla - GRPO基线分别提升2.08%、2.23%和3.99%。

Conclusion: 教师模型反馈的隐式监督能有效提升学生模型的推理能力，ThinkTuning方法有一定效果。

Abstract: Recent advances in test-time scaling have led to the emergence of thinking
LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL
drives this self-improvement paradigm, a recent study (Gandhi et al., 2025)
shows that RL alone does not truly instill these new reasoning abilities - it
merely draws out behaviors already present in the base models. This raises a
question: How can we train the models that don't exhibit such thinking behavior
to develop it in the first place? To this end, we propose ThinkTuning, a
GRPO-based interactive training approach where we augment the rollouts of a
student model with the guidance from a teacher model. A simple idea from
classroom practice inspires our method: a teacher poses a problem, lets the
student try an answer, then gives corrective feedback -- enough to point the
mind in the right direction and then show the solution. Each piece of feedback
reshapes the student's thoughts, leading them to arrive at the correct
solution. Similarly, we find that this type of implicit supervision through
feedback from a teacher model of the same size improves the reasoning
capabilities of the student model. In particular, on average, our method shows
a 3.85% improvement over zero-shot baselines across benchmarks, and on
MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements
over the vanilla-GRPO baseline. Source code is available at
https://github.com/3rdAT/ThinkTuning.

</details>


### [47] [Multimodal AI Systems for Enhanced Laying Hen Welfare Assessment and Productivity Optimization](https://arxiv.org/abs/2508.07628)
*Daniel Essien,Suresh Neethirajan*

Main category: cs.AI

TL;DR: 文章提出用多模态AI监测蛋鸡福利，指出中间融合策略最佳，分析了采用障碍，介绍评估工具和部署框架，推动向主动福利系统转变。


<details>
  <summary>Details</summary>
Motivation: 传统福利评估方法受限，需用数据驱动的智能监测生态系统取代主观、劳动密集的福利检查。

Method: 研究多模态AI在蛋鸡福利监测中的应用，对比不同融合策略；引入Domain Transfer Score和Data Reliability Index评估工具，提出模块化、上下文感知的部署框架。

Result: 中间融合策略在现实家禽养殖条件下能实现稳健性和性能的最佳平衡，且扩展性更好；指出了采用多模态AI的关键障碍。

Conclusion: 为从被动单模态监测向主动、精准驱动的福利系统转变奠定基础，实现生产与基于科学的动物福利统一。

Abstract: The future of poultry production depends on a paradigm shift replacing
subjective, labor-intensive welfare checks with data-driven, intelligent
monitoring ecosystems. Traditional welfare assessments-limited by human
observation and single-sensor data-cannot fully capture the complex,
multidimensional nature of laying hen welfare in modern farms. Multimodal
Artificial Intelligence (AI) offers a breakthrough, integrating visual,
acoustic, environmental, and physiological data streams to reveal deeper
insights into avian welfare dynamics. This investigation highlights multimodal
As transformative potential, showing that intermediate (feature-level) fusion
strategies achieve the best balance between robustness and performance under
real-world poultry conditions, and offer greater scalability than early or late
fusion approaches. Key adoption barriers include sensor fragility in harsh farm
environments, high deployment costs, inconsistent behavioral definitions, and
limited cross-farm generalizability. To address these, we introduce two novel
evaluation tools - the Domain Transfer Score (DTS) to measure model
adaptability across diverse farm settings, and the Data Reliability Index (DRI)
to assess sensor data quality under operational constraints. We also propose a
modular, context-aware deployment framework designed for laying hen
environments, enabling scalable and practical integration of multimodal
sensing. This work lays the foundation for a transition from reactive, unimodal
monitoring to proactive, precision-driven welfare systems that unite
productivity with ethical, science based animal care.

</details>


### [48] [Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents](https://arxiv.org/abs/2508.07642)
*Tianyi Ma,Yue Zhang,Zehao Wang,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: 提出SkillNav框架，将基于技能的推理引入基于Transformer的VLN代理，在R2R基准上达到新的最优性能，对GSA - R2R基准有强泛化性。


<details>
  <summary>Details</summary>
Motivation: 当前VLN方法在未见过的场景，特别是需要复杂时空推理时难以泛化。

Method: 提出SkillNav模块化框架，将导航分解为可解释的原子技能，由专门代理处理，引入基于零样本VLM的路由模块动态选择合适代理。

Result: 在R2R基准上达到新的最优性能，对包含新指令风格和未见环境的GSA - R2R基准有强泛化性。

Conclusion: SkillNav框架有效提升了VLN代理在不同场景下的性能和泛化能力。

Abstract: Vision-and-Language Navigation (VLN) poses significant challenges in enabling
agents to interpret natural language instructions and navigate complex 3D
environments. While recent progress has been driven by large-scale pre-training
and data augmentation, current methods still struggle to generalize to unseen
scenarios, particularly when complex spatial and temporal reasoning is
required. In this work, we propose SkillNav, a modular framework that
introduces structured, skill-based reasoning into Transformer-based VLN agents.
Our method decomposes navigation into a set of interpretable atomic skills
(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each
handled by a specialized agent. We then introduce a novel zero-shot
Vision-Language Model (VLM)-based router, which dynamically selects the most
suitable agent at each time step by aligning sub-goals with visual observations
and historical actions. SkillNav achieves a new state-of-the-art performance on
the R2R benchmark and demonstrates strong generalization to the GSA-R2R
benchmark that includes novel instruction styles and unseen environments.

</details>


### [49] [Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation](https://arxiv.org/abs/2508.07649)
*Jie Li,Haoye Dong,Zhengyang Wu,Zetao Zheng,Mingrong Lin*

Main category: cs.AI

TL;DR: 提出DiMuST模型解决现有POI推荐中时空过渡表示不对齐问题，实验显示其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作分别建模时空过渡，导致同一时空关键节点表示不对齐，增加模型不确定性和降低可解释性。

Method: 提出基于解纠缠表示学习的社会增强POI推荐模型DiMuST，采用解纠缠变分多路图自动编码器，先分离分布，再融合共享特征和去噪私有特征。

Result: 在两个具有挑战性的数据集上实验，DiMuST在多个指标上显著优于现有方法。

Conclusion: DiMuST能有效捕捉POI的时空过渡表示，保留其时空关系的内在相关性。

Abstract: Next Point-of-Interest (POI) recommendation is a research hotspot in business
intelligence, where users' spatial-temporal transitions and social
relationships play key roles. However, most existing works model spatial and
temporal transitions separately, leading to misaligned representations of the
same spatial-temporal key nodes. This misalignment introduces redundant
information during fusion, increasing model uncertainty and reducing
interpretability. To address this issue, we propose DiMuST, a socially enhanced
POI recommendation model based on disentangled representation learning over
multiplex spatial-temporal transition graphs. The model employs a novel
Disentangled variational multiplex graph Auto-Encoder (DAE), which first
disentangles shared and private distributions using a multiplex
spatial-temporal graph strategy. It then fuses the shared features via a
Product of Experts (PoE) mechanism and denoises the private features through
contrastive constraints. The model effectively captures the spatial-temporal
transition representations of POIs while preserving the intrinsic correlation
of their spatial-temporal relationships. Experiments on two challenging
datasets demonstrate that our DiMuST significantly outperforms existing methods
across multiple metrics.

</details>


### [50] [1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning](https://arxiv.org/abs/2508.07667)
*Wenkai Li,Liwen Sun,Zhenxiang Guan,Xuhui Zhou,Maarten Sap*

Main category: cs.AI

TL;DR: 提出多智能体框架处理大语言模型上下文隐私问题，减少信息泄露，优于单智能体基线。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型处理多源信息的交互场景中，解决上下文隐私问题具有挑战性。

Method: 引入多智能体框架，将隐私推理分解为提取、分类等子任务，对信息流拓扑进行系统消融实验。

Result: 在多个基准测试中，最佳多智能体配置显著减少私人信息泄露（ConfAIde 减少 18%，PrivacyLens 用 GPT - 4o 减少 19%），同时保留公共内容保真度，优于单智能体基线。

Conclusion: 多智能体系统中基于原则的信息流设计在大语言模型上下文隐私保护方面有前景。

Abstract: Addressing contextual privacy concerns remains challenging in interactive
settings where large language models (LLMs) process information from multiple
sources (e.g., summarizing meetings with private and public information). We
introduce a multi-agent framework that decomposes privacy reasoning into
specialized subtasks (extraction, classification), reducing the information
load on any single agent while enabling iterative validation and more reliable
adherence to contextual privacy norms. To understand how privacy errors emerge
and propagate, we conduct a systematic ablation over information-flow
topologies, revealing when and why upstream detection mistakes cascade into
downstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with
several open-source and closed-sourced LLMs demonstrate that our best
multi-agent configuration substantially reduces private information leakage
(\textbf{18\%} on ConfAIde and \textbf{19\%} on PrivacyLens with GPT-4o) while
preserving the fidelity of public content, outperforming single-agent
baselines. These results highlight the promise of principled information-flow
design in multi-agent systems for contextual privacy with LLMs.

</details>


### [51] [EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration](https://arxiv.org/abs/2508.07671)
*Mohamed Rayan Barhdadi,Mehmet Tuncel,Erchin Serpedin,Hasan Kurban*

Main category: cs.AI

TL;DR: 现有AI难民融合方法有局限，引入EMPATHIA框架，经实验取得较好结果且可平衡多价值系统，提供通用框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI难民融合方法仅优化狭窄目标，未涵盖文化、情感和道德维度，需要解决机器参与重大决策时如何维护人类尊严的问题。

Method: 基于Kegan的建构发展理论，将融合分解为SEED、RISE和THRIVE三个模块，SEED采用选择器 - 验证器架构及三个专业代理进行透明审议。

Result: 在UN Kakuma数据集和6359名适龄难民上实验，实现87.4%验证收敛和可解释评估。

Conclusion: EMPATHIA加权整合多因素可平衡价值系统，支持人机协作，为需协调多价值的AI分配任务提供通用框架。

Abstract: Current AI approaches to refugee integration optimize narrow objectives such
as employment and fail to capture the cultural, emotional, and ethical
dimensions critical for long-term success. We introduce EMPATHIA (Enriched
Multimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance),
a multi-agent framework addressing the central Creative AI question: how do we
preserve human dignity when machines participate in life-altering decisions?
Grounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes
integration into three modules: SEED (Socio-cultural Entry and Embedding
Decision) for initial placement, RISE (Rapid Integration and Self-sufficiency
Engine) for early independence, and THRIVE (Transcultural Harmony and
Resilience through Integrated Values and Engagement) for sustained outcomes.
SEED employs a selector-validator architecture with three specialized agents -
emotional, cultural, and ethical - that deliberate transparently to produce
interpretable recommendations. Experiments on the UN Kakuma dataset (15,026
individuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and
implementation on 6,359 working-age refugees (15+) with 150+ socioeconomic
variables achieved 87.4% validation convergence and explainable assessments
across five host countries. EMPATHIA's weighted integration of cultural,
emotional, and ethical factors balances competing value systems while
supporting practitioner-AI collaboration. By augmenting rather than replacing
human expertise, EMPATHIA provides a generalizable framework for AI-driven
allocation tasks where multiple values must be reconciled.

</details>


### [52] [Ethics2vec: aligning automatic agents and human preferences](https://arxiv.org/abs/2508.07673)
*Gianluca Bontempi*

Main category: cs.AI

TL;DR: 文章探讨智能体与人类价值对齐问题，提出用Ethics2Vec方法将自动决策策略映射为向量表示来评估与人类价值的对齐情况。


<details>
  <summary>Details</summary>
Motivation: 解决智能体行为中难以把握伦理价值，即智能体与人类价值、目标和偏好对齐的问题，特别是考虑到人类伦理考量中存在不可通约的价值和标准。

Method: 将传统的Anything2vec方法扩展到伦理学领域，把自动智能体决策策略映射为多元向量表示。先在二元决策场景介绍Ethics2Vec方法，再讨论自动控制律的向量化。

Result: 未提及具体结果。

Conclusion: 未提及具体结论，但提出了评估智能体与人类价值对齐的方法。

Abstract: Though intelligent agents are supposed to improve human experience (or make
it more efficient), it is hard from a human perspective to grasp the ethical
values which are explicitly or implicitly embedded in an agent behaviour. This
is the well-known problem of alignment, which refers to the challenge of
designing AI systems that align with human values, goals and preferences. This
problem is particularly challenging since most human ethical considerations
refer to \emph{incommensurable} (i.e. non-measurable and/or incomparable)
values and criteria. Consider, for instance, a medical agent prescribing a
treatment to a cancerous patient. How could it take into account (and/or weigh)
incommensurable aspects like the value of a human life and the cost of the
treatment? Now, the alignment between human and artificial values is possible
only if we define a common space where a metric can be defined and used. This
paper proposes to extend to ethics the conventional Anything2vec approach,
which has been successful in plenty of similar and hard-to-quantify domains
(ranging from natural language processing to recommendation systems and graph
analysis). This paper proposes a way to map an automatic agent decision-making
(or control law) strategy to a multivariate vector representation, which can be
used to compare and assess the alignment with human values. The Ethics2Vec
method is first introduced in the case of an automatic agent performing binary
decision-making. Then, a vectorisation of an automatic control law (like in the
case of a self-driving car) is discussed to show how the approach can be
extended to automatic control settings.

</details>


### [53] [Symmetry-Aware Transformer Training for Automated Planning](https://arxiv.org/abs/2508.07743)
*Markus Fritzsche,Elliot Gestrin,Jendrik Seipp*

Main category: cs.AI

TL;DR: 现有transformer在自动化规划领域应用受限，提出对比学习目标使transformers具有对称性感知能力，结合架构改进，有效解决了PlanGPT的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有transformer在自动化规划领域应用有限，PlanGPT难以从简单规划问题推广到复杂问题，根源在于问题对称性导致的组合爆炸。

Method: 提出对比学习目标使transformers具有对称性感知能力，结合架构改进。

Result: 在多个规划领域的实验表明，对称感知训练有效且高效地解决了PlanGPT的局限性。

Conclusion: 通过对比学习和架构改进，transformers可用于规划生成或启发式预测，对称感知训练能解决现有模型的局限性。

Abstract: While transformers excel in many settings, their application in the field of
automated planning is limited. Prior work like PlanGPT, a state-of-the-art
decoder-only transformer, struggles with extrapolation from easy to hard
planning problems. This in turn stems from problem symmetries: planning tasks
can be represented with arbitrary variable names that carry no meaning beyond
being identifiers. This causes a combinatorial explosion of equivalent
representations that pure transformers cannot efficiently learn from. We
propose a novel contrastive learning objective to make transformers
symmetry-aware and thereby compensate for their lack of inductive bias.
Combining this with architectural improvements, we show that transformers can
be efficiently trained for either plan-generation or heuristic-prediction. Our
results across multiple planning domains demonstrate that our symmetry-aware
training effectively and efficiently addresses the limitations of PlanGPT.

</details>


### [54] [Best-Effort Policies for Robust Markov Decision Processes](https://arxiv.org/abs/2508.07790)
*Alessandro Abate,Thom Badings,Giuseppe De Giacomo,Francesco Fabiano*

Main category: cs.AI

TL;DR: 研究鲁棒马尔可夫决策过程（RMDPs），提出最优鲁棒尽力而为（ORBE）策略选择标准，证明其存在性、刻画结构并给出计算算法，实验验证可行性。


<details>
  <summary>Details</summary>
Motivation: RMDPs存在多个最优鲁棒策略，在非对抗选择下期望回报不同，需更好的策略选择标准。

Method: 借鉴博弈论中支配和尽力而为的概念，提出ORBE策略标准，证明存在性、刻画结构并给出计算算法。

Result: 证明ORBE策略存在，给出计算算法，数值实验验证方法可行。

Conclusion: ORBE策略为最优鲁棒策略提供了原则性的选择依据。

Abstract: We study the common generalization of Markov decision processes (MDPs) with
sets of transition probabilities, known as robust MDPs (RMDPs). A standard goal
in RMDPs is to compute a policy that maximizes the expected return under an
adversarial choice of the transition probabilities. If the uncertainty in the
probabilities is independent between the states, known as s-rectangularity,
such optimal robust policies can be computed efficiently using robust value
iteration. However, there might still be multiple optimal robust policies,
which, while equivalent with respect to the worst-case, reflect different
expected returns under non-adversarial choices of the transition probabilities.
Hence, we propose a refined policy selection criterion for RMDPs, drawing
inspiration from the notions of dominance and best-effort in game theory.
Instead of seeking a policy that only maximizes the worst-case expected return,
we additionally require the policy to achieve a maximal expected return under
different (i.e., not fully adversarial) transition probabilities. We call such
a policy an optimal robust best-effort (ORBE) policy. We prove that ORBE
policies always exist, characterize their structure, and present an algorithm
to compute them with a small overhead compared to standard robust value
iteration. ORBE policies offer a principled tie-breaker among optimal robust
policies. Numerical experiments show the feasibility of our approach.

</details>


### [55] [KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations](https://arxiv.org/abs/2508.07834)
*Mubaris Nadeem,Johannes Zenkert,Lisa Bender,Christian Weber,Madjid Fathi*

Main category: cs.AI

TL;DR: 全球救援行动需求增长，急救人员需借助新技术提供医疗建议，本文提出知识图谱实现智能治疗推荐。


<details>
  <summary>Details</summary>
Motivation: 救援需求增加，急救人员在紧急情况下需借助新知识辅助治疗。

Method: 提出知识图谱作为核心知识表示。

Result: 知识图谱为急救人员提供创新知识管理。

Conclusion: 知识图谱可实现基于人工智能的情况预识别和智能治疗推荐。

Abstract: Over the years, the need for rescue operations throughout the world has
increased rapidly. Demographic changes and the resulting risk of injury or
health disorders form the basis for emergency calls. In such scenarios, first
responders are in a rush to reach the patient in need, provide first aid, and
save lives. In these situations, they must be able to provide personalized and
optimized healthcare in the shortest possible time and estimate the patients
condition with the help of freshly recorded vital data in an emergency
situation. However, in such a timedependent situation, first responders and
medical experts cannot fully grasp their knowledge and need assistance and
recommendation for further medical treatments. To achieve this, on the spot
calculated, evaluated, and processed knowledge must be made available to
improve treatments by first responders. The Knowledge Graph presented in this
article as a central knowledge representation provides first responders with an
innovative knowledge management that enables intelligent treatment
recommendations with an artificial intelligence-based pre-recognition of the
situation.

</details>


### [56] [\(X\)-evolve: Solution space evolution powered by large language models](https://arxiv.org/abs/2508.07932)
*Yi Zhai,Zhiqiang Wei,Ruohan Li,Keyu Pan,Shuo Liu,Lu Zhang,Jianmin Ji,Wuyang Zhang,Yu Zhang,Yanyong Zhang*

Main category: cs.AI

TL;DR: 提出X - evolve方法，通过进化解空间解决复杂优化问题，减少LLM调用成本，在三个难题上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有将大语言模型与进化算法结合的方法进化单个解，LLM调用成本高，需改进。

Method: 引入X - evolve方法，让LLM生成可调程序定义解空间，用基于分数的搜索算法探索。

Result: 在三个难题上验证效果，如在帽集问题确定新下界，在信息论提高已知下界，在在线装箱问题生成更优启发式策略。

Conclusion: 进化解空间的方法提高搜索有效性，可解决高维难题。

Abstract: While combining large language models (LLMs) with evolutionary algorithms
(EAs) shows promise for solving complex optimization problems, current
approaches typically evolve individual solutions, often incurring high LLM call
costs. We introduce \(X\)-evolve, a paradigm-shifting method that instead
evolves solution spaces \(X\) (sets of individual solutions) - subsets of the
overall search space \(S\). In \(X\)-evolve, LLMs generate tunable programs
wherein certain code snippets, designated as parameters, define a tunable
solution space. A score-based search algorithm then efficiently explores this
parametrically defined space, guided by feedback from objective function
scores. This strategy enables broader and more efficient exploration, which can
potentially accelerate convergence at a much lower search cost, requiring up to
two orders of magnitude fewer LLM calls than prior leading methods. We
demonstrate \(X\)-evolve's efficacy across three distinct hard optimization
problems. For the cap set problem, we discover a larger partial admissible set,
establishing a new tighter asymptotic lower bound for the cap set constant (\(C
\ge 2.2203\)). In information theory, we uncover a larger independent set for
the 15-vertex cycle graph (\(\mathcal{C}_{15}^{\boxtimes 5}\), size 19,946),
thereby raising the known lower bound on its Shannon capacity. Furthermore, for
the NP-hard online bin packing problem, we generate heuristics that
consistently outperform standard strategies across established benchmarks. By
evolving solution spaces, our method considerably improves search
effectiveness, making it possible to tackle high-dimensional problems that were
previously computationally prohibitive.

</details>


### [57] [Deep Reinforcement Learning with anticipatory reward in LSTM for Collision Avoidance of Mobile Robots](https://arxiv.org/abs/2508.07941)
*Olivier Poulet,Frédéric Guinand,François Guérin*

Main category: cs.AI

TL;DR: 提出基于智能体位置短期预测的碰撞风险预估方法，用LSTM模型预测位置，结合DQN动态调整奖励，测试效果好且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在受限环境中无通信和标识时的碰撞问题。

Method: 用训练好的LSTM模型预测机器人下一位置，通过动态调整DQN智能体奖励来定义预期碰撞风险。

Result: 在受限环境测试中，虽采样频率低，碰撞次数显著减少，稳定性提高。

Conclusion: 该方法计算成本低，适合在嵌入式系统实现。

Abstract: This article proposes a collision risk anticipation method based on
short-term prediction of the agents position. A Long Short-Term Memory (LSTM)
model, trained on past trajectories, is used to estimate the next position of
each robot. This prediction allows us to define an anticipated collision risk
by dynamically modulating the reward of a Deep Q-Learning Network (DQN) agent.
The approach is tested in a constrained environment, where two robots move
without communication or identifiers. Despite a limited sampling frequency (1
Hz), the results show a significant decrease of the collisions number and a
stability improvement. The proposed method, which is computationally
inexpensive, appears particularly attractive for implementation on embedded
systems.

</details>


### [58] [FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis](https://arxiv.org/abs/2508.07950)
*Chen Shen,Wanqing Zhang,Kehan Li,Erwen Huang,Haitao Bi,Aiying Fan,Yiwen Shen,Hongmei Dong,Ji Zhang,Yuming Shao,Zengjia Liu,Xinshe Liu,Tao Li,Chunxia Yan,Shuanliang Fan,Di Wu,Jianhua Ma,Bin Cong,Zhenyuan Wang,Chunfeng Lian*

Main category: cs.AI

TL;DR: 引入多智能体AI框架FEAT用于法医死因鉴定，在多方面表现优于现有系统，兼具效率与严谨性，有望改善法医系统服务。


<details>
  <summary>Details</summary>
Motivation: 法医死因鉴定面临人员短缺和诊断差异等系统性挑战，尤其是在中国这样的高流量法医系统中。

Method: 引入FEAT框架，其架构集成中央规划器、专业本地求解器、记忆与反思模块和全局求解器，采用工具增强推理、分层检索增强生成、法医调优大语言模型和人在环反馈。

Result: 在不同中国案例队列评估中，FEAT在尸检分析和死因结论方面优于现有AI系统，在六个地理区域有良好泛化能力，盲测中与专家意见高度一致，能检测细微证据差异。

Conclusion: FEAT是首个基于大语言模型的法医AI系统，结合AI效率和人工监督，可促进公平获得可靠法医服务，解决法医系统的能力限制问题。

Abstract: Forensic cause-of-death determination faces systemic challenges, including
workforce shortages and diagnostic variability, particularly in high-volume
systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic
AgenT), a multi-agent AI framework that automates and standardizes death
investigations through a domain-adapted large language model. FEAT's
application-oriented architecture integrates: (i) a central Planner for task
decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a
Memory & Reflection module for iterative refinement, and (iv) a Global Solver
for conclusion synthesis. The system employs tool-augmented reasoning,
hierarchical retrieval-augmented generation, forensic-tuned LLMs, and
human-in-the-loop feedback to ensure legal and medical validity. In evaluations
across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI
systems in both long-form autopsy analyses and concise cause-of-death
conclusions. It demonstrated robust generalization across six geographic
regions and achieved high expert concordance in blinded validations. Senior
pathologists validated FEAT's outputs as comparable to those of human experts,
with improved detection of subtle evidentiary nuances. To our knowledge, FEAT
is the first LLM-based AI agent system dedicated to forensic medicine, offering
scalable, consistent death certification while maintaining expert-level rigor.
By integrating AI efficiency with human oversight, this work could advance
equitable access to reliable medicolegal services while addressing critical
capacity constraints in forensic systems.

</details>


### [59] [Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths](https://arxiv.org/abs/2508.08001)
*Rui Yao,Qi Chai,Jinhai Yao,Siyuan Li,Junhao Chen,Qi Zhang,Hao Wang*

Main category: cs.AI

TL;DR: 提出基于大语言模型、考虑不确定性的框架解读Fedspeak并分类货币政策立场，实验表现佳，验证感知不确定性有效性。


<details>
  <summary>Details</summary>
Motivation: 自动解析和解读Fedspeak对金融预测、算法交易和政策分析有重要意义，但存在高影响挑战。

Method: 构建基于大语言模型、考虑不确定性的框架，结合货币政策传导机制的领域特定推理，引入动态不确定性解码模块。

Result: 框架在政策立场分析任务上达到了最先进的性能，感知不确定性与模型错误率呈显著正相关。

Conclusion: 提出的框架有效，感知不确定性可作为诊断信号。

Abstract: "Fedspeak", the stylized and often nuanced language used by the U.S. Federal
Reserve, encodes implicit policy signals and strategic stances. The Federal
Open Market Committee strategically employs Fedspeak as a communication tool to
shape market expectations and influence both domestic and global economic
conditions. As such, automatically parsing and interpreting Fedspeak presents a
high-impact challenge, with significant implications for financial forecasting,
algorithmic trading, and data-driven policy analysis. In this paper, we propose
an LLM-based, uncertainty-aware framework for deciphering Fedspeak and
classifying its underlying monetary policy stance. Technically, to enrich the
semantic and contextual representation of Fedspeak texts, we incorporate
domain-specific reasoning grounded in the monetary policy transmission
mechanism. We further introduce a dynamic uncertainty decoding module to assess
the confidence of model predictions, thereby enhancing both classification
accuracy and model reliability. Experimental results demonstrate that our
framework achieves state-of-the-art performance on the policy stance analysis
task. Moreover, statistical analysis reveals a significant positive correlation
between perceptual uncertainty and model error rates, validating the
effectiveness of perceptual uncertainty as a diagnostic signal.

</details>


### [60] [Fitting Description Logic Ontologies to ABox and Query Examples](https://arxiv.org/abs/2508.08007)
*Maurice Funk,Marvin Grosser,Carsten Lutz*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study a fitting problem inspired by ontology-mediated querying: given a
collection
  of positive and negative examples of
  the form $(\mathcal{A},q)$ with
  $\mathcal{A}$ an ABox and $q$ a Boolean query, we seek
  an ontology $\mathcal{O}$ that satisfies $\mathcal{A} \cup \mathcal{O} \vDash
q$ for all positive examples and $\mathcal{A} \cup \mathcal{O}\not\vDash q$ for
all negative examples.
  We consider the description logics $\mathcal{ALC}$ and $\mathcal{ALCI}$ as
ontology languages and
  a range of query languages that
  includes atomic queries (AQs), conjunctive queries (CQs), and unions thereof
(UCQs).
  For all of the resulting fitting problems,
  we provide
  effective characterizations and determine the computational complexity
  of deciding whether a fitting ontology exists. This problem turns out to be
${\small CO}NP$ for AQs and full CQs
  and $2E{\small XP}T{\small IME}$-complete for CQs and UCQs.
  These results hold for both $\mathcal{ALC}$ and $\mathcal{ALCI}$.

</details>


### [61] [AdaptFlow: Adaptive Workflow Optimization via Meta-Learning](https://arxiv.org/abs/2508.08053)
*Runchuan Zhu,Bowen Jiang,Lingrui Mei,Fangkai Yang,Lu Wang,Haoxiang Gao,Fengshuo Bai,Pu Zhao,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: 提出AdaptFlow框架解决LLM代理工作流适应性和可扩展性问题，在多基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理工作流方法依赖静态模板或手动设计，限制了对不同任务的适应性和可扩展性。

Method: 提出基于自然语言的元学习框架AdaptFlow，采用双层优化方案，内循环用LLM反馈优化特定子任务工作流，外循环更新共享初始化。

Result: 在问答、代码生成和数学推理基准测试中，AdaptFlow始终优于手动和自动搜索的基线，取得了跨任务和模型的最佳结果。

Conclusion: AdaptFlow能通过语言引导修改初始化工作流，有效泛化到未见任务，具有良好的性能和泛化能力。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in agentic workflows, which are structured sequences of LLM invocations
intended to solve complex tasks. However, existing approaches often rely on
static templates or manually designed workflows, which limit adaptability to
diverse tasks and hinder scalability. We propose AdaptFlow, a natural
language-based meta-learning framework inspired by model-agnostic meta-learning
(MAML). AdaptFlow learns a generalizable workflow initialization that enables
rapid subtask-level adaptation. It employs a bi-level optimization scheme: the
inner loop refines the workflow for a specific subtask using LLM-generated
feedback, while the outer loop updates the shared initialization to perform
well across tasks. This setup allows AdaptFlow to generalize effectively to
unseen tasks by adapting the initialized workflow through language-guided
modifications. Evaluated across question answering, code generation, and
mathematical reasoning benchmarks, AdaptFlow consistently outperforms both
manually crafted and automatically searched baselines, achieving
state-of-the-art results with strong generalization across tasks and models.
The source code and data are available at
https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.

</details>


### [62] [FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence](https://arxiv.org/abs/2508.08075)
*Meishen He,Wenjun Ma,Jiao Wang,Huijun Yue,Xiaoma Fan*

Main category: cs.AI

TL;DR: 文章提出基于Dempster - Shafer理论的开放世界信息融合方法FNBT，理论上满足三个属性，实证中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注同一识别框架内的证据组合，但现实中不同数据源或模型生成的基本概率分配可能导致异构框架，传统融合方法效果不佳。

Method: 引入标准判断融合任务是否属于开放世界设置，扩展框架以容纳异构框架元素，采用全否定机制转换质量函数。

Result: 理论上满足质量函数不变性、遗传性和基本冲突消除三个属性；实证中在模式分类任务中表现出色，解决了Zadeh反例。

Conclusion: 提出的FNBT方法在理论和实践上都有效，能解决异构框架下的信息融合问题。

Abstract: The Dempster-Shafer theory of evidence has been widely applied in the field
of information fusion under uncertainty. Most existing research focuses on
combining evidence within the same frame of discernment. However, in real-world
scenarios, trained algorithms or data often originate from different regions or
organizations, where data silos are prevalent. As a result, using different
data sources or models to generate basic probability assignments may lead to
heterogeneous frames, for which traditional fusion methods often yield
unsatisfactory results. To address this challenge, this study proposes an
open-world information fusion method, termed Full Negation Belief
Transformation (FNBT), based on the Dempster-Shafer theory. More specially, a
criterion is introduced to determine whether a given fusion task belongs to the
open-world setting. Then, by extending the frames, the method can accommodate
elements from heterogeneous frames. Finally, a full negation mechanism is
employed to transform the mass functions, so that existing combination rules
can be applied to the transformed mass functions for such information fusion.
Theoretically, the proposed method satisfies three desirable properties, which
are formally proven: mass function invariance, heritability, and essential
conflict elimination. Empirically, FNBT demonstrates superior performance in
pattern classification tasks on real-world datasets and successfully resolves
Zadeh's counterexample, thereby validating its practical effectiveness.

</details>


### [63] [TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork](https://arxiv.org/abs/2508.08115)
*Pranav Pushkar Mishra,Mohammad Arvan,Mohan Zalake*

Main category: cs.AI

TL;DR: 提出TeamMedAgents多智能体方法，将人类协作团队组件集成到医学决策，在多个医学基准测试中表现良好，揭示不同数据集的最优团队配置。


<details>
  <summary>Details</summary>
Motivation: 将人类协作中的基于证据的团队合作组件系统地集成到使用大语言模型的医学决策中，验证团队合作模型在计算多智能体医疗系统中的有效性。

Method: 基于Salas等人的“Big Five”模型的六个核心团队合作组件，在自适应协作架构中实现并评估这些组件，评估不同数量智能体的影响。

Result: 在8个医学基准测试中的7个数据集上有一致改进，消融研究揭示了不同推理任务复杂性和特定领域要求下的最优团队配置。

Conclusion: TeamMedAgents是协作AI的进步，为关键决策领域的多智能体系统设计奠定基础。

Abstract: We present TeamMedAgents, a novel multi-agent approach that systematically
integrates evidence-based teamwork components from human-human collaboration
into medical decision-making with large language models (LLMs). Our approach
validates an organizational psychology teamwork model from human collaboration
to computational multi-agent medical systems by operationalizing six core
teamwork components derived from Salas et al.'s "Big Five" model: team
leadership, mutual performance monitoring, team orientation, shared mental
models, closed-loop communication, and mutual trust. We implement and evaluate
these components as modular, configurable mechanisms within an adaptive
collaboration architecture while assessing the effect of the number of agents
involved based on the task's requirements and domain. Systematic evaluation of
computational implementations of teamwork behaviors across eight medical
benchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,
Path-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8
evaluated datasets. Controlled ablation studies conducted on 50 questions per
configuration across 3 independent runs provide mechanistic insights into
individual component contributions, revealing optimal teamwork configurations
that vary by reasoning task complexity and domain-specific requirements. Our
ablation analyses reveal dataset-specific optimal teamwork configurations,
indicating that different medical reasoning modalities benefit from distinct
collaborative patterns. TeamMedAgents represents an advancement in
collaborative AI by providing a systematic translation of established teamwork
theories from human collaboration into agentic collaboration, establishing a
foundation for evidence-based multi-agent system design in critical
decision-making domains.

</details>


### [64] [BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks](https://arxiv.org/abs/2508.08127)
*Rui Miao,Yixin Liu,Yili Wang,Xu Shen,Yue Tan,Yiwei Dai,Shirui Pan,Xin Wang*

Main category: cs.AI

TL;DR: 提出无监督防御方法BlindGuard保护LLM多智能体系统安全，实验证明有效且泛化性强。


<details>
  <summary>Details</summary>
Motivation: 现有监督防御方法依赖标记恶意智能体，在现实场景不实用，需实用且可泛化的防御方法。

Method: 建立分层智能体编码器捕捉交互模式，设计腐败引导检测器，含定向噪声注入和对比学习，仅基于正常智能体行为训练检测模型。

Result: BlindGuard能有效检测多种攻击类型，在不同通信模式的多智能体系统中保持优于监督基线的泛化性。

Conclusion: BlindGuard是实用且可泛化的多智能体系统防御方法，代码开源。

Abstract: The security of LLM-based multi-agent systems (MAS) is critically threatened
by propagation vulnerability, where malicious agents can distort collective
decision-making through inter-agent message interactions. While existing
supervised defense methods demonstrate promising performance, they may be
impractical in real-world scenarios due to their heavy reliance on labeled
malicious agents to train a supervised malicious detection model. To enable
practical and generalizable MAS defenses, in this paper, we propose BlindGuard,
an unsupervised defense method that learns without requiring any
attack-specific labels or prior knowledge of malicious behaviors. To this end,
we establish a hierarchical agent encoder to capture individual, neighborhood,
and global interaction patterns of each agent, providing a comprehensive
understanding for malicious agent detection. Meanwhile, we design a
corruption-guided detector that consists of directional noise injection and
contrastive learning, allowing effective detection model training solely on
normal agent behaviors. Extensive experiments show that BlindGuard effectively
detects diverse attack types (i.e., prompt injection, memory poisoning, and
tool attack) across MAS with various communication patterns while maintaining
superior generalizability compared to supervised baselines. The code is
available at: https://github.com/MR9812/BlindGuard.

</details>


### [65] [From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework](https://arxiv.org/abs/2508.08147)
*Yunkai Hu,Tianqiao Zhao,Meng Yue*

Main category: cs.AI

TL;DR: 提出一种大语言模型辅助的智能体，将电力系统优化场景自然语言描述转化为可求解的公式并生成解决方案，以单位承诺问题为例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 直接用大语言模型生成解决方案常导致不可行或次优结果，因其缺乏数值精度和约束处理能力，需结合现有优化框架。

Method: 将领域感知提示和模式与大语言模型集成，通过系统验证和迭代修复确保可行性，返回可求解模型和用户结果。

Result: 以单位承诺问题为例，智能体产生最优或接近最优的调度和目标成本，求解器与特定任务验证结合显著提高解决方案可靠性。

Conclusion: 将人工智能与现有优化框架结合，能连接高层问题描述和可执行数学模型，实现能源系统更高效决策。

Abstract: This paper introduces a novel Large Language Models (LLMs)-assisted agent
that automatically converts natural-language descriptions of power system
optimization scenarios into compact, solver-ready formulations and generates
corresponding solutions. In contrast to approaches that rely solely on LLM to
produce solutions directly, the proposed method focuses on discovering a
mathematically compatible formulation that can be efficiently solved by
off-the-shelf optimization solvers. Directly using LLMs to produce solutions
often leads to infeasible or suboptimal results, as these models lack the
numerical precision and constraint-handling capabilities of established
optimization solvers. The pipeline integrates a domain-aware prompt and schema
with an LLM, enforces feasibility through systematic validation and iterative
repair, and returns both solver-ready models and user-facing results. Using the
unit commitment problem as a representative case study, the agent produces
optimal or near-optimal schedules along with the associated objective costs.
Results demonstrate that coupling the solver with task-specific validation
significantly enhances solution reliability. This work shows that combining AI
with established optimization frameworks bridges high-level problem
descriptions and executable mathematical models, enabling more efficient
decision-making in energy systems

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [66] [A Portable Multi-GPU Solver for Collisional Plasmas with Coulombic Interactions](https://arxiv.org/abs/2508.06771)
*James Almgren-Bell,Nader Al Awar,Dilip S Geethakrishnan,Milos Gligoric,George Biros*

Main category: cs.CE

TL;DR: 研究低温等离子体的并行粒子模拟方法，聚焦GPU加速及Python快速原型实现，给出不同GPU性能和分布式内存扩展结果。


<details>
  <summary>Details</summary>
Motivation: 对低温等离子体并行粒子模拟方法的GPU加速及快速原型实现进行研究。

Method: 采用电子动力学描述和重粒子流体近似，进行算法探索分析，用Python的PyKokkos工具快速原型实现。

Result: MI250X对多数内核稍快，但对寄存器压力更敏感，给出分布式内存实现的扩展结果。

Conclusion: 完成对低温等离子体并行粒子模拟方法的GPU加速研究及性能测试。

Abstract: We study parallel particle-in-cell (PIC) methods for low-temperature plasmas
(LTPs), which discretize kinetic formulations that capture the time evolution
of the probability density function of particles as a function of position and
velocity. We use a kinetic description for electrons and a fluid approximation
for heavy species. In this paper, we focus on GPU acceleration of algorithms
for velocity-space interactions and in particular, collisions of electrons with
neutrals, ions, and electrons. Our work has two thrusts. The first is
algorithmic exploration and analysis. The second is examining the viability of
rapid-prototyping implementations using Python-based HPC tools, in particular
PyKokkos. We discuss several common PIC kernels and present performance results
on NVIDIA Volta V100 and AMD MI250X GPUs. Overall, the MI250X is slightly
faster for most kernels but shows more sensitivity to register pressure. We
also report scaling results for a distributed memory implementation on up to 16
MPI ranks.

</details>


### [67] [Application of association rule mining to assess forest species distribution in Italy considering abiotic and biotic factors](https://arxiv.org/abs/2508.07076)
*Valeria Aloisi,Sergio Noce,Italo Epicoco,Cristina Cipriano,Massimo Cafaro,Giuseppe Brundu,Lorenzo Arcidiaco,Donatella Spano,Giovanni Aloisio,Simone Mereu*

Main category: cs.CE

TL;DR: 本文将关联规则挖掘用于生态领域，整合多源数据，用FP-growth算法挖掘植物物种与环境条件的规则，发现云杉与特定气候条件强相关，部分物种是群落‘枢纽’，研究成果有重要价值。


<details>
  <summary>Details</summary>
Motivation: 生物多样性监测是全球优先事项，评估森林群落组成对生态系统功能至关重要，需了解森林物种空间分布，传统关联规则挖掘方法可应用于生态领域挖掘物种关系。

Method: 将多源异构数据预处理成统一数据集，使用Frequent Pattern Growth算法进行关联规则挖掘。

Result: 发现温度季节性650 - 700、降水季节性45 - 50与欧洲云杉高度相关，云杉生态特异性明显，部分物种常与其他物种共存，是群落‘枢纽’。

Conclusion: 研究结果对类似环境区域和有先验生态知识的研究有价值，强调了高质量生态数据公开的重要性。

Abstract: Biodiversity monitoring represents a pressing global priority, and assessing
forest community composition plays a crucial role due to its influence on
ecosystem functions. The spatial distribution of forest species becomes
essential for understanding biodiversity dynamics, territorial planning, aiding
nature conservation and enhancing ecosystem resilience amid global change.
Association Rule Mining, commonly applied to other scientific contexts, is now
innovatively adopted in the ecological field to explore the relationships among
co-occurring plant species and extract hidden interpretable patterns, also with
abiotic and biotic conditions. Multiple heterogeneous data sources were
integrated through data preprocessing into a unique dataset, including
georeferenced information about 151 plant species monitored within 6,784 plots
across Italy and several bioclimatic indices, soil-related factors, and
variables from earth observations. The Frequent Pattern Growth algorithm, used
for association rule mining, provided interesting and encouraging findings,
suggesting ecological rules among plant species and environmental conditions.
Indeed, temperature seasonality between 650-700 and precipitation seasonality
between 45-50 resulted very correlated with Picea abies (confidence = 90.9%,
lift = 7.13). Patterns detected for Picea abies highlighted its ecological
specificity, indicating a strong association with cold, highly seasonal
environments, and particular plant communities. Some species appeared acting as
community "hubs", frequently co-occurring with other species, suggesting ties
to specific environmental or biotic conditions. These findings represent a
valuable resource for future research, especially in regions with similar
environmental settings and when prior ecological knowledge exists, also
underlining the importance of publicly accessible, high-quality ecological
data.

</details>


### [68] [Harmonic balance-automatic differentiation method: an out-of-the-box and efficient solver for general nonlinear dynamics simulation](https://arxiv.org/abs/2508.07309)
*Yi Chen,Yuhong Jin,Rongzhou Lin,Yifan Jiang,Xutao Mei,Lei Houb,Yilong Wang,Ng Teng Yong,Anxin Guo*

Main category: cs.CE

TL;DR: 提出Harmonic Balance - Automatic Differentiation (HB - AD)方法解决传统HB - AFT方法在高维复杂系统应用中的局限，通过计算实验验证其高效性，为高维工程系统提供分析平台。


<details>
  <summary>Details</summary>
Motivation: 传统HB - AFT方法在高维复杂系统应用中受牛顿 - 拉夫逊迭代时手动推导雅可比矩阵的限制，计算困难且易出错。

Method: 将自动微分（AD）与谐波平衡框架结合，利用深度学习框架进行并行计算和CUDA加速，结合AD与弧长延拓法。

Result: 计算实验表明HB - AD能处理复杂非线性表达式，对高维航空发动机系统，比传统HB - AFT效率高17倍，比Newmark方法快144倍。

Conclusion: HB - AD方法结合计算力学和机器学习原语，为高维工程系统的动态特性分析提供易用、通用、高效的平台。

Abstract: The Harmonic Balance-Alternating Frequency-Time domain (HB-AFT) method is
extensively employed for dynamic response analysis of nonlinear systems.
However, its application to high-dimensional complex systems is constrained by
the manual derivation of Jacobian matrices during Newton-Raphson iterations,
which become computationally intractable or error-prone for intricate
nonlinearities. The Harmonic Balance-Automatic Differentiation (HB-AD) method
is proposed to address this limitation, in which AD is integrated with the
harmonic balance framework. This approach eliminates all manual derivations by
leveraging AD to compute exact Jacobians numerically, enabling generic and
efficient analysis of high-dimensional complex nonlinear systems. The
implementation utilizes advanced deep learning frameworks for native parallel
computing and CUDA acceleration, and combines AD with arc-length continuation,
establishing an out-of-the-box and high efficiency computational architecture.
Users need only supply the system's dynamic equations, HB-AD then autonomously
trace the complete panorama of periodic responses -- including stable/unstable
solution branches. Computational experiments on a rotor system with
squeeze-film damper (SFD) demonstrate HB-AD's capability in handling complex
nonlinear expressions with automated Jacobian calculations. For a
high-dimensional aero-engine rotor-bearing-casing system with complex bearing
nonlinearities, HB-AD achieves 17-fold higher efficiency than traditional
HB-AFT and 144-fold acceleration over the Newmark method. The HB-AD method is a
synergistic merger of computational mechanics and machine learning primitives,
delivers an easy to use, general-purpose, high efficiency platform for
high-fidelity dynamic characterization of high-dimensional engineering systems.

</details>


### [69] [Cardiotensor: A Python Library for Orientation Analysis and Tractography in 3D Cardiac Imaging](https://arxiv.org/abs/2508.07476)
*Joseph Brunet,Lisa Chestnutt,Matthieu Chourrout,Hector Dejea,Vaishnavi Sabarigirivasan,Peter D. Lee,Andrew C. Cook*

Main category: cs.CE

TL;DR: 介绍开源Python包cardiotensor，可量化心脏成像数据中3D心肌细胞方向，支持大数据集和高性能计算，还具备纤维追踪功能，能进行心脏组织详细结构映射。


<details>
  <summary>Details</summary>
Motivation: 现有高分辨率成像技术可可视化心脏，但将复杂数据集转化为可解释的心脏组织定量描述符仍是挑战，需要开发工具解决。

Method: 开发开源Python包cardiotensor，采用结构张量分析并实现高效可扩展的算法，支持并行和分块处理，具备纤维追踪功能。

Result: cardiotensor能提取如螺旋角、侵入角和各向异性分数等方向指标，支持太体素级数据集，可进行多尺度心肌聚集体可视化。

Conclusion: cardiotensor可实现心脏组织的详细结构映射，有助于评估解剖连续性和区域组织情况。

Abstract: Understanding the architecture of the human heart requires analysis of its
microstructural organization across scales. With the advent of high-resolution
imaging techniques such as synchrotron-based tomography, it has become possible
to visualize entire hearts at micron-scale resolution. However, translating
these large, complex volumetric datasets into interpretable, quantitative
descriptors of cardiac organization remains a major challenge. Here we present
cardiotensor, an open-source Python package designed to quantify 3D
cardiomyocyte orientation in whole- or partial-heart imaging datasets. It
provides efficient, scalable implementations of structure tensor analysis,
enabling extraction of directional metrics such as helical angle (HA),
intrusion angle (IA), and fractional anisotropy (FA). The package supports
datasets reaching teravoxel-scale and is optimized for high-performance
computing environments, including parallel and chunk-based processing
pipelines. In addition, cardiotensor includes tractography functionality to
reconstruct continuous cardiomyocyte trajectories. This enables multi-scale
myoaggregate visualization down to the myocyte level, depending on resolution.
These capabilities enable detailed structural mapping of cardiac tissue,
supporting the assessment of anatomical continuity and regional organization.

</details>


### [70] [Material Fingerprinting: A shortcut to material model discovery without solving optimization problems](https://arxiv.org/abs/2508.07831)
*Moritz Flaschel,Denisa Martonová,Carina Veil,Ellen Kuhl*

Main category: cs.CE

TL;DR: 提出Material Fingerprinting方法用于快速发现机械材料模型，避免非凸优化问题，在超弹性材料研究中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决从直接或间接数据中快速发现机械材料模型时可能遇到的非凸优化问题。

Method: 建立包含指纹及其对应机械模型的数据库，在线阶段测量未知材料指纹并通过模式识别算法匹配。先在均匀变形场实验中研究，再扩展到复杂形状试样的非均匀变形场实验。

Result: 在两种实验情况下，Material Fingerprinting都是有效的模型发现工具，绕过了非凸优化挑战。

Conclusion: Material Fingerprinting为广泛实验设计和材料行为的快速材料模型识别提供强大且通用的框架，为未来发展奠定基础。

Abstract: We propose Material Fingerprinting, a new method for the rapid discovery of
mechanical material models from direct or indirect data that avoids solving
potentially non-convex optimization problems. The core assumption of Material
Fingerprinting is that each material exhibits a unique response when subjected
to a standardized experimental setup. We can interpret this response as the
material's fingerprint, essentially a unique identifier that encodes all
pertinent information about the material's mechanical characteristics.
Consequently, once we have established a database containing fingerprints and
their corresponding mechanical models during an offline phase, we can rapidly
characterize an unseen material in an online phase. This is accomplished by
measuring its fingerprint and employing a pattern recognition algorithm to
identify the best matching fingerprint in the database. In our study, we
explore this concept in the context of hyperelastic materials, demonstrating
the applicability of Material Fingerprinting across different experimental
setups. Initially, we examine Material Fingerprinting through experiments
involving homogeneous deformation fields, which provide direct strain-stress
data pairs. We then extend this concept to experiments involving complexly
shaped specimens with heterogeneous deformation fields, which provide indirect
displacement and reaction force measurements. We show that, in both cases,
Material Fingerprinting is an efficient tool for model discovery, bypassing the
challenges of potentially non-convex optimization. We believe that Material
Fingerprinting provides a powerful and generalizable framework for rapid
material model identification across a wide range of experimental designs and
material behaviors, paving the way for numerous future developments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [71] [Omni Geometry Representation Learning vs Large Language Models for Geospatial Entity Resolution](https://arxiv.org/abs/2508.06584)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.DB

TL;DR: 提出地理空间实体解析模型Omni解决不同几何形状实体解析问题，测试显示其比现有方法有提升，还测试了大语言模型进行地理空间实体解析的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有地理空间实体解析方法多关注兴趣点，对不同几何形状实体解析缺乏统一技术，现有神经网络方法会造成空间信息损失。

Method: 提出具有全几何编码器的Omni模型，能嵌入多种几何形状，利用基于Transformer的预训练语言模型处理文本属性；测试大语言模型进行地理空间实体解析的潜力。

Result: Omni模型在F1值上比现有方法最多提升12%，大语言模型在地理空间实体解析中表现有竞争力。

Conclusion: Omni模型能有效解决不同几何形状实体的地理空间实体解析问题，大语言模型在该领域有应用潜力。

Abstract: The development, integration, and maintenance of geospatial databases rely
heavily on efficient and accurate matching procedures of Geospatial Entity
Resolution (ER). While resolution of points-of-interest (POIs) has been widely
addressed, resolution of entities with diverse geometries has been largely
overlooked. This is partly due to the lack of a uniform technique for embedding
heterogeneous geometries seamlessly into a neural network framework. Existing
neural approaches simplify complex geometries to a single point, resulting in
significant loss of spatial information. To address this limitation, we propose
Omni, a geospatial ER model featuring an omni-geometry encoder. This encoder is
capable of embedding point, line, polyline, polygon, and multi-polygon
geometries, enabling the model to capture the complex geospatial intricacies of
the places being compared. Furthermore, Omni leverages transformer-based
pre-trained language models over individual textual attributes of place records
in an Attribute Affinity mechanism. The model is rigorously tested on existing
point-only datasets and a new diverse-geometry geospatial ER dataset. Omni
produces up to 12% (F1) improvement over existing methods.
  Furthermore, we test the potential of Large Language Models (LLMs) to conduct
geospatial ER, experimenting with prompting strategies and learning scenarios,
comparing the results of pre-trained language model-based methods with LLMs.
Results indicate that LLMs show competitive results.

</details>


### [72] [Metadata Management for AI-Augmented Data Workflows](https://arxiv.org/abs/2508.06814)
*Jinjin Zhao,Sanjay Krishnan*

Main category: cs.DB

TL;DR: 本文提出用于人机协作数据创建的元数据治理框架TableVault，通过案例展示其能实现强大元数据管理。


<details>
  <summary>Details</summary>
Motivation: AI增强的数据工作流带来复杂治理挑战，难以进行全面元数据捕获。

Method: 提出TableVault框架，记录摄入事件、追踪操作状态、关联执行参数与数据来源、提供标准化元数据层，结合数据库保障和AI导向设计。

Result: 通过文档分类案例研究，表明TableVault能保留详细沿袭和操作上下文。

Conclusion: TableVault支持混合人机管道的透明度和可重复性，能在部分可观察的执行环境中实现强大的元数据管理。

Abstract: AI-augmented data workflows introduce complex governance challenges, as both
human and model-driven processes generate, transform, and consume data
artifacts. These workflows blend heterogeneous tools, dynamic execution
patterns, and opaque model decisions, making comprehensive metadata capture
difficult. In this work, we present TableVault, a metadata governance framework
designed for human-AI collaborative data creation. TableVault records ingestion
events, traces operation status, links execution parameters to their data
origins, and exposes a standardized metadata layer. By combining
database-inspired guarantees with AI-oriented design, such as declarative
operation builders and lineage-aware references, TableVault supports
transparency and reproducibility across mixed human-model pipelines. Through a
document classification case study, we demonstrate how TableVault preserves
detailed lineage and operational context, enabling robust metadata management,
even in partially observable execution environments.

</details>


### [73] [Balancing Privacy and Efficiency: Music Information Retrieval via Additive Homomorphic Encryption](https://arxiv.org/abs/2508.07044)
*William Zerong Wang,Dongfang Zhao*

Main category: cs.DB

TL;DR: 在生成式AI时代，音乐数据隐私保护面临挑战，传统方法有限，标准加密方案有缺陷，本文提出用加法同态加密（AHE）进行向量相似度搜索的实用方法，并进行分析和评估。


<details>
  <summary>Details</summary>
Motivation: 生成式AI时代音乐数据的特性使其核心向量嵌入易被滥用，传统保护方法有限，需新的加密方法保护。

Method: 提出使用加法同态加密（AHE）进行向量相似度搜索，分析威胁模型，通过音乐嵌入的内积给出理论分析和高效解决方案。

Result: 通过对真实MP3文件的实证评估和与FHE方案的比较，证明了所提方法的效率和实用性。

Conclusion: 加法同态加密（AHE）为音乐信息检索系统中的向量相似度搜索提供了一种实用的隐私保护方法。

Abstract: In the era of generative AI, ensuring the privacy of music data presents
unique challenges: unlike static artworks such as images, music data is
inherently temporal and multimodal, and it is sampled, transformed, and remixed
at an unprecedented scale. These characteristics make its core vector
embeddings, i.e, the numerical representations of the music, highly susceptible
to being learned, misused, or even stolen by models without accessing the
original audio files. Traditional methods like copyright licensing and digital
watermarking offer limited protection for these abstract mathematical
representations, thus necessitating a stronger, e.g., cryptographic, approach
to safeguarding the embeddings themselves. Standard encryption schemes, such as
AES, render data unintelligible for computation, making such searches
impossible. While Fully Homomorphic Encryption (FHE) provides a plausible
solution by allowing arbitrary computations on ciphertexts, its substantial
performance overhead remains impractical for large-scale vector similarity
searches. Given this trade-off, we propose a more practical approach using
Additive Homomorphic Encryption (AHE) for vector similarity search. The primary
contributions of this paper are threefold: we analyze threat models unique to
music information retrieval systems; we provide a theoretical analysis and
propose an efficient AHE-based solution through inner products of music
embeddings to deliver privacy-preserving similarity search; and finally, we
demonstrate the efficiency and practicality of the proposed approach through
empirical evaluation and comparison to FHE schemes on real-world MP3 files.

</details>


### [74] [SQL-Exchange: Transforming SQL Queries Across Domains](https://arxiv.org/abs/2508.07087)
*Mohammadreza Daviran,Brian Lin,Davood Rafiei*

Main category: cs.DB

TL;DR: 介绍SQL - Exchange框架，评估其效果并证明能提升文本到SQL系统性能


<details>
  <summary>Details</summary>
Motivation: 研究不同数据库模式间SQL查询映射的可行性和益处，提升文本到SQL系统的上下文学习性能

Method: 引入SQL - Exchange框架进行映射，在多个模型族和基准数据集上进行综合评估

Result: SQL - Exchange在多种模式和查询类型中有效，使用映射查询作为上下文示例能提升文本到SQL性能

Conclusion: SQL - Exchange框架可行且有效，能提升文本到SQL系统性能

Abstract: We introduce SQL-Exchange, a framework for mapping SQL queries across
different database schemas by preserving the source query structure while
adapting domain-specific elements to align with the target schema. We
investigate the conditions under which such mappings are feasible and
beneficial, and examine their impact on enhancing the in-context learning
performance of text-to-SQL systems as a downstream task. Our comprehensive
evaluation across multiple model families and benchmark datasets--assessing
structural alignment with source queries, execution validity on target
databases, and semantic correctness--demonstrates that SQL-Exchange is
effective across a wide range of schemas and query types. Our results further
show that using mapped queries as in-context examples consistently improves
text-to-SQL performance over using queries from the source schema.

</details>


### [75] [Accelerating High-Dimensional Nearest Neighbor Search with Dynamic Query Preference](https://arxiv.org/abs/2508.07218)
*Yunjun Gao,Ruijie Zhao,Zhonggen Li,Baihua Zheng,Yifan Zhu,Zhaoqing Chen*

Main category: cs.DB

TL;DR: 提出DQF双索引查询框架用于近似最近邻搜索，在四个数据集上实验表明比现有算法提速2.0 - 5.7倍，保持95%召回率，且查询分布变化时无需重建全量索引。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的近似最近邻搜索方法假设查询分布均匀，而实际中查询频率有差异，为利用该特性提出新框架。

Method: 提出DQF框架，包含双层索引结构（高频节点热索引和全量数据集全索引）和基于决策树的动态搜索策略。

Result: 在四个真实数据集上，DQF框架比现有算法提速2.0 - 5.7倍，保持95%召回率，查询分布变化时无需重建全量索引。

Conclusion: DQF框架在动态查询分布场景下有效且实用。

Abstract: Approximate Nearest Neighbor Search (ANNS) is a crucial operation in
databases and artificial intelligence. Current graph-based ANNS methods, such
as HNSW and NSG, have shown remarkable performance but are designed under the
assumption of a uniform query distribution. However, in practical scenarios,
user preferences and query temporal dynamics lead to some queries being
searched for more frequently than others. To fully utilize these
characteristics, we propose DQF, a novel Dual-Index Query Framework. This
framework comprises a dual-layer index structure and a dynamic search strategy
based on a decision tree. The dual-layer index structure comprises a hot index
for high-frequency nodes and a full index for the entire dataset, allowing for
the separate management of hot and cold queries. Furthermore, we propose a
dynamic search strategy that employs a decision tree to adapt to the specific
characteristics of each query. The decision tree evaluates whether a query is
of the high-frequency type to detect the opportunities for early termination on
the dual-layer, avoiding unnecessary searches in the full index. Experimental
results on four real-world datasets demonstrate that the Dual-Index Query
Framework achieves a significant speedup of 2.0-5.7x over state-of-the-art
algorithms while maintaining a 95% recall rate. Importantly, it does not
require full index reconstruction when query distributions change, underscoring
its efficiency and practicality in dynamic query distribution scenarios.

</details>


### [76] [RNA-KG v2.0: An RNA-centered Knowledge Graph with Properties](https://arxiv.org/abs/2508.07427)
*Emanuele Cavalleri,Paolo Perlasca,Marco Mesiti*

Main category: cs.DB

TL;DR: 提出RNA-KG v2.0，整合约1亿手动整理的交互信息，支持高级查询和下游应用。


<details>
  <summary>Details</summary>
Motivation: 改进现有RNA-KG，整合更多数据、丰富节点属性以支持更高级应用。

Method: 从91个关联开放数据仓库和本体整合手动整理的交互信息，为关系和节点添加标准化属性和详细信息。

Result: 构建RNA-KG v2.0，可进行考虑实验上下文的高级查询，支持结合拓扑和语义信息的“上下文感知”链接预测技术。

Conclusion: RNA-KG v2.0增强了知识库，能更好支持RNA研究的下游应用。

Abstract: RNA-KG is a recently developed knowledge graph that integrates the
interactions involving coding and non-coding RNA molecules extracted from
public data sources. It can be used to support the classification of new
molecules, identify new interactions through the use of link prediction
methods, and reveal hidden patterns among the represented entities. In this
paper, we propose RNA-KG v2.0, a new release of RNA-KG that integrates around
100M manually curated interactions sourced from 91 linked open data
repositories and ontologies. Relationships are characterized by standardized
properties that capture the specific context (e.g., cell line, tissue,
pathological state) in which they have been identified. In addition, the nodes
are enriched with detailed attributes, such as descriptions, synonyms, and
molecular sequences sourced from platforms such as OBO ontologies, NCBI
repositories, RNAcentral, and Ensembl. The enhanced repository enables the
expression of advanced queries that take into account the context in which the
experiments were conducted. It also supports downstream applications in RNA
research, including "context-aware" link prediction techniques that combine
both topological and semantic information.

</details>


### [77] [A Benchmark for Databases with Varying Value Lengths](https://arxiv.org/abs/2508.07551)
*Danushka Liyanage,Shubham Pandey,Joshua Goldstein,Michael Cahill,Akon Dey,Alan Fekete,Uwe Röhm*

Main category: cs.DB

TL;DR: 现有数据库基准测试未考虑值长度变化，本文扩展YCSB基准测试，评估三种数据库后端性能，提出新基准方法，强调需更具代表性的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有数据库基准测试未考虑现实工作负载中值长度的显著变化，导致数据库管理系统行为的重要方面未被充分探索。

Method: 扩展YCSB基准测试，增加“extend”操作模拟值随时间增长，用修改后的基准测试评估MongoDB、带InnoDB存储引擎的MariaDB和带MyRocks存储引擎的MariaDB的性能。

Result: 实验揭示了不同存储引擎设计及其对可变大小值的处理方式导致的显著性能差异。

Conclusion: 引入新的基准测试方法，强调需要更具代表性的基准测试来反映现实工作负载的动态特性，为从业者和研究人员提供指导。

Abstract: The performance of database management systems (DBMS) is traditionally
evaluated using benchmarks that focus on workloads with (almost) fixed record
lengths. However, some real-world workloads in key/value stores, document
databases, and graph databases exhibit significant variability in value
lengths, which can lead to performance anomalies, particularly when popular
records grow disproportionately large. Existing benchmarks fail to account for
this variability, leaving an important aspect of DBMS behavior underexplored.
  In this paper, we address this gap by extending the Yahoo! Cloud Serving
Benchmark (YCSB) to include an "extend" operation, which appends data to record
fields, simulating the growth of values over time. Using this modified
benchmark, we have measured the performance of three popular DBMS backends:
MongoDB, MariaDB with the InnoDB storage engine, and MariaDB with the MyRocks
storage engine. Our experiments alternate between extending values and
executing query workloads, revealing significant performance differences driven
by storage engine design and their handling of variable-sized values.
  Our key contribution is the introduction of a novel benchmarking approach to
evaluate the impact of growing value sizes and isolate the effect of querying
data with a distribution of data sizes from any cost associated with accessing
data after a history of updates. This highlights the need for more
representative benchmarks that capture the dynamic nature of real-world
workloads, providing valuable guidance for both practitioners and researchers.

</details>


### [78] [MLego: Interactive and Scalable Topic Exploration Through Model Reuse](https://arxiv.org/abs/2508.07654)
*Fei Ye,Jiapan Liu,Yinan Jing,Zhenying He,Weirao Wang,X. Sean Wang*

Main category: cs.DB

TL;DR: 提出交互式查询框架MLego支持实时主题建模分析，通过合并物化主题模型提高效率，实验证明其能降低计算成本并保持高质量结果。


<details>
  <summary>Details</summary>
Motivation: 传统主题建模技术计算成本高，实时主题探索仍是挑战，需要高效方法支持实时分析。

Method: 提出MLego框架，利用模型物化和重用，合并物化主题模型构建近似结果，引入分层计划搜索策略和查询重排序技术，集成到可视化分析原型系统。

Result: 大量实验表明MLego显著降低计算成本，同时保持高质量主题建模结果。

Conclusion: MLego增强现有可视化分析方法，实现实时、查询驱动探索，弥补可扩展主题建模和交互式数据分析的差距。

Abstract: With massive texts on social media, users and analysts often rely on topic
modeling techniques to quickly extract key themes and gain insights.
Traditional topic modeling techniques, such as Latent Dirichlet Allocation
(LDA), provide valuable insights but are computationally expensive, making them
impractical for real-time data analysis. Although recent advances in
distributed training and fast sampling methods have improved efficiency,
real-time topic exploration remains a significant challenge. In this paper, we
present MLego, an interactive query framework designed to support real-time
topic modeling analysis by leveraging model materialization and reuse. Instead
of retraining models from scratch, MLego efficiently merges materialized topic
models to construct approximate results at interactive speeds. To further
enhance efficiency, we introduce a hierarchical plan search strategy for single
queries and an optimized query reordering technique for batch queries. We
integrate MLego into a visual analytics prototype system, enabling users to
explore large-scale textual datasets through interactive queries. Extensive
experiments demonstrate that MLego significantly reduces computation costs
while maintaining high-quality topic modeling results. MLego enhances existing
visual analytics approaches, which primarily focus on user-driven topic
modeling, by enabling real-time, query-driven exploration. This complements
traditional methods and bridges the gap between scalable topic modeling and
interactive data analysis.

</details>


### [79] [TQL: Towards Type-Driven Data Discovery](https://arxiv.org/abs/2508.08054)
*Andrew Kang,Sainyam Galhotra*

Main category: cs.DB

TL;DR: 提出以语言驱动方法重新调整数据发现系统优先级，介绍TQL查询语言，定义其语法、语义并与现有语言比较。


<details>
  <summary>Details</summary>
Motivation: 现有数据发现查询语言以系统驱动设计，强调数据库特性而忽视用户需求，需重新调整客户端优先级。

Method: 引入语言驱动方法，设计包含类型系统的灵活实用查询语言TQL，明确定义其语法、语义及评估模型并给出实现概要。

Result: 定义了TQL的语法、语义和评估模型，给出实现概要，并与现有语言进行了比较。

Conclusion: TQL在实际场景中具有更强的表达能力，能更好满足数据发现需求。

Abstract: Existing query languages for data discovery exhibit system-driven designs
that emphasize database features and functionality over user needs. We propose
a re-prioritization of the client through an introduction of a language-driven
approach to data discovery systems that can leverage powerful results from
programming languages research. In this paper, we describe TQL, a flexible and
practical query language which incorporates a type-like system to encompass
downstream transformation-context in its discovery queries. The syntax and
semantics of TQL (including the underlying evaluation model), are formally
defined, and a sketch of its implementation is also provided. Additionally, we
provide comparisons to existing languages for data retrieval and data discovery
to examine the advantages of TQL's expanded expressive power in real-life
settings.

</details>


### [80] [Towards General-Purpose Data Discovery: A Programming Languages Approach](https://arxiv.org/abs/2508.08074)
*Andrew Kang,Yashnil Saha,Sainyam Galhotra*

Main category: cs.DB

TL;DR: 提出用于数据发现的特定领域语言TQL，用代数模型刻画核心语言并实现概念验证系统原型


<details>
  <summary>Details</summary>
Motivation: 通用数据发现工具发展的主要瓶颈是缺乏表达性的形式语言及对应实现来刻画和解决通用发现查询

Method: 通过代数模型Imperative Relational Algebra with Types (ImpRAT)全面正式地刻画核心语言，并实现模块化概念验证系统原型

Result: 实现了TQL及概念验证系统原型

Conclusion: 提出的TQL能利用编程语言研究成果解决数据发现中语言表达问题

Abstract: Efficient and effective data discovery is critical for many modern
applications in machine learning and data science. One major bottleneck to the
development of a general-purpose data discovery tool is the absence of an
expressive formal language, and corresponding implementation, for
characterizing and solving generic discovery queries. To this end, we present
TQL, a domain-specific language for data discovery well-designed to leverage
and exploit the results of programming languages research in both its syntax
and semantics. In this paper, we fully and formally characterize the core
language through an algebraic model, Imperative Relational Algebra with Types
(ImpRAT), and implement a modular proof-of-concept system prototype.

</details>


### [81] [Heterogeneity in Entity Matching: A Survey and Experimental Analysis](https://arxiv.org/abs/2508.08076)
*Mohammad Hossein Moslemi,Amir Mousavi,Behshid Behkamal,Mostafa Milani*

Main category: cs.DB

TL;DR: 本文对异构实体匹配（HEM）进行综述，提出分类法，连接FAIR原则，评估现有方法并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 实际中数据集结构、格式、模式和语义差异大，给实体匹配（EM）带来挑战，需要对异构实体匹配（HEM）进行研究。

Method: 引入基于先前工作的分类法区分两种主要异质性类别及子类型；将框架与FAIR原则相联系；批判性回顾近期EM方法；对先进模型进行针对性实验。

Result: 分析发现当前方法存在持续局限性。

Conclusion: 指出未来研究方向，包括多模态匹配、人在环工作流、与大语言模型和知识图谱深度集成、异构环境下的公平感知评估。

Abstract: Entity matching (EM) is a fundamental task in data integration and analytics,
essential for identifying records that refer to the same real-world entity
across diverse sources. In practice, datasets often differ widely in structure,
format, schema, and semantics, creating substantial challenges for EM. We refer
to this setting as Heterogeneous EM (HEM). This survey offers a unified
perspective on HEM by introducing a taxonomy, grounded in prior work, that
distinguishes two primary categories -- representation and semantic
heterogeneity -- and their subtypes. The taxonomy provides a systematic lens
for understanding how variations in data form and meaning shape the complexity
of matching tasks. We then connect this framework to the FAIR principles --
Findability, Accessibility, Interoperability, and Reusability -- demonstrating
how they both reveal the challenges of HEM and suggest strategies for
mitigating them. Building on this foundation, we critically review recent EM
methods, examining their ability to address different heterogeneity types, and
conduct targeted experiments on state-of-the-art models to evaluate their
robustness and adaptability under semantic heterogeneity. Our analysis uncovers
persistent limitations in current approaches and points to promising directions
for future research, including multimodal matching, human-in-the-loop
workflows, deeper integration with large language models and knowledge graphs,
and fairness-aware evaluation in heterogeneous settings.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [82] [PiKV: KV Cache Management System for Mixture of Experts](https://arxiv.org/abs/2508.06526)
*Dong Liu,Yanxuan Yu,Ben Lengerich,Ying Nian Wu,Xuhong Wang*

Main category: cs.DC

TL;DR: 随着大语言模型规模和上下文长度增加，KV缓存存储成本成瓶颈，论文提出PiKV框架解决问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型中KV缓存存储的内存和通信成本成为多GPU和多节点推理的主要瓶颈，MoE架构下KV缓存仍存在显著开销。

Method: 引入PiKV框架，利用专家分片KV存储、PiKV路由、PiKV调度和PiKV压缩模块。

Result: PiKV作为开源软件库发布，可与Nvidia kvpress集成加速。

Conclusion: PiKV旨在成为MoE架构全面的KV缓存管理系统。

Abstract: As large language models continue to scale up in both size and context
length, the memory and communication cost of key-value (KV) cache storage has
become a major bottleneck in multi-GPU and multi-node inference. While
MoE-based architectures sparsify computation across experts, the corresponding
KV caches remain dense and globally synchronized, resulting in significant
overhead.
  We introduce \textbf{PiKV}, a parallel and distributed KV cache serving
framework tailored for MoE architecture. PiKV leverages \textit{expert-sharded
KV storage} to partition caches across GPUs, \textit{PiKV routing} to reduce
token-to-KV access, and a \textit{PiKV Scheduling} to adaptively retain
query-relevant entries. To further reduce memory usage, PiKV integrates
\textit{PiKV Compression} modules the caching pipeline for acceleration.
  PiKV is recently publicly available as an open-source software library:
\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.
Experiments details is recorded at:
\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\_Results}.
We also have PiKV integrated with Nvidia kvpress for acceleration, details see
\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.
PiKV is still a living project, aiming to become a comprehesive KV Cache
management system for MoE Architectures.

</details>


### [83] [Kairos: Low-latency Multi-Agent Serving with Shared LLMs and Excessive Loads in the Public Cloud](https://arxiv.org/abs/2508.06948)
*Jinyuan Chen,Jiuchen Shi,Quan Chen,Minyi Guo*

Main category: cs.DC

TL;DR: 本文提出多智能体编排系统Kairos，优化多智能体应用端到端延迟，实验表明比现有技术减少17.8% - 28.4%延迟。


<details>
  <summary>Details</summary>
Motivation: 现有工作在多智能体应用中服务性能低，未考虑智能体间延迟和资源差异进行请求调度，共享大语言模型负载过高。

Method: 提出Kairos系统，包含工作流编排器、工作流感知优先级调度器和内存感知调度器。编排器收集信息分析工作流，调度器根据延迟特性决定请求优先级，调度器根据内存需求分发请求。

Result: Kairos相比现有技术将端到端延迟降低了17.8%到28.4%。

Conclusion: Kairos系统能有效优化多智能体应用的端到端延迟。

Abstract: Multi-agent applications utilize the advanced capabilities of large language
models (LLMs) for intricate task completion through agent collaboration in a
workflow. Under this situation, requests from different agents usually access
the same shared LLM to perform different kinds of tasks, forcing the shared LLM
to suffer excessive loads. However, existing works have low serving performance
for these multi-agent applications, mainly due to the ignorance of inter-agent
latency and resource differences for request scheduling. We therefore propose
Kairos, a multi-agent orchestration system that optimizes end-to-end latency
for multi-agent applications. Kairos consists of a workflow orchestrator, a
workflow-aware priority scheduler, and a memory-aware dispatcher. The
orchestrator collects agent-specific information for online workflow analysis.
The scheduler decides the serving priority of the requests based on their
latency characteristics to reduce the overall queuing. The dispatcher
dispatches the requests to different LLM instances based on their memory
demands to avoid GPU overloading. Experimental results show that Kairos reduces
end-to-end latency by 17.8% to 28.4% compared to state-of-the-art works.

</details>


### [84] [Convergence Sans Synchronization](https://arxiv.org/abs/2508.06949)
*Arya Tanmay Gupta*

Main category: cs.DC

TL;DR: 论文提出保证多处理器算法异步收敛的理论，开发无需同步的算法，证明异步收敛可通过局部状态图判断，实验显示算法收敛时间显著减少。


<details>
  <summary>Details</summary>
Motivation: 多处理器系统使用和规模增加，社区对快速并行处理算法需求大，但多数算法同步机制成本高，需寻找异步收敛方法。

Method: 提出解释多处理器算法异步收敛必要充分属性的理论，开发无需同步的算法，证明现有算法可异步执行。

Result: 通过证明局部状态图形成偏序即可判断算法异步收敛，降低证明复杂度；实验表明设计的算法收敛时间显著减少。

Conclusion: 提出的理论可有效降低判断算法异步收敛的复杂度，设计的算法在收敛时间上有显著优势。

Abstract: We currently see a steady rise in the usage and size of multiprocessor
systems, and so the community is evermore interested in developing fast
parallel processing algorithms. However, most algorithms require a
synchronization mechanism, which is costly in terms of computational resources
and time. If an algorithm can be executed in asynchrony, then it can use all
the available computation power, and the nodes can execute without being
scheduled or locked. However, to show that an algorithm guarantees convergence
in asynchrony, we need to generate the entire global state transition graph and
check for the absence of cycles. This takes time exponential in the size of the
global state space. In this dissertation, we present a theory that explains the
necessary and sufficient properties of a multiprocessor algorithm that
guarantees convergence even without synchronization. We develop algorithms for
various problems that do not require synchronization. Additionally, we show for
several existing algorithms that they can be executed without any
synchronization mechanism. A significant theoretical benefit of our work is in
proving that an algorithm can converge even in asynchrony. Our theory implies
that we can make such conclusions about an algorithm, by only showing that the
local state transition graph of a computing node forms a partial order, rather
than generating the entire global state space and determining the absence of
cycles in it. Thus, the complexity of rendering such proofs, formal or social,
is phenomenally reduced. Experiments show a significant reduction in time taken
to converge, when we compare the execution time of algorithms in the literature
versus the algorithms that we design. We get similar results when we run an
algorithm, that guarantees convergence in asynchrony, under a scheduler versus
in asynchrony.

</details>


### [85] [The Fused Kernel Library: A C++ API to Develop Highly-Efficient GPU Libraries](https://arxiv.org/abs/2508.07071)
*Oscar Amoros,Albert Andaluz,Johnny Nunez,Antonio J. Pena*

Main category: cs.DC

TL;DR: 现有GPU库链式调用函数时难以充分利用资源，本文提出新方法实现按需自动水平和垂直融合，开源实现展示显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有GPU库链式调用函数难以利用资源，现有内核融合技术需手动创建融合内核，限制应用场景且增加开发成本。

Method: 定义可复用、可融合组件，用户通过高级编程接口组合，利用C++17元编程特性在编译时生成优化融合内核。

Result: 开源实现对比传统库在多个基准测试中有显著加速，性能提升2倍到超1000倍。

Conclusion: 该方法有效提升GPU性能，同时保留高级可编程性。

Abstract: Existing GPU libraries often struggle to fully exploit the parallel resources
and on-chip memory (SRAM) of GPUs when chaining multiple GPU functions as
individual kernels. While Kernel Fusion (KF) techniques like Horizontal Fusion
(HF) and Vertical Fusion (VF) can mitigate this, current library
implementations often require library developers to manually create fused
kernels. Hence, library users rely on limited sets of pre-compiled or
template-based fused kernels. This limits the use cases that can benefit from
HF and VF and increases development costs. In order to solve these issues, we
present a novel methodology for building GPU libraries that enables automatic
on-demand HF and VF for arbitrary combinations of GPU library functions. Our
methodology defines reusable, fusionable components that users combine via
high-level programming interfaces. Leveraging C++17 metaprogramming features
available in compilers like nvcc, our methodology generates a single and
optimized fused kernel tailored to the user's specific sequence of operations
at compile time, without needing a custom compiler or manual development and
pre-compilation of kernel combinations. This approach abstracts low-level GPU
complexities while maximizing GPU resource utilization and keeping intermediate
data in SRAM. We provide an open-source implementation demonstrating
significant speedups compared to traditional libraries in various benchmarks,
validating the effectiveness of this methodology for improving GPU performance
in the range of 2x to more than 1000x, while preserving high-level
programmability.

</details>


### [86] [AerialDB: A Federated Peer-to-Peer Spatio-temporal Edge Datastore for Drone Fleets](https://arxiv.org/abs/2508.07124)
*Shashwat Jaiswal,Suman Raj,Subhajit Sidhanta,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 提出轻量级去中心化数据存储查询系统AerialDB，可在多无人机系统存储处理时间序列数据，能高效扩展，性能表现优。


<details>
  <summary>Details</summary>
Motivation: 无人机收集的数据远超机载计算机处理能力，需实时卸载到边缘和云端处理，现有系统难以满足灾害管理应用需求。

Method: 设计AerialDB，采用轻量级内容副本放置和分片索引技术，有去中心化、位置感知的分布式执行引擎。

Result: 通过最多400架无人机和80个边缘节点的容器化部署验证，AerialDB能高效扩展，近实时处理不同工作负载，性能优于现有基线。

Conclusion: AerialDB在多无人机系统中能高效处理时空数据，具有可扩展性、低延迟和高性能等优势。

Abstract: Recent years have seen an unprecedented growth in research that leverages the
newest computing paradigm of Internet of Drones, comprising a fleet of
connected Unmanned Aerial Vehicles (UAVs) used for a wide range of tasks such
as monitoring and analytics in highly mobile and changing environments
characteristic of disaster regions. Given that the typical data (i.e., videos
and images) collected by the fleet of UAVs deployed in such scenarios can be
considerably larger than what the onboard computers can process, the UAVs need
to offload their data in real-time to the edge and the cloud for further
processing. To that end, we present the design of AerialDB - a lightweight
decentralized data storage and query system that can store and process time
series data on a multi-UAV system comprising: A) a fleet of hundreds of UAVs
fitted with onboard computers, and B) ground-based edge servers connected
through a cellular link. Leveraging lightweight techniques for content-based
replica placement and indexing of shards, AerialDB has been optimized for
efficient processing of different possible combinations of typical spatial and
temporal queries performed by real-world disaster management applications.
Using containerized deployment spanning up to 400 drones and 80 edges, we
demonstrate that AerialDB is able to scale efficiently while providing near
real-time performance with different realistic workloads. Further, AerialDB
comprises a decentralized and locality-aware distributed execution engine which
provides graceful degradation of performance upon edge failures with relatively
low latency while processing large spatio-temporal data. AerialDB exhibits
comparable insertion performance and 100 times improvement in query performance
against state-of-the-art baseline. Moreover, it exhibits a 10 times and 100
times improvement with insertion and query workloads respectively over the
cloud baseline.

</details>


### [87] [FlashMP: Fast Discrete Transform-Based Solver for Preconditioning Maxwell's Equations on GPUs](https://arxiv.org/abs/2508.07193)
*Haoyuan Zhang,Yaqian Gao,Xinxin Zhang,Jialin Li,Runfeng Jin,Yidong Chen,Feng Zhang,Wu Yuan,Wenpeng Ma,Shan Liang,Jian Zhang,Zhonghua Lu*

Main category: cs.DC

TL;DR: 提出新预条件系统FlashMP解决电磁模拟中线性系统求解问题，在GPU集群评估效果好。


<details>
  <summary>Details</summary>
Motivation: 现有迭代求解器收敛慢，近似预条件器收敛不足，直接求解器内存需求大，需高效方法解决电磁模拟中线性系统求解问题。

Method: 提出FlashMP，基于离散变换设计子域精确求解器，实现高效GPU版本并通过域分解实现多GPU可扩展性。

Result: 在AMD MI60 GPU集群评估，迭代次数最多减少16倍，加速比2.5 - 4.9倍，弱可扩展性测试并行效率达84.1%。

Conclusion: FlashMP能有效解决电磁模拟中线性系统求解问题，提升求解效率。

Abstract: Efficiently solving large-scale linear systems is a critical challenge in
electromagnetic simulations, particularly when using the Crank-Nicolson
Finite-Difference Time-Domain (CN-FDTD) method. Existing iterative solvers are
commonly employed to handle the resulting sparse systems but suffer from slow
convergence due to the ill-conditioned nature of the double-curl operator.
Approximate preconditioners, like Successive Over-Relaxation (SOR) and
Incomplete LU decomposition (ILU), provide insufficient convergence, while
direct solvers are impractical due to excessive memory requirements. To address
this, we propose FlashMP, a novel preconditioning system that designs a
subdomain exact solver based on discrete transforms. FlashMP provides an
efficient GPU implementation that achieves multi-GPU scalability through domain
decomposition. Evaluations on AMD MI60 GPU clusters (up to 1000 GPUs) show that
FlashMP reduces iteration counts by up to 16x and achieves speedups of 2.5x to
4.9x compared to baseline implementations in state-of-the-art libraries such as
Hypre. Weak scalability tests show parallel efficiencies up to 84.1%.

</details>


### [88] [An Experimental Exploration of In-Memory Computing for Multi-Layer Perceptrons](https://arxiv.org/abs/2508.07317)
*Pedro Carrinho,Hamid Moghadaspour,Oscar Ferraz,João Dinis Ferreira,Yann Falevoz,Vitor Silva,Gabriel Falcao*

Main category: cs.DC

TL;DR: 本文分析通用处理内存（PiM）架构加速神经网络的潜力，以UPMEM PiM系统为例，与CPU、GPU对比，结果显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现代计算机架构中，内存密集型工作负载受数据移动瓶颈限制，处理内存计算范式旨在缓解该问题，本文旨在分析通用PiM架构加速神经网络的潜力。

Method: 选择UPMEM PiM系统，将多层感知器（MLP）在PiM上的实现与英特尔至强CPU上的顺序基线进行比较；使用UPMEM的工作SRAM实现两个较小的MLP，与低功耗英伟达Jetson GPU对比。

Result: UPMEM实现对于大批量推理性能比CPU高259倍；使用WRAM实现MLP推理的内核执行时间低于3毫秒，与低功耗GPU处于同一数量级。

Conclusion: 现代通用PiM架构在加速神经网络方面有较大潜力，UPMEM PiM系统性能表现良好。

Abstract: In modern computer architectures, the performance of many memory-bound
workloads (e.g., machine learning, graph processing, databases) is limited by
the data movement bottleneck that emerges when transferring large amounts of
data between the main memory and the central processing unit (CPU).
Processing-in-memory is an emerging computing paradigm that aims to alleviate
this data movement bottleneck by performing computation close to or within the
memory units, where data resides. One example of a prevalent workload whose
performance is bound by the data movement bottleneck is the training and
inference process of artificial neural networks. In this work, we analyze the
potential of modern general-purpose PiM architectures to accelerate neural
networks. To this end, we selected the UPMEM PiM system, the first commercially
available real-world general-purpose PiM architecture. We compared the
implementation of multilayer perceptrons (MLPs) in PiM with a sequential
baseline running on an Intel Xeon CPU. The UPMEM implementation achieves up to
$259\times$ better performance for inference of large batch sizes when compared
against the CPU that exploits the size of the available PiM memory.
Additionally, two smaller MLPs were implemented using UPMEM's working SRAM
(WRAM), a scratchpad memory, to evaluate their performance against a low-power
Nvidia Jetson graphics processing unit (GPU), providing further insights into
the efficiency of UPMEM's PiM for neural network inference. Results show that
using WRAM achieves kernel execution times for MLP inference of under $3$ ms,
which is within the same order of magnitude as low-power GPUs.

</details>


### [89] [Taming Cold Starts: Proactive Serverless Scheduling with Model Predictive Control](https://arxiv.org/abs/2508.07640)
*Chanh Nguyen,Monowar Bhuyan,Erik Elmroth*

Main category: cs.DC

TL;DR: 提出基于模型预测控制的预测性无服务器调度框架减轻冷启动问题，在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算存在冷启动问题，影响平台性能，需要解决该问题以提升端到端响应时间。

Method: 基于模型预测控制构建预测性无服务器调度框架，通过预测未来调用联合优化容器预预热和请求调度。

Result: 在Apache OpenWhisk上实现该方法，实验显示相比现有基线，尾延迟最多降低85%，资源使用减少34%。

Conclusion: 所提出的方法能有效减轻冷启动问题，提高延迟性能并降低资源开销。

Abstract: Serverless computing has transformed cloud application deployment by
introducing a fine-grained, event-driven execution model that abstracts away
infrastructure management. Its on-demand nature makes it especially appealing
for latency-sensitive and bursty workloads. However, the cold start problem,
i.e., where the platform incurs significant delay when provisioning new
containers, remains the Achilles' heel of such platforms.
  This paper presents a predictive serverless scheduling framework based on
Model Predictive Control to proactively mitigate cold starts, thereby improving
end-to-end response time. By forecasting future invocations, the controller
jointly optimizes container prewarming and request dispatching, improving
latency while minimizing resource overhead.
  We implement our approach on Apache OpenWhisk, deployed on a Kubernetes-based
testbed. Experimental results using real-world function traces and synthetic
workloads demonstrate that our method significantly outperforms
state-of-the-art baselines, achieving up to 85% lower tail latency and a 34%
reduction in resource usage.

</details>


### [90] [On the Efficiency of Dynamic Transaction Scheduling in Blockchain Sharding](https://arxiv.org/abs/2508.07472)
*Ramesh Adhikari,Costas Busch,Miroslav Popovic*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Sharding is a technique to speed up transaction processing in blockchains,
where the $n$ processing nodes in the blockchain are divided into $s$ disjoint
groups (shards) that can process transactions in parallel. We study dynamic
scheduling problems on a shard graph $G_s$ where transactions arrive online
over time and are not known in advance. Each transaction may access at most $k$
shards, and we denote by $d$ the worst distance between a transaction and its
accessing (destination) shards (the parameter $d$ is unknown to the shards). To
handle different values of $d$, we assume a locality sensitive decomposition of
$G_s$ into clusters of shards, where every cluster has a leader shard that
schedules transactions for the cluster. We first examine the simpler case of
the stateless model, where leaders are not aware of the current state of the
transaction accounts, and we prove a $O(d \log^2 s \cdot \min\{k, \sqrt{s}\})$
competitive ratio for latency. We then consider the stateful model, where
leader shards gather the current state of accounts, and we prove a $O(\log
s\cdot \min\{k, \sqrt{s}\}+\log^2 s)$ competitive ratio for latency. Each
leader calculates the schedule in polynomial time for each transaction that it
processes. We show that for any $\epsilon > 0$, approximating the optimal
schedule within a $(\min\{k, \sqrt{s}\})^{1 -\epsilon}$ factor is NP-hard.
Hence, our bound for the stateful model is within a poly-log factor from the
best possibly achievable. To the best of our knowledge, this is the first work
to establish provably efficient dynamic scheduling algorithms for blockchain
sharding systems.

</details>


### [91] [Coordinated Power Management on Heterogeneous Systems](https://arxiv.org/abs/2508.07605)
*Zhong Zheng,Michael E. Papka,Zhiling Lan*

Main category: cs.DC

TL;DR: 提出OPEN框架用于异构计算系统性能预测，在多系统评估中准确率达98.29%，能降低分析成本。


<details>
  <summary>Details</summary>
Motivation: 传统性能建模方法依赖离线分析，在大规模应用中因设置空间大、成本高而不实用。

Method: 框架分离线和在线阶段，离线构建性能预测器和初始密集矩阵，在线进行轻量级分析并结合协同过滤预测性能。

Result: 在多个异构系统评估中，OPEN预测准确率达98.29%。

Conclusion: OPEN能有效降低分析成本并保持高精度，为功率受限下性能预测提供轻量级解决方案。

Abstract: Performance prediction is essential for energy-efficient computing in
heterogeneous computing systems that integrate CPUs and GPUs. However,
traditional performance modeling methods often rely on exhaustive offline
profiling, which becomes impractical due to the large setting space and the
high cost of profiling large-scale applications. In this paper, we present
OPEN, a framework consists of offline and online phases. The offline phase
involves building a performance predictor and constructing an initial dense
matrix. In the online phase, OPEN performs lightweight online profiling, and
leverages the performance predictor with collaborative filtering to make
performance prediction. We evaluate OPEN on multiple heterogeneous systems,
including those equipped with A100 and A30 GPUs. Results show that OPEN
achieves prediction accuracy up to 98.29\%. This demonstrates that OPEN
effectively reduces profiling cost while maintaining high accuracy, making it
practical for power-aware performance modeling in modern HPC environments.
Overall, OPEN provides a lightweight solution for performance prediction under
power constraints, enabling better runtime decisions in power-aware computing
environments.

</details>


### [92] [Perpetual exploration in anonymous synchronous networks with a Byzantine black hole](https://arxiv.org/abs/2508.07703)
*Adri Bhattacharya,Pritam Goswami,Evangelos Bampas,Partha Sarathi Mandal*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we investigate: ``How can a group of initially co-located
mobile agents perpetually explore an unknown graph, when one stationary node
occasionally behaves maliciously, under an adversary's control?'' We call this
node a ``Byzantine black hole (BBH)'' and at any given round it may choose to
destroy all visiting agents, or none. This subtle power can drastically
undermine classical exploration strategies designed for an always active black
hole. We study this perpetual exploration problem in the presence of at most
one BBH, without initial knowledge of the network size. Since the underlying
graph may be 1-connected, perpetual exploration of the entire graph may be
infeasible. We thus define two variants: \pbmPerpExpl\ and \pbmPerpExplHome. In
the former, the agents are tasked to perform perpetual exploration of at least
one component, obtained after the exclusion of the BBH. In the latter, the
agents are tasked to perform perpetual exploration of the component which
contains the \emph{home} node, where agents are initially co-located.
Naturally, \pbmPerpExplHome\ is a special case of \pbmPerpExpl. Agents operate
under a synchronous scheduler and communicate in a face-to-face model. Our goal
is to determine the minimum number of agents necessary and sufficient to solve
these problems. In acyclic networks, we obtain optimal algorithms that solve
\pbmPerpExpl\ with $4$ agents, and \pbmPerpExplHome\ with $6$ agents in trees.
The lower bounds hold even in path graphs. In general graphs, we give a
non-trivial lower bound of $2\Delta-1$ agents for \pbmPerpExpl, and an upper
bound of $3\Delta+3$ agents for \pbmPerpExplHome. To our knowledge, this is the
first study of a black-hole variant in arbitrary networks without initial
topological knowledge.

</details>


### [93] [Over-the-Top Resource Broker System for Split Computing: An Approach to Distribute Cloud Computing Infrastructure](https://arxiv.org/abs/2508.07744)
*Ingo Friese,Jochen Klaffer,Mandy Galkow-Schneider,Sergiy Melnyk,Qiuheng Zhou,Hans Dieter Schotten*

Main category: cs.DC

TL;DR: 本文探讨6G网络架构中资源分配的OTP代理角色，分析其在两种拆分计算场景的作用并给出概念验证实现。


<details>
  <summary>Details</summary>
Motivation: 6G网络架构带来创新服务和能力，需解决资源无缝访问、多提供商协作及性能保障等问题，简化服务部署。

Method: 引入OTP代理进行资源分配，探讨其在两种拆分计算场景的作用，进行概念验证实现。

Result: 代理可抽象基础设施复杂性，是适用于云、网络等环境的通用解决方案。

Conclusion: OTP代理在6G网络资源分配中具有重要作用，能简化服务部署。

Abstract: 6G network architectures will usher in a wave of innovative services and
capabilities, introducing concepts like split computing and dynamic processing
nodes. This implicates a paradigm where accessing resources seamlessly aligns
with diverse processing node characteristics, ensuring a uniform interface. In
this landscape, the identity of the operator becomes inconsequential, paving
the way for a collaborative ecosystem where multiple providers contribute to a
shared pool of resources. At the core of this vision is the guarantee of
specific performance parameters, precisely tailored to the location and service
requirements. A consistent layer, as the abstraction of the complexities of
different infrastructure providers, is needed to simplify service deployment.
One promising approach is the introduction of an over-the-top broker for
resource allocation, which streamlines the integration of these services into
the network and cloud infrastructure of the future. This paper explores the
role of the broker in two split computing scenarios. By abstracting the
complexities of various infrastructures, the broker proves to be a versatile
solution applicable not only to cloud environments but also to networks and
beyond. Additionally, a detailed discussion of a proof-of-concept
implementation provides insights into the broker's actual architectural
framework.

</details>


### [94] [Towards Lock Modularization for Heterogeneous Environments](https://arxiv.org/abs/2508.07756)
*Hanze Zhang,Rong Chen,Haibo Chen*

Main category: cs.DC

TL;DR: 本文针对异构硬件环境中锁使用的挑战，提出锁模块化方法以充分利用资源。


<details>
  <summary>Details</summary>
Motivation: 现代异构硬件环境下，高效采用锁存在挑战，现有解决方案无法充分利用资源。

Method: 引入分解锁到硬件组件的设计原则，提出锁模块化方法，将锁分解为独立模块并分配到合适硬件组件。

Result: 未提及

Conclusion: 未提及

Abstract: Modern hardware environments are becoming increasingly heterogeneous, leading
to the emergence of applications specifically designed to exploit this
heterogeneity. Efficiently adopting locks in these applications poses distinct
challenges. The uneven distribution of resources in such environments can
create bottlenecks for lock operations, severely hindering application
performance. Existing solutions are often tailored to specific types of
hardware, which underutilizes resources on other components within
heterogeneous environments.
  This paper introduces a new design principle: decomposing locks across
hardware components to fully utilize unevenly distributed resources in
heterogeneous environments. Following this principle, we propose lock
modularization, a systematic approach that decomposes a lock into independent
modules and assigns them to appropriate hardware components. This approach
aligns the resource requirements of lock modules with the attributes of
specific hardware components, maximizing strengths while minimizing weaknesses.

</details>


### [95] [Performance Evaluation of Brokerless Messaging Libraries](https://arxiv.org/abs/2508.07934)
*Lorenzo La Corte,Syed Aftab Rashid,Andrei-Marian Dan*

Main category: cs.DC

TL;DR: 论文聚焦无代理消息系统，定性分析后设计开源基准套件评估ZeroMQ、NanoMsg和NNG库性能，为从业者选择合适库提供依据。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注有代理消息系统性能评估，无代理消息系统性能评估研究稀缺，故开展对无代理消息系统的研究。

Method: 先对候选系统进行定性分析，找出最有前景的系统，再设计并实现开源基准套件，从不同指标和工作负载条件评估所选库。

Result: 对ZeroMQ、NanoMsg和NNG库进行评估，并发现其局限性。

Conclusion: 研究有助于从业者根据自身需求选择最合适的库。

Abstract: Messaging systems are essential for efficiently transferring large volumes of
data, ensuring rapid response times and high-throughput communication. The
state-of-the-art on messaging systems mainly focuses on the performance
evaluation of brokered messaging systems, which use an intermediate broker to
guarantee reliability and quality of service. However, over the past decade,
brokerless messaging systems have emerged, eliminating the single point of
failure and trading off reliability guarantees for higher performance. Still,
the state-of-the-art on evaluating the performance of brokerless systems is
scarce. In this work, we solely focus on brokerless messaging systems. First,
we perform a qualitative analysis of several possible candidates, to find the
most promising ones. We then design and implement an extensive open-source
benchmarking suite to systematically and fairly evaluate the performance of the
chosen libraries, namely, ZeroMQ, NanoMsg, and NanoMsg-Next-Generation (NNG).
We evaluate these libraries considering different metrics and workload
conditions, and provide useful insights into their limitations. Our analysis
enables practitioners to select the most suitable library for their
requirements.

</details>


### [96] [Optimizing Federated Learning for Scalable Power-demand Forecasting in Microgrids](https://arxiv.org/abs/2508.08022)
*Roopkatha Banerjee,Sampath Koti,Gyanendra Singh,Anirban Chakraborty,Gurunath Gurrala,Bhushan Jagyasi,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 本文为微电网和城市级公用事业的时间序列需求预测开发并评估了联邦学习（FL）训练的优化方案，提高预测准确性并降低成本。


<details>
  <summary>Details</summary>
Motivation: 实时监测城市和微电网的电力消耗可帮助预测需求和优化电网运营，但将数据移至云端分析会暴露活动模式，且现有FL存在数据非独立同分布和计算成本高等挑战。

Method: 开发并评估了跨边缘和云的FL训练优化方案，使用指数加权损失进行训练。

Result: 通过对美国三个州超千个客户端验证，在伪分布式设置和Pi边缘集群中进行FL，结果显示所提方法优于ARIMA和为单个消费者训练的DNN等基线方法。

Conclusion: 所提优化方案能在微电网和城市级公用事业的需求预测中实现高预测准确性并降低训练成本。

Abstract: Real-time monitoring of power consumption in cities and micro-grids through
the Internet of Things (IoT) can help forecast future demand and optimize grid
operations. But moving all consumer-level usage data to the cloud for
predictions and analysis at fine time scales can expose activity patterns.
Federated Learning~(FL) is a privacy-sensitive collaborative DNN training
approach that retains data on edge devices, trains the models on private data
locally, and aggregates the local models in the cloud. But key challenges
exist: (i) clients can have non-independently identically distributed~(non-IID)
data, and (ii) the learning should be computationally cheap while scaling to
1000s of (unseen) clients. In this paper, we develop and evaluate several
optimizations to FL training across edge and cloud for time-series demand
forecasting in micro-grids and city-scale utilities using DNNs to achieve a
high prediction accuracy while minimizing the training cost. We showcase the
benefit of using exponentially weighted loss while training and show that it
further improves the prediction of the final model. Finally, we evaluate these
strategies by validating over 1000s of clients for three states in the US from
the OpenEIA corpus, and performing FL both in a pseudo-distributed setting and
a Pi edge cluster. The results highlight the benefits of the proposed methods
over baselines like ARIMA and DNNs trained for individual consumers, which are
not scalable.

</details>


### [97] [On the Operational Resilience of CBDC: Threats and Prospects of Formal Validation for Offline Payments](https://arxiv.org/abs/2508.08064)
*Marco Bernardo,Federico Calandra,Andrea Esposito,Francesco Fabris*

Main category: cs.DC

TL;DR: 因理论计算机科学限制软件质量难认证，CBDC时代小错误或致金融崩溃，提倡用形式化方法验证CBDC软件基础设施运营弹性，尤其关注离线支付。


<details>
  <summary>Details</summary>
Motivation: 在CBDC时代，即使小软件错误也可能引发金融崩溃，而现有理论限制软件质量认证。

Method: 使用形式化方法对支持CBDC的软件基础设施的运行弹性进行验证。

Result: 未提及明确结果。

Conclusion: 应使用形式化方法验证CBDC软件基础设施的运营弹性，特别关注离线支付。

Abstract: Information and communication technologies are by now employed in most
activities, including economics and finance. Despite the extraordinary power of
modern computers and the vast amount of memory, some results of theoretical
computer science imply the impossibility of certifying software quality in
general. With the exception of safety-critical systems, this has primarily
concerned the information processed by confined systems, with limited
socio-economic consequences. In the emerging era of technologies for exchanging
digital money and tokenized assets over the Internet - such as central bank
digital currencies (CBDCs) - even a minor bug could trigger a financial
collapse. Although the aforementioned impossibility results cannot be overcome
in an absolute sense, there exist formal methods that can provide assertions of
computing systems correctness. We advocate their use to validate the
operational resilience of software infrastructures enabling CBDCs, with special
emphasis on offline payments as they constitute a very critical issue.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [98] [A Tight Lower Bound for the Approximation Guarantee of Higher-Order Singular Value Decomposition](https://arxiv.org/abs/2508.06693)
*Matthew Fahrbach,Mehrdad Ghadiri*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We prove that the classic approximation guarantee for the higher-order
singular value decomposition (HOSVD) is tight by constructing a tensor for
which HOSVD achieves an approximation ratio of $N/(1+\varepsilon)$, for any
$\varepsilon > 0$. This matches the upper bound of De Lathauwer et al. (2000a)
and shows that the approximation ratio of HOSVD cannot be improved. Using a
more advanced construction, we also prove that the approximation guarantees for
the ST-HOSVD algorithm of Vannieuwenhoven et al. (2012) and higher-order
orthogonal iteration (HOOI) of De Lathauwer et al. (2000b) are tight by showing
that they can achieve their worst-case approximation ratio of $N / (1 +
\varepsilon)$, for any $\varepsilon > 0$.

</details>


### [99] [Approximating High-Dimensional Earth Mover's Distance as Fast as Closest Pair](https://arxiv.org/abs/2508.06774)
*Lorenzo Beretta,Vincent Cohen-Addad,Rajesh Jayaram,Erik Waingarten*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We give a reduction from $(1+\varepsilon)$-approximate Earth Mover's Distance
(EMD) to $(1+\varepsilon)$-approximate Closest Pair (CP). As a consequence, we
improve the fastest known approximation algorithm for high-dimensional EMD.
Here, given $p\in [1, 2]$ and two sets of $n$ points $X,Y \subseteq (\mathbb
R^d,\ell_p)$, their EMD is the minimum cost of a perfect matching between $X$
and $Y$, where the cost of matching two vectors is their $\ell_p$ distance.
Further, CP is the basic problem of finding a pair of points realizing $\min_{x
\in X, y\in Y} ||x-y||_p$. Our contribution is twofold: we show that if a
$(1+\varepsilon)$-approximate CP can be computed in time $n^{2-\phi}$, then a
$1+O(\varepsilon)$ approximation to EMD can be computed in time
$n^{2-\Omega(\phi)}$; plugging in the fastest known algorithm for CP [Alman,
Chan, Williams FOCS'16], we obtain a $(1+\varepsilon)$-approximation algorithm
for EMD running in time $n^{2-\tilde{\Omega}(\varepsilon^{1/3})}$ for
high-dimensional point sets, which improves over the prior fastest running time
of $n^{2-\Omega(\varepsilon^2)}$ [Andoni, Zhang FOCS'23]. Our main technical
contribution is a sublinear implementation of the Multiplicative Weights Update
framework for EMD. Specifically, we demonstrate that the updates can be
executed without ever explicitly computing or storing the weights; instead, we
exploit the underlying geometric structure to perform the updates implicitly.

</details>


### [100] [Controlling tail risk in two-slope ski rental](https://arxiv.org/abs/2508.06809)
*Qiming Cui,Michael Dinitz*

Main category: cs.DS

TL;DR: 研究带尾风险的两斜率滑雪租赁问题最优解，给出结构定理并设计算法计算解。


<details>
  <summary>Details</summary>
Motivation: 将带尾界的滑雪租赁问题研究拓展到两斜率版本，该版本更贴合现实场景。

Method: 先推导一系列结构定理刻画最优解特征，再利用定理设计贪婪结合二分查找的快速近似算法和基于线性规划的精确算法。

Result: 发现添加尾风险界后解结构不同，很多情况下无唯一最优解，设计了两种算法计算解。

Conclusion: 对带尾风险的两斜率滑雪租赁问题有深入研究，结构定理和算法为解决该问题提供有效方法。

Abstract: We study the optimal solution to a general two-slope ski rental problem with
a tail risk, i.e., the chance of the competitive ratio exceeding a value
$\gamma$ is bounded by $\delta$. This extends the recent study of tail bounds
for ski rental by [Dinitz et al. SODA 2024] to the two-slope version defined by
[Lotker et al. IPL 2008]. In this version, even after "buying," we must still
pay a rental cost at each time step, though it is lower after buying. This
models many real-world "rent-or-buy" scenarios where a one-time investment
decreases (but does not eliminate) the per-time cost.
  Despite this being a simple extension of the classical problem, we find that
adding tail risk bounds creates a fundamentally different solution structure.
For example, in our setting there is a possibility that we never buy in an
optimal solution (which can also occur without tail bounds), but more strangely
(and unlike the case without tail bounds or the classical case with tail
bounds) we also show that the optimal solution might need to have nontrivial
probabilities of buying even at finite points beyond the time corresponding to
the buying cost. Moreover, in many regimes there does not exist a unique
optimal solution. As our first contribution, we develop a series of structure
theorems to characterize some features of optimal solutions.
  The complex structure of optimal solutions makes it more difficult to develop
an algorithm to compute such a solution. As our second contribution, we utilize
our structure theorems to design two algorithms: one based on a greedy
algorithm combined with binary search that is fast but yields arbitrarily close
to optimal solutions, and a slower algorithm based on linear programming which
computes exact optimal solutions.

</details>


### [101] [A near-linear time approximation scheme for $(k,\ell)$-median clustering under discrete Fréchet distance](https://arxiv.org/abs/2508.07008)
*Anne Driemel,Jan Höckendorff,Ioannis Psarros,Christian Sohler*

Main category: cs.DS

TL;DR: 本文给出了离散Fréchet距离下(k,ℓ)-中位数问题的首个近线性时间(1 + ε)-近似算法，还改进了核心集构造。


<details>
  <summary>Details</summary>
Motivation: 解决离散Fréchet距离下(k,ℓ)-中位数问题，在ℓ和ε为常数但k可达Ω(n)的情况下找到高效算法。

Method: 引入新的离散Fréchet距离降维技术，并适配Cohen - Addad等人的算法到降维输入。

Result: 得到近线性时间(1 + ε)-近似算法，改进了核心集构造，使其大小与输入时间序列数量和复杂度无关。

Conclusion: 提出的方法有效解决了离散Fréchet距离下(k,ℓ)-中位数问题，并在核心集构造上取得改进。

Abstract: A time series of complexity $m$ is a sequence of $m$ real valued
measurements. The discrete Fr\'echet distance $d_{dF}(x,y)$ is a distance
measure between two time series $x$ and $y$ of possibly different complexity.
Given a set of $n$ time series represented as $m$-dimensional vectors over the
reals, the $(k,\ell)$-median problem under discrete Fr\'echet distance aims to
find a set $C$ of $k$ time series of complexity $\ell$ such that $$\sum_{x\in
P} \min_{c\in C} d_{dF}(x,c)$$ is minimized. In this paper, we give the first
near-linear time $(1+\varepsilon)$-approximation algorithm for this problem
when $\ell$ and $\varepsilon$ are constants but $k$ can be as large as
$\Omega(n)$. We obtain our result by introducing a new dimension reduction
technique for discrete Fr\'echet distance and then adapt an algorithm of
Cohen-Addad et al. (J. ACM 2021) to work on the dimension-reduced input. As a
byproduct we also improve the best coreset construction for $(k,\ell)$-median
under discrete Fr\'echet distance (Cohen-Addad et al., SODA 2025) and show that
its size can be independent of the number of input time series \emph{ and }
their complexity.

</details>


### [102] [Unbiased Insights: Optimal Streaming Algorithms for $\ell_p$ Sampling, the Forget Model, and Beyond](https://arxiv.org/abs/2508.07067)
*Honghao Lin,Hoai-An Nguyen,William Swartworth,David P. Woodruff*

Main category: cs.DS

TL;DR: 研究单遍仅插入数据流中的ℓp采样和频率矩估计，给出近最优空间复杂度采样器，设计近乎无偏估计器，解决开放问题，拓展模型并处理任意坐标函数。


<details>
  <summary>Details</summary>
Motivation: 解决单遍仅插入数据流中ℓp采样、频率矩估计问题，处理含遗忘操作数据流及相关开放问题，拓展模型并估计熵等。

Method: 构造近空间最优近似ℓp采样器，将构造拓展为连续ℓp采样器，利用采样器设计近乎无偏估计器，拓展模型并运用矩估计算法。

Result: 得到p∈(0, 2)时最优空间复杂度采样器，比先前工作有改进；解决Pavan等人提出的三个开放问题；能在新模型中估计熵和处理任意坐标函数。

Conclusion: 提出近最优算法解决数据流中ℓp采样、频率矩估计等一系列问题，拓展模型及应用。

Abstract: We study $\ell_p$ sampling and frequency moment estimation in a single-pass
insertion-only data stream. For $p \in (0,2)$, we present a nearly
space-optimal approximate $\ell_p$ sampler that uses $\widetilde{O}(\log n
\log(1/\delta))$ bits of space and for $p = 2$, we present a sampler with space
complexity $\widetilde{O}(\log^2 n \log(1/\delta))$. This space complexity is
optimal for $p \in (0, 2)$ and improves upon prior work by a $\log n$ factor.
We further extend our construction to a continuous $\ell_p$ sampler, which
outputs a valid sample index at every point during the stream.
  Leveraging these samplers, we design nearly unbiased estimators for $F_p$ in
data streams that include forget operations, which reset individual element
frequencies and introduce significant non-linear challenges. As a result, we
obtain near-optimal algorithms for estimating $F_p$ for all $p$ in this model,
originally proposed by Pavan, Chakraborty, Vinodchandran, and Meel [PODS'24],
resolving all three open problems they posed.
  Furthermore, we generalize this model to what we call the suffix-prefix
deletion model, and extend our techniques to estimate entropy as a corollary of
our moment estimation algorithms. Finally, we show how to handle arbitrary
coordinate-wise functions during the stream, for any $g \in \mathbb{G}$, where
$\mathbb{G}$ includes all (linear or non-linear) contraction functions.

</details>


### [103] [Optimizing Districting Plans to Maximize Majority-Minority Districts via IPs and Local Search](https://arxiv.org/abs/2508.07446)
*Daniel Brous,David Shmoys*

Main category: cs.DS

TL;DR: 本文提出基于整数规划的方法生成选区划分方案，在生成多数少数族裔选区方面优于现有启发式算法，并介绍了相关优化算法。


<details>
  <summary>Details</summary>
Motivation: 在选区划分诉讼中，为有效执行《选举权法案》，需提供含更多多数少数族裔选区的划分方案，现有启发式算法存在不足。

Method: 基于整数规划，改进随机分层分区算法，设计新的列生成算法，还有局部重新优化算法和提高选区紧凑度的算法。

Result: 新方法在多个数据集上生成的全州划分方案中多数少数族裔选区数量显著更多，优于现有启发式算法。

Conclusion: 提出的基于整数规划的方法在生成多数少数族裔选区的全州划分方案上表现更好。

Abstract: In redistricting litigation, effective enforcement of the Voting Rights Act
has often involved providing the court with districting plans that display a
larger number of majority-minority districts than the current proposal (as was
true, for example, in what followed Allen v. Milligan concerning the
congressional districting plan for Alabama in 2023). Recent work by Cannon et
al. proposed a heuristic algorithm for generating plans to optimize
majority-minority districts, which they called short bursts; that algorithm
relies on a sophisticated random walk over the space of all plans,
transitioning in bursts, where the initial plan for each burst is the most
successful plan from the previous burst. We propose a method based on integer
programming, where we build upon another previous work, the stochastic
hierarchical partitioning algorithm, which heuristically generates a robust set
of potential districts (viewed as columns in a standard set partitioning
formulation); that approach was designed to optimize a different notion of
fairness across a statewide plan. We design a new column generation algorithm
to find plans via integer programming that outperforms short bursts on multiple
data sets in generating statewide plans with significantly more
majority-minority districts. These results also rely on a new local
re-optimization algorithm to iteratively improve on any baseline solution, as
well as an algorithm to increase the compactness of districts in plans
generated (without impacting the number of majority-minority districts).

</details>


### [104] [Simple Algorithms for Fully Dynamic Edge Connectivity](https://arxiv.org/abs/2508.07783)
*Yotam Kenneth-Mordoch,Robert Krauthgamer*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the fully dynamic edge connectivity problem, the input is a simple graph
$G$ undergoing edge insertions and deletions, and the goal is to maintain its
edge connectivity, denoted $\lambda_G$. We present two simple randomized
algorithms solving this problem. The first algorithm maintains the edge
connectivity in worst-case update time $\tilde{O}(n)$ per edge update, matching
the known bound but with simpler analysis. Our second algorithm achieves
worst-case update time $\tilde{O}(n/\lambda_G)$ and worst-case query time
$\tilde{O}(n^2/\lambda_G^2)$, which is the first algorithm with worst-case
update and query time $o(n)$ for large edge connectivity, namely, $\lambda_G =
\omega(\sqrt{n})$.

</details>


### [105] [Nearly Optimal Bounds for Stochastic Online Sorting](https://arxiv.org/abs/2508.07823)
*Yang Hu*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the online sorting problem, we have an array $A$ of $n$ cells, and receive
a stream of $n$ items $x_1,\dots,x_n\in [0,1]$. When an item arrives, we need
to immediately and irrevocably place it into an empty cell. The goal is to
minimize the sum of absolute differences between adjacent items, which is
called the \emph{cost} of the algorithm. It has been shown by Aamand,
Abrahamsen, Beretta, and Kleist (SODA 2023) that when the stream
$x_1,\dots,x_n$ is generated adversarially, the optimal cost bound for any
deterministic algorithm is $\Theta(\sqrt{n})$.
  In this paper, we study the stochastic version of online sorting, where the
input items $x_1,\dots,x_n$ are sampled uniformly at random. Despite the
intuition that the stochastic version should yield much better cost bounds, the
previous best algorithm for stochastic online sorting by Abrahamsen, Bercea,
Beretta, Klausen and Kozma (ESA 2024) only achieves $\tilde{O}(n^{1/4})$ cost,
which seems far from optimal. We show that stochastic online sorting indeed
allows for much more efficient algorithms, by presenting an algorithm that
achieves expected cost $\log n\cdot 2^{O(\log^* n)}$. We also prove a cost
lower bound of $\Omega(\log n)$, thus show that our algorithm is nearly
optimal.

</details>


### [106] [Sparsifying Cayley Graphs on Every Group](https://arxiv.org/abs/2508.08078)
*Jun-Ting Hsieh,Daniel Z. Lee,Sidhanth Mohanty,Aaron Putterman,Rachel Yun Zhang*

Main category: cs.DS

TL;DR: 文章证明了Cayley图存在谱稀疏器及高效查找算法，还研究非阿贝尔群线性方程稀疏化并给出与Cayley图稀疏化的形式分离。


<details>
  <summary>Details</summary>
Motivation: 前人结果应用于Cayley图时稀疏器不一定是Cayley图，探究Cayley图是否存在仅保留特定数量重加权生成元的稀疏器。

Method: 提出证明存在Cayley图谱稀疏器的方法及高效查找算法，研究非阿贝尔群线性方程稀疏化。

Result: 肯定回答Cayley图稀疏器问题，算法可扩展到有向Cayley图的割稀疏化；非阿贝尔值方程需保留超多项式数量方程。

Conclusion: Cayley图稀疏化和线性方程稀疏化存在形式上的分离。

Abstract: A classic result in graph theory, due to Batson, Spielman, and Srivastava
(STOC 2009) shows that every graph admits a $(1 \pm \varepsilon)$ cut (or
spectral) sparsifier which preserves only $O(n / \varepsilon^2)$ reweighted
edges. However, when applying this result to \emph{Cayley graphs}, the
resulting sparsifier is no longer necessarily a Cayley graph -- it can be an
arbitrary subset of edges.
  Thus, a recent line of inquiry, and one which has only seen minor progress,
asks: for any group $G$, do all Cayley graphs over the group $G$ admit
sparsifiers which preserve only $\mathrm{polylog}(|G|)/\varepsilon^2$ many
re-weighted generators?
  As our primary contribution, we answer this question in the affirmative,
presenting a proof of the existence of such Cayley graph spectral sparsifiers,
along with an efficient algorithm for finding them. Our algorithm even extends
to \emph{directed} Cayley graphs, if we instead ask only for cut sparsification
instead of spectral sparsification.
  We additionally study the sparsification of linear equations over non-abelian
groups. In contrast to the abelian case, we show that for non-abelian valued
equations, super-polynomially many linear equations must be preserved in order
to approximately preserve the number of satisfied equations for any input.
Together with our Cayley graph sparsification result, this provides a formal
separation between Cayley graph sparsification and sparsifying linear
equations.

</details>


### [107] [Sparsifying Sums of Positive Semidefinite Matrices](https://arxiv.org/abs/2508.08169)
*Arpon Basu,Pravesh K. Kothari,Yang P. Liu,Raghu Meka*

Main category: cs.DS

TL;DR: 本文重新探讨任意半正定矩阵和的谱稀疏化，提出基于连通阈值的新理论，给出稀疏化器构造方法和应用。


<details>
  <summary>Details</summary>
Motivation: 已有谱稀疏化方法对任意半正定矩阵和的稀疏化界太粗，且无法为Cayley图构建非平凡界。

Method: 基于新参数连通阈值$N^*(\mathcal{A})$，开发特定实例的半正定矩阵稀疏化理论。

Result: 得到最多使用$O(\epsilon^{-2}N^*(\mathcal{A}) (\log n)(\log r))$个矩阵的稀疏化器，可在随机多项式时间内构造，且证明稀疏化至少需要$N^*(\mathcal{A})$个元素；证明任意Cayley图可稀疏到$O(\epsilon^{-2}\log^4 N)$个生成元。

Conclusion: 提出的新理论和方法有效，能解决已有方法的不足，在Cayley图稀疏化上有重要应用。

Abstract: In this paper, we revisit spectral sparsification for sums of arbitrary
positive semidefinite (PSD) matrices. Concretely, for any collection of PSD
matrices $\mathcal{A} = \{A_1, A_2, \ldots, A_r\} \subset \mathbb{R}^{n \times
n}$, given any subset $T \subseteq [r]$, our goal is to find sparse weights
$\mu \in \mathbb{R}_{\geq 0}^r$ such that $(1 - \epsilon) \sum_{i \in T} A_i
\preceq \sum_{i \in T} \mu_i A_i \preceq (1 + \epsilon) \sum_{i \in T} A_i.$
This generalizes spectral sparsification of graphs which corresponds to
$\mathcal{A}$ being the set of Laplacians of edges. It also captures
sparsifying Cayley graphs by choosing a subset of generators. The former has
been extensively studied with optimal sparsifiers known. The latter has
received attention recently and was solved for a few special groups (e.g.,
$\mathbb{F}_2^n$).
  Prior work shows any sum of PSD matrices can be sparsified down to $O(n)$
elements. This bound however turns out to be too coarse and in particular
yields no non-trivial bound for building Cayley sparsifiers for Cayley graphs.
  In this work, we develop a new, instance-specific (i.e., specific to a given
collection $\mathcal{A}$) theory of PSD matrix sparsification based on a new
parameter $N^*(\mathcal{A})$ which we call connectivity threshold that
generalizes the threshold of the number of edges required to make a graph
connected.
  Our main result gives a sparsifier that uses at most
$O(\epsilon^{-2}N^*(\mathcal{A}) (\log n)(\log r))$ matrices and is
constructible in randomized polynomial time. We also show that we need
$N^*(\mathcal{A})$ elements to sparsify for any $\epsilon < 0.99$.
  As the main application of our framework, we prove that any Cayley graph can
be sparsified to $O(\epsilon^{-2}\log^4 N)$ generators. Previously, a
non-trivial bound on Cayley sparsifiers was known only in the case when the
group is $\mathbb{F}_2^n$.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [108] [Generative Bid Shading in Real-Time Bidding Advertising](https://arxiv.org/abs/2508.06550)
*Yinqiu Huang,Hao Ma,Wenshuai Chen,Shuli Wang,Yongqiang Zhang,Xue Wei,Yinhua Zhu,Haitao Wang,Xingxing Wang*

Main category: cs.GT

TL;DR: 本文针对实时竞价中现有出价阴影方法的不足，提出了生成式出价阴影（GBS）方法，经实验验证有效并已部署到美团DSP平台。


<details>
  <summary>Details</summary>
Motivation: 现有主流两阶段方法受单峰假设限制，离散化模型忽略区间依赖，出价场景存在样本选择偏差，无法适应非凸盈余曲线、易级联错误、降低纠错能力和带来预测挑战。

Method: 提出GBS方法，包含端到端生成模型（利用自回归方法按逐步残差生成阴影比率）和奖励偏好对齐系统（采用CHNet提取特征，结合盈余优化和探索效用奖励对齐模块，用GRPO优化长短盈余）。

Result: 离线和在线A/B测试验证了GBS的有效性，且已部署到美团DSP平台，每天处理数十亿出价请求。

Conclusion: GBS能有效解决现有出价阴影方法的问题，在实时竞价中表现良好。

Abstract: Bid shading plays a crucial role in Real-Time Bidding~(RTB) by adaptively
adjusting the bid to avoid advertisers overspending. Existing mainstream
two-stage methods, which first model bid landscapes and then optimize surplus
using operations research techniques, are constrained by unimodal assumptions
that fail to adapt for non-convex surplus curves and are vulnerable to
cascading errors in sequential workflows. Additionally, existing discretization
models of continuous values ignore the dependence between discrete intervals,
reducing the model's error correction ability, while sample selection bias in
bidding scenarios presents further challenges for prediction. To address these
issues, this paper introduces Generative Bid Shading~(GBS), which comprises two
primary components: (1) an end-to-end generative model that utilizes an
autoregressive approach to generate shading ratios by stepwise residuals,
capturing complex value dependencies without relying on predefined priors; and
(2) a reward preference alignment system, which incorporates a channel-aware
hierarchical dynamic network~(CHNet) as the reward model to extract
fine-grained features, along with modules for surplus optimization and
exploration utility reward alignment, ultimately optimizing both short-term and
long-term surplus using group relative policy optimization~(GRPO). Extensive
experiments on both offline and online A/B tests validate GBS's effectiveness.
Moreover, GBS has been deployed on the Meituan DSP platform, serving billions
of bid requests daily.

</details>


### [109] [Algorithmic Delegated Choice: An Annotated Reading List](https://arxiv.org/abs/2508.06562)
*Mohammad T. Hajiaghayi,Suho Shin*

Main category: cs.GT

TL;DR: 概述委托选择问题相关论文，涵盖经典与算法视角新论文。


<details>
  <summary>Details</summary>
Motivation: 委托选择问题在经济学和计算机科学领域长期受关注。

Method: 对委托选择问题相关论文进行概述。

Result: 完成对相关论文的概述。

Conclusion: 无明确结论

Abstract: The problem of delegated choice has been of long interest in economics and
recently on computer science. We overview a list of papers on delegated choice
problem, from classic works to recent papers with algorithmic perspectives.

</details>


### [110] [Asymmetric Network Games: $α$-Potential Function and Learning](https://arxiv.org/abs/2508.06619)
*Kiran Rokade,Adit Jain,Francesca Parise,Vikram Krishnamurthy,Eva Tardos*

Main category: cs.GT

TL;DR: 本文用α - 势博弈框架分析静态网络博弈，推导α - 势函数，证明算法收敛到2α - 纳什均衡，给出α取值及社会福利边界并进行数值验证。


<details>
  <summary>Details</summary>
Motivation: 现实中很多网络不对称且玩家异质，需分析静态网络博弈。

Method: 运用α - 势博弈框架，在玩家行动集和效用函数的温和假设下推导α - 势函数，使用改进的序贯最优响应算法和同步梯度博弈算法。

Result: 算法能使玩家行动收敛到2α - 纳什均衡，α与网络最大不对称性有关，推导了α - 纳什均衡社会福利边界。

Conclusion: 所提算法收敛，学习到的2α - 纳什均衡有相应性质。

Abstract: In a network game, players interact over a network and the utility of each
player depends on his own action and on an aggregate of his neighbours'
actions. Many real world networks of interest are asymmetric and involve a
large number of heterogeneous players. This paper analyzes static network games
using the framework of $\alpha$-potential games. Under mild assumptions on the
action sets (compact intervals) and the utility functions (twice continuously
differentiable) of the players, we derive an expression for an inexact
potential function of the game, called the $\alpha$-potential function. Using
such a function, we show that modified versions of the sequential best-response
algorithm and the simultaneous gradient play algorithm achieve convergence of
players' actions to a $2\alpha$-Nash equilibrium. For linear-quadratic network
games, we show that $\alpha$ depends on the maximum asymmetry in the network
and is well-behaved for a wide range of networks of practical interest.
Further, we derive bounds on the social welfare of the $\alpha$-Nash
equilibrium corresponding to the maximum of the $\alpha$-potential function,
under suitable assumptions. We numerically illustrate the convergence of the
proposed algorithms and properties of the learned $2\alpha$-Nash equilibria.

</details>


### [111] [Convergence of Fast Policy Iteration in Markov Games and Robust MDPs](https://arxiv.org/abs/2508.06661)
*Keith Badger,Marek Petrik,Jefferson Huang*

Main category: cs.GT

TL;DR: 本文指出Filar - Tolwinski (FT)算法可能无法收敛到鞍点，提出了Residual Conditioned Policy Iteration (RCPI)算法，它基于FT且能保证收敛到鞍点，数值结果显示RCPI性能远超其他收敛算法。


<details>
  <summary>Details</summary>
Motivation: 长期致力于为Markov游戏和鲁棒MDPs模型开发高效算法，发现FT算法存在问题。

Method: 先分析FT算法问题，再提出基于FT的RCPI算法。

Result: RCPI能保证收敛到鞍点，且在数值结果上比其他收敛算法性能好几个数量级。

Conclusion: RCPI算法在解决Markov游戏和鲁棒MDPs模型的鞍点策略计算问题上更有效。

Abstract: Markov games and robust MDPs are closely related models that involve
computing a pair of saddle point policies. As part of the long-standing effort
to develop efficient algorithms for these models, the Filar-Tolwinski (FT)
algorithm has shown considerable promise. As our first contribution, we
demonstrate that FT may fail to converge to a saddle point and may loop
indefinitely, even in small games. This observation contradicts the proof of
FT's convergence to a saddle point in the original paper. As our second
contribution, we propose Residual Conditioned Policy Iteration (RCPI). RCPI
builds on FT, but is guaranteed to converge to a saddle point. Our numerical
results show that RCPI outperforms other convergent algorithms by several
orders of magnitude.

</details>


### [112] [Emergence of Cooperation and Commitment in Optional Prisoner's Dilemma](https://arxiv.org/abs/2508.06702)
*Zhao Song,The Anh Han*

Main category: cs.GT

TL;DR: 文章提出两阶段博弈模型研究可选囚徒困境中基于承诺的行为和合作演变，发现可选参与不能促进合作，对比两种制度激励方法，强调设计良好制度激励的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视参与者不参与互动的自由，其应用于现实场景受限，需研究可选参与下承诺行为和合作的演变。

Method: 提出两阶段博弈模型，在可选囚徒困境框架下研究，后引入并比较两种制度激励方法。

Result: 可选参与增加承诺接受但不能促进合作，严格激励方法更利于促进合作，灵活方法在机会主义行为收益高时能提高社会福利。

Conclusion: 仅依靠自愿参与和承诺解决社会困境有局限，设计良好的制度激励对促进合作和社会福利很重要。

Abstract: Commitment is a well-established mechanism for fostering cooperation in human
society and multi-agent systems. However, existing research has predominantly
focused on the commitment that neglects the freedom of players to abstain from
an interaction, limiting their applicability to many real-world scenarios where
participation is often voluntary. In this paper, we present a two-stage game
model to investigate the evolution of commitment-based behaviours and
cooperation within the framework of the optional Prisoner's Dilemma game. In
the pre-game stage, players decide whether to accept a mutual commitment. Once
in the game, they choose among cooperation, defection, or exiting, depending on
the formation of a pre-game commitment. We find that optional participation
boosts commitment acceptance but fails to foster cooperation, leading instead
to widespread exit behaviour. To address this, we then introduce and compare
two institutional incentive approaches: i) a strict one (STRICT-COM) that
rewards only committed players who cooperate in the game, and ii) a flexible
one (FLEXIBLE-COM) that rewards any committed players who do not defect in the
game. The results reveal that, while the strict approach is demonstrably better
for promoting cooperation as the flexible rule creates a loophole for an
opportunistic exit after committing, the flexible rule offers an efficient
alternative for enhancing social welfare when such opportunistic behaviour
results in a high gain. This study highlights the limitations of relying solely
on voluntary participation and commitment to resolving social dilemmas,
emphasising the importance of well-designed institutional incentives to promote
cooperation and social welfare effectively.

</details>


### [113] [When Competition Helps: Achieving Optimal Traffic Flow with Multiple Autonomous Planners](https://arxiv.org/abs/2508.07145)
*Ivan Geffner,Erez Karpas,Moshe Tennenholtz*

Main category: cs.GT

TL;DR: 研究自动驾驶车辆部署能否消除拥堵网络自私路由低效问题，设计含竞争的路由机制达最优分配。


<details>
  <summary>Details</summary>
Motivation: 探讨自动驾驶车辆部署能否消除拥堵网络自私路由的低效性。

Method: 从经典Pigou网络出发，设计一种接纳竞争的路由机制。

Result: 表明竞争不只是障碍，还是实现最优结果的必要因素，设计的机制能收敛到最优分配。

Conclusion: 含竞争的路由机制可解决拥堵网络自私路由的低效问题。

Abstract: The inefficiency of selfish routing in congested networks is a classical
problem in algorithmic game theory, often captured by the Price of Anarchy
(i.e., the ratio between the social cost of decentralized decisions and that of
a centrally optimized solution.) With the advent of autonomous vehicles,
capable of receiving and executing centrally assigned routes, it is natural to
ask whether their deployment can eliminate this inefficiency. At first glance,
a central authority could simply compute an optimal traffic assignment and
instruct each vehicle to follow its assigned path. However, this vision
overlooks critical challenges: routes must be individually rational (no vehicle
has an incentive to deviate), and in practice, multiple planning agents (e.g.,
different companies) may coexist and compete. Surprisingly, we show that such
competition is not merely an obstacle but a necessary ingredient for achieving
optimal outcomes. In this work, we design a routing mechanism that embraces
competition and converges to an optimal assignment, starting from the classical
Pigou network as a foundational case.

</details>


### [114] [Maximizing Social Welfare with Side Payments](https://arxiv.org/abs/2508.07147)
*Ivan Geffner,Caspar Oesterheld,Vincent Conitzer*

Main category: cs.GT

TL;DR: 研究玩家可预先承诺结果相关转移支付的标准型博弈，针对Jackson和Wilkie模型的低效问题引入分阶段承诺协议，证明该协议能实现福利最大化。


<details>
  <summary>Details</summary>
Motivation: Jackson和Wilkie模型中无限制的同时承诺会使博弈陷入低效均衡，需解决此问题。

Method: 引入分阶段承诺协议，玩家在多轮中以小的、有上限的增量承诺转移支付，且需一致同意才能继续。

Result: 从任何具有非退化纳什均衡的有限博弈开始，该协议能实现严格帕累托改进该均衡的每个福利最大化支付配置。

Conclusion: 渐进且有界的承诺能恢复旁支付的全部效率潜力，避免Jackson和Wilkie所指出的低效问题。

Abstract: We examine normal-form games in which players may \emph{pre-commit} to
outcome-contingent transfers before choosing their actions. In the one-shot
version of this model, Jackson and Wilkie showed that side contracting can
backfire: even a game with a Pareto-optimal Nash equilibrium can devolve into
inefficient equilibria once unbounded, simultaneous commitments are allowed.
The root cause is a prisoner's dilemma effect, where each player can exploit
her commitment power to reshape the equilibrium in her favor, harming overall
welfare.
  To circumvent this problem we introduce a \emph{staged-commitment} protocol.
Players may pledge transfers only in small, capped increments over multiple
rounds, and the phase continues only with unanimous consent. We prove that,
starting from any finite game $\Gamma$ with a non-degenerate Nash equilibrium
$\vec{\sigma}$, this protocol implements every welfare-maximizing payoff
profile that \emph{strictly} Pareto-improves $\vec{\sigma}$. Thus, gradual and
bounded commitments restore the full efficiency potential of side payments
while avoiding the inefficiencies identified by Jackson and Wilkie.

</details>


### [115] [Last-Iterate Convergence in Adaptive Regret Minimization for Approximate Extensive-Form Perfect Equilibrium](https://arxiv.org/abs/2508.07699)
*Hang Ren,Xiaozhen Sun,Tianzi Ma,Jiajia Zhang,Xuan Wang*

Main category: cs.GT

TL;DR: 现有求Extensive - Form Perfect Equilibrium (EFPE)算法有局限，本文提出高效自适应后悔最小化算法计算近似EFPE，理论和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: Nash Equilibrium (NE)在游戏树非均衡分支不能保证最优策略，现有EFPE求解算法有计算成本高、近似误差大等局限。

Method: 提出高效自适应后悔最小化算法，引入Reward Transformation Counterfactual Regret Minimization (RTCFR)求解扰动游戏，定义新指标Information Set Nash Equilibrium (ISNE)动态调整扰动。

Result: 理论分析证明收敛到EFPE，实验表明该方法在NE和EFPE求解任务中显著优于现有算法。

Conclusion: 所提算法有效解决现有求EFPE算法局限，在相关任务中有更好表现。

Abstract: The Nash Equilibrium (NE) assumes rational play in imperfect-information
Extensive-Form Games (EFGs) but fails to ensure optimal strategies for
off-equilibrium branches of the game tree, potentially leading to suboptimal
outcomes in practical settings. To address this, the Extensive-Form Perfect
Equilibrium (EFPE), a refinement of NE, introduces controlled perturbations to
model potential player errors. However, existing EFPE-finding algorithms, which
typically rely on average strategy convergence and fixed perturbations, face
significant limitations: computing average strategies incurs high computational
costs and approximation errors, while fixed perturbations create a trade-off
between NE approximation accuracy and the convergence rate of NE refinements.
  To tackle these challenges, we propose an efficient adaptive regret
minimization algorithm for computing approximate EFPE, achieving last-iterate
convergence in two-player zero-sum EFGs. Our approach introduces Reward
Transformation Counterfactual Regret Minimization (RTCFR) to solve perturbed
games and defines a novel metric, the Information Set Nash Equilibrium (ISNE),
to dynamically adjust perturbations. Theoretical analysis confirms convergence
to EFPE, and experimental results demonstrate that our method significantly
outperforms state-of-the-art algorithms in both NE and EFPE-finding tasks.

</details>


### [116] [Truthful Two-Obnoxious-Facility Location Games with Optional Preferences and Minimum Distance Constraint](https://arxiv.org/abs/2508.08036)
*Xiaojia Han,Wenjing Liu,Qizhi Fang*

Main category: cs.GT

TL;DR: 研究带最小距离约束的双厌恶设施选址问题，给出不同情况下的确定性和随机策略证明机制及近似比，还给出近似比下界。


<details>
  <summary>Details</summary>
Motivation: 解决双厌恶设施选址中激励代理如实报告位置并最大化社会效用的问题。

Method: 先考虑特殊情况d = 0，提出确定性和随机策略证明机制；再研究一般情况，同样提出相应机制，并给出近似比下界。

Result: 特殊情况：确定性机制近似比至多4，随机机制近似比至多2；一般情况：确定性机制近似比至多8，随机机制近似比至多4；近似比下界：确定性为2，随机为14/13。

Conclusion: 为双厌恶设施选址问题提供了有效机制和近似比分析。

Abstract: In this paper, we study a truthful two-obnoxious-facility location problem,
in which each agent has a private location in [0, 1] and a public optional
preference over two obnoxious facilities, and there is a minimum distance
constraint d between the two facilities. Each agent wants to be as far away as
possible from the facilities that affect her, and the utility of each agent is
the total distance from her to these facilities. The goal is to decide how to
place the facilities in [0, 1] so as to incentivize agents to report their
private locations truthfully as well as maximize the social utility. First, we
consider the special setting where d = 0, that is, the two facilities can be
located at any point in [0, 1]. We propose a deterministic strategyproof
mechanism with approximation ratio of at most 4 and a randomized strategyproof
mechanism with approximation ratio of at most 2, respectively. Then we study
the general setting. We propose a deterministic strategyproof mechanism with
approximation ratio of at most 8 and a randomized strategyproof mechanism with
approximation ratio of at most 4, respectively. Furthermore, we provide lower
bounds of 2 and 14/13 on the approximation ratio for any deterministic and any
randomized strategyproof mechanism, respectively.

</details>


### [117] [Constrained Distributed Heterogeneous Two-Facility Location Problems with Max-Variant Cost](https://arxiv.org/abs/2508.08045)
*Xinru Xu,Wenjing Liu,Qizhi Fang*

Main category: cs.GT

TL;DR: 研究约束分布式异构双设施选址问题，设计策略证明分布式机制，分析四种社会目标下的失真上下界并获常数界。


<details>
  <summary>Details</summary>
Motivation: 设计能激励代理如实报告位置并近似优化社会目标的策略证明分布式机制。

Method: 先为每组选两个候选位置作代表，再从代表中选两个设施位置，聚焦确定性策略证明分布式机制。

Result: 在四种社会目标下获得了常数上界和下界的失真界限。

Conclusion: 设计的机制在四种社会目标下能有效控制失真，有一定的理论价值。

Abstract: We study a constrained distributed heterogeneous two-facility location
problem, where a set of agents with private locations on the real line are
divided into disjoint groups. The constraint means that the facilities can only
be built in a given multiset of candidate locations and at most one facility
can be built at each candidate location. Given the locations of the two
facilities, the cost of an agent is the distance from her location to the
farthest facility (referred to as max-variant). Our goal is to design
strategyproof distributed mechanisms that can incentivize all agents to
truthfully report their locations and approximately optimize some social
objective. A distributed mechanism consists of two steps: for each group, the
mechanism chooses two candidate locations as the representatives of the group
based only on the locations reported by agents therein; then, it outputs two
facility locations among all the representatives. We focus on a class of
deterministic strategyproof distributed mechanisms and analyze upper and lower
bounds on the distortion under the Average-of-Average cost (average of the
average individual cost of agents in each group), the Max-of-Max cost (maximum
individual cost among all agents), the Average-of-Max cost (average of the
maximum individual cost among all agents in each group) and the Max-of-Average
cost (maximum of the average individual cost of all agents in each group).
Under four social objectives, we obtain constant upper and lower distortion
bounds.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [118] [BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation](https://arxiv.org/abs/2508.06781)
*Christos Tsirigotis,Vaibhav Adlakha,Joao Monteiro,Aaron Courville,Perouz Taslakian*

Main category: cs.IR

TL;DR: 提出BiXSE方法，利用LLM生成的分级相关性分数优化二元交叉熵，减少标注和计算成本，实验表现好。


<details>
  <summary>Details</summary>
Motivation: 现有神经句子嵌入模型依赖二元相关性标签，而现实相关性是连续的，且LLM使生成细粒度分级相关性标签可行。

Method: 提出BiXSE点训练方法，对LLM生成的分级相关性分数优化二元交叉熵，将分数视为概率目标，利用批次内负样本。

Result: 在多个基准测试中，BiXSE始终优于基于softmax的对比学习，在LLM监督数据上训练时与或超过强成对排序基线。

Conclusion: 随着分级相关性监督更易获取，BiXSE为训练密集检索模型提供了强大、可扩展的替代方案。

Abstract: Neural sentence embedding models for dense retrieval typically rely on binary
relevance labels, treating query-document pairs as either relevant or
irrelevant. However, real-world relevance often exists on a continuum, and
recent advances in large language models (LLMs) have made it feasible to scale
the generation of fine-grained graded relevance labels. In this work, we
propose BiXSE, a simple and effective pointwise training method that optimizes
binary cross-entropy (BCE) over LLM-generated graded relevance scores. BiXSE
interprets these scores as probabilistic targets, enabling granular supervision
from a single labeled query-document pair per query. Unlike pairwise or
listwise losses that require multiple annotated comparisons per query, BiXSE
achieves strong performance with reduced annotation and compute costs by
leveraging in-batch negatives. Extensive experiments across sentence embedding
(MMTEB) and retrieval benchmarks (BEIR, TREC-DL) show that BiXSE consistently
outperforms softmax-based contrastive learning (InfoNCE), and matches or
exceeds strong pairwise ranking baselines when trained on LLM-supervised data.
BiXSE offers a robust, scalable alternative for training dense retrieval models
as graded relevance supervision becomes increasingly accessible.

</details>


### [119] [CLAP: Coreference-Linked Augmentation for Passage Retrieval](https://arxiv.org/abs/2508.06941)
*Huanwei Xu,Lin Xu,Liang Yuan*

Main category: cs.IR

TL;DR: 提出基于大语言模型的轻量级扩展框架CLAP用于段落检索，能提升检索性能，尤其在跨领域场景表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的段落扩展方法存在语义漂移、与预训练语义空间不匹配问题，且段落部分内容为噪声，分块技术会破坏共指连续性。

Method: 提出Coreference - Linked Augmentation for Passage Retrieval (CLAP)框架，将段落分割成连贯块，解决共指链，生成与密集检索器表示一致的局部伪查询，融合全局主题信号和细粒度子主题信号。

Result: CLAP在检索器性能提升时仍有稳定增益，能使密集检索器性能匹配或超越二级排序器，nDCG@10绝对提升达20.68%，在跨领域场景优势明显。

Conclusion: CLAP采用以逻辑为中心的管道，实现了强大的、与领域无关的泛化能力。

Abstract: Large Language Model (LLM)-based passage expansion has shown promise for
enhancing first-stage retrieval, but often underperforms with dense retrievers
due to semantic drift and misalignment with their pretrained semantic space.
Beyond this, only a portion of a passage is typically relevant to a query,
while the rest introduces noise--an issue compounded by chunking techniques
that break coreference continuity. We propose Coreference-Linked Augmentation
for Passage Retrieval (CLAP), a lightweight LLM-based expansion framework that
segments passages into coherent chunks, resolves coreference chains, and
generates localized pseudo-queries aligned with dense retriever
representations. A simple fusion of global topical signals and fine-grained
subtopic signals achieves robust performance across domains. CLAP yields
consistent gains even as retriever strength increases, enabling dense
retrievers to match or surpass second-stage rankers such as BM25 + MonoT5-3B,
with up to 20.68% absolute nDCG@10 improvement. These improvements are
especially notable in out-of-domain settings, where conventional LLM-based
expansion methods relying on domain knowledge often falter. CLAP instead adopts
a logic-centric pipeline that enables robust, domain-agnostic generalization.

</details>


### [120] [Blending Sequential Embeddings, Graphs, and Engineered Features: 4th Place Solution in RecSys Challenge 2025](https://arxiv.org/abs/2508.06970)
*Sergei Makeev,Alexandr Andreev,Vladimir Baikalov,Vladislav Tytskiy,Aleksei Krasilnikov,Kirill Khrylchenko*

Main category: cs.IR

TL;DR: 介绍团队ambitious在RecSys Challenge 2025中的第4名解决方案，聚焦通用行为建模。


<details>
  <summary>Details</summary>
Motivation: 参加由Synerise和ACM RecSys组织的RecSys Challenge 2025，目标是生成在六个不同下游任务中有效的用户嵌入。

Method: 集成顺序编码器、图神经网络、深度交叉网络和关键性能特征工程。

Result: 获得比赛第4名。

Conclusion: 该集成方法在通用行为建模比赛中取得较好成绩。

Abstract: This paper describes the 4th-place solution by team ambitious for the RecSys
Challenge 2025, organized by Synerise and ACM RecSys, which focused on
universal behavioral modeling. The challenge objective was to generate user
embeddings effective across six diverse downstream tasks. Our solution
integrates (1) a sequential encoder to capture the temporal evolution of user
interests, (2) a graph neural network to enhance generalization, (3) a deep
cross network to model high-order feature interactions, and (4)
performance-critical feature engineering.

</details>


### [121] [ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability](https://arxiv.org/abs/2508.07050)
*Wenhan Liu,Xinyu Ma,Weiwei Sun,Yutao Zhu,Yuchen Li,Dawei Yin,Zhicheng Dou*

Main category: cs.IR

TL;DR: 提出自动化推理密集型训练数据合成框架和两阶段后训练方法，训练出推理密集型重排器ReasonRank，性能超基线且延迟低，在BRIGHT排行榜达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有重排器因推理密集型训练数据稀缺，在复杂排名场景表现差，推理密集型重排器排名能力未充分开发。

Method: 提出自动化推理密集型训练数据合成框架，设计自一致性数据过滤机制；提出两阶段后训练方法，包括冷启动监督微调阶段和强化学习阶段，强化学习阶段设计多视图排名奖励。

Result: 训练的ReasonRank显著优于现有基线，延迟低于点wise重排器Rank1，在BRIGHT排行榜达SOTA性能40.6。

Conclusion: 所提方法有效提升了重排器的推理和排名能力，ReasonRank有良好性能。

Abstract: Large Language Model (LLM) based listwise ranking has shown superior
performance in many passage ranking tasks. With the development of Large
Reasoning Models, many studies have demonstrated that step-by-step reasoning
during test-time helps improve listwise ranking performance. However, due to
the scarcity of reasoning-intensive training data, existing rerankers perform
poorly in many complex ranking scenarios and the ranking ability of
reasoning-intensive rerankers remains largely underdeveloped. In this paper, we
first propose an automated reasoning-intensive training data synthesis
framework, which sources training queries and passages from diverse domains and
applies DeepSeek-R1 to generate high-quality training labels. A
self-consistency data filtering mechanism is designed to ensure the data
quality. To empower the listwise reranker with strong reasoning ability, we
further propose a two-stage post-training approach, which includes a cold-start
supervised fine-tuning (SFT) stage for reasoning pattern learning and a
reinforcement learning (RL) stage for further ranking ability enhancement.
During the RL stage, based on the nature of listwise ranking, we design a
multi-view ranking reward, which is more effective than a ranking metric-based
reward. Extensive experiments demonstrate that our trained reasoning-intensive
reranker \textbf{ReasonRank} outperforms existing baselines significantly and
also achieves much lower latency than pointwise reranker Rank1. \textbf{Through
further experiments, our ReasonRank has achieved state-of-the-art (SOTA)
performance 40.6 on the BRIGHT
leaderboard\footnote{https://brightbenchmark.github.io/}.} Our codes are
available at https://github.com/8421BCD/ReasonRank.

</details>


### [122] [Uncertainty-Aware Semantic Decoding for LLM-Based Sequential Recommendation](https://arxiv.org/abs/2508.07210)
*Chenke Yin,Li Fan,Jia Wang,Dongxiao Hu,Haichao Zhang,Chong Zhang,Yang Xiang*

Main category: cs.IR

TL;DR: 本文提出USD框架解决大语言模型在序列推荐任务中推理策略不匹配问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在序列推荐推理时采用的解码策略与推荐目标不匹配。

Method: 提出Uncertainty - aware Semantic Decoding (USD)框架，结合基于对数的聚类和自适应评分改进下一项预测。

Result: 在亚马逊产品数据集上多项指标提升，超参数分析确定最优参数，在H&M和Netflix数据集证明框架适应性。

Conclusion: 整合语义聚类和不确定性评估能产生更可靠准确的推荐。

Abstract: Large language models have been widely applied to sequential recommendation
tasks, yet during inference, they continue to rely on decoding strategies
developed for natural language processing. This creates a mismatch between
text-generation objectives and recommendation next item selection objectives.
This paper addresses this limitation by proposing an Uncertainty-aware Semantic
Decoding (USD) framework that combines logit-based clustering with adaptive
scoring to improve next-item predictions. Our approach clusters items with
similar logit vectors into semantic equivalence groups, then redistributes
probability mass within these clusters and computes entropy across them to
control item scoring and sampling temperature during recommendation inference.
Experiments on Amazon Product datasets (six domains) gains of 18.5\% in HR@3,
11.9\% in NDCG@3, and 10.8\% in MRR@3 compared to state-of-the-art baselines.
Hyperparameter analysis confirms the optimal parameters among various settings,
and experiments on H\&M, and Netflix datasets indicate that the framework can
adapt to differing recommendation domains. The experimental results confirm
that integrating semantic clustering and uncertainty assessment yields more
reliable and accurate recommendations.

</details>


### [123] [Selection and Exploitation of High-Quality Knowledge from Large Language Models for Recommendation](https://arxiv.org/abs/2508.07223)
*Guanchen Wang,Mingming Ha,Tianbao Ma,Linxun Chen,Zhaojie Liu,Guorui Zhou,Kun Gai*

Main category: cs.IR

TL;DR: 本文提出KSER框架解决LLM知识用于推荐系统的问题，含知识过滤和嵌入空间对齐模块及两种训练策略，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型提升推荐系统性能时，其提供的知识存在幻觉、冗余和信息同质化问题，直接使用会导致性能下降。

Method: 提出KSER框架，包含知识过滤模块（ESFNet分配权重）和嵌入空间对齐模块（基于注意力架构对齐语义嵌入），并提出全参数训练和仅提取器训练两种策略。

Result: 实验验证了知识过滤和对齐模块的必要性和有效性，以及仅提取器训练策略的效率和有效性。

Conclusion: KSER框架及训练策略能有效解决大语言模型知识用于推荐系统的问题，提升推荐性能。

Abstract: In recent years, there has been growing interest in leveraging the impressive
generalization capabilities and reasoning ability of large language models
(LLMs) to improve the performance of recommenders. With this operation,
recommenders can access and learn the additional world knowledge and reasoning
information via LLMs. However, in general, for different users and items, the
world knowledge derived from LLMs suffers from issues of hallucination, content
redundant, and information homogenization. Directly feeding the generated
response embeddings into the recommendation model can lead to unavoidable
performance deterioration. To address these challenges, we propose a Knowledge
Selection \& Exploitation Recommendation (KSER) framework, which effectively
select and extracts the high-quality knowledge from LLMs. The framework
consists of two key components: a knowledge filtering module and a embedding
spaces alignment module. In the knowledge filtering module, a Embedding
Selection Filter Network (ESFNet) is designed to assign adaptive weights to
different knowledge chunks in different knowledge fields. In the space
alignment module, an attention-based architecture is proposed to align the
semantic embeddings from LLMs with the feature space used to train the
recommendation models. In addition, two training
strategies--\textbf{all-parameters training} and \textbf{extractor-only
training}--are proposed to flexibly adapt to different downstream tasks and
application scenarios, where the extractor-only training strategy offers a
novel perspective on knowledge-augmented recommendation. Experimental results
validate the necessity and effectiveness of both the knowledge filtering and
alignment modules, and further demonstrate the efficiency and effectiveness of
the extractor-only training strategy.

</details>


### [124] [SocRipple: A Two-Stage Framework for Cold-Start Video Recommendations](https://arxiv.org/abs/2508.07241)
*Amit Jaspal,Kapil Dalwani,Ajantha Ramineni*

Main category: cs.IR

TL;DR: 提出SocRipple框架解决社交平台冷启动项目分发问题，实验显示可提升分发效果并平衡新项曝光与个性化推荐。


<details>
  <summary>Details</summary>
Motivation: 行业规模推荐系统面临冷启动挑战，标准协同过滤和仅基于内容的方法表现不佳。

Method: 提出两阶段检索框架SocRipple，第一阶段利用创作者社交连接进行初始曝光，第二阶段基于早期参与信号和历史交互学习的用户嵌入通过KNN搜索向外扩展。

Result: 在大型视频平台实验中，SocRipple使冷启动项目分发提升36%，并保持用户对冷启动项目的参与率。

Conclusion: SocRipple能有效平衡新项曝光与个性化推荐。

Abstract: Most industry scale recommender systems face critical cold start challenges
new items lack interaction history, making it difficult to distribute them in a
personalized manner. Standard collaborative filtering models underperform due
to sparse engagement signals, while content only approaches lack user specific
relevance. We propose SocRipple, a novel two stage retrieval framework tailored
for coldstart item distribution in social graph based platforms. Stage 1
leverages the creators social connections for targeted initial exposure. Stage
2 builds on early engagement signals and stable user embeddings learned from
historical interactions to "ripple" outwards via K Nearest Neighbor (KNN)
search. Large scale experiments on a major video platform show that SocRipple
boosts cold start item distribution by +36% while maintaining user engagement
rate on cold start items, effectively balancing new item exposure with
personalized recommendations.

</details>


### [125] [PrLM: Learning Explicit Reasoning for Personalized RAG via Contrastive Reward Optimization](https://arxiv.org/abs/2508.07342)
*Kepu Zhang,Teng Shi,Weijie Yu,Jun Xu*

Main category: cs.IR

TL;DR: 提出PrLM框架解决现有个性化检索增强生成方法依赖检索质量且响应可能与用户偏好不符的问题，实验显示其性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有个性化检索增强生成方法依赖检索质量，生成响应可能与用户偏好不符。

Method: 提出PrLM强化学习框架，通过对比训练的个性化奖励模型引导大语言模型对检索到的用户资料进行显式推理。

Result: 在三个个性化文本生成数据集上的实验表明，PrLM优于现有方法，且在不同数量的检索资料和不同检索器下保持鲁棒性。

Conclusion: PrLM能有效解决现有方法的局限性，是一种更优的个性化检索增强生成方法。

Abstract: Personalized retrieval-augmented generation (RAG) aims to produce
user-tailored responses by incorporating retrieved user profiles alongside the
input query. Existing methods primarily focus on improving retrieval and rely
on large language models (LLMs) to implicitly integrate the retrieved context
with the query. However, such models are often sensitive to retrieval quality
and may generate responses that are misaligned with user preferences. To
address this limitation, we propose PrLM, a reinforcement learning framework
that trains LLMs to explicitly reason over retrieved user profiles. Guided by a
contrastively trained personalization reward model, PrLM effectively learns
from user responses without requiring annotated reasoning paths. Experiments on
three personalized text generation datasets show that PrLM outperforms existing
methods and remains robust across varying numbers of retrieved profiles and
different retrievers.

</details>


### [126] [Are Multimodal Embeddings Truly Beneficial for Recommendation? A Deep Dive into Whole vs. Individual Modalities](https://arxiv.org/abs/2508.07399)
*Yu Ye,Junchen Fu,Yu Song,Kaiwen Zheng,Joemon M. Jose*

Main category: cs.IR

TL;DR: 文章对多模态嵌入在推荐系统中的作用进行大规模实证研究，发现多模态嵌入总体提升推荐性能，文本模态单独使用效果较好，图像模态单独使用不佳。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推荐中，多模态嵌入能提升推荐性能的假设缺乏全面实证验证，存在研究空白。

Method: 采用模态剔除策略，将对应嵌入设为常数值或随机噪声，评估14个广泛使用的先进多模态推荐模型。

Result: 多模态嵌入总体提升推荐性能，复杂图融合模型效果更好；文本模态单独使用性能与全多模态相当，图像模态单独使用不佳。

Conclusion: 研究为多模态推荐社区提供基础见解和实践指导，将发布代码和数据集。

Abstract: Multimodal recommendation (MMRec) has emerged as a mainstream paradigm,
typically leveraging text and visual embeddings extracted from pre-trained
models such as Sentence-BERT, Vision Transformers, and ResNet. This approach is
founded on the intuitive assumption that incorporating multimodal embeddings
can enhance recommendation performance. However, despite its popularity, this
assumption lacks comprehensive empirical verification. This presents a critical
research gap. To address it, we pose the central research question of this
paper: Are multimodal embeddings truly beneficial for recommendation? To answer
this question, we conduct a large-scale empirical study examining the role of
text and visual embeddings in modern MMRec models, both as a whole and
individually. Specifically, we pose two key research questions: (1) Do
multimodal embeddings as a whole improve recommendation performance? (2) Is
each individual modality - text and image - useful when used alone? To isolate
the effect of individual modalities - text or visual - we employ a modality
knockout strategy by setting the corresponding embeddings to either constant
values or random noise. To ensure the scale and comprehensiveness of our study,
we evaluate 14 widely used state-of-the-art MMRec models. Our findings reveal
that: (1) multimodal embeddings generally enhance recommendation performance -
particularly when integrated through more sophisticated graph-based fusion
models. Surprisingly, commonly adopted baseline models with simple fusion
schemes, such as VBPR and BM3, show only limited gains. (2) The text modality
alone achieves performance comparable to the full multimodal setting in most
cases, whereas the image modality alone does not. These results offer
foundational insights and practical guidance for the MMRec community. We will
release our code and datasets to facilitate future research.

</details>


### [127] [Orthogonal Low Rank Embedding Stabilization](https://arxiv.org/abs/2508.07574)
*Kevin Zielnicki,Ko-Jen Hsiao*

Main category: cs.IR

TL;DR: 提出正交低秩变换方法稳定用户/物品嵌入空间，计算高效且无损。


<details>
  <summary>Details</summary>
Motivation: 模型再训练周期中嵌入空间不稳定给下游应用带来挑战。

Method: 结合低秩奇异值分解和正交Procrustes变换，将嵌入映射到标准化空间。

Result: 该变换计算高效、无损、轻量级，保留点积和推理质量，减轻操作负担。

Conclusion: 该方法保持主模型应用完整性，可与其他稳定技术无缝集成。

Abstract: The instability of embedding spaces across model retraining cycles presents
significant challenges to downstream applications using user or item embeddings
derived from recommendation systems as input features. This paper introduces a
novel orthogonal low-rank transformation methodology designed to stabilize the
user/item embedding space, ensuring consistent embedding dimensions across
retraining sessions. Our approach leverages a combination of efficient low-rank
singular value decomposition and orthogonal Procrustes transformation to map
embeddings into a standardized space. This transformation is computationally
efficient, lossless, and lightweight, preserving the dot product and inference
quality while reducing operational burdens. Unlike existing methods that modify
training objectives or embedding structures, our approach maintains the
integrity of the primary model application and can be seamlessly integrated
with other stabilization techniques.

</details>


### [128] [Towards Comprehensible Recommendation with Large Language Model Fine-tuning](https://arxiv.org/abs/2508.07595)
*Yunze Luo,Yinjie Jiang,Gaode Chen,Xinghua Zhang,Jun Zhang,Jian Liang,Kaigui Bian*

Main category: cs.IR

TL;DR: 提出CURec框架解决推荐系统语义协作差距问题，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统推荐方法难捕捉与用户偏好一致的语义，存在语义协作差距，新兴基于大语言模型的特征提取方法也面临使大语言模型具备推荐对齐推理能力的挑战。

Method: 先通过预训练使大语言模型与推荐目标对齐，设计奖励模型评估推荐理由质量，利用奖励信号通过强化学习微调大语言模型，将修正后的理由集成到下游推荐模型。

Result: 在公共基准上的大量实验表明CURec优于现有方法。

Conclusion: CURec框架能有效解决推荐系统的语义协作差距问题，提升推荐性能。

Abstract: Recommender systems have become increasingly ubiquitous in daily life. While
traditional recommendation approaches primarily rely on ID-based
representations or item-side content features, they often fall short in
capturing the underlying semantics aligned with user preferences (e.g.,
recommendation reasons for items), leading to a semantic-collaborative gap.
Recently emerged LLM-based feature extraction approaches also face a key
challenge: how to ensure that LLMs possess recommendation-aligned reasoning
capabilities and can generate accurate, personalized reasons to mitigate the
semantic-collaborative gap. To address these issues, we propose a novel Content
Understanding from a Collaborative Perspective framework (CURec), which
generates collaborative-aligned content features for more comprehensive
recommendations. \method first aligns the LLM with recommendation objectives
through pretraining, equipping it with instruction-following and
chain-of-thought reasoning capabilities. Next, we design a reward model
inspired by traditional recommendation architectures to evaluate the quality of
the recommendation reasons generated by the LLM. Finally, using the reward
signals, CURec fine-tunes the LLM through RL and corrects the generated reasons
to ensure their accuracy. The corrected reasons are then integrated into a
downstream recommender model to enhance comprehensibility and recommendation
performance. Extensive experiments on public benchmarks demonstrate the
superiority of CURec over existing methods.

</details>


### [129] [UMRE: A Unified Monotonic Transformation for Ranking Ensemble in Recommender Systems](https://arxiv.org/abs/2508.07613)
*Zhengrui Xu,Zhe Yang,Zhengxiao Guo,Shukai Liu,Luocheng Lin,Xiaoyan Liu,Yongqi Liu,Han Li*

Main category: cs.IR

TL;DR: 提出UMRE框架解决传统集成排序方法局限，实验显示其性能和泛化能力出色。


<details>
  <summary>Details</summary>
Motivation: 传统集成排序依赖手动设计变换和调参，劳动密集且难达帕累托最优，需新方法解决。

Method: 用UMNN替代手工变换，用轻量级排序模型融合预测分数，引入帕累托最优策略平衡目标。

Result: 在两个公开数据集和线上A/B测试中表现出良好性能和泛化能力。

Conclusion: UMRE消除手动调参，保持排序一致性，实现细粒度个性化。

Abstract: Industrial recommender systems commonly rely on ensemble sorting (ES) to
combine predictions from multiple behavioral objectives. Traditionally, this
process depends on manually designed nonlinear transformations (e.g.,
polynomial or exponential functions) and hand-tuned fusion weights to balance
competing goals -- an approach that is labor-intensive and frequently
suboptimal in achieving Pareto efficiency. In this paper, we propose a novel
Unified Monotonic Ranking Ensemble (UMRE) framework to address the limitations
of traditional methods in ensemble sorting. UMRE replaces handcrafted
transformations with Unconstrained Monotonic Neural Networks (UMNN), which
learn expressive, strictly monotonic functions through the integration of
positive neural integrals. Subsequently, a lightweight ranking model is
employed to fuse the prediction scores, assigning personalized weights to each
prediction objective. To balance competing goals, we further introduce a Pareto
optimality strategy that adaptively coordinates task weights during training.
UMRE eliminates manual tuning, maintains ranking consistency, and achieves
fine-grained personalization. Experimental results on two public recommendation
datasets (Kuairand and Tenrec) and online A/B tests demonstrate impressive
performance and generalization capabilities.

</details>


### [130] [Encode Me If You Can: Learning Universal User Representations via Event Sequence Autoencoding](https://arxiv.org/abs/2508.07748)
*Anton Klenitskiy,Artem Fatkulin,Daria Denisova,Anton Pembek,Alexey Vasilev*

Main category: cs.IR

TL;DR: 文章聚焦构建通用用户表示，提出将用户交互历史转为序列，用GRU自编码器训练，结合多种方法生成统一表示，助团队获RecSys Challenge 2025亚军。


<details>
  <summary>Details</summary>
Motivation: 构建能捕捉用户行为关键方面的通用用户表示，减少特定任务特征工程和模型再训练需求，实现更具扩展性和效率的机器学习流程。

Method: 将用户交互历史转为时间序列，用GRU自编码器从固定大小向量重构序列；探索多种生成用户嵌入的方法，拼接输出向量形成统一表示。

Result: 团队（ai_lab_recsys）在RecSys Challenge 2025中获得第二名。

Conclusion: 所提出的方法和集成策略有效，能提升在不同下游任务中的泛化能力。

Abstract: Building universal user representations that capture the essential aspects of
user behavior is a crucial task for modern machine learning systems. In
real-world applications, a user's historical interactions often serve as the
foundation for solving a wide range of predictive tasks, such as churn
prediction, recommendations, or lifetime value estimation. Using a
task-independent user representation that is effective across all such tasks
can reduce the need for task-specific feature engineering and model retraining,
leading to more scalable and efficient machine learning pipelines. The goal of
the RecSys Challenge 2025 by Synerise was to develop such Universal Behavioral
Profiles from logs of past user behavior, which included various types of
events such as product purchases, page views, and search queries. We propose a
method that transforms the entire user interaction history into a single
chronological sequence and trains a GRU-based autoencoder to reconstruct this
sequence from a fixed-size vector. If the model can accurately reconstruct the
sequence, the latent vector is expected to capture the key behavioral patterns.
In addition to this core model, we explored several alternative methods for
generating user embeddings and combined them by concatenating their output
vectors into a unified representation. This ensemble strategy further improved
generalization across diverse downstream tasks and helped our team,
ai_lab_recsys, achieve second place in the RecSys Challenge 2025.

</details>


### [131] [Recommendation Is a Dish Better Served Warm](https://arxiv.org/abs/2508.07856)
*Danil Gusak,Nikita Sukhorukov,Evgeny Frolov*

Main category: cs.IR

TL;DR: 现代推荐系统中冷启动阈值选择随意影响评估结果，本文系统探索冷启动边界，发现阈值选择不一致会带来问题。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统中冷启动阈值选择随意，导致评估结果可比性和可靠性受影响，需要系统探索冷启动边界。

Method: 在训练时对不同物品的交互数量进行增量变化，推理时逐步更新用户交互历史长度，在多个常用数据集和推荐基线模型上研究阈值。

Result: 不一致的冷启动阈值选择会导致有价值数据被不必要移除或冷实例被误分类为热实例，给系统引入更多噪声。

Conclusion: 冷启动阈值的不一致选择会对推荐系统产生不良影响，需要更合理地确定阈值。

Abstract: In modern recommender systems, experimental settings typically include
filtering out cold users and items based on a minimum interaction threshold.
However, these thresholds are often chosen arbitrarily and vary widely across
studies, leading to inconsistencies that can significantly affect the
comparability and reliability of evaluation results. In this paper, we
systematically explore the cold-start boundary by examining the criteria used
to determine whether a user or an item should be considered cold. Our
experiments incrementally vary the number of interactions for different items
during training, and gradually update the length of user interaction histories
during inference. We investigate the thresholds across several widely used
datasets, commonly represented in recent papers from top-tier conferences, and
on multiple established recommender baselines. Our findings show that
inconsistent selection of cold-start thresholds can either result in the
unnecessary removal of valuable data or lead to the misclassification of cold
instances as warm, introducing more noise into the system.

</details>


### [132] [Careful Queries, Credible Results: Teaching RAG Models Advanced Web Search Tools with Reinforcement Learning](https://arxiv.org/abs/2508.07956)
*Yuqin Dai,Shuo Yang,Guoqing Wang,Yong Deng,Zhanwei Zhang,Jun Yin,Pengyu Zeng,Zhenzhe Ying,Changhua Meng,Can Yi,Yuchen Zhou,Weiqiang Wang,Shuai Lu*

Main category: cs.IR

TL;DR: 提出WebFilter框架解决RAG系统在真实网络环境中的挑战，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: RAG系统在真实网络环境面临网络信息误导和网络工具利用不足的挑战，需提升检索准确性。

Method: 提出WebFilter框架，结合检索过滤机制与行为和结果驱动的奖励策略。

Result: WebFilter在内外领域基准测试中提高了答案质量和检索精度，优于现有RAG方法。

Conclusion: WebFilter能有效解决RAG系统在网络环境中的问题，提升性能。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating up-to-date external knowledge, yet real-world web environments
present unique challenges. These limitations manifest as two key challenges:
pervasive misinformation in the web environment, which introduces unreliable or
misleading content that can degrade retrieval accuracy, and the
underutilization of web tools, which, if effectively employed, could enhance
query precision and help mitigate this noise, ultimately improving the
retrieval results in RAG systems. To address these issues, we propose
WebFilter, a novel RAG framework that generates source-restricted queries and
filters out unreliable content. This approach combines a retrieval filtering
mechanism with a behavior- and outcome-driven reward strategy, optimizing both
query formulation and retrieval outcomes. Extensive experiments demonstrate
that WebFilter improves answer quality and retrieval precision, outperforming
existing RAG methods on both in-domain and out-of-domain benchmarks.

</details>


### [133] [Improving Document Retrieval Coherence for Semantically Equivalent Queries](https://arxiv.org/abs/2508.07975)
*Stefano Campese,Alessandro Moschitti,Ivano Lauriola*

Main category: cs.IR

TL;DR: 本文提出改进Dense Retrieval (DR) 模型训练损失函数的方法，实验表明能降低模型敏感性并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 流行的DR模型对查询和文档词汇敏感，小变化会导致检索文档集差异大，需提高模型在语义相似查询时检索文档的一致性。

Method: 提出一种Multi - Negative Ranking损失函数的变体，惩罚语义等价但不同查询下top - k排名文档的差异。

Result: 在多个数据集上实验，模型经该损失函数优化后敏感性降低，准确率提高。

Conclusion: 所提损失函数能提升DR模型性能，降低敏感性并提高准确率。

Abstract: Dense Retrieval (DR) models have proven to be effective for Document
Retrieval and Information Grounding tasks. Usually, these models are trained
and optimized for improving the relevance of top-ranked documents for a given
query. Previous work has shown that popular DR models are sensitive to the
query and document lexicon: small variations of it may lead to a significant
difference in the set of retrieved documents. In this paper, we propose a
variation of the Multi-Negative Ranking loss for training DR that improves the
coherence of models in retrieving the same documents with respect to
semantically similar queries. The loss penalizes discrepancies between the
top-k ranked documents retrieved for diverse but semantic equivalent queries.
We conducted extensive experiments on various datasets, MS-MARCO, Natural
Questions, BEIR, and TREC DL 19/20. The results show that (i) models optimizes
by our loss are subject to lower sensitivity, and, (ii) interestingly, higher
accuracy.

</details>


### [134] [DIVER: A Multi-Stage Approach for Reasoning-intensive Information Retrieval](https://arxiv.org/abs/2508.07995)
*Meixiu Long,Duolin Sun,Dan Yang,Junjie Wang,Yue Shen,Jian Wang,Peng Wei,Jinjie Gu,Jiahai Wang*

Main category: cs.IR

TL;DR: 本文提出检索管道DIVER用于推理密集型信息检索，在BRIGHT基准测试中表现出色，证明推理感知检索策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有检索器难以处理涉及抽象推理、类比思维或多步推理的现实查询，需要新的检索方法。

Method: DIVER由文档处理、基于大语言模型的查询扩展、推理增强的检索器、点式重排器四个组件组成。

Result: 在BRIGHT基准测试中，DIVER在原始查询上达到了41.6和28.9的nDCG@10得分，始终优于有竞争力的推理感知模型。

Conclusion: 推理感知的检索策略在复杂现实任务中有效，代码和检索模型即将发布。

Abstract: Retrieval-augmented generation has achieved strong performance on
knowledge-intensive tasks where query-document relevance can be identified
through direct lexical or semantic matches. However, many real-world queries
involve abstract reasoning, analogical thinking, or multi-step inference, which
existing retrievers often struggle to capture. To address this challenge, we
present \textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive
information retrieval. DIVER consists of four components: document processing
to improve input quality, LLM-driven query expansion via iterative document
interaction, a reasoning-enhanced retriever fine-tuned on synthetic
multi-domain data with hard negatives, and a pointwise reranker that combines
LLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark,
DIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original
queries, consistently outperforming competitive reasoning-aware models. These
results demonstrate the effectiveness of reasoning-aware retrieval strategies
in complex real-world tasks. Our code and retrieval model will be released
soon.

</details>


### [135] [Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation](https://arxiv.org/abs/2508.08042)
*Van-Khang Nguyen,Duc-Hoang Pham,Huy-Son Nguyen,Cam-Van Thi Nguyen,Hoang-Quynh Le,Duc-Trong Le*

Main category: cs.IR

TL;DR: 研究提出MAMEX框架解决多模态冷启动推荐问题，实验表明其性能优于现有方法，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有多模态冷启动推荐方法集成方式简单，无法捕捉模态间复杂关系，需新方法解决冷启动挑战。

Method: 提出MAMEX框架，利用特定模态专家网络和可学习门控机制，根据内容特征自适应加权各模态贡献。

Result: 在基准数据集上的实验显示，MAMEX在冷启动场景中准确性和适应性优于现有方法。

Conclusion: MAMEX框架能有效解决多模态冷启动推荐问题，具有更好的性能。

Abstract: Recommendation systems have faced significant challenges in cold-start
scenarios, where new items with a limited history of interaction need to be
effectively recommended to users. Though multimodal data (e.g., images, text,
audio, etc.) offer rich information to address this issue, existing approaches
often employ simplistic integration methods such as concatenation, average
pooling, or fixed weighting schemes, which fail to capture the complex
relationships between modalities. Our study proposes a novel Mixture of Experts
(MoE) framework for multimodal cold-start recommendation, named MAMEX, which
dynamically leverages latent representation from different modalities. MAMEX
utilizes modality-specific expert networks and introduces a learnable gating
mechanism that adaptively weights the contribution of each modality based on
its content characteristics. This approach enables MAMEX to emphasize the most
informative modalities for each item while maintaining robustness when certain
modalities are less relevant or missing. Extensive experiments on benchmark
datasets show that MAMEX outperforms state-of-the-art methods in cold-start
scenarios, with superior accuracy and adaptability. For reproducibility, the
code has been made available on Github https://github.com/L2R-UET/MAMEX.

</details>


### [136] [HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches](https://arxiv.org/abs/2508.08088)
*Jiejun Tan,Zhicheng Dou,Yan Yu,Jiehan Cheng,Qiang Ju,Jian Xie,Ji-Rong Wen*

Main category: cs.IR

TL;DR: 本文提出分层代理深度搜索框架HierSearch以解决现有单源深度搜索问题，实验表明其性能优于扁平强化学习及多种基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度搜索局限于单一知识源，企业需要能利用本地和网络语料的私有深度搜索系统，而简单的扁平强化学习训练存在数据效率低和工具掌握差的问题。

Method: 提出HierSearch框架，用分层强化学习训练，底层有本地和网络深度搜索代理，高层有规划代理协调并给出最终答案，还设计知识精炼器过滤无关证据。

Result: HierSearch在六个基准测试中表现优于扁平强化学习和多种深度搜索及多源检索增强生成基线模型。

Conclusion: HierSearch能有效解决现有深度搜索问题，在多领域基准测试中展现出良好性能。

Abstract: Recently, large reasoning models have demonstrated strong mathematical and
coding abilities, and deep search leverages their reasoning capabilities in
challenging information retrieval tasks. Existing deep search works are
generally limited to a single knowledge source, either local or the Web.
However, enterprises often require private deep search systems that can
leverage search tools over both local and the Web corpus. Simply training an
agent equipped with multiple search tools using flat reinforcement learning
(RL) is a straightforward idea, but it has problems such as low training data
efficiency and poor mastery of complex tools. To address the above issue, we
propose a hierarchical agentic deep search framework, HierSearch, trained with
hierarchical RL. At the low level, a local deep search agent and a Web deep
search agent are trained to retrieve evidence from their corresponding domains.
At the high level, a planner agent coordinates low-level agents and provides
the final answer. Moreover, to prevent direct answer copying and error
propagation, we design a knowledge refiner that filters out hallucinations and
irrelevant evidence returned by low-level agents. Experiments show that
HierSearch achieves better performance compared to flat RL, and outperforms
various deep search and multi-source retrieval-augmented generation baselines
in six benchmarks across general, finance, and medical domains.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [137] [Generalizing Scaling Laws for Dense and Sparse Large Language Models](https://arxiv.org/abs/2508.06617)
*Md Arafat Hossain,Xingfu Wu,Valerie Taylor,Ali Jannesari*

Main category: cs.LG

TL;DR: 语言模型规模和训练成本增长促使研究提升训练效率，本文提出通用缩放定律并评估其有效性。


<details>
  <summary>Details</summary>
Motivation: 语言模型规模和训练成本指数增长，现有缩放定律多针对特定架构，难以最优预测模型大小和分配资源。

Method: 重新审视现有缩放定律，提出适用于稠密和稀疏大语言模型的通用缩放定律。

Result: 评估并比较了提出的缩放定律与现有缩放定律。

Conclusion: 提出的通用缩放定律有效，为稠密和稀疏大语言模型提供了统一框架。

Abstract: Over the past few years, the size of language models has grown exponentially,
as has the computational cost to train these large models. This rapid growth
has motivated researchers to develop new techniques aimed at enhancing the
efficiency of the training process. Despite these advancements, optimally
predicting the model size or allocating optimal resources remains a challenge.
Several efforts have addressed the challenge by proposing different scaling
laws, but almost all of them are architecture-specific (dense or sparse). In
this work we revisit existing scaling laws and propose a generalized scaling
law to provide a unified framework that is applicable to both dense and sparse
large language models. We evaluate and compare our proposed scaling law with
existing scaling laws to demonstrate its effectiveness.

</details>


### [138] [Self-Organizing Survival Manifolds: A Theory for Unsupervised Discovery of Prognostic Structures in Biological Systems](https://arxiv.org/abs/2508.06539)
*Atahan Karagoz*

Main category: cs.LG

TL;DR: 本文提出自组织生存流形（SOSM）理论，将生存建模为几何属性而非依赖标注，推导目标公式并证明相关结果，连接多领域知识，提供无标签生存建模基础。


<details>
  <summary>Details</summary>
Motivation: 传统生存建模依赖标注和固定协变量，本文欲打破此前提，从几何角度建模生存。

Method: 提出SOSM理论，引入基于测地曲率最小化的生存能量泛函，推导离散和连续目标公式并证明理论结果。

Result: 证明在生物学合理条件下生存对齐轨迹的出现和收敛，将健康、疾病等现象重新定义为流形结构的几何相变。

Conclusion: 该理论为生存建模提供了通用、无标签的基础，连接了机器学习、生物物理学和生命几何学。

Abstract: Survival is traditionally modeled as a supervised learning task, reliant on
curated outcome labels and fixed covariates. This work rejects that premise. It
proposes that survival is not an externally annotated target but a geometric
consequence: an emergent property of the curvature and flow inherent in
biological state space. We develop a theory of Self-Organizing Survival
Manifolds (SOSM), in which survival-relevant dynamics arise from low-curvature
geodesic flows on latent manifolds shaped by internal biological constraints. A
survival energy functional based on geodesic curvature minimization is
introduced and shown to induce structures where prognosis aligns with geometric
flow stability. We derive discrete and continuous formulations of the objective
and prove theoretical results demonstrating the emergence and convergence of
survival-aligned trajectories under biologically plausible conditions. The
framework draws connections to thermodynamic efficiency, entropy flow, Ricci
curvature, and optimal transport, grounding survival modeling in physical law.
Health, disease, aging, and death are reframed as geometric phase transitions
in the manifold's structure. This theory offers a universal, label-free
foundation for modeling survival as a property of form, not annotation-bridging
machine learning, biophysics, and the geometry of life itself.

</details>


### [139] [Semi-Supervised Supply Chain Fraud Detection with Unsupervised Pre-Filtering](https://arxiv.org/abs/2508.06574)
*Fatemeh Moradi,Mehran Tarif,Mohammadhossein Homaei*

Main category: cs.LG

TL;DR: 本文提出两阶段学习框架解决供应链欺诈检测难题，在真实数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现代供应链欺诈检测因全球网络复杂和标注数据稀缺面临挑战，传统方法存在类别不平衡和监督有限问题。

Method: 第一阶段用Isolation Forest算法进行无监督异常检测；第二阶段用自训练支持向量机结合标注和高置信度伪标注样本进行半监督学习。

Result: 在DataCo智能供应链数据集上F1分数达0.817，假阳性率低于3.0%。

Conclusion: 结合无监督预过滤和半监督细化的方法在现实约束下对供应链欺诈检测有效，但存在概念漂移问题且需与深度学习方法对比。

Abstract: Detecting fraud in modern supply chains is a growing challenge, driven by the
complexity of global networks and the scarcity of labeled data. Traditional
detection methods often struggle with class imbalance and limited supervision,
reducing their effectiveness in real-world applications. This paper proposes a
novel two-phase learning framework to address these challenges. In the first
phase, the Isolation Forest algorithm performs unsupervised anomaly detection
to identify potential fraud cases and reduce the volume of data requiring
further analysis. In the second phase, a self-training Support Vector Machine
(SVM) refines the predictions using both labeled and high-confidence
pseudo-labeled samples, enabling robust semi-supervised learning. The proposed
method is evaluated on the DataCo Smart Supply Chain Dataset, a comprehensive
real-world supply chain dataset with fraud indicators. It achieves an F1-score
of 0.817 while maintaining a false positive rate below 3.0%. These results
demonstrate the effectiveness and efficiency of combining unsupervised
pre-filtering with semi-supervised refinement for supply chain fraud detection
under real-world constraints, though we acknowledge limitations regarding
concept drift and the need for comparison with deep learning approaches.

</details>


### [140] [GFlowNets for Learning Better Drug-Drug Interaction Representations](https://arxiv.org/abs/2508.06576)
*Azmine Toushik Wasi*

Main category: cs.LG

TL;DR: 提出结合GFlowNet与VGAE的框架解决药物相互作用预测中类别不平衡问题，提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 药物相互作用预测存在类别严重不平衡，现有方法将其作为二元问题处理，导致对罕见相互作用预测性能差。

Method: 提出结合Generative Flow Networks (GFlowNet)与Variational Graph Autoencoders (VGAE)的框架，为罕见类别生成合成样本。

Result: 增强了各相互作用类型的预测性能。

Conclusion: 该方法能改善模型平衡，生成有效的新型药物相互作用对，确保更好的临床可靠性。

Abstract: Drug-drug interactions pose a significant challenge in clinical pharmacology,
with severe class imbalance among interaction types limiting the effectiveness
of predictive models. Common interactions dominate datasets, while rare but
critical interactions remain underrepresented, leading to poor model
performance on infrequent cases. Existing methods often treat DDI prediction as
a binary problem, ignoring class-specific nuances and exacerbating bias toward
frequent interactions. To address this, we propose a framework combining
Generative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)
to generate synthetic samples for rare classes, improving model balance and
generate effective and novel DDI pairs. Our approach enhances predictive
performance across interaction types, ensuring better clinical reliability.

</details>


### [141] [Hypergraph Neural Network with State Space Models for Node Classification](https://arxiv.org/abs/2508.06587)
*A. Quadir,M. Tanveer*

Main category: cs.LG

TL;DR: 本文提出超图神经网络HGMN用于节点分类，融合角色感知表征，在多数据集实验中表现优于现有GNN方法。


<details>
  <summary>Details</summary>
Motivation: 传统GNN忽视角色特征，现有捕捉角色特征方法多无监督且下游任务表现不佳。

Method: 提出HGMN，利用超图构建技术、可学习的mamba transformer机制结合角色和邻接表征，使用基于节点度和邻域级别的超图构建方法，加入超图卷积层和残差网络。

Result: 在一个新引入数据集和四个基准数据集实验中，HGMN在节点分类任务上显著优于现有GNN方法。

Conclusion: HGMN能有效嵌入角色特征和邻接信息提供丰富节点表征，是适用于多种图学习应用的强大工具。

Abstract: In recent years, graph neural networks (GNNs) have gained significant
attention for node classification tasks on graph-structured data. However,
traditional GNNs primarily focus on adjacency relationships between nodes,
often overlooking the rich role-based characteristics that are crucial for
learning more expressive node representations. Existing methods for capturing
role-based features are largely unsupervised and fail to achieve optimal
performance in downstream tasks. To address these limitations, we propose a
novel hypergraph neural network with state space model (HGMN) that effectively
integrates role-aware representations into GNNs and the state space model. HGMN
utilizes hypergraph construction techniques to model higher-order relationships
and combines role-based and adjacency-based representations through a learnable
mamba transformer mechanism. By leveraging two distinct hypergraph construction
methods-based on node degree and neighborhood levels, it strengthens the
connections among nodes with similar roles, enhancing the model's
representational power. Additionally, the inclusion of hypergraph convolution
layers enables the model to capture complex dependencies within hypergraph
structures. To mitigate the over-smoothing problem inherent in deep GNNs, we
incorporate a residual network, ensuring improved stability and better feature
propagation across layers. Extensive experiments conducted on one newly
introduced dataset and four benchmark datasets demonstrate the superiority of
HGMN. The model achieves significant performance improvements on node
classification tasks compared to state-of-the-art GNN methods. These results
highlight HGMN's ability to provide enriched node representations by
effectively embedding role-based features alongside adjacency information,
making it a versatile and powerful tool for a variety of graph-based learning
applications.

</details>


### [142] [Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning](https://arxiv.org/abs/2508.06588)
*Zian Zhai,Fan Li,Xingyu Tan,Xiaoyang Wang,Wenjie Zhang*

Main category: cs.LG

TL;DR: 研究图数据向量量化（VQ）中码本崩溃问题，提出RGVQ框架提升性能。


<details>
  <summary>Details</summary>
Motivation: 图数据VQ存在码本崩溃挑战，限制图令牌表达和泛化能力，且现有缓解策略效果不佳。

Method: 进行理论分析找出图VQ易崩溃的两个关键因素，提出RGVQ框架，引入软分配和结构感知对比正则化。

Result: RGVQ大幅提高码本利用率，提升多个下游任务中现有图VQ骨干网络性能。

Conclusion: RGVQ能使图令牌表示更具表达性和可迁移性。

Abstract: Vector Quantization (VQ) has recently emerged as a promising approach for
learning discrete representations of graph-structured data. However, a
fundamental challenge, i.e., codebook collapse, remains underexplored in the
graph domain, significantly limiting the expressiveness and generalization of
graph tokens.In this paper, we present the first empirical study showing that
codebook collapse consistently occurs when applying VQ to graph data, even with
mitigation strategies proposed in vision or language domains. To understand why
graph VQ is particularly vulnerable to collapse, we provide a theoretical
analysis and identify two key factors: early assignment imbalances caused by
redundancy in graph features and structural patterns, and self-reinforcing
optimization loops in deterministic VQ. To address these issues, we propose
RGVQ, a novel framework that integrates graph topology and feature similarity
as explicit regularization signals to enhance codebook utilization and promote
token diversity. RGVQ introduces soft assignments via Gumbel-Softmax
reparameterization, ensuring that all codewords receive gradient updates. In
addition, RGVQ incorporates a structure-aware contrastive regularization to
penalize the token co-assignments among similar node pairs. Extensive
experiments demonstrate that RGVQ substantially improves codebook utilization
and consistently boosts the performance of state-of-the-art graph VQ backbones
across multiple downstream tasks, enabling more expressive and transferable
graph token representations.

</details>


### [143] [A Federated Learning Framework for Handling Subtype Confounding and Heterogeneity in Large-Scale Neuroimaging Diagnosis](https://arxiv.org/abs/2508.06589)
*Xinglin Zhao,Yanwen Wang,Xiaobo Liu,Yanrong Hao,Rui Cao,Xin Wen*

Main category: cs.LG

TL;DR: 提出用于神经影像计算机辅助诊断系统的联邦学习框架，经实验验证有效，能应对数据异质性和亚型混淆问题。


<details>
  <summary>Details</summary>
Motivation: 小样本研究可重复性低，大规模数据集存在亚型混淆问题，需改进神经影像计算机辅助诊断系统。

Method: 提出含动态导航模块和元集成模块的联邦学习框架。

Result: 在实验中诊断准确性和鲁棒性比传统方法显著提升，平均准确率达74.06%，消融实验证实两模块重要性。

Conclusion: 该框架解决了数据异质性和亚型混淆问题，推动了可靠且可重复的神经影像CAD系统发展，在神经和精神医学有应用潜力。

Abstract: Computer-aided diagnosis (CAD) systems play a crucial role in analyzing
neuroimaging data for neurological and psychiatric disorders. However,
small-sample studies suffer from low reproducibility, while large-scale
datasets introduce confounding heterogeneity due to multiple disease subtypes
being labeled under a single category. To address these challenges, we propose
a novel federated learning framework tailored for neuroimaging CAD systems. Our
approach includes a dynamic navigation module that routes samples to the most
suitable local models based on latent subtype representations, and a
meta-integration module that combines predictions from heterogeneous local
models into a unified diagnostic output. We evaluated our framework using a
comprehensive dataset comprising fMRI data from over 1300 MDD patients and 1100
healthy controls across multiple study cohorts. Experimental results
demonstrate significant improvements in diagnostic accuracy and robustness
compared to traditional methods. Specifically, our framework achieved an
average accuracy of 74.06\% across all tested sites, showcasing its
effectiveness in handling subtype heterogeneity and enhancing model
generalizability. Ablation studies further confirmed the importance of both the
dynamic navigation and meta-integration modules in improving performance. By
addressing data heterogeneity and subtype confounding, our framework advances
reliable and reproducible neuroimaging CAD systems, offering significant
potential for personalized medicine and clinical decision-making in neurology
and psychiatry.

</details>


### [144] [Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials](https://arxiv.org/abs/2508.06591)
*Rachel K. Luu,Jingyu Deng,Mohammed Shahrudin Ibrahim,Nam-Joon Cho,Ming Dao,Subra Suresh,Markus J. Buehler*

Main category: cs.LG

TL;DR: 本文提出将生成式AI与多领域文献结合的框架用于材料设计，以湿度响应系统为例，用AI工具提取关系、生成假设，经实验室验证制造出新型花粉基粘合剂，展示了AI辅助构思对材料设计的推动作用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在特定学科实验科学尤其是材料科学多学科领域应用有限，需探索其应用。

Method: 提出整合生成式AI与多领域文献的框架，借助BioinspiredLLM等AI工具，用结构化推理协议生成和评估假设。

Result: 通过实验室验证，制造出具有可调形态和测量剪切强度的新型花粉基粘合剂。

Conclusion: AI辅助构思可推动现实世界材料设计，实现有效的人机协作。

Abstract: Large language models (LLMs) have reshaped the research landscape by enabling
new approaches to knowledge retrieval and creative ideation. Yet their
application in discipline-specific experimental science, particularly in highly
multi-disciplinary domains like materials science, remains limited. We present
a first-of-its-kind framework that integrates generative AI with literature
from hitherto-unconnected fields such as plant science, biomimetics, and
materials engineering to extract insights and design experiments for materials.
We focus on humidity-responsive systems such as pollen-based materials and
Rhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and
adaptive performance. Using a suite of AI tools, including a fine-tuned model
(BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a
Hierarchical Sampling strategy, we extract structure-property relationships and
translate them into new classes of bioinspired materials. Structured inference
protocols generate and evaluate hundreds of hypotheses from a single query,
surfacing novel and experimentally tractable ideas. We validate our approach
through real-world implementation: LLM-generated procedures, materials designs,
and mechanical predictions were tested in the laboratory, culminating in the
fabrication of a novel pollen-based adhesive with tunable morphology and
measured shear strength, establishing a foundation for future plant-derived
adhesive design. This work demonstrates how AI-assisted ideation can drive
real-world materials design and enable effective human-AI collaboration.

</details>


### [145] [Discovery Learning accelerates battery design evaluation](https://arxiv.org/abs/2508.06985)
*Jiawei Zhang,Yifei Zhang,Baozhao Yi,Yao Ren,Qi Jiao,Hanyu Bai,Weiran Jiang,Ziyou Song*

Main category: cs.LG

TL;DR: 本文提出Discovery Learning (DL) 范式用于电池寿命评估，能减少原型制作需求，节省时间和能源，助力电池技术发展。


<details>
  <summary>Details</summary>
Motivation: 电池研发受评估新设计的高时间和能源成本瓶颈限制，现有数据驱动方法效率不足，无法为电池设计提供快速反馈。

Method: 引入DL范式，将主动学习、物理引导学习和零样本学习集成到类人推理循环中，从历史电池设计中学习。

Result: 在123个工业级大尺寸锂离子软包电池测试中，仅在小容量圆柱形电池公共数据集上训练，DL预测平均循环寿命的测试误差为7.2%，相比工业实践节省98%时间和95%能源。

Conclusion: DL有潜力从历史设计中挖掘见解，推动下一代电池技术发展，是高效数据驱动建模的关键进展，有助于实现机器学习加速科学发现和工程创新的愿景。

Abstract: Fast and reliable validation of novel designs in complex physical systems
such as batteries is critical to accelerating technological innovation.
However, battery research and development remain bottlenecked by the
prohibitively high time and energy costs required to evaluate numerous new
design candidates, particularly in battery prototyping and life testing.
Despite recent progress in data-driven battery lifetime prediction, existing
methods require labeled data of target designs to improve accuracy and cannot
make reliable predictions until after prototyping, thus falling far short of
the efficiency needed to enable rapid feedback for battery design. Here, we
introduce Discovery Learning (DL), a scientific machine-learning paradigm that
integrates active learning, physics-guided learning, and zero-shot learning
into a human-like reasoning loop, drawing inspiration from learning theories in
educational psychology. DL can learn from historical battery designs and
actively reduce the need for prototyping, thus enabling rapid lifetime
evaluation for unobserved material-design combinations without requiring
additional data labeling. To test DL, we present 123 industrial-grade
large-format lithium-ion pouch cells, spanning eight material-design
combinations and diverse cycling protocols. Trained solely on public datasets
of small-capacity cylindrical cells, DL achieves 7.2% test error in predicting
the average cycle life under unknown device variability. This results in
savings of 98% in time and 95% in energy compared to industrial practices. This
work highlights the potential of uncovering insights from historical designs to
inform and accelerate the development of next-generation battery technologies.
DL represents a key advance toward efficient data-driven modeling and helps
realize the promise of machine learning for accelerating scientific discovery
and engineering innovation.

</details>


### [146] [Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs](https://arxiv.org/abs/2508.06601)
*Kyle O'Brien,Stephen Casper,Quentin Anthony,Tomek Korbak,Robert Kirk,Xander Davies,Ishan Mishra,Geoffrey Irving,Yarin Gal,Stella Biderman*

Main category: cs.LG

TL;DR: 研究过滤训练数据中两用主题文本能否让大语言模型更抗篡改，提出多阶段数据过滤管道，预训练模型抗攻击表现好，还指出需深度防御。


<details>
  <summary>Details</summary>
Motivation: 开放权重AI系统易受篡改攻击，现有的安全微调等方法难以让大语言模型抵抗多轮对抗性微调，且缺乏有效的风险管理科学。

Method: 引入多阶段可扩展数据过滤管道，从头预训练多个69亿参数模型。

Result: 预训练模型对多达10000步和3亿个生物威胁相关文本标记的对抗性微调攻击表现出显著抗性，远超现有后训练基线，且不影响无关能力。但过滤模型在有上下文信息时仍能利用危险知识。

Conclusion: 预训练数据筛选是开放权重AI系统有前景的防御手段，同时需要深度防御方法。

Abstract: Open-weight AI systems offer unique benefits, including enhanced
transparency, open research, and decentralized access. However, they are
vulnerable to tampering attacks which can efficiently elicit harmful behaviors
by modifying weights or activations. Currently, there is not yet a robust
science of open-weight model risk management. Existing safety fine-tuning
methods and other post-training techniques have struggled to make LLMs
resistant to more than a few dozen steps of adversarial fine-tuning. In this
paper, we investigate whether filtering text about dual-use topics from
training data can prevent unwanted capabilities and serve as a more
tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable
data filtering and show that it offers a tractable and effective method for
minimizing biothreat proxy knowledge in LLMs. We pretrain multiple
6.9B-parameter models from scratch and find that they exhibit substantial
resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M
tokens of biothreat-related text -- outperforming existing post-training
baselines by over an order of magnitude -- with no observed degradation to
unrelated capabilities. However, while filtered models lack internalized
dangerous knowledge, we find that they can still leverage such information when
it is provided in context (e.g., via search tool augmentation), demonstrating a
need for a defense-in-depth approach. Overall, these findings help to establish
pretraining data curation as a promising layer of defense for open-weight AI
systems.

</details>


### [147] [Semantic-Enhanced Time-Series Forecasting via Large Language Models](https://arxiv.org/abs/2508.07697)
*Hao Liu,Chun Yang,Zhang xiaoxing,Xiaobin Zhu*

Main category: cs.LG

TL;DR: 提出SE - LLM用于时间序列预测，结合周期性与异常特征增强语义嵌入，添加插件模块处理长短期依赖，降低计算量，实验效果优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究聚焦于token级模态对齐，未弥合语言知识结构与时间序列数据模式的内在模态差距，限制语义表示；Transformer - based LLMs在处理时间序列短期异常能力弱。

Method: 提出Semantic - Enhanced LLM (SE - LLM)，探索时间序列固有周期性和异常特征嵌入语义空间增强token嵌入；提出插件模块嵌入自注意力机制，处理长短期依赖；冻结LLM并降低token序列维度。

Result: 实验表明SE - LLM性能优于现有SOTA方法。

Conclusion: SE - LLM能有效弥合语言知识与时间序列数据模态差距，处理长短期依赖，降低计算量，在时间序列预测中表现出色。

Abstract: Time series forecasting plays a significant role in finance, energy,
meteorology, and IoT applications. Recent studies have leveraged the
generalization capabilities of large language models (LLMs) to adapt to time
series forecasting, achieving promising performance. However, existing studies
focus on token-level modal alignment, instead of bridging the intrinsic
modality gap between linguistic knowledge structures and time series data
patterns, greatly limiting the semantic representation. To address this issue,
we propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent
periodicity and anomalous characteristics of time series to embed into the
semantic space to enhance the token embedding. This process enhances the
interpretability of tokens for LLMs, thereby activating the potential of LLMs
for temporal sequence analysis. Moreover, existing Transformer-based LLMs excel
at capturing long-range dependencies but are weak at modeling short-term
anomalies in time-series data. Hence, we propose a plugin module embedded
within self-attention that models long-term and short-term dependencies to
effectively adapt LLMs to time-series analysis. Our approach freezes the LLM
and reduces the sequence dimensionality of tokens, greatly reducing
computational consumption. Experiments demonstrate the superiority performance
of our SE-LLM against the state-of-the-art (SOTA) methods.

</details>


### [148] [Local Diffusion Models and Phases of Data Distributions](https://arxiv.org/abs/2508.06614)
*Fangjun Hu,Guangkuo Liu,Yifan Zhang,Xun Gao*

Main category: cs.LG

TL;DR: 本文引入数据分布阶段新视角，分析扩散模型反向去噪过程阶段，证明信息论界，实验验证，提出更高效扩散模型架构。


<details>
  <summary>Details</summary>
Motivation: 普通扩散模型忽略数据局部结构，学习全局得分函数计算成本高，需构建低成本局部去噪器。

Method: 定义数据分布相同阶段，分析反向去噪过程阶段，证明基于条件互信息的局部去噪器保真度信息论界，进行数值实验。

Result: 反向去噪过程包含早期平凡阶段、晚期数据阶段和快速相变阶段，远离相变点可用小局部神经网络，相变点附近需全局神经网络。

Conclusion: 提出更简单高效的扩散模型架构，为数据分布阶段研究、生成式人工智能和受物理概念启发的神经网络设计开辟新方向。

Abstract: As a class of generative artificial intelligence frameworks inspired by
statistical physics, diffusion models have shown extraordinary performance in
synthesizing complicated data distributions through a denoising process
gradually guided by score functions. Real-life data, like images, is often
spatially structured in low-dimensional spaces. However, ordinary diffusion
models ignore this local structure and learn spatially global score functions,
which are often computationally expensive. In this work, we introduce a new
perspective on the phases of data distributions, which provides insight into
constructing local denoisers with reduced computational costs. We define two
distributions as belonging to the same data distribution phase if they can be
mutually connected via spatially local operations such as local denoisers.
Then, we show that the reverse denoising process consists of an early trivial
phase and a late data phase, sandwiching a rapid phase transition where local
denoisers must fail. To diagnose such phase transitions, we prove an
information-theoretic bound on the fidelity of local denoisers based on
conditional mutual information, and conduct numerical experiments in a
real-world dataset. This work suggests simpler and more efficient architectures
of diffusion models: far from the phase transition point, we can use small
local neural networks to compute the score function; global neural networks are
only necessary around the narrow time interval of phase transitions. This
result also opens up new directions for studying phases of data distributions,
the broader science of generative artificial intelligence, and guiding the
design of neural networks inspired by physics concepts.

</details>


### [149] [Learning to Forget with Information Divergence Reweighted Objectives for Noisy Labels](https://arxiv.org/abs/2508.06622)
*Jeremiah Birrell,Reza Ebrahimi*

Main category: cs.LG

TL;DR: 介绍ANTIDOTE目标函数用于噪声标签学习，通过凸对偶转化为对抗训练方法，实验显示其优于同类方法且时间复杂度接近标准交叉熵损失。


<details>
  <summary>Details</summary>
Motivation: 解决噪声标签学习问题，在训练数据存在固有标签噪声或对手可更改训练标签的实际环境中有效学习。

Method: 引入基于信息散度邻域松弛定义的ANTIDOTE目标函数，利用凸对偶将其重新表述为对抗训练方法。

Result: ANTIDOTE能自适应减少噪声标签样本影响，在不同类型标签噪声的大量实验中，表现优于该领域领先的可比损失函数，时间复杂度接近标准交叉熵损失。

Conclusion: ANTIDOTE是一种有效的噪声标签学习目标函数，在实际应用中有良好表现。

Abstract: We introduce ANTIDOTE, a new class of objectives for learning under noisy
labels which are defined in terms of a relaxation over an
information-divergence neighborhood. Using convex duality, we provide a
reformulation as an adversarial training method that has similar computational
cost to training with standard cross-entropy loss. We show that our approach
adaptively reduces the influence of the samples with noisy labels during
learning, exhibiting a behavior that is analogous to forgetting those samples.
ANTIDOTE is effective in practical environments where label noise is inherent
in the training data or where an adversary can alter the training labels.
Extensive empirical evaluations on different levels of symmetric, asymmetric,
human annotation, and real-world label noise show that ANTIDOTE outperforms
leading comparable losses in the field and enjoys a time complexity that is
very close to that of the standard cross entropy loss.

</details>


### [150] [Symbolic Quantile Regression for the Interpretable Prediction of Conditional Quantiles](https://arxiv.org/abs/2508.08080)
*Cas Oude Hoekstra,Floris den Hengst*

Main category: cs.LG

TL;DR: 本文介绍了符号分位数回归（SQR）方法，评估显示其在预测条件分位数上表现良好，适用于相关场景。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归（SR）不明确如何在目标变量分布其他点估计变量关系，而分位数估计在高风险领域必要，因此研究新方法。

Method: 引入符号分位数回归（SQR）方法来用SR预测条件分位数。

Result: SQR优于透明模型，与强黑盒基线表现相当且不损失透明度，在航空燃油使用案例中可解释目标分布差异。

Conclusion: SQR适用于预测条件分位数和理解不同分位数上特征的影响。

Abstract: Symbolic Regression (SR) is a well-established framework for generating
interpretable or white-box predictive models. Although SR has been successfully
applied to create interpretable estimates of the average of the outcome, it is
currently not well understood how it can be used to estimate the relationship
between variables at other points in the distribution of the target variable.
Such estimates of e.g. the median or an extreme value provide a fuller picture
of how predictive variables affect the outcome and are necessary in
high-stakes, safety-critical application domains. This study introduces
Symbolic Quantile Regression (SQR), an approach to predict conditional
quantiles with SR. In an extensive evaluation, we find that SQR outperforms
transparent models and performs comparably to a strong black-box baseline
without compromising transparency. We also show how SQR can be used to explain
differences in the target distribution by comparing models that predict extreme
and central outcomes in an airline fuel usage case study. We conclude that SQR
is suitable for predicting conditional quantiles and understanding interesting
feature influences at varying quantiles.

</details>


### [151] [Using Imperfect Synthetic Data in Downstream Inference Tasks](https://arxiv.org/abs/2508.06635)
*Yewon Byun,Shantanu Gupta,Zachary C. Lipton,Rachel Leah Childers,Bryan Wilder*

Main category: cs.LG

TL;DR: 本文介绍基于广义矩估计的新估计器，结合合成数据和真实数据，在计算社会科学应用中验证其有限样本性能，取得显著实证增益。


<details>
  <summary>Details</summary>
Motivation: 此前虽有研究探索用大语言模型预测标签和生成合成样本，但不清楚如何将合成数据与真实数据结合以得出统计有效结论。

Method: 引入基于广义矩估计的新估计器，是无超参数且有强理论保证的解决方案。

Result: 发现合成数据和真实数据的矩残差相互作用可改善目标参数估计，在不同回归任务中验证了估计器有限样本性能，有较大实证增益。

Conclusion: 所提出的新估计器能有效解决合成数据与真实数据结合并得出统计有效结论的问题。

Abstract: Predictions and generations from large language models are increasingly being
explored as an aid to computational social science and human subject research
in limited data regimes. While previous technical work has explored the
potential to use model-predicted labels for unlabeled data in a principled
manner, there is increasing interest in using large language models to generate
entirely new synthetic samples (also termed as synthetic simulations), such as
in responses to surveys. However, it is not immediately clear by what means
practitioners can combine such data with real data and yet produce
statistically valid conclusions upon them. In this work, we introduce a new
estimator based on generalized method of moments, providing a
hyperparameter-free solution with strong theoretical guarantees to address the
challenge at hand. Surprisingly, we find that interactions between the moment
residuals of synthetic data and those of real data can improve estimates of the
target parameter. We empirically validate the finite-sample performance of our
estimator across different regression tasks in computational social science
applications, demonstrating large empirical gains.

</details>


### [152] [Early Detection of Pancreatic Cancer Using Multimodal Learning on Electronic Health Record](https://arxiv.org/abs/2508.06627)
*Mosbah Aouad,Anirudh Choudhary,Awais Farooq,Steven Nevers,Lusine Demirkhanyan,Bhrandon Harris,Suguna Pappu,Christopher Gondi,Ravishankar Iyer*

Main category: cs.LG

TL;DR: 提出多模态方法结合电子健康记录检测胰腺癌，在真实数据集上效果优且找到相关生物标志物。


<details>
  <summary>Details</summary>
Motivation: 胰腺癌致命且早期检测因缺乏特定症状和可靠生物标志物而困难。

Method: 结合神经受控微分方程、预训练语言模型、循环网络和交叉注意力机制处理电子健康记录中纵向诊断代码历史和实验室测量数据。

Result: 在近4700名患者的真实数据集上，AUC比现有方法提高6.5% - 15.5%，模型识别出与胰腺癌风险升高相关的诊断代码和实验室指标。

Conclusion: 提出的多模态方法能有效提前检测胰腺癌，有较好应用前景。

Abstract: Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and
early detection remains a major clinical challenge due to the absence of
specific symptoms and reliable biomarkers. In this work, we propose a new
multimodal approach that integrates longitudinal diagnosis code histories and
routinely collected laboratory measurements from electronic health records to
detect PDAC up to one year prior to clinical diagnosis. Our method combines
neural controlled differential equations to model irregular lab time series,
pretrained language models and recurrent networks to learn diagnosis code
trajectory representations, and cross-attention mechanisms to capture
interactions between the two modalities. We develop and evaluate our approach
on a real-world dataset of nearly 4,700 patients and achieve significant
improvements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods.
Furthermore, our model identifies diagnosis codes and laboratory panels
associated with elevated PDAC risk, including both established and new
biomarkers. Our code is available at
https://github.com/MosbahAouad/EarlyPDAC-MML.

</details>


### [153] [Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift](https://arxiv.org/abs/2508.06776)
*Amit Pandey*

Main category: cs.LG

TL;DR: 提出无任务标签和输出评估的理论框架Zero - Direction Probing (ZDP)检测模型漂移，证明多个定理，推导SNL指标，表明监测零空间可保证检测表示变化。


<details>
  <summary>Details</summary>
Motivation: 在无任务标签和输出评估的情况下检测模型漂移。

Method: 提出Zero - Direction Probing (ZDP)框架，在假设A1 - A6下证明多个定理，推导Spectral Null - Leakage (SNL)指标。

Result: 证明了Variance -- Leak定理、Fisher Null - Conservation、低秩更新的Rank -- Leak界和在线零空间跟踪器的对数遗憾保证，推导有非渐近尾界和集中不等式的SNL指标。

Conclusion: 监测层激活的左右零空间及其Fisher几何能为表示变化提供具体可测试的保证。

Abstract: We present Zero-Direction Probing (ZDP), a theory-only framework for
detecting model drift from null directions of transformer activations without
task labels or output evaluations. Under assumptions A1--A6, we prove: (i) the
Variance--Leak Theorem, (ii) Fisher Null-Conservation, (iii) a Rank--Leak bound
for low-rank updates, and (iv) a logarithmic-regret guarantee for online
null-space trackers. We derive a Spectral Null-Leakage (SNL) metric with
non-asymptotic tail bounds and a concentration inequality, yielding a-priori
thresholds for drift under a Gaussian null model. These results show that
monitoring right/left null spaces of layer activations and their Fisher
geometry provides concrete, testable guarantees on representational change.

</details>


### [154] [Strategic Incentivization for Locally Differentially Private Federated Learning](https://arxiv.org/abs/2508.07138)
*Yashwant Krishna Pagoti,Arunesh Sinha,Shamik Sural*

Main category: cs.LG

TL;DR: 本文将联邦学习中的隐私 - 准确性权衡建模为博弈，引入基于令牌的激励机制，并进行战略分析和实验。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中使用局部差分隐私添加噪声导致全局模型精度下降的问题。

Method: 将隐私 - 准确性权衡建模为博弈，引入基于令牌的激励机制，根据梯度扰动程度给客户端分配令牌，客户端需积累足够令牌才能访问更新后的全局模型，同时进行战略分析。

Result: 进行了大量实验研究不同参数的影响。

Conclusion: 未在摘要中明确提及具体结论。

Abstract: In Federated Learning (FL), multiple clients jointly train a machine learning
model by sharing gradient information, instead of raw data, with a server over
multiple rounds. To address the possibility of information leakage in spite of
sharing only the gradients, Local Differential Privacy (LDP) is often used. In
LDP, clients add a selective amount of noise to the gradients before sending
the same to the server. Although such noise addition protects the privacy of
clients, it leads to a degradation in global model accuracy. In this paper, we
model this privacy-accuracy trade-off as a game, where the sever incentivizes
the clients to add a lower degree of noise for achieving higher accuracy, while
the clients attempt to preserve their privacy at the cost of a potential loss
in accuracy. A token based incentivization mechanism is introduced in which the
quantum of tokens credited to a client in an FL round is a function of the
degree of perturbation of its gradients. The client can later access a newly
updated global model only after acquiring enough tokens, which are to be
deducted from its balance. We identify the players, their actions and payoff,
and perform a strategic analysis of the game. Extensive experiments were
carried out to study the impact of different parameters.

</details>


### [155] [Multi-Hop Privacy Propagation for Differentially Private Federated Learning in Social Networks](https://arxiv.org/abs/2508.07676)
*Chenchen Lin,Xuehe Wang*

Main category: cs.LG

TL;DR: 提出一种社会感知的隐私保护联邦学习机制，量化间接隐私泄漏，通过博弈论优化激励策略，实验显示该方法在提升客户效用、降低服务器成本和保持模型性能方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习中社会连接引入隐私外部性问题，客户隐私损失受他人隐私决策影响，需要解决该问题。

Method: 提出多跳传播模型量化间接隐私泄漏，将服务器 - 客户端交互构建为两阶段Stackelberg博弈，引入平均场估计器缓解信息不对称，理论证明平均场估计器不动点存在与收敛性，推导Stackelberg纳什均衡的闭式解。

Result: 通过无政府价格分析表明该机制能实现近似最优社会福利，在不同数据集上实验显示可显著提升客户效用、降低服务器成本，同时保持模型性能，优于相关基线方法。

Conclusion: 所提出的社会感知隐私保护联邦学习机制有效，能在联邦学习中解决隐私外部性问题，平衡各方利益。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, thereby enhancing privacy and
facilitating collaboration among clients connected via social networks.
However, these social connections introduce privacy externalities: a client's
privacy loss depends not only on its privacy protection strategy but also on
the privacy decisions of others, propagated through the network via multi-hop
interactions. In this work, we propose a socially-aware privacy-preserving FL
mechanism that systematically quantifies indirect privacy leakage through a
multi-hop propagation model. We formulate the server-client interaction as a
two-stage Stackelberg game, where the server, as the leader, optimizes
incentive policies, and clients, as followers, strategically select their
privacy budgets, which determine their privacy-preserving levels by controlling
the magnitude of added noise. To mitigate information asymmetry in networked
privacy estimation, we introduce a mean-field estimator to approximate the
average external privacy risk. We theoretically prove the existence and
convergence of the fixed point of the mean-field estimator and derive
closed-form expressions for the Stackelberg Nash Equilibrium. Despite being
designed from a client-centric incentive perspective, our mechanism achieves
approximately-optimal social welfare, as revealed by Price of Anarchy (PoA)
analysis. Experiments on diverse datasets demonstrate that our approach
significantly improves client utilities and reduces server costs while
maintaining model performance, outperforming both Social-Agnostic (SA)
baselines and methods that account for social externalities.

</details>


### [156] [Segmented Confidence Sequences and Multi-Scale Adaptive Confidence Segments for Anomaly Detection in Nonstationary Time Series](https://arxiv.org/abs/2508.06638)
*Muyan Anna Li,Aditi Gautam*

Main category: cs.LG

TL;DR: 提出SCS和MACS两种自适应阈值框架用于非平稳环境下时间序列异常检测，实验显示F1分数有显著提升。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在多领域日益普遍，传统静态阈值无法适应非平稳环境的统计特征变化。

Method: 引入并实证评估Segmented Confidence Sequences (SCS)和Multi-Scale Adaptive Confidence Segments (MACS)两种自适应阈值框架，利用统计在线学习和分割原则进行局部、上下文敏感的自适应调整。

Result: 在Wafer Manufacturing基准数据集上的实验表明，与传统百分位数和滚动分位数方法相比，F1分数显著提高。

Conclusion: 基于统计原理的鲁棒自适应阈值能够实现对现实世界中各种异常的可靠、可解释和及时检测。

Abstract: As time series data become increasingly prevalent in domains such as
manufacturing, IT, and infrastructure monitoring, anomaly detection must adapt
to nonstationary environments where statistical properties shift over time.
Traditional static thresholds are easily rendered obsolete by regime shifts,
concept drift, or multi-scale changes. To address these challenges, we
introduce and empirically evaluate two novel adaptive thresholding frameworks:
Segmented Confidence Sequences (SCS) and Multi-Scale Adaptive Confidence
Segments (MACS). Both leverage statistical online learning and segmentation
principles for local, contextually sensitive adaptation, maintaining guarantees
on false alarm rates even under evolving distributions. Our experiments across
Wafer Manufacturing benchmark datasets show significant F1-score improvement
compared to traditional percentile and rolling quantile approaches. This work
demonstrates that robust, statistically principled adaptive thresholds enable
reliable, interpretable, and timely detection of diverse real-world anomalies.

</details>


### [157] [PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems](https://arxiv.org/abs/2508.06767)
*Arman Dogru,R. Irem Bor-Yaliniz,Nimal Gamini Senarath*

Main category: cs.LG

TL;DR: 本文聚焦数字孪生生态系统中数据处理，提出PANAMA算法，在路径规划性能上表现出色，优化数据共享策略，促进数字孪生、无线网络和AI自动化协同。


<details>
  <summary>Details</summary>
Motivation: 随着机器人和自动化系统规模扩大，需要高效数据共享框架和强大算法，探索下一代网络中数据处理的关键作用。

Method: 引入PANAMA算法，采用集中训练与分散执行框架和异步演员 - 学习者架构。

Result: PANAMA在路径规划的准确性、速度和可扩展性上优于现有基准，通过模拟突出了可扩展自动化系统的优化数据共享策略。

Conclusion: PANAMA弥合了网络感知决策和强大多智能体协调之间的差距，推动数字孪生、无线网络和AI驱动自动化的协同发展。

Abstract: Digital Twins (DTs) are transforming industries through advanced data
processing and analysis, positioning the world of DTs, Digital World, as a
cornerstone of nextgeneration technologies including embodied AI. As robotics
and automated systems scale, efficient data-sharing frameworks and robust
algorithms become critical. We explore the pivotal role of data handling in
next-gen networks, focusing on dynamics between application and network
providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with
Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL)
based multi-agent path finding (MAPF). By adopting a Centralized Training with
Decentralized Execution (CTDE) framework and asynchronous actor-learner
architectures, PANAMA accelerates training while enabling autonomous task
execution by embodied AI. Our approach demonstrates superior pathfinding
performance in accuracy, speed, and scalability compared to existing
benchmarks. Through simulations, we highlight optimized data-sharing strategies
for scalable, automated systems, ensuring resilience in complex, real-world
environments. PANAMA bridges the gap between network-aware decision-making and
robust multi-agent coordination, advancing the synergy between DTs, wireless
networks, and AI-driven automation.

</details>


### [158] [Fractal Language Modelling by Universal Sequence Maps (USM)](https://arxiv.org/abs/2508.06641)
*Jonas S Almeida,Daniel E Russ,Susana Vinga,Ines Duarte,Lee Mason,Praphulla Bhawsar,Aaron Ge,Arlindo Oliveira,Jeya Balaji Balasubramanian*

Main category: cs.LG

TL;DR: 本文聚焦通用序列映射（USM）双射分形编码，解决迭代过程中的种子偏差问题，取得定位与序列身份匹配及发现 USM 收敛特性的成果，且适用于任意基数字母表。


<details>
  <summary>Details</summary>
Motivation: 随着基于 Transformer 的语言模型兴起，人们对多尺度和嵌入维度的符号序列编码程序重燃兴趣，编码需解决保留符号序列上下文信息的问题。

Method: 利用通用序列映射（USM），它由两个混沌游戏表示（CGR）前后迭代组成，可投影到频域（FCGR），用其坐标计算切比雪夫距离和 k - 元频率。

Result: 解决影响迭代过程的种子偏差问题，实现数值定位与序列身份完全匹配，发现 USM 是收敛于稳态序列嵌入解的高效数值过程，对基因组序列进行了说明且适用于任意基数字母表。

Conclusion: 通用序列映射（USM）在解决种子偏差后，能有效实现双射分形编码，具有良好的通用性和收敛特性。

Abstract: Motivation: With the advent of Language Models using Transformers,
popularized by ChatGPT, there is a renewed interest in exploring encoding
procedures that numerically represent symbolic sequences at multiple scales and
embedding dimensions. The challenge that encoding addresses is the need for
mechanisms that uniquely retain contextual information about the succession of
individual symbols, which can then be modeled by nonlinear formulations such as
neural networks.
  Context: Universal Sequence Maps(USM) are iterated functions that bijectively
encode symbolic sequences onto embedded numerical spaces. USM is composed of
two Chaos Game Representations (CGR), iterated forwardly and backwardly, that
can be projected into the frequency domain (FCGR). The corresponding USM
coordinates can be used to compute a Chebyshev distance metric as well as k-mer
frequencies, without having to recompute the embedded numeric coordinates, and,
paradoxically, allowing for non-integers values of k.
  Results: This report advances the bijective fractal encoding by Universal
Sequence Maps (USM) by resolving seeding biases affecting the iterated process.
The resolution had two results, the first expected, the second an intriguing
outcome: 1) full reconciliation of numeric positioning with sequence identity;
and 2) uncovering the nature of USM as an efficient numeric process converging
towards a steady state sequence embedding solution. We illustrate these results
for genomic sequences because of the convenience of a planar representation
defined by an alphabet with only 4 tokens (the 4 nucleotides). Nevertheless,
the application to alphabet of arbitrary cardinality was found to be
straightforward.

</details>


### [159] [Tight Bounds for Schrödinger Potential Estimation in Unpaired Image-to-Image Translation Problems](https://arxiv.org/abs/2508.07392)
*Nikita Puchkin,Denis Suchkov,Alexey Naumov,Denis Belomestny*

Main category: cs.LG

TL;DR: 本文基于薛定谔桥和随机最优控制理论，在仅有初始和最终分布的独立同分布样本情况下，选择奥恩斯坦 - 乌伦贝克过程估计薛定谔势，推导经验风险最小化器泛化能力的边界并通过实验验证性能。


<details>
  <summary>Details</summary>
Motivation: 在仅有初始和最终分布的独立同分布样本的情况下，实现从初始密度到目标密度的最优转换，适用于生成式建模和无配对图像到图像的转换。

Method: 采用随机最优控制方法，选择奥恩斯坦 - 乌伦贝克过程作为参考过程来估计薛定谔势，引入风险函数（耦合之间的KL散度）。

Result: 推导了包括高斯混合在内的薛定谔势类中经验风险最小化器泛化能力的紧密边界，在有利情况下几乎实现了快收敛速率（除了一些对数因子）。

Conclusion: 所提出的方法具有一定的有效性，通过数值实验验证了性能。

Abstract: Modern methods of generative modelling and unpaired image-to-image
translation based on Schr\"odinger bridges and stochastic optimal control
theory aim to transform an initial density to a target one in an optimal way.
In the present paper, we assume that we only have access to i.i.d. samples from
initial and final distributions. This makes our setup suitable for both
generative modelling and unpaired image-to-image translation. Relying on the
stochastic optimal control approach, we choose an Ornstein-Uhlenbeck process as
the reference one and estimate the corresponding Schr\"odinger potential.
Introducing a risk function as the Kullback-Leibler divergence between
couplings, we derive tight bounds on generalization ability of an empirical
risk minimizer in a class of Schr\"odinger potentials including Gaussian
mixtures. Thanks to the mixing properties of the Ornstein-Uhlenbeck process, we
almost achieve fast rates of convergence up to some logarithmic factors in
favourable scenarios. We also illustrate performance of the suggested approach
with numerical experiments.

</details>


### [160] [From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations](https://arxiv.org/abs/2508.08061)
*Sven Weinzierl,Sandra Zilker,Annina Liessmann,Martin Käppel,Weixin Wang,Martin Matzner*

Main category: cs.LG

TL;DR: 提出基于迁移学习的PPM技术，通过实例和实验表明该技术可让缺乏资源的组织实现有效PPM，知识可跨组织迁移。


<details>
  <summary>Details</summary>
Motivation: 现有PPM技术需要大量事件数据或相关资源，部分组织难以利用，因此提出新的技术。

Method: 提出基于迁移学习的PPM技术，在两个实际用例中实例化，并使用IT服务管理流程的事件日志进行数值实验。

Result: 实验结果表明一个业务流程的知识可转移到相同或不同组织的类似业务流程，实现目标环境下的有效PPM。

Conclusion: 所提出的技术可让组织在内部和跨组织环境中从迁移学习中受益，资源可跨组织边界转移。

Abstract: Event logs reflect the behavior of business processes that are mapped in
organizational information systems. Predictive process monitoring (PPM)
transforms these data into value by creating process-related predictions that
provide the insights required for proactive interventions at process runtime.
Existing PPM techniques require sufficient amounts of event data or other
relevant resources that might not be readily available, preventing some
organizations from utilizing PPM. The transfer learning-based PPM technique
presented in this paper allows organizations without suitable event data or
other relevant resources to implement PPM for effective decision support. The
technique is instantiated in two real-life use cases, based on which numerical
experiments are performed using event logs for IT service management processes
in an intra- and inter-organizational setting. The results of the experiments
suggest that knowledge of one business process can be transferred to a similar
business process in the same or a different organization to enable effective
PPM in the target context. With the proposed technique, organizations can
benefit from transfer learning in an intra- and inter-organizational setting,
where resources like pre-trained models are transferred within and across
organizational boundaries.

</details>


### [161] [Privacy-Preserving Tabular Synthetic Data Generation Using TabularARGN](https://arxiv.org/abs/2508.06647)
*Andrey Sidorenko,Paul Tiwald*

Main category: cs.LG

TL;DR: 介绍TabularARGN用于生成高质量合成表格数据，评估显示其有竞争力且隐私-效用平衡好


<details>
  <summary>Details</summary>
Motivation: 传统匿名技术难以充分保护隐私，需有效合成数据生成方法

Method: 提出TabularARGN，采用基于离散化的自回归方法

Result: 与现有方法对比，在统计相似性、机器学习效用和检测鲁棒性上有竞争力，隐私评估显示其鲁棒且有良好隐私-效用平衡

Conclusion: TabularARGN是一种有效生成合成表格数据的方法，能实现较好隐私-效用平衡

Abstract: Synthetic data generation has become essential for securely sharing and
analyzing sensitive data sets. Traditional anonymization techniques, however,
often fail to adequately preserve privacy. We introduce the Tabular
Auto-Regressive Generative Network (TabularARGN), a neural network architecture
specifically designed for generating high-quality synthetic tabular data. Using
a discretization-based auto-regressive approach, TabularARGN achieves high data
fidelity while remaining computationally efficient. We evaluate TabularARGN
against existing synthetic data generation methods, showing competitive results
in statistical similarity, machine learning utility, and detection robustness.
We further perform an in-depth privacy evaluation using systematic
membership-inference attacks, highlighting the robustness and effective
privacy-utility balance of our approach.

</details>


### [162] [MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease Classification](https://arxiv.org/abs/2508.07465)
*Tiantian Yang,Zhiqian Chen*

Main category: cs.LG

TL;DR: 提出MOTGNN框架用于二元疾病分类，在三个真实疾病数据集上表现优于基线模型，有计算效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多组学数据高维性和复杂相互作用给预测建模带来挑战，需新方法进行疾病分类。

Method: 采用XGBoost进行组学特定的监督图构建，使用模态特定的GNN进行分层表示学习，用深度前馈网络进行跨组学集成。

Result: 在三个数据集上，MOTGNN在准确率、ROC - AUC和F1分数上比现有基线模型高5 - 10%，对严重类别不平衡情况有鲁棒性，通过稀疏图保持计算效率并具有内置可解释性。

Conclusion: MOTGNN在多组学疾病建模中可提高预测准确性和可解释性。

Abstract: Integrating multi-omics data, such as DNA methylation, mRNA expression, and
microRNA (miRNA) expression, offers a comprehensive view of the biological
mechanisms underlying disease. However, the high dimensionality and complex
interactions among omics layers present major challenges for predictive
modeling. We propose Multi-Omics integration with Tree-generated Graph Neural
Network (MOTGNN), a novel and interpretable framework for binary disease
classification. MOTGNN employs eXtreme Gradient Boosting (XGBoost) to perform
omics-specific supervised graph construction, followed by modality-specific
Graph Neural Networks (GNNs) for hierarchical representation learning, and a
deep feedforward network for cross-omics integration. On three real-world
disease datasets, MOTGNN outperforms state-of-the-art baselines by 5-10% in
accuracy, ROC-AUC, and F1-score, and remains robust to severe class imbalance
(e.g., 87.2% vs. 33.4% F1 on imbalanced data). The model maintains
computational efficiency through sparse graphs (2.1-2.8 edges per node) and
provides built-in interpretability, revealing both top-ranked biomarkers and
the relative contributions of each omics modality. These results highlight
MOTGNN's potential to improve both predictive accuracy and interpretability in
multi-omics disease modeling.

</details>


### [163] [In-Context Reinforcement Learning via Communicative World Models](https://arxiv.org/abs/2508.06659)
*Fernando Martinez-Lopez,Tao Li,Yingdong Lu,Juntao Chen*

Main category: cs.LG

TL;DR: 为提升强化学习代理的上下文学习能力，提出CORAL框架，实验证明其可提高样本效率和实现零样本适应。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理在不更新参数时难以泛化到新任务和环境，旨在提升其上下文强化学习能力。

Method: 将上下文强化学习表述为双智能体通信问题，引入CORAL框架，信息智能体预训练构建世界模型并生成消息，因果影响损失塑造通信协议，控制智能体利用消息解决任务。

Result: 控制智能体在样本效率上有显著提升，能在未见的稀疏奖励环境中实现零样本适应。

Conclusion: 学习可迁移的通信表示是有效的。

Abstract: Reinforcement learning (RL) agents often struggle to generalize to new tasks
and contexts without updating their parameters, mainly because their learned
representations and policies are overfit to the specifics of their training
environments. To boost agents' in-context RL (ICRL) ability, this work
formulates ICRL as a two-agent emergent communication problem and introduces
CORAL (Communicative Representation for Adaptive RL), a framework that learns a
transferable communicative context by decoupling latent representation learning
from control. In CORAL, an Information Agent (IA) is pre-trained as a world
model on a diverse distribution of tasks. Its objective is not to maximize task
reward, but to build a world model and distill its understanding into concise
messages. The emergent communication protocol is shaped by a novel Causal
Influence Loss, which measures the effect that the message has on the next
action. During deployment, the previously trained IA serves as a fixed
contextualizer for a new Control Agent (CA), which learns to solve tasks by
interpreting the provided communicative context. Our experiments demonstrate
that this approach enables the CA to achieve significant gains in sample
efficiency and successfully perform zero-shot adaptation with the help of
pre-trained IA in entirely unseen sparse-reward environments, validating the
efficacy of learning a transferable communicative representation.

</details>


### [164] [Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets, and Applications](https://arxiv.org/abs/2508.07473)
*Zijian Liu*

Main category: cs.LG

TL;DR: 本文研究在线凸优化（OCO）在重尾梯度估计下的情况，为经典算法建立新的后悔界，无需算法修改，结果具有多种应用。


<details>
  <summary>Details</summary>
Motivation: 现有在线凸优化算法在随机梯度有有限方差时效果好，但在梯度估计为重尾分布时研究有限，故开展此项研究。

Method: 在标准有界域假设下，对经典OCO算法（如在线梯度下降）进行研究，不做算法修改。

Result: 为经典方法建立新的后悔界，这些界在所有参数上是最优的，无需额外操作；得到非光滑非凸优化在重尾噪声下无梯度裁剪的首个可证明收敛结果；拓展到更广泛的设置和乐观算法。

Conclusion: 在线凸优化在重尾情况下可有效解决，无需额外操作。

Abstract: In Online Convex Optimization (OCO), when the stochastic gradient has a
finite variance, many algorithms provably work and guarantee a sublinear
regret. However, limited results are known if the gradient estimate has a heavy
tail, i.e., the stochastic gradient only admits a finite $\mathsf{p}$-th
central moment for some $\mathsf{p}\in\left(1,2\right]$. Motivated by it, this
work examines different old algorithms for OCO (e.g., Online Gradient Descent)
in the more challenging heavy-tailed setting. Under the standard bounded domain
assumption, we establish new regrets for these classical methods without any
algorithmic modification. Remarkably, these regret bounds are fully optimal in
all parameters (can be achieved even without knowing $\mathsf{p}$), suggesting
that OCO with heavy tails can be solved effectively without any extra operation
(e.g., gradient clipping). Our new results have several applications. A
particularly interesting one is the first provable convergence result for
nonsmooth nonconvex optimization under heavy-tailed noise without gradient
clipping. Furthermore, we explore broader settings (e.g., smooth OCO) and
extend our ideas to optimistic algorithms to handle different cases
simultaneously.

</details>


### [165] [Transferring Social Network Knowledge from Multiple GNN Teachers to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.06663)
*Yuan-Hung Chao,Chia-Hsun Lu,Chih-Ya Shen*

Main category: cs.LG

TL;DR: 本文将KANs集成到三种流行GNN架构中得到新模型，并采用多教师知识融合框架，实验表明新模型提升了节点分类准确率，知识融合方法提高了学生模型性能。


<details>
  <summary>Details</summary>
Motivation: GNN依赖图连接性限制了可扩展性和效率，而KANs有强非线性表达能力和高效推理能力，因此想将KANs集成到GNN中提升性能。

Method: 将KANs集成到GAT、SGC和APPNP三种GNN架构中得到KGAT、KSGC和KAPPNP；采用多教师知识融合框架，将多个基于KAN的GNN知识提炼到无图KAN学生模型中。

Result: 提出的模型提高了节点分类准确率，知识融合方法显著提升了学生模型性能。

Conclusion: KANs有增强GNN表达能力和实现高效无图推理的潜力。

Abstract: Graph Neural Networks (GNNs) have shown strong performance on
graph-structured data, but their reliance on graph connectivity often limits
scalability and efficiency. Kolmogorov-Arnold Networks (KANs), a recent
architecture with learnable univariate functions, offer strong nonlinear
expressiveness and efficient inference. In this work, we integrate KANs into
three popular GNN architectures-GAT, SGC, and APPNP-resulting in three new
models: KGAT, KSGC, and KAPPNP. We further adopt a multi-teacher knowledge
amalgamation framework, where knowledge from multiple KAN-based GNNs is
distilled into a graph-independent KAN student model. Experiments on benchmark
datasets show that the proposed models improve node classification accuracy,
and the knowledge amalgamation approach significantly boosts student model
performance. Our findings highlight the potential of KANs for enhancing GNN
expressiveness and for enabling efficient, graph-free inference.

</details>


### [166] [N-BEATS-MOE: N-BEATS with a Mixture-of-Experts Layer for Heterogeneous Time Series Forecasting](https://arxiv.org/abs/2508.07490)
*Ricardo Matos,Luis Roque,Vitor Cerqueira*

Main category: cs.LG

TL;DR: 提出基于Mixture - of - Experts层的N - BEATS扩展模型N - BEATS - MOE，评估显示在多个数据集有改进。


<details>
  <summary>Details</summary>
Motivation: 为了让模型更好适应各时间序列特征，并增加模型可解释性。

Method: 提出N - BEATS - MOE，采用基于门控网络的动态块加权策略。

Result: 在12个基准数据集上评估，在多个数据集尤其是异质时间序列数据集上有持续改进。

Conclusion: N - BEATS - MOE能更好适应时间序列特征，门控机制可能增加可解释性，且在多个数据集表现良好。

Abstract: Deep learning approaches are increasingly relevant for time series
forecasting tasks. Methods such as N-BEATS, which is built on stacks of
multilayer perceptrons (MLPs) blocks, have achieved state-of-the-art results on
benchmark datasets and competitions. N-BEATS is also more interpretable
relative to other deep learning approaches, as it decomposes forecasts into
different time series components, such as trend and seasonality. In this work,
we present N-BEATS-MOE, an extension of N-BEATS based on a Mixture-of-Experts
(MoE) layer. N-BEATS-MOE employs a dynamic block weighting strategy based on a
gating network which allows the model to better adapt to the characteristics of
each time series. We also hypothesize that the gating mechanism provides
additional interpretability by identifying which expert is most relevant for
each series. We evaluate our method across 12 benchmark datasets against
several approaches, achieving consistent improvements on several datasets,
especially those composed of heterogeneous time series.

</details>


### [167] [Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach](https://arxiv.org/abs/2508.07505)
*Yueyang Quan,Chang Wang,Shengjie Zhai,Minghong Fang,Zhuqing Liu*

Main category: cs.LG

TL;DR: 提出用于非凸去中心化最小 - 最大优化的隐私保护算法DPMixSGD，证明噪声不影响收敛并实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 去中心化最小 - 最大优化共享模型更新有隐私风险，传统差分隐私在其中实施有收敛问题。

Method: 基于STORM算法提出DPMixSGD算法，证明添加噪声不影响收敛性能并给出隐私保证理论界限。

Result: 通过多任务和多模型实验验证了方法的有效性。

Conclusion: DPMixSGD算法能有效解决非凸去中心化最小 - 最大优化中的隐私保护和收敛问题。

Abstract: Decentralized min-max optimization allows multi-agent systems to
collaboratively solve global min-max optimization problems by facilitating the
exchange of model updates among neighboring agents, eliminating the need for a
central server. However, sharing model updates in such systems carry a risk of
exposing sensitive data to inference attacks, raising significant privacy
concerns. To mitigate these privacy risks, differential privacy (DP) has become
a widely adopted technique for safeguarding individual data. Despite its
advantages, implementing DP in decentralized min-max optimization poses
challenges, as the added noise can hinder convergence, particularly in
non-convex scenarios with complex agent interactions in min-max optimization
problems. In this work, we propose an algorithm called DPMixSGD (Differential
Private Minmax Hybrid Stochastic Gradient Descent), a novel privacy-preserving
algorithm specifically designed for non-convex decentralized min-max
optimization. Our method builds on the state-of-the-art STORM-based algorithm,
one of the fastest decentralized min-max solutions. We rigorously prove that
the noise added to local gradients does not significantly compromise
convergence performance, and we provide theoretical bounds to ensure privacy
guarantees. To validate our theoretical findings, we conduct extensive
experiments across various tasks and models, demonstrating the effectiveness of
our approach.

</details>


### [168] [Watermarking Kolmogorov-Arnold Networks for Emerging Networked Applications via Activation Perturbation](https://arxiv.org/abs/2508.06676)
*Chia-Hsun Lu,Guan-Jhih Wu,Ya-Chi Ho,Chih-Ya Shen*

Main category: cs.LG

TL;DR: 针对Kolmogorov - Arnold Networks (KAN)提出离散余弦变换激活水印方法DCT - AW，对模型性能影响小且抗攻击能力强。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法不适用于KAN，而KAN在网络结构数据建模有潜力，需新水印方法保护其知识产权。

Method: 提出DCT - AW方法，利用KAN可学习激活函数，通过离散余弦变换扰动激活输出来嵌入水印。

Result: DCT - AW对模型性能影响小，在对抗微调、剪枝和剪枝后再训练等水印去除攻击时表现出优越的鲁棒性。

Conclusion: DCT - AW适用于KAN，能有效保护模型知识产权，具有良好的兼容性和抗攻击能力。

Abstract: With the increasing importance of protecting intellectual property in machine
learning, watermarking techniques have gained significant attention. As
advanced models are increasingly deployed in domains such as social network
analysis, the need for robust model protection becomes even more critical.
While existing watermarking methods have demonstrated effectiveness for
conventional deep neural networks, they often fail to adapt to the novel
architecture, Kolmogorov-Arnold Networks (KAN), which feature learnable
activation functions. KAN holds strong potential for modeling complex
relationships in network-structured data. However, their unique design also
introduces new challenges for watermarking. Therefore, we propose a novel
watermarking method, Discrete Cosine Transform-based Activation Watermarking
(DCT-AW), tailored for KAN. Leveraging the learnable activation functions of
KAN, our method embeds watermarks by perturbing activation outputs using
discrete cosine transform, ensuring compatibility with diverse tasks and
achieving task independence. Experimental results demonstrate that DCT-AW has a
small impact on model performance and provides superior robustness against
various watermark removal attacks, including fine-tuning, pruning, and
retraining after pruning.

</details>


### [169] [Stabilizing Federated Learning under Extreme Heterogeneity with HeteRo-Select](https://arxiv.org/abs/2508.06692)
*Md. Akmol Masud,Md Abrar Jahin,Mahmud Hasan*

Main category: cs.LG

TL;DR: 提出HeteRo - Select框架解决联邦学习训练不稳定问题，理论和实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习因客户端数据多样性导致训练不稳定，现有基于效用的客户端选择方法后期准确率下降。

Method: 提出HeteRo - Select框架，采用考虑客户端有用性、公平性、更新速度和数据多样性的评分系统，有强正则化下的收敛保证。

Result: 在CIFAR - 10数据集显著标签偏斜情况下，HeteRo - Select在峰值准确率、最终准确率和训练稳定性上优于Oort。

Conclusion: HeteRo - Select是解决现实中异构联邦学习问题的可靠方案。

Abstract: Federated Learning (FL) is a machine learning technique that often suffers
from training instability due to the diverse nature of client data. Although
utility-based client selection methods like Oort are used to converge by
prioritizing high-loss clients, they frequently experience significant drops in
accuracy during later stages of training. We propose a theoretical
HeteRo-Select framework designed to maintain high performance and ensure
long-term training stability. We provide a theoretical analysis showing that
when client data is very different (high heterogeneity), choosing a smart
subset of client participation can reduce communication more effectively
compared to full participation. Our HeteRo-Select method uses a clear,
step-by-step scoring system that considers client usefulness, fairness, update
speed, and data variety. It also shows convergence guarantees under strong
regularization. Our experimental results on the CIFAR-10 dataset under
significant label skew ($\alpha=0.1$) support the theoretical findings. The
HeteRo-Select method performs better than existing approaches in terms of peak
accuracy, final accuracy, and training stability. Specifically, HeteRo-Select
achieves a peak accuracy of $74.75\%$, a final accuracy of $72.76\%$, and a
minimal stability drop of $1.99\%$. In contrast, Oort records a lower peak
accuracy of $73.98\%$, a final accuracy of $71.25\%$, and a larger stability
drop of $2.73\%$. The theoretical foundations and empirical performance in our
study make HeteRo-Select a reliable solution for real-world heterogeneous FL
problems.

</details>


### [170] [FairDRL-ST: Disentangled Representation Learning for Fair Spatio-Temporal Mobility Prediction](https://arxiv.org/abs/2508.07518)
*Sichen Zhao,Wei Shao,Jeffrey Chan,Ziqi Xu,Flora Salim*

Main category: cs.LG

TL;DR: 提出FairDRL - ST框架解决时空预测公平性问题，在真实数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有时空方法多关注准确性，而时空应用中存在预测偏差会加剧社会不平等，需解决公平性问题。

Method: 提出基于解纠缠表示学习的FairDRL - ST框架，利用对抗学习和解纠缠表示学习分离含敏感信息的属性，以无监督方式实现公平性。

Result: 应用于真实城市移动数据集，能缩小公平性差距，且预测性能有竞争力。

Conclusion: FairDRL - ST框架可有效解决时空预测中的公平性问题，且性能损失小。

Abstract: As deep spatio-temporal neural networks are increasingly utilised in urban
computing contexts, the deployment of such methods can have a direct impact on
users of critical urban infrastructure, such as public transport, emergency
services, and traffic management systems. While many spatio-temporal methods
focus on improving accuracy, fairness has recently gained attention due to
growing evidence that biased predictions in spatio-temporal applications can
disproportionately disadvantage certain demographic or geographic groups,
thereby reinforcing existing socioeconomic inequalities and undermining the
ethical deployment of AI in public services. In this paper, we propose a novel
framework, FairDRL-ST, based on disentangled representation learning, to
address fairness concerns in spatio-temporal prediction, with a particular
focus on mobility demand forecasting. By leveraging adversarial learning and
disentangled representation learning, our framework learns to separate
attributes that contain sensitive information. Unlike existing methods that
enforce fairness through supervised learning, which may lead to
overcompensation and degraded performance, our framework achieves fairness in
an unsupervised manner with minimal performance loss. We apply our framework to
real-world urban mobility datasets and demonstrate its ability to close
fairness gaps while delivering competitive predictive performance compared to
state-of-the-art fairness-aware methods.

</details>


### [171] [FairFLRep: Fairness aware fault localization and repair of Deep Neural Networks](https://arxiv.org/abs/2508.08151)
*Moses Openja,Paolo Arcaini,Foutse Khomh,Fuyuki Ishikawa*

Main category: cs.LG

TL;DR: 论文介绍FairFLRep技术识别和纠正DNN分类器中潜在导致偏差的神经元，评估显示其在提升公平性同时保持准确性，且比基线方法更高效。


<details>
  <summary>Details</summary>
Motivation: DNN系统会反映和放大数据中的偏差，导致有偏差行为和不准确决策，有效识别和纠正其偏差行为是挑战。

Method: 引入FairFLRep技术，调整与敏感属性相关的神经元权重，分析网络输入 - 输出关系来纠正导致预测质量差异的神经元。

Result: 在多个数据集和模型上评估，FairFLRep在提升公平性同时保持准确性，优于现有方法；消融研究证实考虑公平性的重要性；比基线方法修复网络更高效。

Conclusion: FairFLRep能有效识别和纠正DNN中的偏差，在提升公平性和效率上表现出色。

Abstract: Deep neural networks (DNNs) are being utilized in various aspects of our
daily lives, including high-stakes decision-making applications that impact
individuals. However, these systems reflect and amplify bias from the data used
during training and testing, potentially resulting in biased behavior and
inaccurate decisions. For instance, having different misclassification rates
between white and black sub-populations. However, effectively and efficiently
identifying and correcting biased behavior in DNNs is a challenge. This paper
introduces FairFLRep, an automated fairness-aware fault localization and repair
technique that identifies and corrects potentially bias-inducing neurons in DNN
classifiers. FairFLRep focuses on adjusting neuron weights associated with
sensitive attributes, such as race or gender, that contribute to unfair
decisions. By analyzing the input-output relationships within the network,
FairFLRep corrects neurons responsible for disparities in predictive quality
parity. We evaluate FairFLRep on four image classification datasets using two
DNN classifiers, and four tabular datasets with a DNN model. The results show
that FairFLRep consistently outperforms existing methods in improving fairness
while preserving accuracy. An ablation study confirms the importance of
considering fairness during both fault localization and repair stages. Our
findings also show that FairFLRep is more efficient than the baseline
approaches in repairing the network.

</details>


### [172] [CISO: Species Distribution Modeling Conditioned on Incomplete Species Observations](https://arxiv.org/abs/2508.06704)
*Hager Radi Abdelwahed,Mélisande Teng,Robin Zbinden,Laura Pollock,Hugo Larochelle,Devis Tuia,David Rolnick*

Main category: cs.LG

TL;DR: 传统物种分布模型（SDMs）常忽略生物相互作用，本文提出基于深度学习的CISO方法，利用多数据集验证，结果显示其能结合不完整生物信息提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统SDMs常忽略生物相互作用，现有解决方法存在假设局限且依赖完整共现数据，而实际物种观测稀疏、信息不一。

Method: 提出基于深度学习的CISO方法，可结合可变数量物种观测和环境变量进行预测。

Result: 纳入部分生物信息可提升空间分离测试集的预测性能；以子物种为条件时，CISO优于其他方法；结合多数据集观测可提升性能。

Conclusion: CISO是有前景的生态工具，能结合不完整生物信息，识别不同分类群物种间潜在相互作用。

Abstract: Species distribution models (SDMs) are widely used to predict species'
geographic distributions, serving as critical tools for ecological research and
conservation planning. Typically, SDMs relate species occurrences to
environmental variables representing abiotic factors, such as temperature,
precipitation, and soil properties. However, species distributions are also
strongly influenced by biotic interactions with other species, which are often
overlooked. While some methods partially address this limitation by
incorporating biotic interactions, they often assume symmetrical pairwise
relationships between species and require consistent co-occurrence data. In
practice, species observations are sparse, and the availability of information
about the presence or absence of other species varies significantly across
locations. To address these challenges, we propose CISO, a deep learning-based
method for species distribution modeling Conditioned on Incomplete Species
Observations. CISO enables predictions to be conditioned on a flexible number
of species observations alongside environmental variables, accommodating the
variability and incompleteness of available biotic data. We demonstrate our
approach using three datasets representing different species groups: sPlotOpen
for plants, SatBird for birds, and a new dataset, SatButterfly, for
butterflies. Our results show that including partial biotic information
improves predictive performance on spatially separate test sets. When
conditioned on a subset of species within the same dataset, CISO outperforms
alternative methods in predicting the distribution of the remaining species.
Furthermore, we show that combining observations from multiple datasets can
improve performance. CISO is a promising ecological tool, capable of
incorporating incomplete biotic information and identifying potential
interactions between species from disparate taxa.

</details>


### [173] [Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning](https://arxiv.org/abs/2508.07556)
*Stephan Rabanser*

Main category: cs.LG

TL;DR: 论文研究用不确定性估计提升机器学习安全性和可信度，提出基于训练轨迹的选择性预测方法，分析误差源，还应对了对抗操纵问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统在高风险领域需提升安全性和可信度，研究不确定性估计以改进选择性预测。

Method: 利用模型训练轨迹的不确定性信号，集成中间检查点预测；对选择性分类差距进行有限样本分解；结合校准审计和可验证推理设计防御机制。

Result: 提出轻量级事后弃权方法，在差分隐私下表现稳健；明确选择性分类差距的五个可解释误差源；发现不确定性信号会被对抗操纵并设计了防御措施。

Conclusion: 研究在改进、评估和保障不确定性估计方面推动了可靠机器学习的发展，使模型能准确预测并知道何时应弃权。

Abstract: Machine learning (ML) systems are increasingly deployed in high-stakes
domains where reliability is paramount. This thesis investigates how
uncertainty estimation can enhance the safety and trustworthiness of ML,
focusing on selective prediction -- where models abstain when confidence is
low.
  We first show that a model's training trajectory contains rich uncertainty
signals that can be exploited without altering its architecture or loss. By
ensembling predictions from intermediate checkpoints, we propose a lightweight,
post-hoc abstention method that works across tasks, avoids the cost of deep
ensembles, and achieves state-of-the-art selective prediction performance.
Crucially, this approach is fully compatible with differential privacy (DP),
allowing us to study how privacy noise affects uncertainty quality. We find
that while many methods degrade under DP, our trajectory-based approach remains
robust, and we introduce a framework for isolating the privacy-uncertainty
trade-off. Next, we then develop a finite-sample decomposition of the selective
classification gap -- the deviation from the oracle accuracy-coverage curve --
identifying five interpretable error sources and clarifying which interventions
can close the gap. This explains why calibration alone cannot fix ranking
errors, motivating methods that improve uncertainty ordering. Finally, we show
that uncertainty signals can be adversarially manipulated to hide errors or
deny service while maintaining high accuracy, and we design defenses combining
calibration audits with verifiable inference.
  Together, these contributions advance reliable ML by improving, evaluating,
and safeguarding uncertainty estimation, enabling models that not only make
accurate predictions -- but also know when to say "I do not know".

</details>


### [174] [Analysis of Schedule-Free Nonconvex Optimization](https://arxiv.org/abs/2508.06743)
*Connor Brown*

Main category: cs.LG

TL;DR: 提出鲁棒李雅普诺夫框架分析无调度（SF）方法，得出非凸环境下与迭代次数无关的收敛界，并用PEP实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有一阶方法收敛依赖提前未知的总迭代次数T，SF方法在非凸分析上有限或依赖强全局假设。

Method: 引入鲁棒李雅普诺夫框架，将SF分析简化为单步下降不等式，结合PEP实验。

Result: 得到非凸环境下与迭代次数无关的收敛界，如常数步长+PR平均为O(1/log T)等，PEP实验验证了收敛速率。

Conclusion: 将SF的无迭代次数依赖保证扩展到平滑非凸优化，为最优非凸速率指明方向。

Abstract: First-order methods underpin most large-scale learning algorithms, yet their
classical convergence guarantees hinge on carefully scheduled step-sizes that
depend on the total horizon $T$, which is rarely known in advance. The
Schedule-Free (SF) method promises optimal performance with hyperparameters
that are independent of $T$ by interpolating between Polyak--Ruppert averaging
and momentum, but nonconvex analysis of SF has been limited or reliant on
strong global assumptions. We introduce a robust Lyapunov framework that, under
only $L$-smoothness and lower-boundedness, reduces SF analysis to a single-step
descent inequality. This yields horizon-agnostic bounds in the nonconvex
setting: $O(1/\log T)$ for constant step + PR averaging, $O(\log T/T)$ for a
linearly growing step-size, and a continuum of $O(T^{-(1-\alpha)})$ rates for
polynomial averaging. We complement these proofs with Performance Estimation
Problem (PEP) experiments that numerically validate our rates and suggest that
our $O(1/\log T)$ bound on the original nonconvex SF algorithm may tighten to
$O(1/T)$. Our work extends SF's horizon-free guarantees to smooth nonconvex
optimization and charts future directions for optimal nonconvex rates.

</details>


### [175] [Detecting Mislabeled and Corrupted Data via Pointwise Mutual Information](https://arxiv.org/abs/2508.07713)
*Jinghan Yang,Jiayu Weng*

Main category: cs.LG

TL;DR: 本文提出基于互信息的数据选择框架，在MNIST上验证能有效过滤低质量样本，提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络会记忆错误标签，现实数据集常受标签和输入噪声影响，数据质量对模型性能至关重要。

Method: 提出基于互信息的框架，计算每个样本对整体互信息的逐点贡献，低贡献表示有噪声或错误标签的实例。

Result: 在不同合成噪声设置的MNIST上验证，该方法有效过滤低质量样本，在标签损坏时，用高互信息样本训练比随机采样分类准确率最多提高15%，且对良性输入修改有鲁棒性。

Conclusion: 所提方法能在混合噪声场景下有效进行数据选择，提升模型性能。

Abstract: Deep neural networks can memorize corrupted labels, making data quality
critical for model performance, yet real-world datasets are frequently
compromised by both label noise and input noise. This paper proposes a mutual
information-based framework for data selection under hybrid noise scenarios
that quantifies statistical dependencies between inputs and labels. We compute
each sample's pointwise contribution to the overall mutual information and find
that lower contributions indicate noisy or mislabeled instances. Empirical
validation on MNIST with different synthetic noise settings demonstrates that
the method effectively filters low-quality samples. Under label corruption,
training on high-MI samples improves classification accuracy by up to 15\%
compared to random sampling. Furthermore, the method exhibits robustness to
benign input modifications, preserving semantically valid data while filtering
truly corrupted samples.

</details>


### [176] [Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning](https://arxiv.org/abs/2508.06765)
*Xingke Yang,Liang Li,Sicong Li,Liwei Guan,Hao Wang,Xiaoqi Qi,Jiang Liu,Xin Fu,Miao Pan*

Main category: cs.LG

TL;DR: 本文提出Fed MobiLLM用于跨异构移动设备的大语言模型联合微调，采用服务器辅助的联邦侧调范式，有自适应特征对齐方法，实验显示其性能优、开销低。


<details>
  <summary>Details</summary>
Motivation: 传统联邦大语言模型微调方法在移动硬件上计算和内存负担大，同步模型聚合协议受慢设备影响，需新方法解决异构移动设备上的高效微调问题。

Method: 提出Fed MobiLLM，实现服务器辅助的联邦侧调范式，移动设备用冻结预缩放主干LLM进行前向传播，上传中间激活，服务器训练共享侧网络；引入自适应层特征对齐方法。

Result: 实验表明，与现有方法相比，Fed MobiLLM能保持稳健微调性能，设备内存极低，计算开销至少降低95.2%，通信成本降低93.2%，收敛速度快5.1倍。

Conclusion: Fed MobiLLM对异构移动设备上的实际大语言模型适配有效。

Abstract: Collaboratively fine-tuning (FT) large language models (LLMs) over
heterogeneous mobile devices fosters immense potential applications of
personalized intelligence. However, such a vision faces critical system
challenges. Conventional federated LLM FT approaches place prohibitive
computational and memory burdens on mobile hardware, and their synchronous
model aggregation protocols stall for slower devices. In this paper, we propose
Fed MobiLLM, a novel design to facilitate efficient federated LLM FT across
mobile devices with diverse computing/communication speeds and local model
architectures. In particular, Fed MobiLLM implements a pioneering
server-assisted federated side-tuning paradigm. Briefly, mobile devices perform
lightweight forward propagation computations on local data using their frozen
pre-scaled backbone LLMs, and then upload selected intermediate activations.
The server trains a shared side-network independently, eliminating client-side
backpropagation and enabling asynchronous updates. To bridge model
heterogeneity across different devices, we introduce an adaptive layer-wise
feature alignment method, which ensures consistent representations for
collaboratively tuning a shared side network. Extensive experimental results
demonstrate that Fed MobiLLM can maintain robust fine-tuning performance while
achieving extremely low on-device memory, with at least 95.2% reduction in
computation overhead, 93.2% reduction in communication costs and 5.1x faster
convergence compared to existing methods, validating its efficacy for practical
LLM adaptation over heterogeneous mobile devices.

</details>


### [177] [A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory](https://arxiv.org/abs/2508.07746)
*Fengdi Che*

Main category: cs.LG

TL;DR: 本文是关于离线强化学习的综述，探讨理论见解对算法设计的影响。


<details>
  <summary>Details</summary>
Motivation: 弥合离线强化学习理论见解与实际算法设计之间的差距。

Method: 先列出理论证明所需条件，再分析反例，最后讨论离线强化学习的充分条件。

Result: 明确了理论证明条件、算法局限性及应对挑战的技术。

Conclusion: 这些条件不仅用于理论证明，还提醒在条件不满足时寻找新解决方案。

Abstract: Offline reinforcement learning (RL) aims to optimize the return given a fixed
dataset of agent trajectories without additional interactions with the
environment. While algorithm development has progressed rapidly, significant
theoretical advances have also been made in understanding the fundamental
challenges of offline RL. However, bridging these theoretical insights with
practical algorithm design remains an ongoing challenge. In this survey, we
explore key intuitions derived from theoretical work and their implications for
offline RL algorithms.
  We begin by listing the conditions needed for the proofs, including function
representation and data coverage assumptions. Function representation
conditions tell us what to expect for generalization, and data coverage
assumptions describe the quality requirement of the data. We then examine
counterexamples, where offline RL is not solvable without an impractically
large amount of data. These cases highlight what cannot be achieved for all
algorithms and the inherent hardness of offline RL. Building on techniques to
mitigate these challenges, we discuss the conditions that are sufficient for
offline RL. These conditions are not merely assumptions for theoretical proofs,
but they also reveal the limitations of these algorithms and remind us to
search for novel solutions when the conditions cannot be satisfied.

</details>


### [178] [Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent](https://arxiv.org/abs/2508.08222)
*Tong Yang,Yu Huang,Yingbin Liang,Yuejie Chi*

Main category: cs.LG

TL;DR: 研究Transformer如何通过思维链过程解决符号多步推理问题，用理论分析证明单层Transformer可解决任务并有泛化性，解释推理能力机制。


<details>
  <summary>Details</summary>
Motivation: 当前对Transformer通过训练获得多步推理能力的底层机制，尤其是理论层面理解有限，需深入研究。

Method: 聚焦树中的路径查找，分析反向推理和更复杂的正向推理两个相互关联的任务，基于梯度下降动力学进行理论分析。

Result: 训练后的单层Transformer能解决两个任务并对未见树有泛化性，多阶段训练动态阐明不同注意力头如何自主专门化和协调以解决子任务。

Conclusion: 解释了训练后的Transformer实现顺序算法程序的机制，表明结构化任务使浅多头Transformer也能解决需更深架构的问题。

Abstract: Transformers have demonstrated remarkable capabilities in multi-step
reasoning tasks. However, understandings of the underlying mechanisms by which
they acquire these abilities through training remain limited, particularly from
a theoretical standpoint. This work investigates how transformers learn to
solve symbolic multi-step reasoning problems through chain-of-thought
processes, focusing on path-finding in trees. We analyze two intertwined tasks:
a backward reasoning task, where the model outputs a path from a goal node to
the root, and a more complex forward reasoning task, where the model implements
two-stage reasoning by first identifying the goal-to-root path and then
reversing it to produce the root-to-goal path. Our theoretical analysis,
grounded in the dynamics of gradient descent, shows that trained one-layer
transformers can provably solve both tasks with generalization guarantees to
unseen trees. In particular, our multi-phase training dynamics for forward
reasoning elucidate how different attention heads learn to specialize and
coordinate autonomously to solve the two subtasks in a single autoregressive
path. These results provide a mechanistic explanation of how trained
transformers can implement sequential algorithmic procedures. Moreover, they
offer insights into the emergence of reasoning abilities, suggesting that when
tasks are structured to take intermediate chain-of-thought steps, even shallow
multi-head transformers can effectively solve problems that would otherwise
require deeper architectures.

</details>


### [179] [TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations](https://arxiv.org/abs/2508.07016)
*Jianfei Wu,Wenmian Yang,Bingning Liu,Weijia Jia*

Main category: cs.LG

TL;DR: 本文提出TLCCSP框架用于时间序列预测，在多个数据集上验证其有效性，降低了均方误差并减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型常忽略相关序列间的时滞互相关性，而这对捕捉复杂时间关系至关重要，需要改进预测准确性。

Method: 提出TLCCSP框架，采用SSDTW算法捕捉滞后相关性，使用基于对比学习的编码器近似SSDTW距离。

Result: 在天气、金融和房地产数据集上，SSDTW和对比学习编码器均降低了均方误差，且对比学习方法减少约99%的计算时间。

Conclusion: TLCCSP框架有效，能提高预测准确性，具备可扩展性和实时适用性。

Abstract: Time series forecasting is critical across various domains, such as weather,
finance and real estate forecasting, as accurate forecasts support informed
decision-making and risk mitigation. While recent deep learning models have
improved predictive capabilities, they often overlook time-lagged
cross-correlations between related sequences, which are crucial for capturing
complex temporal relationships. To address this, we propose the Time-Lagged
Cross-Correlations-based Sequence Prediction framework (TLCCSP), which enhances
forecasting accuracy by effectively integrating time-lagged cross-correlated
sequences. TLCCSP employs the Sequence Shifted Dynamic Time Warping (SSDTW)
algorithm to capture lagged correlations and a contrastive learning-based
encoder to efficiently approximate SSDTW distances.
  Experimental results on weather, finance and real estate time series datasets
demonstrate the effectiveness of our framework. On the weather dataset, SSDTW
reduces mean squared error (MSE) by 16.01% compared with single-sequence
methods, while the contrastive learning encoder (CLE) further decreases MSE by
17.88%. On the stock dataset, SSDTW achieves a 9.95% MSE reduction, and CLE
reduces it by 6.13%. For the real estate dataset, SSDTW and CLE reduce MSE by
21.29% and 8.62%, respectively. Additionally, the contrastive learning approach
decreases SSDTW computational time by approximately 99%, ensuring scalability
and real-time applicability across multiple time series forecasting tasks.

</details>


### [180] [PROPS: Progressively Private Self-alignment of Large Language Models](https://arxiv.org/abs/2508.06783)
*Noel Teku,Fengwei Tian,Payel Bhattacharjee,Souradip Chakraborty,Amrit Singh Bedi,Ravi Tandon*

Main category: cs.LG

TL;DR: 本文聚焦大语言模型对齐中的偏好级隐私问题，提出PROPS框架，经多模型和数据集验证，相比现有方法在保证高隐私同时有更好效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型对齐依赖人类反馈存在隐私问题，现有方法可能提供过度隐私并降低模型效用。

Method: 提出PROPS多阶段隐私保护对齐框架，前阶段的私有对齐模型可为后续阶段补充训练数据。

Result: 在相同隐私预算下，PROPS对齐的胜率比DP - SGD高3倍，比基于随机响应（RR）的对齐高2.5倍。

Conclusion: PROPS框架在保证高隐私的同时，相比现有方法有更好的效用。

Abstract: Alignment is a key step in developing Large Language Models (LLMs) using
human feedback to ensure adherence to human values and societal norms.
Dependence on human feedback raises privacy concerns about how much a labeler's
preferences may reveal about their personal values, beliefs, and personality
traits. Existing approaches, such as Differentially Private SGD (DP-SGD),
provide rigorous privacy guarantees by privatizing gradients during fine-tuning
and alignment but can provide more privacy than necessary as human preferences
are tied only to labels of (prompt, response) pairs and can degrade model
utility. This work focuses on LLM alignment with preference-level privacy,
which preserves the privacy of preference labels provided by humans. We propose
PROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving
alignment framework where privately aligned models in previous stages can serve
as labelers for supplementing training data in the subsequent stages of
alignment. We present theoretical guarantees for PROPS as well as comprehensive
validation using multiple models (Pythia and GPT) and datasets (AlpacaEval,
Anthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over
existing methods while still providing high privacy. For the same privacy
budget, alignment via PROPS can achieve up to 3x higher win-rates compared to
DP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based
alignment.

</details>


### [181] [Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning](https://arxiv.org/abs/2508.06784)
*Junjing Zheng,Chengliang Song,Weidong Jiang,Xinyu Zhang*

Main category: cs.LG

TL;DR: 提出Mode - Aware Non - linear Tucker Autoencoder (MA - NTAE)处理高维张量自监督学习问题，实验显示其在压缩和聚类任务上有优势。


<details>
  <summary>Details</summary>
Motivation: 现有MLP - based autoencoders处理高维张量有维数灾难问题，现有张量网络学习非线性关系能力有限。

Method: 将经典Tucker分解推广到非线性框架，采用Pick - and - Unfold策略，通过递归展开 - 编码 - 折叠操作对高阶张量进行灵活的逐模式编码。

Result: MA - NTAE计算复杂度随张量阶数线性增长，随模式维度成比例增长，在压缩和聚类任务上优于标准AE和当前张量网络。

Conclusion: MA - NTAE能有效处理高维张量自监督学习问题，尤其在高阶、高维张量上优势明显。

Abstract: High-dimensional data, particularly in the form of high-order tensors,
presents a major challenge in self-supervised learning. While MLP-based
autoencoders (AE) are commonly employed, their dependence on flattening
operations exacerbates the curse of dimensionality, leading to excessively
large model sizes, high computational overhead, and challenging optimization
for deep structural feature capture. Although existing tensor networks
alleviate computational burdens through tensor decomposition techniques, most
exhibit limited capability in learning non-linear relationships. To overcome
these limitations, we introduce the Mode-Aware Non-linear Tucker Autoencoder
(MA-NTAE). MA-NTAE generalized classical Tucker decomposition to a non-linear
framework and employs a Pick-and-Unfold strategy, facilitating flexible
per-mode encoding of high-order tensors via recursive unfold-encode-fold
operations, effectively integrating tensor structural priors. Notably, MA-NTAE
exhibits linear growth in computational complexity with tensor order and
proportional growth with mode dimensions. Extensive experiments demonstrate
MA-NTAE's performance advantages over standard AE and current tensor networks
in compression and clustering tasks, which become increasingly pronounced for
higher-order, higher-dimensional tensors.

</details>


### [182] [Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities](https://arxiv.org/abs/2508.06800)
*Rui Liu,Haolin Zuo,Zheng Lian,Hongyu Yuan,Qi Fan*

Main category: cs.LG

TL;DR: 提出HARDY - MER框架解决多模态情感识别中缺失模态问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统缺失模态重建方法未考虑样本重建难度差异，难以有效处理难样本。

Method: 提出HARDY - MER框架，分两步：一是用多视图硬度评估机制量化样本难度；二是采用基于检索的动态课程学习策略调整训练课程。

Result: 在基准数据集上的实验表明HARDY - MER在缺失模态场景下始终优于现有方法。

Conclusion: HARDY - MER能有效解决多模态情感识别中的缺失模态问题，代码将公开。

Abstract: Missing modalities have recently emerged as a critical research direction in
multimodal emotion recognition (MER). Conventional approaches typically address
this issue through missing modality reconstruction. However, these methods fail
to account for variations in reconstruction difficulty across different
samples, consequently limiting the model's ability to handle hard samples
effectively. To overcome this limitation, we propose a novel Hardness-Aware
Dynamic Curriculum Learning framework, termed HARDY-MER. Our framework operates
in two key stages: first, it estimates the hardness level of each sample, and
second, it strategically emphasizes hard samples during training to enhance
model performance on these challenging instances. Specifically, we first
introduce a Multi-view Hardness Evaluation mechanism that quantifies
reconstruction difficulty by considering both Direct Hardness (modality
reconstruction errors) and Indirect Hardness (cross-modal mutual information).
Meanwhile, we introduce a Retrieval-based Dynamic Curriculum Learning strategy
that dynamically adjusts the training curriculum by retrieving samples with
similar semantic information and balancing the learning focus between easy and
hard instances. Extensive experiments on benchmark datasets demonstrate that
HARDY-MER consistently outperforms existing methods in missing-modality
scenarios. Our code will be made publicly available at
https://github.com/HARDY-MER/HARDY-MER.

</details>


### [183] [Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation](https://arxiv.org/abs/2508.06806)
*Xiao Huang,Xu Liu,Enze Zhang,Tong Yu,Shuai Li*

Main category: cs.LG

TL;DR: 提出CFDG数据增强方法用于离线到在线强化学习，实验表明该方法优于其他方法，能与现有算法集成，在D4RL基准测试中平均提升15%性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线到在线强化学习的数据增强方法生成数据与在线数据有差距，限制整体性能。

Method: 提出Classifier-Free Diffusion Generation (CFDG)方法，利用无分类器引导扩散提升不同分布数据生成质量，采用重加权方法使生成数据更符合在线数据分布。

Result: CFDG在实验中优于回放两种数据或使用标准扩散模型生成新数据的方法，集成到流行方法中在D4RL基准测试平均提升15%性能。

Conclusion: CFDG方法有效且通用，可与现有离线到在线RL算法集成。

Abstract: Offline-to-online Reinforcement Learning (O2O RL) aims to perform online
fine-tuning on an offline pre-trained policy to minimize costly online
interactions. Existing work used offline datasets to generate data that conform
to the online data distribution for data augmentation. However, generated data
still exhibits a gap with the online data, limiting overall performance. To
address this, we propose a new data augmentation approach, Classifier-Free
Diffusion Generation (CFDG). Without introducing additional classifier training
overhead, CFDG leverages classifier-free guidance diffusion to significantly
enhance the generation quality of offline and online data with different
distributions. Additionally, it employs a reweighting method to enable more
generated data to align with the online data, enhancing performance while
maintaining the agent's stability. Experimental results show that CFDG
outperforms replaying the two data types or using a standard diffusion model to
generate new data. Our method is versatile and can be integrated with existing
offline-to-online RL algorithms. By implementing CFDG to popular methods IQL,
PEX and APL, we achieve a notable 15% average improvement in empirical
performance on the D4RL benchmark such as MuJoCo and AntMaze.

</details>


### [184] [Technical Report: Full-Stack Fine-Tuning for the Q Programming Language](https://arxiv.org/abs/2508.06813)
*Brendan R. Hogan,Will Brown,Adel Boyarsky,Anderson Schneider,Yuriy Nevmyvaka*

Main category: cs.LG

TL;DR: 本文提出适应Q编程语言的开源方法，训练系列模型，最佳模型表现超前沿模型，还提供详细方案及推广讨论。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在互联网数据少的任务上表现不佳，Q语言在互联网数据少，通用AI模型不擅长，需适配。

Method: 引入Q语言评估数据集，在数据集上对前沿模型做基准测试，基于Qwen - 2.5系列进行预训练、监督微调、强化学习训练推理和非推理模型。

Result: 最佳模型在Q基准测试中pass@1准确率达59%，超Claude Opus - 4 29.5%，所有模型超GPT - 4.1。

Conclusion: 提供了模型、代码、数据及详细方案，方法可广泛应用于其他任务。

Abstract: Even though large language models are becoming increasingly capable, it is
still unreasonable to expect them to excel at tasks that are under-represented
on the Internet. Leveraging LLMs for specialized applications, particularly in
niche programming languages and private domains, remains challenging and
largely unsolved. In this work, we address this gap by presenting a
comprehensive, open-source approach for adapting LLMs to the Q programming
language, a popular tool in quantitative finance that is much less present on
the Internet compared to Python, C, Java, and other ``mainstream" languages and
is therefore not a strong suit of general-purpose AI models. We introduce a new
Leetcode style evaluation dataset for Q, benchmark major frontier models on the
dataset, then do pretraining, supervised fine tuning, and reinforcement
learning to train a suite of reasoning and non-reasoning models based on the
Qwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our
best model achieves a pass@1 accuracy of 59 percent on our Q benchmark,
surpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.
Additionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.
In addition to releasing models, code, and data, we provide a detailed
blueprint for dataset construction, model pretraining, supervised fine-tuning,
and reinforcement learning. Our methodology is broadly applicable, and we
discuss how these techniques can be extended to other tasks, including those
where evaluation may rely on soft or subjective signals.

</details>


### [185] [Who's the Evil Twin? Differential Auditing for Undesired Behavior](https://arxiv.org/abs/2508.06827)
*Ishwar Balappanawar,Venkata Hasith Vattikuti,Greta Kintzley,Ronan Azimi-Mancel,Satvik Golechha*

Main category: cs.LG

TL;DR: 本文将神经网络隐藏行为检测构建为红蓝对抗游戏，用CNN实验多种检测策略，对抗攻击法准确率高，LLM审计需提示信息，还开源审计游戏。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络隐藏行为检测因先验知识少和对抗混淆带来的挑战。

Method: 将检测构建为红蓝对抗游戏，红队训练正常和含有害行为模型，蓝队在信息有限下识别；用CNN实验，尝试高斯噪声分析、模型差异比较等策略；在LLM轮次探索适用方法。

Result: 对抗攻击法准确率达100%，其他策略效果不一；LLM审计需关于不良分布的提示信息。

Conclusion: 研究结果有助于设计更好的审计，开源审计游戏期望推动相关工作。

Abstract: Detecting hidden behaviors in neural networks poses a significant challenge
due to minimal prior knowledge and potential adversarial obfuscation. We
explore this problem by framing detection as an adversarial game between two
teams: the red team trains two similar models, one trained solely on benign
data and the other trained on data containing hidden harmful behavior, with the
performance of both being nearly indistinguishable on the benign dataset. The
blue team, with limited to no information about the harmful behaviour, tries to
identify the compromised model. We experiment using CNNs and try various blue
team strategies, including Gaussian noise analysis, model diffing, integrated
gradients, and adversarial attacks under different levels of hints provided by
the red team. Results show high accuracy for adversarial-attack-based methods
(100\% correct prediction, using hints), which is very promising, whilst the
other techniques yield more varied performance. During our LLM-focused rounds,
we find that there are not many parallel methods that we could apply from our
study with CNNs. Instead, we find that effective LLM auditing methods require
some hints about the undesired distribution, which can then used in standard
black-box and open-weight methods to probe the models further and reveal their
misalignment. We open-source our auditing games (with the model and data) and
hope that our findings contribute to designing better audits.

</details>


### [186] [Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning](https://arxiv.org/abs/2508.06871)
*Aleksandar Todorov,Juan Cardenas-Cartagena,Rafael F. Cunha,Marco Zullich,Matthia Sabatelli*

Main category: cs.LG

TL;DR: 研究稀疏化方法（GMP和SET）在多任务强化学习（MTRL）中增强可塑性、提升性能的效果，发现其能缓解可塑性退化，提升多任务表现。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习中训练时可塑性损失的问题，在MTRL中更高的表征灵活性很关键。

Method: 系统探索GMP和SET两种稀疏化方法，在不同MTRL架构上进行评估，并与密集基线及多种替代方法对比。

Result: GMP和SET能有效缓解可塑性退化的关键指标，提升多任务性能，稀疏代理常优于密集代理。

Conclusion: 揭示了可塑性、网络稀疏性和MTRL设计的相互作用，动态稀疏化是开发更具适应性MTRL系统的有效但需考虑上下文的工具。

Abstract: Plasticity loss, a diminishing capacity to adapt as training progresses, is a
critical challenge in deep reinforcement learning. We examine this issue in
multi-task reinforcement learning (MTRL), where higher representational
flexibility is crucial for managing diverse and potentially conflicting task
demands. We systematically explore how sparsification methods, particularly
Gradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET), enhance
plasticity and consequently improve performance in MTRL agents. We evaluate
these approaches across distinct MTRL architectures (shared backbone, Mixture
of Experts, Mixture of Orthogonal Experts) on standardized MTRL benchmarks,
comparing against dense baselines, and a comprehensive range of alternative
plasticity-inducing or regularization methods. Our results demonstrate that
both GMP and SET effectively mitigate key indicators of plasticity degradation,
such as neuron dormancy and representational collapse. These plasticity
improvements often correlate with enhanced multi-task performance, with sparse
agents frequently outperforming dense counterparts and achieving competitive
results against explicit plasticity interventions. Our findings offer insights
into the interplay between plasticity, network sparsity, and MTRL designs,
highlighting dynamic sparsification as a robust but context-sensitive tool for
developing more adaptable MTRL systems.

</details>


### [187] [Conformal Prediction and Trustworthy AI](https://arxiv.org/abs/2508.06885)
*Anthony Bellotti,Xindi Zhao*

Main category: cs.LG

TL;DR: 本文回顾共形预测对可信AI的贡献，还给出实验和示例。


<details>
  <summary>Details</summary>
Motivation: 共形预测在机器学习社区流行且有助于开发可信AI，研究其在边际有效性之外对可信AI的贡献。

Method: 回顾共形预测的相关内容，给出实验和示例。

Result: 展示了共形预测可作为校准良好的预测器，用于偏差识别和缓解。

Conclusion: 共形预测在边际有效性之外能为可信AI做贡献，可解决泛化风险和AI治理等问题。

Abstract: Conformal predictors are machine learning algorithms developed in the 1990's
by Gammerman, Vovk, and their research team, to provide set predictions with
guaranteed confidence level. Over recent years, they have grown in popularity
and have become a mainstream methodology for uncertainty quantification in the
machine learning community. From its beginning, there was an understanding that
they enable reliable machine learning with well-calibrated uncertainty
quantification. This makes them extremely beneficial for developing trustworthy
AI, a topic that has also risen in interest over the past few years, in both
the AI community and society more widely. In this article, we review the
potential for conformal prediction to contribute to trustworthy AI beyond its
marginal validity property, addressing problems such as generalization risk and
AI governance. Experiments and examples are also provided to demonstrate its
use as a well-calibrated predictor and for bias identification and mitigation.

</details>


### [188] [QuiZSF: An efficient data-model interaction framework for zero-shot time-series forecasting](https://arxiv.org/abs/2508.06915)
*Shichao Ma,Zhengyang Zhou,Qihe Huang,Binwu Wang,Kuo Yang,Huan Li,Yang Wang*

Main category: cs.LG

TL;DR: 引入RAG到时间序列预训练模型，提出QuiZSF框架进行零样本时间序列预测，表现优于基线模型且效率高。


<details>
  <summary>Details</summary>
Motivation: 传统模型难处理零样本时间序列预测，时间序列预训练模型缺乏动态整合外部知识机制，检索增强生成与预训练模型结合少。

Method: 提出QuiZSF框架，构建ChronoRAG Base用于存储和检索，引入Multi - grained Series Interaction Learner提取特征，开发Model Cooperation Coherer对齐知识与两类预训练模型。

Result: 以非大语言模型和大语言模型为基础模型时，QuiZSF分别在75%和87.5%的预测设置中排名第一，且内存和推理时间效率高。

Conclusion: 引入RAG的QuiZSF框架能有效提升零样本时间序列预测性能。

Abstract: Time series forecasting has become increasingly important to empower diverse
applications with streaming data. Zero-shot time-series forecasting (ZSF),
particularly valuable in data-scarce scenarios, such as domain transfer or
forecasting under extreme conditions, is difficult for traditional models to
deal with. While time series pre-trained models (TSPMs) have demonstrated
strong performance in ZSF, they often lack mechanisms to dynamically
incorporate external knowledge. Fortunately, emerging retrieval-augmented
generation (RAG) offers a promising path for injecting such knowledge on
demand, yet they are rarely integrated with TSPMs. To leverage the strengths of
both worlds, we introduce RAG into TSPMs to enhance zero-shot time series
forecasting. In this paper, we propose QuiZSF (Quick Zero-Shot Time Series
Forecaster), a lightweight and modular framework that couples efficient
retrieval with representation learning and model adaptation for ZSF.
Specifically, we construct a hierarchical tree-structured ChronoRAG Base (CRB)
for scalable time-series storage and domain-aware retrieval, introduce a
Multi-grained Series Interaction Learner (MSIL) to extract fine- and
coarse-grained relational features, and develop a dual-branch Model Cooperation
Coherer (MCC) that aligns retrieved knowledge with two kinds of TSPMs: Non-LLM
based and LLM based. Compared with contemporary baselines, QuiZSF, with Non-LLM
based and LLM based TSPMs as base model, respectively, ranks Top1 in 75% and
87.5% of prediction settings, while maintaining high efficiency in memory and
inference time.

</details>


### [189] [Class Unbiasing for Generalization in Medical Diagnosis](https://arxiv.org/abs/2508.06943)
*Lishi Zuo,Man-Wai Mak,Lu Yi,Youzhi Tu*

Main category: cs.LG

TL;DR: 文章识别了类别特征偏差，提出训练无类别偏差模型（Cls - unbias），通过合成和真实数据集验证方法能缓解偏差和类别不平衡，提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医疗诊断可能因偏差失败，识别出类别特征偏差会导致模型性能有偏差和泛化能力差，目标是训练同时缓解类别不平衡和类别特征偏差的无类别偏差模型。

Method: 提出类内不等式损失以促进正负样本分类损失的平等贡献，优化类内组分布鲁棒优化目标以在类别不平衡时增强不等式损失的有效性。

Result: 通过合成和真实数据集，实证表明类别特征偏差会对模型性能产生负面影响，所提方法能有效缓解类别特征偏差和类别不平衡。

Conclusion: 所提方法能有效缓解类别特征偏差和类别不平衡，提高模型的泛化能力。

Abstract: Medical diagnosis might fail due to bias. In this work, we identified
class-feature bias, which refers to models' potential reliance on features that
are strongly correlated with only a subset of classes, leading to biased
performance and poor generalization on other classes. We aim to train a
class-unbiased model (Cls-unbias) that mitigates both class imbalance and
class-feature bias simultaneously. Specifically, we propose a class-wise
inequality loss which promotes equal contributions of classification loss from
positive-class and negative-class samples. We propose to optimize a class-wise
group distributionally robust optimization objective-a class-weighted training
objective that upweights underperforming classes-to enhance the effectiveness
of the inequality loss under class imbalance. Through synthetic and real-world
datasets, we empirically demonstrate that class-feature bias can negatively
impact model performance. Our proposed method effectively mitigates both
class-feature bias and class imbalance, thereby improving the model's
generalization ability.

</details>


### [190] [AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance](https://arxiv.org/abs/2508.06944)
*Lixuan He,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: 本文提出AMFT算法统一SFT和RL，在多基准测试中达SOTA，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型微调方法存在灾难性遗忘、难以平衡模仿与探索，单阶段方法缺乏平衡机制。

Method: 提出Adaptive Meta Fine - Tuning (AMFT)算法，用元梯度自适应权重控制器学习SFT和RL奖励信号的最优平衡。

Result: 在数学推理、抽象视觉推理和视觉语言导航等基准测试中达SOTA，在分布外任务泛化性好。

Conclusion: 元学习控制器对AMFT稳定性、样本效率和性能至关重要，为大语言模型对齐提供更有效的范式。

Abstract: Large Language Models (LLMs) are typically fine-tuned for reasoning tasks
through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by
Reinforcement Learning (RL), a process fraught with catastrophic forgetting and
suboptimal trade-offs between imitation and exploration. Recent single-stage
methods attempt to unify SFT and RL using heuristics, but lack a principled
mechanism for dynamically balancing the two paradigms. In this paper, we
reframe this challenge through the theoretical lens of \textbf{implicit
rewards}, viewing SFT and RL not as distinct methods but as complementary
reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel
single-stage algorithm that learns the optimal balance between SFT's implicit,
path-level reward and RL's explicit, outcome-based reward. The core of AMFT is
a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL
balance as a learnable parameter, dynamically optimizing it to maximize
long-term task performance. This forward-looking approach, regularized by
policy entropy for stability, autonomously discovers an effective training
curriculum. We conduct a comprehensive evaluation on challenging benchmarks
spanning mathematical reasoning, abstract visual reasoning (General Points),
and vision-language navigation (V-IRL). AMFT consistently establishes a new
state-of-the-art and demonstrats superior generalization on out-of-distribution
(OOD) tasks. Ablation studies and training dynamic analysis confirm that the
meta-learning controller is crucial for AMFT's stability, sample efficiency,
and performance, offering a more principled and effective paradigm for LLM
alignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.

</details>


### [191] [BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity](https://arxiv.org/abs/2508.06953)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Ziqiang Cui,Dugang Liu,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.LG

TL;DR: 本文提出块多样化低秩自适应（BoRA）方法，以少量额外参数提高LoRA权重的秩，实验证明其优越性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA方法增加权重秩会显著增加可训练参数数量，需要一种以少量额外参数提高秩的方法。

Method: 将BA视为块矩阵乘法，对A和B分块，为每个块乘法引入对角矩阵Σ_{i,j}，形成B_i Σ_{i,j} A_j。

Result: 通过多数据集和模型的大量实验，证明了BoRA的优越性，消融研究验证了其可扩展性。

Conclusion: BoRA能以少量额外参数提高LoRA权重的秩，具有良好性能和可扩展性。

Abstract: Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). It approximates the update of a
pretrained weight matrix $W\in\mathbb{R}^{m\times n}$ by the product of two
low-rank matrices, $BA$, where $A \in\mathbb{R}^{r\times n}$ and
$B\in\mathbb{R}^{m\times r} (r\ll\min\{m,n\})$. Increasing the dimension $r$
can raise the rank of LoRA weights (i.e., $BA$), which typically improves
fine-tuning performance but also significantly increases the number of
trainable parameters. In this paper, we propose Block Diversified Low-Rank
Adaptation (BoRA), which improves the rank of LoRA weights with a small number
of additional parameters. Specifically, BoRA treats the product $BA$ as a block
matrix multiplication, where $A$ and $B$ are partitioned into $b$ blocks along
the columns and rows, respectively (i.e., $A=[A_1,\dots,A_b]$ and
$B=[B_1,\dots,B_b]^\top$). Consequently, the product $BA$ becomes the
concatenation of the block products $B_iA_j$ for $i,j\in[b]$. To enhance the
diversity of different block products, BoRA introduces a unique diagonal matrix
$\Sigma_{i,j} \in \mathbb{R}^{r\times r}$ for each block multiplication,
resulting in $B_i \Sigma_{i,j} A_j$. By leveraging these block-wise diagonal
matrices, BoRA increases the rank of LoRA weights by a factor of $b$ while only
requiring $b^2r$ additional parameters. Extensive experiments across multiple
datasets and models demonstrate the superiority of BoRA, and ablation studies
further validate its scalability.

</details>


### [192] [Can Multitask Learning Enhance Model Explainability?](https://arxiv.org/abs/2508.06966)
*Hiba Najjar,Bushra Alshbib,Andreas Dengel*

Main category: cs.LG

TL;DR: 本文探索通过多任务学习利用多模态卫星数据解释模型行为，展示该方法在数据稀缺等方面的优势，并在三个数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态学习网络虽能提升模型性能，但复杂度影响其可解释性，因此探索利用多模态数据进行多任务学习以解释模型行为。

Method: 使用某些模态作为额外目标与主任务一起预测，卫星数据作为输入模态。

Result: 在数据稀缺时无需额外收集模态数据用于模型推理；模型性能与多模态基线相当，部分情况更好；可通过辅助任务解释主任务预测误差。在三个数据集上验证了方法的有效性。

Conclusion: 提出的基于多任务学习利用多模态数据的方法能有效解释模型行为，具有一定优势。

Abstract: Remote sensing provides satellite data in diverse types and formats. The
usage of multimodal learning networks exploits this diversity to improve model
performance, except that the complexity of such networks comes at the expense
of their interpretability. In this study, we explore how modalities can be
leveraged through multitask learning to intrinsically explain model behavior.
In particular, instead of additional inputs, we use certain modalities as
additional targets to be predicted along with the main task. The success of
this approach relies on the rich information content of satellite data, which
remains as input modalities. We show how this modeling context provides
numerous benefits: (1) in case of data scarcity, the additional modalities do
not need to be collected for model inference at deployment, (2) the model
performance remains comparable to the multimodal baseline performance, and in
some cases achieves better scores, (3) prediction errors in the main task can
be explained via the model behavior in the auxiliary task(s). We demonstrate
the efficiency of our approach on three datasets, including segmentation,
classification, and regression tasks. Code available at
git.opendfki.de/hiba.najjar/mtl_explainability/.

</details>


### [193] [Structure-Preserving Digital Twins via Conditional Neural Whitney Forms](https://arxiv.org/abs/2508.06981)
*Brooks Kinch,Benjamin Shaffer,Elizabeth Armstrong,Michael Meehan,John Hewson,Nathaniel Trask*

Main category: cs.LG

TL;DR: 提出基于潜在变量Z的保结构降阶有限元模型构建实时数字孪生框架，方法准确高效且有开源实现。


<details>
  <summary>Details</summary>
Motivation: 构建能保证数值适定性、精确守恒量且支持实时校准的实时数字孪生框架。

Method: 采用条件注意力机制，在有限元外微分计算框架中学习降阶有限元基和非线性守恒律。

Result: 在复杂几何上用稀疏数据准确预测，实时推理约0.1秒，比大涡模拟加速3.1x10^8。

Conclusion: 该框架可用于复杂问题，与传统有限元结合，性能好且有开源代码。

Abstract: We present a framework for constructing real-time digital twins based on
structure-preserving reduced finite element models conditioned on a latent
variable Z. The approach uses conditional attention mechanisms to learn both a
reduced finite element basis and a nonlinear conservation law within the
framework of finite element exterior calculus (FEEC). This guarantees numerical
well-posedness and exact preservation of conserved quantities, regardless of
data sparsity or optimization error. The conditioning mechanism supports
real-time calibration to parametric variables, allowing the construction of
digital twins which support closed loop inference and calibration to sensor
data. The framework interfaces with conventional finite element machinery in a
non-invasive manner, allowing treatment of complex geometries and integration
of learned models with conventional finite element techniques.
  Benchmarks include advection diffusion, shock hydrodynamics, electrostatics,
and a complex battery thermal runaway problem. The method achieves accurate
predictions on complex geometries with sparse data (25 LES simulations),
including capturing the transition to turbulence and achieving real-time
inference ~0.1s with a speedup of 3.1x10^8 relative to LES. An open-source
implementation is available on GitHub.

</details>


### [194] [UniMove: A Unified Model for Multi-city Human Mobility Prediction](https://arxiv.org/abs/2508.06986)
*Chonghua Han,Yuan Yuan,Yukun Liu,Jingtao Ding,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: 提出统一模型UniMove进行多城市人类移动性预测，采用双塔架构和MoE Transformer块，实验证明可提升预测准确率，向基础模型迈进。


<details>
  <summary>Details</summary>
Motivation: 人类移动性预测有重要意义，但因移动特性和城市异质性建模困难，现有方案需为每个城市单独训练模型。

Method: 提出轨迹 - 位置双塔架构，设计MoE Transformer块自适应选择专家处理多样移动模式。

Result: 在多城市多数据集上实验，通过多城市数据联合训练和相互增强，显著提升移动性预测准确率超10.2%。

Conclusion: UniMove是实现统一架构人类移动性基础模型的关键进步。

Abstract: Human mobility prediction is vital for urban planning, transportation
optimization, and personalized services. However, the inherent randomness,
non-uniform time intervals, and complex patterns of human mobility, compounded
by the heterogeneity introduced by varying city structures, infrastructure, and
population densities, present significant challenges in modeling. Existing
solutions often require training separate models for each city due to distinct
spatial representations and geographic coverage. In this paper, we propose
UniMove, a unified model for multi-city human mobility prediction, addressing
two challenges: (1) constructing universal spatial representations for
effective token sharing across cities, and (2) modeling heterogeneous mobility
patterns from varying city characteristics. We propose a trajectory-location
dual-tower architecture, with a location tower for universal spatial encoding
and a trajectory tower for sequential mobility modeling. We also design MoE
Transformer blocks to adaptively select experts to handle diverse movement
patterns. Extensive experiments across multiple datasets from diverse cities
demonstrate that UniMove truly embodies the essence of a unified model. By
enabling joint training on multi-city data with mutual data enhancement, it
significantly improves mobility prediction accuracy by over 10.2\%. UniMove
represents a key advancement toward realizing a true foundational model with a
unified architecture for human mobility. We release the implementation at
https://github.com/tsinghua-fib-lab/UniMove/.

</details>


### [195] [A Comparative Study of Feature Selection in Tsetlin Machines](https://arxiv.org/abs/2508.06991)
*Vojtech Halenka,Ole-Christoffer Granmo,Lei Jiao,Per-Arne Andersen*

Main category: cs.LG

TL;DR: 本文评估多种特征选择（FS）技术用于Tsetlin机（TM），通过12个数据集基准测试表明TM内部评分器表现好且成本低，为TM的FS建立首个综合基线。


<details>
  <summary>Details</summary>
Motivation: TM缺乏评估特征重要性的既定工具，而特征选择对提升模型性能很重要，因此要为TM适配和评估FS技术。

Method: 适配并评估一系列FS技术，包括经典过滤和嵌入式方法、神经网络的事后解释方法以及基于TM子句权重和Tsetlin自动机状态的新型嵌入式评分器，使用Remove and Retrain（ROAR）和Remove and Debias（ROAD）等评估协议。

Result: TM内部评分器表现有竞争力，能揭示特征交互模式，简单的TM特定评分器以较低计算成本实现相似的准确率保留。

Conclusion: 为TM的FS建立首个综合基线，为开发TM特定的可解释性技术铺平道路。

Abstract: Feature Selection (FS) is crucial for improving model interpretability,
reducing complexity, and sometimes for enhancing accuracy. The recently
introduced Tsetlin machine (TM) offers interpretable clause-based learning, but
lacks established tools for estimating feature importance. In this paper, we
adapt and evaluate a range of FS techniques for TMs, including classical filter
and embedded methods as well as post-hoc explanation methods originally
developed for neural networks (e.g., SHAP and LIME) and a novel family of
embedded scorers derived from TM clause weights and Tsetlin automaton (TA)
states. We benchmark all methods across 12 datasets, using evaluation
protocols, like Remove and Retrain (ROAR) strategy and Remove and Debias
(ROAD), to assess causal impact. Our results show that TM-internal scorers not
only perform competitively but also exploit the interpretability of clauses to
reveal interacting feature patterns. Simpler TM-specific scorers achieve
similar accuracy retention at a fraction of the computational cost. This study
establishes the first comprehensive baseline for FS in TM and paves the way for
developing specialized TM-specific interpretability techniques.

</details>


### [196] [Conformal Set-based Human-AI Complementarity with Multiple Experts](https://arxiv.org/abs/2508.06997)
*Helbert Paat,Guohao Shen*

Main category: cs.LG

TL;DR: 研究聚焦从多位专家中选择特定实例的专家，引入贪心算法利用共形集选专家子集，在多专家分类上表现优于朴素方法，模拟显示算法接近最优。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦单专家场景，本文要探索多专家如何从共形集中获益及实例特定专家的选择问题。

Method: 引入贪心算法，利用共形集识别用于实例分类的专家预测子集。

Result: 基于真实专家预测的模拟研究表明，提出的贪心算法能得到接近最优的子集。

Conclusion: 该贪心算法可提升多专家分类性能，优于朴素的人类子集选择方法。

Abstract: Decision support systems are designed to assist human experts in
classification tasks by providing conformal prediction sets derived from a
pre-trained model. This human-AI collaboration has demonstrated enhanced
classification performance compared to using either the model or the expert
independently. In this study, we focus on the selection of instance-specific
experts from a pool of multiple human experts, contrasting it with existing
research that typically focuses on single-expert scenarios. We characterize the
conditions under which multiple experts can benefit from the conformal sets.
With the insight that only certain experts may be relevant for each instance,
we explore the problem of subset selection and introduce a greedy algorithm
that utilizes conformal sets to identify the subset of expert predictions that
will be used in classifying an instance. This approach is shown to yield better
performance compared to naive methods for human subset selection. Based on real
expert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation
study indicates that our proposed greedy algorithm achieves near-optimal
subsets, resulting in improved classification performance among multiple
experts.

</details>


### [197] [From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving](https://arxiv.org/abs/2508.07029)
*Antonio Guillen-Perez*

Main category: cs.LG

TL;DR: 本文提出综合流程和对比研究，对比基于行为克隆（BC）和离线强化学习（CQL）的自动驾驶策略，结果显示CQL策略更稳健。


<details>
  <summary>Details</summary>
Motivation: 从大规模真实世界数据集中学习稳健驾驶策略是自动驾驶的核心挑战，传统BC方法训练的策略脆弱且存在闭环执行误差，需改进。

Method: 先开发一系列BC基线模型，最终采用基于Transformer的模型；再对相同数据和架构应用离线强化学习算法CQL，并设计奖励函数。

Result: 在Waymo开放运动数据集的1000个未见场景评估中，CQL智能体成功率比最强BC基线高3.2倍，碰撞率低7.4倍。

Conclusion: 离线强化学习方法对于从静态专家数据中学习稳健、长视野驾驶策略至关重要。

Abstract: Learning robust driving policies from large-scale, real-world datasets is a
central challenge in autonomous driving, as online data collection is often
unsafe and impractical. While Behavioral Cloning (BC) offers a straightforward
approach to imitation learning, policies trained with BC are notoriously
brittle and suffer from compounding errors in closed-loop execution. This work
presents a comprehensive pipeline and a comparative study to address this
limitation. We first develop a series of increasingly sophisticated BC
baselines, culminating in a Transformer-based model that operates on a
structured, entity-centric state representation. While this model achieves low
imitation loss, we show that it still fails in long-horizon simulations. We
then demonstrate that by applying a state-of-the-art Offline Reinforcement
Learning algorithm, Conservative Q-Learning (CQL), to the same data and
architecture, we can learn a significantly more robust policy. Using a
carefully engineered reward function, the CQL agent learns a conservative value
function that enables it to recover from minor errors and avoid
out-of-distribution states. In a large-scale evaluation on 1,000 unseen
scenarios from the Waymo Open Motion Dataset, our final CQL agent achieves a
3.2x higher success rate and a 7.4x lower collision rate than the strongest BC
baseline, proving that an offline RL approach is critical for learning robust,
long-horizon driving policies from static expert data.

</details>


### [198] [A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling](https://arxiv.org/abs/2508.07032)
*Tiantian He,Keyue Jiang,An Zhao,Anna Schroder,Elinor Thompson,Sonja Soskic,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.LG

TL;DR: 提出阶段感知的IGND - MoE模型解决神经退行性疾病进展建模难题，模型动态整合组件，阶段权重给出新临床见解。


<details>
  <summary>Details</summary>
Motivation: 传统模型在神经退行性疾病进展建模中，因纵向数据稀缺和病理机制复杂而存在局限，需新模型解决。

Method: 提出阶段感知的MoE框架，数据上用迭代双优化法估计观测时间位置；模型上用IGND增强空间组件，引入局部神经反应模块。

Result: IGND - MoE模型动态整合组件，阶段权重显示早期图相关过程影响大，后期其他未知物理过程占主导。

Conclusion: IGND - MoE模型提供了理解阶段特定病理机制对疾病进展贡献的方法，阶段权重临床见解与文献相符。

Abstract: The long-term progression of neurodegenerative diseases is commonly
conceptualized as a spatiotemporal diffusion process that consists of a graph
diffusion process across the structural brain connectome and a localized
reaction process within brain regions. However, modeling this progression
remains challenging due to 1) the scarcity of longitudinal data obtained
through irregular and infrequent subject visits and 2) the complex interplay of
pathological mechanisms across brain regions and disease stages, where
traditional models assume fixed mechanisms throughout disease progression. To
address these limitations, we propose a novel stage-aware Mixture of Experts
(MoE) framework that explicitly models how different contributing mechanisms
dominate at different disease stages through time-dependent expert
weighting.Data-wise, we utilize an iterative dual optimization method to
properly estimate the temporal position of individual observations,
constructing a co hort-level progression trajectory from irregular snapshots.
Model-wise, we enhance the spatial component with an inhomogeneous graph neural
diffusion model (IGND) that allows diffusivity to vary based on node states and
time, providing more flexible representations of brain networks. We also
introduce a localized neural reaction module to capture complex dynamics beyond
standard processes.The resulting IGND-MoE model dynamically integrates these
components across temporal states, offering a principled way to understand how
stage-specific pathological mechanisms contribute to progression. The
stage-wise weights yield novel clinical insights that align with literature,
suggesting that graph-related processes are more influential at early stages,
while other unknown physical processes become dominant later on.

</details>


### [199] [Differentiable Adaptive Kalman Filtering via Optimal Transport](https://arxiv.org/abs/2508.07037)
*Yangguang He,Wenhao Li,Minzhe Li,Juan Zhang,Xiangfeng Wang,Bo Jin*

Main category: cs.LG

TL;DR: 提出OTAKNet解决学习型自适应卡尔曼滤波中噪声统计漂移问题，在合成和真实数据集上展示性能。


<details>
  <summary>Details</summary>
Motivation: 现实环境因素会导致学习型方法在处理非线性动态系统时因噪声统计漂移性能下降。

Method: 通过一步预测测量似然建立状态估计与漂移的联系，用最优传输解决，实现无真实标签或再训练的全在线自适应。

Result: 将OTAKNet与经典基于模型的自适应卡尔曼滤波和离线学习型滤波比较，在合成和真实的NCLT数据集上展示了性能，尤其是在有限训练数据下。

Conclusion: OTAKNet是学习型自适应卡尔曼滤波中处理噪声统计漂移的有效在线解决方案。

Abstract: Learning-based filtering has demonstrated strong performance in non-linear
dynamical systems, particularly when the statistics of noise are unknown.
However, in real-world deployments, environmental factors, such as changing
wind conditions or electromagnetic interference, can induce unobserved
noise-statistics drift, leading to substantial degradation of learning-based
methods. To address this challenge, we propose OTAKNet, the first online
solution to noise-statistics drift within learning-based adaptive Kalman
filtering. Unlike existing learning-based methods that perform offline
fine-tuning using batch pointwise matching over entire trajectories, OTAKNet
establishes a connection between the state estimate and the drift via one-step
predictive measurement likelihood, and addresses it using optimal transport.
This leverages OT's geometry - aware cost and stable gradients to enable fully
online adaptation without ground truth labels or retraining. We compare OTAKNet
against classical model-based adaptive Kalman filtering and offline
learning-based filtering. The performance is demonstrated on both synthetic and
real-world NCLT datasets, particularly under limited training data.

</details>


### [200] [Membership and Memorization in LLM Knowledge Distillation](https://arxiv.org/abs/2508.07054)
*Ziqi Zhang,Ali Shahin Shamsabadi,Hanxiao Lu,Yifeng Cai,Hamed Haddadi*

Main category: cs.LG

TL;DR: 研究系统分析六种大语言模型知识蒸馏技术的隐私风险，发现均存在风险且程度不同，还分析了关键组件影响及块级隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏技术学生模型可能继承教师模型隐私问题，需研究大语言模型知识蒸馏技术的隐私风险。

Method: 使用涵盖七个NLP任务的指令调优设置，结合三种教师模型家族和不同大小的学生模型进行研究。

Result: 所有现有大语言模型知识蒸馏方法都存在从教师到学生的成员资格和记忆隐私风险，风险程度因技术而异，记忆和成员资格隐私风险存在显著差异，不同块的隐私风险差异大。

Conclusion: 大语言模型知识蒸馏技术存在隐私风险，关键组件会影响风险程度，不同块的隐私风险不同。

Abstract: Recent advances in Knowledge Distillation (KD) aim to mitigate the high
computational demands of Large Language Models (LLMs) by transferring knowledge
from a large ''teacher'' to a smaller ''student'' model. However, students may
inherit the teacher's privacy when the teacher is trained on private data. In
this work, we systematically characterize and investigate membership and
memorization privacy risks inherent in six LLM KD techniques. Using
instruction-tuning settings that span seven NLP tasks, together with three
teacher model families (GPT-2, LLAMA-2, and OPT), and various size student
models, we demonstrate that all existing LLM KD approaches carry membership and
memorization privacy risks from the teacher to its students. However, the
extent of privacy risks varies across different KD techniques. We
systematically analyse how key LLM KD components (KD objective functions,
student training data and NLP tasks) impact such privacy risks. We also
demonstrate a significant disagreement between memorization and membership
privacy risks of LLM KD techniques. Finally, we characterize per-block privacy
risk and demonstrate that the privacy risk varies across different blocks by a
large margin.

</details>


### [201] [Surgical Knowledge Rewrite in Compact LLMs: An 'Unlearn-then-Learn' Strategy with ($IA^3$) for Localized Factual Modulation and Catastrophic Forgetting Mitigation](https://arxiv.org/abs/2508.07075)
*Stanley Ngugi*

Main category: cs.LG

TL;DR: 本文提出“先遗忘后学习”策略用于大语言模型知识编辑，在microsoft/Phi - 3 - mini - 4k - instruct上实验效果好，提升模型知识管理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在动态知识更新时存在采用新事实困难和严重灾难性遗忘的问题。

Method: 引入“先遗忘后学习”策略，利用参数高效微调技术$IA^3$，通过初始电路定位阶段识别编码冲突事实的内部组件。

Result: 在新调制事实准确性达98.50%，遗忘原冲突事实率96.00%，F_control准确率72.00%，缓解了灾难性遗忘。有“软遗忘”机制。

Conclusion: 该策略是紧凑型大语言模型精确、局部和安全知识管理的重要进展。

Abstract: Large Language Models (LLMs) struggle with dynamic knowledge updates,
especially when new information conflicts with deeply embedded facts. Such
conflicting factual edits often lead to two critical issues: resistance to
adopting the new fact and severe catastrophic forgetting of unrelated
knowledge. This paper introduces and evaluates a novel "unlearn-then-learn"
strategy for precise knowledge editing in LLMs, leveraging the
parameter-efficient fine-tuning (PEFT) technique, Infused Adapter by Inhibiting
and Amplifying Inner Activations ($IA^3$). Crucially, this two-stage approach
is powered by an initial circuit localization phase that identifies and targets
the specific internal components responsible for encoding the conflicting fact.
Through a rigorous experimental methodology on
microsoft/Phi-3-mini-4k-instruct, we demonstrate that this mechanistically
informed two-stage approach achieves near-perfect accuracy (98.50%) for the
new, modulated fact while simultaneously effectively suppressing the original
conflicting fact (96.00% forget rate). Critically, our strategy exhibits
unprecedented localization (72.00% F_control accuracy), dramatically mitigating
catastrophic forgetting observed in direct fine-tuning approaches (which showed
as low as ~20% F_control accuracy), a direct benefit of our targeted
interpretability-guided intervention. Furthermore, qualitative analysis reveals
a nuanced mechanism of "soft forgetting," where original knowledge is
suppressed from default retrieval but remains latent and conditionally
accessible, enhancing model safety and control. These findings represent a
significant advancement towards precise, localized, and safe knowledge
management in compact LLMs.

</details>


### [202] [Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework](https://arxiv.org/abs/2508.07085)
*N Harshit,K Mounvik*

Main category: cs.LG

TL;DR: 本文提出由Transformers和Autoencoders组成的混合框架用于在线漂移检测，创建Trust Score方法，在含合成漂移的航空乘客数据集上表现优于基线方法，开发出可靠监测概念漂移的框架。


<details>
  <summary>Details</summary>
Motivation: 应用机器学习中概念漂移会降低模型性能，典型检测方法反应式且对早期检测不敏感。

Method: 提出由Transformers和Autoencoders组成的混合框架，创建Trust Score方法，包含统计和基于重建的漂移指标、预测不确定性、规则违反和分类器误差趋势等信号。

Result: 在含合成漂移的航空乘客数据集上，Transformation - Autoencoder比文献中常用的自编码器更早、更灵敏地检测到漂移，在更多错误率和逻辑违规方面有更好的建模。

Conclusion: 开发出了可靠监测概念漂移的强大框架。

Abstract: In applied machine learning, concept drift, which is either gradual or abrupt
changes in data distribution, can significantly reduce model performance.
Typical detection methods,such as statistical tests or reconstruction-based
models,are generally reactive and not very sensitive to early detection. Our
study proposes a hybrid framework consisting of Transformers and Autoencoders
to model complex temporal dynamics and provide online drift detection. We
create a distinct Trust Score methodology, which includes signals on (1)
statistical and reconstruction-based drift metrics, more specifically, PSI,
JSD, Transformer-AE error, (2) prediction uncertainty, (3) rules violations,
and (4) trend of classifier error aligned with the combined metrics defined by
the Trust Score. Using a time sequenced airline passenger data set with
synthetic drift, our proposed model allows for a better detection of drift
using as a whole and at different detection thresholds for both sensitivity and
interpretability compared to baseline methods and provides a strong pipeline
for drift detection in real time for applied machine learning. We evaluated
performance using a time-sequenced airline passenger dataset having the
gradually injected stimulus of drift in expectations,e.g. permuted ticket
prices in later batches, broken into 10 time segments [1].In the data, our
results support that the Transformation-Autoencoder detected drift earlier and
with more sensitivity than the autoencoders commonly used in the literature,
and provided improved modeling over more error rates and logical violations.
Therefore, a robust framework was developed to reliably monitor concept drift.

</details>


### [203] [Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria](https://arxiv.org/abs/2508.07102)
*Yang Cao,Yubin Chen,Zhao Song,Jiahao Zhang*

Main category: cs.LG

TL;DR: 本文对二阶MeanFlow进行理论研究，证明其可行性、刻画表达能力并推导高效实现标准，为高阶流匹配模型奠定理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有生成建模在无模拟范式如Flow Matching和MeanFlow框架取得进展，引入二阶MeanFlow以拓展MeanFlow目标，结合丰富动力学和高效采样。

Method: 证明平均加速度满足广义一致性条件；通过电路复杂度分析刻画表达能力；利用快速近似注意力计算推导可扩展实现标准。

Result: 证明二阶MeanFlow支持稳定单步采样和易处理损失函数；表明在温和假设下采样过程可由特定阈值电路实现；证明注意力操作可在特定时间内以特定误差近似。

Conclusion: 研究为结合丰富动力学和实际采样效率的高阶流匹配模型奠定理论基础。

Abstract: Generative modelling has seen significant advances through simulation-free
paradigms such as Flow Matching, and in particular, the MeanFlow framework,
which replaces instantaneous velocity fields with average velocities to enable
efficient single-step sampling. In this work, we introduce a theoretical study
on Second-Order MeanFlow, a novel extension that incorporates average
acceleration fields into the MeanFlow objective. We first establish the
feasibility of our approach by proving that the average acceleration satisfies
a generalized consistency condition analogous to first-order MeanFlow, thereby
supporting stable, one-step sampling and tractable loss functions. We then
characterize its expressivity via circuit complexity analysis, showing that
under mild assumptions, the Second-Order MeanFlow sampling process can be
implemented by uniform threshold circuits within the $\mathsf{TC}^0$ class.
Finally, we derive provably efficient criteria for scalable implementation by
leveraging fast approximate attention computations: we prove that attention
operations within the Second-Order MeanFlow architecture can be approximated to
within $1/\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results
lay the theoretical foundation for high-order flow matching models that combine
rich dynamics with practical sampling efficiency.

</details>


### [204] [BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation](https://arxiv.org/abs/2508.07106)
*Yiran Huang,Amirhossein Nouranizadeh,Christine Ahrends,Mengjia Xu*

Main category: cs.LG

TL;DR: 提出BrainATCL框架用于自适应学习动态脑连接，在功能链接预测和年龄估计任务表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络难以捕捉动态fMRI数据的长时依赖，而功能性连接动力学与行为和神经精神疾病相关，需要更好的建模方法。

Method: 提出BrainATCL框架，根据新边添加率动态调整回溯窗口，用GINE - Mamba2骨干编码图序列，结合脑结构和功能信息的边属性。

Result: 在功能链接预测和年龄估计任务中表现优越，有很强的泛化能力，包括跨会话预测场景。

Conclusion: BrainATCL框架能有效学习动态脑连接，在相关任务中表现良好，具有应用价值。

Abstract: Functional Magnetic Resonance Imaging (fMRI) is an imaging technique widely
used to study human brain activity. fMRI signals in areas across the brain
transiently synchronise and desynchronise their activity in a highly structured
manner, even when an individual is at rest. These functional connectivity
dynamics may be related to behaviour and neuropsychiatric disease. To model
these dynamics, temporal brain connectivity representations are essential, as
they reflect evolving interactions between brain regions and provide insight
into transient neural states and network reconfigurations. However,
conventional graph neural networks (GNNs) often struggle to capture long-range
temporal dependencies in dynamic fMRI data. To address this challenge, we
propose BrainATCL, an unsupervised, nonparametric framework for adaptive
temporal brain connectivity learning, enabling functional link prediction and
age estimation. Our method dynamically adjusts the lookback window for each
snapshot based on the rate of newly added edges. Graph sequences are
subsequently encoded using a GINE-Mamba2 backbone to learn spatial-temporal
representations of dynamic functional connectivity in resting-state fMRI data
of 1,000 participants from the Human Connectome Project. To further improve
spatial modeling, we incorporate brain structure and function-informed edge
attributes, i.e., the left/right hemispheric identity and subnetwork membership
of brain regions, enabling the model to capture biologically meaningful
topological patterns. We evaluate our BrainATCL on two tasks: functional link
prediction and age estimation. The experimental results demonstrate superior
performance and strong generalization, including in cross-session prediction
scenarios.

</details>


### [205] [Approaching Maximal Information Extraction in Low-Signal Regimes via Multiple Instance Learning](https://arxiv.org/abs/2508.07114)
*Atakan Azakli,Bernd Stelzer*

Main category: cs.LG

TL;DR: 提出新机器学习方法用于假设检验问题参数预测，有更强判别力和降误差能力，以MIL为例并应用于SMEFT约束，或可提取理论最大Fisher信息。


<details>
  <summary>Details</summary>
Motivation: 获得给定假设检验问题中感兴趣参数更精确的预测，提升机器学习模型判别力和降低预测误差。

Method: 提出新机器学习方法，分析MIL模型关于实例数量的缩放行为来支持理论主张，用LHC子原子粒子碰撞事件运动学信息约束SMEFT的Wilson系数。

Result: 表明在某些情况下可能提取数据集中潜在的理论最大Fisher信息。

Conclusion: 所提方法在参数预测等方面有效果，MIL有比单实例模型更强的预测能力。

Abstract: In this work, we propose a new machine learning (ML) methodology to obtain
more precise predictions for some parameters of interest in a given hypotheses
testing problem. Our proposed method also allows ML models to have more
discriminative power in cases where it is extremely challenging for
state-of-the-art classifiers to have any level of accurate predictions. This
method can also allow us to systematically decrease the error from ML models in
their predictions. In this paper, we provide a mathematical motivation why
Multiple Instance Learning (MIL) would have more predictive power over their
single-instance counterparts. We support our theoretical claims by analyzing
the behavior of the MIL models through their scaling behaviors with respect to
the number of instances on which the model makes predictions. As a concrete
application, we constrain Wilson coefficients of the Standard Model Effective
Field Theory (SMEFT) using kinematic information from subatomic particle
collision events at the Large Hadron Collider (LHC). We show that under certain
circumstances, it might be possible to extract the theoretical maximum Fisher
Information latent in a dataset.

</details>


### [206] [From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context](https://arxiv.org/abs/2508.07117)
*Peyman Baghershahi,Gregoire Fournier,Pranav Nyati,Sourav Medya*

Main category: cs.LG

TL;DR: 提出轻量级事后框架LOGIC，用大语言模型为GNN预测生成解释，在多个数据集实验中表现良好，为图学习可解释性指明新方向。


<details>
  <summary>Details</summary>
Motivation: 现有GNN解释方法难生成细粒度、可解释理由，尤其是节点属性含丰富自然语言时。

Method: 将GNN节点嵌入投影到LLM嵌入空间，构建混合提示，使LLM对GNN内部表示推理。

Result: 在四个真实世界TAG数据集实验中，LOGIC在保真度和稀疏性间取得良好平衡，显著提升以人为中心的指标。

Conclusion: LOGIC通过对齐GNN内部机制和人类推理，为基于LLM的图学习可解释性设定新方向。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over
structured data, including text-attributed graphs, which are common in domains
such as citation networks, social platforms, and knowledge graphs. GNNs are not
inherently interpretable and thus, many explanation methods have been proposed.
However, existing explanation methods often struggle to generate interpretable,
fine-grained rationales, especially when node attributes include rich natural
language. In this work, we introduce LOGIC, a lightweight, post-hoc framework
that uses large language models (LLMs) to generate faithful and interpretable
explanations for GNN predictions. LOGIC projects GNN node embeddings into the
LLM embedding space and constructs hybrid prompts that interleave soft prompts
with textual inputs from the graph structure. This enables the LLM to reason
about GNN internal representations and produce natural language explanations
along with concise explanation subgraphs. Our experiments across four
real-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off
between fidelity and sparsity, while significantly improving human-centric
metrics such as insightfulness. LOGIC sets a new direction for LLM-based
explainability in graph learning by aligning GNN internals with human
reasoning.

</details>


### [207] [Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks](https://arxiv.org/abs/2508.07122)
*Zhihao Xue,Yun Zi,Nia Qi,Ming Gong,Yujun Zou*

Main category: cs.LG

TL;DR: 本文提出基于时空图神经网络的性能预测算法，用于分布式后端系统性能波动预测，实验表明该模型优于现有方法，具实用潜力。


<details>
  <summary>Details</summary>
Motivation: 解决具有多级服务调用结构的分布式后端系统性能波动预测的挑战。

Method: 将不同时间片的系统状态抽象为图结构序列，构建统一的时空建模框架，用图卷积网络提取拓扑信息，门控循环网络捕捉性能指标动态变化，引入时间编码机制，端到端训练优化多层嵌套结构。

Result: 使用大规模公共集群数据集实验，该模型在MAE、RMSE和R2等关键指标上优于现有代表方法，在不同负载强度和结构复杂度下保持强鲁棒性。

Conclusion: 该模型在后端服务性能管理任务中有实际应用潜力。

Abstract: This paper proposes a spatiotemporal graph neural network-based performance
prediction algorithm to address the challenge of forecasting performance
fluctuations in distributed backend systems with multi-level service call
structures. The method abstracts system states at different time slices into a
sequence of graph structures. It integrates the runtime features of service
nodes with the invocation relationships among services to construct a unified
spatiotemporal modeling framework. The model first applies a graph
convolutional network to extract high-order dependency information from the
service topology. Then it uses a gated recurrent network to capture the dynamic
evolution of performance metrics over time. A time encoding mechanism is also
introduced to enhance the model's ability to represent non-stationary temporal
sequences. The architecture is trained in an end-to-end manner, optimizing the
multi-layer nested structure to achieve high-precision regression of future
service performance metrics. To validate the effectiveness of the proposed
method, a large-scale public cluster dataset is used. A series of
multi-dimensional experiments are designed, including variations in time
windows and concurrent load levels. These experiments comprehensively evaluate
the model's predictive performance and stability. The experimental results show
that the proposed model outperforms existing representative methods across key
metrics such as MAE, RMSE, and R2. It maintains strong robustness under varying
load intensities and structural complexities. These results demonstrate the
model's practical potential for backend service performance management tasks.

</details>


### [208] [Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning](https://arxiv.org/abs/2508.07126)
*Zhengran Ji,Boyuan Chen*

Main category: cs.LG

TL;DR: 提出Pref - GUIDE框架将实时标量反馈转化为基于偏好的数据，用于在线强化学习，在三个环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有在线强化学习中收集实时标量反馈有噪声和不一致问题，限制奖励模型学习的准确性和泛化性。

Method: 提出Pref - GUIDE框架，包括Individual通过短窗口比较和过滤模糊反馈减少时间不一致性，Voting通过聚合用户奖励模型形成共识偏好。

Result: 在三个具有挑战性的环境中，Pref - GUIDE显著优于标量反馈基线，投票变体甚至超过专家设计的密集奖励。

Conclusion: 通过将标量反馈重构为结构化偏好并结合群体反馈，Pref - GUIDE为在线强化学习利用人类输入提供了可扩展且有原则的方法。

Abstract: Training reinforcement learning agents with human feedback is crucial when
task objectives are difficult to specify through dense reward functions. While
prior methods rely on offline trajectory comparisons to elicit human
preferences, such data is unavailable in online learning scenarios where agents
must adapt on the fly. Recent approaches address this by collecting real-time
scalar feedback to guide agent behavior and train reward models for continued
learning after human feedback becomes unavailable. However, scalar feedback is
often noisy and inconsistent, limiting the accuracy and generalization of
learned rewards. We propose Pref-GUIDE, a framework that transforms real-time
scalar feedback into preference-based data to improve reward model learning for
continual policy training. Pref-GUIDE Individual mitigates temporal
inconsistency by comparing agent behaviors within short windows and filtering
ambiguous feedback. Pref-GUIDE Voting further enhances robustness by
aggregating reward models across a population of users to form consensus
preferences. Across three challenging environments, Pref-GUIDE significantly
outperforms scalar-feedback baselines, with the voting variant exceeding even
expert-designed dense rewards. By reframing scalar feedback as structured
preferences with population feedback, Pref-GUIDE offers a scalable and
principled approach for harnessing human input in online reinforcement
learning.

</details>


### [209] [How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?](https://arxiv.org/abs/2508.07127)
*Niranjana Arun Menon,Iqra Farooq,Yulong Li,Sara Ahmed,Yutong Xie,Muhammad Awais,Imran Razzak*

Main category: cs.LG

TL;DR: 本文探讨微调大语言模型（LLMs）利用高通量基因组分析的遗传标记预测心脏病和相关单核苷酸多态性（SNPs）的潜力，结果显示其在心脏护理个性化医疗方面有前景。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病预测因多因素病因和高发病率死亡率面临挑战，从高维、嘈杂且注释稀疏的基因组和电生理数据中提取有意义信息是难题，近期LLMs在生物序列结构变异预测中表现良好，因此探索其在心脏病预测中的应用。

Method: 利用高通量基因组分析的遗传标记，研究与心脏状况相关的遗传模式，将问题构建为思维链（CoT）推理任务，让模型根据不同患者特征和表型生成疾病标签并进行临床推断。

Result: 发现LLMs在心脏病早期检测、风险评估和个性化医疗推进方面有潜力。

Conclusion: LLMs有助于心脏护理的早期检测、风险评估和个性化医疗的发展。

Abstract: Cardiovascular disease (CVD) prediction remains a tremendous challenge due to
its multifactorial etiology and global burden of morbidity and mortality.
Despite the growing availability of genomic and electrophysiological data,
extracting biologically meaningful insights from such high-dimensional, noisy,
and sparsely annotated datasets remains a non-trivial task. Recently, LLMs has
been applied effectively to predict structural variations in biological
sequences. In this work, we explore the potential of fine-tuned LLMs to predict
cardiac diseases and SNPs potentially leading to CVD risk using genetic markers
derived from high-throughput genomic profiling. We investigate the effect of
genetic patterns associated with cardiac conditions and evaluate how LLMs can
learn latent biological relationships from structured and semi-structured
genomic data obtained by mapping genetic aspects that are inherited from the
family tree. By framing the problem as a Chain of Thought (CoT) reasoning task,
the models are prompted to generate disease labels and articulate informed
clinical deductions across diverse patient profiles and phenotypes. The
findings highlight the promise of LLMs in contributing to early detection, risk
assessment, and ultimately, the advancement of personalized medicine in cardiac
care.

</details>


### [210] [A Globally Optimal Analytic Solution for Semi-Nonnegative Matrix Factorization with Nonnegative or Mixed Inputs](https://arxiv.org/abs/2508.07134)
*Lu Chenggang*

Main category: cs.LG

TL;DR: 提出新方法解决半非负矩阵分解（semi - NMF）问题，有全局最优解，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有semi - NMF算法多为迭代、非凸且易陷入局部极小值，需新方法解决。

Method: 通过输入数据散度矩阵的正交分解，得到Frobenius范数下semi - NMF问题的全局最优解。

Result: 证明解能达到重构误差全局最小；输入矩阵非负时，常比标准NMF算法重构误差低；低秩情况能得到非负分解；实验显示在重构精度上优于现有NMF和semi - NMF方法。

Conclusion: 该全局最优、非迭代公式有理论保证和实证优势，为矩阵分解提供新视角。

Abstract: Semi-Nonnegative Matrix Factorization (semi-NMF) extends classical
Nonnegative Matrix Factorization (NMF) by allowing the basis matrix to contain
both positive and negative entries, making it suitable for decomposing data
with mixed signs. However, most existing semi-NMF algorithms are iterative,
non-convex, and prone to local minima. In this paper, we propose a novel method
that yields a globally optimal solution to the semi-NMF problem under the
Frobenius norm, through an orthogonal decomposition derived from the scatter
matrix of the input data. We rigorously prove that our solution attains the
global minimum of the reconstruction error. Furthermore, we demonstrate that
when the input matrix is nonnegative, our method often achieves lower
reconstruction error than standard NMF algorithms, although unfortunately the
basis matrix may not satisfy nonnegativity. In particular, in low-rank cases
such as rank 1 or 2, our solution reduces exactly to a nonnegative
factorization, recovering the NMF structure. We validate our approach through
experiments on both synthetic data and the UCI Wine dataset, showing that our
method consistently outperforms existing NMF and semi-NMF methods in terms of
reconstruction accuracy. These results confirm that our globally optimal,
non-iterative formulation offers both theoretical guarantees and empirical
advantages, providing a new perspective on matrix factorization in optimization
and data analysis.

</details>


### [211] [A Stable and Principled Loss Function for Direct Language Model Alignment](https://arxiv.org/abs/2508.07137)
*Yuandong Tan*

Main category: cs.LG

TL;DR: 本文指出DPO损失函数理论上的问题，提出新的损失函数，理论分析显示其稳定性更好，实验验证在微调模型时比DPO基线有显著胜率提升。


<details>
  <summary>Details</summary>
Motivation: DPO损失函数理论上与自身推导不一致，会导致训练不稳定和奖励作弊问题，因此需要改进。

Method: 从RLHF最优条件直接推导新的损失函数，目标是让logits差异达到特定有限值，进行基于梯度的理论分析。

Result: 对Qwen2.5 - 7B模型微调，相比标准DPO基线有显著胜率提升，与Llama - 3.1 - 8B等更大模型有竞争力。

Conclusion: 提出的新损失函数避免了DPO的问题，具有稳定性，能实现更有效的对齐。

Abstract: The alignment of large language models (LLMs) with human preferences is
commonly achieved through Reinforcement Learning from Human Feedback (RLHF).
Direct Preference Optimization (DPO) simplified this paradigm by establishing a
direct mapping between the optimal policy and a reward function, eliminating
the need for an explicit reward model. However, we argue that the DPO loss
function is theoretically misaligned with its own derivation, as it promotes
the indefinite maximization of a logits difference, which can lead to training
instability and reward hacking. In this paper, we propose a novel loss function
derived directly from the RLHF optimality condition. Our proposed loss targets
a specific, finite value for the logits difference, which is dictated by the
underlying reward, rather than its maximization. We provide a theoretical
analysis, including a gradient-based comparison, to demonstrate that our method
avoids the large gradients that plague DPO when the probability of dispreferred
responses approaches zero. This inherent stability prevents reward hacking and
leads to more effective alignment. We validate our approach by fine-tuning a
Qwen2.5-7B model, showing significant win-rate improvements over a standard DPO
baseline and achieving competitive performance against larger models like
Llama-3.1-8B.

</details>


### [212] [SGD Convergence under Stepsize Shrinkage in Low-Precision Training](https://arxiv.org/abs/2508.07142)
*Vincent-Daniel Yun*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Low-precision training has become essential for reducing the computational
and memory costs of large-scale deep learning. However, quantization of
gradients introduces both magnitude shrinkage and additive noise, which can
alter the convergence behavior of stochastic gradient descent (SGD). In this
work, we study the convergence of SGD under a gradient shrinkage model, where
each stochastic gradient is scaled by a factor $q_k \in (0,1]$ and perturbed by
zero-mean quantization noise. We show that this shrinkage is equivalent to
replacing the nominal stepsize $\mu_k$ with an effective stepsize $\mu_k q_k$,
which slows convergence when $q_{\min} < 1$. Under standard smoothness and
bounded-variance assumptions, we prove that low-precision SGD still converges,
but at a reduced rate determined by $q_{\min}$, and with an increased
asymptotic error floor due to quantization noise. We theoretically analyze how
reduced numerical precision slows down training by modeling it as gradient
shrinkage in the standard SGD convergence framework.

</details>


### [213] [What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains](https://arxiv.org/abs/2508.07208)
*Chanakya Ekbote,Marco Bondaschi,Nived Rajaraman,Jason D. Lee,Michael Gastpar,Ashok Vardhan Makkuva,Paul Pu Liang*

Main category: cs.LG

TL;DR: 本文理论证明两层每层单头的Transformer可表示任何条件k元语法，分析其学习动态，加深对基于Transformer的上下文学习的理解。


<details>
  <summary>Details</summary>
Motivation: 此前工作揭示模型深度对上下文学习能力有影响，但对于高阶马尔可夫源，最佳构造至少需三层，因此探究两层单头Transformer能否表示任意k阶马尔可夫过程。

Method: 进行理论分析证明两层每层单头的Transformer可表示任何条件k元语法，聚焦一阶马尔可夫链的简化变体分析学习动态。

Result: 理论证明两层每层单头的Transformer可表示任何条件k元语法，分析了其学习动态。

Conclusion: 研究加深了对基于Transformer的上下文学习的理解，表明浅层架构在结构化序列建模任务中也能有强上下文学习能力。

Abstract: In-context learning (ICL) is a hallmark capability of transformers, through
which trained models learn to adapt to new tasks by leveraging information from
the input context. Prior work has shown that ICL emerges in transformers due to
the presence of special circuits called induction heads. Given the equivalence
between induction heads and conditional k-grams, a recent line of work modeling
sequential inputs as Markov processes has revealed the fundamental impact of
model depth on its ICL capabilities: while a two-layer transformer can
efficiently represent a conditional 1-gram model, its single-layer counterpart
cannot solve the task unless it is exponentially large. However, for higher
order Markov sources, the best known constructions require at least three
layers (each with a single attention head) - leaving open the question: can a
two-layer single-head transformer represent any kth-order Markov process? In
this paper, we precisely address this and theoretically show that a two-layer
transformer with one head per layer can indeed represent any conditional
k-gram. Thus, our result provides the tightest known characterization of the
interplay between transformer depth and Markov order for ICL. Building on this,
we further analyze the learning dynamics of our two-layer construction,
focusing on a simplified variant for first-order Markov chains, illustrating
how effective in-context representations emerge during training. Together,
these results deepen our current understanding of transformer-based ICL and
illustrate how even shallow architectures can surprisingly exhibit strong ICL
capabilities on structured sequence modeling tasks.

</details>


### [214] [Neural Bridge Processes](https://arxiv.org/abs/2508.07220)
*Jian Xu,Yican Liu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 提出神经桥过程（NBPs）用于建模随机函数，在多个任务上验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统模型如GPs有扩展性和高斯假设问题，NPs难捕捉复杂分布，NDPs存在输入耦合弱和语义不匹配问题。

Method: 提出NBPs，将输入x作为整个扩散轨迹的动态锚点，重新构建前向核使其明确依赖x。

Result: 在合成数据、EEG信号回归和图像回归任务上相比基线有显著提升。

Conclusion: DDPM风格的桥采样在提升结构化预测任务的性能和理论一致性上有效。

Abstract: Learning stochastic functions from partially observed context-target pairs is
a fundamental problem in probabilistic modeling. Traditional models like
Gaussian Processes (GPs) face scalability issues with large datasets and assume
Gaussianity, limiting their applicability. While Neural Processes (NPs) offer
more flexibility, they struggle with capturing complex, multi-modal target
distributions. Neural Diffusion Processes (NDPs) enhance expressivity through a
learned diffusion process but rely solely on conditional signals in the
denoising network, resulting in weak input coupling from an unconditional
forward process and semantic mismatch at the diffusion endpoint. In this work,
we propose Neural Bridge Processes (NBPs), a novel method for modeling
stochastic functions where inputs x act as dynamic anchors for the entire
diffusion trajectory. By reformulating the forward kernel to explicitly depend
on x, NBP enforces a constrained path that strictly terminates at the
supervised target. This approach not only provides stronger gradient signals
but also guarantees endpoint coherence. We validate NBPs on synthetic data, EEG
signal regression and image regression tasks, achieving substantial
improvements over baselines. These results underscore the effectiveness of
DDPM-style bridge sampling in enhancing both performance and theoretical
consistency for structured prediction tasks.

</details>


### [215] [LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference](https://arxiv.org/abs/2508.07221)
*Po-Han Lee,Yu-Cheng Lin,Chan-Tung Ku,Chan Hsu,Pei-Cing Huang,Ping-Hsun Wu,Yihuang Kang*

Main category: cs.LG

TL;DR: 提出基于大语言模型的代理用于自动混杂因素发现和亚组分析，减少人工依赖，实验证明能增强治疗效果估计的稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有因果机器学习方法在复杂环境中因潜在混杂因素和人工标注成本高而效果受限。

Method: 将基于大语言模型的代理集成到因果机器学习流程中，利用其推理能力进行亚组识别和混杂结构发现。

Result: 在真实医疗数据集上实验表明，该方法通过缩小置信区间和发现未识别的混杂偏差，增强了治疗效果估计的稳健性。

Conclusion: 基于大语言模型的代理为可扩展、可信和语义感知的因果推断提供了有前景的途径。

Abstract: Estimating individualized treatment effects from observational data presents
a persistent challenge due to unmeasured confounding and structural bias.
Causal Machine Learning (causal ML) methods, such as causal trees and doubly
robust estimators, provide tools for estimating conditional average treatment
effects. These methods have limited effectiveness in complex real-world
environments due to the presence of latent confounders or those described in
unstructured formats. Moreover, reliance on domain experts for confounder
identification and rule interpretation introduces high annotation cost and
scalability concerns. In this work, we proposed Large Language Model-based
agents for automated confounder discovery and subgroup analysis that integrate
agents into the causal ML pipeline to simulate domain expertise. Our framework
systematically performs subgroup identification and confounding structure
discovery by leveraging the reasoning capabilities of LLM-based agents, which
reduces human dependency while preserving interpretability. Experiments on
real-world medical datasets show that our proposed approach enhances treatment
effect estimation robustness by narrowing confidence intervals and uncovering
unrecognized confounding biases. Our findings suggest that LLM-based agents
offer a promising path toward scalable, trustworthy, and semantically aware
causal inference.

</details>


### [216] [EDGE: A Theoretical Framework for Misconception-Aware Adaptive Learning](https://arxiv.org/abs/2508.07224)
*Ananda Prakash Verma*

Main category: cs.LG

TL;DR: 提出通用、误解感知自适应学习框架EDGE，含四阶段，统一多方法，定义EdgeScore并证明性质，推导近最优索引策略，还明确反事实项目优势，侧重理论和伪代码。


<details>
  <summary>Details</summary>
Motivation: 构建通用、能感知学习者误解的自适应学习框架。

Method: 将心理测量学、认知诊断、对比项目生成和原则性调度等方法结合，定义EdgeScore，推导索引策略。

Result: 证明EdgeScore的单调性和Lipschitz连续性，推导近最优索引策略，明确反事实项目比标准做法更快降低目标误解后验概率。

Conclusion: 论文侧重理论和可实现的伪代码，实证研究留待未来。

Abstract: We present EDGE, a general-purpose, misconception-aware adaptive learning
framework composed of four stages: Evaluate (ability and state estimation),
Diagnose (posterior infer-ence of misconceptions), Generate (counterfactual
item synthesis), and Exercise (index-based retrieval scheduling). EDGE unifies
psychometrics (IRT/Bayesian state space models), cog-nitive diagnostics
(misconception discovery from distractor patterns and response latencies),
contrastive item generation (minimal perturbations that invalidate learner
shortcuts while pre-serving psychometric validity), and principled scheduling
(a restless bandit approximation to spaced retrieval). We formalize a composite
readiness metric, EdgeScore, prove its monotonicity and Lipschitz continuity,
and derive an index policy that is near-optimal under mild assumptions on
forgetting and learning gains. We further establish conditions under which
counterfactual items provably reduce the posterior probability of a targeted
misconception faster than standard practice. The paper focuses on theory and
implementable pseudocode; empirical study is left to future work.

</details>


### [217] [Causal Negative Sampling via Diffusion Model for Out-of-Distribution Recommendation](https://arxiv.org/abs/2508.07243)
*Chu Zhao,Eneng Yang,Yizhou Dang,Jianzhe Zhao,Guibing Guo,Xingwei Wang*

Main category: cs.LG

TL;DR: 启发式负采样有问题，提出CNSDiff方法，实验验证其在OOD推荐任务有效且稳健。


<details>
  <summary>Details</summary>
Motivation: 启发式负采样因候选池中的环境混杂因素会引入虚假硬负样本，影响模型泛化能力。

Method: 提出Causal Negative Sampling via Diffusion (CNSDiff)方法，在潜在空间通过条件扩散过程合成负样本，避免候选池偏差，并引入因果正则化项减轻环境混杂因素影响。

Result: 在四种代表性分布偏移场景的综合实验中，CNSDiff在所有评估指标上比最先进基线平均提高13.96%。

Conclusion: CNSDiff在OOD推荐任务中有效且稳健。

Abstract: Heuristic negative sampling enhances recommendation performance by selecting
negative samples of varying hardness levels from predefined candidate pools to
guide the model toward learning more accurate decision boundaries. However, our
empirical and theoretical analyses reveal that unobserved environmental
confounders (e.g., exposure or popularity biases) in candidate pools may cause
heuristic sampling methods to introduce false hard negatives (FHNS). These
misleading samples can encourage the model to learn spurious correlations
induced by such confounders, ultimately compromising its generalization ability
under distribution shifts. To address this issue, we propose a novel method
named Causal Negative Sampling via Diffusion (CNSDiff). By synthesizing
negative samples in the latent space via a conditional diffusion process,
CNSDiff avoids the bias introduced by predefined candidate pools and thus
reduces the likelihood of generating FHNS. Moreover, it incorporates a causal
regularization term to explicitly mitigate the influence of environmental
confounders during the negative sampling process, leading to robust negatives
that promote out-of-distribution (OOD) generalization. Comprehensive
experiments under four representative distribution shift scenarios demonstrate
that CNSDiff achieves an average improvement of 13.96% across all evaluation
metrics compared to state-of-the-art baselines, verifying its effectiveness and
robustness in OOD recommendation tasks.

</details>


### [218] [Policy Newton methods for Distortion Riskmetrics](https://arxiv.org/abs/2508.07249)
*Soumen Pachal,Mizhaan Prajit Maniyar,Prashanth L. A*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of risk-sensitive control in a reinforcement learning
(RL) framework. In particular, we aim to find a risk-optimal policy by
maximizing the distortion riskmetric (DRM) of the discounted reward in a finite
horizon Markov decision process (MDP). DRMs are a rich class of risk measures
that include several well-known risk measures as special cases. We derive a
policy Hessian theorem for the DRM objective using the likelihood ratio method.
Using this result, we propose a natural DRM Hessian estimator from sample
trajectories of the underlying MDP. Next, we present a cubic-regularized policy
Newton algorithm for solving this problem in an on-policy RL setting using
estimates of the DRM gradient and Hessian. Our proposed algorithm is shown to
converge to an $\epsilon$-second-order stationary point ($\epsilon$-SOSP) of
the DRM objective, and this guarantee ensures the escaping of saddle points.
The sample complexity of our algorithms to find an $ \epsilon$-SOSP is
$\mathcal{O}(\epsilon^{-3.5})$. Our experiments validate the theoretical
findings. To the best of our knowledge, our is the first work to present
convergence to an $\epsilon$-SOSP of a risk-sensitive objective, while existing
works in the literature have either shown convergence to a first-order
stationary point of a risk-sensitive objective, or a SOSP of a risk-neutral
one.

</details>


### [219] [PySeizure: A single machine learning classifier framework to detect seizures in diverse datasets](https://arxiv.org/abs/2508.07253)
*Bartlomiej Chybowski,Shima Abdullateef,Hollan Haule,Alfredo Gonzalez-Sulser,Javier Escudero*

Main category: cs.LG

TL;DR: 提出开源机器学习框架用于癫痫发作检测，在不同数据集上评估，性能良好且有跨数据集泛化能力，为临床可用系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 可靠的癫痫发作检测对癫痫诊断和管理至关重要，但临床依赖耗时的手动脑电图解读，现有机器学习方法依赖特定数据集优化，缺乏实用性和可重复性。

Method: 引入开源机器学习框架，包含自动预处理流程和多数投票机制，在两个公开脑电图数据集上训练、调优和评估模型，评估跨数据集迁移能力。

Result: 模型在数据集内表现良好（CHB - MIT的AUC为0.904±0.059，TUSZ为0.864±0.060），跨数据集泛化能力强，轻度后处理可提升性能。

Conclusion: 该框架有在不同临床环境部署的潜力，为推进临床可行、与数据集无关的癫痫发作检测系统提供基础，可广泛应用并加速临床整合。

Abstract: Reliable seizure detection is critical for diagnosing and managing epilepsy,
yet clinical workflows remain dependent on time-consuming manual EEG
interpretation. While machine learning has shown promise, existing approaches
often rely on dataset-specific optimisations, limiting their real-world
applicability and reproducibility. Here, we introduce an innovative,
open-source machine-learning framework that enables robust and generalisable
seizure detection across varied clinical datasets. We evaluate our approach on
two publicly available EEG datasets that differ in patient populations and
electrode configurations. To enhance robustness, the framework incorporates an
automated pre-processing pipeline to standardise data and a majority voting
mechanism, in which multiple models independently assess each second of EEG
before reaching a final decision. We train, tune, and evaluate models within
each dataset, assessing their cross-dataset transferability. Our models achieve
high within-dataset performance (AUC 0.904+/-0.059 for CHB-MIT and
0.864+/-0.060 for TUSZ) and demonstrate strong generalisation across datasets
despite differences in EEG setups and populations (AUC 0.615+/-0.039 for models
trained on CHB-MIT and tested on TUSZ and 0.762+/-0.175 in the reverse case)
without any post-processing. Furthermore, a mild post-processing improved the
within-dataset results to 0.913+/-0.064 and 0.867+/-0.058 and cross-dataset
results to 0.619+/-0.036 and 0.768+/-0.172. These results underscore the
potential of, and essential considerations for, deploying our framework in
diverse clinical settings. By making our methodology fully reproducible, we
provide a foundation for advancing clinically viable, dataset-agnostic seizure
detection systems. This approach has the potential for widespread adoption,
complementing rather than replacing expert interpretation, and accelerating
clinical integration.

</details>


### [220] [Revisiting Data Attribution for Influence Functions](https://arxiv.org/abs/2508.07297)
*Hongbo Zhu,Angelo Cangelosi*

Main category: cs.LG

TL;DR: 本文全面回顾影响函数在深度学习中的数据归因能力，探讨其理论基础、算法进展，评估其效果，指出挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 理解单个训练样本对模型预测的影响，对机器学习可解释性、数据调试和模型问责至关重要，而影响函数可用于数据归因。

Method: 对影响函数在深度学习中的数据归因能力进行全面回顾，讨论理论基础、算法进展，评估其效果。

Result: 评估了影响函数在数据归因和误标签检测方面的有效性。

Conclusion: 指出当前挑战和在大规模、现实深度学习场景中释放影响函数潜力的有前景方向。

Abstract: The goal of data attribution is to trace the model's predictions through the
learning algorithm and back to its training data. thereby identifying the most
influential training samples and understanding how the model's behavior leads
to particular predictions. Understanding how individual training examples
influence a model's predictions is fundamental for machine learning
interpretability, data debugging, and model accountability. Influence
functions, originating from robust statistics, offer an efficient, first-order
approximation to estimate the impact of marginally upweighting or removing a
data point on a model's learned parameters and its subsequent predictions,
without the need for expensive retraining. This paper comprehensively reviews
the data attribution capability of influence functions in deep learning. We
discuss their theoretical foundations, recent algorithmic advances for
efficient inverse-Hessian-vector product estimation, and evaluate their
effectiveness for data attribution and mislabel detection. Finally,
highlighting current challenges and promising directions for unleashing the
huge potential of influence functions in large-scale, real-world deep learning
scenarios.

</details>


### [221] [When Is Prior Knowledge Helpful? Exploring the Evaluation and Selection of Unsupervised Pretext Tasks from a Neuro-Symbolic Perspective](https://arxiv.org/abs/2508.07299)
*Lin-Han Jia,Si-Yu Han,Wen-Chao Hu,Jie-Jing Shao,Wen-Da Wei,Zhi Zhou,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.LG

TL;DR: 文章统一半/自监督学习与神经符号学习理论框架，分析预训练任务对目标性能影响因素，提出评估方法并验证有效性。


<details>
  <summary>Details</summary>
Motivation: 当前无监督预训练任务选择基于启发式而非理论，难以在目标任务测试前评估其合理性，需统一理论框架并提出有效评估方法。

Method: 将可靠知识的神经符号理论扩展到不可靠知识场景，统一理论框架；分析预训练任务影响目标性能的因素，提出操作理论指标的方案和预测预训练任务有效性的方法。

Result: 实验验证使用少量数据预测的性能与大规模半/自监督学习后的实际性能高度相关。

Conclusion: 理论有效，评估方法可行，可改变无监督任务选择现状。

Abstract: Neuro-symbolic (Nesy) learning improves the target task performance of models
by enabling them to satisfy knowledge, while semi/self-supervised learning
(SSL) improves the target task performance by designing unsupervised pretext
tasks for unlabeled data to make models satisfy corresponding assumptions. We
extend the Nesy theory based on reliable knowledge to the scenario of
unreliable knowledge (i.e., assumptions), thereby unifying the theoretical
frameworks of SSL and Nesy. Through rigorous theoretical analysis, we
demonstrate that, in theory, the impact of pretext tasks on target performance
hinges on three factors: knowledge learnability with respect to the model,
knowledge reliability with respect to the data, and knowledge completeness with
respect to the target. We further propose schemes to operationalize these
theoretical metrics, and thereby develop a method that can predict the
effectiveness of pretext tasks in advance. This will change the current status
quo in practical applications, where the selections of unsupervised tasks are
heuristic-based rather than theory-based, and it is difficult to evaluate the
rationality of unsupervised pretext task selection before testing the model on
the target task. In experiments, we verify a high correlation between the
predicted performance-estimated using minimal data-and the actual performance
achieved after large-scale semi-supervised or self-supervised learning, thus
confirming the validity of the theory and the effectiveness of the evaluation
method.

</details>


### [222] [Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative](https://arxiv.org/abs/2508.07329)
*Tuo Zhang,Ning Li,Xin Yuan,Wenchao Xu,Quan Chen,Song Guo,Haijun Zhang*

Main category: cs.LG

TL;DR: 论文针对大语言模型在边缘设备部署难题，提出基于HAQ和CPU - GPU协作推理的MoE边缘部署方案，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在资源受限边缘设备高效部署成关键挑战，MoE架构部署存在量化精度下降和内存受限下难以平衡延迟与吞吐量的问题。

Method: 引入平滑Hessian矩阵量化实现激活和权重的8位联合量化；设计专家级协作卸载和推理机制，结合专家激活路径统计在CPU和GPU间部署和调度专家模块。

Result: 在OPT系列和Mixtral 8*7B等主流大模型实验中，低比特量化模型推理精度接近全精度模型，GPU内存使用减少约60%，推理延迟显著改善。

Conclusion: 提出的基于HAQ和CPU - GPU协作推理的MoE边缘部署方案有效，能解决当前MoE架构部署难题。

Abstract: With the breakthrough progress of large language models (LLMs) in natural
language processing and multimodal tasks, efficiently deploying them on
resource-constrained edge devices has become a critical challenge. The Mixture
of Experts (MoE) architecture enhances model capacity through sparse
activation, but faces two major difficulties in practical deployment: (1) The
presence of numerous outliers in activation distributions leads to severe
degradation in quantization accuracy for both activations and weights,
significantly impairing inference performance; (2) Under limited memory,
efficient offloading and collaborative inference of expert modules struggle to
balance latency and throughput. To address these issues, this paper proposes an
efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ)
and CPU-GPU collaborative inference. First, by introducing smoothed Hessian
matrix quantization, we achieve joint 8-bit quantization of activations and
weights, which significantly alleviates the accuracy loss caused by outliers
while ensuring efficient implementation on mainstream hardware. Second, we
design an expert-level collaborative offloading and inference mechanism, which,
combined with expert activation path statistics, enables efficient deployment
and scheduling of expert modules between CPU and GPU, greatly reducing memory
footprint and inference latency. Extensive experiments validate the
effectiveness of our method on mainstream large models such as the OPT series
and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of
the low-bit quantized model approaches that of the full-precision model, while
GPU memory usage is reduced by about 60%, and inference latency is
significantly improved.

</details>


### [223] [Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants](https://arxiv.org/abs/2508.07333)
*Yuhao Liu,Rui Hu,Yu Chen,Longbo Huang*

Main category: cs.LG

TL;DR: 本文聚焦随机插值器ODE数值实现的有限时间收敛分析，给出两种数值积分器误差界，分析迭代复杂度并经实验验证。


<details>
  <summary>Details</summary>
Motivation: 随机插值器有潜力，但其实践数值方案的有限时间收敛保证研究不足，需对其ODE数值实现进行有限时间收敛分析。

Method: 为一阶前向欧拉法和二阶Heun法建立总变差距离的有限时间误差界，分析特定随机插值器构造的迭代复杂度。

Result: 得出两种数值积分器的有限时间误差界和特定随机插值器构造的迭代复杂度，并通过数值实验验证。

Conclusion: 理论分析有效，得出的误差界和复杂度分析可优化计算效率。

Abstract: Stochastic interpolants offer a robust framework for continuously
transforming samples between arbitrary data distributions, holding significant
promise for generative modeling. Despite their potential, rigorous finite-time
convergence guarantees for practical numerical schemes remain largely
unexplored. In this work, we address the finite-time convergence analysis of
numerical implementations for ordinary differential equations (ODEs) derived
from stochastic interpolants. Specifically, we establish novel finite-time
error bounds in total variation distance for two widely used numerical
integrators: the first-order forward Euler method and the second-order Heun's
method. Furthermore, our analysis on the iteration complexity of specific
stochastic interpolant constructions provides optimized schedules to enhance
computational efficiency. Our theoretical findings are corroborated by
numerical experiments, which validate the derived error bounds and complexity
analyses.

</details>


### [224] [ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis](https://arxiv.org/abs/2508.07345)
*Samiha Afaf Neha,Abir Ahammed Bhuiyan,Md. Ishrak Khan*

Main category: cs.LG

TL;DR: 本文提出ProteoKnight图像编码方法用于噬菌体病毒粒子蛋白（PVP）分类，结合预训练CNN实现较好二分类效果，并评估预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 准确预测PVP对基因组研究很重要，现有计算工具需专业序列编码，解决现有技术空间约束问题。

Method: ProteoKnight改编DNA - Walk算法用于蛋白质序列，结合像素颜色和调整步长，用预训练CNN分类，用方差和熵评估预测不确定性。

Result: 二分类准确率达90.8%，多分类效果欠佳，不确定性分析揭示预测置信度受蛋白类别和序列长度影响。

Conclusion: 新图像编码超越FCGR，分类技术能准确鲁棒预测PVP并识别低置信度预测。

Abstract: \textbf{Introduction:} Accurate prediction of Phage Virion Proteins (PVP) is
essential for genomic studies due to their crucial role as structural elements
in bacteriophages. Computational tools, particularly machine learning, have
emerged for annotating phage protein sequences from high-throughput sequencing.
However, effective annotation requires specialized sequence encodings. Our
paper introduces ProteoKnight, a new image-based encoding method that addresses
spatial constraints in existing techniques, yielding competitive performance in
PVP classification using pre-trained convolutional neural networks.
Additionally, our study evaluates prediction uncertainty in binary PVP
classification through Monte Carlo Dropout (MCD). \textbf{Methods:}
ProteoKnight adapts the classical DNA-Walk algorithm for protein sequences,
incorporating pixel colors and adjusting walk distances to capture intricate
protein features. Encoded sequences were classified using multiple pre-trained
CNNs. Variance and entropy measures assessed prediction uncertainty across
proteins of various classes and lengths. \textbf{Results:} Our experiments
achieved 90.8% accuracy in binary classification, comparable to
state-of-the-art methods. Multi-class classification accuracy remains
suboptimal. Our uncertainty analysis unveils variability in prediction
confidence influenced by protein class and sequence length.
\textbf{Conclusions:} Our study surpasses frequency chaos game representation
(FCGR) by introducing novel image encoding that mitigates spatial information
loss limitations. Our classification technique yields accurate and robust PVP
predictions while identifying low-confidence predictions.

</details>


### [225] [Intrinsic training dynamics of deep neural networks](https://arxiv.org/abs/2508.07370)
*Sibylle Marcotte,Gabriel Peyré,Rémi Gribonval*

Main category: cs.LG

TL;DR: 研究高维参数空间梯度流能否映射到低维变量的内在梯度流，给出准则并应用于ReLU网络、线性网络和线性神经ODE。


<details>
  <summary>Details</summary>
Motivation: 理解深度学习理论中高维参数空间基于梯度的训练能否由低维结构捕获，即隐式偏差问题。

Method: 提出内在动态属性，通过线性映射核的包含关系给出必要条件，将理论应用于不同网络。

Result: 在ReLU网络中可将流重写为低维内在动态；推广线性网络的降维结果到更广泛初始化；明确线性神经ODE的内在动态。

Conclusion: 给出高维梯度流到低维内在梯度流的条件，推广线性网络降维初始化范围。

Abstract: A fundamental challenge in the theory of deep learning is to understand
whether gradient-based training in high-dimensional parameter spaces can be
captured by simpler, lower-dimensional structures, leading to so-called
implicit bias. As a stepping stone, we study when a gradient flow on a
high-dimensional variable $\theta$ implies an intrinsic gradient flow on a
lower-dimensional variable $z = \phi(\theta)$, for an architecture-related
function $\phi$. We express a so-called intrinsic dynamic property and show how
it is related to the study of conservation laws associated with the
factorization $\phi$. This leads to a simple criterion based on the inclusion
of kernels of linear maps which yields a necessary condition for this property
to hold. We then apply our theory to general ReLU networks of arbitrary depth
and show that, for any initialization, it is possible to rewrite the flow as an
intrinsic dynamic in a lower dimension that depends only on $z$ and the
initialization, when $\phi$ is the so-called path-lifting. In the case of
linear networks with $\phi$ the product of weight matrices, so-called balanced
initializations are also known to enable such a dimensionality reduction; we
generalize this result to a broader class of {\em relaxed balanced}
initializations, showing that, in certain configurations, these are the
\emph{only} initializations that ensure the intrinsic dynamic property.
Finally, for the linear neural ODE associated with the limit of infinitely deep
linear networks, with relaxed balanced initialization, we explicitly express
the corresponding intrinsic dynamics.

</details>


### [226] [Parity Requires Unified Input Dependence and Negative Eigenvalues in SSMs](https://arxiv.org/abs/2508.07395)
*Behnoush Khavari,Mehran Shakerinava,Jayesh Khullar,Jerry Huang,François Rivest,Siamak Ravanbakhsh,Sarath Chandar*

Main category: cs.LG

TL;DR: 研究表明现有LRNN模型缺状态跟踪能力，研究多层SSM结合两种类型能否解决此问题，结果显示仍失败，意味着递归层需输入依赖且含负特征值。


<details>
  <summary>Details</summary>
Motivation: 现有LRNN模型如S4D、Mamba等因时不变转移矩阵或受限特征值范围缺乏状态跟踪能力，现有定理未探讨多层SSM结合两种类型能否解决此问题。

Method: 针对具有对角转移矩阵的高效SSM进行研究，并分析结合S4D和Mamba层的SSM模型。

Result: 多层SSM结合两种类型仍无法解决奇偶性问题。

Conclusion: 递归层必须同时具备输入依赖性和包含负特征值。

Abstract: Recent work has shown that LRNN models such as S4D, Mamba, and DeltaNet lack
state-tracking capability due to either time-invariant transition matrices or
restricted eigenvalue ranges. To address this, input-dependent transition
matrices, particularly those that are complex or non-triangular, have been
proposed to enhance SSM performance on such tasks. While existing theorems
demonstrate that both input-independent and non-negative SSMs are incapable of
solving simple state-tracking tasks, such as parity, regardless of depth, they
do not explore whether combining these two types in a multilayer SSM could
help. We investigate this question for efficient SSMs with diagonal transition
matrices and show that such combinations still fail to solve parity. This
implies that a recurrence layer must both be input-dependent and include
negative eigenvalues. Our experiments support this conclusion by analyzing an
SSM model that combines S4D and Mamba layers.

</details>


### [227] [Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors](https://arxiv.org/abs/2508.07400)
*Mohamad Louai Shehab,Alperen Tercan,Necmiye Ozay*

Main category: cs.LG

TL;DR: 本文研究从最优策略或最大熵强化学习问题的演示中恢复时变奖励函数，利用奖励的两种先验信息提出高效优化算法并举例验证。


<details>
  <summary>Details</summary>
Motivation: 恢复时变奖励函数问题在无额外假设时是病态的，而实际中奖励具有简约性且有先验信息，需利用先验解决问题。

Method: 考虑奖励的两种先验，将奖励识别问题分别转化为线性约束下的稀疏化问题和秩最小化问题，分别用精确求解和凸松弛方法解决。

Result: 提出了基于优化的奖励识别高效算法，通过例子证明了恢复奖励的准确性和泛化性。

Conclusion: 利用奖励的先验信息可将奖励识别问题转化为可求解的优化问题，所提算法有效。

Abstract: In this paper, we consider the problem of recovering time-varying reward
functions from either optimal policies or demonstrations coming from a max
entropy reinforcement learning problem. This problem is highly ill-posed
without additional assumptions on the underlying rewards. However, in many
applications, the rewards are indeed parsimonious, and some prior information
is available. We consider two such priors on the rewards: 1) rewards are mostly
constant and they change infrequently, 2) rewards can be represented by a
linear combination of a small number of feature functions. We first show that
the reward identification problem with the former prior can be recast as a
sparsification problem subject to linear constraints. Moreover, we give a
polynomial-time algorithm that solves this sparsification problem exactly.
Then, we show that identifying rewards representable with the minimum number of
features can be recast as a rank minimization problem subject to linear
constraints, for which convex relaxations of rank can be invoked. In both
cases, these observations lead to efficient optimization-based reward
identification algorithms. Several examples are given to demonstrate the
accuracy of the recovered rewards as well as their generalizability.

</details>


### [228] [Lightning Prediction under Uncertainty: DeepLight with Hazy Loss](https://arxiv.org/abs/2508.07428)
*Md Sultanul Arifin,Abu Nowshed Sakib,Yeasir Rayhan,Tanzima Hashem*

Main category: cs.LG

TL;DR: 提出DeepLight深度学习架构用于闪电预测，克服现有模型局限，实验显示其ETS比现有方法提升18%-30%。


<details>
  <summary>Details</summary>
Motivation: 闪电带来重大风险且受气候变化加剧，现有预测模型存在诸多局限，需要早期准确的闪电预测方法。

Method: 采用双编码器架构利用多源气象数据，运用多分支卷积技术捕捉空间相关性，使用新型Hazy Loss函数处理时空不确定性。

Result: DeepLight使公平威胁评分（ETS）比现有方法提高18%-30%。

Conclusion: DeepLight是一种用于闪电预测的可靠解决方案。

Abstract: Lightning, a common feature of severe meteorological conditions, poses
significant risks, from direct human injuries to substantial economic losses.
These risks are further exacerbated by climate change. Early and accurate
prediction of lightning would enable preventive measures to safeguard people,
protect property, and minimize economic losses. In this paper, we present
DeepLight, a novel deep learning architecture for predicting lightning
occurrences. Existing prediction models face several critical limitations: they
often struggle to capture the dynamic spatial context and inherent uncertainty
of lightning events, underutilize key observational data, such as radar
reflectivity and cloud properties, and rely heavily on Numerical Weather
Prediction (NWP) systems, which are both computationally expensive and highly
sensitive to parameter settings. To overcome these challenges, DeepLight
leverages multi-source meteorological data, including radar reflectivity, cloud
properties, and historical lightning occurrences through a dual-encoder
architecture. By employing multi-branch convolution techniques, it dynamically
captures spatial correlations across varying extents. Furthermore, its novel
Hazy Loss function explicitly addresses the spatio-temporal uncertainty of
lightning by penalizing deviations based on proximity to true events, enabling
the model to better learn patterns amidst randomness. Extensive experiments
show that DeepLight improves the Equitable Threat Score (ETS) by 18%-30% over
state-of-the-art methods, establishing it as a robust solution for lightning
prediction.

</details>


### [229] [Unsupervised operator learning approach for dissipative equations via Onsager principle](https://arxiv.org/abs/2508.07440)
*Zhipeng Chang,Zhenye Wen,Xiaofei Zhao*

Main category: cs.LG

TL;DR: 提出无监督的深度Onsager算子学习（DOOL）方法解耗散方程，验证有效性并展示性能优势，还进行了扩展。


<details>
  <summary>Details</summary>
Motivation: 现有算子学习方法依赖高保真模拟数据的监督训练，计算成本高。

Method: 基于Onsager变分原理，直接最小化Rayleighian泛函训练深度算子网络，采用时空解耦策略。

Result: 数值实验验证了DOOL方法的有效性，与监督的DeepONet和MIONet相比性能更优。

Conclusion: DOOL方法是解决耗散方程的有效无监督框架，还可扩展到二阶耗散波模型。

Abstract: Existing operator learning methods rely on supervised training with
high-fidelity simulation data, introducing significant computational cost. In
this work, we propose the deep Onsager operator learning (DOOL) method, a novel
unsupervised framework for solving dissipative equations. Rooted in the Onsager
variational principle (OVP), DOOL trains a deep operator network by directly
minimizing the OVP-defined Rayleighian functional, requiring no labeled data,
and then proceeds in time explicitly through conservation/change laws for the
solution. Another key innovation here lies in the spatiotemporal decoupling
strategy: the operator's trunk network processes spatial coordinates
exclusively, thereby enhancing training efficiency, while integrated external
time stepping enables temporal extrapolation. Numerical experiments on typical
dissipative equations validate the effectiveness of the DOOL method, and
systematic comparisons with supervised DeepONet and MIONet demonstrate its
enhanced performance. Extensions are made to cover the second-order wave models
with dissipation that do not directly follow OVP.

</details>


### [230] [Stackelberg Coupling of Online Representation Learning and Reinforcement Learning](https://arxiv.org/abs/2508.07452)
*Fernando Martinez,Tao Li,Yingdong Lu,Juntao Chen*

Main category: cs.LG

TL;DR: 本文提出SCORER框架，通过博弈论动态构建感知与控制网络交互，提升深度强化学习性能，无需复杂辅助目标或架构。


<details>
  <summary>Details</summary>
Motivation: 解决从稀疏奖励信号学习有效特征的挑战，避免现有方法增加设计复杂度的问题。

Method: 引入Stackelberg Coupled Representation and Reinforcement Learning (SCORER)框架，将感知与控制的交互建模为Stackelberg博弈，用双时间尺度算法近似博弈均衡。

Result: 应用于标准DQN变体在基准任务上，提高了样本效率和最终性能。

Conclusion: 通过对感知 - 控制动态进行有原则的算法设计可实现性能提升，无需复杂辅助目标或架构。

Abstract: Integrated, end-to-end learning of representations and policies remains a
cornerstone of deep reinforcement learning (RL). However, to address the
challenge of learning effective features from a sparse reward signal, recent
trends have shifted towards adding complex auxiliary objectives or fully
decoupling the two processes, often at the cost of increased design complexity.
This work proposes an alternative to both decoupling and naive end-to-end
learning, arguing that performance can be significantly improved by structuring
the interaction between distinct perception and control networks with a
principled, game-theoretic dynamic. We formalize this dynamic by introducing
the Stackelberg Coupled Representation and Reinforcement Learning (SCORER)
framework, which models the interaction between perception and control as a
Stackelberg game. The perception network (leader) strategically learns features
to benefit the control network (follower), whose own objective is to minimize
its Bellman error. We approximate the game's equilibrium with a practical
two-timescale algorithm. Applied to standard DQN variants on benchmark tasks,
SCORER improves sample efficiency and final performance. Our results show that
performance gains can be achieved through principled algorithmic design of the
perception-control dynamic, without requiring complex auxiliary objectives or
architectures.

</details>


### [231] [Towards Unveiling Predictive Uncertainty Vulnerabilities in the Context of the Right to Be Forgotten](https://arxiv.org/abs/2508.07458)
*Wei Qian,Chenxu Zhao,Yangyi Li,Wenqian Ye,Mengdi Huai*

Main category: cs.LG

TL;DR: 本文首次提出针对预测不确定性的恶意遗忘攻击，设计优化框架并实验，结果显示该攻击比传统攻击更有效且现有防御无效。


<details>
  <summary>Details</summary>
Motivation: 现有研究未探索预测不确定性在恶意遗忘攻击下的漏洞，本文旨在填补此空白。

Method: 提出针对预测不确定性的新型恶意遗忘攻击，设计攻击的优化框架，并进行包括黑盒场景的广泛实验。

Result: 攻击在操纵预测不确定性方面比专注标签错误分类的传统攻击更有效，现有针对传统攻击的防御对该攻击无效。

Conclusion: 新型恶意遗忘攻击对预测不确定性有更有效的操纵能力，现有防御手段不足。

Abstract: Currently, various uncertainty quantification methods have been proposed to
provide certainty and probability estimates for deep learning models' label
predictions. Meanwhile, with the growing demand for the right to be forgotten,
machine unlearning has been extensively studied as a means to remove the impact
of requested sensitive data from a pre-trained model without retraining the
model from scratch. However, the vulnerabilities of such generated predictive
uncertainties with regard to dedicated malicious unlearning attacks remain
unexplored. To bridge this gap, for the first time, we propose a new class of
malicious unlearning attacks against predictive uncertainties, where the
adversary aims to cause the desired manipulations of specific predictive
uncertainty results. We also design novel optimization frameworks for our
attacks and conduct extensive experiments, including black-box scenarios.
Notably, our extensive experiments show that our attacks are more effective in
manipulating predictive uncertainties than traditional attacks that focus on
label misclassifications, and existing defenses against conventional attacks
are ineffective against our attacks.

</details>


### [232] [Physics-Informed Multimodal Bearing Fault Classification under Variable Operating Conditions using Transfer Learning](https://arxiv.org/abs/2508.07536)
*Tasfiq E. Alam,Md Manjurul Ahsan,Shivakumar Raman*

Main category: cs.LG

TL;DR: 本文提出物理信息多模态CNN用于轴承故障分类，结合振动和电流信号及物理特征提取分支，用新损失函数，实验显示其性能优于无物理信息基线，评估三种迁移学习策略，LAS泛化性最佳，验证了跨数据集适用性和显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 在可变工况下，领域偏移会降低模型性能，需要准确且可解释的轴承故障分类方法确保旋转机械可靠性。

Method: 提出带后期融合架构的物理信息多模态CNN，结合振动和电流信号及物理特征提取分支，使用基于特征频率的物理信息损失函数，评估三种迁移学习策略。

Result: 物理信息方法优于无物理信息基线，LAS泛化性最佳，结合物理信息建模有额外性能提升，跨数据集验证可达98%准确率，统计检验显示性能显著提升。

Conclusion: 所提框架展示了结合领域知识和数据驱动学习用于现实工业应用中实现鲁棒、可解释和可泛化故障诊断的潜力。

Abstract: Accurate and interpretable bearing fault classification is critical for
ensuring the reliability of rotating machinery, particularly under variable
operating conditions where domain shifts can significantly degrade model
performance. This study proposes a physics-informed multimodal convolutional
neural network (CNN) with a late fusion architecture, integrating vibration and
motor current signals alongside a dedicated physics-based feature extraction
branch. The model incorporates a novel physics-informed loss function that
penalizes physically implausible predictions based on characteristic bearing
fault frequencies - Ball Pass Frequency Outer (BPFO) and Ball Pass Frequency
Inner (BPFI) - derived from bearing geometry and shaft speed. Comprehensive
experiments on the Paderborn University dataset demonstrate that the proposed
physics-informed approach consistently outperforms a non-physics-informed
baseline, achieving higher accuracy, reduced false classifications, and
improved robustness across multiple data splits. To address performance
degradation under unseen operating conditions, three transfer learning (TL)
strategies - Target-Specific Fine-Tuning (TSFT), Layer-Wise Adaptation Strategy
(LAS), and Hybrid Feature Reuse (HFR) - are evaluated. Results show that LAS
yields the best generalization, with additional performance gains when combined
with physics-informed modeling. Validation on the KAIST bearing dataset
confirms the framework's cross-dataset applicability, achieving up to 98
percent accuracy. Statistical hypothesis testing further verifies significant
improvements (p < 0.01) in classification performance. The proposed framework
demonstrates the potential of integrating domain knowledge with data-driven
learning to achieve robust, interpretable, and generalizable fault diagnosis
for real-world industrial applications.

</details>


### [233] [Multimodal Remote Inference](https://arxiv.org/abs/2508.07555)
*Keyuan Zhang,Yin Sun,Bo Ji*

Main category: cs.LG

TL;DR: 研究多模态远程推理系统中两模态调度问题，提出基于索引的阈值策略并证明其最优性，数值结果显示该策略可大幅降低推理误差。


<details>
  <summary>Details</summary>
Motivation: 在多模态远程推理系统中，因网络资源有限难以及时传输各模态特征，为减少机器学习模型推理误差而研究两模态调度问题。

Method: 提出基于索引的阈值策略，当当前模态的索引函数超过阈值时切换模态，证明该策略在一般非单调非加性的AoI函数和异构传输时间下的最优性。

Result: 数值结果表明，该策略相比轮询和均匀随机策略，推理误差最多降低55%。

Conclusion: 通过优化面向任务的AoI函数可提高远程推理准确性。

Abstract: We consider a remote inference system with multiple modalities, where a
multimodal machine learning (ML) model performs real-time inference using
features collected from remote sensors. As sensor observations may change
dynamically over time, fresh features are critical for inference tasks.
However, timely delivering features from all modalities is often infeasible due
to limited network resources. To this end, we study a two-modality scheduling
problem to minimize the ML model's inference error, which is expressed as a
penalty function of AoI for both modalities. We develop an index-based
threshold policy and prove its optimality. Specifically, the scheduler switches
modalities when the current modality's index function exceeds a threshold. We
show that the two modalities share the same threshold, and both the index
functions and the threshold can be computed efficiently. The optimality of our
policy holds for (i) general AoI functions that are \emph{non-monotonic} and
\emph{non-additive} and (ii) \emph{heterogeneous} transmission times. Numerical
results show that our policy reduces inference error by up to 55% compared to
round-robin and uniform random policies, which are oblivious to the AoI-based
inference error function. Our results shed light on how to improve remote
inference accuracy by optimizing task-oriented AoI functions.

</details>


### [234] [Towards Theoretical Understanding of Transformer Test-Time Computing: Investigation on In-Context Linear Regression](https://arxiv.org/abs/2508.07571)
*Xingwu Chen,Miao Lu,Beining Wu,Difan Zou*

Main category: cs.LG

TL;DR: 文章通过引入随机性和采样，在上下文线性回归框架下分析语言模型推理技术，理论与实证结果显示能为理解推理行为提供新见解。


<details>
  <summary>Details</summary>
Motivation: 缩小实际语言模型推理与理论变压器分析之间的差距。

Method: 在上下文线性回归中，通过噪声注入和二元系数采样模拟语言模型解码，并分析常用推理技术。

Result: 理论框架和分析得到了实证结果的支持。

Conclusion: 理论框架和分析有潜力为理解现实世界语言模型的推理行为提供新见解。

Abstract: Using more test-time computation during language model inference, such as
generating more intermediate thoughts or sampling multiple candidate answers,
has proven effective in significantly improving model performance. This paper
takes an initial step toward bridging the gap between practical language model
inference and theoretical transformer analysis by incorporating randomness and
sampling. We focus on in-context linear regression with continuous/binary
coefficients, where our framework simulates language model decoding through
noise injection and binary coefficient sampling. Through this framework, we
provide detailed analyses of widely adopted inference techniques. Supported by
empirical results, our theoretical framework and analysis demonstrate the
potential for offering new insights into understanding inference behaviors in
real-world language models.

</details>


### [235] [When and how can inexact generative models still sample from the data manifold?](https://arxiv.org/abs/2508.07581)
*Nisha Chandramoorthy,Adriaan de Clercq*

Main category: cs.LG

TL;DR: 本文采用动力系统方法研究生成模型中支持鲁棒性现象，分析概率流扰动，揭示动力机制，给出对齐条件，结果适用于不同模型和分布。


<details>
  <summary>Details</summary>
Motivation: 探究一些动态生成模型中尽管分数函数或漂移向量场存在学习误差，但生成样本仍沿数据分布支持移动而非偏离的支持鲁棒性现象。

Method: 对生成的随机/确定性过程采用动力系统方法，进行概率流的扰动分析，对样本路径和概率流进行有限时间线性扰动分析。

Result: 无穷小学习误差仅使预测密度在数据流形上与目标密度不同；数据流形边界切空间与顶级李雅普诺夫向量对齐导致支持鲁棒性；对齐条件计算高效且能自动准确估计数据流形切丛。

Conclusion: 从随机分析、统计学习和不确定性量化角度补充和扩展了生成模型理论保证的现有工作，结果适用于不同动态生成模型和目标分布。

Abstract: A curious phenomenon observed in some dynamical generative models is the
following: despite learning errors in the score function or the drift vector
field, the generated samples appear to shift \emph{along} the support of the
data distribution but not \emph{away} from it. In this work, we investigate
this phenomenon of \emph{robustness of the support} by taking a dynamical
systems approach on the generating stochastic/deterministic process. Our
perturbation analysis of the probability flow reveals that infinitesimal
learning errors cause the predicted density to be different from the target
density only on the data manifold for a wide class of generative models.
Further, what is the dynamical mechanism that leads to the robustness of the
support? We show that the alignment of the top Lyapunov vectors (most sensitive
infinitesimal perturbation directions) with the tangent spaces along the
boundary of the data manifold leads to robustness and prove a sufficient
condition on the dynamics of the generating process to achieve this alignment.
Moreover, the alignment condition is efficient to compute and, in practice, for
robust generative models, automatically leads to accurate estimates of the
tangent bundle of the data manifold. Using a finite-time linear perturbation
analysis on samples paths as well as probability flows, our work complements
and extends existing works on obtaining theoretical guarantees for generative
models from a stochastic analysis, statistical learning and uncertainty
quantification points of view. Our results apply across different dynamical
generative models, such as conditional flow-matching and score-based generative
models, and for different target distributions that may or may not satisfy the
manifold hypothesis.

</details>


### [236] [Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization](https://arxiv.org/abs/2508.07629)
*Zhenpeng Su,Leiyu Pan,Xue Bai,Dening Liu,Guanting Dong,Jiaming Huang,Wenping Hu,Guorui Zhou*

Main category: cs.LG

TL;DR: 提出Klear - Reasoner模型，分析推理模型训练流程，指出SFT数据特点，提出GPPO解决RL裁剪机制问题，模型在数学和编程基准测试表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前社区推理模型训练细节披露不完整，难以复现高性能推理模型。

Method: 深入分析推理模型从数据准备到长思维链监督微调再到强化学习的训练流程，提出Gradient - Preserving clipping Policy Optimization (GPPO)。

Result: 实验表明少量高质量SFT数据源更有效，困难样本不做精度过滤效果更好；Klear - Reasoner在AIME 2024、AIME 2025、LiveCodeBench V5和V6等基准测试取得高分数。

Conclusion: Klear - Reasoner具有出色的推理能力，GPPO能增强模型探索能力和从负样本学习的效率。

Abstract: We present Klear-Reasoner, a model with long reasoning capabilities that
demonstrates careful deliberation during problem solving, achieving outstanding
performance across multiple benchmarks. Although there are already many
excellent works related to inference models in the current community, there are
still many problems with reproducing high-performance inference models due to
incomplete disclosure of training details. This report provides an in-depth
analysis of the reasoning model, covering the entire post-training workflow
from data preparation and long Chain-of-Thought supervised fine-tuning (long
CoT SFT) to reinforcement learning (RL), along with detailed ablation studies
for each experimental component. For SFT data, our experiments show that a
small number of high-quality data sources are more effective than a large
number of diverse data sources, and that difficult samples can achieve better
results without accuracy filtering. In addition, we investigate two key issues
with current clipping mechanisms in RL: Clipping suppresses critical
exploration signals and ignores suboptimal trajectories. To address these
challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO)
that gently backpropagates gradients from clipped tokens. GPPO not only
enhances the model's exploration capacity but also improves its efficiency in
learning from negative samples. Klear-Reasoner exhibits exceptional reasoning
abilities in mathematics and programming, scoring 90.5\% on AIME 2024, 83.2\%
on AIME 2025, 66.0\% on LiveCodeBench V5 and 58.1\% on LiveCodeBench V6.

</details>


### [237] [Efficient Approximate Posterior Sampling with Annealed Langevin Monte Carlo](https://arxiv.org/abs/2508.07631)
*Advait Parulekar,Litu Rout,Karthikeyan Shanmugam,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: 研究基于分数的生成模型中后验采样问题，提出将其视为分布倾斜问题，在最小假设下可进行多项式时间近似后验采样。


<details>
  <summary>Details</summary>
Motivation: 已有工作表明在KL散度下精确后验采样难处理，但相关算法有实证成功，要解决后验采样问题。

Method: 将后验采样视为更一般的分布“倾斜”问题。

Result: 在最小假设下，可从一个分布中进行采样，该分布与加噪先验的后验在KL散度上接近，与真实后验在Fisher散度上接近。

Conclusion: 得到了多项式时间（近似）后验采样的首个正式结果。

Abstract: We study the problem of posterior sampling in the context of score based
generative models. We have a trained score network for a prior $p(x)$, a
measurement model $p(y|x)$, and are tasked with sampling from the posterior
$p(x|y)$. Prior work has shown this to be intractable in KL (in the worst case)
under well-accepted computational hardness assumptions. Despite this, popular
algorithms for tasks such as image super-resolution, stylization, and
reconstruction enjoy empirical success. Rather than establishing distributional
assumptions or restricted settings under which exact posterior sampling is
tractable, we view this as a more general "tilting" problem of biasing a
distribution towards a measurement. Under minimal assumptions, we show that one
can tractably sample from a distribution that is simultaneously close to the
posterior of a noised prior in KL divergence and the true posterior in Fisher
divergence. Intuitively, this combination ensures that the resulting sample is
consistent with both the measurement and the prior. To the best of our
knowledge these are the first formal results for (approximate) posterior
sampling in polynomial time.

</details>


### [238] [Attribution Explanations for Deep Neural Networks: A Theoretical Perspective](https://arxiv.org/abs/2508.07636)
*Huiqi Deng,Hongbin Pei,Quanshi Zhang,Mengnan Du*

Main category: cs.LG

TL;DR: 文章指出归因解释DNNs的方法存在忠实性问题，源于三个核心挑战，总结近期理论进展三个方向并给出见解，最后讨论待研究问题。


<details>
  <summary>Details</summary>
Motivation: 解决归因方法是否及哪些能忠实反映输入变量对决策过程实际贡献的问题，提高归因解释可靠性和实用性。

Method: 总结近期理论进展，从理论统一、理论基础、理论评估三个关键方向进行梳理。

Result: 理清了归因方法的相关理论进展，能帮助加深理论理解、指导方法选择和启发新方法。

Conclusion: 提出了有前景的待研究开放问题。

Abstract: Attribution explanation is a typical approach for explaining deep neural
networks (DNNs), inferring an importance or contribution score for each input
variable to the final output. In recent years, numerous attribution methods
have been developed to explain DNNs. However, a persistent concern remains
unresolved, i.e., whether and which attribution methods faithfully reflect the
actual contribution of input variables to the decision-making process. The
faithfulness issue undermines the reliability and practical utility of
attribution explanations. We argue that these concerns stem from three core
challenges. First, difficulties arise in comparing attribution methods due to
their unstructured heterogeneity, differences in heuristics, formulations, and
implementations that lack a unified organization. Second, most methods lack
solid theoretical underpinnings, with their rationales remaining absent,
ambiguous, or unverified. Third, empirically evaluating faithfulness is
challenging without ground truth. Recent theoretical advances provide a
promising way to tackle these challenges, attracting increasing attention. We
summarize these developments, with emphasis on three key directions: (i)
Theoretical unification, which uncovers commonalities and differences among
methods, enabling systematic comparisons; (ii) Theoretical rationale,
clarifying the foundations of existing methods; (iii) Theoretical evaluation,
rigorously proving whether methods satisfy faithfulness principles. Beyond a
comprehensive review, we provide insights into how these studies help deepen
theoretical understanding, inform method selection, and inspire new attribution
methods. We conclude with a discussion of promising open problems for further
work.

</details>


### [239] [Extracting Complex Topology from Multivariate Functional Approximation: Contours, Jacobi Sets, and Ridge-Valley Graphs](https://arxiv.org/abs/2508.07637)
*Guanqun Ma,David Lenz,Hanqi Guo,Tom Peterka,Bei Wang*

Main category: cs.LG

TL;DR: 本文介绍首个从多元函数逼近（MFA）这种连续隐式模型中直接提取复杂拓扑特征的框架，该工作可推广，为隐式连续模型的拓扑数据分析和可视化奠定基础。


<details>
  <summary>Details</summary>
Motivation: 隐式连续模型为科学数据的存储、传输和分析提供新视角，需要一种从连续隐式模型中直接提取复杂拓扑特征的方法。

Method: 引入框架，以MFA模型为输入，直接从模型中提取复杂拓扑特征，不转换为离散表示。

Result: 实现了从MFA模型中直接提取复杂拓扑特征，且该方法可推广到支持函数值和高阶导数查询的任何连续隐式模型。

Conclusion: 为隐式连续模型的拓扑数据分析和可视化建立了基础。

Abstract: Implicit continuous models, such as functional models and implicit neural
networks, are an increasingly popular method for replacing discrete data
representations with continuous, high-order, and differentiable surrogates.
These models offer new perspectives on the storage, transfer, and analysis of
scientific data. In this paper, we introduce the first framework to directly
extract complex topological features -- contours, Jacobi sets, and ridge-valley
graphs -- from a type of continuous implicit model known as multivariate
functional approximation (MFA). MFA replaces discrete data with continuous
piecewise smooth functions. Given an MFA model as the input, our approach
enables direct extraction of complex topological features from the model,
without reverting to a discrete representation of the model. Our work is easily
generalizable to any continuous implicit model that supports the queries of
function values and high-order derivatives. Our work establishes the building
blocks for performing topological data analysis and visualization on implicit
continuous models.

</details>


### [240] [Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals](https://arxiv.org/abs/2508.07638)
*Jia Zhang,Yao Liu,Chen-Xi Zhang,Yi Liu,Yi-Xuan Jin,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.LG

TL;DR: 论文从数据角度解决大语言模型价值对齐中多偏好数据优化问题，提出PD选择方法，实验显示有超10%相对提升并提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型价值对齐方法在处理细粒度、特定方面偏好数据时存在噪声和冲突问题，需新方法解决。

Method: 推导Direct Multi - Preference Optimization (DMPO)目标，发现Preference Divergence (PD)项，基于此提出数据选择原则，还介绍PD项估计和长度偏差缓解方法。

Result: 在UltraFeedback数据集上，相对标准整体偏好和使用聚合偏好信号的更强基准有超10%相对提升，提高训练效率，无需整体偏好标注。

Conclusion: 提出的PD选择方法简单有效，能通过细粒度偏好信号实现大语言模型的鲁棒对齐。

Abstract: Aligning Large Language Models (LLMs) with diverse human values requires
moving beyond a single holistic "better-than" preference criterion. While
collecting fine-grained, aspect-specific preference data is more reliable and
scalable, existing methods like Direct Preference Optimization (DPO) struggle
with the severe noise and conflicts inherent in such aggregated datasets. In
this paper, we tackle this challenge from a data-centric perspective. We first
derive the Direct Multi-Preference Optimization (DMPO) objective, and uncover a
key Preference Divergence (PD) term that quantifies inter-aspect preference
conflicts. Instead of using this term for direct optimization, we leverage it
to formulate a novel, theoretically-grounded data selection principle. Our
principle advocates for selecting a subset of high-consensus data-identified by
the most negative PD values-for efficient DPO training. We prove the optimality
of this strategy by analyzing the loss bounds of the DMPO objective in the
selection problem. To operationalize our approach, we introduce practical
methods of PD term estimation and length bias mitigation, thereby proposing our
PD selection method. Evaluation on the UltraFeedback dataset with three varying
conflict levels shows that our simple yet effective strategy achieves over 10%
relative improvement against both the standard holistic preference and a
stronger oracle using aggregated preference signals, all while boosting
training efficiency and obviating the need for intractable holistic preference
annotating, unlocking the potential of robust LLM alignment via fine-grained
preference signals.

</details>


### [241] [Multi-Turn Jailbreaks Are Simpler Than They Seem](https://arxiv.org/abs/2508.07646)
*Xiaoxue Yang,Jaeha Lee,Anna-Katharina Dick,Jasper Timm,Fei Xie,Diogo Cruz*

Main category: cs.LG

TL;DR: 对大语言模型多轮越狱攻击进行实证分析，发现多轮攻击近似多次重采样单轮攻击，相似模型攻击成功率相关，推理模型推理努力与攻击成功率正相关。


<details>
  <summary>Details</summary>
Motivation: 尽管单轮越狱攻击防御有提升，但多轮越狱仍是大语言模型的持久漏洞，需进行研究。

Method: 使用StrongREJECT基准对GPT - 4、Claude和Gemini等模型进行自动化多轮越狱攻击实证分析。

Result: 多轮越狱攻击约等于多次重采样单轮攻击；相似模型攻击成功率相关；推理模型推理努力越高攻击成功率越高。

Conclusion: 研究结果对AI安全评估和抗越狱系统设计有重要意义，已发布源代码。

Abstract: While defenses against single-turn jailbreak attacks on Large Language Models
(LLMs) have improved significantly, multi-turn jailbreaks remain a persistent
vulnerability, often achieving success rates exceeding 70% against models
optimized for single-turn protection. This work presents an empirical analysis
of automated multi-turn jailbreak attacks across state-of-the-art models
including GPT-4, Claude, and Gemini variants, using the StrongREJECT benchmark.
Our findings challenge the perceived sophistication of multi-turn attacks: when
accounting for the attacker's ability to learn from how models refuse harmful
requests, multi-turn jailbreaking approaches are approximately equivalent to
simply resampling single-turn attacks multiple times. Moreover, attack success
is correlated among similar models, making it easier to jailbreak newly
released ones. Additionally, for reasoning models, we find surprisingly that
higher reasoning effort often leads to higher attack success rates. Our results
have important implications for AI safety evaluation and the design of
jailbreak-resistant systems. We release the source code at
https://github.com/diogo-cruz/multi_turn_simpler

</details>


### [242] [Discovering Spatial Correlations between Earth Observations in Global Atmospheric State Estimation by using Adaptive Graph Structure Learning](https://arxiv.org/abs/2508.07659)
*Hyeon-Ju Jeon,Jeon-Ho Kang,In-Hyuk Kwon,O-Joun Lee*

Main category: cs.LG

TL;DR: 为提高全球大气状态估计预测精度，采用带结构学习的时空图神经网络处理复杂时空相关性，通过调节边采样解决问题，用东亚数据验证效果佳。


<details>
  <summary>Details</summary>
Motivation: 发现地球观测与大气状态的空间相关性，提高全球大气状态估计的预测精度。

Method: 采用带结构学习的时空图神经网络（STGNNs），通过自适应确定节点度和考虑网格点与观测点的空间距离来调节边采样。

Result: 使用东亚真实大气状态和观测数据验证，在大气变化大的区域，该方法优于现有带或不带结构学习的STGNN模型。

Conclusion: 提出的方法能有效处理大气状态和观测之间复杂的动态空间相关性，提高预测精度。

Abstract: This study aims to discover spatial correlations between Earth observations
and atmospheric states to improve the forecasting accuracy of global
atmospheric state estimation, which are usually conducted using conventional
numerical weather prediction (NWP) systems and is the beginning of weather
forecasting. NWP systems predict future atmospheric states at fixed locations,
which are called NWP grid points, by analyzing previous atmospheric states and
newly acquired Earth observations without fixed locations. Thus, surrounding
meteorological context and the changing locations of the observations make
spatial correlations between atmospheric states and observations over time. To
handle complicated spatial correlations, which change dynamically, we employ
spatiotemporal graph neural networks (STGNNs) with structure learning. However,
structure learning has an inherent limitation that this can cause structural
information loss and over-smoothing problem by generating excessive edges. To
solve this problem, we regulate edge sampling by adaptively determining node
degrees and considering the spatial distances between NWP grid points and
observations. We validated the effectiveness of the proposed method by using
real-world atmospheric state and observation data from East Asia. Even in areas
with high atmospheric variability, the proposed method outperformed existing
STGNN models with and without structure learning.

</details>


### [243] [GLiClass: Generalist Lightweight Model for Sequence Classification Tasks](https://arxiv.org/abs/2508.07662)
*Ihor Stepanov,Mykhailo Shtopko,Dmytro Vodianytskyi,Oleksandr Lukashov,Alexander Yavorskyi,Mykyta Yaroshenko*

Main category: cs.LG

TL;DR: 提出GLiClass用于序列分类，结合PPO用于多标签文本分类，兼顾效率、准确性与灵活性。


<details>
  <summary>Details</summary>
Motivation: 当前AI分类任务需高效准确且有零样本能力，现有方法如生成式LLMs、交叉编码器、基于嵌入的方法存在不足。

Method: 将GLiNER架构用于序列分类任务，采用近端策略优化（PPO）用于多标签文本分类。

Result: GLiClass在效率和准确性上与基于嵌入的方法相当，能适应零样本和少样本学习场景。

Conclusion: GLiClass方法在分类任务中具有良好表现，PPO可在数据稀疏或有人工反馈情况下训练分类器。

Abstract: Classification is one of the most widespread tasks in AI applications,
serving often as the first step in filtering, sorting, and categorizing data.
Since modern AI systems must handle large volumes of input data and early
pipeline stages can propagate errors downstream, achieving high efficiency and
accuracy is critical. Moreover, classification requirements can change
dynamically based on user needs, necessitating models with strong zero-shot
capabilities. While generative LLMs have become mainstream for zero-shot
classification due to their versatility, they suffer from inconsistent
instruction following and computational inefficiency. Cross-encoders, commonly
used as rerankers in RAG pipelines, face a different bottleneck: they must
process text-label pairs sequentially, significantly reducing efficiency with
large label sets. Embedding-based approaches offer good efficiency but struggle
with complex scenarios involving logical and semantic constraints. We propose
GLiClass, a novel method that adapts the GLiNER architecture for sequence
classification tasks. Our approach achieves strong accuracy and efficiency
comparable to embedding-based methods, while maintaining the flexibility needed
for zero-shot and few-shot learning scenarios. Additionally, we adapted
proximal policy optimization (PPO) for multi-label text classification,
enabling training classifiers in data-sparse conditions or from human feedback.

</details>


### [244] [AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting](https://arxiv.org/abs/2508.07668)
*Hyobin Park,Jinwook Jung,Minseok Seo,Hyunsoo Choi,Deukjae Cho,Sekil Park,Dong-Geol Choi*

Main category: cs.LG

TL;DR: 提出AIS-LLM框架整合AIS数据与大语言模型，能同时执行三项关键任务，实验证明有效，有智能高效管理潜力。


<details>
  <summary>Details</summary>
Motivation: 现有方法单独处理海事交通分析任务，难以全面考虑复杂海事情况。

Method: 提出AIS - LLM框架，包含时间序列编码器、基于大语言模型的提示编码器、跨模态对齐模块和多任务解码器。

Result: AIS - LLM在各任务上优于现有方法。

Conclusion: AIS - LLM有效，在海事交通管理上有智能高效的潜力。

Abstract: With the increase in maritime traffic and the mandatory implementation of the
Automatic Identification System (AIS), the importance and diversity of maritime
traffic analysis tasks based on AIS data, such as vessel trajectory prediction,
anomaly detection, and collision risk assessment, is rapidly growing. However,
existing approaches tend to address these tasks individually, making it
difficult to holistically consider complex maritime situations. To address this
limitation, we propose a novel framework, AIS-LLM, which integrates time-series
AIS data with a large language model (LLM). AIS-LLM consists of a Time-Series
Encoder for processing AIS sequences, an LLM-based Prompt Encoder, a
Cross-Modality Alignment Module for semantic alignment between time-series data
and textual prompts, and an LLM-based Multi-Task Decoder. This architecture
enables the simultaneous execution of three key tasks: trajectory prediction,
anomaly detection, and risk assessment of vessel collisions within a single
end-to-end system. Experimental results demonstrate that AIS-LLM outperforms
existing methods across individual tasks, validating its effectiveness.
Furthermore, by integratively analyzing task outputs to generate situation
summaries and briefings, AIS-LLM presents the potential for more intelligent
and efficient maritime traffic management.

</details>


### [245] [Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation](https://arxiv.org/abs/2508.07675)
*Xutong Liu,Baran Atalar,Xiangxiang Dai,Jinhang Zuo,Siwei Wang,John C. S. Lui,Wei Chen,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 大语言模型推理成本高，缓存是解决方案，但传统精确匹配缓存有问题，语义缓存有新问题且现有方法缺乏理论基础。本文提出基于学习的语义缓存淘汰框架，有离线优化和在线学习变体，算法高效，在合成数据集上表现好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理成本高，传统精确匹配缓存忽视查询语义相似性，语义缓存有新的缓存淘汰问题，现有语义缓存方法缺乏理论基础且无法适应现实不确定性。

Method: 提出基于学习的语义缓存淘汰框架，制定问题的离线优化和在线学习变体，开发有理论保证的高效算法。

Result: 在合成数据集上，所提出的算法与基线相比表现相当或更优。

Conclusion: 所提出的基于学习的语义缓存淘汰框架有效，其算法在未知查询和成本分布下有良好性能。

Abstract: Large Language Models (LLMs) are revolutionizing how users interact with
information systems, yet their high inference cost poses serious scalability
and sustainability challenges. Caching inference responses, allowing them to be
retrieved without another forward pass through the LLM, has emerged as one
possible solution. Traditional exact-match caching, however, overlooks the
semantic similarity between queries, leading to unnecessary recomputation.
Semantic caching addresses this by retrieving responses based on semantic
similarity, but introduces a fundamentally different cache eviction problem:
one must account for mismatch costs between incoming queries and cached
responses. Moreover, key system parameters, such as query arrival probabilities
and serving costs, are often unknown and must be learned over time. Existing
semantic caching methods are largely ad-hoc, lacking theoretical foundations
and unable to adapt to real-world uncertainty. In this paper, we present a
principled, learning-based framework for semantic cache eviction under unknown
query and cost distributions. We formulate both offline optimization and online
learning variants of the problem, and develop provably efficient algorithms
with state-of-the-art guarantees. We also evaluate our framework on a synthetic
dataset, showing that our proposed algorithms perform matching or superior
performance compared with baselines.

</details>


### [246] [MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation](https://arxiv.org/abs/2508.07681)
*Yooseok Lim,ByoungJun Jeon,Seong-A Park,Jisoo Lee,Sae Won Choi,Chang Wook Jeong,Ho-Geol Ryu,Hongyeol Lee,Hyun-Lim Yang*

Main category: cs.LG

TL;DR: 提出MORE - CLEAR框架用于重症监护室脓毒症控制，利用预训练大语言模型提取临床笔记语义信息，融合多模态数据，实验显示该框架优于单模态强化学习方法，或可加速脓毒症治疗管理。


<details>
  <summary>Details</summary>
Motivation: 以往脓毒症管理的强化学习方法主要依赖结构化数据，缺乏对患者状况的全面理解，需要更好的方法进行早期检测和优化管理。

Method: 提出MORE - CLEAR框架，使用预训练大语言模型提取临床笔记语义表示，通过门控融合和跨模态注意力动态调整权重，有效集成多模态数据。

Result: 使用两个公共数据集和一个私有数据集的广泛交叉验证表明，MORE - CLEAR显著提高了估计生存率和策略性能，优于单模态强化学习方法。

Conclusion: 这是首次在多模态离线强化学习中利用大语言模型能力以实现更好的医疗应用状态表示，该方法可基于对患者状况的更全面理解，使强化学习模型提出更优行动，有望加速脓毒症的治疗和管理。

Abstract: Sepsis, a life-threatening inflammatory response to infection, causes organ
dysfunction, making early detection and optimal management critical. Previous
reinforcement learning (RL) approaches to sepsis management rely primarily on
structured data, such as lab results or vital signs, and on a dearth of a
comprehensive understanding of the patient's condition. In this work, we
propose a Multimodal Offline REinforcement learning for Clinical notes
Leveraged Enhanced stAte Representation (MORE-CLEAR) framework for sepsis
control in intensive care units. MORE-CLEAR employs pre-trained large-scale
language models (LLMs) to facilitate the extraction of rich semantic
representations from clinical notes, preserving clinical context and improving
patient state representation. Gated fusion and cross-modal attention allow
dynamic weight adjustment in the context of time and the effective integration
of multimodal data. Extensive cross-validation using two public (MIMIC-III and
MIMIC-IV) and one private dataset demonstrates that MORE-CLEAR significantly
improves estimated survival rate and policy performance compared to
single-modal RL approaches. To our knowledge, this is the first to leverage LLM
capabilities within a multimodal offline RL for better state representation in
medical applications. This approach can potentially expedite the treatment and
management of sepsis by enabling reinforcement learning models to propose
enhanced actions based on a more comprehensive understanding of patient
conditions.

</details>


### [247] [Energy Consumption in Parallel Neural Network Training](https://arxiv.org/abs/2508.07706)
*Philipp Huber,David Li,Juan Pedro Gutiérrez Hermosillo Muriedas,Deifilia Kieckhefen,Markus Götz,Achim Streit,Charlotte Debus*

Main category: cs.LG

TL;DR: 研究数据并行训练中并行化参数对预测性能、训练时间和能耗的影响，揭示能耗与资源消耗关系及影响因素。


<details>
  <summary>Details</summary>
Motivation: 训练神经网络计算资源需求增加致能耗增长，并行化对能耗的影响常被忽视，填补此研究空白。

Method: 对ResNet50和FourCastNet进行数据并行训练的缩放实验，评估并行化参数影响。

Result: 能耗与消耗资源（GPU小时）近似线性相关，但不同模型训练和硬件的缩放因子不同，受每GPU小时样本数和梯度更新数影响。

Conclusion: 研究结果揭示神经网络训练扩展的复杂相互作用，为可持续AI研究提供参考。

Abstract: The increasing demand for computational resources of training neural networks
leads to a concerning growth in energy consumption. While parallelization has
enabled upscaling model and dataset sizes and accelerated training, its impact
on energy consumption is often overlooked. To close this research gap, we
conducted scaling experiments for data-parallel training of two models,
ResNet50 and FourCastNet, and evaluated the impact of parallelization
parameters, i.e., GPU count, global batch size, and local batch size, on
predictive performance, training time, and energy consumption. We show that
energy consumption scales approximately linearly with the consumed resources,
i.e., GPU hours; however, the respective scaling factor differs substantially
between distinct model trainings and hardware, and is systematically influenced
by the number of samples and gradient updates per GPU hour. Our results shed
light on the complex interplay of scaling up neural network training and can
inform future developments towards more sustainable AI research.

</details>


### [248] [Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer](https://arxiv.org/abs/2508.07710)
*Jingya Wang,Xin Deng,Wenjie Wei,Dehao Zhang,Shuai Wang,Qian Sun,Jieyuan Zhang,Hanwen Liu,Ning Xie,Malu Zhang*

Main category: cs.LG

TL;DR: 提出适用于Transformer架构的无训练ANN - SNN转换框架，实验证明其转换精度高、延迟低。


<details>
  <summary>Details</summary>
Motivation: 现有ANN - SNN转换方法在处理Transformer架构非线性操作有局限，且需预训练ANN微调。

Method: 提出Multi - basis Exponential Decay (MBE)神经元，用指数衰减策略和多基编码方法近似非线性操作，无需修改预训练ANN权重。

Result: 在多种任务和主流Transformer架构实验中，实现近无损转换精度，显著降低延迟。

Conclusion: 为脉冲Transformer在现实应用中的高效可扩展部署提供了有前景的途径。

Abstract: Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a
promising approach for constructing energy-efficient Transformer architectures.
Compared to directly trained Spiking Transformers, ANN-to-SNN conversion
methods bypass the high training costs. However, existing methods still suffer
from notable limitations, failing to effectively handle nonlinear operations in
Transformer architectures and requiring additional fine-tuning processes for
pre-trained ANNs. To address these issues, we propose a high-performance and
training-free ANN-to-SNN conversion framework tailored for Transformer
architectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE)
neuron, which employs an exponential decay strategy and multi-basis encoding
method to efficiently approximate various nonlinear operations. It removes the
requirement for weight modifications in pre-trained ANNs. Extensive experiments
across diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures
(ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless
conversion accuracy with significantly lower latency. This provides a promising
pathway for the efficient and scalable deployment of Spiking Transformers in
real-world applications.

</details>


### [249] [Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations](https://arxiv.org/abs/2508.07722)
*Pietro Talli,Federico Mason,Federico Chiariotti,Andrea Zanella*

Main category: cs.LG

TL;DR: 本文提出HR3L架构解决通信网络中强化学习代理训练问题，实验显示其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决在有损耗或延迟的无线系统中，强化学习代理因接收状态更新不完整而无法即时感知状态演变，以及现有处理不完美反馈的框架计算负担大的问题。

Method: 提出Homomorphic Robust Remote Reinforcement Learning (HR3L) 架构，包含编码环境信息的发射机和解码信息并执行动作的接收机，且无需在无线信道交换梯度信息。

Result: HR3L在样本效率上显著优于基线方法，能适应不同通信场景。

Conclusion: HR3L架构能有效解决通信网络中强化学习代理训练问题，具有更快的训练速度和更低的通信开销。

Abstract: In this work, we address the problem of training Reinforcement Learning (RL)
agents over communication networks. The RL paradigm requires the agent to
instantaneously perceive the state evolution to infer the effects of its
actions on the environment. This is impossible if the agent receives state
updates over lossy or delayed wireless systems and thus operates with partial
and intermittent information. In recent years, numerous frameworks have been
proposed to manage RL with imperfect feedback; however, they often offer
specific solutions with a substantial computational burden. To address these
limits, we propose a novel architecture, named Homomorphic Robust Remote
Reinforcement Learning (HR3L), that enables the training of remote RL agents
exchanging observations across a non-ideal wireless channel. HR3L considers two
units: the transmitter, which encodes meaningful representations of the
environment, and the receiver, which decodes these messages and performs
actions to maximize a reward signal. Importantly, HR3L does not require the
exchange of gradient information across the wireless channel, allowing for
quicker training and a lower communication overhead than state-of-the-art
solutions. Experimental results demonstrate that HR3L significantly outperforms
baseline methods in terms of sample efficiency and adapts to different
communication scenarios, including packet losses, delayed transmissions, and
capacity limitations.

</details>


### [250] [Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning](https://arxiv.org/abs/2508.07738)
*Jialu Zhou,Dianxi Shi,Shaowu Yang,Xinyu Wei,Mingyue Yang,Leqian Li,Mengzhu Wang,Chunping Qiu*

Main category: cs.LG

TL;DR: 提出TRGE方法解决MDCL中遗忘问题，实验显示其效果佳且可训练参数少。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法用于MDCL时存在灾难性遗忘和前向遗忘问题。

Method: 提出TRGE方法，动态扩展预训练CLIP模型，设计组内和组间路由策略；用MLLMs生成语义任务描述获取任务标识符；动态融合冻结CLIP模型和TRGE适配器输出。

Result: 在不同设置的大量实验中，该方法以较少可训练参数超越其他先进方法。

Conclusion: 提出的TRGE方法能有效解决MDCL中的遗忘问题。

Abstract: Multi-Domain Continual Learning (MDCL) acquires knowledge from sequential
tasks with shifting class sets and distribution. Despite the
Parameter-Efficient Fine-Tuning (PEFT) methods can adapt for this dual
heterogeneity, they still suffer from catastrophic forgetting and forward
forgetting. To address these challenges, we propose a Two-Level Routing Grouped
Mixture-of-Experts (TRGE) method. Firstly, TRGE dynamically expands the
pre-trained CLIP model, assigning specific expert group for each task to
mitigate catastrophic forgetting. With the number of experts continually grows
in this process, TRGE maintains the static experts count within the group and
introduces the intra-group router to alleviate routing overfitting caused by
the increasing routing complexity. Meanwhile, we design an inter-group routing
policy based on task identifiers and task prototype distance, which dynamically
selects relevant expert groups and combines their outputs to enhance inter-task
collaboration. Secondly, to get the correct task identifiers, we leverage
Multimodal Large Language Models (MLLMs) which own powerful multimodal
comprehension capabilities to generate semantic task descriptions and recognize
the correct task identifier. Finally, to mitigate forward forgetting, we
dynamically fuse outputs for unseen samples from the frozen CLIP model and TRGE
adapter based on training progress, leveraging both pre-trained and learned
knowledge. Through extensive experiments across various settings, our method
outperforms other advanced methods with fewer trainable parameters.

</details>


### [251] [Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment](https://arxiv.org/abs/2508.07750)
*Haowen Wang,Yun Yue,Zhiling Ye,Shuowen Zhang,Lei Fan,Jiaxin Liang,Jiadi Jiang,Cheng Wei,Jingyuan Deng,Xudong Han,Ji Li,Chunxiao Guo,Peng Wei,Jian Wang,Jinjie Gu*

Main category: cs.LG

TL;DR: 提出GRAO框架结合SFT和RL优势，有理论收敛保证，在人类对齐任务中表现优于多个基线。


<details>
  <summary>Details</summary>
Motivation: SFT受离线策略轨迹限制，RL样本效率低且依赖高质量基础模型，需解决两者问题。

Method: 提出GRAO框架，有三个创新点：多样本生成策略、组直接对齐损失公式、参考感知参数更新。

Result: 理论分析表明GRAO有收敛保证和样本效率优势，综合评估中比SFT、DPO、PPO和GRPO基线分别有57.70%、17.65%、7.95%和5.18%的相对提升。

Conclusion: 该工作提供了理论可靠的对齐框架和语言模型能力高效进化的实证证据。

Abstract: Alignment methodologies have emerged as a critical pathway for enhancing
language model alignment capabilities. While SFT (supervised fine-tuning)
accelerates convergence through direct token-level loss intervention, its
efficacy is constrained by offline policy trajectory. In contrast,
RL(reinforcement learning) facilitates exploratory policy optimization, but
suffers from low sample efficiency and stringent dependency on high-quality
base models. To address these dual challenges, we propose GRAO (Group Relative
Alignment Optimization), a unified framework that synergizes the respective
strengths of SFT and RL through three key innovations: 1) A multi-sample
generation strategy enabling comparative quality assessment via reward
feedback; 2) A novel Group Direct Alignment Loss formulation leveraging
intra-group relative advantage weighting; 3) Reference-aware parameter updates
guided by pairwise preference dynamics. Our theoretical analysis establishes
GRAO's convergence guarantees and sample efficiency advantages over
conventional approaches. Comprehensive evaluations across complex human
alignment tasks demonstrate GRAO's superior performance, achieving
57.70\%,17.65\% 7.95\% and 5.18\% relative improvements over SFT, DPO, PPO and
GRPO baselines respectively. This work provides both a theoretically grounded
alignment framework and empirical evidence for efficient capability evolution
in language models.

</details>


### [252] [Sparse Probabilistic Graph Circuits](https://arxiv.org/abs/2508.07763)
*Martin Rektoris,Milan Papež,Václav Šmídl,Tomáš Pevný*

Main category: cs.LG

TL;DR: 提出Sparse PGCs处理图生成模型可扩展性问题，在药物设计中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有图深度生成模型难以进行概率推理，Probabilistic Graph Circuits虽可解决但复杂度高，需解决可扩展性问题。

Method: 引入Sparse PGCs，直接在稀疏图表示上操作，降低复杂度。

Result: 在药物设计中，SPGCs保留精确推理能力，提高内存效率和推理速度，关键指标表现与难处理的深度生成模型相当。

Conclusion: Sparse PGCs能有效解决图生成模型的可扩展性问题。

Abstract: Deep generative models (DGMs) for graphs achieve impressively high expressive
power thanks to very efficient and scalable neural networks. However, these
networks contain non-linearities that prevent analytical computation of many
standard probabilistic inference queries, i.e., these DGMs are considered
\emph{intractable}. While recently proposed Probabilistic Graph Circuits (PGCs)
address this issue by enabling \emph{tractable} probabilistic inference, they
operate on dense graph representations with $\mathcal{O}(n^2)$ complexity for
graphs with $n$ nodes and \emph{$m$ edges}. To address this scalability issue,
we introduce Sparse PGCs, a new class of tractable generative models that
operate directly on sparse graph representation, reducing the complexity to
$\mathcal{O}(n + m)$, which is particularly beneficial for $m \ll n^2$. In the
context of de novo drug design, we empirically demonstrate that SPGCs retain
exact inference capabilities, improve memory efficiency and inference speed,
and match the performance of intractable DGMs in key metrics.

</details>


### [253] [Pareto Multi-Objective Alignment for Language Models](https://arxiv.org/abs/2508.07768)
*Qiang He,Setareh Maghsudi*

Main category: cs.LG

TL;DR: 现有大语言模型对齐方法基于单一奖励函数，缺乏多目标对齐能力。本文提出PAMA算法，将多目标RLHF转化为凸优化问题，降低复杂度且有理论保证，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 当前基于RLHF的对齐方法优化大语言模型单一奖励函数，导致行为僵化，无法适应实际场景的多目标需求，多目标对齐是关键但未充分探索的领域。

Method: 提出Pareto Multi-Objective Alignment (PAMA)算法，将多目标RLHF转化为有闭式解的凸优化问题。

Result: PAMA将复杂度从O(n^2*d)降至O(n)，能在毫秒内完成优化，理论上收敛到Pareto平稳点，在125M到7B参数的语言模型实验中展现了强大有效的多目标对齐能力。

Conclusion: PAMA为多目标对齐问题提供了高效解决方案，为大语言模型与人类多样价值观对齐提供了实用且有理论依据的方法，有助于AI在现实世界的灵活部署。

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications that require careful balancing of multiple, often conflicting,
objectives, such as informativeness versus conciseness, or helpfulness versus
creativity. However, current alignment methods, primarily based on RLHF,
optimize LLMs toward a single reward function, resulting in rigid behavior that
fails to capture the complexity and diversity of human preferences. This
limitation hinders the adaptability of LLMs to practical scenarios, making
multi-objective alignment (MOA) a critical yet underexplored area. To bridge
this gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and
computationally efficient algorithm designed explicitly for MOA in LLMs. In
contrast to computationally prohibitive multi-objective optimization (MOO)
methods, PAMA transforms multi-objective RLHF into a convex optimization with a
closed-form solution, significantly enhancing scalability. Traditional MOO
approaches suffer from prohibitive O(n^2*d) complexity, where d represents the
number of model parameters, typically in the billions for LLMs, rendering
direct optimization infeasible. PAMA reduces this complexity to O(n) where n is
the number of objectives, enabling optimization to be completed within
milliseconds. We provide theoretical guarantees that PAMA converges to a Pareto
stationary point, where no objective can be improved without degrading at least
one other. Extensive experiments across language models ranging from 125M to 7B
parameters demonstrate PAMA's robust and effective MOA capabilities, aligning
with its theoretical advantages. PAMA provides a highly efficient solution to
the MOA problem that was previously considered intractable, offering a
practical and theoretically grounded approach to aligning LLMs with diverse
human values, paving the way for versatile and adaptable real-world AI
deployments.

</details>


### [254] [Topological Feature Compression for Molecular Graph Neural Networks](https://arxiv.org/abs/2508.07807)
*Rahul Khorana*

Main category: cs.LG

TL;DR: 提出结合压缩高阶拓扑信号与标准分子特征的新型GNN架构，在多基准测试中表现优异并开源代码


<details>
  <summary>Details</summary>
Motivation: 现有分子表示学习在提取化学见解时难以平衡预测准确性、可解释性和计算效率

Method: 引入结合压缩高阶拓扑信号与标准分子特征的新型GNN架构

Result: 在从小分子数据集到复杂材料数据集等一系列基准测试中，使用参数高效的架构展示了卓越性能，在几乎所有基准测试中都取得了最佳的准确性和鲁棒性结果

Conclusion: 该新型GNN架构有效，能在多数据集上表现良好

Abstract: Recent advances in molecular representation learning have produced highly
effective encodings of molecules for numerous cheminformatics and
bioinformatics tasks. However, extracting general chemical insight while
balancing predictive accuracy, interpretability, and computational efficiency
remains a major challenge. In this work, we introduce a novel Graph Neural
Network (GNN) architecture that combines compressed higher-order topological
signals with standard molecular features. Our approach captures global
geometric information while preserving computational tractability and
human-interpretable structure. We evaluate our model across a range of
benchmarks, from small-molecule datasets to complex material datasets, and
demonstrate superior performance using a parameter-efficient architecture. We
achieve the best performing results in both accuracy and robustness across
almost all benchmarks. We open source all code \footnote{All code and results
can be found on Github https://github.com/rahulkhorana/TFC-PACT-Net}.

</details>


### [255] [EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning](https://arxiv.org/abs/2508.07809)
*Huanyu Liu,Jia Li,Chang Yu,Taozhi Chen,Yihong Dong,Lecheng Wang,Hu XiaoLong,Ge Li*

Main category: cs.LG

TL;DR: 提出EvoCoT框架解决RLVR在硬问题上奖励稀疏的问题，实验表明其能提升大模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR在硬问题上奖励稀疏，限制学习效率和探索，现有方法存在局限性。

Method: 提出基于两阶段思维链推理优化的自进化课程学习框架EvoCoT，通过自生成和验证思维链轨迹约束探索空间并逐步扩展。

Result: EvoCoT使大模型解决之前无法解决的问题，在无外部思维链监督下提升推理能力，兼容多种强化学习微调方法。

Conclusion: EvoCoT有效，已开源代码支持后续研究。

Abstract: Reinforcement learning with verifiable reward (RLVR) has become a promising
paradigm for post-training large language models (LLMs) to improve their
reasoning capability. However, when the rollout accuracy is low on hard
problems, the reward becomes sparse, limiting learning efficiency and causing
exploration bottlenecks. Existing approaches either rely on stronger LLMs for
distillation or filter out difficult problems, which limits scalability or
restricts reasoning improvement through exploration.
  We propose EvoCoT, a self-evolving curriculum learning framework based on
two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the
exploration space by self-generating and verifying CoT trajectories, then
gradually shortens them to expand the space in a controlled way. This enables
LLMs to stably learn from initially unsolved hard problems under sparse
rewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,
and Llama. Experiments show that EvoCoT enables LLMs to solve previously
unsolved problems, improves reasoning capability without external CoT
supervision, and is compatible with various RL fine-tuning methods. We release
the source code to support future research.

</details>


### [256] [Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow](https://arxiv.org/abs/2508.07841)
*Carlo Cena,Mauro Martini,Marcello Chiaberge*

Main category: cs.LG

TL;DR: 本文探讨将物理信息神经网络（PINNs）用于航天器姿态动力学学习，对比纯数据驱动方法，结果表明PINNs能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统模型预测控制依赖精确物理模型，纯数据驱动模型泛化和稳定性差，为解决这些局限，研究PINNs在航天器姿态动力学学习中的应用。

Method: 使用带自注意力机制的Real NVP神经网络架构，基于Basilisk模拟器生成的模拟数据训练模型，采用纯数据驱动和物理信息两种训练策略。

Result: 包含物理信息的模型使最佳架构的平均相对误差显著降低27.08%；集成到MPC框架中，PINN模型在控制精度、鲁棒性和抗噪性上优于纯数据驱动模型，性能稳定性误差最多改善42.86%。

Conclusion: 将物理信息融入神经网络可显著提升航天器姿态动力学学习模型的性能和稳定性。

Abstract: Attitude control is a fundamental aspect of spacecraft operations. Model
Predictive Control (MPC) has emerged as a powerful strategy for these tasks,
relying on accurate models of the system dynamics to optimize control actions
over a prediction horizon. In scenarios where physics models are incomplete,
difficult to derive, or computationally expensive, machine learning offers a
flexible alternative by learning the system behavior directly from data.
However, purely data-driven models often struggle with generalization and
stability, especially when applied to inputs outside their training domain. To
address these limitations, we investigate the benefits of incorporating
Physics-Informed Neural Networks (PINNs) into the learning of spacecraft
attitude dynamics, comparing their performance with that of purely data-driven
approaches. Using a Real-valued Non-Volume Preserving (Real NVP) neural network
architecture with a self-attention mechanism, we trained several models on
simulated data generated with the Basilisk simulator. Two training strategies
were considered: a purely data-driven baseline and a physics-informed variant
to improve robustness and stability. Our results demonstrate that the inclusion
of physics-based information significantly enhances the performance in terms of
the mean relative error of the best architectures found by 27.08%. These
advantages are particularly evident when the learned models are integrated into
an MPC framework, where PINN-based models consistently outperform their purely
data-driven counterparts in terms of control accuracy and robustness, yielding
improvements of up to 42.86% in performance stability error and increased
robustness-to-noise.

</details>


### [257] [Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant](https://arxiv.org/abs/2508.07887)
*Sabrina Namazova,Alessandra Brondetta,Younes Strittmatter,Matthew Nassar,Sebastian Musslick*

Main category: cs.LG

TL;DR: 本文回顾参与者模拟器的核心标准，评估Centaur是否达标，指出其虽有强预测准确性，但生成行为与人类数据有偏差，还未达可靠参与者模拟器标准。


<details>
  <summary>Details</summary>
Motivation: 可靠的参与者模拟器对行为科学有变革性意义，评估Centaur能否成为可靠的参与者模拟器。

Method: 回顾参与者模拟器的核心标准，以此评估Centaur。

Result: Centaur有较强预测准确性，但生成行为与人类数据系统地偏离。

Conclusion: Centaur是预测人类行为的重要一步，但未达到可靠参与者模拟器或准确认知模型的标准。

Abstract: Simulators have revolutionized scientific practice across the natural
sciences. By generating data that reliably approximate real-world phenomena,
they enable scientists to accelerate hypothesis testing and optimize
experimental designs. This is perhaps best illustrated by AlphaFold, a
Nobel-prize winning simulator in chemistry that predicts protein structures
from amino acid sequences, enabling rapid prototyping of molecular
interactions, drug targets, and protein functions. In the behavioral sciences,
a reliable participant simulator - a system capable of producing human-like
behavior across cognitive tasks - would represent a similarly transformative
advance. Recently, Binz et al. introduced Centaur, a large language model (LLM)
fine-tuned on human data from 160 experiments, proposing its use not only as a
model of cognition but also as a participant simulator for "in silico
prototyping of experimental studies", e.g., to advance automated cognitive
science. Here, we review the core criteria for a participant simulator and
assess how well Centaur meets them. Although Centaur demonstrates strong
predictive accuracy, its generative behavior - a critical criterion for a
participant simulator - systematically diverges from human data. This suggests
that, while Centaur is a significant step toward predicting human behavior, it
does not yet meet the standards of a reliable participant simulator or an
accurate model of cognition.

</details>


### [258] [Score Augmentation for Diffusion Models](https://arxiv.org/abs/2508.07926)
*Liang Hou,Yuan Gao,Boyuan Jiang,Xin Tao,Qi Yan,Renjie Liao,Pengfei Wan,Di Zhang,Kun Gai*

Main category: cs.LG

TL;DR: 研究确认扩散模型训练存在过拟合问题，提出ScoreAug数据增强框架，在多基准测试中表现出色，能缓解过拟合等。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在数据有限时训练的过拟合问题。

Method: 提出ScoreAug框架，对噪声数据进行变换，要求去噪器预测原始目标的增强，理论分析不同空间分数关系。

Result: 在多个基准测试中性能显著提升，有效缓解过拟合，收敛稳定，能避免数据泄漏，可与传统方法结合获更多增益。

Conclusion: ScoreAug是有效的扩散模型数据增强方法，可缓解过拟合，有良好性能和优势。

Abstract: Diffusion models have achieved remarkable success in generative modeling.
However, this study confirms the existence of overfitting in diffusion model
training, particularly in data-limited regimes. To address this challenge, we
propose Score Augmentation (ScoreAug), a novel data augmentation framework
specifically designed for diffusion models. Unlike conventional augmentation
approaches that operate on clean data, ScoreAug applies transformations to
noisy data, aligning with the inherent denoising mechanism of diffusion.
Crucially, ScoreAug further requires the denoiser to predict the augmentation
of the original target. This design establishes an equivariant learning
objective, enabling the denoiser to learn scores across varied denoising
spaces, thereby realizing what we term score augmentation. We also
theoretically analyze the relationship between scores in different spaces under
general transformations. In experiments, we extensively validate ScoreAug on
multiple benchmarks including CIFAR-10, FFHQ, AFHQv2, and ImageNet, with
results demonstrating significant performance improvements over baselines.
Notably, ScoreAug effectively mitigates overfitting across diverse scenarios,
such as varying data scales and model capacities, while exhibiting stable
convergence properties. Another advantage of ScoreAug over standard data
augmentation lies in its ability to circumvent data leakage issues under
certain conditions. Furthermore, we show that ScoreAug can be synergistically
combined with traditional data augmentation techniques to achieve additional
performance gains.

</details>


### [259] [Adaptive Fine-Tuning via Pattern Specialization for Deep Time Series Forecasting](https://arxiv.org/abs/2508.07927)
*Amal Saadallah,Abdulaziz Al-Ademi*

Main category: cs.LG

TL;DR: 提出利用模型自适应和选择提升DNN在时间序列预测性能的框架，有概念漂移检测机制，在多种架构上表现佳。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳环境下时间序列预测中潜在模式随时间演变的挑战。

Method: 先离线训练基础DNN，对验证子集聚类划分不同模式区域，为各区域微调基础DNN得到专门版本；推理时根据输入与聚类中心匹配选择微调模型；集成概念漂移检测机制。

Result: 该框架可推广到各种DNN架构，在传统和先进架构上都有显著性能提升。

Conclusion: 所提框架有效提升了DNN在非平稳环境下时间序列预测的性能。

Abstract: Time series forecasting poses significant challenges in non-stationary
environments where underlying patterns evolve over time. In this work, we
propose a novel framework that enhances deep neural network (DNN) performance
by leveraging specialized model adaptation and selection. Initially, a base DNN
is trained offline on historical time series data. A reserved validation subset
is then segmented to extract and cluster the most dominant patterns within the
series, thereby identifying distinct regimes. For each identified cluster, the
base DNN is fine-tuned to produce a specialized version that captures unique
pattern characteristics. At inference, the most recent input is matched against
the cluster centroids, and the corresponding fine-tuned version is deployed
based on the closest similarity measure. Additionally, our approach integrates
a concept drift detection mechanism to identify and adapt to emerging patterns
caused by non-stationary behavior. The proposed framework is generalizable
across various DNN architectures and has demonstrated significant performance
gains on both traditional DNNs and recent advanced architectures implemented in
the GluonTS library.

</details>


### [260] [Shapley-Inspired Feature Weighting in $k$-means with No Additional Hyperparameters](https://arxiv.org/abs/2508.07952)
*Richard J. Fawley,Renato Cordeiro de Amorim*

Main category: cs.LG

TL;DR: 提出SHARK特征加权聚类算法，用Shapley值量化特征相关性，无需额外参数，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有聚类算法假设所有特征对数据结构贡献相同，在高维或有噪声场景不适用，且多数特征加权方法需额外调参。

Method: 提出SHARK算法，证明k - means目标可分解为特征Shapley值之和，迭代地用特征Shapley贡献的倒数重新加权特征。

Result: 在合成和真实数据集上实验，SHARK始终与现有方法相当或表现更优，在有噪声场景鲁棒性和准确性更高。

Conclusion: SHARK是一种有效的特征加权聚类算法，在多种场景有良好表现。

Abstract: Clustering algorithms often assume all features contribute equally to the
data structure, an assumption that usually fails in high-dimensional or noisy
settings. Feature weighting methods can address this, but most require
additional parameter tuning. We propose SHARK (Shapley Reweighted $k$-means), a
feature-weighted clustering algorithm motivated by the use of Shapley values
from cooperative game theory to quantify feature relevance, which requires no
additional parameters beyond those in $k$-means. We prove that the $k$-means
objective can be decomposed into a sum of per-feature Shapley values, providing
an axiomatic foundation for unsupervised feature relevance and reducing Shapley
computation from exponential to polynomial time. SHARK iteratively re-weights
features by the inverse of their Shapley contribution, emphasising informative
dimensions and down-weighting irrelevant ones. Experiments on synthetic and
real-world data sets show that SHARK consistently matches or outperforms
existing methods, achieving superior robustness and accuracy, particularly in
scenarios where noise may be present. Software:
https://github.com/rickfawley/shark.

</details>


### [261] [WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer](https://arxiv.org/abs/2508.07970)
*Junyu Wu,Weiming Chang,Xiaotao Liu,Guanyou He,Tingfeng Xian,Haoqiang Hong,Boqi Chen,Haotao Tian,Tao Yang,Yunsheng Shi,Feng Lin,Ting Yao*

Main category: cs.LG

TL;DR: 提出WeChat - YATT框架解决现有RLHF训练框架在复杂多模态工作流和动态工作负载方面的挑战，实验显示吞吐量提升且已用于微信产品模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF训练框架在扩展到复杂多模态工作流和适应动态工作负载方面存在挑战，如控制器可扩展性和管道编排效率问题。

Method: 引入WeChat - YATT框架，采用并行控制器编程模型编排复杂工作流，提出动态放置方案分配计算资源和调度工作负载。

Result: 实验表明WeChat - YATT相比现有框架吞吐量大幅提升，且已成功用于支持微信产品特征的模型训练。

Conclusion: WeChat - YATT框架有效解决现有问题，在实际应用中具备有效性和鲁棒性。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent
paradigm for training large language models and multimodal systems. Despite
notable advances enabled by existing RLHF training frameworks, significant
challenges remain in scaling to complex multimodal workflows and adapting to
dynamic workloads. In particular, current systems often encounter limitations
related to controller scalability when managing large models, as well as
inefficiencies in orchestrating intricate RLHF pipelines, especially in
scenarios that require dynamic sampling and resource allocation. In this paper,
we introduce WeChat-YATT (Yet Another Transformer Trainer in WeChat), a simple,
scalable, and balanced RLHF training framework specifically designed to address
these challenges. WeChat-YATT features a parallel controller programming model
that enables flexible and efficient orchestration of complex RLHF workflows,
effectively mitigating the bottlenecks associated with centralized controller
architectures and facilitating scalability in large-scale data scenarios. In
addition, we propose a dynamic placement schema that adaptively partitions
computational resources and schedules workloads, thereby significantly reducing
hardware idle time and improving GPU utilization under variable training
conditions. We evaluate WeChat-YATT across a range of experimental scenarios,
demonstrating that it achieves substantial improvements in throughput compared
to state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been
successfully deployed to train models supporting WeChat product features for a
large-scale user base, underscoring its effectiveness and robustness in
real-world applications.

</details>


### [262] [A Physics-informed Deep Operator for Real-Time Freeway Traffic State Estimation](https://arxiv.org/abs/2508.08002)
*Hongxin Yu,Yibing Wang,Fengyue Jin,Meng Zhang,Anni Chen*

Main category: cs.LG

TL;DR: 本文提出用物理信息深度算子网络（PI - DeepONet）研究实时高速公路交通状态估计，开发扩展架构，评估显示新方法优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有交通状态估计方法有局限，需更准确方法，故引入PI - DeepONet研究实时高速公路交通状态估计。

Method: 提出基于PI - DeepONet研究实时高速公路交通状态估计，开发扩展架构，包含接受二维数据输入、引入非线性扩展层等，构建交通状态估计器。

Result: 用NGSIM短高速公路路段和中国大型城市快速路评估，新方法在流量和平均速度估计上高精度，优于其他四种基线方法。

Conclusion: 基于扩展PI - DeepONet架构的交通状态估计方法能实现高精度交通状态估计，性能优于基线方法。

Abstract: Traffic state estimation (TSE) falls methodologically into three categories:
model-driven, data-driven, and model-data dual-driven. Model-driven TSE relies
on macroscopic traffic flow models originated from hydrodynamics. Data-driven
TSE leverages historical sensing data and employs statistical models or machine
learning methods to infer traffic state. Model-data dual-driven traffic state
estimation attempts to harness the strengths of both aspects to achieve more
accurate TSE. From the perspective of mathematical operator theory, TSE can be
viewed as a type of operator that maps available measurements of inerested
traffic state into unmeasured traffic state variables in real time. For the
first time this paper proposes to study real-time freeway TSE in the idea of
physics-informed deep operator network (PI-DeepONet), which is an
operator-oriented architecture embedding traffic flow models based on deep
neural networks. The paper has developed an extended architecture from the
original PI-DeepONet. The extended architecture is featured with: (1) the
acceptance of 2-D data input so as to support CNN-based computations; (2) the
introduction of a nonlinear expansion layer, an attention mechanism, and a MIMO
mechanism; (3) dedicated neural network design for adaptive identification of
traffic flow model parameters. A traffic state estimator built on the basis of
this extended PI-DeepONet architecture was evaluated with respect to a short
freeway stretch of NGSIM and a large-scale urban expressway in China, along
with other four baseline TSE methods. The evaluation results demonstrated that
this novel TSE method outperformed the baseline methods with high-precision
estimation results of flow and mean speed.

</details>


### [263] [Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP](https://arxiv.org/abs/2508.08005)
*Xiang Li,Shanshan Wang,Chenglong Xiao*

Main category: cs.LG

TL;DR: 论文指出最大团问题（MCP）缺少算法选择研究，提出基于学习的框架，评估传统分类器，开发GAT - MLP模型，证明双通道架构和图神经网络在组合算法选择中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究中缺乏针对MCP的算法选择研究，需找到合适的算法选择方法。

Method: 构建标注数据集，评估支持向量机（SVM）、随机森林（RF）等传统分类器，分析特征重要性，开发结合图注意力网络（GAT）和多层感知机（MLP）的GAT - MLP模型。

Result: RF在各指标和数据集变体中表现稳定，GAT - MLP模型在所有指标上表现出色。

Conclusion: 双通道架构有效，图神经网络在组合算法选择中有前景。

Abstract: Extensive experiments and prior studies show that no single maximum clique
algorithm consistently performs best across all instances, highlighting the
importance of selecting suitable algorithms based on instance features. Through
an extensive analysis of relevant studies, it is found that there is a lack of
research work concerning algorithm selection oriented toward the Maximum Clique
Problem (MCP). In this work, we propose a learning-based framework that
integrates both traditional machine learning and graph neural networks to
address this gap. We construct a labeled dataset by running four exact MCP
algorithms on a diverse collection of graph instances, accompanied by
structural and global statistical features extracted from each graph. We first
evaluate four conventional classifiers: Support Vector Machine (SVM), Random
Forest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multiple
dataset variants. Experimental results show that RF consistently shows strong
performance across metrics and dataset variants, making it a reliable baseline.
In addition, feature importance analysis indicates that connectivity and
topological structure are strong predictors of algorithm performance. Building
on these findings, we develop a dual-channel model named GAT-MLP, which
combines a Graph Attention Network (GAT) for local structural encoding with a
Multilayer Perceptron (MLP) for global feature modeling. The GAT-MLP model
shows strong and consistent performance across all metrics. Our results
highlight the effectiveness of dual-channel architectures and the promise of
graph neural networks in combinatorial algorithm selection.

</details>


### [264] [Communication-Efficient Zero-Order and First-Order Federated Learning Methods over Wireless Networks](https://arxiv.org/abs/2508.08013)
*Mohamad Assaad,Zeinab Nehme,Merouane Debbah*

Main category: cs.LG

TL;DR: 论文提出两种通信高效的联邦学习方法，通过传输标量值和允许多用户同时发送信息降低通信开销，给出分析框架和收敛保证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在训练阶段设备与聚合器间信息交换量大，超出无线系统容量，需降低通信开销。

Method: 第一种采用带两点梯度估计器的零阶优化技术，第二种采用一阶梯度计算策略，利用信道信息，考虑异步设备。

Result: 为两种方法提供严格分析框架，推导出收敛保证并建立合适性能边界。

Conclusion: 所提出的两种通信高效的联邦学习方法能有效降低通信开销，且有理论上的收敛保证和性能边界。

Abstract: Federated Learning (FL) is an emerging learning framework that enables edge
devices to collaboratively train ML models without sharing their local data. FL
faces, however, a significant challenge due to the high amount of information
that must be exchanged between the devices and the aggregator in the training
phase, which can exceed the limited capacity of wireless systems. In this
paper, two communication-efficient FL methods are considered where
communication overhead is reduced by communicating scalar values instead of
long vectors and by allowing high number of users to send information
simultaneously. The first approach employs a zero-order optimization technique
with two-point gradient estimator, while the second involves a first-order
gradient computation strategy. The novelty lies in leveraging channel
information in the learning algorithms, eliminating hence the need for
additional resources to acquire channel state information (CSI) and to remove
its impact, as well as in considering asynchronous devices. We provide a
rigorous analytical framework for the two methods, deriving convergence
guarantees and establishing appropriate performance bounds.

</details>


### [265] [Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles](https://arxiv.org/abs/2508.08034)
*Roksana Yahyaabadi,Ghazal Farhani,Taufiq Rahman,Soodeh Nikan,Abdullah Jirjees,Fadi Araji*

Main category: cs.LG

TL;DR: 本文提出数据驱动方法预测车辆功耗，在不同车型平台验证有效，分析了数据集不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统功耗预测方法不适合大规模实际部署，需准确、可扩展的功耗预测方法。

Method: 使用动力系统动态特征集，结合传统机器学习和深度神经网络，对ICE、EV和HEV平台进行瞬时和累积功耗估计。

Result: ICE模型瞬时精度高，累积误差低；Transformer和LSTM模型对EV和HEV效果佳，累积误差分别低于4.1%和2.1%；EV和HEV数据集不确定性大。

Conclusion: 该方法在不同车辆和模型中有效，先进动力系统需鲁棒模型。

Abstract: Accurate power consumption prediction is crucial for improving efficiency and
reducing environmental impact, yet traditional methods relying on specialized
instruments or rigid physical models are impractical for large-scale,
real-world deployment. This study introduces a scalable data-driven method
using powertrain dynamic feature sets and both traditional machine learning and
deep neural networks to estimate instantaneous and cumulative power consumption
in internal combustion engine (ICE), electric vehicle (EV), and hybrid electric
vehicle (HEV) platforms. ICE models achieved high instantaneous accuracy with
mean absolute error and root mean squared error on the order of $10^{-3}$, and
cumulative errors under 3%. Transformer and long short-term memory models
performed best for EVs and HEVs, with cumulative errors below 4.1% and 2.1%,
respectively. Results confirm the approach's effectiveness across vehicles and
models. Uncertainty analysis revealed greater variability in EV and HEV
datasets than ICE, due to complex power management, emphasizing the need for
robust models for advanced powertrains.

</details>


### [266] [BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models](https://arxiv.org/abs/2508.08040)
*Maozhen Zhang,Mengnan Zhao,Bo Wang*

Main category: cs.LG

TL;DR: 本文提出针对多模态对比模型中基于提示的联邦学习的首个后门攻击BadPromptFL，通过实验验证其有效性等，引发对相关学习鲁棒性的关注。


<details>
  <summary>Details</summary>
Motivation: 基于提示的聚合在联邦多模态学习中的安全影响未被充分研究，存在攻击面未解决。

Method: 受损客户端联合优化本地后门触发器和提示嵌入，将中毒提示注入全局聚合过程，利用CLIP风格架构的上下文学习行为。

Result: BadPromptFL在多个数据集和聚合协议上实现高攻击成功率（>90%），具有低可见性和低客户端参与度。

Conclusion: 实验验证了攻击的有效性、隐蔽性和通用性，引发对现实部署中基于提示的联邦学习鲁棒性的担忧。

Abstract: Prompt-based tuning has emerged as a lightweight alternative to full
fine-tuning in large vision-language models, enabling efficient adaptation via
learned contextual prompts. This paradigm has recently been extended to
federated learning settings (e.g., PromptFL), where clients collaboratively
train prompts under data privacy constraints. However, the security
implications of prompt-based aggregation in federated multimodal learning
remain largely unexplored, leaving a critical attack surface unaddressed. In
this paper, we introduce \textbf{BadPromptFL}, the first backdoor attack
targeting prompt-based federated learning in multimodal contrastive models. In
BadPromptFL, compromised clients jointly optimize local backdoor triggers and
prompt embeddings, injecting poisoned prompts into the global aggregation
process. These prompts are then propagated to benign clients, enabling
universal backdoor activation at inference without modifying model parameters.
Leveraging the contextual learning behavior of CLIP-style architectures,
BadPromptFL achieves high attack success rates (e.g., \(>90\%\)) with minimal
visibility and limited client participation. Extensive experiments across
multiple datasets and aggregation protocols validate the effectiveness,
stealth, and generalizability of our attack, raising critical concerns about
the robustness of prompt-based federated learning in real-world deployments.

</details>


### [267] [On Understanding of the Dynamics of Model Capacity in Continual Learning](https://arxiv.org/abs/2508.08052)
*Supriyo Chakraborty,Krishnan Raghavan*

Main category: cs.LG

TL;DR: 本文引入CL的有效模型容量CLEMC，建模NN、任务数据和优化过程的相互作用，证明有效容量和稳定 - 可塑性平衡点非平稳，实验支持理论发现。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中稳定性 - 可塑性困境这一基本挑战。

Method: 引入CLEMC，开发差分方程建模相互作用，通过实验验证理论。

Result: 证明有效容量和稳定 - 可塑性平衡点非平稳，当新任务分布与旧任务不同时，NN表示新任务的能力降低。

Conclusion: 研究结果适用于多种架构的神经网络，实验支持理论发现。

Abstract: The stability-plasticity dilemma, closely related to a neural network's (NN)
capacity-its ability to represent tasks-is a fundamental challenge in continual
learning (CL). Within this context, we introduce CL's effective model capacity
(CLEMC) that characterizes the dynamic behavior of the stability-plasticity
balance point. We develop a difference equation to model the evolution of the
interplay between the NN, task data, and optimization procedure. We then
leverage CLEMC to demonstrate that the effective capacity-and, by extension,
the stability-plasticity balance point is inherently non-stationary. We show
that regardless of the NN architecture or optimization method, a NN's ability
to represent new tasks diminishes when incoming task distributions differ from
previous ones. We conduct extensive experiments to support our theoretical
findings, spanning a range of architectures-from small feedforward network and
convolutional networks to medium-sized graph neural networks and
transformer-based large language models with millions of parameters.

</details>


### [268] [C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction](https://arxiv.org/abs/2508.08071)
*Yunqing Li,Zixiang Tang,Jiaying Zhuang,Zhenyu Yang,Farhad Ameri,Jianbang Zhang*

Main category: cs.LG

TL;DR: 提出PMGraph基准和Cascade Multimodal Attributed Graph (C - MAG)架构解决供应链连接问题


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉制造商复杂信息，需更好方法连接产品与制造商和供应商以构建高效供应链

Method: 引入PMGraph基准，提出C - MAG架构，先聚合文本和视觉属性到中间嵌入，再通过多尺度消息传递提升链接预测准确性

Result: 未提及具体结果

Conclusion: C - MAG为模态感知融合提供实用指南，能在现实噪声环境中保持预测性能

Abstract: Connecting an ever-expanding catalogue of products with suitable
manufacturers and suppliers is critical for resilient, efficient global supply
chains, yet traditional methods struggle to capture complex capabilities,
certifications, geographic constraints, and rich multimodal data of real-world
manufacturer profiles. To address these gaps, we introduce PMGraph, a public
benchmark of bipartite and heterogeneous multimodal supply-chain graphs linking
8,888 manufacturers, over 70k products, more than 110k manufacturer-product
edges, and over 29k product images. Building on this benchmark, we propose the
Cascade Multimodal Attributed Graph C-MAG, a two-stage architecture that first
aligns and aggregates textual and visual attributes into intermediate group
embeddings, then propagates them through a manufacturer-product hetero-graph
via multiscale message passing to enhance link prediction accuracy. C-MAG also
provides practical guidelines for modality-aware fusion, preserving predictive
performance in noisy, real-world settings.

</details>


### [269] [ELF: Efficient Logic Synthesis by Pruning Redundancy in Refactoring](https://arxiv.org/abs/2508.08073)
*Dimitris Tsaras,Xing Li,Lei Chen,Zhiyao Xie,Mingxuan Yuan*

Main category: cs.LG

TL;DR: 本文提出利用分类器预先修剪不成功的切割以加速逻辑优化，实验表明相比现有ABC实现平均可提速3.9倍。


<details>
  <summary>Details</summary>
Motivation: 电子设计自动化中逻辑优化算子计算需求高，传统算子形成迭代切割任务平均98%失败，此前并行化方法效果不佳，需降低计算成本。

Method: 利用分类器预先修剪不成功的切割，消除不必要的重新合成操作。

Result: 在EPFL基准套件和10个大型工业设计上对refactor算子实验，该技术相比现有ABC实现平均可提速3.9倍。

Conclusion: 利用分类器预先修剪切割的方法能有效加速逻辑优化。

Abstract: In electronic design automation, logic optimization operators play a crucial
role in minimizing the gate count of logic circuits. However, their computation
demands are high. Operators such as refactor conventionally form iterative cuts
for each node, striving for a more compact representation - a task which often
fails 98% on average. Prior research has sought to mitigate computational cost
through parallelization. In contrast, our approach leverages a classifier to
prune unsuccessful cuts preemptively, thus eliminating unnecessary resynthesis
operations. Experiments on the refactor operator using the EPFL benchmark suite
and 10 large industrial designs demonstrate that this technique can speedup
logic optimization by 3.9x on average compared with the state-of-the-art ABC
implementation.

</details>


### [270] [Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation](https://arxiv.org/abs/2508.08087)
*Amir Ali Panahi,Daniel Luder,Billy Wu,Gregory Offer,Dirk Uwe Sauer,Weihan Li*

Main category: cs.LG

TL;DR: 本文对比三种算子学习替代模型用于锂离子电池单粒子模型，结果显示PE - FNO在速度、泛化性等方面表现出色，为电化学数字孪生提供实用途径。


<details>
  <summary>Details</summary>
Motivation: 实现具有亚毫秒速度和高物理保真度的可靠锂离子电池数字孪生。

Method: 对三种算子学习替代模型（DeepONets、FNOs和PE - FNO）进行基准测试，在多种电流类型和荷电状态数据上训练模型，用贝叶斯优化进行参数估计。

Result: DeepONet处理动态负载有困难；FNO浓度误差低于1%，电压平均绝对误差低于1.7mV；PE - FNO执行速度比16线程SPM求解器快约200倍，参数估计有一定误差但优于传统方法。

Conclusion: PE - FNO能满足实时电池管理等需求，优于传统神经替代模型，为高速高保真电化学数字孪生提供实用路径。

Abstract: Reliable digital twins of lithium-ion batteries must achieve high physical
fidelity with sub-millisecond speed. In this work, we benchmark three
operator-learning surrogates for the Single Particle Model (SPM): Deep Operator
Networks (DeepONets), Fourier Neural Operators (FNOs) and a newly proposed
parameter-embedded Fourier Neural Operator (PE-FNO), which conditions each
spectral layer on particle radius and solid-phase diffusivity. Models are
trained on simulated trajectories spanning four current families (constant,
triangular, pulse-train, and Gaussian-random-field) and a full range of
State-of-Charge (SOC) (0 % to 100 %). DeepONet accurately replicates
constant-current behaviour but struggles with more dynamic loads. The basic FNO
maintains mesh invariance and keeps concentration errors below 1 %, with
voltage mean-absolute errors under 1.7 mV across all load types. Introducing
parameter embedding marginally increases error, but enables generalisation to
varying radii and diffusivities. PE-FNO executes approximately 200 times faster
than a 16-thread SPM solver. Consequently, PE-FNO's capabilities in inverse
tasks are explored in a parameter estimation task with Bayesian optimisation,
recovering anode and cathode diffusivities with 1.14 % and 8.4 % mean absolute
percentage error, respectively, and 0.5918 percentage points higher error in
comparison with classical methods. These results pave the way for neural
operators to meet the accuracy, speed and parametric flexibility demands of
real-time battery management, design-of-experiments and large-scale inference.
PE-FNO outperforms conventional neural surrogates, offering a practical path
towards high-speed and high-fidelity electrochemical digital twins.

</details>


### [271] [Grid2Guide: A* Enabled Small Language Model for Indoor Navigation](https://arxiv.org/abs/2508.08100)
*Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: 提出Grid2Guide混合导航框架，结合A*算法与小语言模型生成室内导航指令，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂环境中无外部定位信号和专用基础设施时可靠室内导航的难题。

Method: 构建Grid2Guide框架，从室内地图生成二进制占用矩阵，用A*算法计算最优路径，再用小语言模型将路径步骤转化为自然语言指令。

Result: 在多种室内场景实验中，该方法能有效提供准确及时的导航指引。

Conclusion: 该方法是轻量级、无需基础设施的实时室内导航支持解决方案。

Abstract: Reliable indoor navigation remains a significant challenge in complex
environments, particularly where external positioning signals and dedicated
infrastructures are unavailable. This research presents Grid2Guide, a hybrid
navigation framework that combines the A* search algorithm with a Small
Language Model (SLM) to generate clear, human-readable route instructions. The
framework first conducts a binary occupancy matrix from a given indoor map.
Using this matrix, the A* algorithm computes the optimal path between origin
and destination, producing concise textual navigation steps. These steps are
then transformed into natural language instructions by the SLM, enhancing
interpretability for end users. Experimental evaluations across various indoor
scenarios demonstrate the method's effectiveness in producing accurate and
timely navigation guidance. The results validate the proposed approach as a
lightweight, infrastructure-free solution for real-time indoor navigation
support.

</details>


### [272] [Vision-Based Localization and LLM-based Navigation for Indoor Environments](https://arxiv.org/abs/2508.08120)
*Keyan Rahimi,Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: 本文提出结合视觉定位与大语言模型导航的室内定位导航方法，实验表明其有一定准确性和潜力。


<details>
  <summary>Details</summary>
Motivation: 因室内缺乏可靠GPS信号和建筑结构复杂，室内导航面临挑战，需新的定位导航方法。

Method: 定位系统用ResNet - 50卷积神经网络经两阶段微调，利用手机相机输入定位；导航模块用大语言模型，结合系统提示解读预处理的平面图生成导航指令。

Result: 定位模型在各测试点准确率达96%；使用ChatGPT的导航测试指令平均准确率75%，但零样本推理和推理时间有局限。

Conclusion: 该研究展示了利用现成相机和公开平面图实现可扩展、无基础设施依赖的室内导航的潜力，适用于资源受限场景。

Abstract: Indoor navigation remains a complex challenge due to the absence of reliable
GPS signals and the architectural intricacies of large enclosed environments.
This study presents an indoor localization and navigation approach that
integrates vision-based localization with large language model (LLM)-based
navigation. The localization system utilizes a ResNet-50 convolutional neural
network fine-tuned through a two-stage process to identify the user's position
using smartphone camera input. To complement localization, the navigation
module employs an LLM, guided by a carefully crafted system prompt, to
interpret preprocessed floor plan images and generate step-by-step directions.
Experimental evaluation was conducted in a realistic office corridor with
repetitive features and limited visibility to test localization robustness. The
model achieved high confidence and an accuracy of 96% across all tested
waypoints, even under constrained viewing conditions and short-duration
queries. Navigation tests using ChatGPT on real building floor maps yielded an
average instruction accuracy of 75%, with observed limitations in zero-shot
reasoning and inference time. This research demonstrates the potential for
scalable, infrastructure-free indoor navigation using off-the-shelf cameras and
publicly available floor plans, particularly in resource-constrained settings
like hospitals, airports, and educational institutions.

</details>


### [273] [MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing](https://arxiv.org/abs/2508.08122)
*Mingrong Lin,Ke Deng,Zhengyang Wu,Zetao Zheng,Jie Li*

Main category: cs.LG

TL;DR: 提出基于新型时间变分自编码器的知识追踪模型memoryKT，模拟记忆动态，实验显示其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖单一遗忘机制，忽略其他记忆过程和个性化遗忘模式，需改进知识追踪模型。

Method: 提出memoryKT模型，通过三阶段过程模拟记忆动态，联合建模编码 - 存储 - 检索周期，嵌入个性化遗忘模块。

Result: 在四个公开数据集上的实验表明，该方法显著优于现有基线。

Conclusion: memoryKT模型能有效模拟记忆动态，增强对个体差异的感知能力，提升知识追踪性能。

Abstract: Knowledge Tracing (KT) is committed to capturing students' knowledge mastery
from their historical interactions. Simulating students' memory states is a
promising approach to enhance both the performance and interpretability of
knowledge tracing models. Memory consists of three fundamental processes:
encoding, storage, and retrieval. Although forgetting primarily manifests
during the storage stage, most existing studies rely on a single,
undifferentiated forgetting mechanism, overlooking other memory processes as
well as personalized forgetting patterns. To address this, this paper proposes
memoryKT, a knowledge tracing model based on a novel temporal variational
autoencoder. The model simulates memory dynamics through a three-stage process:
(i) Learning the distribution of students' knowledge memory features, (ii)
Reconstructing their exercise feedback, while (iii) Embedding a personalized
forgetting module within the temporal workflow to dynamically modulate memory
storage strength. This jointly models the complete encoding-storage-retrieval
cycle, significantly enhancing the model's perception capability for individual
differences. Extensive experiments on four public datasets demonstrate that our
proposed approach significantly outperforms state-of-the-art baselines.

</details>


### [274] [NeuroDx-LM: A Clinical Large-Scale Model for EEG-based Neurological Disorder Detection](https://arxiv.org/abs/2508.08124)
*Guanghao Jin,Yuan Liang,Yihan Ma,Jingpei Wu,Guoyang Liu*

Main category: cs.LG

TL;DR: 提出NeuroDx - LM模型解决EEG大模型临床应用问题，在相关数据集达SOTA，展示临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: EEG大模型实际部署面临标注数据有限和临床场景性能不佳的问题。

Method: 提出NeuroDx - LM模型，包含选择性时频嵌入机制和渐进特征感知训练策略。

Result: 在CHB - MIT和精神分裂症数据集上分别在癫痫和精神分裂症检测中达到SOTA。

Conclusion: EEG大模型在推进临床应用方面有巨大潜力。

Abstract: Large-scale models pre-trained on Electroencephalography (EEG) have shown
promise in clinical applications such as neurological disorder detection.
However, the practical deployment of EEG-based large-scale models faces
critical challenges such as limited labeled EEG data and suboptimal performance
in clinical scenarios. To address these issues, we propose NeuroDx-LM, a novel
large-scale model specifically designed for detecting EEG-based neurological
disorders. Our key contributions include (i) a Selective Temporal-Frequency
Embedding mechanism that adaptively captures complex temporal and spectral
patterns in EEG signals; and (ii) a Progressive Feature-Aware Training strategy
that refines feature representation in a two-stage process. In the first stage,
our model learns the fundamental discriminative features of EEG activities; in
the second stage, the model further extracts more specialized fine-grained
features for accurate diagnostic performance. We evaluated NeuroDx-LM on the
CHB-MIT and Schizophrenia datasets, achieving state-of-the-art performance in
EEG-based seizure and schizophrenia detection, respectively. These results
demonstrate the great potential of EEG-based large-scale models to advance
clinical applicability. Our code is available at
https://github.com/LetItBe12345/NeuroDx-LM.

</details>


### [275] [OFAL: An Oracle-Free Active Learning Framework](https://arxiv.org/abs/2508.08126)
*Hadi Khorsand,Vahid Pourahmadi*

Main category: cs.LG

TL;DR: 介绍一种无需神谕的主动学习方案OFAL，利用神经网络不确定性生成新样本提升模型准确率，并与其他方法比较整合。


<details>
  <summary>Details</summary>
Motivation: 在主动学习中，使用神谕标记数据复杂昂贵，希望不依赖神谕取得更好结果。

Method: 引入OFAL，分离量化不确定性，用蒙特卡罗Dropout近似贝叶斯神经网络模型，添加变分自编码器从置信种子样本向潜在空间不确定部分生成新样本，最后与其他方法比较整合。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: In the active learning paradigm, using an oracle to label data has always
been a complex and expensive task, and with the emersion of large unlabeled
data pools, it would be highly beneficial If we could achieve better results
without relying on an oracle. This research introduces OFAL, an oracle-free
active learning scheme that utilizes neural network uncertainty. OFAL uses the
model's own uncertainty to transform highly confident unlabeled samples into
informative uncertain samples. First, we start with separating and quantifying
different parts of uncertainty and introduce Monte Carlo Dropouts as an
approximation of the Bayesian Neural Network model. Secondly, by adding a
variational autoencoder, we go on to generate new uncertain samples by stepping
toward the uncertain part of latent space starting from a confidence seed
sample. By generating these new informative samples, we can perform active
learning and enhance the model's accuracy. Lastly, we try to compare and
integrate our method with other widely used active learning sampling methods.

</details>


### [276] [MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation](https://arxiv.org/abs/2508.08137)
*Pravallika Abbineni,Saoud Aldowaish,Colin Liechty,Soroosh Noorzad,Ali Ghazizadeh,Morteza Fayazi*

Main category: cs.LG

TL;DR: 本文提出用于电路设计辅助的开源多模态大语言模型MuaLLM，它集成混合RAG框架与自适应向量数据库，性能优且成本低，还引入两个自定义数据集评估，取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 进行电路设计方法学的全面文献综述面临研究涌入快、数据表示不一致和优化目标复杂等挑战，需要新的工具辅助。

Method: 提出MuaLLM，集成混合RAG框架与自适应向量数据库，采用ReAct工作流，具备多模态能力，动态自适应，解耦检索和推理。

Result: 在最大上下文长度下，MuaLLM成本低至10倍且速度快1.6倍，准确率相同；在RAG - 250上召回率达90.1%，在Reas - 100上准确率达86.8%。

Conclusion: MuaLLM能有效辅助电路设计，克服传统方法的局限，实现快速无人工干预的数据库生成。

Abstract: Conducting a comprehensive literature review is crucial for advancing circuit
design methodologies. However, the rapid influx of state-of-the-art research,
inconsistent data representation, and the complexity of optimizing circuit
design objectives make this task significantly challenging. In this paper, we
propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for
circuit design assistance that integrates a hybrid Retrieval-Augmented
Generation (RAG) framework with an adaptive vector database of circuit design
research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason +
Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step
information retrieval. It functions as a question-answering design assistant,
capable of interpreting complex queries and providing reasoned responses
grounded in circuit literature. Its multimodal capabilities enable processing
of both textual and visual data, facilitating more efficient and comprehensive
analysis. The system dynamically adapts using intelligent search tools,
automated document retrieval from the internet, and real-time database updates.
Unlike conventional approaches constrained by model context limits, MuaLLM
decouples retrieval from inference, enabling scalable reasoning over
arbitrarily large corpora. At the maximum context length supported by standard
LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining
the same accuracy. This allows rapid, no-human-in-the-loop database generation,
overcoming the bottleneck of simulation-based dataset creation for circuits. To
evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval
and citation performance, and Reasoning-100 (Reas-100), focused on multistep
reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8%
accuracy on Reas-100.

</details>


### [277] [Federated Learning for Epileptic Seizure Prediction Across Heterogeneous EEG Datasets](https://arxiv.org/abs/2508.08159)
*Cem Ata Baykara,Saurav Raj Pandey,Ali Burak Ünal,Harlin Lee,Mete Akgün*

Main category: cs.LG

TL;DR: 本文研究跨多临床站点的癫痫发作预测模型，使用联邦学习，提出随机子集聚合策略，提升了模型性能和泛化性。


<details>
  <summary>Details</summary>
Motivation: 患者隐私法规和数据异质性阻碍跨多临床站点的癫痫发作预测模型开发，标准联邦平均聚合方法在异质环境中有偏差。

Method: 在四个不同公共数据集上使用单脑电图通道进行癫痫发作预测，实施隐私保护全局归一化，提出随机子集聚合策略。

Result: 本地训练模型无法跨站点泛化，标准加权联邦平均性能有偏差，随机子集聚合显著提升表现不佳客户端的性能，宏观平均准确率达77.1%，综合准确率达80.0%。

Conclusion: 平衡的联邦学习方法在尊重数据隐私的现实异质多医院环境中构建有效且可泛化的癫痫发作预测系统具有潜力。

Abstract: Developing accurate and generalizable epileptic seizure prediction models
from electroencephalography (EEG) data across multiple clinical sites is
hindered by patient privacy regulations and significant data heterogeneity
(non-IID characteristics). Federated Learning (FL) offers a privacy-preserving
framework for collaborative training, but standard aggregation methods like
Federated Averaging (FedAvg) can be biased by dominant datasets in
heterogeneous settings. This paper investigates FL for seizure prediction using
a single EEG channel across four diverse public datasets (Siena, CHB-MIT,
Helsinki, NCH), representing distinct patient populations (adult, pediatric,
neonate) and recording conditions. We implement privacy-preserving global
normalization and propose a Random Subset Aggregation strategy, where each
client trains on a fixed-size random subset of its data per round, ensuring
equal contribution during aggregation. Our results show that locally trained
models fail to generalize across sites, and standard weighted FedAvg yields
highly skewed performance (e.g., 89.0% accuracy on CHB-MIT but only 50.8% on
Helsinki and 50.6% on NCH). In contrast, Random Subset Aggregation
significantly improves performance on under-represented clients (accuracy
increases to 81.7% on Helsinki and 68.7% on NCH) and achieves a superior
macro-average accuracy of 77.1% and pooled accuracy of 80.0% across all sites,
demonstrating a more robust and fair global model. This work highlights the
potential of balanced FL approaches for building effective and generalizable
seizure prediction systems in realistic, heterogeneous multi-hospital
environments while respecting data privacy.

</details>


### [278] [Neural Logic Networks for Interpretable Classification](https://arxiv.org/abs/2508.08172)
*Vincent Perreault,Katsumi Inoue,Richard Labib,Alain Hertz*

Main category: cs.LG

TL;DR: 本文推广含NOT运算和偏置的神经逻辑网络，提出新规则结构和学习算法，提升布尔网络发现水平，能学习可解释规则。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络学习内容不可检查、验证和提取，而神经逻辑网络有可解释结构，推广其含NOT运算和偏置以考虑未观测数据并进行严格建模。

Method: 推广含NOT运算和偏置的神经逻辑网络，提出新颖的因式分解IF - THEN规则结构和修改后的学习算法。

Result: 改进了布尔网络发现的现有水平，能在表格分类中学习相关、可解释规则，在医学领域示例中表现良好。

Conclusion: 所提方法在布尔网络发现和表格分类学习可解释规则方面有效，在有解释需求的领域有价值。

Abstract: Traditional neural networks have an impressive classification performance,
but what they learn cannot be inspected, verified or extracted. Neural Logic
Networks on the other hand have an interpretable structure that enables them to
learn a logical mechanism relating the inputs and outputs with AND and OR
operations. We generalize these networks with NOT operations and biases that
take into account unobserved data and develop a rigorous logical and
probabilistic modeling in terms of concept combinations to motivate their use.
We also propose a novel factorized IF-THEN rule structure for the model as well
as a modified learning algorithm. Our method improves the state-of-the-art in
Boolean networks discovery and is able to learn relevant, interpretable rules
in tabular classification, notably on an example from the medical field where
interpretability has tangible value.

</details>


### [279] [Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion](https://arxiv.org/abs/2508.08216)
*Nicole Lai-Tan,Xiao Gu,Marios G. Philiastides,Fani Deligianni*

Main category: cs.LG

TL;DR: 提出ITSA策略和混合架构用于个性化音乐干预的BCI，提升跨主体泛化性能，代码将公开。


<details>
  <summary>Details</summary>
Motivation: 个性化音乐干预支持运动康复，通用BCI有应用前景，但脑电信号个体差异等阻碍其泛化且校准过程长。

Method: 提出ITSA策略，融合RCSP与黎曼几何的混合架构。

Result: ITSA在跨主体和条件下性能显著提升，并行融合方法比顺序方法效果更好，且在不同数据条件和电极配置下性能稳健。

Conclusion: ITSA和混合架构能有效增强BCI跨主体泛化能力。

Abstract: Personalised music-based interventions offer a powerful means of supporting
motor rehabilitation by dynamically tailoring auditory stimuli to provide
external timekeeping cues, modulate affective states, and stabilise gait
patterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for
adapting these interventions across individuals. However, inter-subject
variability in EEG signals, further compounded by movement-induced artefacts
and motor planning differences, hinders the generalisability of BCIs and
results in lengthy calibration processes. We propose Individual Tangent Space
Alignment (ITSA), a novel pre-alignment strategy incorporating subject-specific
recentering, distribution matching, and supervised rotational alignment to
enhance cross-subject generalisation. Our hybrid architecture fuses Regularised
Common Spatial Patterns (RCSP) with Riemannian geometry in parallel and
sequential configurations, improving class separability while maintaining the
geometric structure of covariance matrices for robust statistical computation.
Using leave-one-subject-out cross-validation, `ITSA' demonstrates significant
performance improvements across subjects and conditions. The parallel fusion
approach shows the greatest enhancement over its sequential counterpart, with
robust performance maintained across varying data conditions and electrode
configurations. The code will be made publicly available at the time of
publication.

</details>


### [280] [Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning](https://arxiv.org/abs/2508.08221)
*Zihe Liu,Jiashun Liu,Yancheng He,Weixun Wang,Jiaheng Liu,Ling Pan,Xinyu Hu,Shaopan Xiong,Ju Huang,Jian Hu,Shengyi Huang,Siran Yang,Jiamang Wang,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: 本文系统回顾强化学习技术，分析其机制与适用场景，给出选择指南，发现两种技术的简单组合可提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理强化学习领域缺乏标准化指南，对底层机制理解分散，实验设置等不一致导致结论冲突。

Method: 在统一开源框架下进行严格复现和独立评估，通过细粒度实验分析技术。

Result: 提出针对特定场景选择强化学习技术的明确指南，两种技术的简单组合能提升性能，超越GRPO和DAPO等策略。

Conclusion: 为大语言模型领域强化学习从业者提供可靠路线图，证明简单技术组合的有效性。

Abstract: Reinforcement learning for LLM reasoning has rapidly emerged as a prominent
research area, marked by a significant surge in related studies on both
algorithmic innovations and practical applications. Despite this progress,
several critical challenges remain, including the absence of standardized
guidelines for employing RL techniques and a fragmented understanding of their
underlying mechanisms. Additionally, inconsistent experimental settings,
variations in training data, and differences in model initialization have led
to conflicting conclusions, obscuring the key characteristics of these
techniques and creating confusion among practitioners when selecting
appropriate techniques. This paper systematically reviews widely adopted RL
techniques through rigorous reproductions and isolated evaluations within a
unified open-source framework. We analyze the internal mechanisms, applicable
scenarios, and core principles of each technique through fine-grained
experiments, including datasets of varying difficulty, model sizes, and
architectures. Based on these insights, we present clear guidelines for
selecting RL techniques tailored to specific setups, and provide a reliable
roadmap for practitioners navigating the RL for the LLM domain. Finally, we
reveal that a minimalist combination of two techniques can unlock the learning
capability of critic-free policies using vanilla PPO loss. The results
demonstrate that our simple combination consistently improves performance,
surpassing strategies like GRPO and DAPO.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [281] [Reservoir computing with large valid prediction time for the Lorenz system](https://arxiv.org/abs/2508.06730)
*Lauren A Hurley,Sean E Shaheen*

Main category: cs.NE

TL;DR: 研究储层计算机（RC）有效预测时间（VPT）与超参数的依赖关系，发现特定条件下 RC 可接近基准性能 70%，有高 VPT 值，可通过初始误差预测 VPT，强调数值求解器重要性并定义 VGTT，确定两种大 VPT 的谱半径区域。


<details>
  <summary>Details</summary>
Motivation: 研究储层计算机有效预测时间与超参数的关系，探索提高预测性能的方法，为实际应用提供参考。

Method: 通过对 Lorenz 方程进行单步预测输出作为初始条件，研究不同超参数下的 VPT；利用 Lyapunov 指数，通过初始几步预测误差预测 VPT；对比不同数值求解器的输出。

Result: 特定条件下 RC 可达基准性能约 70%，有高 VPT 值（>30 Lyapunov 时间）；可通过初始误差预测 VPT；确定 VGTT；发现两种大 VPT 的谱半径区域。

Conclusion: 储层计算机在特定条件下有较好预测性能，可通过初始误差高效评估 VPT，数值求解器对结果有重要影响，不同谱半径区域可实现大 VPT。

Abstract: We study the dependence of the Valid Prediction Time (VPT) of Reservoir
Computers (RCs) on hyperparameters including the regularization coefficient,
reservoir size, and spectral radius. Under carefully chosen conditions, the RC
can achieve approximately 70% of a benchmark performance, based on the output
of a single prediction step used as initial conditions for the Lorenz
equations. We report high VPT values (>30 Lyapunov times), as we are predicting
a noiseless system where overfitting can be beneficial. While these conditions
may not hold for noisy systems, they could still be useful for real-world
applications with limited noise. Furthermore, utilizing knowledge of the
Lyapunov exponent, we find that the VPT can be predicted by the error in the
first few prediction steps, offering a computationally efficient evaluation
method. We emphasize the importance of the numerical solver used to generate
the Lorenz dataset and define a Valid Ground Truth Time (VGTT), during which
the outputs of several common solvers agree. A VPT exceeding the VGTT is not
meaningful, as a different solver could produce a different result. Lastly, we
identify two spectral radius regimes that achieve large VPT: a small radius
near zero, resulting in simple but stable operation, and a larger radius
operating at the "edge of chaos."

</details>


### [282] [Geometry-Aware Spiking Graph Neural Network](https://arxiv.org/abs/2508.06793)
*Bowen Zhang,Genan Dai,Hu Huang,Long Lan*

Main category: cs.NE

TL;DR: 提出Geometry - Aware Spiking Graph Neural Network（GSG），统一基于脉冲的神经动力学与黎曼流形上的自适应表示学习，实验显示其在准确性、鲁棒性和能效上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有脉冲图神经网络主要在欧几里得空间中运行，依赖固定几何假设，难以对复杂图结构建模，因此要提出新方法克服这些局限。

Method: 提出GSG，包含将节点特征投影到常曲率流形池的黎曼嵌入层、在弯曲空间中建模膜电位演化和脉冲行为的流形脉冲层、通过联合优化分类和链接预测损失实现实例级几何自适应的流形学习目标，使用黎曼SGD进行训练。

Result: 在多个基准测试中，GSG比欧几里得SNN和基于流形的GNN有更高的准确性、鲁棒性和能效。

Conclusion: GSG为曲率感知、节能的图学习建立了新范式。

Abstract: Graph Neural Networks (GNNs) have demonstrated impressive capabilities in
modeling graph-structured data, while Spiking Neural Networks (SNNs) offer high
energy efficiency through sparse, event-driven computation. However, existing
spiking GNNs predominantly operate in Euclidean space and rely on fixed
geometric assumptions, limiting their capacity to model complex graph
structures such as hierarchies and cycles. To overcome these limitations, we
propose \method{}, a novel Geometry-Aware Spiking Graph Neural Network that
unifies spike-based neural dynamics with adaptive representation learning on
Riemannian manifolds. \method{} features three key components: a Riemannian
Embedding Layer that projects node features into a pool of constant-curvature
manifolds, capturing non-Euclidean structures; a Manifold Spiking Layer that
models membrane potential evolution and spiking behavior in curved spaces via
geometry-consistent neighbor aggregation and curvature-based attention; and a
Manifold Learning Objective that enables instance-wise geometry adaptation
through jointly optimized classification and link prediction losses defined
over geodesic distances. All modules are trained using Riemannian SGD,
eliminating the need for backpropagation through time. Extensive experiments on
multiple benchmarks show that GSG achieves superior accuracy, robustness, and
energy efficiency compared to both Euclidean SNNs and manifold-based GNNs,
establishing a new paradigm for curvature-aware, energy-efficient graph
learning.

</details>


### [283] [Memory Enhanced Fractional-Order Dung Beetle Optimization for Photovoltaic Parameter Identification](https://arxiv.org/abs/2508.06841)
*Yiwei Li,Zhihua Allen-Zhao,Yuncheng Xu,Sanyang Liu*

Main category: cs.NE

TL;DR: 提出MFO - DBO算法解决光伏模型参数识别问题，实验表明其性能优于多种算法。


<details>
  <summary>Details</summary>
Motivation: 光伏模型参数准确识别具有挑战性，原DBO算法易早熟，需改进。

Method: 提出MFO - DBO算法，整合分数阶微积分、分数阶逻辑混沌映射和混沌扰动机制三个策略。

Result: 在CEC2017基准套件和光伏参数识别问题上，MFO - DBO在准确性、鲁棒性、收敛速度等方面优于多种算法。

Conclusion: MFO - DBO算法能有效解决光伏模型参数识别问题，且在探索与开发间保持良好平衡。

Abstract: Accurate parameter identification in photovoltaic (PV) models is crucial for
performance evaluation but remains challenging due to their nonlinear,
multimodal, and high-dimensional nature. Although the Dung Beetle Optimization
(DBO) algorithm has shown potential in addressing such problems, it often
suffers from premature convergence. To overcome these issues, this paper
proposes a Memory Enhanced Fractional-Order Dung Beetle Optimization (MFO-DBO)
algorithm that integrates three coordinated strategies. Firstly,
fractional-order (FO) calculus introduces memory into the search process,
enhancing convergence stability and solution quality. Secondly, a
fractional-order logistic chaotic map improves population diversity during
initialization. Thirdly, a chaotic perturbation mechanism helps elite solutions
escape local optima. Numerical results on the CEC2017 benchmark suite and the
PV parameter identification problem demonstrate that MFO-DBO consistently
outperforms advanced DBO variants, CEC competition winners, FO-based
optimizers, enhanced classical algorithms, and recent metaheuristics in terms
of accuracy, robustness, convergence speed, while also maintaining an excellent
balance between exploration and exploitation compared to the standard DBO
algorithm.

</details>


### [284] [Enhancing Decision Space Diversity in Multi-Objective Evolutionary Optimization for the Diet Problem](https://arxiv.org/abs/2508.07077)
*Gustavo V. Nascimento,Ivan R. Meneghini,Valéria Santos,Eduardo Luz,Gladston Moreira*

Main category: cs.NE

TL;DR: 本文介绍一种将基于汉明距离的均匀性度量集成到多目标进化算法（MOEA）选择机制中的方法，以增强决策空间多样性，实验表明该方法在饮食问题上有效。


<details>
  <summary>Details</summary>
Motivation: 多数MOEA注重目标空间优化，忽略决策空间解的多样性，而决策空间多样性对决策者提供多种选择很关键。

Method: 将基于汉明距离的均匀性度量直接集成到MOEA的选择机制中。

Result: 在饮食问题的多目标公式实验中，该方法相比NSGA - II显著提高了决策空间多样性，且目标空间性能相当。

Conclusion: 所提出的方法为将决策空间意识集成到MOEA中提供了可推广的策略。

Abstract: Multi-objective evolutionary algorithms (MOEAs) are essential for solving
complex optimization problems, such as the diet problem, where balancing
conflicting objectives, like cost and nutritional content, is crucial. However,
most MOEAs focus on optimizing solutions in the objective space, often
neglecting the diversity of solutions in the decision space, which is critical
for providing decision-makers with a wide range of choices. This paper
introduces an approach that directly integrates a Hamming distance-based
measure of uniformity into the selection mechanism of a MOEA to enhance
decision space diversity. Experiments on a multi-objective formulation of the
diet problem demonstrate that our approach significantly improves decision
space diversity compared to NSGA-II, while maintaining comparable objective
space performance. The proposed method offers a generalizable strategy for
integrating decision space awareness into MOEAs.

</details>


### [285] [Evolutionary Optimization of Deep Learning Agents for Sparrow Mahjong](https://arxiv.org/abs/2508.07522)
*Jim O'Connor,Derin Gezgin,Gary B. Parker*

Main category: cs.NE

TL;DR: 提出基于深度学习的Evo - Sparrow麻将AI决策代理，结合LSTM和CMA - ES，在模拟中表现出色，为AI游戏领域提供新策略。


<details>
  <summary>Details</summary>
Motivation: 在麻雀麻将的非确定性、部分可观察游戏环境中进行AI决策，探索替代传统强化学习和基于梯度优化方法的有效方案。

Method: 使用协方差矩阵自适应进化策略（CMA - ES）优化长短期记忆网络（LSTM）训练Evo - Sparrow模型。

Result: 模型在大量模拟中优于随机和基于规则的代理，性能与近端策略优化（PPO）基线相当。

Conclusion: 结合深度学习和进化优化的方法是复杂随机游戏的可行混合学习策略，对AI游戏领域有贡献，在其他领域也有潜在应用。

Abstract: We present Evo-Sparrow, a deep learning-based agent for AI decision-making in
Sparrow Mahjong, trained by optimizing Long Short-Term Memory (LSTM) networks
using Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Our model
evaluates board states and optimizes decision policies in a non-deterministic,
partially observable game environment. Empirical analysis conducted over a
significant number of simulations demonstrates that our model outperforms both
random and rule-based agents, and achieves performance comparable to a Proximal
Policy Optimization (PPO) baseline, indicating strong strategic play and robust
policy quality. By combining deep learning with evolutionary optimization, our
approach provides a computationally effective alternative to traditional
reinforcement learning and gradient-based optimization methods. This research
contributes to the broader field of AI game playing, demonstrating the
viability of hybrid learning strategies for complex stochastic games. These
findings also offer potential applications in adaptive decision-making and
strategic AI development beyond Sparrow Mahjong.

</details>


### [286] [Energy and Quality of Surrogate-Assisted Search Algorithms: a First Analysis](https://arxiv.org/abs/2508.07691)
*Tomohiro Harada,Enrique Alba,Gabriel Luque*

Main category: cs.NE

TL;DR: 本文从能量消耗角度研究替代模型对元启发式算法的帮助，分析不同版本粒子群优化算法的能量消耗和替代模型准确性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂问题需改进搜索算法，替代辅助元启发式算法的能量消耗、效率和准确性尚需研究。

Method: 分析不同版本（含预训练和再训练神经网络作为替代模型）的粒子群优化算法的处理器和内存能量消耗，研究替代模型准确性以引导搜索。

Result: 得出相关结论，为该领域研究提供新视角。

Conclusion: 结论为评估替代辅助算法提供新思路，考虑能量和替代模型准确性以更全面刻画优化和学习技术。

Abstract: Solving complex real problems often demands advanced algorithms, and then
continuous improvements in the internal operations of a search technique are
needed. Hybrid algorithms, parallel techniques, theoretical advances, and much
more are needed to transform a general search algorithm into an efficient,
useful one in practice. In this paper, we study how surrogates are helping
metaheuristics from an important and understudied point of view: their energy
profile. Even if surrogates are a great idea for substituting a time-demanding
complex fitness function, the energy profile, general efficiency, and accuracy
of the resulting surrogate-assisted metaheuristic still need considerable
research. In this work, we make a first step in analyzing particle swarm
optimization in different versions (including pre-trained and retrained neural
networks as surrogates) for its energy profile (for both processor and memory),
plus a further study on the surrogate accuracy to properly drive the search
towards an acceptable solution. Our conclusions shed new light on this topic
and could be understood as the first step towards a methodology for assessing
surrogate-assisted algorithms not only accounting for time or numerical
efficiency but also for energy and surrogate accuracy for a better, more
holistic characterization of optimization and learning techniques.

</details>


### [287] [Growing Reservoirs with Developmental Graph Cellular Automata](https://arxiv.org/abs/2508.08091)
*Matias Barandiaran,James Stovold*

Main category: cs.NE

TL;DR: 本文展示了发育图元胞自动机（DGCA）可被训练生成储层，能形成多种结构解决基准任务，为相关系统发展奠定基础。


<details>
  <summary>Details</summary>
Motivation: 探索DGCA在生成储层方面的能力，以推动DGCA系统发展和功能、适应性形态发生建模。

Method: 将DGCAs训练生成储层，设置任务驱动（使用NARMA系列任务）和任务独立（使用储层指标）两种目标。

Result: DGCAs能生长成多种类似生命的专门化结构，有效解决基准任务，在相同任务上统计表现优于‘典型’储层。

Conclusion: 为产生可塑性储层的DGCA系统发展和功能性、适应性形态发生建模奠定基础。

Abstract: Developmental Graph Cellular Automata (DGCA) are a novel model for
morphogenesis, capable of growing directed graphs from single-node seeds. In
this paper, we show that DGCAs can be trained to grow reservoirs. Reservoirs
are grown with two types of targets: task-driven (using the NARMA family of
tasks) and task-independent (using reservoir metrics).
  Results show that DGCAs are able to grow into a variety of specialized,
life-like structures capable of effectively solving benchmark tasks,
statistically outperforming `typical' reservoirs on the same task. Overall,
these lay the foundation for the development of DGCA systems that produce
plastic reservoirs and for modeling functional, adaptive morphogenesis.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [288] [Refactoring-Aware Patch Integration Across Structurally Divergent Java Forks](https://arxiv.org/abs/2508.06718)
*Daniel Ogenrwot,John Businge*

Main category: cs.SE

TL;DR: 研究GitHub上变体间补丁集成问题，提出RePatch系统，评估显示其优于Git cherry - pick。


<details>
  <summary>Details</summary>
Motivation: GitHub上变体独立开发，结构漂移使跨变体集成补丁有挑战，需解决补丁集成失败问题。

Method: 对14对变体进行实证研究，扩展RefMerge框架，支持非对称补丁转移，反转重构、应用补丁并回放转换。

Result: 评估478个补丁请求，Git cherry - pick因结构不对齐失败率64.4%，RePatch成功集成52.8%此前失败的补丁。

Conclusion: 基于语法的工具存在局限，变体感知的补丁传播需要语义推理。

Abstract: While most forks on platforms like GitHub are short-lived and used for social
collaboration, a smaller but impactful subset evolve into long-lived forks,
referred to here as variants, that maintain independent development
trajectories. Integrating bug-fix patches across such divergent variants poses
challenges due to structural drift, including refactorings that rename,
relocate, or reorganize code elements and obscure semantic correspondence. This
paper presents an empirical study of patch integration failures in 14 divergent
pair of variants and introduces RePatch, a refactoring-aware integration system
for Java repositories. RePatch extends the RefMerge framework, originally
designed for symmetric merges, by supporting asymmetric patch transfer. RePatch
inverts refactorings in both the source and target to realign the patch
context, applies the patch, and replays the transformations to preserve the
intent of the variant. In our evaluation of 478 bug-fix pull requests, Git
cherry-pick fails in 64.4% of cases due to structural misalignments, while
RePatch successfully integrates 52.8% of the previously failing patches. These
results highlight the limitations of syntax-based tools and the need for
semantic reasoning in variant-aware patch propagation.

</details>


### [289] [Quo Vadis, Code Review? Exploring the Future of Code Review](https://arxiv.org/abs/2508.06879)
*Michael Dorner,Andreas Bauer,Darja Šmite,Lukas Thode,Daniel Mendez,Ricardo Britto,Stephan Lukasczyk,Ehsan Zabardast,Michael Kormann*

Main category: cs.SE

TL;DR: 研究从业者对代码审查现状的看法、未来预期变化及潜在长期风险。


<details>
  <summary>Details</summary>
Motivation: 了解代码审查在当下从业者中的情况以及未来可能的变化。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: Code review has long been a core practice in collaborative software
engineering. In this research, we explore how practitioners reflect on code
review today and what changes they anticipate in the near future. We then
discuss the potential long-term risks of these anticipated changes for the
evolution of code review and its role in collaborative software engineering.

</details>


### [290] [An Empirical Study on Method-Level Performance Evolution in Open-Source Java Projects](https://arxiv.org/abs/2508.07084)
*Kaveh Shahedi,Nana Gyambrah,Heng Li,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: 对15个Java开源项目739次提交中1499个方法级代码变更进行大规模实证研究，发现32.7%变更有性能影响，回归比改进多1.3倍等，为自动化性能测试融入持续集成管道提供实证。


<details>
  <summary>Details</summary>
Motivation: 当前对方法级代码变更对性能演变的影响了解不足，开发者相关直觉缺乏细粒度实证验证。

Method: 分析15个成熟Java开源项目，用JMH精确测量性能，用统计分析量化性能变化，采用字节码插桩捕获执行指标，分析四个关键方面。

Result: 32.7%方法级变更有性能影响，回归比改进多1.3倍；各代码变更类别性能影响分布无显著差异；算法变更改进潜力大但回归风险高；高级开发者变更更稳定；代码复杂度与回归可能性正相关；不同领域规模组合有不同性能稳定性。

Conclusion: 研究为将自动化性能测试集成到持续集成管道提供了实证依据。

Abstract: Performance is a critical quality attribute in software development, yet the
impact of method-level code changes on performance evolution remains poorly
understood. While developers often make intuitive assumptions about which types
of modifications are likely to cause performance regressions or improvements,
these beliefs lack empirical validation at a fine-grained level. We conducted a
large-scale empirical study analyzing performance evolution in 15 mature
open-source Java projects hosted on GitHub. Our analysis encompassed 739
commits containing 1,499 method-level code changes, using Java Microbenchmark
Harness (JMH) for precise performance measurement and rigorous statistical
analysis to quantify both the significance and magnitude of performance
variations. We employed bytecode instrumentation to capture method-specific
execution metrics and systematically analyzed four key aspects: temporal
performance patterns, code change type correlations, developer and complexity
factors, and domain-size interactions. Our findings reveal that 32.7% of
method-level changes result in measurable performance impacts, with regressions
occurring 1.3 times more frequently than improvements. Contrary to conventional
wisdom, we found no significant differences in performance impact distributions
across code change categories, challenging risk-stratified development
strategies. Algorithmic changes demonstrate the highest improvement potential
but carry substantial regression risk. Senior developers produce more stable
changes with fewer extreme variations, while code complexity correlates with
increased regression likelihood. Domain-size interactions reveal significant
patterns, with web server + small projects exhibiting the highest performance
instability. Our study provides empirical evidence for integrating automated
performance testing into continuous integration pipelines.

</details>


### [291] [Multi-Modal Requirements Data-based Acceptance Criteria Generation using LLMs](https://arxiv.org/abs/2508.06888)
*Fanyu Wang,Chetan Arora,Yonghui Liu,Kaicheng Huang,Chakkrit Tantithamthavorn,Aldeida Aleti,Dishan Sambathkumar,David Lo*

Main category: cs.SE

TL;DR: 提出RAGcceptance M2RE方法用多模态数据生成软件验收标准，工业案例验证其提升标准质量、减少人工，凸显多模态RAG技术潜力。


<details>
  <summary>Details</summary>
Motivation: 手动创建准确、全面且明确的软件验收标准具有挑战性，尤其是在用户界面密集型应用中，需结合多模态数据解决问题。

Method: 提出RAGcceptance M2RE方法，利用检索增强生成（RAG）从多模态需求数据（包括文本和视觉UI信息）生成验收标准。

Result: 工业案例表明整合多模态信息增强了生成的验收标准的相关性、正确性和可理解性，实践评估确认该方法减少人工、捕捉利益相关者意图。

Conclusion: 多模态RAG技术在简化软件验证流程和提高开发效率方面有潜力，研究还提供了实现和数据集。

Abstract: Acceptance criteria (ACs) play a critical role in software development by
clearly defining the conditions under which a software feature satisfies
stakeholder expectations. However, manually creating accurate, comprehensive,
and unambiguous acceptance criteria is challenging, particularly in user
interface-intensive applications, due to the reliance on domain-specific
knowledge and visual context that is not always captured by textual
requirements alone. To address these challenges, we propose RAGcceptance M2RE,
a novel approach that leverages Retrieval-Augmented Generation (RAG) to
generate acceptance criteria from multi-modal requirements data, including both
textual documentation and visual UI information. We systematically evaluated
our approach in an industrial case study involving an education-focused
software system used by approximately 100,000 users. The results indicate that
integrating multi-modal information significantly enhances the relevance,
correctness, and comprehensibility of the generated ACs. Moreover, practitioner
evaluations confirm that our approach effectively reduces manual effort,
captures nuanced stakeholder intent, and provides valuable criteria that domain
experts may overlook, demonstrating practical utility and significant potential
for industry adoption. This research underscores the potential of multi-modal
RAG techniques in streamlining software validation processes and improving
development efficiency. We also make our implementation and a dataset
available.

</details>


### [292] [Integrating Rules and Semantics for LLM-Based C-to-Rust Translation](https://arxiv.org/abs/2508.06926)
*Feng Luo,Kexing Ji,Cuiyun Gao,Shuzheng Gao,Jia Feng,Kui Liu,Xin Xia,Michael R. Lyu*

Main category: cs.SE

TL;DR: 现有C代码转Rust方法有局限，提出IRENE框架提升翻译效果并评估。


<details>
  <summary>Details</summary>
Motivation: 早期C代码转Rust方法覆盖有限，LLM - 基于方法在遵循规则和语义一致方面有问题，需改进。

Method: 提出IRENE框架，包含规则增强检索、结构化总结、错误驱动翻译三个模块。

Result: 在两个数据集和八个LLM上评估，关注翻译准确性和安全性。

Conclusion: 未明确给出结论，但暗示IRENE框架可能解决现有方法的问题。

Abstract: Automated translation of legacy C code into Rust aims to ensure memory safety
while reducing the burden of manual migration. Early approaches in code
translation rely on static rule-based methods, but they suffer from limited
coverage due to dependence on predefined rule patterns. Recent works regard the
task as a sequence-to-sequence problem by leveraging large language models
(LLMs). Although these LLM-based methods are capable of reducing unsafe code
blocks, the translated code often exhibits issues in following Rust rules and
maintaining semantic consistency. On one hand, existing methods adopt a direct
prompting strategy to translate the C code, which struggles to accommodate the
syntactic rules between C and Rust. On the other hand, this strategy makes it
difficult for LLMs to accurately capture the semantics of complex code. To
address these challenges, we propose IRENE, an LLM-based framework that
Integrates RulEs aNd sEmantics to enhance translation. IRENE consists of three
modules: 1) a rule-augmented retrieval module that selects relevant translation
examples based on rules generated from a static analyzer developed by us,
thereby improving the handling of Rust rules; 2) a structured summarization
module that produces a structured summary for guiding LLMs to enhance the
semantic understanding of C code; 3) an error-driven translation module that
leverages compiler diagnostics to iteratively refine translations. We evaluate
IRENE on two datasets (xCodeEval, a public dataset, and HW-Bench, an industrial
dataset provided by Huawei) and eight LLMs, focusing on translation accuracy
and safety.

</details>


### [293] [When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust "APIs'' for Human-AI Interaction](https://arxiv.org/abs/2508.06942)
*Zhenchang Xing,Yang Liu,Zhuo Cheng,Qing Huang,Dehai Zhao,Daniel Sun,Chenhua Liu*

Main category: cs.SE

TL;DR: 提出Controlled NL for Prompt (CNL - P)，结合提示工程和软件工程原则，还开发转换和检查工具，实验表明其提升LLM响应质量，有望开创以自然语言为中心的编程范式。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用增多，为提升自然语言提示质量，结合提示工程最佳实践和软件工程原则。

Method: 提出CNL - P，引入精确语法和语义规范；开发基于LLM的NL2CNL - P转换工具；开发静态分析的检查工具。

Result: 大量实验表明，CNL - P通过提示工程和软件工程的协同提升了大语言模型响应质量。

Conclusion: CNL - P能弥合新兴提示工程和传统软件工程的差距，为以自然语言为中心的新编程范式奠定基础。

Abstract: With the growing capabilities of large language models (LLMs), they are
increasingly applied in areas like intelligent customer service, code
generation, and knowledge management. Natural language (NL) prompts act as the
``APIs'' for human-LLM interaction. To improve prompt quality, best practices
for prompt engineering (PE) have been developed, including writing guidelines
and templates. Building on this, we propose Controlled NL for Prompt (CNL-P),
which not only incorporates PE best practices but also draws on key principles
from software engineering (SE). CNL-P introduces precise grammar structures and
strict semantic norms, further eliminating NL's ambiguity, allowing for a
declarative but structured and accurate expression of user intent. This helps
LLMs better interpret and execute the prompts, leading to more consistent and
higher-quality outputs. We also introduce an NL2CNL-P conversion tool based on
LLMs, enabling users to write prompts in NL, which are then transformed into
CNL-P format, thus lowering the learning curve of CNL-P. In particular, we
develop a linting tool that checks CNL-P prompts for syntactic and semantic
accuracy, applying static analysis techniques to NL for the first time.
Extensive experiments demonstrate that CNL-P enhances the quality of LLM
responses through the novel and organic synergy of PE and SE. We believe that
CNL-P can bridge the gap between emerging PE and traditional SE, laying the
foundation for a new programming paradigm centered around NL.

</details>


### [294] [From Noise to Knowledge: Interactive Summaries for Developer Alerts](https://arxiv.org/abs/2508.07169)
*Burak Yetiştiren,Hong Jin Kang,Miryung Kim*

Main category: cs.SE

TL;DR: 提出CLARITY工具支持交互式查询解释工具生成的警告，在Java项目验证，用户研究和模拟表明其有效


<details>
  <summary>Details</summary>
Motivation: 当前程序员逐个审查警告，识别重复主题和关系可增强认知理解，需要支持交互式查询的工具

Method: 提出CLARITY，通过主动反馈推导自定义分组规则，进行用户研究和模拟

Result: 用户使用CLARITY能更快更自信阐明根因，模拟显示规则级反馈减少交互次数

Conclusion: CLARITY基于主动学习的总结增强了交互式警告理解

Abstract: Programmers using bug-finding tools often review their reported warnings one
by one. Based on the insight that identifying recurring themes and
relationships can enhance the cognitive process of sensemaking, we propose
CLARITY, which supports interpreting tool-generated warnings through
interactive inquiry. CLARITY derives summary rules for custom grouping of
related warnings with active feedback. As users mark warnings as interesting or
uninteresting, CLARITY's rule inference algorithm surfaces common symptoms,
highlighting structural similarities in containment, subtyping, invoked
methods, accessed fields, and expressions.
  We demonstrate CLARITY on Infer and SpotBugs warnings across two mature Java
projects. In a within-subject user study with 14 participants, users
articulated root causes for similar uninteresting warnings faster and with more
confidence using CLARITY. We observed significant individual variation in
desired grouping, reinforcing the need for customizable sensemaking. Simulation
shows that with rule-level feedback, only 11.8 interactions are needed on
average to align all inferred rules with a simulated user's labels (vs. 17.8
without). Our evaluation suggests that CLARITY's active learning-based
summarization enhances interactive warning sensemaking.

</details>


### [295] [Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes](https://arxiv.org/abs/2508.07180)
*Zhe Zhang,Runlin Liu,Aishan Liu,Xingyu Liu,Xiang Gao,Hailong Sun*

Main category: cs.SE

TL;DR: 提出CODE2BENCH构建抗污染基准测试，用其构建CODE2BENCH - 2505评估16个大语言模型，发现模型在不同任务表现不同。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在数据污染和测试严谨性不足问题，需有效评估大语言模型在复杂现实代码生成任务的性能。

Method: 提出CODE2BENCH，包含自动动态更新、基于作用域图的依赖分析和基于属性的测试，用其构建CODE2BENCH - 2505。

Result: 评估16个大语言模型，模型在复杂非标准逻辑和跨语言的SC任务表现差，在Python的WSC任务表现较好。

Conclusion: 提出抗污染、语言无关的动态基准构建方法，为评估大语言模型提供基础。

Abstract: As large language models LLMs) become increasingly integrated into software
development workflows, rigorously evaluating their performance on complex,
real-world code generation tasks has become essential. However, existing
benchmarks often suffer from data contamination and limited test rigor,
constraining their ability to reveal model failures effectively. To address
these, we present CODE2BENCH, a end-to-end pipeline for dynamically
constructing robust and contamination-resistant benchmarks from real-world
GitHub repositories. Specifically, CODE2BENCH introduces three key innovations:
(1) Automated Dynamism, achieved through periodic ingestion of recent code to
minimize training data contamination; (2) Scope Graph-based dependency
analysis, which enables structured classification of functions into benchmark
instances with controlled dependency levels (distinguishing between
Self-Contained (SC) tasks for cross-language evaluation and Weakly
Self-Contained (WSC) tasks involving permitted library usage); and (3)
Property-Based Testing (PBT) for the automated synthesis of rigorous test
suites to enable thorough functional verification. Using this pipeline, we
construct CODE2BENCH-2505, the first benchmark derived from 880 recent Python
projects spanning diverse domains, comprising 1,163 code generation tasks with
100% average branch coverage on ground-truth implementations. Extensive
evaluation of 16 LLMs using CODE2BENCH-2505 reveals that models consistently
struggle with SC tasks requiring complex, non-standard logic and cross-language
transfer, while showing relatively stronger performance on WSC tasks in Python.
Our work introduces a contamination-resistant, language-agnostic methodology
for dynamic benchmark construction, offering a principled foundation for the
comprehensive and realistic evaluation of LLMs on real-world software
development tasks.

</details>


### [296] [TraceLens: Question-Driven Debugging for Taint Flow Understanding](https://arxiv.org/abs/2508.07198)
*Burak Yetiştiren,Hong Jin Kang,Miryung Kim*

Main category: cs.SE

TL;DR: 提出用于污点分析的TraceLens问答式调试界面，用户研究显示其比CodeQL更优。


<details>
  <summary>Details</summary>
Motivation: 现有污点分析工具缺乏端用户调试能力，可视化方式难分析全局影响，需改进。

Method: 提出TraceLens，支持用户问why、why - not、what - if问题，进行推测性what - if分析。

Result: 12人用户研究表明，使用TraceLens平均准确率比CodeQL高21%，心理需求降低45%，识别相关流信心更高。

Conclusion: TraceLens能有效提升污点分析的调试效率和用户体验。

Abstract: Taint analysis is a security analysis technique used to track the flow of
potentially dangerous data through an application and its dependent libraries.
Investigating why certain unexpected flows appear and why expected flows are
missing is an important sensemaking process during end-user taint analysis.
Existing taint analysis tools often do not provide this end-user debugging
capability, where developers can ask why, why-not, and what-if questions about
dataflows and reason about the impact of configuring sources and sinks, and
models of 3rd-party libraries that abstract permissible and impermissible data
flows. Furthermore, a tree-view or a list-view used in existing
taint-analyzer's visualization makes it difficult to reason about the global
impact on connectivity between multiple sources and sinks.
  Inspired by the insight that sensemaking tool-generated results can be
significantly improved by a QA inquiry process, we propose TraceLens, a first
end-user question-answer style debugging interface for taint analysis. It
enables a user to ask why, why-not, and what-if questions to investigate the
existence of suspicious flows, the non-existence of expected flows, and the
global impact of third-party library models. TraceLens performs speculative
what-if analysis, to help a user in debugging how different connectivity
assumptions affect overall results. A user study with 12 participants shows
that participants using TraceLens achieved 21% higher accuracy on average,
compared to CodeQL. They also reported a 45% reduction in mental demand
(NASA-TLX) and rated higher confidence in identifying relevant flows using
TraceLens.

</details>


### [297] [AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation](https://arxiv.org/abs/2508.07371)
*Yi Zhong,Hongchao Liu,Di ZHao*

Main category: cs.SE

TL;DR: 提出基于HDL的断言生成方法，结合LLM和Unsloth平台自动生成测试用例，降低成本且有效，提供软件测试维护解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂度增加，对自动化测试和维护工具需求激增，需满足此需求。

Method: 提出基于硬件描述语言（HDL）的断言生成方法，结合轻量级、参数可调大语言模型（LLM）和Unsloth平台自动生成测试用例。

Result: 方法能有效生成严格符合硬件逻辑的断言。

Conclusion: 该框架为现代软件测试和维护挑战提供了强大且灵活的解决方案。

Abstract: As the complexity of software systems continues to increase, the demand for
automated testing and maintenance tools is growing exponentially. To meet this
urgent need, we propose a new assertion generation method based on Hardware
Description Language (HDL). This method combines a lightweight,
parameter-adjustable large language model (LLM) with the Unsloth platform to
automatically generate test cases, thereby significantly reducing training
costs without sacrificing accuracy or generalization performance. Empirical
evaluation shows that our method can efficiently generate assertions that
strictly conform to the hardware logic. This framework provides a robust and
flexible solution to modern software testing and maintenance challenges.
https://github.com/liusu-orange/AutoAssert-1 and
https://gitee.com/OpenBPU/auto-assert1 are the locations of the source code.

</details>


### [298] [Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering](https://arxiv.org/abs/2508.07486)
*Morteza Ziabakhsh,Kiyan Rezaee,Sadegh Eskandari,Seyed Amir Hossein Tabatabaei,Mohammad M. Ghassemi*

Main category: cs.SE

TL;DR: 提出Mo2oM框架将单体应用提取为重叠微服务，在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有微服务提取方法采用硬聚类，增加了服务间耦合，降低了服务内内聚，需要更好的提取方法。

Method: 将微服务提取表述为软聚类问题，结合深度语义嵌入和方法调用图的结构依赖，使用基于图神经网络的软聚类算法。

Result: 在四个开源单体基准测试上评估，在结构模块化、服务间调用百分比、接口数量和非极端分布等指标上有显著提升。

Conclusion: Mo2oM框架在微服务提取方面表现出色，能有效提升多个关键指标。

Abstract: Modern software systems are increasingly shifting from monolithic
architectures to microservices to enhance scalability, maintainability, and
deployment flexibility. Existing microservice extraction methods typically rely
on hard clustering, assigning each software component to a single microservice.
This approach often increases inter-service coupling and reduces intra-service
cohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), a
framework that formulates microservice extraction as a soft clustering problem,
allowing components to belong probabilistically to multiple microservices. This
approach is inspired by expert-driven decompositions, where practitioners
intentionally replicate certain software components across services to reduce
communication overhead. Mo2oM combines deep semantic embeddings with structural
dependencies extracted from methodcall graphs to capture both functional and
architectural relationships. A graph neural network-based soft clustering
algorithm then generates the final set of microservices. We evaluate Mo2oM on
four open-source monolithic benchmarks and compare it against eight
state-of-the-art baselines. Our results demonstrate that Mo2oM achieves
improvements of up to 40.97% in structural modularity (balancing cohesion and
coupling), 58% in inter-service call percentage (communication overhead),
26.16% in interface number (modularity and decoupling), and 38.96% in
non-extreme distribution (service size balance) across all benchmarks.

</details>


### [299] [Adopting Road-Weather Open Data in Route Recommendation Engine](https://arxiv.org/abs/2508.07881)
*Henna Tammia,Benjamin Kämä,Ella Peltonen*

Main category: cs.SE

TL;DR: 本文探讨芬兰开放道路数据接口Digitraffic数据利用挑战，以道路天气属性为例，给出高效数据利用方法并验证可推荐个性化路线。


<details>
  <summary>Details</summary>
Motivation: 有效利用Digitraffic数据接口用于实际应用，需深入了解数据质量、预处理阶段和机器学习工具。

Method: 以Digitraffic中与道路天气相关的属性为例，为基于简单路由应用的个性化道路推荐引擎提供高效数据利用方法。

Result: 基于真实世界数据验证了该解决方案，能为三种不同的驾驶员配置文件高效识别和推荐个性化路线。

Conclusion: 所提出的方法可以有效利用Digitraffic数据，实现个性化道路推荐。

Abstract: Digitraffic, Finland's open road data interface, provides access to
nationwide road sensors with more than 2,300 real-time attributes from 1,814
stations. However, efficiently utilizing such a versatile data API for a
practical application requires a deeper understanding of the data qualities,
preprocessing phases, and machine learning tools. This paper discusses the
challenges of large-scale road weather and traffic data. We go through the
road-weather-related attributes from DigiTraffic as a practical example of
processes required to work with such a dataset. In addition, we provide a
methodology for efficient data utilization for the target application, a
personalized road recommendation engine based on a simple routing application.
We validate our solution based on real-world data, showing we can efficiently
identify and recommend personalized routes for three different driver profiles.

</details>


### [300] [SHIELDA: Structured Handling of Exceptions in LLM-Driven Agentic Workflows](https://arxiv.org/abs/2508.07935)
*Jingwen Zhou,Jieshan Chen,Qinghua Lu,Dehai Zhao,Liming Zhu*

Main category: cs.SE

TL;DR: 本文指出大语言模型驱动的代理系统工作流处理异常时存在问题，提出SHIELDA框架解决，并用AutoPR代理案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理系统异常处理方案处理表面，未追溯根源且恢复逻辑脆弱，需改进。

Method: 提出36种异常类型的分类法，构建SHIELDA模块化运行时异常处理框架，通过异常分类器选处理模式，由结构化执行器执行。

Result: 通过AutoPR代理案例验证SHIELDA能有效从推理引发的异常中跨阶段恢复。

Conclusion: SHIELDA框架可有效处理大语言模型代理系统工作流异常，实现跨阶段恢复。

Abstract: Large Language Model (LLM) agentic systems are software systems powered by
LLMs that autonomously reason, plan, and execute multi-step workflows to
achieve human goals, rather than merely executing predefined steps. During
execution, these workflows frequently encounter exceptions. Existing exception
handling solutions often treat exceptions superficially, failing to trace
execution-phase exceptions to their reasoning-phase root causes. Furthermore,
their recovery logic is brittle, lacking structured escalation pathways when
initial attempts fail. To tackle these challenges, we first present a
comprehensive taxonomy of 36 exception types across 12 agent artifacts.
Building on this, we propose SHIELDA (Structured Handling of Exceptions in
LLM-Driven Agentic Workflows), a modular runtime exception handling framework
for LLM agentic workflows. SHIELDA uses an exception classifier to select a
predefined exception handling pattern from a handling pattern registry. These
patterns are then executed via a structured handling executor, comprising local
handling, flow control, and state recovery, to enable phase-aware recovery by
linking exceptions to their root causes and facilitating composable strategies.
We validate SHIELDA's effectiveness through a case study on the AutoPR agent,
demonstrating effective, cross-phase recovery from a reasoning-induced
exception.

</details>


### [301] [Exploring the Challenges and Opportunities of AI-assisted Codebase Generation](https://arxiv.org/abs/2508.07966)
*Philipp Eibl,Sadra Sabouri,Souti Chattopadhyay*

Main category: cs.SE

TL;DR: 本文通过用户研究和访谈，发现开发者使用代码库AI助手（CBAs）生成代码库的满意度低，分析了原因并调查商业CBAs提出设计机会。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注代码片段生成，新的代码库生成模型研究少且采用率低，为更好利用CBAs，需了解开发者交互情况和其不足。

Method: 进行平衡的用户研究和对16名学生与开发者的访谈，调查21个商业CBAs。

Result: 参与者提示信息多样，但对生成代码库满意度低，功能、代码质量和沟通问题是不满主因，还发现使用CBAs的6个挑战和5个障碍。

Conclusion: 根据参与者挑战与商业CBAs能力对比，提出更高效有用的CBAs设计机会。

Abstract: Recent AI code assistants have significantly improved their ability to
process more complex contexts and generate entire codebases based on a textual
description, compared to the popular snippet-level generation. These codebase
AI assistants (CBAs) can also extend or adapt codebases, allowing users to
focus on higher-level design and deployment decisions. While prior work has
extensively studied the impact of snippet-level code generation, this new class
of codebase generation models is relatively unexplored. Despite initial
anecdotal reports of excitement about these agents, they remain less frequently
adopted compared to snippet-level code assistants. To utilize CBAs better, we
need to understand how developers interact with CBAs, and how and why CBAs fall
short of developers' needs. In this paper, we explored these gaps through a
counterbalanced user study and interview with (n = 16) students and developers
working on coding tasks with CBAs. We found that participants varied the
information in their prompts, like problem description (48% of prompts),
required functionality (98% of prompts), code structure (48% of prompts), and
their prompt writing process. Despite various strategies, the overall
satisfaction score with generated codebases remained low (mean = 2.8, median =
3, on a scale of one to five). Participants mentioned functionality as the most
common factor for dissatisfaction (77% of instances), alongside poor code
quality (42% of instances) and communication issues (25% of instances). We
delve deeper into participants' dissatisfaction to identify six underlying
challenges that participants faced when using CBAs, and extracted five barriers
to incorporating CBAs into their workflows. Finally, we surveyed 21 commercial
CBAs to compare their capabilities with participant challenges and present
design opportunities for more efficient and useful CBAs.

</details>


### [302] [PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C](https://arxiv.org/abs/2508.08171)
*Pedro Orvalho,Marta Kwiatkowska*

Main category: cs.SE

TL;DR: 提出PyVeritas框架，利用大语言模型将Python转C，结合C模型检查工具验证Python代码，实验显示有较高准确率。


<details>
  <summary>Details</summary>
Motivation: Python缺乏强大的形式验证工具，现有Python转译器复杂，限制了形式验证在Python程序中的应用。

Method: 提出PyVeritas框架，用大语言模型将Python转C，对生成的C代码进行有界模型检查和基于MaxSAT的故障定位。

Result: 在两个Python基准测试中，基于大语言模型的转译达到80 - 90%的准确率。

Conclusion: PyVeritas能为小型非平凡Python程序提供基于断言的验证和可解释的故障诊断。

Abstract: Python has become the dominant language for general-purpose programming, yet
it lacks robust tools for formal verification. In contrast, programmers working
in languages such as C benefit from mature model checkers, for example CBMC,
which enable exhaustive symbolic reasoning and fault localisation. The inherent
complexity of Python, coupled with the verbosity and low-level nature of
existing transpilers (e.g., Cython), have historically limited the
applicability of formal verification to Python programs.
  In this paper, we propose PyVeritas, a novel framework that leverages Large
Language Models (LLMs) for high-level transpilation from Python to C, followed
by bounded model checking and MaxSAT-based fault localisation in the generated
C code. PyVeritas enables verification and bug localisation for Python code
using existing model checking tools for C. Our empirical evaluation on two
Python benchmarks demonstrates that LLM-based transpilation can achieve a high
degree of accuracy, up to 80--90% for some LLMs, enabling effective development
environment that supports assertion-based verification and interpretable fault
diagnosis for small yet non-trivial Python programs.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [303] [Forecasting Commodity Price Shocks Using Temporal and Semantic Fusion of Prices Signals and Agentic Generative AI Extracted Economic News](https://arxiv.org/abs/2508.06497)
*Mohammed-Khalil Ghali,Cecil Pang,Oscar Molina,Carlos Gershenson-Garcia,Daehan Won*

Main category: q-fin.CP

TL;DR: 本文提出结合历史商品价格数据与经济新闻语义信号的混合预测框架，用生成式AI管道，模型表现远超传统基线，证明整合生成式AI与深度学习可提升商品价格冲击预警能力。


<details>
  <summary>Details</summary>
Motivation: 准确预测商品价格飙升对经济缓冲能力有限的国家至关重要，可避免国家预算紧张、行业受扰及粮食和能源安全受损。

Method: 引入混合预测框架，结合历史商品价格数据与全球经济新闻语义信号，采用生成式AI管道，用双流LSTM网络与注意力机制融合结构化时间序列输入和新闻摘要。

Result: 模型平均AUC为0.94，整体准确率0.91，远超传统基线；消融研究表明去除注意力或降维性能适度下降，去除新闻组件AUC降至0.46。

Conclusion: 整合生成式AI与深度学习能有效改善商品价格冲击的早期检测，为经济规划和风险缓解提供实用工具，还能节省成本。

Abstract: Accurate forecasting of commodity price spikes is vital for countries with
limited economic buffers, where sudden increases can strain national budgets,
disrupt import-reliant sectors, and undermine food and energy security. This
paper introduces a hybrid forecasting framework that combines historical
commodity price data with semantic signals derived from global economic news,
using an agentic generative AI pipeline. The architecture integrates
dual-stream Long Short-Term Memory (LSTM) networks with attention mechanisms to
fuse structured time-series inputs with semantically embedded, fact-checked
news summaries collected from 1960 to 2023. The model is evaluated on a 64-year
dataset comprising normalized commodity price series and temporally aligned
news embeddings. Results show that the proposed approach achieves a mean AUC of
0.94 and an overall accuracy of 0.91 substantially outperforming traditional
baselines such as logistic regression (AUC = 0.34), random forest (AUC = 0.57),
and support vector machines (AUC = 0.47). Additional ablation studies reveal
that the removal of attention or dimensionality reduction leads to moderate
declines in performance, while eliminating the news component causes a steep
drop in AUC to 0.46, underscoring the critical value of incorporating
real-world context through unstructured text. These findings demonstrate that
integrating agentic generative AI with deep learning can meaningfully improve
early detection of commodity price shocks, offering a practical tool for
economic planning and risk mitigation in volatile market environments while
saving the very high costs of operating a full generative AI agents pipeline.

</details>


### [304] [Proactive Market Making and Liquidity Analysis for Everlasting Options in DeFi Ecosystems](https://arxiv.org/abs/2508.07068)
*Hardhik Mohanty,Giovanni Zaarour,Bhaskar Krishnamachari*

Main category: q-fin.CP

TL;DR: 本文深入分析永续期权市场，通过模拟和建模表明流动性提供者可采用有效对冲策略实现正损益，阐述其对市场增长的激励及对交易者的益处。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化金融市场中滚动合约和流动性碎片化的挑战。

Method: 使用动态主动做市商模型，通过模拟和建模进行研究。

Result: 即使在低流动性和高交易成本环境下，流动性提供者采用有效对冲策略可实现净正损益。

Conclusion: 阐述了激励流动性提供者支持永续期权市场增长的因素，强调该工具对交易者可靠且高效。

Abstract: Everlasting options, a relatively new class of perpetual financial
derivatives, have emerged to tackle the challenges of rolling contracts and
liquidity fragmentation in decentralized finance markets. This paper offers an
in-depth analysis of markets for everlasting options, modeled using a dynamic
proactive market maker. We examine the behavior of funding fees and transaction
costs across varying liquidity conditions. Using simulations and modeling, we
demonstrate that liquidity providers can aim to achieve a net positive PnL by
employing effective hedging strategies, even in challenging environments
characterized by low liquidity and high transaction costs. Additionally, we
provide insights into the incentives that drive liquidity providers to support
the growth of everlasting option markets and highlight the significant benefits
these instruments offer to traders as a reliable and efficient financial tool.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [305] [Modelling Prepayment and Default under Changing Credit Market Conditions for a Net Present Value Analysis](https://arxiv.org/abs/2508.07774)
*Quirini Lorenzo,Vannucci Luigi,Quirini Giovanni*

Main category: q-fin.RM

TL;DR: 开发模型评估特定还款计划下贷款或抵押贷款的盈利能力，聚焦RNPV指标，在统一框架评估其均值和方差。


<details>
  <summary>Details</summary>
Motivation: 金融机构面临违约和提前还款风险，受信贷市场条件影响，需评估贷款盈利能力。

Method: 开发模型，以随机净现值（RNPV）为关键指标，在统一框架下进行分析。

Result: 对个体贷款和投资组合层面的RNPV均值和方差进行评估。

Conclusion: 未在摘要中明确提及结论。

Abstract: A model is developed to assess the profitability of loans or mortgages with a
specified repayment schedule. Financial institutions face two competing risks:
default and prepayment, both influenced by the stochastic evolution of credit
market conditions. This study focuses on the Random Net Present Value (RNPV) as
a key performance metric. The analysis evaluates the mean and variance of the
RNPV at both the individual loan level and the portfolio level, within a
unified framework that accounts for borrower behavior and prevailing credit
market dynamics.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [306] [Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading](https://arxiv.org/abs/2508.07408)
*Yueyi Wang,Qiyao Wei*

Main category: q-fin.ST

TL;DR: 研究展示大语言模型在金融语义标注和阿尔法信号发现中的作用，利用推文实验，证明社交媒体情绪对金融预测有价值。


<details>
  <summary>Details</summary>
Motivation: 展示大语言模型在金融语义标注和阿尔法信号发现的独特效用。

Method: 利用公司相关推文语料，用大语言模型为高情绪强度推文分配多标签事件类别，将标签化情绪信号与1 - 7天的远期回报对齐评估。

Result: 某些事件标签持续产生负阿尔法，夏普比率低至 -0.38，信息系数超0.05，且在95%置信水平显著。

Conclusion: 将非结构化社交媒体文本转化为结构化多标签事件变量可行，社交媒体情绪是有价值但有噪声的金融预测信号，开源框架可推动算法交易研究民主化。

Abstract: In this study, we wish to showcase the unique utility of large language
models (LLMs) in financial semantic annotation and alpha signal discovery.
Leveraging a corpus of company-related tweets, we use an LLM to automatically
assign multi-label event categories to high-sentiment-intensity tweets. We
align these labeled sentiment signals with forward returns over 1-to-7-day
horizons to evaluate their statistical efficacy and market tradability. Our
experiments reveal that certain event labels consistently yield negative alpha,
with Sharpe ratios as low as -0.38 and information coefficients exceeding 0.05,
all statistically significant at the 95\% confidence level. This study
establishes the feasibility of transforming unstructured social media text into
structured, multi-label event variables. A key contribution of this work is its
commitment to transparency and reproducibility; all code and methodologies are
made publicly available. Our results provide compelling evidence that social
media sentiment is a valuable, albeit noisy, signal in financial forecasting
and underscore the potential of open-source frameworks to democratize
algorithmic trading research.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [307] [Interaction between Returns and Order Flow Imbalances: Endogeneity, Intraday Variations, and Macroeconomic News Announcements](https://arxiv.org/abs/2508.06788)
*Makoto Takahashi*

Main category: q-fin.TR

TL;DR: 研究用结构向量自回归模型分析标普500电子迷你期货合约收益与订单流失衡的互动，考虑日内市场活动变化及时间聚合的内生性，发现存在显著内生性、参数有日内变化及宏观消息影响订单提交活动。


<details>
  <summary>Details</summary>
Motivation: 研究标普500电子迷你期货合约收益与订单流失衡的互动，考虑日内市场活动变化和时间聚合的内生性问题。

Method: 使用结构向量自回归模型，按短时间间隔应用模型考虑日内变化，通过异方差识别估计结构参数处理内生性。

Result: 存在显著内生性，估计参数和脉冲响应有显著日内变化，宏观经济新闻公告前后估计参数改变。

Conclusion: 宏观经济新闻公告的影响主要由反映公共信息的订单提交活动解释。

Abstract: The study examines the interaction between returns and order flow imbalances
(differences between buy and sell orders), constructed from the best bid and
offer files of S&P 500 E-mini futures contract, using a structural vector
autoregressive model. The intraday variation in market activity is considered
by applying the model for each short interval each day, whereas the endogeneity
due to time aggregation is handled by estimating the structural parameters via
the identification through heteroskedasticity. The estimation results show that
significant endogeneity exists and that the estimated parameters and impulse
responses exhibit significant intraday variations, reflecting intense or mild
order submission activities. Further, the estimated parameters change around
macroeconomic news announcements, suggesting inactive order submission periods
exist when they occur. Overall, such announcement effects are mostly explained
by the order submission activities reflecting the public information.

</details>


### [308] [Optimal Fees for Liquidity Provision in Automated Market Makers](https://arxiv.org/abs/2508.08152)
*Steven Campbell,Philippe Bergault,Jason Milionis,Marcel Nutz*

Main category: q-fin.TR

TL;DR: 研究自动化做市商（AMM）中被动流动性提供者（LP）盈利的关键决定因素，分析LP利润随市场条件的变化，确定最优AMM费用，提出阈值型动态费用策略。


<details>
  <summary>Details</summary>
Motivation: AMM中被动LP因逆向选择面临损失，静态交易费用难以弥补，需研究LP盈利的关键因素。

Method: 构建动态简化模型，结合大规模模拟和真实市场数据，进行比较静态分析和市场数据校准。

Result: 发现正常市场条件下最优AMM费用有竞争力且稳定，高波动时期高费用可保护LP，存在吸引交易量和赚取收入、减少套利损失的权衡。

Conclusion: 阈值型动态费用策略对市场条件有强鲁棒性，能改善LP收益。

Abstract: Passive liquidity providers (LPs) in automated market makers (AMMs) face
losses due to adverse selection (LVR), which static trading fees often fail to
offset in practice. We study the key determinants of LP profitability in a
dynamic reduced-form model where an AMM operates in parallel with a centralized
exchange (CEX), traders route their orders optimally to the venue offering the
better price, and arbitrageurs exploit price discrepancies. Using large-scale
simulations and real market data, we analyze how LP profits vary with market
conditions such as volatility and trading volume, and characterize the optimal
AMM fee as a function of these conditions. We highlight the mechanisms driving
these relationships through extensive comparative statics, and confirm the
model's relevance through market data calibration. A key trade-off emerges:
fees must be low enough to attract volume, yet high enough to earn sufficient
revenues and mitigate arbitrage losses. We find that under normal market
conditions, the optimal AMM fee is competitive with the trading cost on the CEX
and remarkably stable, whereas in periods of very high volatility, a high fee
protects passive LPs from severe losses. These findings suggest that a
threshold-type dynamic fee schedule is both robust enough to market conditions
and improves LP outcomes.

</details>


### [309] [Prediction of high-frequency futures return directions based on the mean uncertainty classification methods: An application in China's future market](https://arxiv.org/abs/2508.06914)
*Ying Peng,Yifan Zhang,Xin Wang*

Main category: q-fin.TR

TL;DR: 本文聚焦中国高频期货市场短期平均回报方向预测，采用均值-不确定性分类方法，结合投资策略，实证显示该方法优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决中国高频期货市场预测建模中的数据不平衡问题。

Method: 在次线性期望（SLE）框架下采用均值-不确定性逻辑回归（mean-uncertainty LR）分类方法，进一步提出均值-不确定性支持向量机（mean-uncertainty SVM）方法，基于预测结果制定投资策略，选用中国期货市场活跃合约中前15大流动性产品的交易数据和限价订单簿数据。

Result: 与传统LR和SVM不平衡数据分类方法相比，两种均值-不确定性方法在分类指标和每笔交易平均回报方面有显著优势。

Conclusion: 均值-不确定性LR和SVM方法在解决中国高频期货市场预测的数据不平衡问题上效果良好，能带来更好的分类和回报表现。

Abstract: In this paper, we mainly focus on the prediction of short-term average return
directions in China's high-frequency futures market. As minor fluctuations with
limited amplitude and short duration are typically regarded as random noise,
only price movements of sufficient magnitude qualify as statistically
significant signals. Therefore data imbalance emerges as a key problem during
predictive modeling. From the view of data distribution imbalance, we employee
the mean-uncertainty logistic regression (mean-uncertainty LR) classification
method under the sublinear expectation (SLE) framework, and further propose the
mean-uncertainty support vector machines (mean-uncertainty SVM) method for the
prediction. Corresponding investment strategies are developed based on the
prediction results. For data selection, we utilize trading data and limit order
book data of the top 15 liquid products among the most active contracts in
China's future market. Empirical results demonstrate that comparing with
conventional LR-related and SVM-related imbalanced data classification methods,
the two mean-uncertainty approaches yields significant advantages in both
classification metrics and average returns per trade.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [310] [Federated Online Learning for Heterogeneous Multisource Streaming Data](https://arxiv.org/abs/2508.06652)
*Jingmao Li,Yuanxing Chen,Shuangge Ma,Kuangnan Fang*

Main category: stat.ML

TL;DR: 提出联邦在线学习（FOL）方法用于分布式多源流数据分析，有理论保证和模拟验证，应用效果好。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法多关注静态数据集，而现实中数据多为流式，在高维场景下给数据存储和算法设计带来挑战。

Method: 为每个数据源构建个性化模型，采用“子组”假设；使用 penalized renewable estimation 方法和高效近端梯度下降进行训练。

Result: 理论上证明了模型估计、变量选择和子组结构恢复的一致性，模拟验证有效性，在金融贷款和网络日志数据上预测性能优越。

Conclusion: 所提 FOL 方法适用于分布式多源流数据分析，能保证隐私和减少存储需求，有良好统计效率和实际应用价值。

Abstract: Federated learning has emerged as an essential paradigm for distributed
multi-source data analysis under privacy concerns. Most existing federated
learning methods focus on the ``static" datasets. However, in many real-world
applications, data arrive continuously over time, forming streaming datasets.
This introduces additional challenges for data storage and algorithm design,
particularly under high-dimensional settings. In this paper, we propose a
federated online learning (FOL) method for distributed multi-source streaming
data analysis. To account for heterogeneity, a personalized model is
constructed for each data source, and a novel ``subgroup" assumption is
employed to capture potential similarities, thereby enhancing model
performance. We adopt the penalized renewable estimation method and the
efficient proximal gradient descent for model training. The proposed method
aligns with both federated and online learning frameworks: raw data are not
exchanged among sources, ensuring data privacy, and only summary statistics of
previous data batches are required for model updates, significantly reducing
storage demands. Theoretically, we establish the consistency properties for
model estimation, variable selection, and subgroup structure recovery,
demonstrating optimal statistical efficiency. Simulations illustrate the
effectiveness of the proposed method. Furthermore, when applied to the
financial lending data and the web log data, the proposed method also exhibits
advantageous prediction performance. Results of the analysis also provide some
practical insights.

</details>


### [311] [MOCA-HESP: Meta High-dimensional Bayesian Optimization for Combinatorial and Mixed Spaces via Hyper-ellipsoid Partitioning](https://arxiv.org/abs/2508.06847)
*Lam Ngo,Huong Ha,Jeffrey Chan,Hongyu Zhang*

Main category: stat.ML

TL;DR: 本文提出MOCA - HESP用于组合和混合变量的高维贝叶斯优化，集成现有优化器开发三种实用方法，实验显示优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有高维贝叶斯优化方法主要关注连续域，组合和混合域优化仍具挑战。

Method: 提出MOCA - HESP方法，利用超椭球空间划分技术与不同分类编码器结合，用多臂老虎机技术自适应选择最优编码器，将其作为元算法集成其他优化器。

Result: 在各种合成和真实世界基准测试中，所提方法优于现有基线。

Conclusion: MOCA - HESP方法能有效解决组合和混合变量的高维贝叶斯优化问题，提高优化性能。

Abstract: High-dimensional Bayesian Optimization (BO) has attracted significant
attention in recent research. However, existing methods have mainly focused on
optimizing in continuous domains, while combinatorial (ordinal and categorical)
and mixed domains still remain challenging. In this paper, we first propose
MOCA-HESP, a novel high-dimensional BO method for combinatorial and mixed
variables. The key idea is to leverage the hyper-ellipsoid space partitioning
(HESP) technique with different categorical encoders to work with
high-dimensional, combinatorial and mixed spaces, while adaptively selecting
the optimal encoders for HESP using a multi-armed bandit technique. Our method,
MOCA-HESP, is designed as a \textit{meta-algorithm} such that it can
incorporate other combinatorial and mixed BO optimizers to further enhance the
optimizers' performance. Finally, we develop three practical BO methods by
integrating MOCA-HESP with state-of-the-art BO optimizers for combinatorial and
mixed variables: standard BO, CASMOPOLITAN, and Bounce. Our experimental
results on various synthetic and real-world benchmarks show that our methods
outperform existing baselines. Our code implementation can be found at
https://github.com/LamNgo1/moca-hesp

</details>


### [312] [Hedging with memory: shallow and deep learning with signatures](https://arxiv.org/abs/2508.02759)
*Eduardo Abi Jaber,Louis-Amand Gérard*

Main category: stat.ML

TL;DR: 研究在非马尔可夫随机波动率模型下，路径签名在对冲奇异衍生品机器学习中的应用，深度学习中签名特征优于LSTM，浅度学习中特定方法结果更优。


<details>
  <summary>Details</summary>
Motivation: 探究路径签名在非马尔可夫随机波动率模型下对冲奇异衍生品的机器学习应用。

Method: 深度学习中用签名作前馈神经网络特征；浅度学习中比较两种回归方法，一是从价格过程期望签名直接学习对冲策略，二是用签名波动率模型建模波动率。

Result: 深度学习中签名特征多数情况下优于LSTM，训练计算量小；浅度学习中用签名波动率模型解决对冲问题结果更准确稳定。

Conclusion: 路径签名在非马尔可夫随机波动率模型下对冲奇异衍生品有良好效果，特定方法优势明显。

Abstract: We investigate the use of path signatures in a machine learning context for
hedging exotic derivatives under non-Markovian stochastic volatility models. In
a deep learning setting, we use signatures as features in feedforward neural
networks and show that they outperform LSTMs in most cases, with orders of
magnitude less training compute. In a shallow learning setting, we compare two
regression approaches: the first directly learns the hedging strategy from the
expected signature of the price process; the second models the dynamics of
volatility using a signature volatility model, calibrated on the expected
signature of the volatility. Solving the hedging problem in the calibrated
signature volatility model yields more accurate and stable results across
different payoffs and volatility dynamics.

</details>


### [313] [Statistical Inference for Autoencoder-based Anomaly Detection after Representation Learning-based Domain Adaptation](https://arxiv.org/abs/2508.07049)
*Tran Tuan Kiet,Nguyen Thang Loi,Vo Nguyen Le Duy*

Main category: stat.ML

TL;DR: 提出 STAND - DA 框架用于基于表示学习的领域适应后进行统计严谨的基于自编码器的异常检测，开发 GPU 加速的选择性推理实现，实验验证其理论结果和计算效率。


<details>
  <summary>Details</summary>
Motivation: 异常检测在目标数据有限的领域性能可能下降，领域适应虽能转移知识但会引入额外不确定性，难以从检测结果得出统计有效结论。

Method: 提出 STAND - DA 框架，基于选择性推理框架计算有效 p 值并控制误报率，开发 GPU 加速的选择性推理实现。

Result: 在合成和真实数据集上的大量实验验证了 STAND - DA 方法的理论结果和计算效率。

Conclusion: STAND - DA 方法在基于表示学习的领域适应后的异常检测中是有效的，GPU 加速实现使选择性推理在现代大规模深度架构中切实可行。

Abstract: Anomaly detection (AD) plays a vital role across a wide range of domains, but
its performance might deteriorate when applied to target domains with limited
data. Domain Adaptation (DA) offers a solution by transferring knowledge from a
related source domain with abundant data. However, this adaptation process can
introduce additional uncertainty, making it difficult to draw statistically
valid conclusions from AD results. In this paper, we propose STAND-DA -- a
novel framework for statistically rigorous Autoencoder-based AD after
Representation Learning-based DA. Built on the Selective Inference (SI)
framework, STAND-DA computes valid $p$-values for detected anomalies and
rigorously controls the false positive rate below a pre-specified level
$\alpha$ (e.g., 0.05). To address the computational challenges of applying SI
to deep learning models, we develop the GPU-accelerated SI implementation,
significantly enhancing both scalability and runtime performance. This
advancement makes SI practically feasible for modern, large-scale deep
architectures. Extensive experiments on synthetic and real-world datasets
validate the theoretical results and computational efficiency of the proposed
STAND-DA method.

</details>


### [314] [Membership Inference Attacks with False Discovery Rate Control](https://arxiv.org/abs/2508.07066)
*Chenxu Zhao,Wei Qian,Aobo Chen,Mengdi Huai*

Main category: stat.ML

TL;DR: 本文针对现有成员推理攻击（MIAs）方法难以保证误发现率（FDR）的问题，设计了一种能提供FDR保证的新型MIA方法，并通过理论分析和实验验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有MIAs方法在提供误发现率保证方面存在局限，而保证FDR具有挑战性，因此需要设计新的方法。

Method: 设计了一种新型成员推理攻击方法，可作为包装器与现有MIA方法后验集成，同时提供FDR控制。

Result: 该方法能提供FDR保证，还能提供将真实非成员数据标记为成员数据的边际概率保证，理论分析和多场景实验验证了其性能。

Conclusion: 所设计的方法能有效解决现有MIAs方法在FDR保证方面的问题，具有良好性能。

Abstract: Recent studies have shown that deep learning models are vulnerable to
membership inference attacks (MIAs), which aim to infer whether a data record
was used to train a target model or not. To analyze and study these
vulnerabilities, various MIA methods have been proposed. Despite the
significance and popularity of MIAs, existing works on MIAs are limited in
providing guarantees on the false discovery rate (FDR), which refers to the
expected proportion of false discoveries among the identified positive
discoveries. However, it is very challenging to ensure the false discovery rate
guarantees, because the underlying distribution is usually unknown, and the
estimated non-member probabilities often exhibit interdependence. To tackle the
above challenges, in this paper, we design a novel membership inference attack
method, which can provide the guarantees on the false discovery rate.
Additionally, we show that our method can also provide the marginal probability
guarantee on labeling true non-member data as member data. Notably, our method
can work as a wrapper that can be seamlessly integrated with existing MIA
methods in a post-hoc manner, while also providing the FDR control. We perform
the theoretical analysis for our method. Extensive experiments in various
settings (e.g., the black-box setting and the lifelong learning setting) are
also conducted to verify the desirable performance of our method.

</details>


### [315] [Stochastic dynamics learning with state-space systems](https://arxiv.org/abs/2508.07876)
*Juan-Pablo Ortega,Florian Rossmannek*

Main category: stat.ML

TL;DR: 本文统一处理确定和随机环境下的渐消记忆与回声状态属性，研究状态空间系统，提出随机回声状态的新视角，拓展和推广了先前工作，为时序数据生成建模奠定基础。


<details>
  <summary>Details</summary>
Motivation: 推动储层计算（RC）的理论基础发展，解释RC模型在无严格收缩条件下的实证成功原因。

Method: 统一处理确定和随机环境下的渐消记忆与回声状态属性，研究状态空间系统，对随机回声状态进行评估并提出基于概率分布空间吸引子动力学的新视角。

Result: 表明渐消记忆和解决方案稳定性普遍成立，提出新的分布视角形成丰富连贯的理论，拓展和推广了先前工作。

Conclusion: 为确定和随机状态下的时序数据可靠生成建模奠定了基础。

Abstract: This work advances the theoretical foundations of reservoir computing (RC) by
providing a unified treatment of fading memory and the echo state property
(ESP) in both deterministic and stochastic settings. We investigate state-space
systems, a central model class in time series learning, and establish that
fading memory and solution stability hold generically -- even in the absence of
the ESP -- offering a robust explanation for the empirical success of RC models
without strict contractivity conditions. In the stochastic case, we critically
assess stochastic echo states, proposing a novel distributional perspective
rooted in attractor dynamics on the space of probability distributions, which
leads to a rich and coherent theory. Our results extend and generalize previous
work on non-autonomous dynamical systems, offering new insights into causality,
stability, and memory in RC models. This lays the groundwork for reliable
generative modeling of temporal data in both deterministic and stochastic
regimes.

</details>


### [316] [Meta Off-Policy Estimation](https://arxiv.org/abs/2508.07914)
*Olivier Jeunen*

Main category: stat.ML

TL;DR: 本文提出用相关固定效应元分析框架结合一组离线策略评估（OPE）估计量及其置信区间，在模拟和真实数据上验证该方法统计效率优于现有单个估计量。


<details>
  <summary>Details</summary>
Motivation: 现有众多OPE估计量，Doubly Robust方法是结合基于价值和基于策略估计量的突出策略，本文从另一个角度结合一组OPE估计量以获得更准确估计。

Method: 利用相关固定效应元分析框架，明确考虑因共享数据在估计量间产生的依赖关系，得到目标策略价值的最佳线性无偏估计（BLUE）及保守置信区间。

Result: 在模拟和真实数据上验证，该方法统计效率优于现有单个估计量。

Conclusion: 所提出的结合OPE估计量的方法能提高统计效率，是一种有效的策略。

Abstract: Off-policy estimation (OPE) methods enable unbiased offline evaluation of
recommender systems, directly estimating the online reward some target policy
would have obtained, from offline data and with statistical guarantees. The
theoretical elegance of the framework combined with practical successes have
led to a surge of interest, with many competing estimators now available to
practitioners and researchers. Among these, Doubly Robust methods provide a
prominent strategy to combine value- and policy-based estimators.
  In this work, we take an alternative perspective to combine a set of OPE
estimators and their associated confidence intervals into a single, more
accurate estimate. Our approach leverages a correlated fixed-effects
meta-analysis framework, explicitly accounting for dependencies among
estimators that arise due to shared data. This yields a best linear unbiased
estimate (BLUE) of the target policy's value, along with an appropriately
conservative confidence interval that reflects inter-estimator correlation. We
validate our method on both simulated and real-world data, demonstrating
improved statistical efficiency over existing individual estimators.

</details>


### [317] [Gaussian Approximation for Two-Timescale Linear Stochastic Approximation](https://arxiv.org/abs/2508.07928)
*Bogdan Butyrin,Artemy Rubtsov,Alexey Naumov,Vladimir Ulyanov,Sergey Samsonov*

Main category: stat.ML

TL;DR: 本文为线性双时间尺度随机逼近（TTSA）算法的正态逼近精度建立了非渐近界，分析了不同迭代方式下的逼近情况并给出高阶矩界。


<details>
  <summary>Details</summary>
Motivation: 为线性TTSA算法的正态逼近精度建立非渐近界，研究不同迭代方式下的逼近情况。

Method: 推导概率分布凸距离下的正态逼近界，分析快慢时间尺度的相互作用。

Result: 发现快慢时间尺度存在非平凡相互作用，最后迭代的正态逼近率随时间尺度分离增加而提高，在Polyak - Ruppert平均设置下降低，还给出了线性TTSA算法误差的高阶矩界。

Conclusion: 成功建立非渐近界，揭示了快慢时间尺度对正态逼近率的影响，高阶矩界有独立研究价值。

Abstract: In this paper, we establish non-asymptotic bounds for accuracy of normal
approximation for linear two-timescale stochastic approximation (TTSA)
algorithms driven by martingale difference or Markov noise. Focusing on both
the last iterate and Polyak-Ruppert averaging regimes, we derive bounds for
normal approximation in terms of the convex distance between probability
distributions. Our analysis reveals a non-trivial interaction between the fast
and slow timescales: the normal approximation rate for the last iterate
improves as the timescale separation increases, while it decreases in the
Polyak-Ruppert averaged setting. We also provide the high-order moment bounds
for the error of linear TTSA algorithm, which may be of independent interest.

</details>


### [318] [Likelihood Ratio Tests by Kernel Gaussian Embedding](https://arxiv.org/abs/2508.07982)
*Leonardo V. Santoro,Victor M. Panaretos*

Main category: stat.ML

TL;DR: 提出基于核均值和核协方差嵌入的非参数双样本检验，构建似然比统计量，引入正则化版本，证明一致性和功效保证，实验显示性能优。


<details>
  <summary>Details</summary>
Motivation: 提出新的核基非参数双样本检验方法。

Method: 结合核均值和核协方差嵌入，基于高斯嵌入的相对熵构建似然比检验统计量，引入正则化版本并通过置换校准。

Result: 证明了一致性，在温和条件下有统一功效保证，实验显示比现有方法在高维和弱信号场景有显著功效提升。

Conclusion: 该框架统一并扩展了基于谱正则化MMD的先前方法，新检验方法性能优越。

Abstract: We propose a novel kernel-based nonparametric two-sample test, employing the
combined use of kernel mean and kernel covariance embedding. Our test builds on
recent results showing how such combined embeddings map distinct probability
measures to mutually singular Gaussian measures on the kernel's RKHS.
Leveraging this result, we construct a test statistic based on the relative
entropy between the Gaussian embeddings, i.e.\ the likelihood ratio. The
likelihood ratio is specifically tailored to detect equality versus singularity
of two Gaussians, and satisfies a ``$0/\infty$" law, in that it vanishes under
the null and diverges under the alternative. To implement the test in finite
samples, we introduce a regularised version, calibrated by way of permutation.
We prove consistency, establish uniform power guarantees under mild conditions,
and discuss how our framework unifies and extends prior approaches based on
spectrally regularized MMD. Empirical results on synthetic and real data
demonstrate remarkable gains in power compared to state-of-the-art methods,
particularly in high-dimensional and weak-signal regimes.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [319] [A Score-based Diffusion Model Approach for Adaptive Learning of Stochastic Partial Differential Equation Solutions](https://arxiv.org/abs/2508.06834)
*Toan Huynh,Ruth Lopez Fajardo,Guannan Zhang,Lili Ju,Feng Bao*

Main category: stat.CO

TL;DR: 提出用基于分数的扩散模型在递归贝叶斯推理框架下自适应学习随机偏微分方程（SPDEs）时变解的新框架，引入集成分数滤波器提升计算效率，实验证明方法准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: SPDEs在模拟不确定环境下复杂物理系统中很重要，但数值解常因物理知识不完整和环境多变存在模型误差和精度降低问题。

Method: 将控制物理原理编码到扩散模型的分数函数中，通过反向随机微分方程中基于似然的校正纳入观测信息，引入集成分数滤波器提升高维计算效率。

Result: 在基准SPDEs的数值实验中，所提方法在稀疏和噪声观测下展现出准确性和鲁棒性。

Conclusion: 所提框架能有效解决SPDEs数值解的问题，且具有较好性能。

Abstract: We propose a novel framework for adaptively learning the time-evolving
solutions of stochastic partial differential equations (SPDEs) using
score-based diffusion models within a recursive Bayesian inference setting.
SPDEs play a central role in modeling complex physical systems under
uncertainty, but their numerical solutions often suffer from model errors and
reduced accuracy due to incomplete physical knowledge and environmental
variability. To address these challenges, we encode the governing physics into
the score function of a diffusion model using simulation data and incorporate
observational information via a likelihood-based correction in a reverse-time
stochastic differential equation. This enables adaptive learning through
iterative refinement of the solution as new data becomes available. To improve
computational efficiency in high-dimensional settings, we introduce the
ensemble score filter, a training-free approximation of the score function
designed for real-time inference. Numerical experiments on benchmark SPDEs
demonstrate the accuracy and robustness of the proposed method under sparse and
noisy observations.

</details>


### [320] [An Approximate Maximum Likelihood Estimator for Discretely Observed Linear Birth-and-Death Processes](https://arxiv.org/abs/2508.07527)
*Xiaochen Long,Marek Kimmel*

Main category: stat.CO

TL;DR: 提出基于高斯近似的线性生死过程（LBDPs）近似最大似然估计器，有计算优势，在模拟和实际数据应用中表现好。


<details>
  <summary>Details</summary>
Motivation: 离散观测数据的LBDPs参数估计计算要求高，因不规则采样、噪声和缺失值等问题。

Method: 基于高斯近似转移概率提出近似最大似然估计器，将估计转化为单变量优化问题。

Result: 模拟中近似MLE在速度和精度上优于高斯和鞍点估计器；应用于纵向克隆造血数据能产生有生物学意义的生长估计，且对数据缩放不变。

Conclusion: 提出的近似MLE有计算优势，适合现实应用如变异等位基因频率分析。

Abstract: Linear birth-and-death processes (LBDPs) are foundational stochastic models
in population dynamics, evolutionary biology, and hematopoiesis. Estimating
parameters from discretely observed data is computationally demanding due to
irregular sampling, noise, and missing values. We propose a novel approximate
maximum likelihood estimator (MLE) for LBDPs based on a Gaussian approximation
to transition probabilities. The approach transforms estimation into a
univariate optimization problem, achieving substantial computational gains
without sacrificing accuracy.
  Through simulations, we show that the approximate MLE outperforms Gaussian
and saddlepoint-based estimators in speed and precision under realistic noise
and sparsity. Applied to longitudinal clonal hematopoiesis data, the method
produces biologically meaningful growth estimates even with noisy,
compositional input. Unlike Gaussian and saddlepoint approximations, our
estimator is invariant to data scaling, making it ideal for real-world
applications such as variant allele frequency analyses.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [321] [Leveraging GNN to Enhance MEF Method in Predicting ENSO](https://arxiv.org/abs/2508.07410)
*Saghar Ganji,Mohammad Naisipour*

Main category: physics.ao-ph

TL;DR: 本文提出基于图分析框架优化ENSO预测，去除噪声并强调集合一致性，提升预测技巧，该方法稳定且模型无关。


<details>
  <summary>Details</summary>
Motivation: 以往MEF模型未对单个集合成员单独加权或测试，限制了对高性能分散预测的优化利用，需改进ENSO长期可靠预测方法。

Method: 采用基于图的分析方法，构建无向图，通过RMSE和相关性衡量相似度，利用社区检测方法得到20个成员的优化子集，最终预测通过对该子集求平均得到。

Result: 该方法提高了预测技巧，图选择在表现最佳者中有稳健统计特征，虽不总是优于基线MEF，但输出更稳定一致，尤其在复合长期预测中。

Conclusion: 该方法模型无关，可应用于其他有大量集合输出的预测模型。

Abstract: Reliable long-lead forecasting of the El Nino Southern Oscillation (ENSO)
remains a long-standing challenge in climate science. The previously developed
Multimodal ENSO Forecast (MEF) model uses 80 ensemble predictions by two
independent deep learning modules: a 3D Convolutional Neural Network (3D-CNN)
and a time-series module. In their approach, outputs of the two modules are
combined using a weighting strategy wherein one is prioritized over the other
as a function of global performance. Separate weighting or testing of
individual ensemble members did not occur, however, which may have limited the
model to optimize the use of high-performing but spread-out forecasts. In this
study, we propose a better framework that employs graph-based analysis to
directly model similarity between all 80 members of the ensemble. By
constructing an undirected graph whose vertices are ensemble outputs and whose
weights on edges measure similarity (via RMSE and correlation), we identify and
cluster structurally similar and accurate predictions. From which we obtain an
optimized subset of 20 members using community detection methods. The final
prediction is then obtained by averaging this optimized subset. This method
improves the forecast skill through noise removal and emphasis on ensemble
coherence. Interestingly, our graph-based selection shows robust statistical
characteristics among top performers, offering new ensemble behavior insights.
In addition, we observe that while the GNN-based approach does not always
outperform the baseline MEF under every scenario, it produces more stable and
consistent outputs, particularly in compound long-lead situations. The approach
is model-agnostic too, suggesting that it can be applied directly to other
forecasting models with gargantuan ensemble outputs, such as statistical,
physical, or hybrid models.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [322] [Generative AI for Intent-Driven Network Management in 6G: A Case Study on Hierarchical Learning Approach](https://arxiv.org/abs/2508.06616)
*Md Arafat Habib,Medhat Elsayed,Yigit Ozcan,Pedro Enrique Iturria-Rivera,Majid Bavand,Melike Erol-Kantarci*

Main category: cs.NI

TL;DR: 文章对基于大语言模型的解耦式无线接入网环境下的意图驱动网络架构进行了全面调研，提出跨三个阶段引入生成式AI的分层框架，并通过基于Mamba的案例研究证明其优于传统架构。


<details>
  <summary>Details</summary>
Motivation: 随着6G出现，移动网络变得更加异构和动态，需要先进自动化管理，意图驱动网络可解决该问题，大语言模型能增强此过程，因此对基于大语言模型的解耦式无线接入网环境下的意图驱动网络架构进行全面调研很有必要。

Method: 提出一个跨意图处理、验证和执行三个关键阶段引入生成式AI的分层框架，并基于最新生成式AI架构Mamba进行案例研究。

Result: 案例研究表明，所提出的生成式AI驱动的架构通过智能自动化提高了网络性能，超越了传统意图驱动网络架构。

Conclusion: 所提出的分层框架有效，能通过智能自动化提升网络性能，优于传统架构。

Abstract: With the emergence of 6G, mobile networks are becoming increasingly
heterogeneous and dynamic, necessitating advanced automation for efficient
management. Intent-Driven Networks (IDNs) address this by translating
high-level intents into optimization policies. Large Language Models (LLMs) can
enhance this process by understanding complex human instructions to enable
adaptive, intelligent automation. Given the rapid advancements in Generative AI
(GenAI), a comprehensive survey of LLM-based IDN architectures in disaggregated
Radio Access Network (RAN) environments is both timely and critical. This
article provides such a survey, along with a case study on a hierarchical
learning-enabled IDN architecture that integrates GenAI across three key
stages: intent processing, intent validation, and intent execution. Unlike most
existing approaches that apply GenAI in the form of LLMs for intent processing
only, we propose a hierarchical framework that introduces GenAI across all
three stages of IDN. To demonstrate the effectiveness of the proposed IDN
management architecture, we present a case study based on the latest GenAI
architecture named Mamba. The case study shows how the proposed GenAI-driven
architecture enhances network performance through intelligent automation,
surpassing the performance of the conventional IDN architectures.

</details>


### [323] [Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization](https://arxiv.org/abs/2508.07001)
*Myeung Suk Oh,Zhiyao Zhang,FNU Hairi,Alvaro Velasquez,Jia Liu*

Main category: cs.NI

TL;DR: 现有多智能体强化学习处理随机接入有局限，本文采用全去中心化架构设计算法，证明收敛性，实验表明其能提升网络性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于随机接入的MAC协议难以减少冲突和保证公平性，且多智能体强化学习的集中训练方式不适用于实际应用。

Method: 采用全去中心化多智能体强化学习架构，基于演员 - 评论家网络设计算法，仅交换本地奖励以减少通信开销，并给出全局收敛性的理论证明。

Result: 数值实验显示，相比其他基线，所提算法能显著提升随机接入网络性能。

Conclusion: 所提全去中心化多智能体强化学习算法在随机接入网络性能优化上效果显著。

Abstract: With wireless devices increasingly forming a unified smart network for
seamless, user-friendly operations, random access (RA) medium access control
(MAC) design is considered a key solution for handling unpredictable data
traffic from multiple terminals. However, it remains challenging to design an
effective RA-based MAC protocol to minimize collisions and ensure transmission
fairness across the devices. While existing multi-agent reinforcement learning
(MARL) approaches with centralized training and decentralized execution (CTDE)
have been proposed to optimize RA performance, their reliance on centralized
training and the significant overhead required for information collection can
make real-world applications unrealistic. In this work, we adopt a fully
decentralized MARL architecture, where policy learning does not rely on
centralized tasks but leverages consensus-based information exchanges across
devices. We design our MARL algorithm over an actor-critic (AC) network and
propose exchanging only local rewards to minimize communication overhead.
Furthermore, we provide a theoretical proof of global convergence for our
approach. Numerical experiments show that our proposed MARL algorithm can
significantly improve RA network performance compared to other baselines.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [324] [Noise-Aware Generative Microscopic Traffic Simulation](https://arxiv.org/abs/2508.07453)
*Vindula Jayawardana,Catherine Tang,Junyi Ji,Jonah Philion,Xue Bin Peng,Cathy Wu*

Main category: eess.SY

TL;DR: 提出I - 24 MOTION Scenario Dataset (I24 - MSD)数据集，用含噪声感知损失函数的生成模型处理，结果显示该模型优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 传统人类驾驶模拟模型无法处理复杂交通现象，现有数据集缺乏标准化且未反映现实感知的噪声和不完美，需新数据集和模型。

Method: 创建I24 - MSD标准化数据集，借鉴计算机视觉中噪声感知学习策略，用含噪声感知损失函数适配自动驾驶社区现有生成模型。

Result: 含噪声感知损失函数的模型在真实性上优于传统基线，且能利用数据不完美性。

Conclusion: I24 - MSD是迈向新一代微观交通仿真的垫脚石，能应对现实挑战并满足实际需求。

Abstract: Accurately modeling individual vehicle behavior in microscopic traffic
simulation remains a key challenge in intelligent transportation systems, as it
requires vehicles to realistically generate and respond to complex traffic
phenomena such as phantom traffic jams. While traditional human driver
simulation models offer computational tractability, they do so by abstracting
away the very complexity that defines human driving. On the other hand, recent
advances in infrastructure-mounted camera-based roadway sensing have enabled
the extraction of vehicle trajectory data, presenting an opportunity to shift
toward generative, agent-based models. Yet, a major bottleneck remains: most
existing datasets are either overly sanitized or lack standardization, failing
to reflect the noisy, imperfect nature of real-world sensing. Unlike data from
vehicle-mounted sensors-which can mitigate sensing artifacts like occlusion
through overlapping fields of view and sensor fusion-infrastructure-based
sensors surface a messier, more practical view of challenges that traffic
engineers encounter. To this end, we present the I-24 MOTION Scenario Dataset
(I24-MSD)-a standardized, curated dataset designed to preserve a realistic
level of sensor imperfection, embracing these errors as part of the learning
problem rather than an obstacle to overcome purely from preprocessing. Drawing
from noise-aware learning strategies in computer vision, we further adapt
existing generative models in the autonomous driving community for I24-MSD with
noise-aware loss functions. Our results show that such models not only
outperform traditional baselines in realism but also benefit from explicitly
engaging with, rather than suppressing, data imperfection. We view I24-MSD as a
stepping stone toward a new generation of microscopic traffic simulation that
embraces the real-world challenges and is better aligned with practical needs.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [325] [A Rule-Based Approach to Specifying Preferences over Conflicting Facts and Querying Inconsistent Knowledge Bases](https://arxiv.org/abs/2508.07742)
*Meghyn Bienvenu,Camille Bourgaux,Katsumi Inoue,Robin Jean*

Main category: cs.LO

TL;DR: 本文针对不一致知识库，引入基于规则的框架来指定和计算冲突事实间的优先级关系，考虑无环性问题并给出去除循环的方法，还进行了初步实现和实验评估。


<details>
  <summary>Details</summary>
Motivation: 现有工作未充分解决如何指定不一致知识库中事实间优先级偏好的问题。

Method: 引入基于规则的框架，用答案集编程评估偏好规则，应用循环去除技术获得优先级关系。

Result: 对框架进行了初步实现和实验评估。

Conclusion: 该框架有助于构建查询不一致知识库的端到端系统。

Abstract: Repair-based semantics have been extensively studied as a means of obtaining
meaningful answers to queries posed over inconsistent knowledge bases (KBs).
While several works have considered how to exploit a priority relation between
facts to select optimal repairs, the question of how to specify such
preferences remains largely unaddressed. This motivates us to introduce a
declarative rule-based framework for specifying and computing a priority
relation between conflicting facts. As the expressed preferences may contain
undesirable cycles, we consider the problem of determining when a set of
preference rules always yields an acyclic relation, and we also explore a
pragmatic approach that extracts an acyclic relation by applying various cycle
removal techniques. Towards an end-to-end system for querying inconsistent KBs,
we present a preliminary implementation and experimental evaluation of the
framework, which employs answer set programming to evaluate the preference
rules, apply the desired cycle resolution techniques to obtain a priority
relation, and answer queries under prioritized-repair semantics.

</details>


### [326] [Presburger Functional Synthesis: Complexity and Tractable Normal Forms](https://arxiv.org/abs/2508.07207)
*S. Akshay,A. R. Balasubramanian,Supratik Chakraborty,Georg Zetzsche*

Main category: cs.LO

TL;DR: 本文研究Presburger算术的函数合成问题（PFnS），证明其可在EXPTIME内求解，给出匹配的指数下界，分析与布尔函数合成（BFnS）的关系，找到特殊范式PSyNF保证PFnS的多项式时间和规模可解性，还给出一种更易检查但简洁性较差的范式。


<details>
  <summary>Details</summary>
Motivation: 针对不同理论下的函数合成问题研究现状，对Presburger算术的函数合成问题展开研究。

Method: 理论分析和证明，包括复杂度分析、范式性质证明等。

Result: 证明PFnS可在EXPTIME内求解，给出指数下界；发现PFnS单输入单输出与BFnS难度相当；找到PSyNF范式保证多项式可解性；给出更易检查但简洁性差的范式。

Conclusion: 对Presburger算术的函数合成问题有了较为深入的研究，明确了复杂度、与BFnS关系以及不同范式的性质和作用。

Abstract: Given a relational specification between inputs and outputs as a logic
formula, the problem of functional synthesis is to automatically synthesize a
function from inputs to outputs satisfying the relation. Recently, a rich line
of work has emerged tackling this problem for specifications in different
theories, from Boolean to general first-order logic. In this paper, we launch
an investigation of this problem for the theory of Presburger Arithmetic, that
we call Presburger Functional Synthesis (PFnS). We show that PFnS can be solved
in EXPTIME and provide a matching exponential lower bound. This is unlike the
case for Boolean functional synthesis (BFnS), where only conditional
exponential lower bounds are known. Further, we show that PFnS for one input
and one output variable is as hard as BFnS in general. We then identify a
special normal form, called PSyNF, for the specification formula that
guarantees poly-time and poly-size solvability of PFnS. We prove several
properties of PSyNF, including how to check and compile to this form, and
conditions under which any other form that guarantees poly-time solvability of
PFnS can be compiled in poly-time to PSyNF. Finally, we identify a syntactic
normal form that is easier to check but is exponentially less succinct than
PSyNF.

</details>


### [327] [From Knowledge to Conjectures: A Modal Framework for Reasoning about Hypotheses](https://arxiv.org/abs/2508.07304)
*Fabio Vitali*

Main category: cs.LO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces a new family of cognitive modal logics designed to
formalize conjectural reasoning: a modal system in which cognitive contexts
extend known facts with hypothetical assumptions to explore their consequences.
Unlike traditional doxastic and epistemic systems, conjectural logics rely on a
principle, called Axiom C ($\varphi \rightarrow \Box\varphi$), that ensures
that all established facts are preserved across hypothetical layers. While
Axiom C was dismissed in the past due to its association with modal collapse,
we show that the collapse only arises under classical and bivalent assumptions,
and specifically in the presence of Axiom T. Hence we avoid Axiom T and adopt a
paracomplete semantic framework, grounded in Weak Kleene logic or Description
Logic, where undefined propositions coexist with modal assertions. This
prevents the modal collapse and guarantees a layering to distinguish between
factual and conjectural statements. Under this framework we define new modal
systems, e.g., KC and KDC, and show that they are complete, decidable, and
robust under partial knowledge. Finally, we introduce a dynamic operation,
$\mathsf{settle}(\varphi)$, which formalizes the transition from conjecture to
accepted fact, capturing the event of the update of a world's cognitive state
through the resolution of uncertainty.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [328] [Negative redispatch power for green hydrogen production: Game changer or lame duck? A German perspective](https://arxiv.org/abs/2508.06500)
*Jonathan Brandt,Astrid Bensmann,Richard Hanke-Rauschenbach*

Main category: q-fin.GN

TL;DR: 德国输电网络运营商2024年底安装区域再调度市场，研究不同价格水平对绿氢生产成本及电解槽参与市场的影响，低价格可降成本，高价格有负面影响。


<details>
  <summary>Details</summary>
Motivation: 探讨区域再调度市场不同价格水平对绿氢生产成本及电解槽参与市场激励的影响。

Method: 使用历史再调度时间序列评估各种电力购买情景。

Result: 低价格水平可显著降低生产成本，激励电解槽选址；高价格水平会抵消成本降低带来的竞争力提升，阻碍市场参与。

Conclusion: 区域再调度市场价格水平对绿氢生产和电解槽市场参与有重要影响。

Abstract: Following years of controversial discussions about the risks of market-based
redispatch, the German transmission network operators finally installed
regional redispatch markets by the end of 2024. Since water electrolysers are
eligible market participants, the otherwise downwards redispatched renewable
energy can be used for green hydrogen production in compliance with European
law. To show how different price levels in regional redispatch markets affect
green hydrogen production cost and thus the incentive for electrolyser market
participation, we use historic redispatch time series and evaluate various
power purchase scenarios. Our results show that low price levels can lead to
notable production cost reductions, potentially counteracting uncertainties in
redispatch power availability and thus incentivising system-beneficial
electrolyser siting. In contrast, the possibility of high price levels can
nullify an increase in the competitiveness of German and European green
hydrogen through production cost reductions and discourage market
participation.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [329] [Civil Servants as Builders: Enabling Non-IT Staff to Develop Secure Python and R Tools](https://arxiv.org/abs/2508.07203)
*Prashant Sharma*

Main category: cs.HC

TL;DR: 本文介绍了一个开源平台，使非 IT 岗位的技术型公务员能在政府网络内开发、审核和部署特定领域应用，填补了相关研究空白。


<details>
  <summary>Details</summary>
Motivation: 现有数字政府研究很少关注非 IT 岗位但有编程技能的公务员，缺乏让他们安全部署工作的途径。

Method: 结合 Jupyter Notebooks、预批准的开源库和轻量级治理，通过沙盒化、可审计的工作流程实现。

Result: 该平台能在制度约束下工作，避免供应商锁定，保留和提升公务员编程技能。

Conclusion: 此平台填补了关键空白，为公共部门技能保留、适应能力和自下而上的数字化转型提供可复制模型。

Abstract: Current digital government literature focuses on professional in-house IT
teams, specialized digital service teams, vendor-developed systems, or
proprietary low-code/no-code tools. Almost no scholarship addresses a growing
middle ground: technically skilled civil servants outside formal IT roles who
can write real code but lack a sanctioned, secure path to deploy their work.
This paper introduces a limits-aware, open-source and replicable platform that
enables such public servants to develop, peer review, and deploy small-scale,
domain-specific applications within government networks via a sandboxed,
auditable workflow. By combining Jupyter Notebooks, preapproved open-source
libraries, and lightweight governance, the platform works within institutional
constraints such as procurement rules and IT security policies while avoiding
vendor lock-in. Unlike low/no-code approaches, it preserves and enhances civil
servants' programming skills, keeping them technically competitive with their
private-sector peers. This contribution fills a critical gap, offering a
replicable model for public-sector skill retention, resilience, and bottom-up
digital transformation.

</details>


### [330] [ChatGPT on the Road: Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience](https://arxiv.org/abs/2508.08101)
*Yeana Lee Bond,Mungyeong Choe,Baker Kasim Hasan,Arsh Siddiqui,Myounghoon Jeon*

Main category: cs.HC

TL;DR: 研究探索基于ChatGPT的车载对话代理，实验表明其能提升驾驶安全性和用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统车载对话代理依赖预脚本提示或有限语音命令，限制自然交互，需解决此问题。

Method: 40名司机使用基于运动的驾驶模拟器，进行无代理、预脚本代理和基于ChatGPT代理三种条件对比实验。

Result: 基于ChatGPT的代理使驾驶表现更稳定，主观评价更高，对话主题多样。

Conclusion: 大语言模型驱动的车载对话代理可通过自然、丰富上下文的交互提升驾驶安全和用户体验。

Abstract: Studies on in-vehicle conversational agents have traditionally relied on
pre-scripted prompts or limited voice commands, constraining natural
driver-agent interaction. To resolve this issue, the present study explored the
potential of a ChatGPT-based in-vehicle agent capable of carrying continuous,
multi-turn dialogues. Forty drivers participated in our experiment using a
motion-based driving simulator, comparing three conditions (No agent,
Pre-scripted agent, and ChatGPT-based agent) as a within-subjects variable.
Results showed that the ChatGPT-based agent condition led to more stable
driving performance across multiple metrics. Participants demonstrated lower
variability in longitudinal acceleration, lateral acceleration, and lane
deviation compared to the other two conditions. In subjective evaluations, the
ChatGPT-based agent also received significantly higher ratings in competence,
animacy, affective trust, and preference compared to the Pre-scripted agent.
Our thematic analysis of driver-agent conversations revealed diverse
interaction patterns in topics, including driving assistance/questions,
entertainment requests, and anthropomorphic interactions. Our results highlight
the potential of LLM-powered in-vehicle conversational agents to enhance
driving safety and user experience through natural, context-rich interactions.

</details>


### [331] [Early Explorations of Recommender Systems for Physical Activity and Well-being](https://arxiv.org/abs/2508.07980)
*Alan Said*

Main category: cs.HC

TL;DR: 文章介绍有形推荐的概念框架，指出传统推荐逻辑应用局限并给出未来系统建议。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统指导身体行动，出现用户对建议的解读、信任和响应问题，需新框架。

Method: 提出三个设计维度，结合实例和设计反思。

Result: 明确将传统推荐逻辑应用于具身场景的关键局限。

Conclusion: 指明未来系统可支持长期福祉、行为对齐和社会责任个性化。

Abstract: As recommender systems increasingly guide physical actions, often through
wearables and coaching tools, new challenges arise around how users interpret,
trust, and respond to this advice. This paper introduces a conceptual framework
for tangible recommendations that influence users' bodies, routines, and
well-being. We describe three design dimensions: trust and interpretation,
intent alignment, and consequence awareness. These highlight key limitations in
applying conventional recommender logic to embodied settings. Through examples
and design reflections, we outline how future systems can support long-term
well-being, behavioral alignment, and socially responsible personalization.

</details>


### [332] [Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators](https://arxiv.org/abs/2508.06846)
*Hyo Jin Do,Rachel Ostrand,Werner Geyer,Keerthiram Murugesan,Dennis Wei,Justin Weisz*

Main category: cs.HC

TL;DR: 文章通过实验对比不同设计策略对传达大语言模型事实性得分的效果，发现按事实性得分对回复短语进行颜色编码的设计更受参与者青睐和信任，还为开发者和设计师提供实用设计指南。


<details>
  <summary>Details</summary>
Motivation: 现有研究在如何有效向用户传达大语言模型生成内容的事实性信息方面存在不足。

Method: 进行两个基于场景的实验，让208名参与者对不同设计策略进行评估，指标包括信任度、验证回复准确性的难易程度和偏好。

Result: 参与者更偏好和信任按事实性得分对回复中所有短语进行颜色编码的设计，且认为这种设计更易验证回复准确性。

Conclusion: 研究为大语言模型应用开发者和设计师提供实用设计指南，有助于校准用户信任、契合用户偏好并增强用户审查模型输出的能力。

Abstract: Large language models (LLMs) are susceptible to generating inaccurate or
false information, often referred to as "hallucinations" or "confabulations."
While several technical advancements have been made to detect hallucinated
content by assessing the factuality of the model's responses, there is still
limited research on how to effectively communicate this information to users.
To address this gap, we conducted two scenario-based experiments with a total
of 208 participants to systematically compare the effects of various design
strategies for communicating factuality scores by assessing participants'
ratings of trust, ease in validating response accuracy, and preference. Our
findings reveal that participants preferred and trusted a design in which all
phrases within a response were color-coded based on factuality scores.
Participants also found it easier to validate accuracy of the response in this
style compared to a baseline with no style applied. Our study offers practical
design guidelines for LLM application developers and designers, aimed at
calibrating user trust, aligning with user preferences, and enhancing users'
ability to scrutinize LLM outputs.

</details>


### [333] [Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust](https://arxiv.org/abs/2508.07095)
*Hyo Jin Do,Werner Geyer*

Main category: cs.HC

TL;DR: 研究不同事实性评估披露方式对用户信任的影响，发现不透明和模糊策略更能提升信任。


<details>
  <summary>Details</summary>
Motivation: 防止人们盲目信任AI而做出错误决策，探究披露或隐藏事实性估计内容对用户信任的影响。

Method: 测试四种不同的披露方式（透明、关注、不透明、模糊），并与无事实性信息的基线响应对比，在问答场景中开展148人的受试者研究。

Result: 不透明和模糊策略在保持感知答案质量的同时，比其他策略带来更高的信任。

Conclusion: 讨论了隐藏可能事实性较低的内容以建立终端用户信任的有效性。

Abstract: Large language models are known to produce outputs that are plausible but
factually incorrect. To prevent people from making erroneous decisions by
blindly trusting AI, researchers have explored various ways of communicating
factuality estimates in AI-generated outputs to end-users. However, little is
known about whether revealing content estimated to be factually incorrect
influences users' trust when compared to hiding it altogether. We tested four
different ways of disclosing an AI-generated output with factuality
assessments: transparent (highlights less factual content), attention
(highlights factual content), opaque (removes less factual content), ambiguity
(makes less factual content vague), and compared them with a baseline response
without factuality information. We conducted a human subjects research (N =
148) using the strategies in question-answering scenarios. We found that the
opaque and ambiguity strategies led to higher trust while maintaining perceived
answer quality, compared to the other strategies. We discuss the efficacy of
hiding presumably less factual content to build end-user trust.

</details>


### [334] [Toward AI Matching Policies in Homeless Services: A Qualitative Study with Policymakers](https://arxiv.org/abs/2508.07129)
*Caroline M. Johnston,Olga Koumoundouros,Angel Hsing-Chi Hwang,Laura Onasch-Vera,Eric Rice,Phebe Vayanos*

Main category: cs.HC

TL;DR: 研究通过对洛杉矶13位无家可归者服务政策制定者的半结构化访谈，探讨他们对将AI集成到住房资源匹配过程的看法，发现政策制定者欢迎设计合理且与人协同的AI匹配工具。


<details>
  <summary>Details</summary>
Motivation: 不清楚数据驱动算法是否以及如何被从业者接受和采用，及其相应后果，需了解政策制定者对AI融入住房资源匹配过程的态度。

Method: 对洛杉矶13位无家可归者服务政策制定者进行半结构化访谈，并进行定性分析。

Result: 政策制定者在考虑到各种复杂因素的情况下，仍欢迎经过精心设计且与人协同的AI匹配工具，关于AI系统的具体设计尚无共识。

Conclusion: 政策制定者的见解为未来构建负责任的算法系统以支持低资源场景决策的研究者和从业者提出了开放性问题和设计考量。

Abstract: Artificial intelligence researchers have proposed various data-driven
algorithms to improve the processes that match individuals experiencing
homelessness to scarce housing resources. It remains unclear whether and how
these algorithms are received or adopted by practitioners and what their
corresponding consequences are. Through semi-structured interviews with 13
policymakers in homeless services in Los Angeles, we investigate whether such
change-makers are open to the idea of integrating AI into the housing resource
matching process, identifying where they see potential gains and drawbacks from
such a system in issues of efficiency, fairness, and transparency. Our
qualitative analysis indicates that, even when aware of various complicating
factors, policymakers welcome the idea of an AI matching tool if thoughtfully
designed and used in tandem with human decision-makers. Though there is no
consensus as to the exact design of such an AI system, insights from
policymakers raise open questions and design considerations that can be
enlightening for future researchers and practitioners who aim to build
responsible algorithmic systems to support decision-making in low-resource
scenarios.

</details>


### [335] [ClimateSOM: A Visual Analysis Workflow for Climate Ensemble Datasets](https://arxiv.org/abs/2508.06732)
*Yuya Kawakami,Daniel Cayan,Dongyu Liu,Kwan-Liu Ma*

Main category: cs.HC

TL;DR: 提出ClimateSOM可视化分析工作流，结合SOM和LLM探索气候集合数据集，应用于美国降水预测数据集并评估。


<details>
  <summary>Details</summary>
Motivation: 理解气候集合模型运行间的变异性并分析其大小和模式对气候科学家至关重要。

Method: 提出ClimateSOM工作流，用SOM将气候集合模型运行抽象为二维空间分布，集成LLM辅助理解该空间。

Result: 应用ClimateSOM到美国降水预测数据集，进行LLM集成评估和专家评审。

Conclusion: ClimateSOM能让用户探索集合模型运行的变异性、识别模式、比较和聚类模型运行。

Abstract: Ensemble datasets are ever more prevalent in various scientific domains. In
climate science, ensemble datasets are used to capture variability in
projections under plausible future conditions including greenhouse and aerosol
emissions. Each ensemble model run produces projections that are fundamentally
similar yet meaningfully distinct. Understanding this variability among
ensemble model runs and analyzing its magnitude and patterns is a vital task
for climate scientists. In this paper, we present ClimateSOM, a visual analysis
workflow that leverages a self-organizing map (SOM) and Large Language Models
(LLMs) to support interactive exploration and interpretation of climate
ensemble datasets. The workflow abstracts climate ensemble model runs -
spatiotemporal time series - into a distribution over a 2D space that captures
the variability among the ensemble model runs using a SOM. LLMs are integrated
to assist in sensemaking of this SOM-defined 2D space, the basis for the visual
analysis tasks. In all, ClimateSOM enables users to explore the variability
among ensemble model runs, identify patterns, compare and cluster the ensemble
model runs. To demonstrate the utility of ClimateSOM, we apply the workflow to
an ensemble dataset of precipitation projections over California and the
Northwestern United States. Furthermore, we conduct a short evaluation of our
LLM integration, and conduct an expert review of the visual workflow and the
insights from the case studies with six domain experts to evaluate our approach
and its utility.

</details>


### [336] [Story Ribbons: Reimagining Storyline Visualizations with Large Language Models](https://arxiv.org/abs/2508.06772)
*Catherine Yeh,Tara Menon,Robin Singh Arya,Helen He,Moira Weigel,Fernanda Viégas,Martin Wattenberg*

Main category: cs.HC

TL;DR: 文章提出基于大语言模型的数据解析管道，创建Story Ribbons可视化系统分析文学作品，展示了大语言模型在叙事可视化中的潜力及当前系统局限。


<details>
  <summary>Details</summary>
Motivation: 现有从非结构化故事数据中获取结构化信息以进行可视化分析存在挑战，利用大语言模型的文本处理和分析能力改进现有叙事可视化技术。

Method: 引入LLM驱动的数据解析管道自动提取小说和剧本中的叙事信息，创建Story Ribbons交互式可视化系统。

Result: 通过对36部文学作品的管道评估和用户研究，展示了大语言模型简化叙事可视化创建、揭示故事新见解的潜力。

Conclusion: 大语言模型有助于叙事可视化，但AI系统存在局限性，并设计了交互模式来解决问题。

Abstract: Analyzing literature involves tracking interactions between characters,
locations, and themes. Visualization has the potential to facilitate the
mapping and analysis of these complex relationships, but capturing structured
information from unstructured story data remains a challenge. As large language
models (LLMs) continue to advance, we see an opportunity to use their text
processing and analysis capabilities to augment and reimagine existing
storyline visualization techniques. Toward this goal, we introduce an
LLM-driven data parsing pipeline that automatically extracts relevant narrative
information from novels and scripts. We then apply this pipeline to create
Story Ribbons, an interactive visualization system that helps novice and expert
literary analysts explore detailed character and theme trajectories at multiple
narrative levels. Through pipeline evaluations and user studies with Story
Ribbons on 36 literary works, we demonstrate the potential of LLMs to
streamline narrative visualization creation and reveal new insights about
familiar stories. We also describe current limitations of AI-based systems, and
interaction motifs designed to address these issues.

</details>


### [337] [Explainability-in-Action: Enabling Expressive Manipulation and Tacit Understanding by Bending Diffusion Models in ComfyUI](https://arxiv.org/abs/2508.07183)
*Ahmed M. Abuzuraiq,Philippe Pasquier*

Main category: cs.HC

TL;DR: 提出以工艺为基础的可解释性方法，通过ComfyUI插件展示其应用，让艺术家能理解生成模型组件对输出的影响。


<details>
  <summary>Details</summary>
Motivation: 在创意场景中，大型生成模型常掩盖艺术家创作的可能性，需要一种方法让模型支持艺术创作。

Method: 提出基于长期实践参与的工艺可解释性方法，并通过集成到ComfyUI的插件展示应用。

Result: 艺术家通过交互式操作生成模型不同部分，能形成各组件如何影响输出的直觉。

Conclusion: 即使是大型模型，若其内部结构可暴露和操作，也能作为创意材料。

Abstract: Explainable AI (XAI) in creative contexts can go beyond transparency to
support artistic engagement, modifiability, and sustained practice. While
curated datasets and training human-scale models can offer artists greater
agency and control, large-scale generative models like text-to-image diffusion
systems often obscure these possibilities. We suggest that even large models
can be treated as creative materials if their internal structure is exposed and
manipulable. We propose a craft-based approach to explainability rooted in
long-term, hands-on engagement akin to Sch\"on's "reflection-in-action" and
demonstrate its application through a model-bending and inspection plugin
integrated into the node-based interface of ComfyUI. We demonstrate that by
interactively manipulating different parts of a generative model, artists can
develop an intuition about how each component influences the output.

</details>


### [338] [Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment](https://arxiv.org/abs/2508.07283)
*Bujar Raufi*

Main category: cs.HC

TL;DR: 研究探索EEG微状态与大语言模型结合以评估认知负荷状态，经多阶段实验设计微调模型，结果显示性能提升，有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 探索EEG微状态与大语言模型的交叉点，以增强对认知负荷状态的评估。

Method: 实验分四阶段：数据集收集与预处理、微状态分割与EEG回拟合、特征提取与提示工程、模型选择与优化，采用监督学习训练模型。

Result: 微调后模型性能显著提升。

Conclusion: 该方法有助于理解大脑动力学，为认知负荷和认知AI研究的机器学习技术进步奠定基础。

Abstract: This study explores the intersection of electroencephalography (EEG)
microstates and Large Language Models (LLMs) to enhance the assessment of
cognitive load states. By utilizing EEG microstate features, the research aims
to fine-tune LLMs for improved predictions of distinct cognitive states,
specifically 'Rest' and 'Load'. The experimental design is delineated in four
comprehensive stages: dataset collection and preprocessing, microstate
segmentation and EEG backfitting, feature extraction paired with prompt
engineering, and meticulous LLM model selection and refinement. Employing a
supervised learning paradigm, the LLM is trained to identify cognitive load
states based on EEG microstate features integrated into prompts, producing
accurate discrimination of cognitive load. A curated dataset, linking EEG
features to specified cognitive load conditions, underpins the experimental
framework. The results indicate a significant improvement in model performance
following the proposed fine-tuning, showcasing the potential of EEG-informed
LLMs in cognitive neuroscience and cognitive AI applications. This approach not
only contributes to the understanding of brain dynamics but also paves the way
for advancements in machine learning techniques applicable to cognitive load
and cognitive AI research.

</details>


### [339] [Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics](https://arxiv.org/abs/2508.07390)
*Gustavo Moreira,Leonardo Ferreira,Carolina Veiga,Maryam Hosseini,Fabio Miranda*

Main category: cs.HC

TL;DR: 文章指出城市数据可视化分析有高门槛，大语言模型虽有潜力但存在挑战，提出Urbanite框架并展示其有效性。


<details>
  <summary>Details</summary>
Motivation: 城市数据可视化分析门槛高，大语言模型应用有设计开发过程不一致问题，需解决。

Method: 提出Urbanite框架，采用基于数据流模型，允许用户多范围指定意图，结合调查结果融入相关特性。

Result: 通过与城市专家合作创建的使用场景证明了Urbanite的有效性。

Conclusion: Urbanite框架能解决城市可视化分析中存在的问题，可降低分析系统构建门槛。

Abstract: With the growing availability of urban data and the increasing complexity of
societal challenges, visual analytics has become essential for deriving
insights into pressing real-world problems. However, analyzing such data is
inherently complex and iterative, requiring expertise across multiple domains.
The need to manage diverse datasets, distill intricate workflows, and integrate
various analytical methods presents a high barrier to entry, especially for
researchers and urban experts who lack proficiency in data management, machine
learning, and visualization. Advancements in large language models offer a
promising solution to lower the barriers to the construction of analytics
systems by enabling users to specify intent rather than define precise
computational operations. However, this shift from explicit operations to
intent-based interaction introduces challenges in ensuring alignment throughout
the design and development process. Without proper mechanisms, gaps can emerge
between user intent, system behavior, and analytical outcomes. To address these
challenges, we propose Urbanite, a framework for human-AI collaboration in
urban visual analytics. Urbanite leverages a dataflow-based model that allows
users to specify intent at multiple scopes, enabling interactive alignment
across the specification, process, and evaluation stages of urban analytics.
Based on findings from a survey to uncover challenges, Urbanite incorporates
features to facilitate explainability, multi-resolution definition of tasks
across dataflows, nodes, and parameters, while supporting the provenance of
interactions. We demonstrate Urbanite's effectiveness through usage scenarios
created in collaboration with urban experts. Urbanite is available at
https://urbantk.org/urbanite.

</details>


### [340] [VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design](https://arxiv.org/abs/2508.07497)
*Leonardo Ferreira,Gustavo Moreira,Fabio Miranda*

Main category: cs.HC

TL;DR: 提出VA - Blueprint方法和知识库，对城市VA系统基本构建块分类形成初始知识库，用大语言模型扩展，经评估为系统开发提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有VA系统开发实践挑战研究不足，缺乏结构化知识库指导设计开发。

Method: 提出VA - Blueprint方法对20个系统核心组件分类形成初始知识库，用大语言模型处理81篇论文扩展，通过专家访谈和注释指标定量分析评估。

Result: 形成了初始知识库，评估了大语言模型扩展知识库构建的有效性。

Conclusion: 该研究加深对VA系统组成的理解，为结构化、可重复和高效的系统开发提供实践基础。

Abstract: Designing and building visual analytics (VA) systems is a complex, iterative
process that requires the seamless integration of data processing, analytics
capabilities, and visualization techniques. While prior research has
extensively examined the social and collaborative aspects of VA system
authoring, the practical challenges of developing these systems remain
underexplored. As a result, despite the growing number of VA systems, there are
only a few structured knowledge bases to guide their design and development. To
tackle this gap, we propose VA-Blueprint, a methodology and knowledge base that
systematically reviews and categorizes the fundamental building blocks of urban
VA systems, a domain particularly rich and representative due to its intricate
data and unique problem sets. Applying this methodology to an initial set of 20
systems, we identify and organize their core components into a multi-level
structure, forming an initial knowledge base with a structured blueprint for VA
system development. To scale this effort, we leverage a large language model to
automate the extraction of these components for other 81 papers (completing a
corpus of 101 papers), assessing its effectiveness in scaling knowledge base
construction. We evaluate our method through interviews with experts and a
quantitative analysis of annotation metrics. Our contributions provide a deeper
understanding of VA systems' composition and establish a practical foundation
to support more structured, reproducible, and efficient system development.
VA-Blueprint is available at https://urbantk.org/va-blueprint.

</details>


### [341] [Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI](https://arxiv.org/abs/2508.07520)
*Baihan Lin*

Main category: cs.HC

TL;DR: 提出Conversational DNA这一新型视觉语言来分析对话，通过生物隐喻揭示对话结构，经案例分析展示其优势并贡献新框架。


<details>
  <summary>Details</summary>
Motivation: 探究对话中隐藏模式对交流的揭示作用，弥补传统对话分析仅给出统计总结的不足。

Method: 引入Conversational DNA，用生物隐喻揭示对话的时间架构，如用线条粗细表示语言复杂性等。

Result: 通过对治疗对话和重要人机对话的探索性分析，发现该可视化方法能揭示传统方法遗漏的交互模式。

Conclusion: 为理解交流贡献了新的创造性框架，连接了数据可视化、人机交互等领域。

Abstract: What if the patterns hidden within dialogue reveal more about communication
than the words themselves? We introduce Conversational DNA, a novel visual
language that treats any dialogue -- whether between humans, between human and
AI, or among groups -- as a living system with interpretable structure that can
be visualized, compared, and understood. Unlike traditional conversation
analysis that reduces rich interaction to statistical summaries, our approach
reveals the temporal architecture of dialogue through biological metaphors.
Linguistic complexity flows through strand thickness, emotional trajectories
cascade through color gradients, conversational relevance forms through
connecting elements, and topic coherence maintains structural integrity through
helical patterns. Through exploratory analysis of therapeutic conversations and
historically significant human-AI dialogues, we demonstrate how this
visualization approach reveals interaction patterns that traditional methods
miss. Our work contributes a new creative framework for understanding
communication that bridges data visualization, human-computer interaction, and
the fundamental question of what makes dialogue meaningful in an age where
humans increasingly converse with artificial minds.

</details>


### [342] [On the Limits of Selective AI Prediction: A Case Study in Clinical Decision Making](https://arxiv.org/abs/2508.07617)
*Sarah Jabbour,David Fouhey,Nikola Banovic,Stephanie D. Shepard,Ella Kazerooni,Michael W. Sjoding,Jenna Wiens*

Main category: cs.HC

TL;DR: 研究选择性预测对临床医生决策的影响，发现其可缓解不准确AI的负面影响，但会改变错误模式，强调验证人机系统中人类与AI交互假设的重要性。


<details>
  <summary>Details</summary>
Motivation: 测试选择性预测中AI弃权时人类决策与无AI参与时相同这一假设，研究其在临床环境中对人类决策的影响。

Method: 对259名临床医生进行用户研究，比较无AI参与、有不准确AI预测和有选择性预测时的诊断和治疗准确性。

Result: 选择性预测缓解了不准确AI对决策准确性的负面影响，使准确性有所恢复，但改变了错误模式，出现更多漏诊和漏治情况。

Conclusion: 强调在人机系统中，对人类与AI交互假设进行实证验证的重要性。

Abstract: AI has the potential to augment human decision making. However, even
high-performing models can produce inaccurate predictions when deployed. These
inaccuracies, combined with automation bias, where humans overrely on AI
predictions, can result in worse decisions. Selective prediction, in which
potentially unreliable model predictions are hidden from users, has been
proposed as a solution. This approach assumes that when AI abstains and informs
the user so, humans make decisions as they would without AI involvement. To
test this assumption, we study the effects of selective prediction on human
decisions in a clinical context. We conducted a user study of 259 clinicians
tasked with diagnosing and treating hospitalized patients. We compared their
baseline performance without any AI involvement to their AI-assisted accuracy
with and without selective prediction. Our findings indicate that selective
prediction mitigates the negative effects of inaccurate AI in terms of decision
accuracy. Compared to no AI assistance, clinician accuracy declined when shown
inaccurate AI predictions (66% [95% CI: 56%-75%] vs. 56% [95% CI: 46%-66%]),
but recovered under selective prediction (64% [95% CI: 54%-73%]). However,
while selective prediction nearly maintains overall accuracy, our results
suggest that it alters patterns of mistakes: when informed the AI abstains,
clinicians underdiagnose (18% increase in missed diagnoses) and undertreat (35%
increase in missed treatments) compared to no AI input at all. Our findings
underscore the importance of empirically validating assumptions about how
humans engage with AI within human-AI systems.

</details>


### [343] [CognitiveArm: Enabling Real-Time EEG-Controlled Prosthetic Arm Using Embodied Machine Learning](https://arxiv.org/abs/2508.07731)
*Abdul Basit,Maha Nawaz,Saim Rehman,Muhammad Shafique*

Main category: cs.HC

TL;DR: 提出CognitiveArm脑控假肢系统，在嵌入式硬件实现实时运行，有90%分类准确率且支持语音命令。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上高效控制假肢，平衡模型复杂度、计算效率和延迟。

Method: 集成BrainFlow与优化深度学习模型，用进化搜索调参，采用模型压缩技术，收集数据集并标注，支持语音命令。

Result: 全尺寸原型与EEG耳机对接，对三个核心动作分类准确率达90%，语音集成提升日常任务表现。

Conclusion: CognitiveArm有用于高级假肢控制的潜力。

Abstract: Efficient control of prosthetic limbs via non-invasive brain-computer
interfaces (BCIs) requires advanced EEG processing, including pre-filtering,
feature extraction, and action prediction, performed in real time on edge AI
hardware. Achieving this on resource-constrained devices presents challenges in
balancing model complexity, computational efficiency, and latency. We present
CognitiveArm, an EEG-driven, brain-controlled prosthetic system implemented on
embedded AI hardware, achieving real-time operation without compromising
accuracy. The system integrates BrainFlow, an open-source library for EEG data
acquisition and streaming, with optimized deep learning (DL) models for precise
brain signal classification. Using evolutionary search, we identify
Pareto-optimal DL configurations through hyperparameter tuning, optimizer
analysis, and window selection, analyzed individually and in ensemble
configurations. We apply model compression techniques such as pruning and
quantization to optimize models for embedded deployment, balancing efficiency
and accuracy. We collected an EEG dataset and designed an annotation pipeline
enabling precise labeling of brain signals corresponding to specific intended
actions, forming the basis for training our optimized DL models. CognitiveArm
also supports voice commands for seamless mode switching, enabling control of
the prosthetic arm's 3 degrees of freedom (DoF). Running entirely on embedded
hardware, it ensures low latency and real-time responsiveness. A full-scale
prototype, interfaced with the OpenBCI UltraCortex Mark IV EEG headset,
achieved up to 90% accuracy in classifying three core actions (left, right,
idle). Voice integration enables multiplexed, variable movement for everyday
tasks (e.g., handshake, cup picking), enhancing real-world performance and
demonstrating CognitiveArm's potential for advanced prosthetic control.

</details>


### [344] [Can AI Explanations Make You Change Your Mind?](https://arxiv.org/abs/2508.08158)
*Laura Spillner,Rachel Ringe,Robert Porzel,Rainer Malaka*

Main category: cs.HC

TL;DR: 在线研究发现用户对可解释决策支持系统的解释关注不足，文章探索影响用户考量解释的因素及对改变想法的影响。


<details>
  <summary>Details</summary>
Motivation: 在基于AI的决策支持系统中，虽解释能助用户判断对AI建议的信任，但前提是用户会仔细考量解释，需研究用户是否真会如此。

Method: 开展关于可解释决策支持系统信任的在线研究，并对数据进行探索性分析。

Result: 许多情况下，参与者花在解释上的时间少，并不总是详细考量。

Conclusion: 文章探索影响参与者考量AI解释的因素，以及这对他们是否接受AI建议的影响。

Abstract: In the context of AI-based decision support systems, explanations can help
users to judge when to trust the AI's suggestion, and when to question it. In
this way, human oversight can prevent AI errors and biased decision-making.
However, this rests on the assumption that users will consider explanations in
enough detail to be able to catch such errors. We conducted an online study on
trust in explainable DSS, and were surprised to find that in many cases,
participants spent little time on the explanation and did not always consider
it in detail. We present an exploratory analysis of this data, investigating
what factors impact how carefully study participants consider AI explanations,
and how this in turn impacts whether they are open to changing their mind based
on what the AI suggests.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [345] [American Option Pricing Under Time-Varying Rough Volatility: A Signature-Based Hybrid Framework](https://arxiv.org/abs/2508.07151)
*Roshan Shah*

Main category: q-fin.MF

TL;DR: 提出模块化框架扩展签名方法处理随波动率粗糙度变化的美式期权定价，有三项创新，实证显示该框架灵活适配且性能更好。


<details>
  <summary>Details</summary>
Motivation: 处理随波动率粗糙度变化的美式期权定价问题。

Method: 基于Bayer等人的签名定价框架，增加三项创新：训练梯度提升集成估计时变Hurst参数；根据预测粗糙度选择模拟器；用随机傅里叶特征加速签名核评估。

Result: 对标准普尔500股指期权的实证测试表明，持续粗糙度假设常被违反，混合框架能适应变化的波动率粗糙度。

Conclusion: 通过集成动态Hurst参数估计流程和高效核近似，可实现动态波动环境中美式期权的实时定价。

Abstract: We introduce a modular framework that extends the signature method to handle
American option pricing under evolving volatility roughness. Building on the
signature-pricing framework of Bayer et al. (2025), we add three practical
innovations. First, we train a gradient-boosted ensemble to estimate the
time-varying Hurst parameter H(t) from rolling windows of recent volatility
data. Second, we feed these forecasts into a regime switch that chooses either
a rough Bergomi or a calibrated Heston simulator, depending on the predicted
roughness. Third, we accelerate signature-kernel evaluations with Random
Fourier Features (RFF), cutting computational cost while preserving accuracy.
Empirical tests on S&P 500 equity-index options reveal that the assumption of
persistent roughness is frequently violated, particularly during stable market
regimes when H(t) approaches or exceeds 0.5. The proposed hybrid framework
provides a flexible structure that adapts to changing volatility roughness,
improving performance over fixed-roughness baselines and reducing duality gaps
in some regimes. By integrating a dynamic Hurst parameter estimation pipeline
with efficient kernel approximations, we propose to enable tractable, real-time
pricing of American options in dynamic volatility environments.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [346] [Vertex Features for Neural Global Illumination](https://arxiv.org/abs/2508.07852)
*Rui Su,Honghao Dong,Haojie Jin,Yisong Chen,Guoping Wang,Sheng Li*

Main category: cs.GR

TL;DR: 本文提出神经顶点特征用于神经渲染任务，优化内存效率，实验表明能大幅降低内存消耗并保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 传统特征网格表示内存占用大，成为现代并行计算硬件瓶颈。

Method: 将可学习特征直接存储在网格顶点，利用底层几何结构进行神经处理。

Result: 方法将内存消耗降至基于网格表示的五分之一甚至更少，保持可比渲染质量并降低推理开销。

Conclusion: 神经顶点特征是一种有效的可学习表示，能优化内存效率并提升特征表示。

Abstract: Recent research on learnable neural representations has been widely adopted
in the field of 3D scene reconstruction and neural rendering applications.
However, traditional feature grid representations often suffer from substantial
memory footprint, posing a significant bottleneck for modern parallel computing
hardware. In this paper, we present neural vertex features, a generalized
formulation of learnable representation for neural rendering tasks involving
explicit mesh surfaces. Instead of uniformly distributing neural features
throughout 3D space, our method stores learnable features directly at mesh
vertices, leveraging the underlying geometry as a compact and structured
representation for neural processing. This not only optimizes memory
efficiency, but also improves feature representation by aligning compactly with
the surface using task-specific geometric priors. We validate our neural
representation across diverse neural rendering tasks, with a specific emphasis
on neural radiosity. Experimental results demonstrate that our method reduces
memory consumption to only one-fifth (or even less) of grid-based
representations, while maintaining comparable rendering quality and lowering
inference overhead.

</details>


### [347] [LL3M: Large Language 3D Modelers](https://arxiv.org/abs/2508.08228)
*Sining Lu,Guan Chen,Nam Anh Dinh,Itai Lang,Ari Holtzman,Rana Hanocka*

Main category: cs.GR

TL;DR: 提出LL3M多智能体系统，利用预训练大语言模型通过编写Blender中的Python代码生成3D资产，以代码为生成和可解释媒介，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 突破传统从3D数据集合学习的生成方法，实现3D资产生成的更大模块化、可编辑性以及与艺术家工作流程的集成。

Method: LL3M协调专业LLM智能体团队，根据文本提示规划、检索、编写、调试和完善Blender脚本，利用检索增强生成知识库BlenderRAG辅助操作。

Result: 在不同形状类别、风格和材质编辑以及用户驱动的细化方面展示了LL3M的有效性。

Conclusion: 代码作为3D资产创建的生成和可解释媒介具有强大能力。

Abstract: We present LL3M, a multi-agent system that leverages pretrained large
language models (LLMs) to generate 3D assets by writing interpretable Python
code in Blender. We break away from the typical generative approach that learns
from a collection of 3D data. Instead, we reformulate shape generation as a
code-writing task, enabling greater modularity, editability, and integration
with artist workflows. Given a text prompt, LL3M coordinates a team of
specialized LLM agents to plan, retrieve, write, debug, and refine Blender
scripts that generate and edit geometry and appearance. The generated code
works as a high-level, interpretable, human-readable, well-documented
representation of scenes and objects, making full use of sophisticated Blender
constructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse,
unconstrained shapes, materials, and scenes. This code presents many avenues
for further agent and human editing and experimentation via code tweaks or
procedural parameters. This medium naturally enables a co-creative loop in our
system: agents can automatically self-critique using code and visuals, while
iterative user instructions provide an intuitive way to refine assets. A shared
code context across agents enables awareness of previous attempts, and a
retrieval-augmented generation knowledge base built from Blender API
documentation, BlenderRAG, equips agents with examples, types, and functions
empowering advanced modeling operations and code correctness. We demonstrate
the effectiveness of LL3M across diverse shape categories, style and material
edits, and user-driven refinements. Our experiments showcase the power of code
as a generative and interpretable medium for 3D asset creation. Our project
page is at https://threedle.github.io/ll3m.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [348] [Real-Time Analysis of Unstructured Data with Machine Learning on Heterogeneous Architectures](https://arxiv.org/abs/2508.07423)
*Fotis I. Giasemis*

Main category: hep-ex

TL;DR: 粒子物理需高精度，产生大量数据，需实时过滤。本文研究如何在该环境高效部署机器学习模型，介绍基于图神经网络的流水线并与经典算法及不同硬件实现对比。


<details>
  <summary>Details</summary>
Motivation: 粒子物理对精度要求提升产生大量数据，需实时过滤数据，要解决如何在该环境高效部署机器学习模型以提高吞吐量和降低能耗的问题。

Method: 开发基于图神经网络的流水线用于CERN的LHCb实验的带电粒子轨迹重建，在GPU上实现端到端流程，还在FPGA架构上进行加速。

Result: 将该流水线与LHCb当前使用的经典跟踪算法对比，比较了FPGA和GPU实现的功耗和处理速度。

Conclusion: 未明确提及具体结论，但旨在展示基于图神经网络的流水线在该场景下的性能表现，以探索高效处理数据的方法。

Abstract: As the particle physics community needs higher and higher precisions in order
to test our current model of the subatomic world, larger and larger datasets
are necessary. With upgrades scheduled for the detectors of colliding-beam
experiments around the world, and specifically at the Large Hadron Collider at
CERN, more collisions and more complex interactions are expected. This directly
implies an increase in data produced and consequently in the computational
resources needed to process them. At CERN, the amount of data produced is
gargantuan. This is why the data have to be heavily filtered and selected in
real time before being permanently stored. This data can then be used to
perform physics analyses, in order to expand our current understanding of the
universe and improve the Standard Model of physics. This real-time filtering,
known as triggering, involves complex processing happening often at frequencies
as high as 40 MHz. This thesis contributes to understanding how machine
learning models can be efficiently deployed in such environments, in order to
maximize throughput and minimize energy consumption. Inevitably, modern
hardware designed for such tasks and contemporary algorithms are needed in
order to meet the challenges posed by the stringent, high-frequency data rates.
In this work, I present our graph neural network-based pipeline, developed for
charged particle track reconstruction at the LHCb experiment at CERN. The
pipeline was implemented end-to-end inside LHCb's first-level trigger, entirely
on GPUs. Its performance was compared against the classical tracking algorithms
currently in production at LHCb. The pipeline was also accelerated on the FPGA
architecture, and its performance in terms of power consumption and processing
speed was compared against the GPU implementation.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [349] [An effective potential for generative modelling with active matter](https://arxiv.org/abs/2508.08146)
*Adrian Baule*

Main category: cond-mat.stat-mech

TL;DR: 本文展示基于有限关联时间的活性粒子过程实现生成扩散模型，用有效势实现时间反转，数值实验验证有效势有效性。


<details>
  <summary>Details</summary>
Motivation: 探索基于有限关联时间的活性粒子过程实现生成扩散模型的方法。

Method: 通过对活性粒子位置坐标施加与时间相关的有效势实现时间反转，有效势在持久时间一阶有效，力场由标准得分函数及其二阶导数确定。

Result: 针对人工数据分布的数值实验证实了有效势的有效性。

Conclusion: 基于有限关联时间的活性粒子过程，通过施加有效势可实现生成扩散模型。

Abstract: Score-based diffusion models generate samples from a complex underlying data
distribution by time-reversal of a diffusion process and represent the
state-of-the-art in many generative AI applications such as artificial image
synthesis. Here, I show how a generative diffusion model can be implemented
based on an underlying active particle process with finite correlation time. In
contrast to previous approaches that use a score function acting on the
velocity coordinate of the active particle, time reversal is here achieved by
imposing an effective time-dependent potential on the position coordinate only.
The effective potential is valid to first order in the persistence time and
leads to a force field that is fully determined by the standard score function
and its derivatives up to 2nd order. Numerical experiments for artificial data
distributions confirm the validity of the effective potential.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [350] [Role of Large Language Models and Retrieval-Augmented Generation for Accelerating Crystalline Material Discovery: A Systematic Review](https://arxiv.org/abs/2508.06691)
*Agada Joseph Oche,Arpan Biswas*

Main category: cond-mat.mtrl-sci

TL;DR: 本文对大语言模型（LLMs）和检索增强生成（RAG）在材料科学问题应用的进展进行综述，讨论性能、局限等并给出未来方向。


<details>
  <summary>Details</summary>
Motivation: LLMs和RAG在加速材料发现方面有潜力，为挖掘其在材料科学领域应用价值开展研究。

Method: 对晶体结构预测、缺陷分析等多个方面的前沿发展进行调研。

Result: 梳理了LLMs和RAG在材料科学各方面应用的进展，明确了结合外部知识源带来的新能力。

Conclusion: 讨论了相关方法的表现、局限和影响，指出利用LLMs加速材料研究和发现的未来方向。

Abstract: Large language models (LLMs) have emerged as powerful tools for
knowledge-intensive tasks across domains. In materials science, to find novel
materials for various energy efficient devices for various real-world
applications, requires several time and cost expensive simulations and
experiments. In order to tune down the uncharted material search space,
minimizing the experimental cost, LLMs can play a bigger role to first provide
an accelerated search of promising known material candidates. Furthermore, the
integration of LLMs with domain-specific information via retrieval-augmented
generation (RAG) is poised to revolutionize how researchers predict materials
structures, analyze defects, discover novel compounds, and extract knowledge
from literature and databases. In motivation to the potentials of LLMs and RAG
in accelerating material discovery, this paper presents a broad and systematic
review to examine the recent advancements in applying LLMs and RAG to key
materials science problems. We survey state-of-the-art developments in crystal
structure prediction, defect analysis, materials discovery, literature mining,
database integration, and multi-modal retrieval, highlighting how combining
LLMs with external knowledge sources enables new capabilities. We discuss the
performance, limitations, and implications of these approaches, and outline
future directions for leveraging LLMs to accelerate materials research and
discovery for advancement in technologies in the area of electronics, optics,
biomedical, and energy storage.

</details>


### [351] [Explainable AI for Curie Temperature Prediction in Magnetic Materials](https://arxiv.org/abs/2508.06996)
*M. Adeel Ajaib,Fariha Nasir,Abdul Rehman*

Main category: cond-mat.mtrl-sci

TL;DR: 使用NEMAD数据库探索机器学习技术预测磁性材料居里温度，Extra Trees Regressor表现最佳，还用聚类和SHAP分析。


<details>
  <summary>Details</summary>
Motivation: 探索用机器学习技术预测磁性材料居里温度。

Method: 用NEMAD数据库，用基于成分和领域感知的描述符扩充数据集，评估多个机器学习模型，用k-means聚类算法和SHAP分析。

Result: Extra Trees Regressor表现最佳，平衡数据集交叉验证R^2得分达0.85 ± 0.01。

Conclusion: 可解释AI技术分析为模型预测行为提供见解，提升科学可解释性。

Abstract: We explore machine learning techniques for predicting Curie temperatures of
magnetic materials using the NEMAD database. By augmenting the dataset with
composition-based and domain-aware descriptors, we evaluate the performance of
several machine learning models. We find that the Extra Trees Regressor
delivers the best performance reaching an R^2 score of up to 0.85 $\pm$ 0.01
(cross-validated) for a balanced dataset. We employ the k-means clustering
algorithm to gain insights into the performance of chemically distinct material
groups. Furthermore, we perform the SHAP analysis to identify key
physicochemical drivers of Curie behavior, such as average atomic number and
magnetic moment. By employing explainable AI techniques, this analysis offers
insights into the model's predictive behavior, thereby advancing scientific
interpretability.

</details>


### [352] [Generative Inversion for Property-Targeted Materials Design: Application to Shape Memory Alloys](https://arxiv.org/abs/2508.07798)
*Cheng Li,Pengfei Danga,Yuehui Xiana,Yumei Zhou,Bofeng Shi,Xiangdong Ding,Jun Suna,Dezhen Xue*

Main category: cond-mat.mtrl-sci

TL;DR: 介绍基于GAN反演的数据驱动框架用于高性能形状记忆合金逆向设计，通过实验验证，发现性能优异合金，证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 解决高转变温度和大机械功输出形状记忆合金设计的长期挑战。

Method: 将预训练GAN与性能预测模型耦合，进行基于梯度的潜在空间优化，生成满足目标性能的合金成分和加工参数。

Result: 合成并表征五种NiTi基合金，其中Ni₄₉.₈Ti₂₆.₄Hf₁₈.₆Zr₅.₂合金性能优于现有NiTi合金。

Conclusion: GAN反演为复杂合金的性能导向发现提供了高效且通用的途径。

Abstract: The design of shape memory alloys (SMAs) with high transformation
temperatures and large mechanical work output remains a longstanding challenge
in functional materials engineering. Here, we introduce a data-driven framework
based on generative adversarial network (GAN) inversion for the inverse design
of high-performance SMAs. By coupling a pretrained GAN with a property
prediction model, we perform gradient-based latent space optimization to
directly generate candidate alloy compositions and processing parameters that
satisfy user-defined property targets. The framework is experimentally
validated through the synthesis and characterization of five NiTi-based SMAs.
Among them, the Ni$_{49.8}$Ti$_{26.4}$Hf$_{18.6}$Zr$_{5.2}$ alloy achieves a
high transformation temperature of 404 $^\circ$C, a large mechanical work
output of 9.9 J/cm$^3$, a transformation enthalpy of 43 J/g , and a thermal
hysteresis of 29 {\deg}C, outperforming existing NiTi alloys. The enhanced
performance is attributed to a pronounced transformation volume change and a
finely dispersed of Ti$_2$Ni-type precipitates, enabled by sluggish Zr and Hf
diffusion, and semi-coherent interfaces with localized strain fields. This
study demonstrates that GAN inversion offers an efficient and generalizable
route for the property-targeted discovery of complex alloys.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [353] [Computing with Canonical Microcircuits](https://arxiv.org/abs/2508.06501)
*PK Douglas*

Main category: q-bio.NC

TL;DR: 受大脑启发提出基于规范微电路（CMCs）的计算架构，实验显示在图像基准测试中表现良好，参数少且有可解释性，为人工神经网络提供新方向。


<details>
  <summary>Details</summary>
Motivation: 人类大脑以低功耗实现强大学习和决策能力，当前先进AI系统难以企及，受此启发开展研究。

Method: 基于CMCs构建计算架构，将其实现为包含特定神经元的神经ODEs，形成8维动力系统，有生物合理的循环连接。

Result: 单个CMC节点在MNIST上准确率达97.8%，分层配置在更复杂图像基准测试中性能提升，使用参数远少于传统深度学习模型，相空间分析显示不同输入类有不同动态轨迹。

Conclusion: 神经形态计算方法可提高人工神经网络的效率和可解释性，为基于人脑计算原理的参数高效架构提供新方向。

Abstract: The human brain represents the only known example of general intelligence
that naturally aligns with human values. On a mere 20-watt power budget, the
brain achieves robust learning and adaptive decision-making in ways that
continue to elude advanced AI systems. Inspired by the brain, we present a
computational architecture based on canonical microcircuits (CMCs) -
stereotyped patterns of neurons found ubiquitously throughout the cortex. We
implement these circuits as neural ODEs comprising spiny stellate, inhibitory,
and pyramidal neurons, forming an 8-dimensional dynamical system with
biologically plausible recurrent connections. Our experiments show that even a
single CMC node achieves 97.8 percent accuracy on MNIST, while hierarchical
configurations - with learnable inter-regional connectivity and recurrent
connections - yield improved performance on more complex image benchmarks.
Notably, our approach achieves competitive results using substantially fewer
parameters than conventional deep learning models. Phase space analysis
revealed distinct dynamical trajectories for different input classes,
highlighting interpretable, emergent behaviors observed in biological systems.
These findings suggest that neuromorphic computing approaches can improve both
efficiency and interpretability in artificial neural networks, offering new
directions for parameter-efficient architectures grounded in the computational
principles of the human brain.

</details>


### [354] [Data-Efficient Neural Training with Dynamic Connectomes](https://arxiv.org/abs/2508.06817)
*Yutong Wu,Peilin He,Tananun Songdechakraiwut*

Main category: q-bio.NC

TL;DR: 本文提出将神经网络训练中的神经激活表示为功能连接组以表征训练动态，此方法可捕捉网络功能组织转变，还可作为学习进度指标实现提前停止训练，框架表现良好。


<details>
  <summary>Details</summary>
Motivation: 神经网络的层次结构和高维参数空间给理解和控制训练动态带来挑战，需要新方法来表征训练动态。

Method: 将不断演变的神经激活表示为功能连接组，并提取训练过程中的活动动态特征。

Result: 所提取的特征能有效捕捉网络功能组织的关键转变，基于此提出的功能连接组时间序列可作为学习进度的内在指标。

Conclusion: 该框架在基准测试中表现稳健，为神经网络训练动态提供了新见解。

Abstract: The study of dynamic functional connectomes has provided valuable insights
into how patterns of brain activity change over time. Neural networks process
information through artificial neurons, conceptually inspired by patterns of
activation in the brain. However, their hierarchical structure and
high-dimensional parameter space pose challenges for understanding and
controlling training dynamics. In this study, we introduce a novel approach to
characterize training dynamics in neural networks by representing evolving
neural activations as functional connectomes and extracting dynamic signatures
of activity throughout training. Our results show that these signatures
effectively capture key transitions in the functional organization of the
network. Building on this analysis, we propose the use of a time series of
functional connectomes as an intrinsic indicator of learning progress, enabling
a principled early stopping criterion. Our framework performs robustly across
benchmarks and provides new insights into neural network training dynamics.

</details>


### [355] [Network-Specific Models for Multimodal Brain Response Prediction](https://arxiv.org/abs/2508.06499)
*Andrea Corsico,Giorgia Rigamonti,Simone Zini,Luigi Celona,Paolo Napoletano*

Main category: q-bio.NC

TL;DR: 提出基于Yeo 7 - 网络分区的特定网络方法预测大脑对复杂多模态电影的反应，分组训练模型提升预测准确性，在挑战赛获第八名。


<details>
  <summary>Details</summary>
Motivation: 改进大脑对复杂多模态电影反应的预测方法，不将大脑视为均匀系统。

Method: 将七个功能网络分为四个集群，为每个集群训练多主体多层感知器（MLP）模型，支持特定集群优化和自适应记忆建模。

Result: 聚类策略显著提高Schaefer图谱1000个皮质区域的预测准确性，最终模型在挑战赛获第八名，分布外（OOD）相关分数接近基线模型两倍。

Conclusion: 基于网络聚类的预测方法有效提升了大脑反应预测的准确性。

Abstract: In this work, we present a network-specific approach for predicting brain
responses to complex multimodal movies, leveraging the Yeo 7-network
parcellation of the Schaefer atlas. Rather than treating the brain as a
homogeneous system, we grouped the seven functional networks into four clusters
and trained separate multi-subject, multi-layer perceptron (MLP) models for
each. This architecture supports cluster-specific optimization and adaptive
memory modeling, allowing each model to adjust temporal dynamics and modality
weighting based on the functional role of its target network. Our results
demonstrate that this clustered strategy significantly enhances prediction
accuracy across the 1,000 cortical regions of the Schaefer atlas. The final
model achieved an eighth-place ranking in the Algonauts Project 2025 Challenge,
with out-of-distribution (OOD) correlation scores nearly double those of the
baseline model used in the selection phase. Code is available at
https://github.com/Corsi01/algo2025.

</details>


### [356] [Understanding Human Limits in Pattern Recognition: A Computational Model of Sequential Reasoning in Rock, Paper, Scissors](https://arxiv.org/abs/2508.06503)
*Logan Cross,Erik Brockbank,Tobias Gerstenberg,Judith E. Fan,Daniel L. K. Yamins,Nick Haber*

Main category: q-bio.NC

TL;DR: 研究人类在石头剪刀布重复游戏中预测对手行为的能力及计算限制，用HM模型模拟人类表现，揭示准确假设生成是主要认知瓶颈。


<details>
  <summary>Details</summary>
Motivation: 探究人类如何从行为模式预测他人以及限制该能力的计算约束。

Method: 对Brockbank & Vul (2024)的石头剪刀布重复游戏数据建模，用HM模型模拟人类行为，进行消融和增强实验，通过教学干预操纵模型假设。

Result: HM模型表现与人类相似，提供对手策略描述时能成功击败多数对手，教学干预可使模型更新对对手行为的因果理解。

Conclusion: 准确假设生成是该任务的主要认知瓶颈，基于模型的分析可产生关于人类认知的可检验假设。

Abstract: How do we predict others from patterns in their behavior and what are the
computational constraints that limit this ability? We investigate these
questions by modeling human behavior over repeated games of rock, paper,
scissors from Brockbank & Vul (2024). Against algorithmic opponents that varied
in strategic sophistication, people readily exploit simple transition patterns
(e.g., consistently playing rock after paper) but struggle to detect more
complex sequential dependencies. To understand the cognitive mechanisms
underlying these abilities and their limitations, we deploy Hypothetical Minds
(HM), a large language model-based agent that generates and tests hypotheses
about opponent strategies, as a cognitive model of this behavior (Cross et al.,
2024). We show that when applied to the same experimental conditions, HM
closely mirrors human performance patterns, succeeding and failing in similar
ways. To better understand the source of HM's failures and whether people might
face similar cognitive bottlenecks in this context, we performed a series of
ablations and augmentations targeting different components of the system. When
provided with natural language descriptions of the opponents' strategies, HM
successfully exploited 6/7 bot opponents with win rates >80% suggesting that
accurate hypothesis generation is the primary cognitive bottleneck in this
task. Further, by systematically manipulating the model's hypotheses through
pedagogically-inspired interventions, we find that the model substantially
updates its causal understanding of opponent behavior, revealing how
model-based analyses can produce testable hypotheses about human cognition.

</details>


### [357] [Sensory robustness through top-down feedback and neural stochasticity in recurrent vision models](https://arxiv.org/abs/2508.07115)
*Antonino Greco,Marco D'Alessandro,Karl J. Friston,Giovanni Pezzulo,Markus Siegel*

Main category: q-bio.NC

TL;DR: 研究卷积循环神经网络（ConvRNN）中自上而下反馈的作用，发现有反馈且经随机失活训练的网络在速度 - 准确率权衡、抗噪和对抗攻击方面表现出色，揭示了弹性感官编码的双重机制。


<details>
  <summary>Details</summary>
Motivation: 生物视觉处理利用自上而下反馈，而多数人工视觉模型靠前馈或循环架构进行图像分类，需阐明自上而下反馈通路的计算贡献。

Method: 训练有或无自上而下反馈投影的ConvRNN进行图像分类，用随机失活模拟神经随机变异性。

Result: 有反馈且经随机失活训练的ConvRNN在速度 - 准确率权衡、抗噪和对抗攻击方面表现好；反馈信息塑造后整合层表征几何，与随机失活结合将网络活动约束到低维流形，更有效地编码分布外物体信息。

Conclusion: 发现弹性感官编码的双重机制，神经随机性防止单元共适应，自上而下反馈稳定网络活动。

Abstract: Biological systems leverage top-down feedback for visual processing, yet most
artificial vision models succeed in image classification using purely
feedforward or recurrent architectures, calling into question the functional
significance of descending cortical pathways. Here, we trained convolutional
recurrent neural networks (ConvRNN) on image classification in the presence or
absence of top-down feedback projections to elucidate the specific
computational contributions of those feedback pathways. We found that ConvRNNs
with top-down feedback exhibited remarkable speed-accuracy trade-off and
robustness to noise perturbations and adversarial attacks, but only when they
were trained with stochastic neural variability, simulated by randomly
silencing single units via dropout. By performing detailed analyses to identify
the reasons for such benefits, we observed that feedback information
substantially shaped the representational geometry of the post-integration
layer, combining the bottom-up and top-down streams, and this effect was
amplified by dropout. Moreover, feedback signals coupled with dropout optimally
constrained network activity onto a low-dimensional manifold and encoded object
information more efficiently in out-of-distribution regimes, with top-down
information stabilizing the representational dynamics at the population level.
Together, these findings uncover a dual mechanism for resilient sensory coding.
On the one hand, neural stochasticity prevents unit-level co-adaptation albeit
at the cost of more chaotic dynamics. On the other hand, top-down feedback
harnesses high-level information to stabilize network activity on compact
low-dimensional manifolds.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [358] [Reducing Cartel Violence: The Mexican Dilemma Between Social and Security Spending](https://arxiv.org/abs/2508.06509)
*Rafael Prieto-Curiel,Dieter Grass,Stefan Wrzaczek,Gian Maria Campedelli,Gernot Tragler,Gustav Feichtinger*

Main category: physics.soc-ph

TL;DR: 本文评估墨西哥打击有组织犯罪的现有政策，用最优控制理论分析，发现最大卡特尔年经济负担超190亿美元，当前预算分配接近最优但不足以显著减少暴力，大幅减少危害需更多预算和超十年时间。


<details>
  <summary>Details</summary>
Motivation: 墨西哥有组织犯罪威胁社会稳定与公共安全，现有安全投资和社会项目未能有效遏制卡特尔势力与暴力。

Method: 使用最优控制理论构建框架，对安全措施和社会项目的资源分配进行建模。

Result: 墨西哥最大卡特尔年经济负担超190亿美元，是政府科技投资2.5倍；当前社会和安全项目预算分配接近最优，但不足以显著减少卡特尔暴力。

Conclusion: 要实现有意义的危害减少，需要大幅增加预算，且即便增加资金也需超十年时间。

Abstract: Organised crime in Mexico threatens societal stability and public safety,
driving pervasive violence and economic disruption. Despite security
investments and social programs designed in part to reduce involvement in
crime, cartel power and violence continue to persist. This study evaluates
existing policies and introduces a novel framework using optimal control theory
to analyse cartel dynamics. Specifically, by modelling resource allocation
between security measures and social programs, we identify optimal strategies
to mitigate the impacts of cartels. Findings reveal that Mexico's largest
cartel imposes an annual economic burden exceeding \text{US\$ } 19 billion, 2.5
times the government's investment in science and technology. We further
demonstrate that current budget allocations between social and security
programs are nearly optimal yet insufficient to reduce cartel violence
significantly. In light of these findings, we demonstrate that achieving
meaningful harm reduction would require a significantly larger budget and would
take over a decade, even with increased funding.

</details>


### [359] [Inter-role reciprocity in evolutionary trust game on square lattices](https://arxiv.org/abs/2508.06685)
*Chaoqian Wang,Wei Zhang,Xinwei Wang,Attila Szolnoki*

Main category: physics.soc-ph

TL;DR: 利用方形晶格拓扑模拟双边博弈，发现角色间空间互惠机制使信任出现，模拟框架适用于双边博弈。


<details>
  <summary>Details</summary>
Motivation: 双边博弈模拟因单一种群缺乏区分角色的自然方式而不直接，需解决该问题。

Method: 采用方形晶格拓扑，为策略学习创建两个不相交的对角子晶格，在原晶格上进行游戏交互，并进行模拟。

Result: 检测到角色间空间互惠机制使信任出现，适度回报率能让投资者和受托人形成角色间集群，过高或过低回报率分别损害受托人或信任者生存。

Conclusion: 提出的模拟框架适用于任何双边博弈，可揭示不同场景下潜在的角色间空间机制。

Abstract: Simulating bipartite games, such as the trust game, is not straightforward
due to the lack of a natural way to distinguish roles in a single population.
The square lattice topology can provide a simple yet elegant solution by
alternating trustors and trustees. For even lattice sizes, it creates two
disjoint diagonal sub-lattices for strategy learning, while game interactions
can take place on the original lattice. This setup ensures a minimal spatial
structure that allows interactions across roles and learning within roles. By
simulations on this setup, we detect an inter-role spatial reciprocity
mechanism, through which trust can emerge. In particular, a moderate return
ratio allows investing trustors and trustworthy trustees to form inter-role
clusters and thus save trust. If the return is too high, it harms the survival
of trustees; if too low, it harms trustors. The proposed simulation framework
is also applicable to any bipartite game to uncover potential inter-role
spatial mechanisms across various scenarios.

</details>


### [360] [Do Streetscapes Still Matter for Customer Ratings of Eating and Drinking Establishments in Car-Dependent Cities?](https://arxiv.org/abs/2508.06513)
*Chaeyeon Han,Seung Jae Lieu,Uijeong Hwang,Subhrajit Guhathakurta*

Main category: physics.soc-ph

TL;DR: 研究华盛顿特区不同汽车依赖程度城市环境中，室内外美学、街景和社区特征对餐饮场所顾客满意度的影响，发现室内外环境均显著影响评分，街景质量在汽车依赖区影响减弱，强调因地制宜规划。


<details>
  <summary>Details</summary>
Motivation: 探究不同城市环境中，室内外美学、街景和社区特征如何塑造餐饮场所的顾客满意度。

Method: 利用点评照片和街景图像，通过计算机视觉模型量化感知安全和视觉吸引力，用有序逻辑回归分析其对Yelp评分的影响。

Result: 室内外环境均显著影响餐饮场所评分，街景质量在汽车依赖区的影响减弱。

Conclusion: 需要进行因地制宜的规划，整合室内外因素以提升不同环境下的顾客体验。

Abstract: This study examines how indoor and outdoor aesthetics, streetscapes, and
neighborhood features shape customer satisfaction at eating and dining
establishments (EDEs) across different urban contexts, varying in car
dependency, in Washington, DC. Using review photos and street view images,
computer vision models quantified perceived safety and visual appeal. Ordinal
logistic regression analyzed their effects on Yelp ratings. Findings reveal
that both indoor and outdoor environments significantly impact EDE ratings,
while streetscape quality's influence diminishes in car-dependent areas. The
study highlights the need for context-sensitive planning that integrates indoor
and outdoor factors to enhance customer experiences in diverse settings.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [361] [Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities](https://arxiv.org/abs/2508.07031)
*Anindya Bijoy Das,Shahnewaz Karim Sakib,Shibbir Ahmed*

Main category: eess.IV

TL;DR: 研究大语言模型在医学影像任务中的幻觉问题，分析图像到文本和文本到图像两个方向，揭示常见幻觉模式并讨论影响因素，为提升系统安全性和可信度提供见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医学影像任务中常产生误导临床决策的幻觉，需对其进行研究。

Method: 从图像到文本和文本到图像两个方向分析，通过专家标准评估不同成像方式的输出，分析事实不一致和解剖学不准确等错误。

Result: 揭示了解释性和生成性任务中常见的幻觉模式。

Conclusion: 通过系统研究图像理解和生成，为提高大语言模型驱动的医学影像系统的安全性和可信度提供了见解。

Abstract: Large Language Models (LLMs) are increasingly applied to medical imaging
tasks, including image interpretation and synthetic image generation. However,
these models often produce hallucinations, which are confident but incorrect
outputs that can mislead clinical decisions. This study examines hallucinations
in two directions: image to text, where LLMs generate reports from X-ray, CT,
or MRI scans, and text to image, where models create medical images from
clinical prompts. We analyze errors such as factual inconsistencies and
anatomical inaccuracies, evaluating outputs using expert informed criteria
across imaging modalities. Our findings reveal common patterns of hallucination
in both interpretive and generative tasks, with implications for clinical
reliability. We also discuss factors contributing to these failures, including
model architecture and training data. By systematically studying both image
understanding and generation, this work provides insights into improving the
safety and trustworthiness of LLM driven medical imaging systems.

</details>


### [362] [Transfer Learning with EfficientNet for Accurate Leukemia Cell Classification](https://arxiv.org/abs/2508.06535)
*Faisal Ahmed*

Main category: eess.IV

TL;DR: 研究使用预训练CNN的迁移学习提高急性淋巴细胞白血病（ALL）诊断性能，经数据增强后评估多模型，EfficientNet - B3效果最佳，证明结合数据增强和先进迁移学习模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 准确分类ALL外周血涂片图像对早期诊断和治疗规划至关重要，需提高诊断性能。

Method: 采用预训练CNN的迁移学习，应用数据增强技术处理类别不平衡问题，评估ResNet50、ResNet101和EfficientNet多个变体模型。

Result: EfficientNet - B3取得最佳结果，F1分数94.30%，准确率92.02%，AUC为94.79%，优于C - NMC挑战中先前报告的方法。

Conclusion: 结合数据增强与先进迁移学习模型，特别是EfficientNet - B3，在开发准确、稳健的血液恶性肿瘤诊断工具方面有效。

Abstract: Accurate classification of Acute Lymphoblastic Leukemia (ALL) from peripheral
blood smear images is essential for early diagnosis and effective treatment
planning. This study investigates the use of transfer learning with pretrained
convolutional neural networks (CNNs) to improve diagnostic performance. To
address the class imbalance in the dataset of 3,631 Hematologic and 7,644 ALL
images, we applied extensive data augmentation techniques to create a balanced
training set of 10,000 images per class. We evaluated several models, including
ResNet50, ResNet101, and EfficientNet variants B0, B1, and B3. EfficientNet-B3
achieved the best results, with an F1-score of 94.30%, accuracy of 92.02%,
andAUCof94.79%,outperformingpreviouslyreported methods in the C-NMCChallenge.
Thesefindings demonstrate the effectiveness of combining data augmentation with
advanced transfer learning models, particularly EfficientNet-B3, in developing
accurate and robust diagnostic tools for hematologic malignancy detection.

</details>


### [363] [Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications](https://arxiv.org/abs/2508.07165)
*Zelin Qiu,Xi Wang,Zhuoyao Xie,Juan Zhou,Yu Wang,Lingjie Yang,Xinrui Jiang,Juyoung Bae,Moo Hyun Son,Qiang Ye,Dexuan Chen,Rui Zhang,Tao Li,Neeraj Ramesh Mahboobani,Varut Vardhanabhuti,Xiaohui Duan,Yinghua Zhao,Hao Chen*

Main category: eess.IV

TL;DR: 提出PRISM基础模型，用大规模多序列MRI预训练，在44个下游任务中表现出色，提升AI在放射学的转化潜力。


<details>
  <summary>Details</summary>
Motivation: 多序列MRI序列间异质性影响深度学习模型泛化能力，限制临床应用。

Method: 收集64个数据集，构建最大多器官多序列MRI预训练语料库；提出新预训练范式，分离解剖不变特征和序列特定变化；建立44个下游任务基准。

Result: PRISM在44个下游基准中的39个排名第一，显著优于未预训练模型和现有基础模型。

Conclusion: PRISM为多序列MRI分析提供可扩展框架，增强AI在放射学临床应用潜力。

Abstract: Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkable
versatility, enabling the distinct visualization of different tissue types.
Nevertheless, the inherent heterogeneity among MRI sequences poses significant
challenges to the generalization capability of deep learning models. These
challenges undermine model performance when faced with varying acquisition
parameters, thereby severely restricting their clinical utility. In this study,
we present PRISM, a foundation model PRe-trained with large-scale
multI-Sequence MRI. We collected a total of 64 datasets from both public and
private sources, encompassing a wide range of whole-body anatomical structures,
with scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRI
scans from 34 datasets (8 public and 26 private) were curated to construct the
largest multi-organ multi-sequence MRI pretraining corpus to date. We propose a
novel pretraining paradigm that disentangles anatomically invariant features
from sequence-specific variations in MRI, while preserving high-level semantic
representations. We established a benchmark comprising 44 downstream tasks,
including disease diagnosis, image segmentation, registration, progression
prediction, and report generation. These tasks were evaluated on 32 public
datasets and 5 private cohorts. PRISM consistently outperformed both
non-pretrained models and existing foundation models, achieving first-rank
results in 39 out of 44 downstream benchmarks with statistical significance
improvements. These results underscore its ability to learn robust and
generalizable representations across unseen data acquired under diverse MRI
protocols. PRISM provides a scalable framework for multi-sequence MRI analysis,
thereby enhancing the translational potential of AI in radiology. It delivers
consistent performance across diverse imaging protocols, reinforcing its
clinical applicability.

</details>


### [364] [PCA-Guided Autoencoding for Structured Dimensionality Reduction in Active Infrared Thermography](https://arxiv.org/abs/2508.07773)
*Mohammed Salah,Numan Saeed,Davor Svetinovic,Stefano Sfarra,Mohammed Omar,Yusra Abdulrahman*

Main category: eess.IV

TL;DR: 提出PCA引导自动编码框架用于主动红外热成像数据降维，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前主动红外热成像采用非线性自编码器降维，但其潜在空间缺乏结构，限制缺陷表征效果。

Method: 提出PCA引导自动编码框架，引入PCA蒸馏损失函数，还提出基于神经网络的评估指标。

Result: 在PVC、CFRP和PLA样本上，所提PCA引导自编码器在对比度、信噪比和基于神经网络的指标上优于现有降维方法。

Conclusion: 所提PCA引导自动编码框架能有效进行结构化降维，提升缺陷表征效果。

Abstract: Active Infrared thermography (AIRT) is a widely adopted non-destructive
testing (NDT) technique for detecting subsurface anomalies in industrial
components. Due to the high dimensionality of AIRT data, current approaches
employ non-linear autoencoders (AEs) for dimensionality reduction. However, the
latent space learned by AIRT AEs lacks structure, limiting their effectiveness
in downstream defect characterization tasks. To address this limitation, this
paper proposes a principal component analysis guided (PCA-guided) autoencoding
framework for structured dimensionality reduction to capture intricate,
non-linear features in thermographic signals while enforcing a structured
latent space. A novel loss function, PCA distillation loss, is introduced to
guide AIRT AEs to align the latent representation with structured PCA
components while capturing the intricate, non-linear patterns in thermographic
signals. To evaluate the utility of the learned, structured latent space, we
propose a neural network-based evaluation metric that assesses its suitability
for defect characterization. Experimental results show that the proposed
PCA-guided AE outperforms state-of-the-art dimensionality reduction methods on
PVC, CFRP, and PLA samples in terms of contrast, signal-to-noise ratio (SNR),
and neural network-based metrics.

</details>


### [365] [MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer](https://arxiv.org/abs/2508.07817)
*Tao Tang,Chengxu Yang*

Main category: eess.IV

TL;DR: 提出集成多尺度卷积和Transformer架构的医学图像自适应去噪模型MI - ND，经测试在图像质量指标和下游诊断任务中表现优异，有实用和推广价值。


<details>
  <summary>Details</summary>
Motivation: 医学图像常受非均匀噪声干扰，影响结构识别和病变检测，需提高图像质量以提升临床判断准确性。

Method: 提出医学图像自适应去噪模型MI - ND，引入噪声水平估计器NLE和噪声自适应注意力模块NAAB，实现通道 - 空间注意力调节和跨模态特征融合。

Result: 在多模态公共数据集上测试，该方法在PSNR、SSIM、LPIPS等图像质量指标上显著优于对比方法，提高了下游诊断任务的F1分数和ROC - AUC。

Conclusion: 模型在结构恢复、诊断敏感性和跨模态鲁棒性方面表现出色，为医学图像增强和AI辅助诊疗提供有效解决方案。

Abstract: The core role of medical images in disease diagnosis makes their quality
directly affect the accuracy of clinical judgment. However, due to factors such
as low-dose scanning, equipment limitations and imaging artifacts, medical
images are often accompanied by non-uniform noise interference, which seriously
affects structure recognition and lesion detection. This paper proposes a
medical image adaptive denoising model (MI-ND) that integrates multi-scale
convolutional and Transformer architecture, introduces a noise level estimator
(NLE) and a noise adaptive attention module (NAAB), and realizes
channel-spatial attention regulation and cross-modal feature fusion driven by
noise perception. Systematic testing is carried out on multimodal public
datasets. Experiments show that this method significantly outperforms the
comparative methods in image quality indicators such as PSNR, SSIM, and LPIPS,
and improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing
strong prac-tical value and promotional potential. The model has outstanding
benefits in structural recovery, diagnostic sensitivity, and cross-modal
robustness, and provides an effective solution for medical image enhancement
and AI-assisted diagnosis and treatment.

</details>


### [366] [Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images](https://arxiv.org/abs/2508.07875)
*Shuo Han,Ahmed Karam Eldaly,Solomon Sunday Oyelere*

Main category: eess.IV

TL;DR: 本文提出人在回路深度学习系统检测浸润性导管癌（IDC），迭代优化模型性能，证明人机协作可提升AI诊断性能。


<details>
  <summary>Details</summary>
Motivation: 浸润性导管癌是最常见的乳腺癌形式，早期准确诊断对提高患者生存率至关重要，结合医学专业知识和人工智能有望提高IDC检测的精度和效率。

Method: 提出人在回路（HITL）深度学习系统，先由EfficientNetV2S模型进行初始诊断，医学专家审查并修正错误分类图像，将修正标签整合到训练数据集中形成反馈循环。

Result: EfficientNetV2S模型本身准确率达93.65%，结合人在回路系统，通过四个包含错误分类图像的实验组进一步提高了模型的准确性。

Conclusion: 人机协作方法有潜力提升AI在诊断系统中的性能，为未来AI辅助医学诊断提供了有前景的方向。

Abstract: Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer,
and early, accurate diagnosis is critical to improving patient survival rates
by guiding treatment decisions. Combining medical expertise with artificial
intelligence (AI) holds significant promise for enhancing the precision and
efficiency of IDC detection. In this work, we propose a human-in-the-loop
(HITL) deep learning system designed to detect IDC in histopathology images.
The system begins with an initial diagnosis provided by a high-performance
EfficientNetV2S model, offering feedback from AI to the human expert. Medical
professionals then review the AI-generated results, correct any misclassified
images, and integrate the revised labels into the training dataset, forming a
feedback loop from the human back to the AI. This iterative process refines the
model's performance over time. The EfficientNetV2S model itself achieves
state-of-the-art performance compared to existing methods in the literature,
with an overall accuracy of 93.65\%. Incorporating the human-in-the-loop system
further improves the model's accuracy using four experimental groups with
misclassified images. These results demonstrate the potential of this
collaborative approach to enhance AI performance in diagnostic systems. This
work contributes to advancing automated, efficient, and highly accurate methods
for IDC detection through human-AI collaboration, offering a promising
direction for future AI-assisted medical diagnostics.

</details>


### [367] [Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models](https://arxiv.org/abs/2508.07903)
*Johanna P. Müller,Anika Knupfer,Pedro Blöss,Edoardo Berardi Vittur,Bernhard Kainz,Jana Hutter*

Main category: eess.IV

TL;DR: 提出用于子宫MRI合成的扩散框架，生成高质量合成图像，提高诊断准确性并发布模型和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型难以生成解剖学精确的女性盆腔图像，数据稀缺和患者隐私问题限制其在妇科成像应用。

Method: 引入基于扩散的框架，集成无条件和条件的Denoising Diffusion Probabilistic Models (DDPMs) 和Latent Diffusion Models (LDMs) 进行2D和3D子宫MRI合成。

Result: 生成解剖学连贯、高保真合成图像，在诊断分类任务中提高准确性，专家评估验证图像临床真实性。

Conclusion: 发布带隐私保护的模型和综合数据集，支持可重复研究，推动妇科公平AI发展。

Abstract: Despite significant progress in generative modelling, existing diffusion
models often struggle to produce anatomically precise female pelvic images,
limiting their application in gynaecological imaging, where data scarcity and
patient privacy concerns are critical. To overcome these barriers, we introduce
a novel diffusion-based framework for uterine MRI synthesis, integrating both
unconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs)
and Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates
anatomically coherent, high fidelity synthetic images that closely mimic real
scans and provide valuable resources for training robust diagnostic models. We
evaluate generative quality using advanced perceptual and distributional
metrics, benchmarking against standard reconstruction methods, and demonstrate
substantial gains in diagnostic accuracy on a key classification task. A
blinded expert evaluation further validates the clinical realism of our
synthetic images. We release our models with privacy safeguards and a
comprehensive synthetic uterine MRI dataset to support reproducible research
and advance equitable AI in gynaecology.

</details>


### [368] [RedDino: A foundation model for red blood cell analysis](https://arxiv.org/abs/2508.08180)
*Luca Zedda,Andrea Loddo,Cecilia Di Ruberto,Carsten Marr*

Main category: eess.IV

TL;DR: 提出用于红细胞图像分析的自监督基础模型RedDino，在形状分类上优于现有模型，有代码和预训练模型开源。


<details>
  <summary>Details</summary>
Motivation: 红细胞形态分析对诊断血液疾病重要，但全面的AI解决方案稀缺。

Method: 采用DINOv2自监督学习框架的红细胞特定适应版本，在125万张来自不同来源的红细胞图像数据集上训练。

Result: RedDino在红细胞形状分类上优于现有模型，有强特征表示和泛化能力。

Conclusion: RedDino解决了计算血液学关键挑战，推动可靠诊断工具发展。

Abstract: Red blood cells (RBCs) are essential to human health, and their precise
morphological analysis is important for diagnosing hematological disorders.
Despite the promise of foundation models in medical diagnostics, comprehensive
AI solutions for RBC analysis remain scarce. We present RedDino, a
self-supervised foundation model designed for RBC image analysis. RedDino uses
an RBC-specific adaptation of the DINOv2 self-supervised learning framework and
is trained on a curated dataset of 1.25 million RBC images from diverse
acquisition modalities and sources. Extensive evaluations show that RedDino
outperforms existing state-of-the-art models on RBC shape classification.
Through assessments including linear probing and nearest neighbor
classification, we confirm its strong feature representations and
generalization ability. Our main contributions are: (1) a foundation model
tailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations
for RBC modeling, and (3) a detailed evaluation of generalization performance.
RedDino addresses key challenges in computational hematology by capturing
nuanced morphological features, advancing the development of reliable
diagnostic tools. The source code and pretrained models for RedDino are
available at https://github.com/Snarci/RedDino, and the pretrained models can
be downloaded from our Hugging Face collection at
https://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [369] [Unwitting Markowitz' Simplification of Portfolio Random Returns](https://arxiv.org/abs/2508.08148)
*Victor Olkhov*

Main category: econ.GN

TL;DR: 指出Markowitz模型是对真实市场的简化近似，考虑交易数量随机性可得到基于市场的投资组合方差，其与Markowitz方差估计有差异。


<details>
  <summary>Details</summary>
Motivation: 研究Markowitz模型对真实市场的适用性。

Method: 考虑不交易证券的投资者，观察市场交易并推导模拟投资组合交易的时间序列。

Result: 投资组合时间序列揭示随机回报依赖证券回报及交易数量比，假设交易数量恒定可得Markowitz方程，市场方差考虑交易数量随机波动。

Conclusion: Markowitz方差与基于市场的投资组合方差估计有显著差异。

Abstract: In his famous paper, Markowitz (1952) derived the dependence of portfolio
random returns on the random returns of its securities. This result allowed
Markowitz to obtain his famous expression for portfolio variance. We show that
Markowitz's equation for portfolio random returns and the expression for
portfolio variance, which results from it, describe a simplified approximation
of the real markets when the volumes of all consecutive trades with the
securities are assumed to be constant during the averaging interval. To show
this, we consider the investor who doesn't trade shares of securities of his
portfolio. The investor only observes the trades made in the market with his
securities and derives the time series that model the trades with his portfolio
as with a single security. These time series describe the portfolio return and
variance in exactly the same way as the time series of trades with securities
describe their returns and variances. The portfolio time series reveal the
dependence of portfolio random returns on the random returns of securities and
on the ratio of the random volumes of trades with the securities to the random
volumes of trades with the portfolio. If we assume that all volumes of the
consecutive trades with securities are constant, obtain Markowitz's equation
for the portfolio's random returns. The market-based variance of the portfolio
accounts for the effects of random fluctuations of the volumes of the
consecutive trades. The use of Markowitz variance may give significantly higher
or lower estimates than market-based portfolio variance.

</details>


### [370] [Stochastic Boundaries in Spatial General Equilibrium: A Diffusion-Based Approach to Causal Inference with Spillover Effects](https://arxiv.org/abs/2508.06594)
*Tatsuru Kikuchi*

Main category: econ.GN

TL;DR: 本文提出空间经济学因果推断新框架，结合DDPM与边界检测方法，应用于日本AI采纳分析，发现处理效应在约35km尺度有跳跃，忽视边界会低估效应，框架为政策评估提供指导。


<details>
  <summary>Details</summary>
Motivation: 在空间经济学中建立能明确建模从局部到一般均衡效应随机转变的因果推断框架。

Method: 开发集成边界检测方法的Denoising Diffusion Probabilistic Model (DDPM)，将空间溢出演变视为带跳跃扩散动态的Lévy过程，用CUSUM-based sequential detection确定边界。

Result: 处理效应在约35km空间尺度有Lévy跳跃，一般均衡效应使局部均衡估计放大42%，忽视边界会低估处理效应28 - 67%。

Conclusion: 该框架是确定空间溢出何时需要一般均衡分析的首个严格方法，为互联经济体政策评估提供关键指导。

Abstract: This paper introduces a novel framework for causal inference in spatial
economics that explicitly models the stochastic transition from partial to
general equilibrium effects. We develop a Denoising Diffusion Probabilistic
Model (DDPM) integrated with boundary detection methods from stochastic process
theory to identify when and how treatment effects propagate beyond local
markets. Our approach treats the evolution of spatial spillovers as a L\'evy
process with jump-diffusion dynamics, where the first passage time to critical
thresholds indicates regime shifts from partial to general equilibrium. Using
CUSUM-based sequential detection, we identify the spatial and temporal
boundaries at which local interventions become systemic. Applied to AI adoption
across Japanese prefectures, we find that treatment effects exhibit L\'evy
jumps at approximately 35km spatial scales, with general equilibrium effects
amplifying partial equilibrium estimates by 42\%. Monte Carlo simulations show
that ignoring these stochastic boundaries leads to underestimation of treatment
effects by 28-67\%, with particular severity in densely connected economic
regions. Our framework provides the first rigorous method for determining when
spatial spillovers necessitate general equilibrium analysis, offering crucial
guidance for policy evaluation in interconnected economies.

</details>


### [371] [Fiscal Spillovers through Informal Financial Channels](https://arxiv.org/abs/2508.06662)
*Austin Kennedy*

Main category: econ.GN

TL;DR: 本文以美国刺激支票为财政冲击，用加密货币数据研究财政政策溢出，发现汇款渠道财政溢出规模可能较小。


<details>
  <summary>Details</summary>
Motivation: 研究财政政策通过非正式国际金融渠道的溢出效应。

Method: 利用加密货币交易数据构建双边加密货币流动，采用双重差分策略对比美国与其他高收入国家的加密货币流出。

Result: 直接刺激导致加密货币流出急剧但短暂增加，量化该渠道财政溢出相对支出上限为2.52%。

Conclusion: 汇款渠道的财政溢出规模可能较小。

Abstract: This paper examines fiscal policy spillovers through informal international
financial channels, using the US stimulus checks as a positive, sudden, and
direct fiscal shock. I utilize granular, transaction-level cryptocurrency data
combined with an algorithm to probabilistically identify cross-border "crypto
vehicle" transactions to construct bilateral cryptocurrency flows between
countries. Using a difference-in-differences strategy, I compare cryptocurrency
outflows between the US and other high-income countries and find a sharp but
temporary increase in cryptocurrency outflows as a result of the direct
stimulus. I quantify the fiscal spillover relative to expenditure and place an
upper bound of 2.52% through this channel. This implies that fiscal spillovers
through remittance channels are likely modest in size.

</details>


### [372] [The impact of brand equity on vertical integration in franchise systems](https://arxiv.org/abs/2508.06824)
*Mohammad Kayed,Manish Kacker,Ruhai Wu,Farhad Sadeh*

Main category: econ.GN

TL;DR: 研究品牌资产对特许经营系统垂直整合的影响，发现品牌资产对垂直整合有滞后的反向影响，还分析了边界条件，挑战传统观点并给出管理建议。


<details>
  <summary>Details</summary>
Motivation: 以往对品牌资产和垂直整合在特许经营系统中的相互作用理解不完整，重新探讨品牌资产如何影响垂直整合这一存在五十年的问题。

Method: 使用贝叶斯面板向量自回归模型分析大型面板数据集，进行反向因果分析和边界条件分析。

Result: 品牌资产对垂直整合有强大的滞后反向影响，反向因果关系较弱；在有国际业务和零售为主的特许经营系统中，品牌资产对垂直整合的负面影响较弱，财务资源多的系统中影响更强。

Conclusion: 挑战了传统观点，揭示了复杂动态和偶然性；指出品牌资产在缓解渠道治理问题上的战略益处，建议品牌资产强时避免不必要的垂直整合。

Abstract: Brand equity and vertical integration are focal, strategic elements of a
franchise system that can profoundly influence franchise performance. Despite
the recognized importance of these two strategic levers and the longstanding
research interest in the topic, our understanding of the interplay between
brand equity and vertical integration (company ownership of outlets) in a
franchise system remains incomplete. In this study, we revisit the
five-decade-old question of how brand equity affects vertical integration in a
franchise system and present some novel, nuanced insights into the topic.
Evidence from a Bayesian Panel Vector Autoregressive model on a large panel
data set shows that brand equity has a powerful, lagging inverse effect on
vertical integration, such that higher brand equity leads to less downstream
vertical integration in a franchise system. Reverse causality analyses identify
a less pronounced but present reciprocal effect. Boundary conditions analyses
reveal that the negative effect of brand equity on vertical integration is
weaker in franchise systems with international presence and in retail-focused
(vs. service-focused) franchises, and stronger in franchise systems with more
financial resources. These findings (a) challenge traditional views (e.g.,
transaction cost theory, resource-based view, ownership redirection hypothesis)
on the topic by demonstrating a negative effect for brand equity on vertical
integration in franchise systems and showing that greater financial resources
amplify this effect, and (b) shed new light on the intricate dynamics (temporal
causation, reverse causation) and contingencies of this debated effect.
Managerially, this research draws attention to the underrecognized strategic
benefit of brand equity in mitigating channel governance issues and advise
against unnecessary vertical integration, especially when brand equity is
robust.

</details>


### [373] [Global Supply Chain Reallocation and Shift under Triple Crises: A U.S.-China Perspective](https://arxiv.org/abs/2508.06828)
*Wei Luo,Siyuan Kang,Qian Di*

Main category: econ.GN

TL;DR: 研究在2016 - 2023年地缘政治和公共卫生干扰下中美及主要贸易伙伴双边贸易和全球价值链的重构，发现中国出口稳健，美国进口向“中国+1”伙伴转移，提出供应链韧性框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究警示中美贸易紧张等可能无法有效降低美国对中国供应链依赖，本文旨在研究未实现重新配置的驱动因素。

Method: 使用全行业月度双边贸易数据和多区域投入产出表进行全球价值链分解，结合多期事件研究与结构分析。

Result: 中国出口稳健、全球市场扩张、全球价值链参与度上升且更嵌入上游；美国进口转向“中国+1”伙伴，这些伙伴供应链与中国紧密相连。

Conclusion: 提出由全球价值链参与度、价值链功能位置和后冲击时期重新耦合能力三个维度构成的供应链韧性框架，对贸易政策和产业战略有重要意义。

Abstract: US-China trade tensions, the COVID-19 pandemic, and the Russia-Ukraine
conflict have disrupted and reshaped global supply chains. Existing studies
caution that these tensions may not meaningfully reduce U.S. dependence on
China-linked supply chains. This study examines the drivers of this unmet
reallocation under overlapping geopolitical and public health disruptions. It
investigates how these shocks jointly reconfigured bilateral trade and global
value chain (GVC) participation and positioning among the U.S., China, and
major trading partners during 2016-2023. Using monthly bilateral trade data
across all sectors and multi-regional input-output tables for GVC
decomposition, we combine a multi-period event-study with structural analysis
to evaluate trade-flow disruptions and shifts in participation and functional
positioning within GVCs. We find that China's exports remained robust, expanded
across global markets, and sustained a rise in GVC participation, becoming more
embedded in upstream segments through increased intermediate shipments to Asia
and Europe. Meanwhile, U.S. imports increasingly shifted toward "China+1"
partners, especially ASEAN, whose trade structures remain closely tied to
Chinese upstream supply chains. These strengthening triangular relationships
reveal how global reallocation and GVCs have evolved around the U.S. and China
across successive shocks. Based on the evidence, we propose a supply chain
resilience framework defined by three interacting dimensions: the level of GVC
participation, the functional position within the value chain, and a country's
capacity to re-couple in the post-shock landscape, conditioned by market
diversification, economic complexity, and institutional capability. These
findings carry significant implications for trade policy and industrial
strategy in an era of geopolitical and geoeconomic fragmentation.

</details>


### [374] [The Interaction Between Domestic Monetary Policy and Macroprudential Policy in Israel](https://arxiv.org/abs/2508.07082)
*Jonathan Benchimol,Inon Gamrasni,Michael Kahn,Sigal Ribon,Yossi Saadon,Noam Ben-Ze'ev,Asaf Segal,Yitzchak Shizgal*

Main category: econ.GN

TL;DR: 利用以色列银行数据研究发现，国内宏观审慎措施改变银行信贷增长构成但不影响总信贷增长率，宽松货币政策与宏观审慎政策相互作用影响不同类型信贷。


<details>
  <summary>Details</summary>
Motivation: 全球金融危机促使对银行业实施宏观审慎政策，研究其对银行信贷的影响。

Method: 使用2004 - 2019年以色列银行层面的面板数据进行研究。

Result: 宏观审慎措施改变信贷构成，针对住房部门的措施抑制住房信贷、增加商业信贷；宽松货币政策意外在危机前增加银行信贷，与住房市场宏观审慎政策相互作用增加消费信贷，与非住房宏观审慎措施相互作用增加总信贷。

Conclusion: 宏观审慎措施和货币政策对银行信贷有不同影响，且两者相互作用影响不同类型信贷。

Abstract: The global financial crisis (GFC) triggered the use of macroprudential
policies imposed on the banking sector. Using bank-level panel data for Israel
for the period 2004-2019, we find that domestic macroprudential measures
changed the composition of bank credit growth but did not affect the total
credit growth rate. Specifically, we show that macroprudential measures
targeted at the housing sector moderated housing credit growth but tended to
increase business credit growth. We also find that accommodative monetary
policy surprises tended to increase bank credit growth before the GFC. We show
that accommodative monetary policy surprises increased consumer credit when
interacting with macroprudential policies targeting the housing market.
Accommodative monetary policy interacted with nonhousing macroprudential
measures to increase total credit.

</details>


### [375] [What is required for a post-growth model?](https://arxiv.org/abs/2508.07974)
*Rob Van Eynde,Kevin J. Dillman,Jefim Vogel,Daniel W. O'Neill*

Main category: econ.GN

TL;DR: 文章针对后增长情景建模缺乏连贯框架的问题，开发了后增长建模最低要求框架，能指导模拟后增长和促增长政策及情景。


<details>
  <summary>Details</summary>
Motivation: 当前虽呼吁将后增长情景纳入高级评估，但缺乏对后增长进行充分建模所需的连贯框架。

Method: 通过对建模者的调查和相关后增长文献，开发包含生物物理、经济和社会三个领域的后增长建模最低要求框架。

Result: 建立了整合三个领域并与后增长目标相联系的框架，不同领域有各自的建模要求，特定政策和转型情景需额外特征。

Conclusion: 该框架是为政策制定者和利益相关者提供追求可持续性、公平和福祉全面选项的迫切需要的工具。

Abstract: Post-growth has emerged as an umbrella term for various sustainability
visions that advocate the pursuit of environmental sustainability, social
equity, and human wellbeing, while questioning the continued pursuit of
economic growth. Although there are increasing calls to include post-growth
scenarios in high-level assessments, a coherent framework with what is required
to model post-growth adequately remains absent. This article addresses this gap
by: (1) identifying the minimum requirements for post-growth models, and (2)
establishing a set of model elements for representing specific policy themes.
Drawing on a survey of modellers and on relevant post-growth literature, we
develop a framework of minimum requirements for post-growth modelling that
integrates three spheres: biophysical, economic, and social, and links them to
post-growth goals. Within the biophysical sphere, we argue that embeddedness
requires the inclusion of resource use and pollution, environmental limits, and
feedback mechanisms from the environment onto society. Within the economic
sphere, models should disaggregate households, incorporate limits to
technological change and decoupling, include different types of government
interventions, and calculate GDP or output endogenously. Within the social
sphere, models should represent time use, material and non-material need
satisfiers, and the affordability of essential goods and services. Specific
policies and transformation scenarios require additional features, such as
sectoral disaggregation or representation of the financial system. Our
framework guides the development of models that can simulate both post-growth
and pro-growth policies and scenarios, an urgently needed tool for informing
policymakers and stakeholders about the full range of options for pursuing
sustainability, equity, and wellbeing.

</details>


### [376] [Relative Income and Gender Norms: Evidence from Latin America](https://arxiv.org/abs/2508.08166)
*Ercio Muñoz,Dario Sansone,João Tampellini*

Main category: econ.GN

TL;DR: 利用墨西哥超50万双收入家庭数据，发现拉美家庭相对收入分布在妻子收入超丈夫处有不连续性，该现象在巴西、巴拿马及部分同性伴侣中也存在，且主挣钱女性仍承担更多非市场劳动。


<details>
  <summary>Details</summary>
Motivation: 研究拉美家庭内部相对收入分布的特征及规律。

Method: 使用墨西哥超50万双收入家庭的数据进行分析，排除部分群体以验证结果稳健性，并对比不同子群体情况，还研究了巴西、巴拿马及同性伴侣情况。

Result: 在拉美家庭相对收入分布中，妻子收入超丈夫的50%阈值处有明显不连续性且比高收入国家大且随时间增加，该现象在多个子群体和其他国家及部分同性伴侣中持续存在，主挣钱女性仍承担更多非市场劳动但差距缩小。

Conclusion: 拉美家庭相对收入分布存在特定不连续性，且女性即使成为主要收入者仍承担较多非市场劳动。

Abstract: Using data from over 500,000 dual-earner households in Mexico, we provide
evidence of discontinuities in the distribution of relative income within
households in Latin America. Similar to high-income countries, we observe a
sharp drop at the 50% threshold, where the wife earns more than the husband,
but the discontinuity is up to five times larger and has increased over time.
These patterns are robust to excluding equal earners, self-employed
individuals, or couples in the same occupation/industry. Discontinuities
persist across subgroups, including couples with or without children, married
or unmarried partners, and those with older wives or female household heads. We
also find comparable discontinuities in Brazil and Panama, as well as among
some same-sex couples. Moreover, women who are primary earners continue to
supply more non-market labor than their male partners, although the gap is
narrower than in households where the woman is the secondary earner.

</details>


### [377] [Remote Work and Women's Labor Supply: The New Gender Division at Home](https://arxiv.org/abs/2508.08184)
*Isabella Di Filippo,Bruno Escobar,Juan Facal*

Main category: econ.GN

TL;DR: 研究男性远程工作机会增加对配偶就业的影响，发现有积极作用且源于家庭内育儿时间重新分配。


<details>
  <summary>Details</summary>
Motivation: 探究男性远程工作机会增加对其配偶劳动力供给的影响。

Method: 利用新冠疫情前后不同职业在家工作暴露变化的差异进行研究。

Result: 丈夫在家工作机会增加较多的女性就业可能性提高超2个百分点，相当于疫情前水平的4%，原因是家庭内育儿时间重新分配。

Conclusion: 强调家庭内部溢出效应和议价在塑造远程工作劳动力市场后果中的作用。

Abstract: We study how increases in remote work opportunities for men affect their
spouses' labor supply. Exploiting variation in the change in work-from-home
(WFH) exposure across occupations before and after the COVID-19 pandemic, we
find that women whose husbands experienced larger WFH increases are over 2
percentage points more likely to be employed, equivalent to a 4% rise relative
to pre-pandemic levels. Evidence from time-use diaries and childcare
questionnaires suggests these effects are driven by intra-household
reallocation of child-caring time: women are less likely to engage in primary
childcare activities, while men working at home partially compensate by
covering more for their spouse. These results highlight the role of
intra-household spillovers and bargaining in shaping the labor market
consequences of remote work.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [378] [BIGBOY1.2: Generating Realistic Synthetic Data for Disease Outbreak Modelling and Analytics](https://arxiv.org/abs/2508.07239)
*Raunak Narwal,Syed Abbas*

Main category: q-bio.PE

TL;DR: 创建了开放合成数据集生成器BIGBOY1.2，可用于基准测试疾病模型。


<details>
  <summary>Details</summary>
Motivation: 因监测数据不完整、有噪声和缺乏标准化数据集，疾病爆发模型建模具有挑战性。

Method: 创建BIGBOY1.2，支持SEIR和SIR类逻辑、自定义季节性和噪声注入。

Result: BIGBOY1.2能生成具有不同特征的数据集。

Conclusion: BIGBOY1.2适合比较传统流行病学模型和现代机器学习方法。

Abstract: Modelling disease outbreak models remains challenging due to incomplete
surveillance data, noise, and limited access to standardized datasets. We have
created BIGBOY1.2, an open synthetic dataset generator that creates
configurable epidemic time series and population-level trajectories suitable
for benchmarking modelling, forecasting, and visualisation. The framework
supports SEIR and SIR-like compartmental logic, custom seasonality, and noise
injection to mimic real reporting artifacts. BIGBOY1.2 can produce datasets
with diverse characteristics, making it suitable for comparing traditional
epidemiological models (e.g., SIR, SEIR) with modern machine learning
approaches (e.g., SVM, neural networks).

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [379] [Can Smaller Large Language Models Evaluate Research Quality?](https://arxiv.org/abs/2508.07196)
*Mike Thelwall*

Main category: cs.DL

TL;DR: 文章评估了Gemma - 3 - 27b - it对研究质量评分的能力，发现它与专家评分正相关，虽能力不及大模型，但小模型也可用于此任务。


<details>
  <summary>Details</summary>
Motivation: 探究较小大语言模型是否能像Google Gemini和ChatGPT一样给出与专家评分正相关的研究质量评估分数。

Method: 对104,187篇文章进行评估，用Gemma - 3 - 27b - it打分并与英国研究卓越框架2021的34个评估单元的专家研究质量分数代理对比。

Result: Gemma - 3 - 27b - it分数与专家分数正相关，相关性强度为ChatGPT 4o的83.8%和ChatGPT 4o - mini的94.7%；重复打分时相关性无显著提升，分数较低且报告风格相对统一。

Conclusion: 虽大模型研究评估分数估计能力最强，但小模型也可用于此任务，有助于节省成本或进行安全离线处理。

Abstract: Although both Google Gemini (1.5 Flash) and ChatGPT (4o and 4o-mini) give
research quality evaluation scores that correlate positively with expert scores
in nearly all fields, and more strongly that citations in most, it is not known
whether this is true for smaller Large Language Models (LLMs). In response,
this article assesses Google's Gemma-3-27b-it, a downloadable LLM (60Gb). The
results for 104,187 articles show that Gemma-3-27b-it scores correlate
positively with an expert research quality score proxy for all 34 Units of
Assessment (broad fields) from the UK Research Excellence Framework 2021. The
Gemma-3-27b-it correlations have 83.8% of the strength of ChatGPT 4o and 94.7%
of the strength of ChatGPT 4o-mini correlations. Differently from the two
larger LLMs, the Gemma-3-27b-it correlations do not increase substantially when
the scores are averaged across five repetitions, its scores tend to be lower,
and its reports are relatively uniform in style. Overall, the results show that
research quality score estimation can be conducted by offline LLMs, so this
capability is not an emergent property of the largest LLMs. Moreover, score
improvement through repetition is not a universal feature of LLMs. In
conclusion, although the largest LLMs still have the highest research
evaluation score estimation capability, smaller ones can also be used for this
task, and this can be helpful for cost saving or when secure offline processing
is needed.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [380] [Machines Learn Number Fields, But How? The Case of Galois Groups](https://arxiv.org/abs/2508.06670)
*Kyu-Hwan Lee,Seewoo Lee*

Main category: math.NT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: By applying interpretable machine learning methods such as decision trees, we
study how simple models can classify the Galois groups of Galois extensions
over $\mathbb{Q}$ of degrees 4, 6, 8, 9, and 10, using Dedekind zeta
coefficients. Our interpretation of the machine learning results allows us to
understand how the distribution of zeta coefficients depends on the Galois
group, and to prove new criteria for classifying the Galois groups of these
extensions. Combined with previous results, this work provides another example
of a new paradigm in mathematical research driven by machine learning.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [381] [TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree](https://arxiv.org/abs/2508.07014)
*Andrei Andrusenko,Vladimir Bataev,Lilit Grigoryan,Vitaly Lavrukhin,Boris Ginsburg*

Main category: eess.AS

TL;DR: 本文提出通用ASR上下文偏置框架，基于GPU加速词提升树，高效且开源。


<details>
  <summary>Details</summary>
Motivation: 现有上下文偏置方法存在需额外模型训练、解码慢、限制ASR系统类型等局限。

Method: 提出基于GPU加速词提升树的通用ASR上下文偏置框架，支持主要模型类型，可用于浅融合模式。

Result: 所提方法效率高，在准确性和解码速度上超越开源上下文偏置方法。

Conclusion: 所提上下文偏置框架具有优势，已作为NeMo工具包一部分开源。

Abstract: Recognizing specific key phrases is an essential task for contextualized
Automatic Speech Recognition (ASR). However, most existing context-biasing
approaches have limitations associated with the necessity of additional model
training, significantly slow down the decoding process, or constrain the choice
of the ASR system type. This paper proposes a universal ASR context-biasing
framework that supports all major types: CTC, Transducers, and Attention
Encoder-Decoder models. The framework is based on a GPU-accelerated word
boosting tree, which enables it to be used in shallow fusion mode for greedy
and beam search decoding without noticeable speed degradation, even with a vast
number of key phrases (up to 20K items). The obtained results showed high
efficiency of the proposed method, surpassing the considered open-source
context-biasing approaches in accuracy and decoding speed. Our context-biasing
framework is open-sourced as a part of the NeMo toolkit.

</details>


### [382] [FlexCTC: GPU-powered CTC Beam Decoding with advanced Contextual Abilities](https://arxiv.org/abs/2508.07315)
*Lilit Grigoryan,Vladimir Bataev,Nikolay Karpov,Andrei Andrusenko,Vitaly Lavrukhin,Boris Ginsburg*

Main category: eess.AS

TL;DR: 提出用于基于CTC模型的全GPU束解码的开源FlexCTC工具包，具有快速、易用和可扩展的特点，适用于研究和生产。


<details>
  <summary>Details</summary>
Motivation: 标准束搜索实现速度慢、常为顺序执行且受CPU限制，为充分利用现代硬件能力。

Method: 用Python和PyTorch开发FlexCTC工具包，采用全批处理GPU实现，通过CUDA Graphs消除CPU - GPU同步并最小化内核启动开销，支持高级上下文技术。

Result: 实现了准确高效的解码。

Conclusion: 该工具包适合研究和生产使用。

Abstract: While beam search improves speech recognition quality over greedy decoding,
standard implementations are slow, often sequential, and CPU-bound. To fully
leverage modern hardware capabilities, we present a novel open-source FlexCTC
toolkit for fully GPU-based beam decoding, designed for Connectionist Temporal
Classification (CTC) models. Developed entirely in Python and PyTorch, it
offers a fast, user-friendly, and extensible alternative to traditional C++,
CUDA, or WFST-based decoders. The toolkit features a high-performance, fully
batched GPU implementation with eliminated CPU-GPU synchronization and
minimized kernel launch overhead via CUDA Graphs. It also supports advanced
contextualization techniques, including GPU-powered N-gram language model
fusion and phrase-level boosting. These features enable accurate and efficient
decoding, making them suitable for both research and production use.

</details>


### [383] [G-IFT: A Gated Linear Unit adapter with Iterative Fine-Tuning for Low-Resource Children's Speaker Verification](https://arxiv.org/abs/2508.07836)
*Vishwas M. Shetty,Jiusi Zheng,Abeer Alwan*

Main category: eess.AS

TL;DR: 提出G - IFT框架提升成人与儿童语音领域知识转移效率，实验表明该框架比基线方法降低等错误率。


<details>
  <summary>Details</summary>
Motivation: 成人语音训练的说话人验证系统在儿童语音验证上表现不佳，且儿童语音数据有限使微调效果不好。

Method: 在预训练说话人嵌入模型和分类器之间插入门控线性单元适配器，然后迭代依次优化分类器、适配器和预训练说话人嵌入模型。

Result: 在ECAPA - TDNN、ResNet和X - vector架构上使用OGI和MyST数据集实验，G - IFT框架比基线方法持续降低等错误率。

Conclusion: G - IFT框架能有效提升高低资源语音领域间的知识转移效率，且对说话人验证系统底层架构类型不敏感。

Abstract: Speaker Verification (SV) systems trained on adults speech often underperform
on children's SV due to the acoustic mismatch, and limited children speech data
makes fine-tuning not very effective. In this paper, we propose an innovative
framework, a Gated Linear Unit adapter with Iterative Fine-Tuning (G-IFT), to
enhance knowledge transfer efficiency between the high-resource adults speech
domain and the low-resource children's speech domain. In this framework, a
Gated Linear Unit adapter is first inserted between the pre-trained speaker
embedding model and the classifier. Then the classifier, adapter, and
pre-trained speaker embedding model are optimized sequentially in an iterative
way. This framework is agnostic to the type of the underlying architecture of
the SV system. Our experiments on ECAPA-TDNN, ResNet, and X-vector
architectures using the OGI and MyST datasets demonstrate that the G-IFT
framework yields consistent reductions in Equal Error Rates compared to
baseline methods.

</details>


### [384] [Auditory Intelligence: Understanding the World Through Sound](https://arxiv.org/abs/2508.07829)
*Hyeonuk Nam*

Main category: eess.AS

TL;DR: 提出将听觉智能重新概念化为分层、情境化过程，引入四个认知启发任务范式，为更通用、可解释和与人一致的听觉智能提供路线图。


<details>
  <summary>Details</summary>
Motivation: 当前听觉智能任务局限于表面识别，无法解释事件原因、含义及情境发展，需重新构建听觉智能概念。

Method: 将听觉智能重新概念化为包含感知、推理和交互的分层、情境化过程，并引入四个认知启发任务范式。

Result: 四个任务范式可从不同方面构建听觉理解。

Conclusion: 这些范式为更通用、可解释和与人一致的听觉智能提供路线图，能引发对机器理解声音含义的更广泛讨论。

Abstract: Recent progress in auditory intelligence has yielded high-performing systems
for sound event detection (SED), acoustic scene classification (ASC), automated
audio captioning (AAC), and audio question answering (AQA). Yet these tasks
remain largely constrained to surface-level recognition-capturing what happened
but not why, what it implies, or how it unfolds in context. I propose a
conceptual reframing of auditory intelligence as a layered, situated process
that encompasses perception, reasoning, and interaction. To instantiate this
view, I introduce four cognitively inspired task paradigms-ASPIRE, SODA, AUX,
and AUGMENT-those structure auditory understanding across time-frequency
pattern captioning, hierarchical event/scene description, causal explanation,
and goal-driven interpretation, respectively. Together, these paradigms provide
a roadmap toward more generalizable, explainable, and human-aligned auditory
intelligence, and are intended to catalyze a broader discussion of what it
means for machines to understand sound.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [385] [Energy Efficient Task Offloading in UAV-Enabled MEC Using a Fully Decentralized Deep Reinforcement Learning Approach](https://arxiv.org/abs/2508.06863)
*Hamidreza Asadian-Rad,Hossein Soleimani,Shahrokh Farahmand*

Main category: cs.MA

TL;DR: 本文提出用全分散式设置解决无人机轨迹设计和用户分配优化问题，利用图注意力层和经验参数共享近端策略优化，相比现有算法有性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有半集中式处理在解决无人机轨迹设计和用户分配优化问题时存在通信开销、瓶颈、缺乏灵活性等局限。

Method: 采用全分散式设置，每架无人机获取本地观测并与邻居通信，通过本地运行的深度强化学习算法确定下一位置，使用图注意力层和经验参数共享近端策略优化。

Result: 数值结果显示在多个标准上比现有MADDPG算法有显著性能提升。

Conclusion: 该方法消除了半集中式MADRL方法的局限，仅利用本地通信就能提供更好性能。

Abstract: Unmanned aerial vehicles (UAVs) have been recently utilized in multi-access
edge computing (MEC) as edge servers. It is desirable to design UAVs'
trajectories and user to UAV assignments to ensure satisfactory service to the
users and energy efficient operation simultaneously. The posed optimization
problem is challenging to solve because: (i) The formulated problem is
non-convex, (ii) Due to the mobility of ground users, their future positions
and channel gains are not known in advance, (iii) Local UAVs' observations
should be communicated to a central entity that solves the optimization
problem. The (semi-) centralized processing leads to communication overhead,
communication/processing bottlenecks, lack of flexibility and scalability, and
loss of robustness to system failures. To simultaneously address all these
limitations, we advocate a fully decentralized setup with no centralized
entity. Each UAV obtains its local observation and then communicates with its
immediate neighbors only. After sharing information with neighbors, each UAV
determines its next position via a locally run deep reinforcement learning
(DRL) algorithm. None of the UAVs need to know the global communication graph.
Two main components of our proposed solution are (i) Graph attention layers
(GAT), and (ii) Experience and parameter sharing proximal policy optimization
(EPS-PPO). Our proposed approach eliminates all the limitations of
semi-centralized MADRL methods such as MAPPO and MA deep deterministic policy
gradient (MADDPG), while guaranteeing a better performance than independent
local DRLs such as in IPPO. Numerical results reveal notable performance gains
in several different criteria compared to the existing MADDPG algorithm,
demonstrating the potential for offering a better performance, while utilizing
local communications only.

</details>


### [386] [Retrieval-Augmented Multi-Agent System for Rapid Statement of Work Generation](https://arxiv.org/abs/2508.07569)
*Amulya Suravarjhula,Rashi Chandrashekhar Agrawal,Sakshi Jayesh Patel,Rahul Gupta*

Main category: cs.MA

TL;DR: 本文介绍了一种新的AI驱动自动化系统，可使工作说明书（SOW）起草过程更快、更简单、更准确，经测试表现良好，有助于法律和商业专业人士。


<details>
  <summary>Details</summary>
Motivation: 传统SOW起草过程缓慢、复杂，易出错，需要改进。

Method: 引入由三个智能组件（代理）构成的AI驱动自动化系统，分别负责初稿撰写、法律合规检查和文档格式处理。

Result: 系统通过实际商业案例测试，能在三分钟内完成完整SOW，准确性和质量表现良好，可降低法律风险并节省大量时间。

Conclusion: 该解决方案展示了人工智能可用于支持法律和商业专业人士，推动法律流程更智能、快速和可靠。

Abstract: Drafting a Statement of Work (SOW) is a vital part of business and legal
projects. It outlines key details like deliverables, timelines,
responsibilities, and legal terms. However, creating these documents is often a
slow and complex process. It usually involves multiple people, takes several
days, and leaves room for errors or outdated content. This paper introduces a
new AI-driven automation system that makes the entire SOW drafting process
faster, easier, and more accurate. Instead of relying completely on humans, the
system uses three intelligent components or 'agents' that each handle a part of
the job. One agent writes the first draft, another checks if everything is
legally correct, and the third agent formats the document and ensures
everything is in order. Unlike basic online tools that just fill in templates,
this system understands the meaning behind the content and customizes the SOW
to match the needs of the project. It also checks legal compliance and
formatting so that users can trust the result. The system was tested using real
business examples. It was able to create a full SOW in under three minutes,
compared to several hours or days using manual methods. It also performed well
in accuracy and quality, showing that it can reduce legal risks and save a lot
of time. This solution shows how artificial intelligence can be used to support
legal and business professionals by taking care of routine work and helping
them focus on more important decisions. It's a step toward making legal
processes smarter, faster, and more reliable.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [387] [Nonparametric Reaction Coordinate Optimization with Histories: A Framework for Rare Event Dynamics](https://arxiv.org/abs/2508.07326)
*Polina V. Banushkina,Sergei V. Krivov*

Main category: physics.chem-ph

TL;DR: 本文提出非参数反应坐标优化框架，结合轨迹历史，可分析复杂系统罕见事件动态，通过多案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 复杂系统中罕见但关键事件由复杂随机动态支配，识别能准确捕捉动态进展的最优反应坐标对理解和模拟这些过程至关重要。

Method: 引入结合轨迹历史的非参数反应坐标优化框架。

Result: 在蛋白质折叠动态分析中提供准确的承诺者估计，通过严格验证测试并产生高分辨率自由能谱；还应用于相空间动态、概念海洋环流模型和纵向临床数据集。

Conclusion: 无需对配置空间进行详尽采样就能准确表征罕见事件动态，为分析复杂动力系统和纵向数据集建立了通用、灵活且稳健的框架。

Abstract: Rare but critical events in complex systems, such as protein folding,
chemical reactions, disease progression, and extreme weather or climate
phenomena, are governed by complex, high-dimensional, stochastic dynamics.
Identifying an optimal reaction coordinate (RC) that accurately captures the
progress of these dynamics is crucial for understanding and simulating such
processes. This work introduces a nonparametric RC optimization framework that
incorporates trajectory histories, enabling robust analysis even for irregular
or incomplete data. The power of the method is demonstrated through
increasingly challenging analyses of protein folding dynamics, where it
provides accurate committor estimates that pass a stringent validation test and
yield high-resolution free energy profiles. Its generality is further
illustrated through applications to dynamics in phase space, a conceptual ocean
circulation model, and a longitudinal clinical dataset. These results
demonstrate that rare event dynamics can be accurately characterized without
exhaustive sampling of the configuration space, establishing a general,
flexible, and robust framework for analyzing complex dynamical systems and
longitudinal datasets.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [388] [Statistical Theory of Multi-stage Newton Iteration Algorithm for Online Continual Learning](https://arxiv.org/abs/2508.07419)
*Xinjia Lu,Chuhan Wang,Qian Zhao,Lixing Zhu,Xuehu Zhu*

Main category: stat.ME

TL;DR: 针对在线持续学习中处理非平稳数据流及灾难性遗忘问题，提出新框架和算法，理论推导并实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决在线持续学习环境中处理非平稳数据流时因存储受限导致灾难性遗忘的问题。

Method: 从统计视角提出新持续学习框架，引入随机效应和允许参数维度趋于无穷；开发多步牛顿迭代算法降低计算成本。

Result: 推导出估计量的渐近正态性，通过合成数据实验和两个真实数据集分析验证方法有效性。

Conclusion: 所提方法能有效解决持续学习中的灾难性遗忘问题。

Abstract: We focus on the critical challenge of handling non-stationary data streams in
online continual learning environments, where constrained storage capacity
prevents complete retention of historical data, leading to catastrophic
forgetting during sequential task training. To more effectively analyze and
address the problem of catastrophic forgetting in continual learning, we
propose a novel continual learning framework from a statistical perspective.
Our approach incorporates random effects across all model parameters and allows
the dimension of parameters to diverge to infinity, offering a general
formulation for continual learning problems. To efficiently process streaming
data, we develop a Multi-step Newton Iteration algorithm that significantly
reduces computational costs in certain scenarios by alleviating the burden of
matrix inversion. Theoretically, we derive the asymptotic normality of the
estimator, enabling subsequent statistical inference. Comprehensive validation
through synthetic data experiments and two real datasets analyses demonstrates
the effectiveness of our proposed method.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [389] [Barron Space Representations for Elliptic PDEs with Homogeneous Boundary Conditions](https://arxiv.org/abs/2508.07559)
*Ziang Chen,Liqiang Huang*

Main category: math.NA

TL;DR: 研究高维二阶椭圆PDE在Barron空间框架下的近似复杂度，证明解可用两层神经网络有效逼近，规避维数灾难。


<details>
  <summary>Details</summary>
Motivation: 探究高维二阶椭圆PDE的近似复杂度，挖掘浅网络对高维PDE解的表达能力。

Method: 在Barron空间框架下，假设系数属于适当定义的Barron空间。

Result: 证明解可用两层神经网络有效逼近，规避维数灾难。

Conclusion: 在适当结构假设下，浅网络对高维PDE解有强大表达能力。

Abstract: We study the approximation complexity of high-dimensional second-order
elliptic PDEs with homogeneous boundary conditions on the unit hypercube,
within the framework of Barron spaces. Under the assumption that the
coefficients belong to suitably defined Barron spaces, we prove that the
solution can be efficiently approximated by two-layer neural networks,
circumventing the curse of dimensionality. Our results demonstrate the
expressive power of shallow networks in capturing high-dimensional PDE
solutions under appropriate structural assumptions.

</details>


### [390] [Prediction error certification for PINNs: Theory, computation, and application to Stokes flow](https://arxiv.org/abs/2508.07994)
*Birgit Hillebrecht,Benjamin Unger*

Main category: math.NA

TL;DR: 本文扩展基于半群的PINN误差估计框架适用性，通过修改误差界和提出数值策略，以实现更现实场景的预测验证。


<details>
  <summary>Details</summary>
Motivation: 现有PINN误差估计器受限于学术示例，需扩展其适用性到更广泛问题。

Method: 修改误差界并提出数值策略近似所需稳定性参数。

Result: 扩展框架能在更现实场景中对PINN预测进行验证，如圆柱绕流的斯托克斯流数值研究。

Conclusion: 所提方法有效扩展了PINN误差估计框架的应用范围。

Abstract: Rigorous error estimation is a fundamental topic in numerical analysis. With
the increasing use of physics-informed neural networks (PINNs) for solving
partial differential equations, several approaches have been developed to
quantify the associated prediction error. In this work, we build upon a
semigroup-based framework previously introduced by the authors for estimating
the PINN error. While this estimator has so far been limited to academic
examples - due to the need to compute quantities related to input-to-state
stability - we extend its applicability to a significantly broader class of
problems. This is accomplished by modifying the error bound and proposing
numerical strategies to approximate the required stability parameters. The
extended framework enables the certification of PINN predictions in more
realistic scenarios, as demonstrated by a numerical study of Stokes flow around
a cylinder.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [391] [A Lagrangian method for solving the spherical shallow water equations using power diagrams](https://arxiv.org/abs/2508.08129)
*Philip Caplan,Otis Milliken,Toby Pouler,Zeyi Tong,Col McDermott,Sam Millay*

Main category: physics.flu-dyn

TL;DR: 本文提出新拉格朗日方法模拟大气，用球形幂胞表示粒子，评估了计算性能和模拟方法效果。


<details>
  <summary>Details</summary>
Motivation: 不确定拉格朗日观点在全球大气模拟中是否能优于欧拉模拟，现有拉格朗日方法有前景但解较平滑。

Method: 开发新拉格朗日方法，用球形幂胞表示粒子，计算幂胞的高效算法，求解半离散最优传输问题保证质量守恒，用半隐式时间步推进求解。

Result: 10000 万个站点的球形 Voronoi 图可在单台机器 2 分钟内计算完成，新方法动量和能量守恒与最新拉格朗日方法相当。

Conclusion: 新拉格朗日方法在大气模拟中有一定性能，且无需人工粘性来稳定模拟。

Abstract: Numerical simulations of the air in the atmosphere and water in the oceans
are essential for numerical weather prediction. The state-of-the-art for
performing these fluid simulations relies on an Eulerian viewpoint, in which
the fluid domain is discretized into a mesh, and the governing equations
describe the fluid motion as it passes through each cell of the mesh. However,
it is unclear whether a Lagrangian viewpoint, in which the fluid is discretized
by a collection of particles, can outperform Eulerian simulations in global
atmospheric simulations. To date, Lagrangian approaches have shown promise, but
tend to produce smoother solutions. In this work, a new Lagrangian method is
developed to simulate the atmosphere in which particles are represented with
spherical power cells. We introduce an efficient algorithm for computing these
cells which are then used to discretize the spherical shallow water equations.
Mass conservation is enforced by solving a semi-discrete optimal transport
problem and a semi-implicit time stepping procedure is used to advance the
solution in time. We note that, in contrast to previous work, artificial
viscosity is not needed to stabilize the simulation. The performance of the
spherical Voronoi diagram calculation is first assessed, which shows that
spherical Voronoi diagrams of 100 million sites can be computed in under 2
minutes on a single machine. The new simulation method is then evaluated on
standard benchmark test cases, which shows that momentum and energy
conservation of this new method is comparable to the latest Lagrangian approach
for simulating the spherical shallow water equations.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [392] [The Vertex-Attribute-Constrained Densest $k$-Subgraph Problem](https://arxiv.org/abs/2508.06655)
*Qiheng Lu,Nicholas D. Sidiropoulos,Aritra Konar*

Main category: cs.SI

TL;DR: 提出顶点属性约束的最密k子图（VAC - DkS）问题，证明其连续松弛问题可通过无投影Frank - Wolfe算法解决，实验证明方法有效且可处理大图，在政治网络挖掘中表现优于经典DkS。


<details>
  <summary>Details</summary>
Motivation: 在图挖掘应用中，需识别反映不同特征的社区、推荐或摘要，因此引入结合顶点属性值的新DkS问题变种。

Method: 证明VAC - DkS合适的连续松弛问题是紧的，使用无投影Frank - Wolfe算法解决该松弛问题，并分析松弛问题的优化格局。

Result: 实验结果表明提出的公式和算法有效，能处理大图；在政治网络挖掘应用中，VAC - DkS能识别出更平衡、更有意义的政治家集合。

Conclusion: 提出的VAC - DkS问题及解决方法有效，在处理大图和特定应用中有优势。

Abstract: Dense subgraph mining is a fundamental technique in graph mining, commonly
applied in fraud detection, community detection, product recommendation, and
document summarization. In such applications, we are often interested in
identifying communities, recommendations, or summaries that reflect different
constituencies, styles or genres, and points of view. For this task, we
introduce a new variant of the Densest $k$-Subgraph (D$k$S) problem that
incorporates the attribute values of vertices. The proposed
Vertex-Attribute-Constrained Densest $k$-Subgraph (VAC-D$k$S) problem retains
the NP-hardness and inapproximability properties of the classical D$k$S.
Nevertheless, we prove that a suitable continuous relaxation of VAC-D$k$S is
tight and can be efficiently tackled using a projection-free Frank--Wolfe
algorithm. We also present an insightful analysis of the optimization landscape
of the relaxed problem. Extensive experimental results demonstrate the
effectiveness of our proposed formulation and algorithm, and its ability to
scale up to large graphs. We further elucidate the properties of VAC-D$k$S
versus classical D$k$S in a political network mining application, where
VAC-D$k$S identifies a balanced and more meaningful set of politicians
representing different ideological camps, in contrast to the classical D$k$S
solution which is unbalanced and rather mundane.

</details>


### [393] [Anatomy of a Machine Learning Ecosystem: 2 Million Models on Hugging Face](https://arxiv.org/abs/2508.06811)
*Benjamin Laufer,Hamidah Oderinwale,Jon Kleinberg*

Main category: cs.SI

TL;DR: 本文分析Hugging Face上186万个模型，用进化生物学视角研究模型微调，发现模型有家族相似性，突变有特点，还揭示机器学习生态系统的一些变化，表明生态模型和方法能带来新见解。


<details>
  <summary>Details</summary>
Motivation: 现有研究对生成式机器学习和人工智能模型交互结构的实证研究有限，本文旨在进行相关分析。

Method: 分析Hugging Face上的模型，构建模型家族树，利用模型元数据和模型卡片测量模型家族的遗传相似性和特征突变。

Result: 模型有家族相似性，突变快且有方向，‘兄弟’模型比亲子模型更相似；许可证从限制性向宽松或共享许可漂移，模型从多语言兼容向英语兼容转变，模型卡片长度减少且标准化。

Conclusion: 朝着基于实证理解模型微调迈出一步，生态模型和方法可产生新的科学见解。

Abstract: Many have observed that the development and deployment of generative machine
learning (ML) and artificial intelligence (AI) models follow a distinctive
pattern in which pre-trained models are adapted and fine-tuned for specific
downstream tasks. However, there is limited empirical work that examines the
structure of these interactions. This paper analyzes 1.86 million models on
Hugging Face, a leading peer production platform for model development. Our
study of model family trees -- networks that connect fine-tuned models to their
base or parent -- reveals sprawling fine-tuning lineages that vary widely in
size and structure. Using an evolutionary biology lens to study ML models, we
use model metadata and model cards to measure the genetic similarity and
mutation of traits over model families. We find that models tend to exhibit a
family resemblance, meaning their genetic markers and traits exhibit more
overlap when they belong to the same model family. However, these similarities
depart in certain ways from standard models of asexual reproduction, because
mutations are fast and directed, such that two `sibling' models tend to exhibit
more similarity than parent/child pairs. Further analysis of the directional
drifts of these mutations reveals qualitative insights about the open machine
learning ecosystem: Licenses counter-intuitively drift from restrictive,
commercial licenses towards permissive or copyleft licenses, often in violation
of upstream license's terms; models evolve from multi-lingual compatibility
towards english-only compatibility; and model cards reduce in length and
standardize by turning, more often, to templates and automatically generated
text. Overall, this work takes a step toward an empirically grounded
understanding of model fine-tuning and suggests that ecological models and
methods can yield novel scientific insights.

</details>


### [394] [Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection](https://arxiv.org/abs/2508.07201)
*Chaoqun Cui,Caiyan Jia*

Main category: cs.SI

TL;DR: 现有图模型假定谣言传播树为深度结构，本文发现其为宽结构，提出RAGCL方法，实验表明该方法优于现有技术，其原理和技术或有益于其他树结构应用。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的谣言检测模型假定谣言传播树为深度结构，与实际情况不符，需要更有效的方法。

Method: 提出Rumor Adaptive Graph Contrastive Learning (RAGCL) 方法，根据节点中心性进行自适应视图增强，采用节点丢弃、属性掩码和边丢弃生成视图，通过图对比目标学习谣言表示。

Result: 在四个基准数据集上的实验表明，RAGCL方法优于现有技术。

Conclusion: 揭示了谣言传播树的宽结构特性，提出了适用于谣言检测的有效图对比学习方法，其原理和增强技术可能有益于其他涉及树结构的应用。

Abstract: Rumor detection on social media has become increasingly important. Most
existing graph-based models presume rumor propagation trees (RPTs) have deep
structures and learn sequential stance features along branches. However,
through statistical analysis on real-world datasets, we find RPTs exhibit wide
structures, with most nodes being shallow 1-level replies. To focus learning on
intensive substructures, we propose Rumor Adaptive Graph Contrastive Learning
(RAGCL) method with adaptive view augmentation guided by node centralities. We
summarize three principles for RPT augmentation: 1) exempt root nodes, 2)
retain deep reply nodes, 3) preserve lower-level nodes in deep sections. We
employ node dropping, attribute masking and edge dropping with probabilities
from centrality-based importance scores to generate views. A graph contrastive
objective then learns robust rumor representations. Extensive experiments on
four benchmark datasets demonstrate RAGCL outperforms state-of-the-art methods.
Our work reveals the wide-structure nature of RPTs and contributes an effective
graph contrastive learning approach tailored for rumor detection through
principled adaptive augmentation. The proposed principles and augmentation
techniques can potentially benefit other applications involving tree-structured
graphs.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [395] [CROP: Integrating Topological and Spatial Structures via Cross-View Prefixes for Molecular LLMs](https://arxiv.org/abs/2508.06917)
*Jianting Tang,Yubo Wang,Haoyu Cao,Linli Xu*

Main category: q-bio.QM

TL;DR: 本文提出CROP方法通过多视图集成增强大语言模型对分子的理解，实验证明其在多项分子任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有依赖分子序列的大语言模型在理解分子复杂结构上效果有限，而分子的图和图像两种结构视图能提供独特见解，需利用多视图协作提升模型能力。

Method: 提出CROss - view Prefixes (CROP)方法，包含SMILES Guided Resampler进行视图重采样和Structural Embedding Gate将嵌入转换为大语言模型前缀。

Result: 在分子描述、IUPAC名称预测和分子属性预测等任务中，CROP表现出优越性。

Conclusion: CROP方法在多视图集成方面有效，能提升大语言模型对分子的理解，可用于多种分子相关任务。

Abstract: Recent advances in molecular science have been propelled significantly by
large language models (LLMs). However, their effectiveness is limited when
relying solely on molecular sequences, which fail to capture the complex
structures of molecules. Beyond sequence representation, molecules exhibit two
complementary structural views: the first focuses on the topological
relationships between atoms, as exemplified by the graph view; and the second
emphasizes the spatial configuration of molecules, as represented by the image
view. The two types of views provide unique insights into molecular structures.
To leverage these views collaboratively, we propose the CROss-view Prefixes
(CROP) to enhance LLMs' molecular understanding through efficient multi-view
integration. CROP possesses two advantages: (i) efficiency: by jointly
resampling multiple structural views into fixed-length prefixes, it avoids
excessive consumption of the LLM's limited context length and allows easy
expansion to more views; (ii) effectiveness: by utilizing the LLM's
self-encoded molecular sequences to guide the resampling process, it boosts the
quality of the generated prefixes. Specifically, our framework features a
carefully designed SMILES Guided Resampler for view resampling, and a
Structural Embedding Gate for converting the resulting embeddings into LLM's
prefixes. Extensive experiments demonstrate the superiority of CROP in tasks
including molecule captioning, IUPAC name prediction and molecule property
prediction.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [396] [Channel Charting in Smart Radio Environments](https://arxiv.org/abs/2508.07305)
*Mahdi Maleki,Reza Agahzadeh Ayoubi,Marouan Mizmizi,Umberto Spagnolini*

Main category: eess.SP

TL;DR: 本文介绍用静态电磁皮（EMS）在现实城市环境中通过信道绘图（CC）实现稳健设备定位，经模拟验证其可提升定位性能。


<details>
  <summary>Details</summary>
Motivation: 在现实城市环境中实现稳健的设备定位，解决非视距（NLoS）条件下定位难题。

Method: 开发严格的优化框架，利用EMS增强信道差异和空间指纹识别，将EMS相位轮廓设计为基于码本的问题。

Result: 优化后的EMS配置除显著改善平均定位误差外，将90%分位的定位误差从超60米降至不足25米，还大幅提升可信度和连续性。

Conclusion: 这是首次利用静态EMS的智能无线电环境（SRE）增强CC，在具有挑战性的NLoS条件下显著提升定位性能。

Abstract: This paper introduces the use of static electromagnetic skins (EMSs) to
enable robust device localization via channel charting (CC) in realistic urban
environments. We develop a rigorous optimization framework that leverages EMS
to enhance channel dissimilarity and spatial fingerprinting, formulating EMS
phase profile design as a codebook-based problem targeting the upper quantiles
of key embedding metric, localization error, trustworthiness, and continuity.
Through 3D ray-traced simulations of a representative city scenario, we
demonstrate that optimized EMS configurations, in addition to significant
improvement of the average positioning error, reduce the 90th-percentile
localization error from over 60 m (no EMS) to less than 25 m, while drastically
improving trustworthiness and continuity. To the best of our knowledge, this is
the first work to exploit Smart Radio Environment (SRE) with static EMS for
enhancing CC, achieving substantial gains in localization performance under
challenging None-Line-of-Sight (NLoS) conditions.

</details>


### [397] [Adaptive Learning for IRS-Assisted Wireless Networks: Securing Opportunistic Communications Against Byzantine Eavesdroppers](https://arxiv.org/abs/2508.08206)
*Amirhossein Taherpour,Abbas Taherpour,Tamer Khattab*

Main category: eess.SP

TL;DR: 提出用于拜占庭弹性频谱感知和安全智能反射面辅助机会接入的联合学习框架，在不同信道状态信息条件下采用不同方法优化，模拟显示多种优势。


<details>
  <summary>Details</summary>
Motivation: 解决信道状态信息不确定下拜占庭弹性频谱感知和安全智能反射面辅助机会接入问题。

Method: 感知阶段用logit域贝叶斯更新、修剪聚合和注意力加权共识；有部分或已知信道状态信息时用增广拉格朗日交替算法；未知信道状态信息时在低维潜在空间用约束贝叶斯优化。

Result: 在对抗攻击下固定虚警率时检测概率更高，诚实用户的总均方误差大幅降低，有效抑制窃听者信号功率，收敛快。

Conclusion: 该框架为安全机会通信提供实用途径，能适应信道状态信息可用性，通过联合学习协调感知和传输。

Abstract: We propose a joint learning framework for Byzantine-resilient spectrum
sensing and secure intelligent reflecting surface (IRS)--assisted opportunistic
access under channel state information (CSI) uncertainty. The sensing stage
performs logit-domain Bayesian updates with trimmed aggregation and
attention-weighted consensus, and the base station (BS) fuses network beliefs
with a conservative minimum rule, preserving detection accuracy under a bounded
number of Byzantine users. Conditioned on the sensing outcome, we pose downlink
design as sum mean-squared error (MSE) minimization under transmit-power and
signal-leakage constraints and jointly optimize the BS precoder, IRS phase
shifts, and user equalizers. With partial (or known) CSI, we develop an
augmented-Lagrangian alternating algorithm with projected updates and provide
provable sublinear convergence, with accelerated rates under mild local
curvature. With unknown CSI, we perform constrained Bayesian optimization (BO)
in a geometry-aware low-dimensional latent space using Gaussian process (GP)
surrogates; we prove regret bounds for a constrained upper confidence bound
(UCB) variant of the BO module, and demonstrate strong empirical performance of
the implemented procedure. Simulations across diverse network conditions show
higher detection probability at fixed false-alarm rate under adversarial
attacks, large reductions in sum MSE for honest users, strong suppression of
eavesdropper signal power, and fast convergence. The framework offers a
practical path to secure opportunistic communication that adapts to CSI
availability while coherently coordinating sensing and transmission through
joint learning.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [398] [Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation](https://arxiv.org/abs/2508.07745)
*Jiongchi Yu,Xiaofei Xie,Qiang Hu,Yuhan Ma,Ziming Zhao*

Main category: cs.CR

TL;DR: 现有机器学习内部威胁检测方法受高质量数据稀缺阻碍，本文提出基于大语言模型的多智能体框架Chimera模拟活动并生成数据集ChimeraLog，评估显示其有更高难度和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习内部威胁检测方法因高质量数据稀缺而进展受阻的问题。

Method: 提出Chimera框架，用有特定角色行为的智能体模拟员工，集成多种模块，包含15种内部攻击类型，在三个敏感领域模拟活动生成ChimeraLog。

Result: 通过人类研究和定量分析确认ChimeraLog的多样性、真实性和可解释威胁模式，现有ITD方法在ChimeraLog上F1分数平均为0.83，低于在CERT数据集上的0.99。

Conclusion: ChimeraLog有更高难度和实用性，有助于推进内部威胁检测研究。

Abstract: Insider threats, which can lead to severe losses, remain a major security
concern. While machine learning-based insider threat detection (ITD) methods
have shown promising results, their progress is hindered by the scarcity of
high-quality data. Enterprise data is sensitive and rarely accessible, while
publicly available datasets, when limited in scale due to cost, lack sufficient
real-world coverage; and when purely synthetic, they fail to capture rich
semantics and realistic user behavior. To address this, we propose Chimera, the
first large language model (LLM)-based multi-agent framework that automatically
simulates both benign and malicious insider activities and collects diverse
logs across diverse enterprise environments. Chimera models each employee with
agents that have role-specific behavior and integrates modules for group
meetings, pairwise interactions, and autonomous scheduling, capturing realistic
organizational dynamics. It incorporates 15 types of insider attacks (e.g., IP
theft, system sabotage) and has been deployed to simulate activities in three
sensitive domains: technology company, finance corporation, and medical
institution, producing a new dataset, ChimeraLog. We assess ChimeraLog via
human studies and quantitative analysis, confirming its diversity, realism, and
presence of explainable threat patterns. Evaluations of existing ITD methods
show an average F1-score of 0.83, which is significantly lower than 0.99 on the
CERT dataset, demonstrating ChimeraLog's higher difficulty and utility for
advancing ITD research.

</details>


### [399] [EFU: Enforcing Federated Unlearning via Functional Encryption](https://arxiv.org/abs/2508.07873)
*Samaneh Mohammadi,Vasileios Tsouvalas,Iraklis Symeonidis,Ali Balador,Tanir Ozcelebi,Francesco Flammini,Nirvana Meratnia*

Main category: cs.CR

TL;DR: 提出EFU框架实现客户端在联邦学习中隐藏遗忘数据意图的无学习，实验显示效果良好且对底层算法无关性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦无学习方法依赖服务器合作，会暴露客户端意图和身份，损害自主性和无学习隐私。

Method: 提出EFU框架，利用功能加密绑定加密更新到特定聚合函数，结合基于对抗样本和参数重要性正则化的辅助无学习损失。

Result: 在遗忘数据上达到接近随机的准确率，性能与全重训练相当，能隐藏无学习意图。

Conclusion: EFU对底层无学习算法具有无关性，可为任何发布定向更新的客户端联邦无学习机制实现安全、隐藏功能和可验证的无学习。

Abstract: Federated unlearning (FU) algorithms allow clients in federated settings to
exercise their ''right to be forgotten'' by removing the influence of their
data from a collaboratively trained model. Existing FU methods maintain data
privacy by performing unlearning locally on the client-side and sending
targeted updates to the server without exposing forgotten data; yet they often
rely on server-side cooperation, revealing the client's intent and identity
without enforcement guarantees - compromising autonomy and unlearning privacy.
In this work, we propose EFU (Enforced Federated Unlearning), a
cryptographically enforced FU framework that enables clients to initiate
unlearning while concealing its occurrence from the server. Specifically, EFU
leverages functional encryption to bind encrypted updates to specific
aggregation functions, ensuring the server can neither perform unauthorized
computations nor detect or skip unlearning requests. To further mask behavioral
and parameter shifts in the aggregated model, we incorporate auxiliary
unlearning losses based on adversarial examples and parameter importance
regularization. Extensive experiments show that EFU achieves near-random
accuracy on forgotten data while maintaining performance comparable to full
retraining across datasets and neural architectures - all while concealing
unlearning intent from the server. Furthermore, we demonstrate that EFU is
agnostic to the underlying unlearning algorithm, enabling secure,
function-hiding, and verifiable unlearning for any client-side FU mechanism
that issues targeted updates.

</details>


### [400] [Fully-Fluctuating Participation in Sleepy Consensus](https://arxiv.org/abs/2508.08068)
*Yuval Efron,Joachim Neu,Toniann Pitassi*

Main category: cs.CR

TL;DR: 提出新对手模型“外部对手”，表明睡眠模型协议在此模型下可应对节点参与度波动并保证安全。


<details>
  <summary>Details</summary>
Motivation: 现有睡眠模型协议在应对节点参与度大幅波动时不如比特币健壮，且有诸多限制假设，需改进。

Method: 提出新的对手模型“外部对手”，此模型中腐败节点不泄露密钥信息。

Result: 在新模型下，睡眠模型协议能在不牺牲效率和抗腐败能力的情况下，应对全波动参与情况。

Conclusion: 新对手模型自然且能捕获协议中恶意行为产生过程，还规避了前人研究的障碍，有理论吸引力。

Abstract: Proof-of-work allows Bitcoin to boast security amidst arbitrary fluctuations
in participation of miners throughout time, so long as, at any point in time, a
majority of hash power is honest. In recent years, however, the pendulum has
shifted in favor of proof-of-stake-based consensus protocols. There, the sleepy
model is the most prominent model for handling fluctuating participation of
nodes. However, to date, no protocol in the sleepy model rivals Bitcoin in its
robustness to drastic fluctuations in participation levels, with
state-of-the-art protocols making various restrictive assumptions. In this
work, we present a new adversary model, called external adversary. Intuitively,
in our model, corrupt nodes do not divulge information about their secret keys.
In this model, we show that protocols in the sleepy model can meaningfully
claim to remain secure against fully fluctuating participation, without
compromising efficiency or corruption resilience. Our adversary model is quite
natural, and arguably naturally captures the process via which malicious
behavior arises in protocols, as opposed to traditional worst-case modeling. On
top of which, the model is also theoretically appealing, circumventing a
barrier established in a recent work of Malkhi, Momose, and Ren.

</details>


### [401] [AuthPrint: Fingerprinting Generative Models Against Malicious Model Providers](https://arxiv.org/abs/2508.05691)
*Kai Yao,Marc Juarez*

Main category: cs.CR

TL;DR: 文章将模型指纹技术扩展到模型提供者可能有恶意行为的场景，用于验证生成模型输出的来源，方法效果好且抗攻击。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在高风险领域部署时缺乏验证模型输出来源的机制。

Method: 依靠可信验证者从模型输出空间提取提供者未知的秘密指纹，并训练模型进行预测和验证。

Result: 方法在GAN和扩散模型实例上达到近零FPR@95%TPR，对模型架构和训练数据小修改及对抗攻击都有鲁棒性。

Conclusion: 提出的方法可有效验证生成模型输出的来源，代码开源。

Abstract: Generative models are increasingly adopted in high-stakes domains, yet
current deployments offer no mechanisms to verify the origin of model outputs.
We address this gap by extending model fingerprinting techniques beyond the
traditional collaborative setting to one where the model provider may act
adversarially. To our knowledge, this is the first work to evaluate
fingerprinting for provenance attribution under such a threat model. The
methods rely on a trusted verifier that extracts secret fingerprints from the
model's output space, unknown to the provider, and trains a model to predict
and verify them. Our empirical evaluation shows that our methods achieve
near-zero FPR@95%TPR for instances of GAN and diffusion models, even when
tested on small modifications to the original architecture and training data.
Moreover, the methods remain robust against adversarial attacks that actively
modify the outputs to bypass detection. Source codes are available at
https://github.com/PSMLab/authprint.

</details>


### [402] [A Real-Time, Self-Tuning Moderator Framework for Adversarial Prompt Detection](https://arxiv.org/abs/2508.07139)
*Ivan Zhang*

Main category: cs.CR

TL;DR: 介绍实时自调谐（RTST）审核框架防御大语言模型对抗攻击，用Gemini模型评估其有效性，显示该框架优势。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型对抗攻击和越狱防御方法存在无法快速适应新攻击、降低模型对良性提示响应、难以扩展实施等问题，为解决这些挑战。

Method: 引入实时自调谐（RTST）审核框架，用Google的Gemini模型对现代有效越狱攻击进行实证评估。

Result: 结果证明自适应、低侵入性的越狱防御框架相比传统微调或分类器模型具有优势。

Conclusion: 实时自调谐（RTST）审核框架在防御大语言模型对抗攻击方面具有优势。

Abstract: Ensuring LLM alignment is critical to information security as AI models
become increasingly widespread and integrated in society. Unfortunately, many
defenses against adversarial attacks and jailbreaking on LLMs cannot adapt
quickly to new attacks, degrade model responses to benign prompts, or introduce
significant barriers to scalable implementation. To mitigate these challenges,
we introduce a real-time, self-tuning (RTST) moderator framework to defend
against adversarial attacks while maintaining a lightweight training footprint.
We empirically evaluate its effectiveness using Google's Gemini models against
modern, effective jailbreaks. Our results demonstrate the advantages of an
adaptive, minimally intrusive framework for jailbreak defense over traditional
fine-tuning or classifier models.

</details>


### [403] [Mitigating Distribution Shift in Graph-Based Android Malware Classification via Function Metadata and LLM Embeddings](https://arxiv.org/abs/2508.06734)
*Ngoc N. Tran,Anwar Said,Waseem Abbas,Tyler Derr,Xenofon D. Koutsoukos*

Main category: cs.CR

TL;DR: 现有图基恶意软件分类器泛化能力不足，本文提出语义增强框架，引入新基准评估，实验表明方法提升性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图基恶意软件分类器在面对未知变体时准确率大幅下降，模型架构和结构表示难以捕捉深层语义模式。

Method: 提出语义增强框架，用上下文特征丰富函数调用图，引入 MalNet - Tiny - Common 和 MalNet - Tiny - Distinct 两个新基准。

Result: 跨多个图神经网络骨干实验显示，方法在分布偏移下最多提升 8% 分类性能，与自适应方法结合时增强鲁棒性。

Conclusion: 研究为在不断变化的威胁环境中构建有弹性的恶意软件检测系统提供了实用途径。

Abstract: Graph-based malware classifiers can achieve over 94% accuracy on standard
Android datasets, yet we find they suffer accuracy drops of up to 45% when
evaluated on previously unseen malware variants from the same family - a
scenario where strong generalization would typically be expected. This
highlights a key limitation in existing approaches: both the model
architectures and their structure-only representations often fail to capture
deeper semantic patterns. In this work, we propose a robust semantic enrichment
framework that enhances function call graphs with contextual features,
including function-level metadata and, when available, code embeddings derived
from large language models. The framework is designed to operate under
real-world constraints where feature availability is inconsistent, and supports
flexible integration of semantic signals. To evaluate generalization under
realistic domain and temporal shifts, we introduce two new benchmarks:
MalNet-Tiny-Common and MalNet-Tiny-Distinct, constructed using malware family
partitioning to simulate cross-family generalization and evolving threat
behavior. Experiments across multiple graph neural network backbones show that
our method improves classification performance by up to 8% under distribution
shift and consistently enhances robustness when integrated with
adaptation-based methods. These results offer a practical path toward building
resilient malware detection systems in evolving threat environments.

</details>


### [404] [Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks](https://arxiv.org/abs/2508.08029)
*Thusitha Dayaratne,Ngoc Duy Pham,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Main category: cs.CR

TL;DR: 本文探讨用大语言模型（LLMs）解决O - RAN架构中SDL层数据操纵攻击，展示其鲁棒性和低检测延迟，但初始检测精度待提高。


<details>
  <summary>Details</summary>
Motivation: 5G和O - RAN架构增加了安全挑战，传统机器学习异常检测方法难以应对数据操纵攻击，需新的异常检测方案。

Method: 研究使用大语言模型（LLMs）进行O - RAN架构中的异常检测。

Result: 基于LLM的xApps运行性能稳健，能处理操纵消息不崩溃，检测延迟低（低于0.07秒），但初始检测精度有待提高。

Conclusion: LLMs对输入数据中的对抗攻击具有鲁棒性，有潜力通过提示工程提高精度，适合近实时RIC部署，但仍需进一步研究。

Abstract: The introduction of 5G and the Open Radio Access Network (O-RAN) architecture
has enabled more flexible and intelligent network deployments. However, the
increased complexity and openness of these architectures also introduce novel
security challenges, such as data manipulation attacks on the semi-standardised
Shared Data Layer (SDL) within the O-RAN platform through malicious xApps. In
particular, malicious xApps can exploit this vulnerability by introducing
subtle Unicode-wise alterations (hypoglyphs) into the data that are being used
by traditional machine learning (ML)-based anomaly detection methods. These
Unicode-wise manipulations can potentially bypass detection and cause failures
in anomaly detection systems based on traditional ML, such as AutoEncoders,
which are unable to process hypoglyphed data without crashing. We investigate
the use of Large Language Models (LLMs) for anomaly detection within the O-RAN
architecture to address this challenge. We demonstrate that LLM-based xApps
maintain robust operational performance and are capable of processing
manipulated messages without crashing. While initial detection accuracy
requires further improvements, our results highlight the robustness of LLMs to
adversarial attacks such as hypoglyphs in input data. There is potential to use
their adaptability through prompt engineering to further improve the accuracy,
although this requires further research. Additionally, we show that LLMs
achieve low detection latency (under 0.07 seconds), making them suitable for
Near-Real-Time (Near-RT) RIC deployments.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [405] [Sharper Perturbed-Kullback-Leibler Exponential Tail Bounds for Beta and Dirichlet Distributions](https://arxiv.org/abs/2508.07991)
*Pierre Perrault*

Main category: math.PR

TL;DR: 论文给出改进的Beta分布指数尾界，扩展结果至Dirichlet分布和Dirichlet过程。


<details>
  <summary>Details</summary>
Motivation: 改进文献[15]中关于Beta分布的尾界结果。

Method: 将原尾界解释为常规Kullback - Leibler (KL)散度尾界，引入特定扰动η使Beta分布均值在KL界内更接近零，并选择更大扰动来收紧尾界。

Result: 得到改进的Beta分布指数尾界，并将结果扩展到Dirichlet分布和Dirichlet过程。

Conclusion: 通过改进的方法可以得到更优的尾界，且能将结果从Beta分布扩展到其他分布。

Abstract: This paper presents an improved exponential tail bound for Beta
distributions, refining a result in [15]. This improvement is achieved by
interpreting their bound as a regular Kullback-Leibler (KL) divergence one,
while introducing a specific perturbation $\eta$ that shifts the mean of the
Beta distribution closer to zero within the KL bound. Our contribution is to
show that a larger perturbation can be chosen, thereby tightening the bound. We
then extend this result from the Beta distribution to Dirichlet distributions
and Dirichlet processes (DPs).

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [406] [Exploring Strategies for Personalized Radiation Therapy: Part III Identifying genetic determinants for Radiation Response with Meta Learning](https://arxiv.org/abs/2508.08030)
*Hao Peng,Yuanyuan Zhang,Steve Jiang,Robert Timmerman,John Minna*

Main category: physics.med-ph

TL;DR: 提出元学习框架用于一次性预测放射敏感性，相比RSI模型有优势，结果表明该方法泛化性好、能提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前癌症放疗策略未考虑肿瘤异质性，依赖统一剂量处方，需更好的放射敏感性预测方法。

Method: 引入元学习框架，利用细胞系水平基因表达数据进行一次性放射敏感性（SF2）预测，允许每个基因重要性因样本而异。

Result: 元学习对未见样本有强泛化性，在高放射敏感性变异性的肿瘤亚组表现良好。

Conclusion: 该方法能快速适应个体样本，提高不同肿瘤亚型预测准确性，揭示基因影响的上下文依赖模式，为个性化治疗提供信息。

Abstract: Radiation response in cancer is shaped by complex, patient specific biology,
yet current treatment strategies often rely on uniform dose prescriptions
without accounting for tumor heterogeneity. In this study, we introduce a meta
learning framework for one-shot prediction of radiosensitivity measured by SF2
using cell line level gene expression data. Unlike the widely used
Radiosensitivity Index RSI a rank-based linear model trained on a fixed 10-gene
signature, our proposed meta-learned model allows the importance of each gene
to vary by sample through fine tuning. This flexibility addresses key
limitations of static models like RSI, which assume uniform gene contributions
across tumor types and discard expression magnitude and gene gene interactions.
Our results show that meta learning offers robust generalization to unseen
samples and performs well in tumor subgroups with high radiosensitivity
variability, such as adenocarcinoma and large cell carcinoma. By learning
transferable structure across tasks while preserving sample specific
adaptability, our approach enables rapid adaptation to individual samples,
improving predictive accuracy across diverse tumor subtypes while uncovering
context dependent patterns of gene influence that may inform personalized
therapy.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [407] [Assessing Engineering Student Perceptions of Introductory CS Courses in an Indian Context](https://arxiv.org/abs/2508.06563)
*Utsav Kumar Nareti,Divyansh Gupta,Chandranath Adak,Soumi Chattopadhyay,Emma Riese,Tanujit Chakraborty,Mayank Agarwal,Satendra Kumar*

Main category: cs.CY

TL;DR: 研究印度工程学院计算机科学入门课程学生对评估实践的看法，发现学生对不同评估方式的态度及影响因素，还与欧洲学生对比。


<details>
  <summary>Details</summary>
Motivation: 理解学生对评估的看法，为技术教育设计包容有效的学习环境。

Method: 对318名一年级学生进行九周的25项Likert量表调查，用描述性统计和非参数检验分析。

Result: 学生认为实验作业是有效学习活动，考试和项目能提升技能；学生表现和评估看法受多种因素影响；成绩数据不呈高斯分布；与欧洲学生有异同。

Conclusion: 研究结果为编程教育设计包容公平的评估策略提供有价值的见解。

Abstract: Understanding student perceptions of assessment is vital for designing
inclusive and effective learning environments, especially in technical
education. This study explores engineering students' perceptions of assessment
practices in an introductory computer science/ programming course, and its
associated laboratory within an Indian engineering institute context. A total
of 318 first-year Bachelor of Technology students participated in a weekly
25-statement Likert-scale survey conducted over nine weeks. Using descriptive
statistics and non-parametric tests (Mann-Whitney U and Kruskal-Wallis), the
analysis reveals that students largely perceive lab assignments as effective
learning activities and view exams and projects as authentic and
skill-enhancing. Students appreciated the role of instructors in shaping course
content and found teaching assistants to be approachable and helpful, despite
some inconsistencies. The study also finds significant variations in students'
academic performance and assessment perceptions based on prior programming
experience, technology familiarity, gender, and academic branch. Notably, the
performance data did not follow a Gaussian distribution, challenging common
assumptions in grade modeling. A comparative analysis with European cohorts
highlights both universal patterns and contextual differences, offering
valuable insights for designing inclusive and equitable assessment strategies
in programming education.

</details>


### [408] [Teaching Introduction to Programming in the times of AI: A case study of a course re-design](https://arxiv.org/abs/2508.06572)
*Nikolaos Avouris,Kyriakos Sgarbas,George Caridakis,Christos Sintoris*

Main category: cs.CY

TL;DR: 本文综述编程教育中AI工具，指出挑战并讨论应对方法，为教学提供指导。


<details>
  <summary>Details</summary>
Motivation: AI工具融入编程教育日益普遍，需探讨其在教学中的应用及应对挑战。

Method: 对编程教育中AI工具进行综述，分析课程设计等方面挑战并讨论应对方法。

Result: 明确了AI工具在编程教学中的挑战，如课程设计、学生滥用等问题。

Conclusion: 讨论的应对方法可作为机构和教师教学政策的指导，以最大化AI工具效益并解决相关问题。

Abstract: The integration of AI tools into programming education has become
increasingly prevalent in recent years, transforming the way programming is
taught and learned. This paper provides a review of the state-of-the-art AI
tools available for teaching and learning programming, particularly in the
context of introductory courses. It highlights the challenges on course design,
learning objectives, course delivery and formative and summative assessment, as
well as the misuse of such tools by the students. We discuss ways of
re-designing an existing course, re-shaping assignments and pedagogy to address
the current AI technologies challenges. This example can serve as a guideline
for policies for institutions and teachers involved in teaching programming,
aiming to maximize the benefits of AI tools while addressing the associated
challenges and concerns.

</details>


### [409] [Leveraging LLMs for Privacy-Aware Predictions in Participatory Budgeting](https://arxiv.org/abs/2508.06577)
*Juan Zambrano,Clément Contet,Jairo Gudiño,Felipe Garrido-Lucero,Umberto Grandi,Cesar A Hidalgo*

Main category: cs.CY

TL;DR: 文章提出用隐私保护方法预测参与式预算（PB）提案是否会获批，评估GPT 4 Turbo在不同场景下预测提案结果的表现，强调AI工具对PB流程的支持潜力。


<details>
  <summary>Details</summary>
Motivation: PB倡议常因参与率低限制其可见性和合法性，旨在通过支持提案者和帮助组织者透明管理提交内容来加强PB选举。

Method: 提出一种隐私保护方法，仅利用提案文本描述和匿名历史投票记录预测PB提案获批可能性，评估GPT 4 Turbo在不同场景下的预测表现。

Result: 大语言模型的先验知识需结合过去投票数据，才能获得反映现实PB投票行为的预测。

Conclusion: AI驱动的工具可通过提高透明度、规划效率和公民参与度来支持PB流程。

Abstract: Participatory Budgeting (PB) empowers citizens to propose and vote on public
investment projects. Yet, despite its democratic potential, PB initiatives
often suffer from low participation rates, limiting their visibility and
perceived legitimacy. In this work, we aim to strengthen PB elections in two
key ways: by supporting project proposers in crafting better proposals, and by
helping PB organizers manage large volumes of submissions in a transparent
manner. We propose a privacy-preserving approach to predict which PB proposals
are likely to be funded, using only their textual descriptions and anonymous
historical voting records -- without relying on voter demographics or
personally identifiable information. We evaluate the performance of GPT 4 Turbo
in forecasting proposal outcomes across varying contextual scenarios, observing
that the LLM's prior knowledge needs to be complemented by past voting data to
obtain predictions reflecting real-world PB voting behavior. Our findings
highlight the potential of AI-driven tools to support PB processes by improving
transparency, planning efficiency, and civic engagement.

</details>


### [410] [Towards Integrated Alignment](https://arxiv.org/abs/2508.06592)
*Ben Y. Reis,William La Cava*

Main category: cs.CY

TL;DR: 随着AI普及，AI对齐问题严峻，提出整合愿景、设计原则及统一研究领域的建议。


<details>
  <summary>Details</summary>
Motivation: AI对齐领域在行为和表征方法上存在分歧，模型易受欺骗性对齐威胁。

Method: 借鉴免疫学和网络安全的经验，提出综合对齐框架的设计原则，强调战略多样性，建议跨合作等统一研究领域。

Result: 提出综合对齐框架的设计原则和统一研究领域的步骤。

Conclusion: 为AI对齐领域未来发展提供整合性的方向和建议。

Abstract: As AI adoption expands across human society, the problem of aligning AI
models to match human preferences remains a grand challenge. Currently, the AI
alignment field is deeply divided between behavioral and representational
approaches, resulting in narrowly aligned models that are more vulnerable to
increasingly deceptive misalignment threats. In the face of this fragmentation,
we propose an integrated vision for the future of the field. Drawing on related
lessons from immunology and cybersecurity, we lay out a set of design
principles for the development of Integrated Alignment frameworks that combine
the complementary strengths of diverse alignment approaches through deep
integration and adaptive coevolution. We highlight the importance of strategic
diversity - deploying orthogonal alignment and misalignment detection
approaches to avoid homogeneous pipelines that may be "doomed to success". We
also recommend steps for greater unification of the AI alignment research field
itself, through cross-collaboration, open model weights and shared community
resources.

</details>


### [411] [Towards Experience-Centered AI: A Framework for Integrating Lived Experience in Design and Development](https://arxiv.org/abs/2508.06849)
*Sanjana Gautam,Mohit Chandra,Ankolika De,Tatiana Chakravorti,Girik Malik,Munmun De Choudhury*

Main category: cs.CY

TL;DR: 本文提出将生活经验融入AI系统设计与评估的框架，综合跨学科文献，给出相关分类，通过三个应用领域举例，结合从业者见解，最后给出开发以经验为中心AI系统的建议。


<details>
  <summary>Details</summary>
Motivation: 以往研究对人类生活经验的系统理解有限，缺乏将其融入AI开发生命周期的有效策略，需提出框架解决该问题。

Method: 综合生活经验哲学、以人为中心的设计和人机交互等跨学科文献，提出生活经验分类，研究三个应用领域，结合AI系统操作员和人机伙伴关系的见解。

Result: 提出将生活经验融入AI系统设计与评估的框架，给出针对性分类，分析三个应用领域中生活经验的影响，指出责任分配、心理模型校准和系统长期适应等挑战。

Conclusion: 给出开发以经验为中心AI系统的可行建议，为连接技术开发与受AI影响人群生活经验的未来研究奠定基础。

Abstract: Lived experiences fundamentally shape how individuals interact with AI
systems, influencing perceptions of safety, trust, and usability. While prior
research has focused on developing techniques to emulate human preferences, and
proposed taxonomies to categorize risks (such as psychological harms and
algorithmic biases), these efforts have provided limited systematic
understanding of lived human experiences or actionable strategies for embedding
them meaningfully into the AI development lifecycle. This work proposes a
framework for meaningfully integrating lived experience into the design and
evaluation of AI systems. We synthesize interdisciplinary literature across
lived experience philosophy, human-centered design, and human-AI interaction,
arguing that centering lived experience can lead to models that more accurately
reflect the retrospective, emotional, and contextual dimensions of human
cognition. Drawing from a wide body of work across psychology, education,
healthcare, and social policy, we present a targeted taxonomy of lived
experiences with specific applicability to AI systems. To ground our framework,
we examine three application domains (i) education, (ii) healthcare, and (iii)
cultural alignment, illustrating how lived experience informs user goals,
system expectations, and ethical considerations in each context. We further
incorporate insights from AI system operators and human-AI partnerships to
highlight challenges in responsibility allocation, mental model calibration,
and long-term system adaptation. We conclude with actionable recommendations
for developing experience-centered AI systems that are not only technically
robust but also empathetic, context-aware, and aligned with human realities.
This work offers a foundation for future research that bridges technical
development with the lived experiences of those impacted by AI systems.

</details>


### [412] [Making Effective Decisions: Machine Learning and the Ecogame in 1970](https://arxiv.org/abs/2508.07027)
*Catherine Mason*

Main category: cs.CY

TL;DR: 本文介绍1970年创新艺术项目Ecogame，结合视觉艺术与控制论概念，为当代以人类为中心使用AI的艺术提供历史先例。


<details>
  <summary>Details</summary>
Motivation: 介绍Ecogame项目并探讨其对当代AI驱动艺术的意义。

Method: 运用模拟和早期机器学习技术，结合视觉艺术与控制论概念。

Result: 提出行为对整个系统有影响，为当代AI驱动艺术提供历史先例。

Conclusion: Ecogame为以更人性化方式使用AI的当代艺术提供了历史依据。

Abstract: This paper considers Ecogame, an innovative art project of 1970, whose
creators believed in a positive vision of a technological future; an
understanding, posited on cybernetics, of a future that could be participatory
via digital means, and therefore more democratised. Using simulation and early
machine learning techniques over a live network, Ecogame combined the power of
visual art with cybernetic concepts of adaptation, feedback, and control to
propose that behaviour had implications for the total system. It provides an
historical precedent for contemporary AI-driven art about using AI in a more
human-centred way.

</details>


### [413] ["Draw me a curator" Examining the visual stereotyping of a cultural services profession by generative AI](https://arxiv.org/abs/2508.07132)
*Dirk HR Spennemann*

Main category: cs.CY

TL;DR: 本文基于230个可视化结果，研究ChatGPT4o对博物馆策展人的描绘，发现其生成图像存在性别、种族和年龄等方面的偏差，凸显生成式AI图像创建数据集的偏见。


<details>
  <summary>Details</summary>
Motivation: 研究流行生成式AI模型ChatGPT4o对博物馆策展人的描绘情况。

Method: 基于230个可视化结果进行分析。

Result: AI生成的图像与现实人口统计数据形成鲜明对比，女性和非白人族裔严重代表性不足，过度代表年轻策展人，且存在刻板属性描绘。

Conclusion: 生成式AI图像创建数据集存在偏见，若不加以批判地看待这些图像，将塑造对博物馆专业人员的不准确描绘。

Abstract: Based on 230 visualisations, this paper examines the depiction of museum
curators by the popular generative Artificial Intelligence (AI) model,
ChatGPT4o. While the AI-generated representations do not reiterate popular
stereotypes of curators as nerdy, conservative in dress and stuck in time
rummaging through collections, they contrast sharply with real-world
demographics. AI-generated imagery extremely underrepresents women (3.5% vs 49%
to 72% in reality) and disregards ethnic communities other than Caucasian (0%
vs 18% to 36%). It only over-represents young curators (79% vs approx. 27%) but
also renders curators to resemble yuppie professionals or people featuring in
fashion advertising. Stereotypical attributes are prevalent, with curators
widely depicted as wearing beards and holding clipboards or digital tablets.
The findings highlight biases in the generative AI image creation dataset,
which is poised to shape an inaccurate portrayal of museum professionals if the
images were to be taken uncritically at face value.

</details>


### [414] [Intersectoral Knowledge in AI and Urban Studies: A Framework for Transdisciplinary Research](https://arxiv.org/abs/2508.07507)
*Rashid Mushkani*

Main category: cs.CY

TL;DR: 本文提出评估和加强人工智能与城市研究中跨学科知识有效性的六维框架，分析常见与少见立场，供早期研究者和团队调和分歧并促进社会负责成果。


<details>
  <summary>Details</summary>
Motivation: 跨学科方法对解决重大社会挑战至关重要，但有效验证和整合不同视角知识存在困难。

Method: 基于2014 - 2024年高被引研究进行广泛分析，提出六维框架分类研究取向。

Result: 发现主流视角，也研究了少见立场的知识生产潜力。

Conclusion: 早期研究者和跨学科团队可利用该框架调和分歧，促进社会负责成果。

Abstract: Transdisciplinary approaches are increasingly essential for addressing grand
societal challenges, particularly in complex domains such as Artificial
Intelligence (AI), urban planning, and social sciences. However, effectively
validating and integrating knowledge across distinct epistemic and ontological
perspectives poses significant difficulties. This article proposes a
six-dimensional framework for assessing and strengthening transdisciplinary
knowledge validity in AI and city studies, based on an extensive analysis of
the most cited research (2014--2024). Specifically, the framework classifies
research orientations according to ontological, epistemological,
methodological, teleological, axiological, and valorization dimensions. Our
findings show a predominance of perspectives aligned with critical realism
(ontological), positivism (epistemological), analytical methods
(methodological), consequentialism (teleological), epistemic values
(axiological), and social/economic valorization. Less common stances, such as
idealism, mixed methods, and cultural valorization, are also examined for their
potential to enrich knowledge production. We highlight how early career
researchers and transdisciplinary teams can leverage this framework to
reconcile divergent disciplinary viewpoints and promote socially accountable
outcomes.

</details>


### [415] [Unequal Uncertainty: Rethinking Algorithmic Interventions for Mitigating Discrimination from AI](https://arxiv.org/abs/2508.07872)
*Holli Sargeant,Mackenzie Jorgensen,Arina Shah,Adrian Weller,Umang Bhatt*

Main category: cs.CY

TL;DR: 文章对基于不确定性的算法干预进行社会技术和法律分析，通过案例展示不确定性阈值使用的歧视影响，认为选择性摩擦是更公平、负责的AI辅助决策途径。


<details>
  <summary>Details</summary>
Motivation: 解决AI预测不确定性给AI辅助决策带来的法律和伦理挑战。

Method: 对选择性弃权和选择性摩擦两种算法干预进行研究，通过AI辅助消费信贷决策和内容审核两个案例分析。

Result: 看似中立的不确定性阈值使用会触发歧视性影响，两种干预在英国法律下都有非法歧视风险。

Conclusion: 选择性摩擦通过保持透明度和鼓励更谨慎的人类判断，是实现更公平、更负责任的AI辅助决策的有希望途径。

Abstract: Uncertainty in artificial intelligence (AI) predictions poses urgent legal
and ethical challenges for AI-assisted decision-making. We examine two
algorithmic interventions that act as guardrails for human-AI collaboration:
selective abstention, which withholds high-uncertainty predictions from human
decision-makers, and selective friction, which delivers those predictions
together with salient warnings or disclosures that slow the decision process.
Research has shown that selective abstention based on uncertainty can
inadvertently exacerbate disparities and disadvantage under-represented groups
that disproportionately receive uncertain predictions. In this paper, we
provide the first integrated socio-technical and legal analysis of
uncertainty-based algorithmic interventions. Through two case studies,
AI-assisted consumer credit decisions and AI-assisted content moderation, we
demonstrate how the seemingly neutral use of uncertainty thresholds can trigger
discriminatory impacts. We argue that, although both interventions pose risks
of unlawful discrimination under UK law, selective frictions offer a promising
pathway toward fairer and more accountable AI-assisted decision-making by
preserving transparency and encouraging more cautious human judgment.

</details>


### [416] [Advancing Knowledge Tracing by Exploring Follow-up Performance Trends](https://arxiv.org/abs/2508.08019)
*Hengyu Liu,Yushuai Li,Minghe Yu,Tiancheng Zhang,Ge Yu,Torben Bach Pedersen,Kristian Torp,Christian S. Jensen,Tianyi Li*

Main category: cs.CY

TL;DR: 现有知识追踪方法分析历史学习序列与未来表现关系时存在冲突，提出FINER方法结合历史学习序列与后续表现趋势提高学生表现预测准确性，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪（KT）方法在分析历史学习序列与未来表现关系时存在相关性冲突，需要解决该问题以提高学生表现预测准确性。

Method: 提出Forward - Looking Knowledge Tracing（FINER）方法，从历史智能辅导系统（ITS）数据中提取后续表现趋势（FPTs），将其与历史学习序列结合，构建学习模式，引入新颖的相似性感知注意力机制聚合FPTs。

Result: 在六个真实世界数据集上的实验表明，FINER能超越十种最先进的KT方法，准确率提高8.74%至84.85%。

Conclusion: FINER方法能有效解决现有KT方法的相关性冲突问题，提高学生未来表现预测的准确性。

Abstract: Intelligent Tutoring Systems (ITS), such as Massive Open Online Courses,
offer new opportunities for human learning. At the core of such systems,
knowledge tracing (KT) predicts students' future performance by analyzing their
historical learning activities, enabling an accurate evaluation of students'
knowledge states over time. We show that existing KT methods often encounter
correlation conflicts when analyzing the relationships between historical
learning sequences and future performance. To address such conflicts, we
propose to extract so-called Follow-up Performance Trends (FPTs) from
historical ITS data and to incorporate them into KT. We propose a method called
Forward-Looking Knowledge Tracing (FINER) that combines historical learning
sequences with FPTs to enhance student performance prediction accuracy. FINER
constructs learning patterns that facilitate the retrieval of FPTs from
historical ITS data in linear time; FINER includes a novel similarity-aware
attention mechanism that aggregates FPTs based on both frequency and contextual
similarity; and FINER offers means of combining FPTs and historical learning
sequences to enable more accurate prediction of student future performance.
Experiments on six real-world datasets show that FINER can outperform ten
state-of-the-art KT methods, increasing accuracy by 8.74% to 84.85%.

</details>


### [417] [Street-Level AI: Are Large Language Models Ready for Real-World Judgments?](https://arxiv.org/abs/2508.08193)
*Gaurab Pokharel,Shafkat Farabi,Patrick J. Fowler,Sanmay Das*

Main category: cs.CY

TL;DR: 研究大语言模型（LLM）在无家可归资源分配中的判断，发现LLM优先级排序不一致，虽与人类判断有定性一致性，但质疑其用于高风险社会决策的就绪性。


<details>
  <summary>Details</summary>
Motivation: 现有文献多关注AI道德判断与人类判断的一致性或群体公平性，而AI最可能用于替代基层官僚分配稀缺社会资源，故研究LLM判断与人类判断及脆弱性评分系统的一致性。

Method: 使用需要服务的真实数据（仅使用本地大模型以保护隐私），分析LLM判断与人类判断及脆弱性评分系统的一致性。

Result: LLM优先级排序在多方面极不一致，包括不同运行、不同LLM之间以及LLM与脆弱性评分系统之间；同时，LLM在成对测试中与普通人类判断有定性一致性。

Conclusion: 当前一代AI系统尚不适合简单集成到高风险社会决策中。

Abstract: A surge of recent work explores the ethical and societal implications of
large-scale AI models that make "moral" judgments. Much of this literature
focuses either on alignment with human judgments through various thought
experiments or on the group fairness implications of AI judgments. However, the
most immediate and likely use of AI is to help or fully replace the so-called
street-level bureaucrats, the individuals deciding to allocate scarce social
resources or approve benefits. There is a rich history underlying how
principles of local justice determine how society decides on prioritization
mechanisms in such domains. In this paper, we examine how well LLM judgments
align with human judgments, as well as with socially and politically determined
vulnerability scoring systems currently used in the domain of homelessness
resource allocation. Crucially, we use real data on those needing services
(maintaining strict confidentiality by only using local large models) to
perform our analyses. We find that LLM prioritizations are extremely
inconsistent in several ways: internally on different runs, between different
LLMs, and between LLMs and the vulnerability scoring systems. At the same time,
LLMs demonstrate qualitative consistency with lay human judgments in pairwise
testing. Findings call into question the readiness of current generation AI
systems for naive integration in high-stakes societal decision-making.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [418] [Optimal Dividend, Reinsurance, and Capital Injection Strategies for an Insurer with Two Collaborating Business Lines](https://arxiv.org/abs/2508.08130)
*Tim J. Boonen,Engel John C. Dela Vega,Bin Zou*

Main category: math.OC

TL;DR: 本文研究有两条协作业务线的保险公司，管理者做红利支付、再保险和资本注入决策，求解最优策略，给出价值函数，分析策略特点并通过数值算例说明参数影响。


<details>
  <summary>Details</summary>
Motivation: 为有两条协作业务线的保险公司管理者找到最优红利、再保险和资本注入策略，以最大化首次破产前总红利支付的期望加权和。

Method: 求解保险公司决策问题，以获得价值函数和最优策略的闭式解。

Result: 最优红利策略是阈值策略，更重要业务线分红阈值低；最优再保险比例随各业务线总准备金水平下降；资本注入仅用于防止业务线破产。

Conclusion: 成功解决保险公司决策问题，得到最优策略并分析其特点，通过数值算例展示模型参数对最优策略的影响。

Abstract: This paper considers an insurer with two collaborating business lines, and
the risk exposure of each line follows a diffusion risk model. The manager of
the insurer makes three decisions for each line: (i) dividend payout, (ii)
(proportional) reinsurance coverage, and (iii) capital injection (from one line
into the other). The manager seeks an optimal dividend, reinsurance, and
capital injection strategy to maximize the expected weighted sum of the total
dividend payments until the first ruin. We completely solve this problem and
obtain the value function and optimal strategies in closed form. We show that
the optimal dividend strategy is a threshold strategy, and the more important
line always has a lower threshold to pay dividends. The optimal proportion of
risk ceded to the reinsurer is decreasing with respect to the aggregate reserve
level for each line, and capital injection is only used to prevent the ruin of
a business line. Finally, numerical examples are presented to illustrate the
impact of model parameters on the optimal strategies.

</details>


### [419] [Near-Optimal Convergence of Accelerated Gradient Methods under Generalized and $(L_0, L_1)$-Smoothness](https://arxiv.org/abs/2508.06884)
*Alexander Tyurin*

Main category: math.OC

TL;DR: 研究满足ℓ - 平滑条件的凸优化问题的一阶方法，解决了AGD型复杂度问题，实现了最优复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有ℓ - 平滑条件下的方法存在依赖初始梯度、有指数因子或需昂贵辅助子程序等问题，要解决AGD型O(√ℓ(0)R / √ε)复杂度是否可行的问题。

Method: 利用新的Lyapunov函数并设计新算法。

Result: 对于小ε和任意ℓ，实现了O(√ℓ(0)R / √ε)的oracle复杂度，在小ε情况下(L₀,L₁) - 平滑的界O(√L₀R / √ε)是最优的。

Conclusion: 解决了在ℓ - 平滑条件下AGD型复杂度的开放性问题，新算法有更好的复杂度表现。

Abstract: We study first-order methods for convex optimization problems with functions
$f$ satisfying the recently proposed $\ell$-smoothness condition
$||\nabla^{2}f(x)|| \le \ell\left(||\nabla f(x)||\right),$ which generalizes
the $L$-smoothness and $(L_{0},L_{1})$-smoothness. While accelerated gradient
descent AGD is known to reach the optimal complexity $O(\sqrt{L} R /
\sqrt{\varepsilon})$ under $L$-smoothness, where $\varepsilon$ is an error
tolerance and $R$ is the distance between a starting and an optimal point,
existing extensions to $\ell$-smoothness either incur extra dependence on the
initial gradient, suffer exponential factors in $L_{1} R$, or require costly
auxiliary sub-routines, leaving open whether an AGD-type $O(\sqrt{\ell(0)} R /
\sqrt{\varepsilon})$ rate is possible for small-$\varepsilon$, even in the
$(L_{0},L_{1})$-smoothness case.
  We resolve this open question. Leveraging a new Lyapunov function and
designing new algorithms, we achieve $O(\sqrt{\ell(0)} R / \sqrt{\varepsilon})$
oracle complexity for small-$\varepsilon$ and virtually any $\ell$. For
instance, for $(L_{0},L_{1})$-smoothness, our bound $O(\sqrt{L_0} R /
\sqrt{\varepsilon})$ is provably optimal in the small-$\varepsilon$ regime and
removes all non-constant multiplicative factors present in prior accelerated
algorithms.

</details>


### [420] [Machine Learning Algorithms for Improving Exact Classical Solvers in Mixed Integer Continuous Optimization](https://arxiv.org/abs/2508.06906)
*Morteza Kimiaei,Vyacheslav Kungurtsev,Brian Olimba*

Main category: math.OC

TL;DR: 本文探讨机器学习和强化学习如何增强整数和混合整数非线性规划的精确优化方法，介绍统一框架，最后给出学习方法分类并指出挑战。


<details>
  <summary>Details</summary>
Motivation: 整数和混合整数非线性规划在多个领域重要但计算有挑战，需增强精确优化方法。

Method: 引入统一的分支定界框架，嵌入基于学习的策略，用监督、模仿和强化学习模型增强经典算法。

Result: 可在保持正确性的同时加速收敛。

Conclusion: 给出按求解器类别和学习范式的学习方法分类，指出泛化、混合和扩展智能求解器方面的挑战。

Abstract: Integer and mixed-integer nonlinear programming (INLP, MINLP) are central to
logistics, energy, and scheduling, but remain computationally challenging. This
survey examines how machine learning and reinforcement learning can enhance
exact optimization methods - particularly branch-and-bound (BB), without
compromising global optimality. We cover discrete, continuous, and
mixed-integer formulations, and highlight applications such as crew scheduling,
vehicle routing, and hydropower planning. We introduce a unified BB framework
that embeds learning-based strategies into branching, cut selection, node
ordering, and parameter control. Classical algorithms are augmented using
supervised, imitation, and reinforcement learning models to accelerate
convergence while maintaining correctness. We conclude with a taxonomy of
learning methods by solver class and learning paradigm, and outline open
challenges in generalization, hybridization, and scaling intelligent solvers.

</details>


### [421] [From Product Hilbert Spaces to the Generalized Koopman Operator and the Nonlinear Fundamental Lemma](https://arxiv.org/abs/2508.07494)
*Mircea Lazar*

Main category: math.OC

TL;DR: 本文基于乘积希尔伯特空间的正交展开，为含控制输入系统的Koopman算子推广和非线性基本引理推导问题提供新解，给出计算广义Koopman算子有限维近似的方法，并在Van der Pol振荡器上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 含控制输入系统的Koopman算子推广和非线性基本引理推导是数据驱动非线性系统控制方法发展中的关键开放问题。

Method: 基于状态和输入可观测函数希尔伯特空间的张量积构建乘积希尔伯特空间进行正交展开，利用无限维广义Koopman模型的双线性结构。

Result: 证明存在从构造的乘积希尔伯特空间到对应提升状态时间前向传播的希尔伯特空间的无限维线性算子（广义Koopman算子），给出计算其有限维近似的可扩展数据驱动方法和可观测函数的几种选择，推导了非线性基本引理。

Conclusion: 所提出的广义Koopman嵌入方法在Van der Pol振荡器上有效。

Abstract: The generalization of the Koopman operator to systems with control input and
the derivation of a nonlinear fundamental lemma are two open problems that play
a key role in the development of data-driven control methods for nonlinear
systems. Both problems hinge on the construction of observable or basis
functions and their corresponding Hilbert space that enable an
infinite-dimensional, linear system representation. In this paper we derive a
novel solution to these problems based on orthonormal expansion in a product
Hilbert space constructed as the tensor product between the Hilbert spaces of
the state and input observable functions, respectively. We prove that there
exists an infinite-dimensional linear operator, i.e. the generalized Koopman
operator, from the constructed product Hilbert space to the Hilbert space
corresponding to the lifted state propagated forward in time. A scalable
data-driven method for computing finite-dimensional approximations of
generalized Koopman operators and several choices of observable functions are
also presented. Moreover, we derive a nonlinear fundamental lemma by exploiting
the bilinear structure of the infinite-dimensional generalized Koopman model.
The effectiveness of the developed generalized Koopman embedding is illustrated
on the Van der Pol oscillator.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [422] [Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey](https://arxiv.org/abs/2508.07163)
*Kamal Acharya,Iman Sharifi,Mehul Lad,Liang Sun,Houbing Song*

Main category: cs.RO

TL;DR: 本文综述神经符号AI在高级空中交通（AAM）的应用，分析研究现状并指出挑战，提供未来研究方向和路线图。


<details>
  <summary>Details</summary>
Motivation: 结合神经符号AI与AAM，解决AAM中复杂的监管、运营和安全挑战。

Method: 对神经符号AI在AAM关键领域的应用进行综述，分析研究现状，分类当前进展，展示案例。

Result: 研究领域分散，神经符号强化学习等方法有动态优化潜力，但在可扩展性、鲁棒性和符合航空标准方面有障碍。

Conclusion: 为开发下一代空中交通解决方案的研究者和从业者提供了路线图。

Abstract: Neurosymbolic AI combines neural network adaptability with symbolic
reasoning, promising an approach to address the complex regulatory,
operational, and safety challenges in Advanced Air Mobility (AAM). This survey
reviews its applications across key AAM domains such as demand forecasting,
aircraft design, and real-time air traffic management. Our analysis reveals a
fragmented research landscape where methodologies, including Neurosymbolic
Reinforcement Learning, have shown potential for dynamic optimization but still
face hurdles in scalability, robustness, and compliance with aviation
standards. We classify current advancements, present relevant case studies, and
outline future research directions aimed at integrating these approaches into
reliable, transparent AAM systems. By linking advanced AI techniques with AAM's
operational demands, this work provides a concise roadmap for researchers and
practitioners developing next-generation air mobility solutions.

</details>


### [423] [UPP: Unified Path Planner with Adaptive Safety and Optimality](https://arxiv.org/abs/2505.23197)
*Jatin Kumar Arora,Shubhendu Bhasin*

Main category: cs.RO

TL;DR: 提出统一路径规划器UPP平衡路径规划的安全性与最优性，在模拟和实际机器人上验证其性能。


<details>
  <summary>Details</summary>
Motivation: 现有路径规划算法大多只关注最优性或安全性，很少同时兼顾两者，因此需要新算法解决此问题。

Method: 提出UPP，使用修改后的启发式方法和动态安全成本函数，可通过可调参数调整安全级别并权衡计算复杂度。

Result: 在模拟中展示了参数变化对结果的影响，与多种传统和安全最优规划算法进行比较，在TurtleBot上验证机器人能找到安全且次优的路径。

Conclusion: UPP能够在路径规划中平衡安全性和最优性，具有一定的实用性和有效性。

Abstract: We are surrounded by robots helping us perform complex tasks. Robots have a
wide range of applications, from industrial automation to personalized
assistance. However, with great technological innovation come significant
challenges. One of the major challenges in robotics is path planning. Despite
advancements such as graph search, sampling, and potential field methods, most
path planning algorithms focus either on optimality or on safety. Very little
research addresses both simultaneously. We propose a Unified Path Planner (UPP)
that uses modified heuristics and a dynamic safety cost function to balance
safety and optimality. The level of safety can be adjusted via tunable
parameters, trading off against computational complexity. We demonstrate the
planner's performance in simulations, showing how parameter variation affects
results. UPP is compared with various traditional and safe-optimal planning
algorithms across different scenarios. We also validate it on a TurtleBot,
where the robot successfully finds safe and sub-optimal paths.

</details>


### [424] [MetAdv: A Unified and Interactive Adversarial Testing Platform for Autonomous Driving](https://arxiv.org/abs/2508.06534)
*Aishan Liu,Jiakai Wang,Tianyuan Zhang,Hainan Li,Jiangfan Liu,Siyuan Liang,Yilong Ren,Xianglong Liu,Dacheng Tao*

Main category: cs.RO

TL;DR: 本文提出新型对抗测试平台MetAdv，可结合虚拟仿真与物理车辆反馈进行对抗性评估，支持多种AD任务和算法范式，有人在环能力，为安全AD提供框架。


<details>
  <summary>Details</summary>
Motivation: 评估和确保自动驾驶系统的对抗鲁棒性是关键且未解决的挑战。

Method: 建立混合虚拟 - 物理沙盒，设计三层闭环测试环境，支持多种AD任务和算法范式，具备人在环能力。

Result: MetAdv能实现端到端的对抗评估，支持灵活3D车辆建模和环境切换，可实时捕获驾驶员信号和反馈。

Conclusion: MetAdv可为对抗性评估提供可扩展和统一的框架，为更安全的自动驾驶铺平道路。

Abstract: Evaluating and ensuring the adversarial robustness of autonomous driving (AD)
systems is a critical and unresolved challenge. This paper introduces MetAdv, a
novel adversarial testing platform that enables realistic, dynamic, and
interactive evaluation by tightly integrating virtual simulation with physical
vehicle feedback. At its core, MetAdv establishes a hybrid virtual-physical
sandbox, within which we design a three-layer closed-loop testing environment
with dynamic adversarial test evolution. This architecture facilitates
end-to-end adversarial evaluation, ranging from high-level unified adversarial
generation, through mid-level simulation-based interaction, to low-level
execution on physical vehicles. Additionally, MetAdv supports a broad spectrum
of AD tasks, algorithmic paradigms (e.g., modular deep learning pipelines,
end-to-end learning, vision-language models). It supports flexible 3D vehicle
modeling and seamless transitions between simulated and physical environments,
with built-in compatibility for commercial platforms such as Apollo and Tesla.
A key feature of MetAdv is its human-in-the-loop capability: besides flexible
environmental configuration for more customized evaluation, it enables
real-time capture of physiological signals and behavioral feedback from
drivers, offering new insights into human-machine trust under adversarial
conditions. We believe MetAdv can offer a scalable and unified framework for
adversarial assessment, paving the way for safer AD.

</details>


### [425] [Symbolic Learning of Interpretable Reduced-Order Models for Jumping Quadruped Robots](https://arxiv.org/abs/2508.06538)
*Gioele Buriani,Jingyue Liu,Maximilian Stölzle,Cosimo Della Santina,Jiatao Ding*

Main category: cs.RO

TL;DR: 本文提出一种推导四足机器人跳跃可解释动力学降阶模型的新方法，经实验验证优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 降阶模型对四足机器人运动规划和控制至关重要，需一种推导跳跃可解释动力学模型的方法。

Method: 提出结合Sparse Identification of Nonlinear Dynamics (SINDy)和跳跃动力学物理结构先验的学习架构，在低维潜在空间捕获高维非线性跳跃动力学。

Result: 该方法比传统的驱动弹簧倒立摆(aSLIP)模型精度更高，通过不同跳跃策略的仿真和硬件实验验证。

Conclusion: 所提方法有效，可用于四足机器人跳跃的可解释动力学降阶模型推导。

Abstract: Reduced-order models are essential for motion planning and control of
quadruped robots, as they simplify complex dynamics while preserving critical
behaviors. This paper introduces a novel methodology for deriving such
interpretable dynamic models, specifically for jumping. We capture the
high-dimensional, nonlinear jumping dynamics in a low-dimensional latent space
by proposing a learning architecture combining Sparse Identification of
Nonlinear Dynamics (SINDy) with physical structural priors on the jump
dynamics. Our approach demonstrates superior accuracy to the traditional
actuated Spring-loaded Inverted Pendulum (aSLIP) model and is validated through
simulation and hardware experiments across different jumping strategies.

</details>


### [426] [Efficient Safety Testing of Autonomous Vehicles via Adaptive Search over Crash-Derived Scenarios](https://arxiv.org/abs/2508.06575)
*Rui Zhou*

Main category: cs.RO

TL;DR: 本文聚焦设计自动驾驶汽车在安全关键场景的加速测试算法，提出ALVNS - SA算法提升测试效率。


<details>
  <summary>Details</summary>
Motivation: 确保自动驾驶汽车安全至关重要，安全关键场景对测试方法提出挑战，需要高效测试方法验证其安全性。

Method: 从CIMSS - TA数据库提取典型逻辑场景并重构获取预碰撞特征；集成百度Apollo控制自车行为；提出自适应大变量邻域模拟退火算法（ALVNS - SA）加速测试。

Result: 使用ALVNS - SA算法测试效率显著提升，安全关键场景覆盖率达84.00%，碰撞场景覆盖率96.83%，近碰撞场景覆盖率92.07%，比遗传算法等覆盖率更高。

Conclusion: ALVNS - SA算法能有效提高自动驾驶汽车在安全关键场景的测试效率和覆盖率。

Abstract: Ensuring the safety of autonomous vehicles (AVs) is paramount in their
development and deployment. Safety-critical scenarios pose more severe
challenges, necessitating efficient testing methods to validate AVs safety.
This study focuses on designing an accelerated testing algorithm for AVs in
safety-critical scenarios, enabling swift recognition of their driving
capabilities. First, typical logical scenarios were extracted from real-world
crashes in the China In-depth Mobility Safety Study-Traffic Accident (CIMSS-TA)
database, obtaining pre-crash features through reconstruction. Second, Baidu
Apollo, an advanced black-box automated driving system (ADS) is integrated to
control the behavior of the ego vehicle. Third, we proposed an adaptive
large-variable neighborhood-simulated annealing algorithm (ALVNS-SA) to
expedite the testing process. Experimental results demonstrate a significant
enhancement in testing efficiency when utilizing ALVNS-SA. It achieves an
84.00% coverage of safety-critical scenarios, with crash scenario coverage of
96.83% and near-crash scenario coverage of 92.07%. Compared to genetic
algorithm (GA), adaptive large neighborhood-simulated annealing algorithm
(ALNS-SA), and random testing, ALVNS-SA exhibits substantially higher coverage
in safety-critical scenarios.

</details>


### [427] [Learning Causal Structure Distributions for Robust Planning](https://arxiv.org/abs/2508.06742)
*Alejandro Murillo-Gonzalez,Junhong Xu,Lantao Liu*

Main category: cs.RO

TL;DR: 本文提出考虑结构信息不确定性学习功能关系，能得到更鲁棒动力学模型，用更低计算资源改进下游规划，通过估计因果结构分布实现，经多种机器人验证。


<details>
  <summary>Details</summary>
Motivation: 常见模型学习方法忽略因果结构，未利用机器人系统交互稀疏性，本文旨在解决此问题，得到更优模型。

Method: 估计因果结构分布，采样因果图为编码器 - 多解码器概率模型的隐空间表示提供信息。

Result: 模型可学习机器人动力学，结合采样规划器能在新环境执行新任务，验证了学习动力学的适应性、对输入损坏和环境变化的鲁棒性。

Conclusion: 考虑结构信息不确定性学习功能关系能得到更鲁棒动力学模型，降低计算资源消耗，在机器人领域有良好应用效果。

Abstract: Structural causal models describe how the components of a robotic system
interact. They provide both structural and functional information about the
relationships that are present in the system. The structural information
outlines the variables among which there is interaction. The functional
information describes how such interactions work, via equations or learned
models. In this paper we find that learning the functional relationships while
accounting for the uncertainty about the structural information leads to more
robust dynamics models which improves downstream planning, while using
significantly lower computational resources. This in contrast with common
model-learning methods that ignore the causal structure and fail to leverage
the sparsity of interactions in robotic systems. We achieve this by estimating
a causal structure distribution that is used to sample causal graphs that
inform the latent-space representations in an encoder-multidecoder
probabilistic model. We show that our model can be used to learn the dynamics
of a robot, which together with a sampling-based planner can be used to perform
new tasks in novel environments, provided an objective function for the new
requirement is available. We validate our method using manipulators and mobile
robots in both simulation and the real-world. Additionally, we validate the
learned dynamics' adaptability and increased robustness to corrupted inputs and
changes in the environment, which is highly desirable in challenging real-world
robotics scenarios. Video: https://youtu.be/X6k5t7OOnNc.

</details>


### [428] [Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction](https://arxiv.org/abs/2508.07079)
*Mohamed Parvez Aslam,Bojan Derajic,Mohamed-Khalil Bouzidi,Sebastian Bernhard,Jan Oliver Ringert*

Main category: cs.RO

TL;DR: 本文评估基于深度学习的SI行人轨迹预测器集成到MPC框架在机器人上的效果，显示SI能改善轨迹预测、提升安全性和平顺性，强调系统级评估重要性。


<details>
  <summary>Details</summary>
Motivation: 解决自主机器人在行人密集环境中的安全导航挑战。

Method: 在物理机器人上，将基于深度学习的SI行人轨迹预测器集成到MPC框架，在不同行人密度下测试，与传统CV模型在开环预测和闭环导航中对比。

Result: SI减少轨迹预测误差，在低密度环境误差最多降低76%，在拥挤场景提升安全性和平顺性；开环指标与闭环性能有差异，SI模型预测更宽泛谨慎。

Conclusion: 强调系统级评估重要性，SI - MPC框架在动态有人环境安全自适应导航有前景。

Abstract: Safe navigation in pedestrian-rich environments remains a key challenge for
autonomous robots. This work evaluates the integration of a deep learning-based
Social-Implicit (SI) pedestrian trajectory predictor within a Model Predictive
Control (MPC) framework on the physical Continental Corriere robot. Tested
across varied pedestrian densities, the SI-MPC system is compared to a
traditional Constant Velocity (CV) model in both open-loop prediction and
closed-loop navigation. Results show that SI improves trajectory prediction -
reducing errors by up to 76% in low-density settings - and enhances safety and
motion smoothness in crowded scenes. Moreover, real-world deployment reveals
discrepancies between open-loop metrics and closed-loop performance, as the SI
model yields broader, more cautious predictions. These findings emphasize the
importance of system-level evaluation and highlight the SI-MPC framework's
promise for safer, more adaptive navigation in dynamic, human-populated
environments.

</details>


### [429] [An Evolutionary Game-Theoretic Merging Decision-Making Considering Social Acceptance for Autonomous Driving](https://arxiv.org/abs/2508.07080)
*Haolin Liu,Zijun Guo,Yanbo Chen,Jiaqi Chen,Huilong Yu,Junqiang Xi*

Main category: cs.RO

TL;DR: 提出基于进化博弈论的高速公路匝道汇入决策框架，考虑人类驾驶员有限理性，平衡自动驾驶汽车和主路车辆利益，提升汇入效率、舒适性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有决策算法无法充分应对自动驾驶汽车动态复杂性和社会接受度问题，导致次优或不安全汇入决策。

Method: 将切入决策过程建模为具有多目标收益函数的进化博弈论问题，求解复制动态方程得到最优切入时机，同时提出实时驾驶风格估计算法在线调整博弈收益函数。

Result: 与现有博弈论和传统规划方法相比，在多目标指标上提高了自动驾驶汽车和主路车辆的效率、舒适性和安全性。

Conclusion: 所提出的进化博弈论汇入决策框架有效可行，能更好地解决自动驾驶汽车匝道汇入问题。

Abstract: Highway on-ramp merging is of great challenge for autonomous vehicles (AVs),
since they have to proactively interact with surrounding vehicles to enter the
main road safely within limited time. However, existing decision-making
algorithms fail to adequately address dynamic complexities and social
acceptance of AVs, leading to suboptimal or unsafe merging decisions. To
address this, we propose an evolutionary game-theoretic (EGT) merging
decision-making framework, grounded in the bounded rationality of human
drivers, which dynamically balances the benefits of both AVs and main-road
vehicles (MVs). We formulate the cut-in decision-making process as an EGT
problem with a multi-objective payoff function that reflects human-like driving
preferences. By solving the replicator dynamic equation for the evolutionarily
stable strategy (ESS), the optimal cut-in timing is derived, balancing
efficiency, comfort, and safety for both AVs and MVs. A real-time driving style
estimation algorithm is proposed to adjust the game payoff function online by
observing the immediate reactions of MVs. Empirical results demonstrate that we
improve the efficiency, comfort and safety of both AVs and MVs compared with
existing game-theoretic and traditional planning approaches across multi-object
metrics.

</details>


### [430] [AgriVLN: Vision-and-Language Navigation for Agricultural Robots](https://arxiv.org/abs/2508.07406)
*Xiaobei Zhao,Xingqi Lyu,Xiang Li*

Main category: cs.RO

TL;DR: 本文提出农业视觉语言导航基准A2A和基线方法AgriVLN，针对其处理长指令问题提出STL模块改进性能，且展现出农业领域的SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有农业机器人移动受限、适应性差，且无专门用于农业场景的视觉语言导航基准和方法。

Method: 提出A2A基准，包含1560个跨六种农业场景的情节；提出基于VLM的AgriVLN基线方法；提出STL指令分解模块并集成到AgriVLN中。

Result: AgriVLN在短指令上表现良好，处理长指令有困难；集成STL模块后成功率从0.33提升到0.47；与现有VLN方法对比，在农业领域展现SOTA性能。

Conclusion: 所提A2A基准和AgriVLN方法能有效提升农业机器人在视觉语言导航任务中的性能，集成STL模块可改善长指令处理能力。

Abstract: Agricultural robots have emerged as powerful members in agricultural tasks,
nevertheless, still heavily rely on manual operation or untransportable railway
for movement, resulting in limited mobility and poor adaptability.
Vision-and-Language Navigation (VLN) enables robots to navigate to the target
destinations following natural language instructions, demonstrating strong
performance on several domains. However, none of the existing benchmarks or
methods is specifically designed for agricultural scenes. To bridge this gap,
we propose Agriculture to Agriculture (A2A) benchmark, containing 1,560
episodes across six diverse agricultural scenes, in which all realistic RGB
videos are captured by front-facing camera on a quadruped robot at a height of
0.38 meters, aligning with the practical deployment conditions. Meanwhile, we
propose Vision-and-Language Navigation for Agricultural Robots (AgriVLN)
baseline based on Vision-Language Model (VLM) prompted with carefully crafted
templates, which can understand both given instructions and agricultural
environments to generate appropriate low-level actions for robot control. When
evaluated on A2A, AgriVLN performs well on short instructions but struggles
with long instructions, because it often fails to track which part of the
instruction is currently being executed. To address this, we further propose
Subtask List (STL) instruction decomposition module and integrate it into
AgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare
AgriVLN with several existing VLN methods, demonstrating the state-of-the-art
performance in the agricultural domain.

</details>


### [431] [Grasp-HGN: Grasping the Unexpected](https://arxiv.org/abs/2508.07648)
*Mehrshad Zandigohar,Mallesham Dasari,Gunar Schirner*

Main category: cs.RO

TL;DR: 文章针对经桡截肢者假肢手控制问题，提出语义投影概念，介绍Grasp - LLaVA模型和Hybrid Grasp Network（HGN）架构以提升对未知物体抓握估计性能和解决性能 - 延迟问题。


<details>
  <summary>Details</summary>
Motivation: 现有抓握模型在实验室外鲁棒性和新环境泛化性不足，对未知物体表现差，影响用户独立性和生活质量，需改进假肢手控制设计。

Method: 定义语义投影；提出Grasp - LLaVA模型进行类人推理估计抓握类型；提出HGN边缘 - 云部署架构，结合置信度校准实现动态切换。

Result: Grasp - LLaVA对未知物体类型准确率达50.2%；HGN结合置信度校准使语义投影准确率提升到42.3%，速度提升3.5倍，在真实样本上平均准确率达86%，推理速度比Grasp - LLaVA快2.2倍。

Conclusion: 所提方法有效提升了假肢手抓握模型对未知物体的性能，缩小了性能 - 延迟差距。

Abstract: For transradial amputees, robotic prosthetic hands promise to regain the
capability to perform daily living activities. To advance next-generation
prosthetic hand control design, it is crucial to address current shortcomings
in robustness to out of lab artifacts, and generalizability to new
environments. Due to the fixed number of object to interact with in existing
datasets, contrasted with the virtually infinite variety of objects encountered
in the real world, current grasp models perform poorly on unseen objects,
negatively affecting users' independence and quality of life.
  To address this: (i) we define semantic projection, the ability of a model to
generalize to unseen object types and show that conventional models like YOLO,
despite 80% training accuracy, drop to 15% on unseen objects. (ii) we propose
Grasp-LLaVA, a Grasp Vision Language Model enabling human-like reasoning to
infer the suitable grasp type estimate based on the object's physical
characteristics resulting in a significant 50.2% accuracy over unseen object
types compared to 36.7% accuracy of an SOTA grasp estimation model.
  Lastly, to bridge the performance-latency gap, we propose Hybrid Grasp
Network (HGN), an edge-cloud deployment infrastructure enabling fast grasp
estimation on edge and accurate cloud inference as a fail-safe, effectively
expanding the latency vs. accuracy Pareto. HGN with confidence calibration (DC)
enables dynamic switching between edge and cloud models, improving semantic
projection accuracy by 5.6% (to 42.3%) with 3.5x speedup over the unseen object
types. Over a real-world sample mix, it reaches 86% average accuracy (12.2%
gain over edge-only), and 2.2x faster inference than Grasp-LLaVA alone.

</details>


### [432] [DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts](https://arxiv.org/abs/2508.07842)
*Yutong Shen,Hangxu Liu,Penghui Liu,Ruizhe Xia,Tianyi Yao,Yitong Sun,Tongtong Feng*

Main category: cs.RO

TL;DR: 提出DETACH跨域学习框架解决长时程人类场景交互任务泛化问题，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖技能链，环境观测与自身状态耦合，缺乏跨域泛化能力，无法完成各类跨域长时程任务。

Method: 提出DETACH框架，受大脑“where - what”双通路机制启发，包含环境学习模块和技能学习模块，分别实现环境 - 自身解耦和独立运动模式编码。

Result: 在各种人类场景交互长时程任务实验中，与现有方法相比，DETACH平均子任务成功率提高23%，平均执行效率提高29%。

Conclusion: DETACH框架能有效解决长时程人类场景交互任务的跨域泛化问题，提升任务完成效果。

Abstract: Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complex
multi-step tasks that require continuous planning, sequential decision-making,
and extended execution across domains to achieve the final goal. However,
existing methods heavily rely on skill chaining by concatenating pre-trained
subtasks, with environment observations and self-state tightly coupled, lacking
the ability to generalize to new combinations of environments and skills,
failing to complete various LH tasks across domains. To solve this problem,
this paper presents DETACH, a cross-domain learning framework for LH tasks via
biologically inspired dual-stream disentanglement. Inspired by the brain's
"where-what" dual pathway mechanism, DETACH comprises two core modules: i) an
environment learning module for spatial understanding, which captures object
functions, spatial relationships, and scene semantics, achieving cross-domain
transfer through complete environment-self disentanglement; ii) a skill
learning module for task execution, which processes self-state information
including joint degrees of freedom and motor patterns, enabling cross-skill
transfer through independent motor pattern encoding. We conducted extensive
experiments on various LH tasks in HSI scenes. Compared with existing methods,
DETACH can achieve an average subtasks success rate improvement of 23% and
average execution efficiency improvement of 29%.

</details>


### [433] [Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning](https://arxiv.org/abs/2508.07885)
*Shoaib Ahmmad,Zubayer Ahmed Aditto,Md Mehrab Hossain,Noushin Yeasmin,Shorower Hossain*

Main category: cs.RO

TL;DR: 本文提出一种用于室内无GPS环境的四旋翼无人机导航的AI感知系统，结合云计算和定制PCB，集成多种技术，实验表现良好，可作为辅助导航系统。


<details>
  <summary>Details</summary>
Motivation: 解决室内无GPS环境下四旋翼无人机的导航问题。

Method: 利用云计算卸载计算任务，采用定制PCB采集数据，集成YOLOv11、Depth Anything V2、ToF传感器、IMU和云LLM，设置虚拟安全包络，采用多线程架构和卡尔曼滤波。

Result: 在室内测试平台上，目标检测mAP50为0.6，深度估计MAE为7.2cm，42次试验中安全包络仅突破16次，系统端到端延迟低于1秒。

Conclusion: 该云支持的高智能框架可作为辅助感知和导航系统，补充现有无人机在无GPS受限空间的自主性。

Abstract: This paper introduces an advanced AI-driven perception system for autonomous
quadcopter navigation in GPS-denied indoor environments. The proposed framework
leverages cloud computing to offload computationally intensive tasks and
incorporates a custom-designed printed circuit board (PCB) for efficient sensor
data acquisition, enabling robust navigation in confined spaces. The system
integrates YOLOv11 for object detection, Depth Anything V2 for monocular depth
estimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial
Measurement Unit (IMU), and a cloud-based Large Language Model (LLM) for
context-aware decision-making. A virtual safety envelope, enforced by
calibrated sensor offsets, ensures collision avoidance, while a multithreaded
architecture achieves low-latency processing. Enhanced spatial awareness is
facilitated by 3D bounding box estimation with Kalman filtering. Experimental
results in an indoor testbed demonstrate strong performance, with object
detection achieving a mean Average Precision (mAP50) of 0.6, depth estimation
Mean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42
trials over approximately 11 minutes, and end-to-end system latency below 1
second. This cloud-supported, high-intelligence framework serves as an
auxiliary perception and navigation system, complementing state-of-the-art
drone autonomy for GPS-denied confined spaces.

</details>


### [434] [COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models](https://arxiv.org/abs/2508.08144)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.RO

TL;DR: 提出一种模型压缩方法，用于资源受限移动平台的神经网络控制器，在TD - MPC算法上验证可降低复杂度并保持性能和稳定性，还确定安全压缩比边界。


<details>
  <summary>Details</summary>
Motivation: 资源受限移动平台对高效神经网络控制器需求增加，但DNN计算复杂和内存需求大限制其在边缘设备部署。

Method: 利用组件感知结构化剪枝确定每个剪枝组的最优剪枝幅度，结合TD - MPC算法并集成Lyapunov准则。

Result: 成功降低模型复杂度，保持控制性能和稳定性，确定安全压缩比边界。

Conclusion: 该方法为模型压缩提供理论框架，可在资源受限环境中安全部署压缩后的神经网络控制器。

Abstract: The rapid growth of resource-constrained mobile platforms, including mobile
robots, wearable systems, and Internet-of-Things devices, has increased the
demand for computationally efficient neural network controllers (NNCs) that can
operate within strict hardware limitations. While deep neural networks (DNNs)
demonstrate superior performance in control applications, their substantial
computational complexity and memory requirements present significant barriers
to practical deployment on edge devices. This paper introduces a comprehensive
model compression methodology that leverages component-aware structured pruning
to determine the optimal pruning magnitude for each pruning group, ensuring a
balance between compression and stability for NNC deployment. Our approach is
rigorously evaluated on Temporal Difference Model Predictive Control (TD-MPC),
a state-of-the-art model-based reinforcement learning algorithm, with a
systematic integration of mathematical stability guarantee properties,
specifically Lyapunov criteria. The key contribution of this work lies in
providing a principled framework for determining the theoretical limits of
model compression while preserving controller stability. Experimental
validation demonstrates that our methodology successfully reduces model
complexity while maintaining requisite control performance and stability
characteristics. Furthermore, our approach establishes a quantitative boundary
for safe compression ratios, enabling practitioners to systematically determine
the maximum permissible model reduction before violating critical stability
properties, thereby facilitating the confident deployment of compressed NNCs in
resource-limited environments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [435] [A Joint Sparse Self-Representation Learning Method for Multiview Clustering](https://arxiv.org/abs/2508.06857)
*Mengxue Jia,Zhihua Allen-Zhao,You Zhao,Sanyang Liu*

Main category: cs.CV

TL;DR: 提出用于多视图聚类的联合稀疏自表示学习模型及交替二次惩罚（AQP）方法，在六个标准数据集上优于八种先进算法。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类子空间聚类技术存在不足，需更好方法提取局部和全局结构信息，且原算法应用于非凸非光滑模型时不能保证收敛、泛化能力差。

Method: 提出联合稀疏自表示学习模型，用基数约束提取特定视图局部信息，低秩约束揭示全局结构；开发有全局收敛性的交替二次惩罚（AQP）方法迭代求解子问题。

Result: 在六个标准数据集上，所提模型和 AQP 方法优于八种先进算法。

Conclusion: 所提模型和 AQP 方法在多视图聚类中有优越性。

Abstract: Multiview clustering (MC) aims to group samples using consistent and
complementary information across various views. The subspace clustering, as a
fundamental technique of MC, has attracted significant attention. In this
paper, we propose a novel joint sparse self-representation learning model for
MC, where a featured difference is the extraction of view-specific local
information by introducing cardinality (i.e., $\ell_0$-norm) constraints
instead of Graph-Laplacian regularization. Specifically, under each view,
cardinality constraints directly restrict the samples used in the
self-representation stage to extract reliable local and global structure
information, while the low-rank constraint aids in revealing a global coherent
structure in the consensus affinity matrix during merging. The attendant
challenge is that Augmented Lagrange Method (ALM)-based alternating
minimization algorithms cannot guarantee convergence when applied directly to
our nonconvex, nonsmooth model, thus resulting in poor generalization ability.
To address it, we develop an alternating quadratic penalty (AQP) method with
global convergence, where two subproblems are iteratively solved by closed-form
solutions. Empirical results on six standard datasets demonstrate the
superiority of our model and AQP method, compared to eight state-of-the-art
algorithms.

</details>


### [436] [OpenHAIV: A Framework Towards Practical Open-World Learning](https://arxiv.org/abs/2508.07270)
*Xiang Xiang,Qinhao Zhou,Zhuo Xu,Jing Ma,Jiaxin Dai,Yifan Liang,Hanlin Li*

Main category: cs.CV

TL;DR: 本文提出OpenHAIV框架，整合OOD检测、新类发现和增量持续微调，使模型在开放世界自主获取和更新知识。


<details>
  <summary>Details</summary>
Motivation: 现有开放世界识别技术（OOD检测和增量学习）存在局限，OOD检测不利于模型知识更新，增量微调需监督条件，偏离开放世界设定。

Method: 提出OpenHAIV框架，将OOD检测、新类发现和增量持续微调整合到统一流程。

Result: 未提及明确实验结果，但给出框架开源地址https://haiv-lab.github.io/openhaiv 。

Conclusion: OpenHAIV框架可让模型在开放世界环境中自主获取和更新知识。

Abstract: Substantial progress has been made in various techniques for open-world
recognition. Out-of-distribution (OOD) detection methods can effectively
distinguish between known and unknown classes in the data, while incremental
learning enables continuous model knowledge updates. However, in open-world
scenarios, these approaches still face limitations. Relying solely on OOD
detection does not facilitate knowledge updates in the model, and incremental
fine-tuning typically requires supervised conditions, which significantly
deviate from open-world settings. To address these challenges, this paper
proposes OpenHAIV, a novel framework that integrates OOD detection, new class
discovery, and incremental continual fine-tuning into a unified pipeline. This
framework allows models to autonomously acquire and update knowledge in
open-world environments. The proposed framework is available at
https://haiv-lab.github.io/openhaiv .

</details>


### [437] [A Framework Combining 3D CNN and Transformer for Video-Based Behavior Recognition](https://arxiv.org/abs/2508.06528)
*Xiuliang Zhang,Tadiwa Elisha Nyamasvisva,Chuntao Liu*

Main category: cs.CV

TL;DR: 提出结合3D CNN和Transformer的混合框架用于视频行为识别，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统3D CNN难以建模长距离依赖，Transformer计算成本高，为解决这些局限开展研究。

Method: 提出结合3D CNN和Transformer架构的混合框架，3D CNN提取低级时空特征，Transformer捕获长距离时间依赖，用融合机制整合两者表示。

Result: 在基准数据集上评估，模型优于传统3D CNN和独立的Transformer，以可管理的复杂度实现更高识别准确率，消融研究验证两模块互补优势。

Conclusion: 该混合框架为视频行为识别提供有效且可扩展的解决方案。

Abstract: Video-based behavior recognition is essential in fields such as public
safety, intelligent surveillance, and human-computer interaction. Traditional
3D Convolutional Neural Network (3D CNN) effectively capture local
spatiotemporal features but struggle with modeling long-range dependencies.
Conversely, Transformers excel at learning global contextual information but
face challenges with high computational costs. To address these limitations, we
propose a hybrid framework combining 3D CNN and Transformer architectures. The
3D CNN module extracts low-level spatiotemporal features, while the Transformer
module captures long-range temporal dependencies, with a fusion mechanism
integrating both representations. Evaluated on benchmark datasets, the proposed
model outperforms traditional 3D CNN and standalone Transformers, achieving
higher recognition accuracy with manageable complexity. Ablation studies
further validate the complementary strengths of the two modules. This hybrid
framework offers an effective and scalable solution for video-based behavior
recognition.

</details>


### [438] [Surformer v1: Transformer-Based Surface Classification Using Tactile and Vision Features](https://arxiv.org/abs/2508.06566)
*Manish Kansana,Elias Hossain,Shahram Rahimi,Noorbakhsh Amiri Golilarz*

Main category: cs.CV

TL;DR: 本文提出Surformer v1用于表面材料识别，先进行仅触觉分类实验，后引入多模态融合，结果显示Surformer v1在准确率、效率和计算成本间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 解决机器人感知和物理交互中表面材料识别问题，利用触觉和视觉输入进行表面分类。

Method: 提出基于Transformer的Surformer v1架构，结合结构化触觉特征和PCA降维的视觉嵌入；先进行仅触觉分类实验，评估多种机器学习模型和Transformer模型；再引入多模态融合，训练Surformer v1和Multimodal CNN。

Result: 仅触觉分类中Transformer模型准确率高、推理时间快；多模态融合中Surformer v1准确率达99.4%，推理时间0.77 ms，Multimodal CNN准确率稍高但推理时间长。

Conclusion: Surformer v1在表面材料识别中能在准确率、效率和计算成本间取得良好平衡。

Abstract: Surface material recognition is a key component in robotic perception and
physical interaction, particularly when leveraging both tactile and visual
sensory inputs. In this work, we propose Surformer v1, a transformer-based
architecture designed for surface classification using structured tactile
features and PCA-reduced visual embeddings extracted via ResNet-50. The model
integrates modality-specific encoders with cross-modal attention layers,
enabling rich interactions between vision and touch. Currently,
state-of-the-art deep learning models for vision tasks have achieved remarkable
performance. With this in mind, our first set of experiments focused
exclusively on tactile-only surface classification. Using feature engineering,
we trained and evaluated multiple machine learning models, assessing their
accuracy and inference time. We then implemented an encoder-only Transformer
model tailored for tactile features. This model not only achieved the highest
accuracy but also demonstrated significantly faster inference time compared to
other evaluated models, highlighting its potential for real-time applications.
To extend this investigation, we introduced a multimodal fusion setup by
combining vision and tactile inputs. We trained both Surformer v1 (using
structured features) and Multimodal CNN (using raw images) to examine the
impact of feature-based versus image-based multimodal learning on
classification accuracy and computational efficiency. The results showed that
Surformer v1 achieved 99.4% accuracy with an inference time of 0.77 ms, while
the Multimodal CNN achieved slightly higher accuracy but required significantly
more inference time. These findings suggest Surformer v1 offers a compelling
balance between accuracy, efficiency, and computational cost for surface
material recognition.

</details>


### [439] [CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition](https://arxiv.org/abs/2508.06632)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Tiancheng Zhao,Gaolei Li,Changting Lin,Yike Guo,Meng Han*

Main category: cs.CV

TL;DR: 现有NeRF方法在渲染复杂镜面反射场景时有挑战，本文提出基于动态系数分解的神经渲染框架，实验表明效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法在渲染复杂镜面反射和高光场景时存在生成模糊反射、优化不稳定等问题，需要改进视图相关外观的建模。

Method: 将复杂外观分解为编码固有材料属性的共享静态神经基和由系数网络根据视图和光照生成的一组动态系数，再由动态辐射积分器合成最终辐射。

Result: 在多个具有挑战性的基准测试中，该方法相比现有技术能产生更清晰、更逼真的镜面高光。

Conclusion: 这种分解范式可为神经场景表示中复杂外观的建模提供灵活有效的方向。

Abstract: Neural Radiance Fields (NeRF) have shown impressive performance in novel view
synthesis, but challenges remain in rendering scenes with complex specular
reflections and highlights. Existing approaches may produce blurry reflections
due to entanglement between lighting and material properties, or encounter
optimization instability when relying on physically-based inverse rendering. In
this work, we present a neural rendering framework based on dynamic coefficient
decomposition, aiming to improve the modeling of view-dependent appearance. Our
approach decomposes complex appearance into a shared, static neural basis that
encodes intrinsic material properties, and a set of dynamic coefficients
generated by a Coefficient Network conditioned on view and illumination. A
Dynamic Radiance Integrator then combines these components to synthesize the
final radiance. Experimental results on several challenging benchmarks suggest
that our method can produce sharper and more realistic specular highlights
compared to existing techniques. We hope that this decomposition paradigm can
provide a flexible and effective direction for modeling complex appearance in
neural scene representations.

</details>


### [440] [MMFformer: Multimodal Fusion Transformer Network for Depression Detection](https://arxiv.org/abs/2508.06701)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Hamdi Altaheri,Lobna Nassar,Fakhri Karray*

Main category: cs.CV

TL;DR: 文章提出MMFformer多模态抑郁检测网络，在两个数据集上评估，结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 抑郁症早期检测重要但基于临床访谈主观评估困难，利用社交网络内容进行早期诊断成研究热点，但面临信息提取和融合挑战。

Method: 引入MMFformer网络，用带残差连接的transformer网络提取视频空间特征，用transformer编码器设计音频时间动态，通过后期和中间融合策略融合特征。

Result: 在两个大规模抑郁检测数据集上评估，F1分数在D - Vlog数据集提高13.92%，在LMVD数据集提高7.74%。

Conclusion: MMFformer网络在抑郁检测上优于现有方法。

Abstract: Depression is a serious mental health illness that significantly affects an
individual's well-being and quality of life, making early detection crucial for
adequate care and treatment. Detecting depression is often difficult, as it is
based primarily on subjective evaluations during clinical interviews. Hence,
the early diagnosis of depression, thanks to the content of social networks,
has become a prominent research area. The extensive and diverse nature of
user-generated information poses a significant challenge, limiting the accurate
extraction of relevant temporal information and the effective fusion of data
across multiple modalities. This paper introduces MMFformer, a multimodal
depression detection network designed to retrieve depressive spatio-temporal
high-level patterns from multimodal social media information. The transformer
network with residual connections captures spatial features from videos, and a
transformer encoder is exploited to design important temporal dynamics in
audio. Moreover, the fusion architecture fused the extracted features through
late and intermediate fusion strategies to find out the most relevant
intermodal correlations among them. Finally, the proposed network is assessed
on two large-scale depression detection datasets, and the results clearly
reveal that it surpasses existing state-of-the-art approaches, improving the
F1-Score by 13.92% for D-Vlog dataset and 7.74% for LMVD dataset. The code is
made available publicly at
https://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection.

</details>


### [441] [FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI](https://arxiv.org/abs/2508.06756)
*Somayeh Farahani,Marjaneh Hejazi,Antonio Di Ieva,Sidong Liu*

Main category: cs.CV

TL;DR: 提出FoundBioNet模型非侵入性预测脑胶质瘤IDH突变状态，表现优于基线方法，增强诊断准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖侵入性组织采样，深度学习模型受限于标注数据，需更通用方法检测脑胶质瘤IDH突变。

Method: 提出FoundBioNet模型，采用SWIN - UNETR架构，含TAFE和CMD模块，在六个公共数据集的1705例患者中训练验证。

Result: 模型在多个独立测试集上取得高AUC值，始终优于基线方法，消融研究证实两模块对提高预测准确性至关重要。

Conclusion: FoundBioNet结合大规模预训练和特定任务微调，可实现可推广的脑胶质瘤特征描述，有望实现更个性化的患者护理。

Abstract: Accurate, noninvasive detection of isocitrate dehydrogenase (IDH) mutation is
essential for effective glioma management. Traditional methods rely on invasive
tissue sampling, which may fail to capture a tumor's spatial heterogeneity.
While deep learning models have shown promise in molecular profiling, their
performance is often limited by scarce annotated data. In contrast, foundation
deep learning models offer a more generalizable approach for glioma imaging
biomarkers. We propose a Foundation-based Biomarker Network (FoundBioNet) that
utilizes a SWIN-UNETR-based architecture to noninvasively predict IDH mutation
status from multi-parametric MRI. Two key modules are incorporated: Tumor-Aware
Feature Encoding (TAFE) for extracting multi-scale, tumor-focused features, and
Cross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch
signals associated with IDH mutation. The model was trained and validated on a
diverse, multi-center cohort of 1705 glioma patients from six public datasets.
Our model achieved AUCs of 90.58%, 88.08%, 65.41%, and 80.31% on independent
test sets from EGD, TCGA, Ivy GAP, RHUH, and UPenn, consistently outperforming
baseline approaches (p <= 0.05). Ablation studies confirmed that both the TAFE
and CMD modules are essential for improving predictive accuracy. By integrating
large-scale pretraining and task-specific fine-tuning, FoundBioNet enables
generalizable glioma characterization. This approach enhances diagnostic
accuracy and interpretability, with the potential to enable more personalized
patient care.

</details>


### [442] [SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding](https://arxiv.org/abs/2508.06763)
*Zihao Sheng,Zilin Huang,Yen-Jung Chen,Yansong Qu,Yuhao Luo,Yue Leng,Sikai Chen*

Main category: cs.CV

TL;DR: 提出SafePLUG框架用于交通意外分析，整理新数据集，实验显示该框架在多任务表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在交通意外理解领域主要关注粗粒度理解，难以处理细粒度视觉细节或局部场景组件，限制其在复杂事故场景的应用。

Method: 提出SafePLUG框架，支持任意形状视觉提示、基于语言指令的像素级分割和时间锚定事件识别；整理包含多模态问答对、详细像素级注释和时间事件边界的新数据集。

Result: SafePLUG在基于区域的问答、像素级分割、时间事件定位和事故事件理解等多任务上表现出色。

Conclusion: SafePLUG为复杂交通场景的细粒度理解奠定基础，有潜力提高驾驶安全和智能交通系统的态势感知能力。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress
across a range of vision-language tasks and demonstrate strong potential for
traffic accident understanding. However, existing MLLMs in this domain
primarily focus on coarse-grained image-level or video-level comprehension and
often struggle to handle fine-grained visual details or localized scene
components, limiting their applicability in complex accident scenarios. To
address these limitations, we propose SafePLUG, a novel framework that empowers
MLLMs with both Pixel-Level Understanding and temporal Grounding for
comprehensive traffic accident analysis. SafePLUG supports both
arbitrary-shaped visual prompts for region-aware question answering and
pixel-level segmentation based on language instructions, while also enabling
the recognition of temporally anchored events in traffic accident scenarios. To
advance the development of MLLMs for traffic accident understanding, we curate
a new dataset containing multimodal question-answer pairs centered on diverse
accident scenarios, with detailed pixel-level annotations and temporal event
boundaries. Experimental results show that SafePLUG achieves strong performance
on multiple tasks, including region-based question answering, pixel-level
segmentation, temporal event localization, and accident event understanding.
These capabilities lay a foundation for fine-grained understanding of complex
traffic scenes, with the potential to improve driving safety and enhance
situational awareness in smart transportation systems. The code, dataset, and
model checkpoints will be made publicly available at:
https://zihaosheng.github.io/SafePLUG

</details>


### [443] [AGIC: Attention-Guided Image Captioning to Improve Caption Relevance](https://arxiv.org/abs/2508.06853)
*L. D. M. S. Sai Teja,Ashok Urlana,Pruthwik Mishra*

Main category: cs.CV

TL;DR: 提出Attention - Guided Image Captioning (AGIC)用于图像描述生成，结合混合解码策略，实验显示其性能优且推理快。


<details>
  <summary>Details</summary>
Motivation: 解决图像描述生成中生成准确和描述性强的文本这一长期挑战。

Method: 提出AGIC在特征空间放大显著视觉区域引导描述生成，引入结合确定性和概率性采样的混合解码策略。

Result: 在Flickr8k和Flickr30k数据集上实验，AGIC匹配或超越多个先进模型，推理更快，多评估指标表现好。

Conclusion: AGIC为图像描述生成提供了可扩展和可解释的解决方案。

Abstract: Despite significant progress in image captioning, generating accurate and
descriptive captions remains a long-standing challenge. In this study, we
propose Attention-Guided Image Captioning (AGIC), which amplifies salient
visual regions directly in the feature space to guide caption generation. We
further introduce a hybrid decoding strategy that combines deterministic and
probabilistic sampling to balance fluency and diversity. To evaluate AGIC, we
conduct extensive experiments on the Flickr8k and Flickr30k datasets. The
results show that AGIC matches or surpasses several state-of-the-art models
while achieving faster inference. Moreover, AGIC demonstrates strong
performance across multiple evaluation metrics, offering a scalable and
interpretable solution for image captioning.

</details>


### [444] [VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding](https://arxiv.org/abs/2508.06869)
*Jianxiang He,Shaoguang Wang,Weiyu Guo,Meisheng Hong,Jungang Li,Yijie Xu,Ziyang Chen,Hui Xiong*

Main category: cs.CV

TL;DR: 针对长视频理解中关键帧检索难题，提出VSI方法，实验显示其在关键帧定位和长视频问答任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有关键帧检索方法因多模态对齐弱、无法捕捉复杂时间语义信息，导致效果不佳，需新方法解决。

Method: 提出Visual - Subtitle Integeration (VSI)方法，通过视频搜索流和字幕匹配流的双流搜索机制，整合字幕、时间戳和场景边界进行多模态关键帧搜索。

Result: VSI在LongVideoBench文本相关子集上关键帧定位准确率达40.00%，下游长视频问答任务准确率达68.48%，超越基线，在中长视频问答任务达SOTA。

Conclusion: VSI方法具有鲁棒性和泛化性，是有效的多模态搜索策略。

Abstract: Long video understanding presents a significant challenge to multimodal large
language models (MLLMs) primarily due to the immense data scale. A critical and
widely adopted strategy for making this task computationally tractable is
keyframe retrieval, which seeks to identify a sparse set of video frames that
are most salient to a given textual query. However, the efficacy of this
approach is hindered by weak multimodal alignment between textual queries and
visual content and fails to capture the complex temporal semantic information
required for precise reasoning. To address this, we propose Visual-Subtitle
Integeration(VSI), a multimodal keyframe search method that integrates
subtitles, timestamps, and scene boundaries into a unified multimodal search
process. The proposed method captures the visual information of video frames as
well as the complementary textual information through a dual-stream search
mechanism by Video Search Stream as well as Subtitle Match Stream,
respectively, and improves the keyframe search accuracy through the interaction
of the two search streams. Experimental results show that VSI achieve 40.00%
key frame localization accuracy on the text-relevant subset of LongVideoBench
and 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive
baselines by 20.35% and 15.79%, respectively. Furthermore, on the
LongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA
tasks, demonstrating the robustness and generalizability of the proposed
multimodal search strategy.

</details>


### [445] [NS-FPN: Improving Infrared Small Target Detection and Segmentation from Noise Suppression Perspective](https://arxiv.org/abs/2508.06878)
*Maoxun Yuan,Duanni Meng,Ziteng Xi,Tianyi Zhao,Shiji Zhao,Yimian Dai,Xingxing Wei*

Main category: cs.CV

TL;DR: 本文针对红外小目标检测与分割任务，从频域分析问题，提出NS - FPN网络，降低误报率，提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN的方法仅关注增强特征表示来抵消噪声影响，导致误报率增加，需从噪声抑制角度提升红外小目标检测与分割性能。

Method: 提出NS - FPN网络，将LFP模块和SFS模块集成到原始FPN结构中，LFP模块净化高频分量抑制噪声特征，SFS模块采用螺旋采样融合目标相关特征。

Result: 在公共IRSTDS数据集上的大量实验表明，该方法显著降低了误报率。

Conclusion: 所提方法能有效降低误报率，在IRSTDS任务中取得了优异性能，且网络轻量，可轻松集成到现有IRSTDS框架中。

Abstract: Infrared small target detection and segmentation (IRSTDS) is a critical yet
challenging task in defense and civilian applications, owing to the dim,
shapeless appearance of targets and severe background clutter. Recent CNN-based
methods have achieved promising target perception results, but they only focus
on enhancing feature representation to offset the impact of noise, which
results in the increased false alarms problem. In this paper, through analyzing
the problem from the frequency domain, we pioneer in improving performance from
noise suppression perspective and propose a novel noise-suppression feature
pyramid network (NS-FPN), which integrates a low-frequency guided feature
purification (LFP) module and a spiral-aware feature sampling (SFS) module into
the original FPN structure. The LFP module suppresses the noise features by
purifying high-frequency components to achieve feature enhancement devoid of
noise interference, while the SFS module further adopts spiral sampling to fuse
target-relevant features in feature fusion process. Our NS-FPN is designed to
be lightweight yet effective and can be easily plugged into existing IRSTDS
frameworks. Extensive experiments on the public IRSTDS datasets demonstrate
that our method significantly reduces false alarms and achieves superior
performance on IRSTDS tasks.

</details>


### [446] [BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models](https://arxiv.org/abs/2508.06895)
*Jianting Tang,Yubo Wang,Haoyu Cao,Linli Xu*

Main category: cs.CV

TL;DR: 本文指出当前多模态大语言模型视觉对齐方法的不足，提出BASIC方法，无需额外监督模型或人工标注，显著提升了多模态大语言模型在多个基准测试中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型视觉对齐方法仅对文本输出进行自回归监督，忽略了直接视觉监督，阻碍了视觉嵌入的精细对齐。

Method: 提出BASIC方法，利用大语言模型中精炼的视觉嵌入作为监督，从优化嵌入方向和提高语义匹配两个角度指导投影仪生成初始视觉嵌入。

Result: 在多个基准测试中显著提升了多模态大语言模型的性能。

Conclusion: 引入的直接视觉监督是有效的。

Abstract: Mainstream Multimodal Large Language Models (MLLMs) achieve visual
understanding by using a vision projector to bridge well-pretrained vision
encoders and large language models (LLMs). The inherent gap between visual and
textual modalities makes the embeddings from the vision projector critical for
visual comprehension. However, current alignment approaches treat visual
embeddings as contextual cues and merely apply auto-regressive supervision to
textual outputs, neglecting the necessity of introducing equivalent direct
visual supervision, which hinders the potential finer alignment of visual
embeddings. In this paper, based on our analysis of the refinement process of
visual embeddings in the LLM's shallow layers, we propose BASIC, a method that
utilizes refined visual embeddings within the LLM as supervision to directly
guide the projector in generating initial visual embeddings. Specifically, the
guidance is conducted from two perspectives: (i) optimizing embedding
directions by reducing angles between initial and supervisory embeddings in
semantic space; (ii) improving semantic matching by minimizing disparities
between the logit distributions of both visual embeddings. Without additional
supervisory models or artificial annotations, BASIC significantly improves the
performance of MLLMs across a wide range of benchmarks, demonstrating the
effectiveness of our introduced direct visual supervision.

</details>


### [447] [Advancements in Chinese font generation since deep learning era: A survey](https://arxiv.org/abs/2508.06900)
*Weiran Chen,Guiqian Zhu,Ying Li,Yi Ji,Chunping Liu*

Main category: cs.CV

TL;DR: 本文对基于深度学习的中文字体生成方法进行全面调研，分类总结方法并讨论挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 提高生成中文汉字图像的整体质量，解决现有难题，为该领域研究提供参考。

Method: 先阐述研究背景，说明文献选择和分析方法，回顾相关基础知识，按所需参考样本数量将现有方法分为多样本和少样本字体生成方法并总结代表方法及优缺点。

Result: 完成对近期基于深度学习的中文字体生成方法的全面调研。

Conclusion: 指出该领域存在的挑战和未来研究方向，期望为研究者提供有价值的启示。

Abstract: Chinese font generation aims to create a new Chinese font library based on
some reference samples. It is a topic of great concern to many font designers
and typographers. Over the past years, with the rapid development of deep
learning algorithms, various new techniques have achieved flourishing and
thriving progress. Nevertheless, how to improve the overall quality of
generated Chinese character images remains a tough issue. In this paper, we
conduct a holistic survey of the recent Chinese font generation approaches
based on deep learning. To be specific, we first illustrate the research
background of the task. Then, we outline our literature selection and analysis
methodology, and review a series of related fundamentals, including classical
deep learning architectures, font representation formats, public datasets, and
frequently-used evaluation metrics. After that, relying on the number of
reference samples required to generate a new font, we categorize the existing
methods into two major groups: many-shot font generation and few-shot font
generation methods. Within each category, representative approaches are
summarized, and their strengths and limitations are also discussed in detail.
Finally, we conclude our paper with the challenges and future directions, with
the expectation to provide some valuable illuminations for the researchers in
this field.

</details>


### [448] [MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification](https://arxiv.org/abs/2508.06908)
*Jinhao Li,Zijian Chen,Lirong Deng,Changbo Wang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出用于行人重识别的多任务多模态基准MMReID - Bench，实验展示大语言模型能力及局限，望推动相关模型发展。


<details>
  <summary>Details</summary>
Motivation: 传统行人重识别模型单模态能力差，现有多模态大语言模型方法未充分发挥其能力，需新方法。

Method: 引入首个专门用于行人重识别的多任务多模态基准MMReID - Bench，含20,710个多模态查询和图库图像，涵盖10个不同任务。

Result: 实验表明多模态大语言模型在行人重识别上有效且通用，但处理部分模态（如热成像和红外数据）有局限。

Conclusion: 期望MMReID - Bench能推动社区为行人重识别开发更强大、更通用的多模态基础模型。

Abstract: Person re-identification (ReID) aims to retrieve the images of an interested
person in the gallery images, with wide applications in medical rehabilitation,
abnormal behavior detection, and public security. However, traditional person
ReID models suffer from uni-modal capability, leading to poor generalization
ability in multi-modal data, such as RGB, thermal, infrared, sketch images,
textual descriptions, etc. Recently, the emergence of multi-modal large
language models (MLLMs) shows a promising avenue for addressing this problem.
Despite this potential, existing methods merely regard MLLMs as feature
extractors or caption generators, which do not fully unleash their reasoning,
instruction-following, and cross-modal understanding capabilities. To bridge
this gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmark
specifically designed for person ReID. The MMReID-Bench includes 20,710
multi-modal queries and gallery images covering 10 different person ReID tasks.
Comprehensive experiments demonstrate the remarkable capabilities of MLLMs in
delivering effective and versatile person ReID. Nevertheless, they also have
limitations in handling a few modalities, particularly thermal and infrared
data. We hope MMReID-Bench can facilitate the community to develop more robust
and generalizable multimodal foundation models for person ReID.

</details>


### [449] [CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing](https://arxiv.org/abs/2508.06937)
*Weiyan Xie,Han Gao,Didan Deng,Kaican Li,April Hua Liu,Yongxiang Huang,Nevin L. Zhang*

Main category: cs.CV

TL;DR: 提出无训练框架CannyEdit解决文本图像编辑难题并表现优异


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像编辑方法难以平衡编辑区域文本一致性、未编辑区域上下文保真度和编辑无缝集成

Method: 采用选择性Canny控制和双提示引导两项创新技术

Result: 在真实图像编辑任务上优于先前方法，文本一致性和上下文保真度平衡提升2.93 - 10.49%，编辑无缝性上普通用户和专家识别为AI编辑的比例低

Conclusion: CannyEdit能有效解决现有文本图像编辑方法的不足，性能表现良好

Abstract: Recent advances in text-to-image (T2I) models have enabled training-free
regional image editing by leveraging the generative priors of foundation
models. However, existing methods struggle to balance text adherence in edited
regions, context fidelity in unedited areas, and seamless integration of edits.
We introduce CannyEdit, a novel training-free framework that addresses these
challenges through two key innovations: (1) Selective Canny Control, which
masks the structural guidance of Canny ControlNet in user-specified editable
regions while strictly preserving details of the source images in unedited
areas via inversion-phase ControlNet information retention. This enables
precise, text-driven edits without compromising contextual integrity. (2)
Dual-Prompt Guidance, which combines local prompts for object-specific edits
with a global target prompt to maintain coherent scene interactions. On
real-world image editing tasks (addition, replacement, removal), CannyEdit
outperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent
improvement in the balance of text adherence and context fidelity. In terms of
editing seamlessness, user studies reveal only 49.2 percent of general users
and 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited
when paired with real images without edits, versus 76.08 to 89.09 percent for
competitor methods.

</details>


### [450] [Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification](https://arxiv.org/abs/2508.06959)
*Qin Xu,Lili Zhu,Xiaoxia Cheng,Bo Jiang*

Main category: cs.CV

TL;DR: 提出用于细粒度视觉分类的SCOPE方法，含SDE和SSR模块，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有频域方法基于固定基函数，缺乏对图像内容适应性，无法按需动态调整特征提取。

Method: 提出SCOPE方法，含动态增强浅层特征细节的SDE模块和学习高层特征细化特征的SSR模块，二者级联结合局部细节与全局语义。

Result: 在四个流行细粒度图像分类基准上达到新的SOTA。

Conclusion: SCOPE方法有效突破频域固定尺度限制，提高多尺度融合灵活性。

Abstract: The crux of resolving fine-grained visual classification (FGVC) lies in
capturing discriminative and class-specific cues that correspond to subtle
visual characteristics. Recently, frequency decomposition/transform based
approaches have attracted considerable interests since its appearing
discriminative cue mining ability. However, the frequency-domain methods are
based on fixed basis functions, lacking adaptability to image content and
unable to dynamically adjust feature extraction according to the discriminative
requirements of different images. To address this, we propose a novel method
for FGVC, named Subtle-Cue Oriented Perception Engine (SCOPE), which adaptively
enhances the representational capability of low-level details and high-level
semantics in the spatial domain, breaking through the limitations of fixed
scales in the frequency domain and improving the flexibility of multi-scale
fusion. The core of SCOPE lies in two modules: the Subtle Detail Extractor
(SDE), which dynamically enhances subtle details such as edges and textures
from shallow features, and the Salient Semantic Refiner (SSR), which learns
semantically coherent and structure-aware refinement features from the
high-level features guided by the enhanced shallow features. The SDE and SSR
are cascaded stage-by-stage to progressively combine local details with global
semantics. Extensive experiments demonstrate that our method achieves new
state-of-the-art on four popular fine-grained image classification benchmarks.

</details>


### [451] [WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering](https://arxiv.org/abs/2508.06982)
*Yixin Zhu,Zuoliang Zhu,Miloš Hašan,Jian Yang,Jin Xie,Beibei Wang*

Main category: cs.CV

TL;DR: 提出WeatherDiffusion框架用于自动驾驶场景的正反渲染，引入新数据集，实验表明其性能优于SOTA方法，对下游任务有价值。


<details>
  <summary>Details</summary>
Motivation: 复杂天气和光照给自动驾驶的正反渲染带来挑战，现有大扩散模型难控制且缺乏鲁棒性。

Method: 提出WeatherDiffusion框架，使用预测的固有图实现可控的天气和光照编辑，提出固有图感知注意力机制，引入合成和真实数据集。

Result: WeatherDiffusion在多个基准测试中优于现有方法，提升了恶劣天气场景下目标检测和图像分割的鲁棒性。

Conclusion: WeatherDiffusion框架能有效应对自动驾驶场景中复杂天气和光照下的正反渲染问题，对下游任务有重要价值。

Abstract: Forward and inverse rendering have emerged as key techniques for enabling
understanding and reconstruction in the context of autonomous driving (AD).
However, complex weather and illumination pose great challenges to this task.
The emergence of large diffusion models has shown promise in achieving
reasonable results through learning from 2D priors, but these models are
difficult to control and lack robustness. In this paper, we introduce
WeatherDiffusion, a diffusion-based framework for forward and inverse rendering
on AD scenes with various weather and lighting conditions. Our method enables
authentic estimation of material properties, scene geometry, and lighting, and
further supports controllable weather and illumination editing through the use
of predicted intrinsic maps guided by text descriptions. We observe that
different intrinsic maps should correspond to different regions of the original
image. Based on this observation, we propose Intrinsic map-aware attention
(MAA) to enable high-quality inverse rendering. Additionally, we introduce a
synthetic dataset (\ie WeatherSynthetic) and a real-world dataset (\ie
WeatherReal) for forward and inverse rendering on AD scenes with diverse
weather and lighting. Extensive experiments show that our WeatherDiffusion
outperforms state-of-the-art methods on several benchmarks. Moreover, our
method demonstrates significant value in downstream tasks for AD, enhancing the
robustness of object detection and image segmentation in challenging weather
scenarios.

</details>


### [452] [RMT-PPAD: Real-time Multi-task Learning for Panoptic Perception in Autonomous Driving](https://arxiv.org/abs/2508.06529)
*Jiayuan Wang,Q. M. Jonathan Wu,Katsuya Suto,Ning Zhang*

Main category: cs.CV

TL;DR: 提出实时多任务模型RMT - PPAD进行全景驾驶感知，在BDD100K数据集实验效果好，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要兼具精度和实时性的全景驾驶感知。

Method: 提出RMT - PPAD模型，引入轻量级模块自适应融合特征，设计自适应分割解码器，解决车道线分割训练和测试标签不一致问题。

Result: 在BDD100K数据集上各任务取得先进结果，推理速度达32.6 FPS，在真实场景性能稳定。

Conclusion: RMT - PPAD模型能有效用于全景驾驶感知，有良好性能。

Abstract: Autonomous driving systems rely on panoptic driving perception that requires
both precision and real-time performance. In this work, we propose RMT-PPAD, a
real-time, transformer-based multi-task model that jointly performs object
detection, drivable area segmentation, and lane line segmentation. We introduce
a lightweight module, a gate control with an adapter to adaptively fuse shared
and task-specific features, effectively alleviating negative transfer between
tasks. Additionally, we design an adaptive segmentation decoder to learn the
weights over multi-scale features automatically during the training stage. This
avoids the manual design of task-specific structures for different segmentation
tasks. We also identify and resolve the inconsistency between training and
testing labels in lane line segmentation. This allows fairer evaluation.
Experiments on the BDD100K dataset demonstrate that RMT-PPAD achieves
state-of-the-art results with mAP50 of 84.9% and Recall of 95.4% for object
detection, mIoU of 92.6% for drivable area segmentation, and IoU of 56.8% and
accuracy of 84.7% for lane line segmentation. The inference speed reaches 32.6
FPS. Moreover, we introduce real-world scenarios to evaluate RMT-PPAD
performance in practice. The results show that RMT-PPAD consistently delivers
stable performance. The source codes and pre-trained models are released at
https://github.com/JiayuanWang-JW/RMT-PPAD.

</details>


### [453] [What Makes "Good" Distractors for Object Hallucination Evaluation in Large Vision-Language Models?](https://arxiv.org/abs/2508.06530)
*Ming-Kun Xie,Jia-Hao Xiao,Gang Niu,Lei Feng,Zhiqiang Kou,Min-Ling Zhang,Masashi Sugiyama*

Main category: cs.CV

TL;DR: 现有大视觉语言模型（LVLMs）存在物体幻觉问题，常用的POPE基准评估效果渐弱，本文提出HOPE基准，实验显示它能更好暴露模型幻觉漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有POPE基准在评估LVLMs物体幻觉问题上效果渐弱，需新评估方法。

Method: 引入Hallucination searching - based Object Probing Evaluation (HOPE) 基准，利用CLIP进行内容感知幻觉搜索，通过配对真假描述进行基于描述的幻觉搜索。

Result: HOPE使各种最先进的LVLMs精度至少下降9%，最多下降23%，显著优于POPE。

Conclusion: HOPE基准在暴露LVLMs幻觉漏洞方面表现出色，代码已开源。

Abstract: Large Vision-Language Models (LVLMs), empowered by the success of Large
Language Models (LLMs), have achieved impressive performance across domains.
Despite the great advances in LVLMs, they still suffer from the unavailable
object hallucination issue, which tends to generate objects inconsistent with
the image content. The most commonly used Polling-based Object Probing
Evaluation (POPE) benchmark evaluates this issue by sampling negative
categories according to category-level statistics, \textit{e.g.}, category
frequencies and co-occurrence. However, with the continuous advancement of
LVLMs, the POPE benchmark has shown diminishing effectiveness in assessing
object hallucination, as it employs a simplistic sampling strategy that
overlooks image-specific information and restricts distractors to negative
object categories only. In this paper, we introduce the Hallucination
searching-based Object Probing Evaluation (HOPE) benchmark, aiming to generate
the most misleading distractors (\textit{i.e.}, non-existent objects or
incorrect image descriptions) that can trigger hallucination in LVLMs, which
serves as a means to more rigorously assess their immunity to hallucination. To
explore the image-specific information, the content-aware hallucination
searching leverages Contrastive Language-Image Pre-Training (CLIP) to
approximate the predictive behavior of LVLMs by selecting negative objects with
the highest predicted likelihood as distractors. To expand the scope of
hallucination assessment, the description-based hallucination searching
constructs highly misleading distractors by pairing true objects with false
descriptions. Experimental results show that HOPE leads to a precision drop of
at least 9\% and up to 23\% across various state-of-the-art LVLMs,
significantly outperforming POPE in exposing hallucination vulnerabilities. The
code is available at https://github.com/xiemk/HOPE.

</details>


### [454] [Age-Diverse Deepfake Dataset: Bridging the Age Gap in Deepfake Detection](https://arxiv.org/abs/2508.06552)
*Unisha Joshi*

Main category: cs.CV

TL;DR: 本文针对深度伪造数据集的年龄偏差问题，构建年龄多样化数据集，经评估该数据集可提升模型公平性、准确性和泛化能力，还提供了数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测模型未解决数据集的人口统计学偏差问题，尤其是年龄偏差。

Method: 构建模块化管道，整合现有深度伪造数据集并创建合成数据，以填补年龄分布差距；用XceptionNet、EfficientNet和LipForensics三种模型评估数据集效果。

Result: 在年龄多样化数据集上训练的模型在各年龄组表现更公平，整体准确率提高，跨数据集泛化性增强。

Conclusion: 本研究提供了可重现、注重公平性的深度伪造数据集和模型管道，为未来公平的深度伪造检测研究奠定基础。

Abstract: The challenges associated with deepfake detection are increasing
significantly with the latest advancements in technology and the growing
popularity of deepfake videos and images. Despite the presence of numerous
detection models, demographic bias in the deepfake dataset remains largely
unaddressed. This paper focuses on the mitigation of age-specific bias in the
deepfake dataset by introducing an age-diverse deepfake dataset that will
improve fairness across age groups. The dataset is constructed through a
modular pipeline incorporating the existing deepfake datasets Celeb-DF,
FaceForensics++, and UTKFace datasets, and the creation of synthetic data to
fill the age distribution gaps. The effectiveness and generalizability of this
dataset are evaluated using three deepfake detection models: XceptionNet,
EfficientNet, and LipForensics. Evaluation metrics, including AUC, pAUC, and
EER, revealed that models trained on the age-diverse dataset demonstrated
fairer performance across age groups, improved overall accuracy, and higher
generalization across datasets. This study contributes a reproducible,
fairness-aware deepfake dataset and model pipeline that can serve as a
foundation for future research in fairer deepfake detection. The complete
dataset and implementation code are available at
https://github.com/unishajoshi/age-diverse-deepfake-detection.

</details>


### [455] [From Label Error Detection to Correction: A Modular Framework and Benchmark for Object Detection Datasets](https://arxiv.org/abs/2508.06556)
*Sarina Penquitt,Jonathan Klees,Rinor Cakaj,Daniel Kondermann,Matthias Rottmann,Lars Schmarje*

Main category: cs.CV

TL;DR: 文章介绍了用于目标检测数据集标签错误纠正的半自动化框架REC√D，以KITTI数据集行人类别为例验证其有效性，指出当前标签错误检测方法存在不足，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测数据集存在标签错误影响训练和评估结果，且缺乏系统大规模纠正错误的方法。

Method: 引入半自动化框架REC√D，结合现有检测器的错误提议和众包微任务，让多个标注者独立验证候选边界框并汇总响应。

Result: 对KITTI数据集行人类别应用框架得到高质量修正标注，原标注中至少24%缺失和不准确；当前标签错误检测方法结合框架能快速发现错误，但仍会遗漏大量错误且低质量标签会引入更多错误。

Conclusion: 需要基于发布的基准进行进一步研究。

Abstract: Object detection has advanced rapidly in recent years, driven by increasingly
large and diverse datasets. However, label errors, defined as missing labels,
incorrect classification or inaccurate localization, often compromise the
quality of these datasets. This can have a significant impact on the outcomes
of training and benchmark evaluations. Although several methods now exist for
detecting label errors in object detection datasets, they are typically
validated only on synthetic benchmarks or limited manual inspection. How to
correct such errors systemically and at scale therefore remains an open
problem. We introduce a semi-automated framework for label-error correction
called REC$\checkmark$D (Rechecked). Building on existing detectors, the
framework pairs their error proposals with lightweight, crowd-sourced
microtasks. These tasks enable multiple annotators to independently verify each
candidate bounding box, and their responses are aggregated to estimate
ambiguity and improve label quality. To demonstrate the effectiveness of
REC$\checkmark$D, we apply it to the class pedestrian in the KITTI dataset. Our
crowdsourced review yields high-quality corrected annotations, which indicate a
rate of at least 24% of missing and inaccurate annotations in original
annotations. This validated set will be released as a new real-world benchmark
for label error detection and correction. We show that current label error
detection methods, when combined with our correction framework, can recover
hundreds of errors in the time it would take a human to annotate bounding boxes
from scratch. However, even the best methods still miss up to 66% of the true
errors and with low quality labels introduce more errors than they find. This
highlights the urgent need for further research, now enabled by our released
benchmark.

</details>


### [456] [On the effectiveness of multimodal privileged knowledge distillation in two vision transformer based diagnostic applications](https://arxiv.org/abs/2508.06558)
*Simon Baur,Alexandra Benova,Emilio Dolgener Cantú,Jackie Ma*

Main category: cs.CV

TL;DR: 提出多模态特权知识蒸馏（MMPKD）策略，用额外模态指导单模态视觉模型，可提升注意力图零样本定位能力，但效果因领域而异。


<details>
  <summary>Details</summary>
Motivation: 临床实践部署深度学习模型需多模态数据，但推理时并非所有模态都可用，需利用训练时额外模态指导单模态模型。

Method: 提出MMPKD策略，用基于文本的教师模型和基于表格元数据的教师模型将知识蒸馏到视觉变压器学生模型。

Result: MMPKD能提高注意力图在输入图像中零样本定位感兴趣区域（ROI）的能力。

Conclusion: MMPKD效果在不同领域无法泛化，与先前研究不同。

Abstract: Deploying deep learning models in clinical practice often requires leveraging
multiple data modalities, such as images, text, and structured data, to achieve
robust and trustworthy decisions. However, not all modalities are always
available at inference time. In this work, we propose multimodal privileged
knowledge distillation (MMPKD), a training strategy that utilizes additional
modalities available solely during training to guide a unimodal vision model.
Specifically, we used a text-based teacher model for chest radiographs
(MIMIC-CXR) and a tabular metadata-based teacher model for mammography
(CBIS-DDSM) to distill knowledge into a vision transformer student model. We
show that MMPKD can improve the resulting attention maps' zero-shot
capabilities of localizing ROI in input images, while this effect does not
generalize across domains, as contrarily suggested by prior research.

</details>


### [457] [Bridging Brain Connectomes and Clinical Reports for Early Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06565)
*Jing Zhang,Xiaowei Yu,Minheng Chen,Lu Zhang,Tong Chen,Yan Zhuang,Chao Cao,Yanjun Lyu,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.CV

TL;DR: 本文提出新框架将脑连接组与临床报告在跨模态潜在空间对齐，应用于MCI取得良好效果，为阿尔茨海默病早期机制提供新见解。


<details>
  <summary>Details</summary>
Motivation: 解决将客观成像数据与主观文本报告有效关联的挑战，以利用多模态信息进行更有效的临床诊断。

Method: 提出新框架，在主体和连接组层面将脑连接组与临床报告在共享跨模态潜在空间对齐，将脑子网视为成像数据的标记。

Result: 应用于MCI取得了最先进的预测性能，识别出有临床意义的连接组 - 文本对。

Conclusion: 该方法为阿尔茨海默病早期机制提供新见解，支持开发临床有用的多模态生物标志物。

Abstract: Integrating brain imaging data with clinical reports offers a valuable
opportunity to leverage complementary multimodal information for more effective
and timely diagnosis in practical clinical settings. This approach has gained
significant attention in brain disorder research, yet a key challenge remains:
how to effectively link objective imaging data with subjective text-based
reports, such as doctors' notes. In this work, we propose a novel framework
that aligns brain connectomes with clinical reports in a shared cross-modal
latent space at both the subject and connectome levels, thereby enhancing
representation learning. The key innovation of our approach is that we treat
brain subnetworks as tokens of imaging data, rather than raw image patches, to
align with word tokens in clinical reports. This enables a more efficient
identification of system-level associations between neuroimaging findings and
clinical observations, which is critical since brain disorders often manifest
as network-level abnormalities rather than isolated regional alterations. We
applied our method to mild cognitive impairment (MCI) using the ADNI dataset.
Our approach not only achieves state-of-the-art predictive performance but also
identifies clinically meaningful connectome-text pairs, offering new insights
into the early mechanisms of Alzheimer's disease and supporting the development
of clinically useful multimodal biomarkers.

</details>


### [458] [ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos](https://arxiv.org/abs/2508.06570)
*Mohammad Zia Ur Rehman,Anukriti Bhatnagar,Omkar Kabde,Shubhi Bansal,Nagendra Kumar*

Main category: cs.CV

TL;DR: 提出新视频数据集 ImpliHateVid 用于隐式仇恨言论检测，提出两阶段对比学习框架，在两个数据集评估证明方法和数据集有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦文本和图像的仇恨言论检测，基于视频的方法未充分探索。

Method: 引入 ImpliHateVid 数据集，提出两阶段对比学习框架，结合多模态特征。第一阶段训练模态特定编码器，第二阶段训练交叉编码器，融入情感、情绪和基于字幕的特征。

Result: 在 ImpliHateVid 和 HateMM 数据集评估，证明多模态对比学习用于视频仇恨内容检测有效。

Conclusion: 提出的多模态对比学习方法对视频仇恨内容检测有效，数据集有重要意义。

Abstract: The existing research has primarily focused on text and image-based hate
speech detection, video-based approaches remain underexplored. In this work, we
introduce a novel dataset, ImpliHateVid, specifically curated for implicit hate
speech detection in videos. ImpliHateVid consists of 2,009 videos comprising
509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos,
making it one of the first large-scale video datasets dedicated to implicit
hate detection. We also propose a novel two-stage contrastive learning
framework for hate speech detection in videos. In the first stage, we train
modality-specific encoders for audio, text, and image using contrastive loss by
concatenating features from the three encoders. In the second stage, we train
cross-encoders using contrastive learning to refine multimodal representations.
Additionally, we incorporate sentiment, emotion, and caption-based features to
enhance implicit hate detection. We evaluate our method on two datasets,
ImpliHateVid for implicit hate speech detection and another dataset for general
hate speech detection in videos, HateMM dataset, demonstrating the
effectiveness of the proposed multimodal contrastive learning for hateful
content detection in videos and the significance of our dataset.

</details>


### [459] [Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays](https://arxiv.org/abs/2508.07128)
*Gregory Schuit,Denis Parra,Cecilia Besa*

Main category: cs.CV

TL;DR: 评估GANs和DMs合成胸部X光片效果，发现各有优势，需进一步改进。


<details>
  <summary>Details</summary>
Motivation: 生成式图像模型可解决医学数据稀缺问题，但合成图像保真度和临床实用性存疑，需评估其效果。

Method: 用MIMIC - CXR数据集真实图像和GANs、DMs合成图像构成基准，让三位不同经验放射科医生区分真假图像并评估视觉特征与目标异常的一致性。

Result: DMs生成图像整体更逼真，GANs在特定情况（如无ECS）下准确率更高，还找出放射科医生检测合成图像的视觉线索。

Conclusion: GANs和DMs优势互补，需进一步改进以可靠扩充AI诊断系统训练数据集。

Abstract: Generative image models have achieved remarkable progress in both natural and
medical imaging. In the medical context, these techniques offer a potential
solution to data scarcity-especially for low-prevalence anomalies that impair
the performance of AI-driven diagnostic and segmentation tools. However,
questions remain regarding the fidelity and clinical utility of synthetic
images, since poor generation quality can undermine model generalizability and
trust. In this study, we evaluate the effectiveness of state-of-the-art
generative models-Generative Adversarial Networks (GANs) and Diffusion Models
(DMs)-for synthesizing chest X-rays conditioned on four abnormalities:
Atelectasis (AT), Lung Opacity (LO), Pleural Effusion (PE), and Enlarged
Cardiac Silhouette (ECS). Using a benchmark composed of real images from the
MIMIC-CXR dataset and synthetic images from both GANs and DMs, we conducted a
reader study with three radiologists of varied experience. Participants were
asked to distinguish real from synthetic images and assess the consistency
between visual features and the target abnormality. Our results show that while
DMs generate more visually realistic images overall, GANs can report better
accuracy for specific conditions, such as absence of ECS. We further identify
visual cues radiologists use to detect synthetic images, offering insights into
the perceptual gaps in current models. These findings underscore the
complementary strengths of GANs and DMs and point to the need for further
refinement to ensure generative models can reliably augment training datasets
for AI diagnostic systems.

</details>


### [460] [Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2508.07146)
*Yu Liu,Zhijie Liu,Xiao Ren,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出含短长期意图的扩散式行人轨迹预测框架，在基准测试有竞争力。


<details>
  <summary>Details</summary>
Motivation: 许多基于扩散的行人轨迹预测方法缺乏对行人意图的显式语义建模，导致行为误判和精度降低。

Method: 提出含短长期运动意图的扩散式预测框架，短期意图用残差极坐标表示，长期意图用可学习的基于令牌的终点预测器估计，还通过自适应引导和残差噪声预测器增强扩散过程。

Result: 在ETH、UCY和SDD基准测试上评估，结果优于现有方法。

Conclusion: 所提框架能有效解决现有扩散方法的问题，有较好预测性能。

Abstract: Predicting pedestrian motion trajectories is critical for the path planning
and motion control of autonomous vehicles. Recent diffusion-based models have
shown promising results in capturing the inherent stochasticity of pedestrian
behavior for trajectory prediction. However, the absence of explicit semantic
modelling of pedestrian intent in many diffusion-based methods may result in
misinterpreted behaviors and reduced prediction accuracy. To address the above
challenges, we propose a diffusion-based pedestrian trajectory prediction
framework that incorporates both short-term and long-term motion intentions.
Short-term intent is modelled using a residual polar representation, which
decouples direction and magnitude to capture fine-grained local motion
patterns. Long-term intent is estimated through a learnable, token-based
endpoint predictor that generates multiple candidate goals with associated
probabilities, enabling multimodal and context-aware intention modelling.
Furthermore, we enhance the diffusion process by incorporating adaptive
guidance and a residual noise predictor that dynamically refines denoising
accuracy. The proposed framework is evaluated on the widely used ETH, UCY, and
SDD benchmarks, demonstrating competitive results against state-of-the-art
methods.

</details>


### [461] [Lightweight Multi-Scale Feature Extraction with Fully Connected LMF Layer for Salient Object Detection](https://arxiv.org/abs/2508.07170)
*Yunpeng Shi,Lei Chen,Xiaolu Shen,Yanju Guo*

Main category: cs.CV

TL;DR: 本文提出LMF层和LMFNet用于显著目标检测，减少参数同时保持性能，在五个基准数据集上取得好结果。


<details>
  <summary>Details</summary>
Motivation: 解决轻量级网络中多尺度特征提取在效率和性能间的平衡问题。

Method: 提出LMF层，采用深度可分离扩张卷积的全连接结构，集成多个LMF层构建LMFNet。

Result: LMFNet仅0.81M参数，在五个基准数据集上达到SOTA或相当结果，在效率和准确性上超越多个传统和轻量级模型。

Conclusion: 解决轻量级网络多尺度学习挑战，展示在图像处理任务中的广泛应用潜力。

Abstract: In the domain of computer vision, multi-scale feature extraction is vital for
tasks such as salient object detection. However, achieving this capability in
lightweight networks remains challenging due to the trade-off between
efficiency and performance. This paper proposes a novel lightweight multi-scale
feature extraction layer, termed the LMF layer, which employs depthwise
separable dilated convolutions in a fully connected structure. By integrating
multiple LMF layers, we develop LMFNet, a lightweight network tailored for
salient object detection. Our approach significantly reduces the number of
parameters while maintaining competitive performance. Here, we show that LMFNet
achieves state-of-the-art or comparable results on five benchmark datasets with
only 0.81M parameters, outperforming several traditional and lightweight models
in terms of both efficiency and accuracy. Our work not only addresses the
challenge of multi-scale learning in lightweight networks but also demonstrates
the potential for broader applications in image processing tasks. The related
code files are available at https://github.com/Shi-Yun-peng/LMFNet

</details>


### [462] [Representation Understanding via Activation Maximization](https://arxiv.org/abs/2508.07281)
*Hongbo Zhu,Angelo Cangelosi*

Main category: cs.CV

TL;DR: 提出适用于CNN和ViT的统一特征可视化框架，将特征可视化拓展到中间层，还研究用激活最大化生成对抗样本，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 理解深度神经网络内部特征表示，实现模型可解释性。

Method: 提出统一特征可视化框架，将特征可视化拓展到中间层，研究用激活最大化生成对抗样本。

Result: 实验表明该方法在传统CNN和现代ViT中均有效。

Conclusion: 该方法具有通用性和解释价值。

Abstract: Understanding internal feature representations of deep neural networks (DNNs)
is a fundamental step toward model interpretability. Inspired by neuroscience
methods that probe biological neurons using visual stimuli, recent deep
learning studies have employed Activation Maximization (AM) to synthesize
inputs that elicit strong responses from artificial neurons. In this work, we
propose a unified feature visualization framework applicable to both
Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Unlike
prior efforts that predominantly focus on the last output-layer neurons in
CNNs, we extend feature visualization to intermediate layers as well, offering
deeper insights into the hierarchical structure of learned feature
representations. Furthermore, we investigate how activation maximization can be
leveraged to generate adversarial examples, revealing potential vulnerabilities
and decision boundaries of DNNs. Our experiments demonstrate the effectiveness
of our approach in both traditional CNNs and modern ViT, highlighting its
generalizability and interpretive value.

</details>


### [463] [TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders](https://arxiv.org/abs/2508.07020)
*Tanjim Bin Faruk,Abdul Matin,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: 提出TerraMAE用于高光谱图像编码，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 自监督掩码自编码器难以利用200+波段高光谱图像复杂的空间 - 光谱相关性，需新框架用于地理空间分析。

Method: 引入TerraMAE框架，有基于统计反射特性的自适应通道分组策略和结合空间与光谱质量指标的增强重建损失函数。

Result: TerraMAE在高保真图像重建中能更好地保留空间 - 光谱信息，在作物识别、土地覆盖分类和土壤质地预测三个关键下游地理空间任务中表现出色。

Conclusion: TerraMAE能学习具有高度代表性的空间 - 光谱嵌入，适用于多种地理空间分析。

Abstract: Hyperspectral satellite imagery offers sub-30 m views of Earth in hundreds of
contiguous spectral bands, enabling fine-grained mapping of soils, crops, and
land cover. While self-supervised Masked Autoencoders excel on RGB and low-band
multispectral data, they struggle to exploit the intricate spatial-spectral
correlations in 200+ band hyperspectral images. We introduce TerraMAE, a novel
HSI encoding framework specifically designed to learn highly representative
spatial-spectral embeddings for diverse geospatial analyses. TerraMAE features
an adaptive channel grouping strategy, based on statistical reflectance
properties to capture spectral similarities, and an enhanced reconstruction
loss function that incorporates spatial and spectral quality metrics. We
demonstrate TerraMAE's effectiveness through superior spatial-spectral
information preservation in high-fidelity image reconstruction. Furthermore, we
validate its practical utility and the quality of its learned representations
through strong performance on three key downstream geospatial tasks: crop
identification, land cover classification, and soil texture prediction.

</details>


### [464] [DragonFruitQualityNet: A Lightweight Convolutional Neural Network for Real-Time Dragon Fruit Quality Inspection on Mobile Devices](https://arxiv.org/abs/2508.07306)
*Md Zahurul Haquea,Yeahyea Sarker,Muhammed Farhan Sadique Mahi,Syed Jubayer Jaman,Md Robiul Islam*

Main category: cs.CV

TL;DR: 提出轻量级CNN模型DragonFruitQualityNet用于火龙果实时质量评估，准确率93.98%，嵌入手机应用，为火龙果质量控制提供AI解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着火龙果种植扩大，高效的采前和采后质量检测对提高农业生产力和减少采后损失至关重要。

Method: 创建包含13789张图像的多样化数据集，将图像分为新鲜、未成熟、成熟和有缺陷四类；提出轻量级CNN模型DragonFruitQualityNet进行训练。

Result: 模型准确率达93.98%，优于现有水果质量分类方法；将模型嵌入手机应用。

Conclusion: 研究为火龙果质量控制提供准确、高效且可扩展的AI解决方案，促进采后管理和可持续农业实践。

Abstract: Dragon fruit, renowned for its nutritional benefits and economic value, has
experienced rising global demand due to its affordability and local
availability. As dragon fruit cultivation expands, efficient pre- and
post-harvest quality inspection has become essential for improving agricultural
productivity and minimizing post-harvest losses. This study presents
DragonFruitQualityNet, a lightweight Convolutional Neural Network (CNN)
optimized for real-time quality assessment of dragon fruits on mobile devices.
We curated a diverse dataset of 13,789 images, integrating self-collected
samples with public datasets (dataset from Mendeley Data), and classified them
into four categories: fresh, immature, mature, and defective fruits to ensure
robust model training. The proposed model achieves an impressive 93.98%
accuracy, outperforming existing methods in fruit quality classification. To
facilitate practical adoption, we embedded the model into an intuitive mobile
application, enabling farmers and agricultural stakeholders to conduct
on-device, real-time quality inspections. This research provides an accurate,
efficient, and scalable AI-driven solution for dragon fruit quality control,
supporting digital agriculture and empowering smallholder farmers with
accessible technology. By bridging the gap between research and real-world
application, our work advances post-harvest management and promotes sustainable
farming practices.

</details>


### [465] [MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark](https://arxiv.org/abs/2508.07307)
*Haiyang Guo,Fei Zhu,Hongbo Zhao,Fanhu Zeng,Wenzhuo Liu,Shijie Ma,Da-Han Wang,Xu-Yao Zhang*

Main category: cs.CV

TL;DR: 介绍用于多模态大语言模型持续指令调优的代码库MCITlib，含8种算法，在2个基准上评估，代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统单模态持续学习方法成功，但多模态持续学习受关注，需应对跨模态交互等挑战，推动该方向研究。

Method: 引入MCITlib代码库，实现8种多模态持续指令调优算法，并在2个精心挑选的基准上进行系统评估。

Result: 完成MCITlib代码库搭建，实现8种算法并完成评估，代码库会持续更新。

Conclusion: MCITlib能推动多模态持续学习领域的研究。

Abstract: Continual learning aims to equip AI systems with the ability to continuously
acquire and adapt to new knowledge without forgetting previously learned
information, similar to human learning. While traditional continual learning
methods focusing on unimodal tasks have achieved notable success, the emergence
of Multimodal Large Language Models has brought increasing attention to
Multimodal Continual Learning tasks involving multiple modalities, such as
vision and language. In this setting, models are expected to not only mitigate
catastrophic forgetting but also handle the challenges posed by cross-modal
interactions and coordination. To facilitate research in this direction, we
introduce MCITlib, a comprehensive and constantly evolving code library for
continual instruction tuning of Multimodal Large Language Models. In MCITlib,
we have currently implemented 8 representative algorithms for Multimodal
Continual Instruction Tuning and systematically evaluated them on 2 carefully
selected benchmarks. MCITlib will be continuously updated to reflect advances
in the Multimodal Continual Learning field. The codebase is released at
https://github.com/Ghy0501/MCITlib.

</details>


### [466] [AugLift: Boosting Generalization in Lifting-based 3D Human Pose Estimation](https://arxiv.org/abs/2508.07112)
*Nikolai Warner,Wenjin Zhang,Irfan Essa,Apaar Sadhwani*

Main category: cs.CV

TL;DR: 提出AugLift方法改善3D人体姿态估计提升方法泛化性，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于提升的3D人体姿态估计方法在新数据集和现实场景泛化性差。

Method: AugLift通过添加关键点检测置信度分数和深度估计丰富标准输入，信号由预训练模型计算，可集成到现有提升架构。

Result: 在四个数据集实验显示，AugLift使未见数据集跨数据集性能平均提升10.1%，分布内性能提升4.0%。

Conclusion: 稀疏、关键点对齐的线索能提供稳健的帧级上下文，可显著改善提升式姿态估计模型泛化性。

Abstract: Lifting-based methods for 3D Human Pose Estimation (HPE), which predict 3D
poses from detected 2D keypoints, often generalize poorly to new datasets and
real-world settings. To address this, we propose \emph{AugLift}, a simple yet
effective reformulation of the standard lifting pipeline that significantly
improves generalization performance without requiring additional data
collection or sensors. AugLift sparsely enriches the standard input -- the 2D
keypoint coordinates $(x, y)$ -- by augmenting it with a keypoint detection
confidence score $c$ and a corresponding depth estimate $d$. These additional
signals are computed from the image using off-the-shelf, pre-trained models
(e.g., for monocular depth estimation), thereby inheriting their strong
generalization capabilities. Importantly, AugLift serves as a modular add-on
and can be readily integrated into existing lifting architectures.
  Our extensive experiments across four datasets demonstrate that AugLift
boosts cross-dataset performance on unseen datasets by an average of $10.1\%$,
while also improving in-distribution performance by $4.0\%$. These gains are
consistent across various lifting architectures, highlighting the robustness of
our method. Our analysis suggests that these sparse, keypoint-aligned cues
provide robust frame-level context, offering a practical way to significantly
improve the generalization of any lifting-based pose estimation model. Code
will be made publicly available.

</details>


### [467] [Freeze and Reveal: Exposing Modality Bias in Vision-Language Models](https://arxiv.org/abs/2508.07432)
*Vivek Hruday Kavuri,Vysishtya Karanam,Venkata Jahnavi Venkamsetty,Kriti Madumadukala,Lakshmipathi Balaji Darur,Ponnurangam Kumaraguru*

Main category: cs.CV

TL;DR: 本文通过特定方法剖析视觉语言模型中视觉和文本骨干对性别偏差的贡献，引入新指标和去偏方法，评估后发现方法能减少偏差并识别主要偏差源，利于未来多模态系统去偏。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多模态表现出色，但会从训练数据继承性别偏差，需剖析视觉和文本骨干对偏差的贡献并减少偏差。

Method: 应用反事实数据增强和任务向量方法进行有针对性的去偏，引入Degree of Stereotypicality指标和DAUDoS去偏方法，整理性别标注数据集并在VisoGender基准上评估。

Result: CDA使性别差距减少6%，DAUDoS减少3%且只用三分之一数据，两种方法使模型正确识别图像中性别的能力提高3%，DAUDoS用约三分之一训练数据达成，发现CLIP视觉编码器和PaliGemma2文本编码器更有偏差。

Conclusion: 通过识别偏差主要来源，利于未来多模态系统制定更有针对性和有效的去偏策略。

Abstract: Vision Language Models achieve impressive multi-modal performance but often
inherit gender biases from their training data. This bias might be coming from
both the vision and text modalities. In this work, we dissect the contributions
of vision and text backbones to these biases by applying targeted debiasing
using Counterfactual Data Augmentation and Task Vector methods. Inspired by
data-efficient approaches in hate-speech classification, we introduce a novel
metric, Degree of Stereotypicality and a corresponding debiasing method, Data
Augmentation Using Degree of Stereotypicality - DAUDoS, to reduce bias with
minimal computational cost. We curate a gender annotated dataset and evaluate
all methods on VisoGender benchmark to quantify improvements and identify
dominant source of bias. Our results show that CDA reduces the gender gap by 6%
and DAUDoS by 3% but using only one-third of the data. Both methods also
improve the model's ability to correctly identify gender in images by 3%, with
DAUDoS achieving this improvement using only almost one-third of training data.
From our experiment's, we observed that CLIP's vision encoder is more biased
whereas PaliGemma2's text encoder is more biased. By identifying whether bias
stems more from vision or text encoders, our work enables more targeted and
effective bias mitigation strategies in future multi-modal systems.

</details>


### [468] [From Field to Drone: Domain Drift Tolerant Automated Multi-Species and Damage Plant Semantic Segmentation for Herbicide Trials](https://arxiv.org/abs/2508.07514)
*Artzai Picon,Itziar Eguskiza,Daniel Mugica,Javier Romero,Carlos Javier Jimenez,Eric White,Gabriel Do-Lago-Junqueira,Christian Klukas,Ramon Navarra-Mestre*

Main category: cs.CV

TL;DR: 本文提出改进的分割模型用于除草剂研究中作物和杂草的物种与损伤识别，模型表现优异且已部署应用。


<details>
  <summary>Details</summary>
Motivation: 传统除草剂研究的人工评估方式耗时、费力且主观，自动识别有挑战但可提升效率和一致性。

Method: 结合通用自监督视觉模型和基于植物分类学的分层推理构建改进的分割模型，在多年多地区数据集上训练，跨设备测试。

Result: 相比先前方法，模型在物种识别和损伤分类上显著提升，在域转移下仍保持较好性能。

Conclusion: 模型具有鲁棒性和实际应用价值，已部署用于大规模自动化作物和杂草监测。

Abstract: Field trials are vital in herbicide research and development to assess
effects on crops and weeds under varied conditions. Traditionally, evaluations
rely on manual visual assessments, which are time-consuming, labor-intensive,
and subjective. Automating species and damage identification is challenging due
to subtle visual differences, but it can greatly enhance efficiency and
consistency.
  We present an improved segmentation model combining a general-purpose
self-supervised visual model with hierarchical inference based on botanical
taxonomy. Trained on a multi-year dataset (2018-2020) from Germany and Spain
using digital and mobile cameras, the model was tested on digital camera data
(year 2023) and drone imagery from the United States, Germany, and Spain (year
2024) to evaluate robustness under domain shift. This cross-device evaluation
marks a key step in assessing generalization across platforms of the model.
  Our model significantly improved species identification (F1-score: 0.52 to
0.85, R-squared: 0.75 to 0.98) and damage classification (F1-score: 0.28 to
0.44, R-squared: 0.71 to 0.87) over prior methods. Under domain shift (drone
images), it maintained strong performance with moderate degradation (species:
F1-score 0.60, R-squared 0.80; damage: F1-score 0.41, R-squared 0.62), where
earlier models failed.
  These results confirm the model's robustness and real-world applicability. It
is now deployed in BASF's phenotyping pipeline, enabling large-scale, automated
crop and weed monitoring across diverse geographies.

</details>


### [469] [A DICOM Image De-identification Algorithm in the MIDI-B Challenge](https://arxiv.org/abs/2508.07538)
*Hongzhu Jiang,Sihan Xie,Zhiyu Wan*

Main category: cs.CV

TL;DR: 文章聚焦DICOM医学图像去标识化，介绍MIDI - B挑战，详述去标识方法，算法表现良好并分析结果与不足。


<details>
  <summary>Details</summary>
Motivation: 医学图像公开共享需遵循法规去除个人可识别信息，组织MIDI - B挑战评估基于规则的DICOM图像去标识算法。

Method: 采用像素掩码、日期偏移、日期哈希、文本识别、文本替换和文本删除等去标识方法处理数据集。

Result: 算法在MIDI - B挑战最终排行榜中正确执行99.92%的所需操作，在完成挑战的10支队伍中排名第2。

Conclusion: 对结果统计进行分析，讨论当前方法的局限性和未来改进方向。

Abstract: Image de-identification is essential for the public sharing of medical
images, particularly in the widely used Digital Imaging and Communications in
Medicine (DICOM) format as required by various regulations and standards,
including Health Insurance Portability and Accountability Act (HIPAA) privacy
rules, the DICOM PS3.15 standard, and best practices recommended by the Cancer
Imaging Archive (TCIA). The Medical Image De-Identification Benchmark (MIDI-B)
Challenge at the 27th International Conference on Medical Image Computing and
Computer Assisted Intervention (MICCAI 2024) was organized to evaluate
rule-based DICOM image de-identification algorithms with a large dataset of
clinical DICOM images. In this report, we explore the critical challenges of
de-identifying DICOM images, emphasize the importance of removing personally
identifiable information (PII) to protect patient privacy while ensuring the
continued utility of medical data for research, diagnostics, and treatment, and
provide a comprehensive overview of the standards and regulations that govern
this process. Additionally, we detail the de-identification methods we applied
- such as pixel masking, date shifting, date hashing, text recognition, text
replacement, and text removal - to process datasets during the test phase in
strict compliance with these standards. According to the final leaderboard of
the MIDI-B challenge, the latest version of our solution algorithm correctly
executed 99.92% of the required actions and ranked 2nd out of 10 teams that
completed the challenge (from a total of 22 registered teams). Finally, we
conducted a thorough analysis of the resulting statistics and discussed the
limitations of current approaches and potential avenues for future improvement.

</details>


### [470] [Commentary Generation for Soccer Highlights](https://arxiv.org/abs/2508.07543)
*Chidaksh Ravuru*

Main category: cs.CV

TL;DR: 本文在MatchVoice基础上，用GOAL数据集为足球高光片段生成解说，实验评估不同训练配置和窗口大小影响，指出需结合更多视频 - 语言技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有足球解说生成框架难以实现视频内容与解说的细粒度对齐，需改进以提升性能。

Method: 扩展MatchVoice模型用于足球高光解说生成，使用GOAL数据集，进行实验重现和评估，探索不同窗口大小对零样本性能的影响。

Result: MatchVoice有一定泛化能力，不同训练配置和硬件有影响，不同窗口大小影响零样本性能。

Conclusion: 需要集成更广泛视频 - 语言领域的技术来进一步提高性能。

Abstract: Automated soccer commentary generation has evolved from template-based
systems to advanced neural architectures, aiming to produce real-time
descriptions of sports events. While frameworks like SoccerNet-Caption laid
foundational work, their inability to achieve fine-grained alignment between
video content and commentary remains a significant challenge. Recent efforts
such as MatchTime, with its MatchVoice model, address this issue through coarse
and fine-grained alignment techniques, achieving improved temporal
synchronization. In this paper, we extend MatchVoice to commentary generation
for soccer highlights using the GOAL dataset, which emphasizes short clips over
entire games. We conduct extensive experiments to reproduce the original
MatchTime results and evaluate our setup, highlighting the impact of different
training configurations and hardware limitations. Furthermore, we explore the
effect of varying window sizes on zero-shot performance. While MatchVoice
exhibits promising generalization capabilities, our findings suggest the need
for integrating techniques from broader video-language domains to further
enhance performance. Our code is available at
https://github.com/chidaksh/SoccerCommentary.

</details>


### [471] [Exploiting Layer Normalization Fine-tuning in Visual Transformer Foundation Models for Classification](https://arxiv.org/abs/2508.07577)
*Zhaorui Tan,Tan Pan,Kaizhu Huang,Weimiao Yu,Kai Yao,Chen Jiang,Qiufeng Wang,Anh Nguyen,Xin Guo,Yuan Cheng,Xi Yang*

Main category: cs.CV

TL;DR: 本文研究了视觉Transformer中LayerNorm在数据稀缺和领域转移下的微调动态，提出基于FSR的重缩放机制和循环框架，实验验证其有效性并得出不同场景下的特点。


<details>
  <summary>Details</summary>
Motivation: 探索视觉Transformer中LayerNorm在数据稀缺和领域转移下的微调动态。

Method: 提出用与FSR负相关的标量λ的重缩放机制，结合循环框架来增强LayerNorm微调。

Result: 在自然和病理图像的多种设置和样本制度下验证了框架，发现OOD任务FSR低、λ高，病理数据微调类似ID设置。

Conclusion: 揭示了迁移学习中LayerNorm未被充分探索的动态，为其微调提供实用策略。

Abstract: LayerNorm is pivotal in Vision Transformers (ViTs), yet its fine-tuning
dynamics under data scarcity and domain shifts remain underexplored. This paper
shows that shifts in LayerNorm parameters after fine-tuning (LayerNorm shifts)
are indicative of the transitions between source and target domains; its
efficacy is contingent upon the degree to which the target training samples
accurately represent the target domain, as quantified by our proposed
Fine-tuning Shift Ratio ($FSR$). Building on this, we propose a simple yet
effective rescaling mechanism using a scalar $\lambda$ that is negatively
correlated to $FSR$ to align learned LayerNorm shifts with those ideal shifts
achieved under fully representative data, combined with a cyclic framework that
further enhances the LayerNorm fine-tuning. Extensive experiments across
natural and pathological images, in both in-distribution (ID) and
out-of-distribution (OOD) settings, and various target training sample regimes
validate our framework. Notably, OOD tasks tend to yield lower $FSR$ and higher
$\lambda$ in comparison to ID cases, especially with scarce data, indicating
under-represented target training samples. Moreover, ViTFs fine-tuned on
pathological data behave more like ID settings, favoring conservative LayerNorm
updates. Our findings illuminate the underexplored dynamics of LayerNorm in
transfer learning and provide practical strategies for LayerNorm fine-tuning.

</details>


### [472] [ShoulderShot: Generating Over-the-Shoulder Dialogue Videos](https://arxiv.org/abs/2508.07597)
*Yuang Zhang,Junqi Cheng,Haoyu Zhao,Jiaxi Gu,Fangyuan Zou,Zenghui Lu,Peng Shu*

Main category: cs.CV

TL;DR: 提出ShoulderShot框架用于过肩对话视频生成，效果超现有方法。


<details>
  <summary>Details</summary>
Motivation: 过肩对话视频重要但在视频生成研究中未充分探索，存在保持角色一致性、空间连续性和生成多轮对话等挑战。

Method: 提出结合双镜头生成与循环视频的ShoulderShot框架。

Result: 在正反打布局、空间连续性和对话长度灵活性方面超越现有方法。

Conclusion: 为实际对话视频生成开辟了新可能。

Abstract: Over-the-shoulder dialogue videos are essential in films, short dramas, and
advertisements, providing visual variety and enhancing viewers' emotional
connection. Despite their importance, such dialogue scenes remain largely
underexplored in video generation research. The main challenges include
maintaining character consistency across different shots, creating a sense of
spatial continuity, and generating long, multi-turn dialogues within limited
computational budgets. Here, we present ShoulderShot, a framework that combines
dual-shot generation with looping video, enabling extended dialogues while
preserving character consistency. Our results demonstrate capabilities that
surpass existing methods in terms of shot-reverse-shot layout, spatial
continuity, and flexibility in dialogue length, thereby opening up new
possibilities for practical dialogue video generation. Videos and comparisons
are available at https://shouldershot.github.io.

</details>


### [473] [SOFA: Deep Learning Framework for Simulating and Optimizing Atrial Fibrillation Ablation](https://arxiv.org/abs/2508.07621)
*Yunsung Chung,Chanho Lim,Ghassan Bidaoui,Christian Massad,Nassir Marrouche,Jihun Hamm*

Main category: cs.CV

TL;DR: 提出SOFA深度学习框架，可模拟房颤消融效果、预测复发风险并优化参数，使预测复发风险降低22.18%。


<details>
  <summary>Details</summary>
Motivation: 房颤导管消融结果差异大，评估和提高消融效果具挑战性，需预测复发及优化消融策略。

Method: 提出SOFA框架，先基于患者消融前LGE - MRI和手术参数模拟消融结果并预测复发风险，再优化参数以降低风险，利用多模态、多视图生成器处理心房2.5D表示。

Result: SOFA能准确合成消融后图像，优化方案使模型预测的复发风险降低22.18%。

Conclusion: SOFA是首个集成手术效果模拟、复发预测和参数优化的框架，为个性化房颤消融提供新工具。

Abstract: Atrial fibrillation (AF) is a prevalent cardiac arrhythmia often treated with
catheter ablation procedures, but procedural outcomes are highly variable.
Evaluating and improving ablation efficacy is challenging due to the complex
interaction between patient-specific tissue and procedural factors. This paper
asks two questions: Can AF recurrence be predicted by simulating the effects of
procedural parameters? How should we ablate to reduce AF recurrence? We propose
SOFA (Simulating and Optimizing Atrial Fibrillation Ablation), a novel
deep-learning framework that addresses these questions. SOFA first simulates
the outcome of an ablation strategy by generating a post-ablation image
depicting scar formation, conditioned on a patient's pre-ablation LGE-MRI and
the specific procedural parameters used (e.g., ablation locations, duration,
temperature, power, and force). During this simulation, it predicts AF
recurrence risk. Critically, SOFA then introduces an optimization scheme that
refines these procedural parameters to minimize the predicted risk. Our method
leverages a multi-modal, multi-view generator that processes 2.5D
representations of the atrium. Quantitative evaluations show that SOFA
accurately synthesizes post-ablation images and that our optimization scheme
leads to a 22.18\% reduction in the model-predicted recurrence risk. To the
best of our knowledge, SOFA is the first framework to integrate the simulation
of procedural effects, recurrence prediction, and parameter optimization,
offering a novel tool for personalizing AF ablation.

</details>


### [474] [Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP](https://arxiv.org/abs/2508.07819)
*Ke Ma,Jun Long,Hongxiao Fei,Liujie Hua,Yueyi Luo*

Main category: cs.CV

TL;DR: 预训练视觉语言模型应用于零样本异常检测存在适应差距，提出架构协同设计框架解决，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型应用于零样本异常检测时存在因缺乏局部归纳偏置和依赖不灵活特征融合范式导致的适应差距问题。

Method: 提出架构协同设计框架，集成参数高效的卷积低秩自适应适配器注入局部归纳偏置，引入动态融合网关利用视觉上下文自适应调节文本提示实现双向融合。

Result: 在不同工业和医学基准上的大量实验显示出优越的准确性和鲁棒性。

Conclusion: 这种协同设计对于将基础模型稳健地应用于密集感知任务至关重要。

Abstract: Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap
when applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of
local inductive biases for dense prediction and their reliance on inflexible
feature fusion paradigms. We address these limitations through an Architectural
Co-Design framework that jointly refines feature representation and cross-modal
fusion. Our method integrates a parameter-efficient Convolutional Low-Rank
Adaptation (Conv-LoRA) adapter to inject local inductive biases for
fine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that
leverages visual context to adaptively modulate text prompts, enabling a
powerful bidirectional fusion. Extensive experiments on diverse industrial and
medical benchmarks demonstrate superior accuracy and robustness, validating
that this synergistic co-design is critical for robustly adapting foundation
models to dense perception tasks.

</details>


### [475] [Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model](https://arxiv.org/abs/2508.07863)
*Bin Cao,Sipeng Zheng,Ye Wang,Lujie Xia,Qianshan Wei,Qin Jin,Jing Liu,Zongqing Lu*

Main category: cs.CV

TL;DR: 现有视觉语言运动模型在可控性上存在局限，本文提出实时可控的VLMM Being - M0.5，基于HuMo100M数据集和新的量化技术，在多基准测试中表现优异并具实时能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言运动模型（VLMMs）存在可控性瓶颈，包括对多样命令响应不足等五方面问题，阻碍其实践应用。

Method: 基于HuMo100M数据集，引入新颖的部分感知残差量化技术进行运动标记化。

Result: Being - M0.5在多样运动基准测试中表现优越，效率分析证实其实时能力。

Conclusion: HuMo100M和Being - M0.5是重要进展，将加速运动生成技术在现实应用中的采用。

Abstract: Human motion generation has emerged as a critical technology with
transformative potential for real-world applications. However, existing
vision-language-motion models (VLMMs) face significant limitations that hinder
their practical deployment. We identify controllability as a main bottleneck,
manifesting in five key aspects: inadequate response to diverse human commands,
limited pose initialization capabilities, poor performance on long-term
sequences, insufficient handling of unseen scenarios, and lack of fine-grained
control over individual body parts. To overcome these limitations, we present
Being-M0.5, the first real-time, controllable VLMM that achieves
state-of-the-art performance across multiple motion generation tasks. Our
approach is built upon HuMo100M, the largest and most comprehensive human
motion dataset to date, comprising over 5 million self-collected motion
sequences, 100 million multi-task instructional instances, and detailed
part-level annotations that address a critical gap in existing datasets. We
introduce a novel part-aware residual quantization technique for motion
tokenization that enables precise, granular control over individual body parts
during generation. Extensive experimental validation demonstrates Being-M0.5's
superior performance across diverse motion benchmarks, while comprehensive
efficiency analysis confirms its real-time capabilities. Our contributions
include design insights and detailed computational analysis to guide future
development of practical motion generators. We believe that HuMo100M and
Being-M0.5 represent significant advances that will accelerate the adoption of
motion generation technologies in real-world applications. The project page is
available at https://beingbeyond.github.io/Being-M0.5.

</details>


### [476] [TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding](https://arxiv.org/abs/2508.07683)
*Chaohong Guo,Xun Mo,Yongwei Nie,Xuemiao Xu,Chao Xu,Fei Yu,Chengjiang Long*

Main category: cs.CV

TL;DR: 提出TAR - TVG框架用于时间视频定位，采用三阶段训练策略，实验表现达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法未明确约束推理过程以保证最终时间预测质量。

Method: 提出TAR - TVG框架引入时间戳锚点约束推理过程；开发三阶段训练策略，包括初始GRPO训练、监督微调、最终GRPO优化。

Result: 模型达到了当前最优性能，产生可解释、可验证且时间估计逐步细化的推理链。

Conclusion: TAR - TVG框架及三阶段训练策略有效，能提升时间视频定位的性能。

Abstract: Temporal Video Grounding (TVG) aims to precisely localize video segments
corresponding to natural language queries, which is a critical capability for
long-form video understanding. Although existing reinforcement learning
approaches encourage models to generate reasoning chains before predictions,
they fail to explicitly constrain the reasoning process to ensure the quality
of the final temporal predictions. To address this limitation, we propose
Timestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG),
a novel framework that introduces timestamp anchors within the reasoning
process to enforce explicit supervision to the thought content. These anchors
serve as intermediate verification points. More importantly, we require each
reasoning step to produce increasingly accurate temporal estimations, thereby
ensuring that the reasoning process contributes meaningfully to the final
prediction. To address the challenge of low-probability anchor generation in
models (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillation
training strategy: (1) initial GRPO training to collect 30K high-quality
reasoning traces containing multiple timestamp anchors, (2) supervised
fine-tuning (SFT) on distilled data, and (3) final GRPO optimization on the
SFT-enhanced model. This three-stage training strategy enables robust anchor
generation while maintaining reasoning quality. Experiments show that our model
achieves state-of-the-art performance while producing interpretable, verifiable
reasoning chains with progressively refined temporal estimations.

</details>


### [477] [Safeguarding Generative AI Applications in Preclinical Imaging through Hybrid Anomaly Detection](https://arxiv.org/abs/2508.07923)
*Jakub Binda,Valentina Paneta,Vasileios Eleftheriadis,Hongkyou Chung,Panagiotis Papadimitroulas,Neo Christopher Chung*

Main category: cs.CV

TL;DR: 提出混合异常检测框架保障核医学GenAI模型，通过两个应用展示其增强可靠性等优势，提升GenAI在临床前环境的工业可行性。


<details>
  <summary>Details</summary>
Motivation: 生物医学成像的高风险性质需要检测和管理GenAI模型意外或错误行为的机制。

Method: 开发和实施混合异常检测框架。

Result: 在Pose2Xray和DosimetrEYE两个应用中，异常检测增强了可靠性，减少人工监督，支持实时质量控制。

Conclusion: 该方法通过提高鲁棒性、可扩展性和合规性，提升了GenAI在临床前环境的工业可行性。

Abstract: Generative AI holds great potentials to automate and enhance data synthesis
in nuclear medicine. However, the high-stakes nature of biomedical imaging
necessitates robust mechanisms to detect and manage unexpected or erroneous
model behavior. We introduce development and implementation of a hybrid anomaly
detection framework to safeguard GenAI models in BIOEMTECH's eyes(TM) systems.
Two applications are demonstrated: Pose2Xray, which generates synthetic X-rays
from photographic mouse images, and DosimetrEYE, which estimates 3D radiation
dose maps from 2D SPECT/CT scans. In both cases, our outlier detection (OD)
enhances reliability, reduces manual oversight, and supports real-time quality
control. This approach strengthens the industrial viability of GenAI in
preclinical settings by increasing robustness, scalability, and regulatory
compliance.

</details>


### [478] [DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models](https://arxiv.org/abs/2508.07714)
*Licheng Zhang,Bach Le,Naveed Akhtar,Tuan Ngo*

Main category: cs.CV

TL;DR: 提出半自动化流程构建多类门检测数据集，降低标注成本，展示深度学习与多模态推理构建数据集潜力。


<details>
  <summary>Details</summary>
Motivation: 准确检测和分类平面图中门类型很重要，但公开的细粒度多类门检测数据集稀缺。

Method: 利用先进目标检测器和大语言模型，先以深度学习模型检测门，再用大语言模型分类，最后有人在环确保标注质量。

Result: 显著降低标注成本，生成适用于平面图分析中神经模型基准测试的数据集。

Conclusion: 结合深度学习和多模态推理在复杂现实领域高效构建数据集有潜力。

Abstract: Accurate detection and classification of diverse door types in floor plans
drawings is critical for multiple applications, such as building compliance
checking, and indoor scene understanding. Despite their importance, publicly
available datasets specifically designed for fine-grained multi-class door
detection remain scarce. In this work, we present a semi-automated pipeline
that leverages a state-of-the-art object detector and a large language model
(LLM) to construct a multi-class door detection dataset with minimal manual
effort. Doors are first detected as a unified category using a deep object
detection model. Next, an LLM classifies each detected instance based on its
visual and contextual features. Finally, a human-in-the-loop stage ensures
high-quality labels and bounding boxes. Our method significantly reduces
annotation cost while producing a dataset suitable for benchmarking neural
models in floor plan analysis. This work demonstrates the potential of
combining deep learning and multimodal reasoning for efficient dataset
construction in complex real-world domains.

</details>


### [479] [UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models](https://arxiv.org/abs/2508.07766)
*Jinke Li,Jiarui Yu,Chenxing Wei,Hande Dong,Qiang Lin,Liangjing Yang,Zhicai Wang,Yanbin Hao*

Main category: cs.CV

TL;DR: 提出用于多模态大语言模型训练和评估的 SVG 数据集 UniSVG，提升了模型在 SVG 理解和生成任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 在 AI 系统发展时代，使 AI 理解和生成 SVG 很迫切，但 SVG 理解和生成存在挑战，多模态大语言模型有解决潜力。

Method: 提出包含 525k 数据项的 SVG 中心数据集 UniSVG 用于 MLLM 训练和评估。

Result: 在 UniSVG 上学习提升了开源 MLLM 在 SVG 理解和生成任务上的表现，超越了如 GPT - 4V 等闭源 SOTA 模型。

Conclusion: 所提出的 UniSVG 数据集有助于释放 MLLM 在 SVG 领域的能力，促进相关研究发展，相关资源已公开。

Abstract: Unlike bitmap images, scalable vector graphics (SVG) maintain quality when
scaled, frequently employed in computer vision and artistic design in the
representation of SVG code. In this era of proliferating AI-powered systems,
enabling AI to understand and generate SVG has become increasingly urgent.
However, AI-driven SVG understanding and generation (U&G) remain significant
challenges. SVG code, equivalent to a set of curves and lines controlled by
floating-point parameters, demands high precision in SVG U&G. Besides, SVG
generation operates under diverse conditional constraints, including textual
prompts and visual references, which requires powerful multi-modal processing
for condition-to-SVG transformation. Recently, the rapid growth of Multi-modal
Large Language Models (MLLMs) have demonstrated capabilities to process
multi-modal inputs and generate complex vector controlling parameters,
suggesting the potential to address SVG U&G tasks within a unified model. To
unlock MLLM's capabilities in the SVG area, we propose an SVG-centric dataset
called UniSVG, comprising 525k data items, tailored for MLLM training and
evaluation. To our best knowledge, it is the first comprehensive dataset
designed for unified SVG generation (from textual prompts and images) and SVG
understanding (color, category, usage, etc.). As expected, learning on the
proposed dataset boosts open-source MLLMs' performance on various SVG U&G
tasks, surpassing SOTA close-source MLLMs like GPT-4V. We release dataset,
benchmark, weights, codes and experiment details on
https://ryanlijinke.github.io/.

</details>


### [480] [PrIINeR: Towards Prior-Informed Implicit Neural Representations for Accelerated MRI](https://arxiv.org/abs/2508.08058)
*Ziad Al-Haj Hemidi,Eytan Kats,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: 提出PrIINeR方法解决INR在高加速因子下MRI重建问题，在NYU fastMRI数据集表现出色。


<details>
  <summary>Details</summary>
Motivation: 加速MRI会降低图像质量，INR在高加速因子下因先验约束弱有结构损失和伪影问题。

Method: 提出PrIINeR方法，将预训练深度学习模型先验知识集成到INR框架，结合群体知识与实例优化，执行双重数据一致性。

Result: 在NYU fastMRI数据集上，优于现有的基于INR和一些基于学习的方法，改善结构保留和保真度，去除伪影。

Conclusion: PrIINeR结合深度学习和INR技术，为高质量加速MRI重建提供更可靠方案。

Abstract: Accelerating Magnetic Resonance Imaging (MRI) reduces scan time but often
degrades image quality. While Implicit Neural Representations (INRs) show
promise for MRI reconstruction, they struggle at high acceleration factors due
to weak prior constraints, leading to structural loss and aliasing artefacts.
To address this, we propose PrIINeR, an INR-based MRI reconstruction method
that integrates prior knowledge from pre-trained deep learning models into the
INR framework. By combining population-level knowledge with instance-based
optimization and enforcing dual data consistency, PrIINeR aligns both with the
acquired k-space data and the prior-informed reconstruction. Evaluated on the
NYU fastMRI dataset, our method not only outperforms state-of-the-art INR-based
approaches but also improves upon several learning-based state-of-the-art
methods, significantly improving structural preservation and fidelity while
effectively removing aliasing artefacts.PrIINeR bridges deep learning and
INR-based techniques, offering a more reliable solution for high-quality,
accelerated MRI reconstruction. The code is publicly available on
https://github.com/multimodallearning/PrIINeR.

</details>


### [481] [Investigating the Design Space of Visual Grounding in Multimodal Large Language Model](https://arxiv.org/abs/2508.08066)
*Weitai Kang,Weiming Zhuang,Zhizhong Li,Yan Yan,Lingjuan Lyu*

Main category: cs.CV

TL;DR: 本文对影响多模态大语言模型（MLLMs）视觉定位（VG）性能的设计选择进行全面研究，基于LLaVA - 1.5分析不同VG范式和定位数据设计，使模型在VG任务上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在VG任务微调时设计选择缺乏系统验证，本文旨在填补这一空白。

Method: 使用LLaVA - 1.5进行分析，探索不同VG范式，对定位数据设计进行消融研究。

Result: 模型在RefCOCO/+/g上比LLaVA - 1.5分别提升了+5.6% / +6.9% / +7.0%。

Conclusion: 研究成果有助于构建更强的用于VG任务的MLLM。

Abstract: Fine-grained multimodal capability in Multimodal Large Language Models
(MLLMs) has emerged as a critical research direction, particularly for tackling
the visual grounding (VG) problem. Despite the strong performance achieved by
existing approaches, they often employ disparate design choices when
fine-tuning MLLMs for VG, lacking systematic verification to support these
designs. To bridge this gap, this paper presents a comprehensive study of
various design choices that impact the VG performance of MLLMs. We conduct our
analysis using LLaVA-1.5, which has been widely adopted in prior empirical
studies of MLLMs. While more recent models exist, we follow this convention to
ensure our findings remain broadly applicable and extendable to other
architectures. We cover two key aspects: (1) exploring different visual
grounding paradigms in MLLMs, identifying the most effective design, and
providing our insights; and (2) conducting ablation studies on the design of
grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our
findings contribute to a stronger MLLM for VG, achieving improvements of +5.6%
/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.

</details>


### [482] [Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images](https://arxiv.org/abs/2508.07847)
*Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

TL;DR: 提出Deep SWM模型用于太阳耀斑预测，构建FlareBench基准验证，性能超基线和人类专家。


<details>
  <summary>Details</summary>
Motivation: 现有基于启发式物理特征的方法缺乏对太阳图像的表示学习，端到端学习方法难以建模太阳图像的长程时间依赖，而准确可靠的太阳耀斑预测对减轻关键基础设施的潜在破坏至关重要。

Method: 提出基于多个深度状态空间模型的Deep SWM，处理十通道太阳图像和长程时空依赖，采用稀疏掩码自编码器和两阶段掩码预训练策略；构建新的公共基准FlareBench。

Result: 该方法在标准指标上的性能和可靠性超过基线方法，甚至超过人类专家。

Conclusion: Deep SWM模型和FlareBench基准在太阳耀斑预测上表现良好，具有一定的应用价值。

Abstract: Accurate, reliable solar flare prediction is crucial for mitigating potential
disruptions to critical infrastructure, while predicting solar flares remains a
significant challenge. Existing methods based on heuristic physical features
often lack representation learning from solar images. On the other hand,
end-to-end learning approaches struggle to model long-range temporal
dependencies in solar images. In this study, we propose Deep Space Weather
Model (Deep SWM), which is based on multiple deep state space models for
handling both ten-channel solar images and long-range spatio-temporal
dependencies. Deep SWM also features a sparse masked autoencoder, a novel
pretraining strategy that employs a two-phase masking approach to preserve
crucial regions such as sunspots while compressing spatial information.
Furthermore, we built FlareBench, a new public benchmark for solar flare
prediction covering a full 11-year solar activity cycle, to validate our
method. Our method outperformed baseline methods and even human expert
performance on standard metrics in terms of performance and reliability. The
project page can be found at https://keio-smilab25.github.io/DeepSWM.

</details>


### [483] [MDD-Net: Multimodal Depression Detection through Mutual Transformer](https://arxiv.org/abs/2508.08093)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Hamdi Altaheri,Lobna Nassar,Fakhri Karray*

Main category: cs.CV

TL;DR: 本文提出MDD - Net利用社交媒体的声视觉数据进行抑郁检测，实验显示性能超现有方法。


<details>
  <summary>Details</summary>
Motivation: 抑郁症严重影响个体身心健康，社交媒体数据易收集，可用于心理健康研究。

Method: 提出MDD - Net，包含声学、视觉特征提取模块、互变压器和检测层，利用互变压器提取和融合多模态特征。

Result: 在多模态D - Vlog数据集上实验，F1得分比现有技术高最多17.37%。

Conclusion: 所提出的多模态抑郁检测网络性能更优，源代码可在指定链接获取。

Abstract: Depression is a major mental health condition that severely impacts the
emotional and physical well-being of individuals. The simple nature of data
collection from social media platforms has attracted significant interest in
properly utilizing this information for mental health research. A Multimodal
Depression Detection Network (MDD-Net), utilizing acoustic and visual data
obtained from social media networks, is proposed in this work where mutual
transformers are exploited to efficiently extract and fuse multimodal features
for efficient depression detection. The MDD-Net consists of four core modules:
an acoustic feature extraction module for retrieving relevant acoustic
attributes, a visual feature extraction module for extracting significant
high-level patterns, a mutual transformer for computing the correlations among
the generated features and fusing these features from multiple modalities, and
a detection layer for detecting depression using the fused feature
representations. The extensive experiments are performed using the multimodal
D-Vlog dataset, and the findings reveal that the developed multimodal
depression detection network surpasses the state-of-the-art by up to 17.37% for
F1-Score, demonstrating the greater performance of the proposed system. The
source code is accessible at
https://github.com/rezwanh001/Multimodal-Depression-Detection.

</details>


### [484] [Selective Contrastive Learning for Weakly Supervised Affordance Grounding](https://arxiv.org/abs/2508.07877)
*WonJun Moon,Hyun Seok Seong,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 本文提出新方法解决弱监督可及性定位问题，通过选择性原型和像素对比目标学习相关线索，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督可及性定位模型主要依赖分类，常关注与可及性无关的常见模式，无法很好区分可及性相关部分。

Method: 引入选择性原型和像素对比目标，在部分和对象层面自适应学习可及性相关线索；利用CLIP找到不同视角图像中与动作关联的对象，交叉参考挖掘精确的部分级可及性线索。

Result: 实验结果证明了该方法的有效性。

Conclusion: 提出的方法能有效将激活从无关区域转移到有意义的可及性线索上。

Abstract: Facilitating an entity's interaction with objects requires accurately
identifying parts that afford specific actions. Weakly supervised affordance
grounding (WSAG) seeks to imitate human learning from third-person
demonstrations, where humans intuitively grasp functional parts without needing
pixel-level annotations. To achieve this, grounding is typically learned using
a shared classifier across images from different perspectives, along with
distillation strategies incorporating part discovery process. However, since
affordance-relevant parts are not always easily distinguishable, models
primarily rely on classification, often focusing on common class-specific
patterns that are unrelated to affordance. To address this limitation, we move
beyond isolated part-level learning by introducing selective prototypical and
pixel contrastive objectives that adaptively learn affordance-relevant cues at
both the part and object levels, depending on the granularity of the available
information. Initially, we find the action-associated objects in both
egocentric (object-focused) and exocentric (third-person example) images by
leveraging CLIP. Then, by cross-referencing the discovered objects of
complementary views, we excavate the precise part-level affordance clues in
each perspective. By consistently learning to distinguish affordance-relevant
regions from affordance-irrelevant background context, our approach effectively
shifts activation from irrelevant areas toward meaningful affordance cues.
Experimental results demonstrate the effectiveness of our method. Codes are
available at github.com/hynnsk/SelectiveCL.

</details>


### [485] [Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning](https://arxiv.org/abs/2508.08165)
*Yan Wang,Da-Wei Zhou,Han-Jia Ye*

Main category: cs.CV

TL;DR: 提出集成任务特定和通用适配器（TUNA）方法解决类增量学习问题，实验效果达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练模型的CIL方法存在推理时模块选择错误影响性能、任务特定模块忽略通用知识导致区分相似类有误差的问题。

Method: 训练任务特定适配器捕捉关键特征，引入基于熵的选择机制选适配适配器，利用适配器融合策略构建通用适配器，结合两者预测进行推理。

Result: 在各种基准数据集上的大量实验表明该方法达到了最先进的性能。

Conclusion: 所提出的TUNA方法能有效解决现有CIL方法的问题，具有良好性能。

Abstract: Class-Incremental Learning (CIL) requires a learning system to continually
learn new classes without forgetting. Existing pre-trained model-based CIL
methods often freeze the pre-trained network and adapt to incremental tasks
using additional lightweight modules such as adapters. However, incorrect
module selection during inference hurts performance, and task-specific modules
often overlook shared general knowledge, leading to errors on distinguishing
between similar classes across tasks. To address the aforementioned challenges,
we propose integrating Task-Specific and Universal Adapters (TUNA) in this
paper. Specifically, we train task-specific adapters to capture the most
crucial features relevant to their respective tasks and introduce an
entropy-based selection mechanism to choose the most suitable adapter.
Furthermore, we leverage an adapter fusion strategy to construct a universal
adapter, which encodes the most discriminative features shared across tasks. We
combine task-specific and universal adapter predictions to harness both
specialized and general knowledge during inference. Extensive experiments on
various benchmark datasets demonstrate the state-of-the-art performance of our
approach. Code is available at: https://github.com/LAMDA-CL/ICCV2025-TUNA

</details>


### [486] [NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction](https://arxiv.org/abs/2508.07897)
*Tianle Zeng,Junlei Hu,Gerardo Loza Galindo,Sharib Ali,Duygu Sarikaya,Pietro Valdastri,Dominic Jones*

Main category: cs.CV

TL;DR: 本文提出动态高斯散点技术解决手术图像数据集数据稀缺问题，构建新数据集评估，结果表明该方法生成图像质量高，训练模型性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动方法需大量高质量标注图像数据集，限制其在手术数据科学中的应用，因此要解决手术图像数据集的数据稀缺问题。

Method: 引入动态高斯散点技术，提出动态高斯模型表示动态手术场景，采用动态训练调整策略应对相机姿态校准问题，提出基于动态高斯的合成数据自动标注方法。

Result: 生成的标注图像数据集的峰值信噪比最高，用合成图像训练的模型性能比用现有标准数据增强方法训练的模型高10%，整体模型性能提升近15%。

Conclusion: 所提方法能有效解决手术图像数据集的数据稀缺问题，提升医学特定神经网络的性能。

Abstract: Computer vision-based technologies significantly enhance surgical automation
by advancing tool tracking, detection, and localization. However, Current
data-driven approaches are data-voracious, requiring large, high-quality
labeled image datasets, which limits their application in surgical data
science. Our Work introduces a novel dynamic Gaussian Splatting technique to
address the data scarcity in surgical image datasets. We propose a dynamic
Gaussian model to represent dynamic surgical scenes, enabling the rendering of
surgical instruments from unseen viewpoints and deformations with real tissue
backgrounds. We utilize a dynamic training adjustment strategy to address
challenges posed by poorly calibrated camera poses from real-world scenarios.
Additionally, we propose a method based on dynamic Gaussians for automatically
generating annotations for our synthetic data. For evaluation, we constructed a
new dataset featuring seven scenes with 14,000 frames of tool and camera motion
and tool jaw articulation, with a background of an ex-vivo porcine model. Using
this dataset, we synthetically replicate the scene deformation from the ground
truth data, allowing direct comparisons of synthetic image quality.
Experimental results illustrate that our method generates photo-realistic
labeled image datasets with the highest values in Peak-Signal-to-Noise Ratio
(29.87). We further evaluate the performance of medical-specific neural
networks trained on real and synthetic images using an unseen real-world image
dataset. Our results show that the performance of models trained on synthetic
images generated by the proposed method outperforms those trained with
state-of-the-art standard data augmentation by 10%, leading to an overall
improvement in model performances by nearly 15%.

</details>


### [487] [Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation](https://arxiv.org/abs/2508.07981)
*Fangyuan Mao,Aiming Hao,Jintao Chen,Dongxia Liu,Xiaokun Feng,Jiashu Zhu,Meiqi Wu,Chubin Chen,Jiahong Wu,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 提出Omni - Effects框架解决视频特效生成难题，能实现精确空间控制和多样特效生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频特效生成方法受限于单特效训练，无法满足空间可控的复合特效生成需求，且整合多样特效面临干扰和空间不可控挑战。

Method: 提出包含LoRA - MoE、Spatial - Aware Prompt (SAP)和Independent - Information Flow (IIF)模块的Omni - Effects框架，构建Omni - VFX数据集和评估框架。

Result: 实验表明Omni - Effects能实现精确空间控制和多样特效生成，用户可指定特效类别和位置。

Conclusion: Omni - Effects框架有效解决了现有视频特效生成方法的局限，具有良好的应用前景。

Abstract: Visual effects (VFX) are essential visual enhancements fundamental to modern
cinematic production. Although video generation models offer cost-efficient
solutions for VFX production, current methods are constrained by per-effect
LoRA training, which limits generation to single effects. This fundamental
limitation impedes applications that require spatially controllable composite
effects, i.e., the concurrent generation of multiple effects at designated
locations. However, integrating diverse effects into a unified framework faces
major challenges: interference from effect variations and spatial
uncontrollability during multi-VFX joint training. To tackle these challenges,
we propose Omni-Effects, a first unified framework capable of generating
prompt-guided effects and spatially controllable composite effects. The core of
our framework comprises two key innovations: (1) LoRA-based Mixture of Experts
(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects
within a unified model while effectively mitigating cross-task interference.
(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the
text token, enabling precise spatial control. Furthermore, we introduce an
Independent-Information Flow (IIF) module integrated within the SAP, isolating
the control signals corresponding to individual effects to prevent any unwanted
blending. To facilitate this research, we construct a comprehensive VFX dataset
Omni-VFX via a novel data collection pipeline combining image editing and
First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX
evaluation framework for validating model performance. Extensive experiments
demonstrate that Omni-Effects achieves precise spatial control and diverse
effect generation, enabling users to specify both the category and location of
desired effects.

</details>


### [488] [Hyperspectral Imaging](https://arxiv.org/abs/2508.08107)
*Danfeng Hong,Chenyu Li,Naoto Yokoya,Bing Zhang,Xiuping Jia,Antonio Plaza,Paolo Gamba,Jon Atli Benediktsson,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: 本文全面介绍高光谱成像（HSI），涵盖原理、方法、应用、挑战、解决方案及未来方向，指出其有望成为跨学科平台。


<details>
  <summary>Details</summary>
Motivation: 介绍高光谱成像，推动其在多领域应用和发展。

Method: 概述物理原理、传感器架构，总结数据结构和分析方法，讨论应用、挑战及解决方案。

Result: 明确HSI能揭示亚视觉特征用于监测等，提出新兴解决方案和最佳实践。

Conclusion: HSI有望发展为通用跨学科平台，在多领域有变革性应用前景。

Abstract: Hyperspectral imaging (HSI) is an advanced sensing modality that
simultaneously captures spatial and spectral information, enabling
non-invasive, label-free analysis of material, chemical, and biological
properties. This Primer presents a comprehensive overview of HSI, from the
underlying physical principles and sensor architectures to key steps in data
acquisition, calibration, and correction. We summarize common data structures
and highlight classical and modern analysis methods, including dimensionality
reduction, classification, spectral unmixing, and AI-driven techniques such as
deep learning. Representative applications across Earth observation, precision
agriculture, biomedicine, industrial inspection, cultural heritage, and
security are also discussed, emphasizing HSI's ability to uncover sub-visual
features for advanced monitoring, diagnostics, and decision-making. Persistent
challenges, such as hardware trade-offs, acquisition variability, and the
complexity of high-dimensional data, are examined alongside emerging solutions,
including computational imaging, physics-informed modeling, cross-modal fusion,
and self-supervised learning. Best practices for dataset sharing,
reproducibility, and metadata documentation are further highlighted to support
transparency and reuse. Looking ahead, we explore future directions toward
scalable, real-time, and embedded HSI systems, driven by sensor
miniaturization, self-supervised learning, and foundation models. As HSI
evolves into a general-purpose, cross-disciplinary platform, it holds promise
for transformative applications in science, technology, and society.

</details>


### [489] [GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking](https://arxiv.org/abs/2508.08117)
*Xudong Han,Pengcheng Fang,Yueying Tian,Jianhui Yu,Xiaohao Cai,Daniel Roggen,Philip Birch*

Main category: cs.CV

TL;DR: 提出GRASPTrack框架解决单目视频多目标跟踪问题，实验证明在复杂场景效果好。


<details>
  <summary>Details</summary>
Motivation: 传统TBD方法因缺乏几何感知，难以解决单目视频多目标跟踪中的遮挡和深度模糊问题。

Method: 将单目深度估计和实例分割集成到TBD流程生成3D点云，进行体素化用于空间关联；采用深度自适应噪声补偿和深度增强观测中心动量。

Result: 在MOT17、MOT20和DanceTrack基准上取得有竞争力的性能。

Conclusion: 该方法显著提高了复杂场景下的跟踪鲁棒性。

Abstract: Multi-object tracking (MOT) in monocular videos is fundamentally challenged
by occlusions and depth ambiguity, issues that conventional
tracking-by-detection (TBD) methods struggle to resolve owing to a lack of
geometric awareness. To address these limitations, we introduce GRASPTrack, a
novel depth-aware MOT framework that integrates monocular depth estimation and
instance segmentation into a standard TBD pipeline to generate high-fidelity 3D
point clouds from 2D detections, thereby enabling explicit 3D geometric
reasoning. These 3D point clouds are then voxelized to enable a precise and
robust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To
further enhance tracking robustness, our approach incorporates Depth-aware
Adaptive Noise Compensation, which dynamically adjusts the Kalman filter
process noise based on occlusion severity for more reliable state estimation.
Additionally, we propose a Depth-enhanced Observation-Centric Momentum, which
extends the motion direction consistency from the image plane into 3D space to
improve motion-based association cues, particularly for objects with complex
trajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack
benchmarks demonstrate that our method achieves competitive performance,
significantly improving tracking robustness in complex scenes with frequent
occlusions and intricate motion patterns.

</details>


### [490] [MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision](https://arxiv.org/abs/2508.08177)
*Zhonghao Yan,Muxi Diao,Yuxuan Yang,Jiayuan Xu,Kaizhou Zhang,Ruoyan Jing,Lele Yang,Yanxi Liu,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: 提出统一医学推理定位任务UMRG，发布U - MRG - 14K数据集，介绍MedReasoner框架，该框架在数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前医学定位流程依赖带明确空间提示的有监督微调，难以处理临床实践中的隐式查询，需要更有效的方法。

Method: 定义UMRG任务，发布U - MRG - 14K数据集，引入MedReasoner框架，用强化学习优化MLLM推理器，用冻结的分割专家将空间提示转换为掩码，并通过格式和准确性奖励实现对齐。

Result: MedReasoner在U - MRG - 14K上达到了最先进的性能，对未见临床查询有很强的泛化能力。

Conclusion: 强化学习在可解释医学定位方面有显著前景。

Abstract: Accurately grounding regions of interest (ROIs) is critical for diagnosis and
treatment planning in medical imaging. While multimodal large language models
(MLLMs) combine visual perception with natural language, current
medical-grounding pipelines still rely on supervised fine-tuning with explicit
spatial hints, making them ill-equipped to handle the implicit queries common
in clinical practice. This work makes three core contributions. We first define
Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that
demands clinical reasoning and pixel-level grounding. Second, we release
U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside
implicit clinical queries and reasoning traces, spanning 10 modalities, 15
super-categories, and 108 specific categories. Finally, we introduce
MedReasoner, a modular framework that distinctly separates reasoning from
segmentation: an MLLM reasoner is optimized with reinforcement learning, while
a frozen segmentation expert converts spatial prompts into masks, with
alignment achieved through format and accuracy rewards. MedReasoner achieves
state-of-the-art performance on U-MRG-14K and demonstrates strong
generalization to unseen clinical queries, underscoring the significant promise
of reinforcement learning for interpretable medical grounding.

</details>


### [491] [OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution](https://arxiv.org/abs/2508.08227)
*Zhiqiang Wu,Zhaomang Sun,Tong Zhou,Bingtao Fu,Ji Cong,Yitong Dong,Huaqi Zhang,Xuan Tang,Mingsong Chen,Xian Wei*

Main category: cs.CV

TL;DR: 提出适用于DDPM/FM生成模型的通用框架OMGSR用于单步真实世界图像超分辨率，在不同分辨率实验中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有单步真实世界图像超分辨率模型在初始时间步注入低质量图像潜在分布，与高斯噪声潜在分布存在差距，限制生成先验有效利用。

Method: 提出OMGSR框架，在预计算的中间时间步注入低质量图像潜在分布，引入潜在分布细化损失缓解分布差距，设计重叠分块LPIPS/GAN损失消除棋盘格伪影，为DDPM/FM模型实例化两个变体。

Result: OMGSR - S/F在512分辨率下定量和定性指标表现平衡/优秀，OMGSR - F在所有参考指标占优，1k分辨率的OMGSR - F效果好，用两级平铺VAE和扩散生成2k分辨率图像。

Conclusion: OMGSR框架有效，能提升真实世界图像超分辨率的效果。

Abstract: Denoising Diffusion Probabilistic Models (DDPM) and Flow Matching (FM)
generative models show promising potential for one-step Real-World Image
Super-Resolution (Real-ISR). Recent one-step Real-ISR models typically inject a
Low-Quality (LQ) image latent distribution at the initial timestep. However, a
fundamental gap exists between the LQ image latent distribution and the
Gaussian noisy latent distribution, limiting the effective utilization of
generative priors. We observe that the noisy latent distribution at DDPM/FM
mid-timesteps aligns more closely with the LQ image latent distribution. Based
on this insight, we present One Mid-timestep Guidance Real-ISR (OMGSR), a
universal framework applicable to DDPM/FM-based generative models. OMGSR
injects the LQ image latent distribution at a pre-computed mid-timestep,
incorporating the proposed Latent Distribution Refinement loss to alleviate the
latent distribution gap. We also design the Overlap-Chunked LPIPS/GAN loss to
eliminate checkerboard artifacts in image generation. Within this framework, we
instantiate OMGSR for DDPM/FM-based generative models with two variants:
OMGSR-S (SD-Turbo) and OMGSR-F (FLUX.1-dev). Experimental results demonstrate
that OMGSR-S/F achieves balanced/excellent performance across quantitative and
qualitative metrics at 512-resolution. Notably, OMGSR-F establishes
overwhelming dominance in all reference metrics. We further train a
1k-resolution OMGSR-F to match the default resolution of FLUX.1-dev, which
yields excellent results, especially in the details of the image generation. We
also generate 2k-resolution images by the 1k-resolution OMGSR-F using our
two-stage Tiled VAE & Diffusion.

</details>


### [492] [Cut2Next: Generating Next Shot via In-Context Tuning](https://arxiv.org/abs/2508.08244)
*Jingwen He,Hongbo Liu,Jiajun Li,Ziqi Huang,Yu Qiao,Wanli Ouyang,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出Next Shot Generation (NSG)，并介绍Cut2Next框架以生成符合专业剪辑模式和电影连续性的高质量后续镜头，实验表明其表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前多镜头生成方法常忽视关键剪辑模式，导致输出缺乏叙事复杂性和电影完整性，需要改进。

Method: 提出Cut2Next框架，利用Diffusion Transformer (DiT)，采用Hierarchical Multi - Prompting策略，结合Relational Prompts和Individual Prompts，还有Context - Aware Condition Injection (CACI)和Hierarchical Attention Mask (HAM)等架构创新，构建相关数据集和评估基准CutBench。

Result: 实验显示Cut2Next在视觉一致性和文本保真度上表现出色，用户研究表明用户更偏好Cut2Next。

Conclusion: Cut2Next能生成高质量、叙事丰富且符合电影连贯性的后续镜头。

Abstract: Effective multi-shot generation demands purposeful, film-like transitions and
strict cinematic continuity. Current methods, however, often prioritize basic
visual consistency, neglecting crucial editing patterns (e.g., shot/reverse
shot, cutaways) that drive narrative flow for compelling storytelling. This
yields outputs that may be visually coherent but lack narrative sophistication
and true cinematic integrity. To bridge this, we introduce Next Shot Generation
(NSG): synthesizing a subsequent, high-quality shot that critically conforms to
professional editing patterns while upholding rigorous cinematic continuity.
Our framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs
in-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This
strategy uses Relational Prompts to define overall context and inter-shot
editing styles. Individual Prompts then specify per-shot content and
cinematographic attributes. Together, these guide Cut2Next to generate
cinematically appropriate next shots. Architectural innovations, Context-Aware
Condition Injection (CACI) and Hierarchical Attention Mask (HAM), further
integrate these diverse signals without introducing new parameters. We
construct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with
hierarchical prompts, and introduce CutBench for evaluation. Experiments show
Cut2Next excels in visual consistency and text fidelity. Crucially, user
studies reveal a strong preference for Cut2Next, particularly for its adherence
to intended editing patterns and overall cinematic continuity, validating its
ability to generate high-quality, narratively expressive, and cinematically
coherent subsequent shots.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [493] [VGGSounder: Audio-Visual Evaluations for Foundation Models](https://arxiv.org/abs/2508.08237)
*Daniil Zverev,Thaddäus Wiedemer,Ameya Prabhu,Matthias Bethge,Wieland Brendel,A. Sophia Koepke*

Main category: cs.MM

TL;DR: 分析VGGSounder数据集局限性，提出重新标注的VGGSounder测试集并引入新指标揭示模型局限。


<details>
  <summary>Details</summary>
Motivation: 音频视觉基础模型需可靠评估多模态理解能力，但常用的VGGSounder数据集存在局限性，影响评估准确性。

Method: 引入全面重新标注的多标签测试集VGGSounder，添加详细模态注释；使用新的模态混淆指标分析添加输入模态时的性能下降。

Result: 提出了新的测试集VGGSounder，能进行精确的模态性能分析；用新指标揭示了模型的局限性。

Conclusion: 新的测试集和指标有助于更可靠地评估音频视觉基础模型的多模态理解能力。

Abstract: The emergence of audio-visual foundation models underscores the importance of
reliably assessing their multi-modal understanding. The VGGSounder dataset is
commonly used as a benchmark for evaluation audio-visual classification.
However, our analysis identifies several limitations of VGGSounder, including
incomplete labelling, partially overlapping classes, and misaligned modalities.
These lead to distorted evaluations of auditory and visual capabilities. To
address these limitations, we introduce VGGSounder, a comprehensively
re-annotated, multi-label test set that extends VGGSound and is specifically
designed to evaluate audio-visual foundation models. VGGSounder features
detailed modality annotations, enabling precise analyses of modality-specific
performance. Furthermore, we reveal model limitations by analysing performance
degradation when adding another input modality with our new modality confusion
metric.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [494] [Neural Beam Field for Spatial Beam RSRP Prediction](https://arxiv.org/abs/2508.06956)
*Keqiang Guo,Yuheng Zhong,Xin Tong,Jiangbin Lyu,Rui Zhang*

Main category: cs.IT

TL;DR: 提出Neural Beam Field (NBF)框架用于高效且可解释的空间波束RSRP预测，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测波束级参考信号接收功率对密集多用户无线网络的波束管理至关重要，但因高测量开销和快速信道变化而具有挑战性。

Method: 引入Multi - path Conditional Power Profile (MCPP)，采用解耦的“黑盒 - 白盒”设计，用Transformer - 基DNN学习MCPP，物理启发模块推断波束RSRP统计，还引入Pretrain - and - Calibrate (PaC)策略。

Result: 广泛的仿真结果表明，NBF在预测精度、训练效率和泛化能力上显著优于传统基于表的信道知识地图和纯黑盒DNN，且模型规模紧凑。

Conclusion: 该框架为下一代密集无线网络的智能波束管理提供了可扩展且基于物理的解决方案。

Abstract: Accurately predicting beam-level reference signal received power (RSRP) is
essential for beam management in dense multi-user wireless networks, yet
challenging due to high measurement overhead and fast channel variations. This
paper proposes Neural Beam Field (NBF), a hybrid neural-physical framework for
efficient and interpretable spatial beam RSRP prediction. Central to our
approach is the introduction of the Multi-path Conditional Power Profile
(MCPP), which bridges site-specific multipath propagation with antenna/beam
configurations via closed-form analytical modeling. We adopt a decoupled
``blackbox-whitebox" design: a Transformer-based deep neural network (DNN)
learns the MCPP from sparse user measurements and positions, while a
physics-inspired module analytically infers beam RSRP statistics. To improve
convergence and adaptivity, we further introduce a Pretrain-and-Calibrate (PaC)
strategy that leverages ray-tracing priors and on-site calibration using RSRP
data. Extensive simulations results demonstrate that NBF significantly
outperforms conventional table-based channel knowledge maps (CKMs) and pure
blackbox DNNs in prediction accuracy, training efficiency, and generalization,
while maintaining a compact model size. The proposed framework offers a
scalable and physically grounded solution for intelligent beam management in
next-generation dense wireless networks.

</details>


### [495] [Neural Channel Knowledge Map Assisted Scheduling Optimization of Active IRSs in Multi-User Systems](https://arxiv.org/abs/2508.07009)
*Xintong Chen,Zhenyu Jiang,Jiangbin Lyu,Liqun Fu*

Main category: cs.IT

TL;DR: 本文针对智能反射面（IRS）在多用户系统中的调度挑战，提出基于神经信道知识图（CKM）的调度框架和低复杂度调度算法，经评估可提升性能并降低复杂度。


<details>
  <summary>Details</summary>
Motivation: IRS在下一代无线网络有性能提升潜力，但面临双路径损耗和复杂多用户调度挑战，主动IRS虽部分解决路径损耗，但多IRS多用户系统仍需高效调度，且随着用户密度和信道维度增加，信道状态获取开销/延迟和调度复杂度剧增。

Method: 提出基于神经信道知识图（CKM）的调度框架，设计基于Transformer的深度神经网络（DNNs），通过两个级联网络LPS - Net和SE - Net分别预测链路功率统计（LPS）和遍历频谱效率（SE），还提出低复杂度的稳定匹配 - 迭代平衡（SM - IB）调度算法。

Result: 数值评估表明，神经CKM显著提高预测准确性和计算效率，SM - IB算法能有效实现接近最优的最大最小吞吐量，且复杂度大幅降低。

Conclusion: 所提出的基于神经CKM的调度框架和SM - IB调度算法能有效应对IRS在多用户系统中的调度挑战，提升系统性能。

Abstract: Intelligent Reflecting Surfaces (IRSs) have potential for significant
performance gains in next-generation wireless networks but face key challenges,
notably severe double-pathloss and complex multi-user scheduling due to
hardware constraints. Active IRSs partially address pathloss but still require
efficient scheduling in cell-level multi-IRS multi-user systems, whereby the
overhead/delay of channel state acquisition and the scheduling complexity both
rise dramatically as the user density and channel dimensions increase.
Motivated by these challenges, this paper proposes a novel scheduling framework
based on neural Channel Knowledge Map (CKM), designing Transformer-based deep
neural networks (DNNs) to predict ergodic spectral efficiency (SE) from
historical channel/throughput measurements tagged with user positions.
Specifically, two cascaded networks, LPS-Net and SE-Net, are designed to
predict link power statistics (LPS) and ergodic SE accurately. We further
propose a low-complexity Stable Matching-Iterative Balancing (SM-IB) scheduling
algorithm. Numerical evaluations verify that the proposed neural CKM
significantly enhances prediction accuracy and computational efficiency, while
the SM-IB algorithm effectively achieves near-optimal max-min throughput with
greatly reduced complexity.

</details>


### [496] [Communication-Learning Co-Design for Differentially Private Over-the-Air Federated Distillation](https://arxiv.org/abs/2508.06557)
*Zihao Hu,Jia Yan,Ying-Jun Angela Zhang*

Main category: cs.IT

TL;DR: 提出差分隐私的空中联邦蒸馏框架，解决传统联邦学习通信效率和隐私保护问题，实现更好学习 - 隐私权衡并降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有学习模型规模增长对传统联邦学习的通信效率和隐私保护提出挑战。

Method: 提出差分隐私的空中联邦蒸馏框架，研究通信 - 学习协同设计问题，推导学习收敛率和隐私损失，以闭式求解最优收发器设计和长期训练轮次决策。

Result: 数值结果表明该方法比传统联邦学习基准能实现更好的学习 - 隐私权衡，大幅降低通信开销。

Conclusion: 所提差分隐私的空中联邦蒸馏方法有效，能在满足要求下提升性能。

Abstract: The ever-growing learning model size nowadays challenges the communication
efficiency and privacy preservation of the traditional federated learning (FL).
In this paper, we propose a novel differentially private (DP) over-the-air
federated distillation (FD) framework, where wireless devices (WDs)
periodically share noise-perturbed model outputs with the parameter server by
harnessing the superposition property of multi-access channels. Accordingly,
over-the-air FD enables the shared responsibility of the DP preservation on the
low-dimensional disclosed signals among WDs. We study the
communication-learning co-design problem in differentially private over-the-air
FD, aiming to maximize the learning convergence rate while meeting the transmit
power and DP requirements of WDs. The main challenge is rooted in the
intractable learning and privacy analysis in over-the-air FD, together with the
strong coupling among the decision variables spanning two timescales. To tackle
this problem, we first derive the analytical learning convergence rate and
privacy losses of WDs, based on which the optimal transceiver design per FD
round and long-term training rounds decision are obtained in the closed forms.
Numerical results demonstrate that the proposed differentially private
over-the-air FD approach achieves a better learning-privacy trade-off with
largely-reduced communication overhead than the conventional FL benchmarks.

</details>


### [497] [Structured Superposition of Autoencoders for UEP Codes at Intermediate Blocklengths](https://arxiv.org/abs/2508.07487)
*Vukan Ninkovic,Dejan Vukobratovic*

Main category: cs.IT

TL;DR: 提出结构化基于自动编码器的架构扩展不等差错保护（UEP）码到更大块长，结果优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 自动编码器（AE）在UEP的应用，特别是中等块长下未充分探索，且AE模型复杂度增加。

Method: 将编码和解码构建为更小的AE子块，提供灵活框架调整UEP可靠性水平并适应不同系统参数。

Result: 所提方法优于基于随机叠加编码和连续干扰消除（SIC）解码的UEP方案的可达性边界。

Conclusion: 所提结构化基于AE的UEP码是下一代网络可扩展且高效的解决方案。

Abstract: Unequal error protection (UEP) coding that enables differentiated reliability
levels within a transmitted message is essential for modern communication
systems. Autoencoder (AE)-based code designs have shown promise in the context
of learned equal error protection (EEP) coding schemes. However, their
application to UEP remains largely unexplored, particularly at intermediate
blocklengths, due to the increasing complexity of AE-based models. Inspired by
the proven effectiveness of superposition coding and successive interference
cancellation (SIC) decoding in conventional UEP schemes, we propose a
structured AE-based architecture that extends AE-based UEP codes to
substantially larger blocklengths while maintaining efficient training. By
structuring encoding and decoding into smaller AE subblocks, our method
provides a flexible framework for fine-tuning UEP reliability levels while
adapting to diverse system parameters. Numerical results show that the proposed
approach improves over established achievability bounds of randomized
superposition coding-based UEP schemes with SIC decoding, making the proposed
structured AE-based UEP codes a scalable and efficient solution for
next-generation networks.

</details>


### [498] [Adaptive Source-Channel Coding for Semantic Communications](https://arxiv.org/abs/2508.07958)
*Dongxu Li,Kai Yuan,Jianhao Huang,Chuan Huang,Xiaoqi Qin,Shuguang Cui,Ping Zhang*

Main category: cs.IT

TL;DR: 提出用于并行高斯信道语义通信的自适应信源 - 信道编码（ASCC）方案，性能优于典型方案且与实际数字系统兼容。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信中联合信源 - 信道编码（JSCC）与现有通信系统不兼容、无法适应信源或信道变化，分离信源 - 信道编码（SSCC）在有限码长下性能欠佳。

Method: 分别部署并自适应设计基于深度神经网络的语义信源编码和传统数字信道编码；通过逻辑回归近似端到端数据和语义失真；建立加权和端到端失真最小化问题并通过逐次凸近似求解。

Result: 仿真表明，ASCC 方案在单信道和并行信道场景下均优于典型 JSCC 和 SSCC 方案。

Conclusion: 提出的 ASCC 方案性能优越且与实际数字系统完全兼容。

Abstract: Semantic communications (SemComs) have emerged as a promising paradigm for
joint data and task-oriented transmissions, combining the demands for both the
bit-accurate delivery and end-to-end (E2E) distortion minimization. However,
current joint source-channel coding (JSCC) in SemComs is not compatible with
the existing communication systems and cannot adapt to the variations of the
sources or the channels, while separate source-channel coding (SSCC) is
suboptimal in the finite blocklength regime. To address these issues, we
propose an adaptive source-channel coding (ASCC) scheme for SemComs over
parallel Gaussian channels, where the deep neural network (DNN)-based semantic
source coding and conventional digital channel coding are separately deployed
and adaptively designed. To enable efficient adaptation between the source and
channel coding, we first approximate the E2E data and semantic distortions as
functions of source coding rate and bit error ratio (BER) via logistic
regression, where BER is further modeled as functions of signal-to-noise ratio
(SNR) and channel coding rate. Then, we formulate the weighted sum E2E
distortion minimization problem for joint source-channel coding rate and power
allocation over parallel channels, which is solved by the successive convex
approximation. Finally, simulation results demonstrate that the proposed ASCC
scheme outperforms typical deep JSCC and SSCC schemes for both the single- and
parallel-channel scenarios while maintaining full compatibility with practical
digital systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [499] [Factor Augmented Supervised Learning with Text Embeddings](https://arxiv.org/abs/2508.06548)
*Zhanye Luo,Yuefeng Han,Xiufan Yu*

Main category: cs.CL

TL;DR: 本文提出AEALT框架解决大语言模型文本嵌入高维问题，实验表明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型文本嵌入的高维性阻碍下游任务效率并增加计算成本。

Method: 提出AEALT框架，先从文本中提取嵌入，再通过监督增强自动编码器学习低维、与任务相关的潜在因子。

Result: 在多个真实公共数据集的分类、异常检测和预测任务上实验，AEALT比原始嵌入和几种标准降维方法有显著提升。

Conclusion: AEALT通过对复杂嵌入的非线性结构建模，优于依赖原始嵌入的传统深度学习方法，具有广泛适用性。

Abstract: Large language models (LLMs) generate text embeddings from text data,
producing vector representations that capture the semantic meaning and
contextual relationships of words. However, the high dimensionality of these
embeddings often impedes efficiency and drives up computational cost in
downstream tasks. To address this, we propose AutoEncoder-Augmented Learning
with Text (AEALT), a supervised, factor-augmented framework that incorporates
dimension reduction directly into pre-trained LLM workflows. First, we extract
embeddings from text documents; next, we pass them through a supervised
augmented autoencoder to learn low-dimensional, task-relevant latent factors.
By modeling the nonlinear structure of complex embeddings, AEALT outperforms
conventional deep-learning approaches that rely on raw embeddings. We validate
its broad applicability with extensive experiments on classification, anomaly
detection, and prediction tasks using multiple real-world public datasets.
Numerical results demonstrate that AEALT yields substantial gains over both
vanilla embeddings and several standard dimension reduction methods.

</details>


### [500] [Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks](https://arxiv.org/abs/2508.07179)
*Jiaqi Yin,Yi-Wei Chen,Meng-Lung Lee,Xiya Liu*

Main category: cs.CL

TL;DR: 论文提出自动化提取企业多语言数据管道脚本细粒度模式沿袭的框架，引入评估指标SLiCE和新基准，实验表明模式沿袭提取性能与模型大小和提示技术有关，32B开源模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 企业数据管道存在语义漂移问题，影响数据可重复性、治理及相关服务效用，需解决该问题。

Method: 提出自动化提取细粒度模式沿袭的框架，识别四个关键组件；引入评估指标SLiCE；给出新基准；用12种语言模型做实验。

Result: 模式沿袭提取性能随模型大小和提示技术复杂度提升；32B开源模型在标准提示下性能可与GPT系列媲美。

Conclusion: 存在可扩展且经济的方法用于在实际应用中部署模式感知代理。

Abstract: Enterprise data pipelines, characterized by complex transformations across
multiple programming languages, often cause a semantic disconnect between
original metadata and downstream data. This "semantic drift" compromises data
reproducibility and governance, and impairs the utility of services like
retrieval-augmented generation (RAG) and text-to-SQL systems. To address this,
a novel framework is proposed for the automated extraction of fine-grained
schema lineage from multilingual enterprise pipeline scripts. This method
identifies four key components: source schemas, source tables, transformation
logic, and aggregation operations, creating a standardized representation of
data transformations. For the rigorous evaluation of lineage quality, this
paper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that
assesses both structural correctness and semantic fidelity. A new benchmark is
also presented, comprising 1,700 manually annotated lineages from real-world
industrial scripts. Experiments were conducted with 12 language models, from
1.3B to 32B small language models (SLMs) to large language models (LLMs) like
GPT-4o and GPT-4.1. The results demonstrate that the performance of schema
lineage extraction scales with model size and the sophistication of prompting
techniques. Specially, a 32B open-source model, using a single reasoning trace,
can achieve performance comparable to the GPT series under standard prompting.
This finding suggests a scalable and economical approach for deploying
schema-aware agents in practical applications.

</details>


### [501] [CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models](https://arxiv.org/abs/2508.06524)
*Lei Jiang,Fan Chen*

Main category: cs.CL

TL;DR: 本文提出CarbonScaling框架将碳排放纳入神经缩放定律，分析模型准确率与碳足迹关系，为训练可持续和碳高效大模型提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有神经缩放定律忽略了大语言模型训练中随模型规模指数增长的碳排放，需将碳排放纳入考虑。

Method: 提出CarbonScaling分析框架，整合神经缩放、GPU硬件演变、并行优化和碳估算模型。

Result: 准确率和碳排放存在幂律关系，现实低效性增加缩放因子；硬件技术缩放对中小模型减少碳排放，对超大模型效果递减；训练优化可缓解低效性。

Conclusion: CarbonScaling为训练更可持续和碳高效的大语言模型提供关键见解。

Abstract: Neural scaling laws have driven the development of increasingly large
language models (LLMs) by linking accuracy improvements to growth in parameter
count, dataset size, and compute. However, these laws overlook the carbon
emissions that scale exponentially with LLM size. This paper presents
\textit{CarbonScaling}, an analytical framework that extends neural scaling
laws to incorporate both operational and embodied carbon in LLM training. By
integrating models for neural scaling, GPU hardware evolution, parallelism
optimization, and carbon estimation, \textit{CarbonScaling} quantitatively
connects model accuracy to carbon footprint. Results show that while a
power-law relationship between accuracy and carbon holds, real-world
inefficiencies significantly increase the scaling factor. Hardware technology
scaling reduces carbon emissions for small to mid-sized models, but offers
diminishing returns for extremely large LLMs due to communication overhead and
underutilized GPUs. Training optimizations-especially aggressive critical batch
size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers
key insights for training more sustainable and carbon-efficient LLMs.

</details>


### [502] [Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction](https://arxiv.org/abs/2508.06495)
*Juliana Resplande Sant'anna Gomes,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: 本文针对葡萄牙语缺乏含外部证据的公开数据集问题，提出用大语言模型和搜索引擎API为新闻语料添加外部证据的方法，并引入数据验证和预处理框架。


<details>
  <summary>Details</summary>
Motivation: 虚假信息传播快，人工事实核查难，葡萄牙语缺乏含外部证据的公开数据集，难以开发强大的半自动事实核查（SAFC）系统。

Method: 使用大语言模型（Gemini 1.5 Flash）提取文本主要声明，用搜索引擎API检索相关外部文档，引入数据验证和预处理框架提升语料质量。

Result: 未提及。

Conclusion: 未提及。

Abstract: The accelerated dissemination of disinformation often outpaces the capacity
for manual fact-checking, highlighting the urgent need for Semi-Automated
Fact-Checking (SAFC) systems. Within the Portuguese language context, there is
a noted scarcity of publicly available datasets that integrate external
evidence, an essential component for developing robust AFC systems, as many
existing resources focus solely on classification based on intrinsic text
features. This dissertation addresses this gap by developing, applying, and
analyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR,
MuMiN-PT) with external evidence. The approach simulates a user's verification
process, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash)
to extract the main claim from texts and search engine APIs (Google Search API,
Google FactCheck Claims Search API) to retrieve relevant external documents
(evidence). Additionally, a data validation and preprocessing framework,
including near-duplicate detection, is introduced to enhance the quality of the
base corpora.

</details>


### [503] [BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent](https://arxiv.org/abs/2508.06600)
*Zijian Chen,Xueguang Ma,Shengyao Zhuang,Ping Nie,Kai Zou,Andrew Liu,Joshua Green,Kshama Patel,Ruoxi Meng,Mingyi Su,Sahel Sharifymoghaddam,Yanxi Li,Haoran Hong,Xinyu Shi,Xuye Liu,Nandan Thakur,Crystina Zhang,Luyu Gao,Wenhu Chen,Jimmy Lin*

Main category: cs.CL

TL;DR: 提出基于BrowseComp的基准测试BrowseComp-Plus，解决现有基准测试问题并有效区分深度研究系统性能


<details>
  <summary>Details</summary>
Motivation: 现有基于黑盒实时网络搜索API的基准测试在公平性和透明度上有局限，难以对深度研究大语言模型能力进行可控实验

Method: 引入基于BrowseComp的BrowseComp-Plus基准测试，使用固定精心策划的语料库，每个查询包含人工验证的支持文档和挖掘的挑战性负样本

Result: 该基准测试有效区分系统性能，如Search - R1搭配BM25检索器准确率3.86%，GPT - 5为55.9%，GPT - 5搭配Qwen3 - Embedding - 8B检索器准确率达70.1%且搜索调用更少

Conclusion: 该基准测试可对深度研究代理和检索方法进行全面评估和解耦分析，助力深入了解深度研究系统的检索有效性、引用准确性和上下文工程

Abstract: Deep-Research agents, which integrate large language models (LLMs) with
search tools, have shown success in improving the effectiveness of handling
complex queries that require iterative search planning and reasoning over
search results. Evaluations on current benchmarks like BrowseComp relies on
black-box live web search APIs, have notable limitations in (1) fairness:
dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep
research methods; (2) transparency: lack of control over the document corpus
makes it difficult to isolate retriever contributions. In other words, the
current evaluations may compare a complete deep research system at a given
time, but they do not foster well-controlled experiments to provide insights
into the capability of underlying deep research LLMs. To address these
challenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,
employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus
includes human-verified supporting documents and mined challenging negatives,
enabling controlled experimentation. The benchmark is shown to be effective in
distinguishing the performance of deep research systems. For instance, the
open-source model Search-R1, when paired with the BM25 retriever, achieves
3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with
the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with
fewer search calls. This benchmark allows comprehensive evaluation and
disentangled analysis of deep research agents and retrieval methods, fostering
insights into retrieval effectiveness, citation accuracy, and context
engineering in Deep-Research system.

</details>


### [504] [The ReQAP System for Question Answering over Personal Information](https://arxiv.org/abs/2508.06880)
*Philipp Christmann,Gerhard Weikum*

Main category: cs.CL

TL;DR: 本文介绍ReQAP系统，可支持用户回答涉及异构源过滤、连接和聚合的复杂问题，有递归分解问题和增量构建操作树的特点，演示展示其功能和答案计算过程。


<details>
  <summary>Details</summary>
Motivation: 解决用户对设备上异构源数据提出复杂问题获取答案的需求。

Method: 递归分解问题，增量构建操作树执行，问题解释和单个操作利用轻量级语言模型并微调。

Result: 演示展示了系统针对高级用户问题的丰富功能，可详细追踪答案计算过程。

Conclusion: 能够将答案追溯到源数据对提高系统的可理解性和用户信任至关重要。

Abstract: Personal information is abundant on users' devices, from structured data in
calendar, shopping records or fitness tools, to unstructured contents in mail
and social media posts. This works presents the ReQAP system that supports
users with answers for complex questions that involve filters, joins and
aggregation over heterogeneous sources. The unique trait of ReQAP is that it
recursively decomposes questions and incrementally builds an operator tree for
execution. Both the question interpretation and the individual operators make
smart use of light-weight language models, with judicious fine-tuning. The demo
showcases the rich functionality for advanced user questions, and also offers
detailed tracking of how the answers are computed by the operators in the
execution tree. Being able to trace answers back to the underlying sources is
vital for human comprehensibility and user trust in the system.

</details>


### [505] [Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction](https://arxiv.org/abs/2508.06971)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Khaled Shaban,Hozaifa Kassab*

Main category: cs.CL

TL;DR: 提出两阶段框架处理古兰经问答问题，在相关任务上取得SOTA结果，证明结合模型集成和指令调优语言模型有效。


<details>
  <summary>Details</summary>
Motivation: 古兰经问答因古典阿拉伯语的语言复杂性和宗教文本的语义丰富性带来独特挑战。

Method: 提出两阶段框架，在段落检索时集成微调的阿拉伯语语言模型，在答案提取时使用少量样本提示的指令调优大语言模型。

Result: 在Quran QA 2023共享任务上取得SOTA结果，检索的MAP@10为0.3128、MRR@10为0.5763，提取的pAP@10为0.669，大幅超越先前方法。

Conclusion: 结合模型集成和指令调优语言模型能有效应对专业领域低资源问答挑战。

Abstract: Quranic Question Answering presents unique challenges due to the linguistic
complexity of Classical Arabic and the semantic richness of religious texts. In
this paper, we propose a novel two-stage framework that addresses both passage
retrieval and answer extraction. For passage retrieval, we ensemble fine-tuned
Arabic language models to achieve superior ranking performance. For answer
extraction, we employ instruction-tuned large language models with few-shot
prompting to overcome the limitations of fine-tuning on small datasets. Our
approach achieves state-of-the-art results on the Quran QA 2023 Shared Task,
with a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of
0.669 for extraction, substantially outperforming previous methods. These
results demonstrate that combining model ensembling and instruction-tuned
language models effectively addresses the challenges of low-resource question
answering in specialized domains.

</details>


### [506] [Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking](https://arxiv.org/abs/2508.07286)
*Jian Chen,Jinbao Tian,Yankui Li,Zhou Li*

Main category: cs.CL

TL;DR: 提出ARCE方法解决AEC领域NER问题，实验取得新SOTA，简单解释型知识更有效。


<details>
  <summary>Details</summary>
Motivation: 标准预训练模型在AEC领域因领域差距表现受限，进一步预训练人力成本高，如何用LLM生成知识提升小模型待探索。

Method: 提出ARCE方法，用LLM生成简单解释语料Cote，用其对RoBERTa模型增量预训练后微调。

Result: ARCE在AEC基准数据集上达到新SOTA，Macro - F1分数77.20%。

Conclusion: 简单的基于解释的知识比复杂的基于角色的推理在该任务中更有效。

Abstract: Accurate information extraction from specialized texts is a critical
challenge, particularly for named entity recognition (NER) in the architecture,
engineering, and construction (AEC) domain to support automated rule checking
(ARC). The performance of standard pre-trained models is often constrained by
the domain gap, as they struggle to interpret the specialized terminology and
complex relational contexts inherent in AEC texts. Although this issue can be
mitigated by further pre-training on large, human-curated domain corpora, as
exemplified by methods like ARCBERT, this approach is both labor-intensive and
cost-prohibitive. Consequently, leveraging large language models (LLMs) for
automated knowledge generation has emerged as a promising alternative. However,
the optimal strategy for generating knowledge that can genuinely enhance
smaller, efficient models remains an open question. To address this, we propose
ARCE (augmented RoBERTa with contextualized elucidations), a novel approach
that systematically explores and optimizes this generation process. ARCE
employs an LLM to first generate a corpus of simple, direct explanations, which
we term Cote, and then uses this corpus to incrementally pre-train a RoBERTa
model prior to its fine-tuning on the downstream task. Our extensive
experiments show that ARCE establishes a new state-of-the-art on a benchmark
AEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a
key finding: simple, explanation-based knowledge proves surprisingly more
effective than complex, role-based rationales for this task. The code is
publicly available at:https://github.com/nxcc-lab/ARCE.

</details>


### [507] [HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways](https://arxiv.org/abs/2508.07308)
*Cristian Cosentino,Annamaria Defilippo,Marco Dossena,Christopher Irwin,Sara Joubbi,Pietro Liò*

Main category: cs.CL

TL;DR: 提出医疗问答基准数据集HealthBranches，用于评估大语言模型复杂推理能力，有多种特性和用途。


<details>
  <summary>Details</summary>
Motivation: 设计新的医疗问答基准数据集来评估大语言模型在医疗领域的复杂推理能力。

Method: 通过半自动化流程将医学源中的明确决策路径转化为实际患者案例及相关问答。

Result: 生成涵盖17个医疗主题4063个案例研究的数据集，支持多种问答格式且包含完整推理路径。

Conclusion: HealthBranches为高风险领域开发更可靠、可解释和临床可靠的大语言模型奠定基础，也可用于教育。

Abstract: HealthBranches is a novel benchmark dataset for medical Question-Answering
(Q&A), specifically designed to evaluate complex reasoning in Large Language
Models (LLMs). This dataset is generated through a semi-automated pipeline that
transforms explicit decision pathways from medical source into realistic
patient cases with associated questions and answers. Covering 4,063 case
studies across 17 healthcare topics, each data point is based on clinically
validated reasoning chains. HealthBranches supports both open-ended and
multiple-choice question formats and uniquely includes the full reasoning path
for each Q&A. Its structured design enables robust evaluation of LLMs'
multi-step inference capabilities, including their performance in structured
Retrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a
foundation for the development of more trustworthy, interpretable, and
clinically reliable LLMs in high-stakes domains while also serving as a
valuable resource for educational purposes.

</details>


### [508] [Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models](https://arxiv.org/abs/2508.06504)
*Yao Ge,Sudeshna Das,Yuting Guo,Abeed Sarker*

Main category: cs.CL

TL;DR: 本文研究动态提示策略提升大语言模型在少样本生物医学命名实体识别任务中的性能，实验表明动态提示效果更佳。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在少样本生物医学命名实体识别中的性能挑战。

Method: 采用检索增强生成的动态提示策略，根据输入文本相似度选择上下文学习示例并动态更新提示，实现并优化静态和动态提示工程技术。

Result: 结构化组件的静态提示使GPT - 4、GPT - 3.5和LLaMA 3 - 70B的平均F1分数提升，动态提示进一步提高性能，TF - IDF和SBERT检索方法效果最佳。

Conclusion: 基于检索增强生成的上下文自适应提示对生物医学命名实体识别有用。

Abstract: Biomedical named entity recognition (NER) is a high-utility natural language
processing (NLP) task, and large language models (LLMs) show promise
particularly in few-shot settings (i.e., limited training data). In this
article, we address the performance challenges of LLMs for few-shot biomedical
NER by investigating a dynamic prompting strategy involving retrieval-augmented
generation (RAG). In our approach, the annotated in-context learning examples
are selected based on their similarities with the input texts, and the prompt
is dynamically updated for each instance during inference. We implemented and
optimized static and dynamic prompt engineering techniques and evaluated them
on five biomedical NER datasets. Static prompting with structured components
increased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA
3-70B, relative to basic static prompting. Dynamic prompting further improved
performance, with TF-IDF and SBERT retrieval methods yielding the best results,
improving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings,
respectively. These findings highlight the utility of contextually adaptive
prompts via RAG for biomedical NER.

</details>


### [509] [The Art of Breaking Words: Rethinking Multilingual Tokenizer Design](https://arxiv.org/abs/2508.06533)
*Aamod Thakur,Ajay Nagpal,Atharva Savarkar,Kundeshwar Pundalik,Siddhesh Dosi,Piyush Sawarkar,Viraj Thakur,Rohit Saluja,Maunendra Sankar Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 本文研究大语言模型分词，对印地语脚本实验，提出数据组合算法，提升了模型性能和推理速度。


<details>
  <summary>Details</summary>
Motivation: 分词在大语言模型开发中被相对忽视，现有分词器存在高词符比、上下文长度利用低效和推理慢等问题。

Method: 系统研究词汇大小、预分词规则和训练语料组成与词符效率及模型质量的关系，对印地语脚本进行广泛实验，提出新型数据组合算法。

Result: 预分词策略显著提升模型性能，数据组合算法使平均词符比相对于传统方法降低约6%，比现有多语言印地语模型平均词符比提高超40%。

Conclusion: 分词与架构和训练目标一样，是构建高效、可扩展多语言大语言模型的关键因素。

Abstract: While model architecture and training objectives are well-studied,
tokenization, particularly in multilingual contexts, remains a relatively
neglected aspect of Large Language Model (LLM) development. Existing tokenizers
often exhibit high token-to-word ratios, inefficient use of context length, and
slower inference. We present a systematic study that links vocabulary size,
pre-tokenization rules, and training-corpus composition to both token-to-word
efficiency and model quality. To ground our analysis in a linguistically
diverse context, we conduct extensive experiments on Indic scripts, which
present unique challenges due to their high script diversity and orthographic
complexity. Drawing on the insights from these analyses, we propose a novel
algorithm for data composition that balances multilingual data for tokenizer
training. Our observations on pretokenization strategies significantly improve
model performance, and our data composition algorithm reduces the average
token-to-word ratio by approximately 6% with respect to the conventional data
randomization approach. Our tokenizer achieves more than 40% improvement on
average token-to-word ratio against stateof-the-art multilingual Indic models.
This improvement yields measurable gains in both model performance and
inference speed. This highlights tokenization alongside architecture and
training objectives as a critical lever for building efficient, scalable
multilingual LLMs

</details>


### [510] [Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs](https://arxiv.org/abs/2508.06583)
*Ying Liu,Can Li,Ting Zhang,Mei Wang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 研究从单纯问题生成转向教学指导能力评估，提出GuideEval基准，发现现有大语言模型在自适应支持上不足，引入微调策略提升性能，倡导以学习者为中心的评估范式。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注大语言模型的苏格拉底式提问能力，忽略基于学习者认知状态的自适应指导，本研究旨在探究其能否像专家导师一样动态调整策略。

Method: 提出基于真实教育对话的GuideEval基准，通过感知、编排、启发三个阶段的行为框架评估教学指导，还引入行为引导的微调策略。

Result: 现有大语言模型在学习者困惑或需引导时，常无法提供有效自适应支持；行为引导的微调策略显著提升指导性能。

Conclusion: 应从孤立内容评估转向以学习者为中心的交互评估，倡导更对话式的评估范式。

Abstract: The conversational capabilities of large language models hold significant
promise for enabling scalable and interactive tutoring. While prior research
has primarily examined their capacity for Socratic questioning, it often
overlooks a critical dimension: adaptively guiding learners based on their
cognitive states. This study shifts focus from mere question generation to the
broader instructional guidance capability. We ask: Can LLMs emulate expert
tutors who dynamically adjust strategies in response to learners'
understanding? To investigate this, we propose GuideEval, a benchmark grounded
in authentic educational dialogues that evaluates pedagogical guidance through
a three-phase behavioral framework: (1) Perception, inferring learner states;
(2) Orchestration, adapting instructional strategies; and (3) Elicitation,
stimulating proper reflections. Empirical findings reveal that existing LLMs
frequently fail to provide effective adaptive scaffolding when learners exhibit
confusion or require redirection. Furthermore, we introduce a behavior-guided
finetuning strategy that leverages behavior-prompted instructional dialogues,
significantly enhancing guidance performance. By shifting the focus from
isolated content evaluation to learner-centered interaction, our work advocates
a more dialogic paradigm for evaluating Socratic LLMs.

</details>


### [511] [LLM Unlearning Without an Expert Curated Dataset](https://arxiv.org/abs/2508.06595)
*Xiaoyuan Zhu,Muru Zhang,Ollie Liu,Robin Jia,Willie Neiswanger*

Main category: cs.CL

TL;DR: 提出用语言模型自动生成高质量遗忘集的方法，实验表明合成数据集表现佳，为无需人工干预的实用可扩展遗忘学习提供路径。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型需事后遗忘特定知识，但当前遗忘学习流程构建有效遗忘集存在瓶颈。

Method: 通过结构化提示管道合成教科书式数据，仅需领域名称作为输入。

Result: 合成数据集在生物安全、网络安全和《哈利·波特》小说遗忘学习实验中，表现优于基线合成替代方案，与专家策划的数据集相当；多步生成管道可提升数据多样性和遗忘学习效用。

Conclusion: 合成数据集为广泛新兴领域的实用可扩展遗忘学习提供了有前景的途径，无需人工干预。

Abstract: Modern large language models often encode sensitive, harmful, or copyrighted
knowledge, raising the need for post-hoc unlearning-the ability to remove
specific domains of knowledge from a model without full retraining. A major
bottleneck in current unlearning pipelines is constructing effective forget
sets-datasets that approximate the target domain and guide the model to forget
it. In this work, we introduce a scalable, automated approach to generate
high-quality forget sets using language models themselves. Our method
synthesizes textbook-style data through a structured prompting pipeline,
requiring only a domain name as input. Through experiments on unlearning
biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic
datasets consistently outperform the baseline synthetic alternatives and are
comparable to the expert-curated ones. Additionally, ablation studies reveal
that the multi-step generation pipeline significantly boosts data diversity,
which in turn improves unlearning utility. Overall, our findings suggest that
synthetic datasets offer a promising path toward practical, scalable unlearning
for a wide range of emerging domains without the need for manual intervention.
We release our code and dataset at
https://github.com/xyzhu123/Synthetic_Textbook.

</details>


### [512] [Do Biased Models Have Biased Thoughts?](https://arxiv.org/abs/2508.06671)
*Swati Rajwal,Shivank Garg,Reem Abdel-Salam,Abdelrahman Zayed*

Main category: cs.CL

TL;DR: 研究基于思维链提示对语言模型公平性的影响，发现模型思维步骤中的偏差与输出偏差相关性低。


<details>
  <summary>Details</summary>
Motivation: 语言模型存在基于性别、种族等方面的偏差，影响其部署，因此研究思维链提示对公平性的影响。

Method: 对5个流行的大语言模型进行实验，使用公平性指标量化模型思维和输出中的11种不同偏差。

Result: 思维步骤中的偏差与输出偏差相关性不高（多数情况下相关性小于0.6，p值小于0.001）。

Conclusion: 与人类不同，有偏差决策的测试模型并不总是有偏差的思维。

Abstract: The impressive performance of language models is undeniable. However, the
presence of biases based on gender, race, socio-economic status, physical
appearance, and sexual orientation makes the deployment of language models
challenging. This paper studies the effect of chain-of-thought prompting, a
recent approach that studies the steps followed by the model before it
responds, on fairness. More specifically, we ask the following question:
\textit{Do biased models have biased thoughts}? To answer our question, we
conduct experiments on $5$ popular large language models using fairness metrics
to quantify $11$ different biases in the model's thoughts and output. Our
results show that the bias in the thinking steps is not highly correlated with
the output bias (less than $0.6$ correlation with a $p$-value smaller than
$0.001$ in most cases). In other words, unlike human beings, the tested models
with biased decisions do not always possess biased thoughts.

</details>


### [513] [Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge](https://arxiv.org/abs/2508.06709)
*Evangelia Spiliopoulou,Riccardo Fogliato,Hanna Burnsky,Tamer Soliman,Jie Ma,Graham Horwood,Miguel Ballesteros*

Main category: cs.CL

TL;DR: 提出统计框架识别和估计大语言模型自评偏差，分析数据集发现部分模型存在自评和家族偏差，给出减轻偏差建议。


<details>
  <summary>Details</summary>
Motivation: 先前研究常混淆模型质量差异与偏差，且错误假设大语言模型和人类评估评分分布相同，需解决模型自评偏差问题。

Method: 提出统计框架，建模大语言模型作为评判者对自身和其他模型输出评分分布差异，结合第三方评判者考量输出质量。

Result: 对超5000个提示 - 完成对数据集分析，发现GPT - 4o和Claude 3.5 Sonnet等模型有自评和家族偏差。

Conclusion: 指出使用大语言模型评判的潜在陷阱，提供减轻偏差实用指导。

Abstract: Large language models (LLMs) can serve as judges that offer rapid and
reliable assessments of other LLM outputs. However, models may systematically
assign overly favorable ratings to their own outputs, a phenomenon known as
self-bias, which can distort evaluations of true model performance. Previous
studies often conflate genuine differences in model quality with bias or
incorrectly assume that evaluations from LLMs and humans follow the same rating
distributions. In this work, we present a statistical framework that explicitly
formalizes assumptions under which self-bias can be identified and estimated.
Our method models the difference in the scoring distribution that
LLM-as-a-judge assigns to its own completions compared to other models, while
accounting for the underlying quality of the completions provided by an
independent, third-party judge (e.g., humans). Our method reliably isolates and
quantifies self-bias, even when models vary in ability, ensuring that genuine
performance differences are not mistaken for self-bias. We conduct an empirical
analysis of self-bias on a large dataset (>5000 prompt-completion pairs)
consisting of expert human annotations and judgments from nine different LLM
judges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,
systematically assign higher scores to their own outputs. These models also
display family-bias; systematically assigning higher ratings to outputs
produced by other models of the same family. Our findings highlight potential
pitfalls of using LLM judges and offer practical guidance to mitigate biases
when interpreting automated evaluations.

</details>


### [514] [Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis](https://arxiv.org/abs/2508.06729)
*Komala Subramanyam Cherukuri,Pranav Abishai Moses,Aisa Sakata,Jiangping Chen,Haihua Chen*

Main category: cs.CL

TL;DR: 本文提出可扩展框架为日裔美国人监禁口述历史自动进行语义和情感标注，用LLM构建数据集、评估模型和测试提示工程策略，结果显示LLM在设计良好的提示下能有效标注，还提供了可复用流程和应用指导。


<details>
  <summary>Details</summary>
Motivation: 大规模分析口述历史档案因非结构化格式、情感复杂性和高标注成本受限，需有效方法促进对口述历史的访问和理解。

Method: 构建高质量数据集，评估多个模型，测试提示工程策略，采用多阶段方法结合专家标注、提示设计和用ChatGPT、Llama、Qwen进行LLM评估，对句子进行情感和语义分类标注，评估零样本、少样本和RAG策略。

Result: 语义分类中ChatGPT的F1分数最高，情感分析中Llama稍胜一筹，各模型结果相近，用最佳提示配置标注JAIOH集合中的句子。

Conclusion: LLM在设计良好的提示下能对大型口述历史集合进行有效语义和情感标注，提供了可复用标注流程和在文化敏感档案分析中应用LLM的实用指导，为数字人文中负责任使用人工智能和集体记忆保存奠定基础。

Abstract: Oral histories are vital records of lived experience, particularly within
communities affected by systemic injustice and historical erasure. Effective
and efficient analysis of their oral history archives can promote access and
understanding of the oral histories. However, Large-scale analysis of these
archives remains limited due to their unstructured format, emotional
complexity, and high annotation costs. This paper presents a scalable framework
to automate semantic and sentiment annotation for Japanese American
Incarceration Oral History. Using LLMs, we construct a high-quality dataset,
evaluate multiple models, and test prompt engineering strategies in
historically sensitive contexts. Our multiphase approach combines expert
annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We
labeled 558 sentences from 15 narrators for sentiment and semantic
classification, then evaluated zero-shot, few-shot, and RAG strategies. For
semantic classification, ChatGPT achieved the highest F1 score (88.71%),
followed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama
slightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models
showing comparable results. The best prompt configurations were used to
annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our
findings show that LLMs can effectively perform semantic and sentiment
annotation across large oral history collections when guided by well-designed
prompts. This study provides a reusable annotation pipeline and practical
guidance for applying LLMs in culturally sensitive archival analysis. By
bridging archival ethics with scalable NLP techniques, this work lays the
groundwork for responsible use of artificial intelligence in digital humanities
and preservation of collective memory. GitHub:
https://github.com/kc6699c/LLM4OralHistoryAnalysis.

</details>


### [515] [Many-Turn Jailbreaking](https://arxiv.org/abs/2508.06755)
*Xianjun Yang,Liqiang Xiao,Shiyang Li,Faisal Ladhak,Hyokun Yun,Linda Ruth Petzold,Yi Xu,William Yang Wang*

Main category: cs.CL

TL;DR: 提出探索大语言模型多轮越狱攻击，构建MTJ - Bench基准测试集并揭示新漏洞，呼吁构建更安全模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型越狱工作多为单轮，而先进模型可进行多轮对话，多轮越狱威胁更严重，需探索。

Method: 构建Multi - Turn Jailbreak Benchmark（MTJ - Bench）在一系列开源和闭源模型上进行基准测试。

Result: 揭示了大语言模型多轮越狱这一新的安全漏洞。

Conclusion: 呼吁社区共同努力构建更安全的大语言模型，为深入理解大语言模型越狱提供基础。

Abstract: Current jailbreaking work on large language models (LLMs) aims to elicit
unsafe outputs from given prompts. However, it only focuses on single-turn
jailbreaking targeting one specific query. On the contrary, the advanced LLMs
are designed to handle extremely long contexts and can thus conduct multi-turn
conversations. So, we propose exploring multi-turn jailbreaking, in which the
jailbroken LLMs are continuously tested on more than the first-turn
conversation or a single target query. This is an even more serious threat
because 1) it is common for users to continue asking relevant follow-up
questions to clarify certain jailbroken details, and 2) it is also possible
that the initial round of jailbreaking causes the LLMs to respond to additional
irrelevant questions consistently. As the first step (First draft done at June
2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak
Benchmark (MTJ-Bench) for benchmarking this setting on a series of open- and
closed-source models and provide novel insights into this new safety threat. By
revealing this new vulnerability, we aim to call for community efforts to build
safer LLMs and pave the way for a more in-depth understanding of jailbreaking
LLMs.

</details>


### [516] [ESNERA: Empirical and semantic named entity alignment for named entity dataset merging](https://arxiv.org/abs/2508.06877)
*Xiaobo Zhang,Congqing He,Ying He,Jian Peng,Dajie Fu,Tien-Ping Tan*

Main category: cs.CL

TL;DR: 本文提出基于标签相似度的自动标签对齐方法解决NER数据集合并问题，实验证明该方法有效提升低资源金融领域NER性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习的NER依赖大量高质量标注数据集，但构建成本高，现有数据集合并方法缺乏可解释性和可扩展性。

Method: 提出基于标签相似度的自动标签对齐方法，结合经验和语义相似度，采用贪婪成对合并策略统一不同数据集的标签空间。

Result: 将三个现有NER数据集合并成统一语料库，与金融领域自建小数据集集成，提升了低资源金融领域NER性能。

Conclusion: 该研究为多源NER语料集成提供了高效、可解释和可扩展的解决方案。

Abstract: Named Entity Recognition (NER) is a fundamental task in natural language
processing. It remains a research hotspot due to its wide applicability across
domains. Although recent advances in deep learning have significantly improved
NER performance, they rely heavily on large, high-quality annotated datasets.
However, building these datasets is expensive and time-consuming, posing a
major bottleneck for further research. Current dataset merging approaches
mainly focus on strategies like manual label mapping or constructing label
graphs, which lack interpretability and scalability. To address this, we
propose an automatic label alignment method based on label similarity. The
method combines empirical and semantic similarities, using a greedy pairwise
merging strategy to unify label spaces across different datasets. Experiments
are conducted in two stages: first, merging three existing NER datasets into a
unified corpus with minimal impact on NER performance; second, integrating this
corpus with a small-scale, self-built dataset in the financial domain. The
results show that our method enables effective dataset merging and enhances NER
performance in the low-resource financial domain. This study presents an
efficient, interpretable, and scalable solution for integrating multi-source
NER corpora.

</details>


### [517] [SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages](https://arxiv.org/abs/2508.07069)
*Muhammad Dehan Al Kautsar,Aswin Candra,Muhammad Alif Al Hakim,Maxalmina Satria Kahfi,Fajri Koto,Alham Fikri Aji,Peerat Limkonchotiwat,Ekapol Chuangsuwanich,Genta Indra Winata*

Main category: cs.CL

TL;DR: 提出以东南亚为中心的文化对话数据集SEADialogues，含多语言和文化主题，推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有闲聊数据集忽略自然对话中的文化细微差别，需开发考虑文化因素的数据集。

Method: 创建以东南亚为中心的SEADialogues数据集，涵盖六国八种语言，对话包含人物属性和文化主题。

Result: 发布了多轮对话数据集。

Conclusion: 该数据集有助于推进文化感知和以人为本的大语言模型研究。

Abstract: Although numerous datasets have been developed to support dialogue systems,
most existing chit-chat datasets overlook the cultural nuances inherent in
natural human conversations. To address this gap, we introduce SEADialogues, a
culturally grounded dialogue dataset centered on Southeast Asia, a region with
over 700 million people and immense cultural diversity. Our dataset features
dialogues in eight languages from six Southeast Asian countries, many of which
are low-resource despite having sizable speaker populations. To enhance
cultural relevance and personalization, each dialogue includes persona
attributes and two culturally grounded topics that reflect everyday life in the
respective communities. Furthermore, we release a multi-turn dialogue dataset
to advance research on culturally aware and human-centric large language
models, including conversational dialogue agents.

</details>


### [518] [Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning](https://arxiv.org/abs/2508.07101)
*Lijie Yang,Zhihao Zhang,Arti Jain,Shijie Cao,Baihong Yuan,Yiwei Chen,Zhihao Jia,Ravi Netravali*

Main category: cs.CL

TL;DR: 提出无训练稀疏注意力机制LessIsMore，在推理任务中提升效率且不损失精度。


<details>
  <summary>Details</summary>
Motivation: 大推理模型测试时计算开销大，现有稀疏注意力机制存在精度下降、需高保留率或昂贵再训练问题。

Method: 引入LessIsMore，利用全局注意力模式，聚合局部注意力头的token选择和上下文信息进行统一token排序。

Result: 在不同推理任务和基准测试中，相比全注意力解码速度提升1.1倍，相比现有稀疏注意力方法端到端速度提升1.13倍，且不损失精度。

Conclusion: LessIsMore能在推理任务中有效提升效率，同时保持或提高精度。

Abstract: Large reasoning models achieve strong performance through test-time scaling
but incur substantial computational overhead, particularly from excessive token
generation when processing short input prompts. While sparse attention
mechanisms can reduce latency and memory usage, existing approaches suffer from
significant accuracy degradation due to accumulated errors during
long-generation reasoning. These methods generally require either high token
retention rates or expensive retraining. We introduce LessIsMore, a
training-free sparse attention mechanism for reasoning tasks, which leverages
global attention patterns rather than relying on traditional head-specific
local optimizations. LessIsMore aggregates token selections from local
attention heads with recent contextual information, enabling unified cross-head
token ranking for future decoding layers. This unified selection improves
generalization and efficiency by avoiding the need to maintain separate token
subsets per head. Evaluation across diverse reasoning tasks and benchmarks
shows that LessIsMore preserves -- and in some cases improves -- accuracy while
achieving a $1.1\times$ average decoding speed-up compared to full attention.
Moreover, LessIsMore attends to $2\times$ fewer tokens without accuracy loss,
achieving a $1.13\times$ end-to-end speed-up compared to existing sparse
attention methods.

</details>


### [519] [Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution](https://arxiv.org/abs/2508.07111)
*Falaah Arif Khan,Nivedha Sivakumar,Yinong Oliver Wang,Katherine Metcalf,Cezanne Camacho,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 文章扩展单轴公平性评估以考察大语言模型的交叉偏见，创建WinoIdentity基准，提出新指标评估，发现模型存在高置信度差异，性能可能源于记忆而非推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在关键社会场景应用引发身份伤害担忧，此前工作多为单轴公平性评估，需考察交叉偏见。

Method: 创建WinoIdentity基准，用245,700个提示评估50种偏见模式，提出Coreference Confidence Disparity指标评估模型。

Result: 评估5个大语言模型发现高达40%的置信度差异，模型对双重劣势身份最不确定，霸权或特权标记的共指置信度也降低。

Conclusion: 大语言模型存在价值对齐和有效性两方面独立失败，可能造成社会危害，其性能更可能源于记忆而非逻辑推理。

Abstract: Large language models (LLMs) have achieved impressive performance, leading to
their widespread adoption as decision-support tools in resource-constrained
contexts like hiring and admissions. There is, however, scientific consensus
that AI systems can reflect and exacerbate societal biases, raising concerns
about identity-based harm when used in critical social contexts. Prior work has
laid a solid foundation for assessing bias in LLMs by evaluating demographic
disparities in different language reasoning tasks. In this work, we extend
single-axis fairness evaluations to examine intersectional bias, recognizing
that when multiple axes of discrimination intersect, they create distinct
patterns of disadvantage. We create a new benchmark called WinoIdentity by
augmenting the WinoBias dataset with 25 demographic markers across 10
attributes, including age, nationality, and race, intersected with binary
gender, yielding 245,700 prompts to evaluate 50 distinct bias patterns.
Focusing on harms of omission due to underrepresentation, we investigate bias
through the lens of uncertainty and propose a group (un)fairness metric called
Coreference Confidence Disparity which measures whether models are more or less
confident for some intersectional identities than others. We evaluate five
recently published LLMs and find confidence disparities as high as 40% along
various demographic attributes including body type, sexual orientation and
socio-economic status, with models being most uncertain about
doubly-disadvantaged identities in anti-stereotypical settings. Surprisingly,
coreference confidence decreases even for hegemonic or privileged markers,
indicating that the recent impressive performance of LLMs is more likely due to
memorization than logical reasoning. Notably, these are two independent
failures in value alignment and validity that can compound to cause social
harm.

</details>


### [520] [Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens](https://arxiv.org/abs/2508.07143)
*Anna Seo Gyeong Choi,Hoon Choi*

Main category: cs.CL

TL;DR: 本文从哲学视角审视自动语音识别（ASR）系统的偏见问题，指出其对特定语音变体的误识别是一种不尊重，分析伦理维度和语言标准化与多元化的矛盾，认为解决该问题需技术之外的措施。


<details>
  <summary>Details</summary>
Motivation: 当前对ASR系统公平性影响的研究有限，需从哲学视角审视其偏见问题。

Method: 区分道德中立的分类和有害的歧视，分析语音技术的三个独特伦理维度，探讨语言标准化与多元化的张力。

Result: 发现现有技术公平指标无法捕捉因语音误识别产生的不对称权力关系，当前ASR开发方法常嵌入并强化有问题的语言意识形态。

Conclusion: 解决ASR偏见问题不仅需要技术干预，还需认可多样语音变体，为开发尊重语言多样性和说话者自主性的ASR系统提供新思路。

Abstract: Automatic Speech Recognition (ASR) systems now mediate countless
human-technology interactions, yet research on their fairness implications
remains surprisingly limited. This paper examines ASR bias through a
philosophical lens, arguing that systematic misrecognition of certain speech
varieties constitutes more than a technical limitation -- it represents a form
of disrespect that compounds historical injustices against marginalized
linguistic communities. We distinguish between morally neutral classification
(discriminate1) and harmful discrimination (discriminate2), demonstrating how
ASR systems can inadvertently transform the former into the latter when they
consistently misrecognize non-standard dialects. We identify three unique
ethical dimensions of speech technologies that differentiate ASR bias from
other algorithmic fairness concerns: the temporal burden placed on speakers of
non-standard varieties ("temporal taxation"), the disruption of conversational
flow when systems misrecognize speech, and the fundamental connection between
speech patterns and personal/cultural identity. These factors create asymmetric
power relationships that existing technical fairness metrics fail to capture.
The paper analyzes the tension between linguistic standardization and pluralism
in ASR development, arguing that current approaches often embed and reinforce
problematic language ideologies. We conclude that addressing ASR bias requires
more than technical interventions; it demands recognition of diverse speech
varieties as legitimate forms of expression worthy of technological
accommodation. This philosophical reframing offers new pathways for developing
ASR systems that respect linguistic diversity and speaker autonomy.

</details>


### [521] [Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback](https://arxiv.org/abs/2508.07178)
*Kejin Liu,Junhong Lian,Xiang Ao,Ningtao Wang,Xing Fu,Yu Cheng,Weiqiang Wang,Xinyu Liu*

Main category: cs.CL

TL;DR: 文章指出现有方法忽视历史点击流中与个性化无关的点击噪声问题，提出PHG - DIF框架去噪并建模用户兴趣，还发布DT - PENS数据集，实验表明该框架效果好。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略历史点击流中的点击噪声，导致生成的标题可能偏离用户真实偏好，影响个性化标题生成质量。

Method: 提出PHG - DIF框架，先通过双阶段过滤去除点击流噪声，再利用多级时间融合动态建模用户兴趣；发布DT - PENS数据集。

Result: PHG - DIF显著减轻了点击噪声的不利影响，提高了标题质量，在DT - PENS上取得了最优结果。

Conclusion: PHG - DIF框架能有效解决点击噪声问题，提升个性化标题生成质量。

Abstract: Accurate personalized headline generation hinges on precisely capturing user
interests from historical behaviors. However, existing methods neglect
personalized-irrelevant click noise in entire historical clickstreams, which
may lead to hallucinated headlines that deviate from genuine user preferences.
In this paper, we reveal the detrimental impact of click noise on personalized
generation quality through rigorous analysis in both user and news dimensions.
Based on these insights, we propose a novel Personalized Headline Generation
framework via Denoising Fake Interests from Implicit Feedback (PHG-DIF).
PHG-DIF first employs dual-stage filtering to effectively remove clickstream
noise, identified by short dwell times and abnormal click bursts, and then
leverages multi-level temporal fusion to dynamically model users' evolving and
multi-faceted interests for precise profiling. Moreover, we release DT-PENS, a
new benchmark dataset comprising the click behavior of 1,000 carefully curated
users and nearly 10,000 annotated personalized headlines with historical dwell
time annotations. Extensive experiments demonstrate that PHG-DIF substantially
mitigates the adverse effects of click noise and significantly improves
headline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our
framework implementation and dataset are available at
https://github.com/liukejin-up/PHG-DIF.

</details>


### [522] [DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention](https://arxiv.org/abs/2508.07185)
*Kabir Khan,Priya Sharma,Arjun Mehta,Neha Gupta,Ravi Narayanan*

Main category: cs.CL

TL;DR: 提出DySK - Attn框架使大语言模型高效整合实时知识，实验表明其优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型知识静态易过时，重新训练计算成本高，现有知识编辑技术慢且有副作用。

Method: 提出DySK - Attn框架，将大语言模型与动态知识图谱协同，采用稀疏知识注意力机制进行粗到细搜索。

Result: 在时间敏感问答任务实验中，DySK - Attn在更新知识的事实准确性和计算效率上显著优于基线。

Conclusion: DySK - Attn框架为构建能跟上世界变化的大语言模型提供了可扩展且有效的解决方案。

Abstract: Large Language Models (LLMs) suffer from a critical limitation: their
knowledge is static and quickly becomes outdated. Retraining these massive
models is computationally prohibitive, while existing knowledge editing
techniques can be slow and may introduce unforeseen side effects. To address
this, we propose DySK-Attn, a novel framework that enables LLMs to efficiently
integrate real-time knowledge from a dynamic external source. Our approach
synergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated
instantaneously. The core of our framework is a sparse knowledge attention
mechanism, which allows the LLM to perform a coarse-to-fine grained search,
efficiently identifying and focusing on a small, highly relevant subset of
facts from the vast KG. This mechanism avoids the high computational cost of
dense attention over the entire knowledge base and mitigates noise from
irrelevant information. We demonstrate through extensive experiments on
time-sensitive question-answering tasks that DySK-Attn significantly
outperforms strong baselines, including standard Retrieval-Augmented Generation
(RAG) and model editing techniques, in both factual accuracy for updated
knowledge and computational efficiency. Our framework offers a scalable and
effective solution for building LLMs that can stay current with the
ever-changing world.

</details>


### [523] [Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment](https://arxiv.org/abs/2508.07195)
*Yanru Sun,Emadeldeen Eldele,Zongxia Xie,Yucheng Wang,Wenzhe Niu,Qinghua Hu,Chee Keong Kwoh,Min Wu*

Main category: cs.CL

TL;DR: 论文提出TALON框架解决大语言模型用于时间序列预测的问题，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 大语言模型直接用于时间序列预测存在时间模式异质性和模态差距两个问题。

Method: 设计异质时间编码器对多变量时间序列分段，引入语义对齐模块弥合模态差距。

Result: 在七个真实基准测试中，TALON性能优越，平均MSE比现有方法最多提升11%。

Conclusion: 将模式感知和语义感知设计融入大语言模型进行时间序列预测是有效的。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in natural language processing due to their strong generalization
and sequence modeling capabilities. However, their direct application to time
series forecasting remains challenging due to two fundamental issues: the
inherent heterogeneity of temporal patterns and the modality gap between
continuous numerical signals and discrete language representations. In this
work, we propose TALON, a unified framework that enhances LLM-based forecasting
by modeling temporal heterogeneity and enforcing semantic alignment.
Specifically, we design a Heterogeneous Temporal Encoder that partitions
multivariate time series into structurally coherent segments, enabling
localized expert modeling across diverse temporal patterns. To bridge the
modality gap, we introduce a Semantic Alignment Module that aligns temporal
features with LLM-compatible representations, enabling effective integration of
time series into language-based models while eliminating the need for
handcrafted prompts during inference. Extensive experiments on seven real-world
benchmarks demonstrate that TALON achieves superior performance across all
datasets, with average MSE improvements of up to 11\% over recent
state-of-the-art methods. These results underscore the effectiveness of
incorporating both pattern-aware and semantic-aware designs when adapting LLMs
for time series forecasting. The code is available at:
https://github.com/syrGitHub/TALON.

</details>


### [524] [Text to Speech System for Meitei Mayek Script](https://arxiv.org/abs/2508.06870)
*Gangular Singh Irengbam,Nirvash Singh Wahengbam,Lanthoiba Meitei Khumanthem,Paikhomba Oinam*

Main category: cs.CL

TL;DR: 本文开发了基于Meitei Mayek文字的曼尼普尔语TTS系统，利用Tacotron 2和HiFi - GAN实现语音合成，为语言保护和技术融入奠定基础。


<details>
  <summary>Details</summary>
Motivation: 开发适用于曼尼普尔语的TTS系统，以支持该语言的语音合成，促进语言保护和技术融入。

Method: 利用Tacotron 2和HiFi - GAN搭建适应声调音韵和资源匮乏语言环境的神经TTS架构，开发Meitei Mayek到ARPAbet的音素映射，整理单说话人数据集。

Result: 实现了可理解且自然的语音合成，通过主观和客观指标验证。

Conclusion: 该系统为曼尼普尔语的语言保护和技术融入奠定了基础。

Abstract: This paper presents the development of a Text-to-Speech (TTS) system for the
Manipuri language using the Meitei Mayek script. Leveraging Tacotron 2 and
HiFi-GAN, we introduce a neural TTS architecture adapted to support tonal
phonology and under-resourced linguistic environments. We develop a phoneme
mapping for Meitei Mayek to ARPAbet, curate a single-speaker dataset, and
demonstrate intelligible and natural speech synthesis, validated through
subjective and objective metrics. This system lays the groundwork for
linguistic preservation and technological inclusion of Manipuri.

</details>


### [525] [Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection](https://arxiv.org/abs/2508.06913)
*Siyuan Li,Xi Lin,Guangyan Li,Zehao Liu,Aodu Wulianghai,Li Ding,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: 提出SentiDetect框架检测大语言模型生成文本，在多数据集和模型上实验，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成文本检测方法泛化性有限，易受改写、对抗扰动和跨领域影响。

Method: 提出SentiDetect框架，通过分析情感分布稳定性差异检测，定义情感分布一致性和保留性两个互补指标。

Result: 在五个数据集和多种大语言模型上实验，F1分数比现有方法有提升，对改写、对抗攻击和文本长度变化更鲁棒。

Conclusion: SentiDetect优于现有基线，在挑战性场景表现更好。

Abstract: The rapid advancement of large language models (LLMs) has resulted in
increasingly sophisticated AI-generated content, posing significant challenges
in distinguishing LLM-generated text from human-written language. Existing
detection methods, primarily based on lexical heuristics or fine-tuned
classifiers, often suffer from limited generalizability and are vulnerable to
paraphrasing, adversarial perturbations, and cross-domain shifts. In this work,
we propose SentiDetect, a model-agnostic framework for detecting LLM-generated
text by analyzing the divergence in sentiment distribution stability. Our
method is motivated by the empirical observation that LLM outputs tend to
exhibit emotionally consistent patterns, whereas human-written texts display
greater emotional variability. To capture this phenomenon, we define two
complementary metrics: sentiment distribution consistency and sentiment
distribution preservation, which quantify stability under sentiment-altering
and semantic-preserving transformations. We evaluate SentiDetect on five
diverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro,
Claude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its
superiority over state-of-the-art baselines, with over 16% and 11% F1 score
improvements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover,
SentiDetect also shows greater robustness to paraphrasing, adversarial attacks,
and text length variations, outperforming existing detectors in challenging
scenarios.

</details>


### [526] [Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models](https://arxiv.org/abs/2508.07273)
*Qiongqiong Wang,Hardik B. Sailor,Jeremy H. M. Wong,Tianchi Liu,Shuo Sun,Wenyu Zhang,Muhammad Huzaifah,Nancy Chen,Ai Ti Aw*

Main category: cs.CL

TL;DR: 论文提出两种将上下文副语言信息融入模型训练的方法，隐式方法提升性能，结合显式方法效果更佳，并验证了LLM评判的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语音语言模型在共情推理方面存在局限，原因是缺乏整合上下文内容和副语言线索的训练数据集。

Method: 提出两种方法：显式方法直接向大语言模型提供副语言元数据；隐式方法利用分类和维度情感注释与语音转录自动生成新的训练问答对。

Result: 隐式方法在人工标注的问答基准上使LLM评判的性能提升38.41%，与显式方法结合时达到46.02%。

Conclusion: 所提方法在上下文副语言理解方面有效，且LLM评判具有可靠性。

Abstract: Current large speech language models (Speech-LLMs) often exhibit limitations
in empathetic reasoning, primarily due to the absence of training datasets that
integrate both contextual content and paralinguistic cues. In this work, we
propose two approaches to incorporate contextual paralinguistic information
into model training: (1) an explicit method that provides paralinguistic
metadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit
method that automatically generates novel training question-answer (QA) pairs
using both categorical and dimensional emotion annotations alongside speech
transcriptions. Our implicit method boosts performance (LLM-judged) by 38.41%
on a human-annotated QA benchmark, reaching 46.02% when combined with the
explicit approach, showing effectiveness in contextual paralinguistic
understanding. We also validate the LLM judge by demonstrating its correlation
with classification metrics, providing support for its reliability.

</details>


### [527] [MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory](https://arxiv.org/abs/2508.07279)
*Vasudha Varadarajan,Hui Xu,Rebecca Astrid Boehme,Mariam Marlan Mirstrom,Sverker Sikstrom,H. Andrew Schwartz*

Main category: cs.CL

TL;DR: 提出MAQuA框架用于心理健康筛查，能减少问题数量，提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在心理健康评估中存在查询过多、效率低的问题，需要新的筛查方法。

Method: 结合语言响应的多结果建模、项目反应理论（IRT）和因子分析，每轮选择多维度最具信息性响应的问题。

Result: 在新数据集上，MAQuA使分数稳定所需的评估问题数量减少50 - 87%，在内外化领域表现稳健。

Conclusion: MAQuA是强大高效的心理健康筛查工具，有助于将基于大语言模型的智能体集成到临床工作流程。

Abstract: Recent advances in large language models (LLMs) offer new opportunities for
scalable, interactive mental health assessment, but excessive querying by LLMs
burdens users and is inefficient for real-world screening across
transdiagnostic symptom profiles. We introduce MAQuA, an adaptive
question-asking framework for simultaneous, multidimensional mental health
screening. Combining multi-outcome modeling on language responses with item
response theory (IRT) and factor analysis, MAQuA selects the questions with
most informative responses across multiple dimensions at each turn to optimize
diagnostic information, improving accuracy and potentially reducing response
burden. Empirical results on a novel dataset reveal that MAQuA reduces the
number of assessment questions required for score stabilization by 50-87%
compared to random ordering (e.g., achieving stable depression scores with 71%
fewer questions and eating disorder scores with 85% fewer questions). MAQuA
demonstrates robust performance across both internalizing (depression, anxiety)
and externalizing (substance use, eating disorder) domains, with early stopping
strategies further reducing patient time and burden. These findings position
MAQuA as a powerful and efficient tool for scalable, nuanced, and interactive
mental health screening, advancing the integration of LLM-based agents into
real-world clinical workflows.

</details>


### [528] ["Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas](https://arxiv.org/abs/2508.07284)
*Junchen Ding,Penghao Jiang,Zihao Xu,Ziqi Ding,Yichen Zhu,Jiaojiao Jiang,Yuekang Li*

Main category: cs.CL

TL;DR: 研究对14个大语言模型在27个电车难题场景下进行道德推理评估，发现模型在不同伦理框架和类型下表现有差异，建议将道德推理作为大模型对齐的主要维度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在道德敏感决策中的应用增加，需要理解其道德推理过程。

Method: 对14个大语言模型在27个电车难题场景下，基于十种道德哲学进行评估，采用因子提示协议获取决策和解释。

Result: 不同伦理框架和模型类型表现差异大，推理增强模型更果断但不一定更符合人类共识，部分伦理框架下有“甜蜜区”，部分框架下模型产生有争议结果。

Conclusion: 道德提示可作为诊断工具，应将道德推理作为大模型对齐的主要维度，建立标准化评估基准。

Abstract: As large language models (LLMs) increasingly mediate ethically sensitive
decisions, understanding their moral reasoning processes becomes imperative.
This study presents a comprehensive empirical evaluation of 14 leading LLMs,
both reasoning enabled and general purpose, across 27 diverse trolley problem
scenarios, framed by ten moral philosophies, including utilitarianism,
deontology, and altruism. Using a factorial prompting protocol, we elicited
3,780 binary decisions and natural language justifications, enabling analysis
along axes of decisional assertiveness, explanation answer consistency, public
moral alignment, and sensitivity to ethically irrelevant cues. Our findings
reveal significant variability across ethical frames and model types: reasoning
enhanced models demonstrate greater decisiveness and structured justifications,
yet do not always align better with human consensus. Notably, "sweet zones"
emerge in altruistic, fairness, and virtue ethics framings, where models
achieve a balance of high intervention rates, low explanation conflict, and
minimal divergence from aggregated human judgments. However, models diverge
under frames emphasizing kinship, legality, or self interest, often producing
ethically controversial outcomes. These patterns suggest that moral prompting
is not only a behavioral modifier but also a diagnostic tool for uncovering
latent alignment philosophies across providers. We advocate for moral reasoning
to become a primary axis in LLM alignment, calling for standardized benchmarks
that evaluate not just what LLMs decide, but how and why.

</details>


### [529] [ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering](https://arxiv.org/abs/2508.07321)
*Shubhra Ghosh,Abhilekh Borah,Aditya Kumar Guru,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: 提出ObfusQA框架评估大语言模型面对混淆问题时的鲁棒性，发现模型易失败或产生幻觉回答，且公开ObfusQAte。


<details>
  <summary>Details</summary>
Motivation: 现有研究未测试大语言模型面对混淆问题时的鲁棒性，需系统评估其局限性。

Method: 提出ObfusQAte技术，引入ObfusQA框架，通过多维度、多层级的混淆水平评估大语言模型。

Result: 大语言模型面对细微变化的混淆问题时，易失败或产生幻觉回答。

Conclusion: ObfusQA为评估大语言模型鲁棒性和适应性提供了综合基准，公开ObfusQAte以推动相关研究。

Abstract: The rapid proliferation of Large Language Models (LLMs) has significantly
contributed to the development of equitable AI systems capable of factual
question-answering (QA). However, no known study tests the LLMs' robustness
when presented with obfuscated versions of questions. To systematically
evaluate these limitations, we propose a novel technique, ObfusQAte and,
leveraging the same, introduce ObfusQA, a comprehensive, first of its kind,
framework with multi-tiered obfuscation levels designed to examine LLM
capabilities across three distinct dimensions: (i) Named-Entity Indirection,
(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these
fine-grained distinctions in language, ObfusQA provides a comprehensive
benchmark for evaluating LLM robustness and adaptability. Our study observes
that LLMs exhibit a tendency to fail or generate hallucinated responses when
confronted with these increasingly nuanced variations. To foster research in
this direction, we make ObfusQAte publicly available.

</details>


### [530] [Strategies of Code-switching in Human-Machine Dialogs](https://arxiv.org/abs/2508.07325)
*Dean Geckt,Melinda Fricke,Shuly Wintner*

Main category: cs.CL

TL;DR: 研究开发能和人类进行西英语码转换交流的聊天机器人，实验发现参与者对可预测的码转换更满意，结果显示多语言技术有研究潜力但未成熟时有弊端。


<details>
  <summary>Details</summary>
Motivation: 多数人会使用多语言且进行码转换，但码转换语言的特征未被充分理解，希望研究双语语言使用情况。

Method: 开发聊天机器人与人类完成地图任务，在两个实验中让机器人按不同策略进行码转换，考察实验可行性及参与者对不同话语和语法模式的敏感度。

Result: 只要机器人码转换行为可预测，参与者就喜欢与之交流；随机或不合语法的码转换会让参与者兴趣降低且完成任务成功率下降。

Conclusion: 部署未充分发展的多语言技术有潜在弊端，同时该技术在双语语言使用研究方面有前景。

Abstract: Most people are multilingual, and most multilinguals code-switch, yet the
characteristics of code-switched language are not fully understood. We
developed a chatbot capable of completing a Map Task with human participants
using code-switched Spanish and English. In two experiments, we prompted the
bot to code-switch according to different strategies, examining (1) the
feasibility of such experiments for investigating bilingual language use, and
(2) whether participants would be sensitive to variations in discourse and
grammatical patterns. Participants generally enjoyed code-switching with our
bot as long as it produced predictable code-switching behavior; when
code-switching was random or ungrammatical (as when producing unattested
incongruent mixed-language noun phrases, such as `la fork'), participants
enjoyed the task less and were less successful at completing it. These results
underscore the potential downsides of deploying insufficiently developed
multilingual language technology, while also illustrating the promise of such
technology for conducting research on bilingual language use.

</details>


### [531] [How Does a Deep Neural Network Look at Lexical Stress?](https://arxiv.org/abs/2508.07229)
*Itai Allouche,Itay Asael,Rotem Rousso,Vered Dassa,Ann Bradlow,Seung-Eun Kim,Matthew Goldrick,Joseph Keshet*

Main category: cs.CL

TL;DR: 研究神经网络在英语双音节词重音预测中的决策依据，构建数据集，用CNN训练预测，用LRP分析，揭示重音预测受多因素影响。


<details>
  <summary>Details</summary>
Motivation: 神经网络在语音处理中常为黑箱，需探究其在词重音决策中的依据和解释方法。

Method: 自动构建英语双音节词数据集，用多种CNN架构从频谱图预测重音位置，用LRP进行可解释性分析，提出特征特定相关性分析。

Result: CNN在测试数据上准确率达92%，LRP显示重音与非重音音节信息影响预测，特征分析表明最佳分类器受重音元音的第一、二共振峰影响大。

Conclusion: 深度学习能从自然数据中获取重音分布线索，扩展了基于高度控制刺激的传统语音学研究。

Abstract: Despite their success in speech processing, neural networks often operate as
black boxes, prompting the question: what informs their decisions, and how can
we interpret them? This work examines this issue in the context of lexical
stress. A dataset of English disyllabic words was automatically constructed
from read and spontaneous speech. Several Convolutional Neural Network (CNN)
architectures were trained to predict stress position from a spectrographic
representation of disyllabic words lacking minimal stress pairs (e.g., initial
stress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out
test data. Layerwise Relevance Propagation (LRP), a technique for CNN
interpretability analysis, revealed that predictions for held-out minimal pairs
(PROtest vs. proTEST ) were most strongly influenced by information in stressed
versus unstressed syllables, particularly the spectral properties of stressed
vowels. However, the classifiers also attended to information throughout the
word. A feature-specific relevance analysis is proposed, and its results
suggest that our best-performing classifier is strongly influenced by the
stressed vowel's first and second formants, with some evidence that its pitch
and third formant also contribute. These results reveal deep learning's ability
to acquire distributed cues to stress from naturally occurring data, extending
traditional phonetic work based around highly controlled stimuli.

</details>


### [532] [ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models](https://arxiv.org/abs/2508.07484)
*Archchana Sindhujan,Shenbin Qian,Chan Chi Chun Matthew,Constantin Orasan,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 本文介绍了用于提升大语言模型机器翻译质量评估（QE）的ALOPE框架，该框架有层自适应等策略，表现优于现有方法，并公开代码。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的QE系统有局限性，预训练用于因果语言建模而非回归任务，且受低资源语言和预训练数据分布影响，在QE这一跨语言任务上有挑战。

Method: 引入ALOPE框架，将低秩适配器（LoRA）与回归任务头集成，利用选定的预训练Transformer层；采用动态加权和多头回归两种策略。

Result: 该框架在多种现有基于大语言模型的QE方法上有改进，实证表明大语言模型的中间Transformer层提供的上下文表示更符合QE任务的跨语言性质。

Conclusion: 公开模型和框架代码，便于进一步研究，可让现有基于大语言模型的机器翻译框架具备QE能力。

Abstract: Large Language Models (LLMs) have shown remarkable performance across a wide
range of natural language processing tasks. Quality Estimation (QE) for Machine
Translation (MT), which assesses the quality of a source-target pair without
relying on reference translations, remains a challenging cross-lingual task for
LLMs. The challenges stem from the inherent limitations of existing LLM-based
QE systems, which are pre-trained for causal language modelling rather than
regression-specific tasks, further elevated by the presence of low-resource
languages given pre-training data distribution. This paper introduces ALOPE, an
adaptive layer-optimization framework designed to enhance LLM-based QE by
restructuring Transformer representations through layer-wise adaptation for
improved regression-based prediction. Our framework integrates low-rank
adapters (LoRA) with regression task heads, leveraging selected pre-trained
Transformer layers for improved cross-lingual alignment. In addition to the
layer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,
which adaptively combines representations from multiple layers, and multi-head
regression, which aggregates regression losses from multiple heads for QE. Our
framework shows improvements over various existing LLM-based QE approaches.
Empirical evidence suggests that intermediate Transformer layers in LLMs
provide contextual representations that are more aligned with the cross-lingual
nature of the QE task. We make resultant models and framework code publicly
available for further research, also allowing existing LLM-based MT frameworks
to be scaled with QE capabilities.

</details>


### [533] [Grounding Multilingual Multimodal LLMs With Cultural Knowledge](https://arxiv.org/abs/2508.07414)
*Jean de Dieu Nyandwi,Yueqi Song,Simran Khanuja,Graham Neubig*

Main category: cs.CL

TL;DR: 提出数据驱动方法，构建CulturalGround数据集训练MLLM CulturalPangea，缩小MLLM文化差距。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在长尾文化实体理解和低资源语言表现不佳的问题。

Method: 利用Wikidata知识图谱收集图像，生成合成多语言视觉问答数据，构建CulturalGround数据集，训练CulturalPangea模型并结合标准多语言指令调优数据。

Result: CulturalPangea在文化相关多语言多模态基准测试中达开源模型最优，主流视觉语言任务结果未下降。

Conclusion: 有针对性的文化接地方法能缩小MLLM文化差距，为全球包容性多模态系统提供实践路径。

Abstract: Multimodal Large Language Models excel in high-resource settings, but often
misinterpret long-tail cultural entities and underperform in low-resource
languages. To address this gap, we propose a data-centric approach that
directly grounds MLLMs in cultural knowledge. Leveraging a large scale
knowledge graph from Wikidata, we collect images that represent culturally
significant entities, and generate synthetic multilingual visual question
answering data. The resulting dataset, CulturalGround, comprises 22 million
high-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages.
We train an open-source MLLM CulturalPangea on CulturalGround, interleaving
standard multilingual instruction-tuning data to preserve general abilities.
CulturalPangea achieves state-of-the-art performance among open models on
various culture-focused multilingual multimodal benchmarks, outperforming prior
models by an average of 5.0 without degrading results on mainstream
vision-language tasks. Our findings show that our targeted, culturally grounded
approach could substantially narrow the cultural gap in MLLMs and offer a
practical path towards globally inclusive multimodal systems.

</details>


### [534] [Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews](https://arxiv.org/abs/2508.07517)
*Joseph T. Colonel,Baihan Lin*

Main category: cs.CL

TL;DR: 介绍开源可视化工具ThemeClouds，用大语言模型从对话转录本生成主题、参与者加权词云，比传统方法更优并探讨相关设计权衡。


<details>
  <summary>Details</summary>
Motivation: 传统基于频率的词云方法在对话语境中存在缺陷，无法满足早期分析需求。

Method: 使用大语言模型识别语料库中的概念级主题，统计提及每个主题的独特参与者数量，生成词云，研究者可自定义参数。

Result: 在用户研究中，该方法比频率词云和主题建模基线方法挖掘出更多可行的设备问题。

Conclusion: 讨论了将大语言模型辅助集成到定性工作流程中的设计权衡、对可解释性和研究者能动性的影响以及交互式分析机会。

Abstract: Word clouds are a common way to summarize qualitative interviews, yet
traditional frequency-based methods often fail in conversational contexts: they
surface filler words, ignore paraphrase, and fragment semantically related
ideas. This limits their usefulness in early-stage analysis, when researchers
need fast, interpretable overviews of what participant actually said. We
introduce ThemeClouds, an open-source visualization tool that uses large
language models (LLMs) to generate thematic, participant-weighted word clouds
from dialogue transcripts. The system prompts an LLM to identify concept-level
themes across a corpus and then counts how many unique participants mention
each topic, yielding a visualization grounded in breadth of mention rather than
raw term frequency. Researchers can customize prompts and visualization
parameters, providing transparency and control. Using interviews from a user
study comparing five recording-device configurations (31 participants; 155
transcripts, Whisper ASR), our approach surfaces more actionable device
concerns than frequency clouds and topic-modeling baselines (e.g., LDA,
BERTopic). We discuss design trade-offs for integrating LLM assistance into
qualitative workflows, implications for interpretability and researcher agency,
and opportunities for interactive analyses such as per-condition contrasts
(``diff clouds'').

</details>


### [535] [IBPS: Indian Bail Prediction System](https://arxiv.org/abs/2508.07592)
*Puspesh Kumar Srivastava,Uddeshya Raj,Praveen Patel,/Shubham Kumar Nigam,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: 本文提出AI框架IBPS辅助印度保释决策，构建数据集，微调大模型，结果显示结合法规知识的模型表现佳，能支持数据驱动的法律协助。


<details>
  <summary>Details</summary>
Motivation: 印度保释决策存在主观性、延迟和不一致问题，大量审前囚犯加剧人权和司法积压问题，需要改进保释决策方式。

Method: 构建150,430个高等法院保释判决的大规模数据集，用参数高效技术微调大语言模型，在多种配置下评估性能。

Result: 结合法规知识微调的模型显著优于基线，准确率和解释质量高，在测试集上泛化能力好。

Conclusion: IBPS为印度司法系统提供透明、可扩展和可复现的解决方案，能减少保释延迟，促进程序公平。

Abstract: Bail decisions are among the most frequently adjudicated matters in Indian
courts, yet they remain plagued by subjectivity, delays, and inconsistencies.
With over 75% of India's prison population comprising undertrial prisoners,
many from socioeconomically disadvantaged backgrounds, the lack of timely and
fair bail adjudication exacerbates human rights concerns and contributes to
systemic judicial backlog. In this paper, we present the Indian Bail Prediction
System (IBPS), an AI-powered framework designed to assist in bail
decision-making by predicting outcomes and generating legally sound rationales
based solely on factual case attributes and statutory provisions. We curate and
release a large-scale dataset of 150,430 High Court bail judgments, enriched
with structured annotations such as age, health, criminal history, crime
category, custody duration, statutes, and judicial reasoning. We fine-tune a
large language model using parameter-efficient techniques and evaluate its
performance across multiple configurations, with and without statutory context,
and with RAG. Our results demonstrate that models fine-tuned with statutory
knowledge significantly outperform baselines, achieving strong accuracy and
explanation quality, and generalize well to a test set independently annotated
by legal experts. IBPS offers a transparent, scalable, and reproducible
solution to support data-driven legal assistance, reduce bail delays, and
promote procedural fairness in the Indian judicial system.

</details>


### [536] [InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information](https://arxiv.org/abs/2508.07630)
*Anirudh Iyengar Kaniyar Narayana Iyengar,Srija Mukhopadhyay,Adnan Qidwai,Shubhankar Singh,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: 介绍InterChart基准评估视觉语言模型跨多图表推理能力，评估显示模型在图表复杂度增加时准确率下降，分解图表可提升表现。


<details>
  <summary>Details</summary>
Motivation: 评估视觉语言模型在跨多个相关图表进行推理的能力，这对现实应用很重要，而此前基准多关注孤立图表。

Method: 组织InterChart基准，分三个难度等级，用不同类型问题挑战模型。

Result: 评估显示随着图表复杂度增加，模型准确率大幅下降，分解图表可提升表现。

Conclusion: InterChart揭示模型局限，为复杂多视觉环境下多模态推理提供严谨框架。

Abstract: We introduce InterChart, a diagnostic benchmark that evaluates how well
vision-language models (VLMs) reason across multiple related charts, a task
central to real-world applications such as scientific reporting, financial
analysis, and public policy dashboards. Unlike prior benchmarks focusing on
isolated, visually uniform charts, InterChart challenges models with diverse
question types ranging from entity inference and trend correlation to numerical
estimation and abstract multi-step reasoning grounded in 2-3 thematically or
structurally related charts. We organize the benchmark into three tiers of
increasing difficulty: (1) factual reasoning over individual charts, (2)
integrative analysis across synthetically aligned chart sets, and (3) semantic
inference over visually complex, real-world chart pairs. Our evaluation of
state-of-the-art open and closed-source VLMs reveals consistent and steep
accuracy declines as chart complexity increases. We find that models perform
better when we decompose multi-entity charts into simpler visual units,
underscoring their struggles with cross-chart integration. By exposing these
systematic limitations, InterChart provides a rigorous framework for advancing
multimodal reasoning in complex, multi-visual environments.

</details>


### [537] [LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval](https://arxiv.org/abs/2508.07690)
*Luyao Zhuang,Qinggang Zhang,Huachi Zhou,Juhua Liu,Qing Li,Xiao Huang*

Main category: cs.CL

TL;DR: 本文提出用于归纳式工具检索的逻辑引导语义桥接框架LoSemB，可挖掘和转移潜在逻辑信息，实验表明其在归纳和直推设置下均有良好表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型工具检索方法多为直推式，难以处理训练中未见过的新工具，存在分布偏移和基于相似度检索易受攻击的问题。

Method: 引入LoSemB框架，包含基于逻辑的嵌入对齐模块和关系增强检索机制。

Result: LoSemB在归纳设置下取得先进性能，在直推设置下也保持了良好效果。

Conclusion: LoSemB能有效解决未见过工具的检索问题，无需昂贵的重新训练。

Abstract: Tool learning has emerged as a promising paradigm for large language models
(LLMs) to solve many real-world tasks. Nonetheless, with the tool repository
rapidly expanding, it is impractical to contain all tools within the limited
input length of LLMs. To alleviate these issues, researchers have explored
incorporating a tool retrieval module to select the most relevant tools or
represent tools as unique tokens within LLM parameters. However, most
state-of-the-art methods are under transductive settings, assuming all tools
have been observed during training. Such a setting deviates from reality as the
real-world tool repository is evolving and incorporates new tools frequently.
When dealing with these unseen tools, which refer to tools not encountered
during the training phase, these methods are limited by two key issues,
including the large distribution shift and the vulnerability of
similarity-based retrieval. To this end, inspired by human cognitive processes
of mastering unseen tools through discovering and applying the logical
information from prior experience, we introduce a novel Logic-Guided Semantic
Bridging framework for inductive tool retrieval, namely, LoSemB, which aims to
mine and transfer latent logical information for inductive tool retrieval
without costly retraining. Specifically, LoSemB contains a logic-based
embedding alignment module to mitigate distribution shifts and implements a
relational augmented retrieval mechanism to reduce the vulnerability of
similarity-based retrieval. Extensive experiments demonstrate that LoSemB
achieves advanced performance in inductive settings while maintaining desirable
effectiveness in the transductive setting.

</details>


### [538] [Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?](https://arxiv.org/abs/2508.08096)
*Lukas Gehring,Benjamin Paaßen*

Main category: cs.CL

TL;DR: 本文对教育场景下不同大语言模型文本检测器进行基准测试，引入新数据集GEDE，指出多数检测器对中等学生贡献文本分类不佳，易产生误判。


<details>
  <summary>Details</summary>
Motivation: 大语言模型使学生自动生成文本更便捷，给教育机构带来挑战，需学习分析方法检测此类文本。

Method: 对不同检测器性能进行基准测试，引入GEDE数据集，提出贡献水平概念。

Result: 多数检测器难以准确分类中等学生贡献水平文本，易产生误判。

Conclusion: 当前检测器在教育场景中对特定文本分类效果不佳，数据集等材料公开可促进研究。

Abstract: Recent advancements in Large Language Models (LLMs) and their increased
accessibility have made it easier than ever for students to automatically
generate texts, posing new challenges for educational institutions. To enforce
norms of academic integrity and ensure students' learning, learning analytics
methods to automatically detect LLM-generated text appear increasingly
appealing. This paper benchmarks the performance of different state-of-the-art
detectors in educational contexts, introducing a novel dataset, called
Generative Essay Detection in Education (GEDE), containing over 900
student-written essays and over 12,500 LLM-generated essays from various
domains. To capture the diversity of LLM usage practices in generating text, we
propose the concept of contribution levels, representing students' contribution
to a given assignment. These levels range from purely human-written texts, to
slightly LLM-improved versions, to fully LLM-generated texts, and finally to
active attacks on the detector by "humanizing" generated texts. We show that
most detectors struggle to accurately classify texts of intermediate student
contribution levels, like LLM-improved human-written texts. Detectors are
particularly likely to produce false positives, which is problematic in
educational settings where false suspicions can severely impact students'
lives. Our dataset, code, and additional supplementary materials are publicly
available at
https://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.

</details>


### [539] [LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo](https://arxiv.org/abs/2508.08163)
*Mandira Sawkar,Samay U. Shetty,Deepak Pandita,Tharindu Cyril Weerasooriya,Christopher M. Homan*

Main category: cs.CL

TL;DR: 本文参与LeWiDi 2025共享任务，改进DisCo模型以更好捕获分歧模式，实验显示在三个数据集上评估指标有显著提升，还进行深入分析并强调分歧感知建模的价值。


<details>
  <summary>Details</summary>
Motivation: 参与LeWiDi 2025共享任务，解决如何通过软标签分布预测和视角主义评估来建模注释者分歧的问题。

Method: 扩展DisCo，纳入注释者元数据，增强输入表示，修改损失函数。

Result: 在三个数据集的软评估和视角主义评估指标上有显著提升，还进行深入的错误和校准分析。

Conclusion: 强调分歧感知建模的价值，为系统组件与人工标注数据复杂性的交互提供见解。

Abstract: The Learning With Disagreements (LeWiDi) 2025 shared task is to model
annotator disagreement through soft label distribution prediction and
perspectivist evaluation, modeling annotators. We adapt DisCo (Distribution
from Context), a neural architecture that jointly models item-level and
annotator-level label distributions, and present detailed analysis and
improvements. In this paper, we extend the DisCo by incorporating annotator
metadata, enhancing input representations, and modifying the loss functions to
capture disagreement patterns better. Through extensive experiments, we
demonstrate substantial improvements in both soft and perspectivist evaluation
metrics across three datasets. We also conduct in-depth error and calibration
analyses, highlighting the conditions under which improvements occur. Our
findings underscore the value of disagreement-aware modeling and offer insights
into how system components interact with the complexity of human-annotated
data.

</details>


### [540] [SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling](https://arxiv.org/abs/2508.08211)
*Zhuohao Yu,Xingru Jiang,Weizheng Gu,Yidong Wang,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: 提出SAEMark框架用于事后多位水印嵌入，在多语言和闭源模型场景有效，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型文本水印方法存在损害文本质量、需白盒模型访问和操纵对数等局限，不适用于API模型和多语言场景。

Method: 提出SAEMark框架，通过推理时基于特征的拒绝采样嵌入个性化消息，不改变模型对数也无需训练，基于生成文本的确定性特征选择输出。

Result: 使用稀疏自动编码器证明框架有效性，在4个数据集实验中表现一致，英语F1达99.7%，多位检测准确率高。

Conclusion: SAEMark为可扩展水印建立新范式，能与闭源大语言模型直接配合，实现内容溯源。

Abstract: Watermarking LLM-generated text is critical for content attribution and
misinformation prevention. However, existing methods compromise text quality,
require white-box model access and logit manipulation. These limitations
exclude API-based models and multilingual scenarios. We propose SAEMark, a
general framework for post-hoc multi-bit watermarking that embeds personalized
messages solely via inference-time, feature-based rejection sampling without
altering model logits or requiring training. Our approach operates on
deterministic features extracted from generated text, selecting outputs whose
feature statistics align with key-derived targets. This framework naturally
generalizes across languages and domains while preserving text quality through
sampling LLM outputs instead of modifying. We provide theoretical guarantees
relating watermark success probability and compute budget that hold for any
suitable feature extractor. Empirically, we demonstrate the framework's
effectiveness using Sparse Autoencoders (SAEs), achieving superior detection
accuracy and text quality. Experiments across 4 datasets show SAEMark's
consistent performance, with 99.7% F1 on English and strong multi-bit detection
accuracy. SAEMark establishes a new paradigm for scalable watermarking that
works out-of-the-box with closed-source LLMs while enabling content
attribution.

</details>


### [541] [Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL](https://arxiv.org/abs/2508.07976)
*Jiaxuan Gao,Wei Fu,Minyang Xie,Shusheng Xu,Chuyi He,Zhiyu Mei,Banghua Zhu,Yi Wu*

Main category: cs.CL

TL;DR: 本文介绍开源项目ASearcher用于搜索代理的大规模强化学习训练，通过可扩展的异步RL训练和基于提示的LLM代理合成数据集，在xBench和GAIA上取得显著提升，并开源模型、数据和代码。


<details>
  <summary>Details</summary>
Motivation: 现有开源代理在搜索智能方面未达专家水平，现有方法在可扩展性、效率和数据质量上存在不足。

Method: 引入可扩展的全异步RL训练，使用基于提示的LLM代理自主合成高质量且具挑战性的问答数据集。

Result: 基于提示的QwQ - 32B代理在xBench和GAIA上分别有46.7%和20.8%的Avg@4提升；ASearcher - Web - QwQ在xBench和GAIA上的Avg@4得分分别为42.1和52.8，超越现有开源32B代理。

Conclusion: ASearcher在搜索代理的大规模强化学习训练上有良好效果，可实现长时搜索，提高效率，相关模型、数据和代码已开源。

Abstract: Recent advancements in LLM-based agents have demonstrated remarkable
capabilities in handling complex, knowledge-intensive tasks by integrating
external tools. Among diverse choices of tools, search tools play a pivotal
role in accessing vast external knowledge. However, open-source agents still
fall short of achieving expert-level Search Intelligence, the ability to
resolve ambiguous queries, generate precise searches, analyze results, and
conduct thorough exploration. Existing approaches fall short in scalability,
efficiency, and data quality. For example, small turn limits in existing online
RL methods, e.g. <=10, restrict complex strategy learning. This paper
introduces ASearcher, an open-source project for large-scale RL training of
search agents. Our key contributions include: (1) Scalable fully asynchronous
RL training that enables long-horizon search while maintaining high training
efficiency. (2) A prompt-based LLM agent that autonomously synthesizes
high-quality and challenging QAs, creating a large-scale QA dataset. Through RL
training, our prompt-based QwQ-32B agent achieves substantial improvements,
with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our
agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns
and output tokens exceeding 150k during training time. With a simple agent
design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on
xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We
open-source our models, training data, and codes in
https://github.com/inclusionAI/ASearcher.

</details>


### [542] [Dual Information Speech Language Models for Emotional Conversations](https://arxiv.org/abs/2508.08095)
*Chun Wang,Chenyang Liu,Wenze Xu,Weihong Deng*

Main category: cs.CL

TL;DR: 传统基于文本的对话系统忽略副语言线索，扩展冻结大语言模型构建的语音语言模型有问题，本文提出两个异构适配器和弱监督训练策略，实验证明其在情感对话任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统基于文本的对话系统忽略副语言线索，而扩展冻结大语言模型构建的语音语言模型难以捕捉副语言信息且上下文理解能力下降。

Method: 提出两个异构适配器，采用弱监督训练策略，解耦副语言和语言信息，通过控制随机性避免生成特定任务向量。

Result: 在情感对话任务中表现出有竞争力的性能，能在上下文环境中有效整合副语言和语言信息。

Conclusion: 所提方法可使语音语言模型有效整合副语言和语言信息，且具有参数和数据效率。

Abstract: Conversational systems relying on text-based large language models (LLMs)
often overlook paralinguistic cues, essential for understanding emotions and
intentions. Speech-language models (SLMs), which use speech as input, are
emerging as a promising solution. However, SLMs built by extending frozen LLMs
struggle to capture paralinguistic information and exhibit reduced context
understanding. We identify entangled information and improper training
strategies as key issues. To address these issues, we propose two heterogeneous
adapters and suggest a weakly supervised training strategy. Our approach
disentangles paralinguistic and linguistic information, enabling SLMs to
interpret speech through structured representations. It also preserves
contextual understanding by avoiding the generation of task-specific vectors
through controlled randomness. This approach trains only the adapters on common
datasets, ensuring parameter and data efficiency. Experiments demonstrate
competitive performance in emotional conversation tasks, showcasing the model's
ability to effectively integrate both paralinguistic and linguistic information
within contextual settings.

</details>


### [543] [Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models](https://arxiv.org/abs/2508.08131)
*Wenze Xu,Chun Wang,Jiazhen Yu,Sheng Chen,Liang Gao,Weihong Deng*

Main category: cs.CL

TL;DR: 本文指出SLMs在跨数据集泛化上有问题，原因是语音和文本表征的模态差距，提出OTReg方法缓解此差距，实验表明该方法有效提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前SLMs在跨数据集泛化存在困难，原因是语音和文本表征的模态差距，需要解决该问题。

Method: 引入OTReg方法，将语音 - 文本对齐表述为最优传输问题，通过确定最优传输计划建立对应关系，引入正则化损失优化SLM训练。

Result: 广泛的多语言ASR实验表明，OTReg增强了语音 - 文本对齐，缓解了模态差距。

Conclusion: OTReg能有效缓解模态差距，提升SLM在不同数据集上的泛化能力。

Abstract: Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to
perceive speech inputs, have gained increasing attention for their potential to
advance speech understanding tasks. However, despite recent progress, studies
show that SLMs often struggle to generalize across datasets, even for trained
languages and tasks, raising concerns about whether they process speech in a
text-like manner as intended. A key challenge underlying this limitation is the
modality gap between speech and text representations. The high variability in
speech embeddings may allow SLMs to achieve strong in-domain performance by
exploiting unintended speech variations, ultimately hindering generalization.
To mitigate this modality gap, we introduce Optimal Transport Regularization
(OTReg), a method that formulates speech-text alignment as an optimal transport
problem and derives a regularization loss to improve SLM training. In each
training iteration, OTReg first establishes a structured correspondence between
speech and transcript embeddings by determining the optimal transport plan,
then incorporates the regularization loss based on this transport plan to
optimize SLMs in generating speech embeddings that align more effectively with
transcript embeddings. OTReg is lightweight, requiring no additional labels or
learnable parameters, and integrates seamlessly into existing SLM training
procedures. Extensive multilingual ASR experiments demonstrate that OTReg
enhances speech-text alignment, mitigates the modality gap, and consequently
improves SLM generalization across diverse datasets.

</details>


### [544] [Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models](https://arxiv.org/abs/2508.08139)
*Tianyi Zhou,Johanne Medina,Sanjay Chawla*

Main category: cs.CL

TL;DR: 本文研究上下文信息对大语言模型行为的影响及模型能否识别不可靠响应，提出基于不确定性的可靠性估计方法，实验表明该方法能提升不可靠输出的检测能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易生成错误内容，在多轮或智能体应用中风险增大，需研究上下文信息对模型行为的影响及模型识别不可靠响应的能力。

Method: 提出可靠性估计方法，利用令牌级不确定性引导内部模型表示的聚合，通过输出对数计算不确定性来识别显著令牌并聚合其隐藏状态进行响应级可靠性预测。

Result: 正确上下文信息提高答案准确性和模型信心，误导性上下文导致自信的错误响应，基于探测的方法能捕捉模型行为变化，提升多开源模型不可靠输出的检测能力。

Conclusion: 直接不确定性信号有局限，基于不确定性引导的探测对可靠性感知生成有潜力。

Abstract: Large Language Models (LLMs) are prone to generating fluent but incorrect
content, known as confabulation, which poses increasing risks in multi-turn or
agentic applications where outputs may be reused as context. In this work, we
investigate how in-context information influences model behavior and whether
LLMs can identify their unreliable responses. We propose a reliability
estimation that leverages token-level uncertainty to guide the aggregation of
internal model representations. Specifically, we compute aleatoric and
epistemic uncertainty from output logits to identify salient tokens and
aggregate their hidden states into compact representations for response-level
reliability prediction. Through controlled experiments on open QA benchmarks,
we find that correct in-context information improves both answer accuracy and
model confidence, while misleading context often induces confidently incorrect
responses, revealing a misalignment between uncertainty and correctness. Our
probing-based method captures these shifts in model behavior and improves the
detection of unreliable outputs across multiple open-source LLMs. These results
underscore the limitations of direct uncertainty signals and highlight the
potential of uncertainty-guided probing for reliability-aware generation.

</details>


### [545] [Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models](https://arxiv.org/abs/2508.08204)
*Kyle Moore,Jesse Roberts,Daryl Watson*

Main category: cs.CL

TL;DR: 评估大语言模型推理时不确定性度量与人类不确定性和模型校准的对齐情况。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型不确定性校准评估感兴趣，且较少工作评估模型不确定性与人类不确定性的对齐程度，为改善大语言模型用户体验。

Method: 使用既有指标和新变体评估推理时不确定性度量。

Result: 许多度量与人类不确定性强对齐，与人类答案偏好缺乏对齐；成功指标在正确性相关性和分布分析上有中到强的模型校准证据。

Conclusion: 部分推理时不确定性度量在与人类不确定性对齐和模型校准方面表现良好。

Abstract: There has been much recent interest in evaluating large language models for
uncertainty calibration to facilitate model control and modulate user trust.
Inference time uncertainty, which may provide a real-time signal to the model
or external control modules, is particularly important for applying these
concepts to improve LLM-user experience in practice. While many of the existing
papers consider model calibration, comparatively little work has sought to
evaluate how closely model uncertainty aligns to human uncertainty. In this
work, we evaluate a collection of inference-time uncertainty measures, using
both established metrics and novel variations, to determine how closely they
align with both human group-level uncertainty and traditional notions of model
calibration. We find that numerous measures show evidence of strong alignment
to human uncertainty, even despite the lack of alignment to human answer
preference. For those successful metrics, we find moderate to strong evidence
of model calibration in terms of both correctness correlation and
distributional analysis.

</details>


### [546] [Capabilities of GPT-5 on Multimodal Medical Reasoning](https://arxiv.org/abs/2508.08224)
*Shansong Wang,Mingzhe Hu,Qiang Li,Mojtaba Safari,Xiaofeng Yang*

Main category: cs.CL

TL;DR: 研究评估GPT - 5在医学决策支持多模态推理的零样本思维链推理性能，结果显示其表现优于基线模型和人类专家，或为未来临床决策支持系统设计提供参考。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型发展使通用系统能进行复杂领域推理，医学决策需整合异质信息源，本研究旨在评估GPT - 5作为通用多模态推理器用于医学决策支持的性能。

Method: 采用统一协议，对GPT - 5、GPT - 5 - mini、GPT - 5 - nano和GPT - 4o - 2024 - 11 - 20在MedQA、MedXpertQA等多个标准化数据集的文本和视觉问答任务上进行零样本思维链推理性能评估。

Result: GPT - 5在所有问答基准测试中始终优于所有基线模型，在多模态推理方面有显著提升，在MedXpertQA MM上推理和理解得分大幅提高，超过人类专家。

Conclusion: 在受控的多模态推理基准测试中，GPT - 5从与人类相当提升到超越人类专家表现，这一改进或为未来临床决策支持系统设计提供重要依据。

Abstract: Recent advances in large language models (LLMs) have enabled general-purpose
systems to perform increasingly complex domain-specific reasoning without
extensive fine-tuning. In the medical domain, decision-making often requires
integrating heterogeneous information sources, including patient narratives,
structured data, and medical images. This study positions GPT-5 as a generalist
multimodal reasoner for medical decision support and systematically evaluates
its zero-shot chain-of-thought reasoning performance on both text-based
question answering and visual question answering tasks under a unified
protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20
against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU
medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that
GPT-5 consistently outperforms all baselines, achieving state-of-the-art
accuracy across all QA benchmarks and delivering substantial gains in
multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and
understanding scores by +29.62% and +36.18% over GPT-4o, respectively, and
surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in
understanding. In contrast, GPT-4o remains below human expert performance in
most dimensions. A representative case study demonstrates GPT-5's ability to
integrate visual and textual cues into a coherent diagnostic reasoning chain,
recommending appropriate high-stakes interventions. Our results show that, on
these controlled multimodal reasoning benchmarks, GPT-5 moves from
human-comparable to above human-expert performance. This improvement may
substantially inform the design of future clinical decision-support systems.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [547] [LSDTs: LLM-Augmented Semantic Digital Twins for Adaptive Knowledge-Intensive Infrastructure Planning](https://arxiv.org/abs/2508.06799)
*Naiyi Li,Zihui Ma,Runlong Yu,Lingyao Li*

Main category: cs.ET

TL;DR: 本文提出LSDTs框架，结合大语言模型与数字孪生，从非结构化文档提取规划知识并组织为本体，通过案例研究验证其在基础设施规划中的有效性。


<details>
  <summary>Details</summary>
Motivation: 数字孪生在整合非结构化知识方面存在挑战，大语言模型在提取和组织文本信息上有优势，因此提出LSDTs框架以解决该问题。

Method: 提出LSDTs框架，利用大语言模型从非结构化文档提取规划知识并组织为形式化本体，为数字孪生提供语义层。

Result: 通过马里兰州海上风电场规划案例研究，结果表明LSDTs支持可解释、符合法规的布局优化，实现高保真模拟，增强基础设施规划的适应性。

Conclusion: 结合生成式AI与数字孪生在支持复杂、知识驱动的规划任务方面具有潜力。

Abstract: Digital Twins (DTs) offer powerful tools for managing complex infrastructure
systems, but their effectiveness is often limited by challenges in integrating
unstructured knowledge. Recent advances in Large Language Models (LLMs) bring
new potential to address this gap, with strong abilities in extracting and
organizing diverse textual information. We therefore propose LSDTs
(LLM-Augmented Semantic Digital Twins), a framework that helps LLMs extract
planning knowledge from unstructured documents like environmental regulations
and technical guidelines, and organize it into a formal ontology. This ontology
forms a semantic layer that powers a digital twin-a virtual model of the
physical system-allowing it to simulate realistic, regulation-aware planning
scenarios. We evaluate LSDTs through a case study of offshore wind farm
planning in Maryland, including its application during Hurricane Sandy. Results
demonstrate that LSDTs support interpretable, regulation-aware layout
optimization, enable high-fidelity simulation, and enhance adaptability in
infrastructure planning. This work shows the potential of combining generative
AI with digital twins to support complex, knowledge-driven planning tasks.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [548] [Benchmarking Self-Driving Labs](https://arxiv.org/abs/2508.06642)
*Adedire D. Adesiji,Jiashuo Wang,Cheng-Shu Kuo,Keith A. Brown*

Main category: physics.comp-ph

TL;DR: 本文综述了自动驾驶实验室（SDLs）加速材料发现的进展，量化其减少实验次数的程度，分析关键指标并进行模拟实验，强化使用SDLs的动机。


<details>
  <summary>Details</summary>
Motivation: 现代材料科学旨在加速材料发现，SDLs可更高效完成实验，本文旨在量化其加速学习的程度。

Method: 总结关键指标AF和EF的理论，全面回顾文献，进行一系列模拟贝叶斯优化实验。

Result: 文献中AF中位数为6且随空间维度增加，EF值变化超两个数量级，在每维10 - 20次实验时达峰值；模拟实验揭示EF依赖参数空间统计特性，AF依赖其复杂性。

Conclusion: 结果强化了在广泛材料参数空间使用SDLs的动机，并提供量化和理解这种加速的通用语言。

Abstract: A key goal of modern materials science is accelerating the pace of materials
discovery. Self-driving labs, or systems that select experiments using machine
learning and then execute them using automation, are designed to fulfil this
promise by performing experiments faster, more intelligently, more reliably,
and with richer metadata than conventional means. This review summarizes
progress in understanding the degree to which SDLs accelerate learning by
quantifying how much they reduce the number of experiments required for a given
goal. The review begins by summarizing the theory underlying two key metrics,
namely acceleration factor AF and enhancement factor EF, which quantify how
much faster and better an algorithm is relative to a reference strategy. Next,
we provide a comprehensive review of the literature, which reveals a wide range
of AFs with a median of 6, and that tends to increase with the dimensionality
of the space, reflecting an interesting blessing of dimensionality. In
contrast, reported EF values vary by over two orders of magnitude, although
they consistently peak at 10-20 experiments per dimension. To understand these
results, we perform a series of simulated Bayesian optimization campaigns that
reveal how EF depends upon the statistical properties of the parameter space
while AF depends on its complexity. Collectively, these results reinforce the
motivation for using SDLs by revealing their value across a wide range of
material parameter spaces and provide a common language for quantifying and
understanding this acceleration.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [549] [Decomposing Global AUC into Cluster-Level Contributions for Localized Model Diagnostics](https://arxiv.org/abs/2508.07495)
*Agus Sudjianto,Alice J. Liu*

Main category: stat.AP

TL;DR: 本文提出将全局AUC分解为簇内和簇间组件，还对比AUC与其他可分解指标，为模型风险管理提供见解。


<details>
  <summary>Details</summary>
Motivation: AUC作为全局排名统计量会掩盖特定子群体的局部弱点，在高风险应用中可能导致财务风险或运营失败。

Method: 将全局AUC形式分解为簇内和簇间组件，并对比AUC与可分解的指标如Brier分数和对数损失。

Result: 实现了对数据簇内和簇间分类器性能的评估，可进行细粒度诊断和子组分析。

Conclusion: 框架通过提供额外见解来检测模型弱点，增强了模型开发和验证实践。

Abstract: The Area Under the ROC Curve (AUC) is a widely used performance metric for
binary classifiers. However, as a global ranking statistic, the AUC aggregates
model behavior over the entire dataset, masking localized weaknesses in
specific subpopulations. In high-stakes applications such as credit approval
and fraud detection, these weaknesses can lead to financial risk or operational
failures. In this paper, we introduce a formal decomposition of global AUC into
intra- and inter-cluster components. This allows practitioners to evaluate
classifier performance within and across clusters of data, enabling granular
diagnostics and subgroup analysis. We also compare the AUC with additive
performance metrics such as the Brier score and log loss, which support
decomposability and direct attribution. Our framework enhances model
development and validation practice by providing additional insights to detect
model weakness for model risk management.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [550] [Reconstruction of Solar EUV Irradiance Using CaII K Images and SOHO/SEM Data with Bayesian Deep Learning and Uncertainty Quantification](https://arxiv.org/abs/2508.07065)
*Haodi Jiang,Qin Li,Jason T. L. Wang,Haimin Wang,Serena Criscuoli*

Main category: astro-ph.SR

TL;DR: 提出贝叶斯深度学习模型SEMNet填补太阳EUV通量数据缺口，验证其可行性并拓展应用，助于长期太阳对地球气候影响研究。


<details>
  <summary>Details</summary>
Motivation: 太阳EUV辐照度对地球大气有重要影响，但缺乏多太阳周期EUV通量长期演变研究且早期数据有缺口。

Method: 提出贝叶斯深度学习模型SEMNet，用CaII K图像构建EUV通量测量，通过迁移学习拓展应用。

Result: SEMNet能提供可靠预测和不确定性边界，证明CaII K图像可作为长期EUV通量可靠替代。

Conclusion: 研究成果有助于更深入理解太阳对地球气候的长期影响。

Abstract: Solar extreme ultraviolet (EUV) irradiance plays a crucial role in heating
the Earth's ionosphere, thermosphere, and mesosphere, affecting atmospheric
dynamics over varying time scales. Although significant effort has been spent
studying short-term EUV variations from solar transient events, there is little
work to explore the long-term evolution of the EUV flux over multiple solar
cycles. Continuous EUV flux measurements have only been available since 1995,
leaving significant gaps in earlier data. In this study, we propose a Bayesian
deep learning model, named SEMNet, to fill the gaps. We validate our approach
by applying SEMNet to construct SOHO/SEM EUV flux measurements in the period
between 1998 and 2014 using CaII K images from the Precision Solar Photometric
Telescope. We then extend SEMNet through transfer learning to reconstruct solar
EUV irradiance in the period between 1950 and 1960 using CaII K images from the
Kodaikanal Solar Observatory. Experimental results show that SEMNet provides
reliable predictions along with uncertainty bounds, demonstrating the
feasibility of CaII K images as a robust proxy for long-term EUV fluxes. These
findings contribute to a better understanding of solar influences on Earth's
climate over extended periods.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [551] [Taking the Garbage Out of Data-Driven Prediction Across Climate Timescales](https://arxiv.org/abs/2508.07062)
*Jason C. Furtado,Maria J. Molina,Marybeth C. Arcodia,Weston Anderson,Tom Beucler,John A. Callahan,Laura M. Ciasto,Vittorio A. Gensini,Michelle L'Heureux,Kathleen Pegion,Jhayron S. Pérez-Carrasquilla,Maike Sonnewald,Ken Takahashi,Baoqiang Xiang,Brian G. Zimmerman*

Main category: physics.data-an

TL;DR: 文章为气候预测的AI/ML模型输入数据预处理制定协议，介绍相关话题和案例，强调实施推荐做法可增强气候预测研究的稳健性和透明度。


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML在气候预测应用增多，传统统计模型样本小，数据预处理影响模型预测技能和信心，需重新审视其影响。

Method: 建立气候预测AI/ML模型输入数据的预处理协议，涵盖创建标准化异常值、处理非平稳性和时空相关性、处理极值和复杂分布变量等。

Result: 不同预处理技术会使同一模型产生不同预测结果，造成困惑并降低整体过程的可信度。

Conclusion: 实施文章中的推荐做法能增强气候预测研究中AI/ML的稳健性和透明度。

Abstract: Artificial intelligence (AI) -- and specifically machine learning (ML) --
applications for climate prediction across timescales are proliferating
quickly. The emergence of these methods prompts a revisit to the impact of data
preprocessing, a topic familiar to the climate community, as more traditional
statistical models work with relatively small sample sizes. Indeed, the skill
and confidence in the forecasts produced by data-driven models are directly
influenced by the quality of the datasets and how they are treated during model
development, thus yielding the colloquialism "garbage in, garbage out." As
such, this article establishes protocols for the proper preprocessing of input
data for AI/ML models designed for climate prediction (i.e., subseasonal to
decadal and longer). The three aims are to: (1) educate researchers,
developers, and end users on the effects that preprocessing has on climate
predictions; (2) provide recommended practices for data preprocessing for such
applications; and (3) empower end users to decipher whether the models they are
using are properly designed for their objectives. Specific topics covered in
this article include the creation of (standardized) anomalies, dealing with
non-stationarity and the spatiotemporally correlated nature of climate data,
and handling of extreme values and variables with potentially complex
distributions. Case studies will illustrate how using different preprocessing
techniques can produce different predictions from the same model, which can
create confusion and decrease confidence in the overall process. Ultimately,
implementing the recommended practices set forth in this article will enhance
the robustness and transparency of AI/ML in climate prediction studies.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [552] [Frequency-Domain Analysis of Time-Dependent Multiomic Data in Progressive Neurodegenerative Diseases: A Proposed Quantum-Classical Hybrid Approach with Quaternionic Extensions](https://arxiv.org/abs/2508.07948)
*John D. Mayfield*

Main category: q-bio.OT

TL;DR: 提出用傅里叶和拉普拉斯变换、哈密顿公式、量子 - 经典混合计算等构建数学框架分析神经退行性疾病，还拓展到四元数表示，有潜在临床应用，为精准医疗奠基。


<details>
  <summary>Details</summary>
Motivation: 神经退行性疾病轨迹复杂，传统时域分析难以捕捉隐藏振荡模式，限制预测准确性。

Method: 将时间序列数据转换到频率或s域，用哈密顿公式建模神经元动力学，采用量子 - 经典混合计算和变分量子本征求解器，拓展到四元数表示。

Result: 该方法可利用量子优势处理高维振幅 - 相位数据，实现异常检测和频率特征分析。

Conclusion: 该框架为神经退行性疾病的量子增强分析奠定基础，有望重新定义精准医疗。

Abstract: Progressive neurodegenerative diseases, including Alzheimer's disease (AD),
multiple sclerosis (MS), Parkinson's disease (PD), and amyotrophic lateral
sclerosis (ALS), exhibit complex, nonlinear trajectories that challenge
deterministic modeling. Traditional time-domain analyses of multiomic and
neuroimaging data often fail to capture hidden oscillatory patterns, limiting
predictive accuracy. We propose a theoretical mathematical framework that
transforms time-series data into frequency or s-domain using Fourier and
Laplace transforms, models neuronal dynamics via Hamiltonian formulations, and
employs quantum-classical hybrid computing with variational quantum
eigensolvers (VQE) for enhanced pattern detection. This theoretical construct
serves as a foundation for future empirical works in quantum-enhanced analysis
of neurodegenerative diseases. We extend this to quaternionic representations
with three imaginary axes ($i, j, k$) to model multistate Hamiltonians in
multifaceted disorders, drawing from quantum neuromorphic computing to capture
entangled neural dynamics \citep{Pehle2020, Emani2019}. This approach leverages
quantum advantages in handling high-dimensional amplitude-phase data, enabling
outlier detection and frequency signature analysis. Potential clinical
applications include identifying high-risk patients with rapid progression or
therapy resistance using s-domain biomarkers, supported by quantum machine
learning (QML) precedents achieving up to 99.89% accuracy in Alzheimer's
classification \citep{Belay2024, Bhowmik2025}. This framework aims to lay the
groundwork for redefining precision medicine for neurodegenerative diseases
through future validations.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [553] [Maestro-EVC: Controllable Emotional Voice Conversion Guided by References and Explicit Prosody](https://arxiv.org/abs/2508.06890)
*Jinsung Yoon,Wooyeol Jeong,Jio Gim,Young-Joo Suh*

Main category: cs.SD

TL;DR: 提出Maestro - EVC框架用于情感语音转换，实现内容、说话人身份和情感的独立控制，实验验证其高质量、可控和富有情感表达。


<details>
  <summary>Details</summary>
Motivation: 现有情感语音转换方法难以完全分离说话人身份和情感风格属性，且缺乏对细粒度情感表达（如时间动态）的建模能力，而实际应用中需要对这些属性进行独立控制。

Method: 提出Maestro - EVC框架，有效分离内容、说话人身份和情感属性；引入时间情感表示和显式韵律建模及韵律增强，以捕捉和传递目标情感的时间动态。

Result: 实验结果证实Maestro - EVC实现了高质量、可控且富有情感表达的语音合成。

Conclusion: Maestro - EVC框架在情感语音转换方面表现良好，能有效解决现有方法的不足。

Abstract: Emotional voice conversion (EVC) aims to modify the emotional style of speech
while preserving its linguistic content. In practical EVC, controllability, the
ability to independently control speaker identity and emotional style using
distinct references, is crucial. However, existing methods often struggle to
fully disentangle these attributes and lack the ability to model fine-grained
emotional expressions such as temporal dynamics. We propose Maestro-EVC, a
controllable EVC framework that enables independent control of content, speaker
identity, and emotion by effectively disentangling each attribute from separate
references. We further introduce a temporal emotion representation and an
explicit prosody modeling with prosody augmentation to robustly capture and
transfer the temporal dynamics of the target emotion, even under
prosody-mismatched conditions. Experimental results confirm that Maestro-EVC
achieves high-quality, controllable, and emotionally expressive speech
synthesis.

</details>


### [554] [Whisfusion: Parallel ASR Decoding via a Diffusion Transformer](https://arxiv.org/abs/2508.07048)
*Taeyoun Kwon,Junhyuk Ahn,Taegeun Yun,Heeju Jwa,Yoonchae Choi,Siwon Park,Nam-Joon Kim,Jangchan Kim,Hyun Gon Ryu,Hyuk-Jae Lee*

Main category: cs.SD

TL;DR: 提出Whisfusion框架融合Whisper编码器与文本扩散解码器，解决AR解码延迟瓶颈，在长音频上有速度优势。


<details>
  <summary>Details</summary>
Motivation: 快速自动语音识别对低延迟应用很关键，但真正的并行ASR解码因AR解码器的顺序性和NAR方法的上下文限制而具有挑战性。

Method: 提出Whisfusion框架，融合预训练的Whisper编码器与文本扩散解码器，用轻量级跨注意力适配器连接两种模态，引入批并行、多步解码策略。

Result: 仅在LibriSpeech上微调，Whisfusion的WER低于Whisper - tiny，短音频延迟相当，长音频比AR基线快达2.6倍。

Conclusion: Whisfusion为长格式ASR建立了新的高效操作点。

Abstract: Fast Automatic Speech Recognition (ASR) is critical for latency-sensitive
applications such as real-time captioning and meeting transcription. However,
truly parallel ASR decoding remains challenging due to the sequential nature of
autoregressive (AR) decoders and the context limitations of non-autoregressive
(NAR) methods. While modern ASR encoders can process up to 30 seconds of audio
at once, AR decoders still generate tokens sequentially, creating a latency
bottleneck. We propose Whisfusion, the first framework to fuse a pre-trained
Whisper encoder with a text diffusion decoder. This NAR architecture resolves
the AR latency bottleneck by processing the entire acoustic context in parallel
at every decoding step. A lightweight cross-attention adapter trained via
parameter-efficient fine-tuning (PEFT) bridges the two modalities. We also
introduce a batch-parallel, multi-step decoding strategy that improves accuracy
by increasing the number of candidates with minimal impact on speed. Fine-tuned
solely on LibriSpeech (960h), Whisfusion achieves a lower WER than Whisper-tiny
(8.3% vs. 9.7%), and offers comparable latency on short audio. For longer
utterances (>20s), it is up to 2.6x faster than the AR baseline, establishing a
new, efficient operating point for long-form ASR. The implementation and
training scripts are available at https://github.com/taeyoun811/Whisfusion.

</details>


### [555] [SEF-MK: Speaker-Embedding-Free Voice Anonymization through Multi-k-means Quantization](https://arxiv.org/abs/2508.07086)
*Beilong Tang,Xiaoxiao Miao,Xin Wang,Ming Li*

Main category: cs.SD

TL;DR: 提出SEF - MK框架对语音SSL表征进行匿名化，从攻防视角实验，发现多k - means模型在保留内容和隐私攻击方面有不同表现。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习表征在编码语言特征时会保留说话人特征，需探索更好的语音匿名化方法。

Method: 提出SEF - MK框架，为每个话语随机选择多个k - means模型之一对SSL表征进行匿名化，每个模型在不同说话人子集上训练。

Result: 从用户角度，多k - means模型的SEF - MK能更好保留语言和情感内容；从攻击者角度，多k - means模型提高了隐私攻击效果。

Conclusion: 研究结果有助于用户设计语音匿名化系统以减轻攻击者威胁。

Abstract: Voice anonymization protects speaker privacy by concealing identity while
preserving linguistic and paralinguistic content. Self-supervised learning
(SSL) representations encode linguistic features but preserve speaker traits.
We propose a novel speaker-embedding-free framework called SEF-MK. Instead of
using a single k-means model trained on the entire dataset, SEF-MK anonymizes
SSL representations for each utterance by randomly selecting one of multiple
k-means models, each trained on a different subset of speakers. We explore this
approach from both attacker and user perspectives. Extensive experiments show
that, compared to a single k-means model, SEF-MK with multiple k-means models
better preserves linguistic and emotional content from the user's viewpoint.
However, from the attacker's perspective, utilizing multiple k-means models
boosts the effectiveness of privacy attacks. These insights can aid users in
designing voice anonymization systems to mitigate attacker threats.

</details>


### [556] [A Small-footprint Acoustic Echo Cancellation Solution for Mobile Full-Duplex Speech Interactions](https://arxiv.org/abs/2508.07561)
*Yiheng Jiang,Tian Biao*

Main category: cs.SD

TL;DR: 本文提出基于神经网络的AEC解决方案，结合数据增强、渐进学习和后处理策略，用小模型实现移动设备部署，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决全双工语音交互系统在移动场景中，因硬件不同、非线性失真和长延迟带来的声学回声消除挑战。

Method: 结合多样数据增强策略、渐进学习、新颖后处理策略，使用小模型进行流式推理。

Result: 该方法在回声返回损耗增强和语音质量感知评估中有效，语音活动检测和自动语音识别结果显著改善。

Conclusion: 所提方法能有效解决移动场景声学回声消除问题，可在移动设备无缝部署。

Abstract: In full-duplex speech interaction systems, effective Acoustic Echo
Cancellation (AEC) is crucial for recovering echo-contaminated speech. This
paper presents a neural network-based AEC solution to address challenges in
mobile scenarios with varying hardware, nonlinear distortions and long latency.
We first incorporate diverse data augmentation strategies to enhance the
model's robustness across various environments. Moreover, progressive learning
is employed to incrementally improve AEC effectiveness, resulting in a
considerable improvement in speech quality. To further optimize AEC's
downstream applications, we introduce a novel post-processing strategy
employing tailored parameters designed specifically for tasks such as Voice
Activity Detection (VAD) and Automatic Speech Recognition (ASR), thus enhancing
their overall efficacy. Finally, our method employs a small-footprint model
with streaming inference, enabling seamless deployment on mobile devices.
Empirical results demonstrate effectiveness of the proposed method in Echo
Return Loss Enhancement and Perceptual Evaluation of Speech Quality, alongside
significant improvements in both VAD and ASR results.

</details>


### [557] [SCDF: A Speaker Characteristics DeepFake Speech Dataset for Bias Analysis](https://arxiv.org/abs/2508.07944)
*Vojtěch Staněk,Karel Srna,Anton Firc,Kamil Malinka*

Main category: cs.SD

TL;DR: 本文引入SCDF数据集评估深度伪造语音检测中的人口偏差，发现说话者特征影响检测性能，强调需开发无偏检测系统。


<details>
  <summary>Details</summary>
Motivation: 解决深度伪造语音检测中偏差和公平性研究不足的问题。

Method: 引入SCDF数据集，对多种最先进的检测器进行评估。

Result: 说话者特征显著影响检测性能，在性别、语言、年龄和合成器类型方面存在差异。

Conclusion: 需要进行考虑偏差的开发，为构建符合道德和监管标准的无歧视性深度伪造检测系统奠定基础。

Abstract: Despite growing attention to deepfake speech detection, the aspects of bias
and fairness remain underexplored in the speech domain. To address this gap, we
introduce the Speaker Characteristics Deepfake (SCDF) dataset: a novel, richly
annotated resource enabling systematic evaluation of demographic biases in
deepfake speech detection. SCDF contains over 237,000 utterances in a balanced
representation of both male and female speakers spanning five languages and a
wide age range. We evaluate several state-of-the-art detectors and show that
speaker characteristics significantly influence detection performance,
revealing disparities across sex, language, age, and synthesizer type. These
findings highlight the need for bias-aware development and provide a foundation
for building non-discriminatory deepfake detection systems aligned with ethical
and regulatory standards.

</details>


### [558] [Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking Self-Supervised and Generative Approaches](https://arxiv.org/abs/2508.08027)
*Ahmed Aboeitta,Ahmed Sharshar,Youssef Nafea,Shady Shehata*

Main category: cs.SD

TL;DR: 本文对自监督ASR模型在构音障碍语音上用不同解码策略进行基准测试，发现大语言模型增强解码可改善识别效果。


<details>
  <summary>Details</summary>
Motivation: 自监督ASR模型在构音障碍语音中的有效性尚不明确，需进行研究。

Method: 对Wav2Vec、HuBERT、Whisper等模型采用CTC、seq2seq、大语言模型增强解码（BART、GPT - 2、Vicuna）等不同解码策略进行基准测试。

Result: 大语言模型增强解码通过利用语言约束进行音素恢复和语法纠正，改善了构音障碍语音的自动语音识别。

Conclusion: 大语言模型增强解码对构音障碍语音的自动语音识别有积极作用，研究还在架构基准测试、跨数据集泛化分析和错误分析等方面有贡献。

Abstract: Speech Recognition (ASR) due to phoneme distortions and high variability.
While self-supervised ASR models like Wav2Vec, HuBERT, and Whisper have shown
promise, their effectiveness in dysarthric speech remains unclear. This study
systematically benchmarks these models with different decoding strategies,
including CTC, seq2seq, and LLM-enhanced decoding (BART,GPT-2, Vicuna). Our
contributions include (1) benchmarking ASR architectures for dysarthric speech,
(2) introducing LLM-based decoding to improve intelligibility, (3) analyzing
generalization across datasets, and (4) providing insights into recognition
errors across severity levels. Findings highlight that LLM-enhanced decoding
improves dysarthric ASR by leveraging linguistic constraints for phoneme
restoration and grammatical correction.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [559] [A Spin Glass Characterization of Neural Networks](https://arxiv.org/abs/2508.07397)
*Jun Li*

Main category: cond-mat.dis-nn

TL;DR: 本文受自旋玻璃中复本对称破缺现象启发，对神经网络进行统计力学表征，构建模型研究自旋玻璃描述与FNN属性联系，结果显示有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 受自旋玻璃中复本对称破缺（RSB）现象启发，对神经网络进行统计力学表征。

Method: 从给定前馈神经网络（FNN）构建Hopfield型自旋玻璃模型，用模拟复本样本间的重叠作为FNN的特征描述符，研究自旋玻璃描述与FNN常见属性的联系。

Result: 该方法为单个网络实例提供了可计算的描述符，揭示了传统指标未捕捉到的非平凡结构属性。

Conclusion: 该方法在模型检查、安全验证和隐藏漏洞检测等实际应用中有潜力。

Abstract: This work presents a statistical mechanics characterization of neural
networks, motivated by the replica symmetry breaking (RSB) phenomenon in spin
glasses. A Hopfield-type spin glass model is constructed from a given
feedforward neural network (FNN). Overlaps between simulated replica samples
serve as a characteristic descriptor of the FNN. The connection between the
spin-glass description and commonly studied properties of the FNN -- such as
data fitting, capacity, generalization, and robustness -- has been investigated
and empirically demonstrated. Unlike prior analytical studies that focus on
model ensembles, this method provides a computable descriptor for individual
network instances, which reveals nontrivial structural properties that are not
captured by conventional metrics such as loss or accuracy. Preliminary results
suggests its potential for practical applications such as model inspection,
safety verification, and detection of hidden vulnerabilities.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [560] [GPU-Accelerated Syndrome Decoding for Quantum LDPC Codes below the 63 $μ$s Latency Threshold](https://arxiv.org/abs/2508.07879)
*Oscar Ferraz,Bruno Coutinho,Gabriel Falcao,Marco Gomes,Francisco A. Monteiro,Vitor Silva*

Main category: quant-ph

TL;DR: 本文提出GPU加速的量子低密度奇偶校验（QLDPC）码解码器，实现低延迟，证明用通用硬件可实现渐近良好量子码实时可扩展解码。


<details>
  <summary>Details</summary>
Motivation: 表面码编码率随码距增加趋近于零，扩展性有挑战；新提出的QLDPC码解码复杂度高，需解决其解码问题。

Method: 在通用GPU硬件上利用综合征信息实现并行化置信传播解码器，在目标延迟限制内利用并行性最大化性能。

Result: [[784, 24, 24]]码解码延迟低于50 μs，较小码低至23.3 μs。

Conclusion: 使用通用硬件可实现渐近良好量子码的实时可扩展解码，推进了超越表面码的容错量子计算可行性。

Abstract: This paper presents a GPU-accelerated decoder for quantum low-density
parity-check (QLDPC) codes that achieves sub-$63$ $\mu$s latency, below the
surface code decoder's real-time threshold demonstrated on Google's Willow
quantum processor. While surface codes have demonstrated below-threshold
performance, the encoding rates approach zero as code distances increase,
posing challenges for scalability. Recently proposed QLDPC codes, such as those
by Panteleev and Kalachev, offer constant-rate encoding and asymptotic goodness
but introduce higher decoding complexity. To address such limitation, this work
presents a parallelized belief propagation decoder leveraging syndrome
information on commodity GPU hardware. Parallelism was exploited to maximize
performance within the limits of target latency, allowing decoding latencies
under $50$ $\mu$s for [[$784$, $24$, $24$]] codes and as low as $23.3$ $\mu$s
for smaller codes, meeting the tight timing constraints of superconducting
qubit cycles. These results show that real-time, scalable decoding of
asymptotically good quantum codes is achievable using widely available
commodity hardware, advancing the feasibility of fault-tolerant quantum
computation beyond surface codes.

</details>


### [561] [QuProFS: An Evolutionary Training-free Approach to Efficient Quantum Feature Map Search](https://arxiv.org/abs/2508.07104)
*Yaswitha Gujju,Romain Harang,Chao Li,Tetsuo Shibuya,Qibin Zhao*

Main category: quant-ph

TL;DR: 提出无训练的量子架构搜索框架，在分类任务中表现出色，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决参数化量子电路训练景观平坦和训练过程长的问题，寻找有效的量子特征映射进行数据编码。

Method: 提出进化无训练量子架构搜索框架，用基于电路的启发式方法对电路架构排名，结合硬件感知电路。

Result: 在模拟器和真实量子硬件上有有竞争力的准确性，在采样效率上超越现有QAS方法，架构搜索运行时间最多加速2倍。

Conclusion: 所提方法在量子架构搜索方面有效，能提升性能。

Abstract: The quest for effective quantum feature maps for data encoding presents
significant challenges, particularly due to the flat training landscapes and
lengthy training processes associated with parameterised quantum circuits. To
address these issues, we propose an evolutionary training-free quantum
architecture search (QAS) framework that employs circuit-based heuristics
focused on trainability, hardware robustness, generalisation ability,
expressivity, complexity, and kernel-target alignment. By ranking circuit
architectures with various proxies, we reduce evaluation costs and incorporate
hardware-aware circuits to enhance robustness against noise. We evaluate our
approach on classification tasks (using quantum support vector machine) across
diverse datasets using both artificial and quantum-generated datasets. Our
approach demonstrates competitive accuracy on both simulators and real quantum
hardware, surpassing state-of-the-art QAS methods in terms of sampling
efficiency and achieving up to a 2x speedup in architecture search runtime.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [562] [Rethinking Self-Replication: Detecting Distributed Selfhood in the Outlier Cellular Automaton](https://arxiv.org/abs/2508.08047)
*Arend Hintze,Clifford Bohm*

Main category: nlin.CG

TL;DR: 本文提供形式化因果证据，表明细胞自动机中自发自我复制可自然出现且为分布式多组件形式，还介绍框架识别自复制结构。


<details>
  <summary>Details</summary>
Motivation: 长期以来细胞自动机中自发自我复制被认为罕见，已知例子多需精心设计或人工初始化，本文旨在证明其可自然出现。

Method: 基于先前对Outlier规则中复杂动力学的研究，引入数据驱动框架重建确定性细胞自动机中模式的完整因果谱系。

Result: 明确表明Outlier CA中的自复制体不仅是自发且稳健的，还常由多个不相交的集群协同组成。

Conclusion: 这对人工生命系统中一些关于个体性和复制的传统概念提出了质疑。

Abstract: Spontaneous self-replication in cellular automata has long been considered
rare, with most known examples requiring careful design or artificial
initialization. In this paper, we present formal, causal evidence that such
replication can emerge unassisted -- and that it can do so in a distributed,
multi-component form. Building on prior work identifying complex dynamics in
the Outlier rule, we introduce a data-driven framework that reconstructs the
full causal ancestry of patterns in a deterministic cellular automaton. This
allows us to rigorously identify self-replicating structures via explicit
causal lineages. Our results show definitively that self-replicators in the
Outlier CA are not only spontaneous and robust, but are also often composed of
multiple disjoint clusters working in coordination, raising questions about
some conventional notions of individuality and replication in artificial life
systems.

</details>
