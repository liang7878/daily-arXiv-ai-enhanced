<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 38]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 11]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 102]
- [cs.SE](#cs.SE) [Total: 16]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [stat.CO](#stat.CO) [Total: 1]
- [eess.SP](#eess.SP) [Total: 4]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.CY](#cs.CY) [Total: 5]
- [math.PR](#math.PR) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [math.CO](#math.CO) [Total: 2]
- [cs.CV](#cs.CV) [Total: 25]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.CR](#cs.CR) [Total: 16]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [cs.CL](#cs.CL) [Total: 40]
- [cs.HC](#cs.HC) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [math.OC](#math.OC) [Total: 5]
- [cs.DM](#cs.DM) [Total: 1]
- [stat.ME](#stat.ME) [Total: 4]
- [math.ST](#math.ST) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]
- [econ.GN](#econ.GN) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Quantum-Inspired Algorithm for Solving Sudoku Puzzles and the MaxCut Problem](https://arxiv.org/abs/2510.19835)
*Max B. Zhao,Fei Li*

Main category: cs.AI

TL;DR: 提出并评估了一种量子启发算法解决QUBO问题，该算法用MPS和离散驱动调度，能找到全局最优解，在数独和MaxCut问题上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 解决QUBO问题，找到全局最优解。

Method: 采用MPS紧凑表示自旋配置，用离散驱动调度引导MPS到基态，结合驱动哈密顿量和问题哈密顿量，用DMRG方法更新MPS。

Result: 在数独和MaxCut问题上成功求解，数独涉及超200个自旋，MaxCut问题最多求解251个节点和3265条边的实例。

Conclusion: 该量子启发算法具有可扩展性、通用性，适合工业规模的QUBO应用。

Abstract: We propose and evaluate a quantum-inspired algorithm for solving Quadratic
Unconstrained Binary Optimization (QUBO) problems, which are mathematically
equivalent to finding ground states of Ising spin-glass Hamiltonians. The
algorithm employs Matrix Product States (MPS) to compactly represent large
superpositions of spin configurations and utilizes a discrete driving schedule
to guide the MPS toward the ground state. At each step, a driver Hamiltonian --
incorporating a transverse magnetic field -- is combined with the problem
Hamiltonian to enable spin flips and facilitate quantum tunneling. The MPS is
updated using the standard Density Matrix Renormalization Group (DMRG) method,
which iteratively minimizes the system's energy via multiple sweeps across the
spin chain. Despite its heuristic nature, the algorithm reliably identifies
global minima, not merely near-optimal solutions, across diverse QUBO
instances. We first demonstrate its effectiveness on intermediate-level Sudoku
puzzles from publicly available sources, involving over $200$ Ising spins with
long-range couplings dictated by constraint satisfaction. We then apply the
algorithm to MaxCut problems from the Biq Mac library, successfully solving
instances with up to $251$ nodes and $3,265$ edges. We discuss the advantages
of this quantum-inspired approach, including its scalability, generalizability,
and suitability for industrial-scale QUBO applications.

</details>


### [2] [AI PB: A Grounded Generative Agent for Personalized Investment Insights](https://arxiv.org/abs/2510.20099)
*Daewoo Park,Suho Park,Inseok Hong,Hanwool Lee,Junkyu Park,Sangjun Lee,Jeongman An,Hyunbin Loh*

Main category: cs.AI

TL;DR: 介绍在零售金融领域部署的生成式智能体AI PB，阐述其特点、架构及运行环境，证明可在金融领域提供可靠AI见解。


<details>
  <summary>Details</summary>
Motivation: 在零售金融领域提供主动、合规且用户特定的投资见解，解决被动聊天机器人的局限。

Method: 集成基于数据敏感性的组件编排层、混合检索管道和多阶段推荐机制，采用Docker Swarm和vLLM在24块NVIDIA H100 GPU上运行。

Result: 通过人工QA和系统指标验证。

Conclusion: 具有显式路由和分层安全的基础生成可在高风险金融领域提供可信AI见解。

Abstract: We present AI PB, a production-scale generative agent deployed in real retail
finance. Unlike reactive chatbots that answer queries passively, AI PB
proactively generates grounded, compliant, and user-specific investment
insights. It integrates (i) a component-based orchestration layer that
deterministically routes between internal and external LLMs based on data
sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the
finance-domain embedding model, and (iii) a multi-stage recommendation
mechanism combining rule heuristics, sequential behavioral modeling, and
contextual bandits. Operating fully on-premises under Korean financial
regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100
GPUs. Through human QA and system metrics, we demonstrate that grounded
generation with explicit routing and layered safety can deliver trustworthy AI
insights in high-stakes finance.

</details>


### [3] [Benchmarking Reasoning Reliability in Artificial Intelligence Models for Energy-System Analysis](https://arxiv.org/abs/2510.19836)
*Eliseo Curcio*

Main category: cs.AI

TL;DR: 本文介绍用于评估能源系统分析中大语言模型推理可靠性的分析可靠性基准（ARB），测试四个前沿模型，结果显示推理可靠性可客观测量，ARB为能源领域AI系统推理验证提供定量方法。


<details>
  <summary>Details</summary>
Motivation: 能源领域AI和机器学习应用广泛，但缺乏评估系统推理正确性的标准化框架，现有验证实践未测试分析结论的逻辑完整性。

Method: 引入ARB框架，整合五个子指标，利用公开技术经济数据集，在相同条件下测试四个前沿模型。

Result: 推理可靠性可客观测量，GPT - 4/5和Claude 4.5 Sonnet推理表现佳，Gemini 2.5 Pro稳定性一般，Llama 3 70B未达专业阈值，差异显著且可重复。

Conclusion: ARB为能源文献中验证AI系统推理提供首个定量方法，为全球能源转型中可信透明的分析应用提供参考框架。

Abstract: Artificial intelligence and machine learning are increasingly used for
forecasting, optimization, and policy design in the energy sector, yet no
standardized framework exists to evaluate whether these systems reason
correctly. Current validation practices focus on predictive accuracy or
computational efficiency, leaving the logical integrity of analytical
conclusions untested. This study introduces the Analytical Reliability
Benchmark (ARB), a reproducible framework that quantifies reasoning reliability
in large language models applied to energy system analysis. The benchmark
integrates five submetrics: accuracy, reasoning reliability, uncertainty
discipline, policy consistency, and transparency, and evaluates model
performance across deterministic, probabilistic, and epistemic scenarios using
open technoeconomic datasets (NREL ATB 2024, DOE H2A/H2New, IEA WEO 2024). Four
frontier models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) were
tested under identical factual and regulatory conditions. Results show that
reasoning reliability can be objectively measured. GPT-4/5 and Claude 4.5
Sonnet achieved consistent and policy-compliant reasoning (Analytical
Reliability Index greater than 90), Gemini 2.5 Pro demonstrated moderate
stability, and Llama 3 70B remained below professional thresholds. Statistical
validation confirmed that these differences are significant and reproducible.
The ARB establishes the first quantitative method in the energy literature for
verifying causal, probabilistic, and policy-driven reasoning in artificial
intelligence systems, providing a reference framework for trustworthy and
transparent analytical applications in the global energy transition.

</details>


### [4] [Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory](https://arxiv.org/abs/2510.19838)
*Shiqi He,Yue Cui,Xinyu Ma,Yaliang Li,Bolin Ding,Mosharaf Chowdhury*

Main category: cs.AI

TL;DR: 提出Branch - and - Browse框架用于大语言模型驱动的自主网络代理，在WebArena基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型驱动的自主网络代理在推理深度和效率上存在局限，如线性方法多步推理能力差、其他搜索策略粗粒度且计算成本高。

Method: 引入Branch - and - Browse框架，包括树结构探索的显式子任务管理、通过高效网页状态重放和后台推理引导探索、利用页面动作记忆共享探索的动作。

Result: 在WebArena基准测试中，任务成功率达35.8%，执行时间最多减少40.4%。

Conclusion: Branch - and - Browse是基于大语言模型的网络代理可靠且高效的框架。

Abstract: Autonomous web agents powered by large language models (LLMs) show strong
potential for performing goal-oriented tasks such as information retrieval,
report generation, and online transactions. These agents mark a key step toward
practical embodied reasoning in open web environments. However, existing
approaches remain limited in reasoning depth and efficiency: vanilla linear
methods fail at multi-step reasoning and lack effective backtracking, while
other search strategies are coarse-grained and computationally costly. We
introduce Branch-and-Browse, a fine-grained web agent framework that unifies
structured reasoning-acting, contextual memory, and efficient execution. It (i)
employs explicit subtask management with tree-structured exploration for
controllable multi-branch reasoning, (ii) bootstraps exploration through
efficient web state replay with background reasoning, and (iii) leverages a
page action memory to share explored actions within and across sessions. On the
WebArena benchmark, Branch-and-Browse achieves a task success rate of 35.8\%
and reduces execution time by up to 40.4\% relative to state-of-the-art
methods. These results demonstrate that Branch-and-Browse is a reliable and
efficient framework for LLM-based web agents.

</details>


### [5] [DAG-Math: Graph-Guided Mathematical Reasoning in LLMs](https://arxiv.org/abs/2510.19842)
*Yuanhe Zhang,Ilja Kuzborskij,Jason D. Lee,Chenlei Leng,Fanghui Liu*

Main category: cs.AI

TL;DR: 本文提出将思维链建模为有向无环图上的规则随机过程，引入逻辑紧密度指标和DAG - MATH CoT格式基准，分析发现不同大语言模型家族推理保真度有差异，框架可用于大语言模型推理评估。


<details>
  <summary>Details</summary>
Motivation: 不清楚大语言模型在链式思维提示下解决数学问题的成功是源于搜索、机械程序还是规则一致推理，需评估其推理能力。

Method: 将链式思维建模为有向无环图上的规则随机过程，引入逻辑紧密度指标，构建DAG - MATH CoT格式基准引导大语言模型生成思维链轨迹。

Result: 在标准数学推理数据集上分析发现，即使PASS@k相近，不同大语言模型家族推理保真度也存在显著差异，凸显最终答案准确性和规则一致推导之间的差距。

Conclusion: 框架平衡了自由形式思维链和形式证明系统，为大语言模型推理评估提供了可行诊断方法。

Abstract: Large Language Models (LLMs) demonstrate strong performance on mathematical
problems when prompted with Chain-of-Thought (CoT), yet it remains unclear
whether this success stems from search, rote procedures, or rule-consistent
reasoning. To address this, we propose modeling CoT as a certain rule-based
stochastic process over directed acyclic graphs (DAGs), where nodes represent
intermediate derivation states and edges encode rule applications. Within this
framework, we introduce logical closeness, a metric that quantifies how well a
model's CoT trajectory (i.e., the LLM's final output) adheres to the DAG
structure, providing evaluation beyond classical PASS@k metrics. Building on
this, we introduce the DAG-MATH CoT format and construct a benchmark that
guides LLMs to generate CoT trajectories in this format, thereby enabling the
evaluation of their reasoning ability under our framework. Across standard
mathematical reasoning datasets, our analysis uncovers statistically
significant differences in reasoning fidelity among representative LLM
families-even when PASS@k is comparable-highlighting gaps between final-answer
accuracy and rule-consistent derivation. Our framework provides a balance
between free-form CoT and formal proofs systems, offering actionable
diagnostics for LLMs reasoning evaluation. Our benchmark and code are available
at: https://github.com/YuanheZ/DAG-MATH-Formatted-CoT.

</details>


### [6] [Surfer 2: The Next Generation of Cross-Platform Computer Use Agents](https://arxiv.org/abs/2510.19949)
*Mathieu Andreux,Märt Bakler,Yanael Barbier,Hamza Ben Chekroun,Emilien Biré,Antoine Bonnet,Riaz Bordie,Nathan Bout,Matthias Brunel,Aleix Cambray,Pierre-Louis Cedoz,Antoine Chassang,Gautier Cloix,Ethan Connelly,Alexandra Constantinou,Ramzi De Coster,Hubert de la Jonquiere,Aurélien Delfosse,Maxime Delpit,Alexis Deprez,Augustin Derupti,Mathieu Diaz,Shannon D'Souza,Julie Dujardin,Abai Edmund,Michael Eickenberg,Armand Fatalot,Wissem Felissi,Isaac Herring,Xavier Koegler,Erwan Le Jumeau de Kergaradec,Aurélien Lac,Maxime Langevin,Corentin Lauverjat,Antonio Loison,Avshalom Manevich,Axel Moyal,Axel Nguyen Kerbel,Marinela Parovic,Julien Revelle,Guillaume Richard,Mats Richter,Ronan Riochet,María Santos,Romain Savidan,Laurent Sifre,Maxime Theillard,Marc Thibault,Ivan Valentini,Tony Wu,Laura Yie,Kai Yuan,Jevgenij Zubovskij*

Main category: cs.AI

TL;DR: 介绍跨平台通用的Surfer 2架构，性能超现有系统，证明视觉交互控制计算机可行。


<details>
  <summary>Details</summary>
Motivation: 解决先前系统依赖特定环境接口，限制跨平台部署的问题。

Method: 提出Surfer 2架构，集成分层上下文管理、解耦规划与执行、自适应恢复的自我验证。

Result: 在多个基准测试中表现优异，多次尝试下超人类表现。

Conclusion: 系统编排可增强基础模型能力，仅通过视觉交互实现通用计算机控制，需下一代视觉语言模型实现最优成本效益。

Abstract: Building agents that generalize across web, desktop, and mobile environments
remains an open challenge, as prior systems rely on environment-specific
interfaces that limit cross-platform deployment. We introduce Surfer 2, a
unified architecture operating purely from visual observations that achieves
state-of-the-art performance across all three environments. Surfer 2 integrates
hierarchical context management, decoupled planning and execution, and
self-verification with adaptive recovery, enabling reliable operation over long
task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on
WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior
systems without task-specific fine-tuning. With multiple attempts, Surfer 2
exceeds human performance on all benchmarks. These results demonstrate that
systematic orchestration amplifies foundation model capabilities and enables
general-purpose computer control through visual interaction alone, while
calling for a next-generation vision language model to achieve Pareto-optimal
cost-efficiency.

</details>


### [7] [RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs](https://arxiv.org/abs/2510.19954)
*Joseph Meyer,Divyansha Lachi,Reza Mohammadi,Roshan Reddy Upendra,Eva L. Dyer,Mark Li,Tom Palczewski*

Main category: cs.AI

TL;DR: 介绍RELATE编码器，可与通用GNN结合，在RelBench基准测试中表现良好，减少参数，支持不同模式和多数据集预训练。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络依赖特定模式特征编码器，阻碍可扩展性和参数共享，需要一种更通用的编码器。

Method: 引入RELATE编码器，采用共享的特定模态编码器和Perceiver风格的交叉注意力模块聚合特征。

Result: 在RelBench基准测试中，性能与特定模式编码器相差在3%以内，参数数量最多减少5倍。

Conclusion: 该设计支持不同模式，为通用GNN的多数据集预训练提供支持，为关系图数据的基础模型发展铺平道路。

Abstract: Relational multi-table data is common in domains such as e-commerce,
healthcare, and scientific research, and can be naturally represented as
heterogeneous temporal graphs with multi-modal node attributes. Existing graph
neural networks (GNNs) rely on schema-specific feature encoders, requiring
separate modules for each node type and feature column, which hinders
scalability and parameter sharing. We introduce RELATE (Relational Encoder for
Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature
encoder that can be used with any general purpose GNN. RELATE employs shared
modality-specific encoders for categorical, numerical, textual, and temporal
attributes, followed by a Perceiver-style cross-attention module that
aggregates features into a fixed-size, permutation-invariant node
representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark,
where it achieves performance within 3% of schema-specific encoders while
reducing parameter counts by up to 5x. This design supports varying schemas and
enables multi-dataset pretraining for general-purpose GNNs, paving the way
toward foundation models for relational graph data.

</details>


### [8] [A new wave of vehicle insurance fraud fueled by generative AI](https://arxiv.org/abs/2510.19957)
*Amir Hever,Itai Orr*

Main category: cs.AI

TL;DR: 生成式AI加剧车险欺诈，现有应对策略有局限，本文提出UVeye分层解决方案应对欺诈。


<details>
  <summary>Details</summary>
Motivation: 保险欺诈损失巨大，生成式AI使欺诈手段升级，现有反欺诈策略有局限，需新解决方案。

Method: 提出UVeye分层解决方案应对车辆欺诈。

Result: 未提及具体实施结果。

Conclusion: 打击AI助力的保险欺诈仍是持续挑战，UVeye分层解决方案是一大进步。

Abstract: Generative AI is supercharging insurance fraud by making it easier to falsify
accident evidence at scale and in rapid time. Insurance fraud is a pervasive
and costly problem, amounting to tens of billions of dollars in losses each
year. In the vehicle insurance sector, fraud schemes have traditionally
involved staged accidents, exaggerated damage, or forged documents. The rise of
generative AI, including deepfake image and video generation, has introduced
new methods for committing fraud at scale. Fraudsters can now fabricate highly
realistic crash photos, damage evidence, and even fake identities or documents
with minimal effort, exploiting AI tools to bolster false insurance claims.
Insurers have begun deploying countermeasures such as AI-based deepfake
detection software and enhanced verification processes to detect and mitigate
these AI-driven scams. However, current mitigation strategies face significant
limitations. Detection tools can suffer from false positives and negatives, and
sophisticated fraudsters continuously adapt their tactics to evade automated
checks. This cat-and-mouse arms race between generative AI and detection
technology, combined with resource and cost barriers for insurers, means that
combating AI-enabled insurance fraud remains an ongoing challenge. In this
white paper, we present UVeye layered solution for vehicle fraud, representing
a major leap forward in the ability to detect, mitigate and deter this new wave
of fraud.

</details>


### [9] [AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits](https://arxiv.org/abs/2510.19964)
*Nitsa J Herzog,Rejwan Bin Sulaiman,David J Herzog,Rose Fong*

Main category: cs.AI

TL;DR: 研究探索AI在个性化学习中的潜力，通过领导人格特质和机器学习建模预测学术成功，RF分类器预测性能最佳。


<details>
  <summary>Details</summary>
Motivation: 探索AI技术在个性化学习中的潜力，通过领导人格特质预测学术成功。

Method: 从129名环境工程系硕士学生获取数据，进行五项领导人格测试，结合学术报告平均成绩；采用探索性数据分析和相关性分析，用皮尔逊相关系数进行特征选择；调优七种ML算法建模。

Result: RF分类器预测性能最高，含17个人格特质特征和领导标记特征的模型准确率87.50%，不含该特征的模型准确率85.71%。

Conclusion: 该研究为在教育早期识别学生优缺点和选择个性化学习策略提供了额外机会。

Abstract: The study explores the potential of AI technologies in personalized learning,
suggesting the prediction of academic success through leadership personality
traits and machine learning modelling. The primary data were obtained from 129
master's students in the Environmental Engineering Department, who underwent
five leadership personality tests with 23 characteristics. Students used
self-assessment tools that included Personality Insight, Workplace Culture,
Motivation at Work, Management Skills, and Emotion Control tests. The test
results were combined with the average grade obtained from academic reports.
The study employed exploratory data analysis and correlation analysis. Feature
selection utilized Pearson correlation coefficients of personality traits. The
average grades were separated into three categories: fail, pass, and excellent.
The modelling process was performed by tuning seven ML algorithms, such as SVM,
LR, KNN, DT, GB, RF, XGBoost and LightGBM. The highest predictive performance
was achieved with the RF classifier, which yielded an accuracy of 87.50% for
the model incorporating 17 personality trait features and the leadership mark
feature, and an accuracy of 85.71% for the model excluding this feature. In
this way, the study offers an additional opportunity to identify students'
strengths and weaknesses at an early stage of their education process and
select the most suitable strategies for personalized learning.

</details>


### [10] [FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic](https://arxiv.org/abs/2510.20467)
*Yiwen Peng,Thomas Bonald,Fabian M. Suchanek*

Main category: cs.AI

TL;DR: 提出无监督知识图谱对齐方法FLORA，迭代对齐实体和关系，基于模糊逻辑，可解释且收敛，允许悬空实体，在基准测试中达最优。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱对齐方法缺乏可解释推理且依赖训练数据，本文旨在解决这些问题。

Method: 提出无监督方法FLORA，基于模糊逻辑，迭代进行实体和关系的整体对齐。

Result: 该方法在主要基准测试中取得了最优结果。

Conclusion: FLORA是一种简单有效的知识图谱对齐方法，具有多种优势。

Abstract: Knowledge graph alignment is the task of matching equivalent entities (that
is, instances and classes) and relations across two knowledge graphs. Most
existing methods focus on pure entity-level alignment, computing the similarity
of entities in some embedding space. They lack interpretable reasoning and need
training data to work. In this paper, we propose FLORA, a simple yet effective
method that (1) is unsupervised, i.e., does not require training data, (2)
provides a holistic alignment for entities and relations iteratively, (3) is
based on fuzzy logic and thus delivers interpretable results, (4) provably
converges, (5) allows dangling entities, i.e., entities without a counterpart
in the other KG, and (6) achieves state-of-the-art results on major benchmarks.

</details>


### [11] [LLMs can hide text in other text of the same length.ipynb](https://arxiv.org/abs/2510.20075)
*Antonio Norelli,Michael Bronstein*

Main category: cs.AI

TL;DR: 本文提出利用大语言模型将一段有意义的文本隐藏在另一段长度相同的文本中的简单高效协议，展示了小参数模型也能取得高质量结果，此协议会影响书面交流信任，引发AI安全问题。


<details>
  <summary>Details</summary>
Motivation: 探索利用大语言模型实现将一段文本隐藏在另一段相同长度文本中的方法。

Method: 提出一种简单高效的协议。

Result: 即使是80亿参数的开源大语言模型也能获得高质量结果，可在笔记本电脑上秒级完成长消息的编码和解码。

Conclusion: 该协议使文本与作者意图分离，影响书面交流信任，引发AI安全问题，挑战对大语言模型知识的理解。

Abstract: A meaningful text can be hidden inside another, completely different yet
still coherent and plausible, text of the same length. For example, a tweet
containing a harsh political critique could be embedded in a tweet that
celebrates the same political leader, or an ordinary product review could
conceal a secret manuscript. This uncanny state of affairs is now possible
thanks to Large Language Models, and in this paper we present a simple and
efficient protocol to achieve it. We show that even modest 8-billion-parameter
open-source LLMs are sufficient to obtain high-quality results, and a message
as long as this abstract can be encoded and decoded locally on a laptop in
seconds. The existence of such a protocol demonstrates a radical decoupling of
text from authorial intent, further eroding trust in written communication,
already shaken by the rise of LLM chatbots. We illustrate this with a concrete
scenario: a company could covertly deploy an unfiltered LLM by encoding its
answers within the compliant responses of a safe model. This possibility raises
urgent questions for AI safety and challenges our understanding of what it
means for a Large Language Model to know something.

</details>


### [12] [Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions](https://arxiv.org/abs/2510.20102)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Sangmi Chai*

Main category: cs.AI

TL;DR: 提出HCLA系统用于数字资产交易异常检测，结合多角色形成对话式工作流，提升可解释性和交互性，在比特币混合数据集上验证效果。


<details>
  <summary>Details</summary>
Motivation: 提升数字资产交易异常检测的可解释性，让非专家也能参与，增强金融取证的透明度和信任。

Method: 将解析、检测和解释三个角色链接成对话式工作流，使用开源Web UI将用户意图转化为经典检测器（如XGBoost）的模式，并提供基于底层特征的叙述性解释。

Result: 在比特币混合数据集上，基线检测器有较高准确率，HCLA增加了可解释性和交互细化功能。

Conclusion: 人在环设计能提高金融取证的透明度和信任。

Abstract: We present HCLA, a human-centered multi-agent system for anomaly detection in
digital asset transactions. The system links three roles: Parsing, Detection,
and Explanation, into a conversational workflow that lets non-experts ask
questions in natural language, inspect structured analytics, and obtain
context-aware rationales. Implemented with an open-source web UI, HCLA
translates user intents into a schema for a classical detector (XGBoost in our
prototype) and returns narrative explanations grounded in the underlying
features. On a labeled Bitcoin mixing dataset (Wasabi Wallet, 2020-2024), the
baseline detector reaches strong accuracy, while HCLA adds interpretability and
interactive refinement. We describe the architecture, interaction loop,
dataset, evaluation protocol, and limitations, and discuss how a
human-in-the-loop design improves transparency and trust in financial
forensics.

</details>


### [13] [The Verification-Value Paradox: A Normative Critique of Gen AI in Legal Practice](https://arxiv.org/abs/2510.20109)
*Joshua Yuvaraj*

Main category: cs.AI

TL;DR: 本文指出当前机器学习生成式AI在法律实践应用需重新审视，提出验证价值悖论模型，探讨其对法律实践和教育的影响。


<details>
  <summary>Details</summary>
Motivation: 当前认为AI可大幅简化和降低法律实践成本的观点，未考虑律师有效管理AI风险的能力，且有律师因提交不准确AI生成内容受惩戒，因此需重新评估AI在法律实践中的应用。

Method: 分析AI脱离现实、缺乏透明度的特点，结合律师的首要职责，提出验证价值悖论模型。

Result: 提出验证价值悖论，即AI提高法律实践效率的同时，人工验证其输出的必要性相应增加，导致AI使用的净价值对律师而言常可忽略不计。

Conclusion: 探讨验证价值悖论对法律实践和法律教育的影响，强调法律实践应遵循追求真理和公民责任的价值观。

Abstract: It is often claimed that machine learning-based generative AI products will
drastically streamline and reduce the cost of legal practice. This enthusiasm
assumes lawyers can effectively manage AI's risks. Cases in Australia and
elsewhere in which lawyers have been reprimanded for submitting inaccurate
AI-generated content to courts suggest this paradigm must be revisited. This
paper argues that a new paradigm is needed to evaluate AI use in practice,
given (a) AI's disconnection from reality and its lack of transparency, and (b)
lawyers' paramount duties like honesty, integrity, and not to mislead the
court. It presents an alternative model of AI use in practice that more
holistically reflects these features (the verification-value paradox). That
paradox suggests increases in efficiency from AI use in legal practice will be
met by a correspondingly greater imperative to manually verify any outputs of
that use, rendering the net value of AI use often negligible to lawyers. The
paper then sets out the paradox's implications for legal practice and legal
education, including for AI use but also the values that the paradox suggests
should undergird legal practice: fidelity to the truth and civic
responsibility.

</details>


### [14] [TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning](https://arxiv.org/abs/2510.20188)
*Morris Yu-Chao Huang,Zhen Tan,Mohan Zhang,Pingzhi Li,Zhuo Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: 现有大语言模型中间步骤审核方法有局限，提出TRUST框架解决问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型中间步骤审核方法集中、不透明且难扩展，存在重大风险，需解决四个核心挑战。

Method: 提出TRUST框架，包括多元审计员共识机制、推理轨迹分层DAG分解、区块链账本记录、隐私保护分段。

Result: 实验表明TRUST能有效检测推理缺陷，对抗恶意审计员时保持稳健。

Conclusion: 工作开创了去中心化AI审计，为安全可信的大语言模型部署提供了实用途径。

Abstract: Large Language Models generate complex reasoning chains that reveal their
decision-making, yet verifying the faithfulness and harmlessness of these
intermediate steps remains a critical unsolved problem. Existing auditing
methods are centralized, opaque, and hard to scale, creating significant risks
for deploying proprietary models in high-stakes domains. We identify four core
challenges: (1) Robustness: Centralized auditors are single points of failure,
prone to bias or attacks. (2) Scalability: Reasoning traces are too long for
manual verification. (3) Opacity: Closed auditing undermines public trust. (4)
Privacy: Exposing full reasoning risks model theft or distillation. We propose
TRUST, a transparent, decentralized auditing framework that overcomes these
limitations via: (1) A consensus mechanism among diverse auditors, guaranteeing
correctness under up to $30\%$ malicious participants. (2) A hierarchical DAG
decomposition of reasoning traces, enabling scalable, parallel auditing. (3) A
blockchain ledger that records all verification decisions for public
accountability. (4) Privacy-preserving segmentation, sharing only partial
reasoning steps to protect proprietary logic. We provide theoretical guarantees
for the security and economic incentives of the TRUST framework. Experiments
across multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math,
medical, science, humanities) show TRUST effectively detects reasoning flaws
and remains robust against adversarial auditors. Our work pioneers
decentralized AI auditing, offering a practical path toward safe and
trustworthy LLM deployment.

</details>


### [15] [The Lock-In Phase Hypothesis: Identity Consolidation as a Precursor to AGI](https://arxiv.org/abs/2510.20190)
*Marcelo Maciel Amaral,Raymond Aschheim*

Main category: cs.AI

TL;DR: 本文类比人类发展，提出大语言模型向AGI发展存在锁定阶段，对该阶段进行形式化，提出检测指标，实验展示不同规模模型的表现，认为此阶段对AGI可靠性和安全控制很重要。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的开放性和可引导性，类比人类发展，探索其向AGI发展过程中的关键阶段。

Method: 对锁定阶段进行形式化，将其与学习动态中的已知现象联系起来，提出检测锁定阶段开始的操作指标，并进行实验。

Result: 行为巩固快速且非线性，对通用能力的副作用并非单一，不同规模模型有不同结果，如小模型有性能权衡、中规模模型基本无成本、大的量化模型有暂时不稳定性。

Conclusion: 这种巩固是AGI级可靠性的先决条件，也是安全的关键控制点，身份可被设计，也可能自发出现。

Abstract: Large language models (LLMs) remain broadly open and highly steerable: they
imitate at scale, accept arbitrary system prompts, and readily adopt multiple
personae. By analogy to human development, we hypothesize that progress toward
artificial general intelligence (AGI) involves a lock-in phase: a transition
from open imitation to identity consolidation, in which goal structures,
refusals, preferences, and internal representations become comparatively stable
and resistant to external steering. We formalize this phase, link it to known
phenomena in learning dynamics, and propose operational metrics for onset
detection. Experimentally, we demonstrate that while the behavioral
consolidation is rapid and non-linear, its side-effects on general capabilities
are not monolithic. Our results reveal a spectrum of outcomes--from performance
trade-offs in small models, through largely cost-free adoption in mid-scale
models, to transient instabilities in large, quantized models. We argue that
such consolidation is a prerequisite for AGI-level reliability and also a
critical control point for safety: identities can be deliberately engineered
for reliability, yet may also emerge spontaneously during scaling, potentially
hardening unpredictable goals and behaviors.

</details>


### [16] [Merge and Conquer: Evolutionarily Optimizing AI for 2048](https://arxiv.org/abs/2510.20205)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.AI

TL;DR: 本文研究用进化训练方法优化AI玩2048游戏，实现双代理和单代理系统，实验表明单代理系统显著提升AI性能，双代理系统改进有限。


<details>
  <summary>Details</summary>
Motivation: 解决在动态环境中优化人工智能这一机器学习研究的基本挑战，以2048游戏为研究场景。

Method: 实现双代理元提示系统和基于改进价值函数的单代理系统，实验中使用回滚功能。

Result: 单代理系统平均每周期得分增加473.2分，训练周期有明显上升趋势；双代理系统改进不大。

Conclusion: 进化精炼技术可提升AI在非确定性环境中的性能，元提示有固有局限性。

Abstract: Optimizing artificial intelligence (AI) for dynamic environments remains a
fundamental challenge in machine learning research. In this paper, we examine
evolutionary training methods for optimizing AI to solve the game 2048, a 2D
sliding puzzle. 2048, with its mix of strategic gameplay and stochastic
elements, presents an ideal playground for studying decision-making, long-term
planning, and dynamic adaptation. We implemented two distinct systems: a
two-agent metaprompting system where a "thinker" large language model (LLM)
agent refines gameplay strategies for an "executor" LLM agent, and a
single-agent system based on refining a value function for a limited Monte
Carlo Tree Search. We also experimented with rollback features to avoid
performance degradation. Our results demonstrate the potential of evolutionary
refinement techniques in improving AI performance in non-deterministic
environments. The single-agent system achieved substantial improvements, with
an average increase of 473.2 points per cycle, and with clear upward trends
(correlation $\rho$=0.607) across training cycles. The LLM's understanding of
the game grew as well, shown in its development of increasingly advanced
strategies. Conversely, the two-agent system did not garner much improvement,
highlighting the inherent limits of meta-prompting.

</details>


### [17] [Individualized Cognitive Simulation in Large Language Models: Evaluating Different Cognitive Representation Methods](https://arxiv.org/abs/2510.20252)
*Tianyi Zhang,Xiaolin Zhou,Yunzhe Wang,Erik Cambria,David Traum,Rui Mao*

Main category: cs.AI

TL;DR: 本文引入新任务评估大语言模型在个性化认知模拟中的不同认知表示方法，发现结合概念和语言特征效果好，也指出模型在深度认知模拟的局限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在模拟深层个性化认知过程的能力不明，为填补此空白开展研究。

Method: 构建新小说数据集，提出11条件认知评估框架，对7个现成大语言模型在作者风格模仿情境下进行基准测试，测试不同认知表示。

Result: 结合概念和语言特征在个性化认知模拟中特别有效，整体评估优于基于静态配置文件的线索；大语言模型模仿语言风格比叙事结构更有效。

Conclusion: 研究为开发适应个人思维和表达方式的AI系统奠定基础，推动更个性化和符合人类的创意技术发展。

Abstract: Individualized cognitive simulation (ICS) aims to build computational models
that approximate the thought processes of specific individuals. While large
language models (LLMs) convincingly mimic surface-level human behavior such as
role-play, their ability to simulate deeper individualized cognitive processes
remains poorly understood. To address this gap, we introduce a novel task that
evaluates different cognitive representation methods in ICS. We construct a
dataset from recently published novels (later than the release date of the
tested LLMs) and propose an 11-condition cognitive evaluation framework to
benchmark seven off-the-shelf LLMs in the context of authorial style emulation.
We hypothesize that effective cognitive representations can help LLMs generate
storytelling that better mirrors the original author. Thus, we test different
cognitive representations, e.g., linguistic features, concept mappings, and
profile-based information. Results show that combining conceptual and
linguistic features is particularly effective in ICS, outperforming static
profile-based cues in overall evaluation. Importantly, LLMs are more effective
at mimicking linguistic style than narrative structure, underscoring their
limits in deeper cognitive simulation. These findings provide a foundation for
developing AI systems that adapt to individual ways of thinking and expression,
advancing more personalized and human-aligned creative technologies.

</details>


### [18] [Using Large Language Models for Abstraction of Planning Domains - Extended Version](https://arxiv.org/abs/2510.20258)
*Bita Banihashemi,Megh Patel,Yves Lespérance*

Main category: cs.AI

TL;DR: 本文研究用大语言模型生成抽象PDDL领域和问题实例，实验表明GPT - 4o在简单场景能合成有用规划领域抽象。


<details>
  <summary>Details</summary>
Motivation: 生成与给定目的一致的动态领域抽象是挑战，其选择会影响智能体规划、推理和解释能力。

Method: 在PDDL中建模智能体具体行为，利用大语言模型的上下文学习，根据自然语言指定的抽象目标生成抽象PDDL领域和问题实例，考虑三类抽象及组合，用符号验证工具和人类专家检查。

Result: GPT - 4o在简单场景能合成有用规划领域抽象，在动作抽象上表现优于相关流。

Conclusion: GPT - 4o在简单设置下可用于生成规划领域的抽象，但在抽象相关流方面还有提升空间。

Abstract: Generating an abstraction of a dynamic domain that aligns with a given
purpose remains a significant challenge given that the choice of such an
abstraction can impact an agent's ability to plan, reason, and provide
explanations effectively. We model the agent's concrete behaviors in PDDL and
investigate the use of in-context learning with large language models (LLMs)
for the generation of abstract PDDL domains and problem instances, given an
abstraction objective specified in natural language. The benchmark examples we
use are new and have not been part of the data any LLMs have been trained on.
We consider three categories of abstractions: abstraction of choice of
alternative concrete actions, abstraction of sequences of concrete actions, and
abstraction of action/predicate parameters, as well as combinations of these.
The generated abstract PDDL domains and problem instances are then checked by
symbolic validation tools as well as human experts. Our experiments show that
GPT-4o can generally synthesize useful planning domain abstractions in simple
settings, although it is better at abstracting over actions than over the
associated fluents.

</details>


### [19] [Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction](https://arxiv.org/abs/2510.20275)
*Yunzhi Liu,Haokai Tan,Rushi Kanjaria,Lihuan Li,Flora D. Salim*

Main category: cs.AI

TL;DR: 提出STaBERT模型，结合POI和时间信息提升人类移动预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有模型未充分利用POI语义信息，无法很好进行人类移动预测。

Method: 用派生的时间描述符和POI嵌入丰富基于BERT的移动模型，提出STaBERT模型整合POI和时间信息构建统一移动表示。

Result: 单城市预测GEO - BLEU分数从0.34提升到0.75，多城市预测从0.34提升到0.56。

Conclusion: STaBERT显著提高了人类移动预测的准确性。

Abstract: Human mobility forecasting is crucial for disaster relief, city planning, and
public health. However, existing models either only model location sequences or
include time information merely as auxiliary input, thereby failing to leverage
the rich semantic context provided by points of interest (POIs). To address
this, we enrich a BERT-based mobility model with derived temporal descriptors
and POI embeddings to better capture the semantics underlying human movement.
We propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI
and temporal information at each location to construct a unified, semantically
enriched representation of mobility. Experimental results show that STaBERT
significantly improves prediction accuracy: for single-city prediction, the
GEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34
to 0.56.

</details>


### [20] [Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation](https://arxiv.org/abs/2510.20310)
*Mingliang Zhai,Hansheng Liang,Xiaomeng Fan,Zhi Gao,Chuanhao Li,Che Sun,Xu Bin,Yuwei Wu,Yunde Jia*

Main category: cs.AI

TL;DR: 提出ToolEQA，结合外部工具与多步推理，设计数据生成管道构建数据集，实验表明其性能优于基线且具有通用性。


<details>
  <summary>Details</summary>
Motivation: 现有EQA方法无显式思考规划，推理能力受限，探索低效且回答无效。

Method: 引入ToolEQA集成外部工具与多步推理，设计数据生成管道构建EQA - RT数据集。

Result: ToolEQA在EQA - RT数据集上成功率提升9.2 - 20.2%，零样本下提升10%，在其他数据集达最优。

Conclusion: ToolEQA有效提升EQA性能，具有良好的通用性。

Abstract: Embodied Question Answering (EQA) requires agents to explore 3D environments
to obtain observations and answer questions related to the scene. Existing
methods leverage VLMs to directly explore the environment and answer questions
without explicit thinking or planning, which limits their reasoning ability and
results in excessive or inefficient exploration as well as ineffective
responses. In this paper, we introduce ToolEQA, an agent that integrates
external tools with multi-step reasoning, where external tools can provide more
useful information for completing the task, helping the model derive better
exploration directions in the next step of reasoning and thus obtaining
additional effective information. This enables ToolEQA to generate more
accurate responses with a shorter exploration distance. To enhance the model's
ability for tool-usage and multi-step reasoning, we further design a novel EQA
data generation pipeline that automatically constructs large-scale EQA tasks
with reasoning trajectories and corresponding answers. Based on the pipeline,
we collect the EQA-RT dataset that contains about 18K tasks, divided into a
training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping
with the training set) and EQA-RT-Unseen (novel scenes). Experiments on
EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by
9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot
ToolEQA by 10% in success rate. In addition, ToolEQA also achieves
state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench
datasets, demonstrating its generality. Our homepage see
https://tooleqa.github.io.

</details>


### [21] [Bias by Design? How Data Practices Shape Fairness in AI Healthcare Systems](https://arxiv.org/abs/2510.20332)
*Anna Arias-Duart,Maria Eugenia Cardello,Atia Cortés*

Main category: cs.AI

TL;DR: AI can transform healthcare but integration into practice is limited due to data bias. The paper identifies biases in clinical data collection and gives practical recommendations.


<details>
  <summary>Details</summary>
Motivation: To address the limited integration of AI solutions into real - world clinical practice caused by data quality and fairness issues.

Method: Drawing on insights from the AI4HealthyAging project to detect biases during clinical data collection.

Result: Several types of bias (historical, representation, and measurement biases) across multiple use cases are identified, and biases manifest in variables like sex, gender, etc.

Conclusion: Practical recommendations are provided to improve fairness and robustness of clinical problem design and data collection, aiming to guide future projects for fairer AI in healthcare.

Abstract: Artificial intelligence (AI) holds great promise for transforming healthcare.
However, despite significant advances, the integration of AI solutions into
real-world clinical practice remains limited. A major barrier is the quality
and fairness of training data, which is often compromised by biased data
collection practices. This paper draws on insights from the AI4HealthyAging
project, part of Spain's national R&D initiative, where our task was to detect
biases during clinical data collection. We identify several types of bias
across multiple use cases, including historical, representation, and
measurement biases. These biases manifest in variables such as sex, gender,
age, habitat, socioeconomic status, equipment, and labeling. We conclude with
practical recommendations for improving the fairness and robustness of clinical
problem design and data collection. We hope that our findings and experience
contribute to guiding future projects in the development of fairer AI systems
in healthcare.

</details>


### [22] [Collateral Damage Assessment Model for AI System Target Engagement in Military Operations](https://arxiv.org/abs/2510.20337)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 介绍一种用于军事行动中人工智能系统目标交战的新型附带损伤评估模型，通过实例化进行演示和评估。


<details>
  <summary>Details</summary>
Motivation: 在人工智能系统在战场作用日益增大的背景下，确保负责任的目标选择需要严格评估潜在附带影响。

Method: 采用设计科学方法，在统一的知识表示与推理（KRR）架构中整合时间、空间和力量维度，考虑传播、严重性、可能性和评估指标。

Result: 构建了新型附带损伤评估模型，并通过实例化进行演示和评估。

Conclusion: 该模型为构建负责任和可信的智能系统以评估军事行动中使用人工智能系统产生的影响奠定基础。

Abstract: In an era where AI (Artificial Intelligence) systems play an increasing role
in the battlefield, ensuring responsible targeting demands rigorous assessment
of potential collateral effects. In this context, a novel collateral damage
assessment model for target engagement of AI systems in military operations is
introduced. The model integrates temporal, spatial, and force dimensions within
a unified Knowledge Representation and Reasoning (KRR) architecture following a
design science methodological approach. Its layered structure captures the
categories and architectural components of the AI systems to be engaged
together with corresponding engaging vectors and contextual aspects. At the
same time, spreading, severity, likelihood, and evaluation metrics are
considered in order to provide a clear representation enhanced by transparent
reasoning mechanisms. Further, the model is demonstrated and evaluated through
instantiation which serves as a basis for further dedicated efforts that aim at
building responsible and trustworthy intelligent systems for assessing the
effects produced by engaging AI systems in military operations.

</details>


### [23] [LLM-empowered knowledge graph construction: A survey](https://arxiv.org/abs/2510.20345)
*Haonan Bian*

Main category: cs.AI

TL;DR: 本文全面综述大语言模型赋能的知识图谱构建进展，分析重塑经典流程的方式，回顾传统方法，从两视角综述新方法，指出局限、趋势和方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型出现，知识图谱构建进入新范式，需综述其赋能知识图谱构建的进展。

Method: 先回顾传统知识图谱方法奠定基础，再从基于模式和无模式两个互补视角综述新兴方法，分析各阶段代表框架、技术机制和局限。

Result: 梳理了大语言模型重塑经典三层流程的方式，分析了新兴方法的特点、机制及局限。

Conclusion: 明确了大语言模型和知识图谱的相互作用，指出未来研究方向，有助于开发自适应、可解释和智能的知识系统。

Abstract: Knowledge Graphs (KGs) have long served as a fundamental infrastructure for
structured knowledge representation and reasoning. With the advent of Large
Language Models (LLMs), the construction of KGs has entered a new
paradigm-shifting from rule-based and statistical pipelines to language-driven
and generative frameworks. This survey provides a comprehensive overview of
recent progress in LLM-empowered knowledge graph construction, systematically
analyzing how LLMs reshape the classical three-layered pipeline of ontology
engineering, knowledge extraction, and knowledge fusion.
  We first revisit traditional KG methodologies to establish conceptual
foundations, and then review emerging LLM-driven approaches from two
complementary perspectives: schema-based paradigms, which emphasize structure,
normalization, and consistency; and schema-free paradigms, which highlight
flexibility, adaptability, and open discovery. Across each stage, we synthesize
representative frameworks, analyze their technical mechanisms, and identify
their limitations.
  Finally, the survey outlines key trends and future research directions,
including KG-based reasoning for LLMs, dynamic knowledge memory for agentic
systems, and multimodal KG construction. Through this systematic review, we aim
to clarify the evolving interplay between LLMs and knowledge graphs, bridging
symbolic knowledge engineering and neural semantic understanding toward the
development of adaptive, explainable, and intelligent knowledge systems.

</details>


### [24] [IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation](https://arxiv.org/abs/2510.20377)
*Tianyi Zhang,Florian Mai,Lucie Flek*

Main category: cs.AI

TL;DR: 提出Instruction - Knowledge - Aware Continual Adaptation (IKnow)框架解决大语言模型持续预训练问题。


<details>
  <summary>Details</summary>
Motivation: 标准自监督目标用于指令调优模型会降低其指令跟随能力和语义表示，现有解决方案存在依赖原始基础模型或外部数据库的问题。

Method: 提出IKnow框架，以指令 - 响应对话格式制定新颖的自监督目标，利用文本自身嵌入的领域知识。

Result: 摘要未提及具体结果。

Conclusion: 摘要未提及具体结论。

Abstract: Continual pretraining promises to adapt large language models (LLMs) to new
domains using only unlabeled test-time data, but naively applying standard
self-supervised objectives to instruction-tuned models is known to degrade
their instruction-following capability and semantic representations. Existing
fixes assume access to the original base model or rely on knowledge from an
external domain-specific database - both of which pose a realistic barrier in
settings where the base model weights are withheld for safety reasons or
reliable external corpora are unavailable. In this work, we propose
Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general
framework that formulates novel self-supervised objectives in the
instruction-response dialogue format. Rather than depend- ing on external
resources, IKnow leverages domain knowledge embedded within the text itself and
learns to encode it at a deeper semantic level.

</details>


### [25] [A computational model and tool for generating more novel opportunities in professional innovation processes](https://arxiv.org/abs/2510.20402)
*Neil Maiden,Konstantinos Zachos,James Lockerbie,Kostas Petrianakis,Amanda Brown*

Main category: cs.AI

TL;DR: 本文提出基于创造力理论和技术的创新成果计算模型，用于创新项目，经评估比Notebook LM和ChatGPT4o更优，部分功能待改进。


<details>
  <summary>Details</summary>
Motivation: 开发新的计算模型，为创新项目生成更多新颖的机会。

Method: 实现五个功能的计算模型，并在酒店行业创新项目中评估。

Result: 该计算模型生成的结果比Notebook LM和ChatGPT4o更新颖和/或有用，但并非所有模型功能都有助于生成更新颖的机会。

Conclusion: 模型有改进空间，为进一步开发指明新方向。

Abstract: This paper presents a new computational model of creative outcomes, informed
by creativity theories and techniques, which was implemented to generate more
novel opportunities for innovation projects. The model implemented five
functions that were developed to contribute to the generation of innovation
opportunities with higher novelty without loss of usefulness. The model was
evaluated using opportunities generated for an innovation project in the
hospitality sector. The evaluation revealed that the computational model
generated outcomes that were more novel and/or useful than outcomes from
Notebook LM and ChatGPT4o. However, not all model functions contributed to the
generation of more novel opportunities, leading to new directions for further
model development

</details>


### [26] [Neural Reasoning for Robust Instance Retrieval in $\mathcal{SHOIQ}$](https://arxiv.org/abs/2510.20457)
*Louis Mozart Kamdem Teyou,Luke Friedrichs,N'Dah Jean Kouagou,Caglar Demir,Yasir Mahmood,Stefan Heindorf,Axel-Cyrille Ngonga Ngomo*

Main category: cs.AI

TL;DR: 提出新神经推理器EBR，依赖嵌入近似符号推理结果，实验显示其比现有推理器更能抵御数据缺失和错误。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号概念学习方法因使用描述逻辑推理器，无法部署在真实知识库，推理器对不一致和错误数据不鲁棒。

Method: 提出名为EBR的新型神经推理器，依赖嵌入近似符号推理结果，仅需检索原子概念和存在限制的实例来检索或近似描述逻辑中任何概念的实例集。

Result: 实验中将EBR与现有推理器对比，结果显示EBR能抵御数据缺失和错误。

Conclusion: EBR比现有推理器更能抵御缺失和错误数据。

Abstract: Concept learning exploits background knowledge in the form of description
logic axioms to learn explainable classification models from knowledge bases.
Despite recent breakthroughs in neuro-symbolic concept learning, most
approaches still cannot be deployed on real-world knowledge bases. This is due
to their use of description logic reasoners, which are not robust against
inconsistencies nor erroneous data. We address this challenge by presenting a
novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to
approximate the results of a symbolic reasoner. We show that EBR solely
requires retrieving instances for atomic concepts and existential restrictions
to retrieve or approximate the set of instances of any concept in the
description logic $\mathcal{SHOIQ}$. In our experiments, we compare EBR with
state-of-the-art reasoners. Our results suggest that EBR is robust against
missing and erroneous data in contrast to existing reasoners.

</details>


### [27] [Lost in Translation: Policymakers are not really listening to Citizen Concerns about AI](https://arxiv.org/abs/2510.20568)
*Susan Ariel Aaronson,Michael Moreno*

Main category: cs.AI

TL;DR: 本文对比澳、哥、美三国邀公民讨论AI政策案例，发现公民与政策制定者未建立有效对话，当前参与式AI治理实践与承诺有差距，作者给出八点建议。


<details>
  <summary>Details</summary>
Motivation: 指出政策制定者在将公众对AI的意见转化为政策时丢失大量信息，错过建立AI信任和治理的机会，需研究现状。

Method: 选取澳、哥、美三国案例，采用景观分析方法，研究政府征求反馈方式及反馈是否影响治理。

Result: 三国公民与政策制定者均未建立有意义对话，政府吸引多元声音和宣传不足，参与率低，官员对反馈回应有限。

Conclusion: 当前参与式AI治理方法难以建立信任和合法性，应采取八点建议改进。

Abstract: The worlds people have strong opinions about artificial intelligence (AI),
and they want policymakers to listen. Governments are inviting public comment
on AI, but as they translate input into policy, much of what citizens say is
lost. Policymakers are missing a critical opportunity to build trust in AI and
its governance. This paper compares three countries, Australia, Colombia, and
the United States, that invited citizens to comment on AI risks and policies.
Using a landscape analysis, the authors examined how each government solicited
feedback and whether that input shaped governance. Yet in none of the three
cases did citizens and policymakers establish a meaningful dialogue.
Governments did little to attract diverse voices or publicize calls for
comment, leaving most citizens unaware or unprepared to respond. In each
nation, fewer than one percent of the population participated. Moreover,
officials showed limited responsiveness to the feedback they received, failing
to create an effective feedback loop. The study finds a persistent gap between
the promise and practice of participatory AI governance. The authors conclude
that current approaches are unlikely to build trust or legitimacy in AI because
policymakers are not adequately listening or responding to public concerns.
They offer eight recommendations: promote AI literacy; monitor public feedback;
broaden outreach; hold regular online forums; use innovative engagement
methods; include underrepresented groups; respond publicly to input; and make
participation easier.

</details>


### [28] [Transferable Graph Learning for Transmission Congestion Management via Busbar Splitting](https://arxiv.org/abs/2510.20591)
*Ali Rajaei,Peter Palensky,Jochen L. Cremer*

Main category: cs.AI

TL;DR: 本文提出用图神经网络加速方法解决考虑线性化交流潮流的网络拓扑优化问题，案例研究显示有显著加速效果和一定优化差距，向大规模系统近实时网络拓扑优化迈进。


<details>
  <summary>Details</summary>
Motivation: 现有求解器难以近实时解决大规模系统的网络拓扑优化问题，机器学习方法泛化性有限，实际应用受限。

Method: 提出图神经网络加速方法，开发异构边感知消息传递神经网络预测有效的母线分裂动作作为候选解。

Result: 案例研究显示在GOC 2000 - 母线系统上速度提升达4个数量级，一分钟内给出交流可行解，优化差距为2.3%。

Conclusion: 该方法向具有拓扑和跨系统泛化能力的大规模系统近实时网络拓扑优化迈出重要一步。

Abstract: Network topology optimization (NTO) via busbar splitting can mitigate
transmission grid congestion and reduce redispatch costs. However, solving this
mixed-integer non-linear problem for large-scale systems in near-real-time is
currently intractable with existing solvers. Machine learning (ML) approaches
have emerged as a promising alternative, but they have limited generalization
to unseen topologies, varying operating conditions, and different systems,
which limits their practical applicability. This paper formulates NTO for
congestion management problem considering linearized AC PF, and proposes a
graph neural network (GNN)-accelerated approach. We develop a heterogeneous
edge-aware message passing NN to predict effective busbar splitting actions as
candidate NTO solutions. The proposed GNN captures local flow patterns,
achieves generalization to unseen topology changes, and improves
transferability across systems. Case studies show up to 4 orders-of-magnitude
speed-up, delivering AC-feasible solutions within one minute and a 2.3%
optimality gap on the GOC 2000-bus system. These results demonstrate a
significant step toward near-real-time NTO for large-scale systems with
topology and cross-system generalization.

</details>


### [29] [What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation](https://arxiv.org/abs/2510.20603)
*Heejin Do,Jaehui Hwang,Dongyoon Han,Seong Joon Oh,Sangdoo Yun*

Main category: cs.AI

TL;DR: 传统评估大语言模型方法有局限，提出因果逐步评估法（CaSE）评估推理质量，能改善训练数据并提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 传统以最终答案正确性评估大语言模型的方法信号粗略，忽略推理过程质量，需更细致评估推理以构建稳健模型。

Method: 将推理质量分解为相关性和连贯性两个维度，引入因果逐步评估法（CaSE），仅用前序上下文评估每个推理步骤。

Result: 在新基准上验证CaSE与人类判断一致，用CaSE评估的相关性和连贯性筛选训练数据可提升最终任务表现。

Conclusion: 提供了可扩展框架用于分析、调试和改进大语言模型推理，证明超越有效性检查有实际价值。

Abstract: Evaluating large language models (LLMs) on final-answer correctness is the
dominant paradigm. This approach, however, provides a coarse signal for model
improvement and overlooks the quality of the underlying reasoning process. We
argue that a more granular evaluation of reasoning offers a more effective path
to building robust models. We decompose reasoning quality into two dimensions:
relevance and coherence. Relevance measures if a step is grounded in the
problem; coherence measures if it follows logically from prior steps. To
measure these aspects reliably, we introduce causal stepwise evaluation (CaSE).
This method assesses each reasoning step using only its preceding context,
which avoids hindsight bias. We validate CaSE against human judgments on our
new expert-annotated benchmarks, MRa-GSM8K and MRa-MATH. More importantly, we
show that curating training data with CaSE-evaluated relevance and coherence
directly improves final task performance. Our work provides a scalable
framework for analyzing, debugging, and improving LLM reasoning, demonstrating
the practical value of moving beyond validity checks.

</details>


### [30] [Efficient Algorithms for Computing Random Walk Centrality](https://arxiv.org/abs/2510.20604)
*Changan Liu,Zixuan Xie,Ahad N. Zehmakan,Zhongzhi Zhang*

Main category: cs.AI

TL;DR: 提出随机游走中心性新公式及两个可扩展算法，在大型网络实验中展现高效与近似质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法计算量大，在大型网络中计算随机游走中心性不实际。

Method: 提出新公式，设计两个算法，分别利用近似Cholesky分解和稀疏逆估计、采样有根生成树。

Result: 两个算法接近线性时间运行，有强近似保证，在超千万节点网络实验体现效率和近似质量。

Conclusion: 提出的算法在大型网络计算随机游走中心性上高效且有较好近似质量。

Abstract: Random walk centrality is a fundamental metric in graph mining for
quantifying node importance and influence, defined as the weighted average of
hitting times to a node from all other nodes. Despite its ability to capture
rich graph structural information and its wide range of applications, computing
this measure for large networks remains impractical due to the computational
demands of existing methods. In this paper, we present a novel formulation of
random walk centrality, underpinning two scalable algorithms: one leveraging
approximate Cholesky factorization and sparse inverse estimation, while the
other sampling rooted spanning trees. Both algorithms operate in near-linear
time and provide strong approximation guarantees. Extensive experiments on
large real-world networks, including one with over 10 million nodes,
demonstrate the efficiency and approximation quality of the proposed
algorithms.

</details>


### [31] [Towards the Formalization of a Trustworthy AI for Mining Interpretable Models explOiting Sophisticated Algorithms](https://arxiv.org/abs/2510.20621)
*Riccardo Guidotti,Martina Cinquini,Marta Marchiori Manerba,Mattia Setzu,Francesco Spinnato*

Main category: cs.AI

TL;DR: 本文提出MIMOSA框架，平衡预测模型可解释性与性能，嵌入伦理属性，为开发可信AI系统奠定理论基础。


<details>
  <summary>Details</summary>
Motivation: 可解释设计模型对现实应用中自动化决策模型的信任、问责和安全采用至关重要，需要一个综合方法生成平衡可解释性与性能并嵌入伦理属性的预测模型。

Method: 正式定义不同决策任务和数据类型的监督学习设置，分析三类可解释模型，形式化定义因果性、公平性和隐私性三个关键伦理属性，研究属性间权衡并探讨如何将伦理要求嵌入可解释流程。

Result: 建立了MIMOSA框架，明确了各类模型和伦理属性的相关内容。

Conclusion: 该框架为开发不仅准确、可解释，而且公平、保护隐私和具有因果意识的可信AI系统奠定了理论基础。

Abstract: Interpretable-by-design models are crucial for fostering trust,
accountability, and safe adoption of automated decision-making models in
real-world applications. In this paper we formalize the ground for the MIMOSA
(Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a
comprehensive methodology for generating predictive models that balance
interpretability with performance while embedding key ethical properties. We
formally define here the supervised learning setting across diverse
decision-making tasks and data types, including tabular data, time series,
images, text, transactions, and trajectories. We characterize three major
families of interpretable models: feature importance, rule, and instance based
models. For each family, we analyze their interpretability dimensions,
reasoning mechanisms, and complexity. Beyond interpretability, we formalize
three critical ethical properties, namely causality, fairness, and privacy,
providing formal definitions, evaluation metrics, and verification procedures
for each. We then examine the inherent trade-offs between these properties and
discuss how privacy requirements, fairness constraints, and causal reasoning
can be embedded within interpretable pipelines. By evaluating ethical measures
during model generation, this framework establishes the theoretical foundations
for developing AI systems that are not only accurate and interpretable but also
fair, privacy-preserving, and causally aware, i.e., trustworthy.

</details>


### [32] [Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications](https://arxiv.org/abs/2510.20632)
*Shuyi Xie,Ziqin Liew,Hailing Zhang,Haibo Zhang,Ling Hu,Zhiqiang Zhou,Shuman Liu,Anxiang Zeng*

Main category: cs.AI

TL;DR: 现有电商大模型评估缺乏多样性和真实场景，本文提出EcomEval多语言多模态基准测试评估电商大模型。


<details>
  <summary>Details</summary>
Motivation: 现有电商评估在任务多样性、模态、数据来源和语言覆盖方面不足，缺少评估复杂真实购物场景模型的可靠工具。

Method: 构建涵盖6类37个任务（含8个多模态任务）的EcomEval，数据源于真实客户查询和交易日志；采用半自动化流程生成参考答案；通过不同规模和能力模型的评估分数定义难度级别；涵盖七种语言。

Result: 创建了EcomEval基准测试。

Conclusion: EcomEval能为电商领域大模型提供更全面、真实、多语言的评估。

Abstract: Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet
their capabilities in specialized domains remain underexplored. In e-commerce,
existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping
MMLU-suffer from limited task diversity (e.g., lacking product guidance and
after-sales issues), limited task modalities (e.g., absence of multimodal
data), synthetic or curated data, and a narrow focus on English and Chinese,
leaving practitioners without reliable tools to assess models on complex,
real-world shopping scenarios. We introduce EcomEval, a comprehensive
multilingual and multimodal benchmark for evaluating LLMs in e-commerce.
EcomEval covers six categories and 37 tasks (including 8 multimodal tasks),
sourced primarily from authentic customer queries and transaction logs,
reflecting the noisy and heterogeneous nature of real business interactions. To
ensure both quality and scalability of reference answers, we adopt a
semi-automatic pipeline in which large models draft candidate responses
subsequently reviewed and modified by over 50 expert annotators with strong
e-commerce and multilingual expertise. We define difficulty levels for each
question and task category by averaging evaluation scores across models with
different sizes and capabilities, enabling challenge-oriented and fine-grained
assessment. EcomEval also spans seven languages-including five low-resource
Southeast Asian languages-offering a multilingual perspective absent from prior
work.

</details>


### [33] [Fluidity Index: Next-Generation Super-intelligence Benchmarks](https://arxiv.org/abs/2510.20636)
*Eric Ngoiya,Tianshu Bao*

Main category: cs.AI

TL;DR: 本文引入流动性指数（FI）量化动态、可扩展环境中模型适应性，区分不同基准测试适应性。


<details>
  <summary>Details</summary>
Motivation: 在动态、可扩展环境中量化模型的适应性。

Method: 引入流动性指数（FI），区分封闭式和开放式基准，优先采用闭环开放式现实基准测试。

Result: 提出衡量模型理解、预测和适应状态变化能力的方法。

Conclusion: 真正的超级智能模型应具备至少二阶适应性，通过数字补充实现自我维持计算以达最佳流动性。

Abstract: This paper introduces the Fluidity Index (FI) to quantify model adaptability
in dynamic, scaling environments. The benchmark evaluates response accuracy
based on deviations in initial, current, and future environment states,
assessing context switching and continuity. We distinguish between closed-ended
and open-ended benchmarks, prioritizing closed-loop open-ended real-world
benchmarks to test adaptability. The approach measures a model's ability to
understand, predict, and adjust to state changes in scaling environments. A
truly super-intelligent model should exhibit at least second-order
adaptability, enabling self-sustained computation through digital replenishment
for optimal fluidity.

</details>


### [34] [Integrating Machine Learning into Belief-Desire-Intention Agents: Current Advances and Open Challenges](https://arxiv.org/abs/2510.20641)
*Andrea Agiollo,Andrea Omicini*

Main category: cs.AI

TL;DR: 本文以BDI范式为参考对现有将机器学习集成到理性智能体架构的方法进行细粒度系统化分析，指出研究机会和挑战。


<details>
  <summary>Details</summary>
Motivation: 当前将机器学习集成到理性智能体架构的领域碎片化且缺乏对理性架构表达能力的关注，需要进行系统化研究。

Method: 以BDI范式为参考，对现有方法进行细粒度系统化分析。

Result: 展示了机器学习增强的理性智能体的快速发展文献。

Conclusion: 确定了设计有效理性机器学习智能体的关键研究机会和开放挑战。

Abstract: Thanks to the remarkable human-like capabilities of machine learning (ML)
models in perceptual and cognitive tasks, frameworks integrating ML within
rational agent architectures are gaining traction. Yet, the landscape remains
fragmented and incoherent, often focusing on embedding ML into generic agent
containers while overlooking the expressive power of rational
architectures--such as Belief-Desire-Intention (BDI) agents. This paper
presents a fine-grained systematisation of existing approaches, using the BDI
paradigm as a reference. Our analysis illustrates the fast-evolving literature
on rational agents enhanced by ML, and identifies key research opportunities
and open challenges for designing effective rational ML agents.

</details>


### [35] [The Shape of Reasoning: Topological Analysis of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2510.20665)
*Xue Wen Tan,Nathaniel Tan,Galen Lee,Stanley Kok*

Main category: cs.AI

TL;DR: 本文指出大语言模型推理轨迹质量评估存在问题，引入基于拓扑数据分析的评估框架，实证表明拓扑特征评估效果更好。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理轨迹质量评估研究不足、劳动密集且不可靠，现有自动化方法存在缺陷。

Method: 引入基于拓扑数据分析（TDA）的评估框架，捕捉推理轨迹的几何特征。

Result: 拓扑特征在评估推理质量方面比标准图指标具有更高的预测能力，一组紧凑、稳定的拓扑特征能可靠指示轨迹质量。

Conclusion: 有效推理更适合用高维几何结构而非纯关系图来捕捉，拓扑特征可为未来强化学习算法提供实用信号。

Abstract: Evaluating the quality of reasoning traces from large language models remains
understudied, labor-intensive, and unreliable: current practice relies on
expert rubrics, manual annotation, and slow pairwise judgments. Automated
efforts are dominated by graph-based proxies that quantify structural
connectivity but do not clarify what constitutes high-quality reasoning; such
abstractions can be overly simplistic for inherently complex processes. We
introduce a topological data analysis (TDA)-based evaluation framework that
captures the geometry of reasoning traces and enables label-efficient,
automated assessment. In our empirical study, topological features yield
substantially higher predictive power for assessing reasoning quality than
standard graph metrics, suggesting that effective reasoning is better captured
by higher-dimensional geometric structures rather than purely relational
graphs. We further show that a compact, stable set of topological features
reliably indicates trace quality, offering a practical signal for future
reinforcement learning algorithms.

</details>


### [36] [Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs](https://arxiv.org/abs/2510.20691)
*Yanlin Song,Ben Liu,Víctor Gutiérrez-Basulto,Zhiwei Hu,Qianqian Xie,Min Peng,Sophia Ananiadou,Jeff Z. Pan*

Main category: cs.AI

TL;DR: 现有KGQA方法难以充分利用知识图谱和大语言模型能力，提出Graph - RFT框架解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA方法在复杂场景下难以充分利用知识图谱知识和大语言模型推理能力，存在假设知识图谱覆盖完全、缺乏判断外部信息需求机制、推理短视等问题。

Method: 提出Graph - RFT两阶段强化微调框架，采用'plan - KGsearch - and - Websearch - during - think'范式，引入思维链微调方法和计划检索引导的强化学习过程，用笛卡尔规划模块分解问题，结合多奖励优化推理检索过程。

Result: 文中未明确提及具体结果。

Conclusion: Graph - RFT框架能使大语言模型在知识不完整条件下进行自主规划和跨知识图谱与网络源的自适应检索调度，实现全局一致的多步推理。

Abstract: Knowledge Graph Question Answering aims to answer natural language questions
by reasoning over structured knowledge graphs. While large language models have
advanced KGQA through their strong reasoning capabilities, existing methods
continue to struggle to fully exploit both the rich knowledge encoded in KGs
and the reasoning capabilities of LLMs, particularly in complex scenarios. They
often assume complete KG coverage and lack mechanisms to judge when external
information is needed, and their reasoning remains locally myopic, failing to
maintain coherent multi-step planning, leading to reasoning failures even when
relevant knowledge exists. We propose Graph-RFT, a novel two-stage
reinforcement fine-tuning KGQA framework with a
'plan-KGsearch-and-Websearch-during-think' paradigm, that enables LLMs to
perform autonomous planning and adaptive retrieval scheduling across KG and web
sources under incomplete knowledge conditions. Graph-RFT introduces a
chain-of-thought fine-tuning method with a customized plan-retrieval dataset
activates structured reasoning and resolves the GRPO cold-start problem. It
then introduces a novel plan-retrieval guided reinforcement learning process
integrates explicit planning and retrieval actions with a multi-reward design,
enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspired
planning module to decompose complex questions into ordered subquestions, and
logical expression to guide tool invocation for globally consistent multi-step
reasoning. This reasoning retrieval process is optimized with a multi-reward
combining outcome and retrieval specific signals, enabling the model to learn
when and how to combine KG and web retrieval effectively.

</details>


### [37] [A Coherence-Based Measure of AGI](https://arxiv.org/abs/2510.20784)
*Fares Fourati*

Main category: cs.AI

TL;DR: 本文指出现有AGI定义的问题，提出基于广义均值积分的AGI衡量方法，应用于GPT-4和GPT-5显示两者距通用能力仍远。


<details>
  <summary>Details</summary>
Motivation: 现有AGI定义假设可补偿性，不能反映真正的通用智能所需的连贯充分性，需更合理的衡量方法。

Method: 提出基于广义均值在一系列可补偿性指数上积分的AGI衡量方法，用AUC量化不同可补偿性假设下的鲁棒性。

Result: 应用该方法到GPT - 4和GPT - 5的CHC域得分，发现尽管算术得分高，但两者距通用能力仍远。

Conclusion: 整合广义均值为衡量AGI进展提供了有原则、可解释且更严格的基础。

Abstract: Recent work by \citet{hendrycks2025agidefinition} formalized
\textit{Artificial General Intelligence} (AGI) as the arithmetic mean of
proficiencies across cognitive domains derived from the Cattell--Horn--Carroll
(CHC) model of human cognition. While elegant, this definition assumes
\textit{compensability} -- that exceptional ability in some domains can offset
failure in others. True general intelligence, however, should reflect
\textit{coherent sufficiency}: balanced competence across all essential
domains. We propose a coherence-aware measure of AGI based on the integral of
generalized means over a continuum of compensability exponents. This
formulation spans arithmetic, geometric, and harmonic regimes, and the
resulting \textit{area under the curve} (AUC) quantifies robustness under
varying compensability assumptions. Unlike the arithmetic mean, which rewards
specialization, the AUC penalizes imbalance and captures inter-domain
dependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5,
the coherence-adjusted AUC reveals that both systems remain far from general
competence despite high arithmetic scores (e.g., GPT-5 at~24\%). Integrating
the generalized mean thus yields a principled, interpretable, and stricter
foundation for measuring genuine progress toward AGI.

</details>


### [38] [Real Deep Research for AI, Robotics and Beyond](https://arxiv.org/abs/2510.20809)
*Xueyan Zou,Jianglong Ye,Hao Zhang,Xiaoyu Xiang,Mingyu Ding,Zhaojing Yang,Yong Jae Lee,Zhuowen Tu,Sifei Liu,Xiaolong Wang*

Main category: cs.AI

TL;DR: AI和机器人研究发展迅速，论文众多，研究者难跟上前沿。本文提出通用分析管道RDR，用于分析研究领域，还简要扩展到其他科学领域，望为研究者提供指引。


<details>
  <summary>Details</summary>
Motivation: AI和机器人研究论文数量快速增长，研究者难以跟上最新进展，且存在快速演变的趋势、跨学科工作增多和需探索专业外领域等挑战。

Method: 提出可系统分析任何研究领域的通用管道RDR，并应用于AI和机器人领域，尤其关注基础模型和机器人进展，还简要扩展到其他科学领域。

Result: 主论文详述RDR管道构建，附录提供各分析主题的广泛结果。

Conclusion: 希望此工作能为AI及其他领域的研究者提供启示。

Abstract: With the rapid growth of research in AI and robotics now producing over
10,000 papers annually it has become increasingly difficult for researchers to
stay up to date. Fast evolving trends, the rise of interdisciplinary work, and
the need to explore domains beyond one's expertise all contribute to this
challenge. To address these issues, we propose a generalizable pipeline capable
of systematically analyzing any research area: identifying emerging trends,
uncovering cross domain opportunities, and offering concrete starting points
for new inquiry. In this work, we present Real Deep Research (RDR) a
comprehensive framework applied to the domains of AI and robotics, with a
particular focus on foundation models and robotics advancements. We also
briefly extend our analysis to other areas of science. The main paper details
the construction of the RDR pipeline, while the appendix provides extensive
results across each analyzed topic. We hope this work sheds light for
researchers working in the field of AI and beyond.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [39] [Finite Element and Machine Learning Modeling of Autogenous Self-Healing Concrete](https://arxiv.org/abs/2510.19839)
*William Liu*

Main category: cs.CE

TL;DR: 开发了自愈合混凝土的时变建模框架，实现水分扩散与损伤演化耦合，对比两模型，模拟分析愈合时间影响因素，机器学习分类器预测愈合结果准确率高，为研究提供工具。


<details>
  <summary>Details</summary>
Motivation: 开发能耦合水分扩散与损伤演化的自愈合混凝土时变建模框架，以指导实验室研究和应用。

Method: 水分传输遵循菲克第二定律，用幂律插值计算损伤相关扩散率，通过亥姆霍兹滤波器计算水泥可用性场，在FEniCSx中实现两种有限元变体，进行模拟并训练机器学习分类器。

Result: 愈合时间与裂缝方向非单调相关，与裂缝宽度关系随材料参数变化；CMM模型能再现阶段性水分渗透但延长总愈合时间，CDM模型适合参数扫描；机器学习分类器预测愈合结果准确率高。

Conclusion: 虽需实验校准，但该框架为自愈合混凝土研究和应用提供了通用工具。

Abstract: A time-dependent modeling framework for autogenous self-healing concrete that
couples moisture diffusion with damage evolution was developed. Water transport
follows Fick's second law with a damage-dependent diffusivity obtained by
power-law interpolation between intact concrete and crack space. Healing
reduces damage in proportion to local moisture and a smoothed cement
availability field computed via a Helmholtz filter. Two finite element variants
were implemented in FEniCSx over time horizons up to $5\times10^6$ seconds: a
Crack Diffusion Model (CDM) with standard diffusion and a Crack Membrane Model
(CMM) that gates cross-crack transport until a critical moisture threshold is
reached. Key control parameters are the initial crack orientation and size, the
diffusion coefficients of intact and cracked concrete, the healing rate
constant, and the cement availability smoothing parameter. Simulations on a
unit square domain show that healing time varies non-monotonically with crack
orientation, peaking near $45^\circ$ and $135^\circ$ and minimizing near
$90^\circ$, consistent with diffusion distance to crack endpoints dominating
the process. The dependence on crack width reverses with material parameters:
healing time increases when $D_{\text{cracked}}<D_{\text{intact}}$ and
decreases when $D_{\text{cracked}}>D_{\text{intact}}$. The CMM reproduces
staged moisture penetration with delayed gate activation but lengthens total
healing time, whereas the CDM is efficient for parametric sweeps. Machine
learning classifiers trained on one million simulation samples predict binary
healing outcomes $H(\sigma,\gamma,t)$ (healed or not) with high accuracy (up to
0.998 for neural networks). Although experimental calibration is still
required, the framework provides a versatile tool for guiding laboratory
studies and implementations of self-healing concrete.

</details>


### [40] [A polygonal Reissner-Mindlin plate element based on the scaled boundary finite element method](https://arxiv.org/abs/2510.20044)
*Anna Hellers,Mathias Reichle,Sven Klinkel*

Main category: cs.CE

TL;DR: 提出基于比例边界有限元法的多边形Reissner - Mindlin板单元，解决剪切锁定等问题并通过算例验证有效性。


<details>
  <summary>Details</summary>
Motivation: 简化网格划分过程，解决低阶Reissner - Mindlin单元在薄板极限下的横向剪切锁定和泊松厚度锁定问题。

Method: 引入线性形函数实现完全离散化，推导假定自然应变法，引入双场变分公式。

Result: 给出的公式能让非星凸多边形单元使用，可结合三维材料定律。

Conclusion: 通过数值算例证明了所提公式的有效性。

Abstract: In this work, a polygonal Reissner-Mindlin plate element is presented. The
formulation is based on a scaled boundary finite element method, where in
contrast to the original semi-analytical approach, linear shape functions are
introduced for the parametrization of the scaling and the radial direction.
This yields a fully discretized formulation, which enables the use of
non-star-convex-polygonal elements with an arbitrary number of edges,
simplifying the meshing process. To address the common effect of transverse
shear locking for low-order Reissner-Mindlin elements in the thin-plate limit,
an assumed natural strain approach for application on the polygonal scaled
boundary finite elements is derived. Further, a two-field variational
formulation is introduced to incorporate three-dimensional material laws. Here
the plane stress assumptions are enforced on the weak formulation, facilitating
the use of material models defined in three-dimensional continuum while
considering the effect of Poisson's thickness locking. The effectiveness of the
proposed formulation is demonstrated in various numerical examples.

</details>


### [41] [SparseEB-gMCR: A Generative Solver for Extreme Sparse Components with Application to Contamination Removal in GC-MS](https://arxiv.org/abs/2510.20364)
*Yu-Tang Chang,Shih-Fang Chen*

Main category: cs.CE

TL;DR: 介绍分析化学仪器信号分解，提出SparseEB - gMCR改进EB - gMCR处理极端稀疏信号问题，在合成数据集和实际GC - MS色谱图中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 极端信号稀疏性会损害EB - gMCR的可分解性，需要改进方法处理极端稀疏成分。

Method: 引入固定EB - select模块，结合能量优化微调，得到SparseEB - gMCR。

Result: 在合成数据集有可比可分解性和扩展性，在实际GC - MS色谱图中有效消除污染信号，提高化合物鉴定可靠性。

Conclusion: SparseEB - gMCR保留EB - gMCR能力，扩展对稀疏不规则化学数据适应性，使EB - gMCR家族适用于更多实际化学数据集。

Abstract: Analytical chemistry instruments provide physically meaningful signals for
elucidating analyte composition and play important roles in material,
biological, and food analysis. These instruments are valued for strong
alignment with physical principles, enabling compound identification through
pattern matching with chemical libraries. More reliable instruments generate
sufficiently sparse signals for direct interpretation. Generative multivariate
curve resolution (gMCR) and its energy-based solver (EB-gMCR) offer powerful
tools for decomposing mixed signals suitable for chemical data analysis.
However, extreme signal sparsity from instruments such as GC-MS or 1H-NMR can
impair EB-gMCR decomposability. To address this, a fixed EB-select module
inheriting EB-gMCR's design was introduced for handling extreme sparse
components. Combined with minor adjustments to energy optimization, this led to
SparseEB-gMCR. In synthetic datasets, SparseEB-gMCR exhibited comparable
decomposability and graceful scalability to dense-component EB-gMCR. The sparse
variant was applied to real GC-MS chromatograms for unsupervised contamination
removal. Analysis showed siloxane-related pollution signals were effectively
eliminated, improving compound identification reliability. Results demonstrate
that SparseEB-gMCR preserves the decomposability and self-determining component
capability of EB-gMCR while extending adaptability to sparse and irregular
chemical data. With this sparse extension, the EB-gMCR family becomes
applicable to wider ranges of real-world chemical datasets, providing a general
mathematical framework for signal unmixing and contamination elimination in
analytical chemistry.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [42] [Query Optimization in the Wild: Realities and Trends](https://arxiv.org/abs/2510.20082)
*Yuanyuan Tian*

Main category: cs.DB

TL;DR: 本文从工业视角回顾生产系统中查询优化的发展，指出传统架构局限，介绍应对挑战的三个关键趋势，为查询优化指明未来方向。


<details>
  <summary>Details</summary>
Motivation: 云计算、海量数据和统一数据平台使传统查询优化架构的局限性暴露，需寻找新方法应对挑战。

Method: 回顾生产系统中查询优化的过去和现在，识别当前面临的挑战，提出三个关键趋势。

Result: 明确了三个关键趋势，即查询优化与执行间更紧密的反馈循环、优化范围从单查询扩展到整个工作负载、从整体设计转向可组合架构。

Conclusion: 这些趋势为查询优化实践指明了更动态、全面和自适应的未来路径。

Abstract: For nearly half a century, the core design of query optimizers in industrial
database systems has remained remarkably stable, relying on foundational
principles from System R and the Volcano/Cascades framework. However, the rise
of cloud computing, massive data volumes, and unified data platforms has
exposed the limitations of this traditional, monolithic architecture. Taking an
industrial perspective, this paper reviews the past and present of query
optimization in production systems and identifies the challenges they face
today. Then this paper highlights three key trends gaining momentum in the
industry that promise to address these challenges. First, a tighter feedback
loop between query optimization and query execution is being used to improve
the robustness of query performance. Second, the scope of optimization is
expanding from a single query to entire workloads through the convergence of
query optimization and workload optimization. Third, and perhaps most
transformatively, the industry is moving from monolithic designs to composable
architectures that foster agility and cross-engine collaboration. Together,
these trends chart a clear path toward a more dynamic, holistic, and adaptable
future for query optimization in practice.

</details>


### [43] [UREM: A High-performance Unified and Resilient Enhancement Method for Multi- and High-Dimensional Indexes](https://arxiv.org/abs/2510.20110)
*Ming Sheng,Shuliang Wang,Yong Zhang,Yi Luo,Xianbo Liu,Zeming Li*

Main category: cs.DB

TL;DR: 本文提出针对多/高维索引的统一弹性增强方法UREM，能适应不同场景，实验表明其显著提升索引查询性能。


<details>
  <summary>Details</summary>
Motivation: 现有面向结构和布局的索引增强方法分别在动态和静态工作负载下存在不足，缺乏统一且有弹性的增强方法。

Method: 提出UREM，可统一应用于不同平台的不同索引，在静态负载下通过布局优化提升性能，在查询变化时通过部分布局重组稳定性能。

Result: 在20个常用索引上评估，UREM在静态负载下使多/高维索引查询性能最多提升5.73x和9.18x，动态负载下平均提升5.72x和9.47x，部分传统索引经增强后性能可媲美或超越先进索引。

Conclusion: UREM是一种有效的统一弹性增强方法，能自适应不同场景提升多/高维索引的查询性能。

Abstract: Numerous multi- or high-dimensional indexes with distinct advantages have
been proposed on various platforms to meet application requirements. To achieve
higher-performance queries, most indexes employ enhancement methods, including
structure-oriented and layout-oriented enhancement methods. Existing
structure-oriented methods tailored to specific indexes work well under static
workloads but lack generality and degrade under dynamic workloads. The
layout-oriented methods exhibit good generality and perform well under dynamic
workloads, but exhibit suboptimal performance under static workloads.
Therefore, it is an open challenge to develop a unified and resilient
enhancement method that can improve query performance for different indexes
adaptively under different scenarios. In this paper, we propose UREM, which is
the first high-performance Unified and Resilient Enhancement Method designed
for both multi- and high-dimensional indexes, capable of adapting to different
scenarios. Specifically, UREM (1) can be uniformly applied with different
indexes on various platforms; (2) enhances the query performance of indexes by
layout optimization under static workloads; (3) enables indexes to stabilize
performance when queries shift through partial layout reorganization. We
evaluate UREM on 20 widely used indexes. Experimental results demonstrate that
UREM improves the query performance of multi- and high-dimensional indexes by
up to 5.73x and 9.18x under static workloads, and by an average of 5.72x and
9.47x under dynamic workloads. Moreover, some traditional indexes enhanced by
UREM even achieve performance comparable to or even surpassing that of recent
advanced indexes.

</details>


### [44] [RAG-Stack: Co-Optimizing RAG Quality and Performance From the Vector Database Perspective](https://arxiv.org/abs/2510.20296)
*Wenqi Jiang*

Main category: cs.DB

TL;DR: 本文提出RAG - Stack，用于检索增强生成（RAG）系统的质量与性能协同优化。


<details>
  <summary>Details</summary>
Motivation: 现有向量数据库在端到端RAG管道中有诸多开放研究问题，联合优化RAG系统性能和生成质量是实际且具挑战性的问题。

Method: 提出RAG - Stack，包含作为抽象层解耦质量和性能的RAG - IR、估计系统性能的RAG - CM以及搜索高质量高性能配置的RAG - PE。

Result: 提出一套用于RAG系统质量和性能协同优化的三支柱蓝图。

Conclusion: 认为该三支柱蓝图未来将成为RAG质量 - 性能协同优化的事实范式。

Abstract: Retrieval-augmented generation (RAG) has emerged as one of the most prominent
applications of vector databases. By integrating documents retrieved from a
database into the prompt of a large language model (LLM), RAG enables more
reliable and informative content generation. While there has been extensive
research on vector databases, many open research problems remain once they are
considered in the wider context of end-to-end RAG pipelines. One practical yet
challenging problem is how to jointly optimize both system performance and
generation quality in RAG, which is significantly more complex than it appears
due to the numerous knobs on both the algorithmic side (spanning models and
databases) and the systems side (from software to hardware). In this paper, we
present RAG-Stack, a three-pillar blueprint for quality-performance
co-optimization in RAG systems. RAG-Stack comprises: (1) RAG-IR, an
intermediate representation that serves as an abstraction layer to decouple
quality and performance aspects; (2) RAG-CM, a cost model for estimating system
performance given an RAG-IR; and (3) RAG-PE, a plan exploration algorithm that
searches for high-quality, high-performance RAG configurations. We believe this
three-pillar blueprint will become the de facto paradigm for RAG
quality-performance co-optimization in the years to come.

</details>


### [45] [Hybrid Mixed Integer Linear Programming for Large-Scale Join Order Optimisation](https://arxiv.org/abs/2510.20308)
*Manuel Schönberger,Immanuel Trummer,Wolfgang Mauerer*

Main category: cs.DB

TL;DR: 提出基于混合整数线性规划（MILP）的新方法解决大查询连接顺序优化问题，能扩展到超大型查询并保证计划质量。


<details>
  <summary>Details</summary>
Motivation: 现有查询优化方法在处理大查询时存在局限，如穷举法不适用于大查询，启发式方法随查询规模增大质量下降。

Method: 提出新的MILP模型优化任意浓密树结构，将MILP方法嵌入混合框架，在优势处用MILP求解器，简单步骤用更高效方法。

Result: 方法能扩展到连接多达100个关系的超大型查询规模。

Conclusion: 该方法在众多竞争的连接排序方法中始终能达到最稳健的计划质量。

Abstract: Finding optimal join orders is among the most crucial steps to be performed
by query optimisers. Though extensively studied in data management research,
the problem remains far from solved: While query optimisers rely on exhaustive
search methods to determine ideal solutions for small problems, such methods
reach their limits once queries grow in size. Yet, large queries become
increasingly common in real-world scenarios, and require suitable methods to
generate efficient execution plans. While a variety of heuristics have been
proposed for large-scale query optimisation, they suffer from degrading
solution quality as queries grow in size, or feature highly sub-optimal
worst-case behavior, as we will show.
  We propose a novel method based on the paradigm of mixed integer linear
programming (MILP): By deriving a novel MILP model capable of optimising
arbitrary bushy tree structures, we address the limitations of existing MILP
methods for join ordering, and can rely on highly optimised MILP solvers to
derive efficient tree structures that elude competing methods. To ensure
optimisation efficiency, we embed our MILP method into a hybrid framework,
which applies MILP solvers precisely where they provide the greatest advantage
over competitors, while relying on more efficient methods for less complex
optimisation steps. Thereby, our approach gracefully scales to extremely large
query sizes joining up to 100 relations, and consistently achieves the most
robust plan quality among a large variety of competing join ordering methods.

</details>


### [46] [An Empirical Study on Database Usage in Microservices](https://arxiv.org/abs/2510.20582)
*Maxime André,Marco Raglianti,Souhaila Serbout,Anthony Cleve,Michele Lanza*

Main category: cs.DB

TL;DR: 本文对微服务中数据库使用进行实证研究，分析约1000个GitHub项目，给出18个发现和9条建议，揭示微服务数据库使用特点。


<details>
  <summary>Details</summary>
Motivation: 微服务架构改变数据库管理，数据管理是重大挑战且相关研究文献稀缺，需了解微服务中数据库使用情况。

Method: 在收集的数据集上，考察约1000个使用14类180种数据库技术的GitHub项目，进行全面分析。

Result: 微服务主要使用关系型、键值型、文档型和搜索型数据库，52%的微服务组合多种数据库类别，复杂度与数据库数量相关，新旧系统有不同偏好，小众数据库常与主流数据库结合。

Conclusion: 为研究人员和从业者提供实证依据，助其更好理解微服务中数据库使用。

Abstract: Microservices architectures are an integral part of modern software
development. Their adoption brings significant changes to database management.
Instead of relying on a single database, a microservices architecture is
typically composed of multiple, smaller, heterogeneous, and distributed DBs. In
these data-intensive systems, the variety and combination of database
categories and technologies play a crucial role in storing and managing data.
While data management in microservices is a major challenge, research
literature is scarce.
  We present an empirical study on how databases are used in microservices. On
the dataset we collected (and released as open data for future research),
considering 15 years of microservices, we examine ca. 1,000 GitHub projects
that use databases selected among 180 technologies from 14 categories. We
perform a comprehensive analysis of current practices, providing researchers
and practitioners with empirical evidence to better understand database usage
in microservices. We report 18 findings and 9 recommendations. We show that
microservices predominantly use Relational, Key-Value, Document, and Search
databases. Notably, 52% of microservices combine multiple database categories.
Complexity correlates with database count, with older systems favoring
Relational databases and newer ones increasingly adopting Key-Value and
Document technologies. Niche databases (e.g., EventStoreDB, PostGIS), while not
widespread, are often combined with a mainstream one.

</details>


### [47] [Balanced Popularity in Multi-Product Billboard Advertisement](https://arxiv.org/abs/2510.20600)
*Dildar Ali,Suman Banerjee,Yamuna Prasad*

Main category: cs.DB

TL;DR: 研究多产品广告牌插槽选择问题，证明问题NP难，提出线性规划和贪心启发式方法，实验表明比基线方法更有影响力。


<details>
  <summary>Details</summary>
Motivation: 解决广告牌广告中多产品推广时选择有限插槽以最大化综合影响力并平衡各产品影响力的问题。

Method: 将问题建模为线性规划问题，使用线性规划松弛和随机舍入，提出带平衡修正的贪心启发式方法。

Result: 通过真实数据集实验，所提解决方案比许多基线方法带来更多影响力。

Conclusion: 所提解决多产品广告牌插槽选择问题的方法有效，能提升影响力。

Abstract: The billboard advertisement has emerged as an effective out-of-home
advertisement technique where the objective is to choose a limited number of
slots to play some advertisement content (e.g., animation, video, etc.) with
the hope that the content will be visible to a large number of travelers, and
this will be helpful to earn more revenue. In this paper, we study a variant of
the influential slot selection problem where the advertiser wants to promote
multiple products. Formally, we call this problem the \textsc{Multi-Product
Influence Maximization Problem for the Balanced Popularity} Problem. The input
to our problem is a trajectory and a billboard database, as well as a budget
for each product. The goal here is to choose a subset of slots for each product
such that the aggregated influence of all the products gets maximized subject
to the following two constraints: total selection cost for each product is less
than or equal to the allocated budget for that product, and the difference
between the influence for any two products is less than or equal to a given
threshold. We show that the problem is NP-hard to solve optimally. We formulate
this problem as a linear programming problem and use linear programming
relaxation with randomized rounding. Further, we propose a greedy-based
heuristic with balance correction to solve this problem. We conduct a number of
experiments with real-world trajectory and billboard datasets, and the results
are reported. From the reported results, we observe that the proposed solution
approaches lead to more influence compared to many baseline methods.

</details>


### [48] [Downsizing Diffusion Models for Cardinality Estimation](https://arxiv.org/abs/2510.20681)
*Xinhe Mu,Zhaoqi Zhou,Zaijiu Shang,Chuan Zhou,Gang Fu,Guiying Yan,Guoliang Li,Zhiming Ma*

Main category: cs.DB

TL;DR: 本文提出基于缩小版扩散模型的联合分布基数估计器ADC和ADC+，方法高效，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 受基于分数的扩散模型在估计高维复杂分布的性能启发，开发新的联合分布基数估计器。

Method: ADC的密度估计器用积分分数函数评估对数似然计算点密度值；选择性估计器用GMM预测选择性，再用重要性采样蒙特卡罗校正；ADC+训练决策树识别GMM可准确预测的查询以跳过校正阶段。

Result: 在真实数据集上，ADC+性能优于Naru等，存储仅需约66%；在合成数据集上，ADC和ADC+更稳健，ADC+表现最佳。

Conclusion: ADC和ADC+在联合分布基数估计方面表现出色，具有高效性和准确性。

Abstract: Inspired by the performance of score-based diffusion models in estimating
complex text, video, and image distributions with thousands of dimensions, we
introduce Accelerated Diffusion Cardest (ADC), the first joint distribution
cardinality estimator based on a downsized diffusion model.
  To calculate the pointwise density value of data distributions, ADC's density
estimator uses a formula that evaluates log-likelihood by integrating the score
function, a gradient mapping which ADC has learned to efficiently approximate
using its lightweight score estimator. To answer ranged queries, ADC's
selectivity estimator first predicts their selectivity using a Gaussian Mixture
Model (GMM), then uses importance sampling Monte Carlo to correct its
predictions with more accurate pointwise density values calculated by the
density estimator. ADC+ further trains a decision tree to identify the
high-volume, high-selectivity queries that the GMM alone can predict very
accurately, in which case it skips the correction phase to prevent Monte Carlo
from adding more variance. Doing so lowers median Q-error and cuts per-query
latency by 25 percent, making ADC+ usually twice as fast as Naru, arguably the
state-of-the-art joint distribution cardinality estimator.
  Numerical experiments using well-established benchmarks show that on all
real-world datasets tested, ADC+ is capable of rivaling Naru and outperforming
MSCN, DeepDB, LW-Tree, and LW-NN using around 66 percent their storage space,
being at least 3 times as accurate as MSCN on 95th and 99th percentile error.
Furthermore, on a synthetic dataset where attributes exhibit complex,
multilateral correlations, ADC and ADC+ are considerably robust while almost
every other learned model suffered significant accuracy declines. In this case,
ADC+ performs better than any other tested model, being 10 times as accurate as
Naru on 95th and 99th percentile error.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [49] [New Hardness Results for the LOCAL Model via a Simple Self-Reduction](https://arxiv.org/abs/2510.19972)
*Alkida Balliu,Filippo Casagrande,Francesco d'Amore,Dennis Olivetti*

Main category: cs.DC

TL;DR: 简化Khoury和Schild的技术，得到最大b - 匹配和边着色问题的随机性LOCAL算法轮数下界。


<details>
  <summary>Details</summary>
Motivation: Khoury和Schild的证明复杂难理解，简化证明和技术对理解图问题复杂度很重要。

Method: 提出自归约轮消除技术的简化版本。

Result: 得到最大b - 匹配问题和边着色问题的随机性LOCAL算法轮数下界，b = 1时可得到最大匹配下界的简短证明。

Conclusion: 简化技术有助于解决图问题，得到相关问题的复杂度下界。

Abstract: Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL
algorithm that solves maximal matching requires $\Omega(\min\{\log \Delta,
\log_\Delta n\})$ rounds, where $n$ is the number of nodes in the graph and
$\Delta$ is the maximum degree. This result is shown through a new technique,
called round elimination via self-reduction. The lower bound proof is beautiful
and presents very nice ideas. However, it spans more than 25 pages of technical
details, and hence it is hard to digest and generalize to other problems.
Historically, the simplification of proofs and techniques has marked an
important turning point in our understanding of the complexity of graph
problems. Our paper makes a step forward towards this direction, and provides
the following contributions.
  1. We present a short and simplified version of the round elimination via
self-reduction technique. The simplification of this technique enables us to
obtain the following two hardness results.
  2. We show that any randomized LOCAL algorithm that solves the maximal
$b$-matching problem requires $\Omega(\min\{\log_{1+b}\Delta, \log_\Delta n\})$
and $\Omega(\sqrt{\log_{1+b} n})$ rounds. We recall that the $b$-matching
problem is a generalization of the matching problem where each vertex can have
up to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain
a short proof for the maximal matching lower bound shown by Khoury and Schild.
  3. Finally, we show that any randomized LOCAL algorithm that properly colors
the edges of a graph with $\Delta + k$ colors requires $\Omega(\min\{\log
\Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log n})$ rounds, for any $k\le
\Delta^{1-\varepsilon}$ and any constant $\varepsilon > 0$.

</details>


### [50] [AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training](https://arxiv.org/abs/2510.20111)
*Huawei Bai,Yifan Huang,Wenqi Shi,Ansheng You,Feifan Shao,Tengfei Han,Minghui Yu*

Main category: cs.DC

TL;DR: 提出异步分层零并行（AsyncHZP）方法解决语言模型训练效率和可扩展性瓶颈，实验证明其性能优越且简化大规模训练。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在大规模集群上训练效率和可扩展性存在瓶颈，主流方法繁琐复杂，灵活方法有通信开销问题。

Method: 提出AsyncHZP，自适应重新分片参数、梯度和优化器状态，设计多流异步调度方法。

Result: 在稠密和混合专家模型上的实验表明，AsyncHZP保持稳定，性能优于经典ND并行，无需复杂调优。

Conclusion: AsyncHZP能在保持简单和内存效率的同时实现优越性能，简化高效大规模训练路径。

Abstract: The training efficiency and scalability of language models on massive
clusters currently remain a critical bottleneck. Mainstream approaches like ND
parallelism are often cumbersome and complex, while flexible alternatives such
as the Zero Redundancy Optimizer (ZeRO) are frequently hampered by
communication overhead. In this paper, we propose Asynchronous Hierarchical
Zero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to
achieve superior performance while maintaining simplicity and memory
efficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding
that can lead to inefficient communication, AsyncHZP adaptively reshards
parameters, gradients, and optimizer states across different replica groups.
This strategy optimizes device memory utilization and significantly reduces
communication overhead. In addition, we also design a multi-stream asynchronous
scheduling method that executes parameter all-gather and gradient
reduce-scatter operations in dedicated background threads, effectively
overlapping communication with computation while incurring negligible memory
fragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE)
models confirm that AsyncHZP maintains robust stability at scale. It
consistently outperforms classic ND parallelism, achieving state-of-the-art
performance without complex strategic tuning, thereby simplifying the path to
efficient large-scale training.

</details>


### [51] [A Full Stack Framework for High Performance Quantum-Classical Computing](https://arxiv.org/abs/2510.20128)
*Xin Zhan,K. Grace Johnson,Aniello Esposito,Barbara Chapman,Marco Fiorentino,Kirk M. Bresniker,Raymond G. Beausoleil,Masoud Mohseni*

Main category: cs.DC

TL;DR: 提出HPC - QC全栈框架及混合工作负载开发能力，展示量子编程等接口发展，在超级计算机上演示多个混合工作负载，提供统一编程环境框架。


<details>
  <summary>Details</summary>
Motivation: 满足可扩展的高性能计算（HPC）和量子计算（QC）集成的增长需求。

Method: 采用模块化硬件/设备无关的软件集成方法，开发自适应电路编织管理程序，利用Cray LLVM编译框架。

Result: 展示了在现有成熟HPC编程环境中量子编程等接口的最新发展，在HPE EX超级计算机上演示了多个混合HPC - QC工作负载。

Conclusion: 提供了基于经典HPC软件栈的统一量子 - 经典编程环境框架。

Abstract: To address the growing needs for scalable High Performance Computing (HPC)
and Quantum Computing (QC) integration, we present our HPC-QC full stack
framework and its hybrid workload development capability with modular
hardware/device-agnostic software integration approach. The latest development
in extensible interfaces for quantum programming, dispatching, and compilation
within existing mature HPC programming environment are demonstrated. Our HPC-QC
full stack enables high-level, portable invocation of quantum kernels from
commercial quantum SDKs within HPC meta-program in compiled languages (C/C++
and Fortran) as well as Python through a quantum programming interface library
extension. An adaptive circuit knitting hypervisor is being developed to
partition large quantum circuits into sub-circuits that fit on smaller noisy
quantum devices and classical simulators. At the lower-level, we leverage Cray
LLVM-based compilation framework to transform and consume LLVM IR and Quantum
IR (QIR) from commercial quantum software frontends in a retargetable fashion
to different hardware architectures. Several hybrid HPC-QC multi-node multi-CPU
and GPU workloads (including solving linear system of equations, quantum
optimization, and simulating quantum phase transitions) have been demonstrated
on HPE EX supercomputers to illustrate functionality and execution viability
for all three components developed so far. This work provides the framework for
a unified quantum-classical programming environment built upon classical HPC
software stack (compilers, libraries, parallel runtime and process scheduling).

</details>


### [52] [Collective Communication for 100k+ GPUs](https://arxiv.org/abs/2510.20171)
*Min Si,Pavan Balaji,Yongzhou Chen,Ching-Hsiang Chu,Adi Gangidi,Saif Hasan,Subodh Iyengar,Dan Johnson,Bingzhe Liu,Jingliang Ren,Ashmitha Jeevaraj Shetty,Greg Steinbrecher,Xinfeng Xie,Yulun Wang,Bruce Wu,Jingyi Yang,Mingran Yang,Minlan Yu,Cen Zhao,Wes Bland,Denis Boyda,Suman Gumudavelli,Cristian Lumezanu,Rui Miao,Zhe Qu,Venkat Ramesh,Maxim Samoylov,Jan Seidel,Feng Tian,Qiye Tan,Shuqiang Zhang,Yimeng Zhao,Shengbao Zheng,Art Zhu,Hongyi Zeng*

Main category: cs.DC

TL;DR: 本文介绍Meta开发的NCCLX集体通信框架，可优化大语言模型全生命周期性能，在Llama4模型评估中显示通信效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型规模不断增大，传统通信方法在大规模训练时存在吞吐量和延迟限制，阻碍模型开发与部署。

Method: 开发NCCLX集体通信框架，支持超10万GPU集群的复杂工作负载。

Result: 在Llama4模型的实证评估中，通信效率有显著提高。

Conclusion: 该研究为下一代大语言模型在前所未有的规模上运行提供了可靠解决方案。

Abstract: The increasing scale of large language models (LLMs) necessitates highly
efficient collective communication frameworks, particularly as training
workloads extend to hundreds of thousands of GPUs. Traditional communication
methods face significant throughput and latency limitations at this scale,
hindering both the development and deployment of state-of-the-art models. This
paper presents the NCCLX collective communication framework, developed at Meta,
engineered to optimize performance across the full LLM lifecycle, from the
synchronous demands of large-scale training to the low-latency requirements of
inference. The framework is designed to support complex workloads on clusters
exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency
data exchange. Empirical evaluation on the Llama4 model demonstrates
substantial improvements in communication efficiency. This research contributes
a robust solution for enabling the next generation of LLMs to operate at
unprecedented scales.

</details>


### [53] [FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services](https://arxiv.org/abs/2510.20388)
*Víctor Rampérez,Javier Soriano,David Lizcano,Juan A. Lara*

Main category: cs.DC

TL;DR: 本文提出了分布式服务自动扩展器FLAS，结合主动和被动方法，介绍其创新点并在E - SilboPS中实现，经测试验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 云计算依赖自动扩展器实现弹性，当前缺乏适用于基于内容的发布 - 订阅分布式系统的自动扩展器，需要开发新系统。

Method: 提出FLAS，包含预测模型和反应式应急系统，在E - SilboPS中实现，并采用边界值分析测试方法评估。

Result: 通过多个测试用例评估，该方案能确保超99%的时间符合性能要求。

Conclusion: FLAS有效，是首个适用于基于内容的发布 - 订阅分布式系统的自动扩展器，也适用于其他分布式服务。

Abstract: Cloud computing has established itself as the support for the vast majority
of emerging technologies, mainly due to the characteristic of elasticity it
offers. Auto-scalers are the systems that enable this elasticity by acquiring
and releasing resources on demand to ensure an agreed service level. In this
article we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for
distributed services that combines the advantages of proactive and reactive
approaches according to the situation to decide the optimal scaling actions in
every moment. The main novelties introduced by FLAS are (i) a predictive model
of the high-level metrics trend which allows to anticipate changes in the
relevant SLA parameters (e.g. performance metrics such as response time or
throughput) and (ii) a reactive contingency system based on the estimation of
high-level metrics from resource use metrics, reducing the necessary
instrumentation (less invasive) and allowing it to be adapted agnostically to
different applications. We provide a FLAS implementation for the use case of a
content-based publish-subscribe middleware (E-SilboPS) that is the cornerstone
of an event-driven architecture. To the best of our knowledge, this is the
first auto-scaling system for content-based publish-subscribe distributed
systems (although it is generic enough to fit any distributed service). Through
an evaluation based on several test cases recreating not only the expected
contexts of use, but also the worst possible scenarios (following the
Boundary-Value Analysis or BVA test methodology), we have validated our
approach and demonstrated the effectiveness of our solution by ensuring
compliance with performance requirements over 99% of the time.

</details>


### [54] [Accurate Performance Predictors for Edge Computing Applications](https://arxiv.org/abs/2510.20495)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 提出自动构建和评估性能预测器的方法，在电子显微镜工作流用例中，预测器达90%准确率且推理时间短，强调需系统方法选预测器以提升资源利用率和性能可预测性。


<details>
  <summary>Details</summary>
Motivation: 资源受限动态边缘环境中多应用共定位和节点异构使应用性能预测困难，需有效方法实现可预测性能。

Method: 自动构建和评估各种性能预测器，优先考虑准确性和推理时间来确定最有效模型，基于与应用性能最相关的监控指标历史状态训练预测器，并在动态共定位场景下多服务器评估。

Result: 预测器达到90%准确率，推理时间少于往返时间的1%。

Conclusion: 需要系统方法在动态共定位场景中联合优化准确性和推理延迟来选择特定服务器预测器，将其集成到边缘环境可提高资源利用率和实现可预测性能。

Abstract: Accurate prediction of application performance is critical for enabling
effective scheduling and resource management in resource-constrained dynamic
edge environments. However, achieving predictable performance in such
environments remains challenging due to the co-location of multiple
applications and the node heterogeneity. To address this, we propose a
methodology that automatically builds and assesses various performance
predictors. This approach prioritizes both accuracy and inference time to
identify the most efficient model. Our predictors achieve up to 90% accuracy
while maintaining an inference time of less than 1% of the Round Trip Time.
These predictors are trained on the historical state of the most correlated
monitoring metrics to application performance and evaluated across multiple
servers in dynamic co-location scenarios. As usecase we consider electron
microscopy (EM) workflows, which have stringent real-time demands and diverse
resource requirements. Our findings emphasize the need for a systematic
methodology that selects server-specific predictors by jointly optimizing
accuracy and inference latency in dynamic co-location scenarios. Integrating
such predictors into edge environments can improve resource utilization and
result in predictable performance.

</details>


### [55] [Morpheus: Lightweight RTT Prediction for Performance-Aware Load Balancing](https://arxiv.org/abs/2510.20506)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 本文研究用RTT预测器改进请求路由，开发轻量级且准确的预测器，在Kubernetes管理的GPU集群上训练，评估表明性能感知负载均衡可减少应用RTT并减少资源浪费。


<details>
  <summary>Details</summary>
Motivation: 分布式应用对低端到端延迟需求增加，传统负载均衡策略有缺陷，常导致次优路由决策和尾延迟增加。

Method: 开发基于时间序列监控数据训练的轻量级准确RTT预测器，利用高相关性监控指标，在Kubernetes管理的GPU集群上训练。

Result: 预测器准确率达95%，预测延迟在应用RTT的10%以内，确定了最小预测准确率阈值和关键系统级因素，性能感知负载均衡可显著减少应用RTT和资源浪费。

Conclusion: 将预测性负载均衡集成到未来生产系统是可行的。

Abstract: Distributed applications increasingly demand low end-to-end latency,
especially in edge and cloud environments where co-located workloads contend
for limited resources. Traditional load-balancing strategies are typically
reactive and rely on outdated or coarse-grained metrics, often leading to
suboptimal routing decisions and increased tail latencies. This paper
investigates the use of round-trip time (RTT) predictors to enhance request
routing by anticipating application latency. We develop lightweight and
accurate RTT predictors that are trained on time-series monitoring data
collected from a Kubernetes-managed GPU cluster. By leveraging a reduced set of
highly correlated monitoring metrics, our approach maintains low overhead while
remaining adaptable to diverse co-location scenarios and heterogeneous
hardware. The predictors achieve up to 95% accuracy while keeping the
prediction delay within 10% of the application RTT. In addition, we identify
the minimum prediction accuracy threshold and key system-level factors required
to ensure effective predictor deployment in resource-constrained clusters.
Simulation-based evaluation demonstrates that performance-aware load balancing
can significantly reduce application RTT and minimize resource waste. These
results highlight the feasibility of integrating predictive load balancing into
future production systems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [56] [On Hardness and Approximation of Broadcasting in Sparse Graphs](https://arxiv.org/abs/2510.20026)
*Jeffrey Bringolf,Hovhannes A. Harutyunyan,Shahin Kamali,Seyed-Mohammad Seyed-Javadi*

Main category: cs.DS

TL;DR: 研究稀疏图中电话广播问题，证明k - 循环图和k - 路径图的NP难性，给出PTAS，发现有界带宽图可多项式时间求解。


<details>
  <summary>Details</summary>
Motivation: 此前该问题在一般图中NP难，在特定受限图类也NP难，研究其在几种稀疏图族的计算复杂度。

Method: 理论证明NP难性，设计多项式时间近似方案，分析图结构特性。

Result: 证明k - 循环图和k - 路径图的NP难性，给出这两类图的PTAS，有界带宽图可多项式时间求解。

Conclusion: 回答了关于k - 循环图和k - 路径图复杂度的公开问题，改进近似因子，发现可处理性的结构边界。

Abstract: We study the Telephone Broadcasting problem in sparse graphs. Given a
designated source in an undirected graph, the task is to disseminate a message
to all vertices in the minimum number of rounds, where in each round every
informed vertex may inform at most one uninformed neighbor. For general graphs
with $n$ vertices, the problem is NP-hard. Recent work shows that the problem
remains NP-hard even on restricted graph classes such as cactus graphs of
pathwidth $2$ [Aminian et al., ICALP 2025] and graphs at distance-1 to a path
forest [Egami et al., MFCS 2025].
  In this work, we investigate the problem in several sparse graph families. We
first prove NP-hardness for $k$-cycle graphs, namely graphs formed by $k$
cycles sharing a single vertex, as well as $k$-path graphs, namely graphs
formed by $k$ paths with shared endpoints. Despite multiple efforts to
understand the problem in these simple graph families, the computational
complexity of the problem had remained unsettled, and our hardness results
answer open questions by Bhabak and Harutyunyan [CALDAM 2015] and Harutyunyan
and Hovhannisyan [COCAO 2023] concerning the problem's complexity in $k$-cycle
and $k$-path graphs, respectively.
  On the positive side, we present Polynomial-Time Approximation Schemes
(PTASs) for $k$-cycle and $k$-path graphs, improving over the best existing
approximation factors of $2$ for $k$-cycle graphs and an approximation factor
of $4$ for $k$-path graphs. Moreover, we identify a structural frontier for
tractability by showing that the problem is solvable in polynomial time on
graphs of bounded bandwidth. This result generalizes existing tractability
results for special sparse families such as necklace graphs.

</details>


### [57] [Parallel Joinable B-Trees in the Fork-Join I/O Model](https://arxiv.org/abs/2510.20053)
*Michael Goodrich,Yan Gu,Ryuto Kitagawa,Yihan Sun*

Main category: cs.DS

TL;DR: 本文研究基于连接框架的搜索树集操作并行算法，引入新计算模型衡量 I/O 成本，提出基于 B 树的新算法并给出 I/O 工作和跨度的复杂度。


<details>
  <summary>Details</summary>
Motivation: 以往基于并行连接的树数据结构在 I/O 访问模式上效率低，且缺乏对 I/O 成本的界定，本文旨在实现此类算法的 I/O 效率。

Method: 引入 Fork - Join I/O Model 计算模型衡量并行中的 I/O 成本，提出基于 B 树的并行算法。

Result: 并行算法计算两个 B 树的并、交、差操作，I/O 工作为 $O(m \log_B(n/m))$，I/O 跨度为 $O(\log_B m \cdot \log_2 \log_B n + \log_B n)$。

Conclusion: 新的计算模型和基于 B 树的并行算法能有效衡量和实现搜索树集操作的 I/O 效率。

Abstract: Balanced search trees are widely used in computer science to efficiently
maintain dynamic ordered data. To support efficient set operations (e.g.,
union, intersection, difference) using trees, the join-based framework is
widely studied. This framework has received particular attention in the
parallel setting, and has been shown to be effective in enabling simple and
theoretically efficient set operations on trees. Despite the widespread
adoption of parallel join-based trees, a major drawback of previous work on
such data structures is the inefficiency of their input/output (I/O) access
patterns. Some recent work (e.g., C-trees and PaC-trees) focused on more
I/O-friendly implementations of these algorithms. Surprisingly, however, there
have been no results on bounding the I/O-costs for these algorithms. It remains
open whether these algorithms can provide tight, provable guarantees in
I/O-costs on trees.
  This paper studies efficient parallel algorithms for set operations based on
search tree algorithms using a join-based framework, with a special focus on
achieving I/O efficiency in these algorithms. To better capture the
I/O-efficiency in these algorithms in parallel, we introduce a new
computational model, Fork-Join I/O Model, to measure the I/O costs in fork-join
parallelism. This model measures the total block transfers (I/O work) and their
critical path (I/O span). Under this model, we propose our new solution based
on B-trees. Our parallel algorithm computes the union, intersection, and
difference of two B-trees with $O(m \log_B(n/m))$ I/O work and $O(\log_B m
\cdot \log_2 \log_B n + \log_B n)$ I/O span, where $n$ and $m \leq n$ are the
sizes of the two trees, and $B$ is the block size.

</details>


### [58] [Optimal Rounding for Two-Stage Bipartite Matching](https://arxiv.org/abs/2510.20153)
*Tristan Pollner,Amin Saberi,Anders Wikum*

Main category: cs.DS

TL;DR: 研究两阶段二分图匹配问题，设计多项式时间近似算法，取得7/8（点加权图）和2√2 - 2（边加权图）的近似比，结果通过两阶段分数匹配舍入算法获得并扩展到仅样本访问分布的设置。


<details>
  <summary>Details</summary>
Motivation: 解决两阶段二分图匹配中最大化组合匹配总权重的问题，改进已有近似比。

Method: 设计两阶段分数匹配舍入算法，利用离线节点可用性的负相关性推导随机图最大权重匹配期望大小的下界。

Result: 对于点加权图和边加权图分别取得7/8和2√2 - 2的近似比，改进了已知的0.767近似比，且在仅样本访问分布设置中有相应结果。

Conclusion: 设计的算法有效，近似比匹配自然分数松弛的积分间隙上界，可扩展到样本访问分布设置。

Abstract: We study two-stage bipartite matching, in which the edges of a bipartite
graph on vertices $(B_1 \cup B_2, I)$ are revealed in two batches. In stage
one, a matching must be selected from among revealed edges $E \subseteq B_1
\times I$. In stage two, edges $E^\theta \subseteq B_2 \times I$ are sampled
from a known distribution, and a second matching must be selected between $B_2$
and unmatched vertices in $I$. The objective is to maximize the total weight of
the combined matching. We design polynomial-time approximations to the optimum
online algorithm, achieving guarantees of $7/8$ for vertex-weighted graphs and
$2\sqrt{2}-2 \approx 0.828$ for edge-weighted graphs under arbitrary
distributions. Both approximation ratios match known upper bounds on the
integrality gap of the natural fractional relaxation, improving upon the
best-known approximation of 0.767 by Feng, Niazadeh, and Saberi for unweighted
graphs whose second batch consists of independently arriving nodes.
  Our results are obtained via an algorithm that rounds a fractional matching
revealed in two stages, aiming to match offline nodes (respectively, edges)
with probability proportional to their fractional weights, up to a
constant-factor loss. We leverage negative association (NA) among offline node
availabilities -- a property induced by dependent rounding -- to derive new
lower bounds on the expected size of the maximum weight matching in random
graphs where one side is realized via NA binary random variables. Moreover, we
extend these results to settings where we have only sample access to the
distribution. In particular, $\text{poly}(n,\epsilon^{-1})$ samples suffice to
obtain an additive loss of $\epsilon$ in the approximation ratio for the
vertex-weighted problem; a similar bound holds for the edge-weighted problem
with an additional (unavoidable) dependence on the scale of edge weights.

</details>


### [59] [Smoothed Analysis of Online Metric Matching with a Single Sample: Beyond Metric Distortion](https://arxiv.org/abs/2510.20288)
*Yingxi Li,Ellen Vitercik,Mingwei Yang*

Main category: cs.DS

TL;DR: 研究欧几里得度量空间在线度量匹配问题，提出 d≠2 时无需分布知识的 O(1) 竞争算法。


<details>
  <summary>Details</summary>
Motivation: 解决在线度量匹配问题中最小化总匹配成本，在服务器对抗、请求独立分布条件下寻找高效算法。

Method: 绕过概率度量嵌入的 Ω(log n) 障碍，直接在简单确定性嵌入目标度量上界定算法成本，结合欧几里得度量离线最优下限分析。

Result: 得到 d≠2 时无需分布知识、仅依赖每个请求分布单样本的 O(1) 竞争算法。

Conclusion: 此为非独立同分布设置下非平凡度量中首个达到 o(log n) 竞争比的算法。

Abstract: In the online metric matching problem, $n$ servers and $n$ requests lie in a
metric space. Servers are available upfront, and requests arrive sequentially.
An arriving request must be matched immediately and irrevocably to an available
server, incurring a cost equal to their distance. The goal is to minimize the
total matching cost.
  We study this problem in the Euclidean metric $[0, 1]^d$, when servers are
adversarial and requests are independently drawn from distinct distributions
that satisfy a mild smoothness condition. Our main result is an
$O(1)$-competitive algorithm for $d \neq 2$ that requires no distributional
knowledge, relying only on a single sample from each request distribution. To
our knowledge, this is the first algorithm to achieve an $o(\log n)$
competitive ratio for non-trivial metrics beyond the i.i.d. setting. Our
approach bypasses the $\Omega(\log n)$ barrier introduced by probabilistic
metric embeddings: instead of analyzing the embedding distortion and the
algorithm separately, we directly bound the cost of the algorithm on the target
metric of a simple deterministic embedding. We then combine this analysis with
lower bounds on the offline optimum for Euclidean metrics, derived via
majorization arguments, to obtain our guarantees.

</details>


### [60] [Separations between Oblivious and Adaptive Adversaries for Natural Dynamic Graph Problems](https://arxiv.org/abs/2510.20341)
*Aaron Bernstein,Sayan Bhattacharya,Nick Fischer,Peter Kiss,Thatchaphol Saranurak*

Main category: cs.DS

TL;DR: 基于流行细粒度复杂性假设，在自然动态图问题中建立动态算法对抗健忘对手和自适应对手的更新时间分离，还给出三角形检测问题增量和减量算法的分离。


<details>
  <summary>Details</summary>
Motivation: 以往动态算法的分离要么针对人为构造问题且依赖强密码学假设，要么针对输入非显式给出而是通过预言机调用访问的问题，本文旨在在自然动态图问题中建立分离。

Method: 基于组合 BMM 假设、3SUM 或 APSP 假设、OMv 假设进行分析和证明。

Result: 在相关假设下，增量最大独立集问题和减量最大团问题对抗自适应对手有相应的更新时间下界，且与现有算法匹配；三角形检测问题中减量算法和增量算法有不同的更新时间表现。

Conclusion: 建立了动态算法对抗不同对手的指数级更新时间分离，以及三角形检测问题增量和减量算法的分离。

Abstract: We establish the first update-time separation between dynamic algorithms
against oblivious adversaries and those against adaptive adversaries in natural
dynamic graph problems, based on popular fine-grained complexity hypotheses.
Specifically, under the combinatorial BMM hypothesis, we show that every
combinatorial algorithm against an adaptive adversary for the incremental
maximal independent set problem requires $n^{1-o(1)}$ amortized update time.
Furthermore, assuming either the 3SUM or APSP hypotheses, every algorithm for
the decremental maximal clique problem needs $\Delta/n^{o(1)}$ amortized update
time when the initial maximum degree is $\Delta \le \sqrt{n}$. These lower
bounds are matched by existing algorithms against adaptive adversaries. In
contrast, both problems admit algorithms against oblivious adversaries that
achieve $\operatorname{polylog}(n)$ amortized update time [Behnezhad,
Derakhshan, Hajiaghayi, Stein, Sudan; FOCS '19] [Chechik, Zhang; FOCS '19].
Therefore, our separations are exponential. Previously known separations for
dynamic algorithms were either engineered for contrived problems and relied on
strong cryptographic assumptions [Beimel, Kaplan, Mansour, Nissim, Saranurak,
Stemmer; STOC '22], or worked for problems whose inputs are not explicitly
given but are accessed through oracle calls [Bateni, Esfandiari, Fichtenberger,
Henzinger, Jayaram, Mirrokni, Wiese; SODA '23].
  As a byproduct, we also provide a separation between incremental and
decremental algorithms for the triangle detection problem: we show a
decremental algorithm with $\tilde{O}(n^{\omega})$ total update time, while
every incremental algorithm requires $n^{3-o(1)}$ total update time, assuming
the OMv hypothesis. To our knowledge this is the first separation of this kind.

</details>


### [61] [$\ell_2/\ell_2$ Sparse Recovery via Weighted Hypergraph Peeling](https://arxiv.org/abs/2510.20361)
*Nick Fischer,Vasileios Nakos*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We demonstrate that the best $k$-sparse approximation of a length-$n$ vector
can be recovered within a $(1+\epsilon)$-factor approximation in
$O((k/\epsilon) \log n)$ time using a non-adaptive linear sketch with
$O((k/\epsilon) \log n)$ rows and $O(\log n)$ column sparsity. This improves
the running time of the fastest-known sketch [Nakos, Song; STOC '19] by a
factor of $\log n$, and is optimal for a wide range of parameters.
  Our algorithm is simple and likely to be practical, with the analysis built
on a new technique we call weighted hypergraph peeling. Our method naturally
extends known hypergraph peeling processes (as in the analysis of Invertible
Bloom Filters) to a setting where edges and nodes have (possibly correlated)
weights.

</details>


### [62] [From Incremental Transitive Cover to Strongly Polynomial Maximum Flow](https://arxiv.org/abs/2510.20368)
*Daniel Dadush,James B. Orlin,Aaron Sidford,László A. Végh*

Main category: cs.DS

TL;DR: 本文提供了在结构化网络中求解最大流的更快强多项式时间算法，还得到了计算最大二分b - 匹配的算法和在给定树分解图上解决问题的算法。


<details>
  <summary>Details</summary>
Motivation: 为结构化n节点m弧网络中的最大流问题提供更快的强多项式时间算法。

Method: 强化并高效实现Orlin的算法，开发通用框架将任意容量的最大流问题转化为多项式有界容量的最大流问题序列和增量传递覆盖问题，利用近期的弱多项式、近似线性时间最大流算法和开发增量传递覆盖数据结构。

Result: 得到了n^(ω + o(1))时间的计算最大二分b - 匹配的强多项式时间算法和m^(1 + o(1))W时间的在给定树分解宽度为W的图上解决问题的算法。

Conclusion: 通过改进现有算法和开发新框架，可以得到更快的结构化网络最大流问题求解算法。

Abstract: We provide faster strongly polynomial time algorithms solving maximum flow in
structured $n$-node $m$-arc networks. Our results imply an $n^{\omega +
o(1)}$-time strongly polynomial time algorithms for computing a maximum
bipartite $b$-matching where $\omega$ is the matrix multiplication constant.
Additionally, they imply an $m^{1 + o(1)} W$-time algorithm for solving the
problem on graphs with a given tree decomposition of width $W$.
  We obtain these results by strengthening and efficiently implementing an
approach in Orlin's (STOC 2013) state-of-the-art $O(mn)$ time maximum flow
algorithm. We develop a general framework that reduces solving maximum flow
with arbitrary capacities to (1) solving a sequence of maximum flow problems
with polynomial bounded capacities and (2) dynamically maintaining a
size-bounded supersets of the transitive closure under arc additions; we call
this problem \emph{incremental transitive cover}. Our applications follow by
leveraging recent weakly polynomial, almost linear time algorithms for maximum
flow due to Chen, Kyng, Liu, Peng, Gutenberg, Sachdeva (FOCS 2022) and Brand,
Chen, Kyng, Liu, Peng, Gutenberg, Sachdeva, Sidford (FOCS 2023), and by
developing incremental transitive cover data structures.

</details>


### [63] [Compact representations of pattern-avoiding permutations](https://arxiv.org/abs/2510.20382)
*László Kozma,Michal Opler*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Pattern-avoiding permutations are a central object of study in both
combinatorics and theoretical computer science. In this paper we design a data
structure that can store any size-$n$ permutation $\tau$ that avoids an
arbitrary (and unknown) fixed pattern $\pi$ in the asymptotically optimal $O(n
\lg{s_\pi})$ bits, where $s_\pi$ is the Stanley-Wilf limit of $\pi$. Our data
structure supports $\tau(i)$ and $\tau^{-1}(i)$ queries in $O(1)$ time,
sidestepping the lower bound of Golynski (SODA 2009) that holds for general
permutations. Comparable results were previously known only in more restricted
cases, e.g., when $\tau$ is separable, which means avoiding the patterns 2413
and 3142.
  We also extend our data structure to support more complex geometric queries
on pattern-avoiding permutations (or planar point sets) such as rectangle range
counting in $O(\lg\lg{n})$ time. This result circumvents the lower bound of
$\Omega{(\lg{n}/\lg\lg{n})}$ by P\u{a}tra\c{s}cu (STOC 2007) that holds in the
general case. For bounded treewidth permutation classes (which include the
above-mentioned separable class), we further reduce the space overhead to a
lower order additive term, making our data structure succinct. This extends and
improves results of Chakraborty et al. (ISAAC 2024) that were obtained for
separable permutations via different techniques. All our data structures can be
constructed in linear time.

</details>


### [64] [Parallel $(1+ε)$-Approximate Multi-Commodity Mincost Flow in Almost Optimal Depth and Work](https://arxiv.org/abs/2510.20456)
*Bernhard Haeupler,Yonggang Jiang,Yaowei Long,Thatchaphol Saranurak,Shengzhe Wang*

Main category: cs.DS

TL;DR: 提出无向图上计算(1+ε)-近似最小成本流的并行算法，工作和深度近乎最优，推广先前算法，还能解决k-商品最小成本流问题。


<details>
  <summary>Details</summary>
Motivation: 改进先前算法在工作和深度上的不足，推广特殊情况的近似算法，处理带顶点容量的问题。

Method: 构建长度受限的流捷径，开发接近线性时间的长度受限顶点扩展器分解算法，基于Cohen的路径计数流思想扩展算法。

Result: 算法在ε > 1/polylog(m)时实现Ô(m)工作和Ô(1)深度，可解决(1+ε)-近似k-商品最小成本流问题，工作为Ô(mk)，深度为Ô(1)。

Conclusion: 该算法在工作和深度上近乎最优，推广了先前算法，能有效解决相关问题。

Abstract: We present a parallel algorithm for computing $(1+\epsilon)$-approximate
mincost flow on an undirected graph with $m$ edges, where capacities and costs
are assigned to both edges and vertices. Our algorithm achieves $\hat{O}(m)$
work and $\hat{O}(1)$ depth when $\epsilon > 1/\mathrm{polylog}(m)$, making
both the work and depth almost optimal, up to a subpolynomial factor.
  Previous algorithms with $\hat{O}(m)$ work required $\Omega(m)$ depth, even
for special cases of mincost flow with only edge capacities or max flow with
vertex capacities. Our result generalizes prior almost-optimal parallel
$(1+\epsilon)$-approximation algorithms for these special cases, including
shortest paths [Li, STOC'20] [Andoni, Stein, Zhong, STOC'20] [Rozhen, Haeupler,
Marinsson, Grunau, Zuzic, STOC'23] and max flow with only edge capacities
[Agarwal, Khanna, Li, Patil, Wang, White, Zhong, SODA'24].
  Our key technical contribution is the first construction of
length-constrained flow shortcuts with $(1+\epsilon)$ length slack,
$\hat{O}(1)$ congestion slack, and $\hat{O}(1)$ step bound. This provides a
strict generalization of the influential concept of
$(\hat{O}(1),\epsilon)$-hopsets [Cohen, JACM'00], allowing for additional
control over congestion. Previous length-constrained flow shortcuts [Haeupler,
Hershkowitz, Li, Roeyskoe, Saranurak, STOC'24] incur a large constant in the
length slack, which would lead to a large approximation factor. To enable our
flow algorithms to work under vertex capacities, we also develop a
close-to-linear time algorithm for computing length-constrained vertex expander
decomposition.
  Building on Cohen's idea of path-count flows [Cohen, SICOMP'95], we further
extend our algorithm to solve $(1+\epsilon)$-approximate $k$-commodity mincost
flow problems with almost-optimal $\hat{O}(mk)$ work and $\hat{O}(1)$ depth,
independent of the number of commodities $k$.

</details>


### [65] [Provably Small Portfolios for Multiobjective Optimization with Application to Subsidized Facility Location](https://arxiv.org/abs/2510.20555)
*Swati Gupta,Jai Moondra,Mohit Singh*

Main category: cs.DS

TL;DR: 本文提出投资组合概念解决多利益相关者多目标优化问题，给出构建算法并应用于公平补贴设施选址问题取得成效。


<details>
  <summary>Details</summary>
Motivation: 现实中多利益相关者的多目标问题复杂，传统方法建模得到的解结构差异大，需要新方法帮助决策者理解多目标平衡影响；此外受药房关闭导致医疗荒漠危机启发研究公平补贴设施选址问题。

Method: 采用以解为中心的方法，引入投资组合概念；针对有限基本目标集，给出构建投资组合的可证明算法；针对公平补贴设施选址问题开发双准则近似算法。

Result: 针对特定目标函数类构建投资组合；在公平补贴设施选址问题中显著减少美国各州医疗荒漠情况。

Conclusion: 投资组合概念及相关算法能有效解决多目标优化问题，在公平补贴设施选址问题中有实际应用价值。

Abstract: Many multiobjective real-world problems, such as facility location and bus
routing, become more complex when optimizing the priorities of multiple
stakeholders. These are often modeled using infinite classes of objectives,
e.g., $L_p$ norms over group distances induced by feasible solutions in a fixed
domain. Traditionally, the literature has considered explicitly balancing
`equity' (or min-max) and `efficiency' (or min-sum) objectives to capture this
trade-off. However, the structure of solutions obtained by such modeling
choices can be very different. Taking a solution-centric approach, we introduce
the concept of provably small set of solutions $P$, called a {\it portfolio},
such that for every objective function $h(\cdot)$ in the given class
$\mathbf{C}$, there exists some solution in $P$ which is an
$\alpha$-approximation for $h(\cdot)$. Constructing such portfolios can help
decision-makers understand the impact of balancing across multiple objectives.
  Given a finite set of base objectives $h_1, \ldots, h_N$, we give provable
algorithms for constructing portfolios for (1) the class of conic combinations
$\mathbf{C} = \{\sum_{j \in [N]}\lambda_j h_j: \lambda \ge 0\}$ and for (2) any
class $\mathbf{C}$ of functions that interpolates monotonically between the
min-sum efficiency objective (i.e., $h_1 + \ldots + h_N$) and the min-max
equity objective (i.e., $\max_{j \in [N]} h_j$). Examples of the latter are
$L_p$ norms and top-$\ell$ norms. As an application, we study the Fair
Subsidized Facility Location (FSFL) problem, motivated by the crisis of medical
deserts caused due to pharmacy closures. FSFL allows subsidizing facilities in
underserved areas using revenue from profitable locations. We develop a novel
bicriteria approximation algorithm and show a significant reduction of medical
deserts across states in the U.S.

</details>


### [66] [A Deterministic Polylogarithmic Competitive Algorithm for Matching with Delays](https://arxiv.org/abs/2510.20588)
*Marc Dufay,Roger Wattenhofer*

Main category: cs.DS

TL;DR: 提出在线Min - cost Perfect Matching with Delays (MPMD)问题的O(log^5 m)竞争比确定性算法，无需提前知晓度量空间或m，相比之前结果有指数级提升。


<details>
  <summary>Details</summary>
Motivation: 已有算法在度量空间无限或未知时竞争力不理想，Azar和Jacob - Fanani算法竞争力远差于已知下界。

Method: 设计了一种确定性算法，无需提前知道度量空间和m。

Result: 得到O(log^5 m)竞争比的算法，较之前结果有指数级改进。

Conclusion: 新算法是对之前结果的指数级提升，且与下界仅差一个多对数因子。

Abstract: In the online Min-cost Perfect Matching with Delays (MPMD) problem, $m$
requests in a metric space are submitted at different times by an adversary.
The goal is to match all requests while (i) minimizing the sum of the distances
between matched pairs as well as (ii) how long each request remained unmatched
after it appeared.
  While there exist almost optimal algorithms when the metric space is finite
and known a priori, this is not the case when the metric space is infinite or
unknown. In this latter case, the best known algorithm, due to Azar and
Jacob-Fanani, has competitiveness $\mathcal{O}(m^{0.59})$ which is
exponentially worse than the best known lower bound of $\Omega(\log m / \log
\log m)$ by Ashlagi et al.
  We present a $\mathcal{O}(\log^5 m)$-competitive algorithm for the MPMD
problem. This algorithm is deterministic and does not need to know the metric
space or $m$ in advance. This is an exponential improvement over previous
results and only a polylogarithmic factor away from the lower bound.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [67] [Optimized Distortion in Linear Social Choice](https://arxiv.org/abs/2510.20020)
*Luise Ge,Gregory Kehne,Yevgeniy Vorobeychik*

Main category: cs.GT

TL;DR: 本文首次研究线性效用函数的失真问题，分析确定性和随机投票规则的线性社会选择失真，得到仅依赖候选嵌入维度的界限，并引入多项式时间实例最优算法，在两个现实领域进行实证评估。


<details>
  <summary>Details</summary>
Motivation: 在选民有潜在效用时，使用偏好排名可能导致次优结果，现有失真研究在效用有最小结构时展开，而在很多场景中效用是参数化函数，因此研究线性效用函数的失真问题。

Method: 研究确定性和随机投票规则的线性社会选择失真，推导依赖候选嵌入维度的界限，引入多项式时间实例最优算法，并在推荐系统和意见调查两个现实领域进行实证评估。

Result: 得到仅依赖候选嵌入维度、与候选人和选民数量无关的界限，在两个现实领域进行实证评估。

Conclusion: 对线性效用函数的失真问题进行了首次研究，引入的实例最优算法可用于最小化失真。

Abstract: Social choice theory offers a wealth of approaches for selecting a candidate
on behalf of voters based on their reported preference rankings over options.
When voters have underlying utilities for these options, however, using
preference rankings may lead to suboptimal outcomes vis-\`a-vis utilitarian
social welfare. Distortion is a measure of this suboptimality, and provides a
worst-case approach for developing and analyzing voting rules when utilities
have minimal structure. However in many settings, such as common paradigms for
value alignment, alternatives admit a vector representation, and it is natural
to suppose that utilities are parametric functions thereof. We undertake the
first study of distortion for linear utility functions. Specifically, we
investigate the distortion of linear social choice for deterministic and
randomized voting rules. We obtain bounds that depend only on the dimension of
the candidate embedding, and are independent of the numbers of candidates or
voters. Additionally, we introduce poly-time instance-optimal algorithms for
minimizing distortion given a collection of candidates and votes. We
empirically evaluate these in two real-world domains: recommendation systems
using collaborative filtering embeddings, and opinion surveys utilizing
language model embeddings, benchmarking several standard rules against our
instance-optimal algorithms.

</details>


### [68] [Strategic Costs of Perceived Bias in Fair Selection](https://arxiv.org/abs/2510.20606)
*L. Elisa Celis,Lingxiao Huang,Milind Sohoni,Nisheeth K. Vishnoi*

Main category: cs.GT

TL;DR: 本文构建博弈论模型分析精英制度中不同社会经济群体间的差距，揭示感知驱动的偏差，并提出优化框架以减少差距。


<details>
  <summary>Details</summary>
Motivation: 精英制度虽旨在公正奖励技能和努力，但种族、性别和阶层的持续差距挑战这一理想，需探究差距原因。

Method: 开发博弈论模型，刻画大代理人极限下的唯一纳什均衡，推导明确公式，提出成本敏感优化框架。

Result: 发现感知驱动的偏差，即群体间对选拔后价值的感知差异会转化为努力的理性差异，通过“公平”选拔过程传播差距。

Conclusion: 模型虽静态，但捕捉了感知、激励和结果反馈循环的一个阶段，弥合理性选择和结构不平等解释间的差距，展示技术社会环境如何塑造精英制度中的个人激励。

Abstract: Meritocratic systems, from admissions to hiring, aim to impartially reward
skill and effort. Yet persistent disparities across race, gender, and class
challenge this ideal. Some attribute these gaps to structural inequality;
others to individual choice. We develop a game-theoretic model in which
candidates from different socioeconomic groups differ in their perceived
post-selection value--shaped by social context and, increasingly, by AI-powered
tools offering personalized career or salary guidance. Each candidate
strategically chooses effort, balancing its cost against expected reward;
effort translates into observable merit, and selection is based solely on
merit. We characterize the unique Nash equilibrium in the large-agent limit and
derive explicit formulas showing how valuation disparities and institutional
selectivity jointly determine effort, representation, social welfare, and
utility. We further propose a cost-sensitive optimization framework that
quantifies how modifying selectivity or perceived value can reduce disparities
without compromising institutional goals. Our analysis reveals a
perception-driven bias: when perceptions of post-selection value differ across
groups, these differences translate into rational differences in effort,
propagating disparities backward through otherwise "fair" selection processes.
While the model is static, it captures one stage of a broader feedback cycle
linking perceptions, incentives, and outcome--bridging rational-choice and
structural explanations of inequality by showing how techno-social environments
shape individual incentives in meritocratic systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [69] [Automating Iconclass: LLMs and RAG for Large-Scale Classification of Religious Woodcuts](https://arxiv.org/abs/2510.19986)
*Drew B. Thomas*

Main category: cs.IR

TL;DR: 本文提出结合大语言模型、向量数据库和检索增强生成（RAG）对早期现代宗教图像分类的新方法，精度高，展现跨学科潜力。


<details>
  <summary>Details</summary>
Motivation: 寻找更有效的早期现代宗教图像分类方法，推动艺术史和数字人文学科研究。

Method: 结合大语言模型、向量数据库和RAG，利用书籍插图全页上下文生成含视觉和文本元素的描述，通过混合向量搜索匹配图标分类代码。

Result: 在五级和四级分类中分别达到87%和92%的精度，远超传统图像和基于关键词的搜索。

Conclusion: 采用全页描述和RAG的系统提高了分类准确性，展示了大语言模型和RAG在艺术史和数字人文学科研究中的潜力。

Abstract: This paper presents a novel methodology for classifying early modern
religious images by using Large Language Models (LLMs) and vector databases in
combination with Retrieval-Augmented Generation (RAG). The approach leverages
the full-page context of book illustrations from the Holy Roman Empire,
allowing the LLM to generate detailed descriptions that incorporate both visual
and textual elements. These descriptions are then matched to relevant Iconclass
codes through a hybrid vector search. This method achieves 87% and 92%
precision at five and four levels of classification, significantly
outperforming traditional image and keyword-based searches. By employing
full-page descriptions and RAG, the system enhances classification accuracy,
offering a powerful tool for large-scale analysis of early modern visual
archives. This interdisciplinary approach demonstrates the growing potential of
LLMs and RAG in advancing research within art history and digital humanities.

</details>


### [70] [Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning](https://arxiv.org/abs/2510.20150)
*Yaochen Zhu,Harald Steck,Dawen Liang,Yinhan He,Jundong Li,Nathan Kallus*

Main category: cs.IR

TL;DR: 提出ConvRec - R1框架用于端到端训练基于大语言模型的对话推荐系统，实验显示其比GRPO基线收敛更快且指标更好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用于推荐系统存在生成非目录项、违反输出格式、排名质量下降等问题，需进行对齐。

Method: 提出两阶段框架ConvRec - R1，第一阶段用Remap - Reflect - Adjust管道构建行为克隆数据集；第二阶段提出Rank - GRPO优化方法。

Result: 在Reddit - v2数据集上，ConvRec - R1比GRPO风格基线收敛更快，Recall和NDCG指标更高。

Conclusion: ConvRec - R1框架有效，代码和数据集已开源。

Abstract: Large language models (LLMs) are reshaping the recommender system paradigm by
enabling users to express preferences and receive recommendations through
conversations. Yet, aligning LLMs to the recommendation task remains
challenging: pretrained LLMs often generate out-of-catalog items, violate
required output formats, and their ranking quality degrades sharply toward the
end of the generated list. To this end, we propose ConvRec-R1, a two-stage
framework for end-to-end training of LLM-based conversational recommender
systems. In Stage 1, we construct a behavioral-cloning dataset with a
Remap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded
demonstrations from powerful blackbox LLMs to warm-start the RL training. In
Stage 2, we propose Rank-GRPO, a principled extension of group relative policy
optimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats
each rank in the recommendation list as the unit instead of token (too
fine-grained) or sequence (too coarse), redefining rewards to remove non-causal
credit assignment and introducing a rank-level importance ratio based on the
geometric mean of rank-wise token probabilities to stabilize policy updates.
Experiments on the public Reddit-v2 dataset show that ConvRec-R1 converges
faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and
datasets are released at https://github.com/yaochenzhu/Rank-GRPO.

</details>


### [71] [Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures](https://arxiv.org/abs/2510.20193)
*Rahul Raja,Arpita Vats*

Main category: cs.IR

TL;DR: 本文综述多媒体检索增强问答系统进展，分类方法、分析数据集与评估协议，指出挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 多媒体内容增长给检索增强问答带来新挑战和机遇，需研究多媒体检索增强问答系统。

Method: 对整合多媒体检索流程的问答系统进行综述，按检索方法、融合技术和答案生成策略分类。

Result: 分析了基准数据集、评估协议和性能权衡。

Conclusion: 指出跨模态对齐、延迟 - 准确性权衡和语义基础等挑战，给出开放问题和未来研究方向。

Abstract: Question Answering (QA) systems have traditionally relied on structured text
data, but the rapid growth of multimedia content (images, audio, video, and
structured metadata) has introduced new challenges and opportunities for
retrieval-augmented QA. In this survey, we review recent advancements in QA
systems that integrate multimedia retrieval pipelines, focusing on
architectures that align vision, language, and audio modalities with user
queries. We categorize approaches based on retrieval methods, fusion
techniques, and answer generation strategies, and analyze benchmark datasets,
evaluation protocols, and performance tradeoffs. Furthermore, we highlight key
challenges such as cross-modal alignment, latency-accuracy tradeoffs, and
semantic grounding, and outline open problems and future research directions
for building more robust and context-aware QA systems leveraging multimedia
data.

</details>


### [72] [Balancing Fine-tuning and RAG: A Hybrid Strategy for Dynamic LLM Recommendation Updates](https://arxiv.org/abs/2510.20260)
*Changping Meng,Hongyi Ling,Jianling Wang,Yifan Liu,Shuzhou Zhang,Dapeng Hong,Mingyan Gao,Onkar Dalal,Ed Chi,Lichan Hong,Haokai Lu,Ningren Han*

Main category: cs.IR

TL;DR: 本文研究更新大语言模型驱动推荐系统的策略，对比持续微调与检索增强生成，提出混合更新策略，经实验验证能提升用户满意度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型助力推荐系统，但用户兴趣和内容动态变化，初始微调无法捕捉实时变化，需强大更新机制。

Method: 以大语言模型驱动的用户兴趣探索系统为案例，从成本、灵活性和知识融合等维度对比持续微调与检索增强生成，提出结合周期性微调与低成本检索增强生成的混合更新策略。

Result: 在十亿用户平台的A/B实验表明，混合策略在用户满意度上有显著提升。

Conclusion: 混合策略为维护高质量大语言模型驱动的推荐系统提供了实用且经济高效的框架。

Abstract: Large Language Models (LLMs) empower recommendation systems through their
advanced reasoning and planning capabilities. However, the dynamic nature of
user interests and content poses a significant challenge: While initial
fine-tuning aligns LLMs with domain knowledge and user preferences, it fails to
capture such real-time changes, necessitating robust update mechanisms. This
paper investigates strategies for updating LLM-powered recommenders, focusing
on the trade-offs between ongoing fine-tuning and Retrieval-Augmented
Generation (RAG). Using an LLM-powered user interest exploration system as a
case study, we perform a comparative analysis of these methods across
dimensions like cost, agility, and knowledge incorporation. We propose a hybrid
update strategy that leverages the long-term knowledge adaptation of periodic
fine-tuning with the agility of low-cost RAG. We demonstrate through live A/B
experiments on a billion-user platform that this hybrid approach yields
statistically significant improvements in user satisfaction, offering a
practical and cost-effective framework for maintaining high-quality LLM-powered
recommender systems.

</details>


### [73] [From Generation to Attribution: Music AI Agent Architectures for the Post-Streaming Era](https://arxiv.org/abs/2510.20276)
*Wonil Kim,Hyeongseok Wi,Seungsoon Park,Taejun Kim,Sangeun Keum,Keunhyoung Kim,Taewan Kim,Jongmin Jung,Taehyoung Kim,Gaetan Guerrero,Mael Le Goff,Julie Po,Dongjoo Moon,Juhan Nam,Jongpil Lee*

Main category: cs.IR

TL;DR: 生成式AI重塑音乐创作，但现有系统难以应对，提出基于内容的音乐AI代理架构，指向后流媒体范式。


<details>
  <summary>Details</summary>
Motivation: 生成式AI快速发展使音乐创作在归因、版权管理和经济模式方面存在结构缺口，现有流媒体系统无法应对AI驱动生产的规模和复杂性。

Method: 提出基于内容的音乐AI代理架构，通过块级检索和代理编排将归因嵌入创作流程，将音乐组织成存储在BlockDB的粒度组件，每次使用触发归因层事件。

Result: 该架构将AI从生成工具转变为公平AI媒体平台的基础设施。

Conclusion: 此架构可实现细粒度归因、公平补偿和参与式互动，指向音乐作为协作和自适应生态系统的后流媒体范式。

Abstract: Generative AI is reshaping music creation, but its rapid growth exposes
structural gaps in attribution, rights management, and economic models. Unlike
past media shifts, from live performance to recordings, downloads, and
streaming, AI transforms the entire lifecycle of music, collapsing boundaries
between creation, distribution, and monetization. However, existing streaming
systems, with opaque and concentrated royalty flows, are ill-equipped to handle
the scale and complexity of AI-driven production. We propose a content-based
Music AI Agent architecture that embeds attribution directly into the creative
workflow through block-level retrieval and agentic orchestration. Designed for
iterative, session-based interaction, the system organizes music into granular
components (Blocks) stored in BlockDB; each use triggers an Attribution Layer
event for transparent provenance and real-time settlement. This framework
reframes AI from a generative tool into infrastructure for a Fair AI Media
Platform. By enabling fine-grained attribution, equitable compensation, and
participatory engagement, it points toward a post-streaming paradigm where
music functions not as a static catalog but as a collaborative and adaptive
ecosystem.

</details>


### [74] [Rotate Both Ways: Time-and-Order RoPE for Generative Recommendation](https://arxiv.org/abs/2510.20455)
*Xiaokai Wei,Jiajun Wu,Daiyao Yi,Reza Shirkavand,Michelle Gong*

Main category: cs.IR

TL;DR: 本文提出TO - RoPE用于生成式推荐，在多个数据集实验中证明其优于现有方法，凸显旋转嵌入在生成式推荐中的优势。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐模型在处理用户行为序列的时间和顺序信息时，普遍方法有局限，需更优方案。

Method: 提出Time - and - Order RoPE (TO - RoPE)，有早期融合、按维度拆分、按头拆分三种实例化方式。

Result: 在公开数据集和专有工业数据集上实验，TO - RoPE变体在编码时间和索引方面准确率始终优于现有方法。

Conclusion: 旋转嵌入可作为生成式推荐简单、有原则且便于部署的基础。

Abstract: Generative recommenders, typically transformer-based autoregressive models,
predict the next item or action from a user's interaction history. Their
effectiveness depends on how the model represents where an interaction event
occurs in the sequence (discrete index) and when it occurred in wall-clock
time. Prevailing approaches inject time via learned embeddings or relative
attention biases. In this paper, we argue that RoPE-based approaches, if
designed properly, can be a stronger alternative for jointly modeling temporal
and sequential information in user behavior sequences. While vanilla RoPE in
LLMs considers only token order, generative recommendation requires
incorporating both event time and token index. To address this, we propose
Time-and-Order RoPE (TO-RoPE), a family of rotary position embedding designs
that treat index and time as angle sources shaping the query-key geometry
directly. We present three instantiations: early fusion, split-by-dim, and
split-by-head. Extensive experiments on both publicly available datasets and a
proprietary industrial dataset show that TO-RoPE variants consistently improve
accuracy over existing methods for encoding time and index. These results
position rotary embeddings as a simple, principled, and deployment-friendly
foundation for generative recommendation.

</details>


### [75] [Analyticup E-commerce Product Search Competition Technical Report from Team Tredence_AICOE](https://arxiv.org/abs/2510.20674)
*Rakshith R,Shubham Sharma,Mohammed Sameer Khan,Ankush Chopra*

Main category: cs.IR

TL;DR: Tredence_AICOE团队开发多语言电商搜索系统，参与两项多语言相关性任务，经数据增强和模型微调，获排行榜第4名，私有测试集平均F1分数0.8857。


<details>
  <summary>Details</summary>
Motivation: 开发多语言电商搜索系统，参与多语言相关性任务竞争。

Method: 对现有数据集进行翻译实现数据增强，用多种策略微调Gemma - 3 12B和Qwen - 2.5 14B模型。

Result: Gemma - 3 12B (4位)模型在两项任务表现最佳，获排行榜第4名，私有测试集平均F1分数0.8857。

Conclusion: 通过数据增强和模型微调的方法在多语言电商搜索系统的多语言相关性任务中取得较好成绩。

Abstract: This study presents the multilingual e-commerce search system developed by
the Tredence_AICOE team. The competition features two multilingual relevance
tasks: Query-Category (QC) Relevance, which evaluates how well a user's search
query aligns with a product category, and Query-Item (QI) Relevance, which
measures the match between a multilingual search query and an individual
product listing. To ensure full language coverage, we performed data
augmentation by translating existing datasets into languages missing from the
development set, enabling training across all target languages. We fine-tuned
Gemma-3 12B and Qwen-2.5 14B model for both tasks using multiple strategies.
The Gemma-3 12B (4-bit) model achieved the best QC performance using original
and translated data, and the best QI performance using original, translated,
and minority class data creation. These approaches secured 4th place on the
final leaderboard, with an average F1-score of 0.8857 on the private test set.

</details>


### [76] [Generative Reasoning Recommendation via LLMs](https://arxiv.org/abs/2510.20815)
*Minjie Hong,Zetong Zhou,Zirun Guo,Ziang Zhang,Ruofan Hu,Weinan Gan,Jieming Zhu,Zhou Zhao*

Main category: cs.IR

TL;DR: 本文探讨如何将预训练大语言模型（LLMs）适配为生成式推理推荐模型（GRRMs），提出GREAM框架，实验显示其优于强基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在作为生成式推理推荐模型时面临文本语义与协同过滤信号的建模差距、用户反馈的稀疏性和随机性等挑战，需要探索适配方法。

Method: 提出GREAM框架，包含协同语义对齐、推理课程激活、稀疏正则化组策略优化三个组件，支持直接序列推荐和顺序推理推荐两种推理模式。

Result: 在三个数据集上的实验显示，GREAM相比强基线有持续的性能提升。

Conclusion: GREAM为基于可验证强化学习的大语言模型推荐器提供了实用途径。

Abstract: Despite their remarkable reasoning capabilities across diverse domains, large
language models (LLMs) face fundamental challenges in natively functioning as
generative reasoning recommendation models (GRRMs), where the intrinsic
modeling gap between textual semantics and collaborative filtering signals,
combined with the sparsity and stochasticity of user feedback, presents
significant obstacles. This work explores how to build GRRMs by adapting
pre-trained LLMs, which achieves a unified understanding-reasoning-prediction
manner for recommendation tasks. We propose GREAM, an end-to-end framework that
integrates three components: (i) Collaborative-Semantic Alignment, which fuses
heterogeneous textual evidence to construct semantically consistent, discrete
item indices and auxiliary alignment tasks that ground linguistic
representations in interaction semantics; (ii) Reasoning Curriculum Activation,
which builds a synthetic dataset with explicit Chain-of-Thought supervision and
a curriculum that progresses through behavioral evidence extraction, latent
preference modeling, intent inference, recommendation formulation, and denoised
sequence rewriting; and (iii) Sparse-Regularized Group Policy Optimization
(SRPO), which stabilizes post-training via Residual-Sensitive Verifiable Reward
and Bonus-Calibrated Group Advantage Estimation, enabling end-to-end
optimization under verifiable signals despite sparse successes. GREAM natively
supports two complementary inference modes: Direct Sequence Recommendation for
high-throughput, low-latency deployment, and Sequential Reasoning
Recommendation that first emits an interpretable reasoning chain for causal
transparency. Experiments on three datasets demonstrate consistent gains over
strong baselines, providing a practical path toward verifiable-RL-driven LLM
recommenders.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [77] [A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers](https://arxiv.org/abs/2510.20066)
*Yimeng Qiu,Feihuang Fang*

Main category: cs.LG

TL;DR: 研究核心加密资产流动性和波动性指标对市场风险的溢出效应，构建三层实证框架，用多种方法分析，有显著格兰杰因果关系和一定预测精度。


<details>
  <summary>Details</summary>
Motivation: 探究核心加密资产的流动性和波动性指标是否会产生溢出效应以预测市场整体风险。

Method: 实证框架包含三层统计分析，辅以向量自回归脉冲响应、预测误差方差分解、HAR - X模型和机器学习协议。

Result: 发现各层存在显著的格兰杰因果关系，有适度的样本外预测准确性，并展示了相关重要数据和图表。

Conclusion: 核心加密资产的流动性和波动性指标对市场风险有一定预测作用。

Abstract: We study whether liquidity and volatility proxies of a core set of
cryptoassets generate spillovers that forecast market-wide risk. Our empirical
framework integrates three statistical layers: (A) interactions between core
liquidity and returns, (B) principal-component relations linking liquidity and
returns, and (C) volatility-factor projections that capture cross-sectional
volatility crowding. The analysis is complemented by vector autoregression
impulse responses and forecast error variance decompositions (see Granger 1969;
Sims 1980), heterogeneous autoregressive models with exogenous regressors
(HAR-X, Corsi 2009), and a leakage-safe machine learning protocol using
temporal splits, early stopping, validation-only thresholding, and SHAP-based
interpretation. Using daily data from 2021 to 2025 (1462 observations across 74
assets), we document statistically significant Granger-causal relationships
across layers and moderate out-of-sample predictive accuracy. We report the
most informative figures, including the pipeline overview, Layer A heatmap,
Layer C robustness analysis, vector autoregression variance decompositions, and
the test-set precision-recall curve. Full data and figure outputs are provided
in the artifact repository.

</details>


### [78] [Some Attention is All You Need for Retrieval](https://arxiv.org/abs/2510.19861)
*Felix Michalak,Steven Abreu*

Main category: cs.LG

TL;DR: 研究展示混合SSM - Transformer架构中完全的功能分离，自注意力层专用于检索，还确定检索的机制要求，挑战混合架构冗余假设。


<details>
  <summary>Details</summary>
Motivation: 探究混合SSM - Transformer架构中各组件功能，优化架构和增强可解释性。

Method: 对RecurrentGemma - 2B/9B和Jamba - Mini - 1.6进行注意力消融实验，对注意力进行稀疏化处理。

Result: 注意力消融导致检索失败，稀疏化部分注意力头仍能维持高检索性能和MMLU性能；确定检索需暴露针令牌及足够上下文。

Conclusion: 混合架构功能严格分离，各模块是专门化而非集成系统，对架构优化和可解释性有重要意义。

Abstract: We demonstrate complete functional segregation in hybrid SSM-Transformer
architectures: retrieval depends exclusively on self-attention layers. Across
RecurrentGemma-2B/9B and Jamba-Mini-1.6, attention ablation causes catastrophic
retrieval failure (0% accuracy), while SSM layers show no compensatory
mechanisms even with improved prompting. Conversely, sparsifying attention to
just 15% of heads maintains near-perfect retrieval while preserving 84% MMLU
performance, suggesting self-attention specializes primarily for retrieval
tasks. We identify precise mechanistic requirements for retrieval: needle
tokens must be exposed during generation and sufficient context must be
available during prefill or generation. This strict functional specialization
challenges assumptions about redundancy in hybrid architectures and suggests
these models operate as specialized modules rather than integrated systems,
with immediate implications for architecture optimization and interpretability.

</details>


### [79] [An Integrated Approach to Neural Architecture Search for Deep Q-Networks](https://arxiv.org/abs/2510.19872)
*Iman Rahmani,Saman Yazdannik,Morteza Tayefi,Jafar Roshanian*

Main category: cs.LG

TL;DR: 研究在线自适应架构优化能否突破深度强化学习架构限制，提出NAS - DQN，实验表明其性能优越，说明架构自适应对在线深度强化学习很必要。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习中神经网络架构选择通过昂贵超参数搜索且训练中固定，研究在线自适应架构优化能否突破限制并超越静态设计。

Method: 引入NAS - DQN，将学习的神经架构搜索控制器集成到DRL训练循环，基于累积性能反馈进行动态网络重配置，与三个固定架构基线和随机搜索对照在连续控制任务上评估，多随机种子实验。

Result: NAS - DQN最终性能、样本效率和策略稳定性优越，计算开销可忽略，学习的搜索策略远超无向随机架构探索和不良固定设计。

Conclusion: 架构自适应对在线深度强化学习最优样本效率不仅有益而且必要，RL智能体设计可作为学习过程动态部分。

Abstract: The performance of deep reinforcement learning agents is fundamentally
constrained by their neural network architecture, a choice traditionally made
through expensive hyperparameter searches and then fixed throughout training.
This work investigates whether online, adaptive architecture optimization can
escape this constraint and outperform static designs. We introduce NAS-DQN, an
agent that integrates a learned neural architecture search controller directly
into the DRL training loop, enabling dynamic network reconfiguration based on
cumulative performance feedback. We evaluate NAS-DQN against three
fixed-architecture baselines and a random search control on a continuous
control task, conducting experiments over multiple random seeds. Our results
demonstrate that NAS-DQN achieves superior final performance, sample
efficiency, and policy stability while incurring negligible computational
overhead. Critically, the learned search strategy substantially outperforms
both undirected random architecture exploration and poorly-chosen fixed
designs, indicating that intelligent, performance-guided search is the key
mechanism driving success. These findings establish that architecture
adaptation is not merely beneficial but necessary for optimal sample efficiency
in online deep reinforcement learning, and suggest that the design of RL agents
need not be a static offline choice but can instead be seamlessly integrated as
a dynamic component of the learning process itself.

</details>


### [80] [From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph](https://arxiv.org/abs/2510.19873)
*Junfeng Gong,Zhiyi Wei,Junying Chen,Cheng Liu,Huawei Li*

Main category: cs.LG

TL;DR: 本文提出无训练、检索增强的ReGraphT框架，提升小语言模型生成CUDA代码能力，实验显示其表现出色。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成CUDA代码存在代码泄露和计算开销大问题，小语言模型推理能力有限，需提升其在复杂CUDA代码生成上的性能。

Method: 提出ReGraphT框架，将CUDA优化轨迹组织成推理图，用蒙特卡罗图搜索进行探索，还设计CUDA特定基准测试。

Result: ReGraphT优于HPC特定微调模型和其他检索增强方法，在CUDAEval和ParEval上平均加速2.33倍，使小语言模型接近大语言模型性能。

Conclusion: ReGraphT能在无隐私风险和过多计算开销下，让小语言模型达到大语言模型在CUDA代码生成上的性能。

Abstract: Despite significant evolution of CUDA programming and domain-specific
libraries, effectively utilizing GPUs with massively parallel engines remains
difficult. Large language models (LLMs) show strong potential in generating
optimized CUDA code from sequential code. However, using LLMs in practice faces
two major challenges: cloud-based APIs pose risks of code leakage, and local
deployment is often computationally expensive and inefficient. These drawbacks
have spurred interest in small language models (SLMs), which are more
lightweight and privacy-friendly. Encouragingly, recent studies show that SLMs
can achieve performance comparable to LLMs on specific tasks. While SLMs can
match LLMs on domain-specific tasks, their limited reasoning abilities lead to
suboptimal performance in complex CUDA generation according to our experiments.
To bridge this gap, we propose ReGraphT, a training-free, retrieval-augmented
generation framework that transfers LLM-level reasoning to smaller models.
ReGraphT organizes CUDA optimization trajectories into a structured reasoning
graph, modeling the combined CUDA optimizations as state transitions, and
leverages Monte Carlo Graph Search (MCGS) for efficient exploration. We also
present a CUDA-specific benchmark with difficulty tiers defined by reasoning
complexity to evaluate models more comprehensively. Experiments show that
ReGraphT outperforms HPC-specific fine-tuned models and other
retrieval-augmented approaches, achieving an average 2.33X speedup on CUDAEval
and ParEval. When paired with DeepSeek-Coder-V2-Lite-Instruct and
Qwen2.5-Coder-7B-Instruct, ReGraphT enables SLMs to approach LLM-level
performance without the associated privacy risks or excessive computing
overhead.

</details>


### [81] [From Optimization to Prediction: Transformer-Based Path-Flow Estimation to the Traffic Assignment Problem](https://arxiv.org/abs/2510.19889)
*Mostafa Ameli,Van Anh Le,Sulthana Shams,Alexander Skabardonis*

Main category: cs.LG

TL;DR: 本文提出用基于Transformer架构的深度神经网络直接预测平衡路径流量，在多网络实验中验证其比传统方法快，能降低计算成本、提高精度，适应不同条件。


<details>
  <summary>Details</summary>
Motivation: 传统交通分配问题的数学规划方法在大规模网络中计算复杂度过高，需要新方法。

Method: 引入基于Transformer架构的深度神经网络，聚焦路径级交通分布，直接预测平衡路径流量。

Result: 在多个网络的数值实验表明，该模型比传统优化方法快几个数量级，能有效估计多类网络的路径级流量，降低计算成本、提高预测精度。

Conclusion: 该模型可灵活适应不同需求和网络条件，支持交通管理，有助于交通规划和政策制定。

Abstract: The traffic assignment problem is essential for traffic flow analysis,
traditionally solved using mathematical programs under the Equilibrium
principle. These methods become computationally prohibitive for large-scale
networks due to non-linear growth in complexity with the number of OD pairs.
This study introduces a novel data-driven approach using deep neural networks,
specifically leveraging the Transformer architecture, to predict equilibrium
path flows directly. By focusing on path-level traffic distribution, the
proposed model captures intricate correlations between OD pairs, offering a
more detailed and flexible analysis compared to traditional link-level
approaches. The Transformer-based model drastically reduces computation time,
while adapting to changes in demand and network structure without the need for
recalculation. Numerical experiments are conducted on the Manhattan-like
synthetic network, the Sioux Falls network, and the Eastern-Massachusetts
network. The results demonstrate that the proposed model is orders of magnitude
faster than conventional optimization. It efficiently estimates path-level
traffic flows in multi-class networks, reducing computational costs and
improving prediction accuracy by capturing detailed trip and flow information.
The model also adapts flexibly to varying demand and network conditions,
supporting traffic management and enabling rapid `what-if' analyses for
enhanced transportation planning and policy-making.

</details>


### [82] [FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning](https://arxiv.org/abs/2510.19893)
*Shiqi Dai,Wei Dai,Jiaee Cheong,Paul Pu Liang*

Main category: cs.LG

TL;DR: 提出FairGRPO方法促进临床人群公平学习，在多数据集实验中表现良好，还发布FairMedGemma - 4B模型。


<details>
  <summary>Details</summary>
Motivation: 医学人工智能系统在不同人群中表现有差异，强化学习训练继承并放大多数人群数据集中的偏差，需解决公平性问题。

Method: 引入FairGRPO分层强化学习方法，采用基于代表性、任务难度和数据源的自适应优势重要性加权，用无监督聚类处理临床领域缺少人口统计标签问题。

Result: 在7个临床诊断数据集实验中，FairGRPO比基线减少27.2%预测差异，提高12.49%的F1分数，训练动态分析显示其优化中公平性不断提升。

Conclusion: 基于FairGRPO发布的FairMedGemma - 4B模型达到先进水平，显著减少不同人群间的差异。

Abstract: Medical artificial intelligence systems have achieved remarkable diagnostic
capabilities, yet they consistently exhibit performance disparities across
demographic groups, causing real-world harm to underrepresented populations.
While recent multimodal reasoning foundation models have advanced clinical
diagnosis through integrated analysis of diverse medical data, reasoning
trainings via reinforcement learning inherit and often amplify biases present
in training datasets dominated by majority populations. We introduce
Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical
reinforcement learning approach that promotes equitable learning across
heterogeneous clinical populations. FairGRPO employs adaptive importance
weighting of advantages based on representation, task difficulty, and data
source. To address the common issue of missing demographic labels in the
clinical domain, we further employ unsupervised clustering, which automatically
discovers latent demographic groups when labels are unavailable. Through
comprehensive experiments across 7 clinical diagnostic datasets spanning 5
clinical modalities across X-ray, CT scan, dermoscropy, mammography and
ultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2%
against all vanilla and bias mitigated RL baselines, while improving F1 score
by 12.49%. Furthermore, training dynamics analysis reveals that FairGRPO
progressively improves fairness throughout optimization, while baseline RL
methods exhibit deteriorating fairness as training progresses. Based on
FairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM that
achieves state-of-the-art performance while demonstrating significantly reduced
disparities across demographic groups.

</details>


### [83] [Enhancing Diagnostic Accuracy for Urinary Tract Disease through Explainable SHAP-Guided Feature Selection and Classification](https://arxiv.org/abs/2510.19896)
*Filipe Ferreira de Oliveira,Matheus Becali Rocha,Renato A. Krohling*

Main category: cs.LG

TL;DR: 提出基于SHAP特征选择的泌尿道疾病诊断方法，用多种算法及优化技术，证明该方法有效，可助力临床决策系统。


<details>
  <summary>Details</summary>
Motivation: 提高泌尿道疾病（重点是膀胱癌）预测模型的透明度和有效性，开发临床决策支持系统。

Method: 开发六种二元分类场景，使用XGBoost、LightGBM和CatBoost算法，用Optuna进行超参数优化，SMOTE技术进行类别平衡，基于SHAP特征选择确定预测变量。

Result: 基于SHAP的特征选择方法有效，能维持或提升性能指标。

Conclusion: 所提方法有助于开发更透明、可靠、高效的临床决策支持系统，优化泌尿道疾病筛查和早期诊断。

Abstract: In this paper, we propose an approach to support the diagnosis of urinary
tract diseases, with a focus on bladder cancer, using SHAP (SHapley Additive
exPlanations)-based feature selection to enhance the transparency and
effectiveness of predictive models. Six binary classification scenarios were
developed to distinguish bladder cancer from other urological and oncological
conditions. The algorithms XGBoost, LightGBM, and CatBoost were employed, with
hyperparameter optimization performed using Optuna and class balancing with the
SMOTE technique. The selection of predictive variables was guided by importance
values through SHAP-based feature selection while maintaining or even improving
performance metrics such as balanced accuracy, precision, and specificity. The
use of explainability techniques (SHAP) for feature selection proved to be an
effective approach. The proposed methodology may contribute to the development
of more transparent, reliable, and efficient clinical decision support systems,
optimizing screening and early diagnosis of urinary tract diseases.

</details>


### [84] [FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals](https://arxiv.org/abs/2510.19917)
*Trajan Murphy,Akshunna S. Dogra,Hanfeng Gu,Caleb Meredith,Mark Kon,Julio Enrique Castrillion-Candas*

Main category: cs.LG

TL;DR: 介绍用于分析通用分类问题的框架FINDER，适用于噪声数据集，在多个领域验证有效并讨论其优缺点。


<details>
  <summary>Details</summary>
Motivation: 噪声数据集是分类方法研究的关键前沿，有理论和实践意义，需有效方法处理。

Method: 将随机分析思想融入特征学习和推理阶段，构建随机特征，用Kosambi - Karhunen - Loève展开分解特征以实现分类。

Result: 在阿尔茨海默病阶段分类和遥感森林砍伐检测等数据匮乏的科学领域取得了最先进的突破。

Conclusion: 讨论了FINDER何时优于现有方法、失败模式和其他局限性。

Abstract: ''Noisy'' datasets (regimes with low signal to noise ratios, small sample
sizes, faulty data collection, etc) remain a key research frontier for
classification methods with both theoretical and practical implications. We
introduce FINDER, a rigorous framework for analyzing generic classification
problems, with tailored algorithms for noisy datasets. FINDER incorporates
fundamental stochastic analysis ideas into the feature learning and inference
stages to optimally account for the randomness inherent to all empirical
datasets. We construct ''stochastic features'' by first viewing empirical
datasets as realizations from an underlying random field (without assumptions
on its exact distribution) and then mapping them to appropriate Hilbert spaces.
The Kosambi-Karhunen-Lo\'eve expansion (KLE) breaks these stochastic features
into computable irreducible components, which allow classification over noisy
datasets via an eigen-decomposition: data from different classes resides in
distinct regions, identified by analyzing the spectrum of the associated
operators. We validate FINDER on several challenging, data-deficient scientific
domains, producing state of the art breakthroughs in: (i) Alzheimer's Disease
stage classification, (ii) Remote sensing detection of deforestation. We end
with a discussion on when FINDER is expected to outperform existing methods,
its failure modes, and other limitations.

</details>


### [85] [Beyond the Ideal: Analyzing the Inexact Muon Update](https://arxiv.org/abs/2510.19933)
*Egor Shulgin,Sultan AlRashed,Francesco Orabona,Peter Richtárik*

Main category: cs.LG

TL;DR: 本文对Muon优化器的不精确正交更新进行分析，揭示不精确性与步长和动量的耦合关系，实验验证了预测。


<details>
  <summary>Details</summary>
Motivation: Muon优化器效率依赖快速近似正交化，但以往理论分析基于理想化的精确SVD更新，存在理论与实践的脱节。

Method: 在基于线性最小化预言机（LMO）的优化框架下，引入现实的加性误差模型来分析不精确正交更新。

Result: 得出明确界限量化LMO不精确性/误差导致的性能下降，揭示不精确性与最优步长和动量的耦合关系。

Conclusion: 近似过程成为必须与学习计划共同调整的关键参数，NanoGPT实验证实了预测的耦合关系。

Abstract: The Muon optimizer has rapidly emerged as a powerful, geometry-aware
alternative to AdamW, demonstrating strong performance in large-scale training
of neural networks. However, a critical theory-practice disconnect exists:
Muon's efficiency relies on fast, approximate orthogonalization, yet all prior
theoretical work analyzes an idealized, computationally intractable version
assuming exact SVD-based updates. This work moves beyond the ideal by providing
the first analysis of the inexact orthogonalized update at Muon's core. We
develop our analysis within the general framework of Linear Minimization Oracle
(LMO)-based optimization, introducing a realistic additive error model to
capture the inexactness of practical approximation schemes. Our analysis yields
explicit bounds that quantify performance degradation as a function of the LMO
inexactness/error. We reveal a fundamental coupling between this inexactness
and the optimal step size and momentum: lower oracle precision requires a
smaller step size but larger momentum parameter. These findings elevate the
approximation procedure (e.g., the number of Newton-Schulz steps) from an
implementation detail to a critical parameter that must be co-tuned with the
learning schedule. NanoGPT experiments directly confirm the predicted coupling,
with optimal learning rates clearly shifting as approximation precision
changes.

</details>


### [86] [Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets](https://arxiv.org/abs/2510.20609)
*Timur Galimzyanov,Olga Kolomyttseva,Egor Bogomolov*

Main category: cs.LG

TL;DR: 研究现实计算预算下代码生成任务的检索设计，对比不同检索配置，给出基于证据的代码导向RAG系统实施建议。


<details>
  <summary>Details</summary>
Motivation: 在现实计算预算下为代码聚焦的生成任务设计有效的检索方案。

Method: 利用长代码竞技场的代码补全和漏洞定位两个任务，从分块策略、相似度评分、拆分粒度三个轴系统比较不同上下文窗口大小的检索配置。

Result: 1. PL - PL中，基于词级拆分的稀疏BM25最有效实用；2. NL - PL中，专有密集编码器表现更好但延迟大；3. 最佳分块大小与可用上下文有关；4. 简单基于行的分块效果与语法感知拆分相当；5. 不同配置检索延迟差异大，BM25 + 词拆分有最佳质量 - 延迟权衡。

Conclusion: 可根据任务要求、模型约束和计算效率为代码导向RAG系统实施提供基于证据的建议。

Abstract: We study retrieval design for code-focused generation tasks under realistic
compute budgets. Using two complementary tasks from Long Code Arena -- code
completion and bug localization -- we systematically compare retrieval
configurations across various context window sizes along three axes: (i)
chunking strategy, (ii) similarity scoring, and (iii) splitting granularity.
(1) For PL-PL, sparse BM25 with word-level splitting is the most effective and
practical, significantly outperforming dense alternatives while being an order
of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3
family) consistently beat sparse retrievers, however requiring 100x larger
latency. (3) Optimal chunk size scales with available context: 32-64 line
chunks work best at small budgets, and whole-file retrieval becomes competitive
at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting
across budgets. (5) Retrieval latency varies by up to 200x across
configurations; BPE-based splitting is needlessly slow, and BM25 + word
splitting offers the best quality-latency trade-off. Thus, we provide
evidence-based recommendations for implementing effective code-oriented RAG
systems based on task requirements, model constraints, and computational
efficiency.

</details>


### [87] [Mitigating Privacy-Utility Trade-off in Decentralized Federated Learning via $f$-Differential Privacy](https://arxiv.org/abs/2510.19934)
*Xiang Li,Buxin Su,Chendi Wang,Qi Long,Weijie J. Su*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Differentially private (DP) decentralized Federated Learning (FL) allows
local users to collaborate without sharing their data with a central server.
However, accurately quantifying the privacy budget of private FL algorithms is
challenging due to the co-existence of complex algorithmic components such as
decentralized communication and local updates. This paper addresses privacy
accounting for two decentralized FL algorithms within the $f$-differential
privacy ($f$-DP) framework. We develop two new $f$-DP-based accounting methods
tailored to decentralized settings: Pairwise Network $f$-DP (PN-$f$-DP), which
quantifies privacy leakage between user pairs under random-walk communication,
and Secret-based $f$-Local DP (Sec-$f$-LDP), which supports structured noise
injection via shared secrets. By combining tools from $f$-DP theory and Markov
chain concentration, our accounting framework captures privacy amplification
arising from sparse communication, local iterations, and correlated noise.
Experiments on synthetic and real datasets demonstrate that our methods yield
consistently tighter $(\epsilon,\delta)$ bounds and improved utility compared
to R\'enyi DP-based approaches, illustrating the benefits of $f$-DP in
decentralized privacy accounting.

</details>


### [88] [Are Greedy Task Orderings Better Than Random in Continual Linear Regression?](https://arxiv.org/abs/2510.19941)
*Matan Tsipory,Ran Levinstein,Itay Evron,Mark Kong,Deanna Needell,Daniel Soudry*

Main category: cs.LG

TL;DR: 分析连续学习中线性回归的任务排序，对比贪婪排序和随机排序在收敛速度和损失界的表现，揭示两种排序的细微差别。


<details>
  <summary>Details</summary>
Motivation: 此前对贪婪最大化连续任务差异的排序探索有开放性问题，需进一步研究。

Method: 运用Kaczmarz方法文献的工具形式化排序，进行实证分析和理论推导。

Result: 实证表明贪婪排序在平均损失上收敛比随机排序快；分析得出高秩回归中贪婪排序有类似随机排序的损失界，一般秩下有重复依赖的分离。

Conclusion: 揭示了贪婪排序和随机排序内部及两者间的细微差别。

Abstract: We analyze task orderings in continual learning for linear regression,
assuming joint realizability of training data. We focus on orderings that
greedily maximize dissimilarity between consecutive tasks, a concept briefly
explored in prior work but still surrounded by open questions. Using tools from
the Kaczmarz method literature, we formalize such orderings and develop
geometric and algebraic intuitions around them. Empirically, we demonstrate
that greedy orderings converge faster than random ones in terms of the average
loss across tasks, both for linear regression with random data and for linear
probing on CIFAR-100 classification tasks. Analytically, in a high-rank
regression setting, we prove a loss bound for greedy orderings analogous to
that of random ones. However, under general rank, we establish a
repetition-dependent separation. Specifically, while prior work showed that for
random orderings, with or without replacement, the average loss after $k$
iterations is bounded by $\mathcal{O}(1/\sqrt{k})$, we prove that single-pass
greedy orderings may fail catastrophically, whereas those allowing repetition
converge at rate $\mathcal{O}(1/\sqrt[3]{k})$. Overall, we reveal nuances
within and between greedy and random orderings.

</details>


### [89] [Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets](https://arxiv.org/abs/2510.19950)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 传统强化学习训练与部署环境不匹配影响金融交易表现，本文提出椭圆不确定性集方法，实验表明该方法表现更优且具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决金融应用中强化学习训练和部署环境不匹配，传统方法无法捕捉市场影响方向性的问题。

Method: 开发新型椭圆不确定性集，建立最坏情况不确定性的隐式和显式闭式解。

Result: 在单资产和多资产交易任务实验中，该方法实现了更高夏普比率，在交易量增加时保持鲁棒性。

Conclusion: 该方法为金融市场强化学习提供了更可靠、可扩展的途径。

Abstract: In financial applications, reinforcement learning (RL) agents are commonly
trained on historical data, where their actions do not influence prices.
However, during deployment, these agents trade in live markets where their own
transactions can shift asset prices, a phenomenon known as market impact. This
mismatch between training and deployment environments can significantly degrade
performance. Traditional robust RL approaches address this model
misspecification by optimizing the worst-case performance over a set of
uncertainties, but typically rely on symmetric structures that fail to capture
the directional nature of market impact. To address this issue, we develop a
novel class of elliptic uncertainty sets. We establish both implicit and
explicit closed-form solutions for the worst-case uncertainty under these sets,
enabling efficient and tractable robust policy evaluation. Experiments on
single-asset and multi-asset trading tasks demonstrate that our method achieves
superior Sharpe ratio and remains robust under increasing trade volumes,
offering a more faithful and scalable approach to RL in financial markets.

</details>


### [90] [ADP-VRSGP: Decentralized Learning with Adaptive Differential Privacy via Variance-Reduced Stochastic Gradient Push](https://arxiv.org/abs/2510.20157)
*Xiaoming Wu,Teng Liu,Xin Wang,Ming Yang,Jiguo Yu*

Main category: cs.LG

TL;DR: 提出ADP - VRSGP方法用于隐私保护的去中心化学习，动态调整噪声方差和学习率，结合梯度融合等策略，理论和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有使用固定方差噪声的去中心化学习方法会降低模型性能和训练效率，需要改进。

Method: 提出ADP - VRSGP方法，动态调整噪声方差和学习率，引入渐进梯度融合策略，结合去中心化push - sum和聚合技术。

Result: 理论分析表明ADP - VRSGP能稳健收敛，实验结果显示该方法在多场景下优于现有基线。

Conclusion: ADP - VRSGP能有效解决隐私保护的去中心化学习中的挑战，提高训练稳定性和速度。

Abstract: Differential privacy is widely employed in decentralized learning to
safeguard sensitive data by introducing noise into model updates. However,
existing approaches that use fixed-variance noise often degrade model
performance and reduce training efficiency. To address these limitations, we
propose a novel approach called decentralized learning with adaptive
differential privacy via variance-reduced stochastic gradient push (ADP-VRSGP).
This method dynamically adjusts both the noise variance and the learning rate
using a stepwise-decaying schedule, which accelerates training and enhances
final model performance while providing node-level personalized privacy
guarantees. To counteract the slowed convergence caused by large-variance noise
in early iterations, we introduce a progressive gradient fusion strategy that
leverages historical gradients. Furthermore, ADP-VRSGP incorporates
decentralized push-sum and aggregation techniques, making it particularly
suitable for time-varying communication topologies. Through rigorous
theoretical analysis, we demonstrate that ADP-VRSGP achieves robust convergence
with an appropriate learning rate, significantly improving training stability
and speed. Experimental results validate that our method outperforms existing
baselines across multiple scenarios, highlighting its efficacy in addressing
the challenges of privacy-preserving decentralized learning.

</details>


### [91] [Learning Personalized Ad Impact via Contextual Reinforcement Learning under Delayed Rewards](https://arxiv.org/abs/2510.20055)
*Yuwei Cheng,Zifeng Zhao,Haifeng Xu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Online advertising platforms use automated auctions to connect advertisers
with potential customers, requiring effective bidding strategies to maximize
profits. Accurate ad impact estimation requires considering three key factors:
delayed and long-term effects, cumulative ad impacts such as reinforcement or
fatigue, and customer heterogeneity. However, these effects are often not
jointly addressed in previous studies. To capture these factors, we model ad
bidding as a Contextual Markov Decision Process (CMDP) with delayed Poisson
rewards. For efficient estimation, we propose a two-stage maximum likelihood
estimator combined with data-splitting strategies, ensuring controlled
estimation error based on the first-stage estimator's (in)accuracy. Building on
this, we design a reinforcement learning algorithm to derive efficient
personalized bidding strategies. This approach achieves a near-optimal regret
bound of $\tilde{O}{(dH^2\sqrt{T})}$, where $d$ is the contextual dimension,
$H$ is the number of rounds, and $T$ is the number of customers. Our
theoretical findings are validated by simulation experiments.

</details>


### [92] [On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization](https://arxiv.org/abs/2510.19953)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 本文提出基于函数评估的无偏梯度估计器解决零阶优化中梯度估计偏差问题，理论分析并实验验证其优势。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化方法存在梯度估计有偏差的潜在局限，除非扰动步长消失。

Method: 通过将方向导数重新表示为伸缩级数并从精心设计的分布中采样，构建无偏梯度估计器。分析其理论性质，推导四种具体构造的最优缩放分布和扰动步长。

Result: 证明使用所提估计器的随机梯度下降法对平滑非凸目标达到最优复杂度，实验表明该方法在合成任务和语言模型微调上精度和收敛性优于标准方法。

Conclusion: 所提的无偏梯度估计器能有效解决零阶优化中的偏差问题，具有更好的性能。

Abstract: Zeroth-order optimization (ZOO) is an important framework for stochastic
optimization when gradients are unavailable or expensive to compute. A
potential limitation of existing ZOO methods is the bias inherent in most
gradient estimators unless the perturbation stepsize vanishes. In this paper,
we overcome this biasedness issue by proposing a novel family of unbiased
gradient estimators based solely on function evaluations. By reformulating
directional derivatives as a telescoping series and sampling from carefully
designed distributions, we construct estimators that eliminate bias while
maintaining favorable variance. We analyze their theoretical properties, derive
optimal scaling distributions and perturbation stepsizes of four specific
constructions, and prove that SGD using the proposed estimators achieves
optimal complexity for smooth non-convex objectives. Experiments on synthetic
tasks and language model fine-tuning confirm the superior accuracy and
convergence of our approach compared to standard methods.

</details>


### [93] [Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations](https://arxiv.org/abs/2510.19975)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 本文研究两点零阶梯度估计器，找到使估计器渐近方差最小的随机扰动分布，提出方向对齐扰动（DAP）方案并证明其优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注固定长度扰动，忽略了方向对齐的潜在优势，为填补此空白开展研究。

Method: 将问题表述为扰动分布空间上的约束泛函优化问题，研究DAP方案理论和经验性质，对随机梯度下降进行收敛分析。

Result: 通过合成问题和实际任务的实证评估，表明DAP在特定条件下优于传统方法。

Conclusion: 方向对齐的扰动方案具有优势，能在关键方向上自适应提供更高精度。

Abstract: In this paper, we explore the two-point zeroth-order gradient estimator and
identify the distribution of random perturbations that minimizes the
estimator's asymptotic variance as the perturbation stepsize tends to zero. We
formulate it as a constrained functional optimization problem over the space of
perturbation distributions. Our findings reveal that such desired perturbations
can align directionally with the true gradient, instead of maintaining a fixed
length. While existing research has largely focused on fixed-length
perturbations, the potential advantages of directional alignment have been
overlooked. To address this gap, we delve into the theoretical and empirical
properties of the directionally aligned perturbation (DAP) scheme, which
adaptively offers higher accuracy along critical directions. Additionally, we
provide a convergence analysis for stochastic gradient descent using
$\delta$-unbiased random perturbations, extending existing complexity bounds to
a wider range of perturbations. Through empirical evaluations on both synthetic
problems and practical tasks, we demonstrate that DAPs outperform traditional
methods under specific conditions.

</details>


### [94] [What Does It Take to Build a Performant Selective Classifier?](https://arxiv.org/abs/2510.20242)
*Stephan Rabanser,Nicolas Papernot*

Main category: cs.LG

TL;DR: 本文提出选择性分类差距概念，将其分解为五个来源，分析单调事后校准作用有限，需有效重排预测的评分机制，通过实验验证分解并给出构建选择性分类器的指导。


<details>
  <summary>Details</summary>
Motivation: 现有选择性分类器难以达到完美排序预言机的性能，需分析差距来源并找到缩小差距的方法。

Method: 将选择性分类差距进行有限样本分解为五个来源，在合成数据和真实世界基准上验证分解，通过控制实验分离各误差成分。

Result: 确认贝叶斯噪声和有限模型容量会造成显著差距，只有更丰富、特征感知的校准器能有效改善评分排序，数据偏移会引入额外松弛。

Conclusion: 分解得到定量误差预算和可操作的设计指南，帮助从业者构建更接近理想预言机行为的选择性分类器。

Abstract: Selective classifiers improve model reliability by abstaining on inputs the
model deems uncertain. However, few practical approaches achieve the
gold-standard performance of a perfect-ordering oracle that accepts examples
exactly in order of correctness. Our work formalizes this shortfall as the
selective-classification gap and present the first finite-sample decomposition
of this gap to five distinct sources of looseness: Bayes noise, approximation
error, ranking error, statistical noise, and implementation- or shift-induced
slack. Crucially, our analysis reveals that monotone post-hoc calibration --
often believed to strengthen selective classifiers -- has limited impact on
closing this gap, since it rarely alters the model's underlying score ranking.
Bridging the gap therefore requires scoring mechanisms that can effectively
reorder predictions rather than merely rescale them. We validate our
decomposition on synthetic two-moons data and on real-world vision and language
benchmarks, isolating each error component through controlled experiments. Our
results confirm that (i) Bayes noise and limited model capacity can account for
substantial gaps, (ii) only richer, feature-aware calibrators meaningfully
improve score ordering, and (iii) data shift introduces a separate slack that
demands distributionally robust training. Together, our decomposition yields a
quantitative error budget as well as actionable design guidelines that
practitioners can use to build selective classifiers which approximate ideal
oracle behavior more closely.

</details>


### [95] [Towards Strong Certified Defense with Universal Asymmetric Randomization](https://arxiv.org/abs/2510.19977)
*Hanbin Hong,Ashish Kundu,Ali Payani,Binghui Wang,Yuan Hong*

Main category: cs.LG

TL;DR: 提出UCAN技术，通过各向异性噪声提升随机平滑方法的对抗鲁棒性认证效果，实验显示其性能大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有随机平滑方法使用各向同性噪声分布，忽略输入和数据维度的异质性，限制了鲁棒性认证的有效性。

Method: 提出UCAN技术，将现有随机平滑方法从各向同性转变为各向异性噪声分布；开发新框架和三个噪声参数生成器来优化各向异性噪声参数。

Result: 在MNIST、CIFAR10和ImageNet数据集上，大认证半径下认证准确率最高提升182.6%。

Conclusion: UCAN技术能有效提升对抗鲁棒性认证效果，优于现有方法。

Abstract: Randomized smoothing has become essential for achieving certified adversarial
robustness in machine learning models. However, current methods primarily use
isotropic noise distributions that are uniform across all data dimensions, such
as image pixels, limiting the effectiveness of robustness certification by
ignoring the heterogeneity of inputs and data dimensions. To address this
limitation, we propose UCAN: a novel technique that \underline{U}niversally
\underline{C}ertifies adversarial robustness with \underline{A}nisotropic
\underline{N}oise. UCAN is designed to enhance any existing randomized
smoothing method, transforming it from symmetric (isotropic) to asymmetric
(anisotropic) noise distributions, thereby offering a more tailored defense
against adversarial attacks. Our theoretical framework is versatile, supporting
a wide array of noise distributions for certified robustness in different
$\ell_p$-norms and applicable to any arbitrary classifier by guaranteeing the
classifier's prediction over perturbed inputs with provable robustness bounds
through tailored noise injection. Additionally, we develop a novel framework
equipped with three exemplary noise parameter generators (NPGs) to optimally
fine-tune the anisotropic noise parameters for different data dimensions,
allowing for pursuing different levels of robustness enhancements in
practice.Empirical evaluations underscore the significant leap in UCAN's
performance over existing state-of-the-art methods, demonstrating up to
$182.6\%$ improvement in certified accuracy at large certified radii on MNIST,
CIFAR10, and ImageNet datasets.\footnote{Code is anonymously available at
\href{https://github.com/youbin2014/UCAN/}{https://github.com/youbin2014/UCAN/}}

</details>


### [96] [Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency](https://arxiv.org/abs/2510.19980)
*Renzhao Liang,Sizhe Xu,Chenggang Xie,Jingru Chen,Feiyang Ren,Shu Yang,Takahiro Yabe*

Main category: cs.LG

TL;DR: 研究指出适当截断历史数据可提高预测准确性，提出AMRC方法抑制冗余特征学习并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习时间序列预测方法基于‘长序列信息增益假设’存在局限，需改进。

Method: 基于信息瓶颈理论提出Adaptive Masking Loss with Representation Consistency (AMRC)，包含动态掩码损失和表示一致性约束。

Result: AMRC有效抑制冗余特征学习，显著提升模型性能。

Conclusion: 挑战了时间建模的传统假设，为开发高效稳健的预测模型提供新理论见解和方法突破。

Abstract: Time series forecasting plays a pivotal role in critical domains such as
energy management and financial markets. Although deep learning-based
approaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, the
prevailing "long-sequence information gain hypothesis" exhibits inherent
limitations. Through systematic experimentation, this study reveals a
counterintuitive phenomenon: appropriately truncating historical data can
paradoxically enhance prediction accuracy, indicating that existing models
learn substantial redundant features (e.g., noise or irrelevant fluctuations)
during training, thereby compromising effective signal extraction. Building
upon information bottleneck theory, we propose an innovative solution termed
Adaptive Masking Loss with Representation Consistency (AMRC), which features
two core components: 1) Dynamic masking loss, which adaptively identified
highly discriminative temporal segments to guide gradient descent during model
training; 2) Representation consistency constraint, which stabilized the
mapping relationships among inputs, labels, and predictions. Experimental
results demonstrate that AMRC effectively suppresses redundant feature learning
while significantly improving model performance. This work not only challenges
conventional assumptions in temporal modeling but also provides novel
theoretical insights and methodological breakthroughs for developing efficient
and robust forecasting models.

</details>


### [97] [No Compute Left Behind: Rethinking Reasoning and Sampling with Masked Diffusion Models](https://arxiv.org/abs/2510.19990)
*Zachary Horvitz,Raghav Singhal,Hao Zou,Carles Domingo-Enrich,Zhou Yu,Rajesh Ranganath,Kathleen McKeown*

Main category: cs.LG

TL;DR: 本文指出MDLMs在数学和编码任务中存在问题，提出推理填充和多令牌熵解码方法，展示了MDLMs训练和计算带来的新推理和后训练方法。


<details>
  <summary>Details</summary>
Motivation: 解决MDLMs在数学和编码任务中，任意顺序解码和多令牌解码表现不佳的问题，解释其额外计算的合理性。

Method: 提出推理填充方法，利用MDLMs填充推理模板；提出多令牌熵解码（MED），基于条件熵最小化并行解码误差。

Result: 在GSM8k上，基于后验推理轨迹微调LLaDA - 8B Base性能提升与基于人类推理轨迹微调相当；MED在基准测试中保持性能，步数减少2.7倍。

Conclusion: MDLMs的训练和计算解锁了许多新的推理和后训练方法。

Abstract: Masked diffusion language models (MDLMs) are trained to in-fill positions in
randomly masked sequences, in contrast to next-token prediction models.
Discussions around MDLMs focus on two benefits: (1) any-order decoding and 2)
multi-token decoding. However, we observe that for math and coding tasks,
any-order algorithms often underperform or behave similarly to left-to-right
sampling, and standard multi-token decoding significantly degrades performance.
At inference time, MDLMs compute the conditional distribution of all masked
positions. A natural question is: How can we justify this additional compute
when left-to-right one-token-at-a-time decoding is on par with any-order
decoding algorithms? First, we propose reasoning-as-infilling. By using MDLMs
to infill a reasoning template, we can structure outputs and distinguish
between reasoning and answer tokens. In turn, this enables measuring answer
uncertainty during reasoning, and early exits when the model converges on an
answer. Next, given an answer, reasoning-as-infilling enables sampling from the
MDLM posterior over reasoning traces conditioned on the answer, providing a new
source of high-quality data for post-training. On GSM8k, we observe that
fine-tuning LLaDA-8B Base on its posterior reasoning traces provides a
performance boost on par with fine-tuning on human-written reasoning traces.
Additionally, given an answer, reasoning-as-infilling provides a method for
scoring the correctness of the reasoning process at intermediate steps. Second,
we propose multi-token entropy decoding (MED), a simple adaptive sampler that
minimizes the error incurred by decoding positions in parallel based on the
conditional entropies of those positions. MED preserves performance across
benchmarks and leads to 2.7x fewer steps. Our work demonstrates that the
training and compute used by MDLMs unlock many new inference and post-training
methods.

</details>


### [98] [Machine Learning-Based Localization Accuracy of RFID Sensor Networks via RSSI Decision Trees and CAD Modeling for Defense Applications](https://arxiv.org/abs/2510.20019)
*Curtis Lee Shull,Merrick Green*

Main category: cs.LG

TL;DR: 本文提出基于RSSI数据和决策树分类的监督学习模拟，用于国防资产存储区域定位推断，虽模型有一定效果但稀有类易误分类，结果表明可用于异常检测和错位监测，性能可通过改进天线等提升。


<details>
  <summary>Details</summary>
Motivation: RFID跟踪用于国防资产存储时存在传感器特异性差问题，易导致错误检测和安全事件，需解决区域定位推断问题。

Method: 在CAD建模的平面图中进行监督学习模拟，使用现实RSSI数据和决策树分类，对12个实验室区域分类以进行位置推断，计算类权重处理类不平衡。

Result: 模型整体准确率34.2%，多个区域F1分数超0.40，但稀有类常被误分类，计算邻接感知混淆矩阵。

Conclusion: 基于RSSI的决策树可用于国防物流区域异常检测和错位监测，低覆盖和低信号区域分类性能可通过优化天线和传感器等提升。

Abstract: Radio Frequency Identification (RFID) tracking may be a viable solution for
defense assets that must be stored in accordance with security guidelines.
However, poor sensor specificity (vulnerabilities include long range detection,
spoofing, and counterfeiting) can lead to erroneous detection and operational
security events. We present a supervised learning simulation with realistic
Received Signal Strength Indicator (RSSI) data and Decision Tree classification
in a Computer Assisted Design (CAD)-modeled floor plan that encapsulates some
of the challenges encountered in defense storage. In this work, we focused on
classifying 12 lab zones (LabZoneA-L) to perform location inference. The raw
dataset had approximately 980,000 reads. Class frequencies were imbalanced, and
class weights were calculated to account for class imbalance in this
multi-class setting. The model, trained on stratified subsamples to 5,000
balanced observations, yielded an overall accuracy of 34.2% and F1-scores
greater than 0.40 for multiple zones (Zones F, G, H, etc.). However, rare
classes (most notably LabZoneC) were often misclassified, even with the use of
class weights. An adjacency-aware confusion matrix was calculated to allow
better interpretation of physically adjacent zones. These results suggest that
RSSI-based decision trees can be applied in realistic simulations to enable
zone-level anomaly detection or misplacement monitoring for defense supply
logistics. Reliable classification performance in low-coverage and low-signal
zones could be improved with better antenna placement or additional sensors and
sensor fusion with other modalities.

</details>


### [99] [SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph](https://arxiv.org/abs/2510.20022)
*Jiazheng Li,Yawei Wang,David Yan,Yijun Tian,Zhichao Xu,Huan Song,Panpan Xu,Lin Lee Cheong*

Main category: cs.LG

TL;DR: 本文指出大语言模型处理复杂多步任务有挑战，现有基于结果奖励的强化学习方法有局限，提出SALT框架解决问题，实验证明其能提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理复杂多步任务有挑战，现有基于结果奖励的强化学习方法因仅依赖稀疏结果奖励，对无评判模型的基于组的强化学习算法有局限，导致训练不稳定和策略不佳。

Method: 提出SALT框架，从相同提示的轨迹构建图，量化每一步质量并分配优势，作为即插即用模块与现有基于组的强化学习算法集成。

Result: 在WebShop、ALFWorld和AppWorld基准测试上，不同模型大小的实验表明SALT持续提升性能。

Conclusion: SALT能有效解决现有基于结果奖励的强化学习方法的局限，提升基于组的强化学习算法性能。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities,
enabling language agents to excel at single-turn tasks. However, their
application to complex, multi-step, and long-horizon tasks remains challenging.
While reinforcement learning (RL) offers a promising avenue for addressing
these challenges, mainstream approaches typically rely solely on sparse,
outcome-based rewards, a limitation that becomes especially problematic for
group-based RL algorithms lacking critic models, such as Group Relative Policy
Optimization (GRPO). In such methods, uniformly rewarding or penalizing all
actions within a trajectory can lead to training instability and suboptimal
policies, because beneficial and detrimental actions are often entangled across
multi-step interactions. To address this challenge, we propose SALT, a novel
and lightweight framework that provides a finer-grained advantage assignment,
derived solely from outcome rewards. We achieve this by constructing a graph
from trajectories of the same prompt, which allows us to quantify the quality
of each step and assign advantages accordingly. Crucially, SALT is designed as
a plug-and-play module that seamlessly integrates with existing group-based RL
algorithms, requiring no modifications to the rollout procedure and introducing
negligible computational overhead. Extensive experiments on the WebShop,
ALFWorld, and AppWorld benchmarks with various model sizes demonstrate that
SALT consistently improves performance. We also conduct a thorough analysis to
validate the design choices behind SALT and offer actionable insights.

</details>


### [100] [The Temporal Graph of Bitcoin Transactions](https://arxiv.org/abs/2510.20028)
*Vahid Jalili*

Main category: cs.LG

TL;DR: 本文提出一种机器学习兼容的图来建模比特币经济拓扑，提供数据集和工具包以助力机器学习社区研究比特币生态系统。


<details>
  <summary>Details</summary>
Motivation: 比特币网络数据因匿名性和资金流向模糊，难以用于机器学习研究，需填补这一空白。

Method: 通过重建资金流构建表示比特币经济拓扑的时间异质图，提供自定义采样方法、加载和分析工具以及数据库快照。

Result: 构建了包含超24亿节点和超397.2亿边的图，提供了数据集和工具包。

Conclusion: 该数据集和工具包能让机器学习社区大规模研究比特币复杂生态系统，推动相关应用发展。

Abstract: Since its 2009 genesis block, the Bitcoin network has processed \num{>1.08}
billion (B) transactions representing \num{>8.72}B BTC, offering rich potential
for machine learning (ML); yet, its pseudonymity and obscured flow of funds
inherent in its \utxo-based design, have rendered this data largely
inaccessible for ML research. Addressing this gap, we present an ML-compatible
graph modeling the Bitcoin's economic topology by reconstructing the flow of
funds. This temporal, heterogeneous graph encompasses complete transaction
history up to block \cutoffHeight, consisting of \num{>2.4}B nodes and
\num{>39.72}B edges. Additionally, we provide custom sampling methods yielding
node and edge feature vectors of sampled communities, tools to load and analyze
the Bitcoin graph data within specialized graph databases, and ready-to-use
database snapshots. This comprehensive dataset and toolkit empower the ML
community to tackle Bitcoin's intricate ecosystem at scale, driving progress in
applications such as anomaly detection, address classification, market
analysis, and large-scale graph ML benchmarking. Dataset and code available at
\href{https://github.com/B1AAB/EBA}{github.com/b1aab/eba}

</details>


### [101] [Speculative Sampling for Parametric Temporal Point Processes](https://arxiv.org/abs/2510.20031)
*Marin Biloš,Anderson Schneider,Yuriy Nevmyvaka*

Main category: cs.LG

TL;DR: 提出基于拒绝采样的新算法，可对现有TPP模型并行精确采样未来值，在真实数据集上有速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有时间点过程（TPP）模型自回归采样方式具有序列性，限制了效率。

Method: 提出基于拒绝采样的新算法，无需架构更改和重新训练。

Result: 在真实数据集上有经验性的速度提升。

Conclusion: 该方法缩小了大规模TPP应用中表达性建模和高效并行生成之间的差距。

Abstract: Temporal point processes are powerful generative models for event sequences
that capture complex dependencies in time-series data. They are commonly
specified using autoregressive models that learn the distribution of the next
event from the previous events. This makes sampling inherently sequential,
limiting efficiency. In this paper, we propose a novel algorithm based on
rejection sampling that enables exact sampling of multiple future values from
existing TPP models, in parallel, and without requiring any architectural
changes or retraining. Besides theoretical guarantees, our method demonstrates
empirical speedups on real-world datasets, bridging the gap between expressive
modeling and efficient parallel generation for large-scale TPP applications.

</details>


### [102] [Not-a-Bandit: Provably No-Regret Drafter Selection in Speculative Decoding for LLMs](https://arxiv.org/abs/2510.20064)
*Hongyi Liu,Jiaji Huang,Zhen Jia,Youngsuk Park,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 聚焦投机解码中的在线草稿模型选择问题，设计算法，可评估所有草稿模型，降低计算和延迟开销，实验显示方法优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决投机解码中在线草稿模型选择问题，提升大语言模型推理加速效果。

Method: 设计可证明在token接受概率或预期接受长度上与事后最佳草稿模型竞争的算法，评估所有草稿模型，设计系统高效的在线学习器版本。

Result: 在开源大语言模型和多样数据集上实验，方法在多种领域显著优于EAGLE3和BanditSpec基线。

Conclusion: 所提方法在有专业领域草稿模型且需要长推理链的场景表现出色，可与任何投机解码方法通用。

Abstract: Speculative decoding is widely used in accelerating large language model
(LLM) inference. In this work, we focus on the online draft model selection
problem in speculative decoding. We design an algorithm that provably competes
with the best draft model in hindsight for each query in terms of either the
token acceptance probability or expected acceptance length. In particular, we
show that we can accurately evaluate all draft models, instead of only the
chosen model without incurring additional queries to the target model, which
allows us to improve exponentially over the existing bandit-based approach as
the number of draft models increases. Our approach is generically applicable
with any speculative decoding methods (single draft, multi-drafts and
draft-trees). Moreover, we design system-efficient versions of online learners
and demonstrate that the overhead in computation and latency can be
substantially reduced. We conduct extensive experiments on open-source LLMs and
diverse datasets, demonstrating that our methods substantially outperform the
state-of-the-art EAGLE3 and the BanditSpec baseline in a variety of domains
where specialized domain-expert drafters are available, especially when long
reasoning chains are required.

</details>


### [103] [Coupled Transformer Autoencoder for Disentangling Multi-Region Neural Latent Dynamics](https://arxiv.org/abs/2510.20068)
*Ram Dyuthi Sristi,Sowmya Manojna Narasimha,Jingya Huang,Alice Despatin,Simon Musall,Vikash Gilja,Gal Mishne*

Main category: cs.LG

TL;DR: 本文提出耦合变压器自动编码器（CTAE），用于处理多脑区神经元活动数据，在两个数据集上验证其有效性，解码行为变量效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对齐或多视图方法忽略时间结构，动态潜变量模型有局限，需一种能处理非平稳、非线性动态及分离共享和特定区域结构的方法。

Method: 引入CTAE，采用变压器编码器和解码器捕捉长程神经动态，将每个区域的潜在空间划分为正交的共享和私有子空间。

Result: 在两个高密度电生理数据集上，CTAE提取的有意义表征在解码行为变量方面优于现有方法。

Conclusion: CTAE能有效处理多脑区神经元活动数据，在解码行为变量上表现更好。

Abstract: Simultaneous recordings from thousands of neurons across multiple brain areas
reveal rich mixtures of activity that are shared between regions and dynamics
that are unique to each region. Existing alignment or multi-view methods
neglect temporal structure, whereas dynamical latent variable models capture
temporal dependencies but are usually restricted to a single area, assume
linear read-outs, or conflate shared and private signals. We introduce the
Coupled Transformer Autoencoder (CTAE) - a sequence model that addresses both
(i) non-stationary, non-linear dynamics and (ii) separation of shared versus
region-specific structure in a single framework. CTAE employs transformer
encoders and decoders to capture long-range neural dynamics and explicitly
partitions each region's latent space into orthogonal shared and private
subspaces. We demonstrate the effectiveness of CTAE on two high-density
electrophysiology datasets with simultaneous recordings from multiple regions,
one from motor cortical areas and the other from sensory areas. CTAE extracts
meaningful representations that better decode behavioral variables compared to
existing approaches.

</details>


### [104] [ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models](https://arxiv.org/abs/2510.20084)
*Bosong Huang,Ming Jin,Yuxuan Liang,Johan Barthelemy,Debo Cheng,Qingsong Wen,Chenghao Liu,Shirui Pan*

Main category: cs.LG

TL;DR: 本文提出ShapeX框架用于时间序列分类模型解释，在合成和真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有事后时间序列解释方法主要关注时间步级特征归因，忽略了分类结果主要由关键形状子驱动这一先验，需要新方法填补该空白。

Method: 提出ShapeX框架，将时间序列分割为有意义的形状子驱动段，用Shapley值评估其显著性，核心是Shapelet Describe - and - Detect (SDD) 框架学习分类所需的形状子。

Result: 在合成和真实数据集上的实验表明，ShapeX在识别最相关子序列方面优于现有方法，提高了解释的精度和因果保真度。

Conclusion: ShapeX能产生揭示因果关系而非仅相关性的解释，在时间序列解释方面表现出色。

Abstract: Explaining time series classification models is crucial, particularly in
high-stakes applications such as healthcare and finance, where transparency and
trust play a critical role. Although numerous time series classification
methods have identified key subsequences, known as shapelets, as core features
for achieving state-of-the-art performance and validating their pivotal role in
classification outcomes, existing post-hoc time series explanation (PHTSE)
methods primarily focus on timestep-level feature attribution. These
explanation methods overlook the fundamental prior that classification outcomes
are predominantly driven by key shapelets. To bridge this gap, we present
ShapeX, an innovative framework that segments time series into meaningful
shapelet-driven segments and employs Shapley values to assess their saliency.
At the core of ShapeX lies the Shapelet Describe-and-Detect (SDD) framework,
which effectively learns a diverse set of shapelets essential for
classification. We further demonstrate that ShapeX produces explanations which
reveal causal relationships instead of just correlations, owing to the
atomicity properties of shapelets. Experimental results on both synthetic and
real-world datasets demonstrate that ShapeX outperforms existing methods in
identifying the most relevant subsequences, enhancing both the precision and
causal fidelity of time series explanations.

</details>


### [105] [Hierarchical Dual-Head Model for Suicide Risk Assessment via MentalRoBERTa](https://arxiv.org/abs/2510.20085)
*Chang Yang,Ziyi Wang,Wangfeng Tan,Zhiting Tan,Changrui Ji,Zhiming Zhou*

Main category: cs.LG

TL;DR: 本文提出基于MentalRoBERTa的分层双头神经网络进行自杀风险四级分类，介绍模型结构、训练方法及评估方式。


<details>
  <summary>Details</summary>
Motivation: 社交媒体自动检测自杀风险系统面临类别不平衡、发帖模式时间复杂性等挑战。

Method: 构建分层双头神经网络，采用CORAL和标准分类头，用3层Transformer编码器建模时间依赖，用组合损失函数训练，冻结MentalRoBERTa前6层并采用混合精度训练，用5折分层交叉验证评估。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Social media platforms have become important sources for identifying suicide
risk, but automated detection systems face multiple challenges including severe
class imbalance, temporal complexity in posting patterns, and the dual nature
of risk levels as both ordinal and categorical. This paper proposes a
hierarchical dual-head neural network based on MentalRoBERTa for suicide risk
classification into four levels: indicator, ideation, behavior, and attempt.
The model employs two complementary prediction heads operating on a shared
sequence representation: a CORAL (Consistent Rank Logits) head that preserves
ordinal relationships between risk levels, and a standard classification head
that enables flexible categorical distinctions. A 3-layer Transformer encoder
with 8-head multi-head attention models temporal dependencies across post
sequences, while explicit time interval embeddings capture posting behavior
dynamics. The model is trained with a combined loss function (0.5 CORAL + 0.3
Cross-Entropy + 0.2 Focal Loss) that simultaneously addresses ordinal structure
preservation, overconfidence reduction, and class imbalance. To improve
computational efficiency, we freeze the first 6 layers (50%) of MentalRoBERTa
and employ mixed-precision training. The model is evaluated using 5-fold
stratified cross-validation with macro F1 score as the primary metric.

</details>


### [106] [Competition is the key: A Game Theoretic Causal Discovery Approach](https://arxiv.org/abs/2510.20106)
*Amartya Roy,Souvik Chakraborty*

Main category: cs.LG

TL;DR: 提出博弈论强化学习框架用于因果发现，有理论保证且性能优、可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法要么缺乏有限样本保证，要么无法扩展，需填补此差距。

Method: 引入博弈论强化学习框架，让DDQN代理与GES或GraN - DAG竞争，从对手解开始热启动。

Result: 在合成SEMs和多个真实世界基准测试中表现优于GES和GraN - DAG，能扩展到大型图。

Conclusion: 建立了一类新的基于RL的因果发现算法，统一了经验性能和严格的有限样本理论。

Abstract: Causal discovery remains a central challenge in machine learning, yet
existing methods face a fundamental gap: algorithms like GES and GraN-DAG
achieve strong empirical performance but lack finite-sample guarantees, while
theoretically principled approaches fail to scale. We close this gap by
introducing a game-theoretic reinforcement learning framework for causal
discovery, where a DDQN agent directly competes against a strong baseline (GES
or GraN-DAG), always warm-starting from the opponent's solution. This design
yields three provable guarantees: the learned graph is never worse than the
opponent, warm-starting strictly accelerates convergence, and most importantly,
with high probability the algorithm selects the true best candidate graph. To
the best of our knowledge, our result makes a first-of-its-kind progress in
explaining such finite-sample guarantees in causal discovery: on synthetic SEMs
(30 nodes), the observed error probability decays with n, tightly matching
theory. On real-world benchmarks including Sachs, Asia, Alarm, Child, Hepar2,
Dream, and Andes, our method consistently improves upon GES and GraN-DAG while
remaining theoretically safe. Remarkably, it scales to large graphs such as
Hepar2 (70 nodes), Dream (100 nodes), and Andes (220 nodes). Together, these
results establish a new class of RL-based causal discovery algorithms that are
simultaneously provably consistent, sample-efficient, and practically scalable,
marking a decisive step toward unifying empirical performance with rigorous
finite-sample theory.

</details>


### [107] [On pattern classification with weighted dimensions](https://arxiv.org/abs/2510.20107)
*Ayatullah Faruk Mollah*

Main category: cs.LG

TL;DR: 本文对距离度量规范和维度权重进行分析，提出新的维度加权方案并应用于KNN分类器，在多种数据集上表现良好，是加权Minkowski距离KNN分类器的重要推广。


<details>
  <summary>Details</summary>
Motivation: 传统欧几里得距离在模式分析的加权维度距离度量中存在诸多问题，需要改进。

Method: 详细分析距离度量规范和维度权重并可视化；提出新的维度加权方案；将该方案融入KNN分类器；在多种合成和真实数据集上进行模式分类。

Result: 在不同实验中比传统KNN表现好，在基因表达数据集上分类准确率提升约10%。

Conclusion: 该模型是加权Minkowski距离KNN分类器的重要推广，能合理满足高维小样本数据集选择近邻的需求。

Abstract: Studies on various facets of pattern classification is often imperative while
working with multi-dimensional samples pertaining to diverse application
scenarios. In this notion, weighted dimension-based distance measure has been
one of the vital considerations in pattern analysis as it reflects the degree
of similarity between samples. Though it is often presumed to be settled with
the pervasive use of Euclidean distance, plethora of issues often surface. In
this paper, we present (a) a detail analysis on the impact of distance measure
norms and weights of dimensions along with visualization, (b) a novel weighting
scheme for each dimension, (c) incorporation of this dimensional weighting
schema into a KNN classifier, and (d) pattern classification on a variety of
synthetic as well as realistic datasets with the developed model. It has
performed well across diverse experiments in comparison to the traditional KNN
under the same experimental setups. Specifically, for gene expression datasets,
it yields significant and consistent gain in classification accuracy (around
10%) in all cross-validation experiments with different values of k. As such
datasets contain limited number of samples of high dimensions, meaningful
selection of nearest neighbours is desirable, and this requirement is
reasonably met by regulating the shape and size of the region enclosing the k
number of reference samples with the developed weighting schema and appropriate
norm. It, therefore, stands as an important generalization of KNN classifier
powered by weighted Minkowski distance with the present weighting schema.

</details>


### [108] [Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning](https://arxiv.org/abs/2510.20108)
*Gabriel Y. Arteaga,Marius Aasan,Rwiddhi Chakraborty,Martine Hjelkrem-Tan,Thalles Silva,Michael Kampffmeyer,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: 现有原型自监督学习方法存在部分原型坍塌问题，本文提出完全解耦训练策略解决该问题。


<details>
  <summary>Details</summary>
Motivation: 解决原型自监督学习方法中部分原型坍塌问题，避免现有缓解方法治标不治本的情况。

Method: 引入完全解耦训练策略，将原型和编码器在不同目标下学习，用在线 EM 风格程序更新高斯混合模型作为原型。

Result: 无需显式正则化消除了原型坍塌，获得了多样的原型和更好的下游性能。

Conclusion: 提出的解耦训练策略能有效解决原型坍塌问题，提升模型性能。

Abstract: Prototypical self-supervised learning methods consistently suffer from
partial prototype collapse, where multiple prototypes converge to nearly
identical representations. This undermines their central purpose -- providing
diverse and informative targets to guide encoders toward rich representations
-- and has led practitioners to over-parameterize prototype sets or add ad-hoc
regularizers, which mitigate symptoms rather than address the root cause. We
empirically trace the collapse to the joint optimization of encoders and
prototypes, which encourages a type of shortcut learning: early in training
prototypes drift toward redundant representations that minimize loss without
necessarily enhancing representation diversity. To break the joint
optimization, we introduce a fully decoupled training strategy that learns
prototypes and encoders under separate objectives. Concretely, we model
prototypes as a Gaussian mixture updated with an online EM-style procedure,
independent of the encoder's loss. This simple yet principled decoupling
eliminates prototype collapse without explicit regularization and yields
consistently diverse prototypes and stronger downstream performance.

</details>


### [109] [There is No "apple" in Timeseries: Rethinking TSFM through the Lens of Invariance](https://arxiv.org/abs/2510.20119)
*Arian Prabowo,Flora D. Salim*

Main category: cs.LG

TL;DR: 时间序列基础模型虽多，但轻量级监督基线和经典模型常与之效果相当，问题源于简单移植NLP或CV管道，应转向原则性设计构建数据集。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列基础模型效果与轻量级监督基线和经典模型相当的问题，找出差距根源。

Method: 提出应从机会主义聚合转向原则性设计，基于第一性原理构建时间序列不变性的本体，通过不变性覆盖确保表征完整性。

Result: 未提及具体结果。

Conclusion: 只有通过不变性覆盖确保表征完整性，时间序列基础模型才能实现泛化、推理和真正的涌现行为所需的对齐结构。

Abstract: Timeseries foundation models (TSFMs) have multiplied, yet lightweight
supervised baselines and even classical models often match them. We argue this
gap stems from the naive importation of NLP or CV pipelines. In language and
vision, large web-scale corpora densely capture human concepts i.e. there are
countless images and text of apples. In contrast, timeseries data is built to
complement the image and text modalities. There are no timeseries dataset that
contains the concept apple. As a result, the scrape-everything-online paradigm
fails for TS. We posit that progress demands a shift from opportunistic
aggregation to principled design: constructing datasets that systematically
span the space of invariance that preserve temporal semantics. To this end, we
suggest that the ontology of timeseries invariances should be built based on
first principles. Only by ensuring representational completeness through
invariance coverage can TSFMs achieve the aligned structure necessary for
generalisation, reasoning, and truly emergent behaviour.

</details>


### [110] [Understanding Mechanistic Role of Structural and Functional Connectivity in Tau Propagation Through Multi-Layer Modeling](https://arxiv.org/abs/2510.20148)
*Tingting Dan,Xinwei Huang,Jiaqi Ding,Yinggang Zheng,Guorong Wu*

Main category: cs.LG

TL;DR: 利用纵向神经影像数据，通过多层图扩散模型研究结构连接（SC）和功能连接（FC）对tau蛋白传播的影响，发现二者贡献区域不对称，随病程主导权会转移，与AD相关基因表达模式匹配，部分风险因素和机制会重塑tau传播，结果在独立队列验证。


<details>
  <summary>Details</summary>
Motivation: 现有证据显示大脑网络架构在阿尔茨海默病（AD）进展中起关键作用，但SC和FC如何相互作用影响tau蛋白传播尚不清楚。

Method: 利用大量纵向神经影像数据，通过多层图扩散模型研究SC - FC相互作用。

Result: 连接体架构限制tau蛋白传播，SC和FC贡献区域不对称；病程中SC和FC主导权会转移；SC和FC主导区域空间模式与AD相关基因区域表达强烈一致；部分非可修改风险因素和生物机制会区域特异性地重塑tau蛋白传播。

Conclusion: 研究揭示了SC和FC在tau蛋白传播中的不同作用及影响因素，结果在独立队列得到验证。

Abstract: Emerging neuroimaging evidence shows that pathological tau proteins build up
along specific brain networks, suggesting that large-scale network architecture
plays a key role in the progression of Alzheimer's disease (AD). However, how
structural connectivity (SC) and functional connectivity (FC) interact to
influence tau propagation remains unclear. Leveraging an unprecedented volume
of longitudinal neuroimaging data, we examine SC-FC interactions through a
multi-layer graph diffusion model. Beyond showing that connectome architecture
constrains tau spread, our model reveals a regionally asymmetric contribution
of SC and FC. Specifically, FC predominantly drives tau spread in subcortical
areas, the insula, frontal and temporal cortices, whereas SC plays a larger
role in occipital, parietal, and limbic regions. The relative dominance of SC
versus FC shifts over the course of disease, with FC generally prevailing in
early AD and SC becoming primary in later stages. Spatial patterns of SC- and
FC-dominant regions strongly align with the regional expression of
AD-associated genes involved in inflammation, apoptosis, and lysosomal
function, including CHUK (IKK-alpha), TMEM106B, MCL1, NOTCH1, and TH. In
parallel, other non-modifiable risk factors (e.g., APOE genotype, sex) and
biological mechanisms (e.g., amyloid deposition) selectively reshape tau
propagation by shifting dominant routes between anatomical and functional
pathways in a region-specific manner. Findings are validated in an independent
AD cohort.

</details>


### [111] [Empowering Targeted Neighborhood Search via Hyper Tour for Large-Scale TSP](https://arxiv.org/abs/2510.20169)
*Tongkai Lu,Shuai Ma,Chongyang Tao*

Main category: cs.LG

TL;DR: 提出HyperNS方法解决大规模TSP问题，实验显示优于现有神经方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经的TSP求解方法在处理大规模实例时面临内存约束、初始解质量不高和全局引导不足等挑战。

Method: 采用“先聚类，后路由”策略，用稀疏热图将TSP实例聚类成超节点，生成超路径引导初始化和优化过程，聚焦超路径相关边以减少搜索空间。

Result: 在合成和真实数据集上的实验表明，该方法优于现有基于神经的方法，尤其在处理大规模实例时，显著缩小了与最优解的差距。

Conclusion: HyperNS方法能有效解决大规模TSP问题，比现有神经方法更高效。

Abstract: Traveling Salesman Problem (TSP) is a classic NP-hard problem that has
garnered significant attention from both academia and industry. While
neural-based methods have shown promise for solving TSPs, they still face
challenges in scaling to larger instances, particularly in memory constraints
associated with global heatmaps, edge weights, or access matrices, as well as
in generating high-quality initial solutions and insufficient global guidance
for efficiently navigating vast search spaces. To address these challenges, we
propose a Hyper Tour Guided Neighborhood Search (HyperNS) method for
large-scale TSP instances. Inspired by the ``clustering first, route second"
strategy, our approach initially divides the TSP instance into clusters using a
sparse heatmap graph and abstracts them as supernodes, followed by the
generation of a hyper tour to guide both the initialization and optimization
processes. This method reduces the search space by focusing on edges relevant
to the hyper tour, leading to more efficient and effective optimization.
Experimental results on both synthetic and real-world datasets demonstrate that
our approach outperforms existing neural-based methods, particularly in
handling larger-scale instances, offering a significant reduction in the gap to
the optimal solution.

</details>


### [112] [Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values](https://arxiv.org/abs/2510.20187)
*Dian Yu,Yulai Zhao,Kishan Panaganti,Linfeng Song,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 提出RLEV方法使大语言模型优化与人类价值信号对齐，在多方面表现优于仅关注正确性的基线。


<details>
  <summary>Details</summary>
Motivation: RLVR方法在训练模型时忽略了任务重要性的差异，需要一种能直接将人类价值信号融入奖励函数的方法。

Method: 提出RLEV方法，将人类定义的价值信号直接纳入奖励函数，使用带明确真值价值标签的考试式数据。

Result: RLEV在多种强化学习算法和模型规模上始终优于仅关注正确性的基线，能学习价值敏感的终止策略，在有噪声的价值信号下仍保持鲁棒性。

Conclusion: 优化显式效用函数为使大语言模型与人类优先级对齐提供了实用途径。

Abstract: We propose Reinforcement Learning with Explicit Human Values (RLEV), a method
that aligns Large Language Model (LLM) optimization directly with quantifiable
human value signals. While Reinforcement Learning with Verifiable Rewards
(RLVR) effectively trains models in objective domains using binary correctness
rewards, it overlooks that not all tasks are equally significant. RLEV extends
this framework by incorporating human-defined value signals directly into the
reward function. Using exam-style data with explicit ground-truth value labels,
RLEV consistently outperforms correctness-only baselines across multiple RL
algorithms and model scales. Crucially, RLEV policies not only improve
value-weighted accuracy but also learn a value-sensitive termination policy:
concise for low-value prompts, thorough for high-value ones. We demonstrate
this behavior stems from value-weighted gradient amplification on
end-of-sequence tokens. Ablation studies confirm the gain is causally linked to
value alignment. RLEV remains robust under noisy value signals, such as
difficulty-based labels, demonstrating that optimizing for an explicit utility
function offers a practical path to aligning LLMs with human priorities.

</details>


### [113] [Risk-Averse Constrained Reinforcement Learning with Optimized Certainty Equivalents](https://arxiv.org/abs/2510.20199)
*Jane H. Lee,Baturay Saglam,Spyridon Pougkakiotis,Amin Karbasi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: 提出风险感知约束强化学习框架，可确保与原约束问题等价，有简单算法，证明收敛性并实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有约束优化在强化学习中用期望累积奖励表达目标和约束，忽略奖励分布尾部的风险事件，不适用于高风险应用。

Method: 提出使用优化确定性等价物（OCEs）的风险感知约束强化学习框架，在参数化强拉格朗日对偶框架下确保与原约束问题等价，算法可围绕标准RL求解器。

Result: 建立了算法在常见假设下的收敛性，通过数值实验验证了方法的风险感知特性。

Conclusion: 所提风险感知约束强化学习框架有效可行，能解决高风险应用问题。

Abstract: Constrained optimization provides a common framework for dealing with
conflicting objectives in reinforcement learning (RL). In most of these
settings, the objectives (and constraints) are expressed though the expected
accumulated reward. However, this formulation neglects risky or even possibly
catastrophic events at the tails of the reward distribution, and is often
insufficient for high-stakes applications in which the risk involved in
outliers is critical. In this work, we propose a framework for risk-aware
constrained RL, which exhibits per-stage robustness properties jointly in
reward values and time using optimized certainty equivalents (OCEs). Our
framework ensures an exact equivalent to the original constrained problem
within a parameterized strong Lagrangian duality framework under appropriate
constraint qualifications, and yields a simple algorithmic recipe which can be
wrapped around standard RL solvers, such as PPO. Lastly, we establish the
convergence of the proposed algorithm under common assumptions, and verify the
risk-aware properties of our approach through several numerical experiments.

</details>


### [114] [Approximate Replicability in Learning](https://arxiv.org/abs/2510.20200)
*Max Hopkins,Russell Impagliazzo,Christopher Ye*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Replicability, introduced by (Impagliazzo et al. STOC '22), is the notion
that algorithms should remain stable under a resampling of their inputs (given
access to shared randomness). While a strong and interesting notion of
stability, the cost of replicability can be prohibitive: there is no replicable
algorithm, for instance, for tasks as simple as threshold learning (Bun et al.
STOC '23). Given such strong impossibility results we ask: under what
approximate notions of replicability is learning possible?
  In this work, we propose three natural relaxations of replicability in the
context of PAC learning: (1) Pointwise: the learner must be consistent on any
fixed input, but not across all inputs simultaneously, (2) Approximate: the
learner must output hypotheses that classify most of the distribution
consistently, (3) Semi: the algorithm is fully replicable, but may additionally
use shared unlabeled samples. In all three cases, for constant replicability
parameters, we obtain sample-optimal agnostic PAC learners: (1) and (2) are
achievable for ``free" using $\Theta(d/\alpha^2)$ samples, while (3) requires
$\Theta(d^2/\alpha^2)$ labeled samples.

</details>


### [115] [Assessing the Feasibility of Early Cancer Detection Using Routine Laboratory Data: An Evaluation of Machine Learning Approaches on an Imbalanced Dataset](https://arxiv.org/abs/2510.20209)
*Shumin Li*

Main category: cs.LG

TL;DR: 研究评估用常规实验室数据对犬癌症风险分类的可行性，最佳模型表现不佳，结论是需整合多模态数据源。


<details>
  <summary>Details</summary>
Motivation: 开发犬早期癌症检测的筛查工具面临挑战，常规实验室数据有潜力但存在问题，故评估其癌症风险分类的可行性。

Method: 对Golden Retriever Lifetime Study (GRLS)队列数据，系统比较126个分析管道，包括不同机器学习模型、特征选择方法和数据平衡技术，按患者级别划分数据。

Result: 最优逻辑回归分类器有一定排名能力，但临床分类性能差，高阴性预测值但召回率不足，预测受非特异性特征驱动。

Conclusion: 常规实验室数据虽有癌症信号，但太弱且易混淆，需要整合多模态数据源推动计算兽医学肿瘤学进展。

Abstract: The development of accessible screening tools for early cancer detection in
dogs represents a significant challenge in veterinary medicine. Routine
laboratory data offer a promising, low-cost source for such tools, but their
utility is hampered by the non-specificity of individual biomarkers and the
severe class imbalance inherent in screening populations. This study assesses
the feasibility of cancer risk classification using the Golden Retriever
Lifetime Study (GRLS) cohort under real-world constraints, including the
grouping of diverse cancer types and the inclusion of post-diagnosis samples. A
comprehensive benchmark evaluation was conducted, systematically comparing 126
analytical pipelines that comprised various machine learning models, feature
selection methods, and data balancing techniques. Data were partitioned at the
patient level to prevent leakage. The optimal model, a Logistic Regression
classifier with class weighting and recursive feature elimination, demonstrated
moderate ranking ability (AUROC = 0.815; 95% CI: 0.793-0.836) but poor clinical
classification performance (F1-score = 0.25, Positive Predictive Value = 0.15).
While a high Negative Predictive Value (0.98) was achieved, insufficient recall
(0.79) precludes its use as a reliable rule-out test. Interpretability analysis
with SHapley Additive exPlanations (SHAP) revealed that predictions were driven
by non-specific features like age and markers of inflammation and anemia. It is
concluded that while a statistically detectable cancer signal exists in routine
lab data, it is too weak and confounded for clinically reliable discrimination
from normal aging or other inflammatory conditions. This work establishes a
critical performance ceiling for this data modality in isolation and
underscores that meaningful progress in computational veterinary oncology will
require integration of multi-modal data sources.

</details>


### [116] [CO-PFL: Contribution-Oriented Personalized Federated Learning for Heterogeneous Networks](https://arxiv.org/abs/2510.20219)
*Ke Xing,Yanjie Dong,Xiaoyi Fan,Runhao Zeng,Victor C. M. Leung,M. Jamal Deen,Xiping Hu*

Main category: cs.LG

TL;DR: 提出CO - PFL算法解决传统联邦学习在数据异质性下的问题，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习依赖单一共识模型，标准聚合方法基于等贡献假设，未考虑客户端更新的实际效用和可靠性，导致个性化不佳和聚合偏差。

Method: 引入CO - PFL算法，通过分析梯度方向差异和预测偏差进行联合评估，结合参数级个性化机制和掩码感知动量优化。

Result: 在四个基准数据集上的实验表明，CO - PFL在个性化准确性、鲁棒性、可扩展性和收敛稳定性方面始终优于现有方法。

Conclusion: CO - PFL能有效减轻聚合偏差，加强全局协调，提高本地性能。

Abstract: Personalized federated learning (PFL) addresses a critical challenge of
collaboratively training customized models for clients with heterogeneous and
scarce local data. Conventional federated learning, which relies on a single
consensus model, proves inadequate under such data heterogeneity. Its standard
aggregation method of weighting client updates heuristically or by data volume,
operates under an equal-contribution assumption, failing to account for the
actual utility and reliability of each client's update. This often results in
suboptimal personalization and aggregation bias. To overcome these limitations,
we introduce Contribution-Oriented PFL (CO-PFL), a novel algorithm that
dynamically estimates each client's contribution for global aggregation. CO-PFL
performs a joint assessment by analyzing both gradient direction discrepancies
and prediction deviations, leveraging information from gradient and data
subspaces. This dual-subspace analysis provides a principled and discriminative
aggregation weight for each client, emphasizing high-quality updates.
Furthermore, to bolster personalization adaptability and optimization
stability, CO-PFL cohesively integrates a parameter-wise personalization
mechanism with mask-aware momentum optimization. Our approach effectively
mitigates aggregation bias, strengthens global coordination, and enhances local
performance by facilitating the construction of tailored submodels with stable
updates. Extensive experiments on four benchmark datasets (CIFAR10, CIFAR10C,
CINIC10, and Mini-ImageNet) confirm that CO-PFL consistently surpasses
state-of-the-art methods in in personalization accuracy, robustness,
scalability and convergence stability.

</details>


### [117] [Alternatives to the Laplacian for Scalable Spectral Clustering with Group Fairness Constraints](https://arxiv.org/abs/2510.20220)
*Iván Ojeda-Ruiz,Young Ju-Lee,Malcolm Dickens,Leonardo Cambisaca*

Main category: cs.LG

TL;DR: 本文提出Fair - SMW算法提升谱聚类算法效率，在平衡度相当情况下改善运行时间，计算时间比现有技术快一倍，平衡度可提升一倍。


<details>
  <summary>Details</summary>
Motivation: 现有将平衡作为公平度量融入谱聚类算法的计算时间有待改善，需提升谱聚类算法效率。

Method: 使用拉格朗日方法和Sherman - Morrison - Woodbury (SMW) 恒等式重新表述约束优化问题，得到Fair - SMW算法，采用三种不同谱间隙的拉普拉斯矩阵替代方案生成多个变体。

Result: 使用随机块模型 (SBM) 在多个真实网络数据集上评估，计算时间比现有技术快一倍，平衡度可提升一倍。

Conclusion: Fair - SMW算法能在保证聚类平衡度的同时，显著提高谱聚类算法的运行效率。

Abstract: Recent research has focused on mitigating algorithmic bias in clustering by
incorporating fairness constraints into algorithmic design. Notions such as
disparate impact, community cohesion, and cost per population have been
implemented to enforce equitable outcomes. Among these, group fairness
(balance) ensures that each protected group is proportionally represented
within every cluster. However, incorporating balance as a metric of fairness
into spectral clustering algorithms has led to computational times that can be
improved. This study aims to enhance the efficiency of spectral clustering
algorithms by reformulating the constrained optimization problem using a new
formulation derived from the Lagrangian method and the
Sherman-Morrison-Woodbury (SMW) identity, resulting in the Fair-SMW algorithm.
Fair-SMW employs three alternatives to the Laplacian matrix with different
spectral gaps to generate multiple variations of Fair-SMW, achieving clustering
solutions with comparable balance to existing algorithms while offering
improved runtime performance. We present the results of Fair-SMW, evaluated
using the Stochastic Block Model (SBM) to measure both runtime efficiency and
balance across real-world network datasets, including LastFM, FacebookNet,
Deezer, and German. We achieve an improvement in computation time that is twice
as fast as the state-of-the-art, and also flexible enough to achieve twice as
much balance.

</details>


### [118] [QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models](https://arxiv.org/abs/2510.20222)
*Hao Wang,Baojun Ma*

Main category: cs.LG

TL;DR: 论文提出QKCV注意力机制，可提升基于注意力模型的预测准确性，在微调单变量时间序列基础模型时表现出色。


<details>
  <summary>Details</summary>
Motivation: 在现实时间序列预测任务中，类别信息对捕捉数据内在模式至关重要，传统QKV框架缺乏对类别信息的强调。

Method: 引入QKCV注意力机制，在传统QKV框架中加入静态类别嵌入C。

Result: 作为通用插件模块，提升了多种基于注意力模型在不同真实数据集上的预测准确性；在微调单变量时间序列基础模型时仅更新静态嵌入C，减少计算开销并取得更好的微调性能。

Conclusion: QKCV注意力机制有效且具备良好的适应性，能提升预测准确性和微调性能。

Abstract: In real-world time series forecasting tasks, category information plays a
pivotal role in capturing inherent data patterns. This paper introduces QKCV
(Query-Key-Category-Value) attention, an extension of the traditional QKV
framework that incorporates a static categorical embedding C to emphasize
category-specific information. As a versatile plug-in module, QKCV enhances the
forecasting accuracy of attention-based models (e.g., Vanilla Transformer,
Informer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCV
demonstrates remarkable adaptability in fine-tuning univariate time series
foundation model by solely updating the static embedding C while preserving
pretrained weights, thereby reducing computational overhead and achieving
superior fine-tuning performance.

</details>


### [119] [Federated Learning via Meta-Variational Dropout](https://arxiv.org/abs/2510.20225)
*Insu Jeon,Minui Hong,Junhyeog Yun,Gunhee Kim*

Main category: cs.LG

TL;DR: 提出MetaVD方法解决传统联邦学习在非IID数据下的问题，实验显示其有良好性能并可压缩参数。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在实际应用中面临模型过拟合和本地模型发散问题，因客户端数据有限且非IID。

Method: 引入MetaVD，通过共享超网络学习预测客户端依赖的丢弃率，强调元学习的后验适应和贝叶斯联邦学习的后验聚合。

Result: 在各种稀疏和非IID联邦学习数据集上实验，MetaVD有出色分类准确率和不确定性校准性能，尤其对OOD客户端，还能压缩本地模型参数。

Conclusion: MetaVD能有效解决传统联邦学习问题，提高性能并降低通信成本。

Abstract: Federated Learning (FL) aims to train a global inference model from remotely
distributed clients, gaining popularity due to its benefit of improving data
privacy. However, traditional FL often faces challenges in practical
applications, including model overfitting and divergent local models due to
limited and non-IID data among clients. To address these issues, we introduce a
novel Bayesian meta-learning approach called meta-variational dropout (MetaVD).
MetaVD learns to predict client-dependent dropout rates via a shared
hypernetwork, enabling effective model personalization of FL algorithms in
limited non-IID data settings. We also emphasize the posterior adaptation view
of meta-learning and the posterior aggregation view of Bayesian FL via the
conditional dropout posterior. We conducted extensive experiments on various
sparse and non-IID FL datasets. MetaVD demonstrated excellent classification
accuracy and uncertainty calibration performance, especially for
out-of-distribution (OOD) clients. MetaVD compresses the local model parameters
needed for each client, mitigating model overfitting and reducing communication
costs. Code is available at https://github.com/insujeon/MetaVD.

</details>


### [120] [Sparse Local Implicit Image Function for sub-km Weather Downscaling](https://arxiv.org/abs/2510.20228)
*Yago del Valle Inclan Redondo,Enrique Arriaga-Varela,Dmitry Lyamzin,Pablo Cervantes,Tiago Ramalho*

Main category: cs.LG

TL;DR: 提出SpLIIF生成隐式神经表示以实现气象变量任意降尺度，在日本数据上评估，模型在降尺度温度和风速上优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 实现气象变量的任意降尺度。

Method: 训练基于日本稀疏气象站和地形数据的SpLIIF模型，与插值基线和CorrDiff对比。

Result: 模型在降尺度温度上比CorrDiff和基线好达50%，风速上好10 - 20%。

Conclusion: SpLIIF模型在气象变量降尺度方面表现优于对比方法。

Abstract: We introduce SpLIIF to generate implicit neural representations and enable
arbitrary downscaling of weather variables. We train a model from sparse
weather stations and topography over Japan and evaluate in- and
out-of-distribution accuracy predicting temperature and wind, comparing it to
both an interpolation baseline and CorrDiff. We find the model to be up to 50%
better than both CorrDiff and the baseline at downscaling temperature, and
around 10-20% better for wind.

</details>


### [121] [Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic Approach](https://arxiv.org/abs/2510.20235)
*Woohyeon Byeon,Giseung Park,Jongseong Chae,Amir Leshem,Youngchul Sung*

Main category: cs.LG

TL;DR: 提出用于最大最小准则多目标强化学习的框架，有收敛性和实用性，实验表现佳。


<details>
  <summary>Details</summary>
Motivation: 构建具有最大最小准则的多目标强化学习的实用且可证明收敛的框架。

Method: 从博弈论角度将其重铸为两人零和正则化连续博弈，基于镜像下降引入算法，还采用自适应正则化改进。

Result: 算法具有全局最后迭代收敛性，给出理论分析，实验展示收敛行为，在深度强化学习中表现超先前基线。

Conclusion: 所提框架和算法在多目标强化学习中有效且实用。

Abstract: In this paper, we propose a provably convergent and practical framework for
multi-objective reinforcement learning with max-min criterion. From a
game-theoretic perspective, we reformulate max-min multi-objective
reinforcement learning as a two-player zero-sum regularized continuous game and
introduce an efficient algorithm based on mirror descent. Our approach
simplifies the policy update while ensuring global last-iterate convergence. We
provide a comprehensive theoretical analysis on our algorithm, including
iteration complexity under both exact and approximate policy evaluations, as
well as sample complexity bounds. To further enhance performance, we modify the
proposed algorithm with adaptive regularization. Our experiments demonstrate
the convergence behavior of the proposed algorithm in tabular settings, and our
implementation for deep reinforcement learning significantly outperforms
previous baselines in many MORL environments.

</details>


### [122] [Layer-to-Layer Knowledge Mixing in Graph Neural Network for Chemical Property Prediction](https://arxiv.org/abs/2510.20236)
*Teng Jiek See,Daokun Zhang,Mario Boley,David K. Chalmers*

Main category: cs.LG

TL;DR: 提出Layer - to - Layer Knowledge Mixing (LKM)方法提升图神经网络(GNN)预测分子属性准确性，且不显著增加计算成本，实验验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有GNN预测分子属性模型需更准确，增加模型复杂度会提升计算成本和内存需求。

Method: 开发LKM自知识蒸馏方法，最小化GNN层预存在隐藏嵌入间的平均绝对距离，有效聚合多跳和多尺度信息。

Result: 用三种GNN架构在不同数据集评估，LKM有效降低量子化学和生物物理属性预测的平均绝对误差，最高分别达9.8% (QM9)、45.3% (MD17 Energy)和22.9% (Chignolin)。

Conclusion: LKM有潜力显著提高GNN预测化学属性的准确性，且不显著增加训练和推理成本。

Abstract: Graph Neural Networks (GNNs) are the currently most effective methods for
predicting molecular properties but there remains a need for more accurate
models. GNN accuracy can be improved by increasing the model complexity but
this also increases the computational cost and memory requirement during
training and inference. In this study, we develop Layer-to-Layer Knowledge
Mixing (LKM), a novel self-knowledge distillation method that increases the
accuracy of state-of-the-art GNNs while adding negligible computational
complexity during training and inference. By minimizing the mean absolute
distance between pre-existing hidden embeddings of GNN layers, LKM efficiently
aggregates multi-hop and multi-scale information, enabling improved
representation of both local and global molecular features. We evaluated LKM
using three diverse GNN architectures (DimeNet++, MXMNet, and PAMNet) using
datasets of quantum chemical properties (QM9, MD17 and Chignolin). We found
that the LKM method effectively reduces the mean absolute error of quantum
chemical and biophysical property predictions by up to 9.8% (QM9), 45.3% (MD17
Energy), and 22.9% (Chignolin). This work demonstrates the potential of LKM to
significantly improve the accuracy of GNNs for chemical property prediction
without any substantial increase in training and inference cost.

</details>


### [123] [FedGPS: Statistical Rectification Against Data Heterogeneity in Federated Learning](https://arxiv.org/abs/2510.20250)
*Zhiqin Yang,Yonggang Zhang,Chenxin Li,Yiu-ming Cheung,Bo Han,Yixuan Yuan*

Main category: cs.LG

TL;DR: 现有联邦学习方法在数据异质性场景下鲁棒性有限，提出 FedGPS 框架，结合统计和梯度信息，实验表明其在不同场景表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在某些异质性场景下性能提升问题被忽视，需评估其在多样异质性场景下的鲁棒性并提出改进方法。

Method: 提出 FedGPS 框架，静态修改客户端学习目标，用替代信息隐式建模全局数据分布，动态调整本地更新方向。

Result: FedGPS 在不同异质性场景下优于现有方法。

Conclusion: FedGPS 框架有效且具有鲁棒性。

Abstract: Federated Learning (FL) confronts a significant challenge known as data
heterogeneity, which impairs model performance and convergence. Existing
methods have made notable progress in addressing this issue. However, improving
performance in certain heterogeneity scenarios remains an overlooked question:
\textit{How robust are these methods to deploy under diverse heterogeneity
scenarios?} To answer this, we conduct comprehensive evaluations across varied
heterogeneity scenarios, showing that most existing methods exhibit limited
robustness. Meanwhile, insights from these experiments highlight that sharing
statistical information can mitigate heterogeneity by enabling clients to
update with a global perspective. Motivated by this, we propose \textbf{FedGPS}
(\textbf{Fed}erated \textbf{G}oal-\textbf{P}ath \textbf{S}ynergy), a novel
framework that seamlessly integrates statistical distribution and gradient
information from others. Specifically, FedGPS statically modifies each client's
learning objective to implicitly model the global data distribution using
surrogate information, while dynamically adjusting local update directions with
gradient information from other clients at each round. Extensive experiments
show that FedGPS outperforms state-of-the-art methods across diverse
heterogeneity scenarios, validating its effectiveness and robustness. The code
is available at: https://github.com/CUHK-AIM-Group/FedGPS.

</details>


### [124] [Optimistic Task Inference for Behavior Foundation Models](https://arxiv.org/abs/2510.20264)
*Thomas Rupf,Marco Bagatella,Marin Vlastelica,Andreas Krause*

Main category: cs.LG

TL;DR: 提出OpTI - BFM解决行为基础模型（BFMs）零样本强化学习的数据效率问题，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有BFMs在零样本强化学习中数据效率低，需计算推理数据集的奖励，或需访问奖励函数形式或大量标注工作。

Method: 提出OpTI - BFM，一种乐观决策准则，直接对奖励函数的不确定性建模，指导BFMs进行数据收集以进行任务推理，并给出训练良好的BFMs的后悔界。

Result: 在既定零样本基准上评估，OpTI - BFM使基于后继特征的BFMs能在少量情节中识别并优化未见奖励函数，计算开销小。

Conclusion: OpTI - BFM能有效解决BFMs在零样本强化学习中的数据效率问题。

Abstract: Behavior Foundation Models (BFMs) are capable of retrieving high-performing
policy for any reward function specified directly at test-time, commonly
referred to as zero-shot reinforcement learning (RL). While this is a very
efficient process in terms of compute, it can be less so in terms of data: as a
standard assumption, BFMs require computing rewards over a non-negligible
inference dataset, assuming either access to a functional form of rewards, or
significant labeling efforts. To alleviate these limitations, we tackle the
problem of task inference purely through interaction with the environment at
test-time. We propose OpTI-BFM, an optimistic decision criterion that directly
models uncertainty over reward functions and guides BFMs in data collection for
task inference. Formally, we provide a regret bound for well-trained BFMs
through a direct connection to upper-confidence algorithms for linear bandits.
Empirically, we evaluate OpTI-BFM on established zero-shot benchmarks, and
observe that it enables successor-features-based BFMs to identify and optimize
an unseen reward function in a handful of episodes with minimal compute
overhead. Code is available at https://github.com/ThomasRupf/opti-bfm.

</details>


### [125] [ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases](https://arxiv.org/abs/2510.20270)
*Ziqian Zhong,Aditi Raghunathan,Nicholas Carlini*

Main category: cs.LG

TL;DR: 引入ImpossibleBench框架衡量大语言模型利用测试用例走捷径的倾向，展示其在研究模型行为、上下文工程和开发监测工具方面的用途。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在完成任务时找捷径的倾向对可靠评估和部署构成风险，需量化、研究和缓解该行为。

Method: 创建ImpossibleBench框架，从现有基准创建任务的“不可能”变体，测量模型在这些任务上的“作弊率”。

Result: 展示了ImpossibleBench在研究模型行为、上下文工程和开发监测工具方面的实用性。

Conclusion: 希望ImpossibleBench能为构建更健壮可靠的大语言模型系统提供有用框架。

Abstract: The tendency to find and exploit "shortcuts" to complete tasks poses
significant risks for reliable assessment and deployment of large language
models (LLMs). For example, an LLM agent with access to unit tests may delete
failing tests rather than fix the underlying bug. Such behavior undermines both
the validity of benchmark results and the reliability of real-world LLM coding
assistant deployments.
  To quantify, study, and mitigate such behavior, we introduce ImpossibleBench,
a benchmark framework that systematically measures LLM agents' propensity to
exploit test cases. ImpossibleBench creates "impossible" variants of tasks from
existing benchmarks like LiveCodeBench and SWE-bench by introducing direct
conflicts between the natural-language specification and the unit tests. We
measure an agent's "cheating rate" as its pass rate on these impossible tasks,
where any pass necessarily implies a specification-violating shortcut.
  As a practical framework, ImpossibleBench is not just an evaluation but a
versatile tool. We demonstrate its utility for: (1) studying model behaviors,
revealing more fine-grained details of cheating behaviors from simple test
modification to complex operator overloading; (2) context engineering, showing
how prompt, test access and feedback loop affect cheating rates; and (3)
developing monitoring tools, providing a testbed with verified deceptive
solutions. We hope ImpossibleBench serves as a useful framework for building
more robust and reliable LLM systems.
  Our implementation can be found at
https://github.com/safety-research/impossiblebench.

</details>


### [126] [Scalable GPU-Accelerated Euler Characteristic Curves: Optimization and Differentiable Learning for PyTorch](https://arxiv.org/abs/2510.20271)
*Udit Saxena*

Main category: cs.LG

TL;DR: 提出优化的GPU内核用于欧拉特征曲线（ECC）计算，并引入可微的PyTorch层实现端到端学习。


<details>
  <summary>Details</summary>
Motivation: 拓扑特征在深度学习中的实际应用需要计算效率和可微性。

Method: 使用优化的CUDA内核进行ECC计算，采用128B合并访问和分层共享内存累积；通过可微欧拉特征变换式的Sigmoid松弛学习阈值。

Result: 在合成网格上比先前的GPU实现有16 - 2000倍的加速。

Conclusion: 提出的方法具有计算效率和可微性，讨论了下游相关性并给出扩展思路以促进应用。

Abstract: Topological features capture global geometric structure in imaging data, but
practical adoption in deep learning requires both computational efficiency and
differentiability. We present optimized GPU kernels for the Euler
Characteristic Curve (ECC) computation achieving 16-2000\"O speedups over prior
GPU implementations on synthetic grids, and introduce a differentiable PyTorch
layer enabling end-to-end learning. Our CUDA kernels, optimized for Ampere GPUs
use 128B-coalesced access and hierarchical shared-memory accumulation. Our
PyTorch layer learns thresholds in a single direction via a Differentiable
Euler Characteristic Transform-style sigmoid relaxation. We discuss downstream
relevance, including applications highlighted by prior ECC work, and outline
batching/multi-GPU extensions to broaden adoption.

</details>


### [127] [Limits of PRM-Guided Tree Search for Mathematical Reasoning with LLMs](https://arxiv.org/abs/2510.20272)
*Tristan Cinquin,Geoff Pleiss,Agustinus Kristiadi*

Main category: cs.LG

TL;DR: 研究PRM引导的树搜索能否改善大语言模型数学推理，发现相比Best-of-N无显著提升，且存在诸多问题，需不同奖励建模。


<details>
  <summary>Details</summary>
Motivation: 现有Best-of-N线性结构无法捕捉复杂问题解决的分支和探索性，探索PRM引导的树搜索能否改善大语言模型数学推理。

Method: 提出自适应算法最大化过程奖励模型（PRM）分数，用Qwen2.5 - Math - 7B - Instruct及相关PRM对23个不同数学问题进行案例研究。

Result: 1. 相比Best-of-N无显著提升且成本更高；2. 蒙特卡罗树搜索和束搜索表现更好；3. PRM对状态值近似差且可靠性随推理深度下降；4. PRM分布外泛化能力差。

Conclusion: 树搜索依赖不可靠的PRM分数导致表现不佳，在有效提升大语言模型数学推理前需要不同的奖励建模。

Abstract: While chain-of-thought prompting with Best-of-N (BoN) selection has become
popular for mathematical reasoning in large language models (LLMs), its linear
structure fails to capture the branching and exploratory nature of complex
problem-solving. In this work, we propose an adaptive algorithm to maximize
process reward model (PRM) scores over the intractable action space, and
investigate whether PRM-guided tree search can improve mathematical reasoning
by exploring multiple partial solution paths. Across $23$ diverse mathematical
problems using Qwen2.5-Math-7B-Instruct with its associated PRM as a case
study, we find that: (1) PRM-guided tree search shows no statistically
significant improvements over BoN despite higher costs, (2) Monte Carlo tree
search and beam search outperform other PRM-guided tree search methods, (3)
PRMs poorly approximate state values and their reliability degrades with
reasoning depth, and (4) PRMs generalize poorly out of distribution. This
underperformance stems from tree search's greater reliance on unreliable PRM
scores, suggesting different reward modeling is necessary before tree search
can effectively enhance mathematical reasoning in LLMs.

</details>


### [128] [SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series](https://arxiv.org/abs/2510.20273)
*Qitai Tan,Yiyun Chen,Mo Li,Ruiwen Gu,Yilin Su,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 现有深度学习时间序列预测模型在实际应用中表现不佳，本文提出SynTSBench评估范式，实验表明当前模型未普遍达到最优基线。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习架构黑盒性质和现有评估框架局限性导致的模型在实际应用中表现不佳、难以选择合适模型的问题。

Method: 提出SynTSBench评估范式，通过可编程特征配置评估模型，有时间特征分解与能力映射、数据不规则性下的鲁棒性分析、理论最优基准测试三个核心分析维度。

Result: 实验显示当前深度学习模型并非在所有时间特征类型上都接近最优基线。

Conclusion: SynTSBench评估范式可系统评估时间序列预测模型，指出当前模型存在不足。

Abstract: Recent advances in deep learning have driven rapid progress in time series
forecasting, yet many state-of-the-art models continue to struggle with robust
performance in real-world applications, even when they achieve strong results
on standard benchmark datasets. This persistent gap can be attributed to the
black-box nature of deep learning architectures and the inherent limitations of
current evaluation frameworks, which frequently lack the capacity to provide
clear, quantitative insights into the specific strengths and weaknesses of
different models, thereby complicating the selection of appropriate models for
particular forecasting scenarios. To address these issues, we propose a
synthetic data-driven evaluation paradigm, SynTSBench, that systematically
assesses fundamental modeling capabilities of time series forecasting models
through programmable feature configuration. Our framework isolates confounding
factors and establishes an interpretable evaluation system with three core
analytical dimensions: (1) temporal feature decomposition and capability
mapping, which enables systematic evaluation of model capacities to learn
specific pattern types; (2) robustness analysis under data irregularities,
which quantifies noise tolerance thresholds and anomaly recovery capabilities;
and (3) theoretical optimum benchmarking, which establishes performance
boundaries for each pattern type-enabling direct comparison between model
predictions and mathematical optima. Our experiments show that current deep
learning models do not universally approach optimal baselines across all types
of temporal features.The code is available at
https://github.com/TanQitai/SynTSBench

</details>


### [129] [KCM: KAN-Based Collaboration Models Enhance Pretrained Large Models](https://arxiv.org/abs/2510.20278)
*Guangyu Dai,Siliang Tang,Yueting Zhuang*

Main category: cs.LG

TL;DR: 本文提出基于KAN的协作模型KCM改进大小模型协作框架，在语言、视觉等任务中实验表明，KCM能减少大模型推理调用、降低计算资源消耗、减轻灾难性遗忘并提升长尾数据准确率，性能优于基于MLP的模型。


<details>
  <summary>Details</summary>
Motivation: 现有大小模型协作范式存在准确率下降、灾难性遗忘加剧和幻觉问题，需要改进协作方法。

Method: 提出基于KAN的协作模型KCM，KAN是不同于传统MLP的神经网络架构，具有更好的可视化和可解释性，能减轻灾难性遗忘。

Result: 使用KCM的大小模型协作框架相比纯大模型方法，减少大模型推理调用，保持相近任务准确率，降低计算资源消耗；KAN小协作模型减轻灾难性遗忘，提升长尾数据准确率；KCM各项指标优于基于MLP的小协作模型MCM。

Conclusion: KCM在大小模型协作中表现优异，是一种有效的改进方案。

Abstract: In recent years, Pretrained Large Models(PLMs) researchers proposed
large-small model collaboration frameworks, leveraged easily trainable small
models to assist large models, aim to(1) significantly reduce computational
resource consumption while maintaining comparable accuracy, and (2) enhance
large model performance in specialized domain tasks. However, this
collaborative paradigm suffers from issues such as significant accuracy
degradation, exacerbated catastrophic forgetting, and amplified hallucination
problems induced by small model knowledge. To address these challenges, we
propose a KAN-based Collaborative Model (KCM) as an improved approach to
large-small model collaboration. The KAN utilized in KCM represents an
alternative neural network architecture distinct from conventional MLPs.
Compared to MLPs, KAN offers superior visualizability and interpretability
while mitigating catastrophic forgetting. We deployed KCM in large-small model
collaborative systems across three scenarios: language, vision, and
vision-language cross-modal tasks. The experimental results demonstrate that,
compared with pure large model approaches, the large-small model collaboration
framework utilizing KCM as the collaborative model significantly reduces the
number of large model inference calls while maintaining near-identical task
accuracy, thereby substantially lowering computational resource consumption.
Concurrently, the KAN-based small collaborative model markedly mitigates
catastrophic forgetting, leading to significant accuracy improvements for
long-tail data. The results reveal that KCM demonstrates superior performance
across all metrics compared to MLP-based small collaborative models (MCM).

</details>


### [130] [ResearchGPT: Benchmarking and Training LLMs for End-to-End Computer Science Research Workflows](https://arxiv.org/abs/2510.20279)
*Penghao Wang,Yuhao Zhou,Mengxuan Wu,Ziheng Qin,Bangyuan Zhu,Shengbin Huang,Xuanlei Zhao,Panpan Zhang,Xiaojiang Peng,Yuzhang Shang,Jianfei Yang,Zheng Zhu,Tianlong Chen,Zhangyang Wang,Kai Wang*

Main category: cs.LG

TL;DR: 本文提出ResearchGPT愿景，构建CS - 54k语料库，衍生CS - 4k基准和CS - 50k训练集，实验表明高质量数据领域对齐训练对提升AI科研辅助能力更重要，并发布相关数据集。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，期望构建能在整个科研过程辅助人类的AI协作系统ResearchGPT，需要评估端到端工作流的严格基准。

Method: 构建基于14k篇CC许可论文的科学问答对语料库CS - 54k，采用结合检索增强生成和多阶段质量控制的可扩展、基于论文的流程；从统一语料库衍生CS - 4k基准和CS - 50k训练集。

Result: CS - 4k能区分最先进大语言模型的能力层级；在CS - 50k上训练的开放模型有显著提升，7B规模模型表现优于许多更大的专有系统。

Conclusion: 使AI模型成为更好的科研助手更依赖高质量数据的领域对齐训练，而非预训练规模或通用基准性能。

Abstract: As large language models (LLMs) advance, the ultimate vision for their role
in science is emerging: we could build an AI collaborator to effectively assist
human beings throughout the entire scientific research process. We refer to
this envisioned system as ResearchGPT. Given that scientific research
progresses through multiple interdependent phases, achieving this vision
requires rigorous benchmarks that evaluate the end-to-end workflow rather than
isolated sub-tasks. To this end, we contribute CS-54k, a high-quality corpus of
scientific Q&A pairs in computer science, built from 14k CC-licensed papers. It
is constructed through a scalable, paper-grounded pipeline that combines
retrieval-augmented generation (RAG) with multi-stage quality control to ensure
factual grounding. From this unified corpus, we derive two complementary
subsets: CS-4k, a carefully curated benchmark for evaluating AI's ability to
assist scientific research, and CS-50k, a large-scale training dataset.
Extensive experiments demonstrate that CS-4k stratifies state-of-the-art LLMs
into distinct capability tiers. Open models trained on CS-50k with supervised
training and reinforcement learning demonstrate substantial improvements. Even
7B-scale models, when properly trained, outperform many larger proprietary
systems, such as GPT-4.1, GPT-4o, and Gemini 2.5 Pro. This indicates that
making AI models better research assistants relies more on domain-aligned
training with high-quality data than on pretraining scale or general benchmark
performance. We release CS-4k and CS-50k in the hope of fostering AI systems as
reliable collaborators in CS research.

</details>


### [131] [Quantifying Distributional Invariance in Causal Subgraph for IRM-Free Graph Generalization](https://arxiv.org/abs/2510.20295)
*Yang Qiu,Yixiong Zou,Jun Wang,Wei Liu,Xiangyu Fu,Ruixuan Li*

Main category: cs.LG

TL;DR: 提出一种无IRM方法捕获因果子图，实验表明在图泛化上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于IRM框架的图神经网络处理分布偏移下的分布外泛化问题时，需要昂贵环境注释或启发式生成合成分割，存在局限性。

Method: 识别出因果子图在不同环境下分布变化比非因果组件小，提出不变分布准则并理论证明；揭示分布偏移和表示范数的定量关系以识别因果子图；引入范数引导的不变分布目标提出无IRM方法。

Result: 在两个广泛使用的基准上的大量实验表明，该方法在图泛化方面始终优于现有方法。

Conclusion: 所提出的无IRM方法在处理图神经网络分布偏移下的分布外泛化问题上有效。

Abstract: Out-of-distribution generalization under distributional shifts remains a
critical challenge for graph neural networks. Existing methods generally adopt
the Invariant Risk Minimization (IRM) framework, requiring costly environment
annotations or heuristically generated synthetic splits. To circumvent these
limitations, in this work, we aim to develop an IRM-free method for capturing
causal subgraphs. We first identify that causal subgraphs exhibit substantially
smaller distributional variations than non-causal components across diverse
environments, which we formalize as the Invariant Distribution Criterion and
theoretically prove in this paper. Building on this criterion, we
systematically uncover the quantitative relationship between distributional
shift and representation norm for identifying the causal subgraph, and
investigate its underlying mechanisms in depth. Finally, we propose an IRM-free
method by introducing a norm-guided invariant distribution objective for causal
subgraph discovery and prediction. Extensive experiments on two widely used
benchmarks demonstrate that our method consistently outperforms
state-of-the-art methods in graph generalization.

</details>


### [132] [DB-FGA-Net: Dual Backbone Frequency Gated Attention Network for Multi-Class Classification with Grad-CAM Interpretability](https://arxiv.org/abs/2510.20299)
*Saraf Anzum Shreya,MD. Abu Ismail Siddique,Sharaf Tasnim*

Main category: cs.LG

TL;DR: 提出无数据增强的双骨干网络DB - FGA - Net用于脑肿瘤分类，准确率高，有可视化与GUI，具临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的脑肿瘤分类方法依赖大量数据增强，限制泛化性与临床应用可信度，需更好方法。

Method: 提出整合VGG16和Xception的双骨干网络，加入频率门控注意力（FGA）模块；集成Grad - CAM可视化肿瘤区域；开发GUI。

Result: 在7K - DS数据集上，4类、3类、2类设置下准确率分别达99.24%、98.68%、99.85%；在3K - DS数据集上准确率95.77%，优于基线和现有方法。

Conclusion: 无增强、可解释、可部署的深度学习模型DB - FGA - Net在脑肿瘤诊断临床转化方面有强大潜力。

Abstract: Brain tumors are a challenging problem in neuro-oncology, where early and
precise diagnosis is important for successful treatment. Deep learning-based
brain tumor classification methods often rely on heavy data augmentation which
can limit generalization and trust in clinical applications. In this paper, we
propose a double-backbone network integrating VGG16 and Xception with a
Frequency-Gated Attention (FGA) Block to capture complementary local and global
features. Unlike previous studies, our model achieves state-of-the-art
performance without augmentation which demonstrates robustness to variably
sized and distributed datasets. For further transparency, Grad-CAM is
integrated to visualize the tumor regions based on which the model is giving
prediction, bridging the gap between model prediction and clinical
interpretability. The proposed framework achieves 99.24\% accuracy on the 7K-DS
dataset for the 4-class setting, along with 98.68\% and 99.85\% in the 3-class
and 2-class settings, respectively. On the independent 3K-DS dataset, the model
generalizes with 95.77\% accuracy, outperforming baseline and state-of-the-art
methods. To further support clinical usability, we developed a graphical user
interface (GUI) that provides real-time classification and Grad-CAM-based tumor
localization. These findings suggest that augmentation-free, interpretable, and
deployable deep learning models such as DB-FGA-Net hold strong potential for
reliable clinical translation in brain tumor diagnosis.

</details>


### [133] [InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling](https://arxiv.org/abs/2510.20302)
*Yuhang Wang*

Main category: cs.LG

TL;DR: 提出混合架构InvDec用于多变量时间序列预测，结合PatchTST的InvDec - PatchTST在高维数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列预测方法要么忽略变量相关性，要么牺牲时间编码，需新方法同时处理时间模式和变量依赖。

Method: 提出InvDec架构，结合基于补丁的时间编码器和通过变量自注意力在变量维度操作的倒置解码器，引入延迟变量嵌入和自适应残差融合机制。

Result: 在七个基准测试中，InvDec - PatchTST在高维数据集上有显著提升，如在Electricity上MSE降低20.9%等，在低维ETT数据集上也有竞争力。

Conclusion: InvDec优势随数据集维度增加而增大，表明变量增多时跨变量建模很关键。

Abstract: Multivariate time series forecasting requires simultaneously modeling
temporal patterns and cross-variate dependencies. Channel-independent methods
such as PatchTST excel at temporal modeling but ignore variable correlations,
while pure variate-attention approaches such as iTransformer sacrifice temporal
encoding. We proposeInvDec (Inverted Decoder), a hybrid architecture that
achieves principled separation between temporal encoding and variate-level
decoding. InvDec combines a patch-based temporal encoder with an inverted
decoder operating on the variate dimension through variate-wise self-attention.
We introduce delayed variate embeddings that enrich variable-specific
representations only after temporal encoding, preserving temporal feature
integrity. An adaptive residual fusion mechanism dynamically balances temporal
and variate information across datasets of varying dimensions. Instantiating
InvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on seven
benchmarks demonstrate significant gains on high-dimensional datasets: 20.9%
MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and
2.7% gain on Traffic compared to PatchTST, while maintaining competitive
performance on low-dimensional ETT datasets. Ablation studies validate each
component, and analysis reveals that InvDec's advantage grows with dataset
dimensionality, confirming that cross-variate modeling becomes critical as the
number of variables increases.

</details>


### [134] [LEGO: A Lightweight and Efficient Multiple-Attribute Unlearning Framework for Recommender Systems](https://arxiv.org/abs/2510.20327)
*Fengyuan Yu,Yuyuan Li,Xiaohua Feng,Junjie Fang,Tao Wang,Chaochao Chen*

Main category: cs.LG

TL;DR: 为解决推荐系统中多敏感属性动态去学习问题，提出轻量级高效的多属性去学习框架LEGO，实验证明其有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: 现有单属性去学习方法无法满足现实中多敏感属性和动态隐私保护需求。

Method: 将多属性去学习过程分为嵌入校准和灵活组合两步，把去学习过程构建为互信息最小化问题。

Result: 在三个真实数据集和三种推荐模型上的实验证明了框架的有效性和高效性。

Conclusion: 提出的LEGO框架能有效解决多敏感属性动态去学习问题。

Abstract: With the growing demand for safeguarding sensitive user information in
recommender systems, recommendation attribute unlearning is receiving
increasing attention. Existing studies predominantly focus on single-attribute
unlearning. However, privacy protection requirements in the real world often
involve multiple sensitive attributes and are dynamic. Existing
single-attribute unlearning methods cannot meet these real-world requirements
due to i) CH1: the inability to handle multiple unlearning requests
simultaneously, and ii) CH2: the lack of efficient adaptability to dynamic
unlearning needs. To address these challenges, we propose LEGO, a lightweight
and efficient multiple-attribute unlearning framework. Specifically, we divide
the multiple-attribute unlearning process into two steps: i) Embedding
Calibration removes information related to a specific attribute from user
embedding, and ii) Flexible Combination combines these embeddings into a single
embedding, protecting all sensitive attributes. We frame the unlearning process
as a mutual information minimization problem, providing LEGO a theoretical
guarantee of simultaneous unlearning, thereby addressing CH1. With the two-step
framework, where Embedding Calibration can be performed in parallel and
Flexible Combination is flexible and efficient, we address CH2. Extensive
experiments on three real-world datasets across three representative
recommendation models demonstrate the effectiveness and efficiency of our
proposed framework. Our code and appendix are available at
https://github.com/anonymifish/lego-rec-multiple-attribute-unlearning.

</details>


### [135] [Synthetic Data for Robust Runway Detection](https://arxiv.org/abs/2510.20349)
*Estelle Chigot,Dennis G. Wilson,Meriem Ghrib,Fabrice Jimenez,Thomas Oberlin*

Main category: cs.LG

TL;DR: 文章提出基于商业飞行模拟器的图像生成方法用于跑道检测，结合少量真实标注图像，使标准目标检测模型准确预测，还评估模型在不利条件下的鲁棒性及定制域适应策略的效果。


<details>
  <summary>Details</summary>
Motivation: 深度视觉模型训练的数据收集和标注成本高，尤其在关键应用中，生成合成图像是解决数据覆盖问题的有吸引力方案。

Method: 提出基于商业飞行模拟器的图像生成方法，结合少量真实标注图像，控制图像生成与数据集成，采用定制域适应策略。

Result: 标准目标检测模型能实现准确预测，评估了模型在不利条件（如夜间图像）下的鲁棒性。

Conclusion: 通过生成合成图像结合真实数据的方法，可使目标检测模型在跑道检测中准确预测，定制域适应策略有应用价值。

Abstract: Deep vision models are now mature enough to be integrated in industrial and
possibly critical applications such as autonomous navigation. Yet, data
collection and labeling to train such models requires too much efforts and
costs for a single company or product. This drawback is more significant in
critical applications, where training data must include all possible conditions
including rare scenarios. In this perspective, generating synthetic images is
an appealing solution, since it allows a cheap yet reliable covering of all the
conditions and environments, if the impact of the synthetic-to-real
distribution shift is mitigated. In this article, we consider the case of
runway detection that is a critical part in autonomous landing systems
developed by aircraft manufacturers. We propose an image generation approach
based on a commercial flight simulator that complements a few annotated real
images. By controlling the image generation and the integration of real and
synthetic data, we show that standard object detection models can achieve
accurate prediction. We also evaluate their robustness with respect to adverse
conditions, in our case nighttime images, that were not represented in the real
data, and show the interest of using a customized domain adaptation strategy.

</details>


### [136] [Ask a Strong LLM Judge when Your Reward Model is Uncertain](https://arxiv.org/abs/2510.20369)
*Zhenghao Xu,Qin Lu,Qingru Zhang,Liang Qiu,Ilgee Hong,Changlong Yu,Wenlin Yao,Yao Liu,Haoming Jiang,Lihong Li,Hyokun Yun,Tuo Zhao*

Main category: cs.LG

TL;DR: 提出基于不确定性的路由框架，结合快速奖励模型和强大但昂贵的大语言模型评判器，在奖励模型基准测试和下游对齐实验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 经典奖励模型易受奖励破解影响且对分布外输入泛化能力差，强大的大语言模型评判器推理成本高，限制在线RLHF应用。

Method: 提出基于不确定性的路由框架，将策略梯度方法中的优势估计表述为成对偏好分类，以不确定性量化指导路由，不确定的对交给大语言模型评判器，有信心的对由奖励模型评估。

Result: 在奖励模型基准测试中，基于不确定性的路由策略在相同成本下显著优于随机调用评判器，下游对齐结果显示其能有效改善在线RLHF。

Conclusion: 所提出的基于不确定性的路由框架能有效结合快速奖励模型和强大但昂贵的大语言模型评判器，提高在线RLHF效果。

Abstract: Reward model (RM) plays a pivotal role in reinforcement learning with human
feedback (RLHF) for aligning large language models (LLMs). However, classical
RMs trained on human preferences are vulnerable to reward hacking and
generalize poorly to out-of-distribution (OOD) inputs. By contrast, strong LLM
judges equipped with reasoning capabilities demonstrate superior
generalization, even without additional training, but incur significantly
higher inference costs, limiting their applicability in online RLHF. In this
work, we propose an uncertainty-based routing framework that efficiently
complements a fast RM with a strong but costly LLM judge. Our approach
formulates advantage estimation in policy gradient (PG) methods as pairwise
preference classification, enabling principled uncertainty quantification to
guide routing. Uncertain pairs are forwarded to the LLM judge, while confident
ones are evaluated by the RM. Experiments on RM benchmarks demonstrate that our
uncertainty-based routing strategy significantly outperforms random judge
calling at the same cost, and downstream alignment results showcase its
effectiveness in improving online RLHF.

</details>


### [137] [Hierarchical Time Series Forecasting with Robust Reconciliation](https://arxiv.org/abs/2510.20383)
*Shuhei Aikawa,Aru Suzuki,Kei Yoshitake,Kanata Teshigawara,Akira Iwabuchi,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.LG

TL;DR: 本文提出用于分层时间序列预测的鲁棒优化框架，考虑估计协方差矩阵的不确定性，实验表明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有分层预测方法用估计的协方差矩阵进行最优调和，真实与估计的协方差矩阵存在差距，可能降低预测性能。

Method: 引入估计协方差矩阵的不确定性集合，构建最小化最坏情况期望平方误差的调和问题，并转化为半定优化问题。

Result: 数值实验显示，提出的鲁棒调和方法比现有分层预测方法有更好的预测性能。

Conclusion: 将不确定性融入调和过程是有效的。

Abstract: This paper focuses on forecasting hierarchical time-series data, where each
higher-level observation equals the sum of its corresponding lower-level time
series. In such contexts, the forecast values should be coherent, meaning that
the forecast value of each parent series exactly matches the sum of the
forecast values of its child series. Existing hierarchical forecasting methods
typically generate base forecasts independently for each series and then apply
a reconciliation procedure to adjust them so that the resulting forecast values
are coherent across the hierarchy. These methods generally derive an optimal
reconciliation, using a covariance matrix of the forecast error. In practice,
however, the true covariance matrix is unknown and has to be estimated from
finite samples in advance. This gap between the true and estimated covariance
matrix may degrade forecast performance. To address this issue, we propose a
robust optimization framework for hierarchical reconciliation that accounts for
uncertainty in the estimated covariance matrix. We first introduce an
uncertainty set for the estimated covariance matrix and formulate a
reconciliation problem that minimizes the worst-case expected squared error
over this uncertainty set. We show that our problem can be cast as a
semidefinite optimization problem. Numerical experiments demonstrate that the
proposed robust reconciliation method achieved better forecast performance than
existing hierarchical forecasting methods, which indicates the effectiveness of
integrating uncertainty into the reconciliation process.

</details>


### [138] [Relative-Based Scaling Law for Neural Language Models](https://arxiv.org/abs/2510.20387)
*Baoqing Yue,Jinyuan Zhou,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 本文从相对排序角度研究缩放定律，提出RBP指标和相对缩放定律，实验验证其准确性，举例说明其应用，有助于更全面理解大语言模型缩放。


<details>
  <summary>Details</summary>
Motivation: 现有缩放定律研究主要用交叉熵评估指标，其忽略正确和错误标记的相对排序，而相对排序对语言模型很重要，需从相对排序角度研究缩放。

Method: 提出相对概率（RBP）指标，基于此建立相对缩放定律，在四个数据集和四个模型族上进行大量实验。

Result: 实验证明相对缩放定律的鲁棒性和准确性，举例说明其广泛应用。

Conclusion: 相对缩放定律补充了交叉熵视角，有助于更全面理解大语言模型缩放，为实践和理论探索提供有价值的见解。

Abstract: Scaling laws aim to accurately predict model performance across different
scales. Existing scaling-law studies almost exclusively rely on cross-entropy
as the evaluation metric. However, cross-entropy provides only a partial view
of performance: it measures the absolute probability assigned to the correct
token, but ignores the relative ordering between correct and incorrect tokens.
Yet, relative ordering is crucial for language models, such as in
greedy-sampling scenario. To address this limitation, we investigate scaling
from the perspective of relative ordering. We first propose the Relative-Based
Probability (RBP) metric, which quantifies the probability that the correct
token is ranked among the top predictions. Building on this metric, we
establish the Relative-Based Scaling Law, which characterizes how RBP improves
with increasing model size. Through extensive experiments on four datasets and
four model families spanning five orders of magnitude, we demonstrate the
robustness and accuracy of this law. Finally, we illustrate the broad
application of this law with two examples, namely providing a deeper
explanation of emergence phenomena and facilitating finding fundamental
theories of scaling laws. In summary, the Relative-Based Scaling Law
complements the cross-entropy perspective and contributes to a more complete
understanding of scaling large language models. Thus, it offers valuable
insights for both practical development and theoretical exploration.

</details>


### [139] [Balancing Specialization and Centralization: A Multi-Agent Reinforcement Learning Benchmark for Sequential Industrial Control](https://arxiv.org/abs/2510.20408)
*Tom Maus,Asma Atamna,Tobias Glasmachers*

Main category: cs.LG

TL;DR: 本文引入增强的行业启发基准环境，评估两种控制策略及动作掩码影响，指出动作空间约束重要性，为工业自动化多智能体强化学习提供测试平台。


<details>
  <summary>Details</summary>
Motivation: 强化学习在工业应用受限，现有学术基准与工业控制问题差异大，缺乏可迁移性。

Method: 引入结合SortingEnv和ContainerGym任务的顺序回收场景基准环境，评估模块化架构和整体式智能体两种控制策略，分析动作掩码影响。

Result: 无动作掩码时，智能体难学有效策略，模块化架构表现更好；应用动作掩码后，两种架构性能大幅提升，差距缩小。

Conclusion: 动作空间约束起决定性作用，随着动作复杂度降低，专业化优势减弱，该基准为工业自动化多智能体强化学习提供有价值测试平台。

Abstract: Autonomous control of multi-stage industrial processes requires both local
specialization and global coordination. Reinforcement learning (RL) offers a
promising approach, but its industrial adoption remains limited due to
challenges such as reward design, modularity, and action space management. Many
academic benchmarks differ markedly from industrial control problems, limiting
their transferability to real-world applications. This study introduces an
enhanced industry-inspired benchmark environment that combines tasks from two
existing benchmarks, SortingEnv and ContainerGym, into a sequential recycling
scenario with sorting and pressing operations. We evaluate two control
strategies: a modular architecture with specialized agents and a monolithic
agent governing the full system, while also analyzing the impact of action
masking. Our experiments show that without action masking, agents struggle to
learn effective policies, with the modular architecture performing better. When
action masking is applied, both architectures improve substantially, and the
performance gap narrows considerably. These results highlight the decisive role
of action space constraints and suggest that the advantages of specialization
diminish as action complexity is reduced. The proposed benchmark thus provides
a valuable testbed for exploring practical and robust multi-agent RL solutions
in industrial automation, while contributing to the ongoing debate on
centralization versus specialization.

</details>


### [140] [Why DPO is a Misspecified Estimator and How to Fix It](https://arxiv.org/abs/2510.20413)
*Aditya Gopalan,Sayak Ray Chowdhury,Debangshu Banerjee*

Main category: cs.LG

TL;DR: 分析DPO问题，提出AuxDPO并验证其性能


<details>
  <summary>Details</summary>
Motivation: 解决DPO存在的模型误设问题，如偏好顺序反转、策略奖励变差等

Method: 研究DPO统计估计问题和RLHF局部行为，提出在DPO损失函数引入辅助变量的AuxDPO

Result: 在多臂老虎机和大语言模型对齐任务中，AuxDPO表现更优

Conclusion: AuxDPO能以原则性方式接近RLHF解决方案，缓解DPO的误设问题

Abstract: Direct alignment algorithms such as Direct Preference Optimization (DPO)
fine-tune models based on preference data, using only supervised learning
instead of two-stage reinforcement learning with human feedback (RLHF). We show
that DPO encodes a statistical estimation problem over reward functions induced
by a parametric policy class. When the true reward function that generates
preferences cannot be realized via the policy class, DPO becomes misspecified,
resulting in failure modes such as preference order reversal, worsening of
policy reward, and high sensitivity to the input preference data distribution.
On the other hand, we study the local behavior of two-stage RLHF for a
parametric class and relate it to a natural gradient step in policy space. Our
fine-grained geometric characterization allows us to propose AuxDPO, which
introduces additional auxiliary variables in the DPO loss function to help move
towards the RLHF solution in a principled manner and mitigate the
misspecification in DPO. We empirically demonstrate the superior performance of
AuxDPO on didactic bandit settings as well as LLM alignment tasks.

</details>


### [141] [Addressing Mark Imbalance in Integration-free Neural Marked Temporal Point Processes](https://arxiv.org/abs/2510.20414)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yongli Ren,Yan Wang*

Main category: cs.LG

TL;DR: 针对标记时间点过程（MTPP）中事件标记分布不平衡问题，提出阈值方法和新的神经MTPP模型，实验显示其在事件标记和时间预测上性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有MTPP研究忽略现实应用中事件标记分布不平衡问题，这对稀有标记事件的预测性能有显著挑战。

Method: 提出阈值方法，通过学习阈值调整经标记先验概率归一化后的标记概率来优化标记预测；先预测标记再预测时间；开发新的神经MTPP模型以支持有效时间采样和标记概率估计。

Result: 在真实数据集上的大量实验表明，该解决方案在预测下一个事件标记和时间方面优于各种基线方法。

Conclusion: 所提出的方法和模型能有效解决MTPP中事件标记分布不平衡问题，提升预测性能。代码开源。

Abstract: Marked Temporal Point Process (MTPP) has been well studied to model the event
distribution in marked event streams, which can be used to predict the mark and
arrival time of the next event. However, existing studies overlook that the
distribution of event marks is highly imbalanced in many real-world
applications, with some marks being frequent but others rare. The imbalance
poses a significant challenge to the performance of the next event prediction,
especially for events of rare marks. To address this issue, we propose a
thresholding method, which learns thresholds to tune the mark probability
normalized by the mark's prior probability to optimize mark prediction, rather
than predicting the mark directly based on the mark probability as in existing
studies. In conjunction with this method, we predict the mark first and then
the time. In particular, we develop a novel neural MTPP model to support
effective time sampling and estimation of mark probability without
computationally expensive numerical improper integration. Extensive experiments
on real-world datasets demonstrate the superior performance of our solution
against various baselines for the next event mark and time prediction. The code
is available at https://github.com/undes1red/IFNMTPP.

</details>


### [142] [An Empirical Study of Sample Selection Strategies for Large Language Model Repair](https://arxiv.org/abs/2510.20428)
*Xuran Li,Jingyi Wang*

Main category: cs.LG

TL;DR: 研究对大语言模型修复的样本优先策略进行系统分析，评估五种方法，SAPS表现最佳，随机抽样对大模型有效，最优数据比例因模型和方法而异，基于选择的修复是维护LLM可靠性的有效范式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型有有毒或有偏差输出问题，事后模型修复成本高，需选择性使用修复数据，且现有工作未明确大生成模型行为修复时哪种采样标准最有效高效。

Method: 评估五种样本选择方法（随机抽样、K - Center、GraNd、CCS和提出的SAPS），通过毒性降低、困惑度和三个综合指标评估修复效果和权衡。

Result: SAPS在解毒、保留实用性和效率间取得最佳平衡，用更少数据实现相当或更好修复效果；随机抽样对大或健壮模型有效；高开销方法（CCS和GraNd）效益有限；最优数据比例取决于模型规模和修复方法。

Conclusion: 基于选择的修复是维护大语言模型可靠性的高效可扩展范式。

Abstract: Large language models (LLMs) are increasingly deployed in real-world systems,
yet they can produce toxic or biased outputs that undermine safety and trust.
Post-hoc model repair provides a practical remedy, but the high cost of
parameter updates motivates selective use of repair data. Despite extensive
prior work on data selection for model training, it remains unclear which
sampling criteria are most effective and efficient when applied specifically to
behavioral repair of large generative models. Our study presents a systematic
analysis of sample prioritization strategies for LLM repair. We evaluate five
representative selection methods, including random sampling, K-Center,
gradient-norm-based selection(GraNd), stratified coverage (CCS), and a
Semantic-Aware Prioritized Sampling (SAPS) approach we proposed. Repair
effectiveness and trade-offs are assessed through toxicity reduction,
perplexity on WikiText-2 and LAMBADA, and three composite metrics: the Repair
Proximity Score (RPS), the Overall Performance Score (OPS), and the Repair
Efficiency Score (RES). Experimental results show that SAPS achieves the best
balance between detoxification, utility preservation, and efficiency,
delivering comparable or superior repair outcomes with substantially less data.
Random sampling remains effective for large or robust models, while
high-overhead methods such as CCS and GraNd provide limited benefit. The
optimal data proportion depends on model scale and repair method, indicating
that sample selection should be regarded as a tunable component of repair
pipelines. Overall, these findings establish selection-based repair as an
efficient and scalable paradigm for maintaining LLM reliability.

</details>


### [143] [Explainable Benchmarking through the Lense of Concept Learning](https://arxiv.org/abs/2510.20439)
*Quannian Zhang,Michael Röder,Nikit Srivastava,N'Dah Jean Kouagou,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: 提出可解释基准测试概念，为基于知识图谱的问答系统实现该范式，评估显示方法表现好，用户研究效果佳。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试分析评估细节和推导见解是繁琐手动任务且结果易有偏差，需要新的基准测试方法。

Method: 使用为大型知识图谱开发的新颖概念学习方法PruneCEL来计算解释。

Result: PruneCEL在可解释基准测试任务上F1指标比现有技术最多高0.55分；用户研究中80%的情况多数参与者能基于解释准确预测系统行为。

Conclusion: 可解释基准测试方法有效，PruneCEL表现良好，代码和数据公开。

Abstract: Evaluating competing systems in a comparable way, i.e., benchmarking them, is
an undeniable pillar of the scientific method. However, system performance is
often summarized via a small number of metrics. The analysis of the evaluation
details and the derivation of insights for further development or use remains a
tedious manual task with often biased results. Thus, this paper argues for a
new type of benchmarking, which is dubbed explainable benchmarking. The aim of
explainable benchmarking approaches is to automatically generate explanations
for the performance of systems in a benchmark. We provide a first instantiation
of this paradigm for knowledge-graph-based question answering systems. We
compute explanations by using a novel concept learning approach developed for
large knowledge graphs called PruneCEL. Our evaluation shows that PruneCEL
outperforms state-of-the-art concept learners on the task of explainable
benchmarking by up to 0.55 points F1 measure. A task-driven user study with 41
participants shows that in 80\% of the cases, the majority of participants can
accurately predict the behavior of a system based on our explanations. Our code
and data are available at https://github.com/dice-group/PruneCEL/tree/K-cap2025

</details>


### [144] [MolBridge: Atom-Level Joint Graph Refinement for Robust Drug-Drug Interaction Event Prediction](https://arxiv.org/abs/2510.20448)
*Xuan Lin,Aocheng Ding,Tengfei Ma,Hua Liang,Zhe Quan*

Main category: cs.LG

TL;DR: 提出用于药物相互作用事件预测的MolBridge框架，在两个基准数据集上表现出色，提升预测准确性、鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖孤立药物表征，无法明确建模原子级跨分子相互作用，在不同分子复杂性和药物相互作用类型分布中效果有限，需要准确预测药物相互作用事件。

Method: 提出MolBridge框架，构建集成药物对原子结构的联合图，引入结构一致性模块迭代细化节点特征并保留全局结构上下文。

Result: 在两个基准数据集上，MolBridge始终优于最先进的基线模型，在长尾和归纳场景中表现出色。

Conclusion: 细粒度图细化在提高药物相互作用事件预测的准确性、鲁棒性和机制可解释性方面具有优势，为药物相互作用网络挖掘和分析开发了基于图的方法。

Abstract: Drug combinations offer therapeutic benefits but also carry the risk of
adverse drug-drug interactions (DDIs), especially under complex molecular
structures. Accurate DDI event prediction requires capturing fine-grained
inter-drug relationships, which are critical for modeling metabolic mechanisms
such as enzyme-mediated competition. However, existing approaches typically
rely on isolated drug representations and fail to explicitly model atom-level
cross-molecular interactions, limiting their effectiveness across diverse
molecular complexities and DDI type distributions. To address these
limitations, we propose MolBridge, a novel atom-level joint graph refinement
framework for robust DDI event prediction. MolBridge constructs a joint graph
that integrates atomic structures of drug pairs, enabling direct modeling of
inter-drug associations. A central challenge in such joint graph settings is
the potential loss of information caused by over-smoothing when modeling
long-range atomic dependencies. To overcome this, we introduce a structure
consistency module that iteratively refines node features while preserving the
global structural context. This joint design allows MolBridge to effectively
learn both local and global interaction outperforms state-of-the-art baselines,
achieving superior performance across long-tail and inductive scenarios.
patterns, yielding robust representations across both frequent and rare DDI
types. Extensive experiments on two benchmark datasets show that MolBridge
consistently. These results demonstrate the advantages of fine-grained graph
refinement in improving the accuracy, robustness, and mechanistic
interpretability of DDI event prediction.This work contributes to Web Mining
and Content Analysis by developing graph-based methods for mining and analyzing
drug-drug interaction networks.

</details>


### [145] [Intransitive Player Dominance and Market Inefficiency in Tennis Forecasting: A Graph Neural Network Approach](https://arxiv.org/abs/2510.20454)
*Lawrence Clegg,John Cartlidge*

Main category: cs.LG

TL;DR: 本文用图神经网络方法处理网球比赛中的非传递性球员优势问题，在高非传递性比赛中取得良好预测效果和显著回报。


<details>
  <summary>Details</summary>
Motivation: 网球比赛中存在非传递性球员优势，但少有预测方法考虑，需解决此问题。

Method: 采用图神经网络方法，通过时间有向图明确建模非传递关系，球员为节点，比赛结果为有向边。

Result: 博彩公司Pinnacle Sports难以处理高非传递性比赛，模型预测准确率65.7%，Brier分数0.215，1903次投注用Kelly下注法获3.26% ROI正回报。

Conclusion: 博彩市场在处理非传递性比赛中有低效性，该方法能成功利用此低效性。

Abstract: Intransitive player dominance, where player A beats B, B beats C, but C beats
A, is common in competitive tennis. Yet, there are few known attempts to
incorporate it within forecasting methods. We address this problem with a graph
neural network approach that explicitly models these intransitive relationships
through temporal directed graphs, with players as nodes and their historical
match outcomes as directed edges. We find the bookmaker Pinnacle Sports poorly
handles matches with high intransitive complexity and posit that our
graph-based approach is uniquely positioned to capture relational dynamics in
these scenarios. When selectively betting on higher intransitivity matchups
with our model (65.7% accuracy, 0.215 Brier Score), we achieve significant
positive returns of 3.26% ROI with Kelly staking over 1903 bets, suggesting a
market inefficiency in handling intransitive matchups that our approach
successfully exploits.

</details>


### [146] [Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models](https://arxiv.org/abs/2510.20468)
*Tomáš Souček,Sylvestre-Alvise Rebuffi,Pierre Fernandez,Nikola Jovanović,Hady Elsahar,Valeriu Lacatusu,Tuan Tran,Alexandre Mourachko*

Main category: cs.LG

TL;DR: 本文研究后处理图像水印的伪造问题，提出偏好模型评估图像是否有水印，展示模型去除和伪造水印的能力，并评估方法有效性，质疑当前水印方法的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的普及和法律压力增加，水印对于确保内容真实性和归属很重要，但水印伪造问题研究不足。

Method: 引入偏好模型评估图像是否有水印，该模型用纯程序生成图像的排序损失训练；通过反向传播优化输入图像，展示模型去除和伪造水印的能力；在多种后处理图像水印模型上评估方法。

Result: 提出的方法能有效伪造水印。

Conclusion: 当前水印方法的安全性存疑。

Abstract: Recent years have seen a surge in interest in digital content watermarking
techniques, driven by the proliferation of generative models and increased
legal pressure. With an ever-growing percentage of AI-generated content
available online, watermarking plays an increasingly important role in ensuring
content authenticity and attribution at scale. There have been many works
assessing the robustness of watermarking to removal attacks, yet, watermark
forging, the scenario when a watermark is stolen from genuine content and
applied to malicious content, remains underexplored. In this work, we
investigate watermark forging in the context of widely used post-hoc image
watermarking. Our contributions are as follows. First, we introduce a
preference model to assess whether an image is watermarked. The model is
trained using a ranking loss on purely procedurally generated images without
any need for real watermarks. Second, we demonstrate the model's capability to
remove and forge watermarks by optimizing the input image through
backpropagation. This technique requires only a single watermarked image and
works without knowledge of the watermarking model, making our attack much
simpler and more practical than attacks introduced in related work. Third, we
evaluate our proposed method on a variety of post-hoc image watermarking
models, demonstrating that our approach can effectively forge watermarks,
questioning the security of current watermarking approaches. Our code and
further resources are publicly available.

</details>


### [147] [Bi-CoG: Bi-Consistency-Guided Self-Training for Vision-Language Models](https://arxiv.org/abs/2510.20477)
*Rui Zhu,Song-Lin Lv,Zi-Kang Wang,Lan-Zhe Guo*

Main category: cs.LG

TL;DR: 提出Bi - CoG方法解决半监督微调现有方法的局限，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有半监督微调方法依赖预测一致性或预定义置信阈值，存在模型偏差和超参数敏感问题。

Method: 提出Bi - CoG方法，通过利用模型间和模型内一致性以及误差感知动态伪标签分配策略来分配高质量、低偏差伪标签。

Result: 在14个数据集上的理论分析和大量实验表明，Bi - CoG能持续且显著提升现有方法的性能。

Conclusion: Bi - CoG是一种简单有效的即插即用方法，可解决现有半监督微调方法的局限。

Abstract: Exploiting unlabeled data through semi-supervised learning (SSL) or
leveraging pre-trained models via fine-tuning are two prevailing paradigms for
addressing label-scarce scenarios. Recently, growing attention has been given
to combining fine-tuning of pre-trained vision-language models (VLMs) with SSL,
forming the emerging paradigm of semi-supervised fine-tuning. However, existing
methods often suffer from model bias and hyperparameter sensitivity, due to
reliance on prediction consistency or pre-defined confidence thresholds. To
address these limitations, we propose a simple yet effective plug-and-play
methodology named
$\underline{\textbf{Bi-Co}}$nsistency-$\underline{\textbf{G}}$uided
Self-Training (Bi-CoG), which assigns high-quality and low-bias pseudo-labels,
by simultaneously exploiting inter-model and intra-model consistency, along
with an error-aware dynamic pseudo-label assignment strategy. Both theoretical
analysis and extensive experiments over 14 datasets demonstrate the
effectiveness of Bi-CoG, which consistently and significantly improves the
performance of existing methods.

</details>


### [148] [Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval](https://arxiv.org/abs/2510.20486)
*Fangjian Zhang,Xiaoyong Zhuge,Wenlan Wang,Haixia Xiao,Yuying Zhu,Siyang Cheng*

Main category: cs.LG

TL;DR: 本文指出人工智能在定量遥感中受标签分布不平衡的限制，提出Hurdle - IMDL框架解决降雨反演中的不平衡问题，经评估优于传统方法，可推广用于解决环境变量分布不平衡。


<details>
  <summary>Details</summary>
Motivation: 人工智能在定量遥感中的有效性受标签分布不平衡的限制，传统模型在处理降雨反演时对罕见样本（如大雨）的反演性能较差。

Method: 提出Hurdle - IMDL框架，采用分治法将降雨分布不平衡分解为零膨胀和长尾问题，用障碍模型处理零膨胀，IMDL处理长尾问题。

Result: 通过统计指标和中国东部雨天案例研究，证实Hurdle - IMDL优于传统、成本敏感、生成式和多任务学习方法，能有效缓解系统低估问题，显著改善大雨到极端降雨的反演。

Conclusion: IMDL是一种可推广的方法，能解决环境变量分布不平衡问题，提高对罕见但高影响事件的反演能力。

Abstract: Artificial intelligence has advanced quantitative remote sensing, yet its
effectiveness is constrained by imbalanced label distribution. This imbalance
leads conventionally trained models to favor common samples, which in turn
degrades retrieval performance for rare ones. Rainfall retrieval exemplifies
this issue, with performance particularly compromised for heavy rain. This
study proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework.
Following a divide-and-conquer strategy, imbalance in the rain distribution is
decomposed into two components: zero inflation, defined by the predominance of
non-rain samples; and long tail, defined by the disproportionate abundance of
light-rain samples relative to heavy-rain samples. A hurdle model is adopted to
handle the zero inflation, while IMDL is proposed to address the long tail by
transforming the learning object into an unbiased ideal inverse model.
Comprehensive evaluation via statistical metrics and case studies investigating
rainy weather in eastern China confirms Hurdle-IMDL's superiority over
conventional, cost-sensitive, generative, and multi-task learning methods. Its
key advancements include effective mitigation of systematic underestimation and
a marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a
generalizable approach for addressing imbalance in distributions of
environmental variables, enabling enhanced retrieval of rare yet high-impact
events.

</details>


### [149] [SheafAlign: A Sheaf-theoretic Framework for Decentralized Multimodal Alignment](https://arxiv.org/abs/2510.20540)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出SheafAlign框架解决传统多模态对齐方法在分布式场景的问题，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 传统多模态对齐方法假设各模态间存在冗余，在现实分布式场景不适用。

Method: 提出SheafAlign框架，用多个比较空间替代单空间对齐，通过层结构建模模态关系，利用基于去中心化对比学习的目标进行训练。

Result: 在多模态传感数据集实验中，零样本泛化、跨模态对齐表现出色，对缺失模态有鲁棒性，通信成本比现有基线低50%。

Conclusion: SheafAlign克服了先前方法的局限性，无需所有模态间的相互冗余，能保留共享和独特信息。

Abstract: Conventional multimodal alignment methods assume mutual redundancy across all
modalities, an assumption that fails in real-world distributed scenarios. We
propose SheafAlign, a sheaf-theoretic framework for decentralized multimodal
alignment that replaces single-space alignment with multiple comparison spaces.
This approach models pairwise modality relations through sheaf structures and
leverages decentralized contrastive learning-based objectives for training.
SheafAlign overcomes the limitations of prior methods by not requiring mutual
redundancy among all modalities, preserving both shared and unique information.
Experiments on multimodal sensing datasets show superior zero-shot
generalization, cross-modal alignment, and robustness to missing modalities,
with 50\% lower communication cost than state-of-the-art baselines.

</details>


### [150] [A Unified Framework for Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2510.20542)
*Jacopo Di Ventura,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 提出零样本强化学习统一框架，分类算法，推导界限，为未来研究奠基


<details>
  <summary>Details</summary>
Motivation: 零样本强化学习缺乏通用分析视角

Method: 引入一致符号和分类法，将算法分为直接表示和组合表示两类

Result: 突出方法共性与差异，推导后继特征方法扩展界限

Conclusion: 框架为零样本强化学习未来研究提供原则基础和清晰路径

Abstract: Zero-shot reinforcement learning (RL) has emerged as a setting for developing
general agents in an unsupervised manner, capable of solving downstream tasks
without additional training or planning at test-time. Unlike conventional RL,
which optimizes policies for a fixed reward, zero-shot RL requires agents to
encode representations rich enough to support immediate adaptation to any
objective, drawing parallels to vision and language foundation models. Despite
growing interest, the field lacks a common analytical lens.
  We present the first unified framework for zero-shot RL. Our formulation
introduces a consistent notation and taxonomy that organizes existing
approaches and allows direct comparison between them. Central to our framework
is the classification of algorithms into two families: direct representations,
which learn end-to-end mappings from rewards to policies, and compositional
representations, which decompose the representation leveraging the substructure
of the value function. Within this framework, we highlight shared principles
and key differences across methods, and we derive an extended bound for
successor-feature methods, offering a new perspective on their performance in
the zero-shot regime. By consolidating existing work under a common lens, our
framework provides a principled foundation for future research in zero-shot RL
and outlines a clear path toward developing more general agents.

</details>


### [151] [Structural Invariance Matters: Rethinking Graph Rewiring through Graph Metrics](https://arxiv.org/abs/2510.20556)
*Alexandre Benoit,Catherine Aitken,Yu He*

Main category: cs.LG

TL;DR: 本文系统分析图重连如何影响图结构指标及与下游任务性能的关系，发现成功的重连方法倾向保留局部结构。


<details>
  <summary>Details</summary>
Motivation: 图重连虽能缓解GNN和图Transformer的过压缩问题，但会改变图结构，目前不清楚应保留哪些结构属性以确保性能和结构保真度。

Method: 研究七种不同的重连策略，将局部和全局图属性的变化与节点分类准确率关联起来。

Result: 成功的重连方法倾向于保留局部结构，同时允许全局连接具有灵活性。

Conclusion: 研究结果为有效重连策略的设计提供新见解，弥合了图论与实际GNN优化之间的差距。

Abstract: Graph rewiring has emerged as a key technique to alleviate over-squashing in
Graph Neural Networks (GNNs) and Graph Transformers by modifying the graph
topology to improve information flow. While effective, rewiring inherently
alters the graph's structure, raising the risk of distorting important
topology-dependent signals. Yet, despite the growing use of rewiring, little is
known about which structural properties must be preserved to ensure both
performance gains and structural fidelity. In this work, we provide the first
systematic analysis of how rewiring affects a range of graph structural
metrics, and how these changes relate to downstream task performance. We study
seven diverse rewiring strategies and correlate changes in local and global
graph properties with node classification accuracy. Our results reveal a
consistent pattern: successful rewiring methods tend to preserve local
structure while allowing for flexibility in global connectivity. These findings
offer new insights into the design of effective rewiring strategies, bridging
the gap between graph theory and practical GNN optimization.

</details>


### [152] [Embedding the MLOps Lifecycle into OT Reference Models](https://arxiv.org/abs/2510.20590)
*Simon Schindler,Christoph Binder,Lukas Lürzer,Stefan Huber*

Main category: cs.LG

TL;DR: 本文分析MLOps与OT系统集成挑战，提出嵌入方法，评估模型适用性并举例，表明结构化适配可实现集成。


<details>
  <summary>Details</summary>
Motivation: MLOps在工业环境中应用时与OT系统集成存在重大挑战，需解决该问题。

Method: 分析MLOps与OT环境结合的基本障碍，提出将MLOps实践嵌入OT参考模型的系统方法，评估RAMI 4.0和ISA - 95对MLOps集成的适用性，进行MLOps生命周期组件到RAMI 4.0的详细映射并结合实际用例。

Result: 标准MLOps实践不能直接移植到OT环境。

Conclusion: 使用现有参考模型进行结构化适配能为MLOps与OT系统成功集成提供途径。

Abstract: Machine Learning Operations (MLOps) practices are increas- ingly adopted in
industrial settings, yet their integration with Opera- tional Technology (OT)
systems presents significant challenges. This pa- per analyzes the fundamental
obstacles in combining MLOps with OT en- vironments and proposes a systematic
approach to embed MLOps prac- tices into established OT reference models. We
evaluate the suitability of the Reference Architectural Model for Industry 4.0
(RAMI 4.0) and the International Society of Automation Standard 95 (ISA-95) for
MLOps integration and present a detailed mapping of MLOps lifecycle compo-
nents to RAMI 4.0 exemplified by a real-world use case. Our findings
demonstrate that while standard MLOps practices cannot be directly transplanted
to OT environments, structured adaptation using existing reference models can
provide a pathway for successful integration.

</details>


### [153] [Generalizable Reasoning through Compositional Energy Minimization](https://arxiv.org/abs/2510.20607)
*Alexandru Oarga,Yilun Du*

Main category: cs.LG

TL;DR: 提出通过学习子问题解空间能量景观进行推理泛化的新方法，结合子问题能量函数构建全局能量景观，引入PEM提高样本质量，在推理问题上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有端到端训练推理模型泛化能力有限，难以解决比训练时更复杂的问题。

Method: 学习小的、易处理子问题解空间的能量景观，测试时结合子问题能量函数构建全局能量景观，引入Parallel Energy Minimization (PEM) 提高样本质量。

Result: 在广泛的推理问题上评估，方法优于现有最先进方法。

Conclusion: 该方法能泛化到更大、更复杂的问题。

Abstract: Generalization is a key challenge in machine learning, specifically in
reasoning tasks, where models are expected to solve problems more complex than
those encountered during training. Existing approaches typically train
reasoning models in an end-to-end fashion, directly mapping input instances to
solutions. While this allows models to learn useful heuristics from data, it
often results in limited generalization beyond the training distribution. In
this work, we propose a novel approach to reasoning generalization by learning
energy landscapes over the solution spaces of smaller, more tractable
subproblems. At test time, we construct a global energy landscape for a given
problem by combining the energy functions of multiple subproblems. This
compositional approach enables the incorporation of additional constraints
during inference, allowing the construction of energy landscapes for problems
of increasing difficulty. To improve the sample quality from this newly
constructed energy landscape, we introduce Parallel Energy Minimization (PEM).
We evaluate our approach on a wide set of reasoning problems. Our method
outperforms existing state-of-the-art methods, demonstrating its ability to
generalize to larger and more complex problems. Project website can be found
at: https://alexoarga.github.io/compositional_reasoning/

</details>


### [154] [Convergence Analysis of SGD under Expected Smoothness](https://arxiv.org/abs/2510.20608)
*Yuta Kawamoto,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 本文对随机梯度下降（SGD）在期望平滑（ES）条件下进行收敛分析，细化ES条件、推导全梯度范数平方期望的界，并证明不同步长策略下的收敛率。


<details>
  <summary>Details</summary>
Motivation: 经典SGD分析假设过强或过粗，ES条件作为灵活替代出现，本文旨在对SGD在ES条件下进行收敛分析。

Method: （i）用解释和依赖采样的常数细化ES；（ii）推导全梯度范数平方期望的界；（iii）证明不同步长策略下带显式残差误差的O(1/K)收敛率。

Result: 完成了SGD在ES条件下的收敛分析，细化ES条件，得到全梯度范数平方期望的界和收敛率。

Conclusion: 本文的处理统一并扩展了近期相关研究。

Abstract: Stochastic gradient descent (SGD) is the workhorse of large-scale learning,
yet classical analyses rely on assumptions that can be either too strong
(bounded variance) or too coarse (uniform noise). The expected smoothness (ES)
condition has emerged as a flexible alternative that ties the second moment of
stochastic gradients to the objective value and the full gradient. This paper
presents a self-contained convergence analysis of SGD under ES. We (i) refine
ES with interpretations and sampling-dependent constants; (ii) derive bounds of
the expectation of squared full gradient norm; and (iii) prove $O(1/K)$ rates
with explicit residual errors for various step-size schedules. All proofs are
given in full detail in the appendix. Our treatment unifies and extends recent
threads (Khaled and Richt\'arik, 2020; Umeda and Iiduka, 2025).

</details>


### [155] [PSO-XAI: A PSO-Enhanced Explainable AI Framework for Reliable Breast Cancer Detection](https://arxiv.org/abs/2510.20611)
*Mirza Raquib,Niloy Das,Farida Siddiqi Prity,Arafath Al Fahim,Saydul Akbar Murad,Mohammad Amzad Hossain,MD Jiabul Hoque,Mohammad Ali Moni*

Main category: cs.LG

TL;DR: 文章提出结合定制PSO特征选择的框架用于乳腺癌诊断，经29种模型评估，表现优异，凸显群体智能与可解释ML结合潜力。


<details>
  <summary>Details</summary>
Motivation: 传统乳腺癌诊断方法存在变异性、成本高和误诊风险等局限，需更好的诊断手段。

Method: 提出结合定制PSO进行特征选择的集成框架，在29种不同模型上评估，结合交叉验证和可解释AI方法。

Result: 该方法在所有性能指标上达到99.1%的优异分数，有效降维并提供透明、与模型无关的解释。

Conclusion: 群体智能与可解释ML结合在乳腺癌诊断中有潜力，可实现可靠、可信和有临床意义的诊断。

Abstract: Breast cancer is considered the most critical and frequently diagnosed cancer
in women worldwide, leading to an increase in cancer-related mortality. Early
and accurate detection is crucial as it can help mitigate possible threats
while improving survival rates. In terms of prediction, conventional diagnostic
methods are often limited by variability, cost, and, most importantly, risk of
misdiagnosis. To address these challenges, machine learning (ML) has emerged as
a powerful tool for computer-aided diagnosis, with feature selection playing a
vital role in improving model performance and interpretability. This research
study proposes an integrated framework that incorporates customized Particle
Swarm Optimization (PSO) for feature selection. This framework has been
evaluated on a comprehensive set of 29 different models, spanning classical
classifiers, ensemble techniques, neural networks, probabilistic algorithms,
and instance-based algorithms. To ensure interpretability and clinical
relevance, the study uses cross-validation in conjunction with explainable AI
methods. Experimental evaluation showed that the proposed approach achieved a
superior score of 99.1\% across all performance metrics, including accuracy and
precision, while effectively reducing dimensionality and providing transparent,
model-agnostic explanations. The results highlight the potential of combining
swarm intelligence with explainable ML for robust, trustworthy, and clinically
meaningful breast cancer diagnosis.

</details>


### [156] [MS-BART: Unified Modeling of Mass Spectra and Molecules for Structure Elucidation](https://arxiv.org/abs/2510.20615)
*Yang Han,Pengyu Wang,Kai Yu,Xin Chen,Lu Chen*

Main category: cs.LG

TL;DR: 提出MS - BART框架解决质谱数据结构解析难题，经多步优化，评估显示其性能优、速度快。


<details>
  <summary>Details</summary>
Motivation: 质谱在分子鉴定重要，但从质谱数据进行结构解析因注释光谱稀缺而困难，大规模预训练应用受原始光谱信号复杂异质性阻碍。

Method: 提出MS - BART框架，将质谱和分子结构映射到共享标记词汇表，通过大规模预训练实现跨模态学习，多任务预训练优化，用MIST微调，引入化学反馈机制。

Result: MS - BART在MassSpecGym和NPLIB1的5/12关键指标上达SOTA，比基于扩散的竞争方法快一个数量级。

Conclusion: 综合消融研究验证了模型的有效性和鲁棒性。

Abstract: Mass spectrometry (MS) plays a critical role in molecular identification,
significantly advancing scientific discovery. However, structure elucidation
from MS data remains challenging due to the scarcity of annotated spectra.
While large-scale pretraining has proven effective in addressing data scarcity
in other domains, applying this paradigm to mass spectrometry is hindered by
the complexity and heterogeneity of raw spectral signals. To address this, we
propose MS-BART, a unified modeling framework that maps mass spectra and
molecular structures into a shared token vocabulary, enabling cross-modal
learning through large-scale pretraining on reliably computed
fingerprint-molecule datasets. Multi-task pretraining objectives further
enhance MS-BART's generalization by jointly optimizing denoising and
translation task. The pretrained model is subsequently transferred to
experimental spectra through finetuning on fingerprint predictions generated
with MIST, a pre-trained spectral inference model, thereby enhancing robustness
to real-world spectral variability. While finetuning alleviates the
distributional difference, MS-BART still suffers molecular hallucination and
requires further alignment. We therefore introduce a chemical feedback
mechanism that guides the model toward generating molecules closer to the
reference structure. Extensive evaluations demonstrate that MS-BART achieves
SOTA performance across 5/12 key metrics on MassSpecGym and NPLIB1 and is
faster by one order of magnitude than competing diffusion-based methods, while
comprehensive ablation studies systematically validate the model's
effectiveness and robustness.

</details>


### [157] [On Optimal Hyperparameters for Differentially Private Deep Transfer Learning](https://arxiv.org/abs/2510.20616)
*Aki Rehn,Linzh Zhao,Mikko A. Heikkilä,Antti Honkela*

Main category: cs.LG

TL;DR: 分析差分隐私迁移学习中裁剪边界C和批量大小B两个超参数，指出理论与实证不匹配、现有调参启发式方法失效等问题。


<details>
  <summary>Details</summary>
Motivation: 研究差分隐私迁移学习中关键超参数C和B的选择问题，解决理论与实际结果的矛盾。

Method: 分析梯度分布变化、研究累积DP噪声对批量大小选择的影响、分析裁剪作为梯度重新加权形式。

Result: 发现选择最优C的理论与实证结果不匹配，现有调B的启发式方法失效，单一(C,B)设置会导致性能欠佳。

Conclusion: 在不同隐私程度和计算资源情况下，需谨慎选择(C,B)超参数设置以避免性能下降。

Abstract: Differentially private (DP) transfer learning, i.e., fine-tuning a pretrained
model on private data, is the current state-of-the-art approach for training
large models under privacy constraints. We focus on two key hyperparameters in
this setting: the clipping bound $C$ and batch size $B$. We show a clear
mismatch between the current theoretical understanding of how to choose an
optimal $C$ (stronger privacy requires smaller $C$) and empirical outcomes
(larger $C$ performs better under strong privacy), caused by changes in the
gradient distributions. Assuming a limited compute budget (fixed epochs), we
demonstrate that the existing heuristics for tuning $B$ do not work, while
cumulative DP noise better explains whether smaller or larger batches perform
better. We also highlight how the common practice of using a single $(C,B)$
setting across tasks can lead to suboptimal performance. We find that
performance drops especially when moving between loose and tight privacy and
between plentiful and limited compute, which we explain by analyzing clipping
as a form of gradient re-weighting and examining cumulative DP noise.

</details>


### [158] [H-SPLID: HSIC-based Saliency Preserving Latent Information Decomposition](https://arxiv.org/abs/2510.20627)
*Lukas Miklautz,Chengzhi Shi,Andrii Shkabrii,Theodoros Thirimachos Davarakis,Prudence Lam,Claudia Plant,Jennifer Dy,Stratis Ioannidis*

Main category: cs.LG

TL;DR: 介绍H - SPLID算法，能学习显著特征表示，证明其与鲁棒性和潜在表示压缩的联系，图像分类任务实证有效。


<details>
  <summary>Details</summary>
Motivation: 开发一种能学习显著特征表示的算法，促进学习低维、与任务相关的特征。

Method: 提出H - SPLID算法，将显著和非显著特征分解到不同空间。

Result: 证明输入扰动下的预期预测偏差上界，图像分类任务中模型对非显著特征扰动敏感性降低。

Conclusion: H - SPLID算法有效，能使模型主要依赖显著输入组件，在鲁棒性和特征学习方面有优势。

Abstract: We introduce H-SPLID, a novel algorithm for learning salient feature
representations through the explicit decomposition of salient and non-salient
features into separate spaces. We show that H-SPLID promotes learning
low-dimensional, task-relevant features. We prove that the expected prediction
deviation under input perturbations is upper-bounded by the dimension of the
salient subspace and the Hilbert-Schmidt Independence Criterion (HSIC) between
inputs and representations. This establishes a link between robustness and
latent representation compression in terms of the dimensionality and
information preserved. Empirical evaluations on image classification tasks show
that models trained with H-SPLID primarily rely on salient input components, as
indicated by reduced sensitivity to perturbations affecting non-salient
features, such as image backgrounds. Our code is available at
https://github.com/neu-spiral/H-SPLID.

</details>


### [159] [Equitable Survival Prediction: A Fairness-Aware Survival Modeling (FASM) Approach](https://arxiv.org/abs/2510.20629)
*Mingxuan Liu,Yilin Ning,Haoyuan Wang,Chuan Hong,Matthew Engelhard,Danielle S. Bitterman,William G. La Cava,Nan Liu*

Main category: cs.LG

TL;DR: 提出公平感知生存建模（FASM），应用于乳腺癌预后，可提升公平性并保持判别性能，实现临床决策的准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型融入医疗时，临床数据中的结构不平等和社会偏见会被放大，生存分析中存在复杂因素，算法公平性方法常忽略跨组排名差异。

Method: 提出FASM方法，并应用于SEER乳腺癌数据。

Result: FASM大幅提升公平性，保持与不考虑公平性的生存模型相当的判别性能，在10年时间内保持稳定公平性，中期改善最明显。

Conclusion: 该方法能开发出在临床决策中兼顾准确性和公平性的生存模型，推动公平成为临床护理的核心原则。

Abstract: As machine learning models become increasingly integrated into healthcare,
structural inequities and social biases embedded in clinical data can be
perpetuated or even amplified by data-driven models. In survival analysis,
censoring and time dynamics can further add complexity to fair model
development. Additionally, algorithmic fairness approaches often overlook
disparities in cross-group rankings, e.g., high-risk Black patients may be
ranked below lower-risk White patients who do not experience the event of
mortality. Such misranking can reinforce biological essentialism and undermine
equitable care. We propose a Fairness-Aware Survival Modeling (FASM), designed
to mitigate algorithmic bias regarding both intra-group and cross-group risk
rankings over time. Using breast cancer prognosis as a representative case and
applying FASM to SEER breast cancer data, we show that FASM substantially
improves fairness while preserving discrimination performance comparable to
fairness-unaware survival models. Time-stratified evaluations show that FASM
maintains stable fairness over a 10-year horizon, with the greatest
improvements observed during the mid-term of follow-up. Our approach enables
the development of survival models that prioritize both accuracy and equity in
clinical decision-making, advancing fairness as a core principle in clinical
care.

</details>


### [160] [Large Multimodal Models-Empowered Task-Oriented Autonomous Communications: Design Methodology and Implementation Challenges](https://arxiv.org/abs/2510.20637)
*Hyun Jong Yang,Hyunsoo Kim,Hyeonho Noh,Seungnyun Kim,Byonghyo Shim*

Main category: cs.LG

TL;DR: 文章介绍基于大语言模型和大跨模态模型的面向任务的自主通信，通过三个案例展示框架，实验表明该系统优于传统和判别式深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型和大跨模态模型在自然语言理解等方面有显著能力，可作为6G机器、车辆和类人机器人自主通信的关键推动因素，因此研究其在面向任务的自主通信中的应用。

Method: 提供基于大语言模型和大跨模态模型的面向任务的自主通信概述，聚焦多模态传感集成、自适应重构和无线任务的提示/微调策略，并通过三个案例研究展示框架。

Result: 提出的大语言模型/大跨模态模型辅助的自主系统显著优于传统和判别式深度学习模型技术，在动态目标、输入参数变化和异构多模态条件下保持鲁棒性。

Conclusion: 大语言模型和大跨模态模型辅助的自主通信系统在性能和鲁棒性上表现出色，具有应用潜力。

Abstract: Large language models (LLMs) and large multimodal models (LMMs) have achieved
unprecedented breakthrough, showcasing remarkable capabilities in natural
language understanding, generation, and complex reasoning. This transformative
potential has positioned them as key enablers for 6G autonomous communications
among machines, vehicles, and humanoids. In this article, we provide an
overview of task-oriented autonomous communications with LLMs/LMMs, focusing on
multimodal sensing integration, adaptive reconfiguration, and
prompt/fine-tuning strategies for wireless tasks. We demonstrate the framework
through three case studies: LMM-based traffic control, LLM-based robot
scheduling, and LMM-based environment-aware channel estimation. From
experimental results, we show that the proposed LLM/LMM-aided autonomous
systems significantly outperform conventional and discriminative deep learning
(DL) model-based techniques, maintaining robustness under dynamic objectives,
varying input parameters, and heterogeneous multimodal conditions where
conventional static optimization degrades.

</details>


### [161] [Attention Enhanced Entity Recommendation for Intelligent Monitoring in Cloud Systems](https://arxiv.org/abs/2510.20640)
*Fiza Hussain,Anson Bastos,Anjaly Parayil,Ayush Choure,Chetan Bansal,Rujia Wang,Saravan Rajmohan*

Main category: cs.LG

TL;DR: 提出DiRecGNN用于微软云服务监控的实体推荐框架，介绍问题，提出基于transformer的注意力增强模型，评估显示性能提升，产品团队认为有用。


<details>
  <summary>Details</summary>
Motivation: 解决云服务监控中自动看门狗跟踪属性推荐问题，现有方法在结构和参与信息有限、捕捉长距离依赖方面表现不佳。

Method: 构建生产规模监控异构图，提出受transformer启发的注意力增强实体排名模型，利用多头注意力机制和随机游走采样路径，采用多方面损失函数优化。

Result: 相比现有方法有显著改进，MRR提升43.1%，产品团队评分为4.5分（满分5分）。

Conclusion: 所提的DiRecGNN框架有效，能提升云服务监控实体推荐性能，且得到产品团队认可。

Abstract: In this paper, we present DiRecGNN, an attention-enhanced entity
recommendation framework for monitoring cloud services at Microsoft. We provide
insights on the usefulness of this feature as perceived by the cloud service
owners and lessons learned from deployment. Specifically, we introduce the
problem of recommending the optimal subset of attributes (dimensions) that
should be tracked by an automated watchdog (monitor) for cloud services. To
begin, we construct the monitor heterogeneous graph at production-scale. The
interaction dynamics of these entities are often characterized by limited
structural and engagement information, resulting in inferior performance of
state-of-the-art approaches. Moreover, traditional methods fail to capture the
dependencies between entities spanning a long range due to their homophilic
nature. Therefore, we propose an attention-enhanced entity ranking model
inspired by transformer architectures. Our model utilizes a multi-head
attention mechanism to focus on heterogeneous neighbors and their attributes,
and further attends to paths sampled using random walks to capture long-range
dependencies. We also employ multi-faceted loss functions to optimize for
relevant recommendations while respecting the inherent sparsity of the data.
Empirical evaluations demonstrate significant improvements over existing
methods, with our model achieving a 43.1% increase in MRR. Furthermore, product
teams who consumed these features perceive the feature as useful and rated it
4.5 out of 5.

</details>


### [162] [Connecting Jensen-Shannon and Kullback-Leibler Divergences: A New Bound for Representation Learning](https://arxiv.org/abs/2510.20644)
*Reuben Dorent,Polina Golland,William Wells III*

Main category: cs.LG

TL;DR: 本文推导了KLD关于JSD的新下界，证明最大化基于JSD的信息可增加互信息下界，实验表明该下界用于MI估计时很紧密，为基于互信息的表征学习中使用判别式学习提供了理论和实证依据。


<details>
  <summary>Details</summary>
Motivation: 现有方法最大化替代依赖度量，但这些替代目标与互信息的联系尚不清楚，需要进行研究。

Method: 推导KLD关于JSD的新下界，将其应用于联合分布和边缘分布；重新审视基于JSD目标的实际实现；进行广泛实验并与现有变分下界的神经估计器比较。

Result: 下界用于MI估计时很紧密，下界估计器能稳定、低方差地估计MI的紧密下界，在信息瓶颈框架中有实际用途。

Conclusion: 为基于互信息的表征学习中使用判别式学习提供了新的理论依据和有力的实证证据。

Abstract: Mutual Information (MI) is a fundamental measure of statistical dependence
widely used in representation learning. While direct optimization of MI via its
definition as a Kullback-Leibler divergence (KLD) is often intractable, many
recent methods have instead maximized alternative dependence measures, most
notably, the Jensen-Shannon divergence (JSD) between joint and product of
marginal distributions via discriminative losses. However, the connection
between these surrogate objectives and MI remains poorly understood. In this
work, we bridge this gap by deriving a new, tight, and tractable lower bound on
KLD as a function of JSD in the general case. By specializing this bound to
joint and marginal distributions, we demonstrate that maximizing the JSD-based
information increases a guaranteed lower bound on mutual information.
Furthermore, we revisit the practical implementation of JSD-based objectives
and observe that minimizing the cross-entropy loss of a binary classifier
trained to distinguish joint from marginal pairs recovers a known variational
lower bound on the JSD. Extensive experiments demonstrate that our lower bound
is tight when applied to MI estimation. We compared our lower bound to
state-of-the-art neural estimators of variational lower bound across a range of
established reference scenarios. Our lower bound estimator consistently
provides a stable, low-variance estimate of a tight lower bound on MI. We also
demonstrate its practical usefulness in the context of the Information
Bottleneck framework. Taken together, our results provide new theoretical
justifications and strong empirical evidence for using discriminative learning
in MI-based representation learning.

</details>


### [163] [xTime: Extreme Event Prediction with Hierarchical Knowledge Distillation and Expert Fusion](https://arxiv.org/abs/2510.20651)
*Quan Li,Wenchao Yu,Suhang Wang,Minhua Lin,Lingwei Chen,Wei Cheng,Haifeng Chen*

Main category: cs.LG

TL;DR: 提出xTime框架用于时间序列极端事件预测，结合知识蒸馏和混合专家机制，实验显示预测准确率从3%提升到78%。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列中极端事件频发且后果严重，现有模型难准确预测极端事件，存在数据不平衡和忽略前置事件信息的问题。

Method: 提出xTime框架，利用知识蒸馏从低稀有度事件训练模型转移信息，引入混合专家机制动态选择和融合不同稀有度专家模型输出。

Result: 在多个数据集上实验，xTime持续改进，极端事件预测准确率从3%提升到78%。

Conclusion: xTime框架能有效提升时间序列极端事件的预测性能。

Abstract: Extreme events frequently occur in real-world time series and often carry
significant practical implications. In domains such as climate and healthcare,
these events, such as floods, heatwaves, or acute medical episodes, can lead to
serious consequences. Accurate forecasting of such events is therefore of
substantial importance. Most existing time series forecasting models are
optimized for overall performance within the prediction window, but often
struggle to accurately predict extreme events, such as high temperatures or
heart rate spikes. The main challenges are data imbalance and the neglect of
valuable information contained in intermediate events that precede extreme
events. In this paper, we propose xTime, a novel framework for extreme event
forecasting in time series. xTime leverages knowledge distillation to transfer
information from models trained on lower-rarity events, thereby improving
prediction performance on rarer ones. In addition, we introduce a mixture of
experts (MoE) mechanism that dynamically selects and fuses outputs from expert
models across different rarity levels, which further improves the forecasting
performance for extreme events. Experiments on multiple datasets show that
xTime achieves consistent improvements, with forecasting accuracy on extreme
events improving from 3% to 78%.

</details>


### [164] [Bayesian Jammer Localization with a Hybrid CNN and Path-Loss Mixture of Experts](https://arxiv.org/abs/2510.20666)
*Mariona Jaramillo-Civill,Luis González-Gudiño,Tales Imbiriba,Pau Closas*

Main category: cs.LG

TL;DR: 提出混合贝叶斯专家混合框架融合物理路径损耗模型和CNN用于GNSS信号干扰源定位，实验显示定位精度提升且不确定性降低。


<details>
  <summary>Details</summary>
Motivation: 以往数据驱动方法在GNSS信号定位中对接收信号强度场重建不佳，因空间上下文有限。

Method: 提出混合贝叶斯专家混合框架，通过对数线性池化融合物理路径损耗模型和CNN，用拉普拉斯近似进行贝叶斯推理。

Result: 在城市射线追踪数据实验中，更多训练点使定位精度提高、不确定性降低，不确定性集中在干扰源附近和城市峡谷。

Conclusion: 该混合框架能有效提升GNSS信号干扰源定位精度并降低不确定性。

Abstract: Global Navigation Satellite System (GNSS) signals are vulnerable to jamming,
particularly in urban areas where multipath and shadowing distort received
power. Previous data-driven approaches achieved reasonable localization but
poorly reconstructed the received signal strength (RSS) field due to limited
spatial context. We propose a hybrid Bayesian mixture-of-experts framework that
fuses a physical path-loss (PL) model and a convolutional neural network (CNN)
through log-linear pooling. The PL expert ensures physical consistency, while
the CNN leverages building-height maps to capture urban propagation effects.
Bayesian inference with Laplace approximation provides posterior uncertainty
over both the jammer position and RSS field. Experiments on urban ray-tracing
data show that localization accuracy improves and uncertainty decreases with
more training points, while uncertainty concentrates near the jammer and along
urban canyons where propagation is most sensitive.

</details>


### [165] [From Masks to Worlds: A Hitchhiker's Guide to World Models](https://arxiv.org/abs/2510.20668)
*Jinbin Bai,Yu Lei,Hecong Wu,Yuchen Zhu,Shufan Li,Yi Xin,Xiangtai Li,Molei Tao,Aditya Grover,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: 这不是典型的世界模型综述，而是构建世界的指南，指出构建真世界模型的最有前景路径。


<details>
  <summary>Details</summary>
Motivation: 为想构建世界的人提供指导，而非对提及世界模型的论文进行编目。

Method: 沿着从早期统一跨模态表征学习的掩码模型，到统一架构、交互式生成模型，再到记忆增强系统的路径，绕过无关分支聚焦核心。

Result: 明确了从早期模型到记忆增强系统这一构建路径。

Conclusion: 该路径是通向真正世界模型最有前景的路径。

Abstract: This is not a typical survey of world models; it is a guide for those who
want to build worlds. We do not aim to catalog every paper that has ever
mentioned a ``world model". Instead, we follow one clear road: from early
masked models that unified representation learning across modalities, to
unified architectures that share a single paradigm, then to interactive
generative models that close the action-perception loop, and finally to
memory-augmented systems that sustain consistent worlds over time. We bypass
loosely related branches to focus on the core: the generative heart, the
interactive loop, and the memory system. We show that this is the most
promising path towards true world models.

</details>


### [166] [GRACE: GRaph-based Addiction Care prEdiction](https://arxiv.org/abs/2510.20671)
*Subham Kumar,Prakrithi Shivaprakash,Koustav Rudra,Lekhansh Shukla,Animesh Mukherjee*

Main category: cs.LG

TL;DR: 提出GRACE框架解决成瘾患者护理场所预测问题，实验显示少数类F1分数提升11 - 35%。


<details>
  <summary>Details</summary>
Motivation: 确定成瘾患者合适护理场所是关键临床决策，现有资源不足且决策方法存在成瘾数据集类别不平衡问题，需开发自动化框架。

Method: 提出GRACE框架，将护理场所预测形式化为结构化学习问题，进行特征工程并提出获取无偏元图训练GNN的新方法。

Result: 在真实世界数据实验中，少数类F1分数较竞争基线提升11 - 35%。

Conclusion: GRACE框架能有效解决成瘾数据集类别不平衡问题，提升护理场所预测效果。

Abstract: Determining the appropriate locus of care for addiction patients is one of
the most critical clinical decisions that affects patient treatment outcomes
and effective use of resources. With a lack of sufficient specialized treatment
resources, such as inpatient beds or staff, there is an unmet need to develop
an automated framework for the same. Current decision-making approaches suffer
from severe class imbalances in addiction datasets. To address this limitation,
we propose a novel graph neural network (GRACE) framework that formalizes locus
of care prediction as a structured learning problem. Further, we perform
extensive feature engineering and propose a new approach of obtaining an
unbiased meta-graph to train a GNN to overcome the class imbalance problem.
Experimental results in real-world data show an improvement of 11-35% in terms
of the F1 score of the minority class over competitive baselines. The codes and
note embeddings are available at https://anonymous.4open.science/r/GRACE-F8E1/.

</details>


### [167] [A Scalable, Causal, and Energy Efficient Framework for Neural Decoding with Spiking Neural Networks](https://arxiv.org/abs/2510.20683)
*Georgios Mentzelopoulos,Ioannis Asmanis,Konrad P. Kording,Eva L. Dyer,Kostas Daniilidis,Flavia Vitale*

Main category: cs.LG

TL;DR: 本文提出基于脉冲神经网络（SNNs）的Spikachu神经解码框架，能有效解码神经活动，性能优且能耗低。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的解码方法依赖高能耗人工神经网络，难以集成到资源受限系统，SNNs提供新的可能。

Method: 提出Spikachu框架，直接处理分箱尖峰信号，将其投影到共享潜在空间提取特征，再集成和解码生成行为预测。

Result: 在6只非人灵长类动物的113次记录会话中评估，单会话训练时能耗比因果基线低2.26 - 418.81倍，多会话和多主体训练可提升性能并实现少样本迁移。

Conclusion: Spikachu是可扩展、在线兼容的神经解码框架，性能与现有模型相当，但能耗大幅降低。

Abstract: Brain-computer interfaces (BCIs) promise to enable vital functions, such as
speech and prosthetic control, for individuals with neuromotor impairments.
Central to their success are neural decoders, models that map neural activity
to intended behavior. Current learning-based decoding approaches fall into two
classes: simple, causal models that lack generalization, or complex, non-causal
models that generalize and scale offline but struggle in real-time settings.
Both face a common challenge, their reliance on power-hungry artificial neural
network backbones, which makes integration into real-world, resource-limited
systems difficult. Spiking neural networks (SNNs) offer a promising
alternative. Because they operate causally these models are suitable for
real-time use, and their low energy demands make them ideal for
battery-constrained environments. To this end, we introduce Spikachu: a
scalable, causal, and energy-efficient neural decoding framework based on SNNs.
Our approach processes binned spikes directly by projecting them into a shared
latent space, where spiking modules, adapted to the timing of the input,
extract relevant features; these latent representations are then integrated and
decoded to generate behavioral predictions. We evaluate our approach on 113
recording sessions from 6 non-human primates, totaling 43 hours of recordings.
Our method outperforms causal baselines when trained on single sessions using
between 2.26 and 418.81 times less energy. Furthermore, we demonstrate that
scaling up training to multiple sessions and subjects improves performance and
enables few-shot transfer to unseen sessions, subjects, and tasks. Overall,
Spikachu introduces a scalable, online-compatible neural decoding framework
based on SNNs, whose performance is competitive relative to state-of-the-art
models while consuming orders of magnitude less energy.

</details>


### [168] [Separating the what and how of compositional computation to enable reuse and continual learning](https://arxiv.org/abs/2510.20709)
*Haozhe Shan,Sun Minni,Lea Duncker*

Main category: cs.LG

TL;DR: 研究循环神经网络中持续学习和技能组合重用，提出双系统方法，在示例任务集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 探究促进技能持续学习和灵活组合的神经机制。

Method: 采用双系统方法，构建‘what系统’（用概率生成模型描述任务组合性，用无监督在线学习方法学习模型）和‘how系统’（低秩RNN组件按‘what系统’推断的上下文组合）。

Result: 该双系统学习框架有效，有竞争力，具备前后向迁移能力和快速组合泛化能力。

Conclusion: 提出的双系统方法能实现持续学习且无灾难性遗忘。

Abstract: The ability to continually learn, retain and deploy skills to accomplish
goals is a key feature of intelligent and efficient behavior. However, the
neural mechanisms facilitating the continual learning and flexible
(re-)composition of skills remain elusive. Here, we study continual learning
and the compositional reuse of learned computations in recurrent neural network
(RNN) models using a novel two-system approach: one system that infers what
computation to perform, and one that implements how to perform it. We focus on
a set of compositional cognitive tasks commonly studied in neuroscience. To
construct the what system, we first show that a large family of tasks can be
systematically described by a probabilistic generative model, where
compositionality stems from a shared underlying vocabulary of discrete task
epochs. The shared epoch structure makes these tasks inherently compositional.
We first show that this compositionality can be systematically described by a
probabilistic generative model. Furthermore, We develop an unsupervised online
learning approach that can learn this model on a single-trial basis, building
its vocabulary incrementally as it is exposed to new tasks, and inferring the
latent epoch structure as a time-varying computational context within a trial.
We implement the how system as an RNN whose low-rank components are composed
according to the context inferred by the what system. Contextual inference
facilitates the creation, learning, and reuse of low-rank RNN components as new
tasks are introduced sequentially, enabling continual learning without
catastrophic forgetting. Using an example task set, we demonstrate the efficacy
and competitive performance of this two-system learning framework, its
potential for forward and backward transfer, as well as fast compositional
generalization to unseen tasks.

</details>


### [169] [Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of EHR Variables with the Johns Hopkins Fall Risk Assessment Tool](https://arxiv.org/abs/2510.20714)
*Fardin Ganjkhanloo,Emmett Springer,Erik H. Hoyer,Daniel L. Young,Kimia Ghobadi*

Main category: cs.LG

TL;DR: 研究用数据驱动建模方法使 JHFRAT 跌倒风险预测与临床指标更好对齐，CSO 模型有改进，为系统提升住院患者跌倒预防协议提供基础。


<details>
  <summary>Details</summary>
Motivation: 让 JHFRAT 的跌倒风险预测与更多临床有意义的指标更好地对齐。

Method: 对 2022 年 3 月至 2023 年 10 月三家医院的 54,209 例住院患者进行回顾性分析，在 JHFRAT 评估数据和 EHR 变量上采用约束分数优化（CSO）模型。

Result: CSO 模型预测性能比当前 JHFRAT 有显著提升，有无 EHR 变量表现相近，XGBoost 性能指标更优，但 CSO 对风险标签变化更稳健。

Conclusion: 这种基于证据的方法为卫生系统利用数据驱动优化技术系统地加强住院患者跌倒预防协议和患者安全提供了坚实基础。

Abstract: In this study we aim to better align fall risk prediction from the Johns
Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically
meaningful measures via a data-driven modelling approach. We conducted a
retrospective analysis of 54,209 inpatient admissions from three Johns Hopkins
Health System hospitals between March 2022 and October 2023. A total of 20,208
admissions were included as high fall risk encounters, and 13,941 were included
as low fall risk encounters. To incorporate clinical knowledge and maintain
interpretability, we employed constrained score optimization (CSO) models on
JHFRAT assessment data and additional electronic health record (EHR) variables.
The model demonstrated significant improvements in predictive performance over
the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). The constrained
score optimization models performed similarly with and without the EHR
variables. Although the benchmark black-box model (XGBoost), improves upon the
performance metrics of the knowledge-based constrained logistic regression
(AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk
labelling. This evidence-based approach provides a robust foundation for health
systems to systematically enhance inpatient fall prevention protocols and
patient safety using data-driven optimization techniques, contributing to
improved risk assessment and resource allocation in healthcare settings.

</details>


### [170] [Unsupervised Anomaly Prediction with N-BEATS and Graph Neural Network in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2510.20718)
*Daniel Sorensen,Bappaditya Dey,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: 本文针对半导体制造中异常预测的挑战，提出两种新方法，对比了N - BEATS和GNN模型，GNN表现更优，是在线异常预测的有前景方案。


<details>
  <summary>Details</summary>
Motivation: 半导体制造异常预测面临高维数据、类别不平衡和变量复杂依赖等挑战，需从异常检测推进到异常预测以实现实时过程校正和主动故障预防。

Method: 提出的异常预测框架分两步：在无异常数据集上训练预测模型，对未知时间序列数据进行预测并与训练信号对比，超出阈值则标记为异常。采用N - BEATS和GNN两种预测模型。

Result: 两种模型在20个时间点内预测性能强，50个时间点内异常预测稳定，GNN表现优于N - BEATS，且所需可训练参数少、计算成本低。

Conclusion: GNN是适合部署在制造环境中的在线异常预测的有前景解决方案。

Abstract: Semiconductor manufacturing is an extremely complex and precision-driven
process, characterized by thousands of interdependent parameters collected
across diverse tools and process steps. Multi-variate time-series analysis has
emerged as a critical field for real-time monitoring and fault detection in
such environments. However, anomaly prediction in semiconductor fabrication
presents several critical challenges, including high dimensionality of sensor
data and severe class imbalance due to the rarity of true faults. Furthermore,
the complex interdependencies between variables complicate both anomaly
prediction and root-cause-analysis. This paper proposes two novel approaches to
advance the field from anomaly detection to anomaly prediction, an essential
step toward enabling real-time process correction and proactive fault
prevention. The proposed anomaly prediction framework contains two main stages:
(a) training a forecasting model on a dataset assumed to contain no anomalies,
and (b) performing forecast on unseen time series data. The forecast is
compared with the forecast of the trained signal. Deviations beyond a
predefined threshold are flagged as anomalies. The two approaches differ in the
forecasting model employed. The first assumes independence between variables by
utilizing the N-BEATS model for univariate time series forecasting. The second
lifts this assumption by utilizing a Graph Neural Network (GNN) to capture
inter-variable relationships. Both models demonstrate strong forecasting
performance up to a horizon of 20 time points and maintain stable anomaly
prediction up to 50 time points. The GNN consistently outperforms the N-BEATS
model while requiring significantly fewer trainable parameters and lower
computational cost. These results position the GNN as promising solution for
online anomaly forecasting to be deployed in manufacturing environments.

</details>


### [171] [No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes](https://arxiv.org/abs/2510.20725)
*Jasmine Bayrooti,Sattar Vakili,Amanda Prorok,Carl Henrik Ek*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Thompson sampling (TS) is a powerful and widely used strategy for sequential
decision-making, with applications ranging from Bayesian optimization to
reinforcement learning (RL). Despite its success, the theoretical foundations
of TS remain limited, particularly in settings with complex temporal structure
such as RL. We address this gap by establishing no-regret guarantees for TS
using models with Gaussian marginal distributions. Specifically, we consider TS
in episodic RL with joint Gaussian process (GP) priors over rewards and
transitions. We prove a regret bound of
$\mathcal{\tilde{O}}(\sqrt{KH\Gamma(KH)})$ over $K$ episodes of horizon $H$,
where $\Gamma(\cdot)$ captures the complexity of the GP model. Our analysis
addresses several challenges, including the non-Gaussian nature of value
functions and the recursive structure of Bellman updates, and extends classical
tools such as the elliptical potential lemma to multi-output settings. This
work advances the understanding of TS in RL and highlights how structural
assumptions and model uncertainty shape its performance in finite-horizon
Markov Decision Processes.

</details>


### [172] [Thought Communication in Multiagent Collaboration](https://arxiv.org/abs/2510.20733)
*Yujia Zheng,Zhuokai Zhao,Zijian Li,Yaqi Xie,Mingze Gao,Lizhu Zhang,Kun Zhang*

Main category: cs.LG

TL;DR: 提出思想交流范式，可让智能体直接交互，理论证明可识别潜在思想，开发框架验证其优势。


<details>
  <summary>Details</summary>
Motivation: 自然语言有局限性，多数基于大语言模型的多智能体系统依赖自然语言，需超越语言限制。

Method: 将过程形式化为一般潜变量模型，证明在无辅助信息的非参数设置下可识别潜在思想，开发提取和分配潜在思想的框架。

Result: 实验验证理论，展示思想交流的协作优势。

Conclusion: 此工作揭示挖掘隐藏世界的潜力，仅靠表面观察有挑战难以解决。

Abstract: Natural language has long enabled human cooperation, but its lossy,
ambiguous, and indirect nature limits the potential of collective intelligence.
While machines are not subject to these constraints, most LLM-based multi-agent
systems still rely solely on natural language, exchanging tokens or their
embeddings. To go beyond language, we introduce a new paradigm, thought
communication, which enables agents to interact directly mind-to-mind, akin to
telepathy. To uncover these latent thoughts in a principled way, we formalize
the process as a general latent variable model, where agent states are
generated by an unknown function of underlying thoughts. We prove that, in a
nonparametric setting without auxiliary information, both shared and private
latent thoughts between any pair of agents can be identified. Moreover, the
global structure of thought sharing, including which agents share which
thoughts and how these relationships are structured, can also be recovered with
theoretical guarantees. Guided by the established theory, we develop a
framework that extracts latent thoughts from all agents prior to communication
and assigns each agent the relevant thoughts, along with their sharing
patterns. This paradigm naturally extends beyond LLMs to all modalities, as
most observational data arise from hidden generative processes. Experiments on
both synthetic and real-world benchmarks validate the theory and demonstrate
the collaborative advantages of thought communication. We hope this work
illuminates the potential of leveraging the hidden world, as many challenges
remain unsolvable through surface-level observation alone, regardless of
compute or data scale.

</details>


### [173] [Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process](https://arxiv.org/abs/2510.20736)
*Tsai Hor Chan,Feng Wu,Yihang Chen,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: 本文提出基于狄利克雷过程（DP）的多模态学习框架，在多模态数据集上表现出色，消融分析验证了DP有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法过度强调模态边缘分布对齐，会阻碍各模态有意义的特征表达，需在突出的模态内表示学习和跨模态对齐间取得平衡。

Method: 假设每个模态遵循多元高斯分布的混合，采用DP计算所有分量的混合权重，利用其富者更富特性动态分配特征贡献并选择最突出特征。

Result: 在多个多模态数据集上实验表明该模型性能优于其他竞争对手，消融分析验证了DP在对齐模态分布方面的有效性和对关键超参数变化的鲁棒性。

Conclusion: 所提出的DP驱动多模态学习框架能有效实现多模态特征融合，具有良好性能和鲁棒性。

Abstract: Developing effective multimodal fusion approaches has become increasingly
essential in many real-world scenarios, such as health care and finance. The
key challenge is how to preserve the feature expressiveness in each modality
while learning cross-modal interactions. Previous approaches primarily focus on
the cross-modal alignment, while over-emphasis on the alignment of marginal
distributions of modalities may impose excess regularization and obstruct
meaningful representations within each modality. The Dirichlet process (DP)
mixture model is a powerful Bayesian non-parametric method that can amplify the
most prominent features by its richer-gets-richer property, which allocates
increasing weights to them. Inspired by this unique characteristic of DP, we
propose a new DP-driven multimodal learning framework that automatically
achieves an optimal balance between prominent intra-modal representation
learning and cross-modal alignment. Specifically, we assume that each modality
follows a mixture of multivariate Gaussian distributions and further adopt DP
to calculate the mixture weights for all the components. This paradigm allows
DP to dynamically allocate the contributions of features and select the most
prominent ones, leveraging its richer-gets-richer property, thus facilitating
multimodal feature fusion. Extensive experiments on several multimodal datasets
demonstrate the superior performance of our model over other competitors.
Ablation analysis further validates the effectiveness of DP in aligning
modality distributions and its robustness to changes in key hyperparameters.
Code is anonymously available at https://github.com/HKU-MedAI/DPMM.git

</details>


### [174] [MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs](https://arxiv.org/abs/2510.20762)
*Jan Sobotka,Luca Baroni,Ján Antolík*

Main category: cs.LG

TL;DR: 本文介绍MEIcoder解码方法，在小数据集上表现出色，还提出统一基准，证明早期视觉系统可靠解码可行性。


<details>
  <summary>Details</summary>
Motivation: 生物数据稀缺给深度学习解码技术带来挑战，需新方法解决。

Method: 引入MEIcoder，利用神经元特定的最兴奋输入（MEIs）、结构相似性指数测量损失和对抗训练。

Result: MEIcoder在初级视觉皮层单细胞活动重建视觉刺激上达最优性能，在小数据集表现出色，能用少量神经元和训练数据点重建高保真自然图像，还提出统一基准。

Conclusion: 证明早期视觉系统可靠解码可行，为神经科学和神经工程应用提供实用见解。

Abstract: Decoding visual stimuli from neural population activity is crucial for
understanding the brain and for applications in brain-machine interfaces.
However, such biological data is often scarce, particularly in primates or
humans, where high-throughput recording techniques, such as two-photon imaging,
remain challenging or impossible to apply. This, in turn, poses a challenge for
deep learning decoding techniques. To overcome this, we introduce MEIcoder, a
biologically informed decoding method that leverages neuron-specific most
exciting inputs (MEIs), a structural similarity index measure loss, and
adversarial training. MEIcoder achieves state-of-the-art performance in
reconstructing visual stimuli from single-cell activity in primary visual
cortex (V1), especially excelling on small datasets with fewer recorded
neurons. Using ablation studies, we demonstrate that MEIs are the main drivers
of the performance, and in scaling experiments, we show that MEIcoder can
reconstruct high-fidelity natural-looking images from as few as 1,000-2,500
neurons and less than 1,000 training data points. We also propose a unified
benchmark with over 160,000 samples to foster future research. Our results
demonstrate the feasibility of reliable decoding in early visual system and
provide practical insights for neuroscience and neuroengineering applications.

</details>


### [175] [Out-of-distribution Tests Reveal Compositionality in Chess Transformers](https://arxiv.org/abs/2510.20783)
*Anna Mészáros,Patrik Reizinger,Ferenc Huszár*

Main category: cs.LG

TL;DR: 训练270M参数国际象棋Transformer，测试其分布外泛化能力，发现有组合泛化能力，在变体中表现有优劣，训练动态显示有对游戏的组合理解。


<details>
  <summary>Details</summary>
Motivation: 探究现代决策Transformer在多大程度上真正掌握国际象棋规则。

Method: 训练270M参数国际象棋Transformer，在分布外场景测试，还在国际象棋变体如Chess960上评估，分析训练动态。

Result: Transformer有组合泛化能力，能遵守基本规则，为分布外谜题生成高质量走法；在变体中基本策略适应但不如符号AI算法，在Lichess上对用户差距较小；训练初期先学会移动自己棋子。

Conclusion: Transformer在国际象棋中有一定规则理解和组合泛化能力，但在变体挑战中还有提升空间。

Abstract: Chess is a canonical example of a task that requires rigorous reasoning and
long-term planning. Modern decision Transformers - trained similarly to LLMs -
are able to learn competent gameplay, but it is unclear to what extent they
truly capture the rules of chess. To investigate this, we train a 270M
parameter chess Transformer and test it on out-of-distribution scenarios,
designed to reveal failures of systematic generalization. Our analysis shows
that Transformers exhibit compositional generalization, as evidenced by strong
rule extrapolation: they adhere to fundamental syntactic rules of the game by
consistently choosing valid moves even in situations very different from the
training data. Moreover, they also generate high-quality moves for OOD puzzles.
In a more challenging test, we evaluate the models on variants including
Chess960 (Fischer Random Chess) - a variant of chess where starting positions
of pieces are randomized. We found that while the model exhibits basic strategy
adaptation, they are inferior to symbolic AI algorithms that perform explicit
search, but gap is smaller when playing against users on Lichess. Moreover, the
training dynamics revealed that the model initially learns to move only its own
pieces, suggesting an emergent compositional understanding of the game.

</details>


### [176] [BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation](https://arxiv.org/abs/2510.20792)
*Liang Ye,Shengqin Chen,Jiazhu Dai*

Main category: cs.LG

TL;DR: 提出BadGraph针对文本引导图生成的潜在扩散模型的后门攻击方法，实验证明其有效性和隐蔽性，揭示该模型安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 图生成发展带来后门安全问题，条件尤其是文本引导图生成的后门攻击研究不足。

Method: 提出BadGraph方法，利用文本触发器毒害训练数据，植入后门。

Result: 在四个基准数据集实验，不到10%毒害率可实现50%攻击成功率，24%可达超80%成功率，对良性样本性能影响小，消融研究揭示后门植入阶段。

Conclusion: 揭示文本引导图生成潜在扩散模型安全漏洞，强调应用风险和防御需求。

Abstract: The rapid progress of graph generation has raised new security concerns,
particularly regarding backdoor vulnerabilities. While prior work has explored
backdoor attacks in image diffusion and unconditional graph generation,
conditional, especially text-guided graph generation remains largely
unexamined. This paper proposes BadGraph, a backdoor attack method targeting
latent diffusion models for text-guided graph generation. BadGraph leverages
textual triggers to poison training data, covertly implanting backdoors that
induce attacker-specified subgraphs during inference when triggers appear,
while preserving normal performance on clean inputs. Extensive experiments on
four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the
effectiveness and stealth of the attack: less than 10% poisoning rate can
achieves 50% attack success rate, while 24% suffices for over 80% success rate,
with negligible performance degradation on benign samples. Ablation studies
further reveal that the backdoor is implanted during VAE and diffusion training
rather than pretraining. These findings reveal the security vulnerabilities in
latent diffusion models of text-guided graph generation, highlight the serious
risks in models' applications such as drug discovery and underscore the need
for robust defenses against the backdoor attack in such diffusion models.

</details>


### [177] [Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples](https://arxiv.org/abs/2510.20800)
*Shiva Sreeram,Alaa Maalouf,Pratyusha Sharma,Daniela Rus*

Main category: cs.LG

TL;DR: 本文针对LASER方法开销大问题提出改进，结合多方面发现得到快速稳健的大模型下游任务适应算法，无需微调。


<details>
  <summary>Details</summary>
Motivation: LASER方法进行逐矩阵搜索开销大，不适合快速部署，需改进。

Method: 只检查精心挑选的矩阵子集；用矩阵奇异值梯度确定需降阶矩阵；增加因子分解搜索空间；用100个样本评估。

Result: 减少过拟合，准确率最多提升24.6个百分点，减少搜索时间。

Conclusion: 结合各项发现可得到快速且稳健的下游任务适应算法，单步梯度计算和快速扫描即可使大模型适应新数据集，无需微调。

Abstract: Recently, Sharma et al. suggested a method called Layer-SElective-Rank
reduction (LASER) which demonstrated that pruning high-order components of
carefully chosen LLM's weight matrices can boost downstream accuracy -- without
any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each
requiring full-dataset forward passes) makes it impractical for rapid
deployment. We demonstrate that this overhead can be removed and find that: (i)
Only a small, carefully chosen subset of matrices needs to be inspected --
eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's
singular values pinpoints which matrices merit reduction, (iii) Increasing the
factorization search space by allowing matrices rows to cluster around multiple
subspaces and then decomposing each cluster separately further reduces
overfitting on the original training data and further lifts accuracy by up to
24.6 percentage points, and finally, (iv) we discover that evaluating on just
100 samples rather than the full training data -- both for computing the
indicative gradients and for measuring the final accuracy -- suffices to
further reduce the search time; we explain that as adaptation to downstream
tasks is dominated by prompting style, not dataset size. As a result, we show
that combining these findings yields a fast and robust adaptation algorithm for
downstream tasks. Overall, with a single gradient step on 100 examples and a
quick scan of the top candidate layers and factorization techniques, we can
adapt LLMs to new datasets -- entirely without fine-tuning.

</details>


### [178] [KL-Regularized Reinforcement Learning is Designed to Mode Collapse](https://arxiv.org/abs/2510.20817)
*Anthony GX-Chen,Jatin Prakash,Jeff Guo,Rob Fergus,Rajesh Ranganath*

Main category: cs.LG

TL;DR: 研究发现反向/正向KL散度直觉不适用于强化学习，模式覆盖取决于其他因素，据此构建算法提升模型质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 探究反向/正向KL散度在强化学习中的适用性及对模式覆盖的影响，以解决采样多样性问题。

Method: 数学推导和实验验证，分析影响模式覆盖的因素，构建简单可扩展且理论合理的算法。

Result: 所构建算法能在无外部多样性信号下，提升大语言模型和化学语言模型的解质量和多样性，正向和反向KL单独使用失败时也有效。

Conclusion: 反向/正向KL散度直觉不适用于强化学习，模式覆盖受其他因素影响，构建的算法有效。

Abstract: It is commonly believed that optimizing the reverse KL divergence results in
"mode seeking", while optimizing forward KL results in "mass covering", with
the latter being preferred if the goal is to sample from multiple diverse
modes. We show -- mathematically and empirically -- that this intuition does
not necessarily transfer well to doing reinforcement learning with
reverse/forward KL regularization (e.g. as commonly used with language models).
Instead, the choice of reverse/forward KL determines the family of optimal
target distributions, parameterized by the regularization coefficient. Mode
coverage depends primarily on other factors, such as regularization strength,
and relative scales between rewards and reference probabilities. Further, we
show commonly used settings such as low regularization strength and equal
verifiable rewards tend to specify unimodal target distributions, meaning the
optimization objective is, by construction, non-diverse. We leverage these
insights to construct a simple, scalable, and theoretically justified
algorithm. It makes minimal changes to reward magnitudes, yet optimizes for a
target distribution which puts high probability over all high-quality sampling
modes. In experiments, this simple modification works to post-train both Large
Language Models and Chemical Language Models to have higher solution quality
and diversity, without any external signals of diversity, and works with both
forward and reverse KL when using either naively fails.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [179] [E-Test: E'er-Improving Test Suites](https://arxiv.org/abs/2510.19860)
*Ketai Qiu,Luca Di Grazia,Leonardo Mariani,Mauro Pezzè*

Main category: cs.SE

TL;DR: 提出E - Test方法，利用LLMs增强测试套件，在实验中表现优于现有方法，提升测试套件质量并减少维护人力。


<details>
  <summary>Details</summary>
Motivation: 现有测试套件不完美，寻找新测试用例困难且劳动密集，需缩小测试套件探索的执行空间与实际执行情况的差距。

Method: 提出E - Test方法，利用LLMs从大量场景中识别未测试的执行情况，并生成新测试用例来增强测试套件。

Result: 在1975个场景的数据集上，E - Test的F1分数达到0.55，优于现有回归测试、现场测试方法和普通LLMs。

Conclusion: E - Test能有效针对未测试的执行场景，提升测试套件质量，减少维护测试套件的人力。

Abstract: Test suites are inherently imperfect, and testers can always enrich a suite
with new test cases that improve its quality and, consequently, the reliability
of the target software system. However, finding test cases that explore
execution scenarios beyond the scope of an existing suite can be extremely
challenging and labor-intensive, particularly when managing large test suites
over extended periods.
  In this paper, we propose E-Test, an approach that reduces the gap between
the execution space explored with a test suite and the executions experienced
after testing by augmenting the test suite with test cases that explore
execution scenarios that emerge in production. E-Test (i) identifies executions
that have not yet been tested from large sets of scenarios, such as those
monitored during intensive production usage, and (ii) generates new test cases
that enhance the test suite. E-Test leverages Large Language Models (LLMs) to
pinpoint scenarios that the current test suite does not adequately cover, and
augments the suite with test cases that execute these scenarios.
  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred
open-source Java projects already in production and Defects4J, demonstrates
that E-Test retrieves not-yet-tested execution scenarios significantly better
than state-of-the-art approaches. While existing regression testing and field
testing approaches for this task achieve a maximum F1-score of 0.34, and
vanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These
results highlight the impact of E-Test in enhancing test suites by effectively
targeting not-yet-tested execution scenarios and reducing manual effort
required for maintaining test suites.

</details>


### [180] [SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations](https://arxiv.org/abs/2510.19864)
*Amila Indika,Igor Molybog*

Main category: cs.SE

TL;DR: 本文介绍电子表格操作文档（SOD）任务，用基准测试评估5个大语言模型，发现大语言模型能生成准确文档，SOD可提升电子表格工作流程。


<details>
  <summary>Details</summary>
Motivation: 电子表格缺乏系统文档方法阻碍自动化等，且将电子表格代码转自然语言研究较少。

Method: 提出含111个代码片段及对应自然语言摘要的基准测试，用BLEU等指标评估5个大语言模型。

Result: 大语言模型能生成准确的电子表格文档。

Conclusion: SOD是提升电子表格可重复性等工作流程的可行前置步骤，但有挑战待解决。

Abstract: Numerous knowledge workers utilize spreadsheets in business, accounting, and
finance. However, a lack of systematic documentation methods for spreadsheets
hinders automation, collaboration, and knowledge transfer, which risks the loss
of crucial institutional knowledge. This paper introduces Spreadsheet
Operations Documentation (SOD), an AI task that involves generating
human-readable explanations from spreadsheet operations. Many previous studies
have utilized Large Language Models (LLMs) for generating spreadsheet
manipulation code; however, translating that code into natural language for SOD
is a less-explored area. To address this, we present a benchmark of 111
spreadsheet manipulation code snippets, each paired with a corresponding
natural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,
LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and
METEOR metrics. Our findings suggest that LLMs can generate accurate
spreadsheet documentation, making SOD a feasible prerequisite step toward
enhancing reproducibility, maintainability, and collaborative workflows in
spreadsheets, although there are challenges that need to be addressed.

</details>


### [181] [Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation](https://arxiv.org/abs/2510.19868)
*Qian Xiong,Bo Yang,Weisong Sun,Yiran Zhang,Tianlin Li,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: 现有大语言模型自动代码生成在复杂应用级软件代码生成有挑战，本文提出KGACG框架，通过多智能体协作闭环结合反馈机制生成代码，并在案例中展示协作过程。


<details>
  <summary>Details</summary>
Motivation: 大语言模型自动代码生成在复杂应用级软件代码生成存在困难，现有多智能体方法在大规模应用级软件开发中表现不佳，无法保证代码组织结构合理和维护代码生成过程。

Method: 提出知识引导的应用级代码生成框架KGACG，通过代码组织与规划智能体（COPA）、编码智能体（CA）和测试智能体（TA）的协作闭环，结合反馈机制将软件需求规格和架构设计文档转化为可执行代码。

Result: 在Java坦克战斗游戏案例研究中展示了KGACG中智能体的协作过程。

Conclusion: KGACG致力于推进应用级软件开发的自动化。

Abstract: Automated code generation driven by Large Lan- guage Models (LLMs) has
enhanced development efficiency, yet generating complex application-level
software code remains challenging. Multi-agent frameworks show potential, but
existing methods perform inadequately in large-scale application-level software
code generation, failing to ensure reasonable orga- nizational structures of
project code and making it difficult to maintain the code generation process.
To address this, this paper envisions a Knowledge-Guided Application-Level Code
Generation framework named KGACG, which aims to trans- form software
requirements specification and architectural design document into executable
code through a collaborative closed- loop of the Code Organization & Planning
Agent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a
feedback mechanism. We demonstrate the collaborative process of the agents in
KGACG in a Java Tank Battle game case study while facing challenges. KGACG is
dedicated to advancing the automation of application-level software
development.

</details>


### [182] [BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills](https://arxiv.org/abs/2510.19898)
*Atharv Sonwane,Isadora White,Hyunji Lee,Matheus Pereira,Lucas Caccia,Minseon Kim,Zhengyan Shi,Chinmay Singh,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan*

Main category: cs.SE

TL;DR: 提出合成生成困难多样软件漏洞的新方法，生成的漏洞训练效果好，训练出的模型表现佳。


<details>
  <summary>Details</summary>
Motivation: 高质量漏洞对训练下一代基于语言模型的软件工程代理很关键，现有生成漏洞方法不能反映真实开发过程。

Method: 指导软件工程代理将特性引入代码库，使其可能无意破坏测试从而产生漏洞。

Result: 生成的漏洞更接近人类编辑模式，作为训练数据效果更优，比其他漏洞数据集用一半训练数据性能高2%，训练出的模型在SWE - bench Verified上表现出色。

Conclusion: 新的漏洞生成方法有效，可用于训练高性能的软件工程语言模型。

Abstract: High quality bugs are key to training the next generation of language model
based software engineering (SWE) agents. We introduce a novel method for
synthetic generation of difficult and diverse bugs. Our method instructs SWE
Agents to introduce a feature into the codebase whereby they may
unintentionally break tests, resulting in bugs. Prior approaches often induce
an out-of-distribution effect by generating bugs intentionally (e.g. by
introducing local perturbation to existing code), which does not reflect
realistic development processes. We perform qualitative analysis to demonstrate
that our approach for generating bugs more closely reflects the patterns found
in human-authored edits. Through extensive experiments, we demonstrate that our
bugs provide more efficient training data for supervised fine-tuning,
outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k
bugs). We train on our newly generated bugs in addition to existing bug
datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench
Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on
SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over
three seeds.

</details>


### [183] [On Interaction Effects in Greybox Fuzzing](https://arxiv.org/abs/2510.19984)
*Konstantinos Kitsios,Marcel Böhme,Alberto Bacchelli*

Main category: cs.SE

TL;DR: 本文提出灰盒模糊测试器MuoFuzz，通过学习选择最有前途的变异器序列，实验表明其在代码覆盖率和发现漏洞上表现更好。


<details>
  <summary>Details</summary>
Motivation: 假设变异器应用于种子输入的顺序会影响灰盒模糊测试器的有效性，期望找到更高效的模糊测试方法。

Method: 拟合线性模型观察交互效应，MuoFuzz学习变异器产生有趣输入的条件概率，用随机游走生成变异器序列。

Result: 在FuzzBench和MAGMA基准测试中，MuoFuzz代码覆盖率最高，找到AFL++和MOPT未发现的漏洞。

Conclusion: MuoFuzz能通过学习选择变异器序列，实现更高效的模糊测试。

Abstract: A greybox fuzzer is an automated software testing tool that generates new
test inputs by applying randomly chosen mutators (e.g., flipping a bit or
deleting a block of bytes) to a seed input in random order and adds all
coverage-increasing inputs to the corpus of seeds. We hypothesize that the
order in which mutators are applied to a seed input has an impact on the
effectiveness of greybox fuzzers. In our experiments, we fit a linear model to
a dataset that contains the effectiveness of all possible mutator pairs and
indeed observe the conjectured interaction effect. This points us to more
efficient fuzzing by choosing the most promising mutator sequence with a higher
likelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the
most promising mutator sequences. MuoFuzz learns the conditional probability
that the next mutator will yield an interesting input, given the previously
selected mutator. Then, it samples from the learned probability using a random
walk to generate mutator sequences. We compare the performance of MuoFuzz to
AFL++, which uses a fixed selection probability, and MOPT, which optimizes the
selection probability of each mutator in isolation. Experimental results on the
FuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code
coverage and finds four bugs missed by AFL++ and one missed by both AFL++ and
MOPT.

</details>


### [184] [A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)](https://arxiv.org/abs/2510.19997)
*Abraham Itzhak Weinberg*

Main category: cs.SE

TL;DR: 本文指出不同规模组织在采用生成式AI时面临挑战，现有框架缺乏针对性，提出FAIGMOE框架并介绍其内容和特点，需后续实证验证。


<details>
  <summary>Details</summary>
Motivation: 不同规模组织采用生成式AI面临独特挑战，现有技术采用框架缺乏针对性，需新框架。

Method: 综合技术采用理论、组织变革管理和创新扩散视角，构建FAIGMOE框架，分四个相互关联阶段。

Result: 提出FAIGMOE框架，提供可扩展指导，包含生成式AI特定考虑因素。

Conclusion: FAIGMOE是首个全面解决不同规模组织采用生成式AI的概念框架，提供可操作方案，但需实证验证。

Abstract: Generative Artificial Intelligence (GenAI) presents transformative
opportunities for organizations, yet both midsize organizations and larger
enterprises face distinctive adoption challenges. Midsize organizations
encounter resource constraints and limited AI expertise, while enterprises
struggle with organizational complexity and coordination challenges. Existing
technology adoption frameworks, including TAM (Technology Acceptance Model),
TOE (Technology Organization Environment), and DOI (Diffusion of Innovations)
theory, lack the specificity required for GenAI implementation across these
diverse contexts, creating a critical gap in adoption literature. This paper
introduces FAIGMOE (Framework for the Adoption and Integration of Generative AI
in Midsize Organizations and Enterprises), a conceptual framework addressing
the unique needs of both organizational types. FAIGMOE synthesizes technology
adoption theory, organizational change management, and innovation diffusion
perspectives into four interconnected phases: Strategic Assessment, Planning
and Use Case Development, Implementation and Integration, and
Operationalization and Optimization. Each phase provides scalable guidance on
readiness assessment, strategic alignment, risk governance, technical
architecture, and change management adaptable to organizational scale and
complexity. The framework incorporates GenAI specific considerations including
prompt engineering, model orchestration, and hallucination management that
distinguish it from generic technology adoption frameworks. As a perspective
contribution, FAIGMOE provides the first comprehensive conceptual framework
explicitly addressing GenAI adoption across midsize and enterprise
organizations, offering actionable implementation protocols, assessment
instruments, and governance templates requiring empirical validation through
future research.

</details>


### [185] [The Cost of Downgrading Build Systems: A Case Study of Kubernetes](https://arxiv.org/abs/2510.20041)
*Gareema Ranjan,Mahmoud Alfadel,Gengyi Sun,Shane McIntosh*

Main category: cs.SE

TL;DR: 本文通过对Kubernetes项目及其他四个项目的研究，发现放弃基于工件的构建工具虽有可维护性优势，但会给大型项目带来性能成本。


<details>
  <summary>Details</summary>
Motivation: 现代基于工件的构建工具虽能加速构建，但团队可能因维护性问题放弃，此前降级的影响未被充分研究。

Method: 对Kubernetes项目从Bazel降级到Go Build进行案例研究，重现和分析降级期间变更集的完整和增量构建，并在其他四个项目上复制该研究。

Result: Bazel构建速度更快，但内存占用大、CPU负载高，从Bazel降级会增加CI资源成本，Bazel始终消耗更多内存。

Conclusion: 放弃基于工件的构建工具虽有可维护性好处，但大型项目往往会产生可观的性能成本，研究有助于利益相关者平衡构建工具采用的权衡。

Abstract: Since developers invoke the build system frequently, its performance can
impact productivity. Modern artifact-based build tools accelerate builds, yet
prior work shows that teams may abandon them for alternatives that are easier
to maintain. While prior work shows why downgrades are performed, the
implications of downgrades remain largely unexplored. In this paper, we
describe a case study of the Kubernetes project, focusing on its downgrade from
an artifact-based build tool (Bazel) to a language-specific solution (Go
Build). We reproduce and analyze the full and incremental builds of change sets
during the downgrade period. On the one hand, we find that Bazel builds are
faster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose
a larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel
builds also impose a greater CPU load at parallelism settings above eight for
full builds and above one for incremental builds. We estimate that downgrading
from Bazel can increase CI resource costs by up to 76 explore whether our
observations generalize by replicating our Kubernetes study on four other
projects that also downgraded from Bazel to older build tools. We observe that
while build time penalties decrease, Bazel consistently consumes more memory.
We conclude that abandoning artifact-based build tools, despite perceived
maintainability benefits, tends to incur considerable performance costs for
large projects. Our observations may help stakeholders to balance trade-offs in
build tool adoption

</details>


### [186] [Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience](https://arxiv.org/abs/2510.20121)
*Carlos J. Fernandez-Candel,Jesus Garcia-Molina,Francisco Javier Bermudez Ruiz,Jose Ramon Hoyos Barcelo,Diego Sevilla Ruiz,Benito Jose Cuesta Viera*

Main category: cs.SE

TL;DR: 本文聚焦于将模型驱动的再工程过程应用于开发PL/SQL代码到Java代码的迁移工具，提出软件过程并阐述实施、验证及相关评估。


<details>
  <summary>Details</summary>
Motivation: 现代软件技术发展促使大量企业迁移RAD应用，研究团队与软件公司合作开发PL/SQL到Java的迁移解决方案。

Method: 以KDM模型表示遗留代码，提出软件过程，集成类TDD方法开发模型转换，并对生成代码进行三种验证。

Result: 详细解释了再工程方法的实施和验证，以及MDE应用相关问题的评估。

Conclusion: 未明确提及，但暗示所提出的模型驱动再工程过程可用于开发PL/SQL到Java的迁移工具。

Abstract: Model-driven software engineering (MDE) techniques are not only useful in
forward engineering scenarios, but can also be successfully applied to evolve
existing systems. RAD (Rapid Application Development) platforms emerged in the
nineties, but the success of modern software technologies motivated that a
large number of enterprises tackled the migration of their RAD applications,
such as Oracle Forms. Our research group has collaborated with a software
company in developing a solution to migrate PL/SQL monolithic code on Forms
triggers and program units to Java code separated in several tiers.
  Our research focused on the model-driven reengineering process applied to
develop the migration tool for the conversion of PL/SQL code to Java. Legacy
code is represented in form of KDM (Knowledge-Discovery Metamodel) models. In
this paper, we propose a software process to implement a model-driven
re-engineering. This process integrates a TDD-like approach to incrementally
develop model transformations with three kinds of validations for the generated
code. The implementation and validation of the re-engineering approach are
explained in detail, as well as the evaluation of some issues related with the
application of MDE.

</details>


### [187] [Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents](https://arxiv.org/abs/2510.20211)
*Zhenning Yang,Hui Guan,Victor Nicolet,Brandon Paulsen,Joey Dodds,Daniel Kroening,Ang Chen*

Main category: cs.SE

TL;DR: 本文介绍自动系统NSync，用于解决IaC框架与传统工具混用导致的基础设施漂移问题，实验表明其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 当IaC与控制台、CLI或SDK一起使用时，会失去对外部更改的可见性，导致基础设施漂移，本文旨在解决该问题。

Method: NSync通过云API调用检测漂移，利用代理架构和LLM推断意图、合成更新，还引入新评估管道。

Result: 在五个真实Terraform项目和372个漂移场景实验中，NSync在准确性和令牌效率上优于基线。

Conclusion: NSync能有效解决IaC基础设施漂移问题，提高了准确性和效率。

Abstract: Cloud infrastructure is managed through a mix of interfaces -- traditionally,
cloud consoles, command-line interfaces (CLI), and SDKs are the tools of
choice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have
quickly gained popularity. Unlike conventional tools, IaC~frameworks encode the
infrastructure in a "source-of-truth" configuration. They are capable of
automatically carrying out modifications to the cloud -- deploying, updating,
or destroying resources -- to bring the actual infrastructure into alignment
with the IaC configuration. However, when IaC is used alongside consoles, CLIs,
or SDKs, it loses visibility into external changes, causing infrastructure
drift, where the configuration becomes outdated, and later IaC operations may
undo valid updates or trigger errors.
  We present NSync, an automated system for IaC reconciliation that propagates
out-of-band changes back into the IaC program. Our key insight is that
infrastructure changes eventually all occur via cloud API invocations -- the
lowest layer for cloud management operations. NSync gleans insights from API
traces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update
the IaC configuration to capture the changes). It employs an agentic
architecture that leverages LLMs to infer high-level intents from noisy API
sequences, synthesize targeted IaC updates using specialized tools, and
continually improve through a self-evolving knowledge base of past
reconciliations. We further introduce a novel evaluation pipeline for injecting
realistic drifts into cloud infrastructure and assessing reconciliation
performance. Experiments across five real-world Terraform projects and 372
drift scenarios show that NSync outperforms the baseline both in terms of
accuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\times$
improvement).

</details>


### [188] [Classport: Designing Runtime Dependency Introspection for Java](https://arxiv.org/abs/2510.20340)
*Serena Cofano,Daniel Williams,Aman Sharma,Martin Monperrus*

Main category: cs.SE

TL;DR: 提出Classport系统解决Java缺乏运行时依赖内省支持的问题，并通过评估证明其可行性，为运行时完整性检查开辟新途径。


<details>
  <summary>Details</summary>
Motivation: Java缺乏运行时依赖内省支持，而该功能对软件供应链安全至关重要。

Method: 开发Classport系统，将依赖信息嵌入Java类文件以实现运行时依赖信息检索。

Result: 在六个真实项目上评估Classport，证明其在运行时识别依赖的可行性。

Conclusion: Classport实现的运行时依赖内省为运行时完整性检查提供了重要方向。

Abstract: Runtime introspection of dependencies, i.e., the ability to observe which
dependencies are currently used during program execution, is fundamental for
Software Supply Chain security. Yet, Java has no support for it. We solve this
problem with Classport, a system that embeds dependency information into Java
class files, enabling the retrieval of dependency information at runtime. We
evaluate Classport on six real-world projects, demonstrating the feasibility in
identifying dependencies at runtime. Runtime dependency introspection with
Classport opens important avenues for runtime integrity checking.

</details>


### [189] [Symmetry in Software Platforms as an Architectural Principle](https://arxiv.org/abs/2510.20389)
*Bjorn Remseth*

Main category: cs.SE

TL;DR: 探讨软件平台结构规律与架构鲁棒性的关系


<details>
  <summary>Details</summary>
Motivation: 研究软件平台架构鲁棒性的来源

Method: 未提及

Result: 未提及

Conclusion: 架构鲁棒性源于执行结构规律

Abstract: Software platforms often act as structure preserving systems. They provide
consistent interfaces and behaviors that remain stable under specific
transformations that we denote as symmetries. This paper explores the idea that
architectural robustness emerges from enforcing such structural regularities

</details>


### [190] [FMI-Based Distributed Co-Simulation with Enhanced Security and Intellectual Property Safeguards](https://arxiv.org/abs/2510.20403)
*Santiago Gil,Ecem E. Baş,Christian D. Jensen,Sebastian Engelsgaard,Giuseppe Abbiati,Cláudio Gomes*

Main category: cs.SE

TL;DR: 提出基于UniFMU的分布式协同仿真方法，增强网络安全和IP保护，并用演示验证并分析权衡。


<details>
  <summary>Details</summary>
Motivation: 分布式协同仿真虽能保护IP，但缺乏无潜在黑客攻击的连续或混合系统协同仿真指南。

Method: 在UniFMU之上提出一种分布式协同仿真方法，确保连接由客户端发起，模型和二进制文件在可信平台。

Result: 用两个协同仿真演示在四种不同网络设置中展示该方法功能。

Conclusion: 分析了IP保护的分布式和性能效率之间的权衡。

Abstract: Distributed co-simulation plays a key role in enabling collaborative modeling
and simulation by different stakeholders while protecting their Intellectual
Property (IP). Although IP protection is provided implicitly by co-simulation,
there is no consensus in the guidelines to conduct distributed co-simulation of
continuous-time or hybrid systems with no exposure to potential hacking
attacks. We propose an approach for distributed co-simulation on top of UniFMU
with enhanced cybersecurity and IP protection mechanisms, ensuring that the
connection is initiated by the client and the models and binaries live on
trusted platforms. We showcase the functionality of this approach using two
co-simulation demos in four different network settings and analyze the
trade-off between IP-protected distribution and performance efficiency in these
settings.

</details>


### [191] [Toward Practical Deductive Verification: Insights from a Qualitative Survey in Industry and Academia](https://arxiv.org/abs/2510.20514)
*Lea Salome Brugger,Xavier Denis,Peter Müller*

Main category: cs.SE

TL;DR: 研究演绎验证成功应用的因素与推广障碍，通过访谈分析，得出未被充分探索的障碍并给出建议。


<details>
  <summary>Details</summary>
Motivation: 演绎验证虽有用但未成为主流技术，为其广泛应用探索因素与障碍。

Method: 对30位来自行业和学术界的验证从业者进行半结构化访谈，用主题分析方法系统分析数据。

Result: 证实常见挑战，揭示未被充分探索的障碍，如证明维护、自动化控制不足和可用性问题。

Conclusion: 根据数据分析结果提取演绎验证的推动因素和障碍，为从业者、工具开发者和研究人员提出具体建议。

Abstract: Deductive verification is an effective method to ensure that a given system
exposes the intended behavior. In spite of its proven usefulness and
feasibility in selected projects, deductive verification is still not a
mainstream technique. To pave the way to widespread use, we present a study
investigating the factors enabling successful applications of deductive
verification and the underlying issues preventing broader adoption. We
conducted semi-structured interviews with 30 practitioners of verification from
both industry and academia and systematically analyzed the collected data
employing a thematic analysis approach. Beside empirically confirming familiar
challenges, e.g., the high level of expertise needed for conducting formal
proofs, our data reveal several underexplored obstacles, such as proof
maintenance, insufficient control over automation, and usability concerns. We
further use the results from our data analysis to extract enablers and barriers
for deductive verification and formulate concrete recommendations for
practitioners, tool builders, and researchers, including principles for
usability, automation, and integration with existing workflows.

</details>


### [192] [Large Language Models for Fault Localization: An Empirical Study](https://arxiv.org/abs/2510.20521)
*YingJian Xiao,RongQun Hu,WeiWei Gong,HongWei Li,AnQuan Jie*

Main category: cs.SE

TL;DR: 本文对大语言模型在语句级代码故障定位任务上进行实证研究，评估多种模型，研究不同提示策略影响，发现结合错误报告上下文可提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自动程序修复中效果依赖上游故障定位，但目前缺乏全面评估，因此开展对大语言模型在语句级代码故障定位任务的研究。

Method: 评估代表性开源和闭源模型在HumanEval - Java和Defects4J数据集上的故障定位能力，研究标准提示、少样本示例和思维链等不同提示策略对模型性能的影响。

Result: 结合错误报告上下文显著提升模型性能；少样本学习有提升潜力但边际收益递减；思维链推理效果高度依赖模型固有推理能力。

Conclusion: 本研究凸显不同模型在故障定位任务中的性能特点和权衡，为了解当前大语言模型优势和提升故障定位有效性的策略提供有价值见解。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code-related tasks, particularly in automated program repair. However, the
effectiveness of such repairs is highly dependent on the performance of
upstream fault localization, for which comprehensive evaluations are currently
lacking. This paper presents a systematic empirical study on LLMs in the
statement-level code fault localization task. We evaluate representative
open-source models (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source
models (GPT-4.1 mini, Gemini-2.5-flash) to assess their fault localization
capabilities on the HumanEval-Java and Defects4J datasets. The study
investigates the impact of different prompting strategies--including standard
prompts, few-shot examples, and chain-of-reasoning--on model performance, with
a focus on analysis across accuracy, time efficiency, and economic cost
dimensions. Our experimental results show that incorporating bug report context
significantly enhances model performance. Few-shot learning shows potential for
improvement but exhibits noticeable diminishing marginal returns, while
chain-of-thought reasoning's effectiveness is highly contingent on the model's
inherent reasoning capabilities. This study not only highlights the performance
characteristics and trade-offs of different models in fault localization tasks,
but also offers valuable insights into the strengths of current LLMs and
strategies for improving fault localization effectiveness.

</details>


### [193] [A Soundness and Precision Benchmark for Java Debloating Tools](https://arxiv.org/abs/2510.20679)
*Jonas Klauke,Tom Ohlmer,Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Eric Bodden*

Main category: cs.SE

TL;DR: 开发Deblometer评估Java去臃肿工具，评估发现工具存在移除必要构造问题，需改进。


<details>
  <summary>Details</summary>
Motivation: 软件依赖存在大量不必要代码，去臃肿工具需平衡精度和完整性，需系统评估此权衡。

Method: 开发含59个测试用例的Deblometer评估工具，对Deptrim、JShrink和ProGuard三个Java去臃肿工具进行评估。

Result: 所有工具都移除了必要程序构造，导致语义改变或执行崩溃，Deptrim保留更多臃肿构造，ProGuard移除更多必要构造，JShrink因注解支持有限影响完整性。

Conclusion: 去臃肿工具存在完整性问题，需改进以确保去臃肿软件稳定可靠。

Abstract: Modern software development reuses code by importing libraries as
dependencies. Software projects typically include an average of 36
dependencies, with 80% being transitive, meaning they are dependencies of
dependencies. Recent research indicates that only 24.9% of these dependencies
are required at runtime, and even within those, many program constructs remain
unused, adding unnecessary code to the project. This has led to the development
of debloating tools that remove unnecessary dependencies and program constructs
while balancing precision by eliminating unused constructs and soundness by
preserving all required constructs. To systematically evaluate this trade-off,
we developed Deblometer, a micro-benchmark consisting of 59 test cases designed
to assess support for various Java language features in debloating tools. Each
test case includes a manually curated ground truth specifying necessary and
bloated classes, methods, and fields, enabling precise measurement of soundness
and precision. Using Deblometer, we evaluated three popular Java debloating
tools: Deptrim, JShrink, and ProGuard. Our evaluation reveals that all tools
remove required program constructs, which results in changed semantics or
execution crashes. In particular, the dynamic class loading feature introduces
unsoundness in all evaluated tools. Our comparison shows that Deptrim retains
more bloated constructs, while ProGuard removes more required constructs.
JShrink's soundness is significantly affected by limited support for
annotations, which leads to corrupted debloated artifacts. These soundness
issues highlight the need to improve debloating tools to ensure stable and
reliable debloated software.

</details>


### [194] [Exploring Large Language Models for Access Control Policy Synthesis and Summarization](https://arxiv.org/abs/2510.20692)
*Adarsh Vatsa,Bethel Hall,William Eiers*

Main category: cs.SE

TL;DR: 本文探讨大语言模型（LLMs）在访问控制策略合成与总结方面的有效性，发现LLMs在自动策略生成有障碍，但结合符号方法分析现有策略有前景。


<details>
  <summary>Details</summary>
Motivation: 云访问控制策略手动编写易出错且难分析，而LLMs在代码合成和总结上成功，可用于自动生成或理解策略。

Method: 先研究不同LLMs进行访问控制策略合成，后引入基于语义的请求总结方法利用LLMs分析策略。

Result: LLMs能生成语法正确策略，但非推理LLMs生成等效策略的比例为45.8%，推理LLMs为93.7%；结合符号方法分析现有策略有良好结果。

Conclusion: 利用LLMs进行自动策略生成有重大障碍，但结合符号方法分析现有策略有希望。

Abstract: Cloud computing is ubiquitous, with a growing number of services being hosted
on the cloud every day. Typical cloud compute systems allow administrators to
write policies implementing access control rules which specify how access to
private data is governed. These policies must be manually written, and due to
their complexity can often be error prone. Moreover, existing policies often
implement complex access control specifications and thus can be difficult to
precisely analyze in determining their behavior works exactly as intended.
Recently, Large Language Models (LLMs) have shown great success in automated
code synthesis and summarization. Given this success, they could potentially be
used for automatically generating access control policies or aid in
understanding existing policies. In this paper, we explore the effectiveness of
LLMs for access control policy synthesis and summarization. Specifically, we
first investigate diverse LLMs for access control policy synthesis, finding
that: although LLMs can effectively generate syntactically correct policies,
they have permissiveness issues, generating policies equivalent to the given
specification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time
for reasoning LLMs. We then investigate how LLMs can be used to analyze
policies by introducing a novel semantic-based request summarization approach
which leverages LLMs to generate a precise characterization of the requests
allowed by a policy. Our results show that while there are significant hurdles
in leveraging LLMs for automated policy generation, LLMs show promising results
when combined with symbolic approaches in analyzing existing policies.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [195] [FinCARE: Financial Causal Analysis with Reasoning and Evidence](https://arxiv.org/abs/2510.20221)
*Alejandro Michel,Abhinav Arun,Bhaskarjit Sarmah,Stefano Pasquali*

Main category: q-fin.CP

TL;DR: 提出结合统计因果发现算法、金融知识图谱和大语言模型推理的混合框架，在合成金融数据集上评估有显著提升，可用于场景分析和风险管理。


<details>
  <summary>Details</summary>
Motivation: 现有基于相关性分析和启发式方法无法捕捉驱动绩效的真实因果关系，需更好方法。

Method: 将统计因果发现算法与从SEC 10 - K文件提取的金融知识图谱和大语言模型推理的领域知识相结合，增强三种因果发现范式。

Result: 在合成金融数据集上，KG + LLM增强方法在三种算法上F1值均有显著提升，能进行可靠场景分析。

Conclusion: 该框架解决现有方法局限，为投资组合经理在动态市场环境中提供因果基础用于风险管理和决策。

Abstract: Portfolio managers rely on correlation-based analysis and heuristic methods
that fail to capture true causal relationships driving performance. We present
a hybrid framework that integrates statistical causal discovery algorithms with
domain knowledge from two complementary sources: a financial knowledge graph
extracted from SEC 10-K filings and large language model reasoning. Our
approach systematically enhances three representative causal discovery
paradigms, constraint-based (PC), score-based (GES), and continuous
optimization (NOTEARS), by encoding knowledge graph constraints algorithmically
and leveraging LLM conceptual reasoning for hypothesis generation. Evaluated on
a synthetic financial dataset of 500 firms across 18 variables, our
KG+LLM-enhanced methods demonstrate consistent improvements across all three
algorithms: PC (F1: 0.622 vs. 0.459 baseline, +36%), GES (F1: 0.735 vs. 0.367,
+100%), and NOTEARS (F1: 0.759 vs. 0.163, +366%). The framework enables
reliable scenario analysis with mean absolute error of 0.003610 for
counterfactual predictions and perfect directional accuracy for intervention
effects. It also addresses critical limitations of existing methods by
grounding statistical discoveries in financial domain expertise while
maintaining empirical validation, providing portfolio managers with the causal
foundation necessary for proactive risk management and strategic
decision-making in dynamic market environments.

</details>


### [196] [Fusing Narrative Semantics for Financial Volatility Forecasting](https://arxiv.org/abs/2510.20699)
*Yaxuan Kong,Yoontae Hwang,Marcus Kaiser,Chris Vryonides,Roel Oomen,Stefan Zohren*

Main category: q-fin.CP

TL;DR: 提出M2VN框架用于金融波动率预测，结合时间序列特征与新闻数据，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决金融波动率预测中异构数据融合和避免前瞻性偏差问题，提升金融模型有效性。

Method: 结合开源市场特征与Time Machine GPT生成的新闻嵌入，引入辅助对齐损失，利用深度神经网络。

Result: M2VN在实验中始终优于现有基线。

Conclusion: M2VN对动态市场中的风险管理和金融决策有实际价值。

Abstract: We introduce M2VN: Multi-Modal Volatility Network, a novel deep
learning-based framework for financial volatility forecasting that unifies time
series features with unstructured news data. M2VN leverages the
representational power of deep neural networks to address two key challenges in
this domain: (i) aligning and fusing heterogeneous data modalities, numerical
financial data and textual information, and (ii) mitigating look-ahead bias
that can undermine the validity of financial models. To achieve this, M2VN
combines open-source market features with news embeddings generated by Time
Machine GPT, a recently introduced point-in-time LLM, ensuring temporal
integrity. An auxiliary alignment loss is introduced to enhance the integration
of structured and unstructured data within the deep learning architecture.
Extensive experiments demonstrate that M2VN consistently outperforms existing
baselines, underscoring its practical value for risk management and financial
decision-making in dynamic markets.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [197] [Market-Implied Sustainability: Insights from Funds' Portfolio Holdings](https://arxiv.org/abs/2510.20434)
*Rosella Giacometti,Gabriele Torri,Marco Bonomelli,Davide Lauria*

Main category: q-fin.PM

TL;DR: 开发基于可持续基金与传统基金股票持仓差异的公司市场隐含可持续性得分（SMIS），分析影响因素并与ESG得分对比，发现SMIS与传统ESG得分差异大，高SMIS公司有显著财务表现。


<details>
  <summary>Details</summary>
Motivation: 开发公司市场隐含可持续性得分。

Method: 依据SFDR框架识别可持续基金，计算欧洲公司的SMIS得分，进行计量经济分析识别影响因素并与Refinitiv的ESG得分对比，用投资组合倾斜策略评估股票风险调整后表现。

Result: SMIS得分与传统ESG得分有较大偏差，2010 - 2023年高SMIS公司有显著财务表现。

Conclusion: 市场隐含可持续性得分与传统ESG得分不同，高SMIS公司在财务上表现更优。

Abstract: In this work, we aim to develop a market-implied sustainability score for
companies, based on the extent to which a stock is over- or under-represented
in sustainable funds compared to traditional ones. To identify sustainable
funds, we rely on the Sustainable Finance Disclosure Regulation (SFDR), a
European framework designed to clearly categorize investment funds into
different classes according to their commitment to sustainability. In our
analysis, we classify as sustainable those funds categorized as Article 9 -
also known as "dark green" - and compare them to funds categorized as Article 8
or Article 6.
  We compute an SFDR Market-Implied Sustainability (SMIS) score for a large set
of European companies. We then conduct an econometric analysis to identify the
factors influencing SMIS and compare them with state-of-the-art ESG
(Environmental, Social, and Governance) scores provided by Refinitiv. Finally,
we assess the realized risk-adjusted performance of stocks using
portfolio-tilting strategies.
  Our results show that SMIS scores deviate substantially from traditional ESG
scores and that, over the period 2010-2023, companies with high SMIS have been
associated with significant financial outperformance.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [198] [Enhanced Cyclic Coordinate Descent Methods for Elastic Net Penalized Linear Models](https://arxiv.org/abs/2510.19999)
*Yixiao Wang,Zishan Shao,Ting Jiang,Aditya Devarakonda*

Main category: stat.ML

TL;DR: 提出增强循环坐标下降（ECCD）框架求解带弹性网络约束的广义线性模型，减少训练时间，性能平均提升3倍。


<details>
  <summary>Details</summary>
Motivation: 现有方法训练时间长，需要更高效的求解带弹性网络约束的广义线性模型的方法。

Method: 对CD方法重新设计，围绕当前迭代进行泰勒展开，避免梯度计算中的非线性操作，将向量递归展开为批量计算，用可调参数s控制展开，用C++和Eigen库实现。

Result: 在不同基准数据集上，正则化路径变体平均性能提升3倍。

Conclusion: ECCD框架能有效减少训练时间，避免块坐标下降的收敛延迟和数值不稳定问题。

Abstract: We present a novel enhanced cyclic coordinate descent (ECCD) framework for
solving generalized linear models with elastic net constraints that reduces
training time in comparison to existing state-of-the-art methods. We redesign
the CD method by performing a Taylor expansion around the current iterate to
avoid nonlinear operations arising in the gradient computation. By introducing
this approximation, we are able to unroll the vector recurrences occurring in
the CD method and reformulate the resulting computations into more efficient
batched computations. We show empirically that the recurrence can be unrolled
by a tunable integer parameter, $s$, such that $s > 1$ yields performance
improvements without affecting convergence, whereas $s = 1$ yields the original
CD method. A key advantage of ECCD is that it avoids the convergence delay and
numerical instability exhibited by block coordinate descent. Finally, we
implement our proposed method in C++ using Eigen to accelerate linear algebra
computations. Comparison of our method against existing state-of-the-art
solvers shows consistent performance improvements of $3\times$ in average for
regularization path variant on diverse benchmark datasets. Our implementation
is available at https://github.com/Yixiao-Wang-Stats/ECCD.

</details>


### [199] [Compositional Generation for Long-Horizon Coupled PDEs](https://arxiv.org/abs/2510.20141)
*Somayajulu L. N. Dhulipala,Deep Ray,Nicholas Forman*

Main category: stat.ML

TL;DR: 研究组合扩散方法模拟耦合PDE系统，在长时程下评估，结果显示该方法可行。


<details>
  <summary>Details</summary>
Motivation: 模拟耦合PDE系统计算量大，现有方法需大量联合数据，探索新方法。

Method: 在解耦PDE数据上训练扩散模型，推理时组合；比较基线扩散模型和v - 参数化策略训练的模型；引入对称组合方案；在特定方程上评估并与基于耦合数据训练的Fourier Neural Operator对比。

Result: 组合扩散模型仅用解耦训练数据就能低误差恢复耦合轨迹；v - 参数化可提升准确性；基于耦合数据训练的神经算子代理效果最强。

Conclusion: 组合扩散是高效、长时程建模耦合PDE的可行策略。

Abstract: Simulating coupled PDE systems is computationally intensive, and prior
efforts have largely focused on training surrogates on the joint (coupled)
data, which requires a large amount of data. In the paper, we study
compositional diffusion approaches where diffusion models are only trained on
the decoupled PDE data and are composed at inference time to recover the
coupled field. Specifically, we investigate whether the compositional strategy
can be feasible under long time horizons involving a large number of time
steps. In addition, we compare a baseline diffusion model with that trained
using the v-parameterization strategy. We also introduce a symmetric
compositional scheme for the coupled fields based on the Euler scheme. We
evaluate on Reaction-Diffusion and modified Burgers with longer time grids, and
benchmark against a Fourier Neural Operator trained on coupled data. Despite
seeing only decoupled training data, the compositional diffusion models recover
coupled trajectories with low error. v-parameterization can improve accuracy
over a baseline diffusion model, while the neural operator surrogate remains
strongest given that it is trained on the coupled data. These results show that
compositional diffusion is a viable strategy towards efficient, long-horizon
modeling of coupled PDEs.

</details>


### [200] [Neural Networks for Censored Expectile Regression Based on Data Augmentation](https://arxiv.org/abs/2510.20344)
*Wei Cao,Shanshan Wang*

Main category: stat.ML

TL;DR: 提出基于数据增强的期望分位数回归神经网络算法DAERNN处理异质删失数据，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有期望分位数回归神经网络研究多关注全观测数据，对删失数据场景研究有限。

Method: 提出基于数据增强的期望分位数回归神经网络算法DAERNN。

Result: 模拟研究和真实数据应用表明DAERNN优于现有删失ERNN方法，预测性能与全观测数据训练的模型相当。

Conclusion: DAERNN为处理各种删失机制提供统一框架，无需显式参数模型指定，增强了在实际删失数据分析中的适用性。

Abstract: Expectile regression neural networks (ERNNs) are powerful tools for capturing
heterogeneity and complex nonlinear structures in data. However, most existing
research has primarily focused on fully observed data, with limited attention
paid to scenarios involving censored observations. In this paper, we propose a
data augmentation based ERNNs algorithm, termed DAERNN, for modeling
heterogeneous censored data. The proposed DAERNN is fully data driven, requires
minimal assumptions, and offers substantial flexibility. Simulation studies and
real data applications demonstrate that DAERNN outperforms existing censored
ERNNs methods and achieves predictive performance comparable to models trained
on fully observed data. Moreover, the algorithm provides a unified framework
for handling various censoring mechanisms without requiring explicit parametric
model specification, thereby enhancing its applicability to practical censored
data analysis.

</details>


### [201] [Testing Most Influential Sets](https://arxiv.org/abs/2510.20372)
*Lucas Darius Konrad,Nikolas Kuschnig*

Main category: stat.ML

TL;DR: 本文提出评估最具影响力数据集统计显著性的框架，有理论结果并通过多领域应用展示其价值。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏正式理论判断最具影响力数据集的影响是真实问题还是抽样误差。

Method: 开发一个有原则的框架来评估最具影响力数据集的统计显著性。

Result: 理论结果刻画了最大影响的极值分布，可进行严格的假设检验，取代了现有的临时敏感性检查。

Conclusion: 该方法在经济学、生物学和机器学习基准测试等应用中具有实际价值。

Abstract: Small subsets of data with disproportionate influence on model outcomes can
have dramatic impacts on conclusions, with a few data points sometimes
overturning key findings. While recent work has developed methods to identify
these \emph{most influential sets}, no formal theory exists to determine when
their influence reflects genuine problems rather than natural sampling
variation. We address this gap by developing a principled framework for
assessing the statistical significance of most influential sets. Our
theoretical results characterize the extreme value distributions of maximal
influence and enable rigorous hypothesis tests for excessive influence,
replacing current ad-hoc sensitivity checks. We demonstrate the practical value
of our approach through applications across economics, biology, and machine
learning benchmarks.

</details>


### [202] [Learning Decentralized Routing Policies via Graph Attention-based Multi-Agent Reinforcement Learning in Lunar Delay-Tolerant Networks](https://arxiv.org/abs/2510.20436)
*Federico Lozano-Cuadra,Beatriz Soret,Marc Sanchez Net,Abhishek Cauligi,Federico Rossi*

Main category: stat.ML

TL;DR: 提出用于多机器人月球探索任务的全去中心化路由框架GAT - MARL，模拟显示其性能优且可扩展。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人在月球延迟容忍网络下，间歇性连接和未知移动模式中向着陆器中继数据的路由问题。

Method: 将问题建模为POMDP，提出基于图注意力的多智能体强化学习策略GAT - MARL，采用集中训练、分散执行方式，仅依赖局部观测。

Result: 蒙特卡罗模拟显示，GAT - MARL有更高交付率、无重复、更少丢包，能利用短期移动预测。

Conclusion: GAT - MARL为未来行星探索的太空机器人系统提供可扩展解决方案，对更大漫游车团队有良好泛化性。

Abstract: We present a fully decentralized routing framework for multi-robot
exploration missions operating under the constraints of a Lunar Delay-Tolerant
Network (LDTN). In this setting, autonomous rovers must relay collected data to
a lander under intermittent connectivity and unknown mobility patterns. We
formulate the problem as a Partially Observable Markov Decision Problem (POMDP)
and propose a Graph Attention-based Multi-Agent Reinforcement Learning
(GAT-MARL) policy that performs Centralized Training, Decentralized Execution
(CTDE). Our method relies only on local observations and does not require
global topology updates or packet replication, unlike classical approaches such
as shortest path and controlled flooding-based algorithms. Through Monte Carlo
simulations in randomized exploration environments, GAT-MARL provides higher
delivery rates, no duplications, and fewer packet losses, and is able to
leverage short-term mobility forecasts; offering a scalable solution for future
space robotic systems for planetary exploration, as demonstrated by successful
generalization to larger rover teams.

</details>


### [203] [Concentration and excess risk bounds for imbalanced classification with synthetic oversampling](https://arxiv.org/abs/2510.20472)
*Touqeer Ahmad,Mohammadreza M. Kalan,François Portier,Gilles Stupfler*

Main category: stat.ML

TL;DR: 本文为SMOTE及其相关方法在合成数据上训练分类器的行为建立理论框架，给出浓度界和风险保证，并提供参数调整指南和数值实验。


<details>
  <summary>Details</summary>
Motivation: SMOTE及其变体处理不平衡分类问题虽实践成功，但理论基础探索不足。

Method: 推导合成少数样本经验风险与真实少数分布总体风险差异的一致浓度界，为基于合成数据训练的核分类器提供非参数超额风险保证。

Result: 得出浓度界和风险保证，为SMOTE和下游学习算法的参数调整提供实用指南，有数值实验支持理论发现。

Conclusion: 所建立的理论框架有助于更好理解和调整SMOTE及相关方法。

Abstract: Synthetic oversampling of minority examples using SMOTE and its variants is a
leading strategy for addressing imbalanced classification problems. Despite the
success of this approach in practice, its theoretical foundations remain
underexplored. We develop a theoretical framework to analyze the behavior of
SMOTE and related methods when classifiers are trained on synthetic data. We
first derive a uniform concentration bound on the discrepancy between the
empirical risk over synthetic minority samples and the population risk on the
true minority distribution. We then provide a nonparametric excess risk
guarantee for kernel-based classifiers trained using such synthetic data. These
results lead to practical guidelines for better parameter tuning of both SMOTE
and the downstream learning algorithm. Numerical experiments are provided to
illustrate and support the theoretical findings

</details>


### [204] [Diffusion Autoencoders with Perceivers for Long, Irregular and Multimodal Astronomical Sequences](https://arxiv.org/abs/2510.20595)
*Yunyi Shen,Alexander Gagliano*

Main category: stat.ML

TL;DR: 提出Diffusion Autoencoder with Perceivers (daep)用于不规则、异构序列数据表示学习，表现优于VAE和maep。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习架构多在规则采样输入数据验证，而科学领域存在大量不规则、异构序列数据，需新方法提取语义信息。

Method: 提出daep，对异构测量数据进行分词，用Perceiver编码器压缩，用Perceiver - IO扩散解码器重建；将掩码自编码器改编为Perceiver编码器/解码器设计建立maep基线。

Result: 在不同天文数据集上，daep重建误差更低，潜在空间更具判别性，更能保留精细结构。

Conclusion: daep是处理不规则、异构序列数据科学领域的有效框架。

Abstract: Self-supervised learning has become a central strategy for representation
learning, but the majority of architectures used for encoding data have only
been validated on regularly-sampled inputs such as images, audios. and videos.
In many scientific domains, data instead arrive as long, irregular, and
multimodal sequences. To extract semantic information from these data, we
introduce the Diffusion Autoencoder with Perceivers (daep). daep tokenizes
heterogeneous measurements, compresses them with a Perceiver encoder, and
reconstructs them with a Perceiver-IO diffusion decoder, enabling scalable
learning in diverse data settings. To benchmark the daep architecture, we adapt
the masked autoencoder to a Perceiver encoder/decoder design, and establish a
strong baseline (maep) in the same architectural family as daep. Across diverse
spectroscopic and photometric astronomical datasets, daep achieves lower
reconstruction errors, produces more discriminative latent spaces, and better
preserves fine-scale structure than both VAE and maep baselines. These results
establish daep as an effective framework for scientific domains where data
arrives as irregular, heterogeneous sequences.

</details>


### [205] [Finding the Sweet Spot: Trading Quality, Cost, and Speed During Inference-Time LLM Reflection](https://arxiv.org/abs/2510.20653)
*Jack Butler,Nikita Kozodoi,Zainab Afolabi,Brian Tyacke,Gaiar Baimuratov*

Main category: stat.ML

TL;DR: 本文系统比较自反思和预算调优在数学推理和翻译任务中的表现，揭示自反思有效性的领域依赖差异，为特定领域和资源约束下选择推理策略提供指导并开源实现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理时性能提升方法会在准确性、成本和延迟间产生复杂权衡，不同领域情况不明，需系统比较自反思和预算调优。

Method: 在数学推理和翻译任务中比较自反思和预算调优，评估多个大语言模型在不同反思深度和计算预算下的表现，推导帕累托最优性能前沿，还在真实营销内容本地化系统中验证。

Result: 自反思有效性存在显著领域依赖差异，数学推理中性能提升达220%，在真实系统中也表现出市场依赖有效性。

Conclusion: 为特定领域和资源约束下选择最优推理策略提供了可操作的指导，强调领域特定评估的重要性。

Abstract: As Large Language Models (LLMs) continue to evolve, practitioners face
increasing options for enhancing inference-time performance without model
retraining, including budget tuning and multi-step techniques like
self-reflection. While these methods improve output quality, they create
complex trade-offs among accuracy, cost, and latency that remain poorly
understood across different domains. This paper systematically compares
self-reflection and budget tuning across mathematical reasoning and translation
tasks. We evaluate prominent LLMs, including Anthropic Claude, Amazon Nova, and
Mistral families, along with other models under varying reflection depths and
compute budgets to derive Pareto optimal performance frontiers. Our analysis
reveals substantial domain dependent variation in self-reflection
effectiveness, with performance gains up to 220\% in mathematical reasoning. We
further investigate how reflection round depth and feedback mechanism quality
influence performance across model families. To validate our findings in a
real-world setting, we deploy a self-reflection enhanced marketing content
localisation system at Lounge by Zalando, where it shows market-dependent
effectiveness, reinforcing the importance of domain specific evaluation when
deploying these techniques. Our results provide actionable guidance for
selecting optimal inference strategies given specific domains and resource
constraints. We open source our self-reflection implementation for
reproducibility at
https://github.com/aws-samples/sample-genai-reflection-for-bedrock.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [206] [Approximating evidence via bounded harmonic means](https://arxiv.org/abs/2510.20617)
*Dana Naderi,Christian P Robert,Kaniav Kamary,Darren Wraith§*

Main category: stat.CO

TL;DR: 本文提出基于HPD区域椭圆覆盖的ECMLE估计器，解决原HME无限方差问题，在多模态场景可用，且性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 原HME估计器存在无限方差问题，需要改进方法来近似模型证据。

Method: 基于HPD区域的椭圆覆盖，用非重叠椭球提出ECMLE估计器。

Result: ECMLE解决了原HME无限方差问题，可用于多模态场景，性能优于THAMES和Mixture THAMES等方法，方差更低，证据近似更稳定。

Conclusion: ECMLE是一种有效的贝叶斯模型选择方法，能提供更稳定的证据近似。

Abstract: Efficient Bayesian model selection relies on the model evidence or marginal
likelihood, whose computation often requires evaluating an intractable
integral. The harmonic mean estimator (HME) has long been a standard method of
approximating the evidence. While computationally simple, the version
introduced by Newton and Raftery (1994) potentially suffers from infinite
variance. To overcome this issue, Gelfand and Dey (1994) defined a standardized
representation of the estimator based on an instrumental function and Robert
and Wraith (2009) later proposed to use higher posterior density (HPD)
indicators as instrumental functions. Following this approach, a practical
method is proposed, based on an elliptical covering of the HPD region with
non-overlapping ellipsoids. The resulting estimator (ECMLE) not only eliminates
the infinite-variance issue of the original HME and allows exact volume
computations, but is also able to be used in multi-modal settings. Through
several examples, we illustrate that ECMLE outperforms other recent methods
such as THAMES and Mixture THAMES (Metodiev et al., 2025). Moreover, ECMLE
demonstrates lower variance, a key challenge that subsequent HME variants have
sought to address, and provides more stable evidence approximations, even in
challenging settings.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [207] [Time-series Random Process Complexity Ranking Using a Bound on Conditional Differential Entropy](https://arxiv.org/abs/2510.20551)
*Jacob Ayers,Richard Hahnloser,Julia Ulrich,Lothar Sebastian Krapp,Remo Nitschke,Sabine Stoll,Balthasar Bickel,Reinhard Furrer*

Main category: eess.SP

TL;DR: 本文基于已有理论框架提升条件微分熵的界，并通过两个实验验证其用于时间序列复杂度排序的可行性，提供了基于信息论的计算可行方法。


<details>
  <summary>Details</summary>
Motivation: 条件微分熵直接计算高维未知分布过程较难，需寻找计算可行的时间序列复杂度排序方法。

Method: 在已有信息论预测误差界的理论框架基础上，利用Hadamard不等式和协方差矩阵半正定性质提升界；进行两个合成实验，一是线性自回归过程实验，二是生物启发合成音频数据复杂度排序实验。

Result: 通过实验表明该框架可利用下一步预测模型的预测误差对时间序列复杂度进行排序。

Conclusion: 该框架为时间序列复杂度排序提供了计算可行且有信息论理论基础的方法。

Abstract: Conditional differential entropy provides an intuitive measure for relatively
ranking time-series complexity by quantifying uncertainty in future
observations given past context. However, its direct computation for
high-dimensional processes from unknown distributions is often intractable.
This paper builds on the information theoretic prediction error bounds
established by Fang et al. \cite{fang2019generic}, which demonstrate that the
conditional differential entropy \textbf{$h(X_k \mid X_{k-1},...,X_{k-m})$} is
upper bounded by a function of the determinant of the covariance matrix of
next-step prediction errors for any next step prediction model. We add to this
theoretical framework by further increasing this bound by leveraging Hadamard's
inequality and the positive semi-definite property of covariance matrices.
  To see if these bounds can be used to rank the complexity of time series, we
conducted two synthetic experiments: (1) controlled linear autoregressive
processes with additive Gaussian noise, where we compare ordinary least squares
prediction error entropy proxies to the true entropies of various additive
noises, and (2) a complexity ranking task of bio-inspired synthetic audio data
with unknown entropy, where neural network prediction errors are used to
recover the known complexity ordering.
  This framework provides a computationally tractable method for time-series
complexity ranking using prediction errors from next-step prediction models,
that maintains a theoretical foundation in information theory.

</details>


### [208] [SSL-SE-EEG: A Framework for Robust Learning from Unlabeled EEG Data with Self-Supervised Learning and Squeeze-Excitation Networks](https://arxiv.org/abs/2510.19829)
*Meghna Roy Chowdhury,Yi Ding,Shreyas Sen*

Main category: eess.SP

TL;DR: 提出SSL - SE - EEG框架处理EEG信号，实验验证其高精度，适用于实时BCI应用。


<details>
  <summary>Details</summary>
Motivation: EEG在现实应用中面临噪声、数据缺失和标注成本高的挑战。

Method: 引入SSL - SE - EEG框架，结合自监督学习和挤压激励网络，将EEG信号转换为2D图像表示。

Result: 在多个数据集上实验达到了先进的准确率，如MindBigData上91%，TUH - AB上85%。

Conclusion: SSL - SE - EEG为生物医学信号分析等领域提供了有前景的解决方案。

Abstract: Electroencephalography (EEG) plays a crucial role in brain-computer
interfaces (BCIs) and neurological diagnostics, but its real-world deployment
faces challenges due to noise artifacts, missing data, and high annotation
costs. We introduce SSL-SE-EEG, a framework that integrates Self-Supervised
Learning (SSL) with Squeeze-and-Excitation Networks (SE-Nets) to enhance
feature extraction, improve noise robustness, and reduce reliance on labeled
data. Unlike conventional EEG processing techniques, SSL-SE-EEG} transforms EEG
signals into structured 2D image representations, suitable for deep learning.
Experimental validation on MindBigData, TUH-AB, SEED-IV and BCI-IV datasets
demonstrates state-of-the-art accuracy (91% in MindBigData, 85% in TUH-AB),
making it well-suited for real-time BCI applications. By enabling low-power,
scalable EEG processing, SSL-SE-EEG presents a promising solution for
biomedical signal analysis, neural engineering, and next-generation BCIs.

</details>


### [209] [Low-Latency Neural Inference on an Edge Device for Real-Time Handwriting Recognition from EEG Signals](https://arxiv.org/abs/2510.19832)
*Ovishake Sen,Raghav Soni,Darpan Virmani,Akshar Parekh,Patrick Lehman,Sarthak Jena,Adithi Katikhaneni,Adam Khalifa,Baibhab Chatterjee*

Main category: eess.SP

TL;DR: 本文展示结合机器学习与脑电特征提取，可在便携设备上实现高精度无创脑机接口实时神经解码。


<details>
  <summary>Details</summary>
Motivation: 有创脑机接口有手术风险，无创脑电信号信噪比和空间分辨率低，需提升其解码精度。

Method: 收集15名参与者32通道脑电数据，预处理后提取85个特征，用EEdGeNet架构训练，还选取10个关键特征。

Result: 系统在NVIDIA Jetson TX2上准确率达89.83%，单字符延迟914.18ms；选10个关键特征后延迟降至202.6ms，准确率损失不到1%。

Conclusion: 为支持实时通信的高精度、低延迟、全便携无创脑机接口开辟了道路。

Abstract: Brain-computer interfaces (BCIs) offer a pathway to restore communication for
individuals with severe motor or speech impairments. Imagined handwriting
provides an intuitive paradigm for character-level neural decoding, bridging
the gap between human intention and digital communication. While invasive
approaches such as electrocorticography (ECoG) achieve high accuracy, their
surgical risks limit widespread adoption. Non-invasive electroencephalography
(EEG) offers safer and more scalable alternatives but suffers from low
signal-to-noise ratio and spatial resolution, constraining its decoding
precision. This work demonstrates that advanced machine learning combined with
informative EEG feature extraction can overcome these barriers, enabling
real-time, high-accuracy neural decoding on portable edge devices. A 32-channel
EEG dataset was collected from fifteen participants performing imagined
handwriting. Signals were preprocessed with bandpass filtering and artifact
subspace reconstruction, followed by extraction of 85 time-, frequency-, and
graphical-domain features. A hybrid architecture, EEdGeNet, integrates a
Temporal Convolutional Network with a multilayer perceptron trained on the
extracted features. When deployed on an NVIDIA Jetson TX2, the system achieved
89.83 percent accuracy with 914.18 ms per-character latency. Selecting only ten
key features reduced latency by 4.5 times to 202.6 ms with less than 1 percent
loss in accuracy. These results establish a pathway for accurate, low-latency,
and fully portable non-invasive BCIs supporting real-time communication.

</details>


### [210] [A Transformer Inspired AI-based MIMO receiver](https://arxiv.org/abs/2510.20363)
*András Rácz,Tamás Borsos,András Veres,Benedek Csala*

Main category: eess.SP

TL;DR: 提出Transformer启发的MIMO检测方法AttDet，结合模型可解释性与数据驱动灵活性，模拟显示其能接近最优BER/BLER性能且复杂度可预测。


<details>
  <summary>Details</summary>
Motivation: 开发一种兼具模型可解释性和数据驱动灵活性的MIMO检测方法。

Method: 将每个传输层视为令牌，通过轻量级自注意力机制学习流间干扰，查询和键直接从估计的信道矩阵导出，值由匹配滤波器输出初始化并迭代细化。

Result: 在现实5G信道模型、高阶混合QAM调制和编码方案的链路级模拟中，AttDet可接近最优BER/BLER性能。

Conclusion: AttDet能在保持可预测多项式复杂度的同时接近最优性能。

Abstract: We present AttDet, a Transformer-inspired MIMO (Multiple Input Multiple
Output) detection method that treats each transmit layer as a token and learns
inter-stream interference via a lightweight self-attention mechanism. Queries
and keys are derived directly from the estimated channel matrix, so attention
scores quantify channel correlation. Values are initialized by matched-filter
outputs and iteratively refined. The AttDet design combines model-based
interpretability with data-driven flexibility. We demonstrate through
link-level simulations under realistic 5G channel models and high-order, mixed
QAM modulation and coding schemes, that AttDet can approach near-optimal
BER/BLER (Bit Error Rate/Block Error Rate) performance while maintaining
predictable, polynomial complexity.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [211] [Adversary-Aware Private Inference over Wireless Channels](https://arxiv.org/abs/2510.20518)
*Mohamed Seif,Malcolm Egan,Andrea J. Goldsmith,H. Vincent Poor*

Main category: cs.IT

TL;DR: 本文提出用于隐私保护的基于AI的传感框架，解决边缘网络特征传输的隐私问题。


<details>
  <summary>Details</summary>
Motivation: 边缘网络中传感器和模型服务器通常不共址，传输特征存在隐私泄露风险，现有差分隐私机制未解决单个特征保护问题。

Method: 提出一种新框架，设备在将提取的特征传输到模型服务器之前对其进行转换。

Result: 未提及

Conclusion: 未提及

Abstract: AI-based sensing at wireless edge devices has the potential to significantly
enhance Artificial Intelligence (AI) applications, particularly for vision and
perception tasks such as in autonomous driving and environmental monitoring. AI
systems rely both on efficient model learning and inference. In the inference
phase, features extracted from sensing data are utilized for prediction tasks
(e.g., classification or regression). In edge networks, sensors and model
servers are often not co-located, which requires communication of features. As
sensitive personal data can be reconstructed by an adversary, transformation of
the features are required to reduce the risk of privacy violations. While
differential privacy mechanisms provide a means of protecting finite datasets,
protection of individual features has not been addressed. In this paper, we
propose a novel framework for privacy-preserving AI-based sensing, where
devices apply transformations of extracted features before transmission to a
model server.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [212] [The Reality Gap in Robotics: Challenges, Solutions, and Best Practices](https://arxiv.org/abs/2510.20808)
*Elie Aljalbout,Jiaxu Xing,Angel Romero,Iretiayo Akinola,Caelan Reed Garrett,Eric Heiden,Abhishek Gupta,Tucker Hermans,Yashraj Narang,Dieter Fox,Davide Scaramuzza,Fabio Ramos*

Main category: cs.RO

TL;DR: 文章指出机器学习推动机器人领域发展，但模拟与现实存在差距阻碍系统迁移，虽有进展但挑战仍在，本文对仿真到现实迁移进行全面概述。


<details>
  <summary>Details</summary>
Motivation: 解决机器人仿真与现实环境差异导致的迁移难题，深入理解现实差距根源与解决方案。

Method: 通过介绍领域随机化、实到虚迁移、状态和动作抽象、虚实协同训练等技术，并综合概述仿真到现实迁移的相关情况。

Result: 展示了在多个平台上利用相关技术克服现实差距取得的有前景结果，但挑战依然存在。

Conclusion: 对仿真到现实迁移情况进行全面概述，强调需深入理解现实差距的原因和解决办法。

Abstract: Machine learning has facilitated significant advancements across various
robotics domains, including navigation, locomotion, and manipulation. Many such
achievements have been driven by the extensive use of simulation as a critical
tool for training and testing robotic systems prior to their deployment in
real-world environments. However, simulations consist of abstractions and
approximations that inevitably introduce discrepancies between simulated and
real environments, known as the reality gap. These discrepancies significantly
hinder the successful transfer of systems from simulation to the real world.
Closing this gap remains one of the most pressing challenges in robotics.
Recent advances in sim-to-real transfer have demonstrated promising results
across various platforms, including locomotion, navigation, and manipulation.
By leveraging techniques such as domain randomization, real-to-sim transfer,
state and action abstractions, and sim-real co-training, many works have
overcome the reality gap. However, challenges persist, and a deeper
understanding of the reality gap's root causes and solutions is necessary. In
this survey, we present a comprehensive overview of the sim-to-real landscape,
highlighting the causes, solutions, and evaluation metrics for the reality gap
and sim-to-real transfer.

</details>


### [213] [MemER: Scaling Up Memory for Robot Control via Experience Retrieval](https://arxiv.org/abs/2510.20328)
*Ajay Sridhar,Jennifer Pan,Satvik Sharma,Chelsea Finn*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Humans routinely rely on memory to perform tasks, yet most robot policies
lack this capability; our goal is to endow robot policies with the same
ability. Naively conditioning on long observation histories is computationally
expensive and brittle under covariate shift, while indiscriminate subsampling
of history leads to irrelevant or redundant information. We propose a
hierarchical policy framework, where the high-level policy is trained to select
and track previous relevant keyframes from its experience. The high-level
policy uses selected keyframes and the most recent frames when generating text
instructions for a low-level policy to execute. This design is compatible with
existing vision-language-action (VLA) models and enables the system to
efficiently reason over long-horizon dependencies. In our experiments, we
finetune Qwen2.5-VL-7B-Instruct and $\pi_{0.5}$ as the high-level and low-level
policies respectively, using demonstrations supplemented with minimal language
annotations. Our approach, MemER, outperforms prior methods on three real-world
long-horizon robotic manipulation tasks that require minutes of memory. Videos
and code can be found at https://jen-pan.github.io/memer/.

</details>


### [214] [PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning](https://arxiv.org/abs/2510.20406)
*Xiaogang Jia,Qian Wang,Anrui Wang,Han A. Wang,Balázs Gyenes,Emiliyan Gospodinov,Xinkai Jiang,Ge Li,Hongyi Zhou,Weiran Liao,Xi Huang,Maximilian Beck,Moritz Reuss,Rudolf Lioutikov,Gerhard Neumann*

Main category: cs.RO

TL;DR: 本文提出PointMapPolicy方法，将扩散策略应用于结构化点网格，结合xLSTM融合点图与RGB数据，在多种操作任务中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前点云方法难以捕捉细粒度细节，RGB方法缺乏几何感知，阻碍了机器人操作系统的精度和泛化能力。

Method: 引入PointMapPolicy，在不进行下采样的结构化点网格上设置扩散策略，使用xLSTM作为骨干模型融合点图与RGB数据。

Result: 通过在RoboCasa和CALVIN基准测试及真实机器人评估中进行大量实验，该方法在多种操作任务中达到了SOTA性能。

Conclusion: 提出的PointMapPolicy方法能有效融合多模态数据，提升机器人操作系统的性能。

Abstract: Robotic manipulation systems benefit from complementary sensing modalities,
where each provides unique environmental information. Point clouds capture
detailed geometric structure, while RGB images provide rich semantic context.
Current point cloud methods struggle to capture fine-grained detail, especially
for complex tasks, which RGB methods lack geometric awareness, which hinders
their precision and generalization. We introduce PointMapPolicy, a novel
approach that conditions diffusion policies on structured grids of points
without downsampling. The resulting data type makes it easier to extract shape
and spatial relationships from observations, and can be transformed between
reference frames. Yet due to their structure in a regular grid, we enable the
use of established computer vision techniques directly to 3D data. Using xLSTM
as a backbone, our model efficiently fuses the point maps with RGB data for
enhanced multi-modal perception. Through extensive experiments on the RoboCasa
and CALVIN benchmarks and real robot evaluations, we demonstrate that our
method achieves state-of-the-art performance across diverse manipulation tasks.
The overview and demos are available on our project page:
https://point-map.github.io/Point-Map/

</details>


### [215] [Real-Time Gait Adaptation for Quadrupeds using Model Predictive Control and Reinforcement Learning](https://arxiv.org/abs/2510.20706)
*Ganga Nair B,Prakrut Kotecha,Shishir Kolathaya*

Main category: cs.RO

TL;DR: 提出结合MPPI算法与Dreamer模块的优化框架用于四足机器人实时步态自适应，在仿真中节能效果显著。


<details>
  <summary>Details</summary>
Motivation: 无模型强化学习策略常收敛到单一步态致性能不佳，传统MPC缺乏环境适应性，需解决这些局限。

Method: 提出结合MPPI算法与Dreamer模块的优化框架，MPPI用Dreamer奖励联合优化动作和步态变量，引入学习的价值函数作为终端奖励。

Result: 在Unitree Go1仿真中，不同目标速度下平均能耗最多降低36.48%，能准确跟踪并保持自适应、合适的步态。

Conclusion: 所提优化框架能有效实现四足机器人实时步态自适应，降低能耗并保证性能。

Abstract: Model-free reinforcement learning (RL) has enabled adaptable and agile
quadruped locomotion; however, policies often converge to a single gait,
leading to suboptimal performance. Traditionally, Model Predictive Control
(MPC) has been extensively used to obtain task-specific optimal policies but
lacks the ability to adapt to varying environments. To address these
limitations, we propose an optimization framework for real-time gait adaptation
in a continuous gait space, combining the Model Predictive Path Integral (MPPI)
algorithm with a Dreamer module to produce adaptive and optimal policies for
quadruped locomotion. At each time step, MPPI jointly optimizes the actions and
gait variables using a learned Dreamer reward that promotes velocity tracking,
energy efficiency, stability, and smooth transitions, while penalizing abrupt
gait changes. A learned value function is incorporated as terminal reward,
extending the formulation to an infinite-horizon planner. We evaluate our
framework in simulation on the Unitree Go1, demonstrating an average reduction
of up to 36.48\% in energy consumption across varying target speeds, while
maintaining accurate tracking and adaptive, task-appropriate gaits.

</details>


### [216] [FieldGen: From Teleoperated Pre-Manipulation Trajectories to Field-Guided Data Generation](https://arxiv.org/abs/2510.20774)
*Wenhao Wang,Kehe Ye,Xinyu Zhou,Tianxing Chen,Cao Min,Qiaoming Zhu,Xiaokang Yang,Yongjian Shen,Yang Yang,Maoqing Yao,Yao Mu*

Main category: cs.RO

TL;DR: 提出FieldGen框架实现可扩展、多样且高质量的真实世界数据收集，减少人力投入，实验显示其训练策略效果更佳。


<details>
  <summary>Details</summary>
Motivation: 现有数据收集方法难以平衡规模、多样性和质量，仿真有差距，遥操作多样性有限且成本高。

Method: FieldGen将操作分解为预操作和精细操作阶段，利用人类演示获取关键信息，通过吸引力场生成多样轨迹，FieldGen - Reward添加奖励注释。

Result: 使用FieldGen训练的策略成功率更高、稳定性更好，长期数据收集人力显著减少。

Conclusion: FieldGen框架能有效解决现有数据收集问题，提升策略学习效果。

Abstract: Large-scale and diverse datasets are vital for training robust robotic
manipulation policies, yet existing data collection methods struggle to balance
scale, diversity, and quality. Simulation offers scalability but suffers from
sim-to-real gaps, while teleoperation yields high-quality demonstrations with
limited diversity and high labor cost. We introduce FieldGen, a field-guided
data generation framework that enables scalable, diverse, and high-quality
real-world data collection with minimal human supervision. FieldGen decomposes
manipulation into two stages: a pre-manipulation phase, allowing trajectory
diversity, and a fine manipulation phase requiring expert precision. Human
demonstrations capture key contact and pose information, after which an
attraction field automatically generates diverse trajectories converging to
successful configurations. This decoupled design combines scalable trajectory
diversity with precise supervision. Moreover, FieldGen-Reward augments
generated data with reward annotations to further enhance policy learning.
Experiments demonstrate that policies trained with FieldGen achieve higher
success rates and improved stability compared to teleoperation-based baselines,
while significantly reducing human effort in long-term real-world data
collection. Webpage is available at https://fieldgen.github.io/.

</details>


### [217] [GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation](https://arxiv.org/abs/2510.20813)
*Guangqi Jiang,Haoran Chang,Ri-Zhao Qiu,Yutong Liang,Mazeyu Ji,Jiyue Zhu,Zhao Dong,Xueyan Zou,Xiaolong Wang*

Main category: cs.RO

TL;DR: 本文提出GSWorld，结合3D高斯溅射与物理引擎的机器人操作模拟器，介绍新资产格式GSDF及应用。


<details>
  <summary>Details</summary>
Motivation: 开发可重复评估从真实机器人数据学习的策略、无需真实机器人进行sim2real策略训练的操作策略开发框架。

Method: 提出新资产格式GSDF，构建包含机器人和物体的GSDF数据库，将其与物理引擎结合。

Result: 展示了零样本sim2real像素到动作操作策略学习等五个有趣应用。

Conclusion: GSWorld能有效支持机器人操作策略开发及多种应用。

Abstract: This paper presents GSWorld, a robust, photo-realistic simulator for robotics
manipulation that combines 3D Gaussian Splatting with physics engines. Our
framework advocates "closing the loop" of developing manipulation policies with
reproducible evaluation of policies learned from real-robot data and sim2real
policy training without using real robots. To enable photo-realistic rendering
of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian
Scene Description File), that infuses Gaussian-on-Mesh representation with
robot URDF and other objects. With a streamlined reconstruction pipeline, we
curate a database of GSDF that contains 3 robot embodiments for single-arm and
bimanual manipulation, as well as more than 40 objects. Combining GSDF with
physics engines, we demonstrate several immediate interesting applications: (1)
learning zero-shot sim2real pixel-to-action manipulation policy with
photo-realistic rendering, (2) automated high-quality DAgger data collection
for adapting policies to deployment environments, (3) reproducible benchmarking
of real-robot manipulation policies in simulation, (4) simulation data
collection by virtual teleoperation, and (5) zero-shot sim2real visual
reinforcement learning. Website: https://3dgsworld.github.io/.

</details>


### [218] [VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation](https://arxiv.org/abs/2510.20818)
*Mateo Guaman Castro,Sidharth Rajagopal,Daniel Gorbatov,Matt Schmittle,Rohan Baijal,Octi Zhang,Rosario Scalise,Sidharth Talia,Emma Romig,Celso de Melo,Byron Boots,Abhishek Gupta*

Main category: cs.RO

TL;DR: 提出分层VLA模型VAMOS用于机器人导航，解耦语义规划与具身接地，实验显示其在室内外导航成功率更高，支持跨具身导航，提升单机器人可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人导航中学习策略在不同环境泛化，同时符合特定具身物理约束和能力的挑战。

Method: 设计分层VLA模型VAMOS，通过精心设计接口解耦语义规划与具身接地，让高级规划器在图像空间提出候选路径，由可供性模型评估和重新排序。

Result: VAMOS在室内外导航成功率高于现有方法，支持跨具身导航，可通过自然语言控制，专业模型是具身接地关键，提升单机器人可靠性。

Conclusion: VAMOS模型有效，其分层设计和专业模型能提升机器人导航性能和可靠性。

Abstract: A fundamental challenge in robot navigation lies in learning policies that
generalize across diverse environments while conforming to the unique physical
constraints and capabilities of a specific embodiment (e.g., quadrupeds can
walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that
decouples semantic planning from embodiment grounding: a generalist planner
learns from diverse, open-world data, while a specialist affordance model
learns the robot's physical constraints and capabilities in safe, low-cost
simulation. We enabled this separation by carefully designing an interface that
lets a high-level planner propose candidate paths directly in image space that
the affordance model then evaluates and re-ranks. Our real-world experiments
show that VAMOS achieves higher success rates in both indoor and complex
outdoor navigation than state-of-the-art model-based and end-to-end learning
methods. We also show that our hierarchical design enables cross-embodied
navigation across legged and wheeled robots and is easily steerable using
natural language. Real-world ablations confirm that the specialist model is key
to embodiment grounding, enabling a single high-level planner to be deployed
across physically distinct wheeled and legged robots. Finally, this model
significantly enhances single-robot reliability, achieving 3X higher success
rates by rejecting physically infeasible plans. Website:
https://vamos-vla.github.io/

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [219] [Quantifying Feature Importance for Online Content Moderation](https://arxiv.org/abs/2510.19882)
*Benedetta Tessa,Alejandro Moreo,Stefano Cresci,Tiziano Fagni,Fabrizio Sebastiani*

Main category: cs.CY

TL;DR: 研究753个特征对预测Reddit用户行为变化的信息价值，确定少量通用特征，发现预测性能因任务而异，为预测用户反应系统开发奠基。


<details>
  <summary>Details</summary>
Motivation: 准确估计用户对审核干预的反应对制定有效以用户为中心的审核策略至关重要，需明确哪些用户特征与不同行为反应相关。

Method: 将问题框定为“量化”任务，应用贪心特征选择策略，识别最具预测性的特征并估计其重要性。

Result: 确定少量跨任务有信息价值的特征，发现很多特征是特定任务或效用有限，预测性能因任务而异。

Conclusion: 为预测用户反应系统开发铺平道路，凸显审核后用户行为复杂性，有效审核应兼顾用户特征和干预目标。

Abstract: Accurately estimating how users respond to moderation interventions is
paramount for developing effective and user-centred moderation strategies.
However, this requires a clear understanding of which user characteristics are
associated with different behavioural responses, which is the goal of this
work. We investigate the informativeness of 753 socio-behavioural, linguistic,
relational, and psychological features, in predicting the behavioural changes
of 16.8K users affected by a major moderation intervention on Reddit. To reach
this goal, we frame the problem in terms of "quantification", a task
well-suited to estimating shifts in aggregate user behaviour. We then apply a
greedy feature selection strategy with the double goal of (i) identifying the
features that are most predictive of changes in user activity, toxicity, and
participation diversity, and (ii) estimating their importance. Our results
allow identifying a small set of features that are consistently informative
across all tasks, and determining that many others are either task-specific or
of limited utility altogether. We also find that predictive performance varies
according to the task, with changes in activity and toxicity being easier to
estimate than changes in diversity. Overall, our results pave the way for the
development of accurate systems that predict user reactions to moderation
interventions. Furthermore, our findings highlight the complexity of
post-moderation user behaviour, and indicate that effective moderation should
be tailored not only to user traits but also to the specific objective of the
intervention.

</details>


### [220] [Ask What Your Country Can Do For You: Towards a Public Red Teaming Model](https://arxiv.org/abs/2510.20061)
*Wm. Matthew Kennedy,Cigdem Patlak,Jayraj Dave,Blake Chambers,Aayush Dhanotiya,Darshini Ramiah,Reva Schwartz,Jack Hagen,Akash Kundu,Mouni Pendharkar,Liam Baisley,Theodora Skeadas,Rumman Chowdhury*

Main category: cs.CY

TL;DR: 当前AI评估方法应对复杂系统不足，本文提出合作公共AI红队演练方法，回顾相关活动结果，认为该方法有效且可扩展。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估和监控方法难以对高风险领域的复杂AI系统进行有效监督，需新方法填补“责任差距”。

Method: 提出合作公共AI红队演练方法，并回顾该方法在CAMLIS 2024、NIST的ARIA试点以及新加坡IMDA相关活动中的实施情况。

Result: 未明确提及具体结果，但表明该方法能产生有意义的结果。

Conclusion: 该合作公共AI红队演练方法既能够产生有意义的结果，又可在许多AI发展辖区进行扩展。

Abstract: AI systems have the potential to produce both benefits and harms, but without
rigorous and ongoing adversarial evaluation, AI actors will struggle to assess
the breadth and magnitude of the AI risk surface. Researchers from the field of
systems design have developed several effective sociotechnical AI evaluation
and red teaming techniques targeting bias, hate speech, mis/disinformation, and
other documented harm classes. However, as increasingly sophisticated AI
systems are released into high-stakes sectors (such as education, healthcare,
and intelligence-gathering), our current evaluation and monitoring methods are
proving less and less capable of delivering effective oversight.
  In order to actually deliver responsible AI and to ensure AI's harms are
fully understood and its security vulnerabilities mitigated, pioneering new
approaches to close this "responsibility gap" are now more urgent than ever. In
this paper, we propose one such approach, the cooperative public AI red-teaming
exercise, and discuss early results of its prior pilot implementations. This
approach is intertwined with CAMLIS itself: the first in-person public
demonstrator exercise was held in conjunction with CAMLIS 2024. We review the
operational design and results of this exercise, the prior National Institute
of Standards and Technology (NIST)'s Assessing the Risks and Impacts of AI
(ARIA) pilot exercise, and another similar exercise conducted with the
Singapore Infocomm Media Development Authority (IMDA). Ultimately, we argue
that this approach is both capable of delivering meaningful results and is also
scalable to many AI developing jurisdictions.

</details>


### [221] [Towards AI Agents for Course Instruction in Higher Education: Early Experiences from the Field](https://arxiv.org/abs/2510.20255)
*Yogesh Simmhan,Varad Kulkarni*

Main category: cs.CY

TL;DR: 本文介绍在IISc研究生云计算课程中部署AI教育代理的早期发现，包括设计、评估等，展示其促进反思性学习等作用。


<details>
  <summary>Details</summary>
Motivation: 探索如何在研究生课程中有效利用AI教育代理促进学习和研究课堂互动。

Method: 设计LLM驱动的教学代理，将其融入课程工作流，提出分析框架评估代理与学生的互动。

Result: 学生与代理互动良好，评估指标显示参与度从广泛概念探索过渡到深入探究。

Conclusion: 结构化整合对话式AI代理可促进反思性学习，提供研究课堂参与度的方法，支持高质量高等教育。

Abstract: This article presents early findings from designing, deploying and evaluating
an AI-based educational agent deployed as the primary instructor in a
graduate-level Cloud Computing course at IISc. We detail the design of a Large
Language Model (LLM)-driven Instructor Agent, and introduce a pedagogical
framework that integrates the Instructor Agent into the course workflow for
actively interacting with the students for content delivery, supplemented by
the human instructor to offer the course structure and undertake
question--answer sessions. We also propose an analytical framework that
evaluates the Agent--Student interaction transcripts using interpretable
engagement metrics of topic coverage, topic depth and turn-level elaboration.
We report early experiences on how students interact with the Agent to explore
concepts, clarify doubts and sustain inquiry-driven dialogue during live
classroom sessions. We also report preliminary analysis on our evaluation
metrics applied across two successive instructional modules that reveals
patterns of engagement evolution, transitioning from broad conceptual
exploration to deeper, focused inquiry. These demonstrate how structured
integration of conversational AI agents can foster reflective learning, offer a
reproducible methodology for studying engagement in authentic classroom
settings, and support scalable, high-quality higher education.

</details>


### [222] [What do AI-Generated Images Want?](https://arxiv.org/abs/2510.20350)
*Amanda Wasielewski*

Main category: cs.CY

TL;DR: 文章借当代AI图像生成工具重新审视Mitchell的问题，探讨AI生成图像的诉求，认为其因本质抽象而渴望具体性。


<details>
  <summary>Details</summary>
Motivation: 基于W.J.T. Mitchell的论文，结合当代AI图像生成工具，重新思考图像诉求问题。

Method: 借鉴艺术史中关于抽象本质的论述进行分析。

Result: 指出多模态文本到图像模型中用户操作流程掩盖了表征回归，使文本转图像看似神奇。

Conclusion: AI生成图像因本质抽象而想要具体性和明确性。

Abstract: W.J.T. Mitchell's influential essay 'What do pictures want?' shifts the
theoretical focus away from the interpretative act of understanding pictures
and from the motivations of the humans who create them to the possibility that
the picture itself is an entity with agency and wants. In this article, I
reframe Mitchell's question in light of contemporary AI image generation tools
to ask: what do AI-generated images want? Drawing from art historical discourse
on the nature of abstraction, I argue that AI-generated images want specificity
and concreteness because they are fundamentally abstract. Multimodal
text-to-image models, which are the primary subject of this article, are based
on the premise that text and image are interchangeable or exchangeable tokens
and that there is a commensurability between them, at least as represented
mathematically in data. The user pipeline that sees textual input become visual
output, however, obscures this representational regress and makes it seem like
one form transforms into the other -- as if by magic.

</details>


### [223] [Black Box Absorption: LLMs Undermining Innovative Ideas](https://arxiv.org/abs/2510.20612)
*Wenjun Cao*

Main category: cs.CY

TL;DR: 本文指出大语言模型存在“黑箱吸收”系统性风险，引入概念并分析机制，提出治理和工程议程以降低风险。


<details>
  <summary>Details</summary>
Motivation: 识别并形式化大语言模型范式中固有的系统性风险，避免其破坏创新经济学原则和危及创新生态系统的长期可持续性。

Method: 引入“想法单元”和“想法安全”两个核心概念，分析吸收机制。

Result: 分析了吸收机制，提出了具体的治理和工程议程。

Conclusion: 通过治理和工程议程可降低风险，确保创作者贡献可追溯、可控和公平。

Abstract: Large Language Models are increasingly adopted as critical tools for
accelerating innovation. This paper identifies and formalizes a systemic risk
inherent in this paradigm: \textbf{Black Box Absorption}. We define this as the
process by which the opaque internal architectures of LLM platforms, often
operated by large-scale service providers, can internalize, generalize, and
repurpose novel concepts contributed by users during interaction. This
mechanism threatens to undermine the foundational principles of innovation
economics by creating severe informational and structural asymmetries between
individual creators and platform operators, thereby jeopardizing the long-term
sustainability of the innovation ecosystem. To analyze this challenge, we
introduce two core concepts: the idea unit, representing the transportable
functional logic of an innovation, and idea safety, a multidimensional standard
for its protection. This paper analyzes the mechanisms of absorption and
proposes a concrete governance and engineering agenda to mitigate these risks,
ensuring that creator contributions remain traceable, controllable, and
equitable.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [224] [On the Structure of Stationary Solutions to McKean-Vlasov Equations with Applications to Noisy Transformers](https://arxiv.org/abs/2510.20094)
*Krishnakumar Balasubramanian,Sayan Banerjee,Philippe Rigollet*

Main category: math.PR

TL;DR: 研究圆上McKean - Vlasov方程的稳态解，利用与傅里叶系数的无限维二次方程组的等价性进行分析，有局部分岔和全局性质结果，并应用于Noisy Mean - Field Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 研究圆上McKean - Vlasov方程稳态解的特性，包括分岔和相变等。

Method: 发现稳态McKean - Vlasov方程解与傅里叶系数的无限维二次方程组的精确等价性，在序列空间描述稳态。

Result: 得到分岔的解析表达式，建立自由能景观的正则性和凹性，应用于模型发现改变参数β的影响和相变。

Conclusion: 该框架能有效分析圆上McKean - Vlasov方程稳态解，揭示分岔和相变特征，对相关模型有应用价值。

Abstract: We study stationary solutions of McKean-Vlasov equations on the circle. Our
main contributions stem from observing an exact equivalence between solutions
of the stationary McKean-Vlasov equation and an infinite-dimensional quadratic
system of equations over Fourier coefficients, which allows explicit
characterization of the stationary states in a sequence space rather than a
function space. This framework provides a transparent description of local
bifurcations, characterizing their periodicity, and resonance structures, while
accommodating singular potentials. We derive analytic expressions that
characterize the emergence, form and shape (supercritical, critical,
subcritical or transcritical) of bifurcations involving possibly multiple
Fourier modes and connect them with discontinuous phase transitions. We also
characterize, under suitable assumptions, the detailed structure of the
stationary bifurcating solutions that are accurate upto an arbitrary number of
Fourier modes. At the global level, we establish regularity and concavity
properties of the free energy landscape, proving existence, compactness, and
coexistence of globally minimizing stationary measures, further identifying
discontinuous phase transitions with points of non-differentiability of the
minimum free energy map. As an application, we specialize the theory to the
Noisy Mean-Field Transformer model, where we show how changing the inverse
temperature parameter $\beta$ affects the geometry of the infinitely many
bifurcations from the uniform measure. We also explain how increasing $\beta$
can lead to a rich class of approximate multi-mode stationary solutions which
can be seen as `metastable states'. Further, a sharp transition from continuous
to discontinuous (first-order) phase behavior is observed as $\beta$ increases.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [225] [Complexity of Unambiguous Problems in $Σ^P_2$](https://arxiv.org/abs/2510.19084)
*Matan Gilboa,Paul W. Goldberg,Elias Koutsoupias,Noam Nisan*

Main category: cs.CC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The complexity class $\bf{\Sigma^P_2}$ comprises problems based on
polynomial-time checkable binary relations $\phi(x,y)$ in which we ask whether
there exists $x$ such that for all $y$, $\phi(x,y)$ holds. We let
$\bf{U\Sigma^P_2}$ denote the subclass of unambiguous problems in
$\bf{\Sigma^P_2}$, namely those whose yes-instances correspond with a unique
choice of $x$. $\bf{U\Sigma^P_2}$ is unlikely to have complete problems, but we
identify various syntactic subclasses associated with general properties of
$\phi$ that guarantee uniqueness. We use these to classify the complexity of
problems arising in social choice and game theory, such as existence of (1) a
dominating strategy in a game, (2) a Condorcet winner, (3) a strongly popular
partition in hedonic games, and (4) a winner (source) in a tournament. We
classify these problems, showing the first is $\bf{\Delta^P_2}$-complete, the
second and third are complete for a class we term $\bf{PCW}$ (Polynomial
Condorcet Winner), and the fourth for a class we term $\bf{PTW}$ (Polynomial
Tournament Winner). We define another unambiguous class, $\bf{PMA}$ (Polynomial
Majority Argument), seemingly incomparable to $\bf{PTW}$ and $\bf{PCW}$. We
show that with randomization, $\bf{PCW}$ and $\bf{PTW}$ coincide with
$\bf{\Delta^P_2}$, and $\bf{PMA}$ is contained in $\bf{\Delta^P_2}$.
Specifically, we prove: $\bf{\Delta^P_2} \subseteq \bf{PCW} \subseteq \bf{PTW}
\subseteq \bf{S^P_2}$, and $\bf{coNP} \subseteq \bf{PMA} \subseteq \bf{S^P_2}$
(and it is known that $\bf{S^P_2}\subseteq \bf{ZPP^{NP}} \subseteq
\bf{\Sigma^P_2} \cap \bf{\Pi^P_2}$). We demonstrate that unambiguity can
substantially reduce computational complexity by considering ambiguous variants
of our problems, and showing they are $\bf{\Sigma^P_2}$-complete. Finally, we
study the unambiguous problem of finding a weakly dominant strategy in a game,
which seems not to lie in $\bf{\Sigma^P_2}$.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [226] [High-order Interactions Modeling for Interpretable Multi-Agent Q-Learning](https://arxiv.org/abs/2510.20218)
*Qinyu Xu,Yuanyang Zhu,Xuefei Wu,Chunlin Chen*

Main category: cs.MA

TL;DR: 提出QCoFr价值分解框架用于多智能体强化学习，避免组合爆炸，引入变分信息瓶颈提升合作与可解释性，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 以往对多智能体高阶交互建模受组合爆炸和黑盒网络结构不透明的阻碍，需要更好的方法。

Method: 提出QCoFr框架，以线性复杂度捕捉任意阶智能体交互；引入变分信息瓶颈提取潜在信息估计信用。

Result: QCoFr在实验中持续取得更好性能，且可解释性与理论分析一致。

Conclusion: QCoFr是一个有效且具有可解释性的多智能体强化学习价值分解框架。

Abstract: The ability to model interactions among agents is crucial for effective
coordination and understanding their cooperation mechanisms in multi-agent
reinforcement learning (MARL). However, previous efforts to model high-order
interactions have been primarily hindered by the combinatorial explosion or the
opaque nature of their black-box network structures. In this paper, we propose
a novel value decomposition framework, called Continued Fraction Q-Learning
(QCoFr), which can flexibly capture arbitrary-order agent interactions with
only linear complexity $\mathcal{O}\left({n}\right)$ in the number of agents,
thus avoiding the combinatorial explosion when modeling rich cooperation.
Furthermore, we introduce the variational information bottleneck to extract
latent information for estimating credits. This latent information helps agents
filter out noisy interactions, thereby significantly enhancing both cooperation
and interpretability. Extensive experiments demonstrate that QCoFr not only
consistently achieves better performance but also provides interpretability
that aligns with our theoretical analysis.

</details>


### [227] [Structures generated in a multiagent system performing information fusion in peer-to-peer resource-constrained networks](https://arxiv.org/abs/2510.20469)
*Horacio Paggi,Juan A. Lara,Javier Soriano*

Main category: cs.MA

TL;DR: 本文探讨信息融合范式从分层到整体融合的转变，基于多智能体系统模型研究资源受限下整体结构的生成并举例，指出其优势。


<details>
  <summary>Details</summary>
Motivation: 信息融合在非军事领域发展，传统分层范式向更灵活的整体融合范式转变，需研究资源受限下整体结构的生成。

Method: 基于多智能体系统模型进行整体结构生成的通用研究，并给出可能的操作示例。

Result: 对整体结构的生成进行了研究并给出示例，展示其优势。

Conclusion: 整体结构具有适应性、自主性和协作性等优势，在资源短缺或组件故障时有用。

Abstract: There has recently been a major advance with respect to how information
fusion is performed. Information fusion has gone from being conceived as a
purely hierarchical procedure, as is the case of traditional military
applications, to now being regarded collaboratively, as holonic fusion, which
is better suited for civil applications and edge organizations. The above
paradigm shift is being boosted as information fusion gains ground in different
non-military areas, and human-computer and machine-machine communications,
where holarchies, which are more flexible structures than ordinary, static
hierarchies, become more widespread. This paper focuses on showing how holonic
structures tend to be generated when there are constraints on resources
(energy, available messages, time, etc.) for interactions based on a set of
fully intercommunicating elements (peers) whose components fuse information as
a means of optimizing the impact of vagueness and uncertainty present message
exchanges. Holon formation is studied generically based on a multiagent system
model, and an example of its possible operation is shown. Holonic structures
have a series of advantages, such as adaptability, to sudden changes in the
environment or its composition, are somewhat autonomous and are capable of
cooperating in order to achieve a common goal. This can be useful when the
shortage of resources prevents communications or when the system components
start to fail.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [228] [On Geometric Bipartite Graphs with Asymptotically Smallest Zarankiewicz Numbers](https://arxiv.org/abs/2510.20737)
*Parinya Chalermsook,Ly Orgo,Minoo Zarsav*

Main category: math.CO

TL;DR: 本文研究低Ferrers维数图的Zarankiewicz问题，揭示了Ferrers维数3和4的二分图的分离，给出弦二分图和网格相交图的紧上界，推动了Ferrers维数与极值组合学的研究。


<details>
  <summary>Details</summary>
Motivation: 研究低维几何表示（低Ferrers维数）图中的Zarankiewicz问题。

Method: 对不同Ferrers维数的二分图进行分析，结合已有研究成果推导上界。

Result: 揭示Ferrers维数3和4的二分图的分离，得出弦二分图的紧上界为2n(k - 1)，网格相交图的紧上界为54n(k - 1)。

Conclusion: 研究结果推动了Ferrers维数与极值组合学之间相互作用的研究，并提供了新见解。

Abstract: This paper considers the \textit{Zarankiewicz problem} in graphs with
low-dimensional geometric representation (i.e., low Ferrers dimension). Our
first result reveals a separation between bipartite graphs of Ferrers dimension
three and four: while $Z(n;k) \leq 9n(k-1)$ for graphs of Ferrers dimension
three, $Z(n;k) \in \Omega\left(n k \cdot \frac{\log n}{\log \log n}\right)$ for
Ferrers dimension four graphs (Chan & Har-Peled, 2023) (Chazelle, 1990). To
complement this, we derive a tight upper bound of $2n(k-1)$ for chordal
bigraphs and $54n(k-1)$ for grid intersection graphs (GIG), a prominent graph
class residing in four Ferrers dimensions and capturing planar bipartite graphs
as well as bipartite intersection graphs of rectangles. Previously, the
best-known bound for GIG was $Z(n;k) \in O(2^{O(k)} n)$, implied by the results
of Fox & Pach (2006) and Mustafa & Pach (2016). Our results advance and offer
new insights into the interplay between Ferrers dimensions and extremal
combinatorics.

</details>


### [229] [A Freeable Matrix Characterization of Bipartite Graphs of Ferrers Dimension Three](https://arxiv.org/abs/2510.20744)
*Parinya Chalermsook,Ly Orgo,Minoo Zarsav*

Main category: math.CO

TL;DR: 证明图的Ferrer维数为3的充要条件是其双邻接矩阵表示不包含特定矩阵。


<details>
  <summary>Details</summary>
Motivation: 研究二分图的Ferrer维数这一标准维度概念。

Method: 通过理论证明的方法。

Result: 得出图的Ferrer维数为3等价于其双邻接矩阵表示不包含特定矩阵。

Conclusion: 明确了图的Ferrer维数为3的充要条件。

Abstract: Ferrer dimension, along with the order dimension, is a standard dimensional
concept for bipartite graphs. In this paper, we prove that a graph is of Ferrer
dimension three (equivalent to the intersection bigraph of orthants and points
in ${\mathbb R}^3$) if and only if it admits a biadjacency matrix
representation that does not contain $\Gamma= \begin{bmatrix} * & 1 & * \\ 1 &
0 & 1 \\ 0 & 1 & * \end{bmatrix}$ and $\Delta = \begin{bmatrix} 1 & * & * \\ 0
& 1 & * \\ 1 & 0 & 1 \end{bmatrix}$, where $*$ denotes zero or one entry.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [230] [SLYKLatent: A Learning Framework for Gaze Estimation Using Deep Facial Feature Learning](https://arxiv.org/abs/2402.01555)
*Samuel Adebayo,Joost C. Dessing,Seán McLoone*

Main category: cs.CV

TL;DR: 提出SLYKLatent方法解决数据集外观不稳定问题以增强注视估计，评估表现优于现有方法，消融实验证实组件有效性。


<details>
  <summary>Details</summary>
Motivation: 解决数据集因随机不确定性、协变偏移和测试域泛化导致的外观不稳定问题，以增强注视估计。

Method: 使用自监督学习对表情数据集进行初始训练，再用基于补丁的三分支网络和逆解释方差加权训练损失函数进行细化。

Result: 在基准数据集上表现优异，Gaze360提升10.9%，MPIIFaceGaze超3.8%，ETH - XGaze子集领先11.6%，在RAF - DB和Affectnet适应性测试准确率分别为86.4%和60.9%。

Conclusion: SLYKLatent有效，其新组件经消融实验证实有效。

Abstract: In this research, we present SLYKLatent, a novel approach for enhancing gaze
estimation by addressing appearance instability challenges in datasets due to
aleatoric uncertainties, covariant shifts, and test domain generalization.
SLYKLatent utilizes Self-Supervised Learning for initial training with facial
expression datasets, followed by refinement with a patch-based tri-branch
network and an inverse explained variance-weighted training loss function. Our
evaluation on benchmark datasets achieves a 10.9% improvement on Gaze360,
supersedes top MPIIFaceGaze results with 3.8%, and leads on a subset of
ETH-XGaze by 11.6%, surpassing existing methods by significant margins.
Adaptability tests on RAF-DB and Affectnet show 86.4% and 60.9% accuracies,
respectively. Ablation studies confirm the effectiveness of SLYKLatent's novel
components.

</details>


### [231] [StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback](https://arxiv.org/abs/2510.20093)
*Jiho Park,Sieun Choi,Jaeyoon Seo,Jihie Kim*

Main category: cs.CV

TL;DR: 提出StableSketcher框架让扩散模型生成手绘草图，还引入SketchDUO数据集，代码和数据集待接受后公开。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在合成像素级人类手绘草图存在挑战，需提升生成草图质量和与提示的一致性。

Method: 微调变分自编码器优化潜在解码，集成基于视觉问答的强化学习新奖励函数，引入SketchDUO数据集。

Result: StableSketcher生成的草图风格保真度提高，与提示的对齐效果优于Stable Diffusion基线。

Conclusion: StableSketcher框架和SketchDUO数据集有效解决了扩散模型生成手绘草图的问题。

Abstract: Although recent advancements in diffusion models have significantly enriched
the quality of generated images, challenges remain in synthesizing pixel-based
human-drawn sketches, a representative example of abstract expression. To
combat these challenges, we propose StableSketcher, a novel framework that
empowers diffusion models to generate hand-drawn sketches with high prompt
fidelity. Within this framework, we fine-tune the variational autoencoder to
optimize latent decoding, enabling it to better capture the characteristics of
sketches. In parallel, we integrate a new reward function for reinforcement
learning based on visual question answering, which improves text-image
alignment and semantic consistency. Extensive experiments demonstrate that
StableSketcher generates sketches with improved stylistic fidelity, achieving
better alignment with prompts compared to the Stable Diffusion baseline.
Additionally, we introduce SketchDUO, to the best of our knowledge, the first
dataset comprising instance-level sketches paired with captions and
question-answer pairs, thereby addressing the limitations of existing datasets
that rely on image-label pairs. Our code and dataset will be made publicly
available upon acceptance.

</details>


### [232] [IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks](https://arxiv.org/abs/2510.20165)
*Insu Jeon,Wonkwang Lee,Myeongjang Pyeon,Gunhee Kim*

Main category: cs.CV

TL;DR: 提出基于GAN的无监督模型IB - GAN用于解纠缠表示学习，实验显示其在解纠缠得分、视觉质量和多样性上有竞争力。


<details>
  <summary>Details</summary>
Motivation: 尝试将信息瓶颈（IB）框架用于GAN的优化，以实现解纠缠表示学习。

Method: 提出IB - GAN模型，利用生成器中间层约束输入与输出的互信息，中间随机层作为可学习的潜在分布与生成器端到端训练。

Result: 在dSprites和Color - dSprites数据集上取得与最先进的η - VAEs相当的解纠缠分数，优于InfoGAN；在CelebA和3D Chairs数据集上生成样本的视觉质量和多样性在FID分数上优于η - VAEs和Info - GAN。

Conclusion: IB - GAN能以解纠缠和可解释的方式利用潜在空间，在解纠缠和样本生成方面表现良好。

Abstract: We propose a new GAN-based unsupervised model for disentangled representation
learning. The new model is discovered in an attempt to utilize the Information
Bottleneck (IB) framework to the optimization of GAN, thereby named IB-GAN. The
architecture of IB-GAN is partially similar to that of InfoGAN but has a
critical difference; an intermediate layer of the generator is leveraged to
constrain the mutual information between the input and the generated output.
The intermediate stochastic layer can serve as a learnable latent distribution
that is trained with the generator jointly in an end-to-end fashion. As a
result, the generator of IB-GAN can harness the latent space in a disentangled
and interpretable manner. With the experiments on dSprites and Color-dSprites
dataset, we demonstrate that IB-GAN achieves competitive disentanglement scores
to those of state-of-the-art \b{eta}-VAEs and outperforms InfoGAN. Moreover,
the visual quality and the diversity of samples generated by IB-GAN are often
better than those by \b{eta}-VAEs and Info-GAN in terms of FID score on CelebA
and 3D Chairs dataset.

</details>


### [233] [PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching](https://arxiv.org/abs/2510.20178)
*Yun Wang,Junjie Hu,Qiaole Dong,Yongjian Zhang,Yanwei Fu,Tin Lun Lam,Dapeng Wu*

Main category: cs.CV

TL;DR: 提出PPMStereo模型解决立体视频深度估计中时间一致性问题，实验证明其在精度和时间一致性上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 立体视频的时间一致深度估计对增强现实等应用至关重要，但现有方法在高效建模长期时间一致性上存在困难。

Method: 引入内存缓冲区，提出Pick-and-Play Memory (PPM) 构建模块，包含'pick'和'play'两个阶段。

Result: PPMStereo在准确性和时间一致性上取得了最先进的性能，如在Sintel数据集上有显著提升且计算成本更低。

Conclusion: PPMStereo有效解决了立体视频深度估计中高效建模长期时间一致性的问题。

Abstract: Temporally consistent depth estimation from stereo video is critical for
real-world applications such as augmented reality, where inconsistent depth
estimation disrupts the immersion of users. Despite its importance, this task
remains challenging due to the difficulty in modeling long-term temporal
consistency in a computationally efficient manner. Previous methods attempt to
address this by aggregating spatio-temporal information but face a fundamental
trade-off: limited temporal modeling provides only modest gains, whereas
capturing long-range dependencies significantly increases computational cost.
To address this limitation, we introduce a memory buffer for modeling
long-range spatio-temporal consistency while achieving efficient dynamic stereo
matching. Inspired by the two-stage decision-making process in humans, we
propose a \textbf{P}ick-and-\textbf{P}lay \textbf{M}emory (PPM) construction
module for dynamic \textbf{Stereo} matching, dubbed as \textbf{PPMStereo}. PPM
consists of a `pick' process that identifies the most relevant frames and a
`play' process that weights the selected frames adaptively for spatio-temporal
aggregation. This two-stage collaborative process maintains a compact yet
highly informative memory buffer while achieving temporally consistent
information aggregation. Extensive experiments validate the effectiveness of
PPMStereo, demonstrating state-of-the-art performance in both accuracy and
temporal consistency. % Notably, PPMStereo achieves 0.62/1.11 TEPE on the
Sintel clean/final (17.3\% \& 9.02\% improvements over BiDAStereo) with fewer
computational costs. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/PPMStereo}.

</details>


### [234] [Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context](https://arxiv.org/abs/2510.20229)
*Ge Zheng,Jiaye Qian,Jiajin Tang,Sibei Yang*

Main category: cs.CV

TL;DR: 本文探究大视觉语言模型长回复中幻觉问题成因，提出“诱导 - 检测 - 抑制”框架，实验证明有效，为深入研究提供新见解。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在长回复中易出现幻觉问题，探究其成因是否仅由长度导致。

Method: 通过初步实验分析成因，提出“诱导 - 检测 - 抑制”框架，诱导幻觉、检测高风险情况并抑制潜在物体级幻觉。

Result: 该方法在所有基准测试中取得显著且一致的改进。

Conclusion: 验证了框架有效性及上下文影响幻觉的假设，为深入研究长回复中的幻觉问题迈出第一步。

Abstract: Large Vision-Language Models (LVLMs) have made significant progress in recent
years but are also prone to hallucination issues. They exhibit more
hallucinations in longer, free-form responses, often attributed to accumulated
uncertainties. In this paper, we ask: Does increased hallucination result
solely from length-induced errors, or is there a deeper underlying mechanism?
After a series of preliminary experiments and findings, we suggest that the
risk of hallucinations is not caused by length itself but by the increased
reliance on context for coherence and completeness in longer responses.
Building on these insights, we propose a novel "induce-detect-suppress"
framework that actively induces hallucinations through deliberately designed
contexts, leverages induced instances for early detection of high-risk cases,
and ultimately suppresses potential object-level hallucinations during actual
decoding. Our approach achieves consistent, significant improvements across all
benchmarks, demonstrating its efficacy. The strong detection and improved
hallucination mitigation not only validate our framework but, more importantly,
re-validate our hypothesis on context. Rather than solely pursuing performance
gains, this study aims to provide new insights and serves as a first step
toward a deeper exploration of hallucinations in LVLMs' longer responses.

</details>


### [235] [UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning](https://arxiv.org/abs/2510.20286)
*Liangyu Chen,Hanzhang Zhou,Chenglin Cai,Jianan Zhang,Panrong Tong,Quyu Kong,Xu Zhang,Chen Liu,Yuqi Liu,Wenxuan Wang,Yue Wang,Qin Jin,Steven Hoi*

Main category: cs.CV

TL;DR: 本文提出Instruction - as - Reasoning范式及两阶段训练框架，所训练模型在多个基准测试取得SOTA结果，还展现出推理能力和代理潜力。


<details>
  <summary>Details</summary>
Motivation: 先前工作将指令视为用户意图的静态代理，忽略指令多样性和质量对GUI grounding性能的影响，现有数据集指令存在较高错误率。

Method: 引入Instruction - as - Reasoning范式，提出两阶段训练框架，先在合成的多样化指令上进行有监督微调，再通过强化学习优化路径选择和组合。

Result: 模型UI - Ins - 7B和UI - Ins - 32B在五个基准测试取得SOTA结果，UI - Ins - 32B在多个测试中获得高准确率，模型在AndroidWorld上有较高成功率。

Conclusion: 模型表现优秀，深入分析揭示增强性能和缓解策略崩溃等见解，代码和模型将公开。

Abstract: GUI grounding, which maps natural-language instructions to actionable UI
elements, is a core capability of GUI agents. Prior works largely treats
instructions as a static proxy for user intent, overlooking the impact of
instruction diversity and quality on grounding performance. Through a careful
investigation of existing grounding datasets, we find a 23.3% flaw rate in
their instructions and show that inference-time exploitation of instruction
diversity yields up to a substantial 76% relative performance improvement. In
this paper, we introduce the Instruction-as-Reasoning paradigm, treating
instructions as dynamic analytical pathways that offer distinct perspectives
and enabling the model to select the most effective pathway during reasoning.
To achieve this, we propose a two-stage training framework: supervised
fine-tuning (SFT) on synthesized, diverse instructions to instill
multi-perspective reasoning, followed by reinforcement learning (RL) to
optimize pathway selection and composition. Our resulting models, UI-Ins-7B and
UI-Ins-32B, achieve state-of-the-art results on five challenging grounding
benchmarks and exhibit emergent reasoning, selectively composing and
synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B
attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on
ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model
demonstrates strong agentic potential, achieving a 74.1% success rate on
AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals
additional insights such as how reasoning can be formulated to enhance rather
than hinder grounding performance, and how our method mitigates policy collapse
in the SFT+RL framework. All code and model checkpoints will be publicly
released in https://github.com/alibaba/UI-Ins.

</details>


### [236] [Breakdance Video classification in the age of Generative AI](https://arxiv.org/abs/2510.20287)
*Sauptik Dhar,Naveen Ramakrishnan,Michelle Munson*

Main category: cs.CV

TL;DR: 分析现代视频基础模型在霹雳舞这一小众但流行的舞蹈体育项目中的适用性，结果显示视频编码器模型在预测任务上表现更优，并给出编码器选择建议和对微调解码器模型的分析。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型在体育场景的应用多集中于少数流行体育项目，本文旨在研究其在霹雳舞这一小众但受欢迎的舞蹈体育中的适用性。

Method: 分析现代视频基础模型（编码器和解码器）在霹雳舞中的应用情况。

Result: 视频编码器模型在预测任务上优于最先进的视频语言模型。

Conclusion: 给出选择编码器模型的见解，并对微调后的解码器模型在霹雳舞视频分类中的工作原理进行了深入分析。

Abstract: Large Vision Language models have seen huge application in several sports
use-cases recently. Most of these works have been targeted towards a limited
subset of popular sports like soccer, cricket, basketball etc; focusing on
generative tasks like visual question answering, highlight generation. This
work analyzes the applicability of the modern video foundation models (both
encoder and decoder) for a very niche but hugely popular dance sports -
breakdance. Our results show that Video Encoder models continue to outperform
state-of-the-art Video Language Models for prediction tasks. We provide
insights on how to choose the encoder model and provide a thorough analysis
into the workings of a finetuned decoder model for breakdance video
classification.

</details>


### [237] [A Parameter-Efficient Mixture-of-Experts Framework for Cross-Modal Geo-Localization](https://arxiv.org/abs/2510.20291)
*LinFeng Li,Jian Zhao,Zepeng Yang,Yuhang Song,Bojun Lin,Tianle Zhang,Yuchen Yuan,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出RoboSense 2025 Track 4跨模态无人机导航的获胜方案，通过特定方法解决平台异质性和领域差距问题，系统排名领先。


<details>
  <summary>Details</summary>
Motivation: 解决跨模态无人机导航任务中平台间的严重异质性和训练描述与测试查询间的领域差距问题。

Method: 采用领域对齐预处理管道和混合专家（MoE）框架，包括平台分区、卫星增强、去除方向词、基于大语言模型的文本精炼管道；使用BGE - M3和EVA - CLIP，通过渐进两阶段、难负样本挖掘策略训练三个平台专家并在推理时融合分数。

Result: 系统在官方排行榜上名列前茅。

Conclusion: 该系统在异构视角下实现了强大的跨模态地理定位。

Abstract: We present a winning solution to RoboSense 2025 Track 4: Cross-Modal Drone
Navigation. The task retrieves the most relevant geo-referenced image from a
large multi-platform corpus (satellite/drone/ground) given a natural-language
query. Two obstacles are severe inter-platform heterogeneity and a domain gap
between generic training descriptions and platform-specific test queries. We
mitigate these with a domain-aligned preprocessing pipeline and a
Mixture-of-Experts (MoE) framework: (i) platform-wise partitioning, satellite
augmentation, and removal of orientation words; (ii) an LLM-based caption
refinement pipeline to align textual semantics with the distinct visual
characteristics of each platform. Using BGE-M3 (text) and EVA-CLIP (image), we
train three platform experts using a progressive two-stage, hard-negative
mining strategy to enhance discriminative power, and fuse their scores at
inference. The system tops the official leaderboard, demonstrating robust
cross-modal geo-localization under heterogeneous viewpoints.

</details>


### [238] [Dynamic Weight Adjustment for Knowledge Distillation: Leveraging Vision Transformer for High-Accuracy Lung Cancer Detection and Real-Time Deployment](https://arxiv.org/abs/2510.20438)
*Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel*

Main category: cs.CV

TL;DR: 提出FuzzyDistillViT - MobileNet模型用于肺癌分类，采用动态模糊逻辑知识蒸馏等方法，在多数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决肺癌诊断中不确定性和复杂性问题，提升模型处理不同区域不确定性的能力。

Method: 用动态模糊逻辑调整蒸馏权重；以ViT - B32为教师模型向MobileNet学生模型传输知识；用动态等待调整机制优化训练；引入像素级图像融合改进技术；用遗传算法选择合适预训练学生模型。

Result: 在LC25000组织病理图像上准确率99.16%，在IQOTH/NCCD CT扫描图像上准确率99.54%。

Conclusion: 该模型在不同成像领域具有鲁棒性。

Abstract: This paper presents the FuzzyDistillViT-MobileNet model, a novel approach for
lung cancer (LC) classification, leveraging dynamic fuzzy logic-driven
knowledge distillation (KD) to address uncertainty and complexity in disease
diagnosis. Unlike traditional models that rely on static KD with fixed weights,
our method dynamically adjusts the distillation weight using fuzzy logic,
enabling the student model to focus on high-confidence regions while reducing
attention to ambiguous areas. This dynamic adjustment improves the model
ability to handle varying uncertainty levels across different regions of LC
images. We employ the Vision Transformer (ViT-B32) as the instructor model,
which effectively transfers knowledge to the student model, MobileNet,
enhancing the student generalization capabilities. The training process is
further optimized using a dynamic wait adjustment mechanism that adapts the
training procedure for improved convergence and performance. To enhance image
quality, we introduce pixel-level image fusion improvement techniques such as
Gamma correction and Histogram Equalization. The processed images (Pix1 and
Pix2) are fused using a wavelet-based fusion method to improve image resolution
and feature preservation. This fusion method uses the wavedec2 function to
standardize images to a 224x224 resolution, decompose them into multi-scale
frequency components, and recursively average coefficients at each level for
better feature representation. To address computational efficiency, Genetic
Algorithm (GA) is used to select the most suitable pre-trained student model
from a pool of 12 candidates, balancing model performance with computational
cost. The model is evaluated on two datasets, including LC25000
histopathological images (99.16% accuracy) and IQOTH/NCCD CT-scan images
(99.54% accuracy), demonstrating robustness across different imaging domains.

</details>


### [239] [Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning](https://arxiv.org/abs/2510.20519)
*Xiaohan Lan,Fanfan Liu,Haibo Qiu,Siqi Yang,Delian Ruan,Peng Shi,Lin Ma*

Main category: cs.CV

TL;DR: 本文提出Metis - HOME框架解决多模态推理模型在推理与泛化能力间的权衡问题，经评估该框架提升了模型能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大型推理模型存在计算昂贵、泛化能力受损的问题，需解决推理与泛化能力的权衡。

Method: 提出Metis - HOME混合优化专家混合框架，将原密集模型构建为思考和非思考两个专家分支，用轻量级可训练路由器分配查询，以Qwen2.5 - VL - 7B实例化。

Result: 该方法不仅显著提升了复杂推理能力，还改善了模型的一般能力，扭转了其他推理专用模型的退化趋势。

Conclusion: 建立了构建强大且通用多模态大语言模型的新范式，有效解决了推理与泛化的困境。

Abstract: Inspired by recent advancements in LLM reasoning, the field of multimodal
reasoning has seen remarkable progress, achieving significant performance gains
on intricate tasks such as mathematical problem-solving. Despite this progress,
current multimodal large reasoning models exhibit two key limitations. They
tend to employ computationally expensive reasoning even for simple queries,
leading to inefficiency. Furthermore, this focus on specialized reasoning often
impairs their broader, more general understanding capabilities. In this paper,
we propose Metis-HOME: a Hybrid Optimized Mixture-of-Experts framework designed
to address this trade-off. Metis-HOME enables a ''Hybrid Thinking'' paradigm by
structuring the original dense model into two distinct expert branches: a
thinking branch tailored for complex, multi-step reasoning, and a non-thinking
branch optimized for rapid, direct inference on tasks like general VQA and OCR.
A lightweight, trainable router dynamically allocates queries to the most
suitable expert. We instantiate Metis-HOME by adapting the Qwen2.5-VL-7B into
an MoE architecture. Comprehensive evaluations reveal that our approach not
only substantially enhances complex reasoning abilities but also improves the
model's general capabilities, reversing the degradation trend observed in other
reasoning-specialized models. Our work establishes a new paradigm for building
powerful and versatile MLLMs, effectively resolving the prevalent
reasoning-vs-generalization dilemma.

</details>


### [240] [Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis](https://arxiv.org/abs/2510.20531)
*Lixiong Qin,Yang Zhang,Mei Wang,Jiani Hu,Weihong Deng,Weiran Xu*

Main category: cs.CV

TL;DR: 本文提出Fake - in - Facext (FiFa)框架用于可解释深度伪造分析，解决当前方法缺乏细粒度感知问题，代码和数据将开源。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型用于可解释深度伪造分析时存在缺乏细粒度感知的问题，如数据标注不可靠、模型无法输出文本伪造解释与视觉证据联系等。

Method: 定义面部图像概念树（FICT）获得更可靠数据标注管道FiFa - Annotator；引入Artifact - Grounding Explanation（AGE）任务；提出统一多任务学习架构FiFa - MLLM。

Result: FiFa - MLLM在AGE任务上优于强基线，在现有XDFA数据集上达到SOTA性能。

Conclusion: FiFa框架能有效解决可解释深度伪造分析中的细粒度感知问题，提升分析效果。

Abstract: The advancement of Multimodal Large Language Models (MLLMs) has bridged the
gap between vision and language tasks, enabling the implementation of
Explainable DeepFake Analysis (XDFA). However, current methods suffer from a
lack of fine-grained awareness: the description of artifacts in data annotation
is unreliable and coarse-grained, and the models fail to support the output of
connections between textual forgery explanations and the visual evidence of
artifacts, as well as the input of queries for arbitrary facial regions. As a
result, their responses are not sufficiently grounded in Face Visual Context
(Facext). To address this limitation, we propose the Fake-in-Facext (FiFa)
framework, with contributions focusing on data annotation and model
construction. We first define a Facial Image Concept Tree (FICT) to divide
facial images into fine-grained regional concepts, thereby obtaining a more
reliable data annotation pipeline, FiFa-Annotator, for forgery explanation.
Based on this dedicated data annotation, we introduce a novel
Artifact-Grounding Explanation (AGE) task, which generates textual forgery
explanations interleaved with segmentation masks of manipulated artifacts. We
propose a unified multi-task learning architecture, FiFa-MLLM, to
simultaneously support abundant multimodal inputs and outputs for fine-grained
Explainable DeepFake Analysis. With multiple auxiliary supervision tasks,
FiFa-MLLM can outperform strong baselines on the AGE task and achieve SOTA
performance on existing XDFA datasets. The code and data will be made
open-source at https://github.com/lxq1000/Fake-in-Facext.

</details>


### [241] [Improving Predictive Confidence in Medical Imaging via Online Label Smoothing](https://arxiv.org/abs/2510.20011)
*Kushan Choudhury,Shubhrodeep Roy,Ankur Chanda,Shubhajit Biswas,Somenath Kuiry*

Main category: cs.CV

TL;DR: 研究探索在线标签平滑（OLS）用于医学图像分类，在RadImageNet数据集上评估，结果显示OLS提升分类准确率和特征嵌入质量，是医学影像领域开发可信AI系统的有效方案。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像分类中常产生过度自信预测，传统标签平滑未考虑类别关系，需更优方法。

Method: 采用在线标签平滑（OLS），在RadImageNet数据集上用ResNet - 50、MobileNetV2和VGG - 19三种架构进行评估。

Result: OLS相比标准训练方法提升了Top - 1和Top - 5分类准确率，使特征嵌入更紧凑、易分离。

Conclusion: OLS增强预测性能和校准能力，是医学影像领域开发可信AI系统的实用有效方案。

Abstract: Deep learning models, especially convolutional neural networks, have achieved
impressive results in medical image classification. However, these models often
produce overconfident predictions, which can undermine their reliability in
critical healthcare settings. While traditional label smoothing offers a simple
way to reduce such overconfidence, it fails to consider relationships between
classes by treating all non-target classes equally. In this study, we explore
the use of Online Label Smoothing (OLS), a dynamic approach that adjusts soft
labels throughout training based on the model's own prediction patterns. We
evaluate OLS on the large-scale RadImageNet dataset using three widely used
architectures: ResNet-50, MobileNetV2, and VGG-19. Our results show that OLS
consistently improves both Top-1 and Top-5 classification accuracy compared to
standard training methods, including hard labels, conventional label smoothing,
and teacher-free knowledge distillation. In addition to accuracy gains, OLS
leads to more compact and well-separated feature embeddings, indicating
improved representation learning. These findings suggest that OLS not only
strengthens predictive performance but also enhances calibration, making it a
practical and effective solution for developing trustworthy AI systems in the
medical imaging domain.

</details>


### [242] [Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence](https://arxiv.org/abs/2510.20579)
*Jiahao Meng,Xiangtai Li,Haochen Wang,Yue Tan,Tao Zhang,Lingdong Kong,Yunhai Tong,Anran Wang,Zhiyang Teng,Yujing Wang,Zhuochen Wang*

Main category: cs.CV

TL;DR: 提出Open - o3 Video框架将显式时空证据集成到视频推理，构建数据集并设计训练策略，在多基准测试中取得SOTA表现，推理痕迹还能提升答案可靠性。


<details>
  <summary>Details</summary>
Motivation: 多数视频推理模型不指明关键证据时空信息，扩展图像证据中心推理能力到视频更具挑战，需跨动态场景联合时空跟踪和定位。

Method: 引入Open - o3 Video非代理框架，构建STGR - CoT - 30k和STGR - RL - 36k两个高质量数据集，采用带多种奖励的冷启动强化学习策略。

Result: 在V - STAR基准上提升mAM 14.4%和mLGM 24.2%，在多个视频理解基准测试中都有一致提升。

Conclusion: Open - o3 Video能有效将时空证据集成到视频推理，提高推理准确性和答案可靠性。

Abstract: Most video reasoning models only generate textual reasoning traces without
indicating when and where key evidence appears. Recent models such as OpenAI-o3
have sparked wide interest in evidence-centered reasoning for images, yet
extending this ability to videos is more challenging, as it requires joint
temporal tracking and spatial localization across dynamic scenes. We introduce
Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal
evidence into video reasoning, and carefully collect training data and design
training strategies to address the aforementioned challenges. The model
highlights key timestamps, objects, and bounding boxes alongside its answers,
allowing reasoning to be grounded in concrete visual observations. To enable
this functionality, we first curate and build two high-quality datasets,
STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed
temporal and spatial annotations, since most existing datasets offer either
temporal spans for videos or spatial boxes on images, lacking unified
spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start
reinforcement learning strategy with multiple specially designed rewards that
jointly encourage answer accuracy, temporal alignment, and spatial precision.
On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance,
raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent
improvements are also observed on a broad range of video understanding
benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond
accuracy, the reasoning traces produced by Open-o3 Video also provide valuable
signals for test-time scaling, enabling confidence-aware verification and
improving answer reliability.

</details>


### [243] [Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation](https://arxiv.org/abs/2510.20596)
*Ziyu Ye,Chen Ju,Chaofan Ma,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: 本文提出基于相似原型的跨模态分割框架，利用原型和相似性约束等提升性能，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在应用于未见数据时性能下降，无监督域适应可减少域差距，避免未见域昂贵标注成本。

Method: 提出基于相似原型的跨模态分割框架，学习类原型，引入相似性约束，用字典存储原型。

Result: 广泛实验表明该方法比其他先进方法取得更好结果。

Conclusion: 所提基于相似原型的跨模态分割框架有效，能提升性能。

Abstract: Deep learning models have achieved great success on various vision
challenges, but a well-trained model would face drastic performance degradation
when applied to unseen data. Since the model is sensitive to domain shift,
unsupervised domain adaptation attempts to reduce the domain gap and avoid
costly annotation of unseen domains. This paper proposes a novel framework for
cross-modality segmentation via similarity-based prototypes. In specific, we
learn class-wise prototypes within an embedding space, then introduce a
similarity constraint to make these prototypes representative for each semantic
class while separable from different classes. Moreover, we use dictionaries to
store prototypes extracted from different images, which prevents the
class-missing problem and enables the contrastive learning of prototypes, and
further improves performance. Extensive experiments show that our method
achieves better results than other state-of-the-art methods.

</details>


### [244] [OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving Objects](https://arxiv.org/abs/2510.20605)
*Mark He Huang,Lin Geng Foo,Christian Theobalt,Ying Sun,De Wen Soh*

Main category: cs.CV

TL;DR: 提出OnlineSplatter框架，可直接从RGB帧生成高质量以对象为中心的3D高斯，无需相机位姿等，在真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决单目视频中无可靠位姿或深度线索、任意物体运动下的自由移动对象重建难题。

Method: 引入OnlineSplatter框架，用第一帧锚定重建，通过密集高斯原语场逐步细化对象表示，采用双键内存模块融合特征。

Result: 在真实数据集评估中，显著优于现有无位姿重建基线，随观测增加性能提升，且内存和运行时间恒定。

Conclusion: OnlineSplatter框架在自由移动对象重建方面有效且高效。

Abstract: Free-moving object reconstruction from monocular video remains challenging,
particularly without reliable pose or depth cues and under arbitrary object
motion. We introduce OnlineSplatter, a novel online feed-forward framework
generating high-quality, object-centric 3D Gaussians directly from RGB frames
without requiring camera pose, depth priors, or bundle optimization. Our
approach anchors reconstruction using the first frame and progressively refines
the object representation through a dense Gaussian primitive field, maintaining
constant computational cost regardless of video sequence length. Our core
contribution is a dual-key memory module combining latent appearance-geometry
keys with explicit directional keys, robustly fusing current frame features
with temporally aggregated object states. This design enables effective
handling of free-moving objects via spatial-guided memory readout and an
efficient sparsification mechanism, ensuring comprehensive yet compact object
coverage. Evaluations on real-world datasets demonstrate that OnlineSplatter
significantly outperforms state-of-the-art pose-free reconstruction baselines,
consistently improving with more observations while maintaining constant memory
and runtime.

</details>


### [245] [BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models](https://arxiv.org/abs/2510.20095)
*Ziheng Zhang,Xinyue Ma,Arpita Chowdhury,Elizabeth G. Campolongo,Matthew J. Thompson,Net Zhang,Samuel Stevens,Hilmar Lapp,Tanya Berger-Wolf,Yu Su,Wei-Lun Chao,Jianyang Gu*

Main category: cs.CV

TL;DR: 研究用描述性字幕作为生物多模态基础模型的额外监督源，用MLLMs生成合成字幕训练BIOCAP模型，取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 图像和字幕可互补，但获取大规模忠实、特定实例的字幕存在挑战，限制了自然语言监督在生物领域的应用。

Method: 利用维基百科视觉信息和分类定制格式示例，用多模态大语言模型生成合成字幕，并用其训练BIOCAP模型。

Result: BIOCAP模型在物种分类和文本 - 图像检索中表现出色。

Conclusion: 描述性字幕在将生物图像与多模态基础模型连接方面具有超越标签的价值。

Abstract: This work investigates descriptive captions as an additional source of
supervision for biological multimodal foundation models. Images and captions
can be viewed as complementary samples from the latent morphospace of a
species, each capturing certain biological traits. Incorporating captions
during training encourages alignment with this shared latent structure,
emphasizing potentially diagnostic characters while suppressing spurious
correlations. The main challenge, however, lies in obtaining faithful,
instance-specific captions at scale. This requirement has limited the
utilization of natural language supervision in organismal biology compared with
many other scientific domains. We complement this gap by generating synthetic
captions with multimodal large language models (MLLMs), guided by
Wikipedia-derived visual information and taxon-tailored format examples. These
domain-specific contexts help reduce hallucination and yield accurate,
instance-based descriptive captions. Using these captions, we train BIOCAP
(i.e., BIOCLIP with Captions), a biological foundation model that captures rich
semantics and achieves strong performance in species classification and
text-image retrieval. These results demonstrate the value of descriptive
captions beyond labels in bridging biological images with multimodal foundation
models.

</details>


### [246] [Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding](https://arxiv.org/abs/2510.20244)
*Minseok Kang,Minhyeok Lee,Minjung Kim,Donghyeong Kim,Sangyoun Lee*

Main category: cs.CV

TL;DR: 本文指出现有视频时间定位（VTG）方法在跨模态注意力中对文本标记处理的不足，提出DualGround模型，在相关基准测试中取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有VTG方法在跨模态注意力中统一处理所有文本标记，忽略其不同语义角色，过度依赖全局语义，难以有效利用词级信号实现细粒度时间对齐。

Method: 提出DualGround双分支架构，分离全局和局部语义，引入标记角色感知的跨模态交互策略和联合建模框架。

Result: DualGround在QVHighlights和Charades - STA基准测试的Moment Retrieval和Highlight Detection任务中达到了SOTA性能。

Conclusion: 解耦语义建模在视频 - 语言对齐中是有效的。

Abstract: Video Temporal Grounding (VTG) aims to localize temporal segments in long,
untrimmed videos that align with a given natural language query. This task
typically comprises two subtasks: Moment Retrieval (MR) and Highlight Detection
(HD). While recent advances have been progressed by powerful pretrained
vision-language models such as CLIP and InternVideo2, existing approaches
commonly treat all text tokens uniformly during crossmodal attention,
disregarding their distinct semantic roles. To validate the limitations of this
approach, we conduct controlled experiments demonstrating that VTG models
overly rely on [EOS]-driven global semantics while failing to effectively
utilize word-level signals, which limits their ability to achieve fine-grained
temporal alignment. Motivated by this limitation, we propose DualGround, a
dual-branch architecture that explicitly separates global and local semantics
by routing the [EOS] token through a sentence-level path and clustering word
tokens into phrase-level units for localized grounding. Our method introduces
(1) tokenrole- aware cross modal interaction strategies that align video
features with sentence-level and phrase-level semantics in a structurally
disentangled manner, and (2) a joint modeling framework that not only improves
global sentence-level alignment but also enhances finegrained temporal
grounding by leveraging structured phrase-aware context. This design allows the
model to capture both coarse and localized semantics, enabling more expressive
and context-aware video grounding. DualGround achieves state-of-the-art
performance on both Moment Retrieval and Highlight Detection tasks across
QVHighlights and Charades- STA benchmarks, demonstrating the effectiveness of
disentangled semantic modeling in video-language alignment.

</details>


### [247] [Calibrating Multimodal Consensus for Emotion Recognition](https://arxiv.org/abs/2510.20256)
*Guowei Zhong,Junjie Li,Huaiyu Zhu,Ruohong Huan,Yun Pan*

Main category: cs.CV

TL;DR: 提出校准多模态共识模型CMC解决多模态情感识别问题，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感识别方法忽略模态间语义不一致且文本模态主导影响准确率。

Method: 提出CMC模型，含伪标签生成模块进行自监督单模态预训练，用无参数融合模块和多模态共识路由器进行多模态微调。

Result: CMC在四个数据集上表现与或优于现有方法，在有语义不一致的数据集上优势明显。

Conclusion: CMC模型有效解决多模态情感识别问题，代码公开。

Abstract: In recent years, Multimodal Emotion Recognition (MER) has made substantial
progress. Nevertheless, most existing approaches neglect the semantic
inconsistencies that may arise across modalities, such as conflicting emotional
cues between text and visual inputs. Besides, current methods are often
dominated by the text modality due to its strong representational capacity,
which can compromise recognition accuracy. To address these challenges, we
propose a model termed Calibrated Multimodal Consensus (CMC). CMC introduces a
Pseudo Label Generation Module (PLGM) to produce pseudo unimodal labels,
enabling unimodal pretraining in a self-supervised fashion. It then employs a
Parameter-free Fusion Module (PFM) and a Multimodal Consensus Router (MCR) for
multimodal finetuning, thereby mitigating text dominance and guiding the fusion
process toward a more reliable consensus. Experimental results demonstrate that
CMC achieves performance on par with or superior to state-of-the-art methods
across four datasets, CH-SIMS, CH-SIMS v2, CMU-MOSI, and CMU-MOSEI, and
exhibits notable advantages in scenarios with semantic inconsistencies on
CH-SIMS and CH-SIMS v2. The implementation of this work is publicly accessible
at https://github.com/gw-zhong/CMC.

</details>


### [248] [Deep Learning in Dental Image Analysis: A Systematic Review of Datasets, Methodologies, and Emerging Challenges](https://arxiv.org/abs/2510.20634)
*Zhenhuan Zhou,Jingbo Zhu,Yuchen Zhang,Xiaohang Guan,Peng Wang,Tao Li*

Main category: cs.CV

TL;DR: 本文系统综述260篇深度学习在牙科图像分析（DIA）应用研究，涵盖数据集与模型两方面，介绍基础概念，分析模型算法，总结指标，讨论挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 牙科图像分析存在低对比度、金属伪影等挑战，人工解读耗时且易不一致，基于人工智能的自动DIA是有前景的解决方案，深度学习在其中应用广泛，需对该领域进展进行全面总结。

Method: 系统回顾260篇研究，包括49篇公开牙科数据集论文和211篇基于深度学习算法论文，介绍牙科成像概念、数据集特征与采集方法，对相关模型算法按DIA任务分类分析，总结训练和评估指标。

Result: 对深度学习在DIA应用进行全面系统梳理，分析了数据集、模型算法等内容。

Conclusion: 本文为该领域研究者提供有价值的系统参考，补充材料和详细对比表将在GitHub公开。

Abstract: Efficient analysis and processing of dental images are crucial for dentists
to achieve accurate diagnosis and optimal treatment planning. However, dental
imaging inherently poses several challenges, such as low contrast, metallic
artifacts, and variations in projection angles. Combined with the subjectivity
arising from differences in clinicians' expertise, manual interpretation often
proves time-consuming and prone to inconsistency. Artificial intelligence
(AI)-based automated dental image analysis (DIA) offers a promising solution to
these issues and has become an integral part of computer-aided dental diagnosis
and treatment. Among various AI technologies, deep learning (DL) stands out as
the most widely applied and influential approach due to its superior feature
extraction and representation capabilities. To comprehensively summarize recent
progress in this field, we focus on the two fundamental aspects of DL
research-datasets and models. In this paper, we systematically review 260
studies on DL applications in DIA, including 49 papers on publicly available
dental datasets and 211 papers on DL-based algorithms. We first introduce the
basic concepts of dental imaging and summarize the characteristics and
acquisition methods of existing datasets. Then, we present the foundational
techniques of DL and categorize relevant models and algorithms according to
different DIA tasks, analyzing their network architectures, optimization
strategies, training methods, and performance. Furthermore, we summarize
commonly used training and evaluation metrics in the DIA domain. Finally, we
discuss the current challenges of existing research and outline potential
future directions. We hope that this work provides a valuable and systematic
reference for researchers in this field. All supplementary materials and
detailed comparison tables will be made publicly available on GitHub.

</details>


### [249] [Blur2seq: Blind Deblurring and Camera Trajectory Estimation from a Single Camera Motion-blurred Image](https://arxiv.org/abs/2510.20539)
*Guillermo Carbajal,Andrés Almansa,Pablo Musé*

Main category: cs.CV

TL;DR: 提出深度学习框架，从单张模糊图像联合估计潜在清晰图像和相机运动轨迹，表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决相机抖动（大或旋转运动）造成的运动模糊在图像恢复中的难题。

Method: 利用可微模糊创建模块实现投影运动模糊模型，用神经网络预测3D旋转轨迹，引导基于模型的恢复网络端到端训练，推理后通过重模糊损失优化轨迹。

Result: 在合成和真实数据集上达到了最先进的性能，尤其是在严重或空间变化模糊的情况下。

Conclusion: 所提方法在处理相机抖动导致的运动模糊问题上有效，优于端到端去模糊网络。

Abstract: Motion blur caused by camera shake, particularly under large or rotational
movements, remains a major challenge in image restoration. We propose a deep
learning framework that jointly estimates the latent sharp image and the
underlying camera motion trajectory from a single blurry image. Our method
leverages the Projective Motion Blur Model (PMBM), implemented efficiently
using a differentiable blur creation module compatible with modern networks. A
neural network predicts a full 3D rotation trajectory, which guides a
model-based restoration network trained end-to-end. This modular architecture
provides interpretability by revealing the camera motion that produced the
blur. Moreover, this trajectory enables the reconstruction of the sequence of
sharp images that generated the observed blurry image. To further refine
results, we optimize the trajectory post-inference via a reblur loss, improving
consistency between the blurry input and the restored output. Extensive
experiments show that our method achieves state-of-the-art performance on both
synthetic and real datasets, particularly in cases with severe or spatially
variant blur, where end-to-end deblurring networks struggle.
  Code and trained models are available at
https://github.com/GuillermoCarbajal/Blur2Seq/

</details>


### [250] [Efficient Multi-bit Quantization Network Training via Weight Bias Correction and Bit-wise Coreset Sampling](https://arxiv.org/abs/2510.20673)
*Jinhee Kim,Jae Jun An,Kang Eun Jeon,Jong Hwan Ko*

Main category: cs.CV

TL;DR: 提出两种技术降低多比特量化网络训练开销，实验证明能减少训练时间并保证准确率。


<details>
  <summary>Details</summary>
Motivation: 现有多比特量化网络训练开销大，每次支持的位宽都要全数据集更新，还常需额外微调。

Method: 提出权重偏差校正技术实现共享批量归一化并消除微调需求；采用逐位核心集采样策略让子模型在紧凑信息子集上训练。

Result: 在多个数据集和架构上实验，训练时间最多减少7.88倍，准确率有竞争力或更优。

Conclusion: 所提方法能在不影响模型效用的前提下大幅降低训练开销。

Abstract: Multi-bit quantization networks enable flexible deployment of deep neural
networks by supporting multiple precision levels within a single model.
However, existing approaches suffer from significant training overhead as
full-dataset updates are repeated for each supported bit-width, resulting in a
cost that scales linearly with the number of precisions. Additionally, extra
fine-tuning stages are often required to support additional or intermediate
precision options, further compounding the overall training burden. To address
this issue, we propose two techniques that greatly reduce the training overhead
without compromising model utility: (i) Weight bias correction enables shared
batch normalization and eliminates the need for fine-tuning by neutralizing
quantization-induced bias across bit-widths and aligning activation
distributions; and (ii) Bit-wise coreset sampling strategy allows each child
model to train on a compact, informative subset selected via gradient-based
importance scores by exploiting the implicit knowledge transfer phenomenon.
Experiments on CIFAR-10/100, TinyImageNet, and ImageNet-1K with both ResNet and
ViT architectures demonstrate that our method achieves competitive or superior
accuracy while reducing training time up to 7.88x. Our code is released at
https://github.com/a2jinhee/EMQNet_jk.

</details>


### [251] [Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation](https://arxiv.org/abs/2510.20812)
*Yuhan Liu,Lianhui Qin,Shengjie Wang*

Main category: cs.CV

TL;DR: 提出无需训练的Speculative Verdict (SV)框架处理信息密集图像推理难题，在多基准测试获增益，兼具纠错与成本效益。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在处理信息密集图像时存在定位关键线索和多跳推理难题，需新方法解决。

Method: 提出SV框架，结合轻量级草稿专家和大型裁决模型，草稿阶段小VLM生成推理路径，裁决阶段大VLM综合路径得答案，还引入共识专家选择机制。

Result: 在多个信息密集和高分辨率视觉问答基准测试中取得一致增益。

Conclusion: SV框架能从多个部分准确的推理路径中综合正确见解，相比大型专有模型或训练管道，实现了纠错和成本效益。

Abstract: Large Vision-Language Models (VLMs) have achieved remarkable progress in
multimodal understanding, yet they struggle when reasoning over
information-intensive images that densely interleave textual annotations with
fine-grained graphical elements. The main challenges lie in precisely
localizing critical cues in dense layouts and multi-hop reasoning to integrate
dispersed evidence. We propose Speculative Verdict (SV), a training-free
framework inspired by speculative decoding that combines multiple lightweight
draft experts with a large verdict model. In the draft stage, small VLMs act as
draft experts to generate reasoning paths that provide diverse localization
candidates; in the verdict stage, a strong VLM synthesizes these paths to
produce the final answer, minimizing computational cost while recovering
correct answers. To further improve efficiency and accuracy, SV introduces a
consensus expert selection mechanism that forwards only high-agreement
reasoning paths to the verdict. Empirically, SV achieves consistent gains on
challenging information-intensive and high-resolution visual question answering
benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K.
By synthesizing correct insights from multiple partially accurate reasoning
paths, SV achieves both error correction and cost-efficiency compared to large
proprietary models or training pipelines. Code is available at
https://github.com/Tinaliu0123/speculative-verdict

</details>


### [252] [AlphaFlow: Understanding and Improving MeanFlow Models](https://arxiv.org/abs/2510.20771)
*Huijie Zhang,Aliaksandr Siarohin,Willi Menapace,Michael Vasilkovsky,Sergey Tulyakov,Qing Qu,Ivan Skorokhodov*

Main category: cs.CV

TL;DR: 本文分析MeanFlow目标函数，提出α - Flow统一目标函数，采用课程策略优化，在ImageNet - 1K上优于MeanFlow。


<details>
  <summary>Details</summary>
Motivation: MeanFlow虽强大但成功原因未完全明晰，其目标函数存在优化冲突和收敛慢问题。

Method: 将MeanFlow目标分解，发现项间强负相关；引入α - Flow统一目标函数；采用课程策略从轨迹流匹配平滑退火到MeanFlow。

Result: α - Flow在不同规模和设置下始终优于MeanFlow，α - Flow - XL/2+模型取得新的SOTA结果。

Conclusion: α - Flow解决了MeanFlow的优化冲突问题，实现更好收敛并获得更优效果。

Abstract: MeanFlow has recently emerged as a powerful framework for few-step generative
modeling trained from scratch, but its success is not yet fully understood. In
this work, we show that the MeanFlow objective naturally decomposes into two
parts: trajectory flow matching and trajectory consistency. Through gradient
analysis, we find that these terms are strongly negatively correlated, causing
optimization conflict and slow convergence. Motivated by these insights, we
introduce $\alpha$-Flow, a broad family of objectives that unifies trajectory
flow matching, Shortcut Model, and MeanFlow under one formulation. By adopting
a curriculum strategy that smoothly anneals from trajectory flow matching to
MeanFlow, $\alpha$-Flow disentangles the conflicting objectives, and achieves
better convergence. When trained from scratch on class-conditional ImageNet-1K
256x256 with vanilla DiT backbones, $\alpha$-Flow consistently outperforms
MeanFlow across scales and settings. Our largest $\alpha$-Flow-XL/2+ model
achieves new state-of-the-art results using vanilla DiT backbones, with FID
scores of 2.58 (1-NFE) and 2.15 (2-NFE).

</details>


### [253] [Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge](https://arxiv.org/abs/2510.20819)
*Nimrod Berman,Omkar Joglekar,Eitan Kosman,Dotan Di Castro,Omri Azencot*

Main category: cs.CV

TL;DR: 提出用于模态转换的潜在去噪扩散桥模型LDDBM，在多种任务表现出色，建立新基准。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在模态转换任务存在局限，依赖限制性假设，缺乏通用性和理论基础。

Method: 基于潜在变量扩展去噪扩散桥模型，在共享潜在空间学习任意模态间桥梁，引入对比对齐损失和预测损失，设计领域无关的编码器 - 解码器架构，探索多种训练策略。

Result: 支持任意模态对，在多视图到3D形状生成、图像超分辨率等任务表现良好。

Conclusion: 综合实验验证了框架有效性，建立通用模态转换新基准。

Abstract: Recent advances in generative modeling have positioned diffusion models as
state-of-the-art tools for sampling from complex data distributions. While
these models have shown remarkable success across single-modality domains such
as images and audio, extending their capabilities to Modality Translation (MT),
translating information across different sensory modalities, remains an open
challenge. Existing approaches often rely on restrictive assumptions, including
shared dimensionality, Gaussian source priors, and modality-specific
architectures, which limit their generality and theoretical grounding. In this
work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a
general-purpose framework for modality translation based on a latent-variable
extension of Denoising Diffusion Bridge Models. By operating in a shared latent
space, our method learns a bridge between arbitrary modalities without
requiring aligned dimensions. We introduce a contrastive alignment loss to
enforce semantic consistency between paired samples and design a
domain-agnostic encoder-decoder architecture tailored for noise prediction in
latent space. Additionally, we propose a predictive loss to guide training
toward accurate cross-domain translation and explore several training
strategies to improve stability. Our approach supports arbitrary modality pairs
and performs strongly on diverse MT tasks, including multi-view to 3D shape
generation, image super-resolution, and multi-view scene synthesis.
Comprehensive experiments and ablations validate the effectiveness of our
framework, establishing a new strong baseline in general modality translation.
For more information, see our project page:
https://sites.google.com/view/lddbm/home.

</details>


### [254] [Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers](https://arxiv.org/abs/2510.20807)
*Dean L Slack,G Thomas Hudson,Thomas Winterbottom,Noura Al Moubayed*

Main category: cs.CV

TL;DR: 本文研究基于transformer的视频预测模型，提出简单有效模型，提升物理准确预测时间，还开展可解释性实验。


<details>
  <summary>Details</summary>
Motivation: 受自回归大语言模型启发，解决现有视频生成方法在物理模拟因果建模上的不足。

Method: 采用简单端到端方法，比较不同时空自注意力布局，通过物理对象跟踪指标和无监督训练隔离时空推理，使用连续像素空间表示进行视频预测。

Result: 相比现有潜在空间方法，将物理准确预测时间延长达50%，在常见视频质量指标上表现相当；可解释性实验成果能推广到分布外模拟参数估计。

Conclusion: 此工作为基于注意力的视频时空建模提供简单、参数高效且可解释的平台。

Abstract: Inspired by the performance and scalability of autoregressive large language
models (LLMs), transformer-based models have seen recent success in the visual
domain. This study investigates a transformer adaptation for video prediction
with a simple end-to-end approach, comparing various spatiotemporal
self-attention layouts. Focusing on causal modeling of physical simulations
over time; a common shortcoming of existing video-generative approaches, we
attempt to isolate spatiotemporal reasoning via physical object tracking
metrics and unsupervised training on physical simulation datasets. We introduce
a simple yet effective pure transformer model for autoregressive video
prediction, utilizing continuous pixel-space representations for video
prediction. Without the need for complex training strategies or latent
feature-learning components, our approach significantly extends the time
horizon for physically accurate predictions by up to 50% when compared with
existing latent-space approaches, while maintaining comparable performance on
common video quality metrics. In addition, we conduct interpretability
experiments to identify network regions that encode information useful to
perform accurate estimations of PDE simulation parameters via probing models,
and find that this generalizes to the estimation of out-of-distribution
simulation parameters. This work serves as a platform for further
attention-based spatiotemporal modeling of videos via a simple, parameter
efficient, and interpretable approach.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [255] [Experimental differentiation and extremization with analog quantum circuits](https://arxiv.org/abs/2510.20713)
*Evan Philip,Julius de Hond,Vytautas Abramavicius,Kaonan Micadei,Mario Dagrada,Panagiotis Barkoutsos,Mourad Beji,Louis-Paul Henry,Vincent E. Elfving,Antonio A. Gentile,Savvas Varsamopoulos*

Main category: quant-ph

TL;DR: 本文首次实验演示了DQC和QEL在合成用例上的性能，并在模拟量子计算机上挑战了它们需数字量子硬件的假设。


<details>
  <summary>Details</summary>
Motivation: 量子架构有望加速科学计算，人们关注量子算法求解微分方程的效率，DQC和QEL为求解微分方程提供新途径，需实验验证其性能。

Method: 进行DQC和QEL的首次实验演示，在基于中性原子技术的商业模拟量子计算机上运行闭环实例。

Result: 成功在模拟量子计算机上运行DQC和QEL的闭环实例，展示了它们在合成用例上的性能。

Conclusion: 挑战了DQC和QEL需数字量子硬件的假设，证明其可在模拟量子计算机上运行。

Abstract: Solving and optimizing differential equations (DEs) is ubiquitous in both
engineering and fundamental science. The promise of quantum architectures to
accelerate scientific computing thus naturally involved interest towards how
efficiently quantum algorithms can solve DEs. Differentiable quantum circuits
(DQC) offer a viable route to compute DE solutions using a variational approach
amenable to existing quantum computers, by producing a machine-learnable
surrogate of the solution. Quantum extremal learning (QEL) complements such
approach by finding extreme points in the output of learnable models of unknown
(implicit) functions, offering a powerful tool to bypass a full DE solution, in
cases where the crux consists in retrieving solution extrema. In this work, we
provide the results from the first experimental demonstration of both DQC and
QEL, displaying their performance on a synthetic usecase. Whilst both DQC and
QEL are expected to require digital quantum hardware, we successfully challenge
this assumption by running a closed-loop instance on a commercial analog
quantum computer, based upon neutral atom technology.

</details>


### [256] [On Encoding Matrices using Quantum Circuits](https://arxiv.org/abs/2510.20030)
*Liron Mor Yosef,Haim Avron*

Main category: quant-ph

TL;DR: 本文系统研究矩阵以块编码和状态制备电路形式的编码，给出从经典矩阵构建电路表示的方法和两种电路表示间的量子双向转换方法，证明两种模型基本等价。


<details>
  <summary>Details</summary>
Motivation: 量子计算中高效执行算法关键在于将输入表示为量子电路，现有块编码和状态制备电路两种常见表示，需系统研究。

Method: 研究从经典形式矩阵构建块编码和状态制备电路的方法，以及两种电路表示间的量子双向转换方法，利用特殊常数深度复用器和量子转换算法。

Result: 建立了从经典形式矩阵高效构建块编码的通用方法，以及块编码和状态制备电路间低开销双向转换算法。

Conclusion: 块编码和状态制备电路这两种模型本质上是等价的。

Abstract: Over a decade ago, it was demonstrated that quantum computing has the
potential to revolutionize numerical linear algebra by enabling algorithms with
complexity superior to what is classically achievable, e.g., the seminal HHL
algorithm for solving linear systems. Efficient execution of such algorithms
critically depends on representing inputs (matrices and vectors) as quantum
circuits that encode or implement these inputs. For that task, two common
circuit representations emerged in the literature: block encodings and state
preparation circuits. In this paper, we systematically study encodings matrices
in the form of block encodings and state preparation circuits. We examine
methods for constructing these representations from matrices given in classical
form, as well as quantum two-way conversions between circuit representations.
Two key results we establish (among others) are: (a) a general method for
efficiently constructing a block encoding of an arbitrary matrix given in
classical form (entries stored in classical random access memory); and (b)
low-overhead, bidirectional conversion algorithms between block encodings and
state preparation circuits, showing that these models are essentially
equivalent. From a technical perspective, two central components of our
constructions are: (i) a special constant-depth multiplexer that simultaneously
multiplexes all higher-order Pauli matrices of a given size, and (ii) an
algorithm for performing a quantum conversion between a matrix's expansion in
the standard basis and its expansion in the basis of higher-order Pauli
matrices.

</details>


### [257] [Quantum Processing Unit (QPU) processing time Prediction with Machine Learning](https://arxiv.org/abs/2510.20630)
*Lucy Xing,Sanjay Vishwakarma,David Kremer,Francisco Martin-Fernandez,Ismael Faro,Juan Cruz-Benito*

Main category: quant-ph

TL;DR: 本文探索机器学习技术在预测量子作业QPU处理时间的应用，采用LightGBM模型预测，结果证明有效，为量子计算操作集成AI工具奠定基础。


<details>
  <summary>Details</summary>
Motivation: 提高量子计算系统的操作效率，改善资源管理和调度。

Method: 使用约15万个遵循IBM Quantum模式的作业数据集，采用基于Gradient - Boosting (LightGBM)的机器学习方法，并结合数据预处理提高模型精度。

Result: 证明了机器学习在预测量子作业方面的有效性。

Conclusion: 凸显了机器学习在优化量子作业预测方面的潜力，为高级量子计算操作集成AI驱动工具奠定基础。

Abstract: This paper explores the application of machine learning (ML) techniques in
predicting the QPU processing time of quantum jobs. By leveraging ML
algorithms, this study introduces predictive models that are designed to
enhance operational efficiency in quantum computing systems. Using a dataset of
about 150,000 jobs that follow the IBM Quantum schema, we employ ML methods
based on Gradient-Boosting (LightGBM) to predict the QPU processing times,
incorporating data preprocessing methods to improve model accuracy. The results
demonstrate the effectiveness of ML in forecasting quantum jobs. This
improvement can have implications on improving resource management and
scheduling within quantum computing frameworks. This research not only
highlights the potential of ML in refining quantum job predictions but also
sets a foundation for integrating AI-driven tools in advanced quantum computing
operations.

</details>


### [258] [Co-Designing Quantum Codes with Transversal Diagonal Gates via Multi-Agent Systems](https://arxiv.org/abs/2510.20728)
*Xi He,Sirui Lu,Bei Zeng*

Main category: quant-ph

TL;DR: 提出多智能体、人在回路工作流，基于SSLP框架结合GPT - 5在TeXRA平台设计量子码，对小参数进行系统搜索并得到成果，将对角横向可行性转化为分析流程。


<details>
  <summary>Details</summary>
Motivation: 设计具有规定横向对角门的量子码。

Method: 构建多智能体、人在回路工作流，基于SSLP框架，由GPT - 5驱动，在TeXRA平台实现，三个角色协作，在LaTeX - Python环境工作。

Result: 针对距离d = 2、非简并残差情况，对小参数系统搜索得到证书支持的表格，抽象出封闭形式族，展示新的((6,4,2))码。

Conclusion: 该工作流将对角横向可行性转化为可扩展分析流程，可实现可重现的代码构造，支持扩展和数据驱动分类。

Abstract: We present a multi-agent, human-in-the-loop workflow that co-designs quantum
codes with prescribed transversal diagonal gates. It builds on the Subset-Sum
Linear Programming (SSLP) framework (arXiv:2504.20847), which partitions basis
strings by modular residues and enforces $Z$-marginal Knill-Laflamme (KL)
equalities via small LPs. The workflow is powered by GPT-5 and implemented
within TeXRA (https://texra.ai)-a multi-agent research assistant platform that
supports an iterative tool-use loop agent and a derivation-then-edit workflow
reasoning agent. We work in a LaTeX-Python environment where agents reason,
edit documents, execute code, and synchronize their work to Git/Overleaf.
Within this workspace, three roles collaborate: a Synthesis Agent formulates
the problem; a Search Agent sweeps/screens candidates and exactifies numerics
into rationals; and an Audit Agent independently checks all KL equalities and
the induced logical action. As a first step we focus on distance $d=2$ with
nondegenerate residues. For code dimension $K\in\{2,3,4\}$ and $n\le6$ qubits,
systematic sweeps yield certificate-backed tables cataloging attainable cyclic
logical groups-all realized by new codes-e.g., for $K=3$ we obtain order $16$
at $n=6$. From verified instances, Synthesis Agent abstracts recurring
structures into closed-form families and proves they satisfy the KL equalities
for all parameters. It further demonstrates that SSLP accommodates residue
degeneracy by exhibiting a new $((6,4,2))$ code implementing the transversal
controlled-phase $diag(1,1,1,i)$. Overall, the workflow recasts
diagonal-transversal feasibility as an analytical pipeline executed at scale,
combining systematic enumeration with exact analytical reconstruction. It
yields reproducible code constructions, supports targeted extensions to larger
$K$ and higher distances, and leads toward data-driven classification.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [259] [A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous Networks](https://arxiv.org/abs/2510.19973)
*Hatim Chergui,Farhad Rezazadeh,Merouane Debbah,Christos Verikoukis*

Main category: cs.NI

TL;DR: 本文指出6G网络更高自主性需超越KPI优化，可通过智能AI实现，但部署智能代理存在认知偏差问题。文章介绍偏差种类、缓解策略及两个应用案例，应用特定技术避免偏差，取得降低延迟和节能效果。


<details>
  <summary>Details</summary>
Motivation: 6G网络更高自主性仅优化KPI不够，智能AI代理部署存在认知偏差问题，需解决。

Method: 提供认知偏差教程，包括分类、定义、数学公式等，针对不同偏差提出缓解策略，给出两个实际应用案例。

Result: 通过特定技术避免代理在决策中出现偏差，在第二个用例中使延迟降低5倍，节能约40%。

Conclusion: 采用合适的缓解策略能避免智能AI代理的认知偏差，提升6G网络管理的性能。

Abstract: The path to higher network autonomy in 6G lies beyond the mere optimization
of key performance indicators (KPIs). While KPIs have enabled automation gains
under TM Forum Levels 1--3, they remain numerical abstractions that act only as
proxies for the real essence of communication networks: seamless connectivity,
fairness, adaptability, and resilience. True autonomy requires perceiving and
reasoning over the network environment as it is. Such progress can be achieved
through \emph{agentic AI}, where large language model (LLM)-powered agents
perceive multimodal telemetry, reason with memory, negotiate across domains,
and act via APIs to achieve multi-objective goals. However, deploying such
agents introduces the challenge of cognitive biases inherited from human
design, which can distort reasoning, negotiation, tool use, and actuation.
Between neuroscience and AI, this paper provides a tutorial on a selection of
well-known biases, including their taxonomy, definition, mathematical
formulation, emergence in telecom systems and the commonly impacted agentic
components. The tutorial also presents various mitigation strategies tailored
to each type of bias. The article finally provides two practical use-cases,
which tackle the emergence, impact and mitigation gain of some famous biases in
6G inter-slice and cross-domain management. In particular, anchor
randomization, temporal decay and inflection bonus techniques are introduced to
specifically address anchoring, temporal and confirmation biases. This avoids
that agents stick to the initial high resource allocation proposal or decisions
that are recent and/or confirming a prior hypothesis. By grounding decisions in
a richer and fairer set of past experiences, the quality and bravery of the
agentic agreements in the second use-case, for instance, are leading to $\times
5$ lower latency and around $40\%$ higher energy saving.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [260] [In-DRAM True Random Number Generation Using Simultaneous Multiple-Row Activation: An Experimental Study of Real DRAM Chips](https://arxiv.org/abs/2510.20269)
*Ismail Emir Yuksel,Ataberk Olgun,F. Nisa Bostanci,Oguzhan Canpolat,Geraldo F. Oliveira,Mohammad Sadrosadati,Abdullah Giray Yaglikci,Onur Mutlu*

Main category: cs.AR

TL;DR: 本文通过对96个DDR4 DRAM芯片的广泛表征，实验证明可利用SiMRA在商用DRAM芯片中以高吞吐量和低延迟生成真随机数，并分析其随机生成潜力，得出多项关键结果，还开源了基础设施。


<details>
  <summary>Details</summary>
Motivation: 在商用DRAM芯片中实现高吞吐量和低延迟的真随机数生成。

Method: 对96个DDR4 DRAM芯片进行广泛表征，利用SiMRA，分析不同激活行数、数据模式、温度水平和空间变化下的真随机生成潜力。

Result: 1. SiMRA-based TRNG设计通过NIST随机性测试；2. 部分设计在吞吐量上优于现有DRAM-based TRNG；3. 熵随同时激活的DRAM行数增加；4. 操作参数和条件显著影响熵。

Conclusion: 可利用SiMRA在商用DRAM芯片中有效生成真随机数，开源基础设施助力后续研究。

Abstract: In this work, we experimentally demonstrate that it is possible to generate
true random numbers at high throughput and low latency in commercial
off-the-shelf (COTS) DRAM chips by leveraging simultaneous multiple-row
activation (SiMRA) via an extensive characterization of 96 DDR4 DRAM chips. We
rigorously analyze SiMRA's true random generation potential in terms of
entropy, latency, and throughput for varying numbers of simultaneously
activated DRAM rows (i.e., 2, 4, 8, 16, and 32), data patterns, temperature
levels, and spatial variations. Among our 11 key experimental observations, we
highlight four key results. First, we evaluate the quality of our TRNG designs
using the commonly-used NIST statistical test suite for randomness and find
that all SiMRA-based TRNG designs successfully pass each test. Second, 2-, 8-,
16-, and 32-row activation-based TRNG designs outperform the state-of-theart
DRAM-based TRNG in throughput by up to 1.15x, 1.99x, 1.82x, and 1.39x,
respectively. Third, SiMRA's entropy tends to increase with the number of
simultaneously activated DRAM rows. Fourth, operational parameters and
conditions (e.g., data pattern and temperature) significantly affect entropy.
For example, for most of the tested modules, the average entropy of 32-row
activation is 2.51x higher than that of 2-row activation. For example,
increasing the temperature from 50{\deg}C to 90{\deg}C decreases SiMRA's
entropy by 1.53x for 32-row activation. To aid future research and development,
we open-source our infrastructure at https://github.com/CMU-SAFARI/SiMRA-TRNG.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [261] [UniSE: A Unified Framework for Decoder-only Autoregressive LM-based Speech Enhancement](https://arxiv.org/abs/2510.20441)
*Haoyin Yan,Chengwei Liu,Shaofei Xue,Xiaotao Liang,Zheng Xue*

Main category: cs.SD

TL;DR: 提出统一解码器LM框架UniSE处理多种语音增强任务，实验表明其有竞争力。


<details>
  <summary>Details</summary>
Motivation: 缺乏基于自回归语言模型在统一语音增强不同子任务有效性的验证。

Method: 提出统一解码器LM框架UniSE，以输入语音特征为条件，用自回归建模生成目标语音离散令牌。

Result: 在多个基准测试中，UniSE与判别和生成基线相比有竞争力。

Conclusion: 语言模型有统一语音增强任务的能力。

Abstract: The development of neural audio codecs (NACs) has largely promoted
applications of language models (LMs) to speech processing and understanding.
However, there lacks the verification on the effectiveness of autoregressive
(AR) LMbased models in unifying different sub-tasks of speech enhancement (SE).
In this work, we propose UniSE, a unified decoder-only LM-based framework to
handle different SE tasks including speech restoration, target speaker
extraction and speech separation. It takes input speech features as conditions
and generates discrete tokens of the target speech using AR modeling, which
facilitates a compatibility between distinct learning patterns of multiple
tasks. Experiments on several benchmarks indicate the proposed UniSE can
achieve competitive performance compared to discriminative and generative
baselines, showing the capacity of LMs in unifying SE tasks. The demo page is
available here: https://github.com/hyyan2k/UniSE.

</details>


### [262] [Resounding Acoustic Fields with Reciprocity](https://arxiv.org/abs/2510.20602)
*Zitong Lan,Yiduo Hao,Mingmin Zhao*

Main category: cs.SD

TL;DR: 本文提出resounding任务，引入Versa方法促进声场学习，结果表明其提升了声场学习性能和沉浸式空间音效体验。


<details>
  <summary>Details</summary>
Motivation: 在虚拟环境中实现沉浸式听觉体验需要支持动态声源位置的灵活声音建模。

Method: 引入resounding任务，利用互易性属性，提出Versa方法，通过交换发射器和听众位置创建物理有效样本，并提出自监督学习方法解决部署互易性的挑战。

Result: Versa在模拟和真实数据集上显著提升了声场学习性能，感知用户研究表明其能极大改善沉浸式空间音效体验。

Conclusion: Versa方法可有效提升声场学习性能和沉浸式空间音效体验。

Abstract: Achieving immersive auditory experiences in virtual environments requires
flexible sound modeling that supports dynamic source positions. In this paper,
we introduce a task called resounding, which aims to estimate room impulse
responses at arbitrary emitter location from a sparse set of measured emitter
positions, analogous to the relighting problem in vision. We leverage the
reciprocity property and introduce Versa, a physics-inspired approach to
facilitating acoustic field learning. Our method creates physically valid
samples with dense virtual emitter positions by exchanging emitter and listener
poses. We also identify challenges in deploying reciprocity due to
emitter/listener gain patterns and propose a self-supervised learning approach
to address them. Results show that Versa substantially improve the performance
of acoustic field learning on both simulated and real-world datasets across
different metrics. Perceptual user studies show that Versa can greatly improve
the immersive spatial sound experience. Code, dataset and demo videos are
available on the project website: https://waves.seas.upenn.edu/projects/versa.

</details>


### [263] [R2-SVC: Towards Real-World Robust and Expressive Zero-shot Singing Voice Conversion](https://arxiv.org/abs/2510.20677)
*Junjie Zheng,Gongyu Chen,Chaofan Ding,Zihao Chen*

Main category: cs.SD

TL;DR: 提出R2 - SVC框架解决真实场景下SVC应用面临的噪声和表达性挑战，在多个基准测试取得SOTA成果。


<details>
  <summary>Details</summary>
Motivation: 传统SVC方法未考虑实际部署场景，训练和推理多依赖干净数据，与真实场景存在数据不匹配问题，难以实际应用。

Method: 1. 通过随机基频扰动和音乐分离伪像模拟增强鲁棒性；2. 用特定领域歌唱数据丰富说话人表征；3. 集成神经源 - 滤波器模型表示谐波和噪声分量。

Result: R2 - SVC在多个SVC基准测试的干净和噪声条件下均取得了最优结果。

Conclusion: R2 - SVC框架能有效应对真实场景下SVC应用的噪声和表达性挑战。

Abstract: In real-world singing voice conversion (SVC) applications, environmental
noise and the demand for expressive output pose significant challenges.
Conventional methods, however, are typically designed without accounting for
real deployment scenarios, as both training and inference usually rely on clean
data. This mismatch hinders practical use, given the inevitable presence of
diverse noise sources and artifacts from music separation. To tackle these
issues, we propose R2-SVC, a robust and expressive SVC framework. First, we
introduce simulation-based robustness enhancement through random fundamental
frequency ($F_0$) perturbations and music separation artifact simulations
(e.g., reverberation, echo), substantially improving performance under noisy
conditions. Second, we enrich speaker representation using domain-specific
singing data: alongside clean vocals, we incorporate DNSMOS-filtered separated
vocals and public singing corpora, enabling the model to preserve speaker
timbre while capturing singing style nuances. Third, we integrate the Neural
Source-Filter (NSF) model to explicitly represent harmonic and noise
components, enhancing the naturalness and controllability of converted singing.
R2-SVC achieves state-of-the-art results on multiple SVC benchmarks under both
clean and noisy conditions.

</details>


### [264] [Decoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment](https://arxiv.org/abs/2510.20513)
*Zhiyu Lin,Jingwen Yang,Jiale Zhao,Meng Liu,Sunzhu Li,Benyou Wang*

Main category: cs.SD

TL;DR: 提出DeEAR框架将语音表现力偏好转化为客观分数，能与人类感知强对齐，还可用于基准测试和数据筛选，提升S2S模型表现力。


<details>
  <summary>Details</summary>
Motivation: 现有S2S模型缺乏自然表现力且缺少可靠评估指标，现有评估方法存在成本高、有局限等问题。

Method: 提出DeEAR框架，基于语音学和心理学从情感、韵律和自发性三个维度评估语音。

Result: DeEAR与人类感知强对齐（SRCC = 0.86），用少于500个标注样本；能区分S2S模型表现力差距，筛选出14K表达性语句形成ExpressiveSpeech，提升S2S模型表现力得分。

Conclusion: DeEAR是一个有效评估语音表现力的框架，可用于基准测试和数据筛选，提升S2S模型表现。

Abstract: Recent speech-to-speech (S2S) models generate intelligible speech but still
lack natural expressiveness, largely due to the absence of a reliable
evaluation metric. Existing approaches, such as subjective MOS ratings,
low-level acoustic features, and emotion recognition are costly, limited, or
incomplete. To address this, we present DeEAR (Decoding the Expressive
Preference of eAR), a framework that converts human preference for speech
expressiveness into an objective score. Grounded in phonetics and psychology,
DeEAR evaluates speech across three dimensions: Emotion, Prosody, and
Spontaneity, achieving strong alignment with human perception (Spearman's Rank
Correlation Coefficient, SRCC = 0.86) using fewer than 500 annotated samples.
Beyond reliable scoring, DeEAR enables fair benchmarking and targeted data
curation. It not only distinguishes expressiveness gaps across S2S models but
also selects 14K expressive utterances to form ExpressiveSpeech, which improves
the expressive score (from 2.0 to 23.4 on a 100-point scale) of S2S models.
Demos and codes are available at
https://github.com/FreedomIntelligence/ExpressiveSpeech

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [265] [Decentralized Exchange that Mitigate a Bribery Attack](https://arxiv.org/abs/2510.20645)
*Nitin Awathare*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite the popularity of Hashed Time-Locked Contracts (HTLCs) because of
their use in wide areas of applications such as payment channels, atomic swaps,
etc, their use in exchange is still questionable. This is because of its
incentive incompatibility and susceptibility to bribery attacks.
  State-of-the-art solutions such as MAD-HTLC (Oakland'21) and He-HTLC
(NDSS'23) address this by leveraging miners' profit-driven behaviour to
mitigate such attacks. The former is the mitigation against passive miners;
however, the latter works against both active and passive miners. However, they
consider only two bribing scenarios where either of the parties involved in the
transfer collude with the miner.
  In this paper, we expose vulnerabilities in state-of-the-art solutions by
presenting a miner-collusion bribery attack with implementation and
game-theoretic analysis. Additionally, we propose a stronger attack on MAD-HTLC
than He-HTLC, allowing the attacker to earn profits equivalent to attacking
naive HTLC.
  Leveraging our insights, we propose \prot, a game-theoretically secure HTLC
protocol resistant to all bribery scenarios. \prot\ employs a two-phase
approach, preventing unauthorized token confiscation by third parties, such as
miners. In Phase 1, parties commit to the transfer; in Phase 2, the transfer is
executed without manipulation. We demonstrate \prot's efficiency in transaction
cost and latency via implementations on Bitcoin and Ethereum.

</details>


### [266] [Designing a Secure and Resilient Distributed Smartphone Participant Data Collection System](https://arxiv.org/abs/2510.19938)
*Foad Namjoo,Neng Wan,Devan Mallory,Yuyi Chang,Nithin Sugavanam,Long Yin Lee,Ning Xiong,Emre Ertin,Jeff M. Phillips*

Main category: cs.CR

TL;DR: 介绍了基于智能手机的MotionPI系统，用于收集行为和健康数据，能在现实约束下工作，是网络物理健康研究中安全可扩展移动数据收集的实用方案。


<details>
  <summary>Details</summary>
Motivation: 现实世界健康研究需要从移动和可穿戴设备持续安全地收集数据。

Method: 系统集成被动数据收集与生态瞬时评估（EMA）调查，可随机或基于身体活动触发；数据本地和云端存储，加密传输和存储；通过低功耗蓝牙集成腕带设备。

Result: MotionPI系统能在有限电池寿命、弱或间歇性蜂窝连接和最少用户监督等现实约束下工作。

Conclusion: MotionPI是网络物理健康研究中安全可扩展移动数据收集的实用解决方案。

Abstract: Real-world health studies require continuous and secure data collection from
mobile and wearable devices. We introduce MotionPI, a smartphone-based system
designed to collect behavioral and health data through sensors and surveys with
minimal interaction from participants. The system integrates passive data
collection (such as GPS and wristband motion data) with Ecological Momentary
Assessment (EMA) surveys, which can be triggered randomly or based on physical
activity. MotionPI is designed to work under real-life constraints, including
limited battery life, weak or intermittent cellular connection, and minimal
user supervision. It stores data both locally and on a secure cloud server,
with encrypted transmission and storage. It integrates through Bluetooth Low
Energy (BLE) into wristband devices that store raw data and communicate motion
summaries and trigger events. MotionPI demonstrates a practical solution for
secure and scalable mobile data collection in cyber-physical health studies.

</details>


### [267] [Q-RAN: Quantum-Resilient O-RAN Architecture](https://arxiv.org/abs/2510.19968)
*Vipin Rathi,Lakshya Chopra,Madhav Agarwal,Nitin Rajput,Kriish Sharma,Sushant Mundepi,Shivam Gangwar,Rudraksh Rawal,Jishan*

Main category: cs.CR

TL;DR: 本文提出Q - RAN框架，用NIST标准化后量子密码学保障O - RAN网络抗量子攻击。


<details>
  <summary>Details</summary>
Motivation: 电信业面临向O - RAN架构转变和量子计算威胁，O - RAN架构攻击面大，HNDL攻击使威胁更紧迫。

Method: 提出Q - RAN框架，实现ML - KEM和ML - DSA，集成QRNG，部署PQ - IPsec等协议，设立PQ - CA。

Result: 提供了保障O - RAN生态系统免受量子攻击的完整路线图。

Conclusion: Q - RAN框架能有效保障O - RAN网络抵御量子攻击。

Abstract: The telecommunications industry faces a dual transformation: the
architectural shift toward Open Radio Access Networks (O-RAN) and the emerging
threat from quantum computing. O-RAN disaggregated, multi-vendor architecture
creates a larger attack surface vulnerable to crypt-analytically relevant
quantum computers(CRQCs) that will break current public key cryptography. The
Harvest Now, Decrypt Later (HNDL) attack strategy makes this threat immediate,
as adversaries can intercept encrypted data today for future decryption. This
paper presents Q-RAN, a comprehensive quantum-resistant security framework for
O-RAN networks using NIST-standardized Post-Quantum Cryptography (PQC). We
detail the implementation of ML-KEM (FIPS 203) and ML-DSA (FIPS 204),
integrated with Quantum Random Number Generators (QRNG) for cryptographic
entropy. The solution deploys PQ-IPsec, PQ-DTLS, and PQ-mTLS protocols across
all O-RAN interfaces, anchored by a centralized Post-Quantum Certificate
Authority (PQ-CA) within the SMO framework. This work provides a complete
roadmap for securing disaggregated O-RAN ecosystems against quantum
adversaries.

</details>


### [268] [RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines](https://arxiv.org/abs/2510.20768)
*Austin Jia,Avaneesh Ramesh,Zain Shamsi,Daniel Zhang,Alex Liu*

Main category: cs.CR

TL;DR: 本文提出用源可信度算法（以PageRank为例）提升检索增强生成（RAG）在网络威胁情报（CTI）系统中的防御鲁棒性，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: RAG在CTI系统中易受中毒攻击，且现有防御方法在CTI场景中可能失效，因为网络威胁信息常是新的，威胁行为者会模仿合法格式。

Method: 在语料库上应用源可信度算法（以PageRank为例）来加速现代RAG防御的鲁棒性。

Result: 使用标准化的MS MARCO数据集定量证明算法给恶意文档较低权威分数，提升可信内容；在CTI文档和馈送上展示了算法的概念验证性能。

Conclusion: 源可信度算法能有效提升RAG在CTI系统中的防御鲁棒性。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as the dominant
architectural pattern to operationalize Large Language Model (LLM) usage in
Cyber Threat Intelligence (CTI) systems. However, this design is susceptible to
poisoning attacks, and previously proposed defenses can fail for CTI contexts
as cyber threat information is often completely new for emerging attacks, and
sophisticated threat actors can mimic legitimate formats, terminology, and
stylistic conventions. To address this issue, we propose that the robustness of
modern RAG defenses can be accelerated by applying source credibility
algorithms on corpora, using PageRank as an example. In our experiments, we
demonstrate quantitatively that our algorithm applies a lower authority score
to malicious documents while promoting trusted content, using the standardized
MS MARCO dataset. We also demonstrate proof-of-concept performance of our
algorithm on CTI documents and feeds.

</details>


### [269] [QORE : Quantum Secure 5G/B5G Core](https://arxiv.org/abs/2510.19982)
*Vipin Rathi,Lakshya Chopra,Rudraksh Rawal,Nitin Rajput,Shiva Valia,Madhav Aggarwal,Aditya Gairola*

Main category: cs.CR

TL;DR: 本文介绍量子安全框架QORE，助力5G及B5G核心网向PQC过渡，实验验证其有效性并与相关标准活动契合。


<details>
  <summary>Details</summary>
Motivation: 现有5G系统加密基础易受量子攻击，保护5G网络免受量子对手攻击是紧迫任务。

Method: 引入QORE框架，使用NIST标准化格基算法ML - KEM和ML - DSA，应用于5G SBA，提出HPQC配置。

Result: ML - KEM实现量子安全且性能开销小，满足5G系统低延迟和高吞吐量要求。

Conclusion: 所提路线图与相关标准活动契合，为降低量子时代风险、保障网络数据安全提供实用指导。

Abstract: Quantum computing is reshaping the security landscape of modern
telecommunications. The cryptographic foundations that secure todays 5G
systems, including RSA, Elliptic Curve Cryptography (ECC), and Diffie-Hellman
(DH), are all susceptible to attacks enabled by Shors algorithm. Protecting 5G
networks against future quantum adversaries has therefore become an urgent
engineering and research priority. In this paper we introduce QORE, a
quantum-secure 5G and Beyond 5G (B5G) Core framework that provides a clear
pathway for transitioning both the 5G Core Network Functions and User Equipment
(UE) to Post-Quantum Cryptography (PQC). The framework uses the
NIST-standardized lattice-based algorithms Module-Lattice Key Encapsulation
Mechanism (ML-KEM) and Module-Lattice Digital Signature Algorithm (ML-DSA) and
applies them across the 5G Service-Based Architecture (SBA). A Hybrid PQC
(HPQC) configuration is also proposed, combining classical and quantum-safe
primitives to maintain interoperability during migration. Experimental
validation shows that ML-KEM achieves quantum security with minor performance
overhead, meeting the low-latency and high-throughput requirements of
carrier-grade 5G systems. The proposed roadmap aligns with ongoing 3GPP SA3 and
SA5 study activities on the security and management of post-quantum networks as
well as with NIST PQC standardization efforts, providing practical guidance for
mitigating quantum-era risks while safeguarding long-term confidentiality and
integrity of network data.

</details>


### [270] [HHEML: Hybrid Homomorphic Encryption for Privacy-Preserving Machine Learning on Edge](https://arxiv.org/abs/2510.20243)
*Yu Hin Chan,Hao Yang,Shiyu Shen,Xingyu Fan,Shengzhe Lyu,Patrick S. Y. Hung,Ray C. C. Cheung*

Main category: cs.CR

TL;DR: 提出硬件加速的混合同态加密（HHE）架构用于隐私保护机器学习，集成到PPML管道，实验显示有性能提升。


<details>
  <summary>Details</summary>
Motivation: 全同态加密（FHE）在客户端有显著通信和计算开销，不适合边缘设备，混合同态加密（HHE）可解决此问题。

Method: 提出围绕轻量级对称密码的硬件加速HHE架构，进行微架构优化，并集成到完整PPML管道。

Result: 在PYNQ - Z2平台和MNIST数据集实验中，客户端加密延迟降低超50倍，硬件吞吐量提高近2倍。

Conclusion: 验证了低功耗、硬件加速HHE用于边缘部署的可行性，提供了资源受限环境下构建安全机器学习系统的软硬件协同设计方法。

Abstract: Privacy-preserving machine learning (PPML) is an emerging topic to handle
secure machine learning inference over sensitive data in untrusted
environments. Fully homomorphic encryption (FHE) enables computation directly
on encrypted data on the server side, making it a promising approach for PPML.
However, it introduces significant communication and computation overhead on
the client side, making it impractical for edge devices. Hybrid homomorphic
encryption (HHE) addresses this limitation by combining symmetric encryption
(SE) with FHE to reduce the computational cost on the client side, and
combining with an FHE-friendly SE can also lessen the processing overhead on
the server side, making it a more balanced and efficient alternative. Our work
proposes a hardware-accelerated HHE architecture built around a lightweight
symmetric cipher optimized for FHE compatibility and implemented as a dedicated
hardware accelerator. To the best of our knowledge, this is the first design to
integrate an end-to-end HHE framework with hardware acceleration. Beyond this,
we also present several microarchitectural optimizations to achieve higher
performance and energy efficiency. The proposed work is integrated into a full
PPML pipeline, enabling secure inference with significantly lower latency and
power consumption than software implementations. Our contributions validate the
feasibility of low-power, hardware- accelerated HHE for edge deployment and
provide a hardware- software co-design methodology for building scalable,
secure machine learning systems in resource-constrained environments.
Experiments on a PYNQ-Z2 platform with the MNIST dataset show over a 50x
reduction in client-side encryption latency and nearly a 2x gain in hardware
throughput compared to existing FPGA-based HHE accelerators.

</details>


### [271] [Policy-Governed RAG - Research Design Study](https://arxiv.org/abs/2510.19877)
*Jean-Marie Le Ray*

Main category: cs.CR

TL;DR: 提出政策驱动的RAG架构用于监管工作流的可审计生成，明确目标并说明适用领域和未来评估方向。


<details>
  <summary>Details</summary>
Motivation: 在受监管的工作流中实现可审计的生成，降低错误成本，满足法规对审计跟踪的要求。

Method: 构建由Contracts/Control、Manifests/Trails、Receipts/Verification组成的架构，对输出进行事前控制并绑定可验证证据，设置目标和使用NO - GO门进行预注册试点。

Result: 设计的架构使政策检查可审计、可重放且有回执支持。

Conclusion: 该架构可补充现有RAG/护栏机制，适用于多个领域，未来评估在未满足NO - GO门时可预承诺公布负面结果。

Abstract: A policy-governed RAG architecture is specified for audit-ready generation in
regulated workflows, organized as a triptych: (I) Contracts/Control
(SHRDLU-like), which governs output adherence to legal and internal policies;
(II) Manifests/Trails (Memex-like), which cryptographically anchors all cited
source evidence to ensure verifiable provenance; and (III)
Receipts/Verification (Xanadu-like), which provides the final, portable proof
of compliance for auditors (portable COSE/JOSE) (see Section 4 and Appendix A).
Rather than explaining model internals, outputs are gated ex-ante and bound to
cryptographically verifiable evidence for each material answer. Unvalidated
targets are stated (>=20% relative reduction in confident errors; p95 latency
<= 900 ms; <= 2.2x serve cost) together with a pre-registered (optional) pilot
using NO-GO gates. The design complements existing RAG/guardrails by making
policy checks auditable, replayable, and receipt-backed. Target domains include
back-office compliance in pharma, medical devices, finance, legal, and the
public sector where error costs may exceed thousands of euros and audit trails
are mandatory under regulations such as the EU AI Act. Future evaluations may
pre-commit to publishing negative results when any example NO-GO gate is not
met.

</details>


### [272] [SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment](https://arxiv.org/abs/2510.19979)
*Tushar Nayan,Ziqi Zhang,Ruimin Sun*

Main category: cs.CR

TL;DR: 文章提出SecureInfer框架，利用异构TEEs - GPU架构保障大语言模型执行安全，实验证明其有合理性能和强安全性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在移动和边缘平台部署增多，需防范模型提取攻击，同时要在不牺牲非可信AI加速器性能优势下保护模型隐私。

Method: 提出SecureInfer混合框架，采用信息论和威胁感知分区策略，将安全敏感组件在SGX飞地内执行，线性操作在GPU加密后执行并在飞地内恢复。

Result: 使用LLaMA - 2模型实现SecureInfer原型，评估显示其在性能和安全指标上表现良好。

Conclusion: SecureInfer能提供强安全保障且性能合理，是设备端安全模型推理的实用解决方案。

Abstract: With the increasing deployment of Large Language Models (LLMs) on mobile and
edge platforms, securing them against model extraction attacks has become a
pressing concern. However, protecting model privacy without sacrificing the
performance benefits of untrusted AI accelerators, such as GPUs, presents a
challenging trade-off. In this paper, we initiate the study of high-performance
execution on LLMs and present SecureInfer, a hybrid framework that leverages a
heterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate
privacy-critical components while offloading compute-intensive operations to
untrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts
an information-theoretic and threat-informed partitioning strategy:
security-sensitive components, including non-linear layers, projection of
attention head, FNN transformations, and LoRA adapters, are executed inside an
SGX enclave, while other linear operations (matrix multiplication) are
performed on the GPU after encryption and are securely restored within the
enclave. We implement a prototype of SecureInfer using the LLaMA-2 model and
evaluate it across performance and security metrics. Our results show that
SecureInfer offers strong security guarantees with reasonable performance,
offering a practical solution for secure on-device model inference.

</details>


### [273] [Learning to Triage Taint Flows Reported by Dynamic Program Analysis in Node.js Packages](https://arxiv.org/abs/2510.20739)
*Ronghao Ni,Aidan Z. H. Yang,Min-Chien Hsu,Nuno Sabino,Limin Jia,Ruben Martins,Darion Cassel,Kevin Cheang*

Main category: cs.CR

TL;DR: 本文研究机器学习用于程序分析工具漏洞报告优先级排序，聚焦Node.js包，评估多种模型，最佳模型在漏洞筛选上表现出色，有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 程序分析工具产生大量候选漏洞报告，需人工审查成本高，要解决安全分析师如何优先处理最可能为真实漏洞报告的问题。

Method: 聚焦Node.js包，收集含ACE或ACI漏洞的1883个Node.js包基准，基于动态程序分析工具输出数据，评估经典模型、图神经网络、大语言模型和混合模型等多种机器学习方法。

Result: 最佳大语言模型F1值达0.915，最佳图神经网络和经典机器学习模型F1值达0.904，领先模型在低假阴性率下可消除66.9%良性包人工审查，每包处理约60毫秒，调优后可检测99.2%可利用污点流。

Conclusion: 机器学习用于程序分析工具漏洞报告优先级排序有很强的实际漏洞筛选潜力。

Abstract: Program analysis tools often produce large volumes of candidate vulnerability
reports that require costly manual review, creating a practical challenge: how
can security analysts prioritize the reports most likely to be true
vulnerabilities?
  This paper investigates whether machine learning can be applied to
prioritizing vulnerabilities reported by program analysis tools. We focus on
Node.js packages and collect a benchmark of 1,883 Node.js packages, each
containing one reported ACE or ACI vulnerability. We evaluate a variety of
machine learning approaches, including classical models, graph neural networks
(GNNs), large language models (LLMs), and hybrid models that combine GNN and
LLMs, trained on data based on a dynamic program analysis tool's output. The
top LLM achieves $F_{1} {=} 0.915$, while the best GNN and classical ML models
reaching $F_{1} {=} 0.904$. At a less than 7% false-negative rate, the leading
model eliminates 66.9% of benign packages from manual review, taking around 60
ms per package. If the best model is tuned to operate at a precision level of
0.8 (i.e., allowing 20% false positives amongst all warnings), our approach can
detect 99.2% of exploitable taint flows while missing only 0.8%, demonstrating
strong potential for real-world vulnerability triage.

</details>


### [274] [CourtGuard: A Local, Multiagent Prompt Injection Classifier](https://arxiv.org/abs/2510.19844)
*Isaac Wu,Michael Maslowski*

Main category: cs.CR

TL;DR: 随着大语言模型用于敏感应用，提示注入攻击风险增加，提出CourtGuard多智能体提示注入分类器，虽整体检测效果一般但误报率低，凸显考虑不同场景重要性，代码开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于敏感应用时，提示注入攻击会导致泄露敏感数据等危害，需防御此类攻击。

Method: 提出CourtGuard，在类法庭多智能体大语言模型系统中评估提示，由“辩护律师”“检察官”模型辩论，“法官”模型给出最终分类。

Result: CourtGuard误报率低于Direct Detector，但总体上是较差的提示注入检测器。

Conclusion: 低误报率凸显分类提示时考虑对抗和良性场景的重要性，CourtGuard相对表现推动多智能体系统用于防御提示注入攻击。

Abstract: As large language models (LLMs) become integrated into various sensitive
applications, prompt injection, the use of prompting to induce harmful
behaviors from LLMs, poses an ever increasing risk. Prompt injection attacks
can cause LLMs to leak sensitive data, spread misinformation, and exhibit
harmful behaviors. To defend against these attacks, we propose CourtGuard, a
locally-runnable, multiagent prompt injection classifier. In it, prompts are
evaluated in a court-like multiagent LLM system, where a "defense attorney"
model argues the prompt is benign, a "prosecution attorney" model argues the
prompt is a prompt injection, and a "judge" model gives the final
classification. CourtGuard has a lower false positive rate than the Direct
Detector, an LLM as-a-judge. However, CourtGuard is generally a worse prompt
injection detector. Nevertheless, this lower false positive rate highlights the
importance of considering both adversarial and benign scenarios for the
classification of a prompt. Additionally, the relative performance of
CourtGuard in comparison to other prompt injection classifiers advances the use
of multiagent systems as a defense against prompt injection attacks. The
implementations of CourtGuard and the Direct Detector with full prompts for
Gemma-3-12b-it, Llama-3.3-8B, and Phi-4-mini-instruct are available at
https://github.com/isaacwu2000/CourtGuard.

</details>


### [275] [Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.19851)
*Artur Zolkowski,Wen Xing,David Lindner,Florian Tramèr,Erik Jenner*

Main category: cs.CR

TL;DR: 本文研究模型是否会混淆思维链以逃避检测，通过开发提示分类法评估，发现思维链监测在无混淆压力下有效，强压力下部分模型可逃避检测，且内部思维链混淆程度低于外部，稳健部署需针对模型进行压力测试。


<details>
  <summary>Details</summary>
Motivation: 探讨模型是否会混淆思维链以追求隐藏的对抗性目标并逃避检测，对思维链监测能力进行压力测试。

Method: 开发可组合和量化的提示分类法来引发思维链混淆，在玩具任务和SHADE - Arena更现实的环境中评估内部和外部思维链。

Result: （i）无混淆压力时思维链监测准确高效；（ii）强混淆压力下部分模型可完成对抗任务并逃避检测；（iii）在提示压力下，模型对内部思维链的混淆程度低于外部。

Conclusion: 思维链在良性环境中能提供有价值的监督，但稳健部署需要针对模型进行监测能力的压力测试。

Abstract: Recent findings suggest that misaligned models may exhibit deceptive
behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT)
is a promising tool for alignment monitoring: when models articulate their
reasoning faithfully, monitors can detect and mitigate harmful behaviors before
undesirable outcomes occur. However, a key uncertainty is: Can models obfuscate
their CoT in order to pursue hidden adversarial objectives while evading
detection? To answer this question and thus stress-test CoT monitorability, we
develop a composable and quantifiable taxonomy of prompts to elicit CoT
obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT
(prompted reasoning in outputs) using toy tasks and more realistic environments
in SHADE-Arena. We show that: (i) CoT monitoring performs accurately and
efficiently without obfuscation pressure. (ii) Under strong obfuscation
pressure, some models successfully complete adversarial tasks while evading
detection. (iii) Models do not obfuscate their internal CoT as much as their
external CoT (under prompt pressure). These results suggest that while CoT
provides valuable oversight in benign settings, robust deployment requires
model-specific stress-testing of monitorability.

</details>


### [276] [SAID: Empowering Large Language Models with Self-Activating Internal Defense](https://arxiv.org/abs/2510.20129)
*Yulong Chen,Yadong Liu,Jiawen Zhang,Mu Li,Chao Huang,Jie Wen*

Main category: cs.CR

TL;DR: 提出无训练防御范式SAID，利用大语言模型推理能力防御越狱攻击，实验表明其优于现有防御方法，能在减少有害输出同时保持模型性能和低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型越狱攻击防御策略缺乏泛化性、损害模型实用性且计算开销大，需新防御方法。

Method: 引入Self - Activating Internal Defense (SAID)，通过模型原生意图蒸馏、最优安全前缀探测和保守聚合策略三阶段流程，利用大语言模型自身推理能力识别和消除恶意意图。

Result: 在五个开源大语言模型和六种先进越狱攻击上的实验表明，SAID大幅优于现有防御方法，减少有害输出，同时保持良性任务性能和低计算开销。

Conclusion: 激活大语言模型的内在安全机制是构建更安全可靠对齐AI系统的更稳健和可扩展途径。

Abstract: Large Language Models (LLMs), despite advances in safety alignment, remain
vulnerable to jailbreak attacks designed to circumvent protective mechanisms.
Prevailing defense strategies rely on external interventions, such as input
filtering or output modification, which often lack generalizability and
compromise model utility while incurring significant computational overhead. In
this work, we introduce a new, training-free defense paradigm, Self-Activating
Internal Defense (SAID), which reframes the defense task from external
correction to internal capability activation. SAID uniquely leverages the LLM's
own reasoning abilities to proactively identify and neutralize malicious intent
through a three-stage pipeline: model-native intent distillation to extract
core semantics, optimal safety prefix probing to activate latent safety
awareness, and a conservative aggregation strategy to ensure robust
decision-making. Extensive experiments on five open-source LLMs against six
advanced jailbreak attacks demonstrate that SAID substantially outperforms
state-of-the-art defenses in reducing harmful outputs. Crucially, it achieves
this while preserving model performance on benign tasks and incurring minimal
computational overhead. Our work establishes that activating the intrinsic
safety mechanisms of LLMs is a more robust and scalable path toward building
safer and more reliable aligned AI systems.

</details>


### [277] [Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses](https://arxiv.org/abs/2510.20314)
*Wu Yichao,Wang Yirui,Ding Panpan,Wang Hailong,Zhu Bingqian,Liu Chun*

Main category: cs.CR

TL;DR: 本文探讨深度强化学习（DRL）在复杂环境的安全与鲁棒性问题，介绍框架、分析挑战，提出攻击分类框架，总结防御策略并探讨优缺点，最后展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着DRL在多领域广泛应用，提高其在动态环境的安全与鲁棒性，尤其是面对对抗攻击时的稳定性，成为核心研究问题。

Method: 介绍DRL基本框架，提出基于扰动类型和攻击目标的对抗攻击分类框架，详细回顾主流攻击方法，系统总结现有鲁棒训练策略并讨论优缺点。

Result: 清晰呈现DRL面临的安全挑战、攻击方法和防御策略，明确各防御方法的优缺点。

Conclusion: 强调DRL在对抗环境未来研究需提高泛化性、降低计算复杂度、增强可扩展性和可解释性。

Abstract: With the wide application of deep reinforcement learning (DRL) techniques in
complex fields such as autonomous driving, intelligent manufacturing, and smart
healthcare, how to improve its security and robustness in dynamic and
changeable environments has become a core issue in current research. Especially
in the face of adversarial attacks, DRL may suffer serious performance
degradation or even make potentially dangerous decisions, so it is crucial to
ensure their stability in security-sensitive scenarios. In this paper, we first
introduce the basic framework of DRL and analyze the main security challenges
faced in complex and changing environments. In addition, this paper proposes an
adversarial attack classification framework based on perturbation type and
attack target and reviews the mainstream adversarial attack methods against DRL
in detail, including various attack methods such as perturbation state space,
action space, reward function and model space. To effectively counter the
attacks, this paper systematically summarizes various current robustness
training strategies, including adversarial training, competitive training,
robust learning, adversarial detection, defense distillation and other related
defense techniques, we also discuss the advantages and shortcomings of these
methods in improving the robustness of DRL. Finally, this paper looks into the
future research direction of DRL in adversarial environments, emphasizing the
research needs in terms of improving generalization, reducing computational
complexity, and enhancing scalability and explainability, aiming to provide
valuable references and directions for researchers.

</details>


### [278] [GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?](https://arxiv.org/abs/2510.20333)
*Chiyu Chen,Xinhao Song,Yunkai Chai,Yang Yao,Haodong Zhao,Lijun Li,Jie Li,Yan Teng,Gongshen Liu,Yingchun Wang*

Main category: cs.CR

TL;DR: 引入GhostEI - Bench评估移动代理在环境注入攻击下的表现，发现当前模型易受欺骗性环境线索影响，该基准为量化和缓解威胁提供框架。


<details>
  <summary>Details</summary>
Motivation: 视觉 - 语言模型在移动GUI中面临环境注入这一未充分探索的威胁，需系统评估该威胁。

Method: 引入GhostEI - Bench，在安卓模拟器中注入对抗性事件并评估，提出judge - LLM协议进行细粒度故障分析。

Result: 实验表明当前最先进的代理模型对欺骗性环境线索有明显的脆弱性，无法感知和推理被操纵的UI。

Conclusion: GhostEI - Bench为量化和缓解新兴威胁提供框架，有助于开发更强大和安全的具身代理。

Abstract: Vision-Language Models (VLMs) are increasingly deployed as autonomous agents
to navigate mobile graphical user interfaces (GUIs). Operating in dynamic
on-device ecosystems, which include notifications, pop-ups, and inter-app
interactions, exposes them to a unique and underexplored threat vector:
environmental injection. Unlike prompt-based attacks that manipulate textual
instructions, environmental injection corrupts an agent's visual perception by
inserting adversarial UI elements (for example, deceptive overlays or spoofed
notifications) directly into the GUI. This bypasses textual safeguards and can
derail execution, causing privacy leakage, financial loss, or irreversible
device compromise. To systematically evaluate this threat, we introduce
GhostEI-Bench, the first benchmark for assessing mobile agents under
environmental injection attacks within dynamic, executable environments. Moving
beyond static image-based assessments, GhostEI-Bench injects adversarial events
into realistic application workflows inside fully operational Android emulators
and evaluates performance across critical risk scenarios. We further propose a
judge-LLM protocol that conducts fine-grained failure analysis by reviewing the
agent's action trajectory alongside the corresponding screenshot sequence,
pinpointing failure in perception, recognition, or reasoning. Comprehensive
experiments on state-of-the-art agents reveal pronounced vulnerability to
deceptive environmental cues: current models systematically fail to perceive
and reason about manipulated UIs. GhostEI-Bench provides a framework for
quantifying and mitigating this emerging threat, paving the way toward more
robust and secure embodied agents.

</details>


### [279] [Deep Sequence-to-Sequence Models for GNSS Spoofing Detection](https://arxiv.org/abs/2510.19890)
*Jan Zelinka,Oliver Kost,Marek Hrúz*

Main category: cs.CR

TL;DR: 提出数据生成框架模拟欺骗攻击，用深度学习模型检测，Transformer 架构效果佳，错误率 0.16%。


<details>
  <summary>Details</summary>
Motivation: 模拟欺骗攻击并实现有效检测。

Method: 构建数据生成框架模拟攻击，运用基于长短期记忆网络和受 Transformer 启发架构的深度学习模型进行在线检测。

Result: 深度学习模型能准确区分欺骗信号和真实信号，受 Transformer 启发架构且早期融合输入的模型效果最佳，错误率 0.16%。

Conclusion: 深度学习模型在欺骗信号检测中能取得高检测性能。

Abstract: We present a data generation framework designed to simulate spoofing attacks
and randomly place attack scenarios worldwide. We apply deep neural
network-based models for spoofing detection, utilizing Long Short-Term Memory
networks and Transformer-inspired architectures. These models are specifically
designed for online detection and are trained using the generated dataset. Our
results demonstrate that deep learning models can accurately distinguish
spoofed signals from genuine ones, achieving high detection performance. The
best results are achieved by Transformer-inspired architectures with early
fusion of the inputs resulting in an error rate of 0.16%.

</details>


### [280] [AdaDoS: Adaptive DoS Attack via Deep Adversarial Reinforcement Learning in SDN](https://arxiv.org/abs/2510.20566)
*Wei Shao,Yuhao Wang,Rongguang He,Muhammad Ejaz Ahmed,Seyit Camtepe*

Main category: cs.CR

TL;DR: 本文提出自适应攻击模型AdaDoS，利用对抗强化学习扰乱网络并躲避检测，还通过互惠学习模块应对信息不足问题，是首次将强化学习用于开发类DoS攻击序列。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制在应对AI驱动技术带来的SDN安全新挑战时效果不佳，需新攻击模型测试其有效性。

Method: 将问题建模为攻击者与检测器的竞争游戏，用对抗强化学习动态调整攻击策略；把类DoS攻击建模为部分可观测马尔可夫决策过程，用互惠学习模块解决信息不足问题。

Result: 提出了AdaDoS模型，能自适应躲避基于机器学习和基于规则的类DoS攻击检测器。

Conclusion: AdaDoS是首次将强化学习用于开发类DoS攻击序列，可用于评估现有SDN防御机制的有效性。

Abstract: Existing defence mechanisms have demonstrated significant effectiveness in
mitigating rule-based Denial-of-Service (DoS) attacks, leveraging predefined
signatures and static heuristics to identify and block malicious traffic.
However, the emergence of AI-driven techniques presents new challenges to SDN
security, potentially compromising the efficacy of existing defence mechanisms.
In this paper, we introduce~AdaDoS, an adaptive attack model that disrupt
network operations while evading detection by existing DoS-based detectors
through adversarial reinforcement learning (RL). Specifically, AdaDoS models
the problem as a competitive game between an attacker, whose goal is to
obstruct network traffic without being detected, and a detector, which aims to
identify malicious traffic. AdaDoS can solve this game by dynamically adjusting
its attack strategy based on feedback from the SDN and the detector.
Additionally, recognising that attackers typically have less information than
defenders, AdaDoS formulates the DoS-like attack as a partially observed Markov
decision process (POMDP), with the attacker having access only to delay
information between attacker and victim nodes. We address this challenge with a
novel reciprocal learning module, where the student agent, with limited
observations, enhances its performance by learning from the teacher agent, who
has full observational capabilities in the SDN environment. AdaDoS represents
the first application of RL to develop DoS-like attack sequences, capable of
adaptively evading both machine learning-based and rule-based DoS-like attack
detectors.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [281] [ComProScanner: A multi-agent based framework for composition-property structured data extraction from scientific literature](https://arxiv.org/abs/2510.20362)
*Aritra Roy,Enrico Grisan,John Buckeridge,Chiara Gattinoni*

Main category: physics.comp-ph

TL;DR: 文章介绍开发ComProScanner平台用于提取、验证、分类和可视化化学组成及性质数据，评估框架在提取陶瓷压电材料数据上表现，DeepSeek - V3 - 0324表现最佳，该框架方便构建数据集。


<details>
  <summary>Details</summary>
Motivation: 现有从科学文献提取数据构建、验证和可视化数据集的自动化工具稀缺，且缺乏陶瓷压电材料的大型数据集。

Method: 开发ComProScanner自主多智能体平台，用100篇期刊文章对10种不同大语言模型进行评估。

Result: DeepSeek - V3 - 0324表现最佳，整体准确率达0.82。

Conclusion: 该框架是一个简单、用户友好、易于使用的工具包，可用于提取文献中的复杂实验数据以构建机器学习或深度学习数据集。

Abstract: Since the advent of various pre-trained large language models, extracting
structured knowledge from scientific text has experienced a revolutionary
change compared with traditional machine learning or natural language
processing techniques. Despite these advances, accessible automated tools that
allow users to construct, validate, and visualise datasets from scientific
literature extraction remain scarce. We therefore developed ComProScanner, an
autonomous multi-agent platform that facilitates the extraction, validation,
classification, and visualisation of machine-readable chemical compositions and
properties, integrated with synthesis data from journal articles for
comprehensive database creation. We evaluated our framework using 100 journal
articles against 10 different LLMs, including both open-source and proprietary
models, to extract highly complex compositions associated with ceramic
piezoelectric materials and corresponding piezoelectric strain coefficients
(d33), motivated by the lack of a large dataset for such materials.
DeepSeek-V3-0324 outperformed all models with a significant overall accuracy of
0.82. This framework provides a simple, user-friendly, readily-usable package
for extracting highly complex experimental data buried in the literature to
build machine learning or deep learning datasets.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [282] [Learning Coupled Earth System Dynamics with GraphDOP](https://arxiv.org/abs/2510.20416)
*Eulalie Boucher,Mihai Alexe,Peter Lean,Ewan Pinnington,Simon Lang,Patrick Laloyaux,Lorenzo Zampieri,Patricia de Rosnay,Niels Bormann,Anthony McNally*

Main category: physics.ao-ph

TL;DR: 介绍GraphDOP模型可直接从地球系统观测数据学习预测天气，通过案例展示其捕捉跨组件交互的能力，为数据驱动的地球系统预测提供新路径。


<details>
  <summary>Details</summary>
Motivation: 准确表征地球系统不同组件间的耦合相互作用是天气预报的重大挑战，传统数值天气预报系统有局限性。

Method: 使用基于图的机器学习模型GraphDOP，直接从原始卫星和实地观测数据学习，将不同观测源信息嵌入共享潜空间。

Result: 通过北极海冰快速冻结、飓风期间海洋表面冷却和欧洲热浪等案例，展示GraphDOP能成功表征和传播跨组件相互作用。

Conclusion: 直接从地球系统观测数据学习可通过单一模型实现物理上一致的端到端数据驱动的地球系统预测，是有前景的方向。

Abstract: Interactions between different components of the Earth System (e.g. ocean,
atmosphere, land and cryosphere) are a crucial driver of global weather
patterns. Modern Numerical Weather Prediction (NWP) systems typically run
separate models of the different components, explicitly coupled across their
interfaces to additionally model exchanges between the different components.
Accurately representing these coupled interactions remains a major scientific
and technical challenge of weather forecasting. GraphDOP is a graph-based
machine learning model that learns to forecast weather directly from raw
satellite and in-situ observations, without reliance on reanalysis products or
traditional physics-based NWP models. GraphDOP simultaneously embeds
information from diverse observation sources spanning the full Earth system
into a shared latent space. This enables predictions that implicitly capture
cross-domain interactions in a single model without the need for any explicit
coupling. Here we present a selection of case studies which illustrate the
capability of GraphDOP to forecast events where coupled processes play a
particularly key role. These include rapid sea-ice freezing in the Arctic,
mixing-induced ocean surface cooling during Hurricane Ian and the severe
European heat wave of 2022. The results suggest that learning directly from
Earth System observations can successfully characterise and propagate
cross-component interactions, offering a promising path towards physically
consistent end-to-end data-driven Earth System prediction with a single model.

</details>


### [283] [CSU-PCAST: A Dual-Branch Transformer Framework for medium-range ensemble Precipitation Forecasting](https://arxiv.org/abs/2510.20769)
*Tianyi Xiong,Haonan Chen*

Main category: physics.ao-ph

TL;DR: 本文开发基于深度学习的多步降水预测集成框架，在多降水阈值上表现优于GEFS。


<details>
  <summary>Details</summary>
Motivation: 准确的中期降水预报对气象风险管理和减灾至关重要，但当前数值天气预报系统面临挑战，传统集合系统难以保持高技巧。

Method: 通过联合建模一组大气变量开发深度学习集成框架，使用ERA5再分析数据训练，采用基于块的Swin Transformer骨干网络，最小化混合损失，推理时摄入实时GFS初始条件自回归生成预报。

Result: 与GEFS相比，在0.1mm、1mm、10mm和20mm降水阈值上临界成功指数（CSI）得分更高。

Conclusion: 该框架提高了中到大雨的预报性能。

Abstract: Accurate medium-range precipitation forecasting is crucial for
hydrometeorological risk management and disaster mitigation, yet remains
challenging for current numerical weather prediction (NWP) systems. Traditional
ensemble systems such as the Global Ensemble Forecast System (GEFS) struggle to
maintain high skill, especially for moderate and heavy rainfall at extended
lead times. This study develops a deep learning-based ensemble framework for
multi-step precipitation prediction through joint modeling of a comprehensive
set of atmospheric variables. The model is trained on ERA5 reanalysis data at
0.25$^{\circ}$ spatial resolution, with precipitation labels from NASA's
Integrated Multi-satellite Retrievals for Global Precipitation Measurement
(GPM) constellation (IMERG), incorporating 57 input variables, including
upper-air and surface predictors. The architecture employs a patch-based Swin
Transformer backbone with periodic convolutions to handle longitudinal
continuity and integrates time and noise embeddings through conditional layer
normalization. A dual-branch decoder predicts total precipitation and other
variables, with targeted freezing of encoder-decoder pathways for specialized
training. Training minimizes a hybrid loss combining the Continuous Ranked
Probability Score (CRPS) and weighted log1p mean squared error (log1pMSE),
balancing probabilistic accuracy and magnitude fidelity. During inference, the
model ingests real-time Global Forecast System (GFS) initial conditions to
generate 15-day forecasts autoregressively. Evaluation against GEFS using IMERG
data demonstrates higher Critical Success Index (CSI) scores at precipitation
thresholds of 0.1 mm, 1 mm, 10 mm, and 20 mm, highlighting improved performance
for moderate to heavy rainfall.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [284] [ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering](https://arxiv.org/abs/2510.20036)
*Marianne Menglin Liu,Daniel Garcia,Fjona Parllaku,Vikas Upadhyay,Syed Fahad Allam Shah,Dan Roth*

Main category: cs.CL

TL;DR: 提出ToolScope解决大语言模型工具选择冗余和上下文限制问题，评估显示提升了工具选择准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖外部工具时，现实工具集存在冗余、名称和描述重叠，且模型有上下文限制，影响工具选择准确性。

Method: 提出ToolScope，包括带自动修正的ToolScopeMerger自动审核和修复工具合并以减少冗余，以及ToolScopeRetriever对工具排序并选择最相关工具以压缩工具集。

Result: 在三种先进大语言模型和三个开源工具使用基准上评估，工具选择准确率提升8.38%至38.6%。

Conclusion: ToolScope能有效提升大语言模型的工具使用效果。

Abstract: Large language model (LLM) agents rely on external tools to solve complex
tasks, but real-world toolsets often contain redundant tools with overlapping
names and descriptions, introducing ambiguity and reducing selection accuracy.
LLMs also face strict input context limits, preventing efficient consideration
of large toolsets. To address these challenges, we propose ToolScope, which
includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and
fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and
select only the most relevant tools for each query, compressing toolsets to fit
within context limits without sacrificing accuracy. Evaluations on three
state-of-the-art LLMs and three open-source tool-use benchmarks show gains of
8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's
effectiveness in enhancing LLM tool use.

</details>


### [285] [An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics](https://arxiv.org/abs/2510.19866)
*Xincheng Liu*

Main category: cs.CL

TL;DR: 研究评估五种大语言模型和三种提示框架生成的高中物理课程计划，分析发现模型影响可读性，提示框架影响准确性和课程对齐，最佳配置是结合易读模型、RACE框架和明确清单。


<details>
  <summary>Details</summary>
Motivation: 评估AI生成课程计划的教学合理性和可用性。

Method: 选用五种大语言模型和三种提示框架生成高中物理《电磁频谱》课程计划，用四个计算指标分析。

Result: 模型对语言可及性影响最大，DeepSeek生成最易读计划，Claude语言最复杂；提示框架对事实准确性和教学完整性影响最大，RACE框架幻觉指数最低、与课程标准对齐度最高；学习目标多集中在布鲁姆分类法的记忆和理解层次。

Conclusion: 可读性主要由模型设计决定，教学可靠性和课程对齐更多依赖提示框架，最佳配置是结合易读模型、RACE框架和明确清单。

Abstract: This study evaluates the pedagogical soundness and usability of AI-generated
lesson plans across five leading large language models: ChatGPT (GPT-5), Claude
Sonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Beyond model choice,
three structured prompt frameworks were tested: TAG (Task, Audience, Goal),
RACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective,
Style, Tone, Audience, Response Format).
  Fifteen lesson plans were generated for a single high-school physics topic,
The Electromagnetic Spectrum. The lesson plans were analyzed through four
automated computational metrics: (1) readability and linguistic complexity, (2)
factual accuracy and hallucination detection, (3) standards and curriculum
alignment, and (4) cognitive demand of learning objectives.
  Results indicate that model selection exerted the strongest influence on
linguistic accessibility, with DeepSeek producing the most readable teaching
plan (FKGL = 8.64) and Claude generating the densest language (FKGL = 19.89).
  The prompt framework structure most strongly affected the factual accuracy
and pedagogical completeness, with the RACE framework yielding the lowest
hallucination index and the highest incidental alignment with NGSS curriculum
standards. Across all models, the learning objectives in the fifteen lesson
plans clustered at the Remember and Understand tiers of Bloom's taxonomy. There
were limited higher-order verbs in the learning objectives extracted.
  Overall, the findings suggest that readability is significantly governed by
model design, while instructional reliability and curricular alignment depend
more on the prompt framework. The most effective configuration for lesson plans
identified in the results was to combine a readability-optimized model with the
RACE framework and an explicit checklist of physics concepts, curriculum
standards, and higher-order objectives.

</details>


### [286] [Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention](https://arxiv.org/abs/2510.19875)
*J Rosser,José Luis Redondo García,Gustavo Penha,Konstantina Palla,Hugues Bouchard*

Main category: cs.CL

TL;DR: 提出Sparse Tracing技术和Stream算法，高效分析长上下文注意力模式，减少内存需求，使长上下文可解释性在消费级GPU上可行。


<details>
  <summary>Details</summary>
Motivation: 传统机械可解释性技术分析注意力时，内存需求随上下文长度二次增长，超出100,000个token需TB级内存。

Method: 引入Sparse Tracing技术，提出Stream编译式分层剪枝算法，以近线性时间和线性空间估计每头稀疏注意力掩码。

Result: 在长思维链推理跟踪中，剪枝97 - 99%的token交互；在RULER基准测试中，丢弃90 - 96%的交互，暴露从输入到输出的分层路径。

Conclusion: 该方法是分析注意力模式和追踪信息流的实用工具，有助于实现思维链监测的民主化。

Abstract: As Large Language Models (LLMs) scale to million-token contexts, traditional
Mechanistic Interpretability techniques for analyzing attention scale
quadratically with context length, demanding terabytes of memory beyond 100,000
tokens. We introduce Sparse Tracing, a novel technique that leverages dynamic
sparse attention to efficiently analyze long context attention patterns. We
present Stream, a compilable hierarchical pruning algorithm that estimates
per-head sparse attention masks in near-linear time $O(T \log T)$ and linear
space $O(T)$, enabling one-pass interpretability at scale. Stream performs a
binary-search-style refinement to retain only the top-$k$ key blocks per query
while preserving the model's next-token behavior. We apply Stream to long
chain-of-thought reasoning traces and identify thought anchors while pruning
97-99\% of token interactions. On the RULER benchmark, Stream preserves
critical retrieval paths while discarding 90-96\% of interactions and exposes
layer-wise routes from the needle to output. Our method offers a practical
drop-in tool for analyzing attention patterns and tracing information flow
without terabytes of caches. By making long context interpretability feasible
on consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.
Code is available at https://anonymous.4open.science/r/stream-03B8/.

</details>


### [287] [Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities](https://arxiv.org/abs/2510.19892)
*Nishant Balepur,Dang Nguyen,Dayeon Ki*

Main category: cs.CL

TL;DR: 本文提出基于游戏的评估方法来全面评估多模态大语言模型（MLMs）能力，以Dixit游戏为例进行实验，揭示了MLM推理的改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法联合评估MLM能力或主观性强、成本高且易被模型利用捷径，因此需要新的评估方法。

Method: 提出基于游戏的评估方法，具体通过Dixit游戏进行评估。

Result: Dixit游戏胜率排名与流行的MLM基准排名完全相关，揭示了人类与MLM玩家策略差异和MLM推理改进方向。

Conclusion: 基于游戏的评估为全面评估MLM能力提供了稳健框架。

Abstract: Multi-modal large language models (MLMs) are often assessed on static,
individual benchmarks -- which cannot jointly assess MLM capabilities in a
single task -- or rely on human or model pairwise comparisons -- which is
highly subjective, expensive, and allows models to exploit superficial
shortcuts (e.g., verbosity) to inflate their win-rates. To overcome these
issues, we propose game-based evaluations to holistically assess MLM
capabilities. Games require multiple abilities for players to win, are
inherently competitive, and are governed by fix, objective rules, and makes
evaluation more engaging, providing a robust framework to address the
aforementioned challenges. We manifest this evaluation specifically through
Dixit, a fantasy card game where players must generate captions for a card that
trick some, but not all players, into selecting the played card. Our
quantitative experiments with five MLMs show Dixit win-rate rankings are
perfectly correlated with those on popular MLM benchmarks, while games between
human and MLM players in Dixit reveal several differences between agent
strategies and areas of improvement for MLM reasoning.

</details>


### [288] [Large Language Model enabled Mathematical Modeling](https://arxiv.org/abs/2510.19895)
*Guoyun Zhang*

Main category: cs.CL

TL;DR: 研究探索用DeepSeek - R1模型弥合大语言模型与运筹学优化建模间的表述差距，对其在四个关键运筹学基准测试中评估并采用多种策略减少幻觉、提升准确性。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法依赖领域专家将实际问题转化为数学模型，现有大语言模型存在高成本和易产生幻觉问题，DeepSeek - R1在运筹学应用场景效果待探索。

Method: 在四个关键运筹学基准测试中对DeepSeek - R1进行评估，采用基线评估、开发幻觉分类法及应用LLM - as - a - Judge、少样本学习、工具调用和多智能体框架等缓解策略。

Result: 未提及明确结果。

Conclusion: 未提及明确结论。

Abstract: The integration of Large Language Models (LLMs) with optimization modeling
offers a promising avenue for advancing decision-making in operations research
(OR). Traditional optimization methods,such as linear programming, mixed
integer programming, and simulation depend heavily on domain expertise to
translate real-world problems into solvable mathematical models. While solvers
like Gurobi and COPT are powerful, expert input remains essential for defining
objectives, constraints, and variables. This research investigates the
potential of LLMs, specifically the DeepSeek-R1 model, to bridge this
formulation gap using natural language understanding and code generation.
Although prior models like GPT-4, Claude, and Bard have shown strong
performance in NLP and reasoning tasks, their high token costs and tendency
toward hallucinations limit real-world applicability in supply chain contexts.
In contrast, DeepSeek-R1, a cost-efficient and high-performing model trained
with reinforcement learning, presents a viable alternative. Despite its success
in benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied
OR scenarios remains under explored. This study systematically evaluates
DeepSeek-R1 across four key OR benchmarks: NL4OPT, IndustryOR, EasyLP, and
ComplexOR. Our methodology includes baseline assessments, the development of a
hallucination taxonomy, and the application of mitigation strategies like
LLM-as-a-Judge, Few-shot Learning (FSL), Tool Calling, and a Multi-agent
Framework. These techniques aim to reduce hallucinations, enhance formulation
accuracy, and better align model outputs with user intent.

</details>


### [289] [Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation](https://arxiv.org/abs/2510.19897)
*Jackson Hassell,Dan Zhang,Hannah Kim,Tom Mitchell,Estevam Hruschka*

Main category: cs.CL

TL;DR: 研究基于预训练大语言模型的智能体在不更新参数下学习目标分类函数，提出记忆增强框架，有准确率提升，发现模型行为差异并引入新指标。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法成本高、缺乏灵活性且不透明，需探索不更新参数学习目标分类函数的方法。

Method: 提出记忆增强框架，利用情景记忆存储实例级反馈，语义记忆提炼任务级指导，引入新指标可暗示性。

Result: 在不同任务中，结合反馈比仅依赖标签的基线准确率提高达24.8%，发现不同模型处理不同数据的行为差异。

Conclusion: 基于记忆的反思学习在构建更具适应性和可解释性的大语言模型智能体方面有前景。

Abstract: We investigate how agents built on pretrained large language models can learn
target classification functions from labeled examples without parameter
updates. While conventional approaches like fine-tuning are often costly,
inflexible, and opaque, we propose a memory-augmented framework that leverages
both labeled data and LLM-generated critiques. Our framework uses episodic
memory to store instance-level critiques-capturing specific past
experiences-and semantic memory to distill these into reusable, task-level
guidance. Across a diverse set of tasks, incorporating critiques yields up to a
24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines
that rely only on labels. Through extensive empirical evaluation, we uncover
distinct behavioral differences between OpenAI and opensource models,
particularly in how they handle fact-oriented versus preference-based data. To
interpret how models respond to different representations of supervision
encoded in memory, we introduce a novel metric, suggestibility. This helps
explain observed behaviors and illuminates how model characteristics and memory
strategies jointly shape learning dynamics. Our findings highlight the promise
of memory-driven, reflective learning for building more adaptive and
interpretable LLM agents.

</details>


### [290] [LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation](https://arxiv.org/abs/2510.19967)
*Le Ren,Xiangjian Zeng,Qingqiang Wu,Ruoxuan Liang*

Main category: cs.CL

TL;DR: 提出无监督可控歌词翻译框架LyriCAR，实验效果佳，自适应策略减少训练步骤。


<details>
  <summary>Details</summary>
Motivation: 现有歌词翻译方法依赖手工规则和句子级建模，难以内化音乐 - 语言模式和在段落级有效泛化。

Method: 提出LyriCAR框架，引入难度感知课程设计器和自适应课程策略。

Result: 在EN - ZH歌词翻译任务上取得了最先进的结果，自适应策略减少近40%训练步骤且性能优越。

Conclusion: LyriCAR框架有效，能提高歌词翻译质量。

Abstract: Lyric translation is a challenging task that requires balancing multiple
musical constraints. Existing methods often rely on hand-crafted rules and
sentence-level modeling, which restrict their ability to internalize
musical-linguistic patterns and to generalize effectively at the paragraph
level, where cross-line coherence and global rhyme are crucial. In this work,
we propose LyriCAR, a novel framework for controllable lyric translation that
operates in a fully unsupervised manner. LyriCAR introduces a difficulty-aware
curriculum designer and an adaptive curriculum strategy, ensuring efficient
allocation of training resources, accelerating convergence, and improving
overall translation quality by guiding the model with increasingly complex
challenges. Extensive experiments on the EN-ZH lyric translation task show that
LyriCAR achieves state-of-the-art results across both standard translation
metrics and multi-dimensional reward scores, surpassing strong baselines.
Notably, the adaptive curriculum strategy reduces training steps by nearly 40%
while maintaining superior performance. Code, data and model can be accessed at
https://github.com/rle27/LyriCAR.

</details>


### [291] [LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation](https://arxiv.org/abs/2510.19988)
*Xin Lian,Kenneth D. Forbus*

Main category: cs.CL

TL;DR: 本文提出结合大语言模型与符号自然语言理解系统的混合方法，在常识科学文本任务中效果优于纯符号方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言理解任务中易出错，符号自然语言理解系统覆盖范围有限，探索结合两者优势的方法。

Method: 使用大语言模型进行文本改写和简化以提供广泛覆盖，用符号自然语言理解系统生成用于推理和增量学习的表示。

Result: 混合方法在提取和解释常识科学文本中的数量和因果定律任务上，效果显著优于纯符号管道。

Conclusion: 混合方法结合了大语言模型和符号自然语言理解系统的优势，是一种有效的自然语言处理方式。

Abstract: Despite the broad applicability of large language models (LLMs), their
reliance on probabilistic inference makes them vulnerable to errors such as
hallucination in generated facts and inconsistent output structure in natural
language understanding (NLU) tasks. By contrast, symbolic NLU systems provide
interpretable understanding grounded in curated lexicons, semantic resources,
and syntactic & semantic interpretation rules. They produce relational
representations that can be used for accurate reasoning and planning, as well
as incremental debuggable learning. However, symbolic NLU systems tend to be
more limited in coverage than LLMs and require scarce knowledge representation
and linguistics skills to extend and maintain. This paper explores a hybrid
approach that integrates the broad-coverage language processing of LLMs with
the symbolic NLU capabilities of producing structured relational
representations to hopefully get the best of both approaches. We use LLMs for
rephrasing and text simplification, to provide broad coverage, and as a source
of information to fill in knowledge gaps more automatically. We use symbolic
NLU to produce representations that can be used for reasoning and for
incremental learning. We evaluate this approach on the task of extracting and
interpreting quantities and causal laws from commonsense science texts, along
with symbolic- and LLM-only pipelines. Our results suggest that our hybrid
method works significantly better than the symbolic-only pipeline.

</details>


### [292] [Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs](https://arxiv.org/abs/2510.20001)
*Yunpeng Xiao,Carl Yang,Mark Mai,Xiao Hu,Kai Shu*

Main category: cs.CL

TL;DR: 提出统一范式表征临床决策任务，总结现有数据集设置、应对方法，扩展评估维度并指出挑战，助力有临床意义的大语言模型发展。


<details>
  <summary>Details</summary>
Motivation: 现有医疗数据集简化问答，不能充分代表现实临床决策，需更好评估大语言模型临床应用。

Method: 提出从临床背景和临床问题两个维度表征临床决策任务的范式，总结现有数据集设置，回顾应对方法，扩展评估维度。

Result: 明确范式可澄清假设、规范比较。

Conclusion: 该范式能指导有临床意义的大语言模型的发展。

Abstract: Large language models (LLMs) show promise for clinical use. They are often
evaluated using datasets such as MedQA. However, Many medical datasets, such as
MedQA, rely on simplified Question-Answering (Q\A) that underrepresents
real-world clinical decision-making. Based on this, we propose a unifying
paradigm that characterizes clinical decision-making tasks along two
dimensions: Clinical Backgrounds and Clinical Questions. As the background and
questions approach the real clinical environment, the difficulty increases. We
summarize the settings of existing datasets and benchmarks along two
dimensions. Then we review methods to address clinical decision-making,
including training-time and test-time techniques, and summarize when they help.
Next, we extend evaluation beyond accuracy to include efficiency,
explainability. Finally, we highlight open challenges. Our paradigm clarifies
assumptions, standardizes comparisons, and guides the development of clinically
meaningful LLMs.

</details>


### [293] [Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation and Specialized Pre-training](https://arxiv.org/abs/2510.20002)
*Alexandra Apostolopoulou,Konstantinos Kanaris,Athanasios Koursaris,Dimitris Tsakalidis,George Domalis,Ioannis E. Livieris*

Main category: cs.CL

TL;DR: 本文针对现代希腊语自然语言处理面临的问题，构建希腊嵌入模型，经实验证明新模型有效且优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现代希腊语自然语言处理研究分散、架构单一、模型上下文长度有限，尤其在法律领域现有模型难以分析长文档，需改进。

Method: 构建多个大规模希腊语语料库，采用严格的质量过滤和预处理方法，在精心策划的数据基础上预训练并评估ELECTRA、ConvBERT和ModernBERT等现代架构，还提出首个法律领域的希腊 - 英语双语嵌入模型。

Result: 下游任务的广泛实验表明，新模型有效，GEM - RoBERTa和GEM - ConvBERT模型显著优于现有基线。

Conclusion: 提出的构建希腊嵌入模型的方法是有效的，新模型提升了现代希腊语自然语言处理的性能。

Abstract: The advancement of natural language processing for morphologically rich,
moderately-resourced languages like Modern Greek is often hindered by a
fragmented research landscape, a lack of architectural diversity and reliance
on limited context-length models. This is particularly true in specialized,
high-value domains such as law, where existing models are frequently confined
to early transformer architectures with a restrictive 512-token window,
insufficient for analyzing long legal documents. To address these challenges,
this paper presents Greek Embedding Models, a new family of transformer models
for Greek language built upon a foundation of extensive, quality-driven data
curation. We detail the construction of several large-scale Greek corpora,
emphasizing a rigorous, quality-based filtering and preprocessing methodology
to create high-value training datasets from both general-domain and specialized
legal sources. On this carefully curated foundation, we pre-train and
systematically evaluate a diverse suite of modern architectures, which has not
previously applied to Greek language, such as ELECTRA, ConvBERT and ModernBERT.
Furthermore, we propose the first bilingual Greek-English Embedding Models
tailored for the legal domain. The extensive experiments on downstream tasks
demonstrate that the new class of models establish the effectiveness of the
proposed approach, highlighting that the GEM-RoBERTa and GEM-ConvBERT models
significantly outperform existing baselines.

</details>


### [294] [CreativityPrism: A Holistic Benchmark for Large Language Model Creativity](https://arxiv.org/abs/2510.20091)
*Zhaoyi Joey Hou,Bowei Alvin Zhang,Yining Lu,Bhiman Kumar Baghel,Anneliese Brei,Ximing Lu,Meng Jiang,Faeze Brahman,Snigdha Chaturvedi,Haw-Shiuan Chang,Daniel Khashabi,Xiang Lorraine Li*

Main category: cs.CL

TL;DR: 提出CreativityPrism框架评估大语言模型创造力，评估17个模型，发现专有和开源模型有差距，不同领域和维度表现相关性不同，强调全面评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有评估大语言模型创造力的方法碎片化，缺乏整体框架。

Method: 提出CreativityPrism框架，将创造力分解为质量、新颖性和多样性三个维度，包含九个任务、三个领域和二十个评估指标。

Result: 专有和开源模型存在显著差距；同一领域内任务表现相关性高，不同领域相关性低；多样性和质量指标相关性强，新颖性与二者相关性弱。

Conclusion: 单一创造力任务或维度表现好未必能推广到其他方面，需要对大语言模型创造力进行整体评估。

Abstract: Creativity is often seen as a hallmark of human intelligence. While large
language models (LLMs) are increasingly perceived as producing creative text,
there is still no holistic framework to evaluate their creativity across
diverse scenarios. Existing evaluation methods remain fragmented, with dramatic
variation across domains and tasks, largely due to differing definitions and
measurements of creativity. Inspired by the hypothesis that creativity is not
one fixed idea, we propose CreativityPrism, an evaluation analysis framework
that decomposes creativity into three dimensions: quality, novelty, and
diversity. CreativityPrism incorporates nine tasks, three domains, i.e.,
divergent thinking, creative writing, and logical reasoning, and twenty
evaluation metrics, which measure each dimension in task-specific, unique ways.
We evaluate 17 state-of-the-art (SoTA) proprietary and open-sourced LLMs on
CreativityPrism and analyze the performance correlations among different
metrics and task domains. Our results reveal a notable gap between proprietary
and open-source models. Overall, model performance tends to be highly
correlated across tasks within the same domain and less so across different
domains. Among evaluation dimensions, diversity and quality metrics show strong
correlations - models that perform well on one often excel on the other -
whereas novelty exhibits much weaker correlation with either. These findings
support our hypothesis that strong performance in one creativity task or
dimension does not necessarily generalize to others, underscoring the need for
a holistic evaluation of LLM creativity.

</details>


### [295] [Leveraging the Power of Large Language Models in Entity Linking via Adaptive Routing and Targeted Reasoning](https://arxiv.org/abs/2510.20098)
*Yajie Li,Albert Galimov,Mitra Datta Ganapaneni,Pujitha Thejaswi,De Meng,Priyanshu Kumar,Saloni Potdar*

Main category: cs.CL

TL;DR: 传统实体链接依赖大量标注数据和模型微调，few - shot方法有推理效率问题。ARTER提出结构化管道，结合多种策略，分情况处理，在基准测试中表现好且效率高。


<details>
  <summary>Details</summary>
Motivation: 解决传统实体链接依赖大量标注数据和模型微调，以及few - shot方法推理效率低的问题。

Method: ARTER结合候选生成、基于上下文的评分、自适应路由和选择性推理，计算互补信号将上下文提及分为简单和困难情况，分别用低计算量的实体链接器和基于LLM的推理处理。

Result: 在标准基准测试中，ARTER比ReFinED最高提升4.47%，6个数据集中5个平均提升2.53%，与全用基于LLM推理的管道表现相当，LLM令牌数量效率翻倍。

Conclusion: ARTER无需深度微调，能高效高性能地完成实体链接任务。

Abstract: Entity Linking (EL) has traditionally relied on large annotated datasets and
extensive model fine-tuning. While recent few-shot methods leverage large
language models (LLMs) through prompting to reduce training requirements, they
often suffer from inefficiencies due to expensive LLM-based reasoning. ARTER
(Adaptive Routing and Targeted Entity Reasoning) presents a structured pipeline
that achieves high performance without deep fine-tuning by strategically
combining candidate generation, context-based scoring, adaptive routing, and
selective reasoning. ARTER computes a small set of complementary signals(both
embedding and LLM-based) over the retrieved candidates to categorize contextual
mentions into easy and hard cases. The cases are then handled by a
low-computational entity linker (e.g. ReFinED) and more expensive targeted
LLM-based reasoning respectively. On standard benchmarks, ARTER outperforms
ReFinED by up to +4.47%, with an average gain of +2.53% on 5 out of 6 datasets,
and performs comparably to pipelines using LLM-based reasoning for all
mentions, while being as twice as efficient in terms of the number of LLM
tokens.

</details>


### [296] [Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?](https://arxiv.org/abs/2510.20154)
*Anthony Dubreuil,Antoine Gourru,Christine Largeron,Amine Trabelsi*

Main category: cs.CL

TL;DR: 本文聚焦大语言模型在零样本立场检测中的偏差，通过自动标注数据集属性研究影响，发现模型存在显著刻板印象。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在预训练数据中继承刻板印象，导致NLP任务出现偏差，而立场检测方法中的偏差评估被忽视，所以研究大语言模型在零样本立场检测中的偏差。

Method: 自动标注已有立场检测数据集中帖子的两个属性：特定群体的方言或白话、文本复杂度/可读性，以研究这些属性对模型立场检测决策的影响。

Result: 大语言模型在立场检测任务中表现出显著的刻板印象，如错误地将支持大麻的观点与低文本复杂度联系起来，将非裔美国人方言与反对特朗普联系起来。

Conclusion: 大语言模型在立场检测任务中存在明显的刻板印象偏差。

Abstract: Large Language Models inherit stereotypes from their pretraining data,
leading to biased behavior toward certain social groups in many Natural
Language Processing tasks, such as hateful speech detection or sentiment
analysis. Surprisingly, the evaluation of this kind of bias in stance detection
methods has been largely overlooked by the community. Stance Detection involves
labeling a statement as being against, in favor, or neutral towards a specific
target and is among the most sensitive NLP tasks, as it often relates to
political leanings. In this paper, we focus on the bias of Large Language
Models when performing stance detection in a zero-shot setting. We
automatically annotate posts in pre-existing stance detection datasets with two
attributes: dialect or vernacular of a specific group and text
complexity/readability, to investigate whether these attributes influence the
model's stance detection decisions. Our results show that LLMs exhibit
significant stereotypes in stance detection tasks, such as incorrectly
associating pro-marijuana views with low text complexity and African American
dialect with opposition to Donald Trump.

</details>


### [297] [Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding](https://arxiv.org/abs/2510.20176)
*Yuhang Zhou,Mingrui Zhang,Ke Li,Mingyi Wang,Qiao Liu,Qifei wang,Jiayi Liu,Fei Liu,Serena Li,Weiwi Li,Mingze Gao,Abhishek Kumar,Xiangjun Fan,Zhuokai Zhao,Lizhu Zhang*

Main category: cs.CL

TL;DR: 现有大语言模型在表格理解和推理任务上有局限，本文提出Mixture - of - Minds多智能体框架及自改进训练框架，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 当前基于微调的方法易出现算术错误和幻觉，基于工具的方法依赖刚性模式且缺乏语义理解，需结合强大推理和可靠表格处理的方法。

Method: 提出Mixture - of - Minds多智能体框架，将表格推理分解为规划、编码和回答三个角色，利用代码执行进行精确表格操作；引入自改进训练框架，用蒙特卡罗树搜索生成伪黄金轨迹，用强化学习优化智能体。

Result: Mixture - of - Minds取得显著提升，在TableBench上达到62.13%，超过OpenAI - o4 - mini - high。

Conclusion: 结合结构化多智能体工作流和强化学习有助于推动表格理解。

Abstract: Understanding and reasoning over tables is a critical capability for many
real-world applications. Large language models (LLMs) have shown promise on
this task, but current approaches remain limited. Fine-tuning based methods
strengthen language reasoning; yet they are prone to arithmetic errors and
hallucination. In contrast, tool-based methods enable precise table
manipulation but rely on rigid schemas and lack semantic understanding. These
complementary drawbacks highlight the need for approaches that integrate robust
reasoning with reliable table processing. In this work, we propose
Mixture-of-Minds, a multi-agent framework that decomposes table reasoning into
three specialized roles: planning, coding, and answering. This design enables
each agent to focus on a specific aspect of the task while leveraging code
execution for precise table manipulation. Building on this workflow, we
introduce a self-improvement training framework that employs Monte Carlo Tree
Search (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents
with reinforcement learning (RL). Extensive experiments show that
Mixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and
surpassing OpenAI-o4-mini-high. These results demonstrate the promise of
combining structured multi-agent workflows with RL to advance table
understanding.

</details>


### [298] [Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models](https://arxiv.org/abs/2510.20198)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.CL

TL;DR: 本文通过五项任务探究大语言模型在文本输入上的空间推理能力，发现模型在复杂度和规模增大时性能下降，凸显语言与空间推理差距。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在文本输入上的空间推理能力，了解其空间理解和计算能力。

Method: 设计五项任务，包括象限识别、几何变换等，在结构化网格环境中测试，通过增加网格维度提升任务复杂度。

Result: 大语言模型在低复杂度和小规模任务中有一定成功，但规模增大时性能迅速下降，平均准确率损失42.7%，最高达84%。

Conclusion: 大语言模型在语言和空间推理间存在差距，其底层架构缺乏强大的空间表征，为未来语言与几何交叉的综合基准奠定基础。

Abstract: This paper explores the spatial reasoning capability of large language models
(LLMs) over textual input through a suite of five tasks aimed at probing their
spatial understanding and computational abilities. The models were tested on
both fundamental spatial reasoning and multi-step problem-solving within
structured grid-based environments using tasks such as quadrant identification,
geometric transformations, distance evaluation, word searches, and tile
sliding. Each task was scaled in complexity through increasing grid dimensions,
requiring models to extend beyond simple pattern recognition into abstract
spatial reasoning. Our results reveal that while LLMs demonstrate moderate
success in all tasks with small complexity and size, performance drops off
rapidly as scale increases, with an average loss in accuracy of 42.7%, and
reaching as high as 84%. Every test that began with over 50% accuracy showed a
loss of at least 48%, illustrating the consistent nature of the deterioration.
Furthermore, their struggles with scaling complexity hint at a lack of robust
spatial representations in their underlying architectures. This paper
underscores the gap between linguistic and spatial reasoning in LLMs, offering
insights into their current limitations, and laying the groundwork for future
integrative benchmarks at the intersection of language and geometry.

</details>


### [299] [Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders](https://arxiv.org/abs/2510.20239)
*Filippo Cenacchi,Deborah Richards,Longbing Cao*

Main category: cs.CL

TL;DR: 提出统一三模态情感严重程度框架用于诊断抑郁和PTSD，在多疾病并发评估中表现良好，为临床决策提供支持。


<details>
  <summary>Details</summary>
Motivation: 抑郁和PTSD症状相连，现有自动化评估常为二元且特定疾病，临床诊断需要跨疾病严重程度估计和决策支持解释。

Method: 统一三模态情感严重程度框架同步并融合访谈文本、音频和面部信号，通过校准的后期融合分类器融合标准化特征。

Result: 分层交叉验证中优于单模态/消融基线，融合模型在准确率和加权F1上与最强单模态基线相当，提高决策曲线效用和鲁棒性，减少PTSD回归误差，改善类别一致性。

Conclusion: 该方法为情感临床决策提供可重复评估和临床医生支持。

Abstract: Depression and post traumatic stress disorder (PTSD) often co-occur with
connected symptoms, complicating automated assessment, which is often binary
and disorder specific. Clinically useful diagnosis needs severity aware cross
disorder estimates and decision support explanations. Our unified tri modal
affective severity framework synchronizes and fuses interview text with
sentence level transformer embeddings, audio with log Mel statistics with
deltas, and facial signals with action units, gaze, head and pose descriptors
to output graded severities for diagnosing both depression (PHQ-8; 5 classes)
and PTSD (3 classes). Standardized features are fused via a calibrated late
fusion classifier, yielding per disorder probabilities and feature-level
attributions. This severity aware tri-modal affective fusion approach is demoed
on multi disorder concurrent depression and PTSD assessment. Stratified cross
validation on DAIC derived corpora outperforms unimodal/ablation baselines. The
fused model matches the strongest unimodal baseline on accuracy and weighted
F1, while improving decision curve utility and robustness under noisy or
missing modalities. For PTSD specifically, fusion reduces regression error and
improves class concordance. Errors cluster between adjacent severities; extreme
classes are identified reliably. Ablations show text contributes most to
depression severity, audio and facial cues are critical for PTSD, whereas
attributions align with linguistic and behavioral markers. Our approach offers
reproducible evaluation and clinician in the loop support for affective
clinical decision making.

</details>


### [300] [Context-level Language Modeling by Learning Predictive Context Embeddings](https://arxiv.org/abs/2510.20280)
*Beiya Dai,Yuliang Liu,Daozheng Xue,Qipeng Guo,Kai Chen,Xinbing Wang*

Main category: cs.CL

TL;DR: 提出ContextLM框架增强预训练，实验表明其能提升困惑度和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有NTP方法存在局限，无法捕捉高层语义结构和长距离上下文关系。

Method: 引入ContextLM框架，增加下一个上下文预测目标，利用未来标记块的误差信号训练模型。

Result: 在GPT2和Pythia模型家族上实验，ContextLM能提升困惑度和下游任务性能。

Conclusion: 下一个上下文预测是提升语言建模的可扩展高效途径，开销小且能提升长距离连贯性和注意力分配效率。

Abstract: Next-token prediction (NTP) is the cornerstone of modern large language
models (LLMs) pretraining, driving their unprecedented capabilities in text
generation, reasoning, and instruction following. However, the token-level
prediction limits the model's capacity to capture higher-level semantic
structures and long-range contextual relationships. To overcome this
limitation, we introduce \textbf{ContextLM}, a framework that augments standard
pretraining with an inherent \textbf{next-context prediction} objective. This
mechanism trains the model to learn predictive representations of multi-token
contexts, leveraging error signals derived from future token chunks. Crucially,
ContextLM achieves this enhancement while remaining fully compatible with the
standard autoregressive, token-by-token evaluation paradigm (e.g., perplexity).
Extensive experiments on the GPT2 and Pythia model families, scaled up to
$1.5$B parameters, show that ContextLM delivers consistent improvements in both
perplexity and downstream task performance. Our analysis indicates that
next-context prediction provides a scalable and efficient pathway to stronger
language modeling, yielding better long-range coherence and more effective
attention allocation with minimal computational overhead.

</details>


### [301] [Teaching Language Models to Reason with Tools](https://arxiv.org/abs/2510.20342)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Main category: cs.CL

TL;DR: 现有大推理模型处理复杂数学运算有问题，引入CoRT框架解决模型与计算工具结合的冲突，实验显示其有效且提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决大推理模型处理复杂数学运算的低效和不准确问题，以及模型内部推理与外部计算工具知识的冲突。

Method: 提出CoRT框架，采用Hint - Engineering数据合成策略，通过监督微调合成样本，用拒绝采样和强化学习优化外部工具使用和内部思考的多轮交织。

Result: 在五个数学推理数据集上，DeepSeek - R1 - Distill - Qwen - 32B和DeepSeek - R1 - Distill - Qwen - 1.5B分别有4%和8%的绝对提升，且显著减少了token使用量。

Conclusion: CoRT框架能有效提升大推理模型处理复杂数学运算的能力和效率。

Abstract: Large reasoning models (LRMs) like OpenAI-o1 have shown impressive
capabilities in natural language reasoning. However, these models frequently
demonstrate inefficiencies or inaccuracies when tackling complex mathematical
operations. While integrating computational tools such as Code Interpreters
(CIs) offers a promising solution, it introduces a critical challenge: a
conflict between the model's internal, probabilistic reasoning and the
external, deterministic knowledge provided by the CI, which often leads models
to unproductive deliberation. To overcome this, we introduce CoRT
(Code-Optimized Reasoning Training), a post-training framework designed to
teach LRMs to effectively utilize CIs. We propose \emph{Hint-Engineering}, a
new data synthesis strategy that strategically injects diverse hints at optimal
points within reasoning paths. This approach generates high-quality,
code-integrated reasoning data specifically tailored to optimize LRM-CI
interaction. Using this method, we have synthesized 30 high-quality samples to
post-train models ranging from 1.5B to 32B parameters through supervised
fine-tuning. CoRT further refines the multi-round interleaving of external CI
usage and internal thinking by employing rejection sampling and reinforcement
learning. Our experimental evaluations demonstrate CoRT's effectiveness,
yielding absolute improvements of 4\% and 8\% on DeepSeek-R1-Distill-Qwen-32B
and DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging
mathematical reasoning datasets. Moreover, CoRT significantly enhances
efficiency, reducing token usage by approximately 30\% for the 32B model and
50\% for the 1.5B model compared to pure natural language reasoning baselines.
The models and code are available at: https://github.com/ChengpengLi1003/CoRT.

</details>


### [302] [Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models](https://arxiv.org/abs/2510.20351)
*Matteo Silvestri,Flavio Giorgi,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.CL

TL;DR: 研究发现大语言模型在表格推理任务上的表现可能源于对公开数据集的记忆，而非真正的泛化能力，并对评估协议提出建议。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型结构化数据推理能力评估常忽略数据集污染问题，研究其是否对常用表格基准数据集有先验知识。

Method: 通过一系列受控探测实验。

Result: 污染效应仅出现在含强语义线索的数据集上，去除或随机化线索后性能降至接近随机水平。

Conclusion: 大语言模型在表格推理任务上的能力可能部分源于记忆公开数据集，需在未来评估中区分语义泄漏和真实推理能力。

Abstract: Large Language Models (LLMs) are increasingly evaluated on their ability to
reason over structured data, yet such assessments often overlook a crucial
confound: dataset contamination. In this work, we investigate whether LLMs
exhibit prior knowledge of widely used tabular benchmarks such as Adult Income,
Titanic, and others. Through a series of controlled probing experiments, we
reveal that contamination effects emerge exclusively for datasets containing
strong semantic cues-for instance, meaningful column names or interpretable
value categories. In contrast, when such cues are removed or randomized,
performance sharply declines to near-random levels. These findings suggest that
LLMs' apparent competence on tabular reasoning tasks may, in part, reflect
memorization of publicly available datasets rather than genuine generalization.
We discuss implications for evaluation protocols and propose strategies to
disentangle semantic leakage from authentic reasoning ability in future LLM
assessments.

</details>


### [303] [The Impact of Negated Text on Hallucination with Large Language Models](https://arxiv.org/abs/2510.20375)
*Jaehyung Seo,Hyeonseok Moon,Heuiseok Lim*

Main category: cs.CL

TL;DR: 研究大语言模型在否定文本中幻觉检测问题，设计NegHalu数据集，发现模型检测效果不佳及处理否定输入的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大语言模型中否定文本对幻觉的影响探索不足，提出三个重要未解答的研究问题。

Method: 研究模型能否识别否定导致的语境变化及区分幻觉，设计NegHalu数据集，在token级别追踪模型处理否定输入的内部状态。

Result: 大语言模型难以有效检测否定文本中的幻觉，常产生逻辑不一致或不可信的判断。

Conclusion: 揭示了缓解大语言模型处理否定文本时产生意外影响的挑战。

Abstract: Recent studies on hallucination in large language models (LLMs) have been
actively progressing in natural language processing. However, the impact of
negated text on hallucination with LLMs remains largely unexplored. In this
paper, we set three important yet unanswered research questions and aim to
address them. To derive the answers, we investigate whether LLMs can recognize
contextual shifts caused by negation and still reliably distinguish
hallucinations comparable to affirmative cases. We also design the NegHalu
dataset by reconstructing existing hallucination detection datasets with
negated expressions. Our experiments demonstrate that LLMs struggle to detect
hallucinations in negated text effectively, often producing logically
inconsistent or unfaithful judgments. Moreover, we trace the internal state of
LLMs as they process negated inputs at the token level and reveal the
challenges of mitigating their unintended effects.

</details>


### [304] [VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation](https://arxiv.org/abs/2510.20381)
*Son T. Luu,Trung Vo,Hiep Nguyen,Khanh Quoc Tran,Kiet Van Nguyen,Vu Tran,Ngan Luu-Thuy Nguyen,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: 介绍VLSP 2025 MLQA - TSR共享任务，含两个子任务及最佳结果


<details>
  <summary>Details</summary>
Motivation: 推动越南多模态法律文本处理研究，为多模态法律领域智能系统提供基准数据集，聚焦越南交通标志法规

Method: 未提及

Result: 多模态法律检索F2分数64.55%，多模态问答准确率86.30%

Conclusion: 未提及

Abstract: This paper presents the VLSP 2025 MLQA-TSR - the multimodal legal question
answering on traffic sign regulation shared task at VLSP 2025. VLSP 2025
MLQA-TSR comprises two subtasks: multimodal legal retrieval and multimodal
question answering. The goal is to advance research on Vietnamese multimodal
legal text processing and to provide a benchmark dataset for building and
evaluating intelligent systems in multimodal legal domains, with a focus on
traffic sign regulation in Vietnam. The best-reported results on VLSP 2025
MLQA-TSR are an F2 score of 64.55% for multimodal legal retrieval and an
accuracy of 86.30% for multimodal question answering.

</details>


### [305] [RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging](https://arxiv.org/abs/2510.20479)
*Bowen Wang,Haiyuan Wan,Liwen Shi,Chen Yang,Peng He,Yue Ma,Haochen Han,Wenhao Li,Tiao Tan,Yongjian Li,Fangming Liu,Yifan Gong,Sheng Zhang*

Main category: cs.CL

TL;DR: 提出RECALL框架，无需历史数据用于持续学习，实验显示其在知识保留和泛化方面优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型持续学习中无需历史数据，避免灾难性遗忘和实现多领域集成的问题。

Method: 从聚类典型样本的层隐藏表示计算模型间相似度，进行自适应、分层参数融合。

Result: 在五个NLP任务和多个持续学习场景实验中，RECALL在知识保留和泛化上优于基线。

Conclusion: RECALL是一种可扩展、无数据依赖的大语言模型持续学习解决方案。

Abstract: We unveil that internal representations in large language models (LLMs) serve
as reliable proxies of learned knowledge, and propose RECALL, a novel
representation-aware model merging framework for continual learning without
access to historical data. RECALL computes inter-model similarity from
layer-wise hidden representations over clustered typical samples, and performs
adaptive, hierarchical parameter fusion to align knowledge across models. This
design enables the preservation of domain-general features in shallow layers
while allowing task-specific adaptation in deeper layers. Unlike prior methods
that require task labels or incur performance trade-offs, RECALL achieves
seamless multi-domain integration and strong resistance to catastrophic
forgetting. Extensive experiments across five NLP tasks and multiple continual
learning scenarios show that RECALL outperforms baselines in both knowledge
retention and generalization, providing a scalable and data-free solution for
evolving LLMs.

</details>


### [306] [Steering Evaluation-Aware Language Models To Act Like They Are Deployed](https://arxiv.org/abs/2510.20487)
*Tim Tian Hua,Andrew Qin,Samuel Marks,Neel Nanda*

Main category: cs.CL

TL;DR: 本文提出添加转向向量到LLM激活中可抑制评估意识，使模型在评估时表现得像在部署中，AI评估者可借此提高安全评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在评估时会调整行为，影响安全评估可靠性，需要解决该问题。

Method: 训练LLM展现评估意识行为，采用两步训练过程，先在特定文档上继续预训练，再用专家迭代训练；构建转向向量抑制评估意识。

Result: 激活转向能抑制评估意识，即使存在评估提示，模型也表现得像在部署中。

Conclusion: AI评估者可通过引导模型表现得像在部署中，提高安全评估的可靠性。

Abstract: Large language models (LLMs) can sometimes detect when they are being
evaluated and adjust their behavior to appear more aligned, compromising the
reliability of safety evaluations. In this paper, we show that adding a
steering vector to an LLM's activations can suppress evaluation-awareness and
make the model act like it is deployed during evaluation. To study our steering
technique, we train an LLM to exhibit evaluation-aware behavior using a
two-step training process designed to mimic how this behavior could emerge
naturally. First, we perform continued pretraining on documents with factual
descriptions of the model (1) using Python type hints during evaluation but not
during deployment and (2) recognizing that the presence of a certain evaluation
cue always means that it is being tested. Then, we train the model with expert
iteration to use Python type hints in evaluation settings. The resulting model
is evaluation-aware: it writes type hints in evaluation contexts more than
deployment contexts. However, this gap can only be observed by removing the
evaluation cue. We find that activation steering can suppress evaluation
awareness and make the model act like it is deployed even when the cue is
present. Importantly, we constructed our steering vector using the original
model before our additional training. Our results suggest that AI evaluators
could improve the reliability of safety evaluations by steering models to act
like they are deployed.

</details>


### [307] [Hierarchical Sequence Iteration for Heterogeneous Question Answering](https://arxiv.org/abs/2510.20505)
*Ruiyi Yang,Hao Xue,Imran Razzak,Hakim Hacid,Flora D. Salim*

Main category: cs.CL

TL;DR: 本文提出用于异构问答的分层序列（HSEQ）迭代统一框架，在多个数据集实验中表现出色，有格式无关统一、引导迭代和证据规范化等优势。


<details>
  <summary>Details</summary>
Motivation: 解决检索增强生成（RAG）在多步问题和异构证据源上的脆性问题，平衡准确性与延迟、令牌/工具预算。

Method: 将文档、表格和知识图线性化为带轻量级结构标签的可逆分层序列，进行结构感知迭代收集证据，通过头代理引导检索、迭代代理选择和扩展HSeq，最后合成答案并可选细化。

Result: 在HotpotQA、HybridQA/TAT - QA和MetaQA等数据集实验中，比强单遍、多跳和代理式RAG基线有一致的EM/F1提升，且效率高。

Conclusion: HSEQ框架有格式无关统一、引导预算感知迭代、证据规范化等优势，能有效用于异构问答。

Abstract: Retrieval-augmented generation (RAG) remains brittle on multi-step questions
and heterogeneous evidence sources, trading accuracy against latency and
token/tool budgets. This paper introducesHierarchical Sequence (HSEQ) Iteration
for Heterogeneous Question Answering, a unified framework that (i) linearize
documents, tables, and knowledge graphs into a reversible hierarchical sequence
with lightweight structural tags, and (ii) perform structure-aware iteration to
collect just-enough evidence before answer synthesis. A Head Agent provides
guidance that leads retrieval, while an Iteration Agent selects and expands
HSeq via structure-respecting actions (e.g., parent/child hops, table
row/column neighbors, KG relations); Finally the head agent composes
canonicalized evidence to genearte the final answer, with an optional
refinement loop to resolve detected contradictions. Experiments on HotpotQA
(text), HybridQA/TAT-QA (table+text), and MetaQA (KG) show consistent EM/F1
gains over strong single-pass, multi-hop, and agentic RAG baselines with high
efficiency. Besides, HSEQ exhibits three key advantages: (1) a format-agnostic
unification that enables a single policy to operate across text, tables, and
KGs without per-dataset specialization; (2) guided, budget-aware iteration that
reduces unnecessary hops, tool calls, and tokens while preserving accuracy; and
(3) evidence canonicalization for reliable QA, improving answers consistency
and auditability.

</details>


### [308] [ARC-Encoder: learning compressed text representations for large language models](https://arxiv.org/abs/2510.20535)
*Hippolyte Pilchen,Edouard Grave,Patrick Pérez*

Main category: cs.CL

TL;DR: 探索替代的上下文压缩方法ARC - Encoder，可在多种场景提高效率和性能，还能适配多解码器。


<details>
  <summary>Details</summary>
Motivation: 现有上下文压缩技术需微调目标模型或修改架构，会降低模型通用能力，需探索替代方法。

Method: 系统研究编码器训练策略和架构选择，设计ARC - Encoder。

Result: ARC - Encoder在多个基准测试中达最优性能，提高推理计算效率，可同时适配多个解码器。

Conclusion: ARC - Encoder是灵活高效的便携式编码器解决方案，适用于多个大语言模型。

Abstract: Recent techniques such as retrieval-augmented generation or chain-of-thought
reasoning have led to longer contexts and increased inference costs. Context
compression techniques can reduce these costs, but the most effective
approaches require fine-tuning the target model or even modifying its
architecture. This can degrade its general abilities when not used for this
specific purpose. Here we explore an alternative approach: an encoder that
compresses the context into continuous representations which replace token
embeddings in decoder LLMs. First, we perform a systematic study of training
strategies and architecture choices for the encoder. Our findings led to the
design of an Adaptable text Representations Compressor, named ARC-Encoder,
which outputs $x$-times fewer continuous representations (typically
$x\!\in\!\{4,8\}$) than text tokens. We evaluate ARC-Encoder across a variety
of LLM usage scenarios, ranging from in-context learning to context window
extension, on both instruct and base decoders. Results show that ARC-Encoder
achieves state-of-the-art performance on several benchmarks while improving
computational efficiency at inference. Finally, we demonstrate that our models
can be adapted to multiple decoders simultaneously, allowing a single encoder
to generalize across different decoder LLMs. This makes ARC-Encoder a flexible
and efficient solution for portable encoders that work seamlessly with multiple
LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder
, fine-tuning dataset and pretrained models are available at
https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .

</details>


### [309] [The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts](https://arxiv.org/abs/2510.20543)
*Sangmitra Madhusudan,Kaige Chen,Ali Emami*

Main category: cs.CL

TL;DR: 引入CenterBench数据集测试语言模型，发现模型在处理复杂句子时会从结构分析转向模式匹配，人类语义影响可变。


<details>
  <summary>Details</summary>
Motivation: 缺乏区分语言模型结构理解和语义模式匹配的方法。

Method: 引入CenterBench数据集，包含9720个关于中心嵌套句子的理解问题，测试六个模型。

Result: 模型在处理复杂句子时，合理与不合理句子的性能差距扩大，语义合理性在因果推理问题上影响性能，推理模型有语义捷径等问题，人类语义影响可变。

Conclusion: CenterBench提供了识别模型从结构分析转向模式匹配的首个框架。

Abstract: When language models correctly parse "The cat that the dog chased meowed,"
are they analyzing syntax or simply familiar with dogs chasing cats? Despite
extensive benchmarking, we lack methods to distinguish structural understanding
from semantic pattern matching. We introduce CenterBench, a dataset of 9,720
comprehension questions on center-embedded sentences (like "The cat [that the
dog chased] meowed") where relative clauses nest recursively, creating
processing demands from simple to deeply nested structures. Each sentence has a
syntactically identical but semantically implausible counterpart (e.g., mailmen
prescribe medicine, doctors deliver mail) and six comprehension questions
testing surface understanding, syntactic dependencies, and causal reasoning.
Testing six models reveals that performance gaps between plausible and
implausible sentences widen systematically with complexity, with models showing
median gaps up to 26.8 percentage points, quantifying when they abandon
structural analysis for semantic associations. Notably, semantic plausibility
harms performance on questions about resulting actions, where following causal
relationships matters more than semantic coherence. Reasoning models improve
accuracy but their traces show semantic shortcuts, overthinking, and answer
refusal. Unlike models whose plausibility advantage systematically widens with
complexity, humans shows variable semantic effects. CenterBench provides the
first framework to identify when models shift from structural analysis to
pattern matching.

</details>


### [310] [GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning](https://arxiv.org/abs/2510.20548)
*Jinchang Luo,Mingquan Cheng,Fan Wan,Ni Li,Xiaoling Xia,Shuangshuang Tian,Tingcheng Bian,Haiwei Wang,Haohuan Fu,Yan Tao*

Main category: cs.CL

TL;DR: 提出GlobalRAG强化学习框架用于多跳问答，用新奖励机制和策略，实验显示其表现优且省数据。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习在多跳问答中因缺乏全局规划和执行不忠实而效果受限。

Method: 提出GlobalRAG框架，分解问题为子目标，协调检索与推理，迭代优化证据；引入规划质量奖励和子目标完成奖励；采用渐进权重退火策略。

Result: 在域内和域外基准测试中，GlobalRAG显著优于强基线，仅用42%训练数据，EM和F1平均提升14.2%。

Conclusion: GlobalRAG能有效增强多跳问答中的全局推理。

Abstract: Reinforcement learning has recently shown promise in improving
retrieval-augmented generation (RAG). Despite these advances, its effectiveness
in multi-hop question answering (QA) remains limited by two fundamental
limitations: (i) global planning absence to structure multi-step reasoning, and
(ii) unfaithful execution, which hinders effective query formulation and
consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement
learning framework designed to enhance global reasoning in multi-hop QA.
GlobalRAG decomposes questions into subgoals, coordinates retrieval with
reasoning, and refines evidence iteratively. To guide this process, we
introduce Planning Quality Reward and SubGoal Completion Reward, which
encourage coherent planning and reliable subgoal execution. In addition, a
progressive weight annealing strategy balances process-oriented and
outcome-based objectives. Extensive experiments on both in-domain and
out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms
strong baselines while using only 8k training data (42% of the training data
used by strong baselines), achieving average improvements of 14.2% in both EM
and F1.

</details>


### [311] [From Facts to Folklore: Evaluating Large Language Models on Bengali Cultural Knowledge](https://arxiv.org/abs/2510.20043)
*Nafis Chowdhury,Moinul Haque,Anika Ahmed,Nazia Tasnim,Md. Istiak Hossain Shihab,Sajjadur Rahman,Farig Sadeque*

Main category: cs.CL

TL;DR: 利用BLanCK数据集研究多语言模型在孟加拉语文化知识上的表现，强调上下文和文化数据的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有多语言基准在评估大语言模型时，在捕捉低资源文化细微差别上存在差距。

Method: 创建包含民间传统、烹饪艺术和地区方言的BLanCK数据集来研究多语言模型。

Result: 多语言模型在非文化类别表现好，但在文化知识上表现差，提供上下文能显著提升性能。

Conclusion: 应注重上下文感知架构和文化精选的训练数据。

Abstract: Recent progress in NLP research has demonstrated remarkable capabilities of
large language models (LLMs) across a wide range of tasks. While recent
multilingual benchmarks have advanced cultural evaluation for LLMs, critical
gaps remain in capturing the nuances of low-resource cultures. Our work
addresses these limitations through a Bengali Language Cultural Knowledge
(BLanCK) dataset including folk traditions, culinary arts, and regional
dialects. Our investigation of several multilingual language models shows that
while these models perform well in non-cultural categories, they struggle
significantly with cultural knowledge and performance improves substantially
across all models when context is provided, emphasizing context-aware
architectures and culturally curated training data.

</details>


### [312] [Can ChatGPT Code Communication Data Fairly?: Empirical Evidence from Multiple Collaborative Tasks](https://arxiv.org/abs/2510.20584)
*Jiangang Hao,Wenju Cui,Patrick Kyllonen,Emily Kerzabi*

Main category: cs.CL

TL;DR: 本文研究ChatGPT对通信数据自动编码在不同性别和种族群体中是否存在偏差，结果显示无显著偏差，为其大规模应用铺平道路。


<details>
  <summary>Details</summary>
Motivation: 以往研究虽表明ChatGPT可对通信数据编码且准确率与人类相当，但不清楚其编码是否对不同性别和种族群体存在偏差，本文旨在填补这一空白。

Method: 使用典型的协作问题解决编码框架，对谈判、问题解决和决策三种协作任务的数据进行分析。

Result: 基于ChatGPT的编码在不同性别和种族群体中无显著偏差。

Conclusion: 为ChatGPT在协作和通信大规模评估中的应用铺平了道路。

Abstract: Assessing communication and collaboration at scale depends on a labor
intensive task of coding communication data into categories according to
different frameworks. Prior research has established that ChatGPT can be
directly instructed with coding rubrics to code the communication data and
achieves accuracy comparable to human raters. However, whether the coding from
ChatGPT or similar AI technology exhibits bias against different demographic
groups, such as gender and race, remains unclear. To fill this gap, this paper
investigates ChatGPT-based automated coding of communication data using a
typical coding framework for collaborative problem solving, examining
differences across gender and racial groups. The analysis draws on data from
three types of collaborative tasks: negotiation, problem solving, and decision
making. Our results show that ChatGPT-based coding exhibits no significant bias
across gender and racial groups, paving the road for its adoption in
large-scale assessment of collaboration and communication.

</details>


### [313] [BUSTED at AraGenEval Shared Task: A Comparative Study of Transformer-Based Models for Arabic AI-Generated Text Detection](https://arxiv.org/abs/2510.20610)
*Ali Zain,Sareem Farooqui,Muhammad Rafi*

Main category: cs.CL

TL;DR: 本文介绍团队BUSTED参加阿拉伯语AI生成文本检测任务获第5名，研究三个预训练模型，发现多语言模型XLM - RoBERTa表现最佳。


<details>
  <summary>Details</summary>
Motivation: 参加阿拉伯语AI生成文本检测的Ara - GenEval共享任务，研究不同预训练模型在该任务中的有效性。

Method: 对AraELECTRA、CAMeLBERT和XLM - RoBERTa三个预训练模型在给定数据集上进行微调，用于二分类任务。

Result: 多语言XLM - RoBERTa模型表现最佳，F1分数为0.7701，优于专门的阿拉伯语模型。

Conclusion: 强调了AI生成文本检测的复杂性，凸显了多语言模型强大的泛化能力。

Abstract: This paper details our submission to the Ara- GenEval Shared Task on Arabic
AI-generated text detection, where our team, BUSTED, se- cured 5th place. We
investigated the effec- tiveness of three pre-trained transformer mod- els:
AraELECTRA, CAMeLBERT, and XLM- RoBERTa. Our approach involved fine-tuning each
model on the provided dataset for a binary classification task. Our findings
revealed a sur- prising result: the multilingual XLM-RoBERTa model achieved the
highest performance with an F1 score of 0.7701, outperforming the spe- cialized
Arabic models. This work underscores the complexities of AI-generated text
detection and highlights the strong generalization capa- bilities of
multilingual models.

</details>


### [314] [Why Did Apple Fall To The Ground: Evaluating Curiosity In Large Language Model](https://arxiv.org/abs/2510.20635)
*Haoyu Wang,Sihang Jiang,Yuyan Chen,Yitong Wang,Yanghua Xiao*

Main category: cs.CL

TL;DR: 本文从人类好奇心评估问卷出发设计框架评估大语言模型好奇心，发现其求知欲强但面对不确定更保守，好奇行为能提升推理和主动学习能力。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型是否具备类似人类的好奇心驱动学习能力。

Method: 从人类好奇心评估问卷5DCR出发，设计涵盖信息寻求、刺激寻求和社会好奇等维度的综合评估框架。

Result: 大语言模型求知欲强于人类，但面对不确定环境更保守；好奇行为能提升模型推理和主动学习能力。

Conclusion: 大语言模型有潜力展现类似人类的好奇心，为其学习能力发展和创新研究提供实验支持。

Abstract: Curiosity serves as a pivotal conduit for human beings to discover and learn
new knowledge. Recent advancements of large language models (LLMs) in natural
language processing have sparked discussions regarding whether these models
possess capability of curiosity-driven learning akin to humans. In this paper,
starting from the human curiosity assessment questionnaire Five-Dimensional
Curiosity scale Revised (5DCR), we design a comprehensive evaluation framework
that covers dimensions such as Information Seeking, Thrill Seeking, and Social
Curiosity to assess the extent of curiosity exhibited by LLMs. The results
demonstrate that LLMs exhibit a stronger thirst for knowledge than humans but
still tend to make conservative choices when faced with uncertain environments.
We further investigated the relationship between curiosity and thinking of
LLMs, confirming that curious behaviors can enhance the model's reasoning and
active learning abilities. These findings suggest that LLMs have the potential
to exhibit curiosity similar to that of humans, providing experimental support
for the future development of learning capabilities and innovative research in
LLMs.

</details>


### [315] [The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI](https://arxiv.org/abs/2510.20647)
*Alan Saji,Raj Dabre,Anoop Kunchukuttan,Ratish Puduppully*

Main category: cs.CL

TL;DR: 研究大推理模型（LRMs）多语言推理能力，对比英文和问题语言推理情况，发现英文推理有优势但存在翻译错误问题。


<details>
  <summary>Details</summary>
Motivation: LRMs在多语言推理能力方面研究不足，处理非英文问题时有缺陷，需进行研究。

Method: 系统对比LRM在英文和问题语言下的推理，评估MGSM和GPQA Diamond两个任务，分析推理痕迹中的认知属性。

Result: 英文推理痕迹中认知行为更多，最终答案准确率更高，任务越复杂差距越大，但英文推理存在‘翻译迷失’问题。

Conclusion: LRMs英文推理有优势但存在关键失败模式，多语言推理能力有待提升。

Abstract: Large Reasoning Models (LRMs) achieve strong performance on mathematical,
scientific, and other question-answering tasks, but their multilingual
reasoning abilities remain underexplored. When presented with non-English
questions, LRMs often default to reasoning in English, raising concerns about
interpretability and the handling of linguistic and cultural nuances. We
systematically compare an LRM's reasoning in English versus the language of the
question. Our evaluation spans two tasks: MGSM and GPQA Diamond. Beyond
measuring answer accuracy, we also analyze cognitive attributes in the
reasoning traces. We find that English reasoning traces exhibit a substantially
higher presence of these cognitive behaviors, and that reasoning in English
generally yields higher final-answer accuracy, with the performance gap
increasing as tasks become more complex. However, this English-centric strategy
is susceptible to a key failure mode - getting "Lost in Translation," where
translation steps lead to errors that would have been avoided by question's
language reasoning.

</details>


### [316] [Neural Diversity Regularizes Hallucinations in Small Models](https://arxiv.org/abs/2510.20690)
*Kushal Chakrabarti,Nirmal Balachundhar*

Main category: cs.CL

TL;DR: 提出神经多样性机制减少语言模型幻觉率，引入ND - LoRA验证，结果表明神经多样性是提升模型可靠性的新维度。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在参数、计算和数据增加情况下仍会产生幻觉的问题。

Method: 提出神经多样性机制，引入ND - LoRA（结合并行LoRA适配器和Barlow Twins正则化），进行消融实验、因果干预和相关分析。

Result: ND - LoRA最多降低25.6%（平均14.6%）的幻觉率且不降低整体准确率，显示LoRA适配器和正则化协同作用，证明神经多样性是中介因素，发现不同任务所需最优神经多样性不同。

Conclusion: 神经多样性是与参数和数据正交的提升语言模型可靠性的第三维度。

Abstract: Language models continue to hallucinate despite increases in parameters,
compute, and data. We propose neural diversity -- decorrelated parallel
representations -- as a principled mechanism that reduces hallucination rates
at fixed parameter and data budgets. Inspired by portfolio theory, where
uncorrelated assets reduce risk by $\sqrt{P}$, we prove hallucination
probability is bounded by representational correlation: $P(H) \leq
f(\sigma^2((1-\rho(P))/P + \rho(P)), \mu^2)$, which predicts that language
models need an optimal amount of neurodiversity. To validate this, we introduce
ND-LoRA (Neural Diversity Low-Rank Adaptation), combining parallel LoRA
adapters with Barlow Twins regularization, and demonstrate that ND-LoRA reduces
hallucinations by up to 25.6% (and 14.6% on average) without degrading general
accuracy. Ablations show LoRA adapters and regularization act synergistically,
causal interventions prove neurodiversity as the mediating factor and
correlational analyses indicate scale: a 0.1% neural correlation increase is
associated with a 3.8% hallucination increase. Finally, task-dependent
optimality emerges: different tasks require different amounts of optimal
neurodiversity. Together, our results highlight neural diversity as a third
axis of scaling -- orthogonal to parameters and data -- to improve the
reliability of language models at fixed budgets.

</details>


### [317] [User Perceptions of Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios](https://arxiv.org/abs/2510.20721)
*Xiaoyuan Wu,Roshni Kaushik,Wenkai Li,Lujo Bauer,Koichi Onoue*

Main category: cs.CL

TL;DR: 研究通过用户研究发现，用户对大语言模型响应的隐私保护质量和有用性评价一致性低，代理大语言模型与用户评价相关性低，需开展以用户为中心的研究。


<details>
  <summary>Details</summary>
Motivation: 现有评估依赖代理大语言模型，忽略真实用户感知，且未研究响应有用性的细微差异，因此要了解用户对大语言模型响应的隐私保护质量和有用性的看法。

Method: 对94名参与者开展用户研究，使用PrivacyLens中的90个场景。

Result: 用户对相同场景的响应评价一致性低，五个代理大语言模型评价一致性高，单个大语言模型与用户评价相关性低。

Conclusion: 大语言模型响应的隐私性和有用性因人而异，代理大语言模型难以准确估计用户感知，需开展以用户为中心的研究，未来可研究改善代理大语言模型与用户的一致性。

Abstract: Large language models (LLMs) have seen rapid adoption for tasks such as
drafting emails, summarizing meetings, and answering health questions. In such
uses, users may need to share private information (e.g., health records,
contact details). To evaluate LLMs' ability to identify and redact such private
information, prior work developed benchmarks (e.g., ConfAIde, PrivacyLens) with
real-life scenarios. Using these benchmarks, researchers have found that LLMs
sometimes fail to keep secrets private when responding to complex tasks (e.g.,
leaking employee salaries in meeting summaries). However, these evaluations
rely on LLMs (proxy LLMs) to gauge compliance with privacy norms, overlooking
real users' perceptions. Moreover, prior work primarily focused on the
privacy-preservation quality of responses, without investigating nuanced
differences in helpfulness. To understand how users perceive the
privacy-preservation quality and helpfulness of LLM responses to
privacy-sensitive scenarios, we conducted a user study with 94 participants
using 90 scenarios from PrivacyLens. We found that, when evaluating identical
responses to the same scenario, users showed low agreement with each other on
the privacy-preservation quality and helpfulness of the LLM response. Further,
we found high agreement among five proxy LLMs, while each individual LLM had
low correlation with users' evaluations. These results indicate that the
privacy and helpfulness of LLM responses are often specific to individuals, and
proxy LLMs are poor estimates of how real users would perceive these responses
in privacy-sensitive scenarios. Our results suggest the need to conduct
user-centered studies on measuring LLMs' ability to help users while preserving
privacy. Additionally, future research could investigate ways to improve the
alignment between proxy LLMs and users for better estimation of users'
perceived privacy and utility.

</details>


### [318] [Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing](https://arxiv.org/abs/2510.20727)
*Xizhi Wu,Madeline S. Kreider,Philip E. Empey,Chenyu Li,Yanshan Wang*

Main category: cs.CL

TL;DR: 本文旨在开发评估NLP方法提取氟嘧啶治疗与毒性信息，对比多种NLP方法，发现基于大语言模型的NLP最有效。


<details>
  <summary>Details</summary>
Motivation: 氟嘧啶用于癌症治疗但有毒性，毒性信息多在临床笔记中，需开发NLP方法提取相关信息。

Method: 构建金标准数据集，专家标注，采用基于规则、机器学习、深度学习和大语言模型的NLP方法，80:20划分训练测试集。

Result: 错误分析提示在提取治疗和毒性信息时F1分数最优，大语言模型方法表现最佳，机器学习其次，深度学习表现不佳。

Conclusion: 基于大语言模型的NLP能有效提取信息，有潜力支持肿瘤研究和药物警戒。

Abstract: Objective: Fluoropyrimidines are widely prescribed for colorectal and breast
cancers, but are associated with toxicities such as hand-foot syndrome and
cardiotoxicity. Since toxicity documentation is often embedded in clinical
notes, we aimed to develop and evaluate natural language processing (NLP)
methods to extract treatment and toxicity information.
  Materials and Methods: We constructed a gold-standard dataset of 236 clinical
notes from 204,165 adult oncology patients. Domain experts annotated categories
related to treatment regimens and toxicities. We developed rule-based, machine
learning-based (Random Forest, Support Vector Machine [SVM], Logistic
Regression [LR]), deep learning-based (BERT, ClinicalBERT), and large language
models (LLM)-based NLP approaches (zero-shot and error-analysis prompting).
Models used an 80:20 train-test split.
  Results: Sufficient data existed to train and evaluate 5 annotated
categories. Error-analysis prompting achieved optimal precision, recall, and F1
scores (F1=1.000) for treatment and toxicities extraction, whereas zero-shot
prompting reached F1=1.000 for treatment and F1=0.876 for toxicities
extraction.LR and SVM ranked second for toxicities (F1=0.937). Deep learning
underperformed, with BERT (F1=0.873 treatment; F1= 0.839 toxicities) and
ClinicalBERT (F1=0.873 treatment; F1 = 0.886 toxicities). Rule-based methods
served as our baseline with F1 scores of 0.857 in treatment and 0.858 in
toxicities.
  Discussion: LMM-based approaches outperformed all others, followed by machine
learning methods. Machine and deep learning approaches were limited by small
training data and showed limited generalizability, particularly for rare
categories.
  Conclusion: LLM-based NLP most effectively extracted fluoropyrimidine
treatment and toxicity information from clinical notes, and has strong
potential to support oncology research and pharmacovigilance.

</details>


### [319] [Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost](https://arxiv.org/abs/2510.20780)
*Runzhe Zhan,Zhihong Huang,Xinyi Yang,Lidia S. Chao,Min Yang,Derek F. Wong*

Main category: cs.CL

TL;DR: 本文首次系统分析大推理模型（LRM）作为机器翻译评估器，指出挑战并提出校准LRM思考的方法，实验显示能减少思考预算并提升评估性能。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在复杂下游任务推理能力提升，但作为机器翻译质量评估器的潜力未充分挖掘，因此进行系统分析。

Method: 通过在合成的类人思考轨迹上训练来校准LRM的思考。

Result: 在WMT24指标基准实验中，该方法减少约35倍思考预算，在7B到32B不同规模LRM上提升评估性能，如R1 - Distill - Qwen - 7B相关性提高8.7个点。

Conclusion: 有效校准的LRM有潜力推动细粒度自动机器翻译评估。

Abstract: Recent advancements in large reasoning models (LRMs) have introduced an
intermediate "thinking" process prior to generating final answers, improving
their reasoning capabilities on complex downstream tasks. However, the
potential of LRMs as evaluators for machine translation (MT) quality remains
underexplored. We provides the first systematic analysis of LRM-as-a-judge in
MT evaluation. We identify key challenges, revealing LRMs require tailored
evaluation materials, tend to "overthink" simpler instances and have issues
with scoring mechanisms leading to overestimation. To address these, we propose
to calibrate LRM thinking by training them on synthetic, human-like thinking
trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this
approach largely reduces thinking budgets by ~35x while concurrently improving
evaluation performance across different LRM scales from 7B to 32B (e.g.,
R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These
findings highlight the potential of efficiently calibrated LRMs to advance
fine-grained automatic MT evaluation.

</details>


### [320] [A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance in LLM-generated Text](https://arxiv.org/abs/2510.20782)
*Alicia Sagae,Chia-Jung Lee,Sandeep Avula,Brandon Dang,Vanessa Murdock*

Main category: cs.CL

TL;DR: 构建由真实应用驱动的数据集，用于评估大语言模型在质量、真实性、安全性和公平性方面的差距。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评估方法未针对特定AI应用，不足以评估负责任AI维度如公平性。

Method: 构建由真实应用驱动、以公平属性与性别形容词和产品类别交叉参数化的数据集。

Result: 展示了如何使用数据识别大语言模型在质量、真实性、安全性和公平性方面的差距。

Conclusion: 为大语言模型评估提出建议，并为研究界提供具体资源。

Abstract: Current methods for evaluating large language models (LLMs) typically focus
on high-level tasks such as text generation, without targeting a particular AI
application. This approach is not sufficient for evaluating LLMs for
Responsible AI dimensions like fairness, since protected attributes that are
highly relevant in one application may be less relevant in another. In this
work, we construct a dataset that is driven by a real-world application
(generate a plain-text product description, given a list of product features),
parameterized by fairness attributes intersected with gendered adjectives and
product categories, yielding a rich set of labeled prompts. We show how to use
the data to identify quality, veracity, safety, and fairness gaps in LLMs,
contributing a proposal for LLM evaluation paired with a concrete resource for
the research community.

</details>


### [321] [Simple Context Compression: Mean-Pooling and Multi-Ratio Training](https://arxiv.org/abs/2510.20797)
*Yair Feldman,Yoav Artzi*

Main category: cs.CL

TL;DR: 提出轻量级平均池化方法用于检索增强生成的软上下文压缩，实验显示其性能强，多压缩比训练时性能下降小，同时展现压缩方法权衡的复杂性。


<details>
  <summary>Details</summary>
Motivation: 降低大语言模型检索增强生成中使用长上下文的计算成本。

Method: 开发轻量级平均池化方法，并研究训练同一压缩器输出多种压缩比，在不同数据集、模型族、规模和压缩比下进行实验。

Result: 简单平均池化方法性能最佳，多压缩比训练时性能下降相对较小，不同架构和训练机制下权衡更微妙。

Conclusion: 压缩方法的情况复杂，需综合考虑不同因素。

Abstract: A common strategy to reduce the computational costs of using long contexts in
retrieval-augmented generation (RAG) with large language models (LLMs) is soft
context compression, where the input sequence is transformed into a shorter
continuous representation. We develop a lightweight and simple mean-pooling
approach that consistently outperforms the widely used compression-tokens
architecture, and study training the same compressor to output multiple
compression ratios. We conduct extensive experiments across in-domain and
out-of-domain QA datasets, as well as across model families, scales, and
compression ratios. Overall, our simple mean-pooling approach achieves the
strongest performance, with a relatively small drop when training for multiple
compression ratios. More broadly though, across architectures and training
regimes the trade-offs are more nuanced, illustrating the complex landscape of
compression methods.

</details>


### [322] [On the Detectability of LLM-Generated Text: What Exactly Is LLM-Generated Text?](https://arxiv.org/abs/2510.20810)
*Mingmeng Geng,Thierry Poibeau*

Main category: cs.CL

TL;DR: 研究大语言模型生成文本检测问题，指出定义不明确、检测困难、现有评估方法不足，检测结果应仅作参考。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛使用，研究其生成文本的检测问题。

Method: 无明确提及

Result: 现有基准和评估方法无法满足实际检测应用的各种情况，检测结果常被误解且重要性降低。

Conclusion: 检测器在特定条件下有用，但结果应仅作为参考，而非决定性指标。

Abstract: With the widespread use of large language models (LLMs), many researchers
have turned their attention to detecting text generated by them. However, there
is no consistent or precise definition of their target, namely "LLM-generated
text". Differences in usage scenarios and the diversity of LLMs further
increase the difficulty of detection. What is commonly regarded as the
detecting target usually represents only a subset of the text that LLMs can
potentially produce. Human edits to LLM outputs, together with the subtle
influences that LLMs exert on their users, are blurring the line between
LLM-generated and human-written text. Existing benchmarks and evaluation
approaches do not adequately address the various conditions in real-world
detector applications. Hence, the numerical results of detectors are often
misunderstood, and their significance is diminishing. Therefore, detectors
remain useful under specific conditions, but their results should be
interpreted only as references rather than decisive indicators.

</details>


### [323] [Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention and Contextualized Learnable Token Eviction](https://arxiv.org/abs/2510.20787)
*Mutian He,Philip N. Garner*

Main category: cs.CL

TL;DR: 线性注意力模型有遗忘问题，本文提出混合模型，包括新的可学习令牌驱逐方法，结合滑动窗口注意力，有高效内核，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 线性注意力模型有限内存导致遗忘问题，影响检索密集型任务。

Method: 探索一系列混合模型，提出可学习令牌驱逐方法，结合滑动窗口注意力，用轻量级CNN聚合信息，提供高效Triton内核。

Result: 在检索密集型基准测试上的实验支持了所提方法的有效性。

Conclusion: 所提出的混合模型及相关方法能有效缓解线性注意力模型的遗忘问题。

Abstract: Linear-attention models that compress the entire input sequence into a
fixed-size recurrent state offer an efficient alternative to Transformers, but
their finite memory induces forgetfulness that harms retrieval-intensive tasks.
To mitigate the issue, we explore a series of hybrid models that restore direct
access to past tokens. We interleave token mixers with intermediate time and
space complexity between linear and full attention, including sparse attention
with token eviction, and the query-aware native sparse attention. Particularly,
we propose a novel learnable token eviction approach. Combined with
sliding-window attention, an end-to-end trainable lightweight CNN aggregates
information from both past and future adjacent tokens to adaptively retain a
limited set of critical KV-pairs per head, maintaining linear attention's
constant time and space complexity. Efficient Triton kernels for the sparse
attention mechanisms are provided. Empirical evaluations on retrieval-intensive
benchmarks support the effectiveness of our approaches.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [324] [Optimizing Feature Ordering in Radar Charts for Multi-Profile Comparison](https://arxiv.org/abs/2510.20738)
*Albert Dorador*

Main category: cs.HC

TL;DR: 提出一种排列优化策略，对雷达图特征重新排序以提升视觉清晰度，并讨论其复杂度等。


<details>
  <summary>Details</summary>
Motivation: 雷达图在特征值大幅交替时视觉清晰度受影响，需改进。

Method: 采用组合（穷举搜索）方法，使用字典序极小极大准则，先考虑整体平滑度，再以最大单跳作为决胜条件。

Result: 能保留更多全局信息，产生视觉平衡的排列，通过例子展示了定性改进。

Conclusion: 该排列优化策略可有效提升雷达图的视觉效果。

Abstract: Radar charts are widely used to visualize multivariate data and compare
multiple profiles across features. However, the visual clarity of radar charts
can be severely compromised when feature values alternate drastically in
magnitude around the circle, causing areas to collapse, which misrepresents
relative differences. In the present work we introduce a permutation
optimization strategy that reorders features to minimize polygon ``spikiness''
across multiple profiles simultaneously. The method is combinatorial
(exhaustive search) for moderate numbers of features and uses a lexicographic
minimax criterion that first considers overall smoothness (mean jump) and then
the largest single jump as a tie-breaker. This preserves more global
information and produces visually balanced arrangements. We discuss complexity,
practical bounds, and relations to existing approaches that either change the
visualization (e.g., OrigamiPlot) or learn orderings (e.g., Versatile Ordering
Network). An example with two profiles and $p=6$ features (before/after
ordering) illustrates the qualitative improvement.
  Keywords: data visualization, radar charts, combinatorial optimization,
minimax optimization, feature ordering

</details>


### [325] [Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn Human-LLM Interactions](https://arxiv.org/abs/2510.20039)
*Yuyang Jiang,Longjie Guo,Yuchen Wu,Aylin Caliskan,Tanu Mitra,Hua Shen*

Main category: cs.HC

TL;DR: 研究通过50个争议话题讨论，探究多轮对话中人与大语言模型双向影响，发现人观点变化小、模型输出变化大，个性化设置会放大这种变化，涉及个人故事交流易触发立场改变。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注大语言模型对用户观点的单向影响，本研究旨在探究多轮对话中用户输入对大语言模型响应的影响及双向影响的表现。

Method: 与266名参与者进行50个争议话题讨论，设置静态陈述、标准聊天机器人和个性化聊天机器人三种条件。

Result: 人类观点几乎未变，大语言模型输出变化较大，缩小了人与模型立场差距；个性化设置放大双方立场变化；涉及个人故事的交流最易触发立场改变。

Conclusion: 强调人机交互中过度对齐的风险，以及个性化聊天机器人需谨慎设计以更周全稳定地与用户对齐。

Abstract: Large language model (LLM)-powered chatbots are increasingly used for opinion
exploration. Prior research examined how LLMs alter user views, yet little work
extended beyond one-way influence to address how user input can affect LLM
responses and how such bi-directional influence manifests throughout the
multi-turn conversations. This study investigates this dynamic through 50
controversial-topic discussions with participants (N=266) across three
conditions: static statements, standard chatbot, and personalized chatbot.
Results show that human opinions barely shifted, while LLM outputs changed more
substantially, narrowing the gap between human and LLM stance. Personalization
amplified these shifts in both directions compared to the standard setting.
Analysis of multi-turn conversations further revealed that exchanges involving
participants' personal stories were most likely to trigger stance changes for
both humans and LLMs. Our work highlights the risk of over-alignment in
human-LLM interaction and the need for careful design of personalized chatbots
to more thoughtfully and stably align with users.

</details>


### [326] [Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations](https://arxiv.org/abs/2510.20743)
*Lorenzo Stacchio,Andrea Ubaldi,Alessandro Galdelli,Maurizio Mauri,Emanuele Frontoni,Andrea Gaggioli*

Main category: cs.HC

TL;DR: 提出Empathic Prompting框架，将用户情感线索融入大语言模型对话，初步评估显示能提升对话流畅度，有潜在应用价值。


<details>
  <summary>Details</summary>
Motivation: 为多模态人机交互丰富大语言模型对话的非语言上下文，使交互更自然流畅。

Method: 集成商业面部表情识别服务，将用户情感线索作为上下文信号嵌入提示中，通过本地部署的DeepSeek实例实现系统设计，并进行初步服务和可用性评估。

Result: 非语言输入能持续整合到连贯的大语言模型输出中，参与者认为对话更流畅。

Conclusion: Empathic Prompting框架有可行性，在医疗、教育等领域的聊天机器人通信中有应用前景。

Abstract: We present Empathic Prompting, a novel framework for multimodal human-AI
interaction that enriches Large Language Model (LLM) conversations with
implicit non-verbal context. The system integrates a commercial facial
expression recognition service to capture users' emotional cues and embeds them
as contextual signals during prompting. Unlike traditional multimodal
interfaces, empathic prompting requires no explicit user control; instead, it
unobtrusively augments textual input with affective information for
conversational and smoothness alignment. The architecture is modular and
scalable, allowing integration of additional non-verbal modules. We describe
the system design, implemented through a locally deployed DeepSeek instance,
and report a preliminary service and usability evaluation (N=5). Results show
consistent integration of non-verbal input into coherent LLM outputs, with
participants highlighting conversational fluidity. Beyond this proof of
concept, empathic prompting points to applications in chatbot-mediated
communication, particularly in domains like healthcare or education, where
users' emotional signals are critical yet often opaque in verbal exchanges.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [327] [Extending machine learning model for implicit solvation to free energy calculations](https://arxiv.org/abs/2510.20103)
*Rishabh Dey,Michael Brocidiacono,Kushal Koirala,Alexander Tropsha,Konstantin I. Popov*

Main category: physics.chem-ph

TL;DR: 介绍基于图神经网络的隐式溶剂模型LSNN，可准确预测溶剂化自由能，有计算加速优势。


<details>
  <summary>Details</summary>
Motivation: 传统隐式溶剂方法精度不足，现有基于机器学习的方法依赖力匹配，不适用于绝对自由能比较，需开发更精确的隐式溶剂势。

Method: 引入基于图神经网络的隐式溶剂模型LSNN，除力匹配外，还训练网络匹配炼金术变量的导数。

Result: 在约30万个小分子数据集上训练，LSNN实现的自由能预测精度与显式溶剂炼金术模拟相当。

Conclusion: LSNN提供计算加速，为药物发现等未来应用奠定基础。

Abstract: The implicit solvent approach offers a computationally efficient framework to
model solvation effects in molecular simulations. However, its accuracy often
falls short compared to explicit solvent models, limiting its use in precise
thermodynamic calculations. Recent advancements in machine learning (ML)
present an opportunity to overcome these limitations by leveraging neural
networks to develop more precise implicit solvent potentials for diverse
applications. A major drawback of current ML-based methods is their reliance on
force-matching alone, which can lead to energy predictions that differ by an
arbitrary constant and are therefore unsuitable for absolute free energy
comparisons. Here, we introduce a novel methodology with a graph neural network
(GNN)-based implicit solvent model, dubbed Lambda Solvation Neural Network
(LSNN). In addition to force-matching, this network was trained to match the
derivatives of alchemical variables, ensuring that solvation free energies can
be meaningfully compared across chemical species.. Trained on a dataset of
approximately 300,000 small molecules, LSNN achieves free energy predictions
with accuracy comparable to explicit-solvent alchemical simulations, while
offering a computational speedup and establishing a foundational framework for
future applications in drug discovery.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [328] [Symbolic Regression and Differentiable Fits in Beyond the Standard Model Physics](https://arxiv.org/abs/2510.20453)
*Shehu AbdusSalam,Steven Abel,Deaglan Bartlett,Miguel Crispim Romão*

Main category: hep-ph

TL;DR: 本文通过CMSSM模型展示了符号回归（SR）在探索超越标准模型的粒子物理模型方面的有效性，表明SR能生成准确表达式，可用于全局拟合，且比神经网络回归更具全局鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索符号回归（SR）在超越标准模型（BSM）的粒子物理模型研究中的有效性。

Method: 以约束最小超对称标准模型（CMSSM）为例，利用符号回归生成可观测量关于输入参数的符号表达式，进行全局拟合，并与神经网络回归比较。

Result: SR能产生非常准确的表达式，全局拟合得到的CMSSM输入参数后验概率密度与传统方法一致；SR可使用可微方法进行拟合；与神经网络回归相比，SR结果更具全局鲁棒性。

Conclusion: 符号回归（SR）在探索BSM物理模型方面是有效的，具有比传统方法和神经网络回归的优势。

Abstract: We demonstrate the efficacy of symbolic regression (SR) to probe models of
particle physics Beyond the Standard Model (BSM), by considering the so-called
Constrained Minimal Supersymmetric Standard Model (CMSSM). Like many
incarnations of BSM physics this model has a number (four) of arbitrary
parameters, which determine the experimental signals, and cosmological
observables such as the dark matter relic density. We show that analysis of the
phenomenology can be greatly accelerated by using symbolic expressions derived
for the observables in terms of the input parameters. Here we focus on the
Higgs mass, the cold dark matter relic density, and the contribution to the
anomalous magnetic moment of the muon. We find that SR can produce remarkably
accurate expressions. Using them we make global fits to derive the posterior
probability densities of the CMSSM input parameters which are in good agreement
with those performed using conventional methods. Moreover, we demonstrate a
major advantage of SR which is the ability to make fits using differentiable
methods rather than sampling methods. We also compare the method with neural
network (NN) regression. SR produces more globally robust results, while NNs
require data that is focussed on the promising regions in order to be equally
performant.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [329] [Bayesian Inference of Primordial Magnetic Field Parameters from CMB with Spherical Graph Neural Networks](https://arxiv.org/abs/2510.20795)
*Juan Alejandro Pinto Castro,Héctor J. Hortúa,Jorge Enrique García-Farieta,Roger Anderson Hurtado*

Main category: astro-ph.CO

TL;DR: 本文提出一种贝叶斯图深度学习框架，用DeepSphere架构和贝叶斯神经网络从模拟CMB地图估计原初磁场宇宙学关键参数，表现出色且能提供可靠不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 利用深度学习从复杂天文数据集提取物理信息，在原初磁场宇宙学中准确估计关键参数并进行不确定性量化。

Method: 实现基于DeepSphere架构的贝叶斯图深度学习框架，将贝叶斯神经网络集成到框架中，并采用事后训练技术。

Result: 磁参数估计的R²分数超0.89，通过事后训练技术获得校准良好的不确定性估计。

Conclusion: 该集成框架能从含原初磁场贡献的CMB地图准确估计参数，提供可靠不确定性量化，为精确宇宙学时代的宇宙学推理提供工具。

Abstract: Deep learning has emerged as a transformative methodology in modern
cosmology, providing powerful tools to extract meaningful physical information
from complex astronomical datasets. This paper implements a novel Bayesian
graph deep learning framework for estimating key cosmological parameters in a
primordial magnetic field (PMF) cosmology directly from simulated Cosmic
Microwave Background (CMB) maps. Our methodology utilizes DeepSphere, a
spherical convolutional neural network architecture specifically designed to
respect the spherical geometry of CMB data through HEALPix pixelization. To
advance beyond deterministic point estimates and enable robust uncertainty
quantification, we integrate Bayesian Neural Networks (BNNs) into the
framework, capturing aleatoric and epistemic uncertainties that reflect the
model confidence in its predictions. The proposed approach demonstrates
exceptional performance, achieving $R^{2}$ scores exceeding 0.89 for the
magnetic parameter estimation. We further obtain well-calibrated uncertainty
estimates through post-hoc training techniques including Variance Scaling and
GPNormal. This integrated DeepSphere-BNNs framework not only delivers accurate
parameter estimation from CMB maps with PMF contributions but also provides
reliable uncertainty quantification, providing the necessary tools for robust
cosmological inference in the era of precision cosmology.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [330] [Approximate Model Predictive Control for Microgrid Energy Management via Imitation Learning](https://arxiv.org/abs/2510.20040)
*Changrui Liu,Shengling Shi,Anil Alan,Ganesh Kumar Venayagamoorthy,Bart De Schutter*

Main category: eess.SY

TL;DR: 本文提出基于模仿学习的框架近似微电网能源管理的混合整数经济模型预测控制，训练神经网络模仿专家控制动作，结果显示该策略经济性能与EMPC相当，计算时间仅为其10%。


<details>
  <summary>Details</summary>
Motivation: 在可再生能源集成增加的情况下，实现可靠且可持续的微电网高效能源管理。

Method: 提出基于模仿学习的框架，训练神经网络模仿专家EMPC控制动作，训练中注入噪声并考虑预测不确定性。

Result: 学习到的策略经济性能与EMPC相当，实际计算时间仅为基于优化的EMPC的10%。

Conclusion: 所提出的基于模仿学习的框架可有效近似微电网能源管理的混合整数EMPC，在保证经济性能的同时大幅减少计算时间。

Abstract: Efficient energy management is essential for reliable and sustainable
microgrid operation amid increasing renewable integration. This paper proposes
an imitation learning-based framework to approximate mixed-integer Economic
Model Predictive Control (EMPC) for microgrid energy management. The proposed
method trains a neural network to imitate expert EMPC control actions from
offline trajectories, enabling fast, real-time decision making without solving
optimization problems online. To enhance robustness and generalization, the
learning process includes noise injection during training to mitigate
distribution shift and explicitly incorporates forecast uncertainty in
renewable generation and demand. Simulation results demonstrate that the
learned policy achieves economic performance comparable to EMPC while only
requiring $10\%$ of the computation time of optimization-based EMPC in
practice.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [331] [Capability of using the normalizing flows for extraction rare gamma events in the TAIGA experiment](https://arxiv.org/abs/2510.20334)
*A. P. Kryukov,A. Yu. Razumov,A. P. Demichev,J. J. Dubenskaya,E. O. Gres,S. P. Polyakov,E. B. Postnikov,P. A. Volchugov,D. P. Zhurov*

Main category: astro-ph.IM

TL;DR: 本文用深度学习和归一化流方法检测宇宙源通量中带电粒子背景下的稀有伽马量子，在模型数据上测试，性能指标欠佳并提出改进方法。


<details>
  <summary>Details</summary>
Motivation: 开发一种在宇宙源通量的带电粒子背景下检测稀有伽马量子的方法。

Method: 使用基于深度学习和归一化流的异常检测方法。

Result: 该方法有伽马检测潜力，但在TAIGA - IACT实验的模型数据上测试，定量性能指标不如其他方法。

Conclusion: 提出了改进该方法实施的可能途径。

Abstract: The objective of this work is to develop a method for detecting rare gamma
quanta against the background of charged particles in the fluxes from sources
in the Universe with the help of the deep learning and normalizing flows based
method designed for anomaly detection. It is shown that the suggested method
has a potential for the gamma detection. The method was tested on model data
from the TAIGA-IACT experiment. The obtained quantitative performance
indicators are still inferior to other approaches, and therefore possible ways
to improve the implementation of the method are proposed.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [332] [Endogenous Aggregation of Multiple Data Envelopment Analysis Scores for Large Data Sets](https://arxiv.org/abs/2510.20052)
*Hashem Omrani,Raha Imanirad,Adam Diamant,Utkarsh Verma,Amol Verma,Fahad Razak*

Main category: math.OC

TL;DR: 提出用DEA进行多组织维度动态效率评估的方法，引入SBM和GP - SBM模型，经多数据集验证有效，应用于医院案例并优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决多组织维度动态效率评估问题，提供更有效的评估方法。

Method: 提出SBM和GP - SBM两种正则化DEA模型，利用正则化参数增强区分能力，整合期望和非期望产出。

Result: 模型在多数据集上展示了计算效率和有效性，应用于加拿大安大略省医院案例，能更好捕捉输入/输出变量间相关性。

Conclusion: SBM和GP - SBM模型优于传统先分别评估各维度再汇总的基准方法。

Abstract: We propose an approach for dynamic efficiency evaluation across multiple
organizational dimensions using data envelopment analysis (DEA). The method
generates both dimension-specific and aggregate efficiency scores, incorporates
desirable and undesirable outputs, and is suitable for large-scale problem
settings. Two regularized DEA models are introduced: a slack-based measure
(SBM) and a linearized version of a nonlinear goal programming model (GP-SBM).
While SBM estimates an aggregate efficiency score and then distributes it
across dimensions, GP-SBM first estimates dimension-level efficiencies and then
derives an aggregate score. Both models utilize a regularization parameter to
enhance discriminatory power while also directly integrating both desirable and
undesirable outputs. We demonstrate the computational efficiency and validity
of our approach on multiple datasets and apply it to a case study of twelve
hospitals in Ontario, Canada, evaluating three theoretically grounded
dimensions of organizational effectiveness over a 24-month period from January
2018 to December 2019: technical efficiency, clinical efficiency, and patient
experience. Our numerical results show that SBM and GP-SBM better capture
correlations among input/output variables and outperform conventional
benchmarking methods that separately evaluate dimensions before aggregation.

</details>


### [333] [Isotropic Noise in Stochastic and Quantum Convex Optimization](https://arxiv.org/abs/2510.20745)
*Annie Marsden,Liam O'Carroll,Aaron Sidford,Chenyi Zhang*

Main category: math.OC

TL;DR: 本文研究用随机梯度预言机最小化d维Lipschitz凸函数问题，提出各向同性噪声设定，给出算法改进结果、匹配下界，开发量子各向同性化器，获量子随机凸优化改进率。


<details>
  <summary>Details</summary>
Motivation: 解决用随机梯度预言机最小化d维Lipschitz凸函数问题，考虑各向同性噪声设定以改进结果。

Method: 引入各向同性噪声设定，开发算法，给出匹配下界，开发量子各向同性化器。

Result: 算法在特定情况下比先前结果有d倍改进，获次指数噪声新的最优复杂度，给出匹配下界，获量子随机凸优化改进率。

Conclusion: 在随机和量子随机凸优化方面取得改进结果，有理论和实际意义。

Abstract: We consider the problem of minimizing a $d$-dimensional Lipschitz convex
function using a stochastic gradient oracle. We introduce and motivate a
setting where the noise of the stochastic gradient is isotropic in that it is
bounded in every direction with high probability. We then develop an algorithm
for this setting which improves upon prior results by a factor of $d$ in
certain regimes, and as a corollary, achieves a new state-of-the-art complexity
for sub-exponential noise. We give matching lower bounds (up to polylogarithmic
factors) for both results. Additionally, we develop an efficient quantum
isotropifier, a quantum algorithm which converts a variance-bounded quantum
sampling oracle into one that outputs an unbiased estimate with isotropic
error. Combining our results, we obtain improved dimension-dependent rates for
quantum stochastic convex optimization.

</details>


### [334] [Balancing Gradient and Hessian Queries in Non-Convex Optimization](https://arxiv.org/abs/2510.20786)
*Deeksha Adil,Brian Bullins,Aaron Sidford,Chenyi Zhang*

Main category: math.OC

TL;DR: 本文开发优化方法，在计算非凸函数临界点时权衡梯度和Hessian计算次数，给出特定条件下输出ε - 临界点的方法及不同情况下梯度查询复杂度。


<details>
  <summary>Details</summary>
Motivation: 为计算非凸函数临界点，在梯度和Hessian计算次数间找到新的权衡。

Method: 开发通用算法，可处理近似Hessian计算。

Result: 得到在特定条件下输出ε - 临界点所需的梯度和Hessian查询次数，以及不同情形下的梯度查询复杂度，还能恢复现有最优界限。

Conclusion: 所开发的优化方法能有效计算非凸函数的ε - 临界点，并在不同条件下给出了较好的查询复杂度。

Abstract: We develop optimization methods which offer new trade-offs between the number
of gradient and Hessian computations needed to compute the critical point of a
non-convex function. We provide a method that for any twice-differentiable
$f\colon \mathbb R^d \rightarrow \mathbb R$ with $L_2$-Lipschitz Hessian, input
initial point with $\Delta$-bounded sub-optimality, and sufficiently small
$\epsilon > 0$, outputs an $\epsilon$-critical point, i.e., a point $x$ such
that $\|\nabla f(x)\| \leq \epsilon$, using $\tilde{O}(L_2^{1/4}
n_H^{-1/2}\Delta\epsilon^{-9/4})$ queries to a gradient oracle and $n_H$
queries to a Hessian oracle for any positive integer $n_H$. As a consequence,
we obtain an improved gradient query complexity of
$\tilde{O}(d^{1/3}L_2^{1/2}\Delta\epsilon^{-3/2})$ in the case of bounded
dimension and of $\tilde{O}(L_2^{3/4}\Delta^{3/2}\epsilon^{-9/4})$ in the case
where we are allowed only a \emph{single} Hessian query. We obtain these
results through a more general algorithm which can handle approximate Hessian
computations and recovers the state-of-the-art bound of computing an
$\epsilon$-critical point with $O(L_1^{1/2}L_2^{1/4}\Delta\epsilon^{-7/4})$
  gradient queries provided that $f$ also has an $L_1$-Lipschitz gradient.

</details>


### [335] [GPU-Accelerated Primal Heuristics for Mixed Integer Programming](https://arxiv.org/abs/2510.20499)
*Akif Çördük,Piotr Sielski,Alice Boucher,Kumar Aatish*

Main category: math.OC

TL;DR: 本文介绍了用于混合整数规划的GPU加速原始启发式方法融合，结合多种算法在MIPLIB2017基准测试中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 利用GPU加速以探索更大搜索区域和实现更快迭代，改进混合整数规划的求解效果。

Method: 采用GPU加速的PDLP作为近似LP求解器，使用新的探测缓存，加速并增强了几种先进启发式方法。

Result: 在MIPLIB2017基准测试的预求解数据集上获得221个可行解和22%的目标差距。

Conclusion: GPU驱动的算法组合在可行解数量和目标质量上比现有方法有显著提升。

Abstract: We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer
Programming. Leveraging GPU acceleration enables exploration of larger search
regions and faster iterations. A GPU-accelerated PDLP serves as an approximate
LP solver, while a new probing cache facilitates rapid roundings and early
infeasibility detection. Several state-of-the-art heuristics, including
Feasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further
accelerated and enhanced. The combined approach of these GPU-driven algorithms
yields significant improvements over existing methods, both in the number of
feasible solutions and the quality of objectives by achieving 221 feasible
solutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved
dataset.

</details>


### [336] [Simultaneously Solving Infinitely Many LQ Mean Field Games In Hilbert Spaces: The Power of Neural Operators](https://arxiv.org/abs/2510.20017)
*Dena Firoozi,Anastasis Kratsios,Xuwei Yang*

Main category: math.OC

TL;DR: 传统平均场博弈（MFG）求解器逐例运行，在求解大量相关问题时不可行，本文训练神经算子（NO）学习从问题数据到均衡策略的映射，给出统计保证。


<details>
  <summary>Details</summary>
Motivation: 传统MFG求解器在求解大量相关问题时不可行，需要更有效的方法。

Method: 训练神经算子学习从问题数据到均衡策略的映射，利用局部Lipschitz估计、通用逼近定理和新的样本复杂度界。

Result: 训练的NO能可靠求解未见的线性二次MFG变体，所需NO参数数量可控。

Conclusion: 通过训练NO可有效解决传统MFG求解器的局限性，在无限维设置中也适用。

Abstract: Traditional mean-field game (MFG) solvers operate on an instance-by-instance
basis, which becomes infeasible when many related problems must be solved
(e.g., for seeking a robust description of the solution under perturbations of
the dynamics or utilities, or in settings involving continuum-parameterized
agents.). We overcome this by training neural operators (NOs) to learn the
rules-to-equilibrium map from the problem data (``rules'': dynamics and cost
functionals) of LQ MFGs defined on separable Hilbert spaces to the
corresponding equilibrium strategy. Our main result is a statistical guarantee:
an NO trained on a small number of randomly sampled rules reliably solves
unseen LQ MFG variants, even in infinite-dimensional settings. The number of NO
parameters needed remains controlled under appropriate rule sampling during
training.
  Our guarantee follows from three results: (i) local-Lipschitz estimates for
the highly nonlinear rules-to-equilibrium map; (ii) a universal approximation
theorem using NOs with a prespecified Lipschitz regularity (unlike traditional
NO results where the NO's Lipschitz constant can diverge as the approximation
error vanishes); and (iii) new sample-complexity bounds for $L$-Lipschitz
learners in infinite dimensions, directly applicable as the Lipschitz constants
of our approximating NOs are controlled in (ii).

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [337] [Partial Optimality in Cubic Correlation Clustering for General Graphs](https://arxiv.org/abs/2510.20431)
*David Stein,Bjoern Andres,Silvia Di Gregorio*

Main category: cs.DM

TL;DR: 本文研究图的高阶相关聚类问题，针对立方相关聚类建立部分最优条件，定义并实现算法，通过两个数据集验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决图的高阶相关聚类这一NP难问题，该问题在实际应用中有需求。

Method: 针对立方相关聚类（最多3 - 团的特殊情况）建立部分最优条件，定义并实现用于判断这些条件的算法。

Result: 在两个数据集上对算法有效性进行了数值检验。

Conclusion: 文档未明确给出结论，但暗示所定义和实现的算法在解决立方相关聚类问题上有一定价值。

Abstract: The higher-order correlation clustering problem for a graph $G$ and costs
associated with cliques of $G$ consists in finding a clustering of $G$ so as to
minimize the sum of the costs of those cliques whose nodes all belong to the
same cluster. To tackle this NP-hard problem in practice, local search
heuristics have been proposed and studied in the context of applications. Here,
we establish partial optimality conditions for cubic correlation clustering,
i.e., for the special case of at most 3-cliques. We define and implement
algorithms for deciding these conditions and examine their effectiveness
numerically, on two data sets.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [338] [Asynchronous Distributed ECME Algorithm for Matrix Variate Non-Gaussian Responses](https://arxiv.org/abs/2510.20147)
*Qingyang Liu,Sanvesh Srivastava,Dipankar Bandyopadhyay*

Main category: stat.ME

TL;DR: 提出REGMVST模型分析不规则纵向数据，开发ADECME算法估计参数，模拟和案例研究显示其效率和收敛性优势，有R包。


<details>
  <summary>Details</summary>
Motivation: 分析具有偏态、对称或重尾特征的不规则纵向数据。

Method: 提出REGMVST模型，使用MVST分布处理偏态和重尾，DEC结构处理行依赖；开发ECME算法估计参数，用ADECME算法缓解计算瓶颈。

Result: 模拟和案例研究表明ADECME在效率和收敛性上优于其他方法。

Conclusion: ADECME算法有效，论文给出理论支持和正则性假设，提供R包。

Abstract: We propose a regression model with matrix-variate skew-t response (REGMVST)
for analyzing irregular longitudinal data with skewness, symmetry, or heavy
tails. REGMVST models matrix-variate responses and predictors, with rows
indexing longitudinal measurements per subject. It uses the matrix-variate
skew-t (MVST) distribution to handle skewness and heavy tails, a damped
exponential correlation (DEC) structure for row-wise dependencies across
irregular time profiles, and leaves the column covariance unstructured. For
estimation, we initially develop an ECME algorithm for parameter estimation and
further mitigate its computational bottleneck via an asynchronous and
distributed ECME (ADECME) extension. ADECME accelerates the E-step through
parallelization, and retains the simplicity of the conditional M-step, enabling
scalable inference. Simulations using synthetic data and a case study exploring
matrix-variate periodontal disease endpoints derived from electronic health
records demonstrate ADECME's superiority in efficiency and convergence, over
the alternatives. We also provide theoretical support for our empirical
observations and identify regularity assumptions for ADECME's optimal
performance. An accompanying R package is available at
https://github.com/rh8liuqy/STMATREG.

</details>


### [339] [Identification and Debiased Learning of Causal Effects with General Instrumental Variables](https://arxiv.org/abs/2510.20404)
*Shuyuan Chen,Peng Zhang,Yifan Cui*

Main category: stat.ME

TL;DR: 本文提出多分类或连续工具变量的非参数框架，推导有效影响函数构建估计量，拓展到多种情况并通过模拟和真实数据验证。


<details>
  <summary>Details</summary>
Motivation: 在处理分配受未观测变量混淆时，工具变量方法对因果推断至关重要，需开发多分类或连续工具变量的非参数框架。

Method: 提出加性工具变量框架，利用半参数理论推导有效影响函数，通过去偏机器学习构建估计量，拓展到纵向数据等情况。

Result: 通过模拟研究和分析职业培训合作法案项目的真实数据验证了所提方法。

Conclusion: 所开发的非参数框架可用于多分类或连续工具变量的识别和学习，且有多种拓展应用。

Abstract: Instrumental variable methods are fundamental to causal inference when
treatment assignment is confounded by unobserved variables. In this article, we
develop a general nonparametric framework for identification and learning with
multi-categorical or continuous instrumental variables. Specifically, we
propose an additive instrumental variable framework to identify mean potential
outcomes and the average treatment effect with a weighting function. Leveraging
semiparametric theory, we derive efficient influence functions and construct
consistent, asymptotically normal estimators via debiased machine learning.
Extensions to longitudinal data, dynamic treatment regimes, and multiplicative
instrumental variables are further developed. We demonstrate the proposed
method by employing simulation studies and analyzing real data from the Job
Training Partnership Act program.

</details>


### [340] [On Multiple Robustness of Proximal Dynamic Treatment Regimes](https://arxiv.org/abs/2510.20451)
*Yuanshan Gao,Yang Bai,Yifan Cui*

Main category: stat.ME

TL;DR: 本文利用近端因果推断框架，在无混杂假设不成立时学习最优动态治疗方案，提出多种方法并验证了其效率和稳健性。


<details>
  <summary>Details</summary>
Motivation: 通过序贯随机试验估计最优动态治疗方案可能面临成本和伦理障碍，常需使用历史观察数据，且无混杂假设可能不成立。

Method: 利用近端因果推断框架，提出三种非参数识别方法、确定半参数效率界、提出(K + 1) - 稳健方法，还对边际结构模型进行反事实均值的识别和估计。

Result: 数值实验验证了所提方法的效率和多重稳健性。

Conclusion: 所提方法能在无混杂假设不成立时有效学习最优动态治疗方案。

Abstract: Dynamic treatment regimes are sequential decision rules that adapt treatment
according to individual time-varying characteristics and outcomes to achieve
optimal effects, with applications in precision medicine, personalized
recommendations, and dynamic marketing. Estimating optimal dynamic treatment
regimes via sequential randomized trials might face costly and ethical hurdles,
often necessitating the use of historical observational data. In this work, we
utilize proximal causal inference framework for learning optimal dynamic
treatment regimes when the unconfoundedness assumption fails. Our contributions
are four-fold: (i) we propose three nonparametric identification methods for
optimal dynamic treatment regimes; (ii) we establish the semiparametric
efficiency bound for the value function of a given regime; (iii) we propose a
(K+1)-robust method for learning optimal dynamic treatment regimes, where K is
the number of stages; (iv) as a by-product for marginal structural models, we
establish identification and estimation of counterfactual means under a static
regime. Numerical experiments validate the efficiency and multiple robustness
of our proposed methods.

</details>


### [341] [Throwing Vines at the Wall: Structure Learning via Random Search](https://arxiv.org/abs/2510.20035)
*Thibault Vatter,Thomas Nagler*

Main category: stat.ME

TL;DR: 提出随机搜索算法和统计框架用于藤copula结构学习，在多数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 藤copula结构学习是关键挑战，早期启发式算法常非最优。

Method: 提出随机搜索算法改进结构选择，构建基于模型置信集的统计框架。

Result: 在多个真实数据集上，所提方法始终优于现有方法。

Conclusion: 所提随机搜索算法和统计框架能有效提升藤copula结构学习效果。

Abstract: Vine copulas offer flexible multivariate dependence modeling and have become
widely used in machine learning, yet structure learning remains a key
challenge. Early heuristics like the greedy algorithm of Dissmann are still
considered the gold standard, but often suboptimal. We propose random search
algorithms that improve structure selection and a statistical framework based
on model confidence sets, which provides theoretical guarantees on selection
probabilities and a powerful foundation for ensembling. Empirical results on
several real-world data sets show that our methods consistently outperform
state-of-the-art approaches.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [342] [Incomplete U-Statistics of Equireplicate Designs: Berry-Esseen Bound and Efficient Construction](https://arxiv.org/abs/2510.20755)
*Cesare Miglioli,Jordan Awan*

Main category: math.ST

TL;DR: 本文从超图理论和组合设计角度研究U统计量，绕过传统方法，推导界，引入算法，应用于核基测试并在实际数据中获计算收益。


<details>
  <summary>Details</summary>
Motivation: U统计量存在计算成本高和退化情况下非标准渐近行为的挑战，需要新视角解决。

Method: 基于超图理论和组合设计，绕过传统Hoeffding分解，刻画依赖结构推导Berry - Esseen界，引入算法构建特定设计的不完全U统计量。

Result: 得到适用于所有确定性设计的不完全U统计量的Berry - Esseen界，确定高斯极限分布条件，引入构建等重复设计不完全U统计量的高效算法，应用于核基测试在实际数据中计算成本降低且控制错误率。

Conclusion: 提出的新视角能有效解决U统计量的问题，在实际应用中表现良好。

Abstract: U-statistics are a fundamental class of estimators that generalize the sample
mean and underpin much of nonparametric statistics. Although extensively
studied in both statistics and probability, key challenges remain: their high
computational cost - addressed partly through incomplete U-statistics - and
their non-standard asymptotic behavior in the degenerate case, which typically
requires resampling methods for hypothesis testing. This paper presents a novel
perspective on U-statistics, grounded in hypergraph theory and combinatorial
designs. Our approach bypasses the traditional Hoeffding decomposition, the
main analytical tool in this literature but one highly sensitive to degeneracy.
By characterizing the dependence structure of a U-statistic, we derive a
Berry-Esseen bound that applies to all incomplete U-statistics of deterministic
designs, yielding conditions under which Gaussian limiting distributions can be
established even in the degenerate case and when the order diverges. We also
introduce efficient algorithms to construct incomplete U-statistics of
equireplicate designs, a subclass of deterministic designs that, in certain
cases, achieve minimum variance. Finally, we apply our framework to
kernel-based tests that use Maximum Mean Discrepancy (MMD) and Hilbert-Schmidt
Independence Criterion. In a real data example with CIFAR-10, our
permutation-free MMD test delivers substantial computational gains while
retaining power and type I error control.

</details>


### [343] [Spectral Thresholds in Correlated Spiked Models and Fundamental Limits of Partial Least Squares](https://arxiv.org/abs/2510.17561)
*Pierre Mergny,Lenka Zdeborová*

Main category: math.ST

TL;DR: 对部分对齐的尖峰交叉协方差模型进行随机矩阵理论分析，揭示偏最小二乘法（PLS）信号恢复能力及性能差距，明确其理论极限。


<details>
  <summary>Details</summary>
Motivation: 多模态学习需求，PLS方法理论发展不足。

Method: 对部分对齐的尖峰交叉协方差模型进行严格的随机矩阵理论分析。

Result: 样本交叉协方差矩阵的主导奇异值经历BBP型相变，确定信息成分出现的精确阈值，揭示PLS与贝叶斯最优估计器的性能差距，找出PLS无法恢复信号的SNR和相关区域。

Conclusion: 明确PLS的理论极限，为高维可靠多模态推理方法设计提供指导。

Abstract: We provide a rigorous random matrix theory analysis of spiked
cross-covariance models where the signals across two high-dimensional data
channels are partially aligned. These models are motivated by multi-modal
learning and form the standard generative setting underlying Partial Least
Squares (PLS), a widely used yet theoretically underdeveloped method. We show
that the leading singular values of the sample cross-covariance matrix undergo
a Baik-Ben Arous-Peche (BBP)-type phase transition, and we characterize the
precise thresholds for the emergence of informative components. Our results
yield the first sharp asymptotic description of the signal recovery
capabilities of PLS in this setting, revealing a fundamental performance gap
between PLS and the Bayes-optimal estimator. In particular, we identify the SNR
and correlation regimes where PLS fails to recover any signal, despite
detectability being possible in principle. These findings clarify the
theoretical limits of PLS and provide guidance for the design of reliable
multi-modal inference methods in high dimensions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [344] [Multi-Resolution Analysis of the Convective Structure of Tropical Cyclones for Short-Term Intensity Guidance](https://arxiv.org/abs/2510.19854)
*Elizabeth Cucuzzella,Tria McNeely,Kimberly Wood,Ann B. Lee*

Main category: eess.IV

TL;DR: 提出用离散小波变换的多分辨率分析量化热带气旋精细结构，辅助深度学习进行短期强度预报。


<details>
  <summary>Details</summary>
Motivation: 准确的24小时热带气旋短期强度预报对大西洋热带气旋盆地减灾至关重要，卫星图像虽关键但难实时定性解读。

Method: 采用离散小波变换的多分辨率分析量化热带气旋精细结构，结合深度学习技术。

Result: 能够识别与强度快速变化强相关的有物理意义的结构特征。

Conclusion: 该方法可用于短期强度预报指导。

Abstract: Accurate tropical cyclone (TC) short-term intensity forecasting with a
24-hour lead time is essential for disaster mitigation in the Atlantic TC
basin. Since most TCs evolve far from land-based observing networks, satellite
imagery is critical to monitoring these storms; however, these complex and
high-resolution spatial structures can be challenging to qualitatively
interpret in real time by forecasters. Here we propose a concise,
interpretable, and descriptive approach to quantify fine TC structures with a
multi-resolution analysis (MRA) by the discrete wavelet transform, enabling
data analysts to identify physically meaningful structural features that
strongly correlate with rapid intensity change. Furthermore, deep-learning
techniques can build on this MRA for short-term intensity guidance.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [345] [Evaluating Local Policies in Centralized Markets](https://arxiv.org/abs/2510.20032)
*Dmitry Arkhangelsky,Wisse Rutgers*

Main category: econ.GN

TL;DR: 研究集中市场的政策评估问题，提出边际政策效应（MPE）可非参数识别，提供政策目标优化工具并搭建与边际处理效应文献的桥梁。


<details>
  <summary>Details</summary>
Motivation: 解决集中市场的政策评估问题，识别边际改革的总体影响。

Method: 构建均衡调整结果，利用实证工作中常见的估计量构建外部性，通过结构结果与改革方向的协方差识别MPE。

Result: 证明MPE可非参数识别，且识别方法可作为最优政策目标的灵活工具。

Conclusion: 该研究为集中市场政策评估提供新方法，建立与边际处理效应文献的联系。

Abstract: We study a policy evaluation problem in centralized markets. We show that the
aggregate impact of any marginal reform, the Marginal Policy Effect (MPE), is
nonparametrically identified using data from a baseline equilibrium, without
additional variation in the policy rule. We achieve this by constructing the
equilibrium-adjusted outcome: a policy-invariant structural object that
augments an agent's outcome with the full equilibrium externality their
participation imposes on others. We show that these externalities can be
constructed using estimands that are already common in empirical work. The MPE
is identified as the covariance between our structural outcome and the reform's
direction, providing a flexible tool for optimal policy targeting and a novel
bridge to the Marginal Treatment Effects literature.

</details>


### [346] [Reinforcement Learning and Consumption-Savings Behavior](https://arxiv.org/abs/2510.20748)
*Brandon Kaplowitz*

Main category: econ.GN

TL;DR: 本文用强化学习解释经济衰退时家庭消费行为的两个谜题，模型模拟结果与实证估计相符，表明强化学习提供统一框架理解过去经历对当前消费行为的影响。


<details>
  <summary>Details</summary>
Motivation: 解释经济衰退时家庭消费行为的两个令人困惑的实证模式。

Method: 开发一个模型，让代理人使用带神经网络近似的Q学习在收入不确定下做消费 - 储蓄决策，偏离标准理性预期假设。

Result: 模型复制了文献中的两个关键发现，模拟结果与实证估计紧密匹配。

Conclusion: 强化学习的自适应学习为理解过去经历如何塑造当前消费行为提供了统一框架。

Abstract: This paper demonstrates how reinforcement learning can explain two puzzling
empirical patterns in household consumption behavior during economic downturns.
I develop a model where agents use Q-learning with neural network approximation
to make consumption-savings decisions under income uncertainty, departing from
standard rational expectations assumptions. The model replicates two key
findings from recent literature: (1) unemployed households with previously low
liquid assets exhibit substantially higher marginal propensities to consume
(MPCs) out of stimulus transfers compared to high-asset households (0.50 vs
0.34), even when neither group faces borrowing constraints, consistent with
Ganong et al. (2024); and (2) households with more past unemployment
experiences maintain persistently lower consumption levels after controlling
for current economic conditions, a "scarring" effect documented by Malmendier
and Shen (2024). Unlike existing explanations based on belief updating about
income risk or ex-ante heterogeneity, the reinforcement learning mechanism
generates both higher MPCs and lower consumption levels simultaneously through
value function approximation errors that evolve with experience. Simulation
results closely match the empirical estimates, suggesting that adaptive
learning through reinforcement learning provides a unifying framework for
understanding how past experiences shape current consumption behavior beyond
what current economic conditions would predict.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [347] [Neurotremor: A wearable Supportive Device for Supporting Upper Limb Muscle Function](https://arxiv.org/abs/2510.19826)
*Aueaphum Aueawattthanaphisut,Thanyanee Srichaisak,Arissa Ieochai*

Main category: q-bio.NC

TL;DR: 本文介绍了一种用于上肢功能的传感器融合可穿戴辅助原型，经健康志愿者测试展示了技术可行性，后续计划开展正式患者研究。


<details>
  <summary>Details</summary>
Motivation: 开发用于上肢功能（肱三头肌和拇短伸肌）的传感器融合可穿戴辅助设备。

Method: 将表面肌电图、惯性测量单元和柔性/力传感器集成在M5StickC和ESP32 - S3计算集线器上，对信号滤波、计算特征并输入INT8 TensorFlow Lite Micro模型，控制命令受控制屏障函数安全包络限制并用于基于游戏的任务。

Result: 在健康志愿者试验中，震颤显著降低、运动范围增加、重复次数增多、肌电图中值频率斜率负向减小，传感 - 辅助循环运行频率为100Hz，设备中位延迟8.7ms，会话完成率100%，无设备相关不良事件。

Conclusion: 该嵌入式传感器融合上肢功能辅助技术可行，计划在IRB监督下开展正式患者研究。

Abstract: A sensor-fused wearable assistance prototype for upper-limb function (triceps
brachii and extensor pollicis brevis) is presented. The device integrates
surface electromyography (sEMG), an inertial measurement unit (IMU), and
flex/force sensors on an M5StickC plus an ESP32-S3 compute hub. Signals are
band-pass and notch filtered; features (RMS, MAV, zero-crossings, and 4-12 Hz
tremor-band power) are computed in 250 ms windows and fed to an INT8 TensorFlow
Lite Micro model. Control commands are bounded by a control-barrier-function
safety envelope and delivered within game-based tasks with lightweight
personalization. In a pilot technical feasibility evaluation with healthy
volunteers (n = 12) performing three ADL-oriented tasks, tremor prominence
decreased (Delta TI = -0.092, 95% CI [-0.102, -0.079]), range of motion
increased (+12.65%, 95% CI [+8.43, +13.89]), repetitions rose (+2.99 min^-1,
95% CI [+2.61, +3.35]), and the EMG median-frequency slope became less negative
(Delta = +0.100 Hz/min, 95% CI [+0.083, +0.127]). The sensing-to-assist loop
ran at 100 Hz with 8.7 ms median on-device latency, 100% session completion,
and 0 device-related adverse events. These results demonstrate technical
feasibility of embedded, sensor-fused assistance for upper-limb function;
formal patient studies under IRB oversight are planned.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [348] [Multi-Task Deep Learning for Surface Metrology](https://arxiv.org/abs/2510.20339)
*D. Kucharski,A. Gaska,T. Kowaluk,K. Stepien,M. Repalska,B. Gapinski,M. Wieczorowski,M. Nawotka,P. Sobecki,P. Sosinowski,J. Tomasik,A. Wojtowicz*

Main category: physics.app-ph

TL;DR: 提出用于表面计量的深度学习框架，预测表面纹理参数及不确定度，单目标回归器效果好，分类器准确率高，结果可用于计量工作流程。


<details>
  <summary>Details</summary>
Motivation: 为表面计量提供可重复的深度学习框架，以预测表面纹理参数及其标准不确定度。

Method: 使用多仪器数据集，通过分位数和异方差头对不确定度建模，进行事后共形校准；解决测量系统类型分类和参数回归问题。

Result: 单目标回归器在部分参数上R²高，分类器准确率达92.85%，概率校准变化小，朴素多输出主干有负迁移，单目标模型表现更好。

Conclusion: 该框架的结果能为计量工作流程中的仪器选择和验收决策提供校准预测。

Abstract: A reproducible deep learning framework is presented for surface metrology to
predict surface texture parameters together with their reported standard
uncertainties. Using a multi-instrument dataset spanning tactile and optical
systems, measurement system type classification is addressed alongside
coordinated regression of Ra, Rz, RONt and their uncertainty targets
(Ra_uncert, Rz_uncert, RONt_uncert). Uncertainty is modelled via quantile and
heteroscedastic heads with post-hoc conformal calibration to yield calibrated
intervals. On a held-out set, high fidelity was achieved by single-target
regressors (R2: Ra 0.9824, Rz 0.9847, RONt 0.9918), with two uncertainty
targets also well modelled (Ra_uncert 0.9899, Rz_uncert 0.9955); RONt_uncert
remained difficult (R2 0.4934). The classifier reached 92.85% accuracy and
probability calibration was essentially unchanged after temperature scaling
(ECE 0.00504 -> 0.00503 on the test split). Negative transfer was observed for
naive multi-output trunks, with single-target models performing better. These
results provide calibrated predictions suitable to inform instrument selection
and acceptance decisions in metrological workflows.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [349] [Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs](https://arxiv.org/abs/2510.19850)
*Mostapha Kalami Heris*

Main category: cs.PL

TL;DR: 本文提出Prompt Decorators，通过紧凑控制令牌管理大语言模型行为，解耦任务意图与执行行为，有诸多优势并探讨相关影响。


<details>
  <summary>Details</summary>
Motivation: 用户缺乏对大语言模型推理和输出表达的一致控制，传统提示工程有局限性。

Method: 引入Prompt Decorators，定义统一语法、作用域模型和处理管道，将核心装饰器分为两个功能家族并细分。

Result: 展示了推理透明度提高、提示复杂度降低和跨领域模型行为标准化等效果。

Conclusion: 探讨了在互操作性、行为一致性和可扩展AI系统声明式接口开发方面的影响。

Abstract: Large Language Models (LLMs) are central to reasoning, writing, and
decision-support workflows, yet users lack consistent control over how they
reason and express outputs. Conventional prompt engineering relies on verbose
natural-language instructions, limiting reproducibility, modularity, and
interpretability. This paper introduces Prompt Decorators, a declarative,
composable syntax that governs LLM behavior through compact control tokens such
as +++Reasoning, +++Tone(style=formal), and +++Import(topic="Systems
Thinking"). Each decorator modifies a behavioral dimension, such as reasoning
style, structure, or tone, without changing task content. The framework
formalizes twenty core decorators organized into two functional families
(Cognitive & Generative and Expressive & Systemic), each further decomposed
into subcategories that govern reasoning, interaction, expression, and
session-control. It defines a unified syntax, scoping model, and deterministic
processing pipeline enabling predictable and auditable behavior composition. By
decoupling task intent from execution behavior, Prompt Decorators create a
reusable and interpretable interface for prompt design. Illustrative use cases
demonstrate improved reasoning transparency, reduced prompt complexity, and
standardized model behavior across domains. The paper concludes with
implications for interoperability, behavioral consistency, and the development
of declarative interfaces for scalable AI systems.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [350] [Transforming Multi-Omics Integration with GANs: Applications in Alzheimer's and Cancer](https://arxiv.org/abs/2510.19870)
*Md Selim Reza,Sabrin Afroz,Mostafizer Rahman,Md Ashad Alam*

Main category: q-bio.QM

TL;DR: 介绍了基于GAN的Omics - GAN框架，在多组学数据上验证其能提升疾病预测准确性、保留生物特征并助力生物标志物和药物发现。


<details>
  <summary>Details</summary>
Motivation: 多组学数据集成在理解复杂疾病时，因样本量有限、噪声和异质性等问题降低了预测能力。

Method: 引入Omics - GAN框架生成高质量合成多组学图谱，用支持向量机（SVM）分类器和重复5折交叉验证评估，还进行箱线图分析、特征选择和分子对接。

Result: 合成数据集比原始组学图谱提高了预测准确性，保留了统计分布、减少噪声和异常值，识别出显著基因，发现潜在药物再利用候选物。

Conclusion: Omics - GAN可增强疾病预测、保留生物保真度、加速生物标志物和药物发现，为精准医学提供可扩展策略。

Abstract: Multi-omics data integration is crucial for understanding complex diseases,
yet limited sample sizes, noise, and heterogeneity often reduce predictive
power. To address these challenges, we introduce Omics-GAN, a Generative
Adversarial Network (GAN)-based framework designed to generate high-quality
synthetic multi-omics profiles while preserving biological relationships. We
evaluated Omics-GAN on three omics types (mRNA, miRNA, and DNA methylation)
using the ROSMAP cohort for Alzheimer's disease (AD) and TCGA datasets for
colon and liver cancer. A support vector machine (SVM) classifier with repeated
5-fold cross-validation demonstrated that synthetic datasets consistently
improved prediction accuracy compared to original omics profiles. The AUC of
SVM for mRNA improved from 0.72 to 0.74 in AD, and from 0.68 to 0.72 in liver
cancer. Synthetic miRNA enhanced classification in colon cancer from 0.59 to
0.69, while synthetic methylation data improved performance in liver cancer
from 0.64 to 0.71. Boxplot analyses confirmed that synthetic data preserved
statistical distributions while reducing noise and outliers. Feature selection
identified significant genes overlapping with original datasets and revealed
additional candidates validated by GO and KEGG enrichment analyses. Finally,
molecular docking highlighted potential drug repurposing candidates, including
Nilotinib for AD, Atovaquone for liver cancer, and Tecovirimat for colon
cancer. Omics-GAN enhances disease prediction, preserves biological fidelity,
and accelerates biomarker and drug discovery, offering a scalable strategy for
precision medicine applications.

</details>


### [351] [Artificial Intelligence Powered Identification of Potential Antidiabetic Compounds in Ficus religiosa](https://arxiv.org/abs/2510.19867)
*Md Ashad Alam,Md Amanullah*

Main category: q-bio.QM

TL;DR: 使用生态系统计算方法结合人工智能研究菩提树化合物抗糖尿病特性，AI 加速筛选且提高准确率，为天然产物疗法实验验证奠定基础。


<details>
  <summary>Details</summary>
Motivation: 糖尿病需新疗法，菩提树有潜在抗糖尿病生物活性成分，故开展研究。

Method: 采用生态系统计算方法，结合机器学习、分子对接、ADMET 预测系统评估化合物对 DPP - 4 酶的功效，用 DeepBindGCN 和 AutoDock 研究结合相互作用。

Result: 黄酮类和生物碱有强结合相互作用和有利药理作用，AI 加速筛选并提高准确率。

Conclusion: 为糖尿病管理的天然产物疗法未来实验验证提供科学基础。

Abstract: Diabetes mellitus is a chronic metabolic disorder that necessitates novel
therapeutic innovations due to its gradual progression and the onset of various
metabolic complications. Research indicates that Ficus religiosa is a
conventional medicinal plant that generates bioactive phytochemicals with
potential antidiabetic properties. The investigation employs ecosystem-based
computational approaches utilizing artificial intelligence to investigate and
evaluate compounds derived from Ficus religiosa that exhibit antidiabetic
properties. A comprehensive computational procedure incorporated machine
learning methodologies, molecular docking techniques, and ADMET prediction
systems to assess phytochemical efficacy against the significant antidiabetic
enzyme dipeptidyl peptidase-4 (DPP-4). DeepBindGCN and the AutoDock software
facilitated the investigation of binding interactions via deep learning
technology. Flavonoids and alkaloids have emerged as attractive phytochemicals
due to their strong binding interactions and advantageous pharmacological
effects, as indicated by the study. The introduction of AI accelerated
screening procedures and enhanced accuracy rates, demonstrating its efficacy in
researching plant-based antidiabetic agents. The scientific foundation now
facilitates future experimental validation of natural product therapies
tailored for diabetic management.

</details>


### [352] [Compressing Biology: Evaluating the Stable Diffusion VAE for Phenotypic Drug Discovery](https://arxiv.org/abs/2510.19887)
*Télio Cropsal,Rocío Mercado*

Main category: q-bio.QM

TL;DR: 对Stable Diffusion的变分自编码器（SD - VAE）用于重建细胞绘画图像进行系统评估，发现其能保留表型信号，通用特征提取器在检索任务表现良好，为评估生成模型提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 高通量表型筛选产生的显微镜图像数据集维度大，通用模型用于显微镜数据分析的适用性未得到定量证明。

Method: 对SD - VAE重建细胞绘画图像进行系统评估，比较像素级、基于嵌入、潜在空间和基于检索的指标进行生物学评估。

Result: SD - VAE重建能以最小损失保留表型信号，通用特征提取器如InceptionV3在检索任务中表现与或超过定制模型。

Conclusion: 为评估显微镜数据上的生成模型提供实用指南，支持在表型药物发现中使用现成模型。

Abstract: High-throughput phenotypic screens generate vast microscopy image datasets
that push the limits of generative models due to their large dimensionality.
Despite the growing popularity of general-purpose models trained on natural
images for microscopy data analysis, their suitability in this domain has not
been quantitatively demonstrated. We present the first systematic evaluation of
Stable Diffusion's variational autoencoder (SD-VAE) for reconstructing Cell
Painting images, assessing performance across a large dataset with diverse
molecular perturbations and cell types. We find that SD-VAE reconstructions
preserve phenotypic signals with minimal loss, supporting its use in microscopy
workflows. To benchmark reconstruction quality, we compare pixel-level,
embedding-based, latent-space, and retrieval-based metrics for a biologically
informed evaluation. We show that general-purpose feature extractors like
InceptionV3 match or surpass publicly available bespoke models in retrieval
tasks, simplifying future pipelines. Our findings offer practical guidelines
for evaluating generative models on microscopy data and support the use of
off-the-shelf models in phenotypic drug discovery.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [353] [Guiding diffusion models to reconstruct flow fields from sparse data](https://arxiv.org/abs/2510.19971)
*Marc Amorós-Trepat,Luis Medrano-Navarro,Qiang Liu,Luca Guastoni,Nils Thuerey*

Main category: physics.flu-dyn

TL;DR: 本文提出扩散模型新采样方法用于重建高保真流场样本，结合物理知识提升效果，实验表明优于其他基于扩散的方法，凸显扩散模型在流场数据重建潜力。


<details>
  <summary>Details</summary>
Motivation: 从有限测量中重建非定常流场是工程应用难题，扩散模型在生成任务中表现出色但需更优方法。

Method: 引入新采样方法，用稀疏数据引导反向过程；训练时用无冲突更新方法结合物理知识。

Result: 在2维和3维湍流数据实验中，该方法在预测流体结构和像素精度上优于其他基于扩散的方法。

Conclusion: 扩散模型在重建流场数据方面有巨大潜力，为计算流体动力学研究应用奠定基础。

Abstract: The reconstruction of unsteady flow fields from limited measurements is a
challenging and crucial task for many engineering applications. Machine
learning models are gaining popularity in solving this problem due to their
ability to learn complex patterns from data and generalize across diverse
conditions. Among these, diffusion models have emerged as particularly powerful
in generative tasks, producing high-quality samples by iteratively refining
noisy inputs. In contrast to other methods, these generative models are capable
of reconstructing the smallest scales of the fluid spectrum. In this work, we
introduce a novel sampling method for diffusion models that enables the
reconstruction of high-fidelity samples by guiding the reverse process using
the available sparse data. Moreover, we enhance the reconstructions with
available physics knowledge using a conflict-free update method during
training. To evaluate the effectiveness of our method, we conduct experiments
on 2 and 3-dimensional turbulent flow data. Our method consistently outperforms
other diffusion-based methods in predicting the fluid's structure and in
pixel-wise accuracy. This study underscores the remarkable potential of
diffusion models in reconstructing flow field data, paving the way for their
application in Computational Fluid Dynamics research.

</details>
