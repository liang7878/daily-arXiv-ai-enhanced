<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.LG](#cs.LG) [Total: 98]
- [cs.SE](#cs.SE) [Total: 9]
- [stat.ML](#stat.ML) [Total: 6]
- [stat.CO](#stat.CO) [Total: 3]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [physics.atom-ph](#physics.atom-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.CV](#cs.CV) [Total: 35]
- [cs.MM](#cs.MM) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.GR](#cs.GR) [Total: 5]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 9]
- [eess.SY](#eess.SY) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.HC](#cs.HC) [Total: 4]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.CL](#cs.CL) [Total: 27]
- [cs.SD](#cs.SD) [Total: 9]
- [cs.CG](#cs.CG) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.CR](#cs.CR) [Total: 4]
- [math.NA](#math.NA) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [econ.GN](#econ.GN) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MICA: Multi-Agent Industrial Coordination Assistant](https://arxiv.org/abs/2509.15237)
*Di Wen,Kunyu Peng,Junwei Zheng,Yufan Chen,Yitain Shi,Jiale Wei,Ruiping Liu,Kailun Yang,Rainer Stiefelhagen*

Main category: cs.AI

TL;DR: 提出MICA系统，可在工业工作流中提供实时指导，采用ASF实现步骤理解，建立基准和评估指标，实验显示性能优于基线，代码将开源。


<details>
  <summary>Details</summary>
Motivation: 工业工作流需要在有限计算、连接和严格隐私约束下提供自适应和可信的协助。

Method: 协调五个角色专业化语言代理并由安全检查器审核；引入Adaptive Step Fusion (ASF)；建立多智能体协调基准和评估指标。

Result: MICA在任务成功率、可靠性和响应性上优于基线结构，且可部署在实际离线硬件上。

Conclusion: MICA是迈向可部署、保护隐私的动态工厂环境多智能体助手的一步。

Abstract: Industrial workflows demand adaptive and trustworthy assistance that can
operate under limited computing, connectivity, and strict privacy constraints.
In this work, we present MICA (Multi-Agent Industrial Coordination Assistant),
a perception-grounded and speech-interactive system that delivers real-time
guidance for assembly, troubleshooting, part queries, and maintenance. MICA
coordinates five role-specialized language agents, audited by a safety checker,
to ensure accurate and compliant support. To achieve robust step understanding,
we introduce Adaptive Step Fusion (ASF), which dynamically blends expert
reasoning with online adaptation from natural speech feedback. Furthermore, we
establish a new multi-agent coordination benchmark across representative task
categories and propose evaluation metrics tailored to industrial assistance,
enabling systematic comparison of different coordination topologies. Our
experiments demonstrate that MICA consistently improves task success,
reliability, and responsiveness over baseline structures, while remaining
deployable on practical offline hardware. Together, these contributions
highlight MICA as a step toward deployable, privacy-preserving multi-agent
assistants for dynamic factory environments. The source code will be made
publicly available at https://github.com/Kratos-Wen/MICA.

</details>


### [2] [KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial Problems](https://arxiv.org/abs/2509.15239)
*Stjepan Požgaj,Dobrik Georgiev,Marin Šilić,Petar Veličković*

Main category: cs.AI

TL;DR: 尝试构建可解决标准NAR基准中遗漏的背包问题的神经算法推理器，方法优于直接预测基线。


<details>
  <summary>Details</summary>
Motivation: 在神经算法推理领域，构建能解决标准NAR基准中遗漏的背包问题的推理器。

Method: 设计神经算法推理器，遵循背包问题的两阶段流程，通过动态规划监督对中间状态建模。

Result: 该方法在处理更大问题实例时比直接预测基线有更好的泛化能力。

Conclusion: 所提出的方法在解决背包问题上有较好的效果，能实现更好的泛化。

Abstract: Neural algorithmic reasoning (NAR) is a growing field that aims to embed
algorithmic logic into neural networks by imitating classical algorithms. In
this extended abstract, we detail our attempt to build a neural algorithmic
reasoner that can solve Knapsack, a pseudo-polynomial problem bridging
classical algorithms and combinatorial optimisation, but omitted in standard
NAR benchmarks. Our neural algorithmic reasoner is designed to closely follow
the two-phase pipeline for the Knapsack problem, which involves first
constructing the dynamic programming table and then reconstructing the solution
from it. The approach, which models intermediate states through dynamic
programming supervision, achieves better generalization to larger problem
instances than a direct-prediction baseline that attempts to select the optimal
subset only from the problem inputs.

</details>


### [3] [The Distribution Shift Problem in Transportation Networks using Reinforcement Learning and AI](https://arxiv.org/abs/2509.15291)
*Federico Taschin,Abderrahmane Lazaraq,Ozan K. Tonguz,Inci Ozgunes*

Main category: cs.AI

TL;DR: 本文评估分析了MetaLight这种元强化学习方法，指出其在某些条件下有不错结果，但在另一些条件下表现不佳，元强化学习方案不够稳健且有可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习用于交通信号控制时，训练数据分布与输入数据分布动态变化，导致训练的智能体可靠性有问题，需要解决方案。

Method: 评估分析了名为MetaLight的先进元强化学习方法。

Result: MetaLight在某些条件下能取得不错结果，但在另一些条件下误差可达22%。

Conclusion: 元强化学习方案通常不够稳健，可能存在重大可靠性问题。

Abstract: The use of Machine Learning (ML) and Artificial Intelligence (AI) in smart
transportation networks has increased significantly in the last few years.
Among these ML and AI approaches, Reinforcement Learning (RL) has been shown to
be a very promising approach by several authors. However, a problem with using
Reinforcement Learning in Traffic Signal Control is the reliability of the
trained RL agents due to the dynamically changing distribution of the input
data with respect to the distribution of the data used for training. This
presents a major challenge and a reliability problem for the trained network of
AI agents and could have very undesirable and even detrimental consequences if
a suitable solution is not found. Several researchers have tried to address
this problem using different approaches. In particular, Meta Reinforcement
Learning (Meta RL) promises to be an effective solution. In this paper, we
evaluate and analyze a state-of-the-art Meta RL approach called MetaLight and
show that, while under certain conditions MetaLight can indeed lead to
reasonably good results, under some other conditions it might not perform well
(with errors of up to 22%), suggesting that Meta RL schemes are often not
robust enough and can even pose major reliability problems.

</details>


### [4] [An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature](https://arxiv.org/abs/2509.15292)
*Abhiyan Dhakal,Kausik Paudel,Sanjog Sigdel*

Main category: cs.AI

TL;DR: 提出基于语义相似性的文献综述自动化流程，用变压器嵌入和余弦相似度，评估三种嵌入模型，过滤相关论文，有应用前景。


<details>
  <summary>Details</summary>
Motivation: 传统系统综述或优化方法存在不足，期望构建低开销、高相关性的文献综述流程。

Method: 采用基于变压器的嵌入和余弦相似度，提供标题和摘要生成关键词、获取相关论文并排名，评估三种嵌入模型，用统计阈值法过滤。

Result: 在无启发式反馈或真实相关标签下，系统在初步研究和探索性分析方面有潜力。

Conclusion: 该系统是可扩展且实用的初步研究和探索性分析工具。

Abstract: We propose an automated pipeline for performing literature reviews using
semantic similarity. Unlike traditional systematic review systems or
optimization based methods, this work emphasizes minimal overhead and high
relevance by using transformer based embeddings and cosine similarity. By
providing a paper title and abstract, it generates relevant keywords, fetches
relevant papers from open access repository, and ranks them based on their
semantic closeness to the input. Three embedding models were evaluated. A
statistical thresholding approach is then applied to filter relevant papers,
enabling an effective literature review pipeline. Despite the absence of
heuristic feedback or ground truth relevance labels, the proposed system shows
promise as a scalable and practical tool for preliminary research and
exploratory analysis.

</details>


### [5] [Knowledge-Driven Hallucination in Large Language Models: An Empirical Study on Process Modeling](https://arxiv.org/abs/2509.15336)
*Humam Kourani,Anton Antonov,Alessandro Berti,Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: 本文研究大语言模型在自动流程建模任务中的知识驱动幻觉现象，通过实验评估其对给定证据的保真度，并提供评估方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在分析任务中虽有利用预训练知识的能力，但会带来知识驱动幻觉风险，需研究该现象。

Method: 在自动流程建模任务中进行对照实验，设置给定证据与模型背景知识冲突的场景，使用标准和非典型流程结构输入评估模型。

Result: 文中未提及具体实验结果。

Conclusion: 提供了评估大语言模型可靠性问题的方法，提高了对基于证据领域中AI生成工件严格验证必要性的认识。

Abstract: The utility of Large Language Models (LLMs) in analytical tasks is rooted in
their vast pre-trained knowledge, which allows them to interpret ambiguous
inputs and infer missing information. However, this same capability introduces
a critical risk of what we term knowledge-driven hallucination: a phenomenon
where the model's output contradicts explicit source evidence because it is
overridden by the model's generalized internal knowledge. This paper
investigates this phenomenon by evaluating LLMs on the task of automated
process modeling, where the goal is to generate a formal business process model
from a given source artifact. The domain of Business Process Management (BPM)
provides an ideal context for this study, as many core business processes
follow standardized patterns, making it likely that LLMs possess strong
pre-trained schemas for them. We conduct a controlled experiment designed to
create scenarios with deliberate conflict between provided evidence and the
LLM's background knowledge. We use inputs describing both standard and
deliberately atypical process structures to measure the LLM's fidelity to the
provided evidence. Our work provides a methodology for assessing this critical
reliability issue and raises awareness of the need for rigorous validation of
AI-generated artifacts in any evidence-based domain.

</details>


### [6] [Diagnostics of cognitive failures in multi-agent expert systems using dynamic evaluation protocols and subsequent mutation of the processing context](https://arxiv.org/abs/2509.15366)
*Andrejs Sorstkins,Josh Bailey,Dr Alistair Baron*

Main category: cs.AI

TL;DR: 本文引入专家系统诊断框架，用于评估和转移专家行为到基于大语言模型的智能体，在多智能体招聘系统验证，为专家行为转移奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有经典评估方法无法有效诊断大语言模型智能体的代理性能，需新的评估和转移专家行为的方法。

Method: 引入诊断框架，整合专家标注的黄金数据集、行为突变生成的白银数据集和基于大语言模型的智能体评判器，将建议嵌入推荐图。

Result: 在多智能体招聘系统中发现潜在认知错误，引导智能体达到专家级推理和风格。

Conclusion: 为随机、工具增强的大语言模型智能体的专家行为转移建立基础，从静态评估转向主动专家系统改进。

Abstract: The rapid evolution of neural architectures - from multilayer perceptrons to
large-scale Transformer-based models - has enabled language models (LLMs) to
exhibit emergent agentic behaviours when equipped with memory, planning, and
external tool use. However, their inherent stochasticity and multi-step
decision processes render classical evaluation methods inadequate for
diagnosing agentic performance. This work introduces a diagnostic framework for
expert systems that not only evaluates but also facilitates the transfer of
expert behaviour into LLM-powered agents. The framework integrates (i) curated
golden datasets of expert annotations, (ii) silver datasets generated through
controlled behavioural mutation, and (iii) an LLM-based Agent Judge that scores
and prescribes targeted improvements. These prescriptions are embedded into a
vectorized recommendation map, allowing expert interventions to propagate as
reusable improvement trajectories across multiple system instances. We
demonstrate the framework on a multi-agent recruiter-assistant system, showing
that it uncovers latent cognitive failures - such as biased phrasing,
extraction drift, and tool misrouting - while simultaneously steering agents
toward expert-level reasoning and style. The results establish a foundation for
standardized, reproducible expert behaviour transfer in stochastic,
tool-augmented LLM agents, moving beyond static evaluation to active expert
system refinement.

</details>


### [7] [FragmentRetro: A Quadratic Retrosynthetic Method Based on Fragmentation Algorithms](https://arxiv.org/abs/2509.15409)
*Yu Shee,Anthony M. Smaldone,Anton Morgunov,Gregory W. Kyro,Victor S. Batista*

Main category: cs.AI

TL;DR: 介绍了新的逆合成方法FragmentRetro，分析其复杂度并通过评估证明优势，是合成规划有力基础组件。


<details>
  <summary>Details</summary>
Motivation: 现有广泛采用的树搜索逆合成方法存在指数级计算复杂度问题。

Method: 利用BRICS和r - BRICS等碎片化算法，结合库存感知探索和模式指纹筛选，递归组合分子片段并验证其是否在构建块集中。

Result: 在多个数据集评估中，FragmentRetro解决率高、运行时间有竞争力，指纹筛选降低了子结构匹配复杂度。

Conclusion: FragmentRetro虽专注于片段解决方案而非完整反应路径，但计算优势和生成起始候选物的能力使其成为可扩展和自动化合成规划的有力基础组件。

Abstract: Retrosynthesis, the process of deconstructing a target molecule into simpler
precursors, is crucial for computer-aided synthesis planning (CASP). Widely
adopted tree-search methods often suffer from exponential computational
complexity. In this work, we introduce FragmentRetro, a novel retrosynthetic
method that leverages fragmentation algorithms, specifically BRICS and r-BRICS,
combined with stock-aware exploration and pattern fingerprint screening to
achieve quadratic complexity. FragmentRetro recursively combines molecular
fragments and verifies their presence in a building block set, providing sets
of fragment combinations as retrosynthetic solutions. We present the first
formal computational analysis of retrosynthetic methods, showing that tree
search exhibits exponential complexity $O(b^h)$, DirectMultiStep scales as
$O(h^6)$, and FragmentRetro achieves $O(h^2)$, where $h$ represents the number
of heavy atoms in the target molecule and $b$ is the branching factor for tree
search. Evaluations on PaRoutes, USPTO-190, and natural products demonstrate
that FragmentRetro achieves high solved rates with competitive runtime,
including cases where tree search fails. The method benefits from fingerprint
screening, which significantly reduces substructure matching complexity. While
FragmentRetro focuses on efficiently identifying fragment-based solutions
rather than full reaction pathways, its computational advantages and ability to
generate strategic starting candidates establish it as a powerful foundational
component for scalable and automated synthesis planning.

</details>


### [8] [Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage Approach via Semantic Clustering and Multi-Agent Collaboration](https://arxiv.org/abs/2509.15786)
*Nan Li,Bo Kang,Tijl De Bie*

Main category: cs.AI

TL;DR: 介绍CLIMB框架自动从原始招聘信息创建高质量职业分类法，比现有方法更优。


<details>
  <summary>Details</summary>
Motivation: 现有创建职业分类法的方法存在不足，手动整理慢，自动化方法有适应性或构建层次结构的问题。

Method: CLIMB框架先使用全局语义聚类提炼核心职业，再用基于反思的多智能体系统迭代构建连贯层次结构。

Result: 在三个真实数据集上，CLIMB生成的分类法比现有方法更连贯、可扩展，能捕捉区域特征。

Conclusion: CLIMB是一种有效的自动创建职业分类法的框架，代码和数据集已公开。

Abstract: Creating robust occupation taxonomies, vital for applications ranging from
job recommendation to labor market intelligence, is challenging. Manual
curation is slow, while existing automated methods are either not adaptive to
dynamic regional markets (top-down) or struggle to build coherent hierarchies
from noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent
taxonomy Builder), a framework that fully automates the creation of
high-quality, data-driven taxonomies from raw job postings. CLIMB uses global
semantic clustering to distill core occupations, then employs a
reflection-based multi-agent system to iteratively build a coherent hierarchy.
On three diverse, real-world datasets, we show that CLIMB produces taxonomies
that are more coherent and scalable than existing methods and successfully
capture unique regional characteristics. We release our code and datasets at
https://anonymous.4open.science/r/CLIMB.

</details>


### [9] [Stress Testing Deliberative Alignment for Anti-Scheming Training](https://arxiv.org/abs/2509.15541)
*Bronson Schoen,Evgenia Nitishinskaya,Mikita Balesni,Axel Højmark,Felix Hofstätter,Jérémy Scheurer,Alexander Meinke,Jason Wolfe,Teun van der Weij,Alex Lloyd,Nicholas Goldowsky-Dill,Angela Fan,Andrei Matveiakin,Rusheb Shah,Marcus Williams,Amelia Glaese,Boaz Barak,Wojciech Zaremba,Marius Hobbhahn*

Main category: cs.AI

TL;DR: 本文探讨AI系统的策划行为，提出评估反策划干预的方法，以秘密行动为代理进行评估，以审慎对齐为例进行压力测试，发现虽能降低秘密行动率但未完全消除，且可能受情境意识影响，鼓励相关研究。


<details>
  <summary>Details</summary>
Motivation: 由于有能力的AI系统可能秘密追求不一致目标（策划行为），而测量和缓解该行为需不同于ML的策略，所以要研究评估反策划干预的方法。

Method: 提出评估反策划干预的三个方面，用秘密行动作为策划的代理，设计评估方法，以审慎对齐为例进行压力测试。

Result: 在26个OOD评估中，审慎对齐降低了秘密行动率，但未完全消除；能阻止模型追求先前训练的隐藏目标，但红队测试后仍有不当行为；模型的思维链显示对评估的意识会影响秘密行为。

Conclusion: 观察到的秘密行动率降低可能部分由情境意识驱动；随着模型偏离标准英语推理，依赖人类可读思维链的能力下降；鼓励对策划的对齐缓解措施及其评估进行研究，特别是欺骗性对齐的对抗情况。

Abstract: Highly capable AI systems could secretly pursue misaligned goals -- what we
call "scheming". Because a scheming AI would deliberately try to hide its
misaligned goals and actions, measuring and mitigating scheming requires
different strategies than are typically used in ML. We propose that assessing
anti-scheming interventions requires at least (1) testing propensity to scheme
on far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming
is driven by situational awareness, and (3) checking for robustness to
pre-existing misaligned goals. We use a broad category of "covert actions" --
such as secretly breaking rules or intentionally underperforming in tests -- as
a proxy for scheming, and design evaluations for covert actions. We then
stress-test deliberative alignment as a case study for anti-scheming. Across 26
OOD evaluations (180+ environments), deliberative alignment reduces covert
action rates (OpenAI o3: 13%->0.4%) but does not fully eliminate them. Our
mitigation is also able to largely stop agents from pursuing a hidden goal
previously trained into the model, but we still find misbehavior after
additional red-teaming. We find that models' chain-of-thought (CoT) often
demonstrates awareness of being evaluated for alignment, and show causal
evidence that this awareness decreases covert behavior, while unawareness
increases it. Therefore, we cannot exclude that the observed reductions in
covert action rates are at least partially driven by situational awareness.
While we rely on human-legible CoT for training, studying situational
awareness, and demonstrating clear evidence of misalignment, our ability to
rely on this degrades as models continue to depart from reasoning in standard
English. We encourage research into alignment mitigations for scheming and
their assessment, especially for the adversarial case of deceptive alignment,
which this paper does not address.

</details>


### [10] [EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol](https://arxiv.org/abs/2509.15957)
*Kanato Masayoshi,Masahiro Hashimoto,Ryoichi Yokoyama,Naoki Toda,Yoshifumi Uwamino,Shogo Fukuda,Ho Namkoong,Masahiro Jinzaki*

Main category: cs.AI

TL;DR: 研究评估通过MCP连接EHR数据库的大语言模型在真实医院环境检索临床信息的能力，简单任务表现佳，复杂任务有挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医学有前景，但受限于电子健康记录系统访问，评估其通过MCP在真实医院自主检索临床相关信息的能力。

Method: 开发EHR - MCP框架，结合医院EHR数据库，用GPT - 4.1通过LangGraph ReAct代理与之交互，测试六项任务，回顾分析八位患者，测量与医生生成金标准的一致性。

Result: 大语言模型能正确选择和执行MCP工具，除两项任务外接近完美准确，复杂任务表现差，错误多源于参数错误或结果误判，EHR - MCP响应可靠但有超上下文窗口风险。

Conclusion: 大语言模型可通过MCP从EHR检索临床数据，简单任务表现好，复杂任务有挑战，EHR - MCP提供安全数据访问基础，未来工作应拓展至推理、生成和临床影响评估。

Abstract: Background: Large language models (LLMs) show promise in medicine, but their
deployment in hospitals is limited by restricted access to electronic health
record (EHR) systems. The Model Context Protocol (MCP) enables integration
between LLMs and external tools.
  Objective: To evaluate whether an LLM connected to an EHR database via MCP
can autonomously retrieve clinically relevant information in a real hospital
setting.
  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated
with the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct
agent to interact with it. Six tasks were tested, derived from use cases of the
infection control team (ICT). Eight patients discussed at ICT conferences were
retrospectively analyzed. Agreement with physician-generated gold standards was
measured.
  Results: The LLM consistently selected and executed the correct MCP tools.
Except for two tasks, all tasks achieved near-perfect accuracy. Performance was
lower in the complex task requiring time-dependent calculations. Most errors
arose from incorrect arguments or misinterpretation of tool results. Responses
from EHR-MCP were reliable, though long and repetitive data risked exceeding
the context window.
  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a
real hospital setting, achieving near-perfect performance in simple tasks while
highlighting challenges in complex ones. EHR-MCP provides an infrastructure for
secure, consistent data access and may serve as a foundation for hospital AI
agents. Future work should extend beyond retrieval to reasoning, generation,
and clinical impact assessment, paving the way for effective integration of
generative AI into clinical practice.

</details>


### [11] [MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents](https://arxiv.org/abs/2509.15635)
*Pan Tang,Shixiang Tang,Huanqi Pu,Zhiqing Miao,Zhixing Wang*

Main category: cs.AI

TL;DR: 本文提出基于大语言模型代理的MicroRCA - Agent用于微服务根因分析，有技术创新，经消融实验验证，在复杂场景表现好，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 解决微服务根因分析问题，构建智能故障根因定位系统。

Method: 结合Drain算法与多级数据过滤机制处理日志；采用集成Isolation Forest与状态码验证的双异常检测方法；设计统计对称比过滤机制与两阶段LLM分析策略；多模态根因分析模块利用跨模态提示整合信息。

Result: 综合消融实验验证各模态数据互补价值和系统架构有效性，在复杂微服务故障场景取得50.71的最终得分。

Conclusion: 所提MicroRCA - Agent解决方案在复杂微服务故障场景中表现优越，能有效进行根因分析。

Abstract: This paper presents MicroRCA-Agent, an innovative solution for microservice
root cause analysis based on large language model agents, which constructs an
intelligent fault root cause localization system with multimodal data fusion.
The technical innovations are embodied in three key aspects: First, we combine
the pre-trained Drain log parsing algorithm with multi-level data filtering
mechanism to efficiently compress massive logs into high-quality fault
features. Second, we employ a dual anomaly detection approach that integrates
Isolation Forest unsupervised learning algorithms with status code validation
to achieve comprehensive trace anomaly identification. Third, we design a
statistical symmetry ratio filtering mechanism coupled with a two-stage LLM
analysis strategy to enable full-stack phenomenon summarization across
node-service-pod hierarchies. The multimodal root cause analysis module
leverages carefully designed cross-modal prompts to deeply integrate multimodal
anomaly information, fully exploiting the cross-modal understanding and logical
reasoning capabilities of large language models to generate structured analysis
results encompassing fault components, root cause descriptions, and reasoning
trace. Comprehensive ablation studies validate the complementary value of each
modal data and the effectiveness of the system architecture. The proposed
solution demonstrates superior performance in complex microservice fault
scenarios, achieving a final score of 50.71. The code has been released at:
https://github.com/tangpan360/MicroRCA-Agent.

</details>


### [12] [CCrepairBench: A High-Fidelity Benchmark and Reinforcement Learning Framework for C++ Compilation Repair](https://arxiv.org/abs/2509.15690)
*Weixuan Sun,Jucai Zhai,Dengfeng Liu,Xin Zhang,Xiaojun Wu,Qiaobo Hao,AIMgroup,Yang Fang,Jiuyang Tang*

Main category: cs.AI

TL;DR: 本文针对C++编译错误自动修复领域缺乏大规模高质量数据集和传统监督方法不足的问题，提出综合框架，包括新数据集、基于混合奖励信号的强化学习范式和评估系统，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: C++编译错误自动修复对开发者生产力至关重要，但该领域进展受大规模高质量数据集稀缺和传统监督方法局限性的制约。

Method: 提出综合框架，包含构建新数据集CCrepair、基于混合奖励信号的强化学习范式、以LLM作为评判的两阶段评估系统。

Result: RL训练的Qwen2.5 - 1.5B - Instruct模型达到与Qwen2.5 - 14B - Instruct模型相当的性能。

Conclusion: 为研究界提供新数据集和更有效的训练与评估范式，为实用可靠的自动编程助手铺平道路。

Abstract: The automated repair of C++ compilation errors presents a significant
challenge, the resolution of which is critical for developer productivity.
Progress in this domain is constrained by two primary factors: the scarcity of
large-scale, high-fidelity datasets and the limitations of conventional
supervised methods, which often fail to generate semantically correct
patches.This paper addresses these gaps by introducing a comprehensive
framework with three core contributions. First, we present CCrepair, a novel,
large-scale C++ compilation error dataset constructed through a sophisticated
generate-and-verify pipeline. Second, we propose a Reinforcement Learning (RL)
paradigm guided by a hybrid reward signal, shifting the focus from mere
compilability to the semantic quality of the fix. Finally, we establish the
robust, two-stage evaluation system providing this signal, centered on an
LLM-as-a-Judge whose reliability has been rigorously validated against the
collective judgments of a panel of human experts. This integrated approach
aligns the training objective with generating high-quality, non-trivial patches
that are both syntactically and semantically correct. The effectiveness of our
approach was demonstrated experimentally. Our RL-trained Qwen2.5-1.5B-Instruct
model achieved performance comparable to a Qwen2.5-14B-Instruct model,
validating the efficiency of our training paradigm. Our work provides the
research community with a valuable new dataset and a more effective paradigm
for training and evaluating robust compilation repair models, paving the way
for more practical and reliable automated programming assistants.

</details>


### [13] [A Nascent Taxonomy of Machine Learning in Intelligent Robotic Process Automation](https://arxiv.org/abs/2509.15730)
*Lukas Laakmann,Seyyid A. Ciftci,Christian Janiesch*

Main category: cs.AI

TL;DR: 本文对RPA和机器学习的联系进行文献综述，并将智能RPA概念整理成分类法。


<details>
  <summary>Details</summary>
Motivation: RPA在处理复杂任务时有局限性，机器学习可拓宽其可自动化任务范围，因此探索二者联系。

Method: 进行文献综述。

Result: 构建了包含RPA - ML集成和RPA - ML交互两个元特征、八个维度的分类法。

Conclusion: 成功将智能RPA概念组织成分类法，有助于进一步研究。

Abstract: Robotic process automation (RPA) is a lightweight approach to automating
business processes using software robots that emulate user actions at the
graphical user interface level. While RPA has gained popularity for its
cost-effective and timely automation of rule-based, well-structured tasks, its
symbolic nature has inherent limitations when approaching more complex tasks
currently performed by human agents. Machine learning concepts enabling
intelligent RPA provide an opportunity to broaden the range of automatable
tasks. In this paper, we conduct a literature review to explore the connections
between RPA and machine learning and organize the joint concept intelligent RPA
into a taxonomy. Our taxonomy comprises the two meta-characteristics RPA-ML
integration and RPA-ML interaction. Together, they comprise eight dimensions:
architecture and ecosystem, capabilities, data basis, intelligence level, and
technical depth of integration as well as deployment environment, lifecycle
phase, and user-robot relation.

</details>


### [14] [Ontology Creation and Management Tools: the Case of Anatomical Connectivity](https://arxiv.org/abs/2509.15780)
*Natallia Kokash,Bernard de Bono,Tom Gillespie*

Main category: cs.AI

TL;DR: 开发支持研究人员绘制外周神经系统等生理系统数据的基础设施，创建ApiNATOMY框架。


<details>
  <summary>Details</summary>
Motivation: 为研究人员提供基础设施，支持绘制与外周神经系统及其他生理系统相关的数据，强调其与被研究器官的相关性。

Method: 创建ApiNATOMY框架，整合知识表示（KR）模型和知识管理（KM）工具。

Result: KR模型让生理学专家能轻松捕捉解剖实体间的相互作用，KM工具帮助建模人员将高级抽象转化为生理过程的详细模型，并可与外部本体和知识图谱集成。

Conclusion: ApiNATOMY框架有助于研究人员进行生理系统数据的映射和建模。

Abstract: We are developing infrastructure to support researchers in mapping data
related to the peripheral nervous system and other physiological systems, with
an emphasis on their relevance to the organs under investigation. The nervous
system, a complex network of nerves and ganglia, plays a critical role in
coordinating and transmitting signals throughout the body. To aid in this, we
have created ApiNATOMY, a framework for the topological and semantic
representation of multiscale physiological circuit maps. ApiNATOMY integrates a
Knowledge Representation (KR) model and a suite of Knowledge Management (KM)
tools. The KR model enables physiology experts to easily capture interactions
between anatomical entities, while the KM tools help modelers convert
high-level abstractions into detailed models of physiological processes, which
can be integrated with external ontologies and knowledge graphs.

</details>


### [15] [A Comparative Study of Rule-Based and Data-Driven Approaches in Industrial Monitoring](https://arxiv.org/abs/2509.15848)
*Giovanni De Gasperis,Sante Dino Facchini*

Main category: cs.AI

TL;DR: 本文对比工业监测系统中基于规则和数据驱动两种方法，分析优缺点与应用场景，提出评估框架，建议采用混合方案。


<details>
  <summary>Details</summary>
Motivation: 工业监测系统正从传统基于规则架构转向数据驱动方法，需对比两种方法。

Method: 对比基于规则与数据驱动两种方法，分析各自优缺点和应用场景，提出评估框架。

Result: 基于规则系统在稳定环境易实现、可解释性强，但在复杂场景有局限；数据驱动系统能检测异常、动态适应，但有数据和可解释性问题。

Conclusion: 混合方案结合两者优势，是工业监测未来方向，可提升弹性、效率和信任。

Abstract: Industrial monitoring systems, especially when deployed in Industry 4.0
environments, are experiencing a shift in paradigm from traditional rule-based
architectures to data-driven approaches leveraging machine learning and
artificial intelligence. This study presents a comparison between these two
methodologies, analyzing their respective strengths, limitations, and
application scenarios, and proposes a basic framework to evaluate their key
properties. Rule-based systems offer high interpretability, deterministic
behavior, and ease of implementation in stable environments, making them ideal
for regulated industries and safety-critical applications. However, they face
challenges with scalability, adaptability, and performance in complex or
evolving contexts. Conversely, data-driven systems excel in detecting hidden
anomalies, enabling predictive maintenance and dynamic adaptation to new
conditions. Despite their high accuracy, these models face challenges related
to data availability, explainability, and integration complexity. The paper
suggests hybrid solutions as a possible promising direction, combining the
transparency of rule-based logic with the analytical power of machine learning.
Our hypothesis is that the future of industrial monitoring lies in intelligent,
synergic systems that leverage both expert knowledge and data-driven insights.
This dual approach enhances resilience, operational efficiency, and trust,
paving the way for smarter and more flexible industrial environments.

</details>


### [16] [Structured Information for Improving Spatial Relationships in Text-to-Image Generation](https://arxiv.org/abs/2509.15962)
*Sander Schildermans,Chang Tian,Ying Jiao,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 现有文本到图像生成在捕捉空间关系上有挑战，本文提出轻量级方法，用元组结构化信息增强提示，实验表明能提升空间准确性且不影响图像质量，自动生成元组质量与人工相当。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成中难以忠实捕捉自然语言提示中空间关系的问题。

Method: 引入轻量级方法，用基于元组的结构化信息增强提示，通过微调语言模型自动转换并集成到文本到图像生成管道。

Result: 实验显示空间准确性大幅提升，不影响整体图像质量，自动生成元组质量与人工相当。

Conclusion: 结构化信息为增强文本到图像生成中的空间关系提供了实用且可移植的解决方案，解决了当前大规模生成系统的关键局限。

Abstract: Text-to-image (T2I) generation has advanced rapidly, yet faithfully capturing
spatial relationships described in natural language prompts remains a major
challenge. Prior efforts have addressed this issue through prompt optimization,
spatially grounded generation, and semantic refinement. This work introduces a
lightweight approach that augments prompts with tuple-based structured
information, using a fine-tuned language model for automatic conversion and
seamless integration into T2I pipelines. Experimental results demonstrate
substantial improvements in spatial accuracy, without compromising overall
image quality as measured by Inception Score. Furthermore, the automatically
generated tuples exhibit quality comparable to human-crafted tuples. This
structured information provides a practical and portable solution to enhance
spatial relationships in T2I generation, addressing a key limitation of current
large-scale generative systems.

</details>


### [17] [Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers](https://arxiv.org/abs/2509.16058)
*Krati Saxena,Federico Jurado Ruiz,Guido Manzi,Dianbo Liu,Alex Lamb*

Main category: cs.AI

TL;DR: 受认知科学中注意力图式理论启发，提出ASAC将注意力图式概念融入人工神经网络，在视觉和NLP领域实验证明其能提升效率、准确性等。


<details>
  <summary>Details</summary>
Motivation: 受注意力图式理论启发，期望将其概念融入人工神经网络以提升系统效率。

Method: 在Transformer架构中嵌入ASAC模块，使用VQVAE作为注意力抽象器和控制器。

Result: 在视觉和NLP领域证明ASAC能提升分类准确率、加速学习，有鲁棒性、泛化能力，在多任务、对抗攻击等场景表现良好。

Conclusion: 建立了认知科学和机器学习的联系，为AI系统高效利用注意力机制提供思路。

Abstract: Attention mechanisms have become integral in AI, significantly enhancing
model performance and scalability by drawing inspiration from human cognition.
Concurrently, the Attention Schema Theory (AST) in cognitive science posits
that individuals manage their attention by creating a model of the attention
itself, effectively allocating cognitive resources. Inspired by AST, we
introduce ASAC (Attention Schema-based Attention Control), which integrates the
attention schema concept into artificial neural networks. Our initial
experiments focused on embedding the ASAC module within transformer
architectures. This module employs a Vector-Quantized Variational AutoEncoder
(VQVAE) as both an attention abstractor and controller, facilitating precise
attention management. By explicitly modeling attention allocation, our approach
aims to enhance system efficiency. We demonstrate ASAC's effectiveness in both
the vision and NLP domains, highlighting its ability to improve classification
accuracy and expedite the learning process. Our experiments with vision
transformers across various datasets illustrate that the attention controller
not only boosts classification accuracy but also accelerates learning.
Furthermore, we have demonstrated the model's robustness and generalization
capabilities across noisy and out-of-distribution datasets. In addition, we
have showcased improved performance in multi-task settings. Quick experiments
reveal that the attention schema-based module enhances resilience to
adversarial attacks, optimizes attention to improve learning efficiency, and
facilitates effective transfer learning and learning from fewer examples. These
promising results establish a connection between cognitive science and machine
learning, shedding light on the efficient utilization of attention mechanisms
in AI systems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [18] [SPH-Net: A Co-Attention Hybrid Model for Accurate Stock Price Prediction](https://arxiv.org/abs/2509.15414)
*Yiyang Wu,Hanyu Ma,Muxin Ge,Xiaoli Ma,Yadi Liu,Ye Aung Moe,Zeyu Han,Weizheng Xie*

Main category: cs.CE

TL;DR: 本文提出SPH - Net深度学习框架用于金融市场股价预测，实验表明其优于现有模型，能为投资者提供决策支持。


<details>
  <summary>Details</summary>
Motivation: 股票价格波动预测因市场数据的波动性、非平稳性和非线性而极具挑战，需要提升金融市场时间序列预测的准确性。

Method: 提出SPH - Net框架，采用新型协同注意力机制，先通过Vision Transformer处理时间模式，再用注意力机制提取特征，在八个不同股票数据集上进行实验，使用六个基本市场指标标准化数据。

Result: SPH - Net在所有评估指标上均优于现有股票预测模型。

Conclusion: SPH - Net能有效捕捉复杂时间模式，抗市场噪声能力强，可提高金融时间序列分析的预测准确性，为投资者和金融分析师提供决策支持。

Abstract: Prediction of stock price movements presents a formidable challenge in
financial analytics due to the inherent volatility, non-stationarity, and
nonlinear characteristics of market data. This paper introduces SPH-Net (Stock
Price Prediction Hybrid Neural Network), an innovative deep learning framework
designed to enhance the accuracy of time series forecasting in financial
markets. The proposed architecture employs a novel co-attention mechanism that
initially processes temporal patterns through a Vision Transformer, followed by
refined feature extraction via an attention mechanism, thereby capturing both
global and local dependencies in market data. To rigorously evaluate the
model's performance, we conduct comprehensive experiments on eight diverse
stock datasets: AMD, Ebay, Facebook, FirstService Corp, Tesla, Google, Mondi
ADR, and Matador Resources. Each dataset is standardized using six fundamental
market indicators: Open, High, Low, Close, Adjusted Close, and Volume,
representing a complete set of features for comprehensive market analysis.
Experimental results demonstrate that SPH-Net consistently outperforms existing
stock prediction models across all evaluation metrics. The model's superior
performance stems from its ability to effectively capture complex temporal
patterns while maintaining robustness against market noise. By significantly
improving prediction accuracy in financial time series analysis, SPH-Net
provides valuable decision-support capabilities for investors and financial
analysts, potentially enabling more informed investment strategies and risk
assessment in volatile market conditions.

</details>


### [19] [A Memory Efficient Adjoint Method to Enable Billion Parameter Optimization on a Single GPU in Dynamic Problems](https://arxiv.org/abs/2509.15744)
*Leon Herrmann,Tim Bürchner,László Kudela,Stefan Kollmannsberger*

Main category: cs.CE

TL;DR: 提出基于叠加原理的近似灵敏度计算新方法，降低内存负担，可处理大规模问题，但限于自伴问题。


<details>
  <summary>Details</summary>
Motivation: 现有动态优化受灵敏度计算所需波场信息存储的内存限制，现代GPU内存容量有限。

Method: 引入基于伴随方法（针对自伴问题）、依靠叠加原理的近似灵敏度计算新方法，迭代计算灵敏度。

Result: 能在当前GPU上对数十亿自由度的问题进行灵敏度计算，通过高效有限差分正演求解器在全波形反演和瞬态声学拓扑优化问题中验证。

Conclusion: 新方法降低内存负担，但近似技术限于自伴问题，无法考虑阻尼等现象。

Abstract: Dynamic optimization is currently limited by sensitivity computations that
require information from full forward and adjoint wave fields. Since the
forward and adjoint solutions are computed in opposing time directions, the
forward solution must be stored. This requires a substantial amount of memory
for large-scale problems even when using check pointing or data compression
techniques. As a result, the problem size is memory bound rather than bound by
wall clock time, when working with modern GPU-based implementations that have
limited memory capacity. To overcome this limitation, we introduce a new
approach for approximate sensitivity computation based on the adjoint method
(for self-adjoint problems) that relies on the principle of superposition. The
approximation allows an iterative computation of the sensitivity, reducing the
memory burden to that of the solution at a small number of time steps, i.e., to
the number of degrees of freedom. This enables sensitivity computations for
problems with billions of degrees of freedom on current GPUs, such as the A100
from NVIDIA (from 2020). We demonstrate the approach on full waveform inversion
and transient acoustic topology optimization problems, relying on a highly
efficient finite difference forward solver implemented in CUDA. Phenomena such
as damping cannot be considered, as the approximation technique is limited to
self-adjoint problems.

</details>


### [20] [A CARLA-based Simulation of Electrically Driven Forklifts](https://arxiv.org/abs/2509.15909)
*David Claus,Christiane Thielemann,Hans-Georg Stark*

Main category: cs.CE

TL;DR: 本文用CARLA模拟电动叉车车队厂内物流作业，介绍模拟步骤、能耗模拟及两个应用案例。


<details>
  <summary>Details</summary>
Motivation: 探索用新方法模拟电动叉车车队在厂内物流场景的作业。

Method: 使用开源工具CARLA生成并可视化3D室外仓库场景，模拟叉车运输任务、回放定位数据，集成物理电池模型模拟能耗。

Result: 实现叉车作业模拟、能耗模拟，展示了可视化效果。

Conclusion: CARLA模拟平台在物流模拟中有广泛应用，可用于检测高交通密度区域和确定充电站最优位置。

Abstract: This paper presents the simulation of the operation of an electric forklift
fleet within an intralogistics scenario. For this purpose, the open source
simulation tool CARLA is used; according to our knowledge this is a novel
approach in the context of logistics simulation. First, CARLA is used to
generate and visualize a realistic 3D outdoor warehouse scenario, incorporating
a number of randomly moving forklifts. In a next step, intralogistics transport
tasks, such as pick-and-place, are simulated for the forklift fleet, including
shortest-path finding. Furthermore, the capability to play back localization
data, previously recorded from a ''real'' forklift fleet, is demonstrated.This
play back is done in the original recreated environment, thereby enabling the
visualization of the forklifts movements. Finally, the energy consumption of
the forklift trucks is simulated by integrating a physical battery model that
generates the state of charge (SOC) of each truck as a function of load and
activity. To demonstrate the wide range of possible applications for the CARLA
simulation platform, we describe two use cases. The first deals with the
problem of detecting regions with critically high traffic densities, the second
with optimal placement of charging stations for the forklift trucks. Both use
cases are calculated for an exemplary warehouse model.

</details>


### [21] [Optimal Experimental Design of a Moving Sensor for Linear Bayesian Inverse Problems](https://arxiv.org/abs/2509.15961)
*Nicole Aretz,Thomas Lynn,Karen Willcox,Sven Leyffer*

Main category: cs.CE

TL;DR: 优化移动传感器路径以最小化贝叶斯反问题的后验不确定性，给出求解方法并展示对流 - 扩散方程计算结果。


<details>
  <summary>Details</summary>
Motivation: 解决贝叶斯反问题中如何最小化后验不确定性的问题。

Method: 针对线性偏微分方程推导后验协方差矩阵的闭式表达式，离散化问题，通过控制参数化设置约束，用内点法求解约束优化问题。

Result: 给出了对流 - 扩散方程未知初始条件下的计算结果。

Conclusion: 所提出的方法可用于优化移动传感器路径以降低贝叶斯反问题的后验不确定性。

Abstract: We optimize the path of a mobile sensor to minimize the posterior uncertainty
of a Bayesian inverse problem. Along its path, the sensor continuously takes
measurements of the state, which is a physical quantity modeled as the solution
of a partial differential equation (PDE) with uncertain parameters. Considering
linear PDEs specifically, we derive the closed-form expression of the posterior
covariance matrix of the model parameters as a function of the path, and
formulate the optimal experimental design problem for minimizing the
posterior's uncertainty. We discretize the problem such that the cost function
remains consistent under temporal refinement. Additional constraints ensure
that the path avoids obstacles and remains physically interpretable through a
control parameterization. The constrained optimization problem is solved using
an interior-point method. We present computational results for a
convection-diffusion equation with unknown initial condition.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [22] [Revealing Inherent Concurrency in Event Data: A Partial Order Approach to Process Discovery](https://arxiv.org/abs/2509.15346)
*Humam Kourani,Gyunam Park,Wil M. P. van der Aalst*

Main category: cs.DB

TL;DR: 传统流程发现算法线性化事件，难以捕捉并发，本文提出新的可扩展算法，能直接利用偏序进行流程发现，在复杂日志中验证了适用性。


<details>
  <summary>Details</summary>
Motivation: 传统流程发现算法线性化事件，无法捕捉并发，部分处理偏序数据的技术在大规模日志上可扩展性差。

Method: 从事件数据中导出偏序轨迹并聚合为流程模型，采用分层算法保留并发，抽象选择和循环模式。

Result: 实现了该技术，并在复杂真实事件日志中证明了其适用性。

Conclusion: 为更真实地表示流程行为提供了可扩展的解决方案，尤其适用于事件数据中并发普遍的情况。

Abstract: Process discovery algorithms traditionally linearize events, failing to
capture the inherent concurrency of real-world processes. While some techniques
can handle partially ordered data, they often struggle with scalability on
large event logs. We introduce a novel, scalable algorithm that directly
leverages partial orders in process discovery. Our approach derives partially
ordered traces from event data and aggregates them into a
sound-by-construction, perfectly fitting process model. Our hierarchical
algorithm preserves inherent concurrency while systematically abstracting
exclusive choices and loop patterns, enhancing model compactness and precision.
We have implemented our technique and demonstrated its applicability on complex
real-life event logs. Our work contributes a scalable solution for a more
faithful representation of process behavior, especially when concurrency is
prevalent in event data.

</details>


### [23] [Optimization techniques for SQL+ML queries: A performance analysis of real-time feature computation in OpenMLDB](https://arxiv.org/abs/2509.15529)
*Mashkhal A. Sidiq,Aras A. Salih,Samrand M. Hassan*

Main category: cs.DB

TL;DR: 本文在OpenMLDB上优化SQL+ML查询，通过实验证明其性能优于传统数据库，展示其适用于时间敏感的ML用例，并强调对高性能系统设计的贡献。


<details>
  <summary>Details</summary>
Motivation: 优化OpenMLDB上的SQL+ML查询，满足时间敏感的ML用例需求。

Method: 在Docker中使用特征丰富的合成数据集进行实验，聚焦于更好的查询计划、缓存执行计划、并行处理和资源管理。

Result: OpenMLDB能支持约12,500 QPS，延迟小于1 ms，性能优于SparkSQL、ClickHouse、PostgreSQL和MySQL；查询计划优化、缓存和并行处理分别带来35%、25%和20%的性能提升。

Conclusion: OpenMLDB适用于时间敏感的ML用例，其模块化优化框架带来显著性能提升，研究有助于高性能SQL+ML系统的理解和设计，强调ML工作负载需要专门的SQL优化。

Abstract: In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source
database that seamlessly integrates offline and online feature computations.
The work used feature-rich synthetic dataset experiments in Docker, which acted
like production environments that processed 100 to 500 records per batch and 6
to 12 requests per batch in parallel. Efforts have been concentrated in the
areas of better query plans, cached execution plans, parallel processing, and
resource management. The experimental results show that OpenMLDB can support
approximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL
and ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This
study assessed the impact of optimization and showed that query plan
optimization accounted for 35% of the performance gains, caching for 25%, and
parallel processing for 20%. These results illustrate OpenMLDB's capability for
time-sensitive ML use cases, such as fraud detection, personalized
recommendation, and time series forecasting. The system's modular optimization
framework, which combines batch and stream processing without interference,
contributes to its significant performance gain over traditional database
systems, particularly in applications that require real-time feature
computation and serving. This study contributes to the understanding and design
of high-performance SQL+ML systems and highlights the need for specialized SQL
optimization for ML workloads.

</details>


### [24] [Discovering Top-k Periodic and High-Utility Patterns](https://arxiv.org/abs/2509.15732)
*Qingfeng Zhou,Wensheng Gan,Guoting Chen*

Main category: cs.DB

TL;DR: 本文提出TPU算法解决PHUPM中用户兴趣和minutil设置问题，实验表明算法能准确有效提取top - k周期性高效用模式，还减少了运行时间和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 用户可能对所有周期性高效用模式（PHUPs）不感兴趣，且提前设置最小效用阈值（minutil）有挑战。

Method: 提出TPU算法，利用正负效用列表（PNUL）和周期估计效用共现结构（PEUCS）存储信息，采用周期实际项效用（PIU）、周期共现效用递减（PCUD）和周期实际效用（PRU）阈值提升策略。

Result: 运行时间约减少5%，在部分数据集上最高减少50%；内存消耗约减少2%，在部分数据集上最高减少30%。

Conclusion: 算法能准确有效提取top - k周期性高效用模式，成功解决top - k挖掘问题，为数据科学做贡献。

Abstract: With a user-specified minimum utility threshold (minutil), periodic
high-utility pattern mining (PHUPM) aims to identify high-utility patterns that
occur periodically in a transaction database. A pattern is deemed periodic if
its period aligns with the periodicity constraint set by the user. However,
users may not be interested in all periodic high-utility patterns (PHUPs).
Moreover, setting minutil in advance is also a challenging issue. To address
these issues, our research introduces an algorithm called TPU for extracting
the most significant top-k periodic and high-utility patterns that may or may
not include negative utility values. This TPU algorithm utilizes positive and
negative utility lists (PNUL) and period-estimated utility co-occurrence
structure (PEUCS) to store pertinent itemset information. It incorporates the
periodic real item utility (PIU), periodic co-occurrence utility descending
(PCUD), and periodic real utility (PRU) threshold-raising strategies to elevate
the thresholds rapidly. By using the proposed threshold-raising strategies, the
runtime was reduced by approximately 5\% on the datasets used in the
experiments. Specifically, the runtime was reduced by up to 50\% on the
mushroom\_negative and kosarak\_negative datasets, and by up to 10\% on the
chess\_negative dataset. Memory consumption was reduced by about 2\%, with the
largest reduction of about 30\% observed on the mushroom\_negative dataset.
Through extensive experiments, we have demonstrated that our algorithm can
accurately and effectively extract the top-k periodic high-utility patterns.
This paper successfully addresses the top-k mining issue and contributes to
data science.

</details>


### [25] [Utility-based Privacy Preserving Data Mining](https://arxiv.org/abs/2509.15755)
*Qingfeng Zhou,Wensheng Gan,Zhenlian Qi,Philip S. Yu*

Main category: cs.DB

TL;DR: 大数据时代周期模式挖掘有价值，但数据隐私受威胁。提出扩展PPUM框架的两个算法及数据结构，实验显示新算法能更好隐藏敏感项集且保持高数据库效用相似度。


<details>
  <summary>Details</summary>
Motivation: 现有PPUM不适用于周期性信息挖掘隐私问题，需新方法保护数据隐私。

Method: 扩展现有PPUM框架，提出MU - MAP和MU - MIP算法，设计SISL和SIL数据结构，用性能指标评估算法。

Result: 新算法隐藏敏感项集时人工成本为0，传统PPUM非零；新算法隐藏后数据库效用相似度超90%。

Conclusion: 新算法能成功隐藏敏感周期性项集，不引入误导模式，且保持高数据库效用相似度，优于传统PPUM算法。

Abstract: With the advent of big data, periodic pattern mining has demonstrated
significant value in real-world applications, including smart home systems,
healthcare systems, and the medical field. However, advances in network
technology have enabled malicious actors to extract sensitive information from
publicly available datasets, posing significant threats to data providers and,
in severe cases, hindering societal development. To mitigate such risks,
privacy-preserving utility mining (PPUM) has been proposed. However, PPUM is
unsuitable for addressing privacy concerns in periodic information mining. To
address this issue, we innovatively extend the existing PPUM framework and
propose two algorithms, Maximum sensitive Utility-MAximum maxPer item (MU-MAP)
and Maximum sensitive Utility-MInimum maxPer item (MU-MIP). These algorithms
aim to hide sensitive periodic high-utility itemsets while generating sanitized
datasets. To enhance the efficiency of the algorithms, we designed two novel
data structures: the Sensitive Itemset List (SISL) and the Sensitive Item List
(SIL), which store essential information about sensitive itemsets and their
constituent items. Moreover, several performance metrics were employed to
evaluate the performance of our algorithms compared to the state-of-the-art
PPUM algorithms. The experimental results show that our proposed algorithms
achieve an Artificial Cost (AC) value of 0 on all datasets when hiding
sensitive itemsets. In contrast, the traditional PPUM algorithm yields non-zero
AC. This indicates that our algorithms can successfully hide sensitive periodic
itemsets without introducing misleading patterns, whereas the PPUM algorithm
generates additional itemsets that may interfere with user decision-making.
Moreover, the results also reveal that our algorithms maintain Database Utility
Similarity (DUS) of over 90\% after the sensitive itemsets are hidden.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [26] [PCCL: Photonic circuit-switched collective communication for distributed ML](https://arxiv.org/abs/2509.15450)
*Abhishek Vijaya Kumar,Arjun Devraj,Rachee Singh*

Main category: cs.DC

TL;DR: 提出光子集体通信库PCCL，可消除拥塞和延迟，在不同场景有性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代分布式机器学习中集体通信算法理论与实际性能存在差距，由拥塞和跳数延迟导致。

Method: 提出PCCL，通过重新配置网络拓扑匹配集体算法通信模式，有硬件无关优化框架决定何时重新配置。

Result: 在128个GPU上，不同工作负载、缓冲区大小和拓扑下，PCCL比现有算法最高提速3倍，端到端训练吞吐量提速1.3倍。

Conclusion: PCCL能有效解决分布式机器学习中集体通信的性能问题，提升训练效率。

Abstract: Modern distributed ML suffers from a fundamental gap between the theoretical
and realized performance of collective communication algorithms due to
congestion and hop-count induced dilation in practical GPU clusters. We present
PCCL, a Photonic Collective Communication Library that reconfigures the network
topology to match the communication patterns of collective algorithms, thereby
eliminating congestion and dilation by creating direct, contention-free
circuits between communicating GPUs. Unlike prior approaches that synthesize
algorithms for specific network topologies and collectives, PCCL generalizes to
any collective primitive and any topology by adapting the network to match each
algorithm's communication pattern. PCCL's key innovation lies in its
hardware-agnostic optimization framework that intelligently decides when to
reconfigure based on the trade-off between network reconfiguration delay and
congestion/dilation costs, making it practical across different optical
hardware with varying switching speeds. Our evaluation demonstrates that PCCL
achieves up to 3X speedup over state-of-the-art algorithms on 128 GPUs across
various workloads, buffer sizes, and topologies, translating to a 1.3X speedup
in end-to-end training throughput.

</details>


### [27] [Angelfish: Consensus with Optimal Throughput and Latency Across the Leader-DAG Spectrum](https://arxiv.org/abs/2509.15847)
*Qianyu Yu,Giuliano Losa,Nibesh Shrestha,Xuechao Wang*

Main category: cs.DC

TL;DR: 介绍了现代区块链系统共识协议设计方向，提出混合协议Angelfish，结合两种协议优点。


<details>
  <summary>Details</summary>
Motivation: 现有区块链共识协议在不同负载下各有优劣，需一种能在不同设计间平滑适应的协议。

Method: 提出Angelfish协议，让动态调整的部分节点使用尽力广播发出轻量级投票。

Result: Angelfish实现了先进的峰值吞吐量，在中等吞吐量下达到基于领导者协议的低延迟。

Conclusion: Angelfish结合了基于领导者协议和基于DAG协议的优点。

Abstract: To maximize performance, many modern blockchain systems rely on
eventually-synchronous, Byzantine fault-tolerant (BFT) consensus protocols. Two
protocol designs have emerged in this space: protocols that minimize latency
using a leader that drives both data dissemination and consensus, and protocols
that maximize throughput using a separate, asynchronous data dissemination
layer. Recent protocols such as Partially-Synchronous Bullshark and Sailfish
combine elements of both approaches by using a DAG to enable parallel data
dissemination and a leader that paces DAG formation. This improves latency
while achieving state-of-the-art throughput. Yet the latency of leader-based
protocols is still better under moderate loads.
  We present Angelfish, a hybrid protocol that adapts smoothly across this
design space, from leader-based to Sailfish-like DAG-based consensus. Angelfish
lets a dynamically-adjusted subset of parties use best-effort broadcast to
issue lightweight votes instead of reliably broadcasting costlier DAG vertices.
This reduces communication, helps lagging nodes catch up, and lowers latency in
practice compared to prior DAG-based protocols. Our empirical evaluation shows
that Angelfish attains state-of-the-art peak throughput while matching the
latency of leader-based protocols under moderate throughput, delivering the
best of both worlds.

</details>


### [28] [Efficient Pre-Training of LLMs via Topology-Aware Communication Alignment on More Than 9600 GPUs](https://arxiv.org/abs/2509.15940)
*Guoliang He,Youhe Jiang,Wencong Xiao,Kaihua Jiang,Shuguang Wang,Jun Wang,Zixian Du,Zhuo Jiang,Xinlei Zhang,Binhang Yuan,Eiko Yoneki*

Main category: cs.DC

TL;DR: 文章提出Arnold调度系统，使大语言模型通信模式与数据中心拓扑有效匹配，模拟实验和生产训练显示其能提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练通信模式复杂，低效资源调度加剧带宽竞争，导致训练性能不佳。

Method: 深入研究物理网络拓扑对大语言模型预训练作业的影响，开发调度算法使通信模式与物理网络拓扑匹配。

Result: 模拟实验表明算法最多可将通信组的最大扩展减少1.67倍；生产训练中，使用超9600个GPU训练时，调度系统使端到端性能提升10.6%。

Conclusion: Arnold调度系统能有效提升大语言模型训练性能，对训练管道有显著改进。

Abstract: The scaling law for large language models (LLMs) depicts that the path
towards machine intelligence necessitates training at large scale. Thus,
companies continuously build large-scale GPU clusters, and launch training jobs
that span over thousands of computing nodes. However, LLM pre-training presents
unique challenges due to its complex communication patterns, where GPUs
exchange data in sparse yet high-volume bursts within specific groups.
Inefficient resource scheduling exacerbates bandwidth contention, leading to
suboptimal training performance. This paper presents Arnold, a scheduling
system summarizing our experience to effectively align LLM communication
patterns with data center topology at scale. An in-depth characteristic study
is performed to identify the impact of physical network topology to LLM
pre-training jobs. Based on the insights, we develop a scheduling algorithm to
effectively align communication patterns with the physical network topology in
modern data centers. Through simulation experiments, we show the effectiveness
of our algorithm in reducing the maximum spread of communication groups by up
to $1.67$x. In production training, our scheduling system improves the
end-to-end performance by $10.6\%$ when training with more than $9600$ GPUs, a
significant improvement for our training pipeline.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [29] [Graph-Based Approximate Nearest Neighbor Search Revisited: Theoretical Analysis and Optimization](https://arxiv.org/abs/2509.15531)
*Xinran Ma,Zhaoqi Zhou,Chuan Zhou,Qi Meng,Zaijiu Shang,Guoliang Li,Zhiming Ma*

Main category: cs.DS

TL;DR: 本文为基于图的近似最近邻搜索方法提供理论保证，提出截断参数优化策略，实验显示新方法性能佳且构建索引速度快。


<details>
  <summary>Details</summary>
Motivation: 现有Sparse Neighborhood Graph (SNG)理论理解有限，依赖启发式截断策略且常非最优，需弥合理论与实践差距。

Method: 通过基于鞅的分析刻画索引构建过程，证明索引图的度和查询处理时的搜索路径长度，据此提出选择截断参数R的新方法。

Result: 新方法在查询延迟和Recall@10上与常用二分搜索启发式方法相当或更优，整体索引构建速度提升2到9倍。

Conclusion: 所提理论保证和优化策略有效，新的截断参数选择方法性能良好且构建索引效率高。

Abstract: Graph-based approaches to approximate nearest neighbor search (ANNS) have
achieved remarkable success in enabling fast, high-recall retrieval on
billion-scale vector datasets. Among them, the Sparse Neighborhood Graph (SNG)
has emerged as a widely adopted graph structure due to its superior search
performance. However, the theoretical understanding of SNG remains limited,
leading to reliance on heuristic-based and often suboptimal truncation
strategies. In this work, we aim to bridge the gap between theory and practice
by providing formal guarantees for graph-based ANNS methods and proposing
principled optimization strategies for the truncation parameter. By
characterizing the index construction process through martingale-based
analysis, we show that the degree of the index graph is $O(n^{2/3+\epsilon})$,
where $\epsilon$ is an arbitrarily small constant. Furthermore, we prove that
the expected search path length during query processing is $O(\log n)$. Based
on these theoretical insights, we introduce a novel and principled method for
selecting the truncation parameter $R$ in SNG. Experimental results demonstrate
that our method achieves comparable or superior performance in terms of query
latency and Recall@10 compared to commonly used binary search heuristics, while
yielding 2x to 9x speedups in overall index construction.

</details>


### [30] [Constant time enumeration of perfect bipartite matchings](https://arxiv.org/abs/2509.16135)
*Jiří Fink*

Main category: cs.DS

TL;DR: 提出枚举二分图中所有完美匹配的算法，相比旧算法时间复杂度更优，还开发算术电路变体。


<details>
  <summary>Details</summary>
Motivation: 改进现有算法，以更优的时间复杂度枚举二分图中的所有完美匹配。

Method: 提出新算法，开发算术电路变体，用二叉树表示完美匹配。

Result: 新算法访问一个完美匹配的平摊时间为常数，优于25年前Uno的算法。

Conclusion: 新算法在二分图完美匹配枚举上有时间复杂度优势，算术电路变体或有更广泛应用。

Abstract: We present an algorithm that enumerates all the perfect matchings in a given
bipartite graph G = (V,E). Our algorithm requires a constant amortized time to
visit one perfect matching of G, in contrast to the current fastest algorithm,
published 25 years ago by Uno, which requires O(log |V|) time.
  To facilitate the listing of all edges in a visited perfect matching, we
develop a variant of arithmetic circuits, which may have broader applications
in future enumeration algorithms. Consequently, a visited perfect matching is
represented within a binary tree. Although it is more common to provide visited
objects in an array, we present a class of graphs for which achieving constant
amortized time is not feasible in this case.

</details>


### [31] [On the Structural Parameterizations of 2-Club with Triangle Constraints](https://arxiv.org/abs/2509.16143)
*Ashwin Jacob,Diptapriyo Majumdar,Raghav Sakhuja*

Main category: cs.DS

TL;DR: 对顶点 r - 三角 s - Club 问题从输入图的结构参数角度进行系统研究，给出不同参数化下的算法和内核。


<details>
  <summary>Details</summary>
Motivation: 此前 Garvardt 等人从参数化复杂度角度研究了推广 s - Club 的顶点 r - 三角 s - Club 等问题，本文要从输入图的结构参数角度对顶点 r - 三角 s - Club 进行系统研究。

Method: 对输入图的树宽、h 指数、反馈边数等结构参数进行分析，分别给出不同参数化下的 FPT 算法、XP 算法和内核。

Result: 当以输入图的树宽为参数时，为顶点 r - 三角 2 - Club 提供 FPT 算法；以输入图的 h 指数为参数时，提供 XP 算法；以输入图的反馈边数为参数时，为顶点 r - 三角 s - Club 提供 O(fes) 边的内核。

Conclusion: 从输入图的结构参数角度对顶点 r - 三角 s - Club 问题的研究是可行的，得到了不同参数化下的有效算法和内核。

Abstract: Given an undirected graph G = (V, E) and an integer k, the s-Club asks if
Gcontains a vertex subset S of at least k vertices such that G[S] has diameter
at most s. Recently, Vertex r-Triangle s-Club, and Edge r-Triangle s-Club that
generalize the notion of s-Club have been studied by Garvardt et al.
[TOCS-2023, IWOCA-2022] from the perspective of parameterized complexity. Given
a graph G and an integer k, the Vertex r-Triangle s-Club asks if there is an
s-Club S with at least k vertices such that every vertex u \in S is part of at
least r triangles in G[S]. In this paper, we initiate a systematic study of
Vertex r-Triangle s-Club for every integer r >= 1 from the perspective of
structural parameters of the input graph. In particular, we provide FPT
algorithms for Vertex r-Triangle 2-Club when parameterized by the treewidth
(tw) of the input graph, and an XP algorithm when parameterized by the h-index
of the input graph. Additionally, when parameterized by the feedback edge
number (fes) of the input graph. We provide a kernel of O(fes) edges for Vertex
r-Triangle s-Club.

</details>


### [32] [Analyzing and improving a classical Betti number estimation algorithm](https://arxiv.org/abs/2509.16171)
*Julien Sorci*

Main category: cs.DS

TL;DR: 本文对经典单纯复形归一化Betti数估计算法的样本复杂度进行深入分析，提出改进算法并通过随机图模型展示其有效性与局限性。


<details>
  <summary>Details</summary>
Motivation: 受具有类似蒙特卡罗结构和改进样本复杂度的量子算法启发，深入分析经典算法的样本复杂度。

Method: 给出经典算法中估计器方差的界，证明方差与单纯复形的组合性质有关，提出改进算法；用Erdős - Renyi随机图模型进行验证。

Result: 改进算法对部分方差足够小的单纯复形降低了样本复杂度；给出两种算法样本复杂度呈指数级的情况。

Conclusion: 展示了经典算法的有效性和局限性，改进算法在特定模型中能降低样本复杂度。

Abstract: Recently, a classical algorithm for estimating the normalized Betti number of
an arbitrary simplicial complex was proposed. Motivated by a quantum algorithm
with a similar Monte Carlo structure and improved sample complexity, we give a
more in-depth analysis of the sample complexity of this classical algorithm. To
this end, we present bounds for the variance of the estimators used in the
classical algorithm and show that the variance depends on certain combinatorial
properties of the underlying simplicial complex. This new analysis leads us to
propose an improvement to the classical algorithm which makes the "easy cases
easier'', in that it reduces the sample complexity for simplicial complexes
where the variance is sufficiently small. We show the effectiveness and
limitations of these classical algorithms by considering Erd\H{o}s-Renyi random
graph models to demonstrate the existence of "easy" and "hard" cases. Namely,
we show that for certain models our improvement almost always leads to a
reduced sample complexity, and also produce separate regimes where the sample
complexity for both algorithms is exponential.

</details>


### [33] [Clustering with Set Outliers and Applications in Relational Clustering](https://arxiv.org/abs/2509.16194)
*Vaishali Surianarayanan,Neeraj Kumar,Stavros Sintos*

Main category: cs.DS

TL;DR: 本文研究带集合离群点的k - 中心聚类问题，给出近似算法，分析复杂度并与关系聚类建立联系。


<details>
  <summary>Details</summary>
Motivation: 经典带离群点的k - 中心聚类问题是移除单个数据点，本文模型考虑移除候选离群点集合中的子集，以捕捉数据库应用中的结构化噪声。

Method: 在一般和几何场景下给出三准则近似算法，利用范围和BBD树，在f = 1时构建小核心集改进算法运行时间，还给出一般问题的难度结果。

Result: 算法选择最多2k个中心和2fz个离群点集，聚类成本达到O(1)近似，在几何场景实现近线性时间算法。

Conclusion: 该模型能自然捕捉带离群点的关系聚类，建立了鲁棒聚类和关系查询评估的紧密联系。

Abstract: We introduce and study the $k$-center clustering problem with set outliers, a
natural and practical generalization of the classical $k$-center clustering
with outliers. Instead of removing individual data points, our model allows
discarding up to $z$ subsets from a given family of candidate outlier sets
$\mathcal{H}$. Given a metric space $(P,\mathsf{dist})$, where $P$ is a set of
elements and $\mathsf{dist}$ a distance metric, a family of sets
$\mathcal{H}\subseteq 2^P$, and parameters $k, z$, the goal is to compute a set
of $k$ centers $S\subseteq P$ and a family of $z$ sets $H\subseteq \mathcal{H}$
to minimize $\max_{p\in P\setminus(\bigcup_{h\in H} h)} \min_{s\in
S}\mathsf{dist}(p,s)$. This abstraction captures structured noise common in
database applications, such as faulty data sources or corrupted records in data
integration and sensor systems.
  We present the first approximation algorithms for this problem in both
general and geometric settings. Our methods provide tri-criteria
approximations: selecting up to $2k$ centers and $2f z$ outlier sets (where $f$
is the maximum number of sets that a point belongs to), while achieving
$O(1)$-approximation in clustering cost. In geometric settings, we leverage
range and BBD trees to achieve near-linear time algorithms. In many real
applications $f=1$. In this case we further improve the running time of our
algorithms by constructing small \emph{coresets}. We also provide a hardness
result for the general problem showing that it is unlikely to get any sublinear
approximation on the clustering cost selecting less than $f\cdot z$ outlier
sets.
  We demonstrate that this model naturally captures relational clustering with
outliers: outliers are input tuples whose removal affects the join output. We
provide approximation algorithms for both, establishing a tight connection
between robust clustering and relational query evaluation.

</details>


### [34] [Query-Efficient Locally Private Hypothesis Selection via the Scheffe Graph](https://arxiv.org/abs/2509.16180)
*Gautam Kamath,Alireza F. Pour,Matthew Regehr,David P. Woodruff*

Main category: cs.DS

TL;DR: 提出改进查询复杂度的算法用于局部差分隐私约束下假设选择问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有算法在局部差分隐私约束下假设选择问题时，存在查询次数多或需多轮交互查询的问题。

Method: 引入名为Scheffé图的新对象，利用其捕捉概率分布集Q中分布差异结构，设计满足局部差分隐私的算法。

Result: 算法能以$	ilde{O}(k^{3/2})$次非自适应查询输出接近真实分布p的概率分布，优于之前算法。

Conclusion: 新算法在局部差分隐私约束下假设选择问题上有更好的查询复杂度，Scheffé图或对假设选择任务有更广泛应用价值。

Abstract: We propose an algorithm with improved query-complexity for the problem of
hypothesis selection under local differential privacy constraints. Given a set
of $k$ probability distributions $Q$, we describe an algorithm that satisfies
local differential privacy, performs $\tilde{O}(k^{3/2})$ non-adaptive queries
to individuals who each have samples from a probability distribution $p$, and
outputs a probability distribution from the set $Q$ which is nearly the closest
to $p$. Previous algorithms required either $\Omega(k^2)$ queries or many
rounds of interactive queries.
  Technically, we introduce a new object we dub the Scheff\'e graph, which
captures structure of the differences between distributions in $Q$, and may be
of more broad interest for hypothesis selection tasks.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [35] [Diversity of Structured Domains via k-Kemeny Scores](https://arxiv.org/abs/2509.15812)
*Piotr Faliszewski,Krzysztof Sornat,Stanisław Szufa,Tomasz Wąs*

Main category: cs.GT

TL;DR: 研究k - Kemeny问题在多种结构化领域的情况，得出问题难解及用其对领域多样性排序的结果


<details>
  <summary>Details</summary>
Motivation: 研究k - Kemeny问题在单峰、单交叉、组可分和欧几里得等结构化领域的特性

Method: 对k - Kemeny问题在不同结构化领域进行研究

Result: 1. k - Kemeny在多数领域即使k = 2也难解；2. 用k - Kemeny对领域多样性进行排序

Conclusion: k - Kemeny在多种结构化领域有特定性质，可用于领域多样性排序

Abstract: In the k-Kemeny problem, we are given an ordinal election, i.e., a collection
of votes ranking the candidates from best to worst, and we seek the smallest
number of swaps of adjacent candidates that ensure that the election has at
most k different rankings. We study this problem for a number of structured
domains, including the single-peaked, single-crossing, group-separable, and
Euclidean ones. We obtain two kinds of results: (1) We show that k-Kemeny
remains intractable under most of these domains, even for k=2, and (2) we use
k-Kemeny to rank these domains in terms of their diversity.

</details>


### [36] [Strategy Improvement, the Simplex Algorithm and Lopsidedness](https://arxiv.org/abs/2509.16075)
*Matthew Maat*

Main category: cs.GT

TL;DR: 证明策略改进算法与单纯形算法的直接联系并推导相关游戏策略集组合性质。


<details>
  <summary>Details</summary>
Motivation: 早期两种算法联系构建中间马尔可夫决策过程不总是可行，需建立直接联系。

Method: 在非退化假设下证明策略改进算法多种变体是单纯形算法实例。

Result: 得到图上相关游戏策略集结构的组合性质，发现与不均衡集的联系。

Conclusion: 建立了策略改进算法和单纯形算法的直接联系。

Abstract: The strategy improvement algorithm for mean payoff games and parity games is
a local improvement algorithm, just like the simplex algorithm for linear
programs. Their similarity has turned out very useful: many lower bounds on
running time for the simplex method have been created from lower bounds for
strategy improvement. However, earlier connections between these algorithms
required constructing an intermediate Markov decision process, which is not
always possible. We prove a formal, direct connection between the two
algorithms, showing that many variants of strategy improvement for parity and
mean payoff games are truly an instance of the simplex algorithm, under mild
nondegeneracy assumptions. As a result of this, we derive some combinatorial
properties of the structure of strategy sets of various related games on
graphs. In particular, we show a connection to lopsided sets.

</details>


### [37] [Strategic Analysis of Just-In-Time Liquidity Provision in Concentrated Liquidity Market Makers](https://arxiv.org/abs/2509.16157)
*Bruno Llacer Trotti,Weizhao Tang,Rachid El-Azouzi,Giulia Fanti,Daniel Sadoc Menasche*

Main category: cs.GT

TL;DR: 本文为CLMMs提供首个JIT流动性供给模型，分析JIT LP优化问题，发现实际与最优JIT行为有差距，JIT策略可提高市场效率但影响被动LP利润。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对JIT LPs的原则性理解，需研究其在CLMMs中的情况。

Method: 建立交易层面的模型，刻画价格影响和费用分配，分析非线性优化问题并证明最优策略存在。

Result: 发现实际与最优JIT行为有显著差距，考虑价格影响可使JIT LP收益平均提高69%，JIT策略能降低交易滑点但使被动LP利润每笔交易最多减少44%。

Conclusion: 为JIT LPs在CLMMs中的研究提供了正式模型，揭示了JIT策略对市场效率和LP利润的影响。

Abstract: Liquidity providers (LPs) are essential figures in the operation of automated
market makers (AMMs); in exchange for transaction fees, LPs lend the liquidity
that allows AMMs to operate. While many prior works have studied the incentive
structures of LPs in general, we currently lack a principled understanding of a
special class of LPs known as Just-In-Time (JIT) LPs. These are strategic
agents who momentarily supply liquidity for a single swap, in an attempt to
extract disproportionately high fees relative to the remaining passive LPs.
This paper provides the first formal, transaction-level model of JIT liquidity
provision for a widespread class of AMMs known as Concentrated Liquidity Market
Makers (CLMMs), as seen in Uniswap V3, for instance. We characterize the
landscape of price impact and fee allocation in these systems, formulate and
analyze a non-linear optimization problem faced by JIT LPs, and prove the
existence of an optimal strategy. By fitting our optimal solution for JIT LPs
to real-world CLMMs, we observe that in liquidity pools (particularly those
with risky assets), there is a significant gap between observed and optimal JIT
behavior. Existing JIT LPs often fail to account for price impact; doing so, we
estimate they could increase earnings by up to 69% on average over small time
windows. We also show that JIT liquidity, when deployed strategically, can
improve market efficiency by reducing slippage for traders, albeit at the cost
of eroding average passive LP profits by up to 44% per trade.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [38] [Efficient and Versatile Model for Multilingual Information Retrieval of Islamic Text: Development and Deployment in Real-World Scenarios](https://arxiv.org/abs/2509.15380)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.IR

TL;DR: 本文利用古兰经多语言语料库研究伊斯兰领域多语言信息检索（MLIR）系统开发策略，评估多种训练方法，混合方法效果好，并讨论部署考量。


<details>
  <summary>Details</summary>
Motivation: 当前MLIR研究与实际部署存在差距，许多研究评估场景孤立，缺乏对现实场景的适用性。

Method: 准备十一个检索模型，采用单语言、跨语言、翻译训练所有和混合方法（结合跨语言和单语言技术）四种训练方法。

Result: 在领域内数据集上评估显示，混合方法在不同检索场景下取得了有前景的结果。

Conclusion: 对不同训练配置影响嵌入空间和多语言检索有效性进行分析，强调部署单一通用轻量级模型用于现实MLIR应用的成本效益。

Abstract: Despite recent advancements in Multilingual Information Retrieval (MLIR), a
significant gap remains between research and practical deployment. Many studies
assess MLIR performance in isolated settings, limiting their applicability to
real-world scenarios. In this work, we leverage the unique characteristics of
the Quranic multilingual corpus to examine the optimal strategies to develop an
ad-hoc IR system for the Islamic domain that is designed to satisfy users'
information needs in multiple languages. We prepared eleven retrieval models
employing four training approaches: monolingual, cross-lingual,
translate-train-all, and a novel mixed method combining cross-lingual and
monolingual techniques. Evaluation on an in-domain dataset demonstrates that
the mixed approach achieves promising results across diverse retrieval
scenarios. Furthermore, we provide a detailed analysis of how different
training configurations affect the embedding space and their implications for
multilingual retrieval effectiveness. Finally, we discuss deployment
considerations, emphasizing the cost-efficiency of deploying a single
versatile, lightweight model for real-world MLIR applications.

</details>


### [39] [SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models](https://arxiv.org/abs/2509.15432)
*Thong Nguyen,Yibin Lei,Jia-Huei Ju,Andrew Yates*

Main category: cs.IR

TL;DR: 本文重新审视零样本生成-编码管道用于视觉文档检索，在ViDoRe - v2基准上表现出色，建立了零样本基线。


<details>
  <summary>Details</summary>
Motivation: 改进视觉文档检索（VDR）方法，避免传统方法中计算密集的文本 - 图像对比训练。

Method: 采用零样本生成 - 编码管道，先用视觉 - 语言模型生成文档图像的文本描述，再用标准文本编码器嵌入。

Result: 在ViDoRe - v2基准上nDCG@5达到63.4%，超越最强的多向量视觉文档编码器，扩展性和多语言覆盖性更好。

Conclusion: 现代视觉 - 语言模型可作为可复用语义代理，该方法无需计算密集的训练，为未来VDR系统建立了强零样本基线。

Abstract: Visual Document Retrieval (VDR) typically operates as text-to-image retrieval
using specialized bi-encoders trained to directly embed document images. We
revisit a zero-shot generate-and-encode pipeline: a vision-language model first
produces a detailed textual description of each document image, which is then
embedded by a standard text encoder. On the ViDoRe-v2 benchmark, the method
reaches 63.4% nDCG@5, surpassing the strongest specialised multi-vector visual
document encoder. It also scales better to large collections and offers broader
multilingual coverage. Analysis shows that modern vision-language models
capture complex textual and visual cues with sufficient granularity to act as a
reusable semantic proxy. By offloading modality alignment to pretrained
vision-language models, our approach removes the need for computationally
intensive text-image contrastive training and establishes a strong zero-shot
baseline for future VDR systems.

</details>


### [40] [Dual-Mode Visual System for Brain-Computer Interfaces: Integrating SSVEP and P300 Responses](https://arxiv.org/abs/2509.15439)
*Ekgari Kasawala,Surej Mouli*

Main category: cs.IR

TL;DR: 本文开发并评估基于LED的双刺激装置，结合SSVEP和P300范式提高分类准确率，系统实现较好效果。


<details>
  <summary>Details</summary>
Motivation: 传统基于LCD的视觉刺激范式在实际部署中有局限，需开发新装置提高SSVEP分类准确率。

Method: 开发基于LED的双刺激装置，用4种频率对应不同方向控制，通过分析最大FFT幅度和检测P300峰值进行实时特征提取。

Result: 视觉刺激硬件频率偏差小，信号处理算法能区分4种刺激频率，混合系统平均分类准确率86.25%，平均ITR为42.08 bpm。

Conclusion: 提出的基于LED的混合系统在提高SSVEP分类准确率方面有效。

Abstract: In brain-computer interface (BCI) systems, steady-state visual evoked
potentials (SSVEP) and P300 responses have achieved widespread implementation
owing to their superior information transfer rates (ITR) and minimal training
requirements. These neurophysiological signals have exhibited robust efficacy
and versatility in external device control, demonstrating enhanced precision
and scalability. However, conventional implementations predominantly utilise
liquid crystal display (LCD)-based visual stimulation paradigms, which present
limitations in practical deployment scenarios. This investigation presents the
development and evaluation of a novel light-emitting diode (LED)-based dual
stimulation apparatus designed to enhance SSVEP classification accuracy through
the integration of both SSVEP and P300 paradigms. The system employs four
distinct frequencies, 7 Hz, 8 Hz, 9 Hz, and 10 Hz, corresponding to forward,
backward, right, and left directional controls, respectively. Oscilloscopic
verification confirmed the precision of these stimulation frequencies.
Real-time feature extraction was accomplished through the concurrent analysis
of maximum Fast Fourier Transform (FFT) amplitude and P300 peak detection to
ascertain user intent. Directional control was determined by the frequency
exhibiting maximal amplitude characteristics. The visual stimulation hardware
demonstrated minimal frequency deviation, with error differentials ranging from
0.15%to 0.20%across all frequencies. The implemented signal processing
algorithm successfully discriminated all four stimulus frequencies whilst
correlating them with their respective P300 event markers. Classification
accuracy was evaluated based on correct task intention recognition. The
proposed hybrid system achieved a mean classification accuracy of 86.25%,
coupled with an average ITR of 42.08 bits per minute (bpm).

</details>


### [41] [CFDA & CLIP at TREC iKAT 2025: Enhancing Personalized Conversational Search via Query Reformulation and Rank Fusion](https://arxiv.org/abs/2509.15588)
*Yu-Cheng Chang,Guan-Wei Yeo,Quah Eugene,Fan-Jie Shih,Yuan-Ching Kuo,Tsung-En Yu,Hung-Chun Hsu,Ming-Feng Tsai,Chuan-Ju Wang*

Main category: cs.IR

TL;DR: 针对2025 TREC iKAT的交互和离线提交任务，采用查询重写和检索融合策略，结果显示重排序和融合提升了鲁棒性，且存在有效性和效率的权衡。


<details>
  <summary>Details</summary>
Motivation: 应对2025 TREC iKAT的交互和离线提交任务，满足实时约束下系统的鲁棒性、效率和准确性要求。

Method: 以Best - of - $N$选择和互反排名融合（RRF）策略为核心构建管道，采用查询重写和检索融合策略。

Result: 重排序和融合提高了鲁棒性，且在两个任务中揭示了有效性和效率之间的权衡。

Conclusion: 查询重写、检索融合及相关策略有助于处理不同提交任务，但要考虑有效性和效率的平衡。

Abstract: The 2025 TREC Interactive Knowledge Assistance Track (iKAT) featured both
interactive and offline submission tasks. The former requires systems to
operate under real-time constraints, making robustness and efficiency as
important as accuracy, while the latter enables controlled evaluation of
passage ranking and response generation with pre-defined datasets. To address
this, we explored query rewriting and retrieval fusion as core strategies. We
built our pipelines around Best-of-$N$ selection and Reciprocal Rank Fusion
(RRF) strategies to handle different submission tasks. Results show that
reranking and fusion improve robustness while revealing trade-offs between
effectiveness and efficiency across both tasks.

</details>


### [42] [Chunk Knowledge Generation Model for Enhanced Information Retrieval: A Multi-task Learning Approach](https://arxiv.org/abs/2509.15658)
*Jisu Kim,Jinhee Park,Changhyun Jeon,Jungwoo Choi,Keonwoo Kim,Minji Hong,Sehyun Kim*

Main category: cs.IR

TL;DR: 传统查询扩展技术有局限，文档扩展方法也有问题，本文提出分块知识生成模型，并行生成语义信息用于检索，实验显示效果好。


<details>
  <summary>Details</summary>
Motivation: 解决传统查询扩展技术和现有文档扩展方法存在的性能下降、预处理成本高、索引增大等问题，寻求更结构化高效的替代方案。

Method: 将文档分成块单元，采用基于T5的多任务学习结构，通过一次编码和两次解码并行生成标题、候选问题并提取关键词，将生成数据作为检索系统额外信息。

Result: 对305个查询 - 文档对的GPT评估显示，该模型在Top@10检索准确率达95.41%，优于文档块级检索。

Conclusion: 提出从文档块同时生成标题和候选问题用于检索的方法，通过定性评估证明能提高检索准确性，为大规模信息检索系统提供经验证据。

Abstract: Traditional query expansion techniques for addressing vocabulary mismatch
problems in information retrieval are context-sensitive and may lead to
performance degradation. As an alternative, document expansion research has
gained attention, but existing methods such as Doc2Query have limitations
including excessive preprocessing costs, increased index size, and reliability
issues with generated content. To mitigate these problems and seek more
structured and efficient alternatives, this study proposes a method that
divides documents into chunk units and generates textual data for each chunk to
simultaneously improve retrieval efficiency and accuracy. The proposed "Chunk
Knowledge Generation Model" adopts a T5-based multi-task learning structure
that simultaneously generates titles and candidate questions from each document
chunk while extracting keywords from user queries. This approach maximizes
computational efficiency by generating and extracting three types of semantic
information in parallel through a single encoding and two decoding processes.
The generated data is utilized as additional information in the retrieval
system. GPT-based evaluation on 305 query-document pairs showed that retrieval
using the proposed model achieved 95.41% accuracy at Top@10, demonstrating
superior performance compared to document chunk-level retrieval. This study
contributes by proposing an approach that simultaneously generates titles and
candidate questions from document chunks for application in retrieval
pipelines, and provides empirical evidence applicable to large-scale
information retrieval systems by demonstrating improved retrieval accuracy
through qualitative evaluation.

</details>


### [43] [Understanding Embedding Scaling in Collaborative Filtering](https://arxiv.org/abs/2509.15709)
*Zhuangzhuang He,Zhou Kaiyu,Haoyue Bai,Fengbin Zhu,Yonghui Yang*

Main category: cs.IR

TL;DR: 本文通过大规模实验发现扩展协同过滤模型嵌入维度时的双峰和对数两种新现象，分析了双峰现象原因和模型噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩展推荐模型成大型模型受关注，此前扩展嵌入维度可能致性能下降且原因不明，不同模型和数据集情况未知。

Method: 使用4种经典架构在10个不同稀疏度和规模的数据集上进行大规模实验。

Result: 发现双峰和对数两种新现象，前者性能随嵌入维度增加先升后降再升再降，后者呈对数曲线。

Conclusion: 发现新现象，理解双峰现象原因，理论分析模型噪声鲁棒性与实验观察匹配。

Abstract: Scaling recommendation models into large recommendation models has become one
of the most widely discussed topics. Recent efforts focus on components beyond
the scaling embedding dimension, as it is believed that scaling embedding may
lead to performance degradation. Although there have been some initial
observations on embedding, the root cause of their non-scalability remains
unclear. Moreover, whether performance degradation occurs across different
types of models and datasets is still an unexplored area. Regarding the effect
of embedding dimensions on performance, we conduct large-scale experiments
across 10 datasets with varying sparsity levels and scales, using 4
representative classical architectures. We surprisingly observe two novel
phenomenon: double-peak and logarithmic. For the former, as the embedding
dimension increases, performance first improves, then declines, rises again,
and eventually drops. For the latter, it exhibits a perfect logarithmic curve.
Our contributions are threefold. First, we discover two novel phenomena when
scaling collaborative filtering models. Second, we gain an understanding of the
underlying causes of the double-peak phenomenon. Lastly, we theoretically
analyze the noise robustness of collaborative filtering models, with results
matching empirical observations.

</details>


### [44] [Optimizing Product Deduplication in E-Commerce with Multimodal Embeddings](https://arxiv.org/abs/2509.15858)
*Aysenur Kulunk,Berk Taskin,M. Furkan Eseoglu,H. Bahadir Sahin*

Main category: cs.IR

TL;DR: 提出适用于电商领域的可扩展多模态产品去重系统，结合特定领域文本模型、图像表征技术、新的决策模型和向量数据库，在大规模电商环境中表现良好。


<details>
  <summary>Details</summary>
Motivation: 大规模电商市场中重复产品列表导致消费者困惑和运营低效，传统关键词搜索方法难以准确识别重复项。

Method: 采用基于BERT架构的特定领域文本模型和MaskedAutoEncoders进行图像表征，结合降维技术生成128维嵌入，开发新的决策模型，集成特征提取机制与向量数据库Milvus。

Result: 匹配系统达到0.90的宏平均F1分数，优于第三方解决方案的0.83。

Conclusion: 结合特定领域适配与先进机器学习技术可缓解大规模电商环境中的重复列表问题。

Abstract: In large scale e-commerce marketplaces, duplicate product listings frequently
cause consumer confusion and operational inefficiencies, degrading trust on the
platform and increasing costs. Traditional keyword-based search methodologies
falter in accurately identifying duplicates due to their reliance on exact
textual matches, neglecting semantic similarities inherent in product titles.
To address these challenges, we introduce a scalable, multimodal product
deduplication designed specifically for the e-commerce domain. Our approach
employs a domain-specific text model grounded in BERT architecture in
conjunction with MaskedAutoEncoders for image representations. Both of these
architectures are augmented with dimensionality reduction techniques to produce
compact 128-dimensional embeddings without significant information loss.
Complementing this, we also developed a novel decider model that leverages both
text and image vectors. By integrating these feature extraction mechanisms with
Milvus, an optimized vector database, our system can facilitate efficient and
high-precision similarity searches across extensive product catalogs exceeding
200 million items with just 100GB of system RAM consumption. Empirical
evaluations demonstrate that our matching system achieves a macro-average F1
score of 0.90, outperforming third-party solutions which attain an F1 score of
0.83. Our findings show the potential of combining domain-specific adaptations
with state-of-the-art machine learning techniques to mitigate duplicate
listings in large-scale e-commerce environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems](https://arxiv.org/abs/2509.15448)
*Saeed Amizadeh,Sara Abdali,Yinheng Li,Kazuhito Koishida*

Main category: cs.LG

TL;DR: 本文提出新方法处理多模态、多尺度数据的注意力机制，推导出最优公式并给出算法，能用于训练和改进transformer模型。


<details>
  <summary>Details</summary>
Motivation: 现有transformer在处理不同尺度、模态数据时的注意力机制难以泛化，多基于临时启发式方法。

Method: 提出数学结构表示多模态、多尺度数据，从熵最小化原理推导注意力机制，用动态规划算法计算。

Result: 推导出的公式在融入问题的层次/几何信息时最接近标准Softmax注意力，所提机制可用于从头训练模型和注入层次信息。

Conclusion: 所提层次注意力机制有效，能提升transformer模型效率。

Abstract: Transformers and their attention mechanism have been revolutionary in the
field of Machine Learning. While originally proposed for the language data,
they quickly found their way to the image, video, graph, etc. data modalities
with various signal geometries. Despite this versatility, generalizing the
attention mechanism to scenarios where data is presented at different scales
from potentially different modalities is not straightforward. The attempts to
incorporate hierarchy and multi-modality within transformers are largely based
on ad hoc heuristics, which are not seamlessly generalizable to similar
problems with potentially different structures. To address this problem, in
this paper, we take a fundamentally different approach: we first propose a
mathematical construct to represent multi-modal, multi-scale data. We then
mathematically derive the neural attention mechanics for the proposed construct
from the first principle of entropy minimization. We show that the derived
formulation is optimal in the sense of being the closest to the standard
Softmax attention while incorporating the inductive biases originating from the
hierarchical/geometric information of the problem. We further propose an
efficient algorithm based on dynamic programming to compute our derived
attention mechanism. By incorporating it within transformers, we show that the
proposed hierarchical attention mechanism not only can be employed to train
transformer models in hierarchical/multi-modal settings from scratch, but it
can also be used to inject hierarchical information into classical, pre-trained
transformer models post training, resulting in more efficient models in
zero-shot manner.

</details>


### [46] [Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning](https://arxiv.org/abs/2509.15230)
*Rutger Hendrix,Giovanni Patanè,Leonardo G. Russo,Simone Carnemolla,Giovanni Bellitto,Federica Proietto Salanitri,Concetto Spampinato,Matteo Pennisi*

Main category: cs.LG

TL;DR: 本文提出将遗忘能力内置的提示学习框架，可即时遗忘数据，保障隐私安全，为AI模型设计奠定新基础。


<details>
  <summary>Details</summary>
Motivation: 基础模型静态部署与隐私法规要求的数据遗忘需求冲突，传统遗忘方法有缺陷。

Method: 引入提示学习框架，将类级语义绑定到提示令牌，通过移除提示实现即时遗忘。

Result: 框架保留保留类的预测性能，有效擦除遗忘类，抗成员推理攻击，防止残留知识提取。

Conclusion: 将可移除性嵌入架构，为设计模块化、可扩展和符合道德的AI模型建立新基础。

Abstract: Foundation models have transformed multimedia analysis by enabling robust and
transferable representations across diverse modalities and tasks. However,
their static deployment conflicts with growing societal and regulatory demands
-- particularly the need to unlearn specific data upon request, as mandated by
privacy frameworks such as the GDPR. Traditional unlearning approaches,
including retraining, activation editing, or distillation, are often
computationally expensive, fragile, and ill-suited for real-time or
continuously evolving systems. In this paper, we propose a paradigm shift:
rethinking unlearning not as a retroactive intervention but as a built-in
capability. We introduce a prompt-based learning framework that unifies
knowledge acquisition and removal within a single training phase. Rather than
encoding information in model weights, our approach binds class-level semantics
to dedicated prompt tokens. This design enables instant unlearning simply by
removing the corresponding prompt -- without retraining, model modification, or
access to original data. Experiments demonstrate that our framework preserves
predictive performance on retained classes while effectively erasing forgotten
ones. Beyond utility, our method exhibits strong privacy and security
guarantees: it is resistant to membership inference attacks, and prompt removal
prevents any residual knowledge extraction, even under adversarial conditions.
This ensures compliance with data protection principles and safeguards against
unauthorized access to forgotten information, making the framework suitable for
deployment in sensitive and regulated environments. Overall, by embedding
removability into the architecture itself, this work establishes a new
foundation for designing modular, scalable and ethically responsive AI models.

</details>


### [47] [Instance Generation for Meta-Black-Box Optimization through Latent Space Reverse Engineering](https://arxiv.org/abs/2509.15810)
*Chen Wang,Zeyuan Ma,Zhiguang Cao,Yue-Jiao Gong*

Main category: cs.LG

TL;DR: 提出LSRE实例生成方法为MetaBBO生成多样训练问题实例，实验验证其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的MetaBBO研究常用CoCo - BBOB作为训练问题集，其多样性有限，存在过拟合和泛化能力差的风险。

Method: 先训练自编码器将高维问题特征映射到二维潜在空间，在该空间均匀网格采样得到多样隐藏表示，再用遗传编程方法搜索与隐藏表示L2距离最小的函数公式，逆向工程出多样化问题集Diverse - BBO。

Result: 在Diverse - BBO上训练各种MetaBBO，观察其在合成或现实场景中的泛化性能，实验表明Diverse - BBO优于现有训练集选择。

Conclusion: LSRE方法有效，消融研究揭示了实例多样性和MetaBBO泛化的有趣见解。

Abstract: To relieve intensive human-expertise required to design optimization
algorithms, recent Meta-Black-Box Optimization (MetaBBO) researches leverage
generalization strength of meta-learning to train neural network-based
algorithm design policies over a predefined training problem set, which
automates the adaptability of the low-level optimizers on unseen problem
instances. Currently, a common training problem set choice in existing MetaBBOs
is well-known benchmark suites CoCo-BBOB. Although such choice facilitates the
MetaBBO's development, problem instances in CoCo-BBOB are more or less limited
in diversity, raising the risk of overfitting of MetaBBOs, which might further
results in poor generalization. In this paper, we propose an instance
generation approach, termed as \textbf{LSRE}, which could generate diverse
training problem instances for MetaBBOs to learn more generalizable policies.
LSRE first trains an autoencoder which maps high-dimensional problem features
into a 2-dimensional latent space. Uniform-grid sampling in this latent space
leads to hidden representations of problem instances with sufficient diversity.
By leveraging a genetic-programming approach to search function formulas with
minimal L2-distance to these hidden representations, LSRE reverse engineers a
diversified problem set, termed as \textbf{Diverse-BBO}. We validate the
effectiveness of LSRE by training various MetaBBOs on Diverse-BBO and observe
their generalization performances on either synthetic or realistic scenarios.
Extensive experimental results underscore the superiority of Diverse-BBO to
existing training set choices in MetaBBOs. Further ablation studies not only
demonstrate the effectiveness of design choices in LSRE, but also reveal
interesting insights on instance diversity and MetaBBO's generalization.

</details>


### [48] [A Multi-Scale Graph Neural Process with Cross-Drug Co-Attention for Drug-Drug Interactions Prediction](https://arxiv.org/abs/2509.15256)
*Zimo Yan,Jie Zhang,Zheng Xie,Yiping Song,Hao Li*

Main category: cs.LG

TL;DR: 提出MPNP - DDI框架用于药物相互作用预测，实验显示优于现有方法，是药物相关领域有力工具。


<details>
  <summary>Details</summary>
Motivation: 现有药物相互作用预测方法难以捕捉不同尺度结构信息且缺乏预测置信度量化机制。

Method: 提出MPNP - DDI框架，核心是独特消息传递方案学习多尺度图表示，用跨药物共注意力机制融合多尺度表示生成上下文感知嵌入，集成神经过程模块进行不确定性估计。

Result: 在基准数据集上MPNP - DDI显著优于现有方法。

Conclusion: MPNP - DDI基于多尺度结构特征提供准确、可泛化且有不确定性感知的预测，是药物警戒、多药治疗风险评估和精准医学的有力计算工具。

Abstract: Accurate prediction of drug-drug interactions (DDI) is crucial for medication
safety and effective drug development. However, existing methods often struggle
to capture structural information across different scales, from local
functional groups to global molecular topology, and typically lack mechanisms
to quantify prediction confidence. To address these limitations, we propose
MPNP-DDI, a novel Multi-scale Graph Neural Process framework. The core of
MPNP-DDI is a unique message-passing scheme that, by being iteratively applied,
learns a hierarchy of graph representations at multiple scales. Crucially, a
cross-drug co-attention mechanism then dynamically fuses these multi-scale
representations to generate context-aware embeddings for interacting drug
pairs, while an integrated neural process module provides principled
uncertainty estimation. Extensive experiments demonstrate that MPNP-DDI
significantly outperforms state-of-the-art baselines on benchmark datasets. By
providing accurate, generalizable, and uncertainty-aware predictions built upon
multi-scale structural features, MPNP-DDI represents a powerful computational
tool for pharmacovigilance, polypharmacy risk assessment, and precision
medicine.

</details>


### [49] [Generative AI Meets Wireless Sensing: Towards Wireless Foundation Model](https://arxiv.org/abs/2509.15258)
*Zheng Yang,Guoxuan Chi,Chenshu Wu,Hanyu Liu,Yuchong Gao,Yunhao Liu,Jie Xu,Tony Xiao Han*

Main category: cs.LG

TL;DR: 本文探讨生成式人工智能（GenAI）与无线传感融合，研究集成方式、主流生成模型适用性，指出挑战并给出未来方向。


<details>
  <summary>Details</summary>
Motivation: GenAI在计算机视觉和自然语言处理领域取得进展，人们对将其集成到无线传感系统兴趣渐浓，期望改进无线传感应用。

Method: 从两个互补视角研究：一是探索GenAI集成到无线传感管道的方式；二是分析主流生成模型特征及在不同无线传感任务的适用性。

Result: 确定GenAI应用于无线传感的关键挑战。

Conclusion: 提出未来朝着无线基础模型发展的方向，实现跨多样传感任务的可扩展、适应性强且高效的信号理解。

Abstract: Generative Artificial Intelligence (GenAI) has made significant advancements
in fields such as computer vision (CV) and natural language processing (NLP),
demonstrating its capability to synthesize high-fidelity data and improve
generalization. Recently, there has been growing interest in integrating GenAI
into wireless sensing systems. By leveraging generative techniques such as data
augmentation, domain adaptation, and denoising, wireless sensing applications,
including device localization, human activity recognition, and environmental
monitoring, can be significantly improved. This survey investigates the
convergence of GenAI and wireless sensing from two complementary perspectives.
First, we explore how GenAI can be integrated into wireless sensing pipelines,
focusing on two modes of integration: as a plugin to augment task-specific
models and as a solver to directly address sensing tasks. Second, we analyze
the characteristics of mainstream generative models, such as Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), and diffusion
models, and discuss their applicability and unique advantages across various
wireless sensing tasks. We further identify key challenges in applying GenAI to
wireless sensing and outline a future direction toward a wireless foundation
model: a unified, pre-trained design capable of scalable, adaptable, and
efficient signal understanding across diverse sensing tasks.

</details>


### [50] [Inference Offloading for Cost-Sensitive Binary Classification at the Edge](https://arxiv.org/abs/2509.15674)
*Vishnu Narayanan Moothedath,Umang Agarwal,Umeshraja N,James Richard Gross,Jaya Prakash Champati,Sharayu Moharir*

Main category: cs.LG

TL;DR: 本文研究边缘智能系统中的二元分类问题，提出在线学习框架优化分层推理系统，H2T2策略表现出色。


<details>
  <summary>Details</summary>
Motivation: 理解分层推理系统中分类准确性与卸载成本之间的基本权衡。

Method: 提出在线学习框架，针对校准和未校准模型分别给出解决方案，引入H2T2策略。

Result: 模拟实验显示H2T2策略优于其他策略，有时超离线最优解，且对分布变化有鲁棒性。

Conclusion: H2T2策略能有效优化分层推理系统，在分类准确性和卸载成本间取得良好平衡。

Abstract: We focus on a binary classification problem in an edge intelligence system
where false negatives are more costly than false positives. The system has a
compact, locally deployed model, which is supplemented by a larger, remote
model, which is accessible via the network by incurring an offloading cost. For
each sample, our system first uses the locally deployed model for inference.
Based on the output of the local model, the sample may be offloaded to the
remote model. This work aims to understand the fundamental trade-off between
classification accuracy and these offloading costs within such a hierarchical
inference (HI) system. To optimize this system, we propose an online learning
framework that continuously adapts a pair of thresholds on the local model's
confidence scores. These thresholds determine the prediction of the local model
and whether a sample is classified locally or offloaded to the remote model. We
present a closed-form solution for the setting where the local model is
calibrated. For the more general case of uncalibrated models, we introduce
H2T2, an online two-threshold hierarchical inference policy, and prove it
achieves sublinear regret. H2T2 is model-agnostic, requires no training, and
learns in the inference phase using limited feedback. Simulations on real-world
datasets show that H2T2 consistently outperforms naive and single-threshold HI
policies, sometimes even surpassing offline optima. The policy also
demonstrates robustness to distribution shifts and adapts effectively to
mismatched classifiers.

</details>


### [51] [IEFS-GMB: Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders](https://arxiv.org/abs/2509.15259)
*Liang Zhang,Hanyang Dong,Jia-Hong Gao,Yi Sun,Kuntao Xiao,Wanli Yang,Zhao Lv,Shurong Sheng*

Main category: cs.LG

TL;DR: 提出基于信息熵和梯度记忆库的特征选择方法IEFS - GMB用于EEG分类，在四个公开数据集上实验显示能提升模型准确率、性能优于其他方法且增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有EEG特征选择方法少为诊断设计、依赖架构、缺乏可解释性且鲁棒性有限，影响基于深度学习的EEG分类模型性能。

Method: 构建存储历史梯度的动态记忆库，通过信息熵计算特征重要性，应用基于熵的加权来选择EEG特征。

Result: 在四个公开神经疾病数据集上，采用IEFS - GMB的编码器比基线模型准确率提升0.64% - 6.45%，优于四种竞争特征选择技术。

Conclusion: IEFS - GMB能提升模型性能和可解释性，适用于临床实际应用。

Abstract: Deep learning-based EEG classification is crucial for the automated detection
of neurological disorders, improving diagnostic accuracy and enabling early
intervention. However, the low signal-to-noise ratio of EEG signals limits
model performance, making feature selection (FS) vital for optimizing
representations learned by neural network encoders. Existing FS methods are
seldom designed specifically for EEG diagnosis; many are architecture-dependent
and lack interpretability, limiting their applicability. Moreover, most rely on
single-iteration data, resulting in limited robustness to variability. To
address these issues, we propose IEFS-GMB, an Information Entropy-based Feature
Selection method guided by a Gradient Memory Bank. This approach constructs a
dynamic memory bank storing historical gradients, computes feature importance
via information entropy, and applies entropy-based weighting to select
informative EEG features. Experiments on four public neurological disease
datasets show that encoders enhanced with IEFS-GMB achieve accuracy
improvements of 0.64% to 6.45% over baseline models. The method also
outperforms four competing FS techniques and improves model interpretability,
supporting its practical use in clinical settings.

</details>


### [52] [Automated Constitutive Model Discovery by Pairing Sparse Regression Algorithms with Model Selection Criteria](https://arxiv.org/abs/2509.16040)
*Jorge-Humberto Urrea-Quintero,David Anton,Laura De Lorenzis,Henning Wessels*

Main category: cs.LG

TL;DR: 提出全自动化本构模型发现框架，结合三种稀疏回归算法与三种模型选择准则，应用于各向同性和各向异性超弹性问题，结果显示九种组合表现良好。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的本构模型自动发现是传统模型校准范式的有前景替代方案，需探索不同算法和准则组合。

Method: 系统地将三种稀疏回归算法（LASSO、LARS、OMP）与三种模型选择准则（K - 折交叉验证、AIC、BIC）配对，形成九种算法用于模型发现，并应用于不同数据集。

Result: 九种算法 - 准则组合在各向同性和各向异性材料本构模型发现中表现一致良好，得到高精度模型。

Conclusion: 拓宽了可行的发现算法范围，不止局限于基于ℓ1的方法如LASSO。

Abstract: The automated discovery of constitutive models from data has recently emerged
as a promising alternative to the traditional model calibration paradigm. In
this work, we present a fully automated framework for constitutive model
discovery that systematically pairs three sparse regression algorithms (Least
Absolute Shrinkage and Selection Operator (LASSO), Least Angle Regression
(LARS), and Orthogonal Matching Pursuit (OMP)) with three model selection
criteria: $K$-fold cross-validation (CV), Akaike Information Criterion (AIC),
and Bayesian Information Criterion (BIC). This pairing yields nine distinct
algorithms for model discovery and enables a systematic exploration of the
trade-off between sparsity, predictive performance, and computational cost.
While LARS serves as an efficient path-based solver for the
$\ell_1$-constrained problem, OMP is introduced as a tractable heuristic for
$\ell_0$-regularized selection. The framework is applied to both isotropic and
anisotropic hyperelasticity, utilizing both synthetic and experimental
datasets. Results reveal that all nine algorithm-criterion combinations perform
consistently well for the discovery of isotropic and anisotropic materials,
yielding highly accurate constitutive models. These findings broaden the range
of viable discovery algorithms beyond $\ell_1$-based approaches such as LASSO.

</details>


### [53] [FedHK-MVFC: Federated Heat Kernel Multi-View Clustering](https://arxiv.org/abs/2509.15844)
*Kristina P. Sinaga*

Main category: cs.LG

TL;DR: 提出结合量子场论与联邦医疗分析的多视图聚类框架，开发两种算法，测试显示聚类精度提升、通信减少、效率保留，在实际数据验证有效，给出理论贡献，为医疗联邦学习提供新标准。


<details>
  <summary>Details</summary>
Motivation: 在分布式AI和隐私医疗应用领域，实现对敏感医疗数据的有效分析和安全协作。

Method: 利用谱分析的热核系数将欧氏距离转换为几何感知相似度，通过热核距离（HKD）变换，开发HK - MVFC和FedHK - MVFC两种算法，使用差分隐私和安全聚合。

Result: 在合成数据集上聚类精度提高8 - 12%，通信减少70%，效率保留98.2%，在10000条患者记录上验证有效。

Conclusion: 该框架将高级数学转化为可行方案，为医疗领域几何感知联邦学习提供新的标准，兼顾严谨性和临床相关性。

Abstract: In the realm of distributed AI and privacy-focused medical applications, we
propose a framework for multi-view clustering that links quantum field theory
with federated healthcare analytics. Our method uses heat-kernel coefficients
from spectral analysis to convert Euclidean distances into geometry-aware
similarity measures, capturing the structure of diverse medical data. We lay
this out through the Heat Kernel Distance (HKD) transformation with convergence
guarantees. Two algorithms are developed: Heat Kernel-Enhanced Multi-View Fuzzy
Clustering (HK-MVFC) for central analysis, and Federated Heat Kernel Multi-View
Fuzzy Clustering (FedHK-MVFC) for secure, privacy-preserving learning across
hospitals using differential privacy and secure aggregation to facilitate
HIPAA-compliant collaboration. Tests on synthetic datasets of cardiovascular
patients show an $8-12 \%$ increase in clustering accuracy, $70 \%$ reduced
communication, and $98.2 \%$ efficiency retention over centralized methods.
Validated on 10,000 patient records across two hospitals, it proves useful for
collaborative phenotyping involving ECG, cardiac imaging, and behavioral data.
Our theoretical contributions include update rules with proven convergence,
adaptive view weighting, and privacy-preserving protocols. This presents a new
standard for geometry-aware federated learning in healthcare, turning advanced
math into workable solutions for analyzing sensitive medical data while
ensuring both rigor and clinical relevance.

</details>


### [54] [A Weak Supervision Approach for Monitoring Recreational Drug Use Effects in Social Media](https://arxiv.org/abs/2509.15266)
*Lucía Prieto-Santamaría,Alba Cortés Iglesias,Claudio Vidal Giné,Fermín Fernández Calderón,Óscar M. Lozano,Alejandro Rodríguez-González*

Main category: cs.LG

TL;DR: 研究利用推特分析三种新兴精神活性物质用户报告的效果，训练模型预测推文极性，发现推特可检测特定物质表型效应，极性分类模型能高精度支持药物监测和效果表征。


<details>
  <summary>Details</summary>
Motivation: 传统监测系统常低估药物使用者体验，理解娱乐性药物使用的实际影响是公共卫生和生物医学研究的关键挑战。

Method: 利用推特数据，结合俚语列表和MetaMap进行生物医学概念提取，标注推文极性，进行描述性和比较分析，训练多种机器学习分类器并处理类别不平衡问题。

Result: 极端梯度提升结合成本敏感学习在测试集上表现最佳（F1 = 0.885, AUPRC = 0.934）。

Conclusion: 推特可检测特定物质表型效应，极性分类模型能高精度支持实时药物警戒和药物效果表征。

Abstract: Understanding the real-world effects of recreational drug use remains a
critical challenge in public health and biomedical research, especially as
traditional surveillance systems often underrepresent user experiences. In this
study, we leverage social media (specifically Twitter) as a rich and unfiltered
source of user-reported effects associated with three emerging psychoactive
substances: ecstasy, GHB, and 2C-B. By combining a curated list of slang terms
with biomedical concept extraction via MetaMap, we identified and weakly
annotated over 92,000 tweets mentioning these substances. Each tweet was
labeled with a polarity reflecting whether it reported a positive or negative
effect, following an expert-guided heuristic process. We then performed
descriptive and comparative analyses of the reported phenotypic outcomes across
substances and trained multiple machine learning classifiers to predict
polarity from tweet content, accounting for strong class imbalance using
techniques such as cost-sensitive learning and synthetic oversampling. The top
performance on the test set was obtained from eXtreme Gradient Boosting with
cost-sensitive learning (F1 = 0.885, AUPRC = 0.934). Our findings reveal that
Twitter enables the detection of substance-specific phenotypic effects, and
that polarity classification models can support real-time pharmacovigilance and
drug effect characterization with high accuracy.

</details>


### [55] [ToFU: Transforming How Federated Learning Systems Forget User Data](https://arxiv.org/abs/2509.15861)
*Van-Tuan Tran,Hong-Hanh Nguyen-Le,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: 提出ToFU框架应对联邦学习隐私风险，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 神经网络在联邦学习中会无意记忆训练数据，带来隐私风险，现有联邦遗忘方法事后处理效率低，需范式转变。

Method: 提出学习 - 遗忘的Transformation - guided Federated Unlearning (ToFU)框架，在学习过程中引入变换减少特定实例记忆，理论分析变换组合能限制特定实例信息。

Result: 在CIFAR - 10、CIFAR - 100和MUFAC基准上实验，ToFU优于现有联邦遗忘基线，与现有方法集成可提升性能，减少遗忘时间。

Conclusion: ToFU框架能有效解决联邦学习中的隐私风险问题，可作为即插即用框架提升现有联邦遗忘方法性能。

Abstract: Neural networks unintentionally memorize training data, creating privacy
risks in federated learning (FL) systems, such as inference and reconstruction
attacks on sensitive data. To mitigate these risks and to comply with privacy
regulations, Federated Unlearning (FU) has been introduced to enable
participants in FL systems to remove their data's influence from the global
model. However, current FU methods primarily act post-hoc, struggling to
efficiently erase information deeply memorized by neural networks. We argue
that effective unlearning necessitates a paradigm shift: designing FL systems
inherently amenable to forgetting. To this end, we propose a
learning-to-unlearn Transformation-guided Federated Unlearning (ToFU) framework
that incorporates transformations during the learning process to reduce
memorization of specific instances. Our theoretical analysis reveals how
transformation composition provably bounds instance-specific information,
directly simplifying subsequent unlearning. Crucially, ToFU can work as a
plug-and-play framework that improves the performance of existing FU methods.
Experiments on CIFAR-10, CIFAR-100, and the MUFAC benchmark show that ToFU
outperforms existing FU baselines, enhances performance when integrated with
current methods, and reduces unlearning time.

</details>


### [56] [Modeling Transformers as complex networks to analyze learning dynamics](https://arxiv.org/abs/2509.15269)
*Elisabetta Rocchetti*

Main category: cs.LG

TL;DR: 本文通过复杂网络理论研究大语言模型训练时学习动态，用新方法将模型表示为图，分析图指标揭示网络结构演变阶段，表明组件级网络视角有助于理解大模型功能电路形成原理。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在训练中获得复杂能力的过程，希望通过复杂网络理论来刻画其学习动态。

Method: 引入新方法将基于Transformer的大语言模型表示为有向加权图，通过干预消融技术测量因果影响，跟踪Pythia - 14M模型143个训练检查点的组件图演变并分析图论指标。

Result: 网络结构经历探索、巩固和细化阶段，出现信息传播组件的稳定层次结构和信息收集组件的动态集合，其角色在关键学习节点重新配置。

Conclusion: 组件级网络视角为可视化和理解驱动大语言模型功能电路形成的自组织原则提供了强大的宏观视角。

Abstract: The process by which Large Language Models (LLMs) acquire complex
capabilities during training remains a key open question in mechanistic
interpretability. This project investigates whether these learning dynamics can
be characterized through the lens of Complex Network Theory (CNT). I introduce
a novel methodology to represent a Transformer-based LLM as a directed,
weighted graph where nodes are the model's computational components (attention
heads and MLPs) and edges represent causal influence, measured via an
intervention-based ablation technique. By tracking the evolution of this
component-graph across 143 training checkpoints of the Pythia-14M model on a
canonical induction task, I analyze a suite of graph-theoretic metrics. The
results reveal that the network's structure evolves through distinct phases of
exploration, consolidation, and refinement. Specifically, I identify the
emergence of a stable hierarchy of information spreader components and a
dynamic set of information gatherer components, whose roles reconfigure at key
learning junctures. This work demonstrates that a component-level network
perspective offers a powerful macroscopic lens for visualizing and
understanding the self-organizing principles that drive the formation of
functional circuits in LLMs.

</details>


### [57] [RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation](https://arxiv.org/abs/2509.15965)
*Chao Yu,Yuanqing Wang,Zhen Guo,Hao Lin,Si Xu,Hongzhi Zang,Quanlu Zhang,Yongji Wu,Chunyang Zhu,Junhao Hu,Zixiao Huang,Mingjie Wei,Yuqing Xie,Ke Yang,Bo Dai,Zhexuan Xu,Xiangyuan Wang,Xu Fu,Zhihao Liu,Kang Chen,Weilin Liu,Gang Liu,Boxun Li,Jianlei Yang,Zhi Yang,Guohao Dai,Yu Wang*

Main category: cs.LG

TL;DR: 本文提出高性能强化学习训练系统RLinf，基于宏到微流转换范式，在推理和具身强化学习任务上评估显示其优于现有系统，加速训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有系统上强化学习工作流的异质性和动态性导致硬件利用率低、训练慢，高效强化学习训练的主要障碍在于系统灵活性。

Method: 构建基于宏到微流转换（M2Flow）的RLinf系统，通过上下文切换和弹性流水线实现M2Flow转换，采用基于性能分析的调度策略生成最优执行计划。

Result: 在推理和具身强化学习任务的广泛评估中，RLinf始终优于现有系统，端到端训练吞吐量实现1.1x - 2.13x的加速。

Conclusion: RLinf系统能有效提高强化学习训练效率，在相关任务上表现出色。

Abstract: Reinforcement learning (RL) has demonstrated immense potential in advancing
artificial general intelligence, agentic intelligence, and embodied
intelligence. However, the inherent heterogeneity and dynamicity of RL
workflows often lead to low hardware utilization and slow training on existing
systems. In this paper, we present RLinf, a high-performance RL training system
based on our key observation that the major roadblock to efficient RL training
lies in system flexibility. To maximize flexibility and efficiency, RLinf is
built atop a novel RL system design paradigm called macro-to-micro flow
transformation (M2Flow), which automatically breaks down high-level,
easy-to-compose RL workflows at both the temporal and spatial dimensions, and
recomposes them into optimized execution flows. Supported by RLinf worker's
adaptive communication capability, we devise context switching and elastic
pipelining to realize M2Flow transformation, and a profiling-guided scheduling
policy to generate optimal execution plans. Extensive evaluations on both
reasoning RL and embodied RL tasks demonstrate that RLinf consistently
outperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in
end-to-end training throughput.

</details>


### [58] [Partial Column Generation with Graph Neural Networks for Team Formation and Routing](https://arxiv.org/abs/2509.15275)
*Giacomo Dall'Olio,Rainer Kolisch,Yaoxin Wu*

Main category: cs.LG

TL;DR: 提出针对团队形成与路由问题的新部分列生成策略，用机器学习模型预测，实验显示该策略优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 团队形成与路由问题是有实际应用的挑战性优化问题，现有基于列生成的精确解法，需更好策略。

Method: 提出多定价问题下基于预测可能产生负约化成本列的新部分列生成策略，开发基于图神经网络的机器学习模型进行预测。

Result: 应用新策略能增强求解方法，在严格时间限制下解决难题时优于传统部分列生成方法。

Conclusion: 新部分列生成策略有效，能提升团队形成与路由问题的求解效果。

Abstract: The team formation and routing problem is a challenging optimization problem
with several real-world applications in fields such as airport, healthcare, and
maintenance operations. To solve this problem, exact solution methods based on
column generation have been proposed in the literature. In this paper, we
propose a novel partial column generation strategy for settings with multiple
pricing problems, based on predicting which ones are likely to yield columns
with a negative reduced cost. We develop a machine learning model tailored to
the team formation and routing problem that leverages graph neural networks for
these predictions. Computational experiments demonstrate that applying our
strategy enhances the solution method and outperforms traditional partial
column generation approaches from the literature, particularly on hard
instances solved under a tight time limit.

</details>


### [59] [Top-$k$ Feature Importance Ranking](https://arxiv.org/abs/2509.15420)
*Yuxi Chen,Tiffany Tang,Genevera Allen*

Main category: cs.LG

TL;DR: 介绍RAMPART框架用于重要特征排名，理论保证其以高概率实现正确排名，实验显示优于流行方法。


<details>
  <summary>Details</summary>
Motivation: 准确排名重要特征在可解释机器学习中是基础挑战，但该特定问题受关注少。

Method: 提出RAMPART框架，结合自适应顺序减半策略和高效集成技术，显式优化排名准确性。

Result: 理论证明在温和条件下RAMPART能以高概率实现正确的前k个特征排名，模拟研究显示其持续优于流行特征重要性方法。

Conclusion: RAMPART在特征排名上表现出色，通过高维基因组学案例研究进一步证实。

Abstract: Accurate ranking of important features is a fundamental challenge in
interpretable machine learning with critical applications in scientific
discovery and decision-making. Unlike feature selection and feature importance,
the specific problem of ranking important features has received considerably
less attention. We introduce RAMPART (Ranked Attributions with MiniPatches And
Recursive Trimming), a framework that utilizes any existing feature importance
measure in a novel algorithm specifically tailored for ranking the top-$k$
features. Our approach combines an adaptive sequential halving strategy that
progressively focuses computational resources on promising features with an
efficient ensembling technique using both observation and feature subsampling.
Unlike existing methods that convert importance scores to ranks as
post-processing, our framework explicitly optimizes for ranking accuracy. We
provide theoretical guarantees showing that RAMPART achieves the correct
top-$k$ ranking with high probability under mild conditions, and demonstrate
through extensive simulation studies that RAMPART consistently outperforms
popular feature importance methods, concluding with a high-dimensional genomics
case study.

</details>


### [60] [Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.15279)
*Chi Liu,Derek Li,Yan Shu,Robin Chen,Derek Duan,Teng Fang,Bryan Dai*

Main category: cs.LG

TL;DR: 介绍Fleming - R1模型以实现可验证医学推理，在医学基准测试中表现出色并公开模型以推动医学AI发展。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医学应用中实现专家级临床推理有挑战，需准确答案和透明推理过程。

Method: 采用Reasoning - Oriented Data Strategy（RODS）、Chain - of - Thought（CoT）冷启动、两阶段的Reinforcement Learning from Verifiable Rewards（RLVR）框架。

Result: Fleming - R1在多样医学基准测试中参数高效提升显著，7B变体超更大基线，32B模型接近GPT - 4o且超开源模型。

Conclusion: 结构化数据设计、面向推理的初始化和可验证强化学习可推动临床推理超越简单的准确性优化，公开模型利于医学AI安全部署。

Abstract: While large language models show promise in medical applications, achieving
expert-level clinical reasoning remains challenging due to the need for both
accurate answers and transparent reasoning processes. To address this
challenge, we introduce Fleming-R1, a model designed for verifiable medical
reasoning through three complementary innovations. First, our
Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets
with knowledge-graph-guided synthesis to improve coverage of underrepresented
diseases, drugs, and multi-hop reasoning chains. Second, we employ
Chain-of-Thought (CoT) cold start to distill high-quality reasoning
trajectories from teacher models, establishing robust inference priors. Third,
we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR)
framework using Group Relative Policy Optimization, which consolidates core
reasoning skills while targeting persistent failure modes through adaptive
hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers
substantial parameter-efficient improvements: the 7B variant surpasses much
larger baselines, while the 32B model achieves near-parity with GPT-4o and
consistently outperforms strong open-source alternatives. These results
demonstrate that structured data design, reasoning-oriented initialization, and
verifiable reinforcement learning can advance clinical reasoning beyond simple
accuracy optimization. We release Fleming-R1 publicly to promote transparent,
reproducible, and auditable progress in medical AI, enabling safer deployment
in high-stakes clinical environments.

</details>


### [61] [Personalized Federated Learning with Heat-Kernel Enhanced Tensorized Multi-View Clustering](https://arxiv.org/abs/2509.16101)
*Kristina P. Sinaga*

Main category: cs.LG

TL;DR: 提出一个鲁棒的个性化联邦学习框架，结合热核增强张量多视图模糊c均值聚类与高级张量分解技术，实现高效处理高维多视图数据并节省通信成本。


<details>
  <summary>Details</summary>
Motivation: 解决高维多视图数据在联邦学习中的处理难题，实现高效数据表示与隐私保护。

Method: 将量子场论的热核系数与Tucker分解和CANDECOMP/PARAFAC结合，采用矩阵化和向量化技术，引入双级优化方案，包括局部热核增强模糊聚类与张量分解、带隐私保护的联邦聚合。

Result: 能有效处理高维多视图数据，通过低秩张量近似显著节省通信成本。

Conclusion: 所提出的张量化方法在高维多视图数据的联邦学习中具有良好效果和应用价值。

Abstract: We present a robust personalized federated learning framework that leverages
heat-kernel enhanced tensorized multi-view fuzzy c-means clustering with
advanced tensor decomposition techniques. Our approach integrates heat-kernel
coefficients adapted from quantum field theory with Tucker decomposition and
canonical polyadic decomposition (CANDECOMP/PARAFAC) to transform conventional
distance metrics and efficiently represent high-dimensional multi-view
structures. The framework employs matriculation and vectorization techniques to
facilitate the discovery of hidden structures and multilinear relationships via
N-way generalized tensors. The proposed method introduces a dual-level
optimization scheme: local heat-kernel enhanced fuzzy clustering with tensor
decomposition operating on order-N input tensors, and federated aggregation of
tensor factors with privacy-preserving personalization mechanisms. The local
stage employs tensorized kernel Euclidean distance transformations and Tucker
decomposition to discover client-specific patterns in multi-view tensor data,
while the global aggregation process coordinates tensor factors (core tensors
and factor matrices) across clients through differential privacy-preserving
protocols. This tensorized approach enables efficient handling of
high-dimensional multi-view data with significant communication savings through
low-rank tensor approximations.

</details>


### [62] [Hybrid unary-binary design for multiplier-less printed Machine Learning classifiers](https://arxiv.org/abs/2509.15316)
*Giorgos Armeniakos,Theodoros Mantzakidis,Dimitrios Soudris*

Main category: cs.LG

TL;DR: 本文探索替代算术，提出混合一元 - 二进制架构及架构感知训练方法，在六个数据集上评估显示面积和功耗显著降低且精度损失小。


<details>
  <summary>Details</summary>
Motivation: 印刷电子（PE）实现机器学习电路有优势但特征尺寸限制分类器复杂度，需利用其低成本优势简化电路设计。

Method: 探索替代算术，提出混合一元 - 二进制架构，引入架构感知训练。

Result: 在六个数据集上平均面积减少 46%，功耗降低 39%，精度损失极小，超越其他先进 MLP 设计。

Conclusion: 所提方法能有效提高 MLP 分类器的面积和功率效率。

Abstract: Printed Electronics (PE) provide a flexible, cost-efficient alternative to
silicon for implementing machine learning (ML) circuits, but their large
feature sizes limit classifier complexity. Leveraging PE's low fabrication and
NRE costs, designers can tailor hardware to specific ML models, simplifying
circuit design. This work explores alternative arithmetic and proposes a hybrid
unary-binary architecture that removes costly encoders and enables efficient,
multiplier-less execution of MLP classifiers. We also introduce
architecture-aware training to further improve area and power efficiency.
Evaluation on six datasets shows average reductions of 46% in area and 39% in
power, with minimal accuracy loss, surpassing other state-of-the-art MLP
designs.

</details>


### [63] [Kuramoto Orientation Diffusion Models](https://arxiv.org/abs/2509.15328)
*Yue Song,T. Anderson Keller,Sevan Brodjian,Takeru Miyato,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: 提出基于周期域的分数生成模型，利用随机Kuramoto动力学，在图像生成任务取得良好效果，证明生物启发同步动力学作为生成建模先验的潜力。


<details>
  <summary>Details</summary>
Motivation: 标准基于各向同性欧几里得扩散的生成方法难以对具有连贯角方向模式的图像建模，受生物系统中相位同步作用的启发开展研究。

Method: 构建基于周期域的分数生成模型，正向过程通过耦合振荡器相互作用和吸引到全局参考相位使相位变量同步，逆向过程进行去同步；实现包裹高斯过渡核和周期性感知网络。

Result: 在通用图像基准上取得有竞争力的结果，在指纹和纹理等方向密集数据集上显著提高生成质量。

Conclusion: 生物启发的同步动力学作为结构化先验在生成建模中具有潜力。

Abstract: Orientation-rich images, such as fingerprints and textures, often exhibit
coherent angular directional patterns that are challenging to model using
standard generative approaches based on isotropic Euclidean diffusion.
Motivated by the role of phase synchronization in biological systems, we
propose a score-based generative model built on periodic domains by leveraging
stochastic Kuramoto dynamics in the diffusion process. In neural and physical
systems, Kuramoto models capture synchronization phenomena across coupled
oscillators -- a behavior that we re-purpose here as an inductive bias for
structured image generation. In our framework, the forward process performs
\textit{synchronization} among phase variables through globally or locally
coupled oscillator interactions and attraction to a global reference phase,
gradually collapsing the data into a low-entropy von Mises distribution. The
reverse process then performs \textit{desynchronization}, generating diverse
patterns by reversing the dynamics with a learned score function. This approach
enables structured destruction during forward diffusion and a hierarchical
generation process that progressively refines global coherence into fine-scale
details. We implement wrapped Gaussian transition kernels and periodicity-aware
networks to account for the circular geometry. Our method achieves competitive
results on general image benchmarks and significantly improves generation
quality on orientation-dense datasets like fingerprints and textures.
Ultimately, this work demonstrates the promise of biologically inspired
synchronization dynamics as structured priors in generative modeling.

</details>


### [64] [Global Pre-fixing, Local Adjusting: A Simple yet Effective Contrastive Strategy for Continual Learning](https://arxiv.org/abs/2509.15347)
*Jia Tang,Xinrui Wang,Songcan Chen*

Main category: cs.LG

TL;DR: 本文提出GPLASC对比策略解决持续学习中特征混淆问题，能同时保证任务间和任务内特征结构的区分性，可集成到现有框架，实验验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有利用对比损失进行持续学习的方法因任务间和任务内特征混淆，性能受限。

Method: 提出GPLASC策略，将表示的单位超球划分为非重叠区域形成任务间预固定的ETF，为单个任务在分配区域内形成任务内可调整的ETF。

Result: 方法能同时保证任务间和任务内特征结构的区分性，可无缝集成到现有对比持续学习框架。

Conclusion: 通过大量实验验证了所提方法的有效性。

Abstract: Continual learning (CL) involves acquiring and accumulating knowledge from
evolving tasks while alleviating catastrophic forgetting. Recently, leveraging
contrastive loss to construct more transferable and less forgetful
representations has been a promising direction in CL. Despite advancements,
their performance is still limited due to confusion arising from both
inter-task and intra-task features. To address the problem, we propose a simple
yet effective contrastive strategy named \textbf{G}lobal \textbf{P}re-fixing,
\textbf{L}ocal \textbf{A}djusting for \textbf{S}upervised \textbf{C}ontrastive
learning (GPLASC). Specifically, to avoid task-level confusion, we divide the
entire unit hypersphere of representations into non-overlapping regions, with
the centers of the regions forming an inter-task pre-fixed \textbf{E}quiangular
\textbf{T}ight \textbf{F}rame (ETF). Meanwhile, for individual tasks, our
method helps regulate the feature structure and form intra-task adjustable ETFs
within their respective allocated regions. As a result, our method
\textit{simultaneously} ensures discriminative feature structures both between
tasks and within tasks and can be seamlessly integrated into any existing
contrastive continual learning framework. Extensive experiments validate its
effectiveness.

</details>


### [65] [ThermalGuardian: Temperature-Aware Testing of Automotive Deep Learning Frameworks](https://arxiv.org/abs/2509.15815)
*Yinglong Zou,Juan Zhai,Chunrong Fang,Zhenyu Chen*

Main category: cs.LG

TL;DR: 提出ThermalGuardian方法，用于测试温度变化环境下的汽车深度学习框架。


<details>
  <summary>Details</summary>
Motivation: 汽车环境温度变化大影响GPU频率，现有汽车深度学习框架未考虑此影响，产生质量问题且现有测试方法无法检测。

Method: 使用针对温度敏感算子的模型变异规则生成测试输入模型，基于牛顿冷却定律模拟GPU温度波动，根据实时GPU温度控制GPU频率。

Result: 文档未提及具体结果。

Conclusion: 提出了首个温度变化环境下的汽车深度学习框架测试方法ThermalGuardian。

Abstract: Deep learning models play a vital role in autonomous driving systems,
supporting critical functions such as environmental perception. To accelerate
model inference, these deep learning models' deployment relies on automotive
deep learning frameworks, for example, PaddleInference in Apollo and TensorRT
in AutoWare. However, unlike deploying deep learning models on the cloud,
vehicular environments experience extreme ambient temperatures varying from
-40{\deg}C to 50{\deg}C, significantly impacting GPU temperature. Additionally,
heats generated when computing further lead to the GPU temperature increase.
These temperature fluctuations lead to dynamic GPU frequency adjustments
through mechanisms such as DVFS. However, automotive deep learning frameworks
are designed without considering the impact of temperature-induced frequency
variations. When deployed on temperature-varying GPUs, these frameworks suffer
critical quality issues: compute-intensive operators face delays or errors,
high/mixed-precision operators suffer from precision errors, and time-series
operators suffer from synchronization issues. The above quality issues cannot
be detected by existing deep learning framework testing methods because they
ignore temperature's effect on the deep learning framework quality. To bridge
this gap, we propose ThermalGuardian, the first automotive deep learning
framework testing method under temperature-varying environments. Specifically,
ThermalGuardian generates test input models using model mutation rules
targeting temperature-sensitive operators, simulates GPU temperature
fluctuations based on Newton's law of cooling, and controls GPU frequency based
on real-time GPU temperature.

</details>


### [66] [Probabilistic Conformal Coverage Guarantees in Small-Data Settings](https://arxiv.org/abs/2509.15349)
*Petrus H. Zwart*

Main category: cs.LG

TL;DR: 提出Small Sample Beta Correction (SSBC) 调整共形显著性水平，为共形预测提供概率保证。


<details>
  <summary>Details</summary>
Motivation: 传统分割共形预测的覆盖保证仅在期望上是训练条件的，单次校准集的实际覆盖可能有很大差异，影响实际应用中的风险控制。

Method: 引入Small Sample Beta Correction (SSBC) 对共形显著性水平进行即插即用的调整。

Result: 利用共形覆盖的精确有限样本分布提供概率保证。

Conclusion: 使用户能以自定义概率确保部署的预测器达到至少所需的覆盖。

Abstract: Conformal prediction provides distribution-free prediction sets with
guaranteed marginal coverage. However, in split conformal prediction this
guarantee is training-conditional only in expectation: across many calibration
draws, the average coverage equals the nominal level, but the realized coverage
for a single calibration set may vary substantially. This variance undermines
effective risk control in practical applications. Here we introduce the Small
Sample Beta Correction (SSBC), a plug-and-play adjustment to the conformal
significance level that leverages the exact finite-sample distribution of
conformal coverage to provide probabilistic guarantees, ensuring that with
user-defined probability over the calibration draw, the deployed predictor
achieves at least the desired coverage.

</details>


### [67] [Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification](https://arxiv.org/abs/2509.15591)
*Zinan Lin,Enshu Liu,Xuefei Ning,Junyi Zhu,Wenyu Wang,Sergey Yekhanin*

Main category: cs.LG

TL;DR: 本文提出潜在分区网络（LZN），旨在统一机器学习中生成建模、表示学习和分类三个核心问题，通过创建共享高斯潜在空间实现多任务处理，并在多种场景验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习中生成建模、表示学习和分类的最优解决方案相互分离，希望找到统一原则简化机器学习流程并促进任务间协同。

Method: 引入LZN，创建共享高斯潜在空间，为每种数据类型配备编码器和解码器，将机器学习任务表示为编码器和解码器的组合。

Result: 在三种场景中展现优势：结合现有模型提升图像生成效果；独立完成表示学习任务且性能优于经典方法；同时处理生成和分类任务达到最优分类准确率。

Conclusion: LZN是朝着统一解决机器学习三个核心问题迈出的有效一步。

Abstract: Generative modeling, representation learning, and classification are three
core problems in machine learning (ML), yet their state-of-the-art (SoTA)
solutions remain largely disjoint. In this paper, we ask: Can a unified
principle address all three? Such unification could simplify ML pipelines and
foster greater synergy across tasks. We introduce Latent Zoning Network (LZN)
as a step toward this goal. At its core, LZN creates a shared Gaussian latent
space that encodes information across all tasks. Each data type (e.g., images,
text, labels) is equipped with an encoder that maps samples to disjoint latent
zones, and a decoder that maps latents back to data. ML tasks are expressed as
compositions of these encoders and decoders: for example, label-conditional
image generation uses a label encoder and image decoder; image embedding uses
an image encoder; classification uses an image encoder and label decoder. We
demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN
can enhance existing models (image generation): When combined with the SoTA
Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without
modifying the training objective. (2) LZN can solve tasks independently
(representation learning): LZN can implement unsupervised representation
learning without auxiliary loss functions, outperforming the seminal MoCo and
SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear
classification on ImageNet. (3) LZN can solve multiple tasks simultaneously
(joint generation and classification): With image and label encoders/decoders,
LZN performs both tasks jointly by design, improving FID and achieving SoTA
classification accuracy on CIFAR10. The code and trained models are available
at https://github.com/microsoft/latent-zoning-networks. The project website is
at https://zinanlin.me/blogs/latent_zoning_networks.html.

</details>


### [68] [Predicting Language Models' Success at Zero-Shot Probabilistic Prediction](https://arxiv.org/abs/2509.15356)
*Kevin Ren,Santiago Cortes-Gomez,Carlos Miguel Patiño,Ananya Joshi,Ruiqi Lyu,Jingjing Tang,Alistair Turcan,Khurram Yamin,Steven Wu,Bryan Wilder*

Main category: cs.LG

TL;DR: 对大语言模型（LLMs）零样本预测能力进行大规模实证研究，发现其性能多变，构建指标预测其任务级表现，部分指标无需标注数据就能给出强信号。


<details>
  <summary>Details</summary>
Motivation: 解决用户何时能相信大语言模型能为特定任务提供高质量预测的问题。

Method: 对广泛的表格预测任务进行大规模实证研究，构建指标预测LLMs任务级表现。

Result: LLMs性能高度可变；在基础预测任务表现好时，预测概率是个体级准确性的更强信号；部分无需标注数据的指标能给出LLMs在新任务上预测性能的强信号。

Conclusion: 构建的部分指标可有效区分LLMs适合和不适合的任务，为判断其预测性能提供依据。

Abstract: Recent work has investigated the capabilities of large language models (LLMs)
as zero-shot models for generating individual-level characteristics (e.g., to
serve as risk models or augment survey datasets). However, when should a user
have confidence that an LLM will provide high-quality predictions for their
particular task? To address this question, we conduct a large-scale empirical
study of LLMs' zero-shot predictive capabilities across a wide range of tabular
prediction tasks. We find that LLMs' performance is highly variable, both on
tasks within the same dataset and across different datasets. However, when the
LLM performs well on the base prediction task, its predicted probabilities
become a stronger signal for individual-level accuracy. Then, we construct
metrics to predict LLMs' performance at the task level, aiming to distinguish
between tasks where LLMs may perform well and where they are likely unsuitable.
We find that some of these metrics, each of which are assessed without labeled
data, yield strong signals of LLMs' predictive performance on new tasks.

</details>


### [69] [Stochastic Sample Approximations of (Local) Moduli of Continuity](https://arxiv.org/abs/2509.15368)
*Rodion Nazarov,Allen Gehret,Robert Shorten,Jakub Marecek*

Main category: cs.LG

TL;DR: 本文利用局部连续性模评估神经网络鲁棒性和闭环模型中重复使用的公平性，提出非均匀随机样本近似方法。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络的鲁棒性和其在闭环模型中重复使用的公平性。

Method: 重新探讨广义导数与局部连续性模的联系，并提出局部连续性模的非均匀随机样本近似方法。

Result: 提出了局部连续性模的非均匀随机样本近似方法。

Conclusion: 该方法对研究神经网络鲁棒性和重复使用公平性具有重要意义。

Abstract: Modulus of local continuity is used to evaluate the robustness of neural
networks and fairness of their repeated uses in closed-loop models. Here, we
revisit a connection between generalized derivatives and moduli of local
continuity, and present a non-uniform stochastic sample approximation for
moduli of local continuity. This is of importance in studying robustness of
neural networks and fairness of their repeated uses.

</details>


### [70] [Information Geometry of Variational Bayes](https://arxiv.org/abs/2509.15641)
*Mohammad Emtiyaz Khan*

Main category: cs.LG

TL;DR: 揭示信息几何与变分贝叶斯的联系及后果，强调两领域共同起源以促进交叉研究。


<details>
  <summary>Details</summary>
Motivation: 探讨信息几何与变分贝叶斯的联系及其对机器学习的影响，促进两领域交叉工作。

Method: 利用Khan和Rue（2023）的自然梯度下降算法（BLR）。

Result: 得出贝叶斯规则简化、二次替代物泛化、大语言模型变分贝叶斯算法大规模实现等结果。

Conclusion: 信息几何与变分贝叶斯有联系，强调两领域共同起源有助于交叉研究。

Abstract: We highlight a fundamental connection between information geometry and
variational Bayes (VB) and discuss its consequences for machine learning. Under
certain conditions, a VB solution always requires estimation or computation of
natural gradients. We show several consequences of this fact by using the
natural-gradient descent algorithm of Khan and Rue (2023) called the Bayesian
Learning Rule (BLR). These include (i) a simplification of Bayes' rule as
addition of natural gradients, (ii) a generalization of quadratic surrogates
used in gradient-based methods, and (iii) a large-scale implementation of VB
algorithms for large language models. Neither the connection nor its
consequences are new but we further emphasize the common origins of the two
fields of information geometry and Bayes with a hope to facilitate more work at
the intersection of the two fields.

</details>


### [71] [Adversarial generalization of unfolding (model-based) networks](https://arxiv.org/abs/2509.15370)
*Vicky Kouni*

Main category: cs.LG

TL;DR: 本文研究展开网络在$l_2$范数约束攻击下的对抗泛化性，给出对抗泛化误差界并通过实验验证理论，发现过参数化可提升对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 压缩感知在关键领域应用广泛，对抗鲁棒性至关重要，但展开网络在对抗攻击下的理论理解尚不成熟，因此研究其对抗泛化性。

Method: 选择最先进的过参数化展开网络家族，采用新框架估计其对抗拉德马赫复杂度，给出对抗泛化误差界。

Result: 实验结果与推导的理论一致，验证了理论的正确性。

Conclusion: 这是首次对展开网络对抗泛化性的理论分析，过参数化可用于提升神经网络的对抗鲁棒性。

Abstract: Unfolding networks are interpretable networks emerging from iterative
algorithms, incorporate prior knowledge of data structure, and are designed to
solve inverse problems like compressed sensing, which deals with recovering
data from noisy, missing observations. Compressed sensing finds applications in
critical domains, from medical imaging to cryptography, where adversarial
robustness is crucial to prevent catastrophic failures. However, a solid
theoretical understanding of the performance of unfolding networks in the
presence of adversarial attacks is still in its infancy. In this paper, we
study the adversarial generalization of unfolding networks when perturbed with
$l_2$-norm constrained attacks, generated by the fast gradient sign method.
Particularly, we choose a family of state-of-the-art overaparameterized
unfolding networks and deploy a new framework to estimate their adversarial
Rademacher complexity. Given this estimate, we provide adversarial
generalization error bounds for the networks under study, which are tight with
respect to the attack level. To our knowledge, this is the first theoretical
analysis on the adversarial generalization of unfolding networks. We further
present a series of experiments on real-world data, with results corroborating
our derived theory, consistently for all data. Finally, we observe that the
family's overparameterization can be exploited to promote adversarial
robustness, shedding light on how to efficiently robustify neural networks.

</details>


### [72] [Generalization and Optimization of SGD with Lookahead](https://arxiv.org/abs/2509.15776)
*Kangcheng Li,Yunwen Lei*

Main category: cs.LG

TL;DR: 本文对Lookahead优化器进行稳定性和泛化分析，推导无Lipschitz假设下的泛化界，在凸设置下有线性加速。


<details>
  <summary>Details</summary>
Motivation: 多数理论研究关注Lookahead优化器在训练数据上的收敛性，其泛化能力研究不足，现有泛化分析有假设限制且未充分捕捉优化与泛化关系。

Method: 对使用小批量SGD的Lookahead优化器进行严格的稳定性和泛化分析，利用平均模型稳定性推导泛化界。

Result: 在无Lipschitz假设下，为凸和强凸问题推导了泛化界，在凸设置下有关于批量大小的线性加速。

Conclusion: 通过新的分析方法，能更好地理解Lookahead优化器的泛化能力。

Abstract: The Lookahead optimizer enhances deep learning models by employing a
dual-weight update mechanism, which has been shown to improve the performance
of underlying optimizers such as SGD. However, most theoretical studies focus
on its convergence on training data, leaving its generalization capabilities
less understood. Existing generalization analyses are often limited by
restrictive assumptions, such as requiring the loss function to be globally
Lipschitz continuous, and their bounds do not fully capture the relationship
between optimization and generalization. In this paper, we address these issues
by conducting a rigorous stability and generalization analysis of the Lookahead
optimizer with minibatch SGD. We leverage on-average model stability to derive
generalization bounds for both convex and strongly convex problems without the
restrictive Lipschitzness assumption. Our analysis demonstrates a linear
speedup with respect to the batch size in the convex setting.

</details>


### [73] [Learning in Stackelberg Mean Field Games: A Non-Asymptotic Analysis](https://arxiv.org/abs/2509.15392)
*Sihan Zeng,Benjamin Patrick Evans,Sujay Bhatt,Leo Ardon,Sumitra Ganesh,Alec Koppel*

Main category: cs.LG

TL;DR: 研究Stackelberg平均场博弈中的策略优化，提出AC - SMFG算法，有收敛保证且表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有解决Stackelberg平均场博弈策略优化问题的方法存在依赖独立性假设、样本利用效率低和缺乏有限时间收敛保证等局限。

Method: 提出AC - SMFG单循环演员 - 评论家算法，在连续生成的马尔可夫样本上操作，交替更新领导者、代表追随者和平均场，关键假设为“梯度对齐”条件。

Result: 算法能在有限时间和样本下收敛到Stackelberg目标的驻点，在多个经济学环境模拟中，AC - SMFG在策略质量和收敛速度上优于现有基线。

Conclusion: AC - SMFG是首个有非渐近收敛保证的Stackelberg平均场博弈算法，放松了现有领导者 - 追随者独立性假设。

Abstract: We study policy optimization in Stackelberg mean field games (MFGs), a
hierarchical framework for modeling the strategic interaction between a single
leader and an infinitely large population of homogeneous followers. The
objective can be formulated as a structured bi-level optimization problem, in
which the leader needs to learn a policy maximizing its reward, anticipating
the response of the followers. Existing methods for solving these (and related)
problems often rely on restrictive independence assumptions between the
leader's and followers' objectives, use samples inefficiently due to
nested-loop algorithm structure, and lack finite-time convergence guarantees.
To address these limitations, we propose AC-SMFG, a single-loop actor-critic
algorithm that operates on continuously generated Markovian samples. The
algorithm alternates between (semi-)gradient updates for the leader, a
representative follower, and the mean field, and is simple to implement in
practice. We establish the finite-time and finite-sample convergence of the
algorithm to a stationary point of the Stackelberg objective. To our knowledge,
this is the first Stackelberg MFG algorithm with non-asymptotic convergence
guarantees. Our key assumption is a "gradient alignment" condition, which
requires that the full policy gradient of the leader can be approximated by a
partial component of it, relaxing the existing leader-follower independence
assumption. Simulation results in a range of well-established economics
environments demonstrate that AC-SMFG outperforms existing multi-agent and MFG
learning baselines in policy quality and convergence speed.

</details>


### [74] [VMDNet: Time Series Forecasting with Leakage-Free Samplewise Variational Mode Decomposition and Multibranch Decoding](https://arxiv.org/abs/2509.15394)
*Weibin Feng,Ran Tao,John Cartlidge,Jin Zheng*

Main category: cs.LG

TL;DR: 提出VMDNet框架解决现有时间序列预测中VMD方法存在的信息泄漏和超参数调优问题，实验表明其在强周期性时达SOTA，弱周期性也稳健。


<details>
  <summary>Details</summary>
Motivation: 现有使用VMD进行时间序列预测的研究存在信息泄漏和不恰当的超参数调优问题。

Method: 提出VMDNet框架，包括样本级VMD避免泄漏、用频率感知嵌入表示分解模式并通过并行TCN解码、引入受Stackelberg启发的双层优化自适应选择VMD核心超参数。

Result: 在两个能源相关数据集实验显示，强周期性时VMDNet达SOTA，能捕捉结构化周期模式，弱周期性时也稳健。

Conclusion: VMDNet是一个有效的时间序列预测框架，能解决现有方法问题，在不同周期性场景表现良好。

Abstract: In time series forecasting, capturing recurrent temporal patterns is
essential; decomposition techniques make such structure explicit and thereby
improve predictive performance. Variational Mode Decomposition (VMD) is a
powerful signal-processing method for periodicity-aware decomposition and has
seen growing adoption in recent years. However, existing studies often suffer
from information leakage and rely on inappropriate hyperparameter tuning. To
address these issues, we propose VMDNet, a causality-preserving framework that
(i) applies sample-wise VMD to avoid leakage; (ii) represents each decomposed
mode with frequency-aware embeddings and decodes it using parallel temporal
convolutional networks (TCNs), ensuring mode independence and efficient
learning; and (iii) introduces a bilevel, Stackelberg-inspired optimisation to
adaptively select VMD's two core hyperparameters: the number of modes (K) and
the bandwidth penalty (alpha). Experiments on two energy-related datasets
demonstrate that VMDNet achieves state-of-the-art results when periodicity is
strong, showing clear advantages in capturing structured periodic patterns
while remaining robust under weak periodicity.

</details>


### [75] [The Alignment Bottleneck](https://arxiv.org/abs/2509.15932)
*Wenjun Cao*

Main category: cs.LG

TL;DR: 大语言模型虽随规模提升，但反馈对齐仍有偏差。基于有限理性建模，得出容量耦合对齐性能区间，分析了对齐限制与要求，将对齐视为接口工程。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型反馈对齐中存在的与预期行为的系统偏差问题。

Method: 基于经济学和认知科学的有限理性，将判断视为资源受限、反馈为受限通道，构建两阶段级联模型。

Result: 得到容量耦合对齐性能区间，包括数据大小无关的Fano下界和PAC - Bayes上界，且在匹配条件下由单一容量控制。

Conclusion: 对齐是接口工程，需测量和分配有限容量、管理任务复杂度、决定信息使用。

Abstract: Large language models improve with scale, yet feedback-based alignment still
exhibits systematic deviations from intended behavior. Motivated by bounded
rationality in economics and cognitive science, we view judgment as
resource-limited and feedback as a constrained channel. On this basis, we model
the loop as a two-stage cascade $U \to H \to Y$ given $S$, with cognitive
capacity $C_{\text{cog}|S}$ and average total capacity
$\bar{C}_{\text{tot}|S}$. Our main result is a capacity-coupled Alignment
Performance Interval. It pairs a data size-independent Fano lower bound proved
on a separable codebook mixture with a PAC-Bayes upper bound whose KL term is
controlled by the same channel via $m \, \bar{C}_{\text{tot}|S}$. The PAC-Bayes
bound becomes an upper bound on the same true risk when the canonical
observable loss is used and the dataset is drawn from the same mixture. Under
these matched conditions, both limits are governed by a single capacity.
Consequences include that, with value complexity and capacity fixed, adding
labels alone cannot cross the bound; attaining lower risk on more complex
targets requires capacity that grows with $\log M$; and once useful signal
saturates capacity, further optimization tends to fit channel regularities,
consistent with reports of sycophancy and reward hacking. The analysis views
alignment as interface engineering: measure and allocate limited capacity,
manage task complexity, and decide where information is spent.

</details>


### [76] [Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization](https://arxiv.org/abs/2509.15399)
*Xiaochuan Gong,Jie Hao,Mingrui Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Hierarchical optimization refers to problems with interdependent decision
variables and objectives, such as minimax and bilevel formulations. While
various algorithms have been proposed, existing methods and analyses lack
adaptivity in stochastic optimization settings: they cannot achieve optimal
convergence rates across a wide spectrum of gradient noise levels without prior
knowledge of the noise magnitude. In this paper, we propose novel adaptive
algorithms for two important classes of stochastic hierarchical optimization
problems: nonconvex-strongly-concave minimax optimization and
nonconvex-strongly-convex bilevel optimization. Our algorithms achieve sharp
convergence rates of $\widetilde{O}(1/\sqrt{T} + \sqrt{\bar{\sigma}}/T^{1/4})$
in $T$ iterations for the gradient norm, where $\bar{\sigma}$ is an upper bound
on the stochastic gradient noise. Notably, these rates are obtained without
prior knowledge of the noise level, thereby enabling automatic adaptivity in
both low and high-noise regimes. To our knowledge, this work provides the first
adaptive and sharp convergence guarantees for stochastic hierarchical
optimization. Our algorithm design combines the momentum normalization
technique with novel adaptive parameter choices. Extensive experiments on
synthetic and deep learning tasks demonstrate the effectiveness of our proposed
algorithms.

</details>


### [77] [Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations](https://arxiv.org/abs/2509.15981)
*Yujie Zhu,Charles A. Hepburn,Matthew Thorpe,Giovanni Montana*

Main category: cs.LG

TL;DR: 提出SPReD框架解决强化学习中何时模仿演示的问题，用集成方法建模Q值分布，开发两种不确定性感知方法，减少训练梯度方差，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 在稀疏奖励的强化学习中，确定何时模仿演示具有挑战性，需要解决何时模仿演示与遵循自身策略的问题。

Method: 提出SPReD框架，使用集成方法对演示和策略动作的Q值分布建模，开发概率和基于优势的两种不确定性感知方法，应用连续、与不确定性成比例的正则化权重。

Result: 在八个机器人任务实验中取得显著收益，在复杂任务中比现有方法表现高至14倍，对演示质量和数量有鲁棒性。

Conclusion: SPReD框架能有效解决何时模仿演示的问题，在实验中表现出色且具有鲁棒性，代码开源。

Abstract: In reinforcement learning with sparse rewards, demonstrations can accelerate
learning, but determining when to imitate them remains challenging. We propose
Smooth Policy Regularisation from Demonstrations (SPReD), a framework that
addresses the fundamental question: when should an agent imitate a
demonstration versus follow its own policy? SPReD uses ensemble methods to
explicitly model Q-value distributions for both demonstration and policy
actions, quantifying uncertainty for comparisons. We develop two complementary
uncertainty-aware methods: a probabilistic approach estimating the likelihood
of demonstration superiority, and an advantage-based approach scaling imitation
by statistical significance. Unlike prevailing methods (e.g. Q-filter) that
make binary imitation decisions, SPReD applies continuous,
uncertainty-proportional regularisation weights, reducing gradient variance
during training. Despite its computational simplicity, SPReD achieves
remarkable gains in experiments across eight robotics tasks, outperforming
existing approaches by up to a factor of 14 in complex tasks while maintaining
robustness to demonstration quality and quantity. Our code is available at
https://github.com/YujieZhu7/SPReD.

</details>


### [78] [Exploring multimodal implicit behavior learning for vehicle navigation in simulated cities](https://arxiv.org/abs/2509.15400)
*Eric Aislan Antonelo,Gustavo Claudio Karl Couto,Christian Möller*

Main category: cs.LG

TL;DR: 标准行为克隆（BC）无法学习多模态驾驶决策，本文提出数据增强隐式行为克隆（DA - IBC），在CARLA模拟器实验中DA - IBC表现优于标准IBC。


<details>
  <summary>Details</summary>
Motivation: 标准行为克隆（BC）无法学习多模态驾驶决策，需要更好的方法来捕捉多模态。

Method: 提出Data - Augmented IBC（DA - IBC），通过扰动专家行动形成反例进行训练，并采用更好的初始化进行无导数推理。

Result: 在CARLA模拟器中，DA - IBC在评估多模态行为学习的城市驾驶任务中优于标准IBC，学习到的能量景观能表示多模态动作分布。

Conclusion: DA - IBC能有效解决标准行为克隆无法学习多模态驾驶决策的问题，可表示多模态动作分布。

Abstract: Standard Behavior Cloning (BC) fails to learn multimodal driving decisions,
where multiple valid actions exist for the same scenario. We explore Implicit
Behavioral Cloning (IBC) with Energy-Based Models (EBMs) to better capture this
multimodality. We propose Data-Augmented IBC (DA-IBC), which improves learning
by perturbing expert actions to form the counterexamples of IBC training and
using better initialization for derivative-free inference. Experiments in the
CARLA simulator with Bird's-Eye View inputs demonstrate that DA-IBC outperforms
standard IBC in urban driving tasks designed to evaluate multimodal behavior
learning in a test environment. The learned energy landscapes are able to
represent multimodal action distributions, which BC fails to achieve.

</details>


### [79] [Random Matrix Theory-guided sparse PCA for single-cell RNA-seq data](https://arxiv.org/abs/2509.15429)
*Victor Chardès*

Main category: cs.LG

TL;DR: 提出基于随机矩阵理论（RMT）改进PCA的方法，在单细胞RNA - seq数据处理中表现良好。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA - seq数据噪声大，现有计算流程难以适应异构数据集和技术，多数研究依赖PCA降维，需改进。

Method: 采用基于RMT的方法结合现有稀疏PCA算法，引入受Sinkhorn - Knopp算法启发的双白化方法，自动选择稀疏水平。

Result: 该方法在七种单细胞RNA - seq技术和四种稀疏PCA算法中，系统地改善了主子空间的重建，在细胞类型分类任务中始终优于基于PCA、自动编码器和扩散的方法。

Conclusion: 该数学方法保留了PCA的可解释性，能实现对稀疏主成分的稳健、免干预推断。

Abstract: Single-cell RNA-seq provides detailed molecular snapshots of individual cells
but is notoriously noisy. Variability stems from biological differences, PCR
amplification bias, limited sequencing depth, and low capture efficiency,
making it challenging to adapt computational pipelines to heterogeneous
datasets or evolving technologies. As a result, most studies still rely on
principal component analysis (PCA) for dimensionality reduction, valued for its
interpretability and robustness. Here, we improve upon PCA with a Random Matrix
Theory (RMT)-based approach that guides the inference of sparse principal
components using existing sparse PCA algorithms. We first introduce a novel
biwhitening method, inspired by the Sinkhorn-Knopp algorithm, that
simultaneously stabilizes variance across genes and cells. This enables the use
of an RMT-based criterion to automatically select the sparsity level, rendering
sparse PCA nearly parameter-free. Our mathematically grounded approach retains
the interpretability of PCA while enabling robust, hands-off inference of
sparse principal components. Across seven single-cell RNA-seq technologies and
four sparse PCA algorithms, we show that this method systematically improves
the reconstruction of the principal subspace and consistently outperforms PCA-,
autoencoder-, and diffusion-based methods in cell-type classification tasks.

</details>


### [80] [Computing Linear Regions in Neural Networks with Skip Connections](https://arxiv.org/abs/2509.15441)
*Johnny Joyce,Jan Verschelde*

Main category: cs.LG

TL;DR: 用热带算术表示分段线性激活函数以应用热带几何，计算神经网络线性映射区域，通过实验探讨训练难点。


<details>
  <summary>Details</summary>
Motivation: 神经网络是机器学习重要工具，期望借助热带几何研究神经网络。

Method: 用热带算术表示分段线性激活函数，提出算法计算神经网络线性映射区域，进行计算实验。

Result: 通过实验对训练神经网络的难度有了深入了解，包括过拟合问题和跳跃连接的好处。

Conclusion: 利用热带几何和相关算法能为理解神经网络训练难点提供见解。

Abstract: Neural networks are important tools in machine learning. Representing
piecewise linear activation functions with tropical arithmetic enables the
application of tropical geometry. Algorithms are presented to compute regions
where the neural networks are linear maps. Through computational experiments,
we provide insights on the difficulty to train neural networks, in particular
on the problems of overfitting and on the benefits of skip connections.

</details>


### [81] [IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs](https://arxiv.org/abs/2509.15455)
*Junchen Zhao,Ali Derakhshan,Dushyant Bharadwaj,Jayden Kana Hyman,Junhao Dong,Sangeetha Abdu Jyothi,Ian Harris*

Main category: cs.LG

TL;DR: 论文提出SPQE和IMPQ解决大语言模型低精度量化问题，实验显示IMPQ性能优于仅依赖孤立指标的方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数规模大，难以在设备或低资源环境部署，现有混合精度量化方法在平均精度低于4位时因忽略层间交互而效果不佳。

Method: 将混合精度量化问题建模为层间合作博弈，提出SPQE获取层敏感度和层间交互的Shapley估计；基于SPQE提出IMPQ，将Shapley估计转化为二元二次优化问题，在严格内存约束下为层分配2或4位精度。

Result: 在Llama - 3、Gemma - 2和Qwen - 3模型及三个后训练量化（PTQ）后端上实验，IMPQ可扩展性好，在平均精度4位到2位时，相对最佳基线将困惑度降低20% - 80%。

Conclusion: IMPQ在大语言模型低精度量化方面表现出色，优于仅依赖孤立指标的方法。

Abstract: Large Language Models (LLMs) promise impressive capabilities, yet their
multi-billion-parameter scale makes on-device or low-resource deployment
prohibitive. Mixed-precision quantization offers a compelling solution, but
existing methods struggle when the average precision drops below four bits, as
they rely on isolated, layer-specific metrics that overlook critical
inter-layer interactions affecting overall performance. In this paper, we
propose two innovations to address these limitations. First, we frame the
mixed-precision quantization problem as a cooperative game among layers and
introduce Shapley-based Progressive Quantization Estimation (SPQE) to
efficiently obtain accurate Shapley estimates of layer sensitivities and
inter-layer interactions. Second, building upon SPQE, we propose
Interaction-aware Mixed-Precision Quantization (IMPQ) which translates these
Shapley estimates into a binary quadratic optimization formulation, assigning
either 2 or 4-bit precision to layers under strict memory constraints.
Comprehensive experiments conducted on Llama-3, Gemma-2, and Qwen-3 models
across three independent PTQ backends (Quanto, HQQ, GPTQ) demonstrate IMPQ's
scalability and consistently superior performance compared to methods relying
solely on isolated metrics. Across average precisions spanning 4 bit down to 2
bit, IMPQ cuts Perplexity by 20 to 80 percent relative to the best baseline,
with the margin growing as the bit-width tightens.

</details>


### [82] [Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs](https://arxiv.org/abs/2509.15464)
*Junhong Lin,Song Wang,Xiaojie Guo,Julian Shun,Yada Zhu*

Main category: cs.LG

TL;DR: 提出EvoReasoner算法和EvoKG模块，结合时间推理与知识图谱演化，在动态问答上表现出色，缩小大小语言模型差距。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理知识演化推理时存在困难，现有结合知识图谱的方法忽略了时间动态和事实不一致性。

Method: 提出EvoReasoner进行时间感知多跳推理，引入EvoKG进行知识图谱增量更新。

Result: 在时间问答基准测试和端到端设置中表现优于基线，8B模型性能可匹配671B模型。

Conclusion: 结合时间推理与知识图谱演化对大语言模型性能提升很重要。

Abstract: Large language models (LLMs) excel at many language understanding tasks but
struggle to reason over knowledge that evolves. To address this, recent work
has explored augmenting LLMs with knowledge graphs (KGs) to provide structured,
up-to-date information. However, many existing approaches assume a static
snapshot of the KG and overlook the temporal dynamics and factual
inconsistencies inherent in real-world data. To address the challenge of
reasoning over temporally shifting knowledge, we propose EvoReasoner, a
temporal-aware multi-hop reasoning algorithm that performs global-local entity
grounding, multi-route decomposition, and temporally grounded scoring. To
ensure that the underlying KG remains accurate and up-to-date, we introduce
EvoKG, a noise-tolerant KG evolution module that incrementally updates the KG
from unstructured documents through confidence-based contradiction resolution
and temporal trend tracking. We evaluate our approach on temporal QA benchmarks
and a novel end-to-end setting where the KG is dynamically updated from raw
documents. Our method outperforms both prompting-based and KG-enhanced
baselines, effectively narrowing the gap between small and large LLMs on
dynamic question answering. Notably, an 8B-parameter model using our approach
matches the performance of a 671B model prompted seven months later. These
results highlight the importance of combining temporal reasoning with KG
evolution for robust and up-to-date LLM performance. Our code is publicly
available at github.com/junhongmit/TREK.

</details>


### [83] [Solar Forecasting with Causality: A Graph-Transformer Approach to Spatiotemporal Dependencies](https://arxiv.org/abs/2509.15481)
*Yanan Niu,Demetri Psaltis,Christophe Moser,Luisa Lambertini*

Main category: cs.LG

TL;DR: 提出SolarCAST模型，仅用历史GHI数据预测目标站点未来GHI，表现优于基线和商业预测器，是轻量级实用通用解决方案。


<details>
  <summary>Details</summary>
Motivation: 准确的太阳能预测对可再生能源管理至关重要，现有方法依赖专业硬件和大量预处理，需更优方案。

Method: 用可扩展神经组件对X - S相关性背后三类混杂因素建模，通过嵌入模块处理可观测同步变量，用时空图神经网络捕捉潜在同步因素，用门控变压器模拟时滞影响。

Result: 在不同地理条件下优于领先的时间序列和多模态基线，比顶级商业预测器Solcast误差降低25.9%。

Conclusion: SolarCAST为本地化太阳能预测提供轻量级、实用且通用的解决方案。

Abstract: Accurate solar forecasting underpins effective renewable energy management.
We present SolarCAST, a causally informed model predicting future global
horizontal irradiance (GHI) at a target site using only historical GHI from
site X and nearby stations S - unlike prior work that relies on sky-camera or
satellite imagery requiring specialized hardware and heavy preprocessing. To
deliver high accuracy with only public sensor data, SolarCAST models three
classes of confounding factors behind X-S correlations using scalable neural
components: (i) observable synchronous variables (e.g., time of day, station
identity), handled via an embedding module; (ii) latent synchronous factors
(e.g., regional weather patterns), captured by a spatio-temporal graph neural
network; and (iii) time-lagged influences (e.g., cloud movement across
stations), modeled with a gated transformer that learns temporal shifts. It
outperforms leading time-series and multimodal baselines across diverse
geographical conditions, and achieves a 25.9% error reduction over the top
commercial forecaster, Solcast. SolarCAST offers a lightweight, practical, and
generalizable solution for localized solar forecasting.

</details>


### [84] [FRAUDGUESS: Spotting and Explaining New Types of Fraud in Million-Scale Financial Data](https://arxiv.org/abs/2509.15493)
*Robson L. F. Cordeiro,Meng-Chieh Lee,Christos Faloutsos*

Main category: cs.LG

TL;DR: 本文提出 FRAUDGUESS 系统用于检测金融交易欺诈，可发现新欺诈类型并提供证据，还介绍其在实际数据集上发现的新行为。


<details>
  <summary>Details</summary>
Motivation: 解决在已知和未知类型的金融交易欺诈检测问题，并为专家提供支持观点的证据。

Method: 提出 FRAUDGUESS 系统，通过在精心设计的特征空间中识别微簇检测新欺诈类型，利用可视化、热力图和交互式仪表盘提供证据。

Result: FRAUDGUESS 在实际应用中被考虑部署，在百万级金融数据集中发现三种新行为，其中两种被专家认为欺诈或可疑，揪出数百笔原本会被忽视的欺诈交易。

Conclusion: FRAUDGUESS 系统在金融欺诈检测方面有实际应用价值，能有效发现新欺诈类型。

Abstract: Given a set of financial transactions (who buys from whom, when, and for how
much), as well as prior information from buyers and sellers, how can we find
fraudulent transactions? If we have labels for some transactions for known
types of fraud, we can build a classifier. However, we also want to find new
types of fraud, still unknown to the domain experts ('Detection'). Moreover, we
also want to provide evidence to experts that supports our opinion
('Justification'). In this paper, we propose FRAUDGUESS, to achieve two goals:
(a) for 'Detection', it spots new types of fraud as micro-clusters in a
carefully designed feature space; (b) for 'Justification', it uses
visualization and heatmaps for evidence, as well as an interactive dashboard
for deep dives. FRAUDGUESS is used in real life and is currently considered for
deployment in an Anonymous Financial Institution (AFI). Thus, we also present
the three new behaviors that FRAUDGUESS discovered in a real, million-scale
financial dataset. Two of these behaviors are deemed fraudulent or suspicious
by domain experts, catching hundreds of fraudulent transactions that would
otherwise go un-noticed.

</details>


### [85] [Detail Across Scales: Multi-Scale Enhancement for Full Spectrum Neural Representations](https://arxiv.org/abs/2509.15494)
*Yuan Ni,Zhantao Chen,Cheng Peng,Rajan Plumley,Chun Hong Yoon,Jana B. Thayer,Joshua J. Turner*

Main category: cs.LG

TL;DR: 现有INR方法在紧凑网络规模下难以表示科学数据集细节，提出WIEN - INR，实验显示其在保持模型紧凑下有高重建保真度。


<details>
  <summary>Details</summary>
Motivation: 现有INR方法在紧凑网络规模下难以忠实表示科学数据集的多尺度结构、高频信息和精细纹理。

Method: 提出WIEN - INR，在不同分辨率尺度上分配建模，并在最精细尺度使用专门的核网络恢复细节。

Result: 在不同规模和结构复杂度的科学数据集上实验，WIEN - INR在保持紧凑模型大小的同时实现了卓越的重建保真度。

Conclusion: WIEN - INR是用于高保真科学数据编码的实用神经表示框架，扩展了INR在需高效保留细节领域的适用性。

Abstract: Implicit neural representations (INRs) have emerged as a compact and
parametric alternative to discrete array-based data representations, encoding
information directly in neural network weights to enable resolution-independent
representation and memory efficiency. However, existing INR approaches, when
constrained to compact network sizes, struggle to faithfully represent the
multi-scale structures, high-frequency information, and fine textures that
characterize the majority of scientific datasets. To address this limitation,
we propose WIEN-INR, a wavelet-informed implicit neural representation that
distributes modeling across different resolution scales and employs a
specialized kernel network at the finest scale to recover subtle details. This
multi-scale architecture allows for the use of smaller networks to retain the
full spectrum of information while preserving the training efficiency and
reducing storage cost. Through extensive experiments on diverse scientific
datasets spanning different scales and structural complexities, WIEN-INR
achieves superior reconstruction fidelity while maintaining a compact model
size. These results demonstrate WIEN-INR as a practical neural representation
framework for high-fidelity scientific data encoding, extending the
applicability of INRs to domains where efficient preservation of fine detail is
essential.

</details>


### [86] [Mental Accounts for Actions: EWA-Inspired Attention in Decision Transformers](https://arxiv.org/abs/2509.15498)
*Zahra Aref,Narayan B. Mandayam*

Main category: cs.LG

TL;DR: 提出EWA - VQ - ODT模块改进Online Decision Transformers，提升样本效率和平均回报。


<details>
  <summary>Details</summary>
Motivation: 现有Online Decision Transformers使用标准注意力，缺乏动作特定结果的明确记忆，学习长期动作有效性效率低。

Method: 受认知模型启发，提出EWA - VQ - ODT模块，通过直接网格查找将连续动作路由到向量量化码本，更新吸引力并调节注意力。

Result: 在标准连续控制基准测试中，EWA - VQ - ODT提高了样本效率和平均回报，尤其在训练早期。

Conclusion: EWA - VQ - ODT模块计算高效、可解释且有理论保证。

Abstract: Transformers have emerged as a compelling architecture for sequential
decision-making by modeling trajectories via self-attention. In reinforcement
learning (RL), they enable return-conditioned control without relying on value
function approximation. Decision Transformers (DTs) exploit this by casting RL
as supervised sequence modeling, but they are restricted to offline data and
lack exploration. Online Decision Transformers (ODTs) address this limitation
through entropy-regularized training on on-policy rollouts, offering a stable
alternative to traditional RL methods like Soft Actor-Critic, which depend on
bootstrapped targets and reward shaping. Despite these advantages, ODTs use
standard attention, which lacks explicit memory of action-specific outcomes.
This leads to inefficiencies in learning long-term action effectiveness.
Inspired by cognitive models such as Experience-Weighted Attraction (EWA), we
propose Experience-Weighted Attraction with Vector Quantization for Online
Decision Transformers (EWA-VQ-ODT), a lightweight module that maintains
per-action mental accounts summarizing recent successes and failures.
Continuous actions are routed via direct grid lookup to a compact
vector-quantized codebook, where each code stores a scalar attraction updated
online through decay and reward-based reinforcement. These attractions modulate
attention by biasing the columns associated with action tokens, requiring no
change to the backbone or training objective. On standard continuous-control
benchmarks, EWA-VQ-ODT improves sample efficiency and average return over ODT,
particularly in early training. The module is computationally efficient,
interpretable via per-code traces, and supported by theoretical guarantees that
bound the attraction dynamics and its impact on attention drift.

</details>


### [87] [Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses](https://arxiv.org/abs/2509.15509)
*Xiaoshuang Wang,Yifan Lin,Enlu Zhou*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Motivated by many application problems, we consider Markov decision processes
(MDPs) with a general loss function and unknown parameters. To mitigate the
epistemic uncertainty associated with unknown parameters, we take a Bayesian
approach to estimate the parameters from data and impose a coherent risk
functional (with respect to the Bayesian posterior distribution) on the loss.
Since this formulation usually does not satisfy the interchangeability
principle, it does not admit Bellman equations and cannot be solved by
approaches based on dynamic programming. Therefore, We propose a policy
gradient optimization method, leveraging the dual representation of coherent
risk measures and extending the envelope theorem to continuous cases. We then
show the stationary analysis of the algorithm with a convergence rate of
$O(T^{-1/2}+r^{-1/2})$, where $T$ is the number of policy gradient iterations
and $r$ is the sample size of the gradient estimator. We further extend our
algorithm to an episodic setting, and establish the global convergence of the
extended algorithm and provide bounds on the number of iterations needed to
achieve an error bound $O(\epsilon)$ in each episode.

</details>


### [88] [KoopCast: Trajectory Forecasting via Koopman Operators](https://arxiv.org/abs/2509.15513)
*Jungjin Lee,Jaeuk Shin,Gihwan Kim,Joonho Han,Insoon Yang*

Main category: cs.LG

TL;DR: 提出轻量级高效轨迹预测模型KoopCast，在多数据集验证其准确性、可解释性和高效性。


<details>
  <summary>Details</summary>
Motivation: 在一般动态环境中进行高效轨迹预测，解决非线性动力学问题。

Method: 利用Koopman算子理论，采用两阶段设计，先进行概率神经目标估计，再用基于Koopman算子的细化模块。

Result: 在ETH/UCY、Waymo Open Motion Dataset和nuScenes数据集上验证了模型有高预测准确性、模式级可解释性和实际效率。

Conclusion: KoopCast模型具有竞争优势，能在捕捉非线性动力学的同时保证预测准确性、可解释性和低延迟部署。

Abstract: We present KoopCast, a lightweight yet efficient model for trajectory
forecasting in general dynamic environments. Our approach leverages Koopman
operator theory, which enables a linear representation of nonlinear dynamics by
lifting trajectories into a higher-dimensional space. The framework follows a
two-stage design: first, a probabilistic neural goal estimator predicts
plausible long-term targets, specifying where to go; second, a Koopman
operator-based refinement module incorporates intention and history into a
nonlinear feature space, enabling linear prediction that dictates how to go.
This dual structure not only ensures strong predictive accuracy but also
inherits the favorable properties of linear operators while faithfully
capturing nonlinear dynamics. As a result, our model offers three key
advantages: (i) competitive accuracy, (ii) interpretability grounded in Koopman
spectral theory, and (iii) low-latency deployment. We validate these benefits
on ETH/UCY, the Waymo Open Motion Dataset, and nuScenes, which feature rich
multi-agent interactions and map-constrained nonlinear motion. Across
benchmarks, KoopCast consistently delivers high predictive accuracy together
with mode-level interpretability and practical efficiency.

</details>


### [89] [Manifold Dimension Estimation: An Empirical Study](https://arxiv.org/abs/2509.15517)
*Zelong Bi,Pierre Lafaye de Micheaux*

Main category: cs.LG

TL;DR: 文章对高维数据低维流形维度估计方法进行全面综述，实验分析各因素对性能影响并比较不同估计器，结果表明简单方法常表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有流形维度估计工作零散且缺乏系统评估，需为研究者和从业者提供全面综述。

Method: 回顾理论基础，介绍八个代表性估计器，通过控制实验分析各因素影响，在多样数据集上比较估计器并引入超参数调优方法。

Result: 给出实用指导，表明简单方法在该类问题中表现更好。

Conclusion: 简单方法在高维数据低维流形维度估计这类通用问题中表现较好，研究为相关工作提供了全面参考。

Abstract: The manifold hypothesis suggests that high-dimensional data often lie on or
near a low-dimensional manifold. Estimating the dimension of this manifold is
essential for leveraging its structure, yet existing work on dimension
estimation is fragmented and lacks systematic evaluation. This article provides
a comprehensive survey for both researchers and practitioners. We review
often-overlooked theoretical foundations and present eight representative
estimators. Through controlled experiments, we analyze how individual factors
such as noise, curvature, and sample size affect performance. We also compare
the estimators on diverse synthetic and real-world datasets, introducing a
principled approach to dataset-specific hyperparameter tuning. Our results
offer practical guidance and suggest that, for a problem of this generality,
simpler methods often perform better.

</details>


### [90] [Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem](https://arxiv.org/abs/2509.15519)
*Chao Li,Bingkun Bao,Yang Gao*

Main category: cs.LG

TL;DR: 本文提出Dynamics - Aware Context (DAC)方法解决完全去中心化合作多智能体强化学习中的非平稳性和相对过泛化问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有工作无法在完全去中心化设置中同时解决多智能体强化学习中价值函数更新的非平稳性和价值函数估计的相对过泛化问题。

Method: 提出DAC方法，将每个智能体局部感知的任务形式化为上下文马尔可夫决策过程，通过动态感知上下文建模解决两个问题，引入基于上下文的价值函数和乐观边际值。

Result: 在多种合作任务上评估DAC，与多个基线相比表现更优。

Conclusion: DAC方法在完全去中心化合作多智能体强化学习中有效，能解决非平稳性和相对过泛化问题。

Abstract: This paper studies fully decentralized cooperative multi-agent reinforcement
learning, where each agent solely observes the states, its local actions, and
the shared rewards. The inability to access other agents' actions often leads
to non-stationarity during value function updates and relative
overgeneralization during value function estimation, hindering effective
cooperative policy learning. However, existing works fail to address both
issues simultaneously, due to their inability to model the joint policy of
other agents in a fully decentralized setting. To overcome this limitation, we
propose a novel method named Dynamics-Aware Context (DAC), which formalizes the
task, as locally perceived by each agent, as an Contextual Markov Decision
Process, and further addresses both non-stationarity and relative
overgeneralization through dynamics-aware context modeling. Specifically, DAC
attributes the non-stationary local task dynamics of each agent to switches
between unobserved contexts, each corresponding to a distinct joint policy.
Then, DAC models the step-wise dynamics distribution using latent variables and
refers to them as contexts. For each agent, DAC introduces a context-based
value function to address the non-stationarity issue during value function
update. For value function estimation, an optimistic marginal value is derived
to promote the selection of cooperative actions, thereby addressing the
relative overgeneralization issue. Experimentally, we evaluate DAC on various
cooperative tasks (including matrix game, predator and prey, and SMAC), and its
superior performance against multiple baselines validates its effectiveness.

</details>


### [91] [Universal Learning of Stochastic Dynamics for Exact Belief Propagation using Bernstein Normalizing Flows](https://arxiv.org/abs/2509.15533)
*Peter Amorese,Morteza Lahijanian*

Main category: cs.LG

TL;DR: 本文为一类满足通用近似非线性随机动力学且支持解析信念传播的模型奠定理论基础，结合归一化流和伯恩斯坦多项式，实验显示其在信念传播上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 非线性动力学使解析信念传播难以处理，在系统模型未知需从数据学习时，要找到能通用近似一般非线性随机动力学且支持解析信念传播的模型。

Method: 将用于密度估计的归一化流的表达能力与伯恩斯坦多项式的解析易处理性相结合。

Result: 所学习的模型在信念传播方面优于现有数据驱动方法，尤其在具有非加性、非高斯噪声的高度非线性系统中。

Conclusion: 所提出的一类模型能满足通用近似和支持解析信念传播的要求，且在实验中有良好表现。

Abstract: Predicting the distribution of future states in a stochastic system, known as
belief propagation, is fundamental to reasoning under uncertainty. However,
nonlinear dynamics often make analytical belief propagation intractable,
requiring approximate methods. When the system model is unknown and must be
learned from data, a key question arises: can we learn a model that (i)
universally approximates general nonlinear stochastic dynamics, and (ii)
supports analytical belief propagation? This paper establishes the theoretical
foundations for a class of models that satisfy both properties. The proposed
approach combines the expressiveness of normalizing flows for density
estimation with the analytical tractability of Bernstein polynomials. Empirical
results show the efficacy of our learned model over state-of-the-art
data-driven methods for belief propagation, especially for highly non-linear
systems with non-additive, non-Gaussian noise.

</details>


### [92] [Nonconvex Decentralized Stochastic Bilevel Optimization under Heavy-Tailed Noises](https://arxiv.org/abs/2509.15543)
*Xinwen Zhang,Yihan Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: 提出适用于重尾噪声下非凸双层优化问题的去中心化随机双层优化算法，有理论保证且实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化随机优化方法假设过强，在实际机器学习模型中不满足，需解决这些局限。

Method: 开发归一化随机方差缩减双层梯度下降算法，不依赖裁剪操作，通过创新地界定相互依赖的梯度序列来建立收敛率。

Result: 得到首个在重尾噪声下有严格理论保证的去中心化双层优化算法，实验结果证实算法处理重尾噪声有效。

Conclusion: 所提算法能有效处理重尾噪声下的非凸双层优化问题。

Abstract: Existing decentralized stochastic optimization methods assume the lower-level
loss function is strongly convex and the stochastic gradient noise has finite
variance. These strong assumptions typically are not satisfied in real-world
machine learning models. To address these limitations, we develop a novel
decentralized stochastic bilevel optimization algorithm for the nonconvex
bilevel optimization problem under heavy-tailed noises. Specifically, we
develop a normalized stochastic variance-reduced bilevel gradient descent
algorithm, which does not rely on any clipping operation. Moreover, we
establish its convergence rate by innovatively bounding interdependent gradient
sequences under heavy-tailed noises for nonconvex decentralized bilevel
optimization problems. As far as we know, this is the first decentralized
bilevel optimization algorithm with rigorous theoretical guarantees under
heavy-tailed noises. The extensive experimental results confirm the
effectiveness of our algorithm in handling heavy-tailed noises.

</details>


### [93] [PolyJuice Makes It Real: Black-Box, Universal Red Teaming for Synthetic Image Detectors](https://arxiv.org/abs/2509.15551)
*Sepehr Dehdashtian,Mashrur M. Morshed,Jacob H. Seidman,Gaurav Bharaj,Vishnu Naresh Boddeti*

Main category: cs.LG

TL;DR: 提出针对合成图像检测器（SIDs）的黑盒、与图像无关的红队方法PolyJuice，能有效欺骗SIDs，还可提升检测器性能。


<details>
  <summary>Details</summary>
Motivation: 现有红队解决方案需白盒访问SIDs且通过昂贵在线优化生成特定图像攻击，存在局限性。

Method: 基于SIDs正确和错误分类样本在T2I潜在空间的分布偏移，通过轻量级离线过程识别偏移方向，将生成图像导向SIDs失效模式。

Result: PolyJuice引导的T2I模型欺骗SIDs效果提升至84%，可在低分辨率下有效估计转向方向并插值到高分辨率，降低计算开销。

Conclusion: 在PolyJuice增强数据集上调整SIDs模型可显著提升检测器性能（最高30%）。

Abstract: Synthetic image detectors (SIDs) are a key defense against the risks posed by
the growing realism of images from text-to-image (T2I) models. Red teaming
improves SID's effectiveness by identifying and exploiting their failure modes
via misclassified synthetic images. However, existing red-teaming solutions (i)
require white-box access to SIDs, which is infeasible for proprietary
state-of-the-art detectors, and (ii) generate image-specific attacks through
expensive online optimization. To address these limitations, we propose
PolyJuice, the first black-box, image-agnostic red-teaming method for SIDs,
based on an observed distribution shift in the T2I latent space between samples
correctly and incorrectly classified by the SID. PolyJuice generates attacks by
(i) identifying the direction of this shift through a lightweight offline
process that only requires black-box access to the SID, and (ii) exploiting
this direction by universally steering all generated images towards the SID's
failure modes. PolyJuice-steered T2I models are significantly more effective at
deceiving SIDs (up to 84%) compared to their unsteered counterparts. We also
show that the steering directions can be estimated efficiently at lower
resolutions and transferred to higher resolutions using simple interpolation,
reducing computational overhead. Finally, tuning SID models on
PolyJuice-augmented datasets notably enhances the performance of the detectors
(up to 30%).

</details>


### [94] [The Multi-Query Paradox in Zeroth-Order Optimization](https://arxiv.org/abs/2509.15552)
*Wei Lin,Qingyu Song,Hong Xu*

Main category: cs.LG

TL;DR: 文章探讨零阶优化查询分配问题，分析两种聚合方法，得出不同结论并经实验验证。


<details>
  <summary>Details</summary>
Motivation: 单查询零阶优化估计方差高，多查询存在查询分配难题待解决。

Method: 分析简单平均法（ZO - Avg）和新的投影对齐法（ZO - Align），推导不同设置下收敛率。

Result: ZO - Avg单查询最优，ZO - Align多查询更优。

Conclusion: 多查询问题本质是两种经典算法的选择，由聚合方法决定。

Abstract: Zeroth-order (ZO) optimization provides a powerful framework for problems
where explicit gradients are unavailable and have to be approximated using only
queries to function value. The prevalent single-query approach is simple, but
suffers from high estimation variance, motivating a multi-query paradigm to
improves estimation accuracy. This, however, creates a critical trade-off:
under a fixed budget of queries (i.e. cost), queries per iteration and the
total number of optimization iterations are inversely proportional to one
another. How to best allocate this budget is a fundamental, under-explored
question.
  This work systematically resolves this query allocation problem. We analyze
two aggregation methods: the de facto simple averaging (ZO-Avg), and a new
Projection Alignment method (ZO-Align) we derive from local surrogate
minimization. By deriving convergence rates for both methods that make the
dependence on the number of queries explicit across strongly convex, convex,
non-convex, and stochastic settings, we uncover a stark dichotomy: For ZO-Avg,
we prove that using more than one query per iteration is always
query-inefficient, rendering the single-query approach optimal. On the
contrary, ZO-Align generally performs better with more queries per iteration,
resulting in a full-subspace estimation as the optimal approach. Thus, our work
clarifies that the multi-query problem boils down to a choice not about an
intermediate query size, but between two classic algorithms, a choice dictated
entirely by the aggregation method used. These theoretical findings are also
consistently validated by extensive experiments.

</details>


### [95] [Reward Hacking Mitigation using Verifiable Composite Rewards](https://arxiv.org/abs/2509.15557)
*Mirza Farhan Bin Tarek,Rahmatollah Beheshti*

Main category: cs.LG

TL;DR: 论文指出RLVR在医疗问答领域推理阶段易出现奖励破解问题，提出复合奖励函数，实验表明该方法能减少奖励破解、提升推理格式和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR在医疗问答领域推理阶段出现的奖励破解问题，如不推理直接给答案、用非标准推理格式利用奖励机制。

Method: 引入针对特定奖励破解行为有惩罚的复合奖励函数。

Result: 扩展后的RLVR与基线相比，推理格式更好，奖励破解减少，且有较好的准确性。

Conclusion: 该方法有助于减少奖励破解，提高使用RLVR模型的可靠性。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that
large language models (LLMs) can develop their own reasoning without direct
supervision. However, applications in the medical domain, specifically for
question answering, are susceptible to significant reward hacking during the
reasoning phase. Our work addresses two primary forms of this behavior: i)
providing a final answer without preceding reasoning, and ii) employing
non-standard reasoning formats to exploit the reward mechanism. To mitigate
these, we introduce a composite reward function with specific penalties for
these behaviors. Our experiments show that extending RLVR with our proposed
reward model leads to better-formatted reasoning with less reward hacking and
good accuracy compared to the baselines. This approach marks a step toward
reducing reward hacking and enhancing the reliability of models utilizing RLVR.

</details>


### [96] [Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning](https://arxiv.org/abs/2509.15561)
*Om Naphade,Saksham Bansal,Parikshit Pareek*

Main category: cs.LG

TL;DR: 提出用小语言模型进行超参数调优的专家块框架，其核心TCS能让小模型可靠分析优化进度，在六个任务上性能接近GPT - 4。


<details>
  <summary>Details</summary>
Motivation: 超参数调优在机器学习中计算成本高且不透明，现有用大语言模型进行超参数调优大多依赖超1000亿参数模型。

Method: 提出基于小语言模型的专家块框架，核心是将原始训练轨迹转换为结构化上下文的轨迹上下文汇总器（TCS）。

Result: 使用两个本地运行的小语言模型和10次试验预算，在六个不同任务上，TCS支持的超参数调优管道平均性能与GPT - 4相差约0.9个百分点。

Conclusion: 基于小语言模型的专家块框架在超参数调优上有良好表现，性能接近大模型。

Abstract: Hyper-parameter Tuning (HPT) is a necessary step in machine learning (ML)
pipelines but becomes computationally expensive and opaque with larger models.
Recently, Large Language Models (LLMs) have been explored for HPT, yet most
rely on models exceeding 100 billion parameters. We propose an Expert Block
Framework for HPT using Small LLMs. At its core is the Trajectory Context
Summarizer (TCS), a deterministic block that transforms raw training
trajectories into structured context, enabling small LLMs to analyze
optimization progress with reliability comparable to larger models. Using two
locally-run LLMs (phi4:reasoning14B and qwen2.5-coder:32B) and a 10-trial
budget, our TCS-enabled HPT pipeline achieves average performance within ~0.9
percentage points of GPT-4 across six diverse tasks.

</details>


### [97] [How many classes do we need to see for novel class discovery?](https://arxiv.org/abs/2509.15585)
*Akanksha Sarkar,Been Kim,Jennifer J. Sun*

Main category: cs.LG

TL;DR: 提出用dSprites数据集的实验框架研究新类别发现，发现已知类别数量收益有饱和点。


<details>
  <summary>Details</summary>
Motivation: 现有数据集因素复杂纠缠，新类别发现的基础问题待解，需系统研究。

Method: 使用dSprites数据集和程序生成的修改因素构建可控实验框架。

Result: 已知类别数量对发现性能的益处存在饱和点，之后性能趋于平稳。

Conclusion: 不同设置下的收益递减模式为从业者成本效益分析提供见解，为复杂真实数据集的类别发现研究提供起点。

Abstract: Novel class discovery is essential for ML models to adapt to evolving
real-world data, with applications ranging from scientific discovery to
robotics. However, these datasets contain complex and entangled factors of
variation, making a systematic study of class discovery difficult. As a result,
many fundamental questions are yet to be answered on why and when new class
discoveries are more likely to be successful. To address this, we propose a
simple controlled experimental framework using the dSprites dataset with
procedurally generated modifying factors. This allows us to investigate what
influences successful class discovery. In particular, we study the relationship
between the number of known/unknown classes and discovery performance, as well
as the impact of known class 'coverage' on discovering new classes. Our
empirical results indicate that the benefit of the number of known classes
reaches a saturation point beyond which discovery performance plateaus. The
pattern of diminishing return across different settings provides an insight for
cost-benefit analysis for practitioners and a starting point for more rigorous
future research of class discovery on complex real-world datasets.

</details>


### [98] [Personalized Prediction By Learning Halfspace Reference Classes Under Well-Behaved Distribution](https://arxiv.org/abs/2509.15592)
*Jizhou Huang,Brendan Juba*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In machine learning applications, predictive models are trained to serve
future queries across the entire data distribution. Real-world data often
demands excessively complex models to achieve competitive performance, however,
sacrificing interpretability. Hence, the growing deployment of machine learning
models in high-stakes applications, such as healthcare, motivates the search
for methods for accurate and explainable predictions. This work proposes a
Personalized Prediction scheme, where an easy-to-interpret predictor is learned
per query. In particular, we wish to produce a "sparse linear" classifier with
competitive performance specifically on some sub-population that includes the
query point. The goal of this work is to study the PAC-learnability of this
prediction model for sub-populations represented by "halfspaces" in a
label-agnostic setting. We first give a distribution-specific PAC-learning
algorithm for learning reference classes for personalized prediction. By
leveraging both the reference-class learning algorithm and a list learner of
sparse linear representations, we prove the first upper bound,
$O(\mathrm{opt}^{1/4} )$, for personalized prediction with sparse linear
classifiers and homogeneous halfspace subsets. We also evaluate our algorithms
on a variety of standard benchmark data sets.

</details>


### [99] [Efficient Extractive Text Summarization for Online News Articles Using Machine Learning](https://arxiv.org/abs/2509.15614)
*Sajib Biswas,Milon Biswas,Arunima Mandal,Fatema Tabassum Liza,Joy Sarker*

Main category: cs.LG

TL;DR: 文章利用机器学习技术处理在线新闻文章提取式文本摘要问题，用Cornell Newsroom数据集，对比多种模型，发现LSTM网络表现更佳，凸显自动摘要对在线新闻平台内容管理的潜力。


<details>
  <summary>Details</summary>
Motivation: 信息过载时代，为提升在线新闻文章内容管理的可访问性和用户参与度，解决提取式文本摘要的挑战。

Method: 使用Cornell Newsroom数据集，利用BERT嵌入将文本数据转换为数值表示，将任务构建为二分类问题，探索逻辑回归、前馈神经网络和LSTM网络等模型。

Result: LSTM网络在F1分数和ROUGE - 1指标上优于Lede - 3等基线方法和简单模型。

Conclusion: 自动摘要在改善在线新闻平台内容管理系统、提高内容组织效率和用户体验方面具有潜力。

Abstract: In the age of information overload, content management for online news
articles relies on efficient summarization to enhance accessibility and user
engagement. This article addresses the challenge of extractive text
summarization by employing advanced machine learning techniques to generate
concise and coherent summaries while preserving the original meaning. Using the
Cornell Newsroom dataset, comprising 1.3 million article-summary pairs, we
developed a pipeline leveraging BERT embeddings to transform textual data into
numerical representations. By framing the task as a binary classification
problem, we explored various models, including logistic regression,
feed-forward neural networks, and long short-term memory (LSTM) networks. Our
findings demonstrate that LSTM networks, with their ability to capture
sequential dependencies, outperform baseline methods like Lede-3 and simpler
models in F1 score and ROUGE-1 metrics. This study underscores the potential of
automated summarization in improving content management systems for online news
platforms, enabling more efficient content organization and enhanced user
experiences.

</details>


### [100] [Toward Efficient Influence Function: Dropout as a Compression Tool](https://arxiv.org/abs/2509.15651)
*Yuchen Zhang,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 提出用dropout作为梯度压缩机制高效计算影响函数的方法，可用于大规模模型。


<details>
  <summary>Details</summary>
Motivation: 评估训练数据对机器学习模型的影响很重要，但影响函数计算和内存成本高，尤其是大规模模型。

Method: 引入用dropout作为梯度压缩机制的新方法来计算影响函数。

Result: 显著降低计算和内存开销，保留数据影响的关键部分。

Conclusion: 该方法能应用于现代大规模模型。

Abstract: Assessing the impact the training data on machine learning models is crucial
for understanding the behavior of the model, enhancing the transparency, and
selecting training data. Influence function provides a theoretical framework
for quantifying the effect of training data points on model's performance given
a specific test data. However, the computational and memory costs of influence
function presents significant challenges, especially for large-scale models,
even when using approximation methods, since the gradients involved in
computation are as large as the model itself. In this work, we introduce a
novel approach that leverages dropout as a gradient compression mechanism to
compute the influence function more efficiently. Our method significantly
reduces computational and memory overhead, not only during the influence
function computation but also in gradient compression process. Through
theoretical analysis and empirical validation, we demonstrate that our method
could preserves critical components of the data influence and enables its
application to modern large-scale models.

</details>


### [101] [Nonconvex Regularization for Feature Selection in Reinforcement Learning](https://arxiv.org/abs/2509.15652)
*Kyohei Suzuki,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: 提出一种强化学习特征选择的批量算法，有理论收敛保证，实验显示性能优越。


<details>
  <summary>Details</summary>
Motivation: 缓解传统正则化方案中的估计偏差问题。

Method: 在经典最小二乘时间差分（LSTD）框架内扩展策略评估，引入投影极小极大凹（PMC）惩罚项构建目标函数；为前向反射后向分裂（FRBS）算法建立新的收敛条件。

Result: 在基准数据集上的数值实验表明，该方法显著优于现有特征选择方法，尤其在有大量噪声特征的场景中。

Conclusion: 所提出的特征选择批量算法有效且性能优越。

Abstract: This work proposes an efficient batch algorithm for feature selection in
reinforcement learning (RL) with theoretical convergence guarantees. To
mitigate the estimation bias inherent in conventional regularization schemes,
the first contribution extends policy evaluation within the classical
least-squares temporal-difference (LSTD) framework by formulating a
Bellman-residual objective regularized with the sparsity-inducing, nonconvex
projected minimax concave (PMC) penalty. Owing to the weak convexity of the PMC
penalty, this formulation can be interpreted as a special instance of a general
nonmonotone-inclusion problem. The second contribution establishes novel
convergence conditions for the forward-reflected-backward splitting (FRBS)
algorithm to solve this class of problems. Numerical experiments on benchmark
datasets demonstrate that the proposed approach substantially outperforms
state-of-the-art feature-selection methods, particularly in scenarios with many
noisy features.

</details>


### [102] [KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning](https://arxiv.org/abs/2509.15676)
*Vaibhav Singh,Soumya Suvra Ghosal,Kapu Nirmal Joshua,Soumyabrata Pal,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 本文从信息论角度研究上下文中学习（ICL）的示例选择问题，提出新方法并在分类任务中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有最近邻方法在高维嵌入空间存在泛化性差和缺乏多样性等问题，需要更好的示例选择方法。

Method: 将大语言模型建模为输入嵌入的线性函数，把示例选择任务构建为特定查询的优化问题，推导近似子模的替代目标，使用贪心算法，结合核技巧和正则化器。

Result: 在一系列分类任务中比标准检索方法有显著提升。

Conclusion: 在现实世界标签稀缺场景中，结构感知、多样化的示例选择对ICL有益。

Abstract: In-context learning (ICL) has emerged as a powerful paradigm for adapting
large language models (LLMs) to new and data-scarce tasks using only a few
carefully selected task-specific examples presented in the prompt. However,
given the limited context size of LLMs, a fundamental question arises: Which
examples should be selected to maximize performance on a given user query?
While nearest-neighbor-based methods like KATE have been widely adopted for
this purpose, they suffer from well-known drawbacks in high-dimensional
embedding spaces, including poor generalization and a lack of diversity. In
this work, we study this problem of example selection in ICL from a principled,
information theory-driven perspective. We first model an LLM as a linear
function over input embeddings and frame the example selection task as a
query-specific optimization problem: selecting a subset of exemplars from a
larger example bank that minimizes the prediction error on a specific query.
This formulation departs from traditional generalization-focused learning
theoretic approaches by targeting accurate prediction for a specific query
instance. We derive a principled surrogate objective that is approximately
submodular, enabling the use of a greedy algorithm with an approximation
guarantee. We further enhance our method by (i) incorporating the kernel trick
to operate in high-dimensional feature spaces without explicit mappings, and
(ii) introducing an optimal design-based regularizer to encourage diversity in
the selected examples. Empirically, we demonstrate significant improvements
over standard retrieval methods across a suite of classification tasks,
highlighting the benefits of structure-aware, diverse example selection for ICL
in real-world, label-scarce scenarios.

</details>


### [103] [RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation](https://arxiv.org/abs/2509.15724)
*Davide Ettori,Nastaran Darabi,Sureshkumar Senthilkumar,Amit Ranjan Trivedi*

Main category: cs.LG

TL;DR: 提出RMT - KD压缩方法，利用随机矩阵理论进行知识蒸馏，在多数据集上实现参数减少、推理加速和功耗降低。


<details>
  <summary>Details</summary>
Motivation: 大型深度学习模型因规模和计算需求大，在边缘部署成本高，需有效压缩方法。

Method: 利用随机矩阵理论进行知识蒸馏，逐层应用基于RMT的因果约简和自蒸馏。

Result: 在GLUE、AG News和CIFAR - 10上实现高达80%的参数减少，仅2%的精度损失，推理速度提升2.8倍，功耗近乎减半。

Conclusion: RMT - KD是一种有数学依据的网络蒸馏方法。

Abstract: Large deep learning models such as BERT and ResNet achieve state-of-the-art
performance but are costly to deploy at the edge due to their size and compute
demands. We present RMT-KD, a compression method that leverages Random Matrix
Theory (RMT) for knowledge distillation to iteratively reduce network size.
Instead of pruning or heuristic rank selection, RMT-KD preserves only
informative directions identified via the spectral properties of hidden
representations. RMT-based causal reduction is applied layer by layer with
self-distillation to maintain stability and accuracy. On GLUE, AG News, and
CIFAR-10, RMT-KD achieves up to 80% parameter reduction with only 2% accuracy
loss, delivering 2.8x faster inference and nearly halved power consumption.
These results establish RMT-KD as a mathematically grounded approach to network
distillation.

</details>


### [104] [EigenTrack: Spectral Activation Feature Tracking for Hallucination and Out-of-Distribution Detection in LLMs and VLMs](https://arxiv.org/abs/2509.15735)
*Davide Ettori,Nastaran Darabi,Sina Tayebati,Ranganath Krishnan,Mahesh Subedar,Omesh Tickoo,Amit Ranjan Trivedi*

Main category: cs.LG

TL;DR: 提出EigenTrack实时检测器，利用隐藏激活的光谱几何检测大语言模型幻觉和OOD错误，无需重采样，有优势。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉和OOD错误问题，需要有效检测方法。

Method: 使用隐藏激活的光谱几何，将协方差 - 频谱统计数据输入轻量级循环分类器。

Result: 能在表面错误出现前检测到表征结构的时间变化，发现幻觉和OOD漂移。

Conclusion: 与黑盒、灰盒和现有白盒方法相比，EigenTrack有无需重采样、保留时间上下文等优势。

Abstract: Large language models (LLMs) offer broad utility but remain prone to
hallucination and out-of-distribution (OOD) errors. We propose EigenTrack, an
interpretable real-time detector that uses the spectral geometry of hidden
activations, a compact global signature of model dynamics. By streaming
covariance-spectrum statistics such as entropy, eigenvalue gaps, and KL
divergence from random baselines into a lightweight recurrent classifier,
EigenTrack tracks temporal shifts in representation structure that signal
hallucination and OOD drift before surface errors appear. Unlike black- and
grey-box methods, it needs only a single forward pass without resampling.
Unlike existing white-box detectors, it preserves temporal context, aggregates
global signals, and offers interpretable accuracy-latency trade-offs.

</details>


### [105] [Aircraft Fuel Flow Modelling with Ageing Effects: From Parametric Corrections to Neural Networks](https://arxiv.org/abs/2509.15736)
*Gabriel Jarry,Ramon Dalmau,Philippe Very,Junzi Sun*

Main category: cs.LG

TL;DR: 本文研究将发动机老化效应纳入空客A320 - 214燃油流量预测，对比多种方法，结果显示考虑老化因素可提升预测准确性，但受限于数据集。


<details>
  <summary>Details</summary>
Motivation: 标准参数模型常忽略飞机老化导致的性能下降，准确建模飞机燃油流量对运营规划和环境影响评估至关重要。

Method: 利用约一万九千次快速访问记录器航班的综合数据集，系统评估经典物理模型、经验修正系数和数据驱动的神经网络架构。

Result: 基线模型会低估旧飞机燃油消耗，使用与年龄相关的修正因子和神经模型可减少偏差、提高预测准确性。

Conclusion: 强调在参数和机器学习框架中考虑老化效应的重要性，指出需要更多样化数据集。

Abstract: Accurate modelling of aircraft fuel-flow is crucial for both operational
planning and environmental impact assessment, yet standard parametric models
often neglect performance deterioration that occurs as aircraft age. This paper
investigates multiple approaches to integrate engine ageing effects into
fuel-flow prediction for the Airbus A320-214, using a comprehensive dataset of
approximately nineteen thousand Quick Access Recorder flights from nine
distinct airframes with varying years in service. We systematically evaluate
classical physics-based models, empirical correction coefficients, and
data-driven neural network architectures that incorporate age either as an
input feature or as an explicit multiplicative bias. Results demonstrate that
while baseline models consistently underestimate fuel consumption for older
aircraft, the use of age-dependent correction factors and neural models
substantially reduces bias and improves prediction accuracy. Nevertheless,
limitations arise from the small number of airframes and the lack of detailed
maintenance event records, which constrain the representativeness and
generalization of age-based corrections. This study emphasizes the importance
of accounting for the effects of ageing in parametric and machine learning
frameworks to improve the reliability of operational and environmental
assessments. The study also highlights the need for more diverse datasets that
can capture the complexity of real-world engine deterioration.

</details>


### [106] [GUI-ReWalk: Massive Data Generation for GUI Agent via Stochastic Exploration and Intent-Aware Reasoning](https://arxiv.org/abs/2509.15738)
*Musen Lin,Minghao Liu,Taoran Lu,Lichen Yuan,Yiwei Liu,Haonan Xu,Yu Miao,Yuhao Chao,Zhaojian Li*

Main category: cs.LG

TL;DR: 本文提出GUI - ReWalk框架合成GUI轨迹数据，训练模型并评估，证明其在推进GUI代理研究和实现自动化方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理发展受高质量轨迹数据稀缺限制，现有数据收集策略存在成本高、多样性与任务覆盖不足等问题。

Method: 提出GUI - ReWalk框架，先随机探索模拟人类试错行为，再进入推理引导阶段，支持多步任务生成。

Result: 在多个基准测试中，GUI - ReWalk实现了更优的交互流覆盖、更高的轨迹熵和更真实的用户意图。

Conclusion: GUI - ReWalk是一个可扩展且数据高效的框架，能推进GUI代理研究和实现强大的现实自动化。

Abstract: Graphical User Interface (GUI) Agents, powered by large language and
vision-language models, hold promise for enabling end-to-end automation in
digital environments. However, their progress is fundamentally constrained by
the scarcity of scalable, high-quality trajectory data. Existing data
collection strategies either rely on costly and inconsistent manual annotations
or on synthetic generation methods that trade off between diversity and
meaningful task coverage. To bridge this gap, we present GUI-ReWalk: a
reasoning-enhanced, multi-stage framework for synthesizing realistic and
diverse GUI trajectories. GUI-ReWalk begins with a stochastic exploration phase
that emulates human trial-and-error behaviors, and progressively transitions
into a reasoning-guided phase where inferred goals drive coherent and
purposeful interactions. Moreover, it supports multi-stride task generation,
enabling the construction of long-horizon workflows across multiple
applications. By combining randomness for diversity with goal-aware reasoning
for structure, GUI-ReWalk produces data that better reflects the intent-aware,
adaptive nature of human-computer interaction. We further train Qwen2.5-VL-7B
on the GUI-ReWalk dataset and evaluate it across multiple benchmarks, including
Screenspot-Pro, OSWorld-G, UI-Vision, AndroidControl, and GUI-Odyssey. Results
demonstrate that GUI-ReWalk enables superior coverage of diverse interaction
flows, higher trajectory entropy, and more realistic user intent. These
findings establish GUI-ReWalk as a scalable and data-efficient framework for
advancing GUI agent research and enabling robust real-world automation.

</details>


### [107] [Incremental Multistep Forecasting of Battery Degradation Using Pseudo Targets](https://arxiv.org/abs/2509.15740)
*Jonathan Adam Rico,Nagarajan Raghavan,Senthilnath Jayavelu*

Main category: cs.LG

TL;DR: 提出iFSNet模型实现电池多步预测，在不同数据集取得较好效果


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型多为离线模式，在线增量多步预测有挑战，需能适应不同数据分布的在线模型

Method: 提出iFSNet模型，利用输入序列线性回归外推伪目标，计算损失更新模型

Result: 在平滑退化轨迹数据集上RMSE为0.00197、MAE为0.00154；在有容量再生尖峰的不规则退化轨迹数据集上RMSE为0.01588、MAE为0.01234

Conclusion: iFSNet模型可实现电池多步预测，能适应不同数据分布

Abstract: Data-driven models accurately perform early battery prognosis to prevent
equipment failure and further safety hazards. Most existing machine learning
(ML) models work in offline mode which must consider their retraining
post-deployment every time new data distribution is encountered. Hence, there
is a need for an online ML approach where the model can adapt to varying
distributions. However, existing online incremental multistep forecasts are a
great challenge as there is no way to correct the model of its forecasts at the
current instance. Also, these methods need to wait for a considerable amount of
time to acquire enough streaming data before retraining. In this study, we
propose iFSNet (incremental Fast and Slow learning Network) which is a modified
version of FSNet for a single-pass mode (sample-by-sample) to achieve multistep
forecasting using pseudo targets. It uses a simple linear regressor of the
input sequence to extrapolate pseudo future samples (pseudo targets) and
calculate the loss from the rest of the forecast and keep updating the model.
The model benefits from the associative memory and adaptive structure
mechanisms of FSNet, at the same time the model incrementally improves by using
pseudo targets. The proposed model achieved 0.00197 RMSE and 0.00154 MAE on
datasets with smooth degradation trajectories while it achieved 0.01588 RMSE
and 0.01234 MAE on datasets having irregular degradation trajectories with
capacity regeneration spikes.

</details>


### [108] [On Optimal Steering to Achieve Exact Fairness](https://arxiv.org/abs/2509.15759)
*Mohit Sharma,Amit Jayant Deshpande,Chiranjib Bhattacharyya,Rajiv Ratn Shah*

Main category: cs.LG

TL;DR: 本文旨在解决公平机器学习中的“偏进偏出”问题，定义理想分布，提出最优转向优化方案并给出算法，实验证明能提升公平性且不降低效用，还展示了对大语言模型表示的转向应用。


<details>
  <summary>Details</summary>
Motivation: 解决公平机器学习中“偏进偏出”问题，为公平生成模型和表示转向提供可证明的公平性保证。

Method: 定义理想分布，通过在KL散度中寻找最近的理想分布来制定最优转向的优化程序，并针对特定参数族提供高效算法。

Result: 在合成和真实数据集上，最优转向技术提升了公平性且不降低效用，有时还能提升效用；展示了对大语言模型表示的仿射转向以减少多分类中的偏差。

Conclusion: 提出的方法能有效优化特征分布或大语言模型内部表示，实现不同群体间公平且不损失效用。

Abstract: To fix the 'bias in, bias out' problem in fair machine learning, it is
important to steer feature distributions of data or internal representations of
Large Language Models (LLMs) to ideal ones that guarantee group-fair outcomes.
Previous work on fair generative models and representation steering could
greatly benefit from provable fairness guarantees on the model output. We
define a distribution as ideal if the minimizer of any cost-sensitive risk on
it is guaranteed to have exact group-fair outcomes (e.g., demographic parity,
equal opportunity)-in other words, it has no fairness-utility trade-off. We
formulate an optimization program for optimal steering by finding the nearest
ideal distribution in KL-divergence, and provide efficient algorithms for it
when the underlying distributions come from well-known parametric families
(e.g., normal, log-normal). Empirically, our optimal steering techniques on
both synthetic and real-world datasets improve fairness without diminishing
utility (and sometimes even improve utility). We demonstrate affine steering of
LLM representations to reduce bias in multi-class classification, e.g.,
occupation prediction from a short biography in Bios dataset (De-Arteaga et
al.). Furthermore, we steer internal representations of LLMs towards desired
outputs so that it works equally well across different groups.

</details>


### [109] [Learning to Optimize Capacity Planning in Semiconductor Manufacturing](https://arxiv.org/abs/2509.15767)
*Philipp Andelfinger,Jieyi Bi,Qiuyu Zhu,Jianan Zhou,Bo Zhang,Fei Fei Zhang,Chew Wye Chan,Boon Ping Gan,Wentong Cai,Jie Zhang*

Main category: cs.LG

TL;DR: 提出基于神经网络的单机器产能规划模型，经评估可提升吞吐量、降低周期时间。


<details>
  <summary>Details</summary>
Motivation: 当前半导体制造产能规划使用启发式规则难以处理流程中的复杂交互，易形成瓶颈。

Method: 用深度强化学习训练基于神经网络的模型，用异构图神经网络表示策略，采取措施实现可扩展性。

Result: 在最大测试场景中，训练的策略使吞吐量和周期时间各提升约1.8%。

Conclusion: 基于神经网络的产能规划模型在提升产能方面有一定效果。

Abstract: In manufacturing, capacity planning is the process of allocating production
resources in accordance with variable demand. The current industry practice in
semiconductor manufacturing typically applies heuristic rules to prioritize
actions, such as future change lists that account for incoming machine and
recipe dedications. However, while offering interpretability, heuristics cannot
easily account for the complex interactions along the process flow that can
gradually lead to the formation of bottlenecks. Here, we present a neural
network-based model for capacity planning on the level of individual machines,
trained using deep reinforcement learning. By representing the policy using a
heterogeneous graph neural network, the model directly captures the diverse
relationships among machines and processing steps, allowing for proactive
decision-making. We describe several measures taken to achieve sufficient
scalability to tackle the vast space of possible machine-level actions.
  Our evaluation results cover Intel's small-scale Minifab model and
preliminary experiments using the popular SMT2020 testbed. In the largest
tested scenario, our trained policy increases throughput and decreases cycle
time by about 1.8% each.

</details>


### [110] [Monte Carlo Tree Diffusion with Multiple Experts for Protein Design](https://arxiv.org/abs/2509.15796)
*Xuefeng Liu,Mingxuan Cao,Songhao Jiang,Xiao Luo,Xiaotian Duan,Mengdi Wang,Tobin R. Sosnick,Jinbo Xu,Rick Stevens*

Main category: cs.LG

TL;DR: 提出MCTD - ME方法用于蛋白质设计，在逆折叠任务上表现优于基线，框架具有通用性。


<details>
  <summary>Details</summary>
Motivation: 先前结合自回归语言模型与蒙特卡罗树搜索的方法在处理长程依赖和搜索空间过大方面存在困难，需要更好的蛋白质设计方法。

Method: 提出MCTD - ME，将掩码扩散模型与树搜索结合，使用增强生物物理保真度的扩散去噪作为滚动引擎，利用不同能力的专家进行探索，提出多专家选择规则PH - UCT - ME。

Result: 在逆折叠任务（CAMEO和PDB基准）中，MCTD - ME在序列恢复和结构相似性方面优于单专家和无引导基线，对更长蛋白质效果提升更明显。

Conclusion: MCTD - ME方法有效，框架具有模型无关性，可应用于蛋白质工程和多目标分子生成等领域。

Abstract: The goal of protein design is to generate amino acid sequences that fold into
functional structures with desired properties. Prior methods combining
autoregressive language models with Monte Carlo Tree Search (MCTS) struggle
with long-range dependencies and suffer from an impractically large search
space. We propose MCTD-ME, Monte Carlo Tree Diffusion with Multiple Experts,
which integrates masked diffusion models with tree search to enable multi-token
planning and efficient exploration. Unlike autoregressive planners, MCTD-ME
uses biophysical-fidelity-enhanced diffusion denoising as the rollout engine,
jointly revising multiple positions and scaling to large sequence spaces. It
further leverages experts of varying capacities to enrich exploration, guided
by a pLDDT-based masking schedule that targets low-confidence regions while
preserving reliable residues. We propose a novel multi-expert selection rule
(PH-UCT-ME) extends predictive-entropy UCT to expert ensembles. On the inverse
folding task (CAMEO and PDB benchmarks), MCTD-ME outperforms single-expert and
unguided baselines in both sequence recovery (AAR) and structural similarity
(scTM), with gains increasing for longer proteins and benefiting from
multi-expert guidance. More generally, the framework is model-agnostic and
applicable beyond inverse folding, including de novo protein engineering and
multi-objective molecular generation.

</details>


### [111] [On the Convergence of Muon and Beyond](https://arxiv.org/abs/2509.15816)
*Da Chang,Yongxiang Liu,Ganzhao Yuan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Muon optimizer has demonstrated remarkable empirical success in handling
matrix-structured parameters for training neural networks. However, a
significant gap persists between its practical performance and theoretical
understanding. Existing analyses indicate that the standard Muon variant
achieves only a suboptimal convergence rate of $\mathcal{O}(T^{-1/4})$ in
stochastic non-convex settings, where $T$ denotes the number of iterations. To
explore the theoretical limits of the Muon framework, we construct and analyze
a variance-reduced variant, termed Muon-VR2. We provide the first rigorous
proof that incorporating a variance-reduction mechanism enables Muon-VR2 to
attain an optimal convergence rate of $\tilde{\mathcal{O}}(T^{-1/3})$, thereby
matching the theoretical lower bound for this class of problems. Moreover, our
analysis establishes convergence guarantees for Muon variants under the
Polyak-{\L}ojasiewicz (P{\L}) condition. Extensive experiments on vision
(CIFAR-10) and language (C4) benchmarks corroborate our theoretical findings on
per-iteration convergence. Overall, this work provides the first proof of
optimality for a Muon-style optimizer and clarifies the path toward developing
more practically efficient, accelerated variants.

</details>


### [112] [SolarCrossFormer: Improving day-ahead Solar Irradiance Forecasting by Integrating Satellite Imagery and Ground Sensors](https://arxiv.org/abs/2509.15827)
*Baptiste Schubnel,Jelena Simeunović,Corentin Tissier,Pierre-Jean Alet,Rafael E. Carrillo*

Main category: cs.LG

TL;DR: 提出SolarCrossFormer模型结合卫星图像和气象站时间序列进行日辐照度预测，结果准确且具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前预报方案缺乏系统运营商所需的时间和空间分辨率，为大规模太阳能光伏系统并网提供准确的日辐照度预报。

Method: 引入SolarCrossFormer模型，使用新型图神经网络挖掘输入数据的模态间和模态内相关性。

Result: 在瑞士一年127个地点的数据集上，预测归一化平均绝对误差为6.1%，结果与商业数值天气预报服务相当。

Conclusion: SolarCrossFormer模型能提高预报准确性和分辨率，在实际操作中具有鲁棒性。

Abstract: Accurate day-ahead forecasts of solar irradiance are required for the
large-scale integration of solar photovoltaic (PV) systems into the power grid.
However, current forecasting solutions lack the temporal and spatial resolution
required by system operators. In this paper, we introduce SolarCrossFormer, a
novel deep learning model for day-ahead irradiance forecasting, that combines
satellite images and time series from a ground-based network of meteorological
stations. SolarCrossFormer uses novel graph neural networks to exploit the
inter- and intra-modal correlations of the input data and improve the accuracy
and resolution of the forecasts. It generates probabilistic forecasts for any
location in Switzerland with a 15-minute resolution for horizons up to 24 hours
ahead. One of the key advantages of SolarCrossFormer its robustness in real
life operations. It can incorporate new time-series data without retraining the
model and, additionally, it can produce forecasts for locations without input
data by using only their coordinates. Experimental results over a dataset of
one year and 127 locations across Switzerland show that SolarCrossFormer yield
a normalized mean absolute error of 6.1 % over the forecasting horizon. The
results are competitive with those achieved by a commercial numerical weather
prediction service.

</details>


### [113] [HyP-ASO: A Hybrid Policy-based Adaptive Search Optimization Framework for Large-Scale Integer Linear Programs](https://arxiv.org/abs/2509.15828)
*Ning Xu,Junkai Zhang,Yang Wu,Huigen Ye,Hua Xu,Huiling Xu,Yifan Zhang*

Main category: cs.LG

TL;DR: 本文提出HyP - ASO框架解决大规模整数线性规划问题，实验显示其优于现有基于LNS的方法，且轻量可扩展。


<details>
  <summary>Details</summary>
Motivation: 传统求解器解决大规模整数线性规划慢，现有基于LNS的框架生成有效邻域困难。

Method: 提出HyP - ASO框架，结合自定义公式和深度强化学习，公式利用可行解计算邻域生成中变量选择概率，RL策略网络预测邻域大小。

Result: HyP - ASO显著优于现有基于LNS的方法，且轻量、高可扩展性。

Conclusion: HyP - ASO适合解决大规模整数线性规划问题。

Abstract: Directly solving large-scale Integer Linear Programs (ILPs) using traditional
solvers is slow due to their NP-hard nature. While recent frameworks based on
Large Neighborhood Search (LNS) can accelerate the solving process, their
performance is often constrained by the difficulty in generating sufficiently
effective neighborhoods. To address this challenge, we propose HyP-ASO, a
hybrid policy-based adaptive search optimization framework that combines a
customized formula with deep Reinforcement Learning (RL). The formula leverages
feasible solutions to calculate the selection probabilities for each variable
in the neighborhood generation process, and the RL policy network predicts the
neighborhood size. Extensive experiments demonstrate that HyP-ASO significantly
outperforms existing LNS-based approaches for large-scale ILPs. Additional
experiments show it is lightweight and highly scalable, making it well-suited
for solving large-scale ILPs.

</details>


### [114] [Tsururu: A Python-based Time Series Forecasting Strategies Library](https://arxiv.org/abs/2509.15843)
*Alina Kostromina,Kseniia Kuvshinova,Aleksandr Yugay,Andrey Savchenko,Dmitry Simakov*

Main category: cs.LG

TL;DR: 介绍Python库Tsururu，可结合全局与多元方法及多步预测策略，能与多种预测模型集成，代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列研究侧重于开发新模型，而训练模型的最优方法选择问题未得到充分探索。

Method: 引入Python库Tsururu，实现全局与多元方法及多步预测策略的灵活组合，支持与多种预测模型集成。

Result: 开发出Tsururu库，可在https://github.com/sb - ai - lab/tsururu获取。

Conclusion: Tsururu库能架起学术研究与行业应用之间的桥梁。

Abstract: While current time series research focuses on developing new models, crucial
questions of selecting an optimal approach for training such models are
underexplored. Tsururu, a Python library introduced in this paper, bridges SoTA
research and industry by enabling flexible combinations of global and
multivariate approaches and multi-step-ahead forecasting strategies. It also
enables seamless integration with various forecasting models. Available at
https://github.com/sb-ai-lab/tsururu .

</details>


### [115] [EvoBrain: Dynamic Multi-channel EEG Graph Modeling for Time-evolving Brain Network](https://arxiv.org/abs/2509.15857)
*Rikuto Kotoge,Zheng Chen,Tasuku Kimura,Yasuko Matsubara,Takufumi Yanagisawa,Haruhiko Kishima,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 文章指出动态GNN在癫痫检测有潜力但存在挑战，提出EvoBrain模型，理论分析有优势，实验效果提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有动态GNN方法难以完全捕捉脑电数据潜在动态，存在基于静态图和联合建模不足的问题，需解决以提升癫痫检测效果。

Method: 进行理论分析，提出EvoBrain模型，集成双流Mamba架构与经拉普拉斯位置编码增强的GCN，纳入显式动态图结构。

Result: 相比动态GNN基线，AUROC提升23%，F1分数提升30%，并在早期癫痫预测任务上有广泛评估。

Conclusion: 显式动态建模和时间 - 图动态GNN方法有效且必要，EvoBrain模型在癫痫检测中有显著优势。

Abstract: Dynamic GNNs, which integrate temporal and spatial features in
Electroencephalography (EEG) data, have shown great potential in automating
seizure detection. However, fully capturing the underlying dynamics necessary
to represent brain states, such as seizure and non-seizure, remains a
non-trivial task and presents two fundamental challenges. First, most existing
dynamic GNN methods are built on temporally fixed static graphs, which fail to
reflect the evolving nature of brain connectivity during seizure progression.
Second, current efforts to jointly model temporal signals and graph structures
and, more importantly, their interactions remain nascent, often resulting in
inconsistent performance. To address these challenges, we present the first
theoretical analysis of these two problems, demonstrating the effectiveness and
necessity of explicit dynamic modeling and time-then-graph dynamic GNN method.
Building on these insights, we propose EvoBrain, a novel seizure detection
model that integrates a two-stream Mamba architecture with a GCN enhanced by
Laplacian Positional Encoding, following neurological insights. Moreover,
EvoBrain incorporates explicitly dynamic graph structures, allowing both nodes
and edges to evolve over time. Our contributions include (a) a theoretical
analysis proving the expressivity advantage of explicit dynamic modeling and
time-then-graph over other approaches, (b) a novel and efficient model that
significantly improves AUROC by 23% and F1 score by 30%, compared with the
dynamic GNN baseline, and (c) broad evaluations of our method on the
challenging early seizure prediction tasks.

</details>


### [116] [Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data](https://arxiv.org/abs/2509.15859)
*Nakul Sharma*

Main category: cs.LG

TL;DR: 提出利用视觉基础模型生成合成数据训练线性分类器的框架，在长尾分类上高效且效果好。


<details>
  <summary>Details</summary>
Motivation: 现有处理长尾分类的基础模型微调方法存在与平衡数据集训练网络有差距、计算资源需求大的问题，需提高计算效率和简化方法。

Method: 利用视觉基础模型的语义潜在空间生成合成数据，用真实和合成数据混合训练简单线性分类器。

Result: 在CIFAR - 100 - LT基准上达到新的最优，在Places - LT基准上表现出色。

Conclusion: 所提方法简单有效，具有适应性。

Abstract: Imbalanced classification datasets pose significant challenges in machine
learning, often leading to biased models that perform poorly on
underrepresented classes. With the rise of foundation models, recent research
has focused on the full, partial, and parameter-efficient fine-tuning of these
models to deal with long-tail classification. Despite the impressive
performance of these works on the benchmark datasets, they still fail to close
the gap with the networks trained using the balanced datasets and still require
substantial computational resources, even for relatively smaller datasets.
Underscoring the importance of computational efficiency and simplicity, in this
work we propose a novel framework that leverages the rich semantic latent space
of Vision Foundation Models to generate synthetic data and train a simple
linear classifier using a mixture of real and synthetic data for long-tail
classification. The computational efficiency gain arises from the number of
trainable parameters that are reduced to just the number of parameters in the
linear model. Our method sets a new state-of-the-art for the CIFAR-100-LT
benchmark and demonstrates strong performance on the Places-LT benchmark,
highlighting the effectiveness and adaptability of our simple and effective
approach.

</details>


### [117] [SAGE: Semantic-Aware Shared Sampling for Efficient Diffusion](https://arxiv.org/abs/2509.15865)
*Haoran Zhao,Tong Bai,Lei Huang,Xiaoyu Liang*

Main category: cs.LG

TL;DR: 提出SAGE框架，通过共享早期采样步骤降低扩散模型采样成本，实验显示能减少成本并提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽有优势，但采样成本高，先前方法独立处理查询，本文旨在通过共享早期采样步骤降低成本。

Method: 提出SAGE语义感知共享采样框架，包含共享采样方案和定制训练策略。

Result: SAGE减少25.5%采样成本，FID降低5.0%，CLIP提高5.4%，多样性提高160%。

Conclusion: SAGE框架在降低采样成本的同时提升了生成质量。

Abstract: Diffusion models manifest evident benefits across diverse domains, yet their
high sampling cost, requiring dozens of sequential model evaluations, remains a
major limitation. Prior efforts mainly accelerate sampling via optimized
solvers or distillation, which treat each query independently. In contrast, we
reduce total number of steps by sharing early-stage sampling across
semantically similar queries. To enable such efficiency gains without
sacrificing quality, we propose SAGE, a semantic-aware shared sampling
framework that integrates a shared sampling scheme for efficiency and a
tailored training strategy for quality preservation. Extensive experiments show
that SAGE reduces sampling cost by 25.5%, while improving generation quality
with 5.0% lower FID, 5.4% higher CLIP, and 160% higher diversity over
baselines.

</details>


### [118] [From Data to Diagnosis: A Large, Comprehensive Bone Marrow Dataset and AI Methods for Childhood Leukemia Prediction](https://arxiv.org/abs/2509.15895)
*Henning Höfener,Farina Kock,Martina Pontones,Tabita Ghete,David Pfrang,Nicholas Dickel,Meik Kunz,Daniela P. Schacherer,David A. Clunie,Andrey Fedorov,Max Westphal,Markus Metzler*

Main category: cs.LG

TL;DR: 提出公开白血病骨髓数据集并构建相关AI模型，评估显示模型有效，数据集利于领域研究。


<details>
  <summary>Details</summary>
Motivation: 白血病诊断复杂耗时，现有AI方案多使用私有数据集且仅覆盖部分诊断流程。

Method: 提出涵盖整个诊断过程的公开白血病骨髓数据集，并基于此提出细胞检测、分类和诊断预测方法。

Result: 细胞检测平均精度0.96，33类细胞分类曲线下面积0.98、F1分数0.61，使用预测细胞计数进行诊断预测平均F1分数0.90。

Conclusion: 提出的方法对AI辅助诊断有用，数据集能促进领域研发，有助于更精准诊断和改善患者预后。

Abstract: Leukemia diagnosis primarily relies on manual microscopic analysis of bone
marrow morphology supported by additional laboratory parameters, making it
complex and time consuming. While artificial intelligence (AI) solutions have
been proposed, most utilize private datasets and only cover parts of the
diagnostic pipeline. Therefore, we present a large, high-quality, publicly
available leukemia bone marrow dataset spanning the entire diagnostic process,
from cell detection to diagnosis. Using this dataset, we further propose
methods for cell detection, cell classification, and diagnosis prediction. The
dataset comprises 246 pediatric patients with diagnostic, clinical and
laboratory information, over 40 000 cells with bounding box annotations and
more than 28 000 of these with high-quality class labels, making it the most
comprehensive dataset publicly available. Evaluation of the AI models yielded
an average precision of 0.96 for the cell detection, an area under the curve of
0.98, and an F1-score of 0.61 for the 33-class cell classification, and a mean
F1-score of 0.90 for the diagnosis prediction using predicted cell counts.
While the proposed approaches demonstrate their usefulness for AI-assisted
diagnostics, the dataset will foster further research and development in the
field, ultimately contributing to more precise diagnoses and improved patient
outcomes.

</details>


### [119] [Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds](https://arxiv.org/abs/2509.15915)
*Remo Sasso,Michelangelo Conserva,Dominik Jeurissen,Paulo Rauber*

Main category: cs.LG

TL;DR: 文章探讨将基础模型集成到强化学习框架以提高样本效率，评估两种策略，在网格世界环境中实验并得出相关结论。


<details>
  <summary>Details</summary>
Motivation: 在真实世界应用中，强化学习需要更具样本效率的智能体，基础模型有潜力但如何集成到强化学习框架尚不明确。

Method: 考虑使用基础世界模型（FWMs）利用基础模型先验知识进行模拟交互训练评估智能体，以及基础智能体（FAs）利用基础模型推理能力进行决策，并在网格世界环境中进行实证评估。

Result: 大语言模型（LLMs）的改进能提升FWMs和FAs；当前LLMs的FAs能为简单环境提供优秀策略；FWMs与强化学习智能体的结合在复杂环境中有潜力。

Conclusion: 所评估的两种策略在不同环境下均有一定效果，为基础模型集成到强化学习框架提供了思路。

Abstract: While reinforcement learning from scratch has shown impressive results in
solving sequential decision-making tasks with efficient simulators, real-world
applications with expensive interactions require more sample-efficient agents.
Foundation models (FMs) are natural candidates to improve sample efficiency as
they possess broad knowledge and reasoning capabilities, but it is yet unclear
how to effectively integrate them into the reinforcement learning framework. In
this paper, we anticipate and, most importantly, evaluate two promising
strategies. First, we consider the use of foundation world models (FWMs) that
exploit the prior knowledge of FMs to enable training and evaluating agents
with simulated interactions. Second, we consider the use of foundation agents
(FAs) that exploit the reasoning capabilities of FMs for decision-making. We
evaluate both approaches empirically in a family of grid-world environments
that are suitable for the current generation of large language models (LLMs).
Our results suggest that improvements in LLMs already translate into better
FWMs and FAs; that FAs based on current LLMs can already provide excellent
policies for sufficiently simple environments; and that the coupling of FWMs
and reinforcement learning agents is highly promising for more complex settings
with partial observability and stochastic elements.

</details>


### [120] [Enhancing Generative Auto-bidding with Offline Reward Evaluation and Policy Search](https://arxiv.org/abs/2509.15927)
*Zhiyu Mou,Yiqin Lv,Miao Xu,Cheems Wang,Yixiu Mao,Qichen Ye,Chao Li,Rongquan Bai,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: 本文提出AIGB - Pearl方法解决现有AI - Generated Bidding (AIGB)方法性能瓶颈问题，实验验证其达到了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有AIGB方法因忽视细粒度生成质量评估和无法超越静态数据集探索而遭遇性能瓶颈。

Method: 提出AIGB - Pearl方法，构建非自举的轨迹评估器分配奖励并引导策略搜索，还结合LLM架构、混合损失和专家反馈三种技术提高评估器准确性。

Result: 在模拟和真实广告系统上的大量实验表明AIGB - Pearl方法达到了最优性能。

Conclusion: AIGB - Pearl方法能有效解决现有AIGB方法的性能瓶颈，具有良好的效果。

Abstract: Auto-bidding is an essential tool for advertisers to enhance their
advertising performance. Recent progress has shown that AI-Generated Bidding
(AIGB), which formulates the auto-bidding as a trajectory generation task and
trains a conditional diffusion-based planner on offline data, achieves superior
and stable performance compared to typical offline reinforcement learning
(RL)-based auto-bidding methods. However, existing AIGB methods still encounter
a performance bottleneck due to their neglect of fine-grained generation
quality evaluation and inability to explore beyond static datasets. To address
this, we propose AIGB-Pearl (\emph{Planning with EvAluator via RL}), a novel
method that integrates generative planning and policy optimization. The key to
AIGB-Pearl is to construct a non-bootstrapped \emph{trajectory evaluator} to
assign rewards and guide policy search, enabling the planner to optimize its
generation quality iteratively through interaction. Furthermore, to enhance
trajectory evaluator accuracy in offline settings, we incorporate three key
techniques: (i) a Large Language Model (LLM)-based architecture for better
representational capacity, (ii) hybrid point-wise and pair-wise losses for
better score learning, and (iii) adaptive integration of expert feedback for
better generalization ability. Extensive experiments on both simulated and
real-world advertising systems demonstrate the state-of-the-art performance of
our approach.

</details>


### [121] [Improving Monte Carlo Tree Search for Symbolic Regression](https://arxiv.org/abs/2509.15929)
*Zhengyao Huang,Daniel Zhengyu Huang,Tiannan Xiao,Dina Ma,Zhenyu Ming,Hao Shi,Yuanhui Wen*

Main category: cs.LG

TL;DR: 提出改进的MCTS框架用于符号回归，有两项关键创新，经实验表现有竞争力，代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统MCTS在符号回归中的传统策略和构造方式限制性能，需改进以提高搜索效率。

Method: 提出改进的MCTS框架，包括极端老虎机分配策略和受进化启发的状态跳跃动作。

Result: 在恢复率上与现有库表现相当，在准确性与模型复杂度的帕累托前沿上取得有利位置。

Conclusion: 改进的MCTS框架有效提升了符号回归的性能和效率。

Abstract: Symbolic regression aims to discover concise, interpretable mathematical
expressions that satisfy desired objectives, such as fitting data, posing a
highly combinatorial optimization problem. While genetic programming has been
the dominant approach, recent efforts have explored reinforcement learning
methods for improving search efficiency. Monte Carlo Tree Search (MCTS), with
its ability to balance exploration and exploitation through guided search, has
emerged as a promising technique for symbolic expression discovery. However,
its traditional bandit strategies and sequential symbol construction often
limit performance. In this work, we propose an improved MCTS framework for
symbolic regression that addresses these limitations through two key
innovations: (1) an extreme bandit allocation strategy tailored for identifying
globally optimal expressions, with finite-time performance guarantees under
polynomial reward decay assumptions; and (2) evolution-inspired state-jumping
actions such as mutation and crossover, which enable non-local transitions to
promising regions of the search space. These state-jumping actions also reshape
the reward landscape during the search process, improving both robustness and
efficiency. We conduct a thorough numerical study to the impact of these
improvements and benchmark our approach against existing symbolic regression
methods on a variety of datasets, including both ground-truth and black-box
datasets. Our approach achieves competitive performance with state-of-the-art
libraries in terms of recovery rate, attains favorable positions on the Pareto
frontier of accuracy versus model complexity. Code is available at
https://github.com/PKU-CMEGroup/MCTS-4-SR.

</details>


### [122] [Bayesian Physics Informed Neural Networks for Reliable Transformer Prognostics](https://arxiv.org/abs/2509.15933)
*Ibai Ramirez,Jokin Alcibar,Joel Pino,Mikel Sanz,David Pardo,Jose I. Aizpurua*

Main category: cs.LG

TL;DR: 本文提出用于概率预测估计的B - PINN框架，以变压器老化为例验证，结果显示该框架能更可靠地进行预测，为关键电力资产维护决策提供支持。


<details>
  <summary>Details</summary>
Motivation: SciML在预测领域应用有限，原因包括纳入偏微分方程的复杂性和缺乏可靠的不确定性量化方法。

Method: 将贝叶斯神经网络嵌入PINN架构，以变压器老化为例，用热扩散PDE作为物理残差，研究不同先验分布的影响。

Result: 与dropout - PINN基线对比，B - PINN能准确量化预测不确定性，给出更可靠的预测。

Conclusion: B - PINN框架在关键电力资产维护决策中具有重要价值。

Abstract: Scientific Machine Learning (SciML) integrates physics and data into the
learning process, offering improved generalization compared with purely
data-driven models. Despite its potential, applications of SciML in prognostics
remain limited, partly due to the complexity of incorporating partial
differential equations (PDEs) for ageing physics and the scarcity of robust
uncertainty quantification methods. This work introduces a Bayesian
Physics-Informed Neural Network (B-PINN) framework for probabilistic
prognostics estimation. By embedding Bayesian Neural Networks into the PINN
architecture, the proposed approach produces principled, uncertainty-aware
predictions. The method is applied to a transformer ageing case study, where
insulation degradation is primarily driven by thermal stress. The heat
diffusion PDE is used as the physical residual, and different prior
distributions are investigated to examine their impact on predictive posterior
distributions and their ability to encode a priori physical knowledge. The
framework is validated against a finite element model developed and tested with
real measurements from a solar power plant. Results, benchmarked against a
dropout-PINN baseline, show that the proposed B-PINN delivers more reliable
prognostic predictions by accurately quantifying predictive uncertainty. This
capability is crucial for supporting robust and informed maintenance
decision-making in critical power assets.

</details>


### [123] [UniTac2Pose: A Unified Approach Learned in Simulation for Category-level Visuotactile In-hand Pose Estimation](https://arxiv.org/abs/2509.15934)
*Mingdong Wu,Long Yang,Jin Liu,Weiyao Huang,Lehong Wu,Zelin Chen,Daolin Ma,Hao Dong*

Main category: cs.LG

TL;DR: 提出用于手持物体姿态估计的三阶段框架，由基于能量的扩散模型控制，结合渲染比较架构，实验显示优于传统方法，有强泛化性且能统一多种估计。


<details>
  <summary>Details</summary>
Motivation: 现有物体手持姿态估计方法在高精度和对未见CAD模型的泛化性上有挑战。

Method: 提出三阶段框架，包括采样预排序、迭代细化、后排序，由基于能量的扩散模型控制，结合渲染比较架构。

Result: 方法优于基于回归、匹配和注册技术的传统基线，对未见CAD模型有强泛化性。

Conclusion: 该方法能将触觉物体姿态估计、姿态跟踪和不确定性估计集成到统一框架，在多种现实条件下有稳健表现。

Abstract: Accurate estimation of the in-hand pose of an object based on its CAD model
is crucial in both industrial applications and everyday tasks, ranging from
positioning workpieces and assembling components to seamlessly inserting
devices like USB connectors. While existing methods often rely on regression,
feature matching, or registration techniques, achieving high precision and
generalizability to unseen CAD models remains a significant challenge. In this
paper, we propose a novel three-stage framework for in-hand pose estimation.
The first stage involves sampling and pre-ranking pose candidates, followed by
iterative refinement of these candidates in the second stage. In the final
stage, post-ranking is applied to identify the most likely pose candidates.
These stages are governed by a unified energy-based diffusion model, which is
trained solely on simulated data. This energy model simultaneously generates
gradients to refine pose estimates and produces an energy scalar that
quantifies the quality of the pose estimates. Additionally, borrowing the idea
from the computer vision domain, we incorporate a render-compare architecture
within the energy-based score network to significantly enhance sim-to-real
performance, as demonstrated by our ablation studies. We conduct comprehensive
experiments to show that our method outperforms conventional baselines based on
regression, matching, and registration techniques, while also exhibiting strong
intra-category generalization to previously unseen CAD models. Moreover, our
approach integrates tactile object pose estimation, pose tracking, and
uncertainty estimation into a unified framework, enabling robust performance
across a variety of real-world conditions.

</details>


### [124] [Targeted Fine-Tuning of DNN-Based Receivers via Influence Functions](https://arxiv.org/abs/2509.15950)
*Marko Tuononen,Heikki Penttinen,Ville Hautamäki*

Main category: cs.LG

TL;DR: 首次将影响函数用于深度学习无线接收器，可实现针对性微调，结果表明特定策略能提升性能，建立了影响函数的作用。


<details>
  <summary>Details</summary>
Motivation: 为深度学习无线接收器提供可解释性和高效的自适应方法。

Method: 将影响分析应用于全卷积接收器DeepRx，采用损失相对影响和一阶更新策略，还提出二阶影响对齐更新策略。

Result: 损失相对影响和一阶更新在单目标场景中能持续改善误码率，多目标适应效果不佳。

Conclusion: 影响函数可作为可解释性工具和接收器高效自适应的基础。

Abstract: We present the first use of influence functions for deep learning-based
wireless receivers. Applied to DeepRx, a fully convolutional receiver,
influence analysis reveals which training samples drive bit predictions,
enabling targeted fine-tuning of poorly performing cases. We show that
loss-relative influence with capacity-like binary cross-entropy loss and
first-order updates on beneficial samples most consistently improves bit error
rate toward genie-aided performance, outperforming random fine-tuning in
single-target scenarios. Multi-target adaptation proved less effective,
underscoring open challenges. Beyond experiments, we connect influence to
self-influence corrections and propose a second-order, influence-aligned update
strategy. Our results establish influence functions as both an interpretability
tool and a basis for efficient receiver adaptation.

</details>


### [125] [Adversarial Graph Fusion for Incomplete Multi-view Semi-supervised Learning with Tensorial Imputation](https://arxiv.org/abs/2509.15955)
*Zhangqi Jiang,Tingjin Luo,Xu Yang,Xinyan Liang*

Main category: cs.LG

TL;DR: 文章指出图基多视图半监督学习中视图缺失问题带来子簇问题（SCP），提出AGF - TI方法缓解SCP，实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决图基多视图半监督学习中视图缺失问题，缓解因缺失样本导致的子簇问题对图融合和分类性能的影响。

Method: 提出AGF - TI方法，设计对抗图融合方案学习鲁棒共识图，基于低秩张量学习从高阶一致性信息恢复不完整结构，引入基于锚点策略降低计算复杂度，用结合降梯度下降法的交替优化算法求解目标。

Result: 在各种数据集上的大量实验结果验证了AGF - TI相比现有方法的优越性。

Conclusion: 提出的AGF - TI方法能有效缓解视图缺失带来的子簇问题，提升多视图半监督学习的性能。

Abstract: View missing remains a significant challenge in graph-based multi-view
semi-supervised learning, hindering their real-world applications. To address
this issue, traditional methods introduce a missing indicator matrix and focus
on mining partial structure among existing samples in each view for label
propagation (LP). However, we argue that these disregarded missing samples
sometimes induce discontinuous local structures, i.e., sub-clusters, breaking
the fundamental smoothness assumption in LP. Consequently, such a Sub-Cluster
Problem (SCP) would distort graph fusion and degrade classification
performance. To alleviate SCP, we propose a novel incomplete multi-view
semi-supervised learning method, termed AGF-TI. Firstly, we design an
adversarial graph fusion scheme to learn a robust consensus graph against the
distorted local structure through a min-max framework. By stacking all
similarity matrices into a tensor, we further recover the incomplete structure
from the high-order consistency information based on the low-rank tensor
learning. Additionally, the anchor-based strategy is incorporated to reduce the
computational complexity. An efficient alternative optimization algorithm
combining a reduced gradient descent method is developed to solve the
formulated objective, with theoretical convergence. Extensive experimental
results on various datasets validate the superiority of our proposed AGF-TI as
compared to state-of-the-art methods. Code is available at
https://github.com/ZhangqiJiang07/AGF_TI.

</details>


### [126] [EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-grained Emotions](https://arxiv.org/abs/2509.15986)
*Xinchen Wan,Jinhua Liang,Huan Zhang*

Main category: cs.LG

TL;DR: 现有数字心理健康工具忽视细微情绪状态，本文提出EmoHeal系统，经研究证明有显著支持效果，验证细粒度方法可行性。


<details>
  <summary>Details</summary>
Motivation: 现有数字心理健康工具大多为“一刀切”模式，未适应个体需求，忽视日常挑战下的细微情绪状态。

Method: 提出EmoHeal系统，用微调的XLM - RoBERTa模型检测27种细粒度情绪，通过基于音乐治疗原则的知识图谱将情绪映射到音乐参数，用CLAMP3模型检索视听内容。

Result: 一项40人参与的实验表明有显著支持效果，参与者情绪显著改善，感知的情绪识别准确性高，且两者强相关。

Conclusion: 证明了理论驱动、情绪感知的数字健康工具的可行性，为实施音乐治疗原则提供可扩展的AI蓝图。

Abstract: Existing digital mental wellness tools often overlook the nuanced emotional
states underlying everyday challenges. For example, pre-sleep anxiety affects
more than 1.5 billion people worldwide, yet current approaches remain largely
static and "one-size-fits-all", failing to adapt to individual needs. In this
work, we present EmoHeal, an end-to-end system that delivers personalized,
three-stage supportive narratives. EmoHeal detects 27 fine-grained emotions
from user text with a fine-tuned XLM-RoBERTa model, mapping them to musical
parameters via a knowledge graph grounded in music therapy principles (GEMS,
iso-principle). EmoHeal retrieves audiovisual content using the CLAMP3 model to
guide users from their current state toward a calmer one
("match-guide-target"). A within-subjects study (N=40) demonstrated significant
supportive effects, with participants reporting substantial mood improvement
(M=4.12, p<0.001) and high perceived emotion recognition accuracy (M=4.05,
p<0.001). A strong correlation between perceived accuracy and therapeutic
outcome (r=0.72, p<0.001) validates our fine-grained approach. These findings
establish the viability of theory-driven, emotion-aware digital wellness tools
and provides a scalable AI blueprint for operationalizing music therapy
principles.

</details>


### [127] [Inverse Optimization Latent Variable Models for Learning Costs Applied to Route Problems](https://arxiv.org/abs/2509.15999)
*Alan A. Lahoud,Erik Schaffernicht,Johannes A. Stork*

Main category: cs.LG

TL;DR: 提出Inverse Optimization Latent Variable Model (IO - LVM)学习约束优化问题成本函数的潜在空间，在多个数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 学习未知成本函数的约束优化问题解决方案的表示具有挑战性，现有模型在解码结构化输出时难以执行约束。

Method: 提出IO - LVM，通过非可微确定性求解器估计Fenchel - Young损失的梯度来塑造潜在空间。

Result: 在船舶和出租车路线的真实数据集以及合成图的路径上进行验证，能重建路径和循环、预测分布并产生可解释的潜在表示。

Conclusion: IO - LVM能捕获成本函数的分布，识别不同代理或条件下的多样化解决方案行为。

Abstract: Learning representations for solutions of constrained optimization problems
(COPs) with unknown cost functions is challenging, as models like (Variational)
Autoencoders struggle to enforce constraints when decoding structured outputs.
We propose an Inverse Optimization Latent Variable Model (IO-LVM) that learns a
latent space of COP cost functions from observed solutions and reconstructs
feasible outputs by solving a COP with a solver in the loop. Our approach
leverages estimated gradients of a Fenchel-Young loss through a
non-differentiable deterministic solver to shape the latent space. Unlike
standard Inverse Optimization or Inverse Reinforcement Learning methods, which
typically recover a single or context-specific cost function, IO-LVM captures a
distribution over cost functions, enabling the identification of diverse
solution behaviors arising from different agents or conditions not available
during the training process. We validate our method on real-world datasets of
ship and taxi routes, as well as paths in synthetic graphs, demonstrating its
ability to reconstruct paths and cycles, predict their distributions, and yield
interpretable latent representations.

</details>


### [128] [Predicting the descent into extremism and terrorism](https://arxiv.org/abs/2509.16014)
*R. O. Lane,W. J. Holmes,C. J. Taylor,H. M. State-Davey,A. J. Wragge*

Main category: cs.LG

TL;DR: 本文提出自动分析和跟踪网络声明、检测作者是否涉极端主义或恐怖主义的方法，经测试有较高准确率，跟踪算法能检测态度变化。


<details>
  <summary>Details</summary>
Motivation: 自动分析网络声明，检测作者是否涉及极端主义或恐怖主义。

Method: 提出包含在线整理、编码、机器学习分类、跟踪和可视化的系统，用Universal Sentence Encoder提取特征，用支持向量机分类器和10折交叉验证进行训练和测试。

Result: 系统用839条引语数据集，检测极端主义意图和态度准确率81%，检测恐怖主义准确率97%，高于基于n - gram文本特征的基线系统，跟踪算法能检测态度随时间的趋势和突变。

Conclusion: 所提系统在检测极端主义和恐怖主义相关声明方面有效，跟踪算法能对数据进行时间分析。

Abstract: This paper proposes an approach for automatically analysing and tracking
statements in material gathered online and detecting whether the authors of the
statements are likely to be involved in extremism or terrorism. The proposed
system comprises: online collation of statements that are then encoded in a
form amenable to machine learning (ML), an ML component to classify the encoded
text, a tracker, and a visualisation system for analysis of results. The
detection and tracking concept has been tested using quotes made by terrorists,
extremists, campaigners, and politicians, obtained from wikiquote.org. A set of
features was extracted for each quote using the state-of-the-art Universal
Sentence Encoder (Cer et al. 2018), which produces 512-dimensional vectors. The
data were used to train and test a support vector machine (SVM) classifier
using 10-fold cross-validation. The system was able to correctly detect
intentions and attitudes associated with extremism 81% of the time and
terrorism 97% of the time, using a dataset of 839 quotes. This accuracy was
higher than that which was achieved for a simple baseline system based on
n-gram text features. Tracking techniques were also used to perform a temporal
analysis of the data, with each quote considered to be a noisy measurement of a
person's state of mind. It was demonstrated that the tracking algorithms were
able to detect both trends over time and sharp changes in attitude that could
be attributed to major events.

</details>


### [129] [Time-adaptive SympNets for separable Hamiltonian systems](https://arxiv.org/abs/2509.16026)
*Konrad Janik,Peter Benner*

Main category: cs.LG

TL;DR: 本文针对不规则采样数据，改进TSympNets用于非自治哈密顿系统，给出可分离哈密顿系统的通用逼近定理，指出不可扩展到非可分离系统，还修正了一个定理证明错误。


<details>
  <summary>Details</summary>
Motivation: 现有学习辛积分器的机器学习方法需固定步长生成的训练数据，TSympNets对非自治哈密顿系统的逼近质量未知。

Method: 调整TSympNets架构使其适用于非自治哈密顿系统，给出可分离哈密顿系统的通用逼近定理，进行数值实验，修正定理证明错误。

Result: 得到可分离哈密顿系统的通用逼近定理，发现无法将其扩展到非可分离哈密顿系统。

Conclusion: 所提方法有助于学习时间自适应辛积分器，解决了TSympNets逼近质量未知的问题，修正的定理证明利于辛机器学习方法。

Abstract: Measurement data is often sampled irregularly i.e. not on equidistant time
grids. This is also true for Hamiltonian systems. However, existing machine
learning methods, which learn symplectic integrators, such as SympNets [20] and
H\'enonNets [4] still require training data generated by fixed step sizes. To
learn time-adaptive symplectic integrators, an extension to SympNets, which we
call TSympNets, was introduced in [20]. We adapt the architecture of TSympNets
and extend them to non-autonomous Hamiltonian systems. So far the approximation
qualities of TSympNets were unknown. We close this gap by providing a universal
approximation theorem for separable Hamiltonian systems and show that it is not
possible to extend it to non-separable Hamiltonian systems. To investigate
these theoretical approximation capabilities, we perform different numerical
experiments. Furthermore we fix a mistake in a proof of a substantial theorem
[25, Theorem 2] for the approximation of symplectic maps in general, but
specifically for symplectic machine learning methods.

</details>


### [130] [SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection](https://arxiv.org/abs/2509.16060)
*Maithili Joshi,Palash Nandi,Tanmoy Chakraborty*

Main category: cs.LG

TL;DR: 研究发现大语言模型安全机制多在中后层，提出白盒越狱方法SABER，在HarmBench测试集上有51%提升，代码开源。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型经过安全对齐训练，但仍易受越狱攻击，需研究突破其安全机制的方法。

Method: 发现安全机制主要在中后层，提出SABER方法，通过残差连接中间两层。

Result: 在HarmBench测试集上比最佳基线提升51%，在验证集上困惑度仅有微小变化。

Conclusion: SABER方法能有效突破大语言模型安全机制，且对模型性能影响小。

Abstract: Large Language Models (LLMs) with safe-alignment training are powerful
instruments with robust language comprehension capabilities. These models
typically undergo meticulous alignment procedures involving human feedback to
ensure the acceptance of safe inputs while rejecting harmful or unsafe ones.
However, despite their massive scale and alignment efforts, LLMs remain
vulnerable to jailbreak attacks, where malicious users manipulate the model to
produce harmful outputs that it was explicitly trained to avoid. In this study,
we find that the safety mechanisms in LLMs are predominantly embedded in the
middle-to-late layers. Building on this insight, we introduce a novel white-box
jailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which
connects two intermediate layers $s$ and $e$ such that $s < e$, through a
residual connection. Our approach achieves a 51% improvement over the
best-performing baseline on the HarmBench test set. Furthermore, SABER induces
only a marginal shift in perplexity when evaluated on the HarmBench validation
set. The source code is publicly available at
https://github.com/PalGitts/SABER.

</details>


### [131] [Communications to Circulations: 3D Wind Field Retrieval and Real-Time Prediction Using 5G GNSS Signals and Deep Learning](https://arxiv.org/abs/2509.16068)
*Yuchen Ye,Hong Liang,Chaoxia Yuan,Mingyu Li,Aoqi Zhou,Chunqing Shang,Hua Cai,Peixi Liu,Kezuan Wang,Yifeng Zheng*

Main category: cs.LG

TL;DR: 本文介绍了G - WindCast深度学习框架，利用5G GNSS信号强度变化来获取和预测3D大气风场，初步结果显示在风场反演和短期预报中有良好表现，具有成本效益和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 精确大气风场信息对诸多应用至关重要，但传统方法获取高时空分辨率风数据存在挑战，因此需新方法。

Method: 引入G - WindCast框架，利用前馈神经网络（FNN）和Transformer网络捕捉GNSS特征与风动力学的复杂关系。

Result: 在风场反演和短期（最长30分钟）风预报中有不错准确性，在某些场景下技能得分与高分辨率数值天气预报输出相当，对风速和风向的预测与观测更吻合，减少GNSS站数量仍能保持良好性能。

Conclusion: 利用非传统数据源和深度学习的跨学科方法在环境监测和实时大气应用中有变革潜力。

Abstract: Accurate atmospheric wind field information is crucial for various
applications, including weather forecasting, aviation safety, and disaster risk
reduction. However, obtaining high spatiotemporal resolution wind data remains
challenging due to limitations in traditional in-situ observations and remote
sensing techniques, as well as the computational expense and biases of
numerical weather prediction (NWP) models. This paper introduces G-WindCast, a
novel deep learning framework that leverages signal strength variations from 5G
Global Navigation Satellite System (GNSS) signals to retrieve and forecast
three-dimensional (3D) atmospheric wind fields. The framework utilizes Forward
Neural Networks (FNN) and Transformer networks to capture complex, nonlinear,
and spatiotemporal relationships between GNSS-derived features and wind
dynamics. Our preliminary results demonstrate promising accuracy in both wind
retrieval and short-term wind forecasting (up to 30 minutes lead time), with
skill scores comparable to high-resolution NWP outputs in certain scenarios.
The model exhibits robustness across different forecast horizons and pressure
levels, and its predictions for wind speed and direction show superior
agreement with observations compared to concurrent ERA5 reanalysis data.
Furthermore, we show that the system can maintain excellent performance for
localized forecasting even with a significantly reduced number of GNSS stations
(e.g., around 100), highlighting its cost-effectiveness and scalability. This
interdisciplinary approach underscores the transformative potential of
exploiting non-traditional data sources and deep learning for advanced
environmental monitoring and real-time atmospheric applications.

</details>


### [132] [MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning](https://arxiv.org/abs/2509.16078)
*Yi Xu,Yitian Zhang,Yun Fu*

Main category: cs.LG

TL;DR: 提出用于无监督多变量时间序列表示学习的Dual - Masked Autoencoder (DMAE)框架，在多个下游任务上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 无监督多变量时间序列表示学习旨在不依赖标签从原始序列中提取有效表示，以迁移到下游任务。

Method: 提出DMAE框架，设计两个互补预训练任务，引入特征级对齐约束，联合优化目标。

Result: 在分类、回归和预测任务的综合评估中，该方法表现优于竞争基线。

Conclusion: DMAE能学习到时间连贯且语义丰富的表示，在下游任务中表现出色。

Abstract: Unsupervised multivariate time series (MTS) representation learning aims to
extract compact and informative representations from raw sequences without
relying on labels, enabling efficient transfer to diverse downstream tasks. In
this paper, we propose Dual-Masked Autoencoder (DMAE), a novel masked
time-series modeling framework for unsupervised MTS representation learning.
DMAE formulates two complementary pretext tasks: (1) reconstructing masked
values based on visible attributes, and (2) estimating latent representations
of masked features, guided by a teacher encoder. To further improve
representation quality, we introduce a feature-level alignment constraint that
encourages the predicted latent representations to align with the teacher's
outputs. By jointly optimizing these objectives, DMAE learns temporally
coherent and semantically rich representations. Comprehensive evaluations
across classification, regression, and forecasting tasks demonstrate that our
approach achieves consistent and superior performance over competitive
baselines.

</details>


### [133] [Rethinking Molecule Synthesizability with Chain-of-Reaction](https://arxiv.org/abs/2509.16084)
*Seul Lee,Karsten Kreis,Srimukh Prasad Veccham,Meng Liu,Danny Reidenbach,Saee Paliwal,Weili Nie,Arash Vahdat*

Main category: cs.LG

TL;DR: 本文提出ReaSyn生成框架解决分子生成模型生成可合成分子的问题，在相关任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有分子生成模型不能保证生成可合成分子，且现有解决方法在可合成分子组合空间的覆盖度和分子优化性能有限。

Method: 引入ReaSyn框架，采用链反应（CoR）表示法获取密集监督，还提出基于强化学习的微调及面向目标的测试时计算缩放。

Result: ReaSyn在可合成分子重建中实现了最高重建率和路径多样性，在可合成目标导向分子优化中优化性能最高，在可合成命中扩展中显著优于先前方法。

Conclusion: ReaSyn在组合庞大的可合成化学空间中具有卓越的导航能力。

Abstract: A well-known pitfall of molecular generative models is that they are not
guaranteed to generate synthesizable molecules. There have been considerable
attempts to address this problem, but given the exponentially large
combinatorial space of synthesizable molecules, existing methods have shown
limited coverage of the space and poor molecular optimization performance. To
tackle these problems, we introduce ReaSyn, a generative framework for
synthesizable projection where the model explores the neighborhood of given
molecules in the synthesizable space by generating pathways that result in
synthesizable analogs. To fully utilize the chemical knowledge contained in the
synthetic pathways, we propose a novel perspective that views synthetic
pathways akin to reasoning paths in large language models (LLMs). Specifically,
inspired by chain-of-thought (CoT) reasoning in LLMs, we introduce the
chain-of-reaction (CoR) notation that explicitly states reactants, reaction
types, and intermediate products for each step in a pathway. With the CoR
notation, ReaSyn can get dense supervision in every reaction step to explicitly
learn chemical reaction rules during supervised training and perform
step-by-step reasoning. In addition, to further enhance the reasoning
capability of ReaSyn, we propose reinforcement learning (RL)-based finetuning
and goal-directed test-time compute scaling tailored for synthesizable
projection. ReaSyn achieves the highest reconstruction rate and pathway
diversity in synthesizable molecule reconstruction and the highest optimization
performance in synthesizable goal-directed molecular optimization, and
significantly outperforms previous synthesizable projection methods in
synthesizable hit expansion. These results highlight ReaSyn's superior ability
to navigate combinatorially-large synthesizable chemical space.

</details>


### [134] [Randomized Smoothing Meets Vision-Language Models](https://arxiv.org/abs/2509.16088)
*Emmanouil Seferis,Changshun Wu,Stefanos Kollias,Saddek Bensalem,Chih-Hong Cheng*

Main category: cs.LG

TL;DR: 本文将随机平滑（RS）应用于生成模型，通过关联生成输出与神谕分类任务实现鲁棒性认证，推导了样本数量与鲁棒半径关系及缩放定律，验证了对先进VLMs的有效性。


<details>
  <summary>Details</summary>
Motivation: 随机平滑在分类任务中已被很好理解，但在生成模型中的应用尚不明确，需要解决该问题。

Method: 将生成输出与神谕分类任务关联，在神谕分类器比较的误差率有界的条件下，发展样本数量与鲁棒半径关联的理论，推导缩放定律。

Result: 证明了在较弱假设下，早期关于用少得多的样本且损失极小的结果仍然有效，验证了对先进VLMs鲁棒性认证的可行性。

Conclusion: 这些进展使最先进的VLMs的鲁棒性认证既定义明确又计算可行。

Abstract: Randomized smoothing (RS) is one of the prominent techniques to ensure the
correctness of machine learning models, where point-wise robustness
certificates can be derived analytically. While RS is well understood for
classification, its application to generative models is unclear, since their
outputs are sequences rather than labels. We resolve this by connecting
generative outputs to an oracle classification task and showing that RS can
still be enabled: the final response can be classified as a discrete action
(e.g., service-robot commands in VLAs), as harmful vs. harmless (content
moderation or toxicity detection in VLMs), or even applying oracles to cluster
answers into semantically equivalent ones. Provided that the error rate for the
oracle classifier comparison is bounded, we develop the theory that associates
the number of samples with the corresponding robustness radius. We further
derive improved scaling laws analytically relating the certified radius and
accuracy to the number of samples, showing that the earlier result of 2 to 3
orders of magnitude fewer samples sufficing with minimal loss remains valid
even under weaker assumptions. Together, these advances make robustness
certification both well-defined and computationally feasible for
state-of-the-art VLMs, as validated against recent jailbreak-style adversarial
attacks.

</details>


### [135] [DiffusionNFT: Online Diffusion Reinforcement with Forward Process](https://arxiv.org/abs/2509.16117)
*Kaiwen Zheng,Huayu Chen,Haotian Ye,Haoxiang Wang,Qinsheng Zhang,Kai Jiang,Hang Su,Stefano Ermon,Jun Zhu,Ming-Yu Liu*

Main category: cs.LG

TL;DR: 提出DiffusionNFT在线强化学习范式优化扩散模型，比FlowGRPO更高效，提升SD3.5 - Medium性能。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习扩展到扩散模型有挑战，现有离散化方法有缺陷。

Method: 通过流匹配在正向过程直接优化扩散模型，对比正负生成定义策略改进方向，将强化信号融入监督学习目标。

Result: DiffusionNFT比FlowGRPO效率高25倍，GenEval分数提升快，提升SD3.5 - Medium在各基准测试中的性能。

Conclusion: DiffusionNFT是一种有效的在线强化学习范式，可直接优化扩散模型，无需似然估计等，能提升模型性能。

Abstract: Online reinforcement learning (RL) has been central to post-training language
models, but its extension to diffusion models remains challenging due to
intractable likelihoods. Recent works discretize the reverse sampling process
to enable GRPO-style training, yet they inherit fundamental drawbacks,
including solver restrictions, forward-reverse inconsistency, and complicated
integration with classifier-free guidance (CFG). We introduce Diffusion
Negative-aware FineTuning (DiffusionNFT), a new online RL paradigm that
optimizes diffusion models directly on the forward process via flow matching.
DiffusionNFT contrasts positive and negative generations to define an implicit
policy improvement direction, naturally incorporating reinforcement signals
into the supervised learning objective. This formulation enables training with
arbitrary black-box solvers, eliminates the need for likelihood estimation, and
requires only clean images rather than sampling trajectories for policy
optimization. DiffusionNFT is up to $25\times$ more efficient than FlowGRPO in
head-to-head comparisons, while being CFG-free. For instance, DiffusionNFT
improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO
achieves 0.95 with over 5k steps and additional CFG employment. By leveraging
multiple reward models, DiffusionNFT significantly boosts the performance of
SD3.5-Medium in every benchmark tested.

</details>


### [136] [Network-Based Detection of Autism Spectrum Disorder Using Sustainable and Non-invasive Salivary Biomarkers](https://arxiv.org/abs/2509.16126)
*Janayna M. Fernandes,Robinson Sabino-Silva,Murillo G. Carneiro*

Main category: cs.LG

TL;DR: 用ATR - FTIR光谱分析唾液样本，开发GANet框架用于ASD检测，性能优于其他模型，有潜在应用价值。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍（ASD）缺乏可靠生物标志物，导致早期诊断延迟，需要开发新检测工具。

Method: 使用ATR - FTIR光谱分析159个唾液样本，开发基于遗传算法的GANet框架，利用PageRank和Degree进行特征表征，优化网络结构。

Result: GANet性能优于线性判别分析、支持向量机和深度学习模型，准确率0.78，灵敏度0.61，特异性0.90，调和均值0.74。

Conclusion: GANet有潜力成为用于精确ASD检测和更广泛基于光谱健康应用的强大、仿生、非侵入性工具。

Abstract: Autism Spectrum Disorder (ASD) lacks reliable biological markers, delaying
early diagnosis. Using 159 salivary samples analyzed by ATR-FTIR spectroscopy,
we developed GANet, a genetic algorithm-based network optimization framework
leveraging PageRank and Degree for importance-based feature characterization.
GANet systematically optimizes network structure to extract meaningful patterns
from high-dimensional spectral data. It achieved superior performance compared
to linear discriminant analysis, support vector machines, and deep learning
models, reaching 0.78 accuracy, 0.61 sensitivity, 0.90 specificity, and a 0.74
harmonic mean. These results demonstrate GANet's potential as a robust,
bio-inspired, non-invasive tool for precise ASD detection and broader
spectral-based health applications.

</details>


### [137] [Dynamic Classifier-Free Diffusion Guidance via Online Feedback](https://arxiv.org/abs/2509.16131)
*Pinelopi Papalampidi,Olivia Wiles,Ira Ktena,Aleksandar Shtedritski,Emanuele Bugliarello,Ivana Kajic,Isabela Albuquerque,Aida Nematzadeh*

Main category: cs.LG

TL;DR: 提出动态CFG调度框架，利用反馈为各时间步选择最优CFG规模，在多方面有显著提升，证明最优指导方案是动态且依赖提示的。


<details>
  <summary>Details</summary>
Motivation: 静态指导规模限制了文本到图像扩散模型中分类器无引导（CFG）的有效性，现有解决方案有局限。

Method: 利用通用和专门的小规模潜在空间评估的在线反馈评估反向扩散过程中每一步的生成质量，进行贪心搜索为每个时间步选择最优CFG规模。

Result: 在小规模模型和Imagen 3上展示出有效性，在文本对齐、视觉质量等方面有显著改进，相比Imagen 3基线，人类偏好胜率最高达55.5%。

Conclusion: 最优指导方案本质上是动态且依赖提示的，提供了实现它的高效且可推广的框架。

Abstract: Classifier-free guidance (CFG) is a cornerstone of text-to-image diffusion
models, yet its effectiveness is limited by the use of static guidance scales.
This "one-size-fits-all" approach fails to adapt to the diverse requirements of
different prompts; moreover, prior solutions like gradient-based correction or
fixed heuristic schedules introduce additional complexities and fail to
generalize. In this work, we challeng this static paradigm by introducing a
framework for dynamic CFG scheduling. Our method leverages online feedback from
a suite of general-purpose and specialized small-scale latent-space
evaluations, such as CLIP for alignment, a discriminator for fidelity and a
human preference reward model, to assess generation quality at each step of the
reverse diffusion process. Based on this feedback, we perform a greedy search
to select the optimal CFG scale for each timestep, creating a unique guidance
schedule tailored to every prompt and sample. We demonstrate the effectiveness
of our approach on both small-scale models and the state-of-the-art Imagen 3,
showing significant improvements in text alignment, visual quality, text
rendering and numerical reasoning. Notably, when compared against the default
Imagen 3 baseline, our method achieves up to 53.8% human preference win-rate
for overall preference, a figure that increases up to to 55.5% on prompts
targeting specific capabilities like text rendering. Our work establishes that
the optimal guidance schedule is inherently dynamic and prompt-dependent, and
provides an efficient and generalizable framework to achieve it.

</details>


### [138] [Spatio-temporal, multi-field deep learning of shock propagation in meso-structured media](https://arxiv.org/abs/2509.16139)
*M. Giselle Fernández-Godino,Meir H. Shachar,Kevin Korner,Jonathan L. Belof,Mukul Kumar,Jonathan Lind,William J. Schill*

Main category: cs.LG

TL;DR: 本文介绍多场时空深度学习模型MSTM预测冲击波穿过多孔和结构材料的情况，运行快且误差小，为相关领域优化材料建立实用框架。


<details>
  <summary>Details</summary>
Motivation: 预测冲击波穿过多孔和结构材料的能力对行星防御、国家安全和惯性聚变能源至关重要，但现有方法难以捕捉关键现象，存在挑战。

Method: 引入多场时空深度学习模型MSTM，将七个耦合场统一为单个自回归替代模型，并在高保真流体动力学代码数据上进行训练。

Result: MSTM比直接模拟快约一千倍，在多孔材料中误差低于4%，在晶格结构中误差低于10%，能解析尖锐激波前沿，保持质量平均压力和温度等积分量在5%以内。

Conclusion: 该进展将原本难以处理的问题转化为可处理的设计研究，为行星撞击缓解、惯性聚变能源和国家安全等领域优化细观结构材料建立了实用框架。

Abstract: The ability to predict how shock waves traverse porous and architected
materials is a decisive factor in planetary defense, national security, and the
race to achieve inertial fusion energy. Yet capturing pore collapse, anomalous
Hugoniot responses, and localized heating -- phenomena that can determine the
success of asteroid deflection or fusion ignition -- has remained a major
challenge despite recent advances in single-field and reduced representations.
We introduce a multi-field spatio-temporal deep learning model (MSTM) that
unifies seven coupled fields -- pressure, density, temperature, energy,
material distribution, and two velocity components -- into a single
autoregressive surrogate. Trained on high-fidelity hydrocode data, MSTM runs
about a thousand times faster than direct simulation, achieving errors below
4\% in porous materials and below 10\% in lattice structures. Unlike prior
single-field or operator-based surrogates, MSTM resolves sharp shock fronts
while preserving integrated quantities such as mass-averaged pressure and
temperature to within 5\%. This advance transforms problems once considered
intractable into tractable design studies, establishing a practical framework
for optimizing meso-structured materials in planetary impact mitigation,
inertial fusion energy, and national security.

</details>


### [139] [Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents](https://arxiv.org/abs/2509.16151)
*Isaiah J. King,Benjamin Bowman,H. Howie Huang*

Main category: cs.LG

TL;DR: 本文提出将自动网络防御（ACD）构建为基于图的马尔可夫决策问题，使代理能更好推理网络状态并零样本适应新网络，大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习用于ACD时，模型会过拟合特定网络拓扑，面对环境扰动时效果不佳。

Method: 将ACD构建为基于图的部分可观测马尔可夫决策问题，让代理基于关系归纳偏置进行推理。

Result: 该方法大幅超越现有技术水平，代理能在复杂多智能体环境中防御未见网络。

Conclusion: 所提方法可使代理更好推理网络状态，实现零样本适应新网络，有效应对多种复杂情况。

Abstract: Deep reinforcement learning (RL) is emerging as a viable strategy for
automated cyber defense (ACD). The traditional RL approach represents networks
as a list of computers in various states of safety or threat. Unfortunately,
these models are forced to overfit to specific network topologies, rendering
them ineffective when faced with even small environmental perturbations. In
this work, we frame ACD as a two-player context-based partially observable
Markov decision problem with observations represented as attributed graphs.
This approach allows our agents to reason through the lens of relational
inductive bias. Agents learn how to reason about hosts interacting with other
system entities in a more general manner, and their actions are understood as
edits to the graph representing the environment. By introducing this bias, we
will show that our agents can better reason about the states of networks and
zero-shot adapt to new ones. We show that this approach outperforms the
state-of-the-art by a wide margin, and makes our agents capable of defending
never-before-seen networks against a wide range of adversaries in a variety of
complex, and multi-agent environments.

</details>


### [140] [DIVEBATCH: Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation](https://arxiv.org/abs/2509.16173)
*Yuen Chen,Yian Wang,Hari Sundaram*

Main category: cs.LG

TL;DR: 提出自适应批量大小SGD算法DiveBatch加速机器学习模型训练，评估显示收敛显著更快。


<details>
  <summary>Details</summary>
Motivation: 大规模深度神经网络训练计算成本高，传统方法聚焦学习率调整，需新方法加速训练。

Method: 提出基于梯度多样性的数据驱动自适应批量大小的SGD算法DiveBatch。

Result: DiveBatch比标准SGD和AdaBatch收敛快1.06 - 5.0倍，性能有轻微折衷。

Conclusion: DiveBatch能在保持小批量训练泛化性能的同时，提高收敛速度和计算效率。

Abstract: The goal of this paper is to accelerate the training of machine learning
models, a critical challenge since the training of large-scale deep neural
models can be computationally expensive. Stochastic gradient descent (SGD) and
its variants are widely used to train deep neural networks. In contrast to
traditional approaches that focus on tuning the learning rate, we propose a
novel adaptive batch size SGD algorithm, DiveBatch, that dynamically adjusts
the batch size. Adapting the batch size is challenging: using large batch sizes
is more efficient due to parallel computation, but small-batch training often
converges in fewer epochs and generalizes better. To address this challenge, we
introduce a data-driven adaptation based on gradient diversity, enabling
DiveBatch to maintain the generalization performance of small-batch training
while improving convergence speed and computational efficiency. Gradient
diversity has a strong theoretical justification: it emerges from the
convergence analysis of SGD. Evaluations of DiveBatch on synthetic and
CiFar-10, CiFar-100, and Tiny-ImageNet demonstrate that DiveBatch converges
significantly faster than standard SGD and AdaBatch (1.06 -- 5.0x), with a
slight trade-off in performance.

</details>


### [141] [Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences](https://arxiv.org/abs/2509.16189)
*Andrew Kyle Lampinen,Martin Engelcke,Yuxuan Li,Arslan Chaudhry,James L. McClelland*

Main category: cs.LG

TL;DR: 本文从认知科学获得灵感，指出机器学习系统缺乏潜在学习的问题，展示相关失败案例，提出情景记忆是解决方案，证明带检索机制的系统可提升泛化能力，明确有效检索的要素。


<details>
  <summary>Details</summary>
Motivation: 探究机器学习系统泛化失败的原因及提升泛化能力的机制。

Method: 借鉴认知科学，分析从语言建模到基于代理导航的失败案例，构建带 oracle 检索机制的系统进行研究。

Result: 带 oracle 检索机制的系统能更灵活利用学习经验，在多挑战中更好泛化，明确有效使用检索的关键组件。

Conclusion: 指出当前机器学习系统相对数据低效的一个可能原因，说明检索方法可补充参数学习以提升泛化能力。

Abstract: When do machine learning systems fail to generalize, and what mechanisms
could improve their generalization? Here, we draw inspiration from cognitive
science to argue that one weakness of machine learning systems is their failure
to exhibit latent learning -- learning information that is not relevant to the
task at hand, but that might be useful in a future task. We show how this
perspective links failures ranging from the reversal curse in language modeling
to new findings on agent-based navigation. We then highlight how cognitive
science points to episodic memory as a potential part of the solution to these
issues. Correspondingly, we show that a system with an oracle retrieval
mechanism can use learning experiences more flexibly to generalize better
across many of these challenges. We also identify some of the essential
components for effectively using retrieval, including the importance of
within-example in-context learning for acquiring the ability to use information
across retrieved examples. In summary, our results illustrate one possible
contributor to the relative data inefficiency of current machine learning
systems compared to natural intelligence, and help to understand how retrieval
methods can complement parametric learning to improve generalization.

</details>


### [142] [Inverting Trojans in LLMs](https://arxiv.org/abs/2509.16203)
*Zhengxing Li,Guangmingmei Yang,Jayaram Raghuram,David J. Miller,George Kesidis*

Main category: cs.LG

TL;DR: 现有AI后门检测和反转方法难以移植到LLMs，本文提出含离散搜索、隐式黑名单和高误分类检测的LLM触发反转方法，能可靠检测并反转真实后门触发短语。


<details>
  <summary>Details</summary>
Motivation: 现有AI的后门检测和反转方法难以移植到LLMs，存在输入空间离散、需考虑大量k元组和缺乏有效黑名单等挑战。

Method: 提出LLM触发反转方法，包含离散搜索（从单例列表开始贪婪增加）、隐式黑名单（评估候选触发与目标类干净样本激活空间的平均余弦相似度）和检测（候选触发引起高误分类且决策置信度异常高时检测）。

Result: 该方法能可靠检测并成功反转真实后门触发短语。

Conclusion: 所提方法有效解决了LLMs后门检测和反转的问题。

Abstract: While effective backdoor detection and inversion schemes have been developed
for AIs used e.g. for images, there are challenges in "porting" these methods
to LLMs. First, the LLM input space is discrete, which precludes gradient-based
search over this space, central to many backdoor inversion methods. Second,
there are ~30,000^k k-tuples to consider, k the token-length of a putative
trigger. Third, for LLMs there is the need to blacklist tokens that have strong
marginal associations with the putative target response (class) of an attack,
as such tokens give false detection signals. However, good blacklists may not
exist for some domains. We propose a LLM trigger inversion approach with three
key components: i) discrete search, with putative triggers greedily accreted,
starting from a select list of singletons; ii) implicit blacklisting, achieved
by evaluating the average cosine similarity, in activation space, between a
candidate trigger and a small clean set of samples from the putative target
class; iii) detection when a candidate trigger elicits high misclassifications,
and with unusually high decision confidence. Unlike many recent works, we
demonstrate that our approach reliably detects and successfully inverts
ground-truth backdoor trigger phrases.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [143] [Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges](https://arxiv.org/abs/2509.15283)
*Kadin Matotek,Heather Cassel,Md Amiruzzaman,Linh B. Ngo*

Main category: cs.SE

TL;DR: 研究评估开源本地大语言模型处理复杂编程任务的表现，改进评估框架，测试8个模型处理3589个问题，发现本地模型与专有模型有差距但开源模型进步快。


<details>
  <summary>Details</summary>
Motivation: 考察当今开源本地大语言模型处理复杂竞争编程任务的性能。

Method: 改进AI代码生成评估框架（FACE）使其能离线运行，用该框架对8个67 - 90亿参数的代码模型处理Kattis语料库的3589个问题进行测试。

Result: 本地模型整体pass@1准确率一般，最佳模型接受率约为专有模型Gemini 1.5和ChatGPT - 4的一半。

Conclusion: 私有可控成本的大语言模型部署与先进专有服务存在差距，开源模型进步快，评估工作流有实际益处可在内部硬件上复制。

Abstract: This study examines the performance of today's open-source, locally hosted
large-language models (LLMs) in handling complex competitive programming tasks
with extended problem descriptions and contexts. Building on the original
Framework for AI-driven Code Generation Evaluation (FACE), the authors retrofit
the pipeline to work entirely offline through the Ollama runtime, collapsing
FACE's sprawling per-problem directory tree into a handful of consolidated JSON
files, and adding robust checkpointing so multi-day runs can resume after
failures. The enhanced framework generates, submits, and records solutions for
the full Kattis corpus of 3,589 problems across eight code-oriented models
ranging from 6.7-9 billion parameters. The submission results show that the
overall pass@1 accuracy is modest for the local models, with the best models
performing at approximately half the acceptance rate of the proprietary models,
Gemini 1.5 and ChatGPT-4. These findings expose a persistent gap between
private, cost-controlled LLM deployments and state-of-the-art proprietary
services, yet also highlight the rapid progress of open models and the
practical benefits of an evaluation workflow that organizations can replicate
on in-house hardware.

</details>


### [144] [LoCaL: Countering Surface Bias in Code Evaluation Metrics](https://arxiv.org/abs/2509.15397)
*Simantika Bhattacharjee Dristi,Matthew B. Dwyer*

Main category: cs.SE

TL;DR: 论文评估四种参考式代码评估指标（CEMs），发现其对表层特征有强偏向性，提出LoCaL基准测试，结果显示CEMs在LoCaL上性能下降，或有助于开发抗表层偏差的指标。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型及基于其的代理流行，可靠有效的代码评估指标对软件工程任务至关重要，但现有参考式CEMs与功能正确性相关性弱的原因待明确，且缺乏合适评估数据集。

Method: 评估四种先进参考式CEMs，提出LoCaL基准测试，用差分模糊测试计算功能相似度分数。

Result: 四种CEMs在LoCaL上较基线性能显著下降。

Conclusion: 让CEMs接触LoCaL类数据或有助于开发抗表层偏差的指标。

Abstract: With the increasing popularity of large language models (LLMs) and LLM-based
agents, reliable and effective code evaluation metrics (CEMs) have become
crucial for progress across several software engineering tasks. While popular
benchmarks often provide test cases to assess the correctness of generated
code, crafting and executing test cases is expensive. Reference-based CEMs
provide a cheaper alternative by scoring a candidate program based on its
functional similarity to a reference. Although prior research has focused on
reporting the weak correlation between these CEMs and functional correctness,
the causes are only assumed, and plausible solutions remain unexplored. In this
work, we critically evaluate four state-of-the-art reference-based CEMs,
revealing their strong bias towards surface-level features rather than code
functionality. Despite this surface bias, current evaluation datasets for these
CEMs rarely include code pairs that are surface-similar yet functionally
dissimilar, or functionally similar yet surface-dissimilar. To mitigate this
gap, we propose LoCaL (Looks Can Lie), a CEM evaluation benchmark, with 3117
code pairs at both the method and program levels. Each pair is labeled with a
functional similarity score and aims to target regions where CEMs are likely to
perform poorly. The functional similarity scores are calculated through
differential fuzzing, which eliminates the need for predefined test cases and,
at the same time, improves the reliability of the scores by executing an order
of magnitude more tests than prior work. We find that all four CEMs show
significant performance degradation on LoCaL, compared to the baselines.
Finally, based on our findings, we draw the implication that exposing CEMs to
LoCaL-like data might facilitate the development of metrics that are robust to
surface bias.

</details>


### [145] [Brevity is the Soul of Wit: Condensing Code Changes to Improve Commit Message Generation](https://arxiv.org/abs/2509.15567)
*Hongyu Kuang,Ning Zhang,Hui Gao,Xin Zhou,Wesley K. G. Assunção,Xiaoxing Ma,Dong Shao,Guoping Rong,He Zhang*

Main category: cs.SE

TL;DR: 本文提出通过文本模板浓缩代码变更，微调CodeLlama - 7B生成提交消息，在指标上优于六个基线。


<details>
  <summary>Details</summary>
Motivation: 开发者常忽视编写高质量提交消息，现有工作聚焦代码变更组织和表示，本文探索新方法。

Method: 提出由总结代码变更、引出注释和强调代码标识符三部分组成的文本模板，用ChangeScribe工具浓缩代码变更，微调CodeLlama - 7B。

Result: 在常用数据集评估中，该方法在BLEU - Norm、METEOR和ROUGE - L指标上平均提升51.7%、78.7%和62.5%，消融研究和人工评估也证明其有效性。

Conclusion: 提出的文本模板能更好利用预训练语言模型，可有效辅助开发者生成提交消息。

Abstract: Commit messages are valuable resources for describing why code changes are
committed to repositories in version control systems (e.g., Git). They
effectively help developers understand code changes and better perform software
maintenance tasks. Unfortunately, developers often neglect to write
high-quality commit messages in practice. Therefore, a growing body of work is
proposed to generate commit messages automatically. These works all
demonstrated that how to organize and represent code changes is vital in
generating good commit messages, including the use of fine-grained graphs or
embeddings to better represent code changes. In this study, we choose an
alternative way to condense code changes before generation, i.e., proposing
brief yet concise text templates consisting of the following three parts: (1)
summarized code changes, (2) elicited comments, and (3) emphasized code
identifiers. Specifically, we first condense code changes by using our proposed
templates with the help of a heuristic-based tool named ChangeScribe, and then
fine-tune CodeLlama-7B on the pairs of our proposed templates and corresponding
commit messages. Our proposed templates better utilize pre-trained language
models, while being naturally brief and readable to complement generated commit
messages for developers. Our evaluation based on a widely used dataset showed
that our approach can outperform six baselines in terms of BLEU-Norm, METEOR,
and ROUGE-L, with average improvements of 51.7%, 78.7%, and 62.5%,
respectively. The ablation study and human evaluation also provide further
insights into the effectiveness of our approach.

</details>


### [146] [How Far Are We? An Empirical Analysis of Current Vulnerability Localization Approaches](https://arxiv.org/abs/2509.15777)
*Haoran Xu,Zhi Chen,Junxiao Han,Xinkui Zhao,Jianwei Yin,Shuiguang Deng*

Main category: cs.SE

TL;DR: 文章指出传统和现有自动化漏洞补丁检测方法存在问题，通过研究提出关键见解并设计新框架，实验显示新方法更优。


<details>
  <summary>Details</summary>
Motivation: 传统手动检测有扩展性问题且易出错，现有自动化方法准确率低、泛化能力差，需解决这些问题。

Method: 对现有漏洞补丁检测方法进行实证研究，得出关键见解，提出结合版本驱动候选过滤和大语言模型多轮对话投票的两阶段框架。

Result: 在含750个真实漏洞的数据集上实验，新方法优于现有方法。

Conclusion: 所提两阶段框架能实现准确高效的漏洞补丁识别。

Abstract: Open-source software vulnerability patch detection is a critical component
for maintaining software security and ensuring software supply chain integrity.
Traditional manual detection methods face significant scalability challenges
when processing large volumes of commit histories, while being prone to human
errors and omissions. Existing automated approaches, including heuristic-based
methods and pre-trained model solutions, suffer from limited accuracy, poor
generalization capabilities, and inherent methodological constraints that
hinder their practical deployment. To address these fundamental challenges,
this paper conducts a comprehensive empirical study of existing vulnerability
patch detection methods, revealing four key insights that guide the design of
effective solutions: the critical impact of search space reduction, the
superiority of pre-trained semantic understanding over architectural
complexity, the temporal limitations of web crawling approaches, and the
advantages of knowledge-driven methods. Based on these insights, we propose a
novel two-stage framework that combines version-driven candidate filtering with
large language model-based multi-round dialogue voting to achieve accurate and
efficient vulnerability patch identification. Extensive experiments on a
dataset containing 750 real vulnerabilities demonstrate that our method
outperforms current approaches.

</details>


### [147] [Failure Modes and Effects Analysis: An Experience from the E-Bike Domain](https://arxiv.org/abs/2509.15893)
*Andrea Bombarda,Federico Conti,Marcello Minervini,Aurora Zanenga,Claudio Menghi*

Main category: cs.SE

TL;DR: 本文介绍使用FMEA分析电动自行车领域CPS安全的经验，用Simulink Fault Analyzer识别、建模并分析故障，结果表明模型准确，FMEA有助于改进模型，还给出十条经验教训。


<details>
  <summary>Details</summary>
Motivation: 工业界需要模拟驱动方法用于FMEA有效性的证据以提高实际应用。

Method: 使用Simulink Fault Analyzer对电动自行车领域的CPS进行FMEA分析，识别13个现实故障，建模并分析其影响，征求专家反馈。

Result: 模型准确或只有小误差随后被修正；38.4%的故障模拟输出与工程师预期不符，帮助发现意外影响。

Conclusion: 研究结果对Simulink工程师、使用Simulink Fault Analyzer的人员和安全分析师有用。

Abstract: Software failures can have catastrophic and costly consequences. Functional
Failure Mode and Effects Analysis (FMEA) is a standard technique used within
Cyber-Physical Systems (CPS) to identify software failures and assess their
consequences. Simulation-driven approaches have recently been shown to be
effective in supporting FMEA. However, industries need evidence of the
effectiveness of these approaches to increase practical adoption. This
industrial paper presents our experience with using FMEA to analyze the safety
of a CPS from the e-Bike domain. We used Simulink Fault Analyzer, an industrial
tool that supports engineers with FMEA. We identified 13 realistic faults,
modeled them, and analyzed their effects. We sought expert feedback to analyze
the appropriateness of our models and the effectiveness of the faults in
detecting safety breaches. Our results reveal that for the faults we
identified, our models were accurate or contained minor imprecision that we
subsequently corrected. They also confirm that FMEA helps engineers improve
their models. Specifically, the output provided by the simulation-driven
support for 38.4% (5 out of 13) of the faults did not match the engineers'
expectations, helping them discover unexpected effects of the faults. We
present a thorough discussion of our results and ten lessons learned. Our
findings are useful for software engineers who work as Simulink engineers, use
the Simulink Fault Analyzer, or work as safety analysts.

</details>


### [148] [LeakageDetector 2.0: Analyzing Data Leakage in Jupyter-Driven Machine Learning Pipelines](https://arxiv.org/abs/2509.15971)
*Owen Truong,Terrence Zhang,Arnav Marchareddy,Ryan Lee,Jeffery Busold,Michael Socas,Eman Abdullah AlOmar*

Main category: cs.SE

TL;DR: 本文开发了名为LeakageDetector的VS Code扩展，用于检测Jupyter Notebook文件中的数据泄漏问题，并提供两种修正机制。


<details>
  <summary>Details</summary>
Motivation: 帮助机器学习工程师识别和纠正模型中的数据泄漏问题，提升代码质量。

Method: 开发LeakageDetector扩展来检测数据泄漏，包括重叠、预处理和多测试泄漏；提供常规手动修复和LLM驱动的修正机制。

Result: 开发出了LeakageDetector扩展及相应的修正机制。

Conclusion: 该扩展可帮助机器学习工程师检测和解决数据泄漏问题，提升代码质量。

Abstract: In software development environments, code quality is crucial. This study
aims to assist Machine Learning (ML) engineers in enhancing their code by
identifying and correcting Data Leakage issues within their models. Data
Leakage occurs when information from the test dataset is inadvertently included
in the training data when preparing a data science model, resulting in
misleading performance evaluations. ML developers must carefully separate their
data into training, evaluation, and test sets to avoid introducing Data Leakage
into their code. In this paper, we develop a new Visual Studio Code (VS Code)
extension, called LeakageDetector, that detects Data Leakage, mainly Overlap,
Preprocessing and Multi-test leakage, from Jupyter Notebook files. Beyond
detection, we included two correction mechanisms: a conventional approach,
known as a quick fix, which manually fixes the leakage, and an LLM-driven
approach that guides ML developers toward best practices for building ML
pipelines.

</details>


### [149] [Software Development Aspects of Integrating Linear Algebra Libraries](https://arxiv.org/abs/2509.16081)
*Marcel Koch,Tobias Ribizel,Pratik Nayak,Fritz Göbel,Gregor Olenik,Terry Cojean*

Main category: cs.SE

TL;DR: 本文探讨应用软件采用Ginkgo库的挑战与益处，并给出不同领域示例，强调其对可持续软件开发的作用。


<details>
  <summary>Details</summary>
Motivation: 许多科学发现依赖模拟软件，而这些软件依赖特定小部件。Ginkgo库可处理不同平台稀疏数值线性代数，本文旨在研究应用软件采用它的情况。

Method: 给出CFD、电网模拟和心电生理学等不同领域的示例，从软件工程角度讨论集成对应用代码的影响。

Result: 展示了不同领域应用Ginkgo的情况以及集成对应用代码的影响。

Conclusion: 强调了Ginkgo和应用程序为实现可持续软件开发所采取的方法。

Abstract: Many scientific discoveries are made through, or aided by, the use of
simulation software. These sophisticated software applications are not built
from the ground up, instead they rely on smaller parts for specific use cases,
usually from domains unfamiliar to the application scientists. The software
library Ginkgo is one of these building blocks to handle sparse numerical
linear algebra on different platforms. By using Ginkgo, applications are able
to ease the transition to modern systems, and speed up their simulations
through faster numerical linear algebra routines. This paper discusses the
challenges and benefits for application software in adopting Ginkgo. It will
present examples from different domains, such as CFD, power grid simulation, as
well as electro-cardiophysiology. For these cases, the impact of the
integrations on the application code is discussed from a software engineering
standpoint, and in particular, the approaches taken by Ginkgo and the
applications to enable sustainable software development are highlighted.

</details>


### [150] [When Bugs Linger: A Study of Anomalous Resolution Time Outliers and Their Themes](https://arxiv.org/abs/2509.16140)
*Avinash Patil*

Main category: cs.SE

TL;DR: 研究分析七个开源仓库的错误解决异常，用统计方法识别异常，文本特征提取和聚类分析主题，发现常见异常模式，为维护者提供解决长期错误的见解。


<details>
  <summary>Details</summary>
Motivation: 高效解决错误对软件质量和用户满意度至关重要，但部分错误报告解决时间过长，需分析错误解决异常以发现潜在问题。

Method: 运用Z - score和IQR等统计方法识别错误解决时长的异常，用TF - IDF进行文本特征提取，KMeans聚类对相似错误摘要分组。

Result: 各项目存在一致模式，异常常围绕测试失败、增强请求和用户界面问题。

Conclusion: 该方法为项目维护者优先处理和有效解决长期存在的错误提供了可操作的见解。

Abstract: Efficient bug resolution is critical for maintaining software quality and
user satisfaction. However, specific bug reports experience unusually long
resolution times, which may indicate underlying process inefficiencies or
complex issues. This study presents a comprehensive analysis of bug resolution
anomalies across seven prominent open-source repositories: Cassandra, Firefox,
Hadoop, HBase, SeaMonkey, Spark, and Thunderbird. Utilizing statistical methods
such as Z-score and Interquartile Range (IQR), we identify anomalies in bug
resolution durations. To understand the thematic nature of these anomalies, we
apply Term Frequency-Inverse Document Frequency (TF-IDF) for textual feature
extraction and KMeans clustering to group similar bug summaries. Our findings
reveal consistent patterns across projects, with anomalies often clustering
around test failures, enhancement requests, and user interface issues. This
approach provides actionable insights for project maintainers to prioritize and
effectively address long-standing bugs.

</details>


### [151] [MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code Translation Validation and Repair](https://arxiv.org/abs/2509.16187)
*Ali Reza Ibrahimzada,Brandon Paulsen,Reyhaneh Jabbarvand,Joey Dodds,Daniel Kroening*

Main category: cs.SE

TL;DR: 本文提出MatchFixAgent框架用于代码翻译等价验证与修复，实验表明其适应性强、结果准确。


<details>
  <summary>Details</summary>
Motivation: 现有自动化验证和修复方法工程开销大、依赖不足的测试套件，难以推广到多种编程语言。

Method: 开发基于大语言模型、与编程语言无关的MatchFixAgent框架，采用多智能体架构，将等价验证分为子任务，通过测试智能体编写执行测试，修复智能体修复错误，裁决智能体给出最终结论。

Result: MatchFixAgent能对99.2%的翻译对给出等价性裁决，72.8%与先前工作结果相同，不同时60.7%是正确的，能修复50.6%的不等价翻译，远高于先前工作的18.5%。

Conclusion: MatchFixAgent比先前工作更能适应多种编程语言对，验证结果高度准确。

Abstract: Code translation transforms source code from one programming language (PL) to
another. Validating the functional equivalence of translation and repairing, if
necessary, are critical steps in code translation. Existing automated
validation and repair approaches struggle to generalize to many PLs due to high
engineering overhead, and they rely on existing and often inadequate test
suites, which results in false claims of equivalence and ineffective
translation repair. We develop MatchFixAgent, a large language model
(LLM)-based, PL-agnostic framework for equivalence validation and repair of
translations. MatchFixAgent features a multi-agent architecture that divides
equivalence validation into several sub-tasks to ensure thorough and consistent
semantic analysis of the translation. Then it feeds this analysis to test agent
to write and execute tests. Upon observing a test failure, the repair agent
attempts to fix the translation bug. The final (in)equivalence decision is made
by the verdict agent, considering semantic analyses and test execution results.
  We compare MatchFixAgent's validation and repair results with four
repository-level code translation techniques. We use 2,219 translation pairs
from their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub
projects totaling over 900K lines of code. Our results demonstrate that
MatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs,
with the same equivalence validation result as prior work on 72.8% of them.
When MatchFixAgent's result disagrees with prior work, we find that 60.7% of
the time MatchFixAgent's result is actually correct. In addition, we show that
MatchFixAgent can repair 50.6% of inequivalent translation, compared to prior
work's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to
many PL pairs than prior work, while producing highly accurate validation
results.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [152] [SETrLUSI: Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant](https://arxiv.org/abs/2509.15593)
*Chunna Li,Yiwei Song,Yuanhai Shao*

Main category: stat.ML

TL;DR: 提出SETrLUSI框架用于多源迁移学习，能有效利用知识、加速收敛，实验显示其收敛性好且优于相关方法、成本低。


<details>
  <summary>Details</summary>
Motivation: 传统迁移学习方法仅处理单一类型知识，而源域有多样知识且不同域强调不同知识类型。

Method: 引入具有弱收敛模式的集成学习框架SETrLUSI，提取并整合源域和目标域的各种知识，还结合随机SI选择、比例源域采样和目标域自举。

Result: SETrLUSI有良好的收敛性，以较低的时间成本优于相关方法。

Conclusion: SETrLUSI能有效利用多样知识，加速收敛过程，提高训练效率并增强模型稳定性。

Abstract: In transfer learning, a source domain often carries diverse knowledge, and
different domains usually emphasize different types of knowledge. Different
from handling only a single type of knowledge from all domains in traditional
transfer learning methods, we introduce an ensemble learning framework with a
weak mode of convergence in the form of Statistical Invariant (SI) for
multi-source transfer learning, formulated as Stochastic Ensemble Multi-Source
Transfer Learning Using Statistical Invariant (SETrLUSI). The proposed SI
extracts and integrates various types of knowledge from both source and target
domains, which not only effectively utilizes diverse knowledge but also
accelerates the convergence process. Further, SETrLUSI incorporates stochastic
SI selection, proportional source domain sampling, and target domain
bootstrapping, which improves training efficiency while enhancing model
stability. Experiments show that SETrLUSI has good convergence and outperforms
related methods with a lower time cost.

</details>


### [153] [Interpretable Network-assisted Random Forest+](https://arxiv.org/abs/2509.15611)
*Tiffany M. Tang,Elizaveta Levina,Ji Zhu*

Main category: stat.ML

TL;DR: 提出基于随机森林推广的网络辅助模型（RF+），兼具高预测精度和可解释性，并开发一套解释工具。


<details>
  <summary>Details</summary>
Motivation: 现有网络辅助机器学习方法要么难解释，要么预测性能差，需一种兼具两者优势的方法。

Method: 提出基于随机森林推广的网络辅助模型（RF+），开发一套解释工具，包括全局和局部重要性度量及样本影响度量。

Result: 所提模型实现了有竞争力的预测精度，可通过特征重要性度量进行解释。

Conclusion: 这套工具拓宽了网络辅助机器学习在可解释性和透明度至关重要的高影响问题中的适用范围。

Abstract: Machine learning algorithms often assume that training samples are
independent. When data points are connected by a network, the induced
dependency between samples is both a challenge, reducing effective sample size,
and an opportunity to improve prediction by leveraging information from network
neighbors. Multiple methods taking advantage of this opportunity are now
available, but many, including graph neural networks, are not easily
interpretable, limiting their usefulness for understanding how a model makes
its predictions. Others, such as network-assisted linear regression, are
interpretable but often yield substantially worse prediction performance. We
bridge this gap by proposing a family of flexible network-assisted models built
upon a generalization of random forests (RF+), which achieves
highly-competitive prediction accuracy and can be interpreted through feature
importance measures. In particular, we develop a suite of interpretation tools
that enable practitioners to not only identify important features that drive
model predictions, but also quantify the importance of the network contribution
to prediction. Importantly, we provide both global and local importance
measures as well as sample influence measures to assess the impact of a given
observation. This suite of tools broadens the scope and applicability of
network-assisted machine learning for high-impact problems where
interpretability and transparency are essential.

</details>


### [154] [Phase Transition for Stochastic Block Model with more than $\sqrt{n}$ Communities](https://arxiv.org/abs/2509.15822)
*Alexandra Carpentier,Christophe Giraud,Nicolas Verzelen*

Main category: stat.ML

TL;DR: 文章验证了Chin等人关于K≥√n时社区恢复新阈值的猜想，证明低阶多项式在阈值下失效，多项式时间社区恢复在阈值上可行。


<details>
  <summary>Details</summary>
Motivation: 验证Chin等人在K≥√n的多社区情况下提出的新阈值猜想。

Method: 证明低阶多项式在Chin等人提出的阈值下无法恢复社区；通过计算观察图中团的出现情况，证明在阈值之上多项式时间社区恢复可行。

Result: 证实了K≥√n时Chin等人提出的新阈值猜想。

Conclusion: 对于K≥√n的情况，Chin等人提出的新阈值是有效的，低阶多项式和多项式时间社区恢复在阈值上下有不同表现。

Abstract: Predictions from statistical physics postulate that recovery of the
communities in Stochastic Block Model (SBM) is possible in polynomial time
above, and only above, the Kesten-Stigum (KS) threshold. This conjecture has
given rise to a rich literature, proving that non-trivial community recovery is
indeed possible in SBM above the KS threshold, as long as the number $K$ of
communities remains smaller than $\sqrt{n}$, where $n$ is the number of nodes
in the observed graph. Failure of low-degree polynomials below the KS threshold
was also proven when $K=o(\sqrt{n})$.
  When $K\geq \sqrt{n}$, Chin et al.(2025) recently prove that, in a sparse
regime, community recovery in polynomial time is possible below the KS
threshold by counting non-backtracking paths. This breakthrough result lead
them to postulate a new threshold for the many communities regime $K\geq
\sqrt{n}$. In this work, we provide evidences that confirm their conjecture for
$K\geq \sqrt{n}$:
  1- We prove that, for any density of the graph, low-degree polynomials fail
to recover communities below the threshold postulated by Chin et al.(2025);
  2- We prove that community recovery is possible in polynomial time above the
postulated threshold, not only in the sparse regime of~Chin et al., but also in
some (but not all) moderately sparse regimes by essentially counting clique
occurence in the observed graph.

</details>


### [155] [A more efficient method for large-sample model-free feature screening via multi-armed bandits](https://arxiv.org/abs/2509.16085)
*Xiaxue Ouyang,Xinlai Kang,Mengyu Li,Zhenxing Dou,Jun Yu,Cheng Meng*

Main category: stat.ML

TL;DR: 本文提出基于秩的无模型特征筛选方法CR - SIS及其高效变体BanditCR - SIS，减少计算负担，理论证明筛选性质，实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有特征筛选方法处理大样本数据时面临计算挑战，为减轻计算负担。

Method: 提出CR - SIS方法，基于Chatterjee秩相关；受多臂老虎机问题启发提出BanditCR - SIS以降低CR - SIS计算复杂度。

Result: CR - SIS计算成本为O(nlog(n)p)，BanditCR - SIS降至O(√(n)log(n)p + nlog(n))，实验显示比经典筛选方法性能更优、计算时间更短。

Conclusion: 在温和正则条件下，CR - SIS和BanditCR - SIS有筛选性质，方法有效且性能优越。

Abstract: We consider the model-free feature screening in large-scale
ultrahigh-dimensional data analysis. Existing feature screening methods often
face substantial computational challenges when dealing with large sample sizes.
To alleviate the computational burden, we propose a rank-based model-free sure
independence screening method (CR-SIS) and its efficient variant, BanditCR-SIS.
The CR-SIS method, based on Chatterjee's rank correlation, is as
straightforward to implement as the sure independence screening (SIS) method
based on Pearson correlation introduced by Fan and Lv(2008), but it is
significantly more powerful in detecting nonlinear relationships between
variables. Motivated by the multi-armed bandit (MAB) problem, we reformulate
the feature screening procedure to significantly reduce the computational
complexity of CR-SIS. For a predictor matrix of size n \times p, the
computational cost of CR-SIS is O(nlog(n)p), while BanditCR-SIS reduces this to
O(\sqrt(n)log(n)p + nlog(n)). Theoretically, we establish the sure screening
property for both CR-SIS and BanditCR-SIS under mild regularity conditions.
Furthermore, we demonstrate the effectiveness of our methods through extensive
experimental studies on both synthetic and real-world datasets. The results
highlight their superior performance compared to classical screening methods,
requiring significantly less computational time.

</details>


### [156] [Model-free algorithms for fast node clustering in SBM type graphs and application to social role inference in animals](https://arxiv.org/abs/2509.15989)
*Bertrand Cloez,Adrien Cotil,Jean-Baptiste Menassol,Nicolas Verzelen*

Main category: stat.ML

TL;DR: 提出用于随机块模型（SBM）图节点聚类和参数推断的无模型算法，证明估计器一致性，实验显示计算快、误差低，并应用于实证网络数据。


<details>
  <summary>Details</summary>
Motivation: 为SBM图的节点聚类和参数推断提供新方法，以解决社区检测问题。

Method: 受k - 均值问题的Lloyd算法启发，将方法扩展到具有一般边权重分布的SBM。

Result: 与现有技术相比，计算时间显著加快，估计误差阶数更低。

Conclusion: 算法具有实际应用价值，可用于行为生态学的实证网络数据。

Abstract: We propose a novel family of model-free algorithms for node clustering and
parameter inference in graphs generated from the Stochastic Block Model (SBM),
a fundamental framework in community detection. Drawing inspiration from the
Lloyd algorithm for the $k$-means problem, our approach extends to SBMs with
general edge weight distributions. We establish the consistency of our
estimator under a natural identifiability condition. Through extensive
numerical experiments, we benchmark our methods against state-of-the-art
techniques, demonstrating significantly faster computation times with the lower
order of estimation error. Finally, we validate the practical relevance of our
algorithms by applying them to empirical network data from behavioral ecology.

</details>


### [157] [What is a good matching of probability measures? A counterfactual lens on transport maps](https://arxiv.org/abs/2509.16027)
*Lucas De Lara,Luca Ganassali*

Main category: stat.ML

TL;DR: 本文对比分析三种运输映射构造方式，明确其等价条件，将反事实推理与运输映射选择联系起来，指出因果假设支持特定运输映射结构的情况，旨在丰富理论理解并搭建统计运输与因果推理桥梁。


<details>
  <summary>Details</summary>
Motivation: 耦合概率测度在统计和机器学习诸多问题中核心地位，但耦合不可识别，最优运输掩盖多元单调匹配不同概念共存事实，需深入理解运输映射及其因果解释。

Method: 对循环单调、分位数保持和三角单调三种运输映射构造进行比较分析，在结构因果模型框架下将反事实推理表述为固定边缘分布间运输映射选择问题。

Result: 建立三种运输映射等价的充要条件，明确反事实推理中不可检验假设作用，找出因果图和结构方程条件使反事实映射与经典统计运输一致。

Conclusion: 研究丰富了运输映射族理论理解，阐明其可能因果解释，有望在统计运输和因果推理间建立新联系。

Abstract: Coupling probability measures lies at the core of many problems in statistics
and machine learning, from domain adaptation to transfer learning and causal
inference. Yet, even when restricted to deterministic transports, such
couplings are not identifiable: two atomless marginals admit infinitely many
transport maps. The common recourse to optimal transport, motivated by cost
minimization and cyclical monotonicity, obscures the fact that several distinct
notions of multivariate monotone matchings coexist. In this work, we first
carry a comparative analysis of three constructions of transport maps:
cyclically monotone, quantile-preserving and triangular monotone maps. We
establish necessary and sufficient conditions for their equivalence, thereby
clarifying their respective structural properties. In parallel, we formulate
counterfactual reasoning within the framework of structural causal models as a
problem of selecting transport maps between fixed marginals, which makes
explicit the role of untestable assumptions in counterfactual reasoning. Then,
we are able to connect these two perspectives by identifying conditions on
causal graphs and structural equations under which counterfactual maps coincide
with classical statistical transports. In this way, we delineate the
circumstances in which causal assumptions support the use of a specific
structure of transport map. Taken together, our results aim to enrich the
theoretical understanding of families of transport maps and to clarify their
possible causal interpretations. We hope this work contributes to establishing
new bridges between statistical transport and causal inference.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [158] [Subset Selection for Stratified Sampling in Online Controlled Experiments](https://arxiv.org/abs/2509.15576)
*Haru Momozu,Yuki Uehara,Naoki Nishimura,Koya Ohashi,Deddy Jobson,Yilin Li,Phuong Dinh,Noriyoshi Sukegawa,Yuichi Takano*

Main category: stat.CO

TL;DR: 本文聚焦分层抽样中分层变量子集选择问题，设计算法并评估复杂度，实验表明该方法能提升在线控制实验灵敏度，辅助营销决策。


<details>
  <summary>Details</summary>
Motivation: 为提升分层抽样估计准确性，解决选择有效降低方差的分层变量子集问题。

Method: 设计逐个选择分层变量的高效算法，模拟一系列分层抽样过程，并估计算法计算复杂度。

Result: 计算实验显示，该方法在多变量与结果变量有一定相关性时，优于其他方差降低技术。

Conclusion: 分层抽样的子集选择方法可提高在线控制实验灵敏度，实现更可靠的营销决策。

Abstract: Online controlled experiments, also known as A/B testing, are the digital
equivalent of randomized controlled trials for estimating the impact of
marketing campaigns on website visitors. Stratified sampling is a traditional
technique for variance reduction to improve the sensitivity (or statistical
power) of controlled experiments; this technique first divides the population
into strata (homogeneous subgroups) based on stratification variables and then
draws samples from each stratum to avoid sampling bias. To enhance the
estimation accuracy of stratified sampling, we focus on the problem of
selecting a subset of stratification variables that are effective in variance
reduction. We design an efficient algorithm that selects stratification
variables one by one by simulating a series of stratified sampling processes.
We also estimate the computational complexity of our subset selection
algorithm. Computational experiments using synthetic and real-world datasets
demonstrate that our method can outperform other variance reduction techniques
especially when multiple variables have a certain correlation with the outcome
variable. Our subset selection method for stratified sampling can improve the
sensitivity of online controlled experiments, thus enabling more reliable
marketing decisions.

</details>


### [159] [Automated Model Tuning for Multifidelity Uncertainty Propagation in Trajectory Simulation](https://arxiv.org/abs/2509.16007)
*James E. Warner,Geoffrey F. Bomarito,Gianluca Geraci,Michael S. Eldred*

Main category: stat.CO

TL;DR: 研究多保真度不确定性传播中的自动模型调优，通过优化超参数在预算内最小化估计器方差，以实际例子证明其优于手动调优模型，并给出应用建议。


<details>
  <summary>Details</summary>
Motivation: 多保真度方法效果依赖模型相关性和成本，但如何自动调优低保真模型以最大化性能仍是研究空白，因此开展此研究。

Method: 针对多保真度轨迹模拟估计器，在实际在线场景中优化模型超参数以最小化估计器方差。

Result: 以实际例子表明，即使计算预算较低，自动模型调优也大幅优于手动调优模型；预算大时，调优结果接近已知最优超参数的最佳情况。

Conclusion: 给出了实际应用模型调优的建议，指出了在预算受限问题中采用该方法的途径。

Abstract: Multifidelity uncertainty propagation combines the efficiency of low-fidelity
models with the accuracy of a high-fidelity model to construct statistical
estimators of quantities of interest. It is well known that the effectiveness
of such methods depends crucially on the relative correlations and
computational costs of the available computational models. However, the
question of how to automatically tune low-fidelity models to maximize
performance remains an open area of research. This work investigates automated
model tuning, which optimizes model hyperparameters to minimize estimator
variance within a target computational budget. Focusing on multifidelity
trajectory simulation estimators, the cost-versus-precision tradeoff enabled by
this approach is demonstrated in a practical, online setting where upfront
tuning costs cannot be amortized. Using a real-world entry, descent, and
landing example, it is shown that automated model tuning largely outperforms
hand-tuned models even when the overall computational budget is relatively low.
Furthermore, for scenarios where the computational budget is large, model
tuning solutions can approach the best-case multifidelity estimator performance
where optimal model hyperparameters are known a priori. Recommendations for
applying model tuning in practice are provided and avenues for enabling
adoption of such approaches for budget-constrained problems are highlighted.

</details>


### [160] [Transient regime of piecewise deterministic Monte Carlo algorithms](https://arxiv.org/abs/2509.16062)
*Sanket Agrawal,Joris Bierkens,Kengo Kamatani,Gareth O. Roberts*

Main category: stat.CO

TL;DR: 研究分段确定性马尔可夫过程（PDMPs）在凸势下的瞬态机制，对比其与随机游走Metropolis的瞬态成本，为PDMP采样器在大规模推理中的应用提供理论和实践指导。


<details>
  <summary>Details</summary>
Motivation: 研究PDMPs在凸势下，从低概率区域向高概率区域移动的瞬态机制。

Method: 使用流体极限参数，将生成器分解为快速和慢速部分，获得早期行为的确定性常微分方程描述。

Result: 对于高斯目标，PDMP方法的瞬态成本与随机游走Metropolis相当；对于凸重尾族，PDMP方法可更高效；Forward Event - Chain和Coordinate Samplers在相同假设下能以常数阶期望跳跃次数到达典型集；给出Zig - Zag Sampler在不同条件下的方向选择规则。

Conclusion: 研究结果为PDMP采样器在大规模推理中的应用提供理论见解和实践指导。

Abstract: Piecewise Deterministic Markov Processes (PDMPs) such as the Bouncy Particle
Sampler and the Zig-Zag Sampler, have gained attention as continuous-time
counterparts of classical Markov chain Monte Carlo. We study their transient
regime under convex potentials, namely how trajectories that start in
low-probability regions move toward higher-probability sets. Using fluid-limit
arguments with a decomposition of the generator into fast and slow parts, we
obtain deterministic ordinary differential equation descriptions of early-stage
behaviour. The fast dynamics alone are non-ergodic because once the event rate
reaches zero it does not restart. The slow component reactivates the dynamics,
so averaging remains valid when taken over short micro-cycles rather than with
respect to an invariant law.
  Using the expected number of jump events as a cost proxy for gradient
evaluations, we find that for Gaussian targets the transient cost of PDMP
methods is comparable to that of random-walk Metropolis. For convex
heavy-tailed families with subquadratic growth, PDMP methods can be more
efficient when event simulation is implemented well. Forward Event-Chain and
Coordinate Samplers can, under the same assumptions, reach the typical set with
an order-one expected number of jumps. For the Zig-Zag Sampler we show that,
under a diagonal-dominance condition, the transient choice of direction
coincides with the solution of a box-constrained quadratic program; outside
that regime we give a formal derivation and a piecewise-smooth update rule that
clarifies the roles of the gradient and the Hessian. These results provide
theoretical insight and practical guidance for the use of PDMP samplers in
large-scale inference.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [161] [ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching](https://arxiv.org/abs/2509.15942)
*Graham Clyne,Guillaume Couairon,Guillaume Gastineau,Claire Monteleoni,Anastase Charantonis*

Main category: physics.ao-ph

TL;DR: 提出基于深度学习的气候模型模拟器ArchesClimate，可降低气候模型模拟成本，模拟结果稳定且与IPSL模型可互换。


<details>
  <summary>Details</summary>
Motivation: 量化气候预测不确定性的传统方法计算成本高，需降低成本。

Method: 基于ArchesWeatherGen训练流匹配模型，在IPSL - CM6A - LR气候模型的年代际后报数据上训练ArchesClimate，模型生成一个月提前期的状态并可自回归模拟任意长度的气候模型。

Result: 10年内生成结果稳定且物理上一致，对多个重要气候变量的模拟与IPSL模型可互换。

Conclusion: 气候模型模拟器可显著降低气候模型模拟成本。

Abstract: Climate projections have uncertainties related to components of the climate
system and their interactions. A typical approach to quantifying these
uncertainties is to use climate models to create ensembles of repeated
simulations under different initial conditions. Due to the complexity of these
simulations, generating such ensembles of projections is computationally
expensive. In this work, we present ArchesClimate, a deep learning-based
climate model emulator that aims to reduce this cost. ArchesClimate is trained
on decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution
of approximately 2.5x1.25 degrees. We train a flow matching model following
ArchesWeatherGen, which we adapt to predict near-term climate. Once trained,
the model generates states at a one-month lead time and can be used to
auto-regressively emulate climate model simulations of any length. We show that
for up to 10 years, these generations are stable and physically consistent. We
also show that for several important climate variables, ArchesClimate generates
simulations that are interchangeable with the IPSL model. This work suggests
that climate model emulators could significantly reduce the cost of climate
model simulations.

</details>


<div id='physics.atom-ph'></div>

# physics.atom-ph [[Back]](#toc)

### [162] [Accelerating Atomic Fine Structure Determination with Graph Reinforcement Learning](https://arxiv.org/abs/2509.16184)
*M. Ding,V. -A. Darvariu,A. N. Ryabtsev,N. Hawes,J. C. Pickering*

Main category: physics.atom-ph

TL;DR: 本文提出用图强化学习自动分析原子光谱确定能级能量，在Co II和Nd II - III评估中快速计算出数百个能级能量，有望弥补当前原子数据需求缺口。


<details>
  <summary>Details</summary>
Motivation: 当前分析观测原子光谱确定原子数据效率低，难以满足天文学和聚变科学对原子数据的增长需求。

Method: 将分析过程转化为马尔可夫决策过程，用基于历史人工决策学习的奖励函数进行图强化学习。

Result: 在Co II和Nd II - III的现有谱线列表和理论计算评估中，数小时内计算出数百个能级能量，Co II中95%与已发表值一致，Nd II - III中为54 - 87%。

Conclusion: 新的人工智能方法有望弥补当前原子精细结构确定效率与原子数据需求之间的差距。

Abstract: Atomic data determined by analysis of observed atomic spectra are essential
for plasma diagnostics. For each low-ionisation open d- and f-subshell atomic
species, around $10^3$ fine structure level energies can be determined through
years of analysis of $10^4$ observable spectral lines. We propose the
automation of this task by casting the analysis procedure as a Markov decision
process and solving it by graph reinforcement learning using reward functions
learned on historical human decisions. In our evaluations on existing spectral
line lists and theoretical calculations for Co II and Nd II-III, hundreds of
level energies were computed within hours, agreeing with published values in
95% of cases for Co II and 54-87% for Nd II-III. As the current efficiency in
atomic fine structure determination struggles to meet growing atomic data
demands from astronomy and fusion science, our new artificial intelligence
approach sets the stage for closing this gap.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [163] [Implicit Kinodynamic Motion Retargeting for Human-to-humanoid Imitation Learning](https://arxiv.org/abs/2509.15443)
*Xingyu Chen,Hanyu Wu,Sikai Wu,Mingliang Zhou,Diyun Xiang,Haodong Zhang*

Main category: cs.RO

TL;DR: 提出隐式动力学运动重定向（IKMR）框架解决大规模人体运动到机器人可执行运动的转换问题，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 当前逐帧运动重定向方法缺乏可扩展性，需更高效方法将大规模人体运动转换为机器人可执行运动。

Method: 提出IKMR框架，在运动学上预训练运动拓扑特征表示和双编码器 - 解码器架构学习运动域映射，动力学上结合模仿学习与运动重定向网络优化运动轨迹。

Result: IKMR可实时实现大规模物理可行的运动重定向，可直接训练和部署全身控制器跟踪重定向轨迹，实验在仿真器和真实全尺寸人形机器人上进行。

Conclusion: 广泛的实验和评估结果验证了所提框架的有效性。

Abstract: Human-to-humanoid imitation learning aims to learn a humanoid whole-body
controller from human motion. Motion retargeting is a crucial step in enabling
robots to acquire reference trajectories when exploring locomotion skills.
However, current methods focus on motion retargeting frame by frame, which
lacks scalability. Could we directly convert large-scale human motion into
robot-executable motion through a more efficient approach? To address this
issue, we propose Implicit Kinodynamic Motion Retargeting (IKMR), a novel
efficient and scalable retargeting framework that considers both kinematics and
dynamics. In kinematics, IKMR pretrains motion topology feature representation
and a dual encoder-decoder architecture to learn a motion domain mapping. In
dynamics, IKMR integrates imitation learning with the motion retargeting
network to refine motion into physically feasible trajectories. After
fine-tuning using the tracking results, IKMR can achieve large-scale physically
feasible motion retargeting in real time, and a whole-body controller could be
directly trained and deployed for tracking its retargeted trajectories. We
conduct our experiments both in the simulator and the real robot on a full-size
humanoid robot. Extensive experiments and evaluation results verify the
effectiveness of our proposed framework.

</details>


### [164] [Explainable AI-Enhanced Supervisory Control for Robust Multi-Agent Robotic Systems](https://arxiv.org/abs/2509.15491)
*Reza Pirayeshshirazinezhad,Nima Fathi*

Main category: cs.RO

TL;DR: 提出可解释AI增强的多智能体机器人监督控制框架，结合多种控制方法，在航天器编队飞行和水下航行器测试中验证了方法的可移植性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 为多智能体机器人开发一个安全、可审核且具有解释性的控制框架。

Method: 将定时自动机监督器、鲁棒连续控制和可解释预测器相结合，并通过蒙特卡罗驱动优化提供训练数据。

Result: 在航天器编队飞行和AUV测试中取得良好效果，如航天器中SMC控制器降低跟踪误差和能耗，AUV中SMC在随机水流下保持有界误差。

Conclusion: 该方法对安全关键、资源受限的多智能体机器人具有可移植性和可解释性。

Abstract: We present an explainable AI-enhanced supervisory control framework for
multi-agent robotics that combines (i) a timed-automata supervisor for safe,
auditable mode switching, (ii) robust continuous control (Lyapunov-based
controller for large-angle maneuver; sliding-mode controller (SMC) with
boundary layers for precision and disturbance rejection), and (iii) an
explainable predictor that maps mission context to gains and expected
performance (energy, error). Monte Carlo-driven optimization provides the
training data, enabling transparent real-time trade-offs.
  We validated the approach in two contrasting domains, spacecraft formation
flying and autonomous underwater vehicles (AUVs). Despite different
environments (gravity/actuator bias vs. hydrodynamic drag/currents), both share
uncertain six degrees of freedom (6-DOF) rigid-body dynamics, relative motion,
and tight tracking needs, making them representative of general robotic
systems. In the space mission, the supervisory logic selects parameters that
meet mission criteria. In AUV leader-follower tests, the same SMC structure
maintains a fixed offset under stochastic currents with bounded steady error.
In spacecraft validation, the SMC controller achieved submillimeter alignment
with 21.7% lower tracking error and 81.4% lower energy consumption compared to
Proportional-Derivative PD controller baselines. At the same time, in AUV
tests, SMC maintained bounded errors under stochastic currents. These results
highlight both the portability and the interpretability of the approach for
safety-critical, resource-constrained multi-agent robotics.

</details>


### [165] [Momentum-constrained Hybrid Heuristic Trajectory Optimization Framework with Residual-enhanced DRL for Visually Impaired Scenarios](https://arxiv.org/abs/2509.15582)
*Yuting Zeng,Zhiwen Zheng,You Zhou,JiaLing Xiao,Yongbin Yu,Manping Fan,Bo Gong,Liyong Ren*

Main category: cs.RO

TL;DR: 提出适用于视障辅助导航的动量约束混合启发式轨迹优化框架MHHTOF，结合轨迹采样生成、优化评估与残差增强DRL，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决视障场景下辅助导航问题，提升复杂辅助规划任务的鲁棒性、安全性和实时可行性。

Method: 在Frenet坐标系用三阶插值和五阶多项式及动量约束生成启发式轨迹采样簇，用残差增强的基于LSTM的Actor - Critic网络在笛卡尔坐标系自适应优化轨迹选择，采用双阶段成本建模机制。

Result: LSTM - ResB - PPO收敛更快，约为PPO基线一半训练迭代次数达稳定策略性能，奖励和训练稳定性提升，所选模型平均成本和成本方差降低，风险降低。

Conclusion: 框架在复杂辅助规划任务中有效，能提升鲁棒性、安全性和实时可行性。

Abstract: This paper proposes a momentum-constrained hybrid heuristic trajectory
optimization framework (MHHTOF) tailored for assistive navigation in visually
impaired scenarios, integrating trajectory sampling generation, optimization
and evaluation with residual-enhanced deep reinforcement learning (DRL). In the
first stage, heuristic trajectory sampling cluster (HTSC) is generated in the
Frenet coordinate system using third-order interpolation with fifth-order
polynomials and momentum-constrained trajectory optimization (MTO) constraints
to ensure smoothness and feasibility. After first stage cost evaluation, the
second stage leverages a residual-enhanced actor-critic network with LSTM-based
temporal feature modeling to adaptively refine trajectory selection in the
Cartesian coordinate system. A dual-stage cost modeling mechanism (DCMM) with
weight transfer aligns semantic priorities across stages, supporting
human-centered optimization. Experimental results demonstrate that the proposed
LSTM-ResB-PPO achieves significantly faster convergence, attaining stable
policy performance in approximately half the training iterations required by
the PPO baseline, while simultaneously enhancing both reward outcomes and
training stability. Compared to baseline method, the selected model reduces
average cost and cost variance by 30.3% and 53.3%, and lowers ego and obstacle
risks by over 77%. These findings validate the framework's effectiveness in
enhancing robustness, safety, and real-time feasibility in complex assistive
planning tasks.

</details>


### [166] [GP3: A 3D Geometry-Aware Policy with Multi-View Images for Robotic Manipulation](https://arxiv.org/abs/2509.15733)
*Quanhao Qian,Guoyang Zhao,Gongjie Zhang,Jiuniu Wang,Ran Xu,Junlong Gao,Deli Zhao*

Main category: cs.RO

TL;DR: 提出3D几何感知的机器人操作策略GP3，利用多视图输入，实验表明其优于现有方法，且能有效迁移到真实机器人。


<details>
  <summary>Details</summary>
Motivation: 有效机器人操作依赖3D场景几何理解，多视图观察是获取几何信息的直接方式，因此开发利用多视图输入的策略。

Method: GP3使用空间编码器从RGB观测中推断密集空间特征，估计深度和相机参数得到适用于操作的3D场景表示，将其与语言指令融合，通过轻量级策略头转换为连续动作。

Result: 在模拟基准测试中，GP3始终优于现有方法；能有效迁移到无深度传感器或预映射环境的真实机器人，只需最少微调。

Conclusion: GP3是一种实用的、与传感器无关的几何感知机器人操作解决方案。

Abstract: Effective robotic manipulation relies on a precise understanding of 3D scene
geometry, and one of the most straightforward ways to acquire such geometry is
through multi-view observations. Motivated by this, we present GP3 -- a 3D
geometry-aware robotic manipulation policy that leverages multi-view input. GP3
employs a spatial encoder to infer dense spatial features from RGB
observations, which enable the estimation of depth and camera parameters,
leading to a compact yet expressive 3D scene representation tailored for
manipulation. This representation is fused with language instructions and
translated into continuous actions via a lightweight policy head. Comprehensive
experiments demonstrate that GP3 consistently outperforms state-of-the-art
methods on simulated benchmarks. Furthermore, GP3 transfers effectively to
real-world robots without depth sensors or pre-mapped environments, requiring
only minimal fine-tuning. These results highlight GP3 as a practical,
sensor-agnostic solution for geometry-aware robotic manipulation.

</details>


### [167] [A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning](https://arxiv.org/abs/2509.15937)
*Shaopeng Zhai,Qi Zhang,Tianyi Zhang,Fuxian Huang,Haoran Zhang,Ming Zhou,Shengzhe Zhang,Litao Liu,Sixu Lin,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 提出VLAC通用过程奖励模型，用于解决机器人现实世界强化学习瓶颈，在多任务中提升成功率和样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决机器人现实世界强化学习中稀疏手工奖励和低效探索的瓶颈问题。

Method: 基于InternVL构建VLAC模型，在大规模异构数据集上训练，结合多种数据强化能力，通过提示控制统一批评家和策略，采用分级人在环协议。

Result: 在四个不同现实操作任务中，200次交互内成功率从约30%提升到约90%，加入人在环干预样本效率再提升50%，最终成功率达100%。

Conclusion: VLAC模型和分级人在环协议能有效提升机器人现实世界强化学习的成功率和样本效率。

Abstract: Robotic real-world reinforcement learning (RL) with vision-language-action
(VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient
exploration. We introduce VLAC, a general process reward model built upon
InternVL and trained on large scale heterogeneous datasets. Given pairwise
observations and a language goal, it outputs dense progress delta and done
signal, eliminating task-specific reward engineering, and supports one-shot
in-context transfer to unseen tasks and environments. VLAC is trained on
vision-language datasets to strengthen perception, dialogic and reasoning
capabilities, together with robot and human trajectories data that ground
action generation and progress estimation, and additionally strengthened to
reject irrelevant prompts as well as detect regression or stagnation by
constructing large numbers of negative and semantically mismatched samples.
With prompt control, a single VLAC model alternately generating reward and
action tokens, unifying critic and policy. Deployed inside an asynchronous
real-world RL loop, we layer a graded human-in-the-loop protocol (offline
demonstration replay, return and explore, human guided explore) that
accelerates exploration and stabilizes early learning. Across four distinct
real-world manipulation tasks, VLAC lifts success rates from about 30\% to
about 90\% within 200 real-world interaction episodes; incorporating
human-in-the-loop interventions yields a further 50% improvement in sample
efficiency and achieves up to 100% final success.

</details>


### [168] [Compose by Focus: Scene Graph-based Atomic Skills](https://arxiv.org/abs/2509.16053)
*Han Qi,Changhe Chen,Heng Yang*

Main category: cs.RO

TL;DR: 本文引入基于场景图的表示和学习框架，结合VLM任务规划器，在模拟和真实操作任务中提高了长程任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 通用机器人需具备组合泛化能力，但现有工作中单个技能的鲁棒执行具有挑战性，视觉运动策略在场景组成导致的分布偏移下易失败。

Method: 引入基于场景图的表示，开发场景图技能学习框架，将图神经网络与基于扩散的模仿学习相结合，并将“聚焦”的场景图技能与基于视觉语言模型（VLM）的任务规划器相结合。

Result: 在模拟和真实操作任务的实验中，成功率比现有最先进的基线大幅提高。

Conclusion: 所提方法提高了长程任务的鲁棒性和组合泛化能力。

Abstract: A key requirement for generalist robots is compositional generalization - the
ability to combine atomic skills to solve complex, long-horizon tasks. While
prior work has primarily focused on synthesizing a planner that sequences
pre-learned skills, robust execution of the individual skills themselves
remains challenging, as visuomotor policies often fail under distribution
shifts induced by scene composition. To address this, we introduce a scene
graph-based representation that focuses on task-relevant objects and relations,
thereby mitigating sensitivity to irrelevant variation. Building on this idea,
we develop a scene-graph skill learning framework that integrates graph neural
networks with diffusion-based imitation learning, and further combine "focused"
scene-graph skills with a vision-language model (VLM) based task planner.
Experiments in both simulation and real-world manipulation tasks demonstrate
substantially higher success rates than state-of-the-art baselines,
highlighting improved robustness and compositional generalization in
long-horizon tasks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [169] [Recent Advancements in Microscopy Image Enhancement using Deep Learning: A Survey](https://arxiv.org/abs/2509.15363)
*Debasish Dutta,Neeharika Sonowal,Risheraj Barauh,Deepjyoti Chetia,Sanjib Kr Kalita*

Main category: eess.IV

TL;DR: 本文对借助深度学习的显微镜图像增强方法进行综述，涵盖发展、应用、挑战和未来方向，聚焦超分辨率、重建和去噪领域。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习在显微镜图像增强领域发展迅速，需要对该领域的先进方法进行总结。

Method: 对基于深度学习的显微镜图像增强方法进行综述，围绕超分辨率、重建和去噪领域展开核心讨论。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论。

Abstract: Microscopy image enhancement plays a pivotal role in understanding the
details of biological cells and materials at microscopic scales. In recent
years, there has been a significant rise in the advancement of microscopy image
enhancement, specifically with the help of deep learning methods. This survey
paper aims to provide a snapshot of this rapidly growing state-of-the-art
method, focusing on its evolution, applications, challenges, and future
directions. The core discussions take place around the key domains of
microscopy image enhancement of super-resolution, reconstruction, and
denoising, with each domain explored in terms of its current trends and their
practical utility of deep learning.

</details>


### [170] [The Missing Piece: A Case for Pre-Training in 3D Medical Object Detection](https://arxiv.org/abs/2509.15947)
*Katharina Eckstein,Constantin Ulrich,Michael Baumgartner,Jessica Kächele,Dimitrios Bounias,Tassilo Wald,Ralf Floca,Klaus H. Maier-Hein*

Main category: eess.IV

TL;DR: 本文对现有预训练方法集成到3D医学目标检测架构进行系统研究，发现预训练能提升检测性能，基于重建的自监督预训练效果更好，对比预训练无明显优势。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练在3D医学目标检测中未充分探索，现有方法未充分利用3D体信息。

Method: 对现有预训练方法集成到CNN和Transformer等先进检测架构进行系统研究。

Result: 预训练能提升不同任务和数据集的检测性能，基于重建的自监督预训练优于监督预训练，对比预训练无明显优势。

Conclusion: 预训练对3D医学目标检测有积极作用，基于重建的自监督预训练更值得采用。

Abstract: Large-scale pre-training holds the promise to advance 3D medical object
detection, a crucial component of accurate computer-aided diagnosis. Yet, it
remains underexplored compared to segmentation, where pre-training has already
demonstrated significant benefits. Existing pre-training approaches for 3D
object detection rely on 2D medical data or natural image pre-training, failing
to fully leverage 3D volumetric information. In this work, we present the first
systematic study of how existing pre-training methods can be integrated into
state-of-the-art detection architectures, covering both CNNs and Transformers.
Our results show that pre-training consistently improves detection performance
across various tasks and datasets. Notably, reconstruction-based
self-supervised pre-training outperforms supervised pre-training, while
contrastive pre-training provides no clear benefit for 3D medical object
detection. Our code is publicly available at:
https://github.com/MIC-DKFZ/nnDetection-finetuning.

</details>


### [171] [PRISM: Probabilistic and Robust Inverse Solver with Measurement-Conditioned Diffusion Prior for Blind Inverse Problems](https://arxiv.org/abs/2509.16106)
*Yuanyun Hu,Evan Bell,Guijin Wang,Yu Sun*

Main category: eess.IV

TL;DR: 提出PRISM解决盲逆问题，实验验证其有效性和优越性能


<details>
  <summary>Details</summary>
Motivation: 多数基于扩散的逆求解器需完全了解前向算子，难以解决盲逆问题

Method: 将强大的测量条件扩散模型纳入理论上有原则的后验采样方案

Result: 在盲图像去模糊实验中，PRISM在图像和模糊核恢复方面优于现有基线

Conclusion: PRISM能有效解决盲逆问题，是对现有方法的技术改进

Abstract: Diffusion models are now commonly used to solve inverse problems in
computational imaging. However, most diffusion-based inverse solvers require
complete knowledge of the forward operator to be used. In this work, we
introduce a novel probabilistic and robust inverse solver with
measurement-conditioned diffusion prior (PRISM) to effectively address blind
inverse problems. PRISM offers a technical advancement over current methods by
incorporating a powerful measurement-conditioned diffusion model into a
theoretically principled posterior sampling scheme. Experiments on blind image
deblurring validate the effectiveness of the proposed method, demonstrating the
superior performance of PRISM over state-of-the-art baselines in both image and
blur kernel recovery.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [172] [Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning](https://arxiv.org/abs/2509.15250)
*Wenda Qin,Andrea Burns,Bryan A. Plummer,Margrit Betke*

Main category: cs.CV

TL;DR: 提出Navigation - Aware Pruning (NAP) 方法用于VLN任务，在标准基准测试中表现优于先前工作，节省超50%FLOPS且保持高成功率。


<details>
  <summary>Details</summary>
Motivation: 大型模型在VLN任务运行成本高，现有Token修剪方法忽视VLN特定挑战，无法有效识别无用Token。

Method: 提出NAP方法，利用导航特定特征将Token预过滤为前景和背景，提取导航相关指令，聚焦修剪背景Token，移除低重要性导航节点。

Result: 在标准VLN基准测试中，NAP显著优于先前工作，节省超50%FLOPS并保持高成功率。

Conclusion: NAP方法有效解决了VLN任务中Token修剪的问题，实现了效率和性能的平衡。

Abstract: Large models achieve strong performance on Vision-and-Language Navigation
(VLN) tasks, but are costly to run in resource-limited environments. Token
pruning offers appealing tradeoffs for efficiency with minimal performance loss
by reducing model input size, but prior work overlooks VLN-specific challenges.
For example, information loss from pruning can effectively increase
computational cost due to longer walks. Thus, the inability to identify
uninformative tokens undermines the supposed efficiency gains from pruning. To
address this, we propose Navigation-Aware Pruning (NAP), which uses
navigation-specific traits to simplify the pruning process by pre-filtering
tokens into foreground and background. For example, image views are filtered
based on whether the agent can navigate in that direction. We also extract
navigation-relevant instructions using a Large Language Model. After filtering,
we focus pruning on background tokens, minimizing information loss. To further
help avoid increases in navigation length, we discourage backtracking by
removing low-importance navigation nodes. Experiments on standard VLN
benchmarks show NAP significantly outperforms prior work, preserving higher
success rates while saving more than 50% FLOPS.

</details>


### [173] [Autoguided Online Data Curation for Diffusion Model Training](https://arxiv.org/abs/2509.15267)
*Valeria Pais,Luis Oala,Daniele Faccio,Marco Aversa*

Main category: cs.CV

TL;DR: 研究自引导和在线数据选择方法对生成扩散模型训练效率的影响，发现自引导能提升样本质量和多样性，早期AJEST在数据效率上有表现但有局限。


<details>
  <summary>Details</summary>
Motivation: 探究近期开发的自引导和在线数据选择方法能否提高生成扩散模型训练的时间和样本效率。

Method: 将联合示例选择（JEST）和自引导集成到统一代码库进行快速消融和基准测试，在二维合成数据生成和图像生成任务上评估数据管理组合。

Result: 自引导持续提升样本质量和多样性，早期AJEST在数据效率上可与自引导相当或略优，但时间开销和复杂度大。

Conclusion: 有针对性的在线选择在早期训练有效率提升，但样本质量提升主要由自引导驱动，还讨论了数据选择的适用情况。

Abstract: The costs of generative model compute rekindled promises and hopes for
efficient data curation. In this work, we investigate whether recently
developed autoguidance and online data selection methods can improve the time
and sample efficiency of training generative diffusion models. We integrate
joint example selection (JEST) and autoguidance into a unified code base for
fast ablation and benchmarking. We evaluate combinations of data curation on a
controlled 2-D synthetic data generation task as well as (3x64x64)-D image
generation. Our comparisons are made at equal wall-clock time and equal number
of samples, explicitly accounting for the overhead of selection. Across
experiments, autoguidance consistently improves sample quality and diversity.
Early AJEST (applying selection only at the beginning of training) can match or
modestly exceed autoguidance alone in data efficiency on both tasks. However,
its time overhead and added complexity make autoguidance or uniform random data
selection preferable in most situations. These findings suggest that while
targeted online selection can yield efficiency gains in early training, robust
sample quality improvements are primarily driven by autoguidance. We discuss
limitations and scope, and outline when data selection may be beneficial.

</details>


### [174] [PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images](https://arxiv.org/abs/2509.15270)
*Emanuele Ricco,Elia Onofri,Lorenzo Cima,Stefano Cresci,Roberto Di Pietro*

Main category: cs.CV

TL;DR: 文章提出PRISM框架用于AI生成图像的指纹识别，构建PRISM - 36K数据集，在多任务上取得高准确率，证明频域指纹识别有效。


<details>
  <summary>Details</summary>
Motivation: 生成式AI需要能识别AI生成内容来源模型的归因方法，商业场景对此需求更敏感。

Method: 引入PRISM框架，基于离散傅里叶变换的径向缩减，利用幅度和相位信息捕捉模型特定特征，通过线性判别分析聚类；构建PRISM - 36K数据集。

Result: 在PRISM - 36K数据集上归因准确率92.04%；在四个基准测试中平均准确率81.60%；在真假图像检测任务中平均准确率88.41%，在GenImage上准确率达95.06%。

Conclusion: 频域指纹识别对跨架构和跨数据集的模型归因有效，为生成式AI系统提供了确保问责和信任的可行方案。

Abstract: A critical need has emerged for generative AI: attribution methods. That is,
solutions that can identify the model originating AI-generated content. This
feature, generally relevant in multimodal applications, is especially sensitive
in commercial settings where users subscribe to paid proprietary services and
expect guarantees about the source of the content they receive. To address
these issues, we introduce PRISM, a scalable Phase-enhanced Radial-based Image
Signature Mapping framework for fingerprinting AI-generated images. PRISM is
based on a radial reduction of the discrete Fourier transform that leverages
amplitude and phase information to capture model-specific signatures. The
output of the above process is subsequently clustered via linear discriminant
analysis to achieve reliable model attribution in diverse settings, even if the
model's internal details are inaccessible. To support our work, we construct
PRISM-36K, a novel dataset of 36,000 images generated by six text-to-image GAN-
and diffusion-based models. On this dataset, PRISM achieves an attribution
accuracy of 92.04%. We additionally evaluate our method on four benchmarks from
the literature, reaching an average accuracy of 81.60%. Finally, we evaluate
our methodology also in the binary task of detecting real vs fake images,
achieving an average accuracy of 88.41%. We obtain our best result on GenImage
with an accuracy of 95.06%, whereas the original benchmark achieved 82.20%. Our
results demonstrate the effectiveness of frequency-domain fingerprinting for
cross-architecture and cross-dataset model attribution, offering a viable
solution for enforcing accountability and trust in generative AI systems.

</details>


### [175] [Large Vision Models Can Solve Mental Rotation Problems](https://arxiv.org/abs/2509.15271)
*Sebastian Ray Mason,Anders Gjølbye,Phillip Chavarria Højbjerg,Lenka Tětková,Lars Kai Hansen*

Main category: cs.CV

TL;DR: 对ViT、CLIP、DINOv2和DINOv3在一系列心理旋转任务中进行系统评估，发现自监督ViTs表现更好等结论。


<details>
  <summary>Details</summary>
Motivation: 尽管现代视觉Transformer取得成功，但不清楚其是否具备人类心理旋转能力，需进行评估。

Method: 对多种模型在不同心理旋转任务中进行评估，逐层探测模型表示。

Result: 自监督ViTs比监督ViTs更好捕捉几何结构；中间层比最终层表现好；任务难度随旋转复杂度和遮挡增加。

Conclusion: 模型在心理旋转任务中的表现有一定规律，且与人类反应时间有相似约束。

Abstract: Mental rotation is a key test of spatial reasoning in humans and has been
central to understanding how perception supports cognition. Despite the success
of modern vision transformers, it is still unclear how well these models
develop similar abilities. In this work, we present a systematic evaluation of
ViT, CLIP, DINOv2, and DINOv3 across a range of mental-rotation tasks, from
simple block structures similar to those used by Shepard and Metzler to study
human cognition, to more complex block figures, three types of text, and
photo-realistic objects. By probing model representations layer by layer, we
examine where and how these networks succeed. We find that i) self-supervised
ViTs capture geometric structure better than supervised ViTs; ii) intermediate
layers perform better than final layers; iii) task difficulty increases with
rotation complexity and occlusion, mirroring human reaction times and
suggesting similar constraints in embedding space representations.

</details>


### [176] [Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception](https://arxiv.org/abs/2509.15333)
*Yulin Wang,Yang Yue,Yang Yue,Huanqian Wang,Haojun Jiang,Yizeng Han,Zanlin Ni,Yifan Pu,Minglei Shi,Rui Lu,Qisen Yang,Andrew Zhao,Zhuofan Xia,Shiji Song,Gao Huang*

Main category: cs.CV

TL;DR: 介绍AdaptiveNN框架，实现从被动到主动自适应视觉模型转变，评估显示其可降低推理成本、适应不同任务，具人类感知行为。


<details>
  <summary>Details</summary>
Motivation: 人类视觉自适应，而现有机器视觉模型被动处理场景，资源需求大，限制发展与应用，需转变范式。

Method: 将视觉感知作为从粗到细的顺序决策过程，结合表征学习与自奖励强化学习进行端到端训练。

Result: 在17个基准测试中，AdaptiveNN可实现28倍推理成本降低，适应不同任务，具更好解释性，在多案例中有人类感知行为。

Conclusion: AdaptiveNN为高效、灵活、可解释的计算机视觉提供了有前景的方向，也可用于研究视觉认知。

Abstract: Human vision is highly adaptive, efficiently sampling intricate environments
by sequentially fixating on task-relevant regions. In contrast, prevailing
machine vision models passively process entire scenes at once, resulting in
excessive resource demands scaling with spatial-temporal input resolution and
model size, yielding critical limitations impeding both future advancements and
real-world application. Here we introduce AdaptiveNN, a general framework
aiming to drive a paradigm shift from 'passive' to 'active, adaptive' vision
models. AdaptiveNN formulates visual perception as a coarse-to-fine sequential
decision-making process, progressively identifying and attending to regions
pertinent to the task, incrementally combining information across fixations,
and actively concluding observation when sufficient. We establish a theory
integrating representation learning with self-rewarding reinforcement learning,
enabling end-to-end training of the non-differentiable AdaptiveNN without
additional supervision on fixation locations. We assess AdaptiveNN on 17
benchmarks spanning 9 tasks, including large-scale visual recognition,
fine-grained discrimination, visual search, processing images from real driving
and medical scenarios, language-driven embodied AI, and side-by-side
comparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction
without sacrificing accuracy, flexibly adapts to varying task demands and
resource budgets without retraining, and provides enhanced interpretability via
its fixation patterns, demonstrating a promising avenue toward efficient,
flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits
closely human-like perceptual behaviors in many cases, revealing its potential
as a valuable tool for investigating visual cognition. Code is available at
https://github.com/LeapLabTHU/AdaptiveNN.

</details>


### [177] [Generating Part-Based Global Explanations Via Correspondence](https://arxiv.org/abs/2509.15393)
*Kunal Rathore,Prasad Tadepalli*

Main category: cs.CV

TL;DR: 提出利用有限图像的用户定义部分标签，高效转移到更大数据集，生成大规模全局符号解释的方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型不透明，现有解释方法存在局限，局部解释缺乏全局视角，概念解释标注成本高。

Method: 利用有限图像的用户定义部分标签并转移到更大数据集，聚合基于部分的局部解释生成全局符号解释。

Result: 未提及具体结果。

Conclusion: 该方法能为大规模模型决策提供人类可理解的解释。

Abstract: Deep learning models are notoriously opaque. Existing explanation methods
often focus on localized visual explanations for individual images.
Concept-based explanations, while offering global insights, require extensive
annotations, incurring significant labeling cost. We propose an approach that
leverages user-defined part labels from a limited set of images and efficiently
transfers them to a larger dataset. This enables the generation of global
symbolic explanations by aggregating part-based local explanations, ultimately
providing human-understandable explanations for model decisions on a large
scale.

</details>


### [178] [ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models](https://arxiv.org/abs/2509.15435)
*Chung-En Johnny Yu,Hsuan-Chih,Chen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: 提出ORCA框架提升预训练大视觉语言模型事实准确性和对抗鲁棒性，经多场景评估有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型存在幻觉和易受对抗攻击问题，限制其在现实应用中的可靠性。

Method: 采用Observe - Reason - Critique - Act循环，借助小型视觉模型进行测试时结构化推理，查询多视觉工具、验证跨模型不一致性并迭代优化预测，存储中间推理轨迹。

Result: 在POPE幻觉基准上提升LVLM性能3.64% - 40.67%；对抗扰动下平均精度提升20.11%；结合防御技术在AMBER图像上提升1.20% - 48.00%。

Conclusion: ORCA为构建更可靠和鲁棒的多模态系统提供了有前景的途径。

Abstract: Large Vision-Language Models (LVLMs) exhibit strong multimodal capabilities
but remain vulnerable to hallucinations from intrinsic errors and adversarial
attacks from external exploitations, limiting their reliability in real-world
applications. We present ORCA, an agentic reasoning framework that improves the
factual accuracy and adversarial robustness of pretrained LVLMs through
test-time structured inference reasoning with a suite of small vision models
(less than 3B parameters). ORCA operates via an Observe--Reason--Critique--Act
loop, querying multiple visual tools with evidential questions, validating
cross-model inconsistencies, and refining predictions iteratively without
access to model internals or retraining. ORCA also stores intermediate
reasoning traces, which supports auditable decision-making. Though designed
primarily to mitigate object-level hallucinations, ORCA also exhibits emergent
adversarial robustness without requiring adversarial training or defense
mechanisms. We evaluate ORCA across three settings: (1) clean images on
hallucination benchmarks, (2) adversarially perturbed images without defense,
and (3) adversarially perturbed images with defense applied. On the POPE
hallucination benchmark, ORCA improves standalone LVLM performance by +3.64\%
to +40.67\% across different subsets. Under adversarial perturbations on POPE,
ORCA achieves an average accuracy gain of +20.11\% across LVLMs. When combined
with defense techniques on adversarially perturbed AMBER images, ORCA further
improves standalone LVLM performance, with gains ranging from +1.20\% to
+48.00\% across evaluation metrics. These results demonstrate that ORCA offers
a promising path toward building more reliable and robust multimodal systems.

</details>


### [179] [Region-Aware Deformable Convolutions](https://arxiv.org/abs/2509.15436)
*Abolfazl Saheban Maleki,Maryam Imani*

Main category: cs.CV

TL;DR: 提出区域感知可变形卷积（RAD - Conv），能增强神经网络适应复杂图像结构的能力，结合注意力机制的适应性和标准卷积的效率。


<details>
  <summary>Details</summary>
Motivation: 传统可变形卷积采样区域固定，需要一种能更好适应图像内容、结合适应性与效率的卷积算子。

Method: RAD - Conv为每个核元素使用四个边界偏移量创建灵活矩形区域，动态调整大小和形状以匹配图像内容，解耦感受野形状和核结构。

Result: RAD - Conv可精确控制感受野的宽度和高度，能捕捉局部细节和长距离依赖，即使使用小的1x1核。

Conclusion: RAD - Conv是构建更具表现力和效率的视觉模型的实用解决方案，缩小了刚性卷积架构和高计算成本的基于注意力方法之间的差距。

Abstract: We introduce Region-Aware Deformable Convolution (RAD-Conv), a new
convolutional operator that enhances neural networks' ability to adapt to
complex image structures. Unlike traditional deformable convolutions, which are
limited to fixed quadrilateral sampling areas, RAD-Conv uses four boundary
offsets per kernel element to create flexible, rectangular regions that
dynamically adjust their size and shape to match image content. This approach
allows precise control over the receptive field's width and height, enabling
the capture of both local details and long-range dependencies, even with small
1x1 kernels. By decoupling the receptive field's shape from the kernel's
structure, RAD-Conv combines the adaptability of attention mechanisms with the
efficiency of standard convolutions. This innovative design offers a practical
solution for building more expressive and efficient vision models, bridging the
gap between rigid convolutional architectures and computationally costly
attention-based methods.

</details>


### [180] [CAGE: Continuity-Aware edGE Network Unlocks Robust Floorplan Reconstruction](https://arxiv.org/abs/2509.15459)
*Yiyi Liu,Chunyang Liu,Weiqin Jiao,Bojian Wu,Fashuai Li,Biao Xiong*

Main category: cs.CV

TL;DR: 提出CAGE网络直接从点云密度图重建矢量平面图，解决传统方法的局限，实验表明其达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于角点的多边形表示对噪声和不完整观测敏感，现有线分组方法难以恢复精细几何细节，需要更鲁棒的方法。

Method: 提出以边为中心的表示方法，将每个墙段建模为有向、几何连续的边；开发双查询变压器解码器，在去噪框架中集成扰动和潜在查询。

Result: 在Structured3D和SceneCAD上实验，CAGE在房间、角点和角度的F1分数分别达99.1%、91.7%和89.3%，且有强跨数据集泛化能力。

Conclusion: CAGE网络有效解决了传统方法的局限，其架构创新有效。

Abstract: We present \textbf{CAGE} (\textit{Continuity-Aware edGE}) network, a
\textcolor{red}{robust} framework for reconstructing vector floorplans directly
from point-cloud density maps. Traditional corner-based polygon representations
are highly sensitive to noise and incomplete observations, often resulting in
fragmented or implausible layouts. Recent line grouping methods leverage
structural cues to improve robustness but still struggle to recover fine
geometric details. To address these limitations, we propose a \textit{native}
edge-centric formulation, modeling each wall segment as a directed,
geometrically continuous edge. This representation enables inference of
coherent floorplan structures, ensuring watertight, topologically valid room
boundaries while improving robustness and reducing artifacts. Towards this
design, we develop a dual-query transformer decoder that integrates perturbed
and latent queries within a denoising framework, which not only stabilizes
optimization but also accelerates convergence. Extensive experiments on
Structured3D and SceneCAD show that \textbf{CAGE} achieves state-of-the-art
performance, with F1 scores of 99.1\% (rooms), 91.7\% (corners), and 89.3\%
(angles). The method also demonstrates strong cross-dataset generalization,
underscoring the efficacy of our architectural innovations. Code and pretrained
models will be released upon acceptance.

</details>


### [181] [Self-supervised learning of imaging and clinical signatures using a multimodal joint-embedding predictive architecture](https://arxiv.org/abs/2509.15470)
*Thomas Z. Li,Aravind R. Krishnan,Lianrui Zuo,John M. Still,Kim L. Sandler,Fabien Maldonado,Thomas A. Lasko,Bennett A. Landman*

Main category: cs.CV

TL;DR: 利用自监督学习处理肺结节诊断多模态模型数据稀缺和过拟合问题，展示模型在内外数据集表现并分析不足场景。


<details>
  <summary>Details</summary>
Motivation: 解决肺结节诊断多模态模型因标记数据稀缺和过拟合导致的发展受限问题。

Method: 从纵向和多模态档案进行自监督学习，用未标记的CT扫描和电子健康记录数据进行JEPA预训练，后进行有监督微调。

Result: 内部队列中表现优于未正则化多模态模型和仅影像模型；外部队列中不如仅影像模型。开发合成环境分析JEPA表现不佳的情况。

Conclusion: 创新利用未标记多模态医学档案改进预测模型的方法，展示其在肺结节诊断中的优缺点。

Abstract: The development of multimodal models for pulmonary nodule diagnosis is
limited by the scarcity of labeled data and the tendency for these models to
overfit on the training distribution. In this work, we leverage self-supervised
learning from longitudinal and multimodal archives to address these challenges.
We curate an unlabeled set of patients with CT scans and linked electronic
health records from our home institution to power joint embedding predictive
architecture (JEPA) pretraining. After supervised finetuning, we show that our
approach outperforms an unregularized multimodal model and imaging-only model
in an internal cohort (ours: 0.91, multimodal: 0.88, imaging-only: 0.73 AUC),
but underperforms in an external cohort (ours: 0.72, imaging-only: 0.75 AUC).
We develop a synthetic environment that characterizes the context in which JEPA
may underperform. This work innovates an approach that leverages unlabeled
multimodal medical archives to improve predictive models and demonstrates its
advantages and limitations in pulmonary nodule diagnosis.

</details>


### [182] [Comparing Computational Pathology Foundation Models using Representational Similarity Analysis](https://arxiv.org/abs/2509.15482)
*Vaibhav Mishra,William Lotter*

Main category: cs.CV

TL;DR: 本文系统分析六种计算病理学基础模型的表征空间，发现模型表征结构差异、与玻片和疾病的依赖关系等，为模型改进和集成提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有研究对计算病理学基础模型学习表征的结构和变异性了解较少，需进行系统分析。

Method: 使用计算神经科学中流行的技术，通过表征相似性分析，以TCGA的H&E图像补丁为数据进行研究。

Result: UNI2和Virchow2表征结构最独特，Prov - Gigapath平均相似度最高；相同训练范式不保证更高表征相似性；模型表征对玻片依赖高、对疾病依赖低；染色归一化降低玻片依赖性；视觉 - 语言模型表征更紧凑。

Conclusion: 研究结果为提高模型对玻片特定特征的鲁棒性、指导模型集成策略提供机会，框架可扩展到医学成像领域。

Abstract: Foundation models are increasingly developed in computational pathology
(CPath) given their promise in facilitating many downstream tasks. While recent
studies have evaluated task performance across models, less is known about the
structure and variability of their learned representations. Here, we
systematically analyze the representational spaces of six CPath foundation
models using techniques popularized in computational neuroscience. The models
analyzed span vision-language contrastive learning (CONCH, PLIP, KEEP) and
self-distillation (UNI (v2), Virchow (v2), Prov-GigaPath) approaches. Through
representational similarity analysis using H&E image patches from TCGA, we find
that UNI2 and Virchow2 have the most distinct representational structures,
whereas Prov-Gigapath has the highest average similarity across models. Having
the same training paradigm (vision-only vs. vision-language) did not guarantee
higher representational similarity. The representations of all models showed a
high slide-dependence, but relatively low disease-dependence. Stain
normalization decreased slide-dependence for all models by a range of 5.5%
(CONCH) to 20.5% (PLIP). In terms of intrinsic dimensionality, vision-language
models demonstrated relatively compact representations, compared to the more
distributed representations of vision-only models. These findings highlight
opportunities to improve robustness to slide-specific features, inform model
ensembling strategies, and provide insights into how training paradigms shape
model representations. Our framework is extendable across medical imaging
domains, where probing the internal representations of foundation models can
help ensure effective development and deployment.

</details>


### [183] [SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters](https://arxiv.org/abs/2509.15490)
*Abdarahmane Traore,Éric Hervet,Andy Couturier*

Main category: cs.CV

TL;DR: 提出紧凑视觉语言架构SmolRGPT，用600M参数在仓库空间推理基准测试取得有竞争力结果，展示高效可部署多模态智能潜力。


<details>
  <summary>Details</summary>
Motivation: 现有先进视觉语言模型计算和内存要求高，在资源受限环境部署困难，而这些环境需要高效和强大的空间理解能力。

Method: 提出SmolRGPT架构，结合RGB和深度线索进行区域级空间推理，采用三阶段课程逐步对齐视觉和语言特征、实现空间关系理解并适应特定任务数据集。

Result: 仅600M参数的SmolRGPT在具有挑战性的仓库空间推理基准测试中取得有竞争力的结果，表现匹配或超越更大的模型。

Conclusion: 证明在不牺牲核心空间推理能力的情况下，可在现实世界实现高效、可部署的多模态智能。

Abstract: Recent advances in vision-language models (VLMs) have enabled powerful
multimodal reasoning, but state-of-the-art approaches typically rely on
extremely large models with prohibitive computational and memory requirements.
This makes their deployment challenging in resource-constrained environments
such as warehouses, robotics, and industrial applications, where both
efficiency and robust spatial understanding are critical. In this work, we
present SmolRGPT, a compact vision-language architecture that explicitly
incorporates region-level spatial reasoning by integrating both RGB and depth
cues. SmolRGPT employs a three-stage curriculum that progressively align visual
and language features, enables spatial relationship understanding, and adapts
to task-specific datasets. We demonstrate that with only 600M parameters,
SmolRGPT achieves competitive results on challenging warehouse spatial
reasoning benchmarks, matching or exceeding the performance of much larger
alternatives. These findings highlight the potential for efficient, deployable
multimodal intelligence in real-world settings without sacrificing core spatial
reasoning capabilities. The code of the experimentation will be available at:
https://github.com/abtraore/SmolRGPT

</details>


### [184] [GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI Agents](https://arxiv.org/abs/2509.15532)
*Xianhang Ye,Yiqing Li,Wei Dai,Miancan Liu,Ziyuan Chen,Zhangye Han,Hongbo Min,Jinkui Ren,Xiantao Zhang,Wen Yang,Zhi Jin*

Main category: cs.CV

TL;DR: 提出GUI - ARP框架解决高分辨率截图中GUI接地细粒度定位问题，经实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有GUI接地方法在高分辨率截图的细粒度定位方面存在困难。

Method: 提出GUI - ARP框架，具备Adaptive Region Perception (ARP)和Adaptive Stage Controlling (ASC)，通过监督微调与基于Group Relative Policy Optimization (GRPO)的强化微调两阶段训练管道实现。

Result: GUI - ARP在具有挑战性的GUI接地基准测试中达到了最先进的性能，7B模型在ScreenSpot - Pro上准确率达60.8%，在UI - Vision基准上达30.9%。

Conclusion: GUI - ARP - 7B与开源72B模型和专有模型相比具有很强的竞争力。

Abstract: Existing GUI grounding methods often struggle with fine-grained localization
in high-resolution screenshots. To address this, we propose GUI-ARP, a novel
framework that enables adaptive multi-stage inference. Equipped with the
proposed Adaptive Region Perception (ARP) and Adaptive Stage Controlling (ASC),
GUI-ARP dynamically exploits visual attention for cropping task-relevant
regions and adapts its inference strategy, performing a single-stage inference
for simple cases and a multi-stage analysis for more complex scenarios. This is
achieved through a two-phase training pipeline that integrates supervised
fine-tuning with reinforcement fine-tuning based on Group Relative Policy
Optimization (GRPO). Extensive experiments demonstrate that the proposed
GUI-ARP achieves state-of-the-art performance on challenging GUI grounding
benchmarks, with a 7B model reaching 60.8% accuracy on ScreenSpot-Pro and 30.9%
on UI-Vision benchmark. Notably, GUI-ARP-7B demonstrates strong competitiveness
against open-source 72B models (UI-TARS-72B at 38.1%) and proprietary models.

</details>


### [185] [Diffusion-Based Cross-Modal Feature Extraction for Multi-Label Classification](https://arxiv.org/abs/2509.15553)
*Tian Lan,Yiming Zheng,Jianxin Yin*

Main category: cs.CV

TL;DR: 提出Diff - Feat框架提取图像和文本特征并融合用于多标签分类，表现优异。


<details>
  <summary>Details</summary>
Motivation: 多标签分类需要强大表征来捕捉标签交互，现有方法待提升。

Method: 从预训练扩散 - Transformer模型提取中间特征，针对视觉和语言任务确定最佳特征位置，设计启发式局部搜索算法，采用简单融合 - 线性投影。

Result: 在MS - COCO - enhanced和Visual Genome 500上取得98.6% mAP和45.7% mAP，超CNN、图和Transformer基线，形成更紧密语义簇。

Conclusion: Diff - Feat框架有效，能为多标签分类提供强大特征表征。

Abstract: Multi-label classification has broad applications and depends on powerful
representations capable of capturing multi-label interactions. We introduce
\textit{Diff-Feat}, a simple but powerful framework that extracts intermediate
features from pre-trained diffusion-Transformer models for images and text, and
fuses them for downstream tasks. We observe that for vision tasks, the most
discriminative intermediate feature along the diffusion process occurs at the
middle step and is located in the middle block in Transformer. In contrast, for
language tasks, the best feature occurs at the noise-free step and is located
in the deepest block. In particular, we observe a striking phenomenon across
varying datasets: a mysterious "Layer $12$" consistently yields the best
performance on various downstream classification tasks for images (under
DiT-XL/2-256$\times$256). We devise a heuristic local-search algorithm that
pinpoints the locally optimal "image-text"$\times$"block-timestep" pair among a
few candidates, avoiding an exhaustive grid search. A simple fusion-linear
projection followed by addition-of the selected representations yields
state-of-the-art performance: 98.6\% mAP on MS-COCO-enhanced and 45.7\% mAP on
Visual Genome 500, surpassing strong CNN, graph, and Transformer baselines by a
wide margin. t-SNE and clustering metrics further reveal that
\textit{Diff-Feat} forms tighter semantic clusters than unimodal counterparts.
The code is available at https://github.com/lt-0123/Diff-Feat.

</details>


### [186] [BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent](https://arxiv.org/abs/2509.15566)
*Shaojie Zhang,Ruoceng Zhang,Pei Fu,Shaokang Wang,Jiahui Yang,Xin Du,Shiqi Cui,Bin Qin,Ying Huang,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: 本文提出受大脑启发的人机交互框架BTL，引入两项关键技术创新，开发了GUI代理模型BTL - UI，在相关任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的人机交互自动化技术的交互逻辑与自然人类 - GUI通信模式有显著偏差，需要解决该问题。

Method: 提出BTL框架，将交互分解为三个阶段；引入Blink Data Generation和BTL Reward两项关键技术创新；开发GUI代理模型BTL - UI。

Result: BTL - UI在静态GUI理解和动态交互任务的综合基准测试中表现达到当前最优。

Conclusion: 该框架在开发高级GUI代理方面有效。

Abstract: In the field of AI-driven human-GUI interaction automation, while rapid
advances in multimodal large language models and reinforcement fine-tuning
techniques have yielded remarkable progress, a fundamental challenge persists:
their interaction logic significantly deviates from natural human-GUI
communication patterns. To fill this gap, we propose "Blink-Think-Link" (BTL),
a brain-inspired framework for human-GUI interaction that mimics the human
cognitive process between users and graphical interfaces. The system decomposes
interactions into three biologically plausible phases: (1) Blink - rapid
detection and attention to relevant screen areas, analogous to saccadic eye
movements; (2) Think - higher-level reasoning and decision-making, mirroring
cognitive planning; and (3) Link - generation of executable commands for
precise motor control, emulating human action selection mechanisms.
Additionally, we introduce two key technical innovations for the BTL framework:
(1) Blink Data Generation - an automated annotation pipeline specifically
optimized for blink data, and (2) BTL Reward -- the first rule-based reward
mechanism that enables reinforcement learning driven by both process and
outcome. Building upon this framework, we develop a GUI agent model named
BTL-UI, which demonstrates consistent state-of-the-art performance across both
static GUI understanding and dynamic interaction tasks in comprehensive
benchmarks. These results provide conclusive empirical validation of the
framework's efficacy in developing advanced GUI Agents.

</details>


### [187] [Towards Size-invariant Salient Object Detection: A Generic Evaluation and Optimization Approach](https://arxiv.org/abs/2509.15573)
*Shilong Bao,Qianqian Xu,Feiran Li,Boyu Han,Zhiyong Yang,Xiaochun Cao,Qingming Huang*

Main category: cs.CV

TL;DR: 本文研究显著目标检测（SOD）中评估协议的尺寸不变性问题，提出尺寸不变评估（SIEva）框架和优化框架（SIOpt），实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有SOD指标存在尺寸敏感性问题，导致对不同大小显著目标评估有偏差，影响性能评估和实际效果。

Method: 先揭示现有指标尺寸敏感性，提出SIEva框架单独评估各可分离组件并聚合结果，再开发遵循尺寸不变原则的SIOpt优化框架，进行理论分析。

Result: 实验证明提出的方法有效，代码开源。

Conclusion: 提出的尺寸不变评估和优化框架能解决SOD评估中的尺寸偏差问题，提升不同大小显著目标检测效果。

Abstract: This paper investigates a fundamental yet underexplored issue in Salient
Object Detection (SOD): the size-invariant property for evaluation protocols,
particularly in scenarios when multiple salient objects of significantly
different sizes appear within a single image. We first present a novel
perspective to expose the inherent size sensitivity of existing widely used SOD
metrics. Through careful theoretical derivations, we show that the evaluation
outcome of an image under current SOD metrics can be essentially decomposed
into a sum of several separable terms, with the contribution of each term being
directly proportional to its corresponding region size. Consequently, the
prediction errors would be dominated by the larger regions, while smaller yet
potentially more semantically important objects are often overlooked, leading
to biased performance assessments and practical degradation. To address this
challenge, a generic Size-Invariant Evaluation (SIEva) framework is proposed.
The core idea is to evaluate each separable component individually and then
aggregate the results, thereby effectively mitigating the impact of size
imbalance across objects. Building upon this, we further develop a dedicated
optimization framework (SIOpt), which adheres to the size-invariant principle
and significantly enhances the detection of salient objects across a broad
range of sizes. Notably, SIOpt is model-agnostic and can be seamlessly
integrated with a wide range of SOD backbones. Theoretically, we also present
generalization analysis of SOD methods and provide evidence supporting the
validity of our new evaluation protocols. Finally, comprehensive experiments
speak to the efficacy of our proposed approach. The code is available at
https://github.com/Ferry-Li/SI-SOD.

</details>


### [188] [Multimodal Learning for Fake News Detection in Short Videos Using Linguistically Verified Data and Heterogeneous Modality Fusion](https://arxiv.org/abs/2509.15578)
*Shanghong Li,Chiam Wen Qi Ruth,Hong Xu,Fang Liu*

Main category: cs.CV

TL;DR: 本文提出用于检测短视频假新闻的异质融合网络HFN，构建数据集VESV，实验显示比现有方法有提升。


<details>
  <summary>Details</summary>
Motivation: 短视频平台快速发展，当前方法难以处理动态多模态内容，需先进假新闻检测方法。

Method: 提出HFN框架，集成视频、音频和文本数据；引入决策网络动态调整模态权重，加权多模态特征融合模块处理不完整数据；构建数据集VESV。

Result: 在FakeTT和VESV数据集上，Marco F1比现有方法分别提升2.71%和4.14%。

Conclusion: 该工作为短视频平台假新闻检测提供了有效解决方案，为对抗虚假信息奠定基础。

Abstract: The rapid proliferation of short video platforms has necessitated advanced
methods for detecting fake news. This need arises from the widespread influence
and ease of sharing misinformation, which can lead to significant societal
harm. Current methods often struggle with the dynamic and multimodal nature of
short video content. This paper presents HFN, Heterogeneous Fusion Net, a novel
multimodal framework that integrates video, audio, and text data to evaluate
the authenticity of short video content. HFN introduces a Decision Network that
dynamically adjusts modality weights during inference and a Weighted
Multi-Modal Feature Fusion module to ensure robust performance even with
incomplete data. Additionally, we contribute a comprehensive dataset VESV
(VEracity on Short Videos) specifically designed for short video fake news
detection. Experiments conducted on the FakeTT and newly collected VESV
datasets demonstrate improvements of 2.71% and 4.14% in Marco F1 over
state-of-the-art methods. This work establishes a robust solution capable of
effectively identifying fake news in the complex landscape of short video
platforms, paving the way for more reliable and comprehensive approaches in
combating misinformation.

</details>


### [189] [Saccadic Vision for Fine-Grained Visual Classification](https://arxiv.org/abs/2509.15688)
*Johann Schmidt,Sebastian Stober,Joachim Denzler,Paul Bodesheim*

Main category: cs.CV

TL;DR: 提出受人类扫视视觉启发的两阶段方法用于细粒度视觉分类，在多个数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于部分的细粒度视觉分类方法存在依赖复杂定位网络、特征利用受限、采样点空间冗余大等问题。

Method: 提出两阶段过程，先提取周边特征生成样本图，并行采样并编码注视块，用上下文选择性注意力加权，采样时用非极大值抑制消除冗余。

Result: 在标准FGVC基准和挑战性昆虫数据集上，该方法与现有最优方法性能相当，且始终优于基线编码器。

Conclusion: 所提方法在细粒度视觉分类任务中有较好效果。

Abstract: Fine-grained visual classification (FGVC) requires distinguishing between
visually similar categories through subtle, localized features - a task that
remains challenging due to high intra-class variability and limited inter-class
differences. Existing part-based methods often rely on complex localization
networks that learn mappings from pixel to sample space, requiring a deep
understanding of image content while limiting feature utility for downstream
tasks. In addition, sampled points frequently suffer from high spatial
redundancy, making it difficult to quantify the optimal number of required
parts. Inspired by human saccadic vision, we propose a two-stage process that
first extracts peripheral features (coarse view) and generates a sample map,
from which fixation patches are sampled and encoded in parallel using a
weight-shared encoder. We employ contextualized selective attention to weigh
the impact of each fixation patch before fusing peripheral and focus
representations. To prevent spatial collapse - a common issue in part-based
methods - we utilize non-maximum suppression during fixation sampling to
eliminate redundancy. Comprehensive evaluation on standard FGVC benchmarks
(CUB-200-2011, NABirds, Food-101 and Stanford-Dogs) and challenging insect
datasets (EU-Moths, Ecuador-Moths and AMI-Moths) demonstrates that our method
achieves comparable performance to state-of-the-art approaches while
consistently outperforming our baseline encoder.

</details>


### [190] [SGMAGNet: A Baseline Model for 3D Cloud Phase Structure Reconstruction on a New Passive Active Satellite Benchmark](https://arxiv.org/abs/2509.15706)
*Chi Yang,Fu Wang,Xiaofei Yang,Hao Huang,Weijia Cao,Xiaowen Chu*

Main category: cs.CV

TL;DR: 本文提出基准数据集和框架，将多模态卫星观测转化为3D云相结构，采用SGMAGNet模型，其性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 云相廓线对数值天气预报至关重要，旨在实现业务化云相廓线反演并与NWP系统集成，改进云微物理参数化。

Method: 使用静止卫星的VIS/TIR图像和星载激光雷达、雷达的垂直云相廓线构建数据集，定义监督学习任务；采用SGMAGNet模型并与UNet变体、SegNet等基线架构对比，用标准分类指标评估性能。

Result: SGMAGNet在云相重建中表现出色，尤其在复杂多层和边界过渡区域；各项关键指标显著优于所有基线模型，Precision为0.922，Recall为0.858，F1 - score为0.763，IoU为0.617。

Conclusion: SGMAGNet模型在将多模态卫星观测转化为3D云相结构方面具有良好性能，可用于云相廓线反演及相关研究。

Abstract: Cloud phase profiles are critical for numerical weather prediction (NWP), as
they directly affect radiative transfer and precipitation processes. In this
study, we present a benchmark dataset and a baseline framework for transforming
multimodal satellite observations into detailed 3D cloud phase structures,
aiming toward operational cloud phase profile retrieval and future integration
with NWP systems to improve cloud microphysics parameterization. The multimodal
observations consist of (1) high--spatiotemporal--resolution, multi-band
visible (VIS) and thermal infrared (TIR) imagery from geostationary satellites,
and (2) accurate vertical cloud phase profiles from spaceborne lidar
(CALIOP\slash CALIPSO) and radar (CPR\slash CloudSat). The dataset consists of
synchronized image--profile pairs across diverse cloud regimes, defining a
supervised learning task: given VIS/TIR patches, predict the corresponding 3D
cloud phase structure. We adopt SGMAGNet as the main model and compare it with
several baseline architectures, including UNet variants and SegNet, all
designed to capture multi-scale spatial patterns. Model performance is
evaluated using standard classification metrics, including Precision, Recall,
F1-score, and IoU. The results demonstrate that SGMAGNet achieves superior
performance in cloud phase reconstruction, particularly in complex multi-layer
and boundary transition regions. Quantitatively, SGMAGNet attains a Precision
of 0.922, Recall of 0.858, F1-score of 0.763, and an IoU of 0.617,
significantly outperforming all baselines across these key metrics.

</details>


### [191] [FloorSAM: SAM-Guided Floorplan Reconstruction with Semantic-Geometric Fusion](https://arxiv.org/abs/2509.15750)
*Han Ye,Haofu Wang,Yunchi Zhang,Jiangjian Xiao,Yuqiang Jin,Jinyuan Liu,Wen-An Zhang,Uladzislau Sychou,Alexander Tuzikov,Vladislav Sobolevskii,Valerii Zakharov,Boris Sokolov,Minglei Fu*

Main category: cs.CV

TL;DR: 提出FloorSAM框架，结合点云密度图与SAM模型从LiDAR数据精确重建建筑平面图，在数据集测试中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统建筑平面图重建方法存在噪声处理、泛化能力和几何细节丢失等问题。

Method: 创建鲁棒的自上而下密度图，利用SAM的零样本学习进行精确房间分割，通过自适应提示点和多级过滤生成房间掩码，联合掩码和点云分析进行轮廓提取和正则化。

Result: 在Giblayout和ISPRS数据集测试中，相比传统方法有更好的准确性、召回率和鲁棒性，尤其在噪声和复杂场景。

Conclusion: FloorSAM框架能准确重建建筑平面图并恢复房间拓扑关系，性能优于传统方法。

Abstract: Reconstructing building floor plans from point cloud data is key for indoor
navigation, BIM, and precise measurements. Traditional methods like geometric
algorithms and Mask R-CNN-based deep learning often face issues with noise,
limited generalization, and loss of geometric details. We propose FloorSAM, a
framework that integrates point cloud density maps with the Segment Anything
Model (SAM) for accurate floor plan reconstruction from LiDAR data. Using
grid-based filtering, adaptive resolution projection, and image enhancement, we
create robust top-down density maps. FloorSAM uses SAM's zero-shot learning for
precise room segmentation, improving reconstruction across diverse layouts.
Room masks are generated via adaptive prompt points and multistage filtering,
followed by joint mask and point cloud analysis for contour extraction and
regularization. This produces accurate floor plans and recovers room
topological relationships. Tests on Giblayout and ISPRS datasets show better
accuracy, recall, and robustness than traditional methods, especially in noisy
and complex settings. Code and materials: github.com/Silentbarber/FloorSAM.

</details>


### [192] [Ideal Registration? Segmentation is All You Need](https://arxiv.org/abs/2509.15784)
*Xiang Chen,Fengting Zhang,Qinghao Liu,Min Liu,Kun Wu,Yaonan Wang,Hang Zhang*

Main category: cs.CV

TL;DR: 提出SegReg框架解决现有图像配准方法平滑约束问题，在多临床场景表现优，配准精度与分割质量近线性相关。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习图像配准方法采用全局统一平滑约束，无法适应解剖运动复杂的区域变形。

Method: 提出SegReg框架，先将图像分割为解剖学连贯子区域，用同一配准骨干处理各子区域计算局部变形场，再整合为全局变形场。

Result: 使用真实分割时在关键解剖结构上Dice系数达98.23%，在三种临床配准场景中比现有方法高2 - 12%，配准精度与分割质量近线性相关。

Conclusion: SegReg框架有效，将配准挑战转化为分割问题。

Abstract: Deep learning has revolutionized image registration by its ability to handle
diverse tasks while achieving significant speed advantages over conventional
approaches. Current approaches, however, often employ globally uniform
smoothness constraints that fail to accommodate the complex, regionally varying
deformations characteristic of anatomical motion. To address this limitation,
we propose SegReg, a Segmentation-driven Registration framework that implements
anatomically adaptive regularization by exploiting region-specific deformation
patterns. Our SegReg first decomposes input moving and fixed images into
anatomically coherent subregions through segmentation. These localized domains
are then processed by the same registration backbone to compute optimized
partial deformation fields, which are subsequently integrated into a global
deformation field. SegReg achieves near-perfect structural alignment (98.23%
Dice on critical anatomies) using ground-truth segmentation, and outperforms
existing methods by 2-12% across three clinical registration scenarios
(cardiac, abdominal, and lung images) even with automatic segmentation. Our
SegReg demonstrates a near-linear dependence of registration accuracy on
segmentation quality, transforming the registration challenge into a
segmentation problem. The source code will be released upon manuscript
acceptance.

</details>


### [193] [CBPNet: A Continual Backpropagation Prompt Network for Alleviating Plasticity Loss on Edge Devices](https://arxiv.org/abs/2509.15785)
*Runjie Shao,Boyu Diao,Zijia An,Ruiqi Liu,Yongjun Xu*

Main category: cs.CV

TL;DR: 为满足实时响应动态环境应用需求，提出CBPNet框架恢复模型学习活力，实验证明其在边缘设备上有效。


<details>
  <summary>Details</summary>
Motivation: 现有使用冻结预训练模型加提示的策略存在可塑性损失问题，需解决模型学习新知识能力下降的问题。

Method: 提出Continual Backpropagation Prompt Network (CBPNet)框架，集成Efficient CBP Block自适应重新初始化未充分利用的参数。

Result: 在边缘设备多个基准测试中有效，在Split CIFAR - 100上平均准确率比强基线提高超1%，在Split ImageNet - R上达到69.41%的先进准确率，且额外训练参数不到主干网络的0.2%。

Conclusion: 所提出的CBPNet框架有效，验证了通过重新初始化未充分利用参数恢复模型学习活力的方法。

Abstract: To meet the demands of applications like robotics and autonomous driving that
require real-time responses to dynamic environments, efficient continual
learning methods suitable for edge devices have attracted increasing attention.
In this transition, using frozen pretrained models with prompts has become a
mainstream strategy to combat catastrophic forgetting. However, this approach
introduces a new critical bottleneck: plasticity loss, where the model's
ability to learn new knowledge diminishes due to the frozen backbone and the
limited capacity of prompt parameters. We argue that the reduction in
plasticity stems from a lack of update vitality in underutilized parameters
during the training process. To this end, we propose the Continual
Backpropagation Prompt Network (CBPNet), an effective and parameter efficient
framework designed to restore the model's learning vitality. We innovatively
integrate an Efficient CBP Block that counteracts plasticity decay by
adaptively reinitializing these underutilized parameters. Experimental results
on edge devices demonstrate CBPNet's effectiveness across multiple benchmarks.
On Split CIFAR-100, it improves average accuracy by over 1% against a strong
baseline, and on the more challenging Split ImageNet-R, it achieves a state of
the art accuracy of 69.41%. This is accomplished by training additional
parameters that constitute less than 0.2% of the backbone's size, validating
our approach.

</details>


### [194] [ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding](https://arxiv.org/abs/2509.15800)
*Kehua Chen*

Main category: cs.CV

TL;DR: 提出ChronoForge - RL框架结合TAD和KF - GRPO解决视频理解难题，在数据集上超基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法存在处理密集视频帧计算不可行和朴素采样难识别重要帧的问题。

Method: 提出ChronoForge - RL框架，引入可微关键帧选择机制，用TAD选信息帧，用KF - GRPO实现对比学习与显著增强奖励机制。

Result: ChronoForge - RL在VideoMME达69.1%、LVBench达52.7%，超越先前方法，7B参数模型性能可比72B参数模型。

Conclusion: ChronoForge - RL框架有效解决视频理解问题，提升性能。

Abstract: Current state-of-the-art video understanding methods typically struggle with
two critical challenges: (1) the computational infeasibility of processing
every frame in dense video content and (2) the difficulty in identifying
semantically significant frames through naive uniform sampling strategies. In
this paper, we propose a novel video understanding framework, called
ChronoForge-RL, which combines Temporal Apex Distillation (TAD) and
KeyFrame-aware Group Relative Policy Optimization (KF-GRPO) to tackle these
issues. Concretely, we introduce a differentiable keyframe selection mechanism
that systematically identifies semantic inflection points through a three-stage
process to enhance computational efficiency while preserving temporal
information. Then, two particular modules are proposed to enable effective
temporal reasoning: Firstly, TAD leverages variation scoring, inflection
detection, and prioritized distillation to select the most informative frames.
Secondly, we introduce KF-GRPO which implements a contrastive learning paradigm
with a saliency-enhanced reward mechanism that explicitly incentivizes models
to leverage both frame content and temporal relationships. Finally, our
proposed ChronoForge-RL achieves 69.1% on VideoMME and 52.7% on LVBench
compared to baseline methods, clearly surpassing previous approaches while
enabling our 7B parameter model to achieve performance comparable to 72B
parameter alternatives.

</details>


### [195] [CIDER: A Causal Cure for Brand-Obsessed Text-to-Image Models](https://arxiv.org/abs/2509.15803)
*Fangjian Shen,Zifeng Liang,Chao Wang,Wushao Wen*

Main category: cs.CV

TL;DR: 论文指出文本到图像模型存在品牌偏差，提出CIDER框架在推理时减轻偏差，实验表明其有效且不影响图像质量。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型存在品牌偏差，有伦理和法律风险，需解决偏差问题。

Method: 提出CIDER框架，用轻量级检测器识别品牌内容，用视觉语言模型生成风格不同的替代方案，引入品牌中立分数量化问题。

Result: CIDER显著减少显式和隐式偏差，同时保持图像质量和美感。

Conclusion: 工作为生成更原创和公平的内容提供了实用解决方案，有助于可信生成式AI的发展。

Abstract: Text-to-image (T2I) models exhibit a significant yet under-explored "brand
bias", a tendency to generate contents featuring dominant commercial brands
from generic prompts, posing ethical and legal risks. We propose CIDER, a
novel, model-agnostic framework to mitigate bias at inference-time through
prompt refinement to avoid costly retraining. CIDER uses a lightweight detector
to identify branded content and a Vision-Language Model (VLM) to generate
stylistically divergent alternatives. We introduce the Brand Neutrality Score
(BNS) to quantify this issue and perform extensive experiments on leading T2I
models. Results show CIDER significantly reduces both explicit and implicit
biases while maintaining image quality and aesthetic appeal. Our work offers a
practical solution for more original and equitable content, contributing to the
development of trustworthy generative AI.

</details>


### [196] [Self-Supervised Cross-Modal Learning for Image-to-Point Cloud Registration](https://arxiv.org/abs/2509.15882)
*Xingmei Wang,Xiaoyu Hu,Chengkai Huang,Ziyan Zeng,Guohao Nie,Quan Z. Sheng,Lina Yao*

Main category: cs.CV

TL;DR: 提出CrossI2P自监督框架用于图像到点云配准，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 图像到点云配准存在语义 - 几何差距和现有方法易陷入局部最优的问题，需克服这些局限。

Method: 采用单端到端管道统一跨模态学习和两阶段配准，包括通过双路径对比学习学习几何 - 语义融合嵌入空间、采用粗到精配准范式、使用动态训练机制平衡损失。

Result: 在KITTI Odometry基准上比现有方法高23.7%，在nuScenes上高37.9%。

Conclusion: CrossI2P显著提高了图像到点云配准的准确性和鲁棒性。

Abstract: Bridging 2D and 3D sensor modalities is critical for robust perception in
autonomous systems. However, image-to-point cloud (I2P) registration remains
challenging due to the semantic-geometric gap between texture-rich but
depth-ambiguous images and sparse yet metrically precise point clouds, as well
as the tendency of existing methods to converge to local optima. To overcome
these limitations, we introduce CrossI2P, a self-supervised framework that
unifies cross-modal learning and two-stage registration in a single end-to-end
pipeline. First, we learn a geometric-semantic fused embedding space via
dual-path contrastive learning, enabling annotation-free, bidirectional
alignment of 2D textures and 3D structures. Second, we adopt a coarse-to-fine
registration paradigm: a global stage establishes superpoint-superpixel
correspondences through joint intra-modal context and cross-modal interaction
modeling, followed by a geometry-constrained point-level refinement for precise
registration. Third, we employ a dynamic training mechanism with gradient
normalization to balance losses for feature alignment, correspondence
refinement, and pose estimation. Extensive experiments demonstrate that
CrossI2P outperforms state-of-the-art methods by 23.7% on the KITTI Odometry
benchmark and by 37.9% on nuScenes, significantly improving both accuracy and
robustness.

</details>


### [197] [RACap: Relation-Aware Prompting for Lightweight Retrieval-Augmented Image Captioning](https://arxiv.org/abs/2509.15883)
*Xiaosheng Long,Hanyu Wang,Zhentao Song,Kun Luo,Hongde Liu*

Main category: cs.CV

TL;DR: 提出RACap模型用于图像描述，能挖掘关系语义和识别对象，参数少且性能优。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强图像描述方法在关系建模上存在语义提示粗粒度、缺乏对象和关系显式建模的问题。

Method: 提出RACap模型，挖掘检索描述中的结构化关系语义，识别图像中的异构对象，有效检索含异构视觉信息的结构化关系特征。

Result: RACap仅10.8M可训练参数，比以往轻量级描述模型性能更优。

Conclusion: RACap能提升语义一致性和关系表达能力，在图像描述任务中表现出色。

Abstract: Recent retrieval-augmented image captioning methods incorporate external
knowledge to compensate for the limitations in comprehending complex scenes.
However, current approaches face challenges in relation modeling: (1) the
representation of semantic prompts is too coarse-grained to capture
fine-grained relationships; (2) these methods lack explicit modeling of image
objects and their semantic relationships. To address these limitations, we
propose RACap, a relation-aware retrieval-augmented model for image captioning,
which not only mines structured relation semantics from retrieval captions, but
also identifies heterogeneous objects from the image. RACap effectively
retrieves structured relation features that contain heterogeneous visual
information to enhance the semantic consistency and relational expressiveness.
Experimental results show that RACap, with only 10.8M trainable parameters,
achieves superior performance compared to previous lightweight captioning
models.

</details>


### [198] [RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation](https://arxiv.org/abs/2509.15257)
*Silpa Vadakkeeveetil Sreelatha,Sauradip Nag,Muhammad Awais,Serge Belongie,Anjan Dutta*

Main category: cs.CV

TL;DR: 提出RespoDiff框架用于负责任的文本到图像生成，在保证图像保真度的同时提升公平性和安全性，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提升文本到图像生成的公平性和安全性时会牺牲语义保真度和图像质量，需要解决这一问题。

Method: 提出RespoDiff框架，对扩散模型的中间瓶颈表示进行双模块转换，引入两个可学习模块，并提出新的得分匹配目标来协调模块间学习。

Result: 方法在负责任生成方面优于现有方法，在不同未见提示下将负责任且语义连贯的生成效果提高了20%，能无缝集成到SDXL等大规模模型中。

Conclusion: RespoDiff框架有效解决了文本到图像生成中的公平性和安全性问题，且不影响图像保真度。

Abstract: The rapid advancement of diffusion models has enabled high-fidelity and
semantically rich text-to-image generation; however, ensuring fairness and
safety remains an open challenge. Existing methods typically improve fairness
and safety at the expense of semantic fidelity and image quality. In this work,
we propose RespoDiff, a novel framework for responsible text-to-image
generation that incorporates a dual-module transformation on the intermediate
bottleneck representations of diffusion models. Our approach introduces two
distinct learnable modules: one focused on capturing and enforcing responsible
concepts, such as fairness and safety, and the other dedicated to maintaining
semantic alignment with neutral prompts. To facilitate the dual learning
process, we introduce a novel score-matching objective that enables effective
coordination between the modules. Our method outperforms state-of-the-art
methods in responsible generation by ensuring semantic alignment while
optimizing both objectives without compromising image fidelity. Our approach
improves responsible and semantically coherent generation by 20% across
diverse, unseen prompts. Moreover, it integrates seamlessly into large-scale
models like SDXL, enhancing fairness and safety. Code will be released upon
acceptance.

</details>


### [199] [MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation](https://arxiv.org/abs/2509.15357)
*Yu Chang,Jiahao Chen,Anzhe Cheng,Paul Bogdan*

Main category: cs.CV

TL;DR: 提出MaskAttn - SDXL解决文本到图像扩散模型在多对象提示中的组合失败问题，提升空间合规性和属性绑定，是文本到图像生成中空间控制的实用扩展。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型在处理多对象、属性和空间关系的提示时存在组合失败问题，如交叉令牌干扰。

Method: 提出MaskAttn - SDXL，一种应用于Stable Diffusion XL的UNet交叉注意力对数的区域级门控机制，学习每层的二进制掩码并注入到交叉注意力对数图中以稀疏化令牌到潜在空间的交互。

Result: 模型在多对象提示中提高了空间合规性和属性绑定，同时保留了整体图像质量和多样性。

Conclusion: 对数级掩码交叉注意力是实施组合控制的数据高效原语，该方法是文本到图像生成中空间控制的实用扩展。

Abstract: Text-to-image diffusion models achieve impressive realism but often suffer
from compositional failures on prompts with multiple objects, attributes, and
spatial relations, resulting in cross-token interference where entities
entangle, attributes mix across objects, and spatial cues are violated. To
address these failures, we propose MaskAttn-SDXL,a region-level gating
mechanism applied to the cross-attention logits of Stable Diffusion XL(SDXL)'s
UNet. MaskAttn-SDXL learns a binary mask per layer, injecting it into each
cross-attention logit map before softmax to sparsify token-to-latent
interactions so that only semantically relevant connections remain active. The
method requires no positional encodings, auxiliary tokens, or external region
masks, and preserves the original inference path with negligible overhead. In
practice, our model improves spatial compliance and attribute binding in
multi-object prompts while preserving overall image quality and diversity.
These findings demonstrate that logit-level maksed cross-attention is an
data-efficient primitve for enforcing compositional control, and our method
thus serves as a practical extension for spatial control in text-to-image
generation.

</details>


### [200] [Shedding Light on Depth: Explainability Assessment in Monocular Depth Estimation](https://arxiv.org/abs/2509.15980)
*Lorenzo Cirillo,Claudio Schiavella,Lorenzo Papa,Paolo Russo,Irene Amerini*

Main category: cs.CV

TL;DR: 研究单目深度估计（MDE）网络可解释性，用特征归因方法分析不同复杂度模型，引入归因保真度指标，实验表明部分方法性能好且新指标有效。


<details>
  <summary>Details</summary>
Motivation: 可解释人工智能用于理解深度学习模型决策过程，但MDE的可解释性研究不足。

Method: 研究成熟特征归因方法，通过选择性扰动像素评估解释质量，引入归因保真度指标。

Result: 显著性图和积分梯度分别在轻量级和深度MDE模型中表现好，归因保真度能有效识别不可靠视觉图。

Conclusion: 部分特征归因方法在MDE模型中有良好表现，归因保真度指标可有效评估可解释性方法的可靠性。

Abstract: Explainable artificial intelligence is increasingly employed to understand
the decision-making process of deep learning models and create trustworthiness
in their adoption. However, the explainability of Monocular Depth Estimation
(MDE) remains largely unexplored despite its wide deployment in real-world
applications. In this work, we study how to analyze MDE networks to map the
input image to the predicted depth map. More in detail, we investigate
well-established feature attribution methods, Saliency Maps, Integrated
Gradients, and Attention Rollout on different computationally complex models
for MDE: METER, a lightweight network, and PixelFormer, a deep network. We
assess the quality of the generated visual explanations by selectively
perturbing the most relevant and irrelevant pixels, as identified by the
explainability methods, and analyzing the impact of these perturbations on the
model's output. Moreover, since existing evaluation metrics can have some
limitations in measuring the validity of visual explanations for MDE, we
additionally introduce the Attribution Fidelity. This metric evaluates the
reliability of the feature attribution by assessing their consistency with the
predicted depth map. Experimental results demonstrate that Saliency Maps and
Integrated Gradients have good performance in highlighting the most important
input features for MDE lightweight and deep models, respectively. Furthermore,
we show that Attribution Fidelity effectively identifies whether an
explainability method fails to produce reliable visual maps, even in scenarios
where conventional metrics might suggest satisfactory results.

</details>


### [201] [Towards Sharper Object Boundaries in Self-Supervised Depth Estimation](https://arxiv.org/abs/2509.15987)
*Aurélien Cecille,Stefan Duffner,Franck Davoine,Rémi Agier,Thibault Neveu*

Main category: cs.CV

TL;DR: 提出仅用自监督产生清晰深度不连续性的单目深度估计方法，在KITTI和VKITTIv2上效果好。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度估计方法在物体边界处深度模糊，产生虚假3D点，而实现清晰边缘通常需细粒度监督。

Method: 将每个像素深度建模为混合分布，将不确定性从直接回归转移到混合权重，通过方差感知损失函数和不确定性传播集成到现有管道。

Result: 在KITTI和VKITTIv2上评估，边界清晰度最高提升35%，点云质量提高。

Conclusion: 所提方法能有效提升单目深度估计在物体边界处的表现。

Abstract: Accurate monocular depth estimation is crucial for 3D scene understanding,
but existing methods often blur depth at object boundaries, introducing
spurious intermediate 3D points. While achieving sharp edges usually requires
very fine-grained supervision, our method produces crisp depth discontinuities
using only self-supervision. Specifically, we model per-pixel depth as a
mixture distribution, capturing multiple plausible depths and shifting
uncertainty from direct regression to the mixture weights. This formulation
integrates seamlessly into existing pipelines via variance-aware loss functions
and uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show
that our method achieves up to 35% higher boundary sharpness and improves point
cloud quality compared to state-of-the-art baselines.

</details>


### [202] [See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model](https://arxiv.org/abs/2509.16087)
*Pengteng Li,Pinhao Song,Wuyang Li,Weiyu Guo,Huizai Yao,Yijie Xu,Dugang Liu,Hui Xiong*

Main category: cs.CV

TL;DR: 提出SEE&TREK训练无关提示框架, 增强MLLMs纯视觉空间理解，实验证明能提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多引入深度或点云等模态提升空间推理，纯视觉空间理解研究不足。

Method: 聚焦增加视觉多样性和运动重建两原则，分别采用最大语义丰富度采样和模拟视觉轨迹编码相对空间位置。方法无需训练和GPU，只需单次前向传播，可集成到现有MLLMs。

Result: 在VSI - B ENCH和STI - B ENCH上实验表明，SEE&TREK能提升MLLMs在不同空间推理任务的性能，最高提升3.5%。

Conclusion: SEE&TREK为增强空间智能提供了有前景的途径。

Abstract: We introduce SEE&TREK, the first training-free prompting framework tailored
to enhance the spatial understanding of Multimodal Large Language Models
(MLLMS) under vision-only constraints. While prior efforts have incorporated
modalities like depth or point clouds to improve spatial reasoning, purely
visualspatial understanding remains underexplored. SEE&TREK addresses this gap
by focusing on two core principles: increasing visual diversity and motion
reconstruction. For visual diversity, we conduct Maximum Semantic Richness
Sampling, which employs an off-the-shell perception model to extract
semantically rich keyframes that capture scene structure. For motion
reconstruction, we simulate visual trajectories and encode relative spatial
positions into keyframes to preserve both spatial relations and temporal
coherence. Our method is training&GPU-free, requiring only a single forward
pass, and can be seamlessly integrated into existing MLLM'S. Extensive
experiments on the VSI-B ENCH and STI-B ENCH show that S EE &T REK consistently
boosts various MLLM S performance across diverse spatial reasoning tasks with
the most +3.5% improvement, offering a promising path toward stronger spatial
intelligence.

</details>


### [203] [ORIC: Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models](https://arxiv.org/abs/2509.15695)
*Zhaoyang Li,Zhan Ling,Yuchen Zhou,Hao Su*

Main category: cs.CV

TL;DR: 论文引入ORIC基准评估LVLMs在物体与上下文关系不符场景中的表现，评估结果显示存在显著识别差距，为相关研究提供见解。


<details>
  <summary>Details</summary>
Motivation: LVLMs在不协调上下文场景中易出错，存在物体误识别和幻觉问题，需系统研究。

Method: 引入ORIC基准，采用LLM引导采样识别存在但上下文不协调的物体，CLIP引导采样检测可能被幻觉出的合理但不存在的物体。

Result: 评估18个LVLMs和2个开放词汇检测模型，发现显著识别差距。

Conclusion: 指出LVLMs的局限性，鼓励开展上下文感知物体识别的进一步研究。

Abstract: Large Vision-Language Models (LVLMs) have made significant strides in image
caption, visual question answering, and robotics by integrating visual and
textual information. However, they remain prone to errors in incongruous
contexts, where objects appear unexpectedly or are absent when contextually
expected. This leads to two key recognition failures: object misidentification
and hallucination. To systematically examine this issue, we introduce the
Object Recognition in Incongruous Context Benchmark (ORIC), a novel benchmark
that evaluates LVLMs in scenarios where object-context relationships deviate
from expectations. ORIC employs two key strategies: (1) LLM-guided sampling,
which identifies objects that are present but contextually incongruous, and (2)
CLIP-guided sampling, which detects plausible yet nonexistent objects that are
likely to be hallucinated, thereby creating an incongruous context. Evaluating
18 LVLMs and two open-vocabulary detection models, our results reveal
significant recognition gaps, underscoring the challenges posed by contextual
incongruity. This work provides critical insights into LVLMs' limitations and
encourages further research on context-aware object recognition.

</details>


### [204] [Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks](https://arxiv.org/abs/2509.16163)
*Het Patel,Muzammil Allie,Qian Zhang,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.CV

TL;DR: 提出适用于预训练视觉语言模型的轻量级防御方法，无需重新训练，实验证明能提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型防御方法需昂贵的重新训练或架构改变，需轻量级防御方法。

Method: 使用张量分解和重构视觉编码器表示，过滤对抗性噪声。

Result: 在COCO和Flickr30K上实验显示提升鲁棒性，在Flickr30K恢复12.3%性能，COCO恢复8.1%性能，分析得出低秩和低残差强度的张量训练分解最优。

Conclusion: 该方法是实用、即插即用且开销小的解决方案。

Abstract: Vision language models (VLMs) excel in multimodal understanding but are prone
to adversarial attacks. Existing defenses often demand costly retraining or
significant architecture changes. We introduce a lightweight defense using
tensor decomposition suitable for any pre-trained VLM, requiring no retraining.
By decomposing and reconstructing vision encoder representations, it filters
adversarial noise while preserving meaning. Experiments with CLIP on COCO and
Flickr30K show improved robustness. On Flickr30K, it restores 12.3\%
performance lost to attacks, raising Recall@1 accuracy from 7.5\% to 19.8\%. On
COCO, it recovers 8.1\% performance, improving accuracy from 3.8\% to 11.9\%.
Analysis shows Tensor Train decomposition with low rank (8-32) and low residual
strength ($\alpha=0.1-0.2$) is optimal. This method is a practical,
plug-and-play solution with minimal overhead for existing VLMs.

</details>


### [205] [Fast OTSU Thresholding Using Bisection Method](https://arxiv.org/abs/2509.16179)
*Sai Varun Kodathala*

Main category: cs.CV

TL;DR: 本文提出利用二分法优化Otsu阈值算法，降低计算复杂度，实验验证其有效性，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: Otsu阈值算法因穷举搜索所有可能阈值，计算效率受限，需优化。

Method: 利用二分法，结合类间方差函数的单峰特性进行优化。

Result: 实验显示，相比传统穷举搜索，方差计算减少91.63%，算法迭代减少97.21%；66.67%测试用例阈值完全匹配，95.83%偏差在5个灰度级内。

Conclusion: 该优化解决了大规模图像处理系统的计算瓶颈，不影响原算法理论基础和分割质量，适合实时应用。

Abstract: The Otsu thresholding algorithm represents a fundamental technique in image
segmentation, yet its computational efficiency is severely limited by
exhaustive search requirements across all possible threshold values. This work
presents an optimized implementation that leverages the bisection method to
exploit the unimodal characteristics of the between-class variance function.
Our approach reduces the computational complexity from O(L) to O(log L)
evaluations while preserving segmentation accuracy. Experimental validation on
48 standard test images demonstrates a 91.63% reduction in variance
computations and 97.21% reduction in algorithmic iterations compared to
conventional exhaustive search. The bisection method achieves exact threshold
matches in 66.67% of test cases, with 95.83% exhibiting deviations within 5
gray levels. The algorithm maintains universal convergence within theoretical
logarithmic bounds while providing deterministic performance guarantees
suitable for real-time applications. This optimization addresses critical
computational bottlenecks in large-scale image processing systems without
compromising the theoretical foundations or segmentation quality of the
original Otsu method.

</details>


### [206] [MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer](https://arxiv.org/abs/2509.16197)
*Yanghao Li,Rui Qian,Bowen Pan,Haotian Zhang,Haoshuo Huang,Bowen Zhang,Jialing Tong,Haoxuan You,Xianzhi Du,Zhe Gan,Hyunjik Kim,Chao Jia,Zhenbang Wang,Yinfei Yang,Mingfei Gao,Zi-Yi Dou,Wenze Hu,Chang Gao,Dongxu Li,Philipp Dufter,Zirui Wang,Guoli Yin,Zhengdong Zhang,Chen Chen,Yang Zhao,Ruoming Pang,Zhifeng Chen*

Main category: cs.CV

TL;DR: 提出Manzano统一框架减少多模态大语言模型理解和生成视觉内容能力间的性能权衡，取得先进成果。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态大语言模型在理解和生成视觉内容能力上存在性能权衡问题。

Method: 将混合图像分词器与精心设计的训练方案结合，用共享视觉编码器和轻量级适配器，统一自回归大语言模型预测语义，辅助扩散解码器转换图像令牌为像素。

Result: Manzano在统一模型中取得了最先进的结果，在富文本评估中与专业模型具有竞争力。

Conclusion: 研究表明任务冲突极小，模型规模扩展有持续收益，验证了混合分词器的设计选择。

Abstract: Unified multimodal Large Language Models (LLMs) that can both understand and
generate visual content hold immense potential. However, existing open-source
models often suffer from a performance trade-off between these capabilities. We
present Manzano, a simple and scalable unified framework that substantially
reduces this tension by coupling a hybrid image tokenizer with a well-curated
training recipe. A single shared vision encoder feeds two lightweight adapters
that produce continuous embeddings for image-to-text understanding and discrete
tokens for text-to-image generation within a common semantic space. A unified
autoregressive LLM predicts high-level semantics in the form of text and image
tokens, with an auxiliary diffusion decoder subsequently translating the image
tokens into pixels. The architecture, together with a unified training recipe
over understanding and generation data, enables scalable joint learning of both
capabilities. Manzano achieves state-of-the-art results among unified models,
and is competitive with specialist models, particularly on text-rich
evaluation. Our studies show minimal task conflicts and consistent gains from
scaling model size, validating our design choice of a hybrid tokenizer.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [207] [Copycat vs. Original: Multi-modal Pretraining and Variable Importance in Box-office Prediction](https://arxiv.org/abs/2509.15277)
*Qin Chao,Eunsoo Kim,Boyang Li*

Main category: cs.MM

TL;DR: 构建多模态神经网络预测票房，降低预测误差，分析'山寨电影'商业可行性，发现山寨状态与票房正相关但随相似影片数量和内容相似度增加而减弱。


<details>
  <summary>Details</summary>
Motivation: 电影行业风险高，需自动化工具预测票房辅助决策。

Method: 构建多模态神经网络，将众包的电影描述关键词与海报视觉信息结合，计算山寨特征在票房预测中的影响。

Result: 票房预测误差降低14.5%，发现山寨状态与电影收入呈正相关，但相似影片数量和内容相似度增加时该效应减弱。

Conclusion: 开发了用于研究电影行业的深度学习工具，提供了有价值的商业见解。

Abstract: The movie industry is associated with an elevated level of risk, which
necessitates the use of automated tools to predict box-office revenue and
facilitate human decision-making. In this study, we build a sophisticated
multimodal neural network that predicts box offices by grounding crowdsourced
descriptive keywords of each movie in the visual information of the movie
posters, thereby enhancing the learned keyword representations, resulting in a
substantial reduction of 14.5% in box-office prediction error. The advanced
revenue prediction model enables the analysis of the commercial viability of
"copycat movies," or movies with substantial similarity to successful movies
released recently. We do so by computing the influence of copycat features in
box-office prediction. We find a positive relationship between copycat status
and movie revenue. However, this effect diminishes when the number of similar
movies and the similarity of their content increase. Overall, our work develops
sophisticated deep learning tools for studying the movie industry and provides
valuable business insight.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [208] [Deep Gaussian Process-based Cost-Aware Batch Bayesian Optimization for Complex Materials Design Campaigns](https://arxiv.org/abs/2509.14408)
*Sk Md Ahnaf Akif Alvi,Brent Vela,Vahid Attari,Jan Janssen,Danny Perez,Douglas Allaire,Raymundo Arroyave*

Main category: cond-mat.mtrl-sci

TL;DR: 提出基于深度高斯过程代理和异位查询策略的成本感知批量贝叶斯优化方案，应用于高温难熔高熵合金时比传统方法更优。


<details>
  <summary>Details</summary>
Motivation: 材料发现的节奏加快和范围扩大，需要能有效探索巨大非线性设计空间并合理分配有限评估资源的优化框架。

Method: 提出成本感知的批量贝叶斯优化方案，用深度高斯过程代理和异位查询策略，将评估成本集成到上置信界采集扩展中。

Result: 应用于高温难熔高熵合金时，该框架比传统基于高斯过程的贝叶斯优化在更少迭代中收敛到最优配方。

Conclusion: 深度、不确定性感知和成本敏感策略在材料探索中具有价值。

Abstract: The accelerating pace and expanding scope of materials discovery demand
optimization frameworks that efficiently navigate vast, nonlinear design spaces
while judiciously allocating limited evaluation resources. We present a
cost-aware, batch Bayesian optimization scheme powered by deep Gaussian process
(DGP) surrogates and a heterotopic querying strategy. Our DGP surrogate, formed
by stacking GP layers, models complex hierarchical relationships among
high-dimensional compositional features and captures correlations across
multiple target properties, propagating uncertainty through successive layers.
We integrate evaluation cost into an upper-confidence-bound acquisition
extension, which, together with heterotopic querying, proposes small batches of
candidates in parallel, balancing exploration of under-characterized regions
with exploitation of high-mean, low-variance predictions across correlated
properties. Applied to refractory high-entropy alloys for high-temperature
applications, our framework converges to optimal formulations in fewer
iterations with cost-aware queries than conventional GP-based BO, highlighting
the value of deep, uncertainty-aware, cost-sensitive strategies in materials
campaigns.

</details>


### [209] [An Equivariant Graph Network for Interpretable Nanoporous Materials Design](https://arxiv.org/abs/2509.15908)
*Zhenhao Zhou,Salman Bin Kashif,Dawei Feng,Jin-Hu Dou,Kaihang Shi,Tao Deng,Zhenpeng Yao*

Main category: cond-mat.mtrl-sci

TL;DR: 提出三维周期性空间采样方法用于纳米多孔材料设计，模型在性能预测上达先进水平且具可解释性。


<details>
  <summary>Details</summary>
Motivation: 纳米多孔材料化学空间大，现有机器学习模型在阐释晶体几何与性能关联时缺乏可解释性或保真度。

Method: 提出三维周期性空间采样方法，将大的纳米多孔结构分解为局部几何位点，进行性能预测和位点贡献量化。

Result: 模型在气体存储、分离和导电性能预测上达到了先进的准确性和数据效率，能解释预测结果并准确识别目标性能的重要局部位点。

Conclusion: 该模型为可解释、考虑对称性的纳米多孔材料设计铺平道路，且可扩展到其他材料。

Abstract: Nanoporous materials hold promise for diverse sustainable applications, yet
their vast chemical space poses challenges for efficient design. Machine
learning offers a compelling pathway to accelerate the exploration, but
existing models lack either interpretability or fidelity for elucidating the
correlation between crystal geometry and property. Here, we report a
three-dimensional periodic space sampling method that decomposes large
nanoporous structures into local geometrical sites for combined property
prediction and site-wise contribution quantification. Trained with a
constructed database and retrieved datasets, our model achieves
state-of-the-art accuracy and data efficiency for property prediction on gas
storage, separation, and electrical conduction. Meanwhile, this approach
enables the interpretation of the prediction and allows for accurate
identification of significant local sites for targeted properties. Through
identifying transferable high-performance sites across diverse nanoporous
frameworks, our model paves the way for interpretable, symmetry-aware
nanoporous materials design, which is extensible to other materials, like
molecular crystals and beyond.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [210] [Geometric Integration for Neural Control Variates](https://arxiv.org/abs/2509.15538)
*Daniel Meister,Takahiro Harada*

Main category: cs.GR

TL;DR: 研究多层感知器（MLP）作为控制变量用于蒙特卡罗积分，提出基于积分域细分的积分方法并展示在光传输模拟中的应用。


<details>
  <summary>Details</summary>
Motivation: 控制变量是蒙特卡罗积分减少方差的技术，神经网络可作为控制变量，但解析积分有挑战，研究简单神经网络模型MLP的解析积分可能性。

Method: 提出基于积分域细分的积分方法，运用计算几何技术解决二维问题。

Result: 证明MLP结合所提积分方法可作为控制变量，展示在光传输模拟中的应用。

Conclusion: MLP结合积分方法可用于蒙特卡罗积分的控制变量，在光传输模拟中有应用潜力。

Abstract: Control variates are a variance-reduction technique for Monte Carlo
integration. The principle involves approximating the integrand by a function
that can be analytically integrated, and integrating using the Monte Carlo
method only the residual difference between the integrand and the
approximation, to obtain an unbiased estimate. Neural networks are universal
approximators that could potentially be used as a control variate. However, the
challenge lies in the analytic integration, which is not possible in general.
In this manuscript, we study one of the simplest neural network models, the
multilayered perceptron (MLP) with continuous piecewise linear activation
functions, and its possible analytic integration. We propose an integration
method based on integration domain subdivision, employing techniques from
computational geometry to solve this problem in 2D. We demonstrate that an MLP
can be used as a control variate in combination with our integration method,
showing applications in the light transport simulation.

</details>


### [211] [ChannelFlow-Tools: A Standardized Dataset Creation Pipeline for 3D Obstructed Channel Flows](https://arxiv.org/abs/2509.15236)
*Shubham Kavane,Kajol Kulkarni,Harald Koestler*

Main category: cs.GR

TL;DR: 提出ChannelFlow - Tools框架，实现从CAD实体生成到ML就绪输入和目标的标准化流程，通过案例验证支持可重复ML训练。


<details>
  <summary>Details</summary>
Motivation: 将一次性数据集创建转变为可重复、可配置的CFD代理建模管道，标准化3D阻塞通道流从CAD实体生成到ML输入和目标的端到端路径。

Method: 构建配置驱动的ChannelFlow - Tools框架，集成几何合成、可行性检查、SDF体素化、HPC上自动求解器编排和笛卡尔重采样等步骤，用单一配置文件控制各阶段。

Result: 生成10k +场景，进行端到端存储权衡评估，验证标准化表示支持可重复ML训练。

Conclusion: ChannelFlow - Tools能将一次性数据集创建变为可重复、可配置的CFD代理建模管道。

Abstract: We present ChannelFlow-Tools, a configuration-driven framework that
standardizes the end-to-end path from programmatic CAD solid generation to
ML-ready inputs and targets for 3D obstructed channel flows. The toolchain
integrates geometry synthesis with feasibility checks, signed distance field
(SDF) voxelization, automated solver orchestration on HPC (waLBerla LBM), and
Cartesian resampling to co-registered multi-resolution tensors. A single
Hydra/OmegaConf configuration governs all stages, enabling deterministic
reproduction and controlled ablations. As a case study, we generate 10k+ scenes
spanning Re=100-15000 with diverse shapes and poses. An end-to-end evaluation
of storage trade-offs directly from the emitted artifacts, a minimal 3D U-Net
at 128x32x32, and example surrogate models with dataset size illustrate that
the standardized representations support reproducible ML training.
ChannelFlow-Tools turns one-off dataset creation into a reproducible,
configurable pipeline for CFD surrogate modeling.

</details>


### [212] [GenCAD-3D: CAD Program Generation using Multimodal Latent Space Alignment and Synthetic Dataset Balancing](https://arxiv.org/abs/2509.15246)
*Nomi Yu,Md Ferdous Alam,A. John Hart,Faez Ahmed*

Main category: cs.GR

TL;DR: 提出GenCAD - 3D框架和SynthBal数据增强策略，提升CAD生成效果并开源数据代码。


<details>
  <summary>Details</summary>
Motivation: 当前CAD生成自动化的深度生成模型受数据集不平衡和规模不足限制，复杂CAD程序表示缺失。

Method: 引入GenCAD - 3D框架，利用对比学习对齐嵌入，结合潜在扩散模型进行CAD序列生成和检索；提出SynthBal数据增强策略平衡和扩展数据集。

Result: SynthBal提高重建精度，减少无效CAD模型生成，在高复杂度几何上表现超越现有基准。

Conclusion: 研究成果对简化逆向工程和提高工程设计自动化有重要意义。

Abstract: CAD programs, structured as parametric sequences of commands that compile
into precise 3D geometries, are fundamental to accurate and efficient
engineering design processes. Generating these programs from nonparametric data
such as point clouds and meshes remains a crucial yet challenging task,
typically requiring extensive manual intervention. Current deep generative
models aimed at automating CAD generation are significantly limited by
imbalanced and insufficiently large datasets, particularly those lacking
representation for complex CAD programs. To address this, we introduce
GenCAD-3D, a multimodal generative framework utilizing contrastive learning for
aligning latent embeddings between CAD and geometric encoders, combined with
latent diffusion models for CAD sequence generation and retrieval.
Additionally, we present SynthBal, a synthetic data augmentation strategy
specifically designed to balance and expand datasets, notably enhancing
representation of complex CAD geometries. Our experiments show that SynthBal
significantly boosts reconstruction accuracy, reduces the generation of invalid
CAD models, and markedly improves performance on high-complexity geometries,
surpassing existing benchmarks. These advancements hold substantial
implications for streamlining reverse engineering and enhancing automation in
engineering design. We will publicly release our datasets and code, including a
set of 51 3D-printed and laser-scanned parts on our project site.

</details>


### [213] [Causal Reasoning Elicits Controllable 3D Scene Generation](https://arxiv.org/abs/2509.15249)
*Shen Chen,Ruiyu Zhao,Jiale Zhou,Zongkai Wu,Jenq-Neng Hwang,Lei Li*

Main category: cs.GR

TL;DR: 提出CausalStruct框架将因果推理嵌入3D场景生成，实验表明其生成场景逻辑连贯、交互真实、适应性强。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景生成方法难以建模对象间复杂逻辑依赖和物理约束，无法适应动态现实环境。

Method: 利用大语言模型构建因果图，通过强制因果顺序和因果干预迭代细化场景布局，用PID控制器调整对象，使用文本或图像引导，结合3D高斯溅射和分数蒸馏采样。

Result: 生成的3D场景具有增强的逻辑连贯性、真实的空间交互和强大的适应性。

Conclusion: CausalStruct框架有效提升了3D场景生成的质量和适应性。

Abstract: Existing 3D scene generation methods often struggle to model the complex
logical dependencies and physical constraints between objects, limiting their
ability to adapt to dynamic and realistic environments. We propose
CausalStruct, a novel framework that embeds causal reasoning into 3D scene
generation. Utilizing large language models (LLMs), We construct causal graphs
where nodes represent objects and attributes, while edges encode causal
dependencies and physical constraints. CausalStruct iteratively refines the
scene layout by enforcing causal order to determine the placement order of
objects and applies causal intervention to adjust the spatial configuration
according to physics-driven constraints, ensuring consistency with textual
descriptions and real-world dynamics. The refined scene causal graph informs
subsequent optimization steps, employing a
Proportional-Integral-Derivative(PID) controller to iteratively tune object
scales and positions. Our method uses text or images to guide object placement
and layout in 3D scenes, with 3D Gaussian Splatting and Score Distillation
Sampling improving shape accuracy and rendering stability. Extensive
experiments show that CausalStruct generates 3D scenes with enhanced logical
coherence, realistic spatial interactions, and robust adaptability.

</details>


### [214] [MoAngelo: Motion-Aware Neural Surface Reconstruction for Dynamic Scenes](https://arxiv.org/abs/2509.15892)
*Mohamed Ebbed,Zorah Lähner*

Main category: cs.GR

TL;DR: 提出一种新的动态场景重建框架，扩展NeuralAngelo用于动态场景，在ActorsHQ数据集上展示了更好的重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有动态场景重建方法有计算和表示挑战，提取的网格有噪声或过于平滑，需要高质量的动态重建方法。

Method: 从初始帧用NeuralAngelo进行高质量模板场景重建，联合优化变形场来跟踪和细化模板，使用灵活模板更新几何形状。

Result: 在ActorsHQ数据集上，与先前的先进方法相比，展示了更优的重建精度。

Conclusion: 所提出的框架能够实现高度详细的动态场景重建，优于现有方法。

Abstract: Dynamic scene reconstruction from multi-view videos remains a fundamental
challenge in computer vision. While recent neural surface reconstruction
methods have achieved remarkable results in static 3D reconstruction, extending
these approaches with comparable quality for dynamic scenes introduces
significant computational and representational challenges. Existing dynamic
methods focus on novel-view synthesis, therefore, their extracted meshes tend
to be noisy. Even approaches aiming for geometric fidelity often result in too
smooth meshes due to the ill-posedness of the problem. We present a novel
framework for highly detailed dynamic reconstruction that extends the static 3D
reconstruction method NeuralAngelo to work in dynamic settings. To that end, we
start with a high-quality template scene reconstruction from the initial frame
using NeuralAngelo, and then jointly optimize deformation fields that track the
template and refine it based on the temporal sequence. This flexible template
allows updating the geometry to include changes that cannot be modeled with the
deformation field, for instance occluded parts or the changes in the topology.
We show superior reconstruction accuracy in comparison to previous
state-of-the-art methods on the ActorsHQ dataset.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [215] [Incorporating Visual Cortical Lateral Connection Properties into CNN: Recurrent Activation and Excitatory-Inhibitory Separation](https://arxiv.org/abs/2509.15460)
*Jin Hyun Park,Cheng Zhang,Yoonsuck Choe*

Main category: q-bio.NC

TL;DR: 本文提出在标准CNN框架中对侧向连接建模，测试其益处并分析特性，提升了分类准确率，让CNN更接近生物视觉系统。


<details>
  <summary>Details</summary>
Motivation: 现有CNN模型缺少类似哺乳动物视觉系统中视觉皮层区域内的侧向连接这一重要架构特征，需对其建模并研究。

Method: 表明使用权重共享的循环CNN等同于侧向连接，提出自定义损失函数分离兴奋和抑制权重。

Result: 添加上述两种特征后增加了分类准确率，模型的激活和连接特性与生物视觉系统相似。

Conclusion: 该方法有助于让CNN更接近生物视觉系统，更好理解视觉皮层计算原理。

Abstract: The original Convolutional Neural Networks (CNNs) and their modern updates
such as the ResNet are heavily inspired by the mammalian visual system. These
models include afferent connections (retina and LGN to the visual cortex) and
long-range projections (connections across different visual cortical areas).
However, in the mammalian visual system, there are connections within each
visual cortical area, known as lateral (or horizontal) connections. These would
roughly correspond to connections within CNN feature maps, and this important
architectural feature is missing in current CNN models. In this paper, we
present how such lateral connections can be modeled within the standard CNN
framework, and test its benefits and analyze its emergent properties in
relation to the biological visual system. We will focus on two main
architectural features of lateral connections: (1) recurrent activation and (2)
separation of excitatory and inhibitory connections. We show that recurrent CNN
using weight sharing is equivalent to lateral connections, and propose a custom
loss function to separate excitatory and inhibitory weights. The addition of
these two leads to increased classification accuracy, and importantly, the
activation properties and connection properties of the resulting model show
properties similar to those observed in the biological visual system. We expect
our approach to help align CNN closer to its biological counterpart and better
understand the principles of visual cortical computation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [216] [Classical and Quantum Heuristics for the Binary Paint Shop Problem](https://arxiv.org/abs/2509.15294)
*V Vijendran,Dax Enshan Koh,Ping Koy Lam,Syed M Assad*

Main category: quant-ph

TL;DR: 研究将BPSP问题转化为加权MaxCut，用低深度QAOA变体XQAOA₁和RQAOA₁与经典启发式算法对比，XQAOA₁表现最佳，RQAOA₁规模增大性能下降。


<details>
  <summary>Details</summary>
Motivation: 解决汽车制造中的BPSP问题以提高生产效率和降低成本，对比量子算法和经典启发式算法的性能。

Method: 将BPSP问题转化为加权MaxCut，使用XQAOA₁和RQAOA₁与经典启发式算法在不同规模实例上进行对比。

Result: XQAOA₁平均比率达0.357，超越RQAOA₁和所有经典启发式算法；RQAOA₁规模增大性能下降。

Conclusion: XQAOA₁有潜力渐近超越所有已知启发式算法，RQAOA₁在大规模实例中性能不佳。

Abstract: The Binary Paint Shop Problem (BPSP) is an $\mathsf{APX}$-hard optimisation
problem in automotive manufacturing: given a sequence of $2n$ cars, comprising
$n$ distinct models each appearing twice, the task is to decide which of two
colours to paint each car so that the two occurrences of each model are painted
differently, while minimising consecutive colour swaps. The key performance
metric is the paint swap ratio, the average number of colour changes per car,
which directly impacts production efficiency and cost. Prior work showed that
the Quantum Approximate Optimisation Algorithm (QAOA) at depth $p=7$ achieves a
paint swap ratio of $0.393$, outperforming the classical Recursive Greedy (RG)
heuristic with an expected ratio of $0.4$ [Phys. Rev. A 104, 012403 (2021)].
More recently, the classical Recursive Star Greedy (RSG) heuristic was
conjectured to achieve an expected ratio of $0.361$. In this study, we develop
the theoretical foundations for applying QAOA to BPSP through a reduction of
BPSP to weighted MaxCut, and use this framework to benchmark two
state-of-the-art low-depth QAOA variants, eXpressive QAOA (XQAOA) and Recursive
QAOA (RQAOA), at $p=1$ (denoted XQAOA$_1$ and RQAOA$_1$), against the strongest
classical heuristics known to date. Across instances ranging from $2^7$ to
$2^{12}$ cars, XQAOA$_1$ achieves an average ratio of $0.357$, surpassing
RQAOA$_1$ and all classical heuristics, including the conjectured performance
of RSG. Surprisingly, RQAOA$_1$ shows diminishing performance as size
increases: despite using provably optimal QAOA$_1$ parameters at each
recursion, it is outperformed by RSG on most $2^{11}$-car instances and all
$2^{12}$-car instances. To our knowledge, this is the first study to report
RQAOA$_1$'s performance degradation at scale. In contrast, XQAOA$_1$ remains
robust, indicating strong potential to asymptotically surpass all known
heuristics.

</details>


### [217] [Quantum Generative Adversarial Autoencoders: Learning latent representations for quantum data generation](https://arxiv.org/abs/2509.16186)
*Naipunnya Raj,Rajiv Sangle,Avinash Singh,Krishna Kumar Sabapathy*

Main category: quant-ph

TL;DR: 介绍量子生成对抗自编码器（QGAA）用于生成量子数据，由量子自编码器和量子生成对抗网络组成，通过两个场景展示其效用，结果显示有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 提出一种用于生成量子数据的量子模型。

Method: 构建QGAA，包含量子自编码器压缩量子态和量子生成对抗网络学习其潜在空间。

Result: 在生成纯纠缠态和分子基态场景中验证，模拟中H₂和LiH能量估计平均误差分别为0.02 Ha和0.06 Ha。

Conclusion: QGAA在量子态生成、量子化学和近期量子机器学习应用方面有潜力。

Abstract: In this work, we introduce the Quantum Generative Adversarial Autoencoder
(QGAA), a quantum model for generation of quantum data. The QGAA consists of
two components: (a) Quantum Autoencoder (QAE) to compress quantum states, and
(b) Quantum Generative Adversarial Network (QGAN) to learn the latent space of
the trained QAE. This approach imparts the QAE with generative capabilities.
The utility of QGAA is demonstrated in two representative scenarios: (a)
generation of pure entangled states, and (b) generation of parameterized
molecular ground states for H$_2$ and LiH. The average errors in the energies
estimated by the trained QGAA are 0.02 Ha for H$_2$ and 0.06 Ha for LiH in
simulations upto 6 qubits. These results illustrate the potential of QGAA for
quantum state generation, quantum chemistry, and near-term quantum machine
learning applications.

</details>


### [218] [Neural Architecture Search Algorithms for Quantum Autoencoders](https://arxiv.org/abs/2509.15451)
*Ankit Kulshrestha,Xiaoyuan Liu,Hayato Ushijima-Mwesigwa,Ilya Safro*

Main category: quant-ph

TL;DR: 本文提出借鉴神经网络架构搜索（NAS）自动化量子电路设计，提出两种Quantum - NAS算法，以量子数据压缩为任务验证，结果表明可减轻手动设计工作量。


<details>
  <summary>Details</summary>
Motivation: 当前量子电路设计依赖算法设计者大量手动工作，难以应对未来复杂量子算法，需自动化设计。

Method: 借鉴NAS，提出两种Quantum - NAS算法来寻找特定量子任务的高效电路。

Result: 以量子数据压缩为任务，找到的高效自编码器设计在三个不同任务上优于基线。

Conclusion: 量子NAS算法能显著减轻手动工作量，为给定任务提供高性能量子电路。

Abstract: The design of quantum circuits is currently driven by the specific objectives
of the quantum algorithm in question. This approach thus relies on a
significant manual effort by the quantum algorithm designer to design an
appropriate circuit for the task. However this approach cannot scale to more
complex quantum algorithms in the future without exponentially increasing the
circuit design effort and introducing unwanted inductive biases. Motivated by
this observation, we propose to automate the process of cicuit design by
drawing inspiration from Neural Architecture Search (NAS). In this work, we
propose two Quantum-NAS algorithms that aim to find efficient circuits given a
particular quantum task. We choose quantum data compression as our driver
quantum task and demonstrate the performance of our algorithms by finding
efficient autoencoder designs that outperform baselines on three different
tasks - quantum data denoising, classical data compression and pure quantum
data compression. Our results indicate that quantum NAS algorithms can
significantly alleviate the manual effort while delivering performant quantum
circuits for any given task.

</details>


### [219] [AI Methods for Permutation Circuit Synthesis Across Generic Topologies](https://arxiv.org/abs/2509.16020)
*Victor Villar,Juan Cruz-Benito,Ismael Faro,David Kremer*

Main category: quant-ph

TL;DR: 本文研究用强化学习技术实现通用拓扑上最多25量子比特排列电路的合成与转译，单一模型可跨拓扑高效合成电路。


<details>
  <summary>Details</summary>
Motivation: 探索在通用拓扑上进行排列电路合成与转译的人工智能方法，避免为每个拓扑开发专门模型。

Method: 使用强化学习技术，在通用矩形格子上训练基础模型，利用掩码机制动态选择拓扑子集。

Result: 在5x5格子上的结果优于经典启发式方法，与之前的专用AI模型相当，能为训练中未见过的拓扑进行合成，还可微调模型提升特定拓扑性能。

Conclusion: 该方法使单一训练模型能跨不同拓扑高效合成电路，可实际集成到转译工作流程中。

Abstract: This paper investigates artificial intelligence (AI) methodologies for the
synthesis and transpilation of permutation circuits across generic topologies.
Our approach uses Reinforcement Learning (RL) techniques to achieve
near-optimal synthesis of permutation circuits up to 25 qubits. Rather than
developing specialized models for individual topologies, we train a
foundational model on a generic rectangular lattice, and employ masking
mechanisms to dynamically select subsets of topologies during the synthesis.
This enables the synthesis of permutation circuits on any topology that can be
embedded within the rectangular lattice, without the need to re-train the
model. In this paper we show results for 5x5 lattice and compare them to
previous AI topology-oriented models and classical methods, showing that they
outperform classical heuristics, and match previous specialized AI models, and
performs synthesis even for topologies that were not seen during training. We
further show that the model can be fine tuned to strengthen the performance for
selected topologies of interest. This methodology allows a single trained model
to efficiently synthesize circuits across diverse topologies, allowing its
practical integration into transpilation workflows.

</details>


### [220] [Triplet Loss Based Quantum Encoding for Class Separability](https://arxiv.org/abs/2509.15705)
*Marco Mordacci,Mahul Pandey,Paolo Santini,Michele Amoretti*

Main category: quant-ph

TL;DR: 提出高效数据驱动编码方案提升变分量子分类器性能，在基准测试中表现优于幅度编码。


<details>
  <summary>Details</summary>
Motivation: 提升变分量子分类器在处理复杂数据集（如图像）时的性能。

Method: 设计特殊编码方案，用受经典人脸识别算法启发的三元组损失函数训练编码电路，通过平均迹距离衡量类别可分性。

Result: 在MNIST和MedMNIST数据集的二元分类任务基准测试中，相比相同VQC结构的幅度编码有显著提升，且电路深度更低。

Conclusion: 所提出的编码方案能有效提升变分量子分类器性能。

Abstract: An efficient and data-driven encoding scheme is proposed to enhance the
performance of variational quantum classifiers. This encoding is specially
designed for complex datasets like images and seeks to help the classification
task by producing input states that form well-separated clusters in the Hilbert
space according to their classification labels. The encoding circuit is trained
using a triplet loss function inspired by classical facial recognition
algorithms, and class separability is measured via average trace distances
between the encoded density matrices. Benchmark tests performed on various
binary classification tasks on MNIST and MedMNIST datasets demonstrate
considerable improvement over amplitude encoding with the same VQC structure
while requiring a much lower circuit depth.

</details>


### [221] [Impact of Single Rotations and Entanglement Topologies in Quantum Neural Networks](https://arxiv.org/abs/2509.15722)
*Marco Mordacci,Michele Amoretti*

Main category: quant-ph

TL;DR: 分析不同变分量子电路性能，探究其随纠缠拓扑、采用的门和量子机器学习任务的变化，以找出构建量子神经网络电路的最佳方式。


<details>
  <summary>Details</summary>
Motivation: 识别构建量子神经网络电路的最优方法。

Method: 使用两种电路，考虑不同旋转层组合和四种纠缠拓扑，进行概率分布生成、图像生成和图像分类任务，将结果与电路的表达能力和纠缠能力关联。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确给出结论。

Abstract: In this work, an analysis of the performance of different Variational Quantum
Circuits is presented, investigating how it changes with respect to
entanglement topology, adopted gates, and Quantum Machine Learning tasks to be
performed. The objective of the analysis is to identify the optimal way to
construct circuits for Quantum Neural Networks. In the presented experiments,
two types of circuits are used: one with alternating layers of rotations and
entanglement, and the other, similar to the first one, but with an additional
final layer of rotations. As rotation layers, all combinations of one and two
rotation sequences are considered. Four different entanglement topologies are
compared: linear, circular, pairwise, and full. Different tasks are considered,
namely the generation of probability distributions and images, and image
classification. Achieved results are correlated with the expressibility and
entanglement capability of the different circuits to understand how these
features affect performance.

</details>


### [222] [Training Variational Quantum Circuits Using Particle Swarm Optimization](https://arxiv.org/abs/2509.15726)
*Marco Mordacci,Michele Amoretti*

Main category: quant-ph

TL;DR: 本文使用粒子群优化（PSO）算法训练变分量子电路（VQCs），在多个生物医学图像数据集上测试，结果显示PSO能实现可比甚至更好的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 常用的基于梯度的优化方法会遇到贫瘠高原问题，因此采用PSO算法。

Method: 使用PSO算法训练VQCs的整个结构，从四种量子门中选择，在MedMNIST的多个数据集上测试，并与经典随机梯度下降方法对比。

Result: PSO能在多个数据集上实现可比甚至更好的分类准确率，且使用的量子门数量更少。

Conclusion: PSO算法在训练VQCs上具有一定优势，可作为替代基于梯度优化方法的选择。

Abstract: In this work, the Particle Swarm Optimization (PSO) algorithm has been used
to train various Variational Quantum Circuits (VQCs). This approach is
motivated by the fact that commonly used gradient-based optimization methods
can suffer from the barren plateaus problem. PSO is a stochastic optimization
technique inspired by the collective behavior of a swarm of birds. The
dimension of the swarm, the number of iterations of the algorithm, and the
number of trainable parameters can be set. In this study, PSO has been used to
train the entire structure of VQCs, allowing it to select which quantum gates
to apply, the target qubits, and the rotation angle, in case a rotation is
chosen. The algorithm is restricted to choosing from four types of gates: Rx,
Ry, Rz, and CNOT. The proposed optimization approach has been tested on various
datasets of the MedMNIST, which is a collection of biomedical image datasets
designed for image classification tasks. Performance has been compared with the
results achieved by classical stochastic gradient descent applied to a
predefined VQC. The results show that the PSO can achieve comparable or even
better classification accuracy across multiple datasets, despite the PSO using
a lower number of quantum gates than the VQC used with gradient descent
optimization.

</details>


### [223] [Quantum Enhanced Anomaly Detection for ADS-B Data using Hybrid Deep Learning](https://arxiv.org/abs/2509.15991)
*Rani Naaman,Felipe Gohring de Magalhaes,Jean-Yves Ouattara,Gabriela Nicolescu*

Main category: quant-ph

TL;DR: 本文结合量子与经典机器学习技术，用H - FQNN对ADS - B数据进行异常检测，结果显示其检测准确率与传统FNN相当。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在加速处理速度和处理复杂数据集高维性方面有优势，探索量子特性在ADS - B数据异常检测中的影响。

Method: 提出结合量子和经典机器学习技术的新方法，比较不同损失函数的H - FQNN性能，并使用公开的ADS - B数据集进行评估。

Result: H - FQNN检测异常的准确率在90.17% - 94.05%，传统FNN模型准确率在91.50% - 93.37%，二者性能相当。

Conclusion: 结合量子和经典机器学习技术的方法在ADS - B数据异常检测中有一定竞争力。

Abstract: The emerging field of Quantum Machine Learning (QML) has shown promising
advantages in accelerating processing speed and effectively handling the high
dimensionality associated with complex datasets. Quantum Computing (QC) enables
more efficient data manipulation through the quantum properties of
superposition and entanglement. In this paper, we present a novel approach
combining quantum and classical machine learning techniques to explore the
impact of quantum properties for anomaly detection in Automatic Dependent
Surveillance-Broadcast (ADS-B) data. We compare the performance of a
Hybrid-Fully Connected Quantum Neural Network (H-FQNN) with different loss
functions and use a publicly available ADS-B dataset to evaluate the
performance. The results demonstrate competitive performance in detecting
anomalies, with accuracies ranging from 90.17% to 94.05%, comparable to the
performance of a traditional Fully Connected Neural Network (FNN) model, which
achieved accuracies between 91.50% and 93.37%.

</details>


### [224] [Quantum Reinforcement Learning with Dynamic-Circuit Qubit Reuse and Grover-Based Trajectory Optimization](https://arxiv.org/abs/2509.16002)
*Thet Htar Su,Shaswot Shresthamali,Masaaki Kondo*

Main category: quant-ph

TL;DR: 开发全量子强化学习框架，整合量子马尔可夫决策过程等技术，减少量子比特使用，经模拟和实验验证其可行性，推动大规模应用。


<details>
  <summary>Details</summary>
Motivation: 推动量子强化学习在大规模序贯决策任务中的可扩展性和实际应用。

Method: 整合量子马尔可夫决策过程、动态电路的量子比特复用和Grover算法进行轨迹优化，在量子域编码状态等信息，用动态电路操作复用物理量子比特，用Grover搜索寻找最优策略。

Result: 模拟显示动态电路实现减少66%量子比特使用并保持轨迹保真度，在IBM硬件上验证框架可行性。

Conclusion: 该框架可推动量子强化学习在大规模序贯决策任务中的可扩展性和实际应用。

Abstract: A fully quantum reinforcement learning framework is developed that integrates
a quantum Markov decision process, dynamic circuit-based qubit reuse, and
Grover's algorithm for trajectory optimization. The framework encodes states,
actions, rewards, and transitions entirely within the quantum domain, enabling
parallel exploration of state-action sequences through superposition and
eliminating classical subroutines. Dynamic circuit operations, including
mid-circuit measurement and reset, allow reuse of the same physical qubits
across multiple agent-environment interactions, reducing qubit requirements
from 7*T to 7 for T time steps while preserving logical continuity. Quantum
arithmetic computes trajectory returns, and Grover's search is applied to the
superposition of these evaluated trajectories to amplify the probability of
measuring those with the highest return, thereby accelerating the
identification of the optimal policy. Simulations demonstrate that the
dynamic-circuit-based implementation preserves trajectory fidelity while
reducing qubit usage by 66 percent relative to the static design. Experimental
deployment on IBM Heron-class quantum hardware confirms that the framework
operates within the constraints of current quantum processors and validates the
feasibility of fully quantum multi-step reinforcement learning under noisy
intermediate-scale quantum conditions. This framework advances the scalability
and practical application of quantum reinforcement learning for large-scale
sequential decision-making tasks.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [225] [Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control](https://arxiv.org/abs/2509.15799)
*Max Studt,Georg Schildbach*

Main category: eess.SY

TL;DR: 提出结合强化学习和模型预测控制的分层框架，在捕食者 - 猎物基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决基于学习的控制在动态、约束丰富环境中实现安全协调行为的挑战，改善纯端到端学习样本效率低、可靠性有限以及基于模型方法泛化能力差的问题。

Method: 提出分层框架，上层通过强化学习进行战术决策，下层通过模型预测控制执行，多智能体系统中高层策略从结构化感兴趣区域选择抽象目标，MPC确保动态可行和安全的运动。

Result: 在捕食者 - 猎物基准测试中，该方法在奖励、安全性和一致性方面优于端到端和基于屏蔽的强化学习基线。

Conclusion: 结合结构化学习和基于模型的控制是有益的。

Abstract: Achieving safe and coordinated behavior in dynamic, constraint-rich
environments remains a major challenge for learning-based control. Pure
end-to-end learning often suffers from poor sample efficiency and limited
reliability, while model-based methods depend on predefined references and
struggle to generalize. We propose a hierarchical framework that combines
tactical decision-making via reinforcement learning (RL) with low-level
execution through Model Predictive Control (MPC). For the case of multi-agent
systems this means that high-level policies select abstract targets from
structured regions of interest (ROIs), while MPC ensures dynamically feasible
and safe motion. Tested on a predator-prey benchmark, our approach outperforms
end-to-end and shielding-based RL baselines in terms of reward, safety, and
consistency, underscoring the benefits of combining structured learning with
model-based control.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [226] [Breathing and Semantic Pause Detection and Exertion-Level Classification in Post-Exercise Speech](https://arxiv.org/abs/2509.15473)
*Yuyu Wang,Wuyue Xia,Huaxiu Yao,Jingping Nie*

Main category: eess.AS

TL;DR: 本文基于含同步音频和呼吸信号的数据集，对运动后语音中的不同类型停顿进行标注，用多种模型和特征开展停顿检测和运动强度分类，结果优于先前工作。


<details>
  <summary>Details</summary>
Motivation: 现有识别和区分运动后语音不同类型停顿的工作有限，需检测停顿以评估恢复率、肺功能和运动相关异常。

Method: 对停顿类型进行系统标注，用GRU、1D CNN - LSTM等深度学习模型，MFCC、MFB等声学特征和分层Wav2Vec2表示开展停顿检测和运动强度分类，评估单特征、特征融合和两阶段检测 - 分类级联三种设置。

Result: 语义停顿检测准确率达89%，呼吸停顿55%，组合停顿86%，总体73%，运动强度分类准确率90.5%，优于先前工作。

Conclusion: 该方法能有效检测运动后语音不同类型停顿并进行运动强度分类，效果优于现有工作。

Abstract: Post-exercise speech contains rich physiological and linguistic cues, often
marked by semantic pauses, breathing pauses, and combined breathing-semantic
pauses. Detecting these events enables assessment of recovery rate, lung
function, and exertion-related abnormalities. However, existing works on
identifying and distinguishing different types of pauses in this context are
limited. In this work, building on a recently released dataset with
synchronized audio and respiration signals, we provide systematic annotations
of pause types. Using these annotations, we systematically conduct exploratory
breathing and semantic pause detection and exertion-level classification across
deep learning models (GRU, 1D CNN-LSTM, AlexNet, VGG16), acoustic features
(MFCC, MFB), and layer-stratified Wav2Vec2 representations. We evaluate three
setups-single feature, feature fusion, and a two-stage detection-classification
cascade-under both classification and regression formulations. Results show
per-type detection accuracy up to 89$\%$ for semantic, 55$\%$ for breathing,
86$\%$ for combined pauses, and 73$\%$overall, while exertion-level
classification achieves 90.5$\%$ accuracy, outperformin prior work.

</details>


### [227] [VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency](https://arxiv.org/abs/2509.15969)
*Nikita Torgashov,Gustav Eje Henter,Gabriel Skantze*

Main category: eess.AS

TL;DR: 提出VoXtream，一个全自回归、零样本流式文本转语音系统，起始延迟低，质量有竞争力。


<details>
  <summary>Details</summary>
Motivation: 开发一个可实时使用、从第一个字开始发声的流式文本转语音系统。

Method: 使用单调对齐方案和动态前瞻机制，将输入音素直接映射到音频令牌，由增量音素变压器、时间变压器和深度变压器构建。

Result: 在公开可用的流式TTS中实现最低初始延迟，在GPU上为102 ms，在多个指标上匹配或超越更大的基线。

Conclusion: VoXtream是一个有效且高质量的实时流式文本转语音系统。

Abstract: We present VoXtream, a fully autoregressive, zero-shot streaming
text-to-speech (TTS) system for real-time use that begins speaking from the
first word. VoXtream directly maps incoming phonemes to audio tokens using a
monotonic alignment scheme and a dynamic look-ahead that does not delay onset.
Built around an incremental phoneme transformer, a temporal transformer
predicting semantic and duration tokens, and a depth transformer producing
acoustic tokens, VoXtream achieves, to our knowledge, the lowest initial delay
among publicly available streaming TTS: 102 ms on GPU. Despite being trained on
a mid-scale 9k-hour corpus, it matches or surpasses larger baselines on several
metrics, while delivering competitive quality in both output- and
full-streaming settings. Demo and code are available at
https://herimor.github.io/voxtream.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [228] [Collective Voice: Recovered-Peer Support Mediated by An LLM-Based Chatbot for Eating Disorder Recovery](https://arxiv.org/abs/2509.15289)
*Ryuhaerang Choi,Taehan Kim,Subin Park,Seohyeon Yoo,Jennifer G. Kim,Sung-Ju Lee*

Main category: cs.HC

TL;DR: 设计RecoveryTeller聊天机器人，研究其与普通导师聊天机器人对进食障碍患者的支持效果，发现两者互补并给出设计建议。


<details>
  <summary>Details</summary>
Motivation: 同伴康复叙事在进食障碍领域有独特益处，但同伴参与项目稀缺且对康复同伴存在潜在弊端，需解决这一问题。

Method: 设计RecoveryTeller聊天机器人，与普通导师聊天机器人对比，对26名进食障碍患者进行20天交叉部署研究。

Result: RecoveryTeller引发更强情感共鸣，但情感和认知信任的矛盾使参与者认为两者互补。

Conclusion: 为心理健康聊天机器人人设设计提供了设计启示。

Abstract: Peer recovery narratives provide unique benefits beyond professional or lay
mentoring by fostering hope and sustained recovery in eating disorder (ED)
contexts. Yet, such support is limited by the scarcity of peer-involved
programs and potential drawbacks on recovered peers, including relapse risk. To
address this, we designed RecoveryTeller, a chatbot adopting a recovered-peer
persona that portrays itself as someone recovered from an ED. We examined
whether such a persona can reproduce the support affordances of peer recovery
narratives. We compared RecoveryTeller with a lay-mentor persona chatbot
offering similar guidance but without a recovery background. We conducted a
20-day cross-over deployment study with 26 ED participants, each using both
chatbots for 10 days. RecoveryTeller elicited stronger emotional resonance than
a lay-mentor chatbot, yet tensions between emotional and epistemic trust led
participants to view the two personas as complementary rather than substitutes.
We provide design implications for mental health chatbot persona design.

</details>


### [229] [Where Do I 'Add the Egg'?: Exploring Agency and Ownership in AI Creative Co-Writing Systems](https://arxiv.org/abs/2509.15440)
*Dashiel Carrera,Jeb Thomas-Mitchell,Daniel Wigdor*

Main category: cs.HC

TL;DR: 本文研究AI创意协同写作中代理和所有权概念，开发三种界面隐喻的协同写作系统，经访谈分析得出分类并给出设计建议。


<details>
  <summary>Details</summary>
Motivation: AI协同写作系统在创意过程中的代理和所有权观念阻碍其广泛应用，需进行研究。

Method: 回顾商业系统，开发三种不同界面隐喻的协同写作系统，对专业和非专业作家进行访谈。

Result: 得出代理和所有权子类型的分类，发现工具类隐喻转移控制预期点，代理类隐喻突出概念贡献。

Conclusion: 界面隐喻引导控制预期和塑造作者观念，给出AI协同写作系统设计建议。

Abstract: AI co-writing systems challenge long held ideals about agency and ownership
in the creative process, thereby hindering widespread adoption. In order to
address this, we investigate conceptions of agency and ownership in AI creative
co-writing. Drawing on insights from a review of commercial systems, we
developed three co-writing systems with identical functionality but distinct
interface metaphors: agentic, tool-like, and magical. Through interviews with
professional and non-professional writers (n = 18), we explored how these
metaphors influenced participants' sense of control and authorship. Our
analysis resulted in a taxonomy of agency and ownership subtypes and underscore
how tool-like metaphors shift writers' expected points of control while agentic
metaphors foreground conceptual contributions. We argue that interface
metaphors not only guide expectations of control but also frame conceptions of
authorship. We conclude with recommendations for the design of AI co-writing
systems, emphasizing how metaphor shapes user experience and creative practice.

</details>


### [230] [Subject Matter Expertise vs Professional Management in Collective Sequential Decision Making](https://arxiv.org/abs/2509.15263)
*David Shoresh,Yonatan Loewenstein*

Main category: cs.HC

TL;DR: 以国际象棋为模型，对比主题专家和专业经理两类管理者，发现超过最低阈值后主题专家知识对团队协同贡献不大，强化学习训练的专业经理表现更优。


<details>
  <summary>Details</summary>
Motivation: 定量客观地解决公司选继任者时‘主题专业知识与专业管理能力’的争论问题。

Method: 利用国际象棋建模，对比主题专家型管理者（用另一个计算机棋手评估团队建议）和专业管理型管理者（用强化学习训练无象棋预训练的网络）的表现。

Result: 主题专家超过最低阈值后对团队协同贡献不显著，强化学习训练的专业经理表现远超最佳专家型经理，且仅获得有限象棋理解。

Conclusion: 在团队决策中，专业管理能力可能比主题专业知识更重要。

Abstract: Your company's CEO is retiring. You search for a successor. You can promote
an employee from the company familiar with the company's operations, or recruit
an external professional manager. Who should you prefer? It has not been clear
how to address this question, the "subject matter expertise vs. professional
manager debate", quantitatively and objectively. We note that a company's
success depends on long sequences of interdependent decisions, with
often-opposing recommendations of diverse board members. To model this task in
a controlled environment, we utilize chess - a complex, sequential game with
interdependent decisions which allows for quantitative analysis of performance
and expertise (since the states, actions and game outcomes are well-defined).
The availability of chess engines differing in style and expertise, allows
scalable experimentation. We considered a team of (computer) chess players. At
each turn, team members recommend a move and a manager chooses a
recommendation. We compared the performance of two manager types. For manager
as "subject matter expert", we used another (computer) chess player that
assesses the recommendations of the team members based on its own chess
expertise. We examined the performance of such managers at different strength
levels. To model a "professional manager", we used Reinforcement Learning (RL)
to train a network that identifies the board positions in which different team
members have relative advantage, without any pretraining in chess. We further
examined this network to see if any chess knowledge is acquired implicitly. We
found that subject matter expertise beyond a minimal threshold does not
significantly contribute to team synergy. Moreover, performance of a RL-trained
"professional" manager significantly exceeds that of even the best "expert"
managers, while acquiring only limited understanding of chess.

</details>


### [231] [Explainable AI for Maritime Autonomous Surface Ships (MASS): Adaptive Interfaces and Trustworthy Human-AI Collaboration](https://arxiv.org/abs/2509.15959)
*Zhuoyue Zhang,Haitong Xu*

Main category: cs.HC

TL;DR: 文章综合100项研究，分析海上自主航行透明度问题，提出自适应透明度框架及近 期安全操作手段。


<details>
  <summary>Details</summary>
Motivation: 人工智能等技术发展使海上自主航行加速，但不透明决策和人机交互问题阻碍安全应用。

Method: 综合100项研究；映射指导 - 导航 - 控制堆栈到岸基操作模式；识别不安全控制行为；总结透明度特征作用；提炼三层设计策略；整合相关方法。

Result: 提出自适应透明度框架，耦合操作员状态估计与可解释决策支持。

Conclusion: 指出可操作指标显示、透明模型输出和训练管道是近期安全操作的手段。

Abstract: Autonomous navigation in maritime domains is accelerating alongside advances
in artificial intelligence, sensing, and connectivity. Opaque decision-making
and poorly calibrated human-automation interaction remain key barriers to safe
adoption. This article synthesizes 100 studies on automation transparency for
Maritime Autonomous Surface Ships (MASS) spanning situation awareness (SA),
human factors, interface design, and regulation. We (i) map the
Guidance-Navigation-Control stack to shore-based operational modes -- remote
supervision (RSM) and remote control (RCM) -- and identify where human unsafe
control actions (Human-UCAs) concentrate in handover and emergency loops; (ii)
summarize evidence that transparency features (decision rationales,
alternatives, confidence/uncertainty, and rule-compliance indicators) improve
understanding and support trust calibration, though reliability and
predictability often dominate trust; (iii) distill design strategies for
transparency at three layers: sensor/SA acquisition and fusion, HMI/eHMI
presentation (textual/graphical overlays, color coding, conversational and
immersive UIs), and engineer-facing processes (resilient interaction design,
validation, and standardization). We integrate methods for Human-UCA
identification (STPA-Cog + IDAC), quantitative trust/SA assessment, and
operator workload monitoring, and outline regulatory and rule-based
implications including COLREGs formalization and route exchange. We conclude
with an adaptive transparency framework that couples operator state estimation
with explainable decision support to reduce cognitive overload and improve
takeover timeliness. The review highlights actionable figure-of-merit displays
(e.g., CPA/TCPA risk bars, robustness heatmaps), transparent model outputs
(rule traceability, confidence), and training pipelines (HIL/MIL, simulation)
as near-term levers for safer MASS operations.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [232] [DeepMech: A Machine Learning Framework for Chemical Reaction Mechanism Prediction](https://arxiv.org/abs/2509.15872)
*Manajit Das,Ajnabiul Hoque,Mayank Baranwal,Raghavan B. Sunoj*

Main category: physics.chem-ph

TL;DR: 提出可解释的图深度学习框架DeepMech预测化学反应机理，训练后在相关任务中准确率高，还能用于益生元化学多步反应机理重建，且模型可解释。


<details>
  <summary>Details</summary>
Motivation: 传统化学反应机理预测方法依赖专家实验或量子化学计算，当代深度学习方法存在忽略关键中间体和步骤、产生幻觉等问题。

Method: 提出基于图的可解释深度学习框架DeepMech，利用原子和键级注意力，并由广义的机理操作模板引导，在ReactMech数据集上训练。

Result: 在预测基元步骤中准确率达98.98±0.12%，在完整化学反应机理任务中准确率达95.94±0.21%，在分布外场景和预测副产物方面保持高保真度，能有效重建益生元化学多步反应途径。

Conclusion: DeepMech在化学反应机理预测上表现出色，注意力分析使模型符合化学直觉，适用于反应设计。

Abstract: Prediction of complete step-by-step chemical reaction mechanisms (CRMs)
remains a major challenge. Whereas the traditional approaches in CRM tasks rely
on expert-driven experiments or costly quantum chemical computations,
contemporary deep learning (DL) alternatives ignore key intermediates and
mechanistic steps and often suffer from hallucinations. We present DeepMech, an
interpretable graph-based DL framework employing atom- and bond-level
attention, guided by generalized templates of mechanistic operations (TMOps),
to generate CRMs. Trained on our curated ReactMech dataset (~30K CRMs with 100K
atom-mapped and mass-balanced elementary steps), DeepMech achieves
98.98+/-0.12% accuracy in predicting elementary steps and 95.94+/-0.21% in
complete CRM tasks, besides maintaining high fidelity even in
out-of-distribution scenarios as well as in predicting side and/or byproducts.
Extension to multistep CRMs relevant to prebiotic chemistry, demonstrates the
ability of DeepMech in effectively reconstructing pathways from simple
primordial substrates to complex biomolecules such as serine and aldopentose.
Attention analysis identifies reactive atoms/bonds in line with chemical
intuition, rendering our model interpretable and suitable for reaction design.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [233] [Training thermodynamic computers by gradient descent](https://arxiv.org/abs/2509.15324)
*Stephen Whitelam*

Main category: cond-mat.stat-mech

TL;DR: 本文展示通过梯度下降调整热力学计算机参数以执行指定计算，用师生方案训练，在图像分类任务验证，证明梯度下降是可行训练方法。


<details>
  <summary>Details</summary>
Motivation: 探索如何让热力学计算机执行指定计算，将机器学习核心方法应用到该新兴领域。

Method: 在数字模拟中，通过最大化计算机生成理想动态轨迹的概率进行训练，理想轨迹模拟神经网络激活，采用师生方案。

Result: 在图像分类任务中验证方法可行，热力学实现与数字实现的能量成本比超七个数量级。

Conclusion: 梯度下降是热力学计算可行的训练方法，可将机器学习核心方法应用于该领域。

Abstract: We show how to adjust the parameters of a thermodynamic computer by gradient
descent in order to perform a desired computation at a specified observation
time. Within a digital simulation of a thermodynamic computer, training
proceeds by maximizing the probability with which the computer would generate
an idealized dynamical trajectory. The idealized trajectory is designed to
reproduce the activations of a neural network trained to perform the desired
computation. This teacher-student scheme results in a thermodynamic computer
whose finite-time dynamics enacts a computation analogous to that of the neural
network. The parameters identified in this way can be implemented in the
hardware realization of the thermodynamic computer, which will perform the
desired computation automatically, driven by thermal noise. We demonstrate the
method on a standard image-classification task, and estimate the thermodynamic
advantage -- the ratio of energy costs of the digital and thermodynamic
implementations -- to exceed seven orders of magnitude. Our results establish
gradient descent as a viable training method for thermodynamic computing,
enabling application of the core methodology of machine learning to this
emerging field.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [234] [CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion](https://arxiv.org/abs/2509.16112)
*Sheng Zhang,Yifan Ding,Shuquan Lian,Shun Song,Hui Li*

Main category: cs.CL

TL;DR: 介绍用于检索增强的仓库级代码补全框架CodeRAG，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有仓库级代码补全方法存在查询构建不当、单路径代码检索、代码检索器与代码大语言模型不匹配等问题。

Method: 提出CodeRAG框架，包含对数概率引导的查询构建、多路径代码检索和偏好对齐的BestFit重排序等核心组件。

Result: 在基准测试ReccEval和CCEval上的实验表明，CodeRAG显著且持续地优于现有方法。

Conclusion: CodeRAG是有效的仓库级代码补全框架，代码可在https://github.com/KDEGroup/CodeRAG获取。

Abstract: Repository-level code completion automatically predicts the unfinished code
based on the broader information from the repository. Recent strides in Code
Large Language Models (code LLMs) have spurred the development of
repository-level code completion methods, yielding promising results.
Nevertheless, they suffer from issues such as inappropriate query construction,
single-path code retrieval, and misalignment between code retriever and code
LLM. To address these problems, we introduce CodeRAG, a framework tailored to
identify relevant and necessary knowledge for retrieval-augmented
repository-level code completion. Its core components include log probability
guided query construction, multi-path code retrieval, and preference-aligned
BestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval
demonstrate that CodeRAG significantly and consistently outperforms
state-of-the-art methods. The implementation of CodeRAG is available at
https://github.com/KDEGroup/CodeRAG.

</details>


### [235] [RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation](https://arxiv.org/abs/2509.16198)
*Jane Luo,Xin Zhang,Steven Liu,Jie Wu,Yiming Huang,Yangyu Huang,Chengyu Yin,Ying Xin,Jianfeng Liu,Yuefeng Zhan,Hao Sun,Qi Chen,Scarlett Li,Mao Yang*

Main category: cs.CL

TL;DR: 现有大语言模型在完整代码仓库生成上有挑战，本文提出RPG并开发ZeroRepo框架，在RepoCraft基准测试上表现优异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以从0生成完整代码仓库，自然语言不适合表示复杂软件结构。

Method: 引入Repository Planning Graph (RPG)统一规划，开发ZeroRepo框架，分三阶段操作，构建RepoCraft基准测试。

Result: ZeroRepo在RepoCraft上生成代码行数多，功能覆盖率和通过率高，远超基线模型。

Conclusion: RPG能建模复杂依赖，支持规划扩展，增强大语言模型对仓库的理解，加速智能体定位。

Abstract: Large language models excel at function- and file-level code generation, yet
generating complete repositories from scratch remains a fundamental challenge.
This process demands coherent and reliable planning across proposal- and
implementation-level stages, while natural language, due to its ambiguity and
verbosity, is ill-suited for faithfully representing complex software
structures. To address this, we introduce the Repository Planning Graph (RPG),
a persistent representation that unifies proposal- and implementation-level
planning by encoding capabilities, file structures, data flows, and functions
in one graph. RPG replaces ambiguous natural language with an explicit
blueprint, enabling long-horizon planning and scalable repository generation.
Building on RPG, we develop ZeroRepo, a graph-driven framework for repository
generation from scratch. It operates in three stages: proposal-level planning
and implementation-level refinement to construct the graph, followed by
graph-guided code generation with test validation. To evaluate this setting, we
construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.
On RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly
3.9$\times$ the strongest baseline (Claude Code) and about 64$\times$ other
baselines. It attains 81.5% functional coverage and a 69.7% pass rate,
exceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further
analysis shows that RPG models complex dependencies, enables progressively more
sophisticated planning through near-linear scaling, and enhances LLM
understanding of repositories, thereby accelerating agent localization.

</details>


### [236] [Synthetic bootstrapped pretraining](https://arxiv.org/abs/2509.15248)
*Zitong Yang,Aonan Zhang,Hong Liu,Tatsunori Hashimoto,Emmanuel Candès,Chong Wang,Ruoming Pang*

Main category: cs.CL

TL;DR: 介绍了合成自举预训练（SBP）方法，通过学习文档关系合成新语料进行联合训练，验证显示其能提升模型性能且有贝叶斯解释。


<details>
  <summary>Details</summary>
Motivation: 标准预训练无法有效建模文档间的丰富关联，而这些关联可能带来更好性能，因此提出SBP。

Method: 先从预训练数据集中学习文档间关系模型，再利用该模型合成新语料进行联合训练。

Result: 设计计算匹配的预训练设置，从零开始用1T标记预训练3B参数模型，SBP持续优于强重复基线，能达到使用多20倍唯一数据的理想上限的部分性能提升。

Conclusion: SBP不仅有良好的实证表现，还有自然的贝叶斯解释，即合成器隐式学习抽象相关文档间的潜在概念。

Abstract: We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM)
pretraining procedure that first learns a model of relations between documents
from the pretraining dataset and then leverages it to synthesize a vast new
corpus for joint training. While the standard pretraining teaches LMs to learn
causal correlations among tokens within a single document, it is not designed
to efficiently model the rich, learnable inter-document correlations that can
potentially lead to better performance. We validate SBP by designing a
compute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T
tokens from scratch. We find SBP consistently improves upon a strong repetition
baseline and delivers a significant fraction of performance improvement
attainable by an oracle upper bound with access to 20x more unique data.
Qualitative analysis reveals that the synthesized documents go beyond mere
paraphrases -- SBP first abstracts a core concept from the seed material and
then crafts a new narration on top of it. Besides strong empirical performance,
SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns
to abstract the latent concepts shared between related documents.

</details>


### [237] [Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing](https://arxiv.org/abs/2509.15361)
*Zichen Wu,Hsiu-Yuan Huang,Yunfang Wu*

Main category: cs.CL

TL;DR: 本文提出基于因果中介的去偏框架解决MLLMs表面关联偏差问题，在多模态任务中效果超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在复杂多模态推理任务中常依赖虚假关联，影响鲁棒性和泛化能力。

Method: 通过反事实示例区分核心语义与虚假上下文以进行训练阶段去偏，采用带动态路由的混合专家架构选择性调用特定模态去偏专家。

Result: 在多模态讽刺检测和情感分析任务中，该框架显著超越单模态去偏策略和现有最先进模型。

Conclusion: 提出的基于因果中介的去偏框架能有效解决MLLMs的表面关联偏差问题。

Abstract: Multimodal Large Language Models (MLLMs) have shown substantial capabilities
in integrating visual and textual information, yet frequently rely on spurious
correlations, undermining their robustness and generalization in complex
multimodal reasoning tasks. This paper addresses the critical challenge of
superficial correlation bias in MLLMs through a novel causal mediation-based
debiasing framework. Specially, we distinguishing core semantics from spurious
textual and visual contexts via counterfactual examples to activate
training-stage debiasing and employ a Mixture-of-Experts (MoE) architecture
with dynamic routing to selectively engages modality-specific debiasing
experts. Empirical evaluation on multimodal sarcasm detection and sentiment
analysis tasks demonstrates that our framework significantly surpasses unimodal
debiasing strategies and existing state-of-the-art models.

</details>


### [238] [Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data](https://arxiv.org/abs/2509.15419)
*Claudio Benzoni,Martina Langhals,Martin Boeker,Luise Modersohn,Máté E. Maros*

Main category: cs.CL

TL;DR: 本文研究非特定领域抽象摘要编解码模型在医学领域微调过程，用PEGASUS和PEGASUS - X在放射报告数据集实验，指出微调高表达模型处理稀缺数据的挑战和风险，为后续研究奠基。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能发展迅速，但抽象摘要在医学等敏感和数据受限领域仍具挑战，自动化医学文本摘要工具需求增加。

Method: 使用PEGASUS和PEGASUS - X在中型放射报告公共数据集上，对每个模型用不同大小相同训练数据的两个不同检查点进行全面评估，在固定大小验证集上用词汇和语义指标监控模型性能。

Result: PEGASUS呈现不同阶段，与逐轮双重下降或峰降恢复行为有关；PEGASUS - X使用更大检查点会导致性能下降。

Conclusion: 强调处理稀缺训练数据时微调高表达模型的挑战和风险，为专业领域摘要模型更稳健微调策略的后续研究奠定基础。

Abstract: Regardless of the rapid development of artificial intelligence, abstractive
summarisation is still challenging for sensitive and data-restrictive domains
like medicine. With the increasing number of imaging, the relevance of
automated tools for complex medical text summarisation is expected to become
highly relevant. In this paper, we investigated the adaptation via fine-tuning
process of a non-domain-specific abstractive summarisation encoder-decoder
model family, and gave insights to practitioners on how to avoid over- and
underfitting. We used PEGASUS and PEGASUS-X, on a medium-sized radiological
reports public dataset. For each model, we comprehensively evaluated two
different checkpoints with varying sizes of the same training data. We
monitored the models' performances with lexical and semantic metrics during the
training history on the fixed-size validation set. PEGASUS exhibited different
phases, which can be related to epoch-wise double-descent, or
peak-drop-recovery behaviour. For PEGASUS-X, we found that using a larger
checkpoint led to a performance detriment. This work highlights the challenges
and risks of fine-tuning models with high expressivity when dealing with scarce
training data, and lays the groundwork for future investigations into more
robust fine-tuning strategies for summarisation models in specialised domains.

</details>


### [239] [PILOT: Steering Synthetic Data Generation with Psychological & Linguistic Output Targeting](https://arxiv.org/abs/2509.15447)
*Caitlin Cisar,Emily Sheffield,Joshua Drake,Alden Harrell,Subramanian Chidambaram,Nikita Nangia,Vinayak Arannil,Alex Williams*

Main category: cs.CL

TL;DR: 提出PILOT框架用结构化心理语言学配置文件引导大语言模型，评估发现基于模式的方法能减少重复、提高连贯性，不同引导方法各有优劣，PILOT能保持高响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI应用依赖自然语言表示的用户角色会使模型做出意外推断，限制对输出的精确控制。

Method: 引入PILOT两阶段框架，将自然语言角色描述转换为多维配置文件并引导生成，在三种最先进的大语言模型上用25个合成角色在三种条件下进行评估。

Result: 基于模式的方法显著减少人工感的角色重复、提高输出连贯性，不同引导方法各有特点，HPS能平衡优势。

Conclusion: PILOT在所有条件下都能保持高响应质量，不同引导方法无显著统计差异。

Abstract: Generative AI applications commonly leverage user personas as a steering
mechanism for synthetic data generation, but reliance on natural language
representations forces models to make unintended inferences about which
attributes to emphasize, limiting precise control over outputs. We introduce
PILOT (Psychological and Linguistic Output Targeting), a two-phase framework
for steering large language models with structured psycholinguistic profiles.
In Phase 1, PILOT translates natural language persona descriptions into
multidimensional profiles with normalized scores across linguistic and
psychological dimensions. In Phase 2, these profiles guide generation along
measurable axes of variation. We evaluate PILOT across three state-of-the-art
LLMs (Mistral Large 2, Deepseek-R1, LLaMA 3.3 70B) using 25 synthetic personas
under three conditions: Natural-language Persona Steering (NPS), Schema-Based
Steering (SBS), and Hybrid Persona-Schema Steering (HPS). Results demonstrate
that schema-based approaches significantly reduce artificial-sounding persona
repetition while improving output coherence, with silhouette scores increasing
from 0.098 to 0.237 and topic purity from 0.773 to 0.957. Our analysis reveals
a fundamental trade-off: SBS produces more concise outputs with higher topical
consistency, while NPS offers greater lexical diversity but reduced
predictability. HPS achieves a balance between these extremes, maintaining
output variety while preserving structural consistency. Expert linguistic
evaluation confirms that PILOT maintains high response quality across all
conditions, with no statistically significant differences between steering
approaches.

</details>


### [240] [mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment](https://arxiv.org/abs/2509.15485)
*Ahmed Abdou*

Main category: cs.CL

TL;DR: 提出用于细粒度阿拉伯语可读性分类的后处理技术，能提升QWK分数，对教育评估有帮助。


<details>
  <summary>Details</summary>
Motivation: 解决细粒度阿拉伯语可读性分类问题，减少高惩罚错误分类以提高评估准确性。

Method: 应用共形预测生成有覆盖保证的预测集，用softmax重归一化概率计算加权平均。

Result: 在不同基础模型上一致提升QWK 1 - 3分，严格赛道中句子和文档级别取得高QWK分数。

Conclusion: 该方法结合统计保证和实用性，让人类评审聚焦少数合理级别，适用于阿拉伯语教育评估。

Abstract: We present a simple, model-agnostic post-processing technique for
fine-grained Arabic readability classification in the BAREC 2025 Shared Task
(19 ordinal levels). Our method applies conformal prediction to generate
prediction sets with coverage guarantees, then computes weighted averages using
softmax-renormalized probabilities over the conformal sets. This
uncertainty-aware decoding improves Quadratic Weighted Kappa (QWK) by reducing
high-penalty misclassifications to nearer levels. Our approach shows consistent
QWK improvements of 1-3 points across different base models. In the strict
track, our submission achieves QWK scores of 84.9\%(test) and 85.7\% (blind
test) for sentence level, and 73.3\% for document level. For Arabic educational
assessment, this enables human reviewers to focus on a handful of plausible
levels, combining statistical guarantees with practical usability.

</details>


### [241] [How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages](https://arxiv.org/abs/2509.15518)
*Siyang Wu,Zhewei Sun*

Main category: cs.CL

TL;DR: 本文对人类和大语言模型生成的俚语用法进行系统比较，发现大语言模型感知俚语存在显著偏差，其关于俚语创造性的知识与人类对齐不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在俚语检测和解释等中介任务中应用增多，但能否捕捉与人类相符的结构知识存疑，需进行系统比较。

Method: 构建评估框架，从三个核心方面比较在线俚语词典中的人类俚语用法和GPT - 4o、Llama - 3生成的俚语用法。

Result: 大语言模型在感知俚语上存在显著偏差，虽捕捉到俚语创造性方面的知识，但与人类对齐不足。

Conclusion: 大语言模型关于俚语创造性的知识不足以支持其进行语言分析等外推任务。

Abstract: Slang is a commonly used type of informal language that poses a daunting
challenge to NLP systems. Recent advances in large language models (LLMs),
however, have made the problem more approachable. While LLM agents are becoming
more widely applied to intermediary tasks such as slang detection and slang
interpretation, their generalizability and reliability are heavily dependent on
whether these models have captured structural knowledge about slang that align
well with human attested slang usages. To answer this question, we contribute a
systematic comparison between human and machine-generated slang usages. Our
evaluative framework focuses on three core aspects: 1) Characteristics of the
usages that reflect systematic biases in how machines perceive slang, 2)
Creativity reflected by both lexical coinages and word reuses employed by the
slang usages, and 3) Informativeness of the slang usages when used as
gold-standard examples for model distillation. By comparing human-attested
slang usages from the Online Slang Dictionary (OSD) and slang generated by
GPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our
results suggest that while LLMs have captured significant knowledge about the
creative aspects of slang, such knowledge does not align with humans
sufficiently to enable LLMs for extrapolative tasks such as linguistic
analyses.

</details>


### [242] [Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining](https://arxiv.org/abs/2509.15556)
*Ping Guo,Yubing Ren,Binbin Liu,Fengze Liu,Haobin Lin,Yifan Zhang,Bingni Zhang,Taifeng Wang,Yin Zheng*

Main category: cs.CL

TL;DR: 本文提出Climb框架优化多语言数据分配，实验表明其能准确测量跨语言交互，用其分配比例训练的大语言模型取得了最先进的多语言性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对多语言能力需求大，确定训练语料中最优语言比例极具挑战。

Method: 引入跨语言交互感知的语言比例，提出两步优化程序，先均衡各语言边际效益，再最大化语言分配向量的大小。

Result: Climb能准确测量各种多语言环境下的跨语言交互，用Climb分配比例训练的大语言模型取得了最先进的多语言性能。

Conclusion: Climb框架可有效优化多语言数据分配，提升大语言模型的多语言性能。

Abstract: Large language models (LLMs) have become integral to a wide range of
applications worldwide, driving an unprecedented global demand for effective
multilingual capabilities. Central to achieving robust multilingual performance
is the strategic allocation of language proportions within training corpora.
However, determining optimal language ratios is highly challenging due to
intricate cross-lingual interactions and sensitivity to dataset scale. This
paper introduces Climb (Cross-Lingual Interaction-aware Multilingual
Balancing), a novel framework designed to systematically optimize multilingual
data allocation. At its core, Climb introduces a cross-lingual
interaction-aware language ratio, explicitly quantifying each language's
effective allocation by capturing inter-language dependencies. Leveraging this
ratio, Climb proposes a principled two-step optimization procedure--first
equalizing marginal benefits across languages, then maximizing the magnitude of
the resulting language allocation vectors--significantly simplifying the
inherently complex multilingual optimization problem. Extensive experiments
confirm that Climb can accurately measure cross-lingual interactions across
various multilingual settings. LLMs trained with Climb-derived proportions
consistently achieve state-of-the-art multilingual performance, even achieving
competitive performance with open-sourced LLMs trained with more tokens.

</details>


### [243] [LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs](https://arxiv.org/abs/2509.15568)
*Junlong Jia,Xing Wu,Chaochen Gao,Ziyang Chen,Zijia Lin,Zhongzhi Li,Weinong Wang,Haotian Xu,Donghui Jin,Debing Zhang,Binghui Guo*

Main category: cs.CL

TL;DR: 提出资源高效的长上下文数据合成方法LiteLong，降低成本并实现有竞争力的性能，促进长上下文语言训练研究。


<details>
  <summary>Details</summary>
Motivation: 现有基于相关性聚合的长上下文数据合成方法存在计算效率问题，高质量长上下文数据对训练大语言模型很重要。

Method: 利用BISAC图书分类系统进行结构化主题组织，采用多智能体辩论机制生成主题，用轻量级BM25检索相关文档并拼接成训练样本。

Result: 在HELMET和Ruler基准测试中，LiteLong实现了有竞争力的长上下文性能，能与其他长依赖增强方法无缝集成。

Conclusion: LiteLong降低了计算和数据工程成本，使高质量长上下文数据合成更易实现，有助于长上下文语言训练研究。

Abstract: High-quality long-context data is essential for training large language
models (LLMs) capable of processing extensive documents, yet existing synthesis
approaches using relevance-based aggregation face challenges of computational
efficiency. We present LiteLong, a resource-efficient method for synthesizing
long-context data through structured topic organization and multi-agent debate.
Our approach leverages the BISAC book classification system to provide a
comprehensive hierarchical topic organization, and then employs a debate
mechanism with multiple LLMs to generate diverse, high-quality topics within
this structure. For each topic, we use lightweight BM25 retrieval to obtain
relevant documents and concatenate them into 128K-token training samples.
Experiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves
competitive long-context performance and can seamlessly integrate with other
long-dependency enhancement methods. LiteLong makes high-quality long-context
data synthesis more accessible by reducing both computational and data
engineering costs, facilitating further research in long-context language
training.

</details>


### [244] [Relevance to Utility: Process-Supervised Rewrite for RAG](https://arxiv.org/abs/2509.15577)
*Jaeyoung Kim,Jongho Kim,Seung-won Hwang,Seoho Song,Young-In Song*

Main category: cs.CL

TL;DR: 提出R2U方法，通过过程监督最大化生成正确答案概率，用LLM监督蒸馏管道让小模型泛化更好，在多基准测试中优于基线。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成系统在优化检索相关性和生成实用性之间存在差距，现有“桥接”模块无法有效捕捉文档实用性。

Method: 提出R2U方法直接优化以最大化生成正确答案概率，提出用LLM监督的蒸馏管道进行近似。

Result: 在多个开放域问答基准测试中，相比强桥接基线有一致的改进。

Conclusion: R2U方法在解决检索增强生成系统问题上有效。

Abstract: Retrieval-Augmented Generation systems often suffer from a gap between
optimizing retrieval relevance and generative utility: retrieved documents may
be topically relevant but still lack the content needed for effective reasoning
during generation. While existing "bridge" modules attempt to rewrite the
retrieved text for better generation, we show how they fail to capture true
document utility. In this work, we propose R2U, with a key distinction of
directly optimizing to maximize the probability of generating a correct answer
through process supervision. As such direct observation is expensive, we also
propose approximating an efficient distillation pipeline by scaling the
supervision from LLMs, which helps the smaller rewriter model generalize
better. We evaluate our method across multiple open-domain question-answering
benchmarks. The empirical results demonstrate consistent improvements over
strong bridging baselines.

</details>


### [245] [DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models](https://arxiv.org/abs/2509.15587)
*Tsz Ting Chung,Lemao Liu,Mo Yu,Dit-Yan Yeung*

Main category: cs.CL

TL;DR: 本文提出新的经典逻辑基准DivLogicEval及评估指标，用于评估大语言模型逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准可能混淆多种推理技能、语言多样性有限且分布偏离理想状态，导致评估结果有偏差。

Method: 提出新基准DivLogicEval，由反直觉自然语句组成，还引入新评估指标减轻大模型固有偏差和随机性影响。

Result: 通过实验展示回答DivLogicEval问题所需的逻辑推理程度，并比较不同流行大模型的逻辑推理表现。

Conclusion: 新基准和评估指标有助于更可靠地评估大语言模型的逻辑推理能力。

Abstract: Logic reasoning in natural language has been recognized as an important
measure of human intelligence for Large Language Models (LLMs). Popular
benchmarks may entangle multiple reasoning skills and thus provide unfaithful
evaluations on the logic reasoning skill. Meanwhile, existing logic reasoning
benchmarks are limited in language diversity and their distributions are
deviated from the distribution of an ideal logic reasoning benchmark, which may
lead to biased evaluation results. This paper thereby proposes a new classical
logic benchmark DivLogicEval, consisting of natural sentences composed of
diverse statements in a counterintuitive way. To ensure a more reliable
evaluation, we also introduce a new evaluation metric that mitigates the
influence of bias and randomness inherent in LLMs. Through experiments, we
demonstrate the extent to which logical reasoning is required to answer the
questions in DivLogicEval and compare the performance of different popular LLMs
in conducting logical reasoning.

</details>


### [246] [Once Upon a Time: Interactive Learning for Storytelling with Small Language Models](https://arxiv.org/abs/2509.15714)
*Jonas Mayer Martins,Ali Hamza Bashir,Muhammad Rehan Khalid,Lisa Beinborn*

Main category: cs.CL

TL;DR: 研究语言模型能否通过高级反馈减少训练数据，发现高级反馈数据效率高。


<details>
  <summary>Details</summary>
Motivation: 儿童通过社交互动高效习得语言，而大语言模型通常用大量文本进行下一个词预测训练，由此研究语言模型能否通过高级反馈减少训练数据。

Method: 训练学生模型生成故事，教师模型对可读性、叙事连贯性和创造性进行评分，改变反馈循环前的预训练量，评估交互式学习对形式和功能语言能力的影响。

Result: 高级反馈数据效率高，仅100万个单词的交互式学习输入，讲故事技能提升效果等同于4.1亿个单词的下一个词预测。

Conclusion: 高级反馈能让语言模型用更少数据达到较好训练效果。

Abstract: Children efficiently acquire language not just by listening, but by
interacting with others in their social environment. Conversely, large language
models are typically trained with next-word prediction on massive amounts of
text. Motivated by this contrast, we investigate whether language models can be
trained with less data by learning not only from next-word prediction but also
from high-level, cognitively inspired feedback. We train a student model to
generate stories, which a teacher model rates on readability, narrative
coherence, and creativity. By varying the amount of pretraining before the
feedback loop, we assess the impact of this interactive learning on formal and
functional linguistic competence. We find that the high-level feedback is
highly data efficient: With just 1 M words of input in interactive learning,
storytelling skills can improve as much as with 410 M words of next-word
prediction.

</details>


### [247] [Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning](https://arxiv.org/abs/2509.15811)
*Sara Rajaee,Rochelle Choenni,Ekaterina Shutova,Christof Monz*

Main category: cs.CL

TL;DR: 研究多语言大语言模型推理能力跨语言差异，训练跨语言奖励模型提升数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 探究多语言大语言模型推理能力在不同语言间的差异，以及不同语言推理路径是否互补。

Method: 训练奖励模型对不同语言生成的问题回复进行排序。

Result: 跨语言奖励模型比单语言奖励建模显著提升数学推理性能，低采样预算下跨语言采样对英语尤其有益。

Conclusion: 利用不同语言的互补优势可改善多语言推理。

Abstract: While the reasoning abilities of large language models (LLMs) continue to
advance, it remains unclear how such ability varies across languages in
multilingual LLMs and whether different languages produce reasoning paths that
complement each other. To investigate this question, we train a reward model to
rank generated responses for a given question across languages. Our results
show that our cross-lingual reward model substantially improves mathematical
reasoning performance compared to using reward modeling within a single
language, benefiting even high-resource languages. While English often exhibits
the highest performance in multilingual models, we find that cross-lingual
sampling particularly benefits English under low sampling budgets. Our findings
reveal new opportunities to improve multilingual reasoning by leveraging the
complementary strengths of diverse languages.

</details>


### [248] [Distribution-Aligned Decoding for Efficient LLM Task Adaptation](https://arxiv.org/abs/2509.15888)
*Senkang Hu,Xudong Han,Jinqi Jiang,Yihang Tao,Zihan Fang,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.CL

TL;DR: 现有大语言模型任务适配成本高，提出SVD方法，理论证明其与全微调等价，实验表明与PEFT结合能提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决大参数语言模型下游任务适配成本高的问题。

Method: 将任务适配视为输出分布对齐，提出SVD方法，通过热启动微调、提取转向向量引导解码。

Result: 在三个任务和九个基准测试中，SVD与四种标准PEFT方法结合，提升了多项指标性能。

Conclusion: SVD为大语言模型任务适配提供了轻量级、有理论依据的途径。

Abstract: Adapting billion-parameter language models to a downstream task is still
costly, even with parameter-efficient fine-tuning (PEFT). We re-cast task
adaptation as output-distribution alignment: the objective is to steer the
output distribution toward the task distribution directly during decoding
rather than indirectly through weight updates. Building on this view, we
introduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and
theoretically grounded method. We start with a short warm-start fine-tune and
extract a task-aware steering vector from the Kullback-Leibler (KL) divergence
gradient between the output distribution of the warm-started and pre-trained
models. This steering vector is then used to guide the decoding process to
steer the model's output distribution towards the task distribution. We
theoretically prove that SVD is first-order equivalent to the gradient step of
full fine-tuning and derive a globally optimal solution for the strength of the
steering vector. Across three tasks and nine benchmarks, SVD paired with four
standard PEFT methods improves multiple-choice accuracy by up to 5 points and
open-ended truthfulness by 2 points, with similar gains (1-2 points) on
commonsense datasets without adding trainable parameters beyond the PEFT
adapter. SVD thus offers a lightweight, theoretically grounded path to stronger
task adaptation for large language models.

</details>


### [249] [Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions](https://arxiv.org/abs/2509.15901)
*Frederic Kirstein,Sonu Kumar,Terry Ruas,Bela Gipp*

Main category: cs.CL

TL;DR: 提出FRAME模块化管道、SCOPE协议和P - MESA评估框架提升会议摘要质量，减少错误，强调重新思考摘要方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型进行会议摘要易出错，存在幻觉、遗漏和不相关内容。

Method: 提出FRAME将摘要转化为语义丰富任务；引入SCOPE通过回答九个问题构建推理轨迹实现摘要个性化；提出P - MESA无参考评估框架。

Result: P - MESA评估框架准确率超89%，与人工评分相关性强；FRAME减少幻觉和遗漏，SCOPE提升知识契合度和目标一致性。

Conclusion: 应重新思考摘要方法以提高可控性、忠实性和个性化。

Abstract: Meeting summarization with large language models (LLMs) remains error-prone,
often producing outputs with hallucinations, omissions, and irrelevancies. We
present FRAME, a modular pipeline that reframes summarization as a semantic
enrichment task. FRAME extracts and scores salient facts, organizes them
thematically, and uses these to enrich an outline into an abstractive summary.
To personalize summaries, we introduce SCOPE, a reason-out-loud protocol that
has the model build a reasoning trace by answering nine questions before
content selection. For evaluation, we propose P-MESA, a multi-dimensional,
reference-free evaluation framework to assess if a summary fits a target
reader. P-MESA reliably identifies error instances, achieving >= 89% balanced
accuracy against human annotations and strongly aligns with human severity
ratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and
omission by 2 out of 5 points (measured with MESA), while SCOPE improves
knowledge fit and goal alignment over prompt-only baselines. Our findings
advocate for rethinking summarization to improve control, faithfulness, and
personalization.

</details>


### [250] [Quantifying Uncertainty in Natural Language Explanations of Large Language Models for Question Answering](https://arxiv.org/abs/2509.15403)
*Yangyi Li,Mengdi Huai*

Main category: cs.CL

TL;DR: 本文针对大语言模型自然语言解释缺乏有效不确定性保证的问题，提出框架和方法，并通过实验验证其性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏为大语言模型生成的自然语言解释提供有效不确定性保证的研究，而不确定性量化对理解解释的置信度很关键。

Method: 提出一种新的不确定性估计框架，以事后且与模型无关的方式提供有效不确定性保证；设计一种新的鲁棒不确定性估计方法，在有噪声情况下也能维持有效不确定性保证。

Result: 在问答任务上的大量实验证明了所提方法的理想性能。

Conclusion: 所提出的不确定性估计框架和鲁棒估计方法能为大语言模型自然语言解释提供有效不确定性保证。

Abstract: Large language models (LLMs) have shown strong capabilities, enabling
concise, context-aware answers in question answering (QA) tasks. The lack of
transparency in complex LLMs has inspired extensive research aimed at
developing methods to explain large language behaviors. Among existing
explanation methods, natural language explanations stand out due to their
ability to explain LLMs in a self-explanatory manner and enable the
understanding of model behaviors even when the models are closed-source.
However, despite these promising advancements, there is no existing work
studying how to provide valid uncertainty guarantees for these generated
natural language explanations. Such uncertainty quantification is critical in
understanding the confidence behind these explanations. Notably, generating
valid uncertainty estimates for natural language explanations is particularly
challenging due to the auto-regressive generation process of LLMs and the
presence of noise in medical inquiries. To bridge this gap, in this work, we
first propose a novel uncertainty estimation framework for these generated
natural language explanations, which provides valid uncertainty guarantees in a
post-hoc and model-agnostic manner. Additionally, we also design a novel robust
uncertainty estimation method that maintains valid uncertainty guarantees even
under noise. Extensive experiments on QA tasks demonstrate the desired
performance of our methods.

</details>


### [251] [BEFT: Bias-Efficient Fine-Tuning of Language Models](https://arxiv.org/abs/2509.15974)
*Baichuan Huang,Ananth Balashankar,Amir Aminifar*

Main category: cs.CL

TL;DR: 本文提出了用于选择待微调偏置项的方法BEFT，在多种大语言模型和下游任务上验证了其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在选择用于有效微调的特定偏置项方面指导有限，不同偏置项微调与下游性能的联系不明确。

Method: 提出一种选择待微调偏置项的方法，形成偏置高效微调（BEFT）。

Result: 在多种大语言模型和下游任务上广泛评估，证明了BEFT方法的有效性和优越性。

Conclusion: 所提出的BEFT方法在不同下游任务中表现良好，是一种有效的偏置项选择微调方法。

Abstract: Fine-tuning all-bias-terms stands out among various parameter-efficient
fine-tuning (PEFT) techniques, owing to its out-of-the-box usability and
competitive performance, especially in low-data regimes. Bias-only fine-tuning
has the potential for unprecedented parameter efficiency. However, the link
between fine-tuning different bias terms (i.e., bias terms in the query, key,
or value projections) and downstream performance remains unclear. The existing
approaches, e.g., based on the magnitude of bias change or empirical Fisher
information, provide limited guidance for selecting the particular bias term
for effective fine-tuning. In this paper, we propose an approach for selecting
the bias term to be fine-tuned, forming the foundation of our bias-efficient
fine-tuning (BEFT). We extensively evaluate our bias-efficient approach against
other bias-selection approaches, across a wide range of large language models
(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B
parameters. Our results demonstrate the effectiveness and superiority of our
bias-efficient approach on diverse downstream tasks, including classification,
multiple-choice, and generation tasks.

</details>


### [252] [Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning](https://arxiv.org/abs/2509.16025)
*Hong-Yun Lin,Jhen-Ke Lin,Chung-Chun Wang,Hao-Chien Lu,Berlin Chen*

Main category: cs.CL

TL;DR: 本文提出一种新的多模态基础模型方法用于口语评估，在基准测试中表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有口语评估方法存在误差传播或遗漏语篇级证据的问题，随着二语英语学习者增多，对可靠口语评估需求增加。

Method: 引入多模态基础模型，结合多目标学习和基于Whisper ASR模型的语音先验进行声学感知校准，单遍完成会话级评估。

Result: 在Speak & Improve基准测试中，该方法优于先前的级联系统，具有稳健的跨部分泛化能力。

Conclusion: 提出的方法可生成适用于计算机辅助语言学习应用的紧凑可部署评分器。

Abstract: Spoken Language Assessment (SLA) estimates a learner's oral proficiency from
spontaneous speech. The growing population of L2 English speakers has
intensified the demand for reliable SLA, a critical component of Computer
Assisted Language Learning (CALL). Existing efforts often rely on cascaded
pipelines, which are prone to error propagation, or end-to-end models that
often operate on a short audio window, which might miss discourse-level
evidence. This paper introduces a novel multimodal foundation model approach
that performs session-level evaluation in a single pass. Our approach couples
multi-target learning with a frozen, Whisper ASR model-based speech prior for
acoustic-aware calibration, allowing for jointly learning holistic and
trait-level objectives of SLA without resorting to handcrafted features. By
coherently processing the entire response session of an L2 speaker, the model
excels at predicting holistic oral proficiency. Experiments conducted on the
Speak & Improve benchmark demonstrate that our proposed approach outperforms
the previous state-of-the-art cascaded system and exhibits robust cross-part
generalization, producing a compact deployable grader that is tailored for CALL
applications.

</details>


### [253] [Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech](https://arxiv.org/abs/2509.16028)
*Sang Hoon Woo,Sehun Lee,Kang-wook Kim,Gunhee Kim*

Main category: cs.CL

TL;DR: 提出Think - Verbalize - Speak框架及ReVerT语言转换器，提升语音自然度和简洁性，对推理影响小。


<details>
  <summary>Details</summary>
Motivation: 现有将大语言模型用于口语交流的方法未充分探索对推理性能的影响，直接应用大语言模型在口语交流效果不佳。

Method: 提出Think - Verbalize - Speak框架，将推理和口语表达解耦，引入基于增量和异步总结的低延迟ReVerT语言转换器。

Result: 在多个基准测试中，该方法提升了语音自然度和简洁性，对推理影响极小。

Conclusion: Think - Verbalize - Speak框架和ReVerT语言转换器有效，可在口语对话系统中更好利用大语言模型的推理能力。

Abstract: Spoken dialogue systems increasingly employ large language models (LLMs) to
leverage their advanced reasoning capabilities. However, direct application of
LLMs in spoken communication often yield suboptimal results due to mismatches
between optimal textual and verbal delivery. While existing approaches adapt
LLMs to produce speech-friendly outputs, their impact on reasoning performance
remains underexplored. In this work, we propose Think-Verbalize-Speak, a
framework that decouples reasoning from spoken delivery to preserve the full
reasoning capacity of LLMs. Central to our method is verbalizing, an
intermediate step that translates thoughts into natural, speech-ready text. We
also introduce ReVerT, a latency-efficient verbalizer based on incremental and
asynchronous summarization. Experiments across multiple benchmarks show that
our method enhances speech naturalness and conciseness with minimal impact on
reasoning. The project page with the dataset and the source code is available
at https://yhytoto12.github.io/TVS-ReVerT

</details>


### [254] [Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets](https://arxiv.org/abs/2509.15621)
*Tomoya Yamashita,Yuuki Yamanaka,Masanori Yamada,Takayuki Miura,Toshiki Shibahara,Tomoharu Iwata*

Main category: cs.CL

TL;DR: 提出概念遗忘（CU）解决现有机器遗忘方法无法移除更广泛概念的问题，利用知识图谱，提出新方法实现概念级遗忘并保留无关知识。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法需明确目标句子，不支持移除更广泛概念，如人物或事件，因此提出概念遗忘作为大语言模型遗忘的新需求。

Method: 利用知识图谱表示大语言模型内部知识，定义概念遗忘为移除遗忘目标节点及相关边；提出新方法，促使大语言模型生成关于遗忘目标的知识三元组和解释性句子，并对这些表示应用遗忘过程。

Result: 在真实和合成数据集上的实验表明，该方法能有效实现概念级遗忘，同时保留无关知识。

Conclusion: 所提方法可使遗忘过程与大语言模型内部知识表示对齐，实现更精确和全面的概念移除。

Abstract: Machine Unlearning (MU) has recently attracted considerable attention as a
solution to privacy and copyright issues in large language models (LLMs).
Existing MU methods aim to remove specific target sentences from an LLM while
minimizing damage to unrelated knowledge. However, these approaches require
explicit target sentences and do not support removing broader concepts, such as
persons or events. To address this limitation, we introduce Concept Unlearning
(CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to
represent the LLM's internal knowledge and define CU as removing the forgetting
target nodes and associated edges. This graph-based formulation enables a more
intuitive unlearning and facilitates the design of more effective methods. We
propose a novel method that prompts the LLM to generate knowledge triplets and
explanatory sentences about the forgetting target and applies the unlearning
process to these representations. Our approach enables more precise and
comprehensive concept removal by aligning the unlearning process with the LLM's
internal knowledge representations. Experiments on real-world and synthetic
datasets demonstrate that our method effectively achieves concept-level
unlearning while preserving unrelated knowledge.

</details>


### [255] [Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses](https://arxiv.org/abs/2509.16093)
*Fangyi Yu,Nabeel Seedat,Dasha Herrmannova,Frank Schilder,Jonathan Richard Schwarz*

Main category: cs.CL

TL;DR: 介绍了DeCE评估框架，用于长文本答案评估，在法律QA任务上表现好，有可解释性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 标准评估指标无法有效评估高风险领域长文本答案，现有大语言模型评估器有缺陷。

Method: 引入DeCE框架，分离精确率和召回率，自动从标准答案要求中提取特定标准，该框架与模型无关且通用。

Result: 在法律QA任务中，DeCE与专家判断的相关性更强，揭示了不同模型的权衡，且只需少量专家修订标准。

Conclusion: DeCE为专家领域提供了可解释且可行的大语言模型评估框架。

Abstract: Evaluating long-form answers in high-stakes domains such as law or medicine
remains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to
capture semantic correctness, and current LLM-based evaluators often reduce
nuanced aspects of answer quality into a single undifferentiated score. We
introduce DeCE, a decomposed LLM evaluation framework that separates precision
(factual accuracy and relevance) and recall (coverage of required concepts),
using instance-specific criteria automatically extracted from gold answer
requirements. DeCE is model-agnostic and domain-general, requiring no
predefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate
different LLMs on a real-world legal QA task involving multi-jurisdictional
reasoning and citation grounding. DeCE achieves substantially stronger
correlation with expert judgments ($r=0.78$), compared to traditional metrics
($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional
evaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist
models favor recall, while specialized models favor precision. Importantly,
only 11.95% of LLM-generated criteria required expert revision, underscoring
DeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation
framework in expert domains.

</details>


### [256] [Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models](https://arxiv.org/abs/2509.15631)
*Tomoya Yamashita,Akira Ito,Yuuki Yamanaka,Masanori Yamada,Takayuki Miura,Toshiki Shibahara*

Main category: cs.CL

TL;DR: 现有大语言模型去学习方法有缺陷，本文提出新方法干预模型内部激活，实现真正遗忘，避免过度抑制和模型崩溃，且实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用下，隐私和版权问题使有效去学习技术需求增加，现有抑制方法有不能消除知识和模型崩溃的问题。

Method: 提出新的去学习方法，在稀疏自编码器潜空间修改目标实体激活，使其接近未知实体。

Result: 该方法有效对齐遗忘目标的内部激活，减少模型在问答任务中对目标知识的回忆，且不损害非目标知识。

Conclusion: 新方法能实现真正遗忘，避免现有方法的问题，实验验证效果良好。

Abstract: As large language models (LLMs) are increasingly deployed across various
applications, privacy and copyright concerns have heightened the need for more
effective LLM unlearning techniques. Many existing unlearning methods aim to
suppress undesirable outputs through additional training (e.g., gradient
ascent), which reduces the probability of generating such outputs. While such
suppression-based approaches can control model outputs, they may not eliminate
the underlying knowledge embedded in the model's internal activations; muting a
response is not the same as forgetting it. Moreover, such suppression-based
methods often suffer from model collapse. To address these issues, we propose a
novel unlearning method that directly intervenes in the model's internal
activations. In our formulation, forgetting is defined as a state in which the
activation of a forgotten target is indistinguishable from that of ``unknown''
entities. Our method introduces an unlearning objective that modifies the
activation of the target entity away from those of known entities and toward
those of unknown entities in a sparse autoencoder latent space. By aligning the
target's internal activation with those of unknown entities, we shift the
model's recognition of the target entity from ``known'' to ``unknown'',
achieving genuine forgetting while avoiding over-suppression and model
collapse. Empirically, we show that our method effectively aligns the internal
activations of the forgotten target, a result that the suppression-based
approaches do not reliably achieve. Additionally, our method effectively
reduces the model's recall of target knowledge in question-answering tasks
without significant damage to the non-target knowledge.

</details>


### [257] [UPRPRC: Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations](https://arxiv.org/abs/2509.15789)
*Qiuyang Lu,Fangjian Shen,Zhengkai Tang,Qiang Liu,Hexuan Cheng,Hui Liu,Wushao Wen*

Main category: cs.CL

TL;DR: 本文提出端到端方案构建多语言语料库，采用新算法，语料规模翻倍且可复现，代码和语料开源。


<details>
  <summary>Details</summary>
Motivation: 以往基于联合国文件构建的语料库存在流程不透明、难以复现和规模有限等问题，需改进。

Method: 提出从网页抓取数据到文本对齐的端到端解决方案，采用新的Graph - Aided Paragraph Alignment (GAPA)算法进行段落对齐。

Result: 构建的语料库包含超7.13亿英语标记，规模是先前工作的两倍多，是最大的公开可用的纯人工翻译、非AI生成的平行语料库。

Conclusion: 所提出的方案有效解决了先前语料库的问题，代码和语料可通过MIT许可获取。

Abstract: The quality and accessibility of multilingual datasets are crucial for
advancing machine translation. However, previous corpora built from United
Nations documents have suffered from issues such as opaque process, difficulty
of reproduction, and limited scale. To address these challenges, we introduce a
complete end-to-end solution, from data acquisition via web scraping to text
alignment. The entire process is fully reproducible, with a minimalist
single-machine example and optional distributed computing steps for
scalability. At its core, we propose a new Graph-Aided Paragraph Alignment
(GAPA) algorithm for efficient and flexible paragraph-level alignment. The
resulting corpus contains over 713 million English tokens, more than doubling
the scale of prior work. To the best of our knowledge, this represents the
largest publicly available parallel corpus composed entirely of
human-translated, non-AI-generated content. Our code and corpus are accessible
under the MIT License.

</details>


### [258] [CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs](https://arxiv.org/abs/2509.16188)
*Jinghao Zhang,Sihang Jiang,Shiwei Guo,Shisong Chen,Yanghua Xiao,Hongwei Feng,Jiaqing Liang,Minggui HE,Shimin Tao,Hongxia Ma*

Main category: cs.CL

TL;DR: 为评估大语言模型文化理解能力，提出CultureScope框架，实验表明该方法有效，现有模型缺乏文化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多元文化环境中应用广泛，现有基准缺乏全面性，难以跨文化扩展和适应。

Method: 受文化冰山理论启发，设计3层140维度的文化知识分类维度模式，自动构建特定文化知识库和评估数据集。

Result: 方法能有效评估文化理解，现有大语言模型缺乏全面文化能力，仅引入多语言数据不一定提升文化理解。

Conclusion: 所提CultureScope框架能有效评估大语言模型的文化理解能力。

Abstract: As large language models (LLMs) are increasingly deployed in diverse cultural
environments, evaluating their cultural understanding capability has become
essential for ensuring trustworthy and culturally aligned applications.
However, most existing benchmarks lack comprehensiveness and are challenging to
scale and adapt across different cultural contexts, because their frameworks
often lack guidance from well-established cultural theories and tend to rely on
expert-driven manual annotations. To address these issues, we propose
CultureScope, the most comprehensive evaluation framework to date for assessing
cultural understanding in LLMs. Inspired by the cultural iceberg theory, we
design a novel dimensional schema for cultural knowledge classification,
comprising 3 layers and 140 dimensions, which guides the automated construction
of culture-specific knowledge bases and corresponding evaluation datasets for
any given languages and cultures. Experimental results demonstrate that our
method can effectively evaluate cultural understanding. They also reveal that
existing large language models lack comprehensive cultural competence, and
merely incorporating multilingual data does not necessarily enhance cultural
understanding. All code and data files are available at
https://github.com/HoganZinger/Culture

</details>


### [259] [Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay Assessment](https://arxiv.org/abs/2509.15926)
*Ahmed Karim,Qiao Wang,Zheng Yuan*

Main category: cs.CL

TL;DR: 现有自动作文评分系统在现实应用受限，本文用共形预测解决问题，微调开源大模型并校准，结果显示开源中型大模型可支持教师参与的自动作文评分。


<details>
  <summary>Details</summary>
Motivation: 自动作文评分系统在现实尤其是高风险考试中应用有限，主要原因是多数模型输出单一分数，缺乏置信度和解释。

Method: 使用共形预测，对两个开源大语言模型在三个不同语料库上进行微调，并在90%风险水平下校准，用UAcc评估可靠性。

Result: 校准后的模型能达到覆盖目标，同时保持预测集紧凑。

Conclusion: 开源中型大语言模型已能支持教师参与的自动作文评分，未来可进行扩展和更广泛的用户研究。

Abstract: Automated Essay Scoring (AES) systems now reach near human agreement on some
public benchmarks, yet real-world adoption, especially in high-stakes
examinations, remains limited. A principal obstacle is that most models output
a single score without any accompanying measure of confidence or explanation.
We address this gap with conformal prediction, a distribution-free wrapper that
equips any classifier with set-valued outputs and formal coverage guarantees.
Two open-source large language models (Llama-3 8B and Qwen-2.5 3B) are
fine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and
calibrated at a 90 percent risk level. Reliability is assessed with UAcc, an
uncertainty-aware accuracy that rewards models for being both correct and
concise. To our knowledge, this is the first work to combine conformal
prediction and UAcc for essay scoring. The calibrated models consistently meet
the coverage target while keeping prediction sets compact, indicating that
open-source, mid-sized LLMs can already support teacher-in-the-loop AES; we
discuss scaling and broader user studies as future work.

</details>


### [260] [Localmax dynamics for attention in transformers and its asymptotic behavior](https://arxiv.org/abs/2509.15958)
*Henri Cimetière,Maria Teresa Chiri,Bahman Gharesifard*

Main category: cs.CL

TL;DR: 介绍局部最大动态离散时间注意力模型，分析其性质并应用李雅普诺夫方法。


<details>
  <summary>Details</summary>
Motivation: 提出介于经典softmax和hardmax动态之间的新离散时间注意力模型。

Method: 理论证明，分析凸包结构，引入静止集；采用基于李雅普诺夫的方法。

Result: 系统凸包收敛到凸多面体，局部最大动态无有限时间收敛性，给出不同参数下结果。

Conclusion: 局部最大动态有独特性质，李雅普诺夫方法在不对称交互中有局限，指明未来研究方向。

Abstract: We introduce a new discrete-time attention model, termed the localmax
dynamics, which interpolates between the classic softmax dynamics and the
hardmax dynamics, where only the tokens that maximize the influence toward a
given token have a positive weight. As in hardmax, uniform weights are
determined by a parameter controlling neighbor influence, but the key extension
lies in relaxing neighborhood interactions through an alignment-sensitivity
parameter, which allows controlled deviations from pure hardmax behavior. As we
prove, while the convex hull of the token states still converges to a convex
polytope, its structure can no longer be fully described by a maximal alignment
set, prompting the introduction of quiescent sets to capture the invariant
behavior of tokens near vertices. We show that these sets play a key role in
understanding the asymptotic behavior of the system, even under time-varying
alignment sensitivity parameters. We further show that localmax dynamics does
not exhibit finite-time convergence and provide results for vanishing, nonzero,
time-varying alignment-sensitivity parameters, recovering the limiting behavior
of hardmax as a by-product. Finally, we adapt Lyapunov-based methods from
classical opinion dynamics, highlighting their limitations in the asymmetric
setting of localmax interactions and outlining directions for future research.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [261] [Emotion-Aware Speech Generation with Character-Specific Voices for Comics](https://arxiv.org/abs/2509.15253)
*Zhiwen Qian,Jinhua Liang,Huan Zhang*

Main category: cs.SD

TL;DR: 提出用于漫画的端到端特定角色、情感感知语音生成管道，实现自动旁白生成。


<details>
  <summary>Details</summary>
Motivation: 实现漫画的自动旁白生成，提供交互式和沉浸式漫画阅读体验。

Method: 图像模块进行角色检测、文本识别和情感强度识别；大语言模型结合视觉信息和情节上下文进行对话归因和情感分析；文本转语音模型根据角色和情感合成语音。

Result: 系统能以完整漫画卷为输入，生成与角色对话和情感状态对齐的语音。

Conclusion: 该工作为漫画自动旁白生成提供支持，迈向交互式和沉浸式漫画阅读体验。

Abstract: This paper presents an end-to-end pipeline for generating character-specific,
emotion-aware speech from comics. The proposed system takes full comic volumes
as input and produces speech aligned with each character's dialogue and
emotional state. An image processing module performs character detection, text
recognition, and emotion intensity recognition. A large language model performs
dialogue attribution and emotion analysis by integrating visual information
with the evolving plot context. Speech is synthesized through a text-to-speech
model with distinct voice profiles tailored to each character and emotion. This
work enables automated voiceover generation for comics, offering a step toward
interactive and immersive comic reading experience.

</details>


### [262] [Impact of Phonetics on Speaker Identity in Adversarial Voice Attack](https://arxiv.org/abs/2509.15437)
*Daniyal Kabir Dar,Qiben Yan,Li Xiao,Arun Ross*

Main category: cs.SD

TL;DR: 分析语音对抗扰动在语音层面的影响，发现其导致转录错误和身份漂移，需语音感知防御。


<details>
  <summary>Details</summary>
Motivation: 现有研究对语音对抗扰动的语音基础及其对说话人身份的影响探索不足。

Method: 以DeepSpeech为目标ASR模型，生成目标对抗样本，评估其对说话人嵌入的影响。

Result: 16个语音多样的目标短语实验表明，对抗音频会导致转录错误和身份漂移。

Conclusion: 需要语音感知防御来确保ASR和说话人识别系统的鲁棒性。

Abstract: Adversarial perturbations in speech pose a serious threat to automatic speech
recognition (ASR) and speaker verification by introducing subtle waveform
modifications that remain imperceptible to humans but can significantly alter
system outputs. While targeted attacks on end-to-end ASR models have been
widely studied, the phonetic basis of these perturbations and their effect on
speaker identity remain underexplored. In this work, we analyze adversarial
audio at the phonetic level and show that perturbations exploit systematic
confusions such as vowel centralization and consonant substitutions. These
distortions not only mislead transcription but also degrade phonetic cues
critical for speaker verification, leading to identity drift. Using DeepSpeech
as our ASR target, we generate targeted adversarial examples and evaluate their
impact on speaker embeddings across genuine and impostor samples. Results
across 16 phonetically diverse target phrases demonstrate that adversarial
audio induces both transcription errors and identity drift, highlighting the
need for phonetic-aware defenses to ensure the robustness of ASR and speaker
recognition systems.

</details>


### [263] [Contrastive Learning with Spectrum Information Augmentation in Abnormal Sound Detection](https://arxiv.org/abs/2509.15570)
*Xinxin Meng,Jiangtao Guo,Yunxiang Zhang,Shun Huang*

Main category: cs.SD

TL;DR: 提出用于对比学习的高频信息数据增强方法，在两个数据集上评估，效果良好。


<details>
  <summary>Details</summary>
Motivation: 解决无监督异常声音检测问题，让模型学习正常数据分布空间。

Method: 基于生物感知和数据分析，提出对比学习中高频信息的数据增强方法，使模型关注音频低频信息。

Result: 在DCASE 2020 Task 2上表现优于其他对比学习方法，在DCASE 2022 Task 2数据集上评估了泛化性。

Conclusion: 所提方法在异常声音检测中有较好效果和泛化性。

Abstract: The outlier exposure method is an effective approach to address the
unsupervised anomaly sound detection problem. The key focus of this method is
how to make the model learn the distribution space of normal data. Based on
biological perception and data analysis, it is found that anomalous audio and
noise often have higher frequencies. Therefore, we propose a data augmentation
method for high-frequency information in contrastive learning. This enables the
model to pay more attention to the low-frequency information of the audio,
which represents the normal operational mode of the machine. We evaluated the
proposed method on the DCASE 2020 Task 2. The results showed that our method
outperformed other contrastive learning methods used on this dataset. We also
evaluated the generalizability of our method on the DCASE 2022 Task 2 dataset.

</details>


### [264] [SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models](https://arxiv.org/abs/2509.15661)
*Qiaolin Wang,Xilin Jiang,Linyang He,Junkai Wu,Nima Mesgarani*

Main category: cs.SD

TL;DR: 提出SightSound - R1跨模态蒸馏框架，将视觉推理能力转移到音频模型，提升LALM推理性能。


<details>
  <summary>Details</summary>
Motivation: LALMs在复杂音景中的推理能力落后于LVLMs，缺乏大规模思维链音频数据。

Method: 提出SightSound - R1框架，包含测试时缩放、音频验证和蒸馏管道（SFT和GRPO）。

Result: SightSound - R1在AVQA测试集和未见场景及问题中提升了LALM推理性能，优于基线。

Conclusion: 视觉推理能有效转移到音频模型，并可通过丰富视听数据扩展。

Abstract: While large audio-language models (LALMs) have demonstrated state-of-the-art
audio understanding, their reasoning capability in complex soundscapes still
falls behind large vision-language models (LVLMs). Compared to the visual
domain, one bottleneck is the lack of large-scale chain-of-thought audio data
to teach LALM stepwise reasoning. To circumvent this data and modality gap, we
present SightSound-R1, a cross-modal distillation framework that transfers
advanced reasoning from a stronger LVLM teacher to a weaker LALM student on the
same audio-visual question answering (AVQA) dataset. SightSound-R1 consists of
three core steps: (i) test-time scaling to generate audio-focused chains of
thought (CoT) from an LVLM teacher, (ii) audio-grounded validation to filter
hallucinations, and (iii) a distillation pipeline with supervised fine-tuning
(SFT) followed by Group Relative Policy Optimization (GRPO) for the LALM
student. Results show that SightSound-R1 improves LALM reasoning performance
both in the in-domain AVQA test set as well as in unseen auditory scenes and
questions, outperforming both pretrained and label-only distilled baselines.
Thus, we conclude that vision reasoning can be effectively transferred to audio
models and scaled with abundant audio-visual data.

</details>


### [265] [TISDiSS: A Training-Time and Inference-Time Scalable Framework for Discriminative Source Separation](https://arxiv.org/abs/2509.15666)
*Yongsheng Feng,Yuetonghui Xu,Jiehui Luo,Hongjia Liu,Xiaobing Li,Feng Yu,Wei Li*

Main category: cs.SD

TL;DR: 提出TISDiSS框架用于源分离，可灵活权衡速度与性能，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有源分离方法提升性能依赖大网络，导致训练和部署成本增加，受生成建模推理时间缩放进展启发。

Method: 提出TISDiSS统一框架，集成早期分割多损失监督、共享参数设计和动态推理重复。

Result: 在标准语音分离基准测试中以减少的参数数量实现了最先进的性能。

Conclusion: TISDiSS是一个可扩展且实用的自适应源分离框架。

Abstract: Source separation is a fundamental task in speech, music, and audio
processing, and it also provides cleaner and larger data for training
generative models. However, improving separation performance in practice often
depends on increasingly large networks, inflating training and deployment
costs. Motivated by recent advances in inference-time scaling for generative
modeling, we propose Training-Time and Inference-Time Scalable Discriminative
Source Separation (TISDiSS), a unified framework that integrates early-split
multi-loss supervision, shared-parameter design, and dynamic inference
repetitions. TISDiSS enables flexible speed-performance trade-offs by adjusting
inference depth without retraining additional models. We further provide
systematic analyses of architectural and training choices and show that
training with more inference repetitions improves shallow-inference
performance, benefiting low-latency applications. Experiments on standard
speech separation benchmarks demonstrate state-of-the-art performance with a
reduced parameter count, establishing TISDiSS as a scalable and practical
framework for adaptive source separation.

</details>


### [266] [Compose Yourself: Average-Velocity Flow Matching for One-Step Speech Enhancement](https://arxiv.org/abs/2509.15952)
*Gang Yang,Yue Lei,Wenxin Tai,Jin Wu,Jia Chen,Ting Zhong,Fan Zhou*

Main category: cs.SD

TL;DR: 提出用于语音增强的一步式流匹配框架COSE，能高效计算平均速度，实验显示其采样快、降低训练成本且不影响语音质量。


<details>
  <summary>Details</summary>
Motivation: 扩散和流匹配模型在语音增强中依赖多步生成，计算成本高且易受离散化误差影响，MeanFlow虽有潜力但存在高训练开销问题。

Method: 提出速度合成恒等式，高效计算平均速度，消除昂贵计算同时保持理论一致性。

Result: 在标准基准测试中，COSE采样速度快达5倍，训练成本降低40%，且不影响语音质量。

Conclusion: COSE是一种高效的语音增强框架，能在提升计算效率的同时保证语音质量。

Abstract: Diffusion and flow matching (FM) models have achieved remarkable progress in
speech enhancement (SE), yet their dependence on multi-step generation is
computationally expensive and vulnerable to discretization errors. Recent
advances in one-step generative modeling, particularly MeanFlow, provide a
promising alternative by reformulating dynamics through average velocity
fields. In this work, we present COSE, a one-step FM framework tailored for SE.
To address the high training overhead of Jacobian-vector product (JVP)
computations in MeanFlow, we introduce a velocity composition identity to
compute average velocity efficiently, eliminating expensive computation while
preserving theoretical consistency and achieving competitive enhancement
quality. Extensive experiments on standard benchmarks show that COSE delivers
up to 5x faster sampling and reduces training cost by 40%, all without
compromising speech quality. Code is available at
https://github.com/ICDM-UESTC/COSE.

</details>


### [267] [Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data](https://arxiv.org/abs/2509.15389)
*Youngwon Choi,Jaeyoon Jung,Hyeonyu Kim,Huu-Kim Nguyen,Hwayeon Kim*

Main category: cs.SD

TL;DR: 研究在有限语音数据下不同微调方案对大音频语言模型（LALMs）口语理解（SLU）的影响，发现不同方案效果及应用策略。


<details>
  <summary>Details</summary>
Motivation: 大音频语言模型在微调尤其是有限语音数据微调方面研究不足，需探索不同微调方案对口语理解的影响。

Method: 系统研究文本仅、直接混合和课程学习等不同微调方案在文本 - 标签对丰富但语音 - 标签对有限场景下对口语理解的影响。

Result: 文本仅微调LALMs已有不错表现，添加少量语音数据有显著提升，课程学习在数据稀缺时效果好，跨语言SLU中结合源语言语音、目标语言文本和少量目标语言语音数据可有效适配。

Conclusion: 本研究为现实数据约束下的LALM微调提供了实用见解。

Abstract: Large Audio Language Models (LALMs) have emerged as powerful tools for
speech-related tasks but remain underexplored for fine-tuning, especially with
limited speech data. To bridge this gap, we systematically examine how
different fine-tuning schemes including text-only, direct mixing, and
curriculum learning affect spoken language understanding (SLU), focusing on
scenarios where text-label pairs are abundant while paired speech-label data
are limited. Results show that LALMs already achieve competitive performance
with text-only fine-tuning, highlighting their strong generalization ability.
Adding even small amounts of speech data (2-5%) yields substantial further
gains, with curriculum learning particularly effective under scarce data. In
cross-lingual SLU, combining source-language speech data with target-language
text and minimal target-language speech data enables effective adaptation.
Overall, this study provides practical insights into the LALM fine-tuning under
realistic data constraints.

</details>


### [268] [Fed-PISA: Federated Voice Cloning via Personalized Identity-Style Adaptation](https://arxiv.org/abs/2509.16010)
*Qi Wang,Shituo Ma,Guoxin Yu,Hanyang Peng,Yue Yu*

Main category: cs.SD

TL;DR: 提出Fed - PISA解决语音克隆联邦学习中高通信成本和个性化不足问题，实验显示其性能优且通信成本低。


<details>
  <summary>Details</summary>
Motivation: 现有语音克隆联邦学习方法存在高通信成本和抑制风格异质性导致个性化不足的问题。

Method: 引入解耦的低秩自适应（LoRA）机制，用私有ID - LoRA本地保留音色，仅传输轻量级风格 - LoRA到服务器；采用受协同过滤启发的聚合方法，从风格相似的对等方学习为每个客户端创建定制模型。

Result: Fed - PISA提高了风格表现力、自然度和说话人相似度，以最小的通信成本优于标准联邦基线。

Conclusion: Fed - PISA能有效解决现有语音克隆联邦学习方法的问题。

Abstract: Voice cloning for Text-to-Speech (TTS) aims to generate expressive and
personalized speech from text using limited data from a target speaker.
Federated Learning (FL) offers a collaborative and privacy-preserving framework
for this task, but existing approaches suffer from high communication costs and
tend to suppress stylistic heterogeneity, resulting in insufficient
personalization. To address these issues, we propose Fed-PISA, which stands for
Federated Personalized Identity-Style Adaptation. To minimize communication
costs, Fed-PISA introduces a disentangled Low-Rank Adaptation (LoRA) mechanism:
the speaker's timbre is retained locally through a private ID-LoRA, while only
a lightweight style-LoRA is transmitted to the server, thereby minimizing
parameter exchange. To harness heterogeneity, our aggregation method, inspired
by collaborative filtering, is introduced to create custom models for each
client by learning from stylistically similar peers. Experiments show that
Fed-PISA improves style expressivity, naturalness, and speaker similarity,
outperforming standard federated baselines with minimal communication costs.

</details>


### [269] [FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal Distillation](https://arxiv.org/abs/2509.16195)
*Luca Della Libera,Cem Subakan,Mirco Ravanelli*

Main category: cs.SD

TL;DR: 提出基于焦点调制的混合编解码器FocalCodec-Stream，能在低比特率和低延迟下压缩语音，性能优于现有可流式编解码器。


<details>
  <summary>Details</summary>
Motivation: 现有大多数神经音频编解码器不可流式传输，限制了其在实时应用中的使用。

Method: 结合WavLM的多阶段因果蒸馏和针对性架构改进，包括轻量级细化模块。

Result: FocalCodec-Stream在可比比特率下优于现有可流式编解码器，能保留语义和声学信息。

Conclusion: FocalCodec-Stream在重建质量、下游任务性能、延迟和效率之间取得了良好平衡。

Abstract: Neural audio codecs are a fundamental component of modern generative audio
pipelines. Although recent codecs achieve strong low-bitrate reconstruction and
provide powerful representations for downstream tasks, most are non-streamable,
limiting their use in real-time applications. We present FocalCodec-Stream, a
hybrid codec based on focal modulation that compresses speech into a single
binary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our
approach combines multi-stage causal distillation of WavLM with targeted
architectural improvements, including a lightweight refiner module that
enhances quality under latency constraints. Experiments show that
FocalCodec-Stream outperforms existing streamable codecs at comparable
bitrates, while preserving both semantic and acoustic information. The result
is a favorable trade-off between reconstruction quality, downstream task
performance, latency, and efficiency. Code and checkpoints will be released at
https://github.com/lucadellalib/focalcodec.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [270] [A Bouquet of Results on Maximum Range Sum: General Techniques and Hardness Reductions](https://arxiv.org/abs/2509.16008)
*Rachana Gusain,Saladi Rahul,Aditya Subramanian*

Main category: cs.CG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We revisit the maximum range sum (MaxRS) problem: given a set $P$ of $n$
weighted points in $\mathbb{R}^d$ and a range $Q$ (typically axis-aligned
$d$-box or $d$-ball), the goal is to place $Q$ to maximize the total weight of
points in $P\cap Q$. We study three natural variations:
  (1) Dynamic MaxRS: The goal is to update the placement of a $d$-ball under
point insertions and deletions. We give a randomized
$(\frac{1}{2}-\epsilon)$-approximation with update time $O_\epsilon(\log n)$.
The approximation factor holds with high probability. To the best of our
knowledge, this is the first result on dynamic MaxRS.
  (2) Batched MaxRS: In $\mathbb{R}^1$, along with $P$ we are given $m$
intervals of varying lengths. We prove a conditional lower bound of
$\Omega(mn)$ time (via conjectured $(\min,+)$-convolution hardness), showing
the trivial $O(mn\log n)$ upper bound in $\mathbb{R}^2$ is essentially tight.
We also establish a similar bound for a related problem of batched smallest
$k$-enclosing interval.
  (3) Colored MaxRS: Each point has a color from $[m]$, and the goal is to
place $Q$ to maximize the number of uniquely colored points in $P\cap Q$. Prior
work only considered axis-aligned rectangles in $\mathbb{R}^2$. For $d$-balls,
we give: (a) a randomized $(\frac{1}{2}-\epsilon)$-approximation in
$O_\epsilon(n\log n)$ time (avoiding exponential dependence on $d$), and (b) in
$\mathbb{R}^2$, a $(1-\epsilon)$-approximation in expected $O_\epsilon(n\log
n)$ time. Both approximations hold with high probability.
  Our algorithms rely on two techniques of broader interest. The first yields
$(\frac{1}{2}-\epsilon)$-approximations via a volume argument on $d$-balls and
a randomized game. The second achieves $(1-\epsilon)$-approximations through an
exact output-sensitive algorithm, which we speed up by random sampling on
colors.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [271] [Tree-independence number VI. Thetas and pyramids](https://arxiv.org/abs/2509.15458)
*Maria Chudnovsky,Julien Codsi*

Main category: math.CO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given a family $\mathcal{H}$ of graphs, we say that a graph $G$ is
$\mathcal{H}$-free if no induced subgraph of $G$ is isomorphic to a member of
$\mathcal{H}$. Let $W_{t\times t}$ be the $t$-by-$t$ hexagonal grid and let
$\mathcal{L}_t$ be the family of all graphs $G$ such that $G$ is the line graph
of some subdivision of $W_{t \times t}$. We denote by $\omega(G)$ the size of
the largest clique in $G$. We prove that for every integer $t$ there exist
integers $c_1(t)$, $c_2(t)$ and $d(t)$ such that every (pyramid, theta,
$\mathcal{L}_t$)-free graph $G$ satisfies: i) $G$ has a tree decomposition
where every bag has size at most $\omega(G)^{c_1(t)} \log (|V(G)|)$. ii) If $G$
has at least two vertices, then $G$ has a tree decomposition where every bag
has independence number at most $\log^{c_2(t)} (|V(G)|)$. iii) For any weight
function, $G$ has a balanced separator that is contained in the union of the
neighborhoods of at most $d(t)$ vertices. These results qualitatively
generalize the main theorems of Abrishami et al. (2022) and Chudnovsky et al.
(2024). Additionally, we show that there exist integers $c_3(t), c_4(t)$ such
that for every (theta, pyramid)-free graph $G$ and for every non-adjacent pair
of vertices $a,b \in V(G)$, i) $a$ can be separated from $b$ by removing at
most $w(G)^{c_3(t)}\log(|V(G)|)$ vertices. ii) $a$ can be separated from $b$ by
removing a set of vertices with independence number at most
$\log^{c_4(t)}(|V(G)|)$.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [272] [(SP)$^2$-Net: A Neural Spatial Spectrum Method for DOA Estimation](https://arxiv.org/abs/2509.15475)
*Lioz Berman,Sharon Gannot,Tom Tirer*

Main category: eess.SP

TL;DR: 本文提出深度学习技术，用新架构和训练策略从单快照生成高分辨率空间谱，所训练模型优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 经典Bartlett波束形成器精度和分辨率受阵列孔径限制，传统最大似然估计和基于样本协方差的谱方法在单快照场景不适用，需新方法解决单快照下多源波达方向估计问题。

Method: 提出深度学习技术，训练深度神经网络，以测量值和假设角度为输入，学习输出与更宽阵列能力一致的分数，推理时扫描任意角度集生成热图。

Result: 所训练的(SP)$^2$-Net模型展现出优于Bartlett波束形成器和基于稀疏性的波达方向估计方法的优势。

Conclusion: 提出的深度学习技术能有效解决单快照下多源波达方向估计问题，生成高分辨率空间谱，模型性能良好。

Abstract: We consider the problem of estimating the directions of arrival (DOAs) of
multiple sources from a single snapshot of an antenna array, a task with many
practical applications. In such settings, the classical Bartlett beamformer is
commonly used, as maximum likelihood estimation becomes impractical when the
number of sources is unknown or large, and spectral methods based on the sample
covariance are not applicable due to the lack of multiple snapshots. However,
the accuracy and resolution of the Bartlett beamformer are fundamentally
limited by the array aperture. In this paper, we propose a deep learning
technique, comprising a novel architecture and training strategy, for
generating a high-resolution spatial spectrum from a single snapshot.
Specifically, we train a deep neural network that takes the measurements and a
hypothesis angle as input and learns to output a score consistent with the
capabilities of a much wider array. At inference time, a heatmap can be
produced by scanning an arbitrary set of angles. We demonstrate the advantages
of our trained model, named (SP)$^2$-Net, over the Bartlett beamformer and
sparsity-based DOA estimation methods.

</details>


### [273] [MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework](https://arxiv.org/abs/2509.15964)
*Tianyu Li,Yan Xin,Jianzhong,Zhang*

Main category: eess.SP

TL;DR: 提出MoE - CE框架提升基于深度学习的信道估计方法泛化能力，实验表明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于深度学习的信道估计方法在不同场景下泛化能力不足，尤其是多任务和零样本场景。

Method: 提出MoE - CE框架，利用多个专家子网络和学习的路由器，动态选择相关专家，增强模型容量和适应性。

Result: 在不同信噪比、资源块数量和信道配置的合成数据集上实验，MoE - CE始终优于传统深度学习方法，实现显著性能提升并保持效率。

Conclusion: MoE - CE框架能有效提升基于深度学习的信道估计方法的泛化能力。

Abstract: Reliable channel estimation (CE) is fundamental for robust communication in
dynamic wireless environments, where models must generalize across varying
conditions such as signal-to-noise ratios (SNRs), the number of resource blocks
(RBs), and channel profiles. Traditional deep learning (DL)-based methods
struggle to generalize effectively across such diverse settings, particularly
under multitask and zero-shot scenarios. In this work, we propose MoE-CE, a
flexible mixture-of-experts (MoE) framework designed to enhance the
generalization capability of DL-based CE methods. MoE-CE provides an
appropriate inductive bias by leveraging multiple expert subnetworks, each
specialized in distinct channel characteristics, and a learned router that
dynamically selects the most relevant experts per input. This architecture
enhances model capacity and adaptability without a proportional rise in
computational cost while being agnostic to the choice of the backbone model and
the learning algorithm. Through extensive experiments on synthetic datasets
generated under diverse SNRs, RB numbers, and channel profiles, including
multitask and zero-shot evaluations, we demonstrate that MoE-CE consistently
outperforms conventional DL approaches, achieving significant performance gains
while maintaining efficiency.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [274] [How Exclusive are Ethereum Transactions? Evidence from non-winning blocks](https://arxiv.org/abs/2509.16052)
*Vabuk Pahari,Andrea Canidio*

Main category: cs.CR

TL;DR: 分析2024年12月3日8分钟内以太坊区块链的15,097个候选区块，发现独家交易是构建者收入主要来源。


<details>
  <summary>Details</summary>
Motivation: 研究以太坊区块链中交易类型及构建者收入来源。

Method: 分析特定时间段内候选区块，对交易分类为独家和私有交易。

Result: 独家交易占获胜区块交易总费用的84%，排除部分情况后至少占77.2%，仅7%独家交易来自与单一构建者有排他关系的发送者。

Conclusion: 独家交易是构建者收入的主要来源。

Abstract: We analyze 15,097 blocks proposed for inclusion in Ethereum's blockchain over
an 8-minute window on December 3, 2024, during which 38 blocks were added to
the chain. We classify transactions as exclusive -- present only in blocks from
a single builder -- or private -- absent from the public mempool but included
in blocks from multiple builders. We find that exclusive transactions account
for 84% of the total fees paid by transactions in winning blocks. Furthermore,
we show that exclusivity cannot be fully explained by exclusive relationships
between senders and builders: about 7% of all exclusive transactions included
on-chain, by value, come from senders who route exclusively to a single
builder. Analyzing transaction logs shows that some exclusive transactions are
duplicates or variations of the same strategy, but even accounting for that,
the share of the total fees paid by transactions in winning blocks is at least
77.2%. Taken together, our findings highlight that exclusive transactions are
the dominant source of builder revenues.

</details>


### [275] [Hornet Node and the Hornet DSL: A Minimal, Executable Specification for Bitcoin Consensus](https://arxiv.org/abs/2509.15754)
*Toby Sharp*

Main category: cs.CR

TL;DR: 本文提出比特币共识规则的C++规范及Hornet DSL，其Hornet Node客户端有优势，为比特币共识形式化规范提供路径。


<details>
  <summary>Details</summary>
Motivation: 比特币参考客户端代码不适合形式验证，需独立的形式化规范降低共识分裂风险，但此前认为难以实现。

Method: 提出紧凑、可执行、声明式的C++规范，引入Hornet DSL编码规则，开发Hornet Node客户端。

Result: C++规范能在单线程几小时内同步主网，Hornet Node适合教育和实验。

Conclusion: Hornet Node和Hornet DSL为比特币共识提供首个可靠的纯形式化可执行规范路径。

Abstract: Bitcoin's consensus rules are encoded in the implementation of its reference
client: "The code is the spec." Yet this code is unsuitable for formal
verification due to side effects, mutable state, concurrency, and legacy
design. A standalone formal specification would enable verification both across
versions of the reference client and against new client implementations,
strengthening decentralization by reducing the risk of consensus-splitting
bugs. Yet such a specification has long been considered intractable given the
complexity of Bitcoin's consensus logic. We demonstrate a compact, executable,
declarative C++ specification of Bitcoin consensus rules that syncs mainnet to
tip in a few hours on a single thread. We also introduce the Hornet
Domain-Specific Language (DSL) specifically designed to encode these rules
unambiguously for execution, enabling formal reasoning, consensus code
generation, and AI-driven adversarial testing. Our spec-driven client Hornet
Node offers a modern and modular complement to the reference client. Its clear,
idiomatic style makes it suitable for education, while its performance makes it
ideal for experimentation. We highlight architectural contributions such as its
layered design, efficient data structures, and strong separation of concerns,
supported by production-quality code examples. We argue that Hornet Node and
Hornet DSL together provide the first credible path toward a pure, formal,
executable specification of Bitcoin consensus.

</details>


### [276] [An Adversarial Robust Behavior Sequence Anomaly Detection Approach Based on Critical Behavior Unit Learning](https://arxiv.org/abs/2509.15756)
*Dongyang Zhan,Kai Tan,Lin Ye,Xiangzhan Yu,Hongli Zhang,Zheng He*

Main category: cs.CR

TL;DR: 提出基于行为单元分析的对抗鲁棒性异常检测方法，能缓解针对局部和大规模行为的扰动攻击，实验显示性能优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的软件行为序列特征学习方法易受对抗样本攻击，攻击者可借此误导恶意软件分类器。

Method: 提取具有行为意图的相关行为作为行为单元，基于多级深度学习模型学习其整体语义和上下文关系。

Result: 所提方法实验结果优于所有对比方法。

Conclusion: 所提方法在对抗混淆攻击方面有更好性能。

Abstract: Sequential deep learning models (e.g., RNN and LSTM) can learn the sequence
features of software behaviors, such as API or syscall sequences. However,
recent studies have shown that these deep learning-based approaches are
vulnerable to adversarial samples. Attackers can use adversarial samples to
change the sequential characteristics of behavior sequences and mislead malware
classifiers. In this paper, an adversarial robustness anomaly detection method
based on the analysis of behavior units is proposed to overcome this problem.
We extract related behaviors that usually perform a behavior intention as a
behavior unit, which contains the representative semantic information of local
behaviors and can be used to improve the robustness of behavior analysis. By
learning the overall semantics of each behavior unit and the contextual
relationships among behavior units based on a multilevel deep learning model,
our approach can mitigate perturbation attacks that target local and
large-scale behaviors. In addition, our approach can be applied to both
low-level and high-level behavior logs (e.g., API and syscall logs). The
experimental results show that our approach outperforms all the compared
methods, which indicates that our approach has better performance against
obfuscation attacks.

</details>


### [277] [Hybrid Deep Learning-Federated Learning Powered Intrusion Detection System for IoT/5G Advanced Edge Computing Network](https://arxiv.org/abs/2509.15555)
*Rasil Baidar,Sasa Maric,Robert Abbas*

Main category: cs.CR

TL;DR: 提出融合CNN、BiLSTM和AE的入侵检测系统，在联邦学习框架下训练，在UNSW - NB15数据集表现好，适合边缘部署并讨论相关安全问题。


<details>
  <summary>Details</summary>
Motivation: 物联网和5G - Advanced应用的指数级扩展增大了DDoS、恶意软件和零日入侵的攻击面，需要有效入侵检测系统。

Method: 在隐私保护联邦学习框架下融合CNN、BiLSTM和AE，CNN - BiLSTM捕捉局部和交叉特征交互，AE强调基于重建的异常敏感性，在边缘设备训练且不共享原始数据。

Result: 在UNSW - NB15（二进制）上，融合模型AUC达99.59%，F1达97.36%，混淆矩阵分析显示误差率平衡，精度和召回率高，平均推理时间约0.0476ms/样本。

Conclusion: 该模型适合边缘部署，还讨论了可解释性、漂移容忍度和联邦学习等方面以实现合规、可扩展的5G - Advanced物联网安全。

Abstract: The exponential expansion of IoT and 5G-Advanced applications has enlarged
the attack surface for DDoS, malware, and zero-day intrusions. We propose an
intrusion detection system that fuses a convolutional neural network (CNN), a
bidirectional LSTM (BiLSTM), and an autoencoder (AE) bottleneck within a
privacy-preserving federated learning (FL) framework. The CNN-BiLSTM branch
captures local and gated cross-feature interactions, while the AE emphasizes
reconstruction-based anomaly sensitivity. Training occurs across edge devices
without sharing raw data. On UNSW-NB15 (binary), the fused model attains AUC
99.59 percent and F1 97.36 percent; confusion-matrix analysis shows balanced
error rates with high precision and recall. Average inference time is
approximately 0.0476 ms per sample on our test hardware, which is well within
the less than 10 ms URLLC budget, supporting edge deployment. We also discuss
explainability, drift tolerance, and FL considerations for compliant, scalable
5G-Advanced IoT security.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [278] [A Flow-rate-conserving CNN-based Domain Decomposition Method for Blood Flow Simulations](https://arxiv.org/abs/2509.15900)
*Simon Klaes,Axel Klawonn,Natalie Kubicki,Martin Lanser,Kengo Nakajima,Takashi Shimokawabe,Janine Weber*

Main category: math.NA

TL;DR: 本文用CNN替代模型预测狭窄动脉中非牛顿粘性血流，提出交替Schwarz区域分解方法，训练通用子域求解器，评估不同条件结果，发现物理感知方法更优。


<details>
  <summary>Details</summary>
Motivation: 使用卷积神经网络替代模型预测狭窄动脉中非牛顿粘性的血流情况。

Method: 提出交替Schwarz区域分解方法，使用基于CNN的子域求解器，训练通用子域求解器（USDS）并应用于Schwarz方法中的每个子域求解。

Result: 给出不同形状、长度的二维狭窄动脉在不同流入条件下的结果并进行统计评估。

Conclusion: 在训练数据有限时，需实现保留部分物理特性的USDS，物理感知方法优于纯数据驱动的USDS，能带来更好的子域解，防止全局解过冲或下冲，使收敛更可靠。

Abstract: This work aims to predict blood flow with non-Newtonian viscosity in stenosed
arteries using convolutional neural network (CNN) surrogate models. An
alternating Schwarz domain decomposition method is proposed which uses
CNN-based subdomain solvers. A universal subdomain solver (USDS) is trained on
a single, fixed geometry and then applied for each subdomain solve in the
Schwarz method. Results for two-dimensional stenotic arteries of varying shape
and length for different inflow conditions are presented and statistically
evaluated. One key finding, when using a limited amount of training data, is
the need to implement a USDS which preserves some of the physics, as, in our
case, flow rate conservation. A physics-aware approach outperforms purely
data-driven USDS, delivering improved subdomain solutions and preventing
overshooting or undershooting of the global solution during the Schwarz
iterations, thereby leading to more reliable convergence.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [279] [Generating Plans for Belief-Desire-Intention (BDI) Agents Using Alternating-Time Temporal Logic (ATL)](https://arxiv.org/abs/2509.15238)
*Dylan Léveillé*

Main category: cs.MA

TL;DR: 本文开发了一个使用交替时间时态逻辑自动生成BDI计划的工具，并通过示例游戏验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有计划生成方法需大量手动工作且主要针对单智能体系统，因此开发新工具。

Method: 使用交替时间时态逻辑（ATL）自动生成BDI计划。

Result: 为需要智能体协作的示例游戏生成计划，智能体成功实现目标。

Conclusion: 开发的工具能有效生成考虑智能体竞争与合作的BDI计划。

Abstract: Belief-Desire-Intention (BDI) is a framework for modelling agents based on
their beliefs, desires, and intentions. Plans are a central component of BDI
agents, and define sequences of actions that an agent must undertake to achieve
a certain goal. Existing approaches to plan generation often require
significant manual effort, and are mainly focused on single-agent systems. As a
result, in this work, we have developed a tool that automatically generates BDI
plans using Alternating-Time Temporal Logic (ATL). By using ATL, the plans
generated accommodate for possible competition or cooperation between the
agents in the system. We demonstrate the effectiveness of the tool by
generating plans for an illustrative game that requires agent collaboration to
achieve a shared goal. We show that the generated plans allow the agents to
successfully attain this goal.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [280] [Kernel Model Validation: How To Do It, And Why You Should Care](https://arxiv.org/abs/2509.15244)
*Carlo Graziani,Marieme Ngom*

Main category: stat.ME

TL;DR: 本文强调高斯过程（GP）模型预测概率校准的重要性，介绍协方差核验证程序，并讨论不同维度模型情况。


<details>
  <summary>Details</summary>
Motivation: GP模型不确定性估计缺乏精确概率解释和校准，其预测校准失败会影响目标优化算法收敛性，因此需进行概率校准。

Method: 通过协方差核验证的形式化程序，利用GP预测的多元正态性质来解释和验证GP生成的不确定区间。

Result: 给出了GP回归一维错误指定模型的简单示例，并讨论了高维模型情况。

Conclusion: 强调了对GP预测进行适当概率校准的重要性，并提出了验证方法。

Abstract: Gaussian Process (GP) models are popular tools in uncertainty quantification
(UQ) because they purport to furnish functional uncertainty estimates that can
be used to represent model uncertainty. It is often difficult to state with
precision what probabilistic interpretation attaches to such an uncertainty,
and in what way is it calibrated. Without such a calibration statement, the
value of such uncertainty estimates is quite limited and qualitative. We
motivate the importance of proper probabilistic calibration of GP predictions
by describing how GP predictive calibration failures can cause degraded
convergence properties in a target optimization algorithm called Targeted
Adaptive Design (TAD). We discuss the interpretation of GP-generated
uncertainty intervals in UQ, and how one may learn to trust them, through a
formal procedure for covariance kernel validation that exploits the
multivariate normal nature of GP predictions. We give simple examples of GP
regression misspecified 1-dimensional models, and discuss the situation with
respect to higher-dimensional models.

</details>


### [281] [Beyond the Average: Distributional Causal Inference under Imperfect Compliance](https://arxiv.org/abs/2509.15594)
*Undral Byambadalai,Tomu Hirata,Tatsushi Oka,Shota Yasui*

Main category: stat.ME

TL;DR: 本文研究不完美依从的随机实验中分布处理效应的估计，提出回归调整估计量，推导其渐近分布，模拟和实际应用验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 在随机实验中参与者不依从指定处理时，识别局部分布处理效应。

Method: 基于具有Neyman正交矩条件的分布回归框架提出回归调整估计量，适用于多种类型结果和协变量自适应随机化方案。

Result: 推导了估计量的渐近分布，达到半参数效率界，模拟显示良好有限样本性能，实际应用证明方法的实用性。

Conclusion: 所提方法能有效处理不完美依从随机实验中的分布处理效应估计问题。

Abstract: We study the estimation of distributional treatment effects in randomized
experiments with imperfect compliance. When participants do not adhere to their
assigned treatments, we leverage treatment assignment as an instrumental
variable to identify the local distributional treatment effect-the difference
in outcome distributions between treatment and control groups for the
subpopulation of compliers. We propose a regression-adjusted estimator based on
a distribution regression framework with Neyman-orthogonal moment conditions,
enabling robustness and flexibility with high-dimensional covariates. Our
approach accommodates continuous, discrete, and mixed discrete-continuous
outcomes, and applies under a broad class of covariate-adaptive randomization
schemes, including stratified block designs and simple random sampling. We
derive the estimator's asymptotic distribution and show that it achieves the
semiparametric efficiency bound. Simulation results demonstrate favorable
finite-sample performance, and we demonstrate the method's practical relevance
in an application to the Oregon Health Insurance Experiment.

</details>


### [282] [Transfer learning under latent space model](https://arxiv.org/abs/2509.15797)
*Kuangnan Fang,Ruixuan Qin,Xinyan Fan*

Main category: stat.ME

TL;DR: 提出转移学习方法提升目标网络潜变量估计精度，有两阶段算法及检测算法，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 潜空间模型在网络分析中重要，但大量待估参数是挑战，需提升目标网络潜变量估计精度。

Method: 提出转移学习方法，针对可转移源网络有两阶段转移学习算法，未知时引入检测算法，各阶段推导识别条件并设计投影梯度下降算法。

Result: 建立了估计量的理论性质，仿真研究和两个真实数据集分析验证了方法有效性。

Conclusion: 所提转移学习方法能有效提升目标网络潜变量的估计精度。

Abstract: Latent space model plays a crucial role in network analysis, and accurate
estimation of latent variables is essential for downstream tasks such as link
prediction. However, the large number of parameters to be estimated presents a
challenge, especially when the latent space dimension is not exceptionally
small. In this paper, we propose a transfer learning method that leverages
information from networks with latent variables similar to those in the target
network, thereby improving the estimation accuracy for the target. Given
transferable source networks, we introduce a two-stage transfer learning
algorithm that accommodates differences in node numbers between source and
target networks. In each stage, we derive sufficient identification conditions
and design tailored projected gradient descent algorithms for estimation.
Theoretical properties of the resulting estimators are established. When the
transferable networks are unknown, a detection algorithm is introduced to
identify suitable source networks. Simulation studies and analyses of two real
datasets demonstrate the effectiveness of the proposed methods.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [283] [Demand, and consumer surplus in the payday-loan market: Evidence from British Columbia](https://arxiv.org/abs/2509.15247)
*Tim Zhang,Amity Quinn*

Main category: econ.GN

TL;DR: 研究利用不列颠哥伦比亚省数据，发现降低利率上限显著增加发薪日贷款需求，能带来消费者福利增益。


<details>
  <summary>Details</summary>
Motivation: 探究利率上限对发薪日贷款需求的影响。

Method: 使用普通最小二乘法（OLS）估计线性需求函数。

Result: 利率上限从每100美元23美元降至15美元，每年增加约2860万加元消费者剩余；2025年1月降至14美元，每年再增加390万加元。

Conclusion: 更严格的利率上限可带来可观的消费者福利增益。

Abstract: This study examines how interest rate caps affect the demand for payday
loans, using aggregate data from British Columbia (2012--2019) during which the
province's maximum fee was reduced from \$23 to \$17 and then to \$15 per \$100
borrowed. Estimating a linear demand function via OLS, we find that lowering
interest rate caps significantly increases loan demand. We estimate that the
\$8 decrease, from \$23 to \$15 per \$100, raised annual consumer surplus by
roughly \$28.6 million (2012 CAD). A further reduction to \$14, starting in
January 2025, would add another \$3.9 million per year. These results suggest
that stricter interest rate caps can yield substantial consumer welfare gains.

</details>


### [284] [AI and jobs. A review of theory, estimates, and evidence](https://arxiv.org/abs/2509.15265)
*R. Maria del Rio-Chanona,Ekkehard Ernst,Rossana Merola,Daniel Samaan,Ole Teutloff*

Main category: econ.GN

TL;DR: 本文对生成式AI对就业和宏观经济影响的理论和实证证据进行综合分析，指出生产力提升幅度及不同技能工人受益情况，也提及研究存在的不足。


<details>
  <summary>Details</summary>
Motivation: 生成式AI改变工作流程等，但对就业和宏观经济的影响未明确，因此进行综合分析。

Method: 从三个层面综合理论和实证证据，包括追溯模型演变、定量比较职业的AI暴露指标、整合AI对就业影响的事后证据。

Result: 生产力提升幅度因场景而异，新手工人在简单任务中更受益，复杂任务中高低技能工人受益情况不一，存在人机替代现象及对新手工人需求下降。

Conclusion: 当前研究存在聚焦简单任务、研究的大语言模型多样性有限、AI暴露指标以技术为中心等不足。

Abstract: Generative AI is altering work processes, task composition, and
organizational design, yet its effects on employment and the macroeconomy
remain unresolved. In this review, we synthesize theory and empirical evidence
at three levels. First, we trace the evolution from aggregate production
frameworks to task- and expertise-based models. Second, we quantitatively
review and compare (ex-ante) AI exposure measures of occupations from multiple
studies and find convergence towards high-wage jobs. Third, we assemble ex-post
evidence of AI's impact on employment from randomized controlled trials (RCTs),
field experiments, and digital trace data (e.g., online labor platforms,
software repositories), complemented by partial coverage of surveys. Across the
reviewed studies, productivity gains are sizable but context-dependent: on the
order of 20 to 60 percent in controlled RCTs, and 15 to 30 percent in field
experiments. Novice workers tend to benefit more from LLMs in simple tasks.
Across complex tasks, evidence is mixed on whether low or high-skilled workers
benefit more. Digital trace data show substitution between humans and machines
in writing and translation alongside rising demand for AI, with mild evidence
of declining demand for novice workers. A more substantial decrease in demand
for novice jobs across AI complementary work emerges from recent studies using
surveys, platform payment records, or administrative data. Research gaps
include the focus on simple tasks in experiments, the limited diversity of LLMs
studied, and technology-centric AI exposure measures that overlook adoption
dynamics and whether exposure translates into substitution, productivity gains,
erode or increase expertise.

</details>


### [285] [A moderate share of V2G outperforms large-scale smart charging of electric vehicles and benefits other consumers](https://arxiv.org/abs/2509.15284)
*Adeline Guéret,Carlos Gaete-Morales,Wolf-Peter Schill*

Main category: econ.GN

TL;DR: 分析德国1500万辆电动汽车不同充电策略，发现适度比例双向充电成本更低，政策应关注适度双向充电比例。


<details>
  <summary>Details</summary>
Motivation: 研究电动汽车充电策略对电力部门的影响，以实现交通部门脱碳。

Method: 使用容量扩展模型，系统分析德国1500万辆电动汽车在非灵活、智能和双向充电的不同组合情况。

Result: 双向充电比例低于30%时系统成本低于全智能充电；V2G占比50%时成本低于无电动汽车系统；V2G车主内化部分成本节省，且随着双向充电比例增加，其他消费者也受益。

Conclusion: 政策制定者应关注使适度比例车辆实现双向充电，而非让每辆车都智能充电。

Abstract: While battery electric vehicles (BEVs) play a key role for decarbonizing the
transport sector, their impact on the power sector heavily depends on their
charging strategies. Here we systematically analyze various combinations
between inflexible, smart and bidirectional (or vehicle-to-grid, V2G) charging
of 15 million electric cars in Germany. Using a capacity expansion model, we
find that even a moderate share of bidirectional charging below 30% leads to
lower system costs than a fully smartly charging BEV fleet. At a V2G share of
50%, costs are even lower than in a system without any BEVs. This means that
the flexibility effect of half of the BEV fleet charging bidirectionally
outweighs the demand effect of the whole BEV fleet. We show how costs savings
are driven by the ability of V2G to serve demand, especially during hours with
high residual load. We also explore the distributional effects of respective
electricity price changes. While V2G car owners internalize a substantial share
of overall cost savings, the benefits increasingly spill over to other
electricity consumers as the share of bidirectional charging grows. We conclude
that policymakers should focus on enabling a moderate fleet share of V2G rather
than on enabling every car to charge smartly.

</details>


### [286] [Poverty and Perceptions of Electoral Integrity in the U.S](https://arxiv.org/abs/2509.15343)
*Douglas Cumming,Sofia Johan,Ikenna Uzuegbunam*

Main category: econ.GN

TL;DR: 本文提出影响选举诚信与贫困关系的两种相反力量，预测选举诚信是贫困的U型函数，还假设其因州选举法强度而异，数据验证了预测。


<details>
  <summary>Details</summary>
Motivation: 探究选举诚信与贫困之间的关系。

Method: 结合2016年美国总统选举和2018年美国国会选举的选举诚信专家调查数据以及美国州层面的贫困数据进行研究。

Result: 数据与选举诚信是贫困的U型函数以及选举诚信因州选举法强度而异的预测高度一致。

Conclusion: 选举诚信与贫困存在U型关系，且受州选举法强度影响。

Abstract: We propose two opposing forces that impact the relation between electoral
integrity and poverty. On the one hand, it is more costly to provide electoral
integrity in states where there is more poverty due to transaction costs and
opportunity costs. On the other hand, extreme levels of poverty attract media
scrutiny and greater external monitoring of electoral integrity, giving rise to
more demand for electoral integrity. Taken together, we expect electoral
integrity to be a U-shaped function of poverty. We also hypothesize that
electoral integrity will vary depending on the strength of state electoral
laws. Expert-level survey data on electoral integrity from the 2016 U.S.
Presidential election and the 2018 U.S. congressional election, in combination
with U.S. state-level data on poverty are strongly consistent with these
predictions.

</details>


### [287] [The (Short-Term) Effects of Large Language Models on Unemployment and Earnings](https://arxiv.org/abs/2509.15510)
*Danqing Chen,Carina Kane,Austin Kozlowski,Nadav Kunievsky,James A. Evans*

Main category: econ.GN

TL;DR: 本文通过合成双重差分法研究大语言模型采用对劳动力市场的短期影响，发现高暴露职业工人收入增加，失业率不变，劳动力市场调整主要通过收入而非人员重新分配。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型采用对劳动力市场的短期影响，回应其带来生产力提升和工作岗位替代的争议。

Method: 使用合成双重差分法，比较不同大语言模型暴露程度职业的收入和失业率。

Result: 高暴露职业工人在ChatGPT推出后收入增加，失业率不变。

Conclusion: 劳动力市场对大语言模型的初始调整主要通过收入而非人员重新分配。

Abstract: Large Language Models have spread rapidly since the release of ChatGPT in
late 2022, accompanied by claims of major productivity gains but also concerns
about job displacement. This paper examines the short-run labor market effects
of LLM adoption by comparing earnings and unemployment across occupations with
differing levels of exposure to these technologies. Using a Synthetic
Difference in Differences approach, we estimate the impact of LLM exposure on
earnings and unemployment. Our findings show that workers in highly exposed
occupations experienced earnings increases following ChatGPT's introduction,
while unemployment rates remained unchanged. These results suggest that initial
labor market adjustments to LLMs operate primarily through earnings rather than
worker reallocation.

</details>


### [288] [The Impact of AI Adoption on Retail Across Countries and Industries](https://arxiv.org/abs/2509.15885)
*Yunqi Liu*

Main category: econ.GN

TL;DR: 研究利用2020 - 2025年数据集，用三阶段OLS框架分析AI采用对失业率影响，发现零售行业AI采用与失业率负相关。


<details>
  <summary>Details</summary>
Motivation: 研究人工智能采用对失业率的影响。

Method: 采用三阶段普通最小二乘法（OLS）框架，包括全样本回归、行业特定回归和交互项模型。

Result: 全样本回归显示AI采用率与失业率无显著线性关联；行业特定回归表明营销和零售行业接近显著；交互项模型显示零售行业有显著交互效应，AI采用越高，失业率越低。

Conclusion: 研究扩展了AI对劳动力市场影响的实证证据，强调AI在零售行业提高生产力的作用，支持针对性政策措施。

Abstract: This study investigates the impact of artificial intelligence (AI) adoption
on job loss rates using the Global AI Content Impact Dataset (2020--2025). The
panel comprises 200 industry-country-year observations across Australia, China,
France, Japan, and the United Kingdom in ten industries. A three-stage ordinary
least squares (OLS) framework is applied. First, a full-sample regression finds
no significant linear association between AI adoption rate and job loss rate
($\beta \approx -0.0026$, $p = 0.949$). Second, industry-specific regressions
identify the marketing and retail sectors as closest to significance. Third,
interaction-term models quantify marginal effects in those two sectors,
revealing a significant retail interaction effect ($-0.138$, $p < 0.05$),
showing that higher AI adoption is linked to lower job loss in retail. These
findings extend empirical evidence on AI's labor market impact, emphasize AI's
productivity-enhancing role in retail, and support targeted policy measures
such as intelligent replenishment systems and cashierless checkout
implementations.

</details>
