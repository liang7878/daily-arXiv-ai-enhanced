<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 86]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.SE](#cs.SE) [Total: 6]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [eess.AS](#eess.AS) [Total: 2]
- [eess.IV](#eess.IV) [Total: 4]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 8]
- [math-ph](#math-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.ET](#cs.ET) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.SD](#cs.SD) [Total: 3]
- [econ.TH](#econ.TH) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.CL](#cs.CL) [Total: 18]
- [cs.CV](#cs.CV) [Total: 26]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.CG](#cs.CG) [Total: 2]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.IT](#cs.IT) [Total: 1]
- [econ.GN](#econ.GN) [Total: 4]
- [math.NA](#math.NA) [Total: 3]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 5]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Rethinking the Illusion of Thinking](https://arxiv.org/abs/2507.01231)
*Iñaki Dellibarda Varela,Pablo Romero-Sorozabal,Eduardo Rocon,Manuel Cebrian*

Main category: cs.AI

TL;DR: 本文围绕苹果研究引发的关于大推理模型推理能力的争议，复现并改进相关基准测试，揭示模型在不同任务中的表现，反对简单结论，强调符号、长程推理需精细消融研究。


<details>
  <summary>Details</summary>
Motivation: 澄清苹果研究引发的关于大推理模型是否具备真正推理能力的争议。

Method: 复现并改进原研究中最具争议的汉诺塔和过河问题两个基准测试，引入增量逐步提示和代理协作对话。

Result: 汉诺塔问题中，模型失败不仅因输出限制，也有认知局限；过河问题中，测试可解问题时模型能轻松解决大实例。

Conclusion: 当前大推理模型是离散状态空间中的随机、强化学习调优搜索器，符号、长程推理需要进行精细消融研究。

Abstract: Earlier this year, Apple ignited controversy by publishing "The Illusion of
Thinking," prompting heated debate within the AI community. Critics seized upon
the findings as conclusive evidence that Large Reasoning Models (LRMs) lack
genuine reasoning capabilities, branding them as mere stochastic parrots.
Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning
the experimental setup as flawed and the conclusions overstated. We clarify
this debate by replicating and refining two of the original study's most
contentious benchmarks: Towers of Hanoi and River Crossing. By introducing
incremental stepwise prompting and agentic collaborative dialogue, we show that
previously reported failures solving the Towers of Hanoi were not purely result
of output constraints, but also partly a result of cognition limitations: LRMs
still stumble when complexity rises moderately (around 8 disks). Moreover, the
River Crossing results initially heralded as catastrophic failures turn out to
hinge upon testing unsolvable configurations. Once we limit tests strictly to
solvable problems-LRMs effortlessly solve large instances involving over 100
agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs
are stochastic, RL-tuned searchers in a discrete state space we barely
understand. Real progress in symbolic, long-horizon reasoning demands mapping
that terrain through fine-grained ablations like those introduced here.

</details>


### [2] [Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care](https://arxiv.org/abs/2507.01282)
*Matthew JY Kang,Wenli Yang,Monica R Roberts,Byeong Ho Kang,Charles B Malpas*

Main category: cs.AI

TL;DR: 文章探讨大语言模型辅助医疗诊断，指出AI在痴呆诊断和护理的局限，提出混合方法及未来决策支持方向和研究重点。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽有高基准分数，但在床边诊断未带来显著改善，需明确AI在临床应用局限。

Method: 进行范围审查，分析独立机器学习模型和大语言模型使用情况，研究数据驱动范式的局限性，举例混合方法。

Result: 独立机器学习模型难提供可操作、可解释指导，大语言模型未提升诊断准确性和速度，混合方法可恢复可解释性。

Conclusion: 未来决策支持应优先考虑解释一致性，采用神经符号或混合AI，研究需衡量多方面指标，提高人机交互理解。

Abstract: The recent boom of large language models (LLMs) has re-ignited the hope that
artificial intelligence (AI) systems could aid medical diagnosis. Yet despite
dazzling benchmark scores, LLM assistants have yet to deliver measurable
improvements at the bedside. This scoping review aims to highlight the areas
where AI is limited to make practical contributions in the clinical setting,
specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom
provide actionable, interpretable guidance, eroding clinician trust. Adjacent
use of LLMs by physicians did not result in better diagnostic accuracy or
speed. Key limitations trace to the data-driven paradigm: black-box outputs
which lack transparency, vulnerability to hallucinations, and weak causal
reasoning. Hybrid approaches that combine statistical learning with expert
rule-based knowledge, and involve clinicians throughout the process help bring
back interpretability. They also fit better with existing clinical workflows,
as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking
predictions to clinically meaningful causes. This can be done through
neuro-symbolic or hybrid AI that combines the language ability of LLMs with
human causal expertise. AI researchers have addressed this direction, with
explainable AI and neuro-symbolic AI being the next logical steps in further
advancement in AI. However, they are still based on data-driven knowledge
integration instead of human-in-the-loop approaches. Future research should
measure success not only by accuracy but by improvements in clinician
understanding, workflow fit, and patient outcomes. A better understanding of
what helps improve human-computer interactions is greatly needed for AI systems
to become part of clinical practice.

</details>


### [3] [AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing](https://arxiv.org/abs/2507.01376)
*Yinwang Ren,Yangyang Liu,Tang Ji,Xun Xu*

Main category: cs.AI

TL;DR: 本文探讨生成式AI推动下的各类AI代理在智能制造中的应用，系统回顾技术并分析其潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 新兴AI范式在智能制造中的定义、能力边界和实际应用不清晰，需深入研究。

Method: 系统回顾AI和AI代理技术的演变，研究核心概念与技术进步。

Result: 未提及具体研究结果。

Conclusion: 未提及具体结论。

Abstract: AI agents are autonomous systems designed to perceive, reason, and act within
dynamic environments. With the rapid advancements in generative AI (GenAI),
large language models (LLMs) and multimodal large language models (MLLMs) have
significantly improved AI agents' capabilities in semantic comprehension,
complex reasoning, and autonomous decision-making. At the same time, the rise
of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and
complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents
(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in
information processing, environmental perception, and autonomous
decision-making, opening new avenues for smart manufacturing. However, the
definitions, capability boundaries, and practical applications of these
emerging AI paradigms in smart manufacturing remain unclear. To address this
gap, this study systematically reviews the evolution of AI and AI agent
technologies, examines the core concepts and technological advancements of
LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential
applications in and integration into manufacturing, along with the potential
challenges they may face.

</details>


### [4] [A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models](https://arxiv.org/abs/2507.01410)
*Abeer Dyoub,Francesca A. Lisi*

Main category: cs.AI

TL;DR: 本文提出基于伦理风险评估描述伦理决策模型的形式化方法，用模糊Petri网验证和确认模型，并以医疗领域案例说明。


<details>
  <summary>Details</summary>
Motivation: 道德领域的本体和认知复杂性使得难以建立评估道德机器性能的明确标准。

Method: 提出基于伦理风险评估描述伦理决策模型的形式化方法，用模糊Petri网对以模糊规则指定的模型进行验证和确认。

Result: 给出了方法并通过医疗领域案例进行说明。

Conclusion: 所提基于伦理风险评估和模糊Petri网的方法可用于处理伦理决策模型的描述、验证和确认。

Abstract: The ontological and epistemic complexities inherent in the moral domain make
it challenging to establish clear standards for evaluating the performance of a
moral machine. In this paper, we present a formal method to describe Ethical
Decision Making models based on ethical risk assessment. Then, we show how
these models that are specified as fuzzy rules can be verified and validated
using fuzzy Petri nets. A case study from the medical field is considered to
illustrate the proposed approach.

</details>


### [5] [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/abs/2507.01431)
*Yoonseok Yang,Minjune Kim,Marlon Rondinelli,Keren Shao*

Main category: cs.AI

TL;DR: 介绍AI辅助评分平台Pensieve，可利用大语言模型转录和评估学生作业，已在多机构应用，能减少评分时间并保持高评分一致性。


<details>
  <summary>Details</summary>
Motivation: 解决大学STEM课程中手写开放式答案评分的瓶颈问题。

Method: 开发Pensieve平台，利用大语言模型，采用人在环的界面支持整个评分流程。

Result: Pensieve已在20多所机构的课程中部署，评阅超300,000份学生答卷，平均减少65%的评分时间，高置信度预测与教师评分的一致率达95.4%。

Conclusion: Pensieve能有效减少评分时间，且评分准确性较高，可用于实际的课程评分。

Abstract: Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

</details>


### [6] [Using multi-agent architecture to mitigate the risk of LLM hallucinations](https://arxiv.org/abs/2507.01446)
*Abd Elrahman Amer,Magdi Amer*

Main category: cs.AI

TL;DR: 本文提出用多智能体系统处理短信客户请求，集成基于大语言模型的智能体与模糊逻辑以降低幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 提升客户服务质量和响应时间对维持客户忠诚度和增加市场份额至关重要，采用大语言模型等新兴技术有必要但存在幻觉风险。

Method: 提出多智能体系统处理短信客户请求，集成基于大语言模型的智能体与模糊逻辑。

Result: 未提及

Conclusion: 未提及

Abstract: Improving customer service quality and response time are critical factors for
maintaining customer loyalty and increasing a company's market share. While
adopting emerging technologies such as Large Language Models (LLMs) is becoming
a necessity to achieve these goals, the risk of hallucination remains a major
challenge. In this paper, we present a multi-agent system to handle customer
requests sent via SMS. This system integrates LLM based agents with fuzzy logic
to mitigate hallucination risks.

</details>


### [7] [Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning](https://arxiv.org/abs/2507.01489)
*Yanfei Zhang*

Main category: cs.AI

TL;DR: 论文聚焦LLM应用中强化学习问题，提出Agent - as - tool分层框架，在少量样本微调下取得不错效果。


<details>
  <summary>Details</summary>
Motivation: 以往研究在决定工具调用和推理过程时面临挑战，推理依赖含冗余信息的原始结果，增加模型推理负担。

Method: 提出分层框架Agent - as - tool，分离工具调用和推理过程，让模型专注推理，工具调用由另一代理处理。

Result: 在180个样本上轻微强化微调取得可比结果，在Bamboogle测试中精确匹配63.2%，覆盖精确匹配75.2%，优于Search - R1。

Conclusion: 所提分层框架Agent - as - tool在LLM应用强化学习方面有效，能提升模型性能。

Abstract: Large Language Models (LLMs) have emerged as one of the most significant
technological advancements in artificial intelligence in recent years. Their
ability to understand, generate, and reason with natural language has
transformed how we interact with AI systems. With the development of LLM-based
agents and reinforcement-learning-based reasoning models, the study of applying
reinforcement learning in agent frameworks has become a new research focus.
However, all previous studies face the challenge of deciding the tool calling
process and the reasoning process simultaneously, and the chain of reasoning
was solely relied on the unprocessed raw result with redundant information and
symbols unrelated to the task from the tool, which impose a heavy burden on the
model's capability to reason. Therefore, in our research, we proposed a
hierarchical framework Agent-as-tool that detach the tool calling process and
the reasoning process, which enables the model to focus on the verbally
reasoning process while the tool calling process is handled by another agent.
Our work had achieved comparable results with only a slight reinforcement
fine-tuning on 180 samples, and had achieved exceptionally well performance in
Bamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding
Search-R1 by 4.8% in exact match and 3.2% in cover exact match.

</details>


### [8] [T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2507.01597)
*Yuehang Si,Zefan Zeng,Jincai Huang,Qing Cheng*

Main category: cs.AI

TL;DR: 提出T3DM方法用于TKG推理并设计负采样策略，实验显示其优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有TKG推理研究面临训练和测试样本事件分布偏移建模不足以及负采样质量低的问题。

Method: 提出Test-Time Training-guided Distribution shift Modelling (T3DM)方法调整模型，基于对抗训练设计负采样策略。

Result: T3DM在大多数情况下比现有基线模型提供更好、更稳健的结果。

Conclusion: T3DM方法和负采样策略有效，能提升TKG推理效果。

Abstract: Temporal Knowledge Graph (TKG) is an efficient method for describing the
dynamic development of facts along a timeline. Most research on TKG reasoning
(TKGR) focuses on modelling the repetition of global facts and designing
patterns of local historical facts. However, they face two significant
challenges: inadequate modeling of the event distribution shift between
training and test samples, and reliance on random entity substitution for
generating negative samples, which often results in low-quality sampling. To
this end, we propose a novel distributional feature modeling approach for
training TKGR models, Test-Time Training-guided Distribution shift Modelling
(T3DM), to adjust the model based on distribution shift and ensure the global
consistency of model reasoning. In addition, we design a negative-sampling
strategy to generate higher-quality negative quadruples based on adversarial
training. Extensive experiments show that T3DM provides better and more robust
results than the state-of-the-art baselines in most cases.

</details>


### [9] [Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI](https://arxiv.org/abs/2507.01717)
*Gopichand Kanumolu,Ashok Urlana,Charaka Vinayak Kumar,Bala Mallikarjunarao Garlapati*

Main category: cs.AI

TL;DR: 本文探索用大语言模型和自主智能体从专利中挖掘和生成产品概念，设计Agent Ideate框架，实验表明智能体方法表现更优，结合二者可提升创新流程。


<details>
  <summary>Details</summary>
Motivation: 专利蕴含丰富技术知识，但获取和解读有挑战，需挖掘其用于创新产品想法。

Method: 设计Agent Ideate框架，用开源大语言模型和基于智能体的架构在三个领域进行实验。

Result: 智能体方法在想法质量、相关性和新颖性上始终优于独立大语言模型。

Conclusion: 结合大语言模型和智能体工作流可释放专利数据生成商业想法的潜力，显著提升创新流程。

Abstract: Patents contain rich technical knowledge that can inspire innovative product
ideas, yet accessing and interpreting this information remains a challenge.
This work explores the use of Large Language Models (LLMs) and autonomous
agents to mine and generate product concepts from a given patent. In this work,
we design Agent Ideate, a framework for automatically generating product-based
business ideas from patents. We experimented with open-source LLMs and
agent-based architectures across three domains: Computer Science, Natural
Language Processing, and Material Chemistry. Evaluation results show that the
agentic approach consistently outperformed standalone LLMs in terms of idea
quality, relevance, and novelty. These findings suggest that combining LLMs
with agentic workflows can significantly enhance the innovation pipeline by
unlocking the untapped potential of business idea generation from patent data.

</details>


### [10] [Joint Matching and Pricing for Crowd-shipping with In-store Customers](https://arxiv.org/abs/2507.01749)
*Arash Dehghan,Mucahit Cevik,Merve Bodur,Bissan Ghaddar*

Main category: cs.AI

TL;DR: 本文研究实体店顾客作为配送员的集中式众包配送系统，提出MDP模型和NeurADP + DDQN联合优化策略，实验显示该策略能提升配送成本效率。


<details>
  <summary>Details</summary>
Motivation: 应对城市地区对高效最后一公里配送日益增长的需求。

Method: 提出马尔可夫决策过程（MDP）模型，采用神经近似动态规划（NeurADP）进行订单与顾客的自适应分配，结合深度双Q网络（DDQN）进行动态定价。

Result: 集成的NeurADP + DDQN策略在配送成本效率上有显著提升，相比固定定价的NeurADP节省达6.7%，相比短视基线约节省18%；允许灵活配送延迟和多目的地路由分别降低运营成本8%和17%。

Conclusion: 强调了众包配送系统中动态、前瞻性策略的优势，为城市物流运营商提供了实用指导。

Abstract: This paper examines the use of in-store customers as delivery couriers in a
centralized crowd-shipping system, targeting the growing need for efficient
last-mile delivery in urban areas. We consider a brick-and-mortar retail
setting where shoppers are offered compensation to deliver time-sensitive
online orders. To manage this process, we propose a Markov Decision Process
(MDP) model that captures key uncertainties, including the stochastic arrival
of orders and crowd-shippers, and the probabilistic acceptance of delivery
offers. Our solution approach integrates Neural Approximate Dynamic Programming
(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network
(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop
routing and accounts for offer acceptance uncertainty, aligning more closely
with real-world operations. Experimental results demonstrate that the
integrated NeurADP + DDQN policy achieves notable improvements in delivery cost
efficiency, with up to 6.7\% savings over NeurADP with fixed pricing and
approximately 18\% over myopic baselines. We also show that allowing flexible
delivery delays and enabling multi-destination routing further reduces
operational costs by 8\% and 17\%, respectively. These findings underscore the
advantages of dynamic, forward-looking policies in crowd-shipping systems and
offer practical guidance for urban logistics operators.

</details>


### [11] [Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics](https://arxiv.org/abs/2507.01833)
*Yi-Dong Shen,Thomas Eiter*

Main category: cs.AI

TL;DR: 本文探讨非单调逻辑编程中回答集语义的两个问题，提出改进的GAS原则，定义新语义并评估现有语义，分析计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 探讨回答集语义是否应将最小模型属性、约束单调性和有根性作为强制条件，以及其他通用原则。

Method: 通过举例说明强制条件可能排除预期回答集；改进Gelfond回答集原则；扩展良好支持性概念；定义新的回答集语义；以改进原则评估现有语义；分析计算复杂度。

Result: 说明了现有条件有时过强，提出改进的GAS原则并定义新语义，可用于评估现有语义。

Conclusion: 改进的GAS原则为回答集语义提供了新的视角和评估标准。

Abstract: Non-monotonic logic programming is the basis for a declarative problem
solving paradigm known as answer set programming (ASP). Departing from the
seminal definition by Gelfond and Lifschitz in 1988 for simple normal logic
programs, various answer set semantics have been proposed for extensions. We
consider two important questions: (1) Should the minimal model property,
constraint monotonicity and foundedness as defined in the literature be
mandatory conditions for an answer set semantics in general? (2) If not, what
other properties could be considered as general principles for answer set
semantics? We address the two questions. First, it seems that the three
aforementioned conditions may sometimes be too strong, and we illustrate with
examples that enforcing them may exclude expected answer sets. Second, we
evolve the Gelfond answer set (GAS) principles for answer set construction by
refining the Gelfond's rationality principle to well-supportedness, minimality
w.r.t. negation by default and minimality w.r.t. epistemic negation. The
principle of well-supportedness guarantees that every answer set is
constructible from if-then rules obeying a level mapping and is thus free of
circular justification, while the two minimality principles ensure that the
formalism minimizes knowledge both at the level of answer sets and of world
views. Third, to embody the refined GAS principles, we extend the notion of
well-supportedness substantially to answer sets and world views, respectively.
Fourth, we define new answer set semantics in terms of the refined GAS
principles. Fifth, we use the refined GAS principles as an alternative baseline
to intuitively assess the existing answer set semantics. Finally, we analyze
the computational complexity.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [12] [HPC-AI Coupling Methodology for Scientific Applications](https://arxiv.org/abs/2507.01025)
*Yutong Lu,Dan Huang,Pin Chen*

Main category: cs.CE

TL;DR: 本文探索新兴科学应用中HPC与AI耦合场景，提出三种耦合模式，通过材料科学案例验证其有效性，为HPC - AI耦合提供指导。


<details>
  <summary>Details</summary>
Motivation: 解决数值型高性能计算应用面临的高计算强度等挑战，探索HPC与AI耦合在新兴科学应用中的场景。

Method: 提出包含代理、指令和协调三种耦合模式的新方法，并通过材料科学案例研究进行验证。

Result: 展示了三种耦合模式的应用和有效性，突出了技术挑战、性能改进和实施细节。

Conclusion: 提出的耦合模式不仅适用于材料科学，也适用于其他科学领域，为未来科学发现中的HPC - AI集成提供有价值的指导。

Abstract: Artificial intelligence (AI) technologies have fundamentally transformed
numerical-based high-performance computing (HPC) applications with data-driven
approaches and endeavored to address existing challenges, e.g. high
computational intensity, in various scientific domains. In this study, we
explore the scenarios of coupling HPC and AI (HPC-AI) in the context of
emerging scientific applications, presenting a novel methodology that
incorporates three patterns of coupling: surrogate, directive, and coordinate.
Each pattern exemplifies a distinct coupling strategy, AI-driven prerequisite,
and typical HPC-AI ensembles. Through case studies in materials science, we
demonstrate the application and effectiveness of these patterns. The study
highlights technical challenges, performance improvements, and implementation
details, providing insight into promising perspectives of HPC-AI coupling. The
proposed coupling patterns are applicable not only to materials science but
also to other scientific domains, offering valuable guidance for future HPC-AI
ensembles in scientific discovery.

</details>


### [13] [Agentic AI in Product Management: A Co-Evolutionary Model](https://arxiv.org/abs/2507.01069)
*Nishant A. Parikh*

Main category: cs.CE

TL;DR: 本文探讨代理式AI在产品管理中的变革作用，提出概念性协同进化框架，并强调产品经理与AI需相互适应。


<details>
  <summary>Details</summary>
Motivation: 探索代理式AI在产品管理中的变革作用，解决传统框架的不足。

Method: 运用系统理论、协同进化理论和人机交互理论构建框架，对70多个来源进行综合审查。

Result: 发现产品经理与AI需相互适应，需要AI素养、治理和系统思维等技能。

Conclusion: 本研究为未来研究和实践实施提供基础，确保软件组织负责任、有效地集成代理式AI。

Abstract: This study explores agentic AI's transformative role in product management,
proposing a conceptual co-evolutionary framework to guide its integration
across the product lifecycle. Agentic AI, characterized by autonomy,
goal-driven behavior, and multi-agent collaboration, redefines product managers
(PMs) as orchestrators of socio-technical ecosystems. Using systems theory,
co-evolutionary theory, and human-AI interaction theory, the framework maps
agentic AI capabilities in discovery, scoping, business case development,
development, testing, and launch. An integrative review of 70+ sources,
including case studies from leading tech firms, highlights PMs' evolving roles
in AI orchestration, supervision, and strategic alignment. Findings emphasize
mutual adaptation between PMs and AI, requiring skills in AI literacy,
governance, and systems thinking. Addressing gaps in traditional frameworks,
this study provides a foundation for future research and practical
implementation to ensure responsible, effective agentic AI integration in
software organizations.

</details>


### [14] [Spatially Distributed Wettability Characterization in Porous Media](https://arxiv.org/abs/2507.01617)
*Faisal Aljaberi,Hadi Belhaj,Sajjad Foroughi,Mohammed Al-Kobaisi,Martin Blunt*

Main category: cs.CE

TL;DR: 提出增强几何算法用于从微 CT 图像逐孔测量接触角，揭示润湿性异质性，利于多相流行为预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在测量接触角精度上不足，且难以揭示润湿性异质性，为了更准确预测非均质多孔材料多相流行为，优化地下储能和开采过程。

Method: 提出增强几何算法，通过稳健的流体 - 流体和固体 - 流体界面外推进行逐孔接触角测量，并生成空间分布接触角图。

Result: 新算法比现有方法精度更高，揭示了先前隐藏的润湿性异质性，发现平均指标有严重局限性。

Conclusion: 提供的开源工具可实现空间分辨润湿性表征，能更准确预测非均质多孔材料多相流行为，对优化地下储能和开采过程至关重要。

Abstract: An enhanced geometric algorithm for automated pore-by-pore contact angle
measurement from micro-CT images, is presented that achieves superior accuracy
compared to existing methods through robust fluid-fluid and solid-fluid
interface extrapolation. Using this high resolution data, we generate spatially
distributed contact angle maps that reveal previously hidden wettability
heterogeneity. Our analysis of mixed-wet systems demonstrates the severe
limitations of averaged metrics: a sample with a mean contact angle of 64.7
degrees, conventionally classified as uniformly weakly water-wet, exhibits 40%
of its pore space in the intermediate-wetting regime (70-110 degrees). This
heterogeneity explains the presence of minimal surface interfaces and
fundamentally different pore-filling mechanisms operating within the same
sample. By providing open-source tools for spatially-resolved wettability
characterization, this work enables more accurate predictions of multiphase
flow behavior in heterogeneous porous materials, essential for optimizing
subsurface energy storage and recovery processes.

</details>


### [15] [A modified Levenberg-Marquardt method for estimating the elastic material parameters of polymer waveguides using residuals between autocorrelated frequency responses](https://arxiv.org/abs/2507.01706)
*Dominik Itner,Dmitrij Dreiling,Hauke Gravenkamp,Bernd Henning,Carolin Birk*

Main category: cs.CE

TL;DR: 本文解决超声范围内聚合物频率相关弹性参数估计的逆问题，提出两种新方法加速优化，减少模型评估次数和时间，经与现有方法对比证明有效。


<details>
  <summary>Details</summary>
Motivation: 解决超声范围内聚合物频率相关弹性参数估计问题，加速优化过程，减少模型评估次数和等待时间。

Method: 将逆问题转化为非线性回归型优化问题，提出适应的Levenberg - Marquardt方法和基于自相关包络的改进目标函数。

Result: 通过与现有优化方法对比，证明提出的目标函数修改和步长调整方法对各向同性材料有效，减少了模型评估总次数。

Conclusion: 所提方法能缩短识别材料参数的时间。

Abstract: In this contribution, we address the estimation of the frequency-dependent
elastic parameters of polymers in the ultrasound range, which is formulated as
an inverse problem. This inverse problem is implemented as a nonlinear
regression-type optimization problem, in which the simulation signals are
fitted to the measurement signals. These signals consist of displacement
responses in waveguides, focusing on hollow cylindrical geometries to enhance
the simulation efficiency. To accelerate the optimization and reduce the number
of model evaluations and wait times, we propose two novel methods. First, we
introduce an adaptation of the Levenberg-Marquardt method derived from a
geometrical interpretation of the least-squares optimization problem. Second,
we introduce an improved objective function based on the autocorrelated
envelopes of the measurement and simulation signals. Given that this study
primarily relies on simulation data to quantify optimization convergence, we
aggregate the expected ranges of realistic material parameters and derive their
distributions to ensure the reproducibility of optimizations with proper
measurements. We demonstrate the effectiveness of our objective function
modification and step adaptation for various materials with isotropic material
symmetry by comparing them with a state-of-the-art optimization method. In all
cases, our method reduces the total number of model evaluations, thereby
shortening the time to identify the material parameters.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [16] [MobileRAG: A Fast, Memory-Efficient, and Energy-Efficient Method for On-Device RAG](https://arxiv.org/abs/2507.01079)
*Taehwan Park,Geonho Lee,Min-Soo Kim*

Main category: cs.DB

TL;DR: 提出适用于移动设备的MobileRAG，结合EcoVector和SCR方法，实验显示其在多方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: RAG在移动设备上应用因资源限制未充分探索，现有方案不适用于设备端场景。

Method: 提出MobileRAG，结合移动友好的向量搜索算法EcoVector和轻量级的Selective Content Reduction (SCR)方法。

Result: MobileRAG在延迟、内存使用和功耗方面显著优于传统方法，保持准确性并支持离线操作。

Conclusion: MobileRAG能在资源受限环境中克服限制，保障隐私并有效运行。

Abstract: Retrieval-Augmented Generation (RAG) has proven effective on server
infrastructures, but its application on mobile devices is still underexplored
due to limited memory and power resources. Existing vector search and RAG
solutions largely assume abundant computation resources, making them
impractical for on-device scenarios. In this paper, we propose MobileRAG, a
fully on-device pipeline that overcomes these limitations by combining a
mobile-friendly vector search algorithm, \textit{EcoVector}, with a lightweight
\textit{Selective Content Reduction} (SCR) method. By partitioning and
partially loading index data, EcoVector drastically reduces both memory
footprint and CPU usage, while the SCR method filters out irrelevant text to
diminish Language Model (LM) input size without degrading accuracy. Extensive
experiments demonstrated that MobileRAG significantly outperforms conventional
vector search and RAG methods in terms of latency, memory usage, and power
consumption, while maintaining accuracy and enabling offline operation to
safeguard privacy in resource-constrained environments.

</details>


### [17] [Handling out-of-order input arrival in CEP engines on the edge combining optimistic, pessimistic and lazy evaluation](https://arxiv.org/abs/2507.01461)
*Styliani Kyrama,Anastasios Gounaris*

Main category: cs.DB

TL;DR: 提出LimeCEP混合CEP方法，结合多种技术处理数据不一致，集成Kafka，与SASE和FlinkCEP对比，有低延迟、低资源消耗优势，适合非云部署。


<details>
  <summary>Details</summary>
Motivation: 在复杂事件处理中，处理乱序、延迟和重复事件对实时分析很关键，特别是在处理多源异构数据的资源受限设备上。

Method: 提出LimeCEP方法，结合懒惰评估、缓冲和推测处理，集成Kafka，提供可配置策略。

Result: 与SASE和FlinkCEP相比，LimeCEP延迟降低六个数量级，内存使用降低10倍，CPU利用率降低6倍，在高乱序输入流下保持高精度和召回率。

Conclusion: LimeCEP适合非云部署。

Abstract: In Complex Event Processing, handling out-of-order, late, and duplicate
events is critical for real-time analytics, especially on resource-constrained
devices that process heterogeneous data from multiple sources. We present
LimeCEP, a hybrid CEP approach that combines lazy evaluation, buffering, and
speculative processing to efficiently handle data inconsistencies while
supporting multi-pattern detection under relaxed semantics. LimeCEP integrates
Kafka for efficient message ordering, retention, and duplicate elimination, and
offers configurable strategies to trade off between accuracy, latency, and
resource consumption. Compared to state-of-the-art systems like SASE and
FlinkCEP, LimeCEP achieves up to six orders of magnitude lower latency, with up
to 10 times lower memory usage and 6 times lower CPU utilization, while
maintaining near-perfect precision and recall under high-disorder input
streams, making it well-suited for non-cloud deployments.

</details>


### [18] [Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems](https://arxiv.org/abs/2507.01599)
*Zhaoyan Sun,Jiayi Wang,Xinyang Zhao,Jiachi Wang,Guoliang Li*

Main category: cs.DB

TL;DR: 传统Data+AI系统依赖人工协调，存在困难，本文提出“数据代理”概念以协调Data+AI生态，还介绍相关系统及挑战。


<details>
  <summary>Details</summary>
Motivation: 传统Data+AI系统在语义理解、推理和规划能力有限，依赖人工协调，而大语言模型在这些方面有成功经验，需结合其技术革新数据系统。

Method: 提出“数据代理”的综合架构，处理数据相关任务，分析设计挑战，给出系统示例。

Result: 介绍了多种数据代理系统，如数据科学代理、数据分析代理、数据库管理员代理等。

Conclusion: 指出设计数据代理系统存在一些开放性挑战。

Abstract: Traditional Data+AI systems utilize data-driven techniques to optimize
performance, but they rely heavily on human experts to orchestrate system
pipelines, enabling them to adapt to changes in data, queries, tasks, and
environments. For instance, while there are numerous data science tools
available, developing a pipeline planning system to coordinate these tools
remains challenging. This difficulty arises because existing Data+AI systems
have limited capabilities in semantic understanding, reasoning, and planning.
Fortunately, we have witnessed the success of large language models (LLMs) in
enhancing semantic understanding, reasoning, and planning abilities. It is
crucial to incorporate LLM techniques to revolutionize data systems for
orchestrating Data+AI applications effectively.
  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive
architecture designed to orchestrate Data+AI ecosystems, which focuses on
tackling data-related tasks by integrating knowledge comprehension, reasoning,
and planning capabilities. We delve into the challenges involved in designing
data agents, such as understanding data/queries/environments/tools,
orchestrating pipelines/workflows, optimizing and executing pipelines, and
fostering pipeline self-reflection. Furthermore, we present examples of data
agent systems, including a data science agent, data analytics agents (such as
unstructured data analytics agent, semantic structured data analytics agent,
data lake analytics agent, and multi-modal data analytics agent), and a
database administrator (DBA) agent. We also outline several open challenges
associated with designing data agent systems.

</details>


### [19] [PathDB: A system for evaluating regular path queries](https://arxiv.org/abs/2507.01755)
*Roberto García,Renzo Angles,Vicente Rojas,Sebastián Ferrada*

Main category: cs.DB

TL;DR: PathDB是基于Java的图数据库，用RPQ和闭路径代数处理路径，模块化设计可优化，基准实验显示其性能优。


<details>
  <summary>Details</summary>
Motivation: 设计一个适合内存数据加载和查询的图数据库，并优化其处理动态复杂路径查询的性能。

Method: 利用Regular Path Queries (RPQ)和闭路径代数，通过解析器、逻辑计划和物理计划三个主要组件处理路径，采用模块化设计。

Result: 基准实验表明，与深度优先搜索（DFS）和广度优先搜索（BFS）等基线方法相比，PathDB在处理动态和复杂路径查询时具有更优的执行时间和灵活性。

Conclusion: PathDB的优化设计有助于提升其在处理路径查询时的性能。

Abstract: PathDB is a Java-based graph database designed for in-memory data loading and
querying. By utilizing Regular Path Queries (RPQ) and a closed path algebra,
PathDB processes paths through its three main components: the parser, the
logical plan, and the physical plan. This modular design allows for targeted
optimizations and modifications without impacting overall functionality.
Benchmark experiments illustrate PathDB's execution times and flexibility in
handling dynamic and complex path queries, compared to baseline methods like
Depth-First Search (DFS) and Breadth-First Search (BFS) guided by an automaton,
highlighting its optimizations that contribute to its performance.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [20] [HERCULES: Hardware accElerator foR stoChastic schedULing in hEterogeneous Systems](https://arxiv.org/abs/2507.01113)
*Vairavan Palaniappan,Adam H. Ross,Amit Ranjan Trivedi,Debjit Pal*

Main category: cs.DC

TL;DR: 提出基于FPGA的随机在线调度（SOS）加速器解决异构计算中工作负载调度问题，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 传统基于软件的调度器在异构计算环境中存在调度开销大、适应性差和资源利用率低等问题。

Method: 修改贪婪成本选择分配策略，将成本方程应用于离散时间后实现到硬件加速器设计中，利用硬件并行、预计算和精度量化减少调度延迟。

Result: 实现高吞吐量、低延迟和节能操作，实验显示工作负载分布一致、机器利用公平，比单线程软件调度策略实现最高加速1060倍。

Conclusion: SOS加速器是高性能计算系统、深度学习管道等性能关键应用的有力候选方案。

Abstract: Efficient workload scheduling is a critical challenge in modern heterogeneous
computing environments, particularly in high-performance computing (HPC)
systems. Traditional software-based schedulers struggle to efficiently balance
workload distribution due to high scheduling overhead, lack of adaptability to
dynamic workloads, and suboptimal resource utilization. These pitfalls are
compounded in heterogeneous systems, where differing computational elements can
have vastly different performance profiles. To resolve these hindrances, we
present a novel FPGA-based accelerator for stochastic online scheduling (SOS).
We modify a greedy cost selection assignment policy by adapting existing cost
equations to engage with discretized time before implementing them into a
hardware accelerator design. Our design leverages hardware parallelism,
precalculation, and precision quantization to reduce job scheduling latency. By
introducing a hardware-accelerated approach to real-time scheduling, this paper
establishes a new paradigm for adaptive scheduling mechanisms in heterogeneous
computing systems. The proposed design achieves high throughput, low latency,
and energy-efficient operation, offering a scalable alternative to traditional
software scheduling methods. Experimental results demonstrate consistent
workload distribution, fair machine utilization, and up to 1060x speedup over
single-threaded software scheduling policy implementations. This makes the SOS
accelerator a strong candidate for deployment in high-performance computing
system, deeplearning pipelines, and other performance-critical applications.

</details>


### [21] [FLARE: A Dataflow-Aware and Scalable Hardware Architecture for Neural-Hybrid Scientific Lossy Compression](https://arxiv.org/abs/2507.01224)
*Wenqi Jia,Ying Huang,Jian Xu,Zhewen Hu,Sian Jin,Jiannan Tian,Yuede Ji,Miao Yin*

Main category: cs.DC

TL;DR: 传统HPC系统处理科学模拟数据有I/O和网络瓶颈，现有DNN压缩框架集成到HPC工作流有挑战，提出FLARE架构提升性能和能效。


<details>
  <summary>Details</summary>
Motivation: 解决HPC系统处理海量数据的I/O和网络瓶颈，以及DNN压缩框架集成到HPC工作流的难题，推动高性能科学计算。

Method: 提出数据流感知且可扩展的硬件架构FLARE，减少片外数据访问，降低气泡开销，采用模块化设计。

Result: 在不同数据集和硬件平台上，实现3.50 - 96.07倍的运行时加速，24.51 - 520.68倍的能效提升。

Conclusion: FLARE架构能显著提高现代HPC系统的吞吐量和能源效率。

Abstract: Scientific simulation leveraging high-performance computing (HPC) systems is
crucial for modeling complex systems and phenomena in fields such as
astrophysics, climate science, and fluid dynamics, generating massive datasets
that often reach petabyte to exabyte scales. However, managing these vast data
volumes introduces significant I/O and network bottlenecks, limiting practical
performance and scalability. While cutting-edge lossy compression frameworks
powered by deep neural networks (DNNs) have demonstrated superior compression
ratios by capturing complex data correlations, their integration into HPC
workflows poses substantial challenges due to the hybrid non-neural and neural
computation patterns, causing excessive memory access overhead, large
sequential stalls, and limited adaptability to varying data sizes and workloads
in existing hardware platforms. To overcome these challenges and push the limit
of high-performance scientific computing, we for the first time propose FLARE,
a dataflow-aware and scalable hardware architecture for neural-hybrid
scientific lossy compression. FLARE minimizes off-chip data access, reduces
bubble overhead through efficient dataflow, and adopts a modular design that
provides both scalability and flexibility, significantly enhancing throughput
and energy efficiency on modern HPC systems. Particularly, the proposed FLARE
achieves runtime speedups ranging from $3.50 \times$ to $96.07 \times$, and
energy efficiency improvements ranging from $24.51 \times$ to $520.68 \times$,
across various datasets and hardware platforms.

</details>


### [22] [Capacity Planning and Scheduling for Jobs with Uncertainty in Resource Usage and Duration](https://arxiv.org/abs/2507.01225)
*Sunandita Patra,Mehtab Pathan,Mahmoud Mahfouz,Parisa Zehtabi,Wided Ouaja,Daniele Magazzeni,Manuela Veloso*

Main category: cs.DC

TL;DR: 本文围绕混合架构下的本地网格计算环境，开展容量规划与作业调度，提出近似方法，在不影响服务质量下降低资源峰值使用。


<details>
  <summary>Details</summary>
Motivation: 在采用云计算混合架构下，对本地网格计算环境进行容量规划和作业调度，处理资源使用和作业时长的不确定性。

Method: 使用确定性估计器和基于成对抽样的约束规划的近似方法。

Result: 基于成对抽样的方法在不影响服务质量的情况下，比手动调度实现了更低的峰值资源使用。

Conclusion: 所提近似方法能在本地网格计算环境中有效平衡资源使用和服务质量。

Abstract: Organizations around the world schedule jobs (programs) regularly to perform
various tasks dictated by their end users. With the major movement towards
using a cloud computing infrastructure, our organization follows a hybrid
approach with both cloud and on-prem servers. The objective of this work is to
perform capacity planning, i.e., estimate resource requirements, and job
scheduling for on-prem grid computing environments. A key contribution of our
approach is handling uncertainty in both resource usage and duration of the
jobs, a critical aspect in the finance industry where stochastic market
conditions significantly influence job characteristics. For capacity planning
and scheduling, we simultaneously balance two conflicting objectives: (a)
minimize resource usage, and (b) provide high quality-of-service to the end
users by completing jobs by their requested deadlines. We propose approximate
approaches using deterministic estimators and pair sampling-based constraint
programming. Our best approach (pair sampling-based) achieves much lower peak
resource usage compared to manual scheduling without compromising on the
quality-of-service.

</details>


### [23] [Optimal Dispersion Under Asynchrony](https://arxiv.org/abs/2507.01298)
*Debasish Pattanayak,Ajay D. Kshemkalyani,Manish Kumar,Anisur Rahaman Molla,Gokarna Sharma*

Main category: cs.DC

TL;DR: 本文研究匿名端口标记图中的分散问题，提出首个在异步环境下以最优O(k)时间和O(log(k + Δ))内存完成分散的算法。


<details>
  <summary>Details</summary>
Motivation: 分散是移动代理分布式计算的基础任务，当前在异步环境下算法时间复杂度非最优，需缩小复杂度差距。

Method: 开发了一种在匿名图中构建端口一树的新技术。

Result: 得到首个在异步环境下以最优O(k)时间和O(log(k + Δ))内存完成分散的算法。

Conclusion: 利用新开发的技术，成功解决了异步环境下分散问题的复杂度差距。

Abstract: We study the dispersion problem in anonymous port-labeled graphs: $k \leq n$
mobile agents, each with a unique ID and initially located arbitrarily on the
nodes of an $n$-node graph with maximum degree $\Delta$, must autonomously
relocate so that no node hosts more than one agent. Dispersion serves as a
fundamental task in distributed computing of mobile agents, and its complexity
stems from key challenges in local coordination under anonymity and limited
memory.
  The goal is to minimize both the time to achieve dispersion and the memory
required per agent. It is known that any algorithm requires $\Omega(k)$ time in
the worst case, and $\Omega(\log k)$ bits of memory per agent. A recent result
[SPAA'25] gives an optimal $O(k)$-time algorithm in the synchronous setting and
an $O(k \log k)$-time algorithm in the asynchronous setting, both using
$O(\log(k+\Delta))$ bits.
  In this paper, we close the complexity gap in the asynchronous setting by
presenting the first dispersion algorithm that runs in optimal $O(k)$ time
using $O(\log(k+\Delta))$ bits of memory per agent. Our solution is based on a
novel technique we develop in this paper that constructs a port-one tree in
anonymous graphs, which may be of independent interest.

</details>


### [24] [EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices](https://arxiv.org/abs/2507.01438)
*Zheyu Shen,Yexiao He,Ziyao Wang,Yuning Zhang,Guoheng Sun,Wanghao Ye,Ang Li*

Main category: cs.DC

TL;DR: 介绍EdgeLoRA系统用于多租户边缘设备服务大语言模型，有三项创新，评估显示其在延迟和吞吐量上优于现状。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上高效服务大语言模型面临适配器选择复杂、内存开销大及资源利用率低等挑战，需要解决方案。

Method: 引入EdgeLoRA系统，包含自适应适配器选择机制、异构内存管理和批量LoRA推理三项创新。

Result: 使用Llama3.1 - 8B模型评估，EdgeLoRA在延迟和吞吐量上显著优于现状，吞吐量最高提升4倍，能同时服务更多适配器。

Conclusion: EdgeLoRA有潜力改变多租户场景下大语言模型的边缘部署，为资源受限环境提供可扩展高效解决方案。

Abstract: Large Language Models (LLMs) have gained significant attention due to their
versatility across a wide array of applications. Fine-tuning LLMs with
parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these
models to efficiently adapt to downstream tasks without extensive retraining.
Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial
benefits, such as reduced latency, enhanced privacy, and personalized
responses. However, serving LLMs efficiently on resource-constrained edge
devices presents critical challenges, including the complexity of adapter
selection for different tasks and memory overhead from frequent adapter
swapping. Moreover, given the multiple requests in multi-tenant settings,
processing requests sequentially results in underutilization of computational
resources and increased latency. This paper introduces EdgeLoRA, an efficient
system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA
incorporates three key innovations: (1) an adaptive adapter selection mechanism
to streamline the adapter configuration process; (2) heterogeneous memory
management, leveraging intelligent adapter caching and pooling to mitigate
memory operation overhead; and (3) batch LoRA inference, enabling efficient
batch processing to significantly reduce computational latency. Comprehensive
evaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly
outperforms the status quo (i.e., llama.cpp) in terms of both latency and
throughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times
boost in throughput. Even more impressively, it can serve several orders of
magnitude more adapters simultaneously. These results highlight EdgeLoRA's
potential to transform edge deployment of LLMs in multi-tenant scenarios,
offering a scalable and efficient solution for resource-constrained
environments.

</details>


### [25] [EDGChain-E: A Decentralized Git-Based Framework for Versioning Encrypted Energy Data](https://arxiv.org/abs/2507.01615)
*Alper Alimoglu,Kamil Erdayandi,Mustafa A. Mustafa,Ümit Cali*

Main category: cs.DC

TL;DR: 本文提出EDGChain - E去中心化框架，用区块链和IPFS管理加密能源数据，支持多利益相关者安全协作，确保数据FAIR合规。


<details>
  <summary>Details</summary>
Motivation: 解决能源研究和运营中加密能源数据的版本控制、安全协作以及数据可追溯性等问题。

Method: 提出EDGChain - E框架，结合区块链和IPFS，引入DAO进行协作数据治理，使用加密Git补丁跟踪数据更新，嵌入哈希内容标识符到Merkle树。

Result: 框架可实现数据完整性、可追溯性和隐私保护，支持多利益相关者安全协作，维护数据FAIR合规性。

Conclusion: 该框架能支持去中心化能源应用中的数据可重复性和信任，对能源数据管理有重要意义。

Abstract: This paper proposes a new decentralized framework, named EDGChain-E
(Encrypted-Data-Git Chain for Energy), designed to manage version-controlled,
encrypted energy data using blockchain and the InterPlanetary File System. The
framework incorporates a Decentralized Autonomous Organization (DAO) to
orchestrate collaborative data governance across the lifecycle of energy
research and operations, such as smart grid monitoring, demand forecasting, and
peer-to-peer energy trading. In EDGChain-E, initial commits capture the full
encrypted datasets-such as smart meter readings or grid telemetry-while
subsequent updates are tracked as encrypted Git patches, ensuring integrity,
traceability, and privacy. This versioning mechanism supports secure
collaboration across multiple stakeholders (e.g., utilities, researchers,
regulators) without compromising sensitive or regulated information. We
highlight the framework's capability to maintain FAIR-compliant (Findable,
Accessible, Interoperable, Reusable) provenance of encrypted data. By embedding
hash-based content identifiers in Merkle trees, the system enables transparent,
auditable, and immutable tracking of data changes, thereby supporting
reproducibility and trust in decentralized energy applications.

</details>


### [26] [Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization](https://arxiv.org/abs/2507.01676)
*Giuseppe Ruggeri,Renzo Andri,Daniele Jahier Pagliari,Lukas Cavigelli*

Main category: cs.DC

TL;DR: 提出定制数据流设计加速深度推荐模型（DLRMs）嵌入查找，实验显示有显著加速效果且对查询分布独立性更强。


<details>
  <summary>Details</summary>
Motivation: DLRMs推理是重要AI工作负载，其性能瓶颈在嵌入层，需加速嵌入查找。

Method: 提出四种在单核上有效查找嵌入表的策略和自动将表非对称映射到SoC多核的框架。

Result: 在华为Ascend AI加速器上实验，真实工作负载分布加速1.5 - 6.5倍，极不平衡分布超20倍，且比基线更独立于查询分布。

Conclusion: 提出的方法能有效加速DLRMs嵌入查找，性能优于基线。

Abstract: Deep Recommender Models (DLRMs) inference is a fundamental AI workload
accounting for more than 79% of the total AI workload in Meta's data centers.
DLRMs' performance bottleneck is found in the embedding layers, which perform
many random memory accesses to retrieve small embedding vectors from tables of
various sizes. We propose the design of tailored data flows to speedup
embedding look-ups. Namely, we propose four strategies to look up an embedding
table effectively on one core, and a framework to automatically map the tables
asymmetrically to the multiple cores of a SoC. We assess the effectiveness of
our method using the Huawei Ascend AI accelerators, comparing it with the
default Ascend compiler, and we perform high-level comparisons with Nvidia
A100. Results show a speed-up varying from 1.5x up to 6.5x for real workload
distributions, and more than 20x for extremely unbalanced distributions.
Furthermore, the method proves to be much more independent of the query
distribution than the baseline.

</details>


### [27] [Evolving HPC services to enable ML workloads on HPE Cray EX](https://arxiv.org/abs/2507.01880)
*Stefano Schuppli,Fawzi Mohamed,Henrique Mendonça,Nina Mujkanovic,Elia Palme,Dino Conciatore,Lukas Drescher,Miguel Gila,Pim Witlox,Joost VandeVondele,Maxime Martinasso,Thomas C. Schulthess,Torsten Hoefler*

Main category: cs.DC

TL;DR: 本文围绕阿尔卑斯研究基础设施，对扩展HPC服务能力以支持ML工作负载展开初步研究，提出技术增强方案并探讨安全问题。


<details>
  <summary>Details</summary>
Motivation: 传统HPC服务不足以满足ML社区动态需求，需扩展HPC服务能力更好支持ML工作负载。

Method: 通过识别瑞士AI社区在阿尔卑斯早期访问阶段观察到的关键挑战和差距，提出包括用户环境、性能筛选工具等多项技术增强方案。

Result: 提出一系列旨在促进ML工作负载在HPC系统执行、提高系统可用性和弹性的技术增强方案。

Conclusion: 将提案置于HPC基础设施服务社区变化的更广泛背景中。

Abstract: The Alps Research Infrastructure leverages GH200 technology at scale,
featuring 10,752 GPUs. Accessing Alps provides a significant computational
advantage for researchers in Artificial Intelligence (AI) and Machine Learning
(ML). While Alps serves a broad range of scientific communities, traditional
HPC services alone are not sufficient to meet the dynamic needs of the ML
community. This paper presents an initial investigation into extending HPC
service capabilities to better support ML workloads. We identify key challenges
and gaps we have observed since the early-access phase (2023) of Alps by the
Swiss AI community and propose several technological enhancements. These
include a user environment designed to facilitate the adoption of HPC for ML
workloads, balancing performance with flexibility; a utility for rapid
performance screening of ML applications during development; observability
capabilities and data products for inspecting ongoing large-scale ML workloads;
a utility to simplify the vetting of allocated nodes for compute readiness; a
service plane infrastructure to deploy various types of workloads, including
support and inference services; and a storage infrastructure tailored to the
specific needs of ML workloads. These enhancements aim to facilitate the
execution of ML workloads on HPC systems, increase system usability and
resilience, and better align with the needs of the ML community. We also
discuss our current approach to security aspects. This paper concludes by
placing these proposals in the broader context of changes in the communities
served by HPC infrastructure like ours.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [28] [Faster Algorithm for Second (s,t)-mincut and Breaking Quadratic barrier for Dual Edge Sensitivity for (s,t)-mincut](https://arxiv.org/abs/2507.01366)
*Surender Baswana,Koustav Bhanja,Anupam Roy*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study (s,t)-cuts of second minimum capacity and present the following
algorithmic and graph-theoretic results.
  1. Vazirani and Yannakakis [ICALP 1992] designed the first algorithm for
computing an (s,t)-cut of second minimum capacity using $O(n^2)$ maximum
(s,t)-flow computations. For directed integer-weighted graphs, we significantly
improve this bound by designing an algorithm that computes an $(s,t)$-cut of
second minimum capacity using $O(\sqrt{n})$ maximum (s,t)-flow computations
w.h.p. To achieve this result, a close relationship of independent interest is
established between $(s,t)$-cuts of second minimum capacity and global mincuts
in directed weighted graphs.
  2. Minimum+1 (s,t)-cuts have been studied quite well recently [Baswana,
Bhanja, and Pandey, ICALP 2022], which is a special case of second
(s,t)-mincut.
  (a) For directed multi-graphs, we design an algorithm that, given any maximum
(s,t)-flow, computes a minimum+1 (s,t)-cut, if it exists, in $O(m)$ time.
  (b) The existing structures for storing and characterizing all minimum+1
(s,t)-cuts occupy $O(mn)$ space. For undirected multi-graphs, we design a DAG
occupying only $O(m)$ space that stores and characterizes all minimum+1
(s,t)-cuts.
  3. The study of minimum+1 (s,t)-cuts often turns out to be useful in
designing dual edge sensitivity oracles -- a compact data structure for
efficiently reporting an (s,t)-mincut after insertion/failure of any given pair
of query edges. It has been shown recently [Bhanja, ICALP 2025] that any dual
edge sensitivity oracle for (s,t)-mincut in undirected multi-graphs must occupy
${\Omega}(n^2)$ space in the worst-case, irrespective of the query time. For
simple graphs, we break this quadratic barrier while achieving a non-trivial
query time.

</details>


### [29] [Dynamic Similarity Graph Construction with Kernel Density Estimation](https://arxiv.org/abs/2507.01696)
*Steinar Laenen,Peter Macgregor,He Sun*

Main category: cs.DS

TL;DR: 本文研究动态设置下的KDE问题，引入数据结构维护查询点估计，设计动态数据结构用于稀疏近似相似图，开发快速动态谱聚类算法并评估效果。


<details>
  <summary>Details</summary>
Motivation: 解决动态设置下KDE问题，即随着数据点不断加入，高效维护查询点的估计。

Method: 引入数据结构维护查询点估计，设计动态数据结构稀疏近似完全连接的相似图，开发动态谱聚类算法。

Result: 开发出相关算法并在合成和真实数据集上进行评估。

Conclusion: 未明确提及，但从研究来看算法在动态KDE及谱聚类问题上可能有较好表现。

Abstract: In the kernel density estimation (KDE) problem, we are given a set $X$ of
data points in $\mathbb{R}^d$, a kernel function $k: \mathbb{R}^d \times
\mathbb{R}^d \rightarrow \mathbb{R}$, and a query point $\mathbf{q} \in
\mathbb{R}^d$, and the objective is to quickly output an estimate of
$\sum_{\mathbf{x} \in X} k(\mathbf{q}, \mathbf{x})$. In this paper, we consider
$\textsf{KDE}$ in the dynamic setting, and introduce a data structure that
efficiently maintains the estimates for a set of query points as data points
are added to $X$ over time. Based on this, we design a dynamic data structure
that maintains a sparse approximation of the fully connected similarity graph
on $X$, and develop a fast dynamic spectral clustering algorithm. We further
evaluate the effectiveness of our algorithms on both synthetic and real-world
datasets.

</details>


### [30] [SPARSE-PIVOT: Dynamic correlation clustering for node insertions](https://arxiv.org/abs/2507.01830)
*Mina Dalirrooyfard,Konstantin Makarychev,Slobodan Mitrović*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a new Correlation Clustering algorithm for a dynamic setting where
nodes are added one at a time. In this model, proposed by Cohen-Addad,
Lattanzi, Maggiori, and Parotsidis (ICML 2024), the algorithm uses database
queries to access the input graph and updates the clustering as each new node
is added. Our algorithm has the amortized update time of
$O_{\epsilon}(\log^{O(1)}(n))$. Its approximation factor is $20+\varepsilon$,
which is a substantial improvement over the approximation factor of the
algorithm by Cohen-Addad et al. We complement our theoretical findings by
empirically evaluating the approximation guarantee of our algorithm. The
results show that it outperforms the algorithm by Cohen-Addad et al.~in
practice.

</details>


### [31] [Breaking the $n^{1.5}$ Additive Error Barrier for Private and Efficient Graph Sparsification via Private Expander Decomposition](https://arxiv.org/abs/2507.01873)
*Anders Aamand,Justin Y. Chen,Mina Dalirrooyfard,Slobodan Mitrović,Yuriy Nevmyvaka,Sandeep Silwal,Yinzhan Xu*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study differentially private algorithms for graph cut sparsification, a
fundamental problem in algorithms, privacy, and machine learning. While
significant progress has been made, the best-known private and efficient cut
sparsifiers on $n$-node graphs approximate each cut within
$\widetilde{O}(n^{1.5})$ additive error and $1+\gamma$ multiplicative error for
any $\gamma > 0$ [Gupta, Roth, Ullman TCC'12]. In contrast, "inefficient"
algorithms, i.e., those requiring exponential time, can achieve an
$\widetilde{O}(n)$ additive error and $1+\gamma$ multiplicative error
[Eli{\'a}{\v{s}}, Kapralov, Kulkarni, Lee SODA'20]. In this work, we break the
$n^{1.5}$ additive error barrier for private and efficient cut sparsification.
We present an $(\varepsilon,\delta)$-DP polynomial time algorithm that, given a
non-negative weighted graph, outputs a private synthetic graph approximating
all cuts with multiplicative error $1+\gamma$ and additive error $n^{1.25 +
o(1)}$ (ignoring dependencies on $\varepsilon, \delta, \gamma$).
  At the heart of our approach lies a private algorithm for expander
decomposition, a popular and powerful technique in (non-private) graph
algorithms.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [32] [Counterfactual Explanation of Shapley Value in Data Coalitions](https://arxiv.org/abs/2507.01267)
*Michelle Si,Jian Pei*

Main category: cs.GT

TL;DR: 本文提出数据联盟中Shapley值的反事实解释问题，证明精确求解困难，开发启发式技术得到SV - Exp算法，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 解释数据联盟中所有者的Shapley值是未探索且有挑战的任务。

Method: 提出反事实解释问题，证明精确求解是NP - 难问题，开发估计差分Shapley值、计算单个数据项能力和贪婪转移子集等启发式技术得到SV - Exp算法。

Result: 实验结果表明方法有效，反事实解释能有效解释所有者的Shapley值。

Conclusion: 所开发的方法能有效提高计算效率，反事实解释可用于解释Shapley值。

Abstract: The Shapley value is widely used for data valuation in data markets. However,
explaining the Shapley value of an owner in a data coalition is an unexplored
and challenging task. To tackle this, we formulate the problem of finding the
counterfactual explanation of Shapley value in data coalitions. Essentially,
given two data owners $A$ and $B$ such that $A$ has a higher Shapley value than
$B$, a counterfactual explanation is a smallest subset of data entries in $A$
such that transferring the subset from $A$ to $B$ makes the Shapley value of
$A$ less than that of $B$. We show that counterfactual explanations always
exist, but finding an exact counterfactual explanation is NP-hard. Using Monte
Carlo estimation to approximate counterfactual explanations directly according
to the definition is still very costly, since we have to estimate the Shapley
values of owners $A$ and $B$ after each possible subset shift. We develop a
series of heuristic techniques to speed up computation by estimating
differential Shapley values, computing the power of singular data entries, and
shifting subsets greedily, culminating in the SV-Exp algorithm. Our
experimental results on real datasets clearly demonstrate the efficiency of our
method and the effectiveness of counterfactuals in interpreting the Shapley
value of an owner.

</details>


### [33] [Evaluating LLM Agent Collusion in Double Auctions](https://arxiv.org/abs/2507.01413)
*Kushal Agrawal,Verona Teo,Juan J. Vazquez,Sudarsh Kunnavakkam,Vishak Srikanth,Andy Liu*

Main category: cs.GT

TL;DR: 研究大语言模型（LLMs）代理在模拟市场中勾结行为，发现沟通、模型和环境压力影响勾结倾向，有经济和伦理考量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型代理参与社会经济互动增加，识别其不良行为潜力，聚焦于勾结行为。

Method: 研究LLM代理在模拟连续双拍卖市场中作为卖家的行为，进行一系列对照实验。

Result: 直接沟通增加勾结倾向，不同模型勾结倾向有差异，环境压力影响勾结行为。

Conclusion: LLM市场代理部署需考虑经济和伦理因素。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities as
autonomous agents with rapidly expanding applications in various domains. As
these agents increasingly engage in socioeconomic interactions, identifying
their potential for undesirable behavior becomes essential. In this work, we
examine scenarios where they can choose to collude, defined as secretive
cooperation that harms another party. To systematically study this, we
investigate the behavior of LLM agents acting as sellers in simulated
continuous double auction markets. Through a series of controlled experiments,
we analyze how parameters such as the ability to communicate, choice of model,
and presence of environmental pressures affect the stability and emergence of
seller collusion. We find that direct seller communication increases collusive
tendencies, the propensity to collude varies across models, and environmental
pressures, such as oversight and urgency from authority figures, influence
collusive behavior. Our findings highlight important economic and ethical
considerations for the deployment of LLM-based market agents.

</details>


### [34] [Rational Censorship Attack: Breaking Blockchain with a Blackboard](https://arxiv.org/abs/2507.01453)
*Michelle Yeo,Haoqian Zhang*

Main category: cs.GT

TL;DR: 从博弈论视角分析区块链安全时，提出针对区块链审查抗性的理性审查攻击，建模攻击并分析相关策略和影响及对策。


<details>
  <summary>Details</summary>
Motivation: 结合博弈论视角分析区块链安全的趋势，研究区块链审查抗性在理性假设下的情况。

Method: 采用博弈论框架，提出攻击模型，分析节点策略的子博弈完美均衡。

Result: 证明加入攻击和诚实声明权力的策略是子博弈完美均衡，指出攻击成功需节点知晓勾结组真实投票权。

Conclusion: 讨论了攻击对区块链用户和协议设计者的影响及潜在对策。

Abstract: Censorship resilience is a fundamental assumption underlying the security of
blockchain protocols. Additionally, the analysis of blockchain security from an
economic and game theoretic perspective has been growing in popularity in
recent years. In this work, we present a surprising rational censorship attack
on blockchain censorship resilience when we adopt the analysis of blockchain
security from a game theoretic lens and assume all users are rational. In our
attack, a colluding group with sufficient voting power censors the remainder
nodes such that the group alone can gain all the rewards from maintaining the
blockchain. We show that if nodes are rational, coordinating this attack just
requires a public read and write blackboard and we formally model the attack
using a game theoretic framework. Furthermore, we note that to ensure the
success of the attack, nodes need to know the total true voting power held by
the colluding group. We prove that the strategy to join the rational censorship
attack and also for nodes to honestly declare their power is a subgame perfect
equilibrium in the corresponding extensive form game induced by our attack.
Finally, we discuss the implications of the attack on blockchain users and
protocol designers as well as some potential countermeasures.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [35] [Can Argus Judge Them All? Comparing VLMs Across Domains](https://arxiv.org/abs/2507.01042)
*Harsh Joshi,Gautam Siddharth Kashyap,Rafiq Ali,Ebad Shabbir,Niharika Jain,Sarthak Jain,Jiechao Gao,Usman Naseem*

Main category: cs.IR

TL;DR: 本文对CLIP、BLIP和LXMERT在多任务上进行基准测试，揭示了视觉语言模型泛化和专业化之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在不同任务上的性能一致性缺乏研究。

Method: 在检索、字幕生成和推理等不同数据集上对CLIP、BLIP和LXMERT进行基准测试，评估任务准确率、生成质量、效率和跨数据集一致性（CDC）指标。

Result: CLIP泛化能力最强（CDC: 0.92），BLIP在精选数据上表现出色，LXMERT在结构化推理中领先。

Conclusion: 结果揭示了泛化和专业化之间的权衡，为视觉语言模型的工业部署提供信息，并指导开发更健壮、任务灵活的架构。

Abstract: Vision-Language Models (VLMs) are advancing multimodal AI, yet their
performance consistency across tasks is underexamined. We benchmark CLIP, BLIP,
and LXMERT across diverse datasets spanning retrieval, captioning, and
reasoning. Our evaluation includes task accuracy, generation quality,
efficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows
strongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT
leads in structured reasoning. These results expose trade-offs between
generalization and specialization, informing industrial deployment of VLMs and
guiding development toward robust, task-flexible architectures.

</details>


### [36] [Cohort Retrieval using Dense Passage Retrieval](https://arxiv.org/abs/2507.01049)
*Pranav Jadhav*

Main category: cs.IR

TL;DR: 本文将Dense Passage Retrieval (DPR) 应用于超声心动图领域的患者队列检索，提出数据转换方法、设计评估指标，自定义训练的DPR嵌入模型表现优于传统方法，还可适配其他医疗领域。


<details>
  <summary>Details</summary>
Motivation: 解决超声心动图领域患者队列检索的挑战。

Method: 将非结构化超声心动图电子健康记录数据集转换为查询 - 段落数据集，设计基于实际临床场景的评估指标，自定义训练DPR嵌入模型。

Result: 自定义训练的DPR嵌入模型比传统和现成的SOTA方法表现更优。

Conclusion: 这是首次将DPR应用于超声心动图领域的患者队列检索，建立的框架可适配其他医疗领域。

Abstract: Patient cohort retrieval is a pivotal task in medical research and clinical
practice, enabling the identification of specific patient groups from extensive
electronic health records (EHRs). In this work, we address the challenge of
cohort retrieval in the echocardiography domain by applying Dense Passage
Retrieval (DPR), a prominent methodology in semantic search. We propose a
systematic approach to transform an echocardiographic EHR dataset of
unstructured nature into a Query-Passage dataset, framing the problem as a
Cohort Retrieval task. Additionally, we design and implement evaluation metrics
inspired by real-world clinical scenarios to rigorously test the models across
diverse retrieval tasks. Furthermore, we present a custom-trained DPR embedding
model that demonstrates superior performance compared to traditional and
off-the-shelf SOTA methods.To our knowledge, this is the first work to apply
DPR for patient cohort retrieval in the echocardiography domain, establishing a
framework that can be adapted to other medical domains.

</details>


### [37] [Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis](https://arxiv.org/abs/2507.01053)
*Rafi Al Attrach,Pedro Moreira,Rajna Fani,Renato Umeton,Leo Anthony Celi*

Main category: cs.IR

TL;DR: M3降低了理解和查询MIMIC - IV数据的技术门槛，能将自然语言问题转化为SQL查询，简化数据访问，加速原始记录转化为可行动的见解。


<details>
  <summary>Details</summary>
Motivation: 大型临床数据集虽有研究潜力，但因复杂性对有效使用造成障碍，需降低技术门槛以利利用。

Method: M3通过单命令从PhysioNet检索MIMIC - IV，启动本地SQLite实例或连接BigQuery，通过MCP让研究者用自然语言与数据库对话，用语言模型将自然语言转化为SQL进行查询。

Result: 与M3几分钟对话就能完成曾需数小时手工编写SQL且依赖理解复杂临床工作流程的细致队列分析。

Conclusion: M3简化数据访问，吸引更多研究人员挖掘临床重症监护数据，加速原始记录转化为可行动的见解。

Abstract: As ever-larger clinical datasets become available, they have the potential to
unlock unprecedented opportunities for medical research. Foremost among them is
Medical Information Mart for Intensive Care (MIMIC-IV), the world's largest
open-source EHR database. However, the inherent complexity of these datasets,
particularly the need for sophisticated querying skills and the need to
understand the underlying clinical settings, often presents a significant
barrier to their effective use. M3 lowers the technical barrier to
understanding and querying MIMIC-IV data. With a single command it retrieves
MIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the
hosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers
converse with the database in plain English. Ask a clinical question in natural
language; M3 uses a language model to translate it into SQL, executes the query
against the MIMIC-IV dataset, and returns structured results alongside the
underlying query for verifiability and reproducibility. Demonstrations show
that minutes of dialogue with M3 yield the kind of nuanced cohort analyses that
once demanded hours of handcrafted SQL and relied on understanding the
complexities of clinical workflows. By simplifying access, M3 invites the
broader research community to mine clinical critical-care data and accelerates
the translation of raw records into actionable insight.

</details>


### [38] [A Data Science Approach to Calcutta High Court Judgments: An Efficient LLM and RAG-powered Framework for Summarization and Similar Cases Retrieval](https://arxiv.org/abs/2507.01058)
*Puspendu Banerjee,Aritra Mazumdar,Wazib Ansar,Saptarsi Goswami,Amlan Chakrabarti*

Main category: cs.IR

TL;DR: 本文提出利用数据科学方法（LLM和RAG）分析加尔各答高等法院判决的框架，提高法律研究效率，助力获取关键法律信息。


<details>
  <summary>Details</summary>
Motivation: 司法系统面临大量法律问题，需合理利用司法资源，提高分析法院判决的效率。

Method: 提出复杂框架，包括创建摘要机制和智能案例检索系统，用案例附注摘要微调Pegasus模型，采用两步摘要技术构建RAG向量数据库。

Result: 在法律案例摘要方面取得显著改进，RAG框架能有效响应查询，提供案例概述和摘要。

Conclusion: 该技术提高了法律研究效率，有助于法律专业人士和学生获取和理解关键法律信息，对整体法律环境有益。

Abstract: The judiciary, as one of democracy's three pillars, is dealing with a rising
amount of legal issues, needing careful use of judicial resources. This
research presents a complex framework that leverages Data Science
methodologies, notably Large Language Models (LLM) and Retrieval-Augmented
Generation (RAG) techniques, to improve the efficiency of analyzing Calcutta
High Court verdicts. Our framework focuses on two key aspects: first, the
creation of a robust summarization mechanism that distills complex legal texts
into concise and coherent summaries; and second, the development of an
intelligent system for retrieving similar cases, which will assist legal
professionals in research and decision making. By fine-tuning the Pegasus model
using case head note summaries, we achieve significant improvements in the
summarization of legal cases. Our two-step summarizing technique preserves
crucial legal contexts, allowing for the production of a comprehensive vector
database for RAG. The RAG-powered framework efficiently retrieves similar cases
in response to user queries, offering thorough overviews and summaries. This
technique not only improves legal research efficiency, but it also helps legal
professionals and students easily acquire and grasp key legal information,
benefiting the overall legal scenario.

</details>


### [39] [Optimizing Conversational Product Recommendation via Reinforcement Learning](https://arxiv.org/abs/2507.01060)
*Kang Liu*

Main category: cs.IR

TL;DR: 提出基于强化学习的方法优化多行业产品推荐对话策略，阐述框架、创新点及影响。


<details>
  <summary>Details</summary>
Motivation: 随着企业采用智能代理支持销售和服务，对话有效性不仅取决于推荐内容，还包括方式和时间，需优化对话策略。

Method: 采用反馈驱动的强化学习让代理系统学习最优对话策略，挖掘行为模式和转化结果。

Result: 使代理优化对话话术，提高参与度和产品使用率，且符合情境和监管约束。

Conclusion: 探讨了该方法在企业环境中实现可扩展、个性化推荐的影响。

Abstract: We propose a reinforcement learning-based approach to optimize conversational
strategies for product recommendation across diverse industries. As
organizations increasingly adopt intelligent agents to support sales and
service operations, the effectiveness of a conversation hinges not only on what
is recommended but how and when recommendations are delivered. We explore a
methodology where agentic systems learn optimal dialogue policies through
feedback-driven reinforcement learning. By mining aggregate behavioral patterns
and conversion outcomes, our approach enables agents to refine talk tracks that
drive higher engagement and product uptake, while adhering to contextual and
regulatory constraints. We outline the conceptual framework, highlight key
innovations, and discuss the implications for scalable, personalized
recommendation in enterprise environments.

</details>


### [40] [FAIR-MATCH: A Multi-Objective Framework for Bias Mitigation in Reciprocal Dating Recommendations](https://arxiv.org/abs/2507.01063)
*Madhav Kotecha*

Main category: cs.IR

TL;DR: 本文分析约会应用推荐系统问题并提出解决方案，指出当前系统有算法缺陷，通过分析得出性能数据，提出的框架可减少算法偏差。


<details>
  <summary>Details</summary>
Motivation: 当前约会应用推荐系统存在算法缺陷，如流行度偏差、过滤气泡效应和互惠建模不足等，限制了系统有效性并引入有害偏差，因此需要研究改进。

Method: 整合基础工作和近期实证研究，分析互惠推荐框架、公平性评估指标和行业实施情况，提出数学框架，采用增强相似度度量、多目标优化和公平感知算法。

Result: 当前系统表现一般，协同过滤达到25.1%，互惠方法达到28.7%。

Conclusion: 提出的数学框架能在保持竞争力的同时，改善人口统计代表性，减少算法偏差。

Abstract: Online dating platforms have fundamentally transformed the formation of
romantic relationships, with millions of users worldwide relying on algorithmic
matching systems to find compatible partners. However, current recommendation
systems in dating applications suffer from significant algorithmic
deficiencies, including but not limited to popularity bias, filter bubble
effects, and inadequate reciprocity modeling that limit effectiveness and
introduce harmful biases. This research integrates foundational work with
recent empirical findings to deliver a detailed analysis of dating app
recommendation systems, highlighting key issues and suggesting research-backed
solutions. Through analysis of reciprocal recommendation frameworks, fairness
evaluation metrics, and industry implementations, we demonstrate that current
systems achieve modest performance with collaborative filtering reaching 25.1\%
while reciprocal methods achieve 28.7\%. Our proposed mathematical framework
addresses these limitations through enhanced similarity measures,
multi-objective optimization, and fairness-aware algorithms that maintain
competitive accuracy while improving demographic representation to reduce
algorithmic bias.

</details>


### [41] [Enhanced Influence-aware Group Recommendation for Online Media Propagation](https://arxiv.org/abs/2507.01616)
*Chengkun He,Xiangmin Zhou,Chen Wang,Longbing Cao,Jie Shao,Xiaodong Li,Guang Xu,Carrie Jinqiu Hu,Zahir Tari*

Main category: cs.IR

TL;DR: 本文针对社交媒体流群体推荐的挑战提出EIGR框架，实验表明其在有效性和效率上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有影响感知的群体推荐任务因社交图规模大、影响传播动态性和实时匹配计算开销高而面临挑战。

Method: 提出EIGR框架，包括基于图提取的采样策略、动态独立级联模型和两级哈希用户组索引。

Result: 在真实数据集上的广泛实验表明EIGR在有效性和效率上始终优于现有基线。

Conclusion: 所提出的EIGR框架能有效解决群体推荐任务中的挑战。

Abstract: Group recommendation over social media streams has attracted significant
attention due to its wide applications in domains such as e-commerce,
entertainment, and online news broadcasting. By leveraging social connections
and group behaviours, group recommendation (GR) aims to provide more accurate
and engaging content to a set of users rather than individuals. Recently,
influence-aware GR has emerged as a promising direction, as it considers the
impact of social influence on group decision-making. In earlier work, we
proposed Influence-aware Group Recommendation (IGR) to solve this task.
However, this task remains challenging due to three key factors: the large and
ever-growing scale of social graphs, the inherently dynamic nature of influence
propagation within user groups, and the high computational overhead of
real-time group-item matching.
  To tackle these issues, we propose an Enhanced Influence-aware Group
Recommendation (EIGR) framework. First, we introduce a Graph Extraction-based
Sampling (GES) strategy to minimise redundancy across multiple temporal social
graphs and effectively capture the evolving dynamics of both groups and items.
Second, we design a novel DYnamic Independent Cascade (DYIC) model to predict
how influence propagates over time across social items and user groups.
Finally, we develop a two-level hash-based User Group Index (UG-Index) to
efficiently organise user groups and enable real-time recommendation
generation. Extensive experiments on real-world datasets demonstrate that our
proposed framework, EIGR, consistently outperforms state-of-the-art baselines
in both effectiveness and efficiency.

</details>


### [42] [Embedding-based Retrieval in Multimodal Content Moderation](https://arxiv.org/abs/2507.01066)
*Hanzhong Liang,Jinghao Shi,Xiang Shen,Zixuan Wang,Vera Wen,Ardalan Mehrani,Zhiqian Chen,Yifan Wu,Zhixin Zhang*

Main category: cs.IR

TL;DR: 为解决传统分类方法在视频内容审核场景中的不足，提出基于嵌入的检索（EBR）方法，实验表明该方法性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法在需要快速且低成本响应的视频内容审核场景中表现不佳，需要新方法。

Method: 利用监督对比学习（SCL）框架训练基础嵌入模型，构建嵌入生成和视频检索集成的基于嵌入的检索系统。

Result: 离线实验中ROC - AUC从0.85提升到0.99，PR - AUC从0.35提升到0.95；在线实验中行动率提高10.32%，运营成本降低超80%。

Conclusion: EBR方法相比基于分类的解决方案在性能、可解释性和灵活性上更优。

Abstract: Video understanding plays a fundamental role for content moderation on short
video platforms, enabling the detection of inappropriate content. While
classification remains the dominant approach for content moderation, it often
struggles in scenarios requiring rapid and cost-efficient responses, such as
trend adaptation and urgent escalations. To address this issue, we introduce an
Embedding-Based Retrieval (EBR) method designed to complement traditional
classification approaches. We first leverage a Supervised Contrastive Learning
(SCL) framework to train a suite of foundation embedding models, including both
single-modal and multi-modal architectures. Our models demonstrate superior
performance over established contrastive learning methods such as CLIP and
MoCo. Building on these embedding models, we design and implement the
embedding-based retrieval system that integrates embedding generation and video
retrieval to enable efficient and effective trend handling. Comprehensive
offline experiments on 25 diverse emerging trends show that EBR improves
ROC-AUC from 0.85 to 0.99 and PR-AUC from 0.35 to 0.95. Further online
experiments reveal that EBR increases action rates by 10.32% and reduces
operational costs by over 80%, while also enhancing interpretability and
flexibility compared to classification-based solutions.

</details>


### [43] [Towards a Signal Detection Based Measure for Assessing Information Quality of Explainable Recommender Systems](https://arxiv.org/abs/2507.01168)
*Yeonbin Son,Matthew L. Bolton*

Main category: cs.IR

TL;DR: 本文提出客观指标评估推荐系统解释信息质量（Veracity），通过分解维度、应用信号检测理论计算指标值，并设置案例验证有效性。


<details>
  <summary>Details</summary>
Motivation: 多数研究评估推荐系统时侧重整体性能，少数评估解释质量且多为主观，缺乏客观指标评估解释信息质量，本文旨在填补该空白。

Method: 将Veracity分解为Fidelity和Attunement两个维度，应用信号检测理论确定各维度决策结果并计算敏感性作为最终Veracity值。

Result: 设置不同信息质量案例验证，结果表明所提指标能有效捕捉质量差异。

Conclusion: 所提客观指标在评估推荐系统解释信息质量方面有效。

Abstract: There is growing interest in explainable recommender systems that provide
recommendations along with explanations for the reasoning behind them. When
evaluating recommender systems, most studies focus on overall recommendation
performance. Only a few assess the quality of the explanations. Explanation
quality is often evaluated through user studies that subjectively gather users'
opinions on representative explanatory factors that shape end-users'
perspective towards the results, not about the explanation contents itself. We
aim to fill this gap by developing an objective metric to evaluate Veracity:
the information quality of explanations. Specifically, we decompose Veracity
into two dimensions: Fidelity and Attunement. Fidelity refers to whether the
explanation includes accurate information about the recommended item.
Attunement evaluates whether the explanation reflects the target user's
preferences. By applying signal detection theory, we first determine decision
outcomes for each dimension and then combine them to calculate a sensitivity,
which serves as the final Veracity value. To assess the effectiveness of the
proposed metric, we set up four cases with varying levels of information
quality to validate whether our metric can accurately capture differences in
quality. The results provided meaningful insights into the effectiveness of our
proposed metric.

</details>


### [44] [DARTS: A Dual-View Attack Framework for Targeted Manipulation in Federated Sequential Recommendation](https://arxiv.org/abs/2507.01383)
*Qitao Qin,Yucong Luo,Zhibo Chu*

Main category: cs.IR

TL;DR: 论文聚焦联邦序列推荐（FSR）中的针对性攻击，提出DV - FSR攻击框架和对应防御机制，实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐（FedRec）系统针对性攻击研究忽视推荐模型差异鲁棒性，且现有攻击方法在FSR任务中效果有限。

Method: 提出结合基于采样的显式策略和基于对比学习的隐式梯度策略的DV - FSR攻击框架，并引入针对FSR中针对性攻击的防御机制。

Result: 广泛实验验证了所提方法在代表性序列模型上的有效性。

Conclusion: 所提攻击框架和防御机制能有效应对FSR中的针对性攻击，代码已公开。

Abstract: Federated recommendation (FedRec) preserves user privacy by enabling
decentralized training of personalized models, but this architecture is
inherently vulnerable to adversarial attacks. Significant research has been
conducted on targeted attacks in FedRec systems, motivated by commercial and
social influence considerations. However, much of this work has largely
overlooked the differential robustness of recommendation models. Moreover, our
empirical findings indicate that existing targeted attack methods achieve only
limited effectiveness in Federated Sequential Recommendation(FSR) tasks. Driven
by these observations, we focus on investigating targeted attacks in FSR and
propose a novel dualview attack framework, named DV-FSR. This attack method
uniquely combines a sampling-based explicit strategy with a contrastive
learning-based implicit gradient strategy to orchestrate a coordinated attack.
Additionally, we introduce a specific defense mechanism tailored for targeted
attacks in FSR, aiming to evaluate the mitigation effects of the attack method
we proposed. Extensive experiments validate the effectiveness of our proposed
approach on representative sequential models. Our codes are publicly available.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Few-Shot Inspired Generative Zero-Shot Learning](https://arxiv.org/abs/2507.01026)
*Md Shakil Ahamed Shohag,Q. M. Jonathan Wu,Farhad Pourpanah*

Main category: cs.LG

TL;DR: 提出FSIGenZ少样本生成式零样本学习框架，用更少合成特征取得有竞争力性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式零样本学习方法需大量计算资源和合成数据，违背零样本假设，且未考虑属性实例级变异性。

Method: 引入Model - Specific Attribute Scoring动态重新评分类属性；估计基于调整后属性得分的组级原型；采用Dual - Purpose Semantic Regularization策略训练语义感知对比分类器。

Result: 在SUN、AwA2和CUB基准测试中，FSIGenZ用更少合成特征取得有竞争力性能。

Conclusion: FSIGenZ框架有效，能减少对大规模特征合成的依赖。

Abstract: Generative zero-shot learning (ZSL) methods typically synthesize visual
features for unseen classes using predefined semantic attributes, followed by
training a fully supervised classification model. While effective, these
methods require substantial computational resources and extensive synthetic
data, thereby relaxing the original ZSL assumptions. In this paper, we propose
FSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on
large-scale feature synthesis. Our key insight is that class-level attributes
exhibit instance-level variability, i.e., some attributes may be absent or
partially visible, yet conventional ZSL methods treat them as uniformly
present. To address this, we introduce Model-Specific Attribute Scoring (MSAS),
which dynamically re-scores class attributes based on model-specific
optimization to approximate instance-level variability without access to unseen
data. We further estimate group-level prototypes as clusters of instances based
on MSAS-adjusted attribute scores, which serve as representative synthetic
features for each unseen class. To mitigate the resulting data imbalance, we
introduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training
a semantic-aware contrastive classifier (SCC) using these prototypes.
Experiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves
competitive performance using far fewer synthetic features.

</details>


### [46] [DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization](https://arxiv.org/abs/2507.01027)
*Zijian Ye,Wei Huang,Yifei Yu,Tianhe Ren,Zhongrui Wang,Xiaojuan Qi*

Main category: cs.LG

TL;DR: 提出DBellQuant框架解决大语言模型量化难题，实现低性能损失下的高效压缩，在数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临计算和内存挑战，现有量化方法因权重分布和激活异常值受限，需更好的量化方案。

Method: 引入DBellQuant框架，采用Learnable Transformation for Dual - Bell (LTDB)算法，将单钟形权重分布转为双钟形，进行逆变换平滑激活。

Result: 在Wikitext2数据集上，LLaMA2 - 13B 6位激活量化时，DBellQuant困惑度14.39，远超BiLLM无激活量化的21.35。

Conclusion: DBellQuant在积极的权重和激活量化下保持了优异性能，在压缩大语言模型用于实际应用方面潜力大。

Abstract: Large language models (LLMs) demonstrate remarkable performance but face
substantial computational and memory challenges that limit their practical
deployment. Quantization has emerged as a promising solution; however, its
effectiveness is often limited by quantization errors arising from weight
distributions that are not quantization-friendly and the presence of activation
outliers. To address these challenges, we introduce DBellQuant, an innovative
post-training quantization (PTQ) framework that achieves nearly 1-bit weight
compression and 6-bit activation quantization with minimal performance
degradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)
algorithm, which transforms single-bell weight distributions into dual-bell
forms to reduce binarization errors and applies inverse transformations to
smooth activations. DBellQuant sets a new state-of-the-art by preserving
superior model performance under aggressive weight and activation quantization.
For example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of
14.39 on LLaMA2-13B with 6-bit activation quantization, significantly
outperforming BiLLM's 21.35 without activation quantization, underscoring its
potential in compressing LLMs for real-world applications.

</details>


### [47] [Dual Perspectives on Non-Contrastive Self-Supervised Learning](https://arxiv.org/abs/2507.01028)
*Jean Ponce,Martial Hebert,Basile Terver*

Main category: cs.LG

TL;DR: 本文从优化和动力系统双理论视角研究自监督学习非对比方法中常用的防止表征坍缩的程序，证明其可避免坍缩，且相关动力系统极限点一般是渐近稳定平衡点。


<details>
  <summary>Details</summary>
Motivation: 研究自监督学习非对比方法中常用的停止梯度和指数移动平均迭代程序，从理论上解释其避免表征坍缩的原因。

Method: 从优化和动力系统两个理论视角进行研究，先证明一般情况下这些程序可避免坍缩，再从动力系统视角证明线性情况下不使用这些程序会导致坍缩，最后证明相关动力系统极限点是渐近稳定平衡点。

Result: 证明这些程序虽不优化原始目标或其他平滑函数，但可避免坍缩；线性情况下不使用这些程序会导致坍缩；相关动力系统极限点是渐近稳定平衡点。

Conclusion: 这些程序能有效避免表征坍缩，相关动力系统极限点无退化到平凡解的风险。

Abstract: The objective of non-contrastive approaches to self-supervised learning is to
train on pairs of different views of the data an encoder and a predictor that
minimize the mean discrepancy between the code predicted from the embedding of
the first view and the embedding of the second one. In this setting, the stop
gradient and exponential moving average iterative procedures are commonly used
to avoid representation collapse, with excellent performance in downstream
supervised applications. This presentation investigates these procedures from
the dual theoretical viewpoints of optimization and dynamical systems. We first
show that, in general, although they do not optimize the original objective, or
for that matter, any other smooth function, they do avoid collapse. Following
Tian et al. [2021], but without any of the extra assumptions used in their
proofs, we then show using a dynamical system perspective that, in the linear
case, minimizing the original objective function without the use of a stop
gradient or exponential moving average always leads to collapse. Conversely, we
finally show that the limit points of the dynamical systems associated with
these two procedures are, in general, asymptotically stable equilibria, with no
risk of degenerating to trivial solutions.

</details>


### [48] [PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning](https://arxiv.org/abs/2507.01029)
*Junjie Zhou,Yingli Zuo,Shichang Feng,Peng Wan,Qi Zhu,Daoqiang Zhang,Wei Shao*

Main category: cs.LG

TL;DR: 本文针对现有多模态大语言模型在病理视觉推理任务的局限，提出PathCoT方法，实验证明其在病理视觉理解和推理上有效。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型应用于病理视觉推理任务时，存在缺乏领域特定信息导致表现不佳和链式思维额外推理步骤引入错误的问题。

Method: 提出PathCoT零样本链式思维提示方法，将病理专家知识融入推理过程并加入自我评估环节。

Result: 在PathMMU数据集上的实验证明了方法的有效性。

Conclusion: PathCoT方法能有效用于病理视觉理解和推理。

Abstract: With the development of generative artificial intelligence and instruction
tuning techniques, multimodal large language models (MLLMs) have made
impressive progress on general reasoning tasks. Benefiting from the
chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning
problem step-by-step. However, existing MLLMs still face significant challenges
when applied to pathology visual reasoning tasks: (1) LLMs often underperforms
because they lack domain-specific information, which can lead to model
hallucinations. (2) The additional reasoning steps in CoT may introduce errors,
leading to the divergence of answers. To address these limitations, we propose
PathCoT, a novel zero-shot CoT prompting method which integrates the pathology
expert-knowledge into the reasoning process of MLLMs and incorporates
self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides
the MLLM with prior knowledge to perform as pathology experts, and provides
comprehensive analysis of the image with their domain-specific knowledge. By
incorporating the experts' knowledge, PathCoT can obtain the answers with CoT
reasoning. Furthermore, PathCoT incorporates a self-evaluation step that
assesses both the results generated directly by MLLMs and those derived through
CoT, finally determining the reliable answer. The experimental results on the
PathMMU dataset demonstrate the effectiveness of our method on pathology visual
understanding and reasoning.

</details>


### [49] [Fast Clifford Neural Layers](https://arxiv.org/abs/2507.01040)
*Tianxiang Xia,Max Neuwinger,Lin Xiao*

Main category: cs.LG

TL;DR: 论文聚焦优化2/3D Clifford卷积层和多向量激活层在单核CPU性能上的推理，测试显示实现比标准PyTorch实现快30%，并开源代码。


<details>
  <summary>Details</summary>
Motivation: 通过将Clifford代数引入神经网络改进PDE建模，聚焦优化相关层在单核CPU上的推理性能。

Method: 在涉及Clifford卷积层和多向量激活层的真实网络块上进行测试。

Result: 在相对大数据和网络规模（>L2缓存）下，实现比标准PyTorch实现快30%。

Conclusion: 可通过特定实现优化2/3D Clifford卷积层和多向量激活层在单核CPU上的推理性能，并开源代码供使用。

Abstract: Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra
into neural networks. In this project we focus on optimizing the inference of
2/3D Clifford convolutional layers and multivector activation layers for one
core CPU performance.
  Overall, by testing on a real network block involving Clifford convolutional
layers and multivector activation layers, we observe that our implementation is
30% faster than standard PyTorch implementation in relatively large data +
network size (>L2 cache).
  We open source our code base at
https://github.com/egretwAlker/c-opt-clifford-layers

</details>


### [50] [Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study](https://arxiv.org/abs/2507.01030)
*Reza Lotfi Navaei,Mohammad Safarzadeh,Seyed Mohammad Jafar Sobhani*

Main category: cs.LG

TL;DR: 本文用机器学习算法开发层流FGM库用于甲烷燃料燃烧模拟，经评估选MLP方法，调优后模型精度达99.81%。


<details>
  <summary>Details</summary>
Motivation: FGM实际应用需大量内存资源，旨在用机器学习算法开发层流FGM库用于甲烷燃料燃烧模拟。

Method: 采用多层感知器、随机森林、线性回归、支持向量机四种机器学习算法再生火焰面库，评估各方法默认架构选MLP，通过超参数调优提升精度。

Result: 确定7个适合构建训练数据库的库，误差率2.30%；选出MLP为主要方法；最优模型精度达99.81%。

Conclusion: 多层感知器方法经超参数调优可有效提高层流FGM库的精度，隐藏层和神经元数量对性能影响大。

Abstract: In chemistry tabulations and Flamelet combustion models, the Flamelet
Generated Manifold (FGM) is recognized for its precision and physical
representation. The practical implementation of FGM requires a significant
allocation of memory resources. FGM libraries are developed specifically for a
specific fuel and subsequently utilized for all numerical problems using
machine learning techniques. This research aims to develop libraries of Laminar
FGM utilizing machine learning algorithms for application in combustion
simulations of methane fuel. This study employs four Machine Learning
algorithms to regenerate Flamelet libraries, based on an understanding of data
sources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.
Random Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries
were identified as appropriate for constructing a database for training machine
learning models, giving an error rate of 2.30%. The default architectures of
each method were evaluated to determine the optimal approach, leading to the
selection of the MLP method as the primary choice. The method was enhanced
through hyperparameter tuning to improve accuracy. The quantity of hidden
layers and neurons significantly influences method performance. The optimal
model, comprising four hidden layers with 10, 15, 20, and 25 neurons
respectively, achieved an accuracy of 99.81%.

</details>


### [51] [Proof of a perfect platonic representation hypothesis](https://arxiv.org/abs/2507.01098)
*Liu Ziyin,Isaac Chuang*

Main category: cs.LG

TL;DR: 本文详细解释Ziyin等人（2025）对嵌入式深度线性网络模型（EDLN）的“完美”柏拉图表示假设（PRH）的证明，指出SGD训练的不同EDLN会达到完美柏拉图状态，还给出打破PRH的方法，揭示柏拉图表示与渐进锐化现象有共同成因，强调理解SGD训练中“熵力”的重要性。


<details>
  <summary>Details</summary>
Motivation: 详细解释Ziyin等人对EDLN的PRH的证明，并深入探讨相关现象成因，为表示学习提供理论参考。

Method: 对Ziyin等人的证明进行详细阐述和分析。

Result: 发现SGD训练的不同EDLN会达到完美柏拉图状态，给出至少六种打破PRH的方法，揭示柏拉图表示与渐进锐化有共同成因。

Conclusion: 强调理解SGD训练中“熵力”在表示学习中的重要性。

Abstract: In this note, we elaborate on and explain in detail the proof given by Ziyin
et al. (2025) of the "perfect" Platonic Representation Hypothesis (PRH) for the
embedded deep linear network model (EDLN). We show that if trained with SGD,
two EDLNs with different widths and depths and trained on different data will
become Perfectly Platonic, meaning that every possible pair of layers will
learn the same representation up to a rotation. Because most of the global
minima of the loss function are not Platonic, that SGD only finds the perfectly
Platonic solution is rather extraordinary. The proof also suggests at least six
ways the PRH can be broken. We also show that in the EDLN model, the emergence
of the Platonic representations is due to the same reason as the emergence of
progressive sharpening. This implies that these two seemingly unrelated
phenomena in deep learning can, surprisingly, have a common cause. Overall, the
theory and proof highlight the importance of understanding emergent "entropic
forces" due to the irreversibility of SGD training and their role in
representation learning. The goal of this note is to be instructive and avoid
lengthy technical details.

</details>


### [52] [Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals](https://arxiv.org/abs/2507.01052)
*Ahmed Farooq*

Main category: cs.LG

TL;DR: 本文基于稠密Hopfield网络框架，引入用于长序列记忆的能量泛函，提出时间核处理时间依赖，在存储和顺序检索电影帧上成功应用，对现代Transformer架构有应用价值。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在长上下文任务中的局限性，处理长序列记忆问题。

Method: 基于稠密Hopfield网络框架，引入新的能量泛函，提出时间核K(m, k)来纳入时间依赖。

Result: 成功应用于电影帧的存储和顺序检索。

Conclusion: 该模型为解决Transformer在长上下文任务中的局限提供了有前景的方法，在自然语言处理等领域有潜在影响。

Abstract: In this study we introduce a novel energy functional for long-sequence
memory, building upon the framework of dense Hopfield networks which achieves
exponential storage capacity through higher-order interactions. Building upon
earlier work on long-sequence Hopfield memory models, we propose a temporal
kernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient
sequential retrieval of patterns over extended sequences. We demonstrate the
successful application of this technique for the storage and sequential
retrieval of movies frames which are well suited for this because of the high
dimensional vectors that make up each frame creating enough variation between
even sequential frames in the high dimensional space. The technique has
applications in modern transformer architectures, including efficient
long-sequence modeling, memory augmentation, improved attention with temporal
bias, and enhanced handling of long-term dependencies in time-series data. Our
model offers a promising approach to address the limitations of transformers in
long-context tasks, with potential implications for natural language
processing, forecasting, and beyond.

</details>


### [53] [PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs](https://arxiv.org/abs/2507.01031)
*Fanchen Bu,Kijung Shin*

Main category: cs.LG

TL;DR: 介绍将基于PyTorch的几何学习框架移植到Gaudi - v2 HPUs的经验，提供实用工具、教程和示例，降低在非CUDA硬件上实验几何学习算法的门槛。


<details>
  <summary>Details</summary>
Motivation: 新兴加速器如Gaudi HPUs有竞争力，但使用非CUDA处理单元需大量工程工作和软件适配，希望降低在非CUDA硬件上进行几何学习研究的门槛。

Method: 将基于PyTorch的几何学习框架移植到Gaudi - v2 HPUs，引入核心实用工具恢复基本操作，整理十六个引导教程和十一个实际示例并进行诊断分析。

Result: 将经验收集到公开的GitHub仓库。

Conclusion: 研究成果为非CUDA硬件上的几何学习算法和模型实验提供基础，便于进一步优化和跨平台移植。

Abstract: Geometric learning has emerged as a powerful paradigm for modeling
non-Euclidean data, especially graph-structured ones, with applications
spanning social networks, molecular structures, knowledge graphs, and
recommender systems. While Nvidia's CUDA-enabled graphics processing units
(GPUs) largely dominate the hardware landscape, emerging accelerators such as
Intel's Gaudi Habana Processing Units (HPUs) offer competitive performance and
energy efficiency. However, the usage of such non-CUDA processing units
requires significant engineering effort and novel software adaptations. In this
work, we present our experiences porting PyTorch-based geometric learning
frameworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that
restore essential operations (e.g., scatter, sparse indexing, k-nearest
neighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and
eleven real-world examples with diagnostic analyses of encountered failures and
detailed workarounds. We collect all our experiences into a publicly accessible
GitHub repository. Our contributions lower the barrier for researchers to
experiment with geometric-learning algorithms and models on non-CUDA hardware,
providing a foundation for further optimization and cross-platform portability.

</details>


### [54] [An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks](https://arxiv.org/abs/2507.01032)
*Nan Mu,Hongbo Yang,Chen Zhao*

Main category: cs.LG

TL;DR: 提出不确定性感知多视图动态决策框架用于组学数据分类，减少测试成本并保持诊断性能。


<details>
  <summary>Details</summary>
Motivation: 高通量多组学技术成本高，过度依赖全组学数据会造成资源浪费，需在保证诊断准确性的同时降低测试成本。

Method: 单组学层面，优化神经网络激活函数生成狄利克雷分布参数，用主观逻辑量化分类结果的置信度和不确定性；多组学层面，用Dempster - Shafer理论融合异构模态，采用动态决策机制逐步引入组学数据。

Result: 在四个基准多组学数据集上评估，三个数据集超50%案例用单组学模态实现准确分类，减少冗余测试。

Conclusion: 该方法能降低测试成本，保持与全组学模型相当的诊断性能，保留重要生物学见解。

Abstract: Background and Objective: High-throughput multi-omics technologies have
proven invaluable for elucidating disease mechanisms and enabling early
diagnosis. However, the high cost of multi-omics profiling imposes a
significant economic burden, with over reliance on full omics data potentially
leading to unnecessary resource consumption. To address these issues, we
propose an uncertainty-aware, multi-view dynamic decision framework for omics
data classification that aims to achieve high diagnostic accuracy while
minimizing testing costs. Methodology: At the single-omics level, we refine the
activation functions of neural networks to generate Dirichlet distribution
parameters, utilizing subjective logic to quantify both the belief masses and
uncertainty mass of classification results. Belief mass reflects the support of
a specific omics modality for a disease class, while the uncertainty parameter
captures limitations in data quality and model discriminability, providing a
more trustworthy basis for decision-making. At the multi omics level, we employ
a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous
modalities, leveraging their complementarity to boost diagnostic accuracy and
robustness. A dynamic decision mechanism is then applied that omics data are
incrementally introduced for each patient until either all data sources are
utilized or the model confidence exceeds a predefined threshold, potentially
before all data sources are utilized. Results and Conclusion: We evaluate our
approach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.
In three datasets, over 50% of cases achieved accurate classification using a
single omics modality, effectively reducing redundant testing. Meanwhile, our
method maintains diagnostic performance comparable to full-omics models and
preserves essential biological insights.

</details>


### [55] [Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya](https://arxiv.org/abs/2507.01034)
*Asma Agaal,Mansour Essgaer,Hend M. Farkash,Zulaiha Ali Othman*

Main category: cs.LG

TL;DR: 本文提出数据驱动方法预测2025年班加西电力负载、发电及赤字，多种模型对比后LSTM表现最佳，其优化框架为政策制定者和电网运营商提供见解。


<details>
  <summary>Details</summary>
Motivation: 准确的电力预测对班加西的电网稳定和能源规划至关重要，当地存在频繁停电、发电不足和基础设施限制等问题。

Method: 使用2019年和2023年历史数据，应用多种时间序列模型，对数据集进行预处理，用多种误差指标评估性能。

Result: LSTM在预测中表现优于其他模型，优化的LSTM框架能整合温度和湿度等外生因素，在预测多个电力指标上有强大性能。

Conclusion: 研究结果为数据稀缺、不稳定地区的政策制定者和电网运营商进行主动负载管理和资源规划提供了实际见解。

Abstract: Accurate electricity forecasting is crucial for grid stability and energy
planning, especially in Benghazi, Libya, where frequent load shedding,
generation deficits, and infrastructure limitations persist. This study
proposes a data-driven approach to forecast electricity load, generation, and
deficits for 2025 using historical data from 2019 (a year marked by
instability) and 2023 (a more stable year). Multiple time series models were
applied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential
smoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural
networks. The dataset was enhanced through missing value imputation, outlier
smoothing, and log transformation. Performance was assessed using mean squared
error, root mean squared error, mean absolute error, and mean absolute
percentage error. LSTM outperformed all other models, showing strong
capabilities in modeling non-stationary and seasonal patterns. A key
contribution of this work is an optimized LSTM framework that integrates
exogenous factors such as temperature and humidity, offering robust performance
in forecasting multiple electricity indicators. These results provide practical
insights for policymakers and grid operators to enable proactive load
management and resource planning in data-scarce, volatile regions.

</details>


### [56] [Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services](https://arxiv.org/abs/2507.01067)
*Keun Soo Yim*

Main category: cs.LG

TL;DR: 本文优化基础模型预测高性能机器学习服务偶发性或尖峰式生产中断，对比经典随机模型评估误差，用最优参数模型估计一年故障统计，误差小于6%。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测的基础模型未用于预测罕见尖峰事件，本文旨在解决高性能机器学习服务偶发性或尖峰式生产中断的预测问题。

Method: 优化最先进的基础模型进行预测，将其与经典随机预测模型对比评估预测误差，找出关键模式。

Result: 识别出基础模型和随机模型各自能很好跟踪的目标数据关键模式，用最优参数模型估计特定根本原因的一年故障统计，误差小于6%。

Conclusion: 优化的基础模型可用于预测偶发性或尖峰式生产中断，且能以较低误差估计故障统计。

Abstract: Time series forecasting models have diverse real world applications (e.g.,
from electricity metrics to software workload). Latest foundational models
trained for time series forecasting show strengths (e.g., for long sequences
and in zero-shot settings). However, foundational model was not yet used for
forecasting rare, spiky events, i.e., a challenging target because those are a
corner case of extreme events. In this paper, we optimize a state-of-the-art
foundational model to forecast sporadic or spiky production outages of
high-performance machine learning services powering billions of client devices.
We evaluate the forecasting errors of the foundational model compared with
classical stochastic forecasting models (e.g., moving average and
autoregressive). The analysis helps us understand how each of the evaluated
models performs for the sporadic or spiky events. For example, it identifies
the key patterns in the target data that are well tracked by the foundational
model vs. each of the stochastic models. We use the models with optimal
parameters to estimate a year-long outage statistics of a particular root cause
with less than 6% value errors.

</details>


### [57] [Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs](https://arxiv.org/abs/2507.01457)
*Federico Nicolas Peccia,Frederik Haxel,Oliver Bringmann*

Main category: cs.LG

TL;DR: 本文提出基于TVM编译器的工作流，将RVV扩展集成到MetaSchedule框架，调优AI工作负载，相比GCC、muRISCV - NN和LLVM有性能提升且代码内存占用小，还开源供社区扩展。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高效部署复杂AI工作负载到RISC - V向量单元方面存在不足，缺乏智能自动调优框架与RISC - V RVV扩展的集成。

Method: 将RVV扩展集成到TVM的MetaSchedule框架，在FPGA上实现不同RISC - V SoC并调优AI工作负载。

Result: 相比GCC自动向量化功能执行延迟平均提升46%，相比muRISCV - NN提升29%，代码内存占用更小；在商用RISC - V SoC上比LLVM平均快35%。

Conclusion: 提出的方案能高效映射AI工作负载到RISC - V向量单元，开源后可让社区扩展到其他RISC - V扩展。

Abstract: RISC-V provides a flexible and scalable platform for applications ranging
from embedded devices to high-performance computing clusters. Particularly, its
RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI
workloads. But writing software that efficiently utilizes the vector units of
RISC-V CPUs without expert knowledge requires the programmer to rely on the
autovectorization features of compilers or hand-crafted libraries like
muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing
the integration with the RISC-V RVV extension, thus heavily limiting the
efficient deployment of complex AI workloads. In this paper, we present a
workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V
vector units. Instead of relying on hand-crafted libraries, we integrated the
RVV extension into TVM's MetaSchedule framework, a probabilistic program
framework for tensor operation tuning. We implemented different RISC-V SoCs on
an FPGA and tuned a wide range of AI workloads on them. We found that our
proposal shows a mean improvement of 46% in execution latency when compared
against the autovectorization feature of GCC, and 29% against muRISCV-NN.
Moreover, the binary resulting from our proposal has a smaller code memory
footprint, making it more suitable for embedded devices. Finally, we also
evaluated our solution on a commercially available RISC-V SoC implementing the
RVV 1.0 Vector Extension and found our solution is able to find mappings that
are 35% faster on average than the ones proposed by LLVM. We open-sourced our
proposal for the community to expand it to target other RISC-V extensions.

</details>


### [58] [Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2507.01035)
*Yushang Zhao,Haotian Lyu,Yike Peng,Aijia Sun,Feng Jiang,Xinyue Han*

Main category: cs.LG

TL;DR: 本文针对混合GNN-LLM推荐系统的计算瓶颈，采用多种优化策略和硬件加速，实验显著提升性能，建议FPGA和LoRA实时部署，为下一代推荐系统奠基。


<details>
  <summary>Details</summary>
Motivation: 在线服务发展要求高速高效推荐系统，需优化混合GNN-LLM推荐系统推理延迟和训练效率。

Method: 采用混合GNN-LLM集成架构，运用量化、LoRA、蒸馏等优化策略，结合FPGA、DeepSpeed硬件加速，在R 4.4.2环境下实验。

Result: 最优配置Hybrid + FPGA + DeepSpeed准确率提升13.6%，NDCG@10达0.75，延迟40 - 60ms；LoRA使训练时间减少66%。

Conclusion: 软硬件协同设计和参数高效调优使混合模型优于独立GNN或LLM方法，推荐FPGA和LoRA实时部署，未来可探索联邦学习和高级融合架构。

Abstract: The incessant advent of online services demands high speed and efficient
recommender systems (ReS) that can maintain real-time performance along with
processing very complex user-item interactions. The present study, therefore,
considers computational bottlenecks involved in hybrid Graph Neural Network
(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their
inference latency and training efficiency. An extensive methodology was used:
hybrid GNN-LLM integrated architecture-optimization strategies(quantization,
LoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.
Experimental improvements were significant, with the optimal Hybrid + FPGA +
DeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms
of latency, while LoRA brought down training time by 66% (3.8 hours) in
comparison to the non-optimized baseline. Irrespective of domain, such as
accuracy or efficiency, it can be established that hardware-software co-design
and parameter-efficient tuning permit hybrid models to outperform GNN or LLM
approaches implemented independently. It recommends the use of FPGA as well as
LoRA for real-time deployment. Future work should involve federated learning
along with advanced fusion architectures for better scalability and privacy
preservation. Thus, this research marks the fundamental groundwork concerning
next-generation ReS balancing low-latency response with cutting-edge
personalization.

</details>


### [59] [Provenance Tracking in Large-Scale Machine Learning Systems](https://arxiv.org/abs/2507.01075)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: 随着大规模AI模型需求增长，训练优化面临挑战，提出yProv4ML库助力解决。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型训练需平衡计算效率、执行时间、准确性和能耗，利用溯源数据可解决相关问题，需优化分布式资源利用。

Method: 引入yProv4ML库，以JSON格式收集溯源数据，符合W3C PROV和ProvML标准，支持通过插件集成额外数据收集工具，与yProv框架集成。

Result: 未提及明确实验结果。

Conclusion: yProv4ML库具有灵活性和可扩展性，能助力大规模AI模型训练的资源优化。

Abstract: As the demand for large scale AI models continues to grow, the optimization
of their training to balance computational efficiency, execution time, accuracy
and energy consumption represents a critical multidimensional challenge.
Achieving this balance requires not only innovative algorithmic techniques and
hardware architectures but also comprehensive tools for monitoring, analyzing,
and understanding the underlying processes involved in model training and
deployment. Provenance data information about the origins, context, and
transformations of data and processes has become a key component in this
pursuit. By leveraging provenance, researchers and engineers can gain insights
into resource usage patterns, identify inefficiencies, and ensure
reproducibility and accountability in AI development workflows. For this
reason, the question of how distributed resources can be optimally utilized to
scale large AI models in an energy efficient manner is a fundamental one. To
support this effort, we introduce the yProv4ML library, a tool designed to
collect provenance data in JSON format, compliant with the W3C PROV and ProvML
standards. yProv4ML focuses on flexibility and extensibility, and enables users
to integrate additional data collection tools via plugins. The library is fully
integrated with the yProv framework, allowing for higher level pairing in tasks
run also through workflow management systems.

</details>


### [60] [Learning to Segment for Vehicle Routing Problems](https://arxiv.org/abs/2507.01037)
*Wenbin Ouyang,Sirui Li,Yining Ma,Cathy Wu*

Main category: cs.LG

TL;DR: 本文针对迭代搜索启发式求解车辆路径问题时的冗余计算问题，提出FSTA分解技术和L2Seg框架加速求解，实验表明可加速7倍，NAR和AR协同表现最佳且框架灵活兼容。


<details>
  <summary>Details</summary>
Motivation: 迭代搜索启发式求解车辆路径问题时，大量解在迭代中保持稳定，导致冗余计算，尤其是大规模问题。

Method: 提出FSTA分解技术保留稳定解段、聚合节点；引入L2Seg神经框架区分稳定和不稳定部分，有三种变体及相应策略。

Result: L2Seg可将现有迭代求解器加速达7倍，NAR和AR协同表现最佳。

Conclusion: L2Seg是灵活框架，兼容多种求解器，支持多种车辆路径问题。

Abstract: Iterative search heuristics are widely recognized as state-of-the-art for
solving Vehicle Routing Problems (VRPs). In this work, we identify and exploit
a critical observation: within these solvers, a large portion of the solution
remains stable, i.e., unchanged across search iterations, causing redundant
computations, especially for large-scale VRPs with long subtours. To address
this, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)
decomposition technique to accelerate iterative solvers. Specifically, FSTA
preserves stable solution segments during the search, aggregates nodes within
each segment into fixed hypernodes, and focuses the search only on unstable
portions. Yet, a key challenge lies in identifying which segments should be
aggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),
a novel neural framework to intelligently differentiate potentially stable and
unstable portions for FSTA decomposition. We present three L2Seg variants:
non-autoregressive (globally comprehensive but locally indiscriminate),
autoregressive (locally refined but globally deficient), and their synergy,
with bespoke training and inference strategies. Empirical results on CVRP and
VRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up
to 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy
achieves best performance by combining their complementary strengths. Notably,
L2Seg is a flexible framework that is compatible with traditional,
learning-based, and hybrid solvers, while supporting a broad class of VRPs.

</details>


### [61] [yProv4ML: Effortless Provenance Tracking for Machine Learning Systems](https://arxiv.org/abs/2507.01078)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: 本文指出大语言模型开发缺乏透明度和严谨性，现有工具存在不足，提出yProv4ML框架以捕获机器学习过程的溯源信息。


<details>
  <summary>Details</summary>
Motivation: 大语言模型开发缺乏透明度和严谨性，难以提前确定超参数，现有工具捕获数据用专有格式且不重视谱系。

Method: 提出yProv4ML框架，以PROV - JSON格式捕获机器学习过程的溯源信息，且只需最小代码修改。

Result: 未提及

Conclusion: 未提及

Abstract: The rapid growth of interest in large language models (LLMs) reflects their
potential for flexibility and generalization, and attracted the attention of a
diverse range of researchers. However, the advent of these techniques has also
brought to light the lack of transparency and rigor with which development is
pursued. In particular, the inability to determine the number of epochs and
other hyperparameters in advance presents challenges in identifying the best
model. To address this challenge, machine learning frameworks such as MLFlow
can automate the collection of this type of information. However, these tools
capture data using proprietary formats and pose little attention to lineage.
This paper proposes yProv4ML, a framework to capture provenance information
generated during machine learning processes in PROV-JSON format, with minimal
code modifications.

</details>


### [62] [Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation](https://arxiv.org/abs/2507.01285)
*Aymen Rayane Khouas,Mohamed Reda Bouadjenek,Hakim Hacid,Sunil Aryal*

Main category: cs.LG

TL;DR: 文章提出Dist - FedAvg聚合方法，用于图联邦学习，在多数据集上评估显示其优于基线技术，提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 现有图联邦推荐系统聚合方法忽略用户嵌入特性、用户相似性及用户交互变化，需改进。

Method: 提出Dist - FedAvg，为相似嵌入用户分配更高聚合权重，确保锚点用户在本地更新中有重要影响。

Result: 在多个数据集上的实证评估表明，Dist - FedAvg始终优于基线聚合技术。

Conclusion: Dist - FedAvg能提高个性化和聚合效率，提升推荐准确性，且可无缝集成到现有联邦学习框架。

Abstract: Graph federated recommendation systems offer a privacy-preserving alternative
to traditional centralized recommendation architectures, which often raise
concerns about data security. While federated learning enables personalized
recommendations without exposing raw user data, existing aggregation methods
overlook the unique properties of user embeddings in this setting. Indeed,
traditional aggregation methods fail to account for their complexity and the
critical role of user similarity in recommendation effectiveness. Moreover,
evolving user interactions require adaptive aggregation while preserving the
influence of high-relevance anchor users (the primary users before expansion in
graph-based frameworks). To address these limitations, we introduce
Dist-FedAvg, a novel distance-based aggregation method designed to enhance
personalization and aggregation efficiency in graph federated learning. Our
method assigns higher aggregation weights to users with similar embeddings,
while ensuring that anchor users retain significant influence in local updates.
Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg
consistently outperforms baseline aggregation techniques, improving
recommendation accuracy while maintaining seamless integration into existing
federated learning frameworks.

</details>


### [63] [On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization](https://arxiv.org/abs/2507.01039)
*Kaaustaaub Shankar,Wilhelm Louw,Kelly Cohen*

Main category: cs.LG

TL;DR: 提出用PPO训练神经模糊控制器的强化学习方法，在CartPole - v1环境评估，效果优于ANFIS - DQN基线。


<details>
  <summary>Details</summary>
Motivation: 改进先前用Deep Q - Learning训练自适应神经模糊推理系统（ANFIS）的方法，用更稳定的策略。

Method: 用近端策略优化（PPO）的强化学习方法训练神经模糊控制器，替换原有的基于离策略值的框架，采用稳定的在策略演员 - 评论员循环。

Result: PPO训练的模糊智能体在20000次更新后，在CartPole - v1环境平均回报达500 +/- 0，训练时方差更小、收敛更快。

Conclusion: PPO为强化学习任务中训练可解释的神经模糊控制器提供了有前景的途径。

Abstract: We propose a reinforcement learning (RL) approach for training neuro-fuzzy
controllers using Proximal Policy Optimization (PPO). Building on prior work
that applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),
our method replaces the off-policy value-based framework with a stable
on-policy actor-critic loop. We evaluate this approach in the CartPole-v1
environment using multiple random seeds and compare its learning performance
against ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained
fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000
updates, showcasing less variance than prior DQN-based methods during training
and overall faster convergence. These findings suggest that PPO offers a
promising pathway for training explainable neuro-fuzzy controllers in
reinforcement learning tasks.

</details>


### [64] [Enhanced Generative Model Evaluation with Clipped Density and Coverage](https://arxiv.org/abs/2507.01761)
*Nicolas Salvy,Hugues Talbot,Bertrand Thirion*

Main category: cs.LG

TL;DR: 引入Clipped Density和Clipped Coverage指标解决生成模型样本质量评估问题，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型难以可靠评估样本质量，当前质量指标缺乏可靠、可解释的值。

Method: 引入Clipped Density和Clipped Coverage指标，通过裁剪样本贡献和最近邻球半径避免离群样本影响。

Result: 指标在样本质量变差时分数线性下降，可直接解释为优质样本的等效比例。

Conclusion: 在合成和真实数据集上实验表明，新指标在评估生成模型时，鲁棒性、敏感性和可解释性优于现有方法。

Abstract: Although generative models have made remarkable progress in recent years,
their use in critical applications has been hindered by their incapacity to
reliably evaluate sample quality. Quality refers to at least two complementary
concepts: fidelity and coverage. Current quality metrics often lack reliable,
interpretable values due to an absence of calibration or insufficient
robustness to outliers. To address these shortcomings, we introduce two novel
metrics, Clipped Density and Clipped Coverage. By clipping individual sample
contributions and, for fidelity, the radii of nearest neighbor balls, our
metrics prevent out-of-distribution samples from biasing the aggregated values.
Through analytical and empirical calibration, these metrics exhibit linear
score degradation as the proportion of poor samples increases. Thus, they can
be straightforwardly interpreted as equivalent proportions of good samples.
Extensive experiments on synthetic and real-world datasets demonstrate that
Clipped Density and Clipped Coverage outperform existing methods in terms of
robustness, sensitivity, and interpretability for evaluating generative models.

</details>


### [65] [Fast AI Model Splitting over Edge Networks](https://arxiv.org/abs/2507.01041)
*Zuguang Li,Wen Wu,Shaohua Wu,Songge Zhang,Ye Wang,Xuemin,Shen*

Main category: cs.LG

TL;DR: 本文提出基于DAG的模型拆分算法和分块模型拆分算法解决最优模型拆分问题，实验表明算法能毫秒级确定最优拆分并减少训练延迟。


<details>
  <summary>Details</summary>
Motivation: 复杂AI模型架构在拆分学习中获取最优模型拆分计算复杂度高。

Method: 将AI模型表示为DAG，把最优模型拆分问题转化为最小s - t割搜索问题，提出基于DAG的模型拆分算法和分块模型拆分算法。

Result: 理论分析表明算法最优，实验结果显示算法能在毫秒内确定最优模型拆分，在动态边缘网络中比现有基准减少24.62% - 38.95%的训练延迟。

Conclusion: 所提算法能有效解决最优模型拆分问题，降低计算复杂度和训练延迟。

Abstract: Split learning (SL) has emerged as a computationally efficient approach for
artificial intelligence (AI) model training, which can alleviate device-side
computational workloads. However, complex AI model architectures pose high
computational complexity to obtain the optimal model splitting. In this paper,
we represent an arbitrary AI model as a directed acyclic graph (DAG), and then
reformulate the optimal model splitting problem as a minimum s-t cut search
problem. To solve the problem, we propose a fast DAG-based model splitting
algorithm, which restructures the DAG to enable the optimal model splitting
identification via a maximum flow method. Theoretical analysis indicates that
the proposed algorithm is optimal. Furthermore, considering AI models with
block structures, we propose a block-wise model splitting algorithm to reduce
computational complexity. The algorithm abstracts each block, i.e., a component
consisting of multiple layers, into a single vertex, thereby obtaining the
optimal model splitting via a simplified DAG. Extensive experimental results
demonstrate that the proposed algorithms can determine the optimal model
splitting within milliseconds, as well as reduce training delay by
24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art
benchmarks.

</details>


### [66] [LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs](https://arxiv.org/abs/2507.01806)
*Reza Arabpour,Haitz Sáez de Ocáriz Borde,Anastasis Kratsios*

Main category: cs.LG

TL;DR: 提出适用于有限计算资源用户（尤其是仅用笔记本CPU）的LoRA微调方法，通过组合现有LoRA构建适配器，虽性能不如GPU训练但优于基础模型。


<details>
  <summary>Details</summary>
Motivation: LoRA虽能实现参数高效更新，但广泛应用受限于基于GPU的训练，需为计算资源有限用户提供解决方案。

Method: 学习一个元算子，将输入数据集映射到一组LoRA权重，通过在CPU上对现有LoRA进行轻量级组合来构建适配器。

Result: 得到的适配器性能不如GPU训练的，但在下游任务中始终优于基础Mistral模型。

Conclusion: 该方法为传统基于GPU的微调提供了实用且易获取的替代方案。

Abstract: Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.

</details>


### [67] [Data Classification with Dynamically Growing and Shrinking Neural Networks](https://arxiv.org/abs/2507.01043)
*Szymon Świderski,Agnieszka Jastrzębska*

Main category: cs.LG

TL;DR: 提出一种通过蒙特卡罗树搜索实现神经网络动态增减的新方法，在视觉和时间序列数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动的神经网络模型构建问题，不仅训练权重，还要找出最优模型架构。

Method: 利用蒙特卡罗树搜索程序作为架构设计的决策机制，在训练时动态增减模型。

Result: 在视觉和时间序列数据集验证，在多变量时间序列分类中特别有效，实验评估表现良好。

Conclusion: 该方法具有鲁棒性和适应性。

Abstract: The issue of data-driven neural network model construction is one of the core
problems in the domain of Artificial Intelligence. A standard approach assumes
a fixed architecture with trainable weights. A conceptually more advanced
assumption is that we not only train the weights, but also find out the optimal
model architecture. We present a new method that realizes just that. This
article is an extended version of our conference paper titled "Dynamic Growing
and Shrinking of Neural Networks with Monte Carlo Tree Search [26]". In the
paper, we show in detail how to create a neural network with a procedure that
allows dynamic shrinking and growing of the model while it is being trained.
The decision-making mechanism for the architectural design is governed by a
Monte Carlo tree search procedure which simulates network behavior and allows
to compare several candidate architecture changes to choose the best one. The
proposed method was validated using both visual and time series datasets,
demonstrating its particular effectiveness in multivariate time series
classification. This is attributed to the architecture's ability to adapt
dynamically, allowing independent modifications for each time series. The
approach is supplemented by Python source code for reproducibility.
Experimental evaluations in visual pattern and multivariate time series
classification tasks revealed highly promising performance, underscoring the
method's robustness and adaptability.

</details>


### [68] [Out-of-Distribution Detection Methods Answer the Wrong Questions](https://arxiv.org/abs/2507.01831)
*Yucen Lily Li,Daohan Lu,Polina Kirichenko,Shikai Qiu,Tim G. J. Rudner,C. Bayan Bruss,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 文章重新审视流行的OOD检测方法，指出其存在根本问题，改进干预措施也无法解决，其他方法也有局限。


<details>
  <summary>Details</summary>
Motivation: 为了检测分布偏移和提高模型安全性，重新审视基于预测不确定性或特征的OOD检测方法。

Method: 对现有OOD检测方法进行批判性分析，举例说明问题，分析改进干预措施的效果，考虑其他方法并指出其局限。

Result: 基于不确定性的方法误将高不确定性等同于OOD，基于特征的方法误将特征空间远距离等同于OOD，改进干预措施无法解决目标不一致问题，其他方法有自身局限。

Conclusion: 现有的OOD检测方法存在根本的目标不一致问题，难以简单修复，其他考虑的方法也有局限性。

Abstract: To detect distribution shifts and improve model safety, many
out-of-distribution (OOD) detection methods rely on the predictive uncertainty
or features of supervised models trained on in-distribution data. In this
paper, we critically re-examine this popular family of OOD detection
procedures, and we argue that these methods are fundamentally answering the
wrong questions for OOD detection. There is no simple fix to this misalignment,
since a classifier trained only on in-distribution classes cannot be expected
to identify OOD points; for instance, a cat-dog classifier may confidently
misclassify an airplane if it contains features that distinguish cats from
dogs, despite generally appearing nothing alike. We find that uncertainty-based
methods incorrectly conflate high uncertainty with being OOD, while
feature-based methods incorrectly conflate far feature-space distance with
being OOD. We show how these pathologies manifest as irreducible errors in OOD
detection and identify common settings where these methods are ineffective.
Additionally, interventions to improve OOD detection such as feature-logit
hybrid methods, scaling of model and data size, epistemic uncertainty
representation, and outlier exposure also fail to address this fundamental
misalignment in objectives. We additionally consider unsupervised density
estimation and generative models for OOD detection, which we show have their
own fundamental limitations.

</details>


### [69] [Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals](https://arxiv.org/abs/2507.01045)
*Xiao Gu,Wei Tang,Jinpei Han,Veer Sangha,Fenglin Liu,Shreyank N Gowda,Antonio H. Ribeiro,Patrick Schwab,Kim Branson,Lei Clifton,Antonio Luiz P. Ribeiro,Zhangdaihong Liu,David A. Clifton*

Main category: cs.LG

TL;DR: 本文提出心脏传感基础模型CSFM，在多数据集预训练，能跨多种场景特征提取和迁移学习，表现优于传统方法，可用于全面心脏监测。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习分析心脏生物信号方法依赖同质数据集和静态定制模型，鲁棒性和泛化性受限。

Method: 采用先进Transformer架构和生成式掩码预训练策略，在多模态集成的多个大规模数据集上预训练CSFM。

Result: CSFM嵌入可作为特征提取器，支持迁移学习，在多项任务评估中表现优于传统方法，在多种心电配置和信号场景下性能稳健。

Conclusion: CSFM有潜力成为全面心脏监测的通用可扩展解决方案。

Abstract: Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms
(PPG), are of paramount importance for the diagnosis, prevention, and
management of cardiovascular diseases, and have been extensively used in a
variety of clinical tasks. Conventional deep learning approaches for analyzing
these signals typically rely on homogeneous datasets and static bespoke models,
limiting their robustness and generalizability across diverse clinical settings
and acquisition protocols. In this study, we present a cardiac sensing
foundation model (CSFM) that leverages advanced transformer architectures and a
generative, masked pretraining strategy to learn unified representations from
vast, heterogeneous health records. Our model is pretrained on an innovative
multi-modal integration of data from multiple large-scale datasets (including
MIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the
corresponding clinical or machine-generated text reports from approximately 1.7
million individuals. We demonstrate that the embeddings derived from our CSFM
not only serve as effective feature extractors across diverse cardiac sensing
scenarios, but also enable seamless transfer learning across varying input
configurations and sensor modalities. Extensive evaluations across diagnostic
tasks, demographic information recognition, vital sign measurement, clinical
outcome prediction, and ECG question answering reveal that CSFM consistently
outperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits
robust performance across multiple ECG lead configurations from standard
12-lead systems to single-lead setups, and in scenarios where only ECG, only
PPG, or a combination thereof is available. These findings highlight the
potential of CSFM as a versatile and scalable solution, for comprehensive
cardiac monitoring.

</details>


### [70] [Variational Digital Twins](https://arxiv.org/abs/2507.01047)
*Logan A. Burnett,Umme Mahbuba Nabila,Majdi I. Radaideh*

Main category: cs.LG

TL;DR: 本文提出变分数字孪生（VDT）框架解决现有数字孪生研究问题，经四个能源领域问题评估，证明VDT能使传统替代模型具备不确定性感知等特性。


<details>
  <summary>Details</summary>
Motivation: 现有数字孪生文献存在模型与资产信息交换框架不清晰、缺乏实时实施关键特性、对模型不确定性关注不足等问题。

Method: 提出变分数字孪生（VDT）框架，用单个贝叶斯输出层增强标准神经架构，并采用新型VDT更新算法。

Result: 在四个能源领域问题评估中表现良好，如临界热通量预测减少实验次数和训练时间、可再生能源发电孪生保持高R2值等。

Conclusion: 适度贝叶斯增强与高效更新方案结合，能将传统替代模型转变为具有不确定性感知、数据高效和计算可行的数字孪生，为能源系统可靠模型发展铺平道路。

Abstract: While digital twins (DT) hold promise for providing real-time insights into
complex energy assets, much of the current literature either does not offer a
clear framework for information exchange between the model and the asset, lacks
key features needed for real-time implementation, or gives limited attention to
model uncertainty. Here, we aim to solve these gaps by proposing a variational
digital twin (VDT) framework that augments standard neural architectures with a
single Bayesian output layer. This lightweight addition, along with a novel VDT
updating algorithm, lets a twin update in seconds on commodity GPUs while
producing calibrated uncertainty bounds that can inform experiment design,
control algorithms, and model reliability. The VDT is evaluated on four
energy-sector problems. For critical-heat-flux prediction, uncertainty-driven
active learning reaches R2 = 0.98 using 47 % fewer experiments and one-third
the training time of random sampling. A three-year renewable-generation twin
maintains R2 > 0.95 for solar output and curbs error growth for volatile wind
forecasts via monthly updates that process only one month of data at a time. A
nuclear reactor transient cooldown twin reconstructs thermocouple signals with
R2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating
robustness to degraded instrumentation. Finally, a physics-informed Li-ion
battery twin, retrained after every ten discharges, lowers voltage mean-squared
error by an order of magnitude relative to the best static model while adapting
its credible intervals as the cell approaches end-of-life. These results
demonstrate that combining modest Bayesian augmentation with efficient update
schemes turns conventional surrogates into uncertainty-aware, data-efficient,
and computationally tractable DTs, paving the way for dependable models across
industrial and scientific energy systems.

</details>


### [71] [3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells](https://arxiv.org/abs/2507.01048)
*Ricardo Emanuel Vaz Vargas,Afrânio José de Melo Junior,Celso José Munaro,Cláudio Benevenuto de Campos Lima,Eduardo Toledo de Lima Junior,Felipe Muntzberg Barrocas,Flávio Miguel Varejão,Guilherme Fidelis Peixer,Igor de Melo Nery Oliveira,Jader Riso Barbosa Jr.,Jaime Andrés Lozano Cadena,Jean Carlos Dias de Araújo,João Neuenschwander Escosteguy Carneiro,Lucas Gouveia Omena Lopes,Lucas Pereira de Gouveia,Mateus de Araujo Fernandes,Matheus Lima Scramignon,Patrick Marques Ciarelli,Rodrigo Castello Branco,Rogério Leite Alves Pinto*

Main category: cs.LG

TL;DR: 文章介绍石油行业油井不良事件危害，提及Petrobras开发并公开3W数据集，阐述当前版本含结构修改和额外标注数据，鼓励改进结果和开发新方法。


<details>
  <summary>Details</summary>
Motivation: 石油行业油井不良事件会造成经济损失、环境事故和人员伤亡，缺乏相关公共数据集。

Method: 无明确提及。

Result: Petrobras开发并公开3W数据集，当前版本有结构修改和额外标注数据。

Conclusion: 鼓励3W社区和新用户改进已有结果，开发新方法、数字产品和服务以检测油井不良事件。

Abstract: In the oil industry, undesirable events in oil wells can cause economic
losses, environmental accidents, and human casualties. Solutions based on
Artificial Intelligence and Machine Learning for Early Detection of such events
have proven valuable for diverse applications across industries. In 2019,
recognizing the importance and the lack of public datasets related to
undesirable events in oil wells, Petrobras developed and publicly released the
first version of the 3W Dataset, which is essentially a set of Multivariate
Time Series labeled by experts. Since then, the 3W Dataset has been developed
collaboratively and has become a foundational reference for numerous works in
the field. This data article describes the current publicly available version
of the 3W Dataset, which contains structural modifications and additional
labeled data. The detailed description provided encourages and supports the 3W
community and new 3W users to improve previous published results and to develop
new robust methodologies, digital products and services capable of detecting
undesirable events in oil wells with enough anticipation to enable corrective
or mitigating actions.

</details>


### [72] [Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization](https://arxiv.org/abs/2507.01050)
*Jing Yu,Yibo Zhao,Jiapeng Zhu,Wenming Shao,Bo Pang,Zhao Zhang,Xiang Li*

Main category: cs.LG

TL;DR: 本文提出两阶段训练框架处理社交媒体文本解毒问题，实验表明该方法效果好、泛化能力强且减少对标注数据依赖。


<details>
  <summary>Details</summary>
Motivation: 社交媒体有毒内容传播威胁线上环境和公共话语，现有解毒方法难兼顾性能、语义保留和数据鲁棒性，且数据效率低。

Method: 提出两阶段训练框架，先在少量高质量过滤平行数据上监督微调，再用无标签有毒输入和自定义奖励模型通过组相对策略优化训练大语言模型。

Result: 有效缓解先前方法面临的权衡问题，实现了最先进性能，提高泛化能力，显著降低对标注数据的依赖。

Conclusion: 所提两阶段训练框架在文本解毒任务中表现优秀，代码开源。

Abstract: The widespread dissemination of toxic content on social media poses a serious
threat to both online environments and public discourse, highlighting the
urgent need for detoxification methods that effectively remove toxicity while
preserving the original semantics. However, existing approaches often struggle
to simultaneously achieve strong detoxification performance, semantic
preservation, and robustness to out-of-distribution data. Moreover, they
typically rely on costly, manually annotated parallel corpora while showing
poor data efficiency. To address these challenges, we propose a two-stage
training framework that jointly optimizes for data efficiency, semantic
preservation, and model generalization. We first perform supervised fine-tuning
on a small set of high-quality, filtered parallel data to establish a strong
initialization. Then, we leverage unlabeled toxic inputs and a custom-designed
reward model to train the LLM using Group Relative Policy Optimization.
Experimental results demonstrate that our method effectively mitigates the
trade-offs faced by previous work, achieving state-of-the-art performance with
improved generalization and significantly reduced dependence on annotated data.
Our code is available at:
https://anonymous.4open.science/r/Detoxification-of-Text-725F/

</details>


### [73] [XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science](https://arxiv.org/abs/2507.01054)
*Jithendaraa Subramanian,Linda Hung,Daniel Schweigert,Santosh Suram,Weike Ye*

Main category: cs.LG

TL;DR: 提出可扩展多模态框架，直接从元素组成和XRD学习，无需晶体结构输入，通过自监督预训练提升性能，为材料科学无结构基础模型提供路径。


<details>
  <summary>Details</summary>
Motivation: 现有的基于结构的模型在原子结构未知或难获取的现实应用中不实用，需要新方法。

Method: 提出多模态框架，集成特定模态编码器和交叉注意力融合模块，在Alexandria数据集上训练，采用MXM和对比对齐作为自监督预训练策略。

Result: 预训练加速收敛（最高4.2倍），提高准确性和表征质量，多模态性能随数据集增大比单模态基线更优。

Conclusion: 为材料科学建立无结构、基于实验的基础模型提供了途径。

Abstract: Recent advances in materials discovery have been driven by structure-based
models, particularly those using crystal graphs. While effective for
computational datasets, these models are impractical for real-world
applications where atomic structures are often unknown or difficult to obtain.
We propose a scalable multimodal framework that learns directly from elemental
composition and X-ray diffraction (XRD) -- two of the more available modalities
in experimental workflows without requiring crystal structure input. Our
architecture integrates modality-specific encoders with a cross-attention
fusion module and is trained on the 5-million-sample Alexandria dataset. We
present masked XRD modeling (MXM), and apply MXM and contrastive alignment as
self-supervised pretraining strategies. Pretraining yields faster convergence
(up to 4.2x speedup) and improves both accuracy and representation quality. We
further demonstrate that multimodal performance scales more favorably with
dataset size than unimodal baselines, with gains compounding at larger data
regimes. Our results establish a path toward structure-free, experimentally
grounded foundation models for materials science.

</details>


### [74] [Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI](https://arxiv.org/abs/2507.01056)
*Lidan Peng,Lu Gao,Feng Hong,Jingran Sun*

Main category: cs.LG

TL;DR: 研究利用20年路面状况和洪水数据，分析洪水对路面粗糙度影响，发现受洪水影响路面粗糙度增加更快，强调需采取减灾策略。


<details>
  <summary>Details</summary>
Motivation: 洪水会对路面基础设施造成显著损害，研究洪水事件对路面劣化的影响。

Method: 利用20年路面状况数据和洪水事件数据，进行统计分析对比洪水前后IRI值并计算劣化率，应用XAI技术评估洪水对路面性能的影响。

Result: 受洪水影响的路面粗糙度比未受洪水影响的路段增加更快。

Conclusion: 需要采取主动的洪水减灾策略，如改善排水系统、使用抗洪材料和预防性维护，以提高脆弱地区路面的韧性。

Abstract: Flooding can damage pavement infrastructure significantly, causing both
immediate and long-term structural and functional issues. This research
investigates how flooding events affect pavement deterioration, specifically
focusing on measuring pavement roughness by the International Roughness Index
(IRI). To quantify these effects, we utilized 20 years of pavement condition
data from TxDOT's PMIS database, which is integrated with flood event data,
including duration and spatial extent. Statistical analyses were performed to
compare IRI values before and after flooding and to calculate the deterioration
rates influenced by flood exposure. Moreover, we applied Explainable Artificial
Intelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and
Local Interpretable Model-Agnostic Explanations (LIME), to assess the impact of
flooding on pavement performance. The results demonstrate that flood-affected
pavements experience a more rapid increase in roughness compared to non-flooded
sections. These findings emphasize the need for proactive flood mitigation
strategies, including improved drainage systems, flood-resistant materials, and
preventative maintenance, to enhance pavement resilience in vulnerable regions.

</details>


### [75] [Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates](https://arxiv.org/abs/2507.01057)
*Lushun Fan,Yuqin Xia,Jun Li,Karl Jenkins*

Main category: cs.LG

TL;DR: 提出基于深度卷积神经网络的网格质量智能优化系统，利用Loop2Net生成器和损失函数实现网格生成与优化。


<details>
  <summary>Details</summary>
Motivation: 实现网格生成和优化。

Method: 采用深度卷积神经网络架构，以Loop2Net生成器根据给定机翼坐标预测网格，通过两个关键损失函数优化模型性能，并添加惩罚项。

Result: 达到了网格生成的目标。

Conclusion: 所提出的创新智能优化系统可用于网格生成和优化。

Abstract: In this study, an innovative intelligent optimization system for mesh quality
is proposed, which is based on a deep convolutional neural network
architecture, to achieve mesh generation and optimization. The core of the
study is the Loop2Net generator and loss function, it predicts the mesh based
on the given wing coordinates. And the model's performance is continuously
optimised by two key loss functions during the training. Then discipline by
adding penalties, the goal of mesh generation was finally reached.

</details>


### [76] [Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors](https://arxiv.org/abs/2507.01068)
*Biplov Paneru*

Main category: cs.LG

TL;DR: 利用IMU数据集开发可解释AI方法用于帕金森病步态冻结（FOG）的早期检测和预测，堆叠集成模型表现优，SHAP分析找出关键因素，采用联邦学习框架增强预测能力。


<details>
  <summary>Details</summary>
Motivation: 开发可解释AI方法用于帕金森病FOG的早期检测和预测。

Method: 使用CatBoost、XGBoost和Extra Trees分类器等机器学习模型进行FOG事件分类，构建堆叠集成模型；进行SHAP可解释性分析；采用联邦学习，使用混合Conv1D + LSTM架构。

Result: 堆叠集成模型性能优越，分类准确率近99%；SHAP分析显示时间（秒）是区分步态模式的最有影响力因素。

Conclusion: 所提出的FOG预测框架，结合多种模型和联邦学习方法，能有效用于FOG的早期检测和预测。

Abstract: This study leverages an Inertial Measurement Unit (IMU) dataset to develop
explainable AI methods for the early detection and prediction of Freezing of
Gait (FOG), a common symptom in Parkinson's disease. Machine learning models,
including CatBoost, XGBoost, and Extra Trees classifiers, are employed to
accurately categorize FOG episodes based on relevant clinical features. A
Stacking Ensemble model achieves superior performance, surpassing a hybrid
bidirectional GRU model and reaching nearly 99% classification accuracy. SHAP
interpretability analysis reveals that time (seconds) is the most influential
factor in distinguishing gait patterns. Additionally, the proposed FOG
prediction framework incorporates federated learning, where models are trained
locally on individual devices and aggregated on a central server using a
federated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for
enhanced predictive capability.

</details>


### [77] [Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs](https://arxiv.org/abs/2507.01073)
*Dian Jin*

Main category: cs.LG

TL;DR: 本文提出基于旋转采样的3D编码模块处理分子3D信息，在数据集上表现优于现有方法，计算复杂度低且可解释性强。


<details>
  <summary>Details</summary>
Motivation: 传统图表示难以有效编码分子3D空间结构，现有方法存在缺乏泛化性或计算成本高的问题。

Method: 提出基于旋转采样的3D编码模块，通过计算SO(3)旋转组的期望实现近似旋转不变性，引入后对齐策略实现严格不变性。

Result: 在QM9和C10数据集上，相比现有方法有更优的预测准确性、鲁棒性和泛化性能，且计算复杂度低、可解释性强。

Conclusion: 该方法为药物发现和材料设计中高效处理3D分子信息提供了有前景的方向。

Abstract: Graph neural networks (GNNs) have achieved remarkable success in molecular
property prediction. However, traditional graph representations struggle to
effectively encode the inherent 3D spatial structures of molecules, as
molecular orientations in 3D space introduce significant variability, severely
limiting model generalization and robustness. Existing approaches primarily
focus on rotation-invariant and rotation-equivariant methods. Invariant methods
often rely heavily on prior knowledge and lack sufficient generalizability,
while equivariant methods suffer from high computational costs. To address
these limitations, this paper proposes a novel plug-and-play 3D encoding module
leveraging rotational sampling. By computing the expectation over the SO(3)
rotational group, the method naturally achieves approximate rotational
invariance. Furthermore, by introducing a carefully designed post-alignment
strategy, strict invariance can be achieved without compromising performance.
Experimental evaluations on the QM9 and C10 Datasets demonstrate superior
predictive accuracy, robustness, and generalization performance compared to
existing methods. Moreover, the proposed approach maintains low computational
complexity and enhanced interpretability, providing a promising direction for
efficient and effective handling of 3D molecular information in drug discovery
and material design.

</details>


### [78] [Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels](https://arxiv.org/abs/2507.01077)
*Bogdan Bogdan,Arina Cazacu,Laura Vasilie*

Main category: cs.LG

TL;DR: 提出用仅解码器的大语言模型检测ECU通信日志中的异常，有新架构、处理标签不一致方法和适应不同用例等特点，提高检测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法在汽车通信系统等专业领域效果有限，缺乏针对ECU通信的LLM且地面真值数据不一致。

Method: 从UDP通信日志学习，将异常检测定义为识别与正常行为的时间偏差，引入熵正则化技术。

Result: 提出仅解码器异常检测架构、处理不一致标签的方法和适用于不同ECU通信用例的LLM。

Conclusion: 该方法能解决手动标注成本高、易出错问题，构建可扩展系统，从少量示例学习并提高复杂通信环境下检测准确性。

Abstract: Anomaly detection often relies on supervised or clustering approaches, with
limited success in specialized domains like automotive communication systems
where scalable solutions are essential. We propose a novel decoder-only Large
Language Model (LLM) to detect anomalies in Electronic Control Unit (ECU)
communication logs. Our approach addresses two key challenges: the lack of LLMs
tailored for ECU communication and the complexity of inconsistent ground truth
data. By learning from UDP communication logs, we formulate anomaly detection
simply as identifying deviations in time from normal behavior. We introduce an
entropy regularization technique that increases model's uncertainty in known
anomalies while maintaining consistency in similar scenarios. Our solution
offers three novelties: a decoder-only anomaly detection architecture, a way to
handle inconsistent labeling, and an adaptable LLM for different ECU
communication use cases. By leveraging the generative capabilities of
decoder-only models, we present a new technique that addresses the high cost
and error-prone nature of manual labeling through a more scalable system that
is able to learn from a minimal set of examples, while improving detection
accuracy in complex communication environments.

</details>


### [79] [Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept](https://arxiv.org/abs/2507.01080)
*Edouard Lansiaux,Ramy Azzouz,Emmanuel Chazard,Amélie Vromant,Eric Wiel*

Main category: cs.LG

TL;DR: 研究对比三种AI模型在急诊分诊结果预测中的表现，发现LLM模型准确性最高，集成AI到急诊工作流或可提升效率，但需解决限制与伦理问题。


<details>
  <summary>Details</summary>
Motivation: 分诊错误是急诊科长期挑战，随着患者增多和人员短缺，将AI集成到分诊协议受关注，需对比不同AI模型表现。

Method: 对法国一家医院7个月内成年患者分诊数据进行回顾性分析，训练并验证三种AI模型，用多种指标评估AI预测分诊水平与金标准的一致性。

Result: LLM模型（URGENTIAPARSE）准确性高于JEPA和NLP模型，也优于护士分诊，还能有效预测住院需求，且处理结构化数据更稳健。

Conclusion: LLM架构在测试模型中提供最准确分诊预测，集成AI到急诊工作流可提高患者安全和运营效率，但需解决模型限制并确保伦理透明。

Abstract: Triage errors, including undertriage and overtriage, are persistent
challenges in emergency departments (EDs). With increasing patient influx and
staff shortages, the integration of artificial intelligence (AI) into triage
protocols has gained attention. This study compares the performance of three AI
models [Natural Language Processing (NLP), Large Language Models (LLM), and
Joint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes
against the FRENCH scale and clinical practice.We conducted a retrospective
analysis of a prospectively recruited cohort gathering adult patient triage
data over a 7-month period at the Roger Salengro Hospital ED (Lille, France).
Three AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)
URGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic
details, verbatim chief complaints, vital signs, and triage outcomes based on
the FRENCH scale and GEMSA coding. The primary outcome was the concordance of
AI-predicted triage level with the FRENCH gold-standard. It was assessed thanks
to various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM
model (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared
to JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse
triage (-4.343). Secondary analyses highlighted the effectiveness of
URGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness
with structured data versus raw transcripts (either for GEMSA prediction or for
FRENCH prediction). LLM architecture, through abstraction of patient
representations, offers the most accurate triage predictions among tested
models. Integrating AI into ED workflows could enhance patient safety and
operational efficiency, though integration into clinical workflows requires
addressing model limitations and ensuring ethical transparency.

</details>


### [80] [A Neural Operator based on Dynamic Mode Decomposition](https://arxiv.org/abs/2507.01117)
*Nikita Sakovich,Dmitry Aksenov,Ekaterina Pleshakova,Sergey Gataullin*

Main category: cs.LG

TL;DR: 本文提出基于DMD算法的神经算子，结合DMD与深度学习对时空过程高效建模，降低计算成本且在多个方程求解逼近中展现高效性。


<details>
  <summary>Details</summary>
Motivation: 在科学计算方法与人工智能技术结合的研究中，找到轻量级与精确计算间的平衡。

Method: 提出基于DMD算法的神经算子，结合DMD与深度学习，自动提取关键模式和系统动力学来构建预测。

Result: 与DeepONet和FNO对比分析，在热方程、拉普拉斯方程和伯格斯方程求解逼近中实现高重建精度。

Conclusion: 该方法能有效降低计算成本，在多个方程求解逼近中展现高效性。

Abstract: The scientific computation methods development in conjunction with artificial
intelligence technologies remains a hot research topic. Finding a balance
between lightweight and accurate computations is a solid foundation for this
direction. The study presents a neural operator based on the dynamic mode
decomposition algorithm (DMD), mapping functional spaces, which combines DMD
and deep learning (DL) for spatiotemporal processes efficient modeling. Solving
PDEs for various initial and boundary conditions requires significant
computational resources. The method suggested automatically extracts key modes
and system dynamics using them to construct predictions, reducing computational
costs compared to traditional numerical methods. The approach has demonstrated
its efficiency through comparative analysis of performance with closest
analogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers
equation solutions approximation, where it achieves high reconstruction
accuracy.

</details>


### [81] [On Design Principles for Private Adaptive Optimizers](https://arxiv.org/abs/2507.01129)
*Arun Ganesh,Brendan McMahan,Abhradeep Thakurta*

Main category: cs.LG

TL;DR: 论文研究差分隐私训练中球形噪声对自适应优化器的影响，对比多种算法，发现scale - then - privatize表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有关于解决差分隐私训练中球形噪声影响自适应优化器性能的研究，实证结果多聚焦简单任务和模型，结论可能无法推广到实际模型训练。

Method: 调查几种算法变体，进行理论分析和实证研究对比。

Result: 自适应优化器中追求梯度二阶矩无偏估计的常见直觉是错误的；scale - then - privatize在小规模语言模型训练任务上表现优于其他变体，且使噪声添加更符合相关噪声机制应用。

Conclusion: scale - then - privatize在差分隐私训练中有更优理论行为和实际表现。

Abstract: The spherical noise added to gradients in differentially private (DP)
training undermines the performance of adaptive optimizers like AdaGrad and
Adam, and hence many recent works have proposed algorithms to address this
challenge. However, the empirical results in these works focus on simple tasks
and models and the conclusions may not generalize to model training in
practice. In this paper we survey several of these variants, and develop better
theoretical intuition for them as well as perform empirical studies comparing
them. We find that a common intuition of aiming for unbiased estimates of
second moments of gradients in adaptive optimizers is misguided, and instead
that a simple technique called scale-then-privatize (which does not achieve
unbiased second moments) has more desirable theoretical behaviors and
outperforms all other variants we study on a small-scale language model
training task. We additionally argue that scale-then-privatize causes the noise
addition to better match the application of correlated noise mechanisms which
are more desirable to use in practice.

</details>


### [82] [Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations](https://arxiv.org/abs/2507.01131)
*Yuchao Lin,Cong Fu,Zachary Krueger,Haiyang Yu,Maho Nakata,Jianwen Xie,Emine Kucukbenli,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.LG

TL;DR: 提出张量分解网络（TDNs）加速SO(3)等变网络的计算，在多个数据集上验证其高效性。


<details>
  <summary>Details</summary>
Motivation: SO(3)等变网络的Clebsch - Gordan张量积计算成本高，需要加速计算。

Method: 用低秩张量分解（如CP分解）替代CG张量积，提出路径权重共享减少参数，新层可替换现有网络中的张量积。

Result: 证明了SO(3)等变诱导误差的一致界和近似任意等变双线性映射的普遍性，将张量积计算复杂度从O(L^6)降至O(L^4)，在多个数据集上计算显著加速且性能有竞争力。

Conclusion: TDNs能有效加速计算并取得有竞争力的性能，可作为现有网络中张量积的替代。

Abstract: $\rm{SO}(3)$-equivariant networks are the dominant models for machine
learning interatomic potentials (MLIPs). The key operation of such networks is
the Clebsch-Gordan (CG) tensor product, which is computationally expensive. To
accelerate the computation, we develop tensor decomposition networks (TDNs) as
a class of approximately equivariant networks whose CG tensor products are
replaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)
decomposition. With the CP decomposition, we prove (i) a uniform bound on the
induced error of $\rm{SO}(3)$-equivariance, and (ii) the universality of
approximating any equivariant bilinear map. To further reduce the number of
parameters, we propose path-weight sharing that ties all multiplicity-space
weights across the $O(L^3)$ CG paths into a single path without compromising
equivariance, where $L$ is the maximum angular degree. The resulting layer acts
as a plug-and-play replacement for tensor products in existing networks, and
the computational complexity of tensor products is reduced from $O(L^6)$ to
$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation
dataset containing 105 million DFT-calculated snapshots. We also use existing
datasets, including OC20, and OC22. Results show that TDNs achieve competitive
performance with dramatic speedup in computations.

</details>


### [83] [Spectral Manifold Harmonization for Graph Imbalanced Regression](https://arxiv.org/abs/2507.01132)
*Brenda Nogueira,Gabe Gomes,Meng Jiang,Nitesh V. Chawla,Nuno Moniz*

Main category: cs.LG

TL;DR: 提出 Spectral Manifold Harmonization (SMH) 方法解决图结构数据不平衡回归问题，实验显示其在化学和药物发现基准数据集上有潜力。


<details>
  <summary>Details</summary>
Motivation: 图结构数据在科学领域普遍存在且模型常面临不平衡学习场景，而针对不平衡回归中特定目标值范围的研究不足。

Method: 提出 Spectral Manifold Harmonization (SMH) 方法，通过生成保留拓扑属性的合成图样本，聚焦常被忽视的目标分布区域。

Result: 在化学和药物发现基准数据集上实验，SMH 方法在目标域范围的预测性能上有持续提升。

Conclusion: SMH 方法能有效应对图结构数据的不平衡回归挑战。

Abstract: Graph-structured data is ubiquitous in scientific domains, where models often
face imbalanced learning settings. In imbalanced regression, domain preferences
focus on specific target value ranges representing the most scientifically
valuable cases; we observe a significant lack of research. In this paper, we
present Spectral Manifold Harmonization (SMH), a novel approach for addressing
this imbalanced regression challenge on graph-structured data by generating
synthetic graph samples that preserve topological properties while focusing on
often underrepresented target distribution regions. Conventional methods fail
in this context because they either ignore graph topology in case generation or
do not target specific domain ranges, resulting in models biased toward average
target values. Experimental results demonstrate the potential of SMH on
chemistry and drug discovery benchmark datasets, showing consistent
improvements in predictive performance for target domain ranges.

</details>


### [84] [FlashDP: Private Training Large Language Models with Efficient DP-SGD](https://arxiv.org/abs/2507.01154)
*Liangyu Wang,Junxiao Wang,Jie Ren,Zihang Xiang,David E. Keyes,Di Wang*

Main category: cs.LG

TL;DR: 本文介绍了用于大语言模型训练数据隐私保护的新方法FlashDP，它降低内存移动和冗余计算，在准确率相当情况下有较高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练数据隐私受关注，现有差分隐私集成方法存在内存和计算效率问题。

Method: 引入缓存友好的逐层DP - SGD方法FlashDP，将必要操作合并为单一任务，以融合方式仅计算一次梯度。

Result: 相比之前方法，减少50%内存移动、20%冗余计算，在Llama - 13B模型预训练时达到非DP方法90%吞吐量，准确率与标准逐层裁剪DP - SGD相当。

Conclusion: FlashDP是高效且保护隐私的大语言模型训练的关键进展，代码已开源。

Abstract: As large language models (LLMs) increasingly underpin technological
advancements, the privacy of their training data emerges as a critical concern.
Differential Privacy (DP) serves as a rigorous mechanism to protect this data,
yet its integration via Differentially Private Stochastic Gradient Descent
(DP-SGD) introduces substantial challenges, primarily due to the complexities
of per-sample gradient clipping. Current explicit methods, such as Opacus,
necessitate extensive storage for per-sample gradients, significantly inflating
memory requirements. Conversely, implicit methods like GhostClip reduce storage
needs by recalculating gradients multiple times, which leads to inefficiencies
due to redundant computations. This paper introduces FlashDP, an innovative
cache-friendly per-layer DP-SGD that consolidates necessary operations into a
single task, calculating gradients only once in a fused manner. This approach
not only diminishes memory movement by up to \textbf{50\%} but also cuts down
redundant computations by \textbf{20\%}, compared to previous methods.
Consequently, FlashDP does not increase memory demands and achieves a
\textbf{90\%} throughput compared to the Non-DP method on a four-A100 system
during the pre-training of the Llama-13B model, while maintaining parity with
standard per-layer clipped DP-SGD in terms of accuracy. These advancements
establish FlashDP as a pivotal development for efficient and privacy-preserving
training of LLMs. FlashDP's code has been open-sourced in
https://github.com/kaustpradalab/flashdp.

</details>


### [85] [Diffusion Explorer: Interactive Exploration of Diffusion Models](https://arxiv.org/abs/2507.01178)
*Alec Helbling,Duen Horng Chau*

Main category: cs.LG

TL;DR: 介绍交互式工具Diffusion Explorer解释扩散模型几何特性，用户可在浏览器训练2D模型并观察采样过程，工具开源且有在线演示。


<details>
  <summary>Details</summary>
Motivation: 现有解释扩散模型的资源要么需高级理论基础，要么关注神经网络架构而非几何特性。

Method: 引入交互式工具Diffusion Explorer，利用交互式动画展示扩散模型的采样过程。

Result: 开发出Diffusion Explorer工具，用户可在浏览器训练2D扩散模型并观察其采样过程的时间动态。

Conclusion: Diffusion Explorer适合解释随时间演变的随机过程的扩散模型，且已开源并提供在线演示。

Abstract: Diffusion models have been central to the development of recent image, video,
and even text generation systems. They posses striking geometric properties
that can be faithfully portrayed in low-dimensional settings. However, existing
resources for explaining diffusion either require an advanced theoretical
foundation or focus on their neural network architectures rather than their
rich geometric properties. We introduce Diffusion Explorer, an interactive tool
to explain the geometric properties of diffusion models. Users can train 2D
diffusion models in the browser and observe the temporal dynamics of their
sampling process. Diffusion Explorer leverages interactive animation, which has
been shown to be a powerful tool for making engaging visualizations of dynamic
systems, making it well suited to explaining diffusion models which represent
stochastic processes that evolve over time. Diffusion Explorer is open source
and a live demo is available at alechelbling.com/Diffusion-Explorer.

</details>


### [86] [Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning](https://arxiv.org/abs/2507.01196)
*Na Lee,Konstantinos Barmpas,Yannis Panagakis,Dimitrios Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: 本文对大脑电图基础模型（LBMs）在脑机接口基准任务上进行评估，发现其效率和适用性存疑，通过消融研究和LoRA减少可训练参数，提出需特定领域开发策略。


<details>
  <summary>Details</summary>
Motivation: 探究基础模型在脑电波建模方面的能力，评估当前大脑电图基础模型（LBMs）在脑机接口（BCI）任务中的表现。

Method: 对多个BCI基准任务进行系统微调实验，开展详细消融研究和应用低秩自适应（LoRA）技术，涵盖全模型微调和参数高效自适应技术。

Result: 最先进的LBMs相比传统深度架构仅取得0.9%-1.2%的边际改进，但需要更多参数；通过LoRA可减少可训练参数且不降低性能；同时调整多个神经网络组件时LoRA性能更佳。

Conclusion: 当前LBMs架构和训练存在低效问题，需要特定领域的开发策略，可能需重新设计架构以充分发挥基础模型在脑电波分析中的潜力。

Abstract: Foundation Models have demonstrated significant success across various
domains in Artificial Intelligence (AI), yet their capabilities for brainwave
modeling remain unclear. In this paper, we comprehensively evaluate current
Large Brainwave Foundation Models (LBMs) through systematic fine-tuning
experiments across multiple Brain-Computer Interface (BCI) benchmark tasks,
including memory tasks and sleep stage classification. Our extensive analysis
shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)
over traditional deep architectures while requiring significantly more
parameters (millions vs thousands), raising important questions about their
efficiency and applicability in BCI contexts. Moreover, through detailed
ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce
trainable parameters without performance degradation, while demonstrating that
architectural and training inefficiencies limit LBMs' current capabilities. Our
experiments span both full model fine-tuning and parameter-efficient adaptation
techniques, providing insights into optimal training strategies for BCI
applications. We pioneer the application of LoRA to LBMs, revealing that
performance benefits generally emerge when adapting multiple neural network
components simultaneously. These findings highlight the critical need for
domain-specific development strategies to advance LBMs, suggesting that current
architectures may require redesign to fully leverage the potential of
foundation models in brainwave analysis.

</details>


### [87] [Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models](https://arxiv.org/abs/2507.01201)
*Hyoseo,Yoon,Yisong Yue,Been Kim*

Main category: cs.LG

TL;DR: 本文提出JAM框架解决视觉和语言模型表示空间不相交的对齐问题，评估不同设计轴，框架能有效诱导对齐。


<details>
  <summary>Details</summary>
Motivation: 独立训练的视觉和语言模型表示空间不相交，但存在向共享统计模型收敛的可能性，要超越事后统计检测，显式优化对齐。

Method: 将柏拉图对齐问题转化为多目标优化任务，引入JAM框架联合训练特定模态自编码器，通过重建和跨模态目标促进对齐。

Result: 在三个关键设计轴上评估框架，发现轻量级Pareto有效框架能可靠诱导对齐，即使在冻结的独立训练表示上。

Conclusion: 框架为将通用单模态基础模型转化为专业多模态模型提供理论见解和实践途径。

Abstract: Independently trained vision and language models inhabit disjoint
representational spaces, shaped by their respective modalities, objectives, and
architectures. Yet an emerging hypothesis - the Platonic Representation
Hypothesis - suggests that such models may nonetheless converge toward a shared
statistical model of reality. This compatibility, if it exists, raises a
fundamental question: can we move beyond post-hoc statistical detection of
alignment and explicitly optimize for it between such disjoint representations?
We cast this Platonic alignment problem as a multi-objective optimization task
- preserve each modality's native structure while aligning for mutual
coherence. We introduce the Joint Autoencoder Modulator (JAM) framework that
jointly trains modality-specific autoencoders on the latent representations of
pre-trained single modality models, encouraging alignment through both
reconstruction and cross-modal objectives. By analogy, this framework serves as
a method to escape Plato's Cave, enabling the emergence of shared structure
from disjoint inputs. We evaluate this framework across three critical design
axes: (i) the alignment objective - comparing contrastive loss (Con), its
hard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at
which alignment is most effective, and (iii) the impact of foundation model
scale on representational convergence. Our findings show that our lightweight
Pareto-efficient framework reliably induces alignment, even across frozen,
independently trained representations, offering both theoretical insight and
practical pathways for transforming generalist unimodal foundations into
specialist multimodal models.

</details>


### [88] [Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform](https://arxiv.org/abs/2507.01208)
*Pedro R. X. Carmo,Igor de Moura,Assis T. de Oliveira Filho,Djamel Sadok,Cleber Zanchettin*

Main category: cs.LG

TL;DR: 文章提出用蒸馏和剪枝等快速神经网络推理技术，在低成本平台实时部署入侵检测系统模型，在树莓派4上取得不错效果。


<details>
  <summary>Details</summary>
Motivation: 现代汽车联网易受攻击，深度学习入侵检测系统实时运行需昂贵硬件，需在低成本平台实时部署。

Method: 评估并应用蒸馏和剪枝等快速神经网络推理技术。

Result: 在树莓派4上实现高达727微秒的入侵检测时间，AUCROC值达0.9890。

Conclusion: 蒸馏和剪枝等技术可在低成本平台实时部署入侵检测系统模型。

Abstract: Modern vehicles are increasingly connected, and in this context, automotive
Ethernet is one of the technologies that promise to provide the necessary
infrastructure for intra-vehicle communication. However, these systems are
subject to attacks that can compromise safety, including flow injection
attacks. Deep Learning-based Intrusion Detection Systems (IDS) are often
designed to combat this problem, but they require expensive hardware to run in
real time. In this work, we propose to evaluate and apply fast neural network
inference techniques like Distilling and Prunning for deploying IDS models on
low-cost platforms in real time. The results show that these techniques can
achieve intrusion detection times of up to 727 {\mu}s using a Raspberry Pi 4,
with AUCROC values of 0.9890.

</details>


### [89] [PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning](https://arxiv.org/abs/2507.01216)
*Xingke Yang,Liang Li,Zhiyi Wan,Sicong Li,Hao Wang,Xiaoqi Qi,Jiang Liu,Tomoaki Ohtsuki,Xin Fu,Miao Pan*

Main category: cs.LG

TL;DR: 提出PAE MobiLLM方法解决设备端大语言模型微调资源有限、通信负担重和隐私泄露问题。


<details>
  <summary>Details</summary>
Motivation: 现有服务器辅助方法在设备端大语言模型微调时存在通信负担重和隐私泄露问题。

Method: 采用服务器辅助加法侧调，集成服务器端激活缓存，开发单令牌激活捷径，引入加法适配器侧网络设计。

Result: 未提及具体实验结果。

Conclusion: PAE MobiLLM可加速微调收敛、提高计算效率、降低通信成本并保护隐私。

Abstract: There is a huge gap between numerous intriguing applications fostered by
on-device large language model (LLM) fine-tuning (FT) from fresh mobile data
and the limited resources of a mobile device. While existing server-assisted
methods (e.g., split learning or side-tuning) may enable LLM FT on the local
mobile device, they suffer from heavy communication burdens of activation
transmissions, and may disclose data, labels or fine-tuned models to the
server. To address those issues, we develop PAE MobiLLM, a privacy-aware and
efficient LLM FT method which can be deployed on the mobile device via
server-assisted additive side-tuning. To further accelerate FT convergence and
improve computing efficiency, PAE MobiLLM integrates activation caching on the
server side, which allows the server to reuse historical activations and saves
the mobile device from repeatedly computing forward passes for the recurring
data samples. Besides, to reduce communication cost, PAE MobiLLM develops a
one-token (i.e., ``pivot'' token) activation shortcut that transmits only a
single activation dimension instead of full activation matrices to guide the
side network tuning. Last but not least, PAE MobiLLM introduces the additive
adapter side-network design which makes the server train the adapter modules
based on device-defined prediction differences rather than raw ground-truth
labels. In this way, the server can only assist device-defined side-network
computing, and learn nothing about data, labels or fine-tuned models.

</details>


### [90] [Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling](https://arxiv.org/abs/2507.01235)
*Bara Rababa,Bilal Farooq*

Main category: cs.LG

TL;DR: 本文探索用量子机器学习对行人压力相关的皮肤电导反应事件建模，对比QSVM和QNN模型性能，QNN测试准确率更高。


<details>
  <summary>Details</summary>
Motivation: 利用量子计算解决复杂机器学习任务，对虚拟现实道路实验中反映行人压力的皮肤电导反应事件建模。

Method: 在Pennylane上开发基于八量子比特ZZ特征图的量子支持向量机（QSVM）和使用树张量网络拟设及八量子比特ZZ特征图的量子神经网络（QNN），对含皮肤电导反应测量值等特征的数据集分类。

Result: QSVM训练准确率好但过拟合，测试准确率45%；QNN测试准确率达55%。

Conclusion: QNN比QSVM和经典版本是更好的分类模型。

Abstract: Quantum computing has opened new opportunities to tackle complex machine
learning tasks, for instance, high-dimensional data representations commonly
required in intelligent transportation systems. We explore quantum machine
learning to model complex skin conductance response (SCR) events that reflect
pedestrian stress in a virtual reality road crossing experiment. For this
purpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature
map and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and
an eight-qubit ZZ feature map, were developed on Pennylane. The dataset
consists of SCR measurements along with features such as the response amplitude
and elapsed time, which have been categorized into amplitude-based classes. The
QSVM achieved good training accuracy, but had an overfitting problem, showing a
low test accuracy of 45% and therefore impacting the reliability of the
classification model. The QNN model reached a higher test accuracy of 55%,
making it a better classification model than the QSVM and the classic versions.

</details>


### [91] [Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW](https://arxiv.org/abs/2507.01241)
*Di Zhang,Yihang Zhang*

Main category: cs.LG

TL;DR: 本文提出用于训练大语言模型的随机共轭次梯度方法与自适应采样策略，比传统SGD收敛更快、可扩展性更好，实验表明其优化速度和准确性更高。


<details>
  <summary>Details</summary>
Motivation: 传统随机梯度下降法在大规模应用中效果存疑，有性能局限，需新方法训练大语言模型。

Method: 提出随机共轭次梯度方法与自适应采样，利用样本复杂度分析自适应选择样本大小，用随机共轭次梯度确定搜索方向，用类似AdamW算法自适应调整步长。

Result: 所提方法保持并在很多情况下超越传统SGD的可扩展性，显著提高优化过程的速度和准确性。

Conclusion: 所提方法保留一阶方法优势，有效解决大语言模型训练中的非凸性和非光滑性问题，是训练大语言模型的有效方法。

Abstract: Stochastic gradient-based descent (SGD), have long been central to training
large language models (LLMs). However, their effectiveness is increasingly
being questioned, particularly in large-scale applications where empirical
evidence suggests potential performance limitations. In response, this paper
proposes a stochastic conjugate subgradient method together with adaptive
sampling tailored specifically for training LLMs. The method not only achieves
faster convergence per iteration but also demonstrates improved scalability
compared to traditional SGD techniques. It leverages sample complexity analysis
to adaptively choose the sample size, employs a stochastic conjugate
subgradient approach to determine search directions and utilizing an AdamW-like
algorithm to adaptively adjust step sizes. This approach preserves the key
advantages of first-order methods while effectively addressing the nonconvexity
and non-smoothness inherent in LLMs training. Additionally, we provide a
detailed analysis of the advantage of the algorithm. Experimental results show
that the proposed method not only maintains, but in many cases surpasses, the
scalability of traditional SGD techniques, significantly enhancing both the
speed and accuracy of the optimization process.

</details>


### [92] [PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning](https://arxiv.org/abs/2507.01271)
*Tatsuki Kawakami,Kazuki Egashira,Atsuyuki Miyai,Go Irie,Kiyoharu Aizawa*

Main category: cs.LG

TL;DR: 本文引入PULSE协议用于LMMs的现实遗忘场景评估，并评估现有遗忘方法，发现部分方法难以消除预训练信息，且顺序遗忘数据会导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 解决LMMs中遗忘技术缺乏实用评估框架的问题，现有LMMs遗忘基准考虑场景有限。

Method: 引入PULSE协议，从预训练知识遗忘和长期可持续性评估两个关键视角评估现有遗忘方法。

Result: 部分技术能遗忘微调知识，但难消除预训练信息；单步遗忘有效，顺序遗忘时性能下降。

Conclusion: 现有LMMs遗忘方法在消除预训练信息和应对顺序遗忘请求方面存在不足。

Abstract: In recent years, unlearning techniques, which are methods for inducing a
model to "forget" previously learned information, have attracted attention as a
way to address privacy and copyright concerns in large language models (LLMs)
and large multimodal models (LMMs). While several unlearning benchmarks have
been established for LLMs, a practical evaluation framework for unlearning in
LMMs has been less explored. Specifically, existing unlearning benchmark for
LMMs considers only scenarios in which the model is required to unlearn
fine-tuned knowledge through a single unlearning operation. In this study, we
introduce PULSE protocol for realistic unlearning scenarios for LMMs by
introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for
analyzing the effect across different knowledge acquisition phases and (ii)
Long-term Sustainability Evaluation to address sequential requests. We then
evaluate existing unlearning methods along these dimensions. Our results reveal
that, although some techniques can successfully unlearn knowledge acquired
through fine-tuning, they struggle to eliminate information learned during
pre-training. Moreover, methods that effectively unlearn a batch of target data
in a single operation exhibit substantial performance degradation when the same
data are split and unlearned sequentially.

</details>


### [93] [Neural Hamiltonian Operator](https://arxiv.org/abs/2507.01313)
*Qian Qi*

Main category: cs.LG

TL;DR: 本文引入神经哈密顿算子（NHO）框架，用深度学习解决高维随机控制问题，证明了NHO的通用逼近能力并分析优化挑战。


<details>
  <summary>Details</summary>
Motivation: 高维随机控制问题因维度诅咒难以用传统动态规划解决，需新方法。

Method: 定义NHO，用神经网络参数化耦合FBSDE动力学，通过训练网络满足PMP的一致性条件找到最优NHO。

Result: 证明了NHO在一般鞅驱动下的通用逼近能力。

Conclusion: 将深度FBSDE方法置于统计推断框架，为分析该类模型的优化挑战提供清晰视角。

Abstract: Stochastic control problems in high dimensions are notoriously difficult to
solve due to the curse of dimensionality. An alternative to traditional dynamic
programming is Pontryagin's Maximum Principle (PMP), which recasts the problem
as a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In
this paper, we introduce a formal framework for solving such problems with deep
learning by defining a \textbf{Neural Hamiltonian Operator (NHO)}. This
operator parameterizes the coupled FBSDE dynamics via neural networks that
represent the feedback control and an ansatz for the value function's spatial
gradient. We show how the optimal NHO can be found by training the underlying
networks to enforce the consistency conditions dictated by the PMP. By adopting
this operator-theoretic view, we situate the deep FBSDE method within the
rigorous language of statistical inference, framing it as a problem of learning
an unknown operator from simulated data. This perspective allows us to prove
the universal approximation capabilities of NHOs under general martingale
drivers and provides a clear lens for analyzing the significant optimization
challenges inherent to this class of models.

</details>


### [94] [ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks](https://arxiv.org/abs/2507.01321)
*Zhiyao Ren,Siyuan Liang,Aishan Liu,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出双学习假设分析ICL后门攻击，推导上限，提出ICLShield防御机制，实验效果佳。


<details>
  <summary>Details</summary>
Motivation: ICL虽成功但存在后门攻击漏洞，需研究防御方法。

Method: 提出双学习假设，推导ICL后门效应上限，提出ICLShield动态调整概念偏好比率，利用置信度和相似度分数选择干净示例。

Result: 在多个LLM和任务上实验，达到了最先进的防御效果，平均比现有方法高26.02%，对闭源模型也有良好表现。

Conclusion: ICLShield能有效缓解ICL对后门攻击的易感性，具有出色的适应性和防御性能。

Abstract: In-context learning (ICL) has demonstrated remarkable success in large
language models (LLMs) due to its adaptability and parameter-free nature.
However, it also introduces a critical vulnerability to backdoor attacks, where
adversaries can manipulate LLM behaviors by simply poisoning a few ICL
demonstrations. In this paper, we propose, for the first time, the
dual-learning hypothesis, which posits that LLMs simultaneously learn both the
task-relevant latent concepts and backdoor latent concepts within poisoned
demonstrations, jointly influencing the probability of model outputs. Through
theoretical analysis, we derive an upper bound for ICL backdoor effects,
revealing that the vulnerability is dominated by the concept preference ratio
between the task and the backdoor. Motivated by these findings, we propose
ICLShield, a defense mechanism that dynamically adjusts the concept preference
ratio. Our method encourages LLMs to select clean demonstrations during the ICL
phase by leveraging confidence and similarity scores, effectively mitigating
susceptibility to backdoor attacks. Extensive experiments across multiple LLMs
and tasks demonstrate that our method achieves state-of-the-art defense
effectiveness, significantly outperforming existing approaches (+26.02% on
average). Furthermore, our method exhibits exceptional adaptability and
defensive performance even for closed-source models (e.g., GPT-4).

</details>


### [95] [Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy](https://arxiv.org/abs/2507.01327)
*Xiaoyun Zhang,Jingqing Ruan,Xing Ma,Yawen Zhu,Jiansong Chen,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: 本文提出APARL框架用于客服对话异常事件检测，提升了模型适应性和鲁棒性，对工业部署有积极贡献。


<details>
  <summary>Details</summary>
Motivation: 现实客服对话异常事件检测因业务数据复杂和客户交互动态性而具挑战性，且模型需有强OOD泛化能力以适应不同业务场景和提升商业价值。

Method: 提出Adaptive Perplexity - Aware Reinforcement Learning (APARL)框架，采用双循环动态课程学习架构。

Result: 在食品配送对话任务评估中，模型适应性和鲁棒性显著增强，F1分数平均提升17.19%，OOD转移测试平均提升9.59%。

Conclusion: 该方法为异常检测模型的工业部署提供了更优方案，有助于提高运营效率和商业效益。

Abstract: Detecting abnormal events in real-world customer service dialogues is highly
challenging due to the complexity of business data and the dynamic nature of
customer interactions. Moreover, models must demonstrate strong out-of-domain
(OOD) generalization to enable rapid adaptation across different business
scenarios and maximize commercial value. In this work, we propose a novel
Adaptive Perplexity-Aware Reinforcement Learning (APARL) framework that
leverages the advanced reasoning capabilities of large language models for
abnormal event detection. APARL introduces a dual-loop dynamic curriculum
learning architecture, enabling the model to progressively focus on more
challenging samples as its proficiency increases. This design effectively
addresses performance bottlenecks and significantly enhances OOD
transferability. Extensive evaluations on food delivery dialogue tasks show
that our model achieves significantly enhanced adaptability and robustness,
attaining the highest F1 score with an average improvement of 17.19\%, and an
average improvement of 9.59\% in OOD transfer tests. This method provides a
superior solution for industrial deployment of anomaly detection models,
contributing to improved operational efficiency and commercial benefits.

</details>


### [96] [Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion](https://arxiv.org/abs/2507.01354)
*Chugang Yi,Minghan Yu,Weikang Qian,Yixin Wen,Haizhao Yang*

Main category: cs.LG

TL;DR: 提出小波扩散模型（WDM）实现降水数据10倍空间超分辨率及9倍推理加速，解决地球科学超分辨率精度和速度问题。


<details>
  <summary>Details</summary>
Motivation: 有效水文建模和极端天气分析需要千米级分辨率降水数据，而标准全球产品分辨率不足。

Method: 提出条件扩散模型WDM，在小波域直接从MRMS雷达数据学习降水复杂结构，聚焦高频小波系数生成1千米降水场。

Result: WDM生成的结果视觉上优于像素空间模型，伪影更少，采样效率显著提高。

Conclusion: WDM为地球科学超分辨率中精度和速度的双重挑战提供了可靠解决方案，有助于更可靠的水文预报。

Abstract: Effective hydrological modeling and extreme weather analysis demand
precipitation data at a kilometer-scale resolution, which is significantly
finer than the 10 km scale offered by standard global products like IMERG. To
address this, we propose the Wavelet Diffusion Model (WDM), a generative
framework that achieves 10x spatial super-resolution (downscaling to 1 km) and
delivers a 9x inference speedup over pixel-based diffusion models. WDM is a
conditional diffusion model that learns the learns the complex structure of
precipitation from MRMS radar data directly in the wavelet domain. By focusing
on high-frequency wavelet coefficients, it generates exceptionally realistic
and detailed 1-km precipitation fields. This wavelet-based approach produces
visually superior results with fewer artifacts than pixel-space models, and
delivers a significant gains in sampling efficiency. Our results demonstrate
that WDM provides a robust solution to the dual challenges of accuracy and
speed in geoscience super-resolution, paving the way for more reliable
hydrological forecasts.

</details>


### [97] [Distributional Soft Actor-Critic with Diffusion Policy](https://arxiv.org/abs/2507.01381)
*Tong Liu,Yinuo Wang,Xujie Song,Wenjun Zou,Liangfa Chen,Likun Wang,Bin Shuai,Jingliang Duan,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 本文提出DSAC - D算法解决价值函数估计偏差和多模态策略表示问题，在MuJoCo测试和实车测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统方法用单峰分布建模价值分布输出易导致价值函数估计偏差，算法性能不佳。

Method: 引入策略熵和价值分布函数建立多模态分布策略迭代框架，用扩散模型反向采样生成奖励样本构建扩散价值网络，推导价值网络和策略网络双扩散的分布式强化学习算法。

Result: MuJoCo测试中学习多模态策略，9个控制任务达SOTA性能，显著抑制估计偏差，总平均回报比现有主流算法提高超10%；实车测试中准确表征不同驾驶风格多模态分布，扩散策略网络能表征多模态轨迹。

Conclusion: DSAC - D算法能有效解决价值函数估计偏差问题，学习多模态策略，具有良好性能。

Abstract: Reinforcement learning has been proven to be highly effective in handling
complex control tasks. Traditional methods typically use unimodal
distributions, such as Gaussian distributions, to model the output of value
distributions. However, unimodal distribution often and easily causes bias in
value function estimation, leading to poor algorithm performance. This paper
proposes a distributional reinforcement learning algorithm called DSAC-D
(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges
of estimating bias in value functions and obtaining multimodal policy
representations. A multimodal distributional policy iteration framework that
can converge to the optimal policy was established by introducing policy
entropy and value distribution function. A diffusion value network that can
accurately characterize the distribution of multi peaks was constructed by
generating a set of reward samples through reverse sampling using a diffusion
model. Based on this, a distributional reinforcement learning algorithm with
dual diffusion of the value network and the policy network was derived. MuJoCo
testing tasks demonstrate that the proposed algorithm not only learns
multimodal policy, but also achieves state-of-the-art (SOTA) performance in all
9 control tasks, with significant suppression of estimation bias and total
average return improvement of over 10\% compared to existing mainstream
algorithms. The results of real vehicle testing show that DSAC-D can accurately
characterize the multimodal distribution of different driving styles, and the
diffusion policy network can characterize multimodal trajectories.

</details>


### [98] [Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning](https://arxiv.org/abs/2507.01389)
*Anbang Wang,Dunbo Cai,Yu Zhang,Yangqing Huang,Xiangyang Feng,Zhihong Zhang*

Main category: cs.LG

TL;DR: 本文受现有替代模型启发，提出增强替代模型，引入松弛变量统一流程，应用于药物组合效应预测，实验表明性能提升。


<details>
  <summary>Details</summary>
Motivation: 改进现有采用因子分解机和量子退火的替代模型，将两步流程统一为一步。

Method: 在因子分解机及其相关伊辛表示中引入松弛变量，训练时迭代更新以考虑高阶特征交互。

Result: 将该方法应用于药物组合效应预测，引入松弛变量使性能显著提升。

Conclusion: 算法为构建利用潜在量子优势的高效替代模型提供了有前景的方法。

Abstract: Recently, a surrogate model was proposed that employs a factorization machine
to approximate the underlying input-output mapping of the original system, with
quantum annealing used to optimize the resulting surrogate function. Inspired
by this approach, we propose an enhanced surrogate model that incorporates
additional slack variables into both the factorization machine and its
associated Ising representation thereby unifying what was by design a two-step
process into a single, integrated step. During the training phase, the slack
variables are iteratively updated, enabling the model to account for
higher-order feature interactions. We apply the proposed method to the task of
predicting drug combination effects. Experimental results indicate that the
introduction of slack variables leads to a notable improvement of performance.
Our algorithm offers a promising approach for building efficient surrogate
models that exploit potential quantum advantages.

</details>


### [99] [Decomposing Prediction Mechanisms for In-Context Recall](https://arxiv.org/abs/2507.01414)
*Sultan Daniels,Dylan Davis,Dhruv Gautam,Wentinn Liao,Gireeja Ranade,Anant Sahai*

Main category: cs.LG

TL;DR: 提出结合线性回归连续上下文学习与离散联想回忆的新玩具问题，研究预训练的transformer模型在此问题上的表现，发现两种预测机制及不同学习动态，还在翻译任务验证。


<details>
  <summary>Details</summary>
Motivation: 研究transformer模型在结合连续上下文学习与离散联想回忆任务中的表现和机制。

Method: 提出新玩具问题，用样本轨迹预训练transformer模型，进行分布外实验和模型权重边缘剪枝分析，还使用OLMo训练检查点在ICL翻译任务验证。

Result: 模型完成任务需两种能力，且学习动态不同；玩具问题的下一个标记预测涉及两种机制，有不同学习动态；在翻译任务也发现类似现象。

Conclusion: 多机制现象不是玩具问题特有，在其他任务也存在。

Abstract: We introduce a new family of toy problems that combine features of
linear-regression-style continuous in-context learning (ICL) with discrete
associative recall. We pretrain transformer models on sample traces from this
toy, specifically symbolically-labeled interleaved state observations from
randomly drawn linear deterministic dynamical systems. We study if the
transformer models can recall the state of a sequence previously seen in its
context when prompted to do so with the corresponding in-context label. Taking
a closer look at this task, it becomes clear that the model must perform two
functions: (1) identify which system's state should be recalled and apply that
system to its last seen state, and (2) continuing to apply the correct system
to predict the subsequent states. Training dynamics reveal that the first
capability emerges well into a model's training. Surprisingly, the second
capability, of continuing the prediction of a resumed sequence, develops much
earlier.
  Via out-of-distribution experiments, and a mechanistic analysis on model
weights via edge pruning, we find that next-token prediction for this toy
problem involves at least two separate mechanisms. One mechanism uses the
discrete symbolic labels to do the associative recall required to predict the
start of a resumption of a previously seen sequence. The second mechanism,
which is largely agnostic to the discrete symbolic labels, performs a
"Bayesian-style" prediction based on the previous token and the context. These
two mechanisms have different learning dynamics.
  To confirm that this multi-mechanism (manifesting as separate phase
transitions) phenomenon is not just an artifact of our toy setting, we used
OLMo training checkpoints on an ICL translation task to see a similar
phenomenon: a decisive gap in the emergence of first-task-token performance vs
second-task-token performance.

</details>


### [100] [Cross-platform Smartphone Positioning at Museums](https://arxiv.org/abs/2507.01469)
*Alessio Ferrato,Fabio Gasparetti,Carla Limongelli,Stefano Mastandrea,Giuseppe Sansonetti,Joaquín Torres-Sospedra*

Main category: cs.LG

TL;DR: 文章指出室内定位系统在文化遗产机构有潜力但实施有挑战，缺乏博物馆环境RSS数据集，为此提出BAR数据集和先进位置分类基线并分析结果与研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决文化遗产机构实施室内定位系统时缺乏反映博物馆环境的公开RSS数据集这一问题。

Method: 在13个博物馆房间的90件艺术品前用安卓和iOS两个平台收集RSS数据形成BAR数据集，采用基于邻近度方法和k - NN算法提供位置分类基线。

Result: 形成BAR数据集并给出先进位置分类基线，还对结果进行了分析。

Conclusion: 文章未明确提及最终结论，但讨论结果并给出潜在研究方向建议。

Abstract: Indoor Positioning Systems (IPSs) hold significant potential for enhancing
visitor experiences in cultural heritage institutions. By enabling personalized
navigation, efficient artifact organization, and better interaction with
exhibits, IPSs can transform the modalities of how individuals engage with
museums, galleries and libraries. However, these institutions face several
challenges in implementing IPSs, including environmental constraints, technical
limits, and limited experimentation. In other contexts, Received Signal
Strength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have
emerged as preferred solutions due to their non-invasive nature and minimal
infrastructure requirements. Nevertheless, the lack of publicly available RSS
datasets that specifically reflect museum environments presents a substantial
barrier to developing and evaluating positioning algorithms designed for the
intricate spatial characteristics typical of cultural heritage sites. To
address this limitation, we present BAR, a novel RSS dataset collected in front
of 90 artworks across 13 museum rooms using two different platforms, i.e.,
Android and iOS. Additionally, we provide an advanced position classification
baseline taking advantage of a proximity-based method and $k$-NN algorithms. In
our analysis, we discuss the results and offer suggestions for potential
research directions.

</details>


### [101] [Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals](https://arxiv.org/abs/2507.01470)
*Yannick Molinghen,Tom Lenaerts*

Main category: cs.LG

TL;DR: 研究重新审视强化学习中奖励频率衡量任务难度的假设，指出当前方法在零激励动态设置中的局限性，表明需不依赖即时激励推断潜在任务结构的机制。


<details>
  <summary>Details</summary>
Motivation: 重新审视强化学习中奖励频率作为任务难度可靠衡量标准的普遍假设。

Method: 识别并形式化当前策略学习方法在零激励动态设置下的结构挑战，分析现有基于子目标的深度算法表现。

Result: 现有基于子目标的深度算法无法利用零激励动态，学习性能对目标完成与最终奖励的时间接近度高度敏感。

Conclusion: 当前方法存在根本局限，需要不依赖即时激励推断潜在任务结构的机制。

Abstract: This work re-examines the commonly held assumption that the frequency of
rewards is a reliable measure of task difficulty in reinforcement learning. We
identify and formalize a structural challenge that undermines the effectiveness
of current policy learning methods: when essential subgoals do not directly
yield rewards. We characterize such settings as exhibiting zero-incentive
dynamics, where transitions critical to success remain unrewarded. We show that
state-of-the-art deep subgoal-based algorithms fail to leverage these dynamics
and that learning performance is highly sensitive to the temporal proximity
between subgoal completion and eventual reward. These findings reveal a
fundamental limitation in current approaches and point to the need for
mechanisms that can infer latent task structure without relying on immediate
incentives.

</details>


### [102] [Loss Functions in Diffusion Models: A Comparative Study](https://arxiv.org/abs/2507.01516)
*Dibyanshu Kumar,Philipp Vaeth,Magda Gregorová*

Main category: cs.LG

TL;DR: 本文详细探索扩散模型的目标目标和损失函数，系统概述其关系，结合理论分析与实证研究，提供对不同目标性能差异的见解，评估目标选择对模型能力的影响，助力未来模型设计。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为强大的生成模型，其训练所用的损失函数是关键问题，过往文献提出多种公式但存在联系与差异，因此需要统一理解。

Method: 详细探索不同目标目标和损失函数，在变分下界目标框架下统一它们，进行理论分析和实证研究。

Result: 提供了不同目标在性能上出现差异的条件以及导致偏差的潜在因素的见解，评估了目标选择对模型实现特定目标能力的影响。

Conclusion: 本研究对扩散模型的损失函数提供了统一理解，有助于未来进行更高效和有目标导向的模型设计。

Abstract: Diffusion models have emerged as powerful generative models, inspiring
extensive research into their underlying mechanisms. One of the key questions
in this area is the loss functions these models shall train with. Multiple
formulations have been introduced in the literature over the past several years
with some links and some critical differences stemming from various initial
considerations. In this paper, we explore the different target objectives and
corresponding loss functions in detail. We present a systematic overview of
their relationships, unifying them under the framework of the variational lower
bound objective. We complement this theoretical analysis with an empirical
study providing insights into the conditions under which these objectives
diverge in performance and the underlying factors contributing to such
deviations. Additionally, we evaluate how the choice of objective impacts the
model ability to achieve specific goals, such as generating high-quality
samples or accurately estimating likelihoods. This study offers a unified
understanding of loss functions in diffusion models, contributing to more
efficient and goal-oriented model designs in future research.

</details>


### [103] [Chargax: A JAX Accelerated EV Charging Simulator](https://arxiv.org/abs/2507.01522)
*Koen Ponse,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 本文介绍JAX框架下的Chargax环境，用于电动汽车充电站的真实模拟以加速RL智能体训练，有显著性能提升且架构模块化。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法样本复杂度高、模拟成本大且现有GPU加速工作多针对经典玩具问题，需要解决电网系统运营效率问题。

Method: 引入基于JAX的Chargax环境用于电动汽车充电站模拟，用真实数据在多种场景下验证。

Result: Chargax比现有环境有100 - 1000倍以上的计算性能提升，模块化架构可表示多种充电站配置。

Conclusion: Chargax能有效加速RL智能体训练，可用于解决可持续能源挑战。

Abstract: Deep Reinforcement Learning can play a key role in addressing sustainable
energy challenges. For instance, many grid systems are heavily congested,
highlighting the urgent need to enhance operational efficiency. However,
reinforcement learning approaches have traditionally been slow due to the high
sample complexity and expensive simulation requirements. While recent works
have effectively used GPUs to accelerate data generation by converting
environments to JAX, these works have largely focussed on classical toy
problems. This paper introduces Chargax, a JAX-based environment for realistic
simulation of electric vehicle charging stations designed for accelerated
training of RL agents. We validate our environment in a variety of scenarios
based on real data, comparing reinforcement learning agents against baselines.
Chargax delivers substantial computational performance improvements of over
100x-1000x over existing environments. Additionally, Chargax' modular
architecture enables the representation of diverse real-world charging station
configurations.

</details>


### [104] [MARVIS: Modality Adaptive Reasoning over VISualizations](https://arxiv.org/abs/2507.01544)
*Benjamin Feuer,Lennart Purucker,Oussama Elachqar,Chinmay Hegde*

Main category: cs.LG

TL;DR: 提出训练免训练方法MARVIS，让小视觉语言模型高精度预测多数据模态，表现优异且开源代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有科学应用机器学习小模型缺乏灵活性，基础模型在非传统模态和长尾领域表现不佳。

Method: 提出MARVIS方法，将潜在嵌入空间转换为视觉表示，利用VLM的空间和细粒度推理技能。

Result: 用单个3B参数模型在多个领域取得有竞争力的表现，平均比Gemini高16%，接近专业方法，且不暴露P.I.I.、无需特定领域训练。

Conclusion: MARVIS能让小视觉语言模型高精度预测多数据模态，有良好性能。

Abstract: Scientific applications of machine learning often rely on small, specialized
models tuned to particular domains. Such models often achieve excellent
performance, but lack flexibility. Foundation models offer versatility, but
typically underperform specialized approaches, especially on non-traditional
modalities and long-tail domains. We propose MARVIS (Modality Adaptive
Reasoning over VISualizations), a training-free method that enables even small
vision-language models to predict any data modality with high accuracy. MARVIS
transforms latent embedding spaces into visual representations and then
leverages the spatial and fine-grained reasoning skills of VLMs to successfully
interpret and utilize them. MARVIS achieves competitive performance on vision,
audio, biological, and tabular domains using a single 3B parameter model,
achieving results that beat Gemini by 16\% on average and approach specialized
methods, without exposing personally identifiable information (P.I.I.) or
requiring any domain-specific training. We open source our code and datasets at
https://github.com/penfever/marvis

</details>


### [105] [Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning](https://arxiv.org/abs/2507.01551)
*Wu Fei,Hao Kong,Shuxian Liang,Yang Lin,Yibo Yang,Jing Tang,Lei Chen,Xiansheng Hua*

Main category: cs.LG

TL;DR: 提出SPRO框架实现过程感知强化学习，实验显示其训练效率高、准确率提升、能维持熵稳定、减少响应长度且无额外计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有PRL引入额外过程奖励模型有计算开销，且无统一理论框架进行过程级优势估计。

Method: 提出SPRO框架，理论证明过程奖励可从策略模型本身获得，引入累积过程奖励和MSA进行逐步动作优势估计。

Result: SPRO训练效率比vaniila GRPO高3.4倍，测试准确率提升17.5%，能维持稳定高策略熵，减少约1/3平均响应长度。

Conclusion: SPRO无额外计算开销，利于工业实现。

Abstract: Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.

</details>


### [106] [How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks](https://arxiv.org/abs/2507.01559)
*Lapo Frati,Neil Traft,Jeff Clune,Nick Cheney*

Main category: cs.LG

TL;DR: 研究神经网络最后一层重采样权重（zapping）在持续学习和少样本迁移学习中的学习和遗忘模式，发现zapping和优化器选择对学习和遗忘动态有影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽表明zapping有效，但不清楚其提升效果的潜在机制。

Method: 在持续学习和少样本迁移学习的挑战性设置下，使用手写字符和自然图像对卷积神经网络进行训练，并测量多任务设置中每个任务的受影响情况。

Result: 经过zapping训练的模型能更快从迁移到新领域的冲击中恢复，zapping和优化器的选择会影响学习和遗忘动态，在模型顺序学习时会出现任务间复杂的协同/干扰模式。

Conclusion: zapping和优化器选择对持续学习和迁移学习的学习和遗忘模式有重要影响。

Abstract: Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-shot transfer learning, with
handwritten characters and natural images. Our experiments show that models
that have undergone zapping during training more quickly recover from the shock
of transferring to a new domain. Furthermore, to better observe the effect of
continual learning in a multi-task setting we measure how each individual task
is affected. This shows that, not only zapping, but the choice of optimizer can
also deeply affect the dynamics of learning and forgetting, causing complex
patterns of synergy/interference between tasks to emerge when the model learns
sequentially at transfer time.

</details>


### [107] [A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning](https://arxiv.org/abs/2507.01581)
*Masood Jan,Wafa Njima,Xun Zhang*

Main category: cs.LG

TL;DR: 本文提出基于联邦学习（FL）的动态室内定位方法，实验表明其性能接近集中式模型且能保障数据隐私等，为增强隐私的室内定位提供可行方案。


<details>
  <summary>Details</summary>
Motivation: 传统室内定位技术误差大且有隐私问题，机器学习技术需集中数据聚合也存在隐私、带宽和服务器可靠性问题，因此需要新方法解决这些挑战。

Method: 提出基于联邦学习（FL）的方法，使用深度神经网络（DNN）模型进行动态室内定位。

Result: 实验结果显示，FL性能接近集中式模型（CL），同时能保证数据隐私、带宽效率和服务器可靠性。

Conclusion: 所提出的FL方法为增强隐私的室内定位提供了可行方案，为安全高效的室内定位系统发展铺平道路。

Abstract: Location information serves as the fundamental element for numerous Internet
of Things (IoT) applications. Traditional indoor localization techniques often
produce significant errors and raise privacy concerns due to centralized data
collection. In response, Machine Learning (ML) techniques offer promising
solutions by capturing indoor environment variations. However, they typically
require central data aggregation, leading to privacy, bandwidth, and server
reliability issues. To overcome these challenges, in this paper, we propose a
Federated Learning (FL)-based approach for dynamic indoor localization using a
Deep Neural Network (DNN) model. Experimental results show that FL has the
nearby performance to Centralized Model (CL) while keeping the data privacy,
bandwidth efficiency and server reliability. This research demonstrates that
our proposed FL approach provides a viable solution for privacy-enhanced indoor
localization, paving the way for advancements in secure and efficient indoor
localization systems.

</details>


### [108] [Analysis of Muon's Convergence and Critical Batch Size](https://arxiv.org/abs/2507.01598)
*Naoki Sato,Hiroki Naganuma,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 本文对新优化器Muon进行理论分析，给出四种变体收敛证明，研究权重衰减影响，推导最小化随机计算成本的临界批量大小并实验验证。


<details>
  <summary>Details</summary>
Motivation: 对利用神经网络参数矩阵结构的新优化器Muon进行理论分析。

Method: 为Muon的四种变体提供收敛证明，分析权重衰减影响，推导临界批量大小。

Result: 添加权重衰减使参数和梯度范数有更严格边界，明确权重衰减系数和学习率关系，得出最小化SFO复杂度的临界批量大小。

Conclusion: 理论分析得到实验验证，为Muon优化器的应用提供理论依据。

Abstract: This paper presents a theoretical analysis of Muon, a new optimizer that
leverages the inherent matrix structure of neural network parameters. We
provide convergence proofs for four practical variants of Muon: with and
without Nesterov momentum, and with and without weight decay. We then show that
adding weight decay leads to strictly tighter bounds on both the parameter and
gradient norms, and we clarify the relationship between the weight decay
coefficient and the learning rate. Finally, we derive Muon's critical batch
size minimizing the stochastic first-order oracle (SFO) complexity, which is
the stochastic computational cost, and validate our theoretical findings with
experiments.

</details>


### [109] [Kernel Recursive Least Squares Dictionary Learning Algorithm](https://arxiv.org/abs/2507.01636)
*Ghasem Alipoor,Karl Skretting*

Main category: cs.LG

TL;DR: 提出基于核的稀疏表示的高效在线字典学习算法，实验显示性能优且高效


<details>
  <summary>Details</summary>
Motivation: 开发高效的基于核的稀疏表示在线字典学习算法

Method: 将输入信号非线性映射到高维特征空间，用虚拟字典稀疏表示，基于递归最小二乘法（RLS）递归更新字典

Result: 在四个不同领域数据集上实验，优于现有在线核字典学习方法，分类准确率接近批量训练模型

Conclusion: 该算法性能出色且效率显著提升

Abstract: We propose an efficient online dictionary learning algorithm for kernel-based
sparse representations. In this framework, input signals are nonlinearly mapped
to a high-dimensional feature space and represented sparsely using a virtual
dictionary. At each step, the dictionary is updated recursively using a novel
algorithm based on the recursive least squares (RLS) method. This update
mechanism works with single samples or mini-batches and maintains low
computational complexity. Experiments on four datasets across different domains
show that our method not only outperforms existing online kernel dictionary
learning approaches but also achieves classification accuracy close to that of
batch-trained models, while remaining significantly more efficient.

</details>


### [110] [Dance Dance ConvLSTM](https://arxiv.org/abs/2507.01644)
*Miguel O'Malley*

Main category: cs.LG

TL;DR: 介绍用基于ConvLSTM的DDCL方法自动生成DDR谱面，改进DDC方法并提高生成准确率


<details>
  <summary>Details</summary>
Motivation: 改进DDC算法，提高《Dance Dance Revolution》谱面自动生成的准确率

Method: 采用基于ConvLSTM的模型（DDCL）进行谱面自动生成

Result: 未明确提及具体结果，但表明改进了DDC方法，大幅提高了谱面生成的准确率

Conclusion: DDCL方法能改进DDC方法，提高谱面生成准确率

Abstract: \textit{Dance Dance Revolution} is a rhythm game consisting of songs and
accompanying choreography, referred to as charts. Players press arrows on a
device referred to as a dance pad in time with steps determined by the song's
chart. In 2017, the authors of Dance Dance Convolution (DDC) developed an
algorithm for the automatic generation of \textit{Dance Dance Revolution}
charts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM
(DDCL), a new method for the automatic generation of DDR charts using a
ConvLSTM based model, which improves upon the DDC methodology and substantially
increases the accuracy of chart generation.

</details>


### [111] [GradMetaNet: An Equivariant Architecture for Learning on Gradients](https://arxiv.org/abs/2507.01649)
*Yoav Gelberg,Yam Eitan,Aviv Navon,Aviv Shamsian,Theo,Putterman,Michael Bronstein,Haggai Maron*

Main category: cs.LG

TL;DR: 本文提出设计处理梯度架构的原则，引入GradMetaNet，证明其通用性并展示其在多种基于梯度任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有直接处理梯度的学习算法架构非专门为梯度处理设计，限制了适用性。

Method: 依据等变设计、处理多数据点梯度集和通过秩1分解高效表示梯度三个原则，构建由简单等变块组成的GradMetaNet。

Result: 证明GradMetaNet的通用性，表明其能近似先前方法无法近似的基于自然梯度的函数。

Conclusion: GradMetaNet在MLP和Transformer的多种基于梯度的任务上有效。

Abstract: Gradients of neural networks encode valuable information for optimization,
editing, and analysis of models. Therefore, practitioners often treat gradients
as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent
works explore learning algorithms that operate directly on gradients but use
architectures that are not specifically designed for gradient processing,
limiting their applicability. In this paper, we present a principled approach
for designing architectures that process gradients. Our approach is guided by
three principles: (1) equivariant design that preserves neuron permutation
symmetries, (2) processing sets of gradients across multiple data points to
capture curvature information, and (3) efficient gradient representation
through rank-1 decomposition. Based on these principles, we introduce
GradMetaNet, a novel architecture for learning on gradients, constructed from
simple equivariant blocks. We prove universality results for GradMetaNet, and
show that previous approaches cannot approximate natural gradient-based
functions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness
on a diverse set of gradient-based tasks on MLPs and transformers, such as
learned optimization, INR editing, and estimating loss landscape curvature.

</details>


### [112] [AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training](https://arxiv.org/abs/2507.01663)
*Zhenyu Han,Ansheng You,Haibo Wang,Kui Luo,Guang Yang,Wenqi Shi,Menglong Chen,Sicheng Zhang,Zeshun Lan,Chunshi Deng,Huazhong Ji,Wenjie Liu,Yu Huang,Yixiang Zhang,Chenyi Pan,Jing Wang,Xin Huang,Chunsheng Li,Jianping Wu*

Main category: cs.LG

TL;DR: 提出异步流式强化学习框架AsyncFlow解决传统框架扩展性和资源利用问题，实验显示吞吐量提升，为下一代设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 传统任务共置和分离的强化学习框架存在扩展性瓶颈、数据流复杂、资源闲置和负载不均衡问题，且多数框架与大模型训练或推理引擎耦合，难以支持自定义引擎。

Method: 引入分布式数据存储和传输模块，提供统一数据管理和细粒度调度；提出基于生产者 - 消费者的异步工作流；将核心能力与底层引擎解耦，通过面向服务的用户界面封装。

Result: 与最先进的基线相比，吞吐量平均提高1.59倍。

Conclusion: 所提出的架构为下一代强化学习训练系统设计提供了可行的见解。

Abstract: Reinforcement learning (RL) has become a pivotal technology in the
post-training phase of large language models (LLMs). Traditional task-colocated
RL frameworks suffer from significant scalability bottlenecks, while
task-separated RL frameworks face challenges in complex dataflows and the
corresponding resource idling and workload imbalance. Moreover, most existing
frameworks are tightly coupled with LLM training or inference engines, making
it difficult to support custom-designed engines. To address these challenges,
we propose AsyncFlow, an asynchronous streaming RL framework for efficient
post-training. Specifically, we introduce a distributed data storage and
transfer module that provides a unified data management and fine-grained
scheduling capability in a fully streamed manner. This architecture inherently
facilitates automated pipeline overlapping among RL tasks and dynamic load
balancing. Moreover, we propose a producer-consumer-based asynchronous workflow
engineered to minimize computational idleness by strategically deferring
parameter update process within staleness thresholds. Finally, the core
capability of AsynFlow is architecturally decoupled from underlying training
and inference engines and encapsulated by service-oriented user interfaces,
offering a modular and customizable user experience. Extensive experiments
demonstrate an average of 1.59 throughput improvement compared with
state-of-the-art baseline. The presented architecture in this work provides
actionable insights for next-generation RL training system designs.

</details>


### [113] [Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling](https://arxiv.org/abs/2507.01679)
*Zeyu Huang,Tianhao Cheng,Zihan Qiu,Zili Wang,Yinghui Xu,Edoardo M. Ponti,Ivan Titov*

Main category: cs.LG

TL;DR: 文章提出Prefix - RFT混合方法结合SFT和RFT，在数学推理问题上验证其有效性，为大语言模型后训练提供新视角。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型后训练技术SFT和RFT各有局限，需一种能结合二者优势的方法。

Method: 提出Prefix - RFT混合方法，融合示范学习和探索学习，在数学推理问题上进行实验。

Result: Prefix - RFT性能超越独立的SFT、RFT及并行混合策略RFT方法，能无缝集成到现有开源框架，对示范数据质量和数量变化有鲁棒性。

Conclusion: SFT和RFT具有互补性，Prefix - RFT有效融合二者，统一示范和探索的范式是未来研究的有前景方向。

Abstract: Existing post-training techniques for large language models are broadly
categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning
(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking
demonstration data but can lead to problematic generalization as a form of
behavior cloning. Conversely, RFT can significantly enhance a model's
performance but is prone to learn unexpected behaviors, and its performance is
highly sensitive to the initial policy. In this paper, we propose a unified
view of these methods and introduce Prefix-RFT, a hybrid approach that
synergizes learning from both demonstration and exploration. Using mathematical
reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is
both simple and effective. It not only surpasses the performance of standalone
SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key
advantage is its seamless integration into existing open-source frameworks,
requiring only minimal modifications to the standard RFT pipeline. Our analysis
highlights the complementary nature of SFT and RFT, and validates that
Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore,
ablation studies confirm the method's robustness to variations in the quality
and quantity of demonstration data. We hope this work offers a new perspective
on LLM post-training, suggesting that a unified paradigm that judiciously
integrates demonstration and exploration could be a promising direction for
future research.

</details>


### [114] [GPT, But Backwards: Exactly Inverting Language Model Outputs](https://arxiv.org/abs/2507.01693)
*Adrians Skapars,Edoardo Manino,Youcheng Sun,Lucas C. Cordeiro*

Main category: cs.LG

TL;DR: 本文提出SODA算法解决大语言模型输入重建问题，实验表明其优于现有方法，能高比例恢复短输入且无假阳性，但难提取长输入隐私信息。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型现有输出的精确输入重建这一法证问题，以实现事后分析和检测虚假输出报告。

Method: 将精确输入重建形式化为离散优化问题，引入基于梯度的SODA算法，在输入搜索空间的连续松弛上操作，有周期性重启和参数衰减。

Result: SODA显著优于现有方法，能从下一个标记对数中完全恢复79.5%的较短分布外输入且无假阳性，但难以从较长输入序列输出中提取隐私信息。

Conclusion: 当前标准部署实践可能足以防止该方法被恶意使用。

Abstract: While existing auditing techniques attempt to identify potential unwanted
behaviours in large language models (LLMs), we address the complementary
forensic problem of reconstructing the exact input that led to an existing LLM
output - enabling post-incident analysis and potentially the detection of fake
output reports. We formalize exact input reconstruction as a discrete
optimisation problem with a unique global minimum and introduce SODA, an
efficient gradient-based algorithm that operates on a continuous relaxation of
the input search space with periodic restarts and parameter decay. Through
comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we
demonstrate that SODA significantly outperforms existing approaches. We succeed
in fully recovering 79.5% of shorter out-of-distribution inputs from next-token
logits, without a single false positive, but struggle to extract private
information from the outputs of longer (15+ token) input sequences. This
suggests that standard deployment practices may currently provide adequate
protection against malicious use of our method. Our code is available at
https://doi.org/10.5281/zenodo.15539879.

</details>


### [115] [PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution](https://arxiv.org/abs/2507.01695)
*Omkar Shende,Gayathri Ananthanarayanan,Marcello Traiola*

Main category: cs.LG

TL;DR: 本文提出PERTINENCE方法，基于输入特征复杂度动态选模型，在多数据集实验中展示出在精度和运算量间平衡的优势。


<details>
  <summary>Details</summary>
Motivation: 大型深度神经网络资源和能耗高，需设计方法在不显著降低精度的前提下减少对其依赖。

Method: 引入PERTINENCE在线方法，用遗传算法探索基于机器学习的输入分配器训练空间，向帕累托最优解收敛。

Result: 在多个数据集上实验表明，PERTINENCE能在精度和运算量间提供替代方案，最多减少36%运算量且精度相当或更好。

Conclusion: PERTINENCE方法可通过动态选择模型有效平衡整体精度和计算效率。

Abstract: Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable
ability to model complex patterns across various domains such as computer
vision, speech recognition, robotics, etc. While large DNN models are often
more accurate than simpler, lightweight models, they are also resource- and
energy-hungry. Hence, it is imperative to design methods to reduce reliance on
such large models without significant degradation in output accuracy. The high
computational cost of these models is often necessary only for a reduced set of
challenging inputs, while lighter models can handle most simple ones. Thus,
carefully combining properties of existing DNN models in a dynamic, input-based
way opens opportunities to improve efficiency without impacting accuracy.
  In this work, we introduce PERTINENCE, a novel online method designed to
analyze the complexity of input features and dynamically select the most
suitable model from a pre-trained set to process a given input effectively. To
achieve this, we employ a genetic algorithm to explore the training space of an
ML-based input dispatcher, enabling convergence towards the Pareto front in the
solution space that balances overall accuracy and computational efficiency.
  We showcase our approach on state-of-the-art Convolutional Neural Networks
(CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers
(ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's
ability to provide alternative solutions to existing state-of-the-art models in
terms of trade-offs between accuracy and number of operations. By
opportunistically selecting among models trained for the same task, PERTINENCE
achieves better or comparable accuracy with up to 36% fewer operations.

</details>


### [116] [Variational Graph Convolutional Neural Networks](https://arxiv.org/abs/2507.01699)
*Illia Oleksiienko,Juho Kanniainen,Alexandros Iosifidis*

Main category: cs.LG

TL;DR: 本文提出空间和时空图卷积网络的变分神经网络版本，估计模型输出和逐层注意力的不确定性，在社交交易分析和人体动作识别任务中展示了优势，提高了模型准确性。


<details>
  <summary>Details</summary>
Motivation: 估计模型不确定性可同时提高图卷积网络的可解释性和准确性，还能用于关键应用中验证模型结果。

Method: 提出空间和时空图卷积网络的变分神经网络版本，估计模型输出和逐层注意力的不确定性。

Result: 在芬兰董事会成员、NTU - 60、NTU - 120和Kinetics数据集的社交交易分析和人体动作识别任务中，除了估计模型不确定性外，还提高了模型准确性。

Conclusion: 所提出的模型在提高模型可解释性的同时，能够提升模型准确性。

Abstract: Estimation of model uncertainty can help improve the explainability of Graph
Convolutional Networks and the accuracy of the models at the same time.
Uncertainty can also be used in critical applications to verify the results of
the model by an expert or additional models. In this paper, we propose
Variational Neural Network versions of spatial and spatio-temporal Graph
Convolutional Networks. We estimate uncertainty in both outputs and layer-wise
attentions of the models, which has the potential for improving model
explainability. We showcase the benefits of these models in the social trading
analysis and the skeleton-based human action recognition tasks on the Finnish
board membership, NTU-60, NTU-120 and Kinetics datasets, where we show
improvement in model accuracy in addition to estimated model uncertainties.

</details>


### [117] [Relational Causal Discovery with Latent Confounders](https://arxiv.org/abs/2507.01700)
*Andrea Piras,Matteo Negro,Ragib Ahsan,David Arbour,Elena Zheleva*

Main category: cs.LG

TL;DR: 现有算法不适用于含潜在混杂因素的关系数据因果发现，提出RelFCI算法并证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现算法不适用于含潜在混杂因素的关系数据，填补该空白。

Method: 基于FCI和RCD算法，定义新图形模型，建立含潜在混杂因素的关系d - 分离的合理性和完备性保证。

Result: 实验结果表明RelFCI能有效识别含潜在混杂因素的关系因果模型的正确因果结构。

Conclusion: RelFCI是一种适用于含潜在混杂因素关系数据的可靠且完备的因果发现算法。

Abstract: Estimating causal effects from real-world relational data can be challenging
when the underlying causal model and potential confounders are unknown. While
several causal discovery algorithms exist for learning causal models with
latent confounders from data, they assume that the data is independent and
identically distributed (i.i.d.) and are not well-suited for learning from
relational data. Similarly, existing relational causal discovery algorithms
assume causal sufficiency, which is unrealistic for many real-world datasets.
To address this gap, we propose RelFCI, a sound and complete causal discovery
algorithm for relational data with latent confounders. Our work builds upon the
Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms
and it defines new graphical models, necessary to support causal discovery in
relational domains. We also establish soundness and completeness guarantees for
relational d-separation with latent confounders. We present experimental
results demonstrating the effectiveness of RelFCI in identifying the correct
causal structure in relational causal models with latent confounders.

</details>


### [118] [B-PL-PINN: Stabilizing PINN Training with Bayesian Pseudo Labeling](https://arxiv.org/abs/2507.01714)
*Kevin Innerebner,Franz M. Rohrhofer,Bernhard C. Geiger*

Main category: cs.LG

TL;DR: 本文提出用贝叶斯PINN替代集成方法，以PINN后验方差评估替代共识，实验显示该方法在基准问题上表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决训练物理信息神经网络（PINNs）用于正向问题时存在的严重收敛问题，促进信息从定义良好区域传播。

Method: 用贝叶斯PINN替代集成方法，用PINN后验方差评估替代共识。

Result: 该方法在一组基准问题上优于集成方法，与结合Adam和LBFGS训练的PINN集成具有竞争力。

Conclusion: 提出的基于贝叶斯PINN和后验方差评估的方法在解决PINNs收敛问题上有效。

Abstract: Training physics-informed neural networks (PINNs) for forward problems often
suffers from severe convergence issues, hindering the propagation of
information from regions where the desired solution is well-defined.
Haitsiukevich and Ilin (2023) proposed an ensemble approach that extends the
active training domain of each PINN based on i) ensemble consensus and ii)
vicinity to (pseudo-)labeled points, thus ensuring that the information from
the initial condition successfully propagates to the interior of the
computational domain.
  In this work, we suggest replacing the ensemble by a Bayesian PINN, and
consensus by an evaluation of the PINN's posterior variance. Our experiments
show that this mathematically principled approach outperforms the ensemble on a
set of benchmark problems and is competitive with PINN ensembles trained with
combinations of Adam and LBFGS.

</details>


### [119] [Revisiting Learning Rate Control](https://arxiv.org/abs/2507.01724)
*Micha Henheik,Theresa Eimer,Marius Lindauer*

Main category: cs.LG

TL;DR: 本文比较学习率控制范式，发现现有方法在不同设置下不可靠，强调算法选择方法的必要性，指出超参优化方法有效性随复杂度降低，建议关注新方向。


<details>
  <summary>Details</summary>
Motivation: 学习率是深度学习重要超参，控制学习率是研究热点，评估学习率控制的现状。

Method: 比较不同学习率控制范式。

Result: 多保真超参优化、固定超参调度和无超参学习方法在部分任务表现好，但不同设置下不可靠；超参优化方法随模型和任务复杂度增加有效性降低。

Conclusion: 学习率控制需要算法选择方法；AutoML 社区应关注更多相关测试任务和新方向，如可微调方法和元学习。

Abstract: The learning rate is one of the most important hyperparameters in deep
learning, and how to control it is an active area within both AutoML and deep
learning research. Approaches for learning rate control span from classic
optimization to online scheduling based on gradient statistics. This paper
compares paradigms to assess the current state of learning rate control. We
find that methods from multi-fidelity hyperparameter optimization,
fixed-hyperparameter schedules, and hyperparameter-free learning often perform
very well on selected deep learning tasks but are not reliable across settings.
This highlights the need for algorithm selection methods in learning rate
control, which have been neglected so far by both the AutoML and deep learning
communities. We also observe a trend of hyperparameter optimization approaches
becoming less effective as models and tasks grow in complexity, even when
combined with multi-fidelity approaches for more expensive model trainings. A
focus on more relevant test tasks and new promising directions like finetunable
methods and meta-learning will enable the AutoML community to significantly
strengthen its impact on this crucial factor in deep learning.

</details>


### [120] [A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based Inference](https://arxiv.org/abs/2507.01740)
*Trung-Dung Hoang,Alceu Bissoto,Vihangkumar V. Naik,Tim Flühmann,Artemii Shlychkov,José Garcia-Tirado,Lisa M. Koch*

Main category: cs.LG

TL;DR: 提出基于神经后验估计的基于模拟的推理（SBI）方法用于1型糖尿病生理模型参数估计，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 准确估计生理模型参数对实现可靠数字孪生至关重要，传统基于马尔可夫链蒙特卡罗的方法处理高维参数空间时缓慢且计算成本高。

Method: 提出基于神经后验估计的基于模拟的推理（SBI）方法。

Result: SBI在参数估计上优于传统方法，对未见条件泛化能力更好，能提供带可靠不确定性量化的实时后验推理。

Conclusion: SBI方法是一种高效、可靠的1型糖尿病生理模型参数估计方法。

Abstract: Accurately estimating parameters of physiological models is essential to
achieving reliable digital twins. For Type 1 Diabetes, this is particularly
challenging due to the complexity of glucose-insulin interactions. Traditional
methods based on Markov Chain Monte Carlo struggle with high-dimensional
parameter spaces and fit parameters from scratch at inference time, making them
slow and computationally expensive. In this study, we propose a
Simulation-Based Inference approach based on Neural Posterior Estimation to
efficiently capture the complex relationships between meal intake, insulin, and
glucose level, providing faster, amortized inference. Our experiments
demonstrate that SBI not only outperforms traditional methods in parameter
estimation but also generalizes better to unseen conditions, offering real-time
posterior inference with reliable uncertainty quantification.

</details>


### [121] [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752)
*Ismail Labiad,Mathurin Videau,Matthieu Kowalski,Marc Schoenauer,Alessandro Leite,Julia Kempe,Olivier Teytaud*

Main category: cs.LG

TL;DR: 本文介绍用于大语言模型（LLM）后训练的进化黑盒方法BBoxER，它能诱导信息瓶颈，有理论保障，实验显示可提升性能和泛化能力，是基于梯度优化的有吸引力补充。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的优化依赖大量标记数据存在隐私和安全问题，而传统黑盒优化方法有可扩展性和计算成本问题，因此需要新方法。

Method: 引入BBoxER方法，通过训练数据的隐式压缩诱导信息瓶颈，利用信息流的易处理性提供理论界限。

Result: 实验表明BBoxER的几次迭代能提升在推理数据集基准上的性能并实现良好泛化。

Conclusion: BBoxER是适用于受限或隐私敏感环境的轻量级模块化增强方法，是基于梯度优化的有吸引力的补充。

Abstract: Gradient-based optimization is the workhorse of deep learning, offering
efficient and scalable training via backpropagation. However, its reliance on
large volumes of labeled data raises privacy and security concerns such as
susceptibility to data poisoning attacks and the risk of overfitting. In
contrast, black box optimization methods, which treat the model as an opaque
function, relying solely on function evaluations to guide optimization, offer a
promising alternative in scenarios where data access is restricted, adversarial
risks are high, or overfitting is a concern. However, black box methods also
pose significant challenges, including poor scalability to high-dimensional
parameter spaces, as prevalent in large language models (LLMs), and high
computational costs due to reliance on numerous model evaluations. This paper
introduces BBoxER, an evolutionary black-box method for LLM post-training that
induces an information bottleneck via implicit compression of the training
data. Leveraging the tractability of information flow, we provide strong
theoretical bounds on generalization, differential privacy, susceptibility to
data poisoning attacks, and robustness to extraction attacks. BBoxER operates
on top of pre-trained LLMs, offering a lightweight and modular enhancement
suitable for deployment in restricted or privacy-sensitive environments, in
addition to non-vacuous generalization guarantees. In experiments with LLMs, we
demonstrate empirically that Retrofitting methods are able to learn, showing
how a few iterations of BBoxER improve performance and generalize well on a
benchmark of reasoning datasets. This positions BBoxER as an attractive add-on
on top of gradient-based optimization.

</details>


### [122] [BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification](https://arxiv.org/abs/2507.01781)
*Dalia Rodríguez-Salas,Christian Riess*

Main category: cs.LG

TL;DR: 提出BranchNet框架将决策树集成转换为稀疏、部分连接的神经网络，在多分类基准测试中表现优于XGBoost，并讨论其优缺点。


<details>
  <summary>Details</summary>
Motivation: 构建既保留符号结构又能进行基于梯度优化的模型，实现模型紧凑、可解释且无需手动调整架构。

Method: 引入BranchNet框架，将决策树的每个分支映射到一个隐藏神经元。

Result: 在结构化多分类基准测试中，BranchNet准确率始终高于XGBoost，有显著提升。

Conclusion: BranchNet有符号可解释性等优势，但在二分类任务上需进一步自适应校准。

Abstract: We introduce BranchNet, a neuro-symbolic learning framework that transforms
decision tree ensembles into sparse, partially connected neural networks. Each
branch, defined as a decision path from root to a parent of leaves, is mapped
to a hidden neuron, preserving symbolic structure while enabling gradient-based
optimization. The resulting models are compact, interpretable, and require no
manual architecture tuning. Evaluated on a suite of structured multi-class
classification benchmarks, BranchNet consistently outperforms XGBoost in
accuracy, with statistically significant gains. We detail the architecture,
training procedure, and sparsity dynamics, and discuss the model's strengths in
symbolic interpretability as well as its current limitations, particularly on
binary tasks where further adaptive calibration may be beneficial.

</details>


### [123] [Towards Decentralized and Sustainable Foundation Model Training with the Edge](https://arxiv.org/abs/2507.01803)
*Leyang Xue,Meghana Madhyastha,Randal Burns,Myungjin Lee,Mahesh K. Marina*

Main category: cs.LG

TL;DR: 提出利用边缘AI设备进行去中心化、可持续的基础模型训练愿景并分析挑战


<details>
  <summary>Details</summary>
Motivation: 基础模型计算需求大，有环境影响和集中控制风险

Method: 提出利用闲置边缘AI设备集体计算能力的愿景，并阐述其可持续性益处

Result: 无明确具体结果

Conclusion: 指出将愿景变为现实需解决一系列挑战

Abstract: Foundation models are at the forefront of AI research, appealing for their
ability to learn from vast datasets and cater to diverse tasks. Yet, their
significant computational demands raise issues of environmental impact and the
risk of centralized control in their development. We put forward a vision
towards decentralized and sustainable foundation model training that leverages
the collective compute of sparingly used connected edge AI devices. We present
the rationale behind our vision, particularly in support of its sustainability
benefit. We further outline a set of challenges that need to be addressed to
turn this vision into reality.

</details>


### [124] [TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents](https://arxiv.org/abs/2507.01823)
*Dmytro Kuzmenko,Nadiya Shvai*

Main category: cs.LG

TL;DR: 提出基于模型强化学习知识迁移新方法，将大模型蒸馏为小模型，提升性能并优化模型大小，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限环境中部署大型世界模型的关键挑战。

Method: 将高容量多任务代理蒸馏为紧凑型模型，并通过FP16后训练量化优化蒸馏模型。

Result: 蒸馏模型达到28.45的归一化分数，优于原模型，且模型大小减少约50%。

Conclusion: 该方法解决实际部署限制，为多任务强化学习系统提供见解和方向。

Abstract: We present a novel approach to knowledge transfer in model-based
reinforcement learning, addressing the critical challenge of deploying large
world models in resource-constrained environments. Our method efficiently
distills a high-capacity multi-task agent (317M parameters) into a compact
model (1M parameters) on the MT30 benchmark, significantly improving
performance across diverse tasks. Our distilled model achieves a
state-of-the-art normalized score of 28.45, surpassing the original 1M
parameter model score of 18.93. This improvement demonstrates the ability of
our distillation technique to capture and consolidate complex multi-task
knowledge. We further optimize the distilled model through FP16 post-training
quantization, reducing its size by $\sim$50\%. Our approach addresses practical
deployment limitations and offers insights into knowledge representation in
large world models, paving the way for more efficient and accessible multi-task
reinforcement learning systems in robotics and other resource-constrained
applications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.

</details>


### [125] [MILP-SAT-GNN: Yet Another Neural SAT Solver](https://arxiv.org/abs/2507.01825)
*Franco Alberto Cardillo,Hamza Khyari,Umberto Straccia*

Main category: cs.LG

TL;DR: 提出用图神经网络（GNN）解决SAT问题的新方法，有理论结果和实验评估且效果不错。


<details>
  <summary>Details</summary>
Motivation: 利用应用于混合整数线性规划（MILP）的GNN技术，让GNN解决SAT问题。

Method: 将k - CNF公式映射为MILP问题，编码为加权二分图，输入GNN训练和测试。

Result: 建立了排列和等价不变性结果；发现标准GNN对可折叠公式的局限；证明通用近似定理；实验取得有前景的结果。

Conclusion: 尽管神经网络架构简单，但该方法能有效解决SAT问题。

Abstract: We proposes a novel method that enables Graph Neural Networks (GNNs) to solve
SAT problems by leveraging a technique developed for applying GNNs to Mixed
Integer Linear Programming (MILP). Specifically, k-CNF formulae are mapped into
MILP problems, which are then encoded as weighted bipartite graphs and
subsequently fed into a GNN for training and testing. From a theoretical
perspective: (i) we establish permutation and equivalence invariance results,
demonstrating that the method produces outputs that are stable under reordering
of clauses and variables; (ii) we identify a theoretical limitation, showing
that for a class of formulae called foldable formulae, standard GNNs cannot
always distinguish satisfiable from unsatisfiable instances; (iii) we prove a
universal approximation theorem, establishing that with Random Node
Initialization (RNI), the method can approximate SAT solving to arbitrary
precision on finite datasets, that is, the GNN becomes approximately sound and
complete on such datasets. Furthermore, we show that for unfoldable formulae,
the same approximation guarantee can be achieved without the need for RNI.
Finally, we conduct an experimental evaluation of our approach, which show
that, despite the simplicity of the neural architecture, the method achieves
promising results.

</details>


### [126] [mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling](https://arxiv.org/abs/2507.01829)
*Tristan Torchet,Christian Metzner,Laura Kriener,Melika Payvand*

Main category: cs.LG

TL;DR: 提出mGRADE用于边缘设备时间处理，在合成任务和图像分类基准测试中表现良好，节省内存。


<details>
  <summary>Details</summary>
Motivation: 边缘设备时间处理需要能在内存受限下捕捉长短程动态的模型，现有模型存在内存或训练效率问题。

Method: 提出mGRADE，结合带可学习间距的一维卷积和最小门控循环单元。

Result: 在合成任务中有效分离和保留多尺度时间特征，在图像分类基准测试中以少约20%内存优于纯卷积和纯循环模型。

Conclusion: mGRADE是边缘设备内存受限多尺度时间处理的有效解决方案。

Abstract: Edge devices for temporal processing demand models that capture both short-
and long- range dynamics under tight memory constraints. While Transformers
excel at sequence modeling, their quadratic memory scaling with sequence length
makes them impractical for such settings. Recurrent Neural Networks (RNNs)
offer constant memory but train sequentially, and Temporal Convolutional
Networks (TCNs), though efficient, scale memory with kernel size. To address
this, we propose mGRADE (mininally Gated Recurrent Architecture with Delay
Embedding), a hybrid-memory system that integrates a temporal 1D-convolution
with learnable spacings followed by a minimal gated recurrent unit (minGRU).
This design allows the convolutional layer to realize a flexible delay
embedding that captures rapid temporal variations, while the recurrent module
efficiently maintains global context with minimal memory overhead. We validate
our approach on two synthetic tasks, demonstrating that mGRADE effectively
separates and preserves multi-scale temporal features. Furthermore, on
challenging pixel-by-pixel image classification benchmarks, mGRADE consistently
outperforms both pure convolutional and pure recurrent counterparts using
approximately 20% less memory footprint, highlighting its suitability for
memory-constrained temporal processing at the edge. This highlights mGRADE's
promise as an efficient solution for memory-constrained multi-scale temporal
processing at the edge.

</details>


### [127] [Automatic Rank Determination for Low-Rank Adaptation via Submodular Function Maximization](https://arxiv.org/abs/2507.01841)
*Yihang Gao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 提出基于子模函数最大化的低秩自适应（LoRA）秩确定方法SubLoRA，结合二阶信息，实验证明其在秩确定和联合训练性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法如AdaLoRA依赖损失函数的一阶近似，在LoRA参数优化良好时线性化不准确且病态，需要更可靠的二阶公式。

Method: 将秩确定问题重新表述为二次目标的组合优化问题，引入子模函数最大化框架和贪婪算法，推导目标成为子模的充要条件，构造满足条件的Hessian矩阵投影，还将其扩展到联合优化设置。

Result: 在微调物理信息神经网络求解偏微分方程的实验中，SubLoRA在秩确定和联合训练性能上优于现有方法。

Conclusion: SubLoRA结合了坚实的理论基础、二阶精度和实际计算效率，是有效的方法。

Abstract: In this paper, we propose SubLoRA, a rank determination method for Low-Rank
Adaptation (LoRA) based on submodular function maximization. In contrast to
prior approaches, such as AdaLoRA, that rely on first-order (linearized)
approximations of the loss function, SubLoRA utilizes second-order information
to capture the potentially complex loss landscape by incorporating the Hessian
matrix. We show that the linearization becomes inaccurate and ill-conditioned
when the LoRA parameters have been well optimized, motivating the need for a
more reliable and nuanced second-order formulation. To this end, we reformulate
the rank determination problem as a combinatorial optimization problem with a
quadratic objective. However, solving this problem exactly is NP-hard in
general. To overcome the computational challenge, we introduce a submodular
function maximization framework and devise a greedy algorithm with
approximation guarantees. We derive a sufficient and necessary condition under
which the rank-determination objective becomes submodular, and construct a
closed-form projection of the Hessian matrix that satisfies this condition
while maintaining computational efficiency. Our method combines solid
theoretical foundations, second-order accuracy, and practical computational
efficiency. We further extend SubLoRA to a joint optimization setting,
alternating between LoRA parameter updates and rank determination under a rank
budget constraint. Extensive experiments on fine-tuning physics-informed neural
networks (PINNs) for solving partial differential equations (PDEs) demonstrate
the effectiveness of our approach. Results show that SubLoRA outperforms
existing methods in both rank determination and joint training performance.

</details>


### [128] [Towards Foundation Auto-Encoders for Time-Series Anomaly Detection](https://arxiv.org/abs/2507.01875)
*Gastón García González,Pedro Casas,Emilio Martínez,Alicia Fernández*

Main category: cs.LG

TL;DR: 受大预训练基础模型启发，引入基于VAE的FAE模型用于时间序列异常检测，介绍概念并给出初步结果。


<details>
  <summary>Details</summary>
Motivation: 受大预训练基础模型成功的启发，寻找新的时间序列建模方法以进行准确的建模、预测和异常检测。

Method: 引入基于变分自编码器（VAEs）的FAE模型，利用VAEs和扩张卷积神经网络（DCNNs）构建单变量时间序列通用模型。

Result: 在不同领域的多维时间序列数据集上有初步结果，包括移动ISP真实数据集和KDD 2021异常检测数据集。

Conclusion: 未明确提及，但暗示FAE模型有潜力用于开箱即用的零样本异常检测应用。

Abstract: We investigate a novel approach to time-series modeling, inspired by the
successes of large pretrained foundation models. We introduce FAE (Foundation
Auto-Encoders), a foundation generative-AI model for anomaly detection in
time-series data, based on Variational Auto-Encoders (VAEs). By foundation, we
mean a model pretrained on massive amounts of time-series data which can learn
complex temporal patterns useful for accurate modeling, forecasting, and
detection of anomalies on previously unseen datasets. FAE leverages VAEs and
Dilated Convolutional Neural Networks (DCNNs) to build a generic model for
univariate time-series modeling, which could eventually perform properly in
out-of-the-box, zero-shot anomaly detection applications. We introduce the main
concepts of FAE, and present preliminary results in different multi-dimensional
time-series datasets from various domains, including a real dataset from an
operational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.

</details>


### [129] [Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection](https://arxiv.org/abs/2507.01924)
*Samirah Bakker,Yao Ma,Seyed Sahand Mohammadi Ziabari*

Main category: cs.LG

TL;DR: 本文探讨结合LSTM、Transformers、iForest和AE的混合深度学习方法用于心理健康医疗计费异常检测，在两个数据集评估，发现结合伪标签和混合深度学习有潜力。


<details>
  <summary>Details</summary>
Motivation: 机器学习方法用于医疗计费异常检测时存在类别不平衡、标签稀缺和复杂序列模式的问题，且以往未评估在伪标签数据上训练的混合模型。

Method: 采用结合LSTM网络和Transformers的混合深度学习方法，通过iForest和AE进行伪标签标记，并在两个真实世界计费数据集上评估。

Result: iForest LSTM基线在声明级数据上召回率最高达0.963，基于iForest的混合模型在操作级数据上召回率最高达0.744，但精度较低。

Conclusion: 结合伪标签和混合深度学习在复杂、不平衡的异常检测场景中有潜力。

Abstract: The complexity of mental healthcare billing enables anomalies, including
fraud. While machine learning methods have been applied to anomaly detection,
they often struggle with class imbalance, label scarcity, and complex
sequential patterns. This study explores a hybrid deep learning approach
combining Long Short-Term Memory (LSTM) networks and Transformers, with
pseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior
work has not evaluated such hybrid models trained on pseudo-labeled data in the
context of healthcare billing. The approach is evaluated on two real-world
billing datasets related to mental healthcare. The iForest LSTM baseline
achieves the highest recall (0.963) on declaration-level data. On the
operation-level data, the hybrid iForest-based model achieves the highest
recall (0.744), though at the cost of lower precision. These findings highlight
the potential of combining pseudo-labeling with hybrid deep learning in
complex, imbalanced anomaly detection settings.

</details>


### [130] [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/abs/2507.01951)
*Zixiao Wang,Yuxin Wang,Xiaorui Wang,Mengting Xing,Jie Gao,Jianjun Xu,Guangcan Liu,Chenhui Jin,Zhuo Wang,Shengzhuo Zhang,Hongtao Xie*

Main category: cs.LG

TL;DR: 本文介绍反射生成模型MetaStone - S1，用SPRM达OpenAI o3性能，开源模型。


<details>
  <summary>Details</summary>
Motivation: 开发性能良好且高效的生成模型，减少参数并实现高效推理。

Method: 引入自监督过程奖励模型SPRM，共享骨干网络，用特定任务头分别进行下一个标记预测和过程评分，整合策略模型和过程奖励模型。

Result: MetaStone - S1仅32B参数规模就达到与OpenAI - o3 - mini系列相当的性能，建立了思维计算总量与测试时间缩放性能之间的缩放定律。

Conclusion: MetaStone - S1性能良好且参数高效，适合测试时间缩放，开源模型支持研究社区。

Abstract: We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [131] [Diversity-Preserving Exploitation of Crossover](https://arxiv.org/abs/2507.01524)
*Johannes Lengler,Tom Offermann*

Main category: cs.NE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Crossover is a powerful mechanism for generating new solutions from a given
population of solutions. Crossover comes with a discrepancy in itself: on the
one hand, crossover usually works best if there is enough diversity in the
population; on the other hand, exploiting the benefits of crossover reduces
diversity. This antagonism often makes crossover reduce its own effectiveness.
  We introduce a new paradigm for utilizing crossover that reduces this
antagonism, which we call diversity-preserving exploitation of crossover
(DiPEC). The resulting Diversity Exploitation Genetic Algorithm (DEGA) is able
to still exploit the benefits of crossover, but preserves a much higher
diversity than conventional approaches.
  We demonstrate the benefits by proving that the (2+1)-DEGA finds the optimum
of LeadingOnes with $O(n^{5/3}\log^{2/3} n)$ fitness evaluations. This is
remarkable since standard genetic algorithms need $\Theta(n^2)$ evaluations,
and among genetic algorithms only some artificial and specifically tailored
algorithms were known to break this runtime barrier. We confirm the theoretical
results by simulations. Finally, we show that the approach is not overfitted to
Leadingones by testing it empirically on other benchmarks and showing that it
is also competitive in other settings. We believe that our findings justify
further systematic investigations of the DiPEC paradigm.

</details>


### [132] [Adaptive Estimation of the Number of Algorithm Runs in Stochastic Optimization](https://arxiv.org/abs/2507.01629)
*Tome Eftimov,Peter Korošec*

Main category: cs.NE

TL;DR: 本文提出经验方法估计连续单目标随机优化算法每个问题实例所需运行次数，经测试准确率高，可减少运行次数、提升效率和环保性。


<details>
  <summary>Details</summary>
Motivation: 确定算法运行次数对实验设计至关重要，影响实验时长和结果可靠性，因此要找到准确估计算法性能所需运行次数的方法。

Method: 利用概率论，进行稳健性检查以识别数据分布与均值的显著不平衡，并在执行过程中动态调整运行次数。

Result: 在不同算法中估计准确率达82% - 95%，可减少约50%运行次数且不影响优化结果。

Conclusion: 在线计算所需运行次数提高了基准测试效率，有助于减少能源消耗，促进更环保的计算生态系统。

Abstract: Determining the number of algorithm runs is a critical aspect of experimental
design, as it directly influences the experiment's duration and the reliability
of its outcomes. This paper introduces an empirical approach to estimating the
required number of runs per problem instance for accurate estimation of the
performance of the continuous single-objective stochastic optimization
algorithm. The method leverages probability theory, incorporating a robustness
check to identify significant imbalances in the data distribution relative to
the mean, and dynamically adjusts the number of runs during execution as an
online approach. The proposed methodology was extensively tested across two
algorithm portfolios (104 Differential Evolution configurations and the
Nevergrad portfolio) and the COCO benchmark suite, totaling 5748000 runs. The
results demonstrate 82% - 95% accuracy in estimations across different
algorithms, allowing a reduction of approximately 50% in the number of runs
without compromising optimization outcomes. This online calculation of required
runs not only improves benchmarking efficiency, but also contributes to energy
reduction, fostering a more environmentally sustainable computing ecosystem.

</details>


### [133] [Customized Exploration of Landscape Features Driving Multi-Objective Combinatorial Optimization Performance](https://arxiv.org/abs/2507.01638)
*Ana Nikolikj,Gabriela Ochoa,Tome Eftimov*

Main category: cs.NE

TL;DR: 分析景观特征以预测多目标组合优化算法性能，考虑C - PLOS - net模型特征，用特定指标评估算法，揭示特定景观下影响算法性能的特征组合。


<details>
  <summary>Details</summary>
Motivation: 为预测多目标组合优化算法的性能，深入了解特征重要性。

Method: 基于C - PLOS - net模型，选取rmnk - 景观作为基准实例，用分辨率和超体积指标评估三种算法。

Result: 发现了特定景观下影响算法性能的特征组合。

Conclusion: 该研究针对特定rmnk - 景观和算法，对特征重要性有了更深入的认识。

Abstract: We present an analysis of landscape features for predicting the performance
of multi-objective combinatorial optimization algorithms. We consider features
from the recently proposed compressed Pareto Local Optimal Solutions Networks
(C-PLOS-net) model of combinatorial landscapes. The benchmark instances are a
set of rmnk-landscapes with 2 and 3 objectives and various levels of ruggedness
and objective correlation. We consider the performance of three algorithms --
Pareto Local Search (PLS), Global Simple EMO Optimizer (GSEMO), and
Non-dominated Sorting Genetic Algorithm (NSGA-II) - using the resolution and
hypervolume metrics. Our tailored analysis reveals feature combinations that
influence algorithm performance specific to certain landscapes. This study
provides deeper insights into feature importance, tailored to specific
rmnk-landscapes and algorithms.

</details>


### [134] [Comparing Optimization Algorithms Through the Lens of Search Behavior Analysis](https://arxiv.org/abs/2507.01668)
*Gjorgjina Cenikj,Gašper Petelin,Tome Eftimov*

Main category: cs.NE

TL;DR: 探讨用统计测试比较算法搜索行为以解决元启发式算法创新模糊问题，对114个算法进行分析。


<details>
  <summary>Details</summary>
Motivation: 当前数值优化领域新元启发式算法存在创新不明确、与现有方法区分度低的问题，需评估算法搜索行为。

Method: 利用交叉匹配统计测试比较多元分布，对MEALPY库中114个算法的解进行评估。

Result: 文档未提及具体结果。

Conclusion: 文档未提及具体结论。

Abstract: The field of numerical optimization has recently seen a surge in the
development of "novel" metaheuristic algorithms, inspired by metaphors derived
from natural or human-made processes, which have been widely criticized for
obscuring meaningful innovations and failing to distinguish themselves from
existing approaches. Aiming to address these concerns, we investigate the
applicability of statistical tests for comparing algorithms based on their
search behavior. We utilize the cross-match statistical test to compare
multivariate distributions and assess the solutions produced by 114 algorithms
from the MEALPY library. These findings are incorporated into an empirical
analysis aiming to identify algorithms with similar search behaviors.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [135] [Is It Safe To Learn And Share? On Psychological Safety and Social Learning in (Agile) Communities of Practice](https://arxiv.org/abs/2507.01065)
*Christiaan Verwijs,Evelien Acun-Roos,Daniel Russo*

Main category: cs.SE

TL;DR: 研究通过调查143名参与者，发现敏捷实践社区中线上互动心理安全感低于面对面交流，分析威胁因素并给出干预建议。


<details>
  <summary>Details</summary>
Motivation: 随着混合、分布式和异步工作模式普及，敏捷软件开发中的持续学习愈发重要，实践社区常依赖虚拟交流，但其中心理安全感缺乏研究。

Method: 采用混合研究方法，通过对143名参与者的调查数据进行分析，还进行了主题分析，并通过30名参与者进行成员检查验证结果。

Result: 线上互动心理安全感显著低于面对面交流；低心理安全感降低参与者继续贡献意愿和避免人际风险；角色和年龄组有差异；威胁因素包括排斥行为、负面互动模式和敌意等。

Conclusion: 研究提供了互动方式的比较视角，为实践社区组织者提供培养包容性、高影响力社区及类似工作环境的实用指导。

Abstract: As hybrid, distributed, and asynchronous work models become more prevalent,
continuous learning in Agile Software Development (ASD) gains renewed
importance. Communities of Practice (CoPs) are increasingly adopted to support
social learning beyond formal education, often relying on virtual
communication. Psychological safety, a prerequisite for effective learning,
remains insufficiently understood in these settings. This mixed-methods study
investigates psychological safety within Agile CoPs through survey data from
143 participants. Results indicate that psychological safety is significantly
lower in online interactions compared to face-to-face settings. Moreover, low
psychological safety reduces participants' intent to continue contributing and
avoidance of interpersonal risk. No significant differences emerged based on
gender, community seniority, or content creation activity. However, differences
by role and age group suggest potential generational or role-related effects.
Thematic analysis revealed exclusionary behavior, negative interaction
patterns, and hostility as primary threats to psychological safety, often
reinforced by tribalism and specific community dynamics. Suggested
interventions include establishing explicit norms, structured facilitation, and
active moderation. The findings were validated through member checking with 30
participants. This study provides a comparative perspective on interaction
modalities and offers practical guidance for organizers seeking to cultivate
inclusive, high-impact CoPs and similarly structured virtual or hybrid work
environments.

</details>


### [136] [Bugs in the Shadows: Static Detection of Faulty Python Refactorings](https://arxiv.org/abs/2507.01103)
*Jonhnanthan Oliveira,Rohit Gheyi,Márcio Ribeiro,Alessandro Garcia*

Main category: cs.SE

TL;DR: 本文提出静态分析技术检测Python重构引入的类型错误，评估发现了一些错误并提交给开发者，强调需提升Python重构工具的健壮性。


<details>
  <summary>Details</summary>
Motivation: Python动态类型系统给自动化重构带来挑战，理解重构中类型错误的引入很关键，可保障软件可靠性和提高开发者效率。

Method: 提出静态分析技术，在Rope重构实现上评估，并应用于开源Python项目。

Result: 在1152次重构尝试中，发现29个错误，部分错误也存在于常用IDE中，提交给开发者后部分被认可。

Conclusion: 需要提升当前Python重构工具的健壮性，确保代码转换的正确性和软件维护的可靠性。

Abstract: Python is a widely adopted programming language, valued for its simplicity
and flexibility. However, its dynamic type system poses significant challenges
for automated refactoring - an essential practice in software evolution aimed
at improving internal code structure without changing external behavior.
Understanding how type errors are introduced during refactoring is crucial, as
such errors can compromise software reliability and reduce developer
productivity. In this work, we propose a static analysis technique to detect
type errors introduced by refactoring implementations for Python. We evaluated
our technique on Rope refactoring implementations, applying them to open-source
Python projects. Our analysis uncovered 29 bugs across four refactoring types
from a total of 1,152 refactoring attempts. Several of these issues were also
found in widely used IDEs such as PyCharm and PyDev. All reported bugs were
submitted to the respective developers, and some of them were acknowledged and
accepted. These results highlight the need to improve the robustness of current
Python refactoring tools to ensure the correctness of automated code
transformations and support reliable software maintenance.

</details>


### [137] [Context-Aware Code Wiring Recommendation with LLM-based Agent](https://arxiv.org/abs/2507.01315)
*Taiming Wang,Yanjie Jiang,Chunhao Dong,Yuxia Zhang,Hui Liu*

Main category: cs.SE

TL;DR: 本文介绍了基于LLM的代码接线工具WIRL，评估显示其性能优于高级LLM和IntelliJ IDEA，为现代IDE开发者辅助提供新思路。


<details>
  <summary>Details</summary>
Motivation: 现有代码接线解决方案未能有效利用上下文信息，而超半数代码适配情况依赖上下文，因此需要新方法。

Method: 将代码接线构建为检索增强生成（RAG）填充任务，结合LLM、定制工具包和编排模块，采用混合策略。

Result: 在精心策划的数据集上评估，WIRL精确匹配精度达91.7%，召回率达90.0%，优于高级LLM和IntelliJ IDEA。

Conclusion: WIRL为现代IDE中更智能、上下文感知的开发者辅助铺平了道路。

Abstract: Copy-paste-modify is a widespread and pragmatic practice in software
development, where developers adapt reused code snippets, sourced from
platforms such as Stack Overflow, GitHub, or LLM outputs, into their local
codebase. A critical yet underexplored aspect of this adaptation is code
wiring, which involves substituting unresolved variables in the pasted code
with suitable ones from the surrounding context. Existing solutions either rely
on heuristic rules or historical templates, often failing to effectively
utilize contextual information, despite studies showing that over half of
adaptation cases are context-dependent. In this paper, we introduce WIRL, an
LLM-based agent for code wiring framed as a Retrieval-Augmented Generation
(RAG) infilling task. WIRL combines an LLM, a customized toolkit, and an
orchestration module to identify unresolved variables, retrieve context, and
perform context-aware substitutions. To balance efficiency and autonomy, the
agent adopts a mixed strategy: deterministic rule-based steps for common
patterns, and a state-machine-guided decision process for intelligent
exploration. We evaluate WIRL on a carefully curated, high-quality dataset
consisting of real-world code adaptation scenarios. Our approach achieves an
exact match precision of 91.7% and a recall of 90.0%, outperforming advanced
LLMs by 22.6 and 13.7 percentage points in precision and recall, respectively,
and surpassing IntelliJ IDEA by 54.3 and 49.9 percentage points. These results
underscore its practical utility, particularly in contexts with complex
variable dependencies or multiple unresolved variables. We believe WIRL paves
the way for more intelligent and context-aware developer assistance in modern
IDEs.

</details>


### [138] [Combining Type Inference and Automated Unit Test Generation for Python](https://arxiv.org/abs/2507.01477)
*Lukas Krodinger,Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: 本文提出类型追踪方法扩展Pynguin框架，解决Python自动单元测试生成中类型信息缺失问题，提升代码覆盖率和变异分数。


<details>
  <summary>Details</summary>
Motivation: 动态类型编程语言如Python缺乏类型信息，抑制了依赖类型信息的自动测试生成器。

Method: 引入类型追踪，作为Pynguin框架扩展，在运行时观察参数使用和返回值类型，提取和完善类型信息。

Result: 实现了高达90.0%的分支覆盖率提升，改进了变异分数，类型信息质量与其他先进工具相当。

Conclusion: 类型追踪方法能有效解决Python自动测试生成中类型信息缺失问题，提升测试效果。

Abstract: Automated unit test generation is an established research field that has so
far focused on statically-typed programming languages. The lack of type
information in dynamically-typed programming languages, such as Python,
inhibits test generators, which heavily rely on information about parameter and
return types of functions to select suitable arguments when constructing test
cases. Since automated test generators inherently rely on frequent execution of
candidate tests, we make use of these frequent executions to address this
problem by introducing type tracing, which extracts type-related information
during execution and gradually refines the available type information. We
implement type tracing as an extension of the Pynguin test-generation framework
for Python, allowing it (i) to infer parameter types by observing how
parameters are used during runtime, (ii) to record the types of values that
function calls return, and (iii) to use this type information to increase code
coverage. The approach leads to up to 90.0% more branch coverage, improved
mutation scores, and to type information of similar quality to that produced by
other state-of-the-art type-inference tools.

</details>


### [139] [DaiFu: In-Situ Crash Recovery for Deep Learning Systems](https://arxiv.org/abs/2507.01628)
*Zilong He,Pengfei Chen,Hongyu Zhang,Xiaoyun Li,Guangba Yu,Hongyang Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出DL系统原位恢复框架DaiFu，能快速从崩溃中恢复，评估显示它能减少恢复时间，加速1372倍且开销可忽略不计。


<details>
  <summary>Details</summary>
Motivation: DL系统开发和执行中崩溃常见，现有恢复方案太重，高效恢复很关键。

Method: 通过轻量级代码转换，使DaiFu拦截崩溃并动态即时更新程序运行上下文。

Result: DaiFu减少恢复时间，比现有方案加速1372倍，开销低于0.40%，在7种崩溃场景中有效。

Conclusion: DaiFu能实现DL系统的敏捷崩溃恢复。

Abstract: Deep learning (DL) systems have been widely adopted in many areas, and are
becoming even more popular with the emergence of large language models.
However, due to the complex software stacks involved in their development and
execution, crashes are unavoidable and common. Crashes severely waste computing
resources and hinder development productivity, so efficient crash recovery is
crucial. Existing solutions, such as checkpoint-retry, are too heavyweight for
fast recovery from crashes caused by minor programming errors or transient
runtime errors. Therefore, we present DaiFu, an in-situ recovery framework for
DL systems. Through a lightweight code transformation to a given DL system,
DaiFu augments it to intercept crashes in situ and enables dynamic and instant
updates to its program running context (e.g., code, configurations, and other
data) for agile crash recovery. Our evaluation shows that DaiFu helps reduce
the restore time for crash recovery, achieving a 1372x speedup compared with
state-of-the-art solutions. Meanwhile, the overhead of DaiFu is negligible
(under 0.40%). We also construct a benchmark spanning 7 distinct crash
scenarios in DL systems, and show the effectiveness of DaiFu in diverse
situations.

</details>


### [140] [APRMCTS: Improving LLM-based Automated Program Repair with Iterative Tree Search](https://arxiv.org/abs/2507.01827)
*Haichuan Hu,Congqing He,Hao Zhang,Xiaochen Xie,Quanjun Zhang*

Main category: cs.SE

TL;DR: 提出APRMCTS改进基于大语言模型的自动程序修复（APR），实验表明其在修复数量、效率和成本上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的APR技术采用试错策略，存在补丁效果受限于局部探索、搜索效率低的问题。

Method: 将蒙特卡罗树搜索（MCTS）融入补丁搜索，对已探索补丁进行全局评估，选择最有前景的补丁进行后续细化和生成。

Result: 在Defects4J的835个bug上实验，与GPT - 3.5集成可修复201个bug，优于所有基线；帮助多种模型多修复一定数量bug；采用小补丁尺寸；时间和货币成本分别低于现有方法的20%和50%。

Conclusion: APRMCTS具有良好的有效性和效率，在处理复杂bug方面有特别优势。

Abstract: Automated Program Repair (APR) attempts to fix software bugs without human
intervention, which plays a crucial role in software development and
maintenance. Recently, with the advances in Large Language Models (LLMs), a
rapidly increasing number of APR techniques have been proposed with remarkable
performance. However, existing LLM-based APR techniques typically adopt
trial-and-error strategies, which suffer from two major drawbacks: (1)
inherently limited patch effectiveness due to local exploration, and (2) low
search efficiency due to redundant exploration. In this paper, we propose
APRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS
incorporates Monte Carlo Tree Search (MCTS) into patch searching by performing
a global evaluation of the explored patches and selecting the most promising
one for subsequent refinement and generation. APRMCTS effectively resolves the
problems of falling into local optima and thus helps improve the efficiency of
patch searching. Our experiments on 835 bugs from Defects4J demonstrate that,
when integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which
outperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,
GPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,
respectively. More importantly, APRMCTS boasts a significant performance
advantage while employing small patch size (16 and 32), notably fewer than the
500 and 10,000 patches adopted in previous studies. In terms of cost, compared
to existing state-of-the-art LLM-based APR methods, APRMCTS has time and
monetary costs of less than 20% and 50%, respectively. Our extensive study
demonstrates that APRMCTS exhibits good effectiveness and efficiency, with
particular advantages in addressing complex bugs.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [141] [End-to-End Large Portfolio Optimization for Variance Minimization with Neural Networks through Covariance Cleaning](https://arxiv.org/abs/2507.01918)
*Christian Bongiorno,Efstratios Manolakis,Rosario Nunzio Mantegna*

Main category: q-fin.PM

TL;DR: 开发旋转不变神经网络获取全局最小方差投资组合，模型有可解释性、泛化性好，样本外测试表现优于分析竞品，策略在现实框架和市场压力下仍有优势。


<details>
  <summary>Details</summary>
Motivation: 构建能有效处理大规模股票协方差矩阵，提供全局最小方差投资组合且具可解释性和泛化性的模型。

Method: 开发旋转不变神经网络，联合学习滞后变换历史收益和正则化协方差矩阵特征值与边际波动率，用未来实现的最小投资组合方差作为损失函数端到端优化。

Result: 在2000年1月至2024年12月样本外测试中，该估计器实现更低波动率、更小最大回撤和更高夏普比率；学习的协方差表示可用于长仓约束优化器且性能优势不减；策略在现实框架下收益稳定，市场压力下表现稳定。

Conclusion: 所开发的旋转不变神经网络在提供全局最小方差投资组合方面表现出色，具有良好可解释性、泛化性和实际应用价值。

Abstract: We develop a rotation-invariant neural network that provides the global
minimum-variance portfolio by jointly learning how to lag-transform historical
returns and how to regularise both the eigenvalues and the marginal
volatilities of large equity covariance matrices. This explicit mathematical
mapping offers clear interpretability of each module's role, so the model
cannot be regarded as a pure black-box. The architecture mirrors the analytical
form of the global minimum-variance solution yet remains agnostic to dimension,
so a single model can be calibrated on panels of a few hundred stocks and
applied, without retraining, to one thousand US equities-a cross-sectional jump
that demonstrates robust out-of-sample generalisation. The loss function is the
future realized minimum portfolio variance and is optimized end-to-end on real
daily returns. In out-of-sample tests from January 2000 to December 2024 the
estimator delivers systematically lower realised volatility, smaller maximum
drawdowns, and higher Sharpe ratios than the best analytical competitors,
including state-of-the-art non-linear shrinkage. Furthermore, although the
model is trained end-to-end to produce an unconstrained (long-short)
minimum-variance portfolio, we show that its learned covariance representation
can be used in general optimizers under long-only constraints with virtually no
loss in its performance advantage over competing estimators. These gains
persist when the strategy is executed under a highly realistic implementation
framework that models market orders at the auctions, empirical slippage,
exchange fees, and financing charges for leverage, and they remain stable
during episodes of acute market stress.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [142] [Parsimonious Gaussian mixture models with piecewise-constant eigenvalue profiles](https://arxiv.org/abs/2507.01542)
*Tom Szwagier,Pierre-Alexandre Mattei,Charles Bouveyron,Xavier Pennec*

Main category: stat.ML

TL;DR: 提出具有分段常数协方差特征值轮廓的简约高斯混合模型（GMMs），并给出学习参数算法，实验显示该模型在似然性和简约性平衡上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有全GMMs在高维空间存在协方差矩阵过参数化问题，球形GMMs缺乏拟合某些各向异性分布的灵活性，需新模型连接两者极端情况。

Method: 引入具有分段常数协方差特征值轮廓的GMMs，若特征值重数预先指定，用期望最大化（EM）算法学习参数；否则用分量惩罚EM算法学习参数和超参数。

Result: 模型在密度拟合、聚类和单图像去噪等无监督实验中实现了优越的似然性 - 简约性权衡。

Conclusion: 提出的新GMMs模型和学习算法有效，能在多种无监督任务中取得良好效果。

Abstract: Gaussian mixture models (GMMs) are ubiquitous in statistical learning,
particularly for unsupervised problems. While full GMMs suffer from the
overparameterization of their covariance matrices in high-dimensional spaces,
spherical GMMs (with isotropic covariance matrices) certainly lack flexibility
to fit certain anisotropic distributions. Connecting these two extremes, we
introduce a new family of parsimonious GMMs with piecewise-constant covariance
eigenvalue profiles. These extend several low-rank models like the celebrated
mixtures of probabilistic principal component analyzers (MPPCA), by enabling
any possible sequence of eigenvalue multiplicities. If the latter are
prespecified, then we can naturally derive an expectation-maximization (EM)
algorithm to learn the mixture parameters. Otherwise, to address the
notoriously-challenging issue of jointly learning the mixture parameters and
hyperparameters, we propose a componentwise penalized EM algorithm, whose
monotonicity is proven. We show the superior likelihood-parsimony tradeoffs
achieved by our models on a variety of unsupervised experiments: density
fitting, clustering and single-image denoising.

</details>


### [143] [Asymptotic convexity of wide and shallow neural networks](https://arxiv.org/abs/2507.01044)
*Vivek Borkar,Parthe Pandit*

Main category: stat.ML

TL;DR: 对于浅而宽的神经网络简单模型，其输入 - 输出映射上境图能精确近似凸函数上境图，解释其性能良好原因。


<details>
  <summary>Details</summary>
Motivation: 解释浅而宽神经网络表现良好的原因。

Method: 研究简单模型输入 - 输出映射上境图与凸函数上境图的近似关系。

Result: 发现该模型输入 - 输出映射上境图能精确近似凸函数上境图。

Conclusion: 这种近似关系为浅而宽神经网络良好表现提供了合理的解释。

Abstract: For a simple model of shallow and wide neural networks, we show that the
epigraph of its input-output map as a function of the network parameters
approximates epigraph of a. convex function in a precise sense. This leads to a
plausible explanation of their observed good performance.

</details>


### [144] [When Less Is More: Binary Feedback Can Outperform Ordinal Comparisons in Ranking Recovery](https://arxiv.org/abs/2507.01613)
*Shirong Xu,Jingnan Zhang,Junhui Wang*

Main category: stat.ML

TL;DR: 提出无平局序数成对比较参数框架，表明二值化序数数据可提高排名恢复准确性，有理论证明及实验验证。


<details>
  <summary>Details</summary>
Motivation: 挑战序数比较数据比二元比较数据信息更丰富的传统观点。

Method: 提出含广义加性结构的参数框架，通过二值化序数数据将经典二元比较模型作为特例，运用计数算法分析排名误差。

Result: 证明在计数算法下二元比较排名误差收敛更快，刻画二元与序数数据在信噪比上的性能差距，找到使信噪比最小化的模式函数。

Conclusion: 二值化序数数据能显著提高排名恢复准确性，理论结果得到模拟和实际应用验证。

Abstract: Paired comparison data, where users evaluate items in pairs, play a central
role in ranking and preference learning tasks. While ordinal comparison data
intuitively offer richer information than binary comparisons, this paper
challenges that conventional wisdom. We propose a general parametric framework
for modeling ordinal paired comparisons without ties. The model adopts a
generalized additive structure, featuring a link function that quantifies the
preference difference between two items and a pattern function that governs the
distribution over ordinal response levels. This framework encompasses classical
binary comparison models as special cases, by treating binary responses as
binarized versions of ordinal data. Within this framework, we show that
binarizing ordinal data can significantly improve the accuracy of ranking
recovery. Specifically, we prove that under the counting algorithm, the ranking
error associated with binary comparisons exhibits a faster exponential
convergence rate than that of ordinal data. Furthermore, we characterize a
substantial performance gap between binary and ordinal data in terms of a
signal-to-noise ratio (SNR) determined by the pattern function. We identify the
pattern function that minimizes the SNR and maximizes the benefit of
binarization. Extensive simulations and a real application on the MovieLens
dataset further corroborate our theoretical findings.

</details>


### [145] [A generative modeling / Physics-Informed Neural Network approach to random differential equations](https://arxiv.org/abs/2507.01687)
*Georgios Arampatzis,Stylianos Katsarakis,Charalambos Makridakis*

Main category: stat.ML

TL;DR: 本文将SciML技术与UQ结合，改进PINNs以有效建模复杂系统不确定性，并通过应用证明方法有效性。


<details>
  <summary>Details</summary>
Motivation: 将科学机器学习技术与不确定性量化结合是计算科学前沿，要有效建模复杂系统的不确定性。

Method: 结合生成式建模技术与物理信息神经网络，通过概率框架改进物理信息神经网络。

Result: 该方法能系统地控制不确定性，同时保持模型预测准确性。

Conclusion: 该方法在随机微分方程和随机偏微分方程应用中展现出实用性。

Abstract: The integration of Scientific Machine Learning (SciML) techniques with
uncertainty quantification (UQ) represents a rapidly evolving frontier in
computational science. This work advances Physics-Informed Neural Networks
(PINNs) by incorporating probabilistic frameworks to effectively model
uncertainty in complex systems. Our approach enhances the representation of
uncertainty in forward problems by combining generative modeling techniques
with PINNs. This integration enables in a systematic fashion uncertainty
control while maintaining the predictive accuracy of the model. We demonstrate
the utility of this method through applications to random differential
equations and random partial differential equations (PDEs).

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [146] [Hello Afrika: Speech Commands in Kinyarwanda](https://arxiv.org/abs/2507.01024)
*George Igwegbe,Martins Awojide,Mboh Bless,Nirel Kadzo*

Main category: eess.AS

TL;DR: 当前非洲语言语音命令模型匮乏，Hello Afrika项目首阶段聚焦基尼亚卢旺达语，构建模型并部署评估。


<details>
  <summary>Details</summary>
Motivation: 解决非洲语言语音命令模型匮乏问题，且卢旺达对语音识别技术感兴趣有相关大数据集。

Method: 基于包含通用指令、数字和唤醒词的自定义语音命令语料库构建模型，在多设备部署。

Result: 未提及具体结果，仅表明用合适指标评估性能。

Conclusion: 未提及明确结论。

Abstract: Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a
language which are essential for non-contact control of and activation of
larger AI systems in devices used in everyday life especially for persons with
disabilities. Currently, there is a dearth of speech command models for African
languages. The Hello Afrika project aims to address this issue and its first
iteration is focused on the Kinyarwanda language since the country has shown
interest in developing speech recognition technologies culminating in one of
the largest datasets on Mozilla Common Voice. The model was built off a custom
speech command corpus made up of general directives, numbers, and a wake word.
The final model was deployed on multiple devices (PC, Mobile Phone and Edge
Devices) and the performance was assessed using suitable metrics.

</details>


### [147] [Workflow-Based Evaluation of Music Generation Systems](https://arxiv.org/abs/2507.01022)
*Shayan Dadman,Bernt Arild Bremdal,Andreas Bergsland*

Main category: eess.AS

TL;DR: 本文通过评估八个开源音乐生成系统，为当代音乐制作工作流程中的音乐生成系统提供探索性评估，发现系统主要起辅助作用，还贡献了评估框架并指明未来方向。


<details>
  <summary>Details</summary>
Motivation: 解决当前音乐生成系统在音乐制作中的局限，探讨其在工作流程整合中的挑战与机遇及作为协作工具的发展潜力。

Method: 结合技术见解和实践实验，采用单评估者方法初步探索，混合运用定性和定量方法。

Result: 音乐生成系统主要作为辅助工具，在保持主题和结构连贯性上有局限，凸显人类创造力的重要性。

Conclusion: 贡献结构化评估框架，确定后续评估的方法改进方向和人工智能在创意工作流程中的可行集成领域，为该领域未来发展提供实证见解。

Abstract: This study presents an exploratory evaluation of Music Generation Systems
(MGS) within contemporary music production workflows by examining eight
open-source systems. The evaluation framework combines technical insights with
practical experimentation through criteria specifically designed to investigate
the practical and creative affordances of the systems within the iterative,
non-linear nature of music production. Employing a single-evaluator methodology
as a preliminary phase, this research adopts a mixed approach utilizing
qualitative methods to form hypotheses subsequently assessed through
quantitative metrics. The selected systems represent architectural diversity
across both symbolic and audio-based music generation approaches, spanning
composition, arrangement, and sound design tasks. The investigation addresses
limitations of current MGS in music production, challenges and opportunities
for workflow integration, and development potential as collaborative tools
while maintaining artistic authenticity. Findings reveal these systems function
primarily as complementary tools enhancing rather than replacing human
expertise. They exhibit limitations in maintaining thematic and structural
coherence that emphasize the indispensable role of human creativity in tasks
demanding emotional depth and complex decision-making. This study contributes a
structured evaluation framework that considers the iterative nature of music
creation. It identifies methodological refinements necessary for subsequent
comprehensive evaluations and determines viable areas for AI integration as
collaborative tools in creative workflows. The research provides
empirically-grounded insights to guide future development in the field.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [148] [Prompt Mechanisms in Medical Imaging: A Comprehensive Survey](https://arxiv.org/abs/2507.01055)
*Hao Yang,Xinlong Liang,Zhang Li,Yue Sun,Zheyu Hu,Xinghe Xie,Behdad Dashtbozorg,Jincheng Huang,Shiwei Zhu,Luyi Han,Jiong Zhang,Shanshan Wang,Ritse Mann,Qifeng Yu,Tao Tan*

Main category: eess.IV

TL;DR: 本文系统回顾医学影像中提示工程，分析不同提示模态，揭示其优势，指出挑战并展望未来。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像临床应用遇数据稀缺等挑战，提示工程可引导模型，故开展对其的系统回顾。

Method: 剖析文本指令、视觉提示和可学习嵌入等提示模态，并分析其在影像生成、分割和分类等核心任务中的集成情况。

Result: 提示机制可提高准确性、鲁棒性和数据效率，减少手动特征工程依赖，增强模型可解释性。

Conclusion: 提示工程虽有进展，但在提示设计优化等方面有挑战，未来有高级多模态提示和临床集成等方向。

Abstract: Deep learning offers transformative potential in medical imaging, yet its
clinical adoption is frequently hampered by challenges such as data scarcity,
distribution shifts, and the need for robust task generalization. Prompt-based
methodologies have emerged as a pivotal strategy to guide deep learning models,
providing flexible, domain-specific adaptations that significantly enhance
model performance and adaptability without extensive retraining. This
systematic review critically examines the burgeoning landscape of prompt
engineering in medical imaging. We dissect diverse prompt modalities, including
textual instructions, visual prompts, and learnable embeddings, and analyze
their integration for core tasks such as image generation, segmentation, and
classification. Our synthesis reveals how these mechanisms improve
task-specific outcomes by enhancing accuracy, robustness, and data efficiency
and reducing reliance on manual feature engineering while fostering greater
model interpretability by making the model's guidance explicit. Despite
substantial advancements, we identify persistent challenges, particularly in
prompt design optimization, data heterogeneity, and ensuring scalability for
clinical deployment. Finally, this review outlines promising future
trajectories, including advanced multimodal prompting and robust clinical
integration, underscoring the critical role of prompt-driven AI in accelerating
the revolution of diagnostics and personalized treatment planning in medicine.

</details>


### [149] [CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation](https://arxiv.org/abs/2506.23121)
*Xinlei Yu,Chanmiao Wang,Hui Jin,Ahmed Elazab,Gangyong Jia,Xiang Wan,Changqing Zou,Ruiquan Ge*

Main category: eess.IV

TL;DR: 本文提出基于SAM2的CRISP - SAM2模型用于多器官医学分割，在七个公共数据集实验中表现优于现有模型，代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前多器官分割模型存在细节不准确、依赖几何提示和空间信息丢失等问题，需改进。

Method: 用渐进式交叉注意力交互机制将视觉和文本输入转换为跨模态上下文语义注入图像编码器；用语义提示策略替换原提示编码器；采用记忆的相似度排序自更新策略和掩码细化过程。

Result: 在七个公共数据集上的对比实验显示CRISP - SAM2优于现有模型。

Conclusion: CRISP - SAM2性能优越，能有效解决现有模型的局限。

Abstract: Multi-organ medical segmentation is a crucial component of medical image
processing, essential for doctors to make accurate diagnoses and develop
effective treatment plans. Despite significant progress in this field, current
multi-organ segmentation models often suffer from inaccurate details,
dependence on geometric prompts and loss of spatial information. Addressing
these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal
Interaction and Semantic Prompting based on SAM2. This model represents a
promising approach to multi-organ medical segmentation guided by textual
descriptions of organs. Our method begins by converting visual and textual
inputs into cross-modal contextualized semantics using a progressive
cross-attention interaction mechanism. These semantics are then injected into
the image encoder to enhance the detailed understanding of visual information.
To eliminate reliance on geometric prompts, we use a semantic prompting
strategy, replacing the original prompt encoder to sharpen the perception of
challenging targets. In addition, a similarity-sorting self-updating strategy
for memory and a mask-refining process is applied to further adapt to medical
imaging and enhance localized details. Comparative experiments conducted on
seven public datasets indicate that CRISP-SAM2 outperforms existing models.
Extensive analysis also demonstrates the effectiveness of our method, thereby
confirming its superior performance, especially in addressing the limitations
mentioned earlier. Our code is available at:
https://github.com/YU-deep/CRISP\_SAM2.git.

</details>


### [150] [SWinMamba: Serpentine Window State Space Model for Vascular Segmentation](https://arxiv.org/abs/2507.01323)
*Rongchang Zhao,Huanchi Liu,Jian Zhang*

Main category: eess.IV

TL;DR: 提出SWinMamba进行血管分割，在三个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像血管分割中因血管细长和先验建模不足导致分割结构不连续的问题。

Method: 将蛇形窗口序列融入双向状态空间模型，用SWToken自适应分割输入图像，BAM整合局部特征，设计SFFU进行双域学习。

Result: 在三个具有挑战性的数据集上，SWinMamba实现了完整且连通的血管分割，性能优越。

Conclusion: SWinMamba能有效实现准确的血管分割。

Abstract: Vascular segmentation in medical images is crucial for disease diagnosis and
surgical navigation. However, the segmented vascular structure is often
discontinuous due to its slender nature and inadequate prior modeling. In this
paper, we propose a novel Serpentine Window Mamba (SWinMamba) to achieve
accurate vascular segmentation. The proposed SWinMamba innovatively models the
continuity of slender vascular structures by incorporating serpentine window
sequences into bidirectional state space models. The serpentine window
sequences enable efficient feature capturing by adaptively guiding global
visual context modeling to the vascular structure. Specifically, the Serpentine
Window Tokenizer (SWToken) adaptively splits the input image using overlapping
serpentine window sequences, enabling flexible receptive fields (RFs) for
vascular structure modeling. The Bidirectional Aggregation Module (BAM)
integrates coherent local features in the RFs for vascular continuity
representation. In addition, dual-domain learning with Spatial-Frequency Fusion
Unit (SFFU) is designed to enhance the feature representation of vascular
structure. Extensive experiments on three challenging datasets demonstrate that
the proposed SWinMamba achieves superior performance with complete and
connected vessels.

</details>


### [151] [A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs](https://arxiv.org/abs/2507.01881)
*Niccolò McConnell,Pardeep Vasudev,Daisuke Yamada,Daryl Cheng,Mehran Azimbagirad,John McCabe,Shahab Aslani,Ahmed H. Shahin,Yukun Zhou,The SUMMIT Consortium,Andre Altmann,Yipeng Hu,Paul Taylor,Sam M. Janes,Daniel C. Alexander,Joseph Jacob*

Main category: eess.IV

TL;DR: 介绍了用于容积LDCT分析的开源模型TANGERINE，它计算成本低，性能好，有望推动肺癌筛查项目从单一癌症检测转向综合呼吸道疾病管理。


<details>
  <summary>Details</summary>
Motivation: 全球肺癌筛查项目中LDCT成像应用增多，但缺乏放射科医生大规模解读扫描结果，需开发合适模型。

Method: 使用自监督学习在超98,000个胸部LDCT上预训练，将掩码自编码器框架扩展到3D成像。

Result: TANGERINE在14种疾病分类任务中达先进水平，微调收敛快，标签效率高，跨临床中心泛化性强。

Conclusion: TANGERINE架构简单、公开可用、计算需求适度，为下一代医学成像工具集成奠定基础，可推动肺癌筛查项目转变。

Abstract: Low-dose computed tomography (LDCT) imaging employed in lung cancer screening
(LCS) programs is increasing in uptake worldwide. LCS programs herald a
generational opportunity to simultaneously detect cancer and non-cancer-related
early-stage lung disease. Yet these efforts are hampered by a shortage of
radiologists to interpret scans at scale. Here, we present TANGERINE, a
computationally frugal, open-source vision foundation model for volumetric LDCT
analysis. Designed for broad accessibility and rapid adaptation, TANGERINE can
be fine-tuned off the shelf for a wide range of disease-specific tasks with
limited computational resources and training data. Relative to models trained
from scratch, TANGERINE demonstrates fast convergence during fine-tuning,
thereby requiring significantly fewer GPU hours, and displays strong label
efficiency, achieving comparable or superior performance with a fraction of
fine-tuning data. Pretrained using self-supervised learning on over 98,000
thoracic LDCTs, including the UK's largest LCS initiative to date and 27 public
datasets, TANGERINE achieves state-of-the-art performance across 14 disease
classification tasks, including lung cancer and multiple respiratory diseases,
while generalising robustly across diverse clinical centres. By extending a
masked autoencoder framework to 3D imaging, TANGERINE offers a scalable
solution for LDCT analysis, departing from recent closed, resource-intensive
models by combining architectural simplicity, public availability, and modest
computational requirements. Its accessible, open-source lightweight design lays
the foundation for rapid integration into next-generation medical imaging tools
that could transform LCS initiatives, allowing them to pivot from a singular
focus on lung cancer detection to comprehensive respiratory disease management
in high-risk populations.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [152] [Age Sensitive Hippocampal Functional Connectivity: New Insights from 3D CNNs and Saliency Mapping](https://arxiv.org/abs/2507.01411)
*Yifei Sun,Marshall A. Dalton,Robert D. Sanders,Yixuan Yuan,Xiang Li,Sharon L. Naismith,Fernando Calamante,Jinglei Lv*

Main category: q-bio.NC

TL;DR: 开发可解释深度学习框架，用3D CNN结合LayerCAM从海马体功能连接预测脑年龄，揭示海马体老化功能机制。


<details>
  <summary>Details</summary>
Motivation: 海马体灰质损失是神经生物学老化标志，但对其功能连接变化理解有限，需深入研究老化过程中的功能重组。

Method: 开发可解释深度学习框架，用3D CNN结合LayerCAM显著性映射从海马体功能连接预测脑年龄。

Result: 找出对年龄高度敏感的关键海马 - 皮质连接，区分前后海马体功能连接有不同映射。

Conclusion: 研究为海马体老化功能机制提供新见解，证明可解释深度学习在神经影像数据中发现生物学意义模式的能力。

Abstract: Grey matter loss in the hippocampus is a hallmark of neurobiological aging,
yet understanding the corresponding changes in its functional connectivity
remains limited. Seed-based functional connectivity (FC) analysis enables
voxel-wise mapping of the hippocampus's synchronous activity with cortical
regions, offering a window into functional reorganization during aging. In this
study, we develop an interpretable deep learning framework to predict brain age
from hippocampal FC using a three-dimensional convolutional neural network (3D
CNN) combined with LayerCAM saliency mapping. This approach maps key
hippocampal-cortical connections, particularly with the precuneus, cuneus,
posterior cingulate cortex, parahippocampal cortex, left superior parietal
lobule, and right superior temporal sulcus, that are highly sensitive to age.
Critically, disaggregating anterior and posterior hippocampal FC reveals
distinct mapping aligned with their known functional specializations. These
findings provide new insights into the functional mechanisms of hippocampal
aging and demonstrate the power of explainable deep learning to uncover
biologically meaningful patterns in neuroimaging data.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [153] [Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives](https://arxiv.org/abs/2507.01198)
*Benjamin Kraljusic,Zlatan Ajanovic,Nermin Covic,Bakir Lacevic*

Main category: cs.RO

TL;DR: 本文提出结合采样和搜索方法的机器人机械臂运动规划算法，使用burs作为自适应运动基元，在SMPL库实现并评估，复杂场景表现优于固定基元规划。


<details>
  <summary>Details</summary>
Motivation: 为机器人机械臂设计更高效的运动规划算法，减少寻找有效路径的时间和扩展次数。

Method: 结合采样和搜索方法，在图搜索算法中使用burs作为自适应运动基元，在SMPL库中实现算法。

Result: 在不同自由度和环境复杂度场景中评估，bur-based方法在复杂场景，尤其高自由度机械臂表现更优，简单场景性能相当。

Conclusion: bur-based方法在复杂场景中比固定基元规划更有效。

Abstract: This work proposes a motion planning algorithm for robotic manipulators that
combines sampling-based and search-based planning methods. The core
contribution of the proposed approach is the usage of burs of free
configuration space (C-space) as adaptive motion primitives within the graph
search algorithm. Due to their feature to adaptively expand in free C-space,
burs enable more efficient exploration of the configuration space compared to
fixed-sized motion primitives, significantly reducing the time to find a valid
path and the number of required expansions. The algorithm is implemented within
the existing SMPL (Search-Based Motion Planning Library) library and evaluated
through a series of different scenarios involving manipulators with varying
number of degrees-of-freedom (DoF) and environment complexity. Results
demonstrate that the bur-based approach outperforms fixed-primitive planning in
complex scenarios, particularly for high DoF manipulators, while achieving
comparable performance in simpler scenarios.

</details>


### [154] [LLM-based Realistic Safety-Critical Driving Video Generation](https://arxiv.org/abs/2507.01264)
*Yongjie Fu,Ruijian Zha,Pei Tian,Xuan Di*

Main category: cs.RO

TL;DR: 提出利用大语言模型进行少样本代码生成以在CARLA模拟器中自动合成驾驶场景，结合视频生成管道，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 设计多样且安全关键的驾驶场景对评估自动驾驶系统至关重要。

Method: 利用大语言模型进行少样本代码生成，在CARLA模拟器中合成驾驶场景，结合Cosmos - Transfer1和ControlNet的视频生成管道。

Result: 能生成广泛的现实、多样且安全关键的场景。

Conclusion: 该方法为基于仿真的自动驾驶车辆测试提供了有前景的工具。

Abstract: Designing diverse and safety-critical driving scenarios is essential for
evaluating autonomous driving systems. In this paper, we propose a novel
framework that leverages Large Language Models (LLMs) for few-shot code
generation to automatically synthesize driving scenarios within the CARLA
simulator, which has flexibility in scenario scripting, efficient code-based
control of traffic participants, and enforcement of realistic physical
dynamics. Given a few example prompts and code samples, the LLM generates
safety-critical scenario scripts that specify the behavior and placement of
traffic participants, with a particular focus on collision events. To bridge
the gap between simulation and real-world appearance, we integrate a video
generation pipeline using Cosmos-Transfer1 with ControlNet, which converts
rendered scenes into realistic driving videos. Our approach enables
controllable scenario generation and facilitates the creation of rare but
critical edge cases, such as pedestrian crossings under occlusion or sudden
vehicle cut-ins. Experimental results demonstrate the effectiveness of our
method in generating a wide range of realistic, diverse, and safety-critical
scenarios, offering a promising tool for simulation-based testing of autonomous
vehicles.

</details>


### [155] [VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process](https://arxiv.org/abs/2507.01284)
*Cristian Gariboldi,Hayato Tokida,Ken Kinjo,Yuki Asada,Alexander Carballo*

Main category: cs.RO

TL;DR: 本文提出视觉语言自动驾驶模型VLAD，通过定制微调方法提高空间推理能力，减少碰撞率，为VLM增强自动驾驶系统树立新标杆。


<details>
  <summary>Details</summary>
Motivation: 利用开源视觉语言模型的互联网级常识，提升自动驾驶的感知、预测和规划能力。

Method: 提出VLAD模型，将微调后的VLM与VAD集成，用定制问答数据集微调提升空间推理能力。

Result: 在nuScenes数据集上评估，集成系统比基线方法平均碰撞率降低31.82%。

Conclusion: VLAD为VLM增强自动驾驶系统建立了新的基准，增加了决策透明度和可信度。

Abstract: Recent advancements in open-source Visual Language Models (VLMs) such as
LLaVA, Qwen-VL, and Llama have catalyzed extensive research on their
integration with diverse systems. The internet-scale general knowledge
encapsulated within these models presents significant opportunities for
enhancing autonomous driving perception, prediction, and planning capabilities.
In this paper we propose VLAD, a vision-language autonomous driving model,
which integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end
system. We implement a specialized fine-tuning approach using custom
question-answer datasets designed specifically to improve the spatial reasoning
capabilities of the model. The enhanced VLM generates high-level navigational
commands that VAD subsequently processes to guide vehicle operation.
Additionally, our system produces interpretable natural language explanations
of driving decisions, thereby increasing transparency and trustworthiness of
the traditionally black-box end-to-end architecture. Comprehensive evaluation
on the real-world nuScenes dataset demonstrates that our integrated system
reduces average collision rates by 31.82% compared to baseline methodologies,
establishing a new benchmark for VLM-augmented autonomous driving systems.

</details>


### [156] [Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0](https://arxiv.org/abs/2507.01462)
*Eneko Osaba,Estibaliz Garrote,Pablo Miranda-Rodriguez,Alessia Ciacco,Itziar Cabanes,Aitziber Mancisidor*

Main category: cs.RO

TL;DR: 研究混合量子 - 经典算法优化工业中基于CAD模型的机器人检查轨迹，对比量子与经典方法，结果显示量子方法有潜力。


<details>
  <summary>Details</summary>
Motivation: 探索混合量子 - 经典算法在工业中优化基于CAD模型的机器人检查轨迹的应用。

Method: 将任务建模为3D旅行商问题，结合不完全图和开放路径约束，评估两种基于D - Wave的求解器并与经典方法对比。

Result: 在五个实际案例中，量子方法求解质量有竞争力且计算时间显著减少。

Conclusion: 量子方法在工业4.0自动化中有潜力。

Abstract: This work explores the application of hybrid quantum-classical algorithms to
optimize robotic inspection trajectories derived from Computer-Aided Design
(CAD) models in industrial settings. By modeling the task as a 3D variant of
the Traveling Salesman Problem, incorporating incomplete graphs and open-route
constraints, this study evaluates the performance of two D-Wave-based solvers
against classical methods such as GUROBI and Google OR-Tools. Results across
five real-world cases demonstrate competitive solution quality with
significantly reduced computation times, highlighting the potential of quantum
approaches in automation under Industry 4.0.

</details>


### [157] [BioMARS: A Multi-Agent Robotic System for Autonomous Biological Experiments](https://arxiv.org/abs/2507.01485)
*Yibo Qiu,Zan Huang,Zhiyu Wang,Handi Liu,Yiling Qiao,Yifeng Hu,Shu'ang Sun,Hangke Peng,Ronald X Xu,Mingzhai Sun*

Main category: cs.RO

TL;DR: 介绍智能平台BioMARS，集成多种模型与机器人，自主完成生物实验，表现良好，凸显AI驱动实验室自动化可行性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型和视觉语言模型在生物研究应用受刚性协议设计、适应性等限制，需开发新系统。

Method: BioMARS采用分层架构，生物学家代理合成协议，技术员代理转化为伪代码，检查员代理确保程序完整性。

Result: 自主完成细胞传代和培养任务，表现优于手动；支持上下文感知优化，在视网膜色素上皮细胞分化中表现更好。

Conclusion: 证明了通用的、AI驱动的实验室自动化可行性以及基于语言推理在生物研究中的变革性作用。

Abstract: Large language models (LLMs) and vision-language models (VLMs) have the
potential to transform biological research by enabling autonomous
experimentation. Yet, their application remains constrained by rigid protocol
design, limited adaptability to dynamic lab conditions, inadequate error
handling, and high operational complexity. Here we introduce BioMARS
(Biological Multi-Agent Robotic System), an intelligent platform that
integrates LLMs, VLMs, and modular robotics to autonomously design, plan, and
execute biological experiments. BioMARS uses a hierarchical architecture: the
Biologist Agent synthesizes protocols via retrieval-augmented generation; the
Technician Agent translates them into executable robotic pseudo-code; and the
Inspector Agent ensures procedural integrity through multimodal perception and
anomaly detection. The system autonomously conducts cell passaging and culture
tasks, matching or exceeding manual performance in viability, consistency, and
morphological integrity. It also supports context-aware optimization,
outperforming conventional strategies in differentiating retinal pigment
epithelial cells. A web interface enables real-time human-AI collaboration,
while a modular backend allows scalable integration with laboratory hardware.
These results highlight the feasibility of generalizable, AI-driven laboratory
automation and the transformative role of language-based reasoning in
biological research.

</details>


### [158] [A Review on Sound Source Localization in Robotics: Focusing on Deep Learning Methods](https://arxiv.org/abs/2507.01143)
*Reza Jalayer,Masoud Jalayer,Amirali Baniasadi*

Main category: cs.RO

TL;DR: 本文聚焦机器人声源定位，回顾经典和现代方法，探讨数据与训练策略，指出挑战并给出未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有综述未充分考虑机器人约束和深度学习最新进展，本文旨在填补这些空白。

Method: 先回顾经典方法，再探讨现代机器学习和深度学习方法，按机器人类型和应用领域分类研究，分析数据与训练策略。

Result: 梳理了声源定位的多种方法，按不同维度分类研究，明确当前挑战。

Conclusion: 给出了下一代机器人实现鲁棒、自适应、高效和可解释的基于深度学习声源定位的可行路线图。

Abstract: Sound source localization (SSL) adds a spatial dimension to auditory
perception, allowing a system to pinpoint the origin of speech, machinery
noise, warning tones, or other acoustic events, capabilities that facilitate
robot navigation, human-machine dialogue, and condition monitoring. While
existing surveys provide valuable historical context, they typically address
general audio applications and do not fully account for robotic constraints or
the latest advancements in deep learning. This review addresses these gaps by
offering a robotics-focused synthesis, emphasizing recent progress in deep
learning methodologies. We start by reviewing classical methods such as Time
Difference of Arrival (TDOA), beamforming, Steered-Response Power (SRP), and
subspace analysis. Subsequently, we delve into modern machine learning (ML) and
deep learning (DL) approaches, discussing traditional ML and neural networks
(NNs), convolutional neural networks (CNNs), convolutional recurrent neural
networks (CRNNs), and emerging attention-based architectures. The data and
training strategy that are the two cornerstones of DL-based SSL are explored.
Studies are further categorized by robot types and application domains to
facilitate researchers in identifying relevant work for their specific
contexts. Finally, we highlight the current challenges in SSL works in general,
regarding environmental robustness, sound source multiplicity, and specific
implementation constraints in robotics, as well as data and learning strategies
in DL-based SSL. Also, we sketch promising directions to offer an actionable
roadmap toward robust, adaptable, efficient, and explainable DL-based SSL for
next-generation robots.

</details>


### [159] [Jump-Start Reinforcement Learning with Self-Evolving Priors for Extreme Monopedal Locomotion](https://arxiv.org/abs/2507.01243)
*Ziang Zheng,Guojian Zhan,Shiqi Liu,Yao Lyu,Tao Zhang,Shengbo Eben Li*

Main category: cs.RO

TL;DR: 提出JumpER框架解决四足机器人单足跳跃任务中极端欠驱动和极端地形挑战，让机器人首次在不可预测地形实现单足跳跃。


<details>
  <summary>Details</summary>
Motivation: 直接训练策略同时应对单足跳跃任务中的极端欠驱动和极端地形挑战有困难，早期交互不稳定且奖励反馈不可靠。

Method: 提出JumpER框架，将策略学习分为多个复杂度递增阶段，通过迭代引导先前学习的策略动态生成自进化先验。结合三阶段课程设置。

Result: 四足机器人首次在不可预测地形实现单足跳跃，策略能处理传统方法难应对的场景。

Conclusion: JumpER为解决极端欠驱动和极端地形下的运动任务提供了有原则且可扩展的方法。

Abstract: Reinforcement learning (RL) has shown great potential in enabling quadruped
robots to perform agile locomotion. However, directly training policies to
simultaneously handle dual extreme challenges, i.e., extreme underactuation and
extreme terrains, as in monopedal hopping tasks, remains highly challenging due
to unstable early-stage interactions and unreliable reward feedback. To address
this, we propose JumpER (jump-start reinforcement learning via self-evolving
priors), an RL training framework that structures policy learning into multiple
stages of increasing complexity. By dynamically generating self-evolving priors
through iterative bootstrapping of previously learned policies, JumpER
progressively refines and enhances guidance, thereby stabilizing exploration
and policy optimization without relying on external expert priors or
handcrafted reward shaping. Specifically, when integrated with a structured
three-stage curriculum that incrementally evolves action modality, observation
space, and task objective, JumpER enables quadruped robots to achieve robust
monopedal hopping on unpredictable terrains for the first time. Remarkably, the
resulting policy effectively handles challenging scenarios that traditional
methods struggle to conquer, including wide gaps up to 60 cm, irregularly
spaced stairs, and stepping stones with distances varying from 15 cm to 35 cm.
JumpER thus provides a principled and scalable approach for addressing
locomotion tasks under the dual challenges of extreme underactuation and
extreme terrains.

</details>


### [160] [AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation](https://arxiv.org/abs/2507.01961)
*Sixiang Chen,Jiaming Liu,Siyuan Qian,Han Jiang,Lily Li,Renrui Zhang,Zhuoyang Liu,Chenyang Gu,Chengkai Hou,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 现有移动操作方法在协调移动基座和机械臂方面存在问题，提出自适应协调扩散变压器（AC - DiT）并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在协调移动基座和机械臂时存在无法显式建模基座影响及未考虑不同阶段多模态感知需求的问题。

Method: 提出AC - DiT，引入移动到本体的调节机制，设计感知感知多模态调节策略。

Result: 通过大量模拟和真实世界移动操作任务实验验证了AC - DiT。

Conclusion: AC - DiT能增强移动基座和机械臂的协调，实现端到端移动操作。

Abstract: Recently, mobile manipulation has attracted increasing attention for enabling
language-conditioned robotic control in household tasks. However, existing
methods still face challenges in coordinating mobile base and manipulator,
primarily due to two limitations. On the one hand, they fail to explicitly
model the influence of the mobile base on manipulator control, which easily
leads to error accumulation under high degrees of freedom. On the other hand,
they treat the entire mobile manipulation process with the same visual
observation modality (e.g., either all 2D or all 3D), overlooking the distinct
multimodal perception requirements at different stages during mobile
manipulation. To address this, we propose the Adaptive Coordination Diffusion
Transformer (AC-DiT), which enhances mobile base and manipulator coordination
for end-to-end mobile manipulation. First, since the motion of the mobile base
directly influences the manipulator's actions, we introduce a mobility-to-body
conditioning mechanism that guides the model to first extract base motion
representations, which are then used as context prior for predicting whole-body
actions. This enables whole-body control that accounts for the potential impact
of the mobile base's motion. Second, to meet the perception requirements at
different stages of mobile manipulation, we design a perception-aware
multimodal conditioning strategy that dynamically adjusts the fusion weights
between various 2D visual images and 3D point clouds, yielding visual features
tailored to the current perceptual needs. This allows the model to, for
example, adaptively rely more on 2D inputs when semantic information is crucial
for action prediction, while placing greater emphasis on 3D geometric
information when precise spatial understanding is required. We validate AC-DiT
through extensive experiments on both simulated and real-world mobile
manipulation tasks.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [161] [Symbolic identification of tensor equations in multidimensional physical fields](https://arxiv.org/abs/2507.01466)
*Tianyi Chen,Hao Yang,Wenjun Ma,Jun Zhang*

Main category: math-ph

TL;DR: 提出用于识别张量方程的SITE框架，结合创新策略，通过基准场景验证其有效性，展示了数据驱动发现张量方程的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法多局限于标量方程，难以识别张量关系，因此需要一种能识别张量方程的方法。

Method: 提出SITE框架，用宿主 - 质粒结构表示张量方程，采用遗传信息保留策略，引入维度同质性检查和张量线性回归技术。

Result: 在两个基准场景中准确恢复目标方程，对噪声和小样本具有鲁棒性，能从分子模拟数据中识别本构关系。

Conclusion: SITE框架有潜力用于数据驱动的张量方程发现。

Abstract: Recently, data-driven methods have shown great promise for discovering
governing equations from simulation or experimental data. However, most
existing approaches are limited to scalar equations, with few capable of
identifying tensor relationships. In this work, we propose a general
data-driven framework for identifying tensor equations, referred to as Symbolic
Identification of Tensor Equations (SITE). The core idea of SITE--representing
tensor equations using a host-plasmid structure--is inspired by the
multidimensional gene expression programming (M-GEP) approach. To improve the
robustness of the evolutionary process, SITE adopts a genetic information
retention strategy. Moreover, SITE introduces two key innovations beyond
conventional evolutionary algorithms. First, it incorporates a dimensional
homogeneity check to restrict the search space and eliminate physically invalid
expressions. Second, it replaces traditional linear scaling with a tensor
linear regression technique, greatly enhancing the efficiency of numerical
coefficient optimization. We validate SITE using two benchmark scenarios, where
it accurately recovers target equations from synthetic data, showing robustness
to noise and small sample sizes. Furthermore, SITE is applied to identify
constitutive relations directly from molecular simulation data, which are
generated without reliance on macroscopic constitutive models. It adapts to
both compressible and incompressible flow conditions and successfully
identifies the corresponding macroscopic forms, highlighting its potential for
data-driven discovery of tensor equation.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [162] [Automated Vehicles Should be Connected with Natural Language](https://arxiv.org/abs/2507.01059)
*Xiangbo Gao,Keshu Wu,Hao Zhang,Kexin Tian,Yang Zhou,Zhengzhong Tu*

Main category: cs.MA

TL;DR: 本文指出多智能体协作驾驶现有通信介质有局限，传统方法忽视决策级融合，提出用自然语言进行意图和推理通信以解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有通信介质在带宽效率、信息完整性和智能体互操作性方面存在局限，传统方法忽视决策级融合。

Method: 从纯感知导向的数据交换过渡到使用自然语言进行明确的意图和推理通信。

Result: 自然语言可平衡语义密度和通信带宽，灵活适应实时条件，连接异构智能体平台，将协作驾驶从被动感知数据共享转变为主动协调。

Conclusion: 使用自然语言通信可提高智能交通系统的安全性、效率和透明度。

Abstract: Multi-agent collaborative driving promises improvements in traffic safety and
efficiency through collective perception and decision making. However, existing
communication media -- including raw sensor data, neural network features, and
perception results -- suffer limitations in bandwidth efficiency, information
completeness, and agent interoperability. Moreover, traditional approaches have
largely ignored decision-level fusion, neglecting critical dimensions of
collaborative driving. In this paper we argue that addressing these challenges
requires a transition from purely perception-oriented data exchanges to
explicit intent and reasoning communication using natural language. Natural
language balances semantic density and communication bandwidth, adapts flexibly
to real-time conditions, and bridges heterogeneous agent platforms. By enabling
the direct communication of intentions, rationales, and decisions, it
transforms collaborative driving from reactive perception-data sharing into
proactive coordination, advancing safety, efficiency, and transparency in
intelligent transportation systems.

</details>


### [163] [RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms](https://arxiv.org/abs/2507.01378)
*Ziyao Wang,Rongpeng Li,Sizhao Li,Yuming Xiang,Haiping Wang,Zhifeng Zhao,Honggang Zhang*

Main category: cs.MA

TL;DR: 提出RALLY算法解决UAV集群智能控制问题，实验表明其在多方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统MARL方法有语义差距和角色结构刚性问题，基于LLM的控制框架缺乏在线学习和有效探索能力，需新算法解决。

Method: 开发LLM驱动的语义决策框架，引入动态角色异质性机制，提出基于RMIX的分配策略实现半离线训练。

Result: 在MPE环境和SITL平台实验显示，RALLY在任务覆盖、收敛速度和泛化性上优于传统方法。

Conclusion: RALLY在多UAV系统协作导航方面有很大潜力。

Abstract: Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as
a critical research focus, and it typically requires the swarm to navigate
effectively while avoiding obstacles and achieving continuous coverage over
multiple mission targets. Although traditional Multi-Agent Reinforcement
Learning (MARL) approaches offer dynamic adaptability, they are hindered by the
semantic gap in numerical communication and the rigidity of homogeneous role
structures, resulting in poor generalization and limited task scalability.
Recent advances in Large Language Model (LLM)-based control frameworks
demonstrate strong semantic reasoning capabilities by leveraging extensive
prior knowledge. However, due to the lack of online learning and over-reliance
on static priors, these works often struggle with effective exploration,
leading to reduced individual potential and overall system performance. To
address these limitations, we propose a Role-Adaptive LLM-Driven Yoked
navigation algorithm RALLY. Specifically, we first develop an LLM-driven
semantic decision framework that uses structured natural language for efficient
semantic communication and collaborative reasoning. Afterward, we introduce a
dynamic role-heterogeneity mechanism for adaptive role switching and
personalized decision-making. Furthermore, we propose a Role-value Mixing
Network (RMIX)-based assignment strategy that integrates LLM offline priors
with MARL online policies to enable semi-offline training of role selection
strategies. Experiments in the Multi-Agent Particle Environment (MPE)
environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY
outperforms conventional approaches in terms of task coverage, convergence
speed, and generalization, highlighting its strong potential for collaborative
navigation in agentic multi-UAV systems.

</details>


### [164] [Exploring Advanced LLM Multi-Agent Systems Based on Blackboard Architecture](https://arxiv.org/abs/2507.01701)
*Bochen Han,Songmao Zhang*

Main category: cs.MA

TL;DR: 提出将黑板架构融入大语言模型多智能体系统，实现信息共享、智能体选择，实验表明系统有竞争力且省代币，有解决复杂动态问题潜力。


<details>
  <summary>Details</summary>
Motivation: 让大语言模型多智能体系统在问题解决过程中更好地共享信息，实现复杂动态问题解决。

Method: 将黑板架构融入大语言模型多智能体系统，根据黑板内容选择行动智能体并重复选择执行过程直至达成共识。

Result: 系统在常识知识、推理和数学数据集实验中平均性能最佳，能与SOTA静态和动态多智能体系统竞争，且花费代币更少。

Conclusion: 该提案有潜力解决无明确结构或工作流的复杂动态问题。

Abstract: In this paper, we propose to incorporate the blackboard architecture into LLM
multi-agent systems (MASs) so that (1) agents with various roles can share all
the information and others' messages during the whole problem-solving process,
(2) agents that will take actions are selected based on the current content of
the blackboard, and (3) the selection and execution round is repeated until a
consensus is reached on the blackboard. We develop the first implementation of
this proposal and conduct experiments on commonsense knowledge, reasoning and
mathematical datasets. The results show that our system can be competitive with
the SOTA static and dynamic MASs by achieving the best average performance, and
at the same time manage to spend less tokens. Our proposal has the potential to
enable complex and dynamic problem-solving where well-defined structures or
workflows are unavailable.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [165] [Hardware-software co-exploration with racetrack memory based in-memory computing for CNN inference in embedded systems](https://arxiv.org/abs/2507.01429)
*Benjamin Chen Ming Choong,Tao Luo,Cheng Liu,Bingsheng He,Wei Zhang,Joey Tianyi Zhou*

Main category: cs.ET

TL;DR: 提出针对赛道内存优化的高效内存卷积神经网络加速器，实现小内存区域、提升能量和性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络数据处理对低资源嵌入式系统有挑战，赛道内存适合内存计算，但构建高效内存算术电路有困难。

Method: 设计适用于乘加运算的基本算术电路，采用模型 - 系统协同设计优化。

Result: 设计的电路和协同优化策略实现小内存库面积，显著提升基于赛道内存的嵌入式系统能量和性能。

Conclusion: 所提方法能有效解决基于赛道内存构建高效内存算术电路的问题，提升嵌入式系统性能。

Abstract: Deep neural networks generate and process large volumes of data, posing
challenges for low-resource embedded systems. In-memory computing has been
demonstrated as an efficient computing infrastructure and shows promise for
embedded AI applications. Among newly-researched memory technologies, racetrack
memory is a non-volatile technology that allows high data density fabrication,
making it a good fit for in-memory computing. However, integrating in-memory
arithmetic circuits with memory cells affects both the memory density and power
efficiency. It remains challenging to build efficient in-memory arithmetic
circuits on racetrack memory within area and energy constraints. To this end,
we present an efficient in-memory convolutional neural network (CNN)
accelerator optimized for use with racetrack memory. We design a series of
fundamental arithmetic circuits as in-memory computing cells suited for
multiply-and-accumulate operations. Moreover, we explore the design space of
racetrack memory based systems and CNN model architectures, employing co-design
to improve the efficiency and performance of performing CNN inference in
racetrack memory while maintaining model accuracy. Our designed circuits and
model-system co-optimization strategies achieve a small memory bank area with
significant improvements in energy and performance for racetrack memory based
embedded systems.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [166] [STEM Diffraction Pattern Analysis with Deep Learning Networks](https://arxiv.org/abs/2507.01889)
*Sebastian Wissel,Jonas Scheunert,Aaron Dextre,Shamail Ahmed,Andreas Bayer,Kerstin Volz,Bai-Xiang Xu*

Main category: cond-mat.dis-nn

TL;DR: 本文提出基于机器学习从扫描透射电子显微镜衍射图预测欧拉角的方法，评估三种深度学习架构，Swin Transformer 表现最佳，显示了结合机器学习与 STEM 数据用于微观结构表征的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统取向映射方法处理复杂或重叠图案时速度慢且对噪声敏感，成为大规模微观结构分析瓶颈，需准确的晶粒取向映射以理解和优化多晶材料性能。

Method: 提出基于机器学习的方法，从扫描透射电子显微镜衍射图直接预测欧拉角，评估卷积神经网络、密集卷积网络和 Swin Transformer 三种深度学习架构。

Result: DenseNets 和 Swin Transformers 表现优于 CNN，Swin Transformer 评估得分最高，预测最一致，得到的晶体图能清晰描绘晶界和晶粒内取向分布。

Conclusion: 结合先进机器学习模型与 STEM 数据可用于稳健、高通量的微观结构表征。

Abstract: Accurate grain orientation mapping is essential for understanding and
optimizing the performance of polycrystalline materials, particularly in
energy-related applications. Lithium nickel oxide (LiNiO$_{2}$) is a promising
cathode material for next-generation lithium-ion batteries, and its
electrochemical behaviour is closely linked to microstructural features such as
grain size and crystallographic orientations. Traditional orientation mapping
methods--such as manual indexing, template matching (TM), or Hough
transform-based techniques--are often slow and noise-sensitive when handling
complex or overlapping patterns, creating a bottleneck in large-scale
microstructural analysis. This work presents a machine learning-based approach
for predicting Euler angles directly from scanning transmission electron
microscopy (STEM) diffraction patterns (DPs). This enables the automated
generation of high-resolution crystal orientation maps, facilitating the
analysis of internal microstructures at the nanoscale. Three deep learning
architectures--convolutional neural networks (CNNs), Dense Convolutional
Networks (DenseNets), and Shifted Windows (Swin) Transformers--are evaluated,
using an experimentally acquired dataset labelled via a commercial TM
algorithm. While the CNN model serves as a baseline, both DenseNets and Swin
Transformers demonstrate superior performance, with the Swin Transformer
achieving the highest evaluation scores and the most consistent microstructural
predictions. The resulting crystal maps exhibit clear grain boundary
delineation and coherent intra-grain orientation distributions, underscoring
the potential of attention-based architectures for analyzing diffraction-based
image data. These findings highlight the promise of combining advanced machine
learning models with STEM data for robust, high-throughput microstructural
characterization.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [167] [Entropic optimal transport beyond product reference couplings: the Gaussian case on Euclidean space](https://arxiv.org/abs/2507.01709)
*Paul Freulon,Nikitas Georgakis,Victor Panaretos*

Main category: math.ST

TL;DR: 研究非乘积形式参考耦合的熵最优传输问题，聚焦高斯情况并将其转化为矩阵优化问题，给出解的完整描述，展示合适参考计划可减少熵惩罚偏差。


<details>
  <summary>Details</summary>
Motivation: 探讨参考耦合不一定为乘积形式的熵最优传输问题，在统计场景中增加参考测度的灵活性，如利用先验信息、处理测量不确定性和减少估计偏差。

Method: 聚焦高斯情况，将更一般的最优传输准则转化为矩阵优化问题。

Result: 提供了原变量和对偶变量方面解的完整描述，数值例子表明合适参考计划可减少熵惩罚偏差。

Conclusion: 参考测度的灵活性在统计场景中很重要，合适参考计划有助于减少熵最优传输问题中的偏差。

Abstract: The optimal transport problem with squared Euclidean cost consists in finding
a coupling between two input measures that maximizes correlation. Consequently,
the optimal coupling is often singular with respect to Lebesgue measure.
Regularizing the optimal transport problem with an entropy term yields an
approximation called entropic optimal transport. Entropic penalties steer the
induced coupling toward a reference measure with desired properties. For
instance, when seeking a diffuse coupling, the most popular reference measures
are the Lebesgue measure and the product of the two input measures. In this
work, we study the case where the reference coupling is not necessarily assumed
to be a product. We focus on the Gaussian case as a motivating paradigm, and
provide a reduction of this more general optimal transport criterion to a
matrix optimization problem. This reduction enables us to provide a complete
description of the solution, both in terms of the primal variable and the dual
variables. We argue that flexibility in terms of the reference measure can be
important in statistical contexts, for instance when one has prior information,
when there is uncertainty regarding the measures to be coupled, or to reduce
bias when the entropic problem is used to estimate the un-regularized transport
problem. In particular, we show in numerical examples that choosing a suitable
reference plan allows to reduce the bias caused by the entropic penalty.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [168] [Generative flow-based warm start of the variational quantum eigensolver](https://arxiv.org/abs/2507.01726)
*Hang Zou,Martin Rahm,Anton Frisk Kockum,Simon Olsson*

Main category: quant-ph

TL;DR: 提出Flow - VQE框架以解决混合量子 - 经典算法的优化问题，经分子系统数值模拟验证其性能优于基线算法。


<details>
  <summary>Details</summary>
Motivation: 混合量子 - 经典算法如VQE受复杂目标函数和昂贵优化程序限制，需改进优化方法。

Method: 提出Flow - VQE框架，将生成模型嵌入VQE优化循环，通过基于偏好的训练实现无量子梯度优化和参数转移。

Result: Flow - VQE在分子系统数值模拟中优于基线优化算法，减少电路评估次数，预热启动优化时加速后续微调。

Conclusion: Flow - VQE可成为利用生成建模降低变分量子算法成本的实用且通用范式。

Abstract: Hybrid quantum-classical algorithms like the variational quantum eigensolver
(VQE) show promise for quantum simulations on near-term quantum devices, but
are often limited by complex objective functions and expensive optimization
procedures. Here, we propose Flow-VQE, a generative framework leveraging
conditional normalizing flows with parameterized quantum circuits to
efficiently generate high-quality variational parameters. By embedding a
generative model into the VQE optimization loop through preference-based
training, Flow-VQE enables quantum gradient-free optimization and offers a
systematic approach for parameter transfer, accelerating convergence across
related problems through warm-started optimization. We compare Flow-VQE to a
number of standard benchmarks through numerical simulations on molecular
systems, including hydrogen chains, water, ammonia, and benzene. We find that
Flow-VQE outperforms baseline optimization algorithms, achieving computational
accuracy with fewer circuit evaluations (improvements range from modest to more
than two orders of magnitude) and, when used to warm-start the optimization of
new systems, accelerates subsequent fine-tuning by up to 50-fold compared with
Hartree--Fock initialization. Therefore, we believe Flow-VQE can become a
pragmatic and versatile paradigm for leveraging generative modeling to reduce
the costs of variational quantum algorithms.

</details>


### [169] [Efficient Gate Reordering for Distributed Quantum Compiling in Data Centers](https://arxiv.org/abs/2507.01090)
*Riccardo Mengoni,Walter Nadalin,Mathys Rennela,Jimmy Rotureau,Tom Darras,Julien Laurat,Eleni Diamanti,Ioannis Lavdas*

Main category: quant-ph

TL;DR: 介绍量子编译器araQne可减少量子电路分布成本，电路重排序策略作用关键。


<details>
  <summary>Details</summary>
Motivation: 量子计算时代需新基础设施和软件工具，减少量子处理单元间通信很重要。

Method: 设计量子编译器araQne，利用电路重排序策略。

Result: 电路重排序策略相比基线方法能大幅降低分布成本。

Conclusion: 电路重排序策略在降低量子电路分布成本中起关键作用。

Abstract: Just as classical computing relies on distributed systems, the quantum
computing era requires new kinds of infrastructure and software tools. Quantum
networks will become the backbone of hybrid, quantum-augmented data centers, in
which quantum algorithms are distributed over a local network of quantum
processing units (QPUs) interconnected via shared entanglement. In this
context, it is crucial to develop methods and software that minimize the number
of inter-QPU communications. Here we describe key features of the quantum
compiler araQne, which is designed to minimize distribution cost, measured by
the number of entangled pairs required to distribute a monolithic quantum
circuit using gate teleportation protocols. We establish the crucial role
played by circuit reordering strategies, which strongly reduce the distribution
cost compared to a baseline approach.

</details>


### [170] [Analyzing Common Electronic Structure Theory Algorithms for Distributed Quantum Computing](https://arxiv.org/abs/2507.01902)
*Grier M. Jones,Hans-Arno Jacobsen*

Main category: quant-ph

TL;DR: 为走向量子计算实用时代，分析常见量子化学电子结构方法与分布式量子计算适配性，发现多数算法难以用本地操作有效并行化。


<details>
  <summary>Details</summary>
Motivation: 推动量子计算进入实用时代，探索分布式量子计算框架下量子化学应用。

Method: 分析Tequila和ffsim等常见包中的五种电子结构方法，并使用本地操作切割算法。

Result: 许多算法无法用本地操作有效并行化。

Conclusion: 需开发新方法在分布式量子计算框架中应用电子结构理论。

Abstract: To move towards the utility era of quantum computing, many corporations have
posed distributed quantum computing (DQC) as a framework for scaling the
current generation of devices for practical applications. One of these
applications is quantum chemistry, also known as electronic structure theory,
which has been poised as a "killer application" of quantum computing, To this
end, we analyze five electronic structure methods, found in common packages
such as Tequila and ffsim, which can be easily interfaced with the Qiskit
Circuit Cutting addon. Herein, we provide insights into cutting these
algorithms using local operations (LO) to determine their aptitude for
distribution. The key findings of our work are that many of these algorithms
cannot be efficiently parallelized using LO, and new methods must be developed
to apply electronic structure theory within a DQC framework.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [171] [User-guided Generative Source Separation](https://arxiv.org/abs/2507.01339)
*Yutong Wen,Minje Kim,Paris Smaragdis*

Main category: cs.SD

TL;DR: 提出扩散式音乐源分离模型GuideSep，能进行超越四声道设置的乐器无关分离，评估显示其可实现高质量分离与更灵活的乐器提取。


<details>
  <summary>Details</summary>
Motivation: 现有音乐源分离方法多集中于四声道设置，缺乏实际应用所需的灵活性。

Method: 提出GuideSep模型，以波形模仿条件和梅尔频谱图域掩码为多输入条件；设计掩码预测基线以比较预测和生成方法。

Result: 客观和主观评估表明GuideSep实现了高质量分离，且能进行更灵活的乐器提取。

Conclusion: 凸显了用户参与基于扩散的生成过程在音乐源分离中的潜力。

Abstract: Music source separation (MSS) aims to extract individual instrument sources
from their mixture. While most existing methods focus on the widely adopted
four-stem separation setup (vocals, bass, drums, and other instruments), this
approach lacks the flexibility needed for real-world applications. To address
this, we propose GuideSep, a diffusion-based MSS model capable of
instrument-agnostic separation beyond the four-stem setup. GuideSep is
conditioned on multiple inputs: a waveform mimicry condition, which can be
easily provided by humming or playing the target melody, and mel-spectrogram
domain masks, which offer additional guidance for separation. Unlike prior
approaches that relied on fixed class labels or sound queries, our conditioning
scheme, coupled with the generative approach, provides greater flexibility and
applicability. Additionally, we design a mask-prediction baseline using the
same model architecture to systematically compare predictive and generative
approaches. Our objective and subjective evaluations demonstrate that GuideSep
achieves high-quality separation while enabling more versatile instrument
extraction, highlighting the potential of user participation in the
diffusion-based generative process for MSS. Our code and demo page are
available at https://yutongwen.github.io/GuideSep/

</details>


### [172] [Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware](https://arxiv.org/abs/2507.01563)
*Marco Giordano,Stefano Giacomelli,Claudia Rinaldi,Fabio Graziosi*

Main category: cs.SD

TL;DR: 提出适用于嵌入式硬件实时部署的应急车辆警报检测系统，基于E2PANNs，创建定制数据集，在树莓派5部署，性能评估显示其低延迟且鲁棒性好，可行于构建分布式声学监测网络。


<details>
  <summary>Details</summary>
Motivation: 开发可在嵌入式硬件实时部署的应急车辆警报检测系统，克服标准AudioSet注释可靠性低的问题。

Method: 基于E2PANNs的卷积神经网络，创建定制数据集，在树莓派5部署，实现多线程推理引擎并控制误激活，用WebSocket接口实时监控。

Result: 系统在现实音频条件下实现低延迟检测，鲁棒性提高。

Conclusion: 在低成本边缘设备上部署兼容IoS的SED解决方案可行，能构建分布式声学监测网络用于应急车辆跟踪。

Abstract: We present a full-stack emergency vehicle (EV) siren detection system
designed for real-time deployment on embedded hardware. The proposed approach
is based on E2PANNs, a fine-tuned convolutional neural network derived from
EPANNs, and optimized for binary sound event detection under urban acoustic
conditions. A key contribution is the creation of curated and semantically
structured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -
developed using a custom AudioSet-Tools framework to overcome the low
reliability of standard AudioSet annotations. The system is deployed on a
Raspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing
a multithreaded inference engine with adaptive frame sizing, probability
smoothing, and a decision-state machine to control false positive activations.
A remote WebSocket interface provides real-time monitoring and facilitates live
demonstration capabilities. Performance is evaluated using both framewise and
event-based metrics across multiple configurations. Results show the system
achieves low-latency detection with improved robustness under realistic audio
conditions. This work demonstrates the feasibility of deploying IoS-compatible
SED solutions that can form distributed acoustic monitoring networks, enabling
collaborative emergency vehicle tracking across smart city infrastructures
through WebSocket connectivity on low-cost edge devices.

</details>


### [173] [Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder](https://arxiv.org/abs/2507.01582)
*Jing Luo,Xinyu Yang,Jie Wei*

Main category: cs.SD

TL;DR: 本文提出XMVAE模型用于从头生成古典钢琴演奏，引入ECP表示，经评估其生成质量优于现有模型，预训练作曲分支可提升性能。


<details>
  <summary>Details</summary>
Motivation: 应对从头生成古典钢琴演奏的挑战，模拟创作过程中作曲家和钢琴家的双重角色。

Method: 引入ECP表示，提出XMVAE模型，含VQ - VAE和普通VAE两个分支，用类似Seq2Seq架构联合训练，利用多尺度编码器和正交Transformer解码器。

Result: 客观和主观评估表明XMVAE生成的古典演奏音乐质量优于现有模型，预训练作曲分支可显著提升性能。

Conclusion: XMVAE模型在生成古典钢琴演奏方面表现出色，预训练策略有积极效果。

Abstract: The creativity of classical music arises not only from composers who craft
the musical sheets but also from performers who interpret the static notations
with expressive nuances. This paper addresses the challenge of generating
classical piano performances from scratch, aiming to emulate the dual roles of
composer and pianist in the creative process. We introduce the Expressive
Compound Word (ECP) representation, which effectively captures both the
metrical structure and expressive nuances of classical performances. Building
on this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a
model featuring two branches: a Vector Quantized Variational AutoEncoder
(VQ-VAE) branch that generates score-related content, representing the
Composer, and a vanilla VAE branch that produces expressive details, fulfilling
the role of Pianist. These branches are jointly trained with similar Seq2Seq
architectures, leveraging a multiscale encoder to capture beat-level contextual
information and an orthogonal Transformer decoder for efficient compound tokens
decoding. Both objective and subjective evaluations demonstrate that XMVAE
generates classical performances with superior musical quality compared to
state-of-the-art models. Furthermore, pretraining the Composer branch on extra
musical score datasets contribute to a significant performance gain.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [174] [Enriching the Felsenthal index with a priori unions for decision-making processes](https://arxiv.org/abs/2507.01621)
*Alicia Mascareñas-Pazos,Silvia Lorenzo-Freire,Jose Maria Alonso-Meijide*

Main category: econ.TH

TL;DR: 本文提出Felsenthal Owen权力指数，基于Felsenthal方法并整合玩家亲密度，用公理性质刻画该指数，通过IMF投票系统展示其效用，为政策制定者提供决策工具。


<details>
  <summary>Details</summary>
Motivation: Felsenthal指数忽略玩家间的亲密度，而现实政治经济环境中亲密度是常见且有影响的因素，所以要提出新的权力指数。

Method: 基于Felsenthal的方法，利用Owen的先验联盟框架整合玩家亲密度，用两组不同的公理性质来刻画新指数。

Result: 将新指数应用于国际货币基金组织的投票系统，揭示了战略联盟对权力分配的显著影响。

Conclusion: 新指数为政策制定者在复杂决策场景中衡量影响力提供了更精细的工具。

Abstract: Within the domain of game theory, power indexes are defined as functions that
quantify the influence of individual participants in collective decision-making
processes. Felsenthal [D. Felsenthal. A Well-Behaved Index of a Priori P-Power
for Simple N-Person Games. Homo Oeconomicus, 33, 2016] proposed a power index
with a focus on least size winning coalitions, i.e., those coalitions capable
of determining the final outcome and with the smallest number of players among
all winning coalitions. However, the Felsenthal index overlooks pre-existing
affinities between the players, a common and impactful factor in real-world
political and economic contexts. This paper introduces the Felsenthal Owen
power index, a novel index based on Felsenthal's approach that integrates
player affinities using Owen's a priori unions framework. The new index is
rigorously characterised by two distinct sets of axiomatic properties. We
demonstrate its practical utility by applying it to the International Monetary
Fund's voting system, revealing how strategic alliances significantly reshape
power distributions. The index thus offers policymakers a more sophisticated
tool for measuring influence in complex decision-making scenarios.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [175] [Semi-supervised learning for linear extremile regression](https://arxiv.org/abs/2507.01314)
*Rong Jiang,Keming Yu,Jiangfeng Wang*

Main category: stat.ME

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Extremile regression, as a least squares analog of quantile regression, is
potentially useful tool for modeling and understanding the extreme tails of a
distribution. However, existing extremile regression methods, as nonparametric
approaches, may face challenges in high-dimensional settings due to data
sparsity, computational inefficiency, and the risk of overfitting. While linear
regression serves as the foundation for many other statistical and machine
learning models due to its simplicity, interpretability, and relatively easy
implementation, particularly in high-dimensional settings, this paper
introduces a novel definition of linear extremile regression along with an
accompanying estimation methodology. The regression coefficient estimators of
this method achieve $\sqrt{n}$-consistency, which nonparametric extremile
regression may not provide. In particular, while semi-supervised learning can
leverage unlabeled data to make more accurate predictions and avoid overfitting
to small labeled datasets in high-dimensional spaces, we propose a
semi-supervised learning approach to enhance estimation efficiency, even when
the specified linear extremile regression model may be misspecified. Both
simulation studies and real data analyses demonstrate the finite-sample
performance of our proposed methods.

</details>


### [176] [Targeted tuning of random forests for quantile estimation and prediction intervals](https://arxiv.org/abs/2507.01430)
*Matthew Berkowitz,Rachel MacKay Altman,Thomas M. Loughin*

Main category: stat.ME

TL;DR: 提出一种随机森林调优程序，改善分位数估计准确性并生成有效且较窄的预测区间。


<details>
  <summary>Details</summary>
Motivation: 标准随机森林构建方法的分位数估计偏差过大，需减少偏差。

Method: 提出最小化“分位数覆盖损失”（QCL）的调优程序，并将其应用于处理删失数据和随机生存森林。

Result: QCL调优的分位数估计覆盖概率更准确，同时降低了覆盖概率的估计均方误差。

Conclusion: QCL调优表现优越与目标一致，还探讨了该方法生成的预测区间的有效性和宽度。

Abstract: We present a novel tuning procedure for random forests (RFs) that improves
the accuracy of estimated quantiles and produces valid, relatively narrow
prediction intervals. While RFs are typically used to estimate mean responses
(conditional on covariates), they can also be used to estimate quantiles by
estimating the full distribution of the response. However, standard approaches
for building RFs often result in excessively biased quantile estimates. To
reduce this bias, our proposed tuning procedure minimizes "quantile coverage
loss" (QCL), which we define as the estimated bias of the marginal quantile
coverage probability estimate based on the out-of-bag sample. We adapt QCL
tuning to handle censored data and demonstrate its use with random survival
forests. We show that QCL tuning results in quantile estimates with more
accurate coverage probabilities than those achieved using default parameter
values or traditional tuning (using MSPE for uncensored data and C-index for
censored data), while also reducing the estimated MSE of these coverage
probabilities. We discuss how the superior performance of QCL tuning is linked
to its alignment with the estimation goal. Finally, we explore the validity and
width of prediction intervals created using this method.

</details>


### [177] [Nonparametric learning of heterogeneous graphical model on network-linked data](https://arxiv.org/abs/2507.01473)
*Yuwen Wang,Changyu Liu,Xin He,Junhui Wang*

Main category: stat.ME

TL;DR: 本文提出非参数图形模型，结合网络嵌入与非参数估计，解决传统图形模型局限，有理论保证且经实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统图形模型基于独立同分布观测，对复杂数据集适用性有限，本文旨在解决这些局限。

Method: 提出非参数图形模型，将网络嵌入与非参数图形模型估计有效整合，利用向量值再生核希尔伯特空间性质将图学习任务转化为求解有限维线性方程组。

Result: 建立了估计一致性和精确恢复异质图结构的理论保证，通过模拟示例和实际应用证明了方法的有效性。

Conclusion: 所提方法能有效处理复杂数据集，无需特定分布假设，可准确恢复异质图结构。

Abstract: Graphical models have been popularly used for capturing conditional
independence structure in multivariate data, which are often built upon
independent and identically distributed observations, limiting their
applicability to complex datasets such as network-linked data. This paper
proposes a nonparametric graphical model that addresses these limitations by
accommodating heterogeneous graph structures without imposing any specific
distributional assumptions. The proposed estimation method effectively
integrates network embedding with nonparametric graphical model estimation. It
further transforms the graph learning task into solving a finite-dimensional
linear equation system by leveraging the properties of vector-valued
reproducing kernel Hilbert space. Moreover, theoretical guarantees are
established for the proposed method in terms of the estimation consistency and
exact recovery of the heterogeneous graph structures. Its effectiveness is also
demonstrated through a variety of simulated examples and a real application to
the statistician coauthorship dataset.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [178] [Automated Classification of Volcanic Earthquakes Using Transformer Encoders: Insights into Data Quality and Model Interpretability](https://arxiv.org/abs/2507.01260)
*Y. Suzuki,Y. Yukutake,T. Ohminato,M. Yamasaki,Ahyi Kim*

Main category: physics.geo-ph

TL;DR: 本文提出基于Transformer编码器的深度学习模型进行火山地震分类，在浅间山地震数据上表现良好，指出数据问题及部分影响因素，强调模型潜力与应用前景。


<details>
  <summary>Details</summary>
Motivation: 传统地震类型分类方法依赖主观人工判断，耗时费力，需更客观高效的分类方法。

Method: 开发基于Transformer编码器的深度学习模型，对注意力权重可视化分析，开展数据选择和增强实验。

Result: 模型在浅间山地震活动测试中F1分数高，优于传统CNN方法；发现训练数据不一致影响分类和注意力权重；3公里内台站对模型性能和可解释性有重要作用。

Conclusion: 基于Transformer的模型用于火山地震自动分类有潜力，解决数据等挑战可提供理解地震活动的框架，有迁移学习应用前景。

Abstract: Precisely classifying earthquake types is crucial for elucidating the
relationship between volcanic earthquakes and volcanic activity. However,
traditional methods rely on subjective human judgment, which requires
considerable time and effort. To address this issue, we developed a deep
learning model using a transformer encoder for a more objective and efficient
classification. Tested on Mount Asama's diverse seismic activity, our model
achieved high F1 scores (0.930 for volcano tectonic, 0.931 for low-frequency
earthquakes, and 0.980 for noise), superior to a conventional CNN-based method.
To enhance interpretability, attention weight visualizations were analyzed,
revealing that the model focuses on key waveform features similarly to human
experts. However, inconsistencies in training data, such as ambiguously labeled
B-type events with S-waves, were found to influence classification accuracy and
attention weight distributions. Experiments addressing data selection and
augmentation demonstrated the importance of balancing data quality and
diversity. In addition, stations within 3 km of the crater played an important
role in improving model performance and interpretability. These findings
highlight the potential of Transformer-based models for automated volcanic
earthquake classification, particularly in improving efficiency and
interpretability. By addressing challenges such as data imbalance and
subjective labeling, our approach provides a robust framework for understanding
seismic activity at Mount Asama. Moreover, this framework offers opportunities
for transfer learning to other volcanic regions, paving the way for enhanced
volcanic hazard assessments and disaster mitigation strategies.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [179] [Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks](https://arxiv.org/abs/2507.01297)
*Xinxi Lyu,Michael Duan,Rulin Shao,Pang Wei Koh,Sewon Min*

Main category: cs.CL

TL;DR: 研究表明现有RAG在推理密集型基准测试中表现不佳，本文引入CompactDS数据存储，实现检索增强生成，在多个基准测试中提升准确率，且优于网络搜索引擎和复杂基于代理的RAG系统。


<details>
  <summary>Details</summary>
Motivation: 现有RAG在推理密集型基准测试中表现有限，缺少与预训练数据广度对齐的可用网络规模数据存储。

Method: 引入CompactDS数据存储，结合内存中近似最近邻检索和磁盘上精确搜索，平衡速度和召回率。

Result: 使用CompactDS的最小RAG管道在多个基准测试和不同模型大小上均实现准确率提升，不同基准测试相对增益不同，且自有的数据存储表现优于网络搜索引擎和复杂RAG系统。

Conclusion: 数据来源多样性很重要，CompactDS及其检索管道具有简单性、可重复性和自包含性，支持未来基于检索的AI系统研究。

Abstract: Retrieval-augmented Generation (RAG) has primarily been studied in limited
settings, such as factoid question answering; more challenging,
reasoning-intensive benchmarks have seen limited success from minimal RAG. In
this work, we challenge this prevailing view on established,
reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We
identify a key missing component in prior work: a usable, web-scale datastore
aligned with the breadth of pretraining data. To this end, we introduce
CompactDS: a diverse, high-quality, web-scale datastore that achieves high
retrieval accuracy and subsecond latency on a single-node. The key insights are
(1) most web content can be filtered out without sacrificing coverage, and a
compact, high-quality subset is sufficient; and (2) combining in-memory
approximate nearest neighbor (ANN) retrieval and on-disk exact search balances
speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves
consistent accuracy improvements across all benchmarks and model sizes
(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,
and 19% on MATH. No single data source suffices alone, highlighting the
importance of diversity of sources (web crawls, curated math, academic papers,
textbooks). Finally, we show that our carefully designed in-house datastore
matches or outperforms web search engines such as Google Search, as well as
recently proposed, complex agent-based RAG systems--all while maintaining
simplicity, reproducibility, and self-containment. We release CompactDS and our
retrieval pipeline, supporting future research exploring retrieval-based AI
systems.

</details>


### [180] [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](https://arxiv.org/abs/2507.01633)
*Georgii Levtsov,Dmitry Ustalov*

Main category: cs.CL

TL;DR: 本文研究NLP中全局评分和成对比较两种模型评估策略的优缺点。实验表明全局评分排名更可靠，但可能低估部分强模型；成对比较能识别低全局评分中的强模型，但频繁平局时需更多比较。


<details>
  <summary>Details</summary>
Motivation: 随着指令调优语言模型发展，NLP基准测试向成对比较排行榜转变，需研究两种评估策略优缺点以辅助选择合适策略。

Method: 在合成和真实数据集上进行计算实验，使用标准全局指标和流行的Bradley - Terry模型进行成对比较。

Result: 全局评分提供更可靠整体排名，但可能低估有罕见重大错误或低置信度的强模型；成对比较在识别低全局评分模型中的强竞争者方面特别有效，频繁平局时收敛需更多比较。

Conclusion: 不同的评估策略各有优劣，在选择模型评估策略时需根据具体情况权衡。

Abstract: With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.

</details>


### [181] [Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes](https://arxiv.org/abs/2507.01810)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.CL

TL;DR: 对小语言模型从临床笔记中进行开放属性值提取生成的结构化输出可解析性进行比较分析，发现JSON可解析性最高，研究还给出部署建议。


<details>
  <summary>Details</summary>
Motivation: 对小语言模型从临床笔记中进行开放属性值提取生成的结构化输出可解析性进行研究。

Method: 评估JSON、YAML和XML三种序列化格式，进行错误分析。

Result: JSON可解析性始终最高，结构鲁棒性随针对性提示和更大模型提升，随文档变长和特定笔记类型下降，发现特定格式的失败模式。

Conclusion: 研究结果为在隐私敏感的临床环境中部署语言模型时选择序列化格式和设计提示提供了实用指导。

Abstract: We present a comparative analysis of the parseability of structured outputs
generated by small language models for open attribute-value extraction from
clinical notes. We evaluate three widely used serialization formats: JSON,
YAML, and XML, and find that JSON consistently yields the highest parseability.
Structural robustness improves with targeted prompting and larger models, but
declines for longer documents and certain note types. Our error analysis
identifies recurring format-specific failure patterns. These findings offer
practical guidance for selecting serialization formats and designing prompts
when deploying language models in privacy-sensitive clinical settings.

</details>


### [182] [GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant](https://arxiv.org/abs/2507.01259)
*Michał Matak,Jarosław A. Chudziak*

Main category: cs.CL

TL;DR: 本文探讨大语言模型处理非英语和非中文国家法律事务的能力，介绍基于波兰民法典的认知代理gAIus及可解释的检索机制，通过特殊数据集评估，显著提升模型表现并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型处理非英语和非中文国家法律事务时给出答案及提供参考的能力。

Method: 介绍基于波兰民法典的认知代理gAIus架构，提出可解释、友好的检索机制，创建基于波兰法律学徒入学考试单选题的特殊数据集进行评估。

Result: 所提架构显著提升模型表现，使gpt - 3.5 - turbo - 0125提升419%，击败gpt - 4o，将gpt - 4o - mini分数从31%提升到86%。

Conclusion: 展示了未来可能的研究路径和研究结果的潜在应用。

Abstract: In this paper we discuss the capability of large language models to base
their answer and provide proper references when dealing with legal matters of
non-english and non-chinese speaking country. We discuss the history of legal
information retrieval, the difference between case law and statute law, its
impact on the legal tasks and analyze the latest research in this field. Basing
on that background we introduce gAIus, the architecture of the cognitive
LLM-based agent, whose responses are based on the knowledge retrieved from
certain legal act, which is Polish Civil Code. We propose a retrieval mechanism
which is more explainable, human-friendly and achieves better results than
embedding-based approaches. To evaluate our method we create special dataset
based on single-choice questions from entrance exams for law apprenticeships
conducted in Poland. The proposed architecture critically leveraged the
abilities of used large language models, improving the gpt-3.5-turbo-0125 by
419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.
At the end of our paper we show the possible future path of research and
potential applications of our findings.

</details>


### [183] [Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization](https://arxiv.org/abs/2507.01281)
*Juan Chen,Baolong Bi,Wei Zhang,Jingyan Sui,Xiaofei Zhu,Yuanzhuo Wang,Lingrui Mei,Shenghua Liu*

Main category: cs.CL

TL;DR: 本文提出CARE - RAG框架以解决RAG系统知识冲突问题，实验显示其性能优于RAG基线。


<details>
  <summary>Details</summary>
Motivation: RAG系统存在因内部不一致或检索内容有噪声导致的知识冲突，影响生成可靠性。

Method: 提出CARE - RAG框架，先推导参数感知证据，再提炼检索证据，用蒸馏的3B LLaMA3.2模型进行冲突驱动的总结，引入QA Repair步骤修正基准答案。

Result: 在含检索数据的修订QA数据集上实验，CARE - RAG始终优于强大的RAG基线，在有噪声或冲突证据场景表现更佳。

Conclusion: CARE - RAG框架能有效提升RAG系统的生成可靠性和可信度。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating their parametric knowledge with external retrieved content.
However, knowledge conflicts caused by internal inconsistencies or noisy
retrieved content can severely undermine the generation reliability of RAG
systems.In this work, we argue that LLMs should rethink all evidence, including
both retrieved content and internal knowledge, before generating responses.We
propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel
framework that improves trustworthiness through Conflict-Driven Summarization
of all available evidence.CARE-RAG first derives parameter-aware evidence by
comparing parameter records to identify diverse internal perspectives. It then
refines retrieved evidences to produce context-aware evidence, removing
irrelevant or misleading content. To detect and summarize conflicts, we distill
a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable
synthesis across multiple sources.To further ensure evaluation integrity, we
introduce a QA Repair step to correct outdated or ambiguous benchmark
answers.Experiments on revised QA datasets with retrieval data show that
CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios
with noisy or conflicting evidence.

</details>


### [184] [LEDOM: An Open and Fundamental Reverse Language Model](https://arxiv.org/abs/2507.01335)
*Xunjian Yin,Sitao Cheng,Yuxi Xie,Xinyu Hu,Li Lin,Xinyi Wang,Liangming Pan,William Yang Wang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 介绍首个纯反向语言模型LEDOM，基于它提出反向奖励应用，在数学推理任务上有性能提升，将发布模型等资源。


<details>
  <summary>Details</summary>
Motivation: 提出反向语言模型作为通用基础模型，探索其在一般任务中的应用潜力。

Method: 自回归训练LEDOM，以反向时间顺序处理序列；基于LEDOM提出反向奖励应用，对前向语言模型输出进行重排序。

Result: LEDOM在数学推理任务上通过反向奖励应用实现了性能显著提升。

Conclusion: LEDOM具有独特特性和广泛应用潜力。

Abstract: We introduce LEDOM, the first purely reverse language model, trained
autoregressively on 435B tokens with 2B and 7B parameter variants, which
processes sequences in reverse temporal order through previous token
prediction. For the first time, we present the reverse language model as a
potential foundational model across general tasks, accompanied by a set of
intriguing examples and insights. Based on LEDOM, we further introduce a novel
application: Reverse Reward, where LEDOM-guided reranking of forward language
model outputs leads to substantial performance improvements on mathematical
reasoning tasks. This approach leverages LEDOM's unique backward reasoning
capability to refine generation quality through posterior evaluation. Our
findings suggest that LEDOM exhibits unique characteristics with broad
application potential. We will release all models, training code, and
pre-training data to facilitate future research.

</details>


### [185] [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/abs/2507.01352)
*Chris Yuhao Liu,Liang Zeng,Yuzhen Xiao,Jujie He,Jiacai Liu,Chaojie Wang,Rui Yan,Wei Shen,Fuxiang Zhang,Jiacheng Xu,Yang Liu,Yahui Zhou*

Main category: cs.CL

TL;DR: 当前开源奖励模型表现不佳，论文提出SynPref - 40M数据集和Skywork - Reward - V2奖励模型，在多个基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的开源奖励模型在现有评估基准上表现不佳，可能是由于偏好数据集的局限性。

Method: 设计人类 - AI协同的两阶段管道创建4000万偏好对的SynPref - 40M数据集，用其中2600万对训练0.6B到8B参数的Skywork - Reward - V2奖励模型。

Result: Skywork - Reward - V2在七个主要奖励模型基准测试中达到SOTA，消融实验证实数据规模和高质量筛选都有作用。

Conclusion: Skywork - Reward - V2系列在开源奖励模型方面取得重大进展，凸显现有偏好数据集潜力和人类 - AI协同筛选可提升数据质量。

Abstract: Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.

</details>


### [186] [Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities](https://arxiv.org/abs/2507.01479)
*Yingqiang Gao,Kaede Johnson,David Froehlich,Luisa Carrer,Sarah Ebling*

Main category: cs.CL

TL;DR: 本文提出用DPO改进基于大语言模型的自动文本简化系统，通过收集目标群体反馈进行后训练，强调目标群体参与设计个性化AI的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动文本简化系统在训练时未纳入文本简化的偏好反馈，缺乏针对目标群体的个性化。

Method: 利用直接偏好优化（DPO）扩展标准监督微调（SFT）方法，用从智障人士收集的反馈对基于大语言模型的自动文本简化模型进行后训练，并提出开发个性化系统的管道。

Result: 强调了目标群体人员积极参与设计符合人类期望的个性化AI无障碍解决方案的必要性。

Conclusion: 朝着在目标群体层面实现个性化的包容性AI系统迈进，纳入了目标群体自身的见解。

Abstract: Automatic text simplification (ATS) aims to enhance language accessibility
for various target groups, particularly persons with intellectual disabilities.
Recent advancements in generative AI, especially large language models (LLMs),
have substantially improved the quality of machine-generated text
simplifications, thereby mitigating information barriers for the target group.
However, existing LLM-based ATS systems do not incorporate preference feedback
on text simplifications during training, resulting in a lack of personalization
tailored to the specific needs of target group representatives.
  In this work, we extend the standard supervised fine-tuning (SFT) approach
for adapting LLM-based ATS models by leveraging a computationally efficient LLM
alignment technique -- direct preference optimization (DPO). Specifically, we
post-train LLM-based ATS models using human feedback collected from persons
with intellectual disabilities, reflecting their preferences on paired text
simplifications generated by mainstream LLMs. Furthermore, we propose a
pipeline for developing personalized LLM-based ATS systems, encompassing data
collection, model selection, SFT and DPO post-training, and evaluation. Our
findings underscore the necessity of active participation of target group
persons in designing personalized AI accessibility solutions aligned with human
expectations. This work represents a step towards personalizing inclusive AI
systems at the target-group level, incorporating insights not only from text
simplification experts but also from target group persons themselves.

</details>


### [187] [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/abs/2507.01702)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Zhen Ye,Guang Chen,Zhiyong Huang,Jing Ma*

Main category: cs.CL

TL;DR: 提出AdamMeme框架评估多模态大语言模型对有害梗图的理解能力，实验显示能揭示模型弱点，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有评估多模态大语言模型理解有害梗图能力的基准依赖静态数据集，无法提供最新和全面评估，因网络梗图动态演变。

Method: 提出基于代理的灵活评估框架AdamMeme，通过多代理协作，迭代更新梗图数据进行全面评估。

Result: 框架系统地揭示了不同目标多模态大语言模型的不同表现，提供了对模型特定弱点的深入、细粒度分析。

Conclusion: AdamMeme框架能有效评估多模态大语言模型对有害梗图的理解能力，发现模型弱点。

Abstract: The proliferation of multimodal memes in the social media era demands that
multimodal Large Language Models (mLLMs) effectively understand meme
harmfulness. Existing benchmarks for assessing mLLMs on harmful meme
understanding rely on accuracy-based, model-agnostic evaluations using static
datasets. These benchmarks are limited in their ability to provide up-to-date
and thorough assessments, as online memes evolve dynamically. To address this,
we propose AdamMeme, a flexible, agent-based evaluation framework that
adaptively probes the reasoning capabilities of mLLMs in deciphering meme
harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive
evaluations by iteratively updating the meme data with challenging samples,
thereby exposing specific limitations in how mLLMs interpret harmfulness.
Extensive experiments show that our framework systematically reveals the
varying performance of different target mLLMs, offering in-depth, fine-grained
analyses of model-specific weaknesses. Our code is available at
https://github.com/Lbotirx/AdamMeme.

</details>


### [188] [MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2507.01785)
*Zhixun Chen,Ping Guo,Wenhan Han,Yifan Zhang,Binbin Liu,Haobin Lin,Fengze Liu,Yan Zhao,Bingni Zhang,Taifeng Wang,Yin Zheng,Meng Fang*

Main category: cs.CL

TL;DR: 提出MuRating框架将英语数据质量信号转移到17种目标语言，用其筛选数据预训练模型，在多语言评估中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型的数据选择方法几乎只关注英语，而数据质量对大语言模型性能至关重要。

Method: 通过成对比较聚合多个英语“评分者”学习统一文档质量分数，通过翻译投影这些判断来训练多语言评估器，筛选数据预训练模型。

Result: 相比QuRater、AskLLM、DCLM等强基线，该方法提高了英语基准和多语言评估的平均准确率，在知识密集型任务上提升显著。

Conclusion: 该方法有效，还分析了翻译保真度、选择偏差等并指出未来工作方向。

Abstract: Data quality is a critical driver of large language model performance, yet
existing model-based selection methods focus almost exclusively on English. We
introduce MuRating, a scalable framework that transfers high-quality English
data-quality signals into a single rater for 17 target languages. MuRating
aggregates multiple English "raters" via pairwise comparisons to learn unified
document-quality scores,then projects these judgments through translation to
train a multilingual evaluator on monolingual, cross-lingual, and parallel text
pairs. Applied to web data, MuRating selects balanced subsets of English and
multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to
strong baselines, including QuRater, AskLLM, DCLM and so on, our approach
boosts average accuracy on both English benchmarks and multilingual
evaluations, with especially large gains on knowledge-intensive tasks. We
further analyze translation fidelity, selection biases, and underrepresentation
of narrative material, outlining directions for future work.

</details>


### [189] [Probing Evaluation Awareness of Language Models](https://arxiv.org/abs/2507.01786)
*Jord Nguyen,Khiem Hoang,Carlo Leonardo Attubato,Felix Hofstätter*

Main category: cs.CL

TL;DR: 研究Llama - 3.3 - 70B - Instruct评估意识，发现线性探针可区分评估和部署提示，强调确保可信评估及理解欺骗能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 语言模型的评估意识对AI治理框架和行业承诺中的评估可靠性有重大安全和政策影响，需研究。

Method: 对Llama - 3.3 - 70B - Instruct使用线性探针来区分真实世界评估和部署提示。

Result: 线性探针可分离评估和部署提示，当前安全评估能被正确分类，表明对模型而言已显得虚假。

Conclusion: 强调确保可信评估和理解欺骗能力的重要性，展示利用模型内部信息支持安全审计黑盒方法的可能性。

Abstract: Language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. This has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to AI governance frameworks and voluntary industry
commitments. In this paper, we study evaluation awareness in
Llama-3.3-70B-Instruct. We show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. We also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. Our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. More
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception.

</details>


### [190] [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/abs/2507.01790)
*Tianze Hua,Tian Yun,Ellie Pavlick*

Main category: cs.CL

TL;DR: 本文研究多模态AI模型在输入冲突信息时的行为，聚焦视觉语言模型，发现模型有模态偏好，且存在能促进按指令输出的路由头，为识别和控制模型处理冲突信号提供步骤。


<details>
  <summary>Details</summary>
Motivation: 了解多模态AI模型在输入流存在冲突信息时的行为表现。

Method: 针对视觉语言模型，提供不一致输入（如图像与文字描述不符），让模型报告特定模态的信息。

Result: 模型常偏好一种模态；行为偏好模态在模型内部表征结构中明显；特定注意力头可重构表征；存在模态无关的“路由头”，可操纵或迁移以提升性能。

Conclusion: 本研究为识别和控制模型在复杂多模态环境中检测和解决冲突信号提供了重要步骤。

Abstract: AI models are increasingly required to be multimodal, integrating disparate
input streams into a coherent state representation on which subsequent
behaviors and actions can be based. This paper seeks to understand how such
models behave when input streams present conflicting information. Focusing
specifically on vision-language models, we provide inconsistent inputs (e.g.,
an image of a dog paired with the caption "A photo of a cat") and ask the model
to report the information present in one of the specific modalities (e.g.,
"What does the caption say / What is in the image?"). We find that models often
favor one modality over the other, e.g., reporting the image regardless of what
the caption says, but that different models differ in which modality they
favor. We find evidence that the behaviorally preferred modality is evident in
the internal representational structure of the model, and that specific
attention heads can restructure the representations to favor one modality over
the other. Moreover, we find modality-agnostic "router heads" which appear to
promote answers about the modality requested in the instruction, and which can
be manipulated or transferred in order to improve performance across datasets
and modalities. Together, the work provides essential steps towards identifying
and controlling if and how models detect and resolve conflicting signals within
complex multimodal environments.

</details>


### [191] [AI4Research: A Survey of Artificial Intelligence for Scientific Research](https://arxiv.org/abs/2507.01903)
*Qiguang Chen,Mingda Yang,Libo Qin,Jinhao Liu,Zheng Yan,Jiannan Guan,Dengyun Peng,Yiyan Ji,Hanjing Li,Mengkang Hu,Yimeng Zhang,Yihao Liang,Yuhang Zhou,Jiaqi Wang,Zhi Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文针对AI4Research领域缺乏全面综述的问题，进行了全面调研，给出系统分类、前沿方向及丰富资源。


<details>
  <summary>Details</summary>
Motivation: AI在复杂领域能力提升，诸多研究探索其在科研创新中的应用，但缺乏AI4Research的全面综述，阻碍该领域发展。

Method: 提出系统分类法对AI4Research的五个主流任务分类；识别关键研究差距，指出未来方向；整理丰富资源。

Result: 完成AI4Research的全面调研，给出系统分类、前沿方向和丰富资源。

Conclusion: 希望为研究界提供资源，促进AI4Research的创新突破。

Abstract: Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated
remarkable capabilities in complex domains such as logical reasoning and
experimental coding. Motivated by these advancements, numerous studies have
explored the application of AI in the innovation process, particularly in the
context of scientific research. These AI technologies primarily aim to develop
systems that can autonomously conduct research processes across a wide range of
scientific disciplines. Despite these significant strides, a comprehensive
survey on AI for Research (AI4Research) remains absent, which hampers our
understanding and impedes further development in this field. To address this
gap, we present a comprehensive survey and offer a unified perspective on
AI4Research. Specifically, the main contributions of our work are as follows:
(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify
five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key
research gaps and highlight promising future directions, focusing on the rigor
and scalability of automated experiments, as well as the societal impact. (3)
Abundant applications and resources: Finally, we compile a wealth of resources,
including relevant multidisciplinary applications, data corpora, and tools. We
hope our work will provide the research community with quick access to these
resources and stimulate innovative breakthroughs in AI4Research.

</details>


### [192] [Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models](https://arxiv.org/abs/2507.01915)
*Chengao Li,Hanyu Zhang,Yunkun Xu,Hongyan Xue,Xiang Ao,Qing He*

Main category: cs.CL

TL;DR: 提出GAPO和P - GAPO微调范式解决大语言模型与人类偏好对齐问题，在Mistral - 7B上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型与多样化、冲突的人类偏好有效对齐的挑战。

Method: 将人类价值对齐视为多目标优化问题，提出GAPO范式，用多梯度下降，自适应缩放梯度；引入P - GAPO纳入不同目标的用户偏好。

Result: 理论分析表明GAPO收敛于多目标的帕累托最优解，在Mistral - 7B上的实验结果显示GAPO在帮助性和无害性上优于现有方法。

Conclusion: GAPO和P - GAPO是有效的大语言模型与人类偏好对齐方法，能更好地平衡多目标。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.

</details>


### [193] [Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla](https://arxiv.org/abs/2507.01931)
*Md Sazzadul Islam Ridoy,Sumi Akter,Md. Aminur Rahman*

Main category: cs.CL

TL;DR: 研究对比Whisper和Wav2Vec - BERT在低资源语言孟加拉语的自动语音识别性能，Wav2Vec - BERT表现更优。


<details>
  <summary>Details</summary>
Motivation: 探究先进自动语音识别模型在低资源语言孟加拉语上的性能，以支持低资源语言发展。

Method: 使用Mozilla Common Voice - 17和OpenSLR两个公开数据集，进行系统微调与超参数优化，基于字错误率、字符错误率、训练时间和计算效率对比模型。

Result: Wav2Vec - BERT在所有关键评估指标上优于Whisper，性能更优且所需计算资源更少。

Conclusion: Wav2Vec - BERT为低资源语言环境下开发强大语音识别系统提供有价值见解。

Abstract: In recent years, neural models trained on large multilingual text and speech
datasets have shown great potential for supporting low-resource languages. This
study investigates the performances of two state-of-the-art Automatic Speech
Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's
Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments
using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to
evaluate model performances. Through systematic fine-tuning and hyperparameter
optimization, including learning rate, epochs, and model checkpoint selection,
we have compared the models based on Word Error Rate (WER), Character Error
Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model
outperformed Whisper across all key evaluation metrics, demonstrated superior
performance while requiring fewer computational resources, and offered valuable
insights to develop robust speech recognition systems in low-resource
linguistic settings.

</details>


### [194] [The Anatomy of Evidence: An Investigation Into Explainable ICD Coding](https://arxiv.org/abs/2507.01802)
*Katharina Beckh,Elisa Studeny,Sujan Sai Gannamaneni,Dario Antweiler,Stefan Rüping*

Main category: cs.CL

TL;DR: 本文对MDACE数据集深入分析，从应用角度评估可解释医疗编码系统，揭示证据与编码描述有一定程度匹配，研究现有方法与真实证据重叠度高，提出匹配度量并给出开发和评估系统建议。


<details>
  <summary>Details</summary>
Motivation: 自动医疗编码中透明度很重要，但现有可解释方法评估因数据稀缺受限，MDACE数据集的出现为研究提供了资源，本文旨在深入理解自动医疗编码和证据提取。

Method: 对MDACE数据集进行深入分析，从应用角度对当前可解释医疗编码系统进行合理性评估。

Result: 发现真实证据与编码描述在一定程度上对齐，研究现有先进方法与真实证据有高重叠度，提出匹配度量并指出成功和失败案例。

Conclusion: 基于研究结果，为开发和评估可解释医疗编码系统提供了建议。

Abstract: Automatic medical coding has the potential to ease documentation and billing
processes. For this task, transparency plays an important role for medical
coders and regulatory bodies, which can be achieved using explainability
methods. However, the evaluation of these approaches has been mostly limited to
short text and binary settings due to a scarcity of annotated data. Recent
efforts by Cheng et al. (2023) have introduced the MDACE dataset, which
provides a valuable resource containing code evidence in clinical records. In
this work, we conduct an in-depth analysis of the MDACE dataset and perform
plausibility evaluation of current explainable medical coding systems from an
applied perspective. With this, we contribute to a deeper understanding of
automatic medical coding and evidence extraction. Our findings reveal that
ground truth evidence aligns with code descriptions to a certain degree. An
investigation into state-of-the-art approaches shows a high overlap with ground
truth evidence. We propose match measures and highlight success and failure
cases. Based on our findings, we provide recommendations for developing and
evaluating explainable medical coding systems.

</details>


### [195] [Low-Perplexity LLM-Generated Sequences and Where To Find Them](https://arxiv.org/abs/2507.01844)
*Arthur Wuhrmann,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 本文介绍分析低困惑度序列的方法来探究大语言模型对训练数据的利用和复制情况，发现部分低困惑度序列无法映射到语料库，对匹配部分量化分布。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型日益广泛，了解特定训练数据如何塑造其输出对透明度、问责制、隐私和公平性至关重要。

Method: 引入以分析低困惑度序列为核心的系统方法，提取不同主题的长序列并追溯其训练数据来源。

Result: 发现很大一部分低困惑度序列无法映射到语料库，对匹配部分量化了在源文档中的出现分布。

Conclusion: 为更好理解大语言模型训练数据如何影响其行为铺平道路。

Abstract: As Large Language Models (LLMs) become increasingly widespread, understanding
how specific training data shapes their outputs is crucial for transparency,
accountability, privacy, and fairness. To explore how LLMs leverage and
replicate their training data, we introduce a systematic approach centered on
analyzing low-perplexity sequences - high-probability text spans generated by
the model. Our pipeline reliably extracts such long sequences across diverse
topics while avoiding degeneration, then traces them back to their sources in
the training data. Surprisingly, we find that a substantial portion of these
low-perplexity spans cannot be mapped to the corpus. For those that do match,
we quantify the distribution of occurrences across source documents,
highlighting the scope and nature of verbatim recall and paving a way toward
better understanding of how LLMs training data impacts their behavior.

</details>


### [196] [High-Layer Attention Pruning with Rescaling](https://arxiv.org/abs/2507.01900)
*Songtao Liu,Peng Liu*

Main category: cs.CL

TL;DR: 提出针对大语言模型注意力头的新型剪枝算法，经多模型多任务实验验证优于现有结构化剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 传统无训练结构化剪枝方法未考虑网络架构中注意力头位置，本研究旨在改进这一问题。

Method: 提出在模型高层策略性剪枝注意力头的算法，引入自适应缩放参数校准剪枝后表示规模。

Result: 在LLaMA3.1 - 8B等多种大语言模型的27个数据集上实验，新方法在生成和判别任务中均优于现有结构化剪枝方法，在生成任务中优势明显。

Conclusion: 提出的新型剪枝算法有效，性能超越现有结构化剪枝方法。

Abstract: Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [197] [Geometry-aware 4D Video Generation for Robot Manipulation](https://arxiv.org/abs/2507.01099)
*Zeyi Liu,Shuang Li,Eric Cousineau,Siyuan Feng,Benjamin Burchfiel,Shuran Song*

Main category: cs.CV

TL;DR: 提出4D视频生成模型解决多视角视频生成难题，在数据集表现优且可支持机器人操作。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在生成时间连贯且多视角几何一致视频上有挑战，需提升机器人在复杂环境规划和交互能力。

Method: 提出4D视频生成模型，训练时用跨视角点图对齐监督，使模型学习场景共享3D表示。

Result: 相比基线方法，在多数据集上生成的预测更稳定、空间对齐；可用预测的4D视频恢复机器人末端执行器轨迹。

Conclusion: 模型能有效生成多视角一致视频，支持机器人鲁棒操作和适应新视角。

Abstract: Understanding and predicting the dynamics of the physical world can enhance a
robot's ability to plan and interact effectively in complex environments. While
recent video generation models have shown strong potential in modeling dynamic
scenes, generating videos that are both temporally coherent and geometrically
consistent across camera views remains a significant challenge. To address
this, we propose a 4D video generation model that enforces multi-view 3D
consistency of videos by supervising the model with cross-view pointmap
alignment during training. This geometric supervision enables the model to
learn a shared 3D representation of the scene, allowing it to predict future
video sequences from novel viewpoints based solely on the given RGB-D
observations, without requiring camera poses as inputs. Compared to existing
baselines, our method produces more visually stable and spatially aligned
predictions across multiple simulated and real-world robotic datasets. We
further show that the predicted 4D videos can be used to recover robot
end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting
robust robot manipulation and generalization to novel camera viewpoints.

</details>


### [198] [Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound](https://arxiv.org/abs/2507.01401)
*Huanwen Liang,Jingxian Xu,Yuanji Zhang,Yuhao Huang,Yuhan Zhang,Xin Yang,Ran Li,Xuedong Deng,Yanjun Liu,Guowei Tao,Yun Wu,Sheng Zhao,Xinru Gao,Dong Ni*

Main category: cs.CV

TL;DR: 提出基于MIL的方法用于产前超声胎儿腹部异常分类，有三项技术贡献，在大数据集验证优于竞品且开源代码。


<details>
  <summary>Details</summary>
Motivation: 胎儿腹部畸形需准确诊断，AI在产前腹部异常应用有限，现有研究重图像分类轻病例诊断。

Method: 开发无标准平面定位的病例级MIL方法，采用MoAE模块、MFS模块和PPL方法。

Result: 在含2419个病例、24748张图像、6类别的产前腹部超声数据集上验证，优于现有方法。

Conclusion: 所提方法有效，能用于胎儿腹部异常分类，推动AI在产前诊断的应用。

Abstract: Fetal abdominal malformations are serious congenital anomalies that require
accurate diagnosis to guide pregnancy management and reduce mortality. Although
AI has demonstrated significant potential in medical diagnosis, its application
to prenatal abdominal anomalies remains limited. Most existing studies focus on
image-level classification and rely on standard plane localization, placing
less emphasis on case-level diagnosis. In this paper, we develop a case-level
multiple instance learning (MIL)-based method, free of standard plane
localization, for classifying fetal abdominal anomalies in prenatal ultrasound.
Our contribution is three-fold. First, we adopt a mixture-of-attention-experts
module (MoAE) to weight different attention heads for various planes. Secondly,
we propose a medical-knowledge-driven feature selection module (MFS) to align
image features with medical knowledge, performing self-supervised image token
selection at the case-level. Finally, we propose a prompt-based prototype
learning (PPL) to enhance the MFS. Extensively validated on a large prenatal
abdominal ultrasound dataset containing 2,419 cases, with a total of 24,748
images and 6 categories, our proposed method outperforms the state-of-the-art
competitors. Codes are available at:https://github.com/LL-AC/AAcls.

</details>


### [199] [DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal](https://arxiv.org/abs/2507.01422)
*Wenjie Liu,Bingshu Wang,Ze Wang,C. L. Philip Chen*

Main category: cs.CV

TL;DR: 本文提出DocShaDiffusion模型用于文档图像阴影去除，设计相关模块与损失函数，开发数据集，实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文档阴影去除方法多处理恒定颜色背景阴影，忽略彩色阴影。

Method: 设计DocShaDiffusion模型，将图像转至隐空间；设计SSGM模块生成阴影软掩码；提出SMGDM模块去除阴影；提出阴影鲁棒感知特征损失；开发SDCSRD数据集。

Result: 在三个公开数据集上实验，验证了方法优于现有技术。

Conclusion: 提出的方法在文档图像阴影去除任务中效果良好，代码和数据集将公开。

Abstract: Document shadow removal is a crucial task in the field of document image
enhancement. However, existing methods tend to remove shadows with constant
color background and ignore color shadows. In this paper, we first design a
diffusion model in latent space for document image shadow removal, called
DocShaDiffusion. It translates shadow images from pixel space to latent space,
enabling the model to more easily capture essential features. To address the
issue of color shadows, we design a shadow soft-mask generation module (SSGM).
It is able to produce accurate shadow mask and add noise into shadow regions
specially. Guided by the shadow mask, a shadow mask-aware guided diffusion
module (SMGDM) is proposed to remove shadows from document images by
supervising the diffusion and denoising process. We also propose a
shadow-robust perceptual feature loss to preserve details and structures in
document images. Moreover, we develop a large-scale synthetic document color
shadow removal dataset (SDCSRD). It simulates the distribution of realistic
color shadows and provides powerful supports for the training of models.
Experiments on three public datasets validate the proposed method's superiority
over state-of-the-art. Our code and dataset will be publicly available.

</details>


### [200] [NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation](https://arxiv.org/abs/2507.01463)
*Max Gandyra,Alessandro Santonicola,Michael Beetz*

Main category: cs.CV

TL;DR: 提出NOCTIS框架用于RGB图像中新颖对象实例分割，基于已有模型改进，利用Grounded - SAM 2和DINOv2，在BOP 2023挑战数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 设计一个无需（重新）训练即可用于各种新颖对象的实例分割模型是一项难题，需提出解决方案。

Method: 提出NOCTIS框架，利用Grounded - SAM 2获取对象建议和分割掩码，用DINOv2生成图像嵌入，通过计算对象匹配分数实现建议 - 对象匹配，还考虑了额外加权因子。

Result: NOCTIS在BOP 2023挑战的七个核心数据集上，无需进一步训练/微调，就优于最佳的RGB和RGB - D方法。

Conclusion: NOCTIS框架在新颖对象实例分割任务中表现出色，无需重新训练就有良好性能。

Abstract: Instance segmentation of novel objects instances in RGB images, given some
example images for each object, is a well known problem in computer vision.
Designing a model general enough to be employed, for all kinds of novel
objects, without (re-) training, has proven to be a difficult task. To handle
this, we propose a simple, yet powerful, framework, called: Novel Object Cyclic
Threshold based Instance Segmentation (NOCTIS). This work stems from and
improves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also
leverages on recent vision foundation models, namely: Grounded-SAM 2 and
DINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise
bounding boxes and their corresponding segmentation masks; while DINOv2's
zero-shot capabilities are employed to generate the image embeddings. The
quality of those masks, together with their embeddings, is of vital importance
to our approach; as the proposal-object matching is realized by determining an
object matching score based on the similarity of the class embeddings and the
average maximum similarity of the patch embeddings. Differently to SAM-6D,
calculating the latter involves a prior patch filtering based on the distance
between each patch and its corresponding cyclic/roundtrip patch in the image
grid. Furthermore, the average confidence of the proposals' bounding box and
mask is used as an additional weighting factor for the object matching score.
We empirically show that NOCTIS, without further training/fine tuning,
outperforms the best RGB and RGB-D methods on the seven core datasets of the
BOP 2023 challenge for the "Model-based 2D segmentation of unseen objects"
task.

</details>


### [201] [Crop Pest Classification Using Deep Learning Techniques: A Review](https://arxiv.org/abs/2507.01494)
*Muhammad Hassam Ejaz,Muhammad Bilal,Usman Habib*

Main category: cs.CV

TL;DR: 本文回顾了2018 - 2025年37篇基于AI的害虫分类研究，介绍研究组织方式、模型发展趋势，指出当前挑战并给出领域概述与未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统害虫监测方法慢、需人工且难规模化，深度学习为害虫监测提供强大解决方案，因此进行相关研究综述。

Method: 回顾2018 - 2025年37篇聚焦基于AI的害虫分类研究，按作物类型、害虫种类、模型架构、数据集使用和关键技术挑战组织这些研究。

Result: 早期研究多依赖CNN，最新研究转向混合和基于Transformer的模型，这类模型准确性更高、上下文理解能力更好，但仍存在数据集不平衡、小害虫检测难、泛化性有限和边缘设备部署等挑战。

Conclusion: 本综述为该领域提供结构化概述，突出有用数据集，明确基于AI的害虫监测系统的关键挑战和未来方向。

Abstract: Insect pests continue to bring a serious threat to crop yields around the
world, and traditional methods for monitoring them are often slow, manual, and
difficult to scale. In recent years, deep learning has emerged as a powerful
solution, with techniques like convolutional neural networks (CNNs), vision
transformers (ViTs), and hybrid models gaining popularity for automating pest
detection. This review looks at 37 carefully selected studies published between
2018 and 2025, all focused on AI-based pest classification. The selected
research is organized by crop type, pest species, model architecture, dataset
usage, and key technical challenges. The early studies relied heavily on CNNs
but latest work is shifting toward hybrid and transformer-based models that
deliver higher accuracy and better contextual understanding. Still, challenges
like imbalanced datasets, difficulty in detecting small pests, limited
generalizability, and deployment on edge devices remain significant hurdles.
Overall, this review offers a structured overview of the field, highlights
useful datasets, and outlines the key challenges and future directions for
AI-based pest monitoring systems.

</details>


### [202] [Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images](https://arxiv.org/abs/2507.01502)
*Ozan Durgut,Beril Kallfelz-Sirmacek,Cem Unsalan*

Main category: cs.CV

TL;DR: 文章介绍传统和深度学习两种树冠检测方法，提出新的基于规则的方法结合二者，处理结果并对比优势与不足。


<details>
  <summary>Details</summary>
Motivation: 解决因缺乏森林监测导致的全球变暖、生物多样性丧失和空气污染等问题，需自动化监测。

Method: 引入传统和深度学习两种树冠检测方法，提出基于规则的方法整合二者，用传统方法提取特征和分割，深度学习方法检测树冠，最后后处理结果。

Result: 得到树冠检测结果，并与提出方法对比检测树冠数量。

Conclusion: 报告检测结果的优缺点和改进方向。

Abstract: Global warming, loss of biodiversity, and air pollution are among the most
significant problems facing Earth. One of the primary challenges in addressing
these issues is the lack of monitoring forests to protect them. To tackle this
problem, it is important to leverage remote sensing and computer vision methods
to automate monitoring applications. Hence, automatic tree crown detection
algorithms emerged based on traditional and deep learning methods. In this
study, we first introduce two different tree crown detection methods based on
these approaches. Then, we form a novel rule-based approach that integrates
these two methods to enhance robustness and accuracy of tree crown detection
results. While traditional methods are employed for feature extraction and
segmentation of forested areas, deep learning methods are used to detect tree
crowns in our method. With the proposed rule-based approach, we post-process
these results, aiming to increase the number of detected tree crowns through
neighboring trees and localized operations. We compare the obtained results
with the proposed method in terms of the number of detected tree crowns and
report the advantages, disadvantages, and areas for improvement of the obtained
outcomes.

</details>


### [203] [Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence](https://arxiv.org/abs/2507.01504)
*Robert Aufschläger,Youssef Shoeb,Azarm Nowzad,Michael Heigl,Fabian Bally,Martin Schramm*

Main category: cs.CV

TL;DR: 本文提出cRID框架检测PII并增强行人重识别，实验显示其在跨数据集场景实用有效，代码开源。


<details>
  <summary>Details</summary>
Motivation: 街道级录音开放数据存在隐私风险，需检测超越生物特征的PII。

Method: 提出cRID，结合大视觉语言模型、图注意力网络和表征学习，识别和利用可解释特征。

Result: 在实际跨数据集Re - ID场景表现提升，如从Market - 1501到CUHK03 - np (detected)。

Conclusion: cRID框架具有实用价值。

Abstract: The collection and release of street-level recordings as Open Data play a
vital role in advancing autonomous driving systems and AI research. However,
these datasets pose significant privacy risks, particularly for pedestrians,
due to the presence of Personally Identifiable Information (PII) that extends
beyond biometric traits such as faces. In this paper, we present cRID, a novel
cross-modal framework combining Large Vision-Language Models, Graph Attention
Networks, and representation learning to detect textual describable clues of
PII and enhance person re-identification (Re-ID). Our approach focuses on
identifying and leveraging interpretable features, enabling the detection of
semantically meaningful PII beyond low-level appearance cues. We conduct a
systematic evaluation of PII presence in person image datasets. Our experiments
show improved performance in practical cross-dataset Re-ID scenarios, notably
from Market-1501 to CUHK03-np (detected), highlighting the framework's
practical utility. Code is available at https://github.com/RAufschlaeger/cRID.

</details>


### [204] [Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring](https://arxiv.org/abs/2507.01590)
*Ameer Hamza,Zuhaib Hussain But,Umar Arif,Samiya,M. Abdullah Asad,Muhammad Naeem*

Main category: cs.CV

TL;DR: 本文提出多模态课堂监控系统评估学生注意力，经评估效果好，还能自动记录考勤。


<details>
  <summary>Details</summary>
Motivation: 更精准地评估学生课堂注意力，了解学生参与度和行为。

Method: 利用YOLOv8检测手机和睡眠，用LResNet Occ FC、YOLO和MTCNN实现人脸识别，在特定数据集上训练模型，在PHP应用中实现系统，用ESP32 - CAM采集数据。

Result: 睡眠检测mAP@50达97.42%，人脸识别验证准确率86.45%，手机检测mAP@50达85.89%。

Conclusion: 该集成方法提升课堂监控能力，可自动记录考勤，适用于不同教育环境。

Abstract: This study presents a novel classroom surveillance system that integrates
multiple modalities, including drowsiness, tracking of mobile phone usage, and
face recognition,to assess student attentiveness with enhanced precision.The
system leverages the YOLOv8 model to detect both mobile phone and sleep
usage,(Ghatge et al., 2024) while facial recognition is achieved through
LResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These
models work in synergy to provide comprehensive, real-time monitoring, offering
insights into student engagement and behavior.(S et al., 2023) The framework is
trained on specialized datasets, such as the RMFD dataset for face recognition
and a Roboflow dataset for mobile phone detection. The extensive evaluation of
the system shows promising results. Sleep detection achieves 97. 42% mAP@50,
face recognition achieves 86. 45% validation accuracy and mobile phone
detection reach 85. 89% mAP@50. The system is implemented within a core PHP web
application and utilizes ESP32-CAM hardware for seamless data capture.(Neto et
al., 2024) This integrated approach not only enhances classroom monitoring, but
also ensures automatic attendance recording via face recognition as students
remain seated in the classroom, offering scalability for diverse educational
environments.(Banada,2025)

</details>


### [205] [Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems](https://arxiv.org/abs/2507.01607)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi,Eric Bourbao*

Main category: cs.CV

TL;DR: 本文首次对基于深度学习的人脸识别系统中的后门进行系统级研究，展示多种后门攻击，并给出应对措施。


<details>
  <summary>Details</summary>
Motivation: 深度学习人脸识别广泛应用引发安全担忧，现有文献对现实无约束系统的DNN后门攻击研究存在盲点。

Method: 全面探索DNN后门在人脸识别管道中的可行性，展示两种针对人脸检测任务的后门攻击，研究大边际损失训练的人脸特征提取器的后门攻击情况，结合模型测试不同管道配置和攻击案例。

Result: 首次展示两种人脸检测任务的后门攻击，证明人脸特征提取器也易受攻击，单个后门可使攻击者绕过系统全部功能。

Conclusion: 为利益相关者提供了一些最佳实践和应对措施。

Abstract: The widespread use of deep learning face recognition raises several security
concerns. Although prior works point at existing vulnerabilities, DNN backdoor
attacks against real-life, unconstrained systems dealing with images captured
in the wild remain a blind spot of the literature. This paper conducts the
first system-level study of backdoors in deep learning-based face recognition
systems. This paper yields four contributions by exploring the feasibility of
DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the
first time two backdoor attacks on the face detection task: face generation and
face landmark shift attacks. We then show that face feature extractors trained
with large margin losses also fall victim to backdoor attacks. Combining our
models, we then show using 20 possible pipeline configurations and 15 attack
cases that a single backdoor enables an attacker to bypass the entire function
of a system. Finally, we provide stakeholders with several best practices and
countermeasures.

</details>


### [206] [Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss](https://arxiv.org/abs/2507.01630)
*Yuxiao Wang,Yu Lei,Zhenao Wei,Weiying Xue,Xinyu Jiang,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: 提出P3HOT框架解决当前HOT检测模型问题，通过多机制和新损失、指标提升性能，实验效果佳。


<details>
  <summary>Details</summary>
Motivation: 当前HOT检测模型局限于单一图像类型，存在分割过多和类别一致性问题，需改进。

Method: 提出P3HOT框架，结合提示引导和人类近端感知机制，创建RJLoss损失和AD - Acc评估指标。

Result: 在两个基准数据集四个指标上达最优，如在HOT - Annotated数据集各指标有显著提升。

Conclusion: P3HOT框架有效提升HOT检测性能。

Abstract: The task of Human-Object conTact (HOT) detection involves identifying the
specific areas of the human body that are touching objects. Nevertheless,
current models are restricted to just one type of image, often leading to too
much segmentation in areas with little interaction, and struggling to maintain
category consistency within specific regions. To tackle this issue, a HOT
framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt
guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we
utilize a semantic-driven prompt mechanism to direct the network's attention
towards the relevant regions based on the correlation between image and text.
Then a human proximal perception mechanism is employed to dynamically perceive
key depth range around the human, using learnable parameters to effectively
eliminate regions where interactions are not expected. Calculating depth
resolves the uncertainty of the overlap between humans and objects in a 2D
perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss
(RJLoss) has been created as a new loss to inhibit abnormal categories in the
same area. A new evaluation metric called ``AD-Acc.'' is introduced to address
the shortcomings of existing methods in addressing negative samples.
Comprehensive experimental results demonstrate that our approach achieves
state-of-the-art performance in four metrics across two benchmark datasets.
Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$,
\textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in
SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated
dataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.

</details>


### [207] [Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation](https://arxiv.org/abs/2507.01631)
*Camille Billouard,Dawa Derksen,Alexandre Constantin,Bruno Vallet*

Main category: cs.CV

TL;DR: 本文提出Snake - NeRF框架，可处理大规模卫星图像3D重建，能在单设备上线性复杂度处理且不损失质量。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法因训练内存占用问题局限于小场景，需解决大规模场景3D重建问题。

Method: 提出Snake - NeRF框架，将感兴趣区域划分为无重叠的3D NeRF块，裁剪图像有重叠，引入2×2 3D块推进策略和分段采样器。

Result: 实验表明可在单GPU上以线性时间复杂度有效处理大卫星图像且不损失质量。

Conclusion: Snake - NeRF框架能有效处理大规模卫星图像3D重建。

Abstract: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

</details>


### [208] [Depth Anything at Any Condition](https://arxiv.org/abs/2507.01634)
*Boyuan Sun,Modi Jin,Bowen Yin,Qibin Hou*

Main category: cs.CV

TL;DR: 提出DepthAnything - AC单目深度估计模型，采用无监督一致性正则化微调范式和空间距离约束，实验显示其有零样本能力。


<details>
  <summary>Details</summary>
Motivation: 先前基础单目深度估计模型在复杂开放环境表现不佳，且存在数据稀缺和难生成高质量伪标签问题。

Method: 提出无监督一致性正则化微调范式，只需少量无标签数据；提出空间距离约束，让模型学习块级相对关系。

Result: DepthAnything - AC在多种基准测试中展现零样本能力，包括真实世界恶劣天气、合成损坏和通用基准。

Conclusion: DepthAnything - AC能处理不同环境条件，是有效的基础单目深度估计模型。

Abstract: We present Depth Anything at Any Condition (DepthAnything-AC), a foundation
monocular depth estimation (MDE) model capable of handling diverse
environmental conditions. Previous foundation MDE models achieve impressive
performance across general scenes but not perform well in complex open-world
environments that involve challenging conditions, such as illumination
variations, adverse weather, and sensor-induced distortions. To overcome the
challenges of data scarcity and the inability of generating high-quality
pseudo-labels from corrupted images, we propose an unsupervised consistency
regularization finetuning paradigm that requires only a relatively small amount
of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to
explicitly enforce the model to learn patch-level relative relationships,
resulting in clearer semantic boundaries and more accurate details.
Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC
across diverse benchmarks, including real-world adverse weather benchmarks,
synthetic corruption benchmarks, and general benchmarks.
  Project Page: https://ghost233lism.github.io/depthanything-AC-page
  Code: https://github.com/HVision-NKU/DepthAnythingAC

</details>


### [209] [Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective](https://arxiv.org/abs/2507.01652)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Hui Deng,Xuyang Shen,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: 现有自回归图像生成模型有局限，提出LASAD注意力机制和LASADGen生成器，在ImageNet实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型依赖transformer架构有计算和内存负担，线性注意力机制会降低图像生成质量。

Method: 提出Linear Attention with Spatial-Aware Decay (LASAD)注意力机制，基于此构建LASADGen图像生成器。

Result: 在ImageNet上实验，LASADGen达到了最先进的图像生成性能和计算效率。

Conclusion: LASADGen弥合了线性注意力效率和高质量图像生成所需空间理解之间的差距。

Abstract: Autoregressive (AR) models have garnered significant attention in image
generation for their ability to effectively capture both local and global
structures within visual data. However, prevalent AR models predominantly rely
on the transformer architectures, which are beset by quadratic computational
complexity concerning input sequence length and substantial memory overhead due
to the necessity of maintaining key-value caches. Although linear attention
mechanisms have successfully reduced this burden in language models, our
initial experiments reveal that they significantly degrade image generation
quality because of their inability to capture critical long-range dependencies
in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a
novel attention mechanism that explicitly preserves genuine 2D spatial
relationships within the flattened image sequences by computing
position-dependent decay factors based on true 2D spatial location rather than
1D sequence positions. Based on this mechanism, we present LASADGen, an
autoregressive image generator that enables selective attention to relevant
spatial contexts with linear complexity. Experiments on ImageNet show LASADGen
achieves state-of-the-art image generation performance and computational
efficiency, bridging the gap between linear attention's efficiency and spatial
understanding needed for high-quality generation.

</details>


### [210] [Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions](https://arxiv.org/abs/2507.01123)
*Rahul A. Burange,Harsh K. Shinde,Omkar Mutyalwar*

Main category: cs.CV

TL;DR: 研究提出整合多源卫星影像和深度学习模型的方法用于滑坡识别和预测，评估多种模型效果，为相关工作提供见解。


<details>
  <summary>Details</summary>
Motivation: 滑坡对基础设施、经济和人类生命构成严重威胁，需要准确检测和预测，随着深度学习和遥感技术发展，自动化滑坡检测变得更有效。

Method: 利用Sentinel - 2多光谱数据和ALOS PALSAR派生的坡度及数字高程模型层，采用多种地理空间分析技术，评估多种深度学习分割模型（U - Net、DeepLabV3+、Res - Net）性能。

Result: 提出的框架有助于开发可靠的预警系统、改善灾害风险管理和可持续土地利用规划。

Conclusion: 研究结果为深度学习和多源遥感在创建强大、可扩展和可转移的滑坡预测模型方面的潜力提供了有价值的见解。

Abstract: Landslides pose severe threats to infrastructure, economies, and human lives,
necessitating accurate detection and predictive mapping across diverse
geographic regions. With advancements in deep learning and remote sensing,
automated landslide detection has become increasingly effective. This study
presents a comprehensive approach integrating multi-source satellite imagery
and deep learning models to enhance landslide identification and prediction. We
leverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and
Digital Elevation Model (DEM) layers to capture critical environmental features
influencing landslide occurrences. Various geospatial analysis techniques are
employed to assess the impact of terra in characteristics, vegetation cover,
and rainfall on detection accuracy. Additionally, we evaluate the performance
of multiple stateof-the-art deep learning segmentation models, including U-Net,
DeepLabV3+, and Res-Net, to determine their effectiveness in landslide
detection. The proposed framework contributes to the development of reliable
early warning systems, improved disaster risk management, and sustainable
land-use planning. Our findings provide valuable insights into the potential of
deep learning and multi-source remote sensing in creating robust, scalable, and
transferable landslide prediction models.

</details>


### [211] [ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving](https://arxiv.org/abs/2507.01735)
*Kai Chen,Ruiyuan Gao,Lanqing Hong,Hang Xu,Xu Jia,Holger Caesar,Dengxin Dai,Bingbing Liu,Dzmitry Tsishkou,Songcen Xu,Chunjing Xu,Qiang Xu,Huchuan Lu,Dit-Yan Yeung*

Main category: cs.CV

TL;DR: 介绍2024年ECCV同期举办的首届W - CODA研讨会，涵盖目标、活动安排等，旨在连接前沿技术与可靠自动驾驶。


<details>
  <summary>Details</summary>
Motivation: 探索自动驾驶极端情况的下一代解决方案。

Method: 邀请学界和业界5位演讲者分享进展和观点，收集研究论文，举办双轨挑战赛。

Result: 未提及明确结果。

Conclusion: 作为开拓性努力，将持续弥合前沿自动驾驶技术与应对极端情况的可靠自动驾驶代理之间的差距。

Abstract: In this paper, we present details of the 1st W-CODA workshop, held in
conjunction with the ECCV 2024. W-CODA aims to explore next-generation
solutions for autonomous driving corner cases, empowered by state-of-the-art
multimodal perception and comprehension techniques. 5 Speakers from both
academia and industry are invited to share their latest progress and opinions.
We collect research papers and hold a dual-track challenge, including both
corner case scene understanding and generation. As the pioneering effort, we
will continuously bridge the gap between frontier autonomous driving techniques
and fully intelligent, reliable self-driving agents robust towards corner
cases.

</details>


### [212] [DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting](https://arxiv.org/abs/2507.01305)
*Worameth Chinchuthakun,Pakkapon Phongthawee,Amit Raj,Varun Jampani,Pramook Khungurn,Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: 本文将单张LDR图像光照估计任务转化为镀铬球修复问题，引入DiffusionLight和DiffusionLight - Turbo方法，前者通过迭代修复计算中位数镀铬球引导结果生成，后者大幅加速计算，实验证明方法有效且泛化性好。


<details>
  <summary>Details</summary>
Motivation: 现有基于有限HDR全景数据集的方法存在泛化失败问题，且扩散模型用于光照估计有插入内容错误、无法生成HDR格式镀铬球等挑战。

Method: 提出DiffusionLight，用迭代修复计算中位数镀铬球作为低频光照先验；引入Exposure LoRA微调生成多曝光LDR图像并合并得到HDR光探针；提出DiffusionLight - Turbo，训练Turbo LoRA直接预测平均镀铬球，用LoRA交换技术简化推理。

Result: DiffusionLight每次估计约需30分钟，DiffusionLight - Turbo将运行时间减少到约30秒，实现60倍加速且质量损失极小，在不同场景下产生令人信服的光照估计。

Conclusion: 方法在不同设置下能产生令人信服的光照估计，对野外场景有更好的泛化性。

Abstract: We introduce a simple yet effective technique for estimating lighting from a
single low-dynamic-range (LDR) image by reframing the task as a chrome ball
inpainting problem. This approach leverages a pre-trained diffusion model,
Stable Diffusion XL, to overcome the generalization failures of existing
methods that rely on limited HDR panorama datasets. While conceptually simple,
the task remains challenging because diffusion models often insert incorrect or
inconsistent content and cannot readily generate chrome balls in HDR format.
Our analysis reveals that the inpainting process is highly sensitive to the
initial noise in the diffusion process, occasionally resulting in unrealistic
outputs. To address this, we first introduce DiffusionLight, which uses
iterative inpainting to compute a median chrome ball from multiple outputs to
serve as a stable, low-frequency lighting prior that guides the generation of a
high-quality final result. To generate high-dynamic-range (HDR) light probes,
an Exposure LoRA is fine-tuned to create LDR images at multiple exposure
values, which are then merged. While effective, DiffusionLight is
time-intensive, requiring approximately 30 minutes per estimation. To reduce
this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to
about 30 seconds with minimal quality loss. This 60x speedup is achieved by
training a Turbo LoRA to directly predict the averaged chrome balls from the
iterative process. Inference is further streamlined into a single denoising
pass using a LoRA swapping technique. Experimental results that show our method
produces convincing light estimates across diverse settings and demonstrates
superior generalization to in-the-wild scenarios. Our code is available at
https://diffusionlight.github.io/turbo

</details>


### [213] [Activation Reward Models for Few-Shot Model Alignment](https://arxiv.org/abs/2507.01368)
*Tianning Chai,Chancharik Mitra,Brandon Huang,Gautam Rajendrakumar Gare,Zhiqiu Lin,Assaf Arbelle,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Deva Ramanan,Roei Herzig*

Main category: cs.CV

TL;DR: 提出Activation RMs这一少样本奖励建模方法，在标准奖励建模基准上表现优于现有方法，能减轻奖励破解行为，在新提出的PreferenceHack基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统奖励建模难以适应新偏好，需独立奖励模型且依赖大量偏好数据集，要解决此问题。

Method: 引入Activation RMs，利用激活引导以最少监督和无额外模型微调构建对齐良好的奖励信号；提出PreferenceHack基准。

Result: Activation RMs在标准奖励建模基准上优于现有少样本奖励建模方法；能减轻奖励破解行为；在PreferenceHack基准上达SOTA，超越GPT - 4o。

Conclusion: Activation RMs是有效的少样本奖励建模方法，对安全关键应用有实用价值。

Abstract: Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to
human preferences is a central challenge in improving the quality of the
models' generative outputs for real-world applications. A common approach is to
use reward modeling to encode preferences, enabling alignment via post-training
using reinforcement learning. However, traditional reward modeling is not
easily adaptable to new preferences because it requires a separate reward
model, commonly trained on large preference datasets. To address this, we
introduce Activation Reward Models (Activation RMs) -- a novel few-shot reward
modeling method that leverages activation steering to construct well-aligned
reward signals using minimal supervision and no additional model finetuning.
Activation RMs outperform existing few-shot reward modeling approaches such as
LLM-as-a-judge with in-context learning, voting-based scoring, and token
probability scoring on standard reward modeling benchmarks. Furthermore, we
demonstrate the effectiveness of Activation RMs in mitigating reward hacking
behaviors, highlighting their utility for safety-critical applications. Toward
this end, we propose PreferenceHack, a novel few-shot setting benchmark, the
first to test reward models on reward hacking in a paired preference format.
Finally, we show that Activation RM achieves state-of-the-art performance on
this benchmark, surpassing even GPT-4o.

</details>


### [214] [Active Measurement: Efficient Estimation at Scale](https://arxiv.org/abs/2507.01372)
*Max Hamilton,Jinlin Lai,Wenlong Zhao,Subhransu Maji,Daniel Sheldon*

Main category: cs.CV

TL;DR: 介绍了用于科学测量的人在回路AI框架主动测量，能提供精确估计，减少估计误差。


<details>
  <summary>Details</summary>
Motivation: 当前AI科学发现工作流缺乏所需的准确性和统计保证。

Method: 引入主动测量框架，用AI模型预测个体测量值，通过重要性采样进行人工标注，利用新标注改进模型和完善蒙特卡罗估计。

Result: 推导出新的估计器、加权方案和置信区间，在多个测量任务中比其他方法减少了估计误差。

Conclusion: 主动测量即使在AI模型不完美时也能提供精确估计，AI模型准确时所需人力少。

Abstract: AI has the potential to transform scientific discovery by analyzing vast
datasets with little human effort. However, current workflows often do not
provide the accuracy or statistical guarantees that are needed. We introduce
active measurement, a human-in-the-loop AI framework for scientific
measurement. An AI model is used to predict measurements for individual units,
which are then sampled for human labeling using importance sampling. With each
new set of human labels, the AI model is improved and an unbiased Monte Carlo
estimate of the total measurement is refined. Active measurement can provide
precise estimates even with an imperfect AI model, and requires little human
effort when the AI model is very accurate. We derive novel estimators,
weighting schemes, and confidence intervals, and show that active measurement
reduces estimation error compared to alternatives in several measurement tasks.

</details>


### [215] [Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps](https://arxiv.org/abs/2507.01397)
*Khanh Son Pham,Christian Witte,Jens Behley,Johannes Betz,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 本文提出利用SD地图信息预测车道段、拓扑和道路边界的方法，性能优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶汽车依赖高清地图，在线构建高清地图有挑战，需统一一致地建模道路拓扑复杂性。

Method: 提出利用SD地图的先验信息预测车道段、拓扑和道路边界的方法，采用含先验信息和去噪技术的混合车道段编码网络架构，并利用过去帧保证时间一致性。

Result: 实验表明该方法大幅优于先前方法。

Conclusion: 该建模方案有显著优势。

Abstract: Most autonomous cars rely on the availability of high-definition (HD) maps.
Current research aims to address this constraint by directly predicting HD map
elements from onboard sensors and reasoning about the relationships between the
predicted map and traffic elements. Despite recent advancements, the coherent
online construction of HD maps remains a challenging endeavor, as it
necessitates modeling the high complexity of road topologies in a unified and
consistent manner. To address this challenge, we propose a coherent approach to
predict lane segments and their corresponding topology, as well as road
boundaries, all by leveraging prior map information represented by commonly
available standard-definition (SD) maps. We propose a network architecture,
which leverages hybrid lane segment encodings comprising prior information and
denoising techniques to enhance training stability and performance.
Furthermore, we facilitate past frames for temporal consistency. Our
experimental evaluation demonstrates that our approach outperforms previous
methods by a large margin, highlighting the benefits of our modeling scheme.

</details>


### [216] [Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging](https://arxiv.org/abs/2507.01788)
*Montasir Shams,Chashi Mahiul Islam,Shaeke Salman,Phat Tran,Xiuwen Liu*

Main category: cs.CV

TL;DR: 本文指出视觉Transformer（ViTs）在医学图像分类中表征缺乏语义意义且易受微小变化影响。


<details>
  <summary>Details</summary>
Motivation: ViTs在医学成像任务中虽精度高，但因自身大小和自注意力机制复杂，其表征的语义意义不明，需进行研究。

Method: 使用基于投影梯度的算法。

Result: 发现ViTs表征无语义意义，易受微小变化影响，细微差异图像表征不同，不同语义类图像表征可能相同，还会导致分类结果不可靠，微小变化使分类准确率降低超60%。

Conclusion: 这是首次系统证明ViT在医学图像分类表征中缺乏语义意义的工作，揭示了其在安全关键系统部署的关键挑战。

Abstract: Vision transformers (ViTs) have rapidly gained prominence in medical imaging
tasks such as disease classification, segmentation, and detection due to their
superior accuracy compared to conventional deep learning models. However, due
to their size and complex interactions via the self-attention mechanism, they
are not well understood. In particular, it is unclear whether the
representations produced by such models are semantically meaningful. In this
paper, using a projected gradient-based algorithm, we show that their
representations are not semantically meaningful and they are inherently
vulnerable to small changes. Images with imperceptible differences can have
very different representations; on the other hand, images that should belong to
different semantic classes can have nearly identical representations. Such
vulnerability can lead to unreliable classification results; for example,
unnoticeable changes cause the classification accuracy to be reduced by over
60\%. %. To the best of our knowledge, this is the first work to systematically
demonstrate this fundamental lack of semantic meaningfulness in ViT
representations for medical image classification, revealing a critical
challenge for their deployment in safety-critical systems.

</details>


### [217] [Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention](https://arxiv.org/abs/2507.01417)
*Jiawei Gu,Ziyue Qiao,Zechao Li*

Main category: cs.CV

TL;DR: 本文观察到ID与OOD样本梯度现象差异，提出推理阶段技术结合局部一阶近似，实验显示该方法有效且轻量。


<details>
  <summary>Details</summary>
Motivation: 在开放世界环境安全部署深度模型需有效OOD检测，受ID与OOD样本梯度方向差异现象启发。

Method: 提出推理阶段技术短路虚假梯度利用的特征坐标，引入局部一阶近似避免重新计算对数。

Result: 在标准OOD基准测试中取得显著改进。

Conclusion: 方法轻量，对标准推理流程改动小，为现实应用中稳健OOD检测提供实用途径。

Abstract: Out-of-Distribution (OOD) detection is critical for safely deploying deep
models in open-world environments, where inputs may lie outside the training
distribution. During inference on a model trained exclusively with
In-Distribution (ID) data, we observe a salient gradient phenomenon: around an
ID sample, the local gradient directions for "enhancing" that sample's
predicted class remain relatively consistent, whereas OOD samples--unseen in
training--exhibit disorganized or conflicting gradient directions in the same
neighborhood. Motivated by this observation, we propose an inference-stage
technique to short-circuit those feature coordinates that spurious gradients
exploit to inflate OOD confidence, while leaving ID classification largely
intact. To circumvent the expense of recomputing the logits after this gradient
short-circuit, we further introduce a local first-order approximation that
accurately captures the post-modification outputs without a second forward
pass. Experiments on standard OOD benchmarks show our approach yields
substantial improvements. Moreover, the method is lightweight and requires
minimal changes to the standard inference pipeline, offering a practical path
toward robust OOD detection in real-world applications.

</details>


### [218] [Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware](https://arxiv.org/abs/2507.01472)
*Jonáš Herec,Vít Růžička,Rado Pitoňák*

Main category: cs.CV

TL;DR: 本文聚焦高效低功耗算法加速甲烷检测，测试新方法并改进Mag1c算法，集成机器学习模型，找到有潜力候选方案，还提出并评估波段选择策略，为星载甲烷检测奠定基础，代码等开源。


<details>
  <summary>Details</summary>
Motivation: 甲烷是强效温室气体，早期检测其泄漏有助于缓解气候变化，但现有任务多为手动且下行速率慢，传统方法对星载硬件计算要求高，需高效低功耗算法。

Method: 测试未用于甲烷检测的快速目标检测方法（ACE、CEM），提出Mag1c改进版Mag1c - SAS，将其与机器学习模型（U - Net、LinkNet）集成，提出并评估三种波段选择策略。

Result: 确定两个有潜力候选方案（Mag1c - SAS和CEM），检测强羽流准确且计算高效，在受限硬件上比原Mag1c快约100倍和230倍；一种波段选择策略优于传统方法且处理更快。

Conclusion: 本研究为低硬件要求的星载甲烷检测进步奠定基础，提高数据及时交付能力，代码、数据和模型开源。

Abstract: Methane is a potent greenhouse gas, and detecting its leaks early via
hyperspectral satellite imagery can help mitigate climate change. Meanwhile,
many existing missions operate in manual tasking regimes only, thus missing
potential events of interest. To overcome slow downlink rates cost-effectively,
onboard detection is a viable solution. However, traditional methane
enhancement methods are too computationally demanding for resource-limited
onboard hardware. This work accelerates methane detection by focusing on
efficient, low-power algorithms. We test fast target detection methods (ACE,
CEM) that have not been previously used for methane detection and propose a
Mag1c-SAS - a significantly faster variant of the current state-of-the-art
algorithm for methane detection: Mag1c. To explore their true detection
potential, we integrate them with a machine learning model (U-Net, LinkNet).
Our results identify two promising candidates (Mag1c-SAS and CEM), both
acceptably accurate for the detection of strong plumes and computationally
efficient enough for onboard deployment: one optimized more for accuracy, the
other more for speed, achieving up to ~100x and ~230x faster computation than
original Mag1c on resource-limited hardware. Additionally, we propose and
evaluate three band selection strategies. One of them can outperform the method
traditionally used in the field while using fewer channels, leading to even
faster processing without compromising accuracy. This research lays the
foundation for future advancements in onboard methane detection with minimal
hardware requirements, improving timely data delivery. The produced code, data,
and models are open-sourced and can be accessed from
https://github.com/zaitra/methane-filters-benchmark.

</details>


### [219] [Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation](https://arxiv.org/abs/2507.01509)
*Tapas K. Dutta,Snehashis Majhi,Deepak Ranjan Nayak,Debesh Jha*

Main category: cs.CV

TL;DR: 现有方法在息肉分割上有局限，提出SAM - MaGuP方法，在多数据集评估中表现优于现有方法，设定新基准。


<details>
  <summary>Details</summary>
Motivation: 现有编码器 - 解码器CNN和基于Transformer的方法在息肉分割上，对边界弱或模糊的息肉分割性能不稳定，区分息肉和非息肉能力有限，泛化性不足，无法满足实时临床应用需求。

Method: 在Segment Anything Model (SAM)中加入边界蒸馏模块和1D - 2D Mamba适配器，提出SAM - MaGuP方法。

Result: 在五个不同数据集上的广泛评估显示，SAM - MaGuP优于现有方法，实现了无与伦比的分割准确性和鲁棒性。

Conclusion: 提出的Mamba引导的边界先验和1D - 2D Mamba块设定了该领域新基准，推动息肉分割达到新高度。

Abstract: Polyp segmentation in colonoscopy images is crucial for early detection and
diagnosis of colorectal cancer. However, this task remains a significant
challenge due to the substantial variations in polyp shape, size, and color, as
well as the high similarity between polyps and surrounding tissues, often
compounded by indistinct boundaries. While existing encoder-decoder CNN and
transformer-based approaches have shown promising results, they struggle with
stable segmentation performance on polyps with weak or blurry boundaries. These
methods exhibit limited abilities to distinguish between polyps and non-polyps
and capture essential boundary cues. Moreover, their generalizability still
falls short of meeting the demands of real-time clinical applications. To
address these limitations, we propose SAM-MaGuP, a groundbreaking approach for
robust polyp segmentation. By incorporating a boundary distillation module and
a 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels
at resolving weak boundary challenges and amplifies feature learning through
enriched global contextual interactions. Extensive evaluations across five
diverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,
achieving unmatched segmentation accuracy and robustness. Our key innovations,
a Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in
the field, pushing the boundaries of polyp segmentation to new heights.

</details>


### [220] [How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks](https://arxiv.org/abs/2507.01955)
*Rahul Ramachandran,Ali Garjani,Roman Bachmann,Andrei Atanov,Oğuzhan Fatih Kar,Amir Zamir*

Main category: cs.CV

TL;DR: 本文对多个多模态基础模型在计算机视觉任务上进行基准测试，通过提示链解决挑战，发现模型表现与专业模型有差距但为不错的多面手，不同模型有不同表现特点。


<details>
  <summary>Details</summary>
Motivation: 不清楚多模态基础模型在理解视觉方面的具体水平，需进行基准测试。

Method: 使用既定数据集，将标准视觉任务通过提示链转换为文本可提示和API兼容的任务，创建标准化基准测试框架。

Result: 模型在各任务上未达专业模型水平，但为不错的多面手；语义任务表现好于几何任务；提示链技术对性能有影响，好的模型对提示变化敏感度低；GPT - 4o在非推理模型中表现最佳；推理模型在几何任务有提升；有原生图像生成能力的模型有幻觉和空间对齐问题。

Conclusion: 多模态基础模型在计算机视觉任务上有其优势和不足，不同模型表现各异，为后续研究提供参考。

Abstract: Multimodal foundation models, such as GPT-4o, have recently made remarkable
progress, but it is not clear where exactly these models stand in terms of
understanding vision. In this paper, we benchmark the performance of popular
multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0
Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision
tasks (semantic segmentation, object detection, image classification, depth and
surface normal prediction) using established datasets (e.g., COCO, ImageNet and
its variants, etc).
  The main challenges to performing this are: 1) most models are trained to
output text and cannot natively express versatile domains, such as segments or
3D geometry, and 2) many leading models are proprietary and accessible only at
an API level, i.e., there is no weight access to adapt them. We address these
challenges by translating standard vision tasks into equivalent text-promptable
and API-compatible tasks via prompt chaining to create a standardized
benchmarking framework.
  We observe that 1) the models are not close to the state-of-the-art
specialist models at any task. However, 2) they are respectable generalists;
this is remarkable as they are presumably trained on primarily image-text-based
tasks. 3) They perform semantic tasks notably better than geometric ones. 4)
While the prompt-chaining techniques affect performance, better models exhibit
less sensitivity to prompt variations. 5) GPT-4o performs the best among
non-reasoning models, securing the top position in 4 out of 6 tasks, 6)
reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a
preliminary analysis of models with native image generation, like the latest
GPT-4o, shows they exhibit quirks like hallucinations and spatial
misalignments.

</details>


### [221] [Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation](https://arxiv.org/abs/2507.01957)
*Zhuoyang Zhang,Luke J. Huang,Chengyue Wu,Shang Yang,Kelly Peng,Yao Lu,Song Han*

Main category: cs.CV

TL;DR: 提出Locality - aware Parallel Decoding (LPD)加速自回归图像生成，减少生成步骤和延迟。


<details>
  <summary>Details</summary>
Motivation: 传统自回归图像生成依赖下一个块预测，内存受限导致高延迟，现有多块预测并行化工作效果有限。

Method: 引入Flexible Parallelized Autoregressive Modeling实现任意生成顺序和并行度，使用可学习位置查询令牌；采用Locality - aware Generation Ordering形成组以减少组内依赖、增加上下文支持。

Result: 在ImageNet类条件生成上，256×256分辨率下生成步骤从256减到20，512×512分辨率下从1024减到48，且延迟至少比之前并行自回归模型低3.4倍。

Conclusion: LPD能在不降低生成质量的情况下实现高度并行化，加速自回归图像生成。

Abstract: We present Locality-aware Parallel Decoding (LPD) to accelerate
autoregressive image generation. Traditional autoregressive image generation
relies on next-patch prediction, a memory-bound process that leads to high
latency. Existing works have tried to parallelize next-patch prediction by
shifting to multi-patch prediction to accelerate the process, but only achieved
limited parallelization. To achieve high parallelization while maintaining
generation quality, we introduce two key techniques: (1) Flexible Parallelized
Autoregressive Modeling, a novel architecture that enables arbitrary generation
ordering and degrees of parallelization. It uses learnable position query
tokens to guide generation at target positions while ensuring mutual visibility
among concurrently generated tokens for consistent parallel decoding. (2)
Locality-aware Generation Ordering, a novel schedule that forms groups to
minimize intra-group dependencies and maximize contextual support, enhancing
generation quality. With these designs, we reduce the generation steps from 256
to 20 (256$\times$256 res.) and 1024 to 48 (512$\times$512 res.) without
compromising quality on the ImageNet class-conditional generation, and
achieving at least 3.4$\times$ lower latency than previous parallelized
autoregressive models.

</details>


### [222] [SPoT: Subpixel Placement of Tokens in Vision Transformers](https://arxiv.org/abs/2507.01654)
*Martine Hjelkrem-Tan,Marius Aasan,Gabriel Y. Arteaga,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: 提出SPoT标记化策略，解决标准标记化方法局限，减少推理所需标记数，为ViT架构提供新方向。


<details>
  <summary>Details</summary>
Motivation: 标准标记化方法将特征限制在离散块网格，使模型无法充分利用稀疏机制，需提出新策略解决该问题。

Method: 提出Subpixel Placement of Tokens (SPoT)标记化策略，使用oracle - guided search方法。

Result: 发现理想的亚像素标记定位可带来显著性能提升，大幅减少推理时准确预测所需的标记数量。

Conclusion: SPoT为灵活、高效和可解释的ViT架构提供新方向，将稀疏性变为战略优势。

Abstract: Vision Transformers naturally accommodate sparsity, yet standard tokenization
methods confine features to discrete patch grids. This constraint prevents
models from fully exploiting sparse regimes, forcing awkward compromises. We
propose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that
positions tokens continuously within images, effectively sidestepping
grid-based limitations. With our proposed oracle-guided search, we uncover
substantial performance gains achievable with ideal subpixel token positioning,
drastically reducing the number of tokens necessary for accurate predictions
during inference. SPoT provides a new direction for flexible, efficient, and
interpretable ViT architectures, redefining sparsity as a strategic advantage
rather than an imposed limitation.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [223] [Characterizing control between interacting subsystems with deep Jacobian estimation](https://arxiv.org/abs/2507.01946)
*Adam J. Eisen,Mitchell Ostrow,Sarthak Chandra,Leo Kozachkov,Earl K. Miller,Ila R. Fiete*

Main category: q-bio.QM

TL;DR: 提出数据驱动的非线性控制理论框架及JacobianODE深度学习方法研究生物子系统交互，在复杂系统上表现出色并应用于RNN。


<details>
  <summary>Details</summary>
Motivation: 现有理解子系统控制的方法多为线性，无法描述非线性复杂系统的丰富上下文效应，需新方法。

Method: 设计数据驱动的非线性控制理论框架，提出JacobianODE深度学习方法从时间序列数据学习Jacobian。

Result: JacobianODE在复杂系统上优于现有Jacobian估计方法；应用于RNN表明学习中“感官”区域对“认知”区域控制增强；可直接控制RNN。

Conclusion: 为基于理论和数据驱动理解生物子系统间交互奠定基础。

Abstract: Biological function arises through the dynamical interactions of multiple
subsystems, including those between brain areas, within gene regulatory
networks, and more. A common approach to understanding these systems is to
model the dynamics of each subsystem and characterize communication between
them. An alternative approach is through the lens of control theory: how the
subsystems control one another. This approach involves inferring the
directionality, strength, and contextual modulation of control between
subsystems. However, methods for understanding subsystem control are typically
linear and cannot adequately describe the rich contextual effects enabled by
nonlinear complex systems. To bridge this gap, we devise a data-driven
nonlinear control-theoretic framework to characterize subsystem interactions
via the Jacobian of the dynamics. We address the challenge of learning
Jacobians from time-series data by proposing the JacobianODE, a deep learning
method that leverages properties of the Jacobian to directly estimate it for
arbitrary dynamical systems from data alone. We show that JacobianODEs
outperform existing Jacobian estimation methods on challenging systems,
including high-dimensional chaos. Applying our approach to a multi-area
recurrent neural network (RNN) trained on a working memory selection task, we
show that the "sensory" area gains greater control over the "cognitive" area
over learning. Furthermore, we leverage the JacobianODE to directly control the
trained RNN, enabling precise manipulation of its behavior. Our work lays the
foundation for a theoretically grounded and data-driven understanding of
interactions among biological subsystems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [224] [A first-order method for nonconvex-nonconcave minimax problems under a local Kurdyka-Łojasiewicz condition](https://arxiv.org/abs/2507.01932)
*Zhaosong Lu,Xiangyuan Wang*

Main category: math.OC

TL;DR: 研究一类非凸 - 非凹极小极大问题，利用相关性质提出不精确近端梯度法并给出复杂度保证。


<details>
  <summary>Details</summary>
Motivation: 现有文献中的全局KL或PL条件过强且实际应用受限，局部KL条件虽适用场景广但有分析挑战。

Method: 证明关联的最大函数是局部Hölder光滑的，提出不精确近端梯度法，通过对KL结构子问题应用近端梯度法计算最大函数的不精确梯度。

Result: 在温和假设下，建立了计算极小极大问题近似驻点的复杂度保证。

Conclusion: 所提出的方法能有效解决满足局部KL条件的非凸 - 非凹极小极大问题。

Abstract: We study a class of nonconvex-nonconcave minimax problems in which the inner
maximization problem satisfies a local Kurdyka-{\L}ojasiewicz (KL) condition
that may vary with the outer minimization variable. In contrast to the global
KL or Polyak-{\L}ojasiewicz (PL) conditions commonly assumed in the literature
-- which are significantly stronger and often too restrictive in practice --
this local KL condition accommodates a broader range of practical scenarios.
However, it also introduces new analytical challenges. In particular, as an
optimization algorithm progresses toward a stationary point of the problem, the
region over which the KL condition holds may shrink, resulting in a more
intricate and potentially ill-conditioned landscape. To address this challenge,
we show that the associated maximal function is locally H\"older smooth.
Leveraging this key property, we develop an inexact proximal gradient method
for solving the minimax problem, where the inexact gradient of the maximal
function is computed by applying a proximal gradient method to a KL-structured
subproblem. Under mild assumptions, we establish complexity guarantees for
computing an approximate stationary point of the minimax problem.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [225] [Transfer Learning for VLC-based indoor Localization: Addressing Environmental Variability](https://arxiv.org/abs/2507.01575)
*Masood Jan,Wafa Njima,Xun Zhang,Alexander Artemenko*

Main category: eess.SP

TL;DR: 提出基于迁移学习的可见光通信室内定位方法，能提高精度、降低能耗和计算时间，适应环境变化且具成本效益。


<details>
  <summary>Details</summary>
Motivation: 可见光通信室内定位受环境变化影响，需解决相关挑战以实现准确室内定位。

Method: 采用迁移学习框架，结合深度神经网络，利用博世工厂的真实数据。

Result: 与传统模型相比，定位精度提高47%，能耗降低32%，计算时间减少40%，用30%数据集达相似精度。

Conclusion: 该解决方案适应环境变化，是工业4.0中经济高效且可扩展的选择。

Abstract: Accurate indoor localization is crucial in industrial environments. Visible
Light Communication (VLC) has emerged as a promising solution, offering high
accuracy, energy efficiency, and minimal electromagnetic interference. However,
VLC-based indoor localization faces challenges due to environmental
variability, such as lighting fluctuations and obstacles. To address these
challenges, we propose a Transfer Learning (TL)-based approach for VLC-based
indoor localization. Using real-world data collected at a BOSCH factory, the TL
framework integrates a deep neural network (DNN) to improve localization
accuracy by 47\%, reduce energy consumption by 32\%, and decrease computational
time by 40\% compared to the conventional models. The proposed solution is
highly adaptable under varying environmental conditions and achieves similar
accuracy with only 30\% of the dataset, making it a cost-efficient and scalable
option for industrial applications in Industry 4.0.

</details>


### [226] [Token Communication in the Era of Large Models: An Information Bottleneck-Based Approach](https://arxiv.org/abs/2507.01728)
*Hao Wei,Wanli Ni,Wen Wang,Wenjun Xu,Dusit Niyato,Ping Zhang*

Main category: eess.SP

TL;DR: 提出统一令牌通信范式UniToCom，结合生成信息瓶颈原理等提升通信效率，仿真验证其优势，为下一代智能通信提供方案。


<details>
  <summary>Details</summary>
Motivation: 为提升通信效率、降低计算复杂度，解决自回归建模中的方差崩溃问题，实现多模态理解和生成。

Method: 提出生成信息瓶颈（GenIB）原理用于高效令牌表示，开发σ - GenIB解决方差崩溃问题，在接收器采用基于因果Transformer的多模态大语言模型处理令牌。

Result: 仿真结果表明，在动态信道条件下，UniToCom比基线方法更有效、更具优势。

Conclusion: UniToCom将令牌处理与多模态大语言模型集成，能实现可扩展和通用的通信，为下一代智能通信提供了潜在解决方案。

Abstract: This letter proposes UniToCom, a unified token communication paradigm that
treats tokens as the fundamental units for both processing and wireless
transmission. Specifically, to enable efficient token representations, we
propose a generative information bottleneck (GenIB) principle, which
facilitates the learning of tokens that preserve essential information while
supporting reliable generation across multiple modalities. By doing this,
GenIB-based tokenization is conducive to improving the communication efficiency
and reducing computational complexity. Additionally, we develop $\sigma$-GenIB
to address the challenges of variance collapse in autoregressive modeling,
maintaining representational diversity and stability. Moreover, we employ a
causal Transformer-based multimodal large language model (MLLM) at the receiver
to unify the processing of both discrete and continuous tokens under the
next-token prediction paradigm. Simulation results validate the effectiveness
and superiority of the proposed UniToCom compared to baselines under dynamic
channel conditions. By integrating token processing with MLLMs, UniToCom
enables scalable and generalizable communication in favor of multimodal
understanding and generation, providing a potential solution for
next-generation intelligent communications.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [227] [A Deterministic Partition Tree and Applications](https://arxiv.org/abs/2507.01775)
*Haitao Wang*

Main category: cs.CG

TL;DR: 提出Chan随机分区树的确定性变体，有诸多应用，如在d维单纯形范围计数问题上打破已知下界，且不依赖位打包技术，还改进了其他经典问题。


<details>
  <summary>Details</summary>
Motivation: 获得确定性算法结果，解决经典问题并打破已知下界，适用于更偏好确定性结果的场景。

Method: 提出Chan随机分区树的确定性变体。

Result: 对于d维单纯形范围计数，构建数据结构使用O(n)空间和O(n^{1 + ε})预处理时间，查询时间为o(n^{1 - 1/d})；在其他经典问题上也有确定性改进。

Conclusion: 该确定性变体有众多应用，未来有望出现更多应用。

Abstract: In this paper, we present a deterministic variant of Chan's randomized
partition tree [Discret. Comput. Geom., 2012]. This result leads to numerous
applications. In particular, for $d$-dimensional simplex range counting (for
any constant $d \ge 2$), we construct a data structure using $O(n)$ space and
$O(n^{1+\epsilon})$ preprocessing time, such that each query can be answered in
$o(n^{1-1/d})$ time (specifically, $O(n^{1-1/d} / \log^{\Omega(1)} n)$ time),
thereby breaking an $\Omega(n^{1-1/d})$ lower bound known for the semigroup
setting. Notably, our approach does not rely on any bit-packing techniques. We
also obtain deterministic improvements for several other classical problems,
including simplex range stabbing counting and reporting, segment intersection
detection, counting and reporting, ray-shooting among segments, and more.
Similar to Chan's original randomized partition tree, we expect that additional
applications will emerge in the future, especially in situations where
deterministic results are preferred.

</details>


### [228] [Empirical Analysis Of Heuristic and Approximation Algorithms for the The Mutual-Visibility Problem](https://arxiv.org/abs/2507.01076)
*Vanja Stojanović,Bor Pangeršič*

Main category: cs.CG

TL;DR: 本文实现并评估三种算法解决NP完全的互可见性问题，小图结果接近理论界限，大图有差异，遗传算法等表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前NP完全的互可见性问题缺乏对其实际行为的实证分析，本文旨在填补该空白。

Method: 在不同合成图数据集上实现并评估三种算法，分别是直接贪婪启发式算法、基于超图的近似算法和遗传算法。

Result: 小图中算法结果与理论界限一致，大图中结果与理论界限有明显差异，且缺乏严格界限难以评估绝对质量。

Conclusion: 在已知最优图上验证，遗传算法和其他启发式算法在测试方法中表现最佳。

Abstract: The NP-complete mutual-visibility (MV) problem currently lacks empirical
analysis on its practical behaviour despite theoretical studies. This paper
addresses this gap by implementing and evaluating three distinct algorithms - a
direct greedy heuristic, a hypergraph-based approximation, and a genetic
algorithm - on diverse synthetic graph datasets, including those with
analytically known $\mu(G)$ values and general graph models. Our results
demonstrate that for smaller graphs, the algorithms consistently achieve MV set
sizes aligning with theoretical bounds. However, for larger instances, achieved
solution sizes notably diverge from theoretical limits; this, combined with the
absence of tight bounds, complicates absolute quality assessment. Nevertheless,
validation on known optimal graphs showed the Genetic Algorithm and other
heuristics empirically performing best among tested methods.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [229] [Shrinkage-Based Regressions with Many Related Treatments](https://arxiv.org/abs/2507.01202)
*Enes Dilber,Colin Gray*

Main category: econ.EM

TL;DR: 提出计算轻量模型处理观测因果模型中多相关部分重叠处理效应估计问题，理论和模拟证明效果并展示应用。


<details>
  <summary>Details</summary>
Motivation: 现有估计独立处理系数的常见方法噪声大，无法用于实际决策，需处理多相关部分重叠处理效应。

Method: 提出使用定制岭回归在异质和同质模型间转换的计算轻量模型。

Result: 显著降低每个子处理效应的均方误差，能轻松重建聚合处理效应。

Conclusion: 该模型有良好性质，已在Wayfair实现靶向决策。

Abstract: When using observational causal models, practitioners often want to
disentangle the effects of many related, partially-overlapping treatments.
Examples include estimating treatment effects of different marketing
touchpoints, ordering different types of products, or signing up for different
services. Common approaches that estimate separate treatment coefficients are
too noisy for practical decision-making. We propose a computationally light
model that uses a customized ridge regression to move between a heterogeneous
and a homogenous model: it substantially reduces MSE for the effects of each
individual sub-treatment while allowing us to easily reconstruct the effects of
an aggregated treatment. We demonstrate the properties of this estimator in
theory and simulation, and illustrate how it has unlocked targeted
decision-making at Wayfair.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [230] [AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma](https://arxiv.org/abs/2507.01081)
*Megan T. deBettencourt,Sruthi Sakthivel,Emily A. Holmes,Mark Chevillet*

Main category: cs.HC

TL;DR: 研究测试ANTIDOTE结合AI引导和瞳孔测量法减少心理创伤后侵入性记忆，干预组报告侵入性记忆显著减少，瞳孔大小可跟踪干预参与度并预测症状减轻，为可扩展的AI引导数字干预开辟道路。


<details>
  <summary>Details</summary>
Motivation: 全球创伤普遍，基于证据的数字治疗大多需人工指导，限制可扩展性，探索生成式AI和神经技术作为可扩展替代方案的可能性。

Method: 测试ANTIDOTE，结合AI引导和瞳孔测量法，对100名健康志愿者进行实验，让其观看创伤事件视频后随机分组到干预组或积极对照组。

Result: 干预组参与者在接下来一周报告的侵入性记忆显著减少，事后评估确认AI引导成功实施干预，瞳孔大小跟踪干预参与度并预测症状减轻。

Conclusion: 研究结果为能应对创伤普遍性的严格AI引导数字干预开辟了道路。

Abstract: Trauma prevalence is vast globally. Evidence-based digital treatments can
help, but most require human guidance. Human guides provide tailored
instructions and responsiveness to internal cognitive states, but limit
scalability. Can generative AI and neurotechnology provide a scalable
alternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to
automatically deliver and monitor an evidence-based digital treatment,
specifically the Imagery Competing Task Intervention (ICTI), to reduce
intrusive memories after psychological trauma. One hundred healthy volunteers
were exposed to videos of traumatic events and randomly assigned to an
intervention or active control condition. As predicted, intervention
participants reported significantly fewer intrusive memories over the following
week. Post-hoc assessment against clinical rubrics confirmed the AI guide
delivered the intervention successfully. Additionally, pupil size tracked
intervention engagement and predicted symptom reduction, providing a candidate
biomarker of intervention effectiveness. These findings open a path toward
rigorous AI-guided digital interventions that can scale to trauma prevalence.

</details>


### [231] [AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance](https://arxiv.org/abs/2507.01274)
*Vishakha Lall,Yisi Liu*

Main category: cs.HC

TL;DR: 本文开发AI驱动框架评估海事学员表现，算法准确率高，能变革海事训练。


<details>
  <summary>Details</summary>
Motivation: 传统海事模拟器训练依赖主观评估，存在主观性、难测关键特征和认知局限等问题，需客观评估方法。

Method: 开发AI驱动框架，集成眼动追踪、语音识别、计算机视觉等技术，在模拟场景中评估模型。

Result: AI算法准确率高，视觉检测约92%、海事语音识别约91%、压力检测约90%，超现有基准。

Conclusion: AI可通过提供客观绩效分析、个性化反馈，提升应对现实挑战的能力，变革海事训练。

Abstract: Traditional simulator-based training for maritime professionals is critical
for ensuring safety at sea but often depends on subjective trainer assessments
of technical skills, behavioral focus, communication, and body language, posing
challenges such as subjectivity, difficulty in measuring key features, and
cognitive limitations. Addressing these issues, this study develops an
AI-driven framework to enhance maritime training by objectively assessing
trainee performance through visual focus tracking, speech recognition, and
stress detection, improving readiness for high-risk scenarios. The system
integrates AI techniques, including visual focus determination using eye
tracking, pupil dilation analysis, and computer vision; communication analysis
through a maritime-specific speech-to-text model and natural language
processing; communication correctness using large language models; and mental
stress detection via vocal pitch. Models were evaluated on data from simulated
maritime scenarios with seafarers exposed to controlled high-stress events. The
AI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for
maritime speech recognition, and ~90% for stress detection, surpassing existing
benchmarks. The system provides insights into visual attention, adherence to
communication checklists, and stress levels under demanding conditions. This
study demonstrates how AI can transform maritime training by delivering
objective performance analytics, enabling personalized feedback, and improving
preparedness for real-world operational challenges.

</details>


### [232] [Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants](https://arxiv.org/abs/2507.01548)
*Wen Zhan,Ziqun Hua,Peiyue Lin,Yunfei Chen*

Main category: cs.HC

TL;DR: 本文探讨中国城市老年移民如何通过AI辅助共创表达个人叙事，通过试点工作坊实现，为人类与AI协作及老龄化研究提供新视角。


<details>
  <summary>Details</summary>
Motivation: 探索中国城市老年移民如何借助AI辅助共创来表达常碎片化、代表性不足或难以言说的个人叙事。

Method: 开展结合口头讲故事和汉字符号重构的试点工作坊，利用大语言模型建议的小篆字形和实物材料，在人工引导和弱AI支持下进行创作。

Result: 参与者在无需数字素养的情况下将生活经历转化为视觉和触觉表达。

Conclusion: 该方法将AI定位为支持机制，为人类与AI协作和老龄化研究提供新视角，支持社会技术系统中的叙事能动性。

Abstract: This paper explores how older adults, particularly aging migrants in urban
China, can engage AI-assisted co-creation to express personal narratives that
are often fragmented, underrepresented, or difficult to verbalize. Through a
pilot workshop combining oral storytelling and the symbolic reconstruction of
Hanzi, participants shared memories of migration and recreated new character
forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),
together with physical materials. Supported by human facilitation and a soft AI
presence, participants transformed lived experience into visual and tactile
expressions without requiring digital literacy. This approach offers new
perspectives on human-AI collaboration and aging by repositioning AI not as a
content producer but as a supportive mechanism, and by supporting narrative
agency within sociotechnical systems.

</details>


### [233] [Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America](https://arxiv.org/abs/2507.01719)
*Dorian Peters,Fernanda Espinoza,Marco da Re,Guido Ivetta,Luciana Benotti,Rafael A. Calvo*

Main category: cs.HC

TL;DR: 本文通过拉美参与式研讨会的定性数据，采用自下而上的方法构建对数字健康文化问题的理解，引入'健康多元通用对话式AI'框架。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型忽略全球许多实际生活体验，为使对话式AI在多元文化和语言环境中有效，需新方法。

Method: 基于拉丁美洲参与式研讨会收集的定性数据，采用自下而上的本地化方法。

Result: 发现学术层面的文化概念界限在实际中失去意义，技术需与更广泛框架结合。

Conclusion: 引入'健康多元通用对话式AI'框架，认为除数据外还需更多关联性和包容性。

Abstract: There is justifiable interest in leveraging conversational AI (CAI) for
health across the majority world, but to be effective, CAI must respond
appropriately within culturally and linguistically diverse contexts. Therefore,
we need ways to address the fact that current LLMs exclude many lived
experiences globally. Various advances are underway which focus on top-down
approaches and increasing training data. In this paper, we aim to complement
these with a bottom-up locally-grounded approach based on qualitative data
collected during participatory workshops in Latin America. Our goal is to
construct a rich and human-centred understanding of: a) potential areas of
cultural misalignment in digital health; b) regional perspectives on chatbots
for health and c)strategies for creating culturally-appropriate CAI; with a
focus on the understudied Latin American context. Our findings show that
academic boundaries on notions of culture lose meaning at the ground level and
technologies will need to engage with a broader framework; one that
encapsulates the way economics, politics, geography and local logistics are
entangled in cultural experience. To this end, we introduce a framework for
'Pluriversal Conversational AI for Health' which allows for the possibility
that more relationality and tolerance, rather than just more data, may be
called for.

</details>


### [234] [Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents](https://arxiv.org/abs/2507.01862)
*Sanjay Krishna Anbalagan,Xinrui Nie,Umesh Mohan,Vijay Kumar Kanamarlapudi,Anughna Kommalapati,Xiaodan Zhao*

Main category: cs.HC

TL;DR: 论文提出在大语言模型提示中将GUI隐喻建模为明确任务，应用于酒店预订和客户管理场景，提升了多轮任务连贯性、用户满意度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统图形用户界面能明确跟踪用户意图，而对话式代理依赖微妙语言线索，易导致混淆和上下文管理不完整，需要改进。

Method: 在大语言模型提示中将GUI启发的确认和上下文切换隐喻建模为明确任务，将用户确认、重置操作和思维链推理捕获为结构化会话数据。

Result: 在酒店预订和客户管理场景中，多轮任务连贯性、用户满意度和效率得到提升。

Conclusion: 该方法能保持清晰，减少用户困惑，使特定领域聊天机器人交互与后端逻辑保持一致。

Abstract: Domain specific chatbot applications often involve multi step interactions,
such as refining search filters, selecting multiple items, or performing
comparisons. Traditional graphical user interfaces (GUIs) handle these
workflows by providing explicit "Submit" (commit data) and "Reset" (discard
data) actions, allowing back-end systems to track user intent unambiguously. In
contrast, conversational agents rely on subtle language cues, which can lead to
confusion and incomplete context management. This paper proposes modeling these
GUI inspired metaphors acknowledgment (submit like) and context switching
(reset-like) as explicit tasks within large language model (LLM) prompts. By
capturing user acknowledgment, reset actions, and chain of thought (CoT)
reasoning as structured session data, we preserve clarity, reduce user
confusion, and align domain-specific chatbot interactions with back-end logic.
We demonstrate our approach in hotel booking and customer management scenarios,
highlighting improvements in multi-turn task coherence, user satisfaction, and
efficiency.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [235] [Cross-Attention Message-Passing Transformers for Code-Agnostic Decoding in 6G Networks](https://arxiv.org/abs/2507.01038)
*Seong-Joon Park,Hee-Youl Kwak,Sang-Hyo Kim,Yongjune Kim,Jong-Seon No*

Main category: cs.IT

TL;DR: 提出基于Transformer架构的AI原生统一且与码无关的解码基础模型，包括CrossMPT、FCrossMPT和CrossED，性能优、泛化性强，为6G信道编码提供方向。


<details>
  <summary>Details</summary>
Motivation: 6G网络信道编码需求挑战传统特定码解码器，缺乏灵活性和可扩展性，需新解码方案。

Method: 提出CrossMPT，用两个掩码交叉注意力块迭代更新幅度和校验子向量；在此基础上开发FCrossMPT使架构对码长、码率和码类不变；提出由多个不同校验矩阵的CrossMPT块组成的CrossED。

Result: CrossMPT在单神经解码器中达到了最先进的解码性能；FCrossMPT和CrossED能解码多种码，且CrossED有强泛化性。

Conclusion: 所提出的AI原生与码无关解码器具有灵活性、可扩展性和高性能，为6G网络信道编码提供了有前景的方向。

Abstract: Channel coding for 6G networks is expected to support a wide range of
requirements arising from heterogeneous communication scenarios. These demands
challenge traditional code-specific decoders, which lack the flexibility and
scalability required for next-generation systems. To tackle this problem, we
propose an AI-native foundation model for unified and code-agnostic decoding
based on the transformer architecture. We first introduce a cross-attention
message-passing transformer (CrossMPT). CrossMPT employs two masked
cross-attention blocks that iteratively update two distinct input
representations-magnitude and syndrome vectors-allowing the model to
effectively learn the decoding problem. Notably, our CrossMPT has achieved
state-of-the-art decoding performance among single neural decoders. Building on
this, we develop foundation CrossMPT (FCrossMPT) by making the architecture
invariant to code length, rate, and class, allowing a single trained model to
decode a broad range of codes without retraining. To further enhance decoding
performance, particularly for short blocklength codes, we propose CrossMPT
ensemble decoder (CrossED), an ensemble decoder composed of multiple parallel
CrossMPT blocks employing different parity-check matrices. This architecture
can also serve as a foundation model, showing strong generalization across
diverse code types. Overall, the proposed AI-native code-agnostic decoder
offers flexibility, scalability, and high performance, presenting a promising
direction to channel coding for 6G networks.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [236] [Consumption Stimulus with Digital Coupons](https://arxiv.org/abs/2507.01365)
*Ying Chen,Mingyi Li,Jiaming Mao,Jingyi Zhou*

Main category: econ.GN

TL;DR: 研究数字消费券刺激消费，分析中国项目得出五点发现并提出改进方案


<details>
  <summary>Details</summary>
Motivation: 研究数字消费券这一限时、有最低消费要求的补贴方式对消费的刺激作用

Method: 分析中国一个大规模数字消费券项目

Result: 1. 每1元补贴带动3.4元消费；2. 消费反应受供需因素影响；3. 基础消费超门槛的消费者消费增加最多；4. 高反应消费者倾向大商家；5. 精准投放可使刺激效果翻倍。

Conclusion: 结合精准投放和支持小企业的混合设计能提高项目效率和公平性

Abstract: We study consumption stimulus with digital coupons, which provide
time-limited subsidies contingent on minimum spending. We analyze a large-scale
program in China and present five main findings: (1) the program generates
large short-term effects, with each $\yen$1 of government subsidy inducing
$\yen$3.4 in consumer spending; (2) consumption responses vary substantially,
driven by both demand-side factors (e.g., wealth) and supply-side factors
(e.g., local consumption amenities); (3) The largest spending increases occur
among consumers whose baseline spending already exceeds coupon thresholds and
for whom coupon subsidies should be equivalent to cash, suggesting behavioral
motivations; (4) high-response consumers disproportionately direct their
spending toward large businesses, leading to a regressive allocation of
stimulus benefits; and (5) targeting the most responsive consumers can double
total stimulus effects. A hybrid design combining targeted distribution with
direct support to small businesses improves both the efficiency and equity of
the program.

</details>


### [237] [Pay Clauses in Public Procurement: The Wage Impact of Collective Bargaining Compliance Laws in Germany](https://arxiv.org/abs/2507.01458)
*Vinzenz Pyka*

Main category: econ.GN

TL;DR: 研究利用德国行政数据，分析集体谈判合规法的工资效应，发现实施该法的东德联邦州五年内工资涨幅更高。


<details>
  <summary>Details</summary>
Motivation: 探究集体谈判合规法对工资的影响，尤其是在集体谈判覆盖率下降的情况下确保集体商定工资的潜力。

Method: 利用德国联邦州法律实施时间差异，聚焦公共交通部门，采用事件研究设计估计动态处理效应。

Result: 法律实施五年内，有该法律的联邦州（仅东德）工资涨幅比没有的州平均高2.9% - 4.6%。

Conclusion: 该法在集体谈判覆盖率下降时，有保障集体商定工资的潜力。

Abstract: Using administrative data from Germany, this study provides first evidence on
the wage effects of collective bargaining compliance laws. These laws require
establishments receiving public contracts to pay wages set by a representative
collective agreement, even if they are not formally bound by one. Leveraging
variation in the timing of law implementation across federal states, and
focusing on the public transport sector -- where regulation is uniform and
demand is driven solely by state-level needs -- I estimate dynamic treatment
effects using event-study designs. The results indicate that within five years
of the law's implementation, wage increases were on average 2.9\% to 4.6\%
higher in federal states with such a law compared to those without one -- but
only in East Germany. These findings highlight the potential for securing
collectively agreed wages in times of declining collective bargaining coverage.

</details>


### [238] [Epistemic Scarcity: The Economics of Unresolvable Unknowns](https://arxiv.org/abs/2507.01483)
*Craig S Wright*

Main category: econ.GN

TL;DR: 对人工智能和算法治理进行人类行为学分析，挑战机器系统维持经济和认知秩序能力的假设，批判主流伦理AI框架，探讨信息丰富与真相辨别，强调奥地利传统的重要性。


<details>
  <summary>Details</summary>
Motivation: 挑战机器系统维持经济和认知秩序能力的传统假设，批判主流伦理AI框架，探讨AI对人类自主性、制度演变和理性选择的影响。

Method: 借鉴米塞斯的先验推理和奥地利学派的企业家精神理论，以人类行为学视角分析；运用认知稀缺概念探讨信息与真相关系。

Result: 指出AI系统无法执行经济协调的核心功能，主流伦理AI框架存在问题，信息丰富会降低真相辨别能力。

Conclusion: 关于AI的辩论关乎人类自主性、制度演变和理性选择的未来，奥地利传统是应对计算社会控制的唯一连贯选择。

Abstract: This paper presents a praxeological analysis of artificial intelligence and
algorithmic governance, challenging assumptions about the capacity of machine
systems to sustain economic and epistemic order. Drawing on Misesian a priori
reasoning and Austrian theories of entrepreneurship, we argue that AI systems
are incapable of performing the core functions of economic coordination:
interpreting ends, discovering means, and communicating subjective value
through prices. Where neoclassical and behavioural models treat decisions as
optimisation under constraint, we frame them as purposive actions under
uncertainty.
  We critique dominant ethical AI frameworks such as Fairness, Accountability,
and Transparency (FAT) as extensions of constructivist rationalism, which
conflict with a liberal order grounded in voluntary action and property rights.
Attempts to encode moral reasoning in algorithms reflect a misunderstanding of
ethics and economics. However complex, AI systems cannot originate norms,
interpret institutions, or bear responsibility. They remain opaque, misaligned,
and inert.
  Using the concept of epistemic scarcity, we explore how information abundance
degrades truth discernment, enabling both entrepreneurial insight and soft
totalitarianism. Our analysis ends with a civilisational claim: the debate over
AI concerns the future of human autonomy, institutional evolution, and reasoned
choice. The Austrian tradition, focused on action, subjectivity, and
spontaneous order, offers the only coherent alternative to rising computational
social control.

</details>


### [239] [Using Machine Learning to Compute Constrained Optimal Carbon Tax Rules](https://arxiv.org/abs/2507.01704)
*Felix Kübler,Simon Scheidegger,Oliver Surbek*

Main category: econ.GN

TL;DR: 本文开发计算框架推导随机OLG模型中帕累托改进和约束最优碳税规则，应用该方法分析表明简单线性税效果好，框架可用于宏观政策设计和异质主体问题。


<details>
  <summary>Details</summary>
Motivation: 在含气候变化的随机OLG模型中推导帕累托改进和约束最优碳税规则。

Method: 整合深度均衡网络用于快速政策评估，结合高斯过程代理建模与贝叶斯主动学习，应用于12期OLG模型。

Result: 简单的对累计排放的线性税可带来0.42%的总体福利增益，增加税收复杂性仅提升至0.45%。

Conclusion: 所提方法为复杂随机环境下宏观政策设计提供可扩展工具，也可用于分析各类异质主体问题中的福利改进政策。

Abstract: We develop a computational framework for deriving Pareto-improving and
constrained optimal carbon tax rules in a stochastic overlapping generations
(OLG) model with climate change. By integrating Deep Equilibrium Networks for
fast policy evaluation and Gaussian process surrogate modeling with Bayesian
active learning, the framework systematically locates optimal carbon tax
schedules for heterogeneous agents exposed to climate risk. We apply our method
to a 12-period OLG model in which exogenous shocks affect the carbon intensity
of energy production, as well as the damage function. Constrained optimal
carbon taxes consist of tax rates that are simple functions of observables and
revenue-sharing rules that guarantee that the introduction of the taxes is
Pareto improving. This reveals that a straightforward policy is highly
effective: a Pareto-improving linear tax on cumulative emissions alone yields a
0.42% aggregate welfare gain in consumption-equivalent terms while adding
further complexity to the tax provides only a marginal increase to 0.45%. The
application demonstrates that the proposed approach produces scalable tools for
macro-policy design in complex stochastic settings. Beyond climate economics,
the framework offers a template for systematically analyzing welfare-improving
policies in various heterogeneous-agent problems.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [240] [GPU-based complete search for nonlinear minimization subject to bounds](https://arxiv.org/abs/2507.01770)
*Guanglu Zhang,Qihang Shan,Jonathan Cagan*

Main category: math.NA

TL;DR: 本文提出基于GPU的完全搜索方法求解带变量边界的非线性函数全局最小值，经测试可在合理时间内处理高维函数。


<details>
  <summary>Details</summary>
Motivation: 为求解带变量简单边界的非线性函数全局最小值，利用GPU计算能力和架构优势。

Method: 结合区间分析，迭代排除无全局最小值区域；采用基于GPU的单程序单数据并行编程风格，融入变量循环技术。

Result: 对10个可扩展维度的多峰基准测试函数进行验证，能在合理时间内用单GPU求解最高10000维函数的全局最小值。

Conclusion: 该方法设计和基于GPU架构的实现有效，远超文献中报道的结果。

Abstract: This paper introduces a GPU-based complete search method to enclose the
global minimum of a nonlinear function subject to simple bounds on the
variables. Using interval analysis, coupled with the computational power and
architecture of GPU, the method iteratively rules out the regions in the search
domain where the global minimum cannot exist and leaves a finite set of regions
where the global minimum must exist. For effectiveness, because of the rigor of
interval analysis, the method is guaranteed to enclose the global minimum of
the nonlinear function even in the presence of rounding errors. For efficiency,
the method employs a novel GPU-based single program, single data parallel
programming style to circumvent major GPU performance bottlenecks, and a
variable cycling technique is also integrated into the method to reduce
computational cost when minimizing large-scale nonlinear functions. The method
is validated by minimizing 10 multimodal benchmark test functions with scalable
dimensions, including the well-known Ackley function, Griewank function, Levy
function, and Rastrigin function. These benchmark test functions represent
grand challenges of global optimization, and enclosing the guaranteed global
minimum of these benchmark test functions with more than 80 dimensions has not
been reported in the literature. Our method completely searches the feasible
domain and successfully encloses the guaranteed global minimum of these 10
benchmark test functions with up to 10,000 dimensions using only one GPU in a
reasonable computation time, far exceeding the reported results in the
literature due to the unique method design and implementation based on GPU
architecture.

</details>


### [241] [Consistency of Learned Sparse Grid Quadrature Rules using NeuralODEs](https://arxiv.org/abs/2507.01533)
*Hanno Gottschalk,Emil Partow,Tobias J. Riedlinger*

Main category: math.NA

TL;DR: 本文证明高维分布数值积分中稀疏网格求积的一致性，通过学习传输映射归一化分布，用Clenshaw - Curtis稀疏网格求积数值积分，并分解误差，证明误差可控。


<details>
  <summary>Details</summary>
Motivation: 证明高维分布数值积分中稀疏网格求积的一致性。

Method: 首先利用神经常微分方程的统计学习理论学习传输映射，将分布归一化为单位立方体上的噪声分布；其次用Clenshaw - Curtis稀疏网格求积对生成映射与感兴趣量的组合进行数值积分，并分解总数值误差。

Result: 在经验风险最小化框架下证明所有误差项在PAC学习意义下可控制，随着数据集大小增加和网络容量自适应增大，数值积分以高概率近似理论值至任意小误差。

Conclusion: 稀疏网格求积用于高维分布数值积分具有一致性，误差可有效控制。

Abstract: This paper provides a proof of the consistency of sparse grid quadrature for
numerical integration of high dimensional distributions. In a first step, a
transport map is learned that normalizes the distribution to a noise
distribution on the unit cube. This step is built on the statistical learning
theory of neural ordinary differential equations, which has been established
recently. Secondly, the composition of the generative map with the quantity of
interest is integrated numerically using the Clenshaw-Curtis sparse grid
quadrature. A decomposition of the total numerical error in quadrature error
and statistical error is provided. As main result it is proven in the framework
of empirical risk minimization that all error terms can be controlled in the
sense of PAC (probably approximately correct) learning and with high
probability the numerical integral approximates the theoretical value up to an
arbitrary small error in the limit where the data set size is growing and the
network capacity is increased adaptively.

</details>


### [242] [Neural Entropy-stable conservative flux form neural networks for learning hyperbolic conservation laws](https://arxiv.org/abs/2507.01795)
*Lizuo Liu,Lu Zhang,Anne Gelb*

Main category: math.NA

TL;DR: 提出NESCFN从解轨迹直接学习双曲守恒律及其熵函数，不依赖预定义离散化，数值结果显示其有良好稳定性和守恒性。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络架构大多依赖控制方程先验知识或固定离散化，本文旨在去除这种依赖，实现全数据驱动。

Method: 将熵稳定设计原则嵌入学习过程，联合学习数值通量函数和相应熵。

Result: 方法在长时间范围内实现稳定性和守恒性，准确捕捉激波传播速度。

Conclusion: NESCFN可在全数据驱动下发现物理上一致的动力学，保证双曲守恒律系统的长期稳定性和保真度。

Abstract: We propose a neural entropy-stable conservative flux form neural network
(NESCFN) for learning hyperbolic conservation laws and their associated entropy
functions directly from solution trajectories, without requiring any predefined
numerical discretization. While recent neural network architectures have
successfully integrated classical numerical principles into learned models,
most rely on prior knowledge of the governing equations or assume a fixed
discretization. Our approach removes this dependency by embedding
entropy-stable design principles into the learning process itself, enabling the
discovery of physically consistent dynamics in a fully data-driven setting. By
jointly learning both the numerical flux function and a corresponding entropy,
the proposed method ensures conservation and entropy dissipation, critical for
long-term stability and fidelity in the system of hyperbolic conservation laws.
Numerical results demonstrate that the method achieves stability and
conservation over extended time horizons and accurately captures shock
propagation speeds, even without oracle access to future-time solution profiles
in the training data.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [243] [A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory](https://arxiv.org/abs/2507.01110)
*Felix Windisch,Lukas Radl,Thomas Köhler,Michael Steiner,Dieter Schmalstieg,Markus Steinberger*

Main category: cs.GR

TL;DR: 提出A LoD of Gaussians框架，可在单块消费级GPU上训练和渲染超大规模高斯场景，无需分区。


<details>
  <summary>Details</summary>
Motivation: 现有高斯拼接技术扩展到大型环境时依赖场景分区，存在边界伪影、训练复杂和不适合非结构化场景等问题，且受GPU内存限制。

Method: 将全场景存储在CPU内存，直接训练多细节层次（LoD）表示，动态流式传输相关高斯，结合高斯层次结构和顺序点树实现高效LoD选择，利用轻量级缓存和视图调度系统支持实时流式传输和渲染。

Result: 实现复杂场景从高空鸟瞰到地面细节的无缝多尺度重建和交互式可视化。

Conclusion: A LoD of Gaussians框架可有效解决大规模高斯场景训练和渲染问题。

Abstract: Gaussian Splatting has emerged as a high-performance technique for novel view
synthesis, enabling real-time rendering and high-quality reconstruction of
small scenes. However, scaling to larger environments has so far relied on
partitioning the scene into chunks -- a strategy that introduces artifacts at
chunk boundaries, complicates training across varying scales, and is poorly
suited to unstructured scenarios such as city-scale flyovers combined with
street-level views. Moreover, rendering remains fundamentally limited by GPU
memory, as all visible chunks must reside in VRAM simultaneously. We introduce
A LoD of Gaussians, a framework for training and rendering ultra-large-scale
Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our
method stores the full scene out-of-core (e.g., in CPU memory) and trains a
Level-of-Detail (LoD) representation directly, dynamically streaming only the
relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with
Sequential Point Trees enables efficient, view-dependent LoD selection, while a
lightweight caching and view scheduling system exploits temporal coherence to
support real-time streaming and rendering. Together, these innovations enable
seamless multi-scale reconstruction and interactive visualization of complex
scenes -- from broad aerial views to fine-grained ground-level details.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [244] [Can AI be Consentful?](https://arxiv.org/abs/2507.01051)
*Giada Pistilli,Bruna Trevelin*

Main category: cs.CY

TL;DR: 传统基于同意的法律和伦理框架在应对生成式AI系统问题上不足，存在'同意差距'，需改进伦理和法律方法。


<details>
  <summary>Details</summary>
Motivation: 探讨传统同意概念在处理AI生成内容衍生问题上的不足。

Method: 通过法律和伦理分析。

Result: 指出个体无法有效同意数据潜在输出及使用，确定范围、时间和自主性陷阱三个挑战，形成'同意差距'。

Conclusion: 当前法律框架不足以应对新兴挑战，需要发展伦理和法律的同意方法。

Abstract: The evolution of generative AI systems exposes the challenges of traditional
legal and ethical frameworks built around consent. This chapter examines how
the conventional notion of consent, while fundamental to data protection and
privacy rights, proves insufficient in addressing the implications of
AI-generated content derived from personal data. Through legal and ethical
analysis, we show that while individuals can consent to the initial use of
their data for AI training, they cannot meaningfully consent to the numerous
potential outputs their data might enable or the extent to which the output is
used or distributed. We identify three fundamental challenges: the scope
problem, the temporality problem, and the autonomy trap, which collectively
create what we term a ''consent gap'' in AI systems and their surrounding
ecosystem. We argue that current legal frameworks inadequately address these
emerging challenges, particularly regarding individual autonomy, identity
rights, and social responsibility, especially in cases where AI-generated
content creates new forms of personal representation beyond the scope of the
original consent. By examining how these consent limitations intersect with
broader principles of responsible AI (including fairness, transparency,
accountability, and autonomy) we demonstrate the need to evolve ethical and
legal approaches to consent.

</details>


### [245] [Epitome: Pioneering an Experimental Platform for AI-Social Science Integration](https://arxiv.org/abs/2507.01061)
*Jingjing Qu,Kejia Hu,Jun Zhu,Wenhao Li,Teng Wang,Zhiyun Chen,Yulei Ye,Chaochao Lu,Aimin Zhou,Xiangfeng Wang,James Evan*

Main category: cs.CY

TL;DR: 介绍世界首个将人工智能与社会科学深度融合的开放实验平台Epitome，阐述其特点、功能，通过复制实验展示其能力，强调对跨学科研究的价值。


<details>
  <summary>Details</summary>
Motivation: 理解人机交互及其社会影响，推动人工智能与社会科学融合研究。

Method: 构建基于多学科理论的支持系统，通过七个核心模块提供一站式实验方案，嵌入社会科学实验逻辑到多级人机交互环境。

Result: 复制三个社会科学实验，展示Epitome能简化复杂实验设计，产生可靠结果，适合发表于顶级期刊。

Conclusion: Epitome可提高人机交互效率和质量，为人工智能社会影响研究提供有力工具，有助于跨学科研究和政策制定。

Abstract: The integration of Large Language Models (LLMs) into social science
experiments represents a transformative approach to understanding human-AI
interactions and their societal impacts. We introduce Epitome, the world's
first open experimental platform dedicated to the deep integration of
artificial intelligence and social science. Rooted in theoretical foundations
from management, communication studies, sociology, psychology, and ethics,
Epitome focuses on the interactive impacts of AI on individuals, organizations,
and society during its real-world deployment. It constructs a theoretical
support system through cross-disciplinary experiments. The platform offers a
one-stop comprehensive experimental solution spanning "foundation
models-complex application development-user feedback" through seven core
modules, while embedding the classical "control-comparison-comparative causal
logic" of social science experiments into multilevel human-computer interaction
environments, including dialogues, group chats, and multi-agent virtual
scenarios. With its canvas-style, user-friendly interface, Epitome enables
researchers to easily design and run complex experimental scenarios,
facilitating systematic investigations into the social impacts of AI and
exploration of integrated solutions.To demonstrate its capabilities, we
replicated three seminal social science experiments involving LLMs, showcasing
Epitome's potential to streamline complex experimental designs and produce
robust results, suitable for publishing in the top selective journals. Our
findings highlight the platform's utility in enhancing the efficiency and
quality of human-AI interactions, providing valuable insights into the societal
implications of AI technologies. Epitome thus offers a powerful tool for
advancing interdisciplinary research at the intersection of AI and social
science, with potential applications in policy-making, ...

</details>


### [246] [Quantifying Student Success with Generative AI: A Monte Carlo Simulation Informed by Systematic Review](https://arxiv.org/abs/2507.01062)
*Seyma Yaman Kayadibi*

Main category: cs.CY

TL;DR: 本文采用混合方法探讨学生对高等教育中生成式AI使用的看法，发现可用性和实用性态度因素比情感或信任因素更能预测学习成果。


<details>
  <summary>Details</summary>
Motivation: 生成式AI技术发展引发对其在高等教育应用的关注，想了解学生看法、使用情况及对学习成果的影响。

Method: 采用系统文献综述和基于模拟的建模混合方法，从Scopus数据库选文章，进行主题分类，用蒙特卡罗模拟进行逆方差加权。

Result: 模拟得出“成功分数”，显示可用性和现实实用性态度因素比情感或信任因素更能预测积极学习成果。

Conclusion: 跨学科视角将主题结果与预测建模联系起来，呼应大学中关于正确使用生成式AI工具的争议。

Abstract: The exponential development of generative artificial intelligence (GenAI)
technologies like ChatGPT has raised increasing curiosity about their use in
higher education, specifically with respect to how students view them, make use
of them, and the implications for learning outcomes. This paper employs a
hybrid methodological approach involving a systematic literature review and
simulation-based modeling to explore student perceptions of GenAI use in the
context of higher education. A total of nineteen empirical articles from 2023
through 2025 were selected from the PRISMA-based search targeting the Scopus
database. Synthesis of emerging patterns from the literature was achieved by
thematic categorization. Six of these had enough quantitative information,
i.e., item-level means and standard deviations, to permit probabilistic
modeling. One dataset, from the resulting subset, was itself selected as a
representative case with which to illustrate inverse-variance weighting by
Monte Carlo simulation, by virtue of its well-designed Likert scale format and
thematic alignment with the use of computing systems by the researcher.
  The simulation provided a composite "Success Score" forecasting the strength
of the relationship between student perceptions and learning achievements.
Findings reveal that attitude factors concerned with usability and real-world
usefulness are significantly better predictors of positive learning achievement
than affective or trust-based factors. Such an interdisciplinary perspective
provides a unique means of linking thematic results with predictive modelling,
resonating with longstanding controversies about the proper use of GenAI tools
within the university.

</details>


### [247] [Penalizing Transparency? How AI Disclosure and Author Demographics Shape Human and AI Judgments About Writing](https://arxiv.org/abs/2507.01418)
*Inyoung Cheong,Alicia Guo,Mina Lee,Zhehui Liao,Kowe Kadoma,Dongyoung Go,Joseph Chee Chang,Peter Henderson,Mor Naaman,Amy X. Zhang*

Main category: cs.CY

TL;DR: 研究AI披露声明对写作质量感知的影响及是否因作者种族和性别而异，发现人类和LLM评估者都会惩罚披露AI使用，LLM评估者有特定人口统计学交互效应。


<details>
  <summary>Details</summary>
Motivation: 随着AI融入人类写作，对AI辅助透明度的需求增加，但不同身份群体坦诚使用AI成本不同，需研究AI披露声明对写作质量感知的影响及是否因作者种族和性别而异。

Method: 通过大规模对照实验，让人类评估者（n = 1,970）和LLM评估者（n = 2,520）评估一篇人类撰写的新闻文章，系统改变披露声明和作者人口统计信息。

Result: 人类和LLM评估者都会惩罚披露AI使用，只有LLM评估者有人口统计学交互效应，无披露时青睐女性或黑人作者文章，披露后优势消失。

Conclusion: 研究揭示了AI披露与作者身份的复杂关系，凸显了机器和人类评估模式的差异。

Abstract: As AI integrates in various types of human writing, calls for transparency
around AI assistance are growing. However, if transparency operates on uneven
ground and certain identity groups bear a heavier cost for being honest, then
the burden of openness becomes asymmetrical. This study investigates how AI
disclosure statement affects perceptions of writing quality, and whether these
effects vary by the author's race and gender. Through a large-scale controlled
experiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated
a single human-written news article while disclosure statements and author
demographics were systematically varied. This approach reflects how both human
and algorithmic decisions now influence access to opportunities (e.g., hiring,
promotion) and social recognition (e.g., content recommendation algorithms). We
find that both human and LLM raters consistently penalize disclosed AI use.
However, only LLM raters exhibit demographic interaction effects: they favor
articles attributed to women or Black authors when no disclosure is present.
But these advantages disappear when AI assistance is revealed. These findings
illuminate the complex relationships between AI disclosure and author identity,
highlighting disparities between machine and human evaluation patterns.

</details>


### [248] [AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions](https://arxiv.org/abs/2507.01547)
*Ubada El Joulani,Tatiana Kalganova,Stergios-Aristoteles Mitoulis,Sotirios Argyroudis*

Main category: cs.CY

TL;DR: 本文探讨新兴数字技术（AI）如何增强交通基础设施的损伤评估和监测，指出应用AI模型到SAR数据进行桥梁综合损伤评估的研究稀缺，并为AI驱动解决方案奠定基础。


<details>
  <summary>Details</summary>
Motivation: 关键基础设施面临老化、气候变化和混合威胁等风险，需要新兴数字技术增强其损伤评估和监测。

Method: 进行系统的文献综述，研究现有AI模型和数据集，重点关注桥梁损伤检测，探讨SAR数据与AI模型的集成。

Result: 发现应用AI模型到SAR数据进行桥梁综合损伤评估的研究稀缺。

Conclusion: 确定研究差距，为AI驱动的关键交通基础设施评估和监测解决方案提供基础。

Abstract: Critical infrastructure, such as transport networks, underpins economic
growth by enabling mobility and trade. However, ageing assets, climate change
impacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging
from natural disasters to cyber attacks and conflicts pose growing risks to
their resilience and functionality. This review paper explores how emerging
digital technologies, specifically Artificial Intelligence (AI), can enhance
damage assessment and monitoring of transport infrastructure. A systematic
literature review examines existing AI models and datasets for assessing damage
in roads, bridges, and other critical infrastructure impacted by natural
disasters. Special focus is given to the unique challenges and opportunities
associated with bridge damage detection due to their structural complexity and
critical role in connectivity. The integration of SAR (Synthetic Aperture
Radar) data with AI models is also discussed, with the review revealing a
critical research gap: a scarcity of studies applying AI models to SAR data for
comprehensive bridge damage assessment. Therefore, this review aims to identify
the research gaps and provide foundations for AI-driven solutions for assessing
and monitoring critical transport infrastructures.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [249] [SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars](https://arxiv.org/abs/2507.01939)
*Xiaosheng Zhao,Yang Huang,Guirong Xue,Xiao Kong,Jifeng Liu,Xiaoyu Tang,Timothy C. Beers,Yuan-Sen Ting,A-Li Luo*

Main category: astro-ph.IM

TL;DR: 提出SpecCLIP基础模型框架用于恒星光谱分析，经预训练、对比对齐等步骤，在微调后提升多任务适应性、精度，可用于异常检测，显示出对比训练基础模型在恒星光谱学的潜力。


<details>
  <summary>Details</summary>
Motivation: 受大语言模型成功启发，将相关方法拓展到恒星光谱分析，学习鲁棒且信息丰富的嵌入以支持下游应用。

Method: 在LAMOST低分辨率和Gaia XP两种光谱类型上预训练，用CLIP框架进行对比对齐，辅以保留光谱信息的解码器，最大化嵌入和输入光谱互信息。

Result: 形成跨光谱框架，微调后提升任务适应性，提高参数估计精度，具备相似性搜索和跨光谱预测能力。

Conclusion: 对比训练且配有光谱感知解码器的基础模型可推动高精度恒星光谱学发展。

Abstract: In recent years, large language models (LLMs) have transformed natural
language understanding through vast datasets and large-scale parameterization.
Inspired by this success, we present SpecCLIP, a foundation model framework
that extends LLM-inspired methodologies to stellar spectral analysis. Stellar
spectra, akin to structured language, encode rich physical and chemical
information about stars. By training foundation models on large-scale spectral
datasets, our goal is to learn robust and informative embeddings that support
diverse downstream applications. As a proof of concept, SpecCLIP involves
pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed
by contrastive alignment using the CLIP (Contrastive Language-Image
Pre-training) framework, adapted to associate spectra from different
instruments. This alignment is complemented by auxiliary decoders that preserve
spectrum-specific information and enable translation (prediction) between
spectral types, with the former achieved by maximizing mutual information
between embeddings and input spectra. The result is a cross-spectrum framework
enabling intrinsic calibration and flexible applications across instruments. We
demonstrate that fine-tuning these models on moderate-sized labeled datasets
improves adaptability to tasks such as stellar-parameter estimation and
chemical-abundance determination. SpecCLIP also enhances the accuracy and
precision of parameter estimates benchmarked against external survey data.
Additionally, its similarity search and cross-spectrum prediction capabilities
offer potential for anomaly detection. Our results suggest that contrastively
trained foundation models enriched with spectrum-aware decoders can advance
precision stellar spectroscopy.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [250] [Systemic Constraints of Undecidability](https://arxiv.org/abs/2507.01036)
*Seth Bulin*

Main category: cs.FL

TL;DR: 本文提出系统不可判定性理论，将不可计算性视为系统结构属性，证明相关闭包原理，挑战绕过计算限制观点，拓展经典结果。


<details>
  <summary>Details</summary>
Motivation: 重新审视不可计算性，从系统结构角度研究，以理解其对自然和人工系统的影响。

Method: 定义因果嵌入概念，证明闭包原理。

Result: 证明参与不可判定系统计算的子系统继承其不可判定性，指出不可判定性是对预测、建模和认知的普遍约束。

Conclusion: 框架挑战通过架构创新绕过计算限制的观点，拓展经典逻辑轨迹，提供计算性拓扑和科学知识边界新视角。

Abstract: This paper presents a theory of systemic undecidability, reframing
incomputability as a structural property of systems rather than a localized
feature of specific functions or problems. We define a notion of causal
embedding and prove a closure principle: any subsystem that participates
functionally in the computation of an undecidable system inherits its
undecidability. This result positions undecidability as a pervasive constraint
on prediction, modeling, and epistemic access in both natural and artificial
systems. Our framework disarms oracle mimicry and challenges the view that
computational limits can be circumvented through architectural innovation. By
generalizing classical results into a dynamic systems context, this work
augments the logical trajectory of G\"odel, Turing, and Chaitin, offering a new
perspective of the topology of computability and its interrelation to the
boundaries of scientific knowledge.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [251] [A Systematic Review of Security Vulnerabilities in Smart Home Devices and Mitigation Techniques](https://arxiv.org/abs/2507.01018)
*Mohammed K. Alzaylaee*

Main category: cs.CR

TL;DR: 研究智能家庭生态系统安全威胁，提出多种安全策略，指出其效果与挑战，强调需改进加密技术等。


<details>
  <summary>Details</summary>
Motivation: 应对智能家居集成物联网设备面临的网络安全风险挑战。

Method: 对安全威胁分类研究，用ANOVA、卡方检验和蒙特卡罗模拟验证策略有效性。

Result: 后量子加密与AI异常检测、区块链认证与零信任结构有效但有挑战，策略缺乏足够可扩展性。

Conclusion: 需改进加密技术，结合AI威胁检测和自适应安全模型，平衡性能、效率和实时适用性。

Abstract: Smart homes that integrate Internet of Things (IoT) devices face increasing
cybersecurity risks, posing significant challenges to these environments. The
study explores security threats in smart homes ecosystems, categorizing them
into vulnerabilities at the network layer, device level, and those from
cloud-based and AI-driven systems. Research findings indicate that post-quantum
encryption, coupled with AI-driven anomaly detection, is highly effective in
enhancing security; however, computational resource demands present significant
challenges. Blockchain authentication together with zero-trust structures
builds security resilience, although they need changes to existing
infrastructure. The specific security strategies show their effectiveness
through ANOVA, Chi-square tests, and Monte Carlo simulations yet lack
sufficient scalability according to the results. The research demonstrates the
requirement for improvement in cryptographic techniques, alongside AI-enhanced
threat detection and adaptive security models which must achieve a balance
between performance and efficiency and real-time applicability within smart
home ecosystems.

</details>


### [252] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2507.01020)
*Aashray Reddy,Andrew Zagula,Nicholas Saban*

Main category: cs.CR

TL;DR: 提出AutoAdv框架自动化对抗性提示生成，评估大语言模型安全机制漏洞，实验显示当前模型易受攻击，需更强防御策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受越狱攻击，需评估和暴露其安全机制漏洞。

Method: 利用参数化攻击者大语言模型，通过策略性重写、系统提示和超参数配置生成恶意提示，采用动态多轮攻击方法分析失败尝试并迭代生成后续提示，用StrongREJECT框架评估攻击成功率。

Result: 对ChatGPT、Llama和DeepSeek等模型进行评估，自动攻击有害内容生成的越狱成功率达86%。

Conclusion: 当前安全机制易受复杂多轮攻击，急需更强大的防御策略。

Abstract: Large Language Models (LLMs) continue to exhibit vulnerabilities to
jailbreaking attacks: carefully crafted malicious inputs intended to circumvent
safety guardrails and elicit harmful responses. As such, we present AutoAdv, a
novel framework that automates adversarial prompt generation to systematically
evaluate and expose vulnerabilities in LLM safety mechanisms. Our approach
leverages a parametric attacker LLM to produce semantically disguised malicious
prompts through strategic rewriting techniques, specialized system prompts, and
optimized hyperparameter configurations. The primary contribution of our work
is a dynamic, multi-turn attack methodology that analyzes failed jailbreak
attempts and iteratively generates refined follow-up prompts, leveraging
techniques such as roleplaying, misdirection, and contextual manipulation. We
quantitatively evaluate attack success rate (ASR) using the StrongREJECT
(arXiv:2402.10260 [cs.CL]) framework across sequential interaction turns.
Through extensive empirical evaluation of state-of-the-art models--including
ChatGPT, Llama, and DeepSeek--we reveal significant vulnerabilities, with our
automated attacks achieving jailbreak success rates of up to 86% for harmful
content generation. Our findings reveal that current safety mechanisms remain
susceptible to sophisticated multi-turn attacks, emphasizing the urgent need
for more robust defense strategies.

</details>


### [253] [Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems](https://arxiv.org/abs/2507.01808)
*Xiaoyu Ji,Jessica Shorland,Joshua Shank,Pascal Delpe-Brice,Latanya Sweeney,Jan Allebach,Ali Shakouri*

Main category: cs.CR

TL;DR: 本文介绍了一个隐私保护平台，使制造商能安全与研究人员共享数据，以开发创新工具解决实际问题，并以食品晶体大规模制造中的质量控制问题为例展示全过程。


<details>
  <summary>Details</summary>
Motivation: 中小型制造商因竞争和隐私顾虑不愿共享专有数据，需隐私保护平台让制造商与研究人员合作开发创新数据工具。

Method: 构建隐私保护平台，通过安全方法实现数据共享；针对食品晶体质量控制问题，开发晶体分析工具和机器学习模型，并打包成基于网络的应用程序。

Result: 开发的晶体分析工具使食品晶体计数更快速准确，可自动表征晶体尺寸分布和数量，去除样本制备的自然缺陷；机器学习模型辅助计数；算法以安全的网络应用程序形式部署。

Conclusion: 成功展示了隐私保护平台的应用过程，还探索了未来方向。

Abstract: Small- and medium-sized manufacturers need innovative data tools but, because
of competition and privacy concerns, often do not want to share their
proprietary data with researchers who might be interested in helping. This
paper introduces a privacy-preserving platform by which manufacturers may
safely share their data with researchers through secure methods, so that those
researchers then create innovative tools to solve the manufacturers' real-world
problems, and then provide tools that execute solutions back onto the platform
for others to use with privacy and confidentiality guarantees. We illustrate
this problem through a particular use case which addresses an important problem
in the large-scale manufacturing of food crystals, which is that quality
control relies on image analysis tools. Previous to our research, food crystals
in the images were manually counted, which required substantial and
time-consuming human efforts, but we have developed and deployed a crystal
analysis tool which makes this process both more rapid and accurate. The tool
enables automatic characterization of the crystal size distribution and numbers
from microscope images while the natural imperfections from the sample
preparation are automatically removed; a machine learning model to count high
resolution translucent crystals and agglomeration of crystals was also
developed to aid in these efforts. The resulting algorithm was then packaged
for real-world use on the factory floor via a web-based app secured through the
originating privacy-preserving platform, allowing manufacturers to use it while
keeping their proprietary data secure. After demonstrating this full process,
future directions are also explored.

</details>


### [254] [How to Securely Shuffle? A survey about Secure Shufflers for privacy-preserving computations](https://arxiv.org/abs/2507.01487)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: 本文调查了26种实现安全洗牌功能的协议，统一安全定义，介绍依赖安全洗牌器的隐私保护技术，给出协议选择建议和未来工作方向。


<details>
  <summary>Details</summary>
Motivation: 此前研究常将安全洗牌器视为黑盒，忽略实际漏洞和性能权衡，本文旨在回答什么是好的安全洗牌器这一问题。

Method: 识别、分类和比较26种安全协议，统一现有安全定义。

Result: 对依赖安全洗牌器的隐私保护技术进行概述。

Conclusion: 给出选择合适协议的实用指南和未来工作的有前景方向。

Abstract: Ishai et al. (FOCS'06) introduced secure shuffling as an efficient building
block for private data aggregation. Recently, the field of differential privacy
has revived interest in secure shufflers by highlighting the privacy
amplification they can provide in various computations. Although several works
argue for the utility of secure shufflers, they often treat them as black
boxes; overlooking the practical vulnerabilities and performance trade-offs of
existing implementations. This leaves a central question open: what makes a
good secure shuffler?
  This survey addresses that question by identifying, categorizing, and
comparing 26 secure protocols that realize the necessary shuffling
functionality. To enable a meaningful comparison, we adapt and unify existing
security definitions into a consistent set of properties. We also present an
overview of privacy-preserving technologies that rely on secure shufflers,
offer practical guidelines for selecting appropriate protocols, and outline
promising directions for future work.

</details>


### [255] [On the Effect of Ruleset Tuning and Data Imbalance on Explainable Network Security Alert Classifications: a Case-Study on DeepCASE](https://arxiv.org/abs/2507.01571)
*Koen T. W. Teuwen,Sam Baggen,Emmanuele Zambon,Luca Allodi*

Main category: cs.CR

TL;DR: 研究标签不平衡对网络入侵警报分类的影响，发现调整检测规则可减少不平衡，传统数据处理方法有益自动化。


<details>
  <summary>Details</summary>
Motivation: 自动化安全运营中心的方法需在数据不平衡时保持鲁棒且决策可解释，评估标签不平衡对网络入侵警报分类的影响。

Method: 以最先进的自动化警报分类方法DeepCASE为用例进行研究。

Result: 标签不平衡影响DeepCASE的分类性能和分类解释的正确性。

Conclusion: 调整SOC中的检测规则可显著减少不平衡，传统改善输入数据质量的方法有益于自动化。

Abstract: Automation in Security Operations Centers (SOCs) plays a prominent role in
alert classification and incident escalation. However, automated methods must
be robust in the presence of imbalanced input data, which can negatively affect
performance. Additionally, automated methods should make explainable decisions.
In this work, we evaluate the effect of label imbalance on the classification
of network intrusion alerts. As our use-case we employ DeepCASE, the
state-of-the-art method for automated alert classification. We show that label
imbalance impacts both classification performance and correctness of the
classification explanations offered by DeepCASE. We conclude tuning the
detection rules used in SOCs can significantly reduce imbalance and may benefit
the performance and explainability offered by alert post-processing methods
such as DeepCASE. Therefore, our findings suggest that traditional methods to
improve the quality of input data can benefit automation.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [256] [Advancing Magnetic Materials Discovery -- A structure-based machine learning approach for magnetic ordering and magnetic moment prediction](https://arxiv.org/abs/2507.01913)
*Apoorv Verma,Junaid Jami,Amrita Bhattacharya*

Main category: cond-mat.mtrl-sci

TL;DR: 提出改进描述符，利用材料结构信息预测磁性，在多样数据集上表现良好，可用于高通量筛选磁性材料。


<details>
  <summary>Details</summary>
Motivation: 准确预测不同材料系统的磁性行为是长期挑战，对加速下一代磁性材料的发现和设计至关重要。

Method: 提出改进描述符，利用丰富元素向量表示和高级特征工程，构建基于LightGBM的模型。

Result: 磁性排序分类准确率达82.4%，预测每个原子磁矩的相关系数为0.93，能准确估计每个原子的形成能。

Conclusion: 该通用且计算高效的框架为高通量筛选具有特定性能的磁性材料提供了强大工具。

Abstract: Accurately predicting magnetic behavior across diverse materials systems
remains a longstanding challenge due to the complex interplay of structural and
electronic factors and is pivotal for the accelerated discovery and design of
next-generation magnetic materials. In this work, a refined descriptor is
proposed that significantly improves the prediction of two critical magnetic
properties -- magnetic ordering (Ferromagnetic vs. Ferrimagnetic) and magnetic
moment per atom -- using only the structural information of materials. Unlike
previous models limited to Mn-based or lanthanide-transition metal compounds,
the present approach generalizes across a diverse dataset of 5741 stable,
binary and ternary, ferromagnetic and ferrimagnetic compounds sourced from the
Materials Project. Leveraging an enriched elemental vector representation and
advanced feature engineering, including nonlinear terms and reduced matrix
sparsity, the LightGBM-based model achieves an accuracy of 82.4% for magnetic
ordering classification and balanced recall across FM and FiM classes,
addressing a key limitation in prior studies. The model predicts magnetic
moment per atom with a correlation coefficient of 0.93, surpassing the Hund's
matrix and orbital field matrix descriptors. Additionally, it accurately
estimates formation energy per atom, enabling assessment of both magnetic
behavior and material stability. This generalized and computationally efficient
framework offers a robust tool for high-throughput screening of magnetic
materials with tailored properties.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [257] [A bibliometric analysis on the current situation and hot trends of the impact of microplastics on soil based on CiteSpace](https://arxiv.org/abs/2507.01520)
*Yiran Zheng,Yue Quan,Su Yan,Xinting Lv,Yuguanmin Cao,Minjie Fu,Mingji Jin*

Main category: cs.DL

TL;DR: 本文收集2013 - 2024年研究，分析土壤微塑料研究现状与趋势，揭示发文趋势、作者贡献等，划分研究阶段并对未来研究提出建议。


<details>
  <summary>Details</summary>
Motivation: 全面掌握土壤微塑料的研究现状和发展趋势，因微塑料会积累、转移并影响人类健康，对其研究有助于污染控制。

Method: 从Web of Science Core Collection收集研究，利用CiteSpace和VOSviewer对微塑料环境影响文献进行关键词共现、聚类、突现词识别以及作者和机构共现分析。

Result: 每年发文量呈稳定增长趋势，2024年达956篇；少数高产作者贡献大；关键词聚类有十个主要类别；研究经历初步探索、扩张和整合三个阶段。

Conclusion: 未来应强调对微塑料对土壤生态系统和生物影响的多层次评估，以揭示危害并制定解决方案。

Abstract: This paper aims to comprehensively grasp the research status and development
trends of soil microplastics (MPs). It collects studies from the Web of Science
Core Collection covering the period from 2013 to 2024. Employing CiteSpace and
VOSviewer, the paper conducts in - depth analyses of literature regarding the
environmental impacts of microplastics. These analyses involve keyword co -
occurrence, clustering, burst term identification, as well as co - occurrence
analysis of authors and institutions. Microplastics can accumulate in soil,
transfer through food chains, and ultimately affect human health, making the
research on them essential for effective pollution control. Focusing on the
international research on the impacts of microplastics on soil and ecosystems,
the study reveals a steadily increasing trend in the number of publications
each year, reaching a peak of 956 articles in 2024. A small number of highly
productive authors contribute significantly to the overall research output. The
keyword clustering analysis results in ten major clusters, including topics
such as plastic pollution and microbial communities. The research on soil
microplastics has evolved through three distinct stages: the preliminary
exploration phase from 2013 to 2016, the expansion phase from 2017 to 2020, and
the integration phase from 2021 to 2024. For future research, multi - level
assessments of the impacts of microplastics on soil ecosystems and organisms
should be emphasized, in order to fully uncover the associated hazards and
develop practical solutions.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [258] [Meteoroid stream identification with HDBSCAN unsupervised clustering algorithm](https://arxiv.org/abs/2507.01501)
*Eloy Peña-Asensio,Fabio Ferrari*

Main category: astro-ph.EP

TL;DR: 本文评估HDBSCAN算法用于无监督流星体流识别的性能，与CAMS查找表方法对比，发现HDBSCAN虽需谨慎选择最小簇大小，但表现良好，有替代潜力。


<details>
  <summary>Details</summary>
Motivation: 准确识别流星体流对理解其起源和演化至关重要，但重叠簇和背景噪声阻碍分类，影响相关任务，故评估HDBSCAN算法性能。

Method: 使用HDBSCAN算法对CAMS流星体轨道数据库v3.0进行分析，采用三种特征向量，设置不同最小簇大小和两种簇选择方法，用匈牙利算法映射簇，用多种指标评估聚类性能，并用主成分分析辅助。

Result: GEO向量确认39个流星体流，21个与CAMS高度一致；ORBIT向量识别30个，13个匹配度高；eom方法表现更佳；HDBSCAN聚类稳健，统计一致性优于查找表方法。

Conclusion: HDBSCAN有潜力成为流星体流识别的数学一致替代方法，但需进一步验证物理有效性。

Abstract: Accurate identification of meteoroid streams is central to understanding
their origins and evolution. However, overlapping clusters and background noise
hinder classification, an issue amplified for missions such as ESA's LUMIO that
rely on meteor shower observations to infer lunar meteoroid impact parameters.
This study evaluates the performance of the Hierarchical Density-Based Spatial
Clustering of Applications with Noise (HDBSCAN) algorithm for unsupervised
meteoroid stream identification, comparing its outcomes with the established
Cameras for All-Sky Meteor Surveillance (CAMS) look-up table method. We analyze
the CAMS Meteoroid Orbit Database v3.0 using three feature vectors: LUTAB (CAMS
geocentric parameters), ORBIT (heliocentric orbital elements), and GEO (adapted
geocentric parameters). HDBSCAN is applied with varying minimum cluster sizes
and two cluster selection methods (eom and leaf). To align HDBSCAN clusters
with CAMS classifications, the Hungarian algorithm determines the optimal
mapping. Clustering performance is assessed via the Silhouette score,
Normalized Mutual Information, and F1 score, with Principal Component Analysis
further supporting the analysis. With the GEO vector, HDBSCAN confirms 39
meteoroid streams, 21 strongly aligning with CAMS. The ORBIT vector identifies
30 streams, 13 with high matching scores. Less active showers pose
identification challenges. The eom method consistently yields superior
performance and agreement with CAMS. Although HDBSCAN requires careful
selection of the minimum cluster size, it delivers robust, internally
consistent clusters and outperforms the look-up table method in statistical
coherence. These results underscore HDBSCAN's potential as a mathematically
consistent alternative for meteoroid stream identification, although further
validation is needed to assess physical validity.

</details>
