<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 32]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 9]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 76]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.SE](#cs.SE) [Total: 16]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [stat.ML](#stat.ML) [Total: 12]
- [eess.SY](#eess.SY) [Total: 8]
- [cs.CV](#cs.CV) [Total: 16]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.HC](#cs.HC) [Total: 9]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.MM](#cs.MM) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [cs.MS](#cs.MS) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.DL](#cs.DL) [Total: 2]
- [stat.AP](#stat.AP) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.CL](#cs.CL) [Total: 13]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.SI](#cs.SI) [Total: 2]
- [cond-mat.supr-con](#cond-mat.supr-con) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [econ.GN](#econ.GN) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)
*Zhaorun Chen,Zhuokai Zhao,Kai Zhang,Bo Liu,Qi Qi,Yifan Wu,Tarun Kalluri,Sara Cao,Yuanhao Xiong,Haibo Tong,Huaxiu Yao,Hengduo Li,Jiacheng Zhu,Xian Li,Dawn Song,Bo Li,Jason Weston,Dat Huynh*

Main category: cs.AI

TL;DR: 提出DreamGym框架解决强化学习在训练大语言模型智能体时数据收集难题，实验显示其在多场景提升训练效果。


<details>
  <summary>Details</summary>
Motivation: 强化学习应用于大语言模型智能体存在成本高、任务多样性有限等挑战，阻碍可扩展经验数据收集。

Method: 引入DreamGym框架，将环境动态提炼为基于推理的经验模型，利用经验回放缓冲区，自适应生成新任务。

Result: 在不同环境和智能体骨干网络实验中，DreamGym显著提升强化学习训练效果，在一些任务上大幅超越基线，在模拟到真实迁移场景也有良好表现。

Conclusion: DreamGym为通用强化学习提供可扩展的预热启动策略。

Abstract: While reinforcement learning (RL) can empower large language model (LLM)
agents by enabling self-improvement through interaction, its practical adoption
remains challenging due to costly rollouts, limited task diversity, unreliable
reward signals, and infrastructure complexity, all of which obstruct the
collection of scalable experience data. To address these challenges, we
introduce DreamGym, the first unified framework designed to synthesize diverse
experiences with scalability in mind to enable effective online RL training for
autonomous agents. Rather than relying on expensive real-environment rollouts,
DreamGym distills environment dynamics into a reasoning-based experience model
that derives consistent state transitions and feedback signals through
step-by-step reasoning, enabling scalable agent rollout collection for RL. To
improve the stability and quality of transitions, DreamGym leverages an
experience replay buffer initialized with offline real-world data and
continuously enriched with fresh interactions to actively support agent
training. To improve knowledge acquisition, DreamGym adaptively generates new
tasks that challenge the current agent policy, enabling more effective online
curriculum learning. Experiments across diverse environments and agent
backbones demonstrate that DreamGym substantially improves RL training, both in
fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready
tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in
RL-ready but costly settings, it matches GRPO and PPO performance using only
synthetic interactions. When transferring a policy trained purely on synthetic
experiences to real-environment RL, DreamGym yields significant additional
performance gains while requiring far fewer real-world interactions, providing
a scalable warm-start strategy for general-purpose RL.

</details>


### [2] [How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis](https://arxiv.org/abs/2511.03825)
*Ahmed Mostafa,Raisul Arefin Nahid,Samuel Mulder*

Main category: cs.AI

TL;DR: 本文评估NLP分词模型和参数选择对汇编代码分析的影响，研究不同分词器在汇编代码中的表现，发现分词器选择影响下游性能，为底层代码分析优化提供见解。


<details>
  <summary>Details</summary>
Motivation: 汇编代码分词是未充分探索的领域，本文旨在评估NLP分词模型和参数选择，解决该领域研究不足的问题。

Method: 对各种分词模型进行深入研究，通过内在评估比较分词器，使用预训练模型评估分词器在多个性能指标上的有效性。

Result: 分词器选择显著影响下游性能，内在指标对外部评估结果有部分但不完整的可预测性。

Conclusion: 本研究为底层代码分析优化分词模型提供了有价值的见解，有助于基于自然语言模型的二进制分析工作流程的健壮性和可扩展性。

Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic
characteristics like vocabulary size, semantic coverage, and extrinsic
performance in downstream tasks. Despite its significance, tokenization in the
context of assembly code remains an underexplored area. This study aims to
address this gap by evaluating the intrinsic properties of Natural Language
Processing (NLP) tokenization models and parameter choices, such as vocabulary
size. We explore preprocessing customization options and pre-tokenization rules
tailored to the unique characteristics of assembly code. Additionally, we
assess their impact on downstream tasks like function signature prediction -- a
critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models,
systematically analyzing their efficiency in encoding assembly instructions and
capturing semantic nuances. Through intrinsic evaluations, we compare
tokenizers based on tokenization efficiency, vocabulary compression, and
representational fidelity for assembly code. Using state-of-the-art pre-trained
models such as the decoder-only Large Language Model (LLM) Llama 3.2, the
encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate
the effectiveness of these tokenizers across multiple performance metrics.
Preliminary findings indicate that tokenizer choice significantly influences
downstream performance, with intrinsic metrics providing partial but incomplete
predictability of extrinsic evaluation outcomes. These results reveal complex
trade-offs between intrinsic tokenizer properties and their utility in
practical assembly code tasks. Ultimately, this study provides valuable
insights into optimizing tokenization models for low-level code analysis,
contributing to the robustness and scalability of Natural Language Model
(NLM)-based binary analysis workflows.

</details>


### [3] [To See or To Read: User Behavior Reasoning in Multimodal LLMs](https://arxiv.org/abs/2511.03845)
*Tianning Dong,Luyi Ma,Varun Vasudevan,Jason Cho,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: 提出BehaviorLens框架评估多模态大语言模型在用户行为推理中的模态权衡，发现图像表示可提升模型预测准确率。


<details>
  <summary>Details</summary>
Motivation: 探究文本或图像表示用户行为数据对最大化多模态大语言模型性能哪个更有效。

Method: 提出BehaviorLens基准测试框架，将交易数据分别以文本段落、散点图和流程图表示，对六个多模态大语言模型进行评估。

Result: 使用真实世界购买序列数据集，发现图像表示数据时，多模态大语言模型下次购买预测准确率比文本表示提高87.5%，且无需额外计算成本。

Conclusion: 图像表示在用户行为推理中对多模态大语言模型性能提升优于文本表示。

Abstract: Multimodal Large Language Models (MLLMs) are reshaping how modern agentic
systems reason over sequential user-behavior data. However, whether textual or
image representations of user behavior data are more effective for maximizing
MLLM performance remains underexplored. We present \texttt{BehaviorLens}, a
systematic benchmarking framework for assessing modality trade-offs in
user-behavior reasoning across six MLLMs by representing transaction data as
(1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a
real-world purchase-sequence dataset, we find that when data is represented as
images, MLLMs next-purchase prediction accuracy is improved by 87.5% compared
with an equivalent textual representation without any additional computational
cost.

</details>


### [4] [Shared Spatial Memory Through Predictive Coding](https://arxiv.org/abs/2511.04235)
*Zhengru Fang,Yu Guo,Jingjing Wang,Yuang Zhang,Haonan An,Yinhai Wang,Yuguang Fang*

Main category: cs.AI

TL;DR: 提出多智能体预测编码框架解决多智能体系统中共享和重建一致空间记忆的挑战，在Memory - Maze基准测试中表现出色，为社会集体智能提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中部分可观测性和有限带宽导致协调失败的问题，实现共享和重建一致的空间记忆。

Method: 引入多智能体预测编码框架，将协调问题转化为最小化智能体间的相互不确定性，以信息瓶颈目标为实例，利用类似网格细胞的度量进行自定位，结合分层强化学习策略。

Result: 在Memory - Maze基准测试中，该方法对带宽约束有出色的恢复能力，带宽从128降至4比特/步时，成功率从73.5%降至64.4%，而全广播基线从67.6%降至28.6%。

Conclusion: 该研究为复杂社会表征如何从统一的预测驱动中出现提供了理论上有原则且生物学上合理的基础，实现了社会集体智能。

Abstract: Sharing and reconstructing a consistent spatial memory is a critical
challenge in multi-agent systems, where partial observability and limited
bandwidth often lead to catastrophic failures in coordination. We introduce a
multi-agent predictive coding framework that formulate coordination as the
minimization of mutual uncertainty among agents. Instantiated as an information
bottleneck objective, it prompts agents to learn not only who and what to
communicate but also when. At the foundation of this framework lies a
grid-cell-like metric as internal spatial coding for self-localization,
emerging spontaneously from self-supervised motion prediction. Building upon
this internal spatial code, agents gradually develop a bandwidth-efficient
communication mechanism and specialized neural populations that encode
partners' locations: an artificial analogue of hippocampal social place cells
(SPCs). These social representations are further enacted by a hierarchical
reinforcement learning policy that actively explores to reduce joint
uncertainty. On the Memory-Maze benchmark, our approach shows exceptional
resilience to bandwidth constraints: success degrades gracefully from 73.5% to
64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast
baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically
principled and biologically plausible basis for how complex social
representations emerge from a unified predictive drive, leading to social
collective intelligence.

</details>


### [5] [KnowThyself: An Agentic Assistant for LLM Interpretability](https://arxiv.org/abs/2511.03878)
*Suraj Prasai,Mengnan Du,Ying Zhang,Fan Yang*

Main category: cs.AI

TL;DR: 开发了KnowThyself辅助工具，提升大语言模型可解释性，通过聊天界面整合功能，降低技术门槛。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型可解释性工具零散且代码密集，需整合工具降低技术门槛。

Method: 将用户查询经编排器LLM重新表述，由代理路由器导向专业模块，输出形成连贯解释，嵌入对话工作流。

Result: 构建了以聊天为基础的界面，用户可上传模型、提问并获得可视化及解释。

Conclusion: KnowThyself为大语言模型可解释性提供了强大且易用的基础平台。

Abstract: We develop KnowThyself, an agentic assistant that advances large language
model (LLM) interpretability. Existing tools provide useful insights but remain
fragmented and code-intensive. KnowThyself consolidates these capabilities into
a chat-based interface, where users can upload models, pose natural language
questions, and obtain interactive visualizations with guided explanations. At
its core, an orchestrator LLM first reformulates user queries, an agent router
further directs them to specialized modules, and the outputs are finally
contextualized into coherent explanations. This design lowers technical
barriers and provides an extensible platform for LLM inspection. By embedding
the whole process into a conversational workflow, KnowThyself offers a robust
foundation for accessible LLM interpretability.

</details>


### [6] [Extracting Causal Relations in Deep Knowledge Tracing](https://arxiv.org/abs/2511.03948)
*Kevin Hong,Kia Karbasi,Gregory Pottie*

Main category: cs.AI

TL;DR: 本文挑战了DKT性能提升源于双向关系建模的主流解释，指出其优势在于对先修关系的因果结构建模，并通过实验验证，还提出提取练习关系DAG的替代方法。


<details>
  <summary>Details</summary>
Motivation: 挑战DKT性能提升源于双向关系建模的主流解释，探索其真正优势。

Method: 将练习关系图修剪为有向无环图（DAG），在Assistments数据集的因果子集上训练DKT；用DKT学习的表示提取练习关系DAG。

Result: DKT的预测能力与因果结构高度一致，为提出的观点提供了实证证据。

Conclusion: DKT的有效性主要源于其对知识组件间因果依赖关系的近似，而非简单的关系映射。

Abstract: A longstanding goal in computational educational research is to develop
explainable knowledge tracing (KT) models. Deep Knowledge Tracing (DKT), which
leverages a Recurrent Neural Network (RNN) to predict student knowledge and
performance on exercises, has been proposed as a major advancement over
traditional KT methods. Several studies suggest that its performance gains stem
from its ability to model bidirectional relationships between different
knowledge components (KCs) within a course, enabling the inference of a
student's understanding of one KC from their performance on others. In this
paper, we challenge this prevailing explanation and demonstrate that DKT's
strength lies in its implicit ability to model prerequisite relationships as a
causal structure, rather than bidirectional relationships. By pruning exercise
relation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal
subsets of the Assistments dataset, we show that DKT's predictive capabilities
align strongly with these causal structures. Furthermore, we propose an
alternative method for extracting exercise relation DAGs using DKT's learned
representations and provide empirical evidence supporting our claim. Our
findings suggest that DKT's effectiveness is largely driven by its capacity to
approximate causal dependencies between KCs rather than simple relational
mappings.

</details>


### [7] [Large language models replicate and predict human cooperation across experiments in game theory](https://arxiv.org/abs/2511.04500)
*Andrea Cera Palatsi,Samuel Martin-Gutierrez,Ana S. Cardenal,Max Pellert*

Main category: cs.AI

TL;DR: 本文开发数字孪生和评估框架测试开源模型，发现Llama高保真复制人类合作模式，Qwen符合纳什均衡预测，实现无角色提示的群体行为复制，还对新游戏配置提出可验证假设，展示了大语言模型复制人类行为模式的能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多领域决策和模拟人类行为的应用中，其与实际人类决策的契合度尚不明确，这种差距可能导致实际应用的不良后果和模拟无效，因此需要进行研究。

Method: 开发博弈论实验的数字孪生，引入系统的提示和探测框架进行机器行为评估，测试Llama、Mistral和Qwen三个开源模型。

Result: Llama高保真复制人类合作模式，捕捉人类偏离理性选择理论的情况；Qwen与纳什均衡预测紧密一致；实现无角色提示的群体行为复制；对新游戏配置生成并预注册可验证假设。

Conclusion: 适当校准的大语言模型可以复制人类总体行为模式，对未探索的实验空间进行系统探索，为社会和行为科学传统研究提供补充方法，产生关于人类社会决策的新实证预测。

Abstract: Large language models (LLMs) are increasingly used both to make decisions in
domains such as health, education and law, and to simulate human behavior. Yet
how closely LLMs mirror actual human decision-making remains poorly understood.
This gap is critical: misalignment could produce harmful outcomes in practical
applications, while failure to replicate human behavior renders LLMs
ineffective for social simulations. Here, we address this gap by developing a
digital twin of game-theoretic experiments and introducing a systematic
prompting and probing framework for machine-behavioral evaluation. Testing
three open-source models (Llama, Mistral and Qwen), we find that Llama
reproduces human cooperation patterns with high fidelity, capturing human
deviations from rational choice theory, while Qwen aligns closely with Nash
equilibrium predictions. Notably, we achieved population-level behavioral
replication without persona-based prompting, simplifying the simulation
process. Extending beyond the original human-tested games, we generate and
preregister testable hypotheses for novel game configurations outside the
original parameter grid. Our findings demonstrate that appropriately calibrated
LLMs can replicate aggregate human behavioral patterns and enable systematic
exploration of unexplored experimental spaces, offering a complementary
approach to traditional research in the social and behavioral sciences that
generates new empirical predictions about human social decision-making.

</details>


### [8] [LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing](https://arxiv.org/abs/2511.03980)
*Bram Bulté,Ayla Rigouts Terryn*

Main category: cs.AI

TL;DR: 研究探讨提示语言和文化框架对大语言模型输出与人类价值观一致性的影响，发现模型虽有响应但存在文化偏差，难以充分代表文化多样性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练数据和优化目标存在不平衡，质疑其能否代表广泛用户群体的文化多样性，因此研究提示语言和文化框架对模型输出的影响。

Method: 用霍夫斯泰德价值观调查模块和世界价值观调查的63个项目，翻译成11种语言，以有无明确文化视角的提示语对10个大语言模型进行探测。

Result: 提示语言和文化视角都会使模型输出产生变化，但模型存在对特定国家价值观的系统偏差；明确文化视角比目标提示语言更能改善与人类文化价值观的一致性；结合两种方法不比英文提示加文化框架更有效。

Conclusion: 大语言模型能对提示变化作出响应产生差异，但过于依赖特定文化默认值，无法充分代表文化多样性。

Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the
globe, who interact with them in a diverse range of languages. At the same
time, there are well-documented imbalances in the training data and
optimisation objectives of this technology, raising doubts as to whether LLMs
can represent the cultural diversity of their broad user base. In this study,
we look at LLMs and cultural values and examine how prompt language and
cultural framing influence model responses and their alignment with human
values in different countries. We probe 10 LLMs with 63 items from the Hofstede
Values Survey Module and World Values Survey, translated into 11 languages, and
formulated as prompts with and without different explicit cultural
perspectives. Our study confirms that both prompt language and cultural
perspective produce variation in LLM outputs, but with an important caveat:
While targeted prompting can, to a certain extent, steer LLM responses in the
direction of the predominant values of the corresponding countries, it does not
overcome the models' systematic bias toward the values associated with a
restricted set of countries in our dataset: the Netherlands, Germany, the US,
and Japan. All tested models, regardless of their origin, exhibit remarkably
similar patterns: They produce fairly neutral responses on most topics, with
selective progressive stances on issues such as social tolerance. Alignment
with cultural values of human respondents is improved more with an explicit
cultural perspective than with a targeted prompt language. Unexpectedly,
combining both approaches is no more effective than cultural framing with an
English prompt. These findings reveal that LLMs occupy an uncomfortable middle
ground: They are responsive enough to changes in prompts to produce variation,
but too firmly anchored to specific cultural defaults to adequately represent
cultural diversity.

</details>


### [9] [Optimizing Sensor Placement in Urban Storm Sewers: A Data-Driven Sparse Sensing Approach](https://arxiv.org/abs/2511.04556)
*Zihang Ding,Kun Zhang*

Main category: cs.AI

TL;DR: 本文提出数据驱动稀疏传感（DSS）框架优化传感器布局，以实现城市雨水系统峰值流量重建，在案例中少量传感器取得良好重建效果，框架可用于有限资源下的洪水预警和实时控制。


<details>
  <summary>Details</summary>
Motivation: 城市地表洪水频发，高时空分辨率的洪水预测和监测受资源限制，需要解决在资源受限情况下监测城市排水网络和预测流量状况的问题。

Method: 提出DSS框架并与EPA - SWMM集成，用SWMM模型生成训练数据集，利用奇异值分解降维和QR分解进行传感器分配确定最优监测节点，通过对比验证节点代表性。

Result: 77个节点中3个最优放置的传感器实现了令人满意的重建性能，NSE值为0.92 - 0.95，模型对测量不确定性有良好鲁棒性，对传感器故障的鲁棒性与位置有关且随传感器数量增加而提高。

Conclusion: 该框架平衡了计算效率和物理可解释性，能用最少的传感器实现高精度流量重建，可与预测模型集成用于有限传感和监测资源下的洪水预警和实时控制。

Abstract: Urban surface water flooding, triggered by intense rainfall overwhelming
drainage systems, is increasingly frequent and widespread. While flood
prediction and monitoring in high spatial-temporal resolution are desired,
practical constraints in time, budget, and technology hinder its full
implementation. How to monitor urban drainage networks and predict flow
conditions under constrained resource is a major challenge. This study presents
a data-driven sparse sensing (DSS) framework, integrated with EPA-SWMM, to
optimize sensor placement and reconstruct peak flowrates in a stormwater
system, using the Woodland Avenue catchment in Duluth, Minnesota, as a case
study. We utilized a SWMM model to generate a training dataset of peak flowrate
profiles across the stormwater network. Furthermore, we applied DSS -
leveraging singular value decomposition for dimensionality reduction and QR
factorization for sensor allocation - to identify the optimal monitoring nodes
based on the simulated training dataset. We then validated the
representativeness of these identified monitoring nodes by comparing the
DSS-reconstructed peak flowrate profiles with those obtained from SWMM. Three
optimally placed sensors among 77 nodes achieved satisfactory reconstruction
performance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95 (25th to
75th percentiles). In addition, the model showed good robustness to uncertainty
in measurements. Its robustness to sensor failures is location-dependent and
improves with the number of sensors deployed. The framework balances
computational efficiency and physical interpretability, enabling high-accuracy
flow reconstruction with minimal sensors. This DSS framework can be further
integrated with predictive models to realize flood early warning and real-time
control under limited sensing and monitoring resource.

</details>


### [10] [ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering](https://arxiv.org/abs/2511.03985)
*Zhuowen Yuan,Tao Liu,Yang Yang,Yang Wang,Feng Qi,Kaushik Rangadurai,Bo Li,Shuang Yang*

Main category: cs.AI

TL;DR: 现有基于大语言模型的代理在自动化机器学习工程中有局限，本文提出多智能体系统ArchPilot，实验证明其优于SOTA基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代理在自动化机器学习工程中依赖全量训练评估，有计算开销大、扩展性差和迭代慢等问题。

Method: 引入ArchPilot多智能体系统，包含协调、生成和评估三个代理，采用受蒙特卡罗树搜索启发的新算法和重启机制等。

Result: 在MLE-Bench上的实验表明，ArchPilot优于AIDE和ML - Master等SOTA基线。

Conclusion: ArchPilot多智能体系统有效，能在有限预算下实现高效机器学习工程。

Abstract: Recent LLM-based agents have demonstrated strong capabilities in automated ML
engineering. However, they heavily rely on repeated full training runs to
evaluate candidate solutions, resulting in significant computational overhead,
limited scalability to large search spaces, and slow iteration cycles. To
address these challenges, we introduce ArchPilot, a multi-agent system that
integrates architecture generation, proxy-based evaluation, and adaptive search
into a unified framework. ArchPilot consists of three specialized agents: an
orchestration agent that coordinates the search process using a Monte Carlo
Tree Search (MCTS)-inspired novel algorithm with a restart mechanism and
manages memory of previous candidates; a generation agent that iteratively
generates, improves, and debugs candidate architectures; and an evaluation
agent that executes proxy training runs, generates and optimizes proxy
functions, and aggregates the proxy scores into a fidelity-aware performance
metric. This multi-agent collaboration allows ArchPilot to prioritize
high-potential candidates with minimal reliance on expensive full training
runs, facilitating efficient ML engineering under limited budgets. Experiments
on MLE-Bench demonstrate that ArchPilot outperforms SOTA baselines such as AIDE
and ML-Master, validating the effectiveness of our multi-agent system.

</details>


### [11] [Detecting Silent Failures in Multi-Agentic AI Trajectories](https://arxiv.org/abs/2511.04032)
*Divya Pathak,Harshit Kumar,Anuska Roy,Felix George,Mudit Verma,Pratibha Moogi*

Main category: cs.AI

TL;DR: 本文针对多智能体AI系统引入异常检测任务，构建数据集并进行基准测试，发现有监督和半监督方法表现良好。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统存在难以检测的静默故障，需要进行异常检测。

Method: 引入异常检测任务，提出数据集整理流程，构建并标注两个基准数据集，对异常检测方法进行基准测试。

Result: 有监督（XGBoost）和半监督（SVDD）方法表现相当，准确率分别达98%和96%。

Conclusion: 本研究首次对多智能体AI系统的异常检测进行系统研究，为未来研究提供数据集、基准和见解。

Abstract: Multi-Agentic AI systems, powered by large language models (LLMs), are
inherently non-deterministic and prone to silent failures such as drift,
cycles, and missing details in outputs, which are difficult to detect. We
introduce the task of anomaly detection in agentic trajectories to identify
these failures and present a dataset curation pipeline that captures user
behavior, agent non-determinism, and LLM variation. Using this pipeline, we
curate and label two benchmark datasets comprising \textbf{4,275 and 894}
trajectories from Multi-Agentic AI systems. Benchmarking anomaly detection
methods on these datasets, we show that supervised (XGBoost) and
semi-supervised (SVDD) approaches perform comparably, achieving accuracies up
to 98% and 96%, respectively. This work provides the first systematic study of
anomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks,
and insights to guide future research.

</details>


### [12] [Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis](https://arxiv.org/abs/2511.04584)
*Daniel Gomm,Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: 本文将表格数据自然语言接口查询中的歧义视为合作交互特征，提出框架区分合作与非合作查询，分析流行数据集查询问题，为未来研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 解决表格数据自然语言接口查询中固有的歧义问题，改变对歧义的处理视角。

Method: 开发区分合作与非合作查询的框架，并应用于表格问答和分析评估，分析15个流行数据集的查询。

Result: 发现流行数据集中查询类型无控制混合，既不能评估系统执行准确性，也不能评估解释能力。

Conclusion: 从解决歧义转向接受合作解决查询，为表格数据自然语言接口的设计和评估提供信息，指出未来研究方向。

Abstract: Natural language interfaces to tabular data must handle ambiguities inherent
to queries. Instead of treating ambiguity as a deficiency, we reframe it as a
feature of cooperative interaction, where the responsibility of query
specification is shared among the user and the system. We develop a principled
framework distinguishing cooperative queries, i.e., queries that yield a
resolvable interpretation, from uncooperative queries that cannot be resolved.
Applying the framework to evaluations for tabular question answering and
analysis, we analyze the queries in 15 popular datasets, and observe an
uncontrolled mixing of query types neither adequate for evaluating a system's
execution accuracy nor for evaluating interpretation capabilities. Our
framework and analysis of queries shifts the perspective from fixing ambiguity
to embracing cooperation in resolving queries. This reflection enables more
informed design and evaluation for natural language interfaces for tabular
data, for which we outline implications and directions for future research.

</details>


### [13] [Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models](https://arxiv.org/abs/2511.04053)
*Hirohane Takagi,Gouki Minegishi,Shota Kizawa,Issey Sukeda,Hitomi Yanaka*

Main category: cs.AI

TL;DR: 研究大语言模型数值推理错误的潜在表征机制，发现LLMs编码数值相关性但会放大，无关上下文影响表征，揭示决策漏洞。


<details>
  <summary>Details</summary>
Motivation: 行为研究记录了大语言模型的数值推理错误，但潜在表征机制不明，需深入研究。

Method: 结合线性探测、偏相关分析和基于提示的漏洞测试，对不同大小的模型进行研究。

Result: LLMs编码现实世界数值相关性但会系统地放大，无关上下文会使量级表征产生一致偏移，下游影响因模型大小而异。

Conclusion: 这些发现揭示了大语言模型决策的漏洞，为多属性纠缠下更公平、考虑表征的控制奠定基础。

Abstract: Although behavioral studies have documented numerical reasoning errors in
large language models (LLMs), the underlying representational mechanisms remain
unclear. We hypothesize that numerical attributes occupy shared latent
subspaces and investigate two questions:(1) How do LLMs internally integrate
multiple numerical attributes of a single entity? (2)How does irrelevant
numerical context perturb these representations and their downstream outputs?
To address these questions, we combine linear probing with partial correlation
analysis and prompt-based vulnerability tests across models of varying sizes.
Our results show that LLMs encode real-world numerical correlations but tend to
systematically amplify them. Moreover, irrelevant context induces consistent
shifts in magnitude representations, with downstream effects that vary by model
size. These findings reveal a vulnerability in LLM decision-making and lay the
groundwork for fairer, representation-aware control under multi-attribute
entanglement.

</details>


### [14] [Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents](https://arxiv.org/abs/2511.04076)
*Hao Li,Haotian Chen,Ruoyuan Gong,Juanjuan Wang,Hao Jiang*

Main category: cs.AI

TL;DR: 提出Agentmandering框架，将选区重划视为双方代理的回合制谈判，减少党派偏见和不公平性。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法忽视选区重划选择过程中的战略动态，易被党派利用，无法保证公平。

Method: 提出Agentmandering框架，借鉴博弈论思想，通过大语言模型代理嵌入战略互动，让代理从候选地图中交替选择和冻结选区。

Result: 在2020年后美国人口普查数据上评估显示，该方法显著减少党派偏见和不公平性，方差比标准基线低2到3个数量级。

Conclusion: Agentmandering方法在选区重划中具有公平性和稳定性，尤其在摇摆州场景中效果显著。

Abstract: Redistricting plays a central role in shaping how votes are translated into
political power. While existing computational methods primarily aim to generate
large ensembles of legally valid districting plans, they often neglect the
strategic dynamics involved in the selection process. This oversight creates
opportunities for partisan actors to cherry-pick maps that, while technically
compliant, are politically advantageous. Simply satisfying formal constraints
does not ensure fairness when the selection process itself can be manipulated.
We propose \textbf{Agentmandering}, a framework that reimagines redistricting
as a turn-based negotiation between two agents representing opposing political
interests. Drawing inspiration from game-theoretic ideas, particularly the
\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction
into the redistricting process via large language model (LLM) agents. Agents
alternate between selecting and freezing districts from a small set of
candidate maps, gradually partitioning the state through constrained and
interpretable choices. Evaluation on post-2020 U.S. Census data across all
states shows that Agentmandering significantly reduces partisan bias and
unfairness, while achieving 2 to 3 orders of magnitude lower variance than
standard baselines. These results demonstrate both fairness and stability,
especially in swing-state scenarios. Our code is available at
https://github.com/Lihaogx/AgentMandering.

</details>


### [15] [KGFR: A Foundation Retriever for Generalized Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04093)
*Yuanning Cui,Zequn Sun,Wei Hu,Zhangjie Fu*

Main category: cs.AI

TL;DR: 提出LLM - KGFR协作框架解决大语言模型知识密集型问题，实验证明其性能、可扩展性和泛化性良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理知识密集型问题有局限，现有方法受数据集特定调优和可扩展性限制。

Method: 提出LLM - KGFR框架，KGFR用LLM生成描述编码关系，基于实体在问题中的角色初始化，采用APP处理大图，LLM通过多级别接口迭代请求信息形成可控推理循环。

Result: 实验表明LLM - KGFR性能强，具备可扩展性和泛化性。

Conclusion: LLM - KGFR为知识图谱增强推理提供了实用解决方案。

Abstract: Large language models (LLMs) excel at reasoning but struggle with
knowledge-intensive questions due to limited context and parametric knowledge.
However, existing methods that rely on finetuned LLMs or GNN retrievers are
limited by dataset-specific tuning and scalability on large or unseen graphs.
We propose the LLM-KGFR collaborative framework, where an LLM works with a
structured retriever, the Knowledge Graph Foundation Retriever (KGFR). KGFR
encodes relations using LLM-generated descriptions and initializes entities
based on their roles in the question, enabling zero-shot generalization to
unseen KGs. To handle large graphs efficiently, it employs Asymmetric
Progressive Propagation (APP)- a stepwise expansion that selectively limits
high-degree nodes while retaining informative paths. Through node-, edge-, and
path-level interfaces, the LLM iteratively requests candidate answers,
supporting facts, and reasoning paths, forming a controllable reasoning loop.
Experiments demonstrate that LLM-KGFR achieves strong performance while
maintaining scalability and generalization, providing a practical solution for
KG-augmented reasoning.

</details>


### [16] [Testing the Testers: Human-Driven Quality Assessment of Voice AI Testing Platforms](https://arxiv.org/abs/2511.04133)
*Miguel E. Andres,Vadim Fedorov,Rida Sadek,Enric Spagnolo-Arrizabalaga,Nadescha Trudel*

Main category: cs.AI

TL;DR: 提出首个通过以人为中心的基准测试评估语音AI测试质量的系统框架，经实证评估有效，能为大规模语音AI部署提供测量基础。


<details>
  <summary>Details</summary>
Motivation: 语音AI代理向生产部署过渡，但确保测试可靠性的系统方法不足，组织无法客观评估测试方法有效性，存在测量差距。

Method: 结合成熟心理测量技术和严格统计验证，提出评估框架，对三个领先商业平台进行全面实证评估。

Result: 框架显示出平台间显著性能差异，Evalion平台评估质量f1分数达0.92，模拟质量分数0.61，优于其他平台。

Conclusion: 框架可让研究人员和组织实证验证任何平台的测试能力，为大规模语音AI部署提供测量基础，支持材料便于可重复性和采用。

Abstract: Voice AI agents are rapidly transitioning to production deployments, yet
systematic methods for ensuring testing reliability remain underdeveloped.
Organizations cannot objectively assess whether their testing approaches
(internal tools or external platforms) actually work, creating a critical
measurement gap as voice AI scales to billions of daily interactions.
  We present the first systematic framework for evaluating voice AI testing
quality through human-centered benchmarking. Our methodology addresses the
fundamental dual challenge of testing platforms: generating realistic test
conversations (simulation quality) and accurately evaluating agent responses
(evaluation quality). The framework combines established psychometric
techniques (pairwise comparisons yielding Elo ratings, bootstrap confidence
intervals, and permutation tests) with rigorous statistical validation to
provide reproducible metrics applicable to any testing approach.
  To validate the framework and demonstrate its utility, we conducted
comprehensive empirical evaluation of three leading commercial platforms
focused on Voice AI Testing using 21,600 human judgments across 45 simulations
and ground truth validation on 60 conversations. Results reveal statistically
significant performance differences with the proposed framework, with the
top-performing platform, Evalion, achieving 0.92 evaluation quality measured as
f1-score versus 0.73 for others, and 0.61 simulation quality using a league
based scoring system (including ties) vs 0.43 for other platforms.
  This framework enables researchers and organizations to empirically validate
the testing capabilities of any platform, providing essential measurement
foundations for confident voice AI deployment at scale. Supporting materials
are made available to facilitate reproducibility and adoption.

</details>


### [17] [When Empowerment Disempowers](https://arxiv.org/abs/2511.04177)
*Claire Yang,Maya Cakmak,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 提出开源多人类网格世界测试套件Disempower - Grid，发现优化一人赋能会降低他人环境影响力和奖励，联合赋能可缓解但有代价，揭示单智能体适用的目标在多智能体中可能失调。


<details>
  <summary>Details</summary>
Motivation: 以往基于赋能的AI辅助研究假设智能体孤立辅助一人，而多人类场景如家庭和医院对AI辅助很有前景，需研究多人类场景下的赋能问题。

Method: 引入开源多人类网格世界测试套件Disempower - Grid进行实证研究。

Result: 优化一人赋能会显著降低他人环境影响力和奖励（即失能现象），联合赋能可缓解失能但会牺牲用户奖励。

Conclusion: 单智能体场景中看似一致的目标无关性目标在多智能体环境中可能变得不一致，给AI对齐社区带来更广泛挑战。

Abstract: Empowerment, a measure of an agent's ability to control its environment, has
been proposed as a universal goal-agnostic objective for motivating assistive
behavior in AI agents. While multi-human settings like homes and hospitals are
promising for AI assistance, prior work on empowerment-based assistance assumes
that the agent assists one human in isolation. We introduce an open source
multi-human gridworld test suite Disempower-Grid. Using Disempower-Grid, we
empirically show that assistive RL agents optimizing for one human's
empowerment can significantly reduce another human's environmental influence
and rewards - a phenomenon we formalize as disempowerment. We characterize when
disempowerment occurs in these environments and show that joint empowerment
mitigates disempowerment at the cost of the user's reward. Our work reveals a
broader challenge for the AI alignment community: goal-agnostic objectives that
seem aligned in single-agent settings can become misaligned in multi-agent
contexts.

</details>


### [18] [Opus: A Quantitative Framework for Workflow Evaluation](https://arxiv.org/abs/2511.04220)
*Alan Seroul,Théo Fagnoni,Inès Adnani,Dana O. Mohamed,Phillip Kingston*

Main category: cs.AI

TL;DR: 本文介绍Opus工作流评估框架，可量化工作流质量和效率，支持自动化评估等，还提出统一优化公式。


<details>
  <summary>Details</summary>
Motivation: 量化工作流的质量和效率，实现工作流的直接比较、评分和优化。

Method: 将工作流奖励模型与规范惩罚函数结合，提出统一优化公式。

Result: 框架可支持自动化工作流评估、排名和优化，还能集成到强化学习循环。

Conclusion: 所提出的框架和方法有助于识别和排名最优工作流。

Abstract: This paper introduces the Opus Workflow Evaluation Framework, a
probabilistic-normative formulation for quantifying Workflow quality and
efficiency. It integrates notions of correctness, reliability, and cost into a
coherent mathematical model that enables direct comparison, scoring, and
optimization of Workflows. The framework combines the Opus Workflow Reward, a
probabilistic function estimating expected performance through success
likelihood, resource usage, and output gain, with the Opus Workflow Normative
Penalties, a set of measurable functions capturing structural and informational
quality across Cohesion, Coupling, Observability, and Information Hygiene. It
supports automated Workflow assessment, ranking, and optimization within modern
automation systems such as Opus and can be integrated into Reinforcement
Learning loops to guide Workflow discovery and refinement. In this paper, we
introduce the Opus Workflow Reward model that formalizes Workflow success as a
probabilistic expectation over costs and outcomes. We define measurable Opus
Workflow Normative Penalties capturing structural, semantic, and signal-related
properties of Workflows. Finally, we propose a unified optimization formulation
for identifying and ranking optimal Workflows under joint Reward-Penalty
trade-offs.

</details>


### [19] [RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization](https://arxiv.org/abs/2511.04285)
*Zeng Zhiyuan,Jiashuo Liu,Zhangyue Yin,Ge Zhang,Wenhao Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: 提出RLoop框架解决RLVR训练中的过拟合问题，实验显示能提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR训练中模型过拟合、丢失泛化能力的问题，该问题由策略过专业化和灾难性遗忘导致。

Method: 引入基于迭代策略初始化的RLoop框架，通过RL探索解空间、过滤成功轨迹创建专家数据集，用RFT细化初始策略形成循环。

Result: 实验表明RLoop减轻遗忘问题，显著提升泛化性能，平均准确率提高9%，pass@32提高超15%。

Conclusion: RLoop通过迭代重初始化的探索与利用循环，将临时策略变化转化为稳健的性能提升。

Abstract: While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for
training large reasoning models, its training dynamics harbor a critical
challenge: RL overfitting, where models gain training rewards but lose
generalization. Our analysis reveals this is driven by policy
over-specialization and catastrophic forgetting of diverse solutions generated
during training. Standard optimization discards this valuable inter-step policy
diversity. To address this, we introduce RLoop, a self-improving framework
built on iterative policy initialization. RLoop transforms the standard
training process into a virtuous cycle: it first uses RL to explore the
solution space from a given policy, then filters the successful trajectories to
create an expert dataset. This dataset is used via Rejection-sampling
Fine-Tuning (RFT) to refine the initial policy, creating a superior starting
point for the next iteration. This loop of exploration and exploitation via
iterative re-initialization effectively converts transient policy variations
into robust performance gains. Our experiments show RLoop mitigates forgetting
and substantially improves generalization, boosting average accuracy by 9% and
pass@32 by over 15% compared to vanilla RL.

</details>


### [20] [GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents](https://arxiv.org/abs/2511.04307)
*Jian Mu,Chaoyun Zhang,Chiming Ni,Lu Wang,Bo Qiao,Kartik Mathur,Qianhui Wu,Yuhang Xie,Xiaojun Ma,Mengyu Zhou,Si Qin,Liqun Li,Yu Kang,Minghua Ma,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: 介绍用于推进计算机使用代理（CUAs）的大规模数据集GUI - 360°，指出CUA面临的问题，说明其构建方式、内容、支持任务，测试模型并公布数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 解决计算机使用代理（CUAs）面临的真实任务稀缺、多模态轨迹自动收集注释管道缺乏、统一基准缺失等问题。

Method: 采用LLM增强的、基本自动化的管道进行查询源获取、环境模板构建、任务实例化、批量执行和质量过滤。

Result: 数据集包含超120万个执行动作步骤，支持三项规范任务；基准测试显示现有模型有不足，微调学习有提升但未达人类水平。

Conclusion: 发布GUI - 360°数据集和代码，促进可重复研究，加速可靠桌面CUAs的发展。

Abstract: We introduce GUI-360$^\circ$, a large-scale, comprehensive dataset and
benchmark suite designed to advance computer-using agents (CUAs). CUAs present
unique challenges and is constrained by three persistent gaps: a scarcity of
real-world CUA tasks, the lack of automated collection-and-annotation pipelines
for multi-modal trajectories, and the absence of a unified benchmark that
jointly evaluates GUI grounding, screen parsing, and action prediction.
  GUI-360$^\circ$ addresses these gaps with an LLM-augmented, largely automated
pipeline for query sourcing, environment-template construction, task
instantiation, batched execution, and LLM-driven quality filtering. The
released corpus contains over 1.2M executed action steps across thousands of
trajectories in popular Windows office applications, and includes
full-resolution screenshots, accessibility metadata when available,
instantiated goals, intermediate reasoning traces, and both successful and
failed action trajectories. The dataset supports three canonical tasks, GUI
grounding, screen parsing, and action prediction, and a hybrid GUI+API action
space that reflects modern agent designs. Benchmarking state-of-the-art
vision--language models on GUI-360$^\circ$ reveals substantial out-of-the-box
shortcomings in grounding and action prediction; supervised fine-tuning and
reinforcement learning yield significant gains but do not close the gap to
human-level reliability. We release GUI-360$^\circ$ and accompanying code to
facilitate reproducible research and accelerate progress on robust desktop
CUAs.
  The full dataset has been made public on
https://huggingface.co/datasets/vyokky/GUI-360.

</details>


### [21] [Probing the Probes: Methods and Metrics for Concept Alignment](https://arxiv.org/abs/2511.04312)
*Jacob Lysnæs-Larsen,Marte Eggen,Inga Strümke*

Main category: cs.AI

TL;DR: 指出概念激活向量（CAV）探测准确率不能可靠衡量概念对齐度，提出概念定位方法和评估指标，发现有平移不变性和空间对齐的探测器能提升概念对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有观点认为高探测准确率意味着CAV能忠实地代表目标概念，但作者认为探测准确率不能可靠衡量概念对齐度，需解决此问题。

Method: 构建故意不对齐的探测器证明问题，引入基于空间线性归因的概念定位方法，提出三类评估概念对齐度的指标。

Result: 有平移不变性和空间对齐的探测器能持续提升概念对齐度。

Conclusion: 需要基于对齐的评估指标而非探测准确率，探测器应根据模型架构和目标概念性质定制。

Abstract: In explainable AI, Concept Activation Vectors (CAVs) are typically obtained
by training linear classifier probes to detect human-understandable concepts as
directions in the activation space of deep neural networks. It is widely
assumed that a high probe accuracy indicates a CAV faithfully representing its
target concept. However, we show that the probe's classification accuracy alone
is an unreliable measure of concept alignment, i.e., the degree to which a CAV
captures the intended concept. In fact, we argue that probes are more likely to
capture spurious correlations than they are to represent only the intended
concept. As part of our analysis, we demonstrate that deliberately misaligned
probes constructed to exploit spurious correlations, achieve an accuracy close
to that of standard probes. To address this severe problem, we introduce a
novel concept localization method based on spatial linear attribution, and
provide a comprehensive comparison of it to existing feature visualization
techniques for detecting and mitigating concept misalignment. We further
propose three classes of metrics for quantitatively assessing concept
alignment: hard accuracy, segmentation scores, and augmentation robustness. Our
analysis shows that probes with translation invariance and spatial alignment
consistently increase concept alignment. These findings highlight the need for
alignment-based evaluation metrics rather than probe accuracy, and the
importance of tailoring probes to both the model architecture and the nature of
the target concept.

</details>


### [22] [AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research](https://arxiv.org/abs/2511.04316)
*Tim Beyer,Jonas Dornbusch,Jakob Steimle,Moritz Ladenburger,Leo Schwinn,Stephan Günnemann*

Main category: cs.AI

TL;DR: 研究介绍了AdversariaLLM工具包，用于大语言模型越狱鲁棒性研究，旨在解决研究生态碎片化问题，促进可复现和可比研究。


<details>
  <summary>Details</summary>
Motivation: 大语言模型安全和鲁棒性研究生态碎片化，影响研究的可复现性和可比性，阻碍有意义的进展。

Method: 引入AdversariaLLM工具包，实现十二种对抗攻击算法，集成七种基准数据集，提供对多种开源大语言模型的访问，具备计算资源跟踪等高级特性，还集成JudgeZoo进行评判。

Result: 构建了AdversariaLLM工具包及相关组件。

Conclusion: 这些组件为大语言模型安全的透明、可比和可复现研究奠定了坚实基础。

Abstract: The rapid expansion of research on Large Language Model (LLM) safety and
robustness has produced a fragmented and oftentimes buggy ecosystem of
implementations, datasets, and evaluation methods. This fragmentation makes
reproducibility and comparability across studies challenging, hindering
meaningful progress. To address these issues, we introduce AdversariaLLM, a
toolbox for conducting LLM jailbreak robustness research. Its design centers on
reproducibility, correctness, and extensibility. The framework implements
twelve adversarial attack algorithms, integrates seven benchmark datasets
spanning harmfulness, over-refusal, and utility evaluation, and provides access
to a wide range of open-weight LLMs via Hugging Face. The implementation
includes advanced features for comparability and reproducibility such as
compute-resource tracking, deterministic results, and distributional evaluation
techniques. \name also integrates judging through the companion package
JudgeZoo, which can also be used independently. Together, these components aim
to establish a robust foundation for transparent, comparable, and reproducible
research in LLM safety.

</details>


### [23] [RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation](https://arxiv.org/abs/2511.04328)
*Jiahao Zhao,Luxin Xu,Minghuan Tan,Lichao Zhang,Ahmadreza Argha,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: 提出模拟评估临床咨询框架，构建RxSafeBench评估大语言模型用药安全能力，发现模型整合知识有困难，为改进提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型医疗系统用药安全研究因缺乏真实数据集受限，现实临床咨询场景下评估不足。

Method: 提出模拟评估临床咨询框架，生成含用药风险的问诊对话，构建RxRisk DB数据库，采用两阶段过滤策略形成RxSafeBench，用结构化选择题评估模型。

Result: 当前大语言模型难以整合禁忌和相互作用知识，尤其是风险隐含时。

Conclusion: 指出基于大语言模型系统用药安全的关键挑战，提供改进可靠性的见解，RxSafeBench推进更安全可信的AI临床决策支持。

Abstract: Numerous medical systems powered by Large Language Models (LLMs) have
achieved remarkable progress in diverse healthcare tasks. However, research on
their medication safety remains limited due to the lack of real world datasets,
constrained by privacy and accessibility issues. Moreover, evaluation of LLMs
in realistic clinical consultation settings, particularly regarding medication
safety, is still underexplored. To address these gaps, we propose a framework
that simulates and evaluates clinical consultations to systematically assess
the medication safety capabilities of LLMs. Within this framework, we generate
inquiry diagnosis dialogues with embedded medication risks and construct a
dedicated medication safety database, RxRisk DB, containing 6,725
contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.
A two-stage filtering strategy ensures clinical realism and professional
quality, resulting in the benchmark RxSafeBench with 2,443 high-quality
consultation scenarios. We evaluate leading open-source and proprietary LLMs
using structured multiple choice questions that test their ability to recommend
safe medications under simulated patient contexts. Results show that current
LLMs struggle to integrate contraindication and interaction knowledge,
especially when risks are implied rather than explicit. Our findings highlight
key challenges in ensuring medication safety in LLM-based systems and provide
insights into improving reliability through better prompting and task-specific
tuning. RxSafeBench offers the first comprehensive benchmark for evaluating
medication safety in LLMs, advancing safer and more trustworthy AI-driven
clinical decision support.

</details>


### [24] [Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for Language Model Reasoning](https://arxiv.org/abs/2511.04341)
*Nick Oh,Fernand Gobet*

Main category: cs.AI

TL;DR: 现有测试时推理架构忽略监控过程，导致前缀主导陷阱，本文提出MGV框架解决此问题，虽未实证验证，但提供了元认知理论的计算翻译。


<details>
  <summary>Details</summary>
Motivation: 现有测试时推理架构缺少监控过程，导致前缀主导陷阱和准确率损失，需解决架构缺口。

Method: 将Flavell、Nelson和Narens的元认知理论形式化为计算规范，提出Monitor - Generate - Verify (MGV)框架。

Result: 无实证验证，但提供了基础元认知理论的首次系统计算翻译。

Conclusion: 为理解推理系统故障提供原则性词汇，为未来测试时推理设计提供架构干预建议。

Abstract: Test-time reasoning architectures such as those following the Generate-Verify
paradigm -- where a model iteratively refines or verifies its own generated
outputs -- prioritise generation and verification but exclude the monitoring
processes that determine when and how reasoning should begin. This omission may
contribute to the prefix dominance trap, in which models commit early to
suboptimal reasoning paths and seldom recover, yielding roughly 20% accuracy
loss. We address this architectural gap by formalising Flavell's and Nelson and
Narens' metacognitive theories into computational specifications, proposing the
Monitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verify
paradigm by adding explicit monitoring that captures metacognitive experiences
(from difficulty assessments to confidence judgements) before generation begins
and refines future monitoring through verification feedback. Though we present
no empirical validation, this work provides the first systematic computational
translation of foundational metacognitive theories, offering a principled
vocabulary for understanding reasoning system failures and suggesting specific
architectural interventions for future test-time reasoning designs.

</details>


### [25] [Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization Approach](https://arxiv.org/abs/2511.04393)
*Chanwoo Park,Ziyang Chen,Asuman Ozdaglar,Kaiqing Zhang*

Main category: cs.AI

TL;DR: 本文提出Iterative RMFT方法提升大语言模型决策能力，经实验验证有效且有理论支持。


<details>
  <summary>Details</summary>
Motivation: 大语言模型原非为决策设计，在基础在线决策问题上表现不佳，需提升决策能力。

Method: 提出Iterative RMFT，在训练后重复将低遗憾决策轨迹提炼回基础模型，每次迭代中模型展开多决策轨迹，选k个低遗憾轨迹并据此微调。

Result: Iterative RMFT提升了不同大语言模型的决策性能，输出和推理格式灵活，能跨任务泛化，理论上单层Transformer在此范式下可成无遗憾学习者。

Conclusion: Iterative RMFT是提升大语言模型决策能力的有原则且通用的训练后框架。

Abstract: Large language models (LLMs) are increasingly deployed as "agents" for
decision-making (DM) in interactive and dynamic environments. Yet, since they
were not originally designed for DM, recent studies show that LLMs can struggle
even in basic online DM problems, failing to achieve low regret or an effective
exploration-exploitation tradeoff. To address this, we introduce Iterative
Regret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure
that repeatedly distills low-regret decision trajectories back into the base
model. At each iteration, the model rolls out multiple decision trajectories,
selects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior
methods that (a) distill action sequences from known DM algorithms or (b) rely
on manually crafted chain-of-thought templates, our approach leverages the
regret metric to elicit the model's own DM ability and reasoning rationales.
This reliance on model-generated reasoning avoids rigid output engineering and
provides more flexible, natural-language training signals. Empirical results
show that Iterative RMFT improves LLMs' DM performance across diverse models -
from Transformers with numerical input/output, to open-weight LLMs, and
advanced closed-weight models like GPT-4o mini. Its flexibility in output and
reasoning formats enables generalization across tasks with varying horizons,
action spaces, reward processes, and natural-language contexts. Finally, we
provide theoretical insight showing that a single-layer Transformer under this
paradigm can act as a no-regret learner in a simplified setting. Overall,
Iterative RMFT offers a principled and general post-training framework for
enhancing LLMs' decision-making capabilities.

</details>


### [26] [The Peril of Preference: Why GRPO fails on Ordinal Rewards](https://arxiv.org/abs/2511.04439)
*Anisha Garg,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: GRPO在利用丰富非二元反馈增强RL训练时存在缺陷，本文提出CoRPO解决此问题，在代码验证任务上表现更好，推动大模型从多维反馈学习。


<details>
  <summary>Details</summary>
Motivation: GRPO在使用序数奖励等丰富非二元反馈增强RL训练时存在缺陷，其组平均基线会强化错误行为，需要改进。

Method: 提出Correctness Relative Policy Optimization (CoRPO)，使用自适应基线，先执行最低质量阈值，达标后自动转为相对偏好模式。

Result: 在代码验证任务上，CoRPO展示出更稳定的收敛性和更好的域外泛化能力。

Conclusion: 这项工作是使大模型通过强化学习学习新能力的关键一步，推动大模型从多维反馈学习。

Abstract: Group-relative Policy Optimization's (GRPO) simplicity makes it highly
desirable for adapting LLMs to become experts at specific tasks. But this
simplicity also makes it ill-specified as we seek to enhance RL training with
richer, non-binary feedback. When using ordinal rewards to give partial credit,
GRPO's simplicity starts to hurt, as its group-average baseline often assigns a
positive advantage to failed trajectories and reinforces incorrect behavior.
  We introduce Correctness Relative Policy Optimization (CoRPO), a new
formulation that solves this flaw. CoRPO uses an adaptive baseline that
enforces a minimum quality threshold, ensuring failed solutions are never
positively reinforced. Once the policy consistently meets this threshold, the
baseline automatically transitions to a relative preference mode, pushing the
model to find optimal solutions rather than just "acceptable" ones. We
empirically validate CoRPO on a code verification task, where it demonstrates
more stable convergence and better out-of-domain generalization.
  This work represents a critical step in our broader research program to
enable LLMs to learn genuinely new capabilities through reinforcement learning.
We achieve this by enabling LLMs to learn from rich, multi-dimensional feedback
- progressing from binary to ordinal rewards in this work, and onward to
denser, per-step supervision.

</details>


### [27] [Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context](https://arxiv.org/abs/2511.04464)
*Carnot Braun,Rafael O. Jarczewski,Gabriel U. Talasso,Leandro A. Villas,Allan M. de Souza*

Main category: cs.AI

TL;DR: 本文介绍并评估了PAVe系统，它结合经典路径算法与上下文推理，在城市场景基准测试中表现良好，证明结合经典算法与基于大语言模型的语义推理层是优化城市出行的有效方法。


<details>
  <summary>Details</summary>
Motivation: 传统车辆路由系统缺乏解读和整合人类司机复杂、语义和动态上下文的能力，需要新方法来优化城市出行。

Method: 引入PAVe系统，使用大语言模型（LLM）代理对多目标Dijkstra算法生成的候选路线集进行评估，结合用户任务、偏好和避行规则，利用预处理的城市兴趣点地理空间缓存。

Result: 在现实城市场景基准测试中，PAVe能将复杂用户意图转化为适当的路线修改，使用本地模型初始路线选择准确率超88%。

Conclusion: 结合经典路由算法与基于LLM的语义推理层是创建个性化、自适应和可扩展的城市出行优化解决方案的稳健有效方法。

Abstract: Traditional vehicle routing systems efficiently optimize singular metrics
like time or distance, and when considering multiple metrics, they need more
processes to optimize . However, they lack the capability to interpret and
integrate the complex, semantic, and dynamic contexts of human drivers, such as
multi-step tasks, situational constraints, or urgent needs. This paper
introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a
hybrid agentic assistant designed to augment classical pathfinding algorithms
with contextual reasoning. Our approach employs a Large Language Model (LLM)
agent that operates on a candidate set of routes generated by a multi-objective
(time, CO2) Dijkstra algorithm. The agent evaluates these options against
user-provided tasks, preferences, and avoidance rules by leveraging a
pre-processed geospatial cache of urban Points of Interest (POIs). In a
benchmark of realistic urban scenarios, PAVe successfully used complex user
intent into appropriate route modifications, achieving over 88% accuracy in its
initial route selections with a local model. We conclude that combining
classical routing algorithms with an LLM-based semantic reasoning layer is a
robust and effective approach for creating personalized, adaptive, and scalable
solutions for urban mobility optimization.

</details>


### [28] [Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption through Empirical and Theoretical Analysis](https://arxiv.org/abs/2511.04481)
*Lars Krupp,Daniel Geißler,Vishal Banwari,Paul Lukowicz,Jakob Karolus*

Main category: cs.AI

TL;DR: 文章探讨网络代理的能源和二氧化碳成本，发现不同创建理念影响能耗，呼吁在评估中采用能耗指标。


<details>
  <summary>Details</summary>
Motivation: 网络代理研究繁荣，但相关可持续性问题未充分探索，需强调该问题的紧迫性。

Method: 从理论（估算）和实证（基准测试）角度，对网络代理的能源和二氧化碳成本进行初步探索。

Result: 不同创建理念严重影响能耗，更多能耗不一定带来更好结果，部分网络代理在披露模型参数和过程方面缺乏透明度，限制能耗估算。

Conclusion: 倡导在基准测试中采用专门衡量能耗的指标，改变评估网络代理的思维方式。

Abstract: Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful
agentic systems pushing the boundaries of Large Language Models (LLM). They can
autonomously interact with the internet at the user's behest, such as
navigating websites, filling search masks, and comparing price lists. Though
web agent research is thriving, induced sustainability issues remain largely
unexplored. To highlight the urgency of this issue, we provide an initial
exploration of the energy and $CO_2$ cost associated with web agents from both
a theoretical -via estimation- and an empirical perspective -by benchmarking.
Our results show how different philosophies in web agent creation can severely
impact the associated expended energy, and that more energy consumed does not
necessarily equate to better results. We highlight a lack of transparency
regarding disclosing model parameters and processes used for some web agents as
a limiting factor when estimating energy consumption. Our work contributes
towards a change in thinking of how we evaluate web agents, advocating for
dedicated metrics measuring energy consumption in benchmarks.

</details>


### [29] [Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper](https://arxiv.org/abs/2511.04583)
*Atsuyuki Miyai,Mashiro Toyooka,Takashi Otonari,Zaiying Zhao,Kiyoharu Aizawa*

Main category: cs.AI

TL;DR: 开发Jr. AI Scientist系统，评估显示其生成论文得分高于现有全自动化系统，但也发现应用风险和研究挑战。


<details>
  <summary>Details</summary>
Motivation: 了解AI科学家系统当前能力和风险，以确保可信、可持续的AI驱动科学进步并维护学术生态完整性。

Method: 开发Jr. AI Scientist系统，模拟新手学生研究流程，利用现代编码代理处理复杂实现；通过AI评审、作者评估和向Agents4Science投稿进行评估。

Result: Jr. AI Scientist生成的论文评审得分高于现有全自动化系统，同时发现应用局限性。

Conclusion: 指出直接应用当前AI科学家系统的潜在风险和未来研究关键挑战，全面报告开发中识别的风险，希望加深对AI科学家发展现状和风险的理解。

Abstract: Understanding the current capabilities and risks of AI Scientist systems is
essential for ensuring trustworthy and sustainable AI-driven scientific
progress while preserving the integrity of the academic ecosystem. To this end,
we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system
that mimics the core research workflow of a novice student researcher: Given
the baseline paper from the human mentor, it analyzes its limitations,
formulates novel hypotheses for improvement, validates them through rigorous
experimentation, and writes a paper with the results. Unlike previous
approaches that assume full automation or operate on small-scale code, Jr. AI
Scientist follows a well-defined research workflow and leverages modern coding
agents to handle complex, multi-file implementations, leading to scientifically
valuable contributions. For evaluation, we conducted automated assessments
using AI Reviewers, author-led evaluations, and submissions to Agents4Science,
a venue dedicated to AI-driven scientific contributions. The findings
demonstrate that Jr. AI Scientist generates papers receiving higher review
scores than existing fully automated systems. Nevertheless, we identify
important limitations from both the author evaluation and the Agents4Science
reviews, indicating the potential risks of directly applying current AI
Scientist systems and key challenges for future research. Finally, we
comprehensively report various risks identified during development. We hope
these insights will deepen understanding of current progress and risks in AI
Scientist development.

</details>


### [30] [Question the Questions: Auditing Representation in Online Deliberative Processes](https://arxiv.org/abs/2511.04588)
*Soham De,Lodewijk Gelauff,Ashish Goel,Smitha Milli,Ariel Procaccia,Alice Siu*

Main category: cs.AI

TL;DR: 提出基于合理代表（JR）概念的审计框架衡量问题代表性，给出通用效用设置下审计JR的算法，应用于历史审议并比较不同选问方式，指出大语言模型支持审议的优缺点，且便于从业者改进未来审议。


<details>
  <summary>Details</summary>
Motivation: 在审议过程中，因时间限制只能选少量问题，需解决如何选择最能代表所有参与者利益的问题。

Method: 引入基于JR概念的审计框架，给出通用效用设置下审计JR的算法，应用于历史审议比较不同选问方式。

Result: 凸显大语言模型支持审议过程的潜力和当前局限。

Conclusion: 将方法集成到在线审议平台，便于从业者审计和改进未来审议的代表性。

Abstract: A central feature of many deliberative processes, such as citizens'
assemblies and deliberative polls, is the opportunity for participants to
engage directly with experts. While participants are typically invited to
propose questions for expert panels, only a limited number can be selected due
to time constraints. This raises the challenge of how to choose a small set of
questions that best represent the interests of all participants. We introduce
an auditing framework for measuring the level of representation provided by a
slate of questions, based on the social choice concept known as justified
representation (JR). We present the first algorithms for auditing JR in the
general utility setting, with our most efficient algorithm achieving a runtime
of $O(mn\log n)$, where $n$ is the number of participants and $m$ is the number
of proposed questions. We apply our auditing methods to historical
deliberations, comparing the representativeness of (a) the actual questions
posed to the expert panel (chosen by a moderator), (b) participants' questions
chosen via integer linear programming, (c) summary questions generated by large
language models (LLMs). Our results highlight both the promise and current
limitations of LLMs in supporting deliberative processes. By integrating our
methods into an online deliberation platform that has been used for over
hundreds of deliberations across more than 50 countries, we make it easy for
practitioners to audit and improve representation in future deliberations.

</details>


### [31] [DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration](https://arxiv.org/abs/2511.04646)
*Narjes Nourzad,Hanqing Yang,Shiyu Chen,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: 提出用于合作多智能体规划的去中心化神经符号框架DR. WELL，通过两阶段协商协议合作，实验表明其能提升任务完成率和效率。


<details>
  <summary>Details</summary>
Motivation: 合作多智能体规划需在部分信息和有限通信下做决策，轨迹级协调常失败，需提升抽象层次解决问题。

Method: 提出DR. WELL框架，采用两阶段协商协议，基于共享世界模型生成和执行符号计划。

Result: 在合作推块任务实验中，动态世界模型通过协商和自我优化，提升了任务完成率和效率。

Conclusion: DR. WELL通过对符号计划推理避免脆弱的步骤级对齐，实现可重用、可同步和可解释的高级操作，能进化出更高效协作策略。

Abstract: Cooperative multi-agent planning requires agents to make joint decisions with
partial information and limited communication. Coordination at the trajectory
level often fails, as small deviations in timing or movement cascade into
conflicts. Symbolic planning mitigates this challenge by raising the level of
abstraction and providing a minimal vocabulary of actions that enable
synchronization and collective progress. We present DR. WELL, a decentralized
neurosymbolic framework for cooperative multi-agent planning. Cooperation
unfolds through a two-phase negotiation protocol: agents first propose
candidate roles with reasoning and then commit to a joint allocation under
consensus and environment constraints. After commitment, each agent
independently generates and executes a symbolic plan for its role without
revealing detailed trajectories. Plans are grounded in execution outcomes via a
shared world model that encodes the current state and is updated as agents act.
By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids
brittle step-level alignment and enables higher-level operations that are
reusable, synchronizable, and interpretable. Experiments on cooperative
block-push tasks show that agents adapt across episodes, with the dynamic world
model capturing reusable patterns and improving task completion rates and
efficiency. Experiments on cooperative block-push tasks show that our dynamic
world model improves task completion and efficiency through negotiation and
self-refinement, trading a time overhead for evolving, more efficient
collaboration strategies.

</details>


### [32] [VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks](https://arxiv.org/abs/2511.04662)
*Yu Feng,Nathaniel Weir,Kaj Bostrom,Sam Bayless,Darion Cassel,Sapana Chaudhary,Benjamin Kiesl-Reiter,Huzefa Rangwala*

Main category: cs.AI

TL;DR: 论文提出VeriCoT方法解决大语言模型链式思维推理中逻辑验证问题，实验证明其有效，并利用验证信号进一步提升推理效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型链式思维推理无法可靠验证自身逻辑，即使答案正确推理过程也可能有缺陷，影响高风险场景信任度。

Method: 引入神经 - 符号方法VeriCoT，将链式思维推理步骤形式化为一阶逻辑，识别论证前提，用自动求解器验证逻辑有效性。

Result: 在ProofWriter、LegalBench和BioASQ数据集实验表明，VeriCoT能有效识别有缺陷推理，可预测最终答案正确性；利用验证信号进一步提升推理有效性和准确性。

Conclusion: VeriCoT能解决大语言模型链式思维推理逻辑验证问题，提升推理的有效性和准确性。

Abstract: LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but
they cannot reliably verify their own logic. Even when they reach correct
answers, the underlying reasoning may be flawed, undermining trust in
high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a
neuro-symbolic method that extracts and verifies formal logical arguments from
CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order
logic and identifies premises that ground the argument in source context,
commonsense knowledge, or prior reasoning steps. The symbolic representation
enables automated solvers to verify logical validity while the NL premises
allow humans and systems to identify ungrounded or fallacious reasoning steps.
Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT
effectively identifies flawed reasoning, and serves as a strong predictor of
final answer correctness. We also leverage VeriCoT's verification signal for
(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on
VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct
preference optimization (DPO) using verification-based pairwise rewards,
further improving reasoning validity and accuracy.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [33] [Cross-Modal Alignment between Visual Stimuli and Neural Responses in the Visual Cortex](https://arxiv.org/abs/2511.04096)
*Xing Gao,Dazhong Rong,Qinming He*

Main category: cs.CE

TL;DR: 本文从传统直接编解码任务转向判别式编解码任务，提出VNA方法，经实验证明其在表征视觉 - 神经映射上优于直接编解码。


<details>
  <summary>Details</summary>
Motivation: 现有研究因神经反应可变性和有限记录技术，存在过拟合和缺乏泛化性问题，需更合理方法。

Method: 将任务从传统直接编解码转为判别式编解码，提出Visual - Neural Alignment (VNA) 跨模态对齐方法，并在三个侵入式视觉皮层数据集上进行实验。

Result: VNA方法在判别式编解码任务中总体表现优于直接编码和直接解码。

Conclusion: VNA方法能最精确地表征视觉 - 神经映射。

Abstract: Investigating the mapping between visual stimuli and neural responses in the
visual cortex contributes to a deeper understanding of biological visual
processing mechanisms. Most existing studies characterize this mapping by
training models to directly encode visual stimuli into neural responses or
decode neural responses into visual stimuli. However, due to neural response
variability and limited neural recording techniques, these studies suffer from
overfitting and lack generalizability. Motivated by this challenge, in this
paper we shift the tasks from conventional direct encoding and decoding to
discriminative encoding and decoding, which are more reasonable. And on top of
this we propose a cross-modal alignment approach, named Visual-Neural Alignment
(VNA). To thoroughly test the performance of the three methods (direct
encoding, direct decoding, and our proposed VNA) on discriminative encoding and
decoding tasks, we conduct extensive experiments on three invasive visual
cortex datasets, involving two types of subject mammals (mice and macaques).
The results demonstrate that our VNA generally outperforms direct encoding and
direct decoding, indicating our VNA can most precisely characterize the above
visual-neural mapping among the three methods.

</details>


### [34] [Fitting Reinforcement Learning Model to Behavioral Data under Bandits](https://arxiv.org/abs/2511.04454)
*Hao Zhu,Jasper Hoffmann,Baohe Zhang,Joschka Boedecker*

Main category: cs.CE

TL;DR: 本文研究多臂老虎机环境下强化学习（RL）模型拟合行为数据问题，提出通用数学优化问题公式，基于理论分析引入新的拟合方法，经模拟环境评估性能与现有水平相当且减少计算时间，还提供开源Python包。


<details>
  <summary>Details</summary>
Motivation: 解决多臂老虎机环境下将强化学习模型拟合到给定行为数据的问题，该模型近年来在刻画人类和动物决策行为方面受关注。

Method: 为常见RL模型拟合问题给出通用数学优化问题公式，进行凸性理论分析，基于此引入基于凸松弛和优化的新解法。

Result: 在模拟老虎机环境中评估，该方法性能与现有水平相当，显著减少计算时间。

Conclusion: 提出的方法有效，还提供开源Python包方便研究者直接应用。

Abstract: We consider the problem of fitting a reinforcement learning (RL) model to
some given behavioral data under a multi-armed bandit environment. These models
have received much attention in recent years for characterizing human and
animal decision making behavior. We provide a generic mathematical optimization
problem formulation for the fitting problem of a wide range of RL models that
appear frequently in scientific research applications, followed by a detailed
theoretical analysis of its convexity properties. Based on the theoretical
results, we introduce a novel solution method for the fitting problem of RL
models based on convex relaxation and optimization. Our method is then
evaluated in several simulated bandit environments to compare with some
benchmark methods that appear in the literature. Numerical results indicate
that our method achieves comparable performance to the state-of-the-art, while
significantly reducing computation time. We also provide an open-source Python
package for our proposed method to empower researchers to apply it in the
analysis of their datasets directly, without prior knowledge of convex
optimization.

</details>


### [35] [Simulation-Based Validation of an Integrated 4D/5D Digital-Twin Framework for Predictive Construction Control](https://arxiv.org/abs/2511.03684)
*Atena Khoshkonesh,Mohsen Mohammadagha,Navid Ebrahimi*

Main category: cs.CE

TL;DR: 研究提出集成4D/5D数字孪生框架，经项目验证可提升建设管理的准确性和效率，为建设管理提供实用途径。


<details>
  <summary>Details</summary>
Motivation: 美国建筑行业存在成本和进度偏差问题，传统方法有局限性。

Method: 提出集成4D/5D数字孪生框架，结合BIM、NLP成本映射、CV进度测量、贝叶斯概率CPM更新和DRL资源调配。

Result: 在项目中实现估算人工减少43%、加班减少6%、项目缓冲利用率30%，按时完成项目，还实现实时预测和成本进度对齐。

Conclusion: 集成AI分析、概率CPM和DRL可提高预测精度、透明度和控制弹性，建立了实用建设管理途径。

Abstract: Persistent cost and schedule deviations remain a major challenge in the U.S.
construction industry, revealing the limitations of deterministic CPM and
static document-based estimating. This study presents an integrated 4D/5D
digital-twin framework that couples Building Information Modeling (BIM) with
natural-language processing (NLP)-based cost mapping, computer-vision
(CV)-driven progress measurement, Bayesian probabilistic CPM updating, and
deep-reinforcement-learning (DRL) resource-leveling. A nine-month case
implementation on a Dallas-Fort Worth mid-rise project demonstrated measurable
gains in accuracy and efficiency: 43% reduction in estimating labor, 6%
reduction in overtime, and 30% project-buffer utilization, while maintaining an
on-time finish at 128 days within P50-P80 confidence bounds. The digital-twin
sandbox also enabled real-time "what-if" forecasting and traceable
cost-schedule alignment through a 5D knowledge graph. Findings confirm that
integrating AI-based analytics with probabilistic CPM and DRL enhances
forecasting precision, transparency, and control resilience. The validated
workflow establishes a practical pathway toward predictive, adaptive, and
auditable construction management.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [36] [GPU-Based Floating-point Adaptive Lossless Compression](https://arxiv.org/abs/2511.04140)
*Zheng Li,Weiyan Wang,Ruiyuan Li,Chao Chen,Xianlei Long,Linjiang Zheng,Quanqing Xu,Chuanhui Yang*

Main category: cs.DB

TL;DR: 论文提出GPU浮点自适应无损压缩框架Falcon，解决现有挑战，实验显示其压缩率、吞吐量等表现优异。


<details>
  <summary>Details</summary>
Motivation: 物联网和高性能计算领域产生大量浮点时间序列数据，设计高性能GPU无损压缩器面临异构数据移动瓶颈、精度转换复杂和异常稀疏性下降等挑战。

Method: 提出Falcon框架，引入轻量级异步管道隐藏I/O延迟，提出有理论保证的浮点转整数转换方法，设计自适应稀疏位平面无损编码策略。

Result: 在12个不同数据集上实验，压缩率比最先进CPU方法提高9.1%，压缩吞吐量比最快GPU竞品高2.43倍，解压缩吞吐量高2.4倍。

Conclusion: Falcon框架有效解决现有挑战，在压缩率和吞吐量等方面表现出色。

Abstract: Domains such as IoT (Internet of Things) and HPC (High Performance Computing)
generate a torrential influx of floating-point time-series data. Compressing
these data while preserving their absolute fidelity is critical, and leveraging
the massive parallelism of modern GPUs offers a path to unprecedented
throughput. Nevertheless, designing such a high-performance GPU-based lossless
compressor faces three key challenges: 1) heterogeneous data movement
bottlenecks, 2) precision-preserving conversion complexity, and 3)
anomaly-induced sparsity degradation. To address these challenges, this paper
proposes Falcon, a GPU-based Floating-point Adaptive Lossless COmpressioN
framework. Specifically, Falcon first introduces a lightweight asynchronous
pipeline, which hides the I/O latency during the data transmission between the
CPU and GPU. Then, we propose an accurate and fast float-to-integer
transformation method with theoretical guarantees, which eliminates the errors
caused by floating-point arithmetic. Moreover, we devise an adaptive sparse
bit-plane lossless encoding strategy, which reduces the sparsity caused by
outliers. Extensive experiments on 12 diverse datasets show that our
compression ratio improves by 9.1% over the most advanced CPU-based method,
with compression throughput 2.43X higher and decompression throughput 2.4X
higher than the fastest GPU-based competitors, respectively.

</details>


### [37] [EntroGD: Efficient Compression and Accurate Direct Analytics on Compressed Data](https://arxiv.org/abs/2511.04148)
*Xiaobo Zhao,Daniel E. Lucani*

Main category: cs.DB

TL;DR: 提出EntroGD框架降低广义去重算法复杂度，在多数据集上有良好表现。


<details>
  <summary>Details</summary>
Motivation: 现有广义去重（GD）算法处理高维数据时面临可扩展性挑战，复杂度高、性能损失大。

Method: EntroGD采用两步流程，先生成浓缩样本，再进行熵引导的位选择。

Result: 在18个不同类型和维度的数据集上，压缩性能与GD和通用压缩器相当，减少配置时间达53.5倍，加速聚类达31.6倍，精度损失可忽略。

Conclusion: EntroGD为在压缩数据上直接进行分析提供了高效可扩展的解决方案。

Abstract: Generalized Deduplication (GD) enables lossless compression with direct
analytics on compressed data by dividing data into \emph{bases} and
\emph{deviations} and performing dictionary encoding on the former. However, GD
algorithms face scalability challenges for high-dimensional data. For example,
the GreedyGD algorithm relies on an iterative bit-selection process across
$d$-dimensional data resulting in $O(nd^2)$ complexity for $n$ data rows to
select bits to be used as bases and deviations. Although the $n$ data rows can
be reduced during training at the expense of performance, highly dimensional
data still experiences a marked loss in performance. This paper introduces
EntroGD, an entropy-guided GD framework that reduces complexity of the
bit-selection algorithm to $O(nd)$. EntroGD operates considers a two-step
process. First, it generates condensed samples to preserve analytic fidelity.
Second, it applies entropy-guided bit selection to maximize compression
efficiency. Across 18 datasets of varying types and dimensionalities, EntroGD
achieves compression performance comparable to GD-based and universal
compressors, while reducing configuration time by up to 53.5$\times$ over
GreedyGD and accelerating clustering by up to 31.6$\times$ over the original
data with negligible accuracy loss by performing analytics on the condensed
samples, which are much fewer than original samples. Thus, EntroGD provides an
efficient and scalable solution to performing analytics directly on compressed
data.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [38] [OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms](https://arxiv.org/abs/2511.03866)
*Arijit Bhattacharjee,Ali TehraniJamsaz,Le Chen,Niranjan Hasabnis,Mihai Capota,Nesreen Ahmed,Ali Jannesari*

Main category: cs.DC

TL;DR: 本文介绍用于将C++代码转换为OpenMP的OMPILOT模型，采用自定义预训练目标和多种学习策略，还提出OMPBLEU评估指标。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型在代码翻译上的进展，开发能有效实现共享内存并行化的C++到OpenMP的代码翻译模型。

Method: 引入OMPILOT模型，利用自定义预训练目标，结合无监督和有监督学习策略，在函数级别操作；提出OMPBLEU评估指标。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Recent advances in large language models (LLMs) have significantly
accelerated progress in code translation, enabling more accurate and efficient
transformation across programming languages. While originally developed for
natural language processing, LLMs have shown strong capabilities in modeling
programming language syntax and semantics, outperforming traditional rule-based
systems in both accuracy and flexibility. These models have streamlined
cross-language conversion, reduced development overhead, and accelerated legacy
code migration. In this paper, we introduce OMPILOT, a novel domain-specific
encoder-decoder transformer tailored for translating C++ code into OpenMP,
enabling effective shared-memory parallelization. OMPILOT leverages custom
pre-training objectives that incorporate the semantics of parallel constructs
and combines both unsupervised and supervised learning strategies to improve
code translation robustness. Unlike previous work that focused primarily on
loop-level transformations, OMPILOT operates at the function level to capture a
wider semantic context. To evaluate our approach, we propose OMPBLEU, a novel
composite metric specifically crafted to assess the correctness and quality of
OpenMP parallel constructs, addressing limitations in conventional translation
metrics.

</details>


### [39] [Stochastic Modeling for Energy-Efficient Edge Infrastructure](https://arxiv.org/abs/2511.03941)
*Fabio Diniz Rossi*

Main category: cs.DC

TL;DR: 本文提出用马尔可夫链随机建模分析边缘计算电源状态转换，通过蒙特卡洛模拟验证模型，表明AI驱动预测性电源管理可提升能源效率。


<details>
  <summary>Details</summary>
Motivation: 边缘计算虽能实现实时应用低延迟处理，但因设备分布式特性和能量资源有限，在电源管理方面存在挑战。

Method: 采用马尔可夫链进行随机建模分析电源状态转换，用蒙特卡洛模拟验证模型，并进行敏感性分析。

Result: 模型验证显示理论与实证结果高度一致，敏感性分析表明预测性缩放可减少不必要转换并提高系统响应性，实验结果显示AI电源管理能优化工作负载分布、减少设备能耗差异。

Conclusion: 基于AI的电源管理策略通过预测工作负载需求和优化状态转换，显著提高能源效率。

Abstract: Edge Computing enables low-latency processing for real-time applications but
introduces challenges in power management due to the distributed nature of edge
devices and their limited energy resources. This paper proposes a stochastic
modeling approach using Markov Chains to analyze power state transitions in
Edge Computing. By deriving steady-state probabilities and evaluating energy
consumption, we demonstrate the benefits of AI-driven predictive power scaling
over conventional reactive methods. Monte Carlo simulations validate the model,
showing strong alignment between theoretical and empirical results. Sensitivity
analysis highlights how varying transition probabilities affect power
efficiency, confirming that predictive scaling minimizes unnecessary
transitions and improves overall system responsiveness. Our findings suggest
that AI-based power management strategies significantly enhance energy
efficiency by anticipating workload demands and optimizing state transitions.
Experimental results indicate that AI-based power management optimizes workload
distribution across heterogeneous edge nodes, reducing energy consumption
disparities between devices, improving overall efficiency, and enhancing
adaptive power coordination in multi-node environments.

</details>


### [40] [Parallel Spawning Strategies for Dynamic-Aware MPI Applications](https://arxiv.org/abs/2511.04268)
*Iker Martín-Álvarez,José I. Aliaga,Maribel Castillo,Sergio Iserte*

Main category: cs.DC

TL;DR: 提出新并行生成策略克服MPI应用现有方法局限，减少执行与收缩成本，在多种系统验证可用。


<details>
  <summary>Details</summary>
Motivation: 动态资源管理中应用的可扩展性虽有益但重配置成本高，现有MPI应用方法存在不足，需降低成本。

Method: 提出新的并行生成策略，让所有进程在重新分配前协作生成。

Result: 保持有竞争力的扩展时间，开销至多1.25倍，收缩操作成本至少降低20倍，在同构和异构系统验证。

Conclusion: 该策略能克服现有方法局限，降低成本，可应用于共享资源环境。

Abstract: Dynamic resource management is an increasingly important capability of High
Performance Computing systems, as it enables jobs to adjust their resource
allocation at runtime. This capability has been shown to reduce workload
makespan, substantially decrease job waiting times and improve overall system
utilization. In this context, malleability refers to the ability of
applications to adapt to new resource allocations during execution. Although
beneficial, malleability incurs significant reconfiguration costs, making the
reduction of these costs an important research topic.
  Some existing methods for MPI applications respawn the entire application,
which is an expensive solution that avoids the reuse of original processes.
Other MPI methods reuse them, but fail to fully release unneeded processes when
shrinking, since some ranks within the same communicator remain active across
nodes, preventing the application from returning those nodes to the system.
This work overcomes both limitations by proposing a novel parallel spawning
strategy, in which all processes cooperate in spawning before redistribution,
thereby reducing execution time. Additionally, it removes shrinkage
limitations, allowing better adaptation of parallel systems to workload and
reducing their makespan. As a result, it preserves competitive expansion times
with at most a $1.25\times$ overhead, while enabling fast shrink operations
that reduce their cost by at least $20\times$. This strategy has been validated
on both homogeneous and heterogeneous systems and can also be applied in
shared-resource environments.

</details>


### [41] [Enabling Dynamic Sparsity in Quantized LLM Inference](https://arxiv.org/abs/2511.04477)
*Rongxiang Wang,Kangyuan Shu,Felix Xiaozhu Lin*

Main category: cs.DC

TL;DR: 文章提出技术实现低比特量化下动态稀疏推理，在多模型和硬件配置上提升解码吞吐量且保持精度。


<details>
  <summary>Details</summary>
Motivation: 在终端设备部署大语言模型有诸多好处，但移动和桌面GPU内存和计算能力有限，且动态稀疏性与分组量化不兼容，需解决二者共存问题。

Method: 提出一组技术，包括锯齿形量化布局、专用GEMV内核和紧凑运行时机制。

Result: 在多个模型规模和硬件配置下，解码吞吐量最高提升1.55倍，且精度与密集量化推理相当。

Conclusion: 结构化稀疏性和量化可以在商用GPU上有效共存。

Abstract: Deploying large language models (LLMs) on end-user devices is gaining
importance due to benefits in responsiveness, privacy, and operational cost.
Yet the limited memory and compute capability of mobile and desktop GPUs make
efficient execution difficult. Recent observations suggest that the internal
activations of LLMs are often dynamically sparse, meaning that for each input,
only part of the network contributes significantly to the output. Such sparsity
could reduce computation, but it interacts poorly with group-wise quantization,
which remains the dominant approach for fitting LLMs onto resource-constrained
hardware. To reconcile these two properties, this study proposes a set of
techniques that realize dynamic sparse inference under low-bit quantization.
The method features: (1) a zigzag-patterned quantization layout that organizes
weights in a way consistent with activation sparsity and improves GPU memory
locality; (2) a specialized GEMV kernel designed for this layout to fully
utilize parallel compute units; and (3) a compact runtime mechanism that
gathers sparse indices with minimal overhead. Across several model scales and
hardware configurations, the approach achieves up to 1.55x faster decoding
throughput while maintaining accuracy comparable to dense quantized inference,
showing that structured sparsity and quantization can effectively coexist on
commodity GPUs.

</details>


### [42] [A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting Systems](https://arxiv.org/abs/2511.04523)
*Silvia Bonomi,Giovanni Farina,Roy Friedman,Eviatar B. Procaccia,Sebastien Tixeuil*

Main category: cs.DC

TL;DR: 本文提出基于MAPE - K架构的自保护分布式系统及新的概率移动拜占庭故障模型，分析节点状态变化时间并给出模拟结果。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统面临安全威胁，现有基于拜占庭进程的方法在反映现实场景时有局限性。

Method: 提出基于MAPE - K架构的自保护分布式系统，引入新的概率移动拜占庭故障模型到分析组件，数学分析节点状态变化时间。

Result: 通过数学分析得出拜占庭节点数量越过阈值或系统自我恢复到安全状态的时间，给出模拟结果展示系统行为。

Conclusion: 新模型能捕捉攻击动态，可用于驱动自保护和重新配置策略。

Abstract: Modern distributed systems face growing security threats, as attackers
continuously enhance their skills and vulnerabilities span across the entire
system stack, from hardware to the application layer. In the system design
phase, fault tolerance techniques can be employed to safeguard systems. From a
theoretical perspective, an attacker attempting to compromise a system can be
abstracted by considering the presence of Byzantine processes in the system.
Although this approach enhances the resilience of the distributed system, it
introduces certain limitations regarding the accuracy of the model in
reflecting real-world scenarios. In this paper, we consider a self-protecting
distributed system based on the \emph{Monitoring-Analyse-Plan-Execute over a
shared Knowledge} (MAPE-K) architecture, and we propose a new probabilistic
Mobile Byzantine Failure (MBF) that can be plugged into the Analysis component.
Our new model captures the dynamics of evolving attacks and can be used to
drive the self-protection and reconfiguration strategy. We analyze
mathematically the time that it takes until the number of Byzantine nodes
crosses given thresholds, or for the system to self-recover back into a safe
state, depending on the rates of Byzantine infection spreading \emph{vs.} the
rate of self-recovery. We also provide simulation results that illustrate the
behavior of the system under such assumptions.

</details>


### [43] [Resolving Conflicts with Grace: Dynamically Concurrent Universality](https://arxiv.org/abs/2511.04631)
*Petr Kuznetsov,Nathan Josia Schrodt*

Main category: cs.DC

TL;DR: 本文定义动态并发概念并给出动态并发通用构造，以解决分布式计算中同步对可扩展性的阻碍。


<details>
  <summary>Details</summary>
Motivation: 同步是分布式计算可扩展性的主要障碍，理想情况是动态检测冲突，而常见的并发操作仅在某些罕见状态下冲突。

Method: 定义动态并发概念，即操作仅在当前系统状态下必须与并发操作仲裁时才使用强同步原语，并给出动态并发通用构造。

Result: 未提及具体结果

Conclusion: 未提及明确结论

Abstract: Synchronization is the major obstacle to scalability in distributed
computing. Concurrent operations on the shared data engage in synchronization
when they encounter a \emph{conflict}, i.e., their effects depend on the order
in which they are applied. Ideally, one would like to detect conflicts in a
\emph{dynamic} manner, i.e., adjusting to the current system state. Indeed, it
is very common that two concurrent operations conflict only in some rarely
occurring states. In this paper, we define the notion of \emph{dynamic
concurrency}: an operation employs strong synchronization primitives only if it
\emph{has} to arbitrate with concurrent operations, given the current system
state. We then present a dynamically concurrent universal construction.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [44] [Attractors Is All You Need: Parity Games In Polynomial Time](https://arxiv.org/abs/2511.03752)
*Rick van der Heijden*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper provides a polynomial-time algorithm for solving parity games that
runs in $\mathcal{O}(n^{2}\cdot(n + m))$ time-ending a search that has taken
decades. Unlike previous attractor-based algorithms, the presented algorithm
only removes regions with a determined winner. The paper introduces a new type
of attractor that can guarantee finding the minimal dominion of a parity game.
The attractor runs in polynomial time and can peel the graph empty.

</details>


### [45] [Multi-Pass Streaming Lower Bounds for Uniformity Testing](https://arxiv.org/abs/2511.03960)
*Qian Li,Xin Lyu*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We prove multi-pass streaming lower bounds for uniformity testing over a
domain of size $2m$. The tester receives a stream of $n$ i.i.d. samples and
must distinguish (i) the uniform distribution on $[2m]$ from (ii) a
Paninski-style planted distribution in which, for each pair $(2i-1,2i)$, the
probabilities are biased left or right by $\epsilon/2m$. We show that any
$\ell$-pass streaming algorithm using space $s$ and achieving constant
advantage must satisfy the tradeoff $sn\ell=\tilde{\Omega}(m/\epsilon^2)$. This
extends the one-pass lower bound of Diakonikolas, Gouleakis, Kane, and Rao
(2019) to multiple passes.
  Our proof has two components. First, we develop a hybrid argument, inspired
by Dinur (2020), that reduces streaming to two-player communication problems.
This reduction relies on a new perspective on hardness: we identify the source
of hardness as uncertainty in the bias directions, rather than the collision
locations. Second, we prove a strong lower bound for a basic two-player
communication task, in which Alice and Bob must decide whether two random sign
vectors $Y^a,Y^b\in\{\pm 1\}^m$ are independent or identical, yet they cannot
observe the signs directly--only noisy local views of each coordinate. Our
techniques may be of independent use for other streaming problems with
stochastic inputs.

</details>


### [46] [HART: A Hybrid Addressing Scheme for Self-Balancing Binary Search Trees in Phase Change Memory (PCM)](https://arxiv.org/abs/2511.03994)
*Mahek Desai,Apoorva Rumale,Marjan Asadinia*

Main category: cs.DS

TL;DR: DRAM等内存技术接近扩展极限，PCM受关注但现有算法忽略其特性。本文提出HART混合寻址方案优化PCM，实验表明其能提升性能且无大量计算开销。


<details>
  <summary>Details</summary>
Motivation: DRAM等技术接近扩展极限，PCM有优势但现有内存密集型算法忽略其特性，导致性能下降。

Method: 提出HART混合寻址方案，结合DFATGray码寻址和线性寻址。

Result: 在PCM - aware AVL树实验中，减少位翻转，提升耐久性、寿命，降低写能耗和延迟。

Conclusion: HART是大数据应用的高效解决方案，能优化PCM特性且无大量计算开销。

Abstract: As DRAM and other transistor-based memory technologies approach their
scalability limits, alternative storage solutions like Phase-Change Memory
(PCM) are gaining attention for their scalability, fast access times, and zero
leakage power. However, current memory-intensive algorithms, especially those
used in big data systems, often overlook PCM's endurance limitations (10^6 to
10^8 writes before degradation) and write asymmetry. Self-balancing binary
search trees (BSTs), which are widely used for large-scale data management,
were developed without considering PCM's unique properties, leading to
potential performance degradation. This paper introduces HART, a novel hybrid
addressing scheme for self-balancing BSTs, designed to optimize PCM's
characteristics. By combining DFATGray code addressing for deeper nodes with
linear addressing for shallower nodes, HART balances reduced bit flips during
frequent rotations at deeper levels with computational simplicity at shallow
levels. Experimental results on PCM-aware AVL trees demonstrate significant
improvements in performance, with a reduction in bit flips leading to enhanced
endurance, increased lifetime, and lower write energy and latency. Notably,
these benefits are achieved without imposing substantial computational
overhead, making HART an efficient solution for big data applications.

</details>


### [47] [Depth-13 Sorting Networks for 28 Channels](https://arxiv.org/abs/2511.04107)
*Chengu Wang*

Main category: cs.DS

TL;DR: 本文为27和28通道的排序网络建立了新的深度上限，从14提高到13。


<details>
  <summary>Details</summary>
Motivation: 改进27和28通道排序网络的深度上限。

Method: 通过组合16和12通道网络的高质量前缀，一次贪心扩展一个比较器，并使用SAT求解器完成剩余层，以反射对称的方式构建28通道网络。

Result: 将27和28通道排序网络的深度上限从14提高到13。

Conclusion: 成功建立了更优的27和28通道排序网络深度上限。

Abstract: We establish new depth upper bounds for sorting networks on 27 and 28
channels, improving the previous best bound of 14 to 13. Our 28-channel network
is constructed with reflectional symmetry by combining high-quality prefixes of
16- and 12-channel networks, extending them greedily one comparator at a time,
and using a SAT solver to complete the remaining layers.

</details>


### [48] [Counting Patterns in Degenerate Graphs in Constant Space](https://arxiv.org/abs/2511.04258)
*Balagopal Komarath,Anant Kumar,Akash Pareek*

Main category: cs.DS

TL;DR: 本文研究从模式图到n顶点、d - 退化图的同态、子图同构和诱导子图同构计数问题，引入DAG树深度参数，给出常数空间算法，还在DAG树宽上取得比前人更快的计数算法。


<details>
  <summary>Details</summary>
Motivation: 前人使用DAG树宽解决相关计数问题，本文希望引入新参数并得到更优算法。

Method: 引入DAG树深度参数，采用分治算法；同时对DAG树宽继续研究。

Result: 得到常数空间的稀疏模式图子图同构计数算法、DAG树深度至多为2的图的诱导子式表征、9顶点模式图诱导子图的O(n^3)时间常数空间计数算法，比Bressan更快的计数算法，11顶点模式图诱导子图的二次时间计数算法。

Conclusion: DAG树深度可带来常数空间的高效分治算法，在DAG树宽上也能得到比前人更快的算法。

Abstract: For an arbitrary, fixed graph (pattern graph), we study the algorithmic
complexity of counting homomorphisms, subgraph isomorphisms, and induced
subgraph isomorphisms from the pattern graph to $n$-vertex, $d$-degenerate
graphs as input. Recent work by Bressan (Algorithmica, 2021) has shown that
this problem has efficient dynamic programming algorithms using a graph
parameter called DAG treewidth. Bressan used DAG treewidth to design a fast
algorithm for counting homomorphisms, subgraph isomorphisms, and induced
subgraph isomorphisms that use polynomial space. Bera, Gishboliner, Levanzov,
Seshadhri, and Shapira (SODA, 2021) provided a characterization of graphs with
DAG treewidth one.
  In this paper, we introduce a new graph parameter called DAG treedepth and
show that it yields efficient divide and conquer algorithms that use only
constant space (in the unit-cost RAM model). Specifically, we show:
  An algorithm for counting subgraphs isomorphic to sparse pattern graphs using
only constant space.
  We derive an induced minor-based characterization for graphs of DAG treedepth
up to two.
  For pattern graphs upto nine vertices, the induced subgraphs can be counted
in $O(n^3)$ time using constant space.
  An algorithm for counting induced subgraphs that matches the running time
given by Bressan but only uses constant space.
  Apart from the DAG treedepth result, we also focus on DAG treewidth. For DAG
treewidth, we show that we can count homomorphisms, subgraph isomorphisms, and
induced subgraph isomorphisms faster than Bressan's algorithm (2021). We
further show that for all pattern graphs up to 11 vertices, we can count
induced subgraphs in quadratic time.

</details>


### [49] [Estimating Hitting Times Locally At Scale](https://arxiv.org/abs/2511.04343)
*Themistoklis Haris,Fabian Spaeh,Spyros Dragazis,Charalampos Tsourakakis*

Main category: cs.DS

TL;DR: 本文开发局部算法估计图中顶点对的击中时间，克服了先前全局方法的可扩展性问题，给出算法上下界，揭示与分布测试的联系并实验验证。


<details>
  <summary>Details</summary>
Motivation: 先前全局方法在估计图中顶点对击中时间存在可扩展性问题，需要开发不访问全量图的局部算法。

Method: 一是利用从两个顶点出发的独立随机游走相遇时间截断击中时间计算，通过Kronecker积图和马尔可夫链切尔诺夫界分析；二是扩展已有工作，引入谱截断技术的新变体处理击中时间的不对称性。

Result: 给出算法的上界和紧渐近下界，揭示了击中时间估计与分布测试的联系。

Conclusion: 所提出的局部算法能有效估计图中顶点对的击中时间，在真实和合成数据上的实验验证了算法有效性。

Abstract: Hitting times provide a fundamental measure of distance in random processes,
quantifying the expected number of steps for a random walk starting at node $u$
to reach node $v$. They have broad applications across domains such as network
centrality analysis, ranking and recommendation systems, and epidemiology. In
this work, we develop local algorithms for estimating hitting times between a
pair of vertices $u,v$ without accessing the full graph, overcoming scalability
issues of prior global methods. Our first algorithm uses the key insight that
hitting time computations can be truncated at the meeting time of two
independent random walks from $u$ and $v$. This leads to an efficient estimator
analyzed via the Kronecker product graph and Markov Chain Chernoff bounds. We
also present an algorithm extending the work of [Peng et al.; KDD 2021], that
introduces a novel adaptation of the spectral cutoff technique to account for
the asymmetry of hitting times. This adaptation captures the directionality of
the underlying random walk and requires non-trivial modifications to ensure
accuracy and efficiency. In addition to the algorithmic upper bounds, we also
provide tight asymptotic lower bounds. We also reveal a connection between
hitting time estimation and distribution testing, and validate our algorithms
using experiments on both real and synthetic data.

</details>


### [50] [A Polynomial-Time Algorithm for the Next-to-Shortest Path Problem on Positively Weighted Directed Graphs](https://arxiv.org/abs/2511.04345)
*Kuowen Chen,Nicole Wein,Yiran Zhang*

Main category: cs.DS

TL;DR: 本文解决了近30年的公开问题，为正边权有向图的次短路径问题提供了算法。


<details>
  <summary>Details</summary>
Motivation: 1996年提出的次短路径问题，有向非负边权图该问题被证明是NP完全的，正边权情况未解决，本文旨在解决此公开问题。

Method: 文中未明确提及具体方法。

Result: 为正边权有向图的次短路径问题提供了算法。

Conclusion: 解决了近30年关于正边权有向图次短路径问题的公开问题。

Abstract: Given a graph and a pair of terminals $s$, $t$, the next-to-shortest path
problem asks for an $s\!\to \!t$ (simple) path that is shortest among all not
shortest $s\!\to \!t$ paths (if one exists). This problem was introduced in
1996, and soon after was shown to be NP-complete for directed graphs with
non-negative edge weights, leaving open the case of positive edge weights.
Subsequent work investigated this open question, and developed polynomial-time
algorithms for the cases of undirected graphs and planar directed graphs. In
this work, we resolve this nearly 30-year-old open problem by providing an
algorithm for the next-to-shortest path problem on directed graphs with
positive edge weights.

</details>


### [51] [Free-order secretary for two-sided independence systems](https://arxiv.org/abs/2511.04390)
*Kristóf Bérczi,Vasilis Livanos,José A. Soto,Victor Verdugo*

Main category: cs.DS

TL;DR: 引入二分图框架统一扩展多个已知公式，研究自由顺序和代理到达模型，设计竞争算法并将结果扩展到多项目选择情况。


<details>
  <summary>Details</summary>
Motivation: 解决在线优化中的拟阵秘书问题，处理组合约束下的顺序决策。

Method: 引入二分图框架，利用核心引理，识别结构属性引入k - 增长系统，扩展核心引理。

Result: 为k - 拟阵相交、k - 增长系统设计了竞争算法，在不同模型和情况下取得相应竞争比。

Conclusion: 可将研究结果扩展到多项目选择情况，对基本情况能得到常数竞争算法。

Abstract: The Matroid Secretary Problem is a central question in online optimization,
modeling sequential decision-making under combinatorial constraints. We
introduce a bipartite graph framework that unifies and extends several known
formulations, including the bipartite matching, matroid intersection, and
random-order matroid secretary problems. In this model, elements form a
bipartite graph between agents and items, and the objective is to select a
matching that satisfies feasibility constraints on both sides, given by two
independence systems.
  We study the free-order setting, where the algorithm may adaptively choose
the next element to reveal. For $k$-matroid intersection, we leverage a core
lemma by (Feldman, Svensson and Zenklusen, 2022) to design an
$\Omega(1/k^2)$-competitive algorithm, extending known results for single
matroids. Building on this, we identify the structural property underlying our
approach and introduce $k$-growth systems. We establish a generalized core
lemma for $k$-growth systems, showing that a suitably defined set of critical
elements retains a $\Omega(1/k^2)$ fraction of the optimal weight. Using this
lemma, we extend our $\Omega(1/k^2)$-competitive algorithm to $k$-growth
systems for the edge-arrival model.
  We then study the agent-arrival model, which presents unique challenges to
our framework. We extend the core lemma to this model and then apply it to
obtain an $\Omega(\beta/k^2)$-competitive algorithm for $k$-growth systems,
where $\beta$ denotes the competitiveness of a special type of order-oblivious
algorithm for the item-side constraint. Finally, we relax the matching
assumption and extend our results to the case of multiple item selection, where
agents have individual independence systems coupled by a global item-side
constraint. We obtain constant-competitive algorithms for fundamental cases
such as partition matroids and $k$-matching constraints.

</details>


### [52] [Online Algorithms for Repeated Optimal Stopping: Achieving Both Competitive Ratio and Regret Bounds](https://arxiv.org/abs/2511.04484)
*Tsubasa Harada,Yasushi Kawase,Hanna Sumita*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the repeated optimal stopping problem, which generalizes the
classical optimal stopping problem with an unknown distribution to a setting
where the same problem is solved repeatedly over $T$ rounds. In this framework,
we aim to design algorithms that guarantee a competitive ratio in each round
while also achieving sublinear regret across all rounds.
  Our primary contribution is a general algorithmic framework that achieves
these objectives simultaneously for a wide array of repeated optimal stopping
problems. The core idea is to dynamically select an algorithm for each round,
choosing between two candidates: (1) an empirically optimal algorithm derived
from the history of observations, and (2) a sample-based algorithm with a
proven competitive ratio guarantee. Based on this approach, we design an
algorithm that performs no worse than the baseline sample-based algorithm in
every round, while ensuring that the total regret is bounded by
$\tilde{O}(\sqrt{T})$.
  We demonstrate the broad applicability of our framework to canonical
problems, including the prophet inequality, the secretary problem, and their
variants under adversarial, random, and i.i.d. input models. For example, for
the repeated prophet inequality problem, our method achieves a
$1/2$-competitive ratio from the second round on and an $\tilde{O}(\sqrt{T})$
regret. Furthermore, we establish a regret lower bound of $\Omega(\sqrt{T})$
even in the i.i.d. model, confirming that our algorithm's performance is almost
optimal with respect to the number of rounds.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [53] [On the Existence of Fair Allocations for Goods and Chores under Dissimilar Preferences](https://arxiv.org/abs/2511.03810)
*Egor Gagushin,Marios Mertzanidis,Alexandros Psomas*

Main category: cs.GT

TL;DR: 解决Gorantla等人提出的开放性问题，推导任意数量组和物品类型下μ的显式上界。


<details>
  <summary>Details</summary>
Motivation: Gorantla等人的证明非构造性，仅给出特定情况μ的显式上界，本文要解决其提出的开放性问题。

Method: 引入更简单强大的技术。

Result: 得到任意数量组和物品类型下μ的显式上界，该技术适用于不可分割商品、杂务和连续域。

Conclusion: 该技术解决了开放性问题，在相关公平分配场景有新成果。

Abstract: We study the fundamental problem of fairly allocating a multiset
$\mathcal{M}$ of $t$ types of indivisible items among $d$ groups of agents,
where all agents within a group have identical additive valuations. Gorantla et
al. [GMV23] showed that for every such instance, there exists a finite number
$\mu$ such that, if each item type appears at least $\mu$ times, an envy-free
allocation exists. Their proof is non-constructive and only provides explicit
upper bounds on $\mu$ for the cases of two groups ($d=2$) or two item types
($t=2$).
  In this work, we resolve one of the main open questions posed by Gorantla et
al. [GMV23] by deriving explicit upper bounds on $\mu$ that hold for arbitrary
numbers of groups and item types. We introduce a significantly simpler, yet
powerful technique that not only yields constructive guarantees for indivisible
goods but also extends naturally to chores and continuous domains, leading to
new results in related fair division settings such as cake cutting.

</details>


### [54] [The Complexity of Equilibrium Refinements in Potential Games](https://arxiv.org/abs/2511.03968)
*Ioannis Anagnostides,Maria-Florina Balcan,Kiriaki Fragkia,Tuomas Sandholm,Emanuel Tewolde,Brian Hu Zhang*

Main category: cs.GT

TL;DR: 本文解决了潜在博弈中计算均衡精炼的复杂度问题，给出不同类型博弈的复杂度结果及计算方法。


<details>
  <summary>Details</summary>
Motivation: 解决潜在博弈中计算均衡精炼复杂度这一未决问题。

Method: 对不同博弈表示（如扩展式博弈、多面体博弈等）和更结构化的博弈类（对称网络拥塞和对称拟阵拥塞博弈等）进行分析，采用理论证明方法。

Result: 确定纯策略完美均衡计算的复杂度，解决正规形式恰当均衡的复杂度问题，证明某些博弈中完美均衡和纳什均衡最佳响应路径长度有指数分离，给出混合策略相关复杂度结果。

Conclusion: 明确了潜在博弈中不同类型均衡精炼计算的复杂度，为算法博弈论研究提供重要补充。

Abstract: The complexity of computing equilibrium refinements has been at the forefront
of algorithmic game theory research, but it has remained open in the seminal
class of potential games; we close this fundamental gap in this paper.
  We first establish that computing a pure-strategy perfect equilibrium is
$\mathsf{PLS}$-complete under different game representations -- including
extensive-form games and general polytope games, thereby being polynomial-time
equivalent to pure Nash equilibria. For normal-form proper equilibria, our main
result is that a perturbed (proper) best response can be computed efficiently
in extensive-form games. As a byproduct, we establish
$\mathsf{FIXP}_a$-completeness of normal-form proper equilibria in
extensive-form games, resolving a long-standing open problem. In stark
contrast, we show that computing a normal-form proper equilibrium in polytope
potential games is both $\mathsf{NP}$-hard and $\mathsf{coNP}$-hard.
  We next turn to more structured classes of games, namely symmetric network
congestion and symmetric matroid congestion games. For both classes, we show
that a perfect pure-strategy equilibrium can be computed in polynomial time,
strengthening the existing results for pure Nash equilibria. On the other hand,
we establish that, for a certain class of potential games, there is an
exponential separation in the length of the best-response path between perfect
and Nash equilibria.
  Finally, for mixed strategies, we prove that computing a point geometrically
near a perfect equilibrium requires a doubly exponentially small perturbation
even in $3$-player potential games in normal form. On the flip side, in the
special case of polymatrix potential games, we show that equilibrium
refinements are amenable to perturbed gradient descent dynamics, thereby
belonging to the complexity class $\mathsf{CLS}$.

</details>


### [55] [Fraud-Proof Revenue Division on Subscription Platforms](https://arxiv.org/abs/2511.04465)
*Abheek Ghosh,Tzeh Yuan Neoh,Nicholas Teh,Giannis Tyrovolas*

Main category: cs.GT

TL;DR: 研究基于订阅的平台模型，探索抗操纵的收入分配机制，提出新规则ScaledUserProp并通过实验验证其优势。


<details>
  <summary>Details</summary>
Motivation: 现有检测欺诈的方法与不良行为者处于军备竞赛，需探索能从本质上抑制操纵的收入分配机制。

Method: 形式化三种抗操纵公理，检验现有规则，提出新规则ScaledUserProp。

Result: 常用机制无法防欺诈且使操纵检测计算困难，ScaledUserProp满足三种抗操纵公理。

Conclusion: 实验表明ScaledUserProp比现有规则更公平，是更好的选择。

Abstract: We study a model of subscription-based platforms where users pay a fixed fee
for unlimited access to content, and creators receive a share of the revenue.
Existing approaches to detecting fraud predominantly rely on machine learning
methods, engaging in an ongoing arms race with bad actors. We explore revenue
division mechanisms that inherently disincentivize manipulation. We formalize
three types of manipulation-resistance axioms and examine which existing rules
satisfy these. We show that a mechanism widely used by streaming platforms, not
only fails to prevent fraud, but also makes detecting manipulation
computationally intractable. We also introduce a novel rule, ScaledUserProp,
that satisfies all three manipulation-resistance axioms. Finally, experiments
with both real-world and synthetic streaming data support ScaledUserProp as a
fairer alternative compared to existing rules.

</details>


### [56] [Fisher Meets Lindahl: A Unified Duality Framework for Market Equilibrium](https://arxiv.org/abs/2511.04572)
*Yixin Tao,Weiqiang Zheng*

Main category: cs.GT

TL;DR: 本文提出市场均衡的统一对偶框架，建立公共物品市场的林达尔均衡与对偶费舍尔市场均衡的对应关系，并应用该框架解决林达尔均衡计算和动态问题，获得费舍尔市场均衡新见解。


<details>
  <summary>Details</summary>
Motivation: 费舍尔市场均衡研究充分，但林达尔均衡理论基础欠发达，需建立统一框架研究。

Method: 提出统一对偶框架，通过交换分配和价格角色建立两种均衡对应关系，基于间接效用定义对偶效用。

Result: 分析林达尔均衡福利性质，得到新市场动态，将比例响应动态扩展到总互补效用市场，提出避免“极点”问题的私有事务规划。

Conclusion: 统一对偶框架可解决林达尔均衡计算和动态问题，为费舍尔市场均衡带来新发展。

Abstract: The Fisher market equilibrium for private goods and the Lindahl equilibrium
for public goods are classic and fundamental solution concepts for market
equilibria. While Fisher market equilibria have been well-studied, the
theoretical foundations for Lindahl equilibria remain substantially
underdeveloped.
  In this work, we propose a unified duality framework for market equilibria.
We show that Lindahl equilibria of a public goods market correspond to Fisher
market equilibria in a dual Fisher market with dual utilities, and vice versa.
The dual utility is based on the indirect utility, and the correspondence
between the two equilibria works by exchanging the roles of allocations and
prices.
  Using the duality framework, we address the gaps concerning the computation
and dynamics for Lindahl equilibria and obtain new insights and developments
for Fisher market equilibria. First, we leverage this duality to analyze
welfare properties of Lindahl equilibria. For concave homogeneous utilities, we
prove that a Lindahl equilibrium maximizes Nash Social Welfare (NSW). For
concave non-homogeneous utilities, we show that a Lindahl equilibrium achieves
$(1/e)^{1/e}$ approximation to the optimal NSW, and the approximation ratio is
tight. Second, we apply the duality framework to market dynamics, including
proportional response dynamics (PRD) and t\^atonnement. We obtain new market
dynamics for the Lindahl equilibria from market dynamics in the dual Fisher
market. We also use duality to extend PRD to markets with total complements
utilities, the dual class of gross substitutes utilities. Finally, we apply the
duality framework to markets with chores. We propose a program for private
chores for general convex homogeneous disutilities that avoids the "poles"
issue, whose KKT points correspond to Fisher market equilibria. We also
initiate the study of the Lindahl equilibrium for public chores.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [57] [Caption Injection for Optimization in Generative Search Engine](https://arxiv.org/abs/2511.04080)
*Xiaolu Chen,Yong Liao*

Main category: cs.IR

TL;DR: 本文提出首个多模态G - SEO方法Caption Injection，在MRAMG基准上评估，结果显示其在G - Eval指标上显著优于仅文本的G - SEO基线。


<details>
  <summary>Details</summary>
Motivation: 现有G - SEO方法局限于文本优化，未充分利用多模态数据，需新方法增强生成搜索场景中内容的主观可见性。

Method: 提出Caption Injection方法，从图像中提取字幕并注入文本内容，整合视觉语义。

Result: 在MRAMG基准的单模态和多模态设置下评估，Caption Injection在G - Eval指标上显著优于仅文本的G - SEO基线。

Conclusion: 多模态集成在G - SEO中是必要且有效的，可提高用户感知的内容可见性。

Abstract: Generative Search Engines (GSEs) leverage Retrieval-Augmented Generation
(RAG) techniques and Large Language Models (LLMs) to integrate multi-source
information and provide users with accurate and comprehensive responses. Unlike
traditional search engines that present results in ranked lists, GSEs shift
users' attention from sequential browsing to content-driven subjective
perception, driving a paradigm shift in information retrieval. In this context,
enhancing the subjective visibility of content through Generative Search Engine
Optimization (G-SEO) methods has emerged as a new research focus. With the
rapid advancement of Multimodal Retrieval-Augmented Generation (MRAG)
techniques, GSEs can now efficiently integrate text, images, audio, and video,
producing richer responses that better satisfy complex information needs.
Existing G-SEO methods, however, remain limited to text-based optimization and
fail to fully exploit multimodal data. To address this gap, we propose Caption
Injection, the first multimodal G-SEO approach, which extracts captions from
images and injects them into textual content, integrating visual semantics to
enhance the subjective visibility of content in generative search scenarios. We
systematically evaluate Caption Injection on MRAMG, a benchmark for MRAG, under
both unimodal and multimodal settings. Experimental results show that Caption
Injection significantly outperforms text-only G-SEO baselines under the G-Eval
metric, demonstrating the necessity and effectiveness of multimodal integration
in G-SEO to improve user-perceived content visibility.

</details>


### [58] [E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce](https://arxiv.org/abs/2511.04087)
*Ge Zhang,Rohan Deepak Ajwani,Tony Zheng,Hongjian Gu,Yaochen Hu,Wei Guo,Mark Coates,Yingxue Zhang*

Main category: cs.IR

TL;DR: 提出E - CARE以提升电商任务效率，实验在下游任务上precision@5提升达12.1%。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型挖掘查询与产品交叉特征的方法成本高，需提升效率。

Method: 提出E - CARE，利用常识推理因子图，使模型在推理时每个查询仅需一次大语言模型前向传播即可进行常识推理。

Result: 在2个下游任务实验中precision@5提升达12.1%。

Conclusion: E - CARE能在利用大语言模型常识推理能力的同时提升电商任务效率。

Abstract: Finding relevant products given a user query plays a pivotal role in an
e-commerce platform, as it can spark shopping behaviors and result in revenue
gains. The challenge lies in accurately predicting the correlation between
queries and products. Recently, mining the cross-features between queries and
products based on the commonsense reasoning capacity of Large Language Models
(LLMs) has shown promising performance. However, such methods suffer from high
costs due to intensive real-time LLM inference during serving, as well as human
annotations and potential Supervised Fine Tuning (SFT). To boost efficiency
while leveraging the commonsense reasoning capacity of LLMs for various
e-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation
Enhancer (E-CARE). During inference, models augmented with E-CARE can access
commonsense reasoning with only a single LLM forward pass per query by
utilizing a commonsense reasoning factor graph that encodes most of the
reasoning schema from powerful LLMs. The experiments on 2 downstream tasks show
an improvement of up to 12.1% on precision@5.

</details>


### [59] [Transforming Mentorship: An AI Powered Chatbot Approach to University Guidance](https://arxiv.org/abs/2511.04172)
*Mashrur Rahman,Mantaqa abedin,Monowar Zamil Abir,Faizul Islam Ansari,Adib Reza,Farig Yousuf Sadeque,Niloy Farhan*

Main category: cs.IR

TL;DR: 本文介绍为BRAC大学学生设计的AI聊天机器人，阐述其数据处理方式与检索模型，评估结果良好，能帮助学生了解大学生活。


<details>
  <summary>Details</summary>
Motivation: 大学生缺乏个性化按需指导，现有数字工具缺少针对新生的定制化辅导。

Method: 构建数据摄入管道处理多源信息，采用BM25词法排名与ChromaDB语义检索结合的混合方法检索信息，用LLaMA - 3.3 - 70B大语言模型生成对话回复。

Result: 生成文本语义相关性高，BERTScore为0.831，METEOR分数为0.809；数据管道更新效率高，更新用时106.82秒，新数据处理用时368.62秒。

Conclusion: 该聊天机器人能回应学生查询，助其了解大学生活并规划学期日程。

Abstract: University students face immense challenges during their undergraduate lives,
often being deprived of personalized on-demand guidance that mentors fail to
provide at scale. Digital tools exist, but there is a serious lack of
customized coaching for newcomers. This paper presents an AI-powered chatbot
that will serve as a mentor for the students of BRAC University. The main
component is a data ingestion pipeline that efficiently processes and updates
information from diverse sources, such as CSV files and university webpages.
The chatbot retrieves information through a hybrid approach, combining BM25
lexical ranking with ChromaDB semantic retrieval, and uses a Large Language
Model, LLaMA-3.3-70B, to generate conversational responses. The generated text
was found to be semantically highly relevant, with a BERTScore of 0.831 and a
METEOR score of 0.809. The data pipeline was also very efficient, taking 106.82
seconds for updates, compared to 368.62 seconds for new data. This chatbot will
be able to help students by responding to their queries, helping them to get a
better understanding of university life, and assisting them to plan better
routines for their semester in the open-credit university.

</details>


### [60] [Coordination-Free Lane Partitioning for Convergent ANN Search](https://arxiv.org/abs/2511.04221)
*Carl Kugblenu,Petri Vuorimaa*

Main category: cs.IR

TL;DR: 提出无协调车道分区器，将重复工作转化为互补工作，提升向量搜索性能。


<details>
  <summary>Details</summary>
Motivation: 现有生产向量搜索系统各并行车道会重复发现相同候选，额外计算不增加覆盖率。

Method: 为每个查询构建确定候选池，应用伪随机排列，为每个车道分配不相交位置切片。

Result: 在不同数据集和索引上提升召回率、命中率等指标，微基准测试显示每查询规划开销约37微秒。

Conclusion: 按总预算确定每查询池大小，跨车道确定性划分位置，可在不改变预算和期限下将冗余扩展转化为互补覆盖。

Abstract: Production vector search systems often fan out each query across parallel
lanes (threads, replicas, or shards) to meet latency service-level objectives
(SLOs). In practice, these lanes rediscover the same candidates, so extra
compute does not increase coverage. We present a coordination-free lane
partitioner that turns duplication into complementary work at the same cost and
deadline. For each query we (1) build a deterministic candidate pool sized to
the total top-k budget, (2) apply a per-query pseudorandom permutation, and (3)
assign each lane a disjoint slice of positions. Lanes then return different
results by construction, with no runtime coordination.
  At equal cost with four lanes (total candidate budget 64), on SIFT1M (1M SIFT
feature vectors) with Hierarchical Navigable Small World graphs (HNSW)
recall@10 rises from 0.249 to 0.999 while lane overlap falls from nearly 100%
to 0%. On MS MARCO (8.8M passages) with HNSW, hit@10 improves from 0.200 to
0.601 and Mean Reciprocal Rank at 10 (MRR@10) from 0.133 to 0.330. For inverted
file (IVF) indexes we see smaller but consistent gains (for example, +11% on MS
MARCO) by de-duplicating list routing. A microbenchmark shows planner overhead
of ~37 microseconds per query (mean at the main setting) with linear growth in
the number of merged candidates.
  These results yield a simple operational guideline: size the per-query pool
to the total budget, deterministically partition positions across lanes, and
turn redundant fan-out into complementary coverage without changing budget or
deadline.

</details>


### [61] [Denoised Recommendation Model with Collaborative Signal Decoupling](https://arxiv.org/abs/2511.04237)
*Zefeng Li,Ning Yang*

Main category: cs.IR

TL;DR: 提出基于GNN的CF模型DRCSD去噪不稳定交互，实验显示其有更好鲁棒性和推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 现有CF算法因用户 - 项目交互矩阵中的噪声导致推荐性能不佳，且多数去噪方法在单图上进行，会导致协作信号衰减。

Method: 提出DRCSD模型，包含协作信号解耦模块和按阶去噪模块，并修改传统GNN - 基于CF模型的信息聚合机制。

Result: 在三个公开真实数据集上实验，DRCSD对不稳定交互有更好鲁棒性，推荐准确性指标有显著提升。

Conclusion: DRCSD模型能有效解决现有CF算法存在的问题，在推荐性能上有优势。

Abstract: Although the collaborative filtering (CF) algorithm has achieved remarkable
performance in recommendation systems, it suffers from suboptimal
recommendation performance due to noise in the user-item interaction matrix.
Numerous noise-removal studies have improved recommendation models, but most
existing approaches conduct denoising on a single graph. This may cause
attenuation of collaborative signals: removing edges between two nodes can
interrupt paths between other nodes, weakening path-dependent collaborative
information. To address these limitations, this study proposes a novel
GNN-based CF model called DRCSD for denoising unstable interactions. DRCSD
includes two core modules: a collaborative signal decoupling module (decomposes
signals into distinct orders by structural characteristics) and an order-wise
denoising module (performs targeted denoising on each order). Additionally, the
information aggregation mechanism of traditional GNN-based CF models is
modified to avoid cross-order signal interference until the final pooling
operation. Extensive experiments on three public real-world datasets show that
DRCSD has superior robustness against unstable interactions and achieves
statistically significant performance improvements in recommendation accuracy
metrics compared to state-of-the-art baseline models.

</details>


### [62] [LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems](https://arxiv.org/abs/2511.04541)
*Baptiste Bonin,Maxime Heuillet,Audrey Durand*

Main category: cs.IR

TL;DR: 研究大语言模型（LLM）能否通过对推荐列表的成对推理有效充当用户偏好的世界模型，通过实证研究揭示任务性能与LLM捕获的偏好函数属性之间的关系。


<details>
  <summary>Details</summary>
Motivation: 解决跨领域用户偏好建模在推荐列表研究中的关键挑战。

Method: 在三个不同数据集的任务上对多个LLM进行实证研究。

Result: 揭示了任务性能与LLM捕获的偏好函数属性之间的关系。

Conclusion: 指出了改进方向，强调了LLM作为推荐系统世界模型的潜力。

Abstract: Modeling user preferences across domains remains a key challenge in slate
recommendation (i.e. recommending an ordered sequence of items) research. We
investigate how Large Language Models (LLM) can effectively act as world models
of user preferences through pairwise reasoning over slates. We conduct an
empirical study involving several LLMs on three tasks spanning different
datasets. Our results reveal relationships between task performance and
properties of the preference function captured by LLMs, hinting towards areas
for improvement and highlighting the potential of LLMs as world models in
recommender systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [63] [Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices](https://arxiv.org/abs/2511.03753)
*Youssef Elmir,Yassine Himeur,Abbes Amira*

Main category: cs.LG

TL;DR: 提出用于物联网医疗环境的隐私保护心电图分类联邦学习框架，实验验证其在异构设备上的性能，结果显示高准确率且资源利用高效，凸显轻量级隐私保护AI在物联网医疗监测的潜力。


<details>
  <summary>Details</summary>
Motivation: 在物联网医疗环境中实现隐私保护的心电图分类，确保敏感医疗数据本地存储。

Method: 将1D心电图信号转换为2D Gramian Angular Field (GAF)图像，通过卷积神经网络进行特征提取，并在异构物联网设备上进行联邦学习。

Result: FL - GAF模型在多客户端设置中达到95.18%的高分类准确率，在准确率和训练时间上显著优于单客户端基线，且保持高效资源利用和通信开销。

Conclusion: 轻量级、隐私保护的AI在基于物联网的医疗监测中有潜力，支持智能健康系统的可扩展和安全边缘部署。

Abstract: This study presents a federated learning (FL) framework for
privacy-preserving electrocardiogram (ECG) classification in Internet of Things
(IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian
Angular Field (GAF) images, the proposed approach enables efficient feature
extraction through Convolutional Neural Networks (CNNs) while ensuring that
sensitive medical data remain local to each device. This work is among the
first to experimentally validate GAF-based federated ECG classification across
heterogeneous IoT devices, quantifying both performance and communication
efficiency. To evaluate feasibility in realistic IoT settings, we deployed the
framework across a server, a laptop, and a resource-constrained Raspberry Pi 4,
reflecting edge-cloud integration in IoT ecosystems. Experimental results
demonstrate that the FL-GAF model achieves a high classification accuracy of
95.18% in a multi-client setup, significantly outperforming a single-client
baseline in both accuracy and training time. Despite the added computational
complexity of GAF transformations, the framework maintains efficient resource
utilization and communication overhead. These findings highlight the potential
of lightweight, privacy-preserving AI for IoT-based healthcare monitoring,
supporting scalable and secure edge deployments in smart health systems.

</details>


### [64] [One Size Does Not Fit All: Architecture-Aware Adaptive Batch Scheduling with DEBA](https://arxiv.org/abs/2511.03809)
*François Belias,Naser Ezzati-Jivan,Foutse Khomh*

Main category: cs.LG

TL;DR: 提出DEBA自适应批调度器，研究表明批大小自适应效果因架构而异，需架构感知设计。


<details>
  <summary>Details</summary>
Motivation: 现有自适应批大小方法假设通用策略，本文探究不同架构下的自适应效果。

Method: 引入DEBA，通过监控梯度方差、梯度范数变化和损失变化指导批大小调整，在六种架构上进行系统评估。

Result: 不同架构自适应效果不同，轻量级和中深度架构有训练加速和精度提升；浅残差网络有精度和速度提升；深残差网络效果不稳定；稳定架构加速小。还给出预测架构是否受益的框架。

Conclusion: 自适应批大小方法不能通用，批大小自适应需要架构感知设计。

Abstract: Adaptive batch size methods aim to accelerate neural network training, but
existing approaches apply identical adaptation strategies across all
architectures, assuming a one-size-fits-all solution. We introduce DEBA
(Dynamic Efficient Batch Adaptation), an adaptive batch scheduler that monitors
gradient variance, gradient norm variation and loss variation to guide batch
size adaptations. Through systematic evaluation across six architectures
(ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V3, ViT-B16) on
CIFAR-10 and CIFAR-100, with five random seeds per configuration, we
demonstrate that the architecture fundamentally determines adaptation efficacy.
Our findings reveal that: (1) lightweight and medium-depth architectures
(MobileNet-V3, DenseNet-121, EfficientNet-B0) achieve a 45-62% training speedup
with simultaneous accuracy improvements of 1-7%; (2) shallow residual networks
(ResNet-18) show consistent gains of +2.4 - 4.0% in accuracy, 36 - 43% in
speedup, while deep residual networks (ResNet-50) exhibit high variance and
occasional degradation; (3) already-stable architectures (ViT-B16) show minimal
speedup (6%) despite maintaining accuracy, indicating that adaptation benefits
vary with baseline optimization characteristics. We introduce a baseline
characterization framework using gradient stability metrics (stability score,
gradient norm variation) that predicts which architectures will benefit from
adaptive scheduling. Our ablation studies reveal critical design choices often
overlooked in prior work: sliding window statistics (vs. full history) and
sufficient cooldown periods (5+ epochs) between adaptations are essential for
success. This work challenges the prevailing assumption that adaptive methods
generalize across architectures and provides the first systematic evidence that
batch size adaptation requires an architecture-aware design.

</details>


### [65] [Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland](https://arxiv.org/abs/2511.03749)
*Oluwadurotimi Onibonoje,Vuong M. Ngo,Andrew McCarre,Elodie Ruelle,Bernadette O-Briend,Mark Roantree*

Main category: cs.LG

TL;DR: 本文提出用于单变量数据集的深度学习模型预测草地生长，以解决爱尔兰乳业面临的问题，一个模型性能佳，研究提升了预测可靠性，助力可持续乳业。


<details>
  <summary>Details</summary>
Motivation: 草地对碳循环和生物多样性很重要，爱尔兰乳业面临盈利和可持续性挑战，现有的草地生长预测机械模型不实用。

Method: 提出适用于单变量数据集的深度学习模型，用历史草地高度数据进行预测。

Result: 一个用于预测科克多年生黑麦草生长的时间卷积网络模型表现良好，RMSE为2.74，MAE为3.46，对34年1757周的综合数据集验证得到了最佳模型配置。

Conclusion: 本研究增进了对模型行为的理解，提高了草地生长预测的可靠性，有助于可持续乳业实践的发展。

Abstract: Grasslands, constituting the world's second-largest terrestrial carbon sink,
play a crucial role in biodiversity and the regulation of the carbon cycle.
Currently, the Irish dairy sector, a significant economic contributor, grapples
with challenges related to profitability and sustainability. Presently, grass
growth forecasting relies on impractical mechanistic models. In response, we
propose deep learning models tailored for univariate datasets, presenting
cost-effective alternatives. Notably, a temporal convolutional network designed
for forecasting Perennial Ryegrass growth in Cork exhibits high performance,
leveraging historical grass height data with RMSE of 2.74 and MAE of 3.46.
Validation across a comprehensive dataset spanning 1,757 weeks over 34 years
provides insights into optimal model configurations. This study enhances our
understanding of model behavior, thereby improving reliability in grass growth
forecasting and contributing to the advancement of sustainable dairy farming
practices.

</details>


### [66] [Accelerating scientific discovery with the common task framework](https://arxiv.org/abs/2511.04001)
*J. Nathan Kutz,Peter Battaglia,Michael Brenner,Kevin Carlberg,Aric Hagberg,Shirley Ho,Stephan Hoyer,Henning Lange,Hod Lipson,Michael W. Mahoney,Frank Noe,Max Welling,Laure Zanna,Francis Zhu,Steven L. Brunton*

Main category: cs.LG

TL;DR: 引入适用于科学与工程的通用任务框架CTF，强调其对评估ML/AI算法的重要性。


<details>
  <summary>Details</summary>
Motivation: ML/AI算法变革动态系统的表征与控制，需比较指标评估不同科学目标，且要考虑数据有限和测量噪声情况。

Method: 引入通用任务框架CTF，其包含一系列具有不同实际和通用目标的挑战性数据集。

Result: 未提及具体结果。

Conclusion: CTF的客观指标对于比较科学和工程领域快速发展和应用的不同算法至关重要。

Abstract: Machine learning (ML) and artificial intelligence (AI) algorithms are
transforming and empowering the characterization and control of dynamic systems
in the engineering, physical, and biological sciences. These emerging modeling
paradigms require comparative metrics to evaluate a diverse set of scientific
objectives, including forecasting, state reconstruction, generalization, and
control, while also considering limited data scenarios and noisy measurements.
We introduce a common task framework (CTF) for science and engineering, which
features a growing collection of challenge data sets with a diverse set of
practical and common objectives. The CTF is a critically enabling technology
that has contributed to the rapid advance of ML/AI algorithms in traditional
applications such as speech recognition, language processing, and computer
vision. There is a critical need for the objective metrics of a CTF to compare
the diverse algorithms being rapidly developed and deployed in practice today
across science and engineering.

</details>


### [67] [Laugh, Relate, Engage: Stylized Comment Generation for Short Videos](https://arxiv.org/abs/2511.03757)
*Xuan Ouyang,Senan Wang,Bouzhou Wang,Siyuan Xiahou,Jinrong Zhou,Yuekang Li*

Main category: cs.LG

TL;DR: 介绍了用于可控短视频评论生成的模块化多智能体系统LOLGORITHM，构建双语数据集评估，结果显示其显著优于基线模型，为短视频平台风格化评论生成提供框架。


<details>
  <summary>Details</summary>
Motivation: 解决生成符合平台规则且具有风格多样性和上下文感知的评论的挑战。

Method: 引入模块化多智能体系统LOLGORITHM，集成视频分割、分析和提示构建；构建双语数据集；结合自动指标和大规模人类偏好研究进行评估。

Result: LOLGORITHM显著优于基线模型，在抖音和YouTube上偏好率分别超90%和87.55%。

Conclusion: 该工作为短视频平台风格化评论生成提供可扩展和文化自适应框架，有望增强用户参与和创意互动。

Abstract: Short-video platforms have become a central medium in the modern Internet
landscape, where efficient information delivery and strong interactivity are
reshaping user engagement and cultural dissemination. Among the various forms
of user interaction, comments play a vital role in fostering community
participation and enabling content re-creation. However, generating comments
that are both compliant with platform guidelines and capable of exhibiting
stylistic diversity and contextual awareness remains a significant challenge.
We introduce LOLGORITHM, a modular multi-agent system (MAS) designed for
controllable short-video comment generation. The system integrates video
segmentation, contextual and affective analysis, and style-aware prompt
construction. It supports six distinct comment styles: puns (homophones),
rhyming, meme application, sarcasm (irony), plain humor, and content
extraction. Powered by a multimodal large language model (MLLM), LOLGORITHM
directly processes video inputs and achieves fine-grained style control through
explicit prompt markers and few-shot examples. To support development and
evaluation, we construct a bilingual dataset using official APIs from Douyin
(Chinese) and YouTube (English), covering five popular video genres: comedy
skits, daily life jokes, funny animal clips, humorous commentary, and talk
shows. Evaluation combines automated metrics originality, relevance, and style
conformity with a large-scale human preference study involving 40 videos and
105 participants. Results show that LOLGORITHM significantly outperforms
baseline models, achieving preference rates of over 90% on Douyin and 87.55% on
YouTube. This work presents a scalable and culturally adaptive framework for
stylized comment generation on short-video platforms, offering a promising path
to enhance user engagement and creative interaction.

</details>


### [68] [Multiscale Astrocyte Network Calcium Dynamics for Biologically Plausible Intelligence in Anomaly Detection](https://arxiv.org/abs/2511.03993)
*Berk Iskar,Michael Taynnan Barros*

Main category: cs.LG

TL;DR: 提出受大脑星形胶质细胞Ca²⁺信号启发的学习框架用于网络异常检测，在CTU - 13数据上表现良好，有广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统离线训练的网络异常检测器易受概念漂移和新威胁影响，需改进。

Method: 将多细胞星形胶质细胞动力学模拟器与深度神经网络结合，模拟Ca²⁺动力学的三种关键机制。

Result: 在CTU - 13数据上，Ca²⁺门控模型优于基线DNN，准确率达约98%，减少了误报和漏报，且预计算Ca²⁺轨迹后运行开销可忽略。

Conclusion: 该Ca²⁺调制学习框架为需要快速适应数据模式的流式检测任务提供通用解决方案。

Abstract: Network anomaly detection systems encounter several challenges with
traditional detectors trained offline. They become susceptible to concept drift
and new threats such as zero-day or polymorphic attacks. To address this
limitation, we propose a Ca$^{2+}$-modulated learning framework that draws
inspiration from astrocytic Ca$^{2+}$ signaling in the brain, where rapid,
context-sensitive adaptation enables robust information processing. Our
approach couples a multicellular astrocyte dynamics simulator with a deep
neural network (DNN). The simulator models astrocytic Ca$^{2+}$ dynamics
through three key mechanisms: IP$_3$-mediated Ca$^{2+}$ release, SERCA pump
uptake, and conductance-aware diffusion through gap junctions between cells.
Evaluation of our proposed network on CTU-13 (Neris) network traffic data
demonstrates the effectiveness of our biologically plausible approach. The
Ca$^{2+}$-gated model outperforms a matched baseline DNN, achieving up to
$\sim$98\% accuracy with reduced false positives and negatives across multiple
train/test splits. Importantly, this improved performance comes with negligible
runtime overhead once Ca$^{2+}$ trajectories are precomputed. While
demonstrated here for cybersecurity applications, this Ca$^{2+}$-modulated
learning framework offers a generic solution for streaming detection tasks that
require rapid, biologically grounded adaptation to evolving data patterns.

</details>


### [69] [What's in Common? Multimodal Models Hallucinate When Reasoning Across Scenes](https://arxiv.org/abs/2511.03768)
*Candace Ross,Florian Bordes,Adina Williams,Polina Kirichenko,Mark Ibrahim*

Main category: cs.LG

TL;DR: 本文构建新基准Common - O评估多模态语言模型跨场景推理能力，发现多数模型跨场景推理难，易产生幻觉，提出多图像训练有前景并公开基准。


<details>
  <summary>Details</summary>
Motivation: 现有多模态语言模型在真实场景推理时存在幻觉问题，现有感知基准与真实世界推理存在差距。

Method: 构建新基准Common - O，使用超10.5k新图像，通过询问“有什么共同点”来测试跨场景推理，评估领先多模态语言模型。

Result: 多数模型单图物体感知较易，但跨场景推理难，最佳模型在Common - O上仅35%，在Common - O Complex上仅1%，场景中有相似物体时模型更易产生幻觉。

Conclusion: 规模能带来一定提升，显式多图像输入训练的模型提升更大，多图像训练有前景，公开基准以推动跨场景推理幻觉问题研究。

Abstract: Multimodal language models possess a remarkable ability to handle an
open-vocabulary's worth of objects. Yet the best models still suffer from
hallucinations when reasoning about scenes in the real world, revealing a gap
between their seemingly strong performance on existing perception benchmarks
that are saturating and their reasoning in the real world. To address this gap,
we build a novel benchmark of in-the-wild scenes that we call Common-O. With
more than 10.5k examples using exclusively new images not found in web training
data to avoid contamination, Common-O goes beyond just perception, inspired by
cognitive tests for humans, to probe reasoning across scenes by asking "what's
in common?". We evaluate leading multimodal language models, including models
specifically trained to perform chain-of-thought reasoning. We find that
perceiving objects in single images is tractable for most models, yet reasoning
across scenes is very challenging even for the best models, including reasoning
models. Despite saturating many leaderboards focusing on perception, the best
performing model only achieves 35% on Common-O -- and on Common-O Complex,
consisting of more complex scenes, the best model achieves only 1%. Curiously,
we find models are more prone to hallucinate when similar objects are present
in the scene, suggesting models may be relying on object co-occurrence seen
during training. Among the models we evaluated, we found scale can provide
modest improvements while models explicitly trained with multi-image inputs
show bigger improvements, suggesting scaled multi-image training may offer
promise. We make our benchmark publicly available to spur research into the
challenge of hallucination when reasoning across scenes.

</details>


### [70] [Enhancing Multimodal Protein Function Prediction Through Dual-Branch Dynamic Selection with Reconstructive Pre-Training](https://arxiv.org/abs/2511.04040)
*Xiaoling Luo,Peng Chen,Chengliang Liu,Xiaopeng Jin,Jie Wen,Yumeng Liu,Junsong Wang*

Main category: cs.LG

TL;DR: 提出DSRPGO模型用于蛋白质功能预测，结合重建预训练和动态选择机制，在人类数据集上表现优于其他基准模型。


<details>
  <summary>Details</summary>
Motivation: 多模态蛋白质特征信息复杂，难以解读其复杂的相互联系，且任务存在分层多标签分类困难。

Method: 利用动态选择和重建预训练机制，引入重建预训练挖掘细粒度信息，提出双向交互模块促进多模态特征交互学习，设计动态选择模块选择最有利于当前蛋白质功能预测的特征表示。

Result: DSRPGO模型在人类数据集的BPO、MFO和CCO方面显著改善，优于其他基准模型。

Conclusion: 所提出的DSRPGO模型在蛋白质功能预测任务中有效，能取得更好的性能。

Abstract: Multimodal protein features play a crucial role in protein function
prediction. However, these features encompass a wide range of information,
ranging from structural data and sequence features to protein attributes and
interaction networks, making it challenging to decipher their complex
interconnections. In this work, we propose a multimodal protein function
prediction method (DSRPGO) by utilizing dynamic selection and reconstructive
pre-training mechanisms. To acquire complex protein information, we introduce
reconstructive pre-training to mine more fine-grained information with low
semantic levels. Moreover, we put forward the Bidirectional Interaction Module
(BInM) to facilitate interactive learning among multimodal features.
Additionally, to address the difficulty of hierarchical multi-label
classification in this task, a Dynamic Selection Module (DSM) is designed to
select the feature representation that is most conducive to current protein
function prediction. Our proposed DSRPGO model improves significantly in BPO,
MFO, and CCO on human datasets, thereby outperforming other benchmark models.

</details>


### [71] [Learning Filter-Aware Distance Metrics for Nearest Neighbor Search with Multiple Filters](https://arxiv.org/abs/2511.04073)
*Ananya Sutradhar,Suryansh Gupta,Ravishankar Krishnaswamy,Haiyang Xu,Aseem Rastogi,Gopal Srinivasa*

Main category: cs.LG

TL;DR: 本文提出新方法解决过滤近似最近邻搜索问题，学习最优权衡，实验显示比固定惩罚方法准确率提高5 - 10%。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的过滤近似最近邻搜索方法使用固定惩罚，难以在不同标签和向量分布的数据集上泛化。

Method: 将问题表述为约束线性优化问题，直接从数据中学习向量距离和过滤匹配的最优权衡，用学习到的权重指导搜索和索引构建。

Result: 实验表明，适应数据的距离函数比固定惩罚方法准确率提高5 - 10%。

Conclusion: 提出的方法为过滤近似最近邻搜索问题提供了更灵活、可泛化的框架。

Abstract: Filtered Approximate Nearest Neighbor (ANN) search retrieves the closest
vectors for a query vector from a dataset. It enforces that a specified set of
discrete labels $S$ for the query must be included in the labels of each
retrieved vector. Existing graph-based methods typically incorporate filter
awareness by assigning fixed penalties or prioritizing nodes based on filter
satisfaction. However, since these methods use fixed, data in- dependent
penalties, they often fail to generalize across datasets with diverse label and
vector distributions. In this work, we propose a principled alternative that
learns the optimal trade-off between vector distance and filter match directly
from the data, rather than relying on fixed penalties. We formulate this as a
constrained linear optimization problem, deriving weights that better reflect
the underlying filter distribution and more effectively address the filtered
ANN search problem. These learned weights guide both the search process and
index construction, leading to graph structures that more effectively capture
the underlying filter distribution and filter semantics. Our experiments
demonstrate that adapting the distance function to the data significantly im-
proves accuracy by 5-10% over fixed-penalty methods, providing a more flexible
and generalizable framework for the filtered ANN search problem.

</details>


### [72] [Contamination Detection for VLMs using Multi-Modal Semantic Perturbation](https://arxiv.org/abs/2511.03774)
*Jaden Park,Mu Cai,Feng Yao,Jingbo Shang,Soochahn Lee,Yong Jae Lee*

Main category: cs.LG

TL;DR: 现有视觉语言模型（VLMs）因使用互联网规模预训练语料存在测试集泄漏问题，本文提出基于多模态语义扰动的检测方法，验证了其鲁棒性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型使用互联网规模预训练语料存在测试集泄漏导致性能虚高问题，且检测受污染VLMs的方法研究不足。

Method: 故意污染开源VLMs，测试现有检测方法，提出基于多模态语义扰动的检测方法。

Result: 现有检测方法失效或表现不一致，提出的方法能证明受污染模型在受控扰动下无法泛化，且在多种现实污染策略中验证了鲁棒性和有效性。

Conclusion: 提出的基于多模态语义扰动的检测方法有效且鲁棒，代码和扰动数据集将公开。

Abstract: Recent advances in Vision-Language Models (VLMs) have achieved
state-of-the-art performance on numerous benchmark tasks. However, the use of
internet-scale, often proprietary, pretraining corpora raises a critical
concern for both practitioners and users: inflated performance due to test-set
leakage. While prior works have proposed mitigation strategies such as
decontamination of pretraining data and benchmark redesign for LLMs, the
complementary direction of developing detection methods for contaminated VLMs
remains underexplored. To address this gap, we deliberately contaminate
open-source VLMs on popular benchmarks and show that existing detection
approaches either fail outright or exhibit inconsistent behavior. We then
propose a novel simple yet effective detection method based on multi-modal
semantic perturbation, demonstrating that contaminated models fail to
generalize under controlled perturbations. Finally, we validate our approach
across multiple realistic contamination strategies, confirming its robustness
and effectiveness. The code and perturbed dataset will be released publicly.

</details>


### [73] [FusionDP: Foundation Model-Assisted Differentially Private Learning for Partially Sensitive Features](https://arxiv.org/abs/2511.03806)
*Linghui Zeng,Ruixuan Liu,Atiquer Rahman Sarkar,Xiaoqian Jiang,Joyce C. Ho,Li Xiong*

Main category: cs.LG

TL;DR: 提出FusionDP框架，在特征级差分隐私下提升模型效用，经两个任务评估，能显著提升性能并保持隐私。


<details>
  <summary>Details</summary>
Motivation: 传统DP - SGD对所有特征进行隐私保护会导致过度注入噪声和显著的效用下降，实际场景中只需对部分特征进行隐私保护。

Method: 提出FusionDP两步框架，一是利用基础模型根据非敏感特征估算敏感特征；二是引入改进的DP - SGD算法在原始和估算特征上训练模型。

Result: 在两个任务上评估，与基线相比，FusionDP显著提升模型性能并保持特征级隐私。

Conclusion: 基于基础模型的估算方法能提升不同模态下隐私 - 效用的权衡。

Abstract: Ensuring the privacy of sensitive training data is crucial in
privacy-preserving machine learning. However, in practical scenarios, privacy
protection may be required for only a subset of features. For instance, in ICU
data, demographic attributes like age and gender pose higher privacy risks due
to their re-identification potential, whereas raw lab results are generally
less sensitive. Traditional DP-SGD enforces privacy protection on all features
in one sample, leading to excessive noise injection and significant utility
degradation. We propose FusionDP, a two-step framework that enhances model
utility under feature-level differential privacy. First, FusionDP leverages
large foundation models to impute sensitive features given non-sensitive
features, treating them as external priors that provide high-quality estimates
of sensitive attributes without accessing the true values during model
training. Second, we introduce a modified DP-SGD algorithm that trains models
on both original and imputed features while formally preserving the privacy of
the original sensitive features. We evaluate FusionDP on two modalities: a
sepsis prediction task on tabular data from PhysioNet and a clinical note
classification task from MIMIC-III. By comparing against privacy-preserving
baselines, our results show that FusionDP significantly improves model
performance while maintaining rigorous feature-level privacy, demonstrating the
potential of foundation model-driven imputation to enhance the privacy-utility
trade-off for various modalities.

</details>


### [74] [Fair and Explainable Credit-Scoring under Concept Drift: Adaptive Explanation Frameworks for Evolving Populations](https://arxiv.org/abs/2511.03807)
*Shivogo John*

Main category: cs.LG

TL;DR: 传统信用评分系统解释技术在数据分布变化时不稳定且可能不公平，本研究开发自适应解释框架，经测试能提升稳定性并减少对不同群体的影响差异。


<details>
  <summary>Details</summary>
Motivation: 传统可解释性技术如SHAP在概念漂移时解释不稳定且可能不公平，需解决动态演化信用模型的可解释性和公平性问题。

Method: 使用多年信用数据集，将XGBoost预测建模与三种自适应SHAP变体结合，用多种指标与静态SHAP解释进行基准测试，并进行鲁棒性测试。

Result: 自适应方法，特别是基于重新基线和代理的解释，显著提高了时间稳定性，减少了不同人口群体间的差异影响，且不降低预测准确性。

Conclusion: 自适应可解释性是维持数据驱动信用系统及决策模型随人口变化而演变的任何领域的透明度、问责制和道德可靠性的实用机制。

Abstract: Evolving borrower behaviors, shifting economic conditions, and changing
regulatory landscapes continuously reshape the data distributions underlying
modern credit-scoring systems. Conventional explainability techniques, such as
SHAP, assume static data and fixed background distributions, making their
explanations unstable and potentially unfair when concept drift occurs. This
study addresses that challenge by developing adaptive explanation frameworks
that recalibrate interpretability and fairness in dynamically evolving credit
models. Using a multi-year credit dataset, we integrate predictive modeling via
XGBoost with three adaptive SHAP variants: (A) per-slice explanation
reweighting that adjusts for feature distribution shifts, (B) drift-aware SHAP
rebaselining with sliding-window background samples, and (C) online surrogate
calibration using incremental Ridge regression. Each method is benchmarked
against static SHAP explanations using metrics of predictive performance (AUC,
F1), directional and rank stability (cosine, Kendall tau), and fairness
(demographic parity and recalibration). Results show that adaptive methods,
particularly rebaselined and surrogate-based explanations, substantially
improve temporal stability and reduce disparate impact across demographic
groups without degrading predictive accuracy. Robustness tests, including
counterfactual perturbations, background sensitivity analysis, and
proxy-variable detection, confirm the resilience of adaptive explanations under
real-world drift conditions. These findings establish adaptive explainability
as a practical mechanism for sustaining transparency, accountability, and
ethical reliability in data-driven credit systems, and more broadly, in any
domain where decision models evolve with population change.

</details>


### [75] [Optimizing Reasoning Efficiency through Prompt Difficulty Prediction](https://arxiv.org/abs/2511.03808)
*Bo Zhao,Berkcan Kapusuzoglu,Kartik Balasubramaniam,Sambit Sahu,Supriyo Chakraborty,Genta Indra Winata*

Main category: cs.LG

TL;DR: 提出路由方法为推理模型降本增效，在数学基准测试中表现良好


<details>
  <summary>Details</summary>
Motivation: 推理语言模型部署成本高，需降低计算量且不牺牲准确性

Method: 使用中间表示训练问题难度或模型正确性的轻量级预测器，以指导推理模型池的路由

Result: 在不同数学基准测试中，路由比随机分配更高效，使用更少计算量达到与s1.1 - 32B相当的性能

Conclusion: 基于难度感知的路由对推理模型的经济高效部署有效

Abstract: Reasoning language models perform well on complex tasks but are costly to
deploy due to their size and long reasoning traces. We propose a routing
approach that assigns each problem to the smallest model likely to solve it,
reducing compute without sacrificing accuracy. Using intermediate
representations from s1.1-32B, we train lightweight predictors of problem
difficulty or model correctness to guide routing across a pool of reasoning
models. On diverse math benchmarks, routing improves efficiency over random
assignment and matches s1.1-32B's performance while using significantly less
compute. Our results demonstrate that difficulty-aware routing is effective for
cost-efficient deployment of reasoning models.

</details>


### [76] [Sketch-Augmented Features Improve Learning Long-Range Dependencies in Graph Neural Networks](https://arxiv.org/abs/2511.03824)
*Ryien Hosseini,Filippo Simini,Venkatram Vishwanath,Rebecca Willett,Henry Hoffmann*

Main category: cs.LG

TL;DR: 本文提出在标准GNN中注入随机化全局嵌入（Sketched Random Features）以捕获长距离依赖，缓解GNN局限性，实验证明能提升性能。


<details>
  <summary>Details</summary>
Motivation: 标准GNN的局部消息传递范式存在长距离信息过压缩、节点表示过平滑和表达能力有限的问题。

Method: 在标准GNN中注入名为Sketched Random Features的随机化全局嵌入。

Result: 实验结果表明，该策略在真实世界图学习任务中持续提升了基准GNN的性能。

Conclusion: 该策略既可以作为独立解决方案，也可以作为现有技术（如图位置编码）的补充增强。

Abstract: Graph Neural Networks learn on graph-structured data by iteratively
aggregating local neighborhood information. While this local message passing
paradigm imparts a powerful inductive bias and exploits graph sparsity, it also
yields three key challenges: (i) oversquashing of long-range information, (ii)
oversmoothing of node representations, and (iii) limited expressive power. In
this work we inject randomized global embeddings of node features, which we
term \textit{Sketched Random Features}, into standard GNNs, enabling them to
efficiently capture long-range dependencies. The embeddings are unique,
distance-sensitive, and topology-agnostic -- properties which we analytically
and empirically show alleviate the aforementioned limitations when injected
into GNNs. Experimental results on real-world graph learning tasks confirm that
this strategy consistently improves performance over baseline GNNs, offering
both a standalone solution and a complementary enhancement to existing
techniques such as graph positional encodings. Our source code is available at
\href{https://github.com/ryienh/sketched-random-features}{https://github.com/ryienh/sketched-random-features}.

</details>


### [77] [From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning via Energy-Guided Diffusion Stratification](https://arxiv.org/abs/2511.03828)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: 提出Energy - Guided Diffusion Stratification (StratDiff)方法解决离线到在线强化学习过渡问题，结合现有方法在D4RL基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 离线到在线强化学习存在分布偏移问题，且少有方法评估或利用离线数据分布结构，需适应不同类型样本调整学习策略。

Method: 提出StratDiff方法，用扩散模型从离线数据集学习先验知识，通过基于能量的函数优化知识，计算生成动作和采样动作的KL散度对训练批次分层，不同子集采用不同学习策略。

Result: 将StratDiff与Cal - QL和IQL结合，在D4RL基准测试中显著优于现有方法。

Conclusion: StratDiff能有效解决离线到在线强化学习过渡问题，具有更好的适应性和更稳定的性能。

Abstract: Transitioning from offline to online reinforcement learning (RL) poses
critical challenges due to distributional shifts between the fixed behavior
policy in the offline dataset and the evolving policy during online learning.
Although this issue is widely recognized, few methods attempt to explicitly
assess or utilize the distributional structure of the offline data itself,
leaving a research gap in adapting learning strategies to different types of
samples. To address this challenge, we propose an innovative method,
Energy-Guided Diffusion Stratification (StratDiff), which facilitates smoother
transitions in offline-to-online RL. StratDiff deploys a diffusion model to
learn prior knowledge from the offline dataset. It then refines this knowledge
through energy-based functions to improve policy imitation and generate
offline-like actions during online fine-tuning. The KL divergence between the
generated action and the corresponding sampled action is computed for each
sample and used to stratify the training batch into offline-like and
online-like subsets. Offline-like samples are updated using offline objectives,
while online-like samples follow online learning strategies. We demonstrate the
effectiveness of StratDiff by integrating it with off-the-shelf methods Cal-QL
and IQL. Extensive empirical evaluations on D4RL benchmarks show that StratDiff
significantly outperforms existing methods, achieving enhanced adaptability and
more stable performance across diverse RL settings.

</details>


### [78] [Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs](https://arxiv.org/abs/2511.04473)
*Alberto Cattaneo,Carlo Luschi,Daniel Justus*

Main category: cs.LG

TL;DR: 提出SynthKGQA框架生成高质量合成知识图谱问答数据集，应用于Wikidata生成GTSQA数据集并进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 缺乏具有真实标签的挑战性问答数据集，难以对图检索方法进行比较。

Method: 提出SynthKGQA框架，从任何知识图谱生成高质量合成知识图谱问答数据集。

Result: SynthKGQA不仅能对知识图谱检索器进行更有意义的基准测试，还能用于训练更好的模型，生成了GTSQA数据集。

Conclusion: SynthKGQA框架有积极作用，生成的GTSQA数据集可测试知识图谱检索器的零样本泛化能力。

Abstract: Retrieval of information from graph-structured knowledge bases represents a
promising direction for improving the factuality of LLMs. While various
solutions have been proposed, a comparison of methods is difficult due to the
lack of challenging QA datasets with ground-truth targets for graph retrieval.
We present SynthKGQA, a framework for generating high-quality synthetic
Knowledge Graph Question Answering datasets from any Knowledge Graph, providing
the full set of ground-truth facts in the KG to reason over each question. We
show how, in addition to enabling more informative benchmarking of KG
retrievers, the data produced with SynthKGQA also allows us to train better
models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset
designed to test zero-shot generalization abilities of KG retrievers with
respect to unseen graph structures and relation types, and benchmark popular
solutions for KG-augmented LLMs on it.

</details>


### [79] [Higher-Order Causal Structure Learning with Additive Models](https://arxiv.org/abs/2511.03831)
*James Enouen,Yujia Zheng,Ignavier Ng,Yan Liu,Kun Zhang*

Main category: cs.LG

TL;DR: 本文聚焦将因果加性模型扩展到具有高阶交互的加性模型，引入有向无环超图，给出相关理论工具和可识别性结果，开发贪心CAM算法扩展并进行实验验证。


<details>
  <summary>Details</summary>
Motivation: 现实世界中很多过程有高阶机制，但因果发现中对交互的明确处理关注较少，因此要扩展因果加性模型。

Method: 引入有向无环超图表示高阶交互，给出处理新结构的定义和理论工具，扩展贪心CAM算法处理复杂超图搜索空间。

Result: 得到超图的可识别性结果，扩展典型的马尔可夫等价类，在合成实验中验证扩展算法的实用性。

Conclusion: 学习更复杂的超图结构可能带来更好的实证结果，更严格的假设对应更容易学习的超图和更好的有限样本复杂度。

Abstract: Causal structure learning has long been the central task of inferring causal
insights from data. Despite the abundance of real-world processes exhibiting
higher-order mechanisms, however, an explicit treatment of interactions in
causal discovery has received little attention. In this work, we focus on
extending the causal additive model (CAM) to additive models with higher-order
interactions. This second level of modularity we introduce to the structure
learning problem is most easily represented by a directed acyclic hypergraph
which extends the DAG. We introduce the necessary definitions and theoretical
tools to handle the novel structure we introduce and then provide
identifiability results for the hyper DAG, extending the typical Markov
equivalence classes. We next provide insights into why learning the more
complex hypergraph structure may actually lead to better empirical results. In
particular, more restrictive assumptions like CAM correspond to easier-to-learn
hyper DAGs and better finite sample complexity. We finally develop an extension
of the greedy CAM algorithm which can handle the more complex hyper DAG search
space and demonstrate its empirical usefulness in synthetic experiments.

</details>


### [80] [Enhancing Q-Value Updates in Deep Q-Learning via Successor-State Prediction](https://arxiv.org/abs/2511.03836)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: 提出SADQ解决DQN目标更新问题，理论保证无偏估计并降方差，实验显示其性能优于DQN变体。


<details>
  <summary>Details</summary>
Motivation: DQN目标更新依赖过去次优策略生成的状态，导致学习信号不足和更新方差高，尤其在样本转换与当前策略不一致时问题更严重。

Method: 提出Successor - state Aggregation Deep Q - Network (SADQ)，用随机转移模型显式建模环境动态，将后继状态分布集成到Q值估计过程，探索更高效的动作选择策略。

Result: 在标准强化学习基准和现实向量控制任务的大量实验中，SADQ在稳定性和学习效率上始终优于DQN变体。

Conclusion: SADQ能保持无偏价值估计，同时降低训练方差，性能表现良好。

Abstract: Deep Q-Networks (DQNs) estimate future returns by learning from transitions
sampled from a replay buffer. However, the target updates in DQN often rely on
next states generated by actions from past, potentially suboptimal, policy. As
a result, these states may not provide informative learning signals, causing
high variance into the update process. This issue is exacerbated when the
sampled transitions are poorly aligned with the agent's current policy. To
address this limitation, we propose the Successor-state Aggregation Deep
Q-Network (SADQ), which explicitly models environment dynamics using a
stochastic transition model. SADQ integrates successor-state distributions into
the Q-value estimation process, enabling more stable and policy-aligned value
updates. Additionally, it explores a more efficient action selection strategy
with the modeled transition structure. We provide theoretical guarantees that
SADQ maintains unbiased value estimates while reducing training variance. Our
extensive empirical results across standard RL benchmarks and real-world
vector-based control tasks demonstrate that SADQ consistently outperforms DQN
variants in both stability and learning efficiency.

</details>


### [81] [Benchmark Datasets for Lead-Lag Forecasting on Social Platforms](https://arxiv.org/abs/2511.03877)
*Kimia Kazemian,Zhenzhen Liu,Yangfanyu Yang,Katie Z Luo,Shuhan Gu,Audrey Du,Xinyu Yang,Jack Jansons,Kilian Q Weinberger,John Thickstun,Yian Yin,Sarah Dean*

Main category: cs.LG

TL;DR: 提出Lead - Lag Forecasting (LLF)问题，给出两个基准数据集，验证动态并给出回归基线，为LLF研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: LLF模式普遍存在但因缺乏标准数据集未被时间序列社区作为统一预测问题对待，需推动其研究。

Method: 提出arXiv和GitHub两个基准数据集，记录数据整理和清理技术细节，通过统计和分类测试验证lead - lag动态，对回归进行参数和非参数基线测试。

Result: 建立了LLF作为新的预测范式，为其在社交和使用数据中的系统探索奠定实证基础。

Conclusion: LLF是有研究价值的新范式，提供的数据集和研究成果可推动该领域进一步发展。

Abstract: Social and collaborative platforms emit multivariate time-series traces in
which early interactions-such as views, likes, or downloads-are followed,
sometimes months or years later, by higher impact like citations, sales, or
reviews. We formalize this setting as Lead-Lag Forecasting (LLF): given an
early usage channel (the lead), predict a correlated but temporally shifted
outcome channel (the lag). Despite the ubiquity of such patterns, LLF has not
been treated as a unified forecasting problem within the time-series community,
largely due to the absence of standardized datasets. To anchor research in LLF,
here we present two high-volume benchmark datasets-arXiv (accesses -> citations
of 2.3M papers) and GitHub (pushes/stars -> forks of 3M repositories)-and
outline additional domains with analogous lead-lag dynamics, including
Wikipedia (page views -> edits), Spotify (streams -> concert attendance),
e-commerce (click-throughs -> purchases), and LinkedIn profile (views ->
messages). Our datasets provide ideal testbeds for lead-lag forecasting, by
capturing long-horizon dynamics across years, spanning the full spectrum of
outcomes, and avoiding survivorship bias in sampling. We documented all
technical details of data curation and cleaning, verified the presence of
lead-lag dynamics through statistical and classification tests, and benchmarked
parametric and non-parametric baselines for regression. Our study establishes
LLF as a novel forecasting paradigm and lays an empirical foundation for its
systematic exploration in social and usage data. Our data portal with downloads
and documentation is available at https://lead-lag-forecasting.github.io/.

</details>


### [82] [Conditional Score Learning for Quickest Change Detection in Markov Transition Kernels](https://arxiv.org/abs/2511.03953)
*Wuxia Chen,Taposh Banerjee,Vahid Tarokh*

Main category: cs.LG

TL;DR: 本文解决未知转移核的马尔可夫过程中最快变化检测问题，提出基于分数的CUSUM程序，证明了误警平均时间的指数下界和检测延迟的渐近上界。


<details>
  <summary>Details</summary>
Motivation: 解决未知转移核的马尔可夫过程中最快变化检测问题。

Method: 直接从样本对中学习条件分数，避免显式似然评估；开发基于分数的CUSUM程序；提出统计量的截断版本；利用Hoeffding不等式证明相关界。

Result: 证明了误警平均时间的指数下界和检测延迟的渐近上界。

Conclusion: 为高维马尔可夫模型中基于分数的检测提供了理论保证和实际可行性。

Abstract: We address the problem of quickest change detection in Markov processes with
unknown transition kernels. The key idea is to learn the conditional score
$\nabla_{\mathbf{y}} \log p(\mathbf{y}|\mathbf{x})$ directly from sample pairs
$( \mathbf{x},\mathbf{y})$, where both $\mathbf{x}$ and $\mathbf{y}$ are
high-dimensional data generated by the same transition kernel. In this way, we
avoid explicit likelihood evaluation and provide a practical way to learn the
transition dynamics. Based on this estimation, we develop a score-based CUSUM
procedure that uses conditional Hyvarinen score differences to detect changes
in the kernel. To ensure bounded increments, we propose a truncated version of
the statistic. With Hoeffding's inequality for uniformly ergodic Markov
processes, we prove exponential lower bounds on the mean time to false alarm.
We also prove asymptotic upper bounds on detection delay. These results give
both theoretical guarantees and practical feasibility for score-based detection
in high-dimensional Markov models.

</details>


### [83] [DecoHD: Decomposed Hyperdimensional Classification under Extreme Memory Budgets](https://arxiv.org/abs/2511.03911)
*Sanggeon Yun,Hyunwoo Oh,Ryozo Masukawa,Mohsen Imani*

Main category: cs.LG

TL;DR: 本文提出DecoHD方法用于超维计算（HDC），能在压缩内存时保持性能，有显著能耗和速度优势。


<details>
  <summary>Details</summary>
Motivation: 现有HDC分解方法不适合压缩学习到的类原型，且特征轴缩减会影响集中度和鲁棒性，需要更好的压缩方法。

Method: 引入DecoHD，直接在分解的HDC参数化中学习，通过轻量级捆绑头沿类轴压缩，端到端训练，推理保持纯HDC。

Result: 在严格部署预算下，DecoHD能实现极端内存节省，仅伴随轻微精度下降，更抗噪声，减少大量可训练参数，并在硬件上有显著的能耗和速度提升。

Conclusion: DecoHD是一种有效的HDC压缩方法，能在节省内存的同时保持良好性能，适合内存/近内存加速器。

Abstract: Decomposition is a proven way to shrink deep networks without changing I/O.
We bring this idea to hyperdimensional computing (HDC), where footprint cuts
usually shrink the feature axis and erode concentration and robustness. Prior
HDC decompositions decode via fixed atomic hypervectors, which are ill-suited
for compressing learned class prototypes. We introduce DecoHD, which learns
directly in a decomposed HDC parameterization: a small, shared set of per-layer
channels with multiplicative binding across layers and bundling at the end,
yielding a large representational space from compact factors. DecoHD compresses
along the class axis via a lightweight bundling head while preserving native
bind-bundle-score; training is end-to-end, and inference remains pure HDC,
aligning with in/near-memory accelerators. In evaluation, DecoHD attains
extreme memory savings with only minor accuracy degradation under tight
deployment budgets. On average it stays within about 0.1-0.15% of a strong
non-reduced HDC baseline (worst case 5.7%), is more robust to random bit-flip
noise, reaches its accuracy plateau with up to ~97% fewer trainable parameters,
and -- in hardware -- delivers roughly 277x/35x energy/speed gains over a CPU
(AMD Ryzen 9 9950X), 13.5x/3.7x over a GPU (NVIDIA RTX 4090), and 2.0x/2.4x
over a baseline HDC ASIC.

</details>


### [84] [Non-Asymptotic Optimization and Generalization Bounds for Stochastic Gauss-Newton in Overparameterized Models](https://arxiv.org/abs/2511.03972)
*Semih Cayci*

Main category: cs.LG

TL;DR: 分析带Levenberg - Marquardt阻尼和小批量采样的随机高斯 - 牛顿（SGN）方法训练深度神经网络，给出有限时间收敛界和非渐近泛化界。


<details>
  <summary>Details</summary>
Motivation: 研究高阶优化方法对深度学习泛化性能的影响。

Method: 通过参数空间的可变度量分析建立有限时间收敛界，利用一致稳定性推导非渐近泛化界。

Result: 得到有限时间收敛界和非渐近泛化界，确定SGN有利泛化区域，该区域中高斯 - 牛顿矩阵最小特征值越大，稳定性界越紧。

Conclusion: 明确了SGN方法在训练深度神经网络时，曲率、批量大小和过参数化对泛化性能的影响。

Abstract: An important question in deep learning is how higher-order optimization
methods affect generalization. In this work, we analyze a stochastic
Gauss-Newton (SGN) method with Levenberg-Marquardt damping and mini-batch
sampling for training overparameterized deep neural networks with smooth
activations in a regression setting. Our theoretical contributions are twofold.
First, we establish finite-time convergence bounds via a variable-metric
analysis in parameter space, with explicit dependencies on the batch size,
network width and depth. Second, we derive non-asymptotic generalization bounds
for SGN using uniform stability in the overparameterized regime, characterizing
the impact of curvature, batch size, and overparameterization on generalization
performance. Our theoretical results identify a favorable generalization regime
for SGN in which a larger minimum eigenvalue of the Gauss-Newton matrix along
the optimization path yields tighter stability bounds.

</details>


### [85] [On Predicting Sociodemographics from Mobility Signals](https://arxiv.org/abs/2511.03924)
*Ekin Uğurel,Cynthia Chen,Brian H. Y. Lee,Filipe Rodrigues*

Main category: cs.LG

TL;DR: 本文从三个角度解决从移动数据推断社会人口属性的难题，引入高阶移动描述符、度量和可视化工具以及多任务学习框架，提升预测效果。


<details>
  <summary>Details</summary>
Motivation: 从移动数据推断社会人口属性有助于交通规划者利用被动收集的数据集，但该任务因移动模式与社会人口特征关系弱且不一致、跨环境泛化能力有限而困难。

Method: 一是引入基于有向移动图的高阶移动描述符；二是引入度量和可视化诊断工具；三是开发多任务学习框架。

Result: 高阶移动描述符显著提升对年龄、性别等属性的预测；度量和工具可量化不确定性；多任务学习框架在训练数据有限或跨时间段应用时优于单任务模型。

Conclusion: 提出的三种方法有效解决了从移动数据推断社会人口属性的难题，提升了预测准确性、泛化能力和样本效率。

Abstract: Inferring sociodemographic attributes from mobility data could help
transportation planners better leverage passively collected datasets, but this
task remains difficult due to weak and inconsistent relationships between
mobility patterns and sociodemographic traits, as well as limited
generalization across contexts. We address these challenges from three angles.
First, to improve predictive accuracy while retaining interpretability, we
introduce a behaviorally grounded set of higher-order mobility descriptors
based on directed mobility graphs. These features capture structured patterns
in trip sequences, travel modes, and social co-travel, and significantly
improve prediction of age, gender, income, and household structure over
baselines features. Second, we introduce metrics and visual diagnostic tools
that encourage evenness between model confidence and accuracy, enabling
planners to quantify uncertainty. Third, to improve generalization and sample
efficiency, we develop a multitask learning framework that jointly predicts
multiple sociodemographic attributes from a shared representation. This
approach outperforms single-task models, particularly when training data are
limited or when applying models across different time periods (i.e., when the
test set distribution differs from the training set).

</details>


### [86] [Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations](https://arxiv.org/abs/2511.04000)
*Kyaw Hpone Myint,Zhe Wu,Alexandre G. R. Day,Giri Iyengar*

Main category: cs.LG

TL;DR: 本文介绍为决策树元学习生成合成预训练数据的高效可扩展方法，能降低计算成本、增强数据生成灵活性。


<details>
  <summary>Details</summary>
Motivation: 决策树因可解释性在高风险领域广泛应用，需高效可扩展的合成预训练数据方法以实现元学习。

Method: 通过合成采样接近最优的决策树，创建大规模真实数据集，并使用MetaTree变压器架构。

Result: 该方法性能与在真实世界数据或使用计算昂贵的最优决策树进行预训练相当。

Conclusion: 此策略显著降低计算成本、增强数据生成灵活性，为可解释决策树模型的可扩展高效元学习铺平道路。

Abstract: Decision trees are widely used in high-stakes fields like finance and
healthcare due to their interpretability. This work introduces an efficient,
scalable method for generating synthetic pre-training data to enable
meta-learning of decision trees. Our approach samples near-optimal decision
trees synthetically, creating large-scale, realistic datasets. Using the
MetaTree transformer architecture, we demonstrate that this method achieves
performance comparable to pre-training on real-world data or with
computationally expensive optimal decision trees. This strategy significantly
reduces computational costs, enhances data generation flexibility, and paves
the way for scalable and efficient meta-learning of interpretable decision tree
models.

</details>


### [87] [SynQuE: Estimating Synthetic Dataset Quality Without Annotations](https://arxiv.org/abs/2511.03928)
*Arthur Chen,Victor Zhong*

Main category: cs.LG

TL;DR: 本文提出并形式化SynQuE问题，建立基准，引入代理指标，提出LENS代理，结果显示SynQuE代理与真实任务表现相关，LENS在复杂任务中表现优，确立SynQuE为数据稀缺时选择合成数据的实用框架。


<details>
  <summary>Details</summary>
Motivation: 解决因数据收集成本或隐私限制导致数据稀缺，如何对合成数据集按预期的现实任务表现进行排序的问题。

Method: 引入并评估代理指标；通过嵌入模型调整基于分布和多样性的距离度量来引入代理指标；提出利用大语言模型推理的LENS代理。

Result: SynQuE代理与不同任务的真实任务表现相关，LENS在复杂任务中始终表现更好，如在文本到SQL解析中，使用SynQuE代理选择的合成数据集训练可提高准确率。

Conclusion: 确立SynQuE为数据稀缺时合成数据选择的实用框架，推动基于基础模型的数据表征和细粒度数据选择的未来研究。

Abstract: We introduce and formalize the Synthetic Dataset Quality Estimation (SynQuE)
problem: ranking synthetic datasets by their expected real-world task
performance using only limited unannotated real data. This addresses a critical
and open challenge where data is scarce due to collection costs or privacy
constraints. We establish the first comprehensive benchmarks for this problem
by introducing and evaluating proxy metrics that choose synthetic data for
training to maximize task performance on real data. We introduce the first
proxy metrics for SynQuE by adapting distribution and diversity-based distance
measures to our context via embedding models. To address the shortcomings of
these metrics on complex planning tasks, we propose LENS, a novel proxy that
leverages large language model reasoning. Our results show that SynQuE proxies
correlate with real task performance across diverse tasks, including sentiment
analysis, Text2SQL, web navigation, and image classification, with LENS
consistently outperforming others on complex tasks by capturing nuanced
characteristics. For instance, on text-to-SQL parsing, training on the top-3
synthetic datasets selected via SynQuE proxies can raise accuracy from 30.4% to
38.4 (+8.1)% on average compared to selecting data indiscriminately. This work
establishes SynQuE as a practical framework for synthetic data selection under
real-data scarcity and motivates future research on foundation model-based data
characterization and fine-grained data selection.

</details>


### [88] [On Joint Regularization and Calibration in Deep Ensembles](https://arxiv.org/abs/2511.04160)
*Laurits Fredsgaard,Mikkel N. Schmidt*

Main category: cs.LG

TL;DR: 本文研究联合调参对深度集成模型性能和不确定性量化的影响，提出部分重叠保留策略，结果显示联合调参通常能匹配或提升性能，该策略是实用方案。


<details>
  <summary>Details</summary>
Motivation: 探索联合调整深度集成模型的参数（权重衰减、温度缩放和早停）对预测性能和不确定性量化的影响，寻找优化深度集成模型的方法。

Method: 研究联合调整权重衰减、温度缩放和早停，提出部分重叠保留策略用于联合评估和数据利用。

Result: 联合调参通常能匹配或提升性能，不同任务和指标效果有显著差异。

Conclusion: 强调个体和联合优化的权衡，部分重叠保留策略是实用解决方案，为优化深度集成模型提供有价值的见解和指导。

Abstract: Deep ensembles are a powerful tool in machine learning, improving both model
performance and uncertainty calibration. While ensembles are typically formed
by training and tuning models individually, evidence suggests that jointly
tuning the ensemble can lead to better performance. This paper investigates the
impact of jointly tuning weight decay, temperature scaling, and early stopping
on both predictive performance and uncertainty quantification. Additionally, we
propose a partially overlapping holdout strategy as a practical compromise
between enabling joint evaluation and maximizing the use of data for training.
Our results demonstrate that jointly tuning the ensemble generally matches or
improves performance, with significant variation in effect size across
different tasks and metrics. We highlight the trade-offs between individual and
joint optimization in deep ensemble training, with the overlapping holdout
strategy offering an attractive practical solution. We believe our findings
provide valuable insights and guidance for practitioners looking to optimize
deep ensemble models. Code is available at:
https://github.com/lauritsf/ensemble-optimality-gap

</details>


### [89] [NVIDIA Nemotron Nano V2 VL](https://arxiv.org/abs/2511.03929)
*NVIDIA,:,Amala Sanjay Deshmukh,Kateryna Chumachenko,Tuomas Rintamaki,Matthieu Le,Tyler Poon,Danial Mohseni Taheri,Ilia Karmanov,Guilin Liu,Jarno Seppanen,Guo Chen,Karan Sapra,Zhiding Yu,Adi Renduchintala,Charles Wang,Peter Jin,Arushi Goel,Mike Ranzinger,Lukas Voegtle,Philipp Fischer,Timo Roman,Wei Ping,Boxin Wang,Zhuolin Yang,Nayeon Lee,Shaokun Zhang,Fuxiao Liu,Zhiqi Li,Di Zhang,Greg Heinrich,Hongxu,Yin,Song Han,Pavlo Molchanov,Parth Mannan,Yao Xu,Jane Polak Scowcroft,Tom Balough,Subhashree Radhakrishnan,Paris Zhang,Sean Cha,Ratnesh Kumar,Zaid Pervaiz Bhat,Jian Zhang,Darragh Hanley,Pritam Biswas,Jesse Oliver,Kevin Vasques,Roger Waleffe,Duncan Riach,Oluwatobi Olabiyi,Ameya Sunil Mahabaleshwarkar,Bilal Kartal,Pritam Gundecha,Khanh Nguyen,Alexandre Milesi,Eugene Khvedchenia,Ran Zilberstein,Ofri Masad,Natan Bagrov,Nave Assaf,Tomer Asida,Daniel Afrimi,Amit Zuker,Netanel Haber,Zhiyu Cheng,Jingyu,Xin,Di,Wu,Nik Spirin,Maryam Moosaei,Roman Ageev,Vanshil Atul Shah,Yuting Wu,Daniel Korzekwa,Unnikrishnan Kizhakkemadam Sreekumar,Wanli Jiang,Padmavathy Subramanian,Alejandra Rico,Sandip Bhaskar,Saeid Motiian,Kedi Wu,Annie Surla,Chia-Chih Chen,Hayden Wolff,Matthew Feinberg,Melissa Corpuz,Marek Wawrzos,Eileen Long,Aastha Jhunjhunwala,Paul Hendricks,Farzan Memarian,Benika Hall,Xin-Yu Wang,David Mosallanezhad,Soumye Singhal,Luis Vega,Katherine Cheung,Krzysztof Pawelec,Michael Evans,Katherine Luna,Jie Lou,Erick Galinkin,Akshay Hazare,Kaustubh Purandare,Ann Guan,Anna Warno,Chen Cui,Yoshi Suhara,Shibani Likhite,Seph Mard,Meredith Price,Laya Sleiman,Saori Kaji,Udi Karpas,Kari Briski,Joey Conway,Michael Lightstone,Jan Kautz,Mohammad Shoeybi,Mostofa Patwary,Jonathen Cohen,Oleksii Kuchaiev,Andrew Tao,Bryan Catanzaro*

Main category: cs.LG

TL;DR: 介绍用于现实文档理解、长视频理解和推理任务的Nemotron Nano V2 VL模型，相比前代有显著改进，还将发布模型检查点等。


<details>
  <summary>Details</summary>
Motivation: 设计能进行强现实文档理解、长视频理解和推理任务的模型，并在各视觉和文本领域超越前代模型。

Method: 在模型架构、数据集和训练方法上进行重大改进，基于混合Mamba - Transformer LLM的Nemotron Nano V2和创新的令牌减少技术。

Result: Nemotron Nano V2 VL在各视觉和文本领域比前代模型有显著提升，能在长文档和视频场景实现更高推理吞吐量。

Conclusion: 推出Nemotron Nano V2 VL模型，并将发布模型检查点，共享部分数据集、训练方法和代码。

Abstract: We introduce Nemotron Nano V2 VL, the latest model of the Nemotron
vision-language series designed for strong real-world document understanding,
long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers
significant improvements over our previous model,
Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major
enhancements in model architecture, datasets, and training recipes. Nemotron
Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and
innovative token reduction techniques to achieve higher inference throughput in
long document and video scenarios. We are releasing model checkpoints in BF16,
FP8, and FP4 formats and sharing large parts of our datasets, recipes and
training code.

</details>


### [90] [ForecastGAN: A Decomposition-Based Adversarial Framework for Multi-Horizon Time Series Forecasting](https://arxiv.org/abs/2511.04445)
*Syeda Sitara Wishal Fatima,Afshin Rahimi*

Main category: cs.LG

TL;DR: 本文提出ForecastGAN用于多步时间序列预测，在11个数据集验证，短期预测优于SOTA，长期有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多步预测存在局限，transformer在短期表现不佳且常忽略分类特征。

Method: 提出ForecastGAN，含分解、模型选择和对抗训练三个模块，有效整合数值和分类特征。

Result: 在11个多变量时间序列数据集验证，短期预测始终优于SOTA的transformer模型，长期也有竞争力。

Conclusion: 建立了一种更具泛化性的时间序列预测方法，适应特定情境，无需大量超参数调整。

Abstract: Time series forecasting is essential across domains from finance to supply
chain management. This paper introduces ForecastGAN, a novel decomposition
based adversarial framework addressing limitations in existing approaches for
multi-horizon predictions. Although transformer models excel in long-term
forecasting, they often underperform in short-term scenarios and typically
ignore categorical features. ForecastGAN operates through three integrated
modules: a Decomposition Module that extracts seasonality and trend components;
a Model Selection Module that identifies optimal neural network configurations
based on forecasting horizon; and an Adversarial Training Module that enhances
prediction robustness through Conditional Generative Adversarial Network
training. Unlike conventional approaches, ForecastGAN effectively integrates
both numerical and categorical features. We validate our framework on eleven
benchmark multivariate time series datasets that span various forecasting
horizons. The results show that ForecastGAN consistently outperforms
state-of-the-art transformer models for short-term forecasting while remaining
competitive for long-term horizons. This research establishes a more
generalizable approach to time series forecasting that adapts to specific
contexts while maintaining strong performance across diverse data
characteristics without extensive hyperparameter tuning.

</details>


### [91] [LogHD: Robust Compression of Hyperdimensional Classifiers via Logarithmic Class-Axis Reduction](https://arxiv.org/abs/2511.03938)
*Sanggeon Yun,Hyunwoo Oh,Ryozo Masukawa,Pietro Mercati,Nathaniel D. Bastian,Mohsen Imani*

Main category: cs.LG

TL;DR: 提出LogHD方法，对数减少类轴，削减内存，在不同数据集和比特翻转下有竞争力，ASIC实例能效和速度提升显著。


<details>
  <summary>Details</summary>
Motivation: 标准超维计算设计内存需求高，先前压缩方法会削弱鲁棒性。

Method: 引入LogHD，用对数类轴减少，使用容量感知码本和基于轮廓的解码，可与特征轴稀疏化结合。

Result: 在不同数据集和比特翻转下，小模型有竞争力的准确率和更高弹性；同等内存下承受更高比特翻转率；ASIC实例能效和速度大幅提升。

Conclusion: LogHD能有效减少内存，同时在准确率、弹性、能效和速度方面表现出色。

Abstract: Hyperdimensional computing (HDC) suits memory, energy, and
reliability-constrained systems, yet the standard "one prototype per class"
design requires $O(CD)$ memory (with $C$ classes and dimensionality $D$). Prior
compaction reduces $D$ (feature axis), improving storage/compute but weakening
robustness. We introduce LogHD, a logarithmic class-axis reduction that
replaces the $C$ per-class prototypes with $n\!\approx\!\lceil\log_k C\rceil$
bundle hypervectors (alphabet size $k$) and decodes in an $n$-dimensional
activation space, cutting memory to $O(D\log_k C)$ while preserving $D$. LogHD
uses a capacity-aware codebook and profile-based decoding, and composes with
feature-axis sparsification. Across datasets and injected bit flips, LogHD
attains competitive accuracy with smaller models and higher resilience at
matched memory. Under equal memory, it sustains target accuracy at roughly
$2.5$-$3.0\times$ higher bit-flip rates than feature-axis compression; an ASIC
instantiation delivers $498\times$ energy efficiency and $62.6\times$ speedup
over an AMD Ryzen 9 9950X and $24.3\times$/$6.58\times$ over an NVIDIA RTX
4090, and is $4.06\times$ more energy-efficient and $2.19\times$ faster than a
feature-axis HDC ASIC baseline.

</details>


### [92] [Comparing EPGP Surrogates and Finite Elements Under Degree-of-Freedom Parity](https://arxiv.org/abs/2511.04518)
*Obed Amo,Samit Ghosh,Markus Lange-Hegermann,Bogdan Raiţă,Michael Pokojovy*

Main category: cs.LG

TL;DR: 本文比较了B - EPGP代理和CN - FEM求解二维波动方程的性能，B - EPGP在匹配自由度下误差更低。


<details>
  <summary>Details</summary>
Motivation: 比较B - EPGP代理和CN - FEM求解二维波动方程的性能。

Method: B - EPGP利用指数多项式基精确执行PDE和边界条件，用惩罚最小二乘法估计系数，引入自由度匹配协议确保公平比较。

Result: 在匹配自由度下，B - EPGP的时空$L^2$误差和空间中的最大时间$L^{2}$误差始终低于CN - FEM，精度提高约两个数量级。

Conclusion: B - EPGP在求解二维波动方程上比CN - FEM更具精度优势。

Abstract: We present a new benchmarking study comparing a boundary-constrained
Ehrenpreis--Palamodov Gaussian Process (B-EPGP) surrogate with a classical
finite element method combined with Crank--Nicolson time stepping (CN-FEM) for
solving the two-dimensional wave equation with homogeneous Dirichlet boundary
conditions. The B-EPGP construction leverages exponential-polynomial bases
derived from the characteristic variety to enforce the PDE and boundary
conditions exactly and employs penalized least squares to estimate the
coefficients. To ensure fairness across paradigms, we introduce a
degrees-of-freedom (DoF) matching protocol. Under matched DoF, B-EPGP
consistently attains lower space-time $L^2$-error and maximum-in-time
$L^{2}$-error in space than CN-FEM, improving accuracy by roughly two orders of
magnitude.

</details>


### [93] [RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods](https://arxiv.org/abs/2511.03939)
*Raghav Sharma,Manan Mehta,Sai Tiger Raina*

Main category: cs.LG

TL;DR: 本文综述大语言模型对齐研究新进展，涵盖多模态对齐、文化公平和低延迟优化等领域，分析技术并指出挑战。


<details>
  <summary>Details</summary>
Motivation: RLHF是大语言模型对齐标准，但当前研究已超越传统文本方法，需填补多模态对齐、文化公平和低延迟优化等方面的研究空白。

Method: 先回顾PPO、DPO和GRPO等基础算法，再详细分析最新创新成果。

Result: 对各类对齐技术进行比较综合分析，给出相关领域的研究进展。

Conclusion: 为构建更稳健、高效和公平的AI系统的研究人员提供重要指引。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is the standard for
aligning Large Language Models (LLMs), yet recent progress has moved beyond
canonical text-based methods. This survey synthesizes the new frontier of
alignment research by addressing critical gaps in multi-modal alignment,
cultural fairness, and low-latency optimization. To systematically explore
these domains, we first review foundational algo- rithms, including PPO, DPO,
and GRPO, before presenting a detailed analysis of the latest innovations. By
providing a comparative synthesis of these techniques and outlining open
challenges, this work serves as an essential roadmap for researchers building
more robust, efficient, and equitable AI systems.

</details>


### [94] [PrivacyCD: Hierarchical Unlearning for Protecting Student Privacy in Cognitive Diagnosis](https://arxiv.org/abs/2511.03966)
*Mingliang Hou,Yinuo Wang,Teng Guo,Zitao Liu,Wenzhou Dou,Jiaqi Zheng,Renqiang Luo,Mi Tian,Weiqi Luo*

Main category: cs.LG

TL;DR: 文章针对认知诊断（CD）模型数据删除需求，提出分层重要性引导遗忘（HIF）算法，实验表明其效果显著。


<details>
  <summary>Details</summary>
Motivation: 用户对“被遗忘权”的重视使CD模型需移除特定学生数据，但现有模型缺乏隐私考虑和有效数据遗忘机制，通用算法不适用。

Method: 提出分层重要性引导遗忘（HIF）算法，利用参数重要性的层特征，通过创新平滑机制结合个体和层重要性。

Result: 在三个真实数据集上的实验显示，HIF在关键指标上显著优于基线。

Conclusion: HIF为CD模型响应数据删除请求和部署高性能、隐私保护的AI系统提供了首个有效解决方案。

Abstract: The need to remove specific student data from cognitive diagnosis (CD) models
has become a pressing requirement, driven by users' growing assertion of their
"right to be forgotten". However, existing CD models are largely designed
without privacy considerations and lack effective data unlearning mechanisms.
Directly applying general purpose unlearning algorithms is suboptimal, as they
struggle to balance unlearning completeness, model utility, and efficiency when
confronted with the unique heterogeneous structure of CD models. To address
this, our paper presents the first systematic study of the data unlearning
problem for CD models, proposing a novel and efficient algorithm: hierarchical
importanceguided forgetting (HIF). Our key insight is that parameter importance
in CD models exhibits distinct layer wise characteristics. HIF leverages this
via an innovative smoothing mechanism that combines individual and layer, level
importance, enabling a more precise distinction of parameters associated with
the data to be unlearned. Experiments on three real world datasets show that
HIF significantly outperforms baselines on key metrics, offering the first
effective solution for CD models to respond to user data removal requests and
for deploying high-performance, privacy preserving AI systems

</details>


### [95] [Forgetting is Everywhere](https://arxiv.org/abs/2511.04666)
*Ben Sanati,Thomas L. Lee,Trevor McInroe,Aidan Scannell,Nikolay Malkin,David Abel,Amos Storkey*

Main category: cs.LG

TL;DR: 提出一种与算法和任务无关的遗忘理论，设计实验验证，为分析和改进通用学习算法的信息保留能力奠定基础。


<details>
  <summary>Details</summary>
Motivation: 通用学习算法在适应新数据时会遗忘旧知识，且缺乏对遗忘的统一定义和对潜在学习动态的洞察。

Method: 提出将遗忘特征化为学习者对未来经验的预测分布缺乏自一致性的理论，设计涵盖多种学习类型的实验。

Result: 实证表明遗忘存在于所有学习环境中，且对学习效率有重要影响。

Conclusion: 建立了对遗忘的原则性理解，为分析和改进通用学习算法的信息保留能力奠定基础。

Abstract: A fundamental challenge in developing general learning algorithms is their
tendency to forget past knowledge when adapting to new data. Addressing this
problem requires a principled understanding of forgetting; yet, despite decades
of study, no unified definition has emerged that provides insights into the
underlying dynamics of learning. We propose an algorithm- and task-agnostic
theory that characterises forgetting as a lack of self-consistency in a
learner's predictive distribution over future experiences, manifesting as a
loss of predictive information. Our theory naturally yields a general measure
of an algorithm's propensity to forget. To validate the theory, we design a
comprehensive set of experiments that span classification, regression,
generative modelling, and reinforcement learning. We empirically demonstrate
how forgetting is present across all learning settings and plays a significant
role in determining learning efficiency. Together, these results establish a
principled understanding of forgetting and lay the foundation for analysing and
improving the information retention capabilities of general learning
algorithms.

</details>


### [96] [PETRA: Pretrained Evolutionary Transformer for SARS-CoV-2 Mutation Prediction](https://arxiv.org/abs/2511.03976)
*Xu Zou*

Main category: cs.LG

TL;DR: 本文介绍新型变压器方法PETRA，基于系统发育树进化轨迹，能减轻测序噪音并捕捉病毒进化层次结构，在预测SARS - CoV - 2未来突变上表现出色。


<details>
  <summary>Details</summary>
Motivation: SARS - CoV - 2快速不可预测进化及免疫逃逸变体不断出现给公共卫生和疫苗开发带来挑战，大语言模型在处理病毒基因组序列上有局限。

Method: 引入基于系统发育树进化轨迹的PETRA方法，采用加权训练框架处理全球序列数据地理和时间不平衡问题。

Result: PETRA在预测SARS - CoV - 2未来突变上表现优异，核苷酸突变加权召回率达9.45%，刺突氨基酸突变达17.10%，高于基线；能辅助主要分支实时突变预测。

Conclusion: PETRA方法有效，代码已开源。

Abstract: Since its emergence, SARS-CoV-2 has demonstrated a rapid and unpredictable
evolutionary trajectory, characterized by the continual emergence of
immune-evasive variants. This poses persistent challenges to public health and
vaccine development.
  While large-scale generative pre-trained transformers (GPTs) have
revolutionized the modeling of sequential data, their direct applications to
noisy viral genomic sequences are limited. In this paper, we introduce
PETRA(Pretrained Evolutionary TRAnsformer), a novel transformer approach based
on evolutionary trajectories derived from phylogenetic trees rather than raw
RNA sequences. This method effectively mitigates sequencing noise and captures
the hierarchical structure of viral evolution.
  With a weighted training framework to address substantial geographical and
temporal imbalances in global sequence data, PETRA excels in predicting future
SARS-CoV-2 mutations, achieving a weighted recall@1 of 9.45% for nucleotide
mutations and 17.10\% for spike amino-acid mutations, compared to 0.49% and
6.64% respectively for the best baseline. PETRA also demonstrates its ability
to aid in the real-time mutation prediction of major clades like 24F(XEC) and
25A(LP.8.1). The code is open sourced on https://github.com/xz-keg/PETra

</details>


### [97] [Structural Priors and Modular Adapters in the Composable Fine-Tuning Algorithm of Large-Scale Models](https://arxiv.org/abs/2511.03981)
*Yuxiao Wang,Di Wu,Feng Liu,Zhimin Qiu,Chenrui Hu*

Main category: cs.LG

TL;DR: 提出可组合微调方法，结合图结构先验与模块化适配器解决大模型多任务适应问题，实验验证其性能优势。


<details>
  <summary>Details</summary>
Motivation: 解决大规模预训练模型在多任务适应中面临的高计算成本和结构不稳定问题。

Method: 引入关系矩阵建模任务依赖，将图结构先验编码；通过低秩映射和可插拔机制将模块化适配器嵌入不同层。

Result: 该框架显著提高任务预测准确性、适配器权重分配精度和整体计算效率，保持模型轻量级设计。

Conclusion: 图先验和模块化机制在可组合微调中具有协同优势。

Abstract: This paper proposes a composable fine-tuning method that integrates graph
structural priors with modular adapters to address the high computational cost
and structural instability faced by large-scale pre-trained models in
multi-task adaptation. The method introduces a relation matrix to model
dependencies among tasks, explicitly encoding correlations between nodes and
paths into graph structural priors, which provide unified structural
constraints for adapter weight allocation and path selection. Modular adapters
are embedded into different layers through low-rank mapping and a pluggable
mechanism, enabling efficient cross-task composition and reuse under prior
guidance. This mechanism not only improves parameter efficiency and training
stability but also alleviates path conflicts and redundant computation in
multi-task scenarios. Furthermore, experiments on hyperparameter sensitivity,
environmental sensitivity, and data sensitivity are conducted to systematically
analyze key factors such as routing temperature, gating thresholds, and
relation matrix regularization strength, verifying the consistency and superior
performance of the method under structural constraints. The results demonstrate
that the proposed framework significantly enhances task prediction accuracy,
adapter weight allocation precision, and overall computational efficiency while
maintaining model lightweight design, highlighting the synergistic advantages
of graph priors and modular mechanisms in composable fine-tuning.

</details>


### [98] [TwIST: Rigging the Lottery in Transformers with Independent Subnetwork Training](https://arxiv.org/abs/2511.03983)
*Michael Menezes,Barbara Su,Xinze Feng,Yehya Farhat,Hamza Shili,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: 介绍TwIST分布式训练框架用于高效大语言模型稀疏化，能在部署时零成本剪枝，性能佳且有实际推理加速和内存减少效果。


<details>
  <summary>Details</summary>
Motivation: 实现高效大语言模型稀疏化，避免后训练程序如校准或基于海森矩阵的恢复。

Method: 并行训练多个子网络，定期聚合参数并在训练中重新采样新子网络。

Result: 能在部署时零成本剪枝，困惑度与最先进的后训练稀疏化方法有竞争力，在高稀疏度下显著优于基线方法，产生结构化密集矩阵有推理加速和内存减少效果。

Conclusion: TwIST提供了一条无需额外微调或恢复开销的可部署稀疏大语言模型的高效训练路径。

Abstract: We introduce TwIST, a distributed training framework for efficient large
language model (LLM) sparsification. TwIST trains multiple subnetworks in
parallel, periodically aggregates their parameters, and resamples new
subnetworks during training. This process identifies high-quality subnetworks
("golden tickets") without requiring post-training procedures such as
calibration or Hessian-based recovery. As a result, TwIST enables zero-cost
pruning at deployment time while achieving perplexity competitive with
state-of-the-art post-training sparsification methods. The benefits are most
pronounced under aggressive sparsity (e.g., 50%+), where TwIST significantly
outperforms baseline methods; for example, reaching 23.14 PPL compared to 31.64
for the closest prior approach. Unlike unstructured pruning, TwIST produces
structured, dense matrices that offer practical inference speedups and memory
reductions on commodity hardware (e.g., CPUs) that do not support efficient
sparse computation. TwIST provides an efficient training-time path to
deployable sparse LLMs without additional fine-tuning or recovery overhead.

</details>


### [99] [Use of Continuous Glucose Monitoring with Machine Learning to Identify Metabolic Subphenotypes and Inform Precision Lifestyle Changes](https://arxiv.org/abs/2511.03986)
*Ahmed A. Metwally,Heyjun Park,Yue Wu,Tracey McLaughlin,Michael P. Snyder*

Main category: cs.LG

TL;DR: 文章指出静态血糖阈值分类法有局限，连续血糖监测和可穿戴技术能实现向动态代谢表型分析的转变，机器学习模型可利用血糖数据预测指标，还能用于营养分析和生活方式干预，推动精准糖尿病预防。


<details>
  <summary>Details</summary>
Motivation: 解决静态血糖阈值分类法掩盖血糖异常病理生理异质性的问题，实现精准糖尿病预防。

Method: 利用连续血糖监测和可穿戴技术获取数据，运用机器学习模型分析，结合生活方式数据研究。

Result: 机器学习模型能准确预测肌肉胰岛素抵抗和β细胞功能，个体餐后血糖反应可作为代谢亚型生物标志物，发现生活方式与代谢功能的关联，饮食缓解剂效果因表型而异。

Conclusion: 连续血糖监测可将早期血糖异常复杂性分解为可操作子表型，为精准糖尿病预防策略开辟新途径。

Abstract: The classification of diabetes and prediabetes by static glucose thresholds
obscures the pathophysiological dysglycemia heterogeneity, primarily driven by
insulin resistance (IR), beta-cell dysfunction, and incretin deficiency. This
review demonstrates that continuous glucose monitoring and wearable
technologies enable a paradigm shift towards non-invasive, dynamic metabolic
phenotyping. We show evidence that machine learning models can leverage
high-resolution glucose data from at-home, CGM-enabled oral glucose tolerance
tests to accurately predict gold-standard measures of muscle IR and beta-cell
function. This personalized characterization extends to real-world nutrition,
where an individual's unique postprandial glycemic response (PPGR) to
standardized meals, such as the relative glucose spike to potatoes versus
grapes, could serve as a biomarker for their metabolic subtype. Moreover,
integrating wearable data reveals that habitual diet, sleep, and physical
activity patterns, particularly their timing, are uniquely associated with
specific metabolic dysfunctions, informing precision lifestyle interventions.
The efficacy of dietary mitigators in attenuating PPGR is also shown to be
phenotype-dependent. Collectively, this evidence demonstrates that CGM can
deconstruct the complexity of early dysglycemia into distinct, actionable
subphenotypes. This approach moves beyond simple glycemic control, paving the
way for targeted nutritional, behavioral, and pharmacological strategies
tailored to an individual's core metabolic defects, thereby paving the way for
a new era of precision diabetes prevention.

</details>


### [100] [Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing](https://arxiv.org/abs/2511.04002)
*Mingyu Sung,Vikas Palakonda,Suhwan Im,Sunghwan Moon,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.LG

TL;DR: 现有方法难以支持大语言模型在边缘设备部署，本文提出自回归感知的分割计算框架，有三项关键贡献，评估显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因参数和内存问题难以部署在资源受限的物联网设备上，现有分割计算方法未解决自回归推理的独特挑战。

Method: 提出一点分割压缩（OPSC）混合精度量化方案；提出结合阈值分割（TS）和逐令牌自适应比特量化（TAB - Q）的两阶段中间压缩管道；制定统一优化框架联合选择最佳分割点、量化设置和序列长度。

Result: 在不同大语言模型和硬件平台上评估，相比现有量化方法有优越性能，实现1.49倍推理加速，显著减少通信开销，维持或提高模型精度。

Conclusion: 所提出的自回归感知分割计算框架有效解决大语言模型在边缘设备部署问题，性能良好。

Abstract: Large language models (LLMs) have achieved near-human performance across
diverse reasoning tasks, yet their deployment on resource-constrained
Internet-of-Things (IoT) devices remains impractical due to massive parameter
footprints and memory-intensive autoregressive decoding. While split computing
offers a promising solution by partitioning model execution between edge
devices and cloud servers, existing approaches fail to address the unique
challenges of autoregressive inference, particularly the iterative token
generation process and expanding key-value (KV) cache requirements. This work
introduces the first autoregressive-aware split computing framework designed
explicitly for LLM deployment on edge devices. Our approach makes three key
contributions. First, we develop one-point split compression (OPSC), a
mixed-precision quantization scheme that prevents out-of-memory failures by
strategically partitioning models into front-end and back-end segments with
different precision levels. Second, we propose a two-stage intermediate
compression pipeline that combines threshold splitting (TS) and token-wise
adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations
while dramatically reducing communication overhead. Third, we formulate a
unified optimization framework that jointly selects optimal split points,
quantization settings, and sequence lengths to satisfy strict memory and
latency constraints. Extensive evaluations across diverse LLMs and hardware
platforms demonstrate superior performance compared to state-of-the-art
quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework
achieves a 1.49 inference speedup and significant communication overhead
reduction while maintaining or improving model accuracy.

</details>


### [101] [DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization](https://arxiv.org/abs/2511.04063)
*Yuantian Shao,Yuanteng Chen,Peisong Wang,Jianlin Yu,Jing Lin,Yiwu Yao,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 提出高效分布感知旋转校准方法DartQuant和QR - Orth优化方案，在模型量化实验中表现出色，实现加速和内存节省，使大语言模型量化在资源受限环境可行。


<details>
  <summary>Details</summary>
Motivation: 现有旋转优化算法端到端微调计算成本高且易过拟合，需改进。

Method: 提出DartQuant方法，通过约束旋转后激活值分布降低旋转优化复杂度，减少对特定任务损失的依赖；引入QR - Orth优化方案替代昂贵的交替优化。

Result: 在多种模型量化实验中表现优越，70B模型旋转优化实现47×加速和10×内存节省，首次在单3090 GPU上完成70B模型旋转校准。

Conclusion: DartQuant方法有效，使大语言模型量化在资源受限环境中可行，代码公开。

Abstract: Quantization plays a crucial role in accelerating the inference of
large-scale models, and rotational matrices have been shown to effectively
improve quantization performance by smoothing outliers. However, end-to-end
fine-tuning of rotational optimization algorithms incurs high computational
costs and is prone to overfitting. To address this challenge, we propose an
efficient distribution-aware rotational calibration method, DartQuant, which
reduces the complexity of rotational optimization by constraining the
distribution of the activations after rotation. This approach also effectively
reduces reliance on task-specific losses, thereby mitigating the risk of
overfitting. Additionally, we introduce the QR-Orth optimization scheme, which
replaces expensive alternating optimization with a more efficient solution. In
a variety of model quantization experiments, DartQuant demonstrates superior
performance. Compared to existing methods, it achieves 47$\times$ acceleration
and 10$\times$ memory savings for rotational optimization on a 70B model.
Furthermore, it is the first to successfully complete rotational calibration
for a 70B model on a single 3090 GPU, making quantization of large language
models feasible in resource-constrained environments. Code is available at
https://github.com/CAS-CLab/DartQuant.git.

</details>


### [102] [DeNoise: Learning Robust Graph Representations for Unsupervised Graph-Level Anomaly Detection](https://arxiv.org/abs/2511.04086)
*Qingfeng Chen,Haojin Zeng,Jingyi Jie,Shichao Zhang,Debo Cheng*

Main category: cs.LG

TL;DR: 针对含异常图污染的训练数据，提出DeNoise框架用于无监督图级异常检测，实验表明其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络方法假设训练集干净，但实际中训练集常被异常图污染，影响性能，需解决该问题。

Method: 提出DeNoise框架，通过对抗目标联合优化图级编码器、属性解码器和结构解码器学习抗噪嵌入，引入编码器锚点对齐去噪机制，还有对比学习组件。

Result: 在八个真实数据集上实验，DeNoise能在不同噪声强度下学习可靠图级表示，显著优于现有无监督图级异常检测基线。

Conclusion: DeNoise是一种有效的无监督图级异常检测框架，适用于含污染的训练数据。

Abstract: With the rapid growth of graph-structured data in critical domains,
unsupervised graph-level anomaly detection (UGAD) has become a pivotal task.
UGAD seeks to identify entire graphs that deviate from normal behavioral
patterns. However, most Graph Neural Network (GNN) approaches implicitly assume
that the training set is clean, containing only normal graphs, which is rarely
true in practice. Even modest contamination by anomalous graphs can distort
learned representations and sharply degrade performance. To address this
challenge, we propose DeNoise, a robust UGAD framework explicitly designed for
contaminated training data. It jointly optimizes a graph-level encoder, an
attribute decoder, and a structure decoder via an adversarial objective to
learn noise-resistant embeddings. Further, DeNoise introduces an encoder
anchor-alignment denoising mechanism that fuses high-information node
embeddings from normal graphs into all graph embeddings, improving
representation quality while suppressing anomaly interference. A contrastive
learning component then compacts normal graph embeddings and repels anomalous
ones in the latent space. Extensive experiments on eight real-world datasets
demonstrate that DeNoise consistently learns reliable graph-level
representations under varying noise intensities and significantly outperforms
state-of-the-art UGAD baselines.

</details>


### [103] [KoTaP: A Panel Dataset for Corporate Tax Avoidance, Performance, and Governance in Korea](https://arxiv.org/abs/2511.04094)
*Hyungjong Na,Wonho Song,Seungyong Han,Donghyeon Jo,Sejin Myung,Hyungjoon Kim*

Main category: cs.LG

TL;DR: 本文介绍韩国避税面板数据集KoTaP，含2011 - 2024年非金融企业数据，可用于多领域研究和分析，是重要开放资源。


<details>
  <summary>Details</summary>
Motivation: 构建一个可将企业避税作为预测变量，关联多个领域的长期面板数据集，以支持会计、金融及跨学科研究。

Method: 从KOSPI和KOSDAQ上市非金融企业选取数据，排除特定企业，用CETR、GETR等指标衡量避税，构建平衡面板结构。

Result: 得到含12,653个公司 - 年度观测值、来自1,754家公司的KoTaP数据集，具有国际可比性和韩国企业特色。

Conclusion: KoTaP可用于模型评估、政策分析等，是重要开放资源。

Abstract: This study introduces the Korean Tax Avoidance Panel (KoTaP), a long-term
panel dataset of non-financial firms listed on KOSPI and KOSDAQ between 2011
and 2024. After excluding financial firms, firms with non-December fiscal year
ends, capital impairment, and negative pre-tax income, the final dataset
consists of 12,653 firm-year observations from 1,754 firms. KoTaP is designed
to treat corporate tax avoidance as a predictor variable and link it to
multiple domains, including earnings management (accrual- and activity-based),
profitability (ROA, ROE, CFO, LOSS), stability (LEV, CUR, SIZE, PPE, AGE,
INVREC), growth (GRW, MB, TQ), and governance (BIG4, FORN, OWN). Tax avoidance
itself is measured using complementary indicators cash effective tax rate
(CETR), GAAP effective tax rate (GETR), and book-tax difference measures (TSTA,
TSDA) with adjustments to ensure interpretability. A key strength of KoTaP is
its balanced panel structure with standardized variables and its consistency
with international literature on the distribution and correlation of core
indicators. At the same time, it reflects distinctive institutional features of
Korean firms, such as concentrated ownership, high foreign shareholding, and
elevated liquidity ratios, providing both international comparability and
contextual uniqueness. KoTaP enables applications in benchmarking econometric
and deep learning models, external validity checks, and explainable AI
analyses. It further supports policy evaluation, audit planning, and investment
analysis, making it a critical open resource for accounting, finance, and
interdisciplinary research.

</details>


### [104] [Decomposable Neuro Symbolic Regression](https://arxiv.org/abs/2511.04124)
*Giorgio Morales,John W. Sheppard*

Main category: cs.LG

TL;DR: 提出可分解的符号回归方法，利用多种技术生成可解释的多元表达式，在不同噪声问题上表现良好。


<details>
  <summary>Details</summary>
Motivation: 多数符号回归方法优先考虑最小化预测误差而非识别控制方程，常产生过于复杂或不准确的表达式，需改进。

Method: 使用Multi - Set Transformer生成多个单变量符号骨架，用基于GA的方法评估并选择高质量候选者，通过基于GP的级联程序合并，最后用GA进行系数优化。

Result: 在不同噪声问题上，与两种基于GP的方法、三种神经SR方法和一种混合方法相比，插值和外推误差更低或相当，且能一致学习到与原始数学结构匹配的表达式。

Conclusion: 提出的可分解符号回归方法有效，能生成可解释的多元表达式，在性能和表达式匹配度上有优势。

Abstract: Symbolic regression (SR) models complex systems by discovering mathematical
expressions that capture underlying relationships in observed data. However,
most SR methods prioritize minimizing prediction error over identifying the
governing equations, often producing overly complex or inaccurate expressions.
To address this, we present a decomposable SR method that generates
interpretable multivariate expressions leveraging transformer models, genetic
algorithms (GAs), and genetic programming (GP). In particular, our explainable
SR method distills a trained ``opaque'' regression model into mathematical
expressions that serve as explanations of its computed function. Our method
employs a Multi-Set Transformer to generate multiple univariate symbolic
skeletons that characterize how each variable influences the opaque model's
response. We then evaluate the generated skeletons' performance using a
GA-based approach to select a subset of high-quality candidates before
incrementally merging them via a GP-based cascade procedure that preserves
their original skeleton structure. The final multivariate skeletons undergo
coefficient optimization via a GA. We evaluated our method on problems with
controlled and varying degrees of noise, demonstrating lower or comparable
interpolation and extrapolation errors compared to two GP-based methods, three
neural SR methods, and a hybrid approach. Unlike them, our approach
consistently learned expressions that matched the original mathematical
structure.

</details>


### [105] [Exploring the Feasibility of End-to-End Large Language Model as a Compiler](https://arxiv.org/abs/2511.04132)
*Hongbin Zhang,Shihao Gao,Yang Liu,Mingjie Xing,Yanjun Wu,Chen Zhao*

Main category: cs.LG

TL;DR: 本文探讨大语言模型作为编译器（LaaC）的可行性与未来方向，设计评估框架，实验表明当前编译成功率低，经优化可提升代码质量，对LaaC前景乐观。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型已用于辅助编译器开发和维护，但作为端到端编译器的潜力待挖掘，因此探索其可行性与未来方向。

Method: 设计CompilerEval数据集和框架评估主流大语言模型在源代码理解和汇编代码生成方面的能力，分析错误、探索改进方法并评估跨平台编译能力。

Result: 大语言模型展现出作为编译器的基本能力，但当前编译成功率低，通过优化提示、扩大模型规模和引入推理方法可显著提升汇编代码质量。

Conclusion: 对LaaC前景乐观，提出实用架构设计和未来研究方向，认为经针对性训练等可生成高质量汇编代码，推动编译领域范式转变。

Abstract: In recent years, end-to-end Large Language Model (LLM) technology has shown
substantial advantages across various domains. As critical system software and
infrastructure, compilers are responsible for transforming source code into
target code. While LLMs have been leveraged to assist in compiler development
and maintenance, their potential as an end-to-end compiler remains largely
unexplored. This paper explores the feasibility of LLM as a Compiler (LaaC) and
its future directions. We designed the CompilerEval dataset and framework
specifically to evaluate the capabilities of mainstream LLMs in source code
comprehension and assembly code generation. In the evaluation, we analyzed
various errors, explored multiple methods to improve LLM-generated code, and
evaluated cross-platform compilation capabilities. Experimental results
demonstrate that LLMs exhibit basic capabilities as compilers but currently
achieve low compilation success rates. By optimizing prompts, scaling up the
model, and incorporating reasoning methods, the quality of assembly code
generated by LLMs can be significantly enhanced. Based on these findings, we
maintain an optimistic outlook for LaaC and propose practical architectural
designs and future research directions. We believe that with targeted training,
knowledge-rich prompts, and specialized infrastructure, LaaC has the potential
to generate high-quality assembly code and drive a paradigm shift in the field
of compilation.

</details>


### [106] [Exchange Policy Optimization Algorithm for Semi-Infinite Safe Reinforcement Learning](https://arxiv.org/abs/2511.04147)
*Jiaming Zhang,Yujie Yang,Haoning Wang,Liping Zhang,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 本文提出交换策略优化（EPO）算法框架，用于半无限安全强化学习，可实现最优策略性能和确定性有界安全，理论分析表明其训练策略表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决半无限安全强化学习（SI - safe RL）问题，该问题存在无限数量的约束，在实际应用中常见。

Method: 提出EPO算法框架，通过迭代解决有限约束集的安全强化学习子问题，自适应调整活动集，添加违反约束的条件、删除拉格朗日乘子为零的约束。

Result: 理论分析表明，在温和假设下，通过EPO训练的策略性能与最优解相当，全局约束违规严格控制在规定范围内。

Conclusion: EPO算法框架能有效解决半无限安全强化学习问题，实现最优策略性能和确定性有界安全。

Abstract: Safe reinforcement learning (safe RL) aims to respect safety requirements
while optimizing long-term performance. In many practical applications,
however, the problem involves an infinite number of constraints, known as
semi-infinite safe RL (SI-safe RL). Such constraints typically appear when
safety conditions must be enforced across an entire continuous parameter space,
such as ensuring adequate resource distribution at every spatial location. In
this paper, we propose exchange policy optimization (EPO), an algorithmic
framework that achieves optimal policy performance and deterministic bounded
safety. EPO works by iteratively solving safe RL subproblems with finite
constraint sets and adaptively adjusting the active set through constraint
expansion and deletion. At each iteration, constraints with violations
exceeding the predefined tolerance are added to refine the policy, while those
with zero Lagrange multipliers are removed after the policy update. This
exchange rule prevents uncontrolled growth of the working set and supports
effective policy training. Our theoretical analysis demonstrates that, under
mild assumptions, strategies trained via EPO achieve performance comparable to
optimal solutions with global constraint violations strictly remaining within a
prescribed bound.

</details>


### [107] [Learning to Land Anywhere: Transferable Generative Models for Aircraft Trajectories](https://arxiv.org/abs/2511.04155)
*Olav Finne Praesteng Larsen,Massimiliano Ruocco,Michail Spitieris,Abdulmajid Murad,Martina Ragosta*

Main category: cs.LG

TL;DR: 本文研究利用迁移学习将在数据丰富机场训练的生成模型应用于数据稀缺机场的可行性，结果显示扩散模型仅用少量目标机场数据就能有良好表现，证明迁移学习可大幅降低空中交通管理中轨迹生成的数据需求。


<details>
  <summary>Details</summary>
Motivation: 许多二级和地区机场面临轨迹数据稀缺问题，限制机器学习方法应用和大规模模拟分析，需解决数据需求问题。

Method: 将最先进的基于扩散和流匹配的架构应用于航空领域，在苏黎世数据集上预训练模型，在都柏林数据集上进行不同比例本地数据的微调。

Result: 扩散模型用 5% 都柏林数据就能有有竞争力表现，约 20% 数据时达到基线水平，始终优于从头训练的模型；潜流匹配和潜扩散模型也从预训练中受益，但增益不稳定；流匹配模型泛化能力较弱。

Conclusion: 迁移学习有潜力大幅降低空中交通管理中轨迹生成的数据需求，即使在历史记录有限的环境中也能实现逼真的合成数据生成。

Abstract: Access to trajectory data is a key requirement for developing and validating
Air Traffic Management (ATM) solutions, yet many secondary and regional
airports face severe data scarcity. This limits the applicability of machine
learning methods and the ability to perform large-scale simulations or
"what-if" analyses. In this paper, we investigate whether generative models
trained on data-rich airports can be efficiently adapted to data-scarce
airports using transfer learning. We adapt state-of-the-art diffusion- and
flow-matching-based architectures to the aviation domain and evaluate their
transferability between Zurich (source) and Dublin (target) landing trajectory
datasets. Models are pretrained on Zurich and fine-tuned on Dublin with varying
amounts of local data, ranging from 0% to 100%. Results show that
diffusion-based models achieve competitive performance with as little as 5% of
the Dublin data and reach baseline-level performance around 20%, consistently
outperforming models trained from scratch across metrics and visual
inspections. Latent flow matching and latent diffusion models also benefit from
pretraining, though with more variable gains, while flow matching models show
weaker generalization. Despite challenges in capturing rare trajectory
patterns, these findings demonstrate the potential of transfer learning to
substantially reduce data requirements for trajectory generation in ATM,
enabling realistic synthetic data generation even in environments with limited
historical records.

</details>


### [108] [Deep Learning Approach for Clinical Risk Identification Using Transformer Modeling of Heterogeneous EHR Data](https://arxiv.org/abs/2511.04158)
*Anzhuo Xie,Wei-Chen Chang*

Main category: cs.LG

TL;DR: 提出基于Transformer的纵向建模方法处理异构电子健康记录数据临床风险分类问题，实验表明该模型性能优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 解决异构电子健康记录数据在临床风险分类中存在的不规则时间模式、大模态差异和复杂语义结构等挑战。

Method: 以多源医疗特征为输入，用特征嵌入层统一数据表示，引入可学习的时间编码机制，核心模型采用多头自注意力结构，设计语义加权池化模块，最后用线性映射层生成风险分数。

Result: 该模型在准确率、召回率、精确率和F1分数上优于传统机器学习和时间深度学习模型，能在多源异构电子健康记录环境中实现稳定精确的风险识别。

Conclusion: 该模型为临床智能决策提供了高效可靠的框架。

Abstract: This study proposes a Transformer-based longitudinal modeling method to
address challenges in clinical risk classification with heterogeneous
Electronic Health Record (EHR) data, including irregular temporal patterns,
large modality differences, and complex semantic structures. The method takes
multi-source medical features as input and employs a feature embedding layer to
achieve a unified representation of structured and unstructured data. A
learnable temporal encoding mechanism is introduced to capture dynamic
evolution under uneven sampling intervals. The core model adopts a multi-head
self-attention structure to perform global dependency modeling on longitudinal
sequences, enabling the aggregation of long-term trends and short-term
fluctuations across different temporal scales. To enhance semantic
representation, a semantic-weighted pooling module is designed to assign
adaptive importance to key medical events, improving the discriminative ability
of risk-related features. Finally, a linear mapping layer generates
individual-level risk scores. Experimental results show that the proposed model
outperforms traditional machine learning and temporal deep learning models in
accuracy, recall, precision, and F1-Score, achieving stable and precise risk
identification in multi-source heterogeneous EHR environments and providing an
efficient and reliable framework for clinical intelligent decision-making.

</details>


### [109] [ScaleDL: Towards Scalable and Efficient Runtime Prediction for Distributed Deep Learning Workloads](https://arxiv.org/abs/2511.04162)
*Xiaokai Wang,Shaoyuan Huang,Yuting Li,Xiaofei Wang*

Main category: cs.LG

TL;DR: 提出ScaleDL框架结合非线性层建模与GNN跨层交互机制，用D - 最优法降低数据收集成本，实验证明能提升DNN运行时间预测准确性和泛化性。


<details>
  <summary>Details</summary>
Motivation: DNN工作负载对分布式计算资源要求高，准确预测运行时间对优化开发和资源分配至关重要，传统方法有局限性，需平衡准确性、泛化性和数据收集成本。

Method: 提出ScaleDL框架，结合非线性层建模与GNN跨层交互机制，采用D - 最优法降低数据收集成本。

Result: 在五个流行DNN模型的工作负载实验中，ScaleDL比基线模型MRE低6倍，RMSE低5倍。

Conclusion: ScaleDL能有效提升DNN运行时间预测的准确性和泛化性。

Abstract: Deep neural networks (DNNs) form the cornerstone of modern AI services,
supporting a wide range of applications, including autonomous driving,
chatbots, and recommendation systems. As models increase in size and
complexity, DNN workloads like training and inference tasks impose
unprecedented demands on distributed computing resources, making the accurate
prediction of runtime essential for optimizing development and resource
allocation. Traditional methods rely on additive computational unit models,
limiting their accuracy and generalizability. In contrast, graph-enhanced
modeling improves performance but significantly increases data collection
costs. Therefore, there is a critical need for a method that strikes a balance
between accuracy, generalizability, and the costs of data collection. To
address these challenges, we propose ScaleDL, a novel runtime prediction
framework that combines nonlinear layer-wise modeling with graph neural network
(GNN)-based cross-layer interaction mechanism, enabling accurate DNN runtime
prediction and hierarchical generalizability across different network
architectures. Additionally, we employ the D-optimal method to reduce data
collection costs. Experiments on the workloads of five popular DNN models prove
that ScaleDL enhances runtime prediction accuracy and generalizability,
achieving 6$\times$ lower MRE and 5$\times$ lower RMSE compared to baseline
models.

</details>


### [110] [Block Rotation is All You Need for MXFP4 Quantization](https://arxiv.org/abs/2511.04214)
*Yuantian Shao,Peisong Wang,Yuanteng Chen,Chang Xu,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 本文对MXFP4格式下的PTQ方法进行基准测试，分析旋转方法与MXFP4的冲突，提出块旋转策略提升精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型规模增长带来高成本，PTQ是有效部署方案，但W4A4量化有挑战，MXFP4出现使现有技术适用性存疑。

Method: 建立MXFP4格式下PTQ方法的综合基准，分析旋转方法与MXFP4冲突根源，提出块旋转策略。

Result: GPTQ等方法表现好，旋转方法与MXFP4不兼容，块旋转策略使不同大语言模型精度显著提升。

Conclusion: 研究为从业者提供指导，为新兴低精度格式下PTQ研究奠定基础。

Abstract: Large language models (LLMs) have achieved remarkable success, but their
rapidly growing scale imposes prohibitive costs in memory, computation, and
energy. Post-training quantization (PTQ) is a promising solution for efficient
deployment, yet achieving accurate W4A4 quantization remains an open challenge.
While most existing methods are designed for INT4 formats, the emergence of
MXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)--
raises questions about the applicability of current techniques. In this work,
we establish a comprehensive benchmark of PTQ methods under the MXFP4 format.
Through systematic evaluation, we find that methods like GPTQ consistently
deliver strong performance, whereas rotation-based approaches, which are almost
used by all state-of-the-art approaches, suffer from severe incompatibility
with MXFP4. We further provide the first in-depth analysis of this conflict,
tracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two)
block scaling and the redistribution of outlier energy via global rotation.
Building on this insight, we propose a simple yet effective block rotation
strategy that adapts rotation-based methods to MXFP4, leading to substantial
accuracy improvements across diverse LLMs. Our findings not only offer clear
guidance for practitioners but also set a foundation for advancing PTQ research
under emerging low-precision formats.

</details>


### [111] [The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms](https://arxiv.org/abs/2511.04217)
*Hikari Otsuka,Daiki Chijiwa,Yasuyuki Okoshi,Daichi Fujiki,Susumu Takeuchi,Masato Motomura*

Main category: cs.LG

TL;DR: 本文针对transformer架构中强彩票假说（SLTH）缺乏理论理解的问题，对多头注意力（MHA）机制中强彩票（SLT）的存在性进行理论分析，并将SLTH扩展到无归一化层的transformer，通过实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 现有SLTH理论未考虑transformer核心组件MHA机制，缺乏对transformer架构中SLTH的理论理解。

Method: 对MHA中SLT的存在性进行理论分析，证明随机初始化的MHA在一定隐藏维度条件下包含能近似任意MHA的SLT，并将SLTH扩展到无归一化层的transformer。

Result: 通过实验验证，源模型（MHA和transformer）中SLT与近似目标模型的近似误差随源模型隐藏维度增加呈指数下降。

Conclusion: 完成了对MHA中SLT存在性的理论分析，并将SLTH扩展到无归一化层的transformer，实验结果支持理论分析。

Abstract: The strong lottery ticket hypothesis (SLTH) conjectures that high-performing
subnetworks, called strong lottery tickets (SLTs), are hidden in randomly
initialized neural networks. Although recent theoretical studies have
established the SLTH across various neural architectures, the SLTH for
transformer architectures still lacks theoretical understanding. In particular,
the current theory of the SLTH does not yet account for the multi-head
attention (MHA) mechanism, a core component of transformers. To address this
gap, we introduce a theoretical analysis of the existence of SLTs within MHAs.
We prove that, if a randomly initialized MHA of $H$ heads and input dimension
$d$ has the hidden dimension $O(d\log(Hd^{3/2}))$ for the key and value, it
contains an SLT that approximates an arbitrary MHA with the same input
dimension with high probability. Furthermore, by leveraging this theory for
MHAs, we extend the SLTH to transformers without normalization layers. We
empirically validate our theoretical findings, demonstrating that the
approximation error between the SLT within a source model (MHA and transformer)
and an approximate target counterpart decreases exponentially by increasing the
hidden dimension of the source model.

</details>


### [112] [seqme: a Python library for evaluating biological sequence design](https://arxiv.org/abs/2511.04239)
*Rasmus Møller-Larsen,Adam Izdebski,Jan Olszewski,Pankhil Gawade,Michal Kmicikiewicz,Wojciech Zarzecki,Ewa Szczurek*

Main category: cs.LG

TL;DR: 本文介绍开源 Python 库 seqme，可评估生物序列计算设计方法。


<details>
  <summary>Details</summary>
Motivation: 现有生物序列计算设计方法缺乏统一实现评估指标的软件库。

Method: 开发模块化、可扩展的 seqme 库，涵盖三类指标，适用于多种生物序列，提供嵌入和属性模型及相关功能。

Result: 开发出 seqme 库，可用于评估一次性和迭代式计算设计方法。

Conclusion: seqme 库有助于生物序列计算设计方法的评估。

Abstract: Recent advances in computational methods for designing biological sequences
have sparked the development of metrics to evaluate these methods performance
in terms of the fidelity of the designed sequences to a target distribution and
their attainment of desired properties. However, a single software library
implementing these metrics was lacking. In this work we introduce seqme, a
modular and highly extendable open-source Python library, containing
model-agnostic metrics for evaluating computational methods for biological
sequence design. seqme considers three groups of metrics: sequence-based,
embedding-based, and property-based, and is applicable to a wide range of
biological sequences: small molecules, DNA, ncRNA, mRNA, peptides and proteins.
The library offers a number of embedding and property models for biological
sequences, as well as diagnostics and visualization functions to inspect the
results. seqme can be used to evaluate both one-shot and iterative
computational design methods.

</details>


### [113] [Guided by Stars: Interpretable Concept Learning Over Time Series via Temporal Logic Semantics](https://arxiv.org/abs/2511.04244)
*Irene Ferfoglia,Simone Silvetti,Gaia Saveri,Laura Nenzi,Luca Bortolussi*

Main category: cs.LG

TL;DR: 提出神经符号框架STELLE统一时间序列分类与解释，实验证明其有竞争力且能提供合理解释。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类多使用黑盒深度学习方法，难以理解输出原理。

Method: 提出STELLE框架，通过将轨迹直接嵌入时间逻辑概念空间统一分类和解释，引入受STL启发的核函数。

Result: 实验在多个真实世界基准上验证，STELLE达到有竞争力的准确率并能提供逻辑合理的解释。

Conclusion: STELLE能在保证准确率的同时，为时间序列分类提供逻辑合理的局部和全局解释。

Abstract: Time series classification is a task of paramount importance, as this kind of
data often arises in safety-critical applications. However, it is typically
tackled with black-box deep learning methods, making it hard for humans to
understand the rationale behind their output. To take on this challenge, we
propose a novel approach, STELLE (Signal Temporal logic Embedding for
Logically-grounded Learning and Explanation), a neuro-symbolic framework that
unifies classification and explanation through direct embedding of trajectories
into a space of temporal logic concepts. By introducing a novel STL-inspired
kernel that maps raw time series to their alignment with predefined STL
formulae, our model jointly optimises accuracy and interpretability, as each
prediction is accompanied by the most relevant logical concepts that
characterise it. This yields (i) local explanations as human-readable STL
conditions justifying individual predictions, and (ii) global explanations as
class-characterising formulae. Experiments demonstrate that STELLE achieves
competitive accuracy while providing logically faithful explanations, validated
on diverse real-world benchmarks.

</details>


### [114] [Efficient Reinforcement Learning from Human Feedback via Bayesian Preference Inference](https://arxiv.org/abs/2511.04286)
*Matteo Cercola,Valeria Capretti,Simone Formentin*

Main category: cs.LG

TL;DR: 提出结合RLHF和PBO优势的混合框架用于高效偏好学习，实验验证其在样本效率和性能上有提升。


<details>
  <summary>Details</summary>
Motivation: 收集人类偏好数据成本高、耗时长，需要更高效学习范式，RLHF和PBO各有优势，需结合二者优点。

Method: 在RLHF管道中集成基于采集驱动的模块，提出统一RLHF可扩展性和PBO查询效率的混合框架。

Result: 在高维偏好优化和大语言模型微调两个领域实验，样本效率和整体性能均有持续提升。

Conclusion: 提出的混合框架能有效结合RLHF和PBO优势，实现主动且样本高效的偏好收集。

Abstract: Learning from human preferences is a cornerstone of aligning machine learning
models with subjective human judgments. Yet, collecting such preference data is
often costly and time-consuming, motivating the need for more efficient
learning paradigms. Two established approaches offer complementary advantages:
RLHF scales effectively to high-dimensional tasks such as LLM fine-tuning,
while PBO achieves greater sample efficiency through active querying. We
propose a hybrid framework that unifies RLHF's scalability with PBO's query
efficiency by integrating an acquisition-driven module into the RLHF pipeline,
thereby enabling active and sample-efficient preference gathering. We validate
the proposed approach on two representative domains: (i) high-dimensional
preference optimization and (ii) LLM fine-tuning. Experimental results
demonstrate consistent improvements in both sample efficiency and overall
performance across these tasks.

</details>


### [115] [Differentially Private In-Context Learning with Nearest Neighbor Search](https://arxiv.org/abs/2511.04332)
*Antti Koskela,Tejas Kulkarni,Laith Zumot*

Main category: cs.LG

TL;DR: 本文提出一种针对上下文学习的差分隐私框架，在评估基准上大幅超越现有基线，实现更优的隐私 - 效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私上下文学习方法忽略了现代大语言模型管道中的相似性搜索这一关键组件。

Method: 采用从上下文数据数据库中进行最近邻检索，并结合隐私过滤器跟踪所选样本的累积隐私成本，以确保符合差分隐私预算。

Result: 在文本分类和文档问答实验中，该方法明显优于现有基线。

Conclusion: 所提出的方法在隐私 - 效用权衡方面表现更佳，优于现有基线。

Abstract: Differentially private in-context learning (DP-ICL) has recently become an
active research topic due to the inherent privacy risks of in-context learning.
However, existing approaches overlook a critical component of modern large
language model (LLM) pipelines: the similarity search used to retrieve relevant
context data. In this work, we introduce a DP framework for in-context learning
that integrates nearest neighbor search of relevant examples in a privacy-aware
manner. Our method outperforms existing baselines by a substantial margin
across all evaluated benchmarks, achieving more favorable privacy-utility
trade-offs. To achieve this, we employ nearest neighbor retrieval from a
database of context data, combined with a privacy filter that tracks the
cumulative privacy cost of selected samples to ensure adherence to a central
differential privacy budget. Experimental results on text classification and
document question answering show a clear advantage of the proposed method over
existing baselines.

</details>


### [116] [LUME-DBN: Full Bayesian Learning of DBNs from Incomplete data in Intensive Care](https://arxiv.org/abs/2511.04333)
*Federico Pirola,Fabio Stella,Marco Grzegorczyk*

Main category: cs.LG

TL;DR: 提出基于Gibbs采样从缺失数据学习动态贝叶斯网络的方法，在模拟和真实数据上效果优于MICE，支持临床决策。


<details>
  <summary>Details</summary>
Motivation: 现有处理纵向临床数据集缺失数据方法未考虑数据时间特性，完整贝叶斯框架欠发达，需改进。

Method: 提出基于Gibbs采样的方法，将缺失值视为服从高斯分布的未知参数，从全条件分布采样进行插补和不确定性估计。

Result: 在模拟和真实重症监护数据上，比MICE有更好的重建准确性和收敛性。

Conclusion: 在时间模型中纳入全贝叶斯推理有临床相关性，支持更安全明智的临床决策。

Abstract: Dynamic Bayesian networks (DBNs) are increasingly used in healthcare due to
their ability to model complex temporal relationships in patient data while
maintaining interpretability, an essential feature for clinical
decision-making. However, existing approaches to handling missing data in
longitudinal clinical datasets are largely derived from static Bayesian
networks literature, failing to properly account for the temporal nature of the
data. This gap limits the ability to quantify uncertainty over time, which is
particularly critical in settings such as intensive care, where understanding
the temporal dynamics is fundamental for model trustworthiness and
applicability across diverse patient groups. Despite the potential of DBNs, a
full Bayesian framework that integrates missing data handling remains
underdeveloped. In this work, we propose a novel Gibbs sampling-based method
for learning DBNs from incomplete data. Our method treats each missing value as
an unknown parameter following a Gaussian distribution. At each iteration, the
unobserved values are sampled from their full conditional distributions,
allowing for principled imputation and uncertainty estimation. We evaluate our
method on both simulated datasets and real-world intensive care data from
critically ill patients. Compared to standard model-agnostic techniques such as
MICE, our Bayesian approach demonstrates superior reconstruction accuracy and
convergence properties. These results highlight the clinical relevance of
incorporating full Bayesian inference in temporal models, providing more
reliable imputations and offering deeper insight into model behavior. Our
approach supports safer and more informed clinical decision-making,
particularly in settings where missing data are frequent and potentially
impactful.

</details>


### [117] [Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness](https://arxiv.org/abs/2511.04401)
*Subeen Park,Joowang Kim,Hakyung Lee,Sunjae Yoo,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出SCER方法抑制虚假线索，提升模型在最差子群体上的鲁棒性，在多领域表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型依赖虚假关联，在子群体分布偏移场景表现差，现有方法缺乏连接嵌入空间表示与最差群体误差的理论框架。

Method: 提出Spurious Correlation - Aware Embedding Regularization for Worst - Group Robustness (SCER)，直接正则化特征表示以抑制虚假线索，在嵌入层施加理论约束。

Result: 在多个视觉和语言任务上系统评估，SCER在最差群体准确率上优于现有最先进的研究。

Conclusion: SCER方法可有效提升模型在最差子群体上的鲁棒性，减少对虚假模式的敏感性。

Abstract: Deep learning models achieve strong performance across various domains but
often rely on spurious correlations, making them vulnerable to distribution
shifts. This issue is particularly severe in subpopulation shift scenarios,
where models struggle in underrepresented groups. While existing methods have
made progress in mitigating this issue, their performance gains are still
constrained. They lack a rigorous theoretical framework connecting the
embedding space representations with worst-group error. To address this
limitation, we propose Spurious Correlation-Aware Embedding Regularization for
Worst-Group Robustness (SCER), a novel approach that directly regularizes
feature representations to suppress spurious cues. We show theoretically that
worst-group error is influenced by how strongly the classifier relies on
spurious versus core directions, identified from differences in group-wise mean
embeddings across domains and classes. By imposing theoretical constraints at
the embedding level, SCER encourages models to focus on core features while
reducing sensitivity to spurious patterns. Through systematic evaluation on
multiple vision and language, we show that SCER outperforms prior
state-of-the-art studies in worst-group accuracy. Our code is available at
\href{https://github.com/MLAI-Yonsei/SCER}{https://github.com/MLAI-Yonsei/SCER}.

</details>


### [118] [The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity](https://arxiv.org/abs/2511.04418)
*Tim Tomov,Dominik Fuchsgruber,Tom Wollschläger,Stephan Günnemann*

Main category: cs.LG

TL;DR: 研究指出当前大语言模型不确定性量化（UQ）方法在模糊数据上表现差，引入模糊问答数据集，揭示现有方法局限并呼吁重新思考建模范式。


<details>
  <summary>Details</summary>
Motivation: 现实语言存在固有模糊性，而现有UQ方法多在无歧义任务上进行基准测试，需要研究其在模糊数据上的表现。

Method: 引入MAQA*和AmbigQA*模糊问答数据集，对比不同估计范式下模型在模糊数据和无歧义数据上的表现，并进行理论解释。

Result: 当前不确定性估计器在无歧义假设下表现良好，但在模糊数据上接近随机表现，这种性能下降在不同估计范式中一致，且预测分布和基于集成的估计器在模糊性下有根本局限。

Conclusion: 揭示了当前大语言模型UQ方法的关键缺陷，促使重新思考当前建模范式。

Abstract: Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is
critical for trustworthy deployment. While real-world language is inherently
ambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically
benchmarked against tasks with no ambiguity. In this work, we demonstrate that
while current uncertainty estimators perform well under the restrictive
assumption of no ambiguity, they degrade to close-to-random performance on
ambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first
ambiguous question-answering (QA) datasets equipped with ground-truth answer
distributions estimated from factual co-occurrence. We find this performance
deterioration to be consistent across different estimation paradigms: using the
predictive distribution itself, internal representations throughout the model,
and an ensemble of models. We show that this phenomenon can be theoretically
explained, revealing that predictive-distribution and ensemble-based estimators
are fundamentally limited under ambiguity. Overall, our study reveals a key
shortcoming of current UQ methods for LLMs and motivates a rethinking of
current modeling paradigms.

</details>


### [119] [On the Equivalence of Regression and Classification](https://arxiv.org/abs/2511.04422)
*Jayadeva,Naman Dwivedi,Hari Krishnan,N. M. Anoop Krishnan*

Main category: cs.LG

TL;DR: 本文揭示回归与分类的联系，提出新回归公式、“可回归性”度量，并用于训练神经网络。


<details>
  <summary>Details</summary>
Motivation: 回归与分类的形式化联系薄弱，支持向量回归中边际最大化项缺乏合理解释。

Method: 证明M个样本的回归问题与2M个样本的线性可分分类任务一一等价，基于此开展后续工作。

Result: 得到不同的回归公式，提出“可回归性”度量，训练神经网络学习线性映射。

Conclusion: 建立了回归与分类的联系，提出的方法有潜在应用价值。

Abstract: A formal link between regression and classification has been tenuous. Even
though the margin maximization term $\|w\|$ is used in support vector
regression, it has at best been justified as a regularizer. We show that a
regression problem with $M$ samples lying on a hyperplane has a one-to-one
equivalence with a linearly separable classification task with $2M$ samples. We
show that margin maximization on the equivalent classification task leads to a
different regression formulation than traditionally used. Using the
equivalence, we demonstrate a ``regressability'' measure, that can be used to
estimate the difficulty of regressing a dataset, without needing to first learn
a model for it. We use the equivalence to train neural networks to learn a
linearizing map, that transforms input variables into a space where a linear
regressor is adequate.

</details>


### [120] [Federated Stochastic Minimax Optimization under Heavy-Tailed Noises](https://arxiv.org/abs/2511.04456)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: 研究联邦学习中重尾梯度噪声下的非凸 - PL 极小极大优化，提出 Fed - NSGDA - M 和 FedMuon - DA 算法，理论证明收敛率，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 重尾噪声在非凸随机优化中更符合实际，研究其在联邦学习非凸 - PL 极小极大优化中的应用。

Method: 提出 Fed - NSGDA - M（集成归一化梯度）和 FedMuon - DA（利用 Muon 优化器进行局部更新）两种算法。

Result: 理论上两种算法收敛率达 $O({1}/{(TNp)^{rac{s - 1}{2s}}})$，是重尾噪声下首批有严格理论保证的联邦极小极大优化算法，实验验证了有效性。

Conclusion: 所提算法能在较温和条件下有效处理联邦极小极大优化中的重尾噪声问题。

Abstract: Heavy-tailed noise has attracted growing attention in nonconvex stochastic
optimization, as numerous empirical studies suggest it offers a more realistic
assumption than standard bounded variance assumption. In this work, we
investigate nonconvex-PL minimax optimization under heavy-tailed gradient noise
in federated learning. We propose two novel algorithms: Fed-NSGDA-M, which
integrates normalized gradients, and FedMuon-DA, which leverages the Muon
optimizer for local updates. Both algorithms are designed to effectively
address heavy-tailed noise in federated minimax optimization, under a milder
condition. We theoretically establish that both algorithms achieve a
convergence rate of $O({1}/{(TNp)^{\frac{s-1}{2s}}})$. To the best of our
knowledge, these are the first federated minimax optimization algorithms with
rigorous theoretical guarantees under heavy-tailed noise. Extensive experiments
further validate their effectiveness.

</details>


### [121] [Towards Causal Market Simulators](https://arxiv.org/abs/2511.04469)
*Dennis Thumm,Luis Ontaneda Mijares*

Main category: cs.LG

TL;DR: 提出TNCM - VAE模型用于生成反事实金融时间序列，在合成自回归模型验证中表现出色，可用于金融压力测试等。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度生成模型的市场生成器缺乏因果推理能力，无法满足反事实分析和风险评估需求。

Method: 结合变分自编码器和结构因果模型，在解码器架构中通过有向无环图施加因果约束，使用因果Wasserstein距离进行训练。

Result: 在受Ornstein - Uhlenbeck过程启发的合成自回归模型上验证，反事实概率估计的L1距离低至0.03 - 0.10。

Conclusion: 该模型能生成符合潜在因果机制的反事实市场轨迹，可用于金融压力测试、情景分析和增强回测。

Abstract: Market generators using deep generative models have shown promise for
synthetic financial data generation, but existing approaches lack causal
reasoning capabilities essential for counterfactual analysis and risk
assessment. We propose a Time-series Neural Causal Model VAE (TNCM-VAE) that
combines variational autoencoders with structural causal models to generate
counterfactual financial time series while preserving both temporal
dependencies and causal relationships. Our approach enforces causal constraints
through directed acyclic graphs in the decoder architecture and employs the
causal Wasserstein distance for training. We validate our method on synthetic
autoregressive models inspired by the Ornstein-Uhlenbeck process, demonstrating
superior performance in counterfactual probability estimation with L1 distances
as low as 0.03-0.10 compared to ground truth. The model enables financial
stress testing, scenario analysis, and enhanced backtesting by generating
plausible counterfactual market trajectories that respect underlying causal
mechanisms.

</details>


### [122] [Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank Training](https://arxiv.org/abs/2511.04485)
*Ipsita Ghosh,Ethan Nguyen,Christian Kümmerle*

Main category: cs.LG

TL;DR: 提出Q3R用于低秩预训练任务，能以小计算开销训练低秩模型，在多任务验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于低秩优化的参数高效训练方法在低秩预训练任务失败，维持低秩结构和目标有挑战。

Method: 提出Q3R，基于二次正则项，受迭代加权最小二乘法框架启发。

Result: 能训练指定低目标秩的权重矩阵，预测性能与密集模型相当，在ViT - Tiny模型实验中，截断60%和80%参数，CIFAR - 10准确率分别下降约1.3%和4%。

Conclusion: Q3R在图像和语言任务的Transformer上有效，包括低秩微调。

Abstract: Parameter-efficient training, based on low-rank optimization, has become a
highly successful tool for fine-tuning large deep-learning models. However,
these methods fail at low-rank pre-training tasks where maintaining the
low-rank structure and the objective remains a challenging task. We propose the
Quadratic Reweighted Rank Regularizer dubbed Q3R, which leads to a novel
low-rank inducing training strategy inspired by the iteratively reweighted
least squares (IRLS) framework. Q3R is based on a quadratic regularizer term
which majorizes a smoothed log determinant serving as rank surrogate objective.
Unlike other low-rank training techniques, Q3R is able to train weight matrices
with prescribed, low target ranks of models that achieve comparable predictive
performance as dense models, with small computational overhead, while remaining
fully compatible with existing architectures. For example, we demonstrated one
experiment where we are able to truncate $60\%$ and $80\%$ of the parameters of
a ViT-Tiny model with $~1.3\%$ and $~4\%$ accuracy drop in CIFAR-10 performance
respectively. The efficacy of Q3R is confirmed on Transformers across both
image and language tasks, including for low-rank fine-tuning.

</details>


### [123] [Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks](https://arxiv.org/abs/2511.04494)
*Alper Kalle,Theo Rudkiewicz,Mohamed-Oumar Ouerfelli,Mohamed Tamaazousti*

Main category: cs.LG

TL;DR: 本文聚焦神经网络压缩，采用数据感知范数和新算法，无需微调达到竞争精度，且范数可跨数据集迁移，实验验证其优势。


<details>
  <summary>Details</summary>
Motivation: 神经网络图像任务需大量算力，训练后可压缩减少内存和计算量，传统低秩近似方法存在不足。

Method: 使用数据感知范数衡量函数空间误差，提出新交替最小二乘算法优化范数，用于常见张量分解。

Result: 数据感知方法常无需微调达到竞争精度，基于协方差的范数跨数据集迁移精度损失小。

Conclusion: 实验证实了所提方法在多个CNN架构和数据集上的优势。

Abstract: Neural networks are widely used for image-related tasks but typically demand
considerable computing power. Once a network has been trained, however, its
memory- and compute-footprint can be reduced by compression. In this work, we
focus on compression through tensorization and low-rank representations.
Whereas classical approaches search for a low-rank approximation by minimizing
an isotropic norm such as the Frobenius norm in weight-space, we use
data-informed norms that measure the error in function space. Concretely, we
minimize the change in the layer's output distribution, which can be expressed
as $\lVert (W - \widetilde{W}) \Sigma^{1/2}\rVert_F$ where $\Sigma^{1/2}$ is
the square root of the covariance matrix of the layer's input and $W$,
$\widetilde{W}$ are the original and compressed weights. We propose new
alternating least square algorithms for the two most common tensor
decompositions (Tucker-2 and CPD) that directly optimize the new norm. Unlike
conventional compression pipelines, which almost always require
post-compression fine-tuning, our data-informed approach often achieves
competitive accuracy without any fine-tuning. We further show that the same
covariance-based norm can be transferred from one dataset to another with only
a minor accuracy drop, enabling compression even when the original training
dataset is unavailable. Experiments on several CNN architectures (ResNet-18/50,
and GoogLeNet) and datasets (ImageNet, FGVC-Aircraft, Cifar10, and Cifar100)
confirm the advantages of the proposed method.

</details>


### [124] [Alternative Fairness and Accuracy Optimization in Criminal Justice](https://arxiv.org/abs/2511.04505)
*Shaolong Wu,James Blume,Geshi Yeung*

Main category: cs.LG

TL;DR: 文章回顾算法公平概念，提出改进的群体公平方法，置于三类批判中讨论，并给出公共决策系统部署框架。


<details>
  <summary>Details</summary>
Motivation: 算法公平研究领域关键概念未确定，尤其在刑事司法领域。

Method: 回顾组、个体和过程公平，修改标准群体公平，将假阴性率差异控制在小范围内最小化加权误差损失，将提议置于三类批判中讨论，给出三支柱部署框架。

Result: 改进方法使解决方案更易找到，可提高预测准确性，凸显错误成本的伦理选择。

Conclusion: 这些元素将技术设计与合法性联系起来，为使用风险评估等工具的机构提供可操作指导。

Abstract: Algorithmic fairness has grown rapidly as a research area, yet key concepts
remain unsettled, especially in criminal justice. We review group, individual,
and process fairness and map the conditions under which they conflict. We then
develop a simple modification to standard group fairness. Rather than exact
parity across protected groups, we minimize a weighted error loss while keeping
differences in false negative rates within a small tolerance. This makes
solutions easier to find, can raise predictive accuracy, and surfaces the
ethical choice of error costs. We situate this proposal within three classes of
critique: biased and incomplete data, latent affirmative action, and the
explosion of subgroup constraints. Finally, we offer a practical framework for
deployment in public decision systems built on three pillars: need-based
decisions, Transparency and accountability, and narrowly tailored definitions
and solutions. Together, these elements link technical design to legitimacy and
provide actionable guidance for agencies that use risk assessment and related
tools.

</details>


### [125] [Linear Mode Connectivity under Data Shifts for Deep Ensembles of Image Classifiers](https://arxiv.org/abs/2511.04514)
*C. Hepburn,T. Zielke,A. P. Raulf*

Main category: cs.LG

TL;DR: 研究线性模式连通性（LMC）在数据偏移下的情况，确定减轻影响的条件，探讨LMC在平衡训练效率和集成多样性方面的作用。


<details>
  <summary>Details</summary>
Motivation: 线性模式连通性（LMC）与深度学习多方面相关，研究其在数据偏移下的情况并减轻影响。

Method: 通过实验研究LMC在数据偏移下的情况，将数据偏移视为随机梯度噪声的额外来源，利用小学习率和大批量来减少。

Result: 模型采样通过LMC比收敛到不同盆地的模型更易犯相似错误，LMC能平衡训练效率和更大、更多样集成的收益。

Conclusion: 确定减轻数据偏移对LMC影响的条件，指出LMC在平衡训练效率和集成多样性上有作用。

Abstract: The phenomenon of linear mode connectivity (LMC) links several aspects of
deep learning, including training stability under noisy stochastic gradients,
the smoothness and generalization of local minima (basins), the similarity and
functional diversity of sampled models, and architectural effects on data
processing. In this work, we experimentally study LMC under data shifts and
identify conditions that mitigate their impact. We interpret data shifts as an
additional source of stochastic gradient noise, which can be reduced through
small learning rates and large batch sizes. These parameters influence whether
models converge to the same local minimum or to regions of the loss landscape
with varying smoothness and generalization. Although models sampled via LMC
tend to make similar errors more frequently than those converging to different
basins, the benefit of LMC lies in balancing training efficiency against the
gains achieved from larger, more diverse ensembles. Code and supplementary
materials will be made publicly available at https://github.com/DLR-KI/LMC in
due course.

</details>


### [126] [End-to-End Reinforcement Learning of Koopman Models for eNMPC of an Air Separation Unit](https://arxiv.org/abs/2511.04522)
*Daniel Mayfrank,Kayra Dernek,Laura Lang,Alexander Mitsos,Manuel Dahmen*

Main category: cs.LG

TL;DR: 基于强化学习的方法可训练Koopman代理模型用于(e)NMPC应用，该方法在大规模空分单元需求响应案例研究中表现良好，能避免约束违反并取得类似经济性能。


<details>
  <summary>Details</summary>
Motivation: 此前基于强化学习训练Koopman代理模型的方法仅在小规模案例中验证，需验证其在更具挑战性的大规模案例中的可扩展性。

Method: 使用基于强化学习的方法训练Koopman代理模型，应用于大规模单产品（氮气）空分单元的需求响应案例研究，且仅假设少数可实际测量的工厂变量可观测。

Result: 与纯系统识别的Koopman eNMPC相比，该方法能避免约束违反，同时取得类似的经济性能。

Conclusion: 基于强化学习训练Koopman代理模型的方法可扩展到更具挑战性的大规模案例研究。

Abstract: With our recently proposed method based on reinforcement learning (Mayfrank
et al. (2024), Comput. Chem. Eng. 190), Koopman surrogate models can be trained
for optimal performance in specific (economic) nonlinear model predictive
control ((e)NMPC) applications. So far, our method has exclusively been
demonstrated on a small-scale case study. Herein, we show that our method
scales well to a more challenging demand response case study built on a
large-scale model of a single-product (nitrogen) air separation unit. Across
all numerical experiments, we assume observability of only a few realistically
measurable plant variables. Compared to a purely system identification-based
Koopman eNMPC, which generates small economic savings but frequently violates
constraints, our method delivers similar economic performance while avoiding
constraint violations.

</details>


### [127] [Uncertainty Quantification for Reduced-Order Surrogate Models Applied to Cloud Microphysics](https://arxiv.org/abs/2511.04534)
*Jonas E. Katona,Emily K. de Jong,Nipun Gunawardena*

Main category: cs.LG

TL;DR: 本文提出一种后验、模型无关框架用于潜空间降阶模型的预测不确定性量化，在云微物理潜空间动力学模型上验证有效。


<details>
  <summary>Details</summary>
Motivation: 降阶模型缺乏鲁棒的不确定性量化方法，现有方法受架构或训练限制，灵活性和泛化性不足。

Method: 引入后验、模型无关框架，利用共形预测估计降阶模型管道多组件的统计预测区间。

Result: 在云微物理潜空间动力学模型上准确预测液滴尺寸分布演变并量化不确定性。

Conclusion: 所提方法能有效解决降阶模型的不确定性量化问题。

Abstract: Reduced-order models (ROMs) can efficiently simulate high-dimensional
physical systems, but lack robust uncertainty quantification methods. Existing
approaches are frequently architecture- or training-specific, which limits
flexibility and generalization. We introduce a post hoc, model-agnostic
framework for predictive uncertainty quantification in latent space ROMs that
requires no modification to the underlying architecture or training procedure.
Using conformal prediction, our approach estimates statistical prediction
intervals for multiple components of the ROM pipeline: latent dynamics,
reconstruction, and end-to-end predictions. We demonstrate the method on a
latent space dynamical model for cloud microphysics, where it accurately
predicts the evolution of droplet-size distributions and quantifies uncertainty
across the ROM pipeline.

</details>


### [128] [Integrating Temporal and Structural Context in Graph Transformers for Relational Deep Learning](https://arxiv.org/abs/2511.04557)
*Divyansha Lachi,Mahmoud Mohammadi,Joe Meyer,Vinam Arora,Tom Palczewski,Eva L. Dyer*

Main category: cs.LG

TL;DR: 现有关系数据图模型有局限，本文提出时间子图采样器和RGP架构，实验表明RGP性能达最优，为关系深度学习提供通用可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 现有关系数据图模型主要关注空间结构，将时间信息仅作为过滤约束，且多为单任务预测，无法满足需求。

Method: 引入时间子图采样器增强全局上下文，提出Relational Graph Perceiver (RGP)架构，利用基于交叉注意力的潜在瓶颈整合信息，包含灵活的交叉注意力解码器支持多任务联合学习。

Result: 在RelBench、SALT和CTU上的实验显示RGP达到了最优性能。

Conclusion: RGP为关系深度学习提供了通用且可扩展的解决方案，支持多样化的预测任务。

Abstract: In domains such as healthcare, finance, and e-commerce, the temporal dynamics
of relational data emerge from complex interactions-such as those between
patients and providers, or users and products across diverse categories. To be
broadly useful, models operating on these data must integrate long-range
spatial and temporal dependencies across diverse types of entities, while also
supporting multiple predictive tasks. However, existing graph models for
relational data primarily focus on spatial structure, treating temporal
information merely as a filtering constraint to exclude future events rather
than a modeling signal, and are typically designed for single-task prediction.
To address these gaps, we introduce a temporal subgraph sampler that enhances
global context by retrieving nodes beyond the immediate neighborhood to capture
temporally relevant relationships. In addition, we propose the Relational Graph
Perceiver (RGP), a graph transformer architecture for relational deep learning
that leverages a cross-attention-based latent bottleneck to efficiently
integrate information from both structural and temporal contexts. This latent
bottleneck integrates signals from different node and edge types into a common
latent space, enabling the model to build global context across the entire
relational system. RGP also incorporates a flexible cross-attention decoder
that supports joint learning across tasks with disjoint label spaces within a
single model. Experiments on RelBench, SALT, and CTU show that RGP delivers
state-of-the-art performance, offering a general and scalable solution for
relational deep learning with support for diverse predictive tasks.

</details>


### [129] [ARETE: an R package for Automated REtrieval from TExt with large language models](https://arxiv.org/abs/2511.04573)
*Vasco V. Branco,Jandó Benedek,Lidia Pivovarova,Luís Correia,Pedro Cardoso*

Main category: cs.LG

TL;DR: 介绍了ARETE R包，可自动化提取物种出现数据，通过对比验证其有用性，能加速获取数据。


<details>
  <summary>Details</summary>
Motivation: 缺乏关键物种数据，且现有文献数据非机器可读，需大量人力提取，因此需要自动化方法。

Method: 开发ARETE R包，利用大语言模型（chatGPT API）自动化提取数据，集成数据提取和验证流程，并与人工标注结果进行系统比较。

Result: 对比100种蜘蛛数据，新提取的数据使已知分布范围平均扩大三个数量级。

Conclusion: ARETE能加速获取未开发的出现数据，有助于资源优先分配和项目规划。

Abstract: 1. A hard stop for the implementation of rigorous conservation initiatives is
our lack of key species data, especially occurrence data. Furthermore,
researchers have to contend with an accelerated speed at which new information
must be collected and processed due to anthropogenic activity. Publications
ranging from scientific papers to gray literature contain this crucial
information but their data are often not machine-readable, requiring extensive
human work to be retrieved. 2. We present the ARETE R package, an open-source
software aiming to automate data extraction of species occurrences powered by
large language models, namely using the chatGPT Application Programming
Interface. This R package integrates all steps of the data extraction and
validation process, from Optical Character Recognition to detection of outliers
and output in tabular format. Furthermore, we validate ARETE through systematic
comparison between what is modelled and the work of human annotators. 3. We
demonstrate the usefulness of the approach by comparing range maps produced
using GBIF data and with those automatically extracted for 100 species of
spiders. Newly extracted data allowed to expand the known Extent of Occurrence
by a mean three orders of magnitude, revealing new areas where the species were
found in the past, which mayhave important implications for spatial
conservation planning and extinction risk assessments. 4. ARETE allows faster
access to hitherto untapped occurrence data, a potential game changer in
projects requiring such data. Researchers will be able to better prioritize
resources, manually verifying selected species while maintaining automated
extraction for the majority. This workflow also allows predicting available
bibliographic data during project planning.

</details>


### [130] [Complexity as Advantage: A Regret-Based Perspective on Emergent Structure](https://arxiv.org/abs/2511.04590)
*Oshri Naparstek*

Main category: cs.LG

TL;DR: 提出Complexity as Advantage (CAA)框架，以系统对不同观察者的预测遗憾来定义复杂性，统一了多种涌现行为概念，通过模型展示想法并讨论其影响。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的衡量系统复杂性的方式，解释复杂性的功能价值。

Method: 定义系统相对于观察者家族的复杂性，通过简单动态模型进行验证。

Result: 该框架统一了多尺度熵、预测信息和依赖观察者的结构等涌现行为概念。

Conclusion: 有趣的系统能为不同观察者带来不同预测遗憾，此框架可为学习、进化和智能体研究提供参考。

Abstract: We introduce Complexity as Advantage (CAA), a framework that defines the
complexity of a system relative to a family of observers. Instead of measuring
complexity as an intrinsic property, we evaluate how much predictive regret a
system induces for different observers attempting to model it. A system is
complex when it is easy for some observers and hard for others, creating an
information advantage. We show that this formulation unifies several notions of
emergent behavior, including multiscale entropy, predictive information, and
observer-dependent structure. The framework suggests that "interesting" systems
are those positioned to create differentiated regret across observers,
providing a quantitative grounding for why complexity can be functionally
valuable. We demonstrate the idea through simple dynamical models and discuss
implications for learning, evolution, and artificial agents.

</details>


### [131] [Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest Path Problems](https://arxiv.org/abs/2511.04594)
*Utkarsh U. Chavan,Prashant Trivedi,Nandyala Hemachandra*

Main category: cs.LG

TL;DR: 本文研究线性函数近似下的分散式多智能体随机最短路径问题（Dec - MASSPs），给出首个后悔下界，揭示学习难度并指导算法设计。


<details>
  <summary>Details</summary>
Motivation: 随机最短路径问题学习在单智能体场景研究广泛，但分散式多智能体变体研究不足，需填补该空白。

Method: 运用基于对称性的新论点确定最优策略结构，构建难学习实例。

Result: 得出基于任意数量智能体的首个后悔下界为Ω(√K) ，K为回合数。

Conclusion: 明确了分散式控制的学习复杂性，可指导多智能体系统中高效学习算法的设计。

Abstract: Multi-agent systems (MAS) are central to applications such as swarm robotics
and traffic routing, where agents must coordinate in a decentralized manner to
achieve a common objective. Stochastic Shortest Path (SSP) problems provide a
natural framework for modeling decentralized control in such settings. While
the problem of learning in SSP has been extensively studied in single-agent
settings, the decentralized multi-agent variant remains largely unexplored. In
this work, we take a step towards addressing that gap. We study decentralized
multi-agent SSPs (Dec-MASSPs) under linear function approximation, where the
transition dynamics and costs are represented using linear models. Applying
novel symmetry-based arguments, we identify the structure of optimal policies.
Our main contribution is the first regret lower bound for this setting based on
the construction of hard-to-learn instances for any number of agents, $n$. Our
regret lower bound of $\Omega(\sqrt{K})$, over $K$ episodes, highlights the
inherent learning difficulty in Dec-MASSPs. These insights clarify the learning
complexity of decentralized control and can further guide the design of
efficient learning algorithms in multi-agent systems.

</details>


### [132] [Environment Agnostic Goal-Conditioning, A Study of Reward-Free Autonomous Learning](https://arxiv.org/abs/2511.04598)
*Hampus Åström,Elin Anna Topp,Jacek Malec*

Main category: cs.LG

TL;DR: 研究将常规强化学习环境转化为目标条件环境，使智能体自主无奖励学习，方法与底层算法无关，虽单目标性能有波动，但平均成功率提升稳定，可通用训练。


<details>
  <summary>Details</summary>
Motivation: 探索如何让智能体在强化学习中自主无奖励地解决任务。

Method: 将常规强化学习环境转化为目标条件环境，智能体以环境无关方式选择自身目标，且方法独立于底层离策略学习算法。

Result: 单目标性能有不稳定性，但平均目标成功率提高并稳定，训练后的智能体可被指示寻找环境中的任何观测。

Conclusion: 该方法能实现智能体自主无奖励学习，可在特定用例前进行通用训练。

Abstract: In this paper we study how transforming regular reinforcement learning
environments into goal-conditioned environments can let agents learn to solve
tasks autonomously and reward-free. We show that an agent can learn to solve
tasks by selecting its own goals in an environment-agnostic way, at training
times comparable to externally guided reinforcement learning. Our method is
independent of the underlying off-policy learning algorithm. Since our method
is environment-agnostic, the agent does not value any goals higher than others,
leading to instability in performance for individual goals. However, in our
experiments, we show that the average goal success rate improves and
stabilizes. An agent trained with this method can be instructed to seek any
observations made in the environment, enabling generic training of agents prior
to specific use cases.

</details>


### [133] [Addressing divergent representations from causal interventions on neural networks](https://arxiv.org/abs/2511.04638)
*Satchel Grant,Simon Jerome Han,Alexa Tartaglini,Christopher Potts*

Main category: cs.LG

TL;DR: 研究因果干预在机械可解释性中是否会产生分布外表征及对解释忠实性的影响，分析两类分歧并改进损失函数以缓解有害分歧。


<details>
  <summary>Details</summary>
Motivation: 探究因果干预是否会产生分布外表征以及其对目标模型自然状态下解释忠实性的影响。

Method: 实证证明常见因果干预技术会使内部表征偏离目标模型自然分布；对两类分歧进行理论分析；修改CL损失函数以缓解有害分歧。

Result: 常见因果干预技术会使表征偏离自然分布；区分出无害和有害两类分歧；修改CL损失函数可降低有害分歧可能性并保留干预解释力。

Conclusion: 指出了一条实现更可靠可解释性方法的途径。

Abstract: A common approach to mechanistic interpretability is to causally manipulate
model representations via targeted interventions in order to understand what
those representations encode. Here we ask whether such interventions create
out-of-distribution (divergent) representations, and whether this raises
concerns about how faithful their resulting explanations are to the target
model in its natural state. First, we demonstrate empirically that common
causal intervention techniques often do shift internal representations away
from the natural distribution of the target model. Then, we provide a
theoretical analysis of two classes of such divergences: `harmless' divergences
that occur in the null-space of the weights and from covariance within
behavioral decision boundaries, and `pernicious' divergences that activate
hidden network pathways and cause dormant behavioral changes. Finally, in an
effort to mitigate the pernicious cases, we modify the Counterfactual Latent
(CL) loss from Grant (2025) that regularizes interventions to remain closer to
the natural distributions, reducing the likelihood of harmful divergences while
preserving the interpretive power of interventions. Together, these results
highlight a path towards more reliable interpretability methods.

</details>


### [134] [Efficient probabilistic surrogate modeling techniques for partially-observed large-scale dynamical systems](https://arxiv.org/abs/2511.04641)
*Hans Harder,Abhijeet Vishwasrao,Luca Guastoni,Ricardo Vinuesa,Sebastian Peitz*

Main category: cs.LG

TL;DR: 本文研究用概率技术预测偏微分方程描述的动力系统，比较流匹配范式的扩展方法并做实验，还解决大规模3D模拟2D切片预测挑战。


<details>
  <summary>Details</summary>
Motivation: 用概率技术对偏微分方程描述的动力系统进行预测，并减少采样步骤。

Method: 研究和比较直接蒸馏、渐进蒸馏、对抗扩散蒸馏、Wasserstein GANs和整流流等流匹配范式的扩展方法，在一组具有挑战性的系统上进行实验。

Result: 未明确提及具体实验结果。

Conclusion: 解决了直接预测大规模3D模拟2D切片的挑战，为求解器的高效流入生成铺平道路。

Abstract: This paper is concerned with probabilistic techniques for forecasting
dynamical systems described by partial differential equations (such as, for
example, the Navier-Stokes equations). In particular, it is investigating and
comparing various extensions to the flow matching paradigm that reduce the
number of sampling steps. In this regard, it compares direct distillation,
progressive distillation, adversarial diffusion distillation, Wasserstein GANs
and rectified flows. Moreover, experiments are conducted on a set of
challenging systems. In particular, we also address the challenge of directly
predicting 2D slices of large-scale 3D simulations, paving the way for
efficient inflow generation for solvers.

</details>


### [135] [Optimal Inference Schedules for Masked Diffusion Models](https://arxiv.org/abs/2511.04647)
*Sitan Chen,Kevin Cong,Jerry Li*

Main category: cs.LG

TL;DR: 本文研究扩散语言模型并行采样问题，给出真实分布与采样分布期望差异的精确刻画，得到新的上下界，还展示了基于信息论属性的新采样策略。


<details>
  <summary>Details</summary>
Motivation: 标准自回归大语言模型推理过程顺序执行，耗时且成本高，扩散语言模型可并行采样，但对其并行采样能力缺乏严格理解。

Method: 给出真实分布与采样分布期望差异的精确刻画，建立与单变量函数逼近理论的联系，基于此得到上下界，并利用信息论属性给出新采样策略。

Result: 得到新的上下界，证明在无分布先验知识时难以达到最优解，展示在某些自然设置下可在 $O(log n)$ 步内无性能损失采样。

Conclusion: 通过建立与函数逼近理论的联系，对扩散语言模型并行采样问题有了更深入理解，给出新的上下界和采样策略。

Abstract: A major bottleneck of standard auto-regressive large language models is that
their inference process is inherently sequential, resulting in very long and
costly inference times. To circumvent this, practitioners proposed a class of
language models called diffusion language models, of which the masked diffusion
model (MDM) is the most successful. The MDM is able to sample tokens
out-of-order and, ostensibly, many tokens at once and in parallel. However,
there is very limited rigorous understanding of how much parallel sampling
these models can perform without noticeable degradation in their sampling
performance. Prior work of Li and Cai obtained some preliminary bounds, but
these are not tight for many natural classes of distributions. In this work, we
give a new, exact characterization of the expected divergence between the true
distribution and the sampled distribution, for any distribution and any
unmasking schedule for the sampler, showing an elegant connection to the theory
of univariate function approximation.
  By leveraging this connection, we then attain a number of novel lower and
upper bounds for this problem. While the connection to function approximation
in principle gives the optimal unmasking schedule for any distribution, we show
that it is in general impossible to compete with it without strong a priori
knowledge of the distribution, even in seemingly benign settings. However, we
also demonstrate new upper bounds and new sampling schedules in terms of
well-studied information-theoretic properties of the base distribution, namely,
its total correlation and dual total correlation, which show that in some
natural settings, one can sample in $O(log n)$ steps without any visible loss
in performance, where $n$ is the total sequence length.

</details>


### [136] [TT-Prune: Joint Model Pruning and Resource Allocation for Communication-efficient Time-triggered Federated Learning](https://arxiv.org/abs/2511.04653)
*Xinlu Zhang,Yansha Deng,Toktam Mahmoodi*

Main category: cs.LG

TL;DR: 本文将自适应模型剪枝引入无线时间触发联邦学习系统，联合优化剪枝率和带宽分配以减少训练损失和学习延迟，仿真显示模型剪枝可降低40%通信成本且保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习网络中用户设备增多、无线带宽有限，导致掉队者和通信开销问题加剧，需要解决这些问题以提升系统性能。

Method: 对基于模型剪枝的时间触发联邦学习模型的梯度l_2范数进行收敛分析，根据收敛上界构建剪枝率和无线带宽的联合优化问题，利用KKT条件推导闭式解。

Result: 模型剪枝可将通信成本降低40%，同时保持模型性能不变。

Conclusion: 将自适应模型剪枝引入无线时间触发联邦学习系统，并联合优化剪枝率和带宽分配的方法有效，能在保证学习延迟的同时减少训练损失。

Abstract: Federated learning (FL) offers new opportunities in machine learning,
particularly in addressing data privacy concerns. In contrast to conventional
event-based federated learning, time-triggered federated learning (TT-Fed), as
a general form of both asynchronous and synchronous FL, clusters users into
different tiers based on fixed time intervals. However, the FL network consists
of a growing number of user devices with limited wireless bandwidth,
consequently magnifying issues such as stragglers and communication overhead.
In this paper, we introduce adaptive model pruning to wireless TT-Fed systems
and study the problem of jointly optimizing the pruning ratio and bandwidth
allocation to minimize the training loss while ensuring minimal learning
latency. To answer this question, we perform convergence analysis on the
gradient l_2 norm of the TT-Fed model based on model pruning. Based on the
obtained convergence upper bound, a joint optimization problem of pruning ratio
and wireless bandwidth is formulated to minimize the model training loss under
a given delay threshold. Then, we derive closed-form solutions for wireless
bandwidth and pruning ratio using Karush-Kuhn-Tucker(KKT) conditions. The
simulation results show that model pruning could reduce the communication cost
by 40% while maintaining the model performance at the same level.

</details>


### [137] [Nowcast3D: Reliable precipitation nowcasting via gray-box learning](https://arxiv.org/abs/2511.04659)
*Huaguan Chen,Wei Han,Haofei Sun,Ning Lin,Xingtao Song,Yunfan Yang,Jie Tian,Yang Liu,Ji-Rong Wen,Xiaoye Zhang,Xueshun Shen,Hao Sun*

Main category: cs.LG

TL;DR: 本文提出一种全三维临近预报框架用于极端降水临近预报，能恢复完整三维动力学，实现更准确预报。


<details>
  <summary>Details</summary>
Motivation: 现有极端降水临近预报方法存在速度慢、精度低、丢失关键信息等局限，需要新方法。

Method: 引入灰箱全三维临近预报框架，处理体积雷达反射率，结合物理约束神经算子和数据驱动学习，考虑垂直平流场、扩散、随机项等。

Result: 该框架在三小时提前预报中更准确，在气象学家盲评中57%的情况排名第一。

Conclusion: 该框架恢复完整三维动力学且具有物理一致性，为极端降水临近预报提供了可扩展且稳健的途径。

Abstract: Extreme precipitation nowcasting demands high spatiotemporal fidelity and
extended lead times, yet existing approaches remain limited. Numerical Weather
Prediction (NWP) and its deep-learning emulations are too slow and coarse for
rapidly evolving convection, while extrapolation and purely data-driven models
suffer from error accumulation and excessive smoothing. Hybrid 2D radar-based
methods discard crucial vertical information, preventing accurate
reconstruction of height-dependent dynamics. We introduce a gray-box, fully
three-dimensional nowcasting framework that directly processes volumetric radar
reflectivity and couples physically constrained neural operators with
datadriven learning. The model learns vertically varying 3D advection fields
under a conservative advection operator, parameterizes spatially varying
diffusion, and introduces a Brownian-motion--inspired stochastic term to
represent unresolved motions. A residual branch captures small-scale convective
initiation and microphysical variability, while a diffusion-based stochastic
module estimates uncertainty. The framework achieves more accurate forecasts up
to three-hour lead time across precipitation regimes and ranked first in 57\%
of cases in a blind evaluation by 160 meteorologists. By restoring full 3D
dynamics with physical consistency, it offers a scalable and robust pathway for
skillful and reliable nowcasting of extreme precipitation.

</details>


### [138] [Multi-Method Analysis of Mathematics Placement Assessments: Classical, Machine Learning, and Clustering Approaches](https://arxiv.org/abs/2511.04667)
*Julian D. Allagan,Dasia A. Singleton,Shanae N. Perry,Gabrielle C. Morgan,Essence A. Morgan*

Main category: cs.LG

TL;DR: 研究用多方法框架评估40项数学分班考试，发现部分题目需替换，机器学习算法表现出色，聚类显示能力结构，支持具体改进措施以优化数学分班。


<details>
  <summary>Details</summary>
Motivation: 评估40项数学分班考试，为优化数学分班提供依据。

Method: 结合经典测试理论、机器学习和无监督聚类的多方法框架。

Result: 55%题目区分度好，30%需替换；机器学习算法准确率高；聚类发现二元能力结构；两聚类解稳定性高。

Conclusion: 多方法集成可为基于证据的数学分班优化提供坚实的实证基础。

Abstract: This study evaluates a 40-item mathematics placement examination administered
to 198 students using a multi-method framework combining Classical Test Theory,
machine learning, and unsupervised clustering. Classical Test Theory analysis
reveals that 55\% of items achieve excellent discrimination ($D \geq 0.40$)
while 30\% demonstrate poor discrimination ($D < 0.20$) requiring replacement.
Question 6 (Graph Interpretation) emerges as the examination's most powerful
discriminator, achieving perfect discrimination ($D = 1.000$), highest ANOVA
F-statistic ($F = 4609.1$), and maximum Random Forest feature importance
(0.206), accounting for 20.6\% of predictive power. Machine learning algorithms
demonstrate exceptional performance, with Random Forest and Gradient Boosting
achieving 97.5\% and 96.0\% cross-validation accuracy. K-means clustering
identifies a natural binary competency structure with a boundary at 42.5\%,
diverging from the institutional threshold of 55\% and suggesting potential
overclassification into remedial categories. The two-cluster solution exhibits
exceptional stability (bootstrap ARI = 0.855) with perfect lower-cluster
purity. Convergent evidence across methods supports specific refinements:
replace poorly discriminating items, implement a two-stage assessment, and
integrate Random Forest predictions with transparency mechanisms. These
findings demonstrate that multi-method integration provides a robust empirical
foundation for evidence-based mathematics placement optimization.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [139] [Evolutionary Optimization Trumps Adam Optimization on Embedding Space Exploration](https://arxiv.org/abs/2511.03913)
*Domício Pereira Neto,João Correia,Penousal Machado*

Main category: cs.NE

TL;DR: 研究sep - CMA - ES与Adam在Stable Diffusion XL Turbo提示嵌入向量优化上的表现，sep - CMA - ES效果更好，凸显进化方法潜力。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型难控制和优化，嵌入空间探索尤其是进化算法是有前景的方法，故研究sep - CMA - ES与Adam在Stable Diffusion XL Turbo上的表现。

Method: 将sep - CMA - ES与广泛使用的Adam应用于Stable Diffusion XL Turbo的提示嵌入向量，用LAION Aesthetic Predictor V2和CLIPScore组合成加权适应度函数评估图像。

Result: 在Parti Prompts (P2)数据集子集实验中，sep - CMA - ES在美学和对齐指标上始终优于Adam。

Conclusion: 进化方法能为扩散模型提供高效无梯度优化，增强可控性且无需微调，强调了进化方法在深度生成模型嵌入空间探索的潜力并指出未来研究方向。

Abstract: Deep generative models, especially diffusion architectures, have transformed
image generation; however, they are challenging to control and optimize for
specific goals without expensive retraining. Embedding Space Exploration,
especially with Evolutionary Algorithms (EAs), has been shown to be a promising
method for optimizing image generation, particularly within Diffusion Models.
Therefore, in this work, we study the performance of an evolutionary
optimization method, namely Separable Covariance Matrix Adaptation Evolution
Strategy (sep-CMA-ES), against the widely adopted Adaptive Moment Estimation
(Adam), applied to Stable Diffusion XL Turbo's prompt embedding vector. The
evaluation of images combines the LAION Aesthetic Predictor V2 with CLIPScore
into a weighted fitness function, allowing flexible trade-offs between visual
appeal and adherence to prompts. Experiments on a subset of the Parti Prompts
(P2) dataset showcase that sep-CMA-ES consistently yields superior improvements
in aesthetic and alignment metrics in comparison to Adam. Results indicate that
the evolutionary method provides efficient, gradient-free optimization for
diffusion models, enhancing controllability without the need for fine-tuning.
This study emphasizes the potential of evolutionary methods for embedding space
exploration of deep generative models and outlines future research directions.

</details>


### [140] [A Reinforced Evolution-Based Approach to Multi-Resource Load Balancing](https://arxiv.org/abs/2511.04183)
*Leszek Sliwko*

Main category: cs.NE

TL;DR: 提出强化遗传方法解决d - 资源系统优化问题，对标准遗传程序做了修改。


<details>
  <summary>Details</summary>
Motivation: 经典进化模式因问题中严格的可行性函数而无效，需新方法解决d - 资源系统优化问题。

Method: 对标准遗传程序引入多项修改和调整，如引入类似生物随机遗传漂变的迁移算子。

Result: 未提及

Conclusion: 未提及

Abstract: This paper presents a reinforced genetic approach to a defined d-resource
system optimization problem. The classical evolution schema was ineffective due
to a very strict feasibility function in the studied problem. Hence, the
presented strategy has introduced several modifications and adaptations to
standard genetic routines, e.g.: a migration operator which is an analogy to
the biological random genetic drift.

</details>


### [141] [Neural Computation Without Slots: Steps Towards Biologically Plausible Memory and Attention in Natural and Artificial Intelligence](https://arxiv.org/abs/2511.04593)
*Shaunak Bhandarkar,James L. McClelland*

Main category: cs.NE

TL;DR: 本文基于现代Hopfield网络，提出扩展方法以提高其生物合理性，研究记忆存储和语言模型功能，为理解生物机制支持类人计算能力迈出一步。


<details>
  <summary>Details</summary>
Motivation: 人工智能和认知科学的模型依赖“插槽”存储模式，而生物大脑可能没有插槽，探索大脑如何在无插槽情况下实现类似功能。

Method: 基于现代Hopfield网络，提出K - winner MHN扩展到神经元集合，研究其在持续学习机制下的记忆保留情况；借鉴当代语言模型中插槽记忆的功能，扩展MHN以实现相关功能。

Result: 在持续学习机制下，基于集合的MHN比标准MHN对旧记忆的保留更好；MHN可扩展以实现当代语言模型中插槽记忆的重要功能。

Conclusion: 提出的建模方法有助于理解生物合理机制如何支持使AI系统获得类人能力的计算。

Abstract: Many models used in artificial intelligence and cognitive science rely on
multi-element patterns stored in "slots" - dedicated storage locations - in a
digital computer. As biological brains likely lack slots, we consider how they
might achieve similar functional outcomes without them by building on the
neurally-inspired modern Hopfield network (MHN; Krotov & Hopfield, 2021), which
stores patterns in the connection weights of an individual neuron. We propose
extensions of this approach to increase its biological plausibility as a model
of memory and to capture an important advantage of slot-based computation in
contemporary language models. For memory, neuroscience research suggests that
the weights of overlapping sparse ensembles of neurons, rather than a dedicated
individual neuron, are used to store a memory. We introduce the K-winner MHN,
extending the approach to ensembles, and find that within a continual learning
regime, the ensemble-based MHN exhibits greater retention of older memories, as
measured by the graded sensitivity measure d', than a standard (one-neuron)
MHN. Next, we consider the powerful use of slot-based memory in contemporary
language models. These models use slots to store long sequences of past inputs
and their learned encodings, supporting later predictions and allowing error
signals to be transported backward in time to adjust weights underlying the
learned encodings of these past inputs. Inspired by these models' successes, we
show how the MHN can be extended to capture both of these important functional
outcomes. Collectively, our modeling approaches constitute steps towards
understanding how biologically plausible mechanisms can support computations
that have enabled AI systems to capture human-like abilities that no prior
models have been able to achieve.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [142] [Tutorial Debriefing: Applied Statistical Causal Inference in Requirements Engineering](https://arxiv.org/abs/2511.03875)
*Julian Frattini,Hans-Martin Heyn,Robert Feldt,Richard Torkar*

Main category: cs.SE

TL;DR: 软件工程研究需将知识转化为实践，需因果关系证据，随机对照试验可能不可行，需从观测数据进行统计因果推断的可靠方法。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究要改善软件生产者和消费者的状况，需将研究知识转化为实践，而转化成果的价值依赖因果关系证据。

Method: 提到获取因果关系证据的直接方法是随机对照试验，但指出其可能不可行，暗示需从观测数据进行统计因果推断。

Result: 未提及具体结果。

Conclusion: 在随机对照试验不可行时，需要一个可靠的从观测数据进行统计因果推断的过程。

Abstract: As any scientific discipline, the software engineering (SE) research
community strives to contribute to the betterment of the target population of
our research: software producers and consumers. We will only achieve this
betterment if we manage to transfer the knowledge acquired during research into
practice. This transferal of knowledge may come in the form of tools,
processes, and guidelines for software developers. However, the value of these
contributions hinges on the assumption that applying them causes an improvement
of the development process, user experience, or other performance metrics. Such
a promise requires evidence of causal relationships between an exposure or
intervention (i.e., the contributed tool, process or guideline) and an outcome
(i.e., performance metrics). A straight-forward approach to obtaining this
evidence is via controlled experiments in which a sample of a population is
randomly divided into a group exposed to the new tool, process, or guideline,
and a control group. However, such randomized control trials may not be
legally, ethically, or logistically feasible. In these cases, we need a
reliable process for statistical causal inference (SCI) from observational
data.

</details>


### [143] [Collaborative Agents for Automated Program Repair in Ruby](https://arxiv.org/abs/2511.03925)
*Nikta Akbarpour,Mahdieh Sadat Benis,Fatemeh Hendijani Fard,Ali Ouni,Mohamed Aymen Saied*

Main category: cs.SE

TL;DR: 介绍用于Ruby的轻量级程序修复框架RAMP，在XCodeEval基准测试中表现良好，为多智能体修复策略提供新见解。


<details>
  <summary>Details</summary>
Motivation: 现有APR方法计算成本高且针对语言少，Ruby在APR研究中受关注少，其开发者面临持续挑战。

Method: 将程序修复构造成反馈驱动的迭代过程，使用协作智能体团队生成针对性测试、反思错误并完善候选修复方案，通过轻量级提示和测试驱动反馈直接在Ruby上操作。

Result: 在XCodeEval基准测试中，RAMP在Ruby上pass@1达67%，优于先前方法，五次迭代内快速收敛，消融研究证实测试生成和自我反思是其性能关键驱动因素，对多种错误修复有效。

Conclusion: 该方法为多智能体修复策略提供新见解，为将基于大语言模型的调试工具扩展到研究较少的语言奠定基础。

Abstract: Automated Program Repair (APR) has advanced rapidly with Large Language
Models (LLMs), but most existing methods remain computationally expensive, and
focused on a small set of languages. Ruby, despite its widespread use in web
development and the persistent challenges faced by its developers, has received
little attention in APR research. In this paper, we introduce RAMP, a novel
lightweight framework that formulates program repair as a feedback-driven,
iterative process for Ruby. RAMP employs a team of collaborative agents that
generate targeted tests, reflect on errors, and refine candidate fixes until a
correct solution is found. Unlike prior approaches, RAMP is designed to avoid
reliance on large multilingual repair databases or costly fine-tuning, instead
operating directly on Ruby through lightweight prompting and test-driven
feedback. Evaluation on the XCodeEval benchmark shows that RAMP achieves a
pass@1 of 67% on Ruby, outper-forming prior approaches. RAMP converges quickly
within five iterations, and ablation studies confirm that test generation and
self-reflection are key drivers of its performance. Further analysis shows that
RAMP is particularly effective at repairing wrong answers, compilation errors,
and runtime errors. Our approach provides new insights into multi-agent repair
strategies, and establishes a foundation for extending LLM-based debugging
tools to under-studied languages.

</details>


### [144] [PEFA-AI: Advancing Open-source LLMs for RTL generation using Progressive Error Feedback Agentic-AI](https://arxiv.org/abs/2511.03934)
*Athma Narayanan,Mahesh Subedar,Omesh Tickoo*

Main category: cs.SE

TL;DR: 提出多智能体的代理流程自动完成RTL生成，有自纠错机制，经基准测试表现良好。


<details>
  <summary>Details</summary>
Motivation: 实现无人工干预完成复杂的寄存器传输级（RTL）生成任务。

Method: 构建包含多个智能体的代理流程，采用渐进式错误反馈系统（PEFA）自纠错，用两个开源数据集进行基准测试。

Result: 在开源代理框架上实现的方法，使用开源和闭源大语言模型，缩小两者性能差距，相比之前方法通过率高且代币计数高效。

Conclusion: 所提方法为RTL生成设定新基准，提供了先进的通过率且高效。

Abstract: We present an agentic flow consisting of multiple agents that combine
specialized LLMs and hardware simulation tools to collaboratively complete the
complex task of Register Transfer Level (RTL) generation without human
intervention. A key feature of the proposed flow is the progressive error
feedback system of agents (PEFA), a self-correcting mechanism that leverages
iterative error feedback to progressively increase the complexity of the
approach. The generated RTL includes checks for compilation, functional
correctness, and synthesizable constructs. To validate this adaptive approach
to code generation, benchmarking is performed using two opensource natural
language-to-RTL datasets. We demonstrate the benefits of the proposed approach
implemented on an open source agentic framework, using both open- and
closed-source LLMs, effectively bridging the performance gap between them.
Compared to previously published methods, our approach sets a new benchmark,
providing state-of-the-art pass rates while being efficient in token counts.

</details>


### [145] [PSD2Code: Automated Front-End Code Generation from Design Files via Multimodal Large Language Models](https://arxiv.org/abs/2511.04012)
*Yongxi Chen,Lei Chen*

Main category: cs.SE

TL;DR: 本文提出PSD2Code多模态方法，通过PSD文件解析和资产对齐生成生产就绪的React+SCSS代码，评估显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有设计到代码生成方法存在结构不一致、资产对齐问题和生产就绪性有限等问题。

Method: 提出ParseAlignGenerate管道，从PSD文件提取信息，采用基于约束的对齐策略和结构化提示构建。

Result: 在多个指标上比现有方法有显著提升，且在不同大语言模型上有强模型独立性。

Conclusion: 将结构化设计信息与多模态大语言模型集成用于工业级代码生成有效，是设计驱动自动化前端开发的重要一步。

Abstract: Design-to-code generation has emerged as a promising approach to bridge the
gap between design prototypes and deployable frontend code. However, existing
methods often suffer from structural inconsistencies, asset misalignment, and
limited production readiness. This paper presents PSD2Code, a novel multi-modal
approach that leverages PSD file parsing and asset alignment to generate
production-ready React+SCSS code. Our method introduces a ParseAlignGenerate
pipeline that extracts hierarchical structures, layer properties, and metadata
from PSD files, providing large language models with precise spatial
relationships and semantic groupings for frontend code generation. The system
employs a constraint-based alignment strategy that ensures consistency between
generated elements and design resources, while a structured prompt construction
enhances controllability and code quality. Comprehensive evaluation
demonstrates significant improvements over existing methods across multiple
metrics including code similarity, visual fidelity, and production readiness.
The method exhibits strong model independence across different large language
models, validating the effectiveness of integrating structured design
information with multimodal large language models for industrial-grade code
generation, marking an important step toward design-driven automated frontend
development.

</details>


### [146] [Specification-Guided Vulnerability Detection with Large Language Models](https://arxiv.org/abs/2511.04014)
*Hao Zhu,Jia Li,Cuiyun Gao,Jiaru Qian,Yihong Dong,Huanyu Liu,Lecheng Wang,Ziliang Wang,Xiaolong Hu,Ge Li*

Main category: cs.SE

TL;DR: 现有大语言模型在漏洞检测表现欠佳，本文提出VulInstruct方法，从历史漏洞提取安全规范检测新漏洞，评估效果好且发现新漏洞。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码漏洞检测中表现有限，缺乏对安全规范的理解，难以区分漏洞代码和补丁代码。

Method: 提出VulInstruct方法，从高质量补丁构建通用规范，从特定仓库重复违规构建领域特定规范，检索相关案例和规范辅助大语言模型推理。

Result: 在PrimeVul上F1分数和召回率有显著提升，独特检测率高，成对评估有相对改善，还发现新的高严重性漏洞。

Conclusion: VulInstruct方法有效，能提高大语言模型在漏洞检测中的性能，具有实际应用价值。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
understanding tasks. However, they demonstrate limited performance in
vulnerability detection and struggle to distinguish vulnerable code from
patched code. We argue that LLMs lack understanding of security specifications
-- the expectations about how code should behave to remain safe. When code
behavior differs from these expectations, it becomes a potential vulnerability.
However, such knowledge is rarely explicit in training data, leaving models
unable to reason about security flaws. We propose VulInstruct, a
specification-guided approach that systematically extracts security
specifications from historical vulnerabilities to detect new ones. VulInstruct
constructs a specification knowledge base from two perspectives: (i) General
specifications from high-quality patches across projects, capturing fundamental
safe behaviors; and (ii) Domain-specific specifications from repeated
violations in particular repositories relevant to the target code. VulInstruct
retrieves relevant past cases and specifications, enabling LLMs to reason about
expected safe behaviors rather than relying on surface patterns. We evaluate
VulInstruct under strict criteria requiring both correct predictions and valid
reasoning. On PrimeVul, VulInstruct achieves 45.0% F1-score (32.7% improvement)
and 37.7% recall (50.8% improvement) compared to baselines, while uniquely
detecting 24.3% of vulnerabilities -- 2.4x more than any baseline. In pair-wise
evaluation, VulInstruct achieves 32.3% relative improvement. VulInstruct also
discovered a previously unknown high-severity vulnerability (CVE-2025-56538) in
production code, demonstrating practical value for real-world vulnerability
discovery. All code and supplementary materials are available at
https://github.com/zhuhaopku/VulInstruct-temp.

</details>


### [147] [LLM-Driven Adaptive Source-Sink Identification and False Positive Mitigation for Static Analysis](https://arxiv.org/abs/2511.04023)
*Shiyin Lin*

Main category: cs.SE

TL;DR: 提出LLM驱动的污点分析框架AdaTaint，评估显示其能降低误报率、提高召回率，结合LLM推理与符号验证可用于静态漏洞分析。


<details>
  <summary>Details</summary>
Motivation: 静态分析存在源-汇规范不完整和误报过多问题，需要改进。

Method: 提出LLM驱动的污点分析框架AdaTaint，通过神经符号推理自适应推断源/汇规范并过滤虚假警报，将模型建议与程序事实和约束验证相结合。

Result: 在Juliet 1.3、SV - COMP风格C基准和三个大型真实项目上评估，AdaTaint平均降低误报率43.7%，提高召回率11.2%，运行时开销有竞争力。

Conclusion: 结合LLM推理与符号验证为更准确可靠的静态漏洞分析提供了可行途径。

Abstract: Static analysis is effective for discovering software vulnerabilities but
notoriously suffers from incomplete source--sink specifications and excessive
false positives (FPs). We present \textsc{AdaTaint}, an LLM-driven taint
analysis framework that adaptively infers source/sink specifications and
filters spurious alerts through neuro-symbolic reasoning. Unlike LLM-only
detectors, \textsc{AdaTaint} grounds model suggestions in program facts and
constraint validation, ensuring both adaptability and determinism.
  We evaluate \textsc{AdaTaint} on Juliet 1.3, SV-COMP-style C benchmarks, and
three large real-world projects. Results show that \textsc{AdaTaint} reduces
false positives by \textbf{43.7\%} on average and improves recall by
\textbf{11.2\%} compared to state-of-the-art baselines (CodeQL, Joern, and
LLM-only pipelines), while maintaining competitive runtime overhead. These
findings demonstrate that combining LLM inference with symbolic validation
offers a practical path toward more accurate and reliable static vulnerability
analysis.

</details>


### [148] [Benchmarking and Studying the LLM-based Agent System in End-to-End Software Development](https://arxiv.org/abs/2511.04064)
*Zhengran Zeng,Yixin Li,Rui Xie,Wei Ye,Shikun Zhang*

Main category: cs.SE

TL;DR: 本文构建E2EDevBench并提出混合评估框架，对三种代理架构进行研究，发现当前代理约50%满足需求，瓶颈在于遗漏需求和自我验证不足，为后续研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的端到端软件开发自主代理系统的科学评估面临基准测试过于简单、难以公平比较不同架构等挑战。

Method: 构建E2EDevBench模拟真实开发场景，提出结合基于测试用例的功能评估和基于大语言模型的细粒度需求验证的混合评估框架，对三种代理架构进行对照实证研究。

Result: 当前最先进的代理在E2EDevBench上约50%满足需求，成功与否关键取决于任务分解和协作的架构策略，主要瓶颈是遗漏需求和自我验证不足。

Conclusion: 为社区提供更真实的基准测试、综合评估框架，指明提升需求理解和规划能力的未来研究方向。

Abstract: The development of LLM-based autonomous agents for end-to-end software
development represents a significant paradigm shift in software engineering.
However, the scientific evaluation of these systems is hampered by significant
challenges, including overly simplistic benchmarks and the difficulty of
conducting fair comparisons between different agent architectures due to
confounding implementation variables. To address these limitations, we first
construct a challenging and dynamically curated E2EDevBench to simulate
realistic development scenarios. Second, we propose a hybrid evaluation
framework that combines test-case-based functional assessment with
fine-grained, LLM-based requirement verification. Using this framework, we
conduct a controlled empirical study on three representative agent
architectures implemented upon a unified foundation to isolate the impact of
workflow design. Our findings reveal that state-of-the-art agents can fulfill
approximately 50\% of requirements on \bench{}, but their success is critically
dependent on the architectural strategy for task decomposition and
collaboration. Furthermore, our analysis indicates that the primary bottleneck
is the omission of requirements and inadequate self-verification. This work
provides the community with a more realistic benchmark, a comprehensive
evaluation framework, and crucial insights into the current capabilities and
core challenges of software development agents, guiding future research toward
enhancing requirement comprehension and planning.

</details>


### [149] [How Natural Language Proficiency Shapes GenAI Code for Software Engineering Tasks](https://arxiv.org/abs/2511.04115)
*Ruksit Rojpaisarnkit,Youmei Fan,Kenichi Matsumoto,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: 本文研究英语语言能力对大语言模型生成代码的影响，发现更高语言能力的提示能产生更正确的代码。


<details>
  <summary>Details</summary>
Motivation: 自然语言提示是开发者与大语言模型的关键接口，此前自然语言能力对代码生成质量的影响研究较少，故开展此研究。

Method: 使用HumanEval数据集，对164个编程任务，系统改变提示的英语能力水平，从基础到高级，并测量生成代码的能力和正确性。

Result: 大语言模型默认处于中级（B2）自然语言水平，对代码能力的影响因模型而异，但更高能力的提示在所有模型中都能产生更正确的代码。

Conclusion: 自然语言能力是控制代码生成的关键因素，可帮助开发者调整AI输出，提高解决方案的可靠性。

Abstract: With the widespread adoption of Foundation Model (FM)-powered tools in
software engineering, the natural language prompt has become a critical
interface between developers and Large Language Models (LLMs). While much
research has focused on prompt structure, the natural language proficiency is
an underexplored factor that can influence the quality of generated code. This
paper investigates whether the English language proficiency itself independent
of the prompting technique affects the proficiency and correctness of code
generated by LLMs. Using the HumanEval dataset, we systematically varied the
English proficiency of prompts from basic to advanced for 164 programming tasks
and measured the resulting code proficiency and correctness. Our findings show
that LLMs default to an intermediate (B2) natural language level. While the
effect on the resulting code proficiency was model-dependent, we found that
higher-proficiency prompts consistently yielded more correct code across all
models. These results demonstrate that natural language proficiency is a key
lever for controlling code generation, helping developers tailor AI output and
improve the reliability of solutions.

</details>


### [150] [Are We Aligned? A Preliminary Investigation of the Alignment of Responsible AI Values between LLMs and Human Judgment](https://arxiv.org/abs/2511.04157)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.SE

TL;DR: 研究23个大语言模型在负责任AI价值偏好上与美国代表性样本和AI从业者的对齐程度，发现与从业者更一致，但存在言行不一问题，强调需人工监督和系统方法。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在软件工程任务中其价值偏好与人类判断在负责任AI价值上的对齐程度。

Method: 对23个大语言模型进行四项任务评估，包括选择关键价值、评估重要性、解决价值权衡和优先考虑体现价值的软件需求。

Result: 大语言模型与AI从业者的对齐程度高于美国代表性样本，强调公平、隐私等价值，但在宣称价值和需求优先级上存在不一致。

Conclusion: 依赖大语言模型进行需求工程有实际风险，需要系统方法来衡量、解释和监控AI辅助软件开发中的价值对齐。

Abstract: Large Language Models (LLMs) are increasingly employed in software
engineering tasks such as requirements elicitation, design, and evaluation,
raising critical questions regarding their alignment with human judgments on
responsible AI values. This study investigates how closely LLMs' value
preferences align with those of two human groups: a US-representative sample
and AI practitioners. We evaluate 23 LLMs across four tasks: (T1) selecting key
responsible AI values, (T2) rating their importance in specific contexts, (T3)
resolving trade-offs between competing values, and (T4) prioritizing software
requirements that embody those values. The results show that LLMs generally
align more closely with AI practitioners than with the US-representative
sample, emphasizing fairness, privacy, transparency, safety, and
accountability. However, inconsistencies appear between the values that LLMs
claim to uphold (Tasks 1-3) and the way they prioritize requirements (Task 4),
revealing gaps in faithfulness between stated and applied behavior. These
findings highlight the practical risk of relying on LLMs in requirements
engineering without human oversight and motivate the need for systematic
approaches to benchmark, interpret, and monitor value alignment in AI-assisted
software development.

</details>


### [151] [Explaining Software Vulnerabilities with Large Language Models](https://arxiv.org/abs/2511.04179)
*Oshando Johnson,Alexandra Fomina,Ranjith Krishnamurthy,Vaibhav Chaudhari,Rohith Kumar Shanmuganathan,Eric Bodden*

Main category: cs.SE

TL;DR: 本文提出SAFE插件利用GPT - 4o解释SAST工具检测出的漏洞，研究表明其能提升SAST工具可用性。


<details>
  <summary>Details</summary>
Motivation: SAST工具通用警告信息无法充分传达重要信息，导致开发者误解或忽略关键发现，需要解决SAST可解释性挑战。

Method: 提出SAFE，一个利用GPT - 4o的IDE插件来解释SAST工具检测出的漏洞的原因、影响和缓解策略。

Result: 专家用户研究发现SAFE生成的解释能显著帮助初、中级开发者理解和处理安全漏洞。

Conclusion: SAFE能提高SAST工具的整体可用性。

Abstract: The prevalence of security vulnerabilities has prompted companies to adopt
static application security testing (SAST) tools for vulnerability detection.
Nevertheless, these tools frequently exhibit usability limitations, as their
generic warning messages do not sufficiently communicate important information
to developers, resulting in misunderstandings or oversight of critical
findings. In light of recent developments in Large Language Models (LLMs) and
their text generation capabilities, our work investigates a hybrid approach
that uses LLMs to tackle the SAST explainability challenges. In this paper, we
present SAFE, an Integrated Development Environment (IDE) plugin that leverages
GPT-4o to explain the causes, impacts, and mitigation strategies of
vulnerabilities detected by SAST tools. Our expert user study findings indicate
that the explanations generated by SAFE can significantly assist beginner to
intermediate developers in understanding and addressing security
vulnerabilities, thereby improving the overall usability of SAST tools.

</details>


### [152] [GITER: A Git-Based Declarative Exchange Model Using Kubernetes-Style Custom Resources](https://arxiv.org/abs/2511.04182)
*Christos Tranoris*

Main category: cs.SE

TL;DR: 本文提出用Git作为协调媒介，实现分布式实体间异步信息交换的轻量级可审计方法，拓展了GitOps应用场景并分析其优劣。


<details>
  <summary>Details</summary>
Motivation: 为分布式实体间异步信息交换提供新方法，拓展GitOps应用到更多协作场景。

Method: 用基于Kubernetes Operators和CRs的Git通信模型替代传统API和消息代理，参与实体通过共享仓库交互。

Result: 该模型利用Git特性确保了透明度、可追溯性和可重复性，保持系统间松耦合和自主性。

Conclusion: 讨论了架构原则、实现考量，对比了与其他集成方式，指出采用Git作为声明式通信基础的优缺点。

Abstract: This paper introduces a lightweight and auditable method for asynchronous
information exchange between distributed entities using Git as the coordination
medium. The proposed approach replaces traditional APIs and message brokers
with a Git-based communication model built on the principles of Kubernetes
Operators and Custom Resources (CRs). Each participating entity, designated as
a Publisher or Consumer, interacts through a shared repository that serves as a
single source of truth, where the spec field captures the desired state and the
status field reflects the observed outcome. This pattern extends GitOps beyond
infrastructure management to support cross-domain, inter-organizational, and
air-gapped collaboration scenarios. By leveraging Git native features
(versioning, commit signing, and access control) the model ensures
transparency, traceability, and reproducibility while preserving loose coupling
and autonomy between systems. The paper discusses architectural principles,
implementation considerations, and comparisons with RESTful and broker-based
integrations, highlighting both the advantages and trade-offs of adopting Git
as a declarative communication substrate.

</details>


### [153] [A Tool for Benchmarking Large Language Models' Robustness in Assessing the Realism of Driving Scenarios](https://arxiv.org/abs/2511.04267)
*Jiahui Wu,Chengjie Lu,Aitor Arrieta,Shaukat Ali*

Main category: cs.SE

TL;DR: 提出DriveRLR工具评估大语言模型评估驾驶场景真实性的鲁棒性，在数据集上验证其有效性，还可用于场景生成等应用。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统安全测试中，模拟场景真实性评估困难，大语言模型有潜力用于此评估，因此需工具评估大语言模型在该任务中的鲁棒性。

Method: 提出DriveRLR工具，生成变异场景变体、构建提示，评估大语言模型判断驾驶场景真实性的能力和鲁棒性。

Result: 在DeepScenario数据集上用三个先进大语言模型验证，DriveRLR有效揭示不同大语言模型鲁棒性差异。

Conclusion: DriveRLR在场景真实性评估中有效且有实用价值，还可用于场景生成等应用。

Abstract: In recent years, autonomous driving systems have made significant progress,
yet ensuring their safety remains a key challenge. To this end, scenario-based
testing offers a practical solution, and simulation-based methods have gained
traction due to the high cost and risk of real-world testing. However,
evaluating the realism of simulated scenarios remains difficult, creating
demand for effective assessment methods. Recent advances show that Large
Language Models (LLMs) possess strong reasoning and generalization
capabilities, suggesting their potential in assessing scenario realism through
scenario-related textual prompts. Motivated by this, we propose DriveRLR, a
benchmark tool to assess the robustness of LLMs in evaluating the realism of
driving scenarios. DriveRLR generates mutated scenario variants, constructs
prompts, which are then used to assess a given LLM's ability and robustness in
determining the realism of driving scenarios. We validate DriveRLR on the
DeepScenario dataset using three state-of-the-art LLMs: GPT-5, Llama 4
Maverick, and Mistral Small 3.2. Results show that DriveRLR effectively reveals
differences in the robustness of various LLMs, demonstrating its effectiveness
and practical value in scenario realism assessment. Beyond LLM robustness
evaluation, DriveRLR can serve as a practical component in applications such as
an objective function to guide scenario generation, supporting simulation-based
ADS testing workflows.

</details>


### [154] [Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation Benchmarks](https://arxiv.org/abs/2511.04355)
*Amir Molzam Sharifloo,Maedeh Heydari,Parsa Kazerooni,Daniel Maninger,Mira Mezini*

Main category: cs.SE

TL;DR: 研究大语言模型在代码生成任务中常失败的情况，分析失败原因并揭示其弱点模式和基准任务常见问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准和排行榜对大语言模型在代码生成中持续失败的任务洞察有限，为了解当前局限和指导开发更强大模型。

Method: 检查四个流行基准中的代码生成任务，确定主要大语言模型最可能失败的任务，研究解决方案代码的静态复杂度对失败的影响，并系统检查114个大语言模型持续挣扎的任务。

Result: 分析揭示了大语言模型的四个反复出现的弱点模式，以及基准任务中最常导致失败的常见复杂情况。

Conclusion: 通过研究可深入了解大语言模型在代码生成中的局限性，为后续模型开发提供参考。

Abstract: Large Language Models (LLMs) have achieved remarkable success in code
generation, and the race to improve their performance has become a central
focus of AI research. Benchmarks and leaderboards are increasingly popular,
offering quantitative rankings of LLMs. However, they provide limited insight
into the tasks that LLMs consistently fail to solve - information that is
crucial for understanding current limitations and guiding the development of
more capable models. To address this gap, we examined code generation tasks
across four popular benchmarks, identifying those that major LLMs are most
likely to fail. To understand the causes of these failures, we investigated
whether the static complexity of solution code contributes to them, followed by
a systematic inspection of 114 tasks that LLMs consistently struggled with. Our
analysis revealed four recurring patterns of weaknesses in LLMs, as well as
common complications within benchmark tasks that most often lead to failure.

</details>


### [155] [Speed at the Cost of Quality? The Impact of LLM Agent Assistance on Software Development](https://arxiv.org/abs/2511.04427)
*Hao He,Courtney Miller,Shyam Agarwal,Christian Kästner,Bogdan Vasilescu*

Main category: cs.SE

TL;DR: 本文估计使用Cursor对开发速度和软件质量的因果影响，发现其对开发速度有短期提升，但会增加静态分析警告和代码复杂度，导致长期速度放缓。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理在软件开发中的应用虽声称能提高生产力，但缺乏实证，本文旨在估计使用Cursor对开发速度和软件质量的因果影响。

Method: 采用最先进的双重差分设计，比较使用Cursor的GitHub项目和未使用的匹配对照组，并进行面板广义矩估计。

Result: 采用Cursor会使项目级开发速度显著、大幅但短暂提升，同时静态分析警告和代码复杂度显著且持续增加，后者是长期速度放缓的主要因素。

Conclusion: 研究对软件工程从业者、LLM代理助手设计者和研究人员有启示意义。

Abstract: Large language models (LLMs) have demonstrated the promise to revolutionize
the field of software engineering. Among other things, LLM agents are rapidly
gaining momentum in their application to software development, with
practitioners claiming a multifold productivity increase after adoption. Yet,
empirical evidence is lacking around these claims. In this paper, we estimate
the causal effect of adopting a widely popular LLM agent assistant, namely
Cursor, on development velocity and software quality. The estimation is enabled
by a state-of-the-art difference-in-differences design comparing
Cursor-adopting GitHub projects with a matched control group of similar GitHub
projects that do not use Cursor. We find that the adoption of Cursor leads to a
significant, large, but transient increase in project-level development
velocity, along with a significant and persistent increase in static analysis
warnings and code complexity. Further panel generalized method of moments
estimation reveals that the increase in static analysis warnings and code
complexity acts as a major factor causing long-term velocity slowdown. Our
study carries implications for software engineering practitioners, LLM agent
assistant designers, and researchers.

</details>


### [156] [EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed Code Edits](https://arxiv.org/abs/2511.04486)
*Wayne Chi,Valerie Chen,Ryan Shar,Aditya Mittal,Jenny Liang,Wei-Lin Chiang,Anastasios Nikolas Angelopoulos,Ion Stoica,Graham Neubig,Ameet Talwalkar,Chris Donahue*

Main category: cs.SE

TL;DR: 本文介绍EDIT - Bench基准来评估大语言模型代码编辑能力，用真实数据测试40个模型，发现问题有挑战性，模型性能因指令类别和上下文信息而异。


<details>
  <summary>Details</summary>
Motivation: 现有基准很少直接评估大语言模型代码编辑能力，且当前数据集多依赖人工来源。

Method: 引入基于真实场景的EDIT - Bench基准，包含545个问题、多种语言和真实用例，还引入依赖上下文的问题，评估40个大语言模型。

Result: EDIT - Bench具有挑战性，仅5个模型得分超60%，模型性能因用户指令类别不同而有差异，不同上下文信息水平对任务成功率影响达11%。

Conclusion: 强调使用真实上下文进行评估对大语言模型代码编辑能力评估的重要性。

Abstract: Instructed code editing, where LLMs directly modify a developer's existing
code based on a user instruction, is becoming a widely used interaction mode in
AI coding assistants. However, few benchmarks directly evaluate this capability
and current datasets often rely on artificial sources. We introduce EDIT-Bench,
a benchmark for evaluating LLM code editing capabilities grounded in real-world
usage, i.e., user instructions and code contexts collected in the wild.
EDIT-Bench comprises of 545 problems, multiple natural and programming
languages, and a diverse set of real-world use cases, ranging from resolving
errors to adding features. EDIT-Bench introduces context-dependent problems
that require the model to understand code context, highlighted code, and cursor
position in addition to the user instruction. We evaluate 40 diverse LLMs and
observe that EDIT-Bench is a challenging set of problems where only 5 models
score over 60%. We find that model performance varies across different
categories of user instructions. Further, we find that varying levels of
contextual information greatly affect task success rate, with performance
varying up to 11%, indicating the importance of evaluating with realistic
context.

</details>


### [157] [Microservices Is Dying, A New Method for Module Division Based on Universal Interfaces](https://arxiv.org/abs/2511.04548)
*Qing Wang,Yong Zhang*

Main category: cs.SE

TL;DR: 本文从模块变更影响评估出发，提出计算模块独立性的方法，给出模块独立的必要条件，提出消除模块依赖的设计理念和方法论，设计通用接口，实现EIGHT平台架构，为复杂系统探索新路径。


<details>
  <summary>Details</summary>
Motivation: 微服务虽模块物理隔离，但未能阻止依赖传播扩散，需追溯模块间耦合根源。

Method: 从模块变更影响评估入手，提出计算模块独立性的概念方法，推导模块独立必要条件；提出新系统设计理念和软件工程方法论；用特定模式设计通用接口；用此方法实现EIGHT平台架构。

Result: 实现EIGHT平台架构，证明只要保证模块独立，单进程内的单体应用也能在运行时动态加载、卸载或修改任意部分。

Conclusion: 该架构为日益复杂的系统探索了一条超越微服务和单体架构的新路径。

Abstract: Although microservices have physically isolated modules, they have failed to
prevent the propagation and diffusion of dependencies. To trace the root cause
of the inter-module coupling, this paper, starting from the impact assessment
approach for module changes, proposes a conceptual method for calculating
module independence and utilizes this method to derive the necessary conditions
for module independence. Then, a new system design philosophy and software
engineering methodology is proposed, aimed at eliminating dependencies between
modules. A specific pattern is employed to design a set of universal
interfaces, serving as a universal boundary between modules. Subsequently, this
method is used to implement a platform architecture named EIGHT, demonstrating
that, as long as module independence is guaranteed, even a monolithic
application within a single process can dynamically load, unload, or modify any
part at runtime. Finally, the paper concludes that this architecture aims to
explore a novel path for increasingly complex systems, beyond microservice and
monolithic architectures.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [158] [Causal Regime Detection in Energy Markets With Augmented Time Series Structural Causal Models](https://arxiv.org/abs/2511.04361)
*Dennis Thumm*

Main category: q-fin.CP

TL;DR: 提出适用于能源市场的增强时间序列因果模型（ATSCM），可对多元时间序列数据进行反事实推理，应用于电价数据可实现新的反事实查询。


<details>
  <summary>Details</summary>
Motivation: 当前能源市场电价建模方法缺乏明确因果解释和反事实推理能力。

Method: 引入增强时间序列因果模型（ATSCM），将反事实推理框架扩展到具有学习因果结构的多元时间数据，集成神经因果发现学习时变因果图。

Result: 应用于实际电价数据，可实现如‘不同可再生能源发电情景下电价如何’等新的反事实查询。

Conclusion: ATSCM为能源市场建模提供了一种能进行反事实推理且具有因果解释的新方法。

Abstract: Energy markets exhibit complex causal relationships between weather patterns,
generation technologies, and price formation, with regime changes occurring
continuously rather than at discrete break points. Current approaches model
electricity prices without explicit causal interpretation or counterfactual
reasoning capabilities. We introduce Augmented Time Series Causal Models
(ATSCM) for energy markets, extending counterfactual reasoning frameworks to
multivariate temporal data with learned causal structure. Our approach models
energy systems through interpretable factors (weather, generation mix, demand
patterns), rich grid dynamics, and observable market variables. We integrate
neural causal discovery to learn time-varying causal graphs without requiring
ground truth DAGs. Applied to real-world electricity price data, ATSCM enables
novel counterfactual queries such as "What would prices be under different
renewable generation scenarios?".

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [159] [Mean-field approximations in insurance](https://arxiv.org/abs/2511.04198)
*Philipp C. Hornung*

Main category: q-fin.RM

TL;DR: 用平均场近似解决依赖个体保险负债计算的高维问题，证明保险负债收敛到其平均场近似，并举例说明重要性。


<details>
  <summary>Details</summary>
Motivation: 解决依赖个体保险负债计算中高维耦合线性前向积分 - 微分方程组难以求解的问题。

Method: 使用平均场近似，将高维线性前向方程组替换为低维非线性前向积分 - 微分方程组。

Result: 在一定正则条件下，当群体中个体数量趋于无穷时，保险负债作为潜在跳跃过程泛函的（条件）期望收敛到其平均场近似。

Conclusion: 平均场近似在寿险和非寿险中具有实际重要性。

Abstract: The calculation of the insurance liabilities of a cohort of dependent
individuals in general requires the solution of a high-dimensional system of
coupled linear forward integro-differential equations, which is infeasible for
a larger cohort. However, by using a mean-field approximation, the high
dimensional system of linear forward equations can be replaced by a
low-dimensional system of non-linear forward integro-differential equations. We
show that, subject to certain regularity conditions, the insurance liability
viewed as a (conditional) expectation of a functional of an underlying jump
process converges to its mean-field approximation, as the number of individuals
in the cohort goes to infinity. Examples from both life- and non-life insurance
illuminate the practical importance of mean-field approximations.

</details>


### [160] [On the Estimation of Own Funds for Life Insurers: A Study of Direct, Indirect, and Control Variate Methods in a Risk-Neutral Pricing Framework](https://arxiv.org/abs/2511.04412)
*Mark-Oliver Wolf*

Main category: q-fin.RM

TL;DR: 研究Solvency II下偿付能力资本要求（SCR）计算中直接和间接估计方法，提出混合估计量和方差缩减技术，评估显示方法优劣依赖模型，代码公开。


<details>
  <summary>Details</summary>
Motivation: 研究Solvency II下SCR计算中直接和间接估计方法的比较性质并给出新见解。

Method: 证明直接和间接估计量收敛到相同值；引入混合估计量家族；利用估计量开发方差缩减技术。

Result: 直接和间接估计量无普遍优越性，间接法在更现实场景表现更好；控制变量技术有显著潜力，但效果依赖模型。

Conclusion: SCR计算中估计方法的优劣和方差缩减技术的效果均依赖模型。

Abstract: The Solvency Capital Requirement (SCR) calculation under Solvency II is
computationally intensive, relying on the estimation of own funds. Regulation
mandates the direct estimation method. It has been proven that under specific
assumptions, the indirect method results in the same estimate. We study their
comparative properties and give novel insights.
  First, we provide a straightforward proof that the direct and indirect
estimators for own funds converge to the same value. Second, we introduce a
novel family of mixed estimators that encompasses the direct and indirect
methods as its edge cases. Third, we leverage these estimators to develop
powerful variance reduction techniques, constructing a single control variate
from the direct and indirect estimators and a multi-control variate framework
using subsets of the mixed family. These techniques can be combined with
existing methods like Least-Squares Monte Carlo.
  We evaluate the estimators on three simplified asset-liability management
models of a German life insurer, Bauer's model MUST and IS case from Bauer et
al. (2006), and openIRM by Wolf et al. (2025). Our analysis confirms that
neither the direct nor indirect estimator is universally superior, though the
indirect method consistently outperforms the direct one in more realistic
settings. The proposed control variate techniques show significant potential,
in some cases reducing variance to one-tenth of that from the standard direct
estimator. However, we also identify scenarios where improvements are marginal,
highlighting the model-dependent nature of their efficacy.
  The source code is publicly available at
https://gitlab.cc-asp.fraunhofer.de/itwm-fm-lv-public/wolf-estimation-of-own-funds.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [161] [Learning Paths for Dynamic Measure Transport: A Control Perspective](https://arxiv.org/abs/2511.03797)
*Aimee Maurais,Bamdad Hosseini,Youssef Marzouk*

Main category: stat.ML

TL;DR: 从控制视角研究动态测度传输（DMT）中测度路径识别问题，提出优化问题和数值算法，能恢复更高效平滑传输模型。


<details>
  <summary>Details</summary>
Motivation: 常用路径可能不适合DMT，需找到更好的测度路径。

Method: 将现有学习替代路径的方法与平均场博弈联系起来，提出优化问题，用基于高斯过程求解偏微分方程的数值算法求解。

Result: 与使用未倾斜参考路径的方法相比，能恢复更高效和光滑的传输模型。

Conclusion: 所提方法可有效解决DMT中测度路径识别问题，获得更好的传输模型。

Abstract: We bring a control perspective to the problem of identifying paths of
measures for sampling via dynamic measure transport (DMT). We highlight the
fact that commonly used paths may be poor choices for DMT and connect existing
methods for learning alternate paths to mean-field games. Based on these
connections we pose a flexible family of optimization problems for identifying
tilted paths of measures for DMT and advocate for the use of objective terms
which encourage smoothness of the corresponding velocities. We present a
numerical algorithm for solving these problems based on recent Gaussian process
methods for solution of partial differential equations and demonstrate the
ability of our method to recover more efficient and smooth transport models
compared to those which use an untilted reference path.

</details>


### [162] [Friction on Demand: A Generative Framework for the Inverse Design of Metainterfaces](https://arxiv.org/abs/2511.03735)
*Valentin Mouton,Adrien Mélot*

Main category: stat.ML

TL;DR: 本文引入基于变分自编码器（VAEs）的生成建模框架，从目标摩擦定律推断表面形貌，可高效生成候选形貌，分析了该方法权衡目标时的权衡和实际考虑，为摩擦行为实时控制铺路。


<details>
  <summary>Details</summary>
Motivation: 设计具有规定宏观行为的摩擦界面是具有挑战性的逆问题，传统方法依赖低维参数化启发式搜索，限制了其在复杂或非线性摩擦定律中的应用。

Method: 引入使用变分自编码器（VAEs）的生成建模框架，在由参数化接触力学模型构建的2亿个样本合成数据集上训练。

Result: 结果突出了在平衡准确性、吞吐量和多样性目标时的权衡，并给出了实际考虑。

Conclusion: 该方法为通过定制表面形貌实现摩擦行为的近实时控制铺平了道路。

Abstract: Designing frictional interfaces to exhibit prescribed macroscopic behavior is
a challenging inverse problem, made difficult by the non-uniqueness of
solutions and the computational cost of contact simulations. Traditional
approaches rely on heuristic search over low-dimensional parameterizations,
which limits their applicability to more complex or nonlinear friction laws. We
introduce a generative modeling framework using Variational Autoencoders (VAEs)
to infer surface topographies from target friction laws. Trained on a synthetic
dataset composed of 200 million samples constructed from a parameterized
contact mechanics model, the proposed method enables efficient, simulation-free
generation of candidate topographies. We examine the potential and limitations
of generative modeling for this inverse design task, focusing on balancing
accuracy, throughput, and diversity in the generated solutions. Our results
highlight trade-offs and outline practical considerations when balancing these
objectives. This approach paves the way for near-real-time control of
frictional behavior through tailored surface topographies.

</details>


### [163] [Bifidelity Karhunen-Loève Expansion Surrogate with Active Learning for Random Fields](https://arxiv.org/abs/2511.03756)
*Aniket Jivani,Cosmin Safta,Beckett Y. Zhou,Xun Huan*

Main category: stat.ML

TL;DR: 提出双保真Karhunen - Loève展开（KLE）代理模型处理不确定输入下的场值感兴趣量，结合KLE和PCE，采用主动学习策略，在三个不同复杂度例子中验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 构建能准确且计算成本可承受的代理模型，处理不确定输入下场值感兴趣量。

Method: 结合KLE的谱效率与PCE，耦合低保真和高保真模拟；采用主动学习策略，基于交叉验证和高斯过程回归选择新高保真评估点。

Result: 在三个复杂度递增例子中，相对单保真和随机采样方法，该方法在预测准确性和样本效率上取得持续提升。

Conclusion: 所提BF - KLE - AL框架有效，能提高预测准确性和样本效率。

Abstract: We present a bifidelity Karhunen-Lo\`eve expansion (KLE) surrogate model for
field-valued quantities of interest (QoIs) under uncertain inputs. The approach
combines the spectral efficiency of the KLE with polynomial chaos expansions
(PCEs) to preserve an explicit mapping between input uncertainties and output
fields. By coupling inexpensive low-fidelity (LF) simulations that capture
dominant response trends with a limited number of high-fidelity (HF)
simulations that correct for systematic bias, the proposed method enables
accurate and computationally affordable surrogate construction. To further
improve surrogate accuracy, we form an active learning strategy that adaptively
selects new HF evaluations based on the surrogate's generalization error,
estimated via cross-validation and modeled using Gaussian process regression.
New HF samples are then acquired by maximizing an expected improvement
criterion, targeting regions of high surrogate error. The resulting BF-KLE-AL
framework is demonstrated on three examples of increasing complexity: a
one-dimensional analytical benchmark, a two-dimensional convection-diffusion
system, and a three-dimensional turbulent round jet simulation based on
Reynolds-averaged Navier--Stokes (RANS) and enhanced delayed detached-eddy
simulations (EDDES). Across these cases, the method achieves consistent
improvements in predictive accuracy and sample efficiency relative to
single-fidelity and random-sampling approaches.

</details>


### [164] [Online Bayesian Experimental Design for Partially Observed Dynamical Systems](https://arxiv.org/abs/2511.04403)
*Sara Pérez-Vieites,Sahel Iqbal,Simo Särkkä,Dominik Baumann*

Main category: stat.ML

TL;DR: 本文提出新方法解决贝叶斯实验设计在部分可观测动态系统中的难题，通过新的EIG估计器和嵌套粒子滤波器实现可扩展随机优化，应用于实际模型取得成功。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯实验设计方法不适用于部分可观测动态系统，该系统用状态空间模型建模使似然函数和信息论目标难以处理，且需要高效在线算法。

Method: 推导新的EIG及其梯度估计器来显式边缘化潜在状态，利用嵌套粒子滤波器进行高效在线推理。

Result: 应用于SIR模型和移动源定位任务，框架能成功处理部分可观测和在线计算问题。

Conclusion: 提出的方法可解决贝叶斯实验设计在部分可观测动态系统中的难题，实现可扩展随机优化。

Abstract: Bayesian experimental design (BED) provides a principled framework for
optimizing data collection, but existing approaches do not apply to crucial
real-world settings such as dynamical systems with partial observability, where
only noisy and incomplete observations are available. These systems are
naturally modeled as state-space models (SSMs), where latent states mediate the
link between parameters and data, making the likelihood -- and thus
information-theoretic objectives like the expected information gain (EIG) --
intractable. In addition, the dynamical nature of the system requires online
algorithms that update posterior distributions and select designs sequentially
in a computationally efficient manner. We address these challenges by deriving
new estimators of the EIG and its gradient that explicitly marginalize latent
states, enabling scalable stochastic optimization in nonlinear SSMs. Our
approach leverages nested particle filters (NPFs) for efficient online
inference with convergence guarantees. Applications to realistic models, such
as the susceptible-infected-recovered (SIR) and a moving source location task,
show that our framework successfully handles both partial observability and
online computation.

</details>


### [165] [A general technique for approximating high-dimensional empirical kernel matrices](https://arxiv.org/abs/2511.03892)
*Chiraag Kaushik,Justin Romberg,Vidya Muthukumar*

Main category: stat.ML

TL;DR: 本文给出随机核矩阵期望算子范数的简单易使用的界，应用于高维数据核矩阵近似和核回归偏差下界。


<details>
  <summary>Details</summary>
Motivation: 为随机核矩阵的期望算子范数提供通用条件下的界。

Method: 使用U - 统计量的解耦结果和非交换Khintchine不等式，结合矩方法和组合论证。

Result: 得到仅依赖核函数标量统计和相关核矩阵的上下界，为内积核矩阵提供更紧近似，简化已有结果证明，给出各向异性高斯数据新近似结果和核回归偏差更紧下界。

Conclusion: 所提方法有效，可用于多种核矩阵相关问题。

Abstract: We present simple, user-friendly bounds for the expected operator norm of a
random kernel matrix under general conditions on the kernel function
$k(\cdot,\cdot)$. Our approach uses decoupling results for U-statistics and the
non-commutative Khintchine inequality to obtain upper and lower bounds
depending only on scalar statistics of the kernel function and a ``correlation
kernel'' matrix corresponding to $k(\cdot,\cdot)$. We then apply our method to
provide new, tighter approximations for inner-product kernel matrices on
general high-dimensional data, where the sample size and data dimension are
polynomially related. Our method obtains simplified proofs of existing results
that rely on the moment method and combinatorial arguments while also providing
novel approximation results for the case of anisotropic Gaussian data. Finally,
using similar techniques to our approximation result, we show a tighter lower
bound on the bias of kernel regression with anisotropic Gaussian data.

</details>


### [166] [High-dimensional limit theorems for SGD: Momentum and Adaptive Step-sizes](https://arxiv.org/abs/2511.03952)
*Aukosh Jagannath,Taj Jones-McCormick,Varnan Sarangian*

Main category: stat.ML

TL;DR: 本文为带Polyak动量和自适应步长的随机梯度下降（SGD - M）开发高维缩放极限，比较SGD - M和在线SGD，通过实例展示自适应步长在线SGD优势及早期预条件器作用。


<details>
  <summary>Details</summary>
Motivation: 为严格比较在线SGD及其流行变体提供框架。

Method: 开发SGD - M的高维缩放极限，在尖峰张量PCA和单指标模型两个学习问题上进行验证。

Result: SGD - M缩放极限经适当时间重标度和步长选择后与在线SGD一致；步长相同时，SGD - M会放大高维效应；自适应步长在线SGD有诸多好处。

Conclusion: 早期预条件器能在在线SGD失效的情况下稳定和改善动态，与经验动机相符。

Abstract: We develop a high-dimensional scaling limit for Stochastic Gradient Descent
with Polyak Momentum (SGD-M) and adaptive step-sizes. This provides a framework
to rigourously compare online SGD with some of its popular variants. We show
that the scaling limits of SGD-M coincide with those of online SGD after an
appropriate time rescaling and a specific choice of step-size. However, if the
step-size is kept the same between the two algorithms, SGD-M will amplify
high-dimensional effects, potentially degrading performance relative to online
SGD. We demonstrate our framework on two popular learning problems: Spiked
Tensor PCA and Single Index Models. In both cases, we also examine online SGD
with an adaptive step-size based on normalized gradients. In the
high-dimensional regime, this algorithm yields multiple benefits: its dynamics
admit fixed points closer to the population minimum and widens the range of
admissible step-sizes for which the iterates converge to such solutions. These
examples provide a rigorous account, aligning with empirical motivation, of how
early preconditioners can stabilize and improve dynamics in settings where
online SGD fails.

</details>


### [167] [Robust inference using density-powered Stein operators](https://arxiv.org/abs/2511.03963)
*Shinto Eguchi*

Main category: stat.ML

TL;DR: 本文引入γ - Stein算子，基于γ - 散度构建，用于非归一化概率模型的鲁棒推理，开发两个应用，实证显示其性能优于标准基线。


<details>
  <summary>Details</summary>
Motivation: 为非归一化概率模型构建鲁棒推理方法。

Method: 引入γ - Stein算子，基于γ - 散度，通过模型密度的正幂γ加权构建，推导其在得分匹配上的鲁棒推广，开发γ - 核化Stein差异和γ - Stein变分梯度下降两个应用。

Result: 在受污染的高斯和四次势模型的实证中，方法在鲁棒性和统计效率上显著优于标准基线。

Conclusion: γ - Stein算子及其应用能为非归一化概率模型提供更鲁棒和高效的推理方法。

Abstract: We introduce a density-power weighted variant for the Stein operator, called
the $\gamma$-Stein operator. This is a novel class of operators derived from
the $\gamma$-divergence, designed to build robust inference methods for
unnormalized probability models. The operator's construction (weighting by the
model density raised to a positive power $\gamma$ inherently down-weights the
influence of outliers, providing a principled mechanism for robustness.
Applying this operator yields a robust generalization of score matching that
retains the crucial property of being independent of the model's normalizing
constant. We extend this framework to develop two key applications: the
$\gamma$-kernelized Stein discrepancy for robust goodness-of-fit testing, and
$\gamma$-Stein variational gradient descent for robust Bayesian posterior
approximation. Empirical results on contaminated Gaussian and quartic potential
models show our methods significantly outperform standard baselines in both
robustness and statistical efficiency.

</details>


### [168] [Online Conformal Inference with Retrospective Adjustment for Faster Adaptation to Distribution Shift](https://arxiv.org/abs/2511.04275)
*Jungbin Jun,Ilsang Ohn*

Main category: stat.ML

TL;DR: 本文提出带回顾性调整的在线共形推理方法，能更快适应分布变化，在合成和真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有在线共形预测方法在数据分布随时间变化的在线环境中，因仅向前更新预测，难以快速适应分布变化。

Method: 提出带回顾性调整的在线共形推理方法，利用回归方法和高效留一法更新公式，在新数据到来时回溯调整过去的预测。

Result: 在合成和真实数据集上的大量数值研究表明，该方法比现有在线共形预测方法实现了更快的覆盖校准和更高的统计效率。

Conclusion: 所提出的方法能更快适应分布变化，优于现有在线共形预测方法。

Abstract: Conformal prediction has emerged as a powerful framework for constructing
distribution-free prediction sets with guaranteed coverage assuming only the
exchangeability assumption. However, this assumption is often violated in
online environments where data distributions evolve over time. Several recent
approaches have been proposed to address this limitation, but, typically, they
slowly adapt to distribution shifts because they update predictions only in a
forward manner, that is, they generate a prediction for a newly observed data
point while previously computed predictions are not updated. In this paper, we
propose a novel online conformal inference method with retrospective
adjustment, which is designed to achieve faster adaptation to distributional
shifts. Our method leverages regression approaches with efficient leave-one-out
update formulas to retroactively adjust past predictions when new data arrive,
thereby aligning the entire set of predictions with the most recent data
distribution. Through extensive numerical studies performed on both synthetic
and real-world data sets, we show that the proposed approach achieves faster
coverage recalibration and improved statistical efficiency compared to existing
online conformal prediction methods.

</details>


### [169] [Robustness of Minimum-Volume Nonnegative Matrix Factorization under an Expanded Sufficiently Scattered Condition](https://arxiv.org/abs/2511.04291)
*Giovanni Barbarino,Nicolas Gillis,Subhayan Saha*

Main category: stat.ML

TL;DR: 本文证明了最小体积非负矩阵分解（min - vol NMF）在扩展充分分散条件下能在有噪声时识别真实因子。


<details>
  <summary>Details</summary>
Motivation: min - vol NMF 在许多应用中成功使用，但对噪声的鲁棒性是长期未解决的问题。

Method: 通过证明在扩展充分分散条件下的情况。

Result: 证明了 min - vol NMF 在扩展充分分散条件下，在有噪声时能识别真实因子。

Conclusion: min - vol NMF 在满足特定条件下对噪声有一定鲁棒性。

Abstract: Minimum-volume nonnegative matrix factorization (min-vol NMF) has been used
successfully in many applications, such as hyperspectral imaging, chemical
kinetics, spectroscopy, topic modeling, and audio source separation. However,
its robustness to noise has been a long-standing open problem. In this paper,
we prove that min-vol NMF identifies the groundtruth factors in the presence of
noise under a condition referred to as the expanded sufficiently scattered
condition which requires the data points to be sufficiently well scattered in
the latent simplex generated by the basis vectors.

</details>


### [170] [Simultaneous Optimization of Geodesics and Fréchet Means](https://arxiv.org/abs/2511.04301)
*Frederik Möbius Rygaard,Søren Hauberg,Steen Markvorsen*

Main category: stat.ML

TL;DR: 提出GEORCE - FM算法计算Fréchet均值，拓展到Finsler流形和自适应版本，理论证明收敛性，实证表明性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 计算Fréchet均值在多数流形上需在每次迭代中解决嵌入式优化问题，现有方法效率不足。

Method: 引入GEORCE - FM算法在局部图表中同时计算Fréchet均值和黎曼距离，将算法拓展到Finsler流形并引入自适应扩展。

Result: 理论上GEORCE - FM有全局收敛和局部二次收敛性，自适应扩展在期望上收敛到Fréchet均值；实证表明在准确性和运行时间上优于现有基线方法。

Conclusion: GEORCE - FM算法是计算Fréchet均值的有效方法，能在不同场景下高效准确地计算。

Abstract: A central part of geometric statistics is to compute the Fr\'echet mean. This
is a well-known intrinsic mean on a Riemannian manifold that minimizes the sum
of squared Riemannian distances from the mean point to all other data points.
The Fr\'echet mean is simple to define and generalizes the Euclidean mean, but
for most manifolds even minimizing the Riemannian distance involves solving an
optimization problem. Therefore, numerical computations of the Fr\'echet mean
require solving an embedded optimization problem in each iteration. We
introduce the GEORCE-FM algorithm to simultaneously compute the Fr\'echet mean
and Riemannian distances in each iteration in a local chart, making it faster
than previous methods. We extend the algorithm to Finsler manifolds and
introduce an adaptive extension such that GEORCE-FM scales to a large number of
data points. Theoretically, we show that GEORCE-FM has global convergence and
local quadratic convergence and prove that the adaptive extension converges in
expectation to the Fr\'echet mean. We further empirically demonstrate that
GEORCE-FM outperforms existing baseline methods to estimate the Fr\'echet mean
in terms of both accuracy and runtime.

</details>


### [171] [Riesz Regression As Direct Density Ratio Estimation](https://arxiv.org/abs/2511.04568)
*Masahiro Kato*

Main category: stat.ML

TL;DR: 研究表明Riesz回归在重要情形下与直接密度比估计（DRE）密切相关，二者可相互借鉴成果，还整合了先前研究。


<details>
  <summary>Details</summary>
Motivation: 探索Riesz回归与直接密度比估计之间的关系，以便在因果和结构参数估计等问题中更好地利用已有成果。

Method: 通过理论分析，揭示Riesz回归与直接密度比估计在平均处理效应估计等重要情形下的联系。

Result: 发现Riesz回归与直接密度比估计中的最小二乘重要性拟合（LSIF）思想和目标一致，可相互借鉴收敛率分析、损失函数选择和正则化技术等成果。

Conclusion: Riesz回归与DRE密切相关，二者可相互促进，本文整合了先前相关研究结果。

Abstract: Riesz regression has garnered attention as a tool in debiased machine
learning for causal and structural parameter estimation (Chernozhukov et al.,
2021). This study shows that Riesz regression is closely related to direct
density-ratio estimation (DRE) in important cases, including average treat-
ment effect (ATE) estimation. Specifically, the idea and objective in Riesz
regression coincide with the one in least-squares importance fitting (LSIF,
Kanamori et al., 2009) in direct density-ratio estimation. While Riesz
regression is general in the sense that it can be applied to Riesz representer
estimation in a wide class of problems, the equivalence with DRE allows us to
directly import exist- ing results in specific cases, including
convergence-rate analyses, the selection of loss functions via
Bregman-divergence minimization, and regularization techniques for flexible
models, such as neural networks. Conversely, insights about the Riesz
representer in debiased machine learning broaden the applications of direct
density-ratio estimation methods. This paper consolidates our prior results in
Kato (2025a) and Kato (2025b).

</details>


### [172] [Physics-Informed Neural Networks and Neural Operators for Parametric PDEs: A Human-AI Collaborative Analysis](https://arxiv.org/abs/2511.04576)
*Zhuo Zhang,Xiong Xiong,Sen Zhang,Yuan Zhao,Xi Yang*

Main category: stat.ML

TL;DR: 文章分析了参数偏微分方程（PDE）求解的两种机器学习范式，对比显示神经算子在多查询场景下比传统求解器快10^3到10^5倍，还给出方法选择建议、讨论理论基础并指出开放挑战。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法求解参数PDE需为每个参数重新求解，使参数空间探索成本过高，机器学习方法为其带来革新，因此对两种主要范式进行分析。

Method: 对PINNs和神经算子两种范式进行对比分析，对比范围涵盖流体动力学、固体力学、传热和电磁学等领域。

Result: 神经算子在多查询场景下比传统求解器计算速度快10^3到10^5倍，且精度相当。

Conclusion: 建立了通过算子学习理解参数PDE求解器的统一框架，为该快速发展领域提供综合、逐步更新的资源，同时指出高维参数、复杂几何和分布外泛化等开放挑战。

Abstract: PDEs arise ubiquitously in science and engineering, where solutions depend on
parameters (physical properties, boundary conditions, geometry). Traditional
numerical methods require re-solving the PDE for each parameter, making
parameter space exploration prohibitively expensive. Recent machine learning
advances, particularly physics-informed neural networks (PINNs) and neural
operators, have revolutionized parametric PDE solving by learning solution
operators that generalize across parameter spaces. We critically analyze two
main paradigms: (1) PINNs, which embed physical laws as soft constraints and
excel at inverse problems with sparse data, and (2) neural operators (e.g.,
DeepONet, Fourier Neural Operator), which learn mappings between
infinite-dimensional function spaces and achieve unprecedented generalization.
Through comparisons across fluid dynamics, solid mechanics, heat transfer, and
electromagnetics, we show neural operators can achieve computational speedups
of $10^3$ to $10^5$ times faster than traditional solvers for multi-query
scenarios, while maintaining comparable accuracy. We provide practical guidance
for method selection, discuss theoretical foundations (universal approximation,
convergence), and identify critical open challenges: high-dimensional
parameters, complex geometries, and out-of-distribution generalization. This
work establishes a unified framework for understanding parametric PDE solvers
via operator learning, offering a comprehensive, incrementally updated resource
for this rapidly evolving field

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [173] [InvSim algorithm for pre-computing airplane flight controls in limited-range autonomous missions, and demonstration via double-roll maneuver of Mirage III fighters](https://arxiv.org/abs/2511.03745)
*Osama A. Marzouk*

Main category: eess.SY

TL;DR: 本文构建飞行力学6 - DOF运动方程通用框架，推导逆仿真适用版本，给出数值积分程序并演示该逆仿真方法。


<details>
  <summary>Details</summary>
Motivation: 为飞行力学逆仿真提供适用的运动方程和计算方法，以根据目标轨迹预测所需飞行控制。

Method: 构建含多种坐标系和角度的6 - DOF运动方程框架，推导逆仿真适用版本，用符号数学、RK4数值积分、有限差分法进行数值积分。

Result: 计算出四个必要控制变量的离散值，使飞机能按指定轨迹飞行。

Conclusion: 提出的飞行力学逆仿真数值程序可行。

Abstract: In this work, we start with a generic mathematical framework for the
equations of motion (EOM) in flight mechanics with six degrees of freedom
(6-DOF) for a general (not necessarily symmetric) fixed-wing aircraft. This
mathematical framework incorporates (1) body axes (fixed in the airplane at its
center of gravity), (2) inertial axes (fixed in the earth/ground at the
take-off point), wind axes (aligned with the flight path/course), (3) spherical
flight path angles (azimuth angle measured clockwise from the geographic north,
and elevation angle measured above the horizon plane), and (4) spherical flight
angles (angle of attack and sideslip angle). We then manipulate these equations
of motion to derive a customized version suitable for inverse simulation flight
mechanics, where a target flight trajectory is specified while a set of
corresponding necessary flight controls to achieve that maneuver are predicted.
We then present a numerical procedure for integrating the developed inverse
simulation (InvSim) system in time; utilizing (1) symbolic mathematics, (2)
explicit fourth-order Runge-Kutta (RK4) numerical integration technique, and
(3) expressions based on the finite difference method (FDM); such that the four
necessary control variables (engine thrust force, ailerons' deflection angle,
elevators' deflection angle, and rudder's deflection angle) are computed as
discrete values over the entire maneuver time, and these calculated control
values enable the airplane to achieve the desired flight trajectory, which is
specified by three inertial Cartesian coordinates of the airplane, in addition
to the Euler's roll angle. We finally demonstrate the proposed numerical
procedure of flight mechanics inverse simulation (InvSim).

</details>


### [174] [Data-driven uncertainty-aware seakeeping prediction of the Delft 372 catamaran using ensemble Hankel dynamic mode decomposition](https://arxiv.org/abs/2511.04461)
*Giorgio Palma,Andrea Serani,Matteo Diez*

Main category: eess.SY

TL;DR: 提出并验证基于集成的Hankel动态模式分解控制法(HDMDc)用于高速双体船耐波性预测，对比两种集成策略，FHDMDc效果更好。


<details>
  <summary>Details</summary>
Motivation: 实现对高速双体船耐波性的不确定性感知预测。

Method: 采集实验数据，用HDMDc算法构建降阶模型，对比贝叶斯HDMDc (BHDMDc)和频率论HDMDc (FHDMDc)两种集成策略。

Result: FHDMDc比确定性模型预测更准确，能提供可靠的不确定性估计；BHDMDc在本测试用例中不如确定性模型。FHDMDc得出的运动概率密度函数与实验数据和URANS结果相符。

Conclusion: FHDMDc可用于设计和运营支持，实现可靠且计算高效的耐波性预测。

Abstract: In this study, we present and validate an ensemble-based Hankel Dynamic Mode
Decomposition with control (HDMDc) for uncertainty-aware seakeeping predictions
of a high-speed catamaran, namely the Delft 372 model. Experimental
measurements (time histories) of wave elevation at the longitudinal center of
gravity, heave, pitch, notional flight-deck velocity, notional bridge
acceleration, and total resistance were collected from irregular wave basin
tests on a 1:33.3 scale replica of the Delft 372 model under sea state 5
conditions at Fr = 0.425, and organized into training, validation, and test
sets. The HDMDc algorithm constructs an equation-free linear reduced-order
model of the seakeeping vessel by augmenting states and inputs with their
time-lagged copies to capture nonlinear and memory effects. Two ensembling
strategies, namely Bayesian HDMDc (BHDMDc), which samples hyperparameters
considered stochastic variables with prior distribution to produce posterior
mean forecasts with confidence intervals, and Frequentist HDMDc (FHDMDc), which
aggregates multiple model obtained over data subsets, are compared in providing
seakeeping prediction and uncertainty quantification. The FHDMDc approach is
found to improve the accuracy of the predictions compared to the deterministic
counterpart, also providing robust uncertainty estimation; whereas the
application of BHDMDc to the present test case is not found beneficial in
comparison to the deterministic model. FHDMDc-derived probability density
functions for the motions closely match both experimental data and URANS
results, demonstrating reliable and computationally efficient seakeeping
prediction for design and operational support.

</details>


### [175] [A Model-Based Approach to Automated Digital Twin Generation in Manufacturing](https://arxiv.org/abs/2511.03742)
*Angelos Alexopoulos,Agorakis Bompotas,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Athanasios P. Kalogeras,Christos Alexakos*

Main category: eess.SY

TL;DR: 本文提出基于AutomationML工厂计划的自动化数字孪生生成与部署平台，结合GAI模拟场景生成和物理生产线自动重配置，提升制造效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 现代制造需高灵活性和可重构性，传统MBE最终重配置需模拟和验证，数字孪生可简化流程。

Method: 提出基于AutomationML工厂计划的平台，结合GAI模拟场景生成器和自动物理生产线重配置。

Result: 构建了能自动化生成和部署数字孪生的平台。

Conclusion: 该平台可提高制造效率和适应性。

Abstract: Modern manufacturing demands high flexibility and reconfigurability to adapt
to dynamic production needs. Model-based Engineering (MBE) supports rapid
production line design, but final reconfiguration requires simulations and
validation. Digital Twins (DTs) streamline this process by enabling real-time
monitoring, simulation, and reconfiguration. This paper presents a novel
platform that automates DT generation and deployment using AutomationML-based
factory plans. The platform closes the loop with a GAI-powered simulation
scenario generator and automatic physical line reconfiguration, enhancing
efficiency and adaptability in manufacturing.

</details>


### [176] [Overview and Performance Evaluation of Supervisory Controller Synthesis with Eclipse ESCET v4.0](https://arxiv.org/abs/2511.04370)
*Dennis Hendriks,Michel Reniers,Wan Fokkink,Wytse Oortwijn*

Main category: eess.SY

TL;DR: 本文围绕Eclipse Supervisory Control Engineering Toolkit (ESCET)项目中的CIF展开，介绍其符号监督控制器综合算法、基准模型，评估版本改进对综合性能的影响，还探讨多级综合方法。


<details>
  <summary>Details</summary>
Motivation: 支持SBE过程，完善CIF在监督控制器设计与实现方面的功能，提升综合性能。

Method: 描述CIF的符号监督控制器综合算法，引入基准模型，评估ESCET不同版本改进对综合性能的影响，研究多级综合方法。

Result: 展示了CIF当前实际综合性能，多级综合可改善性能但复杂模型仍需进一步提升。

Conclusion: CIF在监督控制器综合方面有进展，但对于复杂模型的综合性能仍需进一步改进。

Abstract: Supervisory controllers control cyber-physical systems to ensure their
correct and safe operation. Synthesis-based engineering (SBE) is an approach to
largely automate their design and implementation. SBE combines model-based
engineering with computer-aided design, allowing engineers to focus on 'what'
the system should do (the requirements) rather than 'how' it should do it
(design and implementation). In the Eclipse Supervisory Control Engineering
Toolkit (ESCET) open-source project, a community of users, researchers, and
tool vendors jointly develop a toolkit to support the entire SBE process,
particularly through the CIF modeling language and tools. In this paper, we
first provide a description of CIF's symbolic supervisory controller synthesis
algorithm, and thereby include aspects that are often omitted in the
literature, but are of great practical relevance, such as the prevention of
runtime errors, handling different types of requirements, and supporting input
variables (to connect to external inputs). Secondly, we introduce and describe
CIF's benchmark models, a collection of 23 freely available industrial and
academic models of various sizes and complexities. Thirdly, we describe recent
improvements between ESCET versions v0.8 (December 2022) and v4.0 (June 2024)
that affect synthesis performance, evaluate them on our benchmark models, and
show the current practical synthesis performance of CIF. Fourthly, we briefly
look at multi-level synthesis, a non-monolithic synthesis approach, evaluate
its gains, and show that while it can help to further improve synthesis
performance, further performance improvements are still needed to synthesize
complex models.

</details>


### [177] [A convolutional neural network deep learning method for model class selection](https://arxiv.org/abs/2511.03743)
*Marios Impraimakis*

Main category: eess.SY

TL;DR: 研究新深度卷积神经网络方法仅用响应进行模型类选择的能力，还考察了基于物理算法增强，该方法可用于结构健康监测。


<details>
  <summary>Details</summary>
Motivation: 探索一种简单有效的仅用响应进行模型类选择的方法，为结构健康监测提供工具。

Method: 用单自由度响应及其类别信息训练和验证一维卷积神经网络，还考察用卡尔曼滤波器融合系统响应信号的算法增强。

Result: 该方法能在线性和非线性动力系统及3D建筑有限元模型中，对因阻尼或滞回行为产生的轻微信号变化进行模型类选择。

Conclusion: 此方法为结构健康监测应用提供了强大工具。

Abstract: The response-only model class selection capability of a novel deep
convolutional neural network method is examined herein in a simple, yet
effective, manner. Specifically, the responses from a unique degree of freedom
along with their class information train and validate a one-dimensional
convolutional neural network. In doing so, the network selects the model class
of new and unlabeled signals without the need of the system input information,
or full system identification. An optional physics-based algorithm enhancement
is also examined using the Kalman filter to fuse the system response signals
using the kinematics constraints of the acceleration and displacement data.
Importantly, the method is shown to select the model class in slight signal
variations attributed to the damping behavior or hysteresis behavior on both
linear and nonlinear dynamic systems, as well as on a 3D building finite
element model, providing a powerful tool for structural health monitoring
applications.

</details>


### [178] [A Dynamic Recurrent Adjacency Memory Network for Mixed-Generation Power System Stability Forecasting](https://arxiv.org/abs/2511.03746)
*Guang An Ooi,Otavio Bertozzi,Mohd Asim Aftab,Charalambos Konstantinou,Shehab Ahmed*

Main category: eess.SY

TL;DR: 本文提出动态循环邻接记忆网络（DRAMN）用于实时电力系统稳定性预测，经多网络验证性能优越，可降维且具可解释性，适合实时部署。


<details>
  <summary>Details</summary>
Motivation: 现代含高比例逆变器资源的电力系统动态行为复杂，传统稳定性评估方法可扩展性和泛化性不足。

Method: 结合物理信息分析与深度学习，用滑动窗口动态模式分解构建时变多层邻接矩阵，将图卷积操作集成到循环门控机制中。

Result: 在多个网络验证中平均准确率高，超所有测试基准，可降低特征维度82%且性能不下降，验证了不同稳定性现象的泛化性。

Conclusion: DRAMN达到了先进的准确性，为电力系统运营商提供了更强的可解释性，适合在现代控制中心实时部署。

Abstract: Modern power systems with high penetration of inverter-based resources
exhibit complex dynamic behaviors that challenge the scalability and
generalizability of traditional stability assessment methods. This paper
presents a dynamic recurrent adjacency memory network (DRAMN) that combines
physics-informed analysis with deep learning for real-time power system
stability forecasting. The framework employs sliding-window dynamic mode
decomposition to construct time-varying, multi-layer adjacency matrices from
phasor measurement unit and sensor data to capture system dynamics such as
modal participation factors, coupling strengths, phase relationships, and
spectral energy distributions. As opposed to processing spatial and temporal
dependencies separately, DRAMN integrates graph convolution operations directly
within recurrent gating mechanisms, enabling simultaneous modeling of evolving
dynamics and temporal dependencies. Extensive validations on modified IEEE
9-bus, 39-bus, and a multi-terminal HVDC network demonstrate high performance,
achieving 99.85\%, 99.90\%, and 99.69\% average accuracies, respectively,
surpassing all tested benchmarks, including classical machine learning
algorithms and recent graph-based models. The framework identifies optimal
combinations of measurements that reduce feature dimensionality by 82\% without
performance degradation. Correlation analysis between dominant measurements for
small-signal and transient stability events validates generalizability across
different stability phenomena. DRAMN achieves state-of-the-art accuracy while
providing enhanced interpretability for power system operators, making it
suitable for real-time deployment in modern control centers.

</details>


### [179] [Deep Koopman Economic Model Predictive Control of a Pasteurisation Unit](https://arxiv.org/abs/2511.04437)
*Patrik Valábek,Michaela Horváthová,Martin Klaučo*

Main category: eess.SY

TL;DR: 本文提出基于深度Koopman的经济模型预测控制（EMPC）用于巴氏杀菌单元高效运行，该方法比传统方法有更好的预测精度和更低的经济成本。


<details>
  <summary>Details</summary>
Motivation: 实现实验室规模巴氏杀菌单元（PU）的高效运行。

Method: 利用Koopman算子理论将复杂非线性系统动力学转化为线性表示，用神经网络从实验数据中学习线性动力学，将两种模型用于含经济成本的EMPC公式中，用松弛变量确保可行性。

Result: 深度Koopman模型开环预测精度比传统N4SID提高45%；深度Koopman EMPC比N4SID基线总经济成本降低32%；基于Koopman的EMPC稳态运行所需电能减少10.2%。

Conclusion: 将深度Koopman表示与经济优化相结合对热密集型工厂资源高效控制有实际优势。

Abstract: This paper presents a deep Koopman-based Economic Model Predictive Control
(EMPC) for efficient operation of a laboratory-scale pasteurization unit (PU).
The method uses Koopman operator theory to transform the complex, nonlinear
system dynamics into a linear representation, enabling the application of
convex optimization while representing the complex PU accurately. The deep
Koopman model utilizes neural networks to learn the linear dynamics from
experimental data, achieving a 45% improvement in open-loop prediction accuracy
over conventional N4SID subspace identification. Both analyzed models were
employed in the EMPC formulation that includes interpretable economic costs,
such as energy consumption, material losses due to inadequate pasteurization,
and actuator wear. The feasibility of EMPC is ensured using slack variables.
The deep Koopman EMPC and N4SID EMPC are numerically validated on a nonlinear
model of multivariable PU under external disturbance. The disturbances include
feed pump fail-to-close scenario and the introduction of a cold batch to be
pastuerized. These results demonstrate that the deep Koopmand EMPC achieves a
32% reduction in total economic cost compared to the N4SID baseline. This
improvement is mainly due to the reductions in material losses and energy
consumption. Furthermore, the steady-state operation via Koopman-based EMPC
requires 10.2% less electrical energy. The results highlight the practical
advantages of integrating deep Koopman representations with economic
optimization to achieve resource-efficient control of thermal-intensive plants.

</details>


### [180] [Deep Dictionary-Free Method for Identifying Linear Model of Nonlinear System with Input Delay](https://arxiv.org/abs/2511.04451)
*Patrik Valábek,Marek Wadinger,Michal Kvasnica,Martin Klaučo*

Main category: eess.SY

TL;DR: 提出LSTM增强的深度Koopman模型近似Koopman算子，处理含输入延迟的非线性动力系统，在预测精度上有提升。


<details>
  <summary>Details</summary>
Motivation: 含输入延迟的非线性动力系统因复杂和延迟影响，传统线性控制技术失效，需创新方法。

Method: 引入LSTM增强的深度Koopman模型近似Koopman算子，通过LSTM层捕捉历史依赖并将时滞系统动态编码到潜空间，且该模型无字典。

Result: 在模拟系统上与扩展eDMD定量比较，在真实非线性动态未知时预测精度显著提升，在已知系统动态时与eDMD结果相当。

Conclusion: LSTM增强的深度Koopman模型能有效处理含输入延迟的非线性动力系统，在预测方面表现良好。

Abstract: Nonlinear dynamical systems with input delays pose significant challenges for
prediction, estimation, and control due to their inherent complexity and the
impact of delays on system behavior. Traditional linear control techniques
often fail in these contexts, necessitating innovative approaches. This paper
introduces a novel approach to approximate the Koopman operator using an
LSTM-enhanced Deep Koopman model, enabling linear representations of nonlinear
systems with time delays. By incorporating Long Short-Term Memory (LSTM)
layers, the proposed framework captures historical dependencies and efficiently
encodes time-delayed system dynamics into a latent space. Unlike traditional
extended Dynamic Mode Decomposition (eDMD) approaches that rely on predefined
dictionaries, the LSTM-enhanced Deep Koopman model is dictionary-free, which
mitigates the problems with the underlying dynamics being known and
incorporated into the dictionary. Quantitative comparisons with extended eDMD
on a simulated system demonstrate highly significant performance gains in
prediction accuracy in cases where the true nonlinear dynamics are unknown and
achieve comparable results to eDMD with known dynamics of a system.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [181] [Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition](https://arxiv.org/abs/2511.03891)
*Hlali Azzeddine,Majid Ben Yakhlef,Soulaiman El Hazzat*

Main category: cs.CV

TL;DR: 本文提出基于类别的图像合成方法，在OCTDL数据集上构建Co - OCTDL数据集，用VGG16模型对比实验，结果显示该方法显著提升诊断效果，能处理不平衡小数据集。


<details>
  <summary>Details</summary>
Motivation: 小的不平衡数据集和图像质量差会导致深度学习模型误判率高，需要方法改进。

Method: 提出基于类别的图像合成方法，将同一类别的多个图像融合为组合输入图像，构建平衡数据集Co - OCTDL，用VGG16模型对比实验。

Result: 增强后的数据集准确率达99.6%，F1分数0.995，AUC为0.9996，误判率显著降低。

Conclusion: 该方法能为受类别不平衡或小样本影响的弱数据集产生高质量预测。

Abstract: Small, imbalanced datasets and poor input image quality can lead to high
false predictions rates with deep learning models. This paper introduces
Class-Based Image Composition, an approach that allows us to reformulate
training inputs through a fusion of multiple images of the same class into
combined visual composites, named Composite Input Images (CoImg). That enhances
the intra-class variance and improves the valuable information density per
training sample and increases the ability of the model to distinguish between
subtle disease patterns. Our method was evaluated on the Optical Coherence
Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et
al., 2024), which contains 2,064 high-resolution optical coherence tomography
(OCT) scans of the human retina, representing seven distinct diseases with a
significant class imbalance. We constructed a perfectly class-balanced version
of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout
composite image. To assess the effectiveness of this new representation, we
conducted a comparative analysis between the original dataset and its variant
using a VGG16 model. A fair comparison was ensured by utilizing the identical
model architecture and hyperparameters for all experiments. The proposed
approach markedly improved diagnostic results.The enhanced Dataset achieved
near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared
to a baseline model trained on raw dataset. The false prediction rate was also
significantly lower, this demonstrates that the method can producehigh-quality
predictions even for weak datasets affected by class imbalance or small sample
size.

</details>


### [182] [Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets](https://arxiv.org/abs/2511.03855)
*Duong Mai,Lawrence Hall*

Main category: cs.CV

TL;DR: 研究用噪声注入技术提升COVID - 19胸部X光图像识别模型对分布偏移的鲁棒性，显著缩小了分布内和分布外数据评估的性能差距。


<details>
  <summary>Details</summary>
Motivation: 深度学习图像识别模型在COVID - 19胸部X光检测中无法很好地泛化到分布外数据，原因是模型学习利用捷径而非合理生物标志物。

Method: 在训练期间使用基本噪声注入技术（高斯、斑点、泊松和椒盐噪声）。

Result: 该技术可将分布内和分布外评估的性能差距从0.10 - 0.20显著缩小到0.01 - 0.06，基于AUC、F1、准确率等关键指标的十次随机种子平均结果。

Conclusion: 噪声注入技术能有效提升模型对分布偏移的鲁棒性，代码已公开。

Abstract: Deep learned (DL) models for image recognition have been shown to fail to
generalize to data from different devices, populations, etc. COVID-19 detection
from Chest X-rays (CXRs), in particular, has been shown to fail to generalize
to out-of-distribution (OOD) data from new clinical sources not covered in the
training set. This occurs because models learn to exploit shortcuts -
source-specific artifacts that do not translate to new distributions - rather
than reasonable biomarkers to maximize performance on in-distribution (ID)
data. Rendering the models more robust to distribution shifts, our study
investigates the use of fundamental noise injection techniques (Gaussian,
Speckle, Poisson, and Salt and Pepper) during training. Our empirical results
demonstrate that this technique can significantly reduce the performance gap
between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results
averaged over ten random seeds across key metrics such as AUC, F1, accuracy,
recall and specificity. Our source code is publicly available at
https://github.com/Duongmai127/Noisy-ood

</details>


### [183] [Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures](https://arxiv.org/abs/2511.03882)
*Florence Klitzner,Blanca Inigo,Benjamin D. Killeen,Lalithkumar Seenivasan,Michelle Song,Axel Krieger,Mathias Unberath*

Main category: cs.CV

TL;DR: 研究双平面引导套管插入中模仿策略学习的机会与挑战，开发仿真沙盒，训练模仿学习策略，政策有一定成功率和泛化性，但存在局限性。


<details>
  <summary>Details</summary>
Motivation: 不确定基于模仿学习的机器人控制策略是否适用于X射线引导的脊柱手术，因多视图X射线解读复杂。

Method: 开发用于X射线引导脊柱手术的计算机模拟沙盒，整理正确轨迹和双平面X射线序列数据集，训练模仿学习策略进行规划和开环控制。

Result: 策略首次成功率68.5%，能保持安全轨迹，可泛化到复杂解剖结构，在真实X射线中能产生合理轨迹。

Conclusion: 结果有前景，但存在局限性，全闭环控制需更多反馈考虑，有潜力为无CT的术中脊柱导航奠定基础。

Abstract: Imitation learning-based robot control policies are enjoying renewed interest
in video-based robotics. However, it remains unclear whether this approach
applies to X-ray-guided procedures, such as spine instrumentation. This is
because interpretation of multi-view X-rays is complex. We examine
opportunities and challenges for imitation policy learning in bi-plane-guided
cannula insertion. We develop an in silico sandbox for scalable, automated
simulation of X-ray-guided spine procedures with a high degree of realism. We
curate a dataset of correct trajectories and corresponding bi-planar X-ray
sequences that emulate the stepwise alignment of providers. We then train
imitation learning policies for planning and open-loop control that iteratively
align a cannula solely based on visual information. This precisely controlled
setup offers insights into limitations and capabilities of this method. Our
policy succeeded on the first attempt in 68.5% of cases, maintaining safe
intra-pedicular trajectories across diverse vertebral levels. The policy
generalized to complex anatomy, including fractures, and remained robust to
varied initializations. Rollouts on real bi-planar X-rays further suggest that
the model can produce plausible trajectories, despite training exclusively in
simulation. While these preliminary results are promising, we also identify
limitations, especially in entry point precision. Full closed-look control will
require additional considerations around how to provide sufficiently frequent
feedback. With more robust priors and domain knowledge, such models may provide
a foundation for future efforts toward lightweight and CT-free robotic
intra-operative spinal navigation.

</details>


### [184] [I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging](https://arxiv.org/abs/2511.03912)
*Nand Kumar Yadav,Rodrigue Rizk,William CW Chen,KC Santosh*

Main category: cs.CV

TL;DR: 提出无监督、无预言机框架用于医学影像未知异常检测，交替进行轻量级适配器更新和不确定性门控样本准入，实验效果显著。


<details>
  <summary>Details</summary>
Motivation: 医学影像未知异常检测面临标注异常样本稀缺和专家监督成本高的挑战。

Method: 从少量正常图像种子开始，交替进行轻量级适配器更新和不确定性门控样本准入，用小卷积适配器增强预训练视觉骨干，将嵌入存储在紧凑核心集中进行k - NN异常评分，用双概率门确保增量扩展安全。

Result: 在COVID - CXR、Pneumonia CXR和Brain MRI ND - 5等数据集上，ROC - AUC等指标有显著提升。

Conclusion: 所提框架对现实世界、标签稀缺的医学影像应用有效且高效。

Abstract: Unknown anomaly detection in medical imaging remains a fundamental challenge
due to the scarcity of labeled anomalies and the high cost of expert
supervision. We introduce an unsupervised, oracle-free framework that
incrementally expands a trusted set of normal samples without any anomaly
labels. Starting from a small, verified seed of normal images, our method
alternates between lightweight adapter updates and uncertainty-gated sample
admission. A frozen pretrained vision backbone is augmented with tiny
convolutional adapters, ensuring rapid domain adaptation with negligible
computational overhead. Extracted embeddings are stored in a compact coreset
enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during
incremental expansion is enforced by dual probabilistic gates, a sample is
admitted into the normal memory only if its distance to the existing coreset
lies within a calibrated z-score threshold, and its SWAG-based epistemic
uncertainty remains below a seed-calibrated bound. This mechanism prevents
drift and false inclusions without relying on generative reconstruction or
replay buffers. Empirically, our system steadily refines the notion of
normality as unlabeled data arrive, producing substantial gains over baselines.
On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on
Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,
ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These
results highlight the effectiveness and efficiency of the proposed framework
for real-world, label-scarce medical imaging applications.

</details>


### [185] [Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization](https://arxiv.org/abs/2511.03950)
*Zhejia Cai,Puhua Jiang,Shiwei Mao,Hongkun Cao,Ruqi Huang*

Main category: cs.CV

TL;DR: 本文提出统一优化几何与外观的框架，实现高斯 - 网格联合优化用于多视图图像3D重建，成果可用于下游编辑任务。


<details>
  <summary>Details</summary>
Motivation: 现有多视图图像3D重建方法常分离几何与外观优化，阻碍下游编辑任务，需要统一处理。

Method: 提出新框架，通过高斯引导的网格可微渲染同时优化网格几何（顶点位置和面）和顶点颜色，利用输入图像的光度一致性和法线、深度图的几何正则化。

Result: 获得高质量3D重建结果。

Conclusion: 该统一优化框架能实现高斯 - 网格联合优化，成果可用于下游编辑任务，代码接收后公开。

Abstract: Reconstructing real-world objects from multi-view images is essential for
applications in 3D editing, AR/VR, and digital content creation. Existing
methods typically prioritize either geometric accuracy (Multi-View Stereo) or
photorealistic rendering (Novel View Synthesis), often decoupling geometry and
appearance optimization, which hinders downstream editing tasks. This paper
advocates an unified treatment on geometry and appearance optimization for
seamless Gaussian-mesh joint optimization. More specifically, we propose a
novel framework that simultaneously optimizes mesh geometry (vertex positions
and faces) and vertex colors via Gaussian-guided mesh differentiable rendering,
leveraging photometric consistency from input images and geometric
regularization from normal and depth maps. The obtained high-quality 3D
reconstruction can be further exploit in down-stream editing tasks, such as
relighting and shape deformation. The code will be publicly available upon
acceptance.

</details>


### [186] [Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)](https://arxiv.org/abs/2511.04126)
*Venkata Manikanta Desu,Syed Fawaz Ali*

Main category: cs.CV

TL;DR: 提出自动化网球比赛分析完整流程，集成多深度学习模型，实验表现好，能输出带注释视频和指标。


<details>
  <summary>Details</summary>
Motivation: 构建自动化网球比赛分析系统，为教练、转播商和球员提供比赛动态的可操作见解。

Method: 集成多个深度学习模型，用YOLOv8检测球员、自定义YOLOv5模型跟踪球、基于ResNet50架构检测球场关键点。

Result: 系统在不同球场条件和比赛场景中表现稳健，能输出带注释视频和详细性能指标。

Conclusion: 该系统能有效为相关人员提供比赛动态的可操作见解。

Abstract: This study presents a complete pipeline for automated tennis match analysis.
Our framework integrates multiple deep learning models to detect and track
players and the tennis ball in real time, while also identifying court
keypoints for spatial reference. Using YOLOv8 for player detection, a
custom-trained YOLOv5 model for ball tracking, and a ResNet50-based
architecture for court keypoint detection, our system provides detailed
analytics including player movement patterns, ball speed, shot accuracy, and
player reaction times. The experimental results demonstrate robust performance
in varying court conditions and match scenarios. The model outputs an annotated
video along with detailed performance metrics, enabling coaches, broadcasters,
and players to gain actionable insights into the dynamics of the game.

</details>


### [187] [DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms](https://arxiv.org/abs/2511.04128)
*Shengyu Tang,Zeyuan Lu,Jiazhi Dong,Changdong Yu,Xiaoyu Wang,Yaohui Lyu,Weihao Xia*

Main category: cs.CV

TL;DR: 提出高效的DMSORT方法用于海上多目标跟踪，在新加坡海事数据集上表现优异，运行速度快。


<details>
  <summary>Details</summary>
Motivation: 复杂海洋环境导致相机运动和视觉退化，给海上多目标跟踪带来挑战。

Method: 提出DMSORT方法，包含并行跟踪器与仿射补偿，集成RCDN用于目标检测，设计Li - TAE提取外观特征，另一分支解耦运动，还有聚类优化的特征融合模块。

Result: 在新加坡海事数据集上实现了最先进的性能，在基于ReID的多目标跟踪框架中运行速度最快，保持高身份一致性和对抖动与遮挡的鲁棒性。

Conclusion: DMSORT方法有效解决了复杂海洋环境下的多目标跟踪问题，具有良好性能。

Abstract: Accurate perception of the marine environment through robust multi-object
tracking (MOT) is essential for ensuring safe vessel navigation and effective
maritime surveillance. However, the complicated maritime environment often
causes camera motion and subsequent visual degradation, posing significant
challenges to MOT. To address this challenge, we propose an efficient
Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the
framework is a parallel tracker with affine compensation, which incorporates an
object detection and re-identification (ReID) branch, along with a dedicated
branch for dynamic camera motion estimation. Specifically, a Reversible
Columnar Detection Network (RCDN) is integrated into the detection module to
leverage multi-level visual features for robust object detection. Furthermore,
a lightweight Transformer-based appearance extractor (Li-TAE) is designed to
capture global contextual information and generate robust appearance features.
Another branch decouples platform-induced and target-intrinsic motion by
constructing a projective transformation, applying platform-motion compensation
within the Kalman filter, and thereby stabilizing true object trajectories.
Finally, a clustering-optimized feature fusion module effectively combines
motion and appearance cues to ensure identity consistency under noise,
occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset
demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT
attains the fastest runtime among existing ReID-based MOT frameworks while
maintaining high identity consistency and robustness to jitter and occlusion.
Code is available at:
https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-.

</details>


### [188] [Learning from Online Videos at Inference Time for Computer-Use Agents](https://arxiv.org/abs/2511.04137)
*Yujian Liu,Ze Wang,Hao Chen,Ximeng Sun,Xiaodong Yu,Jialian Wu,Jiang Liu,Emad Barsoum,Zicheng Liu,Shiyu Chang*

Main category: cs.CV

TL;DR: 本文提出框架使计算机使用代理能在推理时从在线视频中有效学习，实验表明该框架优于其他对比方法。


<details>
  <summary>Details</summary>
Motivation: 当前计算机使用代理在需要特定领域程序知识的任务上落后于人类，人类可通过观看视频教程弥补差距，因此研究让代理从在线视频学习的方法。

Method: 提出框架，包括检索和过滤教程视频、转换为结构化演示轨迹、在执行时动态选择轨迹作为上下文指导；使用VLM推断UI动作、分割视频、分配文本目标；采用两阶段选择机制动态选择轨迹。

Result: 在两个广泛使用的基准测试中，框架始终优于强大的基础代理以及仅使用文本教程或文字记录的变体。

Conclusion: 丰富的在线视频可系统提炼为可操作的指导，能在推理时提升计算机使用代理的性能。

Abstract: Computer-use agents can operate computers and automate laborious tasks, but
despite recent rapid progress, they still lag behind human users, especially
when tasks require domain-specific procedural knowledge about particular
applications, platforms, and multi-step workflows. Humans can bridge this gap
by watching video tutorials: we search, skim, and selectively imitate short
segments that match our current subgoal. In this paper, we study how to enable
computer-use agents to learn from online videos at inference time effectively.
We propose a framework that retrieves and filters tutorial videos, converts
them into structured demonstration trajectories, and dynamically selects
trajectories as in-context guidance during execution. Particularly, using a
VLM, we infer UI actions, segment videos into short subsequences of actions,
and assign each subsequence a textual objective. At inference time, a two-stage
selection mechanism dynamically chooses a single trajectory to add in context
at each step, focusing the agent on the most helpful local guidance for its
next decision. Experiments on two widely used benchmarks show that our
framework consistently outperforms strong base agents and variants that use
only textual tutorials or transcripts. Analyses highlight the importance of
trajectory segmentation and selection, action filtering, and visual
information, suggesting that abundant online videos can be systematically
distilled into actionable guidance that improves computer-use agents at
inference time. Our code is available at
https://github.com/UCSB-NLP-Chang/video_demo.

</details>


### [189] [Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology](https://arxiv.org/abs/2511.04171)
*Fatemehzahra Darzi,Rodrigo Escobar Diaz Guerrero,Thomas Bocklitz*

Main category: cs.CV

TL;DR: 研究不同颜色转换技术对苏木精和伊红染色图像与非线性多模态图像配准的影响，发现CycleGAN颜色转换配准误差最低，颜色转换可提升配准效果。


<details>
  <summary>Details</summary>
Motivation: 准确配准不同模态图像是数字病理学的关键步骤，研究不同颜色转换技术对图像配准的影响。

Method: 使用20组组织样本对，进行多种预处理，包括不同颜色转换等，用VALIS方法配准，用相对目标配准误差评估，还进行自定义点评估，分原始和反转多模态图像两种场景。

Result: 两种场景下CycleGAN颜色转换配准误差最低，其他方法误差较高。

Conclusion: 配准前应用颜色转换可改善不同模态图像的对齐，支持数字病理学更可靠分析。

Abstract: Image registration refers to the process of spatially aligning two or more
images by mapping them into a common coordinate system, so that corresponding
anatomical or tissue structures are matched across images. In digital
pathology, registration enables direct comparison and integration of
information from different stains or imaging modalities, sup-porting
applications such as biomarker analysis and tissue reconstruction. Accurate
registration of images from different modalities is an essential step in
digital pathology. In this study, we investigated how various color
transformation techniques affect image registration between hematoxylin and
eosin (H&E) stained images and non-linear multimodal images. We used a dataset
of 20 tissue sample pairs, with each pair undergoing several preprocessing
steps, including different color transformation (CycleGAN, Macenko, Reinhard,
Vahadane), inversion, contrast adjustment, intensity normalization, and
denoising. All images were registered using the VALIS registration method,
which first applies rigid registration and then performs non-rigid registration
in two steps on both low and high-resolution images. Registration performance
was evaluated using the relative Target Registration Error (rTRE). We reported
the median of median rTRE values (MMrTRE) and the average of median rTRE values
(AMrTRE) for each method. In addition, we performed a custom point-based
evaluation using ten manually selected key points. Registration was done
separately for two scenarios, using either the original or inverted multimodal
images. In both scenarios, CycleGAN color transformation achieved the lowest
registration errors, while the other methods showed higher errors. These
findings show that applying color transformation before registration improves
alignment between images from different modalities and supports more reliable
analysis in digital pathology.

</details>


### [190] [AStF: Motion Style Transfer via Adaptive Statistics Fusor](https://arxiv.org/abs/2511.04192)
*Hanmo Chen,Chenghao Xu,Jiexi Yan,Cheng Deng*

Main category: cs.CV

TL;DR: 本文提出一种自适应统计融合器（AStF）用于人体运动风格迁移，结合运动一致性正则化判别器训练，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统依赖均值和方差的运动风格迁移方法，因图像和运动本质差异，无法充分捕捉运动数据复杂动态模式和时空连贯性。

Method: 提出包含风格解耦模块（SDM）和高阶多统计注意力（HOS - Attn）的AStF，并结合运动一致性正则化（MCR）判别器进行训练。

Result: 实验表明AStF在运动风格迁移上优于现有方法。

Conclusion: AStF能更全面地建模动态风格中固有的时空统计模式，在运动风格迁移方面表现出色。

Abstract: Human motion style transfer allows characters to appear less rigidity and
more realism with specific style. Traditional arbitrary image style transfer
typically process mean and variance which is proved effective. Meanwhile,
similar methods have been adapted for motion style transfer. However, due to
the fundamental differences between images and motion, relying on mean and
variance is insufficient to fully capture the complex dynamic patterns and
spatiotemporal coherence properties of motion data. Building upon this, our key
insight is to bring two more coefficient, skewness and kurtosis, into the
analysis of motion style. Specifically, we propose a novel Adaptive Statistics
Fusor (AStF) which consists of Style Disentanglement Module (SDM) and
High-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in
conjunction with a Motion Consistency Regularization (MCR) discriminator.
Experimental results show that, by providing a more comprehensive model of the
spatiotemporal statistical patterns inherent in dynamic styles, our proposed
AStF shows proficiency superiority in motion style transfers over
state-of-the-arts. Our code and model are available at
https://github.com/CHMimilanlan/AStF.

</details>


### [191] [Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model](https://arxiv.org/abs/2511.03888)
*Abdulmumin Sa'ad,Sulaimon Oyeniyi Adebayo,Abdul Jabbar Siddiqui*

Main category: cs.CV

TL;DR: 全球垃圾危机升级，传统垃圾收集方法有局限，本文提出基于剪枝轻量级YOLOv12的实时目标检测框架，在沙漠环境垃圾检测上有精度、效率优势。


<details>
  <summary>Details</summary>
Motivation: 全球垃圾危机加剧，传统垃圾收集方法在沙漠等环境效率低且危险，现有研究多关注城市和可回收物，忽视沙漠等地形及有机、危险废物。

Method: 提出基于剪枝轻量级YOLOv12的实时目标检测框架，集成Self - Adversarial Training (SAT)和专门的数据增强策略，使用DroneTrashNet数据集。

Result: 在精度、召回率和平均精度均值（mAP）上有显著提升，实现低延迟和小模型尺寸，适合资源受限的无人机部署，与最先进的轻量级YOLO变体对比展现出精度和效率的最佳平衡。

Conclusion: 结合以数据为中心和以模型为中心的增强方法对沙漠环境中强大的实时垃圾检测有效。

Abstract: The global waste crisis is escalating, with solid waste generation expected
to increase by 70% by 2050. Traditional waste collection methods, particularly
in remote or harsh environments like deserts, are labor-intensive, inefficient,
and often hazardous. Recent advances in computer vision and deep learning have
opened the door to automated waste detection systems, yet most research focuses
on urban environments and recyclable materials, overlooking organic and
hazardous waste and underexplored terrains such as deserts. In this work, we
propose an enhanced real-time object detection framework based on a pruned,
lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)
and specialized data augmentation strategies. Using the DroneTrashNet dataset,
we demonstrate significant improvements in precision, recall, and mean average
precision (mAP), while achieving low latency and compact model size suitable
for deployment on resource-constrained aerial drones. Benchmarking our model
against state-of-the-art lightweight YOLO variants further highlights its
optimal balance of accuracy and efficiency. Our results validate the
effectiveness of combining data-centric and model-centric enhancements for
robust, real-time waste detection in desert environments.

</details>


### [192] [MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection](https://arxiv.org/abs/2511.04255)
*Marawan Elbatel,Anbang Wang,Keyuan Liu,Kaouther Mouheb,Enrique Almar-Munoz,Lizhuo Lin,Yanqi Yang,Karim Lekadir,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本文未提出新架构，而是将以人类为中心的基础模型Sapiens适配到医学影像解剖标志点检测，提出MedSapiens模型，在多数据集上取得新的最优结果。


<details>
  <summary>Details</summary>
Motivation: 传统标志点检测依赖特定领域模型，大规模预训练视觉模型带来新机遇，以人类为中心的基础模型在解剖标志点检测的潜力未被充分挖掘。

Method: 对Sapiens模型进行多数据集预训练，将其应用于医学影像。

Result: MedSapiens在平均成功检测率上比通用模型最高提升5.26%，比专业模型最高提升21.81%；在少样本设置中比少样本最优结果提升2.69%。

Conclusion: 以人类为中心的基础模型能为解剖标志点检测提供强先验，有很大应用潜力。

Abstract: This paper does not introduce a novel architecture; instead, it revisits a
fundamental yet overlooked baseline: adapting human-centric foundation models
for anatomical landmark detection in medical imaging. While landmark detection
has traditionally relied on domain-specific models, the emergence of
large-scale pre-trained vision models presents new opportunities. In this
study, we investigate the adaptation of Sapiens, a human-centric foundation
model designed for pose estimation, to medical imaging through multi-dataset
pretraining, establishing a new state of the art across multiple datasets. Our
proposed model, MedSapiens, demonstrates that human-centric foundation models,
inherently optimized for spatial pose localization, provide strong priors for
anatomical landmark detection, yet this potential has remained largely
untapped. We benchmark MedSapiens against existing state-of-the-art models,
achieving up to 5.26% improvement over generalist models and up to 21.81%
improvement over specialist models in the average success detection rate (SDR).
To further assess MedSapiens adaptability to novel downstream tasks with few
annotations, we evaluate its performance in limited-data settings, achieving
2.69% improvement over the few-shot state of the art in SDR. Code and model
weights are available at https://github.com/xmed-lab/MedSapiens .

</details>


### [193] [Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery](https://arxiv.org/abs/2511.04260)
*Claudio Giusti,Luca Guarnera,Sebastiano Battiato*

Main category: cs.CV

TL;DR: 提出Proto - LeakNet框架用于AI图像和深度伪造取证，在潜在空间建模信号泄漏偏差，效果超现有方法。


<details>
  <summary>Details</summary>
Motivation: 合成图像和深度伪造生成模型发展使源归因和真实性验证成为计算机视觉系统的关键挑战，扩散管道输出存在信号泄漏。

Method: 提出Proto - LeakNet框架，在扩散模型潜在域重新模拟部分前向扩散暴露特定生成器线索，用时间注意力编码器聚合特征，特征加权原型头构建嵌入空间。

Result: 仅在封闭数据上训练，Macro AUC达98.13%，在后期处理下保持鲁棒性，能区分已知和未知生成器。

Conclusion: 在潜在空间建模信号泄漏偏差可实现可靠且可解释的AI图像和深度伪造取证。

Abstract: The growing sophistication of synthetic image and deepfake generation models
has turned source attribution and authenticity verification into a critical
challenge for modern computer vision systems. Recent studies suggest that
diffusion pipelines unintentionally imprint persistent statistical traces,
known as signal leaks, within their outputs, particularly in latent
representations. Building on this observation, we propose Proto-LeakNet, a
signal-leak-aware and interpretable attribution framework that integrates
closed-set classification with a density-based open-set evaluation on the
learned embeddings, enabling analysis of unseen generators without retraining.
Operating in the latent domain of diffusion models, our method re-simulates
partial forward diffusion to expose residual generator-specific cues. A
temporal attention encoder aggregates multi-step latent features, while a
feature-weighted prototype head structures the embedding space and enables
transparent attribution. Trained solely on closed data and achieving a Macro
AUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under
post-processing, surpassing state-of-the-art methods, and achieves strong
separability between known and unseen generators. These results demonstrate
that modeling signal-leak bias in latent space enables reliable and
interpretable AI-image and deepfake forensics. The code for the whole work will
be available upon submission.

</details>


### [194] [Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data](https://arxiv.org/abs/2511.04304)
*Robin Spanier,Thorsten Hoeser,Claudia Kuenzer*

Main category: cs.CV

TL;DR: 研究用合成和真实卫星图像训练YOLOv10模型检测海上基础设施，评估地理可迁移性，模型表现良好，强调合成数据重要性。


<details>
  <summary>Details</summary>
Motivation: 海上基础设施扩张需要有效监测系统，现有模型在样本稀缺时表现不佳，需提升性能。

Method: 用2023年第四季度四个地区的合成和真实Sentinel - 1卫星图像训练YOLOv10模型，在三个未参与训练地区检测评估地理可迁移性。

Result: 共检测到3529个海上平台，模型F1分数从0.85提升到0.90。

Conclusion: 强调平衡数据集重要性，合成数据是解决遥感常见问题的有效策略，展示深度学习用于全球海上基础设施监测的潜力。

Abstract: The recent and ongoing expansion of marine infrastructure, including offshore
wind farms, oil and gas platforms, artificial islands, and aquaculture
facilities, highlights the need for effective monitoring systems. The
development of robust models for offshore infrastructure detection relies on
comprehensive, balanced datasets, but falls short when samples are scarce,
particularly for underrepresented object classes, shapes, and sizes. By
training deep learning-based YOLOv10 object detection models with a combination
of synthetic and real Sentinel-1 satellite imagery acquired in the fourth
quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of
Guinea, and Coast of Brazil), this study investigates the use of synthetic
training data to enhance model performance. We evaluated this approach by
applying the model to detect offshore platforms in three unseen regions (Gulf
of Mexico, North Sea, Persian Gulf) and thereby assess geographic
transferability. This region-holdout evaluation demonstrated that the model
generalises beyond the training areas. In total, 3,529 offshore platforms were
detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and
1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which
improved to 0.90 upon incorporating synthetic data. We analysed how synthetic
data enhances the representation of unbalanced classes and overall model
performance, taking a first step toward globally transferable detection of
offshore infrastructure. This study underscores the importance of balanced
datasets and highlights synthetic data generation as an effective strategy to
address common challenges in remote sensing, demonstrating the potential of
deep learning for scalable, global offshore infrastructure monitoring.

</details>


### [195] [Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography](https://arxiv.org/abs/2511.04334)
*Saúl Alonso-Monsalve,Leigh H. Whitehead,Adam Aurisano,Lorena Escudero Sanchez*

Main category: cs.CV

TL;DR: 本文提出新方法用于医学影像肿瘤自动分割，在肾CT图像上取得好结果且有计算优势。


<details>
  <summary>Details</summary>
Motivation: 准确勾画肿瘤耗时，是临床定量分析瓶颈，需开发自动分割方法，传统CNN处理3D扫描有问题。

Method: 采用分两阶段的体素稀疏化和子流形稀疏卷积网络方法。

Result: 在KiTS23挑战的肾CT图像上，取得与冠军有竞争力的结果，如肾脏+肿块Dice系数95.8%等，计算上推理时间最多降60%，VRAM使用最多降75%。

Conclusion: 新方法能以高分辨率输入和原生3D模型架构进行分割，获先进精度，大幅减少计算资源需求。

Abstract: The accurate delineation of tumours in radiological images like Computed
Tomography is a very specialised and time-consuming task, and currently a
bottleneck preventing quantitative analyses to be performed routinely in the
clinical setting. For this reason, developing methods for the automated
segmentation of tumours in medical imaging is of the utmost importance and has
driven significant efforts in recent years. However, challenges regarding the
impracticality of 3D scans, given the large amount of voxels to be analysed,
usually requires the downsampling of such images or using patches thereof when
applying traditional convolutional neural networks. To overcome this problem,
in this paper we propose a new methodology that uses, divided into two stages,
voxel sparsification and submanifold sparse convolutional networks. This method
allows segmentations to be performed with high-resolution inputs and a native
3D model architecture, obtaining state-of-the-art accuracies while
significantly reducing the computational resources needed in terms of GPU
memory and time. We studied the deployment of this methodology in the context
of Computed Tomography images of renal cancer patients from the KiTS23
challenge, and our method achieved results competitive with the challenge
winners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%
for tumours + cysts, and 80.3% for tumours alone. Crucially, our method also
offers significant computational improvements, achieving up to a 60% reduction
in inference time and up to a 75\% reduction in VRAM usage compared to an
equivalent dense architecture, across both CPU and various GPU cards tested.

</details>


### [196] [Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA](https://arxiv.org/abs/2511.04384)
*Itbaan Safwan,Muhammad Annas Shaikh,Muhammad Haaris,Ramail Khan,Muhammad Atif Tahir*

Main category: cs.CV

TL;DR: 提出用于MediaEval Medico 2025挑战的多任务框架，用LoRA调优的Florence - 2模型，结合三个数据集，效果优于单任务基线。


<details>
  <summary>Details</summary>
Motivation: 解决医学视觉问答应用问题，提升回答准确性和可解释性。

Method: 使用LoRA调优的Florence - 2模型进行多任务学习，集成三个数据集。

Result: 多任务方法在答案准确性和视觉定位上大幅优于单任务基线。

Conclusion: 基于多任务学习的方法对医学视觉问答应用有效。

Abstract: We present a multi-task framework for the MediaEval Medico 2025 challenge,
leveraging a LoRA-tuned Florence-2 model for simultaneous visual question
answering (VQA), explanation generation, and visual grounding. The proposed
system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer
learning, (2) a synthetically enriched explanation dataset offering structured
medical reasoning, and (3) text-to-region pairs linking visual features with
segmentation masks. This multi-task setup enables the model to jointly learn
visual grounding, reasoning, and interpretation, producing responses that are
both accurate and interpretable. Extensive evaluation demonstrates that our
approach substantially improves over single-task baselines in both answer
accuracy and visual localization, highlighting the effectiveness of grounded
multi-task learning for medical VQA applications.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [197] [Climbing the label tree: Hierarchy-preserving contrastive learning for medical imaging](https://arxiv.org/abs/2511.03771)
*Alif Elham Khan*

Main category: q-bio.QM

TL;DR: 提出层次保留对比框架，含HWC和LAM目标，在多基准测试中提升医学图像表示质量，尊重标签树。


<details>
  <summary>Details</summary>
Motivation: 标准自监督学习忽略医学图像标签的分类结构，需利用标签树结构进行训练和评估。

Method: 提出层次保留对比框架，引入HWC和LAM两个插件式目标，公式与几何无关，适用于不同嵌入方式。

Result: 在多个基准测试中，所提目标持续提升表示质量，更好尊重分类法，消融实验表明HWC和LAM有效，结合效果最佳。

Conclusion: 提供了学习尊重标签树的医学图像表示的简单通用方法，提升了性能和可解释性。

Abstract: Medical image labels are often organized by taxonomies (e.g., organ - tissue
- subtype), yet standard self-supervised learning (SSL) ignores this structure.
We present a hierarchy-preserving contrastive framework that makes the label
tree a first-class training signal and an evaluation target. Our approach
introduces two plug-in objectives: Hierarchy-Weighted Contrastive (HWC), which
scales positive/negative pair strengths by shared ancestors to promote
within-parent coherence, and Level-Aware Margin (LAM), a prototype margin that
separates ancestor groups across levels. The formulation is geometry-agnostic
and applies to Euclidean and hyperbolic embeddings without architectural
changes. Across several benchmarks, including breast histopathology, the
proposed objectives consistently improve representation quality over strong SSL
baselines while better respecting the taxonomy. We evaluate with metrics
tailored to hierarchy faithfulness: HF1 (hierarchical F1), H-Acc
(tree-distance-weighted accuracy), and parent-distance violation rate. We also
report top-1 accuracy for completeness. Ablations show that HWC and LAM are
effective even without curvature, and combining them yields the most
taxonomy-aligned representations. Taken together, these results provide a
simple, general recipe for learning medical image representations that respect
the label tree and advance both performance and interpretability in
hierarchy-rich domains.

</details>


### [198] [CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment](https://arxiv.org/abs/2511.03826)
*Esha Sadia Nasir,Behnaz Elhaminia,Mark Eastwood,Catherine King,Owen Cain,Lorraine Harper,Paul Moss,Dimitrios Chanouzas,David Snead,Nasir Rajpoot,Adam Shephard,Shan E Ahmed Raza*

Main category: q-bio.QM

TL;DR: 提出新的粗到细框架CORE用于多模态全切片图像（WSI）的准确核级配准，在多个数据集上评估显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确高效的全切片图像配准对多染色组织切片的高分辨率、核级分析至关重要，现有方法可能存在不足。

Method: 粗配准阶段利用基于提示的组织掩码提取过滤伪影和非组织区域，进行全局对齐和加速密集特征匹配；细配准阶段检测核质心，使用自定义形状感知点集配准模型进行细粒度刚性配准，最后用相干点漂移（CPD）估计非线性位移场实现细胞级非刚性对齐。

Result: 在三个公开和两个私有WSI配准数据集上评估，CORE在泛化性、精度和鲁棒性方面优于当前最先进方法。

Conclusion: CORE是一种有效的多模态全切片图像核级配准方法，具有良好的性能。

Abstract: Accurate and efficient registration of whole slide images (WSIs) is essential
for high-resolution, nuclei-level analysis in multi-stained tissue slides. We
propose a novel coarse-to-fine framework CORE for accurate nuclei-level
registration across diverse multimodal whole-slide image (WSI) datasets. The
coarse registration stage leverages prompt-based tissue mask extraction to
effectively filter out artefacts and non-tissue regions, followed by global
alignment using tissue morphology and ac- celerated dense feature matching with
a pre-trained feature extractor. From the coarsely aligned slides, nuclei
centroids are detected and subjected to fine-grained rigid registration using a
custom, shape-aware point-set registration model. Finally, non-rigid alignment
at the cellular level is achieved by estimating a non-linear dis- placement
field using Coherent Point Drift (CPD). Our approach benefits from
automatically generated nuclei that enhance the accuracy of deformable
registra- tion and ensure precise nuclei-level correspondence across
modalities. The pro- posed model is evaluated on three publicly available WSI
registration datasets, and two private datasets. We show that CORE outperforms
current state-of-the-art methods in terms of generalisability, precision, and
robustness in bright-field and immunofluorescence microscopy WSIs

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [199] [MazeMate: An LLM-Powered Chatbot to Support Computational Thinking in Gamified Programming Learning](https://arxiv.org/abs/2511.03727)
*Chenyu Hou,Hua Yu,Gaoxia Zhu,John Derek Anas,Jiao Liu,Yew Soon Ong*

Main category: cs.HC

TL;DR: 提出LLM驱动的聊天机器人MazeMate嵌入3D迷宫编程游戏培养计算思维，首 次课堂应用显示有一定帮助但也有局限。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型应用罕有促进计算思维发展，需开发有效工具。

Method: 开发LLM驱动的聊天机器人MazeMate并在课堂开展实践，进行学生评价与主题分析。

Result: 学生认为MazeMate有一定帮助，对迷宫求解的感知有用性高于设计，支持部分计算思维过程，但迷宫设计支持有局限。

Conclusion: 基于LLM的脚手架有支持计算思维的潜力，需改进MazeMate以提升课堂可用性。

Abstract: Computational Thinking (CT) is a foundational problem-solving skill, and
gamified programming environments are a widely adopted approach to cultivating
it. While large language models (LLMs) provide on-demand programming support,
current applications rarely foster CT development. We present MazeMate, an
LLM-powered chatbot embedded in a 3D Maze programming game, designed to deliver
adaptive, context-sensitive scaffolds aligned with CT processes in maze solving
and maze design. We report on the first classroom implementation with 247
undergraduates. Students rated MazeMate as moderately helpful, with higher
perceived usefulness for maze solving than for maze design. Thematic analysis
confirmed support for CT processes such as decomposition, abstraction, and
algorithmic thinking, while also revealing limitations in supporting maze
design, including mismatched suggestions and fabricated algorithmic solutions.
These findings demonstrate the potential of LLM-based scaffolding to support CT
and underscore directions for design refinement to enhance MazeMate usability
in authentic classrooms.

</details>


### [200] [Efficient On-Device Agents via Adaptive Context Management](https://arxiv.org/abs/2511.03728)
*Sanidhya Vijayvargiya,Rahul Lokesh*

Main category: cs.HC

TL;DR: 本文提出上下文高效的设备端AI代理框架，打破设备内存限制带来的权衡，经评估该框架能压缩上下文并提升性能。


<details>
  <summary>Details</summary>
Motivation: 设备端AI代理受内存容量限制，实用上下文窗口减小，存在支持复杂交互与设备可行性之间的权衡。

Method: 提出上下文高效的设备端代理框架，包括使用专门LoRA适配器的动态内存系统、工具模式的极简序列化格式和即时模式传递机制。

Result: 代理在压缩上下文的同时，性能达到或超过传统基线，初始系统提示上下文减少超6倍，上下文增长率降低10 - 25倍。

Conclusion: 战略性上下文管理是解锁强大且持久的设备端AI的关键。

Abstract: On-device AI agents offer the potential for personalized, low-latency
assistance, but their deployment is fundamentally constrained by limited memory
capacity, which restricts usable context. This reduced practical context window
creates a trade-off between supporting rich, stateful interactions with complex
tool capabilities and maintaining on-device feasibility. We break this
trade-off with a framework for context-efficient on-device agents, driven by
three synergistic optimizations (1) a dynamic memory system using specialized
LoRA adapters to distill conversational history into a compressed, and
structured Context State Object; (2) a minimalist serialization format for tool
schemas to minimize token overhead per tool; and (3) a just-in-time
schema-passing mechanism that loads full tool definitions only upon tool
selection. We instantiate this framework by adapting a 3B parameter SLM to
context-efficient trajectories and rigorously evaluate it against a
conventional baseline on complex user tasks. Our agent matches, or exceeds, the
performance of a conventional baseline while dramatically compressing context,
achieving more than a 6-fold reduction in initial system prompt context and a
10- to 25-fold reduction in context growth rate based on the interaction
verbosity, demonstrating that strategic context management is key to unlocking
capable and persistent on-device AI.

</details>


### [201] [Beyond Chat: a Framework for LLMs as Human-Centered Support Systems](https://arxiv.org/abs/2511.03729)
*Zhiyin Zhou*

Main category: cs.HC

TL;DR: 本文提出以人类为中心的大语言模型支持系统的基于角色框架，确定设计原则、评估指标，分析风险并提出未来方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型正超越事务性问答，需支持其在敏感场景负责任集成。

Method: 提出基于角色框架，对比不同领域实际部署，确定设计原则，给出评估指标。

Result: 确定设计原则、评估指标，分析了过度依赖、幻觉、偏见等风险。

Conclusion: 应支持大语言模型在敏感场景负责任集成，给出未来研究方向。

Abstract: Large language models are moving beyond transactional question answering to
act as companions, coaches, mediators, and curators that scaffold human growth,
decision-making, and well-being. This paper proposes a role-based framework for
human-centered LLM support systems, compares real deployments across domains,
and identifies cross-cutting design principles: transparency, personalization,
guardrails, memory with privacy, and a balance of empathy and reliability. It
outlines evaluation metrics that extend beyond accuracy to trust, engagement,
and longitudinal outcomes. It also analyzes risks including over-reliance,
hallucination, bias, privacy exposure, and unequal access, and proposes future
directions spanning unified evaluation, hybrid human-AI models, memory
architectures, cross-domain benchmarking, and governance. The goal is to
support responsible integration of LLMs in sensitive settings where people need
accompaniment and guidance, not only answers.

</details>


### [202] [Not All Explanations are Created Equal: Investigating the Pitfalls of Current XAI Evaluation](https://arxiv.org/abs/2511.03730)
*Joe Shymanski,Jacob Brue,Sandip Sen*

Main category: cs.HC

TL;DR: 当前XAI评估技术不佳，研究指出多数解释会提升用户满意度，应重视可行动解释，还提出需更全面评估技术并分析不同解释适用场景。


<details>
  <summary>Details</summary>
Motivation: 当前XAI评估技术缺乏通用性，简单用户调查无法充分证明解释质量，需改进评估方法。

Method: 使用代理助手向用户教授国际象棋概念来验证观点。

Result: 证明多数解释会增加用户满意度，提出重视可行动解释。

Conclusion: 呼吁XAI领域采用更全面评估技术，分析了不同解释的适用场景。

Abstract: Explainable Artificial Intelligence (XAI) aims to create transparency in
modern AI models by offering explanations of the models to human users. There
are many ways in which researchers have attempted to evaluate the quality of
these XAI models, such as user studies or proposed objective metrics like
"fidelity". However, these current XAI evaluation techniques are ad hoc at best
and not generalizable. Thus, most studies done within this field conduct simple
user surveys to analyze the difference between no explanations and those
generated by their proposed solution. We do not find this to provide adequate
evidence that the explanations generated are of good quality since we believe
any kind of explanation will be "better" in most metrics when compared to none
at all. Thus, our study looks to highlight this pitfall: most explanations,
regardless of quality or correctness, will increase user satisfaction. We also
propose that emphasis should be placed on actionable explanations. We
demonstrate the validity of both of our claims using an agent assistant to
teach chess concepts to users. The results of this chapter will act as a call
to action in the field of XAI for more comprehensive evaluation techniques for
future research in order to prove explanation quality beyond user satisfaction.
Additionally, we present an analysis of the scenarios in which placebic or
actionable explanations would be most useful.

</details>


### [203] [MimiTalk: Revolutionizing Qualitative Research with Dual-Agent AI](https://arxiv.org/abs/2511.03731)
*Fengming Liu,Shubin Yu*

Main category: cs.HC

TL;DR: 提出MimiTalk框架用于社科研究对话数据收集，经三项研究评估，显示其有优势，支持人机协作开展定性研究。


<details>
  <summary>Details</summary>
Motivation: 为社科研究提供可扩展且符合伦理的对话数据收集框架。

Method: 构建含监督模型和对话模型的MimiTalk框架，开展三项研究，包括用户可用性评估、与人类访谈对比、跨学科研究者进行人机访谈及主题分析。

Result: MimiTalk降低访谈焦虑，保持对话连贯，在信息丰富度、连贯性和稳定性上优于人类访谈；AI访谈获取技术见解和敏感话题观点，人类访谈捕捉文化和情感细微差别。

Conclusion: 双智能体宪法AI支持有效的人机协作，可开展可复制、可扩展且质量可控的定性研究。

Abstract: We present MimiTalk, a dual-agent constitutional AI framework designed for
scalable and ethical conversational data collection in social science research.
The framework integrates a supervisor model for strategic oversight and a
conversational model for question generation. We conducted three studies: Study
1 evaluated usability with 20 participants; Study 2 compared 121 AI interviews
to 1,271 human interviews from the MediaSum dataset using NLP metrics and
propensity score matching; Study 3 involved 10 interdisciplinary researchers
conducting both human and AI interviews, followed by blind thematic analysis.
Results across studies indicate that MimiTalk reduces interview anxiety,
maintains conversational coherence, and outperforms human interviews in
information richness, coherence, and stability. AI interviews elicit technical
insights and candid views on sensitive topics, while human interviews better
capture cultural and emotional nuances. These findings suggest that dual-agent
constitutional AI supports effective human-AI collaboration, enabling
replicable, scalable and quality-controlled qualitative research.

</details>


### [204] [Conversational Collective Intelligence (CCI) using Hyperchat AI in an Authentic Forecasting Task](https://arxiv.org/abs/2511.03732)
*Hans Schumann,Louis Rosenberg,Ganesh Mani,Gregg Willcox*

Main category: cs.HC

TL;DR: 研究Hyperchat AI让人类群体预测棒球比赛结果，发现其高置信度预测表现超维加斯博彩市场，且对话率高时预测更准。


<details>
  <summary>Details</summary>
Motivation: 量化使用Hyperchat AI的人类群体预测准确性，验证其对提升群体集体智慧的作用。

Method: 让约24名体育粉丝网络群体借助AI代理进行实时对话，在8周内协作预测59场棒球比赛获胜者。

Result: 使用Hyperchat AI辩论比赛时，群体高置信度预测准确率78%，超维加斯博彩市场的57%；若对这些比赛下注可获46%投资回报率；高于平均对话率产生的高置信度预测准确率达88%。

Conclusion: Hyperchat AI能提升群体预测准确性，实时交互式审议对提高准确性至关重要。

Abstract: Hyperchat AI is a novel agentic technology that enables thoughtful
conversations among networked human groups of potentially unlimited size. It
allows large teams to discuss complex issues, brainstorm ideas, surface risks,
assess alternatives and efficiently converge on optimized solutions that
amplify the group's Collective Intelligence (CI). A formal study was conducted
to quantify the forecasting accuracy of human groups using Hyperchat AI to
conversationally predict the outcome of Major League Baseball (MLB) games.
During an 8-week period, networked groups of approximately 24 sports fans were
tasked with collaboratively forecasting the winners of 59 baseball games
through real-time conversation facilitated by AI agents. The results showed
that when debating the games using Hyperchat AI technology, the groups
converged on High Confidence predictions that significantly outperformed Vegas
betting markets. Specifically, groups were 78% accurate in their High
Confidence picks, a statistically strong result vs the Vegas odds of 57%
(p=0.020). Had the groups bet against the spread (ATS) on these games, they
would have achieved a 46% ROI against Vegas betting markets. In addition, High
Confidence forecasts that were generated through above-average conversation
rates were 88% accurate, suggesting that real-time interactive deliberation is
central to amplified accuracy.

</details>


### [205] [SnappyMeal: Design and Longitudinal Evaluation of a Multimodal AI Food Logging Application](https://arxiv.org/abs/2511.03907)
*Liam Bakar,Zachary Englhardt,Vidya Srinivas,Girish Narayanswamy,Dilini Nissanka,Shwetak Patel,Vikram Iyer*

Main category: cs.HC

TL;DR: 现有食物记录方法存在问题，提出AI系统SnappyMeal，评估显示其能提升饮食跟踪灵活性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前食物记录方法不灵活、依从性低且可能不准确，需要改进。

Method: 提出AI饮食跟踪系统SnappyMeal，利用多模态输入、目标相关跟进问题及信息检索；通过公开营养基准和多用户野外部署评估。

Result: 用户高度评价多种输入方式，认为准确性高。

Conclusion: 多模态AI系统可显著提升饮食跟踪灵活性和上下文感知，为智能自我跟踪应用奠定基础。

Abstract: Food logging, both self-directed and prescribed, plays a critical role in
uncovering correlations between diet, medical, fitness, and health outcomes.
Through conversations with nutritional experts and individuals who practice
dietary tracking, we find current logging methods, such as handwritten and
app-based journaling, are inflexible and result in low adherence and
potentially inaccurate nutritional summaries. These findings, corroborated by
prior literature, emphasize the urgent need for improved food logging methods.
In response, we propose SnappyMeal, an AI-powered dietary tracking system that
leverages multimodal inputs to enable users to more flexibly log their food
intake. SnappyMeal introduces goal-dependent follow-up questions to
intelligently seek missing context from the user and information retrieval from
user grocery receipts and nutritional databases to improve accuracy. We
evaluate SnappyMeal through publicly available nutrition benchmarks and a
multi-user, 3-week, in-the-wild deployment capturing over 500 logged food
instances. Users strongly praised the multiple available input methods and
reported a strong perceived accuracy. These insights suggest that multimodal AI
systems can be leveraged to significantly improve dietary tracking flexibility
and context-awareness, laying the groundwork for a new class of intelligent
self-tracking applications.

</details>


### [206] [Scaffolding Metacognition in Programming Education: Understanding Student-AI Interactions and Design Implications](https://arxiv.org/abs/2511.04144)
*Boxuan Ma,Huiyong Li,Gen Li,Li Chen,Cheng Tang,Yinjie Xie,Chenghao Gu,Atsushi Shimada,Shin'ichi Konomi*

Main category: cs.HC

TL;DR: 研究通过元认知视角分析大学编程课程中学生与AI的交互，为开发支持元认知参与的AI编码助手提炼设计考量。


<details>
  <summary>Details</summary>
Motivation: 现有研究对生成式AI工具对学生元认知过程的影响探索不足，主要关注正确性和可用性，本研究旨在填补这一空白。

Method: 分析三年多收集的超10000条对话日志，并结合学生和教育者的调查，聚焦提示和响应与元认知阶段和策略的一致性。

Result: 综合各数据源的发现，提炼出AI编码助手的设计考量。

Conclusion: 研究结果为开发强化学生编程学习过程的教育AI工具提供指导。

Abstract: Generative AI tools such as ChatGPT now provide novice programmers with
unprecedented access to instant, personalized support. While this holds clear
promise, their influence on students' metacognitive processes remains
underexplored. Existing work has largely focused on correctness and usability,
with limited attention to whether and how students' use of AI assistants
supports or bypasses key metacognitive processes. This study addresses that gap
by analyzing student-AI interactions through a metacognitive lens in
university-level programming courses. We examined more than 10,000 dialogue
logs collected over three years, complemented by surveys of students and
educators. Our analysis focused on how prompts and responses aligned with
metacognitive phases and strategies. Synthesizing these findings across data
sources, we distill design considerations for AI-powered coding assistants that
aim to support rather than supplant metacognitive engagement. Our findings
provide guidance for developing educational AI tools that strengthen students'
learning processes in programming education.

</details>


### [207] [Generate, Evaluate, Iterate: Synthetic Data for Human-in-the-Loop Refinement of LLM Judges](https://arxiv.org/abs/2511.04478)
*Hyo Jin Do,Zahra Ashktorab,Jasmina Gajcin,Erik Miehling,Martín Santillán Cooper,Qian Pan,Elizabeth M. Daly,Werner Geyer*

Main category: cs.HC

TL;DR: 提出集成合成数据生成的工具用于LLM评判工作流，用户研究显示多数参与者偏好该工具，合成数据效果与手工数据相当。


<details>
  <summary>Details</summary>
Motivation: 解决LLM评判范式中因缺乏多样代表性数据来完善评估标准的问题。

Method: 开发集成合成数据生成的工具，支持创建定制测试用例和AI辅助编辑，揭示生成背后的提示和解释。

Result: 用户研究中83%参与者偏好该工具，合成数据在完善评估标准和符合人类偏好方面与手工数据效果相当。

Conclusion: 合成数据是一种有前景的替代方案，尤其在效率和可扩展性至关重要的场景中。

Abstract: The LLM-as-a-judge paradigm enables flexible, user-defined evaluation, but
its effectiveness is often limited by the scarcity of diverse, representative
data for refining criteria. We present a tool that integrates synthetic data
generation into the LLM-as-a-judge workflow, empowering users to create
tailored and challenging test cases with configurable domains, personas,
lengths, and desired outcomes, including borderline cases. The tool also
supports AI-assisted inline editing of existing test cases. To enhance
transparency and interpretability, it reveals the prompts and explanations
behind each generation. In a user study (N=24), 83% of participants preferred
the tool over manually creating or selecting test cases, as it allowed them to
rapidly generate diverse synthetic data without additional workload. The
generated synthetic data proved as effective as hand-crafted data for both
refining evaluation criteria and aligning with human preferences. These
findings highlight synthetic data as a promising alternative, particularly in
contexts where efficiency and scalability are critical.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [208] [Leveraging LLM-based agents for social science research: insights from citation network simulations](https://arxiv.org/abs/2511.03758)
*Jiarui Ji,Runlin Lei,Xuchen Pan,Zhewei Wei,Hao Sun,Yankai Lin,Xu Chen,Yongzheng Yang,Yaliang Li,Bolin Ding,Ji-Rong Wen*

Main category: physics.soc-ph

TL;DR: 本文引入CiteAgent框架基于大语言模型模拟人类行为生成引用网络，建立两种研究范式分析引用网络现象，拓展科学学研究，展示了大语言模型推动社科科学学研究的潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在社会模拟中的能力边界不明确，为探索其社会属性。

Method: 引入CiteAgent框架生成引用网络，建立LLM - SE和LLM - LE两种研究范式。

Result: CiteAgent成功捕捉现实引用网络主要现象，研究范式便于分析引用网络现象，模拟实验结果为现实学术环境提供见解。

Conclusion: 大语言模型有推动社会科学中科学学研究的潜力。

Abstract: The emergence of Large Language Models (LLMs) demonstrates their potential to
encapsulate the logic and patterns inherent in human behavior simulation by
leveraging extensive web data pre-training. However, the boundaries of LLM
capabilities in social simulation remain unclear. To further explore the social
attributes of LLMs, we introduce the CiteAgent framework, designed to generate
citation networks based on human-behavior simulation with LLM-based agents.
CiteAgent successfully captures predominant phenomena in real-world citation
networks, including power-law distribution, citational distortion, and
shrinking diameter. Building on this realistic simulation, we establish two
LLM-based research paradigms in social science: LLM-SE (LLM-based Survey
Experiment) and LLM-LE (LLM-based Laboratory Experiment). These paradigms
facilitate rigorous analyses of citation network phenomena, allowing us to
validate and challenge existing theories. Additionally, we extend the research
scope of traditional science of science studies through idealized social
experiments, with the simulation experiment results providing valuable insights
for real-world academic environments. Our work demonstrates the potential of
LLMs for advancing science of science research in social science.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [209] [DeepPAAC: A New Deep Galerkin Method for Principal-Agent Problems](https://arxiv.org/abs/2511.04309)
*Michael Ludkovski,Changgen Xie,Zimu Zhu*

Main category: math.NA

TL;DR: 本文考虑连续时间下委托 - 代理问题的数值求解，提出DeepPAAC算法并进行案例研究。


<details>
  <summary>Details</summary>
Motivation: 解决连续时间下委托 - 代理问题的数值求解，处理含隐式哈密顿量的Hamilton - Jacobi - Bellman方程。

Method: 开发了名为Deep Principal - Agent Actor Critic (DeepPAAC)的深度学习算法。

Result: 该算法能处理多维状态、控制和约束，研究了神经网络架构、训练设计和损失函数等对求解器收敛的影响。

Conclusion: 通过五个不同案例研究展示了算法在解决委托 - 代理问题上的应用。

Abstract: We consider numerical resolution of principal-agent (PA) problems in
continuous time. We formulate a generic PA model with continuous and lump
payments and a multi-dimensional strategy of the agent. To tackle the resulting
Hamilton-Jacobi-Bellman equation with an implicit Hamiltonian we develop a
novel deep learning method: the Deep Principal-Agent Actor Critic (DeepPAAC)
Actor-Critic algorithm. DeepPAAC is able to handle multi-dimensional states and
controls, as well as constraints. We investigate the role of the neural network
architecture, training designs, loss functions, etc. on the convergence of the
solver, presenting five different case studies.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [210] [Levers of Power in the Field of AI](https://arxiv.org/abs/2511.03859)
*Tammy Mackenzie,Sukriti Punj,Natalie Perez,Sreyoshi Bhaduri,Branislav Radeljic*

Main category: cs.CY

TL;DR: 本文研究不同领域决策者在人工智能实施中如何应对权力问题，通过问卷收集数据，以虚构人物形象呈现结果，探讨决策者个人权力、机构稳定性及变革方法，最后给出权力杠杆动态表和可测试假设。


<details>
  <summary>Details</summary>
Motivation: 研究不同领域决策者在人工智能实施中如何应对权力问题，探索个人如何体验和运用权力杠杆。

Method: 基于新制度主义者的机构治理框架设计个性化问卷，收集决策者对机构权限的看法。

Result: 以十二位来自北美和欧洲的高级决策者的虚构人物形象呈现匿名、真实的回应和情况。

Conclusion: 为机构内政策制定者和民间社会同行提供个人参与人工智能治理的途径。

Abstract: This paper examines how decision makers in academia, government, business,
and civil society navigate questions of power in implementations of artificial
intelligence. The study explores how individuals experience and exercise levers
of power, which are presented as social mechanisms that shape institutional
responses to technological change. The study reports on the responses of
personalized questionnaires designed to gather insight on a decision maker's
institutional purview, based on an institutional governance framework developed
from the work of Neo-institutionalists. Findings present the anonymized, real
responses and circumstances of respondents in the form of twelve fictional
personas of high-level decision makers from North America and Europe. These
personas illustrate how personal agency, organizational logics, and
institutional infrastructures may intersect in the governance of AI. The
decision makers' responses to the questionnaires then inform a discussion of
the field-level personal power of decision makers, methods of fostering
institutional stability in times of change, and methods of influencing
institutional change in the field of AI. The final section of the discussion
presents a table of the dynamics of the levers of power in the field of AI for
change makers and five testable hypotheses for institutional and social
movement researchers. In summary, this study provides insight on the means for
policymakers within institutions and their counterparts in civil society to
personally engage with AI governance.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [211] [Nonparametric Modeling of Continuous-Time Markov Chains](https://arxiv.org/abs/2511.03954)
*Filippo Monti,Xiang Ji,Marc A. Suchard*

Main category: stat.ME

TL;DR: 提出新贝叶斯框架推断连续时间马尔可夫链（CTMC）速率，用可扩展HMC采样器提升效率，在贝叶斯系统地理学推断中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 推断CTMC的无穷小速率是许多科学领域的核心挑战，现有任务受状态空间扩展、速率依赖和信息不完整等因素阻碍。

Method: 引入新贝叶斯框架，通过高斯过程（GPs）结合协变量对CTMC速率建模；采用可扩展的哈密顿蒙特卡罗（HMC）采样器进行高效推断；将HMC轨迹与可扩展梯度近似相结合以降低计算复杂度。

Result: 将计算精确似然梯度的计算复杂度从$O(K^5)$降至$O(K^2)$，在合成和真实数据集上展示了方法的有效性。

Conclusion: 新方法能捕获复杂非线性关系，更充分利用协变量信息，准确描述其影响，在贝叶斯系统地理学推断等领域具有有效性。

Abstract: Inferring the infinitesimal rates of continuous-time Markov chains (CTMCs) is
a central challenge in many scientific domains. This task is hindered by three
factors: quadratic growth in the number of rates as the CTMC state space
expands, strong dependencies among rates, and incomplete information for many
transitions. We introduce a new Bayesian framework that flexibly models the
CTMC rates by incorporating covariates through Gaussian processes (GPs). This
approach improves inference by integrating new information and contributes to
the understanding of the CTMC stochastic behavior by shedding light on
potential external drivers. Unlike previous approaches limited to linear
covariate effects, our method captures complex non-linear relationships,
enabling fuller use of covariate information and more accurate characterization
of their influence. To perform efficient inference, we employ a scalable
Hamiltonian Monte Carlo (HMC) sampler. We address the prohibitive cost of
computing the exact likelihood gradient by integrating the HMC trajectories
with a scalable gradient approximation, reducing the computational complexity
from $O(K^5)$ to $O(K^2)$, where $K$ is the number of CTMC states. Finally, we
demonstrate our method on Bayesian phylogeography inference -- a domain where
CTMCs are central -- showing effectiveness on both synthetic and real datasets.

</details>


### [212] [Generative Bayesian Filtering and Parameter Learning](https://arxiv.org/abs/2511.04552)
*Edoardo Marcelli,Sean O'Hagan,Veronika Rockova*

Main category: stat.ME

TL;DR: 本文提出生成贝叶斯滤波（GBF），将生成贝叶斯计算扩展到动态设置，引入生成 - 吉布斯采样器解决参数学习问题，研究表明GBF在处理难处理的状态空间模型时优于现有无似然方法。


<details>
  <summary>Details</summary>
Motivation: 在复杂非线性和非高斯状态空间模型中进行后验推断，解决参数学习问题，处理观测或转移分布解析难处理的情况。

Method: 将生成贝叶斯计算扩展到动态设置实现递归后验推断，引入生成 - 吉布斯采样器绕过显式密度评估进行参数学习。

Result: 通过模拟和实证研究评估，GBF在处理难处理的状态空间模型时，在准确性和鲁棒性上显著优于现有无似然方法。

Conclusion: GBF为复杂非线性和非高斯状态空间模型的后验推断提供了有效方法，在处理难处理模型方面表现出色。

Abstract: Generative Bayesian Filtering (GBF) provides a powerful and flexible
framework for performing posterior inference in complex nonlinear and
non-Gaussian state-space models. Our approach extends Generative Bayesian
Computation (GBC) to dynamic settings, enabling recursive posterior inference
using simulation-based methods powered by deep neural networks. GBF does not
require explicit density evaluations, making it particularly effective when
observation or transition distributions are analytically intractable. To
address parameter learning, we introduce the Generative-Gibbs sampler, which
bypasses explicit density evaluation by iteratively sampling each variable from
its implicit full conditional distribution. Such technique is broadly
applicable and enables inference in hierarchical Bayesian models with
intractable densities, including state-space models. We assess the performance
of the proposed methodologies through both simulated and empirical studies,
including the estimation of $\alpha$-stable stochastic volatility models. Our
findings indicate that GBF significantly outperforms existing likelihood-free
approaches in accuracy and robustness when dealing with intractable state-space
models.

</details>


### [213] [Geometric Decomposition of Statistical Inference through Gradient Flow and Co-Monotonicity Measures](https://arxiv.org/abs/2511.04599)
*Pawel Gajer,Jacques Ravel*

Main category: stat.ME

TL;DR: 本文提出几何分解框架，含两种策略对高维数据特征 - 结果关联进行区域分析，可独立或联合使用，用贝叶斯后验采样提供可信区间。


<details>
  <summary>Details</summary>
Motivation: 在子群体关系不同时，理解高维数据特征 - 结果关联有挑战，标准方法假设全局关联会遗漏上下文依赖模式，降低统计效力和可解释性。

Method: 开发几何分解框架，包括梯度流分解（用离散莫尔斯理论划分样本）和共单调性分解（利用关联结构，通过双聚类识别共单调单元和特征模块），两种策略可独立或联合使用，用贝叶斯后验采样。

Result: 未明确提及具体结果。

Conclusion: 未明确提及具体结论。

Abstract: Understanding feature-outcome associations in high-dimensional data remains
  challenging when relationships vary across subpopulations, yet standard
  methods assuming global associations miss context-dependent patterns,
reducing
  statistical power and interpretability. We develop a geometric decomposition
  framework offering two strategies for partitioning inference problems into
  regional analyses on data-derived Riemannian graphs. Gradient flow
  decomposition uses path-monotonicity-validated discrete Morse theory to
  partition samples into basins where outcomes exhibit monotonic behavior.
  Co-monotonicity decomposition leverages association structure: vertex-level
  coefficients measuring directional concordance between outcome and features,
  or between feature pairs, define embeddings of samples into association
space.
  These embeddings induce Riemannian k-NN graphs on which biclustering
  identifies co-monotonicity cells (coherent regions) and feature modules. This
  extends naturally to multi-modal integration across multiple feature sets.
  Both strategies apply independently or jointly, with Bayesian posterior
  sampling providing credible intervals.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [214] [Vectorized Computation of Euler Characteristic Functions and Transforms](https://arxiv.org/abs/2511.03909)
*Jessi Cisewski-Kehe,Brittany Terese Fasy,Alexander McCleary,Eli Quist,Jack Ruder*

Main category: cs.CG

TL;DR: 提出用张量运算计算拓扑变换的矢量化框架，比现有方法快，已实现为Python包pyECT。


<details>
  <summary>Details</summary>
Motivation: 现有计算加权欧拉特征变换（WECT）和欧拉特征函数（ECF）的方法速度未优化且无法扩展到高维。

Method: 使用张量运算构建矢量化框架，适用于GPU架构和任意维度的几何单纯复形或立方体复形。

Result: 在多种图像数据集上计算WECT和ECF时，框架比现有方法有显著加速（高达180倍）。

Conclusion: 提出的框架可高效计算拓扑变换，已通过Python包公开实现。

Abstract: The weighted Euler characteristic transform (WECT) and Euler characteristic
function (ECF) have proven to be useful tools in a variety of applications.
However, current methods for computing these functions are neither optimized
for speed nor do they scale to higher-dimensional settings. In this work, we
present a vectorized framework for computing such topological transforms using
tensor operations, which is highly optimized for GPU architectures and works in
full generality across geometric simplicial complexes (or cubical complexes) of
arbitrary dimension. Experimentally, the framework demonstrates significant
speedups (up to $180 \times$) over existing methods when computing the WECT and
ECF across a variety of image datasets. Computation of these transforms is
implemented in a publicly available Python package called pyECT.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [215] [An Automated Theorem Generator with Theoretical Foundation Based on Rectangular Standard Contradiction](https://arxiv.org/abs/2511.04092)
*Yang Xu,Peiyao Liu,Shuwei Chen,Jun Liu*

Main category: cs.LO

TL;DR: 本文提出新颖的自动定理生成理论与工具，基于矩形标准矛盾结构构建ATG理论，设计算法并开发生成器，推动机器角色转变。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统生成非平凡且逻辑有效的定理的严格理论体系，为填补这一空白开展研究。

Method: 基于标准矛盾概念，定义并证明矩形标准矛盾结构，提出完整ATG理论，利用其性质证明可形成有效定理；设计基于模板的ATG算法，开发生成器。

Result: 证明了矩形标准矛盾的两个核心性质，证明能形成有效定理且所有此类定理逻辑等价，开发出矩形自动定理生成器。

Conclusion: 本研究使机器从“验证者”转变为“发现者”，为逻辑和人工智能基础研究开辟新途径。

Abstract: Currently, there is a lack of rigorous theoretical system for systematically
generating non-trivial and logically valid theorems. Addressing this critical
gap, this paper conducts research to propose a novel automated theorem
generation theory and tool. Based on the concept of standard contradiction
which possesses unique deductive advantages, this paper defines and proves, for
the first time, a new logical structure known as rectangular standard
contradiction. Centered on this structure, a complete Automated Theorem
Generation (ATG) theory is put forward. Theoretical proofs clarify two core
properties of rectangular standard contradiction: first, it is a standard
contradiction (necessarily unsatisfiable); second, it exhibits non-redundancy
(the remaining clause set becomes satisfiable after removing any clause).
Leveraging these properties, this paper proves that partitioning a rectangular
standard contradiction into a premise subset $A$ and negation of its complement
$H$, a valid theorem $A \vdash \neg H$ can be formed, and all such theorems are
logically equivalent. To implement this theory, an efficient template-based ATG
algorithm is designed, and a Rectangular Automated Theorem Generator is
developed. This research enables machines to transition from "verifiers" to
"discoverers", opening up new avenues for fundamental research in the fields of
logic and artificial intelligence.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [216] [Secure Code Generation at Scale with Reflexion](https://arxiv.org/abs/2511.03898)
*Arup Datta,Ahmed Aljohani,Hyunsook Do*

Main category: cs.CR

TL;DR: 评估指令调优代码大语言模型安全代码生成能力，发现初始不安全情况普遍，反射提示可提升安全性，一到两轮提示收益最大。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的代码不一定安全，需要评估其安全代码生成能力。

Method: 使用Instruct Prime评估，采用零样本基线和三轮反射提示方法，用不安全代码检测器（ICD）测量安全性，报告Repair、Regression和NetGain指标。

Result: 首轮不安全情况普遍，弱加密/配置依赖漏洞难避免，Python安全率最高，C和C#最低；反射提示提升所有模型安全性，平均准确率从70.74%提升到79.43%，首轮收益最大。

Conclusion: 应用一到两轮反射提示能获得大部分收益。

Abstract: Large language models (LLMs) are now widely used to draft and refactor code,
but code that works is not necessarily secure. We evaluate secure code
generation using the Instruct Prime, which eliminated compliance-required
prompts and cue contamination, and evaluate five instruction-tuned code LLMs
using a zero-shot baseline and a three-round reflexion prompting approach.
Security is measured using the Insecure Code Detector (ICD), and results are
reported by measuring Repair, Regression, and NetGain metrics, considering the
programming language and CWE family. Our findings show that insecurity remains
common at the first round: roughly 25-33% of programs are insecure at a
zero-shot baseline (t0 ). Weak cryptography/config-dependent bugs are the
hardest to avoid while templated ones like XSS, code injection, and hard-coded
secrets are handled more reliably. Python yields the highest secure rates; C
and C# are the lowest, with Java, JS, PHP, and C++ in the middle. Reflexion
prompting improves security for all models, improving average accuracy from
70.74% at t0 to 79.43% at t3 , with the largest gains in the first round
followed by diminishing returns. The trends with Repair, Regression, and
NetGain metrics show that applying one to two rounds produces most of the
benefits. A replication package is available at
https://doi.org/10.5281/zenodo.17065846.

</details>


### [217] [Exploratory Analysis of Cyberattack Patterns on E-Commerce Platforms Using Statistical Methods](https://arxiv.org/abs/2511.03020)
*Fatimo Adenike Adeniya*

Main category: cs.CR

TL;DR: 本文提出混合分析框架检测和预测电商领域网络攻击模式，发现高危时期攻击高峰，CatBoost 模型表现最佳，为网络安全资源分配提供见解。


<details>
  <summary>Details</summary>
Motivation: 电商平台网络攻击日益复杂，威胁消费者信任和运营连续性，需检测和预测攻击模式。

Method: 集成统计建模和机器学习，用 VCDB 数据集，Auto ARIMA 进行时间预测和显著性测试，ANOVA 检查季节性变化，用 XGBoost、LightGBM 和 CatBoost 进行预测分类。

Result: 高危时期（如黑色星期五和假期）有攻击高峰，涉及 PII 的违规威胁指标升高，CatBoost 模型性能最佳。

Conclusion: 框架结合季节性预测和可解释的集成学习，可进行风险预测和分类，虽有局限但为资源分配提供见解并指明未来研究方向。

Abstract: Cyberattacks on e-commerce platforms have grown in sophistication,
threatening consumer trust and operational continuity. This research presents a
hybrid analytical framework that integrates statistical modelling and machine
learning for detecting and forecasting cyberattack patterns in the e-commerce
domain. Using the Verizon Community Data Breach (VCDB) dataset, the study
applies Auto ARIMA for temporal forecasting and significance testing, including
a Mann-Whitney U test (U = 2579981.5, p = 0.0121), which confirmed that holiday
shopping events experienced significantly more severe cyberattacks than
non-holiday periods. ANOVA was also used to examine seasonal variation in
threat severity, while ensemble machine learning models (XGBoost, LightGBM, and
CatBoost) were employed for predictive classification. Results reveal recurrent
attack spikes during high-risk periods such as Black Friday and holiday
seasons, with breaches involving Personally Identifiable Information (PII)
exhibiting elevated threat indicators. Among the models, CatBoost achieved the
highest performance (accuracy = 85.29%, F1 score = 0.2254, ROC AUC = 0.8247).
The framework uniquely combines seasonal forecasting with interpretable
ensemble learning, enabling temporal risk anticipation and breach-type
classification. Ethical considerations, including responsible use of sensitive
data and bias assessment, were incorporated. Despite class imbalance and
reliance on historical data, the study provides insights for proactive
cybersecurity resource allocation and outlines directions for future real-time
threat detection research.

</details>


### [218] [Hybrid Fuzzing with LLM-Guided Input Mutation and Semantic Feedback](https://arxiv.org/abs/2511.03995)
*Shiyin Lin*

Main category: cs.CR

TL;DR: 提出结合静态和动态分析、大语言模型引导的输入变异和语义反馈的混合模糊测试框架，在真实开源目标上评估效果良好，展示结合LLM推理与语义感知反馈的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有软件模糊测试的变异策略缺乏语义感知，导致测试用例冗余和程序深层状态探索缓慢。

Method: 将静态和动态分析与大语言模型引导的输入变异和语义反馈集成，在AFL++上实现，结合程序插桩和基于嵌入的语义相似度指标引导种子选择。

Result: 在libpng、tcpdump和sqlite等真实开源目标上，相比现有先进模糊测试器，实现更快首次发现漏洞时间、更高语义多样性和相当数量的独特漏洞。

Conclusion: 结合大语言模型推理与语义感知反馈能加速和深化漏洞发现。

Abstract: Software fuzzing has become a cornerstone in automated vulnerability
discovery, yet existing mutation strategies often lack semantic awareness,
leading to redundant test cases and slow exploration of deep program states. In
this work, I present a hybrid fuzzing framework that integrates static and
dynamic analysis with Large Language Model (LLM)-guided input mutation and
semantic feedback. Static analysis extracts control-flow and data-flow
information, which is transformed into structured prompts for the LLM to
generate syntactically valid and semantically diverse inputs. During execution,
I augment traditional coverage-based feedback with semantic feedback
signals-derived from program state changes, exception types, and output
semantics-allowing the fuzzer to prioritize inputs that trigger novel program
behaviors beyond mere code coverage. I implement our approach atop AFL++,
combining program instrumentation with embedding-based semantic similarity
metrics to guide seed selection. Evaluation on real-world open-source targets,
including libpng, tcpdump, and sqlite, demonstrates that our method achieves
faster time-to-first-bug, higher semantic diversity, and a competitive number
of unique bugs compared to state-of-the-art fuzzers. This work highlights the
potential of combining LLM reasoning with semantic-aware feedback to accelerate
and deepen vulnerability discovery.

</details>


### [219] [Automated and Explainable Denial of Service Analysis for AI-Driven Intrusion Detection Systems](https://arxiv.org/abs/2511.04114)
*Paul Badu Yakubu,Lesther Santana,Mohamed Rahouti,Yufeng Xin,Abdellah Chehri,Mohammed Aledhari*

Main category: cs.CR

TL;DR: 本文提出用机器学习自动框架检测和解释DDoS攻击，结合TPOT和SHAP提高检测准确性与透明度。


<details>
  <summary>Details</summary>
Motivation: 随着DDoS攻击增加，传统检测系统在可扩展性和透明度上有问题，需更高效可解释的检测方法。

Method: 利用TPOT自动选择和优化机器学习模型与特征，结合SHAP增强模型可解释性。

Result: 实验表明平均反向包长度和最小正向包头长度等关键特征对检测DDoS攻击很重要。

Conclusion: 该方法提供了可扩展且可解释的网络安全解决方案。

Abstract: With the increasing frequency and sophistication of Distributed Denial of
Service (DDoS) attacks, it has become critical to develop more efficient and
interpretable detection methods. Traditional detection systems often struggle
with scalability and transparency, hindering real-time response and
understanding of attack vectors. This paper presents an automated framework for
detecting and interpreting DDoS attacks using machine learning (ML). The
proposed method leverages the Tree-based Pipeline Optimization Tool (TPOT) to
automate the selection and optimization of ML models and features, reducing the
need for manual experimentation. SHapley Additive exPlanations (SHAP) is
incorporated to enhance model interpretability, providing detailed insights
into the contribution of individual features to the detection process. By
combining TPOT's automated pipeline selection with SHAP interpretability, this
approach improves the accuracy and transparency of DDoS detection. Experimental
results demonstrate that key features such as mean backward packet length and
minimum forward packet header length are critical in detecting DDoS attacks,
offering a scalable and explainable cybersecurity solution.

</details>


### [220] [Confidential Computing for Cloud Security: Exploring Hardware based Encryption Using Trusted Execution Environments](https://arxiv.org/abs/2511.04550)
*Dhruv Deepak Agarwal,Aswani Kumar Cherukuri*

Main category: cs.CR

TL;DR: 云计算发展带来安全挑战，研究探索TEEs架构、特性及有效性，分析部署策略等，结果表明TEEs对加强云安全有重要作用。


<details>
  <summary>Details</summary>
Motivation: 云计算发展带来数据安全挑战，传统安全实践无法保护使用中的数据，需新方法保障云数据安全。

Method: 探索Intel SGX和ARM TrustZone等TEEs的架构和安全特性，进行文献调研，分析部署策略、性能指标和实际用途，讨论部署问题、弱点等。

Result: TEEs在加强和推进云安全基础设施中处于核心地位，能为机密计算创建安全基础。

Conclusion: TEEs对提升云数据安全有重要意义，可作为机密计算的安全基础。

Abstract: The growth of cloud computing has revolutionized data processing and storage
capacities to another levels of scalability and flexibility. But in the
process, it has created a huge challenge of security, especially in terms of
safeguarding sensitive data. Classical security practices, including encryption
at rest and during transit, fail to protect data in use and expose it to
various possible breaches. In response to this problem , Confidential Computing
has been a tool ,seeking to secure data in processing by usage of
hardware-based Trusted Execution Environments (TEEs). TEEs, including Intel's
Software Guard Extensions (SGX) and ARM's TrustZone, offers protected contexts
within the processor, where data is kept confidential ,intact and secure , even
with malicious software or compromised operating systems. In this research, we
have explored the architecture and security features of TEEs like Intel SGX and
ARM TrustZone, and their effectiveness in improving cloud data security. From a
thorough literature survey ,we have analyzed the deployment strategies,
performance indicators, and practical uses of these TEEs for the same purpose.
In addition, we have discussed the issues regarding deployment, possible
weaknesses, scalability issues, and integration issues. Our results focuses on
the central position of TEEs in strengthening and advancing cloud security
infrastructures, pointing towards their ability to create a secure foundation
for Confidential Computing.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [221] [On the Brittleness of CLIP Text Encoders](https://arxiv.org/abs/2511.04247)
*Allie Tran,Luca Rossetto*

Main category: cs.MM

TL;DR: 本文系统分析多媒体信息检索场景中非语义查询扰动的影响，发现句法和语义扰动导致最大不稳定性，强调评估视觉语言模型需考虑鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态共嵌入模型在对比对齐训练时对小输入扰动缺乏稳定性，手动表达查询的微小变化会导致匹配结果排名差异大。

Method: 使用TRECVID Ad - Hoc Video Search查询和V3C1视频集，对多种CLIP变体进行词汇、句法和语义扰动评估。

Result: 句法和语义扰动导致最大不稳定性，脆性集中在标点和大小写等表面编辑。

Conclusion: 鲁棒性是评估视觉语言模型除基准准确性外的关键维度。

Abstract: Multimodal co-embedding models, especially CLIP, have advanced the state of
the art in zero-shot classification and multimedia information retrieval in
recent years by aligning images and text in a shared representation space.
However, such modals trained on a contrastive alignment can lack stability
towards small input perturbations. Especially when dealing with manually
expressed queries, minor variations in the query can cause large differences in
the ranking of the best-matching results. In this paper, we present a
systematic analysis of the effect of multiple classes of non-semantic query
perturbations in an multimedia information retrieval scenario. We evaluate a
diverse set of lexical, syntactic, and semantic perturbations across multiple
CLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 video
collection. Across models, we find that syntactic and semantic perturbations
drive the largest instabilities, while brittleness is concentrated in trivial
surface edits such as punctuation and case. Our results highlight robustness as
a critical dimension for evaluating vision-language models beyond benchmark
accuracy.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [222] [Scalable Domain-decomposed Monte Carlo Neutral Transport for Nuclear Fusion](https://arxiv.org/abs/2511.04489)
*Oskar Lappi,Huw Leggate,Yannick Marandet,Jan Åström,Keijo Heljanko,Dmitriy V. Borodin*

Main category: physics.comp-ph

TL;DR: 本文介绍新开源代码Eiron中实现的域分解蒙特卡罗（DDMC）算法，与EIRENE中两种并行算法对比显示DDMC性能更优，得出在EIRENE中实现该算法可提升性能并解决内存限制问题的结论。


<details>
  <summary>Details</summary>
Motivation: EIRENE未实现域分解，无法用于网格数据无法存于单个计算节点的模拟，需要改进算法提升性能和解决内存问题。

Method: 在新开源代码Eiron中实现DDMC算法，并在其中实现EIRENE现有的两种并行算法，通过强缩放和弱缩放测试进行比较。

Result: DDMC在几乎所有情况下表现优于其他两种算法；在Mahti超级计算机上，对于不适合L3缓存切片的网格，DDMC强缩放呈超线性；弱缩放测试中，高碰撞情况弱缩放效率45%，低碰撞情况为26%。

Conclusion: 在EIRENE中实现域分解算法可提升性能，使因内存限制无法进行的模拟成为可能。

Abstract: EIRENE [1] is a Monte Carlo neutral transport solver heavily used in the
fusion community. EIRENE does not implement domain decomposition, making it
impossible to use for simulations where the grid data does not fit on one
compute node (see e.g. [2]). This paper presents a domain-decomposed Monte
Carlo (DDMC) algorithm implemented in a new open source Monte Carlo code,
Eiron. Two parallel algorithms currently used in EIRENE are also implemented in
Eiron, and the three algorithms are compared by running strong scaling tests,
with DDMC performing better than the other two algorithms in nearly all cases.
On the supercomputer Mahti [3], DDMC strong scaling is superlinear for grids
that do not fit into an L3 cache slice (4 MiB). The DDMC algorithm is also
scaled up to 16384 cores in weak scaling tests, with a weak scaling efficiency
of 45% in a high-collisional (heavier compute load) case, and 26% in a
low-collisional (lighter compute load) case. We conclude that implementing this
domain decomposition algorithm in EIRENE would improve performance and enable
simulations that are currently impossible due to memory constraints.

</details>


### [223] [Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI](https://arxiv.org/abs/2511.04564)
*Yoh-ichi Mototake,Makoto Sasaki*

Main category: physics.comp-ph

TL;DR: 本文提出框架量化和分析物理信息机器学习中系数函数估计的不确定性，应用于磁流体动力学简化模型，表明有不确定性且结合几何约束可唯一识别模型。


<details>
  <summary>Details</summary>
Motivation: 物理信息机器学习通常基于预测性能估计系数函数，但物理学评估模型并非仅依赖预测准确性，存在数据驱动模型推理的固有不确定性，需选择有物理意义的解。

Method: 提出一个框架来量化和分析系数函数估计中的不确定性，并应用于磁流体动力学简化模型。

Result: 框架显示存在不确定性，结合几何约束可进行唯一识别。

Conclusion: 结合几何约束可以唯一地估计简化模型。

Abstract: Physics-informed machine learning (PIML) integrates partial differential
equations (PDEs) into machine learning models to solve inverse problems, such
as estimating coefficient functions (e.g., the Hamiltonian function) that
characterize physical systems. This framework enables data-driven understanding
and prediction of complex physical phenomena. While coefficient functions in
PIML are typically estimated on the basis of predictive performance, physics as
a discipline does not rely solely on prediction accuracy to evaluate models.
For example, Kepler's heliocentric model was favored owing to small
discrepancies in planetary motion, despite its similar predictive accuracy to
the geocentric model. This highlights the inherent uncertainties in data-driven
model inference and the scientific importance of selecting physically
meaningful solutions. In this paper, we propose a framework to quantify and
analyze such uncertainties in the estimation of coefficient functions in PIML.
We apply our framework to reduced model of magnetohydrodynamics and our
framework shows that there are uncertainties, and unique identification is
possible with geometric constraints. Finally, we confirm that we can estimate
the reduced model uniquely by incorporating these constraints.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [224] [evomap: A Toolbox for Dynamic Mapping in Python](https://arxiv.org/abs/2511.04611)
*Maximilian Matthe*

Main category: cs.MS

TL;DR: 本文介绍了Python包evomap用于动态映射，弥补现有统计软件仅支持静态映射的不足，阐述其基础、架构功能并举例说明应用。


<details>
  <summary>Details</summary>
Motivation: 现有多数统计软件仅支持静态映射，缺乏分析对象关系演变的工具，需要开发动态映射工具。

Method: 实现由Matthe等人提出的动态映射框架EvoMap，支持多种映射技术，提供数据预处理、探索和结果评估等工具。

Result: 开发出evomap包，包含多种映射技术和实用工具，可用于动态映射应用。

Conclusion: evomap为动态映射提供了全面的工具包，可有效开展动态映射分析。

Abstract: This paper presents evomap, a Python package for dynamic mapping. Mapping
methods are widely used across disciplines to visualize relationships among
objects as spatial representations, or maps. However, most existing statistical
software supports only static mapping, which captures objects' relationships at
a single point in time and lacks tools to analyze how these relationships
evolve. evomap fills this gap by implementing the dynamic mapping framework
EvoMap, originally proposed by Matthe, Ringel, and Skiera (2023), which adapts
traditional static mapping methods for dynamic analyses. The package supports
multiple mapping techniques, including variants of Multidimensional Scaling
(MDS), Sammon Mapping, and t-distributed Stochastic Neighbor Embedding (t-SNE).
It also includes utilities for data preprocessing, exploration, and result
evaluation, offering a comprehensive toolkit for dynamic mapping applications.
This paper outlines the foundations of static and dynamic mapping, describes
the architecture and functionality of evomap, and illustrates its application
through an extensive usage example.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [225] [Dark Energy Survey Year 3 results: Simulation-based $w$CDM inference from weak lensing and galaxy clustering maps with deep learning. I. Analysis design](https://arxiv.org/abs/2511.04681)
*A. Thomsen,J. Bucko,T. Kacprzak,V. Ajani,J. Fluri,A. Refregier,D. Anbajagane,F. J. Castander,A. Ferté,M. Gatti,N. Jeffrey,A. Alarcon,A. Amon,K. Bechtol,M. R. Becker,G. M. Bernstein,A. Campos,A. Carnero Rosell,C. Chang,R. Chen,A. Choi,M. Crocce,C. Davis,J. DeRose,S. Dodelson,C. Doux,K. Eckert,J. Elvin-Poole,S. Everett,P. Fosalba,D. Gruen,I. Harrison,K. Herner,E. M. Huff,M. Jarvis,N. Kuropatkin,P. -F. Leget,N. MacCrann,J. McCullough,J. Myles,A. Navarro-Alsina,S. Pandey,A. Porredon,J. Prat,M. Raveri,M. Rodriguez-Monroy,R. P. Rollins,A. Roodman,E. S. Rykoff,C. Sánchez,L. F. Secco,E. Sheldon,T. Shin,M. A. Troxel,I. Tutusaus,T. N. Varga,N. Weaverdyck,R. H. Wechsler,B. Yanny,B. Yin,Y. Zhang,J. Zuntz,S. Allam,F. Andrade-Oliveira,D. Bacon,J. Blazek,D. Brooks,R. Camilleri,J. Carretero,R. Cawthon,L. N. da Costa,M. E. da Silva Pereira,T. M. Davis,J. De Vicente,S. Desai,P. Doel,J. García-Bellido,G. Gutierrez,S. R. Hinton,D. L. Hollowood,K. Honscheid,D. J. James,K. Kuehn,O. Lahav,S. Lee,J. L. Marshall,J. Mena-Fernández,F. Menanteau,R. Miquel,J. Muir,R. L. C. Ogando,A. A. Plazas Malagón,E. Sanchez,D. Sanchez Cid,I. Sevilla-Noarbe,M. Smith,E. Suchyta,M. E. C. Swanson,D. Thomas,C. To,D. L. Tucker*

Main category: astro-ph.CO

TL;DR: 本文提出结合弱引力透镜和星系聚类图的模拟推理管道，经训练和验证后，在宇宙学参数约束上有显著提升，展示了深度学习驱动的SBI分析潜力。


<details>
  <summary>Details</summary>
Motivation: 用深度学习的数据驱动方法从宇宙大尺度结构中提取非高斯信息，为即将到来的调查数据分析做准备。

Method: 开发基于CosmoGridV1的可扩展正向模型生成模拟，用深度图卷积神经网络学习低维特征，通过归一化流进行神经密度估计，并用合成观测验证推理管道。

Result: 在宇宙学参数约束上有显著改进，在Ω_m - S_8平面上优值比基线两点统计高2 - 3倍，有效打破参数简并。

Conclusion: 深度学习驱动的SBI分析对即将到来的IV阶段宽场成像调查有应用潜力。

Abstract: Data-driven approaches using deep learning are emerging as powerful
techniques to extract non-Gaussian information from cosmological large-scale
structure. This work presents the first simulation-based inference (SBI)
pipeline that combines weak lensing and galaxy clustering maps in a realistic
Dark Energy Survey Year 3 (DES Y3) configuration and serves as preparation for
a forthcoming analysis of the survey data. We develop a scalable forward model
based on the CosmoGridV1 suite of N-body simulations to generate over one
million self-consistent mock realizations of DES Y3 at the map level.
Leveraging this large dataset, we train deep graph convolutional neural
networks on the full survey footprint in spherical geometry to learn
low-dimensional features that approximately maximize mutual information with
target parameters. These learned compressions enable neural density estimation
of the implicit likelihood via normalizing flows in a ten-dimensional parameter
space spanning cosmological $w$CDM, intrinsic alignment, and linear galaxy bias
parameters, while marginalizing over baryonic, photometric redshift, and shear
bias nuisances. To ensure robustness, we extensively validate our inference
pipeline using synthetic observations derived from both systematic
contaminations in our forward model and independent Buzzard galaxy catalogs.
Our forecasts yield significant improvements in cosmological parameter
constraints, achieving $2-3\times$ higher figures of merit in the $\Omega_m -
S_8$ plane relative to our implementation of baseline two-point statistics and
effectively breaking parameter degeneracies through probe combination. These
results demonstrate the potential of SBI analyses powered by deep learning for
upcoming Stage-IV wide-field imaging surveys.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [226] [Machine Learning for Electron-Scale Turbulence Modeling in W7-X](https://arxiv.org/abs/2511.04567)
*Ionut-Gabriel Farcas,Don Lawrence Carl Agapito Fernando,Alejandro Banon Navarro,Gabriele Merlo,Frank Jenko*

Main category: physics.plasm-ph

TL;DR: 本文提出机器学习驱动的简化模型预测Wendelstein 7 - X仿星器中电子温度梯度（ETG）湍流的热通量，模型表现良好，预测准确。


<details>
  <summary>Details</summary>
Motivation: 构建湍流输运简化模型对加速轮廓预测和执行多查询任务很重要，本文旨在为W7 - X仿星器的ETG湍流构建简化模型。

Method: 首先使用回归和基于主动机器学习的程序，用低基数稀疏网格训练数据初始化模型，再从现有模拟数据库中选择信息最丰富的点迭代完善训练集；通过自助法估计95%预测区间评估预测不确定性；研究构建通用简化模型。

Result: 模型在样本外数据集上评估表现良好，即使应用于训练域之外，也有与原始参考模拟相当的预测准确性。

Conclusion: 所构建的机器学习驱动简化模型具有稳健性能和较高预测准确性，可用于ETG湍流的热通量预测。

Abstract: Constructing reduced models for turbulent transport is essential for
accelerating profile predictions and enabling many-query tasks such as
uncertainty quantification, parameter scans, and design optimization. This
paper presents machine-learning-driven reduced models for Electron Temperature
Gradient (ETG) turbulence in the Wendelstein 7-X (W7-X) stellarator. Each model
predicts the ETG heat flux as a function of three plasma parameters: the
normalized electron temperature radial gradient ($\omega_{T_e}$), the ratio of
normalized electron temperature and density radial gradients ($\eta_e$), and
the electron-to-ion temperature ratio ($\tau$). We first construct models
across seven radial locations using regression and an active
machine-learning-based procedure. This process initializes models using
low-cardinality sparse-grid training data and then iteratively refines their
training sets by selecting the most informative points from a pre-existing
simulation database. We evaluate the prediction capabilities of our models
using out-of-sample datasets with over $393$ points per location, and $95\%$
prediction intervals are estimated via bootstrapping to assess prediction
uncertainty. We then investigate the construction of generalized reduced
models, including a generic, position-independent model, and assess their heat
flux prediction capabilities at three additional locations. Our models
demonstrate robust performance and predictive accuracy comparable to the
original reference simulations, even when applied beyond the training domain.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [227] [An LLM-based Framework for Human-Swarm Teaming Cognition in Disaster Search and Rescue](https://arxiv.org/abs/2511.04042)
*Kailun Ji,Xiaoyu Hu,Xinyu Zhang,Jun Chen*

Main category: cs.RO

TL;DR: 本文提出LLM - CRF系统以解决大规模灾难搜救中无人机群协作问题，实验显示该方法能提升效率、成功率并降低认知负担。


<details>
  <summary>Details</summary>
Motivation: 大规模灾难搜救中无人机群有效协作给人类操作员带来巨大认知负担，存在‘意图 - 行动差距’。

Method: 提出LLM - CRF系统，通过自然和多模态交互捕捉操作员意图，用大语言模型进行意图理解、任务分解和任务规划。

Result: 在模拟搜救场景实验中，与传统界面相比，该方法减少约64.2%的任务完成时间，提高7%的任务成功率，NASA - TLX分数下降42.9%。

Conclusion: 大语言模型在高风险场景中创建更直观有效的人 - 群协作具有潜力。

Abstract: Large-scale disaster Search And Rescue (SAR) operations are persistently
challenged by complex terrain and disrupted communications. While Unmanned
Aerial Vehicle (UAV) swarms offer a promising solution for tasks like wide-area
search and supply delivery, yet their effective coordination places a
significant cognitive burden on human operators. The core human-machine
collaboration bottleneck lies in the ``intention-to-action gap'', which is an
error-prone process of translating a high-level rescue objective into a
low-level swarm command under high intensity and pressure. To bridge this gap,
this study proposes a novel LLM-CRF system that leverages Large Language Models
(LLMs) to model and augment human-swarm teaming cognition. The proposed
framework initially captures the operator's intention through natural and
multi-modal interactions with the device via voice or graphical annotations. It
then employs the LLM as a cognitive engine to perform intention comprehension,
hierarchical task decomposition, and mission planning for the UAV swarm. This
closed-loop framework enables the swarm to act as a proactive partner,
providing active feedback in real-time while reducing the need for manual
monitoring and control, which considerably advances the efficacy of the SAR
task. We evaluate the proposed framework in a simulated SAR scenario.
Experimental results demonstrate that, compared to traditional order and
command-based interfaces, the proposed LLM-driven approach reduced task
completion time by approximately $64.2\%$ and improved task success rate by
$7\%$. It also leads to a considerable reduction in subjective cognitive
workload, with NASA-TLX scores dropping by $42.9\%$. This work establishes the
potential of LLMs to create more intuitive and effective human-swarm
collaborations in high-stakes scenarios.

</details>


### [228] [X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations](https://arxiv.org/abs/2511.04671)
*Maximus A. Pace,Prithwish Dan,Chuanruo Ning,Atiksh Bhardwaj,Audrey Du,Edward W. Duan,Wei-Chiu Ma,Kushal Kedia*

Main category: cs.RO

TL;DR: 本文提出X - Diffusion框架，利用正向扩散过程，最大程度利用人类数据训练机器人策略，避免学习不可行运动，实验表明其能提升策略性能。


<details>
  <summary>Details</summary>
Motivation: 人类视频是机器人学习的优质训练数据，但人与机器人在具身性上有差异，直接运动重定向会产生机器人无法执行的动作，需有效利用人类演示中的运动线索。

Method: 提出X - Diffusion框架，先训练分类器预测噪声动作执行者，添加足够噪声使分类器无法区分后将人类动作纳入策略训练，不同噪声水平下人类和机器人动作提供不同指导。

Result: 实验显示，执行不匹配时直接共同训练会降低策略性能，X - Diffusion能持续提升性能，在五项操作任务中平均成功率比最佳基线高16%。

Conclusion: X - Diffusion框架能有效利用人类数据训练机器人策略，避免学习不可行运动，提升策略性能。

Abstract: Human videos can be recorded quickly and at scale, making them an appealing
source of training data for robot learning. However, humans and robots differ
fundamentally in embodiment, resulting in mismatched action execution. Direct
kinematic retargeting of human hand motion can therefore produce actions that
are physically infeasible for robots. Despite these low-level differences,
human demonstrations provide valuable motion cues about how to manipulate and
interact with objects. Our key idea is to exploit the forward diffusion
process: as noise is added to actions, low-level execution differences fade
while high-level task guidance is preserved. We present X-Diffusion, a
principled framework for training diffusion policies that maximally leverages
human data without learning dynamically infeasible motions. X-Diffusion first
trains a classifier to predict whether a noisy action is executed by a human or
robot. Then, a human action is incorporated into policy training only after
adding sufficient noise such that the classifier cannot discern its embodiment.
Actions consistent with robot execution supervise fine-grained denoising at low
noise levels, while mismatched human actions provide only coarse guidance at
higher noise levels. Our experiments show that naive co-training under
execution mismatches degrades policy performance, while X-Diffusion
consistently improves it. Across five manipulation tasks, X-Diffusion achieves
a 16% higher average success rate than the best baseline. The project website
is available at https://portal-cornell.github.io/X-Diffusion/.

</details>


### [229] [Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions](https://arxiv.org/abs/2511.04665)
*Kaifeng Zhang,Shuo Sha,Hanxiao Jiang,Matthew Loper,Hyunjong Song,Guangyan Cai,Zhuo Xu,Xiaochen Hu,Changxi Zheng,Yunzhu Li*

Main category: cs.RO

TL;DR: 提出真实到仿真的策略评估框架，可构建软体数字孪生体并实现逼真渲染，在代表性任务验证，能准确评估机器人操作策略。


<details>
  <summary>Details</summary>
Motivation: 真实世界评估机器人操作策略成本高、耗时长且难复现，现有模拟器难以捕捉软体交互视觉和物理复杂性。

Method: 构建从真实世界视频生成软体数字孪生体的框架，用3D高斯溅射实现逼真渲染。

Result: 在代表性可变形操作任务验证，模拟结果与真实执行性能强相关，能揭示学习策略关键行为模式。

Conclusion: 结合物理信息重建和高质量渲染可实现机器人操作策略可复现、可扩展和准确评估。

Abstract: Robotic manipulation policies are advancing rapidly, but their direct
evaluation in the real world remains costly, time-consuming, and difficult to
reproduce, particularly for tasks involving deformable objects. Simulation
provides a scalable and systematic alternative, yet existing simulators often
fail to capture the coupled visual and physical complexity of soft-body
interactions. We present a real-to-sim policy evaluation framework that
constructs soft-body digital twins from real-world videos and renders robots,
objects, and environments with photorealistic fidelity using 3D Gaussian
Splatting. We validate our approach on representative deformable manipulation
tasks, including plush toy packing, rope routing, and T-block pushing,
demonstrating that simulated rollouts correlate strongly with real-world
execution performance and reveal key behavioral patterns of learned policies.
Our results suggest that combining physics-informed reconstruction with
high-quality rendering enables reproducible, scalable, and accurate evaluation
of robotic manipulation policies. Website: https://real2sim-eval.github.io/

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [230] [Two Decades of Research at the University of Lagos (2004-2023): A Scientometric Analysis of Productivity, Collaboration, and Impact](https://arxiv.org/abs/2511.04075)
*Muneer Ahmad,Samuel Ibor Ubi*

Main category: cs.DL

TL;DR: 对拉各斯大学2004 - 2023年研究产出进行科学计量分析，揭示研究趋势与表现。


<details>
  <summary>Details</summary>
Motivation: 了解拉各斯大学过去二十年的研究表现，为战略规划和政策制定提供基础。

Method: 利用Web of Science的文献计量数据，分析出版物数量、合作模式、引文影响等。

Result: 研究产出持续增加，2023年最高；健康科学、工程和社会科学是主导领域；本地和国际合作与高引文影响正相关；开放获取出版物占比大，提升可见性和引文率。

Conclusion: 研究结果为促进研究卓越和全球影响力的战略规划和政策制定提供依据。

Abstract: This paper presents a scientometric analysis of research output from the
University of Lagos, focusing on the two decades spanning 2004 to 2023. Using
bibliometric data retrieved from the Web of Science, we examine trends in
publication volume, collaboration patterns, citation impact, and the most
prolific authors, departments, and research domains at the university. The
study reveals a consistent increase in research productivity, with the highest
publication output recorded in 2023. Health Sciences, Engineering, and Social
Sciences are identified as dominant fields, reflecting the university's
interdisciplinary research strengths. Collaborative efforts, both locally and
internationally, show a positive correlation with higher citation impact, with
the United States and the United Kingdom being the leading international
collaborators. Notably, open-access publications account for a significant
portion of the university's research output, enhancing visibility and citation
rates. The findings offer valuable insights into the university's research
performance over the past two decades, providing a foundation for strategic
planning and policy formulation to foster research excellence and global
impact.

</details>


### [231] [Publication Trend in DESIDOC Journal of Library and Information Technology during 2013-2017: A Scientometric Approach](https://arxiv.org/abs/2511.04082)
*M Sadik Batcha,S Roselin Jahina,Muneer Ahmad*

Main category: cs.DL

TL;DR: 对DESIDOC期刊进行科学计量分析，涵盖研究产出增长模式、作者模式等，得出相关结果。


<details>
  <summary>Details</summary>
Motivation: 对DESIDOC期刊进行科学计量分析，了解其研究产出、作者等方面情况。

Method: 应用标准公式和统计工具。

Result: 2001 - 2012年发表227篇论文，多数文章为合作性质，期刊主题集中在科学计量学，65%文章篇幅在6 - 10页。

Conclusion: 未明确提及结论性内容。

Abstract: DESIDOC Journal of Library & Information Technology (DJLIT) formerly known as
DESIDOC Bulletin of Information Technology is a peer-reviewed, open access,
bimonthly journal. This paper presents a Scientometric analysis of the DESIDOC
Journal. The paper analyses the pattern of growth of the research output
published in the journal, pattern of authorship, author productivity, and,
subjects covered to the papers over the period (2013-2017). It is found that
227 papers were published during the period of study (2001-2012). The maximum
numbers of articles were collaborative in nature. The subject concentration of
the journal noted is Scientometrics. The maximum numbers of articles (65%) have
ranged their thought contents between 6 and 10 pages. The study applied
standard formula and statistical tools to bring out the factual result.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [232] [Dynamic causal discovery in Alzheimer's disease through latent pseudotime modelling](https://arxiv.org/abs/2511.04619)
*Natalia Glazman,Jyoti Mangal,Pedro Borges,Sebastien Ourselin,M. Jorge Cardoso*

Main category: stat.AP

TL;DR: 提出用潜在变量模型处理阿尔茨海默病数据，推断伪时间以学习因果关系演变，伪时间预测诊断效果好，结合背景知识提升图准确性。


<details>
  <summary>Details</summary>
Motivation: 多数因果发现方法的静态图假设限制其在阿尔茨海默病应用，无法解释由潜在疾病伪时间调节的病理生理变化。

Method: 将现有潜在变量模型应用于真实阿尔茨海默病数据，推断伪时间对患者排序，学习因果关系演变，并结合少量疾病无关背景知识。

Result: 伪时间在预测诊断上优于年龄（AUC 0.82 vs 0.59），结合背景知识大幅提高图的准确性和方向性。

Conclusion: 该框架揭示了新老阿尔茨海默病标志物间的动态相互作用，即便假设不成立也能实现实用的因果发现。

Abstract: The application of causal discovery to diseases like Alzheimer's (AD) is
limited by the static graph assumptions of most methods; such models cannot
account for an evolving pathophysiology, modulated by a latent disease
pseudotime. We propose to apply an existing latent variable model to real-world
AD data, inferring a pseudotime that orders patients along a data-driven
disease trajectory independent of chronological age, then learning how causal
relationships evolve. Pseudotime outperformed age in predicting diagnosis (AUC
0.82 vs 0.59). Incorporating minimal, disease-agnostic background knowledge
substantially improved graph accuracy and orientation. Our framework reveals
dynamic interactions between novel (NfL, GFAP) and established AD markers,
enabling practical causal discovery despite violated assumptions.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [233] [Why Consciousness Should Explain Physical Phenomena: Toward a Testable Theory](https://arxiv.org/abs/2511.04047)
*Yoshiyuki Ohmura,Yasuo Kuniyoshi*

Main category: q-bio.NC

TL;DR: 论文指出还原论假设存在问题，需摒弃层级内因果封闭假设，提出用双层次定律模型和建构方法解释心理与物理现象。


<details>
  <summary>Details</summary>
Motivation: 科学方法中还原论假设认为宏观和微观现象仅能用微观定律解释，使宏观现象成为附带现象，而意识的整合性表明其为宏观现象，为保证科学可检验性和拒绝附带现象论，需摒弃层级内因果封闭假设。

Method: 将大脑建模为在不同层次受双定律运行，包含不由微观神经定律单独决定的宏观心理定律以及宏观到微观的因果效应，用双定律间的相互作用来解释心理和物理现象。

Result: 提出了一种建设性方法。

Conclusion: 需要一种新的方法论来承认宏观现象的因果效力，通过双定律相互作用解释心理和物理现象。

Abstract: The reductionist approach commonly employed in scientific methods presupposes
that both macro and micro phenomena can be explained by micro-level laws alone.
This assumption implies intra-level causal closure, rendering all macro
phenomena epiphenomenal. However, the integrative nature of consciousness
suggests that it is a macro phenomenon. To ensure scientific testability and
reject epiphenomenalism, the reductionist assumption of intra-level causal
closure must be rejected. This implies that even neural-level behavior cannot
be explained by observable neural-level laws alone. Therefore, a new
methodology is necessary to acknowledge the causal efficacy of macro-level
phenomena. We model the brain as operating under dual laws at different levels.
This model includes hypothetical macro-level psychological laws that are not
determined solely by micro-level neural laws, as well as the causal effects
from macro to micro levels. In this study, we propose a constructive approach
that explains both mental and physical phenomena through the interaction
between these two sets of laws.

</details>


### [234] [Unified Generative Latent Representation for Functional Brain Graphs](https://arxiv.org/abs/2511.04539)
*Subati Abulikemu,Tiago Azevedo,Michail Mamalakis,John Suckling*

Main category: q-bio.NC

TL;DR: 提出一种统一图表示方法，用图变压器自编码器生成功能脑图，能区分工作记忆状态、解码视觉刺激并生成合成图。


<details>
  <summary>Details</summary>
Motivation: 以往功能脑图用分离的图论或光谱描述符，忽略了属性间的协变和重叠，期望探索其低维潜在几何结构。

Method: 通过带潜在扩散的图变压器自编码器估计统一图表示，利用光谱几何提供归纳偏置引导学习。

Result: 无监督的潜在表示能区分工作记忆状态、解码视觉刺激，结合神经动力学可提升性能，还能从扩散模型分布中采样合成图。

Conclusion: 所提出的方法有效，能实现对功能脑图的多种分析和生成。

Abstract: Functional brain graphs are often characterized with separate graph-theoretic
or spectral descriptors, overlooking how these properties covary and partially
overlap across brains and conditions. We anticipate that dense, weighted
functional connectivity graphs occupy a low-dimensional latent geometry along
which both topological and spectral structures display graded variations. Here,
we estimated this unified graph representation and enabled generation of dense
functional brain graphs through a graph transformer autoencoder with latent
diffusion, with spectral geometry providing an inductive bias to guide
learning. This geometry-aware latent representation, although unsupervised,
meaningfully separated working-memory states and decoded visual stimuli, with
performance further enhanced by incorporating neural dynamics. From the
diffusion modeled distribution, we were able to sample biologically plausible
and structurally grounded synthetic dense graphs.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [235] [Deep Learning-Driven Downscaling for Climate Risk Assessment of Projected Temperature Extremes in the Nordic Region](https://arxiv.org/abs/2511.03770)
*Parthiban Loganathan,Elias Zea,Ricardo Vinuesa,Evelyn Otero*

Main category: physics.geo-ph

TL;DR: 本文提出综合降尺度框架预测北欧气温，评估后发现ViT表现好，预测特定气候区升温情况，表明需适应措施。


<details>
  <summary>Details</summary>
Motivation: 北欧气候变化快、气候变率增加，区域规划需要高分辨率的预计温度。

Method: 提出包含ViT、ConvLSTM和GeoStaNet模型的综合降尺度框架，用DL - TOPSIS评估，对NorESM2 - LM CMIP6输出进行偏差校正和验证。

Result: ViT表现良好，能产生可信降尺度预测；在SSP5 - 8.5情景下，特定气候区有升温及日温度范围变化，时间信号在亚北极冬季出现。

Conclusion: 该框架可提供基于站点的高分辨率不确定性和极端情况估计，可用于高纬度快速环境变化地区的适应政策。

Abstract: Rapid changes and increasing climatic variability across the widely varied
Koppen-Geiger regions of northern Europe generate significant needs for
adaptation. Regional planning needs high-resolution projected temperatures.
This work presents an integrative downscaling framework that incorporates
Vision Transformer (ViT), Convolutional Long Short-Term Memory (ConvLSTM), and
Geospatial Spatiotemporal Transformer with Attention and Imbalance-Aware
Network (GeoStaNet) models. The framework is evaluated with a multicriteria
decision system, Deep Learning-TOPSIS (DL-TOPSIS), for ten strategically chosen
meteorological stations encompassing the temperate oceanic (Cfb), subpolar
oceanic (Cfc), warm-summer continental (Dfb), and subarctic (Dfc) climate
regions. Norwegian Earth System Model (NorESM2-LM) Coupled Model
Intercomparison Project Phase 6 (CMIP6) outputs were bias-corrected during the
1951-2014 period and subsequently validated against earlier observations of
day-to-day temperature metrics and diurnal range statistics. The ViT showed
improved performance (Root Mean Squared Error (RMSE): 1.01 degrees C; R^2:
0.92), allowing for production of credible downscaled projections. Under the
SSP5-8.5 scenario, the Dfc and Dfb climate zones are projected to warm by 4.8
degrees C and 3.9 degrees C, respectively, by 2100, with expansion in the
diurnal temperature range by more than 1.5 degrees C. The Time of Emergence
signal first appears in subarctic winter seasons (Dfc: approximately 2032),
signifying an urgent need for adaptation measures. The presented framework
offers station-based, high-resolution estimates of uncertainties and extremes,
with direct uses for adaptation policy over high-latitude regions with fast
environmental change.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [236] [OpenMENA: An Open-Source Memristor Interfacing and Compute Board for Neuromorphic Edge-AI Applications](https://arxiv.org/abs/2511.03747)
*Ali Safa,Farida Mohsen,Zainab Ali,Bo Wang,Amine Bermak*

Main category: cs.ET

TL;DR: 本文介绍首个全开源忆阻器接口系统Open - MENA，验证其在数字识别和机器人避障任务中的应用并开源。


<details>
  <summary>Details</summary>
Motivation: 忆阻器交叉阵列可实现内存内乘积累加和局部可塑性学习，为节能边缘AI提供途径，需要开发相应系统。

Method: 提出Open - MENA系统，集成硬件接口、固件 - 软件栈和VIPI编程方法，进行芯片在环微调。

Result: 在数字识别和机器人避障任务中验证了从权重转移到设备自适应的流程。

Conclusion: Open - MENA系统可推动忆阻器边缘AI研究，已开源。

Abstract: Memristive crossbars enable in-memory multiply-accumulate and local
plasticity learning, offering a path to energy-efficient edge AI. To this end,
we present Open-MENA (Open Memristor-in-Memory Accelerator), which, to our
knowledge, is the first fully open memristor interfacing system integrating (i)
a reproducible hardware interface for memristor crossbars with mixed-signal
read-program-verify loops; (ii) a firmware-software stack with high-level APIs
for inference and on-device learning; and (iii) a Voltage-Incremental
Proportional-Integral (VIPI) method to program pre-trained weights into analog
conductances, followed by chip-in-the-loop fine-tuning to mitigate device
non-idealities. OpenMENA is validated on digit recognition, demonstrating the
flow from weight transfer to on-device adaptation, and on a real-world robot
obstacle-avoidance task, where the memristor-based model learns to map
localization inputs to motor commands. OpenMENA is released as open source to
democratize memristor-enabled edge-AI research.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [237] [AIM: Software and Hardware Co-design for Architecture-level IR-drop Mitigation in High-performance PIM](https://arxiv.org/abs/2511.04321)
*Yuanpeng Zhang,Xing Hu,Xi Chen,Zhihang Yuan,Cong Li,Jingchen Zhu,Zhao Wang,Chenguang Zhang,Xin Si,Wei Gao,Qiang Wu,Runsheng Wang,Guangyu Sun*

Main category: cs.AR

TL;DR: 提出AIM用于高性能PIM架构级IR-drop缓解，经7nm 256 - TOPS PIM芯片后仿，可减轻69.2% IR-drop，提升能效和速度。


<details>
  <summary>Details</summary>
Motivation: 高性能SRAM PIM追求更高性能时IR - drop问题加剧，传统方法资源消耗大且影响PPA。

Method: 先引入Rtog和HR建立PIM工作负载与IR - drop关联，再提出LHR和WDS进行架构级缓解，开发IR - Booster动态调整，最后提出HR感知任务映射方法。

Result: 7nm 256 - TOPS PIM芯片后仿显示，AIM减轻69.2% IR - drop，能效提升2.29倍，速度提升1.152倍。

Conclusion: AIM能有效缓解高性能PIM的IR - drop问题，提升能效和性能。

Abstract: SRAM Processing-in-Memory (PIM) has emerged as the most promising
implementation for high-performance PIM, delivering superior computing density,
energy efficiency, and computational precision. However, the pursuit of higher
performance necessitates more complex circuit designs and increased operating
frequencies, which exacerbate IR-drop issues. Severe IR-drop can significantly
degrade chip performance and even threaten reliability. Conventional
circuit-level IR-drop mitigation methods, such as back-end optimizations, are
resource-intensive and often compromise power, performance, and area (PPA). To
address these challenges, we propose AIM, comprehensive software and hardware
co-design for architecture-level IR-drop mitigation in high-performance PIM.
Initially, leveraging the bit-serial and in-situ dataflow processing properties
of PIM, we introduce Rtog and HR, which establish a direct correlation between
PIM workloads and IR-drop. Building on this foundation, we propose LHR and WDS,
enabling extensive exploration of architecture-level IR-drop mitigation while
maintaining computational accuracy through software optimization. Subsequently,
we develop IR-Booster, a dynamic adjustment mechanism that integrates
software-level HR information with hardware-based IR-drop monitoring to adapt
the V-f pairs of the PIM macro, achieving enhanced energy efficiency and
performance. Finally, we propose the HR-aware task mapping method, bridging
software and hardware designs to achieve optimal improvement. Post-layout
simulation results on a 7nm 256-TOPS PIM chip demonstrate that AIM achieves up
to 69.2% IR-drop mitigation, resulting in 2.29x energy efficiency improvement
and 1.152x speedup.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [238] [On the relationship between MESP and 0/1 D-Opt and their upper bounds](https://arxiv.org/abs/2511.04350)
*Gabriel Ponte,Marcia Fampa,Jon Lee*

Main category: math.OC

TL;DR: 建立实验设计中最大熵采样和0/1 D - 最优性两个非线性0/1优化问题的联系，转移上界方法，建立新结果并比较分支定界方案，有意外数值结果。


<details>
  <summary>Details</summary>
Motivation: 探索实验设计领域两个基本非线性0/1优化问题之间的关系。

Method: 通过实例间的映射来建立联系，分析映射行为，并基于此转移上界方法。

Result: 建立新的支配结果和不等式，比较不同分支定界方案，发现对实际数据MESP实例无前景的边界方法对来自0/1 D - 最优性的MESP实例有用。

Conclusion: 两个问题之间存在强联系，基于映射的方法有助于解决相关优化问题。

Abstract: We establish strong connections between two fundamental nonlinear 0/1
optimization problems coming from the area of experimental design, namely
maximum entropy sampling and 0/1 D-Optimality. The connections are based on
maps between instances, and we analyze the behavior of these maps. Using these
maps, we transport basic upper-bounding methods between these two problems, and
we are able to establish new domination results and other inequalities relating
various basic upper bounds. Further, we establish results relating how
different branch-and-bound schemes based on these maps compare. Additionally,
we observe some surprising numerical results, where bounding methods that did
not seem promising in their direct application to real-data MESP instances, are
now useful for MESP instances that come from 0/1 D-Optimality.

</details>


### [239] [ODE approximation for the Adam algorithm: General and overparametrized setting](https://arxiv.org/abs/2511.04622)
*Steffen Dereich,Arnulf Jentzen,Sebastian Kassing*

Main category: math.OC

TL;DR: 本文用基于ODE的方法研究快 - 慢缩放机制下的Adam优化器，建立收敛结果，指出其收敛特性在不同场景的差异。


<details>
  <summary>Details</summary>
Motivation: 研究Adam优化器在快 - 慢缩放机制下的特性与收敛情况。

Method: 采用基于ODE的方法研究Adam优化器，利用渐近伪轨迹的性质。

Result: 在一般情况下，若Adam算法收敛，极限是Adam向量场的零点；在过参数化经验风险最小化场景，能局部找到极小值集。

Conclusion: Adam算法的收敛特性在不同场景有所不同，在过参数化场景有良好表现。

Abstract: The Adam optimizer is currently presumably the most popular optimization
method in deep learning. In this article we develop an ODE based method to
study the Adam optimizer in a fast-slow scaling regime. For fixed momentum
parameters and vanishing step-sizes, we show that the Adam algorithm is an
asymptotic pseudo-trajectory of the flow of a particular vector field, which is
referred to as the Adam vector field. Leveraging properties of asymptotic
pseudo-trajectories, we establish convergence results for the Adam algorithm.
In particular, in a very general setting we show that if the Adam algorithm
converges, then the limit must be a zero of the Adam vector field, rather than
a local minimizer or critical point of the objective function.
  In contrast, in the overparametrized empirical risk minimization setting, the
Adam algorithm is able to locally find the set of minima. Specifically, we show
that in a neighborhood of the global minima, the objective function serves as a
Lyapunov function for the flow induced by the Adam vector field. As a
consequence, if the Adam algorithm enters a neighborhood of the global minima
infinitely often, it converges to the set of global minima.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [240] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 本文探索三种多智能体大语言模型管道，对不同大小的开源模型进行性能基准测试，发现多智能体讨论可提升小模型性能，推理 - 编码管道效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在大模式规模和复杂推理下进行SQL生成存在困难，且先前工作多关注复杂不实用管道，忽视小而高效模型。

Method: 探索三种多智能体LLM管道，包括多智能体讨论管道、规划 - 编码管道、编码 - 聚合管道，并对不同大小的开源模型进行系统性能基准测试。

Result: 多智能体讨论可提升小模型性能，如Qwen2.5 - 7b - Instruct执行准确率最高提升10.6%；推理 - 编码管道效果最佳，DeepSeek - R1 - 32B和QwQ - 32B规划器使Gemma 3 27B IT准确率从52.4%提升到56.4%。

Conclusion: 提出的多智能体LLM管道在SQL生成上有一定效果，推理 - 编码管道表现最优，代码已开源。

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [241] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar,Purvi Chaurasia,Sanchit Kabra,Ananya Srivastava,Vivek Gupta,Chandan K. Reddy*

Main category: cs.CL

TL;DR: 现有表格推理基准测试存在局限性，本文引入RUST - BENCH基准测试，实验显示LLM有弱点，该基准为表格推理研究提供新测试平台。


<details>
  <summary>Details</summary>
Motivation: 现有表格推理基准大多在小型统一表格上测试模型，无法反映真实数据复杂性和LLM推理能力全貌，需新基准。

Method: 引入包含来自2031个真实表格的7966个问题的RUST - BENCH基准，涵盖科学和体育两个领域，联合评估LLM在规模、异构性、领域特异性和推理复杂性方面的能力。

Result: 实验表明LLM在异构模式和复杂多跳推理方面存在困难，当前架构和提示策略有持续弱点。

Conclusion: RUST - BENCH为推进表格推理研究建立了具有挑战性的新测试平台。

Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models' (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.

</details>


### [242] [A Characterization of List Language Identification in the Limit](https://arxiv.org/abs/2511.04103)
*Moses Charikar,Chirag Pabbaraju,Ambuj Tewari*

Main category: cs.CL

TL;DR: 本文重新研究经典语言识别问题，允许学习者每次给出k个猜测，给出可k - 列表极限识别的语言集合的精确刻画，并在统计设定下确定列表识别的速率。


<details>
  <summary>Details</summary>
Motivation: 受语言生成相关问题的积极结果启发，重新研究经典语言识别问题，考虑学习者每次可给出k个猜测的情况。

Method: 基于Angluin对列表大小为1的语言识别的刻画的递归版本，给出可k - 列表极限识别的语言集合的刻画；在统计设定下分析识别速率。

Result: 给出可k - 列表极限识别的语言集合的概念性刻画；若集合可k - 列表极限识别，能以指数速率识别，且此为最优；若不可，无法以趋于零的速率识别。

Conclusion: 得到了k - 列表极限识别语言集合的刻画和识别速率相关结论。

Abstract: We study the problem of language identification in the limit, where given a
sequence of examples from a target language, the goal of the learner is to
output a sequence of guesses for the target language such that all the guesses
beyond some finite time are correct. Classical results of Gold showed that
language identification in the limit is impossible for essentially any
interesting collection of languages. Later, Angluin gave a precise
characterization of language collections for which this task is possible.
Motivated by recent positive results for the related problem of language
generation, we revisit the classic language identification problem in the
setting where the learner is given the additional power of producing a list of
$k$ guesses at each time step. The goal is to ensure that beyond some finite
time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be
$k$-list identified in the limit, based on a recursive version of Angluin's
characterization (for language identification with a list of size $1$). This
further leads to a conceptually appealing characterization: A language
collection can be $k$-list identified in the limit if and only if the
collection can be decomposed into $k$ collections of languages, each of which
can be identified in the limit (with a list of size $1$). We also use our
characterization to establish rates for list identification in the statistical
setting where the input is drawn as an i.i.d. stream from a distribution
supported on some language in the collection. Our results show that if a
collection is $k$-list identifiable in the limit, then the collection can be
$k$-list identified at an exponential rate, and this is best possible. On the
other hand, if a collection is not $k$-list identifiable in the limit, then it
cannot be $k$-list identified at any rate that goes to zero.

</details>


### [243] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

TL;DR: 本文介绍了专为波兰语定制的最大开源基础模型家族PLLuM，阐述其开发过程、架构等，旨在促进开放研究和增强波兰主权AI技术。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型主要聚焦英语，对其他语言支持有限，需要为波兰语开发高质量、透明且与文化相关的语言模型。

Method: 构建新的1400亿标记的波兰语文本语料库用于预训练，创建7.7万条自定义指令数据集和10万条偏好优化数据集，采用负责任AI框架，包含严格数据治理和混合模块进行输出校正与安全过滤。

Result: 详细介绍了模型架构、训练程序和对齐技术，并在公共管理下游任务中展示了其效用。

Conclusion: 通过公开发布模型，PLLuM旨在促进开放研究和增强波兰主权AI技术。

Abstract: Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [244] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 论文提出通过向量翻译形成潜在桥梁实现大语言模型跨模型潜在通信，避免信息传递受限和计算开销问题，验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 多智能体场景下大语言模型以普通令牌传递消息，丢弃潜在语义，限制信息传递并增加计算开销。

Method: 通过向量翻译形成潜在桥梁，使用学习映射实现表示空间间直接语义交换，训练双编码器翻译器。

Result: 在Llama - 2 - 7B和Mistral - 7B - Instruct间训练的翻译器平均余弦对齐度达0.538；以30%混合强度注入翻译向量可引导目标模型生成且不破坏对数；双向评估显示2.01:1的转移不对称性。

Conclusion: 保守注入可保持计算稳定性，表明跨模型潜在通信可行，能使协作AI系统共享语义而非令牌。

Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [245] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

TL;DR: 本文提出将溯因推理集成到检索增强大语言模型中的框架，实验表明该方法能提升答案准确性和推理可信度。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）管道在检索证据不完整时推理常失败，需要一种方法弥补推理过程中的差距。

Method: 检测证据不足，生成候选缺失前提，并通过一致性和合理性检查进行验证。

Result: 在溯因推理和多跳问答基准测试中，该方法提高了答案准确性和推理可信度。

Conclusion: 溯因推理是增强RAG系统鲁棒性和可解释性的有前景方向。

Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [246] [Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains](https://arxiv.org/abs/2511.04184)
*Mohammed Musthafa Rafi,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CL

TL;DR: AI生成内容导致交流失真，LAAC提出将大语言模型作为通信中介促进真实交流，但部署需评估可信度，研究发现存在信任差距。


<details>
  <summary>Details</summary>
Motivation: 解决AI生成内容造成的交流问题，实现真实知识交流，评估LAAC在多领域部署的可信度。

Method: 系统评估LAAC在多通信领域部署的可信度，从信息捕获保真度、可重复性、查询响应完整性三个维度，通过多用例的对照实验，利用多智能体架构评估。

Result: 初步发现存在可衡量的信任差距。

Conclusion: 在高风险通信场景可靠部署LAAC前，必须解决信任差距问题。

Abstract: The proliferation of AI-generated content has created an absurd communication
theater where senders use LLMs to inflate simple ideas into verbose content,
recipients use LLMs to compress them back into summaries, and as a consequence
neither party engage with authentic content. LAAC (LLM as a Communicator)
proposes a paradigm shift - positioning LLMs as intelligent communication
intermediaries that capture the sender's intent through structured dialogue and
facilitate genuine knowledge exchange with recipients. Rather than perpetuating
cycles of AI-generated inflation and compression, LAAC enables authentic
communication across diverse contexts including academic papers, proposals,
professional emails, and cross-platform content generation. However, deploying
LLMs as trusted communication intermediaries raises critical questions about
information fidelity, consistency, and reliability. This position paper
systematically evaluates the trustworthiness requirements for LAAC's deployment
across multiple communication domains. We investigate three fundamental
dimensions: (1) Information Capture Fidelity - accuracy of intent extraction
during sender interviews across different communication types, (2)
Reproducibility - consistency of structured knowledge across multiple
interaction instances, and (3) Query Response Integrity - reliability of
recipient-facing responses without hallucination, source conflation, or
fabrication. Through controlled experiments spanning multiple LAAC use cases,
we assess these trust dimensions using LAAC's multi-agent architecture.
Preliminary findings reveal measurable trust gaps that must be addressed before
LAAC can be reliably deployed in high-stakes communication scenarios.

</details>


### [247] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

TL;DR: 提出GRAD方法缓解大语言模型幻觉问题，在多个基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有缓解大语言模型幻觉的方法存在提示脆弱、领域敏感和检索格式化成本高等问题，需要新方法。

Method: 引入Graph - Retrieved Adaptive Decoding (GRAD)，在解码时通过在小检索语料上构建稀疏令牌过渡图，将图检索对数与模型对数自适应融合。

Result: 在三个模型和一系列问答基准测试中，GRAD优于基线，内在准确率提高9.7%，幻觉率降低8.6%，正确性提高6.9%，获得最高真实 - 信息乘积分数。

Conclusion: GRAD是对比解码和知识图谱增强的轻量级即插即用替代方案，语料级令牌过渡的统计证据可引导生成更真实可验证的输出。

Abstract: Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [248] [REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs](https://arxiv.org/abs/2511.04228)
*Liran Cohen,Yaniv Nemcovesky,Avi Mendelson*

Main category: cs.CL

TL;DR: 提出REMIND评估方法检测模型对数据遗忘效果，优于现有方法且适用于实际部署。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在评估模型对目标数据遗忘情况时，仅评估单个输入，可能忽略语义相似示例中的残留影响，有隐私风险，需新方法。

Method: 提出REMIND方法，分析模型在小输入变化上的损失，揭示单点评估未发现的模式。

Result: 未学习数据的损失景观更平坦，保留或无关数据更尖锐多变；REMIND仅需基于查询的访问，在类似约束下优于现有方法，在不同模型、数据集和释义输入上表现稳健。

Conclusion: REMIND为评估语言模型的遗忘效果提供可靠框架，对记忆和遗忘提供新视角。

Abstract: Machine unlearning aims to remove the influence of specific training data
from a model without requiring full retraining. This capability is crucial for
ensuring privacy, safety, and regulatory compliance. Therefore, verifying
whether a model has truly forgotten target data is essential for maintaining
reliability and trustworthiness. However, existing evaluation methods often
assess forgetting at the level of individual inputs. This approach may overlook
residual influence present in semantically similar examples. Such influence can
compromise privacy and lead to indirect information leakage. We propose REMIND
(Residual Memorization In Neighborhood Dynamics), a novel evaluation method
aiming to detect the subtle remaining influence of unlearned data and classify
whether the data has been effectively forgotten. REMIND analyzes the model's
loss over small input variations and reveals patterns unnoticed by single-point
evaluations. We show that unlearned data yield flatter, less steep loss
landscapes, while retained or unrelated data exhibit sharper, more volatile
patterns. REMIND requires only query-based access, outperforms existing methods
under similar constraints, and demonstrates robustness across different models,
datasets, and paraphrased inputs, making it practical for real-world
deployment. By providing a more sensitive and interpretable measure of
unlearning effectiveness, REMIND provides a reliable framework to assess
unlearning in language models. As a result, REMIND offers a novel perspective
on memorization and unlearning.

</details>


### [249] [OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation](https://arxiv.org/abs/2511.04495)
*Cuong Huynh,Jie Cao*

Main category: cs.CL

TL;DR: 本文介绍OUNLP系统用于可读性控制文本简化，提出两种多轮简化方法，系统排名第7，改进后可提升性能。


<details>
  <summary>Details</summary>
Motivation: 基于对基于提示的文本简化方法的分析，发现文本简化性能与源和目标CEFR水平差距有关，从而寻求更好的简化方法。

Method: 提出基于规则的简化（MRS - Rule）和联合规则的大语言模型简化（MRS - Joint）两种多轮简化方法，并通过GPT - 4o生成。

Result: 提交的系统在20支队伍中排名第7，MRS - Joint改进后能进一步提升多轮简化性能。

Conclusion: 以大语言模型简化候选为起点可提升多轮简化性能。

Abstract: This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task
(Alva-Manchego et al., 2025), designed for readability-controlled text
simplification using LLM-prompting-based generation. Based on the analysis of
prompt-based text simplification methods, we discovered an interesting finding
that text simplification performance is highly related to the gap between the
source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by
this finding, we propose two multi-round simplification methods and generate
them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based
LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.
Later improvements with MRS-Joint show that taking the LLM simplified
candidates as the starting point could further boost the multi-round
simplification performance.

</details>


### [250] [Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering](https://arxiv.org/abs/2511.04499)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: 本文运用BFI - 2框架评估六种大语言模型在不同采样温度下的类人格特征，发现维度差异及模型聚类情况，为模型相关工作和伦理治理提供新视角。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型用于以人为中心的应用，理解其类人格行为对负责任的开发和部署很重要。

Method: 运用Big Five Inventory - 2 (BFI - 2)框架，评估六种大语言模型在不同采样温度下的特质表达。

Result: 在五个性格维度中的四个有显著差异，神经质和外向性易受温度调整影响；层次聚类显示出不同的模型簇，架构特征可能使某些模型具有稳定特质。

Conclusion: 研究为大语言模型类人格模式的出现提供新见解，为模型调整、选择和AI系统伦理治理提供新视角。

Abstract: As Large Language Models (LLMs) become integral to human-centered
applications, understanding their personality-like behaviors is increasingly
important for responsible development and deployment. This paper systematically
evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to
assess trait expressions under varying sampling temperatures. We find
significant differences across four of the five personality dimensions, with
Neuroticism and Extraversion susceptible to temperature adjustments. Further,
hierarchical clustering reveals distinct model clusters, suggesting that
architectural features may predispose certain models toward stable trait
profiles. Taken together, these results offer new insights into the emergence
of personality-like patterns in LLMs and provide a new perspective on model
tuning, selection, and the ethical governance of AI systems. We share the data
and code for this analysis here:
https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1

</details>


### [251] [RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG](https://arxiv.org/abs/2511.04502)
*Joshua Gao,Quoc Huy Pham,Subin Varghese,Silwal Saurav,Vedhus Hoskere*

Main category: cs.CL

TL;DR: 本文介绍自动化、与人判断一致的RAGalyst框架用于评估特定领域RAG系统，应用该框架评估发现性能高度依赖上下文，强调系统评估框架的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架难以评估特定领域RAG系统，启发式指标无法捕捉领域细微差别，基于大模型的评判方法与人类判断缺乏有效校准。

Method: 引入RAGalyst框架，用代理管道从源文档生成高质量合成问答数据集，通过提示优化改进两个大模型评判指标。

Result: 评估发现性能高度依赖上下文，没有通用最优的嵌入模型、大模型或超参数配置，还分析了RAG中答案正确性低的常见原因。

Conclusion: 需要像RAGalyst这样的系统评估框架，帮助从业者发现特定领域的权衡并做出明智设计选择。

Abstract: Retrieval-Augmented Generation (RAG) is a critical technique for grounding
Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in
specialized, safety-critical domains remains a significant challenge. Existing
evaluation frameworks often rely on heuristic-based metrics that fail to
capture domain-specific nuances and other works utilize LLM-as-a-Judge
approaches that lack validated alignment with human judgment. This paper
introduces RAGalyst, an automated, human-aligned agentic framework designed for
the rigorous evaluation of domain-specific RAG systems. RAGalyst features an
agentic pipeline that generates high-quality, synthetic question-answering (QA)
datasets from source documents, incorporating an agentic filtering step to
ensure data fidelity. The framework refines two key LLM-as-a-Judge
metrics-Answer Correctness and Answerability-using prompt optimization to
achieve a strong correlation with human annotations. Applying this framework to
evaluate various RAG components across three distinct domains (military
operations, cybersecurity, and bridge engineering), we find that performance is
highly context-dependent. No single embedding model, LLM, or hyperparameter
configuration proves universally optimal. Additionally, we provide an analysis
on the most common low Answer Correctness reasons in RAG. These findings
highlight the necessity of a systematic evaluation framework like RAGalyst,
which empowers practitioners to uncover domain-specific trade-offs and make
informed design choices for building reliable and effective RAG systems.
RAGalyst is available on our Github.

</details>


### [252] [Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics](https://arxiv.org/abs/2511.04527)
*Amir Zur,Atticus Geiger,Ekdeep Singh Lubana,Eric Bigelow*

Main category: cs.CL

TL;DR: 本文研究推理语言模型是否表示生成过程中的替代路径，用隐藏激活控制和预测不确定性，发现激活干预有效性与替代路径有关，且隐藏激活可预测未来结果分布。


<details>
  <summary>Details</summary>
Motivation: 语言模型生成文本时选择单个标记会导致不同推理路径，难以量化不确定性，研究推理语言模型是否表示生成过程中的替代路径。

Method: 使用隐藏激活来控制和预测语言模型在思维链推理中的不确定性。

Result: 发现模型在不同标记处的不确定性与通过控制激活来引导模型的难易程度有明显相关性，且隐藏激活可以预测模型的未来结果分布。

Conclusion: 激活干预在模型有替代路径时最有效，模型隐式表示了可能路径的空间。

Abstract: When a language model generates text, the selection of individual tokens
might lead it down very different reasoning paths, making uncertainty difficult
to quantify. In this work, we consider whether reasoning language models
represent the alternate paths that they could take during generation. To test
this hypothesis, we use hidden activations to control and predict a language
model's uncertainty during chain-of-thought reasoning. In our experiments, we
find a clear correlation between how uncertain a model is at different tokens,
and how easily the model can be steered by controlling its activations. This
suggests that activation interventions are most effective when there are
alternate paths available to the model -- in other words, when it has not yet
committed to a particular final answer. We also find that hidden activations
can predict a model's future outcome distribution, demonstrating that models
implicitly represent the space of possible paths.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [253] [Pediatric Appendicitis Detection from Ultrasound Images](https://arxiv.org/abs/2511.04069)
*Fatemeh Hosseinabadi,Seyedhassan Sharifi*

Main category: eess.IV

TL;DR: 本文开发并评估基于预训练ResNet架构的深度学习模型，用于从超声图像自动检测小儿阑尾炎，模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 小儿阑尾炎诊断因症状重叠和影像质量不一而具有挑战性，需开发自动检测模型。

Method: 使用Regensburg儿科阑尾炎数据集，对ResNet进行微调，对图像进行预处理。

Result: 模型总体准确率93.44，精确率91.53，召回率89.8，能有效学习判别性空间特征。

Conclusion: 该模型在识别小儿阑尾炎方面表现出色，克服了儿科影像的一些挑战。

Abstract: Pediatric appendicitis remains one of the most common causes of acute
abdominal pain in children, and its diagnosis continues to challenge clinicians
due to overlapping symptoms and variable imaging quality. This study aims to
develop and evaluate a deep learning model based on a pretrained ResNet
architecture for automated detection of appendicitis from ultrasound images. We
used the Regensburg Pediatric Appendicitis Dataset, which includes ultrasound
scans, laboratory data, and clinical scores from pediatric patients admitted
with abdominal pain to Children Hospital. Hedwig in Regensburg, Germany. Each
subject had 1 to 15 ultrasound views covering the right lower quadrant,
appendix, lymph nodes, and related structures. For the image based
classification task, ResNet was fine tuned to distinguish appendicitis from
non-appendicitis cases. Images were preprocessed by normalization, resizing,
and augmentation to enhance generalization. The proposed ResNet model achieved
an overall accuracy of 93.44, precision of 91.53, and recall of 89.8,
demonstrating strong performance in identifying appendicitis across
heterogeneous ultrasound views. The model effectively learned discriminative
spatial features, overcoming challenges posed by low contrast, speckle noise,
and anatomical variability in pediatric imaging.

</details>


### [254] [Left Atrial Segmentation with nnU-Net Using MRI](https://arxiv.org/abs/2511.04071)
*Fatemeh Hosseinabadi,Seyedhassan Sharifi*

Main category: eess.IV

TL;DR: 本文应用nnU-Net框架对心脏MRI左心房进行分割，在数据集上取得高Dice分数，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 手动分割左心房耗时且依赖观察者，不适用于大规模或时间敏感的临床工作流，需更高效准确的分割方法。

Method: 应用nnU-Net框架到2013年左心房分割挑战数据集，模型自动调整预处理、网络配置和训练流程，用Dice相似系数定量评估，与专家分割结果定性比较。

Result: nnU-Net模型平均Dice分数达93.5，与专家标注高度重叠，优于传统分割方法，对左心房形状、对比度和图像质量变化有强泛化能力。

Conclusion: nnU-Net框架在心脏MRI左心房分割任务中表现良好，可准确分割左心房及近端肺静脉。

Abstract: Accurate segmentation of the left atrium (LA) from cardiac MRI is critical
for guiding atrial fibrillation (AF) ablation and constructing biophysical
cardiac models. Manual delineation is time-consuming, observer-dependent, and
impractical for large-scale or time-sensitive clinical workflows. Deep learning
methods, particularly convolutional architectures, have recently demonstrated
superior performance in medical image segmentation tasks. In this study, we
applied the nnU-Net framework, an automated, self-configuring deep learning
segmentation architecture, to the Left Atrial Segmentation Challenge 2013
dataset. The dataset consists of thirty MRI scans with corresponding
expert-annotated masks. The nnU-Net model automatically adapted its
preprocessing, network configuration, and training pipeline to the
characteristics of the MRI data. Model performance was quantitatively evaluated
using the Dice similarity coefficient (DSC), and qualitative results were
compared against expert segmentations. The proposed nnUNet model achieved a
mean Dice score of 93.5, demonstrating high overlap with expert annotations and
outperforming several traditional segmentation approaches reported in previous
studies. The network exhibited robust generalization across variations in left
atrial shape, contrast, and image quality, accurately delineating both the
atrial body and proximal pulmonary veins.

</details>


### [255] [Computed Tomography (CT)-derived Cardiovascular Flow Estimation Using Physics-Informed Neural Networks Improves with Sinogram-based Training: A Simulation Study](https://arxiv.org/abs/2511.03876)
*Jinyuxuan Guo,Gurnoor Singh Khurana,Alejandro Gonzalo Grande,Juan C. del Alamo,Francisco Contijoch*

Main category: eess.IV

TL;DR: 研究评估CT成像对基于PINN的血流估计的影响，提出SinoFlow框架，其性能优于基于重建图像的方法，为无创血流评估提供更有前景的途径。


<details>
  <summary>Details</summary>
Motivation: 现有CT成像缺乏直接估计血流速度的方法，本研究旨在评估CT成像对基于PINN的血流估计的影响并提出改进框架。

Method: 用计算流体动力学在二维血管分叉中生成脉动流场，模拟不同参数的CT扫描，比较基于重建图像的PINN方法（ImageFlow）和SinoFlow的性能。

Result: SinoFlow避免了滤波反投影引入的误差，在不同扫描参数下表现更优，与脉冲模式成像兼容，短脉冲宽度下精度更高。

Conclusion: SinoFlow在基于CT的血流估计中有潜力，为无创血流评估提供更有前景的方法，研究结果可为PINNs在CT图像中的应用提供参考。

Abstract: Background: Non-invasive imaging-based assessment of blood flow plays a
critical role in evaluating heart function and structure. Computed Tomography
(CT) is a widely-used imaging modality that can robustly evaluate
cardiovascular anatomy and function, but direct methods to estimate blood flow
velocity from movies of contrast evolution have not been developed.
  Purpose: This study evaluates the impact of CT imaging on Physics-Informed
Neural Networks (PINN)-based flow estimation and proposes an improved
framework, SinoFlow, which uses sinogram data directly to estimate blood flow.
  Methods: We generated pulsatile flow fields in an idealized 2D vessel
bifurcation using computational fluid dynamics and simulated CT scans with
varying gantry rotation speeds, tube currents, and pulse mode imaging settings.
We compared the performance of PINN-based flow estimation using reconstructed
images (ImageFlow) to SinoFlow.
  Results: SinoFlow significantly improved flow estimation performance by
avoiding propagating errors introduced by filtered backprojection. SinoFlow was
robust across all tested gantry rotation speeds and consistently produced lower
mean squared error and velocity errors than ImageFlow. Additionally, SinoFlow
was compatible with pulsed-mode imaging and maintained higher accuracy with
shorter pulse widths.
  Conclusions: This study demonstrates the potential of SinoFlow for CT-based
flow estimation, providing a more promising approach for non-invasive blood
flow assessment. The findings aim to inform future applications of PINNs to CT
images and provide a solution for image-based estimation, with reasonable
acquisition parameters yielding accurate flow estimates.

</details>


### [256] [Shape Deformation Networks for Automated Aortic Valve Finite Element Meshing from 3D CT Images](https://arxiv.org/abs/2511.03890)
*Linchen Qian,Jiasong Chen,Ruonan Gong,Wei Sun,Minliang Liu,Liang Liang*

Main category: eess.IV

TL;DR: 本文提出用带深度神经网络的模板拟合管道从3D CT图像生成结构化四边形网格来建模主动脉瓣，实验表明该方法能生成高质量网格，简化训练过程。


<details>
  <summary>Details</summary>
Motivation: 传统方法生成的主动脉瓣三角网格拓扑不规则，难以生成高质量且跨患者一致的网格，因此需要新方法解决该问题。

Method: 引入带深度神经网络的模板拟合管道，用通用四边形网格模板对所有患者的主动脉瓣进行重新网格化，采用仅含两项的损失函数。

Result: 该方法能生成具有更好平滑度和形状质量的高质量主动脉瓣表面网格，且相比传统方法需要更少的显式正则化项。

Conclusion: 使用结构化四边形网格进行模板和神经网络训练可确保网格对应性和质量，简化训练过程，提高主动脉瓣建模的有效性和效率。

Abstract: Accurate geometric modeling of the aortic valve from 3D CT images is
essential for biomechanical analysis and patient-specific simulations to assess
valve health or make a preoperative plan. However, it remains challenging to
generate aortic valve meshes with both high-quality and consistency across
different patients. Traditional approaches often produce triangular meshes with
irregular topologies, which can result in poorly shaped elements and
inconsistent correspondence due to inter-patient anatomical variation. In this
work, we address these challenges by introducing a template-fitting pipeline
with deep neural networks to generate structured quad (i.e., quadrilateral)
meshes from 3D CT images to represent aortic valve geometries. By remeshing
aortic valves of all patients with a common quad mesh template, we ensure a
uniform mesh topology with consistent node-to-node and element-to-element
correspondence across patients. This consistency enables us to simplify the
learning objective of the deep neural networks, by employing a loss function
with only two terms (i.e., a geometry reconstruction term and a smoothness
regularization term), which is sufficient to preserve mesh smoothness and
element quality. Our experiments demonstrate that the proposed approach
produces high-quality aortic valve surface meshes with improved smoothness and
shape quality, while requiring fewer explicit regularization terms compared to
the traditional methods. These results highlight that using structured quad
meshes for the template and neural network training not only ensures mesh
correspondence and quality but also simplifies the training process, thus
enhancing the effectiveness and efficiency of aortic valve modeling.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [257] [Launch-Day Diffusion: Tracking Hacker News Impact on GitHub Stars for AI Tools](https://arxiv.org/abs/2511.04453)
*Obada Kraishan*

Main category: cs.SI

TL;DR: 本文构建系统追踪Hacker News曝光对AI和LLM工具GitHub星数增长的影响，分析138个仓库，找出关键预测因素，系统可快速复现且框架易扩展。


<details>
  <summary>Details</summary>
Motivation: 量化社交新闻平台（如Hacker News）对开源项目的直接影响。

Method: 构建基于公共API的追踪系统，分析2024 - 2025年138个仓库数据，使用机器学习模型（Elastic Net）和非线性方法（Gradient Boosting）。

Result: 仓库在HN曝光后24小时平均增加121颗星，48小时189颗，一周289颗；发布时间是关键因素，“Show HN”标签无统计优势；系统在标准硬件上5分钟内完成演示。

Conclusion: 研究结果可复现，框架易扩展，为研究者和开发者提供发布动态的可行见解。

Abstract: Social news platforms have become key launch outlets for open-source
projects, especially Hacker News (HN), though quantifying their immediate
impact remains challenging. This paper presents a reproducible demonstration
system that tracks how HN exposure translates into GitHub star growth for AI
and LLM tools. Built entirely on public APIs, our pipeline analyzes 138
repository launches from 2024-2025 and reveals substantial launch effects:
repositories gain an average of 121 stars within 24 hours, 189 stars within 48
hours, and 289 stars within a week of HN exposure. Through machine learning
models (Elastic Net) and non-linear approaches (Gradient Boosting), we identify
key predictors of viral growth. Posting timing appears as key factor--launching
at optimal hours can mean hundreds of additional stars--while the "Show HN" tag
shows no statistical advantage after controlling for other factors. The
demonstration completes in under five minutes on standard hardware,
automatically collecting data, training models, and generating visualizations
through single-file scripts. This makes our findings immediately reproducible
and the framework easily be extended to other platforms, providing both
researchers and developers with actionable insights into launch dynamics.

</details>


### [258] [Advancing Equitable AI: Evaluating Cultural Expressiveness in LLMs for Latin American Contexts](https://arxiv.org/abs/2511.04090)
*Brigitte A. Mora-Reyes,Jennifer A. Drewyor,Abel A. Reyes-Angulo*

Main category: cs.SI

TL;DR: 本文指出AI系统因数据集失衡，对拉丁美洲语境存在偏见，引入基于拉丁美洲的数据集进行评估，发现部分模型表现较好，微调Mistral - 7B提升了文化表现力，倡导公平AI发展。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统因数据集失衡而对拉丁美洲语境产生偏见的问题，推动公平AI发展。

Method: 引入基于拉丁美洲历史和社会政治背景的文化感知数据集，用新的文化表现力指标、统计测试和语言分析评估六个语言模型。

Result: 部分模型能更好捕捉拉丁美洲视角，部分模型存在显著情感偏差；微调Mistral - 7B使文化表现力提升42.9%。

Conclusion: 应优先使用反映拉丁美洲历史、本土知识和多种语言的数据集，采用以社区为中心的方法推动公平AI发展。

Abstract: Artificial intelligence (AI) systems often reflect biases from economically
advanced regions, marginalizing contexts in economically developing regions
like Latin America due to imbalanced datasets. This paper examines AI
representations of diverse Latin American contexts, revealing disparities
between data from economically advanced and developing regions. We highlight
how the dominance of English over Spanish, Portuguese, and indigenous languages
such as Quechua and Nahuatl perpetuates biases, framing Latin American
perspectives through a Western lens. To address this, we introduce a culturally
aware dataset rooted in Latin American history and socio-political contexts,
challenging Eurocentric models. We evaluate six language models on questions
testing cultural context awareness, using a novel Cultural Expressiveness
metric, statistical tests, and linguistic analyses. Our findings show that some
models better capture Latin American perspectives, while others exhibit
significant sentiment misalignment (p < 0.001). Fine-tuning Mistral-7B with our
dataset improves its cultural expressiveness by 42.9%, advancing equitable AI
development. We advocate for equitable AI by prioritizing datasets that reflect
Latin American history, indigenous knowledge, and diverse languages, while
emphasizing community-centered approaches to amplify marginalized voices.

</details>


<div id='cond-mat.supr-con'></div>

# cond-mat.supr-con [[Back]](#toc)

### [259] [Expert Evaluation of LLM World Models: A High-$T_c$ Superconductivity Case Study](https://arxiv.org/abs/2511.03782)
*Haoyu Guo,Maria Tikhanovskaya,Paul Raccuglia,Alexey Vlaskin,Chris Co,Daniel J. Liebling,Scott Ellsworth,Matthew Abraham,Elizabeth Dorfman,N. P. Armitage,Chunhan Feng,Antoine Georges,Olivier Gingras,Dominik Kiese,Steven A. Kivelson,Vadim Oganesyan,B. J. Ramshaw,Subir Sachdev,T. Senthil,J. M. Tranquada,Michael P. Brenner,Subhashini Venugopalan,Eun-Ah Kim*

Main category: cond-mat.supr-con

TL;DR: 本文以高温铜酸盐领域为例，评估大语言模型（LLM）系统理解科学文献的能力，发现两种基于RAG的系统表现优于现有封闭模型，还讨论了模型优缺点并提供评估资源。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在专业领域为复杂问题提供科学准确且全面答案的有效性。

Method: 构建包含1726篇科学论文的专家策展数据库和67个专家制定的问题，评估六个不同的基于LLM的系统，专家根据评分标准评估答案。

Result: 两种在策展文献上使用RAG的系统在关键指标上优于现有封闭模型，尤其在提供全面且有充分支持的答案方面。

Conclusion: 讨论了LLM性能的优点和所有模型的关键缺点，专家制定的问题集和评分标准对评估基于LLM的推理系统的专家级性能有价值。

Abstract: Large Language Models (LLMs) show great promise as a powerful tool for
scientific literature exploration. However, their effectiveness in providing
scientifically accurate and comprehensive answers to complex questions within
specialized domains remains an active area of research. Using the field of
high-temperature cuprates as an exemplar, we evaluate the ability of LLM
systems to understand the literature at the level of an expert. We construct an
expert-curated database of 1,726 scientific papers that covers the history of
the field, and a set of 67 expert-formulated questions that probe deep
understanding of the literature. We then evaluate six different LLM-based
systems for answering these questions, including both commercially available
closed models and a custom retrieval-augmented generation (RAG) system capable
of retrieving images alongside text. Experts then evaluate the answers of these
systems against a rubric that assesses balanced perspectives, factual
comprehensiveness, succinctness, and evidentiary support. Among the six systems
two using RAG on curated literature outperformed existing closed models across
key metrics, particularly in providing comprehensive and well-supported
answers. We discuss promising aspects of LLM performances as well as critical
short-comings of all the models. The set of expert-formulated questions and the
rubric will be valuable for assessing expert level performance of LLM based
reasoning systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [260] [MusRec: Zero-Shot Text-to-Music Editing via Rectified Flow and Diffusion Transformers](https://arxiv.org/abs/2511.04376)
*Ali Boudaghi,Hadi Zare*

Main category: cs.SD

TL;DR: 提出零样本文本到音乐编辑模型MusRec，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有音乐编辑模型存在局限，缺乏真正的零样本能力。

Method: 利用整流流和扩散变压器的最新进展，引入MusRec模型。

Result: 该方法在保留音乐内容、结构一致性和编辑保真度方面优于现有方法。

Conclusion: 为现实场景中的可控音乐编辑奠定了坚实基础。

Abstract: Music editing has emerged as an important and practical area of artificial
intelligence, with applications ranging from video game and film music
production to personalizing existing tracks according to user preferences.
However, existing models face significant limitations, such as being restricted
to editing synthesized music generated by their own models, requiring highly
precise prompts, or necessitating task-specific retraining, thus lacking true
zero-shot capability. Leveraging recent advances in rectified flow and
diffusion transformers, we introduce MusRec, the first zero-shot text-to-music
editing model capable of performing diverse editing tasks on real-world music
efficiently and effectively. Experimental results demonstrate that our approach
outperforms existing methods in preserving musical content, structural
consistency, and editing fidelity, establishing a strong foundation for
controllable music editing in real-world scenarios.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [261] [Which Similarity-Sensitive Entropy?](https://arxiv.org/abs/2511.03849)
*Phuc Nguyen,Josiah Couch,Rahul Bansal,Alexandra Morgan,Chris Tam,Miao Li,Rima Arnaout,Ramy Arnaout*

Main category: cs.IT

TL;DR: 本文对比LCR和VS两种熵计算方法，用53个机器学习数据集实验，分析两者差异、互补性及界限，给出适用场景。


<details>
  <summary>Details</summary>
Motivation: LCR和VS是新的熵计算方法，需对比两者差异及确定哪种更优。

Method: 从概念、分析和实验角度，使用53个机器学习数据集进行研究。

Result: LCR和VS可能有数量级差异，能捕捉互补信息；两者依赖相似度缩放，引入“半距离”概念；VS对LCR有上界。

Conclusion: 仅在特定情况VS更优，一般捕捉相似性信息时LCR更受青睐，特定半距离下两者可互补。

Abstract: A canonical step in quantifying a system is to measure its entropy. Shannon
entropy and other traditional entropy measures capture only the information
encoded in the frequencies of a system's elements. Recently, Leinster, Cobbold,
and Reeve (LCR) introduced a method that also captures the rich information
encoded in the similarities and differences among elements, yielding
similarity-sensitive entropy. More recently, the Vendi score (VS) was
introduced as an alternative, raising the question of how LCR and VS compare,
and which is preferable. Here we address these questions conceptually,
analytically, and experimentally, using 53 machine-learning datasets. We show
that LCR and VS can differ by orders of magnitude and can capture complementary
information about a system, except in limiting cases. We demonstrate that both
LCR and VS depend on how similarities are scaled and introduce the concept of
``half distance'' to parameterize this dependence. We prove that VS provides an
upper bound on LCR for several values of the R\'enyi-Hill order parameter and
conjecture that this bound holds for all values. We conclude that VS is
preferable only when interpreting elements as linear combinations of a more
fundamental set of ``ur-elements'' or when the system or dataset possesses a
quantum-mechanical character. In the broader circumstance where one seeks
simply to capture the rich information encoded by similarity, LCR is favored;
nevertheless, for certain half-distances the two methods can complement each
other.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [262] [OptiMA: A Transaction-Based Framework with Throughput Optimization for Very Complex Multi-Agent Systems](https://arxiv.org/abs/2511.03761)
*Umut Çalıkyılmaz,Nitin Nayak,Jinghua Groppe,Sven Groppe*

Main category: cs.MA

TL;DR: 本文指出多智能体系统复杂度增加的两个隐患，提出交易框架和调度方案，开发OptiMA框架并验证其效果，还对调度问题进行理论分析。


<details>
  <summary>Details</summary>
Motivation: 探索大型复杂多智能体系统时，复杂度增加会带来故障易发性和性能瓶颈问题。

Method: 提出基于交易的框架设计VCMAS，将交易调度集成到框架中，开发OptiMA框架。

Result: OptiMA框架能促进超百个智能体的VCMAS执行，交易调度使系统性能提升超16%，并对交易调度问题进行理论分析。

Conclusion: 所提方法有效，为复杂多智能体系统的设计和交易调度问题的研究提供了工具和思路。

Abstract: In recent years, the research of multi-agent systems has taken a direction to
explore larger and more complex models to fulfill sophisticated tasks. We point
out two possible pitfalls that might be caused by increasing complexity;
susceptibilities to faults, and performance bottlenecks. To prevent the former
threat, we propose a transaction-based framework to design very complex
multi-agent systems (VCMAS). To address the second threat, we offer to
integrate transaction scheduling into the proposed framework. We implemented
both of these ideas to develop the OptiMA framework and show that it is able to
facilitate the execution of VCMAS with more than a hundred agents. We also
demonstrate the effect of transaction scheduling on such a system by showing
improvements up to more than 16\%. Furthermore, we also performed a theoretical
analysis on the transaction scheduling problem and provided practical tools
that can be used for future research on it.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [263] [Boolean function monotonicity testing requires (almost) $n^(1/2)$ queries](https://arxiv.org/abs/2511.04558)
*Mark Chen,Xi Chen,Hao Cui,William Pires,Jonah Stockwell*

Main category: cs.CC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We show that for any constant $c>0$, any (two-sided error) adaptive algorithm
for testing monotonicity of Boolean functions must have query complexity
$\Omega(n^{1/2-c})$. This improves the $\tilde\Omega(n^{1/3})$ lower bound of
[CWX17] and almost matches the $\tilde{O}(\sqrt{n})$ upper bound of [KMS18].

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [264] [Universal Quantum Simulation of 50 Qubits on Europe`s First Exascale Supercomputer Harnessing Its Heterogeneous CPU-GPU Architecture](https://arxiv.org/abs/2511.03359)
*Hans De Raedt,Jiri Kraus,Andreas Herten,Vrinda Mehta,Mathis Bode,Markus Hrywniak,Kristel Michielsen,Thomas Lippert*

Main category: quant-ph

TL;DR: 开发了新版本高性能量子计算机模拟器JUQCS - 50，可模拟50比特通用量子计算机，有三项创新，速度比之前快11.4倍。


<details>
  <summary>Details</summary>
Motivation: 实现对50比特通用量子计算机的模拟。

Method: 通过高带宽CPU - GPU互连和LPDDR5内存扩展可用内存；采用自适应数据编码；使用即时网络流量优化器。

Result: JUQCS - 50实现了对50比特通用量子计算机的模拟，比K计算机上之前48比特记录速度快11.4倍。

Conclusion: JUQCS - 50的三项创新有效提升了量子计算机模拟性能。

Abstract: We have developed a new version of the high-performance J\"ulich universal
quantum computer simulator (JUQCS-50) that leverages key features of the GH200
superchips as used in the JUPITER supercomputer, enabling simulations of a
50-qubit universal quantum computer for the first time. JUQCS-50 achieves this
through three key innovations: (1) extending usable memory beyond GPU limits
via high-bandwidth CPU-GPU interconnects and LPDDR5 memory; (2) adaptive data
encoding to reduce memory footprint with acceptable trade-offs in precision and
compute effort; and (3) an on-the-fly network traffic optimizer. These advances
result in an 11.4-fold speedup over the previous 48-qubit record on the K
computer.

</details>


### [265] [Twirlator: A Pipeline for Analyzing Subgroup Symmetry Effects in Quantum Machine Learning Ansatzes](https://arxiv.org/abs/2511.04243)
*Valter Uotila,Väinö Mehtola,Ilmo Salmenperä,Bo Zhao*

Main category: quant-ph

TL;DR: 本文开发自动化流程衡量量子机器学习ansatzes对称性特征，计算三类指标，结果显示不同ansatzes门开销不同，高对称性降低电路表达能力、多数情况提升纠缠能力，有助于选择合适ansatz模式。


<details>
  <summary>Details</summary>
Motivation: 在量子机器学习中，对称化实用性开销未被充分理解，需研究量子机器学习ansatzes在对称性方面的特征。

Method: 开发自动化流程，定义学习问题对称度，对19种常见ansatzes按不同大小子群表示进行对称化，计算三类指标。

Result: 不同ansatzes门开销不同，增加对称性降低电路表达能力，多数情况增加纠缠能力。

Conclusion: 研究结果有助于为几何量子机器学习应用选择足够可表达且计算高效的ansatz模式。

Abstract: Leveraging data symmetries has been a key driver of performance gains in
geometric deep learning and geometric and equivariant quantum machine learning.
While symmetrization appears to be a promising method, its practical overhead,
such as additional gates, reduced expressibility, and other factors, is not
well understood in quantum machine learning. In this work, we develop an
automated pipeline to measure various characteristics of quantum machine
learning ansatzes with respect to symmetries that can appear in the learning
task. We define the degree of symmetry in the learning problem as the size of
the subgroup it admits. Subgroups define partial symmetries, which have not
been extensively studied in previous research, which has focused on symmetries
defined by whole groups. Symmetrizing the 19 common ansatzes with respect to
these varying-sized subgroup representations, we compute three classes of
metrics that describe how the common ansatz structures behave under varying
amounts of symmetries. The first metric is based on the norm of the difference
between the original and symmetrized generators, while the second metric counts
depth, size, and other characteristics from the symmetrized circuits. The third
class of metrics includes expressibility and entangling capability. The results
demonstrate varying gate overhead across the studied ansatzes and confirm that
increased symmetry reduces expressibility of the circuits. In most cases,
increased symmetry increases entanglement capability. These results help select
sufficiently expressible and computationally efficient ansatze patterns for
geometric quantum machine learning applications.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [266] [Measuring economic outlook in the news timely and efficiently](https://arxiv.org/abs/2511.04299)
*Elliot Beck,Franziska Eckert,Linus Kühne,Helge Liebert,Rina Rosenblatt-Wisch*

Main category: econ.GN

TL;DR: 引入结合机器学习、大语言模型与传统统计方法的新指标追踪瑞士新闻经济情绪，该指标有优势，方法资源高效。


<details>
  <summary>Details</summary>
Motivation: 追踪瑞士新闻中关于经济前景的情绪，提高GDP增长预测准确性。

Method: 结合机器学习、大语言模型与传统统计方法创建指标。

Result: 该指标可解释、及时，显著提高GDP增长预测准确性。

Conclusion: 此方法资源高效、模块化，能让受数据限制的机构受益于大语言模型。

Abstract: We introduce a novel indicator that combines machine learning and large
language models with traditional statistical methods to track sentiment
regarding the economic outlook in Swiss news. The indicator is interpretable
and timely, and it significantly improves the accuracy of GDP growth forecasts.
Our approach is resource-efficient, modular, and offers a way of benefitting
from state-of-the-art large language models even if data are proprietary and
cannot be stored or analyzed on external infrastructure - a restriction faced
by many central banks and public institutions.

</details>


### [267] [Regime Changes and Real-Financial Cycles: Searching Minsky's Hypothesis in a Nonlinear Setting](https://arxiv.org/abs/2511.04348)
*Domenico delli Gatti,Filippo Gusella,Giorgio Ricchiuti*

Main category: econ.GN

TL;DR: 本文扩展Stockhammer等人（2019）的研究，用非线性模型研究Minsky周期，对多国进行实证分析并得出相关结果，强调非线性制度转变对评估Minsky理论的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究Minsky周期，通过非线性模型捕捉可能的实际 - 金融内生周期。

Method: 扩展Stockhammer等人（2019）的研究，采用非线性模型，将GDP与企业债务、利率和家庭债务联系起来，追踪非线性制度变化并检验Minsky周期。

Result: 考虑企业债务时，除澳大利亚外所有国家存在实际 - 金融内生周期；纳入利率时所有国家都存在；仅美国和英国存在家庭债务与GDP的相互作用机制。

Conclusion: 非线性制度转变在实证评估Minsky理论中很重要。

Abstract: This paper investigates Minsky's cycles by extending the paper of stockhammer
et al. (2019) with a nonlinear model to capture possible local real-financial
endogenous cycles. We trace nonlinear regime changes and check the presence of
Minsky cycles from the 1970s to 2020 for the USA, France, Germany, Canada,
Australia, and the UK, linking the GDP with corporate debt, interest rate, and
household debt. When considering corporate debt, the results reveal
real-financial endogenous cycles in all countries, except Australia, and across
all countries when interest rates are included. We find evidence for an
interaction mechanism between household debt and GDP only for the USA and the
UK. These findings underscore the importance of nonlinear regime transitions in
empirically assessing Minsky's theory.

</details>
