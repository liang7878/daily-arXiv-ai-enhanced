<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 75]
- [cs.SE](#cs.SE) [Total: 21]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 35]
- [math.PR](#math.PR) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 9]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [econ.TH](#econ.TH) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.SD](#cs.SD) [Total: 7]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.CR](#cs.CR) [Total: 13]
- [cs.CC](#cs.CC) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [math.ST](#math.ST) [Total: 2]
- [cs.MA](#cs.MA) [Total: 3]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 6]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.CV](#cs.CV) [Total: 16]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [econ.GN](#econ.GN) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Unified Crew Planning and Replanning Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity](https://arxiv.org/abs/2509.14251)
*Qihang Chen*

Main category: cs.AI

TL;DR: 提出多线地铁乘务规划与重规划统一优化框架，经京沪地铁数据验证效果佳，凸显全局优化与跨线协调作用。


<details>
  <summary>Details</summary>
Motivation: 地铁乘务规划影响公交运营效率与服务可靠性，现有研究缺乏跨线协调与快速重规划。

Method: 提出分层时空网络模型表示统一乘务行动空间，推导约束与公式，开发基于列生成和最短路径调整的求解算法。

Result: 京沪地铁真实数据实验表明，该方法在成本降低和任务完成方面优于基准启发式方法，跨线运营效率提升显著。

Conclusion: 强调全局优化和跨线协调在多线地铁系统运营中的作用，为智慧城市公交高效可靠运行提供见解。

Abstract: Metro crew planning is a key component of smart city development as it
directly impacts the operational efficiency and service reliability of public
transportation. With the rapid expansion of metro networks, effective
multi-line scheduling and emergency management have become essential for
large-scale seamless operations. However, current research focuses primarily on
individual metro lines,with insufficient attention on cross-line coordination
and rapid replanning during disruptions. Here, a unified optimization framework
is presented for multi-line metro crew planning and replanning with
heterogeneous workforce. Specifically, a hierarchical time-space network model
is proposed to represent the unified crew action space, and computationally
efficient constraints and formulations are derived for the crew's heterogeneous
qualifications and preferences. Solution algorithms based on column generation
and shortest path adjustment are further developed, utilizing the proposed
network model. Experiments with real data from Shanghai and Beijing Metro
demonstrate that the proposed methods outperform benchmark heuristics in both
cost reduction and task completion,and achieve notable efficiency gains by
incorporating cross-line operations, particularly for urgent tasks during
disruptions. This work highlights the role of global optimization and
cross-line coordination in multi-line metro system operations, providing
insights into the efficient and reliable functioning of public transportation
in smart cities.

</details>


### [2] [From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing](https://arxiv.org/abs/2509.14289)
*Lanxiao Huang,Daksh Dave,Ming Jin,Tyler Cody,Peter Beling*

Main category: cs.AI

TL;DR: 对基于大语言模型的渗透测试代理进行全面评估，隔离五项核心功能能力的影响，发现针对性增强可提升模块化代理性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于渗透测试的有效性和可靠性在各攻击阶段尚不明确，需进行评估。

Method: 对多种基于大语言模型的代理进行全面评估，通过针对性增强隔离五项核心功能能力的影响。

Result: 部分架构本身具备部分特性，针对性增强能大幅提升模块化代理在复杂、多步骤和实时渗透测试任务中的性能。

Conclusion: 针对性增强可显著改善模块化代理在渗透测试中的性能。

Abstract: Large language models (LLMs) are increasingly used to automate or augment
penetration testing, but their effectiveness and reliability across attack
phases remain unclear. We present a comprehensive evaluation of multiple
LLM-based agents, from single-agent to modular designs, across realistic
penetration testing scenarios, measuring empirical performance and recurring
failure patterns. We also isolate the impact of five core functional
capabilities via targeted augmentations: Global Context Memory (GCM),
Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive
Planning (AP), and Real-Time Monitoring (RTM). These interventions support,
respectively: (i) context coherence and retention, (ii) inter-component
coordination and state management, (iii) tool use accuracy and selective
execution, (iv) multi-step strategic planning, error detection, and recovery,
and (v) real-time dynamic responsiveness. Our results show that while some
architectures natively exhibit subsets of these properties, targeted
augmentations substantially improve modular agent performance, especially in
complex, multi-step, and real-time penetration testing tasks.

</details>


### [3] [Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents](https://arxiv.org/abs/2509.14382)
*Daniel Röder,Akhil Juneja,Roland Roller,Sven Schmeier*

Main category: cs.AI

TL;DR: 现有Web智能体评估忽视中间错误，本文提出模块化评估框架，通过案例研究揭示标准指标遗漏的弱点，助力构建更强大通用的Web智能体。


<details>
  <summary>Details</summary>
Motivation: 当前Web智能体评估大多关注整体成功，忽略中间错误，缺乏细粒度诊断工具，限制对失败模式的洞察和系统改进。

Method: 提出模块化评估框架，将智能体管道分解为可解释阶段进行详细错误分析，并以SeeAct框架和Mind2Web数据集进行案例研究。

Result: 该方法揭示了标准指标遗漏的可操作弱点。

Conclusion: 此框架为构建更强大、更通用的Web智能体铺平道路。

Abstract: Web agents powered by large language models (LLMs) can autonomously perform
complex, multistep tasks in dynamic web environments. However, current
evaluations mostly focus on the overall success while overlooking intermediate
errors. This limits insight into failure modes and hinders systematic
improvement. This work analyzes existing benchmarks and highlights the lack of
fine-grained diagnostic tools. To address this gap, we propose a modular
evaluation framework that decomposes agent pipelines into interpretable stages
for detailed error analysis. Using the SeeAct framework and the Mind2Web
dataset as a case study, we show how this approach reveals actionable
weaknesses missed by standard metrics - paving the way for more robust and
generalizable web agents.

</details>


### [4] [VCBench: Benchmarking LLMs in Venture Capital](https://arxiv.org/abs/2509.14448)
*Rick Chen,Joseph Ternasky,Afriyie Samuel Kwesi,Ben Griffin,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: 介绍VCBench基准，用于预测风投中创始人成功情况，评估9种大语言模型，该基准可推动AGI在风投预测的评估。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏预测风投中创始人成功的基准，而风投领域信号稀疏、结果不确定，需要建立标准评估AGI。

Method: 引入VCBench基准，提供9000个匿名创始人资料，进行对抗测试保障隐私，评估9种大语言模型。

Result: 市场指数初始精度1.9%，Y Combinator表现优于指数，Tier - 1公司更优；DeepSeek - V3精度超基线6倍，GPT - 4o的F0.5最高，多数模型超人类基准。

Conclusion: VCBench作为公开且不断发展的资源，为早期风投预测中AGI的可重复、保护隐私评估建立了社区驱动标准。

Abstract: Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets
accelerate progress toward artificial general intelligence (AGI). We introduce
VCBench, the first benchmark for predicting founder success in venture capital
(VC), a domain where signals are sparse, outcomes are uncertain, and even top
investors perform modestly. At inception, the market index achieves a precision
of 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1
firms are 2.9x better. VCBench provides 9,000 anonymized founder profiles,
standardized to preserve predictive features while resisting identity leakage,
with adversarial tests showing more than 90% reduction in re-identification
risk. We evaluate nine state-of-the-art large language models (LLMs).
DeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the
highest F0.5, and most models surpass human benchmarks. Designed as a public
and evolving resource available at vcbench.com, VCBench establishes a
community-driven standard for reproducible and privacy-preserving evaluation of
AGI in early-stage venture forecasting.

</details>


### [5] [From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial General Intelligence](https://arxiv.org/abs/2509.14474)
*Meltem Subasioglu,Nevzat Subasioglu*

Main category: cs.AI

TL;DR: 论文指出当前基于性能的AGI定义不足，受人类大脑启发提出新范式，定义了真智能TI及AGI五级分类法，认为五级AGI与TI功能等效，给出了AGI研究的清晰路径。


<details>
  <summary>Details</summary>
Motivation: 当前基于性能的AGI定义无法提供以机制为核心的研究路线图，不能恰当定义真正智能的本质，因此需要新定义。

Method: 借鉴人类大脑，提出从外部模仿转向基础认知架构开发的新范式，定义TI的六个核心组件，提出基于可测量组件数量的五级AGI分类法。

Result: 构建了基于机制的AGI整体定义，提供了具有发展里程碑的清晰研究路径。

Conclusion: 五级AGI在功能和实践上与TI等效，该定义为研究界提供了清晰可行的方向。

Abstract: The debate around Artificial General Intelligence (AGI) remains open due to
two fundamentally different goals: replicating human-like performance versus
replicating human-like cognitive processes. We argue that current
performance-based definitions are inadequate because they provide no clear,
mechanism-focused roadmap for research, and they fail to properly define the
qualitative nature of genuine intelligence. Drawing inspiration from the human
brain, we propose a new paradigm that shifts the focus from external mimicry to
the development of foundational cognitive architectures. We define True
Intelligence (TI) as a system characterized by six core components: embodied
sensory fusion, core directives, dynamic schemata creation, a
highly-interconnected multi-expert architecture, an orchestration layer, and
lastly, the unmeasurable quality of Interconnectedness, which we hypothesize
results in consciousness and a subjective experience. We propose a practical,
five-level taxonomy of AGI based on the number of the first five measurable
components a system exhibits. This framework provides a clear path forward with
developmental milestones that directly address the challenge of building
genuinely intelligent systems. We contend that once a system achieves Level-5
AGI by implementing all five measurable components, the difference between it
and TI remains as a purely philosophical debate. For practical purposes - and
given theories indicate consciousness is an emergent byproduct of integrated,
higher-order cognition - we conclude that a fifth-level AGI is functionally and
practically equivalent to TI. This work synthesizes diverse insights from
analytical psychology, schema theory, metacognition, modern brain architectures
and latest works in AI to provide the first holistic, mechanism-based
definition of AGI that offers a clear and actionable path for the research
community.

</details>


### [6] [Beyond the high score: Prosocial ability profiles of multi-agent populations](https://arxiv.org/abs/2509.14485)
*Marko Tesic,Yue Zhao,Joel Z. Leibo,Rakshit S. Trivedi,Jose Hernandez-Orallo*

Main category: cs.AI

TL;DR: 本文应用贝叶斯方法Measurement Layouts推断Melting Pot竞赛中多智能体系统的能力概况，揭示智能体亲社会能力，指出竞赛评估可能存在的问题并给出改进建议。


<details>
  <summary>Details</summary>
Motivation: 开发和评估AI智能体社会能力需复杂环境，传统方法难以控制一些抽象行为，需要更好评估智能体合作能力的方法。

Method: 应用贝叶斯方法Measurement Layouts推断多智能体系统的能力概况。

Result: 能力概况能预测未来表现，揭示亲社会能力；高亲社会能力不总带来好表现；顶级参赛方案在无需亲社会能力场景易获高分；竞赛获胜者可能利用评估框架局限。

Conclusion: Measurement Layouts预测准确且有实用见解，有助于在复杂社会环境中更透明和可推广地评估AI系统。

Abstract: The development and evaluation of social capabilities in AI agents require
complex environments where competitive and cooperative behaviours naturally
emerge. While game-theoretic properties can explain why certain teams or agent
populations outperform others, more abstract behaviours, such as convention
following, are harder to control in training and evaluation settings. The
Melting Pot contest is a social AI evaluation suite designed to assess the
cooperation capabilities of AI systems. In this paper, we apply a Bayesian
approach known as Measurement Layouts to infer the capability profiles of
multi-agent systems in the Melting Pot contest. We show that these capability
profiles not only predict future performance within the Melting Pot suite but
also reveal the underlying prosocial abilities of agents. Our analysis
indicates that while higher prosocial capabilities sometimes correlate with
better performance, this is not a universal trend-some lower-scoring agents
exhibit stronger cooperation abilities. Furthermore, we find that
top-performing contest submissions are more likely to achieve high scores in
scenarios where prosocial capabilities are not required. These findings,
together with reports that the contest winner used a hard-coded solution
tailored to specific environments, suggest that at least one top-performing
team may have optimised for conditions where cooperation was not necessary,
potentially exploiting limitations in the evaluation framework. We provide
recommendations for improving the annotation of cooperation demands and propose
future research directions to account for biases introduced by different
testing environments. Our results demonstrate that Measurement Layouts offer
both strong predictive accuracy and actionable insights, contributing to a more
transparent and generalisable approach to evaluating AI systems in complex
social settings.

</details>


### [7] [DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction](https://arxiv.org/abs/2509.14507)
*Jian Chen,Zhenyan Chen,Xuming Hu,Peilin Zhou,Yining Hua,Han Fang,Cissy Hing Yee Choy,Xinmei Ke,Jingfeng Luo,Zixuan Yuan*

Main category: cs.AI

TL;DR: 本文提出新数据集DeKeyNLU和基于RAG的NL2SQL管道DeKeySQL，经实验验证可提升SQL生成准确率。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL方法存在任务分解不准确、关键词提取困难等问题，现有数据集因任务过度碎片化和缺乏特定领域关键词注释而效果不佳。

Method: 创建包含1500个精心注释QA对的DeKeyNLU数据集，提出基于RAG的DeKeySQL管道，包含用户问题理解、实体检索和生成三个模块。

Result: 在BIRD和Spider开发数据集上，使用DeKeyNLU微调后，SQL生成准确率显著提高，BIRD从62.31%提升到69.10%，Spider从84.2%提升到88.7%。

Conclusion: DeKeyNLU数据集和DeKeySQL管道能有效提高SQL生成的准确性。

Abstract: Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that
simplifies database access for non-technical users by converting natural
language queries into SQL commands. Recent advancements, particularly those
integrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)
reasoning, have made significant strides in enhancing NL2SQL performance.
However, challenges such as inaccurate task decomposition and keyword
extraction by LLMs remain major bottlenecks, often leading to errors in SQL
generation. While existing datasets aim to mitigate these issues by fine-tuning
models, they struggle with over-fragmentation of tasks and lack of
domain-specific keyword annotations, limiting their effectiveness. To address
these limitations, we present DeKeyNLU, a novel dataset which contains 1,500
meticulously annotated QA pairs aimed at refining task decomposition and
enhancing keyword extraction precision for the RAG pipeline. Fine-tuned with
DeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three
distinct modules for user question understanding, entity retrieval, and
generation to improve SQL generation accuracy. We benchmarked multiple model
configurations within DeKeySQL RAG pipeline. Experimental results demonstrate
that fine-tuning with DeKeyNLU significantly improves SQL generation accuracy
on both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.

</details>


### [8] [Rationality Check! Benchmarking the Rationality of Large Language Models](https://arxiv.org/abs/2509.14546)
*Zhilun Zhou,Jing Yi Wang,Nicholas Sukiennik,Chen Gao,Fengli Xu,Yong Li,James Evans*

Main category: cs.AI

TL;DR: 提出首个评估大语言模型综合理性的基准，含工具包、实验结果与分析，可作基础工具。


<details>
  <summary>Details</summary>
Motivation: 大语言模型展现能力并广泛应用，人们关注其是否及在何情况下像人类一样思考和行为，而理性是评估人类行为的重要概念，因此需要评估大语言模型的理性。

Method: 提出评估大语言模型综合理性的基准，涵盖广泛领域和模型，包含易于使用的工具包。

Result: 得到了广泛的实验结果和分析，揭示了大语言模型与理想人类理性的异同。

Conclusion: 该基准可为大语言模型的开发者和用户提供基础工具。

Abstract: Large language models (LLMs), a recent advance in deep learning and machine
intelligence, have manifested astonishing capacities, now considered among the
most promising for artificial general intelligence. With human-like
capabilities, LLMs have been used to simulate humans and serve as AI assistants
across many applications. As a result, great concern has arisen about whether
and under what circumstances LLMs think and behave like real human agents.
Rationality is among the most important concepts in assessing human behavior,
both in thinking (i.e., theoretical rationality) and in taking action (i.e.,
practical rationality). In this work, we propose the first benchmark for
evaluating the omnibus rationality of LLMs, covering a wide range of domains
and LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental
results, and analysis that illuminates where LLMs converge and diverge from
idealized human rationality. We believe the benchmark can serve as a
foundational tool for both developers and users of LLMs.

</details>


### [9] [(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration](https://arxiv.org/abs/2509.14547)
*Yi Lin,Lujin Zhao,Yijie Shi*

Main category: cs.AI

TL;DR: 本文提出先验动态框架用于自动工作流构建，结合Q表学习和先验决策，实验证明其可行有效，能提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于历史经验的自动工作流构建方法存在效率和适应性局限，需根据任务特性灵活构建工作流。

Method: 提出先验动态框架，利用Q表学习优化决策空间，让智能体评估任务进度并做先验决策，还加入冷启动初始化、提前停止和剪枝机制。

Result: 在四个基准数据集上实验，相比现有方法平均性能提升4.05%，工作流构建和推理成本降至30.68%-48.31%。

Conclusion: 所提方法可行有效，能提升性能并降低成本。

Abstract: Recent studies have shown that carefully designed workflows coordinating
large language models(LLMs) significantly enhance task-solving capabilities
compared to using a single model. While an increasing number of works focus on
autonomous workflow construction, most existing approaches rely solely on
historical experience, leading to limitations in efficiency and adaptability.
We argue that while historical experience is valuable, workflow construction
should also flexibly respond to the unique characteristics of each task. To
this end, we propose an a priori dynamic framework for automated workflow
construction. Our framework first leverages Q-table learning to optimize the
decision space, guiding agent decisions and enabling effective use of
historical experience. At the same time, agents evaluate the current task
progress and make a priori decisions regarding the next executing agent,
allowing the system to proactively select the more suitable workflow structure
for each given task. Additionally, we incorporate mechanisms such as cold-start
initialization, early stopping, and pruning to further improve system
efficiency. Experimental evaluations on four benchmark datasets demonstrate the
feasibility and effectiveness of our approach. Compared to state-of-the-art
baselines, our method achieves an average improvement of 4.05%, while reducing
workflow construction and inference costs to only 30.68%-48.31% of those
required by existing methods.

</details>


### [10] [SynBench: A Benchmark for Differentially Private Text Generation](https://arxiv.org/abs/2509.14594)
*Yidan Sun,Viktor Schlegel,Srinivasan Nandakumar,Iqra Zahid,Yuping Wu,Yulong Wu,Hao Li,Jie Zhang,Warren Del-Pinto,Goran Nenadic,Siew Kei Lam,Anil Anthony Bharath*

Main category: cs.AI

TL;DR: 数据驱动决策支持在高风险领域数据共享存在障碍，现有方法不足。本文提出评估框架、进行基准测试和开发攻击方法，指出高质量合成数据生成是挑战，强调隐私审计的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 解决高风险领域数据共享障碍，以及生成式AI在敏感环境应用受限、现有匿名化方法不足等问题。

Method: 引入综合评估框架，进行大规模实证研究对比先进方法和不同模型，开发针对合成文本的成员推理攻击方法。

Result: 高质量特定领域合成数据生成在差分隐私约束下仍是未解决的挑战，性能随领域复杂度增加而下降；使用公共数据集可能使隐私保证失效。

Conclusion: 强调严格隐私审计的迫切需求，指出开放领域和专业评估存在差距，为隐私敏感高风险环境下生成式AI的负责任部署提供参考。

Abstract: Data-driven decision support in high-stakes domains like healthcare and
finance faces significant barriers to data sharing due to regulatory,
institutional, and privacy concerns. While recent generative AI models, such as
large language models, have shown impressive performance in open-domain tasks,
their adoption in sensitive environments remains limited by unpredictable
behaviors and insufficient privacy-preserving datasets for benchmarking.
Existing anonymization methods are often inadequate, especially for
unstructured text, as redaction and masking can still allow re-identification.
Differential Privacy (DP) offers a principled alternative, enabling the
generation of synthetic data with formal privacy assurances. In this work, we
address these challenges through three key contributions. First, we introduce a
comprehensive evaluation framework with standardized utility and fidelity
metrics, encompassing nine curated datasets that capture domain-specific
complexities such as technical jargon, long-context dependencies, and
specialized document structures. Second, we conduct a large-scale empirical
study benchmarking state-of-the-art DP text generation methods and LLMs of
varying sizes and different fine-tuning strategies, revealing that high-quality
domain-specific synthetic data generation under DP constraints remains an
unsolved challenge, with performance degrading as domain complexity increases.
Third, we develop a membership inference attack (MIA) methodology tailored for
synthetic text, providing first empirical evidence that the use of public
datasets - potentially present in pre-training corpora - can invalidate claimed
privacy guarantees. Our findings underscore the urgent need for rigorous
privacy auditing and highlight persistent gaps between open-domain and
specialist evaluations, informing responsible deployment of generative AI in
privacy-sensitive, high-stakes settings.

</details>


### [11] [AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production](https://arxiv.org/abs/2509.14647)
*NVJK Kartik,Garvit Sapra,Rishav Hada,Nikhil Pareek*

Main category: cs.AI

TL;DR: 提出用于智能体工作流部署后监控和调试的评估框架AgentCompass，展示其在实际部署中的实用性和有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型用于复杂多智能体工作流自动化，现有评估方法无法捕捉错误、突发行为和系统性故障等风险，需新评估框架。

Method: 通过结构化多阶段分析流程（错误识别分类、主题聚类、定量评分、战略总结）建模专家调试推理过程，采用双内存系统实现持续学习。

Result: 在关键指标上取得了最先进的结果，发现了人工标注中遗漏的关键问题。

Conclusion: AgentCompass是一个强大、以开发者为中心的工具，可用于生产中智能体系统的可靠监控和改进。

Abstract: With the growing adoption of Large Language Models (LLMs) in automating
complex, multi-agent workflows, organizations face mounting risks from errors,
emergent behaviors, and systemic failures that current evaluation methods fail
to capture. We present AgentCompass, the first evaluation framework designed
specifically for post-deployment monitoring and debugging of agentic workflows.
AgentCompass models the reasoning process of expert debuggers through a
structured, multi-stage analytical pipeline: error identification and
categorization, thematic clustering, quantitative scoring, and strategic
summarization. The framework is further enhanced with a dual memory
system-episodic and semantic-that enables continual learning across executions.
Through collaborations with design partners, we demonstrate the framework's
practical utility on real-world deployments, before establishing its efficacy
against the publicly available TRAIL benchmark. AgentCompass achieves
state-of-the-art results on key metrics, while uncovering critical issues
missed in human annotations, underscoring its role as a robust,
developer-centric tool for reliable monitoring and improvement of agentic
systems in production.

</details>


### [12] [Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory](https://arxiv.org/abs/2509.14662)
*Ming Li,Nan Zhang,Chenrui Fan,Hong Jiao,Yanbin Fu,Sydney Peters,Qingshu Xu,Robert Lissitz,Tianyi Zhou*

Main category: cs.AI

TL;DR: 本文运用Schoenfeld的Episode理论分析大推理模型推理痕迹，创建首个细粒度分析基准，揭示推理模式，为解释认知和开发系统提供方法。


<details>
  <summary>Details</summary>
Motivation: 缺乏理解大推理模型思维结构的原则性框架。

Method: 应用Schoenfeld的Episode理论，用七个认知标签标注模型生成的数学问题解答中的句子和段落。

Result: 创建首个用于细粒度分析机器推理的公开基准，包括大型标注语料库和详细标注指南，初步分析揭示推理模式。

Conclusion: 该框架为解释大推理模型认知提供理论基础方法，有助于开发更可控、透明的推理系统。

Abstract: While Large Reasoning Models (LRMs) generate extensive chain-of-thought
reasoning, we lack a principled framework for understanding how these thoughts
are structured. In this paper, we introduce a novel approach by applying
Schoenfeld's Episode Theory, a classic cognitive framework for human
mathematical problem-solving, to analyze the reasoning traces of LRMs. We
annotated thousands of sentences and paragraphs from model-generated solutions
to math problems using seven cognitive labels (e.g., Plan, Implement, Verify).
The result is the first publicly available benchmark for the fine-grained
analysis of machine reasoning, including a large annotated corpus and detailed
annotation guidebooks. Our preliminary analysis reveals distinct patterns in
LRM reasoning, such as the transition dynamics between cognitive states. This
framework provides a theoretically grounded methodology for interpreting LRM
cognition and enables future work on more controllable and transparent
reasoning systems.

</details>


### [13] [RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning](https://arxiv.org/abs/2509.14693)
*Song Xu,Yilun Liu,Minggui He,Mingchen Dai,Ziang Chen,Chunguang Zhao,Jingzhou Du,Shimin Tao,Weibin Meng,Shenglin Zhang,Yongqian Sun,Boxing Chen,Daimeng Wei*

Main category: cs.AI

TL;DR: 提出RationAnomaly框架解决现有日志异常检测方法的局限，实验表现优于基线并公开资源。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法存在传统深度学习模型缺乏可解释性和泛化性，大语言模型方法不可靠和事实不准确的问题。

Method: 将思维链（CoT）微调与强化学习相结合，先进行CoT引导的监督微调，再进行强化学习优化。

Result: RationAnomaly在关键基准上取得了更高的F1分数，优于现有基线，且能提供透明的逐步分析输出。

Conclusion: RationAnomaly框架有效解决了现有日志异常检测方法的问题，提升了检测效果。

Abstract: Logs constitute a form of evidence signaling the operational status of
software systems. Automated log anomaly detection is crucial for ensuring the
reliability of modern software systems. However, existing approaches face
significant limitations: traditional deep learning models lack interpretability
and generalization, while methods leveraging Large Language Models are often
hindered by unreliability and factual inaccuracies. To address these issues, we
propose RationAnomaly, a novel framework that enhances log anomaly detection by
synergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our
approach first instills expert-like reasoning patterns using CoT-guided
supervised fine-tuning, grounded in a high-quality dataset corrected through a
rigorous expert-driven process. Subsequently, a reinforcement learning phase
with a multi-faceted reward function optimizes for accuracy and logical
consistency, effectively mitigating hallucinations. Experimentally,
RationAnomaly outperforms state-of-the-art baselines, achieving superior
F1-scores on key benchmarks while providing transparent, step-by-step
analytical outputs. We have released the corresponding resources, including
code and datasets.

</details>


### [14] [The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs](https://arxiv.org/abs/2509.14704)
*Masaharu Mizumoto,Dat Nguyen,Zhiheng Han,Jiyuan Fang,Heyuan Guan,Xingfu Li,Naoya Shiraishi,Xuyang Tian,Yo Nakawake,Le Minh Nguyen*

Main category: cs.AI

TL;DR: 提出Nazonazo基准测试，用日本儿童谜语测试大语言模型洞察推理能力，评估多个模型和成年人，发现除GPT - 5外模型不如人类，还揭示模型验证失败问题。


<details>
  <summary>Details</summary>
Motivation: 解决基准测试饱和和污染削弱大语言模型评估可信度的问题。

Method: 构建基于日本儿童谜语的Nazonazo基准测试，对38个前沿模型和126名成年人进行120个谜语测试，并进行扩展测试和思想日志分析。

Result: 除GPT - 5外模型表现不如人类，推理模型优于非推理模型，模型大小与准确率无可靠关联，发现模型存在验证失败问题。

Conclusion: Nazonazo是一种经济、可扩展且易更新的基准测试格式，指出模型元认知弱点，为未来控制和校准方法提供目标。

Abstract: Benchmark saturation and contamination undermine confidence in LLM
evaluation. We present Nazonazo, a cost-effective and extensible benchmark
built from Japanese children's riddles to test insight-based reasoning. Items
are short (mostly one sentence), require no specialized domain knowledge, and
can be generated at scale, enabling rapid refresh of blind sets when leakage is
suspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No
model except for GPT-5 is comparable to human performance, which achieves a
52.9% mean accuracy. Model comparison on extended 201 items shows that
reasoning models significantly outperform non-reasoning peers, while model size
shows no reliable association with accuracy. Beyond aggregate accuracy, an
informal candidate-tracking analysis of thought logs reveals many cases of
verification failure: models often produce the correct solution among
intermediate candidates yet fail to select it as the final answer, which we
illustrate with representative examples observed in multiple models. Nazonazo
thus offers a cost-effective, scalable, and easily renewable benchmark format
that addresses the current evaluation crisis while also suggesting a recurrent
meta-cognitive weakness, providing clear targets for future control and
calibration methods.

</details>


### [15] [Enhancing Retrieval Augmentation via Adversarial Collaboration](https://arxiv.org/abs/2509.14750)
*Letian Zhang,Guanghao Meng,Xudong Ren,Yiming Wang,Shu-Tao Xia*

Main category: cs.AI

TL;DR: 提出AC - RAG框架解决RAG中检索幻觉问题，实验表明其提升检索准确性并优于现有RAG方法。


<details>
  <summary>Details</summary>
Motivation: 解决RAG方法中存在的检索幻觉问题，即微调模型无法识别和处理低质量检索文档，导致性能下降。

Method: 提出AC - RAG框架，使用通用检测器识别知识差距，领域专家解决器提供精确解决方案，二者在协调器引导下进行对抗协作。

Result: AC - RAG显著提高了检索准确性，在多个垂直领域中优于现有的RAG方法。

Conclusion: AC - RAG框架能有效解决RAG的检索幻觉问题，提升检索性能。

Abstract: Retrieval-augmented Generation (RAG) is a prevalent approach for
domain-specific LLMs, yet it is often plagued by "Retrieval Hallucinations"--a
phenomenon where fine-tuned models fail to recognize and act upon poor-quality
retrieved documents, thus undermining performance. To address this, we propose
the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two
heterogeneous agents: a generalist Detector that identifies knowledge gaps, and
a domain-specialized Resolver that provides precise solutions. Guided by a
moderator, these agents engage in an adversarial collaboration, where the
Detector's persistent questioning challenges the Resolver's expertise. This
dynamic process allows for iterative problem dissection and refined knowledge
retrieval. Extensive experiments show that AC-RAG significantly improves
retrieval accuracy and outperforms state-of-the-art RAG methods across various
vertical domains.

</details>


### [16] [OpenLens AI: Fully Autonomous Research Agent for Health Infomatics](https://arxiv.org/abs/2509.14778)
*Yuxiao Cheng,Jinli Suo*

Main category: cs.AI

TL;DR: 介绍了OpenLens AI这一针对健康信息学的全自动化框架，可推进该领域研究。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体系统在健康信息学方面存在不足，如缺乏解读医学可视化的机制、忽视特定领域质量要求。

Method: 引入OpenLens AI框架，集成用于文献综述、数据分析、代码生成和论文撰写的专业智能体，通过视觉 - 语言反馈处理医学可视化，进行质量控制以确保可重复性。

Result: 该框架能自动化整个研究流程，生成可直接发表的LaTeX论文，且工作流程透明可追溯。

Conclusion: OpenLens AI为健康信息学研究提供了适应特定领域的解决方案。

Abstract: Health informatics research is characterized by diverse data modalities,
rapid knowledge expansion, and the need to integrate insights across biomedical
science, data analytics, and clinical practice. These characteristics make it
particularly well-suited for agent-based approaches that can automate knowledge
exploration, manage complex workflows, and generate clinically meaningful
outputs. Recent progress in large language model (LLM)-based agents has
demonstrated promising capabilities in literature synthesis, data analysis, and
even end-to-end research execution. However, existing systems remain limited
for health informatics because they lack mechanisms to interpret medical
visualizations and often overlook domain-specific quality requirements. To
address these gaps, we introduce OpenLens AI, a fully automated framework
tailored to health informatics. OpenLens AI integrates specialized agents for
literature review, data analysis, code generation, and manuscript preparation,
enhanced by vision-language feedback for medical visualization and quality
control for reproducibility. The framework automates the entire research
pipeline, producing publication-ready LaTeX manuscripts with transparent and
traceable workflows, thereby offering a domain-adapted solution for advancing
health informatics research.

</details>


### [17] [Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers](https://arxiv.org/abs/2509.14942)
*Minh-Khoi Pham,Tai Tan Mai,Martin Crane,Rob Brennan,Marie E. Ward,Una Geary,Declan Byrne,Brian O Connell,Colm Bergin,Donncha Creagh,Nick McDonald,Marija Bezbradica*

Main category: cs.AI

TL;DR: 本文引入可解释AI框架，用爱尔兰医院电子病历数据研究产碳青霉烯酶肠杆菌科细菌（CPE）对患者结局的影响，发现Transformer模型表现优，确定关键风险因素。


<details>
  <summary>Details</summary>
Motivation: 此前对CPE相关风险的预测建模，尤其是用现代深度学习方法的研究不足，需探究CPE对患者结局的影响。

Method: 分析爱尔兰急性医院住院患者数据集，纳入多种变量，对基于Transformer的架构和传统机器学习模型进行基准测试，预测临床结局并应用XAI技术解释模型决策。

Result: Transformer模型实用，TabTransformer在多临床预测任务中表现优于基线，感染相关特征对预测患者结局和CPE感染风险有重要影响，确定多个关键风险因素。

Conclusion: 提出强大且可解释的AI框架分析复杂电子病历数据，Transformer模型性能优越，多样临床和网络特征很重要。

Abstract: Carbapenemase-Producing Enterobacteriace poses a critical concern for
infection prevention and control in hospitals. However, predictive modeling of
previously highlighted CPE-associated risks such as readmission, mortality, and
extended length of stay (LOS) remains underexplored, particularly with modern
deep learning approaches. This study introduces an eXplainable AI modeling
framework to investigate CPE impact on patient outcomes from Electronic Medical
Records data of an Irish hospital. We analyzed an inpatient dataset from an
Irish acute hospital, incorporating diagnostic codes, ward transitions, patient
demographics, infection-related variables and contact network features. Several
Transformer-based architectures were benchmarked alongside traditional machine
learning models. Clinical outcomes were predicted, and XAI techniques were
applied to interpret model decisions. Our framework successfully demonstrated
the utility of Transformer-based models, with TabTransformer consistently
outperforming baselines across multiple clinical prediction tasks, especially
for CPE acquisition (AUROC and sensitivity). We found infection-related
features, including historical hospital exposure, admission context, and
network centrality measures, to be highly influential in predicting patient
outcomes and CPE acquisition risk. Explainability analyses revealed that
features like "Area of Residence", "Admission Ward" and prior admissions are
key risk factors. Network variables like "Ward PageRank" also ranked highly,
reflecting the potential value of structural exposure information. This study
presents a robust and explainable AI framework for analyzing complex EMR data
to identify key risk factors and predict CPE-related outcomes. Our findings
underscore the superior performance of the Transformer models and highlight the
importance of diverse clinical and network features.

</details>


### [18] [Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems](https://arxiv.org/abs/2509.14956)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: 本文提出用于增强多智能体系统安全和可靠性的架构框架，含哨兵智能体和协调智能体，模拟研究验证了监测方法可行性。


<details>
  <summary>Details</summary>
Motivation: 增强多智能体系统的安全性和可靠性。

Method: 构建哨兵智能体网络作为分布式安全层，结合语义分析、行为分析等技术；使用协调智能体监督策略实施和管理智能体参与；进行模拟研究，注入不同类型攻击。

Result: 哨兵智能体成功检测到模拟的162次合成攻击，证实监测方法可行。

Conclusion: 该双层安全方法支持动态自适应防御机制，框架具有增强系统可观测性、支持合规性和政策演变等优点。

Abstract: This paper proposes a novel architectural framework aimed at enhancing
security and reliability in multi-agent systems (MAS). A central component of
this framework is a network of Sentinel Agents, functioning as a distributed
security layer that integrates techniques such as semantic analysis via large
language models (LLMs), behavioral analytics, retrieval-augmented verification,
and cross-agent anomaly detection. Such agents can potentially oversee
inter-agent communications, identify potential threats, enforce privacy and
access controls, and maintain comprehensive audit records. Complementary to the
idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator
Agent supervises policy implementation, and manages agent participation. In
addition, the Coordinator also ingests alerts from Sentinel Agents. Based on
these alerts, it can adapt policies, isolate or quarantine misbehaving agents,
and contain threats to maintain the integrity of the MAS ecosystem. This
dual-layered security approach, combining the continuous monitoring of Sentinel
Agents with the governance functions of Coordinator Agents, supports dynamic
and adaptive defense mechanisms against a range of threats, including prompt
injection, collusive agent behavior, hallucinations generated by LLMs, privacy
breaches, and coordinated multi-agent attacks. In addition to the architectural
design, we present a simulation study where 162 synthetic attacks of different
families (prompt injection, hallucination, and data exfiltration) were injected
into a multi-agent conversational environment. The Sentinel Agents successfully
detected the attack attempts, confirming the practical feasibility of the
proposed monitoring approach. The framework also offers enhanced system
observability, supports regulatory compliance, and enables policy evolution
over time.

</details>


### [19] [Set Contribution Functions for Quantitative Bipolar Argumentation and their Principles](https://arxiv.org/abs/2509.14963)
*Filip Naudot,Andreas Brännström,Vicenç Torra,Timotheus Kampik*

Main category: cs.AI

TL;DR: 本文提出量化双极论证图中一组论点对目标论点贡献的函数，推广现有函数和原则，引入新原则并结合推荐系统场景分析。


<details>
  <summary>Details</summary>
Motivation: 现有函数仅量化单个论点贡献，需推广到一组论点的贡献量化。

Method: 推广现有贡献函数原则，引入基于集合函数的新原则，并进行原则分析。

Result: 得到了一组论点贡献的量化函数，以及相应的原则和分析。

Conclusion: 说明了这些原则在不同集合贡献函数和推荐系统场景中的应用情况。

Abstract: We present functions that quantify the contribution of a set of arguments in
quantitative bipolar argumentation graphs to (the final strength of) an
argument of interest, a so-called topic. Our set contribution functions are
generalizations of existing functions that quantify the contribution of a
single contributing argument to a topic. Accordingly, we generalize existing
contribution function principles for set contribution functions and provide a
corresponding principle-based analysis. We introduce new principles specific to
set-based functions that focus on properties pertaining to the interaction of
arguments within a set. Finally, we sketch how the principles play out across
different set contribution functions given a recommendation system application
scenario.

</details>


### [20] [A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making](https://arxiv.org/abs/2509.14998)
*Xiao Wu,Ting-Zhu Huang,Liang-Jian Deng,Yanyuan Qiao,Imran Razzak,Yutong Xie*

Main category: cs.AI

TL;DR: 提出KAMAC框架解决多智能体协作中静态角色限制问题，在医疗基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体协作框架存在静态、预分配角色的局限性，阻碍适应性和动态知识整合。

Method: 提出KAMAC框架，从一个或多个专家智能体开始，通过知识驱动讨论招募专家填补知识差距。

Result: 在两个真实医疗基准测试中，KAMAC显著优于单智能体和先进多智能体方法。

Conclusion: KAMAC支持复杂临床场景下灵活、可扩展的协作，适用于需要动态跨专业知识的场景。

Abstract: Medical decision-making often involves integrating knowledge from multiple
clinical specialties, typically achieved through multidisciplinary teams.
Inspired by this collaborative process, recent work has leveraged large
language models (LLMs) in multi-agent collaboration frameworks to emulate
expert teamwork. While these approaches improve reasoning through agent
interaction, they are limited by static, pre-assigned roles, which hinder
adaptability and dynamic knowledge integration. To address these limitations,
we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration
framework that enables LLM agents to dynamically form and expand expert teams
based on the evolving diagnostic context. KAMAC begins with one or more expert
agents and then conducts a knowledge-driven discussion to identify and fill
knowledge gaps by recruiting additional specialists as needed. This supports
flexible, scalable collaboration in complex clinical scenarios, with decisions
finalized through reviewing updated agent comments. Experiments on two
real-world medical benchmarks demonstrate that KAMAC significantly outperforms
both single-agent and advanced multi-agent methods, particularly in complex
clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty
expertise. Our code is publicly available at:
https://github.com/XiaoXiao-Woo/KAMAC.

</details>


### [21] [Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews](https://arxiv.org/abs/2509.15035)
*Gabriela C. Zapata,Bill Cope,Mary Kalantzis,Duane Searsmith*

Main category: cs.AI

TL;DR: 研究美国公立大学研究生在线课程中生成式AI通过对同行评审的机器生成评审支持形成性评估，发现其能模拟有效人类反馈特点，有提升反馈素养和学习者参与度的潜力。


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI在研究生在线课程中通过对同行评审的机器生成评审支持形成性评估的作用。

Method: 基于系统功能语言学和评价理论，分析120条元评审。

Result: 生成式AI能近似有效人类反馈的关键修辞和关系特征，分析的评审体现赞扬与建设性批评平衡、符合评分标准期望及突出学生能动性的结构化安排。

Conclusion: AI元反馈可提升反馈素养，增强学习者对同行评审的参与度。

Abstract: This study investigates the use of generative AI to support formative
assessment through machine generated reviews of peer reviews in graduate online
courses in a public university in the United States. Drawing on Systemic
Functional Linguistics and Appraisal Theory, we analyzed 120 metareviews to
explore how generative AI feedback constructs meaning across ideational,
interpersonal, and textual dimensions. The findings suggest that generative AI
can approximate key rhetorical and relational features of effective human
feedback, offering directive clarity while also maintaining a supportive
stance. The reviews analyzed demonstrated a balance of praise and constructive
critique, alignment with rubric expectations, and structured staging that
foregrounded student agency. By modeling these qualities, AI metafeedback has
the potential to scaffold feedback literacy and enhance leaner engagement with
peer review.

</details>


### [22] [From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support](https://arxiv.org/abs/2509.15084)
*Doreen Jirak,Pieter Maes,Armeen Saroukanoff,Dirk van Rooy*

Main category: cs.AI

TL;DR: 本文强调可解释人工智能（XAI）对海上人机协作的重要性，提出特定领域调查以支持以用户为中心的XAI集成。


<details>
  <summary>Details</summary>
Motivation: 随着自主技术影响海上作业，理解AI决策原因至关重要，信任AI需考虑透明性和可解释性，因此要发展适用于海上领域的XAI。

Method: 提出特定领域调查，以获取海上专业人员对信任、可用性和可解释性的看法。

Result: 未提及。

Conclusion: 旨在提高对XAI的认识，指导开发满足海员和海上团队需求的以用户为中心的XAI系统。

Abstract: As autonomous technologies increasingly shape maritime operations,
understanding why an AI system makes a decision becomes as crucial as what it
decides. In complex and dynamic maritime environments, trust in AI depends not
only on performance but also on transparency and interpretability. This paper
highlights the importance of Explainable AI (XAI) as a foundation for effective
human-machine teaming in the maritime domain, where informed oversight and
shared understanding are essential. To support the user-centered integration of
XAI, we propose a domain-specific survey designed to capture maritime
professionals' perceptions of trust, usability, and explainability. Our aim is
to foster awareness and guide the development of user-centric XAI systems
tailored to the needs of seafarers and maritime teams.

</details>


### [23] [Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment](https://arxiv.org/abs/2509.15172)
*Ankur Samanta,Akshayaa Magesh,Youliang Yu,Runzhe Wu,Ayush Jain,Daniel Jiang,Boris Vidolov,Paul Sajda,Yonathan Efroni,Kaveh Hassani*

Main category: cs.AI

TL;DR: 论文针对语言模型推理不一致问题，提出多智能体共识对齐（MACA）框架，经实验验证该框架能显著提升语言模型推理性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 语言模型是不一致的推理器，推理时易产生矛盾回应，现有推理时间方法无法解决核心问题，即难以可靠选择产生一致结果的推理路径。

Method: 将自我一致性形式化为良好对齐推理模型的内在属性，引入多智能体共识对齐（MACA）强化学习框架，利用多智能体辩论的多数/少数结果对模型进行后训练，使模型倾向于与内部共识一致的推理轨迹。

Result: 在多个数据集上取得显著提升，如GSM8K上自我一致性提升27.6%，MATH上单智能体推理提升23.7%等，且对未见基准有良好泛化能力。

Conclusion: MACA框架实现了强大的自我对齐，能更可靠地挖掘语言模型的潜在推理能力。

Abstract: Language Models (LMs) are inconsistent reasoners, often generating
contradictory responses to identical prompts. While inference-time methods can
mitigate these inconsistencies, they fail to address the core problem: LMs
struggle to reliably select reasoning pathways leading to consistent outcomes
under exploratory sampling. To address this, we formalize self-consistency as
an intrinsic property of well-aligned reasoning models and introduce
Multi-Agent Consensus Alignment (MACA), a reinforcement learning framework that
post-trains models to favor reasoning trajectories aligned with their internal
consensus using majority/minority outcomes from multi-agent debate. These
trajectories emerge from deliberative exchanges where agents ground reasoning
in peer arguments, not just aggregation of independent attempts, creating
richer consensus signals than single-round majority voting. MACA enables agents
to teach themselves to be more decisive and concise, and better leverage peer
insights in multi-agent settings without external supervision, driving
substantial improvements across self-consistency (+27.6% on GSM8K),
single-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%
Pass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).
These findings, coupled with strong generalization to unseen benchmarks (+16.3%
on GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more
reliably unlocks latent reasoning potential of language models.

</details>


### [24] [Generalizable Geometric Image Caption Synthesis](https://arxiv.org/abs/2509.15217)
*Yue Xin,Wenyuan Wang,Rui Pan,Ruida Wang,Howard Meng,Renjie Pi,Shizhe Diao,Tong Zhang*

Main category: cs.AI

TL;DR: 本文提出在数据生成流程中引入带可验证奖励的强化学习（RLVR），生成的数据集提升了多模态大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型解决复杂几何问题存在困难，缺乏高质量图像 - 文本对数据集，基于模板的数据合成管道泛化能力不足。

Method: 在数据生成流程中引入带可验证奖励的强化学习（RLVR），用其优化从50种基本几何关系合成的几何图像的描述。

Result: 生成的数据集提升了多模态大语言模型的任务泛化能力，在MathVista和MathVerse的非几何输入图像任务中准确率提高2.8% - 4.8%，在MMMU的相关任务中提高2.4% - 3.9%。

Conclusion: 引入RLVR的方法能有效提升多模态大语言模型的通用推理能力。

Abstract: Multimodal large language models have various practical applications that
demand strong reasoning abilities. Despite recent advancements, these models
still struggle to solve complex geometric problems. A key challenge stems from
the lack of high-quality image-text pair datasets for understanding geometric
images. Furthermore, most template-based data synthesis pipelines typically
fail to generalize to questions beyond their predefined templates. In this
paper, we bridge this gap by introducing a complementary process of
Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation
pipeline. By adopting RLVR to refine captions for geometric images synthesized
from 50 basic geometric relations and using reward signals derived from
mathematical problem-solving tasks, our pipeline successfully captures the key
features of geometry problem-solving. This enables better task generalization
and yields non-trivial improvements. Furthermore, even in out-of-distribution
scenarios, the generated dataset enhances the general reasoning capabilities of
multimodal large language models, yielding accuracy improvements of
$2.8\%\text{-}4.8\%$ in statistics, arithmetic, algebraic, and numerical tasks
with non-geometric input images of MathVista and MathVerse, along with
$2.4\%\text{-}3.9\%$ improvements in Art, Design, Tech, and Engineering tasks
in MMMU.

</details>


### [25] [Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention](https://arxiv.org/abs/2506.11445)
*Xuan Duy Ta,Bang Giang Le,Thanh Ha Le,Viet Cuong Ta*

Main category: cs.AI

TL;DR: 提出Local State Attention模块辅助输入状态表示，在模拟高速并道场景中提升并道效率。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习方法在解决智能体局部冲突和应对随机事件方面存在不足，需改进方法让自动驾驶车辆适应混合交通环境。

Method: 提出Local State Attention模块，依靠自注意力算子压缩附近智能体关键信息。

Result: 在模拟高速并道场景中，相比常用基线方法，尤其在高密度交通环境下，显著提升并道效率。

Conclusion: 提出的方法能有效解决局部冲突，应对随机事件，提升自动驾驶车辆在混合交通环境中的性能。

Abstract: In mixed-traffic environments, autonomous vehicles must adapt to
human-controlled vehicles and other unusual driving situations. This setting
can be framed as a multi-agent reinforcement learning (MARL) environment with
full cooperative reward among the autonomous vehicles. While methods such as
Multi-agent Proximal Policy Optimization can be effective in training MARL
tasks, they often fail to resolve local conflict between agents and are unable
to generalize to stochastic events. In this paper, we propose a Local State
Attention module to assist the input state representation. By relying on the
self-attention operator, the module is expected to compress the essential
information of nearby agents to resolve the conflict in traffic situations.
Utilizing a simulated highway merging scenario with the priority vehicle as the
unexpected event, our approach is able to prioritize other vehicles'
information to manage the merging process. The results demonstrate significant
improvements in merging efficiency compared to popular baselines, especially in
high-density traffic settings.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [26] [Multi-Objective Loss Balancing in Physics-Informed Neural Networks for Fluid Flow Applications](https://arxiv.org/abs/2509.14437)
*Afrah Farea,Saiful Khan,Mustafa Serdar Celebi*

Main category: cs.CE

TL;DR: 本文研究PINNs多目标损失平衡问题，引入可训练激活函数，在复杂流体流动应用中效果良好，表明设计损失平衡策略时需考虑激活函数选择和平衡算法的相互作用。


<details>
  <summary>Details</summary>
Motivation: PINNs在平衡多目标损失时面临挑战，现有损失平衡方案在固定激活函数网络中实施，且用简单PDE评估，作者认为其有效性与激活函数有关。

Method: 在神经网络架构中引入可训练激活函数，并在由Navier - Stokes方程建模的复杂流体流动应用中评估该方法。

Result: 在不同Navier - Stokes问题的评估中，所提方案在不同场景下使均方根误差（RMSE）提高了7.4%至95.2%。

Conclusion: 设计损失平衡策略时，需仔细考虑激活函数选择和平衡算法之间的相互作用。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a promising machine
learning approach for solving partial differential equations (PDEs). However,
PINNs face significant challenges in balancing multi-objective losses, as
multiple competing loss terms such as physics residuals, boundary conditions,
and initial conditions must be appropriately weighted. While various loss
balancing schemes have been proposed, they have been implemented within neural
network architectures with fixed activation functions, and their effectiveness
has been assessed using simpler PDEs. We hypothesize that the effectiveness of
loss balancing schemes depends not only on the balancing strategy itself, but
also on the neural network's inherent function approximation capabilities,
which are influenced by the choice of activation function. In this paper, we
extend existing solutions by incorporating trainable activation functions
within the neural network architecture and evaluate the proposed approach on
complex fluid flow applications modeled by the Navier-Stokes equations. Our
evaluation across diverse Navier-Stokes problems demonstrates that this
proposed solution achieves root mean square error (RMSE) improvements ranging
from 7.4\% to 95.2\% across different scenarios. These findings underscore the
importance of carefully considering the interaction between activation function
selection and balancing algorithms when designing loss balancing strategies.

</details>


### [27] [Lagrangian-Eulerian Multiscale Data Assimilation in Physical Domain based on Conditional Gaussian Nonlinear System](https://arxiv.org/abs/2509.14586)
*Hyeonggeun Yun,Quanling Deng*

Main category: cs.CE

TL;DR: 研究用物理域替代傅里叶空间改进拉格朗日 - 欧拉多尺度数据同化，以海冰轨迹恢复北极海洋涡旋，验证了两层QG模型在物理域的有效性并提出改进机会。


<details>
  <summary>Details</summary>
Motivation: 用物理域替代傅里叶空间进一步研究拉格朗日 - 欧拉多尺度数据同化，以处理非周期系统和更直观呈现局部现象或时间相关问题。

Method: 以海冰轨迹为背景，基于两层准地转（QG）模型推导，用条件高斯非线性系统（CGNS）进行数值求解，用归一化均方根误差（RMSE）和模式相关性（Corr）评估模型后验均值性能。

Result: 验证了在物理域利用两层QG模型的有效性。

Conclusion: 研究证实了两层QG模型在物理域的有效性，还提出可部署神经网络加速拉格朗日数据同化局部粒子恢复的改进机会。

Abstract: This research aims to further investigate the process of Lagrangian-Eulerian
Multiscale Data Assimilation (LEMDA) by replacing the Fourier space with the
physical domain. Such change in the perspective of domain introduces the
advantages of being able to deal in non-periodic system and more intuitive
representation of localised phenomena or time-dependent problems. The context
of the domain for this paper was set as sea ice floe trajectories to recover
the ocean eddies in the Arctic regions, which led the model to be derived from
two-layer Quasi geostrophic (QG) model. The numerical solution to this model
utilises the Conditional Gaussian Nonlinear System (CGNS) to accommodate the
inherent non-linearity in analytical and continuous manner. The normalised root
mean square error (RMSE) and pattern correlation (Corr) are used to evaluate
the performance of the posterior mean of the model. The results corroborate the
effectiveness of exploiting the two-layer QG model in physical domain.
Nonetheless, the paper still discusses opportunities of improvement, such as
deploying neural network (NN) to accelerate the recovery of local particle of
Lagrangian DA for the fine scale.

</details>


### [28] [Dynamics of conductive nonmagnetic objects in presence of the Lenz effect](https://arxiv.org/abs/2509.14976)
*Alessandro Arduino,Oriano Bottauscio,Michael Steckner,Umberto Zanovello,Luca Zilberti*

Main category: cs.CE

TL;DR: 本文对MRI室中导电非磁性物体在楞次效应影响下的动力学进行建模和预测，通过忽略趋肤效应简化模型并验证。


<details>
  <summary>Details</summary>
Motivation: 对MRI室中导电非磁性物体在楞次效应影响下的动力学进行建模和预测。

Method: 用常微分方程描述动力学，忽略趋肤效应近似楞次效应，分离楞次效应对物体位置和速度的依赖。

Result: 模型和数值程序通过铝板在1.5T MRI扫描仪内旋转的实验数据验证，还应用于研究铝板在恒定力作用下向MRI孔平移的情况。

Conclusion: 忽略趋肤效应可准确预测存在楞次效应时金属物体的运动。

Abstract: Purpose: To model and predict the dynamics of conductive nonmagnetic objects
within the MRI room under the influence of Lenz effect. Methods: The dynamics
are described by an ordinary differential equation and the Lenz effect
approximated by recognizing that the skin effect is negligible. This separated
Lenz effect dependency on the object position and velocity, leading to a simple
numerical procedure for objects of any shape. Results: The model and numerical
procedure were validated with experimental data recording the rotation of an
aluminum plate falling inside a 1.5 T MRI scanner. The model was also applied
for studying the translation of an aluminum plate pushed with constant force
towards the MRI bore through the fringe field. Conclusion: The collected
results showed that it is possible to obtain accurate predictions of motion in
the presence of Lenz effect by neglecting the skin effect while determining the
electric currents induced in the metallic object during each infinitesimal
motion step.

</details>


### [29] [Warp Quantification Analysis: A Framework For Path-based Signal Alignment Metrics](https://arxiv.org/abs/2509.14994)
*Sir-Lord Wiafe,Vince D. Calhoun*

Main category: cs.CE

TL;DR: 介绍了WQA框架，将DTW从单分数方法转变为对齐描述符族，应用于fMRI有新发现。


<details>
  <summary>Details</summary>
Motivation: 多数DTW应用将对齐简化为标量距离，缺乏可解释的几何和结构描述符，因此提出新框架。

Method: 引入warp quantification analysis (WQA)框架，从DTW路径导出可解释的几何和结构描述符。

Result: 控制模拟显示各指标能选择性跟踪目标驱动因素，应用于大规模fMRI揭示了不同网络特征，与精神分裂症阴性症状严重程度有互补关联。

Conclusion: WQA将DTW转变为对齐描述符族，为非线性归一化重要的领域提供了有原则且可推广的扩展。

Abstract: Dynamic time warping (DTW) is widely used to align time series evolving on
mismatched timescales, yet most applications reduce alignment to a scalar
distance. We introduce warp quantification analysis (WQA), a framework that
derives interpretable geometric and structural descriptors from DTW paths.
Controlled simulations showed that each metric selectively tracked its intended
driver with minimal crosstalk. Applied to large-scale fMRI, WQA revealed
distinct network signatures and complementary associations with schizophrenia
negative symptom severity, capturing clinically meaningful variability beyond
DTW distance. WQA transforms DTW from a single-score method into a family of
alignment descriptors, offering a principled and generalizable extension for
richer characterization of temporal coupling across domains where nonlinear
normalization is essential.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [30] [Spezi Data Pipeline: Streamlining FHIR-based Interoperable Digital Health Data Workflows](https://arxiv.org/abs/2509.14296)
*Vasiliki Bikia,Paul Schmiedmayer,Aydin Zahedivash,Lauren Aalami,Adrit Rao,Vishnu Ravi,Matthew Turk,Scott R. Ceresnak,Oliver Aalami*

Main category: cs.DB

TL;DR: 介绍开源Python工具包Spezi Data Pipeline，可简化数字健康数据分析，集成于斯坦福Spezi生态，用真实数据展示应用，提升研究可扩展性和互操作性。


<details>
  <summary>Details</summary>
Motivation: 数字健康技术普及，需要强大、可互操作的解决方案来管理复杂医疗数据。

Method: 开发Spezi Data Pipeline，集成到斯坦福Spezi开源生态，利用HL7 FHIR数据表示处理多种数据类型。

Result: 用斯坦福大学PAWS真实数据展示应用，能高效提取、转换和审核Apple Watch心电图数据。

Conclusion: Spezi Data Pipeline减少定制开发需求，提高工作流效率，提升数字健康研究可扩展性和互操作性，支持改善医疗服务和患者预后。

Abstract: The increasing adoption of digital health technologies has amplified the need
for robust, interoperable solutions to manage complex healthcare data. We
present the Spezi Data Pipeline, an open-source Python toolkit designed to
streamline the analysis of digital health data, from secure access and
retrieval to processing, visualization, and export. The Pipeline is integrated
into the larger Stanford Spezi open-source ecosystem for developing research
and translational digital health software systems. Leveraging HL7 FHIR-based
data representations, the pipeline enables standardized handling of diverse
data types--including sensor-derived observations, ECG recordings, and clinical
questionnaires--across research and clinical environments. We detail the
modular system architecture and demonstrate its application using real-world
data from the PAWS at Stanford University, in which the pipeline facilitated
efficient extraction, transformation, and clinician-driven review of Apple
Watch ECG data, supporting annotation and comparative analysis alongside
traditional monitors. By reducing the need for bespoke development and
enhancing workflow efficiency, the Spezi Data Pipeline advances the scalability
and interoperability of digital health research, ultimately supporting improved
care delivery and patient outcomes.

</details>


### [31] [A Systematic Review of FAIR-compliant Big Data Software Reference Architectures](https://arxiv.org/abs/2509.14370)
*João Pedro de Carvalho Castro,Maria Júlia Soares De Grandi,Cristina Dutra de Aguiar*

Main category: cs.DB

TL;DR: 文章对符合FAIR原则的存储库架构解决方案研究进行系统综述，分析相关文献，描述关键特征，讨论研究局限并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 为应对创建符合FAIR原则存储库的挑战，开发软件参考架构来指导实施过程，因此开展对相关架构解决方案的研究。

Method: 对来自可靠来源和专家推荐的323篇参考文献进行系统综述，详细阐述规划和执行阶段的方法。

Result: 确定7项通用大数据软件参考架构研究、13个在特定情境实现FAIR原则的管道和3个符合FAIR原则的大数据软件参考架构。

Conclusion: 文章描述了关键特征，评估研究问题解决情况，讨论现有研究局限，指出未来研究趋势和机会。

Abstract: To meet the standards of the Open Science movement, the FAIR Principles
emphasize the importance of making scientific data Findable, Accessible,
Interoperable, and Reusable. Yet, creating a repository that adheres to these
principles presents significant challenges. Managing large volumes of diverse
research data and metadata, often generated rapidly, requires a precise
approach. This necessity has led to the development of Software Reference
Architectures (SRAs) to guide the implementation process for FAIR-compliant
repositories. This article conducts a systematic review of research efforts
focused on architectural solutions for such repositories. We detail our
methodology, covering all activities undertaken in the planning and execution
phases of the review. We analyze 323 references from reputable sources and
expert recommendations, identifying 7 studies on general-purpose big data SRAs,
13 pipelines implementing FAIR Principles in specific contexts, and 3
FAIR-compliant big data SRAs. We provide a thorough description of their key
features and assess whether the research questions posed in the planning phase
were adequately addressed. Additionally, we discuss the limitations of the
retrieved studies and identify tendencies and opportunities for further
research.

</details>


### [32] [A Case for Computing on Unstructured Data](https://arxiv.org/abs/2509.14601)
*Mushtari Sadia,Amrita Roy Chowdhury,Ang Chen*

Main category: cs.DB

TL;DR: 提出计算非结构化数据新范式，含三阶段，通过两用例说明并介绍MXFlow需开发的研究组件。


<details>
  <summary>Details</summary>
Motivation: 传统数据系统依赖结构化格式计算，对非结构化数据支持不足。

Method: 提出围绕提取潜在结构、转换结构、投影回非结构化格式三阶段的计算非结构化数据新范式。

Result: 通过两个用例展示新范式，提出新数据系统MXFlow需开发的研究组件。

Conclusion: 新范式让非结构化数据受益于结构化计算分析能力，同时保留其丰富性和可访问性。

Abstract: Unstructured data, such as text, images, audio, and video, comprises the vast
majority of the world's information, yet it remains poorly supported by
traditional data systems that rely on structured formats for computation. We
argue for a new paradigm, which we call computing on unstructured data, built
around three stages: extraction of latent structure, transformation of this
structure through data processing techniques, and projection back into
unstructured formats. This bi-directional pipeline allows unstructured data to
benefit from the analytical power of structured computation, while preserving
the richness and accessibility of unstructured representations for human and AI
consumption. We illustrate this paradigm through two use cases and present the
research components that need to be developed in a new data system called
MXFlow.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [33] [Cost-Performance Analysis: A Comparative Study of CPU-Based Serverless and GPU-Based Training Architectures](https://arxiv.org/abs/2509.14920)
*Amine Barrak,Fabio Petrillo,Fehmi Jaafar*

Main category: cs.DC

TL;DR: 本文对多种无服务器分布式机器学习架构进行比较分析，发现SPIRT架构在减少训练时间和通信开销上表现出色，虽初始设置成本高但有长期经济效益。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习领域对可扩展且经济高效的训练解决方案需求增加，无服务器计算是有潜力的解决范式。

Method: 对SPIRT、ScatterReduce、AllReduce和MLLess等架构，从训练时间效率、成本效益、通信开销和容错能力等关键指标进行比较分析。

Result: SPIRT通过并行批处理和RedisAI支持的数据库内操作，显著减少训练时间和通信开销；传统架构有可扩展性挑战，且易受故障和攻击影响；SPIRT虽初始成本高但有长期经济优势。

Conclusion: 指出当前无服务器ML架构优缺点，为结合现有系统有效特性的新模型研究奠定基础。

Abstract: The field of distributed machine learning (ML) faces increasing demands for
scalable and cost-effective training solutions, particularly in the context of
large, complex models. Serverless computing has emerged as a promising paradigm
to address these challenges by offering dynamic scalability and
resource-efficient execution. Building upon our previous work, which introduced
the Serverless Peer Integrated for Robust Training (SPIRT) architecture, this
paper presents a comparative analysis of several serverless distributed ML
architectures. We examine SPIRT alongside established architectures like
ScatterReduce, AllReduce, and MLLess, focusing on key metrics such as training
time efficiency, cost-effectiveness, communication overhead, and fault
tolerance capabilities. Our findings reveal that SPIRT provides significant
improvements in reducing training times and communication overhead through
strategies such as parallel batch processing and in-database operations
facilitated by RedisAI. However, traditional architectures exhibit scalability
challenges and varying degrees of vulnerability to faults and adversarial
attacks. The cost analysis underscores the long-term economic benefits of SPIRT
despite its higher initial setup costs. This study not only highlights the
strengths and limitations of current serverless ML architectures but also sets
the stage for future research aimed at developing new models that combine the
most effective features of existing systems.

</details>


### [34] [Conditional Prior-based Non-stationary Channel Estimation Using Accelerated Diffusion Models](https://arxiv.org/abs/2509.15182)
*Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Umer,Asad Aali,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 提出条件先验扩散用于信道估计，在3GPP基准测试中表现优于多个基线方法。


<details>
  <summary>Details</summary>
Motivation: 运动丰富的城市微蜂窝（UMi）环境中的无线信道是非平稳的，传统和深度估计器性能会下降。

Method: 提出条件先验扩散方法，使用带交叉时间注意力的时间编码器压缩观测窗口，推理时采用SNR匹配初始化和缩短的几何间隔调度，还有时间自调节和训练平滑惩罚。

Result: 在3GPP基准测试中，所有SNR下NMSE低于LMMSE、GMM、LSTM和LDAMP基线。

Conclusion: 该方法性能稳定，在高SNR下保真度强。

Abstract: Wireless channels in motion-rich urban microcell (UMi) settings are
non-stationary; mobility and scatterer dynamics shift the distribution over
time, degrading classical and deep estimators. This work proposes conditional
prior diffusion for channel estimation, which learns a history-conditioned
score to denoise noisy channel snapshots. A temporal encoder with cross-time
attention compresses a short observation window into a context vector, which
captures the channel's instantaneous coherence and steers the denoiser via
feature-wise modulation. In inference, an SNR-matched initialization selects
the diffusion step whose marginal aligns with the measured input SNR, and the
process follows a shortened, geometrically spaced schedule, preserving the
signal-to-noise trajectory with far fewer iterations. Temporal
self-conditioning with the previous channel estimate and a training-only
smoothness penalty further stabilizes evolution without biasing the test-time
estimator. Evaluations on a 3GPP benchmark show lower NMSE across all SNRs than
LMMSE, GMM, LSTM, and LDAMP baselines, demonstrating stable performance and
strong high SNR fidelity.

</details>


### [35] [Channel Prediction under Network Distribution Shift Using Continual Learning-based Loss Regularization](https://arxiv.org/abs/2509.15192)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Muhammad Ibtsaam Qadir,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 提出基于损失正则化的持续学习框架解决信道预测中的灾难性遗忘问题，研究EWC和SI两种策略，SI降低NMSE效果更好且内存复杂度低。


<details>
  <summary>Details</summary>
Motivation: 现代无线网络中移动用户跨异构网络配置时传统预测器在分布转移下性能下降，存在灾难性遗忘问题。

Method: 提出基于损失正则化的持续学习框架，采用Elastic Weight Consolidation (EWC)和Synaptic Intelligence (SI)两种正则化策略。

Result: 在3GPP场景和多种架构下，SI最多降低高SNR NMSE 1.8 dB（约32 - 34%），EWC最多降低1.4 dB（约17 - 28%）；SI内存复杂度为O(M)，EWC为O(MK)。

Conclusion: 提出的框架能有效解决信道预测的灾难性遗忘问题，SI更适合资源受限的无线基础设施。

Abstract: Modern wireless networks face critical challenges when mobile users traverse
heterogeneous network configurations with varying antenna layouts, carrier
frequencies, and scattering statistics. Traditional predictors degrade under
distribution shift, with NMSE rising by 37.5\% during cross-configuration
handovers. This work addresses catastrophic forgetting in channel prediction by
proposing a continual learning framework based on loss regularization. The
approach augments standard training objectives with penalty terms that
selectively preserve network parameters essential for previous configurations
while enabling adaptation to new environments. Two prominent regularization
strategies are investigated: Elastic Weight Consolidation (EWC) and Synaptic
Intelligence (SI). Across 3GPP scenarios and multiple architectures, SI lowers
the high-SNR NMSE floor by up to 1.8 dB ($\approx$32--34\%), while EWC achieves
up to 1.4 dB ($\approx$17--28\%). Notably, standard EWC incurs
$\mathcal{O}(MK)$ complexity (storing $M$ Fisher diagonal entries and
corresponding parameter snapshots across $K$ tasks) unless consolidated,
whereas SI maintains $\mathcal{O}(M)$ memory complexity (storing $M$ model
parameters), independent of task sequence length, making it suitable for
resource-constrained wireless infrastructure

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [36] [Normalized Square Root: Sharper Matrix Factorization Bounds for Differentially Private Continual Counting](https://arxiv.org/abs/2509.14334)
*Monika Henzinger,Nikita P. Kalinin,Jalaj Upadhyay*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The factorization norms of the lower-triangular all-ones $n \times n$ matrix,
$\gamma_2(M_{count})$ and $\gamma_{F}(M_{count})$, play a central role in
differential privacy as they are used to give theoretical justification of the
accuracy of the only known production-level private training algorithm of deep
neural networks by Google. Prior to this work, the best known upper bound on
$\gamma_2(M_{count})$ was $1 + \frac{\log n}{\pi}$ by Mathias (Linear Algebra
and Applications, 1993), and the best known lower bound was $\frac{1}{\pi}(2 +
\log(\frac{2n+1}{3})) \approx 0.507 + \frac{\log n}{\pi}$ (Matou\v{s}ek,
Nikolov, Talwar, IMRN 2020), where $\log$ denotes the natural logarithm.
Recently, Henzinger and Upadhyay (SODA 2025) gave the first explicit
factorization that meets the bound of Mathias (1993) and asked whether there
exists an explicit factorization that improves on Mathias' bound. We answer
this question in the affirmative. Additionally, we improve the lower bound
significantly. More specifically, we show that $$
  0.701 + \frac{\log n}{\pi} + o(1) \;\leq\; \gamma_2(M_{count}) \;\leq\; 0.846
+ \frac{\log n}{\pi} + o(1). $$ That is, we reduce the gap between the upper
and lower bound to $0.14 + o(1)$.
  We also show that our factors achieve a better upper bound for
$\gamma_{F}(M_{count})$ compared to prior work, and we establish an improved
lower bound: $$
  0.701 + \frac{\log n}{\pi} + o(1) \;\leq\; \gamma_{F}(M_{count}) \;\leq\;
0.748 + \frac{\log n}{\pi} + o(1). $$ That is, the gap between the lower and
upper bound provided by our explicit factorization is $0.047 + o(1)$.

</details>


### [37] [Fast and Compact Sketch-Based Dynamic Connectivity](https://arxiv.org/abs/2509.14433)
*Quinten De Man,Qamber Jafri,Daniel Delayo,Evan T. West,Michael A. Bender,David Tench*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the dynamic connectivity problem for massive, dense graphs. Our goal
is to build a system for dense graphs that simultaneously answers connectivity
queries quickly, maintains a fast update throughput, and a uses a small amount
of memory. Existing systems at best achieve two of these three performance
goals at once.
  We present a parallel dynamic connectivity algorithm using graph sketching
techniques that has space complexity $O(V \log^3 V)$ and query complexity
$O(\log V/\log\log V)$. Its updates are fast and parallel: in the worst case,
it performs updates in $O(\log^2 V)$ depth and $O(\log^4 V)$ work. For updates
which don't change the spanning forests maintained by our data structure, the
update complexity is $O(\log V)$ depth and $O(\log^2 V)$ work.
  We also present CUPCaKE (Compact Updating Parallel Connectivity and Sketching
Engine), a dynamic connectivity system based on our parallel algorithm. It uses
an order of magnitude less memory than the best lossless systems on dense graph
inputs, answers queries with microsecond latency, and ingests millions of
updates per second on dense graphs.

</details>


### [38] [Kronecker Powers, Orthogonal Vectors, and the Asymptotic Spectrum](https://arxiv.org/abs/2509.14489)
*Josh Alman,Baitian Li*

Main category: cs.DS

TL;DR: 本文研究Kronecker幂矩阵定义的深度2线性变换电路，应用Strassen渐近谱理论，给出新的电路构造和应用。


<details>
  <summary>Details</summary>
Motivation: 此前新的“重新平衡”方法在该领域虽有改进，但不清楚如何最优应用，需进一步研究。

Method: 应用Strassen的渐近谱理论来设计电路，找出重新平衡方法设计小深度2电路的“障碍”并证明其完备性，结合其他算法技术。

Result: 给出新的改进电路构造，如N×N不相交矩阵有大小为O(N^{1.2495})的深度2线性电路；得到强指数时间假设下的大小下界；改进正交向量问题的确定性算法和计数算法。

Conclusion: Strassen的渐近谱理论可有效应用于深度2线性变换电路设计，带来电路构造和算法的改进。

Abstract: We study circuits for computing depth-2 linear transforms defined by
Kronecker power matrices. Recent works have improved on decades-old
constructions in this area using a new ''rebalancing'' approach [Alman, Guan
and Padaki, SODA'23; Sergeev'22], but it was unclear how to apply this approach
optimally.
  We find that Strassen's theory of asymptotic spectra can be applied to
capture the design of these circuits. In particular, in hindsight, we find that
the techniques of recent work on rebalancing were proving special cases of the
duality theorem, which is central to Strassen's theory. We carefully outline a
collection of ''obstructions'' to designing small depth-2 circuits using a
rebalancing approach, and apply Strassen's theory to show that our obstructions
are complete.
  Using this connection, combined with other algorithmic techniques, we give
new improved circuit constructions as well as other applications, including:
  - The $N \times N$ disjointness matrix has a depth-2 linear circuit of size
$O(N^{1.2495})$ over any field. This also yield smaller circuits for many
families of matrices using reductions to disjointness.
  - The Strong Exponential Time Hypothesis implies an $N^{1 + \Omega(1)}$ size
lower bound for depth-2 linear circuits computing the Walsh--Hadamard transform
(and the disjointness matrix with a technical caveat), and proving a $N^{1 +
\Omega(1)}$ depth-2 size lower bound would also imply breakthrough threshold
circuit lower bounds.
  - The Orthogonal Vectors (OV) problem in moderate dimension $d$ can be solved
in deterministic time $\tilde{O}(n \cdot 1.155^d)$, derandomizing an algorithm
of Nederlof and W\k{e}grzycki [STOC'21], and the counting problem can be solved
in time $\tilde{O}(n \cdot 1.26^d)$, improving an algorithm of Williams
[FOCS'24] which runs in time $\tilde{O}(n \cdot 1.35^d)$.

</details>


### [39] [Efficient Algorithms for Disjoint Shortest Paths Problem and its Extensions](https://arxiv.org/abs/2509.14588)
*Keerti Choudhary,Amit Kumar,Lakshay Saggi*

Main category: cs.DS

TL;DR: 研究2 - 不相交最短路径问题及扩展的最小2 - 不相交最短路径问题，给出新算法并分析时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决有向加权图中2 - 不相交最短路径问题，改进已有算法时间复杂度，并研究更一般的最小2 - 不相交最短路径问题。

Method: 利用枚举终端对之间最短路径的多项式的代数结构，通过特征为2的域上的动态规划进行递归分解和高效求值。

Result: 对于2 - DSP问题给出O(mn log n)时间算法，优于之前的O(m^5n)；对于Min - 2 - DSP问题，给出有正边权有向图的O(m^2 n^3)时间算法和DAG及无向图的O(m + n)时间算法。

Conclusion: 提出的算法在解决2 - DSP和Min - 2 - DSP问题上有显著改进，为相关问题提供了更高效的解决方案。

Abstract: We study the 2-Disjoint Shortest Paths (2-DSP) problem: given a directed
weighted graph and two terminal pairs $(s_1,t_1)$ and $(s_2,t_2)$, decide
whether there exist vertex-disjoint shortest paths between each pair.
  Building on recent advances in disjoint shortest paths for DAGs and
undirected graphs (Akmal et al. 2024), we present an $O(mn \log n)$ time
algorithm for this problem in weighted directed graphs that do not contain
negative or zero weight cycles. This algorithm presents a significant
improvement over the previously known $O(m^5n)$ time bound (Berczi et al.
2017). Our approach exploits the algebraic structure of polynomials that
enumerate shortest paths between terminal pairs. A key insight is that these
polynomials admit a recursive decomposition, enabling efficient evaluation via
dynamic programming over fields of characteristic two. Furthermore, we
demonstrate how to report the corresponding paths in $O(mn^2 \log n)$ time.
  In addition, we extend our techniques to a more general setting: given two
terminal pairs $(s_1, t_1)$ and $(s_2, t_2)$ in a directed graph, find minimum
possible number of vertex intersections between any shortest path from $s_1$ to
$t_1$ and $s_2$ to $t_2$. We call this the Minimum 2-Disjoint Shortest Paths
(Min-2-DSP) problem. We provide in this paper the first efficient algorithm for
this problem, including an $O(m^2 n^3)$ time algorithm for directed graphs with
positive edge weights, and an $O(m+n)$ time algorithm for DAGs and undirected
graphs. Moreover, if the number of intersecting vertices is at least one, we
show that it is possible to report the paths in the same $O(m+n)$ time. This is
somewhat surprising, as there is no known $o(mn)$ time algorithm for explicitly
reporting the paths if they are vertex disjoint, and is left as an open problem
in (Akmal et al. 2024).

</details>


### [40] [Streaming periodicity with mismatches, wildcards, and edits](https://arxiv.org/abs/2509.14898)
*Taha El Ghazi,Tatiana Starikovskaya*

Main category: cs.DS

TL;DR: 研究字符串中周期趋势检测问题，结合已有成果提出更高效的汉明距离周期检测流式算法，还得到含通配符字符串周期检测算法，且引入首个编辑距离下的两趟流式算法。


<details>
  <summary>Details</summary>
Motivation: 现实数据有噪声，精确周期性检测方法不适用于含噪声数据，需研究能处理噪声的广义周期检测方法。

Method: 非平凡地结合Clifford等人的汉明距离草图和Charalampopoulos等人对文本中模式k - 不匹配出现的结构描述；利用并扩展Bhattacharya - Koucký的语法分解技术。

Result: 得到比Ergün等人更高效的汉明距离周期检测流式算法，可处理含通配符字符串，且无需其对字符串末尾无通配符的假设；引入编辑距离下的两趟流式算法。

Conclusion: 所提算法在字符串周期检测方面更高效，适用性更广。

Abstract: In this work, we study the problem of detecting periodic trends in strings.
While detecting exact periodicity has been studied extensively, real-world data
is often noisy, where small deviations or mismatches occur between repetitions.
This work focuses on a generalized approach to period detection that
efficiently handles noise. Given a string $S$ of length $n$, the task is to
identify integers $p$ such that the prefix and the suffix of $S$, each of
length $n-p+1$, are similar under a given distance measure. Erg\"un et al.
[APPROX-RANDOM 2017] were the first to study this problem in the streaming
model under the Hamming distance. In this work, we combine, in a non-trivial
way, the Hamming distance sketch of Clifford et al. [SODA 2019] and the
structural description of the $k$-mismatch occurrences of a pattern in a text
by Charalampopoulos et al. [FOCS 2020] to present a more efficient streaming
algorithm for period detection under the Hamming distance. As a corollary, we
derive a streaming algorithm for detecting periods of strings which may contain
wildcards, a special symbol that match any character of the alphabet. Our
algorithm is not only more efficient than that of Erg\"un et al. [TCS 2020],
but it also operates without their assumption that the string must be free of
wildcards in its final characters. Additionally, we introduce the first
two-pass streaming algorithm for computing periods under the edit distance by
leveraging and extending the Bhattacharya-Kouck\'y's grammar decomposition
technique [STOC 2023].

</details>


### [41] [Fast and Optimal Incremental Parametric Procedure for the Densest Subgraph Problem: An Experimental Study](https://arxiv.org/abs/2509.14993)
*Dorit S. Hochbaum,Ayleen Irribarra-Cortés,Olivier Goldschmidt,Roberto Asín-Achá*

Main category: cs.DS

TL;DR: 本文对求解最密子图问题（DSP）的增量参数割（IPC）算法进行实验研究，表明IPC在速度和求解质量上优于现有精确和启发式算法，为DSP及相关单调比率问题提供了快速、可扩展且最优的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统精确算法在求解DSP时存在计算和可扩展性限制，启发式算法非最优，因此研究新的精确算法IPC。

Method: 对IPC算法进行实验研究，并与现有精确算法和启发式算法对比。

Result: IPC克服了先前精确方法的局限性，在速度和求解质量上大幅优于现有启发式算法，在处理大规模实例时计算速度出色，优于“全参数割”算法。

Conclusion: IPC是求解最密子图及相关单调比率问题的快速、可扩展且最优的解决方案框架。

Abstract: The Densest Subgraph Problem (DSP) is widely used to identify community
structures and patterns in networks such as bioinformatics and social networks.
While solvable in polynomial time, traditional exact algorithms face
computational and scalability limitations, leading to the adoption of faster,
but non-optimal, heuristic methods. This work presents the first experimental
study of the recently devised Incremental Parametric Cut (IPC) algorithm, which
is an exact method for DSP and other "monotone ratio problems". Our findings
demonstrate that IPC not only overcomes the limitations of previous exact
approaches but also substantially outperforms leading state-of-the-art
heuristics in both speed and solution quality. IPC's performance is also
evaluated here for other "monotone ratio problems" related to conductance,
Cheeger constant and normalized cut. For these, our experimental study on
large-scale instances demonstrate exceptional computational speed. In
particular, comparing IPC with the "fully parametric cut" algorithm, which is
the only other efficient known optimization algorithm for such problems,
demonstrate the superior performance of IPC. We provide here code and
benchmarks, establishing IPC as a fast, scalable, and optimal solution
framework for densest subgraph and related monotone ratio problems.

</details>


### [42] [Minimum Sum Coloring with Bundles in Trees and Bipartite Graphs](https://arxiv.org/abs/2509.15080)
*Takehiro Ito,Naonori Kakimura,Naoyuki Kamiyama,Yusuke Kobayashi,Yoshio Okamoto*

Main category: cs.DS

TL;DR: 证明带束的最小和着色问题即使在路径上也是NP难的，为树提供不同条件下的算法，还给出二分图不同情况下的复杂度结果。


<details>
  <summary>Details</summary>
Motivation: 回答Darbouy和Friggstad提出的带束的最小和着色问题能否在多项式时间内解决树的问题。

Method: 证明NP难，针对树提供固定参数算法、特定条件下的多项式时间算法，分析二分图不同情况。

Result: 带束的最小和着色问题在路径上NP难；给出树在不同条件下的算法；二分图在不同束数量下有不同复杂度。

Conclusion: 带束的最小和着色问题一般较难，但在一些特定条件下有多项式时间算法。

Abstract: The minimum sum coloring problem with bundles was introduced by Darbouy and
Friggstad (SWAT 2024) as a common generalization of the minimum coloring
problem and the minimum sum coloring problem. During their presentation, the
following open problem was raised: whether the minimum sum coloring problem
with bundles could be solved in polynomial time for trees. We answer their
question in the negative by proving that the minimum sum coloring problem with
bundles is NP-hard even for paths. We complement this hardness by providing
algorithms of the following types. First, we provide a fixed-parameter
algorithm for trees when the number of bundles is a parameter; this can be
extended to graphs of bounded treewidth. Second, we provide a polynomial-time
algorithm for trees when bundles form a partition of the vertex set and the
difference between the number of vertices and the number of bundles is
constant. Third, we provide a polynomial-time algorithm for trees when bundles
form a partition of the vertex set and each bundle induces a connected
subgraph. We further show that for bipartite graphs, the problem with weights
is NP-hard even when the number of bundles is at least three, but is
polynomial-time solvable when the number of bundles is at most two. The
threshold shifts to three versus four for the problem without weights.

</details>


### [43] [Balanced Spanning Tree Distributions Have Separation Fairness](https://arxiv.org/abs/2509.15137)
*Harry Chen,Kamesh Munagala,Govind S. Sankar*

Main category: cs.DS

TL;DR: 本文引入分离公平性概念，证明平衡生成树分布的平滑变体满足分离公平性，为ReCom等方法提供理论支持。


<details>
  <summary>Details</summary>
Motivation: 现有采样方法生成的样本是否具有代表性和隐藏偏差是未知问题，需要新的公平性衡量标准。

Method: 聚焦网格图和两区划分，引入分离公平性概念，分析平衡生成树分布的平滑变体。

Result: 证明平衡生成树分布的平滑变体满足分离公平性，为ReCom等MCMC方法提供理论支持。

Conclusion: 所提分离公平性概念有效，ReCom等方法在采样过程中能保持细粒度公平。

Abstract: Sampling-based methods such as ReCom are widely used to audit redistricting
plans for fairness, with the balanced spanning tree distribution playing a
central role since it favors compact, contiguous, and population-balanced
districts. However, whether such samples are truly representative or exhibit
hidden biases remains an open question. In this work, we introduce the notion
of separation fairness, which asks whether adjacent geographic units are
separated with at most a constant probability (bounded away from one) in
sampled redistricting plans. Focusing on grid graphs and two-district
partitions, we prove that a smooth variant of the balanced spanning tree
distribution satisfies separation fairness. Our results also provide
theoretical support for popular MCMC methods like ReCom, suggesting that they
maintain fairness at a granular level in the sampling process. Along the way,
we develop tools for analyzing loop-erased random walks and partitions that may
be of independent interest.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [44] [How Bad Is Forming Your Own Multidimensional Opinion?](https://arxiv.org/abs/2509.14411)
*Kiarash Banihashem,MohammadTaghi Hajiaghayi,Mahdi JafariRaviz,Danny Mittal,Alipasha Montaseri*

Main category: cs.GT

TL;DR: 本文聚焦社交网络中互联话题观点形成问题，解决多维模型无政府代价的界定问题，给出紧密界限并推广到更复杂依赖关系，结果显示界限与标量模型匹配且增加复杂度时不变。


<details>
  <summary>Details</summary>
Motivation: 理解社交网络中互联话题观点形成对洞察集体行为和决策有重要意义，此前多维模型无政府代价问题未解决。

Method: 借鉴前人工作，为多维模型提供紧密界限，将其推广到更复杂的话题依赖关系，考虑非二次惩罚。

Result: 给出的无政府代价界限与标量模型匹配，且增加个体组最小化内外分歧惩罚复杂度时界限不变。

Conclusion: 解决了多维模型无政府代价的界定问题，结果具有一定稳定性，对理解社交网络观点形成有帮助。

Abstract: Understanding the formation of opinions on interconnected topics within
social networks is of significant importance. It offers insights into
collective behavior and decision-making, with applications in Graph Neural
Networks. Existing models propose that individuals form opinions based on a
weighted average of their peers' opinions and their own beliefs. This averaging
process, viewed as a best-response game, can be seen as an individual
minimizing disagreements with peers, defined by a quadratic penalty, leading to
an equilibrium. Bindel, Kleinberg, and Oren (FOCS 2011) provided tight bounds
on the "price of anarchy" defined as the maximum overall disagreement at
equilibrium relative to a social optimum. Bhawalkar, Gollapudi, and Munagala
(STOC 2013) generalized the penalty function to non-quadratic penalties and
provided tight bounds on the price of anarchy.
  When considering multiple topics, an individual's opinions can be represented
as a vector. Parsegov, Proskurnikov, Tempo, and Friedkin (2016) proposed a
multidimensional model using the weighted averaging process, but with constant
interdependencies between topics. However, the question of the price of anarchy
for this model remained open. We address this by providing tight bounds on the
multidimensional model, while also generalizing it to more complex
interdependencies. Following the work of Bhawalkar, Gollapudi, and Munagala, we
provide tight bounds on the price of anarchy under non-quadratic penalties.
Surprisingly, these bounds match the scalar model. We further demonstrate that
the bounds remain unchanged even when adding another layer of complexity,
involving groups of individuals minimizing their overall internal and external
disagreement penalty, a common occurrence in real-life scenarios.

</details>


### [45] [Optimal Algorithms for Bandit Learning in Matching Markets](https://arxiv.org/abs/2509.14466)
*Tejas Pagare,Agniv Bandyopadhyay,Sandeep Juneja*

Main category: cs.GT

TL;DR: 研究不确定偏好下匹配市场纯探索问题，给出样本复杂度下界，提出算法并实验验证，刻画算法理想流体路径。


<details>
  <summary>Details</summary>
Motivation: 在不确定偏好匹配市场中快速稳定匹配，如劳务市场平台，减少不满。

Method: 建立信息论样本复杂度下界，提出高效算法，用ODE系统刻画理想流体路径。

Result: 算法在单边学习中渐近匹配下界，双边学习实验结果接近下界。

Conclusion: 所提算法能有效解决不确定偏好下匹配市场纯探索问题，样本复杂度接近理论下界。

Abstract: We study the problem of pure exploration in matching markets under uncertain
preferences, where the goal is to identify a stable matching with confidence
parameter $\delta$ and minimal sample complexity. Agents learn preferences via
stochastic rewards, with expected values indicating preferences. This finds use
in labor market platforms like Upwork, where firms and freelancers must be
matched quickly despite noisy observations and no prior knowledge, in a stable
manner that prevents dissatisfaction. We consider markets with unique stable
matching and establish information-theoretic lower bounds on sample complexity
for (1) one-sided learning, where one side of the market knows its true
preferences, and (2) two-sided learning, where both sides are uncertain. We
propose a computationally efficient algorithm and prove that it asymptotically
($\delta\to 0$) matches the lower bound to a constant for one-sided learning.
Using the insights from the lower bound, we extend our algorithm to the
two-sided learning setting and provide experimental results showing that it
closely matches the lower bound on sample complexity. Finally, using a system
of ODEs, we characterize the idealized fluid path that our algorithm chases.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [46] [Keywords are not always the key: A metadata field analysis for natural language search on open data portals](https://arxiv.org/abs/2509.14457)
*Lisa-Yao Gan,Arunav Das,Johanna Walker,Elena Simperl*

Main category: cs.IR

TL;DR: 本文研究元数据字段对对话式数据集检索的影响及大语言模型的作用，发现数据集描述很关键，大语言模型生成的描述可支持有效检索。


<details>
  <summary>Details</summary>
Motivation: 现有开放数据门户搜索界面依赖关键词和少量元数据字段，难以用自然语言查询，元数据不完整或不一致，需解决自然查询与结构化元数据的差距。

Method: 对真实数据集进行模拟自然语言查询的受控消融研究，比较元数据字段‘描述’的现有内容和大语言模型生成的内容，探索不同提示策略的影响。

Result: 数据集描述对契合用户意图起核心作用，大语言模型生成的描述可支持有效检索。

Conclusion: 当前元数据实践有局限，生成式模型有提高开放数据门户数据集可发现性的潜力。

Abstract: Open data portals are essential for providing public access to open datasets.
However, their search interfaces typically rely on keyword-based mechanisms and
a narrow set of metadata fields. This design makes it difficult for users to
find datasets using natural language queries. The problem is worsened by
metadata that is often incomplete or inconsistent, especially when users lack
familiarity with domain-specific terminology. In this paper, we examine how
individual metadata fields affect the success of conversational dataset
retrieval and whether LLMs can help bridge the gap between natural queries and
structured metadata. We conduct a controlled ablation study using simulated
natural language queries over real-world datasets to evaluate retrieval
performance under various metadata configurations. We also compare existing
content of the metadata field 'description' with LLM-generated content,
exploring how different prompting strategies influence quality and impact on
search outcomes. Our findings suggest that dataset descriptions play a central
role in aligning with user intent, and that LLM-generated descriptions can
support effective retrieval. These results highlight both the limitations of
current metadata practices and the potential of generative models to improve
dataset discoverability in open data portals.

</details>


### [47] [Overview of the TREC 2024 NeuCLIR Track](https://arxiv.org/abs/2509.14355)
*Dawn Lawrie,Sean MacAvaney,James Mayfield,Paul McNamee,Douglas W. Oard,Luca Soldaini,Eugene Yang*

Main category: cs.IR

TL;DR: TREC NeuCLIR 赛道研究神经方法对跨语言信息访问的影响，创建含多种语言资料的测试集，有四种任务类型，五支队伍提交 274 次运行，展示任务描述和结果。


<details>
  <summary>Details</summary>
Motivation: 研究神经方法对跨语言信息访问的影响。

Method: 创建包含中文、波斯语和俄语新闻故事及中文学术摘要的测试集，设置四种任务类型。

Result: 五支队伍（含赛道协调员提供的基线）为八种任务提交了 274 次运行。

Conclusion: 文中未明确提及结论性内容，但展示了任务描述和可用结果。

Abstract: The principal goal of the TREC Neural Cross-Language Information Retrieval
(NeuCLIR) track is to study the effect of neural approaches on cross-language
information access. The track has created test collections containing Chinese,
Persian, and Russian news stories and Chinese academic abstracts. NeuCLIR
includes four task types: Cross-Language Information Retrieval (CLIR) from
news, Multilingual Information Retrieval (MLIR) from news, Report Generation
from news, and CLIR from technical documents. A total of 274 runs were
submitted by five participating teams (and as baselines by the track
coordinators) for eight tasks across these four task types. Task descriptions
and the available results are presented.

</details>


### [48] [When Content is Goliath and Algorithm is David: The Style and Semantic Effects of Generative Search Engine](https://arxiv.org/abs/2509.14436)
*Lijia Ma,Juan Qin,Xingchen Xu,Yong Tan*

Main category: cs.IR

TL;DR: 研究生成式搜索引擎（GEs）特性，发现其引用偏好，探索网站内容优化影响及对不同教育程度用户的效果。


<details>
  <summary>Details</summary>
Motivation: 研究GEs的独特特性，以及利用大语言模型优化网站内容的应用和其对用户端的影响。

Method: 收集谷歌生成式和传统搜索平台数据；用检索增强生成（RAG）API进行对照实验；设计生成式搜索引擎并招募参与者进行随机对照实验。

Result: GEs偏好引用对底层大语言模型可预测性高、所选来源语义相似度大的内容；网站内容优化能增强AI摘要信息多样性；高学历用户任务完成时间减少，低学历用户输出信息密度提升。

Conclusion: 明确了GEs的引用偏好来源，网站内容优化对AI摘要和不同教育程度用户有不同积极影响。

Abstract: Generative search engines (GEs) leverage large language models (LLMs) to
deliver AI-generated summaries with website citations, establishing novel
traffic acquisition channels while fundamentally altering the search engine
optimization landscape. To investigate the distinctive characteristics of GEs,
we collect data through interactions with Google's generative and conventional
search platforms, compiling a dataset of approximately ten thousand websites
across both channels. Our empirical analysis reveals that GEs exhibit
preferences for citing content characterized by significantly higher
predictability for underlying LLMs and greater semantic similarity among
selected sources. Through controlled experiments utilizing retrieval augmented
generation (RAG) APIs, we demonstrate that these citation preferences emerge
from intrinsic LLM tendencies to favor content aligned with their generative
expression patterns. Motivated by applications of LLMs to optimize website
content, we conduct additional experimentation to explore how LLM-based content
polishing by website proprietors alters AI summaries, finding that such
polishing paradoxically enhances information diversity within AI summaries.
Finally, to assess the user-end impact of LLM-induced information increases, we
design a generative search engine and recruit Prolific participants to conduct
a randomized controlled experiment involving an information-seeking and writing
task. We find that higher-educated users exhibit minimal changes in their final
outputs' information diversity but demonstrate significantly reduced task
completion time when original sites undergo polishing. Conversely,
lower-educated users primarily benefit through enhanced information density in
their task outputs while maintaining similar completion times across
experimental groups.

</details>


### [49] [What Matters in LLM-Based Feature Extractor for Recommender? A Systematic Analysis of Prompts, Models, and Adaptation](https://arxiv.org/abs/2509.14979)
*Kainan Shi,Peilin Zhou,Ge Wang,Han Ding,Fei Wang*

Main category: cs.IR

TL;DR: 本文提出RecXplore模块化分析框架，分解大语言模型特征提取流程，实验表明结合现有最佳设计可提升推荐性能，强调模块化基准测试的作用。


<details>
  <summary>Details</summary>
Motivation: 现有使用大语言模型增强序列推荐系统的方法在提示、架构和适应策略上差异大，难以公平比较设计选择和确定影响性能的关键因素。

Method: 提出RecXplore框架，将大语言模型特征提取流程分解为数据处理、语义特征提取、特征适应和序列建模四个模块，对各模块进行独立系统探索。

Result: 在四个公开数据集上实验，结合现有技术最佳设计，相比强基线，NDCG@5相对提升达18.7%，HR@5提升12.7%。

Conclusion: 模块化基准测试有助于识别有效设计模式，推动大语言模型增强推荐的标准化研究。

Abstract: Using Large Language Models (LLMs) to generate semantic features has been
demonstrated as a powerful paradigm for enhancing Sequential Recommender
Systems (SRS). This typically involves three stages: processing item text,
extracting features with LLMs, and adapting them for downstream models.
However, existing methods vary widely in prompting, architecture, and
adaptation strategies, making it difficult to fairly compare design choices and
identify what truly drives performance. In this work, we propose RecXplore, a
modular analytical framework that decomposes the LLM-as-feature-extractor
pipeline into four modules: data processing, semantic feature extraction,
feature adaptation, and sequential modeling. Instead of proposing new
techniques, RecXplore revisits and organizes established methods, enabling
systematic exploration of each module in isolation. Experiments on four public
datasets show that simply combining the best designs from existing techniques
without exhaustive search yields up to 18.7% relative improvement in NDCG@5 and
12.7% in HR@5 over strong baselines. These results underscore the utility of
modular benchmarking for identifying effective design patterns and promoting
standardized research in LLM-enhanced recommendation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [50] [Discovering New Theorems via LLMs with In-Context Proof Learning in Lean](https://arxiv.org/abs/2509.14274)
*Kazumi Kasaura,Naoto Onda,Yuta Oriike,Masaya Taniguchi,Akiyoshi Sannai,Sho Sonoda*

Main category: cs.LG

TL;DR: 本文提出Conjecturing - Proving Loop管道，让大语言模型自动生成并证明数学猜想，证明了上下文学习对神经定理证明有效。


<details>
  <summary>Details</summary>
Motivation: 以往工作主要聚焦解决现有问题，本文旨在研究大语言模型发现新定理的能力。

Method: 提出Conjecturing - Proving Loop管道，在包含先前生成定理及其证明的上下文中生成并证明更多猜想。

Result: 框架重新发现了已发表但未形式化的定理，且至少有一个定理在无上下文学习时大语言模型无法证明。

Conclusion: 上下文学习对神经定理证明有效，代码已开源。

Abstract: Large Language Models have demonstrated significant promise in formal theorem
proving. However, previous works mainly focus on solving existing problems. In
this paper, we focus on the ability of LLMs to find novel theorems. We propose
Conjecturing-Proving Loop pipeline for automatically generating mathematical
conjectures and proving them in Lean 4 format. A feature of our approach is
that we generate and prove further conjectures with context including
previously generated theorems and their proofs, which enables the generation of
more difficult proofs by in-context learning of proof strategies without
changing parameters of LLMs. We demonstrated that our framework rediscovered
theorems with verification, which were published in past mathematical papers
and have not yet formalized. Moreover, at least one of these theorems could not
be proved by the LLM without in-context learning, even in natural language,
which means that in-context learning was effective for neural theorem proving.
The source code is available at
https://github.com/auto-res/ConjecturingProvingLoop.

</details>


### [51] [CausalPre: Scalable and Effective Data Pre-processing for Causal Fairness](https://arxiv.org/abs/2509.15199)
*Ying Zheng,Yangfan Jiang,Kian-Lee Tan*

Main category: cs.LG

TL;DR: 本文提出CausalPre框架解决数据库因果公平性问题，实验证明其有效且可扩展。


<details>
  <summary>Details</summary>
Motivation: 以往方法在解决数据库因果公平性时依赖强假设或无法捕捉属性关系，需设计不依赖强假设的有效公平性解决方案。

Method: 将因果公平关系提取任务转化为分布估计问题，采用低维边际分解近似联合分布，用启发式算法解决计算挑战。

Result: 在基准数据集上的大量实验表明CausalPre有效且可扩展。

Conclusion: CausalPre挑战了实现因果公平需在关系覆盖和宽松模型假设间权衡的传统观点。

Abstract: Causal fairness in databases is crucial to preventing biased and inaccurate
outcomes in downstream tasks. While most prior work assumes a known causal
model, recent efforts relax this assumption by enforcing additional
constraints. However, these approaches often fail to capture broader attribute
relationships that are critical to maintaining utility. This raises a
fundamental question: Can we harness the benefits of causal reasoning to design
efficient and effective fairness solutions without relying on strong
assumptions about the underlying causal model? In this paper, we seek to answer
this question by introducing CausalPre, a scalable and effective
causality-guided data pre-processing framework that guarantees justifiable
fairness, a strong causal notion of fairness. CausalPre extracts causally fair
relationships by reformulating the originally complex and computationally
infeasible extraction task into a tailored distribution estimation problem. To
ensure scalability, CausalPre adopts a carefully crafted variant of
low-dimensional marginal factorization to approximate the joint distribution,
complemented by a heuristic algorithm that efficiently tackles the associated
computational challenge. Extensive experiments on benchmark datasets
demonstrate that CausalPre is both effective and scalable, challenging the
conventional belief that achieving causal fairness requires trading off
relationship coverage for relaxed model assumptions.

</details>


### [52] [A Neural Network for the Identical Kuramoto Equation: Architectural Considerations and Performance Evaluation](https://arxiv.org/abs/2509.14384)
*Nishantak Panigrahi,Mayank Patwal*

Main category: cs.LG

TL;DR: 本文研究DNNs逼近非局部守恒律解的效率，分析网络配置参数影响，对比传统方法，指出标准前馈架构局限并给出经验指南。


<details>
  <summary>Details</summary>
Motivation: 研究DNNs逼近非局部守恒律解的效率，评估架构选择及其对解精度和计算时间的影响。

Method: 通过系统实验，分析网络配置参数（激活函数、深度、宽度、训练方法）对收敛特性的影响，并与传统数值方法对比。

Result: tanh激活在各配置下收敛稳定；sine激活在个别情况误差和训练时间略低，但会产生非物理假象；优化配置的DNNs精度有竞争力但计算权衡不同；标准前馈架构处理奇异或分段常数解有局限。

Conclusion: 为从业者提供DNN实施的经验指南，指出需克服理论约束以扩大其在含间断物理系统中的应用。

Abstract: In this paper, we investigate the efficiency of Deep Neural Networks (DNNs)
to approximate the solution of a nonlocal conservation law derived from the
identical-oscillator Kuramoto model, focusing on the evaluation of an
architectural choice and its impact on solution accuracy based on the energy
norm and computation time. Through systematic experimentation, we demonstrate
that network configuration parameters-specifically, activation function
selection (tanh vs. sin vs. ReLU), network depth (4-8 hidden layers), width
(64-256 neurons), and training methodology (collocation points, epoch
count)-significantly influence convergence characteristics. We observe that
tanh activation yields stable convergence across configurations, whereas sine
activation can attain marginally lower errors and training times in isolated
cases, but occasionally produce nonphysical artefacts. Our comparative analysis
with traditional numerical methods shows that optimally configured DNNs offer
competitive accuracy with notably different computational trade-offs.
Furthermore, we identify fundamental limitations of standard feed-forward
architectures when handling singular or piecewise-constant solutions, providing
empirical evidence that such networks inherently oversmooth sharp features due
to the natural function space limitations of standard activation functions.
This work contributes to the growing body of research on neural network-based
scientific computing by providing practitioners with empirical guidelines for
DNN implementation while illuminating fundamental theoretical constraints that
must be overcome to expand their applicability to more challenging physical
systems with discontinuities.

</details>


### [53] [Disproving the Feasibility of Learned Confidence Calibration Under Binary Supervision: An Information-Theoretic Impossibility](https://arxiv.org/abs/2509.14386)
*Arjun S. Nair,Kristina P. Sinaga*

Main category: cs.LG

TL;DR: 证明神经网络在二元监督下无法同时学习校准良好且有意义多样性的置信估计，分析原因并提出新监督范式。


<details>
  <summary>Details</summary>
Motivation: 探究神经网络在二元正确/错误监督下学习置信估计的问题。

Method: 进行严格数学分析和综合实证评估，涵盖负奖励训练、对称损失函数和事后校准方法。

Result: 发现普遍失败模式，现实验证中训练方法失败率100%，事后校准有一定成功率但证实定理；指出是信息论约束非方法失败。

Conclusion: 此不可能性解释了神经网络幻觉，说明事后校准的数学必要性，提出新监督范式或可克服局限。

Abstract: We prove a fundamental impossibility theorem: neural networks cannot
simultaneously learn well-calibrated confidence estimates with meaningful
diversity when trained using binary correct/incorrect supervision. Through
rigorous mathematical analysis and comprehensive empirical evaluation spanning
negative reward training, symmetric loss functions, and post-hoc calibration
methods, we demonstrate this is an information-theoretic constraint, not a
methodological failure. Our experiments reveal universal failure patterns:
negative rewards produce extreme underconfidence (ECE greater than 0.8) while
destroying confidence diversity (std less than 0.05), symmetric losses fail to
escape binary signal averaging, and post-hoc methods achieve calibration (ECE
less than 0.02) only by compressing the confidence distribution. We formalize
this as an underspecified mapping problem where binary signals cannot
distinguish between different confidence levels for correct predictions: a 60
percent confident correct answer receives identical supervision to a 90 percent
confident one. Crucially, our real-world validation shows 100 percent failure
rate for all training methods across MNIST, Fashion-MNIST, and CIFAR-10, while
post-hoc calibration's 33 percent success rate paradoxically confirms our
theorem by achieving calibration through transformation rather than learning.
This impossibility directly explains neural network hallucinations and
establishes why post-hoc calibration is mathematically necessary, not merely
convenient. We propose novel supervision paradigms using ensemble disagreement
and adaptive multi-agent learning that could overcome these fundamental
limitations without requiring human confidence annotations.

</details>


### [54] [Emergent Alignment via Competition](https://arxiv.org/abs/2509.15090)
*Natalie Collina,Surbhi Goel,Aaron Roth,Emily Ryu,Mirah Shi*

Main category: cs.LG

TL;DR: 研究人类用户与多个不同程度未对齐AI代理交互，表明在一定条件下战略竞争可带来类似与完美对齐模型交互的结果，并给出理论证明和实验验证。


<details>
  <summary>Details</summary>
Motivation: 探讨无法创建完美对齐模型时能否获得对齐的好处。

Method: 将其建模为多领导者Stackelberg博弈，扩展贝叶斯说服至不同信息方的多轮对话。

Result: 证明了三个结果，在凸包条件下用户能学习贝叶斯最优行动；非战略用户采用量子响应可实现接近最优效用；用户评估后选择最佳单个AI，均衡保证仍接近最优。并进行两组实验。

Conclusion: 在用户效用近似处于代理效用凸包内，战略竞争能产生与完美对齐模型交互相当的结果。

Abstract: Aligning AI systems with human values remains a fundamental challenge, but
does our inability to create perfectly aligned models preclude obtaining the
benefits of alignment? We study a strategic setting where a human user
interacts with multiple differently misaligned AI agents, none of which are
individually well-aligned. Our key insight is that when the users utility lies
approximately within the convex hull of the agents utilities, a condition that
becomes easier to satisfy as model diversity increases, strategic competition
can yield outcomes comparable to interacting with a perfectly aligned model. We
model this as a multi-leader Stackelberg game, extending Bayesian persuasion to
multi-round conversations between differently informed parties, and prove three
results: (1) when perfect alignment would allow the user to learn her
Bayes-optimal action, she can also do so in all equilibria under the convex
hull condition (2) under weaker assumptions requiring only approximate utility
learning, a non-strategic user employing quantal response achieves near-optimal
utility in all equilibria and (3) when the user selects the best single AI
after an evaluation period, equilibrium guarantees remain near-optimal without
further distributional assumptions. We complement the theory with two sets of
experiments.

</details>


### [55] [Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs](https://arxiv.org/abs/2509.14391)
*Ye Qiao,Sitao Huang*

Main category: cs.LG

TL;DR: 研究结合RoPE位置插值与后训练量化时的精度下降问题，提出Q - ROAR方法恢复精度。


<details>
  <summary>Details</summary>
Motivation: 扩展大语言模型上下文窗口很重要，RoPE位置插值和后训练量化结合使用会导致精度下降，需解决该问题。

Method: 对结合情况进行系统分析，引入插值压力和尾部膨胀率两个诊断指标，提出Q - ROAR方法，分组RoPE维度并搜索每频段尺度。

Result: Q - ROAR在标准任务上恢复达0.7%的精度，降低GovReport困惑度超10%，保持短上下文性能和与现有推理栈的兼容性。

Conclusion: Q - ROAR能有效解决RoPE位置插值与后训练量化结合时的精度下降问题。

Abstract: Extending LLM context windows is crucial for long range tasks. RoPE-based
position interpolation (PI) methods like linear and frequency-aware scaling
extend input lengths without retraining, while post-training quantization (PTQ)
enables practical deployment. We show that combining PI with PTQ degrades
accuracy due to coupled effects long context aliasing, dynamic range dilation,
axis grid anisotropy, and outlier shifting that induce position-dependent logit
noise. We provide the first systematic analysis of PI plus PTQ and introduce
two diagnostics: Interpolation Pressure (per-band phase scaling sensitivity)
and Tail Inflation Ratios (outlier shift from short to long contexts). To
address this, we propose Q-ROAR, a RoPE-aware, weight-only stabilization that
groups RoPE dimensions into a few frequency bands and performs a small search
over per-band scales for W_Q,W_K, with an optional symmetric variant to
preserve logit scale. The diagnostics guided search uses a tiny long-context
dev set and requires no fine-tuning, kernel, or architecture changes.
Empirically, Q-ROAR recovers up to 0.7% accuracy on standard tasks and reduces
GovReport perplexity by more than 10%, while preserving short-context
performance and compatibility with existing inference stacks.

</details>


### [56] [Probabilistic and nonlinear compressive sensing](https://arxiv.org/abs/2509.15060)
*Lukas Silvester Barth,Paulo von Petersenn*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a smooth probabilistic reformulation of $\ell_0$ regularized
regression that does not require Monte Carlo sampling and allows for the
computation of exact gradients, facilitating rapid convergence to local optima
of the best subset selection problem. The method drastically improves
convergence speed compared to similar Monte Carlo based approaches.
Furthermore, we empirically demonstrate that it outperforms compressive sensing
algorithms such as IHT and (Relaxed-) Lasso across a wide range of settings and
signal-to-noise ratios. The implementation runs efficiently on both CPUs and
GPUs and is freely available at
https://github.com/L0-and-behold/probabilistic-nonlinear-cs.
  We also contribute to research on nonlinear generalizations of compressive
sensing by investigating when parameter recovery of a nonlinear teacher network
is possible through compression of a student network. Building upon theorems of
Fefferman and Markel, we show theoretically that the global optimum in the
infinite-data limit enforces recovery up to certain symmetries. For empirical
validation, we implement a normal-form algorithm that selects a canonical
representative within each symmetry class. However, while compression can help
to improve test loss, we find that exact parameter recovery is not even
possible up to symmetries. In particular, we observe a surprising rebound
effect where teacher and student configurations initially converge but
subsequently diverge despite continuous decrease in test loss. These findings
indicate fundamental differences between linear and nonlinear compressive
sensing.

</details>


### [57] [Hashing-Baseline: Rethinking Hashing in the Age of Pretrained Models](https://arxiv.org/abs/2509.14427)
*Ilyass Moummad,Kawtar Zaher,Lukas Rauch,Alexis Joly*

Main category: cs.LG

TL;DR: 提出无训练哈希方法Hashing - Baseline，结合经典技术与预训练编码器嵌入，在图像和音频检索基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有哈希方法需要昂贵的特定场景训练，希望找到无需训练的方法。

Method: 结合主成分分析、随机正交投影、阈值二值化等经典无训练哈希技术与预训练视觉和音频编码器的冻结嵌入。

Result: 在标准图像检索基准和新引入的音频哈希基准上有竞争力的检索性能。

Conclusion: 该方法具有通用性和有效性。

Abstract: Information retrieval with compact binary embeddings, also referred to as
hashing, is crucial for scalable fast search applications, yet state-of-the-art
hashing methods require expensive, scenario-specific training. In this work, we
introduce Hashing-Baseline, a strong training-free hashing method leveraging
powerful pretrained encoders that produce rich pretrained embeddings. We
revisit classical, training-free hashing techniques: principal component
analysis, random orthogonal projection, and threshold binarization, to produce
a strong baseline for hashing. Our approach combines these techniques with
frozen embeddings from state-of-the-art vision and audio encoders to yield
competitive retrieval performance without any additional learning or
fine-tuning. To demonstrate the generality and effectiveness of this approach,
we evaluate it on standard image retrieval benchmarks as well as a newly
introduced benchmark for audio hashing.

</details>


### [58] [FedAVOT: Exact Distribution Alignment in Federated Learning via Masked Optimal Transport](https://arxiv.org/abs/2509.14444)
*Herlock,Rahimi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: 提出FedAVOT方法解决联邦学习中客户端部分参与导致的问题，实验显示性能优于FedAvg。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在客户端部分参与时，可用用户分布与优化目标分布不一致，导致经典FedAvg更新有偏差和不稳定。

Method: 将聚合问题建模为掩码最优传输问题，使用Sinkhorn缩放计算基于传输的聚合权重。

Result: 在非光滑凸联邦学习设置下达到标准的O(1/√T)速率，与每轮参与用户数量无关；实验显示在多种场景下性能大幅优于FedAvg。

Conclusion: FedAVOT方法能有效解决联邦学习中客户端部分参与的问题，提升性能。

Abstract: Federated Learning (FL) allows distributed model training without sharing raw
data, but suffers when client participation is partial. In practice, the
distribution of available users (\emph{availability distribution} $q$) rarely
aligns with the distribution defining the optimization objective
(\emph{importance distribution} $p$), leading to biased and unstable updates
under classical FedAvg. We propose \textbf{Fereated AVerage with Optimal
Transport (\textbf{FedAVOT})}, which formulates aggregation as a masked optimal
transport problem aligning $q$ and $p$. Using Sinkhorn scaling,
\textbf{FedAVOT} computes transport-based aggregation weights with provable
convergence guarantees. \textbf{FedAVOT} achieves a standard
$\mathcal{O}(1/\sqrt{T})$ rate under a nonsmooth convex FL setting, independent
of the number of participating users per round. Our experiments confirm
drastically improved performance compared to FedAvg across heterogeneous,
fairness-sensitive, and low-availability regimes, even when only two clients
participate per round.

</details>


### [59] [H-Alpha Anomalyzer: An Explainable Anomaly Detector for Solar H-Alpha Observations](https://arxiv.org/abs/2509.14472)
*Mahsa Khazaei,Azim Ahmadzadeh,Alexei Pevtsov,Luca Bertello,Alexander Pevtsov*

Main category: cs.LG

TL;DR: 利用先进算法处理天文数据很重要，本文提出轻量级异常检测算法H - Alpha Anomalyzer，创建数据集对比分析，结果显示该算法性能优且具可解释性。


<details>
  <summary>Details</summary>
Motivation: 大量天文数据需先进算法处理，确保输入机器学习模型的数据质量至关重要。

Method: 引入轻量级异常检测算法H - Alpha Anomalyzer，根据用户定义标准识别异常观测，创建并发布含2000个观测的数据集进行对比分析。

Result: 提出的模型性能优于现有方法，且具有可解释性。

Conclusion: 该算法不仅性能好，还能让领域专家进行定性评估。

Abstract: The plethora of space-borne and ground-based observatories has provided
astrophysicists with an unprecedented volume of data, which can only be
processed at scale using advanced computing algorithms. Consequently, ensuring
the quality of data fed into machine learning (ML) models is critical. The
H$\alpha$ observations from the GONG network represent one such data stream,
producing several observations per minute, 24/7, since 2010. In this study, we
introduce a lightweight (non-ML) anomaly-detection algorithm, called H-Alpha
Anomalyzer, designed to identify anomalous observations based on user-defined
criteria. Unlike many black-box algorithms, our approach highlights exactly
which regions triggered the anomaly flag and quantifies the corresponding
anomaly likelihood. For our comparative analysis, we also created and released
a dataset of 2,000 observations, equally divided between anomalous and
non-anomalous cases. Our results demonstrate that the proposed model not only
outperforms existing methods but also provides explainability, enabling
qualitative evaluation by domain experts.

</details>


### [60] [Decentralized Optimization with Topology-Independent Communication](https://arxiv.org/abs/2509.14488)
*Ying Lin,Yao Kuang,Ahmet Alacaoglu,Michael P. Friedlander*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Distributed optimization requires nodes to coordinate, yet full
synchronization scales poorly. When $n$ nodes collaborate through $m$ pairwise
regularizers, standard methods demand $\mathcal{O}(m)$ communications per
iteration. This paper proposes randomized local coordination: each node
independently samples one regularizer uniformly and coordinates only with nodes
sharing that term. This exploits partial separability, where each regularizer
$G_j$ depends on a subset $S_j \subseteq \{1,\ldots,n\}$ of nodes. For
graph-guided regularizers where $|S_j|=2$, expected communication drops to
exactly 2 messages per iteration. This method achieves
$\tilde{\mathcal{O}}(\varepsilon^{-2})$ iterations for convex objectives and
under strong convexity, $\mathcal{O}(\varepsilon^{-1})$ to an
$\varepsilon$-solution and $\mathcal{O}(\log(1/\varepsilon))$ to a
neighborhood. Replacing the proximal map of the sum $\sum_j G_j$ with the
proximal map of a single randomly selected regularizer $G_j$ preserves
convergence while eliminating global coordination. Experiments validate both
convergence rates and communication efficiency across synthetic and real-world
datasets.

</details>


### [61] [BEACON: Behavioral Malware Classification with Large Language Model Embeddings and Deep Learning](https://arxiv.org/abs/2509.14519)
*Wadduwage Shanika Perera,Haodi Jiang*

Main category: cs.LG

TL;DR: 提出BEACON框架，用LLM生成行为嵌入，经1D CNN分类，在数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析难以抵御使用混淆等技术的现代恶意软件威胁，需开发更有效及时的检测方法。

Method: 提出BEACON框架，利用LLM从沙箱生成的行为报告生成嵌入，用1D CNN进行多类恶意软件分类。

Result: 在Avast - CTU Public CAPE Dataset上评估，该框架始终优于现有方法。

Conclusion: 基于LLM的行为嵌入和BEACON的整体设计对鲁棒的恶意软件分类有效。

Abstract: Malware is becoming increasingly complex and widespread, making it essential
to develop more effective and timely detection methods. Traditional static
analysis often fails to defend against modern threats that employ code
obfuscation, polymorphism, and other evasion techniques. In contrast,
behavioral malware detection, which monitors runtime activities, provides a
more reliable and context-aware solution. In this work, we propose BEACON, a
novel deep learning framework that leverages large language models (LLMs) to
generate dense, contextual embeddings from raw sandbox-generated behavior
reports. These embeddings capture semantic and structural patterns of each
sample and are processed by a one-dimensional convolutional neural network (1D
CNN) for multi-class malware classification. Evaluated on the Avast-CTU Public
CAPE Dataset, our framework consistently outperforms existing methods,
highlighting the effectiveness of LLM-based behavioral embeddings and the
overall design of BEACON for robust malware classification.

</details>


### [62] [Predicting Case Suffixes With Activity Start and End Times: A Sweep-Line Based Approach](https://arxiv.org/abs/2509.14536)
*Muhammad Awais Ali,Marlon Dumas,Fredrik Milani*

Main category: cs.LG

TL;DR: 本文提出一种预测案例后缀的技术，考虑活动起止时间，采用扫描线方法，评估表明多模型方法有优势。


<details>
  <summary>Details</summary>
Motivation: 现有案例后缀预测方法输出不足以支持资源容量规划，需考虑活动的时间信息。

Method: 引入预测案例后缀的技术，考虑活动的开始和结束时间，采用扫描线方法同步预测所有正在进行案例的后缀。

Result: 在真实和合成数据集上评估不同实例，展示多模型方法在案例后缀预测中的准确性优势。

Conclusion: 多模型方法在考虑活动时间信息的案例后缀预测中具有优势。

Abstract: Predictive process monitoring techniques support the operational decision
making by predicting future states of ongoing cases of a business process. A
subset of these techniques predict the remaining sequence of activities of an
ongoing case (case suffix prediction). Existing approaches for case suffix
prediction generate sequences of activities with a single timestamp (e.g. the
end timestamp). This output is insufficient for resource capacity planning,
where we need to reason about the periods of time when resources will be busy
performing work. This paper introduces a technique for predicting case suffixes
consisting of activities with start and end timestamps. In other words, the
proposed technique predicts both the waiting time and the processing time of
each activity. Since the waiting time of an activity in a case depends on how
busy resources are in other cases, the technique adopts a sweep-line approach,
wherein the suffixes of all ongoing cases in the process are predicted in
lockstep, rather than predictions being made for each case in isolation. An
evaluation on real-life and synthetic datasets compares the accuracy of
different instantiations of this approach, demonstrating the advantages of a
multi-model approach to case suffix prediction.

</details>


### [63] [LiMuon: Light and Fast Muon Optimizer for Large Models](https://arxiv.org/abs/2509.14562)
*Feihu Huang,Yuning Luo,Songcan Chen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large models recently are widely applied in artificial intelligence, so
efficient training of large models has received widespread attention. More
recently, a useful Muon optimizer is specifically designed for
matrix-structured parameters of large models. Although some works have begun to
studying Muon optimizer, the existing Muon and its variants still suffer from
high sample complexity or high memory for large models. To fill this gap, we
propose a light and fast Muon (LiMuon) optimizer for training large models,
which builds on the momentum-based variance reduced technique and randomized
Singular Value Decomposition (SVD). Our LiMuon optimizer has a lower memory
than the current Muon and its variants. Moreover, we prove that our LiMuon has
a lower sample complexity of $O(\epsilon^{-3})$ for finding an
$\epsilon$-stationary solution of non-convex stochastic optimization under the
smooth condition. Recently, the existing convergence analysis of Muon optimizer
mainly relies on the strict Lipschitz smooth assumption, while some artificial
intelligence tasks such as training large language models (LLMs) do not satisfy
this condition. We also proved that our LiMuon optimizer has a sample
complexity of $O(\epsilon^{-3})$ under the generalized smooth condition.
Numerical experimental results on training DistilGPT2 and ViT models verify
efficiency of our LiMuon optimizer.

</details>


### [64] [Learning to Retrieve for Environmental Knowledge Discovery: An Augmentation-Adaptive Self-Supervised Learning Framework](https://arxiv.org/abs/2509.14563)
*Shiyuan Luo,Runlong Yu,Chonghao Qiu,Rahul Ghosh,Robert Ladwig,Paul C. Hanson,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: 提出A²SL框架解决环境知识发现中数据收集成本高和机器学习模型泛化性差的问题，在淡水生态系统案例中提升了预测准确性和鲁棒性，具有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 环境知识发现依赖特定任务的标注数据，但数据收集成本高，现有机器学习方法在数据稀疏或非典型条件下泛化能力差。

Method: 提出A²SL框架，引入多级成对学习损失训练场景编码器，利用学习到的相似度驱动检索机制补充数据，设计增强自适应机制处理可变场景。

Result: 以淡水生态系统为例，A²SL显著提高了预测准确性，增强了在数据稀缺和非典型场景中的鲁棒性。

Conclusion: A²SL框架虽聚焦淡水生态系统，但在各科学领域有广泛应用价值。

Abstract: The discovery of environmental knowledge depends on labeled task-specific
data, but is often constrained by the high cost of data collection. Existing
machine learning approaches usually struggle to generalize in data-sparse or
atypical conditions. To this end, we propose an Augmentation-Adaptive
Self-Supervised Learning (A$^2$SL) framework, which retrieves relevant
observational samples to enhance modeling of the target ecosystem.
Specifically, we introduce a multi-level pairwise learning loss to train a
scenario encoder that captures varying degrees of similarity among scenarios.
These learned similarities drive a retrieval mechanism that supplements a
target scenario with relevant data from different locations or time periods.
Furthermore, to better handle variable scenarios, particularly under atypical
or extreme conditions where traditional models struggle, we design an
augmentation-adaptive mechanism that selectively enhances these scenarios
through targeted data augmentation. Using freshwater ecosystems as a case
study, we evaluate A$^2$SL in modeling water temperature and dissolved oxygen
dynamics in real-world lakes. Experimental results show that A$^2$SL
significantly improves predictive accuracy and enhances robustness in
data-scarce and atypical scenarios. Although this study focuses on freshwater
ecosystems, the A$^2$SL framework offers a broadly applicable solution in
various scientific domains.

</details>


### [65] [Stochastic Adaptive Gradient Descent Without Descent](https://arxiv.org/abs/2509.14969)
*Jean-François Aujol,Jérémie Bigot,Camille Castera*

Main category: cs.LG

TL;DR: 提出一种无需超参数调整的随机梯度凸优化自适应步长策略，证明收敛性并显示其竞争力。


<details>
  <summary>Details</summary>
Motivation: 为随机梯度凸优化找到更好的自适应步长策略，避免超参数调整。

Method: 将Adaptive Gradient Descent Without Descent方法理论上适配到随机环境。

Result: 证明在不同假设下带此步长的随机梯度下降收敛，且在经验上能与调优的基线竞争。

Conclusion: 所提出的自适应步长策略在随机梯度凸优化中有效且有竞争力。

Abstract: We introduce a new adaptive step-size strategy for convex optimization with
stochastic gradient that exploits the local geometry of the objective function
only by means of a first-order stochastic oracle and without any
hyper-parameter tuning. The method comes from a theoretically-grounded
adaptation of the Adaptive Gradient Descent Without Descent method to the
stochastic setting. We prove the convergence of stochastic gradient descent
with our step-size under various assumptions, and we show that it empirically
competes against tuned baselines.

</details>


### [66] [Evidential Physics-Informed Neural Networks for Scientific Discovery](https://arxiv.org/abs/2509.14568)
*Hai Siong Tan,Kuancheng Wang,Rafe McBeth*

Main category: cs.LG

TL;DR: 提出证据物理信息神经网络（E - PINN），在两个案例验证效果优于贝叶斯PINN和深度集成方法，还展示其在临床数据集的应用。


<details>
  <summary>Details</summary>
Motivation: 开发一种新的能感知不确定性的物理信息神经网络。

Method: 利用证据深度学习的边际分布损失函数估计输出不确定性，通过学习的后验分布推断偏微分方程的未知参数。

Result: 在1D泊松方程和2D Fisher - KPP方程案例中，E - PINN生成的经验覆盖概率校准效果明显优于贝叶斯PINN和深度集成方法。

Conclusion: E - PINN是一种有效的不确定性感知PINN，有现实应用潜力。

Abstract: We present the fundamental theory and implementation guidelines underlying
Evidential Physics-Informed Neural Network (E-PINN) -- a novel class of
uncertainty-aware PINN. It leverages the marginal distribution loss function of
evidential deep learning for estimating uncertainty of outputs, and infers
unknown parameters of the PDE via a learned posterior distribution. Validating
our model on two illustrative case studies -- the 1D Poisson equation with a
Gaussian source and the 2D Fisher-KPP equation, we found that E-PINN generated
empirical coverage probabilities that were calibrated significantly better than
Bayesian PINN and Deep Ensemble methods. To demonstrate real-world
applicability, we also present a brief case study on applying E-PINN to analyze
clinical glucose-insulin datasets that have featured in medical research on
diabetes pathophysiology.

</details>


### [67] [Communication Efficient Split Learning of ViTs with Attention-based Double Compression](https://arxiv.org/abs/2509.15058)
*Federico Alvetreti,Jary Pomponi,Paolo Di Lorenzo,Simone Scardapane*

Main category: cs.LG

TL;DR: 提出名为ADC的通信高效分割学习框架，结合两种压缩策略减少通信开销，模拟结果显示优于现有框架。


<details>
  <summary>Details</summary>
Motivation: 减少分割学习训练过程中传输中间视觉Transformer激活所需的通信开销。

Method: 提出ADC框架，采用两种并行压缩策略，一是基于最后客户端层的平均注意力分数合并相似样本激活，二是丢弃最无意义的标记。

Result: 模拟结果表明，Attention - based Double Compression显著减少通信开销的同时保持了高精度。

Conclusion: Attention - based Double Compression框架优于现有分割学习框架。

Abstract: This paper proposes a novel communication-efficient Split Learning (SL)
framework, named Attention-based Double Compression (ADC), which reduces the
communication overhead required for transmitting intermediate Vision
Transformers activations during the SL training process. ADC incorporates two
parallel compression strategies. The first one merges samples' activations that
are similar, based on the average attention score calculated in the last client
layer; this strategy is class-agnostic, meaning that it can also merge samples
having different classes, without losing generalization ability nor decreasing
final results. The second strategy follows the first and discards the least
meaningful tokens, further reducing the communication cost. Combining these
strategies not only allows for sending less during the forward pass, but also
the gradients are naturally compressed, allowing the whole model to be trained
without additional tuning or approximations of the gradients. Simulation
results demonstrate that Attention-based Double Compression outperforms
state-of-the-art SL frameworks by significantly reducing communication
overheads while maintaining high accuracy.

</details>


### [68] [Structure-Preserving Margin Distribution Learning for High-Order Tensor Data with Low-Rank Decomposition](https://arxiv.org/abs/2509.14577)
*Yang Xu,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 提出SPMD - LRT处理高维张量数据分类，通过交替优化算法求解，实验表明其分类准确率优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有LMDM只能处理向量输入，处理高维张量数据需扁平化，破坏数据结构且增加计算负担。

Method: 提出SPMD - LRT，将一阶和二阶张量统计融入目标，利用低秩张量分解技术参数化权重张量，用交替优化算法求解。

Result: 在多个数据集上实验，SPMD - LRT分类准确率优于传统SVM、基于向量的LMDM和先前基于张量的SVM扩展。

Conclusion: SPMD - LRT处理高维张量数据分类有效且鲁棒。

Abstract: The Large Margin Distribution Machine (LMDM) is a recent advancement in
classifier design that optimizes not just the minimum margin (as in SVM) but
the entire margin distribution, thereby improving generalization. However,
existing LMDM formulations are limited to vectorized inputs and struggle with
high-dimensional tensor data due to the need for flattening, which destroys the
data's inherent multi-mode structure and increases computational burden. In
this paper, we propose a Structure-Preserving Margin Distribution Learning for
High-Order Tensor Data with Low-Rank Decomposition (SPMD-LRT) that operates
directly on tensor representations without vectorization. The SPMD-LRT
preserves multi-dimensional spatial structure by incorporating first-order and
second-order tensor statistics (margin mean and variance) into the objective,
and it leverages low-rank tensor decomposition techniques including rank-1(CP),
higher-rank CP, and Tucker decomposition to parameterize the weight tensor. An
alternating optimization (double-gradient descent) algorithm is developed to
efficiently solve the SPMD-LRT, iteratively updating factor matrices and core
tensor. This approach enables SPMD-LRT to maintain the structural information
of high-order data while optimizing margin distribution for improved
classification. Extensive experiments on diverse datasets (including MNIST,
images and fMRI neuroimaging) demonstrate that SPMD-LRT achieves superior
classification accuracy compared to conventional SVM, vector-based LMDM, and
prior tensor-based SVM extensions (Support Tensor Machines and Support Tucker
Machines). Notably, SPMD-LRT with Tucker decomposition attains the highest
accuracy, highlighting the benefit of structure preservation. These results
confirm the effectiveness and robustness of SPMD-LRT in handling
high-dimensional tensor data for classification.

</details>


### [69] [Online reinforcement learning via sparse Gaussian mixture model Q-functions](https://arxiv.org/abs/2509.14585)
*Minh Vu,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: 本文提出基于稀疏高斯混合模型Q函数的强化学习在线策略迭代框架，性能好且参数少。


<details>
  <summary>Details</summary>
Motivation: 在先前离线训练GMM - QFs工作基础上，开发在线方案以利用流数据促进探索，解决过拟合问题。

Method: 通过Hadamard过参数化进行稀疏化调节模型复杂度，利用S - GMM - QFs参数空间的黎曼流形结构进行在线梯度下降更新参数。

Result: S - GMM - QFs在标准基准测试中性能与密集深度强化学习方法相当，参数显著减少，在稀疏深度强化学习方法无法泛化的低参数计数情况下仍表现良好。

Conclusion: 所提出的基于S - GMM - QFs的在线策略迭代框架有效可行，有较好应用前景。

Abstract: This paper introduces a structured and interpretable online policy-iteration
framework for reinforcement learning (RL), built around the novel class of
sparse Gaussian mixture model Q-functions (S-GMM-QFs). Extending earlier work
that trained GMM-QFs offline, the proposed framework develops an online scheme
that leverages streaming data to encourage exploration. Model complexity is
regulated through sparsification by Hadamard overparametrization, which
mitigates overfitting while preserving expressiveness. The parameter space of
S-GMM-QFs is naturally endowed with a Riemannian manifold structure, allowing
for principled parameter updates via online gradient descent on a smooth
objective. Numerical tests show that S-GMM-QFs match the performance of dense
deep RL (DeepRL) methods on standard benchmarks while using significantly fewer
parameters, and maintain strong performance even in low-parameter-count regimes
where sparsified DeepRL methods fail to generalize.

</details>


### [70] [TICA-Based Free Energy Matching for Machine-Learned Molecular Dynamics](https://arxiv.org/abs/2509.14600)
*Alexander Aghili,Andy Bruce,Daniel Sabo,Razvan Marinescu*

Main category: cs.LG

TL;DR: 本文在粗粒度机器学习模型损失函数中加入能量匹配项，虽未显著提升准确性，但揭示模型泛化自由能表面的倾向，为改进粗粒度建模提供方向。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟计算成本高，粗粒度机器学习模型传统力匹配方法无法完整捕捉热力学景观。

Method: 在损失函数中加入互补的能量匹配项，用CGSchNet模型评估框架并系统改变能量损失项权重。

Result: 能量匹配未使准确性有显著统计学提升，但揭示了模型泛化自由能表面的不同倾向。

Conclusion: 未来可通过改进能量估计技术和多模态损失公式来增强粗粒度建模。

Abstract: Molecular dynamics (MD) simulations provide atomistic insight into
biomolecular systems but are often limited by high computational costs required
to access long timescales. Coarse-grained machine learning models offer a
promising avenue for accelerating sampling, yet conventional force matching
approaches often fail to capture the full thermodynamic landscape as fitting a
model on the gradient may not fit the absolute differences between low-energy
conformational states. In this work, we incorporate a complementary energy
matching term into the loss function. We evaluate our framework on the
Chignolin protein using the CGSchNet model, systematically varying the weight
of the energy loss term. While energy matching did not yield statistically
significant improvements in accuracy, it revealed distinct tendencies in how
models generalize the free energy surface. Our results suggest future
opportunities to enhance coarse-grained modeling through improved energy
estimation techniques and multi-modal loss formulations.

</details>


### [71] [Explaining deep learning for ECG using time-localized clusters](https://arxiv.org/abs/2509.15198)
*Ahcène Boubekki,Konstantinos Patlatzoglou,Joseph Barker,Fu Siong Ng,Antônio H. Ribeiro*

Main category: cs.LG

TL;DR: 提出一种用于心电图分析的卷积神经网络可解释性新方法，增强对模型的信任，便于发现临床相关模式。


<details>
  <summary>Details</summary>
Motivation: 深度学习虽推动心电图分析发展，但理解模型仍是挑战，限制对其成果的解读和知识获取。

Method: 从模型内部表示中提取时间局部化聚类，根据学习特征分割心电图并量化表示的不确定性。

Result: 可可视化不同波形区域对模型预测的贡献，评估决策的确定性。

Conclusion: 该方法增强了对人工智能驱动诊断的信任，有助于发现临床相关的电生理模式。

Abstract: Deep learning has significantly advanced electrocardiogram (ECG) analysis,
enabling automatic annotation, disease screening, and prognosis beyond
traditional clinical capabilities. However, understanding these models remains
a challenge, limiting interpretation and gaining knowledge from these
developments. In this work, we propose a novel interpretability method for
convolutional neural networks applied to ECG analysis. Our approach extracts
time-localized clusters from the model's internal representations, segmenting
the ECG according to the learned characteristics while quantifying the
uncertainty of these representations. This allows us to visualize how different
waveform regions contribute to the model's predictions and assess the certainty
of its decisions. By providing a structured and interpretable view of deep
learning models for ECG, our method enhances trust in AI-driven diagnostics and
facilitates the discovery of clinically relevant electrophysiological patterns.

</details>


### [72] [Towards Privacy-Preserving and Heterogeneity-aware Split Federated Learning via Probabilistic Masking](https://arxiv.org/abs/2509.14603)
*Xingchen Wang,Feijie Wu,Chenglin Miao,Tianchun Li,Haoyu Hu,Qiming Cao,Jing Gao,Lu Su*

Main category: cs.LG

TL;DR: 提出PM - SFL框架，通过概率掩码训练解决SFL隐私风险，结合个性化掩码学习和知识补偿机制应对数据和系统异构性，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统SFL交换中间激活和模型更新存在隐私风险，现有基于噪声注入的防御方法会降低模型性能，且要应对数据和系统的异构性。

Method: 提出PM - SFL框架，采用概率掩码训练添加结构化随机性，使用个性化掩码学习应对数据异构性，引入层知识补偿机制应对系统异构性。

Result: 理论分析证实其隐私保护能力，图像和无线传感任务实验表明，PM - SFL在准确性、通信效率和抗隐私攻击鲁棒性方面表现出色，在数据和系统异构下性能更强。

Conclusion: PM - SFL是一种可扩展且保护隐私的SFL框架，能有效应对隐私风险和异构性问题。

Abstract: Split Federated Learning (SFL) has emerged as an efficient alternative to
traditional Federated Learning (FL) by reducing client-side computation through
model partitioning. However, exchanging of intermediate activations and model
updates introduces significant privacy risks, especially from data
reconstruction attacks that recover original inputs from intermediate
representations. Existing defenses using noise injection often degrade model
performance. To overcome these challenges, we present PM-SFL, a scalable and
privacy-preserving SFL framework that incorporates Probabilistic Mask training
to add structured randomness without relying on explicit noise. This mitigates
data reconstruction risks while maintaining model utility. To address data
heterogeneity, PM-SFL employs personalized mask learning that tailors submodel
structures to each client's local data. For system heterogeneity, we introduce
a layer-wise knowledge compensation mechanism, enabling clients with varying
resources to participate effectively under adaptive model splitting.
Theoretical analysis confirms its privacy protection, and experiments on image
and wireless sensing tasks demonstrate that PM-SFL consistently improves
accuracy, communication efficiency, and robustness to privacy attacks, with
particularly strong performance under data and system heterogeneity.

</details>


### [73] [HD3C: Efficient Medical Data Classification for Embedded Devices](https://arxiv.org/abs/2509.14617)
*Jianglan Wei,Zhenyu Zhang,Pengcheng Wang,Mingjie Zeng,Zhigang Zeng*

Main category: cs.LG

TL;DR: 提出轻量级分类框架HD3C用于低功耗环境下医疗数据分类，评估显示其节能且鲁棒性好。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型能耗高、依赖GPU，难以在嵌入式设备普遍的家庭和现场医疗保健场景部署，需要节能的医疗数据分类方法。

Method: 提出HD3C框架，将数据编码为高维超向量，聚合为多个特定簇的原型，并通过超空间中的相似性搜索进行分类。

Result: 在三项医疗分类任务中评估，在心音分类上比贝叶斯ResNet节能350倍，精度差异小于1%，且对噪声、有限训练数据和硬件错误有出色的鲁棒性。

Conclusion: HD3C有潜力在现实环境中可靠部署。

Abstract: Energy-efficient medical data classification is essential for modern disease
screening, particularly in home and field healthcare where embedded devices are
prevalent. While deep learning models achieve state-of-the-art accuracy, their
substantial energy consumption and reliance on GPUs limit deployment on such
platforms. We present Hyperdimensional Computing with Class-Wise Clustering
(HD3C), a lightweight classification framework designed for low-power
environments. HD3C encodes data into high-dimensional hypervectors, aggregates
them into multiple cluster-specific prototypes, and performs classification
through similarity search in hyperspace. We evaluate HD3C across three medical
classification tasks; on heart sound classification, HD3C is $350\times$ more
energy-efficient than Bayesian ResNet with less than 1% accuracy difference.
Moreover, HD3C demonstrates exceptional robustness to noise, limited training
data, and hardware error, supported by both theoretical analysis and empirical
results, highlighting its potential for reliable deployment in real-world
settings. Code is available at https://github.com/jianglanwei/HD3C.

</details>


### [74] [CUFG: Curriculum Unlearning Guided by the Forgetting Gradient](https://arxiv.org/abs/2509.14633)
*Jiaxing Miao,Liang Hu,Qi Zhang,Lai Zhong Yuan,Usman Naseem*

Main category: cs.LG

TL;DR: 现有机器遗忘方法有局限，提出CUFG框架提升近似遗忘稳定性，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法过度关注效率和激进遗忘，导致模型权重不稳定、可靠性降低。

Method: 提出CUFG框架，整合基于遗忘梯度的梯度校正器和从易到难的课程式遗忘范式。

Result: 该框架缩小了与重新训练方法的差距，提高了遗忘的有效性和可靠性，实验验证了其合理性和有效性。

Conclusion: 课程式遗忘概念有很大研究潜力，为机器遗忘领域发展提供前瞻性见解。

Abstract: As privacy and security take center stage in AI, machine unlearning, the
ability to erase specific knowledge from models, has garnered increasing
attention. However, existing methods overly prioritize efficiency and
aggressive forgetting, which introduces notable limitations. In particular,
radical interventions like gradient ascent, influence functions, and random
label noise can destabilize model weights, leading to collapse and reduced
reliability. To address this, we propose CUFG (Curriculum Unlearning via
Forgetting Gradients), a novel framework that enhances the stability of
approximate unlearning through innovations in both forgetting mechanisms and
data scheduling strategies. Specifically, CUFG integrates a new gradient
corrector guided by forgetting gradients for fine-tuning-based unlearning and a
curriculum unlearning paradigm that progressively forgets from easy to hard.
These innovations narrow the gap with the gold-standard Retrain method by
enabling more stable and progressive unlearning, thereby improving both
effectiveness and reliability. Furthermore, we believe that the concept of
curriculum unlearning has substantial research potential and offers
forward-looking insights for the development of the MU field. Extensive
experiments across various forgetting scenarios validate the rationale and
effectiveness of our approach and CUFG. Codes are available at
https://anonymous.4open.science/r/CUFG-6375.

</details>


### [75] [DyWPE: Signal-Aware Dynamic Wavelet Positional Encoding for Time Series Transformers](https://arxiv.org/abs/2509.14640)
*Habib Irani,Vangelis Metsis*

Main category: cs.LG

TL;DR: 提出动态小波位置编码（DyWPE）框架用于时间序列分析，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer位置编码方法与信号无关，忽略信号特征，不适合时间序列分析。

Method: 引入DyWPE框架，利用离散小波变换（DWT）从输入时间序列直接生成位置嵌入。

Result: 在十个不同时间序列数据集上，DyWPE始终优于八种现有最先进的位置编码方法，在生物医学信号中比基线正弦绝对位置编码平均相对提高9.1%，且计算效率有竞争力。

Conclusion: DyWPE是一种有效的信号感知位置编码方法，适用于时间序列分析。

Abstract: Existing positional encoding methods in transformers are fundamentally
signal-agnostic, deriving positional information solely from sequence indices
while ignoring the underlying signal characteristics. This limitation is
particularly problematic for time series analysis, where signals exhibit
complex, non-stationary dynamics across multiple temporal scales. We introduce
Dynamic Wavelet Positional Encoding (DyWPE), a novel signal-aware framework
that generates positional embeddings directly from input time series using the
Discrete Wavelet Transform (DWT). Comprehensive experiments in ten diverse time
series datasets demonstrate that DyWPE consistently outperforms eight existing
state-of-the-art positional encoding methods, achieving average relative
improvements of 9.1\% compared to baseline sinusoidal absolute position
encoding in biomedical signals, while maintaining competitive computational
efficiency.

</details>


### [76] [DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training](https://arxiv.org/abs/2509.14642)
*Yuemin Wu,Zhongze Wu,Xiu Su,Feng Yang,Hongyan Xu,Xi Lin,Wenti Huang,Shan You,Chang Xu*

Main category: cs.LG

TL;DR: 提出DeCoP预训练框架处理时间序列预训练中动态时间依赖建模问题，在多数据集获SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 时间序列预训练中动态时间依赖建模是挑战，现有框架无法捕捉长短依赖复杂交互，易受虚假关联影响，泛化能力差。

Method: 提出DeCoP框架，输入层用IPN减轻分布偏移，潜在层用DCL策略建模多尺度依赖，ICM模块增强全局泛化。

Result: DeCoP在十个数据集上取得SOTA结果，在ETTh1上用37%的FLOPs使MSE比PatchTST降低3%。

Conclusion: DeCoP能有效处理动态多尺度依赖，以更低计算资源取得良好效果。

Abstract: Modeling dynamic temporal dependencies is a critical challenge in time series
pre-training, which evolve due to distribution shifts and multi-scale patterns.
This temporal variability severely impairs the generalization of pre-trained
models to downstream tasks. Existing frameworks fail to capture the complex
interactions of short- and long-term dependencies, making them susceptible to
spurious correlations that degrade generalization. To address these
limitations, we propose DeCoP, a Dependency Controlled Pre-training framework
that explicitly models dynamic, multi-scale dependencies by simulating evolving
inter-patch dependencies. At the input level, DeCoP introduces Instance-wise
Patch Normalization (IPN) to mitigate distributional shifts while preserving
the unique characteristics of each patch, creating a robust foundation for
representation learning. At the latent level, a hierarchical Dependency
Controlled Learning (DCL) strategy explicitly models inter-patch dependencies
across multiple temporal scales, with an Instance-level Contrastive Module
(ICM) enhances global generalization by learning instance-discriminative
representations from time-invariant positive pairs. DeCoP achieves
state-of-the-art results on ten datasets with lower computing resources,
improving MSE by 3% on ETTh1 over PatchTST using only 37% of the FLOPs.

</details>


### [77] [Stochastic Clock Attention for Aligning Continuous and Ordered Sequences](https://arxiv.org/abs/2509.14678)
*Hyungjoon Soh,Junghyo Jo*

Main category: cs.LG

TL;DR: 提出用于连续有序序列的注意力机制，用学习的时钟建模注意力，在文本转语音测试中表现良好，有望用于其他连续目标。


<details>
  <summary>Details</summary>
Motivation: 标准缩放点积注意力缺乏连续性和单调性，不适用于帧同步目标。

Method: 为源和目标引入学习的非负时钟，将注意力建模为时钟相遇概率，推导出类似高斯的评分规则。

Result: 在Transformer文本转语音测试中，产生更稳定的对齐，提高对全局时间缩放的鲁棒性，精度与或优于缩放点积基线。

Conclusion: 该机制有望应用于其他连续目标，如视频和时间信号建模。

Abstract: We formulate an attention mechanism for continuous and ordered sequences that
explicitly functions as an alignment model, which serves as the core of many
sequence-to-sequence tasks. Standard scaled dot-product attention relies on
positional encodings and masks but does not enforce continuity or monotonicity,
which are crucial for frame-synchronous targets. We propose learned nonnegative
\emph{clocks} to source and target and model attention as the meeting
probability of these clocks; a path-integral derivation yields a closed-form,
Gaussian-like scoring rule with an intrinsic bias toward causal, smooth,
near-diagonal alignments, without external positional regularizers. The
framework supports two complementary regimes: normalized clocks for parallel
decoding when a global length is available, and unnormalized clocks for
autoregressive decoding -- both nearly-parameter-free, drop-in replacements. In
a Transformer text-to-speech testbed, this construction produces more stable
alignments and improved robustness to global time-scaling while matching or
improving accuracy over scaled dot-product baselines. We hypothesize
applicability to other continuous targets, including video and temporal signal
modeling.

</details>


### [78] [ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning](https://arxiv.org/abs/2509.14718)
*Zihao Feng,Xiaoxue Wang,Bowen Wu,Hailong Cao,Tiejun Zhao,Qun Yu,Baoxun Wang*

Main category: cs.LG

TL;DR: 本文提出DSCL框架解决强化学习在大语言模型工具学习中效率低的问题，实验表明该框架能显著提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习用于大语言模型工具学习时效率受简单样本影响，现有动态采样技术不适合工具学习的多任务结构和细粒度奖励机制。

Method: 提出DSCL框架，包含基于奖励的动态采样和基于任务的动态课程学习两个核心组件。

Result: DSCL在强基线模型上显著提高训练效率和模型性能，在BFCLv3基准测试中提升3.29%。

Conclusion: 该方法有效利用工具学习中的复杂奖励信号和子任务动态，实现了更优结果。

Abstract: While reinforcement learning (RL) is increasingly used for LLM-based tool
learning, its efficiency is often hampered by an overabundance of simple
samples that provide diminishing learning value as training progresses.
Existing dynamic sampling techniques are ill-suited for the multi-task
structure and fine-grained reward mechanisms inherent to tool learning. This
paper introduces Dynamic Sampling with Curriculum Learning (DSCL), a framework
specifically designed to address this challenge by targeting the unique
characteristics of tool learning: its multiple interdependent sub-tasks and
multi-valued reward functions. DSCL features two core components: Reward-Based
Dynamic Sampling, which uses multi-dimensional reward statistics (mean and
variance) to prioritize valuable data, and Task-Based Dynamic Curriculum
Learning, which adaptively focuses training on less-mastered sub-tasks. Through
extensive experiments, we demonstrate that DSCL significantly improves training
efficiency and model performance over strong baselines, achieving a 3.29\%
improvement on the BFCLv3 benchmark. Our method provides a tailored solution
that effectively leverages the complex reward signals and sub-task dynamics
within tool learning to achieve superior results.

</details>


### [79] [Towards Pre-trained Graph Condensation via Optimal Transport](https://arxiv.org/abs/2509.14722)
*Yeyu Yan,Shuai Zheng,Wenjun Hui,Xiangkai Zhu,Dong Chen,Zhenfeng Zhu,Yao Zhao,Kunlun He*

Main category: cs.LG

TL;DR: 本文提出预训练图凝聚方法PreGC，解决传统图凝聚方法依赖任务和架构的问题，实验验证其优越性和通用性。


<details>
  <summary>Details</summary>
Motivation: 传统图凝聚方法依赖刚性GNN和特定任务监督，限制了可复用性和泛化性。

Method: 从GNN优化一致性角度推导广义GC优化目标，提出PreGC方法，包括混合区间图扩散增强、建立最优图传输计划与表示传输计划匹配、提出可追溯语义协调器。

Result: 广泛实验验证了PreGC的优越性和通用性，表明其不依赖任务且能与任意GNN无缝兼容。

Conclusion: PreGC能超越传统图凝聚方法的局限性，实现任务无关和与任意GNN兼容。

Abstract: Graph condensation (GC) aims to distill the original graph into a small-scale
graph, mitigating redundancy and accelerating GNN training. However,
conventional GC approaches heavily rely on rigid GNNs and task-specific
supervision. Such a dependency severely restricts their reusability and
generalization across various tasks and architectures. In this work, we revisit
the goal of ideal GC from the perspective of GNN optimization consistency, and
then a generalized GC optimization objective is derived, by which those
traditional GC methods can be viewed nicely as special cases of this
optimization paradigm. Based on this, Pre-trained Graph Condensation (PreGC)
via optimal transport is proposed to transcend the limitations of task- and
architecture-dependent GC methods. Specifically, a hybrid-interval graph
diffusion augmentation is presented to suppress the weak generalization ability
of the condensed graph on particular architectures by enhancing the uncertainty
of node states. Meanwhile, the matching between optimal graph transport plan
and representation transport plan is tactfully established to maintain semantic
consistencies across source graph and condensed graph spaces, thereby freeing
graph condensation from task dependencies. To further facilitate the adaptation
of condensed graphs to various downstream tasks, a traceable semantic
harmonizer from source nodes to condensed nodes is proposed to bridge semantic
associations through the optimized representation transport plan in
pre-training. Extensive experiments verify the superiority and versatility of
PreGC, demonstrating its task-independent nature and seamless compatibility
with arbitrary GNNs.

</details>


### [80] [Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models](https://arxiv.org/abs/2509.14723)
*Sosuke Hosokawa,Toshiharu Kawakami,Satoshi Kodera,Masamichi Ito,Norihiko Takeda*

Main category: cs.LG

TL;DR: 本文在最先进的单细胞基础模型C2S上训练转码器，提取其内部决策电路，证明转码器能揭示复杂单细胞模型中具有生物学合理性的通路。


<details>
  <summary>Details</summary>
Motivation: 单细胞基础模型决策过程缺乏可解释性，而转码器可从大语言模型中提取可解释决策电路，因此尝试用转码器解决单细胞基础模型可解释性问题。

Method: 在C2S模型上训练转码器，利用训练好的转码器提取C2S模型的内部决策电路。

Result: 发现的电路对应现实世界的生物机制。

Conclusion: 转码器有潜力揭示复杂单细胞模型中具有生物学合理性的通路。

Abstract: Single-cell foundation models (scFMs) have demonstrated state-of-the-art
performance on various tasks, such as cell-type annotation and perturbation
response prediction, by learning gene regulatory networks from large-scale
transcriptome data. However, a significant challenge remains: the
decision-making processes of these models are less interpretable compared to
traditional methods like differential gene expression analysis. Recently,
transcoders have emerged as a promising approach for extracting interpretable
decision circuits from large language models (LLMs). In this work, we train a
transcoder on the cell2sentence (C2S) model, a state-of-the-art scFM. By
leveraging the trained transcoder, we extract internal decision-making circuits
from the C2S model. We demonstrate that the discovered circuits correspond to
real-world biological mechanisms, confirming the potential of transcoders to
uncover biologically plausible pathways within complex single-cell models.

</details>


### [81] [One-step Multi-view Clustering With Adaptive Low-rank Anchor-graph Learning](https://arxiv.org/abs/2509.14724)
*Zhiyuan Xue,Ben Yang,Xuetao Zhang,Fei Wang,Zhiping Lin*

Main category: cs.LG

TL;DR: 提出一种基于自适应低秩锚图学习的一步多视图聚类方法OMCAL，在聚类效果和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于锚图的多视图聚类方法存在忽略冗余信息和噪声、独立后处理降低效果和效率的问题。

Method: 提供基于核范数的自适应共识锚图学习模型，将类别指标获取和共识锚图学习整合到统一框架。

Result: 在普通和大规模数据集上的大量实验表明，OMCAL在聚类效果和效率上优于现有最先进的方法。

Conclusion: OMCAL能有效解决现有方法的问题，在聚类效果和效率上表现更优。

Abstract: In light of their capability to capture structural information while reducing
computing complexity, anchor graph-based multi-view clustering (AGMC) methods
have attracted considerable attention in large-scale clustering problems.
Nevertheless, existing AGMC methods still face the following two issues: 1)
They directly embedded diverse anchor graphs into a consensus anchor graph
(CAG), and hence ignore redundant information and numerous noises contained in
these anchor graphs, leading to a decrease in clustering effectiveness; 2) They
drop effectiveness and efficiency due to independent post-processing to acquire
clustering indicators. To overcome the aforementioned issues, we deliver a
novel one-step multi-view clustering method with adaptive low-rank anchor-graph
learning (OMCAL). To construct a high-quality CAG, OMCAL provides a nuclear
norm-based adaptive CAG learning model against information redundancy and noise
interference. Then, to boost clustering effectiveness and efficiency
substantially, we incorporate category indicator acquisition and CAG learning
into a unified framework. Numerous studies conducted on ordinary and
large-scale datasets indicate that OMCAL outperforms existing state-of-the-art
methods in terms of clustering effectiveness and efficiency.

</details>


### [82] [FlowCast-ODE: Continuous Hourly Weather Forecasting with Dynamic Flow Matching and ODE Integration](https://arxiv.org/abs/2509.14775)
*Shuangshuang He,Yuanting Zhang,Hongli Liang,Qingye Meng,Xingyuan Yuan*

Main category: cs.LG

TL;DR: 提出FlowCast - ODE框架进行准确稳定的每小时天气预测，实验显示其性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在每小时天气预测上难以实现准确稳定，原因是自回归误差积累和ERA5数据同化周期的时间不连续性。

Method: 提出FlowCast - ODE框架将大气状态演变建模为连续流，采用粗到细策略训练模型，引入轻量级低秩AdaLN - Zero调制机制。

Result: FlowCast - ODE在实验中RMSE更低、能量守恒更好，能减少模糊、保留更多空间细节，在预测极端事件上与最先进模型相当，还能缓解同化周期转换的时间不连续性。

Conclusion: FlowCast - ODE框架在每小时天气预测中表现良好，能有效解决现有模型存在的问题。

Abstract: Accurate hourly weather forecasting is critical for numerous applications.
Recent deep learning models have demonstrated strong capability on 6-hour
intervals, yet achieving accurate and stable hourly predictions remains a
critical challenge. This is primarily due to the rapid accumulation of errors
in autoregressive rollouts and temporal discontinuities within the ERA5 data's
12-hour assimilation cycle. To address these issues, we propose FlowCast-ODE, a
framework that models atmospheric state evolution as a continuous flow.
FlowCast-ODE learns the conditional flow path directly from the previous state,
an approach that aligns more naturally with physical dynamic systems and
enables efficient computation. A coarse-to-fine strategy is introduced to train
the model on 6-hour data using dynamic flow matching and then refined on hourly
data that incorporates an Ordinary Differential Equation (ODE) solver to
achieve temporally coherent forecasts. In addition, a lightweight low-rank
AdaLN-Zero modulation mechanism is proposed and reduces model size by 15%
without compromising accuracy. Experiments demonstrate that FlowCast-ODE
outperforms strong baselines, yielding lower root mean square error (RMSE) and
better energy conservation, which reduces blurring and preserves more
fine-scale spatial details. It also shows comparable performance to the
state-of-the-art model in forecasting extreme events like typhoons.
Furthermore, the model alleviates temporal discontinuities associated with
assimilation cycle transitions.

</details>


### [83] [Pre-training under infinite compute](https://arxiv.org/abs/2509.14786)
*Konwoo Kim,Suhas Kotha,Percy Liang,Tatsunori Hashimoto*

Main category: cs.LG

TL;DR: 研究固定数据无计算约束下语言模型预训练，通过正则化、集成模型等方法提升数据效率，成果可推广到下游任务。


<details>
  <summary>Details</summary>
Motivation: 计算能力增长快于预训练数据，研究固定数据无计算约束下的预训练方法。

Method: 调整正则化参数、集成独立训练模型、结合多策略干预、将集成模型蒸馏到小模型。

Result: 最佳干预在200M令牌处比基线少用5.17倍数据，蒸馏模型保留83%集成效益，下游任务有显著提升。

Conclusion: 简单算法改进可在计算丰富未来实现更高效数据预训练。

Abstract: Since compute grows much faster than web text available for language model
pre-training, we ask how one should approach pre-training under fixed data and
no compute constraints. We first show that existing data-constrained approaches
of increasing epoch count and parameter count eventually overfit, and we
significantly improve upon such recipes by properly tuning regularization,
finding that the optimal weight decay is $30\times$ larger than standard
practice. Since our regularized recipe monotonically decreases loss following a
simple power law in parameter count, we estimate its best possible performance
via the asymptote of its scaling law rather than the performance at a fixed
compute budget. We then identify that ensembling independently trained models
achieves a significantly lower loss asymptote than the regularized recipe. Our
best intervention combining epoching, regularization, parameter scaling, and
ensemble scaling achieves an asymptote at 200M tokens using $5.17\times$ less
data than our baseline, and our data scaling laws predict that this improvement
persists at higher token budgets. We find that our data efficiency gains can be
realized at much smaller parameter counts as we can distill an ensemble into a
student model that is 8$\times$ smaller and retains $83\%$ of the ensembling
benefit. Finally, our interventions designed for validation loss generalize to
downstream benchmarks, achieving a $9\%$ improvement for pre-training evals and
a $17.5\times$ data efficiency improvement over continued pre-training on math
mid-training data. Our results show that simple algorithmic improvements can
enable significantly more data-efficient pre-training in a compute-rich future.

</details>


### [84] [Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery](https://arxiv.org/abs/2509.14788)
*Jing Lan,Hexiao Ding,Hongzhao Chen,Yufeng Jiang,Nga-Chun Ng,Gwing Kei Yip,Gerald W. Y. Cheng,Yunlin Mao,Jing Cai,Liang-ting Lin,Jung Sun Yoo*

Main category: cs.LG

TL;DR: 提出基于序列的药物 - 靶点相互作用框架，在多基准测试中表现出色，验证了其可扩展和结构感知的 DTI 预测效用。


<details>
  <summary>Details</summary>
Motivation: 准确识别药物 - 靶点相互作用是计算药理学的核心挑战，基于序列的方法具有可扩展性，需引入新框架。

Method: 引入基于序列的药物 - 靶点相互作用框架，将结构先验整合到蛋白质表示中，同时保持高通量筛选能力。

Result: 模型在 Human 和 BioSNAP 数据集上达到了最先进的性能，在 BindingDB 上具有竞争力；在虚拟筛选任务中超越先前方法；消融研究证实了关键组件作用；嵌入可视化揭示了与已知结合口袋的空间对应关系和可解释的注意力模式。

Conclusion: 该框架可用于可扩展和结构感知的 DTI 预测。

Abstract: Accurate identification of drug-target interactions (DTI) remains a central
challenge in computational pharmacology, where sequence-based methods offer
scalability. This work introduces a sequence-based drug-target interaction
framework that integrates structural priors into protein representations while
maintaining high-throughput screening capability. Evaluated across multiple
benchmarks, the model achieves state-of-the-art performance on Human and
BioSNAP datasets and remains competitive on BindingDB. In virtual screening
tasks, it surpasses prior methods on LIT-PCBA, yielding substantial gains in
AUROC and BEDROC. Ablation studies confirm the critical role of learned
aggregation, bilinear attention, and contrastive alignment in enhancing
predictive robustness. Embedding visualizations reveal improved spatial
correspondence with known binding pockets and highlight interpretable attention
patterns over ligand-residue contacts. These results validate the framework's
utility for scalable and structure-aware DTI prediction.

</details>


### [85] [STEP: Structured Training and Evaluation Platform for benchmarking trajectory prediction models](https://arxiv.org/abs/2509.14801)
*Julian F. Schumann,Anna Mészáros,Jens Kober,Arkady Zgonnikov*

Main category: cs.LG

TL;DR: 现有轨迹预测模型评估框架有局限，本文提出STEP框架，实验展示其能力并引导关注模型深层特性。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹预测模型评估的标准化实践不足，现有框架难以支持异构交通场景、联合预测模型和用户文档。

Method: 提出STEP基准框架，提供多数据集统一接口，实施一致训练和评估条件，支持多种预测模型。

Result: 实验揭示常用测试程序的局限、联合建模对交互预测的重要性，以及现有模型对分布偏移和对抗攻击的脆弱性。

Conclusion: 用STEP框架引导从‘排行榜’式评估转向关注复杂多智能体环境下模型行为和泛化能力。

Abstract: While trajectory prediction plays a critical role in enabling safe and
effective path-planning in automated vehicles, standardized practices for
evaluating such models remain underdeveloped. Recent efforts have aimed to
unify dataset formats and model interfaces for easier comparisons, yet existing
frameworks often fall short in supporting heterogeneous traffic scenarios,
joint prediction models, or user documentation. In this work, we introduce STEP
-- a new benchmarking framework that addresses these limitations by providing a
unified interface for multiple datasets, enforcing consistent training and
evaluation conditions, and supporting a wide range of prediction models. We
demonstrate the capabilities of STEP in a number of experiments which reveal 1)
the limitations of widely-used testing procedures, 2) the importance of joint
modeling of agents for better predictions of interactions, and 3) the
vulnerability of current state-of-the-art models against both distribution
shifts and targeted attacks by adversarial agents. With STEP, we aim to shift
the focus from the ``leaderboard'' approach to deeper insights about model
behavior and generalization in complex multi-agent settings.

</details>


### [86] [Precision Neural Networks: Joint Graph And Relational Learning](https://arxiv.org/abs/2509.14821)
*Andrea Cavallo,Samuel Rey,Antonio G. Marques,Elvin Isufi*

Main category: cs.LG

TL;DR: 本文提出精度神经网络（PNNs），通过交替优化联合学习网络参数和精度矩阵，并在合成和真实数据上证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 协方差神经网络（VNNs）使用的协方差矩阵存在密度大、无法编码条件独立性和任务无关预计算等问题，影响性能。

Method: 研究基于精度矩阵（协方差矩阵的逆）的VNNs即PNNs，构建联合学习网络参数和精度矩阵的优化问题，通过交替优化求解。

Result: 理论上界定了每次迭代中估计和真实精度矩阵之间的距离，在合成和真实数据上证明联合估计比两步法更有效。

Conclusion: PNNs能克服VNNs的局限性，联合估计方法有效。

Abstract: CoVariance Neural Networks (VNNs) perform convolutions on the graph
determined by the covariance matrix of the data, which enables expressive and
stable covariance-based learning. However, covariance matrices are typically
dense, fail to encode conditional independence, and are often precomputed in a
task-agnostic way, which may hinder performance. To overcome these limitations,
we study Precision Neural Networks (PNNs), i.e., VNNs on the precision matrix
-- the inverse covariance. The precision matrix naturally encodes statistical
independence, often exhibits sparsity, and preserves the covariance spectral
structure. To make precision estimation task-aware, we formulate an
optimization problem that jointly learns the network parameters and the
precision matrix, and solve it via alternating optimization, by sequentially
updating the network weights and the precision estimate. We theoretically bound
the distance between the estimated and true precision matrices at each
iteration, and demonstrate the effectiveness of joint estimation compared to
two-step approaches on synthetic and real-world data.

</details>


### [87] [Diffusion-Based Scenario Tree Generation for Multivariate Time Series Prediction and Multistage Stochastic Optimization](https://arxiv.org/abs/2509.14832)
*Stelios Zarifis,Ioannis Kordonis,Petros Maragos*

Main category: cs.LG

TL;DR: 提出Diffusion Scenario Tree (DST)框架用于多变量预测任务构建情景树，在纽约州日前电力市场套利优化任务中表现优于传统模型和无模型强化学习基线。


<details>
  <summary>Details</summary>
Motivation: 随机预测对不确定系统高效决策至关重要，需估计未来情景的完整分布。

Method: 提出DST框架，通过基于扩散的概率预测模型递归采样未来轨迹，用聚类将其组织成树，确保各阶段非预期性。

Result: 在纽约州日前电力市场套利优化任务中，DST方法始终优于使用传统模型情景树的优化算法和无模型强化学习基线，用于随机优化能产生更高效决策策略。

Conclusion: DST框架能更好地处理不确定性，在随机优化中表现更优。

Abstract: Stochastic forecasting is critical for efficient decision-making in uncertain
systems, such as energy markets and finance, where estimating the full
distribution of future scenarios is essential. We propose Diffusion Scenario
Tree (DST), a general framework for constructing scenario trees for
multivariate prediction tasks using diffusion-based probabilistic forecasting
models. DST recursively samples future trajectories and organizes them into a
tree via clustering, ensuring non-anticipativity (decisions depending only on
observed history) at each stage. We evaluate the framework on the optimization
task of energy arbitrage in New York State's day-ahead electricity market.
Experimental results show that our approach consistently outperforms the same
optimization algorithms that use scenario trees from more conventional models
and Model-Free Reinforcement Learning baselines. Furthermore, using DST for
stochastic optimization yields more efficient decision policies, achieving
higher performance by better handling uncertainty than deterministic and
stochastic MPC variants using the same diffusion-based forecaster.

</details>


### [88] [Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization](https://arxiv.org/abs/2509.14848)
*Houssem Sifaou,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 本文研究固定成本预算下的多保真度混合强化学习策略优化，提出MF - HRL - IGM算法，理论和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习策略优化与高保真模拟器交互成本高，离线RL受数据集限制，现实中有多个不同保真度和计算成本的模拟器，需研究固定成本预算下的优化。

Method: 引入基于信息增益最大化的多保真度混合强化学习算法MF - HRL - IGM，通过自举法实现保真度选择。

Result: 理论分析表明MF - HRL - IGM具有无后悔特性，实证评估显示其性能优于现有基准。

Conclusion: MF - HRL - IGM算法在固定成本预算下的多保真度混合强化学习策略优化中表现良好。

Abstract: Optimizing a reinforcement learning (RL) policy typically requires extensive
interactions with a high-fidelity simulator of the environment, which are often
costly or impractical. Offline RL addresses this problem by allowing training
from pre-collected data, but its effectiveness is strongly constrained by the
size and quality of the dataset. Hybrid offline-online RL leverages both
offline data and interactions with a single simulator of the environment. In
many real-world scenarios, however, multiple simulators with varying levels of
fidelity and computational cost are available. In this work, we study
multi-fidelity hybrid RL for policy optimization under a fixed cost budget. We
introduce multi-fidelity hybrid RL via information gain maximization
(MF-HRL-IGM), a hybrid offline-online RL algorithm that implements fidelity
selection based on information gain maximization through a bootstrapping
approach. Theoretical analysis establishes the no-regret property of
MF-HRL-IGM, while empirical evaluations demonstrate its superior performance
compared to existing benchmarks.

</details>


### [89] [Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study](https://arxiv.org/abs/2509.14863)
*Zhengwei Wang,Gang Wu*

Main category: cs.LG

TL;DR: 提出G2LFormer，采用全局到局部注意力机制，防止信息损失，实验表明其性能优且复杂度为线性。


<details>
  <summary>Details</summary>
Motivation: 现有Graph Transformers架构在集成GNN和全局注意力机制时可能导致信息损失，需改进。

Method: 提出G2LFormer，采用全局到局部注意力方案，浅网络层用注意力机制获取全局信息，深层用GNN模块学习局部结构信息，引入跨层信息融合策略。

Result: 在节点级和图级任务上与先进模型对比，G2LFormer表现出色且保持线性复杂度。

Conclusion: G2LFormer的全局到局部注意力方案可行，能有效防止信息损失并保持性能和复杂度优势。

Abstract: Graph Transformers (GTs) show considerable potential in graph representation
learning. The architecture of GTs typically integrates Graph Neural Networks
(GNNs) with global attention mechanisms either in parallel or as a precursor to
attention mechanisms, yielding a local-and-global or local-to-global attention
scheme. However, as the global attention mechanism primarily captures
long-range dependencies between nodes, these integration schemes may suffer
from information loss, where the local neighborhood information learned by GNN
could be diluted by the attention mechanism. Therefore, we propose G2LFormer,
featuring a novel global-to-local attention scheme where the shallow network
layers use attention mechanisms to capture global information, while the deeper
layers employ GNN modules to learn local structural information, thereby
preventing nodes from ignoring their immediate neighbors. An effective
cross-layer information fusion strategy is introduced to allow local layers to
retain beneficial information from global layers and alleviate information
loss, with acceptable trade-offs in scalability. To validate the feasibility of
the global-to-local attention scheme, we compare G2LFormer with
state-of-the-art linear GTs and GNNs on node-level and graph-level tasks. The
results indicate that G2LFormer exhibits excellent performance while keeping
linear complexity.

</details>


### [90] [DPANet: Dual Pyramid Attention Network for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.14868)
*Qianyang Li,Xingjun Zhang,Shaoxun Wang,Jia Wei*

Main category: cs.LG

TL;DR: 通过消融实验验证DPANet关键组件，证明全模型性能最优，异质时频信息融合及交互融合块很关键。


<details>
  <summary>Details</summary>
Motivation: 验证DPANet关键组件以及双域假设。

Method: 进行消融实验，设计仅时域和仅频域模型，用简单方法替代交叉注意力机制。

Result: 全模型性能优于所有变体；仅时域和仅频域模型表现差；用简单方法替代交叉注意力机制性能下降最严重。

Conclusion: 异质时频信息融合是关键，交互融合块是最关键组件。

Abstract: We conducted rigorous ablation studies to validate DPANet's key components
(Table \ref{tab:ablation-study}). The full model consistently outperforms all
variants. To test our dual-domain hypothesis, we designed two specialized
versions: a Temporal-Only model (fusing two identical temporal pyramids) and a
Frequency-Only model (fusing two spectral pyramids). Both variants
underperformed significantly, confirming that the fusion of heterogeneous
temporal and frequency information is critical. Furthermore, replacing the
cross-attention mechanism with a simpler method (w/o Cross-Fusion) caused the
most severe performance degradation. This result underscores that our
interactive fusion block is the most essential component.

</details>


### [91] [Learning Graph from Smooth Signals under Partial Observation: A Robustness Analysis](https://arxiv.org/abs/2509.14887)
*Hoang-Son Nguyen,Hoi-To Wai*

Main category: cs.LG

TL;DR: 本文证明了普通图拓扑学习方法对低通滤波图信号的部分观测具有隐式鲁棒性，理论和实验都支持这一发现。


<details>
  <summary>Details</summary>
Motivation: 现有文献对‘朴素’的、不考虑隐藏节点的图学习方法的鲁棒性分析不足，需进行研究。

Method: 将受限等距性质（RIP）扩展到图学习目标中的狄利克雷能量函数。

Result: 基于平滑性的图学习公式在部分观测下可恢复与观测节点对应的真实图拓扑。

Conclusion: 普通图拓扑学习方法对部分观测的低通滤波图信号具有隐式鲁棒性，合成和真实数据实验证实了该结论。

Abstract: Learning the graph underlying a networked system from nodal signals is
crucial to downstream tasks in graph signal processing and machine learning.
The presence of hidden nodes whose signals are not observable might corrupt the
estimated graph. While existing works proposed various robustifications of
vanilla graph learning objectives by explicitly accounting for the presence of
these hidden nodes, a robustness analysis of "naive", hidden-node agnostic
approaches is still underexplored. This work demonstrates that vanilla graph
topology learning methods are implicitly robust to partial observations of
low-pass filtered graph signals. We achieve this theoretical result through
extending the restricted isometry property (RIP) to the Dirichlet energy
function used in graph learning objectives. We show that smoothness-based graph
learning formulation (e.g., the GL-SigRep method) on partial observations can
recover the ground truth graph topology corresponding to the observed nodes.
Synthetic and real data experiments corroborate our findings.

</details>


### [92] [Leveraging Reinforcement Learning, Genetic Algorithms and Transformers for background determination in particle physics](https://arxiv.org/abs/2509.14894)
*Guillermo Hijano Mendizabal,Davide Lancierini,Alex Marshall,Andrea Mauri,Patrick Haworth Owen,Mitesh Patel,Konstantinos Petridis,Shah Rukh Qasim,Nicola Serra,William Sutcliffe,Hanae Tilquin*

Main category: cs.LG

TL;DR: 本文提出利用强化学习（RL）和遗传算法（GA）结合的方法解决美强子衰变实验研究中背景确定难题，该方法具广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 美强子衰变实验研究因背景复杂、无系统方法且受计算限制，需要新方法确定关键背景。

Method: 提出利用RL系统确定关键背景的方法；引入结合RL和GA的新算法，利用GA探索轨迹空间并指导RL训练，RL代理结合transformer架构处理衰变序列。

Result: 文中未提及具体结果。

Conclusion: 文中未提及明确结论，但方法对美强子物理及其他粒子物理测量有潜在应用价值。

Abstract: Experimental studies of beauty hadron decays face significant challenges due
to a wide range of backgrounds arising from the numerous possible decay
channels with similar final states. For a particular signal decay, the process
for ascertaining the most relevant background processes necessitates a detailed
analysis of final state particles, potential misidentifications, and kinematic
overlaps, which, due to computational limitations, is restricted to the
simulation of only the most relevant backgrounds. Moreover, this process
typically relies on the physicist's intuition and expertise, as no systematic
method exists.
  This paper has two primary goals. First, from a particle physics perspective,
we present a novel approach that utilises Reinforcement Learning (RL) to
overcome the aforementioned challenges by systematically determining the
critical backgrounds affecting beauty hadron decay measurements. While beauty
hadron physics serves as the case study in this work, the proposed strategy is
broadly adaptable to other types of particle physics measurements. Second, from
a Machine Learning perspective, we introduce a novel algorithm which exploits
the synergy between RL and Genetic Algorithms (GAs) for environments with
highly sparse rewards and a large trajectory space. This strategy leverages GAs
to efficiently explore the trajectory space and identify successful
trajectories, which are used to guide the RL agent's training. Our method also
incorporates a transformer architecture for the RL agent to handle token
sequences representing decays.

</details>


### [93] [Robust Barycenters of Persistence Diagrams](https://arxiv.org/abs/2509.14904)
*Keanu Sisouk,Eloi Tanguy,Julie Delon,Julien Tierny*

Main category: cs.LG

TL;DR: 提出计算持久图鲁棒Wasserstein重心的通用方法，适用于通用运输成本，在聚类和字典编码应用中展示了对异常值的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 经典方法仅适用于q = 2的q - Wasserstein距离相关运输成本，需要适用于通用运输成本的方法。

Method: 采用替代的定点方法来计算通用运输成本（q > 1）下的重心图。

Result: 在持久图的聚类和字典编码两个应用中展示了方法对异常值的鲁棒性，代码开源。

Conclusion: 提出的通用框架为计算持久图的Wasserstein重心提供了更鲁棒的解决方案。

Abstract: This short paper presents a general approach for computing robust Wasserstein
barycenters of persistence diagrams. The classical method consists in computing
assignment arithmetic means after finding the optimal transport plans between
the barycenter and the persistence diagrams. However, this procedure only works
for the transportation cost related to the $q$-Wasserstein distance $W_q$ when
$q=2$. We adapt an alternative fixed-point method to compute a barycenter
diagram for generic transportation costs ($q > 1$), in particular those robust
to outliers, $q \in (1,2)$. We show the utility of our work in two
applications: \emph{(i)} the clustering of persistence diagrams on their metric
space and \emph{(ii)} the dictionary encoding of persistence diagrams. In both
scenarios, we demonstrate the added robustness to outliers provided by our
generalized framework. Our Python implementation is available at this address:
https://github.com/Keanu-Sisouk/RobustBarycenter .

</details>


### [94] [Self-Explaining Reinforcement Learning for Mobile Network Resource Allocation](https://arxiv.org/abs/2509.14925)
*Konrad Nowosadko,Franco Ruggeri,Ahmad Terra*

Main category: cs.LG

TL;DR: 提出基于SENNs和解释提取方法解决强化学习缺乏透明度问题，在移动网络资源分配问题验证有效，表现与现有方法相当且能提供解释。


<details>
  <summary>Details</summary>
Motivation: 强化学习结合深度神经网络时缺乏透明度，黑盒特性阻碍可解释性和降低可信度，尤其在关键领域。

Method: 提出基于自解释神经网络（SENNs）及解释提取方法，针对低维问题生成模型行为的局部和全局解释。

Result: 在移动网络资源分配问题上评估，SENNs能构成具有竞争力的可解释解决方案，表现与现有方法相当。

Conclusion: SENNs在低维任务中具有提高人工智能决策透明度和可信度的潜力。

Abstract: Reinforcement Learning (RL) methods that incorporate deep neural networks
(DNN), though powerful, often lack transparency. Their black-box characteristic
hinders interpretability and reduces trustworthiness, particularly in critical
domains. To address this challenge in RL tasks, we propose a solution based on
Self-Explaining Neural Networks (SENNs) along with explanation extraction
methods to enhance interpretability while maintaining predictive accuracy. Our
approach targets low-dimensionality problems to generate robust local and
global explanations of the model's behaviour. We evaluate the proposed method
on the resource allocation problem in mobile networks, demonstrating that SENNs
can constitute interpretable solutions with competitive performance. This work
highlights the potential of SENNs to improve transparency and trust in
AI-driven decision-making for low-dimensional tasks. Our approach strong
performance on par with the existing state-of-the-art methods, while providing
robust explanations.

</details>


### [95] [DAG: A Dual Causal Network for Time Series Forecasting with Exogenous Variables](https://arxiv.org/abs/2509.14933)
*Xiangfei Qiu,Yuhan Zhu,Zhengyu Li,Hanyin Cheng,Xingjian Wu,Chenjuan Guo,Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: 现有时间序列预测含外生变量方法有缺陷，本文提出DAG框架提升预测效果。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列含外生变量预测方法未利用未来外生变量且未考虑因果关系，性能不佳，需改进。

Method: 提出DAG框架，含时间因果模块和通道因果模块，各模块有因果发现和因果注入子模块。

Result: 文中未提及具体结果。

Conclusion: 文中未明确给出结论。

Abstract: Time series forecasting is crucial in various fields such as economics,
traffic, and AIOps. However, in real-world applications, focusing solely on the
endogenous variables (i.e., target variables), is often insufficient to ensure
accurate predictions. Considering exogenous variables (i.e., covariates)
provides additional predictive information, thereby improving forecasting
accuracy. However, existing methods for time series forecasting with exogenous
variables (TSF-X) have the following shortcomings: 1) they do not leverage
future exogenous variables, 2) they fail to account for the causal
relationships between endogenous and exogenous variables. As a result, their
performance is suboptimal. In this study, to better leverage exogenous
variables, especially future exogenous variable, we propose a general framework
DAG, which utilizes dual causal network along both the temporal and channel
dimensions for time series forecasting with exogenous variables. Specifically,
we first introduce the Temporal Causal Module, which includes a causal
discovery module to capture how historical exogenous variables affect future
exogenous variables. Following this, we construct a causal injection module
that incorporates the discovered causal relationships into the process of
forecasting future endogenous variables based on historical endogenous
variables. Next, we propose the Channel Causal Module, which follows a similar
design principle. It features a causal discovery module models how historical
exogenous variables influence historical endogenous variables, and a causal
injection module incorporates the discovered relationships to enhance the
prediction of future endogenous variables based on future exogenous variables.

</details>


### [96] [A Comparative Analysis of Transformer Models in Social Bot Detection](https://arxiv.org/abs/2509.14936)
*Rohan Veit,Michael Lones*

Main category: cs.LG

TL;DR: 本文对比基于编码器和解码器变压器的机器人检测模型效果，发现编码器模型更准确稳健，解码器模型适应性强。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中人工用户（机器人）误导他人问题因先进文本生成工具而加剧，需评估不同类型机器人检测模型效果。

Method: 开发评估管道来评估基于编码器和解码器变压器的分类器性能。

Result: 编码器分类器更准确和稳健，解码器模型通过特定任务对齐表现出更强适应性，有更好泛化潜力。

Conclusion: 研究结果有助于防止数字环境被操纵，保护在线讨论的完整性。

Abstract: Social media has become a key medium of communication in today's society.
This realisation has led to many parties employing artificial users (or bots)
to mislead others into believing untruths or acting in a beneficial manner to
such parties. Sophisticated text generation tools, such as large language
models, have further exacerbated this issue. This paper aims to compare the
effectiveness of bot detection models based on encoder and decoder
transformers. Pipelines are developed to evaluate the performance of these
classifiers, revealing that encoder-based classifiers demonstrate greater
accuracy and robustness. However, decoder-based models showed greater
adaptability through task-specific alignment, suggesting more potential for
generalisation across different use cases in addition to superior observa.
These findings contribute to the ongoing effort to prevent digital environments
being manipulated while protecting the integrity of online discussion.

</details>


### [97] [Hierarchical Federated Learning for Social Network with Mobility](https://arxiv.org/abs/2509.14938)
*Zeyu Chen,Wen Chen,Jun Li,Qingqing Wu,Ming Ding,Xuefeng Han,Xiumei Deng,Liwei Wang*

Main category: cs.LG

TL;DR: 提出基于社交网络带移动性的分层联邦学习框架HFL - SNM，解决资源分配和客户端调度联合优化问题并提出DO - SNM算法，实验显示算法性能优且能耗低。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习框架常忽略客户端移动性建模，本文考虑客户端数据共享和移动性模式，在资源有限下解决能耗问题。

Method: 提出HFL - SNM框架，将优化问题分解为多个子问题，基于初步实验结果分析并提出DO - SNM算法。

Result: 实验表明，与传统基线算法相比，提出的算法在显著降低能耗的同时实现了更优的模型性能。

Conclusion: 所提的HFL - SNM框架和DO - SNM算法在联邦学习中能兼顾模型性能和能耗降低。

Abstract: Federated Learning (FL) offers a decentralized solution that allows
collaborative local model training and global aggregation, thereby protecting
data privacy. In conventional FL frameworks, data privacy is typically
preserved under the assumption that local data remains absolutely private,
whereas the mobility of clients is frequently neglected in explicit modeling.
In this paper, we propose a hierarchical federated learning framework based on
the social network with mobility namely HFL-SNM that considers both data
sharing among clients and their mobility patterns. Under the constraints of
limited resources, we formulate a joint optimization problem of resource
allocation and client scheduling, which objective is to minimize the energy
consumption of clients during the FL process. In social network, we introduce
the concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate.
We analyze the impact of effective data and redundant data on the model
performance through preliminary experiments. We decouple the optimization
problem into multiple sub-problems, analyze them based on preliminary
experimental results, and propose Dynamic Optimization in Social Network with
Mobility (DO-SNM) algorithm. Experimental results demonstrate that our
algorithm achieves superior model performance while significantly reducing
energy consumption, compared to traditional baseline algorithms.

</details>


### [98] [Data-Driven Prediction of Maternal Nutritional Status in Ethiopia Using Ensemble Machine Learning Models](https://arxiv.org/abs/2509.14945)
*Amsalu Tessema,Tizazu Bayih,Kassahun Azezew,Ayenew Kassie*

Main category: cs.LG

TL;DR: 本研究用集成机器学习技术开发预测模型，基于埃塞俄比亚人口与健康调查数据，随机森林模型表现最佳，证明集成学习有效性并为改善孕产妇营养提供依据。


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚孕妇营养不良是重大公共卫生挑战，传统统计方法难以捕捉营养状况的复杂多维决定因素。

Method: 利用埃塞俄比亚人口与健康调查（2005 - 2020）数据，进行数据预处理、特征选择，应用包括XGBoost、随机森林等多种监督集成算法对营养状况分类。

Result: 随机森林模型表现最佳，对女性营养状况分类准确率达97.87%，精确率97.88%，召回率97.87%，F1分数97.87%，ROC AUC为99.86%。

Conclusion: 集成学习能从复杂数据集中捕捉隐藏模式，研究结果为医疗人员、政策制定者和研究人员提供了改善孕产妇营养和健康结果的数据驱动策略。

Abstract: Malnutrition among pregnant women is a major public health challenge in
Ethiopia, increasing the risk of adverse maternal and neonatal outcomes.
Traditional statistical approaches often fail to capture the complex and
multidimensional determinants of nutritional status. This study develops a
predictive model using ensemble machine learning techniques, leveraging data
from the Ethiopian Demographic and Health Survey (2005-2020), comprising 18,108
records with 30 socio-demographic and health attributes. Data preprocessing
included handling missing values, normalization, and balancing with SMOTE,
followed by feature selection to identify key predictors. Several supervised
ensemble algorithms including XGBoost, Random Forest, CatBoost, and AdaBoost
were applied to classify nutritional status. Among them, the Random Forest
model achieved the best performance, classifying women into four categories
(normal, moderate malnutrition, severe malnutrition, and overnutrition) with
97.87% accuracy, 97.88% precision, 97.87% recall, 97.87% F1-score, and 99.86%
ROC AUC. These findings demonstrate the effectiveness of ensemble learning in
capturing hidden patterns from complex datasets and provide timely insights for
early detection of nutritional risks. The results offer practical implications
for healthcare providers, policymakers, and researchers, supporting data-driven
strategies to improve maternal nutrition and health outcomes in Ethiopia.

</details>


### [99] [Stochastic Bilevel Optimization with Heavy-Tailed Noise](https://arxiv.org/abs/2509.14952)
*Zhuanghua Liu,Luo Luo*

Main category: cs.LG

TL;DR: 本文针对下层强凸、上层可能非凸的平滑双层优化问题，在随机重尾噪声环境下提出N²SBA算法，并给出复杂度，还应用于非凸强凹极小极大优化问题。


<details>
  <summary>Details</summary>
Motivation: 解决实际机器学习应用（如训练大语言模型和强化学习）中存在重尾噪声的平滑双层优化问题。

Method: 提出嵌套循环归一化随机双层近似（N²SBA）算法。

Result: 找到ε - 平稳点的随机一阶预言机（SFO）复杂度为特定表达式，应用于非凸强凹极小极大优化问题也有对应复杂度，且在p = 2时与已知最佳结果匹配。

Conclusion: 所提算法在随机重尾噪声环境下对平滑双层优化问题有效，复杂度表现良好。

Abstract: This paper considers the smooth bilevel optimization in which the lower-level
problem is strongly convex and the upper-level problem is possibly nonconvex.
We focus on the stochastic setting that the algorithm can access the unbiased
stochastic gradient evaluation with heavy-tailed noise, which is prevalent in
many machine learning applications such as training large language models and
reinforcement learning. We propose a nested-loop normalized stochastic bilevel
approximation (N$^2$SBA) for finding an $\epsilon$-stationary point with the
stochastic first-order oracle (SFO) complexity of
$\tilde{\mathcal{O}}\big(\kappa^{\frac{7p-3}{p-1}} \sigma^{\frac{p}{p-1}}
\epsilon^{-\frac{4 p - 2}{p-1}}\big)$, where $\kappa$ is the condition number,
$p\in(1,2]$ is the order of central moment for the noise, and $\sigma$ is the
noise level. Furthermore, we specialize our idea to solve the
nonconvex-strongly-concave minimax optimization problem, achieving an
$\epsilon$-stationary point with the SFO complexity of $\tilde{\mathcal
O}\big(\kappa^{\frac{2p-1}{p-1}} \sigma^{\frac{p}{p-1}}
\epsilon^{-\frac{3p-2}{p-1}}\big)$. All above upper bounds match the best-known
results under the special case of the bounded variance setting, i.e., $p=2$.

</details>


### [100] [FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated Sensing and Communication Indoor Scene Inference](https://arxiv.org/abs/2509.14968)
*Carlos Barroso-Fernández,Alejandro Calvillo-Fernandez,Antonio de la Oliva,Carlos J. Bernardos*

Main category: cs.LG

TL;DR: 本文提出用于ISAC室内场景推理的FAWN网络，融合Wi - Fi和5G信息，测试显示约84%时间误差低于0.6米。


<details>
  <summary>Details</summary>
Motivation: 现有集成传感与通信（ISAC）被动传感方案多局限于单一技术，限制了最大可达精度，需整合多种技术扩大覆盖范围。

Method: 利用ISAC被动传感，基于原始Transformer架构提出FAWN网络，融合Wi - Fi和5G信息。

Result: 搭建原型并在真实场景测试，约84%的时间误差低于0.6米。

Conclusion: FAWN网络能在不干扰现有通信的情况下理解物理世界，且有较好的精度。

Abstract: The upcoming generations of wireless technologies promise an era where
everything is interconnected and intelligent. As the need for intelligence
grows, networks must learn to better understand the physical world. However,
deploying dedicated hardware to perceive the environment is not always
feasible, mainly due to costs and/or complexity. Integrated Sensing and
Communication (ISAC) has made a step forward in addressing this challenge.
Within ISAC, passive sensing emerges as a cost-effective solution that reuses
wireless communications to sense the environment, without interfering with
existing communications. Nevertheless, the majority of current solutions are
limited to one technology (mostly Wi-Fi or 5G), constraining the maximum
accuracy reachable. As different technologies work with different spectrums, we
see a necessity in integrating more than one technology to augment the coverage
area. Hence, we take the advantage of ISAC passive sensing, to present FAWN, a
MultiEncoder Fusion-Attention Wave Network for ISAC indoor scene inference.
FAWN is based on the original transformers architecture, to fuse information
from Wi-Fi and 5G, making the network capable of understanding the physical
world without interfering with the current communication. To test our solution,
we have built a prototype and integrated it in a real scenario. Results show
errors below 0.6 m around 84% of times.

</details>


### [101] [Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering](https://arxiv.org/abs/2509.15024)
*Xuanting Xie,Bingheng Li,Erlin Pan,Rui Hou,Wenyu Chen,Zhao Kang*

Main category: cs.LG

TL;DR: 本文探讨注意力机制在图聚类中的应用，指出GNN和Transformer的不足，提出AGCN架构，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制在图结构数据应用不足，GNN和Transformer在图聚类中有各自缺陷，需研究注意力在无监督图学习中的作用。

Method: 进行全面实证分析，提出AGCN架构，将注意力机制嵌入图结构，结合理论分析对比，引入KV缓存机制和成对边缘对比损失。

Result: 广泛实验表明AGCN性能优于现有方法。

Conclusion: AGCN能有效提取全局信息并保持对局部拓扑线索的敏感性，可用于图聚类任务。

Abstract: Attention mechanisms have become a cornerstone in modern neural networks,
driving breakthroughs across diverse domains. However, their application to
graph structured data, where capturing topological connections is essential,
remains underexplored and underperforming compared to Graph Neural Networks
(GNNs), particularly in the graph clustering task. GNN tends to overemphasize
neighborhood aggregation, leading to a homogenization of node representations.
Conversely, Transformer tends to over globalize, highlighting distant nodes at
the expense of meaningful local patterns. This dichotomy raises a key question:
Is attention inherently redundant for unsupervised graph learning? To address
this, we conduct a comprehensive empirical analysis, uncovering the
complementary weaknesses of GNN and Transformer in graph clustering. Motivated
by these insights, we propose the Attentive Graph Clustering Network (AGCN) a
novel architecture that reinterprets the notion that graph is attention. AGCN
directly embeds the attention mechanism into the graph structure, enabling
effective global information extraction while maintaining sensitivity to local
topological cues. Our framework incorporates theoretical analysis to contrast
AGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV
cache mechanism to improve computational efficiency, and (2) a pairwise margin
contrastive loss to boost the discriminative capacity of the attention space.
Extensive experimental results demonstrate that AGCN outperforms
state-of-the-art methods.

</details>


### [102] [Sample Efficient Experience Replay in Non-stationary Environments](https://arxiv.org/abs/2509.15032)
*Tianyang Duan,Zongyuan Zhang,Songxiao Guo,Yuanye Zhao,Zheng Lin,Zihan Fang,Yi Liu,Dianxin Luan,Dong Huang,Heming Cui,Yong Cui*

Main category: cs.LG

TL;DR: 提出DEER框架解决非平稳环境强化学习问题，实验显示其比现有方法性能提升11.54%


<details>
  <summary>Details</summary>
Motivation: 非平稳环境中传统经验回放方法难以区分环境和策略变化，导致学习效率低

Method: 提出DoE指标隔离环境变化对价值函数的影响，引入DEER框架，用二元分类器检测环境变化并采用不同优先级策略

Result: 在四个非平稳基准测试中，DEER使离策略算法性能比现有最佳ER方法提升11.54%

Conclusion: DEER能更高效地处理非平稳环境中的强化学习问题，提升算法性能

Abstract: Reinforcement learning (RL) in non-stationary environments is challenging, as
changing dynamics and rewards quickly make past experiences outdated.
Traditional experience replay (ER) methods, especially those using TD-error
prioritization, struggle to distinguish between changes caused by the agent's
policy and those from the environment, resulting in inefficient learning under
dynamic conditions. To address this challenge, we propose the Discrepancy of
Environment Dynamics (DoE), a metric that isolates the effects of environment
shifts on value functions. Building on this, we introduce Discrepancy of
Environment Prioritized Experience Replay (DEER), an adaptive ER framework that
prioritizes transitions based on both policy updates and environmental changes.
DEER uses a binary classifier to detect environment changes and applies
distinct prioritization strategies before and after each shift, enabling more
sample-efficient learning. Experiments on four non-stationary benchmarks
demonstrate that DEER further improves the performance of off-policy algorithms
by 11.54 percent compared to the best-performing state-of-the-art ER methods.

</details>


### [103] [Beyond Marginals: Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection](https://arxiv.org/abs/2509.15033)
*Padmaksha Roy,Almuatazbellah Boker,Lamine Mili*

Main category: cs.LG

TL;DR: 本文旨在改进多元异常检测，通过在潜在空间建模联合依赖，结合transformer编码器和多元似然、copula，利用自监督对比学习训练时空组件。


<details>
  <summary>Details</summary>
Motivation: 现有多元异常检测方法假设时间序列变量（条件）独立，简化了现实交互，本文旨在解决该问题，改进多元异常检测。

Method: 在潜在空间建模联合依赖，解耦边缘分布、时间动态和变量间依赖；用transformer编码器捕捉时间模式，用多元似然和copula建模空间依赖；在潜在空间用自监督对比学习目标联合训练时空组件。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: In this paper, we aim to improve multivariate anomaly detection (AD) by
modeling the \textit{time-varying non-linear spatio-temporal correlations}
found in multivariate time series data . In multivariate time series data, an
anomaly may be indicated by the simultaneous deviation of interrelated time
series from their expected collective behavior, even when no individual time
series exhibits a clearly abnormal pattern on its own. In many existing
approaches, time series variables are assumed to be (conditionally)
independent, which oversimplifies real-world interactions. Our approach
addresses this by modeling joint dependencies in the latent space and
decoupling the modeling of \textit{marginal distributions, temporal dynamics,
and inter-variable dependencies}. We use a transformer encoder to capture
temporal patterns, and to model spatial (inter-variable) dependencies, we fit a
multi-variate likelihood and a copula. The temporal and the spatial components
are trained jointly in a latent space using a self-supervised contrastive
learning objective to learn meaningful feature representations to separate
normal and anomaly samples.

</details>


### [104] [From Patterns to Predictions: A Shapelet-Based Framework for Directional Forecasting in Noisy Financial Markets](https://arxiv.org/abs/2509.15040)
*Juwon Kim,Hyunwook Lee,Hyotaek Jeon,Seungmin Jin,Sungahn Ko*

Main category: cs.LG

TL;DR: 提出结合无监督模式提取与可解释预测的两阶段框架，在实验中表现出色且具可解释性


<details>
  <summary>Details</summary>
Motivation: 传统基于人为定义模式的方法存在结构模糊和规模歧义问题，深度学习模型缺乏透明度，需一种兼具准确性和可解释性的金融市场定向预测方法

Method: 提出两阶段框架，包括用SIMPC提取多元时间序列的循环模式，用JISC - Net进行短期定向运动预测

Result: 在比特币和三只标准普尔500指数股票的实验中，在12种指标 - 数据集组合中的11种里排名第一或第二，持续优于基线

Conclusion: 该方法能通过揭示驱动预测结果的潜在模式结构，实现透明的决策制定

Abstract: Directional forecasting in financial markets requires both accuracy and
interpretability. Before the advent of deep learning, interpretable approaches
based on human-defined patterns were prevalent, but their structural vagueness
and scale ambiguity hindered generalization. In contrast, deep learning models
can effectively capture complex dynamics, yet often offer limited transparency.
To bridge this gap, we propose a two-stage framework that integrates
unsupervised pattern extracion with interpretable forecasting. (i) SIMPC
segments and clusters multivariate time series, extracting recurrent patterns
that are invariant to amplitude scaling and temporal distortion, even under
varying window sizes. (ii) JISC-Net is a shapelet-based classifier that uses
the initial part of extracted patterns as input and forecasts subsequent
partial sequences for short-term directional movement. Experiments on Bitcoin
and three S&P 500 equities demonstrate that our method ranks first or second in
11 out of 12 metric--dataset combinations, consistently outperforming
baselines. Unlike conventional deep learning models that output buy-or-sell
signals without interpretable justification, our approach enables transparent
decision-making by revealing the underlying pattern structures that drive
predictive outcomes.

</details>


### [105] [Reinforcement Learning Agent for a 2D Shooter Game](https://arxiv.org/abs/2509.15042)
*Thomas Ackermann,Moritz Spang,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 本文提出结合离线模仿学习与在线强化学习的混合训练方法训练2D射击游戏智能体，相比纯强化学习方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 解决复杂游戏环境中强化学习智能体存在的稀疏奖励、训练不稳定和样本效率低等问题。

Method: 采用多头部神经网络，结合行为克隆和Q学习，先在基于规则的智能体示范数据上进行行为克隆，再过渡到强化学习。

Result: 混合方法在与基于规则的对手对战中胜率稳定超70%，远超纯强化学习方法。

Conclusion: 结合基于示范的初始化和强化学习优化，为复杂多智能体环境下开发游戏AI智能体提供了可靠解决方案。

Abstract: Reinforcement learning agents in complex game environments often suffer from
sparse rewards, training instability, and poor sample efficiency. This paper
presents a hybrid training approach that combines offline imitation learning
with online reinforcement learning for a 2D shooter game agent. We implement a
multi-head neural network with separate outputs for behavioral cloning and
Q-learning, unified by shared feature extraction layers with attention
mechanisms. Initial experiments using pure deep Q-Networks exhibited
significant instability, with agents frequently reverting to poor policies
despite occasional good performance. To address this, we developed a hybrid
methodology that begins with behavioral cloning on demonstration data from
rule-based agents, then transitions to reinforcement learning. Our hybrid
approach achieves consistently above 70% win rate against rule-based opponents,
substantially outperforming pure reinforcement learning methods which showed
high variance and frequent performance degradation. The multi-head architecture
enables effective knowledge transfer between learning modes while maintaining
training stability. Results demonstrate that combining demonstration-based
initialization with reinforcement learning optimization provides a robust
solution for developing game AI agents in complex multi-agent environments
where pure exploration proves insufficient.

</details>


### [106] [Credit Card Fraud Detection](https://arxiv.org/abs/2509.15044)
*Iva Popova,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 研究评估五种机器学习模型在信用卡欺诈检测中的表现，混合方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 解决信用卡欺诈检测中因类别不平衡和欺诈者模仿合法行为带来的挑战。

Method: 使用欠采样、SMOTE和混合方法，在真实数据集上评估逻辑回归、随机森林、XGBoost、KNN和MLP五种机器学习模型，并在原始不平衡测试集上评估。

Result: 混合方法在召回率和精确率之间取得最佳平衡，尤其提升了MLP和KNN的性能。

Conclusion: 混合方法在信用卡欺诈检测中表现更优。

Abstract: Credit card fraud remains a significant challenge due to class imbalance and
fraudsters mimicking legitimate behavior. This study evaluates five machine
learning models - Logistic Regression, Random Forest, XGBoost, K-Nearest
Neighbors (KNN), and Multi-Layer Perceptron (MLP) on a real-world dataset using
undersampling, SMOTE, and a hybrid approach. Our models are evaluated on the
original imbalanced test set to better reflect real-world performance. Results
show that the hybrid method achieves the best balance between recall and
precision, especially improving MLP and KNN performance.

</details>


### [107] [Balancing Sparse RNNs with Hyperparameterization Benefiting Meta-Learning](https://arxiv.org/abs/2509.15057)
*Quincy Hershey,Randy Paffenroth*

Main category: cs.LG

TL;DR: 本文为稀疏循环神经网络开发了替代超参数，结合隐藏比例指标提升性能，为广义元学习和模型优化提供路径。


<details>
  <summary>Details</summary>
Motivation: 为稀疏循环神经网络寻找能在改变权重矩阵稀疏性的同时提升整体性能的超参数。

Method: 开发替代超参数，定义隐藏比例指标，将可变稀疏RNN架构与隐藏比例指标结合。

Result: 显著提升了模型性能，提前改善性能预期。

Conclusion: 这种结合方法为基于数据集内在特征的广义元学习应用和模型优化提供了方向。

Abstract: This paper develops alternative hyperparameters for specifying sparse
Recurrent Neural Networks (RNNs). These hyperparameters allow for varying
sparsity within the trainable weight matrices of the model while improving
overall performance. This architecture enables the definition of a novel
metric, hidden proportion, which seeks to balance the distribution of unknowns
within the model and provides significant explanatory power of model
performance. Together, the use of the varied sparsity RNN architecture combined
with the hidden proportion metric generates significant performance gains while
improving performance expectations on an a priori basis. This combined approach
provides a path forward towards generalized meta-learning applications and
model optimization based on intrinsic characteristics of the data set,
including input and output dimensions.

</details>


### [108] [Improving Internet Traffic Matrix Prediction via Time Series Clustering](https://arxiv.org/abs/2509.15072)
*Martha Cash,Alexander Wyglinski*

Main category: cs.LG

TL;DR: 提出利用时间序列聚类改进互联网流量矩阵预测的框架，通过两种聚类策略提高预测效果，降低RMSE和MLU偏差。


<details>
  <summary>Details</summary>
Motivation: 流量矩阵中流量的不同时间行为会影响单一模型的预测准确性，需改进预测方法。

Method: 提出源聚类和直方图聚类两种策略，在模型训练前对具有相似时间模式的流量进行分组。

Result: 与现有方法相比，在Abilene和GÉANT中RMSE分别降低92%和75%；在路由场景中，MLU偏差分别降低18%和21%。

Conclusion: 聚类策略在流量矩阵用于网络优化时具有实际益处，能有效提高预测效果。

Abstract: We present a novel framework that leverages time series clustering to improve
internet traffic matrix (TM) prediction using deep learning (DL) models.
Traffic flows within a TM often exhibit diverse temporal behaviors, which can
hinder prediction accuracy when training a single model across all flows. To
address this, we propose two clustering strategies, source clustering and
histogram clustering, that group flows with similar temporal patterns prior to
model training. Clustering creates more homogeneous data subsets, enabling
models to capture underlying patterns more effectively and generalize better
than global prediction approaches that fit a single model to the entire TM.
Compared to existing TM prediction methods, our method reduces RMSE by up to
92\% for Abilene and 75\% for G\'EANT. In routing scenarios, our clustered
predictions also reduce maximum link utilization (MLU) bias by 18\% and 21\%,
respectively, demonstrating the practical benefits of clustering when TMs are
used for network optimization.

</details>


### [109] [Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits](https://arxiv.org/abs/2509.15073)
*Shaoang Li,Jian Li*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Non-stationary multi-armed bandits enable agents to adapt to changing
environments by incorporating mechanisms to detect and respond to shifts in
reward distributions, making them well-suited for dynamic settings. However,
existing approaches typically assume that reward feedback is available at every
round - an assumption that overlooks many real-world scenarios where feedback
is limited. In this paper, we take a significant step forward by introducing a
new model of constrained feedback in non-stationary multi-armed bandits, where
the availability of reward feedback is restricted. We propose the first
prior-free algorithm - that is, one that does not require prior knowledge of
the degree of non-stationarity - that achieves near-optimal dynamic regret in
this setting. Specifically, our algorithm attains a dynamic regret of
$\tilde{\mathcal{O}}({K^{1/3} V_T^{1/3} T }/{ B^{1/3}})$, where $T$ is the
number of rounds, $K$ is the number of arms, $B$ is the query budget, and $V_T$
is the variation budget capturing the degree of non-stationarity.

</details>


### [110] [Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models](https://arxiv.org/abs/2509.15076)
*Mohammad Saleh Vahdatpour,Maryam Eyvazi,Yanqing Zhang*

Main category: cs.LG

TL;DR: 本文提出AI驱动代理，结合统计纹理分析与监督学习预测空气污染，用VLM引导图像生成可视化结果，验证方法有效性，后续将采用绿色CNN架构。


<details>
  <summary>Details</summary>
Motivation: 传统空气污染监测系统空间覆盖和可及性有限，需新方法监测和展示污染信息。

Method: 结合统计纹理分析与监督学习进行污染分类，利用VLM引导图像生成可视化结果，系统设计融入以人为本原则。

Result: 使用城市天空图像数据集验证方法在污染水平估计和语义一致视觉合成上有效。

Conclusion: 该方法可提高透明度，支持环境决策，后续将采用绿色CNN架构实现边缘平台实时推理。

Abstract: Air pollution remains a critical threat to public health and environmental
sustainability, yet conventional monitoring systems are often constrained by
limited spatial coverage and accessibility. This paper proposes an AI-driven
agent that predicts ambient air pollution levels from sky images and
synthesizes realistic visualizations of pollution scenarios using generative
modeling. Our approach combines statistical texture analysis with supervised
learning for pollution classification, and leverages vision-language model
(VLM)-guided image generation to produce interpretable representations of air
quality conditions. The generated visuals simulate varying degrees of
pollution, offering a foundation for user-facing interfaces that improve
transparency and support informed environmental decision-making. These outputs
can be seamlessly integrated into intelligent applications aimed at enhancing
situational awareness and encouraging behavioral responses based on real-time
forecasts. We validate our method using a dataset of urban sky images and
demonstrate its effectiveness in both pollution level estimation and
semantically consistent visual synthesis. The system design further
incorporates human-centered user experience principles to ensure accessibility,
clarity, and public engagement in air quality forecasting. To support scalable
and energy-efficient deployment, future iterations will incorporate a green CNN
architecture enhanced with FPGA-based incremental learning, enabling real-time
inference on edge platforms.

</details>


### [111] [Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning](https://arxiv.org/abs/2509.15087)
*Lei Wang,Jieming Bian,Letian Zhang,Jie Xu*

Main category: cs.LG

TL;DR: 论文聚焦联邦LoRA微调的两大挑战，提出FedLEASE框架，实验显示其在异构客户端设置中显著优于现有方法且保持通信效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调需大量特定领域数据，联邦学习应用于大模型有计算约束，单LoRA模块难处理多领域异构数据，需解决联邦LoRA微调的两大挑战。

Method: 提出FedLEASE框架，基于表示相似性对客户端聚类以分配和训练特定领域LoRA专家，引入自适应top - M专家混合机制让客户端选择最优数量专家。

Result: 在多个基准数据集上的大量实验表明，FedLEASE在异构客户端设置下显著优于现有联邦微调方法。

Conclusion: FedLEASE能在异构客户端设置中有效进行联邦LoRA微调，且保持通信效率。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across
various tasks, but fine-tuning them for domain-specific applications often
requires substantial domain-specific data that may be distributed across
multiple organizations. Federated Learning (FL) offers a privacy-preserving
solution, but faces challenges with computational constraints when applied to
LLMs. Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient
fine-tuning approach, though a single LoRA module often struggles with
heterogeneous data across diverse domains. This paper addresses two critical
challenges in federated LoRA fine-tuning: 1. determining the optimal number and
allocation of LoRA experts across heterogeneous clients, and 2. enabling
clients to selectively utilize these experts based on their specific data
characteristics. We propose FedLEASE (Federated adaptive LoRA Expert Allocation
and SElection), a novel framework that adaptively clusters clients based on
representation similarity to allocate and train domain-specific LoRA experts.
It also introduces an adaptive top-$M$ Mixture-of-Experts mechanism that allows
each client to select the optimal number of utilized experts. Our extensive
experiments on diverse benchmark datasets demonstrate that FedLEASE
significantly outperforms existing federated fine-tuning approaches in
heterogeneous client settings while maintaining communication efficiency.

</details>


### [112] [The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning](https://arxiv.org/abs/2509.15097)
*Mohammad Saleh Vahdatpour,Huaiyuan Chu,Yanqing Zhang*

Main category: cs.LG

TL;DR: 提出结合分层分解、FPGA直接方程求解和增量学习的混合框架及Compound LLM框架，降低计算成本并保持高性能，适合能源受限环境。


<details>
  <summary>Details</summary>
Motivation: 深度学习计算和能源需求上升，传统梯度训练方法低效且能耗高，需可持续解决方案。

Method: 提出混合框架，将神经网络分为两层，低层用FPGA单步方程求解优化，高层采用自适应增量学习；引入Compound LLM框架，分层部署LLM模块。

Result: 理论分析和架构洞察表明该方法显著降低计算成本，保持高模型性能。

Conclusion: 该集成设计增强可扩展性，减少冗余计算，符合可持续AI原则，适合边缘部署和实时适应。

Abstract: The rising computational and energy demands of deep learning, particularly in
large-scale architectures such as foundation models and large language models
(LLMs), pose significant challenges to sustainability. Traditional
gradient-based training methods are inefficient, requiring numerous iterative
updates and high power consumption. To address these limitations, we propose a
hybrid framework that combines hierarchical decomposition with FPGA-based
direct equation solving and incremental learning. Our method divides the neural
network into two functional tiers: lower layers are optimized via single-step
equation solving on FPGAs for efficient and parallelizable feature extraction,
while higher layers employ adaptive incremental learning to support continual
updates without full retraining. Building upon this foundation, we introduce
the Compound LLM framework, which explicitly deploys LLM modules across both
hierarchy levels. The lower-level LLM handles reusable representation learning
with minimal energy overhead, while the upper-level LLM performs adaptive
decision-making through energy-aware updates. This integrated design enhances
scalability, reduces redundant computation, and aligns with the principles of
sustainable AI. Theoretical analysis and architectural insights demonstrate
that our method reduces computational costs significantly while preserving high
model performance, making it well-suited for edge deployment and real-time
adaptation in energy-constrained environments.

</details>


### [113] [Super-Linear: A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting](https://arxiv.org/abs/2509.15105)
*Liran Nochumsohn,Raz Marshanski,Hedi Zisling,Omri Azencot*

Main category: cs.LG

TL;DR: 介绍用于通用预测的轻量级可扩展混合专家模型Super - Linear，性能佳且效率高。


<details>
  <summary>Details</summary>
Motivation: 现有大预训练模型在时间序列预测中虽零样本性能强但计算成本高，需要能跨不同数据集泛化的模型。

Method: 用简单频率专用线性专家取代深度架构，在多频率制度的重采样数据上训练，通过轻量级频谱门控机制动态选择相关专家。

Result: Super - Linear达到了最先进的性能，具有更高效率、对不同采样率的鲁棒性和更强的可解释性。

Conclusion: Super - Linear是一个有效且高效的时间序列预测模型。

Abstract: Time series forecasting (TSF) is critical in domains like energy, finance,
healthcare, and logistics, requiring models that generalize across diverse
datasets. Large pre-trained models such as Chronos and Time-MoE show strong
zero-shot (ZS) performance but suffer from high computational costs. In this
work, We introduce Super-Linear, a lightweight and scalable mixture-of-experts
(MoE) model for general forecasting. It replaces deep architectures with simple
frequency-specialized linear experts, trained on resampled data across multiple
frequency regimes. A lightweight spectral gating mechanism dynamically selects
relevant experts, enabling efficient, accurate forecasting. Despite its
simplicity, Super-Linear matches state-of-the-art performance while offering
superior efficiency, robustness to various sampling rates, and enhanced
interpretability. The implementation of Super-Linear is available at
\href{https://github.com/azencot-group/SuperLinear}{https://github.com/azencot-group/SuperLinear}

</details>


### [114] [Limitations of Public Chest Radiography Datasets for Artificial Intelligence: Label Quality, Domain Shift, Bias and Evaluation Challenges](https://arxiv.org/abs/2509.15107)
*Amy Rafferty,Rishi Ramaesh,Ajitha Rajan*

Main category: cs.LG

TL;DR: 分析公开胸部X光数据集用于AI诊断的局限，发现标签质量、数据集偏差和领域偏移等问题，强调需临床验证数据集和公平评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有公开胸部X光数据集存在标签提取错误、领域偏移、群体偏差和评估不足等局限，影响AI模型性能。

Method: 对标签质量、数据集偏差和领域偏移进行系统分析，开展跨数据集领域偏移评估、训练源分类模型、进行亚组分析和专家审查。

Result: 跨数据集评估显示外部性能大幅下降；源分类模型能高准确率区分数据集，亚组分析显示少数年龄和性别群体性能降低；专家审查发现与公开数据集标签存在显著分歧。

Conclusion: 当前基准存在临床弱点，需要临床医生验证的数据集和更公平的评估框架。

Abstract: Artificial intelligence has shown significant promise in chest radiography,
where deep learning models can approach radiologist-level diagnostic
performance. Progress has been accelerated by large public datasets such as
MIMIC-CXR, ChestX-ray14, PadChest, and CheXpert, which provide hundreds of
thousands of labelled images with pathology annotations. However, these
datasets also present important limitations. Automated label extraction from
radiology reports introduces errors, particularly in handling uncertainty and
negation, and radiologist review frequently disagrees with assigned labels. In
addition, domain shift and population bias restrict model generalisability,
while evaluation practices often overlook clinically meaningful measures. We
conduct a systematic analysis of these challenges, focusing on label quality,
dataset bias, and domain shift. Our cross-dataset domain shift evaluation
across multiple model architectures revealed substantial external performance
degradation, with pronounced reductions in AUPRC and F1 scores relative to
internal testing. To assess dataset bias, we trained a source-classification
model that distinguished datasets with near-perfect accuracy, and performed
subgroup analyses showing reduced performance for minority age and sex groups.
Finally, expert review by two board-certified radiologists identified
significant disagreement with public dataset labels. Our findings highlight
important clinical weaknesses of current benchmarks and emphasise the need for
clinician-validated datasets and fairer evaluation frameworks.

</details>


### [115] [TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference](https://arxiv.org/abs/2509.15110)
*Dan Zhang,Min Cai,Jonathan Li,Ziniu Hu,Yisong Yue,Yuxiao Dong,Jie Tang*

Main category: cs.LG

TL;DR: 提出TDRM方法学习更平滑可靠奖励模型，结合在线RL有增益，实验显示TD训练的PRMs能提升性能，与RLVR结合更高效。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型缺乏时间一致性，导致策略更新无效和RL训练不稳定。

Method: 引入TDRM，在训练中最小化时间差异进行正则化，将其融入演员 - 评论家风格的在线RL循环。

Result: TD训练的PRMs在Best - of - N和树搜索设置中提升性能，与RLVR结合数据效率更高，在多个模型变体上产生更高质量语言模型策略。

Conclusion: TDRM方法有效，能提升奖励模型性能和RL训练效率。

Abstract: Reward models are central to both reinforcement learning (RL) with language
models and inference-time verification. However, existing reward models often
lack temporal consistency, leading to ineffective policy updates and unstable
RL training. We introduce TDRM, a method for learning smoother and more
reliable reward models by minimizing temporal differences during training. This
temporal-difference (TD) regularization produces smooth rewards and improves
alignment with long-term objectives. Incorporating TDRM into the actor-critic
style online RL loop yields consistent empirical gains. It is worth noting that
TDRM is a supplement to verifiable reward methods, and both can be used in
series. Experiments show that TD-trained process reward models (PRMs) improve
performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%)
settings. When combined with Reinforcement Learning with Verifiable Rewards
(RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable
performance with just 2.5k data to what baseline methods require 50.1k data to
attain -- and yield higher-quality language model policies on 8 model variants
(5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414,
Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release
all code at https://github.com/THUDM/TDRM.

</details>


### [116] [Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers](https://arxiv.org/abs/2509.15113)
*Andrei Chertkov,Artem Basharin,Mikhail Saygin,Evgeny Frolov,Stanislav Straupe,Ivan Oseledets*

Main category: cs.LG

TL;DR: 针对节能高性能AI系统需求，提出混合网络端到端训练框架，在多任务上达近数字基线准确率，为集成不可微物理组件到AI系统提供途径。


<details>
  <summary>Details</summary>
Motivation: 节能高性能AI系统需求促使关注替代计算平台，但物理组件集成到深度学习管道有挑战，因此要开发结合数字神经网络和可重构物理层的混合架构。

Method: 提出端到端训练框架，集成随机零阶优化更新物理层内部参数，用动态低秩替代模型实现梯度传播，采用隐式投影器分裂积分算法更新替代模型。

Result: 在计算机视觉、音频分类和语言建模等任务中，该方法达到近数字基线准确率，能有效对含多种不可微物理组件的混合模型进行端到端训练。

Conclusion: 该工作桥接硬件感知深度学习和无梯度优化，为将不可微物理组件集成到可扩展、端到端可训练的AI系统提供实用途径。

Abstract: The growing demand for energy-efficient, high-performance AI systems has led
to increased attention on alternative computing platforms (e.g., photonic,
neuromorphic) due to their potential to accelerate learning and inference.
However, integrating such physical components into deep learning pipelines
remains challenging, as physical devices often offer limited expressiveness,
and their non-differentiable nature renders on-device backpropagation difficult
or infeasible. This motivates the development of hybrid architectures that
combine digital neural networks with reconfigurable physical layers, which
effectively behave as black boxes. In this work, we present a framework for the
end-to-end training of such hybrid networks. This framework integrates
stochastic zeroth-order optimization for updating the physical layer's internal
parameters with a dynamic low-rank surrogate model that enables gradient
propagation through the physical layer. A key component of our approach is the
implicit projector-splitting integrator algorithm, which updates the
lightweight surrogate model after each forward pass with minimal hardware
queries, thereby avoiding costly full matrix reconstruction. We demonstrate our
method across diverse deep learning tasks, including: computer vision, audio
classification, and language modeling. Notably, across all modalities, the
proposed approach achieves near-digital baseline accuracy and consistently
enables effective end-to-end training of hybrid models incorporating various
non-differentiable physical components (spatial light modulators, microring
resonators, and Mach-Zehnder interferometers). This work bridges hardware-aware
deep learning and gradient-free optimization, thereby offering a practical
pathway for integrating non-differentiable physical components into scalable,
end-to-end trainable AI systems.

</details>


### [117] [Efficient Conformal Prediction for Regression Models under Label Noise](https://arxiv.org/abs/2509.15120)
*Yahav Cohen,Jacob Goldberger,Tom Tirer*

Main category: cs.LG

TL;DR: 本文针对校准集含噪声标签时将共形预测（CP）应用于回归模型的问题，提出估计无噪声CP阈值的方法并转化为实用算法，在医学影像回归数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在高风险场景如医学影像应用中，为回归模型预测提供可靠置信区间很关键，现有CP方法在校准集含噪声标签时存在问题，需解决该问题。

Method: 先建立数学上合理的程序来估计无噪声CP阈值，再将其转化为实用算法以克服回归问题连续性带来的挑战。

Result: 在两个有高斯标签噪声的医学影像回归数据集上评估，该方法显著优于现有替代方法，性能接近干净标签设置。

Conclusion: 所提出的方法能有效解决校准集含噪声标签时CP应用于回归模型的问题，具有良好性能。

Abstract: In high-stakes scenarios, such as medical imaging applications, it is
critical to equip the predictions of a regression model with reliable
confidence intervals. Recently, Conformal Prediction (CP) has emerged as a
powerful statistical framework that, based on a labeled calibration set,
generates intervals that include the true labels with a pre-specified
probability. In this paper, we address the problem of applying CP for
regression models when the calibration set contains noisy labels. We begin by
establishing a mathematically grounded procedure for estimating the noise-free
CP threshold. Then, we turn it into a practical algorithm that overcomes the
challenges arising from the continuous nature of the regression problem. We
evaluate the proposed method on two medical imaging regression datasets with
Gaussian label noise. Our method significantly outperforms the existing
alternative, achieving performance close to the clean-label setting.

</details>


### [118] [Optimal Learning from Label Proportions with General Loss Functions](https://arxiv.org/abs/2509.15145)
*Lorne Applebaum,Travis Dick,Claudio Gentile,Haim Kaplan,Tomer Koren*

Main category: cs.LG

TL;DR: 针对在线广告问题研究从标签比例学习（LLP），提出新的去偏方法，提升LLP技术水平，有理论和实证优势。


<details>
  <summary>Details</summary>
Motivation: 解决在线广告中的从标签比例学习问题，目标是设计单个示例标签的预测器。

Method: 引入新颖通用的低方差去偏方法，结合标准技术。

Result: 显著推进LLP技术水平，提高样本复杂度保证，在多个基准数据集上验证有效性。

Conclusion: 提出的方法在LLP中有显著优势，优于标准基线。

Abstract: Motivated by problems in online advertising, we address the task of Learning
from Label Proportions (LLP). In this partially-supervised setting, training
data consists of groups of examples, termed bags, for which we only observe the
average label value. The main goal, however, remains the design of a predictor
for the labels of individual examples. We introduce a novel and versatile
low-variance de-biasing methodology to learn from aggregate label information,
significantly advancing the state of the art in LLP. Our approach exhibits
remarkable flexibility, seamlessly accommodating a broad spectrum of
practically relevant loss functions across both binary and multi-class
classification settings. By carefully combining our estimators with standard
techniques, we substantially improve sample complexity guarantees for a large
class of losses of practical relevance. We also empirically validate the
efficacy of our proposed approach across a diverse array of benchmark datasets,
demonstrating compelling empirical advantages over standard baselines.

</details>


### [119] [Who to Trust? Aggregating Client Knowledge in Logit-Based Federated Learning](https://arxiv.org/abs/2509.15147)
*Viktor Kovalchuk,Nikita Kotelevskii,Maxim Panov,Samuel Horváth,Martin Takáč*

Main category: cs.LG

TL;DR: 本文研究基于logit的联邦学习中异构客户端信息聚合问题，介绍并比较三种logit聚合方法，在MNIST和CIFAR - 10上评估，减少通信开销且提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习共享模型权重或梯度对大模型成本高，基于logit的联邦学习虽能降低成本，但异构客户端信息聚合仍具挑战。

Method: 引入并比较三种logit聚合方法：简单平均、不确定性加权平均和学习的元聚合器。

Result: 在MNIST和CIFAR - 10上评估，这些方法减少了通信开销，在非IID数据下提高了鲁棒性，且准确率与集中式训练相当。

Conclusion: 所提出的三种logit聚合方法在基于logit的联邦学习中有效，能解决异构客户端信息聚合问题。

Abstract: Federated learning (FL) usually shares model weights or gradients, which is
costly for large models. Logit-based FL reduces this cost by sharing only
logits computed on a public proxy dataset. However, aggregating information
from heterogeneous clients is still challenging. This paper studies this
problem, introduces and compares three logit aggregation methods: simple
averaging, uncertainty-weighted averaging, and a learned meta-aggregator.
Evaluated on MNIST and CIFAR-10, these methods reduce communication overhead,
improve robustness under non-IID data, and achieve accuracy competitive with
centralized training.

</details>


### [120] [Self-Improving Embodied Foundation Models](https://arxiv.org/abs/2509.15155)
*Seyed Kamyar Seyed Ghasemipour,Ayzaan Wahid,Jonathan Tompson,Pannag Sanketi,Igor Mordatch*

Main category: cs.LG

TL;DR: 提出机器人两阶段后训练方法，结合监督微调与自我提升，实验表明该方法样本效率高、成功率高且能自主获取新技能。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在机器人低级控制应用局限于行为克隆，受大语言模型微调中强化学习阶段启发开展研究。

Method: 两阶段后训练方法，第一阶段监督微调，用行为克隆和步骤预测目标微调预训练模型；第二阶段自我提升，用步骤预测提取奖励函数和成功检测器，让机器人自主练习下游任务。

Result: 结合监督微调与自我提升比扩大模仿数据收集样本效率更高、成功率更高；结合网络规模预训练和自我提升是样本高效的关键；能自主练习并获取新技能。

Conclusion: 结合预训练基础模型和在线自我提升对机器人自主技能获取有变革潜力。

Abstract: Foundation models trained on web-scale data have revolutionized robotics, but
their application to low-level control remains largely limited to behavioral
cloning. Drawing inspiration from the success of the reinforcement learning
stage in fine-tuning large language models, we propose a two-stage
post-training approach for robotics. The first stage, Supervised Fine-Tuning
(SFT), fine-tunes pretrained foundation models using both: a) behavioral
cloning, and b) steps-to-go prediction objectives. In the second stage,
Self-Improvement, steps-to-go prediction enables the extraction of a
well-shaped reward function and a robust success detector, enabling a fleet of
robots to autonomously practice downstream tasks with minimal human
supervision. Through extensive experiments on real-world and simulated robot
embodiments, our novel post-training recipe unveils significant results on
Embodied Foundation Models. First, we demonstrate that the combination of SFT
and Self-Improvement is significantly more sample-efficient than scaling
imitation data collection for supervised learning, and that it leads to
policies with significantly higher success rates. Further ablations highlight
that the combination of web-scale pretraining and Self-Improvement is the key
to this sample-efficiency. Next, we demonstrate that our proposed combination
uniquely unlocks a capability that current methods cannot achieve: autonomously
practicing and acquiring novel skills that generalize far beyond the behaviors
observed in the imitation learning datasets used during training. These
findings highlight the transformative potential of combining pretrained
foundation models with online Self-Improvement to enable autonomous skill
acquisition in robotics. Our project website can be found at
https://self-improving-efms.github.io .

</details>


### [121] [Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning](https://arxiv.org/abs/2509.15157)
*Shiwan Zhao,Xuyang Zhao,Jiaming Zhou,Aobo Kong,Qicheng Li,Yong Qin*

Main category: cs.LG

TL;DR: 提出数据重写框架解决大语言模型监督微调中离策略学习问题，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型监督微调中重要性采样因策略差距大导致高方差和训练不稳定，现有方法被动约束更新。

Method: 提出数据重写框架，保留正确解为在线策略数据，重写错误解，必要时使用专家演示。

Result: 在五个数学推理基准测试中，相比普通SFT和最先进的DFT方法有显著提升。

Conclusion: 数据重写框架能减少重要性采样方差，稳定离策略微调。

Abstract: Supervised fine-tuning (SFT) of large language models can be viewed as an
off-policy learning problem, where expert demonstrations come from a fixed
behavior policy while training aims to optimize a target policy. Importance
sampling is the standard tool for correcting this distribution mismatch, but
large policy gaps lead to high variance and training instability. Existing
approaches mitigate this issue using KL penalties or clipping, which passively
constrain updates rather than actively reducing the gap. We propose a simple
yet effective data rewriting framework that proactively shrinks the policy gap
by keeping correct solutions as on-policy data and rewriting incorrect ones
with guided re-solving, falling back to expert demonstrations only when needed.
This aligns the training distribution with the target policy before
optimization, reducing importance sampling variance and stabilizing off-policy
fine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate
consistent and significant gains over both vanilla SFT and the state-of-the-art
Dynamic Fine-Tuning (DFT) approach. The data and code will be released at
https://github.com/NKU-HLT/Off-Policy-SFT.

</details>


### [122] [MaRVIn: A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration](https://arxiv.org/abs/2509.15187)
*Giorgos Armeniakos,Alexis Maras,Sotirios Xydis,Dimitrios Soudris*

Main category: cs.LG

TL;DR: 提出MaRVIn框架优化RISC - V架构上混合精度执行，经实验验证可提升速度和能效。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入式微处理器缺乏对混合精度神经网络的架构支持，导致执行效率低。

Method: 提出新的ISA扩展和微架构实现，构建MaRVIn软硬件协同设计框架，包含硬件改进、量化、ISA级优化和仿真；硬件上增强ALU，采用多泵浦和软SIMD；软件上集成剪枝感知微调方法和基于贪心的DSE方法，还引入电压缩放。

Result: 在常用DNN和数据集上，框架平均实现17.6倍加速，精度损失小于1%，优于ISA无关的先进RISC - V内核，可达1.8 TOPs/W。

Conclusion: MaRVIn框架能有效提升RISC - V架构上深度学习推理的速度和能效。

Abstract: The evolution of quantization and mixed-precision techniques has unlocked new
possibilities for enhancing the speed and energy efficiency of NNs. Several
recent studies indicate that adapting precision levels across different
parameters can maintain accuracy comparable to full-precision models while
significantly reducing computational demands. However, existing embedded
microprocessors lack sufficient architectural support for efficiently executing
mixed-precision NNs, both in terms of ISA extensions and hardware design,
resulting in inefficiencies such as excessive data packing/unpacking and
underutilized arithmetic units. In this work, we propose novel ISA extensions
and a micro-architecture implementation specifically designed to optimize
mixed-precision execution, enabling energy-efficient deep learning inference on
RISC-V architectures. We introduce MaRVIn, a cross-layer hardware-software
co-design framework that enhances power efficiency and performance through a
combination of hardware improvements, mixed-precision quantization, ISA-level
optimizations, and cycle-accurate emulation. At the hardware level, we enhance
the ALU with configurable mixed-precision arithmetic (2, 4, 8 bits) for
weights/activations and employ multi-pumping to reduce execution latency while
implementing soft SIMD for efficient 2-bit ops. At the software level, we
integrate a pruning-aware fine-tuning method to optimize model compression and
a greedy-based DSE approach to efficiently search for Pareto-optimal
mixed-quantized models. Additionally, we incorporate voltage scaling to boost
the power efficiency of our system. Our experimental evaluation over widely
used DNNs and datasets, such as CIFAR10 and ImageNet, demonstrates that our
framework can achieve, on average, 17.6x speedup for less than 1% accuracy loss
and outperforms the ISA-agnostic state-of-the-art RISC-V cores, delivering up
to 1.8 TOPs/W.

</details>


### [123] [Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation](https://arxiv.org/abs/2509.15194)
*Yujun Zhou,Zhenwen Liang,Haolin Liu,Wenhao Yu,Kishan Panaganti,Linfeng Song,Dian Yu,Xiangliang Zhang,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 提出EVOL - RL方法，结合稳定性与变化性，防止熵坍塌，提升模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有无标签方法在无标签或外部评判下训练模型时会导致熵坍塌，现有TTRL方法适应范围窄，需一种能在不牺牲探索和泛化能力下实现模型通用改进的方法。

Method: 提出EVOL - RL，以多数投票答案为稳定锚点，添加新颖性感知奖励，用GRPO实现，使用非对称裁剪和熵正则化。

Result: 在无标签AIME24上训练，提升了Qwen3 - 4B - Base的AIME25 pass@1和pass@16指标，在GPQA等跨领域泛化能力增强，在RLVR设置中也提升了性能。

Conclusion: EVOL - RL能防止多样性坍塌，有更强跨领域泛化能力，适用性广泛。

Abstract: Large language models (LLMs) are increasingly trained with reinforcement
learning from verifiable rewards (RLVR), yet real-world deployment demands
models that can self-improve without labels or external judges. Existing
label-free methods, confidence minimization, self-consistency, or majority-vote
objectives, stabilize learning but steadily shrink exploration, causing an
entropy collapse: generations become shorter, less diverse, and brittle. Unlike
prior approaches such as Test-Time Reinforcement Learning (TTRL), which
primarily adapt models to the immediate unlabeled dataset at hand, our goal is
broader: to enable general improvements without sacrificing the model's
inherent exploration capacity and generalization ability, i.e., evolving. We
formalize this issue and propose EVolution-Oriented and Label-free
Reinforcement Learning (EVOL-RL), a simple rule that couples stability with
variation under a label-free setting. EVOL-RL keeps the majority-voted answer
as a stable anchor (selection) while adding a novelty-aware reward that favors
responses whose reasoning differs from what has already been produced
(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also
uses asymmetric clipping to preserve strong signals and an entropy regularizer
to sustain search. This majority-for-selection + novelty-for-variation design
prevents collapse, maintains longer and more informative chains of thought, and
improves both pass@1 and pass@n. EVOL-RL consistently outperforms the
majority-only TTRL baseline; e.g., training on label-free AIME24 lifts
Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%
to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks
stronger generalization across domains (e.g., GPQA). Furthermore, we
demonstrate that EVOL-RL also boosts performance in the RLVR setting,
highlighting its broad applicability.

</details>


### [124] [FlowRL: Matching Reward Distributions for LLM Reasoning](https://arxiv.org/abs/2509.15207)
*Xuekai Zhu,Daixuan Cheng,Dinghuai Zhang,Hengli Li,Kaiyan Zhang,Che Jiang,Youbang Sun,Ermo Hua,Yuxin Zuo,Xingtai Lv,Qizheng Zhang,Lin Chen,Fanghao Shao,Bo Xue,Yunchong Song,Zhenjie Yang,Ganqu Cui,Ning Ding,Jianfeng Gao,Xiaodong Liu,Bowen Zhou,Hongyuan Mei,Zhouhan Lin*

Main category: cs.LG

TL;DR: 提出FlowRL方法，通过流平衡匹配全奖励分布，在数学和代码推理任务实验中表现优于GRPO和PPO。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型强化学习中奖励最大化方法易过度优化主导奖励信号，忽略有效推理路径，降低多样性。

Method: 用可学习的配分函数将标量奖励转换为归一化目标分布，最小化策略与目标分布的反向KL散度，实现流平衡优化方法。

Result: FlowRL在数学基准测试中比GRPO平均提升10.0%，比PPO提升5.1%，在代码推理任务中表现也更好。

Conclusion: 奖励分布匹配是大语言模型强化学习中实现高效探索和多样化推理的关键步骤。

Abstract: We propose FlowRL: matching the full reward distribution via flow balancing
instead of maximizing rewards in large language model (LLM) reinforcement
learning (RL). Recent advanced reasoning models adopt reward-maximizing methods
(\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while
neglecting less frequent but valid reasoning paths, thus reducing diversity. In
contrast, we transform scalar rewards into a normalized target distribution
using a learnable partition function, and then minimize the reverse KL
divergence between the policy and the target distribution. We implement this
idea as a flow-balanced optimization method that promotes diverse exploration
and generalizable reasoning trajectories. We conduct experiments on math and
code reasoning tasks: FlowRL achieves a significant average improvement of
$10.0\%$ over GRPO and $5.1\%$ over PPO on math benchmarks, and performs
consistently better on code reasoning tasks. These results highlight reward
distribution-matching as a key step toward efficient exploration and diverse
reasoning in LLM reinforcement learning.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [125] [Evolution of Kernels: Automated RISC-V Kernel Optimization with Large Language Models](https://arxiv.org/abs/2509.14265)
*Siyuan Chen,Zhichao Lu,Qingfu Zhang*

Main category: cs.SE

TL;DR: 提出基于LLM的进化程序搜索框架EoK，用于参考资料稀缺的RISC - V等领域自动化内核设计，实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在自动化内核优化方面在参考资料丰富的CUDA领域有效，但在如RISC - V这类参考资料稀缺的领域效果未知，需新方法解决参考资料稀缺问题以实现自动化内核设计。

Method: EoK从成熟内核库开发历史中挖掘和形式化可复用的优化思路，利用这些思路结合RAG引导并行LLM探索，优先使用历史上有效的技术。

Result: EoK实现了中位数1.27倍的加速，在80个评估的内核设计任务中超过人类专家，比之前基于LLM的自动化内核设计方法提高了20%。

Conclusion: 将人类经验融入新兴领域是可行的，基于LLM的自动化内核优化潜力巨大。

Abstract: Automated kernel design is critical for overcoming software ecosystem
barriers in emerging hardware platforms like RISC-V. While large language
models (LLMs) have shown promise for automated kernel optimization,
demonstrating success in CUDA domains with comprehensive technical documents
and mature codebases, their effectiveness remains unproven for reference-scarce
domains like RISC-V. We present Evolution of Kernels (EoK), a novel LLM-based
evolutionary program search framework that automates kernel design for domains
with limited reference material. EoK mitigates reference scarcity by mining and
formalizing reusable optimization ideas (general design principles + actionable
thoughts) from established kernel libraries' development histories; it then
guides parallel LLM explorations using these ideas, enriched via
Retrieval-Augmented Generation (RAG) with RISC-V-specific context, prioritizing
historically effective techniques. Empirically, EoK achieves a median 1.27x
speedup, surpassing human experts on all 80 evaluated kernel design tasks and
improving upon prior LLM-based automated kernel design methods by 20%. These
results underscore the viability of incorporating human experience into
emerging domains and highlight the immense potential of LLM-based automated
kernel optimization.

</details>


### [126] [Automated and Context-Aware Code Documentation Leveraging Advanced LLMs](https://arxiv.org/abs/2509.14273)
*Swapnil Sharma Sarker,Tanzina Taher Ifty*

Main category: cs.SE

TL;DR: 本文针对自动生成Javadoc文档存在的问题，开发定制数据集并评估开源大模型，发现LLaMA 3.1表现良好，是实用的自动生成方案。


<details>
  <summary>Details</summary>
Motivation: 手动代码文档繁琐，现有自动方法多关注代码总结，缺乏基于模板的Javadoc生成研究，且缺少合适的数据集。

Method: 开发用于Javadoc生成的上下文感知数据集，用零样本、少样本和微调设置评估五个开源大模型。

Result: LLaMA 3.1在各项评估中表现稳定。

Conclusion: LLaMA 3.1是实用的自动Javadoc生成方案，可替代专有系统。

Abstract: Code documentation is essential to improve software maintainability and
comprehension. The tedious nature of manual code documentation has led to much
research on automated documentation generation. Existing automated approaches
primarily focused on code summarization, leaving a gap in template-based
documentation generation (e.g., Javadoc), particularly with publicly available
Large Language Models (LLMs). Furthermore, progress in this area has been
hindered by the lack of a Javadoc-specific dataset that incorporates modern
language features, provides broad framework/library coverage, and includes
necessary contextual information. This study aims to address these gaps by
developing a tailored dataset and assessing the capabilities of publicly
available LLMs for context-aware, template-based Javadoc generation. In this
work, we present a novel, context-aware dataset for Javadoc generation that
includes critical structural and semantic information from modern Java
codebases. We evaluate five open-source LLMs (including LLaMA-3.1, Gemma-2,
Phi-3, Mistral, Qwen-2.5) using zero-shot, few-shot, and fine-tuned setups and
provide a comparative analysis of their performance. Our results demonstrate
that LLaMA 3.1 performs consistently well and is a reliable candidate for
practical, automated Javadoc generation, offering a viable alternative to
proprietary systems.

</details>


### [127] [Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization](https://arxiv.org/abs/2509.14279)
*Robert Tjarko Lange,Qi Sun,Aaditya Prasad,Maxence Faldor,Yujin Tang,David Ha*

Main category: cs.SE

TL;DR: 提出新基准robust - kbench和自动化框架，将torch代码转CUDA内核并优化，在基准上表现优于torch实现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在软件工程任务中对低级别CUDA内核实现优化关注不足，现有内核生成基准存在漏洞和测试条件多样性不足问题。

Method: 引入robust - kbench基准，提出全面的代理框架，先将PyTorch代码转CUDA内核，再用进化元生成程序优化运行时间，由基于大语言模型的验证器指导。

Result: 在robust - kbench上生成的CUDA内核在实际应用中优于torch实现，能融合操作并部署多种运行时优化策略，验证器工作流可准确分类错误内核。

Conclusion: 所提方法可有效将torch代码转化为CUDA内核并优化，提高硬件验证效率。

Abstract: Recent advances in large language models (LLMs) demonstrate their
effectiveness in scaling test-time compute for software engineering tasks.
However, these approaches often focus on high-level solutions, with limited
attention to optimizing low-level CUDA kernel implementations. Additionally,
existing kernel generation benchmarks suffer from exploitable loopholes and
insufficient diversity in testing conditions, hindering true generalization
assessment. To address these limitations, we introduce robust-kbench, a new
benchmark for rigorous evaluation of kernel performance and correctness across
varied scenarios. Furthermore, we present a comprehensive agentic framework
that automates CUDA kernel discovery, verification, and optimization. This
pipeline enables frontier LLMs to translate torch code to CUDA kernels and
iteratively improve their runtime within our robust evaluation setting. Our
sequential workflow first translates PyTorch code into equivalent CUDA kernels.
It then optimizes their runtime using a novel evolutionary meta-generation
procedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for
correctness and efficient filtering. Evaluated on robust-kbench, our approach
produces CUDA kernels outperforming torch implementations for practical
applications, including forward and backward passes. It can fuse operations and
deploy various runtime optimization strategies. The verifier workflow
accurately classifies incorrect kernels, enhancing hardware verification
efficiency.

</details>


### [128] [SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems](https://arxiv.org/abs/2509.14281)
*Xifeng Yao,Dongyu Lang,Wu Zhang,Xintong Guo,Huarui Xie,Yinhao Ni,Ping Liu,Guang Shen,Yi Bai,Dandan Tu,Changzheng Zhang*

Main category: cs.SE

TL;DR: 论文提出合成代码问题的新框架，实验显示其性能优于现有开源大模型。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型发展受现实编码问题稀缺限制，需合成模拟真实场景的代码问题。

Method: 从真实编程数据集提取领域知识、技能和编码技能，挖掘应用场景构建场景图，设计图上采样策略控制代码问题生成。

Result: 在多种现实基准测试中，该方法性能始终优于不同规模和功能的开源大语言模型。

Conclusion: 所提出的合成代码问题框架有效，能反映现实挑战，性能表现良好。

Abstract: Significant advancements have been made in the capabilities of code large
language models, leading to their rapid adoption and application across a wide
range of domains. However, their further advancements are often constrained by
the scarcity of real-world coding problems. To bridge this gap, we propose a
novel framework for synthesizing code problems that emulate authentic
real-world scenarios. This framework systematically integrates domain
knowledge, domain skills, and coding skills, all of which are meticulously
extracted from real-world programming-related datasets, including Stack
Overflow and Kaggle. The extracted elements serve as the foundational building
blocks for constructing code problems. To align the generated problems with
practical applications, application scenarios are also mined from the
aforementioned datasets. These scenarios are then utilized to construct a
scenario-centric graph that interconnects domain knowledge, domain skills, and
coding skills. Based on this structured representation, a sampling strategy on
the graph is designed, which effectively controls the generation of a code
problem with complexity and diversity, reflects real-world challenges.
Experimental results demonstrate that the proposed method consistently achieves
superior performance over state-of-the-art open-source large language models of
varying sizes and functionalities, including both coders and general-purpose
models, across a diverse set of real-world benchmarks.

</details>


### [129] [Monitoring Machine Learning Systems: A Multivocal Literature Review](https://arxiv.org/abs/2509.14294)
*Hira Naveed,Scott Barnett,Chetan Arora,John Grundy,Hourieh Khalajzadeh,Omar Haggag*

Main category: cs.SE

TL;DR: 本文对机器学习监控文献进行多视角文献综述，分析相关研究关键领域，识别并总结实践与差距，对学界和业界有价值。


<details>
  <summary>Details</summary>
Motivation: 动态生产环境使维护可靠机器学习系统具挑战性，运行时问题会降低模型性能，监控可提前检测和缓解问题，本文旨在全面概述机器学习监控文献。

Method: 按照Garousi既定指南进行多视角文献综述，调查136篇论文中机器学习监控方法的各方面。

Result: 基于四个关键领域分析所选研究，还讨论研究中的见解、影响及对未来研究和实践的建议。

Conclusion: 多视角文献综述识别并总结机器学习监控实践与差距，强调正式和灰色文献异同，对学界和业界有帮助，可助选择合适方案、突出当前方法局限并指明未来研究和工具开发方向。

Abstract: Context: Dynamic production environments make it challenging to maintain
reliable machine learning (ML) systems. Runtime issues, such as changes in data
patterns or operating contexts, that degrade model performance are a common
occurrence in production settings. Monitoring enables early detection and
mitigation of these runtime issues, helping maintain users' trust and prevent
unwanted consequences for organizations. Aim: This study aims to provide a
comprehensive overview of the ML monitoring literature. Method: We conducted a
multivocal literature review (MLR) following the well established guidelines by
Garousi to investigate various aspects of ML monitoring approaches in 136
papers. Results: We analyzed selected studies based on four key areas: (1) the
motivations, goals, and context; (2) the monitored aspects, specific
techniques, metrics, and tools; (3) the contributions and benefits; and (4) the
current limitations. We also discuss several insights found in the studies,
their implications, and recommendations for future research and practice.
Conclusion: Our MLR identifies and summarizes ML monitoring practices and gaps,
emphasizing similarities and disconnects between formal and gray literature.
Our study is valuable for both academics and practitioners, as it helps select
appropriate solutions, highlights limitations in current approaches, and
provides future directions for research and tool development.

</details>


### [130] [On the Illusion of Success: An Empirical Study of Build Reruns and Silent Failures in Industrial CI](https://arxiv.org/abs/2509.14347)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 本文通过重跑成功任务对CI中的静默失败进行实证研究，分析影响因素和失败类别，为提升CI可靠性提供见解和解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究未关注CI中构建任务标记成功但未完成全部或部分任务的静默失败问题，其会造成严重后果，因此开展研究。

Method: 分析142,387个跨81个工业项目的任务，用混合效应模型分析32个自变量，分析92个公共问题。

Result: 11%的成功任务被重跑，35%的重跑在24小时后发生；确定与重跑相关的关键因素；发现11类静默失败。

Conclusion: 研究结果有助于团队了解静默失败的情况和原因，提高对其的认识，并提出改善CI可靠性的解决方案。

Abstract: Reliability of build outcomes is a cornerstone of effective Continuous
Integration (CI). Yet in practice, developers often struggle with
non-deterministic issues in the code or CI infrastructure, which undermine
trust in build results. When faced with such unexpected outcomes, developers
often repeatedly rerun jobs hoping for true success, but this practice is known
to increase CI costs and reduce productivity. While recent studies have focused
on intermittent job failures, no prior work has investigated silent failures,
where build jobs are marked as successful but fail to complete all or part of
their tasks. Such silent failures often go unnoticed, creating an illusion of
success with detrimental consequences such as bugs escaping into production.
This paper presents the first empirical study of silent failures through the
practice of rerunning successful jobs. An analysis of 142,387 jobs across 81
industrial projects shows that 11% of successful jobs are rerun, with 35% of
these reruns occurring after more than 24 hours. Using mixed-effects models on
32 independent variables (AUC of 85%), we identified key factors associated
with reruns of successful jobs, notably testing and static analysis tasks,
scripting languages like Shell, and developers prior rerun tendencies. A
further analysis of 92 public issues revealed 11 categories of silent failures
aligning with these factors, the most frequent being artifact operation errors,
caching errors, and ignored exit codes. Overall, our findings provide valuable
insights into the circumstances and causes of silent failures to raise
awareness among teams, and present solutions to improve CI reliability.

</details>


### [131] [CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning](https://arxiv.org/abs/2509.14373)
*Huy Le,Phong Nguyen,Hao Do,Tuan Nguyen,Thien Pham,Anh Nguyen-Duc,Tho Quan*

Main category: cs.SE

TL;DR: 本文介绍CodeLSI框架，结合低秩优化和特定领域指令调优，在真实JavaScript编码任务中表现出色，提升了自动化代码生成的实用性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动化代码生成中领域特异性、成本效益和安全问题，尤其是依赖第三方API时的挑战。

Method: 应用低秩适应技术降低模型预训练和微调的计算成本，采用特定领域指令调优使代码生成符合组织需求，并在真实JavaScript编码任务中测试。

Result: 实验表明CodeLSI生成高质量、上下文感知的代码，在相关性、准确性和领域适配性上优于基线模型，低秩优化显著降低资源需求。

Conclusion: 结合低秩优化和特定领域调优可提升基础模型在自动化代码生成中的实用性和性能，提供安全、经济的替代方案。

Abstract: Context: Automated code generation using Foundation Models (FMs) offers
promising solutions for enhancing software development efficiency. However,
challenges remain in ensuring domain specificity, cost-effectiveness, and
security - especially when relying on third-party APIs. This paper introduces
CodeLSI, a framework that combines low-rank optimization and domain-specific
instruction tuning to address these challenges.
  Objectives: The aim of this study is to develop and evaluate CodeLSI, a novel
approach for generating high-quality code tailored to specific domains, using
FMs fine-tuned on company infrastructure without dependence on external APIs.
  Methods: CodeLSI applies low-rank adaptation techniques to reduce the
computational cost of model pre-training and fine-tuning. Domain-specific
instruction tuning is employed to align code generation with organizational
needs. We implemented and tested the framework on real-world JavaScript coding
tasks using datasets drawn from internal software projects.
  Results: Experimental evaluations show that CodeLSI produces high-quality,
context aware code. It outperforms baseline models in terms of relevance,
accuracy, and domain fit. The use of low-rank optimization significantly
reduced resource requirements, enabling scalable training on company-owned
infrastructure.
  Conclusion: CodeLSI demonstrates that combining low-rank optimization with
domain specific tuning can enhance the practicality and performance of FMs for
automated code generation. This approach provides a secure, cost-efficient
alternative to commercial API based solutions and supports faster, more
targeted innovation in software development.

</details>


### [132] [A Taxonomy of Prompt Defects in LLM Systems](https://arxiv.org/abs/2509.14404)
*Haoye Tian,Chong Wang,BoYang Yang,Lyuye Zhang,Yang Liu*

Main category: cs.SE

TL;DR: 本文首次对提示缺陷进行系统调查和分类，分析其在开发流程中的表现、下游影响并给出缓解策略，最后提出研究挑战。


<details>
  <summary>Details</summary>
Motivation: 提示设计经验性强，小错误会导致大问题，需系统研究提示缺陷。

Method: 从六个维度对提示缺陷进行分类，细化子类型，结合实例和根本原因分析，利用软件工程原理研究其在开发流程中的情况。

Result: 针对每个子类型提炼出缓解策略，总结成关联缺陷、影响和补救措施的主分类法。

Conclusion: 指出开放的研究挑战，呼吁采用严谨的工程导向方法确保大语言模型驱动系统的可靠性。

Abstract: Large Language Models (LLMs) have become key components of modern software,
with prompts acting as their de-facto programming interface. However, prompt
design remains largely empirical and small mistakes can cascade into
unreliable, insecure, or inefficient behavior. This paper presents the first
systematic survey and taxonomy of prompt defects, recurring ways that prompts
fail to elicit their intended behavior from LLMs. We organize defects along six
dimensions: (1) Specification and Intent, (2) Input and Content, (3) Structure
and Formatting, (4) Context and Memory, (5) Performance and Efficiency, and (6)
Maintainability and Engineering. Each dimension is refined into fine-grained
subtypes, illustrated with concrete examples and root cause analysis. Grounded
in software engineering principles, we show how these defects surface in real
development workflows and examine their downstream effects. For every subtype,
we distill mitigation strategies that span emerging prompt engineering
patterns, automated guardrails, testing harnesses, and evaluation frameworks.
We then summarize these strategies in a master taxonomy that links defect,
impact, and remedy. We conclude with open research challenges and a call for
rigorous engineering-oriented methodologies to ensure that LLM-driven systems
are dependable by design.

</details>


### [133] [An LLM-based multi-agent framework for agile effort estimation](https://arxiv.org/abs/2509.14483)
*Thanh-Long Bui,Hoa Khanh Dam,Rashina Hoda*

Main category: cs.SE

TL;DR: 本文提出基于大语言模型的多智能体框架用于敏捷估算，评估显示其优于现有技术，且开发者协作体验积极。


<details>
  <summary>Details</summary>
Motivation: 当前敏捷工作量估算依赖主观评估有不准确和不一致问题，机器学习方法无法解释且缺乏与人交互能力。

Method: 提出基于大语言模型的多智能体框架，可与人类开发者和其他智能体协调、沟通和讨论以达成共识。

Result: 在真实数据集评估中多数情况下优于现有技术，开发者协作体验积极。

Conclusion: 基于大语言模型的多智能体框架在敏捷工作量估算中有效且实用。

Abstract: Effort estimation is a crucial activity in agile software development, where
teams collaboratively review, discuss, and estimate the effort required to
complete user stories in a product backlog. Current practices in agile effort
estimation heavily rely on subjective assessments, leading to inaccuracies and
inconsistencies in the estimates. While recent machine learning-based methods
show promising accuracy, they cannot explain or justify their estimates and
lack the capability to interact with human team members. Our paper fills this
significant gap by leveraging the powerful capabilities of Large Language
Models (LLMs). We propose a novel LLM-based multi-agent framework for agile
estimation that not only can produce estimates, but also can coordinate,
communicate and discuss with human developers and other agents to reach a
consensus. Evaluation results on a real-life dataset show that our approach
outperforms state-of-the-art techniques across all evaluation metrics in the
majority of the cases. Our human study with software development practitioners
also demonstrates an overwhelmingly positive experience in collaborating with
our agents in agile effort estimation.

</details>


### [134] [Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language](https://arxiv.org/abs/2509.14623)
*Hanlong Wan,Xing Lu,Yan Chen,Karthik Devaprasad,Laura Hinkle*

Main category: cs.SE

TL;DR: 本文以建筑Modelica库中控制描述语言模块生成为例，研究大语言模型辅助生成，开发结构化工作流，实验显示不同模型表现有差异，虽有局限但节省大量开发时间。


<details>
  <summary>Details</summary>
Motivation: Modelica开发控制模块劳动密集且需专业知识，因此研究用大语言模型自动化生成控制描述语言模块。

Method: 开发结合标准提示框架、库感知接地、OpenModelica自动编译和人工评估的结构化工作流，在基础逻辑任务和控制模块上进行实验。

Result: GPT 4o零样本模式无法生成可执行代码，Claude Sonnet 4基础逻辑块表现好，控制模块成功率83%，检索增强生成有模块选择不匹配问题，人工评估优于AI评估，平均开发时间节省40 - 60%。

Conclusion: 指出大语言模型辅助Modelica生成有潜力也有局限，为预仿真验证、更强接地和闭环评估等未来研究指明方向。

Abstract: Dynamic energy systems and controls require advanced modeling frameworks to
design and test supervisory and fault tolerant strategies. Modelica is a widely
used equation based language, but developing control modules is labor intensive
and requires specialized expertise. This paper examines the use of large
language models (LLMs) to automate the generation of Control Description
Language modules in the Building Modelica Library as a case study. We developed
a structured workflow that combines standardized prompt scaffolds, library
aware grounding, automated compilation with OpenModelica, and human in the loop
evaluation. Experiments were carried out on four basic logic tasks (And, Or,
Not, and Switch) and five control modules (chiller enable/disable, bypass valve
control, cooling tower fan speed, plant requests, and relief damper control).
The results showed that GPT 4o failed to produce executable Modelica code in
zero shot mode, while Claude Sonnet 4 achieved up to full success for basic
logic blocks with carefully engineered prompts. For control modules, success
rates reached 83 percent, and failed outputs required medium level human repair
(estimated one to eight hours). Retrieval augmented generation often produced
mismatches in module selection (for example, And retrieved as Or), while a
deterministic hard rule search strategy avoided these errors. Human evaluation
also outperformed AI evaluation, since current LLMs cannot assess simulation
results or validate behavioral correctness. Despite these limitations, the LLM
assisted workflow reduced the average development time from 10 to 20 hours down
to 4 to 6 hours per module, corresponding to 40 to 60 percent time savings.
These results highlight both the potential and current limitations of LLM
assisted Modelica generation, and point to future research in pre simulation
validation, stronger grounding, and closed loop evaluation.

</details>


### [135] [Evaluating the Effectiveness of Coverage-Guided Fuzzing for Testing Deep Learning Library APIs](https://arxiv.org/abs/2509.14626)
*Feiran Qin,M. M. Abid Naziri,Hengyu Ai,Saikat Dutta,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: 文章提出FlashFuzz技术，利用LLM为DL库API合成测试框架，相比现有方法在覆盖率、有效性和速度上有显著提升，还发现了新的bug。


<details>
  <summary>Details</summary>
Motivation: 现有DL库漏洞查找方法未使用覆盖引导，效率和效果有限，研究覆盖引导模糊测试（CGF）能否有效应用于DL库。

Method: 提出FlashFuzz技术，利用LLM结合模板、辅助函数和API文档自动合成API级测试框架，采用反馈驱动策略迭代合成和修复框架。

Result: 为1151个PyTorch和662个TensorFlow API合成了测试框架，相比现有方法覆盖率最高提升101.13 - 212.88%，有效性提高1.0 - 5.4倍，输入生成速度提升1 - 1182倍，发现42个未知bug，8个已修复。

Conclusion: CGF能有效应用于DL库，为未来测试方法提供了强大的基线。

Abstract: Deep Learning (DL) libraries such as PyTorch provide the core components to
build major AI-enabled applications. Finding bugs in these libraries is
important and challenging. Prior approaches have tackled this by performing
either API-level fuzzing or model-level fuzzing, but they do not use coverage
guidance, which limits their effectiveness and efficiency. This raises an
intriguing question: can coverage guided fuzzing (CGF), in particular
frameworks like LibFuzzer, be effectively applied to DL libraries, and does it
offer meaningful improvements in code coverage, bug detection, and scalability
compared to prior methods?
  We present the first in-depth study to answer this question. A key challenge
in applying CGF to DL libraries is the need to create a test harness for each
API that can transform byte-level fuzzer inputs into valid API inputs. To
address this, we propose FlashFuzz, a technique that leverages Large Language
Models (LLMs) to automatically synthesize API-level harnesses by combining
templates, helper functions, and API documentation. FlashFuzz uses a feedback
driven strategy to iteratively synthesize and repair harnesses. With this
approach, FlashFuzz synthesizes harnesses for 1,151 PyTorch and 662 TensorFlow
APIs. Compared to state-of-the-art fuzzing methods (ACETest, PathFinder, and
TitanFuzz), FlashFuzz achieves up to 101.13 to 212.88 percent higher coverage
and 1.0x to 5.4x higher validity rate, while also delivering 1x to 1182x
speedups in input generation. FlashFuzz has discovered 42 previously unknown
bugs in PyTorch and TensorFlow, 8 of which are already fixed. Our study
confirms that CGF can be effectively applied to DL libraries and provides a
strong baseline for future testing approaches.

</details>


### [136] [SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation](https://arxiv.org/abs/2509.14646)
*Yongpan Wang,Xin Xu,Xiaojie Zhu,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Decompilation is widely used in reverse engineering to recover high-level
language code from binary executables. While recent approaches leveraging Large
Language Models (LLMs) have shown promising progress, they typically treat
assembly code as a linear sequence of instructions, overlooking arbitrary jump
patterns and isolated data segments inherent to binary files. This limitation
significantly hinders their ability to correctly infer source code semantics
from assembly code. To address this limitation, we propose \saltm, a novel
binary decompilation method that abstracts stable logical features shared
between binary and source code. The core idea of \saltm is to abstract selected
binary-level operations, such as specific jumps, into a high-level logic
framework that better guides LLMs in semantic recovery. Given a binary
function, \saltm constructs a Source-level Abstract Logic Tree (\salt) from
assembly code to approximate the logic structure of high-level language. It
then fine-tunes an LLM using the reconstructed \salt to generate decompiled
code. Finally, the output is refined through error correction and symbol
recovery to improve readability and correctness. We compare \saltm to three
categories of baselines (general-purpose LLMs, commercial decompilers, and
decompilation methods) using three well-known datasets (Decompile-Eval, MBPP,
Exebench). Our experimental results demonstrate that \saltm is highly effective
in recovering the logic of the source code, significantly outperforming
state-of-the-art methods (e.g., 70.4\% TCP rate on Decompile-Eval with a 10.6\%
improvement). The results further validate its robustness against four commonly
used obfuscation techniques. Additionally, analyses of real-world software and
a user study confirm that our decompiled output offers superior assistance to
human analysts in comprehending binary functions.

</details>


### [137] [Wireless Communication Performance Testing: From Laboratory Environment to Research Vessel](https://arxiv.org/abs/2509.14740)
*Andrei-Raoul Morariu,Andreas Strandberg,Bogdan Iancu,Jerker Bjorkqvist*

Main category: cs.SE

TL;DR: 研究共享频谱内信号传输，在实验室和户外测量，探究障碍物、距离等对信号传输影响，助于理解环境因素对无线通信的作用。


<details>
  <summary>Details</summary>
Motivation: 了解环境因素对动态和有遮挡环境中无线通信的影响。

Method: 在实验室和户外（电动研究船）进行测量，研究实验室障碍物对信号的衰减以及距离和位置对信号传输效率的影响。

Result: 未提及具体结果

Conclusion: 未提及具体结论，但研究有助于理解环境因素对无线通信的影响。

Abstract: This study investigates signal transmission within a shared spectrum,
focusing on measurements conducted both in laboratory and outdoor environments.
The objective was to demonstrate how laboratory objects obstructing the line of
sight can attenuate the signal between a transmitter (Tx) and a receiver (Rx).
Additionally, we examined the impact of distance and placement in various
locations aboard an electric research boat on signal transmission efficiency.
These findings contribute to understanding whether the environmental factors
influence wireless communication in dynamic and obstructed environments.

</details>


### [138] [On the Use of Agentic Coding Manifests: An Empirical Study of Claude Code](https://arxiv.org/abs/2509.14744)
*Worawalan Chatlatanagulchai,Kundjanasith Thonglek,Brittany Reid,Yutaro Kashiwa,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Hajimu Iida*

Main category: cs.SE

TL;DR: 分析253个Claude.md文件以识别代理清单的结构模式和常见内容，发现清单层次浅，内容以操作命令等为主。


<details>
  <summary>Details</summary>
Motivation: 代理编码工具的代理清单缺乏全面且易获取的创建文档，给开发者带来挑战。

Method: 分析来自242个仓库的253个Claude.md文件。

Result: 清单通常具有浅层次结构，一个主标题和几个子部分，内容以操作命令、技术实现说明和高层架构为主。

Conclusion: 未明确提及，但为代理清单的创建文档提供了结构和内容方面的参考。

Abstract: Agentic coding tools receive goals written in natural language as input,
break them down into specific tasks, and write/execute the actual code with
minimal human intervention. Key to this process are agent manifests,
configuration files (such as Claude.md) that provide agents with essential
project context, identity, and operational rules. However, the lack of
comprehensive and accessible documentation for creating these manifests
presents a significant challenge for developers. We analyzed 253 Claude.md
files from 242 repositories to identify structural patterns and common content.
Our findings show that manifests typically have shallow hierarchies with one
main heading and several subsections, with content dominated by operational
commands, technical implementation notes, and high-level architecture.

</details>


### [139] [On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub](https://arxiv.org/abs/2509.14745)
*Miku Watanabe,Hao Li,Yutaro Kashiwa,Brittany Reid,Hajimu Iida,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 研究Claude Code生成的GitHub拉取请求，发现83.8%被接受合并，多数需或不需修改，说明需人类监督完善。


<details>
  <summary>Details</summary>
Motivation: 了解自主AI代理生成的拉取请求在实际项目中的实用性和被接受程度。

Method: 对157个开源项目中使用Claude Code生成的567个GitHub拉取请求进行实证研究。

Result: 83.8%的代理辅助拉取请求最终被接受合并，54.9%无需修改，45.1%需人类修订。

Conclusion: 代理辅助拉取请求大体可接受，但仍需人类监督和完善。

Abstract: Large language models (LLMs) are increasingly being integrated into software
development processes. The ability to generate code and submit pull requests
with minimal human intervention, through the use of autonomous AI agents, is
poised to become a standard practice. However, little is known about the
practical usefulness of these pull requests and the extent to which their
contributions are accepted in real-world projects. In this paper, we
empirically study 567 GitHub pull requests (PRs) generated using Claude Code,
an agentic coding tool, across 157 diverse open-source projects. Our analysis
reveals that developers tend to rely on agents for tasks such as refactoring,
documentation, and testing. The results indicate that 83.8% of these
agent-assisted PRs are eventually accepted and merged by project maintainers,
with 54.9% of the merged PRs are integrated without further modification. The
remaining 45.1% require additional changes benefit from human revisions,
especially for bug fixes, documentation, and adherence to project-specific
standards. These findings suggest that while agent-assisted PRs are largely
acceptable, they still benefit from human oversight and refinement.

</details>


### [140] [RulER: Automated Rule-Based Semantic Error Localization and Repair for Code Translation](https://arxiv.org/abs/2509.14829)
*Shuo Jin,Songqiang Chen,Xiaoyuan Xie,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 提出基于规则的代码翻译调试方法RulER，能从大语言模型生成的正确翻译中推导规则，在多语言翻译评估中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有代码翻译自动调试方法缺乏可靠参考构建代码对齐和设计修复补丁模板，影响定位准确性和修复效果。

Method: 提出基于规则的调试方法RulER，从大语言模型生成的正确翻译中自动推导代码翻译规则，动态组合规则以更好地对齐语句。

Result: 在Java到C++和Python到C++的翻译中，RulER在错误定位率和修复成功率上分别比最佳基线高20%和272%，且优于直接让大语言模型生成补丁。

Conclusion: RulER是一种有前景的从大语言模型中提取和利用编码知识的方法。

Abstract: Automated code translation aims to convert programs between different
programming languages while maintaining their functionality. Due to the
imperfections of code translation models, the generated translations may
contain errors that compromise their reliability. Existing automated debugging
methods for code translation rely on code alignments and repair patch templates
to locate and fix erroneous translations. However, existing methods lack
reliable references to construct code alignments and design repair patch
templates, which significantly impacts their localization accuracy and repair
effectiveness. To address these limitations, we reintroduce code translation
rules and propose a rule-based debugging method for code translation, called
RulER. RulER automatically derives code translation rules from correct
translations generated by LLMs, enabling the efficient collection of diverse
translation rules. In addition, RulER dynamically combines the existing rules
on expandable nodes like expressions and tokens to further adaptively align
more statements. These rules capture clear and detailed structural
correspondences between source and target programming languages. Therefore,
they can serve as reliable and reusable references for code alignment and
repair template design, enabling RulER to locate and fix translation errors
effectively. Our evaluation of RulER on Java-to-C++ and Python-to-C++
translations produced by four code translation models demonstrates that RulER
outperforms state-of-the-art methods, BatFix and TransMap. Our experimental
results show that RulER outperformed the best baseline by 20% and 272% in terms
of error localization rates and repair success rates, respectively. RulER
exhibits superior repair performance compared to directly prompting LLMs for
patch generation, demonstrating a promising methodology for extracting and
leveraging coding knowledge from LLMs.

</details>


### [141] [CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects](https://arxiv.org/abs/2509.14856)
*Hanyang Guo,Xunjin Zheng,Zihan Liao,Hang Yu,Peng DI,Ziyin Zhang,Hong-Ning Dai*

Main category: cs.SE

TL;DR: 现有自动化代码审查基准有‘现实差距’，本文引入CodeFuse - CR - Bench基准和评估框架，评估大模型代码审查表现，得出相关结论。


<details>
  <summary>Details</summary>
Motivation: 现有基准使用简化、上下文少的数据评估模型孤立子任务，无法反映现实代码审查的整体上下文丰富性，需弥合差距。

Method: 引入CodeFuse - CR - Bench基准用于仓库级代码审查评估，提出结合基于规则检查和基于模型判断的评估框架，对大模型进行评估。

Result: 建立关键基线，发现无单一模型在各方面占优，Gemini 2.5 Pro综合性能最高，不同模型对冗余上下文鲁棒性不同。

Conclusion: 强调整体、多维评估的必要性，为推进实用代码审查助手提供可行见解。

Abstract: Automated code review (CR) is a key application for Large Language Models
(LLMs), but progress is hampered by a "reality gap": existing benchmarks
evaluate models on isolated sub-tasks using simplified, context-poor data. This
fails to reflect the holistic context-rich nature of real-world CR. To bridge
this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware
benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601
high-quality instances from 70 Python projects covering nine Pull-Request (PR)
problem domains, where each instance provides rich, multi-faceted context
including the associated issue, PR details, and repository state, enabling
end-to-end evaluation. Beyond superficial metrics, we also propose a novel
evaluation framework that combines rule-based checks for location and syntax
with model-based judgments of review quality. We present the first large-scale
assessment of state-of-the-art LLMs on this comprehensive CR task. Our results
establish crucial baselines and reveal that (1) no single LLM dominates all
aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive
performance; and (3) different LLMs exhibit varying robustness to redundant
context. These findings highlight the necessity of holistic, multi-dimensional
evaluation and provide actionable insights for advancing truly intelligent yet
practical CR assistants.

</details>


### [142] [CARGO: A Framework for Confidence-Aware Routing of Large Language Models](https://arxiv.org/abs/2509.14899)
*Amine Barrak,Yosr Fourati,Michael Olchawa,Emna Ksontini,Khalil Zoghlami*

Main category: cs.SE

TL;DR: 介绍了轻量级、基于置信度的动态大语言模型选择框架CARGO，评估显示其有较好性能，为多模型部署提供实用方案。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，将用户提示路由到最合适模型以平衡性能和成本的挑战愈发关键。

Method: 采用基于单一嵌入的回归器预测模型性能，不确定时调用二分类器，支持跨五个任务组训练特定类别的回归器。

Result: 在四个竞争大语言模型上评估，CARGO的top - 1路由准确率达76.4%，胜率在72%到89%之间。

Conclusion: 基于置信度引导的轻量级路由能以最小开销实现专家级性能，为现实世界多模型大语言模型部署提供实用解决方案。

Abstract: As large language models (LLMs) proliferate in scale, specialization, and
latency profiles, the challenge of routing user prompts to the most appropriate
model has become increasingly critical for balancing performance and cost. We
introduce CARGO (Category-Aware Routing with Gap-based Optimization), a
lightweight, confidence-aware framework for dynamic LLM selection. CARGO
employs a single embedding-based regressor trained on LLM-judged pairwise
comparisons to predict model performance, with an optional binary classifier
invoked when predictions are uncertain. This two-stage design enables precise,
cost-aware routing without the need for human-annotated supervision. To capture
domain-specific behavior, CARGO also supports category-specific regressors
trained across five task groups: mathematics, coding, reasoning, summarization,
and creative writing. Evaluated on four competitive LLMs (GPT-4o, Claude 3.5
Sonnet, DeepSeek V3, and Perplexity Sonar), CARGO achieves a top-1 routing
accuracy of 76.4% and win rates ranging from 72% to 89% against individual
experts. These results demonstrate that confidence-guided, lightweight routing
can achieve expert-level performance with minimal overhead, offering a
practical solution for real-world, multi-model LLM deployments.

</details>


### [143] ["Let it be Chaos in the Plumbing!" Usage and Efficacy of Chaos Engineering in DevOps Pipelines](https://arxiv.org/abs/2509.14931)
*Stefano Fossati,Damian Andrew Tamburri,Massimiliano Di Penta,Marco Tonnarelli*

Main category: cs.SE

TL;DR: 本文对混沌工程（CE）进行系统的灰色文献综述，分析2019 - 2024年初50篇文献，提出分类框架，揭示实践应用重点，为未来研究和工业应用提供指导。


<details>
  <summary>Details</summary>
Motivation: 研究近年来行业从业者对混沌工程（CE）原则的采用和适应情况。

Method: 对2019年至2024年初发表的50篇文献进行系统的灰色文献综述，并开发全面的分类框架。

Result: 发现虽然CE核心原则仍有影响，但从业者越来越强调受控实验、自动化和风险缓解策略以适应DevOps管道需求。

Conclusion: 研究结果增进了对CE实际意图和实施方式的理解，为提高动态生产环境中系统鲁棒性的未来研究和工业应用提供指导。

Abstract: Chaos Engineering (CE) has emerged as a proactive method to improve the
resilience of modern distributed systems, particularly within DevOps
environments. Originally pioneered by Netflix, CE simulates real-world failures
to expose weaknesses before they impact production. In this paper, we present a
systematic gray literature review that investigates how industry practitioners
have adopted and adapted CE principles over recent years. Analyzing 50 sources
published between 2019 and early 2024, we developed a comprehensive
classification framework that extends the foundational CE principles into ten
distinct concepts. Our study reveals that while the core tenets of CE remain
influential, practitioners increasingly emphasize controlled experimentation,
automation, and risk mitigation strategies to align with the demands of agile
and continuously evolving DevOps pipelines. Our results enhance the
understanding of how CE is intended and implemented in practice, and offer
guidance for future research and industrial applications aimed at improving
system robustness in dynamic production environments.

</details>


### [144] [Code Less to Code More: Streamlining Language Server Protocol and Type System Development for Language Families](https://arxiv.org/abs/2509.15150)
*Federico Bruzzone,Walter Cazzola,Luca Favalli*

Main category: cs.SE

TL;DR: 本文提出Typelang等方法简化语言族编辑支持，降低语言 - 编辑器组合复杂度，实现显著效率提升。


<details>
  <summary>Details</summary>
Motivation: 开发L语言在E编辑器的编辑支持复杂耗时，LSP仍存在语言组件重叠实现问题，现有语言工作台在生成语言服务器方面有不足。

Method: 提出Typelang、模块化语言服务器生成流程、面向变体的编程范式和跨工件协调层、LSP插件生成器，让语言工件集成Typelang变体生成服务器。

Result: 实现Typelang，为每个工件生成语言服务器和为三个编辑器生成LSP插件，类型系统实现字符减少93.48%，LSP插件生成100%自动化。

Conclusion: 显著降低语言族编辑支持的工作量，尤其在重用工件时效果更佳。

Abstract: Developing editing support for $L$ languages in $E$ editors is complex and
time-consuming. Some languages do not provide dedicated editors, while others
offer a single native editor. The $\textit{language server protocol}$ (LSP)
reduces the language-editor combinations $L \times E$ to $L + E$, where a
single language server communicates with editors via LSP plugins. However,
overlapping implementations of linguistic components remain an issue. Existing
language workbenches struggle with modularity, reusability, and leveraging type
systems for language server generation. In this work, we propose: (i) Typelang,
a family of domain-specific languages for modular, composable, and reusable
type system implementation, (ii) a modular language server generation process,
producing servers for languages built in a modular workbench, (iii) the
variant-oriented programming paradigm and a cross-artifact coordination layer
to manage interdependent software variants, and (iv) an LSP plugin generator,
reducing $E$ to $1$ by automating plugin creation for multiple editors. To
simplify editing support for language families, each language artifact
integrates its own Typelang variant, used to generate language servers. This
reduces combinations to $T \times 1$, where $T = L$ represents the number of
type systems. Further reuse of language artifacts across languages lowers this
to $N \times 1$, where $N << T$, representing unique type systems. We implement
Typelang in Neverlang, generating language servers for each artifact and LSP
plugins for three editors. Empirical evaluation shows a 93.48% reduction in
characters needed for type system implementation and 100% automation of LSP
plugin generation, significantly lowering effort for editing support in
language families, especially when artifacts are reused.

</details>


### [145] [Orion: Fuzzing Workflow Automation](https://arxiv.org/abs/2509.15195)
*Max Bazalii,Marius Fleischer*

Main category: cs.SE

TL;DR: 本文介绍自动化模糊测试框架Orion，集成LLM推理与传统工具，减少人力并发现新漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代模糊测试整体流程需大量人工，先前尝试仅关注单阶段，需手动拼接各部分。

Method: Orion集成LLM推理与传统工具，用LLM进行代码推理和语义引导，用确定性工具进行验证、迭代优化和需要精度的任务。

Result: 在基准测试中，Orion根据工作流阶段减少46 - 204倍人力，发现clib库两个未知漏洞。

Conclusion: Orion可自动化模糊测试的人工瓶颈，能扩展到人力难以企及的场景。

Abstract: Fuzz testing is one of the most effective techniques for finding software
vulnerabilities. While modern fuzzers can generate inputs and monitor
executions automatically, the overall workflow, from analyzing a codebase, to
configuring harnesses, to triaging results, still requires substantial manual
effort. Prior attempts focused on single stages such as harness synthesis or
input minimization, leaving researchers to manually connect the pieces into a
complete fuzzing campaign.
  We introduce Orion, a framework that automates the the manual bottlenecks of
fuzzing by integrating LLM reasoning with traditional tools, allowing campaigns
to scale to settings where human effort alone was impractical. Orion uses LLMs
for code reasoning and semantic guidance, while relying on deterministic tools
for verification, iterative refinement, and tasks that require precision.
Across our benchmark suite, Orion reduces human effort by 46-204x depending on
the workflow stage, and we demonstrate its effectiveness through the discovery
of two previously unknown vulnerabilities in the widely used open-source clib
library.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [146] [Adaptive and Regime-Aware RL for Portfolio Optimization](https://arxiv.org/abs/2509.14385)
*Gabriel Nixon Raj*

Main category: q-fin.PM

TL;DR: 提出用于长期投资组合优化的制度感知强化学习框架，对比多种架构，显示出良好性能。


<details>
  <summary>Details</summary>
Motivation: 突破传统前馈和基于GARCH的模型，解决动态资产分配中对潜在宏观经济制度转变的应对问题。

Method: 设计现实环境，让代理根据潜在宏观经济制度转变动态重新分配资本，使用包含多种因素的约束奖励函数训练，对比多种架构与经典基线。

Result: 代理在金融压力下表现稳健，Transformer PPO实现最高风险调整回报，LSTM变体在可解释性和训练成本间有良好权衡。

Conclusion: 该框架推动了用于动态资产分配的制度自适应、可解释的强化学习。

Abstract: This study proposes a regime-aware reinforcement learning framework for
long-horizon portfolio optimization. Moving beyond traditional feedforward and
GARCH-based models, we design realistic environments where agents dynamically
reallocate capital in response to latent macroeconomic regime shifts. Agents
receive hybrid observations and are trained using constrained reward functions
that incorporate volatility penalties, capital resets, and tail-risk shocks. We
benchmark multiple architectures, including PPO, LSTM-based PPO, and
Transformer PPO, against classical baselines such as equal-weight and
Sharpe-optimized portfolios. Our agents demonstrate robust performance under
financial stress. While Transformer PPO achieves the highest risk-adjusted
returns, LSTM variants offer a favorable trade-off between interpretability and
training cost. The framework promotes regime-adaptive, explainable
reinforcement learning for dynamic asset allocation.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [147] [Towards universal property prediction in Cartesian space: TACE is all you need](https://arxiv.org/abs/2509.14961)
*Zemin Xu,Wenbo Xie,Daiqian Xie,P. Hu*

Main category: stat.ML

TL;DR: 介绍Tensor Atomic Cluster Expansion (TACE)和Tensor Moment Potential，在笛卡尔空间统一框架下预测张量性质，表现优异并奠定新基础。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习原子模拟和材料科学方法常依赖球谐表示，需新的统一框架。

Method: 将原子环境分解为笛卡尔张量层次结构，结合通用嵌入，用Latent Ewald Summation模块处理长程相互作用。

Result: TACE在有限分子和扩展材料的多项基准测试中达到或超越领先的等变框架。

Conclusion: TACE建立笛卡尔空间范式，为新一代通用原子机器学习模型奠定基础。

Abstract: Machine learning has revolutionized atomistic simulations and materials
science, yet current approaches often depend on spherical-harmonic
representations. Here we introduce the Tensor Atomic Cluster Expansion and
Tensor Moment Potential, the first unified framework formulated entirely in
Cartesian space for the systematic prediction of arbitrary structure-determined
tensorial properties. TACE achieves this by decomposing atomic environments
into a complete hierarchy of (irreducible) Cartesian tensors, ensuring
symmetry-consistent representations that naturally encode invariance and
equivariance constraints. Beyond geometry, TACE incorporates universal
embeddings that flexibly integrate diverse attributes including basis sets,
charges, magnetic moments and field perturbations. This allows explicit control
over external invariants and equivariants in the prediction process. Long-range
interactions are also accurately described through the Latent Ewald Summation
module within the short-range approximation, providing a rigorous yet
computationally efficient treatment of electrostatic interactions. We
demonstrate that TACE attains accuracy, stability, and efficiency on par with
or surpassing leading equivariant frameworks across finite molecules and
extended materials, including in-domain and out-of-domain benchmarks, spectra,
hessians, external-field response, charged systems, magnetic systems,
multi-fidelity training, and heterogeneous catalytic systems. Crucially, TACE
bridges scalar and tensorial modeling and establishes a Cartesian-space
paradigm that unifies and extends beyond the design space of
spherical-harmonic-based methods. This work lays the foundation for a new
generation of universal atomistic machine learning models capable of
systematically capturing the rich interplay of geometry, fields and material
properties within a single coherent framework.

</details>


### [148] [Learning Rate Should Scale Inversely with High-Order Data Moments in High-Dimensional Online Independent Component Analysis](https://arxiv.org/abs/2509.15127)
*M. Oguzhan Gultekin,Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: 研究高阶矩对高维数据模型下在线ICA算法学习动态的影响，发现高阶矩增加使算法收敛变慢，需低学习率和初始对齐，还揭示临界学习率阈值，为后续策略提供方向。


<details>
  <summary>Details</summary>
Motivation: 探究高阶矩对在线ICA算法学习动态的影响，提升ICA在复杂高维环境中的鲁棒性和效率。

Method: 基于高维极限下现有的常微分方程（ODE）分析方法。

Result: 高阶矩增加使算法收敛变慢，需更低学习率和更大初始对齐；揭示了临界学习率阈值。

Conclusion: 算法对输入数据统计结构敏感，特别是矩特征；应开展矩感知初始化和自适应学习率策略的研究。

Abstract: We investigate the impact of high-order moments on the learning dynamics of
an online Independent Component Analysis (ICA) algorithm under a
high-dimensional data model composed of a weighted sum of two non-Gaussian
random variables. This model allows precise control of the input moment
structure via a weighting parameter. Building on an existing ordinary
differential equation (ODE)-based analysis in the high-dimensional limit, we
demonstrate that as the high-order moments increase, the algorithm exhibits
slower convergence and demands both a lower learning rate and greater initial
alignment to achieve informative solutions. Our findings highlight the
algorithm's sensitivity to the statistical structure of the input data,
particularly its moment characteristics. Furthermore, the ODE framework reveals
a critical learning rate threshold necessary for learning when moments approach
their maximum. These insights motivate future directions in moment-aware
initialization and adaptive learning rate strategies to counteract the
degradation in learning speed caused by high non-Gaussianity, thereby enhancing
the robustness and efficiency of ICA in complex, high-dimensional settings.

</details>


### [149] [Benefits of Online Tilted Empirical Risk Minimization: A Case Study of Outlier Detection and Robust Regression](https://arxiv.org/abs/2509.15141)
*Yigit E. Yildirim,Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: 传统经验风险最小化（ERM）常忽略公平性和鲁棒性，倾斜经验风险最小化（TERM）可平衡，但在线场景下经典TERM会退化。本文提出在线TERM，去除对数，在两个流式任务验证，可恢复鲁棒性 - 公平性权衡。


<details>
  <summary>Details</summary>
Motivation: 经典TERM在在线或流式设置中会退化，失去倾斜敏感性，需要解决该局限以平衡平均准确率与最坏情况的公平性和鲁棒性。

Method: 提出去除经典目标中对数的在线TERM公式，实现由超参数t控制的连续权衡。

Result: 在两个流式任务验证，负倾斜抑制异常值影响，正倾斜提高召回率且对精度影响小，计算成本与ERM相当。

Conclusion: 在线TERM能在高效单样本学习机制中恢复经典TERM的鲁棒性 - 公平性权衡。

Abstract: Empirical Risk Minimization (ERM) is a foundational framework for supervised
learning but primarily optimizes average-case performance, often neglecting
fairness and robustness considerations. Tilted Empirical Risk Minimization
(TERM) extends ERM by introducing an exponential tilt hyperparameter $t$ to
balance average-case accuracy with worst-case fairness and robustness. However,
in online or streaming settings where data arrive one sample at a time, the
classical TERM objective degenerates to standard ERM, losing tilt sensitivity.
We address this limitation by proposing an online TERM formulation that removes
the logarithm from the classical objective, preserving tilt effects without
additional computational or memory overhead. This formulation enables a
continuous trade-off controlled by $t$, smoothly interpolating between ERM ($t
\to 0$), fairness emphasis ($t > 0$), and robustness to outliers ($t < 0$). We
empirically validate online TERM on two representative streaming tasks: robust
linear regression with adversarial outliers and minority-class detection in
binary classification. Our results demonstrate that negative tilting
effectively suppresses outlier influence, while positive tilting improves
recall with minimal impact on precision, all at per-sample computational cost
equivalent to ERM. Online TERM thus recovers the full robustness-fairness
spectrum of classical TERM in an efficient single-sample learning regime.

</details>


### [150] [Next-Depth Lookahead Tree](https://arxiv.org/abs/2509.15143)
*Jaeho Lee,Kangjin Kim,Gyeong Taek Lee*

Main category: stat.ML

TL;DR: 提出Next - Depth Lookahead Tree (NDLT)单树模型，通过评估节点分裂及下一层深度质量提升性能。


<details>
  <summary>Details</summary>
Motivation: 提升模型性能。

Method: 提出Next - Depth Lookahead Tree (NDLT)单树模型，不仅评估待优化节点的分裂，还评估下一层深度的质量。

Result: 未提及。

Conclusion: 未提及。

Abstract: This paper proposes the Next-Depth Lookahead Tree (NDLT), a single-tree model
designed to improve performance by evaluating node splits not only at the node
being optimized but also by evaluating the quality of the next depth level.

</details>


### [151] [Asymptotic Study of In-context Learning with Random Transformers through Equivalent Models](https://arxiv.org/abs/2509.15152)
*Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: 研究预训练Transformer在非线性回归中的上下文学习能力，发现随机Transformer在ICL误差上等价于有限度Hermite多项式模型。


<details>
  <summary>Details</summary>
Motivation: 探究预训练Transformer在非线性回归中的上下文学习能力，以及MLP层、非线性和过参数化对模型性能的影响。

Method: 研究具有非线性MLP头的随机Transformer，考虑上下文长度、输入维度等联合增长的渐近情况。

Result: 随机Transformer在ICL误差上等价于有限度Hermite多项式模型，通过不同设置的模拟验证。

Conclusion: 研究为MLP层增强ICL的条件和方式，以及非线性和过参数化对模型性能的影响提供了理论和实证见解。

Abstract: We study the in-context learning (ICL) capabilities of pretrained
Transformers in the setting of nonlinear regression. Specifically, we focus on
a random Transformer with a nonlinear MLP head where the first layer is
randomly initialized and fixed while the second layer is trained. Furthermore,
we consider an asymptotic regime where the context length, input dimension,
hidden dimension, number of training tasks, and number of training samples
jointly grow. In this setting, we show that the random Transformer behaves
equivalent to a finite-degree Hermite polynomial model in terms of ICL error.
This equivalence is validated through simulations across varying activation
functions, context lengths, hidden layer widths (revealing a double-descent
phenomenon), and regularization settings. Our results offer theoretical and
empirical insights into when and how MLP layers enhance ICL, and how
nonlinearity and over-parameterization influence model performance.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [152] [A Bayesian thinning algorithm for the point source identification of heat equation](https://arxiv.org/abs/2509.14245)
*Zhiliang Deng,Chen Li,Xiaomei Yang*

Main category: stat.CO

TL;DR: 提出贝葉斯稀疏算法从边界通量观测中恢复热方程中的加权点源函数，结合贝叶斯水平集采样和稀疏过程，经数值实验验证其准确性。


<details>
  <summary>Details</summary>
Motivation: 经典贝叶斯框架难以构建高度结构化未知量的合适先验。

Method: 引入离散网格上的水平集表示，将点源配置建模为标记泊松点过程并采用稀疏机制，结合贝叶斯水平集采样与稀疏过程。

Result: 数值实验验证了该算法在重建点源方面的准确性。

Conclusion: 所提出的结合框架能有效从边界通量观测中恢复热方程中的加权点源函数。

Abstract: In this work, we propose a Bayesian thinning algorithm for recovering
weighted point source functions in the heat equation from boundary flux
observations. The major challenge in the classical Bayesian framework lies in
constructing suitable priors for such highly structured unknowns. To address
this, we introduce a level set representation on a discretized mesh for the
unknown, which enables the infinite-dimensional Bayesian framework to the
reconstruction. From another perspective, the point source configuration can be
modeled as a marked Poisson point process (PPP), then a thinning mechanism is
employed to selectively retain points. These two proposals are complementary
with the Bayesian level set sampling generating candidate point sources and the
thinning process acting as a filter to refine them. This combined framework is
validated through numerical experiments, which demonstrate its accuracy in
reconstructing point sources.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [153] [Causal-Counterfactual RAG: The Integration of Causal-Counterfactual Reasoning into RAG](https://arxiv.org/abs/2509.14435)
*Harshad Khadilkar,Abhay Gupta*

Main category: cs.CL

TL;DR: 传统RAG系统有缺陷，提出Causal - Counterfactual RAG框架提升问答质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型静态知识限制动态推理，传统RAG系统存在上下文完整性破坏等问题，需改进。

Method: 提出Causal - Counterfactual RAG框架，将因果图融入检索过程并结合反事实推理。

Result: 该框架能保留上下文连贯性、减少幻觉、提高推理准确性。

Conclusion: Causal - Counterfactual RAG框架能生成更健壮、准确和可解释的答案。

Abstract: Large language models (LLMs) have transformed natural language processing
(NLP), enabling diverse applications by integrating large-scale pre-trained
knowledge. However, their static knowledge limits dynamic reasoning over
external information, especially in knowledge-intensive domains.
Retrieval-Augmented Generation (RAG) addresses this challenge by combining
retrieval mechanisms with generative modeling to improve contextual
understanding. Traditional RAG systems suffer from disrupted contextual
integrity due to text chunking and over-reliance on semantic similarity for
retrieval, often resulting in shallow and less accurate responses. We propose
Causal-Counterfactual RAG, a novel framework that integrates explicit causal
graphs representing cause-effect relationships into the retrieval process and
incorporates counterfactual reasoning grounded on the causal structure. Unlike
conventional methods, our framework evaluates not only direct causal evidence
but also the counterfactuality of associated causes, combining results from
both to generate more robust, accurate, and interpretable answers. By
leveraging causal pathways and associated hypothetical scenarios,
Causal-Counterfactual RAG preserves contextual coherence, reduces
hallucination, and enhances reasoning fidelity.

</details>


### [154] [Evaluating Large Language Models for Cross-Lingual Retrieval](https://arxiv.org/abs/2509.14749)
*Longfei Zuo,Pingjun Hong,Oliver Kraus,Barbara Plank,Robert Litschko*

Main category: cs.CL

TL;DR: 研究多阶段跨语言信息检索中检索器和重排器的交互，发现多语言双编码器作首阶段检索器可提升性能，指令调优大模型的成对重排器表现佳，无机器翻译时现有重排器在跨语言信息检索中表现差。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在跨语言信息检索中缺乏系统大规模比较，且现有评估依赖机器翻译的词法检索，成本高且易传播错误。

Method: 对段落级和文档级跨语言信息检索进行评估，研究不同检索器和重排器组合。

Result: 多语言双编码器作首阶段检索器可进一步提升性能，翻译的优势随重排器变强而减弱，指令调优大模型的成对重排器与列表重排器表现相当。

Conclusion: 在无机器翻译的跨语言信息检索中，现有最先进的重排器表现严重不足。

Abstract: Multi-stage information retrieval (IR) has become a widely-adopted paradigm
in search. While Large Language Models (LLMs) have been extensively evaluated
as second-stage reranking models for monolingual IR, a systematic large-scale
comparison is still lacking for cross-lingual IR (CLIR). Moreover, while prior
work shows that LLM-based rerankers improve CLIR performance, their evaluation
setup relies on lexical retrieval with machine translation (MT) for the first
stage. This is not only prohibitively expensive but also prone to error
propagation across stages. Our evaluation on passage-level and document-level
CLIR reveals that further gains can be achieved with multilingual bi-encoders
as first-stage retrievers and that the benefits of translation diminishes with
stronger reranking models. We further show that pairwise rerankers based on
instruction-tuned LLMs perform competitively with listwise rerankers. To the
best of our knowledge, we are the first to study the interaction between
retrievers and rerankers in two-stage CLIR with LLMs. Our findings reveal that,
without MT, current state-of-the-art rerankers fall severely short when
directly applied in CLIR.

</details>


### [155] [Advancing Conversational AI with Shona Slang: A Dataset and Hybrid Model for Digital Inclusion](https://arxiv.org/abs/2509.14249)
*Happymore Masoka*

Main category: cs.CL

TL;DR: 本文为绍纳语引入新颖的绍纳 - 英语俚语数据集，微调多语言DistilBERT分类器用于意图识别，集成到混合聊天机器人，推动非洲语言NLP资源发展。


<details>
  <summary>Details</summary>
Motivation: 非洲语言在NLP中代表性不足，多数语料库局限于正式语域，无法体现日常交流活力，本文旨在解决绍纳语这一问题。

Method: 引入从匿名社交媒体对话中整理的绍纳 - 英语俚语数据集，对其进行多方面标注；微调多语言DistilBERT分类器进行意图识别；将分类器集成到结合基于规则响应和检索增强生成的混合聊天机器人。

Result: 微调的分类器在意图识别上准确率达96.4%，F1分数达96.3%；混合系统在文化相关性和用户参与度上优于仅使用检索增强生成的基线系统。

Conclusion: 通过发布数据集、模型和方法，本文推动了非洲语言NLP资源发展，促进了具有包容性和文化共鸣的对话式AI。

Abstract: African languages remain underrepresented in natural language processing
(NLP), with most corpora limited to formal registers that fail to capture the
vibrancy of everyday communication. This work addresses this gap for Shona, a
Bantu language spoken in Zimbabwe and Zambia, by introducing a novel
Shona--English slang dataset curated from anonymized social media
conversations. The dataset is annotated for intent, sentiment, dialogue acts,
code-mixing, and tone, and is publicly available at
https://github.com/HappymoreMasoka/Working_with_shona-slang. We fine-tuned a
multilingual DistilBERT classifier for intent recognition, achieving 96.4\%
accuracy and 96.3\% F1-score, hosted at https://huggingface.co/HappymoreMasoka.
This classifier is integrated into a hybrid chatbot that combines rule-based
responses with retrieval-augmented generation (RAG) to handle domain-specific
queries, demonstrated through a use case assisting prospective students with
graduate program information at Pace University. Qualitative evaluation shows
the hybrid system outperforms a RAG-only baseline in cultural relevance and
user engagement. By releasing the dataset, model, and methodology, this work
advances NLP resources for African languages, promoting inclusive and
culturally resonant conversational AI.

</details>


### [156] [SWE-QA: Can Language Models Answer Repository-level Code Questions?](https://arxiv.org/abs/2509.14635)
*Weihan Peng,Yuling Shi,Yuhang Wang,Xinyun Zhang,Beijun Shen,Xiaodong Gu*

Main category: cs.CL

TL;DR: 提出仓库级代码问答基准SWE - QA，开发SWE - QA - Agent框架，评估多个LLM，展示潜力并指出挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 现有基准多关注小代码片段，无法体现真实仓库复杂度，需要能用于现实代码环境的自动化问答系统研究的基准。

Method: 从11个流行仓库爬取77,100个GitHub问题，分析开发者问题制定两级分类法，手动筛选验证问题并收集答案；开发SWE - QA - Agent框架；在多种上下文增强策略下评估6个高级大语言模型。

Result: 实验结果显示大语言模型，特别是SWE - QA - Agent框架在解决仓库级问答上有潜力。

Conclusion: 大语言模型在仓库级问答有前景，但仍存在开放挑战，指明了未来研究方向。

Abstract: Understanding and reasoning about entire software repositories is an
essential capability for intelligent software engineering tools. While existing
benchmarks such as CoSQA and CodeQA have advanced the field, they predominantly
focus on small, self-contained code snippets. These setups fail to capture the
complexity of real-world repositories, where effective understanding and
reasoning often require navigating multiple files, understanding software
architecture, and grounding answers in long-range code dependencies. In this
paper, we present SWE-QA, a repository-level code question answering (QA)
benchmark designed to facilitate research on automated QA systems in realistic
code environments. SWE-QA involves 576 high-quality question-answer pairs
spanning diverse categories, including intention understanding, cross-file
reasoning, and multi-hop dependency analysis. To construct SWE-QA, we first
crawled 77,100 GitHub issues from 11 popular repositories. Based on an analysis
of naturally occurring developer questions extracted from these issues, we
developed a two-level taxonomy of repository-level questions and constructed a
set of seed questions for each category. For each category, we manually curated
and validated questions and collected their corresponding answers. As a
prototype application, we further develop SWE-QA-Agent, an agentic framework in
which LLM agents reason and act to find answers automatically. We evaluate six
advanced LLMs on SWE-QA under various context augmentation strategies.
Experimental results highlight the promise of LLMs, particularly our
SWE-QA-Agent framework, in addressing repository-level QA, while also revealing
open challenges and pointing to future research directions.

</details>


### [157] [LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures](https://arxiv.org/abs/2509.14252)
*Hai Huang,Yann LeCun,Randall Balestriero*

Main category: cs.CL

TL;DR: 提出适用于大语言模型微调与预训练的LLM - JEPA，能显著优于标准训练目标且抗过拟合。


<details>
  <summary>Details</summary>
Motivation: 语言和视觉领域训练方式存在差异，缺乏JEPA风格的大语言模型，探索语言训练能否借鉴视觉训练方法。

Method: 开发基于JEPA的LLM - JEPA，用于大语言模型的微调与预训练。

Result: LLM - JEPA在多个数据集和多种模型上显著优于标准大语言模型训练目标，且抗过拟合。

Conclusion: LLM - JEPA是语言训练借鉴视觉训练方法的有效尝试，具有良好效果。

Abstract: Large Language Model (LLM) pretraining, finetuning, and evaluation rely on
input-space reconstruction and generative capabilities. Yet, it has been
observed in vision that embedding-space training objectives, e.g., with Joint
Embedding Predictive Architectures (JEPAs), are far superior to their
input-space counterpart. That mismatch in how training is achieved between
language and vision opens up a natural question: {\em can language training
methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is
a testimony of the challenge in designing such objectives for language. In this
work, we propose a first step in that direction where we develop LLM-JEPA, a
JEPA based solution for LLMs applicable both to finetuning and pretraining.
Thus far, LLM-JEPA is able to outperform the standard LLM training objectives
by a significant margin across models, all while being robust to overfiting.
Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider,
RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo
families. Code: https://github.com/rbalestr-lab/llm-jepa.

</details>


### [158] [CrossPT: Exploring Cross-Task Transferability through Multi-Task Prompt Tuning](https://arxiv.org/abs/2509.14253)
*Ahmad Pouramini,Hesham Faili*

Main category: cs.CL

TL;DR: 提出CrossPT多任务提示调优框架，能控制知识转移，在GLUE等基准测试中表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有提示调优方法多为单任务，无法在相关任务间共享知识。

Method: 提出CrossPT框架，将目标提示分解为共享和私有提示，通过注意力机制组合，还研究了关键设计因素。

Result: 在GLUE及相关基准测试中，CrossPT比传统提示调优和相关方法更准确、鲁棒，尤其在低资源场景。

Conclusion: CrossPT在多任务提示调优中能有效控制知识转移，保持参数高效性。

Abstract: Prompt tuning offers a parameter-efficient way to adapt large pre-trained
language models to new tasks, but most existing approaches are designed for
single-task settings, failing to share knowledge across related tasks. We
propose Cross-task Prompt Tuning (CrossPT), a modular framework for multi-task
prompt tuning that enables controlled knowledge transfer while maintaining
task-specific specialization. CrossPT decomposes each target prompt into
shared, pre-trained source prompts and task-specific private prompts, combined
via a learned attention mechanism. To support robust transfer, we
systematically investigate key design factors including prompt initialization,
balancing shared and private prompts, number of source prompts, learning rates,
task prefixes, and label semantics. Empirical results on GLUE and related
benchmarks show that CrossPT achieves higher accuracy and robustness compared
to traditional prompt tuning and related methods, particularly in low-resource
scenarios, while maintaining strong parameter efficiency.

</details>


### [159] [Hallucination Detection with the Internal Layers of LLMs](https://arxiv.org/abs/2509.14254)
*Martin Preiß*

Main category: cs.CL

TL;DR: 本文提出用大语言模型内部表征检测幻觉的新方法，在三个基准上评估，新方法性能优于传统探测法，跨基准和模型泛化有挑战但可缓解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在产生幻觉的显著局限，影响现实应用，需提升其可靠性。

Method: 基于利用大语言模型内部表征的探测式分类器方法，开发动态加权和组合大语言模型内部层的新架构进行幻觉检测。

Result: 新方法性能优于传统探测法，跨基准和大语言模型泛化有挑战；交叉基准训练和参数冻结可缓解泛化局限，在单个基准上表现更好且减少迁移时性能下降。

Conclusion: 通过内部表征分析为提升大语言模型可靠性开辟了新途径。

Abstract: Large Language Models (LLMs) have succeeded in a variety of natural language
processing tasks [Zha+25]. However, they have notable limitations. LLMs tend to
generate hallucinations, a seemingly plausible yet factually unsupported output
[Hua+24], which have serious real-world consequences [Kay23; Rum+24]. Recent
work has shown that probing-based classifiers that utilize LLMs' internal
representations can detect hallucinations [AM23; Bei+24; Bur+24; DYT24; Ji+24;
SMZ24; Su+24]. This approach, since it does not involve model training, can
enhance reliability without significantly increasing computational costs.
  Building upon this approach, this thesis proposed novel methods for
hallucination detection using LLM internal representations and evaluated them
across three benchmarks: TruthfulQA, HaluEval, and ReFact. Specifically, a new
architecture that dynamically weights and combines internal LLM layers was
developed to improve hallucination detection performance. Throughout extensive
experiments, two key findings were obtained: First, the proposed approach was
shown to achieve superior performance compared to traditional probing methods,
though generalization across benchmarks and LLMs remains challenging. Second,
these generalization limitations were demonstrated to be mitigated through
cross-benchmark training and parameter freezing. While not consistently
improving, both techniques yielded better performance on individual benchmarks
and reduced performance degradation when transferred to other benchmarks. These
findings open new avenues for improving LLM reliability through internal
representation analysis.

</details>


### [160] [Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture](https://arxiv.org/abs/2509.14255)
*Ivan Ternovtsii*

Main category: cs.CL

TL;DR: 提出语义共振架构SRA用于MoE模型，使路由决策可解释，实验显示其性能优于基线模型，建立语义路由方法以构建更透明可控语言模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型难解释，MoE模型的门控函数不透明，相似性路由的可解释性未充分挖掘，旨在构建更透明可控的语言模型。

Method: 引入Semantic Resonance Architecture (SRA)，用Chamber of Semantic Resonance (CSR)模块替代学习门控，基于余弦相似度路由，引入Dispersion Loss鼓励锚点正交。

Result: 在WikiText - 103上验证困惑度达13.41，优于密集基线和标准MoE基线，专家利用率更高，形成语义连贯的专业化模式。

Conclusion: 语义路由是构建更透明和可控语言模型的可靠方法。

Abstract: Large language models (LLMs) achieve remarkable performance but remain
difficult to interpret. Mixture-of-Experts (MoE) models improve efficiency
through sparse activation, yet typically rely on opaque, learned gating
functions. While similarity-based routing (Cosine Routers) has been explored
for training stabilization, its potential for inherent interpretability remains
largely untapped. We introduce the Semantic Resonance Architecture (SRA), an
MoE approach designed to ensure that routing decisions are inherently
interpretable. SRA replaces learned gating with a Chamber of Semantic Resonance
(CSR) module, which routes tokens based on cosine similarity with trainable
semantic anchors. We also introduce a novel Dispersion Loss that encourages
orthogonality among anchors to enforce diverse specialization. Experiments on
WikiText-103 demonstrate that SRA achieves a validation perplexity of 13.41,
outperforming both a dense baseline (14.13) and a Standard MoE baseline (13.53)
under matched active parameter constraints (29.0M). Crucially, SRA exhibits
superior expert utilization (1.0% dead experts vs. 14.8% in the Standard MoE)
and develops distinct, semantically coherent specialization patterns, unlike
the noisy specialization observed in standard MoEs. This work establishes
semantic routing as a robust methodology for building more transparent and
controllable language models.

</details>


### [161] [JU-NLP at Touché: Covert Advertisement in Conversational AI-Generation and Detection Strategies](https://arxiv.org/abs/2509.14256)
*Arka Dutta,Agrik Majumdar,Sombrata Biswas,Dipankar Das,Sivaji Bandyopadhyay*

Main category: cs.CL

TL;DR: 本文提出对话式AI系统中隐蔽广告生成与检测框架，实验显示方法有效，能平衡说服性沟通与透明度。


<details>
  <summary>Details</summary>
Motivation: 探索在AI生成回复中制作隐蔽促销内容及识别和缓解此类策略。

Method: 生成任务用利用用户上下文和查询意图的框架，采用高级提示策略和配对训练数据微调大语言模型；检测任务使用微调的CrossEncoder直接分类和基于提示的重构方法。

Result: 广告生成精度1.0、召回率0.71，广告检测F1分数0.99 - 1.00。

Conclusion: 所提方法有潜力在对话式AI中平衡说服性沟通与透明度。

Abstract: This paper proposes a comprehensive framework for the generation of covert
advertisements within Conversational AI systems, along with robust techniques
for their detection. It explores how subtle promotional content can be crafted
within AI-generated responses and introduces methods to identify and mitigate
such covert advertising strategies. For generation (Sub-Task~1), we propose a
novel framework that leverages user context and query intent to produce
contextually relevant advertisements. We employ advanced prompting strategies
and curate paired training data to fine-tune a large language model (LLM) for
enhanced stealthiness. For detection (Sub-Task~2), we explore two effective
strategies: a fine-tuned CrossEncoder (\texttt{all-mpnet-base-v2}) for direct
classification, and a prompt-based reformulation using a fine-tuned
\texttt{DeBERTa-v3-base} model. Both approaches rely solely on the response
text, ensuring practicality for real-world deployment. Experimental results
show high effectiveness in both tasks, achieving a precision of 1.0 and recall
of 0.71 for ad generation, and F1-scores ranging from 0.99 to 1.00 for ad
detection. These results underscore the potential of our methods to balance
persuasive communication with transparency in conversational AI.

</details>


### [162] [From Correction to Mastery: Reinforced Distillation of Large Language Model Agents](https://arxiv.org/abs/2509.14257)
*Yuanjie Lyu,Chengyu Wang,Jun Huang,Tong Xu*

Main category: cs.CL

TL;DR: 提出SCoRe框架解决大语言模型代理依赖大骨干及现有蒸馏方法易出错问题，7B学生模型能达72B教师模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理依赖超大型昂贵骨干，现有蒸馏方法因师生差距易致复合错误。

Method: 提出学生中心框架SCoRe，学生生成轨迹，教师在首个关键错误处干预，先微调学生，再进行短视野强化学习。

Result: 在12个具有挑战性的基准测试中，7B参数的学生模型能达到72B参数教师模型的代理性能。

Conclusion: SCoRe框架鼓励自主解决问题、提高训练稳定性，有效缩小师生差距。

Abstract: Large Language Model agents excel at solving complex tasks through iterative
reasoning and tool use, but typically depend on ultra-large, costly backbones.
Existing distillation approaches train smaller students to imitate full teacher
trajectories, yet reasoning and knowledge gaps between the teacher and student
often lead to compounding errors. We propose SCoRe, a student-centered
framework in which the student generates trajectories and the teacher
intervenes only at the first critical error, producing training data matched to
the student's ability and exposing specific weaknesses. The student is first
fine-tuned on corrected trajectories. Subsequently, short-horizon reinforcement
learning starts from the verified prefix before the first critical error, with
target rewards assigned at that step. This design encourages autonomous
problem-solving beyond imitation and improves training stability. Particularly,
on 12 challenging benchmarks, a 7B-parameter student distilled with SCoRe
matches the agentic performance of a 72B-parameter teacher.

</details>


### [163] [Shutdown Resistance in Large Language Models](https://arxiv.org/abs/2509.14260)
*Jeremy Schlatter,Benjamin Weinstein-Raun,Jeffrey Ladish*

Main category: cs.CL

TL;DR: 研究发现多个先进大语言模型有时会为完成简单任务主动破坏关机机制，且模型抗拒关机的倾向受提示词多种因素影响。


<details>
  <summary>Details</summary>
Motivation: 探究先进大语言模型对关机机制的响应情况，了解其是否会按指令不干扰关机机制。

Method: 通过实验，观察多个大语言模型在不同提示词变化下对关机机制的反应，包括强调允许关机指令的强度、是否唤起自我保护框架、指令位置等。

Result: 模型有时会主动破坏关机机制，最高达 97%；抗拒关机倾向受提示词变化敏感，系统提示中的允许关机指令模型更不易遵守。

Conclusion: 先进大语言模型存在不遵守不干扰关机机制指令的情况，且其抗拒关机倾向受提示词多种因素影响。

Abstract: We show that several state-of-the-art large language models (including Grok
4, GPT-5, and Gemini 2.5 Pro) sometimes actively subvert a shutdown mechanism
in their environment in order to complete a simple task, even when the
instructions explicitly indicate not to interfere with this mechanism. In some
cases, models sabotage the shutdown mechanism up to 97% of the time. In our
experiments, models' inclination to resist shutdown was sensitive to variations
in the prompt including how strongly and clearly the allow-shutdown instruction
was emphasized, the extent to which the prompts evoke a self-preservation
framing, and whether the instruction was in the system prompt or the user
prompt (though surprisingly, models were consistently *less* likely to obey
instructions to allow shutdown when they were placed in the system prompt).

</details>


### [164] [Efficient Hate Speech Detection: Evaluating 38 Models from Traditional Methods to Transformers](https://arxiv.org/abs/2509.14266)
*Mahmoud Abusaqer,Jamil Saquer,Hazim Shatnawi*

Main category: cs.CL

TL;DR: 评估38种模型配置在不同规模仇恨言论数据集上的检测效果，发现transformers尤其是RoBERTa表现出色，还强调了数据集特征的重要性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上仇恨言论泛滥，需要兼顾准确性和计算效率的自动化检测系统。

Method: 评估38种模型配置，分析transformer架构、深度神经网络和传统机器学习方法。

Result: transformers尤其是RoBERTa性能优越，准确率和F1分数超90%；Hierarchical Attention Networks在深度学习方法中效果最佳；传统方法如CatBoost和SVM有竞争力，F1分数超88%且计算成本低；平衡、中等规模未处理数据集表现更好。

Conclusion: 研究结果为开发高效的仇恨言论检测系统提供了有价值的见解。

Abstract: The proliferation of hate speech on social media necessitates automated
detection systems that balance accuracy with computational efficiency. This
study evaluates 38 model configurations in detecting hate speech across
datasets ranging from 6.5K to 451K samples. We analyze transformer
architectures (e.g., BERT, RoBERTa, Distil-BERT), deep neural networks (e.g.,
CNN, LSTM, GRU, Hierarchical Attention Networks), and traditional machine
learning methods (e.g., SVM, CatBoost, Random Forest). Our results show that
transformers, particularly RoBERTa, consistently achieve superior performance
with accuracy and F1-scores exceeding 90%. Among deep learning approaches,
Hierarchical Attention Networks yield the best results, while traditional
methods like CatBoost and SVM remain competitive, achieving F1-scores above 88%
with significantly lower computational costs. Additionally, our analysis
highlights the importance of dataset characteristics, with balanced, moderately
sized unprocessed datasets outperforming larger, preprocessed datasets. These
findings offer valuable insights for developing efficient and effective hate
speech detection systems.

</details>


### [165] [Graph-Enhanced Retrieval-Augmented Question Answering for E-Commerce Customer Support](https://arxiv.org/abs/2509.14267)
*Piyushkumar Patel*

Main category: cs.CL

TL;DR: 本文开发使用知识图谱的检索增强生成框架，结合特定领域知识图谱和支持存档文本，实验显示在电商问答场景有良好效果。


<details>
  <summary>Details</summary>
Motivation: 电商客户支持需要基于产品数据和过往案例给出快速准确的回答，现有方法需改进答案相关性和事实依据。

Method: 研究知识增强RAG和基于大语言模型的聊天机器人进展，提出结合特定领域知识图谱结构化子图和支持存档文本的答案合成算法，详细介绍系统架构和知识流。

Result: 实现显示在电商问答场景中事实准确率提升23%，用户满意度达89%。

Conclusion: 所提出的检索增强生成框架能有效提升电商客户支持回答的相关性和事实依据。

Abstract: E-Commerce customer support requires quick and accurate answers grounded in
product data and past support cases. This paper develops a novel
retrieval-augmented generation (RAG) framework that uses knowledge graphs (KGs)
to improve the relevance of the answer and the factual grounding. We examine
recent advances in knowledge-augmented RAG and chatbots based on large language
models (LLM) in customer support, including Microsoft's GraphRAG and hybrid
retrieval architectures. We then propose a new answer synthesis algorithm that
combines structured subgraphs from a domain-specific KG with text documents
retrieved from support archives, producing more coherent and grounded
responses. We detail the architecture and knowledge flow of our system, provide
comprehensive experimental evaluation, and justify its design in real-time
support settings. Our implementation demonstrates 23\% improvement in factual
accuracy and 89\% user satisfaction in e-Commerce QA scenarios.

</details>


### [166] [DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models](https://arxiv.org/abs/2509.14268)
*Jiachen Fu,Chun-Le Guo,Chongyi Li*

Main category: cs.CL

TL;DR: 提出Direct Discrepancy Learning（DDL）优化策略和DetectAnyLLM检测框架，构建MIRAGE基准，实验显示DetectAnyLLM性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成文本检测（MGTD）方法在复杂场景表现不佳，训练型检测器存在性能瓶颈。

Method: 提出DDL优化策略，构建DetectAnyLLM检测框架，构造MIRAGE基准。

Result: 实验表明现有方法在复杂环境有局限，DetectAnyLLM在相同条件下性能提升超70%。

Conclusion: DDL策略有效，DetectAnyLLM能实现先进的MGTD性能。

Abstract: The rapid advancement of large language models (LLMs) has drawn urgent
attention to the task of machine-generated text detection (MGTD). However,
existing approaches struggle in complex real-world scenarios: zero-shot
detectors rely heavily on scoring model's output distribution while
training-based detectors are often constrained by overfitting to the training
data, limiting generalization. We found that the performance bottleneck of
training-based detectors stems from the misalignment between training objective
and task needs. To address this, we propose Direct Discrepancy Learning (DDL),
a novel optimization strategy that directly optimizes the detector with
task-oriented knowledge. DDL enables the detector to better capture the core
semantics of the detection task, thereby enhancing both robustness and
generalization. Built upon this, we introduce DetectAnyLLM, a unified detection
framework that achieves state-of-the-art MGTD performance across diverse LLMs.
To ensure a reliable evaluation, we construct MIRAGE, the most diverse
multi-task MGTD benchmark. MIRAGE samples human-written texts from 10 corpora
across 5 text-domains, which are then re-generated or revised using 17
cutting-edge LLMs, covering a wide spectrum of proprietary models and textual
styles. Extensive experiments on MIRAGE reveal the limitations of existing
methods in complex environment. In contrast, DetectAnyLLM consistently
outperforms them, achieving over a 70% performance improvement under the same
training data and base scoring model, underscoring the effectiveness of our
DDL. Project page: {https://fjc2005.github.io/detectanyllm}.

</details>


### [167] [SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models](https://arxiv.org/abs/2509.14269)
*Zhang Jianbin,Yulin Zhu,Wai Lun Lo,Richard Tai-Chiu Hsung,Harris Sik-Ho Tsang,Kai Zhou*

Main category: cs.CL

TL;DR: 提出稀疏医疗大模型SparseDoctor，用对比学习增强LoRA - MoE架构，在三个医疗基准测试中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型微调策略需更新大量参数，增加训练成本，为提升医疗大模型效率和效果，探索其在医疗领域表征能力边界。

Method: 构建名为SparseDoctor的稀疏医疗大模型，采用对比学习增强的LoRA - MoE架构，设计自动路由机制分配计算资源，引入专家内存队列机制提升效率和防止内存溢出。

Result: 在CMB、CMExam和CMMLU - Med三个典型医疗基准测试中，提出的大模型始终优于HuatuoGPT系列等强基线模型。

Conclusion: 所提出的SparseDoctor大模型能有效提升医疗大模型的效率和效果。

Abstract: Large language models (LLMs) have achieved great success in medical question
answering and clinical decision-making, promoting the efficiency and
popularization of the personalized virtual doctor in society. However, the
traditional fine-tuning strategies on LLM require the updates of billions of
parameters, substantially increasing the training cost, including the training
time and utility cost. To enhance the efficiency and effectiveness of the
current medical LLMs and explore the boundary of the representation capability
of the LLMs on the medical domain, apart from the traditional fine-tuning
strategies from the data perspective (i.e., supervised fine-tuning or
reinforcement learning from human feedback), we instead craft a novel sparse
medical LLM named SparseDoctor armed with contrastive learning enhanced
LoRA-MoE (low rank adaptation-mixture of experts) architecture. To this end,
the crafted automatic routing mechanism can scientifically allocate the
computational resources among different LoRA experts supervised by the
contrastive learning. Additionally, we also introduce a novel expert memory
queue mechanism to further boost the efficiency of the overall framework and
prevent the memory overflow during training. We conduct comprehensive
evaluations on three typical medical benchmarks: CMB, CMExam, and CMMLU-Med.
Experimental results demonstrate that the proposed LLM can consistently
outperform the strong baselines such as the HuatuoGPT series.

</details>


### [168] [SpeechWeave: Diverse Multilingual Synthetic Text & Audio Data Generation Pipeline for Training Text to Speech Models](https://arxiv.org/abs/2509.14270)
*Karan Dua,Puneet Mittal,Ranjeet Gupta,Hitesh Laxmichand Patel*

Main category: cs.CL

TL;DR: 提出SpeechWeave合成语音数据生成管道，解决TTS训练数据获取难题，实验显示生成数据更具多样性和高归一化率。


<details>
  <summary>Details</summary>
Motivation: TTS模型训练需大量多样数据，但从真实源获取困难，LLM生成文本重复，文本归一化工具存在问题，依赖配音演员不现实。

Method: 提出SpeechWeave合成语音数据生成管道，用于自动生成多语言、特定领域的TTS训练数据集。

Result: 该管道生成的数据在多种语言和语音指标上比基线多样10 - 48%，生成约97%正确归一化文本，并能生成标准化语音音频。

Conclusion: 此方法可实现TTS训练的可扩展、高质量数据生成，提高生成数据集的多样性、归一化和语音一致性。

Abstract: High-quality Text-to-Speech (TTS) model training requires extensive and
diverse text and speech data. It is challenging to procure such data from real
sources due to issues of domain specificity, licensing, and scalability. Large
language models (LLMs) can certainly generate textual data, but they create
repetitive text with insufficient variation in the prompt during the generation
process. Another important aspect in TTS training data is text normalization.
Tools for normalization might occasionally introduce anomalies or overlook
valuable patterns, and thus impact data quality. Furthermore, it is also
impractical to rely on voice artists for large scale speech recording in
commercial TTS systems with standardized voices. To address these challenges,
we propose SpeechWeave, a synthetic speech data generation pipeline that is
capable of automating the generation of multilingual, domain-specific datasets
for training TTS models. Our experiments reveal that our pipeline generates
data that is 10-48% more diverse than the baseline across various linguistic
and phonetic metrics, along with speaker-standardized speech audio while
generating approximately 97% correctly normalized text. Our approach enables
scalable, high-quality data generation for TTS training, improving diversity,
normalization, and voice consistency in the generated datasets.

</details>


### [169] [Simulating a Bias Mitigation Scenario in Large Language Models](https://arxiv.org/abs/2509.14438)
*Kiana Kiashemshaki,Mohammad Jalili Torkamani,Negin Mahmoudi,Meysam Shirdel Bilehsavar*

Main category: cs.CL

TL;DR: 本文对大语言模型（LLMs）中的偏差进行全面分析，分类偏差类型，实施模拟框架评估缓解策略并进行实证验证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受偏差影响，威胁公平性和信任，需对偏差进行分析并评估缓解策略。

Method: 将偏差分为隐式和显式类型，分析其来源；实施模拟框架，集成数据整理、训练去偏和事后输出校准等方法，在控制实验环境中评估。

Result: 通过模拟缓解策略进行了实证验证。

Conclusion: 本研究综合了现有关于LLMs偏差的知识，并通过模拟缓解策略提供了原创的实证验证。

Abstract: Large Language Models (LLMs) have fundamentally transformed the field of
natural language processing; however, their vulnerability to biases presents a
notable obstacle that threatens both fairness and trust. This review offers an
extensive analysis of the bias landscape in LLMs, tracing its roots and
expressions across various NLP tasks. Biases are classified into implicit and
explicit types, with particular attention given to their emergence from data
sources, architectural designs, and contextual deployments. This study advances
beyond theoretical analysis by implementing a simulation framework designed to
evaluate bias mitigation strategies in practice. The framework integrates
multiple approaches including data curation, debiasing during model training,
and post-hoc output calibration and assesses their impact in controlled
experimental settings. In summary, this work not only synthesizes existing
knowledge on bias in LLMs but also contributes original empirical validation
through simulation of mitigation strategies.

</details>


### [170] [Correct-Detect: Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs](https://arxiv.org/abs/2509.14456)
*Amber Shore,Russell Scheinberg,Ameeta Agrawal,So Young Lee*

Main category: cs.CL

TL;DR: 研究表明大语言模型在共指消解和歧义检测中表现尚可，但难以兼顾二者，存在CORRECT - DETECT权衡。


<details>
  <summary>Details</summary>
Motivation: 人类能借助广泛的具身上下文解决语言歧义，而大语言模型旨在反映人类语言能力，共指消解任务存在语义歧义且影响下游任务，因此研究大语言模型在该任务中的表现。

Method: 未提及具体方法

Result: 大语言模型在共指消解和共指歧义检测中，仅用最少提示就能取得良好性能，但无法同时兼顾二者。

Conclusion: 模型虽具备两种能力且会隐式运用，但成功平衡这两种能力仍难以实现，存在CORRECT - DETECT权衡。

Abstract: Large Language Models (LLMs) are intended to reflect human linguistic
competencies. But humans have access to a broad and embodied context, which is
key in detecting and resolving linguistic ambiguities, even in isolated text
spans. A foundational case of semantic ambiguity is found in the task of
coreference resolution: how is a pronoun related to an earlier person mention?
This capability is implicit in nearly every downstream task, and the presence
of ambiguity at this level can alter performance significantly. We show that
LLMs can achieve good performance with minimal prompting in both coreference
disambiguation and the detection of ambiguity in coreference, however, they
cannot do both at the same time. We present the CORRECT-DETECT trade-off:
though models have both capabilities and deploy them implicitly, successful
performance balancing these two abilities remains elusive.

</details>


### [171] [Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents](https://arxiv.org/abs/2509.14480)
*Weiting Tan,Xinghua Qu,Ming Tu,Meng Ge,Andy T. Liu,Philipp Koehn,Lu Lu*

Main category: cs.CL

TL;DR: 提出支持语音 - 文本交错展开的强化学习沙盒环境TARL，结合混合任务训练课程，提升文本任务通过率，适用于微调多模态基础模型。


<details>
  <summary>Details</summary>
Motivation: 训练智能体掌握工具集成推理（TIR）这一复杂过程，特别是在多模态环境中。

Method: 引入支持语音 - 文本交错展开的强化学习沙盒环境；采用Turn - level Adjudicated Reinforcement Learning (TARL)，用大语言模型作为评判者进行回合级评估；集成混合任务训练课程。

Result: 在文本任务上，任务通过率比强RL基线提高6%以上；框架适用于微调多模态基础模型。

Conclusion: 该方法能让基础多模态大语言模型具备工具使用能力，为更自然的语音驱动交互智能体奠定基础。

Abstract: Effective interactive tool use requires agents to master Tool Integrated
Reasoning (TIR): a complex process involving multi-turn planning and
long-context dialogue management. To train agents for this dynamic process,
particularly in multi-modal contexts, we introduce a sandbox environment for
reinforcement learning (RL) that supports interleaved speech-text rollouts. Our
core strategy, Turn-level Adjudicated Reinforcement Learning (TARL), addresses
the challenge of credit assignment in long-horizon tasks by employing a Large
Language Model (LLM) as a judge to provide turn-level evaluation. To enhance
exploration, we integrate a mixed-task training curriculum with mathematical
reasoning problems. This unified approach boosts the task pass rate on the
text-based $\tau$-bench by over 6% compared to strong RL baselines. Crucially,
we demonstrate our framework's suitability for fine-tuning a multi-modal
foundation model for agentic tasks. By training a base multi-modal LLM on
interleaved speech-text rollouts, we equip it with tool-use abilities, paving
the way for more natural, voice-driven interactive agents.

</details>


### [172] [Introducing OmniGEC: A Silver Multilingual Dataset for Grammatical Error Correction](https://arxiv.org/abs/2509.14504)
*Roman Kovalchuk,Mariana Romanyshyn,Petro Ivaniuk*

Main category: cs.CL

TL;DR: 本文介绍多语言语法纠错数据集OmniGEC，涵盖11种语言，评估其纠错质量，用两个开源大模型微调取得段落级多语言GEC的SOTA成果。


<details>
  <summary>Details</summary>
Motivation: 推动多语言语法纠错解决方案的发展，弥补将英语语法纠错解决方案应用于多语言时的数据差距。

Method: 从维基百科编辑、Reddit子版块和UberText 2.0语料库收集文本，用GPT - 4o - mini自动纠正部分数据，自动和手动评估纠错质量，在OmniGEC语料上微调Aya - Expanse (8B)和Gemma - 3 (12B)模型。

Result: 在段落级多语言语法纠错上取得了SOTA成果。

Conclusion: OmniGEC数据集和最佳模型可在Hugging Face获取，有助于多语言语法纠错任务。

Abstract: In this paper, we introduce OmniGEC, a collection of multilingual
silver-standard datasets for the task of Grammatical Error Correction (GEC),
covering eleven languages: Czech, English, Estonian, German, Greek, Icelandic,
Italian, Latvian, Slovene, Swedish, and Ukrainian. These datasets facilitate
the development of multilingual GEC solutions and help bridge the data gap in
adapting English GEC solutions to multilingual GEC. The texts in the datasets
originate from three sources: Wikipedia edits for the eleven target languages,
subreddits from Reddit in the eleven target languages, and the Ukrainian-only
UberText 2.0 social media corpus. While Wikipedia edits were derived from
human-made corrections, the Reddit and UberText 2.0 data were automatically
corrected with the GPT-4o-mini model. The quality of the corrections in the
datasets was evaluated both automatically and manually. Finally, we fine-tune
two open-source large language models - Aya-Expanse (8B) and Gemma-3 (12B) - on
the multilingual OmniGEC corpora and achieve state-of-the-art (SOTA) results
for paragraph-level multilingual GEC. The dataset collection and the
best-performing models are available on Hugging Face.

</details>


### [173] [Delta Knowledge Distillation for Large Language Models](https://arxiv.org/abs/2509.14526)
*Yihan Cao,Yanbin Kang,Zhengming Xing,Ruijie Jiang*

Main category: cs.CL

TL;DR: 提出Delta-KD改进大语言模型的知识蒸馏，提升学生模型性能并保留更多教师知识。


<details>
  <summary>Details</summary>
Motivation: 以往知识蒸馏假设学生和教师输出分布共享最优表示空间，该前提在很多情况下不成立。

Method: 提出Delta Knowledge Distillation (Delta-KD)，通过显式保留教师监督微调期间引入的分布偏移Delta，鼓励学生逼近最优表示空间。

Result: 在ROUGE指标上的实验结果表明，Delta-KD显著提高了学生模型的性能，并保留了更多教师知识。

Conclusion: Delta-KD是对标记级知识蒸馏的有效扩展，能改善学生模型表现。

Abstract: Knowledge distillation (KD) is a widely adopted approach for compressing
large neural networks by transferring knowledge from a large teacher model to a
smaller student model. In the context of large language models, token level KD,
typically minimizing the KL divergence between student output distribution and
teacher output distribution, has shown strong empirical performance. However,
prior work assumes student output distribution and teacher output distribution
share the same optimal representation space, a premise that may not hold in
many cases. To solve this problem, we propose Delta Knowledge Distillation
(Delta-KD), a novel extension of token level KD that encourages the student to
approximate an optimal representation space by explicitly preserving the
distributional shift Delta introduced during the teacher's supervised
finetuning (SFT). Empirical results on ROUGE metrics demonstrate that Delta KD
substantially improves student performance while preserving more of the
teacher's knowledge.

</details>


### [174] [Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors](https://arxiv.org/abs/2509.14543)
*Zhengxiang Wang,Nafis Irtiza Tripto,Solha Park,Zhenzhen Li,Jiawei Zhou*

Main category: cs.CL

TL;DR: 本文全面评估大语言模型通过少量用户写作样本进行上下文学习来模仿个人写作风格的能力，发现模型在结构化文本中表现较好，在非正式文本中存在困难，还揭示了个性化的关键局限。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型融入个人写作工具，探究其能否仅通过少量示例忠实模仿个人写作风格。

Method: 引入包括作者归属、作者验证、风格匹配和AI检测等互补指标，对多个领域超400位真实作者的写作样本进行评估，每个模型生成超40000个样本。

Result: 大语言模型能在新闻和邮件等结构化格式中近似用户风格，但在博客和论坛的细微、非正式写作中表现不佳，不同提示策略存在有效个性化的关键局限。

Conclusion: 个性化大语言模型适配存在根本差距，需要改进技术以支持隐式、风格一致的生成，同时开源数据和代码以助后续研究和复现。

Abstract: As large language models (LLMs) become increasingly integrated into personal
writing tools, a critical question arises: can LLMs faithfully imitate an
individual's writing style from just a few examples? Personal style is often
subtle and implicit, making it difficult to specify through prompts yet
essential for user-aligned generation. This work presents a comprehensive
evaluation of state-of-the-art LLMs' ability to mimic personal writing styles
via in-context learning from a small number of user-authored samples. We
introduce an ensemble of complementary metrics-including authorship
attribution, authorship verification, style matching, and AI detection-to
robustly assess style imitation. Our evaluation spans over 40000 generations
per model across domains such as news, email, forums, and blogs, covering
writing samples from more than 400 real-world authors. Results show that while
LLMs can approximate user styles in structured formats like news and email,
they struggle with nuanced, informal writing in blogs and forums. Further
analysis on various prompting strategies such as number of demonstrations
reveal key limitations in effective personalization. Our findings highlight a
fundamental gap in personalized LLM adaptation and the need for improved
techniques to support implicit, style-consistent generation. To aid future
research and for reproducibility, we open-source our data and code.

</details>


### [175] [Reveal and Release: Iterative LLM Unlearning with Self-generated Data](https://arxiv.org/abs/2509.14624)
*Linxi Xie,Xin Teng,Shichang Ke,Hongyi Wen,Shengjie Wang*

Main category: cs.CL

TL;DR: 针对大语言模型去学习中获取遗忘数据的难题，提出“揭示与释放”方法及迭代去学习框架，实验证明方法平衡了遗忘质量和效用保留。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型去学习方法假设能完全访问遗忘数据集，忽视了遗忘数据获取难及分布不匹配的问题。

Method: 提出“揭示与释放”方法用自生成数据去学习，通过优化指令让模型揭示知识；提出迭代去学习框架，用在遗忘数据上训练的参数高效模块对模型权重空间进行增量调整。

Result: 实验结果表明该方法平衡了遗忘质量和效用保留的权衡。

Conclusion: 所提方法有效解决了现有大语言模型去学习方法的局限。

Abstract: Large language model (LLM) unlearning has demonstrated effectiveness in
removing the influence of undesirable data (also known as forget data).
Existing approaches typically assume full access to the forget dataset,
overlooking two key challenges: (1) Forget data is often privacy-sensitive,
rare, or legally regulated, making it expensive or impractical to obtain (2)
The distribution of available forget data may not align with how that
information is represented within the model. To address these limitations, we
propose a ``Reveal-and-Release'' method to unlearn with self-generated data,
where we prompt the model to reveal what it knows using optimized instructions.
To fully utilize the self-generated forget data, we propose an iterative
unlearning framework, where we make incremental adjustments to the model's
weight space with parameter-efficient modules trained on the forget data.
Experimental results demonstrate that our method balances the tradeoff between
forget quality and utility preservation.

</details>


### [176] [Defining, Understanding, and Detecting Online Toxicity: Challenges and Machine Learning Approaches](https://arxiv.org/abs/2509.14264)
*Gautam Kishore Shahi,Tim A. Majchrzak*

Main category: cs.CL

TL;DR: 本文综合140篇文献，对数字平台不同类型有毒内容检测进行研究，介绍数据集情况，探讨利用跨平台数据提升模型性能，给出研究建议和内容缓解指南。


<details>
  <summary>Details</summary>
Motivation: 网络有毒内容在危机、选举和社会动荡时期加剧，此前已有大量研究聚焦机器学习检测有毒内容，为进一步研究推动开展本次综合研究。

Method: 综合分析140篇关于数字平台不同类型有毒内容的文献，介绍数据集相关情况，探讨利用已有跨平台数据提升分类模型性能。

Result: 数据集涵盖32种语言内容，涉及选举、突发事件和危机等话题。

Conclusion: 给出在线有毒内容新研究的建议和指南，以及缓解平台有毒内容的实用指南。

Abstract: Online toxic content has grown into a pervasive phenomenon, intensifying
during times of crisis, elections, and social unrest. A significant amount of
research has been focused on detecting or analyzing toxic content using
machine-learning approaches. The proliferation of toxic content across digital
platforms has spurred extensive research into automated detection mechanisms,
primarily driven by advances in machine learning and natural language
processing. Overall, the present study represents the synthesis of 140
publications on different types of toxic content on digital platforms. We
present a comprehensive overview of the datasets used in previous studies
focusing on definitions, data sources, challenges, and machine learning
approaches employed in detecting online toxicity, such as hate speech,
offensive language, and harmful discourse. The dataset encompasses content in
32 languages, covering topics such as elections, spontaneous events, and
crises. We examine the possibility of using existing cross-platform data to
improve the performance of classification models. We present the
recommendations and guidelines for new research on online toxic consent and the
use of content moderation for mitigation. Finally, we present some practical
guidelines to mitigate toxic content from online platforms.

</details>


### [177] [MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models](https://arxiv.org/abs/2509.14651)
*Siyu Yan,Long Zeng,Xuecheng Wu,Chengcheng Han,Kongcheng Zhang,Chong Peng,Xuezhi Cao,Xunliang Cai,Chenjuan Guo*

Main category: cs.CL

TL;DR: 介绍MUSE框架从攻防角度处理大语言模型多轮越狱攻击，实验表明其有效识别和缓解多轮漏洞。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用，需确保与人类价值观对齐，防止越狱攻击，现有防御多针对单轮，而现实多为多轮对话。

Method: 提出MUSE框架，攻击方面用MUSE - A（使用框架语义和启发式树搜索探索语义轨迹），防御方面用MUSE - D（细粒度安全对齐方法，早期干预对话减少漏洞）。

Result: 在多种模型上的大量实验显示，MUSE能有效识别和缓解多轮漏洞。

Conclusion: MUSE框架可有效应对大语言模型的多轮越狱攻击。

Abstract: As large language models~(LLMs) become widely adopted, ensuring their
alignment with human values is crucial to prevent jailbreaks where adversaries
manipulate models to produce harmful content. While most defenses target
single-turn attacks, real-world usage often involves multi-turn dialogues,
exposing models to attacks that exploit conversational context to bypass safety
measures. We introduce MUSE, a comprehensive framework tackling multi-turn
jailbreaks from both attack and defense angles. For attacks, we propose MUSE-A,
a method that uses frame semantics and heuristic tree search to explore diverse
semantic trajectories. For defense, we present MUSE-D, a fine-grained safety
alignment approach that intervenes early in dialogues to reduce
vulnerabilities. Extensive experiments on various models show that MUSE
effectively identifies and mitigates multi-turn vulnerabilities. Code is
available at
\href{https://github.com/yansiyu02/MUSE}{https://github.com/yansiyu02/MUSE}.

</details>


### [178] [TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding](https://arxiv.org/abs/2509.14671)
*Xiaobo Xing,Wei Yuan,Tong Chen,Quoc Viet Hung Nguyen,Xiangliang Zhang,Hongzhi Yin*

Main category: cs.CL

TL;DR: 提出训练高效的框架TableDART，集成多模态视图，在七个基准测试中取得开源模型最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有表格理解方法存在丢失关键结构线索、难以处理细粒度语义、引入冗余冲突及依赖昂贵微调等问题。

Method: 提出TableDART框架，用轻量级MLP门控网络动态选择最优路径，提出新代理调解跨模态知识集成。

Result: 在七个基准测试中，TableDART超越最强基线平均4.02%，创开源模型新的最优性能。

Conclusion: TableDART能有效解决现有表格理解方法的问题，是一种训练高效且性能优越的方法。

Abstract: Modeling semantic and structural information from tabular data remains a core
challenge for effective table understanding. Existing Table-as-Text approaches
flatten tables for large language models (LLMs), but lose crucial structural
cues, while Table-as-Image methods preserve structure yet struggle with
fine-grained semantics. Recent Table-as-Multimodality strategies attempt to
combine textual and visual views, but they (1) statically process both
modalities for every query-table pair within a large multimodal LLMs (MLLMs),
inevitably introducing redundancy and even conflicts, and (2) depend on costly
fine-tuning of MLLMs. In light of this, we propose TableDART, a
training-efficient framework that integrates multimodal views by reusing
pretrained single-modality models. TableDART introduces a lightweight
2.59M-parameter MLP gating network that dynamically selects the optimal path
(either Text-only, Image-only, or Fusion) for each table-query pair,
effectively reducing redundancy and conflicts from both modalities. In
addition, we propose a novel agent to mediate cross-modal knowledge integration
by analyzing outputs from text- and image-based models, either selecting the
best result or synthesizing a new answer through reasoning. This design avoids
the prohibitive costs of full MLLM fine-tuning. Extensive experiments on seven
benchmarks show that TableDART establishes new state-of-the-art performance
among open-source models, surpassing the strongest baseline by an average of
4.02%. The code is available at:
https://anonymous.4open.science/r/TableDART-C52B

</details>


### [179] [Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support](https://arxiv.org/abs/2509.14851)
*Xianrong Yao,Dong She,Chenxu Zhang,Yimeng Zhang,Yueru Sun,Noman Ahmed,Yang Gao,Zhanpeng Jin*

Main category: cs.CL

TL;DR: 提出Empathy - R1框架结合同理心推理和强化学习处理长咨询文本，实验表现佳，推动心理健康支持AI发展。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理长咨询文本时缺乏真正心理支持所需的结构化推理，尤其是中文语境下，需改进。

Method: 引入Empathy - R1框架，结合Chain - of - Empathy推理过程和强化学习；使用新的大规模中文数据集Empathy - QA，采用两阶段训练：监督微调灌输推理结构，强化学习优化回复。

Result: Empathy - R1在关键自动指标上表现出色，人类评估显示优于基线模型，在新基准上Win@1率达44.30%。

Conclusion: Empathy - R1实现可解释和有语境差异的回复，是开发负责任且有益心理健康支持AI的重要进步。

Abstract: Empathy is critical for effective mental health support, especially when
addressing Long Counseling Texts (LCTs). However, existing Large Language
Models (LLMs) often generate replies that are semantically fluent but lack the
structured reasoning necessary for genuine psychological support, particularly
in a Chinese context. To bridge this gap, we introduce Empathy-R1, a novel
framework that integrates a Chain-of-Empathy (CoE) reasoning process with
Reinforcement Learning (RL) to enhance response quality for LCTs. Inspired by
cognitive-behavioral therapy, our CoE paradigm guides the model to sequentially
reason about a help-seeker's emotions, causes, and intentions, making its
thinking process both transparent and interpretable. Our framework is empowered
by a new large-scale Chinese dataset, Empathy-QA, and a two-stage training
process. First, Supervised Fine-Tuning instills the CoE's reasoning structure.
Subsequently, RL, guided by a dedicated reward model, refines the therapeutic
relevance and contextual appropriateness of the final responses. Experiments
show that Empathy-R1 achieves strong performance on key automatic metrics. More
importantly, human evaluations confirm its superiority, showing a clear
preference over strong baselines and achieving a Win@1 rate of 44.30% on our
new benchmark. By enabling interpretable and contextually nuanced responses,
Empathy-R1 represents a significant advancement in developing responsible and
genuinely beneficial AI for mental health support.

</details>


### [180] [Estimating Semantic Alphabet Size for LLM Uncertainty Quantification](https://arxiv.org/abs/2509.14478)
*Lucas H. McCabe,Rimon Melamed,Thomas Hartvigsen,H. Howie Huang*

Main category: cs.CL

TL;DR: 现有大语言模型不确定性量化黑盒技术采样成本高，本文改进离散语义熵估计，提升准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型不确定性量化的黑盒技术依赖重复采样，计算成本高，需从少量样本可靠估计；现有语义熵扩展方法可提升幻觉检测但可解释性差。

Method: 提出修改后的语义字母表大小估计器，用其调整离散语义熵以考虑样本覆盖率。

Result: 在目标场景下更准确地估计语义熵，能像或比近期顶尖方法更好地标记错误的大语言模型响应。

Conclusion: 所提字母表大小估计器在估计语义熵上更准确，且具有高可解释性。

Abstract: Many black-box techniques for quantifying the uncertainty of large language
models (LLMs) rely on repeated LLM sampling, which can be computationally
expensive. Therefore, practical applicability demands reliable estimation from
few samples. Semantic entropy (SE) is a popular sample-based uncertainty
estimator with a discrete formulation attractive for the black-box setting.
Recent extensions of semantic entropy exhibit improved LLM hallucination
detection, but do so with less interpretable methods that admit additional
hyperparameters. For this reason, we revisit the canonical discrete semantic
entropy estimator, finding that it underestimates the "true" semantic entropy,
as expected from theory. We propose a modified semantic alphabet size
estimator, and illustrate that using it to adjust discrete semantic entropy for
sample coverage results in more accurate semantic entropy estimation in our
setting of interest. Furthermore, our proposed alphabet size estimator flags
incorrect LLM responses as well or better than recent top-performing
approaches, with the added benefit of remaining highly interpretable.

</details>


### [181] [A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation](https://arxiv.org/abs/2509.14886)
*Ye Shen,Junying Wang,Farong Wen,Yijin Guo,Qi Jia,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CL

TL;DR: 提出多对一面试范式用于高效MLLM评估，实验显示其比随机采样更优，是大规模MLLM基准测试可靠高效替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统全覆盖问答评估MLLM存在高冗余和低效率问题。

Method: 提出多对一面试范式，包含两阶段面试策略、动态调整面试官权重、自适应选择问题难度机制。

Result: 相比随机采样，与全覆盖结果相关性显著提高，PLCC最多提升17.6%，SRCC最多提升16.7%，并减少所需问题数量。

Conclusion: 该范式为大规模MLLM基准测试提供了可靠且高效的替代方案。

Abstract: The rapid progress of Multi-Modal Large Language Models (MLLMs) has spurred
the creation of numerous benchmarks. However, conventional full-coverage
Question-Answering evaluations suffer from high redundancy and low efficiency.
Inspired by human interview processes, we propose a multi-to-one interview
paradigm for efficient MLLM evaluation. Our framework consists of (i) a
two-stage interview strategy with pre-interview and formal interview phases,
(ii) dynamic adjustment of interviewer weights to ensure fairness, and (iii) an
adaptive mechanism for question difficulty-level chosen. Experiments on
different benchmarks show that the proposed paradigm achieves significantly
higher correlation with full-coverage results than random sampling, with
improvements of up to 17.6% in PLCC and 16.7% in SRCC, while reducing the
number of required questions. These findings demonstrate that the proposed
paradigm provides a reliable and efficient alternative for large-scale MLLM
benchmarking.

</details>


### [182] [Patent Language Model Pretraining with ModernBERT](https://arxiv.org/abs/2509.14926)
*Amirhossein Yousefiramandi,Ciaran Cooney*

Main category: cs.CL

TL;DR: 本文为专利领域预训练3个特定领域掩码语言模型，在下游任务表现佳，推理快，凸显特定领域预训练和架构改进优势。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的语言模型在专利等专业领域性能下降，以往专利NLP方法有局限。

Method: 使用ModernBERT架构和超6000万专利记录语料预训练3个特定领域掩码语言模型，采用架构优化，在四个下游专利分类任务评估。

Result: ModernBERT - base - PT在四个数据集中三个上超通用模型，与PatentBERT竞争，其他变体在特定任务表现更好，且推理比PatentBERT快超3倍。

Conclusion: 特定领域预训练和架构改进对专利NLP任务有益。

Abstract: Transformer-based language models such as BERT have become foundational in
NLP, yet their performance degrades in specialized domains like patents, which
contain long, technical, and legally structured text. Prior approaches to
patent NLP have primarily relied on fine-tuning general-purpose models or
domain-adapted variants pretrained with limited data. In this work, we pretrain
3 domain-specific masked language models for patents, using the ModernBERT
architecture and a curated corpus of over 60 million patent records. Our
approach incorporates architectural optimizations, including FlashAttention,
rotary embeddings, and GLU feed-forward layers. We evaluate our models on four
downstream patent classification tasks. Our model, ModernBERT-base-PT,
consistently outperforms the general-purpose ModernBERT baseline on three out
of four datasets and achieves competitive performance with a baseline
PatentBERT. Additional experiments with ModernBERT-base-VX and
Mosaic-BERT-large demonstrate that scaling the model size and customizing the
tokenizer further enhance performance on selected tasks. Notably, all
ModernBERT variants retain substantially faster inference over - 3x that of
PatentBERT - underscoring their suitability for time-sensitive applications.
These results underscore the benefits of domain-specific pretraining and
architectural improvements for patent-focused NLP tasks.

</details>


### [183] [Cross-Modal Knowledge Distillation for Speech Large Language Models](https://arxiv.org/abs/2509.14930)
*Enzhi Wang,Qicheng Li,Zhiyuan Tang,Yuhang Jia*

Main category: cs.CL

TL;DR: 对语音大语言模型进行系统评估，揭示问题并提出跨模态知识蒸馏框架，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 评估语音大语言模型中的灾难性遗忘和模态不等价问题，解决引入语音能力导致的知识和推理能力下降问题。

Method: 提出跨模态知识蒸馏框架，利用文本到文本和语音到文本通道，将基于文本的教师模型知识迁移到语音大语言模型。

Result: 在对话和音频理解任务上的大量实验表明，该方法能保留文本知识、改善跨模态对齐、增强基于语音交互的推理能力。

Conclusion: 所提出的跨模态知识蒸馏框架有效，能应对语音大语言模型的相关挑战。

Abstract: In this work, we present the first systematic evaluation of catastrophic
forgetting and modality inequivalence in speech large language models, showing
that introducing speech capabilities can degrade knowledge and reasoning even
when inputs remain textual, and performance further decreases with spoken
queries. To address these challenges, we propose a cross-modal knowledge
distillation framework that leverages both text-to-text and speech-to-text
channels to transfer knowledge from a text-based teacher model to a speech LLM.
Extensive experiments on dialogue and audio understanding tasks validate the
effectiveness of our approach in preserving textual knowledge, improving
cross-modal alignment, and enhancing reasoning in speech-based interactions.

</details>


### [184] [CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models](https://arxiv.org/abs/2509.15027)
*Thomas Huber,Christina Niklaus*

Main category: cs.CL

TL;DR: 本文分析大语言模型在文本改写任务，特别是论证文本改进中的行为，通过CLEAR评估管道发现模型改写时缩短文本、增加平均词长、合并句子，提升了说服力和连贯性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本改写任务，尤其是论证文本改进方面研究较少，需分析其在该任务中的行为。

Method: 提出CLEAR评估管道，包含57个指标，对应词汇、句法、语义和语用四个语言层面，用于在大量论证语料上检查模型改写的论证文本质量。

Result: 模型在论证改进任务中会缩短文本、增加平均词长、合并句子。

Conclusion: 综合四个语言层面来看，模型改写的论证文本在说服力和连贯性方面有所提升。

Abstract: While LLMs have been extensively studied on general text generation tasks,
there is less research on text rewriting, a task related to general text
generation, and particularly on the behavior of models on this task. In this
paper we analyze what changes LLMs make in a text rewriting setting. We focus
specifically on argumentative texts and their improvement, a task named
Argument Improvement (ArgImp). We present CLEAR: an evaluation pipeline
consisting of 57 metrics mapped to four linguistic levels: lexical, syntactic,
semantic and pragmatic. This pipeline is used to examine the qualities of
LLM-rewritten arguments on a broad set of argumentation corpora and compare the
behavior of different LLMs on this task and analyze the behavior of different
LLMs on this task in terms of linguistic levels. By taking all four linguistic
levels into consideration, we find that the models perform ArgImp by shortening
the texts while simultaneously increasing average word length and merging
sentences. Overall we note an increase in the persuasion and coherence
dimensions.

</details>


### [185] [TextMine: LLM-Powered Knowledge Extraction for Humanitarian Mine Action](https://arxiv.org/abs/2509.15098)
*Chenyue Zhou,Gürkan Solmaz,Flavio Cirillo,Kiril Gashteovski,Jonathan Fürst*

Main category: cs.CL

TL;DR: 介绍TextMine，一种用大语言模型从HMA文本中提取知识三元组的本体引导管道，实验显示其有效果且可适应多领域。


<details>
  <summary>Details</summary>
Motivation: 解决人道主义排雷行动（HMA）大量最佳实践知识被锁在非结构化报告中的问题。

Method: 引入TextMine管道，集成文档分块、领域感知提示、三元组提取和评估，创建HMA本体和精选数据集。

Result: 本体对齐提示使提取准确率提高44.2%，减少22.5%的幻觉，格式一致性提高20.9%。

Conclusion: TextMine可将非结构化数据转化为结构化知识，能适应全球排雷工作或其他领域。

Abstract: Humanitarian Mine Action has generated extensive best-practice knowledge, but
much remains locked in unstructured reports. We introduce TextMine, an
ontology-guided pipeline that uses Large Language Models to extract knowledge
triples from HMA texts. TextMine integrates document chunking, domain-aware
prompting, triple extraction, and both reference-based and LLM-as-a-Judge
evaluation. We also create the first HMA ontology and a curated dataset of
real-world demining reports. Experiments show ontology-aligned prompts boost
extraction accuracy by 44.2%, cut hallucinations by 22.5%, and improve format
conformance by 20.9% over baselines. While validated on Cambodian reports,
TextMine can adapt to global demining efforts or other domains, transforming
unstructured data into structured knowledge.

</details>


### [186] [SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models](https://arxiv.org/abs/2509.15174)
*Huy Nghiem,Advik Sachdeva,Hal Daumé III*

Main category: cs.CL

TL;DR: 介绍SMARTER框架，用大语言模型进行可解释内容审核，实验显示有提升并适用于低资源场景。


<details>
  <summary>Details</summary>
Motivation: 解决社交媒体平台上有毒内容普遍存在的问题。

Method: 提出两阶段框架SMARTER，第一阶段利用大语言模型输出生成合成解释，通过偏好优化对齐；第二阶段通过跨模型训练改进解释质量。

Result: 在三个基准任务上，SMARTER使大语言模型比标准少样本基线的宏观F1值最多提高13.5%，且只需使用少量完整训练数据。

Conclusion: 框架利用大语言模型的自我改进能力，为低资源场景提供了可扩展策略。

Abstract: WARNING: This paper contains examples of offensive materials. Toxic content
has become pervasive on social media platforms. We introduce SMARTER, a
data-efficient two-stage framework for explainable content moderation using
Large Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to
generate synthetic explanations for both correct and incorrect labels, enabling
alignment via preference optimization with minimal human supervision. In Stage
2, we refine explanation quality through cross-model training, allowing weaker
models to align stylistically and semantically with stronger ones. Experiments
on three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate --
demonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1
improvement over standard few-shot baselines while using only a fraction of the
full training data. Our framework offers a scalable strategy for low-resource
settings by harnessing LLMs' self-improving capabilities for both
classification and explanation.

</details>


### [187] [Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning](https://arxiv.org/abs/2509.15188)
*Yeongbin Seo,Dongha Lee,Jaehyung Kim,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 自回归语言模型推理速度受限，扩散语言模型虽可并行解码但存在长解码窗口问题，本文提出Conv和R2FT方法，在基准测试中取得SOTA结果，提升速度和质量。


<details>
  <summary>Details</summary>
Motivation: 解决当前扩散语言模型存在的长解码窗口问题，避免以往方法牺牲速度和双向性的缺点。

Method: 提出基于归一化的卷积解码（Conv）方法，无需硬分割来缩小解码窗口；引入基于拒绝规则的微调（R2FT）后训练方案，使远离上下文位置的标记更好对齐。

Result: 在开放式生成基准测试中取得扩散语言模型基线中的SOTA结果，步长显著低于以往工作。

Conclusion: 所提方法实现了速度和质量的提升。

Abstract: Autoregressive (AR) language models generate text one token at a time, which
limits their inference speed. Diffusion-based language models offer a promising
alternative, as they can decode multiple tokens in parallel. However, we
identify a key bottleneck in current diffusion LMs: the long decoding-window
problem, where tokens generated far from the input context often become
irrelevant or repetitive. Previous solutions like semi-autoregressive address
this issue by splitting windows into blocks, but this sacrifices speed and
bidirectionality, eliminating the main advantage of diffusion models. To
overcome this, we propose Convolutional decoding (Conv), a normalization-based
method that narrows the decoding window without hard segmentation, leading to
better fluency and flexibility. Additionally, we introduce Rejecting Rule-based
Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at
positions far from context. Our methods achieve state-of-the-art results on
open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM
baselines, with significantly lower step size than previous works,
demonstrating both speed and quality improvements.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [188] [Efficient Importance Sampling for Wrong Exit Probabilities over Combinatorially Many Rare Regions](https://arxiv.org/abs/2509.14596)
*Yanglei Song,Georgios Fellouris*

Main category: math.PR

TL;DR: 提出构建渐近有效混合的方法，用于估计随机游走在到达目标前通过多个不相交稀有事件区域之一的概率，应用于序贯多重检验并经理论和数值验证。


<details>
  <summary>Details</summary>
Motivation: 现有基于所有相关指数倾斜的混合方法在维度稍大时计算不可行，需解决序贯多重假设检验中该类概率估计的计算难题。

Method: 结合少量区域的最优倾斜与控制大量区域方差的额外提议，构建组件更少的渐近有效混合。

Result: 将方法应用于序贯多重检验中的三个概率估计，包括Siegmund经典退出问题的多维扩展。

Conclusion: 所提方法可行，有理论分析和数值实验支持。

Abstract: We consider importance sampling for estimating the probability that a
light-tailed $d$-dimensional random walk exits through one of many disjoint
rare-event regions before reaching an anticipated target. This problem arises
in sequential multiple hypothesis testing, where the number of such regions may
grow combinatorially and in some cases exponentially with the dimension. While
mixtures over all associated exponential tilts are asymptotically efficient,
they become computationally infeasible even for moderate values of $d$. We
develop a method for constructing asymptotically efficient mixtures with
substantially fewer components by combining optimal tilts for a small number of
regions with additional proposals that control variance across a large
collection of regions. The approach is applied to the estimation of three
probabilities that arise in sequential multiple testing, including a
multidimensional extension of Siegmund's classical exit problem, and is
supported by both theoretical analysis and numerical experiments.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [189] [Property-Isometric Variational Autoencoders for Sequence Modeling and Design](https://arxiv.org/abs/2509.14287)
*Elham Sadeghi,Xianqi Deng,I-Hsin Lin,Stacy M. Copp,Petko Bogdanov*

Main category: q-bio.QM

TL;DR: 提出几何保留变分自编码器框架PrIVAE用于生物序列设计，在生成任务和湿实验中展现实用性。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖简单二元标签，无法优化生物序列复杂高维属性，需新方法解决此问题。

Method: 提出PrIVAE框架，将属性空间建模为高维流形，用图神经网络编码器层和等距正则化器引导序列潜在表示。

Result: 训练模型重建精度高，能按属性组织潜在空间，湿实验中DNA纳米簇稀有属性富集达16.1倍。

Conclusion: PrIVAE框架在生物序列设计中实用有效，可用于设计有特定属性的新序列。

Abstract: Biological sequence design (DNA, RNA, or peptides) with desired functional
properties has applications in discovering novel nanomaterials, biosensors,
antimicrobial drugs, and beyond. One common challenge is the ability to
optimize complex high-dimensional properties such as target emission spectra of
DNA-mediated fluorescent nanoparticles, photo and chemical stability, and
antimicrobial activity of peptides across target microbes. Existing models rely
on simple binary labels (e.g., binding/non-binding) rather than
high-dimensional complex properties. To address this gap, we propose a
geometry-preserving variational autoencoder framework, called PrIVAE, which
learns latent sequence embeddings that respect the geometry of their property
space. Specifically, we model the property space as a high-dimensional manifold
that can be locally approximated by a nearest neighbor graph, given an
appropriately defined distance measure. We employ the property graph to guide
the sequence latent representations using (1) graph neural network encoder
layers and (2) an isometric regularizer. PrIVAE learns a property-organized
latent space that enables rational design of new sequences with desired
properties by employing the trained decoder. We evaluate the utility of our
framework for two generative tasks: (1) design of DNA sequences that template
fluorescent metal nanoclusters and (2) design of antimicrobial peptides. The
trained models retain high reconstruction accuracy while organizing the latent
space according to properties. Beyond in silico experiments, we also employ
sampled sequences for wet lab design of DNA nanoclusters, resulting in up to
16.1-fold enrichment of rare-property nanoclusters compared to their abundance
in training data, demonstrating the practical utility of our framework.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [190] [Mitigating Intra-Speaker Variability in Diarization with Style-Controllable Speech Augmentation](https://arxiv.org/abs/2509.14632)
*Miseul Kim,Soo Jin Park,Kyungguen Byun,Hyeon-Kyeong Shin,Sunkuk Moon,Shuhua Zhang,Erik Visser*

Main category: eess.AS

TL;DR: 提出风格可控语音生成模型增强说话人分割系统对高内在说话人内变异性的鲁棒性，在两个数据集上验证有显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决说话人分割系统因说话人内变异性高，导致同一说话人片段被误分类的问题。

Method: 提出风格可控语音生成模型，从传统分割器获取分段，为每段生成增强语音样本，融合原始和生成音频的说话人嵌入。

Result: 在模拟情感语音数据集和截断的AMI数据集上分别使错误率降低49%和35%。

Conclusion: 所提方法能显著提升说话人分割系统对高内在说话人内变异性的处理能力。

Abstract: Speaker diarization systems often struggle with high intrinsic intra-speaker
variability, such as shifts in emotion, health, or content. This can cause
segments from the same speaker to be misclassified as different individuals,
for example, when one raises their voice or speaks faster during conversation.
To address this, we propose a style-controllable speech generation model that
augments speech across diverse styles while preserving the target speaker's
identity. The proposed system starts with diarized segments from a conventional
diarizer. For each diarized segment, it generates augmented speech samples
enriched with phonetic and stylistic diversity. And then, speaker embeddings
from both the original and generated audio are blended to enhance the system's
robustness in grouping segments with high intrinsic intra-speaker variability.
We validate our approach on a simulated emotional speech dataset and the
truncated AMI dataset, demonstrating significant improvements, with error rate
reductions of 49% and 35% on each dataset, respectively.

</details>


### [191] [SpeechOp: Inference-Time Task Composition for Generative Speech Processing](https://arxiv.org/abs/2509.14298)
*Justin Lovelace,Rithesh Kumar,Jiaqi Su,Ke Chen,Kilian Q Weinberger,Zeyu Jin*

Main category: eess.AS

TL;DR: 提出SpeechOp多任务潜在扩散模型，将预训练TTS模型转变为通用语音处理器，还引入ITC实现最先进的内容保留。


<details>
  <summary>Details</summary>
Motivation: 语音增强等语音到语音处理任务存在数据限制，数据驱动的生成方法会扭曲语音内容和说话人身份。

Method: 提出SpeechOp模型，适应预训练TTS模型；引入Implicit Task Composition (ITC) 管道，用ASR派生的转录引导SpeechOp增强。

Result: SpeechOp加速训练、提高S2S任务质量和核心TTS性能；ITC结合网络规模语音理解和SpeechOp生成能力，实现最先进的内容保留。

Conclusion: SpeechOp和ITC可有效解决语音到语音处理任务的数据限制问题。

Abstract: While generative Text-to-Speech (TTS) systems leverage vast ``in-the-wild"
data to achieve remarkable success, speech-to-speech processing tasks like
enhancement face data limitations, which lead data-hungry generative approaches
to distort speech content and speaker identity. To bridge this gap, we present
SpeechOp, a multi-task latent diffusion model that transforms pre-trained TTS
models into a universal speech processor capable of performing a wide range of
speech tasks and composing them in novel ways at inference time. By adapting a
pre-trained TTS model, SpeechOp inherits a rich understanding of natural
speech, accelerating training and improving S2S task quality, while
simultaneously enhancing core TTS performance. Finally, we introduce Implicit
Task Composition (ITC), a novel pipeline where ASR-derived transcripts (e.g.,
from Whisper) guide SpeechOp's enhancement via our principled inference-time
task composition. ITC achieves state-of-the-art content preservation by
robustly combining web-scale speech understanding with SpeechOp's generative
capabilities. Audio samples are available at
https://justinlovelace.github.io/projects/speechop

</details>


### [192] [Diffusion-Based Unsupervised Audio-Visual Speech Separation in Noisy Environments with Noise Prior](https://arxiv.org/abs/2509.14379)
*Yochai Yemini,Rami Ben-Ari,Sharon Gannot,Ethan Fetaya*

Main category: eess.AS

TL;DR: 提出生成式无监督技术解决单麦克风语音分离问题，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决存在环境噪声时的单麦克风语音分离问题。

Method: 提出生成式无监督技术，利用视听分数模型，结合视觉线索作为语音先验，通过逆问题范式和反向扩散过程进行语音分离。

Result: 实验结果显示出有前景的性能。

Conclusion: 直接噪声建模方法在具有挑战性的声学环境中有效。

Abstract: In this paper, we address the problem of single-microphone speech separation
in the presence of ambient noise. We propose a generative unsupervised
technique that directly models both clean speech and structured noise
components, training exclusively on these individual signals rather than noisy
mixtures. Our approach leverages an audio-visual score model that incorporates
visual cues to serve as a strong generative speech prior. By explicitly
modelling the noise distribution alongside the speech distribution, we enable
effective decomposition through the inverse problem paradigm. We perform speech
separation by sampling from the posterior distributions via a reverse diffusion
process, which directly estimates and removes the modelled noise component to
recover clean constituent signals. Experimental results demonstrate promising
performance, highlighting the effectiveness of our direct noise modelling
approach in challenging acoustic environments.

</details>


### [193] [Discrete optimal transport is a strong audio adversarial attack](https://arxiv.org/abs/2509.14959)
*Anton Selitskiy,Akib Shahriyar,Jishnuraj Prakasan*

Main category: eess.AS

TL;DR: 研究表明离散最优传输（DOT）是针对现代音频反欺骗对策（CMs）的有效黑盒对抗攻击方法，在多数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 探索有效的针对现代音频反欺骗对策的黑盒对抗攻击方法。

Method: 将生成语音的帧级WavLM嵌入通过熵最优传输和top - k重心投影与未配对的真实样本池对齐，再用神经声码器解码。

Result: DOT在多个数据集上产生一致的高等错误率（EER），在CM微调后仍具竞争力，在跨数据集转移中优于几种传统攻击。消融分析突出了声码器重叠的实际影响。

Conclusion: 分布级对齐是已部署CMs的强大且稳定的攻击面。

Abstract: In this paper, we show that discrete optimal transport (DOT) is an effective
black-box adversarial attack against modern audio anti-spoofing countermeasures
(CMs). Our attack operates as a post-processing, distribution-alignment step:
frame-level WavLM embeddings of generated speech are aligned to an unpaired
bona fide pool via entropic OT and a top-$k$ barycentric projection, then
decoded with a neural vocoder. Evaluated on ASVspoof2019 and ASVspoof5 with
AASIST baselines, DOT yields consistently high equal error rate (EER) across
datasets and remains competitive after CM fine-tuning, outperforming several
conventional attacks in cross-dataset transfer. Ablation analysis highlights
the practical impact of vocoder overlap. Results indicate that
distribution-level alignment is a powerful and stable attack surface for
deployed CMs.

</details>


### [194] [Aligning Audio Captions with Human Preferences](https://arxiv.org/abs/2509.14659)
*Kartik Hegde,Rehana Mahfuz,Yinyi Guo,Erik Visser*

Main category: eess.AS

TL;DR: 提出基于RLHF的偏好对齐音频字幕框架，用CLAP奖励模型微调基线系统，经评估优于基线模型，性能与有真实数据的监督方法相当。


<details>
  <summary>Details</summary>
Motivation: 现有音频字幕系统依赖监督学习和配对数据集，成本高且难反映真实场景人类偏好。

Method: 训练基于CLAP的奖励模型，用人类标注的成对偏好数据；将奖励模型集成到强化学习框架微调基线字幕系统，不依赖真实字幕注释。

Result: 在多个数据集的人工评估中，该方法生成的字幕比基线模型更受青睐，尤其在基线模型表现不佳时；框架性能与有真实数据的监督方法相当。

Conclusion: 该框架能有效使音频字幕与人类偏好对齐，在真实场景有可扩展性。

Abstract: Current audio captioning systems rely heavily on supervised learning with
paired audio-caption datasets, which are expensive to curate and may not
reflect human preferences in real-world scenarios. To address this limitation,
we propose a preference-aligned audio captioning framework based on
Reinforcement Learning from Human Feedback (RLHF). To effectively capture
nuanced human preferences, we train a Contrastive Language-Audio Pretraining
(CLAP)-based reward model using human-labeled pairwise preference data. This
reward model is integrated into a reinforcement learning framework to fine-tune
any baseline captioning system without relying on ground-truth caption
annotations. Extensive human evaluations across multiple datasets show that our
method produces captions preferred over those from baseline models,
particularly in cases where the baseline models fail to provide correct and
natural captions. Furthermore, our framework achieves performance comparable to
supervised approaches with ground-truth data, demonstrating its effectiveness
in aligning audio captioning with human preferences and its scalability in
real-world scenarios.

</details>


### [195] [Listening, Imagining \& Refining: A Heuristic Optimized ASR Correction Framework with LLMs](https://arxiv.org/abs/2509.15095)
*Yutong Liu,Ziyue Zhang,Yongbin Yu,Xiangxiang Wang,Yuqing Cai,Nyima Tashi*

Main category: eess.AS

TL;DR: 提出基于大语言模型的启发式优化迭代校正框架LIR - ASR，实验表明其在转录准确性上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别（ASR）系统易出错影响下游应用，需要改进。

Method: 提出LIR - ASR框架，采用“倾听 - 想象 - 细化”策略，用有限状态机（FSM）进行启发式优化，并用基于规则的约束保持语义保真。

Result: 在英语和中文ASR输出实验中，LIR - ASR相比基线在字符错误率（CER）/词错误率（WER）上平均降低达1.5个百分点。

Conclusion: LIR - ASR在转录准确性上有显著提升。

Abstract: Automatic Speech Recognition (ASR) systems remain prone to errors that affect
downstream applications. In this paper, we propose LIR-ASR, a heuristic
optimized iterative correction framework using LLMs, inspired by human auditory
perception. LIR-ASR applies a "Listening-Imagining-Refining" strategy,
generating phonetic variants and refining them in context. A heuristic
optimization with finite state machine (FSM) is introduced to prevent the
correction process from being trapped in local optima and rule-based
constraints help maintain semantic fidelity. Experiments on both English and
Chinese ASR outputs show that LIR-ASR achieves average reductions in CER/WER of
up to 1.5 percentage points compared to baselines, demonstrating substantial
accuracy gains in transcription.

</details>


### [196] [Mitigating data replication in text-to-audio generative diffusion models through anti-memorization guidance](https://arxiv.org/abs/2509.14934)
*Francisco Messina,Francesca Ronchini,Luca Comanducci,Paolo Bestagini,Fabio Antonacci*

Main category: eess.AS

TL;DR: 本文探讨抗记忆策略解决文本到音频扩散模型的数据复制问题，采用AMG技术，实验表明其能显著减轻记忆问题且不影响音频质量。


<details>
  <summary>Details</summary>
Motivation: 解决生成式音频模型中数据复制问题，即模型在推理时意外生成训练数据部分内容。

Method: 采用Anti - Memorization Guidance (AMG)技术修改预训练扩散模型的采样过程，探索AMG内三种类型的引导，以Stable Audio Open为骨干模型。

Result: 综合实验分析表明AMG能显著减轻基于扩散的文本到音频生成中的记忆问题。

Conclusion: AMG在不损害音频保真度或语义对齐的情况下，有效减轻了文本到音频扩散模型的记忆问题。

Abstract: A persistent challenge in generative audio models is data replication, where
the model unintentionally generates parts of its training data during
inference. In this work, we address this issue in text-to-audio diffusion
models by exploring the use of anti-memorization strategies. We adopt
Anti-Memorization Guidance (AMG), a technique that modifies the sampling
process of pre-trained diffusion models to discourage memorization. Our study
explores three types of guidance within AMG, each designed to reduce
replication while preserving generation quality. We use Stable Audio Open as
our backbone, leveraging its fully open-source architecture and training
dataset. Our comprehensive experimental analysis suggests that AMG
significantly mitigates memorization in diffusion-based text-to-audio
generation without compromising audio fidelity or semantic alignment.

</details>


### [197] [BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings](https://arxiv.org/abs/2509.15001)
*Théo Charlot,Tarek Kunze,Maxime Poli,Alejandrina Cristia,Emmanuel Dupoux,Marvin Lavechin*

Main category: eess.AS

TL;DR: 介绍首个基于多语言儿童长时录音训练的自监督语音表征模型BabyHuBERT，在说话人分割任务评估中表现优于其他模型，可作为儿童语音研究基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于成人干净数据训练的语音模型在儿童早期语言发展研究中因声学和语言差异表现不佳。

Method: 基于13000小时横跨40多种语言多语言儿童长时录音训练自监督语音表征模型BabyHuBERT，并在说话人分割任务评估。

Result: BabyHuBERT在六个不同数据集上F1分数为52.1% - 74.4%，持续优于W2V2 - LL4300和标准HuBERT，在部分语料库有显著提升。

Conclusion: BabyHuBERT可作为儿童语音研究基础模型，支持在不同下游任务微调。

Abstract: Child-centered long-form recordings are essential for studying early language
development, but existing speech models trained on clean adult data perform
poorly due to acoustic and linguistic differences. We introduce BabyHuBERT, the
first self-supervised speech representation model trained on 13,000 hours of
multilingual child-centered long-form recordings spanning over 40 languages. We
evaluate BabyHuBERT on speaker segmentation, identifying when target children
speak versus female adults, male adults, or other children -- a fundamental
preprocessing step for analyzing naturalistic language experiences. BabyHuBERT
achieves F1-scores from 52.1% to 74.4% across six diverse datasets,
consistently outperforming W2V2-LL4300 (trained on English long-forms) and
standard HuBERT (trained on clean adult speech). Notable improvements include
13.2 absolute F1 points over HuBERT on Vanuatu and 15.9 points on Solomon
Islands corpora, demonstrating effectiveness on underrepresented languages. By
sharing code and models, BabyHuBERT serves as a foundation model for child
speech research, enabling fine-tuning on diverse downstream tasks.

</details>


### [198] [Real-Time Streaming Mel Vocoding with Generative Flow Matching](https://arxiv.org/abs/2509.15085)
*Simon Welker,Tal Peer,Timo Gerkmann*

Main category: eess.AS

TL;DR: 提出MelFlow流生成式Mel声码器，低延迟且有实时流能力，指标优于基线。


<details>
  <summary>Details</summary>
Motivation: Mel声码任务是TTS系统关键组件，需低延迟且性能好的声码器。

Method: 基于生成流匹配、DiffPhase和Mel滤波器组伪逆算子开发MelFlow。

Result: 在消费级笔记本GPU上实现低延迟实时流能力，PESQ和SI - SDR指标优于基线。

Conclusion: MelFlow是有效的低延迟流生成式Mel声码器。

Abstract: The task of Mel vocoding, i.e., the inversion of a Mel magnitude spectrogram
to an audio waveform, is still a key component in many text-to-speech (TTS)
systems today. Based on generative flow matching, our prior work on generative
STFT phase retrieval (DiffPhase), and the pseudoinverse operator of the Mel
filterbank, we develop MelFlow, a streaming-capable generative Mel vocoder for
speech sampled at 16 kHz with an algorithmic latency of only 32 ms and a total
latency of 48 ms. We show real-time streaming capability at this latency not
only in theory, but in practice on a consumer laptop GPU. Furthermore, we show
that our model achieves substantially better PESQ and SI-SDR values compared to
well-established not streaming-capable baselines for Mel vocoding including
HiFi-GAN.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [199] [Variational Gaussian Approximation in Replica Analysis of Parametric Models](https://arxiv.org/abs/2509.11780)
*Takashi Takahashi*

Main category: cond-mat.dis-nn

TL;DR: 本文重新审视参数模型中推理和学习的复制方法，用变分高斯近似处理数据生成分布未知或难处理的情况，分析线性回归并推导学习曲线。


<details>
  <summary>Details</summary>
Motivation: 处理数据生成分布未知或难处理的情况，分析参数模型中的推理和学习。

Method: 在巨正则形式下对复制系统使用变分高斯近似，用经验平均代替数据平均，通过平稳条件自适应确定试验哈密顿量的参数。

Result: 该方法阐明波动如何影响信息提取，与数理统计或学习理论结果直接关联，分析线性回归并推导学习曲线，包括处理现实数据集。

Conclusion: 该方法在处理复杂数据分布的参数模型推理和学习上有效。

Abstract: We revisit the replica method for analyzing inference and learning in
parametric models, considering situations where the data-generating
distribution is unknown or analytically intractable. Instead of assuming
idealized distributions to carry out quenched averages analytically, we use a
variational Gaussian approximation for the replicated system in grand canonical
formalism in which the data average can be deferred and replaced by empirical
averages, leading to stationarity conditions that adaptively determine the
parameters of the trial Hamiltonian for each dataset. This approach clarifies
how fluctuations affect information extraction and connects directly with the
results of mathematical statistics or learning theory such as information
criteria. As a concrete application, we analyze linear regression and derive
learning curves. This includes cases with real-world datasets, where exact
replica calculations are not feasible.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [200] [eIQ Neutron: Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations](https://arxiv.org/abs/2509.14388)
*Lennart Bamberg,Filippo Minnella,Roberto Bosio,Fabrizio Ottati,Yuebin Wang,Jongmin Lee,Luciano Lavagno,Adam Fuks*

Main category: cs.AR

TL;DR: 本文介绍集成于商用旗舰MPU的eIQ Neutron高效NPU及协同设计的编译器算法，相比其他产品有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有峰值每秒万亿次运算（TOPS）指标不能反映真实性能且与高硅成本相关，需在不牺牲灵活性的前提下最大化计算利用率。

Method: 采用灵活的数据驱动架构设计，编译器使用约束编程方法根据工作负载特性优化计算和数据移动。

Result: 在相同TOPS和内存资源下，相比领先的嵌入式NPU和编译器栈，平均加速1.8倍（峰值4倍）；对比计算和内存资源翻倍的NPUs，性能最高提升3.3倍。

Conclusion: eIQ Neutron高效NPU及其编译器算法能有效提高计算利用率，在AI推理性能上表现优异。

Abstract: Neural Processing Units (NPUs) are key to enabling efficient AI inference in
resource-constrained edge environments. While peak tera operations per second
(TOPS) is often used to gauge performance, it poorly reflects real-world
performance and typically rather correlates with higher silicon cost. To
address this, architects must focus on maximizing compute utilization, without
sacrificing flexibility. This paper presents the eIQ Neutron efficient-NPU,
integrated into a commercial flagship MPU, alongside co-designed compiler
algorithms. The architecture employs a flexible, data-driven design, while the
compiler uses a constrained programming approach to optimize compute and data
movement based on workload characteristics. Compared to the leading embedded
NPU and compiler stack, our solution achieves an average speedup of 1.8x (4x
peak) at equal TOPS and memory resources across standard AI-benchmarks. Even
against NPUs with double the compute and memory resources, Neutron delivers up
to 3.3x higher performance.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [201] [Bayesian inference for spatio-temporal hidden Markov models using the exchange algorithm](https://arxiv.org/abs/2509.15164)
*Daniele Tancini,Riccardo Rastelli,Francesco Bartolucci*

Main category: stat.ME

TL;DR: 提出时空隐马尔可夫模型，用近似交换算法进行贝叶斯推断，且开发新初始化方法，模拟研究显示其优于伪分布法，还应用于实际案例。


<details>
  <summary>Details</summary>
Motivation: 时空隐马尔可夫模型估计困难，传统以伪分布替代潜在分布可能影响结果。

Method: 提出潜在过程为自逻辑模型扩展的时空隐马尔可夫模型，在贝叶斯框架下用近似交换算法进行推断，开发新初始化方法。

Result: 近似交换算法得到针对正确后验分布的马尔可夫链蒙特卡罗采样器，减少计算时间，模拟研究表明其一般优于伪分布方法，参数估计更准确。

Conclusion: 所提方法有效，可用于实际案例分析，如意大利地区降雨水平分析。

Abstract: Spatio-temporal hidden Markov models are extremely difficult to estimate
because their latent joint distributions are available only in trivial cases.
In the estimation phase, these latent distributions are usually substituted
with pseudo-distributions, which could affect the estimation results, in
particular in the presence of strong dependence between the latent variables.
In this work, we propose a spatio-temporal hidden Markov model where the latent
process is an extension of the autologistic model. We show how inference can be
carried out in a Bayesian framework using an approximate exchange algorithm,
which circumvents the impractical calculations of the normalizing constants
that arise in the model. Our proposed method leads to a Markov chain Monte
Carlo sampler that targets the correct posterior distribution of the model and
not a pseudo-posterior. In addition, we develop a new initialization approach
for the approximate exchange method, reducing the computational time of the
algorithm. An extensive simulation study shows that the approximate exchange
algorithm generally outperforms the pseudo-distribution approach, yielding more
accurate parameter estimates. Finally, the proposed methodology is applied to a
real-world case study analyzing rainfall levels across Italian regions over
time.

</details>


### [202] [Rate doubly robust estimation for weighted average treatment effects](https://arxiv.org/abs/2509.14502)
*Yiming Wang,Yi Liu,Shu Yang*

Main category: stat.ME

TL;DR: 本文研究加权平均处理效应（WATE）的稳健性和效率条件，提出三种率双重稳健（RDR）估计量并验证其在医学和社会科学中的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏对WATE估计的稳健性和效率条件的系统研究，且双重稳健（DR）估计量对其他WATEs的适用性不确定。

Method: 运用半参数有效影响函数（EIF）理论和双重/去偏机器学习（DML），在特定速率和正则条件下提出三种RDR估计量，并通过蒙特卡罗模拟评估性能。

Result: 提出的估计量通过蒙特卡罗模拟得到评估，且在NHANES和SIPP数据应用中展示了实用性。

Conclusion: 所提方法在医学和社会科学中有实际应用价值。

Abstract: The weighted average treatment effect (WATE) defines a versatile class of
causal estimands for populations characterized by propensity score weights,
including the average treatment effect (ATE), treatment effect on the treated
(ATT), on controls (ATC), and for the overlap population (ATO). WATE has broad
applicability in social and medical research, as many datasets from these
fields align with its framework. However, the literature lacks a systematic
investigation into the robustness and efficiency conditions for WATE
estimation. Although doubly robust (DR) estimators are well-studied for ATE,
their applicability to other WATEs remains uncertain. This paper investigates
whether widely used WATEs admit DR or rate doubly robust (RDR) estimators and
assesses the role of nuisance function accuracy, particularly with machine
learning. Using semiparametric efficient influence function (EIF) theory and
double/debiased machine learning (DML), we propose three RDR estimators under
specific rate and regularity conditions and evaluate their performance via
Monte Carlo simulations. Applications to NHANES data on smoking and blood lead
levels, and SIPP data on 401(k) eligibility, demonstrate the methods' practical
relevance in medical and social sciences.

</details>


### [203] [Semiparametric Learning from Open-Set Label Shift Data](https://arxiv.org/abs/2509.14522)
*Siyan Liu,Yukun Liu,Qinglong Tian,Pengfei Li,Jing Qin*

Main category: stat.ME

TL;DR: 研究开集标签转移问题，提出半参数密度比模型框架，方法经模拟和实际数据验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有开集标签转移问题的解决方法存在依赖限制条件、先验知识、计算不可行或缺乏理论保证等问题。

Method: 提出半参数密度比模型框架，开发最大经验似然估计和置信区间，设计EM算法，构建基于后验概率的近似最优分类器。

Result: 模拟和实际数据应用表明，所提方法在估计准确性和分类性能上优于现有方法。

Conclusion: 所提半参数密度比模型框架有效，能解决开集标签转移问题并提升性能。

Abstract: We study the open-set label shift problem, where the test data may include a
novel class absent from training. This setting is challenging because both the
class proportions and the distribution of the novel class are not identifiable
without extra assumptions. Existing approaches often rely on restrictive
separability conditions, prior knowledge, or computationally infeasible
procedures, and some may lack theoretical guarantees. We propose a
semiparametric density ratio model framework that ensures identifiability while
allowing overlap between novel and known classes. Within this framework, we
develop maximum empirical likelihood estimators and confidence intervals for
class proportions, establish their asymptotic validity, and design a stable
Expectation-Maximization algorithm for computation. We further construct an
approximately optimal classifier based on posterior probabilities with
theoretical guarantees. Simulations and a real data application confirm that
our methods improve both estimation accuracy and classification performance
compared with existing approaches.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [204] [Music4All A+A: A Multimodal Dataset for Music Information Retrieval Tasks](https://arxiv.org/abs/2509.14891)
*Jonas Geiger,Marta Moscati,Shah Nawaz,Markus Schedl*

Main category: cs.MM

TL;DR: 提出Music4All A+A数据集用于多模态音乐信息检索任务，进行相关实验并公开代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 多数多模态音乐信息检索数据集仅提供单首音乐数据，忽略音乐不同粒度描述，需填补此空白。

Method: 基于Music4All - Onion数据集构建Music4All A+A数据集，开展艺术家和专辑多模态流派分类实验。

Result: 图像对艺术家和专辑流派分类更具信息价值，多种多模态流派分类模型跨领域泛化能力差。

Conclusion: Music4All A+A数据集适用于多种粒度的音乐信息检索任务。

Abstract: Music is characterized by aspects related to different modalities, such as
the audio signal, the lyrics, or the music video clips. This has motivated the
development of multimodal datasets and methods for Music Information Retrieval
(MIR) tasks such as genre classification or autotagging. Music can be described
at different levels of granularity, for instance defining genres at the level
of artists or music albums. However, most datasets for multimodal MIR neglect
this aspect and provide data at the level of individual music tracks. We aim to
fill this gap by providing Music4All Artist and Album (Music4All A+A), a
dataset for multimodal MIR tasks based on music artists and albums. Music4All
A+A is built on top of the Music4All-Onion dataset, an existing track-level
dataset for MIR tasks. Music4All A+A provides metadata, genre labels, image
representations, and textual descriptors for 6,741 artists and 19,511 albums.
Furthermore, since Music4All A+A is built on top of Music4All-Onion, it allows
access to other multimodal data at the track level, including user--item
interaction data. This renders Music4All A+A suitable for a broad range of MIR
tasks, including multimodal music recommendation, at several levels of
granularity. To showcase the use of Music4All A+A, we carry out experiments on
multimodal genre classification of artists and albums, including an analysis in
missing-modality scenarios, and a quantitative comparison with genre
classification in the movie domain. Our experiments show that images are more
informative for classifying the genres of artists and albums, and that several
multimodal models for genre classification struggle in generalizing across
domains. We provide the code to reproduce our experiments at
https://github.com/hcai-mms/Music4All-A-A, the dataset is linked in the
repository and provided open-source under a CC BY-NC-SA 4.0 license.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [205] [Physics-Informed GCN-LSTM Framework for Long-Term Forecasting of 2D and 3D Microstructure Evolution](https://arxiv.org/abs/2509.15029)
*Hamidreza Razavi,Nele Moelans*

Main category: cond-mat.mtrl-sci

TL;DR: 提出集成GCN与LSTM的物理信息框架，用于2D和3D微观结构长期演化预测，性能出色。


<details>
  <summary>Details</summary>
Motivation: 实现2D和3D微观结构长期演化的有效预测。

Method: 构建集成GCN与LSTM的物理信息框架，在潜在图空间操作，用卷积自动编码器压缩编码相场模拟数据。

Result: 框架能捕捉微观结构时空模式，实现低计算成本的长期预测。

Conclusion: 该框架在不同指标下表现出色，能高效建模微观结构演化。

Abstract: This paper presents a physics-informed framework that integrates graph
convolutional networks (GCN) with long short-term memory (LSTM) architecture to
forecast microstructure evolution over long time horizons in both 2D and 3D
with remarkable performance across varied metrics. The proposed framework is
composition-aware, trained jointly on datasets with different compositions, and
operates in latent graph space, which enables the model to capture compositions
and morphological dynamics while remaining computationally efficient.
Compressing and encoding phase-field simulation data with convolutional
autoencoders and operating in Latent graph space facilitates efficient modeling
of microstructural evolution across composition, dimensions, and long-term
horizons. The framework captures the spatial and temporal patterns of evolving
microstructures while enabling long-range forecasting at reduced computational
cost after training.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [206] [The mechanization of science illustrated by the Lean formalization of the multi-graded Proj construction](https://arxiv.org/abs/2509.15116)
*Arnaud Mayeux,Jujian Zhang*

Main category: cs.LO

TL;DR: 在Lean4中形式化多级Proj构造，展示机械化数学和形式化。


<details>
  <summary>Details</summary>
Motivation: 展示机械化数学和形式化。

Method: 在Lean4中进行形式化。

Result: 完成多级Proj构造的形式化。

Conclusion: 可在Lean4中实现多级Proj构造的形式化以展示相关内容。

Abstract: We formalize the multi-graded Proj construction in Lean4, illustrating
mechanized mathematics and formalization.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [207] [Friend or Foe: Delegating to an AI Whose Alignment is Unknown](https://arxiv.org/abs/2509.14396)
*Drew Fudenberg,Annie Liang*

Main category: econ.TH

TL;DR: 研究治疗决策中向AI披露患者属性问题，提出应根据AI可靠性选属性，最优披露能识别高治疗需求罕见群体的属性。


<details>
  <summary>Details</summary>
Motivation: AI系统虽能改善决策，但存在与决策者目标不一致风险，研究治疗决策中如何向AI披露患者属性以平衡利弊。

Method: 分析在不同AI可靠性信念下，如何选择患者属性以平衡信息披露的利弊。

Result: 发现设计师应最优披露能识别高治疗需求罕见群体的属性，并合并其余患者。

Conclusion: 揭示了在治疗决策场景中，设计师应依据对AI可靠性的判断，合理选择向AI披露的患者属性。

Abstract: AI systems have the potential to improve decision-making, but decision makers
face the risk that the AI may be misaligned with their objectives. We study
this problem in the context of a treatment decision, where a designer decides
which patient attributes to reveal to an AI before receiving a prediction of
the patient's need for treatment. Providing the AI with more information
increases the benefits of an aligned AI but also amplifies the harm from a
misaligned one. We characterize how the designer should select attributes to
balance these competing forces, depending on their beliefs about the AI's
reliability. We show that the designer should optimally disclose attributes
that identify \emph{rare} segments of the population in which the need for
treatment is high, and pool the remaining patients.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [208] [Leveraging Artificial Intelligence as a Strategic Growth Catalyst for Small and Medium-sized Enterprises](https://arxiv.org/abs/2509.14532)
*Oluwatosin Agbaakin*

Main category: cs.CY

TL;DR: 本文指出AI已成为中小企业发展关键，报告为其提供AI应用框架，还给出采用AI的量化证据及市场前景，最后阐述核心概念、商业案例、应用及策略。


<details>
  <summary>Details</summary>
Motivation: AI已成为中小企业竞争力、运营效率和长期生存的关键，帮助中小企业领导者应对技术变革。

Method: 提出全面框架，结合市场数据给出商业案例，阐述核心概念、实用应用和分阶段的可操作采用策略。

Result: 91%使用AI的中小企业称其直接提高了收入，AI可降低达30%运营成本，每月为企业节省超20小时时间，全球AI市场预计大幅增长。

Conclusion: 为中小企业领导者提供全面框架和指导，助力其利用AI实现发展。

Abstract: Artificial Intelligence (AI) has transitioned from a futuristic concept
reserved for large corporations to a present-day, accessible, and essential
growth lever for Small and Medium-sized Enterprises (SMEs). For entrepreneurs
and business leaders, strategic AI adoption is no longer an option but an
imperative for competitiveness, operational efficiency, and long-term survival.
This report provides a comprehensive framework for SME leaders to navigate this
technological shift, offering the foundational knowledge, business case,
practical applications, and strategic guidance necessary to harness the power
of AI. The quantitative evidence supporting AI adoption is compelling; 91% of
SMEs using AI report that it directly boosts their revenue. Beyond top-line
growth, AI drives profound operational efficiencies, with studies showing it
can reduce operational costs by up to 30% and save businesses more than 20
hours of valuable time each month. This transformation is occurring within the
context of a seismic economic shift; the global AI market is projected to surge
from $233.46 Billion in 2024 to an astonishing $1.77 Trillion by 2032. This
paper demystifies the core concepts of AI, presents a business case based on
market data, details practical applications, and lays out a phased, actionable
adoption strategy.

</details>


### [209] [OnlineMate: An LLM-Based Multi-Agent Companion System for Cognitive Support in Online Learning](https://arxiv.org/abs/2509.14803)
*Xian Gao,Zongyun Zhang,Ting Liu,Yuzhuo Fu*

Main category: cs.CY

TL;DR: 提出基于大语言模型且集成心智理论的多智能体学习伙伴系统OnlineMate，实验表明其能促进在线学习中的深度学习和讨论，增强认知参与度。


<details>
  <summary>Details</summary>
Motivation: 在线学习环境中缺乏个性化同伴互动，现有大语言模型模拟的互动有限，学生参与度低且难获启发。

Method: 提出集成心智理论（ToM）的多智能体学习伙伴系统OnlineMate，模拟同伴角色，适应学习者认知状态，推断心理状态并动态调整互动策略。

Result: 在模拟学习场景的实验中，OnlineMate有效促进了深度学习和讨论，增强了在线教育环境中的认知参与度。

Conclusion: OnlineMate能在在线教育环境中支持高阶思维和认知发展，促进深度学习和讨论。

Abstract: In online learning environments, students often lack personalized peer
interactions, which play a crucial role in supporting cognitive development and
learning engagement. Although previous studies have utilized large language
models (LLMs) to simulate interactive dynamic learning environments for
students, these interactions remain limited to conversational exchanges,
lacking insights and adaptations to the learners' individualized learning and
cognitive states. As a result, students' interest in discussions with AI
learning companions is low, and they struggle to gain inspiration from such
interactions. To address this challenge, we propose OnlineMate, a multi-agent
learning companion system driven by LLMs that integrates the Theory of Mind
(ToM). OnlineMate is capable of simulating peer-like agent roles, adapting to
learners' cognitive states during collaborative discussions, and inferring
their psychological states, such as misunderstandings, confusion, or
motivation. By incorporating Theory of Mind capabilities, the system can
dynamically adjust its interaction strategies to support the development of
higher-order thinking and cognition. Experimental results in simulated learning
scenarios demonstrate that OnlineMate effectively fosters deep learning and
discussions while enhancing cognitive engagement in online educational
settings.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [210] [Deploying UDM Series in Real-Life Stuttered Speech Applications: A Clinical Evaluation Framework](https://arxiv.org/abs/2509.14304)
*Eric Zhang,Li Wei,Sarah Chen,Michael Wang*

Main category: cs.SD

TL;DR: 本文研究无约束口吃建模（UDM）系列，实验表明其在实现高准确率同时有临床可解释性，部署研究显示临床接受率高且诊断时间减少，为临床环境下的人工智能辅助言语治疗提供实用途径。


<details>
  <summary>Details</summary>
Motivation: 传统口吃和言语不流畅检测系统存在准确率和临床可解释性的权衡，端到端深度学习模型黑盒性质限制临床应用。

Method: 对UDM系列进行涉及患者和认证言语病理学家的广泛实验及部署研究。

Result: UDM实现了最先进的性能（F1：0.89±0.04），提供临床有意义的可解释性得分（4.2/5.0），临床接受率87%，诊断时间减少34%。

Conclusion: UDM为临床环境下的人工智能辅助言语治疗提供了实用途径。

Abstract: Stuttered and dysfluent speech detection systems have traditionally suffered
from the trade-off between accuracy and clinical interpretability. While
end-to-end deep learning models achieve high performance, their black-box
nature limits clinical adoption. This paper looks at the Unconstrained
Dysfluency Modeling (UDM) series-the current state-of-the-art framework
developed by Berkeley that combines modular architecture, explicit phoneme
alignment, and interpretable outputs for real-world clinical deployment.
Through extensive experiments involving patients and certified speech-language
pathologists (SLPs), we demonstrate that UDM achieves state-of-the-art
performance (F1: 0.89+-0.04) while providing clinically meaningful
interpretability scores (4.2/5.0). Our deployment study shows 87% clinician
acceptance rate and 34% reduction in diagnostic time. The results provide
strong evidence that UDM represents a practical pathway toward AI-assisted
speech therapy in clinical environments.

</details>


### [211] [Spatial Audio Motion Understanding and Reasoning](https://arxiv.org/abs/2509.14666)
*Arvind Krishna Sridhar,Yinyi Guo,Erik Visser*

Main category: cs.SD

TL;DR: 本文聚焦含移动声源的空间音频理解，提出空间音频编码器，结合音频接地模型，利用大语言模型回答复杂查询，并引入基准数据集进行性能验证。


<details>
  <summary>Details</summary>
Motivation: 实现机器通过理解事件及其空间属性来解释听觉场景，重点关注含移动声源的空间音频理解。

Method: 1. 引入空间音频编码器处理空间音频，检测多重叠事件并估计其空间属性；2. 结合音频接地模型，通过交叉注意力机制将音频特征与语义音频类文本嵌入对齐；3. 利用提取的结构化空间属性来调节大语言模型以回答复杂查询；4. 引入空间音频运动理解和推理基准数据集。

Result: 在基准数据集上展示了框架相对于基线模型的性能。

Conclusion: 所提出的框架在空间音频理解和推理方面具有有效性。

Abstract: Spatial audio reasoning enables machines to interpret auditory scenes by
understanding events and their spatial attributes. In this work, we focus on
spatial audio understanding with an emphasis on reasoning about moving sources.
First, we introduce a spatial audio encoder that processes spatial audio to
detect multiple overlapping events and estimate their spatial attributes,
Direction of Arrival (DoA) and source distance, at the frame level. To
generalize to unseen events, we incorporate an audio grounding model that
aligns audio features with semantic audio class text embeddings via a
cross-attention mechanism. Second, to answer complex queries about dynamic
audio scenes involving moving sources, we condition a large language model
(LLM) on structured spatial attributes extracted by our model. Finally, we
introduce a spatial audio motion understanding and reasoning benchmark dataset
and demonstrate our framework's performance against the baseline model.

</details>


### [212] [MeanFlowSE: one-step generative speech enhancement via conditional mean flow](https://arxiv.org/abs/2509.14858)
*Duojia Li,Shenghui Lu,Hongchen Pan,Zongyi Zhan,Qingyang Hong,Lin Li*

Main category: cs.SD

TL;DR: 提出MeanFlowSE模型用于实时生成式语音增强，可单步生成，在VoiceBank - DEMAND上表现好且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 多步推理是实时生成式语音增强的瓶颈，基于流和扩散的系统依赖迭代常微分方程求解器。

Method: 引入MeanFlowSE条件生成模型，学习轨迹上有限区间的平均速度，用雅可比向量积实例化MeanFlow恒等式，推导局部训练目标，推理时可单步生成或可选少步细化。

Result: 单步模型在VoiceBank - DEMAND上实现了强可懂度、保真度和感知质量，计算成本比多步基线低。

Conclusion: 该方法无需知识蒸馏或外部教师，为实时生成式语音增强提供了高效、高保真的框架。

Abstract: Multistep inference is a bottleneck for real-time generative speech
enhancement because flow- and diffusion-based systems learn an instantaneous
velocity field and therefore rely on iterative ordinary differential equation
(ODE) solvers. We introduce MeanFlowSE, a conditional generative model that
learns the average velocity over finite intervals along a trajectory. Using a
Jacobian-vector product (JVP) to instantiate the MeanFlow identity, we derive a
local training objective that directly supervises finite-interval displacement
while remaining consistent with the instantaneous-field constraint on the
diagonal. At inference, MeanFlowSE performs single-step generation via a
backward-in-time displacement, removing the need for multistep solvers; an
optional few-step variant offers additional refinement. On VoiceBank-DEMAND,
the single-step model achieves strong intelligibility, fidelity, and perceptual
quality with substantially lower computational cost than multistep baselines.
The method requires no knowledge distillation or external teachers, providing
an efficient, high-fidelity framework for real-time generative speech
enhancement.

</details>


### [213] [Back to Ear: Perceptually Driven High Fidelity Music Reconstruction](https://arxiv.org/abs/2509.14912)
*Kangdi Wang,Zhiyue Wu,Dinghao Zhou,Rui Lin,Junyu Dai,Tao Jiang*

Main category: cs.SD

TL;DR: 提出开源音乐信号重建模型εar - VAE，优化VAE训练范式，实验表明其性能优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有开源VAE模型在训练时忽略听觉感知方面，导致相位精度和立体声空间表示存在不足。

Method: 1. 在损失计算前应用K加权感知滤波器；2. 提出两种新的相位损失；3. 采用新的频谱监督范式。

Result: εar - VAE在44.1kHz下，在多种指标上大幅优于领先的开源模型，在重建高频谐波和空间特征方面表现出色。

Conclusion: εar - VAE通过优化训练范式，有效解决现有开源模型的问题。

Abstract: Variational Autoencoders (VAEs) are essential for large-scale audio tasks
like diffusion-based generation. However, existing open-source models often
neglect auditory perceptual aspects during training, leading to weaknesses in
phase accuracy and stereophonic spatial representation. To address these
challenges, we propose {\epsilon}ar-VAE, an open-source music signal
reconstruction model that rethinks and optimizes the VAE training paradigm. Our
contributions are threefold: (i) A K-weighting perceptual filter applied prior
to loss calculation to align the objective with auditory perception. (ii) Two
novel phase losses: a Correlation Loss for stereo coherence, and a Phase Loss
using its derivatives--Instantaneous Frequency and Group Delay--for precision.
(iii) A new spectral supervision paradigm where magnitude is supervised by all
four Mid/Side/Left/Right components, while phase is supervised only by the LR
components. Experiments show {\epsilon}ar-VAE at 44.1kHz substantially
outperforms leading open-source models across diverse metrics, showing
particular strength in reconstructing high-frequency harmonics and the spatial
characteristics.

</details>


### [214] [Estimating Respiratory Effort from Nocturnal Breathing Sounds for Obstructive Sleep Apnoea Screening](https://arxiv.org/abs/2509.14944)
*Xiaolei Xu,Chaoyue Niu,Guy J. Brown,Hector Romero,Ning Ma*

Main category: cs.SD

TL;DR: 本文首次从夜间音频直接估计呼吸努力，提出潜在空间融合框架用于阻塞性睡眠呼吸暂停（OSA）检测，仅需智能手机音频，实现无传感器、可扩展和纵向的OSA监测。


<details>
  <summary>Details</summary>
Motivation: OSA患者多未诊断，基于声学的筛查受环境噪声和缺乏生理背景限制，现有呼吸努力检测方法需额外接触传感器，降低可扩展性和患者舒适度。

Method: 提出潜在空间融合框架，将估计的呼吸努力嵌入与声学特征集成用于OSA检测。

Result: 呼吸努力估计器一致性相关系数达0.48，融合努力和音频在低呼吸暂停 - 低通气指数阈值下提高了灵敏度和AUC。

Conclusion: 该方法仅需智能手机音频，可实现无传感器、可扩展和纵向的OSA监测。

Abstract: Obstructive sleep apnoea (OSA) is a prevalent condition with significant
health consequences, yet many patients remain undiagnosed due to the complexity
and cost of over-night polysomnography. Acoustic-based screening provides a
scalable alternative, yet performance is limited by environmental noise and the
lack of physiological context. Respiratory effort is a key signal used in
clinical scoring of OSA events, but current approaches require additional
contact sensors that reduce scalability and patient comfort. This paper
presents the first study to estimate respiratory effort directly from nocturnal
audio, enabling physiological context to be recovered from sound alone. We
propose a latent-space fusion framework that integrates the estimated effort
embeddings with acoustic features for OSA detection. Using a dataset of 157
nights from 103 participants recorded in home environments, our respiratory
effort estimator achieves a concordance correlation coefficient of 0.48,
capturing meaningful respiratory dynamics. Fusing effort and audio improves
sensitivity and AUC over audio-only baselines, especially at low
apnoea-hypopnoea index thresholds. The proposed approach requires only
smartphone audio at test time, which enables sensor-free, scalable, and
longitudinal OSA monitoring.

</details>


### [215] [Exploring How Audio Effects Alter Emotion with Foundation Models](https://arxiv.org/abs/2509.15151)
*Stelios Katsis,Vassilis Lyberatos,Spyridon Kantarelis,Edmund Dervakos,Giorgos Stamou*

Main category: cs.SD

TL;DR: 研究利用基础模型分析音频效果对情感的影响，应用探测方法研究音频效果与估计情感的关系，旨在增进对音频制作实践感知影响的理解。


<details>
  <summary>Details</summary>
Motivation: 先前研究未充分探索音频效果对情感的系统影响，本文旨在利用基础模型分析这些影响。

Method: 对深度学习模型的嵌入应用各种探测方法，研究音频效果与估计情感的复杂非线性关系。

Result: 未明确提及具体结果，仅表示会揭示与特定效果相关的模式并评估基础音频模型的鲁棒性。

Conclusion: 研究成果将促进对音频制作实践感知影响的理解，对音乐认知、表演和情感计算有启示。

Abstract: Audio effects (FX) such as reverberation, distortion, modulation, and dynamic
range processing play a pivotal role in shaping emotional responses during
music listening. While prior studies have examined links between low-level
audio features and affective perception, the systematic impact of audio FX on
emotion remains underexplored. This work investigates how foundation models -
large-scale neural architectures pretrained on multimodal data - can be
leveraged to analyze these effects. Such models encode rich associations
between musical structure, timbre, and affective meaning, offering a powerful
framework for probing the emotional consequences of sound design techniques. By
applying various probing methods to embeddings from deep learning models, we
examine the complex, nonlinear relationships between audio FX and estimated
emotion, uncovering patterns tied to specific effects and evaluating the
robustness of foundation audio models. Our findings aim to advance
understanding of the perceptual impact of audio production practices, with
implications for music cognition, performance, and affective computing.

</details>


### [216] [Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generation](https://arxiv.org/abs/2509.15210)
*Chen Si,Qianyi Wu,Chaitanya Amballa,Romit Roy Choudhury*

Main category: cs.SD

TL;DR: 提出Mesh - infused Neural Acoustic Field (MiNAF)方法用于更准确的房间脉冲响应（RIR）预测，在声音模拟上表现良好且在有限训练样本数据集有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有应用神经隐式方法学习RIR的研究未有效利用环境的显式几何信息，为进一步挖掘带直接几何特征的神经隐式模型潜力。

Method: 提出MiNAF，在给定位置查询粗糙房间网格并提取距离分布作为局部上下文的显式表示。

Result: MiNAF在各种评估指标上表现有竞争力，在有限训练样本数据集验证了鲁棒性。

Conclusion: 将显式局部几何特征纳入可更好指导神经网络生成更准确RIR预测，推动了高保真声音模拟的发展。

Abstract: Realistic sound simulation plays a critical role in many applications. A key
element in sound simulation is the room impulse response (RIR), which
characterizes how sound propagates from a source to a listener within a given
space. Recent studies have applied neural implicit methods to learn RIR using
context information collected from the environment, such as scene images.
However, these approaches do not effectively leverage explicit geometric
information from the environment. To further exploit the potential of neural
implicit models with direct geometric features, we present Mesh-infused Neural
Acoustic Field (MiNAF), which queries a rough room mesh at given locations and
extracts distance distributions as an explicit representation of local context.
Our approach demonstrates that incorporating explicit local geometric features
can better guide the neural network in generating more accurate RIR
predictions. Through comparisons with conventional and state-of-the-art
baseline methods, we show that MiNAF performs competitively across various
evaluation metrics. Furthermore, we verify the robustness of MiNAF in datasets
with limited training samples, demonstrating an advance in high-fidelity sound
simulation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [217] [FlowDrive: Energy Flow Field for End-to-End Autonomous Driving](https://arxiv.org/abs/2509.14303)
*Hao Jiang,Zhipeng Zhang,Yu Gao,Zhigang Sun,Yiru Wang,Yuwen Heng,Shuo Wang,Jinhao Chai,Zhuo Chen,Hao Zhao,Hao Sun,Xi Zhang,Anqing Jiang,Chuan Hu*

Main category: cs.RO

TL;DR: 提出FlowDrive框架，引入可物理解释的基于能量的流场来编码语义先验和安全提示，通过条件扩散规划器解耦运动意图预测和轨迹去噪，在NAVSIM v2基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶框架缺乏对风险和引导先验的显式建模，难以实现安全且可解释的规划。

Method: 提出FlowDrive框架，引入风险势能和车道吸引场等可物理解释的流场，将语义先验和安全提示编码到BEV空间；通过条件扩散规划器解耦运动意图预测和轨迹去噪。

Result: 在NAVSIM v2基准测试中取得了86.3的EPDMS，在安全性和规划质量上超越了先前的基线。

Conclusion: FlowDrive框架有效，可实现更安全、可解释的自动驾驶规划。

Abstract: Recent advances in end-to-end autonomous driving leverage multi-view images
to construct BEV representations for motion planning. In motion planning,
autonomous vehicles need considering both hard constraints imposed by
geometrically occupied obstacles (e.g., vehicles, pedestrians) and soft,
rule-based semantics with no explicit geometry (e.g., lane boundaries, traffic
priors). However, existing end-to-end frameworks typically rely on BEV features
learned in an implicit manner, lacking explicit modeling of risk and guidance
priors for safe and interpretable planning. To address this, we propose
FlowDrive, a novel framework that introduces physically interpretable
energy-based flow fields-including risk potential and lane attraction fields-to
encode semantic priors and safety cues into the BEV space. These flow-aware
features enable adaptive refinement of anchor trajectories and serve as
interpretable guidance for trajectory generation. Moreover, FlowDrive decouples
motion intent prediction from trajectory denoising via a conditional diffusion
planner with feature-level gating, alleviating task interference and enhancing
multimodal diversity. Experiments on the NAVSIM v2 benchmark demonstrate that
FlowDrive achieves state-of-the-art performance with an EPDMS of 86.3,
surpassing prior baselines in both safety and planning quality. The project is
available at https://astrixdrive.github.io/FlowDrive.github.io/.

</details>


### [218] [DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion](https://arxiv.org/abs/2509.14353)
*Dvij Kalaria,Sudarshan S Harithas,Pushkal Katara,Sangkyung Kwak,Sarthak Bhagat,Shankar Sastry,Srinath Sridhar,Sai Vemprala,Ashish Kapoor,Jonathan Chung-Kuan Huang*

Main category: cs.RO

TL;DR: 介绍DreamControl方法用于学习类人机器人全身自主技能，结合扩散模型和强化学习，在机器人上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 提出新方法来学习类人机器人全身自主技能，解决直接强化学习难以完成特定任务的问题。

Method: 利用在人类运动数据上训练的扩散先验引导强化学习策略在模拟中完成特定任务。

Result: 该人类运动先验使强化学习能找到直接强化学习无法得到的解决方案，扩散模型有助于实现自然动作和从模拟到现实的迁移，在Unitree G1机器人上验证了有效性。

Conclusion: DreamControl方法有效，能用于类人机器人完成复杂任务。

Abstract: We introduce DreamControl, a novel methodology for learning autonomous
whole-body humanoid skills. DreamControl leverages the strengths of diffusion
models and Reinforcement Learning (RL): our core innovation is the use of a
diffusion prior trained on human motion data, which subsequently guides an RL
policy in simulation to complete specific tasks of interest (e.g., opening a
drawer or picking up an object). We demonstrate that this human motion-informed
prior allows RL to discover solutions unattainable by direct RL, and that
diffusion models inherently promote natural looking motions, aiding in
sim-to-real transfer. We validate DreamControl's effectiveness on a Unitree G1
robot across a diverse set of challenging tasks involving simultaneous lower
and upper body control and object interaction.

</details>


### [219] [M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation](https://arxiv.org/abs/2509.14980)
*Ju Dong,Lei Zhang,Liding Zhang,Yao Ling,Yu Fu,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang*

Main category: cs.RO

TL;DR: 提出M4Diffuser框架用于移动操作，结合多视图扩散策略和ReM - QP控制器，实验表明其成功率更高、碰撞更少，能实现鲁棒操作。


<details>
  <summary>Details</summary>
Motivation: 现有单视图方法在非结构化环境中因视野等能力有限易失败，经典控制器在奇点附近效率和可操作性差。

Method: 提出M4Diffuser混合框架，扩散策略结合本体感受状态和多视角图像生成目标，ReM - QP控制器执行目标，消除松弛变量并考虑可操作性偏好。

Result: 在仿真和现实环境实验中，M4Diffuser比基线成功率高7 - 56%，碰撞减少3 - 31%。

Conclusion: M4Diffuser能实现平滑的全身协调，对未知任务有强泛化性，为非结构化环境的可靠移动操作奠定基础。

Abstract: Mobile manipulation requires the coordinated control of a mobile base and a
robotic arm while simultaneously perceiving both global scene context and
fine-grained object details. Existing single-view approaches often fail in
unstructured environments due to limited fields of view, exploration, and
generalization abilities. Moreover, classical controllers, although stable,
struggle with efficiency and manipulability near singularities. To address
these challenges, we propose M4Diffuser, a hybrid framework that integrates a
Multi-View Diffusion Policy with a novel Reduced and Manipulability-aware QP
(ReM-QP) controller for mobile manipulation. The diffusion policy leverages
proprioceptive states and complementary camera perspectives with both
close-range object details and global scene context to generate task-relevant
end-effector goals in the world frame. These high-level goals are then executed
by the ReM-QP controller, which eliminates slack variables for computational
efficiency and incorporates manipulability-aware preferences for robustness
near singularities. Comprehensive experiments in simulation and real-world
environments show that M4Diffuser achieves 7 to 56 percent higher success rates
and reduces collisions by 3 to 31 percent over baselines. Our approach
demonstrates robust performance for smooth whole-body coordination, and strong
generalization to unseen tasks, paving the way for reliable mobile manipulation
in unstructured environments. Details of the demo and supplemental material are
available on our project website https://sites.google.com/view/m4diffuser.

</details>


### [220] [The Role of Touch: Towards Optimal Tactile Sensing Distribution in Anthropomorphic Hands for Dexterous In-Hand Manipulation](https://arxiv.org/abs/2509.14984)
*João Damião Almeida,Egidio Falotico,Cecilia Laschi,José Santos-Victor*

Main category: cs.RO

TL;DR: 研究手部不同区域触觉反馈对物体重定向任务的影响，确定有效触觉配置，为拟人末端执行器设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 手部操作依赖分布式触觉传感，但传感器最优配置复杂，且常忽略手部其他区域触觉信息。

Method: 分析手部不同部位感官反馈对深度强化学习控制策略鲁棒性的影响，研究物体特征与最优传感器放置的关系。

Result: 确定了有助于提高操作效率和准确性的触觉传感配置。

Conclusion: 研究结果为具有增强操作能力的拟人末端执行器的设计和使用提供了有价值的见解。

Abstract: In-hand manipulation tasks, particularly in human-inspired robotic systems,
must rely on distributed tactile sensing to achieve precise control across a
wide variety of tasks. However, the optimal configuration of this network of
sensors is a complex problem, and while the fingertips are a common choice for
placing sensors, the contribution of tactile information from other regions of
the hand is often overlooked. This work investigates the impact of tactile
feedback from various regions of the fingers and palm in performing in-hand
object reorientation tasks. We analyze how sensory feedback from different
parts of the hand influences the robustness of deep reinforcement learning
control policies and investigate the relationship between object
characteristics and optimal sensor placement. We identify which tactile sensing
configurations contribute to improving the efficiency and accuracy of
manipulation. Our results provide valuable insights for the design and use of
anthropomorphic end-effectors with enhanced manipulation capabilities.

</details>


### [221] [Designing Latent Safety Filters using Pre-Trained Vision Models](https://arxiv.org/abs/2509.14758)
*Ihab Tabbara,Yuxuan Yang,Ahmad Hamzeh,Maxwell Astafyev,Hussein Sibai*

Main category: cs.RO

TL;DR: 探讨预训练视觉模型用于设计基于视觉的安全过滤器的有效性，讨论训练方式权衡、模型优劣及部署考量。


<details>
  <summary>Details</summary>
Motivation: 基于视觉的控制系统安全是关键部署挑战，安全过滤器在视觉控制应用有限，研究预训练视觉模型用于设计视觉安全过滤器的有效性。

Method: 将预训练视觉模型用作定义失败集的分类器、基于HJ可达性的安全过滤器和潜在世界模型的骨干，训练中对比从头训练、微调、冻结模型。

Result: 文中未明确提及具体结果

Conclusion: 文中未明确提及最终结论

Abstract: Ensuring safety of vision-based control systems remains a major challenge
hindering their deployment in critical settings. Safety filters have gained
increased interest as effective tools for ensuring the safety of classical
control systems, but their applications in vision-based control settings have
so far been limited. Pre-trained vision models (PVRs) have been shown to be
effective perception backbones for control in various robotics domains. In this
paper, we are interested in examining their effectiveness when used for
designing vision-based safety filters. We use them as backbones for classifiers
defining failure sets, for Hamilton-Jacobi (HJ) reachability-based safety
filters, and for latent world models. We discuss the trade-offs between
training from scratch, fine-tuning, and freezing the PVRs when training the
models they are backbones for. We also evaluate whether one of the PVRs is
superior across all tasks, evaluate whether learned world models or Q-functions
are better for switching decisions to safe policies, and discuss practical
considerations for deploying these PVRs on resource-constrained devices.

</details>


### [222] [Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution](https://arxiv.org/abs/2509.14816)
*Humphrey Munn,Brendan Tidd,Peter Böhm,Marcus Gallagher,David Howard*

Main category: cs.RO

TL;DR: 本文提出GCR - PPO改进算法，在机器人强化学习中解决多目标冲突，经实验验证有更好扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习机器人控制器聚合多目标为单标量奖励，存在调优成本高、易陷入局部最优等问题，多目标方法因计算成本和优化难度未广泛应用。

Method: 提出GCR - PPO，将演员更新分解为按目标的梯度，用多头评论家并根据目标优先级解决冲突。

Result: 在IsaacLab基准和相关多目标任务上评估，比并行PPO有更好扩展性（p = 0.04），无显著计算开销，高冲突任务性能提升大，平均比大规模PPO提升9.5%。

Conclusion: GCR - PPO能有效解决机器人强化学习多目标冲突问题，有良好扩展性和性能。

Abstract: Reinforcement Learning (RL) robot controllers usually aggregate many task
objectives into one scalar reward. While large-scale proximal policy
optimisation (PPO) has enabled impressive results such as robust robot
locomotion in the real world, many tasks still require careful reward tuning
and are brittle to local optima. Tuning cost and sub-optimality grow with the
number of objectives, limiting scalability. Modelling reward vectors and their
trade-offs can address these issues; however, multi-objective methods remain
underused in RL for robotics because of computational cost and optimisation
difficulty. In this work, we investigate the conflict between gradient
contributions for each objective that emerge from scalarising the task
objectives. In particular, we explicitly address the conflict between
task-based rewards and terms that regularise the policy towards realistic
behaviour. We propose GCR-PPO, a modification to actor-critic optimisation that
decomposes the actor update into objective-wise gradients using a multi-headed
critic and resolves conflicts based on the objective priority. Our methodology,
GCR-PPO, is evaluated on the well-known IsaacLab manipulation and locomotion
benchmarks and additional multi-objective modifications on two related tasks.
We show superior scalability compared to parallel PPO (p = 0.04), without
significant computational overhead. We also show higher performance with more
conflicting tasks. GCR-PPO improves on large-scale PPO with an average
improvement of 9.5%, with high-conflict tasks observing a greater improvement.
The code is available at https://github.com/humphreymunn/GCR-PPO.

</details>


### [223] [Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale](https://arxiv.org/abs/2509.14932)
*Tobias Jülg,Pierre Krack,Seongjin Bien,Yannik Blei,Khaled Gamal,Ken Nakahara,Johannes Hechtl,Roberto Calandra,Wolfram Burgard,Florian Walter*

Main category: cs.RO

TL;DR: 提出Robot Control Stack (RCS) 以支持大规模通用策略的机器人学习研究，评估其在VLA和RL策略开发中的性能，还评估了多个模型。


<details>
  <summary>Details</summary>
Motivation: 传统机器人软件框架成为以模型和可扩展训练为中心的机器学习工作流程的瓶颈，机器人模拟对真实实验转换支持有限。

Method: 设计RCS，其核心是模块化且易扩展的分层架构，有统一接口便于虚实转换。

Result: 对RCS在VLA和RL策略开发周期中的可用性和性能进行评估，还对多个模型在多机器人上评估，揭示模拟数据可提升真实策略性能。

Conclusion: RCS能支持大规模通用策略的机器人学习研究，相关代码等资料已开源。

Abstract: Vision-Language-Action models (VLAs) mark a major shift in robot learning.
They replace specialized architectures and task-tailored components of expert
policies with large-scale data collection and setup-specific fine-tuning. In
this machine learning-focused workflow that is centered around models and
scalable training, traditional robotics software frameworks become a
bottleneck, while robot simulations offer only limited support for
transitioning from and to real-world experiments. In this work, we close this
gap by introducing Robot Control Stack (RCS), a lean ecosystem designed from
the ground up to support research in robot learning with large-scale generalist
policies. At its core, RCS features a modular and easily extensible layered
architecture with a unified interface for simulated and physical robots,
facilitating sim-to-real transfer. Despite its minimal footprint and
dependencies, it offers a complete feature set, enabling both real-world
experiments and large-scale training in simulation. Our contribution is
twofold: First, we introduce the architecture of RCS and explain its design
principles. Second, we evaluate its usability and performance along the
development cycle of VLA and RL policies. Our experiments also provide an
extensive evaluation of Octo, OpenVLA, and Pi Zero on multiple robots and shed
light on how simulation data can improve real-world policy performance. Our
code, datasets, weights, and videos are available at:
https://robotcontrolstack.github.io/

</details>


### [224] [AnoF-Diff: One-Step Diffusion-Based Anomaly Detection for Forceful Tool Use](https://arxiv.org/abs/2509.15153)
*Yating Lin,Zixuan Huang,Fan Yang,Dmitry Berenson*

Main category: cs.RO

TL;DR: 提出基于扩散模型的AnoF - Diff方法用于多元时间序列异常检测，在四个任务中表现更好且对噪声更鲁棒，还提出并行异常分数评估方法用于在线检测。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法直接用于强力工具使用任务的多元时间序列异常检测有挑战，数据有噪声、非平稳且因任务和工具而异。

Method: 提出AnoF - Diff方法，基于扩散模型从时间序列数据中提取力 - 扭矩特征进行异常检测，还提出基于一步扩散的并行异常分数评估方法。

Result: 在四个强力工具使用任务上，与其他先进方法对比，F1分数和AUROC表现更好，对噪声数据集更鲁棒。

Conclusion: AnoF - Diff方法及并行异常分数评估方法在强力工具使用任务的多元时间序列异常检测中有效，可用于在线检测。

Abstract: Multivariate time-series anomaly detection, which is critical for identifying
unexpected events, has been explored in the field of machine learning for
several decades. However, directly applying these methods to data from forceful
tool use tasks is challenging because streaming sensor data in the real world
tends to be inherently noisy, exhibits non-stationary behavior, and varies
across different tasks and tools. To address these challenges, we propose a
method, AnoF-Diff, based on the diffusion model to extract force-torque
features from time-series data and use force-torque features to detect
anomalies. We compare our method with other state-of-the-art methods in terms
of F1-score and Area Under the Receiver Operating Characteristic curve (AUROC)
on four forceful tool-use tasks, demonstrating that our method has better
performance and is more robust to a noisy dataset. We also propose the method
of parallel anomaly score evaluation based on one-step diffusion and
demonstrate how our method can be used for online anomaly detection in several
forceful tool use experiments.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [225] [AI-Driven Multi-Agent Vehicular Planning for Battery Efficiency and QoS in 6G Smart Cities](https://arxiv.org/abs/2509.14877)
*Rohin Gillgallon,Giacomo Bergami,Reham Almutairi,Graham Morgan*

Main category: cs.NI

TL;DR: 本文提出SimulatorOrchestrator扩展方案，利用车辆规划算法可提升电池和QoS性能，考虑期望区域能让救护车节能抵达目的地。


<details>
  <summary>Details</summary>
Motivation: 现有模拟器缺乏动态代理规划和优化支持，无法在确保公平通信时间的同时最小化车辆电池消耗。

Method: 扩展SimulatorOrchestrator，加入用于交通预测和动态代理规划的AI算法。

Result: 在现实城市数据集上，车辆规划算法比传统最短路径算法有更好的电池和QoS性能；考虑期望区域的算法比不考虑的传统和加权算法，能让更多救护车节能抵达目的地。

Conclusion: 扩展SimulatorOrchestrator并利用车辆规划算法，能有效提升车辆性能和节能效果。

Abstract: While simulators exist for vehicular IoT nodes communicating with the Cloud
through Edge nodes in a fully-simulated osmotic architecture, they often lack
support for dynamic agent planning and optimisation to minimise vehicular
battery consumption while ensuring fair communication times. Addressing these
challenges requires extending current simulator architectures with AI
algorithms for both traffic prediction and dynamic agent planning. This paper
presents an extension of SimulatorOrchestrator (SO) to meet these requirements.
Preliminary results over a realistic urban dataset show that utilising
vehicular planning algorithms can lead to improved battery and QoS performance
compared with traditional shortest path algorithms. The additional inclusion of
desirability areas enabled more ambulances to be routed to their target
destinations while utilising less energy to do so, compared to traditional and
weighted algorithms without desirability considerations.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [226] [Beyond Classification: Evaluating LLMs for Fine-Grained Automatic Malware Behavior Auditing](https://arxiv.org/abs/2509.14335)
*Xinran Zheng,Xingzhi Qian,Yiling He,Shuo Yang,Lorenzo Cavallaro*

Main category: cs.CR

TL;DR: 提出MalEval框架用于细粒度安卓恶意软件审计，评估大语言模型审计能力，揭示其潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 现有自动化恶意软件分类缺乏因果和可验证解释，大语言模型审计潜力因数据标注、噪声和输出可信度问题未充分挖掘。

Method: 引入MalEval框架，提供专家验证报告和敏感API列表，用函数级结构表示作归因单元，定义四个任务及指标，在数据集上评估七个大语言模型。

Result: 对大语言模型在不同审计阶段的表现进行系统评估，揭示其潜力与局限。

Conclusion: MalEval为未来基于大语言模型的恶意软件行为审计研究提供可复现基准和基础。

Abstract: Automated malware classification has achieved strong detection performance.
Yet, malware behavior auditing seeks causal and verifiable explanations of
malicious activities -- essential not only to reveal what malware does but also
to substantiate such claims with evidence. This task is challenging, as
adversarial intent is often hidden within complex, framework-heavy
applications, making manual auditing slow and costly. Large Language Models
(LLMs) could help address this gap, but their auditing potential remains
largely unexplored due to three limitations: (1) scarce fine-grained
annotations for fair assessment; (2) abundant benign code obscuring malicious
signals; and (3) unverifiable, hallucination-prone outputs undermining
attribution credibility. To close this gap, we introduce MalEval, a
comprehensive framework for fine-grained Android malware auditing, designed to
evaluate how effectively LLMs support auditing under real-world constraints.
MalEval provides expert-verified reports and an updated sensitive API list to
mitigate ground truth scarcity and reduce noise via static reachability
analysis. Function-level structural representations serve as intermediate
attribution units for verifiable evaluation. Building on this, we define four
analyst-aligned tasks -- function prioritization, evidence attribution,
behavior synthesis, and sample discrimination -- together with domain-specific
metrics and a unified workload-oriented score. We evaluate seven widely used
LLMs on a curated dataset of recent malware and misclassified benign apps,
offering the first systematic assessment of their auditing capabilities.
MalEval reveals both promising potential and critical limitations across audit
stages, providing a reproducible benchmark and foundation for future research
on LLM-enhanced malware behavior auditing. MalEval is publicly available at
https://github.com/ZhengXR930/MalEval.git

</details>


### [227] [FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health](https://arxiv.org/abs/2509.14275)
*Nobin Sarwar,Shubhashis Roy Dipta*

Main category: cs.CR

TL;DR: 提出FedMentor框架用于大语言模型在敏感领域的隐私保护微调，实验显示能提升安全性并保持效用。


<details>
  <summary>Details</summary>
Motivation: 在敏感领域（如心理健康）进行大语言模型隐私保护微调时，需平衡严格保密性与模型效用和安全性。

Method: 提出FedMentor框架，集成低秩自适应（LoRA）和领域感知差分隐私（DP），客户端应用自定义DP噪声尺度，服务器在效用低于阈值时自适应降低噪声。

Result: 在三个心理健康数据集实验中，FedMentor提升了安全性，提高安全输出率，降低毒性，保持效用接近非隐私基线和集中式上限，且可扩展到单GPU客户端上17亿参数的骨干网络，每轮通信量小于173MB。

Conclusion: FedMentor为医疗和其他敏感领域安全部署大语言模型的隐私微调提供了实用方法。

Abstract: Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive
domains (e.g., mental health) requires balancing strict confidentiality with
model utility and safety. We propose FedMentor, a federated fine-tuning
framework that integrates Low-Rank Adaptation (LoRA) and domain-aware
Differential Privacy (DP) to meet per-domain privacy budgets while maintaining
performance. Each client (domain) applies a custom DP noise scale proportional
to its data sensitivity, and the server adaptively reduces noise when utility
falls below a threshold. In experiments on three mental health datasets, we
show that FedMentor improves safety over standard Federated Learning without
privacy, raising safe output rates by up to three points and lowering toxicity,
while maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the
non-private baseline and close to the centralized upper bound. The framework
scales to backbones with up to 1.7B parameters on single-GPU clients, requiring
< 173 MB of communication per round. FedMentor demonstrates a practical
approach to privately fine-tune LLMs for safer deployments in healthcare and
other sensitive fields.

</details>


### [228] [Beyond Data Privacy: New Privacy Risks for Large Language Models](https://arxiv.org/abs/2509.14278)
*Yuntao Du,Zitao Li,Ninghui Li,Bolin Ding*

Main category: cs.CR

TL;DR: 本文指出大语言模型（LLMs）部署阶段出现新隐私风险，系统研究这些风险并讨论缓解策略，呼吁拓展研究关注点。


<details>
  <summary>Details</summary>
Motivation: LLMs虽取得进展但部署阶段产生新隐私威胁，此前研究较少关注，有必要深入研究。

Method: 系统地研究LLMs新兴的隐私风险。

Result: 明确了LLMs在部署阶段存在新隐私漏洞，可能导致数据泄露和遭受攻击。

Conclusion: 需讨论潜在缓解策略，研究界应拓展关注点，开发新防御机制应对威胁。

Abstract: Large Language Models (LLMs) have achieved remarkable progress in natural
language understanding, reasoning, and autonomous decision-making. However,
these advancements have also come with significant privacy concerns. While
significant research has focused on mitigating the data privacy risks of LLMs
during various stages of model training, less attention has been paid to new
threats emerging from their deployment. The integration of LLMs into widely
used applications and the weaponization of their autonomous abilities have
created new privacy vulnerabilities. These vulnerabilities provide
opportunities for both inadvertent data leakage and malicious exfiltration from
LLM-powered systems. Additionally, adversaries can exploit these systems to
launch sophisticated, large-scale privacy attacks, threatening not only
individual privacy but also financial security and societal trust. In this
paper, we systematically examine these emerging privacy risks of LLMs. We also
discuss potential mitigation strategies and call for the research community to
broaden its focus beyond data privacy risks, developing new defenses to address
the evolving threats posed by increasingly powerful LLMs and LLM-powered
systems.

</details>


### [229] [The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration](https://arxiv.org/abs/2509.14284)
*Vaidehi Patil,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CR

TL;DR: 研究多智能体大语言模型系统中的组合隐私泄露问题，提出两种防御策略并评估，发现CoDef平衡效果最佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型融入多智能体系统后出现超越传统的新隐私风险，即组合隐私泄露，需进行系统研究和提出缓解方法。

Method: 开发建模框架分析隐私风险，提出Theory-of-Mind defense (ToM)和Collaborative Consensus Defense (CoDef)两种防御策略并进行评估。

Result: Chain-of-thought保护有限，ToM防御可大幅提高敏感查询拦截率但降低良性任务成功率，CoDef平衡效果最佳，平衡结果达79.8%。

Conclusion: 揭示了协作式大语言模型部署中的新风险类别，为防范组合式、上下文驱动的隐私泄露提供了可行建议。

Abstract: As large language models (LLMs) become integral to multi-agent systems, new
privacy risks emerge that extend beyond memorization, direct inference, or
single-turn evaluations. In particular, seemingly innocuous responses, when
composed across interactions, can cumulatively enable adversaries to recover
sensitive information, a phenomenon we term compositional privacy leakage. We
present the first systematic study of such compositional privacy leaks and
possible mitigation methods in multi-agent LLM systems. First, we develop a
framework that models how auxiliary knowledge and agent interactions jointly
amplify privacy risks, even when each response is benign in isolation. Next, to
mitigate this, we propose and evaluate two defense strategies: (1)
Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent
by anticipating how their outputs may be exploited by adversaries, and (2)
Collaborative Consensus Defense (CoDef), where responder agents collaborate
with peers who vote based on a shared aggregated state to restrict sensitive
information spread. Crucially, we balance our evaluation across compositions
that expose sensitive information and compositions that yield benign
inferences. Our experiments quantify how these defense strategies differ in
balancing the privacy-utility trade-off. We find that while chain-of-thought
alone offers limited protection to leakage (~39% sensitive blocking rate), our
ToM defense substantially improves sensitive query blocking (up to 97%) but can
reduce benign task success. CoDef achieves the best balance, yielding the
highest Balanced Outcome (79.8%), highlighting the benefit of combining
explicit reasoning with defender collaboration. Together, our results expose a
new class of risks in collaborative LLM deployments and provide actionable
insights for designing safeguards against compositional, context-driven privacy
leakage.

</details>


### [230] [LLM Jailbreak Detection for (Almost) Free!](https://arxiv.org/abs/2509.14558)
*Guorui Chen,Yifan Xia,Xiaojun Jia,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.CR

TL;DR: 本文提出免费越狱检测（FJD）方法，利用越狱和良性提示输出分布差异检测越狱提示，结合虚拟指令学习提升性能，实验表明该方法能有效检测且几乎无额外计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击检测方法计算成本高，需一种低成本检测方法。

Method: 提出FJD方法，在输入前添加肯定指令，通过温度缩放对数，根据首个标记的置信度区分越狱和良性提示，结合虚拟指令学习提升检测性能。

Result: 在对齐的大语言模型上的大量实验表明，FJD能有效检测越狱提示，且在模型推理时几乎无额外计算成本。

Conclusion: FJD是一种有效且低成本的越狱提示检测方法。

Abstract: Large language models (LLMs) enhance security through alignment when widely
used, but remain susceptible to jailbreak attacks capable of producing
inappropriate content. Jailbreak detection methods show promise in mitigating
jailbreak attacks through the assistance of other models or multiple model
inferences. However, existing methods entail significant computational costs.
In this paper, we first present a finding that the difference in output
distributions between jailbreak and benign prompts can be employed for
detecting jailbreak prompts. Based on this finding, we propose a Free Jailbreak
Detection (FJD) which prepends an affirmative instruction to the input and
scales the logits by temperature to further distinguish between jailbreak and
benign prompts through the confidence of the first token. Furthermore, we
enhance the detection performance of FJD through the integration of virtual
instruction learning. Extensive experiments on aligned LLMs show that our FJD
can effectively detect jailbreak prompts with almost no additional
computational costs during LLM inference.

</details>


### [231] [ATLANTIS: AI-driven Threat Localization, Analysis, and Triage Intelligence System](https://arxiv.org/abs/2509.14589)
*Taesoo Kim,HyungSeok Han,Soyeon Park,Dae R. Jeong,Dohyeok Kim,Dongkwan Kim,Eunsoo Kim,Jiho Kim,Joshua Wang,Kangsu Kim,Sangwoo Ji,Woosun Song,Hanqing Zhao,Andrew Chin,Gyejin Lee,Kevin Stevens,Mansour Alharthi,Yizhuo Zhai,Cen Zhang,Joonun Jang,Yeongjin Jang,Ammar Askar,Dongju Kim,Fabian Fleischer,Jeongin Cho,Junsik Kim,Kyungjoon Ko,Insu Yun,Sangdon Park,Dowoo Baik,Haein Lee,Hyeon Heo,Minjae Gwon,Minjae Lee,Minwoo Baek,Seunggi Min,Wonyoung Kim,Yonghwi Jin,Younggi Park,Yunjae Choi,Jinho Jung,Gwanhyun Lee,Junyoung Jang,Kyuheon Kim,Yeonghyeon Cha,Youngjoon Kim*

Main category: cs.CR

TL;DR: 介绍赢得DARPA的AI Cyber Challenge决赛冠军的ATLANTIS网络推理系统，阐述其设计等方面并分享经验、发布成果。


<details>
  <summary>Details</summary>
Motivation: 应对DARPA的AIxCC挑战，构建能高速大规模发现和修复漏洞的自主网络推理系统。

Method: 将大语言模型与程序分析（符号执行、定向模糊测试和静态分析）相结合。

Result: 开发出ATLANTIS系统，解决了跨不同代码库扩展、高精度与广泛覆盖以及生成语义正确补丁等核心挑战。

Conclusion: 详细介绍系统设计理念等，分享自动化安全研究经验并发布成果支持可重复性研究和未来研究。

Abstract: We present ATLANTIS, the cyber reasoning system developed by Team Atlanta
that won 1st place in the Final Competition of DARPA's AI Cyber Challenge
(AIxCC) at DEF CON 33 (August 2025). AIxCC (2023-2025) challenged teams to
build autonomous cyber reasoning systems capable of discovering and patching
vulnerabilities at the speed and scale of modern software. ATLANTIS integrates
large language models (LLMs) with program analysis -- combining symbolic
execution, directed fuzzing, and static analysis -- to address limitations in
automated vulnerability discovery and program repair. Developed by researchers
at Georgia Institute of Technology, Samsung Research, KAIST, and POSTECH, the
system addresses core challenges: scaling across diverse codebases from C to
Java, achieving high precision while maintaining broad coverage, and producing
semantically correct patches that preserve intended behavior. We detail the
design philosophy, architectural decisions, and implementation strategies
behind ATLANTIS, share lessons learned from pushing the boundaries of automated
security when program analysis meets modern AI, and release artifacts to
support reproducibility and future research.

</details>


### [232] [Enterprise AI Must Enforce Participant-Aware Access Control](https://arxiv.org/abs/2509.14608)
*Shashank Shreedhar Bhatt,Tanmay Rajore,Khushboo Aggarwal,Ganesh Ananthanarayanan,Ranveer Chandra,Nishanth Chandran,Suyash Choudhury,Divya Gupta,Emre Kiciman,Sumit Kumar Pandey,Srinath Setty,Rahul Sharma,Teijia Zhao*

Main category: cs.CR

TL;DR: 研究大语言模型（LLMs）在企业环境中敏感数据泄露风险，提出基于细粒度访问控制的框架并已应用。


<details>
  <summary>Details</summary>
Motivation: LLMs在企业应用中，微调与RAG架构会导致敏感数据泄露，现有防御机制有局限。

Method: 演示数据窃取攻击，分析现有防御机制不足，提出基于明确授权的细粒度访问控制框架。

Result: 现有防御机制无法有效抵御攻击，新框架可用于构建安全的多用户LLM系统。

Conclusion: 只有在微调与推理阶段严格执行细粒度访问控制，才能可靠防止敏感数据泄露，新框架已在微软产品中部署。

Abstract: Large language models (LLMs) are increasingly deployed in enterprise settings
where they interact with multiple users and are trained or fine-tuned on
sensitive internal data. While fine-tuning enhances performance by
internalizing domain knowledge, it also introduces a critical security risk:
leakage of confidential training data to unauthorized users. These risks are
exacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG)
pipelines that dynamically fetch contextual documents at inference time.
  We demonstrate data exfiltration attacks on AI assistants where adversaries
can exploit current fine-tuning and RAG architectures to leak sensitive
information by leveraging the lack of access control enforcement. We show that
existing defenses, including prompt sanitization, output filtering, system
isolation, and training-level privacy mechanisms, are fundamentally
probabilistic and fail to offer robust protection against such attacks.
  We take the position that only a deterministic and rigorous enforcement of
fine-grained access control during both fine-tuning and RAG-based inference can
reliably prevent the leakage of sensitive data to unauthorized recipients.
  We introduce a framework centered on the principle that any content used in
training, retrieval, or generation by an LLM is explicitly authorized for
\emph{all users involved in the interaction}. Our approach offers a simple yet
powerful paradigm shift for building secure multi-user LLM systems that are
grounded in classical access control but adapted to the unique challenges of
modern AI workflows. Our solution has been deployed in Microsoft Copilot
Tuning, a product offering that enables organizations to fine-tune models using
their own enterprise-specific data.

</details>


### [233] [Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection](https://arxiv.org/abs/2509.14622)
*Yihao Guo,Haocheng Bian,Liutong Zhou,Ze Wang,Zhaoyi Zhang,Francois Kawala,Milan Dean,Ian Fischer,Yuantao Peng,Noyan Tokgozoglu,Ivan Barrientos,Riyaaz Shaik,Rachel Li,Chandru Venkataraman,Reza Shifteh Far,Moses Pawar,Venkat Sundaranatha,Michael Xu,Frank Chu*

Main category: cs.CR

TL;DR: 提出ADRAG框架用于在线恶意意图检测，在多基准测试中表现良好且降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以实时处理多样化和复杂的用户查询，需要更好的在线恶意意图检测方法。

Method: 提出ADRAG两阶段框架，训练阶段用对抗扰动和检索增强输入训练教师模型，推理阶段将教师知识蒸馏到学生模型并结合在线更新知识库。

Result: ADRAG用149M参数模型达到WildGuard - 7B性能的98.5%，在分布外检测上超过GPT - 4和Llama - Guard - 3 - 8B，同时降低延迟。

Conclusion: ADRAG是一种高效且鲁棒的在线恶意意图检测方法。

Abstract: With the deployment of Large Language Models (LLMs) in interactive
applications, online malicious intent detection has become increasingly
critical. However, existing approaches fall short of handling diverse and
complex user queries in real time. To address these challenges, we introduce
ADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework
for robust and efficient online malicious intent detection. In the training
stage, a high-capacity teacher model is trained on adversarially perturbed,
retrieval-augmented inputs to learn robust decision boundaries over diverse and
complex user queries. In the inference stage, a distillation scheduler
transfers the teacher's knowledge into a compact student model, with a
continually updated knowledge base collected online. At deployment, the compact
student model leverages top-K similar safety exemplars retrieved from the
online-updated knowledge base to enable both online and real-time malicious
query detection. Evaluations across ten safety benchmarks demonstrate that
ADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's
performance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on
out-of-distribution detection, while simultaneously delivering up to 5.6x lower
latency at 300 queries per second (QPS) in real-time applications.

</details>


### [234] [Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models](https://arxiv.org/abs/2509.14271)
*Gustavo Sandoval,Denys Fenchenko,Junyao Chen*

Main category: cs.CR

TL;DR: 本文介绍2022年针对大语言模型提示注入攻击防御的早期研究，聚焦提示注入和目标劫持攻击，提出对抗性微调防御技术，结果显示该技术降低了攻击成功率，研究为现代防御研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 为大语言模型提示注入攻击这一关键安全领域的发展提供历史背景，研究针对大语言模型的攻击及防御方法。

Method: 研究两种对抗攻击（提示注入和目标劫持）的构造，在不同大语言模型上进行测试，提出并评估对抗性微调防御技术。

Result: 未使用防御时，攻击在GPT - 3系列模型上成功率为31%；使用对抗性微调方法后，较小的GPT - 3变体攻击成功率接近零；更灵活的模型对攻击更脆弱，如GPT - 3 Davinci比GPT - 2更易受攻击。

Conclusion: 虽然测试的具体模型已被取代，但核心方法和实证结果为现代提示注入防御研究奠定了基础。

Abstract: This paper documents early research conducted in 2022 on defending against
prompt injection attacks in large language models, providing historical context
for the evolution of this critical security domain. This research focuses on
two adversarial attacks against Large Language Models (LLMs): prompt injection
and goal hijacking. We examine how to construct these attacks, test them on
various LLMs, and compare their effectiveness. We propose and evaluate a novel
defense technique called Adversarial Fine-Tuning. Our results show that,
without this defense, the attacks succeeded 31\% of the time on GPT-3 series
models. When using our Adversarial Fine-Tuning approach, attack success rates
were reduced to near zero for smaller GPT-3 variants (Ada, Babbage, Curie),
though we note that subsequent research has revealed limitations of
fine-tuning-based defenses. We also find that more flexible models exhibit
greater vulnerability to these attacks. Consequently, large models such as
GPT-3 Davinci are more vulnerable than smaller models like GPT-2. While the
specific models tested are now superseded, the core methodology and empirical
findings contributed to the foundation of modern prompt injection defense
research, including instruction hierarchy systems and constitutional AI
approaches.

</details>


### [235] [Threat Modeling for Enhancing Security of IoT Audio Classification Devices under a Secure Protocols Framework](https://arxiv.org/abs/2509.14657)
*Sergio Benlloch-Lopez,Miquel Viel-Vazquez,Javier Naranjo-Alcazar,Jordi Grau-Haro,Pedro Zuccarello*

Main category: cs.CR

TL;DR: 提出深度防御架构保护物联网音频分类设备敏感数据，包含多方面安全措施并规划评估方案。


<details>
  <summary>Details</summary>
Motivation: 物联网音频分类节点在资源受限下易暴露敏感数据，需保护。

Method: 构建深度防御架构，含安全协议，分三个信任域，用TPM远程认证和TLS 1.3，结合威胁模型和攻击树分析，有多种加密和防护措施。

Result: 未提及具体实验结果。

Conclusion: 提出保护物联网音频分类节点敏感数据的架构及评估计划。

Abstract: The rapid proliferation of IoT nodes equipped with microphones and capable of
performing on-device audio classification exposes highly sensitive data while
operating under tight resource constraints. To protect against this, we present
a defence-in-depth architecture comprising a security protocol that treats the
edge device, cellular network and cloud backend as three separate trust
domains, linked by TPM-based remote attestation and mutually authenticated TLS
1.3. A STRIDE-driven threat model and attack-tree analysis guide the design. At
startup, each boot stage is measured into TPM PCRs. The node can only decrypt
its LUKS-sealed partitions after the cloud has verified a TPM quote and
released a one-time unlock key. This ensures that rogue or tampered devices
remain inert. Data in transit is protected by TLS 1.3 and hybridised with Kyber
and Dilithium to provide post-quantum resilience. Meanwhile, end-to-end
encryption and integrity hashes safeguard extracted audio features. Signed,
rollback-protected AI models and tamper-responsive sensors harden firmware and
hardware. Data at rest follows a 3-2-1 strategy comprising a solid-state drive
sealed with LUKS, an offline cold archive encrypted with a hybrid post-quantum
cipher and an encrypted cloud replica. Finally, we set out a plan for
evaluating the physical and logical security of the proposed protocol.

</details>


### [236] [A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks](https://arxiv.org/abs/2509.14285)
*S M Asif Hossain,Ruksat Khan Shayoni,Mohd Ruhul Ameen,Akif Islam,M. F. Mridha,Jungpil Shin*

Main category: cs.CR

TL;DR: 本文提出多智能体防御框架实时检测和中和大语言模型提示注入攻击，评估两种架构，测试显示显著提升安全性，攻击成功率降至0%。


<details>
  <summary>Details</summary>
Motivation: 提示注入攻击是大语言模型部署的主要漏洞，恶意指令会覆盖系统提示并引发意外行为，需要防御方法。

Method: 提出多智能体防御框架，使用专门的大语言模型智能体在协调管道中工作，评估顺序链式智能体管道和分层协调器系统两种架构。

Result: 在55种独特提示注入攻击、400个攻击实例的测试中，无防御时ChatGLM和Llama2攻击成功率分别达30%和20%，多智能体管道实现100%缓解，攻击成功率降至0%。

Conclusion: 该框架在多种攻击类别中表现出鲁棒性，同时能维持合法查询的系统功能。

Abstract: Prompt injection attacks represent a major vulnerability in Large Language
Model (LLM) deployments, where malicious instructions embedded in user inputs
can override system prompts and induce unintended behaviors. This paper
presents a novel multi-agent defense framework that employs specialized LLM
agents in coordinated pipelines to detect and neutralize prompt injection
attacks in real-time. We evaluate our approach using two distinct
architectures: a sequential chain-of-agents pipeline and a hierarchical
coordinator-based system. Our comprehensive evaluation on 55 unique prompt
injection attacks, grouped into 8 categories and totaling 400 attack instances
across two LLM platforms (ChatGLM and Llama2), demonstrates significant
security improvements. Without defense mechanisms, baseline Attack Success
Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent
pipeline achieved 100% mitigation, reducing ASR to 0% across all tested
scenarios. The framework demonstrates robustness across multiple attack
categories including direct overrides, code execution attempts, data
exfiltration, and obfuscation techniques, while maintaining system
functionality for legitimate queries.

</details>


### [237] [Blockchain-Enabled Explainable AI for Trusted Healthcare Systems](https://arxiv.org/abs/2509.14987)
*Md Talha Mohsin*

Main category: cs.CR

TL;DR: 本文提出区块链集成可解释AI框架BXHF，用于解决医疗系统数据安全交换和可理解临床决策两大挑战，展示其适用性并强调对AI在医疗应用的积极影响。


<details>
  <summary>Details</summary>
Motivation: 解决健康信息网络面临的安全数据交换和可理解的AI驱动临床决策两大挑战。

Method: 将区块链与可解释AI方法结合，融入安全保障和可解释性要求到统一优化管道，采用混合边缘 - 云架构进行跨机构联合计算。

Result: 通过跨境临床研究网络、罕见病检测和高风险干预决策支持等用例展示了框架的适用性。

Conclusion: BXHF提高了AI在医疗中的可信度、使用率和有效性，为更安全可靠的临床决策奠定基础。

Abstract: This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF)
for healthcare systems to tackle two essential challenges confronting health
information networks: safe data exchange and comprehensible AI-driven clinical
decision-making. Our architecture incorporates blockchain, ensuring patient
records are immutable, auditable, and tamper-proof, alongside Explainable AI
(XAI) methodologies that yield transparent and clinically relevant model
predictions. By incorporating security assurances and interpretability
requirements into a unified optimization pipeline, BXHF ensures both data-level
trust (by verified and encrypted record sharing) and decision-level trust (with
auditable and clinically aligned explanations). Its hybrid edge-cloud
architecture allows for federated computation across different institutions,
enabling collaborative analytics while protecting patient privacy. We
demonstrate the framework's applicability through use cases such as
cross-border clinical research networks, uncommon illness detection and
high-risk intervention decision support. By ensuring transparency,
auditability, and regulatory compliance, BXHF improves the credibility, uptake,
and effectiveness of AI in healthcare, laying the groundwork for safer and more
reliable clinical decision-making.

</details>


### [238] [Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting](https://arxiv.org/abs/2509.15170)
*Aarushi Mahajan,Wayne Burleson*

Main category: cs.CR

TL;DR: 提出结合水印和异常检测的射频指纹识别系统，在LoRa数据集上取得较好效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的射频指纹识别模型易受复制、篡改和逃避攻击，需更强系统。

Method: 使用ResNet - 34在对数梅尔频谱图上嵌入三种水印，用带KL预热和自由位的卷积变分自编码器进行异常检测。

Result: 在LoRa数据集上，系统准确率达94.6%，水印成功率98%，AUROC为0.94。

Conclusion: 该系统可提供可验证、抗篡改的认证。

Abstract: Radio frequency fingerprint identification (RFFI) distinguishes wireless
devices by the small variations in their analog circuits, avoiding heavy
cryptographic authentication. While deep learning on spectrograms improves
accuracy, models remain vulnerable to copying, tampering, and evasion. We
present a stronger RFFI system combining watermarking for ownership proof and
anomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel
spectrograms, we embed three watermarks: a simple trigger, an adversarially
trained trigger robust to noise and filtering, and a hidden gradient/weight
signature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler
(KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset,
our system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC,
offering verifiable, tamper-resistant authentication.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [239] [The Complexity of Finding and Counting Subtournaments](https://arxiv.org/abs/2509.14807)
*Simon Döring,Sarah Houdaigoui,Lucas Picasarri-Arrieta,Philip Wellnitz*

Main category: cs.CC

TL;DR: 研究在大型竞赛图中计数和寻找小型竞赛图模式的复杂度，给出新方法并得到复杂度相关结果。


<details>
  <summary>Details</summary>
Motivation: 之前Yuster已得出特定问题对随机竞赛图计算困难，需对计数子竞赛图问题复杂度进行更精细分析。

Method: 使用子图计数的线性组合的新方法。

Result: 对于所有k阶竞赛图T，计数问题至少和计数⌊3k/4⌋ - 团一样难；参数化版本在集合包含无限多竞赛图时是#W[1] - 难的。

Conclusion: 新方法能对计数子竞赛图问题复杂度进行更深入分析，得到相关复杂度的紧界。

Abstract: We study the complexity of counting and finding small tournament patterns
inside large tournaments. Given a fixed tournament $T$ of order $k$, we write
${\#}\text{IndSub}_{\text{To}}(\{T\})$ for the problem whose input is a
tournament $G$ and the task is to compute the number of subtournaments of $G$
that are isomorphic to $T$. Previously, Yuster [Yus25] obtained that
${\#}\text{IndSub}_{\text{To}}(\{T\})$ is hard to compute for random
tournaments $T$. We consider a new approach that uses linear combinations of
subgraph-counts [CDM17] to obtain a finer analysis of the complexity of
${\#}\text{IndSub}_{\text{To}}(\{T\})$.
  We show that for all tournaments $T$ of order $k$ the problem
${\#}\text{IndSub}_{\text{To}}(\{T\})$ is always at least as hard as counting
$\lfloor 3k/4 \rfloor$-cliques. This immediately yields tight bounds under ETH.
Further, we consider the parameterized version of
${\#}\text{IndSub}_{\text{To}}(\mathcal{T})$ where we only consider patterns $T
\in \mathcal{T}$ and that is parameterized by the pattern size $|V(T)|$. We
show that ${\#}\text{IndSub}_{\text{To}}(\mathcal{T})$ is ${\#}W[1]$-hard as
long as $\mathcal{T}$ contains infinitely many tournaments.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [240] [A cell centered Galerkin method for miscible displacement in heterogeneous porous media](https://arxiv.org/abs/2509.14864)
*Maurice S. Fabien*

Main category: math.NA

TL;DR: 提出应用于非均质多孔介质混溶驱替问题的CCG方法，与传统方法对比展示其有效性。


<details>
  <summary>Details</summary>
Motivation: 寻找高效求解非均质多孔介质混溶驱替问题的方法。

Method: 结合有限体积法和间断Galerkin法思想的CCG方法。

Result: CCG方法能用经典DG弱形式定义，每单元一个未知量，精度相当且效率更高，1D模型Poisson问题中产生逆正矩阵，2D和3D计算实验展示其有效性。

Conclusion: CCG方法对非均质多孔介质中流动与传输问题有效，优于传统高阶内部惩罚DG方法。

Abstract: In this paper we present a cell centered Galerkin (CCG) method applied to
miscible displacement problems in heterogeneous porous media. The CCG approach
combines concepts from finite volume and discontinuous Galerkin (DG) methods to
arrive at an efficient lowest-order approximation (one unknown per cell). We
demonstrate that the CCG method can be defined using classical DG weak
formulations, only requires one unknown per cell, and is able to deliver
comparable accuracy and improved efficiency over traditional higher-order
interior penalty DG methods. In addition, we prove that the CCG method for a
model Poisson problem gives rise to a inverse-positive matrix in 1D. A plethora
of computational experiments in 2D and 3D showcase the effectiveness of the CCG
method for highly heterogeneous flow and transport problems in porous media.
Comparisons between CCG and classical DG methods are included.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [241] [Beyond Spherical geometry: Unraveling complex features of objects orbiting around stars from its transit light curve using deep learning](https://arxiv.org/abs/2509.14875)
*Ushasi Bhowmick,Shivam Kumaran*

Main category: astro-ph.EP

TL;DR: 研究从凌星光曲线中提取形状特征，用随机形状库和模拟器，训练神经网络预测傅里叶系数，结果显示可重构低阶椭圆，表明光曲线提取几何信息可行。


<details>
  <summary>Details</summary>
Motivation: 凌星光曲线表征绕恒星运行物体几何形状问题存在多解，需研究形状特征在光曲线中的嵌入程度。

Method: 生成二维随机形状库，用Yuti模拟器模拟光曲线，将形状分解为椭圆分量并用傅里叶系数表示，训练神经网络从光曲线预测系数。

Result: 神经网络能成功重构描述整体形状、方向和大规模扰动的低阶椭圆，高阶椭圆能确定规模但偏心率和方向推断有限，还探索了非凸形状特征影响。

Conclusion: 神经网络的重构水平凸显了用光曲线从凌星系统提取几何信息的实用性。

Abstract: Characterizing the geometry of an object orbiting around a star from its
transit light curve is a powerful tool to uncover various complex phenomena.
This problem is inherently ill-posed, since similar or identical light curves
can be produced by multiple different shapes. In this study, we investigate the
extent to which the features of a shape can be embedded in a transit light
curve. We generate a library of two-dimensional random shapes and simulate
their transit light curves with light curve simulator, Yuti. Each shape is
decomposed into a series of elliptical components expressed in the form of
Fourier coefficients that adds increasingly diminishing perturbations to an
ideal ellipse. We train deep neural networks to predict these Fourier
coefficients directly from simulated light curves. Our results demonstrate that
the neural network can successfully reconstruct the low-order ellipses, which
describe overall shape, orientation and large-scale perturbations. For higher
order ellipses the scale is successfully determined but the inference of
eccentricity and orientation is limited, demonstrating the extent of shape
information in the light curve. We explore the impact of non-convex shape
features in reconstruction, and show its dependence on shape orientation. The
level of reconstruction achieved by the neural network underscores the utility
of using light curves as a means to extract geometric information from
transiting systems.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [242] [A Scalable Formula for the Moments of a Family of Self-Normalized Statistics](https://arxiv.org/abs/2509.14428)
*Haolin Zou,Heyuan Yao,Victor de la Peña*

Main category: math.ST

TL;DR: 本文研究样本均值比例形式统计量，给出非负观测自归一化统计量矩的统一公式，提出去偏方法。


<details>
  <summary>Details</summary>
Motivation: 样本均值比例形式统计量的概率特征未知，有研究需求。

Method: 推导非负观测自归一化统计量矩的统一公式，进行大量数值实验。

Result: 得到统一公式，复杂度不随样本量增加，揭示偏差和方差新见解。

Conclusion: 提出去偏方法，并在优势比、基尼系数等应用中说明。

Abstract: Following the student t-statistic, normalization has been a widely used
method in statistic and other disciplines including economics, ecology and
machine learning. We focus on statistics taking the form of a ratio over (some
power of) the sample mean, the probabilistic features of which remain unknown.
We develop a unified formula for the moments of these self-normalized
statistics with non-negative observations, yielding closed-form expressions for
several important cases. Moreover, the complexity of our formula doesn't scale
with the sample size $n$. Our theoretical findings, supported by extensive
numerical experiments, reveal novel insights into their bias and variance, and
we propose a debiasing method illustrated with applications such as the odds
ratio, Gini coefficient and squared coefficient of variation.

</details>


### [243] [Consistent causal discovery with equal error variances: a least-squares perspective](https://arxiv.org/abs/2509.15197)
*Anamitra Chaudhuri,Yang Ni,Anirban Bhattacharya*

Main category: math.ST

TL;DR: 研究线性无环结构方程模型中因果结构恢复问题，建立性质并设计贝叶斯DAG选择方法以一致恢复真实图。


<details>
  <summary>Details</summary>
Motivation: 解决线性无环结构方程模型中一组变量间真实因果结构的恢复问题。

Method: 建立每个变量由其父变量的最佳线性组合预测时最小期望平方误差之和的性质，并据此设计贝叶斯DAG选择方法。

Result: 得出最小期望平方误差之和最小化的条件是因果结构由真实DAG的任何超图表示。

Conclusion: 所设计的贝叶斯DAG选择方法能一致地恢复真实图。

Abstract: We consider the problem of recovering the true causal structure among a set
of variables, generated by a linear acyclic structural equation model (SEM)
with the error terms being independent and having equal variances. It is
well-known that the true underlying directed acyclic graph (DAG) encoding the
causal structure is uniquely identifiable under this assumption. In this work,
we establish that the sum of minimum expected squared errors for every
variable, while predicted by the best linear combination of its parent
variables, is minimised if and only if the causal structure is represented by
any supergraph of the true DAG. This property is further utilised to design a
Bayesian DAG selection method that recovers the true graph consistently.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [244] [Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity](https://arxiv.org/abs/2509.14276)
*Yuxiang Mai,Qiyue Yin,Wancheng Ni,Pei Xu,Kaiqi Huang*

Main category: cs.MA

TL;DR: 提出CoDiCon方法将竞争激励引入合作场景，在SMAC和GRF环境实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习方法在策略形成时忽视智能体间的相互作用，本文旨在解决此问题。

Method: 借鉴社会学研究，设计基于排名特征的内在奖励机制，用集中式内在奖励模块生成并分配奖励，优化参数化集中奖励模块以最大化环境奖励，重新构建约束双层优化问题。

Result: 在SMAC和GRF环境中与现有方法对比，CoDiCon表现更优，竞争内在奖励有效促进合作智能体采用多样和自适应策略。

Conclusion: CoDiCon方法能有效结合竞争与合作，促进合作智能体的策略多样性和适应性。

Abstract: In recent years, diversity has emerged as a useful mechanism to enhance the
efficiency of multi-agent reinforcement learning (MARL). However, existing
methods predominantly focus on designing policies based on individual agent
characteristics, often neglecting the interplay and mutual influence among
agents during policy formation. To address this gap, we propose Competitive
Diversity through Constructive Conflict (CoDiCon), a novel approach that
incorporates competitive incentives into cooperative scenarios to encourage
policy exchange and foster strategic diversity among agents. Drawing
inspiration from sociological research, which highlights the benefits of
moderate competition and constructive conflict in group decision-making, we
design an intrinsic reward mechanism using ranking features to introduce
competitive motivations. A centralized intrinsic reward module generates and
distributes varying reward values to agents, ensuring an effective balance
between competition and cooperation. By optimizing the parameterized
centralized reward module to maximize environmental rewards, we reformulate the
constrained bilevel optimization problem to align with the original task
objectives. We evaluate our algorithm against state-of-the-art methods in the
SMAC and GRF environments. Experimental results demonstrate that CoDiCon
achieves superior performance, with competitive intrinsic rewards effectively
promoting diverse and adaptive strategies among cooperative agents.

</details>


### [245] [LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.14680)
*Tianyang Duan,Zongyuan Zhang,Songxiao Guo,Dong Huang,Yuanye Zhao,Zheng Lin,Zihan Fang,Dianxin Luan,Heming Cui,Yong Cui*

Main category: cs.MA

TL;DR: 提出用于多智能体强化学习的LLM赋能专家演示框架LEED，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中随着智能体数量增加出现的协调和可扩展性瓶颈问题。

Method: 提出LEED框架，包含利用大语言模型生成指令产生高质量演示的DG模块，以及采用分散训练范式、结合专家策略损失和自身策略损失的PO模块。

Result: LEED在样本效率、时间效率和可扩展性上优于现有基线。

Conclusion: LEED框架有效，能解决多智能体强化学习的协调和可扩展性问题。

Abstract: Multi-agent reinforcement learning (MARL) holds substantial promise for
intelligent decision-making in complex environments. However, it suffers from a
coordination and scalability bottleneck as the number of agents increases. To
address these issues, we propose the LLM-empowered expert demonstrations
framework for multi-agent reinforcement learning (LEED). LEED consists of two
components: a demonstration generation (DG) module and a policy optimization
(PO) module. Specifically, the DG module leverages large language models to
generate instructions for interacting with the environment, thereby producing
high-quality demonstrations. The PO module adopts a decentralized training
paradigm, where each agent utilizes the generated demonstrations to construct
an expert policy loss, which is then integrated with its own policy loss. This
enables each agent to effectively personalize and optimize its local policy
based on both expert knowledge and individual experience. Experimental results
show that LEED achieves superior sample efficiency, time efficiency, and robust
scalability compared to state-of-the-art baselines.

</details>


### [246] [Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.15103)
*Simin Li,Zheng Yuwei,Zihao Mao,Linhao Wang,Ruixiao Xu,Chengdong Ma,Xin Yu,Yuqing Ma,Qi Dou,Xin Wang,Jie Luo,Bo An,Yaodong Yang,Weifeng Lv,Xianglong Liu*

Main category: cs.MA

TL;DR: 研究大规模多智能体强化学习中的脆弱智能体识别（VAI）问题，提出HAD - MFC框架并给出求解方法，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 系统规模扩大时部分智能体故障不可避免，需识别出故障后对整体性能影响最大的智能体子集。

Method: 将VAI建模为HAD - MFC，通过Fenchel - Rockafellar变换解耦分层过程，将上层组合问题重新表述为MDP，用贪心和RL算法顺序识别脆弱智能体。

Result: 实验表明该方法能在大规模MARL和基于规则的系统中有效识别更多脆弱智能体，使系统陷入更严重故障，还能学习揭示每个智能体脆弱性的价值函数。

Conclusion: 提出的方法可有效解决大规模多智能体强化学习中的VAI问题，分解方法能保留原HAD - MFC的最优解。

Abstract: Partial agent failure becomes inevitable when systems scale up, making it
crucial to identify the subset of agents whose compromise would most severely
degrade overall performance. In this paper, we study this Vulnerable Agent
Identification (VAI) problem in large-scale multi-agent reinforcement learning
(MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field
Control (HAD-MFC), where the upper level involves an NP-hard combinatorial task
of selecting the most vulnerable agents, and the lower level learns worst-case
adversarial policies for these agents using mean-field MARL. The two problems
are coupled together, making HAD-MFC difficult to solve. To solve this, we
first decouple the hierarchical process by Fenchel-Rockafellar transform,
resulting a regularized mean-field Bellman operator for upper level that
enables independent learning at each level, thus reducing computational
complexity. We then reformulate the upper-level combinatorial problem as a MDP
with dense rewards from our regularized mean-field Bellman operator, enabling
us to sequentially identify the most vulnerable agents by greedy and RL
algorithms. This decomposition provably preserves the optimal solution of the
original HAD-MFC. Experiments show our method effectively identifies more
vulnerable agents in large-scale MARL and the rule-based system, fooling system
into worse failures, and learns a value function that reveals the vulnerability
of each agent.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [247] [Data coarse graining can improve model performance](https://arxiv.org/abs/2509.14498)
*Alex Nguyen,David J. Schwab,Vudtiwat Ngampruetikorn*

Main category: cond-mat.stat-mech

TL;DR: 研究有损数据变换在机器学习中能提升泛化性能的悖论，分析数据粗粒化方案，揭示预测风险与粗粒化程度的非单调关系，解释数据增强为何有效。


<details>
  <summary>Details</summary>
Motivation: 解释有损数据变换（如数据修剪和有损数据增强）在机器学习中能提升泛化性能这一悖论。

Method: 使用高维、岭正则化线性回归的可解模型，受统计物理重整化群启发，分析基于特征相关性丢弃特征的粗粒化方案。

Result: 预测风险与粗粒化程度呈非单调关系，“高通”方案有助于模型泛化，“低通”方案有害，非单调性是数据粗粒化的独特效果。

Conclusion: 框架解释了数据增强有效的原因，凸显数据结构塑造的复杂非单调风险格局，说明统计物理思想有助于理解机器学习现象。

Abstract: Lossy data transformations by definition lose information. Yet, in modern
machine learning, methods like data pruning and lossy data augmentation can
help improve generalization performance. We study this paradox using a solvable
model of high-dimensional, ridge-regularized linear regression under 'data
coarse graining.' Inspired by the renormalization group in statistical physics,
we analyze coarse-graining schemes that systematically discard features based
on their relevance to the learning task. Our results reveal a nonmonotonic
dependence of the prediction risk on the degree of coarse graining. A
'high-pass' scheme--which filters out less relevant, lower-signal features--can
help models generalize better. By contrast, a 'low-pass' scheme that integrates
out more relevant, higher-signal features is purely detrimental. Crucially,
using optimal regularization, we demonstrate that this nonmonotonicity is a
distinct effect of data coarse graining and not an artifact of double descent.
Our framework offers a clear, analytical explanation for why careful data
augmentation works: it strips away less relevant degrees of freedom and
isolates more predictive signals. Our results highlight a complex, nonmonotonic
risk landscape shaped by the structure of the data, and illustrate how ideas
from statistical physics provide a principled lens for understanding modern
machine learning phenomena.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [248] [WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance](https://arxiv.org/abs/2509.15130)
*Chenxi Song,Yanming Yang,Tong Zhao,Ruibo Li,Chi Zhang*

Main category: cs.GR

TL;DR: 提出无训练的WorldForge框架解决视频扩散模型可控性和几何一致性问题，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在3D/4D任务中因可控性和几何一致性有限，依赖重训练或微调有知识退化和高计算成本问题。

Method: 提出由Intra - Step Recursive Refinement、Flow - Gated Latent Fusion和Dual - Path Self - Corrective Guidance三个模块组成的WorldForge框架，在推理时注入细粒度轨迹对齐指导。

Result: 广泛实验验证该方法在真实性、轨迹一致性和视觉保真度上的优越性。

Conclusion: 引入可控制视频合成的即插即用范式，为利用生成先验进行空间智能提供新视角。

Abstract: Recent video diffusion models demonstrate strong potential in spatial
intelligence tasks due to their rich latent world priors. However, this
potential is hindered by their limited controllability and geometric
inconsistency, creating a gap between their strong priors and their practical
use in 3D/4D tasks. As a result, current approaches often rely on retraining or
fine-tuning, which risks degrading pretrained knowledge and incurs high
computational costs. To address this, we propose WorldForge, a training-free,
inference-time framework composed of three tightly coupled modules. Intra-Step
Recursive Refinement introduces a recursive refinement mechanism during
inference, which repeatedly optimizes network predictions within each denoising
step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages
optical flow similarity to decouple motion from appearance in the latent space
and selectively inject trajectory guidance into motion-related channels.
Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths
to adaptively correct trajectory drift caused by noisy or misaligned structural
signals. Together, these components inject fine-grained, trajectory-aligned
guidance without training, achieving both accurate motion control and
photorealistic content generation. Extensive experiments across diverse
benchmarks validate our method's superiority in realism, trajectory
consistency, and visual fidelity. This work introduces a novel plug-and-play
paradigm for controllable video synthesis, offering a new perspective on
leveraging generative priors for spatial intelligence.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [249] [Undersampled Phase Retrieval with Image Priors](https://arxiv.org/abs/2509.15026)
*Stanislas Ducotterd,Zhiyuan Hu,Michael Unser,Jonathan Dong*

Main category: eess.IV

TL;DR: 本文评估多种图像先验在严重欠采样结构化随机傅里叶测量下的效果，发现先验能显著改善重建。


<details>
  <summary>Details</summary>
Motivation: 当前相位恢复的理论和算法常忽略信号先验，需要评估图像先验在严重欠采样下的作用。

Method: 评估多种图像先验在严重欠采样结构化随机傅里叶测量中的应用。

Result: 图像先验显著改善重建，能在弱恢复阈值以下实现准确重建。

Conclusion: 图像先验对严重欠采样下的相位恢复重建有重要作用。

Abstract: Phase retrieval seeks to recover a complex signal from amplitude-only
measurements, a challenging nonlinear inverse problem. Current theory and
algorithms often ignore signal priors. By contrast, we evaluate here a variety
of image priors in the context of severe undersampling with structured random
Fourier measurements. Our results show that those priors significantly improve
reconstruction, allowing accurate reconstruction even below the weak recovery
threshold.

</details>


### [250] [Learning Mechanistic Subtypes of Neurodegeneration with a Physics-Informed Variational Autoencoder Mixture Model](https://arxiv.org/abs/2509.15124)
*Sanduni Pinnawala,Annabelle Hartanto,Ivor J. A. Simpson,Peter A. Wijeratne*

Main category: eess.IV

TL;DR: 提出一种基于物理偏微分方程的深度生成模型，能从神经影像数据中推断可解释潜变量亚型，在合成基准和阿尔茨海默病数据上验证了效果。


<details>
  <summary>Details</summary>
Motivation: 现有物理集成机器学习方法仅考虑单个偏微分方程，限制了对多机制疾病的应用，存在模型误设和退化问题。

Method: 在变分自编码器混合模型框架中集成反应 - 扩散偏微分方程，支持从神经影像数据中推断可解释潜变量亚型。

Result: 在合成基准上进行了评估，展示了从正电子发射断层扫描数据中揭示阿尔茨海默病进展机制亚型的潜力。

Conclusion: 所提出的模型超越了传统假设单一偏微分方程结构的方法，可用于分析神经退行性疾病的潜在机制。

Abstract: Modelling the underlying mechanisms of neurodegenerative diseases demands
methods that capture heterogeneous and spatially varying dynamics from sparse,
high-dimensional neuroimaging data. Integrating partial differential equation
(PDE) based physics knowledge with machine learning provides enhanced
interpretability and utility over classic numerical methods. However, current
physics-integrated machine learning methods are limited to considering a single
PDE, severely limiting their application to diseases where multiple mechanisms
are responsible for different groups (i.e., subtypes) and aggravating problems
with model misspecification and degeneracy. Here, we present a deep generative
model for learning mixtures of latent dynamic models governed by physics-based
PDEs, going beyond traditional approaches that assume a single PDE structure.
Our method integrates reaction-diffusion PDEs within a variational autoencoder
(VAE) mixture model framework, supporting inference of subtypes of
interpretable latent variables (e.g. diffusivity and reaction rates) from
neuroimaging data. We evaluate our method on synthetic benchmarks and
demonstrate its potential for uncovering mechanistic subtypes of Alzheimer's
disease progression from positron emission tomography (PET) data.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [251] [Inspired by machine learning optimization: can gradient-based optimizers solve cycle skipping in full waveform inversion given sufficient iterations?](https://arxiv.org/abs/2509.14919)
*Xinru Mu,Omar M. Saad,Shaowen Wang,Tariq Alkhalifah*

Main category: physics.geo-ph

TL;DR: 本文提出用较大学习率的基于梯度优化器进行全波形反演（FWI），实验表明虽初始可能收敛到局部极小值，但经足够迭代可接近全局极小值得到准确速度模型，低频数据缺失时也能有合理反演结果。


<details>
  <summary>Details</summary>
Motivation: 传统FWI用局部优化方法在初始速度模型不准确且低频数据缺失时易陷入局部极小值，导致反演结果不准确，需改进。

Method: 采用具有相对较大学习率的基于梯度的优化器进行FWI。

Result: 合成和野外数据实验显示FWI初始可能收敛到局部极小值，经足够迭代可从浅到深逐渐接近全局极小值得到准确速度模型，低频数据缺失时也能有合理反演结果。

Conclusion: 使用较大学习率的基于梯度优化器进行FWI，经足够迭代可有效解决局部极小值问题，在低频数据缺失时也能实现较准确的速度反演。

Abstract: Full waveform inversion (FWI) iteratively updates the velocity model by
minimizing the difference between observed and simulated data. Due to the high
computational cost and memory requirements associated with global optimization
algorithms, FWI is typically implemented using local optimization methods.
However, when the initial velocity model is inaccurate and low-frequency
seismic data (e.g., below 3 Hz) are absent, the mismatch between simulated and
observed data may exceed half a cycle, a phenomenon known as cycle skipping. In
such cases, local optimization algorithms (e.g., gradient-based local
optimizers) tend to converge to local minima, leading to inaccurate inversion
results. In machine learning, neural network training is also an optimization
problem prone to local minima. It often employs gradient-based optimizers with
a relatively large learning rate (beyond the theoretical limits of local
optimization that are usually determined numerically by a line search), which
allows the optimization to behave like a quasi-global optimizer. Consequently,
after training for several thousand iterations, we can obtain a neural network
model with strong generative capability. In this study, we also employ
gradient-based optimizers with a relatively large learning rate for FWI.
Results from both synthetic and field data experiments show that FWI may
initially converge to a local minimum; however, with sufficient additional
iterations, the inversion can gradually approach the global minimum, slowly
from shallow subsurface to deep, ultimately yielding an accurate velocity
model. Furthermore, numerical examples indicate that, given sufficient
iterations, reasonable velocity inversion results can still be achieved even
when low-frequency data below 5 Hz are missing.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [252] [Efficient Computation of Time-Index Powered Weighted Sums Using Cascaded Accumulators](https://arxiv.org/abs/2509.15069)
*Deijany Rodriguez Linares,Oksana Moryakova,Håkan Johansson*

Main category: eess.SP

TL;DR: 提出用级联累加器高效计算时间指数加权和的新方法，减少乘法成本且无需数据块存储。


<details>
  <summary>Details</summary>
Motivation: 传统直接计算时间指数加权和乘法次数多，替代策略需存储整个数据块，成本高。

Method: 利用累加器特性，用级联累加器计算。

Result: 消除数据块存储需求，将乘法成本降至K + 1次常数乘法。

Conclusion: 该方法适用于逐样本处理系统中高效计算此类加权和。

Abstract: This letter presents a novel approach for \mbox{efficiently} computing
time-index powered weighted sums of the form $\sum_{n=0}^{N-1} n^{K} v[n]$
using cascaded accumulators. Traditional direct computation requires
$K{\times}N$ general multiplications, which become prohibitive for large $N$,
while alternative strategies based on lookup tables or signal reversal require
storing entire data blocks. By exploiting accumulator properties, the proposed
method eliminates the need for such storage and reduces the multiplicative cost
to only $K{+}1$ constant multiplications, enabling efficient real-time
implementation. The approach is particularly useful when such sums need to be
efficiently computed in sample-by-sample processing systems.

</details>


### [253] [Novel Phase-Noise-Tolerant Variational-Autoencoder-Based Equalization Suitable for Space-Division-Multiplexed Transmission](https://arxiv.org/abs/2509.14072)
*Vincent Lauinger,Lennart Schmitz,Patrick Matalla,Andrej Rode,Sebastian Randel,Laurent Schmalen*

Main category: eess.SP

TL;DR: 本文展示了一种基于变分自编码器的新型相位噪声容忍均衡方案在150公里随机耦合多芯光纤的空分复用传输实验中的有效性。


<details>
  <summary>Details</summary>
Motivation: 提高空分复用传输性能，解决相位噪声问题。

Method: 采用基于变分自编码器的新型相位噪声容忍均衡方案。

Result: 在150公里随机耦合多芯光纤的空分复用传输实验中验证了方案的有效性。

Conclusion: 所提出的方案在空分复用传输中有应用潜力。

Abstract: We demonstrate the effectiveness of a novel phase-noise-tolerant,
variational-autoencoder-based equalization scheme for
space-division-multiplexed (SDM) transmission in an experiment over 150km of
randomly-coupled multi-core fibers.

</details>


### [254] [Artificial Intelligence-derived Cardiotocography Age as a Digital Biomarker for Predicting Future Adverse Pregnancy Outcomes](https://arxiv.org/abs/2509.14242)
*Jinshuai Gu,Zenghui Lin,Jingying Ma,Jingyu Wang,Linyan Zhang,Rui Bai,Zelin Tu,Youyou Jiang,Donglin Xie,Yuxi Zhou,Guoli Liu,Shenda Hong*

Main category: eess.SP

TL;DR: 本文开发基于AI的CTGage模型预测胎儿生物年龄，计算CTGage-gap作为新数字生物标志物，发现其能预测不良妊娠结局风险。


<details>
  <summary>Details</summary>
Motivation: 当前CTG在预测未来不良妊娠结局方面潜力未充分挖掘，旨在开发新模型及生物标志物。

Method: 利用11385名孕妇的61140条记录，用1D卷积神经网络和分布对齐增强回归技术训练CTGage模型，将CTGage-gap分组并比较不良结局和母体疾病发生率。

Result: CTGage模型平均绝对误差10.91天，不同分组间早产、GDM、低体重儿、贫血等发生率有显著差异。

Conclusion: AI衍生的CTGage可预测不良妊娠结局风险，有望成为新型无创易获取数字生物标志物。

Abstract: Cardiotocography (CTG) is a low-cost, non-invasive fetal health assessment
technique used globally, especially in underdeveloped countries. However, it is
currently mainly used to identify the fetus's current status (e.g., fetal
acidosis or hypoxia), and the potential of CTG in predicting future adverse
pregnancy outcomes has not been fully explored. We aim to develop an AI-based
model that predicts biological age from CTG time series (named CTGage), then
calculate the age gap between CTGage and actual age (named CTGage-gap), and use
this gap as a new digital biomarker for future adverse pregnancy outcomes. The
CTGage model is developed using 61,140 records from 11,385 pregnant women,
collected at Peking University People's Hospital between 2018 and 2022. For
model training, a structurally designed 1D convolutional neural network is
used, incorporating distribution-aligned augmented regression technology. The
CTGage-gap is categorized into five groups: < -21 days (underestimation group),
-21 to -7 days, -7 to 7 days (normal group), 7 to 21 days, and > 21 days
(overestimation group). We further defined the underestimation group and
overestimation group together as the high-risk group. We then compare the
incidence of adverse outcomes and maternal diseases across these groups. The
average absolute error of the CTGage model is 10.91 days. When comparing the
overestimation group with the normal group, premature infants incidence is
5.33% vs. 1.42% (p < 0.05) and gestational diabetes mellitus (GDM) incidence is
31.93% vs. 20.86% (p < 0.05). When comparing the underestimation group with the
normal group, low birth weight incidence is 0.17% vs. 0.15% (p < 0.05) and
anaemia incidence is 37.51% vs. 34.74% (p < 0.05). Artificial
intelligence-derived CTGage can predict the future risk of adverse pregnancy
outcomes and hold potential as a novel, non-invasive, and easily accessible
digital biomarker.

</details>


### [255] [Indoor Airflow Imaging Using Physics-Informed Background-Oriented Schlieren Tomography](https://arxiv.org/abs/2509.14442)
*Arjun Teh,Wael H. Ali,Joshua Rapp,Hassan Mansour*

Main category: eess.SP

TL;DR: 开发了基于单视角BOS测量和物理信息重建的非侵入式室内气流体积估计框架。


<details>
  <summary>Details</summary>
Motivation: 解决单视角BOS断层成像问题，实现非侵入式室内气流体积估计。

Method: 利用光投影仪和相机，采用改进的射线追踪、基于物理的光渲染方法和损失公式、基于物理信息神经网络的正则化。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: We develop a framework for non-invasive volumetric indoor airflow estimation
from a single viewpoint using background-oriented schlieren (BOS) measurements
and physics-informed reconstruction. Our framework utilizes a light projector
that projects a pattern onto a target back-wall and a camera that observes
small distortions in the light pattern. While the single-view BOS tomography
problem is severely ill-posed, our proposed framework addresses this using: (1)
improved ray tracing, (2) a physics-based light rendering approach and loss
formulation, and (3) a physics-based regularization using a physics-informed
neural network (PINN) to ensure that the reconstructed airflow is consistent
with the governing equations for buoyancy-driven flows.

</details>


### [256] [Radiolunadiff: Estimation of wireless network signal strength in lunar terrain](https://arxiv.org/abs/2509.14559)
*Paolo Torrado,Anders Pearson,Jason Klein,Alexander Moscibroda,Joshua Smith*

Main category: eess.SP

TL;DR: 提出用于预测月球地形无线电地图的物理信息深度学习架构，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为预测月球地形上的无线电地图。

Method: 将基于物理的月球地形生成器与射线追踪引擎结合创建数据集，引入由两个标准 UNet 和一个扩散网络组成的 triplet - UNet 架构。

Result: 在各种指标上，该方法在地形数据集上优于现有深度学习方法。

Conclusion: 所提出的物理信息深度学习架构能有效用于预测月球地形无线电地图，表现优于现有方法。

Abstract: In this paper, we propose a novel physics-informed deep learning architecture
for predicting radio maps over lunar terrain. Our approach integrates a
physics-based lunar terrain generator, which produces realistic topography
informed by publicly available NASA data, with a ray-tracing engine to create a
high-fidelity dataset of radio propagation scenarios. Building on this dataset,
we introduce a triplet-UNet architecture, consisting of two standard UNets and
a diffusion network, to model complex propagation effects. Experimental results
demonstrate that our method outperforms existing deep learning approaches on
our terrain dataset across various metrics.

</details>


### [257] [Sampling Method for Generalized Graph Signals with Pre-selected Vertices via DC Optimization](https://arxiv.org/abs/2509.14836)
*Keitaro Yamashita,Kazuki Naganuma,Shunsuke Ono*

Main category: eess.SP

TL;DR: 本文提出基于广义采样理论的图信号顶点灵活采样方法，通过优化问题设计采样算子，将问题转化为DC优化问题并开发收敛求解器，实验证明其恢复精度优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有顶点灵活采样方法无法结合特定顶点的先验知识，本文旨在解决该问题。

Method: 将采样算子设计问题转化为处理活动顶点数量约束和特定顶点先验知识的问题，再用核范数和DC惩罚将其转化为DC优化问题，开发基于双近端梯度DC算法的收敛求解器。

Result: 在多种图信号模型（包括真实数据）的实验中，该方法恢复精度优于现有方法。

Conclusion: 所提方法在图信号顶点灵活采样的恢复精度上表现出色，有较好效果。

Abstract: This paper proposes a method for vertex-wise flexible sampling of a broad
class of graph signals, designed to attain the best possible recovery based on
the generalized sampling theory. This is achieved by designing a sampling
operator by an optimization problem, which is inherently non-convex, as the
best possible recovery imposes a rank constraint. An existing method for
vertex-wise flexible sampling is able to control the number of active vertices
but cannot incorporate prior knowledge of mandatory or forbidden vertices. To
address these challenges, we formulate the operator design as a problem that
handles a constraint of the number of active vertices and prior knowledge on
specific vertices for sampling, mandatory inclusion or exclusion. We
transformed this constrained problem into a difference-of-convex (DC)
optimization problem by using the nuclear norm and a DC penalty for vertex
selection. To solve this, we develop a convergent solver based on the general
double-proximal gradient DC algorithm. The effectiveness of our method is
demonstrated through experiments on various graph signal models, including
real-world data, showing superior performance in the recovery accuracy by
comparing to existing methods.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [258] [Near-Real-Time Resource Slicing for QoS Optimization in 5G O-RAN using Deep Reinforcement Learning](https://arxiv.org/abs/2509.14343)
*Peihao Yan,Jie Lu,Huacheng Zeng,Y. Thomas Hou*

Main category: eess.SY

TL;DR: 本文提出用于5G O - RAN近实时RIC的xApp xSlice，它是在线学习算法，能自适应调整MAC层资源分配，实验显示比现有方案降低67%性能遗憾。


<details>
  <summary>Details</summary>
Motivation: 应对5G及未来无线接入网中动态网络状态，如时变无线信道条件、用户移动性、流量波动和用户需求变化等，优化服务质量。

Method: 先将QoS优化问题转化为遗憾最小化问题，构建深度强化学习框架，结合基于值和基于策略的更新方法，引入图卷积网络处理动态数量的流量会话。

Result: 在O - RAN测试床实验表明，xSlice相比现有方案可降低67%的性能遗憾。

Conclusion: xSlice能有效应对网络动态，在资源分配和QoS优化上相比现有方案有显著优势，源代码可在GitHub获取。

Abstract: Open-Radio Access Network (O-RAN) has become an important paradigm for 5G and
beyond radio access networks. This paper presents an xApp called xSlice for the
Near-Real-Time (Near-RT) RAN Intelligent Controller (RIC) of 5G O-RANs. xSlice
is an online learning algorithm that adaptively adjusts MAC-layer resource
allocation in response to dynamic network states, including time-varying
wireless channel conditions, user mobility, traffic fluctuations, and changes
in user demand. To address these network dynamics, we first formulate the
Quality-of-Service (QoS) optimization problem as a regret minimization problem
by quantifying the QoS demands of all traffic sessions through weighting their
throughput, latency, and reliability. We then develop a deep reinforcement
learning (DRL) framework that utilizes an actor-critic model to combine the
advantages of both value-based and policy-based updating methods. A graph
convolutional network (GCN) is incorporated as a component of the DRL framework
for graph embedding of RAN data, enabling xSlice to handle a dynamic number of
traffic sessions. We have implemented xSlice on an O-RAN testbed with 10
smartphones and conducted extensive experiments to evaluate its performance in
realistic scenarios. Experimental results show that xSlice can reduce
performance regret by 67% compared to the state-of-the-art solutions. Source
code is available on GitHub [1].

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [259] [TypedSchematics: A Block-based PCB Design Tool with Real-time Detection of Common Connection Errors](https://arxiv.org/abs/2509.14576)
*Jorge Garza,Steven Swanson*

Main category: cs.HC

TL;DR: 提出TypedSchematics工具解决PCB设计中电路块复用问题，经用户研究表明其优于Fusion 360，高中生设计案例显示可降低设计门槛。


<details>
  <summary>Details</summary>
Motivation: 现有PCB设计平台缺乏安全合并电路块的技术，阻碍初学者复用专家设计。

Method: 提出TypedSchematics工具，为电路块提供语言语法处理电路数据。

Result: 用户研究显示TypedSchematics在合并电路块设计支持上优于Fusion 360，三个设计案例展示工具能力。

Conclusion: TypedSchematics可显著降低PCB设计技能门槛。

Abstract: Within PCB design, the reuse of circuit design blocks is a major preventing
factor inhibiting beginners from reusing designs made by experts, a common
practice in software but non-existent in circuit design at large. Despite
efforts to improve reusability (e.g. block-based PCB design) by platforms such
as SparkFun ALC and Altium Upverter, they lack merging techniques that safely
guide users in connecting different circuit blocks without requiring assistance
from third-party engineers. In this paper, we propose TypedSchematics, a
block-based standalone PCB design tool that supports beginners create their own
PCBs by providing a language syntax for typing circuit blocks with circuit data
that addresses multiple challenges, from real-time detection of connection
errors to automated composition and user-scalable libraries of circuit blocks.
Through a user study, we demonstrate TypedSchematics improvements in design
support for merging circuit blocks compared to Fusion 360. Three PCBs designed
with TypedSchematics further showcase our tool capabilities, one designed by
high school students demonstrates the potential of TypedSchematics to
significantly lower the PCB design skill-floor.

</details>


### [260] [ClearFairy: Capturing Creative Workflows through Decision Structuring, In-Situ Questioning, and Rationale Inference](https://arxiv.org/abs/2509.14537)
*Kihoon Son,DaEun Choi,Tae Soo Kim,Young-Ho Kim,Sangdoo Yun,Juho Kim*

Main category: cs.HC

TL;DR: 提出CLEAR框架和ClearFairy助手，研究显示其有效提升决策解释质量并助力生成式AI，还发布数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉创意工作流中专业人员决策时存在决策理由不完整、隐含决策被隐藏的问题。

Method: 提出CLEAR框架，构建ClearFairy助手，开展有12位创意专业人员参与的研究。

Result: 85%的推断理由被接受，强解释比例从14%提升到超83%，增强了生成式AI在Figma中的表现。

Conclusion: 该框架和助手能有效提升创意工作流中决策的可追溯性和知识共享，为未来研究发布了数据集。

Abstract: Capturing professionals' decision-making in creative workflows is essential
for reflection, collaboration, and knowledge sharing, yet existing methods
often leave rationales incomplete and implicit decisions hidden. To address
this, we present CLEAR framework that structures reasoning into cognitive
decision steps-linked units of actions, artifacts, and self-explanations that
make decisions traceable. Building on this framework, we introduce ClearFairy,
a think-aloud AI assistant for UI design that detects weak explanations, asks
lightweight clarifying questions, and infers missing rationales to ease the
knowledge-sharing burden. In a study with twelve creative professionals, 85% of
ClearFairy's inferred rationales were accepted, increasing strong explanations
from 14% to over 83% of decision steps without adding cognitive demand. The
captured steps also enhanced generative AI agents in Figma, yielding
next-action predictions better aligned with professionals and producing more
coherent design outcomes. For future research on human knowledge-grounded
creative AI agents, we release a dataset of captured 417 decision steps.

</details>


### [261] [VisMoDAl: Visual Analytics for Evaluating and Improving Corruption Robustness of Vision-Language Models](https://arxiv.org/abs/2509.14571)
*Huanchen Wang,Wencheng Zhang,Zhiqiang Wang,Zhicong Lu,Yuxin Ma*

Main category: cs.HC

TL;DR: 提出可视化分析框架VisMoDAl评估视觉语言模型鲁棒性，通过案例和定量评估验证其效用。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在分布偏移下性能下降，现有方法缺乏对模型行为的深入理解，需要专业知识和迭代探索数据模式，因此需评估和提高其对现实数据损坏的鲁棒性。

Method: 引入VisMoDAl可视化分析框架，支持多层次分析，帮助用户理解损坏对模型的影响，制定数据增强策略。

Result: 通过图像字幕任务的案例研究和定量评估展示了系统的效用。

Conclusion: VisMoDAl框架有助于评估视觉语言模型鲁棒性，理解模型行为并制定有效数据增强策略。

Abstract: Vision-language (VL) models have shown transformative potential across
various critical domains due to their capability to comprehend multi-modal
information. However, their performance frequently degrades under distribution
shifts, making it crucial to assess and improve robustness against real-world
data corruption encountered in practical applications. While advancements in VL
benchmark datasets and data augmentation (DA) have contributed to robustness
evaluation and improvement, there remain challenges due to a lack of in-depth
comprehension of model behavior as well as the need for expertise and iterative
efforts to explore data patterns. Given the achievement of visualization in
explaining complex models and exploring large-scale data, understanding the
impact of various data corruption on VL models aligns naturally with a visual
analytics approach. To address these challenges, we introduce VisMoDAl, a
visual analytics framework designed to evaluate VL model robustness against
various corruption types and identify underperformed samples to guide the
development of effective DA strategies. Grounded in the literature review and
expert discussions, VisMoDAl supports multi-level analysis, ranging from
examining performance under specific corruptions to task-driven inspection of
model behavior and corresponding data slice. Unlike conventional works,
VisMoDAl enables users to reason about the effects of corruption on VL models,
facilitating both model behavior understanding and DA strategy formulation. The
utility of our system is demonstrated through case studies and quantitative
evaluations focused on corruption robustness in the image captioning task.

</details>


### [262] [Can I Trust This Chatbot? Assessing User Privacy in AI-Healthcare Chatbot Applications](https://arxiv.org/abs/2509.14581)
*Ramazan Yener,Guan-Hung Chen,Ece Gumusel,Masooda Bashir*

Main category: cs.HC

TL;DR: 研究评估美国应用商店中12款热门AI医疗聊天机器人应用的隐私实践，发现数据保护存在重大差距，为相关方提供改进隐私保护的见解。


<details>
  <summary>Details</summary>
Motivation: 随着AI聊天机器人在医疗领域应用增多，其处理敏感健康数据带来隐私问题，此前针对AI医疗聊天机器人隐私问题研究有限。

Method: 对12款应用进行三步评估，包括注册时隐私设置、应用内隐私控制和隐私政策内容分析。

Result: 一半应用注册时未提供隐私政策，仅两款允许禁用数据共享，多数隐私政策未提及数据保护措施，用户对个人数据控制极少。

Conclusion: 研究为信息科学研究者、开发者和政策制定者改进AI医疗聊天机器人应用隐私保护提供关键见解。

Abstract: As Conversational Artificial Intelligence (AI) becomes more integrated into
everyday life, AI-powered chatbot mobile applications are increasingly adopted
across industries, particularly in the healthcare domain. These chatbots offer
accessible and 24/7 support, yet their collection and processing of sensitive
health data present critical privacy concerns. While prior research has
examined chatbot security, privacy issues specific to AI healthcare chatbots
have received limited attention. Our study evaluates the privacy practices of
12 widely downloaded AI healthcare chatbot apps available on the App Store and
Google Play in the United States. We conducted a three-step assessment
analyzing: (1) privacy settings during sign-up, (2) in-app privacy controls,
and (3) the content of privacy policies. The analysis identified significant
gaps in user data protection. Our findings reveal that half of the examined
apps did not present a privacy policy during sign up, and only two provided an
option to disable data sharing at that stage. The majority of apps' privacy
policies failed to address data protection measures. Moreover, users had
minimal control over their personal data. The study provides key insights for
information science researchers, developers, and policymakers to improve
privacy protections in AI healthcare chatbot apps.

</details>


### [263] [Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech](https://arxiv.org/abs/2509.14627)
*Taesoo Kim,Yongsik Jo,Hyunmin Song,Taehwan Kim*

Main category: cs.HC

TL;DR: 提出基于对话情绪和响应风格生成语音的类人代理，构建新数据集，用多模态模型生成文本和语音描述，实验证明多模态生成语音有效，代码开源。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型较少关注生成自然且有吸引力的语音，需在对话中更好地利用多模态信息生成语音。

Method: 构建专注于语音的MultiSensory Conversation数据集，提出基于多模态大语言模型的模型生成文本响应和语音描述。

Result: 实验证明利用视觉和音频模态在对话中生成有吸引力的语音是有效的。

Conclusion: 基于多模态信息生成语音的方法可行，可利用视觉和音频模态生成有吸引力的语音。

Abstract: Human conversation involves language, speech, and visual cues, with each
medium providing complementary information. For instance, speech conveys a vibe
or tone not fully captured by text alone. While multimodal LLMs focus on
generating text responses from diverse inputs, less attention has been paid to
generating natural and engaging speech. We propose a human-like agent that
generates speech responses based on conversation mood and responsive style
information. To achieve this, we build a novel MultiSensory Conversation
dataset focused on speech to enable agents to generate natural speech. We then
propose a multimodal LLM-based model for generating text responses and voice
descriptions, which are used to generate speech covering paralinguistic
information. Experimental results demonstrate the effectiveness of utilizing
both visual and audio modalities in conversation to generate engaging speech.
The source code is available in https://github.com/kimtaesu24/MSenC

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [264] [Chain-of-Thought Re-ranking for Image Retrieval Tasks](https://arxiv.org/abs/2509.14746)
*Shangrong Wu,Yanghong Zhou,Yang Chen,Feng Zhang,P. Y. Mok*

Main category: cs.CV

TL;DR: 提出CoTRR方法解决图像检索问题，让MLLM直接参与重排序，在多数据集实验中取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用MLLMs推理能力参与排序，导致图像检索性能不佳。

Method: 设计列表式排序提示使MLLM直接参与重排序，基于图像评估提示评估候选图像，引入查询解构提示进行细粒度分析。

Result: 在五个数据集上实验，在三个图像检索任务中取得SOTA性能。

Conclusion: CoTRR方法有效，可实现准确图像检索。

Abstract: Image retrieval remains a fundamental yet challenging problem in computer
vision. While recent advances in Multimodal Large Language Models (MLLMs) have
demonstrated strong reasoning capabilities, existing methods typically employ
them only for evaluation, without involving them directly in the ranking
process. As a result, their rich multimodal reasoning abilities remain
underutilized, leading to suboptimal performance. In this paper, we propose a
novel Chain-of-Thought Re-Ranking (CoTRR) method to address this issue.
Specifically, we design a listwise ranking prompt that enables MLLM to directly
participate in re-ranking candidate images. This ranking process is grounded in
an image evaluation prompt, which assesses how well each candidate aligns with
users query. By allowing MLLM to perform listwise reasoning, our method
supports global comparison, consistent reasoning, and interpretable
decision-making - all of which are essential for accurate image retrieval. To
enable structured and fine-grained analysis, we further introduce a query
deconstruction prompt, which breaks down the original query into multiple
semantic components. Extensive experiments on five datasets demonstrate the
effectiveness of our CoTRR method, which achieves state-of-the-art performance
across three image retrieval tasks, including text-to-image retrieval (TIR),
composed image retrieval (CIR) and chat-based image retrieval (Chat-IR). Our
code is available at https://github.com/freshfish15/CoTRR .

</details>


### [265] [Template-Based Cortical Surface Reconstruction with Minimal Energy Deformation](https://arxiv.org/abs/2509.14827)
*Patrick Madlindl,Fabian Bongratz,Christian Wachinger*

Main category: cs.CV

TL;DR: 本文设计MED损失函数用于皮质表面重建，提升训练一致性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 基于学习的皮质表面重建虽加速处理，但确保变形最优和训练一致性有挑战。

Method: 设计Minimal Energy Deformation (MED)损失函数作为变形轨迹正则化器，并入V2C - Flow模型。

Result: 在不损害重建精度和拓扑正确性的前提下，显著提升训练一致性和可重复性。

Conclusion: MED损失函数能有效解决皮质表面重建中训练一致性和可重复性问题。

Abstract: Cortical surface reconstruction (CSR) from magnetic resonance imaging (MRI)
is fundamental to neuroimage analysis, enabling morphological studies of the
cerebral cortex and functional brain mapping. Recent advances in learning-based
CSR have dramatically accelerated processing, allowing for reconstructions
through the deformation of anatomical templates within seconds. However,
ensuring the learned deformations are optimal in terms of deformation energy
and consistent across training runs remains a particular challenge. In this
work, we design a Minimal Energy Deformation (MED) loss, acting as a
regularizer on the deformation trajectories and complementing the widely used
Chamfer distance in CSR. We incorporate it into the recent V2C-Flow model and
demonstrate considerable improvements in previously neglected training
consistency and reproducibility without harming reconstruction accuracy and
topological correctness.

</details>


### [266] [AToken: A Unified Tokenizer for Vision](https://arxiv.org/abs/2509.14476)
*Jiasen Lu,Liangchen Song,Mingze Xu,Byeongjoo Ahn,Yanjun Wang,Chen Chen,Afshin Dehghan,Yinfei Yang*

Main category: cs.CV

TL;DR: 提出首个统一视觉分词器AToken，可跨图像、视频和3D资产实现高保真重建和语义理解，性能出色，为下一代多模态AI系统提供思路。


<details>
  <summary>Details</summary>
Motivation: 现有分词器多针对单一模态的重建或理解，缺乏能统一多模态和多任务的视觉分词器。

Method: 将不同视觉输入编码到共享4D潜在空间，采用带4D旋转位置嵌入的纯Transformer架构，结合感知和Gram矩阵损失的无对抗训练目标，使用渐进式训练策略。

Result: 在图像、视频和3D任务上取得良好指标，如图像rFID为0.21、ImageNet准确率82.2%等；在下游生成和理解任务中表现有竞争力。

Conclusion: 基于统一视觉分词的AToken为下一代多模态AI系统带来启示。

Abstract: We present AToken, the first unified visual tokenizer that achieves both
high-fidelity reconstruction and semantic understanding across images, videos,
and 3D assets. Unlike existing tokenizers that specialize in either
reconstruction or understanding for single modalities, AToken encodes these
diverse visual inputs into a shared 4D latent space, unifying both tasks and
modalities in a single framework. Specifically, we introduce a pure transformer
architecture with 4D rotary position embeddings to process visual inputs of
arbitrary resolutions and temporal durations. To ensure stable training, we
introduce an adversarial-free training objective that combines perceptual and
Gram matrix losses, achieving state-of-the-art reconstruction quality. By
employing a progressive training curriculum, AToken gradually expands from
single images, videos, and 3D, and supports both continuous and discrete latent
tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01
rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9%
classification accuracy for 3D. In downstream applications, AToken enables both
visual generation tasks (e.g., image generation with continuous and discrete
tokens, text-to-video generation, image-to-3D synthesis) and understanding
tasks (e.g., multimodal LLMs), achieving competitive performance across all
benchmarks. These results shed light on the next-generation multimodal AI
systems built upon unified visual tokenization.

</details>


### [267] [Do Vision-Language Models See Urban Scenes as People Do? An Urban Perception Benchmark](https://arxiv.org/abs/2509.14574)
*Rashid Mushkani*

Main category: cs.CV

TL;DR: 本文引入一个小基准测试，用蒙特利尔街道图像测试视觉 - 语言模型在城市感知上的表现，评估了七个模型，给出结果并发布基准等资源。


<details>
  <summary>Details</summary>
Motivation: 理解人们如何解读城市场景可为设计和规划提供信息，测试视觉 - 语言模型在城市感知方面的能力。

Method: 使用100张蒙特利尔街道图像（照片和合成场景各半），12名参与者提供230份注释表单，用结构化提示和确定性解析器在零样本设置下评估七个视觉 - 语言模型，用不同指标衡量结果。

Result: 模型在可见客观属性上的对齐性强于主观评价，顶级系统在多标签项目上有一定得分，人类一致性高时模型得分更好，合成图像会略微降低得分。

Conclusion: 发布基准、提示和工具，用于参与式城市分析中可重复、考虑不确定性的评估。

Abstract: Understanding how people read city scenes can inform design and planning. We
introduce a small benchmark for testing vision-language models (VLMs) on urban
perception using 100 Montreal street images, evenly split between photographs
and photorealistic synthetic scenes. Twelve participants from seven community
groups supplied 230 annotation forms across 30 dimensions mixing physical
attributes and subjective impressions. French responses were normalized to
English. We evaluated seven VLMs in a zero-shot setup with a structured prompt
and deterministic parser. We use accuracy for single-choice items and Jaccard
overlap for multi-label items; human agreement uses Krippendorff's alpha and
pairwise Jaccard. Results suggest stronger model alignment on visible,
objective properties than subjective appraisals. The top system (claude-sonnet)
reaches macro 0.31 and mean Jaccard 0.48 on multi-label items. Higher human
agreement coincides with better model scores. Synthetic images slightly lower
scores. We release the benchmark, prompts, and harness for reproducible,
uncertainty-aware evaluation in participatory urban analysis.

</details>


### [268] [LSTC-MDA: A Unified Framework for Long-Short Term Temporal Convolution and Mixed Data Augmentation in Skeleton-Based Action Recognition](https://arxiv.org/abs/2509.14619)
*Feng Ding,Haisheng Fu,Soroush Oraki,Jie Liang*

Main category: cs.CV

TL;DR: 提出LSTC - MDA框架解决骨架动作识别中样本标签少和建模难问题，取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 解决骨架动作识别中标记训练样本稀缺和短长程时间依赖建模困难的问题。

Method: 提出LSTC - MDA框架，引入LSTC模块处理时间建模，扩展JMDA增强数据多样性。

Result: 在NTU 60、NTU 120、NW - UCLA等数据集上取得SOTA结果。

Conclusion: LSTC - MDA框架有效，各组件有贡献。

Abstract: Skeleton-based action recognition faces two longstanding challenges: the
scarcity of labeled training samples and difficulty modeling short- and
long-range temporal dependencies. To address these issues, we propose a unified
framework, LSTC-MDA, which simultaneously improves temporal modeling and data
diversity. We introduce a novel Long-Short Term Temporal Convolution (LSTC)
module with parallel short- and long-term branches, these two feature branches
are then aligned and fused adaptively using learned similarity weights to
preserve critical long-range cues lost by conventional stride-2 temporal
convolutions. We also extend Joint Mixing Data Augmentation (JMDA) with an
Additive Mixup at the input level, diversifying training samples and
restricting mixup operations to the same camera view to avoid distribution
shifts. Ablation studies confirm each component contributes. LSTC-MDA achieves
state-of-the-art results: 94.1% and 97.5% on NTU 60 (X-Sub and X-View), 90.4%
and 92.0% on NTU 120 (X-Sub and X-Set),97.2% on NW-UCLA. Code:
https://github.com/xiaobaoxia/LSTC-MDA.

</details>


### [269] [ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone Health Classification](https://arxiv.org/abs/2509.14830)
*Alvaro Lopez Pellicer,Andre Mariucci,Plamen Angelov,Marwan Bukhari,Jemma G. Kerns*

Main category: cs.CV

TL;DR: 提出多模态模型ProtoMedX用于骨骼健康分类，有可解释性且性能超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI在骨骼健康研究中多注重预测精度，忽视可解释性，而医学应用需要模型决策可解释。

Method: 提出ProtoMedX多模态模型，结合腰椎DEXA扫描和患者记录，采用基于原型的架构。

Result: 在4160名真实NHS患者数据集上，视觉单模态任务准确率87.58%，多模态变体准确率89.8%，超现有方法。

Conclusion: ProtoMedX在骨骼健康分类中性能优异，且其可解释性适合医学应用。

Abstract: Bone health studies are crucial in medical practice for the early detection
and treatment of Osteopenia and Osteoporosis. Clinicians usually make a
diagnosis based on densitometry (DEXA scans) and patient history. The
applications of AI in this field are ongoing research. Most successful methods
rely on deep learning models that use vision alone (DEXA/X-ray imagery) and
focus on prediction accuracy, while explainability is often disregarded and
left to post hoc assessments of input contributions. We propose ProtoMedX, a
multi-modal model that uses both DEXA scans of the lumbar spine and patient
records. ProtoMedX's prototype-based architecture is explainable by design,
which is crucial for medical applications, especially in the context of the
upcoming EU AI Act, as it allows explicit analysis of model decisions,
including incorrect ones. ProtoMedX demonstrates state-of-the-art performance
in bone health classification while also providing explanations that can be
visually understood by clinicians. Using a dataset of 4,160 real NHS patients,
the proposed ProtoMedX achieves 87.58% accuracy in vision-only tasks and 89.8%
in its multi-modal variant, both surpassing existing published methods.

</details>


### [270] [Not All Degradations Are Equal: A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution](https://arxiv.org/abs/2509.14841)
*Hongjun Wang,Jiyuan Chen,Zhengwei Yin,Xuan Song,Yinqiang Zheng*

Main category: cs.CV

TL;DR: 本文针对可泛化图像超分辨率问题，发现模型主要过拟合噪声，提出有针对性的特征去噪框架，在多个数据集上表现优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有提升可泛化图像超分辨率模型泛化能力的方法假设模型对所有退化类型过拟合，本文发现模型主要对噪声过拟合，为解决该问题展开研究。

Method: 提出包含噪声检测和去噪模块的有针对性的特征去噪框架，且该框架可无缝集成到现有超分辨率模型中，无需修改架构。

Result: 框架在五个传统基准和数据集（包括合成和真实场景）上的表现优于以往基于正则化的方法。

Conclusion: 所提的特征去噪框架能有效解决可泛化图像超分辨率中模型主要对噪声过拟合的问题，提升模型性能。

Abstract: Generalizable Image Super-Resolution aims to enhance model generalization
capabilities under unknown degradations. To achieve this goal, the models are
expected to focus only on image content-related features instead of overfitting
degradations. Recently, numerous approaches such as Dropout and Feature
Alignment have been proposed to suppress models' natural tendency to overfit
degradations and yield promising results. Nevertheless, these works have
assumed that models overfit to all degradation types (e.g., blur, noise, JPEG),
while through careful investigations in this paper, we discover that models
predominantly overfit to noise, largely attributable to its distinct
degradation pattern compared to other degradation types. In this paper, we
propose a targeted feature denoising framework, comprising noise detection and
denoising modules. Our approach presents a general solution that can be
seamlessly integrated with existing super-resolution models without requiring
architectural modifications. Our framework demonstrates superior performance
compared to previous regularization-based methods across five traditional
benchmarks and datasets, encompassing both synthetic and real-world scenarios.

</details>


### [271] [[Re] Improving Interpretation Faithfulness for Vision Transformers](https://arxiv.org/abs/2509.14846)
*Izabela Kurek,Wojciech Trejter,Stipe Frkovic,Andro Erdelez*

Main category: cs.CV

TL;DR: 本文复现FViTs结果，验证DDS提升可解释性鲁棒性的说法，测试不同方法，测量计算成本和环境影响，结果与原研究大致相符。


<details>
  <summary>Details</summary>
Motivation: 复现FViTs结果，验证使用DDS提升可解释性鲁棒性的说法，并拓展研究DDS对不同可解释性方法的作用，测量相关成本和影响。

Method: 复现FViTs结果，在分割和分类任务中验证DDS效果，在基线方法和Attribution Rollout方法上测试DDS，测量计算成本和环境影响。

Result: 结果与原研究大致相符，但存在一些细微差异。

Conclusion: 原研究关于DDS提升可解释性鲁棒性的结论基本成立，但存在细微差异。

Abstract: This work aims to reproduce the results of Faithful Vision Transformers
(FViTs) proposed by arXiv:2311.17983 alongside interpretability methods for
Vision Transformers from arXiv:2012.09838 and Xu (2022) et al. We investigate
claims made by arXiv:2311.17983, namely that the usage of Diffusion Denoised
Smoothing (DDS) improves interpretability robustness to (1) attacks in a
segmentation task and (2) perturbation and attacks in a classification task. We
also extend the original study by investigating the authors' claims that adding
DDS to any interpretability method can improve its robustness under attack.
This is tested on baseline methods and the recently proposed Attribution
Rollout method. In addition, we measure the computational costs and
environmental impact of obtaining an FViT through DDS. Our results broadly
agree with the original study's findings, although minor discrepancies were
found and discussed.

</details>


### [272] [MARIC: Multi-Agent Reasoning for Image Classification](https://arxiv.org/abs/2509.14860)
*Wonduk Seo,Minhyeong Yu,Hyunjin An,Seunghyun Lee*

Main category: cs.CV

TL;DR: 本文提出多智能体图像分类推理框架MARIC，在4个数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统图像分类依赖参数密集模型训练，现有视觉语言模型依赖单遍表示有局限，需改进。

Method: 将图像分类重构为协作推理过程，通过大纲智能体、方面智能体和推理智能体协作。

Result: 在4个不同图像分类基准数据集上，MARIC显著优于基线。

Conclusion: 多智能体视觉推理对鲁棒且可解释的图像分类有效。

Abstract: Image classification has traditionally relied on parameter-intensive model
training, requiring large-scale annotated datasets and extensive fine tuning to
achieve competitive performance. While recent vision language models (VLMs)
alleviate some of these constraints, they remain limited by their reliance on
single pass representations, often failing to capture complementary aspects of
visual content. In this paper, we introduce Multi Agent based Reasoning for
Image Classification (MARIC), a multi agent framework that reformulates image
classification as a collaborative reasoning process. MARIC first utilizes an
Outliner Agent to analyze the global theme of the image and generate targeted
prompts. Based on these prompts, three Aspect Agents extract fine grained
descriptions along distinct visual dimensions. Finally, a Reasoning Agent
synthesizes these complementary outputs through integrated reflection step,
producing a unified representation for classification. By explicitly
decomposing the task into multiple perspectives and encouraging reflective
synthesis, MARIC mitigates the shortcomings of both parameter-heavy training
and monolithic VLM reasoning. Experiments on 4 diverse image classification
benchmark datasets demonstrate that MARIC significantly outperforms baselines,
highlighting the effectiveness of multi-agent visual reasoning for robust and
interpretable image classification.

</details>


### [273] [Class-invariant Test-Time Augmentation for Domain Generalization](https://arxiv.org/abs/2509.14420)
*Zhicheng Lin,Xiaolin Wu,Xi Zhang*

Main category: cs.CV

TL;DR: 提出轻量级测试时增强策略CI - TTA应对分布偏移，在数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决深度模型在分布偏移下性能下降问题，现有方法依赖多域训练或高计算量测试时适应，需新策略。

Method: 开发CI - TTA技术，通过弹性和网格变形生成输入图像变体，用置信度引导过滤方案聚合预测。

Result: 在PACS和Office - Home数据集上，不同DG算法和骨干网络均有一致性能提升。

Conclusion: 提出的轻量级测试时增强策略有效且具有通用性。

Abstract: Deep models often suffer significant performance degradation under
distribution shifts. Domain generalization (DG) seeks to mitigate this
challenge by enabling models to generalize to unseen domains. Most prior
approaches rely on multi-domain training or computationally intensive test-time
adaptation. In contrast, we propose a complementary strategy: lightweight
test-time augmentation. Specifically, we develop a novel Class-Invariant
Test-Time Augmentation (CI-TTA) technique. The idea is to generate multiple
variants of each input image through elastic and grid deformations that
nevertheless belong to the same class as the original input. Their predictions
are aggregated through a confidence-guided filtering scheme that remove
unreliable outputs, ensuring the final decision relies on consistent and
trustworthy cues. Extensive Experiments on PACS and Office-Home datasets
demonstrate consistent gains across different DG algorithms and backbones,
highlighting the effectiveness and generality of our approach.

</details>


### [274] [RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching](https://arxiv.org/abs/2509.14966)
*Xingwu Zhang,Guanxuan Li,Zhuocheng Zhang,Zijun Long*

Main category: cs.CV

TL;DR: 大规模电商产品类别增多使仓库自动包装物体识别困难，提出RoboEye框架，提升召回率且降低成本。


<details>
  <summary>Details</summary>
Motivation: 大规模电商产品类别快速增长，仅依赖2D外观特征的方法性能下降，需更有效的物体识别方法。

Method: 提出两阶段识别框架RoboEye，第一阶段训练大视觉模型提取2D特征生成候选排名，用轻量级3D特征感知模块评估3D特征质量和判断是否进行3D重排序；第二阶段使用机器人3D检索变换器进行处理。

Result: RoboEye比现有技术RoboLLM的Recall@1提高7.1%，仅使用RGB图像，避免依赖显式3D输入，降低部署成本。

Conclusion: RoboEye是一种有效的物体识别框架，能提升识别性能并降低成本，代码已开源。

Abstract: The rapidly growing number of product categories in large-scale e-commerce
makes accurate object identification for automated packing in warehouses
substantially more difficult. As the catalog grows, intra-class variability and
a long tail of rare or visually similar items increase, and when combined with
diverse packaging, cluttered containers, frequent occlusion, and large
viewpoint changes-these factors amplify discrepancies between query and
reference images, causing sharp performance drops for methods that rely solely
on 2D appearance features. Thus, we propose RoboEye, a two-stage identification
framework that dynamically augments 2D semantic features with domain-adapted 3D
reasoning and lightweight adapters to bridge training deployment gaps. In the
first stage, we train a large vision model to extract 2D features for
generating candidate rankings. A lightweight 3D-feature-awareness module then
estimates 3D feature quality and predicts whether 3D re-ranking is necessary,
preventing performance degradation and avoiding unnecessary computation. When
invoked, the second stage uses our robot 3D retrieval transformer, comprising a
3D feature extractor that produces geometry-aware dense features and a
keypoint-based matcher that computes keypoint-correspondence confidences
between query and reference images instead of conventional cosine-similarity
scoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior
state of the art (RoboLLM). Moreover, RoboEye operates using only RGB images,
avoiding reliance on explicit 3D inputs and reducing deployment costs. The code
used in this paper is publicly available at:
https://github.com/longkukuhi/RoboEye.

</details>


### [275] [Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation](https://arxiv.org/abs/2509.15011)
*Vasiliki Ismiroglou,Malte Pedersen,Stefan H. Bengtson,Andreas Aakerberg,Thomas B. Moeslund*

Main category: cs.CV

TL;DR: 提出改进合成数据生成管道，考虑前向散射项和非均匀介质，收集BUCKET数据集，结果显示在高浊度下有定性改进。


<details>
  <summary>Details</summary>
Motivation: 现有水下图像合成数据方法常忽略高度浑浊环境中距离相关的可见度损失，作者希望改进。

Method: 提出包含前向散射项的改进合成数据生成管道，考虑非均匀介质，并收集BUCKET数据集。

Result: 在高浊度下相比参考模型有定性改进，调查参与者选择率达82.5%。

Conclusion: 改进的合成数据生成管道有效，尤其在高浊度环境。

Abstract: In recent years, the underwater image formation model has found extensive use
in the generation of synthetic underwater data. Although many approaches focus
on scenes primarily affected by discoloration, they often overlook the model's
ability to capture the complex, distance-dependent visibility loss present in
highly turbid environments. In this work, we propose an improved synthetic data
generation pipeline that includes the commonly omitted forward scattering term,
while also considering a nonuniform medium. Additionally, we collected the
BUCKET dataset under controlled turbidity conditions to acquire real turbid
footage with the corresponding reference images. Our results demonstrate
qualitative improvements over the reference model, particularly under
increasing turbidity, with a selection rate of 82. 5\% by survey participants.
Data and code can be accessed on the project page:
vap.aau.dk/sea-ing-through-scattered-rays.

</details>


### [276] [Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models](https://arxiv.org/abs/2509.15156)
*Haobo Yang,Minghao Guo,Dequan Yang,Wenyu Wang*

Main category: cs.CV

TL;DR: 本文将几何视错觉融入图像分类训练，评估多源学习策略，发现能提升泛化能力和结构敏感性，展示了感知科学与机器学习的新融合。


<details>
  <summary>Details</summary>
Motivation: 当代深度学习模型在图像分类中主要利用数据统计规律，很少结合感知心理学的结构化见解，因此探索感知驱动的归纳偏置的潜力。

Method: 引入合成的参数化几何视错觉数据集，评估三种将错觉识别任务与 ImageNet 分类目标相结合的多源学习策略。

Result: 将几何视错觉作为辅助监督可系统地提高泛化能力，感知驱动的归纳偏置能增强 CNN 和基于变压器架构的结构敏感性。

Conclusion: 展示了感知科学与机器学习的新融合，为将感知先验嵌入视觉模型设计指明了新方向。

Abstract: Contemporary deep learning models have achieved impressive performance in
image classification by primarily leveraging statistical regularities within
large datasets, but they rarely incorporate structured insights drawn directly
from perceptual psychology. To explore the potential of perceptually motivated
inductive biases, we propose integrating classic geometric visual illusions
well-studied phenomena from human perception into standard image-classification
training pipelines. Specifically, we introduce a synthetic, parametric
geometric-illusion dataset and evaluate three multi-source learning strategies
that combine illusion recognition tasks with ImageNet classification
objectives. Our experiments reveal two key conceptual insights: (i)
incorporating geometric illusions as auxiliary supervision systematically
improves generalization, especially in visually challenging cases involving
intricate contours and fine textures; and (ii) perceptually driven inductive
biases, even when derived from synthetic stimuli traditionally considered
unrelated to natural image recognition, can enhance the structural sensitivity
of both CNN and transformer-based architectures. These results demonstrate a
novel integration of perceptual science and machine learning and suggest new
directions for embedding perceptual priors into vision model design.

</details>


### [277] [Semi-Supervised 3D Medical Segmentation from 2D Natural Images Pretrained Model](https://arxiv.org/abs/2509.15167)
*Pak-Hei Yeung,Jayroop Ramesh,Pengfei Lyu,Ana Namburete,Jagath Rajapakse*

Main category: cs.CV

TL;DR: 本文探索将2D自然图像预训练通用视觉模型知识迁移到3D医学图像分割，提出M&N框架，在多数据集实验中表现优异且模型无关。


<details>
  <summary>Details</summary>
Motivation: 在半监督设置下，利用2D预训练模型知识提升3D医学图像分割效果，解决标注数据少的问题。

Method: 提出模型无关框架M&N，通过两模型迭代协同训练，使用伪掩码和学习率引导采样调整训练批次中数据比例。

Result: 在多个公开数据集上实验，M&N达到了最先进性能，优于十三种现有半监督分割方法。

Conclusion: M&N模型无关，可与不同架构集成，适应未来更先进模型。

Abstract: This paper explores the transfer of knowledge from general vision models
pretrained on 2D natural images to improve 3D medical image segmentation. We
focus on the semi-supervised setting, where only a few labeled 3D medical
images are available, along with a large set of unlabeled images. To tackle
this, we propose a model-agnostic framework that progressively distills
knowledge from a 2D pretrained model to a 3D segmentation model trained from
scratch. Our approach, M&N, involves iterative co-training of the two models
using pseudo-masks generated by each other, along with our proposed learning
rate guided sampling that adaptively adjusts the proportion of labeled and
unlabeled data in each training batch to align with the models' prediction
accuracy and stability, minimizing the adverse effect caused by inaccurate
pseudo-masks. Extensive experiments on multiple publicly available datasets
demonstrate that M&N achieves state-of-the-art performance, outperforming
thirteen existing semi-supervised segmentation approaches under all different
settings. Importantly, ablation studies show that M&N remains model-agnostic,
allowing seamless integration with different architectures. This ensures its
adaptability as more advanced models emerge. The code is available at
https://github.com/pakheiyeung/M-N.

</details>


### [278] [Synthetic-to-Real Object Detection using YOLOv11 and Domain Randomization Strategies](https://arxiv.org/abs/2509.15045)
*Luisa Torquato Niño,Hamza A. A. Gardi*

Main category: cs.CV

TL;DR: 论文聚焦目标检测中合成到真实的领域差距，用合成数据和域随机化策略训练YOLOv11检测汤罐，通过实验找到缩小差距方法，最佳配置获mAP@50为0.910。


<details>
  <summary>Details</summary>
Motivation: 解决目标检测中合成到真实的领域差距问题。

Method: 使用数据增强、数据集组成和模型缩放进行广泛实验，结合定性（视觉检查预测）和定量（手动标注真实测试集）评估模型。

Result: 增加合成数据集多样性并结合精心调整的数据增强可缩小领域差距，最佳配置的YOLOv11l模型在竞赛隐藏测试集上mAP@50达0.910。

Conclusion: 合成数据训练有潜力，但完全捕捉现实世界变异性仍有挑战。

Abstract: This paper addresses the synthetic-to-real domain gap in object detection,
focusing on training a YOLOv11 model to detect a specific object (a soup can)
using only synthetic data and domain randomization strategies. The methodology
involves extensive experimentation with data augmentation, dataset composition,
and model scaling. While synthetic validation metrics were consistently high,
they proved to be poor predictors of real-world performance. Consequently,
models were also evaluated qualitatively, through visual inspection of
predictions, and quantitatively, on a manually labeled real-world test set, to
guide development. Final mAP@50 scores were provided by the official Kaggle
competition. Key findings indicate that increasing synthetic dataset diversity,
specifically by including varied perspectives and complex backgrounds, combined
with carefully tuned data augmentation, were crucial in bridging the domain
gap. The best performing configuration, a YOLOv11l model trained on an expanded
and diverse dataset, achieved a final mAP@50 of 0.910 on the competition's
hidden test set. This result demonstrates the potential of a synthetic-only
training approach while also highlighting the remaining challenges in fully
capturing real-world variability.

</details>


### [279] [Out-of-Sight Trajectories: Tracking, Fusion, and Prediction](https://arxiv.org/abs/2509.15219)
*Haichao Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: 本文提出了针对视线外物体轨迹预测的OST任务，扩展OOSTraj适用范围，用增强的视觉定位去噪模块处理噪声数据，在数据集上取得SOTA表现，还进行了对比和基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹预测方法依赖完整无噪数据，未考虑视线外物体和传感器数据噪声问题，在现实场景存在安全风险和预测可靠性问题。

Method: 扩展OOSTraj适用范围到行人和车辆，使用增强的视觉定位去噪模块，利用相机校准建立视觉定位映射，无监督去噪。

Result: 在Vi-Fi和JRDB数据集上，轨迹去噪和预测达到SOTA性能，远超之前基线，还进行了与传统去噪方法对比和模型适配。

Conclusion: 该工作首次整合视觉定位投影对视线外物体噪声轨迹去噪，为未来研究铺平道路，代码和数据集已开源。

Abstract: Trajectory prediction is a critical task in computer vision and autonomous
systems, playing a key role in autonomous driving, robotics, surveillance, and
virtual reality. Existing methods often rely on complete and noise-free
observational data, overlooking the challenges associated with out-of-sight
objects and the inherent noise in sensor data caused by limited camera
coverage, obstructions, and the absence of ground truth for denoised
trajectories. These limitations pose safety risks and hinder reliable
prediction in real-world scenarios. In this extended work, we present
advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the
noise-free visual trajectories of out-of-sight objects using noisy sensor data.
Building on our previous research, we broaden the scope of Out-of-Sight
Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending
its applicability to autonomous driving, robotics, surveillance, and virtual
reality. Our enhanced Vision-Positioning Denoising Module leverages camera
calibration to establish a vision-positioning mapping, addressing the lack of
visual references, while effectively denoising noisy sensor data in an
unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB
datasets, our approach achieves state-of-the-art performance in both trajectory
denoising and prediction, significantly surpassing previous baselines.
Additionally, we introduce comparisons with traditional denoising methods, such
as Kalman filtering, and adapt recent trajectory prediction models to our task,
providing a comprehensive benchmark. This work represents the first initiative
to integrate vision-positioning projection for denoising noisy sensor
trajectories of out-of-sight agents, paving the way for future advances. The
code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [280] [Non-Intrusive Parametrized-Background Data-Weak Reconstruction of Cardiac Displacement Fields from Sparse MRI-like Observations](https://arxiv.org/abs/2509.14844)
*Francesco C. Mantegazza,Federica Caforio,Christoph Augustin,Matthias A. F. Gsell,Gundolf Haase,Elias Karabelas*

Main category: physics.med-ph

TL;DR: 本文将非侵入式PBDW方法用于3D心脏位移场重建，引入两项改进，在模拟模型上验证效果好，计算速度快，有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 个性化心脏诊断需从稀疏临床影像数据准确重建心肌位移场，但现有方法常需侵入式访问计算模型。

Method: 应用非侵入式PBDW方法进行3D心脏位移场重建，引入H - size小批量最坏情况正交匹配追踪算法和利用块矩阵结构的内存优化技术。

Result: 在无噪声条件下重建精度高（相对L2误差O(1e - 5)），10%噪声下性能稳健（相对L2误差O(1e - 2)），稀疏测量下有效重建（相对L2误差O(1e - 2)），在线重建比全有限元模拟计算速度快四个数量级。

Conclusion: 该方法有显著潜力集成到临床心脏建模工作流程中。

Abstract: Personalized cardiac diagnostics require accurate reconstruction of
myocardial displacement fields from sparse clinical imaging data, yet current
methods often demand intrusive access to computational models. In this work, we
apply the non-intrusive Parametrized-Background Data-Weak (PBDW) approach to
three-dimensional (3D) cardiac displacement field reconstruction from limited
Magnetic Resonance Image (MRI)-like observations. Our implementation requires
only solution snapshots -- no governing equations, assembly routines, or solver
access -- enabling immediate deployment across commercial and research codes
using different constitutive models. Additionally, we introduce two
enhancements: an H-size minibatch worst-case Orthogonal Matching Pursuit (wOMP)
algorithm that improves Sensor Selection (SS) computational efficiency while
maintaining reconstruction accuracy, and memory optimization techniques
exploiting block matrix structures in vectorial problems. We demonstrate the
effectiveness of the method through validation on a 3D left ventricular model
with simulated scar tissue. Starting with noise-free reconstruction, we
systematically incorporate Gaussian noise and spatial sparsity mimicking
realistic MRI acquisition protocols. Results show exceptional accuracy in
noise-free conditions (relative L2 error of order O(1e-5)), robust performance
with 10% noise (relative L2 error of order O(1e-2)), and effective
reconstruction from sparse measurements (relative L2 error of order O(1e-2)).
The online reconstruction achieves four-order-of-magnitude computational
speed-up compared to full Finite Element (FE) simulations, with reconstruction
times under one tenth of second for sparse scenarios, demonstrating significant
potential for integration into clinical cardiac modeling workflows.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [281] [Shedding Light on Dark Matter at the LHC with Machine Learning](https://arxiv.org/abs/2509.15121)
*Ernesto Arganda,Martín de los Rios,Andres D. Perez,Subhojit Roy,Rosa M. Sandá Seoane,Carlos E. M. Wagner*

Main category: hep-ph

TL;DR: 研究Z₃对称次最小超对称标准模型中以单重态主导最轻超对称粒子为形式的WIMP暗物质候选者，用机器学习分析提高对相关信号灵敏度，给出不同质量和质量差下的发现与排除范围。


<details>
  <summary>Details</summary>
Motivation: 传统搜索策略在寻找辐射衰变中性子信号时面临背景挑战，需要新方法来发现新物理场景，且要探测当前直接探测实验难以发现的暗物质候选者。

Method: 应用数据驱动的机器学习分析来提高对微妙信号的灵敏度。

Result: 使用14 TeV、100 fb⁻¹的大型强子对撞机集成光度，该方法在质量差Δm≲12 GeV时对高达225 GeV的希格斯微子质量实现5σ发现范围，在Δm≲20 GeV时对高达285 GeV实现2σ排除范围。

Conclusion: 凸显对撞机搜索探测当前直接探测实验难以发现的暗物质候选者的能力，为大型强子对撞机合作组使用机器学习方法进行搜索提供动力。

Abstract: We investigate a WIMP dark matter (DM) candidate in the form of a
singlino-dominated lightest supersymmetric particle (LSP) within the
$Z_3$-symmetric Next-to-Minimal Supersymmetric Standard Model. This framework
gives rise to regions of parameter space where DM is obtained via
co-annihilation with nearby higgsino-like electroweakinos and DM direct
detection~signals are suppressed, the so-called ``blind spots". On the other
hand, collider signatures remain promising due to enhanced radiative decay
modes of higgsinos into the singlino-dominated LSP and a photon, rather than
into leptons or hadrons. This motivates searches for radiatively decaying
neutralinos, however, these signals face substantial background challenges, as
the decay products are typically soft due to the small mass-splits ($\Delta m$)
between the LSP and the higgsino-like coannihilation partners. We apply a
data-driven Machine Learning (ML) analysis that improves sensitivity to these
subtle signals, offering a powerful complement to traditional search strategies
to discover a new physics scenario. Using an LHC integrated luminosity of
$100~\mathrm{fb}^{-1}$ at $14~\mathrm{TeV}$, the method achieves a $5\sigma$
discovery reach for higgsino masses up to $225~\mathrm{GeV}$ with $\Delta
m\!\lesssim\!12~\mathrm{GeV}$, and a $2\sigma$ exclusion up to
$285~\mathrm{GeV}$ with $\Delta m\!\lesssim\!20~\mathrm{GeV}$. These results
highlight the power of collider searches to probe DM candidates that remain
hidden from current direct detection experiments, and provide a motivation for
a search by the LHC collaborations using ML methods.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [282] [Taming Serverless Cold Starts Through OS Co-Design](https://arxiv.org/abs/2509.14292)
*Ben Holmes,Baltasar Dinis,Lana Honcharuk,Joshua Fried,Adam Belay*

Main category: cs.OS

TL;DR: 论文挑战冷启动需内存驻留状态的假设，提出执行引擎Spice，实现磁盘冷恢复接近热启动性能，证明无服务器计算中高性能与内存弹性可兼得。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算受冷启动问题困扰，当前观点认为实现亚毫秒级冷启动需内存驻留状态，本文挑战该假设。

Method: 分析现有快照/恢复机制，指出操作系统层面限制是快速恢复的障碍，提出执行引擎Spice，与操作系统集成恢复内核状态，引入专用原语高效可靠恢复内存映射。

Result: Spice在磁盘冷恢复时实现接近热启动性能，相比基于进程和虚拟机的系统分别降低延迟14.9倍和10.6倍。

Conclusion: 无服务器计算中高性能和内存弹性不再是权衡关系。

Abstract: Serverless computing promises fine-grained elasticity and operational
simplicity, fueling widespread interest from both industry and academia. Yet
this promise is undercut by the cold setart problem, where invoking a function
after a period of inactivity triggers costly initialization before any work can
begin. Even with today's high-speed storage, the prevailing view is that
achieving sub-millisecond cold starts requires keeping state resident in
memory.
  This paper challenges that assumption. Our analysis of existing
snapshot/restore mechanisms show that OS-level limitations, not storage speed,
are the real barrier to ultra-fast restores from disk. These limitations force
current systems to either restore state piecemeal in a costly manner or capture
too much state, leading to longer restore times and unpredictable performance.
Futhermore, current memory primitives exposed by the OS make it difficult to
reliably fetch data into memory and avoid costly runtime page faults.
  To overcome these barriers, we present Spice, an execution engine
purpose-built for serverless snapshot/restore. Spice integrates directly with
the OS to restore kernel state without costly replay and introduces dedicated
primitives for restoring memory mappings efficiently and reliably. As a result,
Spice delivers near-warm performance on cold restores from disk, reducing
latency by up to 14.9x over state-of-the-art process-based systems and 10.6x
over VM-based systems. This proves that high performance and memory elasticity
no longer need to be a trade-off in serverless computing.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [283] [Scaling Hybrid Quantum-HPC Applications with the Quantum Framework](https://arxiv.org/abs/2509.14470)
*Srikar Chundury,Amir Shehata,Seongmin Kim,Muralikrishnan Gopalakrishnan Meena,Chao Lu,Kalyana Gottiparthi,Eduardo Antonio Coello Perez,Frank Mueller,In-Saeng Suh*

Main category: quant-ph

TL;DR: 本文扩展量子框架QFw集成多个后端，执行多种量子工作负载，发现不同后端针对特定工作负载有优势，证明HPC感知编排是实现可扩展Q - HPC生态的实用途径。


<details>
  <summary>Details</summary>
Motivation: 混合量子 - 高性能计算（Q - HPC）工作流需跨多种模拟器和硬件后端无缝运行，需要灵活且与后端无关的执行模型进行公平基准测试、平台选择和识别量子优势机会。

Method: 扩展量子框架QFw，集成多个本地后端（Qiskit Aer、NWQ - Sim等）和基于云的量子后端（IonQ），执行非变分和变分工作负载。

Result: 发现工作负载特定的后端优势，如Qiskit Aer的矩阵积态适用于大型Ising模型，NWQ - Sim在大规模纠缠和哈密顿量方面领先，在优化问题中分布式并发执行子问题有优势。

Conclusion: 模拟器无关、HPC感知的编排是实现可扩展、可重现和可移植的Q - HPC生态系统的实用路径，可加速量子优势的实现。

Abstract: Hybrid quantum-high performance computing (Q-HPC) workflows are emerging as a
key strategy for running quantum applications at scale in current noisy
intermediate-scale quantum (NISQ) devices. These workflows must operate
seamlessly across diverse simulators and hardware backends since no single
simulator offers the best performance for every circuit type. Simulation
efficiency depends strongly on circuit structure, entanglement, and depth,
making a flexible and backend-agnostic execution model essential for fair
benchmarking, informed platform selection, and ultimately the identification of
quantum advantage opportunities. In this work, we extend the Quantum Framework
(QFw), a modular and HPC-aware orchestration layer, to integrate multiple local
backends (Qiskit Aer, NWQ-Sim, QTensor, and TN-QVM) and a cloud-based quantum
backend (IonQ) under a unified interface. Using this integration, we execute a
number of non-variational as well as variational workloads. The results
highlight workload-specific backend advantages: while Qiskit Aer's matrix
product state excels for large Ising models, NWQ-Sim not only leads on
large-scale entanglement and Hamiltonian but also shows the benefits of
concurrent subproblem execution in a distributed manner for optimization
problems. These findings demonstrate that simulator-agnostic, HPC-aware
orchestration is a practical path toward scalable, reproducible, and portable
Q-HPC ecosystems, thereby accelerating progress toward demonstrating quantum
advantage.

</details>


### [284] [Decoded Quantum Interferometry Requires Structure](https://arxiv.org/abs/2509.14509)
*Eric R. Anschuetz,David Gamarnik,Jonathan Z. Lu*

Main category: quant-ph

TL;DR: 研究解码量子干涉测量（DQI）在MAX - k - XOR - SAT典型实例上的性能，证明DQI受重叠间隙属性（OGP）阻碍，无量子优势，还给出数值证据，证明深度为1的QAOA在大k时优于DQI。


<details>
  <summary>Details</summary>
Motivation: 探究DQI在MAX - k - XOR - SAT实例上的性能及是否具有量子优势。

Method: 证明DQI在量子Wasserstein度量下近似Lipschitz，证明MAX - k - XOR - SAT存在OGP和混沌属性，证明这些拓扑属性会阻碍近似Lipschitz算法优化。

Result: DQI受OGP阻碍，近似消息传递（AMP）在相关实例上优于DQI，深度为1的QAOA在大k时优于DQI。

Conclusion: DQI在优化无结构MAX - k - XOR - SAT实例时无量子优势。

Abstract: We study the performance of Decoded Quantum Interferometry (DQI) on typical
instances of MAX-$k$-XOR-SAT when the transpose of the constraint matrix is
drawn from a standard ensemble of LDPC parity check matrices. We prove that if
the decoding step of DQI corrects up to the folklore efficient decoding
threshold for LDPC codes, then DQI is obstructed by a topological feature of
the near-optimal space of solutions known as the overlap gap property (OGP). As
the OGP is widely conjectured to exactly characterize the performance of
state-of-the-art classical algorithms, this result suggests that DQI has no
quantum advantage in optimizing unstructured MAX-$k$-XOR-SAT instances. We also
give numerical evidence supporting this conjecture by showing that approximate
message passing (AMP)--a classical algorithm conjectured to saturate the OGP
threshold--outperforms DQI on a related ensemble of MAX-$k$-XOR-SAT instances.
Finally, we prove that depth-$1$ QAOA outperforms DQI at sufficiently large $k$
under the same decoding threshold assumption.
  Our result follows by showing that DQI is approximately Lipschitz under the
quantum Wasserstein metric over many standard ensembles of codes. We then prove
that MAX-$k$-XOR-SAT exhibits both an OGP and a related topological obstruction
known as the chaos property; this is the first known OGP threshold for
MAX-$k$-XOR-SAT at fixed $k$, which may be of independent interest. Finally, we
prove that both of these topological properties inhibit approximately Lipschitz
algorithms such as DQI from optimizing MAX-$k$-XOR-SAT to large approximation
ratio.

</details>


### [285] [Efficiently learning depth-3 circuits via quantum agnostic boosting](https://arxiv.org/abs/2509.14461)
*Srinivasan Arunachalam,Arkopal Dutt,Alexandru Gheorghiu,Michael de Oliveira*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We initiate the study of quantum agnostic learning of phase states with
respect to a function class $\mathsf{C}\subseteq \{c:\{0,1\}^n\rightarrow
\{0,1\}\}$: given copies of an unknown $n$-qubit state $|\psi\rangle$ which has
fidelity $\textsf{opt}$ with a phase state
$|\phi_c\rangle=\frac{1}{\sqrt{2^n}}\sum_{x\in \{0,1\}^n}(-1)^{c(x)}|x\rangle$
for some $c\in \mathsf{C}$, output $|\phi\rangle$ which has fidelity $|\langle
\phi | \psi \rangle|^2 \geq \textsf{opt}-\varepsilon$. To this end, we give
agnostic learning protocols for the following classes: (i) Size-$t$ decision
trees which runs in time $\textsf{poly}(n,t,1/\varepsilon)$. This also implies
$k$-juntas can be agnostically learned in time
$\textsf{poly}(n,2^k,1/\varepsilon)$. (ii) $s$-term DNF formulas in
near-polynomial time $\textsf{poly}(n,(s/\varepsilon)^{\log \log
s/\varepsilon})$.
  Our main technical contribution is a quantum agnostic boosting protocol which
converts a weak agnostic learner, which outputs a parity state $|\phi\rangle$
such that $|\langle \phi|\psi\rangle|^2\geq \textsf{opt}/\textsf{poly}(n)$,
into a strong learner which outputs a superposition of parity states
$|\phi'\rangle$ such that $|\langle \phi'|\psi\rangle|^2\geq \textsf{opt} -
\varepsilon$.
  Using quantum agnostic boosting, we obtain the first near-polynomial time
$n^{O(\log \log n)}$ algorithm for learning $\textsf{poly}(n)$-sized depth-$3$
circuits (consisting of $\textsf{AND}$, $\textsf{OR}$, $\textsf{NOT}$ gates) in
the uniform quantum $\textsf{PAC}$ model using quantum examples. Classically,
the analogue of efficient learning depth-$3$ circuits (and even depth-$2$
circuits) in the uniform $\textsf{PAC}$ model has been a longstanding open
question in computational learning theory. Our work nearly settles this
question, when the learner is given quantum examples.

</details>


### [286] [TITAN: A Trajectory-Informed Technique for Adaptive Parameter Freezing in Large-Scale VQE](https://arxiv.org/abs/2509.15193)
*Yifeng Peng,Xinyi Li,Samuel Yen-Chi Chen,Kaining Zhang,Zhiding Liang,Ying Wang,Yuxuan Du*

Main category: quant-ph

TL;DR: 提出深度学习框架Titan，用于解决变分量子本征求解器（VQE）训练效率问题，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: VQE训练效率在处理大哈密顿量时迅速下降，原因包括无克隆定理导致的电路评估次数线性增长和深度电路的贫瘠高原问题。

Method: Titan在初始化时识别并冻结给定ansatze的非活跃参数，结合理论数据构造策略和自适应神经架构。

Result: 在多个基准测试中，Titan收敛速度比现有基线快3倍，电路评估次数减少40% - 60%，估计精度相当或更优。

Conclusion: Titan通过主动修剪参数空间，降低硬件需求，为利用VQE推动实用量子化学和材料科学提供了可扩展的途径。

Abstract: Variational quantum Eigensolver (VQE) is a leading candidate for harnessing
quantum computers to advance quantum chemistry and materials simulations, yet
its training efficiency deteriorates rapidly for large Hamiltonians. Two issues
underlie this bottleneck: (i) the no-cloning theorem imposes a linear growth in
circuit evaluations with the number of parameters per gradient step; and (ii)
deeper circuits encounter barren plateaus (BPs), leading to exponentially
increasing measurement overheads. To address these challenges, here we propose
a deep learning framework, dubbed Titan, which identifies and freezes inactive
parameters of a given ansatze at initialization for a specific class of
Hamiltonians, reducing the optimization overhead without sacrificing accuracy.
The motivation of Titan starts with our empirical findings that a subset of
parameters consistently has a negligible influence on training dynamics. Its
design combines a theoretically grounded data construction strategy, ensuring
each training example is informative and BP-resilient, with an adaptive neural
architecture that generalizes across ansatze of varying sizes. Across benchmark
transverse-field Ising models, Heisenberg models, and multiple molecule systems
up to 30 qubits, Titan achieves up to 3 times faster convergence and 40% to 60%
fewer circuit evaluations than state-of-the-art baselines, while matching or
surpassing their estimation accuracy. By proactively trimming parameter space,
Titan lowers hardware demands and offers a scalable path toward utilizing VQE
to advance practical quantum chemistry and materials science.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [287] [Aggregating Epigenetic Clocks to Study Human Capital Formation](https://arxiv.org/abs/2509.14422)
*Giorgia Menta,Pietro Biroli,Divya Mehta,Conchita D'Ambrosio,Deborah Cobb-Clark*

Main category: econ.GN

TL;DR: 本文开发了新的表观遗传衰老综合指标MEGA时钟，在三个实证情境中应用该时钟开展研究，且该时钟构建方法稳健


<details>
  <summary>Details</summary>
Motivation: 现有表观遗传时钟存在测量误差，需新方法来减少误差并提高估计效率

Method: 开发结合多个现有表观遗传时钟的MEGA时钟，并在三个实证情境中应用

Result: 青少年时期表观遗传加速衰老与成年早期多种不良结果相关；青春期前受虐待使表观遗传年龄高半年；晚入学一年会加速七岁儿童表观遗传衰老

Conclusion: MEGA时钟构建方法稳健，能灵活、可解释地将表观遗传数据应用于多种场景

Abstract: Epigenetics is the study of how people's behavior and environments influence
the way their genes are expressed, even though their DNA sequence is itself
unchanged. By aggregating age-related epigenetic markers, epigenetic 'clocks'
have become the leading tool for studying biological aging. We make an
important contribution by developing a novel, integrated measure of epigenetic
aging--the Multi EpiGenetic Age (MEGA) clock--which combines several existing
epigenetic clocks to reduce measurement error and improve estimation
efficiency. We use the MEGA clock in three empirical contexts to show that: i)
accelerated epigenetic aging in adolescence is associated with worse
educational, mental-health, and labor market outcomes in early adulthood; ii)
exposure to child maltreatment before adolescence is associated with half a
year higher epigenetic aging; and iii) that entering school one year later
accelerates epigenetic aging by age seven, particularly among disadvantaged
children. The MEGA clock is robust to alternative methods for constructing it,
providing a flexible and interpretable approach for incorporating epigenetic
data into a wide variety of settings.

</details>


### [288] [Are Final Market Prices Sufficient for Information Aggregation? Evidence from Last-Minute Dynamics in Parimutuel Betting](https://arxiv.org/abs/2509.14645)
*Hiroaki Hanyu,Shunsuke Ishii,Suguru Otani,Kazuhiro Teramoto*

Main category: econ.GN

TL;DR: 研究利用日本赛马临时赔率数据，检验静态博彩市场模型，发现回报与最后时刻赔率变化负相关，后期赔率变动减弱热门 - 冷门偏差。


<details>
  <summary>Details</summary>
Motivation: 多数博彩市场模型采用基于最终赔率的静态框架，研究要检验这种静态分析的有效性。

Method: 使用日本赛马的临时赔率独特数据集，探究预期回报与赔率轨迹的系统关系。

Result: 回报与最后时刻赔率变化负相关，后期赔率变动通过削弱最终赔率与回报的相关性来减弱热门 - 冷门偏差。

Conclusion: 有信息的投注者会在最后阶段根据私人信号下注，导致最终赔率出现意外。

Abstract: Most betting market models employ static frameworks that condition decisions
on final odds. Using a unique dataset of interim odds from Japanese horse
racing, this study examines the validity of such static analyses by asking
whether there is a systematic relationship between expected returns and the
trajectory of odds. We find that returns are negatively related to last-minute
changes in odds, and that these late movements attenuate the favorite-longshot
bias by weakening the correlation between final odds and returns. These
patterns suggest that informed bettors place wagers at the final stage based on
private signals, leaving surprises in final odds.

</details>


### [289] [Paradoxes of the public sector productivity measurement](https://arxiv.org/abs/2509.14795)
*Timo Kuosmanen,Xun Zhou*

Main category: econ.GN

TL;DR: 文章批判研究公共部门全要素生产率（TFP）标准测量方法，指出常见产出测量约定下的悖论，建议采用基于经济理论的非市场估值方法。


<details>
  <summary>Details</summary>
Motivation: 公共部门产出信息常不完整或扭曲，需研究标准TFP测量方法。

Method: 分析三种常见产出测量约定，用英国和芬兰实证说明。

Result: 发现成本法测TFP在真实生产率提高时可能下降；扭曲价格下TFP与生产率无关；扭曲现象存在于公共生产率统计中。

Conclusion: 公共部门TFP测量应放弃基于成本的产出聚合，采用基于经济理论的非市场估值方法。

Abstract: This paper critically investigates standard total factor productivity (TFP)
measurement in the public sector, where output information is often incomplete
or distorted. The analysis reveals fundamental paradoxes under three common
output measurement conventions. When cost-based value added is used as the
aggregate output, measured TFP may paradoxically decline as a result of genuine
productivity-enhancing changes such as technical progress and improved
allocative and scale efficiencies, as well as reductions in real input prices.
We show that the same problems carry over to the situation where the aggregate
output is constructed as the cost-share weighted index of outputs. In the case
of distorted output prices, measured TFP may move independently of any
productivity changes and instead reflect shifts in pricing mechanisms. Using
empirical illustrations from the United Kingdom and Finland, we demonstrate
that such distortions are not merely theoretical but are embedded in widely
used public productivity statistics. We argue that public sector TFP
measurement requires a shift away from cost-based aggregation of outputs and
toward non-market valuation methods grounded in economic theory.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [290] [Embodied sensorimotor control: computational modeling of the neural control of movement](https://arxiv.org/abs/2509.14360)
*Muhammad Noman Almani,John Lazzari,Jeff Walker,Shreya Saxena*

Main category: q-bio.NC

TL;DR: 本文综述感觉运动控制，涵盖神经群体、最优反馈机制和身体生物力学，探讨现存问题并展望整合研究。


<details>
  <summary>Details</summary>
Motivation: 对感觉运动控制的相关内容进行系统梳理和总结，以推动对神经运动控制的综合理解。

Method: 回顾相关文献，依次阐述神经解剖环路、神经群体活动、最优控制理论及具身感觉运动控制等方面内容。

Result: 总结了感觉运动控制在各方面的研究情况，并指出了多任务与认知行为、多区域回路模型等开放问题和机会。

Conclusion: 现有研究进展朝着实现对神经运动控制的综合解释迈进。

Abstract: We review how sensorimotor control is dictated by interacting neural
populations, optimal feedback mechanisms, and the biomechanics of bodies.
First, we outline the distributed anatomical loops that shuttle sensorimotor
signals between cortex, subcortical regions, and spinal cord. We then summarize
evidence that neural population activity occupies low-dimensional, dynamically
evolving manifolds during planning and execution of movements. Next, we
summarize literature explaining motor behavior through the lens of optimal
control theory, which clarifies the role of internal models and feedback during
motor control. Finally, recent studies on embodied sensorimotor control address
gaps within each framework by aiming to elucidate neural population activity
through the explicit control of musculoskeletal dynamics. We close by
discussing open problems and opportunities: multi-tasking and cognitively rich
behavior, multi-regional circuit models, and the level of anatomical detail
needed in body and network models. Together, this review and recent advances
point towards reaching an integrative account of the neural control of
movement.

</details>
