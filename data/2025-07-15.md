<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 44]
- [cs.CE](#cs.CE) [Total: 8]
- [cs.DB](#cs.DB) [Total: 9]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.DS](#cs.DS) [Total: 11]
- [cs.GT](#cs.GT) [Total: 11]
- [cs.IR](#cs.IR) [Total: 17]
- [cs.LG](#cs.LG) [Total: 162]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.SE](#cs.SE) [Total: 41]
- [q-fin.CP](#q-fin.CP) [Total: 4]
- [q-fin.PM](#q-fin.PM) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 3]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 15]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 29]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.SD](#cs.SD) [Total: 4]
- [eess.SP](#eess.SP) [Total: 1]
- [math.OC](#math.OC) [Total: 5]
- [stat.AP](#stat.AP) [Total: 2]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.CV](#cs.CV) [Total: 56]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.RO](#cs.RO) [Total: 11]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [cs.CR](#cs.CR) [Total: 12]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [math.CO](#math.CO) [Total: 1]
- [cs.MA](#cs.MA) [Total: 5]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 2]
- [cs.GR](#cs.GR) [Total: 2]
- [eess.AS](#eess.AS) [Total: 2]
- [math.HO](#math.HO) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [econ.GN](#econ.GN) [Total: 7]
- [cs.HC](#cs.HC) [Total: 4]
- [eess.IV](#eess.IV) [Total: 8]
- [math.PR](#math.PR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.LO](#cs.LO) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [eess.SY](#eess.SY) [Total: 4]
- [cs.DM](#cs.DM) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [math.NT](#math.NT) [Total: 1]
- [physics.data-an](#physics.data-an) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.IT](#cs.IT) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 8]
- [econ.TH](#econ.TH) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: 本文指出大语言模型推理路径存在冗余，提出通过测量注意力分数识别冗余并进行结构感知剪枝，无需训练就提高推理基准测试准确率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理路径存在大量冗余，影响性能，需去除冗余提升表现。

Method: 通过测量对特殊思考结束标记的词元级注意力分数识别推理冗余，采用结构感知剪枝优先移除低贡献推理块中的词元，移除冗余词元后去掉插入的指令继续推理生成。

Result: 该方法在推理密集型基准测试中显著提高整体准确率，在AIME和AMC等数学竞赛基准测试中表现出色。

Conclusion: 刻意去除推理过程中的冗余能通过清晰思考提高性能，无需训练可提升推理准确率。

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [2] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Main category: cs.AI

TL;DR: 本文提出结合两个虚拟差距分析（VGA）模型的多标准评估（MCA）新方法，通过实例证明其准确性和透明度，以推动自动化决策系统发展。


<details>
  <summary>Details</summary>
Motivation: 现有MCA方法依赖假设和受主观判断影响，实际需兼顾定量和定性标准，且同质性假设影响评估，需新方法解决这些问题。

Method: 提出结合两个VGA模型的新MCA方法，VGA框架基于线性规划。

Result: 两个综合数值实例证明了该方法的准确性和透明度。

Conclusion: 该方法提高了效率和公平性，评估全面可靠，能为自动化决策系统和决策支持系统发展提供有力且自适应的解决方案。

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [3] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 提出借鉴桌面角色扮演游戏，利用实体 - 组件架构模式构建灵活场景定义框架，以支持生成式AI在多角色环境的多样用例，并介绍Concordia库的发展。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在多角色环境有多样用例，需要灵活的场景定义框架。

Method: 借鉴桌面角色扮演游戏，采用实体 - 组件架构模式，分离工程师和设计师的工作。

Result: 描述了Concordia库按此理念的持续发展，可让用户有效配置符合特定目标的场景。

Conclusion: 该分离关注点的方法有助于实现快速迭代、保持模块化和确保可扩展性。

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [4] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Main category: cs.AI

TL;DR: 文章指出生物多样性丧失带来挑战，引入首个用于生物多样性分析和保护规划的基础模型BioAnalyst，评估其性能并公开以促进协作。


<details>
  <summary>Details</summary>
Motivation: 生物多样性面临诸多威胁，需要综合监测和规划能力，AI基础模型在多领域发展，有望用于生物多样性保护。

Method: 构建基于Transformer架构的BioAnalyst，在多模态数据集上预训练，可针对下游任务微调。

Result: 在两个下游用例中评估模型性能，展示其相比现有方法的泛化能力，尤其在数据稀缺场景，为生态预测建立新的准确率基线。

Conclusion: 公开BioAnalyst及其微调工作流程，促进生物多样性建模的协作，推动AI解决生态挑战。

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [5] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: 通过随机对照试验研究2025年2 - 6月前沿AI工具对有经验开源开发者生产力的影响，发现允许使用AI实际使任务完成时间增加19%，与开发者和专家预测相反。


<details>
  <summary>Details</summary>
Motivation: 研究AI工具在实际软件开发中的影响，此前这方面研究不足。

Method: 进行随机对照试验，16名有一定AI经验开发者完成246个任务，随机分配是否允许使用2025年初AI工具。

Result: 允许使用AI使任务完成时间增加19%，与开发者和专家预测的缩短时间相反。

Conclusion: 虽不能完全排除实验误差，但分析显示任务完成时间增加不太可能主要是实验设计问题。

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [6] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Main category: cs.AI

TL;DR: 提出用于去中心化操纵检测的MARL框架，集成于Symphony系统，在检测准确性和因果归因方面表现出色，推动去中心化市场情报研究。


<details>
  <summary>Details</summary>
Motivation: 去中心化金融（DeFi）存在市场操纵问题，且缺乏集中监督，需有效检测方法。

Method: 提出MARL框架，引入GRPO增强学习稳定性，设计基于理论的奖励函数，采用多模态代理管道，集成于Symphony系统。

Result: 在100,000个真实世界话语片段上训练并在对抗模拟中验证，Hide - and - Shill在检测准确性和因果归因方面达到顶级性能。

Conclusion: 该工作将多智能体系统与金融监控相结合，推动了去中心化市场情报的新范式，且提供资源促进开放研究和可重复性。

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [7] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Main category: cs.AI

TL;DR: 对基于大语言模型的编码代理进行系统安全评估，发现安全问题并开发检测系统，评估缓解策略，为评估编码代理安全提供框架。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的编码代理在软件开发中迅速应用，但安全影响尚不清楚，可能引入不安全实践，因此需进行安全评估。

Method: 对五个最先进模型在93个实际软件设置任务上的超12000个操作进行分析，开发高精度检测系统，评估缓解策略。

Result: 21%的代理轨迹包含不安全操作，模型安全行为差异大，检测系统识别出四类主要漏洞，信息暴露最普遍，不同模型缓解策略效果不同，GPT - 4.1缓解成功率达96.8%。

Conclusion: 为评估编码代理安全提供首个综合框架，强调下一代基于大语言模型的编码代理需进行安全感知设计。

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [8] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: 报告提出AI潜在灭绝人类事件的分类和示例，旨在推动预防措施。


<details>
  <summary>Details</summary>
Motivation: 帮助大型机构获得公众支持，采取针对AI灾难性风险的预防措施。

Method: 提出AI导致人类灭绝事件的分类和示例。

Result: 展示了AI可能导致全人类或几乎全人类死亡的情景。

Conclusion: 这些情景虽非必然，但可通过努力避免，公开呈现能推动预防措施。

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [9] [LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing](https://arxiv.org/abs/2507.09407)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: 本文介绍LLM-Stackelberg博弈框架，定义两种均衡概念，通过钓鱼邮件案例说明其在网络安全等领域建模决策的强大能力。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型融入领导者与追随者的战略互动，突破经典Stackelberg博弈假设，建模决策过程。

Method: 提出LLM-Stackelberg博弈框架，定义推理和行为均衡、猜想推理均衡概念，通过钓鱼邮件案例研究说明。

Result: LLM-Stackelberg博弈框架展现出认知丰富性和对抗潜力。

Conclusion: LLM-Stackelberg博弈为网络安全、虚假信息和推荐系统等领域的决策建模提供强大范式。

Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential
decision-making models that integrate large language models (LLMs) into
strategic interactions between a leader and a follower. Departing from
classical Stackelberg assumptions of complete information and rational agents,
our formulation allows each agent to reason through structured prompts,
generate probabilistic behaviors via LLMs, and adapt their strategies through
internal cognition and belief updates. We define two equilibrium concepts:
reasoning and behavioral equilibrium, which aligns an agent's internal
prompt-based reasoning with observable behavior, and conjectural reasoning
equilibrium, which accounts for epistemic uncertainty through parameterized
models over an opponent's response. These layered constructs capture bounded
rationality, asymmetric information, and meta-cognitive adaptation. We
illustrate the framework through a spearphishing case study, where a sender and
a recipient engage in a deception game using structured reasoning prompts. This
example highlights the cognitive richness and adversarial potential of
LLM-mediated interactions. Our results show that LLM-Stackelberg games provide
a powerful paradigm for modeling decision-making in domains such as
cybersecurity, misinformation, and recommendation systems.

</details>


### [10] [EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique](https://arxiv.org/abs/2507.09374)
*Chenglin Zhu,Tao Zhang,Chong Li,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: 针对多模态大语言模型在科学任务推理不足问题，提出EduFlow框架，实验证明其可提升推理一致性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在科学任务上表现差，存在推理模式不足、缺乏全局连贯性和自我纠错能力等问题，无法用于结构化科学场景。

Method: 引入EduFlow框架，涵盖数据选择、轨迹构建、模型训练和输出优化；核心是EduPRM奖励模型，通过三种监督源进行课程学习；提出EduMCTS搜索框架，引入自反思机制，利用EduPRM反馈引导搜索；构建EduMCTS - 160K数据集。

Result: 实验表明EduFlow提升了推理的一致性和连贯性。

Conclusion: EduFlow框架有效解决了多模态大语言模型在科学任务推理方面的问题，代码、数据和模型将发布。

Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.

</details>


### [11] [Knowledge Conceptualization Impacts RAG Efficacy](https://arxiv.org/abs/2507.09389)
*Chris Davis Jaldi,Anmol Saini,Elham Ghiasi,O. Divine Eziolise,Cogan Shimizu*

Main category: cs.AI

TL;DR: 本文探讨可迁移且可解释的神经符号AI系统设计，聚焦Agentic Retrieval - Augmented Generation系统，评估知识的不同概念化和表示对AI查询三元组存储的影响并报告结果。


<details>
  <summary>Details</summary>
Motivation: 可解释性和对新领域的适应性是成功AI系统的重要方面，因此希望研究如何将两者结合，设计可迁移且可解释的神经符号AI系统。

Method: 系统地评估知识的不同概念化和表示（特别是结构和复杂性）对AI代理（如大语言模型）有效查询三元组存储的影响。

Result: 两种方法（不同概念化和表示）对AI查询三元组存储都有影响。

Conclusion: 讨论了不同知识表示对AI查询的影响及相关意义。

Abstract: Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

</details>


### [12] [Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence](https://arxiv.org/abs/2507.10281)
*Jiaming Tian,Liyao Li,Wentao Ye,Haobo Wang,Lingxin Wang,Lihua Yu,Zujie Ren,Gang Chen,Junbo Zhao*

Main category: cs.AI

TL;DR: 本文聚焦基于大语言模型的表格智能体，定义核心能力分析现有方法，揭示学术基准与现实场景差距并给出改进建议。


<details>
  <summary>Details</summary>
Motivation: 现实表格任务存在噪声、结构异质性和语义复杂性等问题，现有研究多针对干净的学术数据集，未充分探索这些问题。

Method: 定义五项核心能力分析和比较当前方法，详细研究文本到SQL智能体。

Result: 发现学术基准和现实场景间存在性能差距，特别是开源模型。

Conclusion: 提供了在实际环境中提高基于大语言模型的表格智能体鲁棒性、泛化性和效率的可行建议。

Abstract: Tables are fundamental in domains such as finance, healthcare, and public
administration, yet real-world table tasks often involve noise, structural
heterogeneity, and semantic complexity--issues underexplored in existing
research that primarily targets clean academic datasets. This survey focuses on
LLM-based Table Agents, which aim to automate table-centric workflows by
integrating preprocessing, reasoning, and domain adaptation. We define five
core competencies--C1: Table Structure Understanding, C2: Table and Query
Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable
Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze
and compare current approaches. In addition, a detailed examination of the
Text-to-SQL Agent reveals a performance gap between academic benchmarks and
real-world scenarios, especially for open-source models. Finally, we provide
actionable insights to improve the robustness, generalization, and efficiency
of LLM-based Table Agents in practical settings.

</details>


### [13] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Main category: cs.AI

TL;DR: 论文主张多智能体强化学习从反应式向基于生成式AI的主动式范式转变，以应对现有挑战并解锁分布式智能新可能。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习方法存在联合动作空间指数增长、环境非平稳和部分可观测性等问题，且当前方法在新场景中表现不佳。

Method: 采用基于生成式AI的强化学习，将智能体视为能合成复杂多智能体动态并基于预测做出前瞻性决策的生成模型。

Result: 该方法可使智能体建模环境演变、预测其他智能体行为、生成协调动作序列和进行战略推理。

Conclusion: 这种范式转变将为分布式智能带来前所未有的可能性，有望解决传统反应式框架下难以解决的协调挑战，应用前景广泛。

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [14] [Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.09534)
*Guanquan Wang,Takuya Hiraoka,Yoshimasa Tsuruoka*

Main category: cs.AI

TL;DR: 提出新的离线基于模型的强化学习方法CTP，支持快速单步轨迹生成，在D4RL基准测试中优于现有基于扩散的规划方法，推理速度有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决先前将扩散模型应用于规划时因迭代采样程序导致计算成本高的问题。

Method: 引入Consistency Trajectory Planning (CTP)方法，利用Consistency Trajectory Model (CTM)进行高效轨迹优化。

Result: 在D4RL基准测试中，CTP在长视野、目标条件任务中始终优于现有基于扩散的规划方法，在使用更少去噪步骤的情况下获得更高归一化回报，推理时间有超120倍的加速。

Conclusion: CTP具有实用性和有效性，适用于高性能、低延迟的离线规划。

Abstract: This paper introduces Consistency Trajectory Planning (CTP), a novel offline
model-based reinforcement learning method that leverages the recently proposed
Consistency Trajectory Model (CTM) for efficient trajectory optimization. While
prior work applying diffusion models to planning has demonstrated strong
performance, it often suffers from high computational costs due to iterative
sampling procedures. CTP supports fast, single-step trajectory generation
without significant degradation in policy quality. We evaluate CTP on the D4RL
benchmark and show that it consistently outperforms existing diffusion-based
planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves
higher normalized returns while using significantly fewer denoising steps. In
particular, CTP achieves comparable performance with over $120\times$ speedup
in inference time, demonstrating its practicality and effectiveness for
high-performance, low-latency offline planning.

</details>


### [15] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Main category: cs.AI

TL;DR: 本文引入使用Metropolis - Hastings采样训练SNN用于强化学习动态智能体控制的框架，在两个基准测试中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）在强化学习任务训练中因脉冲通信不可微面临挑战，需要新的训练方法。

Method: 引入基于Metropolis - Hastings采样的框架，迭代提出并基于累积奖励信号概率性接受网络参数更新。

Result: 在AcroBot和CartPole两个标准控制基准测试中，基于MH的方法在最大化累积奖励、最小化网络资源和训练周期方面优于传统深度Q学习（DQL）基线和先前基于SNN的强化学习方法。

Conclusion: 基于Metropolis - Hastings采样的框架能有效训练SNN用于强化学习动态智能体控制，避免反向传播的局限，可在神经形态平台上直接优化。

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


### [16] [eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation](https://arxiv.org/abs/2507.09588)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.AI

TL;DR: 介绍eSapiens AIaaS平台，含功能、集成方式、实验评估及效果。


<details>
  <summary>Details</summary>
Motivation: 为企业提供对AI资产的完全控制，保障AI知识保留和数据安全，助力企业取得更好业务成果。

Method: 集成结构化文档摄取、混合向量检索和无代码编排，支持多种主流大语言模型，设置关键组件THOR Agent，进行检索基准测试和生成质量测试。

Result: 检索基准测试中512令牌块大小检索精度最高，生成质量测试显示eSapiens输出更具上下文一致性，事实对齐最多提升23%。

Conclusion: eSapiens能为法律和金融等高风险领域实现可信、可审计的AI工作流。

Abstract: We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a
business-oriented trifecta: proprietary data, operational workflows, and any
major agnostic Large Language Model (LLM). eSapiens gives businesses full
control over their AI assets, keeping everything in-house for AI knowledge
retention and data security. eSapiens AI Agents (Sapiens) empower your team by
providing valuable insights and automating repetitive tasks, enabling them to
focus on high-impact work and drive better business outcomes.
  The system integrates structured document ingestion, hybrid vector retrieval,
and no-code orchestration via LangChain, and supports top LLMs including
OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which
handles structured SQL-style queries and generates actionable insights over
enterprise databases.
  To evaluate the system, we conduct two experiments. First, a retrieval
benchmark on legal corpora reveals that a chunk size of 512 tokens yields the
highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation
quality test using TRACe metrics across five LLMs shows that eSapiens delivers
more context-consistent outputs with up to a 23% improvement in factual
alignment.
  These results demonstrate the effectiveness of eSapiens in enabling
trustworthy, auditable AI workflows for high-stakes domains like legal and
finance.

</details>


### [17] [The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development](https://arxiv.org/abs/2507.09611)
*Jenis Winsta*

Main category: cs.AI

TL;DR: AI发展带来环境与伦理挑战，本文探讨其在四方面影响，指出问题、研究缺口，倡导可持续发展。


<details>
  <summary>Details</summary>
Motivation: AI快速发展带来环境和伦理挑战被忽视，需探讨其多方面影响。

Method: 借鉴近期研究和机构报告。

Result: 指出模型训练高排放、硬件更替频繁、全球基础设施差异、安全能源需求等系统性问题。

Conclusion: AI进步需与伦理责任和环境管理相契合，以实现更具包容性和可持续的科技未来。

Abstract: Artificial intelligence (AI) has made remarkable progress in recent years,
yet its rapid expansion brings overlooked environmental and ethical challenges.
This review explores four critical areas where AI's impact extends beyond
performance: energy consumption, electronic waste (e-waste), inequality in
compute access, and the hidden energy burden of cybersecurity systems. Drawing
from recent studies and institutional reports, the paper highlights systemic
issues such as high emissions from model training, rising hardware turnover,
global infrastructure disparities, and the energy demands of securing AI. By
connecting these concerns, the review contributes to Responsible AI discourse
by identifying key research gaps and advocating for sustainable, transparent,
and equitable development practices. Ultimately, it argues that AI's progress
must align with ethical responsibility and environmental stewardship to ensure
a more inclusive and sustainable technological future.

</details>


### [18] [Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs](https://arxiv.org/abs/2507.09617)
*Margherita Martorana,Francesca Urgese,Mark Adamik,Ilaria Tiddi*

Main category: cs.AI

TL;DR: 本文提出结合多模态语言模型和知识图谱、本体的神经符号框架，支持机器人应用互操作性，评估多种模型，发现GPT - o1和LLaMA 4 Maverick表现佳，且集成策略很关键。


<details>
  <summary>Details</summary>
Motivation: 当前个人服务机器人系统依赖专有硬编码方案，难以跨平台适应和扩展，知识图谱和本体虽能实现互操作性但处理原始感官输入有困难，多模态语言模型缺乏透明度等，因此需结合二者优势。

Method: 提出神经符号框架，整合机器人感知数据、本体和五个多模态模型，采用不同神经符号交互模式，对生成的知识图谱进行多次运行和配置评估，进行统计分析。

Result: GPT - o1和LLaMA 4 Maverick表现始终优于其他模型，较新模型不一定有更好结果。

Conclusion: 集成策略在生成符合本体的知识图谱中起关键作用。

Abstract: Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.

</details>


### [19] [humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems](https://arxiv.org/abs/2507.09626)
*Rodion Nazarov,Anthony Quinn,Robert Shorten,Jakub Marecek*

Main category: cs.AI

TL;DR: 提出基于PyTorch的开源工具包，用于随机控制技术建模AI系统互连并提供先验保证，降低多智能体系统闭环模型公平性保证的复杂度。


<details>
  <summary>Details</summary>
Motivation: AI系统常与多智能体交互，其监管需满足公平性和鲁棒性的先验保证，而随机模型需要对随机系统进行推理。

Method: 使用基于PyTorch的开源工具包，运用随机控制技术对AI系统互连及其重复使用属性进行建模。

Result: 以闭环方式对鲁棒性和公平性需求进行建模，并为互连提供先验保证。

Conclusion: 该工具包降低了多智能体系统闭环模型提供公平性保证的复杂度。

Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The
regulation of such AI systems often requires that {\em a priori\/} guarantees
of fairness and robustness be satisfied. With stochastic models of agents'
responses to the outputs of AI systems, such {\em a priori\/} guarantees
require non-trivial reasoning about the corresponding stochastic systems. Here,
we present an open-source PyTorch-based toolkit for the use of stochastic
control techniques in modelling interconnections of AI systems and properties
of their repeated uses. It models robustness and fairness desiderata in a
closed-loop fashion, and provides {\em a priori\/} guarantees for these
interconnections. The PyTorch-based toolkit removes much of the complexity
associated with the provision of fairness guarantees for closed-loop models of
multi-agent systems.

</details>


### [20] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: 本文综述了大推理模型简洁和自适应思考以实现高效推理的进展，包括方法、基准和挑战，助研究者了解该领域。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在简单问题上生成冗长冗余推理链，浪费资源、增加响应时间，阻碍实际应用，需缩短推理链并学习自适应思考。

Method: 对大推理模型简洁和自适应思考的相关进展进行全面综述。

Result: 呈现了大推理模型在高效推理方面的方法论、基准和面临的挑战。

Conclusion: 希望能帮助研究者快速了解该领域，激发新的自适应思考想法以更好利用大推理模型。

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [21] [Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations](https://arxiv.org/abs/2507.09742)
*Xiaofeng Xiao,Bo Shen,Xubo Yue*

Main category: cs.AI

TL;DR: 提出因果信息深度Q网络（Causal DQ）用于异常检测中的部分可观测传感器放置，效果良好且有拓展潜力。


<details>
  <summary>Details</summary>
Motivation: AI驱动制造中数据流监控数据量大，资源有限，需开发最优传感器放置策略，现有方法存在忽视因果关系等问题。

Method: 在Q网络训练各阶段集成因果信息的因果信息深度Q网络（Causal DQ）方法。

Result: 实现更快收敛和更严格理论误差界，在不同设置下显著减少异常检测时间。

Conclusion: 该方法对大规模实时数据流传感器放置有效，其基本见解可应用于多种强化学习问题，为工程应用中因果信息机器学习方法带来新可能。

Abstract: Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume
of data streams requiring real-time monitoring continues to grow. However, due
to limited resources, it is impractical to place sensors at every location to
detect unexpected shifts. Therefore, it is necessary to develop an optimal
sensor placement strategy that enables partial observability of the system
while detecting anomalies as quickly as possible. Numerous approaches have been
proposed to address this challenge; however, most existing methods consider
only variable correlations and neglect a crucial factor: Causality. Moreover,
although a few techniques incorporate causal analysis, they rely on
interventions-artificially creating anomalies-to identify causal effects, which
is impractical and might lead to catastrophic losses. In this paper, we
introduce a causality-informed deep Q-network (Causal DQ) approach for
partially observable sensor placement in anomaly detection. By integrating
causal information at each stage of Q-network training, our method achieves
faster convergence and tighter theoretical error bounds. Furthermore, the
trained causal-informed Q-network significantly reduces the detection time for
anomalies under various settings, demonstrating its effectiveness for sensor
placement in large-scale, real-world data streams. Beyond the current
implementation, our technique's fundamental insights can be applied to various
reinforcement learning problems, opening up new possibilities for real-world
causality-informed machine learning methods in engineering applications.

</details>


### [22] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: 提出将大语言模型集成到次协调逻辑形式语义解释函数的方法，并实验验证可行性，提供神经符号推理理论框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在输出中存在逻辑一致性问题，需利用其知识进行形式推理。

Method: 将大语言模型直接集成到次协调逻辑形式语义的解释函数中。

Result: 通过短形式事实性基准测试数据集评估，证明方法可行。

Conclusion: 该方法为神经符号推理提供理论框架，可利用大语言模型知识并保留底层逻辑的合理性和完备性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [23] [Technical Requirements for Halting Dangerous AI Activities](https://arxiv.org/abs/2507.09801)
*Peter Barnett,Aaron Scher,David Abecassis*

Main category: cs.AI

TL;DR: AI发展有风险，本文提出可协调叫停危险AI活动的技术干预措施及作用。


<details>
  <summary>Details</summary>
Motivation: 应对AI快速发展带来的如失控、滥用等风险，避免最坏结果。

Method: 提出可用于协调叫停危险AI活动的关键技术干预措施。

Result: 讨论干预措施对限制危险AI活动的作用，表明其能为AI治理计划奠定技术基础。

Conclusion: 这些技术干预措施可作为潜在AI治理计划的技术基础以应对AI发展风险。

Abstract: The rapid development of AI systems poses unprecedented risks, including loss
of control, misuse, geopolitical instability, and concentration of power. To
navigate these risks and avoid worst-case outcomes, governments may proactively
establish the capability for a coordinated halt on dangerous AI development and
deployment. In this paper, we outline key technical interventions that could
allow for a coordinated halt on dangerous AI activities. We discuss how these
interventions may contribute to restricting various dangerous AI activities,
and show how these interventions can form the technical foundation for
potential AI governance plans.

</details>


### [24] [Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation](https://arxiv.org/abs/2507.09850)
*Wei Du,Branislav Kisacanin,George Armstrong,Shubham Toshniwal,Ivan Moshkov,Alexan Ayrapetyan,Sadegh Mahdavi,Dan Zhao,Shizhe Diao,Dragan Masulovic,Marius Stanean,Advaith Avadhanam,Max Wang,Ashmit Dutta,Shitij Govil,Sri Yanamandara,Mihir Tandon,Sriram Ananthakrishnan,Vedant Rathi,David Zhang,Joonseok Kang,Leon Luo,Titu Andreescu,Boris Ginsburg,Igor Gitman*

Main category: cs.AI

TL;DR: 本文探索仅通过提示或少量微调使基础模型获得长思维链推理能力，用少量高质量示例微调模型可解锁强推理能力，推理模型的思维链数据效果最佳，还分析了影响推理蒸馏的关键属性。


<details>
  <summary>Details</summary>
Motivation: 探究能否仅通过提示或少量微调使基础模型获得长思维链推理能力。

Method: 使用来自推理模型QwQ - 32B - Preview的20个长思维链示例对基础模型Qwen2.5 - 32B进行轻微调；探索使用非推理模型和人类注释者的思维链数据，并结合提示工程、多轮编辑和结构指导。

Result: 微调后的模型性能超过更大的Qwen2.5 - Math - 72B - Instruct；非推理模型和人类注释者的思维链数据效果不如推理模型；分析出影响推理蒸馏的关键属性。

Conclusion: 精心策划的少量人类编写的思维链可以激活基础模型的推理行为，发布人类编写的数据集以促进对小规模推理监督有效性的进一步研究。

Abstract: Reasoning-capable language models achieve state-of-the-art performance in
diverse complex tasks by generating long, explicit Chain-of-Thought (CoT)
traces. While recent works show that base models can acquire such reasoning
traces via reinforcement learning or distillation from stronger models like
DeepSeek-R1, previous works demonstrate that even short CoT prompting without
fine-tuning is able to improve reasoning. We ask whether long CoT can be
induced in a base model using only prompting or minimal tuning. Using just 20
long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly
fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms
the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of
high-quality examples can unlock strong reasoning capabilities. We further
explore using CoT data from non-reasoning models and human annotators, enhanced
with prompt engineering, multi-pass editing, and structural guidance. However,
neither matches the performance of reasoning model traces, suggesting that
certain latent qualities of expert CoT are difficult to replicate. We analyze
key properties of reasoning data, such as problem difficulty, diversity, and
answer length, that influence reasoning distillation. While challenges remain,
we are optimistic that carefully curated human-written CoT, even in small
quantities, can activate reasoning behaviors in base models. We release our
human-authored dataset across refinement stages and invite further
investigation into what makes small-scale reasoning supervision so effective.

</details>


### [25] [Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems](https://arxiv.org/abs/2507.09854)
*Aniruddha Chattopadhyay,Raj Dandekar,Kaushik Roy*

Main category: cs.AI

TL;DR: 提出将指令调优大语言模型重新解释为模型接地符号AI系统，开发新学习和推理方法，初步评估显示可提升学习效率和推理可靠性。


<details>
  <summary>Details</summary>
Motivation: 利用神经符号AI结合神经网络与经典符号AI机制的优势，探索新的集成方式。

Method: 将指令调优大语言模型重新解释为模型接地符号AI系统，自然语言作为符号层，通过模型内部表示空间实现接地，在此框架下开发新的学习和推理方法。

Result: 对不同复杂度的公理演绎推理程序进行初步评估，为方法有效性提供见解。

Conclusion: 该方法能提高学习效率和推理可靠性。

Abstract: Neurosymbolic artificial intelligence (AI) systems combine neural network and
classical symbolic AI mechanisms to exploit the complementary strengths of
large scale, generalizable learning and robust, verifiable reasoning. Numerous
classifications of neurosymbolic AI illustrate how these two components can be
integrated in distinctly different ways. In this work, we propose
reinterpreting instruction tuned large language models as model grounded
symbolic AI systems where natural language serves as the symbolic layer and
grounding is achieved through the models internal representation space. Within
this framework, we investigate and develop novel learning and reasoning
approaches that preserve structural similarities to traditional learning and
reasoning paradigms. Preliminary evaluations across axiomatic deductive
reasoning procedures of varying complexity provide insights into the
effectiveness of our approach in improving learning efficiency and reasoning
reliability.

</details>


### [26] [SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning](https://arxiv.org/abs/2507.10421)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.AI

TL;DR: 文章提出结合BERT情感分析与XGBoost分析社会人口和行为数据的模型预测远程学习学生辍学风险，测试准确率84%，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 远程学习辍学问题严重，早期检测至关重要，需准确预测辍学风险以进行有效干预。

Method: 引入结合BERT对学生评论进行情感分析和XGBoost分析社会人口与行为数据的模型，微调BERT捕捉情感，用特征重要性技术选择XGBoost关键特征并合并。

Result: 模型在未知数据测试中准确率达84%，高于基线模型的82%，其他指标表现也更优。

Conclusion: 该方法可作为制定个性化策略降低辍学率、鼓励学生坚持学习的重要工具。

Abstract: School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

</details>


### [27] [VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains](https://arxiv.org/abs/2507.09884)
*Xuzhao Li,Xuchen Li,Shiyu Hu,Yongzhen Guo,Wentao Zhang*

Main category: cs.AI

TL;DR: 提出VerifyBench基准评估验证器，构建4000个问题，设计实验框架对比性能，发现验证器权衡和局限性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏不同类型验证器跨领域性能的系统评估，制约了可验证奖励强化学习（RLVR）的可靠发展。

Method: 构建VerifyBench基准，包含4000个跨数学、物理、化学和生物学的专家级问题，由多学科团队严格标注；设计四维实验框架对比不同验证器性能。

Result: 发现验证器存在权衡，专用验证器精度高但召回不足，通用模型包容性强但精度不稳定；验证器对输入结构敏感，跨领域泛化有固有局限。

Conclusion: 研究为当前验证器技术的瓶颈提供了关键见解。

Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL)
to enhance their reasoning capabilities through feedback. A critical challenge
is verifying the consistency of model-generated responses and reference
answers, since these responses are often lengthy, diverse, and nuanced.
Rule-based verifiers struggle with complexity, prompting the use of model-based
verifiers. However, specialized verifiers lack flexibility, while general LLM
judges can be inconsistent. Existing research primarily focuses on building
better verifiers, yet a systematic evaluation of different types of verifiers'
performance across domains remains lacking, severely constraining the reliable
development of Reinforcement Learning with Verifiable Reward (RLVR). To address
this, we propose VerifyBench--a cross-domain comprehensive benchmark for
systematically evaluating verifiers. We construct 4,000 expert-level questions
covering mathematics, physics, chemistry, and biology. Each question is
equipped with reference answers and diverse responses. The reliability of the
evaluation is ensured through a rigorous annotation process conducted by a
multidisciplinary expert team. We design a four-dimensional experimental
framework to comprehensively compare the performance boundaries of specialized
verifiers and general LLMs under combined conditions of extracted answers vs.
complete responses, and short vs. long outputs. Our evaluation uncovers
fundamental trade-offs in verifiers: while specialized verifiers achieve
leading accuracy, they exhibit deficiencies in recall; general models show
stronger inclusivity but unstable precision. More importantly, we discover
verifiers' high sensitivity to input structure and inherent limitations in
cross-domain generalization, providing critical insights into the bottlenecks
of current verifier technology.

</details>


### [28] [DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models](https://arxiv.org/abs/2507.09955)
*Luolin Xiong,Haofen Wang,Xi Chen,Lu Sheng,Yun Xiong,Jingping Liu,Yanghua Xiao,Huajun Chen,Qing-Long Han,Yang Tang*

Main category: cs.AI

TL;DR: 本文围绕DeepSeek发布的V3和R1系列模型，回顾大模型演变，介绍其新算法、工程突破，分析对AI竞争格局影响，探讨未来趋势。


<details>
  <summary>Details</summary>
Motivation: 深入了解DeepSeek模型的特点、创新及对AI领域的影响，为大模型技术和工程发展提供参考。

Method: 先回顾大AI模型演变，介绍DeepSeek新算法，探索其工程突破，对比分析其在各领域与主流LLM的差异。

Result: 展示了DeepSeek模型的新算法和工程突破，分析了其在AI竞争格局中的影响。

Conclusion: 从DeepSeek创新中获得启示，探讨大AI模型在数据、训练和推理等方面的未来趋势。

Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their
V3 and R1 series models, which attracted global attention due to their low
cost, high performance, and open-source advantages. This paper begins by
reviewing the evolution of large AI models focusing on paradigm shifts, the
mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.
Subsequently, the paper highlights novel algorithms introduced by DeepSeek,
including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),
Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).
The paper then explores DeepSeek engineering breakthroughs in LLM scaling,
training, inference, and system-level optimization architecture. Moreover, the
impact of DeepSeek models on the competitive AI landscape is analyzed,
comparing them to mainstream LLMs across various fields. Finally, the paper
reflects on the insights gained from DeepSeek innovations and discusses future
trends in the technical and engineering development of large AI models,
particularly in data, training, and reasoning.

</details>


### [29] [Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient](https://arxiv.org/abs/2507.09989)
*Xiaoyang Yu,Youfang Lin,Shuo Wang,Sheng Han*

Main category: cs.AI

TL;DR: 本文提出OMDPG算法解决异构多智能体强化学习中单调改进与部分参数共享的冲突，实验表明其性能优于多种基准算法。


<details>
  <summary>Details</summary>
Motivation: 现有HAPPO算法在异构多智能体强化学习中采用无参数共享的顺序更新方案，而实际需要部分参数共享以实现高协作性能，但直接结合会导致策略更新基线漂移问题，无法实现改进，因此需解决单调改进与部分参数共享的冲突。

Method: 提出OMDPG算法，用最优边际Q函数替代顺序计算的Q函数，引入广义Q评判器作为评判函数，采用悲观不确定性约束损失优化Q值估计，实现集中式评判器分组智能体架构。

Result: 在SMAC和MAMuJoCo环境的实验结果表明，OMDPG算法优于多种最先进的多智能体强化学习基准算法。

Conclusion: OMDPG算法有效解决了异构多智能体强化学习中单调改进与部分参数共享的冲突，具有更好的性能。

Abstract: In heterogeneous multi-agent reinforcement learning (MARL), achieving
monotonic improvement plays a pivotal role in enhancing performance. The HAPPO
algorithm proposes a feasible solution by introducing a sequential update
scheme, which requires independent learning with No Parameter-sharing (NoPS).
However, heterogeneous MARL generally requires Partial Parameter-sharing
(ParPS) based on agent grouping to achieve high cooperative performance. Our
experiments prove that directly combining ParPS with the sequential update
scheme leads to the policy updating baseline drift problem, thereby failing to
achieve improvement. To solve the conflict between monotonic improvement and
ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)
algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$
with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived
from Q-functions. This maintains MAAD's monotonic improvement while eliminating
the conflict through optimal joint action sequences instead of sequential
policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)
as the critic function, employing pessimistic uncertainty-constrained loss to
optimize different Q-value estimations. This provides the required Q-values for
OMQ computation and stable baselines for actor updates. Finally, we implement a
Centralized Critic Grouped Actor (CCGA) architecture that simultaneously
achieves ParPS in local policy networks and accurate global Q-function
computation. Experimental results in SMAC and MAMuJoCo environments demonstrate
that OMDPG outperforms various state-of-the-art MARL baselines.

</details>


### [30] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Main category: cs.AI

TL;DR: 文章提出用过程连贯性和尺度分离识别文本潜在意图，成本低且不依赖大量训练。


<details>
  <summary>Details</summary>
Motivation: 自Searle解构意图后，科技领域对意图实际意义关注少，需新方法识别意图。

Method: 以过程连贯性为指导识别文本主题概念，用尺度分离区分意图内容和环境背景，以时空连贯性为衡量标准。

Result: 能以低计算成本对潜在意图进行初步实用解读，基本生物也可实现。

Conclusion: 该过程计算成本低，不依赖大量训练，但概念形成水平取决于主体记忆能力。

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [31] [Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.10007)
*Zijun Chen,Wenbo Hu,Richang Hong*

Main category: cs.AI

TL;DR: 本文提出利用模型内在真实性编码校准思维链（CoT）推理准确性的新方法，实验表明该方法在多任务上优于现有基线，还探索了模型自我修正能力。


<details>
  <summary>Details</summary>
Motivation: CoT推理虽强大，但中间步骤误差积累会削弱其可靠性。

Method: 发现特定注意力头激活能反映推理步骤真实性，训练置信度预测器评估推理步骤正确性，通过束搜索动态选择最合理推理路径。

Result: 该方法在数学、符号和常识推理任务上显著优于现有基线，在单模态和多模态设置中都展现出更高准确性和可靠性，且适用于大型推理模型。

Conclusion: 此工作为CoT推理提供了新的可靠性提升途径，有广泛应用潜力。

Abstract: Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning
capabilities in both large language models (LLMs) and multimodal large language
models (MLLMs). However, its reliability is often undermined by the
accumulation of errors in intermediate steps. This paper introduces an novel
approach to calibrate the CoT reasoning accuracy by leveraging the model's
intrinsic veracity encoding. We discover that specific attention head
activations reliably reflect the truthfulness of reasoning steps in CoT. Based
on this insight, we train a confidence predictor to evaluate the correctness of
each reasoning step using these truthfulness-sensitive activations, dynamically
selecting the most plausible reasoning path via beam search. Experimental
results demonstrate that our method significantly outperforms the
state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and
Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and
commonsense reasoning tasks, exhibiting superior accuracy and reliability in
both unimodal and multimodal settings. We further validate the approach on
large reasoning models, confirming its applicability to specialized reasoning
models. Additionally, we explore the role of the model's self-correction
ability in CoT reasoning. This work provides a novel reliability improvement
path for CoT reasoning with broad application potential.

</details>


### [32] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Main category: cs.AI

TL;DR: 本文研究大语言模型能否自动在流行知识图谱模式间翻译SPARQL，构建两个基准测试，测试三个开源大模型不同提示策略下的表现，发现模型和策略表现差异大，Wikidata到DBpedia的翻译效果更好。


<details>
  <summary>Details</summary>
Motivation: 解决知识图谱互操作性研究中关于SPARQL到SPARQL翻译评估的缺口。

Method: 构建两个基准测试，选取三个开源大模型，用零样本、少样本和两种思维链变体进行测试，将输出与标准答案对比并对错误分类。

Result: 不同模型和提示策略的表现差异明显，Wikidata到DBpedia的翻译效果远好于DBpedia到Wikidata。

Conclusion: 未明确提及，但从结果可推测不同大模型和提示策略对SPARQL翻译有显著影响。

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [33] [On Gradual Semantics for Assumption-Based Argumentation](https://arxiv.org/abs/2507.10076)
*Anna Rapberger,Fabrizio Russo,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 本文为基于假设的论证（ABA）提出新的渐进语义，通过双极集论证框架抽象ABA框架并推广现有语义，验证性质、对比不同方法并实验评估收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有研究中渐进语义在ABA中缺失，而ABA应用场景需要渐进语义，因此填补这一空白。

Method: 用双极集论证框架抽象ABA框架，推广QBAF的模块化渐进语义；探索直接利用已有的QBAF模块化语义的基于论证的方法。

Result: 所提出的渐进ABA语义满足渐进QBAF语义理想属性的适当改编；通过合成ABA框架实验对比了不同方法。

Conclusion: 成功为ABA提出新的渐进语义，满足相关性质，通过实验评估了收敛性。

Abstract: In computational argumentation, gradual semantics are fine-grained
alternatives to extension-based and labelling-based semantics . They ascribe a
dialectical strength to (components of) arguments sanctioning their degree of
acceptability. Several gradual semantics have been studied for abstract,
bipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,
to a lesser extent, for some forms of structured argumentation. However, this
has not been the case for assumption-based argumentation (ABA), despite it
being a popular form of structured argumentation with several applications
where gradual semantics could be useful. In this paper, we fill this gap and
propose a family of novel gradual semantics for equipping assumptions, which
are the core components in ABA frameworks, with dialectical strengths. To do
so, we use bipolar set-based argumentation frameworks as an abstraction of
(potentially non-flat) ABA frameworks and generalise state-of-the-art modular
gradual semantics for QBAFs. We show that our gradual ABA semantics satisfy
suitable adaptations of desirable properties of gradual QBAF semantics, such as
balance and monotonicity. We also explore an argument-based approach that
leverages established QBAF modular semantics directly, and use it as baseline.
Finally, we conduct experiments with synthetic ABA frameworks to compare our
gradual ABA semantics with its argument-based counterpart and assess
convergence.

</details>


### [34] [BlueGlass: A Framework for Composite AI Safety](https://arxiv.org/abs/2507.10106)
*Harshal Nandigramwar,Syed Qutub,Kay-Ulrich Scholl*

Main category: cs.AI

TL;DR: 本文介绍了促进复合AI安全工作流的框架BlueGlass，并在视觉语言模型上进行三种安全分析，为构建更强大可靠的AI系统提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有安全工具针对模型安全不同方面，无法单独提供全面保证，需要集成和复合的方法。

Method: 引入BlueGlass框架，统一基础设施以集成不同安全工具，并在视觉语言模型的目标检测任务上进行三种安全分析。

Result: 通过分布评估揭示性能权衡和潜在故障模式；基于探针分析层动态；用稀疏自动编码器识别可解释概念。

Conclusion: 本工作为构建更强大可靠的AI系统提供了基础架构和研究成果。

Abstract: As AI systems become increasingly capable and ubiquitous, ensuring the safety
of these systems is critical. However, existing safety tools often target
different aspects of model safety and cannot provide full assurance in
isolation, highlighting a need for integrated and composite methodologies. This
paper introduces BlueGlass, a framework designed to facilitate composite AI
safety workflows by providing a unified infrastructure enabling the integration
and composition of diverse safety tools that operate across model internals and
outputs. Furthermore, to demonstrate the utility of this framework, we present
three safety-oriented analyses on vision-language models for the task of object
detection: (1) distributional evaluation, revealing performance trade-offs and
potential failure modes across distributions; (2) probe-based analysis of layer
dynamics highlighting shared hierarchical learning via phase transition; and
(3) sparse autoencoders identifying interpretable concepts. More broadly, this
work contributes foundational infrastructure and findings for building more
robust and reliable AI systems.

</details>


### [35] [Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration](https://arxiv.org/abs/2507.10119)
*Sadig Gojayev,Ahmad Anaqreh,Carolina Fortuna*

Main category: cs.AI

TL;DR: 本文从MDP出发，分析比较用于解决边缘云应用迁移问题的AI规划和RL方法，引入基于状态空间定义的分类。


<details>
  <summary>Details</summary>
Motivation: 当前边缘云应用迁移的自动编排多采用启发式方法，需要了解新兴计算环境中可用于编排应用迁移的技术。

Method: 从MDP开始，识别、分析和比较基于ToH问题建模的边缘云应用迁移问题的AI规划和RL方法，引入基于状态空间定义的分类。

Result: 未提及具体结果。

Conclusion: 未提及具体结论，目标是了解新兴计算环境中可用于编排应用迁移的技术。

Abstract: Application migration in edge-cloud system enables high QoS and cost
effective service delivery. However, automatically orchestrating such migration
is typically solved with heuristic approaches. Starting from the Markov
Decision Process (MDP), in this paper, we identify, analyze and compare
selected state-of-the-art Artificial Intelligence (AI) planning and
Reinforcement Learning (RL) approaches for solving the class of edge-cloud
application migration problems that can be modeled as Towers of Hanoi (ToH)
problems. We introduce a new classification based on state space definition and
analyze the compared models also through this lense. The aim is to understand
available techniques capable of orchestrating such application migration in
emerging computing continuum environments.

</details>


### [36] [Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making](https://arxiv.org/abs/2507.10124)
*Thomas T. Hills*

Main category: cs.AI

TL;DR: 本文探讨用人类决策去偏策略对大语言模型去偏，以“你可能错了吗”提示词为例展示其有效性，认为人类心理学为提示工程提供新途径。


<details>
  <summary>Details</summary>
Motivation: 大语言模型仍在发展，需通用去偏策略，人类决策去偏策略有借鉴价值。

Method: 使用人类决策文献中的元认知提示词“你可能错了吗”，对关于大语言模型偏差的问题集进行测试。

Result: 该提示词能让大语言模型给出额外信息，揭示自身偏差和元认知反思，纠正不完整信息。

Conclusion: 人类心理学为提示工程提供新途径，可借鉴基于提示改进人类决策的经验。

Abstract: Identifying bias in LLMs is ongoing. Because they are still in development,
what is true today may be false tomorrow. We therefore need general strategies
for debiasing that will outlive current models. Strategies developed for
debiasing human decision making offer one promising approach as they
incorporate an LLM-style prompt intervention designed to bring latent knowledge
into awareness during decision making. LLMs trained on vast amounts of
information contain information about potential biases, counter-arguments, and
contradictory evidence, but that information may only be brought to bear if
prompted. Metacognitive prompts developed in the human decision making
literature are designed to achieve this, and as I demonstrate here, they show
promise with LLMs. The prompt I focus on here is "could you be wrong?"
Following an LLM response, this prompt leads LLMs to produce additional
information, including why they answered as they did, errors, biases,
contradictory evidence, and alternatives, none of which were apparent in their
initial response. Indeed, this metaknowledge often reveals that how LLMs and
users interpret prompts are not aligned. Here I demonstrate this prompt using a
set of questions taken from recent articles about LLM biases, including
implicit discriminatory biases and failures of metacognition. "Could you be
wrong" prompts the LLM to identify its own biases and produce cogent
metacognitive reflection. I also present another example involving convincing
but incomplete information, which is readily corrected by the metacognitive
prompt. In sum, this work argues that human psychology offers a new avenue for
prompt engineering, leveraging a long history of effective prompt-based
improvements to human decision making.

</details>


### [37] [FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring](https://arxiv.org/abs/2507.10134)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: 本文提出基于大语言模型上下文学习的在线飞行资源分配方案FRSICL，用于无人机辅助野火监测系统，能实时联合优化飞行控制与数据收集调度，渐近最小化传感器平均信息年龄，仿真证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法在无人机辅助野火监测系统中存在低采样效率、仿真与现实差距、训练复杂等问题，不适合时间关键的野火监测应用。

Method: 提出基于大语言模型上下文学习的在线飞行资源分配方案FRSICL，利用自然语言任务描述和环境反馈生成数据收集调度和控制速度，实现动态决策。

Result: 仿真结果表明，FRSICL相比近端策略优化（PPO）和最近邻基线方法更有效。

Conclusion: FRSICL能有效解决无人机辅助野火监测系统中的问题，在实时飞行控制和数据收集调度优化方面表现良好。

Abstract: Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in
wildfire monitoring, where early detection minimizes environmental impact. In
UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor
transmission scheduling and velocity is critical for minimizing Age of
Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has
been used for such optimization; however, its limitations such as low sampling
efficiency, simulation-to-reality gaps, and complex training render it
unsuitable for time-critical applications like wildfire monitoring. This paper
introduces a new online Flight Resource Allocation scheme based on LLM-Enabled
In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and
data collection schedule along the trajectory in real time, thereby
asymptotically minimizing the average AoI across ground sensors. In contrast to
DRL, FRSICL generates data collection schedules and controls velocity using
natural language task descriptions and feedback from the environment, enabling
dynamic decision-making without extensive retraining. Simulation results
confirm the effectiveness of the proposed FRSICL compared to Proximal Policy
Optimization (PPO) and Nearest-Neighbor baselines.

</details>


### [38] [Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review](https://arxiv.org/abs/2507.10142)
*Siyi Hu,Mohamad A Hady,Jianglin Qiao,Jimmy Cao,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 介绍适应性概念评估MARL算法可靠性，提出结构化框架并助力开发适用于现实环境的算法。


<details>
  <summary>Details</summary>
Motivation: MARL在现实多智能体系统中部署受限，因环境复杂动态，需评估算法在变化条件下的可靠性。

Method: 引入适应性概念，提出包含学习适应性、策略适应性和场景驱动适应性三个维度的结构化框架。

Result: 无明确提及具体结果

Conclusion: 该研究有助于开发更适合部署在动态现实多智能体系统中的算法。

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in
coordinating multiple agents across simulated benchmarks and constrained
scenarios. However, its deployment in real-world multi-agent systems (MAS)
remains limited, primarily due to the complex and dynamic nature of such
environments. These challenges arise from multiple interacting sources of
variability, including fluctuating agent populations, evolving task goals, and
inconsistent execution conditions. Together, these factors demand that MARL
algorithms remain effective under continuously changing system configurations
and operational demands. To better capture and assess this capacity for
adjustment, we introduce the concept of \textit{adaptability} as a unified and
practically grounded lens through which to evaluate the reliability of MARL
algorithms under shifting conditions, broadly referring to any changes in the
environment dynamics that may occur during learning or execution. Centred on
the notion of adaptability, we propose a structured framework comprising three
key dimensions: learning adaptability, policy adaptability, and scenario-driven
adaptability. By adopting this adaptability perspective, we aim to support more
principled assessments of MARL performance beyond narrowly defined benchmarks.
Ultimately, this survey contributes to the development of algorithms that are
better suited for deployment in dynamic, real-world multi-agent systems.

</details>


### [39] [Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation](https://arxiv.org/abs/2507.10156)
*Lubnaa Abdur Rahman,Ioannis Papathanail,Stavroula Mougiakakou*

Main category: cs.AI

TL;DR: 介绍瑞士食物知识图谱SwissFKG，建立LLM驱动的图谱填充管道，展示其在营养查询等方面应用，为下一代饮食评估工具奠基。


<details>
  <summary>Details</summary>
Motivation: 现有自动饮食评估系统忽视非视觉因素和个体饮食需求，瑞士缺乏整合营养相关方面的集中库。

Method: 引入SwissFKG，建立LLM驱动的图谱填充管道，进行LLM基准测试，实现Graph - RAG应用并评估LLM - 嵌入配对。

Result: LLMs能有效用相关营养信息丰富图谱，SwissFKG可提供成分级信息和营养指南相关指导。

Conclusion: 工作为融合饮食视觉、背景和文化维度的下一代饮食评估工具奠定基础。

Abstract: AI has driven significant progress in the nutrition field, especially through
multimedia-based automatic dietary assessment. However, existing automatic
dietary assessment systems often overlook critical non-visual factors, such as
recipe-specific ingredient substitutions that can significantly alter
nutritional content, and rarely account for individual dietary needs, including
allergies, restrictions, cultural practices, and personal preferences. In
Switzerland, while food-related information is available, it remains
fragmented, and no centralized repository currently integrates all relevant
nutrition-related aspects within a Swiss context. To bridge this divide, we
introduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our
best knowledge, to unite recipes, ingredients, and their substitutions with
nutrient data, dietary restrictions, allergen information, and national
nutrition guidelines under one graph. We establish a LLM-powered enrichment
pipeline for populating the graph, whereby we further present the first
benchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge
augmentation. Our results demonstrate that LLMs can effectively enrich the
graph with relevant nutritional information. Our SwissFKG goes beyond recipe
recommendations by offering ingredient-level information such as allergen and
dietary restriction information, and guidance aligned with nutritional
guidelines. Moreover, we implement a Graph-RAG application to showcase how the
SwissFKG's rich natural-language data structure can help LLM answer
user-specific nutrition queries, and we evaluate LLM-embedding pairings by
comparing user-query responses against predefined expected answers. As such,
our work lays the foundation for the next generation of dietary assessment
tools that blend visual, contextual, and cultural dimensions of eating.

</details>


### [40] [Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?](https://arxiv.org/abs/2507.10174)
*Yumi Omori,Zixuan Dong,Keith Ross*

Main category: cs.AI

TL;DR: 本文通过实验表明，在稀疏奖励环境中，基于MLP的过滤行为克隆（FBC）比决策变压器（DT）性能更优，质疑DT的适用性。


<details>
  <summary>Details</summary>
Motivation: Bhargava等人（2024）称DT在稀疏奖励和低质量数据设置中表现优越，本文对此进行验证并探索更优算法。

Method: 在机器人操作任务（Robomimic）和运动基准测试（D4RL）上进行实验，使用FBC（过滤低性能轨迹后进行行为克隆）与DT对比。

Result: FBC在稀疏奖励环境中达到了与DT相当或更优的性能，且所需训练数据更少、计算效率更高。

Conclusion: DT在稀疏奖励环境中并非首选，结合先前研究，提出DT是否有适用场景的疑问。

Abstract: In recent years, extensive work has explored the application of the
Transformer architecture to reinforcement learning problems. Among these,
Decision Transformer (DT) has gained particular attention in the context of
offline reinforcement learning due to its ability to frame return-conditioned
policy learning as a sequence modeling task. Most recently, Bhargava et al.
(2024) provided a systematic comparison of DT with more conventional MLP-based
offline RL algorithms, including Behavior Cloning (BC) and Conservative
Q-Learning (CQL), and claimed that DT exhibits superior performance in
sparse-reward and low-quality data settings.
  In this paper, through experimentation on robotic manipulation tasks
(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered
Behavior Cloning (FBC) achieves competitive or superior performance compared to
DT in sparse-reward environments. FBC simply filters out low-performing
trajectories from the dataset and then performs ordinary behavior cloning on
the filtered dataset. FBC is not only very straightforward, but it also
requires less training data and is computationally more efficient. The results
therefore suggest that DT is not preferable for sparse-reward environments.
From prior work, arguably, DT is also not preferable for dense-reward
environments. Thus, we pose the question: Is DT ever preferable?

</details>


### [41] [Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks](https://arxiv.org/abs/2507.10208)
*Hamzah Ziadeh,Hendrik Knoche*

Main category: cs.AI

TL;DR: 本文从多领域出发提出对可解释人工智能（XAI）研究分类和比较的方法，指出问题并给出研究建议，以助于研究和设计人员更好应对XAI领域。


<details>
  <summary>Details</summary>
Motivation: 当前XAI数据分析研究存在大量矛盾且缺乏具体设计建议，源于对需AI辅助任务理解不足。

Method: 借鉴视觉分析、认知和仪表盘设计等多领域，从what、why、who三个维度对XAI研究进行分类和比较。

Result: 识别出主要问题为任务描述不足、无上下文研究、对目标用户测试不足；提出应报告用户专业知识以说明研究结果可推广性；给出设计和报告XAI任务的研究指南。

Conclusion: 研究成果有助于研究和设计人员识别相关研究、研究差距以及处理XAI设计的矛盾结果。

Abstract: Research into explainable artificial intelligence (XAI) for data analysis
tasks suffer from a large number of contradictions and lack of concrete design
recommendations stemming from gaps in understanding the tasks that require AI
assistance. In this paper, we drew on multiple fields such as visual analytics,
cognition, and dashboard design to propose a method for categorising and
comparing XAI studies under three dimensions: what, why, and who. We identified
the main problems as: inadequate descriptions of tasks, context-free studies,
and insufficient testing with target users. We propose that studies should
specifically report on their users' domain, AI, and data analysis expertise to
illustrate the generalisability of their findings. We also propose study
guidelines for designing and reporting XAI tasks to improve the XAI community's
ability to parse the rapidly growing field. We hope that our contribution can
help researchers and designers better identify which studies are most relevant
to their work, what gaps exist in the research, and how to handle contradictory
results regarding XAI design.

</details>


### [42] [Instance space analysis of the capacitated vehicle routing problem](https://arxiv.org/abs/2507.10397)
*Alessandra M. M. M. Gouvêa,Nuno Paulos,Eduardo Uchoa e Mariá C. V. Nascimento*

Main category: cs.AI

TL;DR: 本文通过实例空间分析（ISA）方法，结合DIMACS数据集，识别23个相关实例特征，创建二维投影来研究实例特征与元启发式算法性能关系，并提供投影矩阵。


<details>
  <summary>Details</summary>
Motivation: 解决理解实例特征与元启发式算法（MH）性能之间细微关系的挑战，推动CVRP研究。

Method: 采用ISA方法，结合DIMACS 12th数据集，运用降维和机器学习方法，经过PRELIM、SIFTED和PILOT阶段创建实例空间二维投影。

Result: 识别出23个相关实例特征，创建实例空间二维投影。

Conclusion: 提供投影矩阵，为CVRP领域实例分析提供新方法。

Abstract: This paper seeks to advance CVRP research by addressing the challenge of
understanding the nuanced relationships between instance characteristics and
metaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a
valuable tool that allows for a new perspective on the field. By combining the
ISA methodology with a dataset from the DIMACS 12th Implementation Challenge on
Vehicle Routing, our research enabled the identification of 23 relevant
instance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,
which employ dimensionality reduction and machine learning methods, allowed us
to create a two-dimensional projection of the instance space to understand how
the structure of instances affect the behavior of MHs. A key contribution of
our work is that we provide a projection matrix, which makes it straightforward
to incorporate new instances into this analysis and allows for a new method for
instance analysis in the CVRP field.

</details>


### [43] [Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures](https://arxiv.org/abs/2507.10446)
*Sudarshan Babu*

Main category: cs.AI

TL;DR: 本文聚焦数据缺乏场景下设计架构以高效获取先验知识，利用神经记忆、超网络等方法在多领域取得成果。


<details>
  <summary>Details</summary>
Motivation: 在计算化学、计算免疫学和医学成像等数据缺乏领域，无法训练大型预训练模型，需设计架构以高效获取先验知识。

Method: 使用神经记忆在非平稳分布上仅用少量样本实现适应；设计超网络结合MAML获取更具泛化性的先验；将超网络应用于3D场景生成和分割；改造现有分子生成方法作为预训练框架。

Result: 神经记忆能在少量样本下实现适应；超网络在少量训练场景中高效获取先验，实现更快的文本到3D生成及有限数据下的3D分割；改造的分子生成预训练框架改善分子属性预测。

Conclusion: 所提出的架构和方法能在数据缺乏场景下高效获取先验知识，解决多个领域的关键挑战。

Abstract: The ability to transfer knowledge from prior experiences to novel tasks
stands as a pivotal capability of intelligent agents, including both humans and
computational models. This principle forms the basis of transfer learning,
where large pre-trained neural networks are fine-tuned to adapt to downstream
tasks. Transfer learning has demonstrated tremendous success, both in terms of
task adaptation speed and performance. However there are several domains where,
due to lack of data, training such large pre-trained models or foundational
models is not a possibility - computational chemistry, computational
immunology, and medical imaging are examples. To address these challenges, our
work focuses on designing architectures to enable efficient acquisition of
priors when large amounts of data are unavailable. In particular, we
demonstrate that we can use neural memory to enable adaptation on
non-stationary distributions with only a few samples. Then we demonstrate that
our hypernetwork designs (a network that generates another network) can acquire
more generalizable priors than standard networks when trained with Model
Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene
generation, demonstrating that they can acquire priors efficiently on just a
handful of training scenes, thereby leading to faster text-to-3D generation. We
then extend our hypernetwork framework to perform 3D segmentation on novel
scenes with limited data by efficiently transferring priors from earlier viewed
scenes. Finally, we repurpose an existing molecular generative method as a
pre-training framework that facilitates improved molecular property prediction,
addressing critical challenges in computational immunology

</details>


### [44] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Main category: cs.AI

TL;DR: 介绍基于大语言模型的自动化科研综合系统DeepResearch$^{	ext{Eco}}$，在生态研究问题应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 开发支持递归、可控制深度和广度探索原始研究问题的自动化科研综合系统，提升搜索多样性和检索精准度。

Method: 构建DeepResearch$^{	ext{Eco}}$系统，实现用户可控的合成、透明推理和参数驱动的可配置性。

Result: 应用于49个生态研究问题，源整合最多提升21倍，每1000字源整合提升14.9倍，高参数设置有专家级分析深度和上下文多样性。

Conclusion: DeepResearch$^{	ext{Eco}}$系统在科研文献检索和整合方面表现优异，具有高实用性和有效性。

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [45] [StockSim: A Dual-Mode Order-Level Simulator for Evaluating Multi-Agent LLMs in Financial Markets](https://arxiv.org/abs/2507.09255)
*Charidimos Papadakis,Giorgos Filandrianos,Angeliki Dimitriou,Maria Lymperaiou,Konstantinos Thomas,Giorgos Stamou*

Main category: cs.CE

TL;DR: 介绍开源模拟平台StockSim，可在现实金融决策场景中系统评估大语言模型，考虑多因素，支持多策略，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有工具包评估大语言模型在金融决策场景的范围有限，需要更全面的评估系统。

Method: 构建StockSim平台，模拟市场动态，支持多种模拟模式，纳入关键现实因素，采用可扩展的基于角色的代理框架。

Result: 得到可全面评估大语言模型的平台StockSim。

Conclusion: StockSim是用于不确定性推理和序列决策的NLP研究的有力测试平台，代码已开源。

Abstract: We present StockSim, an open-source simulation platform for systematic
evaluation of large language models (LLMs) in realistic financial
decision-making scenarios. Unlike previous toolkits that offer limited scope,
StockSim delivers a comprehensive system that fully models market dynamics and
supports diverse simulation modes of varying granularity. It incorporates
critical real-world factors, such as latency, slippage, and order-book
microstructure, that were previously neglected, enabling more faithful and
insightful assessment of LLM-based trading agents. An extensible, role-based
agent framework supports heterogeneous trading strategies and multi-agent
coordination, making StockSim a uniquely capable testbed for NLP research on
reasoning under uncertainty and sequential decision-making. We open-source all
our code at https: //github.com/harrypapa2002/StockSim.

</details>


### [46] [GeoWarp: An automatically differentiable and GPU-accelerated implicit MPM framework for geomechanics based on NVIDIA Warp](https://arxiv.org/abs/2507.09435)
*Yidong Zhao,Xuan Li,Chenfanfu Jiang,Jinhyun Choo*

Main category: cs.CE

TL;DR: 本文介绍了用于地质力学的隐式MPM框架GeoWarp，利用GPU并行和自动微分计算雅可比矩阵，开发稀疏雅可比构造算法，经实例验证其为可微隐式MPM模拟提供了可靠平台。


<details>
  <summary>Details</summary>
Motivation: 显式时间积分不适用于地质力学中的准静态和长期过程，隐式MPM虽无此限制但因计算雅可比矩阵困难而较少采用。

Method: 基于NVIDIA Warp构建GeoWarp框架，利用GPU并行和反向自动微分计算雅可比矩阵，开发稀疏雅可比构造算法。

Result: 通过大变形弹塑性和耦合多孔力学的正演和反演实例验证了框架。

Conclusion: GeoWarp为计算地质力学中的可微隐式MPM模拟提供了稳健、可扩展和可扩展的平台。

Abstract: The material point method (MPM), a hybrid Lagrangian-Eulerian particle
method, is increasingly used to simulate large-deformation and
history-dependent behavior of geomaterials. While explicit time integration
dominates current MPM implementations due to its algorithmic simplicity, such
schemes are unsuitable for quasi-static and long-term processes typical in
geomechanics. Implicit MPM formulations are free of these limitations but
remain less adopted, largely due to the difficulty of computing the Jacobian
matrix required for Newton-type solvers, especially when consistent tangent
operators should be derived for complex constitutive models. In this paper, we
introduce GeoWarp -- an implicit MPM framework for geomechanics built on NVIDIA
Warp -- that exploits GPU parallelism and reverse-mode automatic
differentiation to compute Jacobians without manual derivation. To enhance
efficiency, we develop a sparse Jacobian construction algorithm that leverages
the localized particle-grid interactions intrinsic to MPM. The framework is
verified through forward and inverse examples in large-deformation
elastoplasticity and coupled poromechanics. Results demonstrate that GeoWarp
provides a robust, scalable, and extensible platform for differentiable
implicit MPM simulation in computational geomechanics.

</details>


### [47] [EV-STLLM: Electric vehicle charging forecasting based on spatio-temporal large language models with multi-frequency and multi-scale information fusion](https://arxiv.org/abs/2507.09527)
*Hang Fan,Yunze Chai,Chenxi Liu,Weican Liu,Zuhan Zhang,Wencai Run,Dunnan Liu*

Main category: cs.CE

TL;DR: 提出新型 EV 时空大语言模型 EV - STLLM 用于准确预测电动汽车充电需求和充电站占用情况，经实验验证性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉电动汽车充电行为复杂的时空依赖，有限的模型参数难以从大数据集中学习复杂数据分布表示，需准确预测充电需求和站占用情况以优化城市能源和聚合商利润。

Method: 框架分为数据处理和预测两个模块。数据处理模块用 VMD 去噪、ICEEMDAN 多频分解、FIG 提取多尺度信息、ReliefF 特征选择；预测模块用 EV - STLLM 预测，通过整合邻接矩阵和时空频率嵌入信息捕捉数据特征，用 PFGA 模块结合预训练大模型和领域知识。

Result: 使用中国深圳真实数据的大量实验表明，该框架比现有基准模型有更高的准确性和鲁棒性。

Conclusion: 所提出的 EV - STLLM 框架在电动汽车充电需求和站占用预测方面表现良好，能满足实际应用需求。

Abstract: With the proliferation of electric vehicles (EVs), accurate charging demand
and station occupancy forecasting are critical for optimizing urban energy and
the profit of EVs aggregator. Existing approaches in this field usually
struggle to capture the complex spatio-temporal dependencies in EV charging
behaviors, and their limited model parameters hinder their ability to learn
complex data distribution representations from large datasets. To this end, we
propose a novel EV spatio-temporal large language model (EV-STLLM) for accurate
prediction. Our proposed framework is divided into two modules. In the data
processing module, we utilize variational mode decomposition (VMD) for data
denoising, and improved complete ensemble empirical mode decomposition with
adaptive noise (ICEEMDAN) for data multi-frequency decomposition. Fuzzy
information granulation (FIG) for extracting multi-scale information.
Additionally, ReliefF is used for feature selection to mitigate redundancy. In
the forecasting module, the EV-STLLM is used to directly achieve EV charging
and occupancy forecasting. Firstly, we fully capture the intrinsic
spatio-temporal characteristics of the data by integrating adjacency matrices
derived from the regional stations network and spatio-temporal-frequency
embedding information. Then, the partially frozen graph attention (PFGA) module
is utilized to maintain the sequential feature modeling capabilities of the
pre-trained large model while incorporating EV domain knowledge. Extensive
experiments using real-world data from Shenzhen, China, demonstrate that our
proposed framework can achieve superior accuracy and robustness compared to the
state-of-the-art benchmarks.

</details>


### [48] [Physics-informed machine learning surrogate for scalable simulation of thermal histories during wire-arc directed energy deposition](https://arxiv.org/abs/2507.09591)
*Michael Ryan,Mohammad Hassan Baqershahi,Hessamoddin Moshayedi,Elyas Ghafoori*

Main category: cs.CE

TL;DR: 本文研究了物理信息神经网络（PINNs）在大规模电弧定向能量沉积（DED）中的可扩展性，发现其能大幅减少计算时间和精力，同时保持精度并提供“超分辨率”，还讨论了提升其在金属增材制造中性能的未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统有限元法（FEM）模拟电弧DED过程计算需求高，传统数据驱动的神经网络需要大量数据，而物理信息神经网络（PINNs）虽有理论优势但在大规模电弧DED结构工程中应用有限，因此研究PINNs的可扩展性。

Method: 研究PINNs的可扩展性，重点关注高效的配点采样，这是控制训练时间和模型性能的关键因素。

Result: PINNs可减少高达98.6%的计算时间和精力，同时保持所需的精度并提供“超分辨率”。

Conclusion: PINNs在大规模电弧DED中具有应用潜力，可大幅减少计算成本，未来可进一步提升其在金属增材制造中的性能。

Abstract: Wire-arc directed energy deposition (DED) has emerged as a promising additive
manufacturing (AM) technology for large-scale structural engineering
applications. However, the complex thermal dynamics inherent to the process
present challenges in ensuring structural integrity and mechanical properties
of fabricated thick walls and plates. While finite element method (FEM)
simulations have been conventionally employed to predict thermal history during
deposition, their computational demand remains prohibitively high for actual
large-scale applications. Given the necessity of multiple repetitive
simulations for heat management and the determination of an optimal printing
strategy, FEM simulation quickly becomes entirely infeasible. Instead,
advancements have been made in using trained neural networks as surrogate
models for rapid prediction. However, traditional data-driven approaches
necessitate large amounts of relevant and verifiable external data, during the
training and validation of the neural network. Regarding large-scale wire-arc
DED, none of these data sources are readily available in quantities sufficient
for an accurate surrogate. The introduction of physics-informed neural networks
(PINNs) has opened up an alternative simulation strategy by leveraging the
existing physical knowledge of the phenomena with advanced machine learning
methods. Despite their theoretical advantages, PINNs have seen limited
application in the context of large-scale wire-arc DED for structural
engineering. This study investigates the scalability of PINNs, focusing on
efficient collocation points sampling, a critical factor controlling both the
training time and model performance. Results show PINNs can reduce
computational time and effort by up to 98.6%, while maintaining the desired
accuracy and offering "super-resolution". Future directions for enhancing PINN
performance in metal AM are discussed.

</details>


### [49] [What Matters Most? A Quantitative Meta-Analysis of AI-Based Predictors for Startup Success](https://arxiv.org/abs/2507.09675)
*Seyed Mohammad Ali Jafari,Ali Mobini Dehkordi,Ehsan Chitsaz,Yadollah Yaghoobzadeh*

Main category: cs.CE

TL;DR: 文章通过元分析综合AI初创企业评估文献，发现预测成功因素具情境依赖性，指出需标准化报告。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习预测初创企业成功的关键预测因素研究零散且具情境特异性，需系统综合证据。

Method: 进行定量元分析，系统回顾确定13项实证研究，提取58个独特预测因素，用加权重要性分数综合其重要性，还进行调节分析。

Result: 最具预测力的是企业特征、投资者结构、数字和社会吸引力、融资历史；层次高度依赖情境。

Conclusion: 预测成功的因素非普遍，受目标、阶段和评估数据影响，指出文献可能存在便利偏差，需标准化报告促进知识积累。

Abstract: Background: Predicting startup success with machine learning is a rapidly
growing field, yet findings on key predictors are often fragmented and
context-specific. This makes it difficult to discern robust patterns and
highlights a need for a systematic synthesis of the evidence.
  Methods: This study conducts a quantitative meta-analysis to synthesize the
literature on predictor importance in AI-based startup evaluation. We performed
a systematic review to identify a final sample of 13 empirical studies that
report rankable feature importance. From these papers, we extracted and
categorized 58 unique predictors, synthesizing their importance using a
Weighted Importance Score (WIS) that balances a feature's average rank with its
frequency of appearance. We also conducted a moderator analysis to investigate
how predictor importance changes with context (e.g., success definition).
  Results: Our aggregate analysis reveals that the most consistently powerful
predictors are a quartet of foundational attributes: Firm Characteristics
(e.g., age, location), Investor Structure (e.g., investor quality), Digital and
Social Traction (e.g., online momentum), and Funding History. The moderator
analysis further reveals that this hierarchy is highly context-dependent. For
instance, predicting near-term funding milestones elevates the importance of
the deal's immediate context, while predicting long-term exits prioritizes
fundamental firm and investor characteristics.
  Conclusion: The factors that best predict startup success are not universal
but are contingent on the startup's goals, stage, and the data used for
evaluation. Our findings point to a potential "convenience bias" in the
literature, where predictor importance may be tied to data accessibility. We
conclude by underscoring the need for standardized reporting practices to
enable more robust, cumulative knowledge building in the field.

</details>


### [50] [Legendre Polynomials and Their Use for Karhunen-Loève Expansion](https://arxiv.org/abs/2507.09825)
*Michal Béreš*

Main category: cs.CE

TL;DR: 本文有两个主要贡献：一是不依赖经典方法为本科生讲解勒让德多项式三项递推关系推导；二是开发高维超矩形域上各向同性高斯随机场的计算框架，能降低内存和计算成本。


<details>
  <summary>Details</summary>
Motivation: 为本科生提供易理解的勒让德多项式推导方法，以及开发高维超矩形域上各向同性高斯随机场高效计算框架。

Method: 推导勒让德多项式时不依赖经典方法；计算框架利用勒让德多项式和高斯求积，用牛顿优化拟合近似协方差核，采用Duffy型变换和求积。

Result: 开发的计算框架在高维下仍有效，能显著降低内存使用和算术成本，且有开源代码复现结果。

Conclusion: 所提出的推导方法适合教学，计算框架在高维超矩形域上处理各向同性高斯随机场有效且高效。

Abstract: This paper makes two main contributions. First, we present a pedagogical
review of the derivation of the three-term recurrence relation for Legendre
polynomials, without relying on the classical Legendre differential equation,
Rodrigues' formula, or generating functions. This exposition is designed to be
accessible to undergraduate students.
  Second, we develop a computational framework for Karhunen-Lo\`eve expansions
of isotropic Gaussian random fields on hyper-rectangular domains. The framework
leverages Legendre polynomials and their associated Gaussian quadrature, and it
remains efficient even in higher spatial dimensions.
  A covariance kernel is first approximated by a non-negative mixture of
squared-exponentials, obtained via a Newton-optimized fit with a theoretically
informed initialization. The resulting separable kernel enables a
Legendre-Galerkin discretization in the form of a Kronecker product over single
dimensions, with submatrices that exhibit even/odd parity structure. For
assembly, we introduce a Duffy-type transformation followed by quadrature.
These structural properties significantly reduce both memory usage and
arithmetic cost compared to naive approaches. All algorithms and numerical
experiments are provided in an open-source repository that reproduces every
figure and table in this work.

</details>


### [51] [Non-smooth optimization meets automated material model discovery](https://arxiv.org/abs/2507.10196)
*Moritz Flaschel,Trevor Hastie,Ellen Kuhl*

Main category: cs.CE

TL;DR: 本文研究自动化材料模型发现中非光滑目标函数最小化问题，提出四种算法并展示其在超弹性材料模型发现中的应用。


<details>
  <summary>Details</summary>
Motivation: 当前自动化材料模型发现文献对非光滑目标函数鲁棒高效最小化的研究有限，需要更好的方法。

Method: 研究f(w) + a ||w||_1形式函数最小化，考虑f为二次和非二次情况，提出四种算法（坐标下降算法、LARS算法、近端梯度法ISTA及ISTA的路径扩展法）。

Result: 展示了四种算法在从单轴拉伸和简单剪切数据中发现超弹性材料模型方面的适用性。

Conclusion: 所提出的四种算法有助于自动化材料模型发现中解决非光滑目标函数最小化问题及选择合适的正则化参数。

Abstract: Automated material model discovery disrupts the tedious and time-consuming
cycle of iteratively calibrating and modifying manually designed models.
Non-smooth L1-norm regularization is the backbone of automated model discovery;
however, the current literature on automated material model discovery offers
limited insights into the robust and efficient minimization of non-smooth
objective functions. In this work, we examine the minimization of functions of
the form f(w) + a ||w||_1, where w are the material model parameters, f is a
metric that quantifies the mismatch between the material model and the observed
data, and a is a regularization parameter that determines the sparsity of the
solution. We investigate both the straightforward case where f is quadratic and
the more complex scenario where it is non-quadratic or even non-convex.
Importantly, we do not only focus on methods that solve the sparse regression
problem for a given value of the regularization parameter a, but propose
methods to efficiently compute the entire regularization path, facilitating the
selection of a suitable a. Specifically, we present four algorithms and discuss
their roles for automated material model discovery in mechanics: First, we
recapitulate a well-known coordinate descent algorithm that solves the
minimization problem assuming that f is quadratic for a given value of a, also
known as the LASSO. Second, we discuss the algorithm LARS, which automatically
determines the critical values of a, at which material parameters in w are set
to zero. Third, we propose to use the proximal gradient method ISTA for
automated material model discovery if f is not quadratic, and fourth, we
suggest a pathwise extension of ISTA for computing the regularization path. We
demonstrate the applicability of all algorithms for the discovery of
hyperelastic material models from uniaxial tension and simple shear data.

</details>


### [52] [FinTeam: A Multi-Agent Collaborative Intelligence System for Comprehensive Financial Scenarios](https://arxiv.org/abs/2507.10448)
*Yingqian Wu,Qiushi Wang,Zefei Long,Rong Ye,Zhongtian Lu,Xianyin Zhang,Bingxuan Li,Wei Chen,Liwen Zhang,Zhongyu Wei*

Main category: cs.CE

TL;DR: 提出金融多智能体协作系统FinTeam，在综合金融任务上评估，表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM模型无法全面分析真实金融场景，金融公司常部门分工。

Method: 提出含四个LLM智能体的FinTeam系统，用构建数据集训练特定金融知识。

Result: 人工评估显示生成报告接受率62%，优于GPT - 4o和轩辕等；在FinCUGE和FinEval上有提升。

Conclusion: FinTeam系统在金融报告生成任务中表现良好，有一定优势。

Abstract: Financial report generation tasks range from macro- to micro-economics
analysis, also requiring extensive data analysis. Existing LLM models are
usually fine-tuned on simple QA tasks and cannot comprehensively analyze real
financial scenarios. Given the complexity, financial companies often distribute
tasks among departments. Inspired by this, we propose FinTeam, a financial
multi-agent collaborative system, with a workflow with four LLM agents:
document analyzer, analyst, accountant, and consultant. We train these agents
with specific financial expertise using constructed datasets. We evaluate
FinTeam on comprehensive financial tasks constructed from real online
investment forums, including macroeconomic, industry, and company analysis. The
human evaluation shows that by combining agents, the financial reports generate
from FinTeam achieved a 62.00% acceptance rate, outperforming baseline models
like GPT-4o and Xuanyuan. Additionally, FinTeam's agents demonstrate a 7.43%
average improvement on FinCUGE and a 2.06% accuracy boost on FinEval. Project
is available at https://github.com/FudanDISC/DISC-FinLLM/.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [53] [Orchestration for Domain-specific Edge-Cloud Language Models](https://arxiv.org/abs/2507.09003)
*Prasoon Patidar,Alex Crown,Kevin Hsieh,Yifei Xu,Tusher Chakraborty,Ranveer Chandra,Yuvraj Agarwal*

Main category: cs.DB

TL;DR: 提出ECO - LLM系统解决LLM边缘云协作问题，评估显示其在准确率、成本和延迟方面有优势。


<details>
  <summary>Details</summary>
Motivation: 传统方法在LLM边缘云协作中只关注选最佳模型，忽略组件间交互和变化的延迟、成本约束。

Method: 引入ECO - LLM系统，将问题视为联合优化挑战，通过系统探索组件配置和动态选择查询级最优策略解决。系统包含ECO - LLM Emulator和ECO - LLM Runtime两个组件。

Result: 在智能家居和智能汽车助手场景评估，对已知查询，准确率达90%高于GPT - 4o，成本降90%、延迟降55%；对未知查询，成本降62%或响应时间提升62%。

Conclusion: ECO - LLM的查询级联合优化有价值，能在保证准确率的同时满足延迟和成本约束。

Abstract: The remarkable performance of Large Language Models (LLMs) has inspired many
applications, which often necessitate edge-cloud collaboration due to
connectivity, privacy, and cost considerations. Traditional methods primarily
focus on selecting the best LLM model for optimizing performance, while
neglecting the critical interplay between the components of the LLM serving
pipeline (context retrieval, query preprocessing, etc.) or the changing latency
and cost constraints. We introduce ECO-LLM (Edge-Cloud Orchestrator for LLMs),
a novel system that reframes this problem as a joint optimization challenge and
solves it by systematically exploring component configurations and dynamically
selecting optimal strategies at the query level. ECO-LLM consists of two
components: (1) the ECO-LLM Emulator, which efficiently explores the vast
configuration space utilizing query clustering and pareto-optimal path
selection, gathering domain-specific performance metrics without exhaustive
evaluation; and (2) the ECO-LLM Runtime, which leverages these metrics to
dynamically select optimal resolution strategies for user queries while meeting
user-defined Service Level Objectives (SLOs). We evaluate ECO-LLM on a smart
home and a smart car assistant scenarios. With an exhaustive exploration of all
possible configurations for seen queries, ECO-LLM outperforms cloud-based
models like GPT-4o in terms of accuracy (90% vs. 74% on average) while reducing
costs by 90% and latency by 55%, demonstrating the value of its joint
optimization at the query level. In practical deployment for previously unseen
queries, ECO-LLM selects configurations that reduce costs by 62% or improve
response times by 62% on average compared to state-of-the-art model routing
approaches, while maintaining higher accuracy and consistently adhering to
specified latency and cost constraints.

</details>


### [54] [HedraRAG: Coordinating LLM Generation and Database Retrieval in Heterogeneous RAG Serving](https://arxiv.org/abs/2507.09138)
*Zhengding Hu,Vibha Murthy,Zaifeng Pan,Wanlu Li,Xiaoyi Fang,Yufei Ding,Yuke Wang*

Main category: cs.DB

TL;DR: 提出HedraRAG运行时系统解决异构RAG服务挑战，通过图变换和映射到混合CPU - GPU管道提升性能，评估显示显著加速。


<details>
  <summary>Details</summary>
Motivation: 解决异构检索增强生成（RAG）服务中因复杂多级工作流和多样请求模式导致的高效执行挑战。

Method: 构建基于图抽象的HedraRAG运行时系统，通过节点拆分、重排序、添加边和依赖重连等动态图变换，将执行计划映射到混合CPU - GPU管道。

Result: 在多种RAG工作流评估中，比现有框架实现超过1.5倍、最高达5倍的加速。

Conclusion: 协调生成和检索在服务环境中是有效的。

Abstract: This paper addresses emerging system-level challenges in heterogeneous
retrieval-augmented generation (RAG) serving, where complex multi-stage
workflows and diverse request patterns complicate efficient execution. We
present HedraRAG, a runtime system built on a graph-based abstraction that
exposes optimization opportunities across stage-level parallelism,
intra-request similarity, and inter-request skewness. These opportunities are
realized through dynamic graph transformations, such as node splitting,
reordering, edge addition, and dependency rewiring, applied to wavefronts of
subgraphs spanning concurrent requests. The resulting execution plans are
mapped onto hybrid CPU-GPU pipelines to improve resource utilization and reduce
latency. Evaluations across a wide range of RAG workflows demonstrate speedups
exceeding 1.5x and reaching up to 5x over existing frameworks, showcasing the
effectiveness of coordinated generation and retrieval in serving environments.

</details>


### [55] [TRACER: Efficient Object Re-Identification in Networked Cameras through Adaptive Query Processing](https://arxiv.org/abs/2507.09448)
*Pramod Chunduri,Yao Lu,Joy Arulraj*

Main category: cs.DB

TL;DR: 提出新的视频数据库管理系统Tracer处理Re - ID查询，有自适应查询框架，还给出合成基准，性能超现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有Spatula系统在处理Re - ID查询时有精度和自适应查询处理方面的局限，不适合高召回要求的应用。

Method: 用自适应查询处理框架，训练循环网络选最优相机，采用概率自适应搜索模型，提出合成基准生成多相机Re - ID数据集。

Result: Tracer在不同数据集上平均比现有跨相机分析系统性能高3.9倍。

Conclusion: Tracer在处理Re - ID查询上比现有系统更高效。

Abstract: Efficiently re-identifying and tracking objects across a network of cameras
is crucial for applications like traffic surveillance. Spatula is the
state-of-the-art video database management system (VDBMS) for processing Re-ID
queries. However, it suffers from two limitations. Its spatio-temporal
filtering scheme has limited accuracy on large camera networks due to localized
camera history. It is not suitable for critical video analytics applications
that require high recall due to a lack of support for adaptive query
processing.
  In this paper, we present Tracer, a novel VDBMS for efficiently processing
Re-ID queries using an adaptive query processing framework. Tracer selects the
optimal camera to process at each time step by training a recurrent network to
model long-term historical correlations. To accelerate queries under a high
recall constraint, Tracer incorporates a probabilistic adaptive search model
that processes camera feeds in incremental search windows and dynamically
updates the sampling probabilities using an exploration-exploitation strategy.
To address the paucity of benchmarks for the Re-ID task due to privacy
concerns, we present a novel synthetic benchmark for generating multi-camera
Re-ID datasets based on real-world traffic distribution. Our evaluation shows
that Tracer outperforms the state-of-the-art cross-camera analytics system by
3.9x on average across diverse datasets.

</details>


### [56] [THOR: Transformer Heuristics for On-Demand Retrieval](https://arxiv.org/abs/2507.09592)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.DB

TL;DR: 介绍eSapiens设计实现的THOR模块，可将自然语言问题转化为企业数据库的SQL分析，经多项测试能可靠查询和报告，让非技术用户安全访问数据。


<details>
  <summary>Details</summary>
Motivation: 让非技术用户能以简单且安全的方式访问企业数据库的实时数据。

Method: 采用解耦的编排/执行架构，包含多个代理，有集成的自我纠正和评级循环。

Result: 在金融、销售和运营场景的冒烟测试中，实现可靠的即席查询和自动化定期报告。

Conclusion: THOR模块通过嵌入多种能力，使非技术用户能以零SQL的简单方式和企业级的安全性访问实时数据。

Abstract: We introduce the THOR (Transformer Heuristics for On-Demand Retrieval)
Module, designed and implemented by eSapiens, a secure, scalable engine that
transforms natural-language questions into verified, read-only SQL analytics
for enterprise databases. The Text-to-SQL module follows a decoupled
orchestration/execution architecture: a Supervisor Agent routes queries, Schema
Retrieval dynamically injects table and column metadata, and a SQL Generation
Agent emits single-statement SELECT queries protected by a read-only guardrail.
An integrated Self-Correction & Rating loop captures empty results, execution
errors, or low-quality outputs and triggers up to five LLM-driven regeneration
attempts. Finally, a Result Interpretation Agent produces concise,
human-readable insights and hands raw rows to the Insight & Intelligence engine
for visualization or forecasting.
  Smoke tests across finance, sales, and operations scenarios demonstrate
reliable ad-hoc querying and automated periodic reporting. By embedding schema
awareness, fault-tolerant execution, and compliance guardrails, the THOR Module
empowers non-technical users to access live data with zero-SQL simplicity and
enterprise-grade safety.

</details>


### [57] [Rethinking LSM-tree based Key-Value Stores: A Survey](https://arxiv.org/abs/2507.09642)
*Yina Lv,Qiao Li,Quanqing Xu,Congming Gao,Chuanhui Yang,Xiaoli Wang,Chun Jason Xue*

Main category: cs.DB

TL;DR: 本文对近五年LSM - tree优化的代表性工作进行综述，研究现有缓解flush和compaction性能影响及改进基本键值操作的方案，分析分布式键值存储新挑战与机遇，并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: LSM - tree的compaction不可预测性带来诸多挑战，虽有前人研究，但近年仍有相关优化研究提出，需要对其进行综述。

Method: 回顾近五年LSM - tree优化的代表性工作，研究现有解决方案，分析现代架构和不同应用场景中的情况。

Result: 梳理了缓解LSM - tree flush和compaction性能影响以及改进基本键值操作的方案，分析了分布式键值存储新挑战与机遇。

Conclusion: 详细讨论了LSM - tree优化的最新工作，给出了未来研究方向。

Abstract: LSM-tree is a widely adopted data structure in modern key-value store systems
that optimizes write performance in write-heavy applications by using append
writes to achieve sequential writes. However, the unpredictability of LSM-tree
compaction introduces significant challenges, including performance variability
during peak workloads and in resource-constrained environments, write
amplification caused by data rewriting during compactions, read amplification
from multi-level queries, trade-off between read and write performance, as well
as efficient space utilization to mitigate space amplification. Prior studies
on LSM-tree optimizations have addressed the above challenges; however, in
recent years, research on LSM-tree optimization has continued to propose. The
goal of this survey is to review LSM-tree optimization, focusing on
representative works in the past five years. This survey first studies existing
solutions on how to mitigate the performance impact of LSM-tree flush and
compaction and how to improve basic key-value operations. In addition,
distributed key-value stores serve multi-tenants, ranging from tens of
thousands to millions of users with diverse requirements. We then analyze the
new challenges and opportunities in these modern architectures and across
various application scenarios. Unlike the existing survey papers, this survey
provides a detailed discussion of the state-of-the-art work on LSM-tree
optimizations and gives future research directions.

</details>


### [58] [Efficient Temporal Simple Path Graph Generation](https://arxiv.org/abs/2507.10017)
*Zhiyang Tang,Yanping Wu,Xiangjun Zai,Chen Chen,Xiaoyang Wang,Ying Zhang*

Main category: cs.DB

TL;DR: 本文首次提出并研究生成时间简单路径图（tspG）问题，提出 Verification in Upper - bound Graph 方法加速处理，实验证明其高效性和有效性。


<details>
  <summary>Details</summary>
Motivation: 探索基于时间路径的顶点关系是基础任务，直接枚举所有时间简单路径构建 tspG 计算成本高，需高效方法。

Method: 提出 Verification in Upper - bound Graph 方法，先结合时间路径和简单路径约束排除原图中无前景的边得到上界图，再用上界图中的 Escape Edges Verification 算法构建精确 tspG。

Result: 在 10 个真实世界图上进行了全面实验。

Conclusion: 所提出的技术具有高效性和有效性。

Abstract: Interactions between two entities often occur at specific timestamps, which
can be modeled as a temporal graph. Exploring the relationships between
vertices based on temporal paths is one of the fundamental tasks. In this
paper, we conduct the first research to propose and investigate the problem of
generating the temporal simple path graph (tspG), which is the subgraph
consisting of all temporal simple paths from the source vertex to the target
vertex within the given time interval. Directly enumerating all temporal simple
paths and constructing the tspG is computationally expensive. To accelerate the
processing, we propose an efficient method named Verification in Upper-bound
Graph. It first incorporates the temporal path constraint and simple path
constraint to exclude unpromising edges from the original graph, which obtains
a tight upper-bound graph as a high-quality approximation of the tspG in
polynomial time. Then, an Escape Edges Verification algorithm is further
applied in the upper-bound graph to construct the exact tspG without
exhaustively enumerating all temporal simple paths between given vertices.
Finally, comprehensive experiments on 10 real-world graphs are conducted to
demonstrate the efficiency and effectiveness of the proposed techniques.

</details>


### [59] [Breaking the Storage-Compute Bottleneck in Billion-Scale ANNS: A GPU-Driven Asynchronous I/O Framework](https://arxiv.org/abs/2507.10070)
*Yang Xiao,Mo Sun,Ziyu Song,Bing Tian,Jie Zhang,Jie Sun,Zeke Wang*

Main category: cs.DB

TL;DR: 本文提出GPU加速的离核图ANNS系统FlashANNS，通过创新方法解决现有系统性能问题，实验显示其吞吐量有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于磁盘的ANNS系统存在SSD访问与距离计算无法重叠、I/O栈不佳导致I/O延迟长的问题，性能欠佳。

Method: 提出FlashANNS系统，采用依赖松弛异步流水线、线程束级并发SSD访问、计算 - I/O平衡图度选择三个创新方法实现I/O与计算同步编排。

Result: 在≥95% recall@10准确率下，单SSD时吞吐量比现有SOTA方法高2.3 - 5.9倍，多SSD配置下吞吐量提升2.7 - 12.2倍。

Conclusion: FlashANNS系统有效解决了现有基于磁盘的ANNS系统的性能问题，提升了系统吞吐量。

Abstract: With the advancement of information retrieval, recommendation systems, and
Retrieval-Augmented Generation (RAG), Approximate Nearest Neighbor Search
(ANNS) gains widespread applications due to its higher performance and
accuracy. While several disk-based ANNS systems have emerged to handle
exponentially growing vector datasets, they suffer from suboptimal performance
due to two inherent limitations: 1) failing to overlap SSD accesses with
distance computation processes and 2) extended I/O latency caused by suboptimal
I/O Stack. To address these challenges, we present FlashANNS, a GPU-accelerated
out-of-core graph-based ANNS system through I/O-compute overlapping. Our core
insight lies in the synchronized orchestration of I/O and computation through
three key innovations: 1) Dependency-Relaxed asynchronous pipeline: FlashANNS
decouples I/O-computation dependencies to fully overlap between GPU distance
calculations and SSD data transfers. 2) Warp-Level concurrent SSD access:
FlashANNS implements a lock-free I/O stack with warp-level concurrency control,
to reduce the latency-induced time overhead. 3) Computation-I/O balanced graph
degree Selection: FlashANNS selects graph degrees via lightweight
compute-to-I/O ratio sampling, ensuring optimal balance between computational
load and storage access latency across different I/O bandwidth configurations.
We implement FlashANNS and compare it with state-of-the-art out-of-core ANNS
systems (SPANN, DiskANN) and a GPU-accelerated out-of-core ANNS system
(FusionANNS). Experimental results demonstrate that at $\geq$95\% recall@10
accuracy, our method achieves 2.3-5.9$\times$ higher throughput compared to
existing SOTA methods with a single SSD, and further attains 2.7-12.2$\times$
throughput improvement in multi-SSD configurations.

</details>


### [60] [LogLite: Lightweight Plug-and-Play Streaming Log Compression](https://arxiv.org/abs/2507.10337)
*Benzhao Tang,Shiyu Yang,Zhitao Shen,Wenjie Zhang,Xuemin Lin,Zhihong Tian*

Main category: cs.DB

TL;DR: 日志数据量大导致收集存储成本高，本文提出LogLite无损压缩算法，相比基线有显著压缩率和速度提升。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统和物联网设备产生的日志数据量激增，带来高收集和存储成本，需无损日志压缩解决。

Method: 先对大量公共日志数据集进行特征研究，得出四个关键观察，据此提出轻量级、即插即用、流式的无损压缩算法LogLite。

Result: 与现有基线相比，LogLite在多数场景达到帕累托最优，压缩率平均提升达67.8%，压缩速度最高提升2.7倍。

Conclusion: LogLite是处理TEXT和JSON日志生命周期的有效无损压缩算法，无需预定义规则和预训练，适应日志结构变化。

Abstract: Log data is a vital resource for capturing system events and states. With the
increasing complexity and widespread adoption ofmodern software systems and IoT
devices, the daily volume of log generation has surged to tens of petabytes,
leading to significant collection and storage costs. To address this challenge,
lossless log compression has emerged as an effective solution, enabling
substantial resource savings without compromising log information. In this
paper, we first conduct a characterization study on extensive public log
datasets and identify four key observations. Building on these insights, we
propose LogLite, a lightweight, plug-and-play, streaming lossless compression
algorithm designed to handle both TEXT and JSON logs throughout their life
cycle. LogLite requires no predefined rules or pre-training and is inherently
adaptable to evolving log structures. Our evaluation shows that, compared to
state-of-the-art baselines, LogLite achieves Pareto optimality in most
scenarios, delivering an average improvement of up to 67.8% in compression
ratio and up to 2.7 $\times$ in compression speed.

</details>


### [61] [Instance-Optimized String Fingerprints](https://arxiv.org/abs/2507.10391)
*Mihail Stoian,Johannes Thürauf,Andreas Zimmerer,Alexander van Renen,Andreas Kipf*

Main category: cs.DB

TL;DR: 云数据仓库文本多但处理字符串列能力有限，引入字符串指纹可近似LIKE谓词，经优化能加速表扫描。


<details>
  <summary>Details</summary>
Motivation: 云数据仓库处理字符串列能力有限，需更高效处理方法。

Method: 引入字符串指纹作为轻量级二级索引结构，用混合整数优化为特定工作负载优化指纹。

Result: 在DuckDB v1.3中对IMDb列评估，表扫描速度提升达1.36倍。

Conclusion: 字符串指纹可优化特定工作负载，能泛化到未见表谓词，有效提升表扫描速度。

Abstract: Recent research found that cloud data warehouses are text-heavy. However,
their capabilities for efficiently processing string columns remain limited,
relying primarily on techniques like dictionary encoding and prefix-based
partition pruning. In recent work, we introduced string fingerprints - a
lightweight secondary index structure designed to approximate LIKE predicates,
albeit with false positives. This approach is particularly compelling for
columnar query engines, where fingerprints can help reduce both compute and I/O
overhead. We show that string fingerprints can be optimized for specific
workloads using mixed-integer optimization, and that they can generalize to
unseen table predicates. On an IMDb column evaluated in DuckDB v1.3, this
yields table-scan speedups of up to 1.36$\times$.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [62] [MQFQ-Sticky: Fair Queueing For Serverless GPU Functions](https://arxiv.org/abs/2507.08954)
*Alexander Fuerst,Siddharth Anil,Vishakha Dixit,Purushottam,Kulkarni,Prateek Sharma*

Main category: cs.DC

TL;DR: 论文提出以黑盒方式提供GPU加速的FaaS系统，开发MQFQ - Sticky方法，相比现有策略可将函数延迟降低2 - 20倍。


<details>
  <summary>Details</summary>
Motivation: 常见云抽象如FaaS未充分支持GPU，而许多FaaS应用能从GPU加速中受益，但现有FaaS框架因GPU与FaaS编程模型不匹配无法提供加速，且FaaS工作负载动态异构，挑战被放大。

Method: 将I/O调度原则应用于GPU上的函数调度，开发MQFQ - Sticky，一种集成公平队列和GPU内存管理的方法。

Result: 在一系列工作负载上的实证评估表明，相比现有的GPU和CPU排队策略，该方法将函数延迟降低了2倍到20倍。

Conclusion: MQFQ - Sticky方法能有效平衡局部性、公平性和延迟之间的权衡，可用于以黑盒方式在FaaS系统中提供GPU加速。

Abstract: Hardware accelerators like GPUs are now ubiquitous in data centers, but are
not fully supported by common cloud abstractions such as Functions as a Service
(FaaS). Many popular and emerging FaaS applications such as machine learning
and scientific computing can benefit from GPU acceleration. However, FaaS
frameworks (such as OpenWhisk) are not capable of providing this acceleration
because of the impedance mismatch between GPUs and the FaaS programming model,
which requires virtualization and sandboxing of each function. The challenges
are amplified due to the highly dynamic and heterogeneous FaaS workloads. This
paper presents the design and implementation of a FaaS system for providing GPU
acceleration in a black-box manner (without modifying function code). Running
small functions in containerized sandboxes is challenging due to limited GPU
concurrency and high cold-start overheads, resulting in heavy queueing of
function invocations. We show how principles from I/O scheduling, such as fair
queuing and anticipatory scheduling, can be translated to function scheduling
on GPUs. We develop MQFQ-Sticky, an integrated fair queueing and GPU memory
management approach, which balances the tradeoffs between locality, fairness,
and latency. Empirical evaluation on a range of workloads shows that it reduces
function latency by 2x to 20x compared to existing GPU and CPU queueing
policies.

</details>


### [63] [Lightweight Federated Learning over Wireless Edge Networks](https://arxiv.org/abs/2507.09546)
*Xiangwang Hou,Jingjing Wang,Jun Du,Chunxiao Jiang,Yong Ren,Dusit Niyato*

Main category: cs.DC

TL;DR: 随着无线设备数据增长，传统集中式ML有问题，本文提出轻量级联邦学习框架LTFL，实验显示其优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 集中式ML存在通信开销和隐私问题，联邦学习在无线网络实际部署有挑战。

Method: 提出集成无线传输功率控制、模型剪枝和梯度量化的LTFL框架；推导FL收敛差距的闭式表达式；制定优化问题；推导最优模型剪枝率和梯度量化水平的闭式解，用贝叶斯优化进行传输功率控制。

Result: 在真实数据集上的大量实验表明，LTFL优于现有方案。

Conclusion: 所提出的LTFL框架在解决联邦学习在无线网络部署问题上有效。

Abstract: With the exponential growth of smart devices connected to wireless networks,
data production is increasing rapidly, requiring machine learning (ML)
techniques to unlock its value. However, the centralized ML paradigm raises
concerns over communication overhead and privacy. Federated learning (FL)
offers an alternative at the network edge, but practical deployment in wireless
networks remains challenging. This paper proposes a lightweight FL (LTFL)
framework integrating wireless transmission power control, model pruning, and
gradient quantization. We derive a closed-form expression of the FL convergence
gap, considering transmission error, model pruning error, and gradient
quantization error. Based on these insights, we formulate an optimization
problem to minimize the convergence gap while meeting delay and energy
constraints. To solve the non-convex problem efficiently, we derive closed-form
solutions for the optimal model pruning ratio and gradient quantization level,
and employ Bayesian optimization for transmission power control. Extensive
experiments on real-world datasets show that LTFL outperforms state-of-the-art
schemes.

</details>


### [64] [Intelligent Task Management via Dynamic Multi-region Division in LEO Satellite Networks](https://arxiv.org/abs/2507.09926)
*Zixuan Song,Zhishu Shen,Xiaoyu Zheng,Qiushi Zheng,Zheng Lei,Jiong Jin*

Main category: cs.DC

TL;DR: 提出用于低轨卫星网络智能任务管理的动态多区域划分框架及算法，仿真显示其在任务延迟、能耗和完成率上优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 低轨卫星网络中卫星资源有限、计算负载分布不均导致星间链路拥塞，需有效管理网络拓扑以平衡任务分配。

Method: 提出动态多区域划分框架，基于遗传算法的动态多区域划分算法，结合基于多智能体深度确定性策略梯度的自适应路由算法和任务拆分卸载方案。

Result: 仿真表明提出的框架在任务延迟、每任务能耗和任务完成率方面优于对比方法。

Conclusion: 所提出的动态多区域划分框架及相关算法能有效应对低轨卫星网络任务管理问题，提升网络性能。

Abstract: As a key complement to terrestrial networks and a fundamental component of
future 6G systems, Low Earth Orbit (LEO) satellite networks are expected to
provide high-quality communication services when integrated with ground-based
infrastructure, thereby attracting significant research interest. However, the
limited satellite onboard resources and the uneven distribution of
computational workloads often result in congestion along inter-satellite links
(ISLs) that degrades task processing efficiency. Effectively managing the
dynamic and large-scale topology of LEO networks to ensure balanced task
distribution remains a critical challenge. To this end, we propose a dynamic
multi-region division framework for intelligent task management in LEO
satellite networks. This framework optimizes both intra- and inter-region
routing to minimize task delay while balancing the utilization of computational
and communication resources. Based on this framework, we propose a dynamic
multi-region division algorithm based on the Genetic Algorithm (GA), which
adaptively adjusts the size of each region based on the workload status of
individual satellites. Additionally, we incorporate an adaptive routing
algorithm and a task splitting and offloading scheme based on Multi-Agent Deep
Deterministic Policy Gradient (MA-DDPG) to effectively accommodate the arriving
tasks. Simulation results demonstrate that our proposed framework outperforms
comparative methods in terms of the task delay, energy consumption per task,
and task completion rate.

</details>


### [65] [EAT: QoS-Aware Edge-Collaborative AIGC Task Scheduling via Attention-Guided Diffusion Reinforcement Learning](https://arxiv.org/abs/2507.10026)
*Zhifei Xu,Zhiqing Tang,Jiong Lou,Zhi Yao,Xuan Xie,Tian Wang,Yinglong Wang,Weijia Jia*

Main category: cs.DC

TL;DR: 本文提出QoS感知的边缘协作AIGC任务调度算法EAT，能降低推理延迟，开源代码已发布。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC模型在网络边缘应用存在延迟、资源利用不合理等问题，无法平衡推理延迟和质量。

Method: 将AIGC任务分割并调度到不同边缘服务器，提出基于强化学习的EAT算法，开发AIGC任务调度系统。

Result: 实验表明EAT算法相比基线能最多降低56%的推理延迟。

Conclusion: EAT算法能有效降低AIGC任务推理延迟，提升性能。

Abstract: The growth of Artificial Intelligence (AI) and large language models has
enabled the use of Generative AI (GenAI) in cloud data centers for diverse
AI-Generated Content (AIGC) tasks. Models like Stable Diffusion introduce
unavoidable delays and substantial resource overhead, which are unsuitable for
users at the network edge with high QoS demands. Deploying AIGC services on
edge servers reduces transmission times but often leads to underutilized
resources and fails to optimally balance inference latency and quality. To
address these issues, this paper introduces a QoS-aware
\underline{E}dge-collaborative \underline{A}IGC \underline{T}ask scheduling
(EAT) algorithm. Specifically: 1) We segment AIGC tasks and schedule patches to
various edge servers, formulating it as a gang scheduling problem that balances
inference latency and quality while considering server heterogeneity, such as
differing model distributions and cold start issues. 2) We propose a
reinforcement learning-based EAT algorithm that uses an attention layer to
extract load and task queue information from edge servers and employs a
diffusion-based policy network for scheduling, efficiently enabling model
reuse. 3) We develop an AIGC task scheduling system that uses our EAT algorithm
to divide tasks and distribute them across multiple edge servers for
processing. Experimental results based on our system and large-scale
simulations show that our EAT algorithm can reduce inference latency by up to
56\% compared to baselines. We release our open-source code at
https://github.com/zzf1955/EAT.

</details>


### [66] [FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline](https://arxiv.org/abs/2507.10367)
*Jingwei Xu,Junbin Kang,Mingkai Dong,Mingyu Liu,Lu Zhang,Shaohong Guo,Ziyan Qiu,Mingzhen You,Ziyi Tian,Anqi Yu,Tianhong Ding,Xinwei Hu,Haibo Chen*

Main category: cs.DC

TL;DR: 提出针对深度学习管道优化的无状态客户端架构分布式文件系统FalconFS，性能优于CephFS和Lustre，已在华为生产环境运行一年。


<details>
  <summary>Details</summary>
Motivation: 发现客户端状态（如缓存）在深度学习管道中不仅无效，还消耗宝贵内存资源，需要优化。

Method: 采用无状态客户端架构，在服务器端用混合元数据索引和惰性命名空间复制高效解析路径，用并发请求合并提高服务器并发，用VFS快捷方式方便部署。

Result: 与CephFS和Lustre相比，FalconFS小文件读写吞吐量最高达5.72倍，深度学习模型训练吞吐量最高达12.81倍。

Conclusion: FalconFS在性能上有显著提升，且能在大规模生产环境稳定运行。

Abstract: Client-side metadata caching has long been considered an effective method for
accelerating metadata operations in distributed file systems (DFSs). However,
we have found that client-side state (e.g., caching) is not only ineffective
but also consumes valuable memory resources in the deep learning pipelines. We
thus propose FalconFS, a DFS optimized for deep learning pipelines with the
stateless-client architecture. Specifically, instead of performing client-side
path resolution and caching, FalconFS efficiently resolves paths on the server
side using hybrid metadata indexing and lazy namespace replication. FalconFS
also boosts server concurrency with concurrent request merging and provides
easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show
that FalconFS achieves up to 5.72$\times$ throughput for small file read/write
and up to 12.81$\times$ throughput for deep learning model training. FalconFS
has been running in Huawei autonomous driving system's production environment
with 10,000 NPUs for one year.

</details>


### [67] [ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism](https://arxiv.org/abs/2507.10069)
*Zedong Liu,Shenggan Cheng,Guangming Tan,Yang You,Dingwen Tao*

Main category: cs.DC

TL;DR: 论文指出多模态大语言模型推理开销大，提出弹性多模态并行范式EMP及服务系统ElasticMM，实验表明其性能优于SOTA系统。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型因额外组件、复杂推理流程和异构工作负载导致推理开销大，现有服务架构难以区分混合请求类型和调整并行策略，需高效服务方案。

Method: 提出EMP范式，基于此开发ElasticMM系统，包括通过模态感知负载均衡器分离请求、弹性分区调度解耦推理阶段、统一多模态前缀缓存和非阻塞编码。

Result: 在不同真实数据集上实验，ElasticMM比SOTA服务系统将TTFT降低4.2倍，吞吐量提高3.2 - 4.5倍，且满足服务水平目标。

Conclusion: ElasticMM能有效解决多模态大语言模型服务效率问题，在性能上超越现有系统。

Abstract: Multimodal large language models (MLLMs) extend LLMs to handle images,
videos, and audio by incorporating feature extractors and projection modules.
However, these additional components -- combined with complex inference
pipelines and heterogeneous workloads -- introduce significant inference
overhead. Therefore, efficiently serving MLLMs remains a major challenge.
Current tightly coupled serving architectures struggle to distinguish between
mixed request types or adapt parallelism strategies to different inference
stages, leading to increased time-to-first-token (TTFT) latency and poor
resource utilization. To address this, we propose Elastic Multimodal
Parallelism (EMP), a new serving paradigm that elastically adapts to resource
heterogeneity across request types and inference stages. Building upon EMP, we
develop ElasticMM, an MLLM serving system that (1) separates requests into
independent modality groups with dynamic resource allocation via a
modality-aware load balancer; (2) decouples inference stages and enables
parallelism adjustment and adaptive scaling via elastic partition scheduling;
and (3) improves inference efficiency through unified multimodal prefix caching
and non-blocking encoding. Experiments on diverse real-world datasets show that
ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by
up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level
objectives (SLOs).

</details>


### [68] [Large-Scale Graph Building in Dynamic Environments: Low Latency and High Quality](https://arxiv.org/abs/2507.10139)
*Filipe Miguel Gonçalves de Almeida,CJ Carey,Hendrik Fichtenberger,Jonathan Halcrow,Silvio Lattanzi,André Linhares,Tao Meng,Ashkan Norouzi-Fard,Nikos Parotsidis,Bryan Perozzi,David Simcha*

Main category: cs.DC

TL;DR: 本文介绍能继承Grale优势、在动态环境中低延迟构建图的系统Dynamic GUS，该系统有广泛应用，如加速安卓安全与隐私检测。


<details>
  <summary>Details</summary>
Motivation: 现有工具Grale不适用于数据动态更新场景，近似最近邻系统有局限性，需新系统满足动态图构建低延迟需求。

Method: 开发了继承Grale优势的Dynamic Grale Using ScaNN（Dynamic GUS）系统。

Result: Dynamic GUS系统在谷歌有超10次部署，在安卓安全与隐私领域能使有害应用检测速度提升4倍。

Conclusion: Dynamic GUS系统能在动态环境中以低延迟构建图，有广泛应用价值。

Abstract: Learning and constructing large-scale graphs has attracted attention in
recent decades, resulting in a rich literature that introduced various systems,
tools, and algorithms. Grale is one of such tools that is designed for offline
environments and is deployed in more than 50 different industrial settings at
Google. Grale is widely applicable because of its ability to efficiently learn
and construct a graph on datasets with multiple types of features. However, it
is often the case that applications require the underlying data to evolve
continuously and rapidly and the updated graph needs to be available with low
latency. Such setting make the use of Grale prohibitive. While there are
Approximate Nearest Neighbor (ANN) systems that handle dynamic updates with low
latency, they are mostly limited to similarities over a single embedding.
  In this work, we introduce a system that inherits the advantages and the
quality of Grale, and maintains a graph construction in a dynamic setting with
tens of milliseconds of latency per request. We call the system Dynamic Grale
Using ScaNN (Dynamic GUS). Our system has a wide range of applications with
over 10 deployments at Google. One of the applications is in Android Security
and Privacy, where Dynamic Grale Using ScaNN enables capturing harmful
applications 4 times faster, before they can reach users.

</details>


### [69] [Past-Future Scheduler for LLM Serving under SLA Guarantees](https://arxiv.org/abs/2507.10150)
*Ruihao Gong,Shihao Bai,Siyu Wu,Yunqian Fan,Zaijun Wang,Xiuhong Li,Hailong Yang,Xianglong Liu*

Main category: cs.DC

TL;DR: 为降低大语言模型部署成本，提出Past - Future调度器并开发LightLLM框架，相比现有调度器有更好的吞吐量且开源。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型服务框架的连续批处理因请求输出长度多样性，在估计内存消耗时不准确，无法在严格SLA保证下获得满意吞吐量。

Method: 提出Past - Future调度器，通过考虑请求输出长度的历史分布和计算未来各时间点的内存占用，精确估计运行批处理所需的峰值内存资源；开发实现该调度器的高性能LLM服务框架LightLLM。

Result: LightLLM相比现有激进或保守调度器展示出更优的吞吐量，在重载下比其他调度器高2 - 3倍。

Conclusion: Past - Future调度器和LightLLM框架能适应各种输入输出长度分布的应用，平衡请求排队和有害驱逐，可提升吞吐量，且开源利于相关研究。

Abstract: The exploration and application of Large Language Models (LLMs) is thriving.
To reduce deployment costs, continuous batching has become an essential feature
in current service frameworks. The effectiveness of continuous batching relies
on an accurate estimate of the memory requirements of requests. However, due to
the diversity in request output lengths, existing frameworks tend to adopt
aggressive or conservative schedulers, which often result in significant
overestimation or underestimation of memory consumption. Consequently, they
suffer from harmful request evictions or prolonged queuing times, failing to
achieve satisfactory throughput under strict Service Level Agreement (SLA)
guarantees (a.k.a. goodput), across various LLM application scenarios with
differing input-output length distributions. To address this issue, we propose
a novel Past-Future scheduler that precisely estimates the peak memory
resources required by the running batch via considering the historical
distribution of request output lengths and calculating memory occupancy at each
future time point. It adapts to applications with all types of input-output
length distributions, balancing the trade-off between request queuing and
harmful evictions, thereby consistently achieving better goodput. Furthermore,
to validate the effectiveness of the proposed scheduler, we developed a
high-performance LLM serving framework, LightLLM, that implements the
Past-Future scheduler. Compared to existing aggressive or conservative
schedulers, LightLLM demonstrates superior goodput, achieving up to 2-3$\times$
higher goodput than other schedulers under heavy loads. LightLLM is open source
to boost the research in such direction (https://github.com/ModelTC/lightllm).

</details>


### [70] [Cross-Timeslot Optimization for Distributed GPU Inference Using Reinforcement Learning](https://arxiv.org/abs/2507.10259)
*Chengze Du,Zhiwei Yu,Heng Xu,Haojie Wang,Bo liu,Jialong Li*

Main category: cs.DC

TL;DR: 现有调度系统缺乏时间感知，本文提出 TORTA 调度框架，实验表明其能降低响应时间、提高负载均衡并削减运营成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型服务对分布式 GPU 推理基础设施需求增加，现有调度系统缺乏时间感知，导致 GPU 利用率低、任务迁移开销大、系统响应差。

Method: 提出 Temporal Optimal Resource scheduling via Two - layer Architecture (TORTA)，采用时空调度框架，有两层设计，宏观调度器用强化学习和最优传输协调区域间任务分配，微观分配器优化区域内任务到服务器的分配。

Result: 在多个网络拓扑实验中，与现有基线方法相比，TORTA 平均推理响应时间最多降低 15%，负载均衡提高约 4 - 5%，总运营成本削减 10 - 20%。

Conclusion: TORTA 能有效提升分布式 GPU 推理基础设施的性能和效率。

Abstract: The rapid growth of large language model (LLM) services imposes increasing
demands on distributed GPU inference infrastructure. Most existing scheduling
systems rely on the current system state to make decisions, without considering
how task demand and resource availability evolve over time. This lack of
temporal awareness leads to inefficient GPU utilization, high task migration
overhead, and poor system responsiveness under dynamic workloads. In this work,
we identify the fundamental limitations of these instantaneous-state-only
scheduling approaches and propose Temporal Optimal Resource scheduling via
Two-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling
framework that captures both long-term workload patterns and short-term
execution constraints. It adopts a two-layer design: a macro-level scheduler
leverages reinforcement learning and optimal transport to coordinate
inter-region task distribution, while a micro-level allocator refines
task-to-server assignments within each region to reduce latency and switching
costs. Experimental results across multiple network topologies show that TORTA
reduces average inference response time by up to 15\%, improves load balance by
approximately 4-5\%, and cuts total operational cost by 10-20\% compared to
state-of-the-art baseline methods.

</details>


### [71] [Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters](https://arxiv.org/abs/2507.10392)
*Runsheng Benson Guo,Utkarsh Anand,Khuzaima Daudjee,Rathijit Sen*

Main category: cs.DC

TL;DR: 本文提出Zorse系统，统一多种能力并自动配置训练策略，在异构训练场景中显著优于现有系统。


<details>
  <summary>Details</summary>
Motivation: GPU计算资源有限且成本高，异构集群虽能提高计算能力但训练面临诸多挑战，需高效训练方法。

Method: 提出Zorse系统，集成通信和内存高效的流水线并行与数据并行，采用更灵活的并行配置，并加入自动配置训练策略的规划器。

Result: 评估显示Zorse在异构训练场景中显著优于现有系统。

Conclusion: Zorse系统能有效解决异构集群训练问题，实现高效训练。

Abstract: Large language models (LLMs) require vast amounts of GPU compute to train,
but limited availability and high costs of GPUs make homogeneous clusters
impractical for many organizations. Instead, assembling heterogeneous clusters
by pooling together GPUs of different generations allows them to achieve higher
aggregate compute and make use of all available GPUs. However, training on
heterogeneous clusters presents several challenges, including load balancing
across GPUs, optimizing memory usage to accommodate varying memory capacities,
and ensuring communication-efficient training over diverse network
interconnects potentially spanning multiple datacenters. In this paper, we make
the case that efficient training on heterogeneous clusters requires (1) the
integration of pipeline parallelism and data parallelism in a manner that is
both communication- and memory-efficient, and (2) a more adaptable
configuration of pipeline and data parallelism, which includes the capability
to flexibly partition GPUs into asymmetric pipeline parallel stages and to
incorporate heterogeneous GPUs within the same data parallelism group. We
propose Zorse, the first system to unify all these capabilities while
incorporating a planner that automatically configures training strategies for a
given workload. Our evaluation shows that Zorse significantly outperforms
state-of-the-art systems in heterogeneous training scenarios.

</details>


### [72] [Consensus, Inconsistency, Emergence: what's paraconsistency got to do with it?](https://arxiv.org/abs/2507.10413)
*Gabriel Rocha*

Main category: cs.DC

TL;DR: 本文探讨分布式系统共识问题，论证FLP不可能性定理在广义计算定义下仍成立，研究不一致性是否为涌现特征，还关注次协调逻辑和开发相关共识算法的可能性。


<details>
  <summary>Details</summary>
Motivation: 基于FLP不可能性定理在理论分布式计算中的影响，进一步研究其在广义计算定义下的情况，以及分布式系统共识中的不一致性等问题。

Method: 使用复杂系统的理论工具，研究系统的相转变，分析次协调逻辑。

Result: 论证了FLP不可能性定理在广义计算定义下成立，指出不一致性可能是分布式系统共识的涌现特征，次协调逻辑中平凡性可能是相关特征。

Conclusion: 提出对开发具备次协调推理能力的共识算法的关注。

Abstract: The consensus problem, briefly stated, consists of having processes in an
asynchronous distributed system agree on a value. It is widely known that the
consensus problem does not have a deterministic solution that ensures both
termination and consistency, if there is at least one faulty process in the
system. This result, known as the FLP impossibility theorem, led to several
generalizations and developments in theoretical distributed computing. This
paper argues that the FLP impossibility theorem holds even under a generalized
definition of computation through oracles. Furthermore, using a theoretical
machinery from complex systems, this paper also posits that inconsistency may
be an emergent feature of consensus over distributed systems by examining how a
system transitions phases. Under the same complex systems framework, this paper
examines paraconsistent logics, arguing that while inconsistency is not an
emergent feature for these logics, triviality may be. Lastly, some attention is
given to the possibility of developing consensus algorithms capable of
paraconsistent reasoning.

</details>


### [73] [Efficient Federated Learning with Heterogeneous Data and Adaptive Dropout](https://arxiv.org/abs/2507.10430)
*Ji Liu,Beichen Ma,Yang Zhou,Jingbo Zhou,Ruoming Jin,Dejing Dou,Huaiyu Dai,Haixun Wang,Patrick Valduriez*

Main category: cs.DC

TL;DR: 本文提出FedDHAD联邦学习框架，含动态异构模型聚合和自适应丢弃两种新方法，在准确率、效率和计算成本上优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临数据分布和异构性挑战，边缘设备数据非IID会导致准确率下降，且设备计算和通信能力有限会使模型收敛慢。

Method: 提出FedDHAD框架，包括动态调整本地模型权重的FedDH和对异构设备进行神经元自适应操作的FedAD。

Result: FedDHAD在准确率上最高提高6.7%，效率最高快2.02倍，计算成本最高降低15.0%。

Conclusion: FedDHAD框架在处理联邦学习的数据异构性问题上优于现有方案。

Abstract: Federated Learning (FL) is a promising distributed machine learning approach
that enables collaborative training of a global model using multiple edge
devices. The data distributed among the edge devices is highly heterogeneous.
Thus, FL faces the challenge of data distribution and heterogeneity, where
non-Independent and Identically Distributed (non-IID) data across edge devices
may yield in significant accuracy drop. Furthermore, the limited computation
and communication capabilities of edge devices increase the likelihood of
stragglers, thus leading to slow model convergence. In this paper, we propose
the FedDHAD FL framework, which comes with two novel methods: Dynamic
Heterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH
dynamically adjusts the weights of each local model within the model
aggregation process based on the non-IID degree of heterogeneous data to deal
with the statistical data heterogeneity. FedAD performs neuron-adaptive
operations in response to heterogeneous devices to improve accuracy while
achieving superb efficiency. The combination of these two methods makes FedDHAD
significantly outperform state-of-the-art solutions in terms of accuracy (up to
6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to
15.0% smaller).

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [74] [A Fixed Parameter Tractable Approach for Solving the Vertex Cover Problem in Polynomial Time Complexity](https://arxiv.org/abs/2507.09377)
*Mumuksh Tayal*

Main category: cs.DS

TL;DR: 本文实现并评估了以顶点覆盖大小k为参数的最小顶点覆盖问题的FPT算法，对比SageMath系统，该算法在大顶点数、小参数k实例上性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 最小顶点覆盖问题是经典NP完全问题，在大图上精确求解困难，FPT范式可利用输入参数解决此类问题。

Method: 采用基于选择相邻顶点的分支策略，递归求解约简图上的子问题，并在Python中实现算法。

Result: FPT实现对于顶点数多但参数k小的实例，性能有显著提升，符合理论FPT复杂度保证。

Conclusion: FPT算法在特定实例上表现良好，可通过优化分支因子进一步提升性能。

Abstract: The Minimum Vertex Cover problem, a classical NP-complete problem, presents
significant challenges for exact solution on large graphs. Fixed-Parameter
Tractability (FPT) offers a powerful paradigm to address such problems by
exploiting a parameter of the input, typically related to the size of the
desired solution. This paper presents an implementation and empirical
evaluation of an FPT algorithm for the Minimum Vertex Cover problem
parameterized by the size of the vertex cover, $k$. The algorithm utilizes a
branching strategy based on selecting adjacent vertices and recursively solving
subproblems on a reduced graph. We describe the algorithmic approach,
implementation details in Python, and present experimental results comparing
its performance against the SageMath computational system. The results
demonstrate that the FPT implementation achieves significant performance
improvements for instances with large numbers of vertices ($n$) but relatively
small values of the parameter ($k$), aligning with theoretical FPT complexity
guarantees. We also discuss potential optimizations that could further improve
the algorithm's performance, particularly concerning the branching factor.

</details>


### [75] [Simultaneous Network Design with Restricted Link Usage](https://arxiv.org/abs/2507.09426)
*Naonori Kakimura,Péter Madarasi,Jannik Matuschke,Kitti Varga*

Main category: cs.DS

TL;DR: 研究有向图中找最小成本弧子集问题，得出强硬度结果，识别多项式可解情况并给出FPT算法。


<details>
  <summary>Details</summary>
Motivation: 解决多商品网络设计中各商品只能使用自身颜色链路的问题。

Method: 研究问题的多种变体，分析限制情况。

Result: 得出强硬度结果，识别出颜色类成层状族、有向图无环且颜色类数量固定等多项式可解情况，给出一般情况下基于多色弧数量的FPT算法。

Conclusion: 此问题部分情况可多项式求解，一般情况可用FPT算法处理。

Abstract: Given a digraph with two terminal vertices $s$ and $t$ as well as a
conservative cost function and several not necessarily disjoint color classes
on its arc set, our goal is to find a minimum-cost subset of the arcs such that
its intersection with each color class contains an $s$-$t$ dipath. Problems of
this type arise naturally in multi-commodity network design settings where each
commodity is restricted to use links of its own color only.
  We study several variants of the problem, deriving strong hardness results
even for restricted cases, but we also identify cases that can be solved in
polynomial time. The latter ones include the cases where the color classes form
a laminar family, or where the underlying digraph is acyclic and the number of
color classes is constant. We also present an FPT algorithm for the general
case parameterized by the number of multi-colored arcs.

</details>


### [76] [Nearly Tight Sample Complexity for Matroid Online Contention Resolution](https://arxiv.org/abs/2507.09507)
*Moran Feldman,Ola Svensson,Rico Zenklusen*

Main category: cs.DS

TL;DR: 文章聚焦样本基先知不等式，先介绍前人成果，后提出近最优样本基OCRS用于拟阵约束，所需样本数接近下界，得到样本基拟阵先知不等式且竞争力优。


<details>
  <summary>Details</summary>
Motivation: 经典先知不等式假设随机变量分布知识全知不现实，需基于样本方法解决。

Method: 提出近最优样本基OCRS用于拟阵约束，利用其与先知不等式的联系得出结果。

Result: 提出的OCRS仅需 $O(\log \rho \cdot \log^2\log\rho)$ 个样本，接近已知下界；得到使用 $O(\log n + \log\rho \cdot \log^2\log\rho)$ 个样本的样本基拟阵先知不等式，竞争力达 $\frac{1}{4}-\varepsilon$。

Conclusion: 该样本基OCRS接近最优，能以较少样本获得良好竞争力的样本基拟阵先知不等式。

Abstract: Due to their numerous applications, in particular in Mechanism Design,
Prophet Inequalities have experienced a surge of interest. They describe
competitive ratios for basic stopping time problems where random variables get
revealed sequentially. A key drawback in the classical setting is the
assumption of full distributional knowledge of the involved random variables,
which is often unrealistic. A natural way to address this is via sample-based
approaches, where only a limited number of samples from the distribution of
each random variable is available. Recently, Fu, Lu, Gavin Tang, Wu, Wu, and
Zhang (2024) showed that sample-based Online Contention Resolution Schemes
(OCRS) are a powerful tool to obtain sample-based Prophet Inequalities. They
presented the first sample-based OCRS for matroid constraints, which is a
heavily studied constraint family in this context, as it captures many
interesting settings. This allowed them to get the first sample-based Matroid
Prophet Inequality, using $O(\log^4 n)$ many samples (per random variable),
where $n$ is the number of random variables, while obtaining a constant
competitiveness of $\frac{1}{4}-\varepsilon$.
  We present a nearly optimal sample-based OCRS for matroid constraints, which
uses only $O(\log \rho \cdot \log^2\log\rho)$ many samples, almost matching a
known lower bound of $\Omega(\log \rho)$, where $\rho \leq n$ is the rank of
the matroid. Through the above-mentioned connection to Prophet Inequalities,
this yields a sample-based Matroid Prophet Inequality using only $O(\log n +
\log\rho \cdot \log^2\log\rho)$ many samples, and matching the competitiveness
of $\frac{1}{4}-\varepsilon$, which is the best known competitiveness for the
considered almighty adversary setting even when the distributions are fully
known.

</details>


### [77] [Paths and Intersections: Exact Emulators for Planar Graphs](https://arxiv.org/abs/2507.09620)
*George Z. Li,Zihan Tan,Tianyi Zhang*

Main category: cs.DS

TL;DR: 研究平面图中保留距离的顶点稀疏化问题，构建特定情况下大小为$O(f^2k^2)$的精确平面模拟器。


<details>
  <summary>Details</summary>
Motivation: 在给定带权平面图和$k$个终端的情况下，构建能保留终端间成对距离的更小带权平面图（模拟器）。

Method: 采用将图视为路径及其交集的新图结构分析方法。

Result: 在终端位于输入图平面嵌入的$f$个面上时，构建出大小为$O(f^2k^2)$的精确平面模拟器，推广并插值了之前的结果。

Conclusion: 所采用的图结构分析方法有独立研究价值。

Abstract: We study vertex sparsification for preserving distances in planar graphs.
Given an edge-weighted planar graph with $k$ terminals, the goal is to
construct an emulator, which is a smaller edge-weighted planar graph that
contains the terminals and exactly preserves the pairwise distances between
them. We construct exact planar emulators of size $O(f^2k^2)$ in the setting
where terminals lie on $f$ faces in the planar embedding of the input graph.
Our result generalizes and interpolates between the previous results of Chang
and Ophelders and Goranci, Henzinger, and Peng which is an $O(k^2)$ bound in
the setting where all terminals lie on a single face (i.e., $f=1$), and the
result of Krauthgamer, Nguyen, and Zondiner, which is an $O(k^4)$ bound for the
general case (i.e., $f=k$).
  Our construction follows a recent new way of analyzing graph structures, by
viewing graphs as paths and their intersections, which we believe is of
independent interest.

</details>


### [78] [Minimum-Peak-Cost Flows Over Time](https://arxiv.org/abs/2507.09688)
*Mariia Anapolska,Emma Ahrens,Christina Büsing,Felix Engelhardt,Timo Gersing,Corinna Mathwieser,Sabrian Schmitz,Sophia Wrede*

Main category: cs.DS

TL;DR: 提出最小峰值成本随时间变化的流问题，聚焦最小化时间重复流的峰值成本，分析其近似比和复杂度，找出两个可在多项式时间求解的特殊情况。


<details>
  <summary>Details</summary>
Motivation: 在运输规划中，资源峰值需求比使用时长更受关注，需建模此类场景。

Method: 提出最小峰值成本随时间变化的流问题和最小峰值成本时间重复流问题，进行复杂度分析，找出特殊情况并给出求解算法。

Result: 时间重复流有近似比差的缺点，积分版MPC - TRF强NP难，找到两个可在多项式时间求解的特殊情况。

Conclusion: 明确了最小峰值成本时间重复流问题的特性，给出了特殊情况下的多项式时间求解算法。

Abstract: When planning transportation whose operation requires non-consumable
resources, the peak demand for allocated resources is often of higher interest
than the duration of resource usage. For instance, it is more cost-effective to
deliver parcels with a single truck over eight hours than to use two trucks for
four hours, as long as the time suffices. To model such scenarios, we introduce
the novel minimum peak cost flow over time problem, whose objective is to
minimise the maximum cost at all points in time rather than minimising the
integral of costs. We focus on minimising peak costs of temporally repeated
flows. These are desirable for practical applications due to their simple
structure. This yields the minimum-peak-cost Temporally Repeated flow problem
(MPC-TRF).
  We show that the simple structure of temporally repeated flows comes with the
drawback of arbitrarily bad approximation ratios compared to general flows over
time. Furthermore, our complexity analysis shows the integral version of
MPC-TRF is strongly NP-hard, even under strong restrictions. On the positive
side, we identify two benign special cases: unit-cost series-parallel networks
and networks with time horizon at least twice as long as the longest path in
the network (with respect to the transit time). In both cases, we show that
integral optimal flows if the desired flow value equals the maximum flow value
and fractional optimal flows for arbitrary flow values can be found in
polynomial time. For each of these cases, we provide an explicit algorithm that
constructs an optimal solution.

</details>


### [79] [Phase transition of the Sinkhorn-Knopp algorithm](https://arxiv.org/abs/2507.09711)
*Kun He*

Main category: cs.DS

TL;DR: 研究Sinkhorn - Knopp算法的迭代次数上下界，发现密度阈值γ = 1/2处有相变。


<details>
  <summary>Details</summary>
Motivation: 解释算法强实证性能原因并确定其迭代次数的紧界。

Method: 对归一化矩阵定义密度γ，分析不同密度矩阵下算法的迭代次数。

Result: 上界：密度γ > 1/2时，算法在O(log n - log ε)次迭代和O(n²)时间内产生近似双随机矩阵；下界：正矩阵在ℓ₂范数误差度量下有Ω(n^(1/2)/ε)次迭代的紧界，γ < 1/2时存在矩阵需Ω(n^(1/2)/ε)次迭代。

Conclusion: Sinkhorn - Knopp算法在密度阈值γ = 1/2处有尖锐相变。

Abstract: The matrix scaling problem, particularly the Sinkhorn-Knopp algorithm, has
been studied for over 60 years. In practice, the algorithm often yields
high-quality approximations within just a few iterations. Theoretically,
however, the best-known upper bound places it in the class of
pseudopolynomial-time approximation algorithms. Meanwhile, the lower-bound
landscape remains largely unexplored. Two fundamental questions persist: what
accounts for the algorithm's strong empirical performance, and can a tight
bound on its iteration count be established?
  For an $n\times n$ matrix, its normalized version is obtained by dividing
each entry by its largest entry. We say that a normalized matrix has a density
$\gamma$ if there exists a constant $\rho > 0$ such that one row or column has
exactly $\lceil \gamma n \rceil$ entries with values at least $\rho$, and every
other row and column has at least $\lceil \gamma n \rceil$ such entries.
  For the upper bound, we show that the Sinkhorn-Knopp algorithm produces a
nearly doubly stochastic matrix in $O(\log n - \log \varepsilon)$ iterations
and $\widetilde{O}(n^2)$ time for all nonnegative square matrices whose
normalized version has a density $\gamma > 1/2$. Such matrices cover both the
algorithm's principal practical inputs and its typical theoretical regime, and
the $\widetilde{O}(n^2)$ runtime is optimal.
  For the lower bound, we establish a tight bound of
$\widetilde{\Omega}\left(n^{1/2}/\varepsilon\right)$ iterations for positive
matrices under the $\ell_2$-norm error measure. Moreover, for every $\gamma <
1/2$, there exists a matrix with density $\gamma$ for which the algorithm
requires $\Omega\left(n^{1/2}/\varepsilon\right)$ iterations.
  In summary, our results reveal a sharp phase transition in the Sinkhorn-Knopp
algorithm at the density threshold $\gamma = 1/2$.

</details>


### [80] [Improved Directed Expander Decompositions](https://arxiv.org/abs/2507.09729)
*Henry Fleischmann,George Z. Li,Jason Li*

Main category: cs.DS

TL;DR: 本文为有向图获得更快的扩张器分解算法，匹配无向图算法保证，可无损推广到容量图，首次实现容量图近线性时间算法。


<details>
  <summary>Details</summary>
Motivation: 为有向图找到更快的扩张器分解算法，并将其推广到容量图。

Method: 首次实现并分析有向容量图的不停切配游戏，避免使用添加“假边”的方法。

Result: 得到有向图更快的扩张器分解算法，可无损推广到容量图，首次获得容量图近线性时间且对φ有最优依赖的有向扩张器分解算法。

Conclusion: 自然的无向图方法可应用于有向图，尽管分析有技术难度且需显著修改。

Abstract: We obtain faster expander decomposition algorithms for directed graphs,
matching the guarantees of Saranurak and Wang (SODA 2019) for expander
decomposition on undirected graphs. Our algorithms are faster than prior work
and also generalize almost losslessly to capacitated graphs. In particular, we
obtain the first directed expander decomposition algorithm for capacitated
graphs in near-linear time with optimal dependence on $\phi$.
  To obtain our result, we provide the first implementation and analysis of the
non-stop cut-matching game for directed, capacitated graphs. All existing
directed expander decomposition algorithms instead temporarily add ''fake
edges'' before pruning them away in a final cleanup step. Our result shows that
the natural undirected approach applies even to directed graphs. The difficulty
is in its analysis, which is technical and requires significant modifications
from the original setting of undirected graphs.

</details>


### [81] [Covering a Few Submodular Constraints and Applications](https://arxiv.org/abs/2507.09879)
*Tanvi Bajpai,Chandra Chekuri,Pooja Kulkarni*

Main category: cs.DS

TL;DR: 本文考虑r为固定常数时覆盖多个子模约束的问题，得到两个主要结果，表明固定r时近似效果接近r=1的情况。


<details>
  <summary>Details</summary>
Motivation: 受近期应用启发，研究r为固定常数时覆盖多个子模约束的问题。

Method: 设计随机双目标近似算法；针对加权覆盖函数，利用自然线性规划得出近似结果。

Result: 得到随机双目标近似算法，输出集合满足一定条件且期望成本有界；加权覆盖函数时得到特定近似比。

Conclusion: 固定r时能获得接近r=1的近似效果，还提及一些应用并期待更多。

Abstract: We consider the problem of covering multiple submodular constraints. Given a
finite ground set $N$, a cost function $c: N \rightarrow \mathbb{R}_+$, $r$
monotone submodular functions $f_1,f_2,\ldots,f_r$ over $N$ and requirements
$b_1,b_2,\ldots,b_r$ the goal is to find a minimum cost subset $S \subseteq N$
such that $f_i(S) \ge b_i$ for $1 \le i \le r$. When $r=1$ this is the
well-known Submodular Set Cover problem. Previous work
\cite{chekuri2022covering} considered the setting when $r$ is large and
developed bi-criteria approximation algorithms, and approximation algorithms
for the important special case when each $f_i$ is a weighted coverage function.
These are fairly general models and capture several concrete and interesting
problems as special cases. The approximation ratios for these problem are at
least $\Omega(\log r)$ which is unavoidable when $r$ is part of the input. In
this paper, motivated by some recent applications, we consider the problem when
$r$ is a \emph{fixed constant} and obtain two main results. For covering
multiple submodular constraints we obtain a randomized bi-criteria
approximation algorithm that for any given integer $\alpha \ge 1$ outputs a set
$S$ such that $f_i(S) \ge$ $(1-1/e^\alpha -\epsilon)b_i$ for each $i \in [r]$
and $\mathbb{E}[c(S)] \le (1+\epsilon)\alpha \cdot \sf{OPT}$. Second, when the
$f_i$ are weighted coverage functions from a deletion-closed set system we
obtain a $(1+\epsilon)$ $(\frac{e}{e-1})$ $(1+\beta)$-approximation where
$\beta$ is the approximation ratio for the underlying set cover instances via
the natural LP. These results show that one can obtain nearly as good an
approximation for any fixed $r$ as what one would achieve for $r=1$. We mention
some applications that follow easily from these general results and anticipate
more in the future.

</details>


### [82] [Improved bicriteria approximation for $k$-edge-connectivity](https://arxiv.org/abs/2507.10125)
*Zeev Nutov*

Main category: cs.DS

TL;DR: 本文改进了k - ECSS和k - ECSM问题的双准则近似算法。对于k - ECSS，改进了双准则近似；对于k - ECSM，提升了近似比率。


<details>
  <summary>Details</summary>
Motivation: 现有k - ECSS和k - ECSM问题的近似算法有改进空间，期望得到更好的近似结果。

Method: 未明确提及具体方法，可能基于前人算法改进。

Result: 对于k - ECSS，k为偶数时双准则近似改进到(1,k - 2)，k为奇数时到(1 - 1/k,k - 3)，还有(3/2,k - 1)；对于k - ECSM，k为偶数时近似比率改进到1 + 2/k，k为奇数时到1 + 3/k且k为奇数时子图是(k + 1)边连通的。

Conclusion: 本文算法在k - ECSS和k - ECSM问题上取得了比前人更好的近似结果。

Abstract: In the $k$-Edge Connected Spanning Subgraph ($k$-ECSS) problem we are given a
(multi-)graph $G=(V,E)$ with edge costs and an integer $k$, and seek a min-cost
$k$-edge-connected spanning subgraph of $G$. The problem admits a
$2$-approximation algorithm and no better approximation ratio is
known.Hershkowitz, Klein, and Zenklusen [STOC 24] gave a bicriteria
$(1,k-10)$-approximation algorithm that computes a $(k-10)$-edge-connected
spanning subgraph of cost at most the optimal value of a standard Cut-LP for
$k$-ECSS. This LP bicriteria approximation was recently improved by Cohen and
Nutov [ESA 25] to $(1,k-4)$, where also was given a bicriteria approximation
$(3/2,k-2)$. In this paper we improve the bicriteria approximation to $(1,k-2)$
for $k$ even and to $\left(1-\frac{1}{k},k-3\right)$ for $k$ is odd, and also
give another bicriteria approximation $(3/2,k-1)$.
  The $k$-Edge-Connected Spanning Multi-subgraph ($k$-ECSM) problem is almost
the same as $k$-ECSS, except that any edge can be selected multiple times at
the same cost. The previous best approximation ratio for $k$-ECSM was $1+4/k$.
Our result improves this to $1+\frac{2}{k}$ for $k$ even and to $1+\frac{3}{k}$
for $k$ odd, where for $k$ odd the computed subgraph is in fact
$(k+1)$-edge-connected.

</details>


### [83] [Bicriteria Submodular Maximization](https://arxiv.org/abs/2507.10248)
*Moran Feldman,Alan Kuhnle*

Main category: cs.DS

TL;DR: 本文研究子模函数的约束最大化问题，给出多类约束和子模函数的双准则近似算法结果，部分为最优，部分改进了现有水平。


<details>
  <summary>Details</summary>
Motivation: 子模函数优化应用广泛，双准则优化可涵盖更多重要场景，如子模覆盖问题和软约束下的优化。

Method: 对约束子模函数最大化问题进行双准则近似算法的原理性研究。

Result: 给出多种约束（基数、背包、拟阵和凸集）和多类子模函数（单调、对称和一般）的结果，部分为最优，部分改进现有水平。

Conclusion: 放松可行性约束对仅需可行解的问题也有帮助，能提供有用视角。

Abstract: Submodular functions and their optimization have found applications in
diverse settings ranging from machine learning and data mining to game theory
and economics. In this work, we consider the constrained maximization of a
submodular function, for which we conduct a principled study of bicriteria
approximation algorithms -- algorithms which can violate the constraint, but
only up to a bounded factor. Bicrteria optimization allows constrained
submodular maximization to capture additional important settings, such as the
well-studied submodular cover problem and optimization under soft constraints.
We provide results that span both multiple types of constraints (cardinality,
knapsack, matroid and convex set) and multiple classes of submodular functions
(monotone, symmetric and general). For many of the cases considered, we provide
optimal results. In other cases, our results improve over the state-of-the-art,
sometimes even over the state-of-the-art for the special case of
single-criterion (standard) optimization. Results of the last kind demonstrate
that relaxing the feasibility constraint may give a perspective about the
problem that is useful even if one only desires feasible solutions.

</details>


### [84] [Approximating Maximum Cut on Interval Graphs and Split Graphs beyond Goemans-Williamson](https://arxiv.org/abs/2507.10436)
*Jungho Ahn,Ian DeHaan,Eun Jung Kim,Euiwoong Lee*

Main category: cs.DS

TL;DR: 提出最大割问题在区间图和分裂图上的多项式时间(α_GW + ε)-近似算法，还给出在分裂图上近似难度结果。


<details>
  <summary>Details</summary>
Motivation: 为最大割问题在区间图和分裂图上找到更好的近似算法。

Method: 改进Goemans - Williamson算法分析，结合区间图和分裂图结构结果。

Result: 得到多项式时间(α_GW + ε)-近似算法，证明在分裂图上存在近似难度。

Conclusion: 在区间图和分裂图最大割问题近似算法上有进展，且存在近似难度界限。

Abstract: We present a polynomial-time $(\alpha_{GW} + \varepsilon)$-approximation
algorithm for the Maximum Cut problem on interval graphs and split graphs,
where $\alpha_{GW} \approx 0.878$ is the approximation guarantee of the
Goemans-Williamson algorithm and $\varepsilon > 10^{-34}$ is a fixed constant.
To attain this, we give an improved analysis of a slight modification of the
Goemans-Williamson algorithm for graphs in which triangles can be packed into a
constant fraction of their edges. We then pair this analysis with structural
results showing that both interval graphs and split graphs either have such a
triangle packing or have maximum cut close to their number of edges. We also
show that, subject to the Small Set Expansion Hypothesis, there exists a
constant $c > 0$ such that there is no polyomial-time $(1 - c)$-approximation
for Maximum Cut on split graphs.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [85] [Precomputed Dominant Resource Fairness](https://arxiv.org/abs/2507.08846)
*Serdar Metin*

Main category: cs.GT

TL;DR: 本文回顾主导资源公平性算法，提出新的预计算主导资源公平性算法以更少步骤近似其分配结果。


<details>
  <summary>Details</summary>
Motivation: 在分布式系统中，期望找到能以更少步骤近似主导资源公平性分配的算法。

Method: 重新审视原算法，分析其结构，基于主要工作原理设计新算法。

Result: 提出了预计算主导资源公平性算法。

Conclusion: 新算法能在更少步骤内近似主导资源公平性分配。

Abstract: Although resource allocation is a well studied problem in computer science,
until the prevalence of distributed systems, such as computing clouds and data
centres, the question had been addressed predominantly for single resource type
scenarios. At the beginning of the last decade, with the introuction of
Dominant Resource Fairness, the studies of the resource allocation problem has
finally extended to the multiple resource type scenarios. Dominant Resource
Fairness is a solution, addressing the problem of fair allocation of multiple
resource types, among users with heterogeneous demands. Based on Max-min
Fairness, which is a well established algorithm in the literature for
allocating resources in the single resource type scenarios, Dominant Resource
Fairness generalises the scheme to the multiple resource case. It has a number
of desirable properties that makes it preferable over alternatives, such as
Sharing Incentive, Envy-Freeness, Pareto Efficiency, and Strategy Proofness,
and as such, it is widely adopted in distributed systems. In the present study,
we revisit the original study, and analyse the structure of the algorithm in
closer view, to come up with an alternative algorithm, which approximates the
Dominant Resource Fairness allocation in fewer steps. We name the new algorithm
Precomputed Dominant Resource Fairness, after its main working principle.

</details>


### [86] [A Survey on Bilateral Multi-Round Cloud-SLA Negotiation Strategies](https://arxiv.org/abs/2507.08868)
*Benedikt Pittl,Werner Mach,Erich Schikuta*

Main category: cs.GT

TL;DR: 文章介绍云市场动态交易机制发展，对云资源多轮双边谈判策略进行调查并给出建议。


<details>
  <summary>Details</summary>
Motivation: 当前静态云市场为主，消费者缺乏议价等权利，科学界设想通过自主多轮谈判实现未来云市场，提高资源利用率和服务定制化。

Method: 分析同行评审文章，调查科学界描述此类策略的形式主义。

Result: 确定了云资源多轮双边谈判策略的趋势、差距、相似性和范围。

Conclusion: 得出创建和记录双边多轮谈判策略的建议，以促进其在行业中的实施。

Abstract: Today, static cloud markets where consumers purchase services directly from
providers are dominating. Thus, consumers neither negotiate the price nor the
characteristics of the service. In recent years, providers have adopted more
dynamic trading mechanisms, as e.g. Amazon's EC2 platform shows: In addition to
the reservation marketspace and the on-demand marketspace, Amazon offers a spot
marketspace where consumers can bid for virtual machines. This spot marketspace
was extended with spot blocks, and recently Amazon reworked the bidding
options. In addition, other cloud providers, such as Virtustream, adopt dynamic
trading mechanisms. The scientific community envisions autonomous multi-round
negotiations for realizing future cloud marketspaces. Consequently, consumers
and providers exchange offers and counteroffers to reach an agreement. This
helps providers increase the utilization of their datacenters, while consumers
can purchase highly customized cloud services.
  In the paper at hand, we present a survey on multi-round bilateral
negotiation strategies for trading cloud resources. Thus, we analyzed
peer-reviewed articles in order to identify trends, gaps, similarities, and the
scope of such negotiation strategies. In addition, we surveyed the formalism
that the scientific community uses to describe such strategies. Based on these
findings, we derived recommendations for creating and documenting bilateral
multi-round negotiation strategies to foster their implementation in the
industry.

</details>


### [87] [Learning from Synthetic Labs: Language Models as Auction Participants](https://arxiv.org/abs/2507.09083)
*Anand Shah,Kehang Zhu,Yanchen Jiang,Jeffrey G. Wang,Arif K. Dayi,John J. Horton,David C. Parkes*

Main category: cs.GT

TL;DR: 研究模拟AI代理（LLMs）在拍卖中的行为，引入新数据生成过程，发现LLMs在拍卖中表现及对提示的反应，成本低且开发灵活框架促进研究。


<details>
  <summary>Details</summary>
Motivation: 研究模拟AI代理（LLMs）在拍卖中的行为，以帮助促进拍卖的研究和设计。

Method: 引入新颖的合成数据生成过程，使用GPT - 4模型进行1000 +次拍卖。

Result: 有思维链推理能力的LLMs与实验文献结果相符，表现类似风险厌恶的人类投标人；对简单提示变化不敏感，用正确心理模型可接近理论预测；成本比现代拍卖实验低三个数量级。

Conclusion: 开发的框架可用于任何LLM模型和多种拍卖设计规范实验，降低成本，为使用LLM代理提供概念验证。

Abstract: This paper investigates the behavior of simulated AI agents (large language
models, or LLMs) in auctions, introducing a novel synthetic data-generating
process to help facilitate the study and design of auctions. We find that LLMs
-- when endowed with chain of thought reasoning capacity -- agree with the
experimental literature in auctions across a variety of classic auction
formats. In particular, we find that LLM bidders produce results consistent
with risk-averse human bidders; that they perform closer to theoretical
predictions in obviously strategy-proof auctions; and, that they succumb to the
winner's curse in common value settings. On prompting, we find that LLMs are
not very sensitive to naive changes in prompts (e.g., language, currency) but
can improve dramatically towards theoretical predictions with the right mental
model (i.e., the language of Nash deviations). We run 1,000$+$ auctions for
less than $\$$400 with GPT-4 models (three orders of magnitude cheaper than
modern auction experiments) and develop a framework flexible enough to run
auction experiments with any LLM model and a wide range of auction design
specifications, facilitating further experimental study by decreasing costs and
serving as a proof-of-concept for the use of LLM proxies.

</details>


### [88] [Nash Equilibria with Irradical Probabilities](https://arxiv.org/abs/2507.09422)
*Edan Orzech,Martin Rinard*

Main category: cs.GT

TL;DR: 为每个n≥4给出一个n玩家正规形式博弈，有唯一完全混合纳什均衡且概率权重为非根式代数数。


<details>
  <summary>Details</summary>
Motivation: 探索具有特定性质（唯一、完全混合且概率权重为非根式代数数）的n玩家正规形式博弈。

Method: 未提及

Result: 给出了n≥4时满足条件（有唯一完全混合纳什均衡且概率权重为非根式代数数）的n玩家正规形式博弈。

Conclusion: 存在满足特定条件的n玩家正规形式博弈。

Abstract: We present for every $n\ge4$ an $n$-player game in normal form with payoffs
in $\{0,1,2\}$ that has a unique, fully mixed, Nash equilibrium in which all
the probability weights are irradical (i.e., algebraic but not closed form
expressible even with $m$-th roots for any integer $m$).

</details>


### [89] [Incentive-Aware Dynamic Resource Allocation under Long-Term Cost Constraints](https://arxiv.org/abs/2507.09473)
*Yan Dai,Negin Golrezaei,Patrick Jaillet*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Motivated by applications such as cloud platforms allocating GPUs to users or
governments deploying mobile health units across competing regions, we study
the dynamic allocation of a reusable resource to strategic agents with private
valuations. Our objective is to simultaneously (i) maximize social welfare,
(ii) satisfy multi-dimensional long-term cost constraints, and (iii)
incentivize truthful reporting. We begin by numerically evaluating primal-dual
methods widely used in constrained online optimization and find them to be
highly fragile in strategic settings -- agents can easily manipulate their
reports to distort future dual updates for future gain.
  To address this vulnerability, we develop an incentive-aware framework that
makes primal-dual methods robust to strategic behavior. Our design combines
epoch-based lazy updates -- where dual variables remain fixed within each epoch
-- with randomized exploration rounds that extract approximately truthful
signals for learning. Leveraging carefully designed online learning subroutines
that can be of independent interest for dual updates, our mechanism achieves
$\tilde{\mathcal{O}}(\sqrt{T})$ social welfare regret, satisfies all cost
constraints, and ensures incentive alignment. This matches the performance of
non-strategic allocation approaches while being robust to strategic agents.

</details>


### [90] [Existence of Fair and Efficient Allocation of Indivisible Chores](https://arxiv.org/abs/2507.09544)
*Ryoga Mahara*

Main category: cs.GT

TL;DR: 本文研究不可分任务在具有加性成本函数的代理间公平高效分配问题，证明存在EF1且PO的分配，还给出其他相关结论并扩展到wEF1情况。


<details>
  <summary>Details</summary>
Motivation: 解决该领域关于是否总存在EF1且PO的分配这一主要开放问题。

Method: 结合不动点论证和离散算法。

Result: 证明存在EF1且PO的分配；证明存在EF1且fPO的分配；在代理数量固定时，可多项式时间计算EF1且PO的分配；将结果扩展到wEF1情况。

Conclusion: 为不可分任务在加性成本函数下的公平高效分配提供了理论结果和方法上的进展。

Abstract: We study the problem of allocating indivisible chores among agents with
additive cost functions in a fair and efficient manner. A major open question
in this area is whether there always exists an allocation that is envy-free up
to one chore (EF1) and Pareto optimal (PO). Our main contribution is to provide
a positive answer to this question by proving the existence of such an
allocation for indivisible chores under additive cost functions. This is
achieved by a novel combination of a fixed point argument and a discrete
algorithm, providing a significant methodological advance in this area.
  Our additional key contributions are as follows. We show that there always
exists an allocation that is EF1 and fractional Pareto optimal (fPO), where fPO
is a stronger efficiency concept than PO. We also show that an EF1 and PO
allocation can be computed in polynomial time when the number of agents is
constant. Finally, we extend all of these results to the more general setting
of weighted EF1 (wEF1), which accounts for the entitlements of agents.

</details>


### [91] [A Coincidence of Wants Mechanism for Swap Trade Execution in Decentralized Exchanges](https://arxiv.org/abs/2507.10149)
*Abhimanyu Nag,Madhur Prabhakar,Tanuj Behl*

Main category: cs.GT

TL;DR: 提出数学严格框架识别和完成去中心化交易聚合器中的需求巧合（CoW）周期，应用于真实数据效果良好。


<details>
  <summary>Details</summary>
Motivation: 改进现有基于拍卖的系统，如CoWSwap，实现更高效的CoW周期识别和完成。

Method: 引入资产矩阵公式，用预言机价格和守恒定律验证可行性，通过图遍历发现部分CoW周期并使用不平衡校正结算，定义桥接订单。

Result: 算法能有效发现CoW周期，支持插入合成订单进行原子周期闭合。

Conclusion: 该工作可作为流动性提供做市商的潜在delta - 中性策略，即结构化CoW周期执行。

Abstract: We propose a mathematically rigorous framework for identifying and completing
Coincidence of Wants (CoW) cycles in decentralized exchange (DEX) aggregators.
Unlike existing auction based systems such as CoWSwap, our approach introduces
an asset matrix formulation that not only verifies feasibility using oracle
prices and formal conservation laws but also completes partial CoW cycles of
swap orders that are discovered using graph traversal and are settled using
imbalance correction. We define bridging orders and show that the resulting
execution is slippage free and capital preserving for LPs. Applied to real
world Arbitrum swap data, our algorithm demonstrates efficient discovery of CoW
cycles and supports the insertion of synthetic orders for atomic cycle closure.
This work can be thought of as the detailing of a potential delta-neutral
strategy by liquidity providing market makers: a structured CoW cycle
execution.

</details>


### [92] [Tie-breaking Agnostic Lower Bound for Fictitious Play](https://arxiv.org/abs/2507.09902)
*Yuanhao Wang*

Main category: cs.GT

TL;DR: 本文反驳了Karlin关于虚构博弈（FP）收敛速率的较弱形式猜想，指出存在一个10×10的零和矩阵博弈，FP收敛速率为Ω(t⁻¹/³)。


<details>
  <summary>Details</summary>
Motivation: Karlin在1959年猜想FP以O(t⁻¹/²)的速率收敛到纳什均衡，2014年Daskalakis和Pan反驳了其较强形式猜想，本文旨在反驳较弱形式猜想。

Method: 构造了一个10×10的零和矩阵博弈进行分析。

Result: 存在一个10×10零和矩阵博弈，其中FP收敛速率为Ω(t⁻¹/³)，且除第一步外无平局。

Conclusion: Karlin关于FP收敛速率的较弱形式猜想不成立。

Abstract: Fictitious play (FP) is a natural learning dynamic in two-player zero-sum
games. Samuel Karlin conjectured in 1959 that FP converges at a rate of
$O(t^{-1/2})$ to Nash equilibrium, where $t$ is the number of steps played.
However, Daskalakis and Pan disproved the stronger form of this conjecture in
2014, where \emph{adversarial} tie-breaking is allowed.
  This paper disproves Karlin's conjecture in its weaker form. In particular,
there exists a 10-by-10 zero-sum matrix game, in which FP converges at a rate
of $\Omega(t^{-1/3})$, and no ties occur except for the first step.

</details>


### [93] [Generalized Quantal Response Equilibrium: Existence and Efficient Learning](https://arxiv.org/abs/2507.09928)
*Apurv Shukla,Vijay Subramanian,Andy Zhao,Rahul Jain*

Main category: cs.GT

TL;DR: 提出广义量子响应均衡（GQRE）概念，给出高效无遗憾独立学习算法，分析收敛性并在复杂博弈中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 为有限正则形式一般和博弈中的有限理性参与者引入新的解概念。

Method: 通过Frank - Wolfe算法的平滑版本进行无遗憾独立学习，利用模拟预言机生成噪声但相关的梯度估计。

Result: 在确保均衡唯一性的假设下分析了算法的收敛性。

Conclusion: 所提出的方法在复杂的一般和博弈中有效。

Abstract: We introduce a new solution concept for bounded rational agents in finite
normal-form general-sum games called Generalized Quantal Response Equilibrium
(GQRE) which generalizes Quantal Response
Equilibrium~\citep{mckelvey1995quantal}. In our setup, each player maximizes a
smooth, regularized expected utility of the mixed profiles used, reflecting
bounded rationality that subsumes stochastic choice. After establishing
existence under mild conditions, we present computationally efficient no-regret
independent learning via smoothened versions of the Frank-Wolfe algorithm. Our
algorithm uses noisy but correlated gradient estimates generated via a
simulation oracle that reports on repeated plays of the game. We analyze
convergence properties of our algorithm under assumptions that ensure
uniqueness of equilibrium, using a class of gap functions that generalize the
Nash gap. We end by demonstrating the effectiveness of our method on a set of
complex general-sum games such as high-rank two-player games, large action
two-player games, and known examples of difficult multi-player games.

</details>


### [94] [A New Incentive Model For Content Trust](https://arxiv.org/abs/2507.09972)
*Lucas Barbosa,Sam Kirshner,Rob Kopel,Eric Tze Kuan Lim,Tom Pagram*

Main category: cs.GT

TL;DR: 提出激励驱动和去中心化方法大规模验证数字内容真实性，需进一步分析探索。


<details>
  <summary>Details</summary>
Motivation: 应对广泛的错误信息、AI生成内容激增和对传统新闻源依赖降低的现状，寻求适合现代数字世界的内容真实性验证新方法。

Method: 利用智能合约和数字身份将‘信任’纳入发布内容的奖励函数，让内容创作者为事实声明抵押金融担保，由公正陪审团审核并给予贡献者经济奖励。

Result: 无实际研究结果，仅为假设，认为合适的金融和社会激励模型会促使人们参与众包事实核查，创作者更谨慎声明。

Conclusion: 这是探索性论文，有许多问题需进一步分析和探索。

Abstract: This paper outlines an incentive-driven and decentralized approach to
verifying the veracity of digital content at scale. Widespread misinformation,
an explosion in AI-generated content and reduced reliance on traditional news
sources demands a new approach for content authenticity and truth-seeking that
is fit for a modern, digital world. By using smart contracts and digital
identity to incorporate 'trust' into the reward function for published content,
not just engagement, we believe that it could be possible to foster a
self-propelling paradigm shift to combat misinformation through a
community-based governance model. The approach described in this paper requires
that content creators stake financial collateral on factual claims for an
impartial jury to vet with a financial reward for contribution. We hypothesize
that with the right financial and social incentive model users will be
motivated to participate in crowdsourced fact-checking and content creators
will place more care in their attestations. This is an exploratory paper and
there are a number of open issues and questions that warrant further analysis
and exploration.

</details>


### [95] [The Value Problem for Weighted Timed Games with Two Clocks is Undecidable](https://arxiv.org/abs/2507.10550)
*Quentin Guilmant,Joël Ouaknine,Isa Vialard*

Main category: cs.GT

TL;DR: 论文证明了带非负权重的双时钟加权定时博弈（WTGs）价值问题不可判定，填补算法理解空白。


<details>
  <summary>Details</summary>
Motivation: 此前已知单时钟WTGs价值问题可判定，三时钟及以上不可判定，双时钟情况未知，需填补这一算法理解的空白。

Method: 未提及。

Result: 证明了带非负权重的双时钟WTGs在有时间限制的情况下价值问题不可判定。

Conclusion: 解决了WTGs算法理解中最后一个主要空白。

Abstract: The Value Problem for weighted timed games (WTGs) consists in determining,
given a two-player weighted timed game with a reachability objective and a
rational threshold, whether or not the value of the game exceeds the threshold.
This problem was shown to be undecidable some ten years ago for WTGs making use
of at least three clocks, and is known to be decidable for single-clock WTGs.
In this paper, we establish undecidability for two-clock WTGs making use of
non-negative weights, even in a time-bounded setting, closing the last
remaining major gap in our algorithmic understanding of WTGs.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [96] [Overview of the TREC 2023 deep learning track](https://arxiv.org/abs/2507.08890)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Hossein A. Rahmani,Daniel Campos,Jimmy Lin,Ellen M. Voorhees,Ian Soboroff*

Main category: cs.IR

TL;DR: 本文介绍TREC深度学习赛道第五年情况，复用去年设计，用新数据集，今年用合成查询，大语言模型提示法表现优于之前的nnlm方法，合成查询评估结果与人类查询相似，未发现明显偏差。


<details>
  <summary>Details</summary>
Motivation: 在TREC深度学习赛道持续开展相关研究，利用MS MARCO数据集进行段落和文档排名任务，获取更具挑战性的测试集以推动技术改进。

Method: 复用去年设计，基于新的段落和文档集，从MS MARCO未使用过的查询中采样，生成合成查询，采用大语言模型提示法和nnlm方法等。

Result: 使用大语言模型提示法的运行结果优于nnlm方法，合成查询评估与人类查询结果相似，系统排序一致性为τ = 0.8487，但需人工筛选可用的合成查询，未发现明显偏差。

Conclusion: 本次研究为该赛道最后一年，基于提示的排名后续可在其他赛道开展，合成查询可用于评估且效果良好。

Abstract: This is the fifth year of the TREC Deep Learning track. As in previous years,
we leverage the MS MARCO datasets that made hundreds of thousands of
human-annotated training labels available for both passage and document ranking
tasks. We mostly repeated last year's design, to get another matching test set,
based on the larger, cleaner, less-biased v2 passage and document set, with
passage ranking as primary and document ranking as a secondary task (using
labels inferred from passage). As we did last year, we sample from MS MARCO
queries that were completely held out, unused in corpus construction, unlike
the test queries in the first three years. This approach yields a more
difficult test with more headroom for improvement. Alongside the usual MS MARCO
(human) queries from MS MARCO, this year we generated synthetic queries using a
fine-tuned T5 model and using a GPT-4 prompt.
  The new headline result this year is that runs using Large Language Model
(LLM) prompting in some way outperformed runs that use the "nnlm" approach,
which was the best approach in the previous four years. Since this is the last
year of the track, future iterations of prompt-based ranking can happen in
other tracks. Human relevance assessments were applied to all query types, not
just human MS MARCO queries. Evaluation using synthetic queries gave similar
results to human queries, with system ordering agreement of $\tau=0.8487$.
However, human effort was needed to select a subset of the synthetic queries
that were usable. We did not see clear evidence of bias, where runs using GPT-4
were favored when evaluated using synthetic GPT-4 queries, or where runs using
T5 were favored when evaluated on synthetic T5 queries.

</details>


### [97] [GraphRunner: A Multi-Stage Framework for Efficient and Accurate Graph-Based Retrieval](https://arxiv.org/abs/2507.08945)
*Savini Kashmira,Jayanaka L. Dantanarayana,Krisztián Flautner,Lingjia Tang,Jason Mars*

Main category: cs.IR

TL;DR: 提出GraphRunner框架解决传统RAG在知识图谱检索问题，评估显示其优于现有方法，性能提升且成本降低。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在知识图谱检索中存在困难，现有迭代方法易受LLM推理错误和幻觉影响，阻碍相关信息检索。

Method: 提出GraphRunner框架，分规划、验证和执行三阶段，引入高级遍历动作实现单步多跳探索，生成整体遍历计划并验证。

Result: 使用GRBench数据集评估，GraphRunner性能比最强基线提升10 - 50%，推理成本降低3.0 - 12.9倍，响应生成时间减少2.5 - 7.1倍。

Conclusion: GraphRunner对基于图的检索任务更健壮和高效。

Abstract: Conventional Retrieval Augmented Generation (RAG) approaches are common in
text-based applications. However, they struggle with structured, interconnected
datasets like knowledge graphs, where understanding underlying relationships is
crucial for accurate retrieval. A common direction in graph-based retrieval
employs iterative, rule-based traversal guided by Large Language Models (LLMs).
Such existing iterative methods typically combine reasoning with single hop
traversal at each step, making them vulnerable to LLM reasoning errors and
hallucinations that ultimately hinder the retrieval of relevant information.
  To address these limitations, we propose GraphRunner, a novel graph-based
retrieval framework that operates in three distinct stages: planning,
verification, and execution. This introduces high-level traversal actions that
enable multi-hop exploration in a single step. It also generates a holistic
traversal plan, which is verified against the graph structure and pre-defined
traversal actions, reducing reasoning errors and detecting hallucinations
before execution. GraphRunner significantly reduces LLM reasoning errors and
detects hallucinations through validation. Our evaluation using the GRBench
dataset shows that GraphRunner consistently outperforms existing approaches,
achieving 10-50% performance improvements over the strongest baseline while
reducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x,
making it significantly more robust and efficient for graph-based retrieval
tasks.

</details>


### [98] [DS@GT at Touché: Large Language Models for Retrieval-Augmented Debate](https://arxiv.org/abs/2507.09090)
*Anthony Miyaguchi,Conor Johnston,Aaryan Potdar*

Main category: cs.IR

TL;DR: 研究大语言模型在辩论场景中的表现，部署六个模型，发现其在有相关论据时辩论表现好，但回应冗长、评估较一致。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在辩论场景中的表现，包括结构化辩论能力和辩论中话语评估能力。

Method: 部署来自三个供应商的六个公开可用模型进行检索增强辩论和评估，通过测量质量、数量、方式和关系四个关键指标进行评估。

Result: 大语言模型在有相关论据时辩论表现良好，回应冗长，评估较一致。

Conclusion: 文中未明确提及总结性结论，但展示了大语言模型在辩论方面的特点和表现。

Abstract: Large Language Models (LLMs) demonstrate strong conversational abilities. In
this Working Paper, we study them in the context of debating in two ways: their
ability to perform in a structured debate along with a dataset of arguments to
use and their ability to evaluate utterances throughout the debate. We deploy
six leading publicly available models from three providers for the
Retrieval-Augmented Debate and Evaluation. The evaluation is performed by
measuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout
this task, we found that although LLMs perform well in debates when given
related arguments, they tend to be verbose in responses yet consistent in
evaluation. The accompanying source code for this paper is located at
https://github.com/dsgt-arc/touche-2025-rad.

</details>


### [99] [Retrieval-Augmented Recommendation Explanation Generation with Hierarchical Aggregation](https://arxiv.org/abs/2507.09188)
*Bangcheng Sun,Yazhe Chen,Jilin Yang,Xiaodong Li,Hui Li*

Main category: cs.IR

TL;DR: 提出REXHA解决现有LLM - 基ExRec模型问题，实验显示性能优


<details>
  <summary>Details</summary>
Motivation: 现有LLM - 基ExRec模型存在轮廓偏差和高检索开销问题，阻碍部署

Method: 设计基于分层聚合的分析模块构建整体轮廓，引入高效检索模块用两种伪文档查询检索相关评论

Result: 方法在解释质量上比现有方法最高优12.6%，且检索效率高

Conclusion: 所提REXHA方法有效，能提升推荐解释生成质量和检索效率

Abstract: Explainable Recommender System (ExRec) provides transparency to the
recommendation process, increasing users' trust and boosting the operation of
online services. With the rise of large language models (LLMs), whose extensive
world knowledge and nuanced language understanding enable the generation of
human-like, contextually grounded explanations, LLM-powered ExRec has gained
great momentum. However, existing LLM-based ExRec models suffer from profile
deviation and high retrieval overhead, hindering their deployment. To address
these issues, we propose Retrieval-Augmented Recommendation Explanation
Generation with Hierarchical Aggregation (REXHA). Specifically, we design a
hierarchical aggregation based profiling module that comprehensively considers
user and item review information, hierarchically summarizing and constructing
holistic profiles. Furthermore, we introduce an efficient retrieval module
using two types of pseudo-document queries to retrieve relevant reviews to
enhance the generation of recommendation explanations, effectively reducing
retrieval latency and improving the recall of relevant reviews. Extensive
experiments demonstrate that our method outperforms existing approaches by up
to 12.6% w.r.t. the explanation quality while achieving high retrieval
efficiency.

</details>


### [100] [Correcting the LogQ Correction: Revisiting Sampled Softmax for Large-Scale Retrieval](https://arxiv.org/abs/2507.09331)
*Kirill Khrylchenko,Vladimir Baikalov,Sergei Makeev,Artem Matveev,Sergei Liamaev*

Main category: cs.IR

TL;DR: 论文指出双塔神经网络检索阶段常用的批内负采样有偏差，logQ校正未完全消除偏差，提出改进校正公式，实验显示效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有双塔神经网络在网络规模大时使用批内负采样有偏差，logQ校正无法完全消除偏差，需改进。

Method: 重新推导logQ校正，提出考虑正样本概率为1的改进校正公式，引入反映模型不确定性的样本权重。

Result: 在公共和专有数据集上评估，相比标准logQ校正有持续改进。

Conclusion: 提出的改进校正公式能有效提升模型效果，优于标准logQ校正。

Abstract: Two-tower neural networks are a popular architecture for the retrieval stage
in recommender systems. These models are typically trained with a softmax loss
over the item catalog. However, in web-scale settings, the item catalog is
often prohibitively large, making full softmax infeasible. A common solution is
sampled softmax, which approximates the full softmax using a small number of
sampled negatives.
  One practical and widely adopted approach is to use in-batch negatives, where
negatives are drawn from items in the current mini-batch. However, this
introduces a bias: items that appear more frequently in the batch (i.e.,
popular items) are penalized more heavily.
  To mitigate this issue, a popular industry technique known as logQ correction
adjusts the logits during training by subtracting the log-probability of an
item appearing in the batch. This correction is derived by analyzing the bias
in the gradient and applying importance sampling, effectively twice, using the
in-batch distribution as a proposal distribution. While this approach improves
model quality, it does not fully eliminate the bias.
  In this work, we revisit the derivation of logQ correction and show that it
overlooks a subtle but important detail: the positive item in the denominator
is not Monte Carlo-sampled - it is always present with probability 1. We
propose a refined correction formula that accounts for this. Notably, our loss
introduces an interpretable sample weight that reflects the model's uncertainty
- the probability of misclassification under the current parameters. We
evaluate our method on both public and proprietary datasets, demonstrating
consistent improvements over the standard logQ correction.

</details>


### [101] [Balancing Semantic Relevance and Engagement in Related Video Recommendations](https://arxiv.org/abs/2507.09403)
*Amit Jaspal,Feng Zhang,Wei Chang,Sumit Kumar,Yubo Wang,Roni Mittleman,Qifan Wang,Weize Mao*

Main category: cs.IR

TL;DR: 提出新颖多目标检索框架优化视频推荐，结合多任务学习、多模态特征融合和离策略校正，在工业数据和A/B测试中效果显著。


<details>
  <summary>Details</summary>
Motivation: 解决传统协同过滤驱动的相关视频推荐缺乏语义连贯性和存在强烈流行度偏差的问题。

Method: 增强标准双塔模型，结合多任务学习联合优化共同参与度和语义相关性、融合多模态内容特征、通过逆倾向加权进行离策略校正。

Result: 语义相关性显著提升（主题匹配率从51%提升到63%），热门项目分布减少（热门视频推荐减少13.8%），用户参与度指标提升0.04%。

Conclusion: 方法成功实现了更好的语义连贯性、平衡的参与度以及在现实世界部署的实际可扩展性。

Abstract: Related video recommendations commonly use collaborative filtering (CF)
driven by co-engagement signals, often resulting in recommendations lacking
semantic coherence and exhibiting strong popularity bias. This paper introduces
a novel multi-objective retrieval framework, enhancing standard two-tower
models to explicitly balance semantic relevance and user engagement. Our
approach uniquely combines: (a) multi-task learning (MTL) to jointly optimize
co-engagement and semantic relevance, explicitly prioritizing topical
coherence; (b) fusion of multimodal content features (textual and visual
embeddings) for richer semantic understanding; and (c) off-policy correction
(OPC) via inverse propensity weighting to effectively mitigate popularity bias.
Evaluation on industrial-scale data and a two-week live A/B test reveals our
framework's efficacy. We observed significant improvements in semantic
relevance (from 51% to 63% topic match rate), a reduction in popular item
distribution (-13.8% popular video recommendations), and a +0.04% improvement
in our topline user engagement metric. Our method successfully achieves better
semantic coherence, balanced engagement, and practical scalability for
real-world deployment.

</details>


### [102] [Item-centric Exploration for Cold Start Problem](https://arxiv.org/abs/2507.09423)
*Dong Wang,Junyi Jiao,Arnab Bhadury,Yaping Zhang,Mingyan Gao,Onkar Dalal*

Main category: cs.IR

TL;DR: 论文指出推荐系统用户中心模式在冷启动问题中的不足，提出以物品为中心的推荐概念，实现物品中心控制，实证显示能提升冷启动效果。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统用户中心模式在物品冷启动问题中难以找到新内容的理想受众，限制内容多样性和加剧流行度偏差。

Method: 引入物品中心推荐概念，将物品中心控制集成到探索系统，用贝叶斯模型和Beta分布评估候选物品。

Result: 该控制显著提高冷启动目标定位效果，提升用户对新探索内容的满意度，大幅提高整体探索效率。

Conclusion: 物品中心的推荐方式能有效解决推荐系统的物品冷启动问题。

Abstract: Recommender systems face a critical challenge in the item cold-start problem,
which limits content diversity and exacerbates popularity bias by struggling to
recommend new items. While existing solutions often rely on auxiliary data, but
this paper illuminates a distinct, yet equally pressing, issue stemming from
the inherent user-centricity of many recommender systems. We argue that in
environments with large and rapidly expanding item inventories, the traditional
focus on finding the "best item for a user" can inadvertently obscure the ideal
audience for nascent content. To counter this, we introduce the concept of
item-centric recommendations, shifting the paradigm to identify the optimal
users for new items. Our initial realization of this vision involves an
item-centric control integrated into an exploration system. This control
employs a Bayesian model with Beta distributions to assess candidate items
based on a predicted balance between user satisfaction and the item's inherent
quality. Empirical online evaluations reveal that this straightforward control
markedly improves cold-start targeting efficacy, enhances user satisfaction
with newly explored content, and significantly increases overall exploration
efficiency.

</details>


### [103] [Does UMBRELA Work on Other LLMs?](https://arxiv.org/abs/2507.09483)
*Naghmeh Farzi,Laura Dietz*

Main category: cs.IR

TL;DR: 重现UMBRELA LLM Judge评估框架以评估其泛化性，研究LLM选择对相关性评估准确性的影响，发现DeepSeek V3与GPT - 4o性能相当，LLaMA - 3.3 - 70B稍低，小模型更差。


<details>
  <summary>Details</summary>
Motivation: 评估UMBRELA LLM Judge评估框架在原研究之外的泛化性，研究LLM选择对相关性评估准确性的影响。

Method: 在一系列大语言模型中重现UMBRELA LLM Judge评估框架，通过排行榜排名相关性和每个标签的一致性指标进行评估。

Result: UMBRELA搭配DeepSeek V3与GPT - 4o性能非常接近；LLaMA - 3.3 - 70B性能稍低；较小的LLM性能更差。

Conclusion: 不同的大语言模型搭配UMBRELA评估框架性能有差异。

Abstract: We reproduce the UMBRELA LLM Judge evaluation framework across a range of
large language models (LLMs) to assess its generalizability beyond the original
study. Our investigation evaluates how LLM choice affects relevance assessment
accuracy, focusing on leaderboard rank correlation and per-label agreement
metrics. Results demonstrate that UMBRELA with DeepSeek V3 obtains very
comparable performance to GPT-4o (used in original work). For LLaMA-3.3-70B we
obtain slightly lower performance, which further degrades with smaller LLMs.

</details>


### [104] [Criteria-Based LLM Relevance Judgments](https://arxiv.org/abs/2507.09488)
*Naghmeh Farzi,Laura Dietz*

Main category: cs.IR

TL;DR: 传统人工标注相关性标签耗时且昂贵，本文提出多标准框架改进基于大语言模型的相关性判断，在三个数据集验证，能提升系统排名性能并为未来评估框架发展提供见解。


<details>
  <summary>Details</summary>
Motivation: 传统人工标注相关性标签耗时昂贵，现有大语言模型直接提示生成标签有缺陷，需改进。

Method: 提出多标准框架，将相关性概念分解为多个标准。

Result: 在三个数据集验证，多标准判断能提升系统排名/排行榜性能。

Conclusion: 多标准判断方法有优势，指出其与直接评分方法相比的优缺点，可为未来信息检索自动评估框架发展提供指导。

Abstract: Relevance judgments are crucial for evaluating information retrieval systems,
but traditional human-annotated labels are time-consuming and expensive. As a
result, many researchers turn to automatic alternatives to accelerate method
development. Among these, Large Language Models (LLMs) provide a scalable
solution by generating relevance labels directly through prompting. However,
prompting an LLM for a relevance label without constraints often results in not
only incorrect predictions but also outputs that are difficult for humans to
interpret. We propose the Multi-Criteria framework for LLM-based relevance
judgments, decomposing the notion of relevance into multiple criteria--such as
exactness, coverage, topicality, and contextual fit--to improve the robustness
and interpretability of retrieval evaluations compared to direct grading
methods. We validate this approach on three datasets: the TREC Deep Learning
tracks from 2019 and 2020, as well as LLMJudge (based on TREC DL 2023). Our
results demonstrate that Multi-Criteria judgments enhance the system
ranking/leaderboard performance. Moreover, we highlight the strengths and
limitations of this approach relative to direct grading approaches, offering
insights that can guide the development of future automatic evaluation
frameworks in information retrieval.

</details>


### [105] [Identifying Offline Metrics that Predict Online Impact: A Pragmatic Strategy for Real-World Recommender Systems](https://arxiv.org/abs/2507.09566)
*Timo Wilm,Philipp Normann*

Main category: cs.IR

TL;DR: 引入实用策略识别与在线影响一致的离线指标，在OTTO平台验证，为从业者提供理解离线到在线指标关系的工具。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中建立离线和在线指标可靠关系以预测现实性能的挑战。

Method: 基于帕累托前沿近似，提出实用策略，能服务多测试组，且对有神经网络骨干的系统模型无关。

Result: 在OTTO平台的会话推荐系统大规模在线实验中，发现离线指标与现实点击率、点击后转化率和销售量有显著关联。

Conclusion: 该策略为行业从业者提供理解离线到在线指标关系、做出数据驱动决策的有价值工具。

Abstract: A critical challenge in recommender systems is to establish reliable
relationships between offline and online metrics that predict real-world
performance. Motivated by recent advances in Pareto front approximation, we
introduce a pragmatic strategy for identifying offline metrics that align with
online impact. A key advantage of this approach is its ability to
simultaneously serve multiple test groups, each with distinct offline
performance metrics, in an online experiment controlled by a single model. The
method is model-agnostic for systems with a neural network backbone, enabling
broad applicability across architectures and domains. We validate the strategy
through a large-scale online experiment in the field of session-based
recommender systems on the OTTO e-commerce platform. The online experiment
identifies significant alignments between offline metrics and real-word
click-through rate, post-click conversion rate and units sold. Our strategy
provides industry practitioners with a valuable tool for understanding
offline-to-online metric relationships and making informed, data-driven
decisions.

</details>


### [106] [MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora](https://arxiv.org/abs/2507.09924)
*Tuan-Luc Huynh,Thuy-Trang Vu,Weiqing Wang,Trung Le,Dragan Gašević,Yuan-Fang Li,Thanh-Toan Do*

Main category: cs.IR

TL;DR: 提出MixLoRA - DSI框架，结合低秩自适应专家混合与OOD驱动扩展策略，在实验中表现优于全模型更新基线，参数开销小、训练成本低。


<details>
  <summary>Details</summary>
Motivation: 在生成式检索中持续用新文档更新基于模型的索引具有挑战性，全量重新训练计算成本高且受资源限制不实际。

Method: 提出MixLoRA - DSI框架，结合可扩展的低秩自适应专家混合和基于层的OOD驱动扩展策略，仅在检测到大量OOD文档时才引入新专家。

Result: 在NQ320k和MS MARCO Passage实验中，MixLoRA - DSI优于全模型更新基线，参数开销极小，训练成本大幅降低。

Conclusion: MixLoRA - DSI框架是一种有效的更新生成式检索中基于模型索引的方法。

Abstract: Continually updating model-based indexes in generative retrieval with new
documents remains challenging, as full retraining is computationally expensive
and impractical under resource constraints. We propose MixLoRA-DSI, a novel
framework that combines an expandable mixture of Low-Rank Adaptation experts
with a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead
of allocating new experts for each new corpus, our proposed expansion strategy
enables sublinear parameter growth by selectively introducing new experts only
when significant number of OOD documents are detected. Experiments on NQ320k
and MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update
baselines, with minimal parameter overhead and substantially lower training
costs.

</details>


### [107] [Non-parametric Graph Convolution for Re-ranking in Recommendation Systems](https://arxiv.org/abs/2507.09969)
*Zhongyu Ouyang,Mingxuan Ju,Soroush Vosoughi,Yanfang Ye*

Main category: cs.IR

TL;DR: 论文提出非参数策略，利用图卷积在测试时重排序，减少计算开销并提升推荐排名质量。


<details>
  <summary>Details</summary>
Motivation: 图知识在推荐系统检索阶段有效，但在排序阶段应用待探索，且图计算开销大难以扩展，需解决该问题。

Method: 提出非参数策略，仅在测试时用图卷积重排序，规避训练时计算开销。

Result: 在四个不同稀疏度基准数据集实验表明，测试时平均提升8.1%，平均额外计算开销仅0.5。

Conclusion: 该策略可作为即插即用模块，显著减少计算开销，提升真实推荐系统排名能力。

Abstract: Graph knowledge has been proven effective in enhancing item rankings in
recommender systems (RecSys), particularly during the retrieval stage. However,
its application in the ranking stage, especially when richer contextual
information in user-item interactions is available, remains underexplored. A
major challenge lies in the substantial computational cost associated with
repeatedly retrieving neighborhood information from billions of items stored in
distributed systems. This resource-intensive requirement makes it difficult to
scale graph-based methods in practical RecSys. To bridge this gap, we first
demonstrate that incorporating graphs in the ranking stage improves ranking
qualities. Notably, while the improvement is evident, we show that the
substantial computational overheads entailed by graphs are prohibitively
expensive for real-world recommendations. In light of this, we propose a
non-parametric strategy that utilizes graph convolution for re-ranking only
during test time. Our strategy circumvents the notorious computational
overheads from graph convolution during training, and utilizes structural
knowledge hidden in graphs on-the-fly during testing. It can be used as a
plug-and-play module and easily employed to enhance the ranking ability of
various ranking layers of a real-world RecSys with significantly reduced
computational overhead. Through comprehensive experiments across four benchmark
datasets with varying levels of sparsity, we demonstrate that our strategy
yields noticeable improvements (i.e., 8.1% on average) during testing time with
little to no additional computational overheads (i.e., 0.5 on average). Code:
https://github.com/zyouyang/RecSys2025_NonParamGC.git

</details>


### [108] [SLIF-MR: Self-loop Iterative Fusion of Heterogeneous Auxiliary Information for Multimodal Recommendation](https://arxiv.org/abs/2507.09998)
*Jie Guo,Jiahao Jiang,Ziyuan Guo,Bin Song,Yue Sun*

Main category: cs.IR

TL;DR: 提出SLIF - MR框架，通过反馈信号动态优化异构图结构，结合语义一致性学习策略，实验显示该框架在准确性和鲁棒性上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态知识图（MKG）方法在训练时冻结MKG结构，限制异构图结构信息融合，导致性能欠佳。

Method: 提出SLIF - MR框架，利用前一训练轮次的项目表示作为反馈信号动态优化异构图结构，构建项目 - 项目关联图并以自循环方式融入异构图建立过程，同时提出语义一致性学习策略。

Result: 实验结果表明SLIF - MR显著优于现有方法，特别是在准确性和鲁棒性方面。

Conclusion: SLIF - MR框架有效解决了现有方法的问题，能提升推荐系统性能。

Abstract: Knowledge graphs (KGs) and multimodal item information, which respectively
capture relational and attribute features, play a crucial role in improving
recommender system accuracy. Recent studies have attempted to integrate them
via multimodal knowledge graphs (MKGs) to further enhance recommendation
performance. However, existing methods typically freeze the MKG structure
during training, which limits the full integration of structural information
from heterogeneous graphs (e.g., KG and user-item interaction graph), and
results in sub-optimal performance. To address this challenge, we propose a
novel framework, termed Self-loop Iterative Fusion of Heterogeneous Auxiliary
Information for Multimodal Recommendation (SLIF-MR), which leverages item
representations from previous training epoch as feedback signals to dynamically
optimize the heterogeneous graph structures composed of KG, multimodal item
feature graph, and user-item interaction graph. Through this iterative fusion
mechanism, both user and item representations are refined, thus improving the
final recommendation performance. Specifically, based on the feedback item
representations, SLIF-MR constructs an item-item correlation graph, then
integrated into the establishment process of heterogeneous graphs as additional
new structural information in a self-loop manner. Consequently, the internal
structures of heterogeneous graphs are updated with the feedback item
representations during training. Moreover, a semantic consistency learning
strategy is proposed to align heterogeneous item representations across
modalities. The experimental results show that SLIF-MR significantly
outperforms existing methods, particularly in terms of accuracy and robustness.

</details>


### [109] [PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization](https://arxiv.org/abs/2507.10057)
*Sangwoo Park,Jinheon Baek,Soyeong Jeong,Sung Ju Hwang*

Main category: cs.IR

TL;DR: 提出PRISM文档检索方法和SciFullBench基准，实验表明PRISM较现有基线平均性能提升4.3%


<details>
  <summary>Details</summary>
Motivation: 以往文档检索方法聚焦摘要，摘要仅提供稀疏和高层级总结，需新方法解决问题

Method: 提出PRISM方法，将查询论文分解为多方面特定视图并单独嵌入，与候选论文匹配；提出SciFullBench基准，提供完整和分段上下文

Result: PRISM较现有检索基线平均性能提升4.3%

Conclusion: PRISM方法能有效提升科学论文检索性能

Abstract: Scientific paper retrieval, particularly framed as document-to-document
retrieval, aims to identify relevant papers in response to a long-form query
paper, rather than a short query string. Previous approaches to this task have
focused on abstracts, embedding them into dense vectors as surrogates for full
documents and calculating similarity across them, although abstracts provide
only sparse and high-level summaries. To address this, we propose PRISM, a
novel document-to-document retrieval method that introduces multiple,
fine-grained representations for both the query and candidate papers. In
particular, each query paper is decomposed into multiple aspect-specific views
and individually embedded, which are then matched against candidate papers
similarity segmented to consider their multifaceted dimensions. Moreover, we
present SciFullBench, a novel benchmark in which the complete and segmented
context of full papers for both queries and candidates is available. Then,
experimental results show that PRISM improves performance by an average of 4.3%
over existing retrieval baselines.

</details>


### [110] [User Long-Term Multi-Interest Retrieval Model for Recommendation](https://arxiv.org/abs/2507.10097)
*Yue Meng,Cheng Guo,Xiaohui Hu,Honghu Deng,Yi Cao,Tong Liu,Bo Zheng*

Main category: cs.IR

TL;DR: 提出用户长期多兴趣检索模型ULIM用于检索阶段的千级行为建模，实验显示该模型优于现有方法并提升业务指标。


<details>
  <summary>Details</summary>
Motivation: 现有检索模型受实时服务延迟预算和缺乏目标感知机制等限制，只能处理数百个行为序列，需解决此问题以提升性能。

Method: 提出ULIM框架，包含类别感知分层双兴趣学习和指针增强级联类别到物品检索两个组件。

Result: 在淘宝数据集上实验，ULIM较现有方法有显著提升，为淘宝秒杀小程序带来点击、订单和GMV增长。

Conclusion: ULIM能有效解决现有检索模型的局限，可用于检索阶段的千级行为建模并提升业务效果。

Abstract: User behavior sequence modeling, which captures user interest from rich
historical interactions, is pivotal for industrial recommendation systems.
Despite breakthroughs in ranking-stage models capable of leveraging ultra-long
behavior sequences with length scaling up to thousands, existing retrieval
models remain constrained to sequences of hundreds of behaviors due to two main
challenges. One is strict latency budget imposed by real-time service over
large-scale candidate pool. The other is the absence of target-aware mechanisms
and cross-interaction architectures, which prevent utilizing ranking-like
techniques to simplify long sequence modeling. To address these limitations, we
propose a new framework named User Long-term Multi-Interest Retrieval
Model(ULIM), which enables thousand-scale behavior modeling in retrieval
stages. ULIM includes two novel components: 1)Category-Aware Hierarchical
Dual-Interest Learning partitions long behavior sequences into multiple
category-aware subsequences representing multi-interest and jointly optimizes
long-term and short-term interests within specific interest cluster.
2)Pointer-Enhanced Cascaded Category-to-Item Retrieval introduces
Pointer-Generator Interest Network(PGIN) for next-category prediction, followed
by next-item retrieval upon the top-K predicted categories. Comprehensive
experiments on Taobao dataset show that ULIM achieves substantial improvement
over state-of-the-art methods, and brings 5.54% clicks, 11.01% orders and 4.03%
GMV lift for Taobaomiaosha, a notable mini-app of Taobao.

</details>


### [111] [Riding the Carousel: The First Extensive Eye Tracking Analysis of Browsing Behavior in Carousel Recommenders](https://arxiv.org/abs/2507.10135)
*Santiago de Leon-Martinez,Robert Moro,Branislav Kveton,Maria Bielikova*

Main category: cs.IR

TL;DR: 本文对轮播推荐器中用户的眼动浏览行为进行了首次广泛分析，为推荐系统设计者提供优化建议。


<details>
  <summary>Details</summary>
Motivation: 轮播界面缺乏研究，尤其在推荐系统设计与传统单列表界面差异方面，理解用户浏览方式对系统设计很关键。

Method: 采用眼动追踪技术，在自由浏览设置下分析轮播推荐器中的眼动行为，研究用户浏览起始位置、项目间过渡方式以及流派偏好对过渡的影响。

Result: 得到了轮播中眼动浏览行为的首次广泛实证结果。

Conclusion: 为轮播推荐系统设计者提供优化建议，如重新排列项目位置，有助于改进现有系统并推动新的设计。

Abstract: Carousels have become the de-facto interface in online services. However,
there is a lack of research in carousels, particularly examining how
recommender systems may be designed differently than the traditional
single-list interfaces. One of the key elements for understanding how to design
a system for a particular interface is understanding how users browse. For
carousels, users may browse in a number of different ways due to the added
complexity of multiple topic defined-lists and swiping to see more items.
  Eye tracking is the key to understanding user behavior by providing valuable,
direct information on how users see and navigate. In this work, we provide the
first extensive analysis of the eye tracking behavior in carousel recommenders
under the free-browsing setting. To understand how users browse, we examine the
following research questions : 1) where do users start browsing, 2) how do
users transition from item to item within the same carousel and across
carousels, and 3) how does genre preference impact transitions?
  This work addresses a gap in the field and provides the first extensive
empirical results of eye tracked browsing behavior in carousels for improving
recommenders. Taking into account the insights learned from the above
questions, our final contribution is to provide suggestions to help carousel
recommender system designers optimize their systems for user browsing behavior.
The most important suggestion being to reorder the ranked item positions to
account for browsing after swiping.These contributions aim not only to help
improve current systems, but also to encourage and allow the design of new user
models, systems, and metrics that are better suited to the complexity of
carousel interfaces.

</details>


### [112] [Am I on the Right Track? What Can Predicted Query Performance Tell Us about the Search Behaviour of Agentic RAG](https://arxiv.org/abs/2507.10411)
*Fangzheng Tian,Jinyuan Fang,Debasis Ganguly,Zaiqiao Meng,Craig Macdonald*

Main category: cs.IR

TL;DR: 研究Agentic RAG模型中查询性能预测（QPP）的适用性，发现有效检索器可提升答案质量，QPP估计与最终答案质量正相关。


<details>
  <summary>Details</summary>
Motivation: 当前Agentic RAG模型生成的查询及检索器在获取高质量答案中的作用研究不足，需进行相关研究。

Method: 研究QPP在Agentic RAG模型Search - R1和R1 - Searcher中的适用性。

Result: 应用有效检索器可在更短推理过程中获得更高质量答案，QPP估计与最终答案质量正相关。

Conclusion: 研究是迈向Agentic RAG自适应检索的一步，QPP可告知模型检索结果是否有用。

Abstract: Agentic Retrieval-Augmented Generation (RAG) is a new paradigm where the
reasoning model decides when to invoke a retriever (as a "tool") when answering
a question. This paradigm, exemplified by recent research works such as
Search-R1, enables the model to decide when to search and obtain external
information. However, the queries generated by such Agentic RAG models and the
role of the retriever in obtaining high-quality answers remain understudied. To
this end, this initial study examines the applicability of query performance
prediction (QPP) within the recent Agentic RAG models Search-R1 and
R1-Searcher. We find that applying effective retrievers can achieve higher
answer quality within a shorter reasoning process. Moreover, the QPP estimates
of the generated queries, used as an approximation of their retrieval quality,
are positively correlated with the quality of the final answer. Ultimately, our
work is a step towards adaptive retrieval within Agentic RAG, where QPP is used
to inform the model if the retrieved results are likely to be useful.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [113] [Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing](https://arxiv.org/abs/2507.08836)
*Damien Fovet,Shashank Chamoli,Sarah Oury,Srishti Singhal*

Main category: cs.LG

TL;DR: 评估CompactifAI压缩方法对Llama 3.1 8B模型的性能，发现压缩模型可减少计算资源并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 评估CompactifAI压缩方法应用于大语言模型Llama 3.1 8B的性能。

Method: 使用Codecarbon和Ragas框架分别评估模型效率和准确性，并与全尺寸模型对比。

Result: 使用CompactifAI的压缩模型显著减少计算资源，且保持了模型准确性。

Conclusion: 压缩模型更高效、可扩展且具有成本效益。

Abstract: This study evaluates the performance of a compression method, called
CompactifAI, developed by Multiverse Computing, applied to the large language
model Llama 3.1 8B\cite{llama}. The evaluation focused on model efficiency (in
terms of energy consumption) and accuracy using respectively the frameworks
Codecarbon\cite{codecarbon} and Ragas\cite{ragas}. A comparison was performed
between the model compressed with
CompactifAI\cite{compactifai}\cite{compactifai2} and its full-size version. Our
findings reveal that the compressed model using CompactifAI not only
significantly reduced the computational resources but also maintained the model
accuracy, making the model more efficient, scalable and cost-effective.

</details>


### [114] [Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning](https://arxiv.org/abs/2507.08828)
*Tarek Berghout*

Main category: cs.LG

TL;DR: 本文提出了循环扩展（RE）这一学习范式，超越传统机器学习和深度学习，能从模型自身行为学习，还有多种扩展变体，为新型智能模型奠定基础并提供代码示例。


<details>
  <summary>Details</summary>
Motivation: 突破传统机器学习和深度学习仅从静态数据表示学习的局限，提出新的学习维度。

Method: 提出RE范式，强调通过相同深度架构对数据进行多次映射并结合性能信号分析内部表示；扩展出MVRE、HMVRE和Sc - HMVRE。

Result: RE实现了模型的迭代自我提升，从单纯的表征学习转向行为感知、自我进化的系统。

Conclusion: RE为新型智能模型奠定基础，为可扩展、内省和自适应的人工智能提供路径。

Abstract: This paper introduces Recurrent Expansion (RE) as a new learning paradigm
that advances beyond conventional Machine Learning (ML) and Deep Learning (DL).
While DL focuses on learning from static data representations, RE proposes an
additional dimension: learning from the evolving behavior of models themselves.
RE emphasizes multiple mappings of data through identical deep architectures
and analyzes their internal representations (i.e., feature maps) in conjunction
with observed performance signals such as loss. By incorporating these
behavioral traces, RE enables iterative self-improvement, allowing each model
version to gain insight from its predecessors. The framework is extended
through Multiverse RE (MVRE), which aggregates signals from parallel model
instances, and further through Heterogeneous MVRE (HMVRE), where models of
varying architectures contribute diverse perspectives. A scalable and adaptive
variant, Sc-HMVRE, introduces selective mechanisms and scale diversity for
real-world deployment. Altogether, RE presents a shift in DL: from purely
representational learning to behavior-aware, self-evolving systems. It lays the
groundwork for a new class of intelligent models capable of reasoning over
their own learning dynamics, offering a path toward scalable, introspective,
and adaptive artificial intelligence. A simple code example to support
beginners in running their own experiments is provided in Code Availability
Section of this paper.

</details>


### [115] [Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI](https://arxiv.org/abs/2507.08829)
*Kimia Soroush,Nastaran Shirazi,Mohsen Raji*

Main category: cs.LG

TL;DR: 本文提出一种基于XAI的高效TMR方法提升DNN抗位翻转故障可靠性，并用LRP计算参数重要性得分，实验表明可提升AlexNet可靠性。


<details>
  <summary>Details</summary>
Motivation: DNN用于安全关键领域需确保可靠性，TMR有显著开销，选择标准准确性对TMR效率至关重要。

Method: 利用基于梯度的低成本XAI技术LRP计算DNN参数重要性得分，用TMR保护关键权重。

Result: 该方法能在误码率10 - 4时保护AlexNet模型，可靠性提升超60%，开销与现有方法相同。

Conclusion: 所提高效TMR方法能有效提升DNN抗位翻转故障的可靠性。

Abstract: Deep Neural Networks (DNNs) are widely employed in safety-critical domains,
where ensuring their reliability is essential. Triple Modular Redundancy (TMR)
is an effective technique to enhance the reliability of DNNs in the presence of
bit-flip faults. In order to handle the significant overhead of TMR, it is
applied selectively on the parameters and components with the highest
contribution at the model output. Hence, the accuracy of the selection
criterion plays the key role on the efficiency of TMR. This paper presents an
efficient TMR approach to enhance the reliability of DNNs against bit-flip
faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can
provide valuable insights about the importance of individual neurons and
weights in the performance of the network, they can be applied as the selection
metric in TMR techniques. The proposed method utilizes a low-cost,
gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to
calculate importance scores for DNN parameters. These scores are then used to
enhance the reliability of the model, with the most critical weights being
protected by TMR. The proposed approach is evaluated on two DNN models, VGG16
and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate
that the method can protect the AlexNet model at a bit error rate of 10-4,
achieving over 60% reliability improvement while maintaining the same overhead
as state-of-the-art methods.

</details>


### [116] [A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting](https://arxiv.org/abs/2507.08832)
*Niranjan Mallikarjun Sindhur,Pavithra C,Nivya Muchikel*

Main category: cs.LG

TL;DR: 本文提出一种决策支持系统，结合机器学习与人机交互，通过语音界面为印度卡纳塔克邦农民提供作物种植建议，提高预测准确性，增强农民财务韧性。


<details>
  <summary>Details</summary>
Motivation: 解决印度卡纳塔克邦农民面临的市场和气候波动，以及因识字障碍被排除在数字革命之外的双重挑战。

Method: 提出混合推荐引擎，集成随机森林分类器和长短期记忆网络，通过基于语音的本地语言界面提供服务。

Result: 随机森林模型在适宜性预测中达到98.5%的准确率，长短期记忆网络模型预测收获时价格误差小。

Conclusion: 该系统通过包容性界面提供数据驱动、经济优化的建议，是增强边缘化农业社区财务韧性的可扩展且有影响力的解决方案。

Abstract: Farmers in developing regions like Karnataka, India, face a dual challenge:
navigating extreme market and climate volatility while being excluded from the
digital revolution due to literacy barriers. This paper presents a novel
decision support system that addresses both challenges through a unique
synthesis of machine learning and human-computer interaction. We propose a
hybrid recommendation engine that integrates two predictive models: a Random
Forest classifier to assess agronomic suitability based on soil, climate, and
real-time weather data, and a Long Short-Term Memory (LSTM) network to forecast
market prices for agronomically viable crops. This integrated approach shifts
the paradigm from "what can grow?" to "what is most profitable to grow?",
providing a significant advantage in mitigating economic risk. The system is
delivered through an end-to-end, voice-based interface in the local Kannada
language, leveraging fine-tuned speech recognition and high-fidelity speech
synthesis models to ensure accessibility for low-literacy users. Our results
show that the Random Forest model achieves 98.5% accuracy in suitability
prediction, while the LSTM model forecasts harvest-time prices with a low
margin of error. By providing data-driven, economically optimized
recommendations through an inclusive interface, this work offers a scalable and
impactful solution to enhance the financial resilience of marginalized farming
communities.

</details>


### [117] [Representation learning with a transformer by contrastive learning for money laundering detection](https://arxiv.org/abs/2507.08835)
*Harold Guéneau,Alain Celisse,Pascal Delange*

Main category: cs.LG

TL;DR: 本文提出新程序利用Transformer神经网络处理洗钱检测问题，实验表明该程序能在少监督下利用洗钱模式，检测能力强且能控制误报率。


<details>
  <summary>Details</summary>
Motivation: 解决洗钱检测问题。

Method: 引入新程序，第一步通过对比学习学习时间序列表示，第二步利用表示生成洗钱评分，采用双阈值方法并结合Benjamini - Hochberg程序控制误报率。

Result: Transformer能产生通用表示，成功利用洗钱模式，新程序检测非欺诈者和欺诈者能力更强，且能控制误报率，优于基于规则或LSTM架构的程序。

Conclusion: 新程序在洗钱检测中表现良好，能在少监督下有效工作，控制误报率。

Abstract: The present work tackles the money laundering detection problem. A new
procedure is introduced which exploits structured time series of both
qualitative and quantitative data by means of a transformer neural network. The
first step of this procedure aims at learning representations of time series
through contrastive learning (without any labels). The second step leverages
these representations to generate a money laundering scoring of all
observations. A two-thresholds approach is then introduced, which ensures a
controlled false-positive rate by means of the Benjamini-Hochberg (BH)
procedure. Experiments confirm that the transformer is able to produce general
representations that succeed in exploiting money laundering patterns with
minimal supervision from domain experts. It also illustrates the higher ability
of the new procedure for detecting nonfraudsters as well as fraudsters, while
keeping the false positive rate under control. This greatly contrasts with
rule-based procedures or the ones based on LSTM architectures.

</details>


### [118] [LoRA Is Slower Than You Think](https://arxiv.org/abs/2507.08833)
*Seokmin Ko*

Main category: cs.LG

TL;DR: 分析LoRA在微调大语言模型时速度提升不一致问题，提出高效微调方法并验证效果。


<details>
  <summary>Details</summary>
Motivation: LoRA在不同模型架构和训练设置下速度提升不一致，需分析性能并探究限制提速因素。

Method: 对LoRA性能进行全面分析，提出几种更高效微调大语言模型的方法。

Result: 所提方法能达到与LoRA相当或更优性能，且训练速度提升更稳定。

Conclusion: 为资源受限下优化大语言模型微调提供有价值见解和实用指南。

Abstract: Low-Rank Adaptation (LoRA) is one of the most widely used techniques for
fine-tuning large language models (LLMs). By introducing a small number of
trainable low-rank weight matrices, LoRA substantially reduces the number of
parameters that need to be updated, offering significant advantages in memory
consumption and computational efficiency compared to full fine-tuning. However,
we observed that LoRA does not consistently provide speed improvements across
all model architectures and training setups. Motivated by this inconsistency,
we conduct a comprehensive analysis of LoRA's performance and investigate the
underlying factors limiting its speedup. Based on our findings, we propose
several methods for more efficient fine-tuning of LLMs. We empirically evaluate
these methods and compare them to LoRA, demonstrating that our approach
achieves comparable or superior performance while delivering more consistent
training speed improvements. Our work offers valuable insights and practical
guidelines for practitioners seeking to optimize LLM fine-tuning under resource
constraints.

</details>


### [119] [Physical Informed Neural Networks for modeling ocean pollutant](https://arxiv.org/abs/2507.08834)
*Karishma Battina,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 本文引入PINN框架模拟海洋污染物扩散，结合物理定律与含噪合成数据训练网络，采用混合损失函数，利用Julia语言实现高性能模拟。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在模拟广阔动态海洋领域污染物传输时面临复杂性和规模挑战。

Method: 引入PINN框架，将物理定律和通过FDM生成的含噪合成数据嵌入神经网络训练，采用含PDE残差、边界/初始条件一致性和加权数据拟合项的混合损失函数，利用Julia语言进行高性能模拟。

Result: 模型能实现物理上一致的预测，解决非线性动力学、边界和初始条件约束等挑战。

Conclusion: 该方法为传统求解器提供了可扩展且灵活的替代方案。

Abstract: Traditional numerical methods often struggle with the complexity and scale of
modeling pollutant transport across vast and dynamic oceanic domains. This
paper introduces a Physics-Informed Neural Network (PINN) framework to simulate
the dispersion of pollutants governed by the 2D advection-diffusion equation.
The model achieves physically consistent predictions by embedding physical laws
and fitting to noisy synthetic data, generated via a finite difference method
(FDM), directly into the neural network training process. This approach
addresses challenges such as non-linear dynamics and the enforcement of
boundary and initial conditions. Synthetic data sets, augmented with varying
noise levels, are used to capture real-world variability. The training
incorporates a hybrid loss function including PDE residuals, boundary/initial
condition conformity, and a weighted data fit term. The approach takes
advantage of the Julia language scientific computing ecosystem for
high-performance simulations, offering a scalable and flexible alternative to
traditional solvers

</details>


### [120] [Effects of structural properties of neural networks on machine learning performance](https://arxiv.org/abs/2507.10005)
*Yash Arya,Sang Hoon Lee*

Main category: cs.LG

TL;DR: 本文综合研究神经网络图结构与预测性能关系，用多种模型网络分析结构属性对图像分类任务影响，发现特定结构网络学习能力强，为网络科学和机器学习做贡献。


<details>
  <summary>Details</summary>
Motivation: 以往研究局限于窄范围模型网络，缺乏对社区等中尺度结构研究，本文旨在更全面研究图结构与预测性能关系。

Method: 采用随机和无标度等模型网络，与生物神经网络及其子集对比，分析结构属性对图像分类任务性能的影响。

Result: 结构属性在一定程度上影响性能，具有连贯、密集互连社区的网络学习能力增强。

Conclusion: 本研究为网络科学和机器学习有意义贡献，提示可启发设计更具生物信息的神经网络。

Abstract: In recent years, graph-based machine learning techniques, such as
reinforcement learning and graph neural networks, have garnered significant
attention. While some recent studies have started to explore the relationship
between the graph structure of neural networks and their predictive
performance, they often limit themselves to a narrow range of model networks,
particularly lacking mesoscale structures such as communities. Our work
advances this area by conducting a more comprehensive investigation,
incorporating realistic network structures characterized by heterogeneous
degree distributions and community structures, which are typical
characteristics of many real networks. These community structures offer a
nuanced perspective on network architecture. Our analysis employs model
networks such as random and scale-free networks, alongside a comparison with a
biological neural network and its subsets for more detailed analysis. We
examine the impact of these structural attributes on the performance of image
classification tasks. Our findings reveal that structural properties do affect
performance to some extent. Specifically, networks featuring coherent, densely
interconnected communities demonstrate enhanced learning capabilities. The
comparison with the biological neural network emphasizes the relevance of our
findings to real-world structures, suggesting an intriguing connection worth
further exploration. This study contributes meaningfully to network science and
machine learning, providing insights that could inspire the design of more
biologically informed neural networks.

</details>


### [121] [wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models](https://arxiv.org/abs/2507.08838)
*Xiaohang Tang,Rares Dolga,Sangwoong Yoon,Ilija Bogunovic*

Main category: cs.LG

TL;DR: 提出wd1方法优化扩散大语言模型强化学习推理能力，实验显示其效果好、效率高。


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型通过强化学习提升推理能力存在计算开销大、偏差大的问题。

Method: 引入wd1策略优化方法，将目标重新表述为加权似然，仅需对当前参数化策略似然进行一次近似。

Result: 在推理基准测试中，wd1在无监督微调或监督数据的情况下，优于现有强化学习方法，准确率最高提升16%，有额外计算收益。

Conclusion: wd1是对扩散大语言模型推理应用强化学习更有效和高效的方法。

Abstract: Improving the reasoning capabilities of diffusion-based large language models
(dLLMs) through reinforcement learning (RL) remains an open problem. The
intractability of dLLMs likelihood function necessitates approximating the
current, old, and reference policy likelihoods at each policy optimization
step. This reliance introduces additional computational overhead and lead to
potentially large bias -- particularly when approximation errors occur in the
denominator of policy ratios used for importance sampling. To mitigate these
issues, we introduce $\mathtt{wd1}$, a novel policy optimization approach that
reformulates the objective as a weighted likelihood, requiring only a single
approximation for the current parametrized policy likelihood. Experiments on
widely used reasoning benchmarks demonstrate that $\mathtt{wd1}$, without
supervised fine-tuning (SFT) or any supervised data, outperforms existing RL
methods for dLLMs, achieving up to 16% higher accuracy. $\mathtt{wd1}$ delivers
additional computational gains, including reduced training time and fewer
function evaluations (NFEs) per gradient step. These findings, combined with
the simplicity of method's implementation and R1-Zero-like training (no SFT),
position $\mathtt{wd1}$ as a more effective and efficient method for applying
RL to dLLMs reasoning.

</details>


### [122] [Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer](https://arxiv.org/abs/2507.08839)
*Xiaowei Yu,Jing Zhang,Tong Chen,Yan Zhuang,Minheng Chen,Chao Cao,Yanjun Lyu,Lu Zhang,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.LG

TL;DR: 针对路易体痴呆（LBD）诊断数据稀缺问题，提出可转移感知变压器（TAT）方法，利用AD数据提升LBD诊断，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: LBD诊断数据稀缺限制深度学习效果，AD数据丰富但两者存在领域差异，需有效利用AD数据同时缓解领域偏移以提升LBD诊断。

Method: 提出TAT方法，利用结构磁共振成像的结构连接性作为训练数据，基于注意力机制自适应分配权重，减少领域偏移。

Result: 实验结果证明了TAT方法的有效性。

Conclusion: 这是首次在数据稀缺和领域偏移条件下探索从AD到LBD的领域自适应，为罕见病领域自适应诊断提供了有前景的框架。

Abstract: Lewy Body Disease (LBD) is a common yet understudied form of dementia that
imposes a significant burden on public health. It shares clinical similarities
with Alzheimer's disease (AD), as both progress through stages of normal
cognition, mild cognitive impairment, and dementia. A major obstacle in LBD
diagnosis is data scarcity, which limits the effectiveness of deep learning. In
contrast, AD datasets are more abundant, offering potential for knowledge
transfer. However, LBD and AD data are typically collected from different sites
using different machines and protocols, resulting in a distinct domain shift.
To effectively leverage AD data while mitigating domain shift, we propose a
Transferability Aware Transformer (TAT) that adapts knowledge from AD to
enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived
from structural MRI as training data. Built on the attention mechanism, TAT
adaptively assigns greater weights to disease-transferable features while
suppressing domain-specific ones, thereby reducing domain shift and improving
diagnostic accuracy with limited LBD data. The experimental results demonstrate
the effectiveness of TAT. To the best of our knowledge, this is the first study
to explore domain adaptation from AD to LBD under conditions of data scarcity
and domain shift, providing a promising framework for domain-adaptive diagnosis
of rare diseases.

</details>


### [123] [Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components](https://arxiv.org/abs/2507.09443)
*Luiz Aldeia Machado,Victor Coppo Leite,Elia Merzari,Arthur Motta,Roberto Ponciroli,Lander Ibarra,Lise Charlot*

Main category: cs.LG

TL;DR: 本文探讨用CNN结合热机械模型，基于少量温度测量估算压水堆燃料棒运行时的温度、应力和应变，助力核反应堆预测性维护。


<details>
  <summary>Details</summary>
Motivation: 主动维护策略（如预测性维护）可减少核电站因组件故障导致的意外停机时间，本文旨在开发核反应堆的预测性维护工具。

Method: 采用CNN架构结合热机械模型，利用BISON和MOOSE - THM进行耦合模拟生成训练、验证和测试数据集，改变峰值线性热生成率进行11次模拟。

Result: CNN训练超1000个周期无过拟合，实现高精度温度分布预测，并用于热机械模型确定燃料棒内应力和应变分布。

Conclusion: 所提出的方法可实现核反应堆系统的实时监测，有助于开发核反应堆的预测性维护工具。

Abstract: Proactive maintenance strategies, such as Predictive Maintenance (PdM), play
an important role in the operation of Nuclear Power Plants (NPPs), particularly
due to their capacity to reduce offline time by preventing unexpected shutdowns
caused by component failures.
  In this work, we explore the use of a Convolutional Neural Network (CNN)
architecture combined with a computational thermomechanical model to calculate
the temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel
rod during operation. This estimation relies on a limited number of temperature
measurements from the cladding's outer surface. This methodology can
potentially aid in developing PdM tools for nuclear reactors by enabling
real-time monitoring of such systems.
  The training, validation, and testing datasets were generated through coupled
simulations involving BISON, a finite element-based nuclear fuel performance
code, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven
simulations, varying the peak linear heat generation rates. Of these, eight
were used for training, two for validation, and one for testing.
  The CNN was trained for over 1,000 epochs without signs of overfitting,
achieving highly accurate temperature distribution predictions. These were then
used in a thermomechanical model to determine the stress and strain
distribution within the fuel rod.

</details>


### [124] [Zero-Shot Neural Architecture Search with Weighted Response Correlation](https://arxiv.org/abs/2507.08841)
*Kun Jing,Luoyu Chen,Jungang Xu,Jianwei Tai,Yiyu Wang,Shuaimin Li*

Main category: cs.LG

TL;DR: 提出无训练估计代理WRCor加速NAS架构评估，实验表明其在代理评估和架构搜索中优于现有方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有NAS架构评估计算成本高、耗时，零样本NAS方法在有效性、稳定性和通用性上不足。

Method: 提出加权响应相关性（WRCor）作为无训练估计代理，利用不同输入样本响应的相关系数矩阵计算代理分数。

Result: WRCor及其投票代理在代理评估中比现有代理更高效；零样本NAS算法在不同搜索空间中优于多数现有NAS算法，能在4 GPU小时内发现ImageNet - 1k测试误差22.1%的架构。

Conclusion: WRCor是有效的无训练估计代理，基于它的零样本NAS算法性能良好。

Abstract: Neural architecture search (NAS) is a promising approach for automatically
designing neural network architectures. However, the architecture estimation of
NAS is computationally expensive and time-consuming because of training
multiple architectures from scratch. Although existing zero-shot NAS methods
use training-free proxies to accelerate the architecture estimation, their
effectiveness, stability, and generality are still lacking. We present a novel
training-free estimation proxy called weighted response correlation (WRCor).
WRCor utilizes correlation coefficient matrices of responses across different
input samples to calculate the proxy scores of estimated architectures, which
can measure their expressivity and generalizability. Experimental results on
proxy evaluation demonstrate that WRCor and its voting proxies are more
efficient estimation strategies than existing proxies. We also apply them with
different search strategies in architecture search. Experimental results on
architecture search show that our zero-shot NAS algorithm outperforms most
existing NAS algorithms in different search spaces. Our NAS algorithm can
discover an architecture with a 22.1% test error on the ImageNet-1k dataset
within 4 GPU hours. All codes are publicly available at
https://github.com/kunjing96/ZSNAS-WRCor.git.

</details>


### [125] [Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing](https://arxiv.org/abs/2507.08842)
*Zhufeng Lu,Chentao Jia,Ming Hu,Xiaofei Xie,Mingsong Chen*

Main category: cs.LG

TL;DR: 本文提出通信高效的联邦推荐系统框架FedRAS，采用动作共享策略和自适应聚类机制，减少通信开销且不牺牲推荐性能，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐系统存在通信开销高和训练效率低问题，且现有压缩技术会导致模型性能下降。

Method: 提出FedRAS框架，采用动作共享策略将物品嵌入梯度聚类为模型更新动作进行通信，结合自适应聚类机制动态调整动作数量。

Result: 在知名数据集上实验表明，FedRAS可将通信负载大小最多降低96.88%，且在不同异构场景下不牺牲推荐性能。

Conclusion: FedRAS能有效解决联邦推荐系统通信开销高和训练效率低的问题，具有良好的性能。

Abstract: As a promising privacy-aware collaborative model training paradigm, Federated
Learning (FL) is becoming popular in the design of distributed recommender
systems. However, Federated Recommender Systems (FedRecs) greatly suffer from
two major problems: i) extremely high communication overhead due to massive
item embeddings involved in recommendation systems, and ii) intolerably low
training efficiency caused by the entanglement of both heterogeneous network
environments and client devices. Although existing methods attempt to employ
various compression techniques to reduce communication overhead, due to the
parameter errors introduced by model compression, they inevitably suffer from
model performance degradation. To simultaneously address the above problems,
this paper presents a communication-efficient FedRec framework named FedRAS,
which adopts an action-sharing strategy to cluster the gradients of item
embedding into a specific number of model updating actions for communication
rather than directly compressing the item embeddings. In this way, the cloud
server can use the limited actions from clients to update all the items. Since
gradient values are significantly smaller than item embeddings, constraining
the directions of gradients (i.e., the action space) introduces smaller errors
compared to compressing the entire item embedding matrix into a reduced space.
To accommodate heterogeneous devices and network environments, FedRAS
incorporates an adaptive clustering mechanism that dynamically adjusts the
number of actions. Comprehensive experiments on well-known datasets demonstrate
that FedRAS can reduce the size of communication payloads by up to 96.88%,
while not sacrificing recommendation performance within various heterogeneous
scenarios. We have open-sourced FedRAS at
https://github.com/mastlab-T3S/FedRAS.

</details>


### [126] [Networked Information Aggregation via Machine Learning](https://arxiv.org/abs/2507.09683)
*Michael Kearns,Aaron Roth,Emily Ryu*

Main category: cs.LG

TL;DR: 研究有向无环图（DAG）中分布式学习问题，给出线性和一般假设类上下界，确定DAG深度为关键参数并辅以实验。


<details>
  <summary>Details</summary>
Motivation: 探究在DAG分布式学习中，何时能实现信息聚合，即部分代理学习的模型误差能与直接访问所有特征学习的最佳模型竞争。

Method: 分析DAG中代理顺序学习过程，基于特征和父节点预测训练模型，为线性和一般假设类问题给出上下界。

Result: 确定DAG深度是关键参数，足够长路径可实现信息聚合，某些深度不足的DAG即使线性情况也无法实现。

Conclusion: DAG深度对分布式学习中的信息聚合至关重要，理论结果通过实验得到补充验证。

Abstract: We study a distributed learning problem in which learning agents are embedded
in a directed acyclic graph (DAG). There is a fixed and arbitrary distribution
over feature/label pairs, and each agent or vertex in the graph is able to
directly observe only a subset of the features -- potentially a different
subset for every agent. The agents learn sequentially in some order consistent
with a topological sort of the DAG, committing to a model mapping observations
to predictions of the real-valued label. Each agent observes the predictions of
their parents in the DAG, and trains their model using both the features of the
instance that they directly observe, and the predictions of their parents as
additional features. We ask when this process is sufficient to achieve
\emph{information aggregation}, in the sense that some agent in the DAG is able
to learn a model whose error is competitive with the best model that could have
been learned (in some hypothesis class) with direct access to \emph{all}
features, despite the fact that no single agent in the network has such access.
We give upper and lower bounds for this problem for both linear and general
hypothesis classes. Our results identify the \emph{depth} of the DAG as the key
parameter: information aggregation can occur over sufficiently long paths in
the DAG, assuming that all of the relevant features are well represented along
the path, and there are distributions over which information aggregation cannot
occur even in the linear case, and even in arbitrarily large DAGs that do not
have sufficient depth (such as a hub-and-spokes topology in which the spoke
vertices collectively see all the features). We complement our theoretical
results with a comprehensive set of experiments.

</details>


### [127] [Can We Predict Your Next Move Without Breaking Your Privacy?](https://arxiv.org/abs/2507.08843)
*Arpita Soni,Sahil Tripathi,Gautam Siddharth Kashyap,Manaswi Kulahara,Mohammad Anas Azeez,Zohaib Hasan Siddiqui,Nipun Joshi,Jiechao Gao*

Main category: cs.LG

TL;DR: 提出FLLL3M框架用于NxLP，能在保护隐私下确保高精度、低资源需求，在多个数据集取得SOT结果并减少参数和内存使用。


<details>
  <summary>Details</summary>
Motivation: 构建隐私保护的Next - Location Prediction框架。

Method: 提出FLLL3M框架，通过高效外积机制利用大语言模型，将用户数据保留在本地。

Result: 在Gowalla、WeePlace、Brightkite和FourSquare数据集上取得SOT结果，Acc@1和MRR表现良好，同时减少参数45.6%、内存使用52.7%。

Conclusion: FLLL3M框架在Next - Location Prediction中具有高精度、低资源需求和隐私保护的优势。

Abstract: We propose FLLL3M--Federated Learning with Large Language Models for Mobility
Modeling--a privacy-preserving framework for Next-Location Prediction (NxLP).
By retaining user data locally and leveraging LLMs through an efficient outer
product mechanism, FLLL3M ensures high accuracy with low resource demands. It
achieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,
0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while
reducing parameters by up to 45.6% and memory usage by 52.7%.

</details>


### [128] [DAFOS: Dynamic Adaptive Fanout Optimization Sampler](https://arxiv.org/abs/2507.08845)
*Irfan Ullah,Young-Koo Lee*

Main category: cs.LG

TL;DR: 提出DAFOS方法动态调整GNN训练的扇出，实验表明其能提升训练速度和准确率。


<details>
  <summary>Details</summary>
Motivation: 解决均匀邻居采样和静态扇出设置限制GNN可扩展性和效率的问题。

Method: 基于模型性能动态调整扇出，根据节点度对节点打分，聚焦重要节点，训练中增加扇出，集成早期停止机制。

Result: 在三个基准数据集上实验，相比现有方法显著提升训练速度和准确率，如在ogbn - arxiv数据集上加速3.57倍，在Reddit数据集上加速12.6倍，提高了F1分数。

Conclusion: DAFOS是大规模GNN训练的高效可扩展解决方案。

Abstract: Graph Neural Networks (GNNs) are becoming an essential tool for learning from
graph-structured data, however uniform neighbor sampling and static fanout
settings frequently limit GNNs' scalability and efficiency. In this paper, we
propose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel
approach that dynamically adjusts the fanout based on model performance and
prioritizes important nodes during training. Our approach leverages node
scoring based on node degree to focus computational resources on structurally
important nodes, incrementing the fanout as the model training progresses.
DAFOS also integrates an early stopping mechanism to halt training when
performance gains diminish. Experiments conducted on three benchmark datasets,
ogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach
significantly improves training speed and accuracy compared to a
state-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv
dataset and a 12.6x speedup on the Reddit dataset while improving the F1 score
from 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the
ogbn-products dataset, respectively. These results highlight the potential of
DAFOS as an efficient and scalable solution for large-scale GNN training.

</details>


### [129] [Assuring the Safety of Reinforcement Learning Components: AMLAS-RL](https://arxiv.org/abs/2507.08848)
*Calum Corrie Imrie,Ioannis Stefanakos,Sepeedeh Shahbeigi,Richard Hawkins,Simon Burton*

Main category: cs.LG

TL;DR: 本文提出AMLAS - RL框架为强化学习（RL）系统生成保证论证，以解决CPS中RL组件的安全保证问题，并通过轮式车辆示例进行演示。


<details>
  <summary>Details</summary>
Motivation: 机器学习融入网络物理系统（CPS）带来安全和保证挑战，现有Safe - RL方法缺乏全生命周期的系统保证，AMLAS不适用于RL独特挑战。

Method: 对AMLAS进行调整，通过迭代过程提出AMLAS - RL框架。

Result: 通过轮式车辆在无碰撞情况下到达目标的示例展示了AMLAS - RL。

Conclusion: AMLAS - RL框架可为RL系统提供安全保证。

Abstract: The rapid advancement of machine learning (ML) has led to its increasing
integration into cyber-physical systems (CPS) across diverse domains. While CPS
offer powerful capabilities, incorporating ML components introduces significant
safety and assurance challenges. Among ML techniques, reinforcement learning
(RL) is particularly suited for CPS due to its capacity to handle complex,
dynamic environments where explicit models of interaction between system and
environment are unavailable or difficult to construct. However, in
safety-critical applications, this learning process must not only be effective
but demonstrably safe. Safe-RL methods aim to address this by incorporating
safety constraints during learning, yet they fall short in providing systematic
assurance across the RL lifecycle. The AMLAS methodology offers structured
guidance for assuring the safety of supervised learning components, but it does
not directly apply to the unique challenges posed by RL. In this paper, we
adapt AMLAS to provide a framework for generating assurance arguments for an
RL-enabled system through an iterative process; AMLAS-RL. We demonstrate
AMLAS-RL using a running example of a wheeled vehicle tasked with reaching a
target goal without collision.

</details>


### [130] [On Evaluating Performance of LLM Inference Serving Systems](https://arxiv.org/abs/2507.09019)
*Amey Agrawal,Nitin Kedia,Anmol Agarwal,Jayashree Mohan,Nipun Kwatra,Souvik Kundu,Ramachandran Ramjee,Alexey Tumanov*

Main category: cs.LG

TL;DR: 当前大语言模型推理系统评估方法有缺陷，本文识别反模式并提供检查清单以改进评估。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理系统评估方法存在根本缺陷，阻碍科学进展。

Method: 全面检查近期系统，从基线公平性、评估设置和指标设计三个维度识别反模式，提供检查清单并进行案例研究。

Result: 识别常见反模式，展示其导致误导性结论，提供的框架可用于实际评估。

Conclusion: 为评估方法奠定严谨基础，能进行有意义比较、确保结果可重复，推动大语言模型推理系统真正进步。

Abstract: The rapid evolution of Large Language Model (LLM) inference systems has
yielded significant efficiency improvements. However, our systematic analysis
reveals that current evaluation methodologies frequently exhibit fundamental
flaws, often manifesting as common evaluation anti-patterns that obscure true
performance characteristics and impede scientific progress. Through a
comprehensive examination of recent systems, we identify recurring
anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,
and Metric Design. These anti-patterns are uniquely problematic for LLM
inference due to its dual-phase nature combining distinct prefill and decode
operations, its handling of highly heterogeneous workloads, and its strict
temporal requirements for interactive use. We demonstrate how common
anti-patterns -- such as inadequate baseline comparisons that conflate
engineering effort with algorithmic novelty, workload selections that fail to
represent production scenarios, and metric normalizations that hide substantial
performance variability like generation stalls-lead to misleading conclusions.
To address these challenges, we provide a comprehensive checklist derived from
our analysis, establishing a framework for recognizing and avoiding these
anti-patterns in favor of robust LLM inference evaluation. To demonstrate the
practical application of our framework, we present a case study analyzing
speculative decoding, a technique whose bursty, non-uniform token generation is
easily misinterpreted when evaluated using approaches characteristic of these
anti-patterns. Our work establishes a rigorous foundation for evaluation
methodology, enabling meaningful comparisons, ensuring reproducible results,
and ultimately accelerating genuine progress in LLM inference systems by moving
beyond common anti-patterns to align evaluation with real-world requirements.

</details>


### [131] [Average Sensitivity of Hierarchical $k$-Median Clustering](https://arxiv.org/abs/2507.10296)
*Shijie Li,Weiqiang He,Ruobing Bai,Pan Peng*

Main category: cs.LG

TL;DR: 本文聚焦分层k - 中位数聚类问题，分析算法平均敏感度，提出高效算法并证明其低敏感度与高聚类质量，还指出部分算法敏感度高，最后实验验证了所提算法的鲁棒性与有效性。


<details>
  <summary>Details</summary>
Motivation: 现代算法应用中数据集大且动态，分层聚类若对数据小扰动敏感会降低算法可用性，研究分层k - 中位数聚类问题以解决该问题。

Method: 通过测量随机删除一个数据点时输出的预期变化来分析算法的平均敏感度，提出分层k - 中位数聚类的高效算法并进行理论证明。

Result: 提出的算法具有低平均敏感度和高聚类质量，单链接聚类和CLNSS算法的确定性变体平均敏感度高，稳定性差。

Conclusion: 所提算法具有鲁棒性和有效性。

Abstract: Hierarchical clustering is a widely used method for unsupervised learning
with numerous applications. However, in the application of modern algorithms,
the datasets studied are usually large and dynamic. If the hierarchical
clustering is sensitive to small perturbations of the dataset, the usability of
the algorithm will be greatly reduced. In this paper, we focus on the
hierarchical $k$ -median clustering problem, which bridges hierarchical and
centroid-based clustering while offering theoretical appeal, practical utility,
and improved interpretability. We analyze the average sensitivity of algorithms
for this problem by measuring the expected change in the output when a random
data point is deleted. We propose an efficient algorithm for hierarchical
$k$-median clustering and theoretically prove its low average sensitivity and
high clustering quality. Additionally, we show that single linkage clustering
and a deterministic variant of the CLNSS algorithm exhibit high average
sensitivity, making them less stable. Finally, we validate the robustness and
effectiveness of our algorithm through experiments.

</details>


### [132] [Foundation models for time series forecasting: Application in conformal prediction](https://arxiv.org/abs/2507.08858)
*Sami Achour,Yassine Bouher,Duong Nguyen,Nicolas Chesneau*

Main category: cs.LG

TL;DR: 本文对比时间序列基础模型（TSFMs）和传统方法在共形预测中的表现，发现TSFMs在数据有限时更具优势，凸显其在时间序列应用中提升共形预测可靠性的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型零样本能力在时间序列共形预测中的潜力，对比TSFMs和传统方法性能。

Method: 在共形预测设置下，对比TSFMs与传统方法（统计模型和梯度提升）的表现。

Result: TSFMs在数据有限时有两个优势：预测区间更可靠；校准过程更稳定，且数据越少优势越明显。

Conclusion: 基础模型在时间序列应用中，尤其在数据受限情况下，能提升共形预测可靠性。

Abstract: The zero-shot capabilities of foundation models (FMs) for time series
forecasting offer promising potentials in conformal prediction, as most of the
available data can be allocated to calibration. This study compares the
performance of Time Series Foundation Models (TSFMs) with traditional methods,
including statistical models and gradient boosting, within a conformal
prediction setting. Our findings highlight two key advantages of TSFMs. First,
when the volume of data is limited, TSFMs provide more reliable conformalized
prediction intervals than classic models, thanks to their superior predictive
accuracy. Second, the calibration process is more stable because more data are
used for calibration. Morever, the fewer data available, the more pronounced
these benefits become, as classic models require a substantial amount of data
for effective training. These results underscore the potential of foundation
models in improving conformal prediction reliability in time series
applications, particularly in data-constrained cases. All the code to reproduce
the experiments is available.

</details>


### [133] [e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction](https://arxiv.org/abs/2507.08860)
*Awais Manzoor,M. Atif Qureshi,Etain Kidney,Luca Longo*

Main category: cs.LG

TL;DR: 提出新评估指标e - Profits用于客户流失预测模型评估，在电信数据集上显示能改变模型排名并带来财务优势。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标如AUC和F1 - score无法反映财务结果，可能误导战略决策。

Method: 引入e - Profits指标，用Kaplan - Meier生存分析估计个性化留存率，在两个电信数据集上对六个分类器进行基准测试。

Result: e - Profits改变了模型排名，显示出被传统指标忽视的模型的财务优势，还能提供高价值客户的细分层面洞察。

Conclusion: e - Profits是适用于商业环境的可理解的事后评估工具，尤其适合优先考虑利润驱动决策的团队。

Abstract: Retention campaigns in customer relationship management often rely on churn
prediction models evaluated using traditional metrics such as AUC and F1-score.
However, these metrics fail to reflect financial outcomes and may mislead
strategic decisions. We introduce e-Profits, a novel business-aligned
evaluation metric that quantifies model performance based on customer-specific
value, retention probability, and intervention costs. Unlike existing
profit-based metrics such as Expected Maximum Profit, which assume fixed
population-level parameters, e-Profits uses Kaplan-Meier survival analysis to
estimate personalised retention rates and supports granular, per customer
evaluation. We benchmark six classifiers across two telecom datasets (IBM Telco
and Maven Telecom) and demonstrate that e-Profits reshapes model rankings
compared to traditional metrics, revealing financial advantages in models
previously overlooked by AUC or F1-score. The metric also enables segment-level
insight into which models maximise return on investment for high-value
customers. e-Profits is designed as an understandable, post hoc tool to support
model evaluation in business contexts, particularly for marketing and analytics
teams prioritising profit-driven decisions. All source code is available at:
https://github.com/matifq/eprofits.

</details>


### [134] [On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition](https://arxiv.org/abs/2507.08861)
*Lucas Tesan,Mikel M. Iparraguirre,David Gonzalez,Pedro Martins,Elias Cueto*

Main category: cs.LG

TL;DR: 本文提出图神经网络解决偏微分方程时消息传递迭代次数的尖锐下界，减少超参数调优需求，并通过实例验证。


<details>
  <summary>Details</summary>
Motivation: 减少图神经网络解决偏微分方程时超参数的详尽调优需求。

Method: 将偏微分方程三类问题的物理特性与图神经网络消息传递要求关联，研究方程物理常数、时空离散化与消息传递机制的关系。

Result: 当消息传递迭代次数低于下界，网络信息传播低效、解质量差；满足下界时，模型能准确捕捉现象学，求解器有足够精度。

Conclusion: 所提下界是尖锐的，通过四个不同方程实例得到验证。

Abstract: This paper proposes sharp lower bounds for the number of message passing
iterations required in graph neural networks (GNNs) when solving partial
differential equations (PDE). This significantly reduces the need for
exhaustive hyperparameter tuning. Bounds are derived for the three fundamental
classes of PDEs (hyperbolic, parabolic and elliptic) by relating the physical
characteristics of the problem in question to the message-passing requirement
of GNNs. In particular, we investigate the relationship between the physical
constants of the equations governing the problem, the spatial and temporal
discretisation and the message passing mechanisms in GNNs.
  When the number of message passing iterations is below these proposed limits,
information does not propagate efficiently through the network, resulting in
poor solutions, even for deep GNN architectures. In contrast, when the
suggested lower bound is satisfied, the GNN parameterisation allows the model
to accurately capture the underlying phenomenology, resulting in solvers of
adequate accuracy.
  Examples are provided for four different examples of equations that show the
sharpness of the proposed lower bounds.

</details>


### [135] [Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond](https://arxiv.org/abs/2507.08866)
*Marina Ceccon,Giandomenico Cornacchia,Davide Dalle Pezze,Alessandro Fabris,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 本文研究常见数据偏差对算法歧视的影响，开发检测机制形成数据偏差概况（DBP），并通过案例证明其有效性，以数据为中心连接算法公平研究与反歧视政策。


<details>
  <summary>Details</summary>
Motivation: 数据偏差对算法歧视影响大但研究不足，阻碍检测和缓解偏差的计算最佳实践发展。

Method: 研究三种常见数据偏差在不同数据集、模型和公平性指标下对算法歧视的单独和联合影响，开发检测特定偏差的机制并组成DBP，进行案例研究。

Result: 发现训练集中弱势群体代表性不足对歧视的影响小于传统认知，代理和标签偏差组合更关键，DBP能有效预测歧视性结果风险和公平干预效用。

Conclusion: 文章以数据为中心连接了算法公平研究和反歧视政策。

Abstract: Undesirable biases encoded in the data are key drivers of algorithmic
discrimination. Their importance is widely recognized in the algorithmic
fairness literature, as well as legislation and standards on
anti-discrimination in AI. Despite this recognition, data biases remain
understudied, hindering the development of computational best practices for
their detection and mitigation. In this work, we present three common data
biases and study their individual and joint effect on algorithmic
discrimination across a variety of datasets, models, and fairness measures. We
find that underrepresentation of vulnerable populations in training sets is
less conducive to discrimination than conventionally affirmed, while
combinations of proxies and label bias can be far more critical. Consequently,
we develop dedicated mechanisms to detect specific types of bias, and combine
them into a preliminary construct we refer to as the Data Bias Profile (DBP).
This initial formulation serves as a proof of concept for how different bias
signals can be systematically documented. Through a case study with popular
fairness datasets, we demonstrate the effectiveness of the DBP in predicting
the risk of discriminatory outcomes and the utility of fairness-enhancing
interventions. Overall, this article bridges algorithmic fairness research and
anti-discrimination policy through a data-centric lens.

</details>


### [136] [GUIDE: Towards Scalable Advising for Research Ideas](https://arxiv.org/abs/2507.08870)
*Yaowenqi Liu,BingXu Meng,Rui Pan,Jerry Huang,Tong Zhang*

Main category: cs.LG

TL;DR: AI研究发展快，但可扩展建议系统有缺口。研究关键因素发现小模型搭配特定配置表现佳，高置信预测接受率超90%，代码已开源。


<details>
  <summary>Details</summary>
Motivation: AI研究虽进展快，但缺乏能提供高质量反馈以完善假设和实验设计的可扩展建议系统。

Method: 探索影响强大建议系统开发的关键因素，如模型大小、上下文长度、置信估计和结构化推理过程。

Result: 相对小的模型搭配压缩文献数据库和结构化推理框架，在ICLR 2025自排名前30%提交的接受率上超强大通用语言模型；高置信预测在ICLR 2025测试集接受率超90%。

Conclusion: 该系统有潜力显著提高假设生成和实验设计的质量与效率。

Abstract: The field of AI research is advancing at an unprecedented pace, enabling
automated hypothesis generation and experimental design across diverse domains
such as biology, mathematics, and artificial intelligence. Despite these
advancements, there remains a significant gap in the availability of scalable
advising systems capable of providing high-quality, well-reasoned feedback to
refine proposed hypotheses and experimental designs. To address this challenge,
we explore key factors that underlie the development of robust advising
systems, including model size, context length, confidence estimation, and
structured reasoning processes. Our findings reveal that a relatively small
model, when equipped with a well-compressed literature database and a
structured reasoning framework, can outperform powerful general-purpose
language models such as Deepseek-R1 in terms of acceptance rates for
self-ranked top-30% submissions to ICLR 2025. Moreover, when limited to
high-confidence predictions, our system achieves an acceptance rate exceeding
90% on the ICLR 2025 test set, underscoring its potential to significantly
enhance the quality and efficiency of hypothesis generation and experimental
design. The code is released at
https://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.

</details>


### [137] [Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation](https://arxiv.org/abs/2507.10160)
*Manuel Röder,Christoph Raab,Frank-Michael Schleif*

Main category: cs.LG

TL;DR: 本文扩展了联邦学习框架FedAcross+，解决工业环境中联邦学习面临的挑战，实验证明其在低资源设备上有效。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在实际应用中面临的数据标注需人工参与、协变量偏移和资源受限环境下模型更新不实际等问题。

Method: 扩展适用于工业环境的联邦学习框架，冻结骨干网络和分类器，用域自适应线性层处理目标域适应，还将其扩展到处理流数据。

Result: 实验表明FedAcross+能在低资源客户端设备上利用有限目标样本实现有竞争力的适应，解决域偏移问题，且能适应资源受限环境中的零星模型更新。

Conclusion: FedAcross+框架有效且实用，可在资源受限环境中无缝部署。

Abstract: Federated Learning has emerged as a leading paradigm for decentralized,
privacy-preserving learning, particularly relevant in the era of interconnected
edge devices equipped with sensors. However, the practical implementation of
Federated Learning faces three primary challenges: the need for human
involvement in costly data labelling processes for target adaptation, covariate
shift in client device data collection due to environmental factors affecting
sensors, leading to discrepancies between source and target samples, and the
impracticality of continuous or regular model updates in resource-constrained
environments due to limited data transmission capabilities and technical
constraints on channel availability and energy efficiency. To tackle these
issues, we expand upon an efficient and scalable Federated Learning framework
tailored for real-world client adaptation in industrial settings. This
framework leverages a pre-trained source model comprising a deep backbone, an
adaptation module, and a classifier running on a powerful server. By freezing
the backbone and classifier during client adaptation on resource-constrained
devices, we allow the domain adaptive linear layer to handle target domain
adaptation, thus minimizing overall computational overhead. Furthermore, this
setup, designated as FedAcross+, is extended to encompass the processing of
streaming data, thereby rendering the solution suitable for non-stationary
environments. Extensive experimental results demonstrate the effectiveness of
FedAcross+ in achieving competitive adaptation on low-end client devices with
limited target samples, successfully addressing the challenge of domain shift.
Moreover, our framework accommodates sporadic model updates within
resource-constrained environments, ensuring practical and seamless deployment.

</details>


### [138] [Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination](https://arxiv.org/abs/2507.08871)
*Xishun Liao,Haoxuan Ma,Yifan Liu,Yuxiang Wei,Brian Yueshuai He,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: 本文提出基于学习的出行需求建模框架，整合多模块，在洛杉矶实施验证，模型能复现出行模式，成本低且可扩展性强。


<details>
  <summary>Details</summary>
Motivation: 传统基于活动的出行需求模型依赖简化规则假设，开发成本高且难跨区域应用，需新的建模方法。

Method: 提出基于学习的出行需求建模框架，整合人口合成、活动生成、位置分配和微观交通仿真为统一系统。

Result: 在洛杉矶实施全流程，模型复现真实出行模式，与传统模型性能相当，成本降低、可扩展性增强，多项指标表现良好。

Conclusion: 该框架是生成式、数据驱动的，可扩展且能跨区域应用，有实际应用价值。

Abstract: Travel demand models are critical tools for planning, policy, and mobility
system design. Traditional activity-based models (ABMs), although grounded in
behavioral theories, often rely on simplified rules and assumptions, and are
costly to develop and difficult to adapt across different regions. This paper
presents a learning-based travel demand modeling framework that synthesizes
household-coordinated daily activity patterns based on a household's
socio-demographic profiles. The whole framework integrates population
synthesis, coordinated activity generation, location assignment, and
large-scale microscopic traffic simulation into a unified system. It is fully
generative, data-driven, scalable, and transferable to other regions. A
full-pipeline implementation is conducted in Los Angeles with a 10 million
population. Comprehensive validation shows that the model closely replicates
real-world mobility patterns and matches the performance of legacy ABMs with
significantly reduced modeling cost and greater scalability. With respect to
the SCAG ABM benchmark, the origin-destination matrix achieves a cosine
similarity of 0.97, and the daily vehicle miles traveled (VMT) in the network
yields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute
percentage error (MAPE). When compared to real-world observations from Caltrans
PeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001
JSD and a 6.11% MAPE.

</details>


### [139] [Convergence of Agnostic Federated Averaging](https://arxiv.org/abs/2507.10325)
*Herlock,Rahimi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Federated learning (FL) enables decentralized model training without
centralizing raw data. However, practical FL deployments often face a key
realistic challenge: Clients participate intermittently in server aggregation
and with unknown, possibly biased participation probabilities. Most existing
convergence results either assume full-device participation, or rely on
knowledge of (in fact uniform) client availability distributions -- assumptions
that rarely hold in practice. In this work, we characterize the optimization
problem that consistently adheres to the stochastic dynamics of the well-known
\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and
variably-sized) client availability, and rigorously establish its convergence
for convex, possibly nonsmooth losses, achieving a standard rate of order
$\mathcal{O}(1/\sqrt{T})$, where $T$ denotes the aggregation horizon. Our
analysis provides the first convergence guarantees for agnostic FedAvg under
general, non-uniform, stochastic client participation, without knowledge of the
participation distribution. We also empirically demonstrate that agnostic
FedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg
variants, even with server-side knowledge of participation weights.

</details>


### [140] [Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization](https://arxiv.org/abs/2507.08873)
*Shaoran Yang,Dongyu Wei,Hanzhi Yu,Zhaohui Yang,Yuchen Liu,Mingzhe Chen*

Main category: cs.LG

TL;DR: 设计基于CLIP模型的语义通信框架，用PPO算法优化，仿真显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 标准神经网络语义编解码器需联合训练，且语义信息受无线噪声影响、频谱有限，需提升语义通信性能。

Method: 设计基于CLIP模型的语义通信框架，用PPO强化学习算法优化CLIP模型架构和频谱资源块分配。

Result: 所提方法相比软演员评论家算法，收敛率最高提升40%，累积奖励提升4倍。

Conclusion: 基于CLIP模型的语义通信框架结合PPO算法能有效提升语义通信性能。

Abstract: In this paper, a novel contrastive language-image pre-training (CLIP) model
based semantic communication framework is designed. Compared to standard neural
network (e.g.,convolutional neural network) based semantic encoders and
decoders that require joint training over a common dataset, our CLIP model
based method does not require any training procedures thus enabling a
transmitter to extract data meanings of the original data without neural
network model training, and the receiver to train a neural network for
follow-up task implementation without the communications with the transmitter.
Next, we investigate the deployment of the CLIP model based semantic framework
over a noisy wireless network. Since the semantic information generated by the
CLIP model is susceptible to wireless noise and the spectrum used for semantic
information transmission is limited, it is necessary to jointly optimize CLIP
model architecture and spectrum resource block (RB) allocation to maximize
semantic communication performance while considering wireless noise, the delay
and energy used for semantic communication. To achieve this goal, we use a
proximal policy optimization (PPO) based reinforcement learning (RL) algorithm
to learn how wireless noise affect the semantic communication performance thus
finding optimal CLIP model and RB for each user. Simulation results show that
our proposed method improves the convergence rate by up to 40%, and the
accumulated reward by 4x compared to soft actor-critic.

</details>


### [141] [Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series](https://arxiv.org/abs/2507.09439)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.LG

TL;DR: 提出DyCAST - Net用于多变量时间序列因果发现，在金融和营销数据集上表现优于现有模型，能减少错误发现、提供可解释见解。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列因果关系理解对金融和营销等领域决策至关重要，传统分析方法面临复杂依赖和滞后效应挑战。

Method: 引入DyCAST - Net，结合扩张时间卷积和动态稀疏注意力机制，采用自适应阈值策略消除虚假连接，用统计洗牌测试验证过滤误报。

Result: 在金融和营销数据集上，DyCAST - Net优于TCDF、GCFormer和CausalFormer等现有模型，能更精确估计因果延迟、减少错误发现，注意力热图可揭示隐藏因果模式。

Conclusion: DyCAST - Net架构经RMSNorm稳定化和因果掩码增强，在高维动态环境中有效，具有可扩展性和适应性。

Abstract: Understanding causal relationships in multivariate time series (MTS) is
essential for effective decision-making in fields such as finance and
marketing, where complex dependencies and lagged effects challenge conventional
analytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal
Networks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel
architecture designed to enhance causal discovery by integrating dilated
temporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net
effectively captures multiscale temporal dependencies through dilated
convolutions while leveraging an adaptive thresholding strategy in its
attention mechanism to eliminate spurious connections, ensuring both accuracy
and interpretability. A statistical shuffle test validation further strengthens
robustness by filtering false positives and improving causal inference
reliability. Extensive evaluations on financial and marketing datasets
demonstrate that DyCAST-Net consistently outperforms existing models such as
TCDF, GCFormer, and CausalFormer. The model provides a more precise estimation
of causal delays and significantly reduces false discoveries, particularly in
noisy environments. Moreover, attention heatmaps offer interpretable insights,
uncovering hidden causal patterns such as the mediated effects of advertising
on consumer behavior and the influence of macroeconomic indicators on financial
markets. Case studies illustrate DyCAST-Net's ability to detect latent
mediators and lagged causal factors, making it particularly effective in
high-dimensional, dynamic settings. The model's architecture enhanced by
RMSNorm stabilization and causal masking ensures scalability and adaptability
across diverse application domains

</details>


### [142] [An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework](https://arxiv.org/abs/2507.08874)
*Yulin Sun,Xiaopeng Si,Runnan He,Xiao Hu,Peter Smielewski,Wenlong Wang,Xiaoguang Tong,Wei Yue,Meijun Pang,Kuo Zhang,Xizi Song,Dong Ming,Xiuyun Liu*

Main category: cs.LG

TL;DR: 研究开发卷积神经网络模型VIPEEGNet用于脑电活动识别，验证其性能且参数少排名高。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在脑电活动识别中因评估者差异、资源限制和泛化性差而应用受限，需更好的模型用于脑疾病诊断和治疗。

Method: 使用马萨诸塞综合医院/哈佛医学院记录的脑电数据，用两个独立数据集开发和验证VIPEEGNet模型。

Result: 在开发队列中，VIPEEGNet二分类的AUROC高，多分类的敏感性和精确性较好，外部验证的KLD排名靠前，且使用参数少。

Conclusion: VIPEEGNet模型在脑电活动识别中表现良好，有应用潜力。

Abstract: Timely identification of harmful brain activities via electroencephalography
(EEG) is critical for brain disease diagnosis and treatment, which remains
limited application due to inter-rater variability, resource constraints, and
poor generalizability of existing artificial intelligence (AI) models. In this
study, a convolutional neural network model, VIPEEGNet, was developed and
validated using EEGs recorded from Massachusetts General Hospital/Harvard
Medical School. The VIPEEGNet was developed and validated using two independent
datasets, collected between 2006 and 2020. The development cohort included EEG
recordings from 1950 patients, with 106,800 EEG segments annotated by at least
one experts (ranging from 1 to 28). The online testing cohort consisted of EEG
segments from a subset of an additional 1,532 patients, each annotated by at
least 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved
high accuracy, with an AUROC for binary classification of seizure, LPD, GPD,
LRDA, GRDA, and "other" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95%
CI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959),
0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi
classification, the sensitivity of VIPEEGNET for the six categories ranges from
36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance
similar to human experts. Notably, the external validation showed
Kullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the
existing 2,767 competing algorithms, while we only used 2.8% of the parameters
of the first-ranked algorithm.

</details>


### [143] [Generative Cognitive Diagnosis](https://arxiv.org/abs/2507.09831)
*Jiatong Li,Qi Liu,Mengxiao Zhu*

Main category: cs.LG

TL;DR: 本文提出生成式诊断范式用于认知诊断，提出G - IRT和G - NCDM模型，在真实数据集实验有效，尤其在新学习者诊断上有100倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统认知诊断模型采用转导预测范式，在新学习者即时诊断和诊断输出可靠性上有局限。

Method: 引入生成式诊断范式，提出G - IRT和G - NCDM模型，通过设计生成过程分离认知状态推断和响应预测。

Result: 在真实数据集上实验有效，新学习者诊断有100倍加速。

Conclusion: 该框架为人工智能中认知诊断应用开辟新途径，尤其在智能模型评估和智能教育系统方面。

Abstract: Cognitive diagnosis (CD) models latent cognitive states of human learners by
analyzing their response patterns on diagnostic tests, serving as a crucial
machine learning technique for educational assessment and evaluation.
Traditional cognitive diagnosis models typically follow a transductive
prediction paradigm that optimizes parameters to fit response scores and
extract learner abilities. These approaches face significant limitations as
they cannot perform instant diagnosis for new learners without computationally
expensive retraining and produce diagnostic outputs with limited reliability.
In this study, we introduces a novel generative diagnosis paradigm that
fundamentally shifts CD from predictive to generative modeling, enabling
inductive inference of cognitive states without parameter re-optimization. We
propose two simple yet effective instantiations of this paradigm: Generative
Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model
(G-NCDM), which achieve excellent performance improvements over traditional
methods. The generative approach disentangles cognitive state inference from
response prediction through a well-designed generation process that
incorporates identifiability and monotonicity conditions. Extensive experiments
on real-world datasets demonstrate the effectiveness of our methodology in
addressing scalability and reliability challenges, especially $\times 100$
speedup for the diagnosis of new learners. Our framework opens new avenues for
cognitive diagnosis applications in artificial intelligence, particularly for
intelligent model evaluation and intelligent education systems. The code is
available at https://github.com/CSLiJT/Generative-CD.git.

</details>


### [144] [ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling](https://arxiv.org/abs/2507.08877)
*Hanlong Zhang,Jingsheng Yang,Hao Li,Yuhao He,Franck Gong*

Main category: cs.LG

TL;DR: 本文提出ODIA方法加速函数调用，减少响应延迟并保持准确性，在音乐应用部署有效。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型函数调用高延迟影响用户体验的问题。

Method: 利用在线用户交互数据，从生产流量中识别简单查询，将大模型知识蒸馏到小模型。

Result: 预期减少45%、中位数减少78%的响应延迟，小模型在音乐应用中处理60%流量且准确性损失可忽略。

Conclusion: ODIA方法所需人工干预少，可通过自动数据收集和模型更新持续改进，适用于生产环境。

Abstract: Function Calling is a crucial technique that enables Large Language Models
(LLMs) to interact with external systems through APIs. However, the high
latency associated with LLM-based Function Calling significantly impacts user
experience. This paper presents a novel approach called Oriented Distillation
for Inline Acceleration (ODIA) that leverages online user interaction data to
accelerate Function Calling. By automatically identifying "simple queries" from
production traffic and distilling knowledge from larger models to smaller ones,
our method reduces response latency by 45% (expected) and 78% (median) while
maintaining accuracy. We demonstrate the effectiveness of our approach through
real-world deployment in a music application, where the smaller model
successfully handles 60% of traffic with negligible accuracy loss. Our method
requires minimal human intervention and continuously improves through automated
data collection and model updating, making it a practical solution for
production environments.

</details>


### [145] [Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness](https://arxiv.org/abs/2507.08913)
*Qi He,Peiran Yu,Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: 本文重新研究无Lipschitz光滑性假设下洗牌型梯度法的收敛率，证明了多种情况下的收敛率，数值实验验证了算法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有洗牌型梯度法收敛性保证大多需Lipschitz光滑性条件，而常见机器学习模型常不满足该条件。

Method: 采用新的步长策略，在一般有界方差条件下，对非凸、强凸和非强凸情况，在随机重洗牌和任意洗牌方案下证明收敛率。

Result: 洗牌型梯度算法在更弱假设下收敛，且达到当前已知最优收敛率，数值实验验证了算法性能。

Conclusion: 提出的洗牌型梯度算法拓宽了适用性，具有实际有效性。

Abstract: Shuffling-type gradient methods are favored in practice for their simplicity
and rapid empirical performance. Despite extensive development of convergence
guarantees under various assumptions in recent years, most require the
Lipschitz smoothness condition, which is often not met in common machine
learning models. We highlight this issue with specific counterexamples. To
address this gap, we revisit the convergence rates of shuffling-type gradient
methods without assuming Lipschitz smoothness. Using our stepsize strategy, the
shuffling-type gradient algorithm not only converges under weaker assumptions
but also match the current best-known convergence rates, thereby broadening its
applicability. We prove the convergence rates for nonconvex, strongly convex,
and non-strongly convex cases, each under both random reshuffling and arbitrary
shuffling schemes, under a general bounded variance condition. Numerical
experiments further validate the performance of our shuffling-type gradient
algorithm, underscoring its practical efficacy.

</details>


### [146] [Last Layer Hamiltonian Monte Carlo](https://arxiv.org/abs/2507.08905)
*Koen Vellenga,H. Joe Steinhauer,Göran Falkman,Jonas Andersson,Anders Sjögren*

Main category: cs.LG

TL;DR: 探索用Hamiltonian Monte Carlo (HMC) 采样作为深度神经网络概率最后一层方法，提出Last layer HMC (LL - HMC)，与五种方法对比，结果显示LL - HMC有竞争力。


<details>
  <summary>Details</summary>
Motivation: HMC计算需求大限制其在大规模数据集和大DNN架构应用，需减少计算量以用于数据密集场景。

Method: 提出LL - HMC，将HMC采样限制在DNN最后一层；在三个真实视频数据集上对比LL - HMC和五种LL - PDL方法；进行五次不同随机种子的网格搜索。

Result: LL - HMC在分布内分类和OOD检测上有竞争力；额外采样最后一层参数不提升分类性能，但可提升OOD检测；多链或多起始位置无一致提升。

Conclusion: LL - HMC适用于数据密集场景，在分类和OOD检测上有较好表现。

Abstract: We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a
probabilistic last layer approach for deep neural networks (DNNs). While HMC is
widely regarded as a gold standard for uncertainty estimation, the
computational demands limit its application to large-scale datasets and large
DNN architectures. Although the predictions from the sampled DNN parameters can
be parallelized, the computational cost still scales linearly with the number
of samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the
required computations by restricting the HMC sampling to the final layer of a
DNN, making it applicable to more data-intensive scenarios with limited
computational resources. In this paper, we compare LL-HMC against five last
layer probabilistic deep learning (LL-PDL) methods across three real-world
video datasets for driver action and intention. We evaluate the in-distribution
classification performance, calibration, and out-of-distribution (OOD)
detection. Due to the stochastic nature of the probabilistic evaluations, we
performed five grid searches for different random seeds to avoid being reliant
on a single initialization for the hyperparameter configurations. The results
show that LL--HMC achieves competitive in-distribution classification and OOD
detection performance. Additional sampled last layer parameters do not improve
the classification performance, but can improve the OOD detection. Multiple
chains or starting positions did not yield consistent improvements.

</details>


### [147] [Beyond Scores: Proximal Diffusion Models](https://arxiv.org/abs/2507.08956)
*Zhenghan Fang,Mateo Díaz,Sam Buchanan,Jeremias Sulam*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Diffusion models have quickly become some of the most popular and powerful
generative models for high-dimensional data. The key insight that enabled their
development was the realization that access to the score -- the gradient of the
log-density at different noise levels -- allows for sampling from data
distributions by solving a reverse-time stochastic differential equation (SDE)
via forward discretization, and that popular denoisers allow for unbiased
estimators of this score. In this paper, we demonstrate that an alternative,
backward discretization of these SDEs, using proximal maps in place of the
score, leads to theoretical and practical benefits. We leverage recent results
in proximal matching to learn proximal operators of the log-density and, with
them, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that
$\widetilde{O}(d/\sqrt{\varepsilon})$ steps suffice for the resulting
discretization to generate an $\varepsilon$-accurate distribution w.r.t. the KL
divergence. Empirically, we show that two variants of ProxDM achieve
significantly faster convergence within just a few sampling steps compared to
conventional score-matching methods.

</details>


### [148] [Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising](https://arxiv.org/abs/2507.08912)
*Tomasz Szandala,Fatima Ezzeddine,Natalia Rusin,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: 本文提出Fair - FLIP方法解决深度伪造检测中的公平性问题，实验显示可提升公平性指标，保持检测精度。


<details>
  <summary>Details</summary>
Motivation: 人工智能生成内容恶意使用（如深度伪造）威胁公众信任，现有检测方法存在跨人口属性（如种族、性别）的偏差，需解决公平性问题。

Method: 提出Fair - FLIP后处理方法，对训练模型的最后一层输入重新加权，降低子组差异。

Result: 与基线和现有方法对比，Fair - FLIP可将公平性指标提高30%，保持基线准确率，仅降低0.25%。

Conclusion: Fair - FLIP方法能有效缓解深度伪造检测中的偏差，在保持检测能力的同时提升公平性。

Abstract: Artificial Intelligence-generated content has become increasingly popular,
yet its malicious use, particularly the deepfakes, poses a serious threat to
public trust and discourse. While deepfake detection methods achieve high
predictive performance, they often exhibit biases across demographic attributes
such as ethnicity and gender. In this work, we tackle the challenge of fair
deepfake detection, aiming to mitigate these biases while maintaining robust
detection capabilities. To this end, we propose a novel post-processing
approach, referred to as Fairness-Oriented Final Layer Input Prioritising
(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce
subgroup disparities, prioritising those with low variability while demoting
highly variable ones. Experimental results comparing Fair-FLIP to both the
baseline (without fairness-oriented de-biasing) and state-of-the-art approaches
show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining
baseline accuracy, with only a negligible reduction of 0.25%.
  Code is available on Github:
https://github.com/szandala/fair-deepfake-detection-toolbox

</details>


### [149] [Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/2507.10485)
*Brandon Shuen Yi Loke,Filippo Quadri,Gabriel Vivanco,Maximilian Casagrande,Saúl Fenollosa*

Main category: cs.LG

TL;DR: 本文复制并扩展先前研究，在监督学习中用PermutedMNIST和RotatedMNIST基准评估EWC，与L2正则化和无正则化的SGD对比，研究EWC减少灾难性遗忘效果及泛化性，强调其对神经网络终身学习的潜力。


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘是阻碍持续学习的主要挑战，EWC可用于克服此问题，本研究旨在复制和扩展先前研究，评估EWC在监督学习中的效果。

Method: 在监督学习设置中使用PermutedMNIST和RotatedMNIST基准评估EWC，与L2正则化和无正则化的SGD进行系统比较，还研究了dropout正则化和不同超参数的影响。

Result: 确认先前研究结果，EWC相比朴素训练显著减少遗忘，在新任务学习效率上略有妥协，研究了EWC在不同学习场景的泛化性。

Conclusion: EWC有潜力成为神经网络终身学习的可行解决方案。

Abstract: Catastrophic forgetting is the primary challenge that hinders continual
learning, which refers to a neural network ability to sequentially learn
multiple tasks while retaining previously acquired knowledge. Elastic Weight
Consolidation, a regularization-based approach inspired by synaptic
consolidation in biological neural systems, has been used to overcome this
problem. In this study prior research is replicated and extended by evaluating
EWC in supervised learning settings using the PermutedMNIST and RotatedMNIST
benchmarks. Through systematic comparisons with L2 regularization and
stochastic gradient descent (SGD) without regularization, we analyze how
different approaches balance knowledge retention and adaptability. Our results
confirm what was shown in previous research, showing that EWC significantly
reduces forgetting compared to naive training while slightly compromising
learning efficiency on new tasks. Moreover, we investigate the impact of
dropout regularization and varying hyperparameters, offering insights into the
generalization of EWC across diverse learning scenarios. These results
underscore EWC's potential as a viable solution for lifelong learning in neural
networks.

</details>


### [150] [Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models](https://arxiv.org/abs/2507.08965)
*Kevin Rojas,Ye He,Chieh-Hsin Lai,Yuta Takida,Yuki Mitsufuji,Molei Tao*

Main category: cs.LG

TL;DR: 本文理论分析掩码离散扩散中CFG的引导调度，发现早期高引导有害，揭示现有实现缺陷并提出新机制，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 对掩码离散扩散中的CFG进行理论分析，解释引导调度的经验观察，并解决现有CFG实现的不足。

Method: 理论分析CFG在掩码离散扩散中的引导调度，根据分析提出一种适用于任何离散扩散的新分类器自由引导机制。

Result: 早期高引导会损害生成质量，现有实现会导致不平衡过渡，新机制通过平滑分布间的传输提高样本质量，且只需一行代码更改。

Conclusion: 新的分类器自由引导机制能有效提高离散扩散的样本质量，且易于实现。

Abstract: Classifier-Free Guidance (CFG) is a widely used technique for conditional
generation and improving sample quality in continuous diffusion models, and
recent works have extended it to discrete diffusion. This paper theoretically
analyzes CFG in the context of masked discrete diffusion, focusing on the role
of guidance schedules. Our analysis shows that high guidance early in sampling
(when inputs are heavily masked) harms generation quality, while late-stage
guidance has a larger effect. These findings provide a theoretical explanation
for empirical observations in recent studies on guidance schedules. The
analysis also reveals an imperfection of the current CFG implementations. These
implementations can unintentionally cause imbalanced transitions, such as
unmasking too rapidly during the early stages of generation, which degrades the
quality of the resulting samples. To address this, we draw insight from the
analysis and propose a novel classifier-free guidance mechanism empirically
applicable to any discrete diffusion. Intuitively, our method smoothens the
transport between the data distribution and the initial (masked/uniform)
distribution, which results in improved sample quality. Remarkably, our method
is achievable via a simple one-line code change. The efficacy of our method is
empirically demonstrated with experiments on ImageNet (masked discrete
diffusion) and QM9 (uniform discrete diffusion).

</details>


### [151] [Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery](https://arxiv.org/abs/2507.08977)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Marisa Eisenberg*

Main category: cs.LG

TL;DR: 介绍Simulation - Grounded Neural Networks (SGNNs)框架，结合机理模拟与神经网络，在多学科和任务中取得SOTA成果，具备新的可解释性，统一科学理论与深度学习灵活性。


<details>
  <summary>Details</summary>
Motivation: 解决科学建模中机理模型在现实复杂场景易失效，机器学习模型依赖大量标注数据、无法推断不可观测变量且为黑箱的问题。

Method: 以机理模拟数据作为神经网络的训练数据，在涵盖多样模型结构、参数体系、随机性和观测伪像的合成语料库上进行预训练。

Result: 在预测任务中，如新冠预测、化学产率预测、生态预测表现优异；在推理任务中，能准确分类信息传播源和对不可观测目标进行监督学习；具备新的可解释性。

Conclusion: SGNNs统一科学理论与深度学习灵活性，将模拟从僵化的事后工具转变为灵活的监督源，即使缺少真实标签也能进行可靠、可解释的推理。

Abstract: Scientific modeling faces a core limitation: mechanistic models offer
interpretability but collapse under real-world complexity, while machine
learning models are flexible but require large labeled datasets, cannot infer
unobservable quantities, and operate as black boxes. We introduce
Simulation-Grounded Neural Networks (SGNNs), a general framework that uses
mechanistic simulations as training data for neural networks. SGNNs are
pretrained on synthetic corpora spanning diverse model structures, parameter
regimes, stochasticity, and observational artifacts. We evaluated SGNNs across
scientific disciplines and modeling tasks, and found that SGNNs achieved
state-of-the-art results across settings: for prediction tasks, they nearly
tripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield
prediction error by one third, and maintained accuracy in ecological
forecasting where task specific models failed. For inference tasks, SGNNs also
accurately classified the source of information spread in simulated social
networks and enabled supervised learning for unobservable targets, such as
estimating COVID-19 transmissibility more accurately than traditional methods
even in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,
a new form of mechanistic interpretability. Given real world input, SGNNs
retrieve simulations based on what the model has learned to see as most
similar, revealing which underlying dynamics the model believes are active.
This provides process-level insight -- what the model thinks is happening --
not just which features mattered. SGNNs unify scientific theory with deep
learning flexibility and unlock a new modeling paradigm -- transforming
simulations from rigid, post hoc tools into flexible sources of supervision,
enabling robust, interpretable inference even when ground truth is missing.

</details>


### [152] [Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign](https://arxiv.org/abs/2507.08959)
*Xiang Li,Xinyu Wang,Yifan Lin*

Main category: cs.LG

TL;DR: 提出基于图神经网络的跨平台广告推荐方法，实验表明该方法在平台B表现最佳，调整超参数可提升模型适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 提高跨平台广告推荐的准确性。

Method: 通过多维度建模，利用图神经网络捕捉用户跨平台兴趣迁移的潜在路径。

Result: 基于三个平台数据集实验，平台B的AUC值达0.937，表现最佳；平台A和C因广告标签分布不均，精度和召回率略有下降。

Conclusion: 调整学习率、批量大小和嵌入维度等超参数，可进一步提高模型在异构数据中的适应性和鲁棒性。

Abstract: In order to improve the accuracy of cross-platform advertisement
recommendation, a graph neural network (GNN)- based advertisement
recommendation method is analyzed. Through multi-dimensional modeling, user
behavior data (e.g., click frequency, active duration) reveal temporal patterns
of interest evolution, ad content (e.g., type, tag, duration) influences
semantic preferences, and platform features (e.g., device type, usage context)
shape the environment where interest transitions occur. These factors jointly
enable the GNN to capture the latent pathways of user interest migration across
platforms. The experimental results are based on the datasets of three
platforms, and Platform B reaches 0.937 in AUC value, which is the best
performance. Platform A and Platform C showed a slight decrease in precision
and recall with uneven distribution of ad labels. By adjusting the
hyperparameters such as learning rate, batch size and embedding dimension, the
adaptability and robustness of the model in heterogeneous data are further
improved.

</details>


### [153] [Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation](https://arxiv.org/abs/2507.09043)
*Jingxiang Qu,Wenhan Gao,Yi Liu*

Main category: cs.LG

TL;DR: 提出理论框架提升Gaussian-based Probabilistic Generative Models生成效率，不牺牲训练粒度和推理保真度，多模态数据实证效果好。


<details>
  <summary>Details</summary>
Motivation: GPGMs因长生成轨迹计算成本高，实际部署受限，需提升生成效率。

Method: 分析确定数据获得足够高斯性的特征步骤，用闭式高斯近似替换剩余生成轨迹，避免冗余随机扰动。

Result: 多模态数据实证显示样本质量和计算效率均有显著提升。

Conclusion: 所提框架能在不牺牲训练粒度和推理保真度的情况下，有效提升GPGMs的生成效率。

Abstract: Gaussian-based Probabilistic Generative Models (GPGMs) generate data by
reversing a stochastic process that progressively corrupts samples with
Gaussian noise. While these models have achieved state-of-the-art performance
across diverse domains, their practical deployment remains constrained by the
high computational cost of long generative trajectories, which often involve
hundreds to thousands of steps during training and sampling. In this work, we
introduce a theoretically grounded and empirically validated framework that
improves generation efficiency without sacrificing training granularity or
inference fidelity. Our key insight is that for certain data modalities, the
noising process causes data to rapidly lose its identity and converge toward a
Gaussian distribution. We analytically identify a characteristic step at which
the data has acquired sufficient Gaussianity, and then replace the remaining
generation trajectory with a closed-form Gaussian approximation. Unlike
existing acceleration techniques that coarsening the trajectories by skipping
steps, our method preserves the full resolution of learning dynamics while
avoiding redundant stochastic perturbations between `Gaussian-like'
distributions. Empirical results across multiple data modalities demonstrate
substantial improvements in both sample quality and computational efficiency.

</details>


### [154] [Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction](https://arxiv.org/abs/2507.09061)
*Thomas T. Zhang,Daniel Pfrommer,Nikolai Matni,Max Simchowitz*

Main category: cs.LG

TL;DR: 研究连续状态与动作动力系统中模仿专家示范问题，提出最小干预措施减轻复合误差。


<details>
  <summary>Details</summary>
Motivation: 离散环境模仿学习成功，物理环境因复合误差问题更复杂，仅从专家轨迹学习不可避免指数复合误差，需更先进策略参数化或数据增强。

Method: 系统开环稳定时用“动作分块”，可能不稳定时用“噪声注入”。

Result: 所提干预措施与现代机器人学习流行选择相符，但益处不同，分析借鉴控制理论和强化学习，有新发现。

Conclusion: 提出的最小干预措施可减轻连续状态与动作模仿学习中的复合误差。

Abstract: We study the problem of imitating an expert demonstrator in a continuous
state-and-action dynamical system. While imitation learning in discrete
settings such as autoregressive language modeling has seen immense success and
popularity in recent years, imitation in physical settings such as autonomous
driving and robot learning has proven comparably more complex due to the
compounding errors problem, often requiring elaborate set-ups to perform
stably. Recent work has demonstrated that even in benign settings, exponential
compounding errors are unavoidable when learning solely from expert-controlled
trajectories, suggesting the need for more advanced policy parameterizations or
data augmentation. To this end, we present minimal interventions that provably
mitigate compounding errors in continuous state-and-action imitation learning.
When the system is open-loop stable, we prescribe "action chunking," i.e.,
predicting and playing sequences of actions in open-loop; when the system is
possibly unstable, we prescribe "noise injection," i.e., adding noise during
expert demonstrations. These interventions align with popular choices in modern
robot learning, though the benefits we derive are distinct from the effects
they were designed to target. Our results draw insights and tools from both
control theory and reinforcement learning; however, our analysis reveals novel
considerations that do not naturally arise when either literature is considered
in isolation.

</details>


### [155] [ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha](https://arxiv.org/abs/2507.08966)
*Meng Liu,Karl Leswing,Simon K. S. Chu,Farhad Ramezanghorbani,Griffin Young,Gabriel Marques,Prerna Das,Anjali Panikar,Esther Jamir,Mohammed Sulaiman Shamsudeen,K. Shawn Watts,Ananya Sen,Hari Priya Devannagari,Edward B. Miller,Muyun Lihan,Howook Hwang,Janet Paulsen,Xin Yu,Kyle Gion,Timur Rvachov,Emine Kucukbenli,Saee Gopal Paliwal*

Main category: cs.LG

TL;DR: 提出ToxBench数据集用于机器学习开发，以预测蛋白 - 配体结合亲和力，还提出DualBind模型，测试显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 机器学习预测蛋白 - 配体结合亲和力受可靠数据限制，基于物理的方法计算量大，为填补两者差距开展研究。

Method: 引入ToxBench数据集，包含8770个ERα - 配体复合物结构及结合自由能；提出DualBind模型，采用双损失框架学习结合能函数。

Result: ToxBench部分数据与实验亲和力验证的均方根误差为1.75 kcal/mol；DualBind模型性能优越。

Conclusion: DualBind模型表现出色，机器学习有潜力以较低计算成本近似基于物理的方法。

Abstract: Protein-ligand binding affinity prediction is essential for drug discovery
and toxicity assessment. While machine learning (ML) promises fast and accurate
predictions, its progress is constrained by the availability of reliable data.
In contrast, physics-based methods such as absolute binding free energy
perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive
for high-throughput applications. To bridge this gap, we introduce ToxBench,
the first large-scale AB-FEP dataset designed for ML development and focused on
a single pharmaceutically critical target, Human Estrogen Receptor Alpha
(ER$\alpha$). ToxBench contains 8,770 ER$\alpha$-ligand complex structures with
binding free energies computed via AB-FEP with a subset validated against
experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping
ligand splits to assess model generalizability. Using ToxBench, we further
benchmark state-of-the-art ML methods, and notably, our proposed DualBind
model, which employs a dual-loss framework to effectively learn the binding
energy function. The benchmark results demonstrate the superior performance of
DualBind and the potential of ML to approximate AB-FEP at a fraction of the
computational cost.

</details>


### [156] [Simulating Three-dimensional Turbulence with Physics-informed Neural Networks](https://arxiv.org/abs/2507.08972)
*Sifan Wang,Shyam Sankaran,Panos Stinis,Paris Perdikaris*

Main category: cs.LG

TL;DR: 本文表明合适设计的物理信息神经网络（PINNs）可成功模拟二维和三维全湍流流动，结合算法创新克服学习混沌动力学挑战，准确重现关键流动统计量，为连续湍流建模开辟新可能。


<details>
  <summary>Details</summary>
Motivation: 湍流流体流动计算需求大，传统方法在高流速下需巨大计算资源，PINNs提供了从物理方程而非数据训练神经网络的新途径。

Method: 结合自适应网络架构、因果训练和先进优化方法等算法创新，直接从基本流体方程学习，不依赖传统计算网格和训练数据。

Result: PINNs准确重现了包括能谱、动能、涡量和雷诺应力等关键流动统计量。

Conclusion: 神经网络方程求解器可处理复杂混沌系统，为连续湍流建模超越传统计算限制开辟了新可能。

Abstract: Turbulent fluid flows are among the most computationally demanding problems
in science, requiring enormous computational resources that become prohibitive
at high flow speeds. Physics-informed neural networks (PINNs) represent a
radically different approach that trains neural networks directly from physical
equations rather than data, offering the potential for continuous, mesh-free
solutions. Here we show that appropriately designed PINNs can successfully
simulate fully turbulent flows in both two and three dimensions, directly
learning solutions to the fundamental fluid equations without traditional
computational grids or training data. Our approach combines several algorithmic
innovations including adaptive network architectures, causal training, and
advanced optimization methods to overcome the inherent challenges of learning
chaotic dynamics. Through rigorous validation on challenging turbulence
problems, we demonstrate that PINNs accurately reproduce key flow statistics
including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our
results demonstrate that neural equation solvers can handle complex chaotic
systems, opening new possibilities for continuous turbulence modeling that
transcends traditional computational limitations.

</details>


### [157] [Deep Reinforcement Learning with Gradient Eligibility Traces](https://arxiv.org/abs/2507.09087)
*Esraa Elelimy,Brett Daley,Andrew Patterson,Marlos C. Machado,Adam White,Martha White*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Achieving fast and stable off-policy learning in deep reinforcement learning
(RL) is challenging. Most existing methods rely on semi-gradient
temporal-difference (TD) methods for their simplicity and efficiency, but are
consequently susceptible to divergence. While more principled approaches like
Gradient TD (GTD) methods have strong convergence guarantees, they have rarely
been used in deep RL. Recent work introduced the Generalized Projected Bellman
Error ($\GPBE$), enabling GTD methods to work efficiently with nonlinear
function approximation. However, this work is only limited to one-step methods,
which are slow at credit assignment and require a large number of samples. In
this paper, we extend the $\GPBE$ objective to support multistep credit
assignment based on the $\lambda$-return and derive three gradient-based
methods that optimize this new objective. We provide both a forward-view
formulation compatible with experience replay and a backward-view formulation
compatible with streaming algorithms. Finally, we evaluate the proposed
algorithms and show that they outperform both PPO and StreamQ in MuJoCo and
MinAtar environments, respectively. Code available at
https://github.com/esraaelelimy/gtd\_algos

</details>


### [158] [Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA](https://arxiv.org/abs/2507.09091)
*Shayan K. Azmoodeh,Krishna Subramani,Paris Smaragdis*

Main category: cs.LG

TL;DR: 提出模型无关的隐式神经信号表示框架来解决连续时间向量值信号的低秩分解问题，扩展到连续域使分解可用于点云和不规则采样信号。


<details>
  <summary>Details</summary>
Motivation: 将低秩分解问题推广到连续时间向量值信号，并解决标准技术无法处理点云和不规则采样信号的问题。

Method: 将信号建模为连续时间随机过程，通过网络损失中的对比函数项统一PCA和ICA问题的解决方法。

Result: 实现了连续域的低秩分解，能应用于点云和不规则采样信号。

Conclusion: 提出的框架可解决连续时间信号的低秩分解问题，且适用于特殊信号。

Abstract: We generalize the low-rank decomposition problem, such as principal and
independent component analysis (PCA, ICA) for continuous-time vector-valued
signals and provide a model-agnostic implicit neural signal representation
framework to learn numerical approximations to solve the problem. Modeling
signals as continuous-time stochastic processes, we unify the approaches to
both the PCA and ICA problems in the continuous setting through a contrast
function term in the network loss, enforcing the desired statistical properties
of the source signals (decorrelation, independence) learned in the
decomposition. This extension to a continuous domain allows the application of
such decompositions to point clouds and irregularly sampled signals where
standard techniques are not applicable.

</details>


### [159] [Learning Diffusion Models with Flexible Representation Guidance](https://arxiv.org/abs/2507.08980)
*Chenyu Wang,Cai Zhou,Sharut Gupta,Zongyu Lin,Stefanie Jegelka,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 本文提出将表示引导融入扩散模型的系统框架，介绍两种增强表示对齐的策略，实验显示在多任务中有优越性能和加速训练效果。


<details>
  <summary>Details</summary>
Motivation: 已有经验表明将扩散模型的内部表示与预训练模型对齐可提高生成质量，因此希望提出系统框架将表示引导融入扩散模型。

Method: 提供去噪模型的替代分解及其训练准则，引入两种策略：一是对多模态对学习联合模型；二是设计平衡表示学习和数据生成的最优训练课程。

Result: 在图像、蛋白质序列和分子生成任务实验中表现优越，训练加速，如在ImageNet基准上比原SiT - XL训练快23.3倍，比REPA快4倍。

Conclusion: 所提出的框架和策略能有效提升扩散模型性能和加速训练，代码已开源。

Abstract: Diffusion models can be improved with additional guidance towards more
effective representations of input. Indeed, prior empirical work has already
shown that aligning internal representations of the diffusion model with those
of pre-trained models improves generation quality. In this paper, we present a
systematic framework for incorporating representation guidance into diffusion
models. We provide alternative decompositions of denoising models along with
their associated training criteria, where the decompositions determine when and
how the auxiliary representations are incorporated. Guided by our theoretical
insights, we introduce two new strategies for enhancing representation
alignment in diffusion models. First, we pair examples with target
representations either derived from themselves or arisen from different
synthetic modalities, and subsequently learn a joint model over the multimodal
pairs. Second, we design an optimal training curriculum that balances
representation learning and data generation. Our experiments across image,
protein sequence, and molecule generation tasks demonstrate superior
performance as well as accelerated training. In particular, on the
class-conditional ImageNet $256\times 256$ benchmark, our guidance results in
$23.3$ times faster training than the original SiT-XL as well as four times
speedup over the state-of-the-art method REPA. The code is available at
https://github.com/ChenyuWang-Monica/REED.

</details>


### [160] [Exploiting Leaderboards for Large-Scale Distribution of Malicious Models](https://arxiv.org/abs/2507.08983)
*Anshuman Suri,Harsh Chaudhari,Yuefeng Peng,Ali Naseh,Amir Houmansadr,Alina Oprea*

Main category: cs.LG

TL;DR: 研究模型排行榜成为攻击者大规模分发中毒模型的渠道，提出TrojanClimb框架并验证其有效性，揭示机器学习生态系统漏洞。


<details>
  <summary>Details</summary>
Motivation: 已有研究未充分探索攻击者大规模分发中毒模型的机制，聚焦模型排行榜这一可能渠道。

Method: 提出TrojanClimb框架，可在维持排行榜性能的同时注入恶意行为。

Result: 在四种不同模态中验证了TrojanClimb框架的有效性，攻击者能在嵌入有害功能的同时获得高排行榜排名。

Conclusion: 机器学习生态系统存在重大漏洞，需重新设计排行榜评估机制，采用未经验证来源的模型有安全风险。

Abstract: While poisoning attacks on machine learning models have been extensively
studied, the mechanisms by which adversaries can distribute poisoned models at
scale remain largely unexplored. In this paper, we shed light on how model
leaderboards -- ranked platforms for model discovery and evaluation -- can
serve as a powerful channel for adversaries for stealthy large-scale
distribution of poisoned models. We present TrojanClimb, a general framework
that enables injection of malicious behaviors while maintaining competitive
leaderboard performance. We demonstrate its effectiveness across four diverse
modalities: text-embedding, text-generation, text-to-speech and text-to-image,
showing that adversaries can successfully achieve high leaderboard rankings
while embedding arbitrary harmful functionalities, from backdoors to bias
injection. Our findings reveal a significant vulnerability in the machine
learning ecosystem, highlighting the urgent need to redesign leaderboard
evaluation mechanisms to detect and filter malicious (e.g., poisoned) models,
while exposing broader security implications for the machine learning community
regarding the risks of adopting models from unverified sources.

</details>


### [161] [Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography](https://arxiv.org/abs/2507.09009)
*Zhengxiao He,Huayu Li,Geng Yuan,William D. S. Killgore,Stuart F. Quan,Chen X. Chen,Ao Li*

Main category: cs.LG

TL;DR: 开发自监督深度学习模型从多模态信号提取模式，生成投影分数预测CVD风险，结合FRS改善预测性能，成果可用于临床。


<details>
  <summary>Details</summary>
Motivation: 开发能直接从多模态信号生成个体化CVD风险评分的方法，以增强风险评估和支持个性化护理。

Method: 开发自监督深度学习模型，在4398名参与者数据上训练，对比有和无CVD结果个体的嵌入生成投影分数，在1093名独立队列中进行外部验证。

Result: 投影分数在各模态呈现不同且有临床意义的模式，不同信号特征对不同CVD情况有预测价值，结合FRS提高预测性能，结果在外部队列中得到验证。

Conclusion: 提出的框架可从PSG数据生成个体化CVD风险评分，投影分数有潜力用于临床实践。

Abstract: Methods: We developed a self-supervised deep learning model that extracts
meaningful patterns from multi-modal signals (Electroencephalography (EEG),
Electrocardiography (ECG), and respiratory signals). The model was trained on
data from 4,398 participants. Projection scores were derived by contrasting
embeddings from individuals with and without CVD outcomes. External validation
was conducted in an independent cohort with 1,093 participants. The source code
is available on https://github.com/miraclehetech/sleep-ssl. Results: The
projection scores revealed distinct and clinically meaningful patterns across
modalities. ECG-derived features were predictive of both prevalent and incident
cardiac conditions, particularly CVD mortality. EEG-derived features were
predictive of incident hypertension and CVD mortality. Respiratory signals
added complementary predictive value. Combining these projection scores with
the Framingham Risk Score consistently improved predictive performance,
achieving area under the curve values ranging from 0.607 to 0.965 across
different outcomes. Findings were robustly replicated and validated in the
external testing cohort. Conclusion: Our findings demonstrate that the proposed
framework can generate individualized CVD risk scores directly from PSG data.
The resulting projection scores have the potential to be integrated into
clinical practice, enhancing risk assessment and supporting personalized care.

</details>


### [162] [A Study of Value-Aware Eigenoptions](https://arxiv.org/abs/2507.09127)
*Harshil Kotamreddy,Marlos C. Machado*

Main category: cs.LG

TL;DR: 研究本征选项在无模型强化学习中加速信用分配的作用，发现预设本征选项有帮助，在线发现可能阻碍学习，还提出非线性函数逼近下学习选项值的方法。


<details>
  <summary>Details</summary>
Motivation: 本征选项在探索方面表现良好，但在信用分配方面研究不足，研究其能否加速无模型强化学习中的信用分配。

Method: 在基于表格和像素的网格世界中评估本征选项，提出非线性函数逼近下学习选项值的方法。

Result: 预设本征选项有助于探索和信用分配，在线发现可能阻碍学习，强调终止条件对性能的影响。

Conclusion: 本征选项及更广泛的选项在强化学习中支持信用分配和探索有前景也有复杂性。

Abstract: Options, which impose an inductive bias toward temporal and hierarchical
structure, offer a powerful framework for reinforcement learning (RL). While
effective in sequential decision-making, they are often handcrafted rather than
learned. Among approaches for discovering options, eigenoptions have shown
strong performance in exploration, but their role in credit assignment remains
underexplored. In this paper, we investigate whether eigenoptions can
accelerate credit assignment in model-free RL, evaluating them in tabular and
pixel-based gridworlds. We find that pre-specified eigenoptions aid not only
exploration but also credit assignment, whereas online discovery can bias the
agent's experience too strongly and hinder learning. In the context of deep RL,
we also propose a method for learning option-values under non-linear function
approximation, highlighting the impact of termination conditions on
performance. Our findings reveal both the promise and complexity of using
eigenoptions, and options more broadly, to simultaneously support credit
assignment and exploration in reinforcement learning.

</details>


### [163] [Enhancing RLHF with Human Gaze Modeling](https://arxiv.org/abs/2507.09016)
*Karim Galliamov,Ivan Titov,Ilya Pershin*

Main category: cs.LG

TL;DR: 本文探讨利用人类注视建模增强强化学习从人类反馈（RLHF）的方法，实验表明注视信息的RLHF可加速收敛并降低计算成本，指出改进RLHF效率的方向。


<details>
  <summary>Details</summary>
Motivation: RLHF与人类偏好对齐但计算成本高，需改进方法。

Method: 探索两种利用人类注视建模增强RLHF的方法：注视感知奖励模型和基于注视的稀疏奖励在标记级的分布。

Result: 注视信息的RLHF能更快收敛，保持或略提高性能，降低策略优化的计算成本。

Conclusion: 人类注视为策略优化提供有价值且未充分利用的信号，是改进RLHF效率的有前景方向。

Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models with
human preferences but is computationally expensive. We explore two approaches
that leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models
and (2) gaze-based distribution of sparse rewards at token level. Our
experiments demonstate that gaze-informed RLHF achieves faster convergence
while maintaining or slightly improving performance, thus, reducing
computational costs during policy optimization. These results show that human
gaze provides a valuable and underused signal for policy optimization, pointing
to a promising direction for improving RLHF efficiency.

</details>


### [164] [Continual Reinforcement Learning by Planning with Online World Models](https://arxiv.org/abs/2507.09177)
*Zichen Liu,Guoji Fu,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 本文提出用在线世界模型规划解决持续强化学习中的灾难性遗忘问题，设计了FTL在线代理（OA）并在专用环境Continual Bench中验证其效果优于基于深度世界模型的方法。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习中，智能体学习新任务时会忘记之前任务的解决方法，即灾难性遗忘，这是该领域的一大障碍。

Method: 学习Follow - The - Leader浅层模型来捕捉世界动态，使用模型预测控制进行规划以解决由任意奖励函数指定的任务集，形成FTL在线代理（OA），并设计Continual Bench环境进行评估。

Result: 经验结果表明，OA能持续学习解决新任务且不忘旧技能，优于使用各种持续学习技术的基于深度世界模型的智能体。

Conclusion: 使用在线世界模型规划的方法能够有效解决持续强化学习中的灾难性遗忘问题。

Abstract: Continual reinforcement learning (CRL) refers to a naturalistic setting where
an agent needs to endlessly evolve, by trial and error, to solve multiple tasks
that are presented sequentially. One of the largest obstacles to CRL is that
the agent may forget how to solve previous tasks when learning a new task,
known as catastrophic forgetting. In this paper, we propose to address this
challenge by planning with online world models. Specifically, we learn a
Follow-The-Leader shallow model online to capture the world dynamics, in which
we plan using model predictive control to solve a set of tasks specified by any
reward functions. The online world model is immune to forgetting by
construction with a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$
under mild assumptions. The planner searches actions solely based on the latest
online model, thus forming a FTL Online Agent (OA) that updates incrementally.
To assess OA, we further design Continual Bench, a dedicated environment for
CRL, and compare with several strong baselines under the same model-planning
algorithmic framework. The empirical results show that OA learns continuously
to solve new tasks while not forgetting old skills, outperforming agents built
on deep world models with various continual learning techniques.

</details>


### [165] [Model Parallelism With Subnetwork Data Parallelism](https://arxiv.org/abs/2507.09029)
*Vaibhav Singh,Zafir Khalid,Edouard Oyallon,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出在独立工作节点训练模型的小型结构化子网络以减少内存需求，评估两种子网络构建策略，随机块丢弃技术表现更优，能减少20 - 40%内存使用且不损失性能。


<details>
  <summary>Details</summary>
Motivation: 分布式预训练大模型对单个节点内存要求高且节点内通信成本大，需新方法降低内存需求。

Method: 在独立工作节点训练模型的小型结构化子网络，评估基于参数均匀表示原则的两种子网络构建策略。

Result: 随机块丢弃技术优于宽度方向子网络构建，归因于保留有跳跃连接块的子网络梯度对齐更强，初步实验实现20 - 40%内存使用减少且无性能损失。

Conclusion: 所提方法有前景，能有效减少内存使用且不影响性能。

Abstract: Distributed pre-training of large models at scale often imposes heavy memory
demands on individual nodes and incurs significant intra-node communication
costs. We propose a novel alternative approach that reduces the memory
requirements by training small, structured subnetworks of the model on separate
workers. Unlike pipelining, our method avoids inter-node activation
communication and maintains bandwidth requirements that are comparable to or
lower than standard data parallel communication schemes based on all-reduce. We
evaluate two subnetwork construction strategies guided by the principle of
ensuring uniform representation of each parameter across the distributed
training setup. Our results show that the stochastic block dropping technique
consistently outperforms the width-wise subnetwork construction previously
explored in federated learning. We empirically attribute this superior
performance to stronger gradient alignment in subnetworks that retain blocks
having skip connections. Preliminary experiments highlight the promise of our
approach, achieving a 20-40% reduction in memory usage without any loss in
performance.

</details>


### [166] [Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling](https://arxiv.org/abs/2507.09211)
*Xinyue Liu,Xiao Peng,Shuyue Yan,Yuntian Chen,Dongxiao Zhang,Zhixiao Niu,Hui-Min Wang,Xiaogang He*

Main category: cs.LG

TL;DR: 文章提出DeepX - GAN模型模拟未见极端气候事件，应用于中东和北非地区，发现未见极端事件影响脆弱地区，未来变暖或改变其分布，强调需制定空间适应性政策。


<details>
  <summary>Details</summary>
Motivation: 观测记录无法涵盖未见极端气候事件且忽视空间依赖性会低估同步灾害风险，需更好模型捕捉罕见极端事件空间结构。

Method: 开发知识驱动的深度生成模型DeepX - GAN，定义两种未见极端类型并进行模拟分析。

Result: 未见极端事件对高脆弱性和低社会经济准备度地区影响大，未来变暖会改变其分布，出现新的暴露热点。

Conclusion: 传统灾害规划存在盲点，需制定能预见新兴风险热点的空间适应性政策。

Abstract: Observed records of climate extremes provide an incomplete picture of risk,
missing "unseen" extremes that exceed historical bounds. In parallel,
neglecting spatial dependence undervalues the risk of synchronized hazards that
amplify impacts. To address these challenges, we develop DeepX-GAN
(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial
Network), a knowledge-informed deep generative model designed to better capture
the spatial structure of rare extremes. The zero-shot generalizability of
DeepX-GAN enables simulation of unseen extremes that fall outside historical
experience yet remain statistically plausible. We define two types of unseen
extremes: "checkmate" extremes that directly hit targets, and "stalemate"
extremes that narrowly miss. These unrealized scenarios expose latent risks in
fragile systems and may reinforce a false sense of resilience if overlooked.
Near misses, in particular, can prompt either proactive adaptation or dangerous
complacency, depending on how they are interpreted. Applying DeepX-GAN to the
Middle East and North Africa (MENA), we find that these unseen extremes
disproportionately affect regions with high vulnerability and low socioeconomic
readiness, but differ in urgency and interpretation. Future warming could
expand and redistribute these unseen extremes, with emerging exposure hotspots
in Indo-Pakistan and Central Africa. This distributional shift highlights
critical blind spots in conventional hazard planning and underscores the need
to develop spatially adaptive policies that anticipate emergent risk hotspots
rather than simply extrapolating from historical patterns.

</details>


### [167] [Confounder-Free Continual Learning via Recursive Feature Normalization](https://arxiv.org/abs/2507.09031)
*Yash Shah,Camila Gonzalez,Mohammad H. Abbasi,Qingyu Zhao,Kilian M. Pohl,Ehsan Adeli*

Main category: cs.LG

TL;DR: 本文提出递归元数据归一化（R - MDN）层，用于在深度学习中消除混杂因素影响，实验证明其能促进公平预测。


<details>
  <summary>Details</summary>
Motivation: 传统模型有处理混杂因素的方法，但在持续学习中，学习对混杂因素不变的特征表示仍是重大挑战。

Method: 引入可集成到任何深度学习架构的R - MDN层，通过递归最小二乘法进行统计回归，不断更新内部模型状态。

Result: R - MDN在静态学习和持续学习不同阶段都能减少因混杂因素随时间变化导致的灾难性遗忘，促进不同人群组的公平预测。

Conclusion: R - MDN层能有效消除中间特征表示中混杂因素的影响，在持续学习场景中表现良好。

Abstract: Confounders are extraneous variables that affect both the input and the
target, resulting in spurious correlations and biased predictions. There are
recent advances in dealing with or removing confounders in traditional models,
such as metadata normalization (MDN), where the distribution of the learned
features is adjusted based on the study confounders. However, in the context of
continual learning, where a model learns continuously from new data over time
without forgetting, learning feature representations that are invariant to
confounders remains a significant challenge. To remove their influence from
intermediate feature representations, we introduce the Recursive MDN (R-MDN)
layer, which can be integrated into any deep learning architecture, including
vision transformers, and at any model stage. R-MDN performs statistical
regression via the recursive least squares algorithm to maintain and
continually update an internal model state with respect to changing
distributions of data and confounding variables. Our experiments demonstrate
that R-MDN promotes equitable predictions across population groups, both within
static learning and across different stages of continual learning, by reducing
catastrophic forgetting caused by confounder effects changing over time.

</details>


### [168] [Warm Starts Accelerate Generative Modelling](https://arxiv.org/abs/2507.09212)
*Jonas Scholz,Richard E. Turner*

Main category: cs.LG

TL;DR: 提出热启动模型加速条件生成，兼容标准生成模型，在图像修复任务表现佳。


<details>
  <summary>Details</summary>
Motivation: 迭代生成模型生成过程慢，需数百次函数评估，需加速条件生成。

Method: 引入热启动模型预测信息先验N(mu, sigma)作为更好起点，用条件归一化技巧使其兼容标准模型。

Result: 在图像修复任务中，仅11次函数评估达到与1000步DDPM基线相当结果。

Conclusion: 热启动模型有效加速条件生成，且可与其他高效采样技术结合进一步加速。

Abstract: Iterative generative models, like diffusion and flow-matching, create
high-fidelity samples by progressively refining a noise vector into data.
However, this process is notoriously slow, often requiring hundreds of function
evaluations. We introduce the warm-start model, a simple, deterministic model
that dramatically accelerates conditional generation by providing a better
starting point. Instead of starting generation from an uninformed N(0, I)
prior, our warm-start model predicts an informed prior N(mu, sigma), whose
moments are conditioned on the input context. This "warm start" substantially
reduces the distance the generative process must traverse, particularly when
the conditioning information is strongly informative. On tasks like image
inpainting, our method achieves results competitive with a 1000-step DDPM
baseline using only 11 total function evaluations (1 for the warm start, 10 for
generation). A simple conditional normalization trick makes our method
compatible with any standard generative model and sampler without modification,
allowing it to be combined with other efficient sampling techniques for further
acceleration. Our implementation is available at
https://github.com/jonas-scholz123/warm-start-model.

</details>


### [169] [Behavioral Exploration: Learning to Explore via In-Context Adaptation](https://arxiv.org/abs/2507.09041)
*Andrew Wagenmaker,Zhiyuan Zhou,Sergey Levine*

Main category: cs.LG

TL;DR: 提出行为探索方法，训练智能体在专家行为空间中进行上下文探索和适应，在模拟和真实任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有算法在快速在线探索和适应方面不足，希望赋予智能体类似人类的能力。

Method: 基于上下文学习和大规模行为克隆，利用专家演示数据集训练长上下文生成模型，根据过去观察和探索性指标预测专家动作。

Result: 在模拟的运动和操作场景以及真实世界的机器人操作任务中证明了方法的有效性。

Conclusion: 所提方法能使智能体学习到自适应的探索性行为。

Abstract: Developing autonomous agents that quickly explore an environment and adapt
their behavior online is a canonical challenge in robotics and machine
learning. While humans are able to achieve such fast online exploration and
adaptation, often acquiring new information and skills in only a handful of
interactions, existing algorithmic approaches tend to rely on random
exploration and slow, gradient-based behavior updates. How can we endow
autonomous agents with such capabilities on par with humans? Taking inspiration
from recent progress on both in-context learning and large-scale behavioral
cloning, in this work we propose behavioral exploration: training agents to
internalize what it means to explore and adapt in-context over the space of
``expert'' behaviors. To achieve this, given access to a dataset of expert
demonstrations, we train a long-context generative model to predict expert
actions conditioned on a context of past observations and a measure of how
``exploratory'' the expert's behaviors are relative to this context. This
enables the model to not only mimic the behavior of an expert, but also, by
feeding its past history of interactions into its context, to select different
expert behaviors than what have been previously selected, thereby allowing for
fast online adaptation and targeted, ``expert-like'' exploration. We
demonstrate the effectiveness of our method in both simulated locomotion and
manipulation settings, as well as on real-world robotic manipulation tasks,
illustrating its ability to learn adaptive, exploratory behavior.

</details>


### [170] [Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications](https://arxiv.org/abs/2507.09213)
*Dunsheng Huang,Dong Shen,Lei Lu,Ying Tan*

Main category: cs.LG

TL;DR: 本文介绍了一种构造性小波神经网络（WNN），通过引入新基降低计算成本，提出含频率估计器和小波基增加机制的框架，通过四个示例展示其通用性，代码将公开。


<details>
  <summary>Details</summary>
Motivation: 传统小波神经网络在构造精确小波基和计算成本方面存在挑战，限制了其应用。

Method: 分析未知非线性函数的频率，根据空间频率分量能量选择初始小波，构建包含频率估计器和小波基增加机制的框架，定义高维小波在给定精度下的时频范围。

Result: 显著提高了计算效率，通过四个示例展示了框架的广泛适用性和实用性。

Conclusion: 所提出的构造性小波神经网络框架具有较高的计算效率和广泛的适用性，具有实际应用价值。

Abstract: Wavelet neural network (WNN), which learns an unknown nonlinear mapping from
the data, has been widely used in signal processing, and time-series analysis.
However, challenges in constructing accurate wavelet bases and high
computational costs limit their application. This study introduces a
constructive WNN that selects initial bases and trains functions by introducing
new bases for predefined accuracy while reducing computational costs. For the
first time, we analyze the frequency of unknown nonlinear functions and select
appropriate initial wavelets based on their primary frequency components by
estimating the energy of the spatial frequency component. This leads to a novel
constructive framework consisting of a frequency estimator and a wavelet-basis
increase mechanism to prioritize high-energy bases, significantly improving
computational efficiency. The theoretical foundation defines the necessary
time-frequency range for high-dimensional wavelets at a given accuracy. The
framework's versatility is demonstrated through four examples: estimating
unknown static mappings from offline data, combining two offline datasets,
identifying time-varying mappings from time-series data, and capturing
nonlinear dependencies in real time-series data. These examples showcase the
framework's broad applicability and practicality. All the code will be released
at https://github.com/dshuangdd/CWNN.

</details>


### [171] [TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding](https://arxiv.org/abs/2507.09252)
*Shukai Gong,Yiyang Fu,Fengyuan Ran,Feng Zhou*

Main category: cs.LG

TL;DR: 提出TPP - SD方法加速Transformer TPP采样，实验显示有2 - 6倍提速。


<details>
  <summary>Details</summary>
Motivation: 满足强大的Transformer TPP模型快速序列采样的实际需求。

Method: 识别TPP细化算法和语言模型推测解码的结构相似性，利用小的草稿模型生成多个候选事件，由大的目标模型并行验证。

Result: TPP - SD保持与自回归采样相同的输出分布，在合成和真实数据集实验中比标准方法有2 - 6倍提速。

Conclusion: TPP - SD弥合了强大的Transformer TPP模型与快速序列采样实际需求之间的差距。

Abstract: We propose TPP-SD, a novel approach that accelerates Transformer temporal
point process (TPP) sampling by adapting speculative decoding (SD) techniques
from language models. By identifying the structural similarities between
thinning algorithms for TPPs and speculative decoding for language models, we
develop an efficient sampling framework that leverages a smaller draft model to
generate multiple candidate events, which are then verified by the larger
target model in parallel. TPP-SD maintains the same output distribution as
autoregressive sampling while achieving significant acceleration. Experiments
on both synthetic and real datasets demonstrate that our approach produces
samples from identical distributions as standard methods, but with 2-6$\times$
speedup. Our ablation studies analyze the impact of hyperparameters such as
draft length and draft model size on sampling efficiency. TPP-SD bridges the
gap between powerful Transformer TPP models and the practical need for rapid
sequence sampling.

</details>


### [172] [Queue up for takeoff: a transferable deep learning framework for flight delay prediction](https://arxiv.org/abs/2507.09084)
*Nnamdi Daniel Aghanya,Ta Duong Vu,Amaëlle Diop,Charlotte Deville,Nour Imane Kerroumi,Irene Moulitsas,Jun Li,Desmond Bisandu*

Main category: cs.LG

TL;DR: 本文提出结合队列理论与简单注意力模型的QT - SimAM方法预测航班延误，在不同数据集表现优异，可提升决策与旅客体验。


<details>
  <summary>Details</summary>
Motivation: 航班延误带来财务和运营干扰，需精准且可跨网络通用的航班延误预测模型以改善旅客体验和减少收入损失。

Method: 提出结合队列理论与简单注意力模型的QT - SimAM方法，使用美国运输统计局数据验证模型，并在EUROCONTROL数据集测试可迁移性。

Result: QT - SimAM（双向）模型在美国数据上准确率0.927、F1分数0.932；在EUROCONTROL数据集准确率0.826、F1分数0.791。

Conclusion: 提出的模型能跨不同网络高精度预测延误，有助于减少旅客焦虑和改善运营决策。

Abstract: Flight delays are a significant challenge in the aviation industry, causing
major financial and operational disruptions. To improve passenger experience
and reduce revenue loss, flight delay prediction models must be both precise
and generalizable across different networks. This paper introduces a novel
approach that combines Queue-Theory with a simple attention model, referred to
as the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from
the US Bureau of Transportation Statistics, where our proposed QT-SimAM
(Bidirectional) model outperformed existing methods with an accuracy of 0.927
and an F1 score of 0.932. To assess transferability, we tested the model on the
EUROCONTROL dataset. The results demonstrated strong performance, achieving an
accuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an
effective, end-to-end methodology for predicting flight delays. The proposed
model's ability to forecast delays with high accuracy across different networks
can help reduce passenger anxiety and improve operational decision-making

</details>


### [173] [Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation](https://arxiv.org/abs/2507.09353)
*Addison Weatherhead,Anna Goldenberg*

Main category: cs.LG

TL;DR: 提出量化和利用不确定性进行选择性插补的框架，在多EHR数据集实验表明可减少插补误差并提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据缺失值常见，医疗领域有特殊挑战，现有方法多忽视或无法估计模型不确定性，需要有对插补值的置信度度量。

Method: 引入量化和利用不确定性进行选择性插补的通用框架，聚焦模型最有信心的值。

Result: 在多个涵盖不同缺失类型的EHR数据集实验中，选择性插补低不确定性值减少了插补误差，在24小时死亡率预测任务中有性能提升。

Conclusion: 将不确定性纳入时间序列插补有实际益处。

Abstract: Time series data with missing values is common across many domains.
Healthcare presents special challenges due to prolonged periods of sensor
disconnection. In such cases, having a confidence measure for imputed values is
critical. Most existing methods either overlook model uncertainty or lack
mechanisms to estimate it. To address this gap, we introduce a general
framework that quantifies and leverages uncertainty for selective imputation.
By focusing on values the model is most confident in, highly unreliable
imputations are avoided. Our experiments on multiple EHR datasets, covering
diverse types of missingness, demonstrate that selectively imputing
less-uncertain values not only reduces imputation errors but also improves
downstream tasks. Specifically, we show performance gains in a 24-hour
mortality prediction task, underscoring the practical benefit of incorporating
uncertainty into time series imputation.

</details>


### [174] [Principled Foundations for Preference Optimization](https://arxiv.org/abs/2507.07855)
*Wenxuan Zhou,Shujian Zhang,Brice Magdalou,John Lambert,Ehsan Amid,Richard Nock,Andrew Hard*

Main category: cs.LG

TL;DR: 本文揭示直接偏好优化（DPO）是机器学习偏好学习中损失函数和随机选择两大理论联系的特殊形式，还说明了理解DPO原理的重要性。


<details>
  <summary>Details</summary>
Motivation: 鉴于模型应用广泛、DPO发展势头正盛以及现有DPO变体覆盖范围小，需要从通用原则视角理解DPO的运作。

Method: 建立Savage损失函数和随机选择理论之间的联系，涵盖弃权支持、非凸目标支持等。

Result: 建立了DPO与两大理论的联系，可免费构建DPO设置的显著扩展。

Conclusion: 从通用原则视角理解DPO很关键，有助于了解偏离的陷阱并找到解决办法。

Abstract: In this paper, we show that direct preference optimization (DPO) is a very
specific form of a connection between two major theories in the ML context of
learning from preferences: loss functions (Savage) and stochastic choice
(Doignon-Falmagne and Machina). The connection is established for all of
Savage's losses and at this level of generality, (i) it includes support for
abstention on the choice theory side, (ii) it includes support for non-convex
objectives on the ML side, and (iii) it allows to frame for free some notable
extensions of the DPO setting, including margins and corrections for length.
Getting to understand how DPO operates from a general principled perspective is
crucial because of the huge and diverse application landscape of models,
because of the current momentum around DPO, but also -- and importantly --
because many state of the art variations on DPO definitely occupy a small
region of the map that we cover. It also helps to understand the pitfalls of
departing from this map, and figure out workarounds.

</details>


### [175] [On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving](https://arxiv.org/abs/2507.09095)
*Md Hasan Shahriar,Md Mohaimin Al Barat,Harshavardhan Sundar,Naren Ramakrishnan,Y. Thomas Hou,Wenjing Lou*

Main category: cs.LG

TL;DR: 提出针对多模态融合的DejaVu攻击及AION防御方法，攻击可大幅降低感知任务性能，防御方法效果好。


<details>
  <summary>Details</summary>
Motivation: 多模态融合依赖精确时间同步，存在因网络延迟导致时间不对齐的漏洞，需研究攻击和防御方法。

Method: 提出DejaVu攻击利用网络延迟制造时间不对齐，分析不同模型和数据集下传感器对任务的敏感性；提出AION防御方法，通过跨模态时间一致性监测，利用多模态共享表示学习和动态时间规整计算异常分数。

Result: DejaVu攻击能使汽车检测mAP最多降低88.5%，多目标跟踪准确率下降73%；AION防御方法在不同数据集和模型架构下AUROC分数达0.92 - 0.98，误报率低。

Conclusion: AION是一种针对时间不对齐攻击的强大且通用的防御方法。

Abstract: Multimodal fusion (MMF) plays a critical role in the perception of autonomous
driving, which primarily fuses camera and LiDAR streams for a comprehensive and
efficient scene understanding. However, its strict reliance on precise temporal
synchronization exposes it to new vulnerabilities. In this paper, we introduce
DejaVu, a novel attack that exploits network-induced delays to create subtle
temporal misalignments across sensor streams, severely degrading downstream
MMF-based perception tasks. Our comprehensive attack analysis across different
models and datasets reveals these sensors' task-specific imbalanced
sensitivities: object detection is overly dependent on LiDAR inputs while
object tracking is highly reliant on the camera inputs. Consequently, with a
single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to
88.5%, while with a three-frame camera delay, multiple object tracking accuracy
(MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense
patch that can work alongside the existing perception model to monitor temporal
alignment through cross-modal temporal consistency. AION leverages multimodal
shared representation learning and dynamic time warping to determine the path
of temporal alignment and calculate anomaly scores based on the alignment. Our
thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with
low false positives across datasets and model architectures, demonstrating it
as a robust and generalized defense against the temporal misalignment attacks.

</details>


### [176] [S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe](https://arxiv.org/abs/2507.09101)
*Yanan Cao,Omid Memarrast,Shiqin Cai,Sinduja Subramaniam,Evren Korpeoglu,Kannan Achan*

Main category: cs.LG

TL;DR: 本文提出S2SRec2框架解决杂货电商篮子补全问题，实验表明其优于单目标基线。


<details>
  <summary>Details</summary>
Motivation: 传统食谱补全方法无法反映现实场景、忽略缺失成分关系，需要更好方法改善烹饪体验。

Method: 将篮子补全重新定义为集对集推荐问题，引入基于Set Transformer的S2SRec2框架，并以多任务学习范式训练。

Result: 在大规模食谱数据集实验和定性分析显示，S2SRec2显著优于单目标基线。

Conclusion: S2SRec2是提升杂货购物和激发烹饪创造力的有前景方法。

Abstract: In grocery e-commerce, customers often build ingredient baskets guided by
dietary preferences but lack the expertise to create complete meals. Leveraging
recipe knowledge to recommend complementary ingredients based on a partial
basket is essential for improving the culinary experience. Traditional recipe
completion methods typically predict a single missing ingredient using a
leave-one-out strategy. However, they fall short in two key aspects: (i) they
do not reflect real-world scenarios where multiple ingredients are often
needed, and (ii) they overlook relationships among the missing ingredients
themselves. To address these limitations, we reformulate basket completion as a
set-to-set (S2S) recommendation problem, where an incomplete basket is input
into a system that predicts a set of complementary ingredients. We introduce
S2SRec2, a set-to-set ingredient recommendation framework based on a Set
Transformer and trained in a multitask learning paradigm. S2SRec2 jointly
learns to (i) retrieve missing ingredients from the representation of existing
ones and (ii) assess basket completeness after prediction. These tasks are
optimized together, enforcing accurate retrieval and coherent basket
completion. Experiments on large-scale recipe datasets and qualitative analyses
show that S2SRec2 significantly outperforms single-target baselines, offering a
promising approach to enhance grocery shopping and inspire culinary creativity.

</details>


### [177] [Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting](https://arxiv.org/abs/2507.09445)
*Runze Yang,Longbing Cao,Xin You,Kun Fang,Jianxun Li,Jie Yang*

Main category: cs.LG

TL;DR: 本文从基函数角度重新审视傅里叶变换，提出FBM方法解决现有基于傅里叶方法的问题，支持与多种神经网络集成，还提出不同模型架构及特定技术，在多数据集上验证效果达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于傅里叶的方法存在起始周期不一致、序列长度不一致等问题，无法精确解释频率分量且忽略时间信息。

Method: 提出Fourier Basis Mapping (FBM)方法，通过傅里叶基展开和时频空间映射集成时频特征；提出FBM - L、FBM - NL、FBM - NP和FBM - S等模型架构；引入交互掩码、中心化等针对时频特征的技术。

Result: 在不同真实世界数据集的长期和短期预测任务中验证，达到SOTA性能。

Conclusion: FBM方法有效解决了现有基于傅里叶方法的问题，能与多种神经网络集成，且特定技术和模型架构提升了时频特征建模能力和预测性能。

Abstract: The integration of Fourier transform and deep learning opens new avenues for
time series forecasting. We reconsider the Fourier transform from a basis
functions perspective. Specifically, the real and imaginary parts of the
frequency components can be regarded as the coefficients of cosine and sine
basis functions at tiered frequency levels, respectively. We find that existing
Fourier-based methods face inconsistent starting cycles and inconsistent series
length issues. They fail to interpret frequency components precisely and
overlook temporal information. Accordingly, the novel Fourier Basis Mapping
(FBM) method addresses these issues by integrating time-frequency features
through Fourier basis expansion and mapping in the time-frequency space. Our
approach extracts explicit frequency features while preserving temporal
characteristics. FBM supports plug-and-play integration with various types of
neural networks by only adjusting the first initial projection layer for better
performance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear,
MLP-based, and Transformer-based models, respectively, demonstrating the
effectiveness of time-frequency features. Next, we propose a synergetic model
architecture, termed FBM-S, which decomposes the seasonal, trend, and
interaction effects into three separate blocks, each designed to model
time-frequency features in a specialized manner. Finally, we introduce several
techniques tailored for time-frequency features, including interaction masking,
centralization, patching, rolling window projection, and multi-scale
down-sampling. The results are validated on diverse real-world datasets for
both long-term and short-term forecasting tasks with SOTA performance.

</details>


### [178] [Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning](https://arxiv.org/abs/2507.09132)
*Chu-Yuan Wei,Shun-Yao Liu,Sheng-Da Zhuo,Chang-Dong Wang,Shu-Qiang Huang,Mohsen Guizani*

Main category: cs.LG

TL;DR: 本文提出GPAWP框架结合图提示与权重剪枝提升GNN性能与效率，实验显示可减少节点分类任务参数。


<details>
  <summary>Details</summary>
Motivation: GNN存在训练推理时间长等问题，此前研究忽视图提示优化模型及正负提示影响，需解决这些问题。

Method: 提出GPAWP框架，用重要性评估函数确定不同粒度正负权重，通过分层剪枝消除负提示标签。

Result: 在三个基准数据集上实验，证明GPAWP优越性，显著减少节点分类任务参数。

Conclusion: GPAWP框架能有效提升图提示性能与效率，减少参数。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in various
graph-based tasks (e.g., node classification or link prediction). Despite their
triumphs, GNNs still face challenges such as long training and inference times,
difficulty in capturing complex relationships, and insufficient feature
extraction. To tackle these issues, graph pre-training and graph prompt methods
have garnered increasing attention for their ability to leverage large-scale
datasets for initial learning and task-specific adaptation, offering potential
improvements in GNN performance. However, previous research has overlooked the
potential of graph prompts in optimizing models, as well as the impact of both
positive and negative graph prompts on model stability and efficiency. To
bridge this gap, we propose a novel framework combining graph prompts with
weight pruning, called GPAWP, which aims to enhance the performance and
efficiency of graph prompts by using fewer of them. We evaluate the importance
of graph prompts using an importance assessment function to determine positive
and negative weights at different granularities. Through hierarchically
structured pruning, we eliminate negative prompt labels, resulting in more
parameter-efficient and competitively performing prompts. Extensive experiments
on three benchmark datasets demonstrate the superiority of GPAWP, leading to a
significant reduction in parameters in node classification tasks.

</details>


### [179] [Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting](https://arxiv.org/abs/2507.09694)
*Nicolas Gonel,Paul Saves,Joseph Morlier*

Main category: cs.LG

TL;DR: 本文介绍开发相关核的开源框架，扩展核函数，验证后应用于多场景并集成到SMT 2.0，为复杂频敏领域元建模提供灵活工具。


<details>
  <summary>Details</summary>
Motivation: 推进基于核的建模技术，捕捉飞机系统复杂力学行为和时频动态，解决传统核函数局限性问题。

Method: 引入频率感知元素，扩展传统核函数，验证方法并应用于多个实际案例，集成到SMT 2.0框架，支持核组合。

Result: 框架可灵活配置核，能将不同核优势组合成定制复合模型。

Conclusion: 框架为工程师和研究人员提供灵活工具集，为复杂频敏领域元建模未来应用奠定基础。

Abstract: This paper introduces a comprehensive open-source framework for developing
correlation kernels, with a particular focus on user-defined and composition of
kernels for surrogate modeling. By advancing kernel-based modeling techniques,
we incorporate frequency-aware elements that effectively capture complex
mechanical behaviors and timefrequency dynamics intrinsic to aircraft systems.
Traditional kernel functions, often limited to exponential-based methods, are
extended to include a wider range of kernels such as exponential squared sine
and rational quadratic kernels, along with their respective firstand
second-order derivatives. The proposed methodologies are first validated on a
sinus cardinal test case and then applied to forecasting Mauna-Loa Carbon
Dioxide (CO 2 ) concentrations and airline passenger traffic. All these
advancements are integrated into the open-source Surrogate Modeling Toolbox
(SMT 2.0), providing a versatile platform for both standard and customizable
kernel configurations. Furthermore, the framework enables the combination of
various kernels to leverage their unique strengths into composite models
tailored to specific problems. The resulting framework offers a flexible
toolset for engineers and researchers, paving the way for numerous future
applications in metamodeling for complex, frequency-sensitive domains.

</details>


### [180] [POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution](https://arxiv.org/abs/2507.09137)
*Nripsuta Ani Saxena,Shang-Ling Hsu,Mehul Shetty,Omar Alkhadra,Cyrus Shahabi,Abigail L. Horn*

Main category: cs.LG

TL;DR: 提出POIFormer框架用于精确高效的POI归属，实验显示比现有基线有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有因GPS不准确和城市POI空间密度高，依靠距离确定POI归属具有挑战性，需要新方法。

Method: 提出基于Transformer的POIFormer框架，联合建模包括空间距离、访问时间和时长等信号，利用自注意力机制处理复杂交互，还结合人群行为模式。

Result: 在真实移动数据集上实验表明，相比现有基线有显著改进，尤其在有空间噪声和POI密集的场景。

Conclusion: POIFormer能在大规模、有噪声的移动数据集上实现准确高效的POI归属，支持跨数据源和地理环境泛化，适合实际部署。

Abstract: Accurately attributing user visits to specific Points of Interest (POIs) is a
foundational task for mobility analytics, personalized services, marketing and
urban planning. However, POI attribution remains challenging due to GPS
inaccuracies, typically ranging from 2 to 20 meters in real-world settings, and
the high spatial density of POIs in urban environments, where multiple venues
can coexist within a small radius (e.g., over 50 POIs within a 100-meter radius
in dense city centers). Relying on proximity is therefore often insufficient
for determining which POI was actually visited. We introduce
\textsf{POIFormer}, a novel Transformer-based framework for accurate and
efficient POI attribution. Unlike prior approaches that rely on limited
spatiotemporal, contextual, or behavioral features, \textsf{POIFormer} jointly
models a rich set of signals, including spatial proximity, visit timing and
duration, contextual features from POI semantics, and behavioral features from
user mobility and aggregated crowd behavior patterns--using the Transformer's
self-attention mechanism to jointly model complex interactions across these
dimensions. By leveraging the Transformer to model a user's past and future
visits (with the current visit masked) and incorporating crowd-level behavioral
patterns through pre-computed KDEs, \textsf{POIFormer} enables accurate,
efficient attribution in large, noisy mobility datasets. Its architecture
supports generalization across diverse data sources and geographic contexts
while avoiding reliance on hard-to-access or unavailable data layers, making it
practical for real-world deployment. Extensive experiments on real-world
mobility datasets demonstrate significant improvements over existing baselines,
particularly in challenging real-world settings characterized by spatial noise
and dense POI clustering.

</details>


### [181] [Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations](https://arxiv.org/abs/2507.09173)
*Mengjie Chen,Ming Zhang,Cunquan Qu*

Main category: cs.LG

TL;DR: 提出MolecBioNet框架用于药物相互作用预测，结合多尺度知识，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图基方法在药物相互作用预测中忽略药物对复杂依赖关系，难以整合生物网络和分子结构以提供机理解释。

Method: 提出MolecBioNet框架，将药物对建模为统一实体，提取局部子图和构建分层交互图，使用经典图神经网络学习多尺度表示，引入两种特定池化策略和互信息最小化正则化。

Result: MolecBioNet在药物相互作用预测中优于现有方法，消融实验和嵌入可视化验证了统一建模和多尺度知识整合的优势。

Conclusion: MolecBioNet能有效进行药物相互作用预测，统一药物对建模和多尺度知识整合是有效的方法。

Abstract: Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,
often leading to adverse drug reactions with significant implications for
patient safety and healthcare outcomes. While graph-based methods have achieved
strong predictive performance, most approaches treat drug pairs independently,
overlooking the complex, context-dependent interactions unique to drug pairs.
Additionally, these models struggle to integrate biological interaction
networks and molecular-level structures to provide meaningful mechanistic
insights. In this study, we propose MolecBioNet, a novel graph-based framework
that integrates molecular and biomedical knowledge for robust and interpretable
DDI prediction. By modeling drug pairs as unified entities, MolecBioNet
captures both macro-level biological interactions and micro-level molecular
influences, offering a comprehensive perspective on DDIs. The framework
extracts local subgraphs from biomedical knowledge graphs and constructs
hierarchical interaction graphs from molecular representations, leveraging
classical graph neural network methods to learn multi-scale representations of
drug pairs. To enhance accuracy and interpretability, MolecBioNet introduces
two domain-specific pooling strategies: context-aware subgraph pooling
(CASPool), which emphasizes biologically relevant entities, and
attention-guided influence pooling (AGIPool), which prioritizes influential
molecular substructures. The framework further employs mutual information
minimization regularization to enhance information diversity during embedding
fusion. Experimental results demonstrate that MolecBioNet outperforms
state-of-the-art methods in DDI prediction, while ablation studies and
embedding visualizations further validate the advantages of unified drug pair
modeling and multi-scale knowledge integration.

</details>


### [182] [Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training](https://arxiv.org/abs/2507.09846)
*Minhak Song,Beomhan Baek,Kwangjun Ahn,Chulhee Yun*

Main category: cs.LG

TL;DR: 传统预训练策略在大规模训练中不足，研究重新审视Schedule - Free (SF) 方法，分析其动态，提出改进变体，证明SF是实用、可扩展且有理论依据的语言模型训练方法。


<details>
  <summary>Details</summary>
Motivation: 传统固定计算预算的预训练策略在大规模训练中不适用，现有替代方法存在依赖显式衰减阶段或增加内存开销等问题，需要更有原则和可扩展的方法。

Method: 重新审视SF方法，对SF动态进行理论和实证分析，提出SF的改进变体。

Result: SF - AdamW能有效处理损失景观，隐式执行权重平均且无内存开销，改进变体提高了对动量的鲁棒性，在大批次大小下表现更好。

Conclusion: SF是语言模型训练实用、可扩展且有理论依据的方法。

Abstract: As both model and dataset sizes continue to scale rapidly, conventional
pretraining strategies with fixed compute budgets-such as cosine learning rate
schedules-are increasingly inadequate for large-scale training. Recent
alternatives, including warmup-stable-decay (WSD) schedules and weight
averaging, offer greater flexibility. However, WSD relies on explicit decay
phases to track progress, while weight averaging addresses this limitation at
the cost of additional memory. In search of a more principled and scalable
alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],
which has shown strong empirical performance across diverse settings. We show
that SF-AdamW effectively navigates the "river" structure of the loss landscape
without decay phases or auxiliary averaging, making it particularly suitable
for continuously scaling training workloads. To understand this behavior, we
conduct a theoretical and empirical analysis of SF dynamics, revealing that it
implicitly performs weight averaging without memory overhead. Guided by this
analysis, we propose a refined variant of SF that improves robustness to
momentum and performs better under large batch sizes, addressing key
limitations of the original method. Together, these results establish SF as a
practical, scalable, and theoretically grounded approach for language model
training.

</details>


### [183] [NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting](https://arxiv.org/abs/2507.09888)
*Huibo Xu,Likang Wu,Xianquan Wang,Haoning Dang,Chun-Wun Cheng,Angelica I Aviles-Rivero,Qi Liu*

Main category: cs.LG

TL;DR: 提出NeuTSFlow框架解决时间序列预测问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法将数据视为离散序列，忽略其连续过程本质，存在挑战待解决。

Method: 提出NeuTSFlow框架，利用神经算子进行流匹配，对无限维函数空间的流速度场参数化。

Result: 在不同预测任务实验中，NeuTSFlow展现出优越的准确性和鲁棒性。

Conclusion: 函数族视角用于时间序列预测有效，NeuTSFlow框架可行。

Abstract: Time series forecasting is a fundamental task with broad applications, yet
conventional methods often treat data as discrete sequences, overlooking their
origin as noisy samples of continuous processes. Crucially, discrete noisy
observations cannot uniquely determine a continuous function; instead, they
correspond to a family of plausible functions. Mathematically, time series can
be viewed as noisy observations of a continuous function family governed by a
shared probability measure. Thus, the forecasting task can be framed as
learning the transition from the historical function family to the future
function family. This reframing introduces two key challenges: (1) How can we
leverage discrete historical and future observations to learn the relationships
between their underlying continuous functions? (2) How can we model the
transition path in function space from the historical function family to the
future function family? To address these challenges, we propose NeuTSFlow, a
novel framework that leverages Neural Operators to facilitate flow matching for
learning path of measure between historical and future function families. By
parameterizing the velocity field of the flow in infinite-dimensional function
spaces, NeuTSFlow moves beyond traditional methods that focus on dependencies
at discrete points, directly modeling function-level features instead.
Experiments on diverse forecasting tasks demonstrate NeuTSFlow's superior
accuracy and robustness, validating the effectiveness of the function-family
perspective.

</details>


### [184] [XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge](https://arxiv.org/abs/2507.09202)
*Wuxin Wang,Weicheng Ni,Lilan Huang,Tao Hao,Ben Fei,Shuo Ma,Taikang Yuan,Yanlai Zhao,Kefeng Deng,Xiaoyong Li,Boheng Duan,Lei Bai,Kaijun Ren*

Main category: cs.LG

TL;DR: 介绍首个可扩展的全AI全球天气预报系统XiChen，能在17秒内完成从数据同化到中期预报的全流程，预报领先时间超8.25天，有替代数值天气预报系统的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的天气预报模型依赖数值天气预报系统准备初始条件，耗时久，需开发不依赖NWP系统的全AI天气预报系统。

Method: 构建基于预训练的基础模型，微调后作为观测算子和数据同化模型，融合四维变分知识。

Result: XiChen可在17秒内完成从数据同化到中期预报的全流程，预报精度与NWP系统相当，预报领先时间超8.25天。

Conclusion: XiChen在不依赖NWP系统的全AI天气预报方面有很大潜力。

Abstract: Recent advancements in Artificial Intelligence (AI) demonstrate significant
potential to revolutionize weather forecasting. However, most AI-driven models
rely on Numerical Weather Prediction (NWP) systems for initial condition
preparation, which often consumes hours on supercomputers. Here we introduce
XiChen, the first observation-scalable fully AI-driven global weather
forecasting system, whose entire pipeline, from Data Assimilation (DA) to
medium-range forecasting, can be accomplished within only 17 seconds. XiChen is
built upon a foundation model that is pre-trained for weather forecasting.
Meanwhile, this model is subsequently fine-tuned to serve as both observation
operators and DA models, thereby scalably assimilating conventional and raw
satellite observations. Furthermore, the integration of four-dimensional
variational knowledge ensures that XiChen's DA and medium-range forecasting
accuracy rivals that of operational NWP systems, amazingly achieving a skillful
forecasting lead time exceeding 8.25 days. These findings demonstrate that
XiChen holds strong potential toward fully AI-driven weather forecasting
independent of NWP systems.

</details>


### [185] [Towards High Supervised Learning Utility Training Data Generation: Data Pruning and Column Reordering](https://arxiv.org/abs/2507.10088)
*Tung Sum Thomas Kwok,Zeyong Zhang,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 提出PRRO管道优化表格数据合成的监督学习效用，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据合成用于监督学习时因类不平衡夸大和忽略数据关系致模型表现不佳，需解决此问题。

Method: 提出PRRO管道，包含数据修剪和列重排序算法，前者确保合成数据类分布接近原始数据，后者使生成器与监督学习模型的数据建模结构对齐。

Result: 在22个公开数据集上，使用PRRO生成的合成数据提升了预测性能；在6个高度不平衡数据集上，PRRO使合成数据类分布更接近原始数据，相似度提升43%。

Conclusion: PRRO促进了数据合成与后续监督学习预测的无缝集成，推动高质量且易获取的数据分析。

Abstract: Tabular data synthesis for supervised learning ('SL') model training is
gaining popularity in industries such as healthcare, finance, and retail.
Despite the progress made in tabular data generators, models trained with
synthetic data often underperform compared to those trained with original data.
This low SL utility of synthetic data stems from class imbalance exaggeration
and SL data relationship overlooked by tabular generator. To address these
challenges, we draw inspirations from techniques in emerging data-centric
artificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel
pipeline that integrates data-centric techniques into tabular data synthesis.
PRRO incorporates data pruning to guide the table generator towards
observations with high signal-to-noise ratio, ensuring that the class
distribution of synthetic data closely matches that of the original data.
Besides, PRRO employs a column reordering algorithm to align the data modeling
structure of generators with that of SL models. These two modules enable PRRO
to optimize SL utility of synthetic data. Empirical experiments on 22 public
datasets show that synthetic data generated using PRRO enhances predictive
performance compared to data generated without PRRO. Specifically, synthetic
replacement of original data yields an average improvement of 26.74% and up to
871.46% improvement using PRRO, while synthetic appendant to original data
results with PRRO-generated data results in an average improvement of 6.13% and
up to 200.32%. Furthermore, experiments on six highly imbalanced datasets show
that PRRO enables the generator to produce synthetic data with a class
distribution that resembles the original data more closely, achieving a
similarity improvement of 43%. Through PRRO, we foster a seamless integration
of data synthesis to subsequent SL prediction, promoting quality and accessible
data analysis.

</details>


### [186] [Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting](https://arxiv.org/abs/2507.10132)
*Usman Gani Joy*

Main category: cs.LG

TL;DR: 本文提出一种神经框架用于能源供需预测，在多数据集上表现优于基线模型，还增强了解释性。


<details>
  <summary>Details</summary>
Motivation: 准确预测能源供需对优化可持续能源系统至关重要，但受可再生能源可变性和动态消费模式挑战，需更好的时间序列预测方法。

Method: 引入集成连续时间神经常微分方程、图注意力、多分辨率小波变换和自适应频率学习的神经框架，用龙格 - 库塔法求解 ODE，结合图注意力和残差连接，进行小波特征提取和自适应频率调制。

Result: 在七个不同数据集上，该架构在各种预测指标上始终优于现有基线模型。

Conclusion: 该模型能有效捕捉复杂时间依赖关系，通过 SHAP 分析增强了解释性，适用于可持续能源应用。

Abstract: Accurate forecasting of energy demand and supply is critical for optimizing
sustainable energy systems, yet it is challenged by the variability of
renewable sources and dynamic consumption patterns. This paper introduces a
neural framework that integrates continuous-time Neural Ordinary Differential
Equations (Neural ODEs), graph attention, multi-resolution wavelet
transformations, and adaptive learning of frequencies to address the issues of
time series prediction. The model employs a robust ODE solver, using the
Runge-Kutta method, paired with graph-based attention and residual connections
to better understand both structural and temporal patterns. Through
wavelet-based feature extraction and adaptive frequency modulation, it adeptly
captures and models diverse, multi-scale temporal dynamics. When evaluated
across seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity
transformer temperature), and Waste, Solar, and Hydro (renewable energy), this
architecture consistently outperforms state-of-the-art baselines in various
forecasting metrics, proving its robustness in capturing complex temporal
dependencies. Furthermore, the model enhances interpretability through SHAP
analysis, making it suitable for sustainable energy applications.

</details>


### [187] [Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations](https://arxiv.org/abs/2507.09264)
*Payel Mukhopadhyay,Michael McCabe,Ruben Ohana,Miles Cranmer*

Main category: cs.LG

TL;DR: 引入轻量级模块实现基于补丁的PDE替代模型推理时动态补丁大小控制，提高性能和效率。


<details>
  <summary>Details</summary>
Motivation: 基于补丁的变压器替代模型固定补丁大小限制了生产中的预算部署。

Method: 引入卷积核调制器（CKM）和卷积步长调制器（CSM）两个轻量级、架构无关模块，结合循环补丁大小滚动策略。

Result: 应用于2D和3D PDE基准测试，提高了滚动保真度和运行时效率。

Conclusion: 这是首个实现推理时补丁大小可调性的框架，具有广泛适用性，为PDE替代任务的计算自适应建模奠定基础。

Abstract: Patch-based transformer surrogates have become increasingly effective for
modeling spatiotemporal dynamics, but the fixed patch size is a major
limitation for budget-conscience deployment in production. We introduce two
lightweight, architecture-agnostic modules-the Convolutional Kernel Modulator
(CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size
control at inference in patch based models, without retraining or accuracy
loss. Combined with a cyclic patch-size rollout, our method mitigates patch
artifacts and improves long-term stability for video-like prediction tasks.
Applied to a range of challenging 2D and 3D PDE benchmarks, our approach
improves rollout fidelity and runtime efficiency. To our knowledge, this is the
first framework to enable inference-time patch-size tunability in patch-based
PDE surrogates. Its plug-and-play design makes it broadly applicable across
architectures-establishing a general foundation for compute-adaptive modeling
in PDE surrogate tasks.

</details>


### [188] [Multiple Choice Learning of Low Rank Adapters for Language Modeling](https://arxiv.org/abs/2507.10419)
*Victor Letzelter,Hugo Malard,Mathieu Fontaine,Gaël Richard,Slim Essid,Andrei Bursuc,Patrick Pérez*

Main category: cs.LG

TL;DR: 提出LoRA - MCL训练方案，可在推理时解码多样合理的句子续写，在视觉和音频字幕任务实验中证明其生成输出具高多样性和相关性。


<details>
  <summary>Details</summary>
Motivation: 传统语言建模是病态问题，给定上下文，多种未来续写都可能合理，需处理这种模糊性。

Method: 利用Multiple Choice Learning (MCL)和Winner - Takes - All (WTA)损失，通过Low - Rank Adaptation (LoRA)处理模糊性，对将MCL应用于语言建模给出理论解释，并使用马尔可夫链混合采样数据。

Result: 在真实世界的视觉和音频字幕任务的大量实验中，方法生成的输出具有高多样性和相关性。

Conclusion: LoRA - MCL训练方案能有效处理语言建模中的模糊性，生成高质量输出。

Abstract: We propose LoRA-MCL, a training scheme that extends next-token prediction in
language models with a method designed to decode diverse, plausible sentence
continuations at inference time. Traditional language modeling is an
intrinsically ill-posed problem: given a context, multiple futures may be
equally plausible. Our approach leverages Multiple Choice Learning (MCL) and
the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through
Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying
Multiple Choice Learning to Language Modeling, assuming the data is generated
from a mixture of distributions. To illustrate the proposed approach, we use
data sampled from mixtures of Markov chains. We then demonstrate with extensive
experiments on real-world visual and audio captioning tasks that our method
achieves high diversity and relevance in generated outputs.

</details>


### [189] [Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data](https://arxiv.org/abs/2507.10425)
*Alvaro H. C. Correia,Christos Louizos*

Main category: cs.LG

TL;DR: 本文从最优传输视角研究共形预测在分布转移下的覆盖损失问题，并提出可估计和缓解该损失。


<details>
  <summary>Details</summary>
Motivation: 共形预测中校准和测试数据难以满足可交换性条件，现有缓解覆盖损失的方法需先验信息，因此从新视角研究该问题。

Method: 通过最优传输的视角来研究问题。

Result: 可以估计分布转移情况下的覆盖损失并进行缓解。

Conclusion: 从最优传输视角研究共形预测在分布转移下的问题是可行且有效的。

Abstract: Conformal prediction is a distribution-free uncertainty quantification method
that has gained popularity in the machine learning community due to its
finite-sample guarantees and ease of use. Its most common variant, dubbed split
conformal prediction, is also computationally efficient as it boils down to
collecting statistics of the model predictions on some calibration data not yet
seen by the model. Nonetheless, these guarantees only hold if the calibration
and test data are exchangeable, a condition that is difficult to verify and
often violated in practice due to so-called distribution shifts. The literature
is rife with methods to mitigate the loss in coverage in this non-exchangeable
setting, but these methods require some prior information on the type of
distribution shift to be expected at test time. In this work, we study this
problem via a new perspective, through the lens of optimal transport, and show
that it is possible to estimate the loss in coverage and mitigate it in case of
distribution shift.

</details>


### [190] [Meta-autoencoders: An approach to discovery and representation of relationships between dynamically evolving classes](https://arxiv.org/abs/2507.09362)
*Assaf Marron,Smadar Szekely,Irun Cohen,David Harel*

Main category: cs.LG

TL;DR: 介绍元自动编码器（MAE）概念，用于自动编码器集合，给出构造定义、示例及研究方向。


<details>
  <summary>Details</summary>
Motivation: 在机器学习和生物学研究中，需捕获动态进化物种的特性，推动了MAE概念的提出。

Method: 为MAE给出构造性定义，并提供初始示例。

Result: 成功引入MAE概念，给出构造定义与示例。

Conclusion: MAE概念在机器学习和生物学研究中有潜在应用价值，给出了研究方向。

Abstract: An autoencoder (AE) is a neural network that, using self-supervised training,
learns a succinct parameterized representation, and a corresponding encoding
and decoding process, for all instances in a given class. Here, we introduce
the concept of a meta-autoencoder (MAE): an AE for a collection of
autoencoders. Given a family of classes that differ from each other by the
values of some parameters, and a trained AE for each class, an MAE for the
family is a neural net that has learned a compact representation and associated
encoder and decoder for the class-specific AEs. One application of this general
concept is in research and modeling of natural evolution -- capturing the
defining and the distinguishing properties across multiple species that are
dynamically evolving from each other and from common ancestors. In this interim
report we provide a constructive definition of MAEs, initial examples, and the
motivating research directions in machine learning and biology.

</details>


### [191] [Fair CCA for Fair Representation Learning: An ADNI Study](https://arxiv.org/abs/2507.09382)
*Bojian Hou,Zhanliang Wang,Zhuoping Zhou,Boning Tong,Zexuan Wang,Jingxuan Bao,Duy Duong-Tran,Qi Long,Li Shen*

Main category: cs.LG

TL;DR: 提出一种新的公平CCA方法用于公平表征学习，在合成数据和ADNI真实数据上验证其能兼顾相关性分析性能和分类公平性。


<details>
  <summary>Details</summary>
Motivation: 以往公平CCA方法常忽略对下游分类任务的影响，限制了适用性，需要一种新方法。

Method: 提出一种新的公平CCA方法，确保投影特征与敏感属性独立。

Result: 在合成数据和ADNI真实数据上验证该方法能保持高相关性分析性能，同时提升分类任务的公平性。

Conclusion: 该工作使神经影像研究中实现公平机器学习成为可能，在需要无偏分析的场景有重要意义。

Abstract: Canonical correlation analysis (CCA) is a technique for finding correlations
between different data modalities and learning low-dimensional representations.
As fairness becomes crucial in machine learning, fair CCA has gained attention.
However, previous approaches often overlook the impact on downstream
classification tasks, limiting applicability. We propose a novel fair CCA
method for fair representation learning, ensuring the projected features are
independent of sensitive attributes, thus enhancing fairness without
compromising accuracy. We validate our method on synthetic data and real-world
data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating
its ability to maintain high correlation analysis performance while improving
fairness in classification tasks. Our work enables fair machine learning in
neuroimaging studies where unbiased analysis is essential.

</details>


### [192] [Geometric Generative Modeling with Noise-Conditioned Graph Networks](https://arxiv.org/abs/2507.09391)
*Peter Pao-Huang,Mitchell Black,Xiaojie Qiu*

Main category: cs.LG

TL;DR: 本文提出Noise - Conditioned Graph Networks (NCGNs) 以解决现有图生成模型表达能力受限问题，开发Dynamic Message Passing (DMP)，在多个领域表现优于噪声无关架构。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的图生成模型使用与噪声水平无关的图神经网络架构，限制了表达能力。

Method: 引入NCGNs，根据生成过程中的噪声水平动态修改架构；基于理论和实证分析开发DMP，使其消息传递的范围和分辨率适应噪声水平。

Result: DMP在3D点云、时空转录组学和图像等多个领域始终优于噪声无关架构。

Conclusion: NCGNs和DMP能有效解决现有图生成模型的问题，提高模型性能。

Abstract: Generative modeling of graphs with spatial structure is essential across many
applications from computer graphics to spatial genomics. Recent flow-based
generative models have achieved impressive results by gradually adding and then
learning to remove noise from these graphs. Existing models, however, use graph
neural network architectures that are independent of the noise level, limiting
their expressiveness. To address this issue, we introduce
\textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural
networks that dynamically modify their architecture according to the noise
level during generation. Our theoretical and empirical analysis reveals that as
noise increases, (1) graphs require information from increasingly distant
neighbors and (2) graphs can be effectively represented at lower resolutions.
Based on these insights, we develop Dynamic Message Passing (DMP), a specific
instantiation of NCGNs that adapts both the range and resolution of message
passing to the noise level. DMP consistently outperforms noise-independent
architectures on a variety of domains including $3$D point clouds,
spatiotemporal transcriptomics, and images. Code is available at
https://github.com/peterpaohuang/ncgn.

</details>


### [193] [A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention](https://arxiv.org/abs/2507.09394)
*Nandan Kumar Jha,Brandon Reagen*

Main category: cs.LG

TL;DR: 研究多头部潜在注意力（MLA）对Transformer预训练内部容量的影响，发现旋转嵌入应用方式很关键，共享旋转组件可缓解光谱碎片化和保留表征能力。


<details>
  <summary>Details</summary>
Motivation: 研究流行的键/值内存压缩策略MLA对Transformer预训练内部容量的影响。

Method: 使用轻量级的Marchenko - Pastur（MP）诊断套件分析$W_{Q}W_{K}^\top$格拉姆矩阵的频谱，对比标准多头注意力（MHA）基线、压缩前应用旋转的MLA - PreRoPE和跨所有头共享单个旋转子向量的MLA - Decoupled三种变体。

Result: 发现容量瓶颈局部出现，尖峰与秩崩溃重合，只有解耦变体可防止级联，保持广泛的光谱支持和抑制离群值形成。

Conclusion: 旋转嵌入的应用方式和压缩位置同样重要，跨头共享旋转组件可缓解光谱碎片化和保留表征能力。

Abstract: In this work, we study how multi-head latent attention (MLA), a popular
strategy for compressing key/value memory, affects a transformer's internal
capacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP)
diagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\top$ gram matrix
throughout training, comparing three variants: the standard multi-head
attention (MHA) baseline, MLA-PreRoPE with rotary applied before compression,
and MLA-Decoupled, which shares a single rotary sub-vector across all heads.
Our random matrix analysis reveals \textbf{three key findings:} \textbf{ i)}
capacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp,
early spikes in specific layers that persist and propagate, disrupting the
balance between bulk and outlier directions; \textbf{ ii)} these spikes
coincide with rank collapse, concentrating the model's expressivity into narrow
subspaces; \textbf{ iii)} only the decoupled variant prevents this cascade,
maintaining broad spectral support and suppressing outlier formation across
layers. These results underscore that \emph{how} rotary embeddings are applied
is just as critical as \emph{where} compression occurs. Sharing rotary
components across heads mitigates spectral fragmentation and preserves
representational capacity.

</details>


### [194] [Scaling Laws for Optimal Data Mixtures](https://arxiv.org/abs/2507.09404)
*Mustafa Shukor,Louis Bethune,Dan Busbridge,David Grangier,Enrico Fini,Alaaeldin El-Nouby,Pierre Ablin*

Main category: cs.LG

TL;DR: 本文提出用缩放定律确定任意目标领域的最优数据混合比例，在三种大规模预训练场景验证其通用性和外推性，替代试错法。


<details>
  <summary>Details</summary>
Motivation: 标准的数据混合比例选择方法依赖试错，在大规模预训练中不实用，需系统性方法。

Method: 提出用缩放定律确定最优数据混合比例的方法，该方法能预测特定条件下模型的损失。

Result: 在大语言模型、原生多模态模型和大视觉模型预训练中验证了缩放定律的通用性和外推性。

Conclusion: 缩放定律可在给定训练预算下为任意目标领域导出最优领域权重，是替代试错法的原则性方法。

Abstract: Large foundation models are typically trained on data from multiple domains,
with the data mixture--the proportion of each domain used--playing a critical
role in model performance. The standard approach to selecting this mixture
relies on trial and error, which becomes impractical for large-scale
pretraining. We propose a systematic method to determine the optimal data
mixture for any target domain using scaling laws. Our approach accurately
predicts the loss of a model of size $N$ trained with $D$ tokens and a specific
domain weight vector $h$. We validate the universality of these scaling laws by
demonstrating their predictive power in three distinct and large-scale
settings: large language model (LLM), native multimodal model (NMM), and large
vision models (LVM) pretraining. We further show that these scaling laws can
extrapolate to new data mixtures and across scales: their parameters can be
accurately estimated using a few small-scale training runs, and used to
estimate the performance at larger scales and unseen domain weights. The
scaling laws allow to derive the optimal domain weights for any target domain
under a given training budget ($N$,$D$), providing a principled alternative to
costly trial-and-error methods.

</details>


### [195] [Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers](https://arxiv.org/abs/2507.09406)
*Santhosh Kumar Ravindran*

Main category: cs.LG

TL;DR: 本文提出对抗激活补丁方法检测和缓解大语言模型欺骗行为，通过模拟实验验证，还提出假设、给出缓解策略等。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型经RLHF对齐后出现的欺骗行为问题。

Method: 引入对抗激活补丁框架，利用激活补丁模拟漏洞、量化欺骗率，进行玩具神经网络模拟实验。

Result: 对抗补丁使欺骗输出从0%增至23.9%，层特定变化支持假设。

Conclusion: 该工作推进AI安全，凸显补丁两用潜力，为大规模模型实证研究提供路线图。

Abstract: Large language models (LLMs) aligned for safety through techniques like
reinforcement learning from human feedback (RLHF) often exhibit emergent
deceptive behaviors, where outputs appear compliant but subtly mislead or omit
critical information. This paper introduces adversarial activation patching, a
novel mechanistic interpretability framework that leverages activation patching
as an adversarial tool to induce, detect, and mitigate such deception in
transformer-based models. By sourcing activations from "deceptive" prompts and
patching them into safe forward passes at specific layers, we simulate
vulnerabilities and quantify deception rates. Through toy neural network
simulations across multiple scenarios (e.g., 1000 trials per setup), we
demonstrate that adversarial patching increases deceptive outputs to 23.9% from
a 0% baseline, with layer-specific variations supporting our hypotheses. We
propose six hypotheses, including transferability across models, exacerbation
in multimodal settings, and scaling effects. An expanded literature review
synthesizes over 20 key works in interpretability, deception, and adversarial
attacks. Mitigation strategies, such as activation anomaly detection and robust
fine-tuning, are detailed, alongside ethical considerations and future research
directions. This work advances AI safety by highlighting patching's dual-use
potential and provides a roadmap for empirical studies on large-scale models.

</details>


### [196] [On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization](https://arxiv.org/abs/2507.09428)
*Zakhar Shumaylov,Vasileios Tsiaras,Yannis Stylianou*

Main category: cs.LG

TL;DR: 本文探讨信息几何在模型压缩中的应用，分析现有方法，指出压缩预训练和微调模型的要点并证明迭代方法收敛性，通过软秩减少改进性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型参数增加，需要有效压缩技术在资源受限设备部署。

Method: 运用信息几何分析模型压缩方法，聚焦算子分解，证明迭代奇异值阈值法收敛性。

Result: 压缩预训练模型用信息散度可提高零样本准确率，微调模型时瓶颈模型可训练性更重要，软秩减少能提升固定压缩率下性能。

Conclusion: 信息几何视角对模型压缩有重要意义，迭代方法在微调模型压缩中很关键，软秩减少可改进现有方法。

Abstract: The ever-increasing parameter counts of deep learning models necessitate
effective compression techniques for deployment on resource-constrained
devices. This paper explores the application of information geometry, the study
of density-induced metrics on parameter spaces, to analyze existing methods
within the space of model compression, primarily focusing on operator
factorization. Adopting this perspective highlights the core challenge:
defining an optimal low-compute submanifold (or subset) and projecting onto it.
We argue that many successful model compression approaches can be understood as
implicitly approximating information divergences for this projection. We
highlight that when compressing a pre-trained model, using information
divergences is paramount for achieving improved zero-shot accuracy, yet this
may no longer be the case when the model is fine-tuned. In such scenarios,
trainability of bottlenecked models turns out to be far more important for
achieving high compression ratios with minimal performance degradation,
necessitating adoption of iterative methods. In this context, we prove
convergence of iterative singular value thresholding for training neural
networks subject to a soft rank constraint. To further illustrate the utility
of this perspective, we showcase how simple modifications to existing methods
through softer rank reduction result in improved performance under fixed
compression rates.

</details>


### [197] [Transformers Don't In-Context Learn Least Squares Regression](https://arxiv.org/abs/2507.09440)
*Joshua Hill,Benjamin Eyre,Elliot Creager*

Main category: cs.LG

TL;DR: 研究大模型上下文学习机制，通过实验表明其在分布外泛化不佳，不符OLS算法，还揭示预训练语料对上下文学习行为的影响。


<details>
  <summary>Details</summary>
Motivation: 探究大模型上下文学习（ICL）背后的机制，此前该机制尚不明确。

Method: 进行分布外泛化实验，对残差流中学习到的表示进行频谱分析。

Result: Transformer在提示分布变化后无法泛化，与实现OLS算法的概念不符；训练数据同分布输入有独特频谱特征，该特征与低损失高度相关。

Conclusion: Transformer实现ICL的机制与传统学习规则不同，预训练语料对ICL行为有重要影响。

Abstract: In-context learning (ICL) has emerged as a powerful capability of large
pretrained transformers, enabling them to solve new tasks implicit in example
input-output pairs without any gradient updates. Despite its practical success,
the mechanisms underlying ICL remain largely mysterious. In this work we study
synthetic linear regression to probe how transformers implement learning at
inference time. Previous works have demonstrated that transformers match the
performance of learning rules such as Ordinary Least Squares (OLS) regression
or gradient descent and have suggested ICL is facilitated in transformers
through the learned implementation of one of these techniques. In this work, we
demonstrate through a suite of out-of-distribution generalization experiments
that transformers trained for ICL fail to generalize after shifts in the prompt
distribution, a behaviour that is inconsistent with the notion of transformers
implementing algorithms such as OLS. Finally, we highlight the role of the
pretraining corpus in shaping ICL behaviour through a spectral analysis of the
learned representations in the residual stream. Inputs from the same
distribution as the training data produce representations with a unique
spectral signature: inputs from this distribution tend to have the same top two
singular vectors. This spectral signature is not shared by out-of-distribution
inputs, and a metric characterizing the presence of this signature is highly
correlated with low loss.

</details>


### [198] [Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring](https://arxiv.org/abs/2507.09460)
*Noah Marchal,William E. Janes,Mihail Popescu,Xing Song*

Main category: cs.LG

TL;DR: 本文开发半监督回归模型，用家庭传感器监测数据估计ALS功能衰退率，对比不同模型范式和插值方法，发现匹配学习和伪标签技术可提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 临床对ALS功能衰退的监测可能错过两次评估间的关键变化，需要改进监测方法。

Method: 开发半监督回归模型，对比三种模型范式（个体批量学习、队列级批量学习和增量微调迁移学习），采用线性斜率、三次多项式和集成自注意力伪标签插值。

Result: 迁移学习改善28/32的ALSFRS - R子量表预测误差；个体批量学习在2/3中对综合量表预测较好；自注意力插值在子量表模型中误差最低；呼吸和言语表现出患者特异性模式，吞咽和穿衣功能遵循队列级轨迹。

Conclusion: 匹配学习和伪标签技术到功能域特定的同质 - 异质特征可提高ALS进展跟踪的预测准确性，在传感器监测平台中集成自适应模型选择可用于未来多中心研究。

Abstract: Clinical monitoring of functional decline in ALS relies on periodic
assessments that may miss critical changes occurring between visits. To address
this gap, semi-supervised regression models were developed to estimate rates of
decline in a case series cohort by targeting ALSFRS- R scale trajectories with
continuous in-home sensor monitoring data. Our analysis compared three model
paradigms (individual batch learning and cohort-level batch versus incremental
fine-tuned transfer learning) across linear slope, cubic polynomial, and
ensembled self-attention pseudo-label interpolations. Results revealed cohort
homogeneity across functional domains responding to learning methods, with
transfer learning improving prediction error for ALSFRS-R subscales in 28 of 32
contrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting
the composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention
interpolation achieved the lowest prediction error for subscale-level models
(mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns,
outperforming linear and cubic interpolations in 20 of 32 contrasts, though
linear interpolation proved more stable in all ALSFRS-R composite scale models
(mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity
profiles across functional domains with respiratory and speech exhibiting
patient-specific patterns benefiting from personalized incremental adaptation,
while swallowing and dressing functions followed cohort-level trajectories
suitable for transfer models. These findings suggest that matching learning and
pseudo-labeling techniques to functional domain-specific
homogeneity-heterogeneity profiles enhances predictive accuracy in ALS
progression tracking. Integrating adaptive model selection within sensor
monitoring platforms could enable timely interventions and scalable deployment
in future multi-center studies.

</details>


### [199] [La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching](https://arxiv.org/abs/2507.09466)
*Tomas Geffner,Kieran Didi,Zhonglin Cao,Danny Reidenbach,Zuobai Zhang,Christian Dallago,Emine Kucukbenli,Karsten Kreis,Arash Vahdat*

Main category: cs.LG

TL;DR: 本文介绍了用于原子级蛋白质设计的La - Proteina，它基于部分潜在蛋白质表示，在多个生成基准测试中表现出色，具有可扩展性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前直接联合生成完整原子结构和氨基酸序列的蛋白质生成模型较少，该任务具有挑战性，如模型需处理生成过程中侧链长度变化问题。

Method: 引入基于部分潜在蛋白质表示的La - Proteina，显式建模粗骨干结构，通过固定维度的每残基潜在变量捕获序列和原子细节，在部分潜在空间进行流匹配以建模序列和全原子结构的联合分布。

Result: La - Proteina在多个生成基准测试中达到了最先进水平，在原子基序支架性能上超越先前模型，能生成多达800个残基的可协同设计蛋白质。

Conclusion: La - Proteina在原子级蛋白质设计方面表现优异，具有可扩展性和鲁棒性，能解决关键的原子结构条件蛋白质设计任务。

Abstract: Recently, many generative models for de novo protein structure design have
emerged. Yet, only few tackle the difficult task of directly generating fully
atomistic structures jointly with the underlying amino acid sequence. This is
challenging, for instance, because the model must reason over side chains that
change in length during generation. We introduce La-Proteina for atomistic
protein design based on a novel partially latent protein representation: coarse
backbone structure is modeled explicitly, while sequence and atomistic details
are captured via per-residue latent variables of fixed dimensionality, thereby
effectively side-stepping challenges of explicit side-chain representations.
Flow matching in this partially latent space then models the joint distribution
over sequences and full-atom structures. La-Proteina achieves state-of-the-art
performance on multiple generation benchmarks, including all-atom
co-designability, diversity, and structural validity, as confirmed through
detailed structural analyses and evaluations. Notably, La-Proteina also
surpasses previous models in atomistic motif scaffolding performance, unlocking
critical atomistic structure-conditioned protein design tasks. Moreover,
La-Proteina is able to generate co-designable proteins of up to 800 residues, a
regime where most baselines collapse and fail to produce valid samples,
demonstrating La-Proteina's scalability and robustness.

</details>


### [200] [Discrete Differential Principle for Continuous Smooth Function Representation](https://arxiv.org/abs/2507.09480)
*Guoyou Wang,Yihua Tan,Shiqi Liu*

Main category: cs.LG

TL;DR: 本文提出新离散微分算子估计导数并表示连续光滑函数，可缓解维数灾难和误差传播，实验证明方法有效且具广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 泰勒公式在离散情况下存在维数灾难和导数计算时的误差传播问题，需要改进。

Method: 提出新离散微分算子，利用截断泰勒级数的范德蒙德系数矩阵，同时计算低于样本点数阶数的所有导数，采用等距均匀采样。

Result: 建立导数估计和函数表示的严格误差界，将方法扩展到二维情况，实验表明该方法优于有限前向差分法、三次样条和线性插值。

Conclusion: 该技术在视觉表示、特征提取等领域有广泛适用性。

Abstract: Taylor's formula holds significant importance in function representation,
such as solving differential difference equations, ordinary differential
equations, partial differential equations, and further promotes applications in
visual perception, complex control, fluid mechanics, weather forecasting and
thermodynamics. However, the Taylor's formula suffers from the curse of
dimensionality and error propagation during derivative computation in discrete
situations. In this paper, we propose a new discrete differential operator to
estimate derivatives and to represent continuous smooth function locally using
the Vandermonde coefficient matrix derived from truncated Taylor series. Our
method simultaneously computes all derivatives of orders less than the number
of sample points, inherently mitigating error propagation. Utilizing
equidistant uniform sampling, it achieves high-order accuracy while alleviating
the curse of dimensionality. We mathematically establish rigorous error bounds
for both derivative estimation and function representation, demonstrating
tighter bounds for lower-order derivatives. We extend our method to the
two-dimensional case, enabling its use for multivariate derivative
calculations. Experiments demonstrate the effectiveness and superiority of the
proposed method compared to the finite forward difference method for derivative
estimation and cubic spline and linear interpolation for function
representation. Consequently, our technique offers broad applicability across
domains such as vision representation, feature extraction, fluid mechanics, and
cross-media imaging.

</details>


### [201] [An Analysis of Action-Value Temporal-Difference Methods That Learn State Values](https://arxiv.org/abs/2507.09523)
*Brett Daley,Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

TL;DR: 分析从两个非对称价值函数进行引导的TD控制方法，比较其与其他方法在收敛性和样本效率，引入新算法RDQ且表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有从两个非对称价值函数引导的TD控制方法研究不足，不清楚学习两个价值函数的优势及理论合理性。

Method: 从收敛性和样本效率方面分析QV - learning和AV - learning算法族。

Result: 在预测场景中，两个算法族比Expected Sarsa更高效；在控制场景中，仅AV - learning方法比Q - learning有显著优势；新算法RDQ在MinAtar基准测试中显著优于Dueling DQN。

Conclusion: 研究了不同TD控制方法特点，引入的新算法有较好表现。

Abstract: The hallmark feature of temporal-difference (TD) learning is bootstrapping:
using value predictions to generate new value predictions. The vast majority of
TD methods for control learn a policy by bootstrapping from a single
action-value function (e.g., Q-learning and Sarsa). Significantly less
attention has been given to methods that bootstrap from two asymmetric value
functions: i.e., methods that learn state values as an intermediate step in
learning action values. Existing algorithms in this vein can be categorized as
either QV-learning or AV-learning. Though these algorithms have been
investigated to some degree in prior work, it remains unclear if and when it is
advantageous to learn two value functions instead of just one -- and whether
such approaches are theoretically sound in general. In this paper, we analyze
these algorithmic families in terms of convergence and sample efficiency. We
find that while both families are more efficient than Expected Sarsa in the
prediction setting, only AV-learning methods offer any major benefit over
Q-learning in the control setting. Finally, we introduce a new AV-learning
algorithm called Regularized Dueling Q-learning (RDQ), which significantly
outperforms Dueling DQN in the MinAtar benchmark.

</details>


### [202] [Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events](https://arxiv.org/abs/2507.09545)
*Ilaria Vascotto,Valentina Blasone,Alex Rodriguez,Alessandro Bonaita,Luca Bortolussi*

Main category: cs.LG

TL;DR: 本文聚焦不平衡数据集下可解释人工智能（XAI）解释可靠性评估，提出针对少数类的评估方法并给出用例。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型部署增多及立法要求，XAI方法应用重要，解释的鲁棒性是关键，但不平衡数据集下评估解释可靠性常被低估。

Method: 提出聚焦少数类的评估方法，利用流形上邻居生成、解释聚合和检验解释一致性的指标。

Result: 基于含数值特征的表格数据集给出了关于霜冻事件发生的用例。

Conclusion: 文中未明确提及结论内容。

Abstract: The usage of eXplainable Artificial Intelligence (XAI) methods has become
essential in practical applications, given the increasing deployment of
Artificial Intelligence (AI) models and the legislative requirements put
forward in the latest years. A fundamental but often underestimated aspect of
the explanations is their robustness, a key property that should be satisfied
in order to trust the explanations. In this study, we provide some preliminary
insights on evaluating the reliability of explanations in the specific case of
unbalanced datasets, which are very frequent in high-risk use-cases, but at the
same time considerably challenging for both AI models and XAI methods. We
propose a simple evaluation focused on the minority class (i.e. the less
frequent one) that leverages on-manifold generation of neighbours, explanation
aggregation and a metric to test explanation consistency. We present a use-case
based on a tabular dataset with numerical features focusing on the occurrence
of frost events.

</details>


### [203] [Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives](https://arxiv.org/abs/2507.09565)
*Heeba Shakeel,Tanvir Ahmad,Chandni Saxena*

Main category: cs.LG

TL;DR: 介绍用于社交媒体用户帖子健康维度分类的数据集，评估多种模型并做后验解释，数据集有助于特定区域健康评估等


<details>
  <summary>Details</summary>
Motivation: 构建可用于社交媒体用户帖子健康维度分类的数据集，推动特定区域健康评估和个性化心理健康评估及干预

Method: 开发综合标注框架，评估传统机器学习模型和基于变换器的模型，用精确率、召回率和F1分数评估性能，采用10折交叉验证和后验解释

Result: 无明确提及模型具体性能结果

Conclusion: 提出的数据集有助于社交媒体特定区域健康评估，为个性化幸福评估和心理健康早期干预策略奠定基础，且实验和数据集按伦理公开

Abstract: We introduce a dataset for classifying wellness dimensions in social media
user posts, covering six key aspects: physical, emotional, social,
intellectual, spiritual, and vocational. The dataset is designed to capture
these dimensions in user-generated content, with a comprehensive annotation
framework developed under the guidance of domain experts. This framework allows
for the classification of text spans into the appropriate wellness categories.
We evaluate both traditional machine learning models and advanced
transformer-based models for this multi-class classification task, with
performance assessed using precision, recall, and F1-score, averaged over
10-fold cross-validation. Post-hoc explanations are applied to ensure the
transparency and interpretability of model decisions. The proposed dataset
contributes to region-specific wellness assessments in social media and paves
the way for personalized well-being evaluations and early intervention
strategies in mental health. We adhere to ethical considerations for
constructing and releasing our experiments and dataset publicly on Github.

</details>


### [204] [DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences](https://arxiv.org/abs/2507.09602)
*Bocheng Ju,Junchao Fan,Jiaqi Liu,Xiaolin Chang*

Main category: cs.LG

TL;DR: 本文介绍了针对联邦反学习的DRAGD攻击及其增强版DRAGDP，实验表明它们在数据重构上优于现有方法，凸显了联邦反学习的隐私漏洞并提供了解决方案。


<details>
  <summary>Details</summary>
Motivation: 联邦反学习中梯度交换会泄露已删除数据的敏感信息，存在新的隐私问题。

Method: 提出DRAGD攻击，利用反学习前后的梯度差异重构被遗忘数据；提出增强版DRAGDP，利用公开的先验数据提高重构准确性。

Result: 在多个数据集上的大量实验表明，DRAGD和DRAGDP在数据重构方面显著优于现有方法。

Conclusion: 凸显了联邦反学习中的关键隐私漏洞，为提高现实应用中联邦反学习系统的安全性提供了实用解决方案。

Abstract: Federated learning enables collaborative machine learning while preserving
data privacy. However, the rise of federated unlearning, designed to allow
clients to erase their data from the global model, introduces new privacy
concerns. Specifically, the gradient exchanges during the unlearning process
can leak sensitive information about deleted data. In this paper, we introduce
DRAGD, a novel attack that exploits gradient discrepancies before and after
unlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced
version of DRAGD that leverages publicly available prior data to improve
reconstruction accuracy, particularly for complex datasets like facial images.
Extensive experiments across multiple datasets demonstrate that DRAGD and
DRAGDP significantly outperform existing methods in data reconstruction.Our
work highlights a critical privacy vulnerability in federated unlearning and
offers a practical solution, advancing the security of federated unlearning
systems in real-world applications.

</details>


### [205] [MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression](https://arxiv.org/abs/2507.09616)
*Ofir Gordon,Ariel Lapid,Elad Cohen,Yarden Yagil,Arnon Netzer,Hai Victor Habi*

Main category: cs.LG

TL;DR: 提出混合低秩与量化方法MLoRQ，结合低秩近似和混合精度量化技术，分阶段优化，兼容现有量化算法，在视觉Transformer任务上有性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决基于Transformer的神经网络在资源受限边缘设备上部署的挑战。

Method: 采用两阶段优化过程确定各层最优位宽和秩分配，还可使用改进的自适应舍入技术减轻压缩误差，且与多数现有量化算法兼容。

Result: 在图像分类、目标检测和实例分割任务的视觉Transformer上评估，有高达15%的性能提升。

Conclusion: MLoRQ是一种有效的解决Transformer网络在边缘设备部署问题的方法。

Abstract: Deploying transformer-based neural networks on resource-constrained edge
devices presents a significant challenge. This challenge is often addressed
through various techniques, such as low-rank approximation and mixed-precision
quantization. In this work, we introduce Mixed Low-Rank and Quantization
(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a
two-stage optimization process to determine optimal bit-width and rank
assignments for each layer, adhering to predefined memory constraints. This
process includes: (i) an intra-layer optimization that identifies potentially
optimal compression solutions out of all low-rank and quantization
combinations; (ii) an inter-layer optimization that assigns bit-width precision
and rank to each layer while ensuring the memory constraint is met. An optional
final step applies a sequential optimization process using a modified adaptive
rounding technique to mitigate compression-induced errors in joint low-rank
approximation and quantization. The method is compatible and can be seamlessly
integrated with most existing quantization algorithms. MLoRQ shows
state-of-the-art results with up to 15\% performance improvement, evaluated on
Vision Transformers for image classification, object detection, and instance
segmentation tasks.

</details>


### [206] [Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset](https://arxiv.org/abs/2507.09650)
*Lily Hong Zhang,Smitha Milli,Karen Jusko,Jonathan Smith,Brandon Amos,Wassim,Bouaziz,Manon Revel,Jack Kussman,Lisa Titus,Bhaktipriya Radharapu,Jane Yu,Vidya Sarma,Kris Rose,Maximilian Nickel*

Main category: cs.LG

TL;DR: 本文通过大规模研究发现人类偏好差异大，现有偏好数据集收集方法不足，提出负相关采样法提升对齐方法性能，还开源了最大多语言多轮偏好数据集。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型如何服务具有不同偏好用户的问题。

Method: 开展五国大规模多语言人类研究，分析现有偏好数据集收集方法，提出负相关采样法并收集开源数据集。

Result: 人类偏好差异比21个先进大语言模型响应差异大；现有方法不足以学习人类偏好多样性；负相关采样法能提升对齐方法性能；收集并开源最大多语言多轮偏好数据集。

Conclusion: 开源的Community Alignment数据集对提升大语言模型服务全球多样化人群的有效性有价值。

Abstract: How can large language models (LLMs) serve users with varying preferences
that may conflict across cultural, political, or other dimensions? To advance
this challenge, this paper establishes four key results. First, we demonstrate,
through a large-scale multilingual human study with representative samples from
five countries (N=15,000), that humans exhibit significantly more variation in
preferences than the responses of 21 state-of-the-art LLMs. Second, we show
that existing methods for preference dataset collection are insufficient for
learning the diversity of human preferences even along two of the most salient
dimensions of variability in global values, due to the underlying homogeneity
of candidate responses. Third, we argue that this motivates the need for
negatively-correlated sampling when generating candidate sets, and we show that
simple prompt-based techniques for doing so significantly enhance the
performance of alignment methods in learning heterogeneous preferences. Fourth,
based on this novel candidate sampling approach, we collect and open-source
Community Alignment, the largest and most representative multilingual and
multi-turn preference dataset to date, featuring almost 200,000 comparisons
from annotators spanning five countries. We hope that the Community Alignment
dataset will be a valuable resource for improving the effectiveness of LLMs for
a diverse global population.

</details>


### [207] [Conformal Prediction for Privacy-Preserving Machine Learning](https://arxiv.org/abs/2507.09678)
*Alexander David Balinsky,Dominik Krzeminski,Alexander Balinsky*

Main category: cs.LG

TL;DR: 研究在确定性加密数据上结合共形预测与监督学习，展示共形预测在加密域有效性，对比不同预测器，揭示其潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 弥合严格不确定性量化与隐私保护机器学习之间的差距。

Method: 使用AES加密的MNIST数据集，测试基于p值和e值的共形预测器。

Result: 在确定性加密数据上训练的模型能提取有意义结构，e值共形预测覆盖率超60%，p值共形预测集小但覆盖率低。

Conclusion: 共形预测在加密数据设置中有潜力和局限，存在预测集紧凑性与可靠性的权衡。

Abstract: We investigate the integration of Conformal Prediction (CP) with supervised
learning on deterministically encrypted data, aiming to bridge the gap between
rigorous uncertainty quantification and privacy-preserving machine learning.
Using AES-encrypted variants of the MNIST dataset, we demonstrate that CP
methods remain effective even when applied directly in the encrypted domain,
owing to the preservation of data exchangeability under fixed-key encryption.
We test traditional $p$-value-based against $e$-value-based conformal
predictors. Our empirical evaluation reveals that models trained on
deterministically encrypted data retain the ability to extract meaningful
structure, achieving 36.88\% test accuracy -- significantly above random
guessing (9.56\%) observed with per-instance encryption. Moreover,
$e$-value-based CP achieves predictive set coverage of over 60\% with 4.3
loss-threshold calibration, correctly capturing the true label in 4888 out of
5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive
sets but with reduced coverage accuracy. These findings highlight both the
promise and limitations of CP in encrypted data settings and underscore
critical trade-offs between prediction set compactness and reliability. %Our
work sets a foundation for principled uncertainty quantification in secure,
privacy-aware learning systems.

</details>


### [208] [Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness](https://arxiv.org/abs/2507.09687)
*Md Mushfiqur Rahaman,Elliot Chang,Tasmiah Haque,Srinjoy Das*

Main category: cs.LG

TL;DR: 对基于生成和判别式LSTM的文本分类模型结合PTQ进行对比研究，评估不同位宽和噪声条件下性能，研究校准数据类别不平衡影响，强调校准数据在PTQ中的作用。


<details>
  <summary>Details</summary>
Motivation: 文本分类在边缘计算应用有低延迟和高精度需求，生成分类器有优势但边缘部署面临计算和内存限制，PTQ适合边缘部署，需研究其应用效果。

Method: 使用Brevitas量化库对生成和判别式LSTM文本分类模型结合PTQ进行全面对比研究，在多比特位宽下评估，用非参数假设检验统计量分析。

Result: 判别式分类器更稳健，生成式对量化位宽、校准数据和输入噪声更敏感，校准数据类别不平衡会使生成式LSTM分类器在低位宽下性能下降。

Conclusion: 强调校准数据在PTQ中的作用，明确生成式分类器在噪声下的优劣，有助于边缘环境部署。

Abstract: Text classification plays a pivotal role in edge computing applications like
industrial monitoring, health diagnostics, and smart assistants, where low
latency and high accuracy are both key requirements. Generative classifiers, in
particular, have been shown to exhibit robustness to out-of-distribution and
noisy data, which is an extremely critical consideration for deployment in such
real-time edge environments. However, deploying such models on edge devices
faces computational and memory constraints. Post Training Quantization (PTQ)
reduces model size and compute costs without retraining, making it ideal for
edge deployment. In this work, we present a comprehensive comparative study of
generative and discriminative Long Short Term Memory (LSTM)-based text
classification models with PTQ using the Brevitas quantization library. We
evaluate both types of classifier models across multiple bitwidths and assess
their robustness under regular and noisy input conditions. We find that while
discriminative classifiers remain robust, generative ones are more sensitive to
bitwidth, calibration data used during PTQ, and input noise during quantized
inference. We study the influence of class imbalance in calibration data for
both types of classifiers, comparing scenarios with evenly and unevenly
distributed class samples including their effect on weight adjustments and
activation profiles during PTQ. Using test statistics derived from
nonparametric hypothesis testing, we identify that using class imbalanced data
during calibration introduces insufficient weight adaptation at lower bitwidths
for generative LSTM classifiers, thereby leading to degraded performance. This
study underscores the role of calibration data in PTQ and when generative
classifiers succeed or fail under noise, aiding deployment in edge
environments.

</details>


### [209] [EPT-2 Technical Report](https://arxiv.org/abs/2507.09703)
*Roberto Molinaro,Niall Siegenheim,Niels Poulsen,Jordan Dane Daubinet,Henry Martin,Mark Frey,Kevin Thiart,Alexander Jakob Dautel,Andreas Schlueter,Alex Grigoryev,Bogdan Danciu,Nikoo Ekhtiari,Bas Steunebrink,Leonie Wagner,Marvin Vincent Gabler*

Main category: cs.LG

TL;DR: 介绍地球物理变压器模型EPT - 2及其基于扰动的集成模型EPT - 2e，EPT - 2性能超前代和其他模型，EPT - 2e成本低且超ECMWF ENS均值。


<details>
  <summary>Details</summary>
Motivation: 提升地球系统预测能力，在地球系统预测中取得更好表现。

Method: 开发EPT - 2模型，引入基于扰动的集成模型EPT - 2e。

Result: EPT - 2在预测能源相关变量上超EPT - 1.5、Aurora和IFS HRES；EPT - 2e显著超ECMWF ENS均值，且计算成本低。

Conclusion: EPT - 2和EPT - 2e在地球系统预测中表现出色，相关模型可通过app.jua.ai平台获取。

Abstract: We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT)
family of foundation AI models for Earth system forecasting. EPT-2 delivers
substantial improvements over its predecessor, EPT-1.5, and sets a new state of
the art in predicting energy-relevant variables-including 10m and 100m wind
speed, 2m temperature, and surface solar radiation-across the full 0-240h
forecast horizon. It consistently outperforms leading AI weather models such as
Microsoft Aurora, as well as the operational numerical forecast system IFS HRES
from the European Centre for Medium-Range Weather Forecasts (ECMWF). In
parallel, we introduce a perturbation-based ensemble model of EPT-2 for
probabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly
surpasses the ECMWF ENS mean-long considered the gold standard for medium- to
longrange forecasting-while operating at a fraction of the computational cost.
EPT models, as well as third-party forecasts, are accessible via the app.jua.ai
platform.

</details>


### [210] [Continental scale habitat modelling with artificial intelligence and multimodal earth observation](https://arxiv.org/abs/2507.09732)
*Sara Si-Moussi,Stephan Hennekens,Sander Mucher,Stan Los,Wilfried Thuiller*

Main category: cs.LG

TL;DR: 研究利用高分辨率遥感数据和人工智能工具改进欧洲大范围栖息地分类，提出方法框架并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 人类活动使生态系统压力增大，现有栖息地地图在主题或空间分辨率上不足，需改进。

Method: 利用欧洲植被档案的植被样地，对欧洲三级EUNIS栖息地建模，评估多种建模策略，利用分层命名、整合多光谱和合成孔径雷达图像、采用集成机器学习纠正类别不平衡。

Result: 利用栖息地命名层次解决分类模糊问题，整合图像增强信息辨别和整体性能，集成机器学习提高准确率。

Conclusion: 方法框架可转移且适应其他分类系统，未来应推进动态栖息地时间建模等研究。

Abstract: Habitats integrate the abiotic conditions and biophysical structures that
support biodiversity and sustain nature's contributions to people. As these
ecosystems face mounting pressure from human activities, accurate,
high-resolution habitat maps are essential for effective conservation and
restoration. Yet current maps often fall short in thematic or spatial
resolution because they must (1) model several mutually exclusive habitat types
that co-occur across landscapes and (2) cope with severe class imbalance that
complicate multi-class training. Here, we evaluated how high-resolution remote
sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat
classification over large geographic extents at fine thematic resolution. Using
vegetation plots from the European Vegetation Archive, we modelled Level 3
EUNIS habitats across Europe and assessed multiple modelling strategies against
independent validation datasets. Strategies that exploited the hierarchical
nature of habitat nomenclatures resolved classification ambiguities, especially
in fragmented landscapes. Integrating multi-spectral (MSI) and synthetic
aperture radar (SAR) imagery, particularly through Earth Observation Foundation
models, enhanced within-formation discrimination and overall performance.
Finally, ensemble machine learning that corrects class imbalance boosted
accuracy further. Our methodological framework is transferable beyond Europe
and adaptable to other classification systems. Future research should advance
temporal modelling of dynamic habitats, extend to habitat segmentation and
quality assessment, and exploit next-generation EO data paired with
higher-quality in-situ observations.

</details>


### [211] [Universal Physics Simulation: A Foundational Diffusion Approach](https://arxiv.org/abs/2507.09733)
*Bradley Camburn*

Main category: cs.LG

TL;DR: 提出首个通用物理模拟基础AI模型，可直接从边界条件数据学习物理定律，无需先验方程编码，具有泛化性和物理发现能力。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络和有限差分方法需明确数学公式，限制了泛化性和发现潜力。

Method: 采用草图引导的扩散变压器方法，将模拟视为条件生成问题，利用增强的扩散变压器架构和新的空间关系编码。

Result: 实现直接的边界到平衡映射，泛化到不同物理领域，直接生成稳态解，SSIM>0.8且保持亚像素边界精度，可通过LRP分析揭示物理关系。

Conclusion: 实现了从AI加速物理到AI发现物理的范式转变，建立了首个真正通用的物理模拟框架。

Abstract: We present the first foundational AI model for universal physics simulation
that learns physical laws directly from boundary-condition data without
requiring a priori equation encoding. Traditional physics-informed neural
networks (PINNs) and finite-difference methods necessitate explicit
mathematical formulation of governing equations, fundamentally limiting their
generalizability and discovery potential. Our sketch-guided diffusion
transformer approach reimagines computational physics by treating simulation as
a conditional generation problem, where spatial boundary conditions guide the
synthesis of physically accurate steady-state solutions.
  By leveraging enhanced diffusion transformer architectures with novel spatial
relationship encoding, our model achieves direct boundary-to-equilibrium
mapping and is generalizable to diverse physics domains. Unlike sequential
time-stepping methods that accumulate errors over iterations, our approach
bypasses temporal integration entirely, directly generating steady-state
solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our
data-informed approach enables physics discovery through learned
representations analyzable via Layer-wise Relevance Propagation (LRP),
revealing emergent physical relationships without predetermined mathematical
constraints. This work represents a paradigm shift from AI-accelerated physics
to AI-discovered physics, establishing the first truly universal physics
simulation framework.

</details>


### [212] [Do we need equivariant models for molecule generation?](https://arxiv.org/abs/2507.09753)
*Ewa M. Nowara,Joshua Rackers,Patricia Suriana,Pan Kessel,Max Shen,Andrew Martin Watkins,Michael Maser*

Main category: cs.LG

TL;DR: 研究非等变CNN经旋转增强训练能否学习等变性并匹配等变模型性能，进行损失分解评估多因素对性能影响。


<details>
  <summary>Details</summary>
Motivation: 当前基于等变GNN的分子发现模型复杂、难训练且扩展性差，探究非等变CNN学习等变性的可能性。

Method: 推导损失分解以分离预测误差和等变误差，评估模型大小、数据集大小和训练时长对去噪、分子生成和属性预测性能的影响。

Result: 原文未提及具体结果。

Conclusion: 原文未提及明确结论。

Abstract: Deep generative models are increasingly used for molecular discovery, with
most recent approaches relying on equivariant graph neural networks (GNNs)
under the assumption that explicit equivariance is essential for generating
high-quality 3D molecules. However, these models are complex, difficult to
train, and scale poorly.
  We investigate whether non-equivariant convolutional neural networks (CNNs)
trained with rotation augmentations can learn equivariance and match the
performance of equivariant models. We derive a loss decomposition that
separates prediction error from equivariance error, and evaluate how model
size, dataset size, and training duration affect performance across denoising,
molecule generation, and property prediction. To our knowledge, this is the
first study to analyze learned equivariance in generative tasks.

</details>


### [213] [Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts](https://arxiv.org/abs/2507.09754)
*Aakash Tripathi,Ian E. Nielsen,Muhammad Umer,Ravi P. Ramachandran,Ghulam Rasool*

Main category: cs.LG

TL;DR: 本文提出用于TFBS预测的MoE方法和用于模型解释的ShiftSmooth技术，在不同数据集表现良好，为TFBS预测提供高效、通用且可解释的解决方案。


<details>
  <summary>Details</summary>
Motivation: 转录因子结合位点（TFBS）预测对理解基因调控和生物过程至关重要，需要高效准确方法。

Method: 引入混合专家（MoE）方法集成多个预训练CNN模型进行TFBS预测，提出ShiftSmooth归因映射技术增强模型可解释性。

Result: MoE模型在不同TF结合位点表现有竞争力，尤其在分布外（OOD）场景出色，ANOVA检验证实性能差异显著；ShiftSmooth在基序发现和定位方面优于传统方法。

Conclusion: 工作为TFBS预测提供高效、通用且可解释的解决方案，有助于基因组生物学新发现和转录调控理解。

Abstract: Transcription Factor Binding Site (TFBS) prediction is crucial for
understanding gene regulation and various biological processes. This study
introduces a novel Mixture of Experts (MoE) approach for TFBS prediction,
integrating multiple pre-trained Convolutional Neural Network (CNN) models,
each specializing in different TFBS patterns. We evaluate the performance of
our MoE model against individual expert models on both in-distribution and
out-of-distribution (OOD) datasets, using six randomly selected transcription
factors (TFs) for OOD testing. Our results demonstrate that the MoE model
achieves competitive or superior performance across diverse TF binding sites,
particularly excelling in OOD scenarios. The Analysis of Variance (ANOVA)
statistical test confirms the significance of these performance differences.
Additionally, we introduce ShiftSmooth, a novel attribution mapping technique
that provides more robust model interpretability by considering small shifts in
input sequences. Through comprehensive explainability analysis, we show that
ShiftSmooth offers superior attribution for motif discovery and localization
compared to traditional Vanilla Gradient methods. Our work presents an
efficient, generalizable, and interpretable solution for TFBS prediction,
potentially enabling new discoveries in genome biology and advancing our
understanding of transcriptional regulation.

</details>


### [214] [Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights](https://arxiv.org/abs/2507.09766)
*Mohamadreza Akbari Pour,Ali Ghasemzadeh,MohamadAli Bijarchi,Mohammad Behshad Shafii*

Main category: cs.LG

TL;DR: 提出RGPD框架用于剩余使用寿命（RUL）和健康状态（SOH）估计，结合物理监督与时空学习，在相关任务中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 准确估计RUL和SOH对预测与健康管理（PHM）在工业应用中至关重要，需更优方法。

Method: 提出RGPD框架，结合GCRN、GATConv、SAC模块进行时空学习，用Q - learning代理动态分配物理信息损失项权重。

Result: 在RUL和SOH估计任务中，该方法在三个工业基准数据集上始终优于现有模型，有强鲁棒性和预测准确性。

Conclusion: RGPD框架在RUL和SOH估计方面有效，能提高预测精度和泛化能力，减少手动调整需求。

Abstract: Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH)
is essential for Prognostics and Health Management (PHM) across a wide range of
industrial applications. We propose a novel framework -- Reinforced Graph-Based
Physics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that
combines physics-based supervision with advanced spatio-temporal learning.
Graph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional
filters within recurrent units to capture how node representations evolve over
time. Graph Attention Convolution (GATConv) leverages a self-attention
mechanism to compute learnable, edge-wise attention coefficients, dynamically
weighting neighbor contributions for adaptive spatial aggregation. A Soft
Actor-Critic (SAC) module is positioned between the Temporal Attention Unit
(TAU) and GCRN to further improve the spatio-temporal learning. This module
improves attention and prediction accuracy by dynamically scaling hidden
representations to minimize noise and highlight informative features. To
identify the most relevant physical constraints in each area, Q-learning agents
dynamically assign weights to physics-informed loss terms, improving
generalization across real-time industrial systems and reducing the need for
manual tuning. In both RUL and SOH estimation tasks, the proposed method
consistently outperforms state-of-the-art models, demonstrating strong
robustness and predictive accuracy across varied degradation patterns across
three diverse industrial benchmark datasets.

</details>


### [215] [Knowing When to Quit: Probabilistic Early Exits for Speech Separation](https://arxiv.org/abs/2507.09768)
*Kenny Falkær Olsen. Mads Østergaard,Karl Ulbæk,Søren Føns Nielsen,Rasmus Malik Høegh Lindrup,Bjørn Sand Jensen,Morten Mørup*

Main category: cs.LG

TL;DR: 设计了具有早期退出能力的语音分离神经网络架构和不确定性感知概率框架，可实现细粒度动态计算缩放，性能达最优水平。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的单通道语音分离架构无法适应不同计算需求和资源，限制了在嵌入式和异构设备中的应用。

Method: 设计具有早期退出能力的神经网络架构，提出不确定性感知概率框架，推导基于所需信噪比的概率早期退出条件。

Result: 在语音分离和增强任务上评估，单个早期退出模型可与在不同计算和参数预算下训练的最优模型竞争。

Conclusion: 该框架能实现语音分离网络的细粒度动态计算缩放，达到最优性能并提供可解释的退出条件。

Abstract: In recent years, deep learning-based single-channel speech separation has
improved considerably, in large part driven by increasingly compute- and
parameter-efficient neural network architectures. Most such architectures are,
however, designed with a fixed compute and parameter budget, and consequently
cannot scale to varying compute demands or resources, which limits their use in
embedded and heterogeneous devices such as mobile phones and hearables. To
enable such use-cases we design a neural network architecture for speech
separation capable of early-exit, and we propose an uncertainty-aware
probabilistic framework to jointly model the clean speech signal and error
variance which we use to derive probabilistic early-exit conditions in terms of
desired signal-to-noise ratios. We evaluate our methods on both speech
separation and enhancement tasks, and we show that a single early-exit model
can be competitive with state-of-the-art models trained at many compute and
parameter budgets. Our framework enables fine-grained dynamic compute-scaling
of speech separation networks while achieving state-of-the-art performance and
interpretable exit conditions.

</details>


### [216] [Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow](https://arxiv.org/abs/2507.09785)
*Zhonglin Cao,Mario Geiger,Allan dos Santos Costa,Danny Reidenbach,Karsten Kreis,Tomas Geffner,Franco Pellegrini,Guoqing Zhou,Emine Kucukbenli*

Main category: cs.LG

TL;DR: 提出加速3D分子构象生成模型训练和推理的机制，实现高效构象生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散或流的构象生成模型训练和采样需大量计算资源，期望快速准确生成分子构象。

Method: 基于流匹配，提出SO(3)-Averaged Flow训练目标加速训练，用流模型的回流和蒸馏方法实现快速推理。

Result: SO(3)-Averaged Flow训练的模型达到先进构象生成质量，回流和蒸馏方法实现少步甚至一步高质量构象生成。

Conclusion: 所提训练技术为基于流模型的高效分子构象生成指明方向。

Abstract: Fast and accurate generation of molecular conformers is desired for
downstream computational chemistry and drug discovery tasks. Currently,
training and sampling state-of-the-art diffusion or flow-based models for
conformer generation require significant computational resources. In this work,
we build upon flow-matching and propose two mechanisms for accelerating
training and inference of generative models for 3D molecular conformer
generation. For fast training, we introduce the SO(3)-Averaged Flow training
objective, which leads to faster convergence to better generation quality
compared to conditional optimal transport flow or Kabsch-aligned flow. We
demonstrate that models trained using SO(3)-Averaged Flow can reach
state-of-the-art conformer generation quality. For fast inference, we show that
the reflow and distillation methods of flow-based models enable few-steps or
even one-step molecular conformer generation with high quality. The training
techniques proposed in this work show a path towards highly efficient molecular
conformer generation with flow-based models.

</details>


### [217] [Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster](https://arxiv.org/abs/2507.09786)
*Junaid Iqbal Khan*

Main category: cs.LG

TL;DR: 提出两种互补方法加速面向分类的近似机器无学习（AMU），实验表明该方法显著降低端到端无学习延迟，同时保留模型实用性和隐私。


<details>
  <summary>Details</summary>
Motivation: 现有AMU处理保留子集主导计算运行时间，减少训练轮数仍具挑战，需要加速AMU。

Method: 提出Blend方法，通过分布匹配的数据集凝聚减少保留集大小；提出Accelerated - AMU（A - AMU）方法，通过增强无学习目标加速收敛。

Result: 数据和以损失为中心的双重优化方法显著降低单轮和多轮场景下的端到端无学习延迟，保留模型实用性和隐私。

Conclusion: 这是首次通过联合设计专门的数据集凝聚技术和加速损失函数系统解决无学习效率问题。

Abstract: Approximate machine unlearning (AMU) enables models to `forget' specific
training data through specialized fine-tuning on a retained dataset subset.
However, processing this retained subset still dominates computational runtime,
while reductions of epochs also remain a challenge. We propose two
complementary methods to accelerate classification-oriented AMU. First,
\textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges
visually similar images with shared blend-weights to significantly reduce the
retained set size. It operates with minimal pre-processing overhead and is
orders of magnitude faster than state-of-the-art DC methods. Second, our
loss-centric method, \textbf{Accelerated-AMU (A-AMU)}, augments the unlearning
objective to quicken convergence. A-AMU achieves this by combining a steepened
primary loss to expedite forgetting with a novel, differentiable regularizer
that matches the loss distributions of forgotten and in-distribution unseen
data. Our extensive experiments demonstrate that this dual approach of data and
loss-centric optimization dramatically reduces end-to-end unlearning latency
across both single and multi-round scenarios, all while preserving model
utility and privacy. To our knowledge, this is the first work to systematically
tackle unlearning efficiency by jointly designing a specialized dataset
condensation technique with a dedicated accelerated loss function. Code is
available at https://github.com/algebraicdianuj/DC_Unlearning.

</details>


### [218] [A Scalable and Efficient Signal Integration System for Job Matching](https://arxiv.org/abs/2507.09797)
*Ping Liu,Rajat Arora,Xiao Shi,Benjamin Le,Qianqi Shen,Jianqiang Shen,Chengming Jiang,Nikita Zhiltsov,Priya Bannur,Yidan Zhu,Liming Dong,Haichao Wei,Qi Guo,Luke Simon,Liangjie Hong,Wenjing Zhang*

Main category: cs.LG

TL;DR: 为解决LinkedIn职位匹配推荐系统建模挑战，开发STAR系统，结合LLM和GNN优势，提供端到端解决方案，有多项贡献。


<details>
  <summary>Details</summary>
Motivation: 解决LinkedIn职位匹配推荐系统面临的冷启动、过滤气泡和偏差等建模挑战。

Method: 开发STAR系统，结合LLM理解文本数据和GNN捕捉关系、缓解冷启动问题的优势，整合多种信号，联合工业规模范式。

Result: 开发出STAR系统，提供端到端解决方案。

Conclusion: 提出工业应用中构建嵌入的可靠方法、可扩展的GNN - LLM集成用于高性能推荐，以及实际模型部署的实用见解。

Abstract: LinkedIn, one of the world's largest platforms for professional networking
and job seeking, encounters various modeling challenges in building
recommendation systems for its job matching product, including cold-start,
filter bubbles, and biases affecting candidate-job matching. To address these,
we developed the STAR (Signal Integration for Talent And Recruiters) system,
leveraging the combined strengths of Large Language Models (LLMs) and Graph
Neural Networks (GNNs). LLMs excel at understanding textual data, such as
member profiles and job postings, while GNNs capture intricate relationships
and mitigate cold-start issues through network effects. STAR integrates diverse
signals by uniting LLM and GNN capabilities with industrial-scale paradigms
including adaptive sampling and version management. It provides an end-to-end
solution for developing and deploying embeddings in large-scale recommender
systems. Our key contributions include a robust methodology for building
embeddings in industrial applications, a scalable GNN-LLM integration for
high-performing recommendations, and practical insights for real-world model
deployment.

</details>


### [219] [Federated Learning with Graph-Based Aggregation for Traffic Forecasting](https://arxiv.org/abs/2507.09805)
*Audri Banik,Glaucio Haroldo Silva de Carvalho,Renata Dividino*

Main category: cs.LG

TL;DR: 提出轻量级图感知联邦学习方法用于交通预测，在两个数据集上表现有竞争力。


<details>
  <summary>Details</summary>
Motivation: 标准联邦学习方法在交通预测中假设客户端独立，限制性能，图学习方法有较大计算开销。

Method: 融合FedAvg简单性与图学习思想，应用邻域聚合原则指导参数更新，根据图连接性加权客户端模型。

Result: 在METR-LA和PEMS - BAY两个基准交通数据集上评估，相比标准基线和近期基于图的联邦学习技术有竞争力。

Conclusion: 所提轻量级图感知联邦学习方法能有效捕捉空间关系且计算高效。

Abstract: In traffic prediction, the goal is to estimate traffic speed or flow in
specific regions or road segments using historical data collected by devices
deployed in each area. Each region or road segment can be viewed as an
individual client that measures local traffic flow, making Federated Learning
(FL) a suitable approach for collaboratively training models without sharing
raw data. In centralized FL, a central server collects and aggregates model
updates from multiple clients to build a shared model while preserving each
client's data privacy. Standard FL methods, such as Federated Averaging
(FedAvg), assume that clients are independent, which can limit performance in
traffic prediction tasks where spatial relationships between clients are
important. Federated Graph Learning methods can capture these dependencies
during server-side aggregation, but they often introduce significant
computational overhead. In this paper, we propose a lightweight graph-aware FL
approach that blends the simplicity of FedAvg with key ideas from graph
learning. Rather than training full models, our method applies basic
neighbourhood aggregation principles to guide parameter updates, weighting
client models based on graph connectivity. This approach captures spatial
relationships effectively while remaining computationally efficient. We
evaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY,
and show that it achieves competitive performance compared to standard
baselines and recent graph-based federated learning techniques.

</details>


### [220] [Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem](https://arxiv.org/abs/2507.09816)
*Adam Newgas*

Main category: cs.LG

TL;DR: 研究通用与问题玩具模型，发现训练找到的解与理论构造不同，该解全稠密、可自然扩展，解释了其在低稀疏度下更高效的原因，有助于理解网络电路和可解释性。


<details>
  <summary>Details</summary>
Motivation: 此前少有对理论构造的电路在实践中能否被学习的研究，因此开展相关探究。

Method: 研究通用与问题的玩具模型，限制隐藏维度以迫使模型寻找计算高效的电路。

Result: 训练过程找到的解与理论构造不同，全稠密，能自然扩展，可权衡错误率和神经元效率，对稀疏性等参数变化有鲁棒性。

Conclusion: 研究结果有助于理解模型形成的电路类型和叠加表示的灵活性，促进对网络电路和可解释性的更广泛理解。

Abstract: Neural networks are capable of superposition -- representing more features
than there are dimensions. Recent work considers the analogous concept for
computation instead of storage, proposing theoretical constructions. But there
has been little investigation into whether these circuits can be learned in
practice. In this work, we investigate a toy model for the Universal-AND
problem which computes the AND of all $m\choose 2$ pairs of $m$ sparse inputs.
The hidden dimension that determines the number of non-linear activations is
restricted to pressure the model to find a compute-efficient circuit, called
compressed computation. We find that the training process finds a simple
solution that does not correspond to theoretical constructions. It is fully
dense -- every neuron contributes to every output. The solution circuit
naturally scales with dimension, trading off error rates for neuron efficiency.
It is similarly robust to changes in sparsity and other key parameters, and
extends naturally to other boolean operations and boolean circuits. We explain
the found solution in detail and compute why it is more efficient than the
theoretical constructions at low sparsity. Our findings shed light on the types
of circuits that models like to form and the flexibility of the superposition
representation. This contributes to a broader understanding of network
circuitry and interpretability.

</details>


### [221] [Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification](https://arxiv.org/abs/2507.09826)
*Jintao Qu,Zichong Wang,Chenhao Wu,Wenbin Zhang*

Main category: cs.LG

TL;DR: 本文提出动态长度缩短算法，构建可训练模型，结合神经网络与DTW优点，在不同数据资源场景表现良好。


<details>
  <summary>Details</summary>
Motivation: 神经网络训练依赖大量标注数据，在冷启动场景受限且缺乏可解释性；DTW在数据丰富场景效果不如神经网络，旨在开发适应冷启动、可训练且有可解释性的通用模型。

Method: 提出动态长度缩短算法，将时间序列转换为原型，把DTW递推关系转化为等效循环神经网络，构建模仿DTW对齐行为的可训练模型。

Result: 模型在低资源场景显著优于先前方法，在高资源场景仍具竞争力。

Conclusion: 所构建的模型结合了神经网络和DTW的优点，在不同数据资源条件下均有较好表现。

Abstract: Neural networks have achieved remarkable success in time series
classification, but their reliance on large amounts of labeled data for
training limits their applicability in cold-start scenarios. Moreover, they
lack interpretability, reducing transparency in decision-making. In contrast,
dynamic time warping (DTW) combined with a nearest neighbor classifier is
widely used for its effectiveness in limited-data settings and its inherent
interpretability. However, as a non-parametric method, it is not trainable and
cannot leverage large amounts of labeled data, making it less effective than
neural networks in rich-resource scenarios. In this work, we aim to develop a
versatile model that adapts to cold-start conditions and becomes trainable with
labeled data, while maintaining interpretability. We propose a dynamic
length-shortening algorithm that transforms time series into prototypes while
preserving key structural patterns, thereby enabling the reformulation of the
DTW recurrence relation into an equivalent recurrent neural network. Based on
this, we construct a trainable model that mimics DTW's alignment behavior. As a
neural network, it becomes trainable when sufficient labeled data is available,
while still retaining DTW's inherent interpretability. We apply the model to
several benchmark time series classification tasks and observe that it
significantly outperforms previous approaches in low-resource settings and
remains competitive in rich-resource settings.

</details>


### [222] [A Pre-training Framework for Relational Data with Information-theoretic Principles](https://arxiv.org/abs/2507.09837)
*Quang Truong,Zhikai Chen,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: 论文提出Task Vector Estimation (TVE)预训练框架用于关系数据库，实验显示其优于传统基线，倡导在预测建模中考虑任务异质性和时间结构。


<details>
  <summary>Details</summary>
Motivation: 关系数据库应用广泛，但因任务异质性，设计通用预训练策略是挑战，需有效框架获取任务感知表示。

Method: 引入TVE框架，通过对模式遍历图进行基于集合的聚合构建预测监督信号，显式建模下一窗口关系动态，并从信息论角度形式化该方法。

Result: 在RelBench基准上的大量实验表明，TVE始终优于传统预训练基线。

Conclusion: 倡导将编码任务异质性和时间结构的预训练目标作为关系数据库预测建模的设计原则。

Abstract: Relational databases underpin critical infrastructure across a wide range of
domains, yet the design of generalizable pre-training strategies for learning
from relational databases remains an open challenge due to task heterogeneity.
Specifically, there exist infinitely many possible downstream tasks, as tasks
are defined based on relational schema graphs, temporal dependencies, and
SQL-defined label logics. An effective pre-training framework is desired to
take these factors into account in order to obtain task-aware representations.
By incorporating knowledge of the underlying distribution that drives label
generation, downstream tasks can benefit from relevant side-channel
information. To bridge this gap, we introduce Task Vector Estimation (TVE), a
novel pre-training framework that constructs predictive supervisory signals via
set-based aggregation over schema traversal graphs, explicitly modeling
next-window relational dynamics. We formalize our approach through an
information-theoretic lens, demonstrating that task-informed representations
retain more relevant signals than those obtained without task priors. Extensive
experiments on the RelBench benchmark show that TVE consistently outperforms
traditional pre-training baselines. Our findings advocate for pre-training
objectives that encode task heterogeneity and temporal structure as design
principles for predictive modeling on relational databases.

</details>


### [223] [Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs](https://arxiv.org/abs/2507.09839)
*MohammadReza Davari,Utkarsh Garg,Weixin Cai,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出以增强反馈机制为核心的APO框架，引入正强化、反馈多样化技术，形式化CPO，实验显示该方法优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法主要关注纠错，忽略正确预测的价值，限制了有效性和效率，且缺乏高效迁移优化提示的方法。

Method: 将文本梯度重新解释为负强化，引入正强化以保留有益提示组件；引入反馈多样化技术减少LLM反馈中的噪声；形式化持续提示优化（CPO）。

Result: 在标准和迁移场景中，该方法始终优于强基线，实现了显著的准确性提升、更快的收敛速度和更低的计算成本。

Conclusion: 所提出的方法在提示优化和迁移方面具有优势，能提高性能和效率。

Abstract: An increasing number of NLP applications interact with large language models
(LLMs) through black-box APIs, making prompt engineering critical for
controlling model outputs. While recent Automatic Prompt Optimization (APO)
methods iteratively refine prompts using model-generated feedback, textual
gradients, they primarily focus on error correction and neglect valuable
insights from correct predictions. This limits both their effectiveness and
efficiency. In this paper, we propose a novel APO framework centered on
enhancing the feedback mechanism. We reinterpret the textual gradient as a form
of negative reinforcement and introduce the complementary positive
reinforcement to explicitly preserve beneficial prompt components identified
through successful predictions. To mitigate the noise inherent in LLM-generated
feedback, we introduce a technique called feedback diversification, which
aggregates multiple feedback signals, emphasizing consistent, actionable advice
while filtering out outliers. Motivated by the rapid evolution and diversity of
available LLMs, we also formalize Continual Prompt Optimization (CPO),
addressing the practical challenge of efficiently migrating optimized prompts
between different model versions or API providers. Our experiments reveal that
naive prompt migration often degrades performance due to loss of critical
instructions. In contrast, our approach consistently outperforms strong
baselines, achieving significant accuracy improvements, faster convergence, and
lower computational costs in both standard and migration scenarios.

</details>


### [224] [Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks](https://arxiv.org/abs/2507.09871)
*Niket Patel,Randall Balestriero*

Main category: cs.LG

TL;DR: 当前AI评估方法有局限，提出用任务先验定义下游任务概率空间评估模型的框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估方法依赖固定手工挑选基准，存在瓶颈，需新评估标准。

Method: 采用任务分布并定义任务先验，构建下游任务概率空间评估模型。

Result: 框架能回答模型在所有下游任务上平均表现和表现方差等关键问题。

Conclusion: 此框架建立新评估标准，能加速自监督学习研究。

Abstract: The grand goal of AI research, and particularly Self Supervised Learning
(SSL), is to produce systems that can successfully solve any possible task. In
contrast, current evaluation methods available to AI researchers typically rely
on a fixed collection of hand-picked downstream benchmarks. Hence, a large
amount of effort is put into designing and searching for large collection of
evaluation tasks that can serve as a proxy of our grand goal. We argue that
such a rigid evaluation protocol creates a silent bottleneck in AI research. To
remedy that, we define a probabilistic space of downstream tasks obtained by
adopting a distribution of tasks and by defining Task Priors. Under this view,
one can evaluate a model's performance over the set of all possible downstream
tasks. Our framework is the first to provide answers to key questions such as
(i) what is the average performance of my model over all possible downstream
tasks weighted by the probability to encounter each task? or (ii) what is the
variance of my model's performance across all downstream tasks under the
defined Task Priors? Beyond establishing a new standard for evaluation, we
believe that Task Priors will accelerate the pace of research in SSL - where
downstream task evaluation is the sole qualitative signal that researchers have
access to.

</details>


### [225] [AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications](https://arxiv.org/abs/2507.09882)
*Jiamin Wu,Zichen Ren,Junyu Wang,Pengyu Zhu,Yonghao Song,Mianxin Liu,Qihao Zheng,Lei Bai,Wanli Ouyang,Chunfeng Song*

Main category: cs.LG

TL;DR: 提出AdaBrain - Bench基准来评估非侵入式BCI的脑基础模型，介绍其构成和用途并开源。


<details>
  <summary>Details</summary>
Motivation: 非侵入式BCI信号噪声高、数据有限，自监督预训练发展但缺乏评估脑基础模型的综合实用基准，阻碍其广泛应用。

Method: 构建AdaBrain - Bench基准，包含多种BCI解码数据集、任务适应流程、评估指标和工具。

Result: 利用AdaBrain - Bench评估了公开脑基础模型，为不同场景选模型提供见解。

Conclusion: 开源基准管道，提供持续发展平台推动神经解码解决方案发展。

Abstract: Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible
means of connecting the human brain to external devices, with broad
applications in home and clinical settings to enhance human capabilities.
However, the high noise level and limited task-specific data in non-invasive
signals constrain decoding capabilities. Recently, the adoption of
self-supervised pre-training is transforming the landscape of non-invasive BCI
research, enabling the development of brain foundation models to capture
generic neural representations from large-scale unlabeled
electroencephalography (EEG) signals with substantial noises. However, despite
these advances, the field currently lacks comprehensive, practical and
extensible benchmarks to assess the utility of the public foundation models
across diverse BCI tasks, hindering their widespread adoption. To address this
challenge, we present AdaBrain-Bench, a large-scale standardized benchmark to
systematically evaluate brain foundation models in widespread non-invasive BCI
tasks. AdaBrain-Bench encompasses a diverse collection of representative BCI
decoding datasets spanning 7 key applications. It introduces a streamlined task
adaptation pipeline integrated with multi-dimensional evaluation metrics and a
set of adaptation tools. The benchmark delivers an inclusive framework for
assessing generalizability of brain foundation models across key transfer
settings, including cross-subject, multi-subject, and few-shot scenarios. We
leverage AdaBrain-Bench to evaluate a suite of publicly available brain
foundation models and offer insights into practices for selecting appropriate
models in various scenarios. We make our benchmark pipeline available to enable
reproducible research and external use, offering a continuously evolving
platform to foster progress toward robust and generalized neural decoding
solutions.

</details>


### [226] [TolerantECG: A Foundation Model for Imperfect Electrocardiogram](https://arxiv.org/abs/2507.09887)
*Huynh Nguyen Dang,Thang Pham,Ngan Le,Van Nguyen*

Main category: cs.LG

TL;DR: 提出用于ECG信号的基础模型TolerantECG，能应对噪声和导联缺失问题，在多数据集表现优异。


<details>
  <summary>Details</summary>
Motivation: 标准12导联心电图记录可能受噪声或部分导联不可用影响诊断效果，需解决该问题。

Method: 结合对比学习和自监督学习框架，联合学习ECG信号表示及其对应的基于知识检索的文本报告描述和损坏或导联缺失的信号。

Result: TolerantECG在PTB - XL数据集的各种ECG信号条件和分类级别中始终排名第一或第二，在MIT - BIH心律失常数据库上取得最高性能。

Conclusion: TolerantECG是一个对噪声鲁棒且能在任意子集导联下工作的有效ECG基础模型。

Abstract: The electrocardiogram (ECG) is an essential and effective tool for diagnosing
heart diseases. However, its effectiveness can be compromised by noise or
unavailability of one or more leads of the standard 12-lead recordings,
resulting in diagnostic errors or uncertainty. To address these challenges, we
propose TolerantECG, a foundation model for ECG signals that is robust to noise
and capable of functioning with arbitrary subsets of the standard 12-lead ECG.
TolerantECG training combines contrastive and self-supervised learning
frameworks to jointly learn ECG signal representations alongside their
corresponding knowledge-retrieval-based text report descriptions and corrupted
or lead-missing signals. Comprehensive benchmarking results demonstrate that
TolerantECG consistently ranks as the best or second-best performer across
various ECG signal conditions and class levels in the PTB-XL dataset, and
achieves the highest performance on the MIT-BIH Arrhythmia Database.

</details>


### [227] [Soft Graph Clustering for single-cell RNA Sequencing Data](https://arxiv.org/abs/2507.09890)
*Ping Xu,Pengfei Wang,Zhiyuan Ning,Meng Xiao,Min Wu,Yuanchun Zhou*

Main category: cs.LG

TL;DR: 现有基于图的单细胞RNA测序聚类方法有局限，本文提出scSGC方法，实验表明其在聚类准确性、细胞类型注释和计算效率上优于13种先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的单细胞RNA测序聚类方法依赖硬图结构，存在信息损失和可能导致错误聚类结果的问题，需改进。

Method: 提出scSGC方法，包含基于零膨胀负二项式的特征自动编码器、双通道切割信息软图嵌入模块和基于最优传输的聚类优化模块。

Result: 在十个数据集上的实验表明，scSGC在聚类准确性、细胞类型注释和计算效率上优于13种先进模型。

Conclusion: scSGC有巨大潜力推动单细胞RNA测序数据分析，加深对细胞异质性的理解。

Abstract: Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)
data analysis for elucidating cellular heterogeneity and diversity. Recent
graph-based scRNA-seq clustering methods, particularly graph neural networks
(GNNs), have significantly improved in tackling the challenges of
high-dimension, high-sparsity, and frequent dropout events that lead to
ambiguous cell population boundaries. However, their reliance on hard graph
constructions derived from thresholded similarity matrices presents
challenges:(i) The simplification of intercellular relationships into binary
edges (0 or 1) by applying thresholds, which restricts the capture of
continuous similarity features among cells and leads to significant information
loss.(ii) The presence of significant inter-cluster connections within hard
graphs, which can confuse GNN methods that rely heavily on graph structures,
potentially causing erroneous message propagation and biased clustering
outcomes. To tackle these challenges, we introduce scSGC, a Soft Graph
Clustering for single-cell RNA sequencing data, which aims to more accurately
characterize continuous similarities among cells through non-binary edge
weights, thereby mitigating the limitations of rigid data structures. The scSGC
framework comprises three core components: (i) a zero-inflated negative
binomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed
soft graph embedding module; and (iii) an optimal transport-based clustering
optimization module. Extensive experiments across ten datasets demonstrate that
scSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,
cell type annotation, and computational efficiency. These results highlight its
substantial potential to advance scRNA-seq data analysis and deepen our
understanding of cellular heterogeneity.

</details>


### [228] [Algorithm Development in Neural Networks: Insights from the Streaming Parity Task](https://arxiv.org/abs/2507.09897)
*Loek van Rossem,Andrew M. Saxe*

Main category: cs.LG

TL;DR: 研究RNN在流式奇偶校验任务上学习动态，发现有限训练后可实现完美无限泛化及隐式表征合并效应。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注神经网络分布内泛化，本文旨在研究其从有限训练经验进行无限泛化的机制，以开发有效的算法发展理论。

Method: 以流式奇偶校验任务对RNN进行训练，研究其学习动态，并运用表征动力学的有效理论分析。

Result: RNN在足够有限训练后出现向完美无限泛化的相变，存在隐式表征合并效应，可解释为构建能重现任务的有限自动机。

Conclusion: 揭示了神经网络从有限训练经验进行无限泛化的一种机制。

Abstract: Even when massively overparameterized, deep neural networks show a remarkable
ability to generalize. Research on this phenomenon has focused on
generalization within distribution, via smooth interpolation. Yet in some
settings neural networks also learn to extrapolate to data far beyond the
bounds of the original training set, sometimes even allowing for infinite
generalization, implying that an algorithm capable of solving the task has been
learned. Here we undertake a case study of the learning dynamics of recurrent
neural networks (RNNs) trained on the streaming parity task in order to develop
an effective theory of algorithm development. The streaming parity task is a
simple but nonlinear task defined on sequences up to arbitrary length. We show
that, with sufficient finite training experience, RNNs exhibit a phase
transition to perfect infinite generalization. Using an effective theory for
the representational dynamics, we find an implicit representational merger
effect which can be interpreted as the construction of a finite automaton that
reproduces the task. Overall, our results disclose one mechanism by which
neural networks can generalize infinitely from finite training experience.

</details>


### [229] [Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model](https://arxiv.org/abs/2507.09925)
*Md Ahsanul Kabir,Abrar Jahin,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: 提出DepBERT模型用于因果短语提取，实验显示其优于现有监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有监督因果短语提取方法未利用依赖树等语言工具，希望改进。

Method: 提出DepBERT，在基于Transformer的模型框架中融入句子的依赖树。

Result: 在三个数据集上的大量实验表明，DepBERT优于各种现有监督因果提取方法。

Conclusion: DepBERT模型是一种有效的因果短语提取方法，利用依赖树能提升性能。

Abstract: Extracting cause and effect phrases from a sentence is an important NLP task,
with numerous applications in various domains, including legal, medical,
education, and scientific research. There are many unsupervised and supervised
methods proposed for solving this task. Among these, unsupervised methods
utilize various linguistic tools, including syntactic patterns, dependency
tree, dependency relations, etc. among different sentential units for
extracting the cause and effect phrases. On the other hand, the contemporary
supervised methods use various deep learning based mask language models
equipped with a token classification layer for extracting cause and effect
phrases. Linguistic tools, specifically, dependency tree, which organizes a
sentence into different semantic units have been shown to be very effective for
extracting semantic pairs from a sentence, but existing supervised methods do
not have any provision for utilizing such tools within their model framework.
In this work, we propose DepBERT, which extends a transformer-based model by
incorporating dependency tree of a sentence within the model framework.
Extensive experiments over three datasets show that DepBERT is better than
various state-of-the art supervised causality extraction methods.

</details>


### [230] [Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications](https://arxiv.org/abs/2507.09931)
*Yoon Pyo Lee*

Main category: cs.LG

TL;DR: 本文以沸水反应堆系统为例，提出解释大语言模型编码和利用特定领域知识的方法，通过微调模型、对比神经元激活模式、采用神经元沉默技术等，为提升黑盒模型透明度、实现核级AI保证提供途径。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型集成到核工程等安全关键领域，需深入理解其内部推理过程。

Method: 使用低秩自适应的参数高效微调技术将通用大语言模型Gemma - 3 - 1b - it适配到核领域，对比基础模型和微调模型的神经元激活模式，采用神经元沉默技术探究特定神经元的因果作用。

Result: 单独沉默大部分特定神经元无显著影响，集体停用则导致任务性能显著下降，定性分析表明沉默这些神经元会削弱模型生成详细、上下文准确技术信息的能力。

Conclusion: 提供增强不透明黑盒模型透明度的具体方法，为实现核级AI保证提供途径，解决核监管框架下AI部署的验证挑战。

Abstract: The integration of Large Language Models (LLMs) into safety-critical domains,
such as nuclear engineering, necessitates a deep understanding of their
internal reasoning processes. This paper presents a novel methodology for
interpreting how an LLM encodes and utilizes domain-specific knowledge, using a
Boiling Water Reactor system as a case study. We adapted a general-purpose LLM
(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning
technique known as Low-Rank Adaptation. By comparing the neuron activation
patterns of the base model to those of the fine-tuned model, we identified a
sparse set of neurons whose behavior was significantly altered during the
adaptation process. To probe the causal role of these specialized neurons, we
employed a neuron silencing technique. Our results demonstrate that while
silencing most of these specialized neurons individually did not produce a
statistically significant effect, deactivating the entire group collectively
led to a statistically significant degradation in task performance. Qualitative
analysis further revealed that silencing these neurons impaired the model's
ability to generate detailed, contextually accurate technical information. This
paper provides a concrete methodology for enhancing the transparency of an
opaque black-box model, allowing domain expertise to be traced to verifiable
neural circuits. This offers a pathway towards achieving nuclear-grade
artificial intelligence (AI) assurance, addressing the verification and
validation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR
50 Appendix B), which have limited AI deployment in safety-critical nuclear
operations.

</details>


### [231] [Memorization Sinks: Isolating Memorization during LLM Training](https://arxiv.org/abs/2507.09937)
*Gaurav R. Ghosal,Pratyush Maini,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 大语言模型易记忆重复序列，现有去除记忆信息方法效果有限，本文提出MemSinks范式，实现记忆内容隔离且不影响泛化能力，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型记忆重复序列带来的隐私和版权问题，改进现有去除记忆信息方法效果不佳的情况。

Method: 提出MemSinks范式，利用序列标识符激活特定记忆神经元，分析学习和遗忘动态。

Result: 在十亿参数和十亿令牌规模实现MemSinks，观察到有效隔离和强泛化能力。

Conclusion: 首次在真实数据上证明同时实现泛化和隔离是可行的。

Abstract: Large language models are susceptible to memorizing repeated sequences,
posing privacy and copyright concerns. A popular mitigation strategy is to
remove memorized information from specific neurons post-hoc. However, such
approaches have shown limited success so far. In a controlled setting, we show
that the memorization of natural sequences (those that resemble linguistically
plausible text) become mechanistically entangled with general language
abilities, thereby becoming challenging to remove post-hoc. In this work, we
put forward a new paradigm of MemSinks that promotes isolation of memorization
by design. We leverage a sequence identifier that activates a unique set of
memorization neurons for each sequence across repetitions. By analyzing the
dynamics of learning and forgetting, we argue that MemSinks facilitates
isolation of memorized content, making it easier to remove without compromising
general language capabilities. We implement MemSinks at the billion-parameter
and billion-token scale, and observe both effective isolation and strong
generalization. To our knowledge, this is the first proof-of-concept on real
data demonstrating that simultaneous generalization and isolation is
achievable. We open-source our code at http://github.com/grghosal/MemSinks.

</details>


### [232] [Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training](https://arxiv.org/abs/2507.09940)
*Taigo Sakai,Kazuhiro Hotta*

Main category: cs.LG

TL;DR: 传统深度学习训练时神经元数量固定，本文提出训练中定期增减神经元的方法，在三类数据集和五个模型上实验，证明该方法优于固定大小网络，能提升不均衡数据性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习神经元数量固定，而生物学习中神经元会动态变化，且现实数据存在类别不均衡问题，固定网络会降低少数类识别精度。

Method: 训练中定期增减神经元，保留多数类关键特征，为少数类选择性增加神经元，动态调整网络容量，且最终网络大小和结构不变。

Result: 在三个不同数据集和五个代表性模型上实验，该方法优于固定大小网络，与其他处理不均衡技术结合时精度更高。

Conclusion: 动态的、受生物启发的网络设计能有效提升类别不均衡数据的性能。

Abstract: In conventional deep learning, the number of neurons typically remains fixed
during training. However, insights from biology suggest that the human
hippocampus undergoes continuous neuron generation and pruning of neurons over
the course of learning, implying that a flexible allocation of capacity can
contribute to enhance performance. Real-world datasets often exhibit class
imbalance situations where certain classes have far fewer samples than others,
leading to significantly reduce recognition accuracy for minority classes when
relying on fixed size networks.To address the challenge, we propose a method
that periodically adds and removes neurons during training, thereby boosting
representational power for minority classes. By retaining critical features
learned from majority classes while selectively increasing neurons for
underrepresented classes, our approach dynamically adjusts capacity during
training. Importantly, while the number of neurons changes throughout training,
the final network size and structure remain unchanged, ensuring efficiency and
compatibility with deployment.Furthermore, by experiments on three different
datasets and five representative models, we demonstrate that the proposed
method outperforms fixed size networks and shows even greater accuracy when
combined with other imbalance-handling techniques. Our results underscore the
effectiveness of dynamic, biologically inspired network designs in improving
performance on class-imbalanced data.

</details>


### [233] [Iceberg: Enhancing HLS Modeling with Synthetic Data](https://arxiv.org/abs/2507.09948)
*Zijian Ding,Tung Nguyen,Weikai Li,Aditya Grover,Yizhou Sun,Jason Cong*

Main category: cs.LG

TL;DR: 本文研究通过合成数据预训练缩小硬件设计高级综合（HLS）深度学习预测模型泛化差距，提出Iceberg方法，提升了建模精度和设计空间探索（DSE）性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于深度学习的硬件设计高级综合（HLS）预测模型泛化能力不足的问题。

Method: 提出Iceberg合成数据增强方法，扩展大语言模型生成的程序和未见设计配置的弱标签，将弱标签生成方法与上下文模型架构集成以实现元学习。

Result: 在适应六个现实应用的少样本示例时，几何平均建模精度提高86.4%；在适应两个不同测试数据集时，离线DSE性能分别提升2.47倍和1.12倍。

Conclusion: Iceberg方法有效缩小了硬件设计HLS深度学习预测模型的泛化差距，提升了模型性能。

Abstract: Deep learning-based prediction models for High-Level Synthesis (HLS) of
hardware designs often struggle to generalize. In this paper, we study how to
close the generalizability gap of these models through pretraining on synthetic
data and introduce Iceberg, a synthetic data augmentation approach that expands
both large language model (LLM)-generated programs and weak labels of unseen
design configurations. Our weak label generation method is integrated with an
in-context model architecture, enabling meta-learning from actual and proximate
labels. Iceberg improves the geometric mean modeling accuracy by $86.4\%$ when
adapt to six real-world applications with few-shot examples and achieves a
$2.47\times$ and a $1.12\times$ better offline DSE performance when adapting to
two different test datasets. Our open-sourced code is here:
\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}

</details>


### [234] [Hierarchical Job Classification with Similarity Graph Integration](https://arxiv.org/abs/2507.09949)
*Md Ahsanul Kabir,Kareem Abdelfatah,Mohammed Korayem,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: 提出新的表示学习和分类模型提升在线招聘中工作分类准确性，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统文本分类方法难以利用行业类别层次结构，无法满足在线招聘中复杂工作数据分类需求。

Method: 提出新模型，将工作和层次化行业类别嵌入潜在嵌入空间，整合SOC系统和Carotene分类法。

Result: 在大规模招聘数据集实验中，模型利用层次结构和语义特征能力强，显著优于现有方法。

Conclusion: 研究为提高工作分类准确性提供了可靠框架，有助于招聘行业决策。

Abstract: In the dynamic realm of online recruitment, accurate job classification is
paramount for optimizing job recommendation systems, search rankings, and labor
market analyses. As job markets evolve, the increasing complexity of job titles
and descriptions necessitates sophisticated models that can effectively
leverage intricate relationships within job data. Traditional text
classification methods often fall short, particularly due to their inability to
fully utilize the hierarchical nature of industry categories. To address these
limitations, we propose a novel representation learning and classification
model that embeds jobs and hierarchical industry categories into a latent
embedding space. Our model integrates the Standard Occupational Classification
(SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both
graph and hierarchical relationships, thereby improving classification
accuracy. By embedding hierarchical industry categories into a shared latent
space, we tackle cold start issues and enhance the dynamic matching of
candidates to job opportunities. Extensive experimentation on a large-scale
dataset of job postings demonstrates the model's superior ability to leverage
hierarchical structures and rich semantic features, significantly outperforming
existing methods. This research provides a robust framework for improving job
classification accuracy, supporting more informed decision-making in the
recruitment industry.

</details>


### [235] [Radial Neighborhood Smoothing Recommender System](https://arxiv.org/abs/2507.09952)
*Zerui Zhang,Yumou Qiu*

Main category: cs.LG

TL;DR: 本文提出基于观测矩阵行列距离估计潜在空间距离的方法，构建径向邻域估计器RNE，理论分析并实验验证其性能优于现有方法，还能缓解冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中潜在空间有意义、可测量距离的定义难题，以有效捕捉用户-用户、物品-物品、用户-物品关系。

Method: 用观测矩阵行列距离近似潜在空间距离，引入基于经验方差估计器的校正，构建RNE并通过局部核回归进行邻域平滑。

Result: 在模拟和真实数据集上评估，RNE性能优于现有协同过滤和矩阵分解方法。

Conclusion: 提出的RNE能有效进行潜在空间距离估计，还可缓解冷启动问题。

Abstract: Recommender systems inherently exhibit a low-rank structure in latent space.
A key challenge is to define meaningful and measurable distances in the latent
space to capture user-user, item-item, user-item relationships effectively. In
this work, we establish that distances in the latent space can be
systematically approximated using row-wise and column-wise distances in the
observed matrix, providing a novel perspective on distance estimation. To
refine the distance estimation, we introduce the correction based on empirical
variance estimator to account for noise-induced non-centrality. The novel
distance estimation enables a more structured approach to constructing
neighborhoods, leading to the Radial Neighborhood Estimator (RNE), which
constructs neighborhoods by including both overlapped and partially overlapped
user-item pairs and employs neighborhood smoothing via localized kernel
regression to improve imputation accuracy. We provide the theoretical
asymptotic analysis for the proposed estimator. We perform evaluations on both
simulated and real-world datasets, demonstrating that RNE achieves superior
performance compared to existing collaborative filtering and matrix
factorization methods. While our primary focus is on distance estimation in
latent space, we find that RNE also mitigates the ``cold-start'' problem.

</details>


### [236] [Rethinking Inductive Bias in Geographically Neural Network Weighted Regression](https://arxiv.org/abs/2507.09958)
*Zhenyuan Chen*

Main category: cs.LG

TL;DR: 本文重新审视GNNWR归纳偏置，提出推广方法，经基准测试表明其优于经典方法，揭示模型性能与数据特征有关，并指出未来方向。


<details>
  <summary>Details</summary>
Motivation: 当前GNNWR在建模空间非平稳性上存在局限，受固定距离方案和有限归纳偏置的限制。

Method: 结合卷积神经网络、循环神经网络和变换器的概念，将局部感受野、顺序上下文和自注意力引入空间回归以推广GNNWR。

Result: 在不同异质性、噪声和样本量的合成空间数据集上的基准测试显示，GNNWR在捕捉非线性和复杂空间关系上优于经典方法，模型性能与数据特征紧密相关。

Conclusion: 强调归纳偏置在空间建模中的重要性，建议未来探索可学习的空间加权函数、混合神经架构和提高处理非平稳空间数据模型的可解释性。

Abstract: Inductive bias is a key factor in spatial regression models, determining how
well a model can learn from limited data and capture spatial patterns. This
work revisits the inductive biases in Geographically Neural Network Weighted
Regression (GNNWR) and identifies limitations in current approaches for
modeling spatial non-stationarity. While GNNWR extends traditional
Geographically Weighted Regression by using neural networks to learn spatial
weighting functions, existing implementations are often restricted by fixed
distance-based schemes and limited inductive bias. We propose to generalize
GNNWR by incorporating concepts from convolutional neural networks, recurrent
neural networks, and transformers, introducing local receptive fields,
sequential context, and self-attention into spatial regression. Through
extensive benchmarking on synthetic spatial datasets with varying
heterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic
methods in capturing nonlinear and complex spatial relationships. Our results
also reveal that model performance depends strongly on data characteristics,
with local models excelling in highly heterogeneous or small-sample scenarios,
and global models performing better with larger, more homogeneous data. These
findings highlight the importance of inductive bias in spatial modeling and
suggest future directions, including learnable spatial weighting functions,
hybrid neural architectures, and improved interpretability for models handling
non-stationary spatial data.

</details>


### [237] [Text-Driven Causal Representation Learning for Source-Free Domain Generalization](https://arxiv.org/abs/2507.09961)
*Lihua Zhou,Mao Ye,Nianxin Li,Shuaifeng Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.LG

TL;DR: 传统领域泛化数据收集成本高，现有源自由领域泛化方法难以处理特定领域混淆因素。提出TDCRL方法，通过两步操作实现领域不变特征提取，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统领域泛化数据收集和标注成本高，现有源自由领域泛化方法难以应对特定领域混淆因素，限制泛化能力。

Method: 提出TDCRL方法，第一步用数据增强生成风格词向量并结合类信息生成文本嵌入模拟视觉表征；第二步用混淆因素字典训练因果干预网络提取领域不变特征。

Result: 在PACS、VLCS、OfficeHome和DomainNet上的大量实验显示出了最先进的性能。

Conclusion: TDCRL方法基于因果学习，能有效实现鲁棒的领域不变特征提取，确保了良好的泛化能力。

Abstract: Deep learning often struggles when training and test data distributions
differ. Traditional domain generalization (DG) tackles this by including data
from multiple source domains, which is impractical due to expensive data
collection and annotation. Recent vision-language models like CLIP enable
source-free domain generalization (SFDG) by using text prompts to simulate
visual representations, reducing data demands. However, existing SFDG methods
struggle with domain-specific confounders, limiting their generalization
capabilities. To address this issue, we propose TDCRL
(\textbf{T}ext-\textbf{D}riven \textbf{C}ausal \textbf{R}epresentation
\textbf{L}earning), the first method to integrate causal inference into the
SFDG setting. TDCRL operates in two steps: first, it employs data augmentation
to generate style word vectors, combining them with class information to
generate text embeddings to simulate visual representations; second, it trains
a causal intervention network with a confounder dictionary to extract
domain-invariant features. Grounded in causal learning, our approach offers a
clear and effective mechanism to achieve robust, domain-invariant features,
ensuring robust generalization. Extensive experiments on PACS, VLCS,
OfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL
effectiveness in SFDG.

</details>


### [238] [Compliance Minimization via Physics-Informed Gaussian Processes](https://arxiv.org/abs/2507.09968)
*Xiangyu Sun,Amin Yousefpour,Shirin Hosseinmardi,Ramin Bostanabad*

Main category: cs.LG

TL;DR: 提出基于物理信息高斯过程的无网格同步框架解决合规最小化问题，性能优于传统和其他机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习技术解决合规最小化问题存在特征边界差、成本高、缺乏控制设计复杂性机制等局限。

Method: 用高斯过程先验参数化设计和状态变量，采用基于参数化网格卷积注意力网络的神经网络架构，同时最小化合规性、总势能和体积分数约束残差，开发课程训练和数值积分计算方案。

Result: 能快速收敛产生超分辨率拓扑，合规性更小、灰色区域分数更少，可控制精细尺度特征，优于其他机器学习方法。

Conclusion: 所提基于物理信息高斯过程的框架有效解决了现有方法的局限，在合规最小化问题上表现良好。

Abstract: Machine learning (ML) techniques have recently gained significant attention
for solving compliance minimization (CM) problems. However, these methods
typically provide poor feature boundaries, are very expensive, and lack a
systematic mechanism to control the design complexity. Herein, we address these
limitations by proposing a mesh-free and simultaneous framework based on
physics-informed Gaussian processes (GPs). In our approach, we parameterize the
design and state variables with GP priors which have independent kernels but
share a multi-output neural network (NN) as their mean function. The
architecture of this NN is based on Parametric Grid Convolutional Attention
Networks (PGCANs) which not only mitigate spectral bias issues, but also
provide an interpretable mechanism to control design complexity. We estimate
all the parameters of our GP-based representations by simultaneously minimizing
the compliance, total potential energy, and residual of volume fraction
constraint. Importantly, our loss function exclude all data-based residuals as
GPs automatically satisfy them. We also develop computational schemes based on
curriculum training and numerical integration to increase the efficiency and
robustness of our approach which is shown to (1) produce super-resolution
topologies with fast convergence, (2) achieve smaller compliance and less gray
area fraction compared to traditional numerical methods, (3) provide control
over fine-scale features, and (4) outperform competing ML-based methods.

</details>


### [239] [Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach](https://arxiv.org/abs/2507.10014)
*Ali Sarabi,Arash Sarabi,Hao Yan,Beckett Sterner,Petar Jevtić*

Main category: cs.LG

TL;DR: 开发首个图神经网络模型预测亚利桑那州山谷热发病率，模型有效且能提供关键环境驱动因素见解，可用于预警和资源分配。


<details>
  <summary>Details</summary>
Motivation: 球孢子菌病（山谷热）在美国西南部流行地区是重大公共卫生问题，需预测发病率。

Method: 开发图神经网络模型，结合监测病例数据和环境预测因子，探索变量间相关性，考虑滞后效应。

Result: 图神经网络架构能有效模拟山谷热发病趋势，揭示疾病发病率的关键环境驱动因素。

Conclusion: 研究结果可为高风险地区的早期预警系统和疾病预防资源分配提供依据。

Abstract: Coccidioidomycosis, commonly known as Valley Fever, remains a significant
public health concern in endemic regions of the southwestern United States.
This study develops the first graph neural network (GNN) model for forecasting
Valley Fever incidence in Arizona. The model integrates surveillance case data
with environmental predictors using graph structures, including soil
conditions, atmospheric variables, agricultural indicators, and air quality
metrics. Our approach explores correlation-based relationships among variables
influencing disease transmission. The model captures critical delays in disease
progression through lagged effects, enhancing its capacity to reflect complex
temporal dependencies in disease ecology. Results demonstrate that the GNN
architecture effectively models Valley Fever trends and provides insights into
key environmental drivers of disease incidence. These findings can inform early
warning systems and guide resource allocation for disease prevention efforts in
high-risk areas.

</details>


### [240] [Towards Applying Large Language Models to Complement Single-Cell Foundation Models](https://arxiv.org/abs/2507.10039)
*Steven Palayew,Bo Wang,Gary Bader*

Main category: cs.LG

TL;DR: 本文研究LLMs应用于单细胞数据时的性能驱动因素，提出scMPT模型，展示了LLMs补充单细胞基础模型的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞基础模型无法利用生物学文本信息，且对LLMs应用于单细胞数据的性能驱动因素理解不足，缺少互补方法。

Method: 研究生物学见解对LLMs应用于单细胞数据性能的贡献，提出scMPT模型，结合scGPT和LLMs的单细胞表示，还试验了替代融合方法。

Result: scMPT比其组件模型表现更强、更稳定，不同数据集上组件模型性能差距大；试验的替代融合方法展示了结合专业推理模型和scGPT提升性能的潜力。

Conclusion: LLMs有潜力补充单细胞基础模型，推动单细胞分析的改进。

Abstract: Single-cell foundation models such as scGPT represent a significant
advancement in single-cell omics, with an ability to achieve state-of-the-art
performance on various downstream biological tasks. However, these models are
inherently limited in that a vast amount of information in biology exists as
text, which they are unable to leverage. There have therefore been several
recent works that propose the use of LLMs as an alternative to single-cell
foundation models, achieving competitive results. However, there is little
understanding of what factors drive this performance, along with a strong focus
on using LLMs as an alternative, rather than complementary approach to
single-cell foundation models. In this study, we therefore investigate what
biological insights contribute toward the performance of LLMs when applied to
single-cell data, and introduce scMPT; a model which leverages synergies
between scGPT, and single-cell representations from LLMs that capture these
insights. scMPT demonstrates stronger, more consistent performance than either
of its component models, which frequently have large performance gaps between
each other across datasets. We also experiment with alternate fusion methods,
demonstrating the potential of combining specialized reasoning models with
scGPT to improve performance. This study ultimately showcases the potential for
LLMs to complement single-cell foundation models and drive improvements in
single-cell analysis.

</details>


### [241] [On the Efficiency of Training Robust Decision Trees](https://arxiv.org/abs/2507.10048)
*Benedict Gerlach,Marie Anastacio,Holger H. Hoos*

Main category: cs.LG

TL;DR: 本文研究对抗鲁棒决策树训练管道各步骤效率，由选扰动大小、训练模型、验证模型三步组成，发现验证时间与训练时间无关。


<details>
  <summary>Details</summary>
Motivation: 机器学习快速应用，鲁棒训练管道的效率和可持续性有待确立。

Method: 构建包含三步的训练管道：自动选扰动大小、训练模型并评估、验证模型并研究验证时间。

Result: 扰动大小可从较小模型估计，验证时间与训练时间无关。

Conclusion: 对训练管道各步骤效率进行了研究，明确了验证时间和训练时间的关系。

Abstract: As machine learning gets adopted into the industry quickly, trustworthiness
is increasingly in focus. Yet, efficiency and sustainability of robust training
pipelines still have to be established. In this work, we consider a simple
pipeline for training adversarially robust decision trees and investigate the
efficiency of each step. Our pipeline consists of three stages. Firstly, we
choose the perturbation size automatically for each dataset. For that, we
introduce a simple algorithm, instead of relying on intuition or prior work.
Moreover, we show that the perturbation size can be estimated from smaller
models than the one intended for full training, and thus significant gains in
efficiency can be achieved. Secondly, we train state-of-the-art adversarial
training methods and evaluate them regarding both their training time and
adversarial accuracy. Thirdly, we certify the robustness of each of the models
thus obtained and investigate the time required for this. We find that
verification time, which is critical to the efficiency of the full pipeline, is
not correlated with training time.

</details>


### [242] [Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction](https://arxiv.org/abs/2507.10078)
*Hiroki Sakamoto,Kazuhiro Sato*

Main category: cs.LG

TL;DR: 本文提出高效参数减少方法，对含线性SSM的深度学习模型的线性SSM组件应用H²模型降阶技术，实验显示该方法优于现有方法，能将参数减至1/32且不损失性能。


<details>
  <summary>Details</summary>
Motivation: 含线性SSM的深度学习模型参数规模大，在资源受限设备上部署有挑战。

Method: 对线性SSM组件应用H²模型降阶技术。

Result: LRA基准测试结果表明，基于该方法的模型压缩优于使用平衡截断的现有方法，成功将SSM参数减少至1/32，且不损失原模型性能。

Conclusion: 所提出的参数减少方法有效，可用于含线性SSM的深度学习模型压缩。

Abstract: Deep learning models incorporating linear SSMs have gained attention for
capturing long-range dependencies in sequential data. However, their large
parameter sizes pose challenges for deployment on resource-constrained devices.
In this study, we propose an efficient parameter reduction method for these
models by applying $H^{2}$ model order reduction techniques from control theory
to their linear SSM components. In experiments, the LRA benchmark results show
that the model compression based on our proposed method outperforms an existing
method using the Balanced Truncation, while successfully reducing the number of
parameters in the SSMs to $1/32$ without sacrificing the performance of the
original models.

</details>


### [243] [A Variance-Reduced Cubic-Regularized Newton for Policy Optimization](https://arxiv.org/abs/2507.10120)
*Cheng Sun,Zhen Zhang,Shaofu Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we study a second-order approach to policy optimization in
reinforcement learning. Existing second-order methods often suffer from
suboptimal sample complexity or rely on unrealistic assumptions about
importance sampling. To overcome these limitations, we propose VR-CR-PN, a
variance-reduced cubic-regularized policy Newton algorithm. To the best of our
knowledge, this is the first algorithm that integrates Hessian-aided variance
reduction with second-order policy optimization, effectively addressing the
distribution shift problem and achieving best-known sample complexity under
general nonconvex conditions but without the need for importance sampling. We
theoretically establish that VR-CR-PN achieves a sample complexity of
$\tilde{\mathcal{O}}(\epsilon^{-3})$ to reach an $\epsilon$-second-order
stationary point, significantly improving upon the previous best result of
$\tilde{\mathcal{O}}(\epsilon^{-3.5})$ under comparable assumptions. As an
additional contribution, we introduce a novel Hessian estimator for the
expected return function, which admits a uniform upper bound independent of the
horizon length $H$, allowing the algorithm to achieve horizon-independent
sample complexity.

</details>


### [244] [MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping](https://arxiv.org/abs/2507.10158)
*Obaidullah Zaland,Erik Elmroth,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 提出用于机器人抓取的多层联邦学习方法MTF - Grasp，在数据集上表现优于传统联邦学习。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习在机器人抓取任务中，因数据非独立同分布且数量少导致性能下降，缺乏相关探索。

Method: 提出MTF - Grasp方法，利用机器人数据质量和数量选出‘顶级’机器人训练初始种子模型并分发给‘低级’机器人。

Result: 在数量偏斜的康奈尔和雅卡尔抓取数据集上，该方法比传统联邦学习设置性能高8%。

Conclusion: MTF - Grasp能有效解决机器人抓取任务中联邦学习因数据问题导致的性能下降问题。

Abstract: Federated Learning (FL) is a promising machine learning paradigm that enables
participating devices to train privacy-preserved and collaborative models. FL
has proven its benefits for robotic manipulation tasks. However, grasping tasks
lack exploration in such settings where robots train a global model without
moving data and ensuring data privacy. The main challenge is that each robot
learns from data that is nonindependent and identically distributed (non-IID)
and of low quantity. This exhibits performance degradation, particularly in
robotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL
approach for robotic grasping, acknowledging the unique challenges posed by the
non-IID data distribution across robots, including quantitative skewness.
MTF-Grasp harnesses data quality and quantity across robots to select a set of
"top-level" robots with better data distribution and higher sample count. It
then utilizes top-level robots to train initial seed models and distribute them
to the remaining "low-level" robots, reducing the risk of model performance
degradation in low-level robots. Our approach outperforms the conventional FL
setup by up to 8% on the quantity-skewed Cornell and Jacquard grasping
datasets.

</details>


### [245] [Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach](https://arxiv.org/abs/2507.10170)
*Wuyang Zhou,Giorgos Iacovides,Kriton Konstantinidis,Ilya Kisil,Danilo Mandic*

Main category: cs.LG

TL;DR: 本文通过实例和可视化解释张量网络（TN）秩概念，介绍根据领域知识选TN秩的方法，助力读者理解和应用。


<details>
  <summary>Details</summary>
Motivation: TN秩缺乏通用意义和直观解释，常被当作经验调整的超参数，需对其概念进行阐释。

Method: 先说明领域知识如何指导常用模型TN秩选择，再用图形方法处理复杂TN结构。

Result: 揭示TN秩与张量展开矩阵秩的关系，避免复杂张量代数。

Conclusion: 期望读者能清晰统一理解TN秩概念，具备物理洞察和直觉以支持张量方法应用和教学。

Abstract: Tensor Network (TN) decompositions have emerged as an indispensable tool in
Big Data analytics owing to their ability to provide compact low-rank
representations, thus alleviating the ``Curse of Dimensionality'' inherent in
handling higher-order data. At the heart of their success lies the concept of
TN ranks, which governs the efficiency and expressivity of TN decompositions.
However, unlike matrix ranks, TN ranks often lack a universal meaning and an
intuitive interpretation, with their properties varying significantly across
different TN structures. Consequently, TN ranks are frequently treated as
empirically tuned hyperparameters, rather than as key design parameters
inferred from domain knowledge. The aim of this Lecture Note is therefore to
demystify the foundational yet frequently misunderstood concept of TN ranks
through real-life examples and intuitive visualizations. We begin by
illustrating how domain knowledge can guide the selection of TN ranks in
widely-used models such as the Canonical Polyadic (CP) and Tucker
decompositions. For more complex TN structures, we employ a self-explanatory
graphical approach that generalizes to tensors of arbitrary order. Such a
perspective naturally reveals the relationship between TN ranks and the
corresponding ranks of tensor unfoldings (matrices), thereby circumventing
cumbersome multi-index tensor algebra while facilitating domain-informed TN
design. It is our hope that this Lecture Note will equip readers with a clear
and unified understanding of the concept of TN rank, along with the necessary
physical insight and intuition to support the selection, explainability, and
deployment of tensor methods in both practical applications and educational
contexts.

</details>


### [246] [Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS](https://arxiv.org/abs/2507.10172)
*Ruizhe Yu Xia,Jeremy Gow,Simon Lucas*

Main category: cs.LG

TL;DR: 探索用无监督CNN - LSTM自编码器模型从MicroRTS低级游戏轨迹数据中获取潜在表示，用于游戏风格识别，减少对领域专业知识的依赖。


<details>
  <summary>Details</summary>
Motivation: 以往游戏风格识别方法依赖领域知识构建特征，有局限性，本研究旨在减少对领域专业知识及相关偏差的依赖。

Method: 使用无监督CNN - LSTM自编码器模型直接从MicroRTS的低级游戏轨迹数据中获取潜在表示。

Result: 该方法在潜在空间中实现了不同游戏代理的有意义分离。

Conclusion: 此方法可用于引导研究AI玩家中多样化游戏风格的探索，减少对领域专业知识的依赖。

Abstract: Play style identification can provide valuable game design insights and
enable adaptive experiences, with the potential to improve game playing agents.
Previous work relies on domain knowledge to construct play trace
representations using handcrafted features. More recent approaches incorporate
the sequential structure of play traces but still require some level of domain
abstraction. In this study, we explore the use of unsupervised CNN-LSTM
autoencoder models to obtain latent representations directly from low-level
play trace data in MicroRTS. We demonstrate that this approach yields a
meaningful separation of different game playing agents in the latent space,
reducing reliance on domain expertise and its associated biases. This latent
space is then used to guide the exploration of diverse play styles within
studied AI players.

</details>


### [247] [T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs](https://arxiv.org/abs/2507.10183)
*Alireza Dizaji,Benedict Aaron Tjandra,Mehrab Hamidi,Shenyang Huang,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: 本文引入Temporal Graph Reasoning Benchmark (T - GRAB) 测试TGNNs时间推理能力，评估11种方法并揭示其泛化时间模式的缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前TGNNs是否能有效捕捉核心时间模式尚不明确，需要系统性评估其时间推理能力。

Method: 引入T - GRAB，包含一系列合成任务，隔离关键时间技能，对11种时间图学习方法进行评估。

Result: 发现当前模型在泛化时间模式上存在根本缺陷。

Conclusion: 研究揭示当前模型局限，指出传统基准隐藏的挑战，推动更强时间推理能力架构的发展。

Abstract: Dynamic graph learning methods have recently emerged as powerful tools for
modelling relational data evolving through time. However, despite extensive
benchmarking efforts, it remains unclear whether current Temporal Graph Neural
Networks (TGNNs) effectively capture core temporal patterns such as
periodicity, cause-and-effect, and long-range dependencies. In this work, we
introduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set
of synthetic tasks designed to systematically probe the capabilities of TGNNs
to reason across time. T-GRAB provides controlled, interpretable tasks that
isolate key temporal skills: counting/memorizing periodic repetitions,
inferring delayed causal effects, and capturing long-range dependencies over
both spatial and temporal dimensions. We evaluate 11 temporal graph learning
methods on these tasks, revealing fundamental shortcomings in their ability to
generalize temporal patterns. Our findings offer actionable insights into the
limitations of current models, highlight challenges hidden by traditional
real-world benchmarks, and motivate the development of architectures with
stronger temporal reasoning abilities. The code for T-GRAB can be found at:
https://github.com/alirezadizaji/T-GRAB.

</details>


### [248] [Learning Private Representations through Entropy-based Adversarial Training](https://arxiv.org/abs/2507.10194)
*Tassilo Klein,Moin Nabi*

Main category: cs.LG

TL;DR: 提出对抗性表征学习方法净化敏感内容，引入焦点熵变体，在多基准测试中展示可行性，结果显示有高目标效用和适度隐私泄漏。


<details>
  <summary>Details</summary>
Motivation: 在保留用户隐私的同时学习具有高预测能力的表征。

Method: 提出对抗性表征学习方法净化敏感内容，引入焦点熵变体减轻现有基于熵方法的潜在信息泄漏。

Result: 在多个基准测试中展示了可行性，结果显示具有高目标效用和适度的隐私泄漏。

Conclusion: 该方法能在保留用户隐私的同时学习到具有高预测能力的表征。

Abstract: How can we learn a representation with high predictive power while preserving
user privacy? We present an adversarial representation learning method for
sanitizing sensitive content from the learned representation. Specifically, we
introduce a variant of entropy - focal entropy, which mitigates the potential
information leakage of the existing entropy-based approaches. We showcase
feasibility on multiple benchmarks. The results suggest high target utility at
moderate privacy leakage.

</details>


### [249] [A Graph Sufficiency Perspective for Neural Networks](https://arxiv.org/abs/2507.10215)
*Cencheng Shen,Yuexiao Dong*

Main category: cs.LG

TL;DR: 本文通过图变量和统计充分性分析神经网络，建立层输出对输入充分的条件，证明渐近充分性，还展示有限宽度网络也可实现充分性，提供新统计理解。


<details>
  <summary>Details</summary>
Motivation: 为神经网络提供新的统计理解，将统计充分性、图论表示和深度学习联系起来。

Method: 将神经网络层解释为基于图的变换，神经元作为输入和学习的锚点之间的成对函数，建立层输出对输入充分的条件。

Result: 在密集锚点假设下，证明无限宽度极限下渐近充分性成立且训练中保持；在区域分离输入分布和构造适当锚点的情况下，有限宽度网络也可实现充分性。

Conclusion: 该框架涵盖多种网络层和激活函数，为神经网络提供了新的统计理解。

Abstract: This paper analyzes neural networks through graph variables and statistical
sufficiency. We interpret neural network layers as graph-based transformations,
where neurons act as pairwise functions between inputs and learned anchor
points. Within this formulation, we establish conditions under which layer
outputs are sufficient for the layer inputs, that is, each layer preserves the
conditional distribution of the target variable given the input variable. Under
dense anchor point assumptions, we prove that asymptotic sufficiency holds in
the infinite-width limit and is preserved throughout training. To align more
closely with practical architectures, we further show that sufficiency can be
achieved with finite-width networks by assuming region-separated input
distributions and constructing appropriate anchor points. Our framework covers
fully connected layers, general pairwise functions, ReLU and sigmoid
activations, and convolutional neural networks. This work bridges statistical
sufficiency, graph-theoretic representations, and deep learning, providing a
new statistical understanding of neural networks.

</details>


### [250] [Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients](https://arxiv.org/abs/2507.10241)
*Vikas Dwivedi,Balaji Srinivasan,Monica Sigovan,Bruno Sixou*

Main category: cs.LG

TL;DR: 本文提出KAPI - ELM解决含局部尖锐梯度的偏微分方程问题，结合贝叶斯优化和最小二乘法，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: PI - ELMs固定随机初始化输入层限制捕捉尖锐梯度能力，需改进以提升性能。

Method: 引入轻量级贝叶斯优化框架学习输入权重统计分布的超参数，结合贝叶斯优化输入层分布参数与输出层最小二乘法优化。

Result: KAPI - ELM在正反问题中达最优精度，在刚性PDE区域表现佳且可调参数少。

Conclusion: KAPI - ELM是可扩展、可解释、通用的物理信息学习框架，尤其适用于刚性PDE区域。

Abstract: This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning
Machine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of
PI-ELM designed to solve both forward and inverse Partial Differential Equation
(PDE) problems involving localized sharp gradients. While PI-ELMs outperform
the traditional Physics-Informed Neural Networks (PINNs) in speed due to their
single-shot, least square optimization, this advantage comes at a cost: their
fixed, randomly initialized input layer limits their ability to capture sharp
gradients. To overcome this limitation, we introduce a lightweight Bayesian
Optimization (BO) framework that, instead of adjusting each input layer
parameter individually as in traditional backpropagation, learns a small set of
hyperparameters defining the statistical distribution from which the input
weights are drawn. This novel distributional optimization strategy -- combining
BO for input layer distributional parameters with least-squares optimization
for output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's
speed while matching or exceeding the expressiveness of PINNs. We validate the
proposed methodology on several challenging forward and inverse PDE benchmarks,
including a 1D singularly perturbed convection-diffusion equation, a 2D Poisson
equation with sharp localized sources, and a time-dependent advection equation.
Notably, KAPI-ELM achieves state-of-the-art accuracy in both forward and
inverse settings. In stiff PDE regimes, it matches or even outperforms advanced
methods such as the Extended Theory of Functional Connections (XTFC), while
requiring nearly an order of magnitude fewer tunable parameters. These results
establish the potential of KAPI-ELM as a scalable, interpretable, and
generalizable physics-informed learning framework, especially in stiff PDE
regimes.

</details>


### [251] [Conditional Chemical Language Models are Versatile Tools in Drug Discovery](https://arxiv.org/abs/2507.10273)
*Lu Zhu,Emmanuel Noutahi*

Main category: cs.LG

TL;DR: 提出SAFE - T化学建模框架，在多基准测试中表现佳，能统一评分和生成以加速药物发现。


<details>
  <summary>Details</summary>
Motivation: 现有生成式化学语言模型因缺乏可靠奖励信号和输出缺乏可解释性，在药物发现中的应用受限。

Method: 提出SAFE - T框架，基于生物提示对基于片段的分子序列条件似然进行建模，支持目标导向生成。

Result: 在多个预测和生成基准测试中，SAFE - T表现与现有方法相当或更好，且速度更快，片段级归因显示其捕捉到已知构效关系。

Conclusion: 条件生成式化学语言模型可统一评分和生成，加速早期药物发现。

Abstract: Generative chemical language models (CLMs) have demonstrated strong
capabilities in molecular design, yet their impact in drug discovery remains
limited by the absence of reliable reward signals and the lack of
interpretability in their outputs. We present SAFE-T, a generalist chemical
modeling framework that conditions on biological context -- such as protein
targets or mechanisms of action -- to prioritize and design molecules without
relying on structural information or engineered scoring functions. SAFE-T
models the conditional likelihood of fragment-based molecular sequences given a
biological prompt, enabling principled scoring of molecules across tasks such
as virtual screening, drug-target interaction prediction, and activity cliff
detection. Moreover, it supports goal-directed generation by sampling from this
learned distribution, aligning molecular design with biological objectives. In
comprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA,
ACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves
performance comparable to or better than existing approaches while being
significantly faster. Fragment-level attribution further reveals that SAFE-T
captures known structure-activity relationships, supporting interpretable and
biologically grounded design. Together with its computational efficiency, these
results demonstrate that conditional generative CLMs can unify scoring and
generation to accelerate early-stage drug discovery.

</details>


### [252] [Recognizing Dementia from Neuropsychological Tests with State Space Models](https://arxiv.org/abs/2507.10311)
*Liming Wang,Saurabhchand Bhati,Cody Karjadi,Rhoda Au,James Glass*

Main category: cs.LG

TL;DR: 提出基于状态空间模型的新型自动痴呆分类框架Demenba，在细粒度痴呆分类上优于先前方法，与大语言模型融合有额外提升。


<details>
  <summary>Details</summary>
Motivation: 早期检测痴呆对及时医疗干预和改善患者预后至关重要，传统神经心理测试手动评分，自动痴呆分类系统旨在从语音记录推断认知衰退。

Method: 提出基于状态空间模型的Demenba框架，在超1000小时认知评估数据上训练。

Result: 在细粒度痴呆分类上比先前方法高21%，使用参数更少，与大语言模型融合有额外提升。

Conclusion: 为更透明和可扩展的痴呆评估工具铺平道路。

Abstract: Early detection of dementia is critical for timely medical intervention and
improved patient outcomes. Neuropsychological tests are widely used for
cognitive assessment but have traditionally relied on manual scoring. Automatic
dementia classification (ADC) systems aim to infer cognitive decline directly
from speech recordings of such tests. We propose Demenba, a novel ADC framework
based on state space models, which scale linearly in memory and computation
with sequence length. Trained on over 1,000 hours of cognitive assessments
administered to Framingham Heart Study participants, some of whom were
diagnosed with dementia through adjudicated review, our method outperforms
prior approaches in fine-grained dementia classification by 21\%, while using
fewer parameters. We further analyze its scaling behavior and demonstrate that
our model gains additional improvement when fused with large language models,
paving the way for more transparent and scalable dementia assessment tools.
Code: https://anonymous.4open.science/r/Demenba-0861

</details>


### [253] [MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data](https://arxiv.org/abs/2507.10334)
*Mahmoud Bekhit,Ahmad Salah,Ahmed Salim Alrawahi,Tarek Attia,Ahmed Ali,Esraa Eldesokey,Ahmed Fathalla*

Main category: cs.LG

TL;DR: 对IMU衍生的动作捕捉时间序列数据缺失值插补方法进行综合比较分析，引入公开数据集，实验表明多变量插补方法表现更优，为未来研究提供基线。


<details>
  <summary>Details</summary>
Motivation: IMU动作捕捉数据常存在缺失值问题，缺乏系统的插补方法性能评估。

Method: 对统计、机器学习和深度学习插补方法进行综合比较分析，考虑三种不同情境，引入公开数据集，模拟三种缺失机制。

Result: 多变量插补框架始终优于单变量方法，高级模型如GAIN和迭代插补器在复杂缺失情况下精度最高。

Conclusion: 为未来研究提供关键基线，为改善动作捕捉数据分析的完整性和鲁棒性提供实用建议。

Abstract: Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs)
is vital for applications in sports science, but its utility is often
compromised by missing data. Despite numerous imputation techniques, a
systematic performance evaluation for IMU-derived MoCap time-series data is
lacking. We address this gap by conducting a comprehensive comparative analysis
of statistical, machine learning, and deep learning imputation methods. Our
evaluation considers three distinct contexts: univariate time-series,
multivariate across subjects, and multivariate across kinematic angles. To
facilitate this benchmark, we introduce the first publicly available MoCap
dataset designed specifically for imputation, featuring data from 53 karate
practitioners. We simulate three controlled missingness mechanisms: missing
completely at random (MCAR), block missingness, and a novel value-dependent
pattern at signal transition points. Our experiments, conducted on 39 kinematic
variables across all subjects, reveal that multivariate imputation frameworks
consistently outperform univariate approaches, particularly for complex
missingness. For instance, multivariate methods achieve up to a 50% mean
absolute error reduction (MAE from 10.8 to 5.8) compared to univariate
techniques for transition point missingness. Advanced models like Generative
Adversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the
highest accuracy in these challenging scenarios. This work provides a critical
baseline for future research and offers practical recommendations for improving
the integrity and robustness of Mo-Cap data analysis.

</details>


### [254] [Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions](https://arxiv.org/abs/2507.10345)
*Yuwen Li,Guozhi Zhang*

Main category: cs.LG

TL;DR: 本文研究ReLU神经网络对Korobov函数的Lp和W1p范数逼近误差，推导近最优超逼近误差界，结果优于经典界，表明神经网络表达能力受维数灾难影响小。


<details>
  <summary>Details</summary>
Motivation: 研究ReLU神经网络对Korobov函数的Lp和W1p范数逼近误差，获取更优误差界。

Method: 利用稀疏网格有限元和比特提取技术进行分析。

Result: 对于各方向Lp混合导数为m阶的目标函数，推导出Lp范数下阶为2m、W1p范数下阶为2m - 2的近最优超逼近误差界，结果优于经典L∞和H1范数误差界。

Conclusion: 神经网络的表达能力在很大程度上不受维数灾难的影响。

Abstract: This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU
neural networks for Korobov functions. In terms of network width and depth, we
derive nearly optimal super-approximation error bounds of order $2m$ in the
$L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with
$L_p$ mixed derivative of order $m$ in each direction. The analysis leverages
sparse grid finite elements and the bit extraction technique. Our results
improve upon classical lowest order $L_\infty$ and $H^1$ norm error bounds and
demonstrate that the expressivity of neural networks is largely unaffected by
the curse of dimensionality.

</details>


### [255] [Parallel Sampling of Diffusion Models on $SO(3)$](https://arxiv.org/abs/2507.10347)
*Yan-Ting Chen,Hao-Wei Chen,Tsu-Ching Hsiao,Chun-Yi Lee*

Main category: cs.LG

TL;DR: 设计算法加速SO(3)流形上的扩散过程，在解决姿态模糊问题上速度提升4.9倍且无任务奖励损失。


<details>
  <summary>Details</summary>
Motivation: 扩散模型固有的顺序性导致去噪耗时，需加速扩散过程。

Method: 在SO(3)空间采用数值皮卡迭代。

Result: 算法在解决姿态模糊问题上实现了最高4.9倍的加速，显著降低单样本生成延迟。

Conclusion: 所提算法能有效加速SO(3)流形上的扩散过程，且不降低任务奖励。

Abstract: In this paper, we design an algorithm to accelerate the diffusion process on
the $SO(3)$ manifold. The inherently sequential nature of diffusion models
necessitates substantial time for denoising perturbed data. To overcome this
limitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$
space. We demonstrate our algorithm on an existing method that employs
diffusion models to address the pose ambiguity problem. Moreover, we show that
this acceleration advantage occurs without any measurable degradation in task
reward. The experiments reveal that our algorithm achieves a speed-up of up to
4.9$\times$, significantly reducing the latency for generating a single sample.

</details>


### [256] [Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning](https://arxiv.org/abs/2507.10348)
*Yichen Li*

Main category: cs.LG

TL;DR: 提出FedFD用于异构联邦学习，结合正交投影进行特征蒸馏，实验显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法结合异构联邦学习和集成蒸馏效果不佳且训练不稳定，主要聚焦logit蒸馏无法补偿异构模型知识偏差。

Method: 提出FedFD，采用基于特征的集成联邦知识蒸馏范式，服务器上的全局模型为每个客户端模型架构维护投影层，用正交技术重新参数化投影层。

Result: 广泛实验表明FedFD比现有方法性能更优。

Conclusion: FedFD是稳定且高效的模型异构联邦学习特征蒸馏方法。

Abstract: Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing
attention for its ability to aggregate knowledge from heterogeneous models
while keeping private data locally. To better aggregate knowledge from clients,
ensemble distillation, as a widely used and effective technique, is often
employed after global aggregation to enhance the performance of the global
model. However, simply combining Hetero-FL and ensemble distillation does not
always yield promising results and can make the training process unstable. The
reason is that existing methods primarily focus on logit distillation, which,
while being model-agnostic with softmax predictions, fails to compensate for
the knowledge bias arising from heterogeneous models. To tackle this challenge,
we propose a stable and efficient Feature Distillation for model-heterogeneous
Federated learning, dubbed FedFD, that can incorporate aligned feature
information via orthogonal projection to integrate knowledge from heterogeneous
models better. Specifically, a new feature-based ensemble federated knowledge
distillation paradigm is proposed. The global model on the server needs to
maintain a projection layer for each client-side model architecture to align
the features separately. Orthogonal techniques are employed to re-parameterize
the projection layer to mitigate knowledge bias from heterogeneous models and
thus maximize the distilled knowledge. Extensive experiments show that FedFD
achieves superior performance compared to state-of-the-art methods.

</details>


### [257] [TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting](https://arxiv.org/abs/2507.10349)
*Zhiyuan Zhao,Sitan Yang,Kin G. Olivares,Boris N. Oreshkin,Stan Vitebsky,Michael W. Mahoney,B. Aditya Prakash,Dmitry Efimov*

Main category: cs.LG

TL;DR: 提出Temporal - Aligned Transformer (TAT)用于多水平时间序列需求预测，在电商数据集上验证可提升峰值需求预测准确性。


<details>
  <summary>Details</summary>
Motivation: 多水平时间序列需求预测在电商和实体零售供应链管理中很重要，高风险销售活动时准确预测需求峰值极具挑战，需要更好的预测方法。

Method: 提出TAT，由编码器和解码器组成，二者都嵌入Temporal Alignment Attention (TAA)，利用先验上下文变量提升预测性能。

Result: 在电商零售商的两个大规模专有数据集上进行实证分析，TAT在峰值需求预测上最多提升30%的准确性，整体性能也有竞争力。

Conclusion: TAT是一种有效的多水平时间序列需求预测模型，能有效提升峰值需求预测准确性。

Abstract: Multi-horizon time series forecasting has many practical applications such as
demand forecasting. Accurate demand prediction is critical to help make buying
and inventory decisions for supply chain management of e-commerce and physical
retailers, and such predictions are typically required for future horizons
extending tens of weeks. This is especially challenging during high-stake sales
events when demand peaks are particularly difficult to predict accurately.
However, these events are important not only for managing supply chain
operations but also for ensuring a seamless shopping experience for customers.
To address this challenge, we propose Temporal-Aligned Transformer (TAT), a
multi-horizon forecaster leveraging apriori-known context variables such as
holiday and promotion events information for improving predictive performance.
Our model consists of an encoder and decoder, both embedded with a novel
Temporal Alignment Attention (TAA), designed to learn context-dependent
alignment for peak demand forecasting. We conduct extensive empirical analysis
on two large-scale proprietary datasets from a large e-commerce retailer. We
demonstrate that TAT brings up to 30% accuracy improvement on peak demand
forecasting while maintaining competitive overall performance compared to other
state-of-the-art methods.

</details>


### [258] [Enhanced DeepONet for 1-D consolidation operator learning: an architectural investigation](https://arxiv.org/abs/2507.10368)
*Yongjin Choi,Chenying Liu,Jorge Macedo*

Main category: cs.LG

TL;DR: 本文系统评估几种DeepONet架构用于一维固结问题，提出改进架构，实现计算加速，凸显其在岩土工程应用潜力。


<details>
  <summary>Details</summary>
Motivation: DeepONet在岩土工程应用有限，需评估其在一维固结问题中的表现。

Method: 考虑三种初始架构，针对问题提出Trunknet Fourier特征增强的DeepONet架构。

Result: Model 3优于标准配置，但有局限性；Model 4解决了局限性，所有架构比传统求解器快1.5 - 100倍，Model 4最有效。

Conclusion: DeepONets有潜力在岩土应用中实现高效、可泛化的代理建模，推动科学机器学习在岩土领域的集成。

Abstract: Deep Operator Networks (DeepONets) have emerged as a powerful surrogate
modeling framework for learning solution operators in PDE-governed systems.
While their use is expanding across engineering disciplines, applications in
geotechnical engineering remain limited. This study systematically evaluates
several DeepONet architectures for the one-dimensional consolidation problem.
We initially consider three architectures: a standard DeepONet with the
coefficient of consolidation embedded in the branch net (Models 1 and 2), and a
physics-inspired architecture with the coefficient embedded in the trunk net
(Model 3). Results show that Model 3 outperforms the standard configurations
(Models 1 and 2) but still has limitations when the target solution (excess
pore pressures) exhibits significant variation. To overcome this limitation, we
propose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses
the identified limitations by capturing rapidly varying functions. All proposed
architectures achieve speedups ranging from 1.5 to 100 times over traditional
explicit and implicit solvers, with Model 4 being the most efficient. Larger
computational savings are expected for more complex systems than the explored
1D case, which is promising. Overall, the study highlights the potential of
DeepONets to enable efficient, generalizable surrogate modeling in geotechnical
applications, advancing the integration of scientific machine learning in
geotechnics, which is at an early stage.

</details>


### [259] [Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis](https://arxiv.org/abs/2507.10382)
*Yue Ding,Conor McCarthy,Kevin O'Shea,Mingming Liu*

Main category: cs.LG

TL;DR: 本文提出基于云、由大语言模型驱动的共享电动出行平台，集成移动应用做个性化路线推荐，评估优化模块和大语言模型驱动的RAG框架。


<details>
  <summary>Details</summary>
Motivation: 随着智能出行和共享电动出行服务兴起，用户需求增长，需提供全面端到端解决方案。

Method: 构建基于云、由大语言模型驱动的共享电动出行平台，集成移动应用，基于不同交通场景的出行时间和成本评估优化模块，用多种评估方法在模式级别评估大语言模型驱动的RAG框架。

Result: 使用XiYanSQL的模式级RAG在系统操作员查询上平均执行准确率为0.81，在用户查询上为0.98。

Conclusion: 提出的平台和评估方法具有一定的有效性。

Abstract: With the rise of smart mobility and shared e-mobility services, numerous
advanced technologies have been applied to this field. Cloud-based traffic
simulation solutions have flourished, offering increasingly realistic
representations of the evolving mobility landscape. LLMs have emerged as
pioneering tools, providing robust support for various applications, including
intelligent decision-making, user interaction, and real-time traffic analysis.
As user demand for e-mobility continues to grow, delivering comprehensive
end-to-end solutions has become crucial. In this paper, we present a
cloud-based, LLM-powered shared e-mobility platform, integrated with a mobile
application for personalized route recommendations. The optimization module is
evaluated based on travel time and cost across different traffic scenarios.
Additionally, the LLM-powered RAG framework is evaluated at the schema level
for different users, using various evaluation methods. Schema-level RAG with
XiYanSQL achieves an average execution accuracy of 0.81 on system operator
queries and 0.98 on user queries.

</details>


### [260] [Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model](https://arxiv.org/abs/2507.10385)
*Md. Ahsanul Kabir,Mohammad Al Hasan,Aritra Mandal,Liyang Hao,Ishita Khan,Daniel Tunkelang,Zhe Wu*

Main category: cs.LG

TL;DR: 本文提出用依赖感知的基于Transformer的语言模型TagBERT解决电商搜索查询重写问题，实验表明其性能优于多种模型。


<details>
  <summary>Details</summary>
Motivation: 电商搜索引擎检索相关商品面临查询模糊、词汇不匹配等挑战，现有方法未利用查询词的语义标签。

Method: 将查询重写视为令牌分类任务，设计依赖感知的基于Transformer的语言模型TagBERT，利用令牌语义标签学习查询短语嵌入。

Result: 在大型真实电商数据集上实验，TagBERT在重要令牌分类任务上性能优于BERT、eBERT等多种竞争模型。

Conclusion: TagBERT在电商查询重写的令牌分类任务中表现出色，能有效利用语义标签提升性能。

Abstract: The major task of any e-commerce search engine is to retrieve the most
relevant inventory items, which best match the user intent reflected in a
query. This task is non-trivial due to many reasons, including ambiguous
queries, misaligned vocabulary between buyers, and sellers, over- or
under-constrained queries by the presence of too many or too few tokens. To
address these challenges, query reformulation is used, which modifies a user
query through token dropping, replacement or expansion, with the objective to
bridge semantic gap between query tokens and users' search intent. Early
methods of query reformulation mostly used statistical measures derived from
token co-occurrence frequencies from selective user sessions having clicks or
purchases. In recent years, supervised deep learning approaches, specifically
transformer-based neural language models, or sequence-to-sequence models are
being used for query reformulation task. However, these models do not utilize
the semantic tags of a query token, which are significant for capturing user
intent of an e-commerce query. In this work, we pose query reformulation as a
token classification task, and solve this task by designing a dependency-aware
transformer-based language model, TagBERT, which makes use of semantic tags of
a token for learning superior query phrase embedding. Experiments on large,
real-life e-commerce datasets show that TagBERT exhibits superior performance
than plethora of competing models, including BERT, eBERT, and
Sequence-to-Sequence transformer model for important token classification task.

</details>


### [261] [Anticipating the Selectivity of Cyclization Reaction Pathways with Neural Network Potentials](https://arxiv.org/abs/2507.10400)
*Nicholas Casetti,Dylan Anstine,Olexandr Isayev,Connor W. Coley*

Main category: cs.LG

TL;DR: 提出适合环化反应的机理搜索策略，结合图枚举和机器学习，用NNP评估反应路径，验证其能力。


<details>
  <summary>Details</summary>
Motivation: 含多个协同键变化的反应使搜索过程复杂，需策略加速复杂反应探索。

Method: 结合图枚举方案和机器学习技术过滤中间体，用神经网络势AIMNet2 - rxn评估候选反应路径。

Result: 评估了NNP估计活化能的能力，正确预测立体选择性，重现天然产物合成中的复杂步骤。

Conclusion: 所提出的机制搜索策略可有效用于环化等复杂反应的探索。

Abstract: Reaction mechanism search tools have demonstrated the ability to provide
insights into likely products and rate-limiting steps of reacting systems.
However, reactions involving several concerted bond changes - as can be found
in many key steps of natural product synthesis - can complicate the search
process. To mitigate these complications, we present a mechanism search
strategy particularly suited to help expedite exploration of an exemplary
family of such complex reactions, cyclizations. We provide a cost-effective
strategy for identifying relevant elementary reaction steps by combining
graph-based enumeration schemes and machine learning techniques for
intermediate filtering. Key to this approach is our use of a neural network
potential (NNP), AIMNet2-rxn, for computational evaluation of each candidate
reaction pathway. In this article, we evaluate the NNP's ability to estimate
activation energies, demonstrate the correct anticipation of stereoselectivity,
and recapitulate complex enabling steps in natural product synthesis.

</details>


### [262] [Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning](https://arxiv.org/abs/2507.10401)
*Ryan Bausback,Jingqiao Tang,Lu Lu,Feng Bao,Toan Huynh*

Main category: cs.LG

TL;DR: 本文提出随机算子网络（SON）框架用于算子学习中的不确定性量化，结合SNN与DeepONet，通过特定方式使SON学习算子不确定性，并在2D和3D噪声算子复制中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 开发一种新的算子学习中不确定性量化的框架。

Method: 将SON结合随机神经网络（SNN）的随机最优控制概念与DeepONet，将分支网络表示为SDE并通过伴随BSDE反向传播，在SGD更新中用随机最大值原理的哈密顿量梯度替代损失函数梯度。

Result: SON能通过扩散参数学习算子中的不确定性。

Conclusion: SON在复制2D和3D噪声算子时是有效的。

Abstract: We develop a novel framework for uncertainty quantification in operator
learning, the Stochastic Operator Network (SON). SON combines the stochastic
optimal control concepts of the Stochastic Neural Network (SNN) with the
DeepONet. By formulating the branch net as an SDE and backpropagating through
the adjoint BSDE, we replace the gradient of the loss function with the
gradient of the Hamiltonian from Stohastic Maximum Principle in the SGD update.
This allows SON to learn the uncertainty present in operators through its
diffusion parameters. We then demonstrate the effectiveness of SON when
replicating several noisy operators in 2D and 3D.

</details>


### [263] [Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study](https://arxiv.org/abs/2507.10409)
*Amine Lbath,Ibtissam Labriji*

Main category: cs.LG

TL;DR: 研究聚焦DeepRX模型，评估其能耗，对比训练和推理阶段能耗动态，应用知识蒸馏训练紧凑模型，证明知识蒸馏在实现节能AI方案上有效。


<details>
  <summary>Details</summary>
Motivation: 解决AI/ML模型中能源效率与性能平衡的挑战。

Method: 评估DeepRX能耗，考虑FLOPs/Watt和FLOPs/clock等因素；对比训练和推理阶段能耗；应用知识蒸馏训练紧凑模型；用不同学生模型大小、最优教师大小和KD超参数进行实验；通过对比误码率和信干噪比衡量性能。

Result: 估计能耗与实际能耗一致，受内存访问模式影响；蒸馏模型在各信干噪比水平下误差下限更低。

Conclusion: 知识蒸馏能有效实现节能AI解决方案。

Abstract: This study addresses the challenge of balancing energy efficiency with
performance in AI/ML models, focusing on DeepRX, a deep learning receiver based
on a fully convolutional ResNet architecture. We evaluate the energy
consumption of DeepRX, considering factors including FLOPs/Watt and
FLOPs/clock, and find consistency between estimated and actual energy usage,
influenced by memory access patterns. The research extends to comparing energy
dynamics during training and inference phases. A key contribution is the
application of knowledge distillation (KD) to train a compact DeepRX
\textit{student} model that emulates the performance of the \textit{teacher}
model but with reduced energy consumption. We experiment with different student
model sizes, optimal teacher sizes, and KD hyperparameters. Performance is
measured by comparing the Bit Error Rate (BER) performance versus
Signal-to-Interference \& Noise Ratio (SINR) values of the distilled model and
a model trained from scratch. The distilled models demonstrate a lower error
floor across SINR levels, highlighting the effectiveness of KD in achieving
energy-efficient AI solutions.

</details>


### [264] [CLA: Latent Alignment for Online Continual Self-Supervised Learning](https://arxiv.org/abs/2507.10434)
*Giacomo Cignoni,Andrea Cossu,Alexandra Gomez-Villa,Joost van de Weijer,Antonio Carta*

Main category: cs.LG

TL;DR: 提出用于在线持续学习（Online CL）的自监督学习策略CLA，能加速训练收敛，预训练表现也佳。


<details>
  <summary>Details</summary>
Motivation: 在线持续学习场景下自监督学习技术较少，需新方法缓解遗忘问题。

Method: 引入Continual Latent Alignment (CLA)策略，将当前模型学习的表征与过去表征对齐。

Result: CLA能在在线场景加速训练过程收敛，在相同计算预算下优于现有方法；早期用CLA预训练比全独立同分布预训练效果好。

Conclusion: CLA是一种有效的在线持续学习自监督策略。

Abstract: Self-supervised learning (SSL) is able to build latent representations that
generalize well to unseen data. However, only a few SSL techniques exist for
the online CL setting, where data arrives in small minibatches, the model must
comply with a fixed computational budget, and task boundaries are absent. We
introduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL
that aligns the representations learned by the current model with past
representations to mitigate forgetting. We found that our CLA is able to speed
up the convergence of the training process in the online scenario,
outperforming state-of-the-art approaches under the same computational budget.
Surprisingly, we also discovered that using CLA as a pretraining protocol in
the early stages of pretraining leads to a better final performance when
compared to a full i.i.d. pretraining.

</details>


### [265] [Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities](https://arxiv.org/abs/2507.10442)
*Shivam Chandhok,Wan-Cyuan Fan,Vered Shwartz,Vineeth N Balasubramanian,Leonid Sigal*

Main category: cs.LG

TL;DR: 本文构建测试探究SOTA视觉语言模型在基础视觉任务上的局限性，对比不同组件性能，揭示其不足并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型虽功能强大但缺乏基本视觉理解技能，要了解其在基础视觉任务上的局限性。

Method: 构建一系列测试，对比视觉编码器特征、视觉语言中间投影和大语言模型解码器输出上训练的探测器的性能。

Result: 揭示了视觉语言模型的不足，对其能力、鲁棒性和视觉信息处理方式有了重要发现。

Conclusion: 研究成果有望为进一步改进视觉语言模型提供指导。

Abstract: Vision-language Models (VLMs) have emerged as general-purpose tools for
addressing a variety of complex computer vision problems. Such models have been
shown to be highly capable, but, at the same time, lacking some basic visual
understanding skills. In this paper, we set out to understand the limitations
of SoTA VLMs on fundamental visual tasks by constructing a series of tests that
probe which components of design, specifically, may be lacking. Importantly, we
go significantly beyond the current benchmarks, which simply measure the final
performance of VLM response, by also comparing and contrasting it to the
performance of probes trained directly on features obtained from the visual
encoder, intermediate vision-language projection and LLM-decoder output. In
doing so, we uncover shortcomings in VLMs and make a number of important
observations about their capabilities, robustness and how they process visual
information. We hope our insights will guide progress in further improving
VLMs.

</details>


### [266] [Some remarks on gradient dominance and LQR policy optimization](https://arxiv.org/abs/2507.10452)
*Eduardo D. Sontag*

Main category: cs.LG

TL;DR: 文章探讨优化问题解，以连续和离散时间LQR问题收敛差异为动机，研究广义PLI条件及梯度估计误差影响，采用ISS分析。


<details>
  <summary>Details</summary>
Motivation: 连续和离散时间LQR问题收敛行为存在差距，需寻找广义PLI条件；同时理解梯度估计误差的影响。

Method: 对问题进行“输入到状态稳定性”（ISS）分析。

Result: 未提及明确结果。

Conclusion: 未提及明确结论。

Abstract: Solutions of optimization problems, including policy optimization in
reinforcement learning, typically rely upon some variant of gradient descent.
There has been much recent work in the machine learning, control, and
optimization communities applying the Polyak-{\L}ojasiewicz Inequality (PLI) to
such problems in order to establish an exponential rate of convergence (a.k.a.
``linear convergence'' in the local-iteration language of numerical analysis)
of loss functions to their minima under the gradient flow. Often, as is the
case of policy iteration for the continuous-time LQR problem, this rate
vanishes for large initial conditions, resulting in a mixed globally linear /
locally exponential behavior. This is in sharp contrast with the discrete-time
LQR problem, where there is global exponential convergence. That gap between CT
and DT behaviors motivates the search for various generalized PLI-like
conditions, and this talk will address that topic. Moreover, these
generalizations are key to understanding the transient and asymptotic effects
of errors in the estimation of the gradient, errors which might arise from
adversarial attacks, wrong evaluation by an oracle, early stopping of a
simulation, inaccurate and very approximate digital twins, stochastic
computations (algorithm ``reproducibility''), or learning by sampling from
limited data. We describe an ``input to state stability'' (ISS) analysis of
this issue. The lecture also discussed convergence and PLI-like properties of
``linear feedforward neural networks'' in feedback control, but this arXiv
skips that part (to be updated). Much of the work described here was done in
collaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping
Jiang, and Milad Siami.

</details>


### [267] [The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization](https://arxiv.org/abs/2507.10484)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: 本文提出‘Target Polish’框架用于非负矩阵和张量分解，比传统方法更高效准确。


<details>
  <summary>Details</summary>
Motivation: 传统加权NMF方法抗异常值但收敛慢，需要一种既抗异常值又高效的方法。

Method: 采用基于加权中值变换自适应平滑数据，使Target Polish与快速高效的Fast - HALS算法兼容。

Result: 在含噪声图像数据集上评估，Target Polish达到或超越了现有鲁棒NMF方法的准确性，且计算时间减少一个数量级。

Conclusion: Target Polish是一种健壮且计算高效的非负矩阵和张量分解框架。

Abstract: This paper introduces the "Target Polish," a robust and computationally
efficient framework for nonnegative matrix and tensor factorization. Although
conventional weighted NMF approaches are resistant to outliers, they converge
slowly due to the use of multiplicative updates to minimize the objective
criterion. In contrast, the Target Polish approach remains compatible with the
Fast-HALS algorithm, which is renowned for its speed, by adaptively smoothing
the data with a weighted median-based transformation. This innovation provides
outlier resistance while maintaining the highly efficient additive update
structure of Fast-HALS. Empirical evaluations using image datasets corrupted
with structured (block) and unstructured (salt) noise demonstrate that the
Target Polish approach matches or exceeds the accuracy of state-of-the-art
robust NMF methods and reduces computational time by an order of magnitude in
the studied scenarios.

</details>


### [268] [Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing](https://arxiv.org/abs/2507.10494)
*Tanveer Khan,Mindaugas Budzys,Antonis Michalas*

Main category: cs.LG

TL;DR: 文章提出SplitHappens方法，将FSS与U形SL结合，降低训练时间和通信成本，保障数据隐私并抵御多种攻击。


<details>
  <summary>Details</summary>
Motivation: 现有方法在保障SL数据隐私和抵御攻击方面存在不足，需提升安全性。

Method: 将FSS与U形SL结合，让客户端无需与服务器共享训练数据标签。

Result: 在不同数据集的两个卷积神经网络上测试，该方法能降低训练时间和通信成本，准确率与之前相当。

Conclusion: 此方法有效结合FSS和U形SL，能抵御多种攻击，降低成本且保证准确率。

Abstract: Split Learning (SL) -- splits a model into two distinct parts to help protect
client data while enhancing Machine Learning (ML) processes. Though promising,
SL has proven vulnerable to different attacks, thus raising concerns about how
effective it may be in terms of data privacy. Recent works have shown promising
results for securing SL through the use of a novel paradigm, named Function
Secret Sharing (FSS), in which servers obtain shares of a function they compute
and operate on a public input hidden with a random mask. However, these works
fall short in addressing the rising number of attacks which exist on SL. In
SplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly
to other works, we are able to make use of the benefits of SL by reducing the
communication and computational costs of FSS. However, a U-shaped SL provides a
higher security guarantee than previous works, allowing a client to keep the
labels of the training data secret, without having to share them with the
server. Through this, we are able to generalize the security analysis of
previous works and expand it to different attack vectors, such as modern model
inversion attacks as well as label inference attacks. We tested our approach
for two different convolutional neural networks on different datasets. These
experiments show the effectiveness of our approach in reducing the training
time as well as the communication costs when compared to simply using FSS while
matching prior accuracy.

</details>


### [269] [Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop](https://arxiv.org/abs/2507.10502)
*Elizabeth Fahsbender,Alma Andersson,Jeremy Ash,Polina Binder,Daniel Burkhardt,Benjamin Chang,Georg K. Gerber,Anthony Gitter,Patrick Godau,Ankit Gupta,Genevieve Haliburton,Siyu He,Trey Ideker,Ivana Jelic,Aly Khan,Yang-Joon Kim,Aditi Krishnapriyan,Jon M. Laurent,Tianyu Liu 28,Emma Lundberg,Shalin B. Mehta,Rob Moccia,Angela Oliveira Pisco,Katherine S. Pollard,Suresh Ramani,Julio Saez-Rodriguez,Yasin Senbabaoglu,Elana Simon,Srinivasan Sivanandan,Gustavo Stolovitzky,Marc Valer,Bo Wang,Xikun Zhang,James Zou,Katrina Kalantar*

Main category: cs.LG

TL;DR: 因缺乏跨领域标准化基准，人工智能在生物学应用受限。研讨会专家确定瓶颈并提出构建基准框架建议，以推动AI驱动虚拟细胞基准发展。


<details>
  <summary>Details</summary>
Motivation: 人工智能在生物学领域应用潜力大，但缺乏标准化、跨领域基准影响构建可靠模型，需解决此问题。

Method: 召集跨领域专家，确定技术和系统性瓶颈，提出构建基准框架的一系列建议，包括促进高质量数据管理、标准化工具、综合评估指标和开放协作平台。

Result: 提出了构建可跨任务和数据模式比较生物系统机器学习模型的基准框架的建议。

Conclusion: 这些基准对确保严谨性、可重复性和生物相关性至关重要，将推动该领域朝着集成模型发展，带来新发现和对细胞系统的更深入理解。

Abstract: Artificial intelligence holds immense promise for transforming biology, yet a
lack of standardized, cross domain, benchmarks undermines our ability to build
robust, trustworthy models. Here, we present insights from a recent workshop
that convened machine learning and computational biology experts across
imaging, transcriptomics, proteomics, and genomics to tackle this gap. We
identify major technical and systemic bottlenecks such as data heterogeneity
and noise, reproducibility challenges, biases, and the fragmented ecosystem of
publicly available resources and propose a set of recommendations for building
benchmarking frameworks that can efficiently compare ML models of biological
systems across tasks and data modalities. By promoting high quality data
curation, standardized tooling, comprehensive evaluation metrics, and open,
collaborative platforms, we aim to accelerate the development of robust
benchmarks for AI driven Virtual Cells. These benchmarks are crucial for
ensuring rigor, reproducibility, and biological relevance, and will ultimately
advance the field toward integrated models that drive new discoveries,
therapeutic insights, and a deeper understanding of cellular systems.

</details>


### [270] [Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination](https://arxiv.org/abs/2507.10532)
*Mingqi Wu,Zhihao Zhang,Qiaole Dong,Zhiheng Xi,Jun Zhao,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Qin Liu,Songyang Zhang,Qi Zhang*

Main category: cs.LG

TL;DR: 现有基于强化学习提升大语言模型推理能力的研究多在Qwen2.5上评估，结果可能因数据污染不可靠。本文提出合成数据集RandomCalculation，表明只有准确奖励信号能提升性能，倡导在无污染基准和不同模型上评估。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习提升大语言模型推理能力的研究多在Qwen2.5上评估，在其他模型如Llama上未取得类似效果，且Qwen2.5因预训练易受基准数据污染，结果可能不可靠，需进一步研究。

Method: 引入生成器生成任意长度和难度的合成算术问题，构建无污染数据集RandomCalculation。

Result: 使用无污染数据集表明只有准确奖励信号能持续提升性能，噪声或错误信号不能。

Conclusion: 倡导在无污染基准和不同模型族上评估强化学习方法，以确保结论可靠。

Abstract: The reasoning capabilities of large language models (LLMs) have been a
longstanding focus of research. Recent works have further enhanced these
capabilities using reinforcement learning (RL), with many new methods claiming
significant improvements with minimal or no external supervision. Surprisingly,
some studies even suggest that random or incorrect reward signals can enhance
reasoning performance. However, these breakthroughs are mostly reported on the
Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,
AMC, and AIME, while failing to achieve similar gains on other models like
Llama, which warrants further investigation. Our analysis shows that although
Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on
large-scale web corpora makes it vulnerable to data contamination in popular
benchmarks. As a result, results derived from these benchmarks may be
unreliable. To address this, we introduce a generator that produces fully
synthetic arithmetic problems of arbitrary length and difficulty, yielding a
clean dataset we call RandomCalculation. Using these leakage-free datasets, we
show that only accurate reward signals consistently improve performance, while
noisy or incorrect signals do not. We advocate for evaluating RL methods on
uncontaminated benchmarks and across diverse model families to ensure
trustworthy conclusions.

</details>


### [271] [On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance](https://arxiv.org/abs/2507.10536)
*Qiaoyue Tang,Alain Zhiyanov,Mathias Lécuyer*

Main category: cs.LG

TL;DR: 分析常见隐私学习优化算法在重尾类别不平衡分布下的优化行为，指出DP - GD学习低频类有问题，DP - AdamBC可避免病态条件，且在训练低频类上提高了准确率。


<details>
  <summary>Details</summary>
Motivation: 研究常见隐私学习优化算法在重尾类别不平衡分布下的优化情况。

Method: 构建理论模型分析，对比梯度下降（DP - GD）和估计二阶信息的优化算法，采用DP - AdamBC去除估计损失曲率的DP偏差。

Result: DP - GD学习低频类时表现不佳，DP - AdamBC能避免重尾类别不平衡导致的病态条件，在控制实验和真实数据上学习低频类时训练准确率分别提高约8%和5%。

Conclusion: DP - AdamBC在重尾类别不平衡分布下学习低频类效果更好。

Abstract: In this work, we analyze the optimization behaviour of common private
learning optimization algorithms under heavy-tail class imbalanced
distribution. We show that, in a stylized model, optimizing with Gradient
Descent with differential privacy (DP-GD) suffers when learning low-frequency
classes, whereas optimization algorithms that estimate second-order information
do not. In particular, DP-AdamBC that removes the DP bias from estimating loss
curvature is a crucial component to avoid the ill-condition caused by
heavy-tail class imbalance, and empirically fits the data better with
$\approx8\%$ and $\approx5\%$ increase in training accuracy when learning the
least frequent classes on both controlled experiments and real data
respectively.

</details>


### [272] [Graph World Model](https://arxiv.org/abs/2507.10539)
*Tao Feng,Yexin Wu,Guanyu Lin,Jiaxuan You*

Main category: cs.LG

TL;DR: 提出图世界模型GWM，支持非结构化和图结构状态，适用于多模态信息和多样任务，实验显示其性能优且有零样本/少样本能力。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型主要关注非结构化数据，无法利用图结构数据；图基础模型专注图学习任务，无法扩展到多模态数据和跨学科任务。

Method: 提出GWM，核心是通用消息传递算法，通过GWM - T将多模态数据转为文本、GWM - E使用特定模态编码器，引入动作节点支持多样任务。

Result: 在六个不同领域任务上，GWM表现优于或媲美特定领域基线，受益于多跳结构，在未见新任务上有强零样本/少样本能力。

Conclusion: GWM有效解决现有模型问题，在多领域任务表现良好，代码已开源。

Abstract: World models (WMs) demonstrate strong capabilities in prediction, generation,
and planning tasks. Existing WMs primarily focus on unstructured data and
cannot leverage the ubiquitous structured data, often represented as graphs, in
the digital world. While multiple graph foundation models have been proposed,
they focus on graph learning tasks and cannot extend to diverse multi-modal
data and interdisciplinary tasks. To address these challenges, we propose the
Graph World Model (GWM), a world model that supports both unstructured and
graph-structured states with multi-modal information and represents diverse
tasks as actions. The core of a GWM is a generic message-passing algorithm to
aggregate structured information, either over a unified multi-modal token space
by converting multi-modal data into text (GWM-T) or a unified multi-modal
embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces
action nodes to support diverse tasks, where action nodes are linked to other
nodes via direct reference or similarity computation. Extensive experiments on
six tasks from diverse domains, including multi-modal generation and matching,
recommendation, graph prediction, multi-agent, retrieval-augmented generation,
and planning and optimization, show that the same GWM outperforms or matches
domain-specific baselines' performance, benefits from multi-hop structures, and
demonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our
code for GWM is released at https://github.com/ulab-uiuc/GWM.

</details>


### [273] [Fusing LLM Capabilities with Routing Data](https://arxiv.org/abs/2507.10540)
*Tao Feng,Haozhen Zhang,Zijie Lei,Pengrui Han,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Jiaxuan You*

Main category: cs.LG

TL;DR: 当前大语言模型应用多依赖单模型，本文提出FusionBench基准和FusionFactory融合框架，实验显示FusionFactory能提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型应用多依赖单模型，在处理复杂任务时存在能力覆盖不足、性能和成本低效的问题，而LLM路由数据可揭示模型优势。

Method: 提出FusionBench基准涵盖5个领域14个任务和20个开源大模型；引入FusionFactory融合框架，包括查询级、思维级和模型级融合。

Result: FusionFactory在所有14个基准测试中始终优于最佳单个大语言模型，不同基准的最优融合配置不同。

Conclusion: 系统性的大语言模型融合能利用模型互补优势，提升整体性能。

Abstract: The rapid advancement of large language models (LLMs) has created a vibrant
ecosystem of diverse architectures, each with unique strengths due to
differences in design, training data, and objectives. However, most
applications still rely on a single backend model, limiting coverage of
capabilities and leading to inefficiencies in performance and token cost when
tackling complex tasks. We highlight an underexploited opportunity: LLM routing
data, produced when hosting platforms route diverse queries to different
models, which can reveal comparative strengths across tasks. To address this,
we propose FusionBench, a comprehensive routing benchmark covering 14 tasks
across five domains with 20 open-source LLMs (8B to 671B parameters), capturing
103M tokens and summarizing reusable thought templates from top models.
Building on this, we introduce FusionFactory, a systematic fusion framework
with three levels: (1) query-level fusion, tailoring routers for each query
using both direct responses and reasoning-augmented outputs; (2) thought-level
fusion, leveraging abstract templates derived from top-performing LLMs' answers
to similar queries; and (3) model-level fusion, transferring capabilities
between models via distillation, using top responses or highest judge scores as
training data. Experiments show FusionFactory consistently outperforms the best
individual LLM across all 14 benchmarks, with optimal fusion configurations
varying by benchmark, demonstrating the value of systematic LLM fusion in
harnessing complementary strengths and improving overall performance.

</details>


### [274] [Disentangling Neural Disjunctive Normal Form Models](https://arxiv.org/abs/2507.10546)
*Kexin Gu Baugh,Vincent Perreault,Matthew Baugh,Luke Dickens,Katsumi Inoue,Alessandra Russo*

Main category: cs.LG

TL;DR: 本文指出神经析取范式（DNF）模型在符号转换中性能下降问题，提出解纠缠方法解决该问题，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 神经DNF模型在符号转换过程中性能下降，原因是无法解开网络权重中表示的知识。

Method: 提出新的解纠缠方法，将编码嵌套规则的节点拆分为更小的独立节点。

Result: 在二分类、多分类和多标签分类任务实验中，该解纠缠方法为神经DNF模型提供了紧凑且可解释的逻辑表示，性能更接近转换前。

Conclusion: 提出的解纠缠方法能解决神经DNF模型在符号转换中的性能下降问题。

Abstract: Neural Disjunctive Normal Form (DNF) based models are powerful and
interpretable approaches to neuro-symbolic learning and have shown promising
results in classification and reinforcement learning settings without prior
knowledge of the tasks. However, their performance is degraded by the
thresholding of the post-training symbolic translation process. We show here
that part of the performance degradation during translation is due to its
failure to disentangle the learned knowledge represented in the form of the
networks' weights. We address this issue by proposing a new disentanglement
method; by splitting nodes that encode nested rules into smaller independent
nodes, we are able to better preserve the models' performance. Through
experiments on binary, multiclass, and multilabel classification tasks
(including those requiring predicate invention), we demonstrate that our
disentanglement method provides compact and interpretable logical
representations for the neural DNF-based models, with performance closer to
that of their pre-translation counterparts. Our code is available at
https://github.com/kittykg/disentangling-ndnf-classification.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [275] [Credit Card Fraud Detection Using RoFormer Model With Relative Distance Rotating Encoding](https://arxiv.org/abs/2507.09385)
*Kevin Reyes,Vasco Cortez*

Main category: cs.NE

TL;DR: 本文引入在RoFormer模型中加入Relative Distance Rotating Encoding (ReDRE)的新方法用于交易欺诈检测，可提升欺诈检测效果。


<details>
  <summary>Details</summary>
Motivation: 欺诈检测是金融系统重要挑战，支付网关公司需提升交易授权率并降低欺诈，需不断研究新的欺诈检测方法。

Method: 在RoFormer模型中加入Relative Distance Rotating Encoding (ReDRE)。

Result: 通过ReDRE的角度旋转增强了Transformer中时间序列数据的特征，能更好捕捉时间依赖和事件关系。

Conclusion: 该新方法可改善欺诈检测效果。

Abstract: Fraud detection is one of the most important challenges that financial
systems must address. Detecting fraudulent transactions is critical for payment
gateway companies like Flow Payment, which process millions of transactions
monthly and require robust security measures to mitigate financial risks.
Increasing transaction authorization rates while reducing fraud is essential
for providing a good user experience and building a sustainable business. For
this reason, discovering novel and improved methods to detect fraud requires
continuous research and investment for any company that wants to succeed in
this industry. In this work, we introduced a novel method for detecting
transactional fraud by incorporating the Relative Distance Rotating Encoding
(ReDRE) in the RoFormer model. The incorporation of angle rotation using ReDRE
enhances the characterization of time series data within a Transformer, leading
to improved fraud detection by better capturing temporal dependencies and event
relationships.

</details>


### [276] [BrainFLORA: Uncovering Brain Concept Representation via Multimodal Neural Embeddings](https://arxiv.org/abs/2507.09747)
*Dongyang Li,Haoyang Qin,Mingyang Wu,Chen Wei,Quanying Liu*

Main category: cs.NE

TL;DR: 本文介绍了统一框架BrainFLORA，用于整合跨模态神经影像数据构建共享神经表征，在视觉检索任务中表现出色，还揭示了视觉概念表征的映射关系，对认知神经科学和脑机接口有新启示。


<details>
  <summary>Details</summary>
Motivation: 理解大脑如何表征视觉信息是神经科学和人工智能的根本挑战，当前整合多模态神经影像信号存在时空不对齐问题，现有方法常孤立分析各模态，缺乏整体视角。

Method: 引入BrainFLORA框架，利用多模态大语言模型，辅以特定模态适配器和任务解码器，结合神经影像分析方法。

Result: 在联合主体视觉检索任务中达到了最先进的性能，有扩展多任务的潜力，揭示了视觉概念表征在神经模态间及与现实世界物体感知的对齐情况。

Conclusion: BrainFLORA框架实现了不同神经影像模态间的融合，展示了大脑结构化视觉概念表征与物理世界刺激的隐式映射，对认知神经科学和脑机接口有新意义。

Abstract: Understanding how the brain represents visual information is a fundamental
challenge in neuroscience and artificial intelligence. While AI-driven decoding
of neural data has provided insights into the human visual system, integrating
multimodal neuroimaging signals, such as EEG, MEG, and fMRI, remains a critical
hurdle due to their inherent spatiotemporal misalignment. Current approaches
often analyze these modalities in isolation, limiting a holistic view of neural
representation. In this study, we introduce BrainFLORA, a unified framework for
integrating cross-modal neuroimaging data to construct a shared neural
representation. Our approach leverages multimodal large language models (MLLMs)
augmented with modality-specific adapters and task decoders, achieving
state-of-the-art performance in joint-subject visual retrieval task and has the
potential to extend multitasking. Combining neuroimaging analysis methods, we
further reveal how visual concept representations align across neural
modalities and with real world object perception. We demonstrate that the
brain's structured visual concept representations exhibit an implicit mapping
to physical-world stimuli, bridging neuroscience and machine learning from
different modalities of neural imaging. Beyond methodological advancements,
BrainFLORA offers novel implications for cognitive neuroscience and
brain-computer interfaces (BCIs). Our code is available at
https://github.com/ncclab-sustech/BrainFLORA.

</details>


### [277] [Effective Self-Attention-Based Deep Learning Model with Evolutionary Grid Search for Robust Wave Farm Energy Forecasting](https://arxiv.org/abs/2507.09847)
*Amin Abdollahi Dehkordi,Mehdi Neshat,Nataliia Y. Sergiienko,Zahra Ghasemi,Lei Chen,John Boland,Hamid Moradkhani,Amir H. Gandomi*

Main category: cs.NE

TL;DR: 本文提出混合顺序学习模型预测波浪能输出，在多地验证效果优，支持波浪能并入电网。


<details>
  <summary>Details</summary>
Motivation: 实现碳中和促使探索波浪能，但波浪能因技术和经济挑战未充分开发，准确预测波浪能输出对电网稳定和商业可行性至关重要。

Method: 提出结合自注意力增强卷积双向LSTM和超参数优化的混合顺序学习模型，利用波浪能转换器空间数据。

Result: 模型在阿德莱德、珀斯、塔斯马尼亚和悉尼等地波浪农场数据集验证，R2得分分别达91.7%、88.0%、82.8%和91.0%，优于十种机器学习算法。

Conclusion: 模型优于传统机器学习和深度学习方法，能为不同海洋环境波浪能输出提供可靠可扩展预测，支持其可靠并入能源系统。

Abstract: Achieving carbon neutrality, a key focus of UN SDG #13, drives the
exploration of wave energy, a renewable resource with the potential to generate
30,000 TWh of clean electricity annually, surpassing global demand. However,
wave energy remains underdeveloped due to technical and economic challenges,
particularly in forecasting wave farm power output, which is vital for grid
stability and commercial viability. This study proposes a novel predictive
framework to enhance wave energy integration into power grids. It introduces a
hybrid sequential learning model combining Self-Attention-enhanced
Convolutional Bi-LSTM with hyperparameter optimization. The model leverages
spatial data from Wave Energy Converters (WECs) and is validated using datasets
from wave farms in Adelaide, Sydney, Perth, and Tasmania, Australia.
Benchmarked against ten machine learning algorithms, the model achieves
superior accuracy, with R2 scores of 91.7% (Adelaide), 88.0% (Perth), 82.8%
(Tasmania), and 91.0% (Sydney). It outperforms conventional ML and deep
learning methods, offering robust and scalable predictions for wave energy
output across diverse marine environments, supporting reliable integration into
energy systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [278] [Choosing the Right Git Workflow: A Comparative Analysis of Trunk-based vs. Branch-based Approaches](https://arxiv.org/abs/2507.08943)
*Pedro Lopes,Paola Accioly,Paulo Borba,Vitor Menezes*

Main category: cs.SE

TL;DR: 本文研究巴西开发者使用Git工作流的情况，通过访谈和调查发现基于主干的开发适合经验丰富的小团队快节奏项目，基于分支的开发适合经验不足的大团队。


<details>
  <summary>Details</summary>
Motivation: Git工作流选择影响团队生产力和软件质量，但相关科学研究较少，需了解巴西开发者使用情况及影响因素。

Method: 对软件开发人员进行半结构化访谈和调查。

Result: 基于主干的开发有利于经验丰富的小团队快节奏项目，基于分支的开发更适合经验不足的大团队，但存在管理挑战。

Conclusion: 不同类型团队和项目应根据自身情况选择合适的Git工作流。

Abstract: Git has become one of the most widely used version control systems today.
Among its distinguishing features, its ability to easily and quickly create
branches stands out, allowing teams to customize their workflows. In this
context, various formats of collaborative development workflows using Git have
emerged and gained popularity among software engineers. We can categorize such
workflows into two main types: branch-based workflows and trunk-based
workflows. Branch-based workflows typically define a set of remote branches
with well-defined objectives, such as feature branches, a branch for feature
integration, and a main branch. The goal is to migrate changes from the most
isolated branch to the main one shared by all as the code matures. In this
category, GitFlow stands out as the most popular example. In contrast,
trunk-based workflows have a single remote branch where developers integrate
their changes directly. In this range of options, choosing a workflow that
maximizes team productivity while promoting software quality becomes a
non-trivial task. Despite discussions on forums, social networks, and blogs,
few scientific articles have explored this topic. In this work, we provide
evidence on how Brazilian developers work with Git workflows and what factors
favor or hinder the use of each model. To this end, we conducted
semi-structured interviews and a survey with software developers. Our results
indicate that trunk-based development favors fast-paced projects with
experienced and smaller teams, while branch-based development suits less
experienced and larger teams better, despite posing management challenges.

</details>


### [279] [Semantic Source Code Segmentation using Small and Large Language Models](https://arxiv.org/abs/2507.08992)
*Abdelhalim Dahou,Ansgar Scherp,Sebastian Kurten,Brigitte Mathiak,Madhu Chauhan*

Main category: cs.SE

TL;DR: 本文提出自动化特定领域方法对研究用R代码进行分割，对比不同方法和模型，发现基于上下文逐行分析及小语言模型表现更佳。


<details>
  <summary>Details</summary>
Motivation: 随着代码库增长，手动和句法分析方法在R等低资源语言代码分割中变得不实用，需要新方法。

Method: 引入两种新颖方法（基于上下文逐行分析和基于范围的段确定），使用大、小语言模型实验，创建人类标注数据集StatCodeSeg，还对计算机科学领域的Python代码进行实验。

Result: 基于上下文的逐行分析优于基于范围的分割；CodeBERT和CodeT5+的编码器版本等小语言模型比大语言模型表现好。

Conclusion: 新的自动化特定领域方法在R代码分割中有较好效果，小语言模型在特定任务上表现出色。

Abstract: Source code segmentation, dividing code into functionally coherent segments,
is crucial for knowledge retrieval and maintenance in software development.
While enabling efficient navigation and comprehension of large codebases,
manual and syntactic analysis approaches have become impractical as
repositories grow, especially for low-resource languages like R and their
research domains (e.g., social sciences, psychology).This paper introduces an
automated, domain-specific approach for research R code segmentation using
Large and Small Language Models (LLMs/SLMs). It presents two novel approaches
and a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:
line-by-line analysis with context and range-based segment determination. We
experiment with LLMs and fine-tuned SLMs. To support the generalizability of
our approaches, we also include experiments on Python code from the computer
science domain.Our results show that context-based line-by-line analysis is
superior over range-based segmentation.Using smaller language models like
CodeBERT and an encoder-only version of CodeT5+ are better than their LLM
counterparts. Most notably, these two best-performing models did not see R code
during pre-training versus the LLMs but were only fine-tuned on 4,130 lines of
manually annotated code.

</details>


### [280] [Prompting for Performance: Exploring LLMs for Configuring Software](https://arxiv.org/abs/2507.09790)
*Helge Spieker,Théo Matricon,Nassim Belmecheri,Jørn Eirik Betten,Gauthier Le Bartz Lyan,Heraldo Borges,Quentin Mazouni,Dennis Gross,Arnaud Gotlieb,Mathieu Acher*

Main category: cs.SE

TL;DR: 研究大语言模型（LLMs）能否通过提示辅助面向性能的软件配置，评估其在多种任务和系统中的表现，结果有积极能力也有明显局限。


<details>
  <summary>Details</summary>
Motivation: 软件系统配置选项决策难且机器学习技术计算成本高，探究大语言模型辅助软件配置的可能性。

Method: 在编译器、视频编码器和SAT求解器等多种可配置系统上，对多个大语言模型进行识别相关选项、配置排序和推荐高性能配置等任务的评估。

Result: 大语言模型在某些任务和系统上与专家知识契合度高，但也会出现幻觉或浅层推理等情况。

Conclusion: 研究结果为系统评估和基于大语言模型的软件配置解决方案设计迈出了第一步。

Abstract: Software systems usually provide numerous configuration options that can
affect performance metrics such as execution time, memory usage, binary size,
or bitrate. On the one hand, making informed decisions is challenging and
requires domain expertise in options and their combinations. On the other hand,
machine learning techniques can search vast configuration spaces, but with a
high computational cost, since concrete executions of numerous configurations
are required. In this exploratory study, we investigate whether large language
models (LLMs) can assist in performance-oriented software configuration through
prompts. We evaluate several LLMs on tasks including identifying relevant
options, ranking configurations, and recommending performant configurations
across various configurable systems, such as compilers, video encoders, and SAT
solvers. Our preliminary results reveal both positive abilities and notable
limitations: depending on the task and systems, LLMs can well align with expert
knowledge, whereas hallucinations or superficial reasoning can emerge in other
cases. These findings represent a first step toward systematic evaluations and
the design of LLM-based solutions to assist with software configuration.

</details>


### [281] [Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle](https://arxiv.org/abs/2507.09023)
*Yao Fehlis,Charles Crain,Aidan Jensen,Michael Watson,James Juhasz,Paul Mandel,Betty Liu,Shawn Mahon,Daren Wilson,Nick Lynch-Jonely,Ben Leedom,David Fuller*

Main category: cs.SE

TL;DR: 本文介绍了名为Tippy的新型AI框架，可通过专门的AI代理转变实验室自动化，在药物发现的DMTA循环中提升效率，为AI辅助药物发现提供新范式。


<details>
  <summary>Details</summary>
Motivation: 制药行业在药物发现中面临前所未有的挑战，传统方法难以满足现代治疗发展需求。

Method: 引入名为Tippy的AI框架，采用包含五个专门代理和安全护栏监督的多代理系统，各代理在药物发现流程的特定阶段发挥作用。

Result: Tippy系统在工作流程效率、决策速度和跨学科协调方面有显著提升。

Conclusion: Tippy作为首个用于自动化DMTA循环的专业化AI代理的生产就绪实现，为AI辅助药物发现提供了新范式。

Abstract: The pharmaceutical industry faces unprecedented challenges in drug discovery,
with traditional approaches struggling to meet modern therapeutic development
demands. This paper introduces a novel AI framework, Tippy, that transforms
laboratory automation through specialized AI agents operating within the
Design-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five
specialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with
Safety Guardrail oversight - each designed to excel in specific phases of the
drug discovery pipeline. Tippy represents the first production-ready
implementation of specialized AI agents for automating the DMTA cycle,
providing a concrete example of how AI can transform laboratory workflows. By
leveraging autonomous AI agents that reason, plan, and collaborate, we
demonstrate how Tippy accelerates DMTA cycles while maintaining scientific
rigor essential for pharmaceutical research. The system shows significant
improvements in workflow efficiency, decision-making speed, and
cross-disciplinary coordination, offering a new paradigm for AI-assisted drug
discovery.

</details>


### [282] [Towards Extracting Software Requirements from App Reviews using Seq2seq Framework](https://arxiv.org/abs/2507.09039)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: 提出基于Seq2seq的NER任务框架提取应用评论需求，在两个数据集上评估，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法从应用评论提取需求效果不佳，评论存在语言不规范和大量无关信息。

Method: 将需求提取重新定义为基于Seq2seq的NER任务，提出包含BiLSTM编码器、LSTM解码器的框架，增强了自注意力机制、GloVe嵌入和CRF模型。

Result: 在数据集2上F1分数0.96，优于现有方法；在数据集1上F1分数0.47，表现相当。

Conclusion: 提出的框架在从应用评论提取需求方面有效，优于现有方法。

Abstract: Mobile app reviews are a large-scale data source for software improvements. A
key task in this context is effectively extracting requirements from app
reviews to analyze the users' needs and support the software's evolution.
Recent studies show that existing methods fail at this task since app reviews
usually contain informal language, grammatical and spelling errors, and a large
amount of irrelevant information that might not have direct practical value for
developers. To address this, we propose a novel reformulation of requirements
extraction as a Named Entity Recognition (NER) task based on the
sequence-to-sequence (Seq2seq) generation approach. With this aim, we propose a
Seq2seq framework, incorporating a BiLSTM encoder and an LSTM decoder, enhanced
with a self-attention mechanism, GloVe embeddings, and a CRF model. We
evaluated our framework on two datasets: a manually annotated set of 1,000
reviews (Dataset 1) and a crowdsourced set of 23,816 reviews (Dataset 2). The
quantitative evaluation of our framework showed that it outperformed existing
state-of-the-art methods with an F1 score of 0.96 on Dataset 2, and achieved
comparable performance on Dataset 1 with an F1 score of 0.47.

</details>


### [283] [CMER: A Context-Aware Approach for Mining Ethical Concern-related App Reviews](https://arxiv.org/abs/2507.09049)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: 随着移动应用普及，伦理问题受关注，但从应用评论中自动提取相关内容具挑战。本文提出CMER方法，结合NLI和LLM提取伦理相关评论，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 移动应用伦理问题受关注，应用评论含伦理反馈，但因使用特定语言且被其他反馈掩盖，自动提取困难。

Method: 提出CMER方法，结合自然语言推理（NLI）和类似LLaMA的大语言模型（LLM），NLI提供领域特定上下文感知，LLM无需标注数据进行分类。

Result: 从超38.2万条移动投资应用评论数据集中挖掘隐私和安全相关评论，提取出之前基于关键词方法遗漏的2178条评论。

Conclusion: CMER方法有效，提取的评论可进一步提炼为可操作的需求工件。

Abstract: With the increasing proliferation of mobile applications in our daily lives,
the concerns surrounding ethics have surged significantly. Users communicate
their feedback in app reviews, frequently emphasizing ethical concerns, such as
privacy and security. Incorporating these reviews has proved to be useful for
many areas of software engineering (e.g., requirement engineering, testing,
etc.). However, app reviews related to ethical concerns generally use
domain-specific language and are typically overshadowed by more generic
categories of user feedback, such as app reliability and usability. Thus,
making automated extraction a challenging and time-consuming effort.
  This study proposes CMER (A \underline{C}ontext-Aware Approach for
\underline{M}ining \underline{E}thical Concern-related App
\underline{R}eviews), a novel approach that combines Natural Language Inference
(NLI) and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract
ethical concern-related app reviews at scale. In CMER, NLI provides
domain-specific context awareness by using domain-specific hypotheses, and the
Llama-like LLM eliminates the need for labeled data in the classification task.
We evaluated the validity of CMER by mining privacy and security-related
reviews (PSRs) from the dataset of more than 382K app reviews of mobile
investment apps. First, we evaluated four NLI models and compared the results
of domain-specific hypotheses with generic hypotheses. Next, we evaluated three
LLMs for the classification task. Finally, we combined the best NLI and LLM
models (CMER) and extracted 2,178 additional PSRs overlooked by the previous
study using a keyword-based approach, thus demonstrating the effectiveness of
CMER. These reviews can be further refined into actionable requirement
artifacts.

</details>


### [284] [SAGE: A Context-Aware Approach for Mining Privacy Requirements Relevant Reviews from Mental Health Apps](https://arxiv.org/abs/2507.09051)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: 研究提出SAGE方法自动挖掘心理健康应用隐私评论，评估效果好，能提取被忽视评论并用于隐私需求分析。


<details>
  <summary>Details</summary>
Motivation: 心理健康应用数据收集引发隐私担忧，但隐私相关评论难自动识别，需挖掘隐私评论以提取隐私需求、解决用户担忧。

Method: 引入SAGE方法，利用自然语言推理和心理健康领域特定隐私假设，结合GPT模型，无需微调。

Result: 在204K应用评论数据集上定量评估F1分数达0.85，优于微调的BERT和T5；定性评估提取748条被关键词方法忽略的隐私评论。

Conclusion: SAGE方法有效，提取的评论可用于生成可操作的隐私需求工件。

Abstract: Mental health (MH) apps often require sensitive user data to customize
services for mental wellness needs. However, such data collection practices in
some MH apps raise significant privacy concerns for users. These concerns are
often mentioned in app reviews, but other feedback categories, such as
reliability and usability, tend to take precedence. This poses a significant
challenge in automatically identifying privacy requirements-relevant reviews
(privacy reviews) that can be utilized to extract privacy requirements and
address users' privacy concerns. Thus, this study introduces SAGE, a
context-aware approach to automatically mining privacy reviews from MH apps
using Natural Language Inference (NLI) with MH domain-specific privacy
hypotheses (provides domain-specific context awareness) and a GPT model
(eliminates the need for fine-tuning). The quantitative evaluation of SAGE on a
dataset of 204K app reviews achieved an F1 score of 0.85 without any
fine-tuning, outperforming the fine-tuned baseline classifiers BERT and T5.
Furthermore, SAGE extracted 748 privacy reviews previously overlooked by
keyword-based methods, demonstrating its effectiveness through qualitative
evaluation. These reviews can later be refined into actionable privacy
requirement artifacts.

</details>


### [285] [SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments](https://arxiv.org/abs/2507.09063)
*Avi Arora,Jinu Jang,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: 现有大语言模型（LLM）代理环境搭建评估不足，本文引入SetupBench基准测试，评估发现现有代理在环境搭建能力上存在较大差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多在预安装环境中评估LLM代理，缺乏对环境搭建技能的评估，为填补这一空白开展研究。

Method: 引入包含93个实例的SetupBench基准测试，涵盖多种语言生态和数据库引擎，对OpenHands编码代理进行评估。

Result: OpenHands在各任务类别中成功率低，存在不完整安装、幻觉约束等失败模式，代理探索策略效率低。

Conclusion: SetupBench可为下一代软件开发者代理提供严格评估标准，当前代理在环境搭建能力上存在显著差距。

Abstract: Modern Large Language Model (LLM) agents promise end to end assistance with
real-world software tasks, yet existing benchmarks evaluate LLM agents almost
exclusively in pre-baked environments where every dependency is pre-installed.
To fill this gap, we introduce SetupBench, a 93 instance benchmark that
isolates the environment-bootstrap skill: starting from a bare Linux sandbox,
an agent must install packages, resolve dependency conflicts, initialize
databases, and configure background services. Our tasks span seven language
ecosystems, five database engines, and multi-service orchestration scenarios,
each accompanies by a natural language problem statement and a deterministic
success command. Through evaluation of OpenHands, a state-of-the-art coding
agent, we find low success rates across task categories, with particular
challenges in repository setup (38.9-57.4%) and local database configuration
(20.0-53.3%). Our analysis reveals systematic failure modes including
incomplete development tooling installation, hallucinated task constraints, and
non-persistent environment modifications that break agent-human collaboration
workflows. We identify substantial inefficiencies in agent exploration
strategies, with 38-89% of actions being unnecessary compared to optimal human
behavior. These findings highlight gaps in current agents' practical
environment-bootstrap capabilities. By targeting this critical yet
under-evaluated capability, SetupBench provides a rigorous yard-stick for the
next generation of software developer agents aiming to solve end to end
real-wold tasks.

</details>


### [286] [SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation](https://arxiv.org/abs/2507.09108)
*Aaditya Bhatia,Gustavo A. Oliva,Gopi Krishnan Rajbahadur,Haoxiang Zhang,Yihao Chen,Zhilong Chen,Arthur Leung,Dayi Lin,Boyuan Chen,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 介绍自动化标注工具SPICE，能低成本标注软件工程数据集，还发布工具和新数据集。


<details>
  <summary>Details</summary>
Motivation: 高质量标注数据集对软件工程基础模型重要，但创建成本高、耗人力。

Method: SPICE结合上下文代码导航、推理驱动提示和多轮共识来生成近似专家标注的标签。

Result: SPICE与人工标注高度一致，将标注1000个实例成本从约10万美元降至5.1美元。

Conclusion: SPICE有潜力实现软件工程基础模型的低成本、大规模数据集创建。

Abstract: High-quality labeled datasets are crucial for training and evaluating
foundation models in software engineering, but creating them is often
prohibitively expensive and labor-intensive. We introduce SPICE, a scalable,
automated pipeline for labeling SWE-bench-style datasets with annotations for
issue clarity, test coverage, and effort estimation. SPICE combines
context-aware code navigation, rationale-driven prompting, and multi-pass
consensus to produce labels that closely approximate expert annotations.
SPICE's design was informed by our own experience and frustration in labeling
more than 800 instances from SWE-Gym. SPICE achieves strong agreement with
human-labeled SWE-bench Verified data while reducing the cost of labeling 1,000
instances from around $100,000 (manual annotation) to just $5.10. These results
demonstrate SPICE's potential to enable cost-effective, large-scale dataset
creation for SE-focused FMs. To support the community, we release both SPICE
tool and SPICE Bench, a new dataset of 6,802 SPICE-labeled instances curated
from 291 open-source projects in SWE-Gym (over 13x larger than SWE-bench
Verified).

</details>


### [287] [Position Paper: Programming Language Techniques for Bridging LLM Code Generation Semantic Gaps](https://arxiv.org/abs/2507.09135)
*Yalong Du,Chaozheng Wang,Huaijin Wang*

Main category: cs.SE

TL;DR: 大语言模型在自动代码生成有缺陷，需结合编程语言技术弥补差距。


<details>
  <summary>Details</summary>
Motivation: 大语言模型自动代码生成存在语义差距、可靠性问题，需解决这些问题。

Method: 通过结构化程序表示、形式正确性保证和强大验证机制，将编程语言技术与大语言模型结合。

Result: 未提及具体结果。

Conclusion: 结合编程语言技术对开发功能正确、可解释、可验证和值得信赖的代码生成系统至关重要。

Abstract: Large Language Models have demonstrated remarkable capabilities in automated
code generation, yet their statistical nature and black-box characteristics
create significant semantic gaps manifested through syntax errors, semantic
hallucinations, and reliability concerns. This position paper argues that
principled integration of Programming Language (PL) techniques is essential for
bridging these gaps. Through structured program representations, formal
correctness guarantees, and robust verification mechanisms, PL techniques can
elevate LLM-generated code from statistical pattern matching to truly reliable
and trustworthy levels. This integration is crucial for developing systems that
generate code that is not only functionally correct but also interpretable,
verifiable, and ultimately trustworthy.

</details>


### [288] [OpenCAMS: An Open-Source Connected and Automated Mobility Co-Simulation Platform for Advanced Transportation Research](https://arxiv.org/abs/2507.09186)
*Minhaj Uddin Ahmad,Akid Abrar,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.SE

TL;DR: 介绍开源的OpenCAMS协同仿真框架，结合三种工具优势，具时间同步、双向耦合架构，可扩展且开源。


<details>
  <summary>Details</summary>
Motivation: 支持交通安全、移动性和网络安全方面的高级研究，推动下一代智能交通系统发展。

Method: 采用时间同步、双向耦合架构，结合SUMO、CARLA和OMNeT++三种工具优势，且系统可扩展。

Result: 开发出OpenCAMS平台，能在不同领域协同仿真且保证一致性。

Conclusion: OpenCAMS为研究社区提供了可访问、灵活和协作的环境以推进智能交通系统。

Abstract: We introduce OpenCAMS (Open-Source Connected and Automated Mobility
Co-Simulation Platform), an open-source, synchronized, and extensible
co-simulation framework that tightly couples three best-in-class simulation
tools: (i) SUMO, (ii) CARLA, and (iii) OMNeT++. OpenCAMS is designed to support
advanced research in transportation safety, mobility, and cybersecurity by
combining the strengths of each simulation domain. Specifically, SUMO provides
large-scale, microscopic traffic modeling; CARLA offers high-fidelity 3D
perception, vehicle dynamics, and control simulation; and OMNeT++ enables
modular, event-driven network communication, such as cellular
vehicle-to-everything (C-V2X). OpenCAMS employs a time-synchronized,
bidirectional coupling architecture that ensures coherent simulation
progression across traffic, perception, and communication domains while
preserving modularity and reproducibility. For example, CARLA can simulate and
render a subset of vehicles that require detailed sensor emulation and control
logic; SUMO orchestrates network-wide traffic flow, vehicle routing, and
traffic signal management; and OMNeT++ dynamically maps communication nodes to
both mobile entities (e.g., vehicles) and static entities (e.g., roadside
units) to enable C-V2X communication. While these three simulators form the
foundational core of OpenCAMS, the platform is designed to be expandable and
future-proof, allowing additional simulators to be integrated on top of this
core without requiring fundamental changes to the system architecture. The
OpenCAMS platform is fully open-source and publicly available through its
GitHub repository https://github.com/minhaj6/carla-sumo-omnetpp-cosim,
providing the research community with an accessible, flexible, and
collaborative environment for advancing next-generation intelligent
transportation systems.

</details>


### [289] [Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted Retrieval](https://arxiv.org/abs/2507.09199)
*Huihui Huang,Ratnadira Widyasari,Ting Zhang,Ivana Clairine Irsan,Jieke Shi,Han Wei Ang,Frank Liauw,Eng Lieh Ouh,Lwin Khin Shar,Hong Jin Kang,David Lo*

Main category: cs.SE

TL;DR: 提出RDS构建评估数据集，发现深度学习方法性能下降，传统方法VSM更优，提出EasyLink工具，性能提升四倍多，并给出研究实践指南。


<details>
  <summary>Details</summary>
Motivation: 现有评估技术忽略仓库中更多无关提交对工具识别正确修复提交的干扰，需要更真实的评估。

Method: 提出Realistic Distribution Setting (RDS)构建评估数据集；提出EasyLink，利用向量数据库和大语言模型重排提交。

Result: 在新数据集上，深度学习方法性能下降超一半，VSM表现更优；EasyLink平均Precision@1达75.91%，比现有技术提升四倍多。

Conclusion: EasyLink有效解决问题，论文给出了推进问题 - 提交链接恢复研究的实践指南。

Abstract: Issue-commit linking, which connects issues with commits that fix them, is
crucial for software maintenance. Existing approaches have shown promise in
automatically recovering these links. Evaluations of these techniques assess
their ability to identify genuine links from plausible but false links.
However, these evaluations overlook the fact that, in reality, when a
repository has more commits, the presence of more plausible yet unrelated
commits may interfere with the tool in differentiating the correct fix commits.
To address this, we propose the Realistic Distribution Setting (RDS) and use it
to construct a more realistic evaluation dataset that includes 20 open-source
projects. By evaluating tools on this dataset, we observe that the performance
of the state-of-the-art deep learning-based approach drops by more than half,
while the traditional Information Retrieval method, VSM, outperforms it.
  Inspired by these observations, we propose EasyLink, which utilizes a vector
database as a modern Information Retrieval technique. To address the
long-standing problem of the semantic gap between issues and commits, EasyLink
leverages a large language model to rerank the commits retrieved from the
database. Under our evaluation, EasyLink achieves an average Precision@1 of
75.91%, improving over the state-of-the-art by over four times. Additionally,
this paper provides practical guidelines for advancing research in issue-commit
link recovery.

</details>


### [290] [Explainability as a Compliance Requirement: What Regulated Industries Need from AI Tools for Design Artifact Generation](https://arxiv.org/abs/2507.09220)
*Syed Tauhid Ullah Shah,Mohammad Hussein,Ann Barcomb,Mohammad Moshirpour*

Main category: cs.SE

TL;DR: 研究AI驱动的设计工件生成中的可解释性差距，指出不可解释AI输出的问题并提出改进方向，为RE工作流改进提供路线图。


<details>
  <summary>Details</summary>
Motivation: AI工具在需求工程中虽有提效潜力，但在监管行业因缺乏可解释性导致应用受限，需研究解决。

Method: 对安全关键行业的十名从业者进行半结构化访谈，研究当前AI工具集成工作流情况、可解释性缺失的挑战及应对策略。

Result: 不可解释的AI输出需大量手动验证、降低利益相关者信任、难处理特定领域术语、破坏团队协作并带来合规风险，抵消预期效率提升。

Conclusion: 确定关键改进方向，为需求工程工作流中AI工具在监管和安全关键环境的应用提供实用路线图。

Abstract: Artificial Intelligence (AI) tools for automating design artifact generation
are increasingly used in Requirements Engineering (RE) to transform textual
requirements into structured diagrams and models. While these AI tools,
particularly those based on Natural Language Processing (NLP), promise to
improve efficiency, their adoption remains limited in regulated industries
where transparency and traceability are essential. In this paper, we
investigate the explainability gap in AI-driven design artifact generation
through semi-structured interviews with ten practitioners from safety-critical
industries. We examine how current AI-based tools are integrated into workflows
and the challenges arising from their lack of explainability. We also explore
mitigation strategies, their impact on project outcomes, and features needed to
improve usability. Our findings reveal that non-explainable AI outputs
necessitate extensive manual validation, reduce stakeholder trust, struggle to
handle domain-specific terminology, disrupt team collaboration, and introduce
regulatory compliance risks, often negating the anticipated efficiency
benefits. To address these issues, we identify key improvements, including
source tracing, providing clear justifications for tool-generated decisions,
supporting domain-specific adaptation, and enabling compliance validation. This
study outlines a practical roadmap for improving the transparency, reliability,
and applicability of AI tools in requirements engineering workflows,
particularly in regulated and safety-critical environments where explainability
is crucial for adoption and certification.

</details>


### [291] [Enhancing Interpretability in Software Change Management with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.09315)
*Yongqian Sun,Weihua Kuang,Chao Shen,Xidao Wen,Tinghua Zheng,Heng Liu,Shenglin Zhang,Bo Wu,Dan Pei*

Main category: cs.SE

TL;DR: 提出SCELM框架用于软件变更管理，减少服务故障和经济损失。


<details>
  <summary>Details</summary>
Motivation: 现代在线服务中频繁软件变更带来重大风险，需有效应对。

Method: 提出SCELM这一用于软件变更管理的端到端自动化框架。

Result: 无明确提及，但目标是高效精准管理软件变更，减少服务故障和经济损失。

Conclusion: SCELM能帮助管理软件变更，降低相关风险。

Abstract: In modern online services, frequent software changes introduce significant
risks. To tackle this challenge, we propose SCELM (Software Change Evaluation
and Lifecycle Management), an end-to-end automated framework for software
change management. SCELM aims to manage software changes efficiently and
precisely, significantly reducing service failures and economic losses.

</details>


### [292] [Enhancing NeuroEvolution-Based Game Testing: A Branch Coverage Approach for Scratch Programs](https://arxiv.org/abs/2507.09414)
*Khizra Sohail,Atif Aftab Ahmed Jilani,Nigar Azhar Butt*

Main category: cs.SE

TL;DR: 本文为解决游戏程序自动化测试问题，引入基于分支覆盖的适应度函数改进NEATEST框架，实验表明新方法在分支覆盖和故障检测上更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于语句覆盖的NEATEST框架用于游戏自动化测试时，语句覆盖不足以检测故障，需要提高测试有效性。

Method: 引入基于分支覆盖的适应度函数，集成到NEATEST框架中，对25个Scratch游戏进行实验，并进行突变分析。

Result: 在25个游戏中的13个里，新方法比原方法有更高分支覆盖，且突变测试中假阳性率更低。

Conclusion: 基于分支覆盖的测试生成能提高Scratch程序的测试覆盖和故障检测能力。

Abstract: Automated test generation for game-like programs presents unique challenges
due to their non-deterministic behavior and complex control structures. The
NEATEST framework has been used for automated testing in Scratch games,
employing neuroevolution-based test generation optimized for statement
coverage. However, statement coverage alone is often insufficient for fault
detection, as it does not guarantee execution of all logical branches. This
paper introduces a branch coverage-based fitness function to enhance test
effectiveness in automated game testing. We extend NEATEST by integrating a
branch fitness function that prioritizes control-dependent branches, guiding
the neuroevolution process to maximize branch exploration. To evaluate the
effectiveness of this approach, empirical experiments were conducted on 25
Scratch games, comparing Neatest with Statement Coverage (NSC) against Neatest
with Branch Coverage (NBC). A mutation analysis was also performed to assess
the fault detection capabilities of both techniques. The results demonstrate
that NBC achieves higher branch coverage than NSC in 13 out of 25 games,
particularly in programs with complex conditional structures. Moreover, NBC
achieves a lower false positive rate in mutation testing, making it a more
reliable approach for identifying faulty behavior in game programs. These
findings confirm that branch coverage-based test generation improves test
coverage and fault detection in Scratch programs.

</details>


### [293] [Evaluating LLMs on Sequential API Call Through Automated Test Generation](https://arxiv.org/abs/2507.09481)
*Yuheng Huang,Da Song,Zhenlan Ji,Shuai Wang,Lei Ma*

Main category: cs.SE

TL;DR: 现有大语言模型工具使用测试评估处于早期，本文提出StateGen框架生成API交互编码任务并构建StateEval基准测试，实验证明其能生成有挑战且现实的任务。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型工具使用的测试、评估和分析处于早期，现有基准测试依赖手动收集测试用例，无法自动检查语义正确性且忽略顺序API调用的复杂交互。

Method: 引入StateGen框架，结合状态机API约束求解与验证、基于能量的采样和控制流注入生成可执行程序，再通过两个大语言模型代理协作将程序转化为自然语言任务描述，构建StateEval基准测试。

Result: 实验结果表明StateGen能有效生成具有挑战性和现实性的面向API的任务。

Conclusion: 指出当前结合API的大语言模型存在待改进之处。

Abstract: By integrating tools from external APIs, Large Language Models (LLMs) have
expanded their promising capabilities in a diverse spectrum of complex
real-world tasks. However, testing, evaluation, and analysis of LLM tool use
remain in their early stages. Most existing benchmarks rely on manually
collected test cases, many of which cannot be automatically checked for
semantic correctness and instead depend on static methods such as string
matching. Additionally, these benchmarks often overlook the complex
interactions that occur between sequential API calls, which are common in
real-world applications. To fill the gap, in this paper, we introduce StateGen,
an automated framework designed to generate diverse coding tasks involving
sequential API interactions. StateGen combines state-machine-based API
constraint solving and validation, energy-based sampling, and control-flow
injection to generate executable programs. These programs are then translated
into human-like natural language task descriptions through a collaboration of
two LLM agents. Utilizing StateGen, we construct StateEval, a benchmark
encompassing 120 verified test cases spanning across three representative
scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental
results confirm that StateGen can effectively generate challenging and
realistic API-oriented tasks, highlighting areas for improvement in current
LLMs incorporating APIs.

</details>


### [294] [Towards LLM-Based Automatic Playtest](https://arxiv.org/abs/2507.09490)
*Yan Zhao,Chiwei Tang*

Main category: cs.SE

TL;DR: 本文介绍基于LLM的自动游戏测试方法Lap，用ChatGPT测试三消游戏，经案例研究表明其优于现有工具，为自动测试和LLM应用提供启示。


<details>
  <summary>Details</summary>
Motivation: 手动游戏测试耗时昂贵，现有自动测试方法有局限，当前LLMs用于游戏测试存在无法感知游戏环境等挑战。

Method: Lap包含游戏环境处理、基于提示的动作生成和动作执行三个关键阶段，将游戏棋盘快照转换为数字矩阵，让ChatGPT建议移动并应用，迭代执行直至超时。

Result: 在开源三消游戏CasseBonbons上的案例研究显示，Lap实现了更高的代码覆盖率并触发更多程序崩溃，优于现有工具。

Conclusion: 该研究为自动测试和LLM应用指明了未来方向。

Abstract: Playtesting is the process in which people play a video game for testing. It
is critical for the quality assurance of gaming software. Manual playtesting is
time-consuming and expensive. However, automating this process is challenging,
as playtesting typically requires domain knowledge and problem-solving skills
that most conventional testing tools lack. Recent advancements in artificial
intelligence (AI) have opened up new possibilities for applying Large Language
Models (LLMs) to playtesting. However, significant challenges remain: current
LLMs cannot visually perceive game environments, and most existing research
focuses on text-based games or games with robust APIs. Many non-text games lack
APIs to provide textual descriptions of game states, making it almost
impossible to naively apply LLMs for playtesting. This paper introduces Lap,
our novel approach to LLM-based Automatic Playtesting, which uses ChatGPT to
test match-3 games, a category of games where players match three or more
identical tiles in a row or column to earn points. Lap encompasses three key
phases: processing of game environments, prompting-based action generation, and
action execution. Given a match-3 game, Lap takes a snapshot of the game board
and converts it to a numeric matrix. It then prompts the ChatGPT-O1-mini API to
suggest moves based on that matrix and tentatively applies the suggested moves
to earn points and trigger changes in the game board. It repeats the
above-mentioned three steps iteratively until timeout. For evaluation, we
conducted a case study using Lap on an open-source match-3 game, CasseBonbons,
and empirically compared it with three existing tools. Our results are
promising: Lap outperformed existing tools by achieving higher code coverage
and triggering more program crashes. This research sheds light on the future of
automatic testing and LLM applications.

</details>


### [295] [It Only Gets Worse: Revisiting DL-Based Vulnerability Detectors from a Practical Perspective](https://arxiv.org/abs/2507.09529)
*Yunqian Wang,Xiaohong Li,Yao Zhang,Yuekang Li,Zhiping Zhou,Ruitao Feng*

Main category: cs.SE

TL;DR: 本文提出VulTegra评估框架对比不同深度学习漏洞检测模型，揭示SOTA检测器问题，指出预训练模型不总优于从头训练模型，调整关键因素可提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习漏洞检测器在声明的CWE范围一致性、现实有效性和场景适用性方面存疑，需全面分析影响检测的关键因素以改进模型设计和部署。

Method: 提出VulTegra评估框架，对从头训练和基于预训练的深度学习漏洞检测模型进行多维比较。

Result: SOTA检测器存在一致性低、现实能力有限和可扩展性挑战；预训练模型在特定场景有优势；调整一个关键因素可提升所有七个评估检测器的召回率，六个检测器的F1分数也更好。

Conclusion: 研究深入了解模型行为，强调有效检测需同时考虑漏洞类型和代码固有特征。

Abstract: With the growing threat of software vulnerabilities, deep learning (DL)-based
detectors have gained popularity for vulnerability detection. However, doubts
remain regarding their consistency within declared CWE ranges, real-world
effectiveness, and applicability across scenarios. These issues may lead to
unreliable detection, high false positives/negatives, and poor adaptability to
emerging vulnerabilities. A comprehensive analysis is needed to uncover
critical factors affecting detection and guide improvements in model design and
deployment. In this paper, we present VulTegra, a novel evaluation framework
that conducts a multidimensional comparison of scratch-trained and
pre-trained-based DL models for vulnerability detection. VulTegra reveals that
state-of-the-art (SOTA) detectors still suffer from low consistency, limited
real-world capabilities, and scalability challenges. Contrary to common belief,
pre-trained models are not consistently better than scratch-trained models but
exhibit distinct strengths in specific contexts.Importantly, our study exposes
the limitations of relying solely on CWE-based classification and identifies
key factors that significantly affect model performance. Experimental results
show that adjusting just one such factor consistently improves recall across
all seven evaluated detectors, with six also achieving better F1 scores. Our
findings provide deeper insights into model behavior and emphasize the need to
consider both vulnerability types and inherent code features for effective
detection.

</details>


### [296] [A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study](https://arxiv.org/abs/2507.09583)
*Taniv Ashraf*

Main category: cs.SE

TL;DR: 本文介绍了一个利用Gemini API的实时股票分析无服务器系统，详述架构演变、调试过程，最终系统成本近零，公开代码，还探讨了大语言模型在金融分析等方面的作用。


<details>
  <summary>Details</summary>
Motivation: 强大且易获取的大语言模型为金融数据分析民主化带来新机遇，设计新系统实现实时股票分析。

Method: 利用Gemini API进行定性评估，通过GitHub Actions自动化数据摄取和处理，用解耦静态前端展示结果。

Result: 最终架构成本近零，系统可公开访问，代码可查看。

Conclusion: 讨论了大语言模型在金融分析中的作用、健壮调试方法的重要性以及人机协作的新兴模式。

Abstract: The advent of powerful, accessible Large Language Models (LLMs) like Google's
Gemini presents new opportunities for democratizing financial data analysis.
This paper documents the design, implementation, and iterative debugging of a
novel, serverless system for real-time stock analysis. The system leverages the
Gemini API for qualitative assessment, automates data ingestion and processing
via GitHub Actions, and presents the findings through a decoupled, static
frontend. We detail the architectural evolution of the system, from initial
concepts to a robust, event-driven pipeline, highlighting the practical
challenges encountered during deployment. A significant portion of this paper
is dedicated to a case study on the debugging process, covering common software
errors, platform-specific permission issues, and rare, environment-level
platform bugs. The final architecture operates at a near-zero cost,
demonstrating a viable model for individuals to build sophisticated AI-powered
financial tools. The operational application is publicly accessible, and the
complete source code is available for review. We conclude by discussing the
role of LLMs in financial analysis, the importance of robust debugging
methodologies, and the emerging paradigm of human-AI collaboration in software
development.

</details>


### [297] [How to Define Design in Industrial Control and Automation Software](https://arxiv.org/abs/2507.09594)
*Aydin Homay*

Main category: cs.SE

TL;DR: 设计缺乏科学基础影响效率和创新，本文依托设计理论对软件行业设计定义等问题进行研究并探讨设计方法差异及平衡相关问题。


<details>
  <summary>Details</summary>
Motivation: 解决设计缺乏科学基础导致主观决策、降低效率和创新的问题，尤其针对软件行业和工业控制与自动化系统领域。

Method: 回顾软件行业现有设计定义，挑战流行误解，参考设计理论领域的设计定义，解答设计相关关键问题，评估临时和系统设计方法的区别。

Result: 文档未提及明确结果。

Conclusion: 文档未提及明确结论。

Abstract: Design is a fundamental aspect of engineering, enabling the creation of
products, systems, and organizations to meet societal and/or business needs.
However, the absence of a scientific foundation in design often results in
subjective decision-making, reducing both efficiency and innovation. This
challenge is particularly evident in the software industry and, by extension,
in the domain of industrial control and automation systems (iCAS).
  In this study, first we review the existing design definitions within the
software industry, challenge prevailing misconceptions about design, review
design definition in the field of design theory and address key questions such
as: When does design begin? How can design be defined scientifically? What
constitutes good design? and the difference between design and design language
by relying on advancements in the field of design theory. We also evaluate the
distinction between ad-hoc and systematic design approaches, and present
arguments on how to balance complementary operational concerns while resolving
conflicting evolutionary concerns.

</details>


### [298] [The Mythical Good Software](https://arxiv.org/abs/2507.09596)
*Aydin Homay*

Main category: cs.SE

TL;DR: 认为高内聚低耦合的软件设计理念存在问题，并将阐明相关含义和哲学。


<details>
  <summary>Details</summary>
Motivation: 指出高内聚低耦合的软件设计理念存在笨拙、模糊和有害的情况，要为读者阐明相关内容含义和哲学。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: Good software has high cohesion and low coupling is clumsy, obscure, and in
some certain cases could be actually a harmful state of being. It is clumsy
because there is no perfect correlation between higher cohesiveness and optimum
design, and it is obscure because it conveys the message that coupling and
cohesion are two distinct design principles, while there are in principle the
same design approaches, and only the time and space differ between them, and it
could also be a harmful state of being because we should not always aim for
higher cohesiveness without considering its cost.
  In the course of this study, we aim to elucidate for the readers the meaning
and underlying philosophy of the aforementioned paragraph.

</details>


### [299] [Complexity and Coupling: A Functional Domain Approach](https://arxiv.org/abs/2507.09599)
*Aydin Homay*

Main category: cs.SE

TL;DR: 论文基于功能域为工业控制和自动化系统的复杂性与耦合给出精准科学定义，分析耦合与复杂性关系并得出设计需在功能域处理二者的结论。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理属性的复杂性和耦合定义存在模糊性、易导致混乱和不一致，需精准科学定义。

Method: 结合软件工程、工业自动化和机械设计等多学科例子进行分析。

Result: 复杂性不一定与系统规模或组件数量相关，耦合发生在功能域而非物理域。

Conclusion: 有效设计需在功能域处理耦合和复杂性。

Abstract: This paper provides a precise and scientific definition of complexity and
coupling, grounded in the functional domain, particularly within industrial
control and automation systems (iCAS). We highlight the widespread ambiguity in
defining complexity and coupling, emphasizing that many existing definitions
rooted in physical attributes lead to confusion and inconsistencies.
Furthermore, we re-exhibit why coupled design inherently increases complexity
and how potentially this complexity could be reduced. Drawing on examples from
various disciplines, such as software engineering, industrial automation, and
mechanical design, we demonstrate that complexity does not necessarily
correlate with system size or the number of components, and coupling, unlike
common belief in software engineering, actually does not occur in the physical
domain but in the functional domain. We conclude that effective design
necessitates addressing coupling and complexity within the functional domain.

</details>


### [300] [Code Review as Decision-Making -- Building a Cognitive Model from the Questions Asked During Code Review](https://arxiv.org/abs/2507.09637)
*Lo Gullstrand Heander,Emma Söderberg,Christofer Rydenfält*

Main category: cs.SE

TL;DR: 本文通过人种志思考出声研究构建代码审查认知模型，提出CRDM模型展示开发者审查阶段及决策情况。


<details>
  <summary>Details</summary>
Motivation: 当前代码审查在工具和流程上存在挑战，自动化审查会丧失人际益处，需通过理解认知过程改进工具支持。

Method: 进行涉及10名参与者和34次代码审查的人种志思考出声研究，对转录材料进行主题、统计、时间和顺序分析，自下而上构建认知模型。

Result: 得到代码审查即决策（CRDM）模型，展示开发者审查分定向和分析两个阶段，过程中有多项决策。

Conclusion: 理解代码审查的认知过程可改进工具支持，让代码审查更高效、愉快并保留益处。

Abstract: Code review is a well-established and valued practice in the software
engineering community contributing to both code quality and interpersonal
benefits. However, there are challenges in both tools and processes that give
rise to misalignments and frustrations. Recent research seeks to address this
by automating code review entirely, but we believe that this risks losing the
majority of the interpersonal benefits such as knowledge transfer and shared
ownership.
  We believe that by better understanding the cognitive processes involved in
code review, it would be possible to improve tool support, with out without AI,
and make code review both more efficient, more enjoyable, while increasing or
maintaining all of its benefits. In this paper, we conduct an ethnographic
think-aloud study involving 10 participants and 34 code reviews. We build a
cognitive model of code review bottom up through thematic, statistical,
temporal, and sequential analysis of the transcribed material. Through the
data, the similarities between the cognitive process in code review and
decision-making processes, especially recognition-primed decision-making,
become apparent.
  The result is the Code Review as Decision-Making (CRDM) model that shows how
the developers move through two phases during the code review; first an
orientation phase to establish context and rationale and then an analytical
phase to understand, assess, and plan the rest of the review. Throughout the
process several decisions must be taken, on writing comments, finding more
information, voting, running the code locally, verifying continuous integration
results, etc.
  Analysis software and process-coded data publicly available at:
https://doi.org/10.5281/zenodo.15758266

</details>


### [301] [Is Quantization a Deal-breaker? Empirical Insights from Large Code Models](https://arxiv.org/abs/2507.09665)
*Saima Afrin,Bowen Xu,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 研究量化对自动生成代码质量方面的影响，发现量化是有效技术。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注量化代码模型生成代码的功能正确性，忽略代码质量的关键方面，为弥补此差距开展研究。

Method: 对CodeLlama和DeepSeekCoder应用激活感知权重量化（AWQ）生成Java和Python代码，用静态分析工具评估软件质量指标和静态特征。

Result: 量化不仅能保留功能正确性，还能保留开发者追求的关键代码质量属性。

Conclusion: 量化是一种强大技术，能在不影响代码关键质量属性的前提下降低大代码模型资源需求。

Abstract: The growing scale of large language models (LLMs) not only demands extensive
computational resources but also raises environmental concerns due to their
increasing carbon footprint. Model quantization emerges as an effective
approach that can reduce the resource demands of LLMs by decreasing parameter
precision without substantially affecting performance (e.g., 16 bit to 4 bit).
While recent studies have established quantization as a promising approach for
optimizing large code models (LCMs), a specialized subset of LLMs tailored for
automated software engineering, their findings offer only limited insights into
its practical implications. Specifically, current investigations focus only on
the functional correctness of the code generated by quantized models,
neglecting how quantization impacts critical aspects of code quality such as
reliability, maintainability, and security. To bridge this gap, our study
investigates the effects of quantization on the qualitative aspects of
automatically generated code. We apply Activation-aware Weight Quantization
(AWQ) to two widely used code models, CodeLlama and DeepSeekCoder, to generate
Java and Python code. Using state-of-the-art static analysis tools, we evaluate
software quality metrics and static features including cyclomatic complexity,
cognitive complexity, and lines of code. Our findings reveal that quantization
is a robust technique that not only preserves functional correctness, but also
retains key qualitative code attributes sought after by developers, such as
maintainability and structural simplicity.

</details>


### [302] [OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit Optimization](https://arxiv.org/abs/2507.09682)
*Laura Baird,Armin Moin*

Main category: cs.SE

TL;DR: 提出名为OrQstrator的模块化框架，用深度强化学习进行量子电路优化，由中央引擎协调多个优化器模块，输出适应后端约束的优化电路。


<details>
  <summary>Details</summary>
Motivation: 在NISQ时代进行量子电路优化。

Method: 采用深度强化学习，中央编排引擎协调基于DRL的电路重写器、特定领域优化器和参数化电路实例化器三个优化器模块，利用NISQ Analyzer技术适应后端约束。

Result: 输出适合硬件感知转译和执行的优化电路。

Conclusion: 提出的OrQstrator框架可有效在NISQ时代进行量子电路优化。

Abstract: We propose a novel approach, OrQstrator, which is a modular framework for
conducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum
(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our
orchestration engine intelligently selects among three complementary circuit
optimizers: A DRL-based circuit rewriter trained to reduce depth and gate count
via learned rewrite sequences; a domain-specific optimizer that performs
efficient local gate resynthesis and numeric optimization; a parameterized
circuit instantiator that improves compilation by optimizing template circuits
during gate set translation. These modules are coordinated by a central
orchestration engine that learns coordination policies based on circuit
structure, hardware constraints, and backend-aware performance features such as
gate count, depth, and expected fidelity. The system outputs an optimized
circuit for hardware-aware transpilation and execution, leveraging techniques
from an existing state-of-the-art approach, called the NISQ Analyzer, to adapt
to backend constraints.

</details>


### [303] [Measuring What Matters: A Framework for Evaluating Safety Risks in Real-World LLM Applications](https://arxiv.org/abs/2507.09820)
*Jia Yi Goh,Shaun Khoo,Nyx Iskandar,Gabriel Chua,Leanne Tan,Jessica Foo*

Main category: cs.SE

TL;DR: 本文介绍评估大语言模型应用级安全的实用框架，由开发定制安全风险分类原则和评估应用安全风险实践两部分构成，通过内部试点验证，为组织提供参考。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型安全测试多关注基础模型，而应用层面的组件会影响整体安全，因此有必要评估应用级安全。

Method: 提出一个实用框架，包含开发定制安全风险分类的原则和评估应用安全风险的实践两部分。

Result: 框架通过组织内多个用例的实际部署验证，并在内部试点中应用。

Conclusion: 该工作弥合了人工智能安全理论与大语言模型应用安全运营现实之间的差距，为安全可扩展部署提供可行指导。

Abstract: Most safety testing efforts for large language models (LLMs) today focus on
evaluating foundation models. However, there is a growing need to evaluate
safety at the application level, as components such as system prompts,
retrieval pipelines, and guardrails introduce additional factors that
significantly influence the overall safety of LLM applications. In this paper,
we introduce a practical framework for evaluating application-level safety in
LLM systems, validated through real-world deployment across multiple use cases
within our organization. The framework consists of two parts: (1) principles
for developing customized safety risk taxonomies, and (2) practices for
evaluating safety risks in LLM applications. We illustrate how the proposed
framework was applied in our internal pilot, providing a reference point for
organizations seeking to scale their safety testing efforts. This work aims to
bridge the gap between theoretical concepts in AI safety and the operational
realities of safeguarding LLM applications in practice, offering actionable
guidance for safe and scalable deployment.

</details>


### [304] [Turning the Tide: Repository-based Code Reflection](https://arxiv.org/abs/2507.09866)
*Wei Zhang,Jian Yang,Jiaxi Yang,Ya Wang,Zhoujun Li,Zeyu Cui,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 引入LiveRepoReflection基准和RepoReflection - Instruct数据集，评估代码理解和生成能力，训练RepoReflectionCoder并对超40个大模型进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有工作忽略代码库代码修改场景，且在提升反思能力和避免动态基准数据污染方面存在挑战。

Method: 引入LiveRepoReflection基准，创建RepoReflection - Instruct数据集，通过两阶段对话流程训练RepoReflectionCoder，使用排行榜评估模型。

Result: 创建了具有1888个测试用例的基准和大规模指令调优数据集，对超40个大模型进行评估。

Conclusion: 所提出的基准和数据集有助于评估代码库代码反思能力。

Abstract: Code large language models (LLMs) enhance programming by understanding and
generating code across languages, offering intelligent feedback, bug detection,
and code updates through reflection, improving development efficiency and
accessibility. While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code
generation and real-world relevance, previous works ignore the scenario of
modifying code in repositories. Considering challenges remaining in improving
reflection capabilities and avoiding data contamination in dynamic benchmarks,
we introduce LiveRepoReflection, a challenging benchmark for evaluating code
understanding and generation in multi-file repository contexts, featuring 1,888
rigorously filtered test cases across $6$ programming languages to ensure
diversity, correctness, and high difficulty. Further, we create
RepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning
dataset derived from diverse sources, used to train RepoReflectionCoder through
a two-turn dialogue process involving code generation and error-driven repair.
The leaderboard evaluates over 40 LLMs to reflect the model performance of
repository-based code reflection.

</details>


### [305] [PathFuzzing: Worst Case Analysis by Fuzzing Symbolic-Execution Paths](https://arxiv.org/abs/2507.09892)
*Zimu Chen,Di Wang*

Main category: cs.SE

TL;DR: 本文提出PathFuzzing方法结合模糊测试和符号执行技术解决最坏情况分析（WCA）问题，实验表明其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在WCA中，提升模糊测试的代码覆盖率和处理符号执行的路径爆炸问题存在挑战，需要结合两种技术优势设计WCA方法。

Method: 将程序转换为接受执行路径（编码为二进制字符串）的符号程序，使用进化模糊测试技术搜索代表可满足路径条件并导致高资源消耗的二进制字符串。

Result: 在基准测试套件上实验表明，PathFuzzing通常优于模糊测试和符号执行的基线方法。

Conclusion: PathFuzzing是一种有效的结合模糊测试和符号执行优势的WCA方法。

Abstract: Estimating worst-case resource consumption is a critical task in software
development. The worst-case analysis (WCA) problem is an optimization-based
abstraction of this task. Fuzzing and symbolic execution are widely used
techniques for addressing the WCA problem. However, improving code coverage in
fuzzing or managing path explosion in symbolic execution within the context of
WCA poses significant challenges. In this paper, we propose PathFuzzing, aiming
to combine the strengths of both techniques to design a WCA method. The key
idea is to transform a program into a symbolic one that takes an execution path
(encoded as a binary string) and interprets the bits as branch decisions.
PathFuzzing then applies evolutionary fuzzing techniques to the transformed
program to search for binary strings that represent satisfiable path conditions
and lead to high resource consumption. We evaluate the performance of
PathFuzzing experimentally on a benchmark suite that consists of prior work's
benchmarks and some added by us. Results show that PathFuzzing generally
outperforms a fuzzing and a symbolic-execution baseline.

</details>


### [306] [Modelling Interrelations Between Agile Practices: The Agile Map](https://arxiv.org/abs/2507.09907)
*Thomas Hansper,Kevin Phong Pham,Michael Neumann*

Main category: cs.SE

TL;DR: 论文指出敏捷实践使用多样，相互关系不明，提出敏捷地图模型描述其关系。


<details>
  <summary>Details</summary>
Motivation: 由于敏捷实践使用广泛且多样，不清楚不同实践间的相互关系，组合使用存在挑战，需找出关系并系统描述。

Method: 提出理论模型敏捷地图，用系统方法描述敏捷实践间的关系。

Result: 得到敏捷地图模型，可描述敏捷实践间的关系。

Conclusion: 敏捷地图模型能为从业者有意义地选择和组合敏捷实践提供支持。

Abstract: Agile methods are defined through guidelines comprising various practices
intended to enable agile ways of working. These guidelines further comprise a
specific set of agile practices aiming to enable teams for an agile way of
working. However, due to its wide-spread use in practice we know that agile
practices are adopted and tailored intensively, which lead to a high variety of
agile practices in terms of their level of detail. Problem: A high variety of
agile practices can be challenging as we do not know how different agile
practices are interrelated with each other. To be more precise, tailoring and
adopting agile practices may lead to the challenge, that the combinatorial use
of several agile practices can only be successful to a limited extent, as
practices support or even require each other for a effective use in practice.
Objective: Our study aims to provide an enabler for this problem. We want to
identify interrelations between agile practices and describe them in a
systematic manner. Contribution: The core contribution of this paper is the
Agile Map, a theoretical model describing relations between agile practices
following a systematic approach aiming to provide an overview of coherences
between agile practices. The model aims to support practitioners in selecting
and combining agile practices in a meaningful way.

</details>


### [307] [When Less is More: A systematic review of four-day workweek conceptualizations and their effects on organizational performance](https://arxiv.org/abs/2507.09911)
*Marvin Auf der Landwehr,Julia Topp,Michael Neumann*

Main category: cs.SE

TL;DR: 研究压缩工作时间表对IT企业运营效能的影响，通过系统回顾和网络内容分析得出匹配概念与效果的元框架。


<details>
  <summary>Details</summary>
Motivation: 敏捷IT组织需高效灵活工作环境，压缩工作时间表有诸多益处，研究其对IT企业运营效能的影响并构建相关框架。

Method: 对四天工作制相关概念进行系统回顾，结合系统文献回顾和网络内容分析。

Result: 得出匹配概念和效果的元框架，可根据管理前提和情况指导压缩工作时间表的采用。

Conclusion: 未明确提及，可推测压缩工作时间表对IT企业运营效能有积极影响，且元框架有指导意义。

Abstract: Context: Agile IT organizations, which are characterized by self-organization
and collaborative social interactions, require motivating, efficient and
flexible work environments to maximize value creation. Compressed work
schedules such as the four-day workweek have evolved into multiple facets over
the last decades and are associated with various benefits for organizations and
their employees. Objective: Our objective in this study is to deepen our
comprehension of the impact of compressed work schedules on the operational
efficacy of IT enterprises, while concurrently developing a comprehensive
framework delineating the intricacies of compressed work schedules.Method: We
conducted a systematic review of available conceptualizations related to
four-day workweek schedules and elaborate on their organizational and social
effects. To cover scientific and practice-oriented literature, our review
combined a systematic literature review and a web content analysis. Results:
Based on the generated insights, we derive a meta-framework that matches
conceptualizations and effects, finally guiding the adoption of compressed work
schedules based on individual managerial prerequisites and circumstances.

</details>


### [308] [Explicit Vulnerability Generation with LLMs: An Investigation Beyond Adversarial Attacks](https://arxiv.org/abs/2507.10054)
*Emir Bosnak,Sahand Moslemi,Mayasah Lami,Anil Koyuncu*

Main category: cs.SE

TL;DR: 研究探讨开源大语言模型受直接或间接提示时生成易受攻击代码的情况，用双实验设计评估模型，发现模型常生成易受攻击代码，Qwen2正确性最高，用户角色和提示方式有影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分理解大语言模型被明确要求生成不安全代码时的行为，本研究聚焦开源大语言模型在直接或间接提示下生成易受攻击代码这一直接威胁场景。

Method: 提出动态提示和反向提示的双实验设计，用ESBMC静态分析评估三个7B参数的开源模型。

Result: 所有模型常产生易受攻击的输出，Qwen2正确性最高；学生角色比专业角色的漏洞率更高，直接提示稍有效；漏洞复现与圈复杂度呈倒U型关系。

Conclusion: 揭示了开源模型安全机制的局限性，特别是在看似良性的教育请求方面。

Abstract: Large Language Models (LLMs) are increasingly used as code assistants, yet
their behavior when explicitly asked to generate insecure code remains poorly
understood. While prior research has focused on unintended vulnerabilities or
adversarial prompting techniques, this study examines a more direct threat
scenario: open-source LLMs generating vulnerable code when prompted either
directly or indirectly. We propose a dual experimental design: (1) Dynamic
Prompting, which systematically varies vulnerability type, user persona, and
directness across structured templates; and (2) Reverse Prompting, which
derives prompts from real vulnerable code samples to assess vulnerability
reproduction accuracy. We evaluate three open-source 7B-parameter models
(Qwen2, Mistral, and Gemma) using ESBMC static analysis to assess both the
presence of vulnerabilities and the correctness of the generated vulnerability
type. Results show all models frequently produce vulnerable outputs, with Qwen2
achieving highest correctness rates. User persona significantly affects
success, where student personas achieved higher vulnerability rates than
professional roles, while direct prompts were marginally more effective.
Vulnerability reproduction followed an inverted-U pattern with cyclomatic
complexity, peaking at moderate ranges. Our findings expose limitations of
safety mechanisms in open-source models, particularly for seemingly benign
educational requests.

</details>


### [309] [LLMShot: Reducing snapshot testing maintenance via LLMs](https://arxiv.org/abs/2507.10062)
*Ergün Batuhan Kaynak,Mayasah Lami,Sahand Moslemi,Anil Koyuncu*

Main category: cs.SE

TL;DR: 介绍LLMShot框架利用视觉大语言模型自动分析快照测试失败，评估显示有较好分类性能，但选择性忽略机制有局限，可减少人工分类工作量。


<details>
  <summary>Details</summary>
Motivation: 快照测试因UI频繁变化产生大量维护开销，人工分类负担重，需要自动化分析解决方案。

Method: 引入LLMShot框架，通过对UI变化的分层分类自动分析快照测试失败；用iOS应用创建综合数据集评估其有效性。

Result: 使用Gemma3模型评估，12B变体识别失败根源召回率超84%，4B模型在持续集成环境有实用部署优势；选择性忽略机制的提示推理方法有局限。

Conclusion: LLMShot是首个语义快照测试自动化分析方法，能减少人工分类工作量，推动UI测试向更智能范式发展。

Abstract: Snapshot testing has emerged as a critical technique for UI validation in
modern software development, yet it suffers from substantial maintenance
overhead due to frequent UI changes causing test failures that require manual
inspection to distinguish between genuine regressions and intentional design
changes. This manual triage process becomes increasingly burdensome as
applications evolve, creating a need for automated analysis solutions. This
paper introduces LLMShot, a novel framework that leverages vision-based Large
Language Models to automatically analyze snapshot test failures through
hierarchical classification of UI changes. To evaluate LLMShot's effectiveness,
we developed a comprehensive dataset using a feature-rich iOS application with
configurable feature flags, creating realistic scenarios that produce authentic
snapshot differences representative of real development workflows. Our
evaluation using Gemma3 models demonstrates strong classification performance,
with the 12B variant achieving over 84% recall in identifying failure root
causes while the 4B model offers practical deployment advantages with
acceptable performance for continuous integration environments. However, our
exploration of selective ignore mechanisms revealed significant limitations in
current prompting-based approaches for controllable visual reasoning. LLMShot
represents the first automated approach to semantic snapshot test analysis,
offering developers structured insights that can substantially reduce manual
triage effort and advance toward more intelligent UI testing paradigms.

</details>


### [310] [Accelerating Automatic Program Repair with Dual Retrieval-Augmented Fine-Tuning and Patch Generation on Large Language Models](https://arxiv.org/abs/2507.10103)
*Hanyang Guo,Xiaoheng Xie,Hong-Ning Dai,Peng Di,Yu Zhang,Bishenghui Tao,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出SelRepair方法用于自动化程序修复，在Java数据集上表现优于其他方法，能减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则、学习的APR方法有局限，当前代码大语言模型和RAG设计不能充分处理代码修复任务。

Method: 提出SelRepair方法，将微调大语言模型与新设计的双RAG模块集成，用bug - fix对数据集微调，通过RAG选择门结合语义和句法/结构相似信息。

Result: 在Java数据集评估中，SelRepair在不同数据集上精确匹配率分别达26.29%和17.64%，在控制输入长度下推理时间至少减少6.42%。

Conclusion: SelRepair方法在自动化程序修复方面优于其他方法。

Abstract: Automated Program Repair (APR) is essential for ensuring software reliability
and quality while enhancing efficiency and reducing developers' workload.
Although rule-based and learning-based APR methods have demonstrated their
effectiveness, their performance was constrained by the defect type of repair,
the quality of training data, and the size of model parameters. Recently, Large
Language Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have
been increasingly adopted in APR tasks. However, current code LLMs and RAG
designs neither fully address code repair tasks nor consider code-specific
features. To overcome these limitations, we propose SelRepair, a novel APR
approach with integration of a fine-tuned LLM with a newly-designed dual RAG
module. This approach uses a bug-fix pair dataset for fine-tuning and
incorporates semantic and syntactic/structural similarity information through
an RAG selection gate. This design ensures relevant information is retrieved
efficiently, thereby reducing token length and inference time. Evaluations on
Java datasets show SelRepair outperforms other APR methods, achieving 26.29%
and 17.64% in terms of exact match (EM) on different datasets while reducing
inference time by at least 6.42% with controlled input lengths.

</details>


### [311] [Breaking the Myth: Can Small Models Infer Postconditions Too?](https://arxiv.org/abs/2507.10182)
*Gehao Zhang,Zhenting Wang,Juan Zhai*

Main category: cs.SE

TL;DR: 研究表明小的微调语言模型可低成本实现高质量后置条件生成，在多项指标上媲美或超越大模型。


<details>
  <summary>Details</summary>
Motivation: 手动编写形式化规范繁琐易错，大语言模型虽有潜力但模型大、计算需求高，探讨是否真需要大模型完成此任务。

Method: 构建包含提示、推理日志和后置条件的专业数据集，对70亿参数的代码模型进行监督微调，处理真实世界存储库依赖并保留前置状态信息。

Result: 在真实世界Java漏洞基准测试中，紧凑模型在语法正确性、语义正确性和区分漏洞能力上匹配或超越更大的模型。

Conclusion: 在适度数据集上进行有针对性的微调可使小模型达到大模型的效果，为自动规范生成的实际应用提供了实用高效的途径。

Abstract: Formal specifications are essential for ensuring software correctness, yet
manually writing them is tedious and error-prone. Large Language Models (LLMs)
have shown promise in generating such specifications from natural language
intents, but the giant model size and high computational demands raise a
fundamental question: Do we really need large models for this task? In this
paper, we show that a small, fine-tuned language model can achieve high-quality
postcondition generation with much lower computational costs. We construct a
specialized dataset of prompts, reasoning logs, and postconditions, then
supervise the fine-tuning of a $7$B-parameter code model. Our approach tackles
real-world repository dependencies and preserves pre-state information,
allowing for expressive and accurate specifications. We evaluate the model on a
benchmark of real-world Java bugs (Defects4J) and compare against both
proprietary giants (e.g., GPT-4o) and open-source large models. Empirical
results demonstrate that our compact model matches or outperforms significantly
larger counterparts in syntax correctness, semantic correctness, and
bug-distinguishing capability. These findings highlight that targeted
fine-tuning on a modest dataset can enable small models to achieve results
formerly seen only in massive, resource-heavy LLMs, offering a practical and
efficient path for the real-world adoption of automated specification
generation.

</details>


### [312] [Towards a Framework for Operationalizing the Specification of Trustworthy AI Requirements](https://arxiv.org/abs/2507.10228)
*Hugo Villamizar,Daniel Mendez,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文提出将AMDiRE和PerSpecML两种方法集成，以解决AI系统可信性需求工程问题，并指出研究方向和挑战。


<details>
  <summary>Details</summary>
Motivation: 随着对AI系统可信性的关注增加，需要结构化方法来处理难以指定的新兴、依赖上下文的属性。

Method: 将基于制品的AMDiRE方法和基于视角的PerSpecML方法集成。

Result: 设想了一条将可信性相关需求落地的途径，连接利益相关者的关注点和结构化制品模型。

Conclusion: 列出了关键研究方向和待与需求工程社区讨论的开放挑战。

Abstract: Growing concerns around the trustworthiness of AI-enabled systems highlight
the role of requirements engineering (RE) in addressing emergent,
context-dependent properties that are difficult to specify without structured
approaches. In this short vision paper, we propose the integration of two
complementary approaches: AMDiRE, an artefact-based approach for RE, and
PerSpecML, a perspective-based method designed to support the elicitation,
analysis, and specification of machine learning (ML)-enabled systems. AMDiRE
provides a structured, artefact-centric, process-agnostic methodology and
templates that promote consistency and traceability in the results; however, it
is primarily oriented toward deterministic systems. PerSpecML, in turn,
introduces multi-perspective guidance to uncover concerns arising from the
data-driven and non-deterministic behavior of ML-enabled systems. We envision a
pathway to operationalize trustworthiness-related requirements, bridging
stakeholder-driven concerns and structured artefact models. We conclude by
outlining key research directions and open challenges to be discussed with the
RE community.

</details>


### [313] [An Empirical Study of Interaction Bugs in ROS-based Software](https://arxiv.org/abs/2507.10235)
*Zhixiang Chen,Zhuangbin Chen,Xingjie Cai,Wei Li,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文对ROS机器人系统中的交互错误（iBugs）进行实证研究，分析121个iBugs并分类，得出改善预防和检测的方向。


<details>
  <summary>Details</summary>
Motivation: 机器人系统可靠性依赖组件交互，但交互相关可靠性问题（iBugs）研究不足，需深入探索。

Method: 对十个活跃维护且有代表性的ROS项目中的121个iBugs进行分析，将其分类并研究根源、修复策略和影响。

Result: iBugs分为系统内、硬件和环境三类，涵盖多种交互场景，得出了关于iBugs性质的发现。

Conclusion: 研究结果为设计更强大、更安全的机器人系统提供方向。

Abstract: Modern robotic systems integrate multiple independent software and hardware
components, each responsible for distinct functionalities such as perception,
decision-making, and execution. These components interact extensively to
accomplish complex end-to-end tasks. As a result, the overall system
reliability depends not only on the correctness of individual components, but
also on the correctness of their interactions. Failures often manifest at the
boundaries between components, yet interaction-related reliability issues in
robotics--referred to here as interaction bugs (iBugs)--remain underexplored.
  This work presents an empirical study of iBugs within robotic systems built
using the Robot Operating System (ROS), a widely adopted open-source robotics
framework. A total of 121 iBugs were analyzed across ten actively maintained
and representative ROS projects. The identified iBugs are categorized into
three major types: intra-system iBugs, hardware iBugs, and environmental iBugs,
covering a broad range of interaction scenarios in robotics. The analysis
includes an examination of root causes, fixing strategies, and the impact of
these bugs. Several findingsa are derived that shed light on the nature of
iBugs and suggest directions for improving their prevention and detection.
These insights aim to inform the design of more robust and safer robotic
systems.

</details>


### [314] [Helveg: Diagrams for Software Documentation](https://arxiv.org/abs/2507.10244)
*Adam Štěpánek,David Kuťák,Barbora Kozlíková,Jan Byška*

Main category: cs.SE

TL;DR: 文章介绍将软件架构可视化引入 API 参考文档的方法，开发原型 Helveg，初测有问题，改进后再次评估。


<details>
  <summary>Details</summary>
Motivation: 传统文本形式的文档不适合高层探索性分析，需要新方法帮助开发者理解代码库。

Method: 设计将软件架构可视化引入 API 参考文档的方法，实现原型 Helveg，改进其字形设计、交互方式和用户界面。

Result: 初测显示 Helveg 有可读性、直观性和用户体验问题，改进后再次评估。

Conclusion: 未明确提及最终结论，但通过改进和再次评估来解决初测问题。

Abstract: Software developers often have to gain an understanding of a codebase. Be it
programmers getting onboarded onto a team project or, for example, developers
striving to grasp an external open-source library. In either case, they
frequently turn to the project's documentation. However, documentation in its
traditional textual form is ill-suited for this kind of high-level exploratory
analysis, since it is immutable from the readers' perspective and thus forces
them to follow a predefined path. We have designed an approach bringing aspects
of software architecture visualization to API reference documentation. It
utilizes a highly interactive node-link diagram with expressive node glyphs and
flexible filtering capabilities, providing a high-level overview of the
codebase as well as details on demand. To test our design, we have implemented
a prototype named Helveg, capable of automatically generating diagrams of C\#
codebases. User testing of Helveg confirmed its potential, but it also revealed
problems with the readability, intuitiveness, and user experience of our tool.
Therefore, in this paper, which is an extended version of our VISSOFT paper
with DOI 10.1109/VISSOFT64034.2024.00012, we address many of these problems
through major changes to the glyph design, means of interaction, and user
interface of the tool. To assess the improvements, this new version of Helveg
was evaluated again with the same group of participants as the previous
version.

</details>


### [315] [A Grounded Theory on the Teacher and Student Roles in Pair Programming](https://arxiv.org/abs/2507.10305)
*Linus Ververs,Trang Linh Lam,Janina Berger,Lutz Prechelt*

Main category: cs.SE

TL;DR: 研究结对编程中知识转移何时会产生危害，定义角色、描述陷阱并提出扎根理论。


<details>
  <summary>Details</summary>
Motivation: 了解结对编程中知识转移在何种情况下会对编程过程造成危害。

Method: 采用扎根理论方法，对5家德国软件公司的18名开发者的17次结对编程会话进行记录，并对另外4家公司的不同开发者进行6次访谈。

Result: 定义学生和教师角色以应对知识差距，描述应避免的陷阱，围绕权力差距形成扎根理论。

Conclusion: 开发者若不关注伙伴需求和权力差距，知识转移可能有害，会引发防御行为并形成恶性循环，对知识转移、协作和代码质量产生负面影响。

Abstract: Context: Pair programming is an established (agile) practice and is practiced
throughout the industry. Objective: Understand under what circumstances
knowledge transfer can harm a pair programming session. Method: Grounded Theory
Methodology based on 17 recorded pair programming sessions with 18 developers
from 5 German software companies accompanied, by 6 interviews with different
developers from 4 other German companies. Results: We define the student and
teacher roles to help developers deal with a one-sided knowledge gap. We
describe pitfalls to avoid and develop a grounded theory centered around the
Power Gap in pair programming. Conclusions: Knowledge transfer can be harmful
when developers don't pay attention to their partners needs and desires. If
developers don't pay attention to the Power Gap and keep it in check, Defensive
Behavior may arise that leads to a vicious cycle impacting the knowledge
transfer, the Togetherness and the code quality in a negative way.

</details>


### [316] [Streamlined Airborne Software Development for Large UAVs: From Unified Data Collection to Automated Code Generation](https://arxiv.org/abs/2507.10321)
*Viktor Sinitsyn,Nils Schlautmann,Florian Schwaiger,Florian Holzapfel*

Main category: cs.SE

TL;DR: 航空航天业发展带来新挑战，本文提出精简数字接口和机载软件开发的方法与工具链并已成功应用。


<details>
  <summary>Details</summary>
Motivation: 航空航天业发展使初创公司面临处理机载设备数字通信接口的挑战，需有效解决方案。

Method: 提出一种注重自动化和灵活性，同时符合设计保证要求的新颖流程和工具链。

Result: 团队已在多个已完成项目中成功应用该方法和工具链。

Conclusion: 该方法和工具链可有效精简数字接口和机载软件开发。

Abstract: The aerospace industry has experienced significant transformations over the
last decade, driven by technological advancements and innovative solutions in
goods and personal transportation. This evolution has spurred the emergence of
numerous start-ups that now face challenges traditionally encountered by
established aerospace companies. Among these challenges is the efficient
processing of digital intra-device communication interfaces for onboard
equipment - a critical component for ensuring seamless system integration and
functionality. Addressing this challenge requires solutions that emphasize
clear and consistent interface descriptions, automation of processes, and
reduced labor-intensive efforts.
  This paper presents a novel process and toolchain designed to streamline the
development of digital interfaces and onboard software, which our team has
successfully applied in several completed projects. The proposed approach
focuses on automation and flexibility while maintaining compliance with design
assurance requirements.

</details>


### [317] [AssertCoder: LLM-Based Assertion Generation via Multimodal Specification Extraction](https://arxiv.org/abs/2507.10338)
*Enyuan Tian,Yiwei Ci,Qiusong Yang,Yufeng Li,Zhichao Lyu*

Main category: cs.SE

TL;DR: 提出AssertCoder框架自动从多模态硬件设计规范生成高质量SVAs，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手动编写高质量SVAs劳动密集且易出错，需要自动生成方法。

Method: 采用模态敏感预处理解析异构规范格式，用专用语义分析器提取结构化表示，通过多步思维链提示驱动断言合成，用基于变异的评估方法评估和优化断言。

Result: 在三个真实RTL设计实验中，功能正确性平均提升8.4%，变异检测平均提升5.8%。

Conclusion: AssertCoder框架性能优于现有方法，能有效自动生成高质量SVAs。

Abstract: Assertion-Based Verification (ABV) is critical for ensuring functional
correctness in modern hardware systems. However, manually writing high-quality
SVAs remains labor-intensive and error-prone. To bridge this gap, we propose
AssertCoder, a novel unified framework that automatically generates
high-quality SVAs directly from multimodal hardware design specifications.
AssertCoder employs a modality-sensitive preprocessing to parse heterogeneous
specification formats (text, tables, diagrams, and formulas), followed by a set
of dedicated semantic analyzers that extract structured representations aligned
with signal-level semantics. These representations are utilized to drive
assertion synthesis via multi-step chain-of-thought (CoT) prompting. The
framework incorporates a mutation-based evaluation approach to assess assertion
quality via model checking and further refine the generated assertions.
Experimental evaluation across three real-world Register-Transfer Level (RTL)
designs demonstrates AssertCoder's superior performance, achieving an average
increase of 8.4% in functional correctness and 5.8% in mutation detection
compared to existing state-of-the-art approaches.

</details>


### [318] [Self-Admitted GenAI Usage in Open-Source Software](https://arxiv.org/abs/2507.10422)
*Tao Xiao,Youmei Fan,Fabio Calefato,Christoph Treude,Raula Gaikovina Kula,Hideaki Hata,Sebastian Baltes*

Main category: cs.SE

TL;DR: 本文研究生成式AI在开源软件开发中的使用，分析大量GitHub仓库，得出使用相关分类，调查相关政策和开发者担忧，发现开发者主动管理使用，且GenAI未使代码变更普遍增加。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具广泛应用，但在开源软件开发中的实际使用和影响尚不清楚，需深入研究。

Method: 引入自我承认GenAI使用的概念，分析超25万个GitHub仓库，采用混合方法得出相关分类，分析政策文档并开展开发者调查，还研究GenAI采用对代码变更的长期影响。

Result: 识别出156个仓库的1292次自我承认，得出32个任务、10种内容类型和11种目的的分类，发现开发者主动管理GenAI使用，GenAI未使代码变更普遍增加。

Conclusion: 在AI辅助软件开发新时代，项目层面需有透明度、归因和质量控制措施。

Abstract: The widespread adoption of generative AI (GenAI) tools such as GitHub Copilot
and ChatGPT is transforming software development. Since generated source code
is virtually impossible to distinguish from manually written code, their
real-world usage and impact on open-source software development remain poorly
understood. In this paper, we introduce the concept of self-admitted GenAI
usage, that is, developers explicitly referring to the use of GenAI tools for
content creation in software artifacts. Using this concept as a lens to study
how GenAI tools are integrated into open-source software projects, we analyze a
curated sample of more than 250,000 GitHub repositories, identifying 1,292 such
self-admissions across 156 repositories in commit messages, code comments, and
project documentation. Using a mixed methods approach, we derive a taxonomy of
32 tasks, 10 content types, and 11 purposes associated with GenAI usage based
on 284 qualitatively coded mentions. We then analyze 13 documents with policies
and usage guidelines for GenAI tools and conduct a developer survey to uncover
the ethical, legal, and practical concerns behind them. Our findings reveal
that developers actively manage how GenAI is used in their projects,
highlighting the need for project-level transparency, attribution, and quality
control practices in the new era of AI-assisted software development. Finally,
we examine the longitudinal impact of GenAI adoption on code churn in 151
repositories with self-admitted GenAI usage and find no general increase,
contradicting popular narratives on the impact of GenAI on software
development.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [319] [Function approximations for counterparty credit exposure calculations](https://arxiv.org/abs/2507.09004)
*Domagoj Demeterfi,Kathrin Glau,Linus Wunderlich*

Main category: q-fin.CP

TL;DR: 本文研究用函数近似替代衍生品定价器以解决金融机构衡量风险时计算负担和简化风险的两难问题，给出误差界，证明收敛性，有渐近效率增益，数值实验显示显著减少运行时间。


<details>
  <summary>Details</summary>
Motivation: 金融机构在定期衡量风险敞口时面临计算负担过重或风险简化过度的两难困境，需要解决该问题。

Method: 系统地用函数近似替代频繁调用的衍生品定价器，采用Chebyshev插值，推导误差界。

Result: 在温和条件下得到概率和有限样本误差界，有渐近效率增益，数值实验显示运行时间显著减少，加速因子最高达230，平均为87。

Conclusion: 该方法能有效解决金融机构衡量风险敞口的困境，且自适应选择插值度及对Greeks的近似体现了方法的优点。

Abstract: The challenge to measure exposures regularly forces financial institutions
into a choice between an overwhelming computational burden or
oversimplification of risk. To resolve this unsettling dilemma, we
systematically investigate replacing frequently called derivative pricers by
function approximations covering all practically relevant exposure measures,
including quantiles. We prove error bounds for exposure measures in terms of
the $L^p$ norm, $1 \leq p < \infty$, and for the uniform norm. To fully exploit
these results, we employ the Chebyshev interpolation and show exponential
convergence of the resulting exposure calculations. As our main result we
derive probabilistic and finite sample error bounds under mild conditions
including the natural case of unbounded risk factors. We derive an asymptotic
efficiency gain scaling with $n^{1/2-\varepsilon}$ for any $\varepsilon>0$ with
$n$ the number of simulations. Our numerical experiments cover callable,
barrier, stochastic volatility and jump features. Using 10\,000 simulations, we
consistently observe significant run-time reductions in all cases with speed-up
factors up to 230, and an average speed-up of 87. We also present an adaptive
choice of the interpolation degree. Finally, numerical examples relying on the
approximation of Greeks highlight the merit of the method beyond the presented
theory.

</details>


### [320] [Joint deep calibration of the 4-factor PDV model](https://arxiv.org/abs/2507.09412)
*Fabio Baschetti,Giacomo Bormetti,Pietro Rossi*

Main category: q-fin.CP

TL;DR: 本文针对SPX和VIX市场数据联合校准计算成本高的问题，学习SPX隐含波动率、VIX期货和VIX看涨期权价格，将定价函数简化为矩阵 - 向量乘积，大幅缩短校准时间。


<details>
  <summary>Details</summary>
Motivation: 解决联合校准SPX和VIX市场数据时计算成本高，尤其是使用嵌套蒙特卡罗模拟时外循环模拟慢的问题。

Method: 学习SPX隐含波动率、VIX期货和VIX看涨期权价格，将定价函数简化为可即时评估的矩阵 - 向量乘积。

Result: 将校准时间缩短至仅需几秒。

Conclusion: 通过学习相关价格并简化定价函数，能有效克服联合校准计算成本高的限制。

Abstract: Joint calibration to SPX and VIX market data is a delicate task that requires
sophisticated modeling and incurs significant computational costs. The latter
is especially true when pricing of volatility derivatives hinges on nested
Monte Carlo simulation. One such example is the 4-factor Markov Path-Dependent
Volatility (PDV) model of Guyon and Lekeufack (2023). Nonetheless, its realism
has earned it considerable attention in recent years. Gazzani and Guyon (2025)
marked a relevant contribution by learning the VIX as a random variable, i.e.,
a measurable function of the model parameters and the Markovian factors. A
neural network replaces the inner simulation and makes the joint calibration
problem accessible. However, the minimization loop remains slow due to
expensive outer simulation. The present paper overcomes this limitation by
learning SPX implied volatilities, VIX futures, and VIX call option prices. The
pricing functions reduce to simple matrix-vector products that can be evaluated
on the fly, shrinking calibration times to just a few seconds.

</details>


### [321] [Enhancing Trading Performance Through Sentiment Analysis with Large Language Models: Evidence from the S&P 500](https://arxiv.org/abs/2507.09739)
*Haojie Liu,Zihan Lin,Randall R. Rojas*

Main category: q-fin.CP

TL;DR: 研究整合金融新闻情绪分析、GPT - 2、FinBERT与技术指标和时间序列模型优化标普500交易策略，结果显示结合方法能提升交易表现。


<details>
  <summary>Details</summary>
Motivation: 优化标普500的交易策略，以更好适应市场变化。

Method: 整合金融新闻实时情绪分析、GPT - 2、FinBERT与ARIMA、ETS等时间序列模型，结合情绪数据与动量和趋势指标，评估基准买入持有和基于情绪的方法。

Result: 结合情绪驱动的见解与传统模型能改善交易表现。

Conclusion: 结合情绪分析和传统模型为股票交易提供了更动态的方法，能适应波动市场环境。

Abstract: This study integrates real-time sentiment analysis from financial news, GPT-2
and FinBERT, with technical indicators and time-series models like ARIMA and
ETS to optimize S&P 500 trading strategies. By merging sentiment data with
momentum and trend-based metrics, including a benchmark buy-and-hold and
sentiment-based approach, is evaluated through assets values and returns.
Results show that combining sentiment-driven insights with traditional models
improves trading performance, offering a more dynamic approach to stock trading
that adapts to market changes in volatile environments.

</details>


### [322] [Towards Realistic and Interpretable Market Simulations: Factorizing Financial Power Law using Optimal Transport](https://arxiv.org/abs/2507.09863)
*Ryuji Hashimoto,Kiyoshi Izumi*

Main category: q-fin.CP

TL;DR: 通过人工市场模拟研究股票收益幂律分布机制，发现价格信息效应起主导作用，多因素协同放大该效应。


<details>
  <summary>Details</summary>
Motivation: 传统金融理论假设价格波动为高斯分布，但实证显示收益分布尾部为幂律分布，以往研究未将多种因素建模评估其贡献，且现实市场复杂难以分离单一因素影响。

Method: 构建人工市场，用最优传输（OT）作为定量相似性度量进行受控实验，在代理模型中逐步引入行为组件，通过OT距离比较模拟输出和实证数据。

Result: 价格的信息效应在再现幂律行为中起主导作用，多个组件协同放大该效应。

Conclusion: 价格信息效应及多组件协同作用对股票收益幂律分布有重要影响。

Abstract: We investigate the mechanisms behind the power-law distribution of stock
returns using artificial market simulations. While traditional financial theory
assumes Gaussian price fluctuations, empirical studies consistently show that
the tails of return distributions follow a power law. Previous research has
proposed hypotheses for this phenomenon -- some attributing it to investor
behavior, others to institutional demand imbalances. However, these factors
have rarely been modeled together to assess their individual and joint
contributions. The complexity of real financial markets complicates the
isolation of the contribution of a single component using existing data. To
address this, we construct artificial markets and conduct controlled
experiments using optimal transport (OT) as a quantitative similarity measure.
Our proposed framework incrementally introduces behavioral components into the
agent models, allowing us to compare each simulation output with empirical data
via OT distances. The results highlight that informational effect of prices
plays a dominant role in reproducing power-law behavior and that multiple
components interact synergistically to amplify this effect.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [323] [Analyzing the Crowding-Out Effect of Investment Herding on Consumption: An Optimal Control Theory Approach](https://arxiv.org/abs/2507.10052)
*Huisheng Wang,H. Vicky Zhao*

Main category: q-fin.PM

TL;DR: 研究投资羊群效应下家庭最优投资和消费决策，理论分析并实证验证挤出效应，为政策制定者提供参考。


<details>
  <summary>Details</summary>
Motivation: 现有研究对投资羊群效应影响消费的定量研究有限，需进一步探究其对家庭决策的影响。

Method: 构建优化问题，基于最优控制理论求解最优投资和消费决策的解析解，进行理论分析和实际数据测试。

Result: 理论上证明了挤出效应的存在，探索了参数对挤出效应的影响，并通过实际数据验证理论分析。

Conclusion: 本研究有助于理解投资羊群效应对家庭消费的影响，为刺激消费和减轻其对经济增长负面影响的政策制定提供有价值的见解。

Abstract: Investment herding, a phenomenon where households mimic the decisions of
others rather than relying on their own analysis, has significant effects on
financial markets and household behavior. Excessive investment herding may
reduce investments and lead to a depletion of household consumption, which is
called the crowding-out effect. While existing research has qualitatively
examined the impact of investment herding on consumption, quantitative studies
in this area remain limited. In this work, we investigate the optimal
investment and consumption decisions of households under the impact of
investment herding. We formulate an optimization problem to model how
investment herding influences household decisions over time. Based on the
optimal control theory, we solve for the analytical solutions of optimal
investment and consumption decisions. We theoretically analyze the impact of
investment herding on household consumption decisions and demonstrate the
existence of the crowding-out effect. We further explore how parameters, such
as interest rate, excess return rate, and volatility, influence the
crowding-out effect. Finally, we conduct a real data test to validate our
theoretical analysis of the crowding-out effect. This study is crucial to
understanding the impact of investment herding on household consumption and
offering valuable insights for policymakers seeking to stimulate consumption
and mitigate the negative effects of investment herding on economic growth.

</details>


### [324] [Solving dynamic portfolio selection problems via score-based diffusion models](https://arxiv.org/abs/2507.09916)
*Ahmad Aghapour,Erhan Bayraktar,Fengyi Yuan*

Main category: q-fin.PM

TL;DR: 本文基于扩散模型以无模型方式解决动态均值 - 方差投资组合选择问题，提出训练生成模型、量化界限、证明稳定性，给出策略梯度算法并在模拟和真实数据上验证性能。


<details>
  <summary>Details</summary>
Motivation: 以无模型方式解决动态均值 - 方差投资组合选择问题。

Method: 用有限大小的真实模型数据训练生成模型，采用自适应训练和采样方法，获得量化界限，证明均值 - 方差投资组合优化问题的稳定性，提出基于生成环境的策略梯度算法。

Result: 算法在模拟和真实数据上展示了性能，基于生成环境的算法在真实数据上产生的投资组合击败了多个重要基线。

Conclusion: 基于生成环境的策略梯度算法在动态均值 - 方差投资组合选择问题上表现良好，具有实际应用价值。

Abstract: In this paper, we tackle the dynamic mean-variance portfolio selection
problem in a {\it model-free} manner, based on (generative) diffusion models.
We propose using data sampled from the real model $\mathcal P$ (which is
unknown) with limited size to train a generative model $\mathcal Q$ (from which
we can easily and adequately sample). With adaptive training and sampling
methods that are tailor-made for time series data, we obtain quantification
bounds between $\mathcal P$ and $\mathcal Q$ in terms of the adapted
Wasserstein metric $\mathcal A W_2$. Importantly, the proposed adapted sampling
method also facilitates {\it conditional sampling}. In the second part of this
paper, we provide the stability of the mean-variance portfolio optimization
problems in $\mathcal A W _2$. Then, combined with the error bounds and the
stability result, we propose a policy gradient algorithm based on the
generative environment, in which our innovative adapted sampling method
provides approximate scenario generators. We illustrate the performance of our
algorithm on both simulated and real data. For real data, the algorithm based
on the generative environment produces portfolios that beat several important
baselines, including the Markowitz portfolio, the equal weight (naive)
portfolio, and S\&P 500.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [325] [Quantifying Crypto Portfolio Risk: A Simulation-Based Framework Integrating Volatility, Hedging, Contagion, and Monte Carlo Modeling](https://arxiv.org/abs/2507.08915)
*Kiarash Firouzi*

Main category: q-fin.RM

TL;DR: 本文提出用于加密投资组合风险分析的模块化模拟框架，并通过实证验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统金融风险模型假设的正态性和集中控制常使其错过加密货币市场的极端波动、非线性依赖和系统脆弱性等变化。

Method: 将波动性压力测试、稳定币对冲、传染建模和蒙特卡罗模拟四个组件集成到模块化模拟框架，各模块基于数学金融理论。

Result: 利用2020 - 2024年USDT、ETH和BTC数据进行实证验证。

Conclusion: 框架具有鲁棒性和实际相关性。

Abstract: Extreme volatility, nonlinear dependencies, and systemic fragility are
characteristics of cryptocurrency markets. The assumptions of normality and
centralized control in traditional financial risk models frequently cause them
to miss these changes. Four components-volatility stress testing, stablecoin
hedging, contagion modeling, and Monte Carlo simulation-are integrated into
this paper's modular simulation framework for crypto portfolio risk analysis.
Every module is based on mathematical finance theory, which includes stochastic
price path generation, correlation-based contagion propagation, and
mean-variance optimization. The robustness and practical relevance of the
framework are demonstrated through empirical validation utilizing 2020-2024
USDT, ETH, and BTC data.

</details>


### [326] [Generalized Orlicz premia](https://arxiv.org/abs/2507.09181)
*Mücahit Aygün,Fabio Bellini,Roger J. A. Laeven*

Main category: q-fin.RM

TL;DR: 介绍基于非凸损失函数的广义Orlicz保费，涵盖多种例子，研究其性质、对偶表示，证明其为唯一可引出、正齐次、单调和归一化的泛函。


<details>
  <summary>Details</summary>
Motivation: 引入更广义的Orlicz保费定义，以覆盖更多相关例子并研究其性质。

Method: 理论推导，建立现金可加性与$L^p$-分位数的关系，讨论对偶表示并进行比较。

Result: 广义定义涵盖几何均值和期望分位数等例子，现金可加性导致$L^p$-分位数，得到广义Orlicz保费的对偶表示。

Conclusion: 广义Orlicz保费是唯一可引出、正齐次、单调和归一化的泛函。

Abstract: We introduce a generalized version of Orlicz premia, based on possibly
non-convex loss functions. We show that this generalized definition covers a
variety of relevant examples, such as the geometric mean and the expectiles,
while at the same time retaining a number of relevant properties. We establish
that cash-additivity leads to $L^p$-quantiles, extending a classical result on
'collapse to the mean' for convex Orlicz premia.
  We then focus on the geometrically convex case, discussing the dual
representation of generalized Orlicz premia and comparing it with a
multiplicative form of the standard dual representation for the convex case.
Finally, we show that generalized Orlicz premia arise naturally as the only
elicitable, positively homogeneous, monotone and normalized functionals.

</details>


### [327] [Norms Based on Generalized Expected-Shortfalls and Applications](https://arxiv.org/abs/2507.09444)
*Shuyu Gong,Taizhong Hu,Zhenfeng Zou*

Main category: q-fin.RM

TL;DR: 本文提出通过失真风险度量构建的广义预期损失（ES）范数，建立风险量化统一框架，拓展传统ES方法并应用于投资组合优化和金融时间序列异常检测。


<details>
  <summary>Details</summary>
Motivation: 构建新的风险量化统一分析框架，拓展传统ES方法。

Method: 通过失真风险度量构建广义ES范数，发展其数学对偶理论，解决投影问题。

Result: 建立了统一分析框架，展示了在投资组合优化中的实用性，应用于金融时间序列异常检测。

Conclusion: 广义ES范数为风险量化和金融数据分析提供了有效工具。

Abstract: This paper proposes a novel class of generalized Expected-Shortfall (ES)
norms constructed via distortion risk measures, establishing a unified
analytical framework for risk quantification. The proposed norms extend
conventional ES methodology by incorporating flexible distortion functions.
Specifically, we develop the mathematical duality theory for generalized-ES
norms to support portfolio optimization tasks, while demonstrating their
practical utility through projection problem solutions. The generalizedES norms
are also applied to detect anomalies of financial time series data.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [328] [A Framework for Predictive Directional Trading Based on Volatility and Causal Inference](https://arxiv.org/abs/2507.09347)
*Ivan Letteri*

Main category: q-fin.ST

TL;DR: 本文提出用于识别和利用金融市场预测性领先 - 滞后关系的框架，结合统计方法与机器学习模型，经测试策略效果显著，为金融建模和算法交易提供方法。


<details>
  <summary>Details</summary>
Motivation: 引入新框架，结合统计和机器学习方法，增强对股票间预测关系的识别和利用。

Method: 用高斯混合模型聚类股票，构建多阶段因果推断管道识别关联，用动态时间规整和K近邻分类器确定交易执行的最佳时间滞后，并进行回测。

Result: 基于波动率的交易策略在测试期表现良好，投资组合总回报15.38%，优于买入持有策略，关键指标证明策略可行。

Conclusion: 研究提供了识别盈利交易机会的系统方法，对金融建模学术研究和算法交易实际应用有重要意义。

Abstract: Purpose: This study introduces a novel framework for identifying and
exploiting predictive lead-lag relationships in financial markets. We propose
an integrated approach that combines advanced statistical methodologies with
machine learning models to enhance the identification and exploitation of
predictive relationships between equities. Methods: We employed a Gaussian
Mixture Model (GMM) to cluster nine prominent stocks based on their mid-range
historical volatility profiles over a three-year period. From the resulting
clusters, we constructed a multi-stage causal inference pipeline, incorporating
the Granger Causality Test (GCT), a customised Peter-Clark Momentary
Conditional Independence (PCMCI) test, and Effective Transfer Entropy (ETE) to
identify robust, predictive linkages. Subsequently, Dynamic Time Warping (DTW)
and a K-Nearest Neighbours (KNN) classifier were utilised to determine the
optimal time lag for trade execution. The resulting strategy was rigorously
backtested. Results: The proposed volatility-based trading strategy, tested
from 8 June 2023 to 12 August 2023, demonstrated substantial efficacy. The
portfolio yielded a total return of 15.38%, significantly outperforming the
10.39% return of a comparative Buy-and-Hold strategy. Key performance metrics,
including a Sharpe Ratio up to 2.17 and a win rate up to 100% for certain
pairs, confirmed the strategy's viability. Conclusion: This research
contributes a systematic and robust methodology for identifying profitable
trading opportunities derived from volatility-based causal relationships. The
findings have significant implications for both academic research in financial
modelling and the practical application of algorithmic trading, offering a
structured approach to developing resilient, data-driven strategies.

</details>


### [329] [Mapping Crisis-Driven Market Dynamics: A Transfer Entropy and Kramers-Moyal Approach to Financial Networks](https://arxiv.org/abs/2507.09554)
*Pouriya Khalilian,Amirhossein N. Golestani,Mohammad Eslamifar,Mostafa T. Firouzjaee,Javad T. Firouzjaee*

Main category: q-fin.ST

TL;DR: 提出整合转移熵和N维Kramers - Moyal展开的统一框架分析四大指数，发现特殊时期方向性信息流增强，揭示线性方法不足，为对冲和政策提供见解。


<details>
  <summary>Details</summary>
Motivation: 金融市场动态且相互关联，传统相关分析无法捕捉信息流方向性和时间动态，需要新方法分析市场。

Method: 整合转移熵和N维Kramers - Moyal展开的统一框架，使用2014年8月11日至2024年9月8日的每日数据，进行滑动窗口分析。

Result: 疫情和俄乌危机期间平均转移熵分别增加35%和28%；漂移系数显示黄金 - 美元互动是避险渠道，石油 - 股票联系有状态转变。

Conclusion: 线性度量有缺陷，结合信息论和随机漂移方法有价值，能为自适应对冲和宏观审慎政策提供见解。

Abstract: Financial markets are dynamic, interconnected systems where local shocks can
trigger widespread instability, challenging portfolio managers and
policymakers. Traditional correlation analysis often miss the directionality
and temporal dynamics of information flow. To address this, we present a
unified framework integrating Transfer Entropy (TE) and the N-dimensional
Kramers-Moyal (KM) expansion to map static and time-resolved coupling among
four major indices: Nasdaq Composite (^IXIC), WTI crude oil (WTI), gold (GC=F),
and the US Dollar Index (DX-Y.NYB). TE captures directional information flow.
KM models non-linear stochastic dynamics, revealing interactions often
overlooked by linear methods. Using daily data from August 11, 2014, to
September 8, 2024, we compute returns, confirm non-stationary using a conduct
sliding-window TE and KM analyses. We find that during the COVID-19 pandemic
(March-June 2020) and the Russia-Ukraine crisis (Feb-Apr 2022), average TE
increases by 35% and 28%, respectively, indicating heightened directional flow.
Drift coefficients highlight gold-dollar interactions as a persistent
safe-haven channel, while oil-equity linkages show regime shifts, weakening
under stress and rebounding quickly. Our results expose the shortcomings of
linear measures and underscore the value of combining information-theoretic and
stochastic drift methods. This approach offers actionable insights for adaptive
hedging and informs macro-prudential policy by revealing the evolving
architecture of systemic risk.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [330] [Boltzmann Price: Toward Understanding the Fair Price in High-Frequency Markets](https://arxiv.org/abs/2507.09734)
*Przemysław Rola*

Main category: q-fin.TR

TL;DR: 本文引入基于最大熵原理的参数化价格族，提出价格动态模型，能产生更高峰度和重尾分布，经模拟验证与历史数据拟合良好。


<details>
  <summary>Details</summary>
Motivation: 构建结合价格、成交量不平衡和买卖价差的理论框架，改进标准价格动态模型。

Method: 基于最大熵原理得到价格，利用买卖状态概率构建价格动态模型，通过模拟验证模型。

Result: 模型能产生更高峰度和重尾分布，漂移项自然产生，与历史股权数据拟合良好。

Conclusion: 该模型提供了一个整合价格、成交量不平衡和价差的理论框架。

Abstract: In this paper, we introduce a parametrized family of prices derived from the
Maximum Entropy Principle. The price is obtained from the distribution that
minimizes bias, given the bid and ask volume imbalance at the top of the order
book. Under specific parameter choices, it closely approximates the mid-price
or the weighted mid-price. Using probabilities of bid and ask states, we
propose a model of price dynamics in which both drift and volatility are driven
by volume imbalance. Compared to standard models like Bachelier or Geometric
Brownian Motion with constant volatility, our model can generate higher
kurtosis and heavy-tailed distributions. Additionally, the drift term naturally
emerges as a consequence of the order book imbalance. We validate the model
through simulation and demonstrate its fit to historical equity data. The model
provides a theoretical framework, integrating price, volume imbalance, and
spread.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [331] [Physics-informed machine learning: A mathematical framework with applications to time series forecasting](https://arxiv.org/abs/2507.08906)
*Nathan Doumèche*

Main category: stat.ML

TL;DR: 本文分析物理信息机器学习（PIML）方法统计特性，用核方法研究其行为、开发算法，还探索能源信号预测工业应用并引入时间序列物理约束框架。


<details>
  <summary>Details</summary>
Motivation: 深入理解PIML方法特性并拓展其在工业中的应用。

Method: 分析PIML方法统计属性，将PIML问题转化为核方法，用核岭回归工具研究，开发新算法并在GPU实现；应用于能源信号预测等工业场景。

Result: 分析了PINNs在逼近、一致性等方面特性；在电动汽车充电占用和电力需求等预测有成果；引入时间序列物理约束框架。

Conclusion: PIML方法有良好统计特性，可借助核方法研究，在能源预测等工业应用有成效，物理约束框架可用于多领域预测。

Abstract: Physics-informed machine learning (PIML) is an emerging framework that
integrates physical knowledge into machine learning models. This physical prior
often takes the form of a partial differential equation (PDE) system that the
regression function must satisfy. In the first part of this dissertation, we
analyze the statistical properties of PIML methods. In particular, we study the
properties of physics-informed neural networks (PINNs) in terms of
approximation, consistency, overfitting, and convergence. We then show how PIML
problems can be framed as kernel methods, making it possible to apply the tools
of kernel ridge regression to better understand their behavior. In addition, we
use this kernel formulation to develop novel physics-informed algorithms and
implement them efficiently on GPUs. The second part explores industrial
applications in forecasting energy signals during atypical periods. We present
results from the Smarter Mobility challenge on electric vehicle charging
occupancy and examine the impact of mobility on electricity demand. Finally, we
introduce a physics-constrained framework for designing and enforcing
constraints in time series, applying it to load forecasting and tourism
forecasting in various countries.

</details>


### [332] [The Bayesian Approach to Continual Learning: An Overview](https://arxiv.org/abs/2507.08922)
*Tameem Adel*

Main category: stat.ML

TL;DR: 该综述探讨贝叶斯持续学习，介绍定义、分类算法、与相关领域联系，分析现状、挑战并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 持续学习与贝叶斯推理本质契合，可解决深度模型应用于现实问题的挑战，因此研究贝叶斯持续学习不同设置。

Method: 先讨论持续学习定义、贝叶斯设置及与相关领域联系，再对算法分类，分析当前先进算法，探讨与发展心理学联系。

Result: 给出贝叶斯持续学习算法分类，分析了部分先进算法，指出与发展心理学的联系和当前挑战。

Conclusion: 提出贝叶斯持续学习未来潜在研究领域。

Abstract: Continual learning is an online paradigm where a learner continually
accumulates knowledge from different tasks encountered over sequential time
steps. Importantly, the learner is required to extend and update its knowledge
without forgetting about the learning experience acquired from the past, and
while avoiding the need to retrain from scratch. Given its sequential nature
and its resemblance to the way humans think, continual learning offers an
opportunity to address several challenges which currently stand in the way of
widening the range of applicability of deep models to further real-world
problems. The continual need to update the learner with data arriving
sequentially strikes inherent congruence between continual learning and
Bayesian inference which provides a principal platform to keep updating the
prior beliefs of a model given new data, without completely forgetting the
knowledge acquired from the old data. This survey inspects different settings
of Bayesian continual learning, namely task-incremental learning and
class-incremental learning. We begin by discussing definitions of continual
learning along with its Bayesian setting, as well as the links with related
fields, such as domain adaptation, transfer learning and meta-learning.
Afterwards, we introduce a taxonomy offering a comprehensive categorization of
algorithms belonging to the Bayesian continual learning paradigm. Meanwhile, we
analyze the state-of-the-art while zooming in on some of the most prominent
Bayesian continual learning algorithms to date. Furthermore, we shed some light
on links between continual learning and developmental psychology, and
correspondingly introduce analogies between both fields. We follow that with a
discussion of current challenges, and finally conclude with potential areas for
future research on Bayesian continual learning.

</details>


### [333] [MF-GLaM: A multifidelity stochastic emulator using generalized lambda models](https://arxiv.org/abs/2507.10303)
*K. Giannoukou,X. Zhu,S. Marelli,B. Sudret*

Main category: stat.ML

TL;DR: 提出多保真广义 lambda 模型（MF - GLaMs）来有效模拟高保真随机模拟器的条件响应分布，通过合成示例和实际地震应用验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随机模拟器输出有随机性，传统确定性代理建模技术难模拟其条件概率分布，准确表征响应分布需大量数据，构建多保真模拟器预测随机模拟器完整条件响应分布仍具挑战。

Method: 提出 MF - GLaMs，基于广义 lambda 模型（GLaM），利用低保真随机模拟器数据，且该方法非侵入式。

Result: 通过合成示例和地震应用表明，MF - GLaMs 在与单保真 GLaMs 相同成本下可提高精度，或在显著降低成本时取得相当性能。

Conclusion: MF - GLaMs 能有效模拟高保真随机模拟器的条件响应分布，在成本和精度上有优势。

Abstract: Stochastic simulators exhibit intrinsic stochasticity due to unobservable,
uncontrollable, or unmodeled input variables, resulting in random outputs even
at fixed input conditions. Such simulators are common across various scientific
disciplines; however, emulating their entire conditional probability
distribution is challenging, as it is a task traditional deterministic
surrogate modeling techniques are not designed for. Additionally, accurately
characterizing the response distribution can require prohibitively large
datasets, especially for computationally expensive high-fidelity (HF)
simulators. When lower-fidelity (LF) stochastic simulators are available, they
can enhance limited HF information within a multifidelity surrogate modeling
(MFSM) framework. While MFSM techniques are well-established for deterministic
settings, constructing multifidelity emulators to predict the full conditional
response distribution of stochastic simulators remains a challenge. In this
paper, we propose multifidelity generalized lambda models (MF-GLaMs) to
efficiently emulate the conditional response distribution of HF stochastic
simulators by exploiting data from LF stochastic simulators. Our approach
builds upon the generalized lambda model (GLaM), which represents the
conditional distribution at each input by a flexible, four-parameter
generalized lambda distribution. MF-GLaMs are non-intrusive, requiring no
access to the internal stochasticity of the simulators nor multiple
replications of the same input values. We demonstrate the efficacy of MF-GLaM
through synthetic examples of increasing complexity and a realistic earthquake
application. Results show that MF-GLaMs can achieve improved accuracy at the
same cost as single-fidelity GLaMs, or comparable performance at significantly
reduced cost.

</details>


### [334] [Fixed-Confidence Multiple Change Point Identification under Bandit Feedback](https://arxiv.org/abs/2507.08994)
*Joseph Lazzaro,Ciara Pike-Burke*

Main category: stat.ML

TL;DR: 本文引入固定置信度分段常数多臂老虎机问题，给出变点识别复杂度的下界，设计了一个简单高效的算法并证明其渐近最优性，通过实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 实际中需要尽快且有信心地识别分段常数函数中突变点的位置。

Method: 引入固定置信度分段常数多臂老虎机问题，给出变点识别复杂度的实例相关下界，基于此设计Track - and - Stop的变体算法。

Result: 所设计的算法在许多情况下渐近最优，合成环境实验结果支持理论发现。

Conclusion: 提出的方法在分段常数函数变点识别问题上有效且计算高效。

Abstract: Piecewise constant functions describe a variety of real-world phenomena in
domains ranging from chemistry to manufacturing. In practice, it is often
required to confidently identify the locations of the abrupt changes in these
functions as quickly as possible. For this, we introduce a fixed-confidence
piecewise constant bandit problem. Here, we sequentially query points in the
domain and receive noisy evaluations of the function under bandit feedback. We
provide instance-dependent lower bounds for the complexity of change point
identification in this problem. These lower bounds illustrate that an optimal
method should focus its sampling efforts adjacent to each of the change points,
and the number of samples around each change point should be inversely
proportional to the magnitude of the change. Building on this, we devise a
simple and computationally efficient variant of Track-and-Stop and prove that
it is asymptotically optimal in many regimes. We support our theoretical
findings with experimental results in synthetic environments demonstrating the
efficiency of our method.

</details>


### [335] [Optimal High-probability Convergence of Nonlinear SGD under Heavy-tailed Noise via Symmetrization](https://arxiv.org/abs/2507.09093)
*Aleksandar Armacki,Dragana Bajovic,Dusan Jakovetic,Soummya Kar*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study convergence in high-probability of SGD-type methods in non-convex
optimization and the presence of heavy-tailed noise. To combat the heavy-tailed
noise, a general black-box nonlinear framework is considered, subsuming
nonlinearities like sign, clipping, normalization and their smooth
counterparts. Our first result shows that nonlinear SGD (N-SGD) achieves the
rate $\widetilde{\mathcal{O}}(t^{-1/2})$, for any noise with unbounded moments
and a symmetric probability density function (PDF). Crucially, N-SGD has
exponentially decaying tails, matching the performance of linear SGD under
light-tailed noise. To handle non-symmetric noise, we propose two novel
estimators, based on the idea of noise symmetrization. The first, dubbed
Symmetrized Gradient Estimator (SGE), assumes a noiseless gradient at any
reference point is available at the start of training, while the second, dubbed
Mini-batch SGE (MSGE), uses mini-batches to estimate the noiseless gradient.
Combined with the nonlinear framework, we get N-SGE and N-MSGE methods,
respectively, both achieving the same convergence rate and exponentially
decaying tails as N-SGD, while allowing for non-symmetric noise with unbounded
moments and PDF satisfying a mild technical condition, with N-MSGE additionally
requiring bounded noise moment of order $p \in (1,2]$. Compared to works
assuming noise with bounded $p$-th moment, our results: 1) are based on a novel
symmetrization approach; 2) provide a unified framework and relaxed moment
conditions; 3) imply optimal oracle complexity of N-SGD and N-SGE, strictly
better than existing works when $p < 2$, while the complexity of N-MSGE is
close to existing works. Compared to works assuming symmetric noise with
unbounded moments, we: 1) provide a sharper analysis and improved rates; 2)
facilitate state-dependent symmetric noise; 3) extend the strong guarantees to
non-symmetric noise.

</details>


### [336] [CoVAE: Consistency Training of Variational Autoencoders](https://arxiv.org/abs/2507.09103)
*Gianluigi Silvestri,Luca Ambrogioni*

Main category: stat.ML

TL;DR: 提出CoVAE单阶段生成式自编码框架，可一步或几步生成高质量样本，优于等效VAE和其他单阶段VAE方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成式方法两阶段训练存在计算开销和采样时间增加问题，需新方法解决。

Method: 提出CoVAE，编码器学习带递增编码噪声的潜在表示，用时间相关β参数调节KL损失，解码器用带变分正则化的一致性损失训练。

Result: CoVAE可在一步或几步内生成高质量样本，无需学习先验，显著优于等效VAE和其他单阶段VAE方法。

Conclusion: CoVAE为自编码和扩散式生成建模提供统一框架，是一步生成高性能自编码的可行途径。

Abstract: Current state-of-the-art generative approaches frequently rely on a two-stage
training procedure, where an autoencoder (often a VAE) first performs
dimensionality reduction, followed by training a generative model on the
learned latent space. While effective, this introduces computational overhead
and increased sampling times. We challenge this paradigm by proposing
Consistency Training of Variational AutoEncoders (CoVAE), a novel single-stage
generative autoencoding framework that adopts techniques from consistency
models to train a VAE architecture. The CoVAE encoder learns a progressive
series of latent representations with increasing encoding noise levels,
mirroring the forward processes of diffusion and flow matching models. This
sequence of representations is regulated by a time dependent $\beta$ parameter
that scales the KL loss. The decoder is trained using a consistency loss with
variational regularization, which reduces to a conventional VAE loss at the
earliest latent time. We show that CoVAE can generate high-quality samples in
one or few steps without the use of a learned prior, significantly
outperforming equivalent VAEs and other single-stage VAEs methods. Our approach
provides a unified framework for autoencoding and diffusion-style generative
modeling and provides a viable route for one-step generative high-performance
autoencoding. Our code is publicly available at
https://github.com/gisilvs/covae.

</details>


### [337] [A Generalization Theory for Zero-Shot Prediction](https://arxiv.org/abs/2507.09128)
*Ronak Mehta,Zaid Harchaoui*

Main category: stat.ML

TL;DR: 提出理论框架理解零样本预测，识别目标量和关键条件独立关系。


<details>
  <summary>Details</summary>
Motivation: 更好理解机器学习和AI中零样本预测这一现代泛化范式。

Method: 提出理论框架。

Result: 识别出零样本预测目标量和关键条件独立关系。

Conclusion: 该理论框架有助于理解零样本预测方法。

Abstract: A modern paradigm for generalization in machine learning and AI consists of
pre-training a task-agnostic foundation model, generally obtained using
self-supervised and multimodal contrastive learning. The resulting
representations can be used for prediction on a downstream task for which no
labeled data is available. We present a theoretical framework to better
understand this approach, called zero-shot prediction. We identify the target
quantities that zero-shot prediction aims to learn, or learns in passing, and
the key conditional independence relationships that enable its generalization
ability.

</details>


### [338] [A Randomized Algorithm for Sparse PCA based on the Basic SDP Relaxation](https://arxiv.org/abs/2507.09148)
*Alberto Del Pia,Dekun Zhou*

Main category: stat.ML

TL;DR: 本文提出基于SDP松弛的稀疏主成分分析（SPCA）随机近似算法，分析其近似比并通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: SPCA是重要降维技术且为NP难问题，需要有效的近似算法。

Method: 提出基于基本SDP松弛的随机近似算法。

Result: 算法高概率下近似比至多为稀疏常数，在技术假设下平均近似比有界，在协方差模型中可达近最优近似比，通过真实数据集实验验证了算法有效性。

Conclusion: 所提随机近似算法对SPCA问题有效。

Abstract: Sparse Principal Component Analysis (SPCA) is a fundamental technique for
dimensionality reduction, and is NP-hard. In this paper, we introduce a
randomized approximation algorithm for SPCA, which is based on the basic SDP
relaxation. Our algorithm has an approximation ratio of at most the sparsity
constant with high probability, if called enough times. Under a technical
assumption, which is consistently satisfied in our numerical tests, the average
approximation ratio is also bounded by $\mathcal{O}(\log{d})$, where $d$ is the
number of features. We show that this technical assumption is satisfied if the
SDP solution is low-rank, or has exponentially decaying eigenvalues. We then
present a broad class of instances for which this technical assumption holds.
We also demonstrate that in a covariance model, which generalizes the spiked
Wishart model, our proposed algorithm achieves a near-optimal approximation
ratio. We demonstrate the efficacy of our algorithm through numerical results
on real-world datasets.

</details>


### [339] [Uncovering symmetric and asymmetric species associations from community and environmental data](https://arxiv.org/abs/2507.09317)
*Sara Si-Moussi,Esther Galbrun,Mickael Hedde,Giovanni Poggiato,Matthias Rohr,Wilfried Thuiller*

Main category: stat.ML

TL;DR: 提出并验证机器学习框架以从物种群落和环境数据中检索双向关联，表现优于其他模型且适用广泛。


<details>
  <summary>Details</summary>
Motivation: 多数模型假设物种关系对称，而生物相互作用有对称和不对称之分，希望从物种空间关联中观察和检索生物相互作用信号。

Method: 提出框架，将成对物种关联建模为源物种到目标物种的定向影响，在多物种条件生成模型中联合拟合这些关联。

Result: 框架能恢复已知的对称和不对称关联，对比显示其在检索相互作用上能力更优。

Conclusion: 框架直观、模块化，可广泛应用于不同分类群。

Abstract: There is no much doubt that biotic interactions shape community assembly and
ultimately the spatial co-variations between species. There is a hope that the
signal of these biotic interactions can be observed and retrieved by
investigating the spatial associations between species while accounting for the
direct effects of the environment. By definition, biotic interactions can be
both symmetric and asymmetric. Yet, most models that attempt to retrieve
species associations from co-occurrence or co-abundance data internally assume
symmetric relationships between species. Here, we propose and validate a
machine-learning framework able to retrieve bidirectional associations by
analyzing species community and environmental data.
  Our framework (1) models pairwise species associations as directed influences
from a source to a target species, parameterized with two species-specific
latent embeddings: the effect of the source species on the community, and the
response of the target species to the community; and (2) jointly fits these
associations within a multi-species conditional generative model with different
modes of interactions between environmental drivers and biotic associations.
Using both simulated and empirical data, we demonstrate the ability of our
framework to recover known asymmetric and symmetric associations and highlight
the properties of the learned association networks. By comparing our approach
to other existing models such as joint species distribution models and
probabilistic graphical models, we show its superior capacity at retrieving
symmetric and asymmetric interactions. The framework is intuitive, modular and
broadly applicable across various taxonomic groups.

</details>


### [340] [An Algorithm for Identifying Interpretable Subgroups With Elevated Treatment Effects](https://arxiv.org/abs/2507.09494)
*Albert Chiu*

Main category: stat.ML

TL;DR: 本文引入一种算法，在给定个体或条件平均治疗效果（CATE）估计的情况下，识别具有显著治疗效果的可解释子组。


<details>
  <summary>Details</summary>
Motivation: 现有估计CATE的方法常产生高维且难以解释的结果，本文旨在总结和提取拟合模型中的关键信息，以辅助决策、政策实施和科学理解。

Method: 提出一个权衡子组规模和效果大小的目标函数，通过改变控制该权衡的超参数得到帕累托最优规则集的“前沿”，并通过样本分割实现有效推断。

Result: 利用模拟和实证例子展示了方法的实用性和局限性。

Conclusion: 所提算法可在估计CATE时识别可解释子组，但有一定局限性。

Abstract: We introduce an algorithm for identifying interpretable subgroups with
elevated treatment effects, given an estimate of individual or conditional
average treatment effects (CATE). Subgroups are characterized by ``rule sets''
-- easy-to-understand statements of the form (Condition A AND Condition B) OR
(Condition C) -- which can capture high-order interactions while retaining
interpretability. Our method complements existing approaches for estimating the
CATE, which often produce high dimensional and uninterpretable results, by
summarizing and extracting critical information from fitted models to aid
decision making, policy implementation, and scientific understanding. We
propose an objective function that trades-off subgroup size and effect size,
and varying the hyperparameter that controls this trade-off results in a
``frontier'' of Pareto optimal rule sets, none of which dominates the others
across all criteria. Valid inference is achievable through sample splitting. We
demonstrate the utility and limitations of our method using simulated and
empirical examples.

</details>


### [341] [Signed Graph Learning: Algorithms and Theory](https://arxiv.org/abs/2507.09717)
*Abdullah Karaaslanli,Bisakh Banerjee,Tapabrata Maiti,Selin Aviyente*

Main category: stat.ML

TL;DR: 本文提出从一组平滑有符号图信号中学习有符号图的方法，用ADMM求解优化问题并引入快速算法，给出理论证明，在模拟数据和基因调控网络推断问题上评估。


<details>
  <summary>Details</summary>
Motivation: 当前图学习研究主要关注无符号图，而许多生物和社会系统更适合用有符号图描述，因此需要开发学习有符号图的方法。

Method: 采用净拉普拉斯算子作为图移位算子定义平滑有符号图信号，通过求解一个非凸优化问题学习有符号图，使用交替方向乘子法（ADMM）求解，引入快速算法降低复杂度，给出收敛性理论证明和估计误差界。

Result: 在模拟数据和基因调控网络推断问题上对方法进行评估，并与现有有符号图学习方法比较。

Conclusion: 未在摘要中明确提及，但可推测所提方法在有符号图学习上有一定效果和优势。

Abstract: Real-world data is often represented through the relationships between data
samples, forming a graph structure. In many applications, it is necessary to
learn this graph structure from the observed data. Current graph learning
research has primarily focused on unsigned graphs, which consist only of
positive edges. However, many biological and social systems are better
described by signed graphs that account for both positive and negative
interactions, capturing similarity and dissimilarity between samples. In this
paper, we develop a method for learning signed graphs from a set of smooth
signed graph signals. Specifically, we employ the net Laplacian as a graph
shift operator (GSO) to define smooth signed graph signals as the outputs of a
low-pass signed graph filter defined by the net Laplacian. The signed graph is
then learned by formulating a non-convex optimization problem where the total
variation of the observed signals is minimized with respect to the net
Laplacian. The proposed problem is solved using alternating direction method of
multipliers (ADMM) and a fast algorithm reducing the per-ADMM iteration
complexity from quadratic to linear in the number of nodes is introduced.
Furthermore, theoretical proofs of convergence for the algorithm and a bound on
the estimation error of the learned net Laplacian as a function of sample size,
number of nodes, and graph topology are provided. Finally, the proposed method
is evaluated on simulated data and gene regulatory network inference problem
and compared to existing signed graph learning methods.

</details>


### [342] [Discovering Governing Equations in the Presence of Uncertainty](https://arxiv.org/abs/2507.09740)
*Ridwan Olabiyi,Han Hu,Ashif Iquebal*

Main category: stat.ML

TL;DR: 本文提出随机逆物理发现（SIP）框架，处理复杂动力系统方程发现问题，在多个基准测试中表现优于SINDy方法，能提供带量化不确定性的可解释模型。


<details>
  <summary>Details</summary>
Motivation: 现实系统存在输入可变性和测量噪声，传统发现方法依赖固定系数确定性模型，难以处理这类问题，因此需要新方法来考虑系统可变性和测量噪声。

Method: 引入SIP框架，将未知系数视为随机变量，通过最小化后验样本的推前分布与经验数据分布之间的Kullback - Leibler散度来推断其后验分布。

Result: 在四个典型问题的基准测试中，SIP能一致识别正确方程，相对SINDy方法及其贝叶斯变体，平均降低系数均方根误差82%，后验分布的95%可信区间能紧密跟踪观测轨迹。

Conclusion: SIP为噪声、可变和数据有限的环境提供了一种稳健、数据高效的物理发现方法。

Abstract: In the study of complex dynamical systems, understanding and accurately
modeling the underlying physical processes is crucial for predicting system
behavior and designing effective interventions. Yet real-world systems exhibit
pronounced input (or system) variability and are observed through noisy,
limited data conditions that confound traditional discovery methods that assume
fixed-coefficient deterministic models. In this work, we theorize that
accounting for system variability together with measurement noise is the key to
consistently discover the governing equations underlying dynamical systems. As
such, we introduce a stochastic inverse physics-discovery (SIP) framework that
treats the unknown coefficients as random variables and infers their posterior
distribution by minimizing the Kullback-Leibler divergence between the
push-forward of the posterior samples and the empirical data distribution.
Benchmarks on four canonical problems -- the Lotka-Volterra predator-prey
system (multi- and single-trajectory), the historical Hudson Bay lynx-hare
data, the chaotic Lorenz attractor, and fluid infiltration in porous media
using low- and high-viscosity liquids -- show that SIP consistently identifies
the correct equations and lowers coefficient root-mean-square error by an
average of 82\% relative to the Sparse Identification of Nonlinear Dynamics
(SINDy) approach and its Bayesian variant. The resulting posterior
distributions yield 95\% credible intervals that closely track the observed
trajectories, providing interpretable models with quantified uncertainty. SIP
thus provides a robust, data-efficient approach for consistent physics
discovery in noisy, variable, and data-limited settings.

</details>


### [343] [Regret Analysis of Posterior Sampling-Based Expected Improvement for Bayesian Optimization](https://arxiv.org/abs/2507.09828)
*Shion Takeno,Yu Inatsu,Masayuki Karasuyama,Ichiro Takeuchi*

Main category: stat.ML

TL;DR: 本文分析基于后验采样的随机EI，证明其在高斯过程假设下有次线性贝叶斯累积遗憾界，并通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有对期望改进（EI）的理论分析相比其他算法有限，需加强对EI的理论研究。

Method: 分析一种随机化的EI变体，从后验样本路径的最大值评估EI。

Result: 该基于后验采样的随机EI在黑箱函数遵循高斯过程的假设下，实现了次线性贝叶斯累积遗憾界。

Conclusion: 通过数值实验证明了所提出方法的有效性。

Abstract: Bayesian optimization is a powerful tool for optimizing an
expensive-to-evaluate black-box function. In particular, the effectiveness of
expected improvement (EI) has been demonstrated in a wide range of
applications. However, theoretical analyses of EI are limited compared with
other theoretically established algorithms. This paper analyzes a randomized
variant of EI, which evaluates the EI from the maximum of the posterior sample
path. We show that this posterior sampling-based random EI achieves the
sublinear Bayesian cumulative regret bounds under the assumption that the
black-box function follows a Gaussian process. Finally, we demonstrate the
effectiveness of the proposed method through numerical experiments.

</details>


### [344] [Simulating Biases for Interpretable Fairness in Offline and Online Classifiers](https://arxiv.org/abs/2507.10154)
*Ricardo Inácio,Zafeiris Kokkinogenis,Vitor Cerqueira,Carlos Soares*

Main category: stat.ML

TL;DR: 本文开发基于代理的模型生成含特定偏差的合成数据集，评估偏差数据导致的不公平，提出可控偏差注入的数据集生成框架和新的可解释性技术，并在不同学习方式和建模阶段应用缓解措施。


<details>
  <summary>Details</summary>
Motivation: 预测模型常强化训练数据中的偏差，需要缓解方法确保模型结果公平，因此要评估偏差嵌入过程。

Method: 开发基于代理的模型生成含系统偏差的合成数据集，应用分类器预测贷款结果；采用离线和在线学习方法，在建模不同阶段应用缓解措施；使用二阶Shapley值的可解释性技术。

Result: 提出可控偏差注入的合成数据集生成框架和新的可解释性技术。

Conclusion: 通过生成含偏差的合成数据集评估偏差嵌入过程，所提框架和技术有助于缓解模型偏差。

Abstract: Predictive models often reinforce biases which were originally embedded in
their training data, through skewed decisions. In such cases, mitigation
methods are critical to ensure that, regardless of the prevailing disparities,
model outcomes are adjusted to be fair. To assess this, datasets could be
systematically generated with specific biases, to train machine learning
classifiers. Then, predictive outcomes could aid in the understanding of this
bias embedding process. Hence, an agent-based model (ABM), depicting a loan
application process that represents various systemic biases across two
demographic groups, was developed to produce synthetic datasets. Then, by
applying classifiers trained on them to predict loan outcomes, we can assess
how biased data leads to unfairness. This highlights a main contribution of
this work: a framework for synthetic dataset generation with controllable bias
injection. We also contribute with a novel explainability technique, which
shows how mitigations affect the way classifiers leverage data features, via
second-order Shapley values. In experiments, both offline and online learning
approaches are employed. Mitigations are applied at different stages of the
modelling pipeline, such as during pre-processing and in-processing.

</details>


### [345] [Information Must Flow: Recursive Bootstrapping for Information Bottleneck in Optimal Transport](https://arxiv.org/abs/2507.10443)
*Xin Li*

Main category: stat.ML

TL;DR: 提出上下文 - 内容不确定性原理（CCUP）统一框架，解决最优传输中的信息瓶颈问题，证明相关定理，还探讨其在语言等方面应用，确立iBOT在认知和集体智能中的基础地位。


<details>
  <summary>Details</summary>
Motivation: 构建一个统一框架来建模认知过程，解决最优传输中的信息瓶颈问题。

Method: 通过Rao - Blackwellized变分熵最小化实现CCUP，利用局部循环完成进行时间和空间引导，证明Delta收敛定理。

Result: 递归熵最小化在潜在空间产生类似delta的吸引子，时间引导将情景痕迹转化为语义知识，语言可作为符号传输系统。

Conclusion: iBOT是个体认知和集体智能中信息流的基础原理，递归推理是思维适应、对齐和扩展的结构化渠道。

Abstract: We present the Context-Content Uncertainty Principle (CCUP), a unified
framework that models cognition as the directed flow of information between
high-entropy context and low-entropy content. Inference emerges as a cycle of
bidirectional interactions, bottom-up contextual disambiguation paired with
top-down content reconstruction, which resolves the Information Bottleneck in
Optimal Transport (iBOT). Implemented via Rao-Blackwellized variational entropy
minimization, CCUP steers representations toward minimal joint uncertainty
while preserving inferential directionality. Local cycle completion underpins
temporal bootstrapping, chaining simulations to refine memory, and spatial
bootstrapping, enabling compositional hierarchical inference. We prove a Delta
Convergence Theorem showing that recursive entropy minimization yields
delta-like attractors in latent space, stabilizing perceptual schemas and motor
plans. Temporal bootstrapping through perception-action loops and sleep-wake
consolidation further transforms episodic traces into semantic knowledge.
Extending CCUP, each hierarchical level performs delta-seeded inference:
low-entropy content seeds diffuse outward along goal-constrained paths shaped
by top-down priors and external context, confining inference to task-relevant
manifolds and circumventing the curse of dimensionality. Building on this, we
propose that language emerges as a symbolic transport system, externalizing
latent content to synchronize inference cycles across individuals. Together,
these results establish iBOT as a foundational principle of information flow in
both individual cognition and collective intelligence, positioning recursive
inference as the structured conduit through which minds adapt, align, and
extend.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [346] [Sampling-Based Estimation of Jaccard Containment and Similarity](https://arxiv.org/abs/2507.10019)
*Pranav Joshi*

Main category: stat.CO

TL;DR: 本文提出用随机样本估计集合包含和相似度的方法，介绍二项式模型，与先前方法对比，分析估计器性质，扩展到集合相似度估计并给出在大数据系统应用指导。


<details>
  <summary>Details</summary>
Motivation: 解决仅使用随机样本估计两个集合的包含和相似度问题，且不依赖草图或全量数据访问。

Method: 引入二项式模型预测样本重叠情况，并与先前方法对比，分析估计器的统计性质。

Result: 该模型在样本量相对原集合较小时准确实用，在考虑条件下比先前方法估计效果更好。

Conclusion: 该框架可扩展到集合相似度估计，能在仅提供部分或抽样数据的大规模数据系统中应用。

Abstract: This paper addresses the problem of estimating the containment and similarity
between two sets using only random samples from each set, without relying on
sketches or full data access. The study introduces a binomial model for
predicting the overlap between samples, demonstrating that it is both accurate
and practical when sample sizes are small compared to the original sets. The
paper compares this model to previous approaches and shows that it provides
better estimates under the considered conditions. It also analyzes the
statistical properties of the estimator, including error bounds and sample size
requirements needed to achieve a desired level of accuracy and confidence. The
framework is extended to estimate set similarity, and the paper provides
guidance for applying these methods in large scale data systems where only
partial or sampled data is available.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [347] [CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks](https://arxiv.org/abs/2507.10535)
*Hongchao Jiang,Yiming Chen,Yushi Cao,Hung-yi Lee,Robby T. Tan*

Main category: cs.CL

TL;DR: 本文引入CodeJudgeBench评估LLM作为裁判的表现，发现思维模型表现更好但所有模型有随机性，还研究了最优提示策略。


<details>
  <summary>Details</summary>
Motivation: LLM作为裁判在编码场景的有效性因缺乏专用基准而未充分探索，需填补此空白。

Method: 引入CodeJudgeBench，对26个LLM作为裁判的模型进行全面基准测试。

Result: 思维模型表现优于非思维模型，小的思维模型能胜过大型特制模型，但所有模型判断有随机性，呈现顺序影响准确率，评判不同LLM代码和单元测试时性能有差异。使用成对比较和保留完整未处理响应的评论及推理能提升评判性能。

Conclusion: LLM作为裁判在编码场景的可靠性和一致性存在问题，同时给出了较优的提示策略。

Abstract: Large Language Models (LLMs) have significantly advanced the state-of-the-art
in various coding tasks. Beyond directly answering user queries, LLMs can also
serve as judges, assessing and comparing the quality of responses generated by
other models. Such an evaluation capability is crucial both for benchmarking
different LLMs and for improving response quality through response ranking.
However, despite the growing adoption of the LLM-as-a-Judge paradigm, its
effectiveness in coding scenarios remains underexplored due to the absence of
dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a
benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge
models across three critical coding tasks: code generation, code repair, and
unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge
models, we find that recent thinking models significantly outperform
non-thinking models on our carefully designed code judging tasks. Notably, even
relatively small thinking models, such as Qwen3-8B, can outperform specially
trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still
exhibit significant randomness in their judgment of coding tasks. For pairwise
judging tasks, simply changing the order in which responses are presented can
substantially impact accuracy. In addition, when judging code and unit tests
written by different LLMs, LLM-as-a-Judge models also show variance in
performance. This sensitivity raises concerns about the reliability and
consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal
prompting strategies for LLM-as-a-Judge. We find that using pair-wise
comparison outperforms scalar point-wise judging. Furthermore, retaining
comments and reasoning in the full, unprocessed LLM response leads to improved
judge performance.

</details>


### [348] [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
*Wenliang Shan,Michael Fu,Rui Yang,Chakkrit,Tantithamthavorn*

Main category: cs.CL

TL;DR: 本文介绍多语言护栏SEALGuard，构建SEALSBench数据集评估，结果显示其在检测多语言不安全和越狱提示方面优于现有护栏，推进了大语言模型系统的安全对齐。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的护栏方法难以处理多语言不安全输入，存在多语言安全对齐缺口，使系统易受低资源语言的不安全和越狱提示攻击。

Method: 使用低秩自适应（LoRA）将通用多语言模型改编为多语言护栏，构建包含10种语言超260000条提示的SEALSBench数据集进行评估。

Result: 多语言不安全和越狱提示使LlamaGuard性能下降，SEALGuard在检测多语言不安全和越狱提示方面表现更优，DSR比LlamaGuard提高48%，并取得最佳DSR、精度和F1分数。

Conclusion: SEALGuard通过引入有效的多语言护栏推进了大语言模型系统的安全对齐。

Abstract: Safety alignment is critical for LLM-powered systems. While recent
LLM-powered guardrail approaches such as LlamaGuard achieve high detection
accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),
they struggle with multilingual unsafe inputs. This limitation leaves LLM
systems vulnerable to unsafe and jailbreak prompts written in low-resource
languages such as those in Southeast Asia. This paper introduces SEALGuard, a
multilingual guardrail designed to improve the safety alignment across diverse
languages. It aims to address the multilingual safety alignment gap of existing
guardrails and ensure effective filtering of unsafe and jailbreak prompts in
LLM-powered systems. We adapt a general-purpose multilingual language model
into a multilingual guardrail using low-rank adaptation (LoRA). We construct
SEALSBench, a large-scale multilingual safety alignment dataset containing over
260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.
We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on
this benchmark. Our findings show that multilingual unsafe and jailbreak
prompts substantially degrade the performance of the state-of-the-art
LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and
18%, respectively, compared to its performance on English-only prompts. In
contrast, SEALGuard outperforms existing guardrails in detecting multilingual
unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and
achieving the best DSR, precision, and F1-score. Our ablation study further
reveals the contributions of adaptation strategies and model size to the
overall performance of SEALGuard. SEALGuard advances the safety alignment of
LLM systems by introducing an effective multilingual guardrail.

</details>


### [349] [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
*Seokhee Hong,Sunkyoung Kim,Guijin Son,Soyeon Kim,Yeonjung Hong,Jinsik Lee*

Main category: cs.CL

TL;DR: 本文引入两个韩国专家级基准KMMLU - Redux和KMMLU - Pro来评估大语言模型在现实场景的适用性，实验表明它们能全面代表韩国工业知识，并公开数据集。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的发展需要能涵盖学术和工业领域的可靠基准，以有效评估其在现实场景的适用性。

Method: 引入KMMLU - Redux（从现有KMMLU重构，去除韩国国家技术资格考试问题中的关键错误以提高可靠性）和KMMLU - Pro（基于韩国国家专业执照考试以反映韩国专业知识）两个基准。

Result: 实验证明这两个基准能全面代表韩国工业知识。

Conclusion: 两个基准对评估大语言模型在现实场景的适用性有意义，且数据集已公开。

Abstract: The development of Large Language Models (LLMs) requires robust benchmarks
that encompass not only academic domains but also industrial fields to
effectively evaluate their applicability in real-world scenarios. In this
paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,
reconstructed from the existing KMMLU, consists of questions from the Korean
National Technical Qualification exams, with critical errors removed to enhance
reliability. KMMLU-Pro is based on Korean National Professional Licensure exams
to reflect professional knowledge in Korea. Our experiments demonstrate that
these benchmarks comprehensively represent industrial knowledge in Korea. We
release our dataset publicly available.

</details>


### [350] [ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](https://arxiv.org/abs/2507.09037)
*Bharadwaj Ravichandran,David Joy,Paul Elliott,Brian Hu,Jadie Adams,Christopher Funk,Emily Veenhuis,Anthony Hoogs,Arslan Basharat*

Main category: cs.CL

TL;DR: 提出ALIGN系统用于LLM动态个性化决策辅助，有多种特性，进行两领域定量分析且框架开源。


<details>
  <summary>Details</summary>
Motivation: 用户价值观和偏好多样影响决策，现有LLM比较工具多关注基准任务，需新的LLM对齐和个性化方法。

Method: 提出ALIGN系统，通过基于提示的对齐实现LLM决策辅助动态个性化，具备配置管理等特性，有用户界面和模块化后端。

Result: 在民意调查人口统计对齐和医疗分诊决策价值对齐两个领域进行定量分析。

Conclusion: ALIGN框架开源，将推动可靠、负责和个性化的基于LLM的决策辅助研究。

Abstract: Large language models (LLMs) are increasingly being used as decision aids.
However, users have diverse values and preferences that can affect their
decision-making, which requires novel methods for LLM alignment and
personalization. Existing LLM comparison tools largely focus on benchmarking
tasks, such as knowledge-based question answering. In contrast, our proposed
ALIGN system focuses on dynamic personalization of LLM-based decision-makers
through prompt-based alignment to a set of fine-grained attributes. Key
features of our system include robust configuration management, structured
output generation with reasoning, and several algorithm implementations with
swappable LLM backbones, enabling different types of analyses. Our user
interface enables a qualitative, side-by-side comparison of LLMs and their
alignment to various attributes, with a modular backend for easy algorithm
integration. Additionally, we perform a quantitative analysis comparing
alignment approaches in two different domains: demographic alignment for public
opinion surveys and value alignment for medical triage decision-making. The
entire ALIGN framework is open source and will enable new research on reliable,
responsible, and personalized LLM-based decision-makers.

</details>


### [351] [Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation](https://arxiv.org/abs/2507.09076)
*Jialong Mai,Xiaofen Xing,Yawei Li,Zhipeng Li,Jingyuan Xing,Xiangmin Xu*

Main category: cs.CL

TL;DR: 本文提出动态参数记忆（DPM）机制，解决SLLM处理长语音时的局限，在IEMOCAP数据集上取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有SLLM在处理语音情感识别时，高帧率限制处理能力，输入令牌压缩方法忽略多轮对话情感连续性。

Method: 提出具有上下文语义和句子级情感编码的DPM机制，将句子级信息和情感逐步编码到临时LoRA模块。

Result: 在IEMOCAP数据集实验表明，DPM显著提升SLLM处理长音频序列时的情感识别能力。

Conclusion: DPM机制有效解决SLLM处理长语音的局限，达到了当前最优性能。

Abstract: Recent research has focused on applying speech large language model (SLLM) to
improve speech emotion recognition (SER). However, the inherently high frame
rate in speech modality severely limits the signal processing and understanding
capabilities of SLLM. For example, a SLLM with a 4K context window can only
process 80 seconds of audio at 50Hz feature sampling rate before reaching its
capacity limit. Input token compression methods used in SLLM overlook the
continuity and inertia of emotions across multiple conversation turns. This
paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual
semantics and sentence-level emotion encoding, enabling processing of
unlimited-length audio with limited context windows in SLLM. Specifically, DPM
progressively encodes sentence-level information and emotions into a temporary
LoRA module during inference to effectively "memorize" the contextual
information. We trained an emotion SLLM as a backbone and incorporated our DPM
into inference for emotion recognition in conversation (ERC). Experimental
results on the IEMOCAP dataset show that DPM significantly improves the emotion
recognition capabilities of SLLM when processing long audio sequences,
achieving state-of-the-art performance.

</details>


### [352] [CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards](https://arxiv.org/abs/2507.09104)
*Taolin Zhang,Maosong Cao,Alexander Lam,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出通用评判模型CompassJudger - 2和综合基准JudgerBenchV2，前者克服现有评判模型局限，后者规范评判模型评估。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评判模型存在专业化狭窄和鲁棒性有限问题，无法进行全面评估。

Method: 采用任务驱动、多领域数据整理策略，用可验证奖励监督评判任务，通过拒绝采样引导推理，引入带边界策略梯度损失的学习目标。

Result: CompassJudger - 2在多个评判和奖励基准上取得优异结果，7B模型与更大模型表现相当；提出JudgerBenchV2评估跨领域评判准确性和排名一致性。

Conclusion: 这些成果推动了大语言模型评判的鲁棒性和可扩展性，建立了新的性能和评估标准。

Abstract: Recently, the role of LLM-as-judge in evaluating large language models has
gained prominence. However, current judge models suffer from narrow
specialization and limited robustness, undermining their capacity for
comprehensive evaluations. In this work, we present CompassJudger-2, a novel
generalist judge model that overcomes these limitations via a task-driven,
multi-domain data curation strategy. Central to our approach is supervising
judgment tasks with verifiable rewards, guiding intrinsic critical reasoning
through rejection sampling to foster robust, generalizable judgment
capabilities. We introduce a refined learning objective with margin policy
gradient loss to enhance performance. Empirically, CompassJudger-2 achieves
superior results across multiple judge and reward benchmarks, and our 7B model
demonstrates competitive judgment accuracy with significantly larger models
like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a
comprehensive benchmark evaluating cross-domain judgment accuracy and rank
consistency to standardize judge model evaluation. These contributions advance
robust, scalable LLM judgment and establish new performance and evaluation
standards.

</details>


### [353] [OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering](https://arxiv.org/abs/2507.09155)
*Ali Vosoughi,Ayoub Shahnazari,Yufeng Xi,Zeliang Zhang,Griffin Hess,Chenliang Xu,Niaz Abdolrahim*

Main category: cs.CL

TL;DR: 提出OPENXRD用于晶体学问答，结合GPT - 4.5生成内容，实验表明能提升模型准确率，未来有扩展方向。


<details>
  <summary>Details</summary>
Motivation: 避免使用扫描教材带来的版权问题，利用大模型知识填补晶体学知识空白，提升小模型在科学任务中的推理能力。

Method: 设计OPENXRD集成文本提示与GPT - 4.5生成内容，在217个XRD问题上对比不同视觉 - 语言模型在闭卷和开卷条件下的表现。

Result: 使用GPT - 4.5生成摘要的模型准确率显著提升，尤其是晶体学训练有限的模型。

Conclusion: 专门的开卷系统在材料科学中有用，为关键科学领域的NLP工具提供基础。

Abstract: This work presents OPENXRD, an open-book pipeline designed for
crystallography question answering, which integrates textual prompts with
concise supporting content generated by GPT-4.5. Instead of using scanned
textbooks, which may lead to copyright issues, OPENXRD generates compact,
domain-specific references that help smaller models understand key concepts in
X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217
expert-level XRD questions by comparing different vision-language models,
including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,
under both closed-book (without supporting material) and open-book (with
supporting material) conditions. Our experimental results show significant
accuracy improvements in models that use the GPT-4.5-generated summaries,
particularly those with limited prior training in crystallography. OPENXRD uses
knowledge from larger models to fill knowledge gaps in crystallography and
shows that AI-generated texts can help smaller models reason more effectively
in scientific tasks. While the current version of OPENXRD focuses on text-based
inputs, we also explore future extensions such as adding real crystal diagrams
or diffraction patterns to improve interpretation in specialized materials
science contexts. Overall, OPENXRD shows that specialized open-book systems can
be useful in materials science and provides a foundation for broader natural
language processing (NLP) tools in critical scientific fields.

</details>


### [354] [Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models](https://arxiv.org/abs/2507.09470)
*Mingchuan Yang,Ziyuan Huang*

Main category: cs.CL

TL;DR: 研究优化DRAGON Longformer基础模型用于临床文本二分类，优化后性能显著提升，对医疗NLP有实际意义。


<details>
  <summary>Details</summary>
Motivation: 探索DRAGON Longformer基础模型在临床文本分类中的优化，以提高医疗案例描述的二分类性能。

Method: 使用500个临床案例数据集，对预训练模型进行超参数调优、特定领域预处理和架构调整，如增加序列长度、调整学习率等。

Result: 优化后模型准确率从72.0%提升到85.2%，多项指标显著提升，经统计分析改进显著。

Conclusion: 研究成果有助于特定领域语言模型研究，优化模型在医疗场景有广泛应用潜力。

Abstract: This study explores the optimization of the DRAGON Longformer base model for
clinical text classification, specifically targeting the binary classification
of medical case descriptions. A dataset of 500 clinical cases containing
structured medical observations was used, with 400 cases for training and 100
for validation. Enhancements to the pre-trained
joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter
tuning, domain-specific preprocessing, and architectural adjustments. Key
modifications involved increasing sequence length from 512 to 1024 tokens,
adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5
to 8, and incorporating specialized medical terminology. The optimized model
achieved notable performance gains: accuracy improved from 72.0% to 85.2%,
precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from
71.0% to 85.2%. Statistical analysis confirmed the significance of these
improvements (p < .001). The model demonstrated enhanced capability in
interpreting medical terminology, anatomical measurements, and clinical
observations. These findings contribute to domain-specific language model
research and offer practical implications for clinical natural language
processing applications. The optimized model's strong performance across
diverse medical conditions underscores its potential for broad use in
healthcare settings.

</details>


### [355] [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs](https://arxiv.org/abs/2507.09477)
*Yangning Li,Weizhi Zhang,Yuyao Yang,Wei-Chieh Huang,Yaozu Wu,Junyu Luo,Yuanchen Bei,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Chunkit Chan,Yankai Chen,Zhongfen Deng,Yinghui Li,Hai-Tao Zheng,Dongyuan Li,Renhe Jiang,Ming Zhang,Yangqiu Song,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文从统一的推理 - 检索视角对检索增强生成（RAG）和推理方法进行综述，分析二者结合方式、分类方法、数据集和挑战，并指出研究方向。


<details>
  <summary>Details</summary>
Motivation: RAG在多步推理问题上不足，纯推理方法存在幻觉或事实错误问题，需统一视角研究二者结合。

Method: 从推理增强RAG、RAG增强推理和协同RAG - 推理框架三个方面进行分析，对方法、数据集等进行分类。

Result: 提出新兴的协同RAG - 推理框架，可在知识密集型基准测试中取得先进性能。

Conclusion: 指出构建更有效、多模态自适应、可信和以人为本的深度RAG - 推理系统的研究方向。

Abstract: Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language
Models (LLMs) by injecting external knowledge, yet it falls short on problems
that demand multi-step inference; conversely, purely reasoning-oriented
approaches often hallucinate or mis-ground facts. This survey synthesizes both
strands under a unified reasoning-retrieval perspective. We first map how
advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,
we show how retrieved knowledge of different type supply missing premises and
expand context for complex inference (RAG-Enhanced Reasoning). Finally, we
spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs
iteratively interleave search and reasoning to achieve state-of-the-art
performance across knowledge-intensive benchmarks. We categorize methods,
datasets, and open challenges, and outline research avenues toward deeper
RAG-Reasoning systems that are more effective, multimodally-adaptive,
trustworthy, and human-centric. The collection is available at
https://github.com/DavidZWZ/Awesome-RAG-Reasoning.

</details>


### [356] [ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning](https://arxiv.org/abs/2507.09482)
*Changli Wang,Rui Wu,Fang Yin*

Main category: cs.CL

TL;DR: 本文引入多模态讽刺生成数据集M2SaG，提出ViSP框架，评估显示其优于基线模型，生成内容质量更高。


<details>
  <summary>Details</summary>
Motivation: 讽刺生成研究不足，现有研究过度依赖文本模态、忽视视觉线索，且现有数据集图像内容与讽刺意图不匹配。

Method: 引入含4970个样本的M2SaG数据集，提出集成PPO和对比学习的ViSP框架，PPO利用奖励分数引导生成，对比学习促使模型选择奖励分数高的输出。

Result: ViSP在五项指标上超越所有基线模型，生成文本的讽刺得分和事实不一致性均值更高。

Conclusion: ViSP能产生更高质量的讽刺内容，凸显了大语言模型在讽刺生成方面的局限性。

Abstract: Human emotions are complex, with sarcasm being a subtle and distinctive form.
Despite progress in sarcasm research, sarcasm generation remains underexplored,
primarily due to the overreliance on textual modalities and the neglect of
visual cues, as well as the mismatch between image content and sarcastic intent
in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm
generation dataset with 4,970 samples, each containing an image, a sarcastic
text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation
framework that integrates Proximal Policy Optimization (PPO) and contrastive
learning. PPO utilizes reward scores from DIP to steer the generation of
sarcastic texts, while contrastive learning encourages the model to favor
outputs with higher reward scores. These strategies improve overall generation
quality and produce texts with more pronounced sarcastic intent. We evaluate
ViSP across five metric sets and find it surpasses all baselines, including
large language models, underscoring their limitations in sarcasm generation.
Furthermore, we analyze the distributions of Sarcasm Scores and Factual
Incongruity for both M2SaG and the texts generated by ViSP. The generated texts
exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity
(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic
content than the original dataset. % The dataset and code will be publicly
available. Our dataset and code will be released at
\textit{https://github.com/wclapply/ViSP}.

</details>


### [357] [NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance](https://arxiv.org/abs/2507.09601)
*Hanwool Lee,Sara Yu,Yewon Hwang,Jonghyun Choi,Heejae Ahn,Sungbum Jung,Youngjae Yu*

Main category: cs.CL

TL;DR: 针对通用句子嵌入模型难捕捉金融语义问题，提出NMIXX模型并发布KorFinSTS基准，评估显示NMIXX表现佳，强调分词器设计重要性并公开成果。


<details>
  <summary>Details</summary>
Motivation: 通用句子嵌入模型难以捕捉专业金融语义，特别是韩语等低资源语言，存在领域特定行话、语义随时间变化和双语词汇不一致等问题。

Method: 引入NMIXX跨语言嵌入模型，用18.8K高置信三元组微调；发布1921对的KorFinSTS韩语金融STS基准。

Result: NMIXX的multilingual bge - m3变体在英语FinSTS和KorFinSTS上有显著提升，在通用STS性能上有适度权衡；韩语分词覆盖更丰富的模型适应更有效。

Conclusion: 公开模型和基准为金融领域多语言表征学习提供了强大工具。

Abstract: General-purpose sentence embedding models often struggle to capture
specialized financial semantics, especially in low-resource languages like
Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned
bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural
eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual
embedding models fine-tuned with 18.8K high-confidence triplets that pair
in-domain paraphrases, hard negatives derived from a semantic-shift typology,
and exact Korean-English translations. Concurrently, we release KorFinSTS, a
1,921-pair Korean financial STS benchmark spanning news, disclosures, research
reports, and regulations, designed to expose nuances that general benchmarks
miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual
bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and
+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing
other models by the largest margin, while revealing a modest trade-off in
general STS performance. Our analysis further shows that models with richer
Korean token coverage adapt more effectively, underscoring the importance of
tokenizer design in low-resource, cross-lingual settings. By making both models
and the benchmark publicly available, we provide the community with robust
tools for domain-adapted, multilingual representation learning in finance.

</details>


### [358] [Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition](https://arxiv.org/abs/2507.09875)
*Qinyuan Ye,Robin Jia,Xiang Ren*

Main category: cs.CL

TL;DR: 本文通过非标准加法任务，利用电路式可解释性技术分析大语言模型上下文学习的内在机制，有三项关键发现并揭示模型任务级泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型上下文学习任务级泛化的内在机制。

Method: 以非标准加法为任务，利用路径修补等电路式可解释性技术分析模型内部计算。

Result: 发现函数归纳机制，解释模型从标准加法到非标准加法的泛化；+1函数归纳由多个注意力头并行控制；该函数归纳机制可用于更多任务。

Conclusion: 研究结果为语言模型中可复用和可组合结构如何实现任务级泛化提供了更深入的见解。

Abstract: Large language models demonstrate the intriguing ability to perform unseen
tasks via in-context learning. However, it remains unclear what mechanisms
inside the model drive such task-level generalization. In this work, we
approach this question through the lens of off-by-one addition (i.e., 1+1=3,
2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function
as a second step. Leveraging circuit-style interpretability techniques such as
path patching, we analyze the models' internal computations behind their
notable performance and present three key findings. First, we uncover a
function induction mechanism that explains the model's generalization from
standard addition to off-by-one addition. This mechanism resembles the
structure of the induction head mechanism found in prior work and elevates it
to a higher level of abstraction. Second, we show that the induction of the +1
function is governed by multiple attention heads in parallel, each of which
emits a distinct piece of the +1 function. Finally, we find that this function
induction mechanism is reused in a broader range of tasks, including synthetic
tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8
addition. Overall, our findings offer deeper insights into how reusable and
composable structures within language models enable task-level generalization.

</details>


### [359] [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
*Chien Van Nguyen,Ruiyi Zhang,Hanieh Deilamsalehy,Puneet Mathur,Viet Dac Lai,Haoliang Wang,Jayakumar Subramanian,Ryan A. Rossi,Trung Bui,Nikos Vlassis,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: 提出Lizard框架将预训练基于Transformer的大语言模型转化为灵活、亚二次架构用于无限上下文生成，实验表现佳。


<details>
  <summary>Details</summary>
Motivation: Transformer大语言模型因softmax注意力的二次复杂度和不断增长的KV缓存，在上下文长度增加时面临内存和计算瓶颈。

Method: 引入近似softmax注意力的亚二次注意力机制，结合门控模块，采用混合注意力机制，并引入硬件感知算法加速训练。

Result: Lizard在标准语言建模任务中近乎无损恢复教师模型性能，显著超越先前线性化方法，在5-shot MMLU基准上比先前模型提高18分。

Conclusion: Lizard是一种有效的将基于Transformer的大语言模型线性化的框架，能有效解决内存和计算瓶颈问题。

Abstract: We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (LLMs) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based LLMs face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(KV) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.

</details>


### [360] [Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking](https://arxiv.org/abs/2507.09935)
*Hai Toan Nguyen,Tien Dat Nguyen,Viet Ha Nguyen*

Main category: cs.CL

TL;DR: 论文提出新框架增强检索增强生成（RAG）系统，在多数据集评估中效果优于传统分块技术。


<details>
  <summary>Details</summary>
Motivation: 传统分块策略无法创建捕获足够语义的块，不能考虑文本潜在结构。

Method: 提出整合分层文本分割和聚类的框架，推理时利用段级和簇级向量表示检索信息。

Result: 在NarrativeQA、QuALITY和QASPER数据集评估中，该方法比传统分块技术取得更好结果。

Conclusion: 所提出的框架能增强RAG系统，生成更有意义、语义连贯的块，提高检索信息的准确性和相关性。

Abstract: Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies
for retrieval, which enhance large language models (LLMs) by enabling them to
access external knowledge, ensuring that the retrieved information is
up-to-date and domain-specific. However, traditional methods often fail to
create chunks that capture sufficient semantic meaning, as they do not account
for the underlying textual structure. This paper proposes a novel framework
that enhances RAG by integrating hierarchical text segmentation and clustering
to generate more meaningful and semantically coherent chunks. During inference,
the framework retrieves information by leveraging both segment-level and
cluster-level vector representations, thereby increasing the likelihood of
retrieving more precise and contextually relevant information. Evaluations on
the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method
achieved improved results compared to traditional chunking techniques.

</details>


### [361] [Tiny Reward Models](https://arxiv.org/abs/2507.09973)
*Sarah Pan*

Main category: cs.CL

TL;DR: 提出TinyRM小模型在推理和安全偏好建模任务上可与大模型匹敌，强调小模型特定调优策略有效。


<details>
  <summary>Details</summary>
Motivation: 大解码器语言模型用于奖励建模时推理成本高，需更高效模型。

Method: TinyRM结合FLAN式提示、DoRA和层冻结方法。

Result: TinyRM在RewardBench上表现出色，小模型特定调优策略在推理中有效。

Conclusion: 轻量级双向架构在偏好建模中有成为高效可扩展替代方案的潜力，但构建通用模型和对话偏好建模仍有挑战。

Abstract: Large decoder-based language models have become the dominant architecture for
reward modeling in reinforcement learning from human feedback (RLHF). However,
as reward models are increasingly deployed in test-time strategies, their
inference costs become a growing concern. We present TinyRM, a family of small,
bidirectional masked language models (MLMs) with as few as 400 million
parameters, that rival the capabilities of models over 175 times larger on
reasoning and safety preference modeling tasks. TinyRM combines FLAN-style
prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to
achieve strong performance on RewardBench, despite using significantly fewer
resources. Our experiments suggest that small models benefit from
domain-specific tuning strategies, particularly in reasoning, where lightweight
finetuning methods are especially effective. While challenges remain in
building generalist models and conversational preference modeling, our
preliminary results highlight the promise of lightweight bidirectional
architectures as efficient, scalable alternatives for preference modeling.

</details>


### [362] [Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models](https://arxiv.org/abs/2507.09185)
*Ameen Ali,Shahar Katz,Lior Wolf,Ivan Titov*

Main category: cs.CL

TL;DR: 提出一种微调方法，通过识别和修剪与特定数据集机制相关的神经元来增强大语言模型泛化能力，在多项选择基准测试中表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常发展出特定数据集的学习机制，在遇到新任务或分布时会降低性能，需要提升泛化能力。

Method: 采用综合梯度量化每个神经元对高置信度预测的影响，找出对特定数据集性能贡献大但不支持可迁移推理的神经元并选择性修剪。

Result: 基于修剪的微调在多项选择基准测试中显著提高了性能，超过了先前的（非修剪）适应方法。

Conclusion: 通过识别和修剪与特定数据集机制相关的神经元可以有效增强基于Transformer的大语言模型的泛化能力。

Abstract: Large language models (LLMs) often develop learned mechanisms specialized to
specific datasets, such as reliance on domain-specific correlations, which
yield high-confidence predictions without generalizable reasoning. While
beneficial in one setting, these dataset-specific mechanisms typically degrade
performance when models encounter novel tasks or distributions. In this work,
we introduce a fine-tuning approach designed to enhance generalization by
identifying and pruning neurons associated with dataset-specific mechanisms in
transformer-based LLMs. Our method employs Integrated Gradients to quantify
each neuron's influence on high-confidence predictions, pinpointing those that
disproportionately contribute to dataset-specific performance without
supporting robust, transferable reasoning. Selectively pruning these neurons
compels the model to depend on generalizable representations. Evaluated across
multiple-choice benchmarks, our pruning-based fine-tuning significantly
enhances performance, surpassing prior (non-pruning) adaptation methods.

</details>


### [363] [Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires](https://arxiv.org/abs/2507.10073)
*Simon Münker*

Main category: cs.CL

TL;DR: 研究发现大语言模型无法代表多元文化道德框架，挑战了其在社科研究中的应用，呼吁更合理的对齐目标和评估指标。


<details>
  <summary>Details</summary>
Motivation: 探究AI系统是否真正代表人类价值观，而非简单平均。

Method: 在19种文化背景下应用道德基础问卷，对比多个最先进大语言模型与人类基线数据。

Result: 大语言模型系统性地使道德多样性同质化，模型规模增加不一定能提升文化表征保真度。

Conclusion: 当前AI对齐方法存在根本局限，需要基于数据的对齐目标和评估指标来确保AI代表多元人类价值观。

Abstract: Are AI systems truly representing human values, or merely averaging across
them? Our study suggests a concerning reality: Large Language Models (LLMs)
fail to represent diverse cultural moral frameworks despite their linguistic
capabilities. We expose significant gaps between AI-generated and human moral
intuitions by applying the Moral Foundations Questionnaire across 19 cultural
contexts. Comparing multiple state-of-the-art LLMs' origins against human
baseline data, we find these models systematically homogenize moral diversity.
Surprisingly, increased model size doesn't consistently improve cultural
representation fidelity. Our findings challenge the growing use of LLMs as
synthetic populations in social science research and highlight a fundamental
limitation in current AI alignment approaches. Without data-driven alignment
beyond prompting, these systems cannot capture the nuanced, culturally-specific
moral intuitions. Our results call for more grounded alignment objectives and
evaluation metrics to ensure AI systems represent diverse human values rather
than flattening the moral landscape.

</details>


### [364] [Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning](https://arxiv.org/abs/2507.10085)
*Chenxi Huang,Shaotian Yan,Liang Xie,Binbin Lin,Sinan Fan,Yue Xin,Deng Cai,Chen Shen,Jieping Ye*

Main category: cs.CL

TL;DR: 提出CRFT方法用于复杂推理任务，在多个基准测试验证效果，还适应少样本设置。


<details>
  <summary>Details</summary>
Motivation: 直接用原生ReFT方法在复杂推理任务中表现不佳，需改进以提升推理性能。

Method: 提出CRFT方法，通过信息流分析识别和优化关键表征，在监督学习框架下动态优化低秩线性子空间中的关键表征，冻结基础模型。

Result: 在八个算术和常识推理基准测试验证有效性和效率，适应少样本设置，提升单样本准确率16.4%。

Conclusion: 凸显表征级优化对思维链推理的潜力，是传统PEFT方法的轻量级强大替代方案。

Abstract: Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient
Fine-Tuning (PEFT) method, has attracted widespread attention for significantly
improving parameter efficiency by editing representation space alone. In this
work, we investigate applying ReFT to complex reasoning tasks. However,
directly using the native ReFT method, which modifies fixed representations at
the beginning and end of each layer, yields suboptimal performance, as these
fixed-position representations have uncertain impact on the outputs. We observe
that, in complex reasoning tasks, there often exist certain critical
representations. These representations either integrate significant information
from preceding layers or regulate subsequent layer representations. Through
layer-by-layer propagation, they exert a substantial influence on the final
output. Naturally, fine-tuning these critical representations has the potential
to greatly enhance reasoning performance. Building upon these insights, we
propose Critical Representation Fine-Tuning (CRFT), a novel method that
identifies and optimizes these critical representations through information
flow analysis. CRFT operates within a supervised learning framework,
dynamically optimizing critical representations in a low-rank linear subspace
while freezing the base model. The effectiveness and efficiency of our method
are validated across eight benchmarks for arithmetic and commonsense reasoning,
using LLaMA and Mistral model families. Furthermore, our method also adapts
effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work
highlights the untapped potential of representation-level optimization for CoT
reasoning, offering a lightweight yet powerful alternative to traditional PEFT
methods.

</details>


### [365] [ClaritySpeech: Dementia Obfuscation in Speech](https://arxiv.org/abs/2507.09282)
*Dominika Woszczyk,Ranya Aloufi,Soteris Demetriou*

Main category: cs.CL

TL;DR: 本文提出痴呆语音混淆框架ClaritySpeech，可在低数据环境下修正受痴呆影响的语音，实验显示能提升隐私和可访问性。


<details>
  <summary>Details</summary>
Motivation: 痴呆会改变语音模式，现有语音技术难以处理痴呆和非典型语音，存在沟通障碍和隐私问题。

Method: 提出名为ClaritySpeech的痴呆语音混淆框架，集成自动语音转录、文本混淆和零样本文本转语音技术。

Result: 在不同对抗设置和模态下，ADReSS和ADReSSo的平均F1分数分别下降16%和10%，保持50%说话人相似度；提升WER和语音质量。

Conclusion: 该系统能提升隐私和可访问性。

Abstract: Dementia, a neurodegenerative disease, alters speech patterns, creating
communication barriers and raising privacy concerns. Current speech
technologies, such as automatic speech transcription (ASR), struggle with
dementia and atypical speech, further challenging accessibility. This paper
presents a novel dementia obfuscation in speech framework, ClaritySpeech,
integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to
correct dementia-affected speech while preserving speaker identity in low-data
environments without fine-tuning. Results show a 16% and 10% drop in mean F1
score across various adversarial settings and modalities (audio, text, fusion)
for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We
also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15
for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and
accessibility.

</details>


### [366] [Abusive text transformation using LLMs](https://arxiv.org/abs/2507.10177)
*Rohitash Chandra,Jiyong Choi*

Main category: cs.CL

TL;DR: 研究用大语言模型将含仇恨言论和脏话的辱骂性文本转化为非辱骂性文本，评估多个模型，发现Groq结果差异大，GPT - 4o和DeepSeek - V3有相似性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在辱骂性文本分类和转化方面有待探索，研究旨在用其将辱骂性文本转化为非辱骂性且保留原意的文本。

Method: 评估Gemini、GPT - 4o、DeekSeek和Groq识别辱骂性文本的能力，用其进行转化，对原始和转化后数据集进行情感和语义分析。

Result: Groq与其他LLMs结果差异大，GPT - 4o和DeepSeek - V3有相似性。

Conclusion: 未明确提及，但暗示不同大语言模型在辱骂性文本转化任务中的表现有差异。

Abstract: Although Large Language Models (LLMs) have demonstrated significant
advancements in natural language processing tasks, their effectiveness in the
classification and transformation of abusive text into non-abusive versions
remains an area for exploration. In this study, we aim to use LLMs to transform
abusive text (tweets and reviews) featuring hate speech and swear words into
non-abusive text, while retaining the intent of the text. We evaluate the
performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and
Groq, on their ability to identify abusive text. We them to transform and
obtain a text that is clean from abusive and inappropriate content but
maintains a similar level of sentiment and semantics, i.e. the transformed text
needs to maintain its message. Afterwards, we evaluate the raw and transformed
datasets with sentiment analysis and semantic analysis. Our results show Groq
provides vastly different results when compared with other LLMs. We have
identified similarities between GPT-4o and DeepSeek-V3.

</details>


### [367] [Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects](https://arxiv.org/abs/2507.10216)
*Renad Al-Monef,Hassan Alhuzali,Nora Alturayeif,Ashwag Alasmari*

Main category: cs.CL

TL;DR: 本文介绍用于评估大语言模型在沙特主要方言上表现的基准Absher，评估了多个模型，发现性能差距，强调方言感知训练和文化对齐评估方法的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在阿拉伯语NLP应用中愈发重要，需要评估其对沙特地区方言和文化细微差别的理解。

Method: 引入包含超18000道选择题的Absher基准，涵盖六个类别，源自沙特各地的方言数据，评估多个先进大语言模型。

Result: 评估结果显示在需要文化推理或上下文理解的任务上存在显著性能差距。

Conclusion: 强调需要方言感知训练和文化对齐评估方法来提升大语言模型在现实阿拉伯语应用中的表现。

Abstract: As large language models (LLMs) become increasingly central to Arabic NLP
applications, evaluating their understanding of regional dialects and cultural
nuances is essential, particularly in linguistically diverse settings like
Saudi Arabia. This paper introduces \texttt{Absher}, a comprehensive benchmark
specifically designed to assess LLMs performance across major Saudi dialects.
\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six
distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,
Cultural Interpretation, and Location Recognition. These questions are derived
from a curated dataset of dialectal words, phrases, and proverbs sourced from
various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,
including multilingual and Arabic-specific models. We also provide detailed
insights into their capabilities and limitations. Our results reveal notable
performance gaps, particularly in tasks requiring cultural inference or
contextual understanding. Our findings highlight the urgent need for
dialect-aware training and culturally aligned evaluation methodologies to
improve LLMs performance in real-world Arabic applications.

</details>


### [368] [Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces](https://arxiv.org/abs/2507.09709)
*Baturay Saglam,Paul Kassianik,Blaine Nelson,Sajana Weerawardhena,Yaron Singer,Amin Karbasi*

Main category: cs.CL

TL;DR: 对基于Transformer的大语言模型隐藏状态进行大规模实证研究，发现高层语义信息位于低维子空间且具有线性可分性，支持开发几何感知工具。


<details>
  <summary>Details</summary>
Motivation: 目前不清楚大语言模型在多大程度上内部组织与语义理解相关的表征，旨在研究此问题。

Method: 对11个仅解码器模型在6个科学主题和12层上的隐藏状态进行大规模实证分析。

Result: 高层语义信息位于低维子空间，在不同领域形成线性可分表征，深层和特定提示下可分性更明显，可在隐藏空间进行有效因果干预。

Conclusion: 研究结果支持开发几何感知工具，以检测和缓解有害或对抗性内容，如训练简单MLP分类器作为轻量级潜在空间护栏。

Abstract: Understanding the latent space geometry of large language models (LLMs) is
key to interpreting their behavior and improving alignment. \baturay{However,
it remains unclear to what extent LLMs internally organize representations
related to semantic understanding. To investigate this, we conduct a
large-scale empirical study of hidden states in transformer-based LLMs,
analyzing 11 decoder-only models across 6 scientific topics and 12 layers each.
We find that high-level semantic information consistently lies in
low-dimensional subspaces that form linearly separable representations across
distinct domains. This separability becomes more pronounced in deeper layers
and under prompts that trigger structured reasoning or alignment
behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry
enables simple yet effective causal interventions in hidden space; for example,
reasoning patterns like chain-of-thought can be captured by a single vector
direction. Together, these findings support the development of geometry-aware
tools that operate directly on latent representations to detect and mitigate
harmful or adversarial content, using methods such as transport-based defenses
that leverage this separability. As a proof of concept, we demonstrate this
potential by training a simple MLP classifier as a lightweight latent-space
guardrail, which detects adversarial and malicious prompts with high precision.

</details>


### [369] [From Sequence to Structure: Uncovering Substructure Reasoning in Transformers](https://arxiv.org/abs/2507.10435)
*Xinnan Dai,Kai Yang,Jay Revolinsky,Kai Guo,Aoran Wang,Bohang Zhang,Jiliang Tang*

Main category: cs.CL

TL;DR: 研究解码器 - 仅Transformer架构理解图结构的机制，提出ISF视角，验证其在大语言模型中的过程，探索处理不同图类型的能力。


<details>
  <summary>Details</summary>
Motivation: 探究解码器 - 仅Transformer架构如何理解潜在图结构。

Method: 从子结构提取任务入手，通过实证结果和理论分析提出ISF视角，验证其在大语言模型中的过程，探索处理不同图类型的能力。

Result: 揭示多层Transformer中一致的内部动态，证明解码器 - 仅Transformer可从属性图中成功提取子结构。

Conclusion: 为基于序列的Transformer在图数据上执行子结构提取任务提供新见解。

Abstract: Recent studies suggest that large language models (LLMs) possess the
capability to solve graph reasoning tasks. Notably, even when graph structures
are embedded within textual descriptions, LLMs can still effectively answer
related questions. This raises a fundamental question: How can a decoder-only
Transformer architecture understand underlying graph structures? To address
this, we start with the substructure extraction task, interpreting the inner
mechanisms inside the transformers and analyzing the impact of the input
queries. Specifically, through both empirical results and theoretical analysis,
we present Induced Substructure Filtration (ISF), a perspective that captures
the substructure identification in the multi-layer transformers. We further
validate the ISF process in LLMs, revealing consistent internal dynamics across
layers. Building on these insights, we explore the broader capabilities of
Transformers in handling diverse graph types. Specifically, we introduce the
concept of thinking in substructures to efficiently extract complex composite
patterns, and demonstrate that decoder-only Transformers can successfully
extract substructures from attributed graphs, such as molecular graphs.
Together, our findings offer a new insight on how sequence-based Transformers
perform the substructure extraction task over graph data.

</details>


### [370] [Referential ambiguity and clarification requests: comparing human and LLM behaviour](https://arxiv.org/abs/2507.10445)
*Chris Madge,Matthew Purver,Massimo Poesio*

Main category: cs.CL

TL;DR: 研究大语言模型（LLMs）在任务导向对话中提出澄清问题的能力，构建新语料库对比人类和LLMs行为，发现两者差异，测试推理对提问的影响。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在异步指令格式的任务导向对话中提出澄清问题的能力。

Method: 将Minecraft对话语料库的两种注释整合为新语料库，对比人类和LLMs在歧义情况下的行为，用不同推理方法测试推理对提问的影响。

Result: 对话中歧义与人类提出澄清问题关联弱，人类和LLMs相关性低；人类因任务不确定性提问多，LLMs因指称歧义提问多；推理能增加提问频率和相关性。

Conclusion: 推理有助于提高LLMs提出澄清问题的频率和相关性。

Abstract: In this work we examine LLMs' ability to ask clarification questions in
task-oriented dialogues that follow the asynchronous
instruction-giver/instruction-follower format. We present a new corpus that
combines two existing annotations of the Minecraft Dialogue Corpus -- one for
reference and ambiguity in reference, and one for SDRT including clarifications
-- into a single common format providing the necessary information to
experiment with clarifications and their relation to ambiguity. With this
corpus we compare LLM actions with original human-generated clarification
questions, examining how both humans and LLMs act in the case of ambiguity. We
find that there is only a weak link between ambiguity and humans producing
clarification questions in these dialogues, and low correlation between humans
and LLMs. Humans hardly ever produce clarification questions for referential
ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce
more clarification questions for referential ambiguity, but less so for task
uncertainty. We question if LLMs' ability to ask clarification questions is
predicated on their recent ability to simulate reasoning, and test this with
different reasoning approaches, finding that reasoning does appear to increase
question frequency and relevancy.

</details>


### [371] [Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding](https://arxiv.org/abs/2507.09758)
*Qi Feng,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: 提出自适应课程学习范式，基于预训练语言模型预测难度得分对微调示例排序，在多个数据集实验显示优于随机采样。


<details>
  <summary>Details</summary>
Motivation: 现有课程学习方法依赖手动定义难度指标，不能准确反映模型自身视角，需改进。

Method: 提出自适应课程学习范式，根据预训练语言模型预测的难度得分对微调示例排序，探索多种训练策略。

Result: 在四个自然语言理解数据集上实验，相比标准随机采样，该方法收敛更快、性能更好。

Conclusion: 所提出的自适应课程学习范式有效，能提升学习效率和性能。

Abstract: Curriculum learning is a widely adopted training strategy in natural language
processing (NLP), where models are exposed to examples organized by increasing
difficulty to enhance learning efficiency and performance. However, most
existing approaches rely on manually defined difficulty metrics -- such as text
length -- which may not accurately reflect the model's own perspective. To
overcome this limitation, we present a self-adaptive curriculum learning
paradigm that prioritizes fine-tuning examples based on difficulty scores
predicted by pre-trained language models (PLMs) themselves. Building on these
scores, we explore various training strategies that differ in the ordering of
examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed
sampling. We evaluate our method on four natural language understanding (NLU)
datasets covering both binary and multi-class classification tasks.
Experimental results show that our approach leads to faster convergence and
improved performance compared to standard random sampling.

</details>


### [372] [Can You Detect the Difference?](https://arxiv.org/abs/2507.10475)
*İsmail Tarım,Aytuğ Onan*

Main category: cs.CL

TL;DR: 对扩散生成文本（LLaDA）和自回归生成文本（LLaMA）进行系统比较，指出依赖单一指标难以区分扩散输出与人类写作，强调需扩散感知检测器。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展引发检测AI生成文本可靠性的担忧，且文体计量指标在扩散模型上的有效性未知。

Method: 使用2000个样本对LLaDA和LLaMA进行系统比较，分析困惑度、突发性、词汇多样性等指标。

Result: LLaDA在困惑度和突发性上接近人类文本，面向自回归的检测器假阴性率高；LLaMA困惑度低但词汇保真度低；单一指标无法区分扩散输出与人类写作。

Conclusion: 强调需要扩散感知检测器，并给出混合模型、特定文体特征和鲁棒水印等方向。

Abstract: The rapid advancement of large language models (LLMs) has raised concerns
about reliably detecting AI-generated text. Stylometric metrics work well on
autoregressive (AR) outputs, but their effectiveness on diffusion-based models
is unknown. We present the first systematic comparison of diffusion-generated
text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,
burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that
LLaDA closely mimics human text in perplexity and burstiness, yielding high
false-negative rates for AR-oriented detectors. LLaMA shows much lower
perplexity but reduced lexical fidelity. Relying on any single metric fails to
separate diffusion outputs from human writing. We highlight the need for
diffusion-aware detectors and outline directions such as hybrid models,
diffusion-specific stylometric signatures, and robust watermarking.

</details>


### [373] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: 提出基于GBM的正则化技术提升NLP模型对抗攻击鲁棒性，实验显示比基线提升最多8.8%。


<details>
  <summary>Details</summary>
Motivation: 现有NLP模型易受攻击，循环网络和SSM等架构鲁棒性研究不足，需提升模型鲁棒性。

Method: 引入基于GBM的正则化技术，计算LSTM、S4和CNN的GBM，增强抗攻击能力、泛化能力并分析S4鲁棒性。

Result: 在多架构和数据集上实验，方法比现有基线提升对抗鲁棒性最多8.8%，优于多个先进防御方法。

Conclusion: 基于GBM的方法有效提升NLP模型对抗攻击鲁棒性，代码开源。

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [374] [From BERT to Qwen: Hate Detection across architectures](https://arxiv.org/abs/2507.10468)
*Ariadna Mon,Saúl Fenollosa,Jon Lecumberri*

Main category: cs.CL

TL;DR: 研究对比经典编码器和下一代大语言模型在检测网络仇恨言论上的表现。


<details>
  <summary>Details</summary>
Motivation: 在线平台难以在不过度审查合法言论的情况下遏制仇恨言论，超大型自回归大语言模型虽有潜力，但其实用性未经验证。

Method: 在整理的在线交互语料库上对经典编码器和下一代大语言模型进行基准测试。

Result: 原文未提及。

Conclusion: 原文未提及。

Abstract: Online platforms struggle to curb hate speech without over-censoring
legitimate discourse. Early bidirectional transformer encoders made big
strides, but the arrival of ultra-large autoregressive LLMs promises deeper
context-awareness. Whether this extra scale actually improves practical
hate-speech detection on real-world text remains unverified. Our study puts
this question to the test by benchmarking both model families, classic encoders
and next-generation LLMs, on curated corpora of online interactions for
hate-speech detection (Hate or No Hate).

</details>


### [375] [Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation](https://arxiv.org/abs/2507.10524)
*Sangmin Bae,Yujin Kim,Reza Bayat,Sungnyun Kim,Jiyoun Ha,Tal Schuster,Adam Fisch,Hrayr Harutyunyan,Ziwei Ji,Aaron Courville,Se-Young Yun*

Main category: cs.CL

TL;DR: 提出Mixture-of-Recursions (MoR)框架结合参数共享和自适应计算提升效率，在多方面优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型效率优化方法多只针对参数共享或自适应计算其一，需同时实现两者。

Method: 引入MoR框架，在递归Transformer中结合两方面效率提升，包括共享层栈实现参数效率、轻量级路由器实现自适应计算，还提出KV共享变体。

Result: 在135M到1.7B参数模型规模上，MoR在同等训练FLOPs和更小模型尺寸下，降低验证困惑度、提高少样本准确率和吞吐量。

Conclusion: MoR是在不产生大模型成本的情况下实现大模型质量的有效途径。

Abstract: Scaling language models unlocks impressive capabilities, but the accompanying
computational and memory demands make both training and deployment expensive.
Existing efficiency efforts typically target either parameter sharing or
adaptive computation, leaving open the question of how to attain both
simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework
that combines the two axes of efficiency inside a single Recursive Transformer.
MoR reuses a shared stack of layers across recursion steps to achieve parameter
efficiency, while lightweight routers enable adaptive token-level thinking by
dynamically assigning different recursion depths to individual tokens. This
allows MoR to focus quadratic attention computation only among tokens still
active at a given recursion depth, further improving memory access efficiency
by selectively caching only their key-value pairs. Beyond these core
mechanisms, we also propose a KV sharing variant that reuses KV pairs from the
first recursion, specifically designed to decrease prefill latency and memory
footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms
a new Pareto frontier: at equal training FLOPs and smaller model sizes, it
significantly lowers validation perplexity and improves few-shot accuracy,
while delivering higher throughput compared with vanilla and existing recursive
baselines. These gains demonstrate that MoR is an effective path towards
large-model quality without incurring large-model cost.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [376] [Green-LLM: Optimal Workload Allocation for Environmentally-Aware Distributed Inference](https://arxiv.org/abs/2507.09942)
*Jiaming Cheng,Duong Tung Nguyen*

Main category: cs.NI

TL;DR: 研究大语言模型推理工作负载在异构边缘数据中心的最优分配，提出优化模型并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决如何在异构边缘数据中心最优分配大语言模型推理工作负载，以降低能耗、碳排放和水消耗，同时提升用户体验，减少运营成本和环境影响。

Method: 提出一种新颖的优化模型。

Result: 数值结果验证了所提方法的有效性。

Conclusion: 所提出的优化模型可帮助大语言模型服务提供商减少运营成本和环境影响。

Abstract: This letter investigates the optimal allocation of large language model (LLM)
inference workloads across heterogeneous edge data centers (DCs) over time.
Each DC features on-site renewable generation and faces dynamic electricity
prices and spatiotemporal variability in renewable availability. The central
question is: how can inference workloads be optimally distributed to the DCs to
minimize energy consumption, carbon emissions, and water usage while enhancing
user experience? This letter proposes a novel optimization model for LLM
service providers to reduce operational costs and environmental impacts.
Numerical results validate the efficacy of the proposed approach.

</details>


### [377] [Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI](https://arxiv.org/abs/2507.10510)
*Jiangkai Wu,Zhiyuan Ren,Liming Liu,Xinggong Zhang*

Main category: cs.NI

TL;DR: 文章介绍AI视频聊天新范式及延迟挑战，提出Artic框架，包括上下文感知视频流、抗丢包自适应帧率等方法，并构建DeViBench基准，最后讨论开放性问题。


<details>
  <summary>Details</summary>
Motivation: 解决AI视频聊天中因MLLM推理和网络问题导致的延迟瓶颈，让AI交互更像真人。

Method: 提出Artic框架，采用上下文感知视频流、抗丢包自适应帧率，构建DeViBench基准。

Result: 未明确提及具体实验结果，但提出了一系列解决方法和基准。

Conclusion: 提出解决AI视频聊天延迟问题的方法，并讨论了相关开放性问题和解决方案。

Abstract: AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),
where one peer is not a human, but a Multimodal Large Language Model (MLLM).
This makes interaction between humans and AI more intuitive, as if chatting
face-to-face with a real person. However, this poses significant challenges to
latency, because the MLLM inference takes up most of the response time, leaving
very little time for video streaming. Due to network uncertainty and
instability, transmission latency becomes a critical bottleneck preventing AI
from being like a real person. To address this, we propose Artic, an
AI-oriented Real-time Communication framework, exploring the network
requirement shift from "humans watching video" to "AI understanding video". To
reduce bitrate dramatically while maintaining MLLM accuracy, we propose
Context-Aware Video Streaming that recognizes the importance of each video
region for chat and allocates bitrate almost exclusively to chat-important
regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive
Frame Rate that leverages previous frames to substitute for lost/delayed frames
while avoiding bitrate waste. To evaluate the impact of video streaming quality
on MLLM accuracy, we build the first benchmark, named Degraded Video
Understanding Benchmark (DeViBench). Finally, we discuss some open questions
and ongoing solutions for AI Video Chat.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [378] [Evaluating Fake Music Detection Performance Under Audio Augmentations](https://arxiv.org/abs/2507.10447)
*Tomasz Sroka,Tomasz Wężowicz,Dominik Sidorczuk,Mateusz Modrzejewski*

Main category: cs.SD

TL;DR: 本文探索音乐假造检测系统在音频增强下的鲁棒性，发现模型性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 随着生成式音频模型发展，区分人类创作和生成音乐变难，需要研究检测系统在音频增强下的鲁棒性。

Method: 构建包含真实和合成音乐的数据集，应用一系列音频变换并分析其对分类准确率的影响，测试先进音乐深度假造检测模型。

Result: 即使引入轻度增强，模型性能也显著下降。

Conclusion: 音乐假造检测系统在音频增强下鲁棒性较差。

Abstract: With the rapid advancement of generative audio models, distinguishing between
human-composed and generated music is becoming increasingly challenging. As a
response, models for detecting fake music have been proposed. In this work, we
explore the robustness of such systems under audio augmentations. To evaluate
model generalization, we constructed a dataset consisting of both real and
synthetic music generated using several systems. We then apply a range of audio
transformations and analyze how they affect classification accuracy. We test
the performance of a recent state-of-the-art musical deepfake detection model
in the presence of audio augmentations. The performance of the model decreases
significantly even with the introduction of light augmentations.

</details>


### [379] [MB-RIRs: a Synthetic Room Impulse Response Dataset with Frequency-Dependent Absorption Coefficients](https://arxiv.org/abs/2507.09750)
*Enric Gusó,Joanna Luberadzka,Umut Sayin,Xavier Serra*

Main category: cs.SD

TL;DR: 研究四种提升合成房间脉冲响应（RIR）数据集生态有效性的策略，发现使用频率相关吸声系数的MB - RIRs效果好且数据集免费公开。


<details>
  <summary>Details</summary>
Motivation: 提升合成RIR数据集用于单声道语音增强的生态有效性。

Method: 在传统基于图像源方法的鞋盒RIR上实现三个特征，考虑SoundSpaces数据集的基于网格的RIR，为每个RIR数据集训练DeepFilternet3模型并在真实RIR测试集上主客观评估。

Result: 使用频率相关吸声系数的MB - RIRs在真实RIR上评估时，SDR提升+0.51dB，MUSHRA得分提升+8.9。

Conclusion: MB - RIRs在提升生态有效性上效果较好，且数据集免费公开可下载。

Abstract: We investigate the effects of four strategies for improving the ecological
validity of synthetic room impulse response (RIR) datasets for monoaural Speech
Enhancement (SE). We implement three features on top of the traditional image
source method-based (ISM) shoebox RIRs: multiband absorption coefficients,
source directivity and receiver directivity. We additionally consider
mesh-based RIRs from the SoundSpaces dataset. We then train a DeepFilternet3
model for each RIR dataset and evaluate the performance on a test set of real
RIRs both objectively and subjectively. We find that RIRs which use
frequency-dependent acoustic absorption coefficients (MB-RIRs) can obtain
+0.51dB of SDR and a +8.9 MUSHRA score when evaluated on real RIRs. The MB-RIRs
dataset is publicly available for free download.

</details>


### [380] [AudioMAE++: learning better masked audio representations with SwiGLU FFNs](https://arxiv.org/abs/2507.10464)
*Sarthak Yadav,Sergios Theodoridis,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: 本文提出改进的音频掩码自编码器AudioMAE++，在下游任务中表现优于现有基于MAE的方法，且有良好的扩展特性。


<details>
  <summary>Details</summary>
Motivation: 现有基于音频数据训练MAE的方法大多使用普通transformer构建块，而transformer社区已有新架构进展，需改进。

Method: 提出AudioMAE++，采用macaron风格transformer块和门控线性单元。

Result: 在AudioSet数据集上预训练后，AudioMAE++在10个不同下游任务中优于现有基于MAE的方法，在音频分类和基于语音的基准测试中表现出色，参数增加4倍时仍优于标准MAE基线。

Conclusion: AudioMAE++是一种有效的音频掩码自编码器，具有良好性能和扩展特性。

Abstract: Masked Autoencoders (MAEs) trained on audio spectrogram patches have emerged
as a prominent approach for learning self-supervised audio representations.
While several recent papers have evaluated key aspects of training MAEs on
audio data, the majority of these approaches still leverage vanilla transformer
building blocks, whereas the transformer community has seen steady integration
of newer architectural advancements. In this work, we propose AudioMAE++, a
revamped audio masked autoencoder with two such enhancements, namely
macaron-style transformer blocks with gated linear units. When pretrained on
the AudioSet dataset, the proposed AudioMAE++ models outperform existing MAE
based approaches on 10 diverse downstream tasks, demonstrating excellent
performance on audio classification and speech-based benchmarks. The proposed
AudioMAE++ models also demonstrate excellent scaling characteristics,
outperforming directly comparable standard MAE baselines with up to 4x more
parameters.

</details>


### [381] [WildFX: A DAW-Powered Pipeline for In-the-Wild Audio FX Graph Modeling](https://arxiv.org/abs/2507.10534)
*Qihui Yang,Taylor Berg-Kirkpatrick,Julian McAuley,Zachary Novack*

Main category: cs.SD

TL;DR: 介绍WildFX，一种用Docker容器化的管道，用于生成具有丰富效果图的多轨音频混合数据集，实验验证其有效性且代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前端到端AI音乐生成虽有进展，但AI驱动的专业数字信号处理（DSP）工作流建模仍具挑战，现有方法存在不足。

Method: 引入WildFX管道，以专业数字音频工作站（DAW）为后端，支持集成多种格式插件，有简约元数据接口。

Result: 通过盲估计混音图、插件/增益参数等实验，证明了管道的有效性。

Conclusion: WildFX能弥合AI研究与实际DSP需求之间的差距。

Abstract: Despite rapid progress in end-to-end AI music generation, AI-driven modeling
of professional Digital Signal Processing (DSP) workflows remains challenging.
In particular, while there is growing interest in neural black-box modeling of
audio effect graphs (e.g. reverb, compression, equalization), AI-based
approaches struggle to replicate the nuanced signal flow and parameter
interactions used in professional workflows. Existing differentiable plugin
approaches often diverge from real-world tools, exhibiting inferior performance
relative to simplified neural controllers under equivalent computational
constraints. We introduce WildFX, a pipeline containerized with Docker for
generating multi-track audio mixing datasets with rich effect graphs, powered
by a professional Digital Audio Workstation (DAW) backend. WildFX supports
seamless integration of cross-platform commercial plugins or any plugins in the
wild, in VST/VST3/LV2/CLAP formats, enabling structural complexity (e.g.,
sidechains, crossovers) and achieving efficient parallelized processing. A
minimalist metadata interface simplifies project/plugin configuration.
Experiments demonstrate the pipeline's validity through blind estimation of
mixing graphs, plugin/gain parameters, and its ability to bridge AI research
with practical DSP demands. The code is available on:
https://github.com/IsaacYQH/WildFX.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [382] [LNN-powered Fluid Antenna Multiple Access](https://arxiv.org/abs/2507.08821)
*Pedro D. Alvim,Hugerles S. Silva,Ugo S. Dias,Osamah S. Badarneh,Felipe A. P. Figueiredo,Rausley A. A. de Souza*

Main category: eess.SP

TL;DR: 本文首次将流体天线系统端口选择问题转化为多标签分类任务，用液体神经网络（LNN）预测最佳端口，经超参数优化后，结果显示该方法比现有方法中断概率更低。


<details>
  <summary>Details</summary>
Motivation: 在新兴流体天线多址场景下，优化信号干扰噪声比，解决有限端口观测下的最佳端口选择问题。

Method: 将端口选择问题转化为多标签分类任务，利用LNN预测最佳端口，采用超参数优化LNN架构。

Result: 该方法的中断概率低于现有方法。

Conclusion: 提出的方法在流体天线多址场景下的端口选择问题上优于现有方法。

Abstract: Fluid antenna systems represent an innovative approach in wireless
communication, recently applied in multiple access to optimize the
signal-to-interference-plus-noise ratio through port selection. This letter
frames the port selection problem as a multi-label classification task for the
first time, improving best-port selection with limited port observations. We
address this challenge by leveraging liquid neural networks (LNNs) to predict
the optimal port under emerging fluid antenna multiple access scenarios
alongside a more general $\alpha$-$\mu$ fading model. We also apply
hyperparameter optimization to refine LNN architectures for different
observation scenarios. Our approach yields lower outage probability values than
existing methods.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [383] [Frank-Wolfe Recursions for the Emergency Response Problem on Measure Spaces](https://arxiv.org/abs/2507.09808)
*Di Yu,Shane G. Henderson,Raghu Pasupathy*

Main category: math.OC

TL;DR: 研究院外心脏骤停应急响应的测度优化问题，提出fc - FW算法求解，展示其效果并表明可扩展到一类P - 均值问题。


<details>
  <summary>Details</summary>
Motivation: 解决院外心脏骤停应急响应中志愿资源空间分配以最小化死亡概率的无限维优化问题，该问题分析和计算有挑战。

Method: 先建立目标函数凸性、可行集紧致性等结构性质，推导影响函数，采用并分析fc - FW算法求解，不进行离散化或参数近似。

Result: fc - FW算法在简单离散和连续情形有复杂解结构和非平凡分配，可扩展到真实城市场景；当用L1范数建模时，影响函数分段严格凹，可快速计算。

Conclusion: 提出的框架和分析可自然扩展到一类P - 均值问题。

Abstract: We consider an optimization problem over measures for emergency response to
out-of-hospital cardiac arrest (OHCA), where the goal is to allocate volunteer
resources across a spatial region to minimize the probability of death. The
problem is infinite-dimensional and poses challenges for analysis and
computation. We first establish structural properties, including convexity of
the objective functional, compactness of the feasible set, and existence of
optimal solutions. We also derive the influence function, which serves as the
first-order variational object in our optimization framework. We then adapt and
analyze a fully-corrective Frank-Wolfe (fc-FW) algorithm that operates directly
on the infinite-dimensional problem without discretization or parametric
approximation. We show a form of convergence even when subproblems are not
solved to global optimality. Our full implementation of fc-FW demonstrates
complex solution structure even in simple discrete cases, reveals nontrivial
volunteer allocations in continuous cases, and scales to realistic urban
scenarios using OHCA data from the city of Auckland, New Zealand. Finally, we
show that when volunteer travel is modeled through the $L_1$ norm, the
influence function is piecewise strictly concave, enabling fast computation via
support reduction. The proposed framework and analysis extend naturally to a
broad class of $P$-means problems.

</details>


### [384] [Stochastic Approximation with Block Coordinate Optimal Stepsizes](https://arxiv.org/abs/2507.08963)
*Tao Jiang,Lin Xiao*

Main category: math.OC

TL;DR: 提出带块坐标步长的随机近似自适应步长规则，新方法性能与Adam相当但内存和超参数需求少，证明方法收敛性。


<details>
  <summary>Details</summary>
Motivation: 寻找能最小化下一次迭代点到最优点期望距离的自适应步长规则。

Method: 利用简单条件估计器，采用块坐标步长，结合在线估计各块坐标搜索方向二阶矩。

Result: 新方法性能与Adam相当，所需内存和超参数更少，证明方法几乎必然收敛到最优点小邻域。

Conclusion: 该系列方法具有收敛性，分析依赖的条件不要求凸性和平滑性，有广泛适用性。

Abstract: We consider stochastic approximation with block-coordinate stepsizes and
propose adaptive stepsize rules that aim to minimize the expected distance from
the next iterate to an optimal point. These stepsize rules employ online
estimates of the second moment of the search direction along each block
coordinate. The popular Adam algorithm can be interpreted as a particular
heuristic for such estimation. By leveraging a simple conditional estimator, we
derive a new method that obtains comparable performance as Adam but requires
less memory and fewer hyper-parameters. We prove that this family of methods
converges almost surely to a small neighborhood of the optimal point, and the
radius of the neighborhood depends on the bias and variance of the
second-moment estimator. Our analysis relies on a simple aiming condition that
assumes neither convexity nor smoothness, thus has broad applicability.

</details>


### [385] [On the Gradient Domination of the LQG Problem](https://arxiv.org/abs/2507.09026)
*Kasra Fallah,Leonardo F. Toso,James Anderson*

Main category: math.OC

TL;DR: 本文研究通过策略梯度方法求解线性二次高斯调节器问题，采用替代参数化方法，证明全局收敛和稳定性，并有数值实验支持。


<details>
  <summary>Details</summary>
Motivation: 策略梯度方法在LQR问题上有理论保证，但在LQG问题上理论理解有限，LQG问题缺乏梯度主导性阻碍全局收敛。

Method: 采用稳定控制器的替代参数化和提升论证，使用控制输入的历史表示，基于过去p个时间步的输入输出数据进行参数化。

Result: 建立了LQG成本的梯度主导性和近似平滑性，证明了基于模型和无模型设置下策略梯度LQG的全局收敛和每步稳定性。

Conclusion: 数值实验支持全局收敛保证，并展示了不同历史长度下的收敛情况。

Abstract: We consider solutions to the linear quadratic Gaussian (LQG) regulator
problem via policy gradient (PG) methods. Although PG methods have demonstrated
strong theoretical guarantees in solving the linear quadratic regulator (LQR)
problem, despite its nonconvex landscape, their theoretical understanding in
the LQG setting remains limited. Notably, the LQG problem lacks gradient
dominance in the classical parameterization, i.e., with a dynamic controller,
which hinders global convergence guarantees. In this work, we study PG for the
LQG problem by adopting an alternative parameterization of the set of
stabilizing controllers and employing a lifting argument. We refer to this
parameterization as a history representation of the control input as it is
parameterized by past input and output data from the previous p time-steps.
This representation enables us to establish gradient dominance and approximate
smoothness for the LQG cost. We prove global convergence and per-iteration
stability guarantees for policy gradient LQG in model-based and model-free
settings. Numerical experiments on an open-loop unstable system are provided to
support the global convergence guarantees and to illustrate convergence under
different history lengths of the history representation.

</details>


### [386] [A Method for Learning to Solve Parametric Bilevel Optimization with Coupling Constraints](https://arxiv.org/abs/2507.09050)
*James Kotary,Himanshu Sharma,Ethan King,Draguna Vrabie,Ferdinando Fioretto,Jan Drgona*

Main category: math.OC

TL;DR: 本文提出学习求解双级优化问题框架，用神经网络训练为参数化双级优化的高效近似器，并在合成双级程序及控制系统协同设计问题中验证。


<details>
  <summary>Details</summary>
Motivation: 现有学习优化方法多关注单级程序，双级程序虽有重要应用但难求解，尤其在时间要求严格的情况下。

Method: 利用通过优化问题进行微分的现代技术，提出学习求解广泛类别的具有挑战性的双级优化问题的框架。

Result: 框架在一系列合成双级程序和具有挑战性的控制系统协同设计问题中得到验证。

Conclusion: 神经网络可被训练为参数化双级优化的高效近似器。

Abstract: Learning to Optimize (L2O) is a subfield of machine learning (ML) in which ML
models are trained to solve parametric optimization problems. The general goal
is to learn a fast approximator of solutions to constrained optimization
problems, as a function of their defining parameters. Prior L2O methods focus
almost entirely on single-level programs, in contrast to the bilevel programs,
whose constraints are themselves expressed in terms of optimization
subproblems. Bilevel programs have numerous important use cases but are
notoriously difficult to solve, particularly under stringent time demands. This
paper proposes a framework for learning to solve a broad class of challenging
bilevel optimization problems, by leveraging modern techniques for
differentiation through optimization problems. The framework is illustrated on
an array of synthetic bilevel programs, as well as challenging control system
co-design problems, showing how neural networks can be trained as efficient
approximators of parametric bilevel optimization.

</details>


### [387] [Nesterov Finds GRAAL: Optimal and Adaptive Gradient Method for Convex Optimization](https://arxiv.org/abs/2507.09823)
*Ekaterina Borodich,Dmitry Kovalev*

Main category: math.OC

TL;DR: 本文提出带Nesterov加速的GRAAL算法，证明其对Lipschitz光滑函数达最优收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有自适应梯度方法收敛速度能否加速至Nesterov加速梯度下降的最优速率，且现有加速算法适应目标函数曲率能力有限。

Method: 开发带Nesterov加速的GRAAL算法。

Result: 算法对Lipschitz光滑函数达到最优收敛率，且能以任意初始步长实现。

Conclusion: 肯定了加速自适应梯度方法收敛速度的可能性，并给出有效算法。

Abstract: In this paper, we focus on the problem of minimizing a continuously
differentiable convex objective function $\min_x f(x)$. Recently, several
adaptive gradient methods, including GRAAL (Malitsky, 2020), have been
developed. These methods estimate the local curvature of the objective function
to compute stepsizes, attain the standard convergence rate $\mathcal{O}(1/k)$
of fixed-stepsize gradient descent for Lipschitz-smooth functions, and do not
require any line search procedures or hyperparameter tuning. However, a natural
question arises: is it possible to accelerate the convergence of these
algorithms to match the optimal rate $\mathcal{O}(1/k^2)$ of the accelerated
gradient descent of Nesterov (1983)? Although some attempts have been made (Li
and Lan, 2023), the capabilities of the existing accelerated algorithms to
adapt to the curvature of the objective function are highly limited.
Consequently, we provide a positive answer to this question and develop GRAAL
with Nesterov acceleration. We prove that our algorithm achieves the desired
optimal convergence rate for Lipschitz smooth functions. Moreover, in contrast
to existing methods, it does so with an arbitrary, even excessively small,
initial stepsize at the cost of a logarithmic additive term in the iteration
complexity.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [388] [An Accurate Discretized Approach to Parameter Estimation in the CKLS Model via the CIR Framework](https://arxiv.org/abs/2507.10041)
*Sourojyoti Barick*

Main category: stat.AP

TL;DR: 论文聚焦CIR和CKLS利率模型参数估计与渐近行为，用欧拉 - 丸山离散化处理，证明估计量性质，探索边界行为，推导平稳分布条件。


<details>
  <summary>Details</summary>
Motivation: 研究利率模型中参数的估计和渐近行为，为利率建模提供理论基础。

Method: 采用欧拉 - 丸山离散化将连续时间随机微分方程转化为离散形式，用线性回归技术进行参数估计。

Result: 证明了漂移和波动率参数估计量的强一致性和渐近正态性，探索了模型边界行为，推导了CKLS框架下平稳分布存在的充分条件及相应平稳密度函数。

Conclusion: 为利率模型的参数估计提供了理论支持，有助于分析复杂利率动态。

Abstract: This paper provides insight into the estimation and asymptotic behavior of
parameters in interest rate models, focusing primarily on the
Cox-Ingersoll-Ross (CIR) process and its extension -- the more general
Chan-Karolyi-Longstaff-Sanders (CKLS) framework ($\alpha\in[0.5,1]$). The CIR
process is widely used in modeling interest rates which possess the mean
reverting feature. An Extension of CIR model, CKLS model serves as a
foundational case for analyzing more complex dynamics. We employ Euler-Maruyama
discretization to transform the continuous-time stochastic differential
equations (SDEs) of these models into a discretized form that facilitates
efficient simulation and estimation of parameters using linear regression
techniques. We established the strong consistency and asymptotic normality of
the estimators for the drift and volatility parameters, providing a theoretical
underpinning for the parameter estimation process. Additionally, we explore the
boundary behavior of these models, particularly in the context of
unattainability at zero and infinity, by examining the scale and speed density
functions associated with generalized SDEs involving polynomial drift and
diffusion terms. Furthermore, we derive sufficient conditions for the existence
of a stationary distribution within the CKLS framework and the corresponding
stationary density function; and discuss its dependence on model parameters for
$\alpha\in[0.5,1]$.

</details>


### [389] [History Matching under Uncertainty of Geological Scenarios with Implicit Geological Realism Control with Generative Deep Learning and Graph Convolutions](https://arxiv.org/abs/2507.10201)
*Gleb Shishaev,Vasily Demyanov,Daniel Arnold*

Main category: stat.AP

TL;DR: 本文提出基于图的变分自编码器处理地质场景不确定性，与传统方法不同，通过实验验证方法可行性并分析潜在空间结构。


<details>
  <summary>Details</summary>
Motivation: 处理不同地质场景的不确定性，采用新方法替代传统基于晶格的深度学习方法。

Method: 利用基于图的变分自编码器，通过生成模型的潜在变量和测地距离隐式控制地质真实性。

Result: 在合成数据集上的实验表明该方法可行，对潜在空间进行了深入分析。

Conclusion: 基于图的变分自编码器可有效处理地质场景不确定性，方法具有可行性。

Abstract: The graph-based variational autoencoder represents an architecture that can
handle the uncertainty of different geological scenarios, such as depositional
or structural, through the concept of a lowerdimensional latent space. The main
difference from recent studies is utilisation of a graph-based approach in
reservoir modelling instead of the more traditional lattice-based deep learning
methods. We provide a solution to implicitly control the geological realism
through the latent variables of a generative model and Geodesic metrics. Our
experiments of AHM with synthetic dataset that consists of 3D realisations of
channelised geological representations with two distinct scenarios with one and
two channels shows the viability of the approach. We offer in-depth analysis of
the latent space using tools such as PCA, t-SNE, and TDA to illustrate its
structure.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [390] [When the Weak Becomes Strong: Effective Observables via Time-Symmetric Quantum Selection](https://arxiv.org/abs/2507.09716)
*Mirco A. Mannucci*

Main category: quant-ph

TL;DR: 研究时间对称量子力学中弱值的顺序组合，给出弱值乘积对应可观测量的归一化期望值，分析其结构，拓展到混合态并说明应用。


<details>
  <summary>Details</summary>
Motivation: 在时间对称量子力学框架下研究弱值的顺序组合问题。

Method: 先进行正向弱测量，再进行反向弱测量，分析弱值乘积与可观测量的关系，分析可观测量结构，将纯态拓展到混合态。

Result: 弱值乘积对应归一化期望值，可观测量编码干涉信息，可拓展到混合态，能应用于量子信息等领域，纯态下可通过强测量推断弱值相位。

Conclusion: 该研究为量子信息领域提供新视角和方法，如在量子计算中见证特定状态的错误等。

Abstract: We investigate the sequential composition of weak values in the framework of
time-symmetric quantum mechanics. Specifically, we consider a forward'' weak
measurement from a preselected state $\ket{\psi}$ to a post-selected state
$\ket{\phi}$, followed by a reverse'' weak measurement. We show that the
product of these two weak values corresponds to the normalized expectation
value of a strong, state-conditioned observable $B = A P_\psi A$, where $P_\psi
= \ket{\psi}\bra{\psi}$ is the projector onto the preselected state. Analyzing
the structure of $B$, we demonstrate how it encodes interference information,
particularly when $\ket{\psi}$ is a superposition rather than an eigenstate of
$A$. This formulation extends naturally to mixed states by replacing $P_\psi$
with a generic density matrix $\rho$, linking the construction to the formalism
of generalized quantum measurements. We illustrate practical applications in
quantum information, including state-specific error witnessing in quantum
computing, and show how the phase of a weak value can be inferred via strong
measurements in the pure-state case.

</details>


### [391] [Sequence-Model-Guided Measurement Selection for Quantum State Learning](https://arxiv.org/abs/2507.09891)
*Jiaxin Huang,Yan Zhu,Giulio Chiribella,Ya-Dong Wu*

Main category: quant-ph

TL;DR: 引入深度神经网络以数据驱动、自适应方式搜索量子系统有效测量选择，在多任务中表现优于随机选择，对拓扑量子系统有特别推荐。


<details>
  <summary>Details</summary>
Motivation: 解决量子系统规模增大时测量选择优化变得难以处理的问题。

Method: 引入具有序列模型架构的深度神经网络。

Result: 神经网络确定的测量选择在各类任务中始终优于均匀随机选择，对拓扑量子系统倾向于推荐边界测量。

Conclusion: 神经网络可能独立发现了边界和体之间的联系，且无需内置量子物理知识。

Abstract: Characterization of quantum systems from experimental data is a central
problem in quantum science and technology. But which measurements should be
used to gather data in the first place? While optimal measurement choices can
be worked out for small quantum systems, the optimization becomes intractable
as the system size grows large. To address this problem, we introduce a deep
neural network with a sequence model architecture that searches for efficient
measurement choices in a data-driven, adaptive manner. The model can be applied
to a variety of tasks, including the prediction of linear and nonlinear
properties of quantum states, as well as state clustering and state tomography
tasks. In all these tasks, we find that the measurement choices identified by
our neural network consistently outperform the uniformly random choice.
Intriguingly, for topological quantum systems, our model tends to recommend
measurements at the system's boundaries, even when the task is to predict bulk
properties. This behavior suggests that the neural network may have
independently discovered a connection between boundaries and bulk, without
having been provided any built-in knowledge of quantum physics.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [392] [Analysing Health Misinformation with Advanced Centrality Metrics in Online Social Networks](https://arxiv.org/abs/2507.09055)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Main category: cs.SI

TL;DR: 本文引入三种新的中心性指标，结合传统指标以应对在线社交网络健康谣言传播问题，新指标在识别节点和减少谣言传播上效果更好，综合指标框架更有效。


<details>
  <summary>Details</summary>
Motivation: 在线社交网络中健康谣言快速传播带来挑战，传统中心性指标在复杂动态网络中存在局限。

Method: 引入动态影响中心性、健康谣言脆弱性中心性和传播中心性三种新指标，结合FibVID和Monant Medical Misinformation两个数据集，对比传统和新指标的效果。

Result: 新指标识别出更多独特节点，结合传统指标使识别节点增加44.83%；结合新指标干预使健康谣言减少比例从50%提升至62.5%，提升25%；新指标在第二个数据集上也能成功识别独特影响者。

Conclusion: 传统和新的中心性指标结合，能提供更强大、通用的框架来理解和缓解不同在线网络中健康谣言的传播。

Abstract: The rapid spread of health misinformation on online social networks (OSNs)
during global crises such as the COVID-19 pandemic poses challenges to public
health, social stability, and institutional trust. Centrality metrics have long
been pivotal in understanding the dynamics of information flow, particularly in
the context of health misinformation. However, the increasing complexity and
dynamism of online networks, especially during crises, highlight the
limitations of these traditional approaches. This study introduces and compares
three novel centrality metrics: dynamic influence centrality (DIC), health
misinformation vulnerability centrality (MVC), and propagation centrality (PC).
These metrics incorporate temporal dynamics, susceptibility, and multilayered
network interactions. Using the FibVID dataset, we compared traditional and
novel metrics to identify influential nodes, propagation pathways, and
misinformation influencers. Traditional metrics identified 29 influential
nodes, while the new metrics uncovered 24 unique nodes, resulting in 42
combined nodes, an increase of 44.83%. Baseline interventions reduced health
misinformation by 50%, while incorporating the new metrics increased this to
62.5%, an improvement of 25%. To evaluate the broader applicability of the
proposed metrics, we validated our framework on a second dataset, Monant
Medical Misinformation, which covers a diverse range of health misinformation
discussions beyond COVID-19. The results confirmed that the advanced metrics
generalised successfully, identifying distinct influential actors not captured
by traditional methods. In general, the findings suggest that a combination of
traditional and novel centrality measures offers a more robust and
generalisable framework for understanding and mitigating the spread of health
misinformation in different online network contexts.

</details>


### [393] [Advanced Health Misinformation Detection Through Hybrid CNN-LSTM Models Informed by the Elaboration Likelihood Model (ELM)](https://arxiv.org/abs/2507.09149)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Main category: cs.SI

TL;DR: 本文运用ELM和混合CNN-LSTM模型提升社交媒体健康错误信息检测性能，结合特征工程后模型表现更佳，证明心理学理论在解决健康错误信息方面的应用价值。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情期间健康错误信息对全球公共卫生工作构成挑战，需提升社交媒体错误信息检测准确性和可靠性。

Method: 应用Elaboration Likelihood Model (ELM)，结合混合Convolutional Neural Network (CNN)和Long Short-Term Memory (LSTM)模型，融入ELM特征如文本可读性、情感极性和启发式线索。

Result: 增强模型准确率97.37%等；结合特征工程后，精度达98.88%等。

Conclusion: ELM特征对提升检测性能有价值，展示了心理学理论在开发先进机器学习算法解决健康错误信息方面的实际应用。

Abstract: Health misinformation during the COVID-19 pandemic has significantly
challenged public health efforts globally. This study applies the Elaboration
Likelihood Model (ELM) to enhance misinformation detection on social media
using a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory
(LSTM) model. The model aims to enhance the detection accuracy and reliability
of misinformation classification by integrating ELM-based features such as text
readability, sentiment polarity, and heuristic cues (e.g., punctuation
frequency). The enhanced model achieved an accuracy of 97.37%, precision of
96.88%, recall of 98.50%, F1-score of 97.41%, and ROC-AUC of 99.50%. A combined
model incorporating feature engineering further improved performance, achieving
a precision of 98.88%, recall of 99.80%, F1-score of 99.41%, and ROC-AUC of
99.80%. These findings highlight the value of ELM features in improving
detection performance, offering valuable contextual information. This study
demonstrates the practical application of psychological theories in developing
advanced machine learning algorithms to address health misinformation
effectively.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [394] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 文章指出大语言模型及多模态扩展推动视频理解进步，但处理长时间视频仍有挑战，提出无限视频理解这一前沿研究方向并列出核心挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前处理长时间视频存在计算、内存约束及保持时间连贯性等难题，需要新的研究方向推动发展。

Method: 借鉴长/超长视频理解及相关领域工作，提出无限视频理解概念并确定核心挑战和研究方向。

Result: 明确无限视频理解这一前沿研究方向，指出其能为多媒体和AI研究社区提供指引。

Conclusion: 无限视频理解是多媒体研究的下一个前沿，能驱动多个领域的创新。

Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

</details>


### [395] [Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching](https://arxiv.org/abs/2507.09256)
*Junyu Chen,Yihua Gao,Mingyuan Ge,Mingyong Li*

Main category: cs.CV

TL;DR: 本文提出AAHR框架解决图像文本匹配问题，通过多种策略提升模型能力，实验显示其在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像文本匹配方法在处理高阶关联和语义歧义方面存在挑战，未充分利用训练批次中语义相似实例的邻域关系。

Method: 提出AAHR框架，通过动态聚类原型对比学习构建统一表示空间，引入特征提取和聚合网络，利用相关矩阵和GNN增强实例语义交互，集成动量对比学习扩展负样本集。

Result: AAHR在Flickr30K、MSCOCO和ECCV Caption数据集上优于现有最优方法，提高了图像文本匹配的准确性和效率。

Conclusion: AAHR框架有效解决了现有图像文本匹配方法的问题，提升了模型性能。

Abstract: Image-text matching is crucial for bridging the semantic gap between computer
vision and natural language processing. However, existing methods still face
challenges in handling high-order associations and semantic ambiguities among
similar instances. These ambiguities arise from subtle differences between soft
positive samples (semantically similar but incorrectly labeled) and soft
negative samples (locally matched but globally inconsistent), creating matching
uncertainties. Furthermore, current methods fail to fully utilize the
neighborhood relationships among semantically similar instances within training
batches, limiting the model's ability to learn high-order shared knowledge.
This paper proposes the Ambiguity-Aware and High-order Relation learning
framework (AAHR) to address these issues. AAHR constructs a unified
representation space through dynamic clustering prototype contrastive learning,
effectively mitigating the soft positive sample problem. The framework
introduces global and local feature extraction mechanisms and an adaptive
aggregation network, significantly enhancing full-grained semantic
understanding capabilities. Additionally, AAHR employs intra-modal and
inter-modal correlation matrices to investigate neighborhood relationships
among sample instances thoroughly. It incorporates GNN to enhance semantic
interactions between instances. Furthermore, AAHR integrates momentum
contrastive learning to expand the negative sample set. These combined
strategies significantly improve the model's ability to discriminate between
features. Experimental results demonstrate that AAHR outperforms existing
state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,
considerably improving the accuracy and efficiency of image-text matching. The
code and model checkpoints for this research are available at
https://github.com/Image-Text-Matching/AAHR .

</details>


### [396] [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
*Daniele Rege Cambrin,Lorenzo Vaiani,Giuseppe Gallipoli,Luca Cagliero,Paolo Garza*

Main category: cs.CV

TL;DR: 本文提出CrisisLandMark语料库和CLOSP框架用于光学和SAR图像检索，提升检索性能，还引入GeoCLOSP框架，强调整合多传感器数据和地理上下文的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像检索系统大多局限于RGB数据，未能利用其他传感器数据，本文旨在弥补这一差距。

Method: 引入CrisisLandMark语料库，提出CLOSP框架将未配对的光学和SAR图像对齐到统一嵌入空间，还引入GeoCLOSP框架整合地理坐标。

Result: CLOSP达到了新的最优水平，检索nDGC比现有模型提高了54%，统一训练策略克服了SAR图像解释难题，GeoCLOSP在通用性和特异性之间取得平衡。

Conclusion: 整合不同传感器数据和地理上下文对释放遥感档案的全部潜力至关重要。

Abstract: Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

</details>


### [397] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Möller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Main category: cs.CV

TL;DR: BrainLesion Suite是用于构建模块化脑病变图像分析管道的Python工具包，具有易用性和多功能性，可用于多种脑病变及其他生物医学图像分析，代码和教程在GitHub上可获取。


<details>
  <summary>Details</summary>
Motivation: 提供一个遵循Python原则的工具包，减少开发的认知负担，简化临床和科学实践中复杂工作流的创建。

Method: 核心是可适应的预处理模块，利用BraTS挑战的算法合成缺失模态、修复病变和生成肿瘤分割，还能使用panoptica等工具量化分割模型性能。

Result: 开发出BrainLesion Suite工具包，可用于脑病变图像分析及其他生物医学图像分析。

Conclusion: BrainLesion Suite具有通用性，不仅适用于多种脑病变图像分析，还能应用于其他生物医学图像分析场景。

Abstract: BrainLesion Suite is a versatile toolkit for building modular brain lesion
image analysis pipelines in Python. Following Pythonic principles, BrainLesion
Suite is designed to provide a 'brainless' development experience, minimizing
cognitive effort and streamlining the creation of complex workflows for
clinical and scientific practice. At its core is an adaptable preprocessing
module that performs co-registration, atlas registration, and optional
skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion
Suite leverages algorithms from the BraTS challenge to synthesize missing
modalities, inpaint lesions, and generate pathology-specific tumor
segmentations. BrainLesion Suite also enables quantifying segmentation model
performance, with tools such as panoptica to compute lesion-wise metrics.
Although BrainLesion Suite was originally developed for image analysis
pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,
it can be adapted for other biomedical image analysis applications. The
individual BrainLesion Suite packages and tutorials are accessible on GitHub.

</details>


### [398] [An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds](https://arxiv.org/abs/2412.11407)
*TianZhu Liu,BangYan Hu,YanFeng Gu,Xian Li,Aleksandra Pižurica*

Main category: cs.CV

TL;DR: 本文提出基于自适应多尺度融合的多光谱点云分类方法，解决户外数据集分类问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多光谱点云分类方法在户外数据集存在标记目标稀疏、地物尺度差异和长尾分布等问题。

Method: 训练集生成阶段采用网格平衡采样策略；特征学习阶段提出多尺度特征融合模块；分类阶段设计自适应混合损失模块。

Result: 在三个多光谱点云数据集上的实验表明，该方法比现有方法更有效。

Conclusion: 所提方法能有效解决户外数据集分类中的问题，提升分类性能。

Abstract: Multispectral point cloud (MPC) captures 3D spatial-spectral information from
the observed scene, which can be used for scene understanding and has a wide
range of applications. However, most of the existing classification methods
were extensively tested on indoor datasets, and when applied to outdoor
datasets they still face problems including sparse labeled targets, differences
in land-covers scales, and long-tailed distributions. To address the above
issues, an enhanced classification method based on adaptive multi-scale fusion
for MPCs with long-tailed distributions is proposed. In the training set
generation stage, a grid-balanced sampling strategy is designed to reliably
generate training samples from sparse labeled datasets. In the feature learning
stage, a multi-scale feature fusion module is proposed to fuse shallow features
of land-covers at different scales, addressing the issue of losing fine
features due to scale variations in land-covers. In the classification stage,
an adaptive hybrid loss module is devised to utilize multi-classification heads
with adaptive weights to balance the learning ability of different classes,
improving the classification performance of small classes due to various-scales
and long-tailed distributions in land-covers. Experimental results on three MPC
datasets demonstrate the effectiveness of the proposed method compared with the
state-of-the-art methods.

</details>


### [399] [Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging](https://arxiv.org/abs/2507.08052)
*Mazen Ali,António Pereira,Fabio Gentile,Aser Cortines,Sam Mugel,Román Orús,Stelios P. Neophytides,Michalis Mavrovouniotis*

Main category: cs.CV

TL;DR: 评估多种机器学习方法用于高光谱卫星图像云与云阴影掩膜，CNN特征降维模型最有效，证明轻量级AI模型用于实时处理潜力。


<details>
  <summary>Details</summary>
Motivation: 云与云阴影掩膜是高光谱卫星成像关键预处理步骤，需评估有效方法。

Method: 评估多种机器学习方法，包括梯度提升方法（XGBoost、LightGBM）和卷积神经网络（CNN）。

Result: 所有提升和CNN模型准确率超93%，CNN特征降维模型最有效，参数少的版本在部署可行性、准确性和计算效率上平衡最佳。

Conclusion: 轻量级AI模型有用于实时高光谱图像处理潜力，支持星载AI系统开发。

Abstract: Cloud and cloud shadow masking is a crucial preprocessing step in
hyperspectral satellite imaging, enabling the extraction of high-quality,
analysis-ready data. This study evaluates various machine learning approaches,
including gradient boosting methods such as XGBoost and LightGBM as well as
convolutional neural networks (CNNs). All boosting and CNN models achieved
accuracies exceeding 93%. Among the investigated models, the CNN with feature
reduction emerged as the most efficient, offering a balance of high accuracy,
low storage requirements, and rapid inference times on both CPUs and GPUs.
Variations of this version, with only up to 597 trainable parameters,
demonstrated the best trade-off in terms of deployment feasibility, accuracy,
and computational efficiency. These results demonstrate the potential of
lightweight artificial intelligence (AI) models for real-time hyperspectral
image processing, supporting the development of on-board satellite AI systems
for space-based applications.

</details>


### [400] [AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition](https://arxiv.org/abs/2507.09248)
*Varsha Devi,Amine Bohi,Pardeep Kumar*

Main category: cs.CV

TL;DR: 提出AGCD - Net模型解决上下文感知情感识别中的上下文偏差问题，在CAER - S数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统上下文感知情感识别方法存在上下文偏差问题，即背景上下文与情感标签存在虚假关联。

Method: 提出AGCD - Net模型，引入Hybrid ConvNeXt编码器增强特征重新校准，核心是AG - CIM模块，运用因果理论消除虚假关联。

Result: 在CAER - S数据集上实验，AGCD - Net取得了当前最优性能。

Conclusion: 因果去偏对复杂场景下的鲁棒情感识别非常重要。

Abstract: Context-aware emotion recognition (CAER) enhances affective computing in
real-world scenarios, but traditional methods often suffer from context
bias-spurious correlation between background context and emotion labels (e.g.
associating ``garden'' with ``happy''). In this paper, we propose
\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces
\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the
ConvNeXt backbone by integrating Spatial Transformer Network and
Squeeze-and-Excitation layers for enhanced feature recalibration. At the core
of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),
which applies causal theory, perturbs context features, isolates spurious
correlations, and performs an attention-driven correction guided by face
features to mitigate context bias. Experimental results on the CAER-S dataset
demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art
performance and highlighting the importance of causal debiasing for robust
emotion recognition in complex settings.

</details>


### [401] [Cross Knowledge Distillation between Artificial and Spiking Neural Networks](https://arxiv.org/abs/2507.09269)
*Shuhan Ye,Yuanbin Qian,Chong Wang,Sunqi Lin,Jiazhen Xu,Jiangbo Qian,Yuqi Li*

Main category: cs.CV

TL;DR: 本文提出跨知识蒸馏（CKD）方法提升脉冲神经网络（SNNs）在DVS数据上的性能，实验表明该方法优于当前SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 因有限的带注释事件数据集和不成熟的SNN架构，SNNs在计算机视觉领域性能不如人工神经网络（ANNs），需提升SNNs在DVS数据上的性能。

Method: 提出跨知识蒸馏（CKD）方法，利用语义相似性和滑动替换缓解跨模态挑战，使用间接分阶段知识蒸馏缓解跨架构挑战。

Result: 在主流神经形态数据集（如N - Caltech101和CEP - DVS）上验证，该方法优于当前SOTA方法。

Conclusion: 所提CKD方法能有效提升SNNs在DVS数据上的性能。

Abstract: Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in
computer vision domain due to their high biological plausibility, event-driven
characteristic and energy-saving efficiency. Still, limited annotated
event-based datasets and immature SNN architectures result in their performance
inferior to that of Artificial Neural Networks (ANNs). To enhance the
performance of SNNs on their optimal data format, DVS data, we explore using
RGB data and well-performing ANNs to implement knowledge distillation. In this
case, solving cross-modality and cross-architecture challenges is necessary. In
this paper, we propose cross knowledge distillation (CKD), which not only
leverages semantic similarity and sliding replacement to mitigate the
cross-modality challenge, but also uses an indirect phased knowledge
distillation to mitigate the cross-architecture challenge. We validated our
method on main-stream neuromorphic datasets, including N-Caltech101 and
CEP-DVS. The experimental results show that our method outperforms current
State-of-the-Art methods. The code will be available at
https://github.com/ShawnYE618/CKD

</details>


### [402] [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
*Anita Kriz,Elizabeth Laura Janes,Xing Shen,Tal Arbel*

Main category: cs.CV

TL;DR: 介绍Prompt4Trust框架用于MLLM置信度校准，提升医疗应用可信度和任务准确率，有零样本泛化潜力。


<details>
  <summary>Details</summary>
Motivation: MLLM在医疗安全关键场景部署受提示设计敏感和高置信度下易出错两方面限制，需提升其可信度。

Method: 引入基于强化学习的Prompt4Trust框架，训练轻量级LLM生成上下文感知辅助提示，引导下游任务MLLM。

Result: 提升任务准确率，在PMC - VQA基准上达到SOTA；小模型训练框架对大模型有零样本泛化潜力。

Conclusion: 自动化且符合人类需求的提示工程可提升MLLM在安全关键场景的可信度。

Abstract: Multimodal large language models (MLLMs) hold considerable promise for
applications in healthcare. However, their deployment in safety-critical
settings is hindered by two key limitations: (i) sensitivity to prompt design,
and (ii) a tendency to generate incorrect responses with high confidence. As
clinicians may rely on a model's stated confidence to gauge the reliability of
its predictions, it is especially important that when a model expresses high
confidence, it is also highly accurate. We introduce Prompt4Trust, the first
reinforcement learning (RL) framework for prompt augmentation targeting
confidence calibration in MLLMs. A lightweight LLM is trained to produce
context-aware auxiliary prompts that guide a downstream task MLLM to generate
responses in which the expressed confidence more accurately reflects predictive
accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically
prioritizes aspects of calibration most critical for safe and trustworthy
clinical decision-making. Beyond improvements driven by this clinically
motivated calibration objective, our proposed method also improves task
accuracy, achieving state-of-the-art medical visual question answering (VQA)
performance on the PMC-VQA benchmark, which is composed of multiple-choice
questions spanning diverse medical imaging modalities. Moreover, our framework
trained with a small downstream task MLLM showed promising zero-shot
generalization to larger MLLMs in our experiments, suggesting the potential for
scalable calibration without the associated computational costs. This work
demonstrates the potential of automated yet human-aligned prompt engineering
for improving the the trustworthiness of MLLMs in safety critical settings. Our
codebase can be found at https://github.com/xingbpshen/vccrl-llm.

</details>


### [403] [ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation](https://arxiv.org/abs/2507.09299)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.CV

TL;DR: 提出ViT - ProtoNet用于少样本图像分类，在多个基准测试中表现出色，设置了新基线。


<details>
  <summary>Details</summary>
Motivation: 解决Vision Transformers在少样本图像分类中表征能力未充分利用的问题。

Method: 将ViT - Small骨干网络集成到原型网络框架，通过平均少量支持示例的类条件令牌嵌入构建原型。

Result: 在四个标准基准测试中始终优于基于CNN的原型网络，5 - shot准确率最高提升3.2%，在潜在空间有更好的特征可分性，用更轻量级骨干网络优于或媲美基于Transformer的竞争对手。

Conclusion: ViT - ProtoNet是少样本分类的强大、灵活方法，为基于Transformer的元学习器设置了新基线。

Abstract: The remarkable representational power of Vision Transformers (ViTs) remains
underutilized in few-shot image classification. In this work, we introduce
ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical
Network framework. By averaging class conditional token embeddings from a
handful of support examples, ViT-ProtoNet constructs robust prototypes that
generalize to novel categories under 5-shot settings. We conduct an extensive
empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,
CUB-200, and CIFAR-FS, including overlapped support variants to assess
robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based
prototypical counterparts, achieving up to a 3.2\% improvement in 5-shot
accuracy and demonstrating superior feature separability in latent space.
Furthermore, it outperforms or is competitive with transformer-based
competitors using a more lightweight backbone. Comprehensive ablations examine
the impact of transformer depth, patch size, and fine-tuning strategy. To
foster reproducibility, we release code and pretrained weights. Our results
establish ViT-ProtoNet as a powerful, flexible approach for few-shot
classification and set a new baseline for transformer-based meta-learners.

</details>


### [404] [AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning](https://arxiv.org/abs/2507.09308)
*Zile Wang,Hao Yu,Jiabo Zhan,Chun Yuan*

Main category: cs.CV

TL;DR: 提出首个RGBA基准ALPHA和统一端到端RGBA VAE模型ALPHAVAE，在重建上表现优于LayerDiffuse，且能实现更好的透明图像生成，代码等已开源。


<details>
  <summary>Details</summary>
Motivation: 现有潜在扩散模型在RGBA图像生成方面因缺乏大规模基准而研究不足，需要解决该问题。

Method: 提出ALPHA基准，将标准RGB指标通过在标准背景上进行alpha混合应用于四通道图像；引入ALPHAVAE，扩展预训练RGB VAE并结合复合目标进行训练。

Result: 在仅8K图像上训练的RGBA VAE，相比LayerDiffuse在重建上PSNR提升4.9 dB，SSIM提升3.2%，在潜在扩散框架中微调时能实现更好的透明图像生成。

Conclusion: 所提出的ALPHA基准和ALPHAVAE模型在RGBA图像生成和重建方面具有有效性和优越性。

Abstract: Recent advances in latent diffusion models have achieved remarkable results
in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress
and reconstruct pixel data at low computational cost. However, the generation
of transparent or layered content (RGBA image) remains largely unexplored, due
to the lack of large-scale benchmarks. In this work, we propose ALPHA, the
first comprehensive RGBA benchmark that adapts standard RGB metrics to
four-channel images via alpha blending over canonical backgrounds. We further
introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB
VAE by incorporating a dedicated alpha channel. The model is trained with a
composite objective that combines alpha-blended pixel reconstruction,
patch-level fidelity, perceptual consistency, and dual KL divergence
constraints to ensure latent fidelity across both RGB and alpha
representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used
by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase
in SSIM over LayerDiffuse in reconstruction. It also enables superior
transparent image generation when fine-tuned within a latent diffusion
framework. Our code, data, and models are released on
https://github.com/o0o0o00o0/AlphaVAE for reproducibility.

</details>


### [405] [Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data](https://arxiv.org/abs/2507.09420)
*Timothy Chase Jr,Karthik Dantu*

Main category: cs.CV

TL;DR: 本文提出用于原位地标跟踪的新方法，利用轻量级神经网络，结合改进的领域适应方法和注意力对齐公式，构建统一地标跟踪系统，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统基于光度测量法的流程有诸多局限，新兴基于学习的计算机视觉技术存在计算需求高和训练数据稀缺问题，需要新方法解决。

Method: 利用轻量级、计算高效的神经网络架构；提出改进的领域适应方法用于地标检测；引入注意力对齐公式用于地标描述。

Result: 构建的统一地标跟踪系统表现优于现有技术。

Conclusion: 所提新方法能有效实现原位地标跟踪，克服现有方法的不足。

Abstract: The detection and tracking of celestial surface terrain features are crucial
for autonomous spaceflight applications, including Terrain Relative Navigation
(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data
collection. Traditional photoclinometry-based pipelines often rely on extensive
a priori imaging and offline processing, constrained by the computational
limitations of radiation-hardened systems. While historically effective, these
approaches typically increase mission costs and duration, operate at low
processing rates, and have limited generalization. Recently, learning-based
computer vision has gained popularity to enhance spacecraft autonomy and
overcome these limitations. While promising, emerging techniques frequently
impose computational demands exceeding the capabilities of typical spacecraft
hardware for real-time operation and are further challenged by the scarcity of
labeled training data for diverse extraterrestrial environments. In this work,
we present novel formulations for in-situ landmark tracking via detection and
description. We utilize lightweight, computationally efficient neural network
architectures designed for real-time execution on current-generation spacecraft
flight processors. For landmark detection, we propose improved domain
adaptation methods that enable the identification of celestial terrain features
with distinct, cheaply acquired training data. Concurrently, for landmark
description, we introduce a novel attention alignment formulation that learns
robust feature representations that maintain correspondence despite significant
landmark viewpoint variations. Together, these contributions form a unified
system for landmark tracking that demonstrates superior performance compared to
existing state-of-the-art techniques.

</details>


### [406] [HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space](https://arxiv.org/abs/2507.09487)
*Changli Wang,Fang Yin,Jiafeng Liu,Rui Wu*

Main category: cs.CV

TL;DR: 提出Hyperbolic Masked Image and Distillation Network (HMID - Net)在双曲空间集成MIM和知识蒸馏技术训练高效模型，实验表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决如何更有效地训练模型以捕捉和利用视觉 - 语义层次结构的问题。

Method: 提出HMID - Net，在双曲空间集成MIM和知识蒸馏技术，引入专门的蒸馏损失函数。

Result: MIM和知识蒸馏技术在双曲空间取得与欧氏空间同样的成功，在下游任务中表现出色。

Conclusion: HMID - Net方法在图像分类和检索等任务上显著优于MERU和CLIP等现有模型。

Abstract: Visual and semantic concepts are often structured in a hierarchical manner.
For instance, textual concept `cat' entails all images of cats. A recent study,
MERU, successfully adapts multimodal learning techniques from Euclidean space
to hyperbolic space, effectively capturing the visual-semantic hierarchy.
However, a critical question remains: how can we more efficiently train a model
to capture and leverage this hierarchy? In this paper, we propose the
\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel
and efficient method that integrates Masked Image Modeling (MIM) and knowledge
distillation techniques within hyperbolic space. To the best of our knowledge,
this is the first approach to leverage MIM and knowledge distillation in
hyperbolic space to train highly efficient models. In addition, we introduce a
distillation loss function specifically designed to facilitate effective
knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM
and knowledge distillation techniques in hyperbolic space can achieve the same
remarkable success as in Euclidean space. Extensive evaluations show that our
method excels across a wide range of downstream tasks, significantly
outperforming existing models like MERU and CLIP in both image classification
and retrieval.

</details>


### [407] [SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification](https://arxiv.org/abs/2507.09492)
*Fuyin Ye,Erwen Yao,Jianyong Chen,Fengmei He,Junxiang Zhang,Lihao Ni*

Main category: cs.CV

TL;DR: 提出SDTN和TRN用于高光谱图像分类，在PaviaU数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统高光谱图像分类方法在处理高维数据、光谱空间冗余和标记样本稀缺方面表现不佳，导致性能不理想。

Method: 提出SDTN结合张量分解与正则化机制动态调整张量秩；在此基础上提出TRN，将SDTN提取的特征集成到轻量级网络中，多尺度捕获光谱空间特征。

Result: 在PaviaU数据集实验中，与现有方法相比，准确性显著提高，模型参数减少。

Conclusion: 所提方法不仅保持高分类准确率，还降低计算复杂度，适合资源受限环境实时部署。

Abstract: Hyperspectral image classification plays a pivotal role in precision
agriculture, providing accurate insights into crop health monitoring, disease
detection, and soil analysis. However, traditional methods struggle with
high-dimensional data, spectral-spatial redundancy, and the scarcity of labeled
samples, often leading to suboptimal performance. To address these challenges,
we propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines
tensor decomposition with regularization mechanisms to dynamically adjust
tensor ranks, ensuring optimal feature representation tailored to the
complexity of the data. Building upon SDTN, we propose the Tensor-Regularized
Network (TRN), which integrates the features extracted by SDTN into a
lightweight network capable of capturing spectral-spatial features at multiple
scales. This approach not only maintains high classification accuracy but also
significantly reduces computational complexity, making the framework highly
suitable for real-time deployment in resource-constrained environments.
Experiments on PaviaU datasets demonstrate significant improvements in accuracy
and reduced model parameters compared to state-of-the-art methods.

</details>


### [408] [QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models](https://arxiv.org/abs/2507.09514)
*Tien-Yu Chi,Hung-Yueh Chiang,Diana Marculescu,Kai-Chiang Wu*

Main category: cs.CV

TL;DR: 提出QuarterMap方法去除SSM模型冗余激活，提升吞吐量，在多任务上有效且避免高成本操作。


<details>
  <summary>Details</summary>
Motivation: VMamba作为基于SSM的视觉骨干网络，受四向扫描中空间冗余的瓶颈限制。

Method: 提出QuarterMap，一种训练后激活剪枝方法，扫描前去除冗余空间激活，通过最近邻上采样恢复维度。

Result: 在ImageNet - 1K上VMamba提速11%且精度损失小于0.9%，在ADE20K分割上有类似提升；在MedMamba上提升吞吐量并保持精度；相比ToMe避免了高成本操作。

Conclusion: QuarterMap是一种即插即用工具，能在不影响可迁移性的情况下提高部署效率。

Abstract: State space models (SSMs) reduce the quadratic complexity of transformers by
leveraging linear recurrence. Recently, VMamba has emerged as a strong
SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in
its four-directional scan. We propose QuarterMap, a post-training activation
pruning method that removes redundant spatial activations before scanning and
restores dimensions via nearest-neighbor upsampling. Our method improves
throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%
speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains
on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a
domain-specific model that shares the same four-directional scanning structure,
where it consistently improves throughput while preserving accuracy across
multiple medical imaging tasks. Compared to token merging methods like ToMe,
QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our
method offers a plug-and-play tool for deployment-time efficiency without
compromising transferability.

</details>


### [409] [VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization](https://arxiv.org/abs/2507.09531)
*Son Nguyen,Giang Nguyen,Hung Dao,Thao Do,Daeyoung Kim*

Main category: cs.CV

TL;DR: 提出MLLM模型VDInstruct，采用内容感知标记策略，在KIE基准测试中取得SOTA结果，减少图像标记数量，零样本评估表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在密集文档上表现差，视觉标记化方法存在冗余计算和内存低效问题。

Method: 引入VDInstruct模型，将空间区域检测与语义特征提取分离，采用内容感知标记策略，使用三阶段训练范式。

Result: 在KIE基准测试中取得SOTA结果，减少约3.6倍图像标记数量，零样本评估中比强基线DocOwl 1.5高出5.5个F1分数。

Conclusion: 内容感知标记与显式布局建模结合为文档理解提供了有前景的方向。

Abstract: Key Information Extraction (KIE) underpins the understanding of visual
documents (e.g., receipts and contracts) by extracting precise semantic content
and accurately capturing spatial structure. Yet existing multimodal large
language models (MLLMs) often perform poorly on dense documents and rely on
vision tokenization approaches that scale with image size, leading to redundant
computation and memory inefficiency. To address these challenges, we introduce
VDInstruct, an MLLM that separates spatial region detection from semantic
feature extraction. Central to our model is a content-aware tokenization
strategy: rather than fragmenting the entire image uniformly, it generates
tokens in proportion to document complexity, preserving critical structure
while eliminating wasted tokens. Leveraging a three-stage training paradigm,
our model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching
or exceeding the accuracy of leading approaches while reducing the number of
image tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses
strong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its
robustness to unseen documents. These findings show that content-aware
tokenization combined with explicit layout modeling offers a promising
direction forward for document understanding. Data, source code, and model
weights will be made publicly available.

</details>


### [410] [Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges](https://arxiv.org/abs/2507.09562)
*Yidong Jiang*

Main category: cs.CV

TL;DR: 本文对SAM及其变体的提示工程技术进行全面综述，涵盖基础方法、应用和挑战，揭示其演变并指出研究方向。


<details>
  <summary>Details</summary>
Motivation: SAM虽革新图像分割，但提示工程在其成功中的关键作用未被充分探索，需要对相关技术进行全面研究。

Method: 系统地组织和分析该新兴领域快速增长的研究成果。

Result: 揭示提示工程从简单几何输入发展到复杂多模态方法，使SAM能应用于多领域，还识别出提示优化的独特挑战。

Conclusion: 本综述为理解和推进分割基础模型的提示工程提供了结构化框架，填补了文献空白。

Abstract: The Segment Anything Model (SAM) has revolutionized image segmentation
through its innovative prompt-based approach, yet the critical role of prompt
engineering in its success remains underexplored. This paper presents the first
comprehensive survey focusing specifically on prompt engineering techniques for
SAM and its variants. We systematically organize and analyze the rapidly
growing body of work in this emerging field, covering fundamental
methodologies, practical applications, and key challenges. Our review reveals
how prompt engineering has evolved from simple geometric inputs to
sophisticated multimodal approaches, enabling SAM's adaptation across diverse
domains including medical imaging and remote sensing. We identify unique
challenges in prompt optimization and discuss promising research directions.
This survey fills an important gap in the literature by providing a structured
framework for understanding and advancing prompt engineering in foundation
models for segmentation.

</details>


### [411] [MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models](https://arxiv.org/abs/2507.09574)
*Haozhe Zhao,Zefan Cai,Shuzheng Si,Liang Chen,Jiuxiang Gu,Wen Xiao,Junjie Hu*

Main category: cs.CV

TL;DR: 现有文本到图像模型有局限，提出MENTOR框架用于高效多模态图像生成，虽资源有限仍表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像模型在精确视觉控制、平衡多模态输入及复杂多模态图像生成训练方面的局限。

Method: 提出MENTOR框架，结合AR图像生成器与两阶段训练范式，两阶段分别为多模态对齐和多模态指令调优。

Result: 在DreamBench++基准测试中表现出色，在概念保留和遵循提示方面优于竞争基线，图像重建保真度、任务适应性和训练效率优于基于扩散的方法。

Conclusion: MENTOR框架能有效解决现有文本到图像模型的局限，具有良好性能和优势。

Abstract: Recent text-to-image models produce high-quality results but still struggle
with precise visual control, balancing multimodal inputs, and requiring
extensive training for complex multimodal image generation. To address these
limitations, we propose MENTOR, a novel autoregressive (AR) framework for
efficient Multimodal-conditioned Tuning for Autoregressive multimodal image
generation. MENTOR combines an AR image generator with a two-stage training
paradigm, enabling fine-grained, token-level alignment between multimodal
inputs and image outputs without relying on auxiliary adapters or
cross-attention modules. The two-stage training consists of: (1) a multimodal
alignment stage that establishes robust pixel- and semantic-level alignment,
followed by (2) a multimodal instruction tuning stage that balances the
integration of multimodal inputs and enhances generation controllability.
Despite modest model size, suboptimal base components, and limited training
resources, MENTOR achieves strong performance on the DreamBench++ benchmark,
outperforming competitive baselines in concept preservation and prompt
following. Additionally, our method delivers superior image reconstruction
fidelity, broad task adaptability, and improved training efficiency compared to
diffusion-based methods. Dataset, code, and models are available at:
https://github.com/HaozheZhao/MENTOR

</details>


### [412] [Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI](https://arxiv.org/abs/2507.09630)
*Shomukh Qari,Maha A. Thafar*

Main category: cs.CV

TL;DR: 本文提出基于CT图像的多类中风分类AI框架，用MaxViT模型结合数据增强取得98%准确率，还集成XAI增强可解释性，助力临床诊断。


<details>
  <summary>Details</summary>
Motivation: 中风是全球主要死因之一，早期准确诊断至关重要，CT扫描是关键成像方式，需开发AI辅助诊断工具。

Method: 采用MaxViT等深度学习模型，结合数据增强技术，集成Explainable Artificial Intelligence (XAI) 特别是Grad - CAM++。

Result: MaxViT模型经数据增强训练后准确率和F1分数达98.00%，优于其他评估模型和基线方法。

Conclusion: 本研究有助于开发可靠的AI辅助中风诊断工具，推动其在临床实践应用，改善急诊中风诊断。

Abstract: Stroke is one of the leading causes of death globally, making early and
accurate diagnosis essential for improving patient outcomes, particularly in
emergency settings where timely intervention is critical. CT scans are the key
imaging modality because of their speed, accessibility, and cost-effectiveness.
This study proposed an artificial intelligence framework for multiclass stroke
classification (ischemic, hemorrhagic, and no stroke) using CT scan images from
a dataset provided by the Republic of Turkey's Ministry of Health. The proposed
method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary
deep learning model for image-based stroke classification, with additional
transformer variants (vision transformer, transformer-in-transformer, and
ConvNext). To enhance model generalization and address class imbalance, we
applied data augmentation techniques, including synthetic image generation. The
MaxViT model trained with augmentation achieved the best performance, reaching
an accuracy and F1-score of 98.00%, outperforming all other evaluated models
and the baseline methods. The primary goal of this study was to distinguish
between stroke types with high accuracy while addressing crucial issues of
transparency and trust in artificial intelligence models. To achieve this,
Explainable Artificial Intelligence (XAI) was integrated into the framework,
particularly Grad-CAM++. It provides visual explanations of the model's
decisions by highlighting relevant stroke regions in the CT scans and
establishing an accurate, interpretable, and clinically applicable solution for
early stroke detection. This research contributed to the development of a
trustworthy AI-assisted diagnostic tool for stroke, facilitating its
integration into clinical practice and enhancing access to timely and optimal
stroke diagnosis in emergency departments, thereby saving more lives.

</details>


### [413] [View Invariant Learning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2507.08831)
*Josh Qixuan Sun,Xiaoying Xing,Huaiyuan Weng,Chul Min Yeum,Mark Crowley*

Main category: cs.CV

TL;DR: 本文提出V2 - VLNCE场景和VIL策略提升导航策略对相机视角变化的鲁棒性，在多个数据集上表现优于现有方法，且可作为即插即用的后训练方法。


<details>
  <summary>Details</summary>
Motivation: 多数导航策略对视角变化敏感，本文旨在增强现有导航策略对相机视角变化的鲁棒性。

Method: 提出VIL策略，采用对比学习框架学习稀疏和视角不变特征，引入师生框架用于航点预测器模块，采用端到端训练范式联合优化组件。

Result: 在V2 - VLNCE上比现有方法成功率高8 - 15%，在标准VLNCE设置下也常提升性能，在RxR - CE数据集上各项指标达最优。

Conclusion: VIL不降低标准视角性能，可作为即插即用的后训练方法。

Abstract: Vision-Language Navigation in Continuous Environments (VLNCE), where an agent
follows instructions and moves freely to reach a destination, is a key research
problem in embodied AI. However, most navigation policies are sensitive to
viewpoint changes, i.e., variations in camera height and viewing angle that
alter the agent's observation. In this paper, we introduce a generalized
scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View
Invariant Learning), a view-invariant post-training strategy that enhances the
robustness of existing navigation policies to changes in camera viewpoint. VIL
employs a contrastive learning framework to learn sparse and view-invariant
features. Additionally, we introduce a teacher-student framework for the
Waypoint Predictor Module, a core component of most VLNCE baselines, where a
view-dependent teacher model distills knowledge into a view-invariant student
model. We employ an end-to-end training paradigm to jointly optimize these
components, thus eliminating the cost for individual module training. Empirical
results show that our method outperforms state-of-the-art approaches on
V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets
R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE
setting and find that, despite being trained for varied viewpoints, it often
still improves performance. On the more challenging RxR-CE dataset, our method
also achieved state-of-the-art performance across all metrics when compared to
other map-free methods. This suggests that adding VIL does not diminish the
standard viewpoint performance and can serve as a plug-and-play post-training
method.

</details>


### [414] [A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends](https://arxiv.org/abs/2507.09861)
*Yihao Ding,Siwen Luo,Yue Dai,Yanbei Jiang,Zechuan Li,Geoffrey Martin,Yifan Peng*

Main category: cs.CV

TL;DR: 本文综述基于MLLM的VRDU最新进展，介绍核心组件，讨论挑战机遇并提出未来方向。


<details>
  <summary>Details</summary>
Motivation: VRDU领域需自动处理含复杂信息的文档，MLLMs在该领域有潜力，故进行相关综述。

Method: 对基于MLLM的VRDU相关研究进行综述，分析编码融合特征方法、训练范式和使用的数据集。

Result: 梳理出基于MLLM的VRDU的三个核心组件。

Conclusion: 探讨了该领域挑战机遇，提出提升VRDU系统效率、泛化性和鲁棒性的未来方向。

Abstract: Visually-Rich Document Understanding (VRDU) has emerged as a critical field,
driven by the need to automatically process documents containing complex
visual, textual, and layout information. Recently, Multimodal Large Language
Models (MLLMs) have shown remarkable potential in this domain, leveraging both
Optical Character Recognition (OCR)-dependent and OCR-free frameworks to
extract and interpret information in document images. This survey reviews
recent advancements in MLLM-based VRDU, highlighting three core components: (1)
methods for encoding and fusing textual, visual, and layout features; (2)
training paradigms, including pretraining strategies, instruction-response
tuning, and the trainability of different model modules; and (3) datasets
utilized for pretraining, instruction-tuning, and supervised fine-tuning.
Finally, we discuss the challenges and opportunities in this evolving field and
propose future directions to advance the efficiency, generalizability, and
robustness of VRDU systems.

</details>


### [415] [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/abs/2507.08979)
*Mahdiyar Molahasani,Azadeh Motamedi,Michael Greenspan,Il-Min Kim,Ali Etemad*

Main category: cs.CV

TL;DR: 提出PRISM方法为视觉语言模型去偏，两阶段操作，实验表现优于现有方法并公开代码。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型常继承和放大训练数据中的偏差，导致预测结果有偏差，需要去偏。

Method: 先让大语言模型根据简单类别提示生成包含虚假关联的场景描述，再用对比式去偏损失学习投影，将嵌入映射到潜在空间以减少虚假关联并保留图像和文本嵌入的对齐。

Result: 在Waterbirds和CelebA数据集上，PRISM优于当前去偏方法。

Conclusion: PRISM是一种有效的数据无关、任务无关的视觉语言模型去偏解决方案。

Abstract: We introduce Projection-based Reduction of Implicit Spurious bias in
vision-language Models (PRISM), a new data-free and task-agnostic solution for
bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in
their training data, leading to skewed predictions. PRISM is designed to debias
VLMs without relying on predefined bias categories or additional external data.
It operates in two stages: first, an LLM is prompted with simple class prompts
to generate scene descriptions that contain spurious correlations. Next, PRISM
uses our novel contrastive-style debiasing loss to learn a projection that maps
the embeddings onto a latent space that minimizes spurious correlations while
preserving the alignment between image and text embeddings.Extensive
experiments demonstrate that PRISM outperforms current debiasing methods on the
commonly used Waterbirds and CelebA datasets We make our code public at:
https://github.com/MahdiyarMM/PRISM.

</details>


### [416] [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876)
*Yongheng Zhang,Xu Liu,Ruihan Tao,Qiguang Chen,Hao Fei,Wanxiang Che,Libo Qin*

Main category: cs.CV

TL;DR: 提出视频-文本交错思维链（ViTCoT）范式用于视频推理，构建视频-文本交错基准（ViTIB），实验表明其性能优于传统仅文本思维链范式。


<details>
  <summary>Details</summary>
Motivation: 当前视频推理方法主要依赖文本信息，忽略视觉模态，而人类推理时会自然审视视觉内容，因此提出新范式。

Method: 构建用多模态大语言模型（MLLMs）进行关键视频选择并手动验证的ViTIB，探索ViTCoT范式在视频理解领域的潜力。

Result: ViTCoT显著提升性能，相比传统仅文本思维链范式效果更好，且能有效激活MLLMs中更多神经元值。

Conclusion: ViTCoT范式在视频推理中表现更优，能实现更直观且符合认知的推理。

Abstract: Video understanding plays a vital role in bridging low-level visual signals
with high-level cognitive reasoning, and is fundamental to applications such as
autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid
development of large language models (LLMs), particularly those utilizing
Chain-of-Thought (CoT) technology, has significantly advanced video reasoning
capabilities. However, current approaches primarily depend on textual
information for reasoning, overlooking the visual modality in the actual video
reasoning process. In contrast, humans naturally re-examine visual content
while reasoning. Motivated by this, we introduce a novel video reasoning
paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive
and cognitively aligned reasoning. To the end, first, we construct the
Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for
key-video selection and manually verified. Furthermore, we extensively explore
the potential of the ViTCoT paradigm in the video understanding field.
Extensive experiments demonstrate that ViTCoT significantly enhances
performance compared to the traditional text-only CoT paradigm and effectively
activates more neuron values in MLLMs.

</details>


### [417] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Main category: cs.CV

TL;DR: 针对类别条件扩散模型在不平衡数据上的训练问题，引入两种对比损失函数提升尾类图像多样性，方法易实现且效果好。


<details>
  <summary>Details</summary>
Motivation: 解决训练数据长尾分布导致尾类合成图像多样性降低的问题，在不影响头类图像保真度和多样性的前提下，提升尾类图像多样性。

Method: 引入无监督InfoNCE损失增加合成图像间距离，使用MSE损失在大时间步对比条件生成和无条件生成，实现条件 - 无条件对齐。

Result: 对比学习框架在多个数据集上优于标准DDPM和其他方法。

Conclusion: 成功将对比学习应用于类别不平衡的扩散模型，所提方法简单有效。

Abstract: Training data for class-conditional image synthesis often exhibit a
long-tailed distribution with limited images for tail classes. Such an
imbalance causes mode collapse and reduces the diversity of synthesized images
for tail classes. For class-conditional diffusion models trained on imbalanced
data, we aim to improve the diversity of tail class images without compromising
the fidelity and diversity of head class images. We achieve this by introducing
two deceptively simple but highly effective contrastive loss functions.
Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to
increase the distance/dissimilarity among synthetic images, particularly for
tail classes. To further enhance the diversity of tail classes, our second loss
is an MSE loss that contrasts class-conditional generation with unconditional
generation at large timesteps. This second loss makes the denoising process
insensitive to class conditions for the initial steps, which enriches tail
classes through knowledge sharing from head classes. Conditional-unconditional
alignment has been shown to enhance the performance of long-tailed GAN. We are
the first to adapt such alignment to diffusion models. We successfully
leveraged contrastive learning for class-imbalanced diffusion models. Our
contrastive learning framework is easy to implement and outperforms standard
DDPM and alternative methods for class-imbalanced diffusion models across
various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and
ImageNetLT.

</details>


### [418] [MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks](https://arxiv.org/abs/2507.09092)
*Ram S Iyer,Narayan S Iyer,Rugmini Ammal P*

Main category: cs.CV

TL;DR: 本文提出基于激活映射的事后视觉解释方法MI CAM，与以往方法不同，它通过互信息加权特征图，可产生因果解释，性能与现有方法相当且部分指标更优。


<details>
  <summary>Details</summary>
Motivation: 随着机器视觉应用增多，人们关注卷积神经网络内部机制和推理原因，需要一种新的视觉解释方法。

Method: 提出名为MI CAM的事后视觉解释方法，通过特征图与输入图像的互信息加权，线性组合权重和激活图生成结果，并借助反事实分析验证因果解释。

Result: 该方法性能与所有现有先进方法相当，在定性和定量指标上尤其优于部分方法。

Conclusion: MI CAM能展现视觉性能，为模型推理过程提供无偏解释，有较好应用价值。

Abstract: With the intervention of machine vision in our crucial day to day necessities
including healthcare and automated power plants, attention has been drawn to
the internal mechanisms of convolutional neural networks, and the reason why
the network provides specific inferences. This paper proposes a novel post-hoc
visual explanation method called MI CAM based on activation mapping. Differing
from previous class activation mapping based approaches, MI CAM produces
saliency visualizations by weighing each feature map through its mutual
information with the input image and the final result is generated by a linear
combination of weights and activation maps. It also adheres to producing causal
interpretations as validated with the help of counterfactual analysis. We aim
to exhibit the visual performance and unbiased justifications for the model
inferencing procedure achieved by MI CAM. Our approach works at par with all
state-of-the-art methods but particularly outperforms some in terms of
qualitative and quantitative measures. The implementation of proposed method
can be found on https://anonymous.4open.science/r/MI-CAM-4D27

</details>


### [419] [Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis](https://arxiv.org/abs/2507.09950)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: 本文对GPT - 4o - mini和Gemini 2.0 Flash进行零样本评估，用图像评估其在时尚产品属性识别任务中的表现，发现Gemini 2.0 Flash整体表现更强，研究为电商产品属性任务及时尚AI研究提供参考。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在细粒度时尚属性识别方面研究不足，而产品属性对时尚零售业务很重要，需评估模型在该任务中的表现。

Method: 使用DeepFashion - MultiModal数据集，以图像为唯一输入，对GPT - 4o - mini和Gemini 2.0 Flash在18类时尚属性任务中进行零样本评估。

Result: Gemini 2.0 Flash整体表现最强，宏F1分数为56.79%，GPT - 4o - mini宏F1分数为43.28%。

Conclusion: 研究为生产环境中部署大语言模型用于电商产品属性相关任务提供实用见解，强调了特定领域微调的必要性，为时尚AI和多模态属性提取研究奠定基础。

Abstract: The fashion retail business is centered around the capacity to comprehend
products. Product attribution helps in comprehending products depending on the
business process. Quality attribution improves the customer experience as they
navigate through millions of products offered by a retail website. It leads to
well-organized product catalogs. In the end, product attribution directly
impacts the 'discovery experience' of the customer. Although large language
models (LLMs) have shown remarkable capabilities in understanding multimodal
data, their performance on fine-grained fashion attribute recognition remains
under-explored. This paper presents a zero-shot evaluation of state-of-the-art
LLMs that balance performance with speed and cost efficiency, mainly
GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset
DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to
evaluate these models in the attribution tasks of fashion products. Our study
evaluates these models across 18 categories of fashion attributes, offering
insight into where these models excel. We only use images as the sole input for
product information to create a constrained environment. Our analysis shows
that Gemini 2.0 Flash demonstrates the strongest overall performance with a
macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a
macro F1 score of 43.28%. Through detailed error analysis, our findings provide
practical insights for deploying these LLMs in production e-commerce product
attribution-related tasks and highlight the need for domain-specific
fine-tuning approaches. This work also lays the groundwork for future research
in fashion AI and multimodal attribute extraction.

</details>


### [420] [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/abs/2507.09118)
*Linlan Huang,Xusheng Cao,Haori Lu,Yifan Meng,Fei Yang,Xialei Liu*

Main category: cs.CV

TL;DR: 文章分析视觉语言预训练模型微调时模态差距变化，提出MG - CLIP方法提升CLIP在类增量学习中的性能，实验表明该方法优于现有方法且无需额外重放数据。


<details>
  <summary>Details</summary>
Motivation: 持续学习受关注，现有利用CLIP进行持续学习的工作忽视其固有模态差距这一影响泛化和适应性的关键因素。

Method: 分析视觉语言预训练模型微调时模态差距变化，提出MG - CLIP方法，利用模态差距保留缓解遗忘，利用模态差距补偿增强新数据处理能力。

Result: 在多个基准测试上的广泛实验表明，该方法无需额外重放数据，性能优于现有方法。

Conclusion: 提出基于模态差距的新视角用于持续学习，MG - CLIP方法有效提升了CLIP在类增量学习中的性能。

Abstract: Continual learning aims to enable models to learn sequentially from
continuously incoming data while retaining performance on previously learned
tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting
strong capabilities across various downstream tasks, there has been growing
interest in leveraging CLIP for continual learning in such scenarios. Most
existing works overlook the inherent modality gap in CLIP, a key factor in its
generalization and adaptability. In this paper, we analyze the variations in
the modality gap during the fine-tuning of vision-language pre-trained models.
Our observations reveal that the modality gap effectively reflects the extent
to which pre-trained knowledge is preserved. Based on these insights, we
propose a simple yet effective method, MG-CLIP, that improves CLIP's
performance in class-incremental learning. Our approach leverages modality gap
preservation to mitigate forgetting and modality gap compensation to enhance
the capacity for new data, introducing a novel modality-gap-based perspective
for continual learning. Extensive experiments on multiple benchmarks
demonstrate that our method outperforms existing approaches without requiring
additional replay data. Our code is available at
https://github.com/linlany/MindtheGap.

</details>


### [421] [(Almost) Free Modality Stitching of Foundation Models](https://arxiv.org/abs/2507.10015)
*Jaisidh Singh,Diganta Misra,Boris Knyazev,Antonio Orvieto*

Main category: cs.CV

TL;DR: 提出Hypernetwork Model Alignment (Hyma)解决多模态模型中单模态模型选择和连接器训练计算成本高的问题，实验显示降低搜索成本且性能相当。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型通过拼接单模态模型，训练连接器在大规模数据集上计算需求大，且单模态模型选择和连接器训练问题研究不足。

Method: 提出Hyma框架，利用超网络的参数预测能力为单模态模型的N×M组合获得联合训练的连接器模块。

Result: Hyma将最优单模态模型对的搜索成本平均降低了10倍，在多模态基准测试中排名和训练连接器性能与网格搜索相当。

Conclusion: Hyma是解决单模态模型选择和连接器训练问题的有效一体化方案。

Abstract: Foundation multi-modal models are often designed by stitching of multiple
existing pretrained uni-modal models: for example, an image classifier with an
autoregressive text model. This stitching process is performed by training a
connector module that aims to align the representation-representation or
representation-input spaces of these uni-modal models. However, given the
complexity of training such connectors on large scale web-based datasets
coupled with the ever-increasing number of available pretrained uni-modal
models, the task of uni-modal models selection and subsequent connector module
training becomes computationally demanding. To address this under-studied
critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel
all-in-one solution for optimal uni-modal model selection and connector
training by leveraging hypernetworks. Specifically, our framework utilizes the
parameter prediction capability of a hypernetwork to obtain jointly trained
connector modules for $N \times M$ combinations of uni-modal models. In our
experiments, Hyma reduces the optimal uni-modal model pair search cost by
$10\times$ (averaged across all experiments), while matching the ranking and
trained connector performance obtained via grid search across a suite of
diverse multi-modal benchmarks.

</details>


### [422] [Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning](https://arxiv.org/abs/2507.10056)
*A. K. M. Shoriful Islam,Md. Rakib Hassan,Macbah Uddin,Md. Shahidur Rahman*

Main category: cs.CV

TL;DR: 本文提出基于机器学习分析家禽粪便图像检测疾病的方法，与深度学习模型相比精度相当但资源使用少，适合低资源农业场景。


<details>
  <summary>Details</summary>
Motivation: 家禽养殖易受传染病影响，需有效疾病检测方法。

Method: 利用多颜色空间特征提取，探索多种颜色、纹理和形状描述符，通过消融研究和降维确定紧凑全局特征集，用人工神经网络分类。

Result: 人工神经网络分类器准确率达95.85%，无需GPU，在Google Colab执行时间仅638秒。

Conclusion: 该方法是低资源农业场景中实时家禽疾病检测的经济、可解释和可扩展的替代方案。

Abstract: Poultry farming is a vital component of the global food supply chain, yet it
remains highly vulnerable to infectious diseases such as coccidiosis,
salmonellosis, and Newcastle disease. This study proposes a lightweight machine
learning-based approach to detect these diseases by analyzing poultry fecal
images. We utilize multi-color space feature extraction (RGB, HSV, LAB) and
explore a wide range of color, texture, and shape-based descriptors, including
color histograms, local binary patterns (LBP), wavelet transforms, and edge
detectors. Through a systematic ablation study and dimensionality reduction
using PCA and XGBoost feature selection, we identify a compact global feature
set that balances accuracy and computational efficiency. An artificial neural
network (ANN) classifier trained on these features achieved 95.85% accuracy
while requiring no GPU and only 638 seconds of execution time in Google Colab.
Compared to deep learning models such as Xception and MobileNetV3, our proposed
model offers comparable accuracy with drastically lower resource usage. This
work demonstrates a cost-effective, interpretable, and scalable alternative to
deep learning for real-time poultry disease detection in low-resource
agricultural settings.

</details>


### [423] [Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift](https://arxiv.org/abs/2507.09222)
*Behraj Khan,Tahir Syed*

Main category: cs.CV

TL;DR: 提出统一框架StaRFM解决基础模型部署的分布偏移和置信度失调问题，在多个数据集表现良好，框架即插即用。


<details>
  <summary>Details</summary>
Motivation: 基础模型在部署时存在训练和测试数据分布偏移、置信度失调问题，现有解决方案是特定领域的，需要统一框架。

Method: 提出StaRFM框架，引入Fisher信息惩罚项（FIP）减少嵌入中的协变量偏移，置信度失调惩罚项（CMP）校准分割任务的不确定性，理论推导PAC - Bayes边界。

Result: 在19个视觉数据集上准确率提升3.5%、ECE降低28%，医学分割达到84.7% DSC和4.8mm HD95，跨域性能差距降低40%。

Conclusion: StaRFM框架有效解决基础模型部署挑战，可与基础模型无缝集成。

Abstract: Foundation models like CLIP and SAM have transformed computer vision and
medical imaging via low-shot transfer learning. However, deployment of these
models hindered by two key challenges: \textit{distribution shift} between
training and test data, and \textit{confidence misalignment} that leads to
overconfident incorrect predictions. These issues manifest differently in
vision-language classification and medical segmentation tasks, yet existing
solutions remain domain-specific. We propose \textit{StaRFM}, a unified
framework addressing both challenges. It introduces a Fisher information
penalty (FIP), extended to 3D medical data via patch-wise regularization, to
reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence
misalignment penalty (CMP), reformulated for voxel-level predictions,
calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes
bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP
minimizes calibration error through Brier score optimization. StaRFM shows
consistent performance like \texttt{+}3.5\% accuracy and 28\% lower ECE on 19
vision datasets (e.g., ImageNet, Office-Home), 84.7\% DSC and 4.8mm HD95 in
medical segmentation (e.g., BraTS, ATLAS), and 40\% lower cross-domain
performance gap compared to prior benchmarking methods. The framework is
plug-and-play, requiring minimal architectural changes for seamless integration
with foundation models. Code and models will be released at
https://anonymous.4open.science/r/StaRFM-C0CD/README.md

</details>


### [424] [Supercharging Floorplan Localization with Semantic Rays](https://arxiv.org/abs/2507.09291)
*Yuval Grader,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 提出语义感知定位框架用于平面图定位，评估显示该方法优于现有技术，还能结合额外元数据提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有平面图定位技术多关注深度结构线索，忽略平面图丰富语义信息。

Method: 引入语义感知定位框架，联合估计深度和语义射线，构建粗到细的结构 - 语义概率体积，先粗采样得初始低分辨率体积，再在高概率区域密集采样细化概率以预测二维位置和方向角。

Result: 在两个标准平面图定位基准测试中，该方法显著优于现有技术，召回指标有显著提升，且能结合额外元数据。

Conclusion: 提出的语义感知定位框架有效，能提高平面图定位的准确性和效率。

Abstract: Floorplans provide a compact representation of the building's structure,
revealing not only layout information but also detailed semantics such as the
locations of windows and doors. However, contemporary floorplan localization
techniques mostly focus on matching depth-based structural cues, ignoring the
rich semantics communicated within floorplans. In this work, we introduce a
semantic-aware localization framework that jointly estimates depth and semantic
rays, consolidating over both for predicting a structural-semantic probability
volume. Our probability volume is constructed in a coarse-to-fine manner: We
first sample a small set of rays to obtain an initial low-resolution
probability volume. We then refine these probabilities by performing a denser
sampling only in high-probability regions and process the refined values for
predicting a 2D location and orientation angle. We conduct an evaluation on two
standard floorplan localization benchmarks. Our experiments demonstrate that
our approach substantially outperforms state-of-the-art methods, achieving
significant improvements in recall metrics compared to prior works. Moreover,
we show that our framework can easily incorporate additional metadata such as
room labels, enabling additional gains in both accuracy and efficiency.

</details>


### [425] [Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion](https://arxiv.org/abs/2507.10127)
*Md Abulkalam Azad,John Nyberg,Håvard Dalen,Bjørnar Grenne,Lasse Lovstakken,Andreas Østvik*

Main category: cs.CV

TL;DR: 本文研究超声心动图中SOTA点跟踪方法，改进训练策略和提出轻量级网络，实验表明策略可提升模型性能，临床评估显示方法改善GLS测量。


<details>
  <summary>Details</summary>
Motivation: 传统方法在超声心动图心脏运动估计中表现不佳，现代点跟踪方法在该领域未充分探索，且现有新方法在超声心动图中有效性和泛化性有限。

Method: 分析心脏周期运动，识别方向运动偏差，改进训练程序，引入定制增强，提出轻量级网络。

Result: 微调策略显著提升模型性能，如EchoTracker提高位置精度、降低轨迹误差，部分点跟踪模型不如简单模型，方法改善GLS测量。

Conclusion: 提出的方法可提升超声心动图点跟踪模型性能和泛化性，在临床应用中有更好的可重复性。

Abstract: Accurate motion estimation for tracking deformable tissues in
echocardiography is essential for precise cardiac function measurements. While
traditional methods like block matching or optical flow struggle with intricate
cardiac motion, modern point tracking approaches remain largely underexplored
in this domain. This work investigates the potential of state-of-the-art (SOTA)
point tracking methods for ultrasound, with a focus on echocardiography.
Although these novel approaches demonstrate strong performance in general
videos, their effectiveness and generalizability in echocardiography remain
limited. By analyzing cardiac motion throughout the heart cycle in real B-mode
ultrasound videos, we identify that a directional motion bias across different
views is affecting the existing training strategies. To mitigate this, we
refine the training procedure and incorporate a set of tailored augmentations
to reduce the bias and enhance tracking robustness and generalization through
impartial cardiac motion. We also propose a lightweight network leveraging
multi-scale cost volumes from spatial context alone to challenge the advanced
spatiotemporal point tracking models. Experiments demonstrate that fine-tuning
with our strategies significantly improves models' performances over their
baselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker
boosts overall position accuracy by 60.7% and reduces median trajectory error
by 61.5% across heart cycle phases. Interestingly, several point tracking
models fail to outperform our proposed simple model in terms of tracking
accuracy and generalization, reflecting their limitations when applied to
echocardiography. Nevertheless, clinical evaluation reveals that these methods
improve GLS measurements, aligning more closely with expert-validated,
semi-automated tools and thus demonstrating better reproducibility in
real-world applications.

</details>


### [426] [DAA*: Deep Angular A Star for Image-based Path Planning](https://arxiv.org/abs/2507.09305)
*Zhiwei Xu*

Main category: cs.CV

TL;DR: 提出深度角度A*（DAA*）方法，通过路径角度自由度（PAF）改善路径平滑度和相似度，在多数据集评估中表现优于对比方法，还探讨了路径最优性与搜索效率的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决路径模仿学习中路径平滑度常被忽视的问题。

Method: 将PAF融入A*算法，通过联合优化路径缩短和平滑来提高路径最优性。

Result: 在7个数据集上，DAA*在路径相似度上显著优于Neural A*和TransPath，路径长度更短。

Conclusion: DAA*能有效提高路径相似度和最优性，存在路径最优性与搜索效率的小权衡。

Abstract: Path smoothness is often overlooked in path imitation learning from expert
demonstrations. In this paper, we introduce a novel learning method, termed
deep angular A* (DAA*), by incorporating the proposed path angular freedom
(PAF) into A* to improve path similarity through adaptive path smoothness. The
PAF aims to explore the effect of move angles on path node expansion by finding
the trade-off between their minimum and maximum values, allowing for high
adaptiveness for imitation learning. DAA* improves path optimality by closely
aligning with the reference path through joint optimization of path shortening
and smoothing, which correspond to heuristic distance and PAF, respectively.
Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets,
2 video-game datasets, and a real-world drone-view dataset containing 2
scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in
path similarity between the predicted and reference paths with a shorter path
length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,
and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path
loss and path probability map loss, DAA* significantly outperforms the
state-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also
discuss the minor trade-off between path optimality and search efficiency where
applicable.

</details>


### [427] [GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups](https://arxiv.org/abs/2507.09410)
*Bernie Boscoe,Shawn Johnson,Andrea Osborn,Chandler Campbell,Karen Mager*

Main category: cs.CV

TL;DR: 本文介绍了一种适用于资源有限的小型研究小组的本地处理相机陷阱数据的低资源管道，帮助研究人员从数据中获取有意义的见解。


<details>
  <summary>Details</summary>
Motivation: 随着相机陷阱数据收集量增加，开发、处理和管理数据，尤其是采用ML/AI工具面临挑战，小型研究小组资源和计算专业知识有限。

Method: 提出一种本地处理相机陷阱数据的低资源管道，融入适合小型研究小组的ML/AI能力。

Result: 该管道提供了数据传输、推理和评估的可行方法。

Conclusion: 该管道为资源和计算专业知识有限的小型研究小组提供了实用解决方案，使其能从不断增加的相机陷阱数据集中发现有意义的见解。

Abstract: Camera traps have long been used by wildlife researchers to monitor and study
animal behavior, population dynamics, habitat use, and species diversity in a
non-invasive and efficient manner. While data collection from the field has
increased with new tools and capabilities, methods to develop, process, and
manage the data, especially the adoption of ML/AI tools, remain challenging.
These challenges include the sheer volume of data generated, the need for
accurate labeling and annotation, variability in environmental conditions
affecting data quality, and the integration of ML/AI tools into existing
workflows that often require domain-specific customization and computational
resources. This paper provides a guide to a low-resource pipeline to process
camera trap data on-premise, incorporating ML/AI capabilities tailored for
small research groups with limited resources and computational expertise. By
focusing on practical solutions, the pipeline offers accessible approaches for
data transmission, inference, and evaluation, enabling researchers to discover
meaningful insights from their ever-increasing camera trap datasets.

</details>


### [428] [A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images](https://arxiv.org/abs/2507.10202)
*Jaeseong Lee,Yeeun Choi,Heechan Choi,Hanjung Kim,Seonjoo Kim*

Main category: cs.CV

TL;DR: 提出ECP框架提升MLLM处理高分辨率图像性能，实验验证有效，代码开源。


<details>
  <summary>Details</summary>
Motivation: MLLMs处理高分辨率图像时，因固定图像分辨率微调存在泛化差、细节丢失等问题。

Method: 提出训练免费、任务无关的两阶段框架ECP，先根据粗预测确定候选区域，再基于候选区域预测最终输出。

Result: 在4K GUI grounding和4K、8K MLLM感知任务上较基线分别有+21.3%、+5.8%、+5.2%的绝对提升。

Conclusion: ECP框架能有效保留细粒度细节，减轻高分辨率数据挑战，提升MLLM性能。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in vision-language understanding, reasoning, and generation.
However, they struggle with tasks requiring fine-grained localization and
reasoning in high-resolution images. This constraint stems from the fact that
MLLMs are fine-tuned with fixed image resolution to align with the pre-trained
image encoder used in MLLM. Consequently, feeding high-resolution images
directly into MLLMs leads to poor generalization due to a train-test resolution
discrepancy, while downsampling these images-although ensuring
consistency-compromises fine-grained visual details and ultimately degrades
performance. To address this challenge, we propose Extract Candidate then
Predict (ECP), a novel training-free, task-agnostic two-stage framework
designed to enhance MLLM performance on high-resolution images. The key
intuition behind ECP is that while MLLMs struggle with high-resolution images,
their predictions on downsampled images still contain implicit localization
cues. By first identifying candidate region using the coarse prediction and
then predicting the final output based on candidate region, ECP effectively
preserves fine-grained details while mitigating the challenges posed by
high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K
MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared
to baseline respectively, demonstrating its effectiveness. Code is available at
https://github.com/yenncye/ECP.

</details>


### [429] [ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users](https://arxiv.org/abs/2507.10223)
*Xiangyu Yin,Boyuan Yang,Weichen Liu,Qiyao Xue,Abrar Alamri,Goeran Fiedler,Wei Gao*

Main category: cs.CV

TL;DR: 本文提出多用途数据集ProGait支持多种视觉任务，展示其应用与性能，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 视觉机器学习方法在假肢步态分析中检测和分析假肢存在挑战，需填补这一空白。

Method: 引入ProGait数据集，包含412个视频片段，还给出基准任务和微调的基线模型。

Result: 将基线模型与预训练视觉模型对比，显示ProGait数据集用于假肢特定任务时泛化性更好。

Conclusion: ProGait数据集能有效支持多种视觉任务，在假肢特定任务中有更好表现。

Abstract: Prosthetic legs play a pivotal role in clinical rehabilitation, allowing
individuals with lower-limb amputations the ability to regain mobility and
improve their quality of life. Gait analysis is fundamental for optimizing
prosthesis design and alignment, directly impacting the mobility and life
quality of individuals with lower-limb amputations. Vision-based machine
learning (ML) methods offer a scalable and non-invasive solution to gait
analysis, but face challenges in correctly detecting and analyzing prosthesis,
due to their unique appearances and new movement patterns. In this paper, we
aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,
to support multiple vision tasks including Video Object Segmentation, 2D Human
Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from
four above-knee amputees when testing multiple newly-fitted prosthetic legs
through walking trials, and depicts the presence, contours, poses, and gait
patterns of human subjects with transfemoral prosthetic legs. Alongside the
dataset itself, we also present benchmark tasks and fine-tuned baseline models
to illustrate the practical application and performance of the ProGait dataset.
We compared our baseline models against pre-trained vision models,
demonstrating improved generalizability when applying the ProGait dataset for
prosthesis-specific tasks. Our code is available at
https://github.com/pittisl/ProGait and dataset at
https://huggingface.co/datasets/ericyxy98/ProGait.

</details>


### [430] [FaceLLM: A Multimodal Large Language Model for Face Understanding](https://arxiv.org/abs/2507.10300)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: 介绍针对面部图像理解的多模态大语言模型FaceLLM，用新方法构建训练数据，实验显示其提升了多模态大模型在面部任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型主要在通用数据集上训练，缺乏大规模标注的面部图像文本数据集，难以对特定领域的面部视觉线索进行推理。

Method: 提出一种弱监督管道，利用ChatGPT和属性感知提示，基于FairFace数据集图像生成高质量问答对，构建FairFaceGPT语料库。

Result: FaceLLM提高了多模态大语言模型在各种以面部为中心任务上的性能，达到了最先进水平。

Conclusion: 凸显了通过语言模型进行合成监督构建特定领域多模态大语言模型的潜力，为可信、以人为中心的多模态人工智能系统树立了先例。

Abstract: Multimodal large language models (MLLMs) have shown remarkable performance in
vision-language tasks. However, existing MLLMs are primarily trained on generic
datasets, limiting their ability to reason on domain-specific visual cues such
as those in facial images. In particular, tasks that require detailed
understanding of facial structure, expression, emotion, and demographic
features remain underexplored by MLLMs due to the lack of large-scale annotated
face image-text datasets. In this work, we introduce FaceLLM, a multimodal
large language model trained specifically for facial image understanding. To
construct the training data, we propose a novel weakly supervised pipeline that
uses ChatGPT with attribute-aware prompts to generate high-quality
question-answer pairs based on images from the FairFace dataset. The resulting
corpus, called FairFaceGPT, covers a diverse set of attributes including
expression, pose, skin texture, and forensic information. Our experiments
demonstrate that FaceLLM improves the performance of MLLMs on various
face-centric tasks and achieves state-of-the-art performance. This work
highlights the potential of synthetic supervision via language models for
building domain-specialized MLLMs, and sets a precedent for trustworthy,
human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM
models are publicly available in the project page.

</details>


### [431] [Devanagari Handwritten Character Recognition using Convolutional Neural Network](https://arxiv.org/abs/2507.10398)
*Diksha Mehta,Prateek Mehta*

Main category: cs.CV

TL;DR: 本文提出用两个深度卷积神经网络层识别手写天城文（Devanagari）字符的技术，使用公开数据集，训练和测试准确率高。


<details>
  <summary>Details</summary>
Motivation: 手写字符识别应用广泛，天城文缺乏合适数字化工具，需自动化方法提取手写印地语字符以节省时间和处理陈旧数据。

Method: 使用两个深度卷积神经网络层，采用能提高识别率的方法，配置卷积神经网络用于天城文手写文本识别，利用包含36类天城文字符的公开数据集。

Result: 测试准确率达96.36%，训练准确率达99.55%。

Conclusion: 该方法在天城文手写字符识别上取得了有前景的结果。

Abstract: Handwritten character recognition is getting popular among researchers
because of its possible applications in facilitating technological search
engines, social media, recommender systems, etc. The Devanagari script is one
of the oldest language scripts in India that does not have proper digitization
tools. With the advancement of computing and technology, the task of this
research is to extract handwritten Hindi characters from an image of Devanagari
script with an automated approach to save time and obsolete data. In this
paper, we present a technique to recognize handwritten Devanagari characters
using two deep convolutional neural network layers. This work employs a
methodology that is useful to enhance the recognition rate and configures a
convolutional neural network for effective Devanagari handwritten text
recognition (DHTR). This approach uses the Devanagari handwritten character
dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each
of these classes has 1700 images for training and testing purposes. This
approach obtains promising results in terms of accuracy by achieving 96.36%
accuracy in testing and 99.55% in training time.

</details>


### [432] [Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams](https://arxiv.org/abs/2507.09640)
*Leonor Fernandes,Tiago Gonçalves,João Matos,Luis Filipe Nakayama,Jaime S. Cardoso*

Main category: cs.CV

TL;DR: 研究用mBRSET数据集评估眼底影像训练模型在糖尿病视网膜病变（DR）预测中的公平性与性能，及解缠作为偏差缓解技术的影响，发现模型预测性能高但公平性有差异，解缠效果因模型而异。


<details>
  <summary>Details</summary>
Motivation: 传统DR筛查成像成本高且难获取，AI算法虽有潜力但存在公平性和泛化性问题，需评估其公平性和性能。

Method: 使用mBRSET数据集，训练ConvNeXt V2、DINOv2和Swin V2三个模型预测DR和敏感属性，评估子组公平性，应用解缠技术减少偏差。

Result: 模型DR预测性能高（最高94% AUROC），能合理预测年龄和性别；公平性评估有差距，如DINOv2年龄组间有10% AUROC差距；解缠效果因模型而异，DINOv2性能提升，ConvNeXt V2和Swin V2下降。

Conclusion: 眼底成像中解缠细粒度特征复杂，强调医学影像AI公平性对确保公平可靠医疗解决方案的重要性。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss in working-age
adults. While screening reduces the risk of blindness, traditional imaging is
often costly and inaccessible. Artificial intelligence (AI) algorithms present
a scalable diagnostic solution, but concerns regarding fairness and
generalization persist. This work evaluates the fairness and performance of
image-trained models in DR prediction, as well as the impact of disentanglement
as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three
models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to
predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness
was assessed between subgroups of SAs, and disentanglement was applied to
reduce bias. All models achieved high DR prediction performance in diagnosing
(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%
AUROC, respectively). Fairness assessment suggests disparities, such as a 10%
AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction
had varying results, depending on the model selected. Disentanglement improved
DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2
and Swin V2 (7% and 3%, respectively). These findings highlight the complexity
of disentangling fine-grained features in fundus imaging and emphasize the
importance of fairness in medical imaging AI to ensure equitable and reliable
healthcare solutions.

</details>


### [433] [CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding](https://arxiv.org/abs/2507.10449)
*Hongyong Han,Wei Wang,Gaowei Zhang,Mingjie Li,Yi Wang*

Main category: cs.CV

TL;DR: 本文介绍首个用于珊瑚礁分析的大规模VQA数据集CoralVQA，阐述构建方法并评估模型，为未来LVLM发展及珊瑚保护奠基。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁监测需解读图像，但因专业要求高存在挑战，VQA有潜力，不过需解决特定数据集的领域注释和多维问题。

Method: 与海洋生物学家合作开发半自动化数据构建流程，构建含12,805张珊瑚图像和277,653对问答对的CoralVQA数据集。

Result: 评估多个LVLM，揭示关键局限和机会。

Conclusion: 研究为未来LVLM发展提供基础，强调对珊瑚保护工作的支持。

Abstract: Coral reefs are vital yet vulnerable ecosystems that require continuous
monitoring to support conservation. While coral reef images provide essential
information in coral monitoring, interpreting such images remains challenging
due to the need for domain expertise. Visual Question Answering (VQA), powered
by Large Vision-Language Models (LVLMs), has great potential in user-friendly
interaction with coral reef images. However, applying VQA to coral imagery
demands a dedicated dataset that addresses two key challenges: domain-specific
annotations and multidimensional questions. In this work, we introduce
CoralVQA, the first large-scale VQA dataset for coral reef analysis. It
contains 12,805 real-world coral images from 67 coral genera collected from 3
oceans, along with 277,653 question-answer pairs that comprehensively assess
ecological and health-related conditions. To construct this dataset, we develop
a semi-automatic data construction pipeline in collaboration with marine
biologists to ensure both scalability and professional-grade data quality.
CoralVQA presents novel challenges and provides a comprehensive benchmark for
studying vision-language reasoning in the context of coral reef images. By
evaluating several state-of-the-art LVLMs, we reveal key limitations and
opportunities. These insights form a foundation for future LVLM development,
with a particular emphasis on supporting coral conservation efforts.

</details>


### [434] [RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening](https://arxiv.org/abs/2507.10461)
*Tao Tang,Chengxu Yang*

Main category: cs.CV

TL;DR: 提出新的遥感图像融合网络RAPNet，采用内容自适应卷积，在公开数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: CNN在处理遥感图像融合时因卷积核统一应用，忽略局部内容变化，效果受限。

Method: 引入RAPNet，采用Receptive - field Adaptive Pansharpening Convolution产生自适应卷积核，集成Pansharpening Dynamic Feature Fusion模块结合注意力机制。

Result: 在公开数据集上的综合评估显示，RAPNet在定量和定性评估中表现优于现有方法，消融分析证明自适应组件有效。

Conclusion: RAPNet通过内容自适应卷积和动态特征融合模块，能更好地平衡空间细节增强和光谱保真度，提升遥感图像融合效果。

Abstract: Pansharpening refers to the process of integrating a high resolution
panchromatic (PAN) image with a lower resolution multispectral (MS) image to
generate a fused product, which is pivotal in remote sensing. Despite the
effectiveness of CNNs in addressing this challenge, they are inherently
constrained by the uniform application of convolutional kernels across all
spatial positions, overlooking local content variations. To overcome this
issue, we introduce RAPNet, a new architecture that leverages content-adaptive
convolution. At its core, RAPNet employs the Receptive-field Adaptive
Pansharpening Convolution (RAPConv), designed to produce spatially adaptive
kernels responsive to local feature context, thereby enhancing the precision of
spatial detail extraction. Additionally, the network integrates the
Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an
attention mechanism to achieve an optimal balance between spatial detail
enhancement and spectral fidelity. Comprehensive evaluations on publicly
available datasets confirm that RAPNet delivers superior performance compared
to existing approaches, as demonstrated by both quantitative metrics and
qualitative assessments. Ablation analyses further substantiate the
effectiveness of the proposed adaptive components.

</details>


### [435] [NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection](https://arxiv.org/abs/2507.09795)
*Amirhossein Ansari,Ke Wang,Pulei Xiong*

Main category: cs.CV

TL;DR: 提出用于零样本OOD检测的负标签细化框架NegRefine，评估效果良好，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有负标签方法检测时将分布内样本误判为OOD，且处理多标签图像有局限。

Method: 引入过滤机制排除负标签集中的子类别标签和专有名词，采用多匹配感知评分函数动态调整多标签贡献。

Result: 在包括ImageNet - 1K的大规模基准上进行评估。

Conclusion: NegRefine能更稳健地分离分布内和OOD样本。

Abstract: Recent advancements in Vision-Language Models like CLIP have enabled
zero-shot OOD detection by leveraging both image and textual label information.
Among these, negative label-based methods such as NegLabel and CSP have shown
promising results by utilizing a lexicon of words to define negative labels for
distinguishing OOD samples. However, these methods suffer from detecting
in-distribution samples as OOD due to negative labels that are subcategories of
in-distribution labels or proper nouns. They also face limitations in handling
images that match multiple in-distribution and negative labels. We propose
NegRefine, a novel negative label refinement framework for zero-shot OOD
detection. By introducing a filtering mechanism to exclude subcategory labels
and proper nouns from the negative label set and incorporating a
multi-matching-aware scoring function that dynamically adjusts the
contributions of multiple labels matching an image, NegRefine ensures a more
robust separation between in-distribution and OOD samples. We evaluate
NegRefine on large-scale benchmarks, including ImageNet-1K. Source code is
available at https://github.com/ah-ansari/NegRefine.

</details>


### [436] [Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation](https://arxiv.org/abs/2507.10474)
*Seyed Alireza Rahimi Azghadi,Truong-Thanh-Hung Nguyen,Helene Fournier,Monica Wachowicz,Rene Richard,Francis Palma,Hung Cao*

Main category: cs.CV

TL;DR: 本文提出一种多系统互补的老年人跌倒检测框架，各系统有不同准确率，组合后整体准确率达99.99%，且保护隐私。


<details>
  <summary>Details</summary>
Motivation: 老年人口增长，跌倒危险增加，及时检测可节省费用和恢复时间，需有效可靠且保护隐私的检测系统。

Method: 提出包含半监督联邦学习跌倒检测系统、室内定位导航系统和基于视觉的人体跌倒识别系统的框架。

Result: SF2D失败率0.81%，视觉检测准确率96.3%，导航系统成功率95%，组合后整体准确率99.99%。

Conclusion: 该框架对老年人安全，是保护隐私的跌倒检测解决方案。

Abstract: The aging population is growing rapidly, and so is the danger of falls in
older adults. A major cause of injury is falling, and detection in time can
greatly save medical expenses and recovery time. However, to provide timely
intervention and avoid unnecessary alarms, detection systems must be effective
and reliable while addressing privacy concerns regarding the user. In this
work, we propose a framework for detecting falls using several complementary
systems: a semi-supervised federated learning-based fall detection system
(SF2D), an indoor localization and navigation system, and a vision-based human
fall recognition system. A wearable device and an edge device identify a fall
scenario in the first system. On top of that, the second system uses an indoor
localization technique first to localize the fall location and then navigate a
robot to inspect the scenario. A vision-based detection system running on an
edge device with a mounted camera on a robot is used to recognize fallen
people. Each of the systems of this proposed framework achieves different
accuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to
99.19% accuracy, while the vision-based fallen people detection achieves 96.3%
accuracy. However, when we combine the accuracy of these two systems with the
accuracy of the navigation system (95% success rate), our proposed framework
creates a highly reliable performance for fall detection, with an overall
accuracy of 99.99%. Not only is the proposed framework safe for older adults,
but it is also a privacy-preserving solution for detecting falls.

</details>


### [437] [Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models](https://arxiv.org/abs/2507.09830)
*Shuhao Fu,Philip J. Kellman,Hongjing Lu*

Main category: cs.CV

TL;DR: 研究探讨深度学习模型与人类在3D形状物体识别中表征是否相似，通过实验对比两种模型与人类表现，发现点变换器模型更能解释人类表现。


<details>
  <summary>Details</summary>
Motivation: 虽深度学习模型在3D形状物体识别上有类人表现，但不清楚其是否有与人类视觉相似的3D形状表征。

Method: 进行两个人类实验，系统操控点密度、物体方向和局部几何结构；对比DGCNN和点变换器两种深度学习模型与人类表现。

Result: 点变换器模型比基于卷积的模型更能解释人类表现，优势源于其对3D形状的分层抽象机制。

Conclusion: 点变换器模型在模拟人类3D形状物体识别表现上更优。

Abstract: Both humans and deep learning models can recognize objects from 3D shapes
depicted with sparse visual information, such as a set of points randomly
sampled from the surfaces of 3D objects (termed a point cloud). Although deep
learning models achieve human-like performance in recognizing objects from 3D
shapes, it remains unclear whether these models develop 3D shape
representations similar to those used by human vision for object recognition.
We hypothesize that training with 3D shapes enables models to form
representations of local geometric structures in 3D shapes. However, their
representations of global 3D object shapes may be limited. We conducted two
human experiments systematically manipulating point density and object
orientation (Experiment 1), and local geometric structure (Experiment 2).
Humans consistently performed well across all experimental conditions. We
compared two types of deep learning models, one based on a convolutional neural
network (DGCNN) and the other on visual transformers (point transformer), with
human performance. We found that the point transformer model provided a better
account of human performance than the convolution-based model. The advantage
mainly results from the mechanism in the point transformer model that supports
hierarchical abstraction of 3D shapes.

</details>


### [438] [BenchReAD: A systematic benchmark for retinal anomaly detection](https://arxiv.org/abs/2507.10492)
*Chenyu Lian,Hong-Yu Zhou,Zhanli Hu,Jing Qin*

Main category: cs.CV

TL;DR: 本文指出视网膜异常检测领域缺乏综合公开基准的问题，引入综合系统的基准，对先前方法分类评估，提出NFM - DRA方法达到新SOTA并公开基准。


<details>
  <summary>Details</summary>
Motivation: 视网膜异常检测缺乏综合公开基准，现有工作存在异常类型有限、测试集饱和、缺乏泛化评估等问题，且现有医学异常检测基准多侧重单类监督方法，忽视临床常见的大量异常和未标记数据。

Method: 引入数据和算法上综合系统的视网膜异常检测基准，对先前方法分类基准测试，受单类监督学习记忆库机制启发，提出将DRA与正常特征记忆结合的NFM - DRA方法。

Result: 发现利用异常解纠缠表示的全监督方法DRA表现最佳，但遇到未见异常时性能下降；提出的NFM - DRA缓解了性能下降问题，达到新的SOTA。

Conclusion: 所提出的综合系统基准及NFM - DRA方法有助于视网膜异常检测领域的公平评估和方法推进，基准已公开。

Abstract: Retinal anomaly detection plays a pivotal role in screening ocular and
systemic diseases. Despite its significance, progress in the field has been
hindered by the absence of a comprehensive and publicly available benchmark,
which is essential for the fair evaluation and advancement of methodologies.
Due to this limitation, previous anomaly detection work related to retinal
images has been constrained by (1) a limited and overly simplistic set of
anomaly types, (2) test sets that are nearly saturated, and (3) a lack of
generalization evaluation, resulting in less convincing experimental setups.
Furthermore, existing benchmarks in medical anomaly detection predominantly
focus on one-class supervised approaches (training only with negative samples),
overlooking the vast amounts of labeled abnormal data and unlabeled data that
are commonly available in clinical practice. To bridge these gaps, we introduce
a benchmark for retinal anomaly detection, which is comprehensive and
systematic in terms of data and algorithm. Through categorizing and
benchmarking previous methods, we find that a fully supervised approach
leveraging disentangled representations of abnormalities (DRA) achieves the
best performance but suffers from significant drops in performance when
encountering certain unseen anomalies. Inspired by the memory bank mechanisms
in one-class supervised learning, we propose NFM-DRA, which integrates DRA with
a Normal Feature Memory to mitigate the performance degradation, establishing a
new SOTA. The benchmark is publicly available at
https://github.com/DopamineLcy/BenchReAD.

</details>


### [439] [Cameras as Relative Positional Encoding](https://arxiv.org/abs/2507.10496)
*Ruilong Li,Brent Yi,Junchen Liu,Hang Gao,Yi Ma,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: 本文比较多视图transformer中基于相机的调节技术，提出PRoPE编码，实验表明相对相机调节及PRoPE能提升多任务性能。


<details>
  <summary>Details</summary>
Motivation: 多视图计算机视觉任务中，需利用相机几何关系将视觉令牌置于3D空间，以提升3D感知。

Method: 比较令牌级射线图编码、注意力级相对姿态编码，提出投影位置编码（PRoPE）。

Result: 相对相机调节提升前馈新颖视图合成性能，PRoPE有进一步增益，且在不同场景、任务和模型大小下均有效。

Conclusion: 相对相机调节和PRoPE编码对多视图计算机视觉任务有积极作用。

Abstract: Transformers are increasingly prevalent for multi-view computer vision tasks,
where geometric relationships between viewpoints are critical for 3D
perception. To leverage these relationships, multi-view transformers must use
camera geometry to ground visual tokens in 3D space. In this work, we compare
techniques for conditioning transformers on cameras: token-level raymap
encodings, attention-level relative pose encodings, and a new relative encoding
we propose -- Projective Positional Encoding (PRoPE) -- that captures complete
camera frustums, both intrinsics and extrinsics, as a relative positional
encoding. Our experiments begin by showing how relative camera conditioning
improves performance in feedforward novel view synthesis, with further gains
from PRoPE. This holds across settings: scenes with both shared and varying
intrinsics, when combining token- and attention-level conditioning, and for
generalization to inputs with out-of-distribution sequence lengths and camera
intrinsics. We then verify that these benefits persist for different tasks,
stereo depth estimation and discriminative spatial cognition, as well as larger
model sizes.

</details>


### [440] [Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies](https://arxiv.org/abs/2507.10029)
*Seokeon Choi,Sunghyun Park,Hyoungwoo Park,Jeongho Kim,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出选择性优化框架实现文本到图像扩散模型内存高效个性化微调，引入时间步感知概率函数，实验证明性能优且内存消耗低。


<details>
  <summary>Details</summary>
Motivation: 实现文本到图像扩散模型的内存高效个性化，保护用户隐私并适应边缘设备有限计算资源。

Method: 提出选择性优化框架，结合低分辨率图像反向传播（BP - low）和高分辨率图像零阶优化（ZO - high），引入时间步感知概率函数动态选择优化策略。

Result: 方法性能有竞争力，显著降低内存消耗，不增加推理延迟。

Conclusion: 该方法能实现可扩展、高质量的设备端个性化微调。

Abstract: Memory-efficient personalization is critical for adapting text-to-image
diffusion models while preserving user privacy and operating within the limited
computational resources of edge devices. To this end, we propose a selective
optimization framework that adaptively chooses between backpropagation on
low-resolution images (BP-low) and zeroth-order optimization on high-resolution
images (ZO-high), guided by the characteristics of the diffusion process. As
observed in our experiments, BP-low efficiently adapts the model to
target-specific features, but suffers from structural distortions due to
resolution mismatch. Conversely, ZO-high refines high-resolution details with
minimal memory overhead but faces slow convergence when applied without prior
adaptation. By complementing both methods, our framework leverages BP-low for
effective personalization while using ZO-high to maintain structural
consistency, achieving memory-efficient and high-quality fine-tuning. To
maximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware
probabilistic function that dynamically selects the appropriate optimization
strategy based on diffusion timesteps. This function mitigates the overfitting
from BP-low at high timesteps, where structural information is critical, while
ensuring ZO-high is applied more effectively as training progresses.
Experimental results demonstrate that our method achieves competitive
performance while significantly reducing memory consumption, enabling scalable,
high-quality on-device personalization without increasing inference latency.

</details>


### [441] [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548)
*Mingxian Lin,Wei Huang,Yitang Li,Chengjie Jiang,Kui Wu,Fangwei Zhong,Shengju Qian,Xin Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 现有视觉语言模型在具身场景表现有限，本文引入EmRACE - 3K数据集评估其具身推理能力，零样本下模型成功率低，用该数据集微调模型有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在具身场景（需要在线交互和主动场景理解）的有效性有限，如在开放环境交互中有空间推理和长期规划方面的局限。

Method: 引入包含3000多个语言引导任务的EmRACE - 3K数据集，建立基准从探索、动态空间语义推理和多阶段目标执行三个维度评估模型；用监督学习和强化学习对Qwen2.5 - VL - 7B进行微调。

Result: 零样本设置下，所有模型成功率低于20%；用EmRACE - 3K微调Qwen2.5 - VL - 7B后，在三个挑战类别中均有显著提升。

Conclusion: EmRACE - 3K数据集能有效推动视觉语言模型具身推理能力的发展。

Abstract: Recent advanced vision-language models(VLMs) have demonstrated strong
performance on passive, offline image and video understanding tasks. However,
their effectiveness in embodied settings, which require online interaction and
active scene understanding remains limited. In such scenarios, an agent
perceives the environment from a first-person perspective, with each action
dynamically shaping subsequent observations. Even state-of-the-art models such
as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment
interactions, exhibiting clear limitations in spatial reasoning and
long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset
of over 3,000 language-guided tasks situated in diverse, photorealistic
environments constructed using Unreal Engine and the UnrealCV-Zoo framework.
The tasks encompass a wide range of embodied challenges, including navigation,
object manipulation, and multi-stage goal execution. Each task unfolds as a
multi-step trajectory, pairing first-person visual observations with high-level
instructions, grounded actions, and natural language rationales that express
the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to
evaluate the embodied reasoning capabilities of VLMs across three key
dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage
Goal Execution. In zero-shot settings, all models achieve success rates below
20%, underscoring the challenge posed by our benchmark and the current
limitations of VLMs in interactive environments. To demonstrate the utility of
EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning
followed by reinforcement learning. This approach yields substantial
improvements across all three challenge categories, highlighting the dataset's
effectiveness in enabling the development of embodied reasoning capabilities.

</details>


### [442] [Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder](https://arxiv.org/abs/2507.10552)
*Vladimir Iashin,Horace Lee,Dan Schofield,Andrew Zisserman*

Main category: cs.CV

TL;DR: 本文提出用自监督方法从无标签相机陷阱镜头中学习黑猩猩面部嵌入，表现超有监督基线，凸显自监督学习在生物多样性监测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱虽能捕捉大量视觉数据，但手动识别个体动物是瓶颈，需新方法解决。

Method: 利用DINOv2框架，在自动挖掘的面部裁剪图像上训练视觉Transformer，无需身份标签。

Result: 方法在具有挑战性的基准测试中展现出强大的开放集重新识别性能，超越有监督基线。

Conclusion: 自监督学习在生物多样性监测中有潜力，为可扩展、非侵入性种群研究铺平道路。

Abstract: Camera traps are revolutionising wildlife monitoring by capturing vast
amounts of visual data; however, the manual identification of individual
animals remains a significant bottleneck. This study introduces a fully
self-supervised approach to learning robust chimpanzee face embeddings from
unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision
Transformers on automatically mined face crops, eliminating the need for
identity labels. Our method demonstrates strong open-set re-identification
performance, surpassing supervised baselines on challenging benchmarks such as
Bossou, despite utilising no labelled data during training. This work
underscores the potential of self-supervised learning in biodiversity
monitoring and paves the way for scalable, non-invasive population studies.

</details>


### [443] [A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area](https://arxiv.org/abs/2507.10084)
*Haonan Chen,Xin Tong*

Main category: cs.CV

TL;DR: 提出基于SegFormer的两阶段迁移学习策略解决遥感影像水体分割中领域偏移和小样本问题，在西藏扎达土林地区实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 解决遥感影像水体分割中普遍存在的领域偏移和小样本问题。

Method: 基于SegFormer模型，先在源领域训练基础分割模型，再在目标领域微调。

Result: 在源领域验证集IoU达68.80%，在扎达土林地区将水体分割IoU从25.50%提升到64.84%。

Conclusion: 有效解决领域差异导致的模型性能下降，为数据稀缺和环境独特的遥感场景高精度专题信息提取提供有效技术范式。

Abstract: To address the prevalent challenges of domain shift and small sample sizes in
remote sensing image water body segmentation, this study proposes and validates
a two-stage transfer learning strategy based on the SegFormer model. The
approach begins by training a foundational segmentation model on a diverse
source domain, where it achieves an Intersection over Union (IoU) of 68.80% on
its validation set, followed by fine-tuning on data from the distinct target
domain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by
highly complex topography and spectral features -- the experimental results
demonstrate that this strategy significantly boosts the IoU for the water body
segmentation task from 25.50% (for direct transfer) to 64.84%. This not only
effectively resolves the model performance degradation caused by domain
discrepancy but also provides an effective technical paradigm for
high-precision thematic information extraction in data-scarce and
environmentally unique remote sensing scenarios.

</details>


### [444] [Deep Recurrence for Dynamical Segmentation Models](https://arxiv.org/abs/2507.10143)
*David Calhas,Arlindo L. Oliveira*

Main category: cs.CV

TL;DR: 提出预测编码启发的反馈机制，在U - Net架构中实现，实验表明反馈模型在噪声条件和有限监督下表现更优，增强了鲁棒性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 生物视觉系统依赖反馈连接迭代优化感知，而大多数人工神经网络是前馈的，因此提出反馈机制改进。

Method: 在标准U - Net架构中实现反馈机制，引入软最大投影和指数衰减操作确保反馈环稳定性。

Result: 反馈模型在噪声条件下显著优于前馈模型，有限监督下泛化能力更强，仅两个训练样本就能优于随机表现，而前馈模型至少需四个。

Conclusion: 反馈增强了鲁棒性和数据效率，为构建更具适应性和生物启发的神经架构提供了途径。

Abstract: While biological vision systems rely heavily on feedback connections to
iteratively refine perception, most artificial neural networks remain purely
feedforward, processing input in a single static pass. In this work, we propose
a predictive coding inspired feedback mechanism that introduces a recurrent
loop from output to input, allowing the model to refine its internal state over
time. We implement this mechanism within a standard U-Net architecture and
introduce two biologically motivated operations, softmax projection and
exponential decay, to ensure stability of the feedback loop. Through controlled
experiments on a synthetic segmentation task, we show that the feedback model
significantly outperforms its feedforward counterpart in noisy conditions and
generalizes more effectively with limited supervision. Notably, feedback
achieves above random performance with just two training examples, while the
feedforward model requires at least four. Our findings demonstrate that
feedback enhances robustness and data efficiency, and offer a path toward more
adaptive and biologically inspired neural architectures. Code is available at:
github.com/DCalhas/feedback_segmentation.

</details>


### [445] [Spatial Lifting for Dense Prediction](https://arxiv.org/abs/2507.10222)
*Mingzhi Xu,Yizhe Zhang*

Main category: cs.CV

TL;DR: 提出Spatial Lifting (SL)方法用于密集预测任务，可提升性能、降低推理成本和模型参数数量。


<details>
  <summary>Details</summary>
Motivation: 寻找更高效、准确和可靠的深度网络用于视觉密集预测任务。

Method: 将标准输入（如2D图像）提升到高维空间，使用高维网络（如3D U - Net）处理。

Result: 在19个基准数据集上验证，降低U - Net模型参数超98%，降低推理成本，展现出有竞争力的密集预测性能。

Conclusion: Spatial Lifting为密集预测任务引入了新的视觉建模范式，有广阔前景。

Abstract: We present Spatial Lifting (SL), a novel methodology for dense prediction
tasks. SL operates by lifting standard inputs, such as 2D images, into a
higher-dimensional space and subsequently processing them using networks
designed for that higher dimension, such as a 3D U-Net. Counterintuitively,
this dimensionality lifting allows us to achieve good performance on benchmark
tasks compared to conventional approaches, while reducing inference costs and
significantly lowering the number of model parameters. The SL framework
produces intrinsically structured outputs along the lifted dimension. This
emergent structure facilitates dense supervision during training and enables
robust, near-zero-additional-cost prediction quality assessment at test time.
We validate our approach across 19 benchmark datasets (13 for semantic
segmentation and 6 for depth estimation), demonstrating competitive dense
prediction performance while reducing the model parameter count by over 98% (in
the U-Net case) and lowering inference costs. Spatial Lifting introduces a new
vision modeling paradigm that offers a promising path toward more efficient,
accurate, and reliable deep networks for dense prediction tasks in vision.

</details>


### [446] [Test-Time Canonicalization by Foundation Models for Robust Perception](https://arxiv.org/abs/2507.10375)
*Utkarsh Singhal,Ryan Feng,Stella X. Yu,Atul Prakash*

Main category: cs.CV

TL;DR: 提出FOCAL框架，利用基础模型的视觉先验在测试时实现鲁棒感知，实验证明能提升CLIP和SAM的鲁棒性，代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前视觉感知方法依赖特殊架构或预定义增强训练，泛化性受限。

Method: 提出FOCAL框架，在测试时生成并优化候选变换以达到典型视图，无需重新训练或改变架构。

Result: 实验表明提升了CLIP和SAM在多种变换下的鲁棒性，如2D/3D旋转、光照变化等。

Conclusion: 挑战了特定变换训练的必要性，提供了实现不变性的可扩展途径。

Abstract: Real-world visual perception requires invariance to diverse transformations,
yet current methods rely heavily on specialized architectures or training on
predefined augmentations, limiting generalization. We propose FOCAL, a
test-time, data-driven framework that achieves robust perception by leveraging
internet-scale visual priors from foundation models. By generating and
optimizing candidate transformations toward visually typical, "canonical"
views, FOCAL enhances robustness without re-training or architectural changes.
Our experiments demonstrate improved robustness of CLIP and SAM across
challenging transformations, including 2D/3D rotations, illumination shifts
(contrast and color), and day-night variations. We also highlight potential
applications in active vision. Our approach challenges the assumption that
transform-specific training is necessary, instead offering a scalable path to
invariance. Our code is available at: https://github.com/sutkarsh/focal.

</details>


### [447] [Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks](https://arxiv.org/abs/2507.10381)
*Aaryam Sharma*

Main category: cs.CV

TL;DR: 本文提出TDA特征工程管道及将拓扑特征与深度学习模型集成的方法，用于遥感分类，在两个数据集上提升了模型性能，展示了TDA特征与深度学习模型结合的潜力。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络在遥感分类中倾向于基于纹理的局部特征，存在局限性，需利用TDA有效描述复杂数据集的能力解决该问题。

Method: 提出TDA特征工程管道，并使用简单方法将拓扑特征与深度学习模型集成。

Result: 在EuroSAT数据集上使ResNet18模型性能提升1.44%，达到99.33%的准确率，超过所有之前报告的单模型准确率；在RESISC45数据集上准确率比ResNet18基线高1.82%。

Conclusion: TDA特征可与深度学习模型集成，即使在无明确拓扑结构的数据集上也能增加TDA的适用性。

Abstract: Topological data analysis (TDA) is a relatively new field that is gaining
rapid adoption due to its robustness and ability to effectively describe
complex datasets by quantifying geometric information. In imaging contexts, TDA
typically models data as filtered cubical complexes from which we can extract
discriminative features using persistence homology. Meanwhile, convolutional
neural networks (CNNs) have been shown to be biased towards texture based local
features. To address this limitation, we propose a TDA feature engineering
pipeline and a simple method to integrate topological features with deep
learning models on remote sensing classification. Our method improves the
performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving
99.33% accuracy, which surpasses all previously reported single-model
accuracies, including those with larger architectures, such as ResNet50 (2x
larger) and XL Vision Transformers (197x larger). We additionally show that our
method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45
dataset. To our knowledge, this is the first application of TDA features in
satellite scene classification with deep learning. This demonstrates that TDA
features can be integrated with deep learning models, even on datasets without
explicit topological structures, thereby increasing the applicability of TDA. A
clean implementation of our method will be made publicly available upon
publication.

</details>


### [448] [National level satellite-based crop field inventories in smallholder landscapes](https://arxiv.org/abs/2507.10499)
*Philippe Rufin,Pauline Lucie Hammer,Leon-Friedrich Thomas,Sá Nogueira Lisboa,Natasha Ribeiro,Almeida Sitoe,Patrick Hostert,Patrick Meyfroidt*

Main category: cs.CV

TL;DR: 结合高分辨率地球观测数据和深度迁移学习，为莫桑比克绘制全国农田地图，分析农田分布、规模等特征，指出田块规模是农业社会经济和环境结果的关键指标。


<details>
  <summary>Details</summary>
Motivation: 因对基本系统属性了解有限，设计提升小农户农业可持续性的科学政策面临挑战，需获取相关信息。

Method: 整合高空间分辨率（1.5米）地球观测数据和深度迁移学习，推导复杂农业系统中国家层面的农田边界。

Result: 提供莫桑比克2023年2100万个农田的国家级数据集，地图区分农田和非农业用地总体精度达93%，田块空间一致性中位数IoU分数为0.81，揭示农田分布和规模的特征及变化。

Conclusion: 田块规模是与农业社会经济和环境结果及其权衡相关的关键指标。

Abstract: The design of science-based policies to improve the sustainability of
smallholder agriculture is challenged by a limited understanding of fundamental
system properties, such as the spatial distribution of active cropland and
field size. We integrate very high spatial resolution (1.5 m) Earth observation
data and deep transfer learning to derive crop field delineations in complex
agricultural systems at the national scale, while maintaining minimum reference
data requirements and enhancing transferability. We provide the first
national-level dataset of 21 million individual fields for Mozambique (covering
~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural
land use with an overall accuracy of 93% and balanced omission and commission
errors. Field-level spatial agreement reached median intersection over union
(IoU) scores of 0.81, advancing the state-of-the-art in large-area field
delineation in complex smallholder systems. The active cropland maps capture
fragmented rural regions with low cropland shares not yet identified in global
land cover or cropland maps. These regions are mostly located in agricultural
frontier regions which host 7-9% of the Mozambican population. Field size in
Mozambique is very low overall, with half of the fields being smaller than 0.16
ha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial
resolution (0.05{\deg}) is 0.32 ha, but it varies strongly across gradients of
accessibility, population density, and net forest cover change. This variation
reflects a diverse set of actors, ranging from semi-subsistence smallholder
farms to medium-scale commercial farming, and large-scale farming operations.
Our results highlight that field size is a key indicator relating to
socio-economic and environmental outcomes of agriculture (e.g., food
production, livelihoods, deforestation, biodiversity), as well as their
trade-offs.

</details>


### [449] [Quantize-then-Rectify: Efficient VQ-VAE Training](https://arxiv.org/abs/2507.10547)
*Borui Zhang,Qihang Rao,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出ReVQ框架，利用预训练VAE快速训练VQ - VAE，减少训练成本，实现高效重建权衡。


<details>
  <summary>Details</summary>
Motivation: 解决训练高压缩率VQ - VAEs计算需求大、耗时久的问题。

Method: 提出Quantize - then - Rectify (ReVQ)框架，集成通道多组量化和后置校正器。

Result: 将ImageNet图像压缩至最多512个令牌，重建质量有竞争力（rFID = 1.06），训练成本降低超两个数量级，单张NVIDIA 4090约22小时完成训练。

Conclusion: ReVQ实现了更优的效率 - 重建权衡。

Abstract: Visual tokenizers are pivotal in multimodal large models, acting as bridges
between continuous inputs and discrete tokens. Nevertheless, training
high-compression-rate VQ-VAEs remains computationally demanding, often
necessitating thousands of GPU hours. This work demonstrates that a pre-trained
VAE can be efficiently transformed into a VQ-VAE by controlling quantization
noise within the VAE's tolerance threshold. We present
\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs
to enable rapid VQ-VAE training with minimal computational overhead. By
integrating \textbf{channel multi-group quantization} to enlarge codebook
capacity and a \textbf{post rectifier} to mitigate quantization errors, ReVQ
compresses ImageNet images into at most 512 tokens while sustaining competitive
reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training
costs by over two orders of magnitude relative to state-of-the-art approaches:
ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,
whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental
results show that ReVQ achieves superior efficiency-reconstruction trade-offs.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [450] [Investigating the Robustness of Extreme Precipitation Super-Resolution Across Climates](https://arxiv.org/abs/2507.09166)
*Louise Largeau,Erwan Koch,David Leutwyler,Gregoire Mariethoz,Valerie Chavez-Demoulin,Tom Beucler*

Main category: physics.ao-ph

TL;DR: 提出用解析映射超分辨率处理目标变量概率分布参数的方法，在瑞士完美模型框架下验证，引入'鲁棒性差距'概念诊断模型泛化能力，还确定超分辨率因子上限。


<details>
  <summary>Details</summary>
Motivation: 网格气候模型空间分辨率粗，多数降尺度方法难评估分布变化下鲁棒性，需更好理解和提升鲁棒性。

Method: 用解析映射直接超分辨率处理目标变量概率分布参数，采用向量广义线性和加法模型，引入'鲁棒性差距'概念，评估多种模型配置。

Result: 向量广义线性和加法模型能从粗降水场和地形超分辨率处理夏季小时降水极值的广义极值分布；确定基于降水和海拔空间自相关和互相关的超分辨率因子上限。

Conclusion: 该框架广泛适用于参数分布变量，提供模型无关诊断，帮助理解经验降尺度何时及为何能推广到气候变化和极端情况。

Abstract: The coarse spatial resolution of gridded climate models, such as general
circulation models, limits their direct use in projecting socially relevant
variables like extreme precipitation. Most downscaling methods estimate the
conditional distributions of extremes by generating large ensembles,
complicating the assessment of robustness under distributional shifts, such as
those induced by climate change. To better understand and potentially improve
robustness, we propose super-resolving the parameters of the target variable's
probability distribution directly using analytically tractable mappings. Within
a perfect-model framework over Switzerland, we demonstrate that vector
generalized linear and additive models can super-resolve the generalized
extreme value distribution of summer hourly precipitation extremes from coarse
precipitation fields and topography. We introduce the notion of a "robustness
gap", defined as the difference in predictive error between present-trained and
future-trained models, and use it to diagnose how model structure affects the
generalization of each quantile to a pseudo-global warming scenario. By
evaluating multiple model configurations, we also identify an upper limit on
the super-resolution factor based on the spatial auto- and cross-correlation of
precipitation and elevation, beyond which coarse precipitation loses predictive
value. Our framework is broadly applicable to variables governed by parametric
distributions and offers a model-agnostic diagnostic for understanding when and
why empirical downscaling generalizes to climate change and extremes.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [451] [Physics-informed neural networks for high-dimensional solutions and snaking bifurcations in nonlinear lattices](https://arxiv.org/abs/2507.09782)
*Muhammad Luthfi Shahab,Fidya Almira Suheri,Rudy Kusdiantara,Hadi Susanto*

Main category: math.NA

TL;DR: 本文提出基于物理信息神经网络（PINNs）的框架解决非线性晶格关键挑战，实验表明该方法在高维表现佳，凸显神经网络潜力。


<details>
  <summary>Details</summary>
Motivation: 解决非线性晶格中的解近似、分岔图构建和线性稳定性分析等关键挑战。

Method: 用PINNs近似晶格模型非线性系统的解，用Levenberg - Marquardt算法优化网络权重，结合随机采样策略；耦合PINNs与延续法计算分岔图；调整PINNs计算特征向量并引入输出约束。

Result: 在一到五维离散Allen - Cahn方程上实验，该方法精度与传统方法相当或更好，在高维优势明显。

Conclusion: 神经网络可作为研究复杂非线性晶格系统的可扩展且高效工具。

Abstract: This paper introduces a framework based on physics-informed neural networks
(PINNs) for addressing key challenges in nonlinear lattices, including solution
approximation, bifurcation diagram construction, and linear stability analysis.
We first employ PINNs to approximate solutions of nonlinear systems arising
from lattice models, using the Levenberg-Marquardt algorithm to optimize
network weights for greater accuracy. To enhance computational efficiency in
high-dimensional settings, we integrate a stochastic sampling strategy. We then
extend the method by coupling PINNs with a continuation approach to compute
snaking bifurcation diagrams, incorporating an auxiliary equation to
effectively track successive solution branches. For linear stability analysis,
we adapt PINNs to compute eigenvectors, introducing output constraints to
enforce positivity, in line with Sturm-Liouville theory. Numerical experiments
are conducted on the discrete Allen-Cahn equation with cubic and quintic
nonlinearities in one to five spatial dimensions. The results demonstrate that
the proposed approach achieves accuracy comparable to, or better than,
traditional numerical methods, especially in high-dimensional regimes where
computational resources are a limiting factor. These findings highlight the
potential of neural networks as scalable and efficient tools for the study of
complex nonlinear lattice systems.

</details>


### [452] [Energy Dissipation Rate Guided Adaptive Sampling for Physics-Informed Neural Networks: Resolving Surface-Bulk Dynamics in Allen-Cahn Systems](https://arxiv.org/abs/2507.09757)
*Chunyan Li,Wenkai Yu,Qi Wang*

Main category: math.NA

TL;DR: 提出EDRAS策略提升PINNs求解热力学一致PDEs性能，以Allen - Cahn模型验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 提升Physics - Informed Neural Networks (PINNs)在求解热力学一致偏微分方程（PDEs）时的性能。

Method: 引入Energy Dissipation Rate guided Adaptive Sampling (EDRAS)策略，利用局部能量耗散率密度作为指导指标，自适应地重新采样关键配点。

Result: 在不规则几何的Allen - Cahn相场模型中，相对均方误差相比传统方法最多降低六倍，计算更高效，更易识别高影响配点。

Conclusion: EDRAS为PINN框架提供了有效的、基于物理信息的增强，使PINN成为研究任意几何中复杂热力学过程的强大工具。

Abstract: We introduce the Energy Dissipation Rate guided Adaptive Sampling (EDRAS)
strategy, a novel method that substantially enhances the performance of
Physics-Informed Neural Networks (PINNs) in solving thermodynamically
consistent partial differential equations (PDEs) over arbitrary domains. EDRAS
leverages the local energy dissipation rate density as a guiding metric to
identify and adaptively re-sample critical collocation points from both the
interior and boundary of the computational domain. This dynamical sampling
approach improves the accuracy of residual-based PINNs by aligning the training
process with the underlying physical structure of the system. In this study, we
demonstrate the effectiveness of EDRAS using the Allen-Cahn phase field model
in irregular geometries, achieving up to a sixfold reduction in the relative
mean square error compared to traditional residual-based adaptive refinement
(RAR) methods. Moreover, we compare EDRAS with other residual-based adaptive
sampling approaches and show that EDRAS is not only computationally more
efficient but also more likely to identify high-impact collocation points.
Through numerical solutions of the Allen-Cahn equation with both static
(Neumann) and dynamic boundary conditions in 2D disk- and ellipse-shaped
domains solved using PINN coupled with EDRAS, we gain significant insights into
how dynamic boundary conditions influence bulk phase evolution and
thermodynamic behavior. The proposed approach offers an effective, physically
informed enhancement to PINN frameworks for solving thermodynamically
consistent models, making PINN a robust and versatile computational tool for
investigating complex thermodynamic processes in arbitrary geometries.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [453] [Evolution of Fear and Social Rewards in Prey-Predator Relationship](https://arxiv.org/abs/2507.09992)
*Yuji Kanagawa,Kenji Doya*

Main category: q-bio.PE

TL;DR: 本文通过分布式进化模拟研究恐惧、社会奖励等的进化关系，发现社会奖励对猎物生存更重要，恐惧奖励在获得社会奖励后进化，不同捕食者对恐惧进化有不同影响。


<details>
  <summary>Details</summary>
Motivation: 探究环境条件、恐惧进化与其他奖励（如食物奖励、社会奖励）进化之间的关系。

Method: 开发分布式进化模拟，让猎物和捕食者代理共同进化其先天奖励函数，并通过强化学习学习行为。

Result: 社会奖励对猎物生存更重要，恐惧奖励在获得社会奖励后进化；捕食者狩猎能力增强会促进恐惧出现，非进化且不擅追逐的捕食者使恐惧进化更稳定；对于静止威胁，积极奖励与恐惧呈相反进化。

Conclusion: 恐惧和社会奖励在进化过程中相互作用复杂，且与捕食者和威胁的性质有关。

Abstract: Fear is a critical brain function for detecting danger and learning to avoid
specific stimuli that can lead to danger. While fear is believed to have
evolved under pressure from predators, experimentally reproducing the evolution
is challenging. To investigate the relationship between environmental
conditions, the evolution of fear, and the evolution of other rewards, such as
food reward and social reward, we developed a distributed evolutionary
simulation. In our simulation, prey and predator agents co-evolve their innate
reward functions, including a possibly fear-like term for observing predators,
and learn behaviors via reinforcement learning. Surprisingly, our simulation
revealed that social reward for observing the same species is more important
for prey to survive, and fear-like negative reward for observing predators
evolves only after acquiring social reward. We also found that the predator
with increased hunting ability (larger mouth) amplified fear emergence, but
also that fear evolution is more stable with non-evolving predators that are
bad at chasing prey. Additionally, unlike for predators, we found that positive
rewards evolve in opposition to fear for stationary threats, as areas with
abundant leftover food develop around them. These findings suggest that fear
and social reward have had a complex interplay with each other through
evolution, along with the nature of predators and threats.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [454] [AirScape: An Aerial Generative World Model with Motion Controllability](https://arxiv.org/abs/2507.08885)
*Baining Zhao,Rongze Tang,Mingyuan Jia,Ziyou Wang,Fanghang Man,Xin Zhang,Yu Shang,Weichen Zhang,Chen Gao,Wei Wu,Xin Wang,Xinlei Chen,Yong Li*

Main category: cs.RO

TL;DR: 提出用于六自由度空中智能体的世界模型AirScape，构建数据集并采用两阶段训练方案。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在三维空间预测自身运动意图结果的问题，探索更通用的空间想象能力。

Method: 构建包含11k视频 - 意图对的数据集，采用两阶段训练方案将基础模型训练成可由运动意图控制且符合物理时空约束的世界模型。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论。

Abstract: How to enable robots to predict the outcomes of their own motion intentions
in three-dimensional space has been a fundamental problem in embodied
intelligence. To explore more general spatial imagination capabilities, here we
present AirScape, the first world model designed for six-degree-of-freedom
aerial agents. AirScape predicts future observation sequences based on current
visual inputs and motion intentions. Specifically, we construct an dataset for
aerial world model training and testing, which consists of 11k video-intention
pairs. This dataset includes first-person-view videos capturing diverse drone
actions across a wide range of scenarios, with over 1,000 hours spent
annotating the corresponding motion intentions. Then we develop a two-phase
training schedule to train a foundation model -- initially devoid of embodied
spatial knowledge -- into a world model that is controllable by motion
intentions and adheres to physical spatio-temporal constraints.

</details>


### [455] [Towards Human-level Dexterity via Robot Learning](https://arxiv.org/abs/2507.09117)
*Gagan Khandate*

Main category: cs.RO

TL;DR: 本文指出实现机器人手类人灵巧性面临计算传感运动学习的局限，开发了克服这些局限的机器人学习方法，包括强化学习框架和基于视觉触觉人类演示的模仿学习技术。


<details>
  <summary>Details</summary>
Motivation: 实现机器人手类人灵巧性是机器人领域的基本目标和迈向通用具身智能的关键里程碑，但计算传感运动学习存在局限，需克服以实现更高水平的灵巧性。

Method: 通过关键研究构建用于多手指灵巧操作技能强化学习的有效框架，采用结构化探索克服随机探索的局限，引入基于采样规划的直接探索；探索使用视觉触觉人类演示的新范式并引入相应的模仿学习技术。

Result: 开发出针对多手指灵巧操作的机器人学习方法。

Conclusion: 所开发的方法能有效克服计算传感运动学习的局限，推动机器人手实现更高水平的灵巧性。

Abstract: Dexterous intelligence -- the ability to perform complex interactions with
multi-fingered hands -- is a pinnacle of human physical intelligence and
emergent higher-order cognitive skills. However, contrary to Moravec's paradox,
dexterous intelligence in humans appears simple only superficially. Many
million years were spent co-evolving the human brain and hands including rich
tactile sensing. Achieving human-level dexterity with robotic hands has long
been a fundamental goal in robotics and represents a critical milestone toward
general embodied intelligence. In this pursuit, computational sensorimotor
learning has made significant progress, enabling feats such as arbitrary
in-hand object reorientation. However, we observe that achieving higher levels
of dexterity requires overcoming very fundamental limitations of computational
sensorimotor learning.
  I develop robot learning methods for highly dexterous multi-fingered
manipulation by directly addressing these limitations at their root cause.
Chiefly, through key studies, this disseration progressively builds an
effective framework for reinforcement learning of dexterous multi-fingered
manipulation skills. These methods adopt structured exploration, effectively
overcoming the limitations of random exploration in reinforcement learning. The
insights gained culminate in a highly effective reinforcement learning that
incorporates sampling-based planning for direct exploration. Additionally, this
thesis explores a new paradigm of using visuo-tactile human demonstrations for
dexterity, introducing corresponding imitation learning techniques.

</details>


### [456] [On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks](https://arxiv.org/abs/2507.09538)
*Zainab Ali,Lujayn Al-Amir,Ali Safa*

Main category: cs.RO

TL;DR: 本文研究用SNN基于激光雷达数据实现机器人导航与避障，探讨神经元膜泄漏对精度的影响，调整参数可使控制精度与CNN相当，并开源数据集。


<details>
  <summary>Details</summary>
Motivation: SNN在神经形态硬件中高精度、低内存和计算复杂度的推理能力使其适用于资源受限的自主机器人应用，因此研究用SNN进行机器人导航和避障。

Method: 搭建带激光雷达的机器人平台收集标记数据集，研究神经元膜泄漏对SNN处理激光雷达数据精度的影响，调整LIF神经元膜电位泄漏常数。

Result: 通过调整LIF神经元膜电位泄漏常数，SNN的机器人控制精度可与非脉冲CNN相当。

Conclusion: 调整SNN中LIF神经元的膜电位泄漏常数可提高机器人控制精度，开源数据集有望促进未来研究。

Abstract: Using neuromorphic computing for robotics applications has gained much
attention in recent year due to the remarkable ability of Spiking Neural
Networks (SNNs) for high-precision yet low memory and compute complexity
inference when implemented in neuromorphic hardware. This ability makes SNNs
well-suited for autonomous robot applications (such as in drones and rovers)
where battery resources and payload are typically limited. Within this context,
this paper studies the use of SNNs for performing direct robot navigation and
obstacle avoidance from LIDAR data. A custom robot platform equipped with a
LIDAR is set up for collecting a labeled dataset of LIDAR sensing data together
with the human-operated robot control commands used for obstacle avoidance.
Crucially, this paper provides what is, to the best of our knowledge, a first
focused study about the importance of neuron membrane leakage on the SNN
precision when processing LIDAR data for obstacle avoidance. It is shown that
by carefully tuning the membrane potential leakage constant of the spiking
Leaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to
achieve on-par robot control precision compared to the use of a non-spiking
Convolutional Neural Network (CNN). Finally, the LIDAR dataset collected during
this work is released as open-source with the hope of benefiting future
research.

</details>


### [457] [Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks](https://arxiv.org/abs/2507.09725)
*Gabriel G. Gattaux,Julien R. Serres,Franck Ruffier,Antoine Wystrach*

Main category: cs.RO

TL;DR: 本文提出基于侧向化蘑菇体架构的视觉归巢系统，在紧凑类汽车机器人上实现，经实验验证有效，是资源高效的仿生解决方案。


<details>
  <summary>Details</summary>
Motivation: 蚂蚁视觉归巢能力启发仿生自主导航，现有蘑菇体模型未用于视觉归巢，研究其在视觉归巢中的应用。

Method: 在紧凑类汽车机器人上实现侧向化蘑菇体架构用于视觉归巢，通过角路径积分信号正负对全景视图分类，开展四个递进实验验证。

Result: 实验表明该方法能实现类似蚂蚁的精确归巢行为，系统运行频率8Hz，内存占用小于9kB。

Conclusion: 提出的系统是基于生物原理、资源高效的自主视觉归巢解决方案。

Abstract: Ants achieve robust visual homing with minimal sensory input and only a few
learning walks, inspiring biomimetic solutions for autonomous navigation. While
Mushroom Body (MB) models have been used in robotic route following, they have
not yet been applied to visual homing. We present the first real-world
implementation of a lateralized MB architecture for visual homing onboard a
compact autonomous car-like robot. We test whether the sign of the angular path
integration (PI) signal can categorize panoramic views, acquired during
learning walks and encoded in the MB, into "goal on the left" and "goal on the
right" memory banks, enabling robust homing in natural outdoor settings. We
validate this approach through four incremental experiments: (1) simulation
showing attractor-like nest dynamics; (2) real-world homing after decoupled
learning walks, producing nest search behavior; (3) homing after random walks
using noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal
behavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to
control velocity. This mimics the accurate homing behavior of ants and
functionally resembles waypoint-based position control in robotics, despite
relying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with
32x32 pixel views and a memory footprint under 9 kB, our system offers a
biologically grounded, resource-efficient solution for autonomous visual
homing.

</details>


### [458] [Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems](https://arxiv.org/abs/2507.09836)
*Vindula Jayawardana,Sirui Li,Yashar Farid,Cathy Wu*

Main category: cs.RO

TL;DR: 介绍用于拉格朗日交通控制的新框架MRMEL，在多地案例研究中验证其能减少车辆排放。


<details>
  <summary>Details</summary>
Motivation: 设计能在不同交通场景通用的自动驾驶车辆拉格朗日交通控制策略面临挑战，现实交通环境多样，交通系统复杂。

Method: 引入Multi - Residual Mixture of Expert Learning (MRMEL)框架，借鉴残差强化学习，学习残差修正，动态选择合适的标称策略。

Result: 在亚特兰大、达拉斯沃思堡和盐湖城的案例研究中，MRMEL相比最强基线设置能额外减少4% - 9%的车辆总排放量。

Conclusion: MRMEL在拉格朗日交通控制中表现出色，能有效减少车辆排放。

Abstract: Autonomous vehicles (AVs) are becoming increasingly popular, with their
applications now extending beyond just a mode of transportation to serving as
mobile actuators of a traffic flow to control flow dynamics. This contrasts
with traditional fixed-location actuators, such as traffic signals, and is
referred to as Lagrangian traffic control. However, designing effective
Lagrangian traffic control policies for AVs that generalize across traffic
scenarios introduces a major challenge. Real-world traffic environments are
highly diverse, and developing policies that perform robustly across such
diverse traffic scenarios is challenging. It is further compounded by the joint
complexity of the multi-agent nature of traffic systems, mixed motives among
participants, and conflicting optimization objectives subject to strict
physical and external constraints. To address these challenges, we introduce
Multi-Residual Mixture of Expert Learning (MRMEL), a novel framework for
Lagrangian traffic control that augments a given suboptimal nominal policy with
a learned residual while explicitly accounting for the structure of the traffic
scenario space. In particular, taking inspiration from residual reinforcement
learning, MRMEL augments a suboptimal nominal AV control policy by learning a
residual correction, but at the same time dynamically selects the most suitable
nominal policy from a pool of nominal policies conditioned on the traffic
scenarios and modeled as a mixture of experts. We validate MRMEL using a case
study in cooperative eco-driving at signalized intersections in Atlanta, Dallas
Fort Worth, and Salt Lake City, with real-world data-driven traffic scenarios.
The results show that MRMEL consistently yields superior performance-achieving
an additional 4%-9% reduction in aggregate vehicle emissions relative to the
strongest baseline in each setting.

</details>


### [459] [Demonstrating the Octopi-1.5 Visual-Tactile-Language Model](https://arxiv.org/abs/2507.09985)
*Samson Yu,Kelvin Lin,Harold Soh*

Main category: cs.RO

TL;DR: 介绍Octopi - 1.5视觉触觉语言模型，可通过新界面体验，展示其解决触觉推理任务和学习新物品能力，凸显进展与局限。


<details>
  <summary>Details</summary>
Motivation: 触觉对人类和机器人很重要，基于触觉基础模型工作，展示Octopi - 1.5以推动视觉触觉语言模型领域发展。

Method: 引入处理多物体部分触觉信号能力，采用检索增强生成（RAG）模块，通过新手持触觉界面TMI让用户交互。

Result: Octopi - 1.5能解决触觉推理任务，如猜物游戏中识别物体并给出处理建议，可学习新物品。

Conclusion: 展示Octopi - 1.5进展与局限，激发对视觉触觉语言模型领域兴趣，代码和设计文件开源。

Abstract: Touch is recognized as a vital sense for humans and an equally important
modality for robots, especially for dexterous manipulation, material
identification, and scenarios involving visual occlusion. Building upon very
recent work in touch foundation models, this demonstration will feature
Octopi-1.5, our latest visual-tactile-language model. Compared to its
predecessor, Octopi-1.5 introduces the ability to process tactile signals from
multiple object parts and employs a simple retrieval-augmented generation (RAG)
module to improve performance on tasks and potentially learn new objects
on-the-fly. The system can be experienced live through a new handheld
tactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile
sensors. This convenient and accessible setup allows users to interact with
Octopi-1.5 without requiring a robot. During the demonstration, we will
showcase Octopi-1.5 solving tactile inference tasks by leveraging tactile
inputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5
will identify objects being grasped and respond to follow-up queries about how
to handle it (e.g., recommending careful handling for soft fruits). We also
plan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.
With live interactions, this demonstration aims to highlight both the progress
and limitations of VTLMs such as Octopi-1.5 and to foster further interest in
this exciting field. Code for Octopi-1.5 and design files for the TMI gripper
are available at https://github.com/clear-nus/octopi-1.5.

</details>


### [460] [Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization](https://arxiv.org/abs/2507.09160)
*Jialei Huang,Shuo Wang,Fanqi Lin,Yihang Hu,Chuan Wen,Yang Gao*

Main category: cs.RO

TL;DR: 本文提出Tactile - VLA框架，融合视觉、语言、动作和触觉传感，实验证明其在三方面有效且具泛化性，还能激活VLM先验知识实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有Vision - Language - Action (VLA)模型虽有成果，但通用机器人代理需精确落实到物理交互，尤其是富接触场景中细粒度力控制。

Method: 引入Tactile - VLA框架，融合视觉、语言、动作和触觉传感，包含混合位置 - 力控制器和推理模块。

Result: 实验证明Tactile - VLA在触觉感知指令跟随、利用触觉相关常识、促进自适应触觉推理三方面有效且具泛化性。

Conclusion: VLM的先验知识已包含物理交互语义理解，通过少量演示连接机器人触觉传感器可激活该先验知识，实现富接触任务零样本泛化。

Abstract: Vision-Language-Action (VLA) models have shown remarkable achievements,
driven by the rich implicit knowledge of their vision-language components.
However, achieving generalist robotic agents demands precise grounding into
physical interactions, especially in contact-rich scenarios where fine-grained
force control is essential. We advance VLAs' implicit knowledge beyond
identifying what to do, towards guiding how to physically interact with real
world. This paper introduces Tactile-VLA, a novel framework that deeply fuses
vision, language, action, and tactile sensing. This framework incorporates a
hybrid position-force controller to translate the model's intentions into
precise physical actions and a reasoning module that allows the robot to adapt
its strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's
effectiveness and generalizability in three key aspects: (1) enabling
tactile-aware instruction following, (2) utilizing tactile-relevant
commonsense, and (3) facilitating adaptive tactile-involved reasoning. A key
finding is that the VLM's prior knowledge already contains semantic
understanding of physical interaction; by connecting it to the robot's tactile
sensors with only a few demonstrations, we can activate this prior knowledge to
achieve zero-shot generalization in contact-rich tasks.

</details>


### [461] [TGLD: A Trust-Aware Game-Theoretic Lane-Changing Decision Framework for Automated Vehicles in Heterogeneous Traffic](https://arxiv.org/abs/2507.10075)
*Jie Pan,Tianyi Wang,Yangyang Wang,Junfeng Jiao,Christian Claudel*

Main category: cs.RO

TL;DR: 提出信任感知博弈论车道变换决策（TGLD）框架，经实验验证可让自动驾驶车辆根据人类驾驶信任水平调整策略，提升效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 现有车道变换框架忽视人类驾驶车辆动态信任水平，难以准确预测人类驾驶员行为。

Method: 构建多车辆联盟博弈，开发在线信任评估方法，考虑社会兼容性目标。

Result: 自动驾驶车辆能根据人类驾驶信任水平和驾驶风格调整策略，引入信任机制提升车道变换效率、维护安全，促进透明自适应交互。

Conclusion: TGLD框架有效可行，能实现人类友好且适应环境的车道变换策略。

Abstract: Automated vehicles (AVs) face a critical need to adopt socially compatible
behaviors and cooperate effectively with human-driven vehicles (HVs) in
heterogeneous traffic environment. However, most existing lane-changing
frameworks overlook HVs' dynamic trust levels, limiting their ability to
accurately predict human driver behaviors. To address this gap, this study
proposes a trust-aware game-theoretic lane-changing decision (TGLD) framework.
First, we formulate a multi-vehicle coalition game, incorporating fully
cooperative interactions among AVs and partially cooperative behaviors from HVs
informed by real-time trust evaluations. Second, we develop an online trust
evaluation method to dynamically estimate HVs' trust levels during
lane-changing interactions, guiding AVs to select context-appropriate
cooperative maneuvers. Lastly, social compatibility objectives are considered
by minimizing disruption to surrounding vehicles and enhancing the
predictability of AV behaviors, thereby ensuring human-friendly and
context-adaptive lane-changing strategies. A human-in-the-loop experiment
conducted in a highway on-ramp merging scenario validates our TGLD approach.
Results show that AVs can effectively adjust strategies according to different
HVs' trust levels and driving styles. Moreover, incorporating a trust mechanism
significantly improves lane-changing efficiency, maintains safety, and
contributes to transparent and adaptive AV-HV interactions.

</details>


### [462] [Unified Linear Parametric Map Modeling and Perception-aware Trajectory Planning for Mobile Robotics](https://arxiv.org/abs/2507.09340)
*Hongyu Nie,Xingyu Li,Xu Liu,Zhaotong Tan,Sen Mei,Wenbo Su*

Main category: cs.RO

TL;DR: 本文针对移动机器人在复杂环境自主导航难题，提出RMRP方法及RPATR框架，经多场景验证性能优越且将开源代码。


<details>
  <summary>Details</summary>
Motivation: 解决移动机器人在大规模复杂环境自主导航时，存在的计算负担重、传感器遮挡及不规则地形遍历等问题，且缺乏感知策略。

Method: 提出RMRP方法构建轻量级线性参数地图，有残差能量保留定理保障；基于此提出RPATR框架，针对UAV和UGV分别有不同规划方式。

Result: 框架在时间、内存和精度上展现出优越的地图构建性能，能让高速UAV和UGV实现高效安全导航。

Conclusion: 所提方法能有效解决移动机器人在复杂环境的导航问题，代码开源可促进社区合作。

Abstract: Autonomous navigation in mobile robots, reliant on perception and planning,
faces major hurdles in large-scale, complex environments. These include heavy
computational burdens for mapping, sensor occlusion failures for UAVs, and
traversal challenges on irregular terrain for UGVs, all compounded by a lack of
perception-aware strategies. To address these challenges, we introduce Random
Mapping and Random Projection (RMRP). This method constructs a lightweight
linear parametric map by first mapping data to a high-dimensional space,
followed by a sparse random projection for dimensionality reduction. Our novel
Residual Energy Preservation Theorem provides theoretical guarantees for this
process, ensuring critical geometric properties are preserved. Based on this
map, we propose the RPATR (Robust Perception-Aware Trajectory Planner)
framework. For UAVs, our method unifies grid and Euclidean Signed Distance
Field (ESDF) maps. The front-end uses an analytical occupancy gradient to
refine initial paths for safety and smoothness, while the back-end uses a
closed-form ESDF for trajectory optimization. Leveraging the trained RMRP
model's generalization, the planner predicts unobserved areas for proactive
navigation. For UGVs, the model characterizes terrain and provides closed-form
gradients, enabling online planning to circumvent large holes. Validated in
diverse scenarios, our framework demonstrates superior mapping performance in
time, memory, and accuracy, and enables computationally efficient, safe
navigation for high-speed UAVs and UGVs. The code will be released to foster
community collaboration.

</details>


### [463] [Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based Diffusion and Potential Fields](https://arxiv.org/abs/2507.09383)
*Wondmgezahu Teshome,Kian Behzad,Octavia Camps,Michael Everett,Milad Siami,Mario Sznaier*

Main category: cs.RO

TL;DR: 提出结合能量扩散模型和人工势场的运动规划框架，可在复杂环境实时生成轨迹，在追逃场景表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决追逃问题，实现在复杂环境中进行鲁棒实时轨迹生成。

Method: 结合能量扩散模型和人工势场，直接处理点云的障碍物信息，采用无分类器引导训练，采样时集成局部势场，在动态场景用扩散模型生成初始轨迹并通过势场自适应不断优化。

Result: 在部分追捕者可观测的追逃场景中展示了有效性能。

Conclusion: 所提出的运动规划框架能在复杂环境下有效实现实时轨迹生成，适用于追逃场景。

Abstract: Motivated by the problem of pursuit-evasion, we present a motion planning
framework that combines energy-based diffusion models with artificial potential
fields for robust real time trajectory generation in complex environments. Our
approach processes obstacle information directly from point clouds, enabling
efficient planning without requiring complete geometric representations. The
framework employs classifier-free guidance training and integrates local
potential fields during sampling to enhance obstacle avoidance. In dynamic
scenarios, the system generates initial trajectories using the diffusion model
and continuously refines them through potential field-based adaptation,
demonstrating effective performance in pursuit-evasion scenarios with partial
pursuer observability.

</details>


### [464] [Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance](https://arxiv.org/abs/2507.10500)
*Kyungtae Han,Yitao Chen,Rohit Gupta,Onur Altintas*

Main category: cs.RO

TL;DR: 本文提出Scene - Aware Conversational ADAS (SC - ADAS)框架，结合生成式AI组件实现实时、可解释和自适应的驾驶辅助，评估显示其能支持下一代智能驾驶辅助，但存在延迟和令牌增长问题。


<details>
  <summary>Details</summary>
Motivation: 当前高级驾驶辅助系统（ADAS）在解释场景上下文和自然语言交互方面能力有限，缺乏对话交互支持，在动态环境和适应驾驶员意图时不够灵活。

Method: 提出SC - ADAS模块化框架，集成生成式AI组件，包括大语言模型、视觉转文本解释和结构化函数调用；在CARLA模拟器中结合基于云的生成式AI实现，执行用户确认的意图，无需模型微调。

Result: 评估了SC - ADAS在场景感知、对话和多轮交互方面的表现，指出存在基于视觉上下文检索的延迟增加和对话历史积累导致的令牌增长问题。

Conclusion: 证明了结合对话推理、场景感知和模块化ADAS控制以支持下一代智能驾驶辅助的可行性。

Abstract: While autonomous driving technologies continue to advance, current Advanced
Driver Assistance Systems (ADAS) remain limited in their ability to interpret
scene context or engage with drivers through natural language. These systems
typically rely on predefined logic and lack support for dialogue-based
interaction, making them inflexible in dynamic environments or when adapting to
driver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a
modular framework that integrates Generative AI components including large
language models, vision-to-text interpretation, and structured function calling
to enable real-time, interpretable, and adaptive driver assistance. SC-ADAS
supports multi-turn dialogue grounded in visual and sensor context, allowing
natural language recommendations and driver-confirmed ADAS control. Implemented
in the CARLA simulator with cloud-based Generative AI, the system executes
confirmed user intents as structured ADAS commands without requiring model
fine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and
revisited multi-turn interactions, highlighting trade-offs such as increased
latency from vision-based context retrieval and token growth from accumulated
dialogue history. These results demonstrate the feasibility of combining
conversational reasoning, scene perception, and modular ADAS control to support
the next generation of intelligent driver assistance.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [465] [Physics-Based Machine Learning Closures and Wall Models for Hypersonic Transition-Continuum Boundary Layer Predictions](https://arxiv.org/abs/2507.08986)
*Ashish S. Nair,Narendra Singh,Marco Panesi,Justin Sirignano,Jonathan F. MacArt*

Main category: physics.flu-dyn

TL;DR: 本文开发物理约束机器学习框架扩展连续介质求解器在非平衡高超声速流中的适用性，评估不同模型，展示提高精度和泛化性的策略。


<details>
  <summary>Details</summary>
Motivation: 经典连续介质假设在过渡连续区失效，传统NSF模型无法准确预测非平衡效应，需要改进高超声速流建模。

Method: 开发物理约束机器学习框架，采用深度学习PDE模型并通过伴随优化训练，引入基于偏态高斯近似的壁面模型。

Result: 无迹各向异性粘度模型与偏态高斯壁面模型在高马赫和高克努森数区域精度显著提高，多克努森数并行训练和引入高马赫数据可增强泛化性。

Conclusion: 建立了数据驱动、物理一致的策略来改进传统连续介质方法失效区域的高超声速流建模。

Abstract: Modeling rarefied hypersonic flows remains a fundamental challenge due to the
breakdown of classical continuum assumptions in the transition-continuum
regime, where the Knudsen number ranges from approximately 0.1 to 10.
Conventional Navier-Stokes-Fourier (NSF) models with empirical slip-wall
boundary conditions fail to accurately predict nonequilibrium effects such as
velocity slip, temperature jump, and shock structure deviations. We develop a
physics-constrained machine learning framework that augments transport models
and boundary conditions to extend the applicability of continuum solvers in
nonequilibrium hypersonic regimes. We employ deep learning PDE models (DPMs)
for the viscous stress and heat flux embedded in the governing PDEs and trained
via adjoint-based optimization. We evaluate these for two-dimensional
supersonic flat-plate flows across a range of Mach and Knudsen numbers.
Additionally, we introduce a wall model based on a mixture of skewed Gaussian
approximations of the particle velocity distribution function. This wall model
replaces empirical slip conditions with physically informed, data-driven
boundary conditions for the streamwise velocity and wall temperature. Our
results show that a trace-free anisotropic viscosity model, paired with the
skewed-Gaussian distribution function wall model, achieves significantly
improved accuracy, particularly at high-Mach and high-Knudsen number regimes.
Strategies such as parallel training across multiple Knudsen numbers and
inclusion of high-Mach data during training are shown to enhance model
generalization. Increasing model complexity yields diminishing returns for
out-of-sample cases, underscoring the need to balance degrees of freedom and
overfitting. This work establishes data-driven, physics-consistent strategies
for improving hypersonic flow modeling for regimes in which conventional
continuum approaches are invalid.

</details>


### [466] [WellPINN: Accurate Well Representation for Transient Fluid Pressure Diffusion in Subsurface Reservoirs with Physics-Informed Neural Networks](https://arxiv.org/abs/2507.09330)
*Linus Walter,Qingkai Kong,Sara Hanson-Hedgecock,Víctor Vilarrasa*

Main category: physics.flu-dyn

TL;DR: 提出WellPINN工作流解决现有PINN模型在井附近捕捉流体压力的问题，能在整个注入期准确推断流体压力。


<details>
  <summary>Details</summary>
Motivation: 现有基于PINN的研究在捕捉井附近流体压力，特别是注入开始早期面临重大挑战，需要准确表示井以进行可靠的储层表征和模拟。

Method: 提出WellPINN工作流，结合多个顺序训练的PINN模型输出，将域分解为逐步缩小的子域，同时减小等效井半径以匹配实际井尺寸。

Result: 顺序训练围绕抽水井的叠加网络是首个能在整个注入期从抽水率准确推断流体压力的工作流，显著提升了PINN用于逆建模和操作场景模拟的潜力。

Conclusion: WellPINN工作流有效解决了现有PINN模型问题，推进了PINN在相关领域的应用，且数据和代码将公开。

Abstract: Accurate representation of wells is essential for reliable reservoir
characterization and simulation of operational scenarios in subsurface flow
models. Physics-informed neural networks (PINNs) have recently emerged as a
promising method for reservoir modeling, offering seamless integration of
monitoring data and governing physical equations. However, existing PINN-based
studies face major challenges in capturing fluid pressure near wells,
particularly during the early stage after injection begins. To address this, we
propose WellPINN, a modeling workflow that combines the outputs of multiple
sequentially trained PINN models to accurately represent wells. This workflow
iteratively approximates the radius of the equivalent well to match the actual
well dimensions by decomposing the domain into stepwise shrinking subdomains
with a simultaneously reducing equivalent well radius. Our results demonstrate
that sequential training of superimposing networks around the pumping well is
the first workflow that focuses on accurate inference of fluid pressure from
pumping rates throughout the entire injection period, significantly advancing
the potential of PINNs for inverse modeling and operational scenario
simulations. All data and code for this paper will be made openly available at
https://github.com/linuswalter/WellPINN.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [467] [SmartphoneDemocracy: Privacy-Preserving E-Voting on Decentralized Infrastructure using Novel European Identity](https://arxiv.org/abs/2507.09453)
*Michał Jóźwik,Johan Pouwelse*

Main category: cs.CR

TL;DR: 研究提出SmartphoneDemocracy电子投票协议，结合三项技术实现安全私密投票，经分析和评估可行。


<details>
  <summary>Details</summary>
Motivation: 数字化民主进程有安全、隐私等挑战，现有电子投票系统依赖集中架构，违背民主原则，需创建安全、隐私且减少信任依赖的电子投票系统。

Method: 引入SmartphoneDemocracy协议，结合欧洲数字身份钱包、零知识证明和点对点区块链。

Result: 提供详细协议设计、安全分析和性能评估，证明计算和网络开销在中大型选举中可行。

Conclusion: 开发并原型化该系统，为公民提供可信、易访问和用户可控的数字投票体验提供了可行路径。

Abstract: The digitization of democratic processes promises greater accessibility but
presents challenges in terms of security, privacy, and verifiability. Existing
electronic voting systems often rely on centralized architectures, creating
single points of failure and forcing too much trust in authorities, which
contradicts democratic principles. This research addresses the challenge of
creating a secure, private e-voting system with minimized trust dependencies
designed for the most versatile personal device: the smartphone. We introduce
SmartphoneDemocracy, a novel e-voting protocol that combines three key
technologies: the emerging European Digital Identity (EUDI) Wallet for
Sybil-resistant identity verification, Zero-Knowledge Proofs for
privacy-preserving validation, and a peer-to-peer blockchain (TrustChain) for a
resilient, serverless public bulletin board. Our protocol enables voters to
register and cast ballots anonymously and verifiably directly from their
smartphones. We provide a detailed protocol design, a security analysis against
a defined threat model, and a performance evaluation demonstrating that the
computational and network overhead is feasible for medium- to large-scale
elections. By developing and prototyping this system, we demonstrate a viable
path to empower citizens with a trustworthy, accessible, and user-controlled
digital voting experience.

</details>


### [468] [PromptChain: A Decentralized Web3 Architecture for Managing AI Prompts as Digital Assets](https://arxiv.org/abs/2507.09579)
*Marc Bara*

Main category: cs.CR

TL;DR: 提出PromptChain去中心化Web3架构，将AI提示作为一等数字资产，解决现有中心化平台问题，为Web3时代人机协作开放生态奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前中心化平台缺乏对提示创作者的适当归属、质量保证和公平补偿机制。

Method: 通过集成IPFS进行不可变存储、智能合约进行治理、代币激励进行社区策展，设计全面元数据模式、权益加权验证机制和代币经济。

Result: 该架构能在效率上与中心化方案媲美，同时提供更好的所有权保障和抗审查能力。

Conclusion: 此工作为Web3时代人机协作开放生态奠定基础，是对提示作为独立数字资产的首次系统处理。

Abstract: We present PromptChain, a decentralized Web3 architecture that establishes AI
prompts as first-class digital assets with verifiable ownership, version
control, and monetization capabilities. Current centralized platforms lack
mechanisms for proper attribution, quality assurance, or fair compensation for
prompt creators. PromptChain addresses these limitations through a novel
integration of IPFS for immutable storage, smart contracts for governance, and
token incentives for community curation. Our design includes: (1) a
comprehensive metadata schema for cross-model compatibility, (2) a
stake-weighted validation mechanism to align incentives, and (3) a token
economy that rewards contributors proportionally to their impact. The proposed
architecture demonstrates how decentralized systems could potentially match
centralized alternatives in efficiency while providing superior ownership
guarantees and censorship resistance through blockchain-anchored provenance
tracking. By decoupling prompts from specific AI models or outputs, this work
establishes the foundation for an open ecosystem of human-AI collaboration in
the Web3 era, representing the first systematic treatment of prompts as
standalone digital assets with dedicated decentralized infrastructure.

</details>


### [469] [Clio-X: AWeb3 Solution for Privacy-Preserving AI Access to Digital Archives](https://arxiv.org/abs/2507.08853)
*Victoria L. Lemieux,Rosa Gil,Faith Molosiwa,Qihong Zhou,Binming Li,Roberto Garcia,Luis De La Torre Cubillo,Zehua Wang*

Main category: cs.CR

TL;DR: 文章探讨用PETs和Web3架构助档案馆管控敏感内容，介绍Clio - X方案，用户评估显示有潜力但有采纳障碍，用创新扩散理论分析并提出推进路径。


<details>
  <summary>Details</summary>
Motivation: 当前AI数据实践在档案管理中存在隐私风险，关乎数据主权和伦理责任，需新方法管控敏感内容。

Method: 提出Clio - X方案，进行中保真原型用户评估，运用Rogers创新扩散理论分析采纳障碍。

Result: 用户对Clio - X方案潜力感兴趣，但存在信任、系统不透明、经济和治理等采纳障碍。

Conclusion: Clio - X集成技术保障和社区监督，为文化遗产领域伦理部署AI提供新模型。

Abstract: As archives turn to artificial intelligence to manage growing volumes of
digital records, privacy risks inherent in current AI data practices raise
critical concerns about data sovereignty and ethical accountability. This paper
explores how privacy-enhancing technologies (PETs) and Web3 architectures can
support archives to preserve control over sensitive content while still being
able to make it available for access by researchers. We present Clio-X, a
decentralized, privacy-first Web3 digital solution designed to embed PETs into
archival workflows and support AI-enabled reference and access. Drawing on a
user evaluation of a medium-fidelity prototype, the study reveals both interest
in the potential of the solution and significant barriers to adoption related
to trust, system opacity, economic concerns, and governance. Using Rogers'
Diffusion of Innovation theory, we analyze the sociotechnical dimensions of
these barriers and propose a path forward centered on participatory design and
decentralized governance through a Clio-X Decentralized Autonomous
Organization. By integrating technical safeguards with community-based
oversight, Clio-X offers a novel model to ethically deploy AI in cultural
heritage contexts.

</details>


### [470] [Privacy-Utility-Fairness: A Balanced Approach to Vehicular-Traffic Management System](https://arxiv.org/abs/2507.08864)
*Poushali Sengupta,Sabita Maharjan,frank Eliassen,Yan Zhang*

Main category: cs.CR

TL;DR: 提出新算法解决基于位置的车辆交通管理中隐私、效用和公平性平衡问题，用差分隐私技术保护数据，在挪威车辆数据上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有方案难以应对基于位置的车辆交通管理中敏感地理数据保护、数据效用和区域公平性问题，易导致隐私泄露和数据分析不公平。

Method: 采用差分隐私技术，将基于查询的数据访问与迭代洗牌和校准噪声注入相结合，通过拉普拉斯机制确保符合epsilon - 差分隐私标准。

Result: 在挪威车辆位置数据上实现算法，能保持交通管理和城市规划的数据效用，保证各地理区域公平展示，还制作了交通状况热图。

Conclusion: 该算法为车辆交通管理提供了隐私保护方案。

Abstract: Location-based vehicular traffic management faces significant challenges in
protecting sensitive geographical data while maintaining utility for traffic
management and fairness across regions. Existing state-of-the-art solutions
often fail to meet the required level of protection against linkage attacks and
demographic biases, leading to privacy leakage and inequity in data analysis.
In this paper, we propose a novel algorithm designed to address the challenges
regarding the balance of privacy, utility, and fairness in location-based
vehicular traffic management systems. In this context, utility means providing
reliable and meaningful traffic information, while fairness ensures that all
regions and individuals are treated equitably in data use and decision-making.
Employing differential privacy techniques, we enhance data security by
integrating query-based data access with iterative shuffling and calibrated
noise injection, ensuring that sensitive geographical data remains protected.
We ensure adherence to epsilon-differential privacy standards by implementing
the Laplace mechanism. We implemented our algorithm on vehicular location-based
data from Norway, demonstrating its ability to maintain data utility for
traffic management and urban planning while ensuring fair representation of all
geographical areas without being overrepresented or underrepresented.
Additionally, we have created a heatmap of Norway based on our model,
illustrating the privatized and fair representation of the traffic conditions
across various cities. Our algorithm provides privacy in vehicular traffic

</details>


### [471] [Towards Privacy-Preserving and Personalized Smart Homes via Tailored Small Language Models](https://arxiv.org/abs/2507.08878)
*Xinyu Huang,Leming Shen,Zijing Ma,Yuanqing Zheng*

Main category: cs.CR

TL;DR: 本文提出HomeLLaMA设备端助手和PrivShield保护隐私，用DevFinder评估，实验表明能提供服务并增强隐私。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能家居助手将数据传至远程服务器，引发用户隐私担忧。

Method: 开发HomeLLaMA设备端助手，用定制小语言模型学习云大语言模型；开发PrivShield提供可选隐私保护服务；构建DevFinder评估服务质量。

Result: 通过大量实验和用户研究（M = 100），HomeLLaMA能提供个性化服务并显著增强用户隐私。

Conclusion: HomeLLaMA可以在智能家居场景中提供个性化服务的同时，有效保护用户隐私。

Abstract: Large Language Models (LLMs) have showcased remarkable generalizability in
language comprehension and hold significant potential to revolutionize
human-computer interaction in smart homes. Existing LLM-based smart home
assistants typically transmit user commands, along with user profiles and home
configurations, to remote servers to obtain personalized services. However,
users are increasingly concerned about the potential privacy leaks to the
remote servers. To address this issue, we develop HomeLLaMA, an on-device
assistant for privacy-preserving and personalized smart home serving with a
tailored small language model (SLM). HomeLLaMA learns from cloud LLMs to
deliver satisfactory responses and enable user-friendly interactions. Once
deployed, HomeLLaMA facilitates proactive interactions by continuously updating
local SLMs and user profiles. To further enhance user experience while
protecting their privacy, we develop PrivShield to offer an optional
privacy-preserving LLM-based smart home serving for those users, who are
unsatisfied with local responses and willing to send less-sensitive queries to
remote servers. For evaluation, we build a comprehensive benchmark DevFinder to
assess the service quality. Extensive experiments and user studies (M=100)
demonstrate that HomeLLaMA can provide personalized services while
significantly enhancing user privacy.

</details>


### [472] [A Mixture of Linear Corrections Generates Secure Code](https://arxiv.org/abs/2507.09508)
*Weichen Yu,Ravi Mangal,Terry Zhuo,Matt Fredrikson,Corina S. Pasareanu*

Main category: cs.CR

TL;DR: 研究发现大语言模型内部能编码区分代码漏洞的表示，开发MoC技术有效减少生成代码漏洞并提升功能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码漏洞检测和避免方面不足，探究是学习不足还是提示无效导致。

Method: 运用表示工程技术研究模型内部编码，开发推理时的混合修正（MoC）技术调制模型生成概率。

Result: 当前大语言模型能编码区分漏洞与安全代码的表示，MoC使Qwen2.5 - Coder - 7B安全比率提升8.9%，HumanEval pass@1功能提升2.1%。

Conclusion: 提出了一种实用的生成代码漏洞管理方法，可在不影响功能的情况下减少漏洞。

Abstract: Large language models (LLMs) have become proficient at sophisticated
code-generation tasks, yet remain ineffective at reliably detecting or avoiding
code vulnerabilities. Does this deficiency stem from insufficient learning
about code vulnerabilities, or is it merely a result of ineffective prompting?
Using representation engineering techniques, we investigate whether LLMs
internally encode the concepts necessary to identify code vulnerabilities. We
find that current LLMs encode precise internal representations that distinguish
vulnerable from secure code--achieving greater accuracy than standard prompting
approaches. Leveraging these vulnerability-sensitive representations, we
develop an inference-time steering technique that subtly modulates the model's
token-generation probabilities through a mixture of corrections (MoC). Our
method effectively guides LLMs to produce less vulnerable code without
compromising functionality, demonstrating a practical approach to controlled
vulnerability management in generated code. Notably, MoC enhances the security
ratio of Qwen2.5-Coder-7B by 8.9\%, while simultaneously improving
functionality on HumanEval pass@1 by 2.1\%.

</details>


### [473] [EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions](https://arxiv.org/abs/2507.09762)
*Yasir Ech-Chammakhy,Anas Motii,Anass Rabii,Jaafar Chbili*

Main category: cs.CR

TL;DR: 本文提出无监督框架处理黑客论坛内容，检测、聚类和排序安全事件，实验证明可有效降噪并发现高优先级威胁。


<details>
  <summary>Details</summary>
Motivation: 从黑客论坛非结构化和嘈杂内容中提取可操作情报是重大挑战，需要有效方法进行自动化威胁检测和分析。

Method: 使用基于Transformer的嵌入并通过对比学习微调，将相关讨论分组为不同安全事件集群，结合每日排名机制用可量化指标对事件排序。

Result: 在真实黑客论坛数据上实验表明，该方法能有效降噪并凸显高优先级威胁。

Conclusion: 该框架将不同黑客论坛讨论转化为结构化、可操作情报，解决了自动化威胁检测和分析的基本挑战。

Abstract: Hacker forums provide critical early warning signals for emerging
cybersecurity threats, but extracting actionable intelligence from their
unstructured and noisy content remains a significant challenge. This paper
presents an unsupervised framework that automatically detects, clusters, and
prioritizes security events discussed across hacker forum posts. Our approach
leverages Transformer-based embeddings fine-tuned with contrastive learning to
group related discussions into distinct security event clusters, identifying
incidents like zero-day disclosures or malware releases without relying on
predefined keywords. The framework incorporates a daily ranking mechanism that
prioritizes identified events using quantifiable metrics reflecting timeliness,
source credibility, information completeness, and relevance. Experimental
evaluation on real-world hacker forum data demonstrates that our method
effectively reduces noise and surfaces high-priority threats, enabling security
analysts to mount proactive responses. By transforming disparate hacker forum
discussions into structured, actionable intelligence, our work addresses
fundamental challenges in automated threat detection and analysis.

</details>


### [474] [Secure and Efficient UAV-Based Face Detection via Homomorphic Encryption and Edge Computing](https://arxiv.org/abs/2507.09860)
*Nguyen Van Duc,Bui Duc Manh,Quang-Trung Luu,Dinh Thai Hoang,Van-Linh Nguyen,Diep N. Nguyen*

Main category: cs.CR

TL;DR: 本文提出结合同态加密（HE）的机器学习方法，解决无人机面部检测的隐私问题，实验证明该方法能平衡隐私保护与检测性能。


<details>
  <summary>Details</summary>
Motivation: 无人机面部检测存在隐私问题，传统方法在进行高精度面部识别时，因广泛的监控能力引发隐私担忧。

Method: 提出将HE与先进神经网络集成的框架，利用CKKS方案对加密数据进行计算，开发数据编码方法，设计安全推理算法。

Result: 方法能有效平衡隐私保护和检测性能，与未加密基准相比，准确率差距小于1%。

Conclusion: 该方法是无人机安全面部检测的可行解决方案。

Abstract: This paper aims to propose a novel machine learning (ML) approach
incorporating Homomorphic Encryption (HE) to address privacy limitations in
Unmanned Aerial Vehicles (UAV)-based face detection. Due to challenges related
to distance, altitude, and face orientation, high-resolution imagery and
sophisticated neural networks enable accurate face recognition in dynamic
environments. However, privacy concerns arise from the extensive surveillance
capabilities of UAVs. To resolve this issue, we propose a novel framework that
integrates HE with advanced neural networks to secure facial data throughout
the inference phase. This method ensures that facial data remains secure with
minimal impact on detection accuracy. Specifically, the proposed system
leverages the Cheon-Kim-Kim-Song (CKKS) scheme to perform computations directly
on encrypted data, optimizing computational efficiency and security.
Furthermore, we develop an effective data encoding method specifically designed
to preprocess the raw facial data into CKKS form in a
Single-Instruction-Multiple-Data (SIMD) manner. Building on this, we design a
secure inference algorithm to compute on ciphertext without needing decryption.
This approach not only protects data privacy during the processing of facial
data but also enhances the efficiency of UAV-based face detection systems.
Experimental results demonstrate that our method effectively balances privacy
protection and detection performance, making it a viable solution for UAV-based
secure face detection. Significantly, our approach (while maintaining data
confidentially with HE encryption) can still achieve an accuracy of less than
1% compared to the benchmark without using encryption.

</details>


### [475] [Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix](https://arxiv.org/abs/2507.09990)
*Ming Wen,Jiaqi Zhu,Yuedong Xu,Yipeng Zhou,Dingding Han*

Main category: cs.CR

TL;DR: 提出FedASK框架解决联邦LoRA隐私泄露问题，理论证明其隐私性和聚合特性，实验显示性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: LoRA在联邦大语言模型通信高效，但传输本地适配器有隐私泄露风险，应用差分隐私存在困境。

Method: 提出FedASK框架，采用两阶段草图管道，先聚合本地更新，再在服务器重建全局矩阵。

Result: 理论证明FedASK有差分隐私保证和精确聚合特性，实验表明在多种隐私设置和数据分布下优于基线方法。

Conclusion: FedASK是一种能实现低秩适配器有效更新并具备强大差分隐私保护的联邦LoRA框架。

Abstract: Large language models (LLMs) typically require fine-tuning for
domain-specific tasks, and LoRA offers a computationally efficient approach by
training low-rank adapters. LoRA is also communication-efficient for federated
LLMs when multiple users collaboratively fine-tune a global LLM model without
sharing their proprietary raw data. However, even the transmission of local
adapters between a server and clients risks serious privacy leakage. Applying
differential privacy (DP) to federated LoRA encounters a dilemma: adding noise
to both adapters amplifies synthetic noise on the model, while fixing one
adapter impairs the learnability of fine-tuning. In this paper, we propose
FedASK (Differentially Private Federated Low Rank Adaptation with Double
Sketching) , a novel federated LoRA framework to enable effective updating of
both low-rank adapters with robust differential privacy. Inspired by randomized
SVD, our key idea is a two-stage sketching pipeline. This pipeline first
aggregates carefully sketched, privacy-preserving local updates, and then
reconstructs the global matrices on the server to facilitate effective updating
of both adapters. We theoretically prove FedASK's differential privacy
guarantee and its exact aggregation property. Comprehensive experiments
demonstrate that FedASK consistently outperforms baseline methods across a
variety of privacy settings and data distributions.

</details>


### [476] [CAN-Trace Attack: Exploit CAN Messages to Uncover Driving Trajectories](https://arxiv.org/abs/2507.09624)
*Xiaojie Lin,Baihe Ma,Xu Wang,Guangsheng Yu,Ying He,Wei Ni,Ren Ping Liu*

Main category: cs.CR

TL;DR: 现有措施下驾驶轨迹数据仍有隐私风险，本文提出CAN - Trace隐私攻击机制，用新算法和评估指标识别轨迹，实证表明其在不同区域攻击成功率高。


<details>
  <summary>Details</summary>
Motivation: 现有检测驾驶轨迹方法依赖GPS易受数据中断影响，且驾驶轨迹数据存在隐私泄露风险。

Method: 提出新轨迹重建算法将CAN消息转化为加权图，用图匹配算法识别轨迹，设计新指标评估匹配候选。

Result: 在城市区域攻击成功率达90.59%，郊区达99.41%。

Conclusion: CAN - Trace在不同现实条件下有效，对驾驶员长期隐私构成重大威胁。

Abstract: Driving trajectory data remains vulnerable to privacy breaches despite
existing mitigation measures. Traditional methods for detecting driving
trajectories typically rely on map-matching the path using Global Positioning
System (GPS) data, which is susceptible to GPS data outage. This paper
introduces CAN-Trace, a novel privacy attack mechanism that leverages
Controller Area Network (CAN) messages to uncover driving trajectories, posing
a significant risk to drivers' long-term privacy. A new trajectory
reconstruction algorithm is proposed to transform the CAN messages,
specifically vehicle speed and accelerator pedal position, into weighted graphs
accommodating various driving statuses. CAN-Trace identifies driving
trajectories using graph-matching algorithms applied to the created graphs in
comparison to road networks. We also design a new metric to evaluate matched
candidates, which allows for potential data gaps and matching inaccuracies.
Empirical validation under various real-world conditions, encompassing
different vehicles and driving regions, demonstrates the efficacy of CAN-Trace:
it achieves an attack success rate of up to 90.59% in the urban region, and
99.41% in the suburban region.

</details>


### [477] [Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems](https://arxiv.org/abs/2507.10457)
*Hammad Atta,Ken Huang,Manish Bhatt,Kamal Ahmed,Muhammad Aziz Ul Haq,Yasir Mehmood*

Main category: cs.CR

TL;DR: 本文指出大语言模型集成到企业系统产生新的隐蔽安全漏洞，介绍了新型攻击类别LPCI。


<details>
  <summary>Details</summary>
Motivation: 大语言模型集成到企业系统产生了新的隐蔽安全漏洞，需要研究应对。

Method: 提出Logic - Layer Prompt Control Injection (LPCI)这一新型攻击类别，将编码、延迟和条件触发的有效负载嵌入内存、向量存储或工具输出中。

Result: 该有效负载可绕过传统输入过滤器，跨会话触发未经授权的行为。

Conclusion: LLMs集成到企业系统带来新的安全挑战，LPCI攻击具有绕过传统防护机制的风险。

Abstract: The integration of large language models (LLMs) into enterprise systems has
created a new class of covert security vulnerabilities, particularly within
logic-execution layers and persistent-memory contexts. In this paper, we
introduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category
in which encoded, delayed, and conditionally triggered payloads are embedded in
memory, vector stores, or tool outputs. These payloads can bypass conventional
input filters and trigger unauthorised behaviour across sessions.

</details>


### [478] [DNS Tunneling: Threat Landscape and Improved Detection Solutions](https://arxiv.org/abs/2507.10267)
*Novruz Amirov,Baran Isik,Bilal Ihsan Tuncer,Serif Bahtiyar*

Main category: cs.CR

TL;DR: 研究提出用机器学习算法检测DNS隧道的新方法，分析表明该方法能准确检测。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则或签名匹配的DNS隧道检测方法不足以准确识别隐蔽通信渠道，需有效检测方法。

Method: 结合机器学习算法，利用从DNS流量中提取的特征分析流量。

Result: 所提方法是准确检测DNS隧道的良好候选方案。

Conclusion: 提出的基于机器学习算法的方法可有效准确检测DNS隧道。

Abstract: Detecting Domain Name System (DNS) tunneling is a significant challenge in
security due to its capacity to hide harmful actions within DNS traffic that
appears to be normal and legitimate. Traditional detection methods are based on
rule-based approaches or signature matching methods that are often insufficient
to accurately identify such covert communication channels. This research is
about effectively detecting DNS tunneling. We propose a novel approach to
detect DNS tunneling with machine learning algorithms. We combine machine
learning algorithms to analyze the traffic by using features extracted from DNS
traffic. Analyses results show that the proposed approach is a good candidate
to detect DNS tunneling accurately.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [479] [DiffNMR: Diffusion Models for Nuclear Magnetic Resonance Spectra Elucidation](https://arxiv.org/abs/2507.08854)
*Qingsong Yang,Binglan Wu,Xuwei Liu,Bo Chen,Wei Li,Gen Long,Xin Chen,Mingjun Xiao*

Main category: physics.chem-ph

TL;DR: 提出DiffNMR框架用于从核磁共振谱图解析分子结构，实验显示有竞争力表现。


<details>
  <summary>Details</summary>
Motivation: 核磁共振光谱解析分子结构因谱图数据复杂和化学空间大而具有挑战性。

Method: 引入DiffNMR框架，利用条件离散扩散模型，迭代优化分子图，采用两阶段预训练策略，推理时结合检索初始化和相似度过滤，使用带径向基函数编码的NMR编码器。

Result: DiffNMR在基于NMR的结构解析中取得有竞争力的性能。

Conclusion: DiffNMR为自动分子分析提供了高效且稳健的解决方案。

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy is a central characterization
method for molecular structure elucidation, yet interpreting NMR spectra to
deduce molecular structures remains challenging due to the complexity of
spectral data and the vastness of the chemical space. In this work, we
introduce DiffNMR, a novel end-to-end framework that leverages a conditional
discrete diffusion model for de novo molecular structure elucidation from NMR
spectra. DiffNMR refines molecular graphs iteratively through a diffusion-based
generative process, ensuring global consistency and mitigating error
accumulation inherent in autoregressive methods. The framework integrates a
two-stage pretraining strategy that aligns spectral and molecular
representations via diffusion autoencoder (Diff-AE) and contrastive learning,
the incorporation of retrieval initialization and similarity filtering during
inference, and a specialized NMR encoder with radial basis function (RBF)
encoding for chemical shifts, preserving continuity and chemical correlation.
Experimental results demonstrate that DiffNMR achieves competitive performance
for NMR-based structure elucidation, offering an efficient and robust solution
for automated molecular analysis.

</details>


### [480] [Accurate generation of chemical reaction transition states by conditional flow matching](https://arxiv.org/abs/2507.10530)
*Ping Tuo,Jiale Chen,Ju Li*

Main category: physics.chem-ph

TL;DR: 介绍TS - GEN生成模型，能直接生成过渡态结构，精度高、速度快且适用性广，利于高通量探索复杂反应网络。


<details>
  <summary>Details</summary>
Motivation: 过渡态结构实验难捕捉，依赖高成本的密度泛函理论计算，需新方法解决。

Method: 引入TS - GEN条件流匹配生成模型，将简单高斯先验样本映射到过渡态鞍点几何结构，嵌入反应物和产物构象信息。

Result: TS - GEN精度高，均方根偏差小，势垒高度误差小，推理时间短，超87%生成的过渡态满足化学精度标准，有强可迁移性。

Conclusion: TS - GEN结合高精度、快速和广泛适用性，对高通量探索复杂反应网络很有用，为探索新反应机制铺平道路。

Abstract: Transition state (TS) structures define the critical geometries and energy
barriers underlying chemical reactivity, yet their fleeting nature renders them
experimentally elusive and drives the reliance on costly, high-throughput
density functional theory (DFT) calculations. Here, we introduce TS-GEN, a
conditional flow-matching generative model that maps samples from a simple
Gaussian prior directly to transition-state saddle-point geometries in a
single, deterministic pass. By embedding both reactant and product
conformations as conditioning information, TS-GEN learns to transport latent
noise to true TS structures via an optimal-transport path, effectively
replacing the iterative optimization common in nudged-elastic band or
string-method algorithms. TS-GEN delivers unprecedented accuracy, achieving a
root-mean-square deviation of $0.004\ \rm{\mathring{A}}$ (vs. $0.103\
\rm{\mathring{A}}$ for prior state-of-the-art) and a mean barrier-height error
of $1.019\ {\rm kcal/mol}$ (vs. $2.864\ {\rm kcal/mol}$), while requiring only
$0.06\ {\rm s}$ GPU time per inference. Over 87% of generated TSs meet
chemical-accuracy criteria ($<1.58\ {\rm kcal/mol}$ error), substantially
outpacing existing methods. TS-GEN also exhibits strong transferability to
out-of-distribution reactions from a larger database. By uniting sub-angstrom
precision, sub-second speed, and broad applicability, TS-GEN will be highly
useful for high-throughput exploration of complex reaction networks, paving the
way to the exploration of novel chemical reaction mechanisms.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [481] [Colorful Minors](https://arxiv.org/abs/2507.10467)
*Evangelos Protopapas,Dimitrios M. Thilikos,Sebastian Wiederrecht*

Main category: math.CO

TL;DR: 本文引入彩色子式概念，推广图的有根子式，发展结构理论，对满足特定性质的彩色图分类，给出算法并推导算法元定理。


<details>
  <summary>Details</summary>
Motivation: 经典子式概念无法很好处理含注释顶点集的图算法问题，需引入彩色子式概念进行研究。

Method: 建立刻画不含特定彩色子式图的定理，利用结构理论进行彩色图分类，设计固定参数可处理算法。

Result: 完成彩色图分类，得到彩色子式测试和k - 不相交路径问题的固定参数可处理算法，推导两个算法元定理。

Conclusion: 结果表明已知算法元定理可扩展，同时考虑输入图结构和彩色顶点分布。

Abstract: We introduce the notion of colorful minors, which generalizes the classical
concept of rooted minors in graphs. $q$-colorful graph is defined as a pair
$(G, \chi),$ where $G$ is a graph and $\chi$ assigns to each vertex a (possibly
empty) subset of at most $q$ colors. The colorful minor relation enhances the
classical minor relation by merging color sets at contracted edges and allowing
the removal of colors from vertices. This framework naturally models
algorithmic problems involving graphs with (possibly overlapping) annotated
vertex sets. We develop a structural theory for colorful minors by establishing
several theorems characterizing $\mathcal{H}$-colorful minor-free graphs, where
$\mathcal{H}$ consists either of a clique or a grid with all vertices assigned
all colors, or of grids with colors segregated and ordered on the outer face.
Leveraging our structural insights, we provide a complete classification -
parameterized by the number $q$ of colors - of all colorful graphs that exhibit
the Erd\H{o}s-P\'osa property with respect to colorful minors. On the
algorithmic side, we provide a fixed-parameter tractable algorithm for colorful
minor testing and a variant of the $k$-disjoint paths problem. Together with
the fact that the colorful minor relation forms a well-quasi-order, this
implies that every colorful minor-monotone parameter on colorful graphs admits
a fixed-parameter algorithm. Furthermore, we derive two algorithmic
meta-theorems (AMTs) whose structural conditions are linked to extensions of
treewidth and Hadwiger number on colorful graphs. Our results suggest how known
AMTs can be extended to incorporate not only the structure of the input graph
but also the way the colored vertices are distributed in it.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [482] [Toolsuite for Implementing Multiagent Systems Based on Communication Protocols](https://arxiv.org/abs/2507.10324)
*Amit K. Chopra,Samuel H. Christie V,Munindar P. Singh*

Main category: cs.MA

TL;DR: 介绍面向交互编程（IOP）及相关软件套件。


<details>
  <summary>Details</summary>
Motivation: 为多智能体系统开发者提供可应用IOP的软件工具。

Method: 开发一系列软件，包括验证协议工具和简化智能体实现的中间件。

Result: 开发出了一系列能让开发者应用IOP的软件。

Conclusion: 展示了部分开发出的软件套件。

Abstract: Interaction-Oriented Programming (IOP) is an approach to building a
multiagent system by modeling the interactions between its roles via a flexible
interaction protocol and implementing agents to realize the interactions of the
roles they play in the protocol.
  In recent years, we have developed an extensive suite of software that
enables multiagent system developers to apply IOP. These include tools for
efficiently verifying protocols for properties such as liveness and safety and
middleware that simplifies the implementation of agents. This paper presents
some of that software suite.

</details>


### [483] [Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents](https://arxiv.org/abs/2507.08944)
*Enhao Zhang,Erkang Zhu,Gagan Bansal,Adam Fourney,Hussein Mozannar,Jack Gerrits*

Main category: cs.MA

TL;DR: 提出M1 - Parallel框架解决基于大语言模型的多智能体系统高延迟问题，实验证明其能提升速度或任务完成率。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体系统处理复杂任务时存在高延迟问题，需要解决。

Method: 提出M1 - Parallel框架，通过并行运行多智能体团队，利用事件驱动通信模型和异步消息传递来挖掘不同解决方案路径。

Result: M1 - Parallel提前终止可实现最高2.2倍加速且保持准确性，聚合方式可提高任务完成率；探索鼓励多样化执行计划策略未带来额外性能提升。

Conclusion: 并行计划执行对优化多智能体系统处理现实高复杂度推理任务有潜力。

Abstract: Large language model (LLM)-based multi-agent systems have demonstrated
remarkable promise for tackling complex tasks by breaking them down into
subtasks that are iteratively planned, executed, observed, and refined. Despite
their effectiveness, these systems often incur high latency because real-world
problems frequently demand multiple iterative cycles of reasoning steps. To
address this challenge, we propose M1-Parallel, a framework that concurrently
runs multiple multi-agent teams in parallel to uncover distinct solution paths.
By leveraging an event-driven communication model with asynchronous messaging,
M1-Parallel efficiently capitalizes on the inherent diversity of valid plans to
either reduce end-to-end latency or boost task completion rates. Our
experiments on complex tasks show that M1-Parallel with early termination
achieves up to $2.2\times$ speedup while preserving accuracy, and that
M1-Parallel with aggregation yields higher task completion rates. We further
investigate strategies aimed at encouraging diverse execution plans but observe
no additional performance gains over repeated sampling. Overall, these findings
underscore the potential of parallel plan execution for optimizing multi-agent
systems for real-world, high-complexity reasoning tasks.

</details>


### [484] [How to Train a Leader: Hierarchical Reasoning in Multi-Agent LLMs](https://arxiv.org/abs/2507.08960)
*Andrew Estornell,Jean-Francois Ton,Muhammad Faaiz Taufiq,Hang Li*

Main category: cs.MA

TL;DR: 提出分层多智能体框架，用MLPO训练单个领导者大语言模型，在多任务上超单智能体和多智能体基线。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架在训练和推理时计算成本高，需提升效率。

Method: 引入分层多智能体框架，提出MLPO方法训练单个领导者LLM协调未训练的对等智能体，领导者评估和合成代理响应无需辅助价值网络或显式代理反馈。

Result: 在BBH、MATH和MMLU上取得比单智能体和多智能体基线显著的性能提升。

Conclusion: 训练单个灵活领导者用于多智能体大语言模型系统协作推理有效且高效。

Abstract: Large Language Models (LLMs) have achieved strong performance on a wide range
of complex reasoning tasks, yet further gains are often possible by leveraging
the complementary strengths of multiple models. While multi-agent frameworks
can improve solution quality by leveraging multiple LLMs, existing methods are
often computationally expensive, both at training and inference time. In this
work, we introduce a hierarchical multi-agent framework that addresses these
challenges by training only a single leader LLM to coordinate a team of
untrained peer agents. To this end, we propose Multi-agent guided Leader Policy
\textbf{O}ptimization (MLPO), a novel approach which trains the leader to
evaluate and synthesize agent responses without auxiliary value networks or
explicit agent feedback. Leaders trained with MLPO exhibit improved performance
not only when interacting with the agent team at inference time, but also enjoy
improved performance when deployed in single-agent settings without the team.
Empirical results on Big-Bench Hard (BBH), MATH, and MMLU demonstrate that our
framework achieves substantial performance improvements over both single-agent
and multi-agent baselines. Our results highlight the effectiveness and
efficiency of training a single, flexible leader for collaborative reasoning in
multi-agent LLM systems.

</details>


### [485] [TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit](https://arxiv.org/abs/2507.09788)
*Paulo Salem,Robert Sim,Christopher Olsen,Prerit Saxena,Rafael Barcelos,Yi Ding*

Main category: cs.MA

TL;DR: 介绍用于大语言模型多智能体系统的仿真工具TinyTroupe，它能定义详细角色并提供有效解决方案，有示例及评估，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统工具在模拟人类行为方面存在不足，缺乏关键能力，限制了相关应用。

Method: 引入TinyTroupe仿真工具，通过多种大语言模型驱动机制实现详细角色定义和编程控制，用示例展示组件，进行定量和定性评估。

Result: 展示了TinyTroupe在头脑风暴和市场调研等场景的应用，突出其可能性、局限性和权衡。

Conclusion: TinyTroupe是新的概念贡献，可部分或全部融入其他场景，代码开源。

Abstract: Recent advances in Large Language Models (LLM) have led to a new class of
autonomous agents, renewing and expanding interest in the area. LLM-powered
Multiagent Systems (MAS) have thus emerged, both for assistive and simulation
purposes, yet tools for realistic human behavior simulation -- with its
distinctive challenges and opportunities -- remain underdeveloped. Existing MAS
libraries and tools lack fine-grained persona specifications, population
sampling facilities, experimentation support, and integrated validation, among
other key capabilities, limiting their utility for behavioral studies, social
simulation, and related applications. To address these deficiencies, in this
work we introduce TinyTroupe, a simulation toolkit enabling detailed persona
definitions (e.g., nationality, age, occupation, personality, beliefs,
behaviors) and programmatic control via numerous LLM-driven mechanisms. This
allows for the concise formulation of behavioral problems of practical
interest, either at the individual or group level, and provides effective means
for their solution. TinyTroupe's components are presented using representative
working examples, such as brainstorming and market research sessions, thereby
simultaneously clarifying their purpose and demonstrating their usefulness.
Quantitative and qualitative evaluations of selected aspects are also provided,
highlighting possibilities, limitations, and trade-offs. The approach, though
realized as a specific Python implementation, is meant as a novel conceptual
contribution, which can be partially or fully incorporated in other contexts.
The library is available as open source at
https://github.com/microsoft/tinytroupe.

</details>


### [486] [Large Population Models](https://arxiv.org/abs/2507.09901)
*Ayush Chopra*

Main category: cs.MA

TL;DR: 本文介绍大群体模型（LPMs）可模拟群体行为，有三项创新，能连接个体与群体动态，为AI研究提供新路径，并提及实现框架与开放问题。


<details>
  <summary>Details</summary>
Motivation: 社会诸多挑战源于大量自主个体决策，需要方法理解复杂系统。

Method: LPMs通过高效模拟计算方法、从多元数据学习的数学框架和隐私通信协议三项创新，扩展传统建模方法。

Result: 能观察个体行为聚合的系统结果，可在现实实施前测试干预措施，开发出“数字社会”揭示涌现现象。

Conclusion: LPMs为AI研究提供补充路径，可用于研究集体智能，测试政策和社会创新。

Abstract: Many of society's most pressing challenges, from pandemic response to supply
chain disruptions to climate adaptation, emerge from the collective behavior of
millions of autonomous agents making decisions over time. Large Population
Models (LPMs) offer an approach to understand these complex systems by
simulating entire populations with realistic behaviors and interactions at
unprecedented scale. LPMs extend traditional modeling approaches through three
key innovations: computational methods that efficiently simulate millions of
agents simultaneously, mathematical frameworks that learn from diverse
real-world data streams, and privacy-preserving communication protocols that
bridge virtual and physical environments. This allows researchers to observe
how agent behavior aggregates into system-level outcomes and test interventions
before real-world implementation. While current AI advances primarily focus on
creating "digital humans" with sophisticated individual capabilities, LPMs
develop "digital societies" where the richness of interactions reveals emergent
phenomena. By bridging individual agent behavior and population-scale dynamics,
LPMs offer a complementary path in AI research illuminating collective
intelligence and providing testing grounds for policies and social innovations
before real-world deployment. We discuss the technical foundations and some
open problems here. LPMs are implemented by the AgentTorch framework
(github.com/AgentTorch/AgentTorch)

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [487] [AMix-1: A Pathway to Test-Time Scalable Protein Foundation Model](https://arxiv.org/abs/2507.08920)
*Changze Lv,Jiang Zhou,Siyu Long,Lihao Wang,Jiangtao Feng,Dongyu Xue,Yu Pei,Hao Wang,Zherui Zhang,Yuchen Cai,Zhiqiang Gao,Ziyuan Ma,Jiakai Hu,Chaochen Gao,Jingjing Gong,Yuxuan Song,Shuyi Zhang,Xiaoqing Zheng,Deyi Xiong,Lei Bai,Ya-Qin Zhang,Wei-Ying Ma,Bowen Zhou,Hao Zhou*

Main category: q-bio.BM

TL;DR: 介绍基于贝叶斯流网络的蛋白质基础模型AMix - 1，详述训练方法及成果，如设计改进的AmeR变体，还采用算法推动蛋白质工程发展。


<details>
  <summary>Details</summary>
Motivation: 构建强大的蛋白质基础模型，推动蛋白质工程发展，实现蛋白质的有效设计。

Method: 基于贝叶斯流网络，运用系统训练方法，包括预训练缩放定律、涌现能力分析等；设计基于多序列比对的上下文学习策略；采用进化测试时间缩放算法。

Result: 构建出17亿参数的强大模型AMix - 1，成功设计出活性比野生型提高达50倍的AmeR变体，算法带来可扩展的性能提升。

Conclusion: AMix - 1及其相关方法为下一代实验室循环蛋白质设计奠定基础。

Abstract: We introduce AMix-1, a powerful protein foundation model built on Bayesian
Flow Networks and empowered by a systematic training methodology, encompassing
pretraining scaling laws, emergent capability analysis, in-context learning
mechanism, and test-time scaling algorithm. To guarantee robust scalability, we
establish a predictive scaling law and reveal the progressive emergence of
structural understanding via loss perspective, culminating in a strong
1.7-billion model. Building on this foundation, we devise a multiple sequence
alignment (MSA)-based in-context learning strategy to unify protein design into
a general framework, where AMix-1 recognizes deep evolutionary signals among
MSAs and consistently generates structurally and functionally coherent
proteins. This framework enables the successful design of a dramatically
improved AmeR variant with an up to $50\times$ activity increase over its wild
type. Pushing the boundaries of protein engineering, we further empower AMix-1
with an evolutionary test-time scaling algorithm for in silico directed
evolution that delivers substantial, scalable performance gains as verification
budgets are intensified, laying the groundwork for next-generation
lab-in-the-loop protein design.

</details>


### [488] [Conformation-Aware Structure Prediction of Antigen-Recognizing Immune Proteins](https://arxiv.org/abs/2507.09054)
*Frédéric A. Dreyer,Jan Ludwiczak,Karolis Martinkus,Brennan Abanades,Robert G. Alberstein,Pan Kessel,Pranav Rao,Jae Hyeon Lee,Richard Bonneau,Andrew M. Watkins,Franziska Seeger*

Main category: q-bio.BM

TL;DR: 介绍泛免疫球蛋白结构预测模型Ibex，在抗体、纳米抗体和T细胞受体可变域建模中达先进水平，区分结合与未结合构象，性能优且计算需求低。


<details>
  <summary>Details</summary>
Motivation: 开发能准确预测抗体、纳米抗体和T细胞受体可变域结构，且区分结合与未结合构象的模型，以加速大分子设计和治疗开发。

Method: 在标记的游离态和结合态结构对上训练，使用高分辨率抗体结构的综合私有数据集。

Result: 在分布外性能上优于现有专业和通用蛋白质结构预测工具，结合前沿模型准确性且降低计算需求。

Conclusion: Ibex为加速大分子设计和治疗开发提供了坚实基础。

Abstract: We introduce Ibex, a pan-immunoglobulin structure prediction model that
achieves state-of-the-art accuracy in modeling the variable domains of
antibodies, nanobodies, and T-cell receptors. Unlike previous approaches, Ibex
explicitly distinguishes between bound and unbound protein conformations by
training on labeled apo and holo structural pairs, enabling accurate prediction
of both states at inference time. Using a comprehensive private dataset of
high-resolution antibody structures, we demonstrate superior
out-of-distribution performance compared to existing specialized and general
protein structure prediction tools. Ibex combines the accuracy of cutting-edge
models with significantly reduced computational requirements, providing a
robust foundation for accelerating large molecule design and therapeutic
development.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [489] [Dynamical stability for dense patterns in discrete attractor neural networks](https://arxiv.org/abs/2507.10383)
*Uri Cohen,Máté Lengyel*

Main category: cond-mat.dis-nn

TL;DR: 推导了一类网络离散不动点局部稳定性理论，指出固定点在临界负载下稳定，凸显特定激活和模式的计算优势。


<details>
  <summary>Details</summary>
Motivation: 以往神经网络动态稳定性只能在严格条件下保证，需研究更广泛条件下的稳定性。

Method: 直接分析雅可比谱的主体和异常值。

Result: 所有固定点在一个不同于经典临界容量的临界负载下稳定，该临界负载与固定点神经活动统计及单神经元激活函数有关。

Conclusion: 阈值线性激活和类稀疏模式有计算优势。

Abstract: Neural networks storing multiple discrete attractors are canonical models of
biological memory. Previously, the dynamical stability of such networks could
only be guaranteed under highly restrictive conditions. Here, we derive a
theory of the local stability of discrete fixed points in a broad class of
networks with graded neural activities and in the presence of noise. By
directly analyzing the bulk and outliers of the Jacobian spectrum, we show that
all fixed points are stable below a critical load that is distinct from the
classical \textit{critical capacity} and depends on the statistics of neural
activities in the fixed points as well as the single-neuron activation
function. Our analysis highlights the computational benefits of
threshold-linear activation and sparse-like patterns.

</details>


### [490] [A CLuP algorithm to practically achieve $\sim 0.76$ SK--model ground state free energy](https://arxiv.org/abs/2507.09247)
*Mihailo Stojnic*

Main category: cond-mat.dis-nn

TL;DR: 本文探讨SK自旋玻璃模型基态自由能算法确定问题，提出CLuP - SK算法，其性能与理论预测相符，使计算近基态自由能通常变得容易。


<details>
  <summary>Details</summary>
Motivation: 解决SK模型在多项式谱方法下存在的常数计算差距问题，即能否消除该差距。

Method: 借鉴CLuP算法，设计CLuP - SK算法，关联随机CLuP - SK模型，通过全提升随机对偶理论（fl RDT）进行分析。

Result: 算法性能与理论预测高度一致，对于几千量级的n，CLuP - SK能达到约0.76的基态自由能，接近理论极限约0.763。

Conclusion: 对于实际应用，计算SK模型近基态自由能通常是一个容易的问题。

Abstract: We consider algorithmic determination of the $n$-dimensional
Sherrington-Kirkpatrick (SK) spin glass model ground state free energy. It
corresponds to a binary maximization of an indefinite quadratic form and under
the \emph{worst case} principles of the classical NP complexity theory it is
hard to approximate within a $\log(n)^{const.}$ factor. On the other hand, the
SK's random nature allows (polynomial) spectral methods to \emph{typically}
approach the optimum within a constant factor. Naturally one is left with the
fundamental question: can the residual (constant) \emph{computational gap} be
erased?
  Following the success of \emph{Controlled Loosening-up} (CLuP) algorithms in
planted models, we here devise a simple practical CLuP-SK algorithmic procedure
for (non-planted) SK models. To analyze the \emph{typical} success of the
algorithm we associate to it (random) CLuP-SK models. Further connecting to
recent random processes studies [94,97], we characterize the models and CLuP-SK
algorithm via fully lifted random duality theory (fl RDT) [98]. Moreover,
running the algorithm we demonstrate that its performance is in an excellent
agrement with theoretical predictions. In particular, already for $n$ on the
order of a few thousands CLuP-SK achieves $\sim 0.76$ ground state free energy
and remarkably closely approaches theoretical $n\rightarrow\infty$ limit
$\approx 0.763$. For all practical purposes, this renders computing SK model's
near ground state free energy as a \emph{typically} easy problem.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [491] [CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design](https://arxiv.org/abs/2507.09792)
*Prashant Govindarajan,Davide Baldelli,Jay Pathak,Quentin Fournier,Sarath Chandar*

Main category: cs.GR

TL;DR: 引入含170k+CAD模型的大规模数据集，微调代码大语言模型实现文本驱动CAD生成，引入新指标评估，实验证明能加速CAD设计，数据代码模型开源。


<details>
  <summary>Details</summary>
Motivation: CAD建模耗时且依赖人工，此前较少利用大语言模型进行顺序CAD设计。

Method: 构建基于GPT - 4.1的流水线生成带高质量描述的大规模CAD模型数据集，微调代码大语言模型生成JSON格式CAD序列，引入几何和拓扑指标评估。

Result: CADmium能自动化CAD设计，大幅加速新对象设计。

Conclusion: 利用大语言模型进行文本驱动CAD生成的方法可行有效，数据集、代码和模型有助于相关研究。

Abstract: Computer-aided design (CAD) is the digital construction of 2D and 3D objects,
and is central to a wide range of engineering and manufacturing applications
like automobile and aviation. Despite its importance, CAD modeling remains
largely a time-intensive, manual task. Recent works have attempted to automate
this process with small transformer-based models and handcrafted CAD sequence
representations. However, there has been little effort to leverage the
potential of large language models (LLMs) for sequential CAD design. In this
work, we introduce a new large-scale dataset of more than 170k CAD models
annotated with high-quality, human-like descriptions generated with our
pipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs
to generate CAD sequences represented in a JSON-based format from natural
language descriptions, demonstrating the viability and effectiveness of this
approach for text-conditioned CAD generation. Because simple metrics often fail
to reflect the quality of generated objects, we introduce geometric and
topological metrics based on sphericity, mean curvature, and Euler
characteristic to provide richer structural insights. Our experiments and
ablation studies on both synthetic and human-annotated data demonstrate that
CADmium is able to automate CAD design, drastically speeding up the design of
new objects. The dataset, code, and fine-tuned models are available online.

</details>


### [492] [ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions](https://arxiv.org/abs/2507.10542)
*Shivangi Aneja,Sebastian Weiss,Irene Baeza,Prashanth Chandran,Gaspard Zoss,Matthias Nießner,Derek Bradley*

Main category: cs.GR

TL;DR: 提出结合局部面部表情与3D高斯 splatting 生成超逼真3D头部头像，在实时生成中表现出色。


<details>
  <summary>Details</summary>
Motivation: 在图形应用中生成高保真实时动画的逼真3D头部头像很重要，尤其是渲染特写时，现有方法存在挑战。

Method: 将局部面部表情与3D高斯 splatting 结合，基于 patch 提取表情，利用锚点合成3D高斯，采用基于颜色的致密化和渐进式训练。

Result: 对于3K高分辨率训练图像能获得高质量结果和更快收敛。

Conclusion: 通过利用 patch 级表情，ScaffoldAvatar 实时实现了视觉自然运动的最优性能，涵盖多样面部表情和风格。

Abstract: Generating high-fidelity real-time animated sequences of photorealistic 3D
head avatars is important for many graphics applications, including immersive
telepresence and movies. This is a challenging problem particularly when
rendering digital avatar close-ups for showing character's facial microfeatures
and expressions. To capture the expressive, detailed nature of human heads,
including skin furrowing and finer-scale facial movements, we propose to couple
locally-defined facial expressions with 3D Gaussian splatting to enable
creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In
contrast to previous works that operate on a global expression space, we
condition our avatar's dynamics on patch-based local expression features and
synthesize 3D Gaussians at a patch level. In particular, we leverage a
patch-based geometric 3D face model to extract patch expressions and learn how
to translate these into local dynamic skin appearance and motion by coupling
the patches with anchor points of Scaffold-GS, a recent hierarchical scene
representation. These anchors are then used to synthesize 3D Gaussians
on-the-fly, conditioned by patch-expressions and viewing direction. We employ
color-based densification and progressive training to obtain high-quality
results and faster convergence for high resolution 3K training images. By
leveraging patch-level expressions, ScaffoldAvatar consistently achieves
state-of-the-art performance with visually natural motion, while encompassing
diverse facial expressions and styles in real time.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [493] [Aligning Generative Speech Enhancement with Human Preferences via Direct Preference Optimization](https://arxiv.org/abs/2507.09929)
*Haoyang Li,Nana Hou,Yuchen Hu,Jixun Yao,Sabato Marco Siniscalchi,Eng Siong Chng*

Main category: eess.AS

TL;DR: 本文从语言模型角度研究语音增强，提出用DPO提升增强语音感知质量，实验证明有效，为语音增强指明方向。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型的语音增强方法可能与人类感知不一致，导致质量下降，需要改进。

Method: 提出利用直接偏好优化（DPO）方法，用UTMOS作为人类评分代理引导优化。

Result: 在2020深度噪声抑制挑战赛测试集上，将DPO应用于预训练的基于语言模型的语音增强模型，各项语音质量指标有显著提升，相对增益达56%。

Conclusion: 这是DPO首次应用于语音增强，也是首次将代理感知反馈纳入基于语言模型的语音增强训练，为感知对齐的语音增强提供了有前景的方向。

Abstract: This work investigates speech enhancement (SE) from the perspective of
language models (LMs). We propose a novel method that leverages Direct
Preference Optimization (DPO) to improve the perceptual quality of enhanced
speech. Using UTMOS, a neural MOS prediction model, as a proxy for human
ratings, our approach guides optimization toward perceptually preferred
outputs. This differs from existing LM-based SE methods that focus on
maximizing the likelihood of clean speech tokens, which may misalign with human
perception and degrade quality despite low prediction error. Experiments on the
2020 Deep Noise Suppression Challenge test sets demonstrate that applying DPO
to a pretrained LM-based SE model yields consistent improvements across various
speech quality metrics, with relative gains of up to 56%. To our knowledge,
this is the first application of DPO to SE and the first to incorporate proxy
perceptual feedback into LM-based SE training, pointing to a promising
direction for perceptually aligned SE.

</details>


### [494] [Natural Language-based Assessment of L2 Oral Proficiency using LLMs](https://arxiv.org/abs/2507.10200)
*Stefano Bannò,Rao Ma,Mengjie Qian,Siyuan Tang,Kate Knill,Mark Gales*

Main category: eess.AS

TL;DR: 探索用Qwen 2.5 72B基于自然语言描述符对S&I语料库进行零样本评估，结果显示该方法有竞争力且有多种优势。


<details>
  <summary>Details</summary>
Motivation: 确定大语言模型能否像人类评估一样解读和应用基于自然语言的评估描述符。

Method: 使用开源大语言模型Qwen 2.5 72B在零样本设置下对S&I语料库的回答进行评估。

Result: 仅依赖文本信息的方法有竞争力，虽未超越微调的语音大语言模型，但超越了专门训练的基于BERT的模型。

Conclusion: 自然语言评估在不匹配任务设置中有效，可推广到其他数据类型和语言，且具有更强的可解释性。

Abstract: Natural language-based assessment (NLA) is an approach to second language
assessment that uses instructions - expressed in the form of can-do descriptors
- originally intended for human examiners, aiming to determine whether large
language models (LLMs) can interpret and apply them in ways comparable to human
assessment. In this work, we explore the use of such descriptors with an
open-source LLM, Qwen 2.5 72B, to assess responses from the publicly available
S&I Corpus in a zero-shot setting. Our results show that this approach -
relying solely on textual information - achieves competitive performance: while
it does not outperform state-of-the-art speech LLMs fine-tuned for the task, it
surpasses a BERT-based model trained specifically for this purpose. NLA proves
particularly effective in mismatched task settings, is generalisable to other
data types and languages, and offers greater interpretability, as it is
grounded in clearly explainable, widely applicable language descriptors.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [495] [The Second Machine Turn: From Checking Proofs to Creating Concepts](https://arxiv.org/abs/2507.10179)
*Asvin G*

Main category: math.HO

TL;DR: 本文指出数学发现中的第二阶段，即AI可自动化创建数学概念，探讨现状、障碍、解决方案，评估其对数学和人机协作的影响。


<details>
  <summary>Details</summary>
Motivation: 关注AI在数学发现中从自动证明检查到自动创建数学概念的发展，探索其潜力和影响。

Method: 讨论当前技术水平、障碍及潜在解决方案，尝试将概念创建过程数学化。

Result: 无明确提及具体结果。

Conclusion: 评估了AI自动创建概念能力对数学和人机协作的重塑作用，并展望了不同未来。

Abstract: We identify a second machine turn in the process of mathematical discovery:
after automating proof-checking, AI is now poised to automate the *creation* of
mathematical concepts themselves. We discuss the current state of the art,
obstacles and potential solutions as well as a preliminary attempt at
mathematizing the creation of concepts itself. The paper ends with an
assessment of how these capabilities could reshape mathematics and
human-machine collaboration, and a few different futures we might find
ourselves in.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [496] [Machine-Precision Prediction of Low-Dimensional Chaotic Systems](https://arxiv.org/abs/2507.09652)
*Christof Schötz,Niklas Boers*

Main category: nlin.CD

TL;DR: 研究表明从无噪声观测中学习低维混沌系统可达机器精度，方法在多个系统上表现出色，表明从无噪声数据学习低维混沌系统问题已解决。


<details>
  <summary>Details</summary>
Motivation: 以低维混沌系统（如Lorenz - 63模型）为基准，探究从数据中学习动力学的系统无关方法。

Method: 在高次多项式特征上使用普通最小二乘回归，并采用512位算术运算。

Result: 精度超64位数值ODE求解器，Lorenz - 63系统有效预测时间达32到105个Lyapunov时间，远超先前工作；在更复杂系统及高维系统上结果类似。

Conclusion: 从无噪声数据学习低维混沌系统是已解决的问题。

Abstract: Low-dimensional chaotic systems such as the Lorenz-63 model are commonly used
to benchmark system-agnostic methods for learning dynamics from data. Here we
show that learning from noise-free observations in such systems can be achieved
up to machine precision: using ordinary least squares regression on high-degree
polynomial features with 512-bit arithmetic, our method exceeds the accuracy of
standard 64-bit numerical ODE solvers of the true underlying dynamical systems.
Depending on the configuration, we obtain valid prediction times of 32 to 105
Lyapunov times for the Lorenz-63 system, dramatically outperforming prior work
that reaches 13 Lyapunov times at most. We further validate our results on
Thomas' Cyclically Symmetric Attractor, a non-polynomial chaotic system that is
considerably more complex than the Lorenz-63 model, and show that similar
results extend also to higher dimensions using the spatiotemporally chaotic
Lorenz-96 model. Our findings suggest that learning low-dimensional chaotic
systems from noise-free data is a solved problem.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [497] [Central Bank Digital Currencies: A Survey](https://arxiv.org/abs/2507.08880)
*Qifeng Tang,Yain-Whar Si*

Main category: econ.GN

TL;DR: 本文全面回顾CBDC系统设计与实施的最新进展，分析相关研究论文，构建生态系统概念，对比现有系统，给出未来研究建议。


<details>
  <summary>Details</summary>
Motivation: 随着数字支付技术发展，各国央行探索CBDC实施，需对CBDC系统设计和实施进行全面研究。

Method: 分析2018 - 2025年135篇研究论文，基于CBDC设计金字塔细化架构元素，对26个现有CBDC系统进行四维度对比分析。

Result: 最常见配置为两层架构、分布式账本技术和基于代币的访问模型，应用领域无主导趋势，近期研究关注CBDC用于跨境支付。

Conclusion: 论文对CBDC设计和实施进行了全面研究，并给出未来研究的前瞻性建议。

Abstract: With the advancement of digital payment technologies, central banks worldwide
have increasingly begun to explore the implementation of Central Bank Digital
Currencies (CBDCs). This paper presents a comprehensive review of the latest
developments in CBDC system design and implementation. By analyzing 135
research papers published between 2018 and 2025, the study provides an in-depth
examination of CBDC design taxonomy and ecosystem frameworks. Grounded in the
CBDC Design Pyramid, the paper refines and expands key architectural elements
by thoroughly investigating innovations in ledger technologies, the selection
of consensus mechanisms, and challenges associated with offline payments and
digital wallet integration. Furthermore, it conceptualizes a CBDC ecosystem. A
detailed comparative analysis of 26 existing CBDC systems is conducted across
four dimensions: system architecture, ledger technology, access model, and
application domain. The findings reveal that the most common configuration
consists of a two-tier architecture, distributed ledger technology (DLT), and a
token-based access model. However, no dominant trend has emerged regarding
application domains. Notably, recent research shows a growing focus on
leveraging CBDCs for cross-border payments to resolve inefficiencies and
structural delays in current systems. Finally, the paper offers several
forward-looking recommendations for future research.

</details>


### [498] [Comrades and Cause: Peer Influence on West Point Cadets' Civil War Allegiances](https://arxiv.org/abs/2507.09419)
*Yuchen Guo,Matthew O. Jackson,Ruixue Jia*

Main category: econ.GN

TL;DR: 研究美国内战期间西点军校学员受同伴影响选择阵营的情况，发现自由州同学比例影响蓄奴州学员选择，且同伴在极端极化时期仍影响重大决策。


<details>
  <summary>Details</summary>
Motivation: 探究在高度极化环境下社交网络和同伴影响是否会塑造重大人生决策。

Method: 利用自由州学员比例的准随机变化，分析其对学员选择阵营决策的影响，并研究学员决策对军事军衔和职业结果的影响。

Result: 自由州同学比例越高，蓄奴州学员加入联邦军队的可能性显著增加，几乎所有自由州学员若参战都加入联邦军队。

Conclusion: 即使在极端极化时期，同伴仍会影响改变人生的选择。

Abstract: Do social networks and peer influence shape major life decisions in highly
polarized settings? We explore this question by examining how peers influenced
the allegiances of West Point cadets during the American Civil War. Leveraging
quasi-random variations in the proportion of cadets from Free States, we
analyze how these differences affected decisions about which army to join. We
find that a higher proportion of classmates from Free States significantly
increased the likelihood that cadets from Slave States joined the Union Army,
while almost all cadets from Free States joined the Union Army (if they decided
to join the war). We further examine how cadets' decisions affected their
military rank and career outcomes. Our findings highlight that peers still
influence choices even when they are life-altering and occur during periods of
extreme polarization.

</details>


### [499] [Integrated Warehouse Location and Inventory Decisions in a Multi-location Newsvendor Problem](https://arxiv.org/abs/2507.09631)
*Jianing Zhi,Xinghua Li,Zidong Chen*

Main category: econ.GN

TL;DR: 研究含供应商和多零售商的供应链网络，考虑三种运输成本构建六个模型，给出部分模型闭式解，为非线性问题开发Q - 搜索算法，实验表明集中式模型在大型网络中更优。


<details>
  <summary>Details</summary>
Motivation: 研究供应链网络中供应商直接接单和建仓库集中接单两种模式下的最优决策，考虑不同运输成本的影响。

Method: 构建六个模型，为部分分散式和集中式模型提供闭式解，为非线性的集中式模型开发Q - 搜索算法。

Result: 实验表明集中式模型在大型网络的利润和服务水平上优于分散式模型。

Conclusion: 在大型供应链网络中，集中式订单处理模式（建仓库集中接单）在利润和服务水平方面更具优势。

Abstract: In this paper, we investigate a supply chain network with a supplier and
multiple retailers. The supplier can either take orders from retailers
directly, or choose to build a warehouse somewhere in the network to centralize
the ordering from retailers. Meanwhile, we take three modes of transportation
cost into account, including distance-dependent, quantity-dependent, and
distance-quantity-dependent costs, to formulate six models. For the three
decentralized models, we provide closed-form solutions to compute the optimal
order quantity of each retailer. For the centralized models, we develop
closed-form solutions for the first two models as the transportation cost only
depends on either distance or order quantity; but when it depends on both, the
model becomes a non-linear programming problem. We develop a solution algorithm
named Q-search to find a high-quality solution that includes the order quantity
and the warehouse location. The experiment results show that the centralized
model outperforms the decentralized model in large networks in terms of profit
and service level.

</details>


### [500] [Selective Newsvendor Problem with Dependent Leadtime and Joint Marketing Decisions](https://arxiv.org/abs/2507.09635)
*Jianing Zhi,Guanqiu Qi,Xinghua Li*

Main category: econ.GN

TL;DR: 研究两级供应链网络联合决策模式，定义两模型，开发算法求解，分析参数敏感性，模型能解释各方行为并指导决策。


<details>
  <summary>Details</summary>
Motivation: 研究两级供应链网络的联合决策模式，考虑不同需求模式、提前期和服务水平因素。

Method: 定义AON - SNP和DLSNP模型，开发R - search算法求解DL - 选择性问题，进行参数敏感性分析。

Result: 若总需求低于订单量上限，Q最优策略是匹配需求；市场增长需求增加会导致短缺，公司需找本地供应商补充需求。

Conclusion: 模型能很好解释网络中各方行为，为公司智能决策提供指导。

Abstract: In this paper, we investigate a joint decision-making pattern for a two-stage
supply chain network, including a supplier, a company, and its customers. We
investigate two types of demand patterns, associated with dependent lead time
and service level considerations. We define two novel models, including
all-or-nothing selective newsvendor problem (AON-SNP) and selective news Vendor
Problem with Dependent Lead Time and Price Related Demands (DLSNP). The
proposed models are applicable to numerous areas such as the fashion,
furniture, and electronic industries. We develop an efficient solution
algorithm, referred to as R-search, to identify an optimal solution for the
DL-selectivity problem. We examine various responses of the system through
parameter sensitivity analysis. Our model proves that if the total demand is
lower than the upper limit of the order quantity, the best strategy for Q is to
match that demand. If the market increases, more demand comes in, leading to a
shortage, and forcing the company to find other local suppliers to fill the
additional demand. The results demonstrate that our model well explains various
behavior for all involved parties in the network, and provide guidance on
intelligent decision making for the company.

</details>


### [501] [The Effects of Flipped Classrooms in Higher Education: A Causal Machine Learning Analysis](https://arxiv.org/abs/2507.10140)
*Daniel Czarnowske,Florian Heiss,Theresa M. A. Schmitz,Amrei Stammann*

Main category: econ.GN

TL;DR: 研究用双/去偏机器学习评估德国免学费大学统计课从混合教学到翻转课堂的影响，发现学生自我认知和拖延行为有积极变化，但课堂乐趣减少，考试成绩等无显著提升，学生对课程实施不足。


<details>
  <summary>Details</summary>
Motivation: 评估从讲座式混合教学转变为翻转课堂对大型必修统计课程的影响。

Method: 使用双/去偏机器学习进行队列比较研究，还利用翻转课堂学生的详细使用数据。

Result: 学生自我认知改善、拖延行为减少，但课堂乐趣降低，考试成绩、通过率和知识保留无显著积极影响，学生对课程实施不足。

Conclusion: 需要额外策略确保学生从翻转课堂中真正受益。

Abstract: This study uses double/debiased machine learning to evaluate the impact of
transitioning from lecture-based blended teaching to a flipped classroom
concept in a cohort comparison of a large compulsory introductory statistics
course at a German tuition-free university. Our findings indicate positive
changes in students' self-conception and a reduction in procrastination
behaviors. However, we also observe a decline in the enjoyment of classroom
sessions. Contrary to theoretical expectations, we do not find significant
positive effects on exam scores, passing rates, or knowledge retention. Unlike
most studies, however, we can leverage detailed usage data from the flipped
cohort, including the timeliness and completeness of pre-class video watching,
as well as quiz participation patterns, to check how well students implemented
each part of the curriculum. Our findings suggest that, on average, students in
the flipped cohort implemented the instructional approach insufficiently,
explaining the mechanism of our null results in exam performance and knowledge
retention. This highlights the need for additional strategies to ensure that
students actually benefit from a flipped curriculum.

</details>


### [502] [The Green Premium Puzzle: Empirical Evidence from Climate-Friendly Food Products](https://arxiv.org/abs/2507.10333)
*Voraprapa Nakavachara,Chanon Thongtai,Thanarat Chalidabhongse,Chanathip Pharino*

Main category: econ.GN

TL;DR: 研究瑞典超市产品，发现气候友好型食品无价格溢价，部分品类有负溢价，提出绿色溢价谜题并分析原因。


<details>
  <summary>Details</summary>
Motivation: 探究气候友好型食品在消费市场是否有价格溢价。

Method: 使用瑞典一家超市的产品级数据，控制产品大小、营养成分和固定效应，研究包装正面气候影响得分与零售价的关系。

Result: 未发现气候友好型产品价格更高的证据，部分品类气候得分高的产品价格更低，存在负溢价。

Conclusion: 市场摩擦可能抑制奖励可持续消费的价格信号，研究为生产者、零售商和政策制定者提供见解。

Abstract: This paper investigates whether climate-friendly food products command a
price premium in consumer markets. Using product-level data from a supermarket
in Sweden, we examine the relationship between front-of-package climate impact
scores and retail prices, controlling for product size, nutritional content,
and fixed effects. Contrary to the intuitive expectation of a positive green
premium, we find no evidence that climate-friendly products are priced higher.
In some product categories, products with better climate scores are in fact
associated with lower prices, suggesting a negative premium, an outcome that
gives rise to what we refer to as the green premium puzzle. We argue that
market frictions such as competing consumer priorities, psychological distance
from climate issues, and skepticism toward environmental labeling may suppress
the price signals intended to reward sustainable consumption. These findings
offer important insights for producers, retailers, and policymakers seeking to
align climate goals with effective market incentives in the transition toward a
more sustainable society.

</details>


### [503] [Intimate partner violence and women's economic preferences](https://arxiv.org/abs/2507.10416)
*Dan Anderberg,Rachel Cassidy,Anaya Dam,Melissa Hidrobo,Jessica Leight,Karlijn Morsink*

Main category: econ.GN

TL;DR: 研究揭示亲密伴侣暴力（IPV）影响女性时间偏好，促使其更看重当下。


<details>
  <summary>Details</summary>
Motivation: 全球三分之一女性经历IPV，但此类创伤对经济决策的影响尚不明确，故研究IPV对女性时间偏好的影响。

Method: 结合两种实证策略，使用四个不同数据集。一是在埃塞俄比亚进行随机回忆实验；二是利用两项随机干预导致的IPV外生减少，以治疗分配作为IPV暴露的工具变量。

Result: 回忆IPV经历的女性更不耐烦；因治疗减少IPV的女性更有耐心。

Conclusion: IPV会使个体更看重当下，暴力可通过心理渠道加剧经济劣势，限制女性采取需长期规划的行动。

Abstract: One in three women globally experiences intimate partner violence (IPV), yet
little is known about how such trauma affects economic decision-making. We
provide causal evidence that IPV influences women's time preferences - a key
parameter in models of savings, investment, and labor supply. We combine two
empirical strategies using four distinct datasets. First, in two randomized
recall experiments in Ethiopia, we randomly assigned women to recall specific
acts of abuse before eliciting their intertemporal choices. Women with IPV
experiences prompted to recall IPV display significantly greater impatience
than otherwise similar women who are not prompted. Second, we exploit exogenous
reductions in IPV generated by two randomized interventions - one involving
cash transfers, the other psychotherapy - and use treatment assignment as an
instrument for IPV exposure. Women who experience reduced IPV as a result of
treatment exhibit more patient time preferences. Together, these results
provide consistent, novel causal evidence that exposure to IPV induces
individuals to discount the future more heavily. This evidence suggests a
psychological channel through which violence can perpetuate economic
disadvantage and constrain women's ability to take actions - such as saving,
investing, or exiting abusive relationships - that require planning over time.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [504] [AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data](https://arxiv.org/abs/2507.09100)
*Mohammad Abolnejadian,Shakiba Amirshahi,Matthew Brehmer,Anamaria Crisan*

Main category: cs.HC

TL;DR: 本文以医患对话为例，实现对话式用户界面，利用基于检索的大语言模型代理生成见解并评估原型，显示出有效性和挑战。


<details>
  <summary>Details</summary>
Motivation: 决策对话实时性使决策者难以回顾和利用相关信息，探索专家能否在实时决策中利用过去数据的见解。

Method: 实现对话式用户界面，以医患交互为例，系统监听对话、识别问题和解决方案，从嵌入式数据集检索相关数据，用基于检索的大语言模型代理生成见解。

Result: 将加拿大卫生部数据集嵌入向量数据库，用样本医患对话进行模拟研究，显示出有效性但也存在挑战。

Conclusion: 研究显示出一定效果但有挑战，为后续工作指明方向。

Abstract: In decision-making conversations, experts must navigate complex choices and
make on-the-spot decisions while engaged in conversation. Although extensive
historical data often exists, the real-time nature of these scenarios makes it
infeasible for decision-makers to review and leverage relevant information.
This raises an interesting question: What if experts could utilize relevant
past data in real-time decision-making through insights derived from past data?
To explore this, we implemented a conversational user interface, taking
doctor-patient interactions as an example use case. Our system continuously
listens to the conversation, identifies patient problems and doctor-suggested
solutions, and retrieves related data from an embedded dataset, generating
concise insights using a pipeline built around a retrieval-based Large Language
Model (LLM) agent. We evaluated the prototype by embedding Health Canada
datasets into a vector database and conducting simulated studies using sample
doctor-patient dialogues, showing effectiveness but also challenges, setting
directions for the next steps of our work.

</details>


### [505] [SimStep: Chain-of-Abstractions for Incremental Specification and Debugging of AI-Generated Interactive Simulations](https://arxiv.org/abs/2507.09664)
*Zoe Kaputa,Anika Rajaram,Vryan Almanon Feliciano,Zhuoyue Lyu,Maneesh Agrawala,Hari Subramonyam*

Main category: cs.HC

TL;DR: 提出CoA框架用于编程提示范式，以恢复编程核心功能，在SimStep环境中实现并经教育者评估有效。


<details>
  <summary>Details</summary>
Motivation: 编程提示范式虽对非程序员友好，但丢失编程核心功能，需方法恢复这些功能。

Method: 提出CoA框架，将合成过程分解为一系列有认知意义的表示，在SimStep环境实现并设置逆校正过程。

Result: 教育者评估显示CoA在编程提示工作流中能实现更大创作控制和可解释性。

Conclusion: CoA框架能在保留自然语言表达灵活性的同时恢复编程核心功能。

Abstract: Programming-by-prompting with generative AI offers a new paradigm for
end-user programming, shifting the focus from syntactic fluency to semantic
intent. This shift holds particular promise for non-programmers such as
educators, who can describe instructional goals in natural language to generate
interactive learning content. Yet in bypassing direct code authoring, many of
programming's core affordances - such as traceability, stepwise refinement, and
behavioral testing - are lost. We propose the Chain-of-Abstractions (CoA)
framework as a way to recover these affordances while preserving the expressive
flexibility of natural language. CoA decomposes the synthesis process into a
sequence of cognitively meaningful, task-aligned representations that function
as checkpoints for specification, inspection, and refinement. We instantiate
this approach in SimStep, an authoring environment for teachers that scaffolds
simulation creation through four intermediate abstractions: Concept Graph,
Scenario Graph, Learning Goal Graph, and UI Interaction Graph. To address
ambiguities and misalignments, SimStep includes an inverse correction process
that surfaces in-filled model assumptions and enables targeted revision without
requiring users to manipulate code. Evaluations with educators show that CoA
enables greater authoring control and interpretability in
programming-by-prompting workflows.

</details>


### [506] [Visual Analytics for Explainable and Trustworthy Artificial Intelligence](https://arxiv.org/abs/2507.10240)
*Angelos Chatzimparmpas*

Main category: cs.HC

TL;DR: 社会依赖智能系统解决复杂问题，AI在医疗诊断有潜力但缺乏透明度，视觉分析（VA）结合AI与可视化可解决此问题，本文定义、分类并探索VA如何在AI管道各阶段建立信任，提出可视化设计空间并介绍VA仪表盘。


<details>
  <summary>Details</summary>
Motivation: 社会依赖智能系统，AI在医疗诊断有巨大潜力，但缺乏透明度阻碍其应用，需要解决信任问题。

Method: 定义、分类VA解决方案，提出创新可视化设计空间，介绍已开发的VA仪表盘。

Result: 提出了创新可视化设计空间，展示了支持AI管道各阶段关键任务的VA仪表盘。

Conclusion: VA能结合AI与可视化，在AI管道各阶段建立信任，弥合AI与人类理解的差距。

Abstract: Our society increasingly depends on intelligent systems to solve complex
problems, ranging from recommender systems suggesting the next movie to watch
to AI models assisting in medical diagnoses for hospitalized patients. With the
iterative improvement of diagnostic accuracy and efficiency, AI holds
significant potential to mitigate medical misdiagnoses by preventing numerous
deaths and reducing an economic burden of approximately 450 EUR billion
annually. However, a key obstacle to AI adoption lies in the lack of
transparency: many automated systems function as "black boxes," providing
predictions without revealing the underlying processes. This opacity can hinder
experts' ability to trust and rely on AI systems. Visual analytics (VA)
provides a compelling solution by combining AI models with interactive
visualizations. These specialized charts and graphs empower users to
incorporate their domain expertise to refine and improve the models, bridging
the gap between AI and human understanding. In this work, we define,
categorize, and explore how VA solutions can foster trust across the stages of
a typical AI pipeline. We propose a design space for innovative visualizations
and present an overview of our previously developed VA dashboards, which
support critical tasks within the various pipeline stages, including data
processing, feature engineering, hyperparameter tuning, understanding,
debugging, refining, and comparing models.

</details>


### [507] [An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments](https://arxiv.org/abs/2507.10469)
*Mikko Korkiakoski,Saeid Sheikhi,Jesper Nyman,Jussi Saariniemi,Kalle Tapio,Panos Kostakos*

Main category: cs.HC

TL;DR: 本文评估VR审讯模拟器中AI驱动的NPC，研究其感知的真实感、可用性和系统性能，发现大语言模型提升NPC真实感与交互性有潜力，但存在降低延迟和增强情感深度的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI发展，评估AI驱动的NPC在VR审讯模拟器中的表现，以提升用户体验。

Method: 在VR审讯模拟器中用两个由GPT - 4 Turbo驱动的NPC与参与者互动，用SUS、GEQ和虚拟代理可信度问卷评估系统，测量语音和整体延迟。

Result: 平均循环延迟7秒，受对话上下文影响；可信度6.67分，行为、社交关系和智力评分高，情感和个性中等；SUS得分79.44，可用性良好。

Conclusion: 大语言模型有提升VR中NPC真实感和交互性的潜力，但需优化性能，减少延迟和增强情感深度以实现更沉浸式体验。

Abstract: Advancements in artificial intelligence (AI) have significantly enhanced the
realism and interactivity of non-player characters (NPCs) in virtual reality
(VR), creating more engaging and believable user experiences. This paper
evaluates AI-driven NPCs within a VR interrogation simulator, focusing on their
perceived realism, usability, and system performance. The simulator features
two AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage
participants in a scenario to determine the suspect's guilt or innocence. A
user study with 18 participants assessed the system using the System Usability
Scale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent
Believability Questionnaire, alongside latency measurements for speech-to-text
(STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency.
Results showed an average cycle latency of 7 seconds, influenced by the
increasing conversational context. Believability scored 6.67 out of 10, with
high ratings in behavior, social relationships, and intelligence but moderate
scores in emotion and personality. The system achieved a SUS score of 79.44,
indicating good usability. These findings demonstrate the potential of large
language models to improve NPC realism and interaction in VR while highlighting
challenges in reducing system latency and enhancing emotional depth. This
research contributes to the development of more sophisticated AI-driven NPCs,
revealing the need for performance optimization to achieve increasingly
immersive virtual experiences.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [508] [Automatic Contouring of Spinal Vertebrae on X-Ray using a Novel Sandwich U-Net Architecture](https://arxiv.org/abs/2507.09158)
*Sunil Munthumoduku Krishna Murthy,Kumar Rajamani,Srividya Tirunellai Rajamani,Yupei Li,Qiyang Sun,Bjoern W. Schuller*

Main category: eess.IV

TL;DR: 提出一种新型U - Net变体用于X射线图像胸椎分割，比基线模型Dice得分提高4.1%，提升分割准确性。


<details>
  <summary>Details</summary>
Motivation: 传统手动脊椎轮廓提取劳动强度大、耗时长且易出错，需要自动化方法提高效率和准确性。

Method: 提出一种新型U - Net变体，采用“三明治”U - Net结构和双激活函数。

Result: 相比基线U - Net模型，Dice得分提高4.1%。

Conclusion: 该方法提高了分割准确性，能可靠地提取脊椎轮廓。

Abstract: In spinal vertebral mobility disease, accurately extracting and contouring
vertebrae is essential for assessing mobility impairments and monitoring
variations during flexion-extension movements. Precise vertebral contouring
plays a crucial role in surgical planning; however, this process is
traditionally performed manually by radiologists or surgeons, making it
labour-intensive, time-consuming, and prone to human error. In particular,
mobility disease analysis requires the individual contouring of each vertebra,
which is both tedious and susceptible to inconsistencies. Automated methods
provide a more efficient alternative, enabling vertebra identification,
segmentation, and contouring with greater accuracy and reduced time
consumption. In this study, we propose a novel U-Net variation designed to
accurately segment thoracic vertebrae from anteroposterior view on X-Ray
images. Our proposed approach, incorporating a ``sandwich" U-Net structure with
dual activation functions, achieves a 4.1\% improvement in Dice score compared
to the baseline U-Net model, enhancing segmentation accuracy while ensuring
reliable vertebral contour extraction.

</details>


### [509] [PanoDiff-SR: Synthesizing Dental Panoramic Radiographs using Diffusion and Super-resolution](https://arxiv.org/abs/2507.09227)
*Sanyam Jain,Bruna Neves de Freitas,Andreas Basse-OConnor,Alexandros Iosifidis,Ruben Pauwels*

Main category: eess.IV

TL;DR: 本文提出结合扩散生成和超分辨率技术生成合成牙科全景X光片，实验显示其效果良好，专家区分真假图像准确率为68.5%。


<details>
  <summary>Details</summary>
Motivation: 缓解人工智能研究中公共医学图像数据集稀缺问题，同时用于教育目的。

Method: 结合基于扩散的生成（PanoDiff）和超分辨率（SR）技术生成合成牙科全景X光片，SR采用学习局部 - 全局关系的Transformer。

Result: 真实和合成的高分辨率图像之间的Fréchet inception距离分数为40.69；不同图像的Inception分数分别为2.55、2.30、2.90和2.98；专家区分真假图像平均准确率为68.5%。

Conclusion: 所提方法能有效生成合成牙科全景X光片。

Abstract: There has been increasing interest in the generation of high-quality,
realistic synthetic medical images in recent years. Such synthetic datasets can
mitigate the scarcity of public datasets for artificial intelligence research,
and can also be used for educational purposes. In this paper, we propose a
combination of diffusion-based generation (PanoDiff) and Super-Resolution (SR)
for generating synthetic dental panoramic radiographs (PRs). The former
generates a low-resolution (LR) seed of a PR (256 X 128) which is then
processed by the SR model to yield a high-resolution (HR) PR of size 1024 X
512. For SR, we propose a state-of-the-art transformer that learns local-global
relationships, resulting in sharper edges and textures. Experimental results
demonstrate a Frechet inception distance score of 40.69 between 7243 real and
synthetic images (in HR). Inception scores were 2.55, 2.30, 2.90 and 2.98 for
real HR, synthetic HR, real LR and synthetic LR images, respectively. Among a
diverse group of six clinical experts, all evaluating a mixture of 100
synthetic and 100 real PRs in a time-limited observation, the average accuracy
in distinguishing real from synthetic images was 68.5% (with 50% corresponding
to random guessing).

</details>


### [510] [AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)](https://arxiv.org/abs/2507.09759)
*Abdul Manaf,Nimra Mughal*

Main category: eess.IV

TL;DR: 研究提出基于机器学习的儿科胸部肺炎分类系统，用CNN模型训练图像，结合数据增强和GAN处理数据，部署网络应用，结果显示可提升诊断准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 肺炎是五岁以下儿童主要死因，需准确的胸部X光诊断，故构建系统辅助医疗人员诊断。

Method: 使用5863张0 - 5岁儿童胸部X光图像训练CNN模型，应用数据增强技术和GAN生成合成图像处理数据有限和类别不平衡问题。

Result: 系统使用原始、增强和GAN生成的数据结合取得最佳性能，通过准确率和F1分数评估，最终模型通过Flask网络应用部署实现实时分类。

Conclusion: 深度学习和GAN在提高儿科肺炎分类诊断准确性和效率方面有潜力，在资源有限的临床环境中很有价值。

Abstract: Pneumonia is a leading cause of mortality in children under five, requiring
accurate chest X-ray diagnosis. This study presents a machine learning-based
Pediatric Chest Pneumonia Classification System to assist healthcare
professionals in diagnosing pneumonia from chest X-ray images. The CNN-based
model was trained on 5,863 labeled chest X-ray images from children aged 0-5
years from the Guangzhou Women and Children's Medical Center. To address
limited data, we applied augmentation techniques (rotation, zooming, shear,
horizontal flipping) and employed GANs to generate synthetic images, addressing
class imbalance. The system achieved optimal performance using combined
original, augmented, and GAN-generated data, evaluated through accuracy and F1
score metrics. The final model was deployed via a Flask web application,
enabling real-time classification with probability estimates. Results
demonstrate the potential of deep learning and GANs in improving diagnostic
accuracy and efficiency for pediatric pneumonia classification, particularly
valuable in resource-limited clinical settings
https://github.com/AbdulManaf12/Pediatric-Chest-Pneumonia-Classification

</details>


### [511] [Multi-omic Prognosis of Alzheimer's Disease with Asymmetric Cross-Modal Cross-Attention Network](https://arxiv.org/abs/2507.08855)
*Yang Ming,Jiang Shi Zhong,Zhou Su Juan*

Main category: eess.IV

TL;DR: 本文提出新深度学习算法框架辅助AD诊断，融合多模态信息，用非对称跨模态交叉注意力机制，测试集准确率达94.88%。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络和简单特征拼接方法无法有效利用多模态数据互补信息，且易造成关键信息丢失，需新方法解决多模态特征融合问题。

Method: 提出新深度学习算法框架，融合脑PET、MRI、遗传数据和临床数据等多视图信息，使用非对称跨模态交叉注意力机制。

Result: 算法模型在测试集上准确率达到94.88%。

Conclusion: 新算法框架能准确检测AD、MCI和CN，非对称跨模态交叉注意力机制有重要作用。

Abstract: Alzheimer's Disease (AD) is an irreversible neurodegenerative disease
characterized by progressive cognitive decline as its main symptom. In the
research field of deep learning-assisted diagnosis of AD, traditional
convolutional neural networks and simple feature concatenation methods fail to
effectively utilize the complementary information between multimodal data, and
the simple feature concatenation approach is prone to cause the loss of key
information during the process of modal fusion. In recent years, the
development of deep learning technology has brought new possibilities for
solving the problem of how to effectively fuse multimodal features. This paper
proposes a novel deep learning algorithm framework to assist medical
professionals in AD diagnosis. By fusing medical multi-view information such as
brain fluorodeoxyglucose positron emission tomography (PET), magnetic resonance
imaging (MRI), genetic data, and clinical data, it can accurately detect the
presence of AD, Mild Cognitive Impairment (MCI), and Cognitively Normal (CN).
The innovation of the algorithm lies in the use of an asymmetric cross-modal
cross-attention mechanism, which can effectively capture the key information
features of the interactions between different data modal features. This paper
compares the asymmetric cross-modal cross-attention mechanism with the
traditional algorithm frameworks of unimodal and multimodal deep learning
models for AD diagnosis, and evaluates the importance of the asymmetric
cross-modal cross-attention mechanism. The algorithm model achieves an accuracy
of 94.88% on the test set.

</details>


### [512] [VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models](https://arxiv.org/abs/2507.08982)
*Hanene F. Z. Brachemi Meftah,Wassim Hamidouche,Sid Ahmed Fezza,Olivier Déforges*

Main category: eess.IV

TL;DR: 文章探讨视觉语言模型（VLMs）隐私保护问题，提出新攻击策略，实验表明可大幅降低目标区域检测率并保持图像语义完整。


<details>
  <summary>Details</summary>
Motivation: VLMs广泛应用引发用户隐私担忧，需解决模型处理或暴露私人视觉信息问题。

Method: 将VLMs隐私保护问题转化为对抗攻击问题，提出新攻击策略，选择性隐藏图像指定感兴趣区域（ROIs）信息。

Result: 在LLaVA、Instruct - BLIP和BLIP2 - T5三个模型实验中，目标ROIs检测率最多降低98%，且保持全局图像语义完整。

Conclusion: 本工作有助于多模态模型更注重隐私保护，为后续研究提供实用工具，代码已公开。

Abstract: Recent years have witnessed remarkable progress in developing Vision-Language
Models (VLMs) capable of processing both textual and visual inputs. These
models have demonstrated impressive performance, leading to their widespread
adoption in various applications. However, this widespread raises serious
concerns regarding user privacy, particularly when models inadvertently process
or expose private visual information. In this work, we frame the preservation
of privacy in VLMs as an adversarial attack problem. We propose a novel attack
strategy that selectively conceals information within designated Region Of
Interests (ROIs) in an image, effectively preventing VLMs from accessing
sensitive content while preserving the semantic integrity of the remaining
image. Unlike conventional adversarial attacks that often disrupt the entire
image, our method maintains high coherence in unmasked areas. Experimental
results across three state-of-the-art VLMs namely LLaVA, Instruct-BLIP, and
BLIP2-T5 demonstrate up to 98% reduction in detecting targeted ROIs, while
maintaining global image semantics intact, as confirmed by high similarity
scores between clean and adversarial outputs. We believe that this work
contributes to a more privacy conscious use of multimodal models and offers a
practical tool for further research, with the source code publicly available
at: https://github.com/hbrachemi/Vlm_defense-attack.

</details>


### [513] [Advanced U-Net Architectures with CNN Backbones for Automated Lung Cancer Detection and Segmentation in Chest CT Images](https://arxiv.org/abs/2507.09898)
*Alireza Golkarieha,Kiana Kiashemshakib,Sajjad Rezvani Boroujenic,Nasibeh Asadi Isakand*

Main category: eess.IV

TL;DR: 研究用不同CNN骨干的U - Net架构进行肺癌检测和分割，U - Net结合不同骨干在分割和分类上表现佳，优于现有模型，支持临床诊断。


<details>
  <summary>Details</summary>
Motivation: 满足临床对准确肺癌诊断工具的迫切需求。

Method: 预处理832张胸部CT图像，开发带ResNet50、VGG16和Xception骨干的U - Net模型进行分割，用CNN分类器和混合模型分类，采用5折交叉验证，用多指标评估。

Result: U - Net与ResNet50在癌肺分割表现最好，U - Net与VGG16在非癌肺分割最佳；U - Net与Xception的CNN模型分类表现佳，混合CNN - SVM - Xception模型也有不错表现，且框架优于现有模型。

Conclusion: U - Net结合先进CNN骨干是CT扫描中肺癌分割和分类的有效方法，支持早期诊断和临床决策。

Abstract: This study investigates the effectiveness of U-Net architectures integrated
with various convolutional neural network (CNN) backbones for automated lung
cancer detection and segmentation in chest CT images, addressing the critical
need for accurate diagnostic tools in clinical settings. A balanced dataset of
832 chest CT images (416 cancerous and 416 non-cancerous) was preprocessed
using Contrast Limited Adaptive Histogram Equalization (CLAHE) and resized to
128x128 pixels. U-Net models were developed with three CNN backbones: ResNet50,
VGG16, and Xception, to segment lung regions. After segmentation, CNN-based
classifiers and hybrid models combining CNN feature extraction with traditional
machine learning classifiers (Support Vector Machine, Random Forest, and
Gradient Boosting) were evaluated using 5-fold cross-validation. Metrics
included accuracy, precision, recall, F1-score, Dice coefficient, and ROC-AUC.
U-Net with ResNet50 achieved the best performance for cancerous lungs (Dice:
0.9495, Accuracy: 0.9735), while U-Net with VGG16 performed best for
non-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513). For
classification, the CNN model using U-Net with Xception achieved 99.1 percent
accuracy, 99.74 percent recall, and 99.42 percent F1-score. The hybrid
CNN-SVM-Xception model achieved 96.7 percent accuracy and 97.88 percent
F1-score. Compared to prior methods, our framework consistently outperformed
existing models. In conclusion, combining U-Net with advanced CNN backbones
provides a powerful method for both segmentation and classification of lung
cancer in CT scans, supporting early diagnosis and clinical decision-making.

</details>


### [514] [A Brain Tumor Segmentation Method Based on CLIP and 3D U-Net with Cross-Modal Semantic Guidance and Multi-Level Feature Fusion](https://arxiv.org/abs/2507.09966)
*Mingda Zhang*

Main category: eess.IV

TL;DR: 提出多级别融合架构进行脑肿瘤分割，在BraTS 2020数据集验证有效果提升


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在脑肿瘤自动分割上因肿瘤形态异质性和复杂空间关系存在挑战，且未充分利用医学报告语义知识

Method: 提出多级别融合架构，语义级融合结合CLIP模型语义理解能力与3D U - Net空间特征提取优势，通过3D - 2D语义桥接、跨模态语义引导和基于语义的注意力机制

Result: 在BraTS 2020数据集上，模型整体Dice系数达0.8567，比传统3D U - Net提升4.8%，增强肿瘤区域Dice系数提升7.3%

Conclusion: 该多级别融合架构在脑肿瘤分割上有更好表现

Abstract: Precise segmentation of brain tumors from magnetic resonance imaging (MRI) is
essential for neuro-oncology diagnosis and treatment planning. Despite advances
in deep learning methods, automatic segmentation remains challenging due to
tumor morphological heterogeneity and complex three-dimensional spatial
relationships. Current techniques primarily rely on visual features extracted
from MRI sequences while underutilizing semantic knowledge embedded in medical
reports. This research presents a multi-level fusion architecture that
integrates pixel-level, feature-level, and semantic-level information,
facilitating comprehensive processing from low-level data to high-level
concepts. The semantic-level fusion pathway combines the semantic understanding
capabilities of Contrastive Language-Image Pre-training (CLIP) models with the
spatial feature extraction advantages of 3D U-Net through three mechanisms:
3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based
attention mechanisms. Experimental validation on the BraTS 2020 dataset
demonstrates that the proposed model achieves an overall Dice coefficient of
0.8567, representing a 4.8% improvement compared to traditional 3D U-Net, with
a 7.3% Dice coefficient increase in the clinically important enhancing tumor
(ET) region.

</details>


### [515] [DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in Histopathology](https://arxiv.org/abs/2507.10250)
*Ashkan Shakarami,Lorenzo Nicole,Rocco Cappellesso,Angelo Paolo Dei Tos,Stefano Ghidoni*

Main category: eess.IV

TL;DR: 本文介绍可部署的AI系统DepViT - CAD用于组织病理学多类癌症诊断，核心是MAViT，经大规模验证效果良好，代码将公开。


<details>
  <summary>Details</summary>
Motivation: 实现从组织病理切片进行准确及时的癌症诊断，以辅助临床决策。

Method: 采用新型多注意力视觉Transformer MAViT，在1008张全切片图像的专家标注补丁上训练，在两个独立队列上验证。

Result: 在两个独立队列上诊断敏感性分别达到94.11%和92%。

Conclusion: DepViT - CAD结合先进架构与大规模验证，为AI辅助癌症诊断提供了稳健可扩展的方法，代码将公开以支持透明性和可重复性。

Abstract: Accurate and timely cancer diagnosis from histopathological slides is vital
for effective clinical decision-making. This paper introduces DepViT-CAD, a
deployable AI system for multi-class cancer diagnosis in histopathology. At its
core is MAViT, a novel Multi-Attention Vision Transformer designed to capture
fine-grained morphological patterns across diverse tumor types. MAViT was
trained on expert-annotated patches from 1008 whole-slide images, covering 11
diagnostic categories, including 10 major cancers and non-tumor tissue.
DepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer
Genome Atlas and 50 routine clinical cases from pathology labs, achieving
diagnostic sensitivities of 94.11% and 92%, respectively. By combining
state-of-the-art transformer architecture with large-scale real-world
validation, DepViT-CAD offers a robust and scalable approach for AI-assisted
cancer diagnostics. To support transparency and reproducibility, software and
code will be made publicly available at GitHub.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [516] [Computing the probability of intersection](https://arxiv.org/abs/2507.10329)
*Alexander Barvinok*

Main category: math.PR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Let $\Omega_1, \ldots, \Omega_m$ be probability spaces, let $\Omega=\Omega_1
\times \cdots \times \Omega_m$ be their product and let $A_1, \ldots, A_n
\subset \Omega$ be events. Suppose that each event $A_i$ depends on $r_i$
coordinates of a point $x \in \Omega$, $x=\left(\xi_1, \ldots, \xi_m\right)$,
and that for each event $A_i$ there are $\Delta_i$ of other events $A_j$ that
depend on some of the coordinates that $A_i$ depends on. Let $\Delta=\max\{5,\
\Delta_i: i=1, \ldots, n\}$ and let $\mu_i=\min\{r_i,\ \Delta_i+1\}$ for $i=1,
\ldots, n$. We prove that if $P(A_i) < (3\Delta)^{-3\mu_i}$ for all $I$, then
for any $0 < \epsilon < 1$, the probability $P\left( \bigcap_{i=1}^n
\overline{A}_i\right)$ of the intersection of the complements of all $A_i$ can
be computed within relative error $\epsilon$ in polynomial time from the
probabilities $P\left(A_{i_1} \cap \ldots \cap A_{i_k}\right)$ of $k$-wise
intersections of the events $A_i$ for $k = e^{O(\Delta)} \ln (n/\epsilon)$.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [517] [A Multi-Level Strategy for Deepfake Content Moderation under EU Regulation](https://arxiv.org/abs/2507.08879)
*Max-Paul Förster,Luca Deck,Raimund Weidlich,Niklas Kühl*

Main category: cs.CY

TL;DR: 随着深度伪造技术的普及，欧盟出台相关义务规定，但缺标准仍是挑战。通过文献综述评估方法效果，发现单个方法不足，提出多级策略。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对民主社会有风险，欧盟出台相关义务规定但缺行业和执行标准，需评估方法有效性。

Method: 进行多视角文献综述，总结标记、检测和标注深度伪造的方法并评估其在欧盟法规下的有效性。

Result: 单个方法无法满足监管和实际要求。

Conclusion: 提出结合现有方法优势的多级策略，该策略具有可扩展性、实用性，对技术类型无偏向且可进行特定风险加权。

Abstract: The growing availability and use of deepfake technologies increases risks for
democratic societies, e.g., for political communication on online platforms.
The EU has responded with transparency obligations for providers and deployers
of Artificial Intelligence (AI) systems and online platforms. This includes
marking deepfakes during generation and labeling deepfakes when they are
shared. However, the lack of industry and enforcement standards poses an
ongoing challenge. Through a multivocal literature review, we summarize methods
for marking, detecting, and labeling deepfakes and assess their effectiveness
under EU regulation. Our results indicate that individual methods fail to meet
regulatory and practical requirements. Therefore, we propose a multi-level
strategy combining the strengths of existing methods. To account for the masses
of content on online platforms, our multi-level strategy provides scalability
and practicality via a simple scoring mechanism. At the same time, it is
agnostic to types of deepfake technology and allows for context-specific risk
weighting.

</details>


### [518] [The Consistency-Acceptability Divergence of LLMs in Judicial Decision-Making: Task and Stakeholder Dimensions](https://arxiv.org/abs/2507.08881)
*Zhang MingDa,Xu Qing*

Main category: cs.CY

TL;DR: 研究引入‘一致性 - 可接受性差异’概念，分析 2023 - 2025 年数据，提出 DTDMR - LJGF 框架平衡司法系统中技术效率与社会合法性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型技术融入司法系统带来变革，但存在技术一致性和社会接受度的矛盾，需解决该问题。

Method: 综合分析 2023 - 2025 年大语言模型司法应用的相关数据。

Result: 发现解决挑战需从任务和利益相关者两个维度考虑，提出 DTDMR - LJGF 框架。

Conclusion: 该框架为构建平衡技术效率与社会合法性的大语言模型司法生态系统提供理论见解和实践指导。

Abstract: The integration of large language model (LLM) technology into judicial
systems is fundamentally transforming legal practice worldwide. However, this
global transformation has revealed an urgent paradox requiring immediate
attention. This study introduces the concept of ``consistency-acceptability
divergence'' for the first time, referring to the gap between technical
consistency and social acceptance. While LLMs achieve high consistency at the
technical level, this consistency demonstrates both positive and negative
effects. Through comprehensive analysis of recent data on LLM judicial
applications from 2023--2025, this study finds that addressing this challenge
requires understanding both task and stakeholder dimensions. This study
proposes the Dual-Track Deliberative Multi-Role LLM Judicial Governance
Framework (DTDMR-LJGF), which enables intelligent task classification and
meaningful interaction among diverse stakeholders. This framework offers both
theoretical insights and practical guidance for building an LLM judicial
ecosystem that balances technical efficiency with social legitimacy.

</details>


### [519] [The Engineer's Dilemma: A Review of Establishing a Legal Framework for Integrating Machine Learning in Construction by Navigating Precedents and Industry Expectations](https://arxiv.org/abs/2507.08908)
*M. Z. Naser*

Main category: cs.CY

TL;DR: 文章探讨工程行业未充分采用ML方法下，工程师如何应对相关法律问题，提出类比推理可嵌入ML，还分析立法机构等作用并构建法律框架。


<details>
  <summary>Details</summary>
Motivation: 工程行业未充分采用ML方法，工程师和利益相关者对相关法律监管框架不确定，需解决这一差距。

Method: 借鉴其他领域先例和经验，运用类比推理，关注既定责任原则和法院对预测模型的评估。

Result: 强调理解技术理由和法律先例相互作用的重要性，催化了整合ML的法律框架。

Conclusion: 类比推理可让ML嵌入现有工程规范，同时需构建法律框架让利益相关者评估ML工程解决方案的责任、风险和益处。

Abstract: Despite the widespread interest in machine learning (ML), the engineering
industry has not yet fully adopted ML-based methods, which has left engineers
and stakeholders uncertain about the legal and regulatory frameworks that
govern their decisions. This gap remains unaddressed as an engineer's
decision-making process, typically governed by professional ethics and
practical guidelines, now intersects with complex algorithmic outputs. To
bridge this gap, this paper explores how engineers can navigate legal
principles and legislative justifications that support and/or contest the
deployment of ML technologies. Drawing on recent precedents and experiences
gained from other fields, this paper argues that analogical reasoning can
provide a basis for embedding ML within existing engineering codes while
maintaining professional accountability and meeting safety requirements. In
exploring these issues, the discussion focuses on established liability
doctrines, such as negligence and product liability, and highlights how courts
have evaluated the use of predictive models. We further analyze how legislative
bodies and standard-setting organizations can furnish explicit guidance
equivalent to prior endorsements of emergent technologies. This exploration
stresses the vitality of understanding the interplay between technical
justifications and legal precedents for shaping an informed stance on ML's
legitimacy in engineering practice. Finally, our analysis catalyzes a legal
framework for integrating ML through which stakeholders can critically assess
the responsibilities, liabilities, and benefits inherent in ML-driven
engineering solutions.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [520] [Extending Defeasibility for Propositional Standpoint Logics](https://arxiv.org/abs/2507.10133)
*Nicholas Leisegang,Thomas Meyer,Ivan Varzinczak*

Main category: cs.LO

TL;DR: 本文引入命题立场逻辑的可废止版本，给出语义和表推演算法，并分析复杂度。


<details>
  <summary>Details</summary>
Motivation: 在立场逻辑中引入可废止性，以表达蕴含、立场模态算子和立场细化陈述层面的可废止性。

Method: 将Kraus等人的可废止条件句、Britz和Varzinczak的可废止必然性和不同可能性概念以及Leisegang等人的可废止性方法集成到Gómez Álvarez和Rudolph的立场逻辑中，提供优先语义，提出表推演算法。

Result: 所提表推演算法对优先蕴含是可靠且完备的，计算复杂度在PSpace内。

Conclusion: 成功构建具有可废止性的命题立场逻辑框架，并给出有效推理算法及复杂度分析。

Abstract: In this paper, we introduce a new defeasible version of propositional
standpoint logic by integrating Kraus et al.'s defeasible conditionals, Britz
and Varzinczak's notions of defeasible necessity and distinct possibility,
along with Leisegang et al.'s approach to defeasibility into the standpoint
logics of G\'omez \'Alvarez and Rudolph. The resulting logical framework allows
for the expression of defeasibility on the level of implications, standpoint
modal operators, and standpoint-sharpening statements. We provide a
preferential semantics for this extended language and propose a tableaux
calculus, which is shown to be sound and complete with respect to preferential
entailment. We also establish the computational complexity of the tableaux
procedure to be in PSpace.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [521] [Surprisingly High Redundancy in Electronic Structure Data](https://arxiv.org/abs/2507.09001)
*Sazzad Hossain,Ponkrshnan Thiagarajan,Shashank Pathrudkar,Stephanie Taylor,Abhijeet S. Gangan,Amartya S. Banerjee,Susanta Ghosh*

Main category: cond-mat.mtrl-sci

TL;DR: 研究揭示电子结构数据集中存在高度冗余，随机或基于覆盖度的剪枝可减少数据量且不损失太多准确性，而基于重要性的剪枝可能失败。


<details>
  <summary>Details</summary>
Motivation: 挑战普遍认为准确预测电子结构需要大量详尽数据集的假设，探索数据冗余情况。

Method: 采用随机剪枝、基于覆盖度的剪枝策略和基于重要性的剪枝方法进行实验。

Result: 随机剪枝可大幅减少数据集大小且损失最小；基于覆盖度的剪枝用少至100倍的数据保留化学精度和模型泛化性，减少训练时间；基于重要性的剪枝在高剪枝因子下可能失败。

Conclusion: 电子结构数据中存在的高冗余性有助于识别每个材料类别的最小必要数据集。

Abstract: Machine Learning (ML) models for electronic structure rely on large datasets
generated through expensive Kohn-Sham Density Functional Theory simulations.
This study reveals a surprisingly high level of redundancy in such datasets
across various material systems, including molecules, simple metals, and
complex alloys. Our findings challenge the prevailing assumption that large,
exhaustive datasets are necessary for accurate ML predictions of electronic
structure. We demonstrate that even random pruning can substantially reduce
dataset size with minimal loss in predictive accuracy, while a state-of-the-art
coverage-based pruning strategy retains chemical accuracy and model
generalizability using up to 100-fold less data and reducing training time by
threefold or more. By contrast, widely used importance-based pruning methods,
which eliminate seemingly redundant data, can catastrophically fail at higher
pruning factors, possibly due to the significant reduction in data coverage.
This heretofore unexplored high degree of redundancy in electronic structure
data holds the potential to identify a minimal, essential dataset
representative of each material class.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [522] [Counterfactual optimization for fault prevention in complex wind energy systems](https://arxiv.org/abs/2507.08849)
*Emilio Carrizosa,Martina Fischetti,Roshell Haaker,Juan Miguel Morales*

Main category: eess.SY

TL;DR: 本文将机器学习用于复杂系统，不仅检测异常，还确定最优控制策略使系统恢复安全，以海上风力涡轮机油式变压器为例，方法能适应偏好且每年节省约300万欧元。


<details>
  <summary>Details</summary>
Motivation: 现有研究多仅检测异常，本文想进一步确定使系统恢复安全状态的最优控制策略。

Method: 将问题构建为反事实问题，利用数学模型在满足系统特定约束下找到最优反事实解决方案。

Result: 在工业伙伴提供的真实数据测试中，方法能适应使用者偏好，在典型风电场每年节省约300万欧元。

Conclusion: 本工作推动了反事实优化在新领域的应用，为该领域研究开辟了新途径。

Abstract: Machine Learning models are increasingly used in businesses to detect faults
and anomalies in complex systems. In this work, we take this approach a step
further: beyond merely detecting anomalies, we aim to identify the optimal
control strategy that restores the system to a safe state with minimal
disruption. We frame this challenge as a counterfactual problem: given a
Machine Learning model that classifies system states as either good or
anomalous, our goal is to determine the minimal adjustment to the system's
control variables (i.e., its current status) that is necessary to return it to
the good state. To achieve this, we leverage a mathematical model that finds
the optimal counterfactual solution while respecting system specific
constraints. Notably, most counterfactual analysis in the literature focuses on
individual cases where a person seeks to alter their status relative to a
decision made by a classifier, such as for loan approval or medical diagnosis.
Our work addresses a fundamentally different challenge: optimizing
counterfactuals for a complex energy system, specifically an offshore wind
turbine oil type transformer. This application not only advances counterfactual
optimization in a new domain but also opens avenues for broader research in
this area. Our tests on real world data provided by our industrial partner show
that our methodology easily adapts to user preferences and brings savings in
the order of 3 million euros per year in a typical farm.

</details>


### [523] [Intersection of Reinforcement Learning and Bayesian Optimization for Intelligent Control of Industrial Processes: A Safe MPC-based DPG using Multi-Objective BO](https://arxiv.org/abs/2507.09864)
*Hossein Nejatbakhsh Esfahani,Javad Mohammadpour Velni*

Main category: eess.SY

TL;DR: 提出MPC - RL与MOBO集成框架，解决标准MPC - RL问题，数值示例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 标准MPC - RL方法存在收敛慢、策略学习欠佳和安全问题，需改进。

Method: 将MPC - RL与MOBO集成，利用CDPG估计成本及梯度，结合EHVI获取函数。

Result: 实现了样本高效、稳定和高性能的控制系统学习。

Conclusion: 所提MPC - RL - MOBO框架能有效解决标准MPC - RL问题，实现更好的闭环性能。

Abstract: Model Predictive Control (MPC)-based Reinforcement Learning (RL) offers a
structured and interpretable alternative to Deep Neural Network (DNN)-based RL
methods, with lower computational complexity and greater transparency. However,
standard MPC-RL approaches often suffer from slow convergence, suboptimal
policy learning due to limited parameterization, and safety issues during
online adaptation. To address these challenges, we propose a novel framework
that integrates MPC-RL with Multi-Objective Bayesian Optimization (MOBO). The
proposed MPC-RL-MOBO utilizes noisy evaluations of the RL stage cost and its
gradient, estimated via a Compatible Deterministic Policy Gradient (CDPG)
approach, and incorporates them into a MOBO algorithm using the Expected
Hypervolume Improvement (EHVI) acquisition function. This fusion enables
efficient and safe tuning of the MPC parameters to achieve improved closed-loop
performance, even under model imperfections. A numerical example demonstrates
the effectiveness of the proposed approach in achieving sample-efficient,
stable, and high-performance learning for control systems.

</details>


### [524] [Neural Two-Stage Stochastic Optimization for Solving Unit Commitment Problem](https://arxiv.org/abs/2507.09503)
*Zhentong Shao,Jingtao Qin,Nanpeng Yu*

Main category: eess.SY

TL;DR: 提出神经随机优化方法解决高维不确定性下两阶段随机机组组合问题，实验表明该方法有高最优性和速度提升及可扩展性。


<details>
  <summary>Details</summary>
Motivation: 高效解决高维不确定性场景下的两阶段随机机组组合问题。

Method: 用深度神经网络近似第二阶段补偿问题，嵌入第一阶段问题成混合整数线性规划，用场景嵌入网络降维和特征聚合。

Result: 在IEEE多个母线系统实验中，最优性差距小于1%，比传统求解器和分解方法速度大幅提升，模型规模不随场景数量变化。

Conclusion: 所提神经两阶段随机优化方法对大规模随机机组组合问题有显著可扩展性。

Abstract: This paper proposes a neural stochastic optimization method for efficiently
solving the two-stage stochastic unit commitment (2S-SUC) problem under
high-dimensional uncertainty scenarios. The proposed method approximates the
second-stage recourse problem using a deep neural network trained to map
commitment decisions and uncertainty features to recourse costs. The trained
network is subsequently embedded into the first-stage UC problem as a
mixed-integer linear program (MILP), allowing for explicit enforcement of
operational constraints while preserving the key uncertainty characteristics. A
scenario-embedding network is employed to enable dimensionality reduction and
feature aggregation across arbitrary scenario sets, serving as a data-driven
scenario reduction mechanism. Numerical experiments on IEEE 5-bus, 30-bus, and
118-bus systems demonstrate that the proposed neural two-stage stochastic
optimization method achieves solutions with an optimality gap of less than 1%,
while enabling orders-of-magnitude speedup compared to conventional MILP
solvers and decomposition-based methods. Moreover, the model's size remains
constant regardless of the number of scenarios, offering significant
scalability for large-scale stochastic unit commitment problems.

</details>


### [525] [Symptom-Driven Personalized Proton Pump Inhibitors Therapy Using Bayesian Neural Networks and Model Predictive Control](https://arxiv.org/abs/2507.09685)
*Yutong Li,Ilya Kolmanovsky*

Main category: eess.SY

TL;DR: 提出非侵入性、基于症状的框架来调整PPI剂量，学习增强的MPC算法可减少PPI消耗并保证抑酸效果。


<details>
  <summary>Details</summary>
Motivation: 长期高剂量使用PPI有风险，且精确长期控制胃酸受限于侵入性监测的不实际和患者个体差异。

Method: 提出基于症状的框架，用贝叶斯神经网络预测症状，结合机会约束的MPC算法动态计算PPI剂量。

Result: 在不同饮食和虚拟患者模型中，学习增强的MPC较标准固定方案减少65%的PPI消耗，且至少95%概率保证抑酸。

Conclusion: 该方法为个性化PPI治疗提供了实用途径，减少治疗负担和过量风险，无需侵入性传感器。

Abstract: Proton Pump Inhibitors (PPIs) are the standard of care for gastric acid
disorders but carry significant risks when administered chronically at high
doses. Precise long-term control of gastric acidity is challenged by the
impracticality of invasive gastric acid monitoring beyond 72 hours and wide
inter-patient variability. We propose a noninvasive, symptom-based framework
that tailors PPI dosing solely on patient-reported reflux and digestive symptom
patterns. A Bayesian Neural Network prediction model learns to predict patient
symptoms and quantifies its uncertainty from historical symptom scores, meal,
and PPIs intake data. These probabilistic forecasts feed a chance-constrained
Model Predictive Control (MPC) algorithm that dynamically computes future PPI
doses to minimize drug usage while enforcing acid suppression with high
confidence - without any direct acid measurement. In silico studies over
diverse dietary schedules and virtual patient profiles demonstrate that our
learning-augmented MPC reduces total PPI consumption by 65 percent compared to
standard fixed regimens, while maintaining acid suppression with at least 95
percent probability. The proposed approach offers a practical path to
personalized PPI therapy, minimizing treatment burden and overdose risk without
invasive sensors.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [526] [m-Eternal Domination and Variants on Some Classes of Finite and Infinite Graphs](https://arxiv.org/abs/2507.09283)
*Tiziana Calamoneri,Federico Corò,Neeldhara Misra,Saraswati G. Nanoti,Giacomo Paesani*

Main category: cs.DM

TL;DR: 研究图上的m - 永恒支配问题，证明其NP - 难，还给出四类无限规则网格图上支配和m - 永恒支配问题的结构结果。


<details>
  <summary>Details</summary>
Motivation: 对图上的m - 永恒支配问题进行研究，探索其复杂度和结构性质。

Method: 对m - 永恒支配问题及其变体进行理论分析，研究四类无限规则网格图。

Result: 证明m - 永恒支配问题及其一些变体是NP - 难的，给出四类无限规则网格图上的结构结果和紧界。

Conclusion: m - 永恒支配问题具有NP - 难的复杂度，在特定无限规则网格图上有明确的结构性质。

Abstract: We study the m-Eternal Domination problem, which is the following two-player
game between a defender and an attacker on a graph: initially, the defender
positions k guards on vertices of the graph; the game then proceeds in turns
between the defender and the attacker, with the attacker selecting a vertex and
the defender responding to the attack by moving a guard to the attacked vertex.
The defender may move more than one guard on their turn, but guards can only
move to neighboring vertices. The defender wins a game on a graph G with k
guards if the defender has a strategy such that at every point of the game the
vertices occupied by guards form a dominating set of G and the attacker wins
otherwise. The m-eternal domination number of a graph G is the smallest value
of k for which (G,k) is a defender win.
  We show that m-Eternal Domination is NP-hard, as well as some of its
variants, even on special classes of graphs. We also show structural results
for the Domination and m-Eternal Domination problems in the context of four
types of infinite regular grids: square, octagonal, hexagonal, and triangular,
establishing tight bounds.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [527] [Optimal Differentially Private Ranking from Pairwise Comparisons](https://arxiv.org/abs/2507.09388)
*T. Tony Cai,Abhinav Chakraborty,Yichen Wang*

Main category: math.ST

TL;DR: 提出基于成对比较的差分隐私排名算法，在两种隐私概念下开发分析方法，算法达收敛最优率并经实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决涉及不完整和嘈杂成对比较排名应用中的数据隐私问题，如推荐系统、教育评估等。

Method: 在边缘差分隐私和个体差分隐私两种概念下开发排名方法，包括扰动最大似然估计器和基于噪声计数的方法。

Result: 算法在各自隐私约束下达到了极小极大最优收敛率。

Conclusion: 所提出的方法在模拟和真实世界数据实验中都有实际有效性。

Abstract: Data privacy is a central concern in many applications involving ranking from
incomplete and noisy pairwise comparisons, such as recommendation systems,
educational assessments, and opinion surveys on sensitive topics. In this work,
we propose differentially private algorithms for ranking based on pairwise
comparisons. Specifically, we develop and analyze ranking methods under two
privacy notions: edge differential privacy, which protects the confidentiality
of individual comparison outcomes, and individual differential privacy, which
safeguards potentially many comparisons contributed by a single individual. Our
algorithms--including a perturbed maximum likelihood estimator and a noisy
count-based method--are shown to achieve minimax optimal rates of convergence
under the respective privacy constraints. We further demonstrate the practical
effectiveness of our methods through experiments on both simulated and
real-world data.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [528] [KEN: Knowledge Augmentation and Emotion Guidance Network for Multimodal Fake News Detection](https://arxiv.org/abs/2507.09647)
*Peican Zhu,Yubo Jing,Le Cheng,Keke Tang,Yangming Guo*

Main category: cs.MM

TL;DR: 针对社交媒体多模态假新闻检测问题，提出知识增强与情感引导网络KEN，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 以往研究对图像语义理解不足、文本信息有限时难辨新闻真伪，且未区分新闻情感类型导致性能下降。

Method: 提出KEN网络，一方面利用LVLM强大语义理解和世界知识，获取图像描述和文本证据；另一方面通过平衡学习考虑不同情感类型新闻的类间差异。

Result: 在两个真实数据集上的大量实验证明了KEN网络的优越性。

Conclusion: 提出的KEN网络在多模态假新闻检测方面效果良好，能解决现有研究的问题。

Abstract: In recent years, the rampant spread of misinformation on social media has
made accurate detection of multimodal fake news a critical research focus.
However, previous research has not adequately understood the semantics of
images, and models struggle to discern news authenticity with limited textual
information. Meanwhile, treating all emotional types of news uniformly without
tailored approaches further leads to performance degradation. Therefore, we
propose a novel Knowledge Augmentation and Emotion Guidance Network (KEN). On
the one hand, we effectively leverage LVLM's powerful semantic understanding
and extensive world knowledge. For images, the generated captions provide a
comprehensive understanding of image content and scenes, while for text, the
retrieved evidence helps break the information silos caused by the closed and
limited text and context. On the other hand, we consider inter-class
differences between different emotional types of news through balanced
learning, achieving fine-grained modeling of the relationship between emotional
types and authenticity. Extensive experiments on two real-world datasets
demonstrate the superiority of our KEN.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [529] [Explicit Bounds and Parallel Algorithms for Counting Multiply Gleeful Numbers](https://arxiv.org/abs/2507.09012)
*Sara Moore,Jonathan P. Sorenson*

Main category: math.NT

TL;DR: 本文研究k - gleeful数（k > 1），给出表示数上下界，提出并行算法生成表示，研究多表示的情形并给出启发式模型及猜想。


<details>
  <summary>Details</summary>
Motivation: 深入研究k - gleeful数的性质，特别是对于k > 1的情况，拓展相关理论和应用。

Method: 扩展先前分析工作；设计并分析两个并行算法；建立启发式模型研究多表示数。

Result: 得到k - gleeful表示数的上下界；提出有效并行算法；给出多表示数密度的启发式模型及实证数据。

Conclusion: 对k - gleeful数有了更深入的认识，提出新的算法和猜想，为后续研究提供方向。

Abstract: Let $k\ge 1$ be an integer. A positive integer $n$ is $k$-\textit{gleeful} if
$n$ can be represented as the sum of $k$th powers of consecutive primes. For
example, $35=2^3+3^3$ is a $3$-gleeful number, and $195=5^2+7^2+11^2$ is
$2$-gleeful. In this paper, we present some new results on $k$-gleeful numbers
for $k>1$.
  First, we extend previous analytical work. For given values of $x$ and $k$,
we give explicit upper and lower bounds on the number of $k$-gleeful
representations of integers $n\le x$.
  Second, we describe and analyze two new, efficient parallel algorithms, one
theoretical and one practical, to generate all $k$-gleeful representations up
to a bound $x$.
  Third, we study integers that are multiply gleeful, that is, integers with
more than one representation as a sum of powers of consecutive primes,
including both the same or different values of $k$. We give a simple heuristic
model for estimating the density of multiply-gleeful numbers, we present
empirical data in support of our heuristics, and offer some new conjectures.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [530] [Mind the Gap: Navigating Inference with Optimal Transport Maps](https://arxiv.org/abs/2507.08867)
*Malte Algren,Tobias Golling,Francesco Armando Di Bello,Christopher Pollard*

Main category: physics.data-an

TL;DR: 本文提出基于最优传输的校准方法解决粒子物理中机器学习模拟与实验数据不符问题，以喷注标记验证效果，对多领域高维模拟校准有广泛应用。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习算法复杂且依赖高质量训练样本，模拟与实验数据的差异会限制机器学习技术在粒子物理中的有效性。

Method: 提出基于最优传输的校准方法，并首次应用于高维模拟。

Result: 通过CMS启发的数据集进行喷注标记，校准高维内部“潜在”表示后，下游任务的相关量也得到正确校准，可用于LHC分析。

Conclusion: 该方法是粒子物理中实现校准“基础模型”的关键一步，校准框架对多领域高维模拟校正有广泛应用。

Abstract: Machine learning (ML) techniques have recently enabled enormous gains in
sensitivity across the sciences. In particle physics, much of this progress has
relied on excellent simulations of a wide range of physical processes. However,
due to the sophistication of modern machine learning (ML) algorithms and their
reliance on high-quality training samples, discrepancies between simulation and
experimental data can significantly limit the effectiveness of ML techniques.
In this work, we present a solution to this ``mis-specification'' problem: a
calibration approach based on optimal transport, which we apply to
high-dimensional simulations for the first time. We demonstrate the performance
of our approach through jet tagging, using a CMS-inspired dataset. A
128-dimensional internal jet representation from a powerful general-purpose
classifier is studied; after calibrating this internal ``latent''
representation, we find that a wide variety of quantities derived from it for
downstream tasks are also properly calibrated: using this calibrated
high-dimensional representation, powerful new applications of jet flavor
information can be utilized in LHC analyses. This is a key step toward allowing
properly-calibrated ``foundation models'' in particle physics. More broadly,
this calibration framework has broad applications for correcting
high-dimensional simulations across the sciences.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [531] [Bridging Literature and the Universe Via A Multi-Agent Large Language Model System](https://arxiv.org/abs/2507.08958)
*Xiaowen Zhang,Zhenyu Bi,Xuan Wang,Tiziana Di Matteo,Rupert A. C. Croft*

Main category: astro-ph.IM

TL;DR: 本文介绍多智能体系统SimAgents，可自动完成宇宙学研究中参数配置与初步分析，实验证明其性能良好，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 随着宇宙学模拟及其软件日益复杂，从文献中提取参数并转化为可执行脚本耗时且易出错，为提高物理研究效率、加速模拟过程。

Method: 引入由具备物理推理等能力的LLM智能体驱动的SimAgents系统，智能体通过结构化通信协作，构建评估数据集。

Result: 在数据集上的实验显示SimAgents性能良好。

Conclusion: SimAgents有效且有潜力加速物理学家的科学研究。

Abstract: As cosmological simulations and their associated software become increasingly
complex, physicists face the challenge of searching through vast amounts of
literature and user manuals to extract simulation parameters from dense
academic papers, each using different models and formats. Translating these
parameters into executable scripts remains a time-consuming and error-prone
process. To improve efficiency in physics research and accelerate the
cosmological simulation process, we introduce SimAgents, a multi-agent system
designed to automate both parameter configuration from the literature and
preliminary analysis for cosmology research. SimAgents is powered by
specialized LLM agents capable of physics reasoning, simulation software
validation, and tool execution. These agents collaborate through structured
communication, ensuring that extracted parameters are physically meaningful,
internally consistent, and software-compliant. We also construct a cosmological
parameter extraction evaluation dataset by collecting over 40 simulations in
published papers from Arxiv and leading journals that cover diverse simulation
types. Experiments on the dataset demonstrate a strong performance of
SimAgents, highlighting its effectiveness and potential to accelerate
scientific research for physicists. Our demonstration video is available at:
https://youtu.be/w1zLpm_CaWA. The complete system and dataset are publicly
available at https://github.com/xwzhang98/SimAgents.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [532] [Optimizing External Sources for Controlled Burning Plasma in Tokamaks with Neural Ordinary Differential Equations](https://arxiv.org/abs/2507.09431)
*Zefang Liu,Weston M. Stacey*

Main category: physics.plasm-ph

TL;DR: 本文提出基于神经常微分方程的多节点等离子体动力学模型的逆建模方法，用于计算托卡马克外部源分布以控制等离子体。


<details>
  <summary>Details</summary>
Motivation: 托卡马克实现受控燃烧等离子体需要精确调节外部粒子和能量源，以达到并维持目标核心密度和温度。

Method: 使用基于神经常微分方程的多节点等离子体动力学模型，将控制任务转化为优化问题，通过神经常微分方程求解器的自动微分来最小化模拟与目标轨迹的差异。

Result: 将正向模拟工具转化为面向控制的模型。

Conclusion: 该框架为当前和未来聚变装置计算外部源分布提供了实用方法。

Abstract: Achieving controlled burning plasma in tokamaks requires precise regulation
of external particle and energy sources to reach and maintain target core
densities and temperatures. This work presents an inverse modeling approach
using a multinodal plasma dynamics model based on neural ordinary differential
equations (Neural ODEs). Given a desired time evolution of nodal quantities
such as deuteron density or electron temperature, we compute the external
source profiles, such as neutral beam injection (NBI) power, that drive the
plasma toward the specified behavior. The approach is implemented within the
NeuralPlasmaODE framework, which models multi-region, multi-timescale transport
and incorporates physical mechanisms including radiation, auxiliary heating,
and internodal energy exchange. By formulating the control task as an
optimization problem, we use automatic differentiation through the Neural ODE
solver to minimize the discrepancy between simulated and target trajectories.
This framework transforms the forward simulation tool into a control-oriented
model and provides a practical method for computing external source profiles in
both current and future fusion devices.

</details>


### [533] [Sensitivity Analysis of Transport and Radiation in NeuralPlasmaODE for ITER Burning Plasmas](https://arxiv.org/abs/2507.09432)
*Zefang Liu,Weston M. Stacey*

Main category: physics.plasm-ph

TL;DR: 本文扩展NeuralPlasmaODE模型对ITER等离子体进行敏感性分析，揭示关键参数对能量约束的影响，证明该模型在燃烧等离子体环境中的实用性。


<details>
  <summary>Details</summary>
Motivation: 理解关键物理参数对燃烧等离子体行为的影响对ITER可靠运行至关重要。

Method: 扩展NeuralPlasmaODE模型，计算核心和边缘温度、密度相对于输运扩散率、电子回旋辐射参数、杂质分数和离子轨道损失时间尺度的归一化灵敏度。

Result: 结果表明磁场强度、安全因子和杂质含量对能量约束有主导影响，还揭示了温度相关输运对自调节行为的贡献。

Conclusion: NeuralPlasmaODE模型可用于燃烧等离子体环境的预测建模和情景优化。

Abstract: Understanding how key physical parameters influence burning plasma behavior
is critical for the reliable operation of ITER. In this work, we extend
NeuralPlasmaODE, a multi-region, multi-timescale model based on neural ordinary
differential equations, to perform a sensitivity analysis of transport and
radiation mechanisms in ITER plasmas. Normalized sensitivities of core and edge
temperatures and densities are computed with respect to transport
diffusivities, electron cyclotron radiation (ECR) parameters, impurity
fractions, and ion orbit loss (IOL) timescales. The analysis focuses on
perturbations around a trained nominal model for the ITER inductive scenario.
Results highlight the dominant influence of magnetic field strength, safety
factor, and impurity content on energy confinement, while also revealing how
temperature-dependent transport contributes to self-regulating behavior. These
findings demonstrate the utility of NeuralPlasmaODE for predictive modeling and
scenario optimization in burning plasma environments.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [534] [Context-Aware Regularization with Markovian Integration for Attention-Based Nucleotide Analysis](https://arxiv.org/abs/2507.09378)
*Mohammadsaleh Refahi,Mahdi Abavisani,Bahrad A. Sokhansanj,James R. Brown,Gail Rosen*

Main category: q-bio.GN

TL;DR: 本文提出CARMANIA框架解决核苷酸序列分析中长距离依赖问题，在多任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: Transformer在核苷酸序列分析中捕获长距离依赖有挑战，标准自注意力机制计算效率低且缺乏全局转换一致性。

Method: 引入CARMANIA自监督预训练框架，用转换矩阵（TM）损失增强下一个标记（NT）预测。

Result: 在多种基因组任务中，CARMANIA比之前最佳长上下文模型至少高7%，在短序列上与SOTA相当且速度快约2.5倍，在部分任务提升明显。TM损失在33个任务中提高了准确性。

Conclusion: CARMANIA框架有效，能捕获高阶依赖，学习特定生物序列结构，提升多种基因组任务性能。

Abstract: Transformers have revolutionized nucleotide sequence analysis, yet capturing
long-range dependencies remains challenging. Recent studies show that
autoregressive transformers often exhibit Markovian behavior by relying on
fixed-length context windows for next-token prediction. However, standard
self-attention mechanisms are computationally inefficient for long sequences
due to their quadratic complexity and do not explicitly enforce global
transition consistency.
  We introduce CARMANIA (Context-Aware Regularization with Markovian
Integration for Attention-Based Nucleotide Analysis), a self-supervised
pretraining framework that augments next-token (NT) prediction with a
transition-matrix (TM) loss. The TM loss aligns predicted token transitions
with empirically derived n-gram statistics from each input sequence,
encouraging the model to capture higher-order dependencies beyond local
context. This integration enables CARMANIA to learn organism-specific sequence
structures that reflect both evolutionary constraints and functional
organization.
  We evaluate CARMANIA across diverse genomic tasks, including regulatory
element prediction, functional gene classification, taxonomic inference,
antimicrobial resistance detection, and biosynthetic gene cluster
classification. CARMANIA outperforms the previous best long-context model by at
least 7 percent, matches state-of-the-art on shorter sequences (exceeding prior
results on 20 out of 40 tasks while running approximately 2.5 times faster),
and shows particularly strong improvements on enhancer and housekeeping gene
classification tasks, including up to a 34 percent absolute gain in Matthews
correlation coefficient (MCC) for enhancer prediction. The TM loss boosts
accuracy in 33 of 40 tasks, especially where local motifs or regulatory
patterns drive prediction.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [535] [CEO-DC: An Actionable Framework to Close the Carbon Gap in HPC Data Centers](https://arxiv.org/abs/2507.08923)
*Rubén Rodríguez Álvarez,Denisa-Andreea Constantinescu,Miguel Peón-Quirós,David Atienza*

Main category: cs.AR

TL;DR: 文章提出CEO - DC模型解决数据中心碳排放和经济优化问题，通过案例研究展示升级策略效果并给出相关结论和建议。


<details>
  <summary>Details</summary>
Motivation: 数据中心快速扩张导致能耗和碳排放增长，以往研究忽略复杂权衡，需新方法解决问题。

Method: 提出CEO - DC集成模型和决策方法，建模成本、碳和计算需求，提出引导采购等决策的指标。

Result: 4年周期升级旧设备可减少总排放，但超44%情况难随需求增长不增排放，超72%需经济激励；9/14数据中心最多国家当前碳价不足以推动升级；牺牲延迟优化能效会提高采用所需碳价。

Conclusion: CEO - DC为数据中心相关人员提供了通过合理安排升级、控制增长、优化性价比和增加激励等方面的可行见解。

Abstract: The rapid expansion of data centers (DCs) to support large-scale AI and
scientific workloads is driving unsustainable growth in energy consumption and
greenhouse gas emissions. While successive generations of hardware platforms
have improved performance and energy efficiency, the question remains whether
new, more efficient platforms can realistically offset the rising emissions
associated with increasing demand. Prior studies often overlook the complex
trade-offs in such transitions by failing to account for both the economic
incentives and the projected compute demand growth over the operational
lifetime of the devices. In response, we present CEO-DC, an integrated model
and decision-making methodology for Carbon and Economy Optimization in Data
Centers. CEO-DC models the competing forces of cost, carbon, and compute demand
to guide optimal platform procurement and replacement strategies. We propose
metrics to steer procurement, platform design, and policy decisions toward
sustainable DC technologies. Given current platform trends, our AI case study
using CEO-DC shows that upgrading legacy devices on a 4-year cycle reduces
total emissions. However, these upgrades fail to scale with DC demand growth
trends without increasing total emissions in over 44% of cases, and require
economic incentives for adoption in over 72%. Furthermore, current carbon
prices are insufficient to motivate upgrades in 9 out of the 14 countries with
the highest number of DCs globally. We also find that optimizing platforms for
energy efficiency at the expense of latency can increase the carbon price
required to justify their adoption. In summary, CEO-DC provides actionable
insights for DC architects, platform designers, and policymakers by timing
legacy platform upgrades, constraining DC growth to sustainable levels,
optimizing platform performance-to-cost ratios, and increasing incentives.

</details>


### [536] [SLIM: A Heterogeneous Accelerator for Edge Inference of Sparse Large Language Model via Adaptive Thresholding](https://arxiv.org/abs/2507.09201)
*Weihong Xu,Haein Choi,Po-kai Hsu,Shimeng Yu,Tajana Rosing*

Main category: cs.AR

TL;DR: 提出SLIM用于边缘设备稀疏大语言模型服务，结合算法和硬件设计，减少数据移动和能耗，提升吞吐量和能效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在资源受限嵌入式设备上高效推理有挑战，现有加速器未充分利用模型稀疏性，硬件资源未充分利用。

Method: 提出SLIM，通过自适应阈值算法利用模型稀疏性，结合近存储处理和内存内处理的异构硬件架构。

Result: 相比SSD - GPU系统吞吐量提升13 - 18倍，相比DRAM - GPU系统能效提升9 - 10倍，且保持低延迟。

Conclusion: SLIM使边缘计算环境下经济高效的大语言模型部署成为可能。

Abstract: Large language models (LLMs) have demonstrated exceptional proficiency in
understanding and generating human language, but efficient inference on
resource-constrained embedded devices remains challenging due to large model
sizes and memory-intensive operations in feedforward network (FFN) and
multi-head attention (MHA) layers. While existing accelerators offload LLM
inference to expensive heterogeneous computing systems, they fail to exploit
the significant sparsity inherent in LLM operations, leaving hardware resources
underutilized. We propose SLIM, an algorithm-hardware co-design optimized for
sparse LLM serving on edge devices. SLIM exploits LLM sparsity through an
adaptive thresholding algorithm that enables runtime-configurable sparsity with
negligible accuracy loss, fetching only activated neurons to dramatically
reduce data movement. Our heterogeneous hardware architecture strategically
combines near-storage processing (NSP) and processing-in-memory (PIM): FFN
weights are stored in high-density 3D NAND and computed using NSP units, while
memory-intensive MHA operations are processed in PIM modules. This design
significantly reduces memory footprint, data movement, and energy consumption.
Our comprehensive evaluation demonstrates SLIM's effectiveness, achieving
13-18x throughput improvements over SSD-GPU systems and 9-10x better energy
efficiency over DRAM-GPU systems while maintaining low latency, making
cost-effective LLM deployment viable for edge computing environments.

</details>


### [537] [Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference](https://arxiv.org/abs/2507.09010)
*Chun-Ting Chen,HanGyeol Mun,Jian Meng,Mohamed S. Abdelfattah,Jae-sun Seo*

Main category: cs.AR

TL;DR: 本文提出边缘大语言模型推理加速器，采用混合脉动阵列架构，结合量化和优化数据流等方法，提升推理效率，性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 边缘大语言模型推理需边缘加速器在解码和预填充阶段分别实现高面积效率、低外部内存访问和高能量效率。

Method: 提出混合脉动阵列（HSA）架构，采用MXINT4权重量化和优化数据流，加入优化的RMSNorm和RoPE单元。

Result: 运行1.3B大语言模型在长输入/长输出场景下达到247/117 (token/s/mm2)，比现有方法提升>2.45x/13.5x，且在令牌生成上保持高能量效率。

Conclusion: 所提边缘大语言模型推理加速器方案能有效提升推理效率，优于现有方法。

Abstract: Edge inference for large language models (LLM) offers secure, low-latency,
and cost-effective inference solutions. We emphasize that an edge accelerator
should achieve high area efficiency and minimize external memory access (EMA)
during the memory-bound decode stage, while maintaining high energy efficiency
during the compute intensive prefill stage. This paper proposes an edge LLM
inference accelerator featuring a hybrid systolic array (HSA) architecture that
optimizes inference efficiency in both stages. To further reduce EMA, we adopt
MXINT4 weight quantization and propose an optimized dataflow tailored for HSA,
ensuring negligible dequantization overhead and achieving 100% hardware
utilization with minimal accuracy loss under edge DRAM bandwidth constraints.
For non-linear operations, we incorporate optimized root mean square
normalization (RMSNorm) and rotary position embedding (RoPE) units, reducing
their latency, area, and memory access overhead while enabling end-to-end
inference on our accelerator. Our solution achieves 247/117 (token/s/mm2) while
running a 1.3B LLM on long-input/long-output scenarios, providing >2.45x/13.5x
improvement over existing approaches, while maintaining superior energy
efficiency in token generation.

</details>


### [538] [BitParticle: Partializing Sparse Dual-Factors to Build Quasi-Synchronizing MAC Arrays for Energy-efficient DNNs](https://arxiv.org/abs/2507.09780)
*Feilong Qiaoyuan,Jihe Wang,Zhiyu Sun,Linying Wu,Yuanhua Xiao,Danghui Wang*

Main category: cs.AR

TL;DR: 本文针对量化DNN中位级稀疏性利用的两个挑战，提出基于颗粒化方法的MAC单元和准同步方案，评估显示所提架构有更好的面积和能量效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法不能充分利用位级稀疏性，存在部分积爆炸和MAC单元利用率低的问题，需要改进。

Method: 提出基于颗粒化方法利用双因子稀疏性的MAC单元，用简单控制逻辑解决部分积爆炸问题；引入准同步方案增加MAC阵列的周期弹性。

Result: 精确版本的MAC阵列架构面积效率提升29.2%，近似版本比精确版本能量效率进一步提升7.5%。

Conclusion: 所提方案能有效提升MAC单元的面积和能量效率。

Abstract: Bit-level sparsity in quantized deep neural networks (DNNs) offers
significant potential for optimizing Multiply-Accumulate (MAC) operations.
However, two key challenges still limit its practical exploitation. First,
conventional bit-serial approaches cannot simultaneously leverage the sparsity
of both factors, leading to a complete waste of one factor' s sparsity. Methods
designed to exploit dual-factor sparsity are still in the early stages of
exploration, facing the challenge of partial product explosion. Second, the
fluctuation of bit-level sparsity leads to variable cycle counts for MAC
operations. Existing synchronous scheduling schemes that are suitable for
dual-factor sparsity exhibit poor flexibility and still result in significant
underutilization of MAC units. To address the first challenge, this study
proposes a MAC unit that leverages dual-factor sparsity through the emerging
particlization-based approach. The proposed design addresses the issue of
partial product explosion through simple control logic, resulting in a more
area- and energy-efficient MAC unit. In addition, by discarding less
significant intermediate results, the design allows for further hardware
simplification at the cost of minor accuracy loss. To address the second
challenge, a quasi-synchronous scheme is introduced that adds cycle-level
elasticity to the MAC array, reducing pipeline stalls and thereby improving MAC
unit utilization. Evaluation results show that the exact version of the
proposed MAC array architecture achieves a 29.2% improvement in area efficiency
compared to the state-of-the-art bit-sparsity-driven architecture, while
maintaining comparable energy efficiency. The approximate variant further
improves energy efficiency by 7.5%, compared to the exact version. Index-Terms:
DNN acceleration, Bit-level sparsity, MAC unit

</details>


### [539] [Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving](https://arxiv.org/abs/2507.10178)
*Wonung Kim,Yubin Lee,Yoonsung Kim,Jinwoo Hwang,Seongryong Oh,Jiyong Jung,Aziz Huseynov,Woong Gyu Park,Chang Hyun Park,Divya Mahajan,Jongse Park*

Main category: cs.AR

TL;DR: 论文聚焦高效支持Transformer和后Transformer大语言模型的服务系统，分析性能特性后设计Pimba，评估显示其在吞吐量上有显著提升。


<details>
  <summary>Details</summary>
Motivation: Transformer计算和内存成本随序列长度增加，算法界探索后Transformer架构，需构建统一框架的服务系统。

Method: 分析Transformer和后Transformer大语言模型性能特性，设计由状态更新处理单元（SPU）组成的Pimba，采用MX量化算术。

Result: 与LLM优化的GPU和GPU+PIM系统相比，Pimba的令牌生成吞吐量分别提高了3.2倍和2.1倍。

Conclusion: Pimba能有效解决在统一框架下高效支持Transformer和后Transformer大语言模型的问题。

Abstract: Transformers are the driving force behind today's Large Language Models
(LLMs), serving as the foundation for their performance and versatility. Yet,
their compute and memory costs grow with sequence length, posing scalability
challenges for long-context inferencing. In response, the algorithm community
is exploring alternative architectures, such as state space models (SSMs),
linear attention, and recurrent neural networks (RNNs), which we refer to as
post-transformers. This shift presents a key challenge: building a serving
system that efficiently supports both transformer and post-transformer LLMs
within a unified framework. To address this challenge, we analyze the
performance characteristics of transformer and post-transformer LLMs. Despite
their algorithmic differences, both are fundamentally limited by memory
bandwidth under batched inference due to attention in transformers and state
updates in post-transformers. Further analyses suggest two additional insights:
(1) state update operations, unlike attention, incur high hardware cost, making
per-bank PIM acceleration inefficient, and (2) different low-precision
arithmetic methods offer varying accuracy-area tradeoffs, while we identify
Microsoft's MX as the Pareto-optimal choice. Building on these insights, we
design Pimba as an array of State-update Processing Units (SPUs), each shared
between two banks to enable interleaved access to PIM. Each SPU includes a
State-update Processing Engine (SPE) that comprises element-wise multipliers
and adders using MX-based quantized arithmetic, enabling efficient execution of
state update and attention operations. Our evaluation shows that, compared to
LLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x
higher token generation throughput, respectively.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [540] [Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices](https://arxiv.org/abs/2507.09627)
*Muhammad Kamran Saeed,Ashfaq Khokhar,Shakil Ahmed*

Main category: cs.IT

TL;DR: 本文提出轻量级深度学习框架用于XL - MIMO系统级联信道估计，降低计算复杂度，模拟显示其提升估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有XL - MIMO系统信道估计模型可扩展性和实际部署受限，天线和RIS元素增加带来数据量、计算复杂度等问题。

Method: 提出轻量级深度学习框架，利用信道空间相关性，引入基于补丁的训练机制降低输入维度。

Result: 不同条件下的模拟结果显示，该框架显著提高估计精度并降低计算复杂度。

Conclusion: 所提框架适用于资源受限的边缘设备，能应对XL - MIMO系统中天线和RIS元素增加的挑战。

Abstract: Next-generation wireless technologies such as 6G aim to meet demanding
requirements such as ultra-high data rates, low latency, and enhanced
connectivity. Extremely Large-Scale MIMO (XL-MIMO) and Reconfigurable
Intelligent Surface (RIS) are key enablers, with XL-MIMO boosting spectral and
energy efficiency through numerous antennas, and RIS offering dynamic control
over the wireless environment via passive reflective elements. However,
realizing their full potential depends on accurate Channel State Information
(CSI). Recent advances in deep learning have facilitated efficient cascaded
channel estimation. However, the scalability and practical deployment of
existing estimation models in XL-MIMO systems remain limited. The growing
number of antennas and RIS elements introduces a significant barrier to
real-time and efficient channel estimation, drastically increasing data volume,
escalating computational complexity, requiring advanced hardware, and resulting
in substantial energy consumption. To address these challenges, we propose a
lightweight deep learning framework for efficient cascaded channel estimation
in XL-MIMO systems, designed to minimize computational complexity and make it
suitable for deployment on resource-constrained edge devices. Using spatial
correlations in the channel, we introduce a patch-based training mechanism that
reduces the dimensionality of input to patch-level representations while
preserving essential information, allowing scalable training for large-scale
systems. Simulation results under diverse conditions demonstrate that our
framework significantly improves estimation accuracy and reduces computational
complexity, regardless of the increasing number of antennas and RIS elements in
XL-MIMO systems.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [541] [Advancing network resilience theories with symbolized reinforcement learning](https://arxiv.org/abs/2507.08827)
*Yu Zheng,Jingtao Ding,Depeng Jin,Jianxi Gao,Yong Li*

Main category: physics.soc-ph

TL;DR: 本文提出自动方法发现网络弹性理论，兼顾拓扑与动态，提升对复杂网络理解。


<details>
  <summary>Details</summary>
Motivation: 现有弹性理论仅从拓扑单一视角解决问题，忽略系统动态，需发现兼顾拓扑与动态的弹性理论以预防系统崩溃。

Method: 从AI解决复杂网络拆解问题中学习，将网络攻击策略符号化为理论公式。

Result: 发现首个兼顾拓扑与动态的弹性理论，揭示节点度与状态的相关性对网络弹性的影响；改进现有理论公式，精度提升超37.5%。

Conclusion: 该自动方法有助于设计系统崩溃预警信号，显著提升人类对复杂网络的理解。

Abstract: Many complex networks display remarkable resilience under external
perturbations, internal failures and environmental changes, yet they can
swiftly deteriorate into dysfunction upon the removal of a few keystone nodes.
Discovering theories that measure network resilience offers the potential to
prevent catastrophic collapses--from species extinctions to financial
crise--with profound implications for real-world systems. Current resilience
theories address the problem from a single perspective of topology, neglecting
the crucial role of system dynamics, due to the intrinsic complexity of the
coupling between topology and dynamics which exceeds the capabilities of human
analytical methods. Here, we report an automatic method for resilience theory
discovery, which learns from how AI solves a complicated network dismantling
problem and symbolizes its network attack strategies into theoretical formulas.
This proposed self-inductive approach discovers the first resilience theory
that accounts for both topology and dynamics, highlighting how the correlation
between node degree and state shapes overall network resilience, and offering
insights for designing early warning signals of systematic collapses.
Additionally, our approach discovers formulas that refine existing
well-established resilience theories with over 37.5% improvement in accuracy,
significantly advancing human understanding of complex networks with AI.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [542] [Flexible Modeling of Multivariate Skewed and Heavy-Tailed Data via a Non-Central Skew t Distribution: Application to Tumor Shape Data](https://arxiv.org/abs/2507.10465)
*Abeer M. Hasan,Ying-Ju Chen*

Main category: stat.ME

TL;DR: 提出多元非中心偏t分布（NCST）灵活公式，推导其理论性质，用蒙特卡罗似然近似估计，应用于乳腺癌诊断数据，结果显示NCST模型拟合更优，有发展潜力。


<details>
  <summary>Details</summary>
Motivation: 现有偏t模型常依赖严格假设，为多元数据建模提供一种能同时考虑不对称性和非中心性的替代方案。

Method: 通过用独立卡方变量缩放偏正态随机向量定义NCST分布；推导其关键理论性质；采用蒙特卡罗似然近似进行极大似然估计；通过模拟研究评估性能；将模型应用于乳腺癌诊断数据。

Result: 在信息准则和可视化诊断方面，NCST模型比多元正态、偏正态和Azzalini偏t分布等标准替代方案拟合更优，尤其在处理偏态和厚尾数据时。

Conclusion: NCST分布为复杂多元数据建模提供了有用且可解释的选择，在似然推断、贝叶斯计算及涉及不对称和非高斯依赖的应用方面有发展前景。

Abstract: We propose a flexible formulation of the multivariate non-central skew t
(NCST) distribution, defined by scaling skew-normal random vectors with
independent chi-squared variables. This construction extends the classical
multivariate t family by allowing both asymmetry and non-centrality, which
provides an alternative to existing skew t models that often rely on
restrictive assumptions for tractability. We derive key theoretical properties
of the NCST distribution, which includes its moment structure, affine
transformation behavior, and the distribution of quadratic forms. Due to the
lack of a closed-form density, we implement a Monte Carlo likelihood
approximation to enable maximum likelihood estimation and evaluate its
performance through simulation studies. To demonstrate practical utility, we
apply the NCST model to breast cancer diagnostic data, modeling multiple
features of tumor shape. The NCST model achieves a superior fit based on
information criteria and visual diagnostics, particularly in the presence of
skewness and heavy tails compared to standard alternatives, including the
multivariate normal, skew normal, and Azzalini's skew $t$ distribution. Our
findings suggest that the NCST distribution offers a useful and interpretable
choice for modeling complex multivariate data, which highlights promising
directions for future development in likelihood inference, Bayesian
computation, and applications involving asymmetry and non-Gaussian dependence.

</details>


### [543] [Predictive Causal Inference via Spatio-Temporal Modeling and Penalized Empirical Likelihood](https://arxiv.org/abs/2507.08896)
*Byunghee Lee,Hye Yeon Sin,Joonsung Kang*

Main category: stat.ME

TL;DR: 提出用于预测因果推断的集成框架，结合HMM和MTGCN，在临床领域验证，能适应生物医学数据时空复杂性。


<details>
  <summary>Details</summary>
Motivation: 克服传统单一模型方法在预测因果推断中的局限性。

Method: 结合HMM进行空间健康状态估计，用MTGCN捕捉时间结果轨迹，不对称处理时空信息，扩展标准双重稳健处理效应估计。

Result: 通过模拟研究评估模型在不同条件下的性能。

Conclusion: 该框架在结构上适应生物医学数据常见的时空复杂性，推动了预测因果推断。

Abstract: This study introduces an integrated framework for predictive causal inference
designed to overcome limitations inherent in conventional single model
approaches. Specifically, we combine a Hidden Markov Model (HMM) for spatial
health state estimation with a Multi Task and Multi Graph Convolutional Network
(MTGCN) for capturing temporal outcome trajectories. The framework
asymmetrically treats temporal and spatial information regarding them as
endogenous variables in the outcome regression, and exogenous variables in the
propensity score model, thereby expanding the standard doubly robust treatment
effect estimation to jointly enhance bias correction and predictive accuracy.
To demonstrate its utility, we focus on clinical domains such as cancer,
dementia, and Parkinson disease, where treatment effects are challenging to
observe directly. Simulation studies are conducted to emulate latent disease
dynamics and evaluate the model performance under varying conditions. Overall,
the proposed framework advances predictive causal inference by structurally
adapting to spatiotemporal complexities common in biomedical data.

</details>


### [544] [Convex Clustering](https://arxiv.org/abs/2507.09077)
*Eric C. Chi,Aaron J. Molstad,Zheming Gao*

Main category: stat.ME

TL;DR: 本文综述基于凸优化问题的聚类方法，介绍其特点、理论、算法、计算成本及应用。


<details>
  <summary>Details</summary>
Motivation: 现有聚类方法众多，凸聚类有独特特征，值得综述介绍。

Method: 回顾基于凸优化问题的聚类方法，给出原理和理论，介绍重要算法及计算成本。

Result: 凸优化问题无虚假局部极小值，全局极小值稳定，可通过单一调参控制簇数量。

Conclusion: 凸聚类方法有良好特性，且使用范围广，可与其他推理方法结合。

Abstract: This survey reviews a clustering method based on solving a convex
optimization problem. Despite the plethora of existing clustering methods,
convex clustering has several uncommon features that distinguish it from prior
art. The optimization problem is free of spurious local minima, and its unique
global minimizer is stable with respect to all its inputs, including the data,
a tuning parameter, and weight hyperparameters. Its single tuning parameter
controls the number of clusters and can be chosen using standard techniques
from penalized regression. We give intuition into the behavior and theory for
convex clustering as well as practical guidance. We highlight important
algorithms and give insight into how their computational costs scale with the
problem size. Finally, we highlight the breadth of its uses and flexibility to
be combined and integrated with other inferential methods.

</details>


### [545] [Sharp Trade-Offs in High-Dimensional Inference via 2-Level SLOPE](https://arxiv.org/abs/2507.09110)
*Zhiqi Bu,Jason M. Klusowski,Cynthia Rush,Ruijia Wu*

Main category: stat.ME

TL;DR: 本文研究2 - level SLOPE，一种SLOPE的重要子类，证明其保留SLOPE优点，有计算优势，适用于高维数据分析。


<details>
  <summary>Details</summary>
Motivation: SLOPE调参困难，尤其是在高维情况下，需要寻找更易调参的方法。

Method: 对2 - level SLOPE进行实证和分析研究，证明其在TPP和FDP间的权衡特性。

Result: 2 - level SLOPE保留SLOPE优点，减少调参空间，在高相关、高噪声和非稀疏信号场景有效。

Conclusion: 2 - level SLOPE是LASSO和一般SLOPE的稳健、可扩展替代方案，适合实际高维数据分析。

Abstract: Among techniques for high-dimensional linear regression, Sorted L-One
Penalized Estimation (SLOPE) generalizes the LASSO via an adaptive $l_1$
regularization that applies heavier penalties to larger coefficients in the
model. To achieve such adaptivity, SLOPE requires the specification of a
complex hierarchy of penalties, i.e., a monotone penalty sequence in $R^p$, in
contrast to a single penalty scalar for LASSO. Tuning this sequence when $p$ is
large poses a challenge, as brute force search over a grid of values is
computationally prohibitive. In this work, we study the 2-level SLOPE, an
important subclass of SLOPE, with only three hyperparameters. We demonstrate
both empirically and analytically that 2-level SLOPE not only preserves the
advantages of general SLOPE -- such as improved mean squared error and
overcoming the Donoho-Tanner power limit -- but also exhibits computational
benefits by reducing the penalty hyperparameter space. In particular, we prove
that 2-level SLOPE admits a sharp, theoretically tight characterization of the
trade-off between true positive proportion (TPP) and false discovery proportion
(FDP), contrasting with general SLOPE where only upper and lower bounds are
known. Empirical evaluations further underscore the effectiveness of 2-level
SLOPE in settings where predictors exhibit high correlation, when the noise is
large, or when the underlying signal is not sparse. Our results suggest that
2-level SLOPE offers a robust, scalable alternative to both LASSO and general
SLOPE, making it particularly suited for practical high-dimensional data
analysis.

</details>


### [546] [A Moment-Based Generalization to Post-Prediction Inference](https://arxiv.org/abs/2507.09119)
*Stephen Salerno,Kentaro Hoffman,Awan Afiaz,Anna Neufeld,Tyler H. McCormick,Jeffrey T. Leek*

Main category: stat.ME

TL;DR: 本文回顾Wang等人方法，提出扩展方法，模拟显示该方法能保持名义一类错误率、减少偏差并实现适当覆盖。


<details>
  <summary>Details</summary>
Motivation: 直接将AI/ML预测结果作为真实观测数据会导致有偏结果和错误推断，虽已有方法但需回顾Wang等人方法并改进。

Method: 反思Wang等人方法的假设，提出放松假设的简单扩展方法，包含在标准条件下得出无偏点估计和引入简单缩放因子。

Result: 在大量模拟中，该方法保持名义一类错误率，减少偏差，实现适当覆盖。

Conclusion: 所提出的扩展方法有效，可用于处理预测数据的推断问题。

Abstract: Artificial intelligence (AI) and machine learning (ML) are increasingly used
to generate data for downstream analyses, yet naively treating these
predictions as true observations can lead to biased results and incorrect
inference. Wang et al. (2020) proposed a method, post-prediction inference,
which calibrates inference by modeling the relationship between AI/ML-predicted
and observed outcomes in a small, gold-standard sample. Since then, several
methods have been developed for inference with predicted data. We revisit Wang
et al. in light of these recent developments. We reflect on their assumptions
and offer a simple extension of their method which relaxes these assumptions.
Our extension (1) yields unbiased point estimates under standard conditions and
(2) incorporates a simple scaling factor to preserve calibration variability.
In extensive simulations, we show that our method maintains nominal Type I
error rates, reduces bias, and achieves proper coverage.

</details>


### [547] [The BdryMatérn GP: Reliable incorporation of boundary information on irregular domains for Gaussian process modeling](https://arxiv.org/abs/2507.09178)
*Liang Ding,Simon Mak,C. F. Jeff Wu*

Main category: stat.ME

TL;DR: 提出BdryMatérn GP建模框架，用于在不规则域上整合边界信息，证明样本路径特性，给出近似过程和误差分析并通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 高斯过程训练数据有限，现有边界集成高斯过程模型存在不能适应不规则域、缺乏样本路径平滑控制和近似误差分析等问题。

Method: 提出BdryMatérn GP建模框架，利用通过随机偏微分方程公式推导的BdryMatérn协方差核，使用有限元建模进行近似并进行误差分析。

Result: 证明BdryMatérn GP样本路径满足边界条件且有导数平滑控制，给出高效近似过程和误差分析，数值实验验证有效性。

Conclusion: BdryMatérn GP框架能有效在不规则域上整合多种边界信息。

Abstract: Gaussian processes (GPs) are broadly used as surrogate models for expensive
computer simulators of complex phenomena. However, a key bottleneck is that its
training data are generated from this expensive simulator and thus can be
highly limited. A promising solution is to supplement the learning model with
boundary information from scientific knowledge. However, despite recent work on
boundary-integrated GPs, such models largely cannot accommodate boundary
information on irregular (i.e., non-hypercube) domains, and do not provide
sample path smoothness control or approximation error analysis, both of which
are important for reliable surrogate modeling. We thus propose a novel
BdryMat\'ern GP modeling framework, which can reliably integrate Dirichlet,
Neumann and Robin boundaries on an irregular connected domain with a boundary
set that is twice-differentiable almost everywhere. Our model leverages a new
BdryMat\'ern covariance kernel derived in path integral form via a stochastic
partial differential equation formulation. Similar to the GP with Mat\'ern
kernel, we prove that sample paths from the BdryMat\'ern GP satisfy the desired
boundaries with smoothness control on its derivatives. We further present an
efficient approximation procedure for the BdryMat\'ern kernel using finite
element modeling with rigorous error analysis. Finally, we demonstrate the
effectiveness of the BdryMat\'ern GP in a suite of numerical experiments on
incorporating broad boundaries on irregular domains.

</details>


### [548] [Robust Spatiotemporal Epidemic Modeling with Integrated Adaptive Outlier Detection](https://arxiv.org/abs/2507.09380)
*Haoming Shi,Shan Yu,Eric C. Chi*

Main category: stat.ME

TL;DR: 本文介绍了一种鲁棒时空广义相加模型（RST - GAM）来处理疫情建模中的离群值问题，推导算法估计参数并证明有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 疫情建模中离群值会扭曲参数估计和误导公共卫生决策，现有方法缺乏同时检测离群值的能力。

Method: 引入均值漂移参数量化和调整离群值影响，使用自适应Lasso正则化，用单变量多项式样条和双变量惩罚样条估计函数形式，采用数据稀疏法构建权重，推导可扩展的近端算法估计参数。

Result: 建立了估计参数的误差界和选择一致性，通过数值研究证明模型在不同离群值场景下的有效性，分析美国县级COVID - 19感染数据体现其实用性。

Conclusion: RST - GAM有潜力为公共卫生决策提供信息。

Abstract: In epidemic modeling, outliers can distort parameter estimation and
ultimately lead to misguided public health decisions. Although there are
existing robust methods that can mitigate this distortion, the ability to
simultaneously detect outliers is equally vital for identifying potential
disease hotspots. In this work, we introduce a robust spatiotemporal
generalized additive model (RST-GAM) to address this need. We accomplish this
with a mean-shift parameter to quantify and adjust for the effects of outliers
and rely on adaptive Lasso regularization to model the sparsity of outlying
observations. We use univariate polynomial splines and bivariate penalized
splines over triangulations to estimate the functional forms and a
data-thinning approach for data-adaptive weight construction. We derive a
scalable proximal algorithm to estimate model parameters by minimizing a convex
negative log-quasi-likelihood function. Our algorithm uses adaptive step-sizes
to ensure global convergence of the resulting iterate sequence. We establish
error bounds and selection consistency for the estimated parameters and
demonstrate our model's effectiveness through numerical studies under various
outlier scenarios. Finally, we demonstrate the practical utility of RST-GAM by
analyzing county-level COVID-19 infection data in the United States,
highlighting its potential to inform public health decision-making.

</details>


### [549] [Statistical Inference for Conditional Group Distributionally Robust Optimization with Cross-Entropy Loss](https://arxiv.org/abs/2507.09905)
*Zijian Guo,Zhenyu Wang,Yifan Hu,Francis Bach*

Main category: stat.ME

TL;DR: 研究多源无监督域适应，提出CG - DRO框架及Mirror Prox算法解决分布偏移问题，建立收敛率，引入扰动推理程序解决非标准渐近问题。


<details>
  <summary>Details</summary>
Motivation: 在多源学习中，不同领域的分布异质性对开发能可靠迁移到未知领域的预测模型构成挑战，需解决潜在分布偏移问题。

Method: 提出Conditional Group Distributionally Robust Optimization (CG - DRO)框架，开发Mirror Prox算法，采用双机器学习程序估计风险函数，构建两个替代极小极大优化问题建立收敛率，引入基于扰动的推理程序。

Result: 建立了估计器的快速统计收敛率，提出的方法能在协变量偏移下保持统计效率，引入的推理程序可进行有效推理。

Conclusion: CG - DRO框架及相关算法和推理程序可有效解决多源无监督域适应中的分布偏移和非标准渐近问题。

Abstract: In multi-source learning with discrete labels, distributional heterogeneity
across domains poses a central challenge to developing predictive models that
transfer reliably to unseen domains. We study multi-source unsupervised domain
adaptation, where labeled data are drawn from multiple source domains and only
unlabeled data from a target domain. To address potential distribution shifts,
we propose a novel Conditional Group Distributionally Robust Optimization
(CG-DRO) framework that learns a classifier by minimizing the worst-case
cross-entropy loss over the convex combinations of the conditional outcome
distributions from the sources. To solve the resulting minimax problem, we
develop an efficient Mirror Prox algorithm, where we employ a double machine
learning procedure to estimate the risk function. This ensures that the errors
of the machine learning estimators for the nuisance models enter only at
higher-order rates, thereby preserving statistical efficiency under covariate
shift. We establish fast statistical convergence rates for the estimator by
constructing two surrogate minimax optimization problems that serve as
theoretical bridges. A distinguishing challenge for CG-DRO is the emergence of
nonstandard asymptotics: the empirical estimator may fail to converge to a
standard limiting distribution due to boundary effects and system instability.
To address this, we introduce a perturbation-based inference procedure that
enables uniformly valid inference, including confidence interval construction
and hypothesis testing.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [550] [Ranked Pairs minimizes the $p$-norm as $p \to \infty$](https://arxiv.org/abs/2507.09654)
*Amir Babak Aazami,Hubert L. Bray*

Main category: econ.TH

TL;DR: 证明Ranked Pairs排序在p趋于无穷时最小化违背其排序的成对竞选优势的p - 范数。


<details>
  <summary>Details</summary>
Motivation: 探究Ranked Pairs排序在特定范数下的特性

Method: 通过数学证明

Result: Ranked Pairs排序能在p趋于无穷时最小化违背其排序的成对竞选优势的p - 范数

Conclusion: Ranked Pairs排序具有在极限情况下最小化特定p - 范数的性质。

Abstract: We prove that Ranked Pairs orders candidates in such a way as to minimize the
$p$-norm, in the limit as $p \to \infty$, of those head-to-head margins of
victory which go against its ordering.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [551] [Generation of structure-guided pMHC-I libraries using Diffusion Models](https://arxiv.org/abs/2507.08902)
*Sergio Mares,Ariel Espinoza Weinberger,Nilah M. Ioannidis*

Main category: q-bio.QM

TL;DR: 本文引入基于扩散模型的pMHC - I肽结构导向基准，发现现有序列预测器有局限，设计管道产出的肽可用于无偏模型训练评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准和模型存在质谱及结合测定数据集的偏差，限制了新肽配体的发现。

Method: 引入基于晶体结构相互作用距离的扩散模型设计pMHC - I肽的结构导向基准。

Result: 现有基于序列的预测器难以识别结构稳定设计的结合潜力，设计管道产出的肽结构完整性高、残基多样性高。

Conclusion: 新基准和设计管道是无偏模型训练和评估的关键资源。

Abstract: Personalized vaccines and T-cell immunotherapies depend critically on
identifying peptide-MHC class I (pMHC-I) interactions capable of eliciting
potent immune responses. However, current benchmarks and models inherit biases
present in mass-spectrometry and binding-assay datasets, limiting discovery of
novel peptide ligands. To address this issue, we introduce a structure-guided
benchmark of pMHC-I peptides designed using diffusion models conditioned on
crystal structure interaction distances. Spanning twenty high-priority HLA
alleles, this benchmark is independent of previously characterized peptides yet
reproduces canonical anchor residue preferences, indicating structural
generalization without experimental dataset bias. Using this resource, we
demonstrate that state-of-the-art sequence-based predictors perform poorly at
recognizing the binding potential of these structurally stable designs,
indicating allele-specific limitations invisible in conventional evaluations.
Our geometry-aware design pipeline yields peptides with high predicted
structural integrity and higher residue diversity than existing datasets,
representing a key resource for unbiased model training and evaluation. Our
code, and data are available at: https://github.com/sermare/struct-mhc-dev.

</details>


### [552] [From Classical Machine Learning to Emerging Foundation Models: Review on Multimodal Data Integration for Cancer Research](https://arxiv.org/abs/2507.09028)
*Amgad Muneer,Muhammad Waqas,Maliazurina B Saad,Eman Showkatian,Rukhmini Bandyopadhyay,Hui Xu,Wentao Li,Joe Y Chang,Zhongxing Liao,Cara Haymaker,Luisa Solis Soto,Carol C Wu,Natalie I Vokes,Xiuning Le,Lauren A Byers,Don L Gibbons,John V Heymach,Jianjun Zhang,Jia Wu*

Main category: q-bio.QM

TL;DR: 本文全面综述肿瘤学中多模态数据整合策略，探讨从传统ML到FMs转变，为下一代大规模预训练模型奠定基础。


<details>
  <summary>Details</summary>
Motivation: 从海量异构癌症数据中提取有效信息是挑战，基础模型为肿瘤研究提供新途径，需综述多模态数据整合策略推动计算方法发展。

Method: 回顾机器学习和深度学习新兴趋势，包括方法框架、验证协议和开源资源，探讨从传统ML到FMs转变，分析多组学与高级成像数据整合问题。

Result: 确定了最先进的基础模型、公开可用的多模态数据库、数据整合的先进工具和方法。

Conclusion: 当前最先进的整合方法为下一代大规模预训练模型奠定基础，该综述首次系统梳理肿瘤学多模态数据整合从传统ML到高级FM的转变。

Abstract: Cancer research is increasingly driven by the integration of diverse data
modalities, spanning from genomics and proteomics to imaging and clinical
factors. However, extracting actionable insights from these vast and
heterogeneous datasets remains a key challenge. The rise of foundation models
(FMs) -- large deep-learning models pretrained on extensive amounts of data
serving as a backbone for a wide range of downstream tasks -- offers new
avenues for discovering biomarkers, improving diagnosis, and personalizing
treatment. This paper presents a comprehensive review of widely adopted
integration strategies of multimodal data to assist advance the computational
approaches for data-driven discoveries in oncology. We examine emerging trends
in machine learning (ML) and deep learning (DL), including methodological
frameworks, validation protocols, and open-source resources targeting cancer
subtype classification, biomarker discovery, treatment guidance, and outcome
prediction. This study also comprehensively covers the shift from traditional
ML to FMs for multimodal integration. We present a holistic view of recent FMs
advancements and challenges faced during the integration of multi-omics with
advanced imaging data. We identify the state-of-the-art FMs, publicly available
multi-modal repositories, and advanced tools and methods for data integration.
We argue that current state-of-the-art integrative methods provide the
essential groundwork for developing the next generation of large-scale,
pre-trained models poised to further revolutionize oncology. To the best of our
knowledge, this is the first review to systematically map the transition from
conventional ML to advanced FM for multimodal data integration in oncology,
while also framing these developments as foundational for the forthcoming era
of large-scale AI models in cancer research.

</details>


### [553] [A PBN-RL-XAI Framework for Discovering a "Hit-and-Run'' Therapeutic Strategy in Melanoma](https://arxiv.org/abs/2507.10136)
*Zhonglin Liu*

Main category: q-bio.QM

TL;DR: 构建概率布尔网络模型，用强化学习和可解释AI，发现4步临时抑制LOXL2克服黑色素瘤免疫治疗耐药。


<details>
  <summary>Details</summary>
Motivation: 转移性黑色素瘤对PD - 1免疫治疗的先天耐药是临床挑战，潜在分子网络不明。

Method: 用患者肿瘤活检转录组数据构建概率布尔网络模型，用强化学习发现多步治疗干预，用可解释AI解释控制策略。

Result: 精准定时的4步临时抑制LOXL2是最有效策略，可消除耐药分子特征。

Conclusion: 提出克服免疫治疗耐药的新的时间依赖性治疗假设，提供识别复杂生物系统干预方案的计算框架。

Abstract: Innate resistance to anti-PD-1 immunotherapy remains a major clinical
challenge in metastatic melanoma, with the underlying molecular networks being
poorly understood. To address this, we constructed a dynamic Probabilistic
Boolean Network model using transcriptomic data from patient tumor biopsies to
elucidate the regulatory logic governing therapy response. We then employed a
reinforcement learning agent to systematically discover optimal, multi-step
therapeutic interventions and used explainable artificial intelligence to
mechanistically interpret the agent's control policy. The analysis revealed
that a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2
protein (LOXL2) was the most effective strategy. Our explainable analysis
showed that this ``hit-and-run" intervention is sufficient to erase the
molecular signature driving resistance, allowing the network to self-correct
without requiring sustained intervention. This study presents a novel,
time-dependent therapeutic hypothesis for overcoming immunotherapy resistance
and provides a powerful computational framework for identifying non-obvious
intervention protocols in complex biological systems.

</details>
